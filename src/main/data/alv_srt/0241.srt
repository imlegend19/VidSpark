1
00:00:00,000 --> 00:00:02,230
and then there is there

2
00:00:02,280 --> 00:00:06,800
the second parameter which is the widest death rate for that which is the probability

3
00:00:06,800 --> 00:00:12,770
that somebody gets so the effective low here is that this happens with probability that

4
00:00:12,850 --> 00:00:16,260
and all pull all the question asking is

5
00:00:17,260 --> 00:00:24,100
whether whether survive this this model is called SAS where

6
00:00:24,140 --> 00:00:28,610
and this is this is like a state diagram right a note can be either

7
00:00:28,610 --> 00:00:32,410
either of these two states it can be healthy and susceptible to the virus before

8
00:00:32,410 --> 00:00:36,590
it can be effective in can fact people and with probability

9
00:00:37,320 --> 00:00:38,670
enabling factors

10
00:00:38,690 --> 00:00:41,660
and the probability that the BBC going here

11
00:00:41,710 --> 00:00:43,220
get get well

12
00:00:43,230 --> 00:00:45,260
and the question is

13
00:00:45,900 --> 00:00:49,100
one thing i should say is we have these wires

14
00:00:49,150 --> 00:00:53,140
which is the ratio of parameters like this would be highly infectious virus and this

15
00:00:53,140 --> 00:00:54,630
is how easy it is to get

16
00:00:55,920 --> 00:01:00,820
and the basic questions that people ask is that if you don't know what is

17
00:01:00,840 --> 00:01:05,730
called the epidemic threshold so given the graph we want to have the value of

18
00:01:05,740 --> 00:01:10,410
the epidemic threshold so that if the if the strength of the virus

19
00:01:10,420 --> 00:01:15,320
it is below the threshold the virus get sixty even if you producing the network

20
00:01:15,320 --> 00:01:17,920
and otherwise the stays alive for

21
00:01:18,000 --> 00:01:20,130
so what is

22
00:01:20,930 --> 00:01:23,730
is a graph

23
00:01:23,780 --> 00:01:27,160
and we want to get this epidemic threshold

24
00:01:27,200 --> 00:01:32,020
and the question is so just interested in what this epidemic threshold depends for example

25
00:01:32,030 --> 00:01:37,000
the of the network sort of the higher this threshold should be started below threshold

26
00:01:37,000 --> 00:01:40,830
should be higher the more people that connected these it should be for the virus

27
00:01:40,830 --> 00:01:44,380
to take over and for example then the question is is the highest degree is

28
00:01:44,380 --> 00:01:46,220
the average degree then

29
00:01:46,230 --> 00:01:49,200
then the other thing would also they on the right the more the network is

30
00:01:49,200 --> 00:01:51,000
sort of of spread out the

31
00:01:51,020 --> 00:01:53,490
part of it would be for the virus to take over

32
00:01:53,510 --> 00:01:56,930
and i don't want to go into too many details but it turns so the

33
00:01:56,930 --> 00:02:00,580
answer is very simple and this is this is the answer so what he says

34
00:02:00,580 --> 00:02:04,630
it if you go into take large the largest tiger of of the adjacency matrix

35
00:02:04,630 --> 00:02:06,500
of the of the grand

36
00:02:06,510 --> 00:02:12,000
that we are interested in what it says that one of the slides

37
00:02:12,050 --> 00:02:17,330
i j largest eigen value of the adjacency matrix this is the epidemic threshold right

38
00:02:17,340 --> 00:02:23,860
so if the ratio of the virus that is is over or below this value

39
00:02:23,870 --> 00:02:26,910
this tell us the virus lies in the world

40
00:02:26,960 --> 00:02:31,170
so what is here is that just seem to have a tie the lighter side

41
00:02:31,250 --> 00:02:35,190
sort of a strong enough to capture all the properties of the graph to tell

42
00:02:35,190 --> 00:02:38,470
us whether the widest survival nine

43
00:02:38,490 --> 00:02:43,130
so i've said and here is an example of the of the simulation so we

44
00:02:43,130 --> 00:02:46,410
have an autonomous systems can to be like the graph of thinking

45
00:02:48,130 --> 00:02:49,700
it's sort of small but

46
00:02:49,760 --> 00:02:54,520
right if we are above the epidemic threshold so we start with all the nodes

47
00:02:54,520 --> 00:02:59,180
in fact and then if you are above the threshold this sort of reaches steady

48
00:02:59,180 --> 00:03:03,920
state and the white space in the in the in the network for while if

49
00:03:03,920 --> 00:03:08,340
we are below the threshold the virus dies thinking and if we are at the

50
00:03:08,340 --> 00:03:12,960
threshold sort of that slow and and there are ways to quantify what what we

51
00:03:12,960 --> 00:03:16,820
mean by quickly and what mean by slowly but this is just to give a

52
00:03:16,820 --> 00:03:19,830
flavor of one line of research on

53
00:03:19,840 --> 00:03:25,740
how things spread over the network

54
00:03:25,760 --> 00:03:27,930
o this study these are the parameters

55
00:03:27,940 --> 00:03:32,150
right so somebody comes and tells you this is the african is about so this

56
00:03:32,150 --> 00:03:34,010
is the parameters of the model

57
00:03:34,380 --> 00:03:37,600
i know the properties of the virus

58
00:03:39,810 --> 00:03:43,960
so now i want to talk about the other line of the the diffusion that's

59
00:03:43,960 --> 00:03:49,380
called the different models of diffusion in networks and you hear something similar going so

60
00:03:49,420 --> 00:03:52,890
we have some nodes are should act and then this

61
00:03:52,900 --> 00:03:56,730
nodes spread their influence on the network to other nodes and so on so for

62
00:03:56,730 --> 00:04:02,470
example if nodes and are active what particular time then they can influence the them

63
00:04:02,490 --> 00:04:06,690
know that gets influenced and so on and this continues and this will be the

64
00:04:06,690 --> 00:04:11,450
processes that we are studying and there are two ways to think about it so

65
00:04:11,450 --> 00:04:15,840
the first one that i will that point is the threshold models

66
00:04:15,870 --> 00:04:20,780
and again so what we have we have a weighted graph where the weight

67
00:04:20,780 --> 00:04:25,670
thirty seconds of by first piano concert played by going glenn gould

68
00:04:27,010 --> 00:04:29,610
this is thirty seconds of silence

69
00:04:29,620 --> 00:04:36,000
and the interesting thing is that while you're here to while you listen to music

70
00:04:36,000 --> 00:04:40,270
than the auditory cortex contains some the senior field

71
00:04:40,290 --> 00:04:42,360
it's just the queen incidents

72
00:04:42,370 --> 00:04:46,520
so this was a very good test bed for us to to try to measure

73
00:04:46,520 --> 00:04:47,590
the senior

74
00:04:47,600 --> 00:04:51,420
if it's it's actually something very very difficult

75
00:04:51,460 --> 00:04:53,980
two to measure

76
00:04:56,760 --> 00:05:01,320
in fact the reason why people are interested in the senior fields is because in

77
00:05:02,390 --> 00:05:07,150
also my green are any lesion

78
00:05:07,200 --> 00:05:10,810
actually has the senior fields

79
00:05:10,870 --> 00:05:17,540
and so stroke while my greenwood been lesions like that and given that you now

80
00:05:17,540 --> 00:05:23,000
have established that that you actually able to reproduce so to say the paradigm on

81
00:05:23,000 --> 00:05:28,740
sub in an unsupervised manner you can now use this technology for monitoring stroke patients

82
00:05:29,220 --> 00:05:31,850
you could and things like that

83
00:05:31,860 --> 00:05:34,720
that was the medical idea

84
00:05:34,730 --> 00:05:36,730
behind that

85
00:05:39,810 --> 00:05:42,520
so what are the conclusions that we can draw

86
00:05:42,530 --> 00:05:46,730
so first of all we saw that this reliability assessment works

87
00:05:46,740 --> 00:05:52,710
this allows us to select between algorithms or select

88
00:05:52,710 --> 00:05:59,350
iteratively atively between algorithms so we take our favourite doesn't algorithms and we take the

89
00:05:59,350 --> 00:06:03,650
one that is most able extract the first component

90
00:06:04,220 --> 00:06:08,610
you know go to the subspace where the first component doesn't live and do this

91
00:06:08,620 --> 00:06:10,720
over until we finish

92
00:06:11,550 --> 00:06:16,340
so this actually makes a lot of this so this would be operating these algorithms

93
00:06:16,770 --> 00:06:19,560
in some kind of hybrid deflation mode

94
00:06:19,580 --> 00:06:21,810
and you can gain a lot from that

95
00:06:21,870 --> 00:06:29,350
but also this has been used by the group of trouble money for assessing the

96
00:06:29,390 --> 00:06:33,910
stability of clusters solutions

97
00:06:33,930 --> 00:06:36,520
so but not asking questions

98
00:06:36,580 --> 00:06:41,740
yesterday and so i had to you know get some slides

99
00:06:41,740 --> 00:06:46,240
so i mean this is never told about this so far

100
00:06:48,040 --> 00:06:51,870
this also has to do with ICA so this so to say the end of

101
00:06:51,870 --> 00:06:53,470
the ICA a part of

102
00:06:53,490 --> 00:06:58,390
so i i mentioned briefly that there are some

103
00:06:58,410 --> 00:06:59,770
some signals

104
00:06:59,810 --> 00:07:01,890
that you can use for

105
00:07:01,950 --> 00:07:10,020
for the brain computer and facing like the slow cortical potentials to the dynamic features

106
00:07:10,020 --> 00:07:12,560
like you're them

107
00:07:14,790 --> 00:07:20,640
there was the third one which i don't forget but anyway the

108
00:07:21,100 --> 00:07:28,910
the issue is that it is believed that synchronization in the brain synchronization between certain

109
00:07:28,910 --> 00:07:33,600
ensembles of neurons is actually means for computation

110
00:07:33,620 --> 00:07:38,370
and i think your used it also for BCI studies

111
00:07:38,390 --> 00:07:40,790
and so

112
00:07:40,830 --> 00:07:43,410
there was what what was worrying

113
00:07:43,430 --> 00:07:45,140
as in this

114
00:07:45,200 --> 00:07:51,260
work was actually t to be using this synchronisation between

115
00:07:51,270 --> 00:07:56,140
you know ensembles of neurons as we can measure it with e in order to

116
00:07:56,520 --> 00:07:59,000
to improve or BCI performance

117
00:07:59,020 --> 00:08:04,540
of the BCI performance of system but there is a systematic problem that i will

118
00:08:04,600 --> 00:08:09,120
show you and also show you the way to solve it so

119
00:08:09,580 --> 00:08:14,540
imagine you have some dynamical system

120
00:08:14,540 --> 00:08:19,390
and so this is very abstract but you could think of some ensembles that have

121
00:08:19,390 --> 00:08:21,830
some some dynamics

122
00:08:21,850 --> 00:08:24,700
the brain of neural assemblies

123
00:08:26,870 --> 00:08:28,660
what you would be interested in

124
00:08:28,680 --> 00:08:36,660
so so imagine these these ensembles they they make some they have some oscillations

125
00:08:36,680 --> 00:08:40,160
and you would like to measure the

126
00:08:40,180 --> 00:08:45,200
if you measure the core as they say there signal that that

127
00:08:45,220 --> 00:08:48,950
that is as the phase difference of of

128
00:08:48,970 --> 00:08:50,910
and like this

129
00:08:51,700 --> 00:08:55,120
if you take the correlation it would be zero

130
00:08:55,120 --> 00:09:01,240
but if you take the local phase so the local phase angle at every point

131
00:09:01,240 --> 00:09:07,540
so the first so how do explain this into intuitively so if you define the

132
00:09:07,540 --> 00:09:09,600
local phase

133
00:09:09,660 --> 00:09:11,120
is like

134
00:09:12,140 --> 00:09:17,850
how do you say this this thing from watch the hand watch that moves around

135
00:09:17,950 --> 00:09:25,380
OK so you define your day it year oscillators like that by their local faces

136
00:09:25,380 --> 00:09:29,830
and you have the same second oscillator like that and in this example that i

137
00:09:29,830 --> 00:09:34,350
just made you see that the correlation of the phases will will be very

138
00:09:34,370 --> 00:09:37,370
strong because they are fixed and

139
00:09:37,390 --> 00:09:41,700
so there phase locked so to say and

140
00:09:41,720 --> 00:09:44,180
it is believed that

141
00:09:44,200 --> 00:09:50,310
synchronous oscillations of very good means for actually

142
00:09:50,330 --> 00:09:51,950
computing with the brain

143
00:09:51,970 --> 00:10:00,140
OK so let me just so if i would be this phase this local phase

144
00:10:01,470 --> 00:10:03,220
this the definition of the

145
00:10:03,240 --> 00:10:08,990
of the phase synchronization index which is the exponent of i and this is the

146
00:10:08,990 --> 00:10:12,770
problem and this is a no sorry this is the

147
00:10:12,790 --> 00:10:17,790
the difference between block faces of two sources OK

148
00:10:17,790 --> 00:10:20,390
this is just numbers

149
00:10:20,410 --> 00:10:25,120
so you would have say two dynamical systems that could go or go around some

150
00:10:25,120 --> 00:10:27,080
attractor and you would

151
00:10:27,100 --> 00:10:33,790
i have some phase locking between and you can can compute yourself come on

152
00:10:33,810 --> 00:10:43,330
so you can measure the instantaneous phases by

153
00:10:43,330 --> 00:10:45,330
handwaving or by

154
00:10:45,330 --> 00:10:49,480
compute derivatives

155
00:10:49,540 --> 00:10:51,890
and solve

156
00:10:51,950 --> 00:10:54,870
four equal to zero

157
00:10:55,080 --> 00:11:04,120
i'm pretty much all the calculations i'm doing here with this new example i'm going

158
00:11:04,120 --> 00:11:05,560
to do them again

159
00:11:07,620 --> 00:11:11,470
generality for log linear models and conditional random fields

160
00:11:13,870 --> 00:11:16,140
what's the derivative of

161
00:11:16,200 --> 00:11:18,540
so what i need is the

162
00:11:20,750 --> 00:11:22,680
d by the state

163
00:11:24,640 --> 00:11:26,980
say to to the age

164
00:11:27,000 --> 00:11:29,410
one minus data

165
00:11:29,410 --> 00:11:31,370
to the n minus h

166
00:11:31,370 --> 00:11:34,970
and let me point out so this is the bottom of page three in the

167
00:11:34,970 --> 00:11:39,660
notes and there's a typo in the notes

168
00:11:40,450 --> 00:11:43,200
it says d by DP

169
00:11:43,270 --> 00:11:46,910
but what we what actually means by the state

170
00:11:50,060 --> 00:11:55,730
and i have an occasional question so this is the curly d

171
00:11:55,770 --> 00:12:00,870
four derivative instead of the regular d for derivative and what's the difference in meaning

172
00:12:00,870 --> 00:12:07,060
between curly DNA regular differ derivative

173
00:12:11,140 --> 00:12:12,790
a single parameter so

174
00:12:12,850 --> 00:12:14,620
if you have a function

175
00:12:14,660 --> 00:12:15,870
say have

176
00:12:15,870 --> 00:12:20,620
which is a function of a single argument say e

177
00:12:20,640 --> 00:12:24,870
then that function is called one derivative the director with respect and you can call

178
00:12:24,870 --> 00:12:26,310
that debate easy

179
00:12:26,350 --> 00:12:30,540
but if a function which has more than one argument is going to derivative with

180
00:12:30,540 --> 00:12:33,450
respect to each of its arguments and so on

181
00:12:33,450 --> 00:12:36,370
the user could be t to indicate the derivative

182
00:12:36,750 --> 00:12:45,060
with respect to any particular argument and something that is quite common phenomenon here when

183
00:12:45,060 --> 00:12:48,060
we do dealing with maximum likelihood is that

184
00:12:48,060 --> 00:12:53,040
there's some ambiguity about how many arguments your functions have

185
00:12:53,080 --> 00:12:57,930
because you think of something you think of some arguments is being fixed and then

186
00:12:57,930 --> 00:13:02,890
other and and if an argument is fixed is not really an argument but

187
00:13:02,910 --> 00:13:06,580
you can always then say well suppose not fixed and then you could take the

188
00:13:06,580 --> 00:13:09,540
derivative with respect to that so here

189
00:13:09,620 --> 00:13:12,770
this likelihood

190
00:13:13,430 --> 00:13:16,730
i was thinking of the exercise is being fixed so we're not gonna take the

191
00:13:16,730 --> 00:13:18,310
derivative with respect to them

192
00:13:18,390 --> 00:13:21,930
but you know there's something which is even more fixed and the excise which is

193
00:13:21,930 --> 00:13:25,100
an because even before we

194
00:13:25,140 --> 00:13:26,950
drew the training set

195
00:13:27,000 --> 00:13:32,410
we knew that there was a constant and of how many training examples control

196
00:13:34,890 --> 00:13:36,970
so in a sense

197
00:13:37,100 --> 00:13:41,580
this function is a function of and it's a function of the axis through agents

198
00:13:41,580 --> 00:13:43,750
the function of data

199
00:13:44,750 --> 00:13:49,730
but and i'm thinking of was being totally fixed and h thinking of is being

200
00:13:49,730 --> 00:13:53,930
fixed for the time being and really the only thing i'm thinking of is being

201
00:13:53,930 --> 00:13:58,910
varying hero status i could write the by these data but i i chose to

202
00:13:58,910 --> 00:14:01,040
write curly divide the

203
00:14:01,400 --> 00:14:03,980
but it's always

204
00:14:04,000 --> 00:14:07,600
you should always ask yourself so well what is what is fixed and what is

205
00:14:07,600 --> 00:14:11,620
not fixed and they can be degrees of fix this

206
00:14:11,710 --> 00:14:13,310
OK so now

207
00:14:13,310 --> 00:14:15,700
and it's gonna work out this derivative

208
00:14:15,810 --> 00:14:19,640
and you know the

209
00:14:19,640 --> 00:14:23,640
the derivative of product

210
00:14:23,660 --> 00:14:25,020
so the

211
00:14:25,040 --> 00:14:27,100
the mnemonic is

212
00:14:27,120 --> 00:14:32,020
FG prime is f prime g plus fifty prime

213
00:14:32,060 --> 00:14:33,730
and then

214
00:14:33,980 --> 00:14:37,120
as composer g prime

215
00:14:39,970 --> 00:14:43,520
OK it's f of g prime

216
00:14:43,580 --> 00:14:48,060
so now we have a prime of g

217
00:14:48,100 --> 00:14:50,520
times g prime

218
00:14:50,580 --> 00:14:54,120
so that was this is the chain rule for derivatives

219
00:14:54,370 --> 00:14:57,540
product so what do i get here

220
00:14:57,600 --> 00:15:02,270
and then so i want to the root of state of the age is just

221
00:15:02,270 --> 00:15:02,980
a on

222
00:15:03,020 --> 00:15:08,700
a monomial so that's h times stated age minus one so i get h times

223
00:15:08,700 --> 00:15:11,200
stated to the age minus one

224
00:15:11,210 --> 00:15:14,770
times one minus data to the and minus h

225
00:15:16,950 --> 00:15:18,450
and then here the

226
00:15:18,500 --> 00:15:20,790
data h stays fixed

227
00:15:20,790 --> 00:15:25,810
and i get n minus h times

228
00:15:25,830 --> 00:15:29,020
one minus stated to the n minus h minus one

229
00:15:29,080 --> 00:15:32,410
and then i need a

230
00:15:32,410 --> 00:15:36,120
minus one because i had a minus data here

231
00:15:40,160 --> 00:15:43,430
i'm going to do so this is

232
00:15:43,480 --> 00:15:45,660
the sum of two terms

233
00:15:45,750 --> 00:15:46,790
i'm going to

234
00:15:46,790 --> 00:15:49,210
right it a single term

235
00:15:53,810 --> 00:15:56,430
stated to the power is minus one

236
00:15:56,430 --> 00:16:01,290
times one minus data to the power and minors aged minus one

237
00:16:03,470 --> 00:16:08,430
more age times one minus data

238
00:16:11,500 --> 00:16:15,810
um and minus age

239
00:16:15,870 --> 00:16:21,040
data so this is just rewriting this expression

240
00:16:23,390 --> 00:16:26,480
and now it's got the derivative

241
00:16:26,540 --> 00:16:28,430
now i want to

242
00:16:28,480 --> 00:16:30,870
set it equal to zero

243
00:16:41,950 --> 00:16:45,250
so the

244
00:16:45,290 --> 00:16:47,020
during the

245
00:16:47,060 --> 00:16:48,640
equals zero

246
00:16:49,730 --> 00:16:57,000
and there are three cases

247
00:16:57,100 --> 00:17:03,620
one of the three cases

248
00:17:06,230 --> 00:17:09,430
the first cases stated equals zero

249
00:17:09,450 --> 00:17:11,120
the second case is

250
00:17:11,160 --> 00:17:13,350
one minus state equals zero

251
00:17:13,350 --> 00:17:15,200
that case is

252
00:17:15,210 --> 00:17:17,790
age times one minus data

253
00:17:19,700 --> 00:17:21,390
and minus age data

254
00:17:23,810 --> 00:17:25,850
these two cases

255
00:17:25,870 --> 00:17:30,180
you know in these two cases the whole thing equals zero

256
00:17:30,180 --> 00:17:32,920
this probably looks

257
00:17:33,000 --> 00:17:38,690
maybe similar to what you've got no much but some the CCD cameras here

258
00:17:38,700 --> 00:17:40,370
the spectrometers in here

259
00:17:40,390 --> 00:17:41,940
we use

260
00:17:41,960 --> 00:17:44,760
a fully functional light microscope

261
00:17:44,860 --> 00:17:47,280
this allows us to

262
00:17:47,320 --> 00:17:51,480
use all the optical options to visualize sample

263
00:17:51,500 --> 00:17:55,020
so if you have a difficult samples to see if you need to use polarization

264
00:17:55,410 --> 00:17:58,770
fluorescence was type of techniques you can use

265
00:17:58,780 --> 00:18:03,730
this microscope by modifying goodbye makes them i optics for the microscope

266
00:18:03,750 --> 00:18:07,200
you can get this already shown

267
00:18:07,220 --> 00:18:08,810
as a company

268
00:18:09,570 --> 00:18:13,690
spectroscopy is a small part of the range of about ten percent of the total

269
00:18:14,770 --> 00:18:21,380
what high course the area and the company itself is metrology company and we manufacture

270
00:18:21,380 --> 00:18:22,540
measuring two

271
00:18:22,590 --> 00:18:28,600
these precision grating stages are all built by ourselves because we have a high precision

272
00:18:29,290 --> 00:18:31,290
metal on metal to

273
00:18:31,300 --> 00:18:34,890
metro metallurgical company not metrology mythological

274
00:18:34,900 --> 00:18:40,610
we actually have a very accurate measurements and we actually can do this very very

275
00:18:41,710 --> 00:18:45,260
and move the stages so these are all built into the system

276
00:18:45,270 --> 00:18:50,410
a wide choice of excitation wavelengths standard system can run from

277
00:18:50,450 --> 00:18:57,700
the two four for all the way out to a thirty using standard lasers and

278
00:18:57,700 --> 00:19:02,970
we have no done some work ten sixty four doing some roman work in that

279
00:19:03,830 --> 00:19:09,130
so the choice of is there we can get down with this my option here

280
00:19:09,130 --> 00:19:12,340
two about ten wave numbers from the laser light

281
00:19:12,410 --> 00:19:17,030
we have an option to image true imaging of raman not mapping

282
00:19:17,040 --> 00:19:21,430
o point by point mapping is something by using dielectric filters you can filter twenty

283
00:19:22,570 --> 00:19:26,540
twenty with number band from the sample and it because using a CCD

284
00:19:26,920 --> 00:19:30,250
you can actually cause the roman image onto the CCD chip

285
00:19:30,290 --> 00:19:32,760
for rapid looking at

286
00:19:32,770 --> 00:19:34,700
the issues surface control

287
00:19:34,710 --> 00:19:36,410
very useful thing to do

288
00:19:36,420 --> 00:19:40,530
some put something we'll talk about it on the whole system i think comes as

289
00:19:40,530 --> 00:19:41,860
it stands here

290
00:19:41,940 --> 00:19:46,320
and that thing all you require is an electric clock you go into the means

291
00:19:46,320 --> 00:19:49,200
and ready to go

292
00:19:49,210 --> 00:19:52,890
because of the multi-user potential

293
00:19:52,910 --> 00:19:54,430
we developed

294
00:19:54,440 --> 00:19:56,050
an additional

295
00:19:56,070 --> 00:20:01,770
alignment optimisation and automated calibration and automated execution search

296
00:20:01,790 --> 00:20:05,130
everything is still be done in controlled manually

297
00:20:05,150 --> 00:20:08,140
but in this we call college o here

298
00:20:08,160 --> 00:20:12,940
we have to adopt so we can sample a white light source and any unsourced

299
00:20:13,140 --> 00:20:17,600
and these can all be used to automatically set the system and also allows you

300
00:20:17,610 --> 00:20:23,510
to validate the results and your results validated automatically if you choose to do so

301
00:20:23,530 --> 00:20:25,610
the advantage here

302
00:20:25,620 --> 00:20:28,410
is that you can in fact

303
00:20:28,430 --> 00:20:31,510
like almost anyone using instruments

304
00:20:31,530 --> 00:20:35,060
within reason we cannot break it

305
00:20:35,070 --> 00:20:40,060
so what we're looking now is the multi-user system and we this was developed specifically

306
00:20:42,410 --> 00:20:46,420
ITN really the pharmaceutical people who wanted to use running

307
00:20:46,440 --> 00:20:51,060
but they don't want to have a high-level scientists spending all the time on the

308
00:20:51,060 --> 00:20:54,380
twenty of the non-technical people running it

309
00:20:54,440 --> 00:21:00,130
usually with a good education is good scientific background but basically somebody can go and

310
00:21:00,130 --> 00:21:02,370
use these men get the results

311
00:21:02,380 --> 00:21:05,040
the same goes no for all

312
00:21:05,170 --> 00:21:07,420
a lot of academic institutions

313
00:21:07,440 --> 00:21:09,700
because the publisher parish

314
00:21:10,800 --> 00:21:13,450
the the guy is also a weak and run

315
00:21:13,580 --> 00:21:17,440
you spend a week setting up your experiment getting samples

316
00:21:17,460 --> 00:21:20,700
if you go to a traditional roman history you can spend two or three weeks

317
00:21:20,700 --> 00:21:25,380
setting up i'm an instrument to do their analysis with this data samples and run

318
00:21:25,400 --> 00:21:30,860
analysis one day all finished spend the rest of the ten rating of

319
00:21:30,910 --> 00:21:35,550
for some students that starts an advantage for other the students that's an even bigger

320
00:21:35,550 --> 00:21:40,850
advantage but more importantly for the these the simulator for the

321
00:21:40,870 --> 00:21:45,580
supervisors it means we are getting results published

322
00:21:45,600 --> 00:21:48,990
so everything is going and everything is actually being done

323
00:21:49,010 --> 00:21:54,510
and in the modern science department that is critical is critical to the running of

324
00:21:54,510 --> 00:21:55,650
the unit

325
00:21:56,470 --> 00:21:57,540
everything here

326
00:21:57,550 --> 00:22:03,160
it's fully motorized wavelengths as i see it can be changed automatically and

327
00:22:03,180 --> 00:22:07,560
it automatically aligns itself all system like to do everything on its own

328
00:22:11,530 --> 00:22:12,770
it would be

329
00:22:12,790 --> 00:22:16,750
a top-of-the-range instrument with this is not at the moment we have three lasers on

330
00:22:18,610 --> 00:22:20,350
box no

331
00:22:20,390 --> 00:22:22,200
encloses the microscope

332
00:22:22,220 --> 00:22:25,100
and the reason we put enclosures on

333
00:22:25,190 --> 00:22:27,620
and it was set up with safety

334
00:22:27,630 --> 00:22:30,660
yeah that was part of the reason for the most important reason was as i

335
00:22:31,360 --> 00:22:34,870
the pharmaceutical industry they want to put this into the laboratory

336
00:22:34,940 --> 00:22:37,770
they don't want to put it into the red

337
00:22:37,780 --> 00:22:41,440
so with that don't do close to one of the problems with the roman it's

338
00:22:41,440 --> 00:22:44,630
fluorescent lights give around

339
00:22:44,720 --> 00:22:50,620
you close the door there's the fluorescent light you've got noise and interference here missing

340
00:22:50,850 --> 00:22:55,270
the other reason is an addon remove you running along mapping experiments and you take

341
00:22:55,270 --> 00:22:59,280
a long time to analyse the last thing you want someone walking instead of what's

342
00:22:59,280 --> 00:23:01,930
happening in your switching on the lights

343
00:23:01,950 --> 00:23:06,100
and interfering with the experiment so with these you can actually set it up i'm

344
00:23:06,100 --> 00:23:08,980
going to run fully automatically

345
00:23:09,010 --> 00:23:11,050
we can make up on

346
00:23:11,110 --> 00:23:14,190
i understand the system can mode up to four ways of

347
00:23:14,200 --> 00:23:19,070
just because you got four lasers doesn't mean you restricted to four wavelengths

348
00:23:19,080 --> 00:23:23,880
if you take the system three two five five one four seventy five percent three

349
00:23:23,880 --> 00:23:25,700
to five in the cat

350
00:23:25,720 --> 00:23:30,210
so o five one four are good i and diode seventy eighty five

351
00:23:30,460 --> 00:23:32,290
ninety CAD

352
00:23:32,530 --> 00:23:35,550
three to five will give you three to five and four four two

353
00:23:35,560 --> 00:23:37,950
so potentially you could have two wavelengths on their

354
00:23:37,970 --> 00:23:42,520
the more than argon ions will give you four five seven forty eight o five

355
00:23:42,520 --> 00:23:49,840
one four saying three wavelengths on this LST five a six wavelength one system

356
00:23:50,220 --> 00:23:57,080
for the nanotechnology group particularly looking at the silicon wafers looking at the enhancement materials

357
00:23:57,160 --> 00:24:01,150
looking national resident this can be very useful

358
00:24:02,130 --> 00:24:07,730
if you wanted to then go down into the UV and having water cooled quiz

359
00:24:07,880 --> 00:24:14,230
are with two forty four o two six in the movie you i would suggest

360
00:24:14,230 --> 00:24:16,700
you might that separately and then fitted in

361
00:24:16,710 --> 00:24:22,710
the system is a big water cool is very expensive movie about eighty thousand dollars

362
00:24:23,440 --> 00:24:28,270
to fulfill is at the moment which is not an insignificant amount of money that

363
00:24:28,270 --> 00:24:30,270
could also be added

364
00:24:30,280 --> 00:24:34,500
to this this instrument understand the important thing is

365
00:24:34,510 --> 00:24:35,680
quick change the

366
00:24:35,700 --> 00:24:37,150
you to me

367
00:24:37,220 --> 00:24:39,450
but more importantly

368
00:24:39,470 --> 00:24:42,310
automated changeover

369
00:24:42,310 --> 00:24:47,810
serious health effects now what is holding the fixed in this picture me well T

370
00:24:47,810 --> 00:24:54,400
is equal to you plus so you plus is fixed is a constant in other

371
00:24:55,660 --> 00:24:59,370
now where the curves along which you plus b is the constant well they are

372
00:24:59,370 --> 00:25:00,080
these lines

373
00:25:01,420 --> 00:25:06,470
these lines along which you want to be

374
00:25:06,500 --> 00:25:12,880
equals a constant on tedious accounts for the reason I'm holding clear constant is because

375
00:25:12,880 --> 00:25:17,790
the 1st integration only allows you to change years held fixed

376
00:25:20,060 --> 00:25:27,200
OK you let you increase as you increase is held fixed

377
00:25:27,860 --> 00:25:33,760
by traversing these lines in this direction that's the direction on which she was increasing

378
00:25:34,200 --> 00:25:39,240
I integrate from the point from the you value where they leave the region intent

379
00:25:39,300 --> 00:25:44,040
the region what's the value where they enter the region you is equal to

380
00:25:45,490 --> 00:25:49,680
everybody would know that not so many people will be able to figure out what

381
00:25:49,680 --> 00:25:51,700
to put forward leaves the region

382
00:25:53,480 --> 00:25:58,920
what's you the value of you when it leaves the region well this is the

383
00:25:58,920 --> 00:26:01,540
curve equals 0

384
00:26:01,550 --> 00:26:04,180
but the equals 0 is

385
00:26:04,180 --> 00:26:06,030
in another language

386
00:26:06,140 --> 00:26:07,620
u equals to

387
00:26:07,660 --> 00:26:12,270
the latest u equals 0 or you will equals in other words they enter the

388
00:26:12,270 --> 00:26:17,530
region where u equals 0 and they leave where you with his team has led

389
00:26:20,650 --> 00:26:23,660
this is the

390
00:26:26,140 --> 00:26:30,490
and how about the other guys full which cheese do I wanna do this well

391
00:26:30,490 --> 00:26:32,990
I wanted for all these values

392
00:26:33,010 --> 00:26:39,100
well now the T. value here that's the starting line here t is 0 all

393
00:26:39,160 --> 00:26:43,810
and here is not 0 and if I go out and cover the whole first-quarter

394
00:26:43,810 --> 00:26:48,360
and I'll be going letting T increases to infinity the some of you in the

395
00:26:48,360 --> 00:26:52,080
eye will be letting increasing so 0 3

396
00:26:52,290 --> 00:26:58,700
so all this is an exercise in changing taking this double integral and UV coordinates

397
00:26:58,990 --> 00:27:04,190
and changing it to this double integral an equivalent double integral over the same region

398
00:27:04,190 --> 00:27:05,310
but now in

399
00:27:05,340 --> 00:27:10,380
UT coordinates now that's the answer somewhere here is the answer because

400
00:27:11,960 --> 00:27:13,900
because like if

401
00:27:15,200 --> 00:27:20,620
since the 1st integration is with respect to you

402
00:27:22,240 --> 00:27:25,300
can migrate outside

403
00:27:25,340 --> 00:27:26,920
because it doesn't involve you

404
00:27:29,340 --> 00:27:33,780
that only involves T and to is only caught by the 2nd integration

405
00:27:34,260 --> 00:27:39,240
so I can put this outside where and upward the integral from 0 to infinity

406
00:27:39,640 --> 00:27:46,700
of e to the negative st time what's left funny expression but you're under desert

407
00:27:46,700 --> 00:27:48,000
island found

408
00:27:48,200 --> 00:27:54,800
this funny expression integral from 0 to T of the minus you

409
00:27:54,810 --> 00:27:59,120
the ensure the convolution exactly

410
00:28:02,960 --> 00:28:05,460
so all you have to do is get the idea that there might be a

411
00:28:05,460 --> 00:28:07,460
formula sit down

412
00:28:07,470 --> 00:28:11,840
change variables a double integral that you formula well

413
00:28:13,140 --> 00:28:14,680
it is

414
00:28:15,360 --> 00:28:20,380
and I would like to spend much of the rest of the period

415
00:28:21,290 --> 00:28:25,950
In other words so that's how it relates will plus transform that's how it comes

416
00:28:25,950 --> 00:28:30,840
out of all plus transform here's how to use it you know calculating will plus

417
00:28:30,840 --> 00:28:36,230
transform directly from the other and of course solve problems will plus transform problems differential

418
00:28:36,230 --> 00:28:40,920
equation using the convolution but I have to tell you that most people convolution is

419
00:28:40,920 --> 00:28:45,170
very important and most people who use it don't use it in connection with will

420
00:28:45,170 --> 00:28:47,900
plus transform the uses for its own sake

421
00:28:49,340 --> 00:28:50,660
1 of the 1st

422
00:28:51,460 --> 00:28:57,580
place so learned that you know outside MIT people use the convolution with from my

423
00:28:57,580 --> 00:29:05,620
daughter who knows she's environmental engineering consultant the environmental consultant she does risk assessment stuff

424
00:29:05,620 --> 00:29:09,100
like that but anyway he had this paper on acid rain she was trying to

425
00:29:09,100 --> 00:29:11,230
refer clients and

426
00:29:11,880 --> 00:29:18,740
she said knows all about calculated acid rain falls on our soil from there you

427
00:29:18,740 --> 00:29:22,580
will stuff leaches into a river but there are other things happen to it on

428
00:29:22,590 --> 00:29:28,050
the way and soil combined in various ways reduces the acidity of the things that

429
00:29:28,320 --> 00:29:33,520
chemical reactions take place of all anyway she said well that they calculated the and

430
00:29:33,520 --> 00:29:38,040
how much the water again river gets polluted which it but it's a convolution

431
00:29:38,900 --> 00:29:43,080
should that what what's the convolution so I told that she was too young to

432
00:29:43,080 --> 00:29:47,040
learn about the convolutions and I should say I thought I'd better look it up

433
00:29:47,040 --> 00:29:52,780
1st RBI you cost convolution with basil puzzle that applications so I read that there

434
00:29:52,960 --> 00:29:58,530
interesting and I had been thinking about it I think other people come to be

435
00:29:58,530 --> 00:30:02,800
you know some guy with problem about the drilled ice cores of the north pole

436
00:30:02,800 --> 00:30:08,080
and the from the radioactive carbon and so on and with deducing various things about

437
00:30:08,080 --> 00:30:13,680
the climate you 60 billion years ago and it was all convolution being asked me

438
00:30:13,680 --> 00:30:17,940
if I can explain that to him so let me give you a sort of

439
00:30:17,940 --> 00:30:23,380
an all-purpose state at a simple all-purpose model which can be adapted which is a

440
00:30:23,380 --> 00:30:27,300
very good way of thinking of the convolution

441
00:30:29,560 --> 00:30:32,240
is a problem of radioactive dumping

442
00:30:37,400 --> 00:30:45,080
it notes by the way was so I'm just want take a chance on and

443
00:30:46,250 --> 00:30:51,140
listen to what I'm saying relevant scribbling everything down maybe it's a available to figure

444
00:30:51,140 --> 00:30:51,840
out the source

445
00:30:52,550 --> 00:30:57,900
so problem is we have some factory or some nuclear plant or something like that

446
00:30:58,180 --> 00:31:04,470
is producing nuclear material duo radioactive waste and certainly not always at the same rate

447
00:31:04,510 --> 00:31:06,470
of and then it

448
00:31:06,660 --> 00:31:13,250
Florida dumps it on a pile somewhere so radioactive waste

449
00:31:13,440 --> 00:31:23,160
is dumb and there's a dumping function of goal-directed today don't right

450
00:31:23,250 --> 00:31:30,310
that's the dumping rate let's say years and years you like that units and Quantity

451
00:31:30,340 --> 00:31:33,180
kg either whatever value of

452
00:31:34,220 --> 00:31:36,720
now what is the dumping rate means

453
00:31:37,560 --> 00:31:41,260
the dumping rate means that

454
00:31:41,470 --> 00:31:45,120
if I on the time axis

455
00:31:45,140 --> 00:31:50,580
if I have 2 times which a close together for example 2 successive days

456
00:31:50,730 --> 00:31:56,190
new midnight and 2 successive then there's a time interval between them all call that

457
00:31:56,190 --> 00:31:57,790
delta T

458
00:31:58,140 --> 00:32:02,640
to say the dumping rate is half of the means that

459
00:32:02,660 --> 00:32:05,990
the amount Don

460
00:32:06,940 --> 00:32:09,680
is that in this time interval

461
00:32:09,680 --> 00:32:15,430
on the fifteenth

462
00:32:15,560 --> 00:32:23,490
they're moving out of it so i mean maybe to mention a few things which

463
00:32:23,490 --> 00:32:28,690
i have talked about this central dogma

464
00:32:28,720 --> 00:32:36,100
i cannot really answer when it was because they not really educated about that very

465
00:32:36,100 --> 00:32:41,450
few things happen now the first of all there's not only one m on e

466
00:32:41,450 --> 00:32:46,120
what i mean generally that's not only one protein which can be generated from the

467
00:32:46,210 --> 00:32:48,660
there are several ways to there

468
00:32:48,710 --> 00:32:51,970
regulation point so i would like

469
00:32:52,070 --> 00:32:53,370
now what

470
00:32:53,380 --> 00:32:57,490
i think that for instance

471
00:32:57,510 --> 00:33:01,930
to include what moving into is a certain decision and this can be regulated

472
00:33:01,980 --> 00:33:05,390
and depending on external conditions or depending on other

473
00:33:05,400 --> 00:33:06,110
i mean

474
00:33:06,150 --> 00:33:11,150
conditions that might be the case this entrance is removed when i remove this allows

475
00:33:11,150 --> 00:33:16,230
a lot of flexibility not only the transcription which is regulated as the

476
00:33:17,890 --> 00:33:22,820
processing and also the translation is regulated we have a lot of different control points

477
00:33:22,860 --> 00:33:26,110
leading not only to one protein but have

478
00:33:26,120 --> 00:33:27,920
put into supporting it

479
00:33:36,670 --> 00:33:37,540
i think

480
00:33:37,610 --> 00:33:46,920
OK so maybe not thing to mention is that

481
00:33:46,970 --> 00:33:51,880
nowadays it's not only the protein which are also a lot of on a on

482
00:33:51,890 --> 00:33:57,410
courting on a lot at all on a translator forty

483
00:33:57,420 --> 00:34:00,660
on may a just have a whole bunch of

484
00:34:00,710 --> 00:34:03,030
and this might be considerably

485
00:34:03,130 --> 00:34:07,990
so in particular for human known that only

486
00:34:08,010 --> 00:34:12,600
in one of two percent of the human genome actually try to encode for protein

487
00:34:12,610 --> 00:34:13,990
and all the rest

488
00:34:14,010 --> 00:34:18,040
it seems like that and i mean the question what is the stuff and it

489
00:34:18,040 --> 00:34:23,680
seems like it could be on a on eighty nine according to one

490
00:34:23,920 --> 00:34:25,990
structure element

491
00:34:26,000 --> 00:34:31,160
so it's interesting to look into

492
00:34:39,740 --> 00:34:44,960
you mention a few popular types of measurement

493
00:34:45,850 --> 00:34:51,400
what i talked about already there are sequences so

494
00:34:51,430 --> 00:34:56,410
there are a lot of different people can sequenced genomes to be sequenced on a

495
00:34:56,460 --> 00:34:59,370
and we get a big databases full of

496
00:34:59,690 --> 00:35:05,430
what we also get a population data so this is

497
00:35:05,450 --> 00:35:10,320
not only one sequence we have maybe different populations

498
00:35:10,690 --> 00:35:13,980
organisms and have many variations

499
00:35:14,000 --> 00:35:15,780
i talk about one

500
00:35:15,780 --> 00:35:21,030
what also get factual they data like three d structure of proteins

501
00:35:22,840 --> 00:35:24,990
expression data

502
00:35:25,050 --> 00:35:30,130
so we can measure when a certain gene on the benefits of climate conditions

503
00:35:30,170 --> 00:35:36,180
and we have protein networks could be interaction data what could be regulation

504
00:35:36,200 --> 00:35:39,720
so when which protein regulates which

505
00:35:39,790 --> 00:35:43,240
so one which protein interacts with which he

506
00:35:43,260 --> 00:35:45,550
OK so we have actually network here

507
00:35:45,590 --> 00:35:48,670
the corner

508
00:35:48,720 --> 00:35:52,430
just text data compression we have arguments

509
00:35:55,620 --> 00:36:00,690
OK let me mention a bit more on the way think

510
00:36:00,720 --> 00:36:08,750
it was developed by sun nineteen seventy and nowadays because you can essentially stretches of

511
00:36:08,760 --> 00:36:11,860
about a one thousand nucleotides long length

512
00:36:13,290 --> 00:36:15,530
he essentially when would like to

513
00:36:15,540 --> 00:36:19,400
sequence the genome we break it up into small pieces and sequence of these small

514
00:36:20,430 --> 00:36:23,440
so and if the sequence enough

515
00:36:23,460 --> 00:36:26,430
then we can overlap these pieces

516
00:36:26,450 --> 00:36:29,140
and you can find out the full

517
00:36:29,140 --> 00:36:35,350
so problematic of course is when you repeat sequence so one part which is repeated

518
00:36:35,350 --> 00:36:38,740
several times then you cannot really we construct

519
00:36:38,760 --> 00:36:40,440
how the original

520
00:36:40,490 --> 00:36:42,740
you cannot sequence of the whole worldview

521
00:36:43,390 --> 00:36:48,320
and these these repeats along the thousands really problematic so then you have to do

522
00:36:48,320 --> 00:36:51,170
some specific sequencing

523
00:36:52,410 --> 00:36:58,360
so that essentially to spread one of the human genome project with a look at

524
00:36:58,730 --> 00:36:59,810
this small

525
00:36:59,870 --> 00:37:01,940
they look at smaller parts may be

526
00:37:01,960 --> 00:37:08,260
million base pairs or just KB and then sequenced only those parts only took random

527
00:37:08,260 --> 00:37:13,200
part from these from this hundred KB sequence those that overlap

528
00:37:13,250 --> 00:37:19,360
so that they could we can put forward cetera cetera just randomly sequence the lot

529
00:37:19,640 --> 00:37:22,560
and then tried to reassemble the whole image

530
00:37:23,980 --> 00:37:31,640
the cost for the human genome was like three billion dollars including the technology technology

531
00:37:31,650 --> 00:37:39,450
and this has gotten much much cheaper it's cheaper by the house and recently the

532
00:37:39,450 --> 00:37:44,070
you know what i think what have been sequenced by or what you think that

533
00:37:44,300 --> 00:37:45,810
what's left millions

534
00:37:45,820 --> 00:37:48,610
you are

535
00:37:49,690 --> 00:37:55,910
so population data it's not just one genome which is so i mean that every

536
00:37:56,930 --> 00:38:01,140
individual has its own you know and the you know might be different from from

537
00:38:01,140 --> 00:38:02,240
the next video

538
00:38:03,040 --> 00:38:07,810
what we really have to understand what are the differences between these different phenotypes whether

539
00:38:08,400 --> 00:38:13,320
and the genomic information and related to genetic information like

540
00:38:13,420 --> 00:38:16,050
how people look like

541
00:38:16,090 --> 00:38:20,700
OK so and you the new sequencing technology one need new technology in general

542
00:38:20,750 --> 00:38:25,660
allows us to determine the variation between populations and individuals

543
00:38:26,230 --> 00:38:29,750
first of all there is so i mean that the

544
00:38:30,640 --> 00:38:32,450
if you at the area

545
00:38:32,500 --> 00:38:36,380
and the next generation sequencing techniques which are much more

546
00:38:36,420 --> 00:38:38,860
what you get out there is

547
00:38:38,900 --> 00:38:43,240
essentially sequences like those usually referencing

548
00:38:43,240 --> 00:38:47,620
how well would be like if i classify this data pointing to all the possible

549
00:38:47,620 --> 00:38:49,110
classes that i have

550
00:38:49,110 --> 00:38:50,740
and then compare

551
00:38:50,860 --> 00:38:57,270
each class with the total given by some world so that the form this distribution

552
00:38:57,270 --> 00:39:01,780
has want you can work this out in a little bit more

553
00:39:06,110 --> 00:39:08,930
i'm gonna skip this part of the softmax here

554
00:39:08,960 --> 00:39:10,170
and i just when

555
00:39:10,190 --> 00:39:15,860
one minute that this galaxy in

556
00:39:16,110 --> 00:39:22,970
the naive bayes or bayesian classifier has a linear geometry if the covariance matrices are

557
00:39:22,970 --> 00:39:23,840
the same

558
00:39:23,850 --> 00:39:25,840
so the ratio

559
00:39:27,520 --> 00:39:32,770
the posterior probability for any two classes is a linear function

560
00:39:32,780 --> 00:39:36,350
of the inputs if the covariance matrices are equal

561
00:39:37,000 --> 00:39:39,170
and what that means is that

562
00:39:39,190 --> 00:39:41,170
all of this

563
00:39:41,190 --> 00:39:43,000
thing that we do really

564
00:39:43,030 --> 00:39:44,770
is giving us nothing more

565
00:39:44,790 --> 00:39:46,980
then the same picture we had

566
00:39:46,990 --> 00:39:49,250
at the beginning

567
00:39:50,040 --> 00:39:51,740
it's just a linear

568
00:39:51,880 --> 00:39:58,250
separation between the classes in other words you as reliable where you think it equally

569
00:39:58,250 --> 00:40:02,060
probable to be class zero class one that's

570
00:40:02,160 --> 00:40:06,110
surfaces is just the wind the covariance matrices equal

571
00:40:06,120 --> 00:40:10,480
if the covariance matrices are different if you have a separate gases for each one

572
00:40:10,560 --> 00:40:11,850
like this

573
00:40:13,370 --> 00:40:18,100
the contour equal probability is a conic section

574
00:40:19,300 --> 00:40:24,000
parabola or hyperbola or something like that

575
00:40:24,320 --> 00:40:26,970
which by the way is linear

576
00:40:26,980 --> 00:40:30,100
in the sufficient statistics based

577
00:40:30,110 --> 00:40:34,370
i a conic section is linear in this case is

578
00:40:34,940 --> 00:40:38,920
it's gone but many in space one

579
00:40:39,050 --> 00:40:41,490
x squared

580
00:40:45,050 --> 00:40:47,410
he festering in your mind

581
00:40:47,420 --> 00:40:50,250
so i want to talk

582
00:40:50,800 --> 00:40:56,470
so so that's just the link between the gas model and the and the linear

583
00:40:57,450 --> 00:41:00,540
and this is just a slide to tell you

584
00:41:00,740 --> 00:41:02,300
exponential family

585
00:41:02,300 --> 00:41:06,200
class conditional models are linear in the sufficient statistics

586
00:41:06,220 --> 00:41:09,760
so if you think of the sufficient statistic is an extended space

587
00:41:09,780 --> 00:41:15,550
which augments the original feature vector having the class conditional model which is any exponential

588
00:41:15,550 --> 00:41:16,740
family model

589
00:41:16,760 --> 00:41:21,000
it is linear in the sufficient statistics based that exponential family model

590
00:41:22,040 --> 00:41:25,580
you can think of these as just extended linear classifiers

591
00:41:26,000 --> 00:41:32,340
all of the class conditional models is exponential

592
00:41:32,350 --> 00:41:36,000
OK so the last thing i want to do here before you take a short

593
00:41:36,000 --> 00:41:39,960
break is is the discrete equivalent of

594
00:41:39,970 --> 00:41:41,130
this classifier

595
00:41:41,140 --> 00:41:44,270
which is the discrete bayesian classifier

596
00:41:44,290 --> 00:41:49,130
so if the inputs are categorical which can really fit together into the

597
00:41:49,150 --> 00:41:53,290
what should we do well the simplest thing to do is to have a joint

598
00:41:54,370 --> 00:41:56,260
which is the table so this is is

599
00:41:56,270 --> 00:42:01,540
the conditional probability table conditioned on the class

600
00:42:01,540 --> 00:42:06,080
and of course is conceptually correct but it has huge practical problem which is that

601
00:42:06,080 --> 00:42:08,520
if you say a hundred binary inputs

602
00:42:08,630 --> 00:42:14,050
then in two classes each class conditional model as like two to one hundred twenty

603
00:42:14,800 --> 00:42:18,070
and there's no way to be able to to estimate that for any reasonable

604
00:42:20,290 --> 00:42:25,120
so for example if you're sixteen by sixteen digits in two hundred fifty six grey

605
00:42:25,120 --> 00:42:29,300
levels for each one each class conditional model as

606
00:42:29,330 --> 00:42:31,320
sixteen time sixteen

607
00:42:31,330 --> 00:42:33,140
that's the number

608
00:42:33,300 --> 00:42:39,640
variables two hundred fifty six numbers setting two hundred fifty six to sixteen sixteen

609
00:42:40,340 --> 00:42:44,770
and it has caused most of the entries in these tables will be zero

610
00:42:46,320 --> 00:42:49,960
your probability of being in a particular class of zero zero and you want know

611
00:42:52,170 --> 00:42:54,210
why are we going to do here

612
00:42:54,240 --> 00:42:58,180
we're going to to regularisation just like i told you about the gases

613
00:42:58,190 --> 00:43:03,420
we're going to use one or more of our three regularisation tracks which was factorizations

614
00:43:03,430 --> 00:43:06,040
parameter tying and priors

615
00:43:06,050 --> 00:43:12,000
OK and each of these tracks again leads to very common discrete classifiers

616
00:43:12,020 --> 00:43:16,840
so let's first talk about the factorisation true

617
00:43:16,870 --> 00:43:19,820
factorizations trace yields

618
00:43:19,830 --> 00:43:22,190
classifier called naive bayes

619
00:43:22,200 --> 00:43:28,710
eighty phase but despite the derogatory name i pretty predicted by our very seriously

620
00:43:28,950 --> 00:43:32,930
it seems like a ridiculous idea but it works extremely well

621
00:43:33,770 --> 00:43:38,670
if you're doing a paper in which you are studying classification

622
00:43:38,680 --> 00:43:42,570
you are morally obligated to compare your method two

623
00:43:42,600 --> 00:43:45,160
naive bayes and nearest neighbour

624
00:43:45,170 --> 00:43:51,590
and verify for yourself that these seemingly foolish classifiers did not actually be here

625
00:43:51,600 --> 00:43:57,320
could be surprised how often naive bayes nearest neighbour will actually be compared

626
00:43:57,330 --> 00:44:02,270
but nonetheless there are many much much better than naive bayes

627
00:44:02,390 --> 00:44:05,580
like like kernel classifiers are gaussian process

628
00:44:05,810 --> 00:44:10,010
priors but let me tell you what naive bayes and the naive bayes assumption is

629
00:44:12,170 --> 00:44:14,970
the attributes are the

630
00:44:14,990 --> 00:44:17,800
OK this is clearly ridiculous assumption

631
00:44:17,820 --> 00:44:22,770
let's take for example the naive bayes classifier for

632
00:44:24,560 --> 00:44:29,410
so what the naive bayes classifiers say oh well let's say that the features are

633
00:44:29,410 --> 00:44:31,510
just the binary presence of certain

634
00:44:31,640 --> 00:44:34,300
he goes nigeria your email

635
00:44:34,340 --> 00:44:35,700
does by

636
00:44:35,710 --> 00:44:39,420
here in you know the stock appeared you know there's more to it appearing even those

637
00:44:39,530 --> 00:44:42,020
by actor appearing in

638
00:44:42,040 --> 00:44:44,300
about in other words publishing like

639
00:44:45,580 --> 00:44:46,420
and this

640
00:44:46,420 --> 00:44:51,870
the naive bayes assumption is that if you know instead the probability of each words

641
00:44:51,870 --> 00:44:53,990
occurring independent

642
00:44:54,040 --> 00:44:58,710
but it clearly true that if there were more in e-mail it's much more likely

643
00:44:58,720 --> 00:45:01,370
to work approved is going to hurt you

644
00:45:02,080 --> 00:45:05,920
or if were the average person was in other words that are more likely to

645
00:45:05,920 --> 00:45:11,040
occur so it's not really the case that things are independent given the class but

646
00:45:11,150 --> 00:45:15,130
that's the assumption we make the naive bayes and so the naive bayes

647
00:45:15,290 --> 00:45:16,260
the world

648
00:45:18,460 --> 00:45:24,570
in this graphical model you have the class year in conditioned on the class every

649
00:45:24,570 --> 00:45:26,380
one of the attributes

650
00:45:26,400 --> 00:45:28,520
are independent

651
00:45:28,530 --> 00:45:31,910
so it's just a big

652
00:45:32,040 --> 00:45:37,440
thing like this were all the attributes are independent in

653
00:45:39,040 --> 00:45:43,680
training in naive bayes classifiers really really easy you just sort all the data into

654
00:45:43,680 --> 00:45:47,500
classes all the data from glacier and one but all the data class one another

655
00:45:47,500 --> 00:45:52,800
bucket for each class you estimate the distribution of the i th variable

656
00:45:52,810 --> 00:45:55,570
completely independent

657
00:45:56,160 --> 00:46:01,650
first of all you would just take email spam in measure how often in email

658
00:46:01,650 --> 00:46:06,890
spam what fraction of the messages doesn't work nigeria occurs these three percent so the

659
00:46:06,910 --> 00:46:11,300
parameter is point zero three the probability of nigeria on

660
00:46:11,320 --> 00:46:15,300
given the email spam you take all the non spam email that's what fraction of

661
00:46:15,300 --> 00:46:17,390
those messages containing the word nigeria

662
00:46:18,100 --> 00:46:22,770
unless you have some legitimate existing relationship with nigeria probably is very low low

663
00:46:22,770 --> 00:46:28,760
should choose by a simulation

664
00:46:28,810 --> 00:46:30,610
the universe the

665
00:46:30,670 --> 00:46:32,980
five this ago

666
00:46:32,990 --> 00:46:39,150
or move from one thousand eight

667
00:46:40,090 --> 00:46:45,380
which is the fastest growing

668
00:46:45,720 --> 00:46:47,840
part of the

669
00:46:47,850 --> 00:46:52,320
to be because we

670
00:46:52,340 --> 00:46:56,040
be aware that

671
00:46:56,060 --> 00:46:58,130
if you there

672
00:46:59,770 --> 00:47:00,900
so far

673
00:47:02,220 --> 00:47:03,230
o is

674
00:47:03,690 --> 00:47:04,940
real world

675
00:47:06,880 --> 00:47:13,230
by the way is not there

676
00:47:17,700 --> 00:47:20,020
which try

677
00:47:31,550 --> 00:47:35,680
they want

678
00:47:35,690 --> 00:47:41,020
and you know by being in

679
00:47:41,040 --> 00:47:43,790
you should be

680
00:47:43,800 --> 00:47:44,950
it is

681
00:47:48,900 --> 00:47:50,420
the job

682
00:47:51,730 --> 00:47:54,610
so as you

683
00:47:54,620 --> 00:47:58,130
one is dangerous

684
00:47:59,940 --> 00:48:01,870
over here

685
00:48:01,920 --> 00:48:03,760
so i will

686
00:48:03,770 --> 00:48:07,370
that story is dangerous

687
00:48:07,570 --> 00:48:10,240
there that are lot more

688
00:48:11,700 --> 00:48:16,930
well all can be doing you

689
00:48:26,400 --> 00:48:27,500
official my

690
00:48:34,900 --> 00:48:37,390
and may be involved

691
00:48:37,410 --> 00:48:40,310
there's no

692
00:48:40,370 --> 00:48:43,040
they or

693
00:48:43,120 --> 00:48:45,160
no more

694
00:48:49,820 --> 00:48:52,140
you see four

695
00:48:52,150 --> 00:48:56,500
on the other hand

696
00:48:56,650 --> 00:48:59,830
march four or five

697
00:48:59,850 --> 00:49:02,180
that you

698
00:49:03,850 --> 00:49:05,480
say want to

699
00:49:06,010 --> 00:49:08,410
wait wait

700
00:49:10,190 --> 00:49:12,870
model is this

701
00:49:16,110 --> 00:49:16,810
so o

702
00:49:18,450 --> 00:49:20,780
i don't think

703
00:49:20,800 --> 00:49:25,470
i think they are wrong

704
00:49:25,570 --> 00:49:28,560
even a we are

705
00:49:28,570 --> 00:49:31,770
the year that

706
00:49:41,660 --> 00:49:44,300
did a lot

707
00:49:49,440 --> 00:49:52,680
the idea is that it

708
00:50:09,650 --> 00:50:10,730
i e

709
00:50:10,760 --> 00:50:13,250
early in the

710
00:50:13,300 --> 00:50:18,780
that season

711
00:50:19,080 --> 00:50:22,840
you need to this is the neuron

712
00:50:22,880 --> 00:50:28,430
is you is completion day components is the

713
00:50:28,440 --> 00:50:29,300
he c

714
00:50:31,660 --> 00:50:35,430
right or wrong it is who

715
00:50:36,270 --> 00:50:37,200
you can

716
00:50:39,360 --> 00:50:42,950
what is it if he were our well

717
00:50:42,960 --> 00:50:48,300
there is which

718
00:50:48,360 --> 00:50:50,040
it is

719
00:50:53,370 --> 00:50:57,320
there is a POV fork out

720
00:50:58,370 --> 00:50:59,990
for any

721
00:51:00,000 --> 00:51:03,170
the area

722
00:51:06,060 --> 00:51:09,880
we see is

723
00:51:09,900 --> 00:51:11,200
more and more

724
00:51:12,990 --> 00:51:16,250
born in that it is

725
00:51:16,330 --> 00:51:22,420
is the problem we use we used to use

726
00:51:22,560 --> 00:51:28,790
mechanical all the

727
00:51:29,160 --> 00:51:32,380
new story of

728
00:51:33,460 --> 00:51:36,760
damage or disease

729
00:51:36,880 --> 00:51:40,060
so this

730
00:51:43,020 --> 00:51:44,470
for more

731
00:51:46,190 --> 00:51:47,560
he is

732
00:51:47,580 --> 00:51:52,420
there is only one nation in

733
00:51:57,590 --> 00:52:00,110
if you like can

734
00:52:03,640 --> 00:52:06,020
so in the US

735
00:52:06,030 --> 00:52:10,370
aidan accuracy is in his

736
00:52:12,950 --> 00:52:15,190
you can use

737
00:52:15,210 --> 00:52:19,970
the class of disease of

738
00:52:26,290 --> 00:52:27,400
i mean that's

739
00:52:27,430 --> 00:52:29,140
that's why

740
00:52:29,150 --> 00:52:31,870
so this is a consequence

741
00:52:31,890 --> 00:52:37,430
did the following this is really might

742
00:52:37,440 --> 00:52:39,600
the primary goal

743
00:52:39,620 --> 00:52:41,320
is is

744
00:52:41,400 --> 00:52:45,550
i used

745
00:52:47,400 --> 00:52:49,060
so she

746
00:52:49,140 --> 00:52:53,080
the beginning of this is in

747
00:52:53,100 --> 00:52:55,680
i'm going we have for

748
00:52:55,690 --> 00:52:56,940
it's too

749
00:52:56,960 --> 00:52:59,090
which is on

750
00:52:59,150 --> 00:53:00,870
this is where

751
00:53:00,890 --> 00:53:02,470
you might try

752
00:53:03,880 --> 00:53:06,030
in the last year

753
00:53:06,050 --> 00:53:10,700
i know that there is about as well for as

754
00:53:10,720 --> 00:53:15,620
since it is

755
00:53:15,660 --> 00:53:21,680
you should not all and then in my own

756
00:53:21,700 --> 00:53:24,110
one is a

757
00:53:24,120 --> 00:53:26,690
which is running

758
00:53:26,710 --> 00:53:28,770
we use their

759
00:53:28,970 --> 00:53:32,450
the role is

760
00:53:32,730 --> 00:53:35,250
you use

761
00:53:37,250 --> 00:53:39,360
this is

762
00:53:39,460 --> 00:53:44,230
in such a setting so

763
00:53:44,250 --> 00:53:51,010
i was very shy it seems to me there is no mention of TV well

764
00:53:53,960 --> 00:53:59,930
sorry to all of the one that was going on this is

765
00:53:59,940 --> 00:54:01,430
you may

766
00:54:02,170 --> 00:54:06,510
the single was

767
00:54:06,640 --> 00:54:09,160
it means that

768
00:54:09,170 --> 00:54:12,460
there is no need to win

769
00:54:12,510 --> 00:54:17,680
i will buy you a bit about

770
00:54:20,960 --> 00:54:22,090
if you do

771
00:54:22,090 --> 00:54:26,220
right and if the observed reward was small

772
00:54:26,240 --> 00:54:28,340
you pretend as if

773
00:54:28,340 --> 00:54:30,420
the other action was

774
00:54:30,470 --> 00:54:32,420
the but their actions

775
00:54:32,440 --> 00:54:35,340
so you assume that the other action was

776
00:54:35,380 --> 00:54:38,820
good action to take so you tell your classifier

777
00:54:38,840 --> 00:54:41,700
to take that action instead

778
00:54:41,760 --> 00:54:45,970
the other actions and this is the importance weight

779
00:54:46,010 --> 00:54:50,400
so this is the word is less than one-half here

780
00:54:50,450 --> 00:54:53,110
and the importance weights similarly

781
00:54:53,130 --> 00:54:54,340
as you do here

782
00:54:54,340 --> 00:54:56,530
by one half minus or

783
00:54:56,550 --> 00:55:00,570
over the probability so the importance weighted approach

784
00:55:00,630 --> 00:55:06,420
was essentially this except that he used zero instead of one have

785
00:55:06,450 --> 00:55:12,050
so there was no offset and here you can offset by any amount

786
00:55:12,050 --> 00:55:18,470
between zero and one including zero which would give you the importance weighted approach

787
00:55:19,440 --> 00:55:27,880
one half which is sort of the minimax solution

788
00:55:27,900 --> 00:55:32,010
so this is what you do is to actions and let's see how and why

789
00:55:32,010 --> 00:55:38,450
it helps and you were you feel that you feel this transformed data set into

790
00:55:38,450 --> 00:55:41,220
your favourite importance weighted

791
00:55:41,220 --> 00:55:43,200
binary classifier

792
00:55:43,550 --> 00:55:51,200
you learn the classifier and then you should use it as your policy

793
00:55:51,360 --> 00:55:55,650
i'm circa here

794
00:56:06,240 --> 00:56:12,380
yes sure sure you can you if your actions are

795
00:56:12,470 --> 00:56:16,070
you have to actions

796
00:56:16,130 --> 00:56:21,420
but they can be either positive or negative here you're just looking at the single

797
00:56:22,610 --> 00:56:27,240
and this is just the variable that the notes you're action that can be either

798
00:56:27,240 --> 00:56:29,090
positive or negative

799
00:56:29,220 --> 00:56:33,630
and you just invented the action in the observed reward was small

800
00:56:33,630 --> 00:56:38,450
so if it was the positive you pretend that the true label is negative

801
00:56:38,510 --> 00:56:39,700
if you observe

802
00:56:39,740 --> 00:56:41,400
and acts

803
00:56:41,420 --> 00:56:44,130
with the positive labels

804
00:56:44,130 --> 00:56:46,260
but smaller reward

805
00:56:46,280 --> 00:56:56,090
you pretend as if the true label is negative

806
00:56:56,270 --> 00:57:06,070
union where it the there was a paper at KDD last year actually it's called

807
00:57:08,400 --> 00:57:11,630
so how can the world be found

808
00:57:11,880 --> 00:57:17,030
so this is a

809
00:57:17,070 --> 00:57:22,280
this is this comes from your exploration so from the environment

810
00:57:22,320 --> 00:57:26,550
right so you take an action and you assume that you get some really for

811
00:57:26,550 --> 00:57:28,400
that action

812
00:57:28,450 --> 00:57:30,920
so it depends on your application

813
00:57:32,180 --> 00:57:37,550
and in the advertisment for example it can be either click and not as simple

814
00:57:37,550 --> 00:57:42,700
as zero one

815
00:57:42,700 --> 00:57:44,670
so we would not

816
00:57:44,680 --> 00:57:50,920
really address this problem here

817
00:57:50,920 --> 00:57:55,900
will you be talking about some experiment we can probe are

818
00:57:56,720 --> 00:58:02,590
and then the draw an action from our action choosing distributions

819
00:58:02,630 --> 00:58:07,300
and and this probability one over p where p is the probability of choosing that

820
00:58:07,300 --> 00:58:10,570
action and this is the

821
00:58:10,570 --> 00:58:14,800
absolute distance from the offset from one half

822
00:58:14,880 --> 00:58:19,530
so you were larger than one have been generated

823
00:58:19,550 --> 00:58:21,760
ax comedy so this is this

824
00:58:21,780 --> 00:58:26,840
rejection sampling and that john talked about

825
00:58:26,880 --> 00:58:30,510
right so probably well so you this

826
00:58:30,570 --> 00:58:36,300
larger than one half which generated we using a is our actions otherwise we invert

827
00:58:36,300 --> 00:58:37,550
the action

828
00:58:37,570 --> 00:58:44,800
we use the other action is the true label

829
00:58:44,860 --> 00:58:48,450
so the problem induced by

830
00:58:48,510 --> 00:58:50,400
importance weighted

831
00:58:50,450 --> 00:58:52,680
this importance weighted conversion

832
00:58:52,780 --> 00:58:56,760
is noisy and this week the offset and the

833
00:58:56,780 --> 00:58:59,240
can be used to decrease

834
00:58:59,260 --> 00:59:05,110
the noise in the induced problem and i'll go through several examples to see why

835
00:59:05,610 --> 00:59:08,450
it does so

836
00:59:08,470 --> 00:59:10,030
what i missed

837
00:59:10,050 --> 00:59:15,590
so should they go through the examples known OK

838
00:59:15,720 --> 00:59:20,470
OK so let's say that we have an example axe

839
00:59:20,470 --> 00:59:23,110
and they have two actions and these are

840
00:59:23,130 --> 00:59:28,130
the words of these sections so the weight of the first actions one half and

841
00:59:28,130 --> 00:59:32,720
the reward of the second action is one

842
00:59:32,740 --> 00:59:34,150
and let's say that

843
00:59:34,150 --> 00:59:36,420
there's some distribution

844
00:59:36,650 --> 00:59:42,300
and so because this action left and call this action right

845
00:59:44,800 --> 00:59:50,510
so what's the probability of and there are some distribution over the actions that the

846
00:59:50,510 --> 00:59:54,900
user so what's the probability of generating

847
00:59:54,940 --> 00:59:57,260
this binary example

848
00:59:57,320 --> 01:00:01,840
versus this binary examples of what's the induced distribution

849
01:00:01,880 --> 01:00:05,110
for the binary classification problems

850
01:00:05,110 --> 01:00:08,150
i'm going talk about multi label problems

851
01:00:14,880 --> 01:00:19,150
so it's just to slide about

852
01:00:21,470 --> 01:00:24,970
previous work the

853
01:00:25,080 --> 01:00:26,740
OK it turns out that

854
01:00:27,280 --> 01:00:29,850
some of the techniques

855
01:00:29,930 --> 01:00:32,080
it use in

856
01:00:35,400 --> 01:00:39,030
works quite well with multi label

857
01:00:39,050 --> 01:00:40,810
in fact

858
01:00:40,850 --> 01:00:42,630
what you do

859
01:00:43,280 --> 01:00:44,850
the graph

860
01:00:44,910 --> 01:00:45,780
instead of

861
01:00:45,800 --> 01:00:49,060
using a single variable for each

862
01:00:49,060 --> 01:00:51,460
a single node in the graph for each variable

863
01:00:51,480 --> 01:00:54,030
this this the graph is going to take multiple labels

864
01:00:54,050 --> 01:00:57,050
you have to have some sort of encoding

865
01:00:57,060 --> 01:00:58,730
of the variables

866
01:00:58,780 --> 01:01:00,350
different values

867
01:01:00,410 --> 01:01:02,240
in the graph so

868
01:01:02,250 --> 01:01:07,420
the simplest way is just to have graph from different levels this would be

869
01:01:09,270 --> 01:01:13,390
if you have label five five possible values for each label

870
01:01:13,410 --> 01:01:15,720
right so you stack them up

871
01:01:15,720 --> 01:01:17,660
disparity label

872
01:01:17,670 --> 01:01:21,660
and x is just spatial

873
01:01:21,720 --> 01:01:23,380
mentioned and you have

874
01:01:24,550 --> 01:01:26,030
here is one

875
01:01:27,110 --> 01:01:30,170
partition of the graph like this

876
01:01:30,190 --> 01:01:32,550
corresponds to

877
01:01:32,860 --> 01:01:35,360
once again

878
01:01:35,560 --> 01:01:38,520
what the value of the

879
01:01:38,520 --> 01:01:42,450
of one of the variables should be so in particular

880
01:01:42,500 --> 01:01:44,990
if you

881
01:01:45,220 --> 01:01:51,250
represent a single variable

882
01:01:51,300 --> 01:01:52,470
like this

883
01:01:52,470 --> 01:01:54,050
having zero

884
01:01:54,310 --> 01:01:56,580
i want

885
01:01:56,580 --> 01:02:00,110
the variable x that i'm talking about that i have failed to show called x

886
01:02:01,100 --> 01:02:03,390
x one x two

887
01:02:05,330 --> 01:02:07,420
and if my

888
01:02:09,440 --> 01:02:10,890
of the graph

889
01:02:10,950 --> 01:02:13,890
happens to

890
01:02:13,950 --> 01:02:17,550
divided like this

891
01:02:17,640 --> 01:02:21,550
and this means that i'm going to give

892
01:02:22,140 --> 01:02:24,300
the label medical they should call is

893
01:02:24,320 --> 01:02:27,480
label zero label one label two then

894
01:02:28,490 --> 01:02:30,760
the variable x

895
01:02:30,770 --> 01:02:32,890
it's the value held

896
01:02:36,230 --> 01:02:38,710
the labels are generally held equals

897
01:02:38,740 --> 01:02:40,610
zero up to

898
01:02:41,270 --> 01:02:42,820
so each

899
01:02:42,830 --> 01:02:44,240
these labels

900
01:02:44,290 --> 01:02:46,140
corresponds to in

901
01:02:47,930 --> 01:02:50,170
this gets cut

902
01:02:50,180 --> 01:02:52,430
because you want that

903
01:02:52,450 --> 01:02:53,320
the label

904
01:02:53,330 --> 01:02:57,520
now this trick here in that you don't want

905
01:02:57,540 --> 01:03:00,240
the cut come back again like that

906
01:03:00,960 --> 01:03:03,150
so that's easily fixed

907
01:03:03,260 --> 01:03:07,420
you want to make this so that each column corresponds get cut in any one

908
01:03:09,210 --> 01:03:12,520
and so what you do is you put backward edges

909
01:03:13,870 --> 01:03:17,330
these images have infinite weight

910
01:03:17,370 --> 01:03:19,550
right so

911
01:03:19,620 --> 01:03:23,050
if you've got this zero down here

912
01:03:23,890 --> 01:03:25,540
this zero

913
01:03:25,550 --> 01:03:30,230
and this is one in your your bias on the basis of binary probably cut

914
01:03:30,230 --> 01:03:31,770
problem still

915
01:03:31,800 --> 01:03:32,990
these will be

916
01:03:32,990 --> 01:03:37,580
binary cut problem but each column represents in this way

917
01:03:37,820 --> 01:03:40,270
more than one label

918
01:03:40,320 --> 01:03:43,420
now if you've got

919
01:03:43,430 --> 01:03:46,170
cut like that this is why

920
01:03:46,800 --> 01:03:48,620
it's got to

921
01:03:49,010 --> 01:03:49,730
it's got to do

922
01:03:50,020 --> 01:03:52,330
got across three times released

923
01:03:52,370 --> 01:03:56,120
could do something like this

924
01:03:56,170 --> 01:03:58,260
because this this has to be in

925
01:04:00,770 --> 01:04:02,320
and this one has to be

926
01:04:02,450 --> 01:04:03,670
the zero

927
01:04:03,710 --> 01:04:09,070
so this would be the all these would be zero these would be one

928
01:04:09,070 --> 01:04:12,920
to not put the same distribution over words in every topic and i'm also notice

929
01:04:12,930 --> 01:04:17,970
we have not even begun to discuss computation

930
01:04:18,000 --> 01:04:20,640
there is no algorithm at play

931
01:04:20,670 --> 01:04:22,950
this is just thinking about the

932
01:04:22,970 --> 01:04:24,930
posterior distribution in this model

933
01:04:25,000 --> 01:04:30,070
OK so one intuition is yes sparse priors help

934
01:04:30,120 --> 01:04:33,240
encourage this but this can happens anyway

935
01:04:33,250 --> 01:04:36,640
and you see this with the probabilistic latent semantic indexing model

936
01:04:36,660 --> 01:04:37,930
this happens anyway

937
01:04:37,940 --> 01:04:39,800
because it's

938
01:04:39,800 --> 01:04:43,940
to the benefit of the posterior to divide up the words

939
01:04:44,040 --> 01:04:49,270
with the exceptions of course like a word like bank has high probability and two

940
01:04:49,280 --> 01:04:52,670
in two topical will will be captured by this

941
01:04:52,690 --> 01:04:55,420
but it's in the posteriors interest to do that

942
01:04:55,440 --> 01:04:58,170
OK so that's point number one

943
01:04:58,220 --> 01:05:02,890
and i realize these are very organised points so i'm i'm still trying to think

944
01:05:02,890 --> 01:05:03,640
about this

945
01:05:04,770 --> 01:05:05,750
o point number two

946
01:05:07,000 --> 01:05:08,840
in a mixture model

947
01:05:08,850 --> 01:05:12,940
this is enough to find clusters of co occurring words now beginning to co occurrence

948
01:05:14,100 --> 01:05:21,360
what's the mixture model somebody tell me into two sentences one mixture model is

949
01:05:21,370 --> 01:05:22,850
or in one sentence that's fine

950
01:05:27,290 --> 01:05:32,330
that's right sum of different distribution so and in in the context of of modelling

951
01:05:32,330 --> 01:05:34,320
text the way we're doing here

952
01:05:36,860 --> 01:05:39,770
i now understand why don't we get through my slides because

953
01:05:39,780 --> 01:05:44,450
i prefer to ramble like this in the context of what we're doing here we've

954
01:05:45,190 --> 01:05:50,550
let's call them topic proportions pi and zeta psi

955
01:05:53,770 --> 01:05:55,670
and got in words

956
01:05:55,700 --> 01:05:58,820
and got the document

957
01:05:58,850 --> 01:06:00,590
and they all come from

958
01:06:00,600 --> 01:06:03,870
one of these

959
01:06:03,910 --> 01:06:09,170
OK it's this it's it's a very similar model except instead of each document getting

960
01:06:09,170 --> 01:06:14,050
a distribution over terms each document only gets one topic assigned to it OK so

961
01:06:14,050 --> 01:06:18,190
on the document i'm from topic thirty seven all

962
01:06:18,920 --> 01:06:23,320
trillion of my words come from topic thirty seven

963
01:06:24,250 --> 01:06:25,200
that's a mixture

964
01:06:25,210 --> 01:06:26,390
it's like

965
01:06:26,410 --> 01:06:29,340
every document belongs to this one topic

966
01:06:30,000 --> 01:06:33,590
in a mixture model

967
01:06:34,400 --> 01:06:36,710
topics are going to

968
01:06:36,760 --> 01:06:40,330
by the way i don't expect you to understand that sentence because i don't understand

969
01:06:40,350 --> 01:06:41,600
it seems

970
01:06:41,610 --> 01:06:46,490
like you need some editing in a mixture model with this refers to in in

971
01:06:46,490 --> 01:06:49,170
a mixture model

972
01:06:49,190 --> 01:06:50,820
i think it may refer to model

973
01:06:56,080 --> 01:06:58,590
that's a mixture model yes

974
01:06:58,600 --> 01:07:07,550
this is a mixed membership model

975
01:07:07,560 --> 01:07:17,410
and this is the next year

976
01:07:22,120 --> 01:07:26,460
that's right they're both kind of mixture models so this is like an infinite well

977
01:07:26,670 --> 01:07:30,710
information i mean something else now this is like a mixture model where mixing over

978
01:07:30,710 --> 01:07:34,750
the over theta and this is the mixture model were mixing over the finite number

979
01:07:34,750 --> 01:07:37,660
of topics k

980
01:07:42,370 --> 01:07:47,710
no because there are k topics different documents can exhibit different topics so i'm the

981
01:07:48,670 --> 01:07:52,110
yes we considering in the whole collection that's deeply

982
01:07:52,120 --> 01:07:54,430
everybody on board with mixture modeling

983
01:07:56,020 --> 01:08:00,620
so in a mixture model when you contemplate the posterior you're going to get co

984
01:08:00,620 --> 01:08:07,250
occurring words this gets to the y co occurrence playsrole right because

985
01:08:07,270 --> 01:08:13,870
because the if if if i come from one topic then the words in my

986
01:08:13,870 --> 01:08:18,410
document all should have high probability in that topic and what words is going to

987
01:08:18,410 --> 01:08:22,060
want to put in that topic with high probability the words that are all happy

988
01:08:22,060 --> 01:08:23,350
together in in

989
01:08:23,360 --> 01:08:24,750
in my document

990
01:08:24,760 --> 01:08:28,030
and so those are words that co occur in the document

991
01:08:28,050 --> 01:08:29,490
it's a mixture model

992
01:08:30,280 --> 01:08:33,060
capture precisely the sets of

993
01:08:33,070 --> 01:08:35,180
recurring co occurring words

994
01:08:35,200 --> 01:08:39,400
so that clear

995
01:08:39,420 --> 01:08:44,840
i'll say it again so if a mixture model assigns each document just one topic

996
01:08:46,460 --> 01:08:51,540
what that means necessarily is that all of the words in each document came from

997
01:08:51,540 --> 01:08:56,260
just one topic that means that it's in the

998
01:08:56,260 --> 01:09:00,590
bayesian machinery interest to

999
01:09:00,640 --> 01:09:05,410
place co occurring words together in one topic so they all have high probability if

1000
01:09:05,420 --> 01:09:09,180
two words that co occur lot were in different topics then we're totally

1001
01:09:09,200 --> 01:09:13,890
it's screwed because we are only allowed to be in one topic and so the

1002
01:09:13,890 --> 01:09:17,880
words that are in my document that co occur so that those two words co

1003
01:09:17,880 --> 01:09:21,420
occur document that probably zero and one of the topics

1004
01:09:21,430 --> 01:09:23,360
so when i know i'm being confusing

1005
01:09:26,660 --> 01:09:29,500
this is the rules of the model

1006
01:09:29,510 --> 01:09:37,060
in the midst of all of this to get you get something that kind of

1007
01:09:37,060 --> 01:09:40,430
look like topics yes so let me tell you when we get to the next

1008
01:09:40,430 --> 01:09:42,640
point thank you for

1009
01:09:42,660 --> 01:09:45,750
moving me to the next point OK

1010
01:09:45,750 --> 01:09:47,660
mixture model co occurrence

1011
01:09:47,720 --> 01:09:52,920
hopefully that same which has been made in your brain now

1012
01:09:52,930 --> 01:09:55,560
in LDA

1013
01:09:55,570 --> 01:10:01,750
so that's the second point in LDA directly on the topic proportions can encourage sparsity

1014
01:10:01,870 --> 01:10:07,600
in other words a document will be penalized for using too many topics

1015
01:10:07,610 --> 01:10:10,320
OK what i mean

1016
01:10:10,350 --> 01:10:12,610
if i go backwards got

1017
01:10:12,860 --> 01:10:15,630
i mean this

1018
01:10:15,650 --> 01:10:20,320
if i put a certain per hyperparameter on my dearest way

1019
01:10:20,360 --> 01:10:21,540
alpha equals

1020
01:10:21,550 --> 01:10:26,940
o point o one ten topics then the prior over the topic proportions seems to

1021
01:10:28,240 --> 01:10:34,640
topic proportions theta sparse the only place high probability with which are on

1022
01:10:36,050 --> 01:10:38,370
distribution that have

1023
01:10:38,390 --> 01:10:41,740
a small number of atoms with high probability

1024
01:10:41,740 --> 01:10:46,450
OK the distribution over distributions places high probability over these distributions that have a small

1025
01:10:46,450 --> 01:10:50,360
number of atoms that have high probability

1026
01:10:50,380 --> 01:10:52,020
clear j

1027
01:10:58,580 --> 01:11:00,870
here is alpha so small

1028
01:11:00,890 --> 01:11:05,250
that we we are essentially asking for a mixture model

1029
01:11:05,260 --> 01:11:07,640
if alpha gets pushed all the way down to zero

1030
01:11:07,660 --> 01:11:11,770
then we were we've got a mixture model on our hands more or less

1031
01:11:11,770 --> 01:11:15,710
and the posterior because this is saying every day i all i want is for

1032
01:11:15,710 --> 01:11:20,020
every document to be in one topic and notice that which topic it doesn't matter

1033
01:11:20,020 --> 01:11:23,940
so right

1034
01:11:23,960 --> 01:11:28,170
six minutes later just for the card

1035
01:11:28,340 --> 01:11:31,480
one the six minutes at the end

1036
01:11:31,990 --> 01:11:38,130
so i'm going to use the six minutes before talking about planning generative model so

1037
01:11:38,130 --> 01:11:38,860
we are

1038
01:11:38,880 --> 01:11:44,230
slide relaxing the condition of the the previous target the condition that was that

1039
01:11:44,250 --> 01:11:48,500
you should be able to create a transition probabilities

1040
01:11:48,520 --> 01:11:51,200
the densities and the rewards

1041
01:11:51,230 --> 01:11:54,900
and so what if if you can do that you can always sample and so

1042
01:11:54,900 --> 01:11:57,750
this is like an entry

1043
01:11:57,760 --> 01:12:01,290
two are the more interesting cares for and

1044
01:12:01,300 --> 01:12:06,430
you can only interact with the system and this is the way you get yourself

1045
01:12:06,470 --> 01:12:10,890
so that way we make and was saying so that the more that we use

1046
01:12:12,900 --> 01:12:14,620
you can

1047
01:12:14,640 --> 01:12:16,540
you can

1048
01:12:16,550 --> 01:12:19,470
ask black box to return

1049
01:12:19,510 --> 01:12:24,520
around the next state from any given state for any given action

1050
01:12:24,570 --> 01:12:28,970
so there's more that you are going to use this is called the generative model

1051
01:12:30,550 --> 01:12:36,190
the purposes of again finding finding a good policy and the efficient

1052
01:12:37,850 --> 01:12:42,210
well the question is how to do that and so the obvious idea against two

1053
01:12:42,210 --> 01:12:44,520
to evaluation

1054
01:12:46,160 --> 01:12:51,640
maybe at this stage you you will are these start to understand why

1055
01:12:51,660 --> 01:12:55,840
we have to talk that much about dynamic programming and value creation all this stuff

1056
01:12:56,740 --> 01:12:59,960
many of these are given such is based on

1057
01:13:00,000 --> 01:13:01,630
and those ideas

1058
01:13:02,460 --> 01:13:04,790
here the idea is as follows

1059
01:13:04,800 --> 01:13:09,080
so this is very similar to that on the discretisation mentors

1060
01:13:09,100 --> 01:13:13,940
the only thing that changes that we can not compute approximate MDP so we have

1061
01:13:13,960 --> 01:13:15,710
to do approximate conquered

1062
01:13:15,720 --> 01:13:19,490
we have to use more samples he said i

1063
01:13:20,630 --> 01:13:25,140
just coming up is that approximate the so what you do is that

1064
01:13:25,160 --> 01:13:31,710
in a preprocessing step you just generating a bunch of sound policy generating base and

1065
01:13:31,770 --> 01:13:34,830
so that those the samples that are similar to

1066
01:13:34,860 --> 01:13:38,210
what we used in the on the discretisation method

1067
01:13:38,260 --> 01:13:40,270
and here will an arbitrary

1068
01:13:40,270 --> 01:13:45,520
distribution you see be just pick your favourite distribution you

1069
01:13:45,990 --> 01:13:49,110
and so there is some samples from the distribution

1070
01:13:49,150 --> 01:13:51,640
and then for each of these plants

1071
01:13:51,650 --> 01:13:56,180
you generate a bunch of next state so this is again monte carlo remembered by

1072
01:13:56,210 --> 01:13:59,360
the addition you want to compute an integral

1073
01:13:59,370 --> 01:14:03,700
are you want to compute anything for every action and so in order to be

1074
01:14:03,700 --> 01:14:05,900
able to compute intended

1075
01:14:05,900 --> 01:14:09,760
in approximate marriage is generated the next state

1076
01:14:09,770 --> 01:14:14,580
and generate the next it's but but from the transition from the current so this

1077
01:14:14,580 --> 01:14:17,650
is the assumption so we assume that you can do it so there is this

1078
01:14:17,650 --> 01:14:19,580
black box that returning

1079
01:14:19,620 --> 01:14:21,870
the samples you ask for is

1080
01:14:21,890 --> 01:14:25,430
next is given the current state and actions

1081
01:14:25,450 --> 01:14:29,120
and this black was just returns at on the next day

1082
01:14:29,180 --> 01:14:32,240
according to the transition probability kernel

1083
01:14:33,400 --> 01:14:38,080
the black box is also able to return immediately turns

1084
01:14:38,140 --> 01:14:39,960
immediate rewards

1085
01:14:40,640 --> 01:14:43,730
so there is this other guys

1086
01:14:43,740 --> 01:14:47,460
and then you just do value iteration so hard the iteration

1087
01:14:47,510 --> 01:14:49,050
so well

1088
01:14:49,060 --> 01:14:53,420
at the point at this specific point x of i

1089
01:14:53,460 --> 01:14:57,280
you can really constitute a good approximation to the

1090
01:14:57,300 --> 01:15:00,520
to target about to be

1091
01:15:00,550 --> 01:15:04,240
i updated values function we

1092
01:15:04,240 --> 01:15:06,280
what value iteration does

1093
01:15:06,300 --> 01:15:08,730
is that it's computing this into ground

1094
01:15:10,860 --> 01:15:15,800
what's what's the way of computing the center for the next four years

1095
01:15:15,830 --> 01:15:19,870
so the obvious way of computing this integral is to have a bunch of samples

1096
01:15:19,870 --> 01:15:24,480
generated from the transition probabilities

1097
01:15:24,490 --> 01:15:28,810
and use the same person among the countless the players in the

1098
01:15:28,870 --> 01:15:30,300
is an average

1099
01:15:30,300 --> 01:15:33,710
so this is what we do here

1100
01:15:33,710 --> 01:15:36,150
so we compute these values

1101
01:15:36,170 --> 01:15:39,590
and well of course the we take the max because in value iteration you take

1102
01:15:39,590 --> 01:15:40,900
the max

1103
01:15:40,960 --> 01:15:42,400
over the actions

1104
01:15:42,420 --> 01:15:45,080
and once you compute its value as well

1105
01:15:45,080 --> 01:15:48,880
what happens is that you have you have values

1106
01:15:48,920 --> 01:15:50,320
all our

1107
01:15:50,370 --> 01:15:51,830
a bunch of

1108
01:15:55,140 --> 01:15:57,900
so here is x one

1109
01:15:57,970 --> 01:16:01,850
is actually here is actually maybe you have three states only

1110
01:16:01,870 --> 01:16:05,320
very efficient in terms of computational

1111
01:16:06,420 --> 01:16:07,790
this would be the

1112
01:16:07,820 --> 01:16:12,250
maybe the true value is here the true value is here and the relies here

1113
01:16:12,250 --> 01:16:17,190
and then you compute ages this by this very so what do do

1114
01:16:17,200 --> 01:16:21,150
you set up a regression problem right so you have this

1115
01:16:21,230 --> 01:16:26,370
independent variables and you have this that and the very bottom up sorry

1116
01:16:26,410 --> 01:16:28,010
so these are the beat

1117
01:16:28,020 --> 01:16:30,000
the samples of the target

1118
01:16:30,010 --> 01:16:33,210
so this is just a regression problem so you want to come up

1119
01:16:33,260 --> 01:16:34,670
the function

1120
01:16:34,700 --> 01:16:36,870
that some whole

1121
01:16:37,080 --> 01:16:42,270
makes the behaviour of the optimal value function so you set up this regression problem

1122
01:16:42,280 --> 01:16:44,710
you have access to fly

1123
01:16:44,720 --> 01:16:46,440
and you have the targets

1124
01:16:47,290 --> 01:16:49,210
had offered i

1125
01:16:49,220 --> 01:16:51,650
this is a regression problem you saw

1126
01:16:51,670 --> 01:16:54,890
maybe you set up his first aggressions

1127
01:16:54,940 --> 01:16:57,600
so you pick a function space f

1128
01:16:57,650 --> 01:16:58,950
and so you

1129
01:16:58,960 --> 01:17:01,970
you are searching for the function

1130
01:17:02,000 --> 01:17:04,640
that minimizes this list are

1131
01:17:04,690 --> 01:17:06,760
this means that

1132
01:17:09,290 --> 01:17:14,080
what this function space could be so this could be space of network so

1133
01:17:14,080 --> 01:17:16,080
basically what you do here

1134
01:17:16,090 --> 01:17:18,030
most of the cases a year

1135
01:17:18,040 --> 01:17:20,340
optimizing some veins

1136
01:17:20,350 --> 01:17:24,450
or you could use support vector regression to solve this regression

1137
01:17:25,270 --> 01:17:28,660
and it will not be really discourse problem it would be

1138
01:17:28,670 --> 01:17:32,900
slightly different but it's all the same so that the procedure

1139
01:17:32,950 --> 01:17:33,790
and then

1140
01:17:33,810 --> 01:17:37,790
you have these two choices again that you could generate the first

1141
01:17:37,810 --> 01:17:43,320
in each of the traditions of traditional generate sample at the very beginning and we

1142
01:17:43,320 --> 01:17:45,470
use those samples

1143
01:17:45,520 --> 01:17:50,250
and what happens is that you can prove the nice things about this

1144
01:17:50,340 --> 01:17:52,730
it turns out that

1145
01:17:52,760 --> 01:17:59,220
well the reason i'm showing you respond OK if i should use screen

1146
01:17:59,320 --> 01:18:01,660
o who

1147
01:18:01,670 --> 01:18:03,620
it was five

1148
01:18:03,650 --> 01:18:05,700
going to right

1149
01:18:05,710 --> 01:18:06,900
and so

1150
01:18:06,920 --> 01:18:09,230
after computed

1151
01:18:09,280 --> 01:18:15,810
after this computation for k kappa decade relations you compute the greedy policy with respect

1152
01:18:15,820 --> 01:18:16,520
to your

1153
01:18:16,530 --> 01:18:18,230
o system by function

1154
01:18:18,250 --> 01:18:20,140
and you want to get the ball

1155
01:18:20,200 --> 01:18:25,000
on the difference between the optimal value function the best back from that you can

1156
01:18:25,000 --> 01:18:26,130
hope for

1157
01:18:26,140 --> 01:18:28,890
and the actual path that you are going to get

1158
01:18:28,900 --> 01:18:33,020
and this high high-quality come upon you can improve on this plant

1159
01:18:33,080 --> 01:18:37,460
so the nice thing about this bond again is that well and covered and then

1160
01:18:37,460 --> 01:18:42,130
let's let's give a specific example and specific example is

1161
01:18:42,170 --> 01:18:47,840
and silver chloride to a solution already containing sodium chloride

1162
01:18:47,900 --> 01:18:53,840
at silver chloride to a solution of point one molar

1163
01:18:53,860 --> 01:18:55,210
in ACL

1164
01:18:55,230 --> 01:19:00,000
so i really have chloride present is interesting if if i said that

1165
01:19:00,050 --> 01:19:02,000
instead of adding silver chloride

1166
01:19:02,000 --> 01:19:07,460
to pure water and at silver chloride to water that already contains some silver chloride

1167
01:19:07,460 --> 01:19:11,630
you say well that's obvious to contain some silver chloride can put as much as

1168
01:19:11,630 --> 01:19:14,300
they otherwise could but now i'm going to say

1169
01:19:14,320 --> 01:19:19,360
is that silver chloride sodium chloride does the presence of sodium chloride have any impact

1170
01:19:19,360 --> 01:19:23,270
on the dissolution of silver chloride so again look at

1171
01:19:23,280 --> 01:19:30,540
arrhenius theory of electrolytic dissociation so silver chloride will dissociate and go into solution of

1172
01:19:31,540 --> 01:19:35,940
cation and chloride anion and sodium chloride is present as

1173
01:19:36,000 --> 01:19:37,750
sodium ion

1174
01:19:37,750 --> 01:19:40,270
and chloride

1175
01:19:40,280 --> 01:19:41,880
so we've got now

1176
01:19:41,880 --> 01:19:43,190
no here

1177
01:19:43,230 --> 01:19:46,570
multiple sources of chloride ions

1178
01:19:46,590 --> 01:19:48,800
multiple sources

1179
01:19:48,800 --> 01:19:51,110
of chloride ions

1180
01:19:51,210 --> 01:19:57,020
get some chloride ions from silver chloride some chloride ions from

1181
01:19:57,070 --> 01:20:01,650
sodium chloride is so the consequence of that is that i know i can no

1182
01:20:01,650 --> 01:20:05,550
longer assume that the concentration of silver

1183
01:20:05,550 --> 01:20:10,540
i am equals the concentration of chloride that's no longer the case

1184
01:20:10,550 --> 01:20:13,800
because i've got some chloride ion that came from sodium

1185
01:20:14,770 --> 01:20:16,170
that's the problem

1186
01:20:16,190 --> 01:20:18,730
that's the problem so what do i do

1187
01:20:18,800 --> 01:20:21,090
i'm going to try to find the this

1188
01:20:21,150 --> 01:20:26,170
saturation solubility of silver chloride under these new circumstances so i'm going to go back

1189
01:20:26,170 --> 01:20:29,230
to my solubility product ksp

1190
01:20:29,230 --> 01:20:36,480
KSP KSP is going to equal the product of the silver ion concentration

1191
01:20:39,150 --> 01:20:43,400
concentration of chloride ions and i can take a stab at what this is

1192
01:20:43,460 --> 01:20:47,460
i'm told that this is on the order of zero point one molar

1193
01:20:47,520 --> 01:20:51,230
and we saw that in the case of pure silver chloride it's ten to the

1194
01:20:51,230 --> 01:20:53,840
minus five more

1195
01:20:53,840 --> 01:20:59,840
ten minus five before the magnitude lower even when we have full dissociation here so

1196
01:20:59,840 --> 01:21:05,750
i'm going to assume that ten minus one more is so much greater than

1197
01:21:05,750 --> 01:21:08,040
ten to the minus five or

1198
01:21:08,040 --> 01:21:11,880
which is what we got in pure silver chloride that i'm going to neglect the

1199
01:21:11,880 --> 01:21:18,190
contribution of chloride ions from silver chloride and say that the concentration of chloride is

1200
01:21:18,190 --> 01:21:23,570
simply equal to

1201
01:21:23,590 --> 01:21:27,820
zero point one because several point one is so much greater

1202
01:21:28,920 --> 01:21:30,040
what is

1203
01:21:30,130 --> 01:21:32,590
left over from the previous

1204
01:21:32,650 --> 01:21:36,610
calculation and now i can still make the

1205
01:21:36,690 --> 01:21:44,270
connection between the dissolution of silver chloride salt and the concentration of silver ion that

1206
01:21:44,280 --> 01:21:46,860
still exists is one to one so

1207
01:21:46,920 --> 01:21:51,980
this will then represent the sea of silver chloride times

1208
01:21:52,020 --> 01:21:56,650
zero point one and that's equal to this one point eight times ten to the

1209
01:21:58,150 --> 01:22:00,210
what is the ten to the minus ten

1210
01:22:00,300 --> 01:22:03,400
so i can solve for the concentration of

1211
01:22:03,440 --> 01:22:06,440
silver chloride

1212
01:22:06,500 --> 01:22:08,320
and that gives me

1213
01:22:08,340 --> 01:22:10,590
one point eight

1214
01:22:10,590 --> 01:22:13,900
times ten to the minus nine molar

1215
01:22:13,920 --> 01:22:18,820
so the presence of the chloride from sodium has had a tremendous

1216
01:22:18,840 --> 01:22:24,520
repressive effect on the dissolution of silver chloride

1217
01:22:24,570 --> 01:22:25,860
this is called

1218
01:22:25,860 --> 01:22:28,070
the common ion effect

1219
01:22:28,070 --> 01:22:33,590
because i have a common iron from both solitude someone influences the solubility of the

1220
01:22:37,000 --> 01:22:41,550
fact and we can use the common ion effect to our advantage if i want

1221
01:22:41,550 --> 01:22:48,130
to process something in order to cause precipitation very very fine particles i can load

1222
01:22:48,150 --> 01:22:54,630
solution with one song you introduce the second solitude and trigger the precipitation and doing

1223
01:22:54,630 --> 01:22:55,750
so control

1224
01:22:55,770 --> 01:22:57,150
particle size

1225
01:22:57,170 --> 01:22:58,630
and the shape

1226
01:22:58,650 --> 01:23:03,520
and thereby prepare material for subsequent processing in fact

1227
01:23:03,550 --> 01:23:07,040
you can use this if you want to determine if you've got water has been

1228
01:23:08,360 --> 01:23:12,020
simple thing you can do if you go back to that table eight seven

1229
01:23:12,050 --> 01:23:14,170
we all know that silver nitrate

1230
01:23:14,170 --> 01:23:16,670
silver nitrate is very very high

1231
01:23:16,690 --> 01:23:18,880
the solubility in water

1232
01:23:18,900 --> 01:23:24,750
so i introduced silver nitrate into water and the question is is there any chlorine

1233
01:23:26,150 --> 01:23:29,090
and if there's any chlorine present we know

1234
01:23:29,110 --> 01:23:34,570
that the saturation solubility of silver chloride is ten to the minus five molar what

1235
01:23:34,570 --> 01:23:39,070
will happen is if you've got chlorinated water you and silver nitrate

1236
01:23:39,130 --> 01:23:39,750
you all

1237
01:23:39,770 --> 01:23:41,880
cf a shower

1238
01:23:41,880 --> 01:23:43,540
a very very fine

1239
01:23:45,770 --> 01:23:48,020
which is the silver chloride

1240
01:23:48,070 --> 01:23:50,480
so this is a very good test for

1241
01:23:50,480 --> 01:23:54,380
now that's on the back of your hand out there this is not really because

1242
01:23:54,380 --> 01:23:56,650
we're not going to we're not going to collect it

1243
01:23:56,650 --> 01:23:58,340
you throw it away

1244
01:23:59,380 --> 01:24:04,430
it's here to intended to do two things one to show you something about the

1245
01:24:04,430 --> 01:24:08,440
method that we will be using here and two

1246
01:24:08,470 --> 01:24:12,900
to show you something of the level of this discourse the the level of the

1247
01:24:14,490 --> 01:24:17,820
the questions that i'm asking on this

1248
01:24:17,830 --> 01:24:19,070
the query is

1249
01:24:19,080 --> 01:24:22,800
questions asked on the quiz are the types of things we would expect you to

1250
01:24:22,800 --> 01:24:26,090
know at the end of the course not them

1251
01:24:26,100 --> 01:24:29,180
but at the end of the course some of these are difficult

1252
01:24:30,920 --> 01:24:36,270
if you find yourself getting most of these answers correct then don't take this course

1253
01:24:36,320 --> 01:24:39,600
be wasting your time with your money here so don't do this if you you

1254
01:24:39,600 --> 01:24:45,390
find the end of got sixteen seventeen eighteen these are alright let's start with a

1255
01:24:45,390 --> 01:24:48,330
little bit of classical music here and this

1256
01:24:48,450 --> 01:24:51,230
engages questions one and two

1257
01:24:51,250 --> 01:24:55,780
who is the composer of the piece that you're about to hear and what is

1258
01:24:55,780 --> 01:24:58,940
its title what's it called

1259
01:24:58,990 --> 01:25:19,120
so let's that's where i'm going to ask them to take you the next piece

1260
01:25:19,120 --> 01:25:23,580
here we talk about that just from moment the composer anybody know the composer of

1261
01:25:23,580 --> 01:25:30,580
this reader OK some people do some people don't gentleman over here dark doctor

1262
01:25:30,650 --> 01:25:35,900
OK that is beethoven ludwig van beethoven and you know the name of the piece

1263
01:25:35,950 --> 01:25:39,640
OK symphony number five now if you're sitting next to the church of god this

1264
01:25:39,640 --> 01:25:41,000
kind of so much

1265
01:25:41,050 --> 01:25:44,880
i'm not going down the tube in this course don't be intimidated by by this

1266
01:25:44,880 --> 01:25:49,750
as a saying we're going to build everybody up here together so that was beethoven's

1267
01:25:49,750 --> 01:25:54,830
fifth symphony the beginning of a famous passage in the history of classical music let's

1268
01:25:54,830 --> 01:25:58,960
listen to another composition here who is the composer of this

1269
01:25:59,010 --> 01:26:03,110
and what in what composition is this piece

1270
01:26:18,370 --> 01:26:38,170
that's probably enough for that so who is the composer of that anybody know the

1271
01:26:40,170 --> 01:26:45,330
fewer people do in linear integrated of your hand

1272
01:26:45,370 --> 01:26:46,990
again they

1273
01:26:47,020 --> 01:26:52,740
what's the what's new in what composition does beethoven used this particular

1274
01:26:54,080 --> 01:26:59,540
OK in the ninth symphony again it's a long really worried now don't be we

1275
01:26:59,540 --> 01:27:00,980
don't be worried here

1276
01:27:01,000 --> 01:27:04,150
OK so nice now

1277
01:27:04,160 --> 01:27:05,220
i believe

1278
01:27:05,240 --> 01:27:06,680
that music

1279
01:27:07,840 --> 01:27:09,160
a magical

1280
01:27:09,180 --> 01:27:10,850
potions magical spell

1281
01:27:10,860 --> 01:27:17,070
music gets to do a particular kinds of things gets us to feel particular kinds

1282
01:27:17,070 --> 01:27:21,850
of words and i think these two pieces by the same composer get

1283
01:27:21,860 --> 01:27:24,520
cause us to feel very

1284
01:27:24,540 --> 01:27:31,770
very different because different moods different psychological state to come over as one of them

1285
01:27:31,790 --> 01:27:34,130
it goes this way

1286
01:27:34,210 --> 01:27:41,710
and the other

1287
01:27:41,720 --> 01:27:54,080
i do a little experiment have never done this before never done this before but

1288
01:27:54,090 --> 01:27:55,550
i like to do the following

1289
01:27:55,570 --> 01:27:58,050
and that is to ask you

1290
01:28:00,040 --> 01:28:05,240
think about what each piece causes you to fall into and i've put some adjectives

1291
01:28:05,240 --> 01:28:09,850
up on the board of the group them by orson welles because i'm going to

1292
01:28:09,850 --> 01:28:14,720
ask you to raise your right hand if you respond one way to appease on

1293
01:28:14,720 --> 01:28:17,740
your left hand if you respond to others so on they are group there we've

1294
01:28:17,740 --> 01:28:25,450
got positive happy secure under the l group we've got negative anxious and settled

1295
01:28:25,460 --> 01:28:29,500
so i've chosen pieces maybe was slightly different fields here let's see let's see what

1296
01:28:29,500 --> 01:28:30,980
we do with this now

1297
01:28:30,990 --> 01:28:33,450
please number one

1298
01:28:33,510 --> 01:28:45,480
how do you feel about that now here's piece two

1299
01:28:45,490 --> 01:28:48,980
so here's one and play it again

1300
01:28:49,090 --> 01:28:59,940
if you raise your right hand or left hand as your response to that

1301
01:29:01,460 --> 01:29:03,200
here's piece one

1302
01:29:04,470 --> 01:29:11,340
right and left

1303
01:29:11,350 --> 01:29:11,950
all right

1304
01:29:12,550 --> 01:29:16,720
those of you that are raising your hand some more recent here but that's okay

1305
01:29:16,730 --> 01:29:20,080
those of you you know almost unanimously say

1306
01:29:20,600 --> 01:29:25,020
that's the beat of the fifth symphony sound somewhat ominous two is somewhat faithful to

1307
01:29:25,020 --> 01:29:30,240
this and the beethoven ninth symphony conversely has a different sort of feel to it

1308
01:29:30,280 --> 01:29:34,720
indeed does anybody know the title of the beethoven ninth symphony with the setting of

1309
01:29:34,720 --> 01:29:37,520
a poem by friedrich schiller recall

1310
01:29:37,550 --> 01:29:39,010
altered your

1311
01:29:39,030 --> 01:29:45,720
o to join so how does beethoven go about making the older joy joyful

1312
01:29:45,730 --> 01:29:48,680
what is he doing here this is what we're going to be doing our course

1313
01:29:48,680 --> 01:29:52,740
we're gonna be zeroing in on this music can anybody tell me why to a

1314
01:29:52,740 --> 01:29:57,770
person in this room we all responded positively to the ninth symphony and somewhat more

1315
01:29:57,800 --> 01:30:00,930
anxiously to the system

1316
01:30:00,940 --> 01:30:03,690
you tell me one thing

1317
01:30:03,690 --> 01:30:06,840
you're going to do is to look at all the social features of the two

1318
01:30:06,840 --> 01:30:10,940
strings to two big structures and you're going to in some way count the number

1319
01:30:10,950 --> 01:30:12,880
of social action that they have in common

1320
01:30:13,030 --> 01:30:14,640
the challenge now

1321
01:30:14,660 --> 01:30:20,380
once you have these general principle is is just to to know into to devise

1322
01:30:20,380 --> 01:30:22,280
efficient ways to do that

1323
01:30:22,320 --> 01:30:28,030
and know devising efficient ways means that you have to

1324
01:30:30,130 --> 01:30:37,400
identify what kind of substructures can be inherited efficiently

1325
01:30:37,450 --> 01:30:38,850
and can be

1326
01:30:38,850 --> 01:30:47,400
our and what kind of substructures carry enough information on the global structure

1327
01:30:47,990 --> 01:30:51,920
actually how how to do that and there's another idea

1328
01:30:52,470 --> 01:30:59,590
dating to the late nineties which is the idea of fisher kernels were you to

1329
01:30:59,600 --> 01:31:06,480
there's a process in two steps where you first learn generative models of your structured

1330
01:31:06,480 --> 01:31:12,880
data so generative models is directly connected to what acidic talk talk to talk to

1331
01:31:12,880 --> 01:31:19,490
you about yesterday and graphical models for instance you can when i say fisher kernels

1332
01:31:19,500 --> 01:31:21,110
the first use of fisher kernels

1333
01:31:21,530 --> 01:31:25,670
by a clan or slow was with

1334
01:31:25,720 --> 01:31:27,460
he then markov models

1335
01:31:27,470 --> 01:31:35,150
the use hidden markov models to model of the density of self of sequences protein

1336
01:31:36,090 --> 01:31:38,850
and they use that to to to to

1337
01:31:38,940 --> 01:31:41,380
to do the fisher kernels i'm going back to two

1338
01:31:41,420 --> 01:31:44,530
this example later

1339
01:31:45,390 --> 01:31:51,180
so they're remark that i did before is that there exists a tremendous amount of

1340
01:31:51,230 --> 01:31:56,900
work on building kernels for sequences i intentionally focus on research such kernels because i

1341
01:31:56,900 --> 01:32:01,740
don't have time and i have to have the whole knowledge about that but i

1342
01:32:01,740 --> 01:32:03,370
think that this idea

1343
01:32:03,380 --> 01:32:09,730
especially specifically this idea of comparing substructures of the whole structures into two to derive

1344
01:32:10,170 --> 01:32:17,660
a similarity function based on all the constitutions is very powerful idea

1345
01:32:20,010 --> 01:32:24,290
let's consider the problem of of of

1346
01:32:24,290 --> 01:32:31,790
deriving a carol for for sequences here you have a sequence

1347
01:32:31,840 --> 01:32:33,590
of amino acids

1348
01:32:33,600 --> 01:32:39,140
let's say that's amino acids acids and why you're going to do i'm going to

1349
01:32:39,140 --> 01:32:45,960
describe what is called the spectrum kernel something that was proposed by palestinian coauthors in

1350
01:32:46,000 --> 01:32:47,880
two thousand two

1351
01:32:47,900 --> 01:32:53,410
and the the strategy is very simple simple given two different sequences it just count

1352
01:32:53,410 --> 01:33:00,380
the number of common k mers cameras just like UK graph shows you you're going

1353
01:33:00,380 --> 01:33:02,920
to to look at subsequences of of

1354
01:33:02,930 --> 01:33:04,240
of size k

1355
01:33:04,260 --> 01:33:07,500
and you're looking to count the number of subsequences of size k that you have

1356
01:33:07,500 --> 01:33:11,130
in common the two subsequently in the two sequences

1357
01:33:11,130 --> 01:33:13,550
and computer value from this counts

1358
01:33:13,560 --> 01:33:20,220
in the first four four proteins you have an alphabet of twenty symbols they're all

1359
01:33:20,220 --> 01:33:24,630
amino acids and what you're going to be going to do is in some way

1360
01:33:24,640 --> 01:33:28,190
if you do in a very naive way you're going to

1361
01:33:28,210 --> 01:33:35,410
two two two will go through all your your words sequences and you will collect

1362
01:33:35,410 --> 01:33:39,180
all the numbers to the number of times or if you have a b and

1363
01:33:39,180 --> 01:33:41,670
then a BB in the BBC

1364
01:33:41,680 --> 01:33:43,740
you do that

1365
01:33:43,750 --> 01:33:45,420
it is that you that

1366
01:33:45,440 --> 01:33:47,900
i mean that each

1367
01:33:47,900 --> 01:33:48,930
three mirrors

1368
01:33:48,950 --> 01:33:54,140
each k mer corresponds to one coordinating in this long vector is different different

1369
01:33:54,450 --> 01:33:58,640
and then you do that for you sequences that for second sequence and then you

1370
01:33:58,640 --> 01:34:03,060
have vector your vector representation of your data and then you can do

1371
01:34:03,070 --> 01:34:05,940
everything you you you you want

1372
01:34:05,970 --> 01:34:12,880
so there's there's something that it's important

1373
01:34:13,750 --> 01:34:17,920
if you look at and that i don't know if it's the case

1374
01:34:20,620 --> 01:34:22,390
so it's twenty to the three

1375
01:34:22,440 --> 01:34:27,600
the this is the size of the vectors is very very big vector it's actually

1376
01:34:28,900 --> 01:34:34,660
it's a very sparse they use there there are very few nonzero entries in this

1377
01:34:34,660 --> 01:34:42,240
victory so explicitly constructing these kind of vectors given sequences and trying to explicitly constant

1378
01:34:42,240 --> 01:34:46,290
is that is is is is not very efficient

1379
01:34:46,490 --> 01:34:49,580
and the nice way

1380
01:34:49,800 --> 01:34:57,940
that leslie and her coco others found to to do that is to exploit the

1381
01:34:58,630 --> 01:35:00,980
structure of suffix trees

1382
01:35:01,050 --> 01:35:07,120
so i don't know if you're aware of specifics trees but there are those are

1383
01:35:07,620 --> 01:35:12,290
structure very efficient if you very efficient for instance if you want to you have

1384
01:35:12,290 --> 01:35:19,530
a whole dictionary and you want to find the word containing i don't know ABC

1385
01:35:19,580 --> 01:35:26,070
then this is precisely the kind of thing of things that travels on electronic dictionary

1386
01:35:26,110 --> 01:35:29,190
it is very easy to find words that contain some subsequence

1387
01:35:29,690 --> 01:35:33,750
and the reason why it's very efficient to do that is because they use suffix

1388
01:35:33,750 --> 01:35:35,170
trees suffix trees

1389
01:35:35,220 --> 01:35:38,050
are our structures

1390
01:35:38,090 --> 01:35:41,300
that are very compact and their help you

1391
01:35:42,780 --> 01:35:47,910
and organize all your data and help you

1392
01:35:47,950 --> 01:35:52,760
find a subset sequence of things

1393
01:35:52,780 --> 01:35:57,300
OK so the the thing is that they take the they have the they they

1394
01:35:57,320 --> 01:36:02,950
took advantage of this kind of structures and used it to compute to efficiently compute

1395
01:36:05,130 --> 01:36:13,160
the dot product between the the the the victorious that i showed you just before

1396
01:36:16,530 --> 01:36:20,570
i mean that i should have put it in in red because this is the

1397
01:36:20,570 --> 01:36:21,840
crux of

1398
01:36:21,880 --> 01:36:23,090
building kernels

1399
01:36:23,090 --> 01:36:27,410
four forest structured data again

1400
01:36:27,510 --> 01:36:34,110
very powerful idea for the literals to build kernels for for structured data is to

1401
01:36:34,860 --> 01:36:39,090
objects with respect to all the substructures that you have

1402
01:36:39,160 --> 01:36:40,800
and the

1403
01:36:40,800 --> 01:36:44,780
the real question is to do that very efficient

1404
01:36:44,820 --> 01:36:54,950
and doing that very efficiently makes you consider nice structure like a tree struck suffix

1405
01:36:54,950 --> 01:37:02,880
tree structures or dynamic programming or or things very very beautiful things that come for

1406
01:37:02,880 --> 01:37:04,200
computer science

1407
01:37:04,240 --> 01:37:11,300
from the ongoing point of view so my structures myself greetings and and again the

1408
01:37:11,300 --> 01:37:14,410
the the the thing is to find

1409
01:37:14,470 --> 01:37:16,470
the nice trade of

1410
01:37:16,490 --> 01:37:20,050
again between how

1411
01:37:20,070 --> 01:37:25,800
how informative are structures that you have and how easy it is to to make

1412
01:37:25,800 --> 01:37:29,680
computations on those substructures

1413
01:37:29,880 --> 01:37:34,930
so the results that they had protein classification problem where

1414
01:37:35,010 --> 01:37:39,840
just comparable to other methods but not better and

1415
01:37:39,880 --> 01:37:45,070
these pools the need of another sequence kernel they they propose another one a little

1416
01:37:45,070 --> 01:37:46,610
bit more involved

1417
01:37:46,640 --> 01:37:52,610
not not that very are different but they used what they call the mismatch string

1418
01:37:54,050 --> 01:37:59,110
if you want if you really want the details on these people i invite you

1419
01:37:59,110 --> 01:38:03,910
to just down the papers you can download them and so it's

1420
01:38:03,910 --> 01:38:09,270
what we learn is that shape varies across categories but not within categories in shape

1421
01:38:09,280 --> 01:38:13,720
is just behaves just like colored here but these other dimensions of texture colour and

1422
01:38:13,720 --> 01:38:16,100
size as you as you might guess

1423
01:38:16,100 --> 01:38:18,100
i don't know

1424
01:38:18,120 --> 01:38:22,200
and if we if we then go and apply this to the case of the

1425
01:38:24,480 --> 01:38:27,750
you know that there was actually tested the sort of transfer fees so now we

1426
01:38:27,750 --> 01:38:30,940
have a new object in a new category number five here

1427
01:38:31,000 --> 01:38:33,460
and a new shape

1428
01:38:34,130 --> 01:38:35,870
texture new color

1429
01:38:35,890 --> 01:38:36,650
so on

1430
01:38:36,680 --> 01:38:40,530
what's important in this simulation of course any models that to do something like this

1431
01:38:40,530 --> 01:38:42,890
for this task is that

1432
01:38:43,520 --> 01:38:49,520
we assume there's more possible shapes textures colors and so on then actually observed during

1433
01:38:49,520 --> 01:38:52,940
training may be a lot more but the key thing about this kind of hierarchical

1434
01:38:52,940 --> 01:38:58,060
bayesian model is it can learn expectations even for them the values of dimensions that

1435
01:38:58,060 --> 01:39:01,440
have been seen so even for example for shape

1436
01:39:01,490 --> 01:39:03,500
that has never been seen before

1437
01:39:03,510 --> 01:39:08,750
paired with the category labels and then we have these three test items this one

1438
01:39:08,750 --> 01:39:11,320
here corresponds to having the same shape

1439
01:39:11,550 --> 01:39:16,410
as as the as the dax but a different new texture and color

1440
01:39:16,430 --> 01:39:18,560
this one has a different shape

1441
01:39:18,600 --> 01:39:21,590
and the different color but the same texture and so on and what the model

1442
01:39:21,590 --> 01:39:22,950
tells us is that

1443
01:39:23,000 --> 01:39:27,150
this one is much more likely to have come from the same category this one

1444
01:39:27,150 --> 01:39:31,430
came from the these two so relatively this is the best choice for next so

1445
01:39:31,430 --> 01:39:36,590
even this very simple model is capable of of capturing this kind of abstract transfer

1446
01:39:36,600 --> 01:39:38,700
that the that children seem to show

1447
01:39:38,710 --> 01:39:43,190
now this model is is very simple and it's easy to come up with limitations

1448
01:39:43,190 --> 01:39:47,650
for example assuming that we have a a single dimension of shape maybe seems like

1449
01:39:47,850 --> 01:39:51,490
a limitation and there's lots of extensions which you can go through which i'm i'm

1450
01:39:51,490 --> 01:39:54,630
not going to go through here because i'm going to shift to a different topic

1451
01:39:54,630 --> 01:39:56,600
you know we can work with more

1452
01:39:56,600 --> 01:40:00,410
distributed representations of shape for example and get the same kind of effect of this

1453
01:40:00,410 --> 01:40:06,090
talk about one extension because it's it's again corresponds to an interesting developmental milestones and

1454
01:40:06,090 --> 01:40:11,810
it has a nice machine learning correlate which is interesting kind of selective transfer depending

1455
01:40:11,810 --> 01:40:15,130
on knowledge of ontological kinds

1456
01:40:16,920 --> 01:40:20,000
the shape bias that we saw was a phenomenon that takes into an h two

1457
01:40:20,420 --> 01:40:21,870
by age three

1458
01:40:21,880 --> 01:40:25,220
children know about the shape bias but they also know that other kinds of biases

1459
01:40:25,220 --> 01:40:29,090
for example and they restrict them appropriately to certain

1460
01:40:29,100 --> 01:40:34,170
large-scale categories so they apply the shape bias to solid object categories like ball book

1461
01:40:34,170 --> 01:40:35,870
toothbrush but

1462
01:40:35,950 --> 01:40:41,170
for non solid substances like for example toothpaste they apply material bias you can introduce

1463
01:40:41,190 --> 01:40:47,180
new weird gel nonsolid substance or if we had everything

1464
01:40:47,190 --> 01:40:50,490
and they will not care about the shape of the

1465
01:40:50,510 --> 01:40:54,550
of that entity but they will care about its material properties in generalizing and were

1466
01:40:54,560 --> 01:40:58,270
again that's correct bias so it takes a little bit longer to learn so how

1467
01:40:58,270 --> 01:41:01,650
can we model this kind of selective transfer well you might say

1468
01:41:01,690 --> 01:41:07,140
suppose we have we know the ontological kind whether something is solid object or nonsolid

1469
01:41:09,120 --> 01:41:13,350
that's the easy variable here then we could just learn a separate dirichlet multinomial model

1470
01:41:13,650 --> 01:41:15,070
for each

1471
01:41:15,110 --> 01:41:20,940
if for each ontological category you know this is just adding another level to the

1472
01:41:20,940 --> 01:41:27,940
bayesian hierarchy so here capturing variability in features for solid objects knowing that say all

1473
01:41:28,280 --> 01:41:33,080
all things in this category are solid but shape is variable

1474
01:41:33,100 --> 01:41:37,450
across objects although not within category and other features are variable within categories while the

1475
01:41:37,450 --> 01:41:42,350
cross-country and then the lower level learning specifics about what words refer to which shapes

1476
01:41:42,530 --> 01:41:46,450
and then we can we will learn something corresponding over here on the material side

1477
01:41:46,480 --> 01:41:47,320
but of course

1478
01:41:47,320 --> 01:41:52,230
this itself is already oversimplified because the data don't necessarily come labelled this way we

1479
01:41:52,230 --> 01:41:56,660
don't appear in know what the right ontological kinds are the this is chicken and

1480
01:41:56,660 --> 01:42:01,610
egg problem we don't know how to partition the object categories into

1481
01:42:01,670 --> 01:42:06,010
these these different parts of the of the hierarchical structure

1482
01:42:06,570 --> 01:42:11,490
a solution to this is just in to add inference at this higher level using

1483
01:42:11,490 --> 01:42:16,450
a nonparametric prior over this partition in particular using a chinese restaurant

1484
01:42:16,530 --> 01:42:20,380
process prior so we're saying that

1485
01:42:22,740 --> 01:42:24,410
each object category

1486
01:42:24,430 --> 01:42:28,950
belongs to some higher level ontological kind that's the easy vector but we don't know

1487
01:42:28,950 --> 01:42:33,530
how many ontological kinds are which object categories in which ontological kind but we can

1488
01:42:33,530 --> 01:42:36,280
do inference over that at the same time as we do inference about these two

1489
01:42:36,280 --> 01:42:40,860
levels of parameters and in fact the simple toy datasets recover the right kind of

1490
01:42:40,860 --> 01:42:41,760
structure here

1491
01:42:41,780 --> 01:42:46,550
so it's this is this is kind of nice it's certainly related to CRP mixtures

1492
01:42:46,680 --> 01:42:52,240
but instead of clustering objects or clustering categories of objects and the what the objects

1493
01:42:52,240 --> 01:42:55,200
in a conventional clustering model the objects

1494
01:42:55,220 --> 01:42:58,740
i have something in common here the categories of objects don't really have anything concrete

1495
01:42:58,740 --> 01:43:02,550
in common in fact the most distinctive feature which is the shape back of objects

1496
01:43:02,550 --> 01:43:06,950
in the category is totally different for every category but what they have in common

1497
01:43:07,150 --> 01:43:09,510
is that they draw on the same kind of prior

1498
01:43:09,570 --> 01:43:13,090
so it's this kind of

1499
01:43:13,130 --> 01:43:18,840
class in the sense clustering but you get an abstract level by combining a nonparametric

1500
01:43:19,010 --> 01:43:23,780
cluster prior with a hierarchical bayesian model is a powerful way to do selective transfer

1501
01:43:23,820 --> 01:43:25,670
and in fact very recently

1502
01:43:25,900 --> 01:43:30,550
some other researchers at MIT dan roy unless the cabling have to find a related

1503
01:43:30,550 --> 01:43:35,200
model for selected transfer task in in in in an applied setting where you're trying

1504
01:43:35,200 --> 01:43:39,950
to learn it's project where you're trying to learn whether user will accept meeting invitation

1505
01:43:40,300 --> 01:43:43,840
and they learn the naive bayes classifier for each user but then they define a

1506
01:43:43,840 --> 01:43:45,550
CRP prior

1507
01:43:45,550 --> 01:43:50,090
over the parameters of those naive bayes classifiers and they able to sort of improved

1508
01:43:50,090 --> 01:43:54,800
transfer by triggered by figuring out just exactly which users should be should be used

1509
01:43:54,800 --> 01:43:56,680
to transfer to which other users

1510
01:43:56,700 --> 01:43:59,340
so this kind of idea could have

1511
01:43:59,400 --> 01:44:01,170
i think a lot of implications

1512
01:44:01,170 --> 01:44:06,200
other applications of machine learning now what i want to do next is turned to

1513
01:44:07,130 --> 01:44:10,740
setting which is in some sense more knowledge rich in the in the previous setting

1514
01:44:10,740 --> 01:44:13,610
it was nice because the time it was very simple

1515
01:44:13,630 --> 01:44:18,590
elegant mathematical model of the very important developmental phenomenon but there wasn't much interesting structure

1516
01:44:18,590 --> 01:44:23,010
knowledge representation going on which i was arguing is so crucial for human learning so

1517
01:44:23,010 --> 01:44:26,990
to get that what we're going to switch to a different task which is another

1518
01:44:26,990 --> 01:44:30,840
one of these tasks generalizing from a few examples it's task that many psychologists have

1519
01:44:32,010 --> 01:44:35,670
really since the mid-seventies there's a lot of data here that you can model

1520
01:44:36,090 --> 01:44:38,320
and our models can get much more quantity

1521
01:44:38,410 --> 01:44:41,240
so here's the kind of task

1522
01:44:41,240 --> 01:44:45,550
it's in the form of evaluating the strength of an inductive argument but again it's

1523
01:44:45,550 --> 01:44:49,740
like generalizing a new concept from a few examples so these are the premises guerrillas

1524
01:44:49,740 --> 01:44:54,340
have t nine hormones seals have t nine hormones scrolls have t nine hormones and

1525
01:44:54,340 --> 01:44:57,610
you ask how likely is it that the conclusion is true given these premises how

1526
01:44:57,610 --> 01:45:01,550
likely is it that horses have t nine hormones given that these two so to

1527
01:45:01,570 --> 01:45:05,180
compare this with say the word learning case there's a new concept which is having

1528
01:45:05,180 --> 01:45:08,150
t nine hormones and you get a few examples of things and that concept and

1529
01:45:08,150 --> 01:45:10,860
you have to judge the probability that something else is in the concepts of the

1530
01:45:10,860 --> 01:45:14,760
same kind of task but it's it's in some sense easier to study

1531
01:45:14,760 --> 01:45:23,440
talk about something even more challenging than making these bots fight successfully and that is

1532
01:45:23,490 --> 01:45:26,140
to have a team AI

1533
01:45:30,360 --> 01:45:31,580
well it works not

1534
01:45:31,680 --> 01:45:39,480
you probably have the right codec if you want to go

1535
01:45:41,930 --> 01:45:43,190
that's a good idea

1536
01:45:45,430 --> 01:45:50,700
so it's quite clear that if you want to have a team behaving in in

1537
01:45:50,710 --> 01:45:57,040
some interesting ways in the video working together then you have these additional problems of

1538
01:45:57,040 --> 01:46:00,350
cooperation and coordination of the team effort

1539
01:46:02,840 --> 01:46:07,580
now there is essentially two approaches to this you can view the team as one

1540
01:46:08,760 --> 01:46:13,960
and this age is very complex beast and has to do all of this coordination

1541
01:46:14,460 --> 01:46:17,440
essentially from a centralized

1542
01:46:17,490 --> 01:46:20,460
the decision function point-of-view

1543
01:46:20,480 --> 01:46:24,770
and what have view that is of course much more elegant is to view this

1544
01:46:24,770 --> 01:46:31,610
as a distributed system we agents that have individual policies but certain ways of communicating

1545
01:46:32,050 --> 01:46:35,360
and thereby of cooperating

1546
01:46:35,370 --> 01:46:42,970
and i would just like to point out is probably most famous example

1547
01:46:42,990 --> 01:46:48,970
robocop where people have been studying these multiagent systems for quite awhile

1548
01:46:48,980 --> 01:46:54,030
we do i don't actually know which type of this is

1549
01:46:54,100 --> 01:46:58,000
the this thing works

1550
01:46:58,580 --> 01:47:00,860
well these are not my thing here i think

1551
01:47:02,620 --> 01:47:08,220
the nice thing is in robocup that's very very different leagues some of them i

1552
01:47:08,220 --> 01:47:12,570
think i do with some of them are real robots of course

1553
01:47:12,590 --> 01:47:15,910
and some of them follow this idea of

1554
01:47:15,920 --> 01:47:23,270
of the abstraction by just simulating a soccer games and also stimulating the inputs the

1555
01:47:23,350 --> 01:47:25,280
individual players get

1556
01:47:25,690 --> 01:47:27,970
and so this is very

1557
01:47:28,820 --> 01:47:35,030
very abstract version of of soccer of course and there is a huge literature on

1558
01:47:35,060 --> 01:47:38,880
how people have approach this problem that the typical approach goes

1559
01:47:39,350 --> 01:47:44,840
four nice approach if you like is to study subproblem you could have a

1560
01:47:44,870 --> 01:47:51,340
three attackers in front of the goal and two defenders and the goalie and then

1561
01:47:51,350 --> 01:47:53,380
you would have

1562
01:47:53,400 --> 01:47:58,520
policy is say parametrized by neural networks and you could for example copy of the

1563
01:47:58,520 --> 01:48:04,180
policy into each of the attacking players and you would have a different policy for

1564
01:48:04,190 --> 01:48:10,220
the two defenders and even different policy for the goalkeeper and then you would just

1565
01:48:11,290 --> 01:48:17,490
and you would get the reinforcement signals from the results of play you could imagine

1566
01:48:17,490 --> 01:48:24,510
that you get very positive feedback if for example the the attackers before they would

1567
01:48:24,510 --> 01:48:28,840
get the positive feedback and in that case this defenders would get a negative feedback

1568
01:48:28,840 --> 01:48:31,520
signal and

1569
01:48:31,540 --> 01:48:36,990
the interesting thing is that people never really succeeded in in generating corporation in this

1570
01:48:36,990 --> 01:48:42,270
way if only enough information of what the other agents were doing is made available

1571
01:48:42,270 --> 01:48:49,540
to any single agent and those are quite interesting pieces of work

1572
01:48:49,560 --> 01:48:56,260
so in this particular example of robocop the typical decisions that have to be made

1573
01:48:56,260 --> 01:49:00,720
are far field player if he should read the efficient if it should pass or

1574
01:49:00,830 --> 01:49:02,300
he he should shoot

1575
01:49:02,320 --> 01:49:07,920
for a goalkeeper as he has to predict whether ball might be going or if

1576
01:49:07,930 --> 01:49:11,940
the if the ball is passed in front of the goal then of course he

1577
01:49:11,940 --> 01:49:17,730
has to switch to another focus point and the really exciting thing is if by

1578
01:49:17,730 --> 01:49:24,910
training single agents here you can get emergence emergent behavior however there's a caveat again

1579
01:49:24,910 --> 01:49:29,230
this is not a game designer i mean it's not there are nasty guys but

1580
01:49:29,230 --> 01:49:33,470
they have this very strong focus on predictability and so they may not be very

1581
01:49:33,470 --> 01:49:39,720
happy with this emergent behavior so it's very important to test the limits of this

1582
01:49:40,970 --> 01:49:46,710
so well robocup is a relatively simple one

1583
01:49:46,710 --> 01:49:48,470
many constructive algorithm from it

1584
01:49:48,890 --> 01:49:51,990
and if you look at you know a space of possible features right in the

1585
01:49:51,990 --> 01:49:55,560
way that computer vision was done is in the last twenty years is that people

1586
01:49:55,560 --> 01:49:58,240
are trying to figure out what the right representations to use

1587
01:49:58,940 --> 01:50:00,910
based on images so if you look at different

1588
01:50:02,490 --> 01:50:04,340
there's things like sifts

1589
01:50:04,840 --> 01:50:07,480
so text arms and you know all kinds of stuff

1590
01:50:07,890 --> 01:50:08,720
that people are

1591
01:50:09,090 --> 01:50:09,940
i have been developing

1592
01:50:10,800 --> 01:50:14,250
if you look at the audio features again spectrograms them after he sees

1593
01:50:14,930 --> 01:50:18,460
flux and again all kinds of different features to use

1594
01:50:19,040 --> 01:50:25,140
right one question that's you know we're interested in and in general deploying communities interested

1595
01:50:25,150 --> 01:50:28,370
in is that how can we do feature learning can we actually figure out what

1596
01:50:28,430 --> 01:50:30,060
the right features to use from these data

1597
01:50:32,620 --> 01:50:36,020
and it turns out that's you know i've shown you some of the examples of

1598
01:50:36,190 --> 01:50:41,640
recognition it was actually a remarkable breakthrough because by using these models and in learning

1599
01:50:41,640 --> 01:50:42,970
the features themselves

1600
01:50:43,710 --> 01:50:49,530
right you can outperform basically the state of the art recognition systems that have been

1601
01:50:49,530 --> 01:50:53,510
used and know that the vision community has been using speech recognition community have been

1602
01:50:53,510 --> 01:50:56,790
using in the past ten fifteen years

1603
01:50:58,750 --> 01:51:00,440
so one particular model

1604
01:51:01,450 --> 01:51:05,840
this is sort of the oldest model ford for learning features is is sparse coding

1605
01:51:05,840 --> 01:51:09,370
model has been used a lot it's it's it's a very popular models one of

1606
01:51:09,370 --> 01:51:12,290
the building blocks of of many deep learning algorithms

1607
01:51:14,080 --> 01:51:15,350
and the idea is the following

1608
01:51:15,860 --> 01:51:16,390
you've have

1609
01:51:16,850 --> 01:51:21,860
a set of input data vectors x one to seven and you want to learn a dictionary of bases

1610
01:51:22,690 --> 01:51:26,520
and you want additional bases such that you can reconstruct the data

1611
01:51:27,180 --> 01:51:29,310
based on on these bases

1612
01:51:29,820 --> 01:51:35,970
right so it's a very simple formulation but the key thing here is that what you'd like to do

1613
01:51:36,570 --> 01:51:37,990
is you'd like these

1614
01:51:40,660 --> 01:51:42,300
these coefficients to be sparse

1615
01:51:42,800 --> 01:51:46,680
so what is essentially saying is that well each data object is gonna be represented

1616
01:51:46,680 --> 01:51:48,620
as a sparse linear combination of basis

1617
01:51:49,200 --> 01:51:52,380
so you know you can imagine that you have you know ten thousand bases but

1618
01:51:52,380 --> 01:51:57,120
you might wanna say well only ten of these vectors should be used for reconstructing

1619
01:51:57,120 --> 01:51:58,110
the data right

1620
01:52:00,740 --> 01:52:01,480
that's the idea

1621
01:52:01,960 --> 01:52:05,870
and this belongs to the class of so-called unsupervised learning algorithm there is no labels

1622
01:52:05,870 --> 01:52:08,860
are just trying to find the right basis so that can reconstruct the data is

1623
01:52:09,080 --> 01:52:09,540
as as

1624
01:52:10,160 --> 01:52:10,860
as much as you can

1625
01:52:11,730 --> 01:52:17,360
so for example for natural images if i take a natural images ethical patches from those natural images

1626
01:52:18,040 --> 01:52:21,440
then this is how these bases would look like so you can see a lot

1627
01:52:21,440 --> 01:52:24,940
of them look like little edges or r a little bit more like

1628
01:52:28,330 --> 01:52:30,930
that's seems like the right structure extract from images

1629
01:52:31,780 --> 01:52:32,580
at least when you talk to

1630
01:52:34,290 --> 01:52:36,990
neuroscientists they believe that that's actually something that's

1631
01:52:37,820 --> 01:52:40,050
happens in the visual area one human brain

1632
01:52:41,680 --> 01:52:43,750
if i give you a new example a new patch

1633
01:52:44,180 --> 01:52:45,250
then i can essentially

1634
01:52:46,780 --> 01:52:49,780
reconstructed is made is the sum of these three patches so can sort of see

1635
01:52:49,780 --> 01:52:51,930
that u u picks up by a

1636
01:52:52,180 --> 01:52:58,180
a sparse number or few number of these bases and can reconstruct an example and it turns out that these

1637
01:52:59,250 --> 01:53:00,790
the new representation that you get

1638
01:53:01,730 --> 01:53:06,830
despite the presentation that together what is one particular with the feature representation it turns

1639
01:53:06,830 --> 01:53:09,380
out to be much more useful to deal with

1640
01:53:09,940 --> 01:53:11,260
then the pixels themselves

1641
01:53:11,760 --> 01:53:14,990
right so working in this basis if you want for example classify

1642
01:53:15,720 --> 01:53:17,720
these images as turns out to be much more

1643
01:53:19,890 --> 01:53:21,640
has much better generalization performance

1644
01:53:23,190 --> 01:53:26,700
so how would you do training of of of these models well

1645
01:53:27,290 --> 01:53:29,090
u half an arm

1646
01:53:29,490 --> 01:53:30,960
inputs and you'd like to learn

1647
01:53:31,410 --> 01:53:32,040
the bases

1648
01:53:32,980 --> 01:53:37,410
so you have two different terms and one term is essentially saying well you'd want

1649
01:53:37,410 --> 01:53:39,280
to reconstruct the data as as

1650
01:53:40,370 --> 01:53:44,730
without introducing any years so you'd you'd like to have a small reconstruction error

1651
01:53:45,250 --> 01:53:49,010
on the other hand we also have a sparsity penalty you say well i don't

1652
01:53:49,030 --> 01:53:51,590
use a water-based reconstructed i just want use a few

1653
01:53:52,190 --> 01:53:55,070
basis to reconstruct this is kind of like controls so

1654
01:53:55,470 --> 01:53:56,680
the complexity of the model

1655
01:53:59,470 --> 01:54:02,540
and the optimization itself can be done fairly easily

1656
01:54:03,050 --> 01:54:07,600
it turns out that's you know for a fixed if you're fixing the basis phi why up to fight

1657
01:54:08,500 --> 01:54:10,750
and you solving forty coefficients in

1658
01:54:11,510 --> 01:54:19,670
then it turns out to be standard lasso problem problem right so just squared loss plus one penalty vicious lasso

1659
01:54:20,060 --> 01:54:24,910
and they can be solved fairly efficiently using quadratic programming so that's that's standard

1660
01:54:25,790 --> 01:54:27,000
you know machine learning one one

1661
01:54:28,960 --> 01:54:30,380
and then when you fix thee

1662
01:54:31,050 --> 01:54:36,780
activations you'd optimise for the dictionary basis and then again that's a convex optimization problem

1663
01:54:36,780 --> 01:54:38,740
is a quadratic programming problem and you can solve

1664
01:54:39,330 --> 01:54:41,180
so consider these alternating

1665
01:54:42,690 --> 01:54:45,270
minimization fixing the basis finding the activations

1666
01:54:45,730 --> 01:54:48,320
fixing the activations reflecting the new basis

1667
01:54:49,450 --> 01:54:51,840
so that's a pretty straightforward and now the best time

1668
01:54:51,840 --> 01:54:57,560
there's is also symmetric successive over relax asian which is you do every exotic combination

1669
01:54:57,560 --> 01:55:01,450
of upper and lower triangular diagonal parts of the original matrix and you put it

1670
01:55:01,450 --> 01:55:05,680
into a big messy thing and then you back solve OK so that's another classic

1671
01:55:05,680 --> 01:55:09,290
thing that people do OK so very classic

1672
01:55:09,310 --> 01:55:13,170
methods and can be thought of nothing more than preconditioned iterative methods are

1673
01:55:13,190 --> 01:55:14,360
and can

1674
01:55:15,170 --> 01:55:18,850
so these things still are to are too slow into an unreliable in the sense

1675
01:55:18,850 --> 01:55:22,770
of gauss i doubt if it converges great if it doesn't it's not so clear

1676
01:55:22,800 --> 01:55:26,470
you right so the problem is is that you know

1677
01:55:26,550 --> 01:55:31,770
our goal is to generate solvers for which they work systematically right what i would

1678
01:55:31,770 --> 01:55:34,770
like to be able to do is generated solver for which i can

1679
01:55:34,780 --> 01:55:39,060
put it into a program role into a doctor's office and have them music twenty

1680
01:55:39,060 --> 01:55:41,740
four seven and not phoned me up in the middle of the night and say

1681
01:55:41,740 --> 01:55:43,360
this already in the works

1682
01:55:43,400 --> 01:55:46,910
i mean there's other kind of solvers right to the big distinction here to make

1683
01:55:46,910 --> 01:55:48,470
the the reliability right

1684
01:55:48,490 --> 01:55:52,150
so i would imagine if all you we want to solver for which

1685
01:55:52,190 --> 01:55:55,380
you're going to go home and use it yourself personally

1686
01:55:55,390 --> 01:55:59,720
its sales one two hundred times about the failure rate of the shuttle that may

1687
01:55:59,720 --> 01:56:03,530
be just fine right i mean the spatial that's all you need

1688
01:56:03,540 --> 01:56:07,360
if that's the case you want to send this selling products

1689
01:56:07,380 --> 01:56:10,900
then failure rates one two hundred and a little bit is too high in the

1690
01:56:10,930 --> 01:56:14,570
lower failure rate so i would like to do is actually have our that always

1691
01:56:15,390 --> 01:56:18,620
so that's what i mean by unreliable most of the stuff works nice most of

1692
01:56:18,620 --> 01:56:20,720
the time but it will fail

1693
01:56:20,730 --> 01:56:23,180
and this the wrong OK

1694
01:56:26,450 --> 01:56:30,120
so so in order to be able to come up with a theory for which

1695
01:56:30,120 --> 01:56:33,550
we can claim that they are the works were systematically in other words we can

1696
01:56:33,550 --> 01:56:39,340
give guarantees unfortunately we have to start restricting the number of kind of matrices they

1697
01:56:39,350 --> 01:56:40,420
come up

1698
01:56:40,430 --> 01:56:44,840
fortunately if you look at most people in this room they think this is a

1699
01:56:44,840 --> 01:56:48,050
very natural restriction and so they're not unhappy at all

1700
01:56:48,100 --> 01:56:52,700
you're not happy but but nonetheless it is a major restriction on the types of

1701
01:56:52,700 --> 01:56:56,410
matrices we're going to be able to solve so let's talk about the graph plots

1702
01:56:56,410 --> 01:56:57,670
in which you all know

1703
01:56:57,680 --> 01:57:02,560
then we take a weighted undirected graph you some albeit weights are

1704
01:57:03,640 --> 01:57:08,730
strictly positive you define the incidence matrix you find the degree matrix which is just

1705
01:57:08,730 --> 01:57:11,130
simply the sum of the weights

1706
01:57:11,180 --> 01:57:16,500
giving you a diagonal matrix and from that and the la plata simply d minus

1707
01:57:16,540 --> 01:57:20,140
and so for what we're gonna do now for the next while then is we're

1708
01:57:20,140 --> 01:57:21,930
going to restrict ourself to

1709
01:57:23,630 --> 01:57:29,660
linear systems where the underlying system is as the graph of plus

1710
01:57:33,010 --> 01:57:36,420
so the graph plotting comes up and in many places

1711
01:57:37,800 --> 01:57:39,900
every every other talk here

1712
01:57:39,900 --> 01:57:43,850
let me just make this review some of the classic places it comes up

1713
01:57:43,870 --> 01:57:48,150
one of the reasons it's useful to do that is is that even if you

1714
01:57:48,150 --> 01:57:50,560
don't care about these applications

1715
01:57:50,620 --> 01:57:54,730
and you're still using graph of in is very useful to know that

1716
01:57:54,750 --> 01:57:57,520
the system you're solving has very other

1717
01:57:57,540 --> 01:57:59,770
and a multitude of other meanings

1718
01:57:59,770 --> 01:58:02,410
so you're solving the system for some other meeting

1719
01:58:02,430 --> 01:58:05,600
you know what's some marketing problem or something

1720
01:58:05,650 --> 01:58:09,650
but it may have some very natural meaning in terms resistors or in terms of

1721
01:58:09,650 --> 01:58:13,570
spring mass system other things right so let's go through some

1722
01:58:13,600 --> 01:58:16,880
so one idea is severely changes the conductor

1723
01:58:16,900 --> 01:58:18,910
with conductance w i j

1724
01:58:20,730 --> 01:58:24,570
so if you let v be a column vector of voltages

1725
01:58:24,680 --> 01:58:26,060
then lv

1726
01:58:26,070 --> 01:58:27,810
it will see

1727
01:58:27,840 --> 01:58:33,320
then c will be the residual current needed to maintain the given voltage

1728
01:58:33,620 --> 01:58:37,070
OK so

1729
01:58:37,200 --> 01:58:42,880
OK so in particular what it says then it is that in order to maintain

1730
01:58:42,880 --> 01:58:46,400
the voltage at those nodes are going to have to either inject current to remove

1731
01:58:46,400 --> 01:58:51,040
current them out you have to inject or are removed is precisely c

1732
01:58:51,060 --> 01:58:54,300
you can also turn this on its head if you know how much current you

1733
01:58:54,300 --> 01:58:55,690
want to inject

1734
01:58:55,700 --> 01:59:00,360
then what voltages will flow to in order to maintain to have those current

1735
01:59:00,400 --> 01:59:06,170
OK and so the classic way to an overview of the glaciers very useful

1736
01:59:07,040 --> 01:59:10,640
so another place this comes up that's basically the heat equation

1737
01:59:10,680 --> 01:59:12,490
and so that's the standard

1738
01:59:12,640 --> 01:59:14,610
places comes up

1739
01:59:14,630 --> 01:59:18,020
and other places comes up as a random walk

1740
01:59:18,130 --> 01:59:24,020
again here the transition matrix ends up being simply the inverse square diagonal matrix we

1741
01:59:24,020 --> 01:59:25,090
had before

1742
01:59:25,100 --> 01:59:26,730
times posterior

1743
01:59:26,750 --> 01:59:30,870
an interesting question and it is to compute the fundamental eigen vectors of these kind

1744
01:59:30,870 --> 01:59:36,430
of systems and one amazing thing here is that fast solvers which will talk about

1745
01:59:36,430 --> 01:59:38,150
this film and tanks over

1746
01:59:38,170 --> 01:59:39,760
is very theoretical

1747
01:59:39,770 --> 01:59:43,060
but it's it's interesting in the sense that says that the problem of getting harder

1748
01:59:43,060 --> 01:59:45,640
so i won't be able to prove that you could do that it takes more

1749
01:59:47,750 --> 01:59:50,820
what ends up happening is well maybe this is the wrong place for it in

1750
01:59:50,820 --> 01:59:52,610
the talk about

1751
01:59:52,620 --> 01:59:56,730
if you're interested in i can vectors of graph plus unions

1752
01:59:56,740 --> 02:00:01,010
so the standard theory would say you trying to do forward powers the way to

1753
02:00:01,010 --> 02:00:03,360
find cycles vectors

1754
02:00:03,380 --> 02:00:08,880
so we found both experimentally and theoretically actually doing inverse powering is the trick that

1755
02:00:08,880 --> 02:00:12,900
works much better if you in fact simply do take here

1756
02:00:12,900 --> 02:00:17,640
i initial guess for nike in vector and you simply then take the inverse power

1757
02:00:17,680 --> 02:00:23,030
that gas and repeat that process after log iterations will converge to something with very

1758
02:00:23,030 --> 02:00:27,090
small rayleigh quotient in practice something that looks like an idea to

1759
02:00:27,100 --> 02:00:30,770
OK so it's very powerful track which may be over that but

1760
02:00:30,790 --> 02:00:31,950
you know that

1761
02:00:31,980 --> 02:00:34,700
that's very nice idea OK

1762
02:00:34,770 --> 02:00:39,440
so to the other place that will questions come up is in spring mass systems

1763
02:00:40,800 --> 02:00:47,300
again suppose we have a weighted graph and suppose we view instead the constants

1764
02:00:47,310 --> 02:00:48,910
as spring constants

1765
02:00:48,930 --> 02:00:51,340
the w ideas spring constant

1766
02:00:53,190 --> 02:00:55,610
and suppose we let and

1767
02:00:55,630 --> 02:00:58,840
the diagonal matrix of mass constants

1768
02:00:59,700 --> 02:01:01,450
a standard results

1769
02:01:01,490 --> 02:01:04,650
is that if you look at the item pairs

1770
02:01:04,660 --> 02:01:05,980
to this system

1771
02:01:05,980 --> 02:01:10,980
part of speech information and syntactic information seems to be fairly relevant for reordering

1772
02:01:11,000 --> 02:01:16,590
some publication hope that a little bit more if you believe this text as building

1773
02:01:16,590 --> 02:01:19,960
models are talking about non surface forms part of part of speech tags

1774
02:01:20,000 --> 02:01:22,050
does it help to you

1775
02:01:22,070 --> 02:01:27,190
you could also use actually bigger blocks for part of speech tags if you might

1776
02:01:27,190 --> 02:01:29,170
have seen a seven

1777
02:01:29,190 --> 02:01:31,840
but part of speech text sequence very often

1778
02:01:31,840 --> 02:01:35,110
so does that help you with reordering

1779
02:01:35,150 --> 02:01:36,520
and the answers

1780
02:01:36,550 --> 02:01:40,050
yes but things get very complicated

1781
02:01:41,710 --> 02:01:43,690
so that that's one thing to do

1782
02:01:43,880 --> 02:01:49,730
there's also the idea to use some kind of shallow syntax two

1783
02:01:49,790 --> 02:01:52,340
not only have part of speech tags but have

1784
02:01:52,340 --> 02:01:53,520
you know

1785
02:01:53,750 --> 02:01:56,610
tax that span multiple births

1786
02:01:56,860 --> 02:01:59,650
so check there is

1787
02:01:59,670 --> 02:02:02,920
you actually want to have like two words subject there

1788
02:02:04,000 --> 02:02:06,840
and to do with that you can say well the first one is the beginning

1789
02:02:06,840 --> 02:02:11,000
of the noun phrase and the second was the continuation of the noun phrase that

1790
02:02:11,000 --> 02:02:16,650
allows you to label things chunk things into the syntactic chunks and maybe using these

1791
02:02:16,650 --> 02:02:19,690
kind of things helps as well

1792
02:02:25,440 --> 02:02:28,340
this is more than long range reordering

1793
02:02:28,360 --> 02:02:33,650
this is work that kind of finished at this point one

1794
02:02:34,360 --> 02:02:35,920
factor models that's it

1795
02:02:35,960 --> 02:02:38,520
any questions for us

1796
02:02:49,250 --> 02:02:53,000
so i mean a lot of the stuff available because it's one of the fundamental

1797
02:02:53,000 --> 02:02:57,650
problems of people work in natural language processing so for english

1798
02:02:58,000 --> 02:03:00,940
there are several part of speech tags you can get

1799
02:03:00,960 --> 02:03:04,980
that they basically all train on things like the penn treebank

1800
02:03:04,980 --> 02:03:09,800
so there are so the one tag are used for initial rules

1801
02:03:09,820 --> 02:03:15,300
tiger which is available from his webpage stone technology in the nineties

1802
02:03:15,360 --> 02:03:20,170
probably also the the possibility of a lot of maximum entropy tag as that can

1803
02:03:20,170 --> 02:03:23,710
be trained if you have a corpus annotated part of speech tags

1804
02:03:23,710 --> 02:03:24,690
so not

1805
02:03:24,710 --> 02:03:29,380
sure participation spanish but i think there are things that treebank and part of speech

1806
02:03:30,460 --> 02:03:35,170
so those things are available

1807
02:03:36,840 --> 02:03:43,500
so so you the way we got subject object possibility by passing things so if

1808
02:03:43,500 --> 02:03:46,270
you have a pass so you can get these things

1809
02:03:46,610 --> 02:03:52,020
what seems a bit harder to get but is something available is this real morphological

1810
02:03:52,020 --> 02:03:55,980
analyses that say this is the root for the word in this in this has

1811
02:03:55,980 --> 02:03:57,570
this property

1812
02:03:59,170 --> 02:04:00,920
again for german we have

1813
02:04:00,980 --> 02:04:03,940
some i know i mean we've been using things

1814
02:04:04,050 --> 02:04:05,590
are available

1815
02:04:08,900 --> 02:04:09,920
yeah i

1816
02:04:09,960 --> 02:04:12,750
mean you know spanish new work was shown

1817
02:04:24,360 --> 02:04:26,570
as the

1818
02:04:35,710 --> 02:04:40,730
so i think part of speech taggers exist for many many languages process exist

1819
02:04:40,790 --> 02:04:42,400
if there's a treebank

1820
02:04:42,400 --> 02:04:46,300
mean there's a lot of effort in many languages developed readings

1821
02:04:47,460 --> 02:04:52,460
morphological analyses it harder to get the space some people do research on

1822
02:04:52,480 --> 02:04:55,770
it just send an email asking if you can get the tool also time to

1823
02:04:55,770 --> 02:04:57,170
give it to us

1824
02:04:57,170 --> 02:04:59,520
it was

1825
02:05:30,170 --> 02:05:37,400
the core region

1826
02:05:56,670 --> 02:05:59,300
there's so much o OK

1827
02:05:59,320 --> 02:06:04,020
so i said about the two sides civil society

1828
02:06:04,030 --> 02:06:08,590
OK so this is just to get some results on this

1829
02:06:08,590 --> 02:06:13,960
papers written as we speak so the basic idea is that if you look at

1830
02:06:13,960 --> 02:06:15,800
the paris peace sequence

1831
02:06:15,960 --> 02:06:19,250
you can learn much larger patterns

1832
02:06:19,270 --> 02:06:23,030
then you if you just look at the surface for so the phrase we extract

1833
02:06:23,420 --> 02:06:27,920
the phrase expecially the phrase we used to be the average length two words

1834
02:06:29,130 --> 02:06:32,340
and given the input sentence

1835
02:06:32,380 --> 02:06:35,520
there might be seven part of speech tags and sequence of you know how to

1836
02:06:36,770 --> 02:06:39,020
you have whatever you probably for each

1837
02:06:39,020 --> 02:06:42,110
noun phrase and translate from spanish to english

1838
02:06:42,110 --> 02:06:47,210
you have seen that particular pattern of whatever action known as the adjective noun

1839
02:06:47,460 --> 02:06:53,340
determine you have now and into whatever the terminology tragedy using those kind of patterns

1840
02:06:53,340 --> 02:06:54,500
often enough

1841
02:06:54,520 --> 02:06:57,190
so you should learn to be able to learn

1842
02:06:57,250 --> 02:07:02,000
patterns on that level fairly frequently and there has been work on doing this in

1843
02:07:02,000 --> 02:07:03,090
in reordering

1844
02:07:03,130 --> 02:07:07,570
before according you learn all the ordering patterns and then

1845
02:07:07,610 --> 02:07:09,320
we use search tool

1846
02:07:09,320 --> 02:07:11,440
following this we are independent

1847
02:07:11,440 --> 02:07:15,920
here the idea is that you do this also in in the factor model as

1848
02:07:15,920 --> 02:07:17,210
well so you have

1849
02:07:18,630 --> 02:07:19,750
you do you

1850
02:07:19,900 --> 02:07:21,880
first translate

1851
02:07:21,900 --> 02:07:24,460
and this part of speech patterns

1852
02:07:24,460 --> 02:07:28,520
and then you fill in the noun phrase afterwards

1853
02:07:28,550 --> 02:07:29,320
so you

1854
02:07:29,820 --> 02:07:32,000
you kind of have

1855
02:07:33,570 --> 02:07:40,090
kind of the true level of phrase segmentation of the input one is segmented it

1856
02:07:40,090 --> 02:07:43,420
in a certain way on the part of speech for the part of speech in

1857
02:07:43,420 --> 02:07:45,670
any segment and it little bit finer

1858
02:07:45,730 --> 02:07:49,630
on the actual surface form of words

1859
02:07:49,650 --> 02:07:51,270
but does

1860
02:07:51,270 --> 02:07:54,980
make things a bit more complicated with the decoding because the way it is presented

1861
02:07:54,980 --> 02:07:59,570
according as the do all these segmentation in the same granularity

1862
02:07:59,590 --> 02:08:03,920
so if you if you have longer patterns for some factors that for others

