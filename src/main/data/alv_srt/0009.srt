1
00:00:00,000 --> 00:00:06,040
portfolio optimization you will do optimal control you'll do machine learning

2
00:00:06,060 --> 00:00:10,800
and do i mean like actually do numerical stuff and would just be stupid programming

3
00:00:10,800 --> 00:00:11,780
it's going to be

4
00:00:11,790 --> 00:00:15,220
learning actually figuring out what's going on it will be obvious and

5
00:00:15,270 --> 00:00:17,570
stuff will happen it will be will be good stuff

6
00:00:17,590 --> 00:00:18,630
OK so

7
00:00:19,140 --> 00:00:23,000
i'm trying to think what else to say before we

8
00:00:23,050 --> 00:00:26,980
we started on the class may able set in context out how different from two

9
00:00:26,980 --> 00:00:29,630
sixty three and stuff like that would be a good thing in the was going

10
00:00:29,630 --> 00:00:35,210
to start and so the two sixty three which is a linear dynamical systems class

11
00:00:35,210 --> 00:00:39,600
that i would call that kind of basic material it's useful in terms of fields

12
00:00:39,600 --> 00:00:42,220
it's very it's what very widely used

13
00:00:42,240 --> 00:00:46,780
a lot of people know it so it's used in economics it's used in navigation

14
00:00:46,780 --> 00:00:51,780
in all of these areas control signal processing is basically all done this way

15
00:00:53,030 --> 00:00:55,050
a lot of stuff is done this way

16
00:00:55,060 --> 00:00:59,880
and it's a great first what you know first look

17
00:00:59,890 --> 00:01:05,230
at sort of how these mathematically sophisticated methods actually end up getting used

18
00:01:07,560 --> 00:01:12,910
sixty four is is revisited and actually it's quite it's quite different a couple of

19
00:01:12,910 --> 00:01:15,750
ways that the first ways it is you might even just say this is two

20
00:01:15,750 --> 00:01:19,220
sixty three beyond least squares so in two sixty three

21
00:01:19,230 --> 00:01:22,370
and by the way in other courses similar to that so when you first look

22
00:01:22,370 --> 00:01:27,650
at statistics in your first look course on statistical signal processing or whatever it's the

23
00:01:27,650 --> 00:01:30,400
same sort everything is gaussians

24
00:01:30,410 --> 00:01:36,100
you know all all distributions are and you know all all objective and constraints are

25
00:01:36,100 --> 00:01:39,200
quadratic and so on and you do this because

26
00:01:39,240 --> 00:01:43,440
this analytical formulas for an and then behind the analytical formulas we have very good

27
00:01:43,440 --> 00:01:45,470
computational tools to

28
00:01:46,040 --> 00:01:47,580
your computational teeth

29
00:01:47,600 --> 00:01:49,840
the the theory so

30
00:01:49,890 --> 00:01:53,260
here there going to be all it is going to be way way beyond that

31
00:01:53,260 --> 00:01:58,050
would have inequalities we're going to have all sorts of other interesting norms and other

32
00:01:58,050 --> 00:02:01,830
functions so we see it it is out this it's a much richer class of

33
00:02:01,830 --> 00:02:03,340
problems and is class

34
00:02:03,420 --> 00:02:08,580
these are probably much closer to a lot of real problems once where

35
00:02:08,630 --> 00:02:12,110
we can handle i mean you don't have the concept of an inequality into sixty

36
00:02:12,110 --> 00:02:17,040
three linear systems concept you just don't have that you have no way to deal

37
00:02:17,810 --> 00:02:20,720
noise with a lot class in distribution

38
00:02:20,730 --> 00:02:22,170
in your first class

39
00:02:22,180 --> 00:02:27,490
on statistical estimation for example so the things we look at things like that

40
00:02:27,500 --> 00:02:28,280
it's going to be

41
00:02:28,340 --> 00:02:31,200
i will be but i should also say that the number of people who know

42
00:02:31,210 --> 00:02:33,460
this material is

43
00:02:33,480 --> 00:02:35,070
relative to

44
00:02:35,080 --> 00:02:37,550
two sixty three material very small

45
00:02:37,570 --> 00:02:39,020
public couple thousand

46
00:02:39,040 --> 00:02:42,980
so is growing rapidly but is just a couple of thousand so that means you're

47
00:02:42,980 --> 00:02:44,310
going to move from

48
00:02:44,320 --> 00:02:47,560
you know material it is city three material that's scale of this kind from the

49
00:02:47,560 --> 00:02:51,960
sixties and seventies not a whole lot later on we maybe i mean somebody there

50
00:02:51,960 --> 00:02:54,510
but it's it's kind of it

51
00:02:54,540 --> 00:02:56,290
it's not stuff that's no

52
00:02:56,300 --> 00:02:59,990
this was in contrast to the boundary of knowledge so

53
00:03:00,010 --> 00:03:01,430
which makes it but

54
00:03:01,490 --> 00:03:02,830
so means

55
00:03:03,100 --> 00:03:06,650
who knows maybe maybe leading push the boundaries actually i hope you do because that's

56
00:03:06,650 --> 00:03:09,180
kind of the point class to train u

57
00:03:09,260 --> 00:03:13,000
due to go find your your corner of the boundary

58
00:03:13,020 --> 00:03:16,510
and then start and end start prospecting

59
00:03:16,530 --> 00:03:17,430
OK so

60
00:03:17,440 --> 00:03:19,800
i can't think of anything generic to say

61
00:03:19,810 --> 00:03:24,230
so maybe all to start the rest will actually start the class any questions

62
00:03:24,510 --> 00:03:31,110
that is the problem if you no way of knowing no problem

63
00:03:31,150 --> 00:03:32,890
but you take instead

64
00:03:33,010 --> 00:03:40,660
the example one of the americans you know that's you find that that's that's no

65
00:03:41,400 --> 00:03:47,300
OK so all i should also say this about about the something about the background

66
00:03:47,300 --> 00:03:50,670
i i should say well it's obviously classes

67
00:03:50,680 --> 00:03:54,690
list of electrical engineering and not actually sure why but anyway it seems to me

68
00:03:54,700 --> 00:03:55,900
to tradition

69
00:03:55,910 --> 00:03:59,690
last i checked that the department i'm in so it seems like it just seemed

70
00:03:59,690 --> 00:04:02,120
like a good idea all around

71
00:04:02,170 --> 00:04:06,580
i should say this you don't need to know anything about electrical in fact i'm

72
00:04:06,580 --> 00:04:09,970
going to talk about we'll talk about lots of applications throughout the core

73
00:04:11,790 --> 00:04:12,980
actually honestly

74
00:04:12,990 --> 00:04:16,760
probably for half i don't even know what i'm talking about so

75
00:04:16,770 --> 00:04:21,270
and that will surely happen now you'll be in some field out super-duper oversimplify it

76
00:04:21,270 --> 00:04:24,590
and you can you welcome to point that out of the classes if lectures boring

77
00:04:24,590 --> 00:04:26,490
at that point you can

78
00:04:26,610 --> 00:04:28,430
you shake your head and

79
00:04:28,440 --> 00:04:34,070
and say no it's totally off or whatever then i will have and continued

80
00:04:34,090 --> 00:04:39,000
so the point is you know i'll talk about circuit design

81
00:04:39,000 --> 00:04:41,030
in contrast to

82
00:04:41,040 --> 00:04:47,710
i disagree with the human rights report that disagree with the doctor x objective here

83
00:04:47,710 --> 00:04:49,710
it's how you're describing

84
00:04:49,720 --> 00:04:53,930
that you end up expressing the subjectivity

85
00:04:56,150 --> 00:04:59,920
you get this

86
00:05:00,100 --> 00:05:03,410
and this is just an example of the kind of objective things that we do

87
00:05:03,410 --> 00:05:08,920
markup it's not a major thing but the government added has amended this citizenship she

88
00:05:08,950 --> 00:05:14,050
backed so on issues presented objectively is true but we will note that the source

89
00:05:14,060 --> 00:05:20,250
of this information is the report for the reference of it

90
00:05:20,690 --> 00:05:25,580
OK now sources the term for better for worse that we use for

91
00:05:25,690 --> 00:05:28,210
so you are attributing the subjectivity two

92
00:05:28,260 --> 00:05:31,280
other people use the term opinion holder

93
00:05:31,290 --> 00:05:36,000
more general it's the experience in the private state anyway in this talk that's what

94
00:05:36,010 --> 00:05:39,550
source means

95
00:05:39,580 --> 00:05:43,710
and importantly sources are nested

96
00:05:43,770 --> 00:05:48,330
so let's consider this sentence the report is full of absurdities and in the next

97
00:05:50,800 --> 00:05:54,000
the entire sentence is attributed to the writer

98
00:05:54,010 --> 00:05:56,340
because he or she wrote this sentence

99
00:05:56,390 --> 00:05:59,560
and then you have

100
00:05:59,580 --> 00:06:01,320
the words

101
00:06:02,310 --> 00:06:06,420
attributes things by the writer to mean

102
00:06:06,430 --> 00:06:09,760
and notice that the full of absurdities there

103
00:06:09,800 --> 00:06:15,300
that subjectivity should be attributed to numa according to the writer

104
00:06:16,940 --> 00:06:17,940
and so

105
00:06:17,950 --> 00:06:23,070
that's the nesting source for this example

106
00:06:23,090 --> 00:06:26,050
and here we have a frame based representations

107
00:06:28,050 --> 00:06:32,470
our scheme of course you can translate into XML so

108
00:06:32,480 --> 00:06:38,140
we have an objective speech of banker is the part of the sentence that's relevant

109
00:06:38,150 --> 00:06:39,780
the text anchor

110
00:06:39,790 --> 00:06:44,570
so the first one is for the entire sentence and that's attributed to the writer

111
00:06:44,770 --> 00:06:46,140
know that the writer

112
00:06:46,150 --> 00:06:51,570
the who the sources is implicit the writer could said i write that

113
00:06:51,590 --> 00:06:56,270
and then it would have been explicit here its implicit in these are implicit ones

114
00:06:56,270 --> 00:07:01,200
are pretty common they're not all just the writer is said that something like that

115
00:07:01,220 --> 00:07:07,640
and at the outside level it's just objectively presented that someone said something on a

116
00:07:07,640 --> 00:07:09,380
particular day

117
00:07:09,420 --> 00:07:17,300
first we have the direct subjective annotation to the source here is numerator the intensity

118
00:07:17,300 --> 00:07:20,450
is high and expressive intensity is neutral

119
00:07:20,470 --> 00:07:22,120
so what does that mean

120
00:07:22,130 --> 00:07:24,310
for intensity

121
00:07:24,370 --> 00:07:28,050
that's the only when you interpret the overall private state

122
00:07:28,060 --> 00:07:30,230
you judge what's the overall and

123
00:07:30,250 --> 00:07:32,380
let's see

124
00:07:32,390 --> 00:07:34,650
and so it's full of absurdities

125
00:07:34,660 --> 00:07:39,050
the expression intensity marks how much

126
00:07:39,050 --> 00:07:44,470
text anger itself contributes to the subjectivity so in this case the words set itself

127
00:07:44,710 --> 00:07:47,920
isn't contributing anything so it's neutral

128
00:07:47,920 --> 00:07:55,840
additionally we mark the expressive subjective elements full of absurdities and it's attributed numerator with

129
00:07:55,840 --> 00:07:58,220
intensity high

130
00:07:58,230 --> 00:08:06,030
right so consider this one the US fears spillover

131
00:08:06,040 --> 00:08:11,880
name professor of foreign affairs at central university for nationalities to do that k

132
00:08:11,920 --> 00:08:18,930
right so that it is the true is writer name and here you have us

133
00:08:18,950 --> 00:08:21,180
numerator from the inside out

134
00:08:21,920 --> 00:08:26,600
so this is the immediate source of you as its USS spheres

135
00:08:26,660 --> 00:08:28,530
that year

136
00:08:28,560 --> 00:08:32,850
is according to an email because it's in what name aside and then of course

137
00:08:32,850 --> 00:08:39,020
the whole thing is attributed to the writer because the writer of the sentence

138
00:08:39,060 --> 00:08:41,780
and here's what you get for this sentence

139
00:08:42,820 --> 00:08:44,920
at the outside level of the writer

140
00:08:44,970 --> 00:08:49,600
you just have an objective speeches because that level

141
00:08:50,480 --> 00:08:55,180
somebody said something and there a professor of foreign affairs at the university it's being

142
00:08:55,180 --> 00:09:00,680
presented as true and there isn't any evaluation or motion or anything of the writer

143
00:09:00,680 --> 00:09:02,530
being expressed

144
00:09:02,570 --> 00:09:07,230
interestingly this here is also an objective speech event

145
00:09:07,290 --> 00:09:12,500
because name it just said that the US fears something it depends on the context

146
00:09:12,500 --> 00:09:19,470
of the article all of this annotation is contextual bandit leaders are supposed to reinterpreted

147
00:09:19,470 --> 00:09:22,710
as it is in that context and i

148
00:09:23,090 --> 00:09:28,680
and judge whether it's being just objectively presented are not in this article it's

149
00:09:28,720 --> 00:09:31,950
it wasn't it wasn't controversial

150
00:09:32,000 --> 00:09:33,020
or anything

151
00:09:33,030 --> 00:09:36,420
so here according to name of the US fears spillover

152
00:09:36,470 --> 00:09:40,570
and then finally we need real down to the theories you have the direct subjective

153
00:09:40,570 --> 00:09:43,080
and here you have subjectivity

154
00:09:43,100 --> 00:09:49,180
you may not appear at all i mean that may not even set

155
00:09:49,200 --> 00:09:55,600
the report analysis to a couple more examples just to help you understand more the

156
00:09:55,600 --> 00:09:59,310
report been strongly criticizing condemned by many countries

157
00:09:59,330 --> 00:10:03,960
so the entire sentence again is objective speech events

158
00:10:04,050 --> 00:10:11,230
and i strongly criticizing condemned is attributed to many countries writer

159
00:10:11,250 --> 00:10:14,820
this one is interesting

160
00:10:14,840 --> 00:10:19,960
as usual the US state department published its annual report on human rights practices in

161
00:10:19,960 --> 00:10:21,810
world countries last monday

162
00:10:21,820 --> 00:10:28,570
and as usual the portion about china contains little to many absurdities exaggerations fabrications so

163
00:10:28,570 --> 00:10:32,880
this is from translation of an article for the chinese press

164
00:10:32,960 --> 00:10:39,150
so here's what here here's here's what you get for this one the first one

165
00:10:39,170 --> 00:10:45,020
just that sentence it's it's just objective there's any subjectivity in that one

166
00:10:45,420 --> 00:10:52,560
so the entire first sense is injective speech events and then the same with the

167
00:10:52,560 --> 00:10:57,680
second and sorry for the second one is the second sentence

168
00:10:57,690 --> 00:11:04,580
that's obviously subjectivity being expressed by the writer him or herself

169
00:11:05,200 --> 00:11:10,460
because the second sentence is saying it's a little too so many absurdities and exaggerations

170
00:11:10,460 --> 00:11:11,600
OK so

171
00:11:11,700 --> 00:11:15,940
one you could have you could simply take the habitation data for the reference we

172
00:11:16,110 --> 00:11:21,460
have sufficient for that you could type used that to define the features used to

173
00:11:21,600 --> 00:11:23,550
be called up tree

174
00:11:23,560 --> 00:11:27,040
we can do this for

175
00:11:27,420 --> 00:11:30,350
all nineteen one reference you could

176
00:11:31,720 --> 00:11:37,610
by that time we haven't used the information that states usually according to that means

177
00:11:37,710 --> 00:11:42,460
snake at in its name appears in one one

178
00:11:42,480 --> 00:11:46,640
then it also it's likely to appear in another

179
00:11:46,670 --> 00:11:50,450
so there are relations between you could have some nodes are related to each other

180
00:11:50,720 --> 00:11:56,650
and the more likely it is that if you get one of the few

181
00:11:56,780 --> 00:12:02,380
that's why we had a kind of second layer which was combining all the scores

182
00:12:02,590 --> 00:12:07,100
of the first layer using these predictions

183
00:12:07,410 --> 00:12:13,020
the input features as well so this is improved predictions quite a bit

184
00:12:14,640 --> 00:12:16,810
and here is what we want are

185
00:12:17,100 --> 00:12:20,870
what you see here is the

186
00:12:20,890 --> 00:12:25,920
false discovery on the x axis and the mean recovery over the twenty of the

187
00:12:25,920 --> 00:12:28,760
nineteen sessions what you type

188
00:12:29,010 --> 00:12:32,450
four different choices of special

189
00:12:32,810 --> 00:12:38,610
and we looked at according regions at UTR intron entrance into the genital region

190
00:12:40,530 --> 00:12:45,390
actually depending on special it what the false discovery rate of two percent

191
00:12:45,450 --> 00:12:48,770
it like i said

192
00:12:50,090 --> 00:12:53,480
it can be thirty five percent of all snakes

193
00:12:53,480 --> 00:12:55,530
you can identify it

194
00:12:55,580 --> 00:13:00,310
rather low of discovery rate of discovery rate is like

195
00:13:00,350 --> 00:13:03,370
the fraction of positive

196
00:13:03,380 --> 00:13:05,040
which are predicted to be positive

197
00:13:05,050 --> 00:13:07,100
but are not about

198
00:13:07,120 --> 00:13:10,460
ensure that

199
00:13:10,470 --> 00:13:16,830
OK so in these starts here this was predicted by other company but

200
00:13:16,880 --> 00:13:23,030
they are better on the core region about that much worse on the intergenic and

201
00:13:23,030 --> 00:13:25,940
in trying to regions

202
00:13:26,190 --> 00:13:31,150
OK so

203
00:13:31,190 --> 00:13:35,810
it turns out that those ips which are close to each other are the most

204
00:13:35,810 --> 00:13:36,810
problematic one

205
00:13:36,830 --> 00:13:41,830
what we have here is what what got here is the accuracy what the recovery

206
00:13:42,090 --> 00:13:49,190
and explores discovery surgery recovery force states which are part of way we're close to

207
00:13:49,190 --> 00:13:52,450
another city or other one of twenty one

208
00:13:52,530 --> 00:13:57,310
so those lips are very far away from each other and the

209
00:13:57,340 --> 00:14:01,750
curiously here with the recovery is very high like fifty percent but as soon as

210
00:14:01,750 --> 00:14:07,040
we get closer to the recovery is going very much

211
00:14:07,050 --> 00:14:10,590
so if the distance is only one six i

212
00:14:10,620 --> 00:14:13,530
then the picture probably looks like this that you are going to be able to

213
00:14:15,580 --> 00:14:19,720
and compared the approach with machine learning approach

214
00:14:19,720 --> 00:14:24,690
and what we have seen it is much better in identifying synapses which are close

215
00:14:24,690 --> 00:14:25,790
to each other

216
00:14:25,980 --> 00:14:30,860
so it wouldn't that region but not good in this region so it's nice very

217
00:14:30,860 --> 00:14:35,520
complementing the predictions on the others

218
00:14:35,820 --> 00:14:39,770
but the rest was of course can be water

219
00:14:39,820 --> 00:14:43,670
first and this be used

220
00:14:43,670 --> 00:14:45,050
label sequence learning

221
00:14:45,070 --> 00:14:50,430
so the idea is that you have maybe a genomic region we have some known

222
00:14:50,430 --> 00:14:53,250
labels we know that it's you

223
00:14:53,270 --> 00:14:55,840
insertions here maybe deletion

224
00:14:55,840 --> 00:14:57,230
and we

225
00:14:57,250 --> 00:15:03,070
come up with a segmentation of the sequence so we define the region as i

226
00:15:03,070 --> 00:15:06,630
wouldn't want it just density of

227
00:15:06,650 --> 00:15:08,170
one other forty

228
00:15:08,190 --> 00:15:11,790
and portable businesses are already that was

229
00:15:11,810 --> 00:15:17,550
so this is like the segmentation which would like to be defined based on the

230
00:15:17,550 --> 00:15:19,480
is written

231
00:15:19,540 --> 00:15:25,100
then we have a simple data model which we would like to match to this

232
00:15:25,100 --> 00:15:28,040
sequence positions and so it can be either could

233
00:15:28,050 --> 00:15:30,130
what we want

234
00:15:30,500 --> 00:15:37,840
and then this position states between which model like the UK of the habitation

235
00:15:38,000 --> 00:15:41,960
habitation entity

236
00:15:41,980 --> 00:15:44,960
so this model so it should i mean is

237
00:15:44,980 --> 00:15:51,810
the state very simple and reason essentially sign every state

238
00:15:51,860 --> 00:15:54,650
two label for every position that you know

239
00:15:55,360 --> 00:15:58,130
but not the markov the model

240
00:15:58,150 --> 00:16:03,070
now we can use just the label sequence learning approach which shows

241
00:16:03,090 --> 00:16:04,980
it's just another example

242
00:16:05,000 --> 00:16:11,420
so this region where we have no labels using the target page which we derived

243
00:16:11,440 --> 00:16:12,940
from the low note label

244
00:16:12,940 --> 00:16:16,960
these states which are detected by calling by

245
00:16:16,960 --> 00:16:23,070
the prediction of those isolated can be predicted others are isolated cannot be pretty easily

246
00:16:23,520 --> 00:16:27,650
this is the habitation signal and i would like to predict based on the hybridization

247
00:16:27,650 --> 00:16:31,310
signal where the boundaries where the polymorphic regions

248
00:16:31,320 --> 00:16:34,230
critical to the more recent

249
00:16:34,230 --> 00:16:37,770
this could be point what it

250
00:16:38,650 --> 00:16:40,070
prediction that

251
00:16:40,090 --> 00:16:42,670
we want you to stop here and here

252
00:16:42,690 --> 00:16:44,860
almost correct question is how

253
00:16:44,900 --> 00:16:47,170
how do practice here

254
00:16:48,210 --> 00:16:54,110
we defined kind of juicy measure and how to break overlapping by seventy five percent

255
00:16:54,110 --> 00:16:56,250
of the

256
00:16:56,250 --> 00:17:01,500
so in this case it would be great because and

257
00:17:02,070 --> 00:17:07,980
can identify fifty five percent of all we want to the same the specificity of

258
00:17:07,980 --> 00:17:13,770
nine so among all predictions nine or ten right

259
00:17:13,880 --> 00:17:18,480
behind about half of all three one

260
00:17:18,710 --> 00:17:28,150
so previously there was only heuristic method which had a much lower sedative

261
00:17:28,210 --> 00:17:37,190
i think this is a bit

262
00:17:38,710 --> 00:17:40,500
very few people

263
00:17:43,150 --> 00:17:46,070
now the question is

264
00:17:46,130 --> 00:17:50,690
we have this name calling i would have this prediction are with what we want

265
00:17:50,690 --> 00:17:56,050
the vector and the semantics that we introduced before in the compression phase and this

266
00:17:56,050 --> 00:17:58,810
is the idea that we call the

267
00:17:58,810 --> 00:18:04,970
structured output discriminant analysis since it will be seen as in an extension of the

268
00:18:04,980 --> 00:18:07,110
linear discriminant analysis two

269
00:18:07,170 --> 00:18:12,330
this setting of structured output problems

270
00:18:12,350 --> 00:18:19,150
so the problem that we have to solve this one maximizing

271
00:18:19,200 --> 00:18:26,140
this the ratio of this is an optimisation a convex optimisation problem perhaps you will

272
00:18:26,140 --> 00:18:32,010
see that year and to solve it the first thing for is we should have

273
00:18:32,010 --> 00:18:37,200
said that if the matrix is not positive semi definite we should introduce the article

274
00:18:37,200 --> 00:18:38,670
in relation to

275
00:18:38,690 --> 00:18:41,300
and to solve that

276
00:18:41,430 --> 00:18:48,390
the problem and out we can solve the problem by simple matrix inversion

277
00:18:48,400 --> 00:18:54,890
but in theory inventing these metrics can be

278
00:18:54,890 --> 00:18:59,580
can have high computational cost but we

279
00:19:00,230 --> 00:19:09,350
we use the fast conjugate gradient methods and tools to speed up the optimisation face

280
00:19:09,390 --> 00:19:16,400
i would just to give an idea about to these two radical stuff that perhaps

281
00:19:16,400 --> 00:19:23,900
i do wonder what warren and we developed rademacher bound that show that the learning

282
00:19:23,900 --> 00:19:31,440
with our upper bounds is effectively achieved and we wondered that my holds

283
00:19:31,460 --> 00:19:32,420
even when

284
00:19:32,430 --> 00:19:35,400
b and c are estimated by sampling

285
00:19:35,470 --> 00:19:41,180
and what want to see in in this way we can really spread out

286
00:19:41,200 --> 00:19:43,930
the motivation there

287
00:19:43,940 --> 00:19:49,100
our algorithm so we can see the two directions of sampling the first one is

288
00:19:49,340 --> 00:19:54,350
that for each input output pair in the training set we consider only a limited

289
00:19:54,350 --> 00:19:59,480
number of incorrect outputs to estimate the

290
00:19:59,490 --> 00:20:03,900
in metastasis by sampling and also we consider only if you need a number of

291
00:20:03,900 --> 00:20:06,970
input output space in the training set

292
00:20:08,630 --> 00:20:13,300
in the bound was show in the next slide that basically we prove

293
00:20:13,310 --> 00:20:20,310
that year the empirical expectation of the estimated loss estimated the scenes we are computing

294
00:20:20,310 --> 00:20:26,830
b and c by sampling is a good approximate upper bound for the expected loss

295
00:20:26,960 --> 00:20:29,740
and we should also remember that

296
00:20:29,760 --> 00:20:35,090
the upper bound to that we defined before is an upper bound on the ranking

297
00:20:35,090 --> 00:20:36,060
loss so

298
00:20:36,060 --> 00:20:42,040
effectively what we're doing is we're minimising

299
00:20:42,060 --> 00:20:44,550
this presentation of the ranking loss

300
00:20:46,830 --> 00:20:55,360
this is about and just to give you some insight the five times here are

301
00:20:55,400 --> 00:21:02,140
and then the empirical estimate of the expected a lot of them such a loss

302
00:21:02,310 --> 00:21:11,190
the time with something during the metric quantities that decrees that was the inverse of

303
00:21:11,190 --> 00:21:13,990
the square root of n and have

304
00:21:14,000 --> 00:21:17,460
and though the two terms

305
00:21:17,470 --> 00:21:23,370
and here is the cost again goals as the inverse of the square root of

306
00:21:23,370 --> 00:21:26,140
n and help so that means that

307
00:21:26,140 --> 00:21:30,330
but they quantity on the right side go to zero when the number of samples

308
00:21:30,330 --> 00:21:32,790
is increased

309
00:21:32,800 --> 00:21:39,300
another approach that we can see a

310
00:21:39,430 --> 00:21:45,090
by the end of the previous approach is that it in that we call the

311
00:21:45,090 --> 00:21:47,060
true core approach

312
00:21:47,070 --> 00:21:55,790
are since we asked ourselves for other we define the discovery of training set another

313
00:21:55,790 --> 00:22:01,080
possible approach would be to consider that the contributor of feature

314
00:22:01,440 --> 00:22:06,590
input output pairs is independent from the contributor of the order so it means that

315
00:22:06,590 --> 00:22:14,240
the mean and they call and the right answer computed for each piece can be

316
00:22:14,240 --> 00:22:18,970
summed in that way we we should assume to have an estimate of the

317
00:22:18,990 --> 00:22:24,650
the mean of the distribution of the four on four of the input output pair

318
00:22:24,650 --> 00:22:26,360
of the training set

319
00:22:26,360 --> 00:22:28,620
and as well for the covariance

320
00:22:28,640 --> 00:22:32,470
this is what we call the this core approach the

321
00:22:32,590 --> 00:22:37,500
problem define problem is this one as you can see the structure of the problem

322
00:22:38,200 --> 00:22:40,560
similar to the what one in

323
00:22:40,560 --> 00:22:46,050
four decide where we again have a convex optimisation problem can with that can be

324
00:22:46,050 --> 00:22:49,870
solved by simple matrix inversion

325
00:22:49,890 --> 00:22:57,640
and basically they are assigned to that i will show you later are quite similar

326
00:22:57,640 --> 00:23:02,230
in both cases is if you work it out of this form where you can

327
00:23:02,230 --> 00:23:08,230
see that we are minimizing kind of upper bound on the ranking loss again

328
00:23:08,260 --> 00:23:13,960
so that's why perhaps that you might very similar but for this one that we

329
00:23:15,280 --> 00:23:24,220
try different strategy because basically when we maximize this myself of the linear constraints of

330
00:23:24,220 --> 00:23:25,910
here is

331
00:23:25,950 --> 00:23:29,560
the same kind of plot we had before and then this is the role over

332
00:23:32,450 --> 00:23:36,290
which is one of our strategy it had the bottom of the curve

333
00:23:36,310 --> 00:23:38,990
so we we may very well be tree

334
00:23:39,040 --> 00:23:42,040
now he are compared

335
00:23:48,740 --> 00:23:51,370
here again is

336
00:23:51,410 --> 00:23:53,700
this curve that was

337
00:23:53,700 --> 00:23:58,350
designed by doing the reverting using dynamic programming bestfixed

338
00:23:58,370 --> 00:23:59,740
this is sort of

339
00:23:59,790 --> 00:24:02,240
the best hit rate you can achieve when you

340
00:24:02,280 --> 00:24:07,350
use the union of all twelve strategies

341
00:24:08,660 --> 00:24:10,350
so this is the one

342
00:24:15,290 --> 00:24:17,760
tries to

343
00:24:18,990 --> 00:24:23,560
there the real cache with the idea of cash

344
00:24:23,580 --> 00:24:27,680
it has very good performance but

345
00:24:27,700 --> 00:24:32,330
in the sense that it has a small miss rate but

346
00:24:33,410 --> 00:24:35,220
it has a problem

347
00:24:35,260 --> 00:24:37,180
and the problem is

348
00:24:37,220 --> 00:24:40,950
it has a lot of factors very far over in this direction

349
00:24:40,970 --> 00:24:44,450
which is a lot of i you want to stay below this line to the

350
00:24:44,450 --> 00:24:45,950
left of these lines

351
00:24:46,010 --> 00:24:48,930
but if you do demand roll over

352
00:24:48,970 --> 00:24:50,850
which is no prefetching

353
00:24:51,780 --> 00:24:53,260
you some kind

354
00:24:53,260 --> 00:24:57,490
close to the best fixed but if you do a little bit of refactoring

355
00:24:57,510 --> 00:25:01,780
you beat bestfixed on this dataset

356
00:25:01,790 --> 00:25:03,810
another dataset

357
00:25:03,830 --> 00:25:08,760
there we were very close to this basically fitting curve

358
00:25:08,780 --> 00:25:10,580
and in some cases even beat

359
00:25:10,600 --> 00:25:12,640
this curve

360
00:25:13,640 --> 00:25:18,990
we feel like we decided very stringent compared we actually needed actually i do not

361
00:25:20,180 --> 00:25:23,810
i don't know how to design a better offline strategy

362
00:25:23,810 --> 00:25:30,430
except well except the one that would look at the future and that some

363
00:25:32,640 --> 00:25:37,740
so this was a very it's pretty stringent strategy

364
00:25:37,760 --> 00:25:41,640
OK demand rollover which into any actually always as good as almost as good as

365
00:25:41,640 --> 00:25:44,760
the best fixed small amounts of wheat

366
00:25:44,790 --> 00:25:47,950
prefetching always beat bestfixed

367
00:25:51,120 --> 00:25:52,910
remember best fixed

368
00:25:52,950 --> 00:25:56,680
i was the one up here

369
00:25:56,700 --> 00:26:02,040
and we have fifteen to twenty two percent fewer missus than bestfixed

370
00:26:02,060 --> 00:26:05,010
and we always be LOU which is the defacto

371
00:26:05,020 --> 00:26:10,040
that's the one that is the most

372
00:26:10,060 --> 00:26:15,930
OK we can be the best three curves and the i behaviour is also pretty

373
00:26:19,510 --> 00:26:24,370
operating system in many of these parameters tweaking problems

374
00:26:24,410 --> 00:26:28,580
the suitable for online learning

375
00:26:38,450 --> 00:26:40,970
the algorithms

376
00:26:40,990 --> 00:26:46,490
from that area as heuristic to sign good online algorithms for practical problems i'm going

377
00:26:46,490 --> 00:26:48,680
to do this now for disk spin down and

378
00:26:48,700 --> 00:26:53,970
skip lists are talking a little bit about these other applications

379
00:26:54,010 --> 00:26:56,790
so any questions that

380
00:26:58,200 --> 00:26:59,870
regarding any part of the county

381
00:27:03,540 --> 00:27:06,870
so again

382
00:27:06,890 --> 00:27:11,040
what we did

383
00:27:14,370 --> 00:27:21,620
caching application was

384
00:27:21,640 --> 00:27:26,600
here lot miss of different policies

385
00:27:26,600 --> 00:27:30,310
what you can and he is the best guys

386
00:27:30,310 --> 00:27:32,040
over time

387
00:27:34,850 --> 00:27:39,510
i have decided compared and again i want to compare against sort of the best

388
00:27:39,510 --> 00:27:41,970
shifting policy

389
00:27:41,990 --> 00:27:43,870
and they had to

390
00:27:45,990 --> 00:27:50,410
online algorithms that could actually handle this case

391
00:27:50,450 --> 00:27:53,760
we're the best expert is shifting over time

392
00:27:53,780 --> 00:27:58,970
i couldn't it wasn't enough just to do the loss update that actually doesn't work

393
00:27:58,990 --> 00:28:09,760
you need to do invented new kind of update and he is again the updates

394
00:28:09,780 --> 00:28:10,740
you do

395
00:28:10,740 --> 00:28:14,180
either minus loss update

396
00:28:14,200 --> 00:28:17,160
and then you do

397
00:28:17,180 --> 00:28:20,260
no weight is one minus all the time

398
00:28:20,260 --> 00:28:25,080
well you just got a lots of times the past average

399
00:28:25,100 --> 00:28:27,310
that's what you should remember

400
00:28:27,330 --> 00:28:30,470
the combined update works

401
00:28:30,640 --> 00:28:34,470
to update your beliefs on

402
00:28:34,520 --> 00:28:36,760
a bunch of experts

403
00:28:36,780 --> 00:28:40,470
special case of this is if the if the loss

404
00:28:40,540 --> 00:28:43,990
happens to be long-lasting this would be bayes rule

405
00:28:43,990 --> 00:28:45,140
and this

406
00:28:45,160 --> 00:28:48,450
when you share to the uniform weight but if this average would be the uniform

407
00:28:48,450 --> 00:28:52,310
with that's kind of robust base and you can actually motivated and it's

408
00:28:52,700 --> 00:28:57,220
the probabilistic interpretation when new shared the past average

409
00:28:58,160 --> 00:29:00,060
could not find

410
00:29:00,540 --> 00:29:03,520
patient interpretation of that update

411
00:29:03,580 --> 00:29:08,600
it works provably well but i couldn't find innovation interpretation

412
00:29:08,640 --> 00:29:16,700
this is kind of an interesting so when you pick makes in the past averaged

413
00:29:16,810 --> 00:29:21,010
what happens is the weights of all the experts that were good in the past

414
00:29:21,010 --> 00:29:24,410
they still glowing that they don't go all the way down to zero they still

415
00:29:24,430 --> 00:29:25,990
global little bit

416
00:29:26,010 --> 00:29:29,600
and then when you come back need the same experts again

417
00:29:29,600 --> 00:29:32,640
they pulled up very fast

418
00:29:32,660 --> 00:29:36,290
and i'm very much fascinated by

419
00:29:36,370 --> 00:29:38,410
this glowing saying

420
00:29:38,430 --> 00:29:39,910
so you

421
00:29:40,620 --> 00:29:44,220
i want to learn something and you sort of

422
00:29:44,260 --> 00:29:47,810
and you need to do this in online learning you want to learn something

423
00:29:47,870 --> 00:29:51,550
but you what anything that has been good in the past you sort of want

424
00:29:51,550 --> 00:29:54,180
to memorize and remember a little bit

425
00:29:54,220 --> 00:29:57,580
because most likely your data is going to shift back

426
00:29:57,620 --> 00:29:58,680
to the same

427
00:29:58,700 --> 00:30:00,620
type of data

428
00:30:00,680 --> 00:30:05,240
and then you want to recover fast so this is sort of the first algorithm

429
00:30:07,280 --> 00:30:10,510
was good at shifting

430
00:30:11,260 --> 00:30:16,290
it was even better at shifting back to something

431
00:30:16,310 --> 00:30:19,180
it has been good at before

432
00:30:19,180 --> 00:30:22,680
and i want wanted do all kinds of algorithms that have this kind of property

433
00:30:22,680 --> 00:30:27,040
OK i will give

434
00:30:27,050 --> 00:30:28,520
a brief

435
00:30:28,540 --> 00:30:29,960
actually less than

436
00:30:29,970 --> 00:30:32,140
there's an introduction or overview

437
00:30:33,130 --> 00:30:34,890
statistical machine learning

438
00:30:34,940 --> 00:30:37,390
what the other quarters

439
00:30:37,720 --> 00:30:40,630
will cover in much more depth and detail

440
00:30:40,640 --> 00:30:43,250
so it

441
00:30:43,270 --> 00:30:47,360
for those who have no idea about machine learning to get some big picture but

442
00:30:47,360 --> 00:30:50,570
probably understand only ten percent of the stuff on it

443
00:30:50,610 --> 00:30:55,260
for those who already took a course in machine learning it maybe serves refreshments

444
00:30:55,310 --> 00:31:00,520
OK so

445
00:31:00,580 --> 00:31:03,130
the first ever given me overview

446
00:31:03,170 --> 00:31:06,450
then linear methods for regression

447
00:31:06,510 --> 00:31:08,090
that sounds pretty boring

448
00:31:08,090 --> 00:31:12,660
good old fashion statistics but if you don't master that forget about the rest

449
00:31:12,660 --> 00:31:15,480
and the more powerful than you think of

450
00:31:15,980 --> 00:31:18,160
non linear methods for regression

451
00:31:20,190 --> 00:31:22,130
if you have complex problems

452
00:31:22,150 --> 00:31:23,330
like in real life

453
00:31:23,340 --> 00:31:24,230
you need to

454
00:31:24,260 --> 00:31:27,380
select models and process models

455
00:31:27,490 --> 00:31:30,630
you have to deal with large problems

456
00:31:30,680 --> 00:31:35,740
are in so far as small as i i these independent identically distributed data

457
00:31:36,240 --> 00:31:38,630
there are various other kinds of

458
00:31:38,820 --> 00:31:42,600
and supervised learning this of unsupervised learning

459
00:31:43,850 --> 00:31:48,710
learning for rick interactive setting its reinforcement learning

460
00:31:48,730 --> 00:31:52,630
explain what that is and then to the summary

461
00:31:55,490 --> 00:31:59,120
first we could ask i mean why do we need machine learning why do we

462
00:31:59,120 --> 00:32:00,790
need to learn

463
00:32:00,900 --> 00:32:02,380
from related fields

464
00:32:02,430 --> 00:32:04,620
applications of machine learning

465
00:32:04,650 --> 00:32:07,800
the three short months we talk to me

466
00:32:07,850 --> 00:32:12,120
supervised unsupervised reinforcement learning from other to is

467
00:32:12,120 --> 00:32:14,770
mini introduction to probabilities

468
00:32:18,870 --> 00:32:22,680
most of the things that you on this slide this in in the evening

469
00:32:22,710 --> 00:32:26,400
it's a wiki page about probability is if you don't know this stuff

470
00:32:27,200 --> 00:32:29,620
OK so what is machine learning

471
00:32:29,900 --> 00:32:34,780
so you could define it as machine learning is concerned with the development of algorithms

472
00:32:34,780 --> 00:32:37,210
and techniques that allow computers to learn

473
00:32:37,220 --> 00:32:39,990
OK that's pretty obvious this machine and flow

474
00:32:40,020 --> 00:32:43,300
so it should be about machine learning

475
00:32:43,490 --> 00:32:50,000
in this context it's the process of gaining understanding by constructing models of observed data

476
00:32:50,000 --> 00:32:52,500
with the intention to use them for

477
00:32:54,870 --> 00:32:59,220
the best related fields is artificial intelligence

478
00:32:59,240 --> 00:33:01,970
some people think that machine learning is a part of the i

479
00:33:02,020 --> 00:33:04,240
some people think it's different

480
00:33:05,780 --> 00:33:08,340
i want to develop smart algorithms

481
00:33:08,390 --> 00:33:09,740
this statistics

482
00:33:09,750 --> 00:33:14,390
something that machine learning stuff just some obscure branch of statistics

483
00:33:14,400 --> 00:33:15,800
o thing

484
00:33:15,810 --> 00:33:18,770
i believe it's quite different from statistics

485
00:33:19,020 --> 00:33:23,170
i think it's the same

486
00:33:23,210 --> 00:33:26,810
but if you look at the journals and publications is hard to get the statistics

487
00:33:26,810 --> 00:33:30,830
from the machine learning journal and machine learning paper statistics journal so there must be

488
00:33:30,830 --> 00:33:35,640
some difference and machine learning is more concerned with either of them

489
00:33:37,680 --> 00:33:39,560
and statistics more of it

490
00:33:39,590 --> 00:33:44,680
best estimators and so on there is a large degree of overlap

491
00:33:44,740 --> 00:33:49,020
OK data data mining a lot of data we need to process it somehow and

492
00:33:49,020 --> 00:33:53,490
of course there is a continuum from two machine learning

493
00:33:53,500 --> 00:33:58,270
and of course computer science because machine learning want to develop algorithms and computer science

494
00:33:58,270 --> 00:33:59,400
a lot of

495
00:33:59,400 --> 00:34:05,440
nice algorithms efficient algorithms for problems have been developed and then you can use in

496
00:34:05,440 --> 00:34:07,740
machine learning

497
00:34:07,780 --> 00:34:08,690
OK so

498
00:34:08,710 --> 00:34:10,310
why should we learn

499
00:34:10,340 --> 00:34:14,580
the first i mean there's something they don't need to learn for instance to calculate

500
00:34:17,120 --> 00:34:20,840
i mean if you can add numbers many multiply you know the rules you just

501
00:34:20,840 --> 00:34:23,830
do it you don't need to learn from

502
00:34:24,780 --> 00:34:26,670
payrolls statements

503
00:34:26,710 --> 00:34:28,560
so learning is used when

504
00:34:28,610 --> 00:34:30,180
human expertise

505
00:34:30,180 --> 00:34:32,170
it doesn't exist

506
00:34:32,180 --> 00:34:33,270
i i mean

507
00:34:33,340 --> 00:34:36,930
robert navigating en masse

508
00:34:36,990 --> 00:34:40,850
humans are unable to explain their expertise

509
00:34:40,870 --> 00:34:43,230
like in speech recognition that all

510
00:34:43,490 --> 00:34:45,780
pretty good at recognizing

511
00:34:46,700 --> 00:34:47,930
but it's

512
00:34:47,950 --> 00:34:49,970
very hard to

513
00:34:49,990 --> 00:34:52,510
right former model explicit model

514
00:34:52,510 --> 00:34:56,000
so that's what we're

515
00:34:58,190 --> 00:35:02,970
which is kind of funny because i knew to manchester

516
00:35:03,060 --> 00:35:09,360
who was just peter might as well just this one thing that are

517
00:35:09,750 --> 00:35:17,010
wilson the risk interest is known for is being a bit right

518
00:35:17,120 --> 00:35:22,250
but i believe this thing but

519
00:35:22,330 --> 00:35:25,980
well want to see what happens

520
00:35:25,990 --> 00:35:29,300
concepts and models

521
00:35:30,520 --> 00:35:38,740
no really really good that we start even for people who are particularly for large

522
00:35:38,960 --> 00:35:41,370
one the of the

523
00:35:48,720 --> 00:35:57,140
actually all of these are our what's going on but we were

524
00:35:58,840 --> 00:36:01,820
what's going on there

525
00:36:01,840 --> 00:36:05,300
it's more it's going to be an

526
00:36:06,310 --> 00:36:08,240
it's hard work

527
00:36:08,550 --> 00:36:10,900
so for example

528
00:36:15,090 --> 00:36:18,060
i understand what's going on to say

529
00:36:18,200 --> 00:36:20,260
well we

530
00:36:21,670 --> 00:36:25,630
i mean this is this is getting

531
00:36:25,650 --> 00:36:31,170
comfortable with just over an ontology looking

532
00:36:37,960 --> 00:36:40,570
one of the things that makes it really easy to use

533
00:36:40,590 --> 00:36:43,050
actually something actually useful

534
00:36:43,060 --> 00:36:46,210
how do you know where the great

535
00:36:47,710 --> 00:36:53,040
the language is that people publish columns

536
00:36:54,360 --> 00:36:56,010
in the

537
00:36:57,780 --> 00:36:59,530
the other dollars

538
00:37:04,250 --> 00:37:07,490
you might presented in a public forum

539
00:37:08,350 --> 00:37:12,320
now we know that

540
00:37:12,480 --> 00:37:14,010
to years

541
00:37:14,030 --> 00:37:18,010
and one of the canterbury's we can do is learning from them

542
00:37:18,030 --> 00:37:21,020
so for someone to give a little bit background

543
00:37:21,030 --> 00:37:24,220
ontologies are very high-level going to try

544
00:37:24,320 --> 00:37:26,670
despite all the

545
00:37:26,690 --> 00:37:28,820
will land

546
00:37:28,860 --> 00:37:31,500
so i think the whole what you're

547
00:37:31,520 --> 00:37:34,520
that by the way they are

548
00:37:34,530 --> 00:37:35,570
OK so so

549
00:37:35,580 --> 00:37:37,980
this is sort of very generic

550
00:37:38,110 --> 00:37:39,180
he was

551
00:37:39,200 --> 00:37:41,230
all is

552
00:37:41,250 --> 00:37:46,460
well it is so small you want see the subject

553
00:37:46,480 --> 00:37:48,220
usually conceptual

554
00:37:54,780 --> 00:38:02,340
on what we're doing is an ontology as they are computer science is that there

555
00:38:02,360 --> 00:38:05,010
computational artifacts

556
00:38:07,290 --> 00:38:09,740
you know

557
00:38:09,760 --> 00:38:14,300
all four were sometimes we can hide this

558
00:38:14,320 --> 00:38:19,600
just like we can programs you know you can if you're using its own programs

559
00:38:21,880 --> 00:38:23,550
the world

560
00:38:23,570 --> 00:38:25,360
i don't think about this

561
00:38:25,440 --> 00:38:28,300
here but really

562
00:38:28,320 --> 00:38:32,890
so three percent computational

563
00:38:35,110 --> 00:38:36,010
is there

564
00:38:38,430 --> 00:38:41,870
or you get something that machine

565
00:38:41,890 --> 00:38:44,400
when need to

566
00:38:44,410 --> 00:38:48,830
during world program is that

567
00:38:48,880 --> 00:38:55,570
right round something on this is also true for ontologies lot of things that deal

568
00:38:56,360 --> 00:39:02,810
on a general way that counts were not part of the applications like

569
00:39:06,770 --> 00:39:13,040
so that they usually do some form of inference help

570
00:39:13,220 --> 00:39:14,850
so it's very important

571
00:39:14,870 --> 00:39:16,160
it's one of these

572
00:39:16,220 --> 00:39:19,940
these things are

573
00:39:20,140 --> 00:39:22,220
because the

574
00:39:22,240 --> 00:39:27,060
our world is generally ontologies are directed more or less

575
00:39:29,770 --> 00:39:31,260
pieces of the world

576
00:39:32,150 --> 00:39:36,740
other ways you can use them to do what is sometimes called structural also so

577
00:39:36,740 --> 00:39:38,760
for example you can model

578
00:39:38,770 --> 00:39:41,840
the part of the world in your computer and is here

579
00:39:46,260 --> 00:39:51,620
but primarily it directed towards the world

580
00:39:52,400 --> 00:39:54,500
that is that when you're working with

581
00:39:54,830 --> 00:39:59,320
all were built on what is really

582
00:39:59,330 --> 00:40:01,020
well i think about

583
00:40:01,040 --> 00:40:03,500
what kind of world you want to describe

584
00:40:03,550 --> 00:40:07,700
when i think about the world war and when you reading be also called

585
00:40:09,150 --> 00:40:10,020
they want to

586
00:40:10,030 --> 00:40:12,410
this right

587
00:40:15,980 --> 00:40:17,120
one be

588
00:40:17,140 --> 00:40:26,070
very general constraints in general only the the notation also consists of running things are

589
00:40:28,760 --> 00:40:30,690
we don't want to be so

590
00:40:30,710 --> 00:40:35,550
he first ran for

591
00:40:36,470 --> 00:40:41,030
the program is well what are

592
00:40:41,100 --> 00:40:42,960
computers are also

593
00:40:42,980 --> 00:40:46,770
that would sort of defeat the purpose of doing this kind of model

594
00:40:46,790 --> 00:40:48,300
sometimes it's best to

595
00:40:48,310 --> 00:40:50,870
unfortunately we cannot do it

596
00:40:53,640 --> 00:40:55,720
there so

597
00:40:55,740 --> 00:40:57,830
the precision

598
00:40:57,880 --> 00:40:59,770
was a lot

599
00:40:59,790 --> 00:41:03,940
four great well on source

600
00:41:04,010 --> 00:41:05,600
well that's

601
00:41:08,480 --> 00:41:11,030
what are these just

602
00:41:16,880 --> 00:41:21,890
but are twelve months which is

603
00:41:21,890 --> 00:41:23,810
and i put it down on the table

604
00:41:23,830 --> 00:41:28,160
now having a nice conversation drinking beverages

605
00:41:28,200 --> 00:41:32,560
and it's about twelve thirty one o'clock now in starbucks is closing

606
00:41:32,560 --> 00:41:34,200
and it's time to walk back

607
00:41:34,220 --> 00:41:36,830
to a person

608
00:41:36,830 --> 00:41:39,350
we walking back to pierce

609
00:41:39,370 --> 00:41:44,530
and we leave this starbucks we make a left on chapel street walking up to

610
00:41:44,530 --> 00:41:48,310
york i'm getting a little sleepy

611
00:41:48,350 --> 00:41:50,930
my friend looks at me and says

612
00:41:51,010 --> 00:41:53,430
i feel fun

613
00:41:53,490 --> 00:41:55,430
what actually happening

614
00:41:55,430 --> 00:41:59,700
the heart is beating a little faster

615
00:41:59,720 --> 00:42:02,990
her palms are beginning to swell

616
00:42:03,010 --> 00:42:06,200
progress is coming a little shorter than it otherwise would

617
00:42:06,220 --> 00:42:08,760
how do we know

618
00:42:08,780 --> 00:42:12,240
so war here

619
00:42:12,310 --> 00:42:17,850
i don't think i felt this way in a very long time

620
00:42:19,950 --> 00:42:23,560
could be the coffee or decaff

621
00:42:23,580 --> 00:42:26,490
what could this be work

622
00:42:26,490 --> 00:42:30,490
then she turns and she looks

623
00:42:30,510 --> 00:42:32,640
and she says

624
00:42:32,780 --> 00:42:36,560
one day this is been

625
00:42:36,580 --> 00:42:42,640
what we have moved on and why it's almost like being in love

626
00:42:43,000 --> 00:42:45,890
it is almost like being in love except what it really is is two shots

627
00:42:45,890 --> 00:42:47,910
of cabinets

628
00:42:47,930 --> 00:42:55,060
causing a rapid heart rate respiration rate increased in respiration sweaty palms

629
00:42:55,060 --> 00:42:58,990
but i don't realize she doesn't realize that's what it is

630
00:43:00,100 --> 00:43:07,060
turns the most salient was also psychologists terms of most salient objects in immediate social

631
00:43:08,430 --> 00:43:09,910
that be

632
00:43:11,950 --> 00:43:17,330
is one that's the idea of this attribution aroused something else i don't know what

633
00:43:17,330 --> 00:43:18,740
that is

634
00:43:18,740 --> 00:43:22,470
it's best if you don't know what that is or even if you do mistakenly

635
00:43:22,470 --> 00:43:25,030
attributed mister attributed

636
00:43:26,350 --> 00:43:31,220
physical attraction romance intimacy passion and commitment

637
00:43:31,220 --> 00:43:32,120
it's love

638
00:43:32,140 --> 00:43:33,030
i no

639
00:43:33,070 --> 00:43:36,240
i don't necessarily recommend

640
00:43:36,580 --> 00:43:40,410
that you do this thought experiment in

641
00:43:40,530 --> 00:43:43,640
in vivo this weekend

642
00:43:43,680 --> 00:43:47,010
although if you're lonely you might want to try it but

643
00:43:47,060 --> 00:43:49,160
we can go we can take this idea

644
00:43:49,930 --> 00:43:52,930
we can we can actually do research on this we could take it to the

645
00:43:52,930 --> 00:43:58,890
last but before i tell you about love experiments the most famous field experiments on

646
00:43:58,890 --> 00:44:00,600
this idea

647
00:44:00,620 --> 00:44:05,160
we call this the rickety bridge experiment and there's a bridge at the university of

648
00:44:05,160 --> 00:44:07,350
british columbia

649
00:44:08,620 --> 00:44:12,100
crosses a river that runs through campus

650
00:44:12,200 --> 00:44:16,290
and the rickety there is actually two bridges the rickety bridge

651
00:44:16,310 --> 00:44:18,970
it is one that's kind of a rope bridge

652
00:44:18,990 --> 00:44:21,450
it's hundreds of feet above the river

653
00:44:21,450 --> 00:44:26,330
it's ways in the breeze it's only about three feet wide you kind of hold

654
00:44:26,330 --> 00:44:30,990
on to it carefully and you cross the river it's pretty scary went across that

655
00:44:31,010 --> 00:44:33,060
somebody better than using this bridge

656
00:44:33,080 --> 00:44:38,410
still there yes another great OK there is another way to cross the river

657
00:44:38,430 --> 00:44:44,490
it's on a low bridge near the water solid wood planks nice and why

658
00:44:44,490 --> 00:44:47,850
the hand railings made out of solid wood

659
00:44:47,890 --> 00:44:49,850
and you can cross the bridge

660
00:44:49,870 --> 00:44:54,140
so what to investigators interested british columbia did

661
00:44:54,200 --> 00:44:56,280
very simply position

662
00:44:56,330 --> 00:44:58,370
once again

663
00:44:58,370 --> 00:45:02,780
an attractive actor confederate

664
00:45:02,830 --> 00:45:05,910
on one side of the bridge

665
00:45:05,930 --> 00:45:07,600
she was one of

666
00:45:07,620 --> 00:45:09,370
but you know that

667
00:45:10,100 --> 00:45:12,350
crossing the bridge

668
00:45:12,350 --> 00:45:15,180
and she would intercept them as they can cause

669
00:45:15,220 --> 00:45:17,700
a rickety bridge or the low bridge

670
00:45:17,720 --> 00:45:20,180
and she would have a few questions

671
00:45:20,180 --> 00:45:23,240
and conclude with can you write me a story

672
00:45:24,180 --> 00:45:28,140
it would help me out with my experiment just credible story right now

673
00:45:28,200 --> 00:45:30,200
then she would collect the story

674
00:45:30,220 --> 00:45:31,530
and she would say

675
00:45:31,550 --> 00:45:36,580
if you have any questions about this experiment here is my phone number actually this

676
00:45:36,580 --> 00:45:40,350
happens when you're experiments with the phone number

677
00:45:40,410 --> 00:45:42,470
what happens

678
00:45:44,120 --> 00:45:52,260
man male students across the rickety bridge they wrote these sexy stories with interesting content

679
00:45:52,260 --> 00:45:53,620
with kind of

680
00:45:54,510 --> 00:45:58,450
kind of little bit tribal themes

681
00:45:58,450 --> 00:46:01,700
and the people on the road on the solid bridge

682
00:46:01,750 --> 00:46:03,370
just wrote pretty

683
00:46:03,450 --> 00:46:05,260
boring stories

684
00:46:05,370 --> 00:46:09,810
the people across the rickety bridge were more likely to call up later say

685
00:46:09,810 --> 00:46:12,510
yeah i like to talk about the experiment i was in

686
00:46:12,530 --> 00:46:15,180
going into the starbucks

687
00:46:15,200 --> 00:46:17,370
decaff don't you

688
00:46:19,600 --> 00:46:23,410
and the people in the low bridge were much less likely to color

689
00:46:23,450 --> 00:46:28,850
the idea what was going on well what this was interpreted as mister attributed arousal

690
00:46:28,850 --> 00:46:33,660
on the rickety bridge you're swaying in the breeze hundreds of feet above the water

691
00:46:34,060 --> 00:46:36,060
the bridge seems unstable

692
00:46:36,080 --> 00:46:41,790
maybe i'll make maybe want your heart beating years palms are sweating you're breathing harder

693
00:46:41,810 --> 00:46:43,830
you meet this person and

694
00:46:43,850 --> 00:46:45,600
she seems more attractive

695
00:46:45,620 --> 00:46:48,430
because you're feeling all these things in your attribute it

696
00:46:48,470 --> 00:46:50,490
two to the attraction

697
00:46:50,510 --> 00:46:53,410
now there's a reason why this study

698
00:46:53,450 --> 00:46:55,510
is bad science

699
00:46:55,580 --> 00:46:57,910
there's a major flaws

700
00:46:57,950 --> 00:47:00,160
in this time

701
00:47:00,200 --> 00:47:01,530
the clue to the floor

702
00:47:01,560 --> 00:47:05,720
is that you can even call this study an experiment

703
00:47:05,850 --> 00:47:08,050
what's the floor

704
00:47:15,870 --> 00:47:20,100
people who take the rickety bridge might be the kind of people more looking for

705
00:47:20,100 --> 00:47:23,270
if you already have the deduction of five from guam

706
00:47:23,270 --> 00:47:26,960
and you have the option of fireflies from gamma

707
00:47:27,770 --> 00:47:32,460
you can extend the deduction right that size deducible from again

708
00:47:32,580 --> 00:47:35,270
so you see the modus ponens that's going on

709
00:47:35,350 --> 00:47:38,270
five five site decide

710
00:47:38,980 --> 00:47:41,540
the rule is different from biological

711
00:47:41,540 --> 00:47:44,850
if there is the deduction of five from guam

712
00:47:44,870 --> 00:47:48,100
then there's the deduction of box five get

713
00:47:48,120 --> 00:47:52,210
that's just the rules telling that's the way the right

714
00:47:54,940 --> 00:47:58,210
those are the rules i haven't showed you the axioms you

715
00:47:58,270 --> 00:48:02,040
but what we're going do is this is how we can build derivation

716
00:48:02,060 --> 00:48:05,750
now i'm going to move on to the axioms hopefully

717
00:48:07,310 --> 00:48:11,770
i'm just formalizing what derivation what's the derivation

718
00:48:11,810 --> 00:48:14,920
so to finite tree of judgments

719
00:48:14,960 --> 00:48:21,170
with the route is gamma norton stuff i not let's rubber only semantic stuff because

720
00:48:21,230 --> 00:48:28,790
it's forbidden

721
00:48:28,870 --> 00:48:46,350
so what is it it's a tree with root gamma not stuff i know

722
00:48:48,120 --> 00:48:50,350
so it's a tree like this

723
00:48:50,350 --> 00:48:52,120
and the leaves

724
00:48:52,140 --> 00:48:53,730
are either

725
00:48:53,750 --> 00:48:57,020
judgement instances or i d instances

726
00:48:57,040 --> 00:49:00,540
OK so the leaves over here i have to end with the rule i d

727
00:49:00,560 --> 00:49:02,770
or have in with the rules

728
00:49:02,830 --> 00:49:04,890
i x

729
00:49:04,900 --> 00:49:06,690
and everything in between

730
00:49:06,710 --> 00:49:08,770
has to be glued together

731
00:49:08,790 --> 00:49:11,830
using these three rules

732
00:49:12,670 --> 00:49:16,310
so if you have a binary step

733
00:49:18,100 --> 00:49:19,480
and you know that this

734
00:49:19,500 --> 00:49:23,350
and this will match this part

735
00:49:23,370 --> 00:49:25,100
and you conclude

736
00:49:25,100 --> 00:49:26,690
to go to the parents

737
00:49:26,710 --> 00:49:30,830
which is the it's alive show you

738
00:49:30,850 --> 00:49:36,290
so the only thing i'm showing you the axiom schema and here the axiom schema

739
00:49:36,370 --> 00:49:37,890
or three

740
00:49:37,900 --> 00:49:40,420
for classical logic

741
00:49:40,420 --> 00:49:41,270
and one

742
00:49:41,290 --> 00:49:45,020
which is the model shape which have already shown you is valid they should be

743
00:49:45,080 --> 00:49:51,140
getting suspicious about my exercises of k they were chosen very specifically

744
00:49:51,160 --> 00:49:55,460
so the exercises asked to show that these guys were valid

745
00:49:55,560 --> 00:49:57,660
and this guy was valid

746
00:49:57,690 --> 00:50:01,230
and surprise surprise they turn up as my actions scheme

747
00:50:03,810 --> 00:50:05,270
how is the axioms

748
00:50:06,730 --> 00:50:09,900
we use it in this way five is an instance of an axiom schema so

749
00:50:09,900 --> 00:50:12,540
what you do is if i give you some five here

750
00:50:12,560 --> 00:50:16,610
you have to look at the shapes and see whether that particular phi is an

751
00:50:16,610 --> 00:50:18,100
instance of it

752
00:50:18,120 --> 00:50:22,560
that's all that's independent of gamma are and i'm going to use some abbreviations for

753
00:50:22,560 --> 00:50:25,790
the other connectives i don't want to go through

754
00:50:25,810 --> 00:50:27,790
inventing more and more rules

755
00:50:27,850 --> 00:50:31,100
OK so here's an example

756
00:50:31,120 --> 00:50:36,000
i claim that this is the derivation

757
00:50:36,060 --> 00:50:38,330
of box p one

758
00:50:38,350 --> 00:50:43,210
from gamma which consists of pain or implies p one

759
00:50:43,270 --> 00:50:44,670
it's just work through it

760
00:50:44,730 --> 00:50:47,810
let's work through a top down not sure if

761
00:50:47,890 --> 00:50:51,120
my lectures are in technicolor on location there

762
00:50:51,140 --> 00:50:54,430
but still the other way will work bottom up because that's the the pictures that

763
00:50:54,940 --> 00:50:58,560
i can claim that that's an instance of necessitation

764
00:50:58,600 --> 00:51:01,900
and the colouring is supposed to help

765
00:51:03,060 --> 00:51:05,960
there is the rule necessitation

766
00:51:05,960 --> 00:51:08,080
all of this stuff is in gamma

767
00:51:08,140 --> 00:51:11,440
gamma goes up there unchanged

768
00:51:11,440 --> 00:51:13,900
this is supposed to be box phi one

769
00:51:14,020 --> 00:51:15,980
so the green b is five

770
00:51:16,000 --> 00:51:17,920
and we just drop the box

771
00:51:21,120 --> 00:51:23,330
go up one step

772
00:51:23,350 --> 00:51:25,330
gamma is now

773
00:51:25,330 --> 00:51:29,750
the stuff in red again remains unchanged

774
00:51:29,770 --> 00:51:32,020
the site is this gonna

775
00:51:32,040 --> 00:51:34,600
p one

776
00:51:34,600 --> 00:51:36,770
the phi is p nor

777
00:51:36,790 --> 00:51:41,190
and over here we connect the find site using an indication

778
00:51:41,190 --> 00:51:43,690
so the p norm is the same as that

779
00:51:43,710 --> 00:51:45,770
the p one is the same as that

780
00:51:45,790 --> 00:51:47,670
and there is an implication

781
00:51:51,420 --> 00:51:54,120
and finally we have the maturity

782
00:51:54,400 --> 00:51:57,940
in this case both of them are instances of idea because all you do is

783
00:51:57,940 --> 00:51:59,000
you shake

784
00:51:59,000 --> 00:52:01,920
it's p your member of this list yesterday

785
00:52:01,920 --> 00:52:05,830
it's pain known implies b one member this this yesterday

786
00:52:05,850 --> 00:52:07,980
that's it

787
00:52:08,000 --> 00:52:10,420
at no point did i talk about model

788
00:52:10,670 --> 00:52:15,190
o point to i talk about truth valuations all i did was to

789
00:52:15,290 --> 00:52:16,890
applied pattern matching

790
00:52:16,890 --> 00:52:18,670
but you computer cond

791
00:52:20,810 --> 00:52:24,420
is a different derivation of the same thing

792
00:52:25,710 --> 00:52:26,730
p not

793
00:52:26,750 --> 00:52:30,210
you know it implies p one goes box p one

794
00:52:30,250 --> 00:52:33,170
and by doing bottom-up or top-down

795
00:52:33,850 --> 00:52:35,040
doing it all

796
00:52:35,040 --> 00:52:36,000
let's have a look

797
00:52:37,350 --> 00:52:39,210
that's the top down

798
00:52:39,210 --> 00:52:44,270
this is an instance of ideas because this is in their a set membership

799
00:52:44,270 --> 00:52:48,830
this is this excitation because i leave the left hand side alone input box in

800
00:52:48,830 --> 00:52:51,000
front of the whole thing

801
00:52:51,000 --> 00:52:53,770
i having written up the rules so you'll have to look at the rules on

802
00:52:53,770 --> 00:52:55,370
your slides right

803
00:52:55,400 --> 00:52:58,940
i leave the formula allows put box in front

804
00:52:58,960 --> 00:52:59,960
now over here

805
00:52:59,980 --> 00:53:01,170
i'm using

806
00:53:01,170 --> 00:53:03,530
OK so let's let's think about

807
00:53:03,580 --> 00:53:05,720
and symbols

808
00:53:08,270 --> 00:53:10,620
let x be the

809
00:53:10,650 --> 00:53:12,980
the outcome of a random experiment

810
00:53:12,980 --> 00:53:16,600
and let's be very very concrete here OK

811
00:53:16,610 --> 00:53:21,410
let's look at the figure first OK now

812
00:53:21,420 --> 00:53:24,920
this will be the index

813
00:53:24,930 --> 00:53:26,460
all of of my random

814
00:53:26,730 --> 00:53:30,480
experiment okay so i perform sort of

815
00:53:30,540 --> 00:53:32,100
i a sort of take

816
00:53:32,160 --> 00:53:33,520
sort of take x

817
00:53:33,540 --> 00:53:37,680
OK i'm explained differently

818
00:53:38,370 --> 00:53:40,600
can take values from an alphabet

819
00:53:40,610 --> 00:53:45,020
a a represents an alphabet right and actually to be quite quite natural in this

820
00:53:45,770 --> 00:53:51,040
stolen from david mackay's book who has read david mackay's book

821
00:53:51,100 --> 00:53:52,530
a little bit

822
00:53:59,170 --> 00:54:00,610
all right

823
00:54:00,620 --> 00:54:04,370
x takes values from a with probability p of x

824
00:54:04,410 --> 00:54:07,690
OK so let's take let's take a concrete example

825
00:54:07,770 --> 00:54:12,420
a concrete example would be if if the alphabet was actually an alphabet you could

826
00:54:12,420 --> 00:54:13,410
also be

827
00:54:13,420 --> 00:54:17,120
you know colors sizes anything you want

828
00:54:19,240 --> 00:54:21,570
if you were letters from the alphabet

829
00:54:21,590 --> 00:54:27,580
and he was the the corresponding probabilities that x will take the corresponding value in

830
00:54:28,860 --> 00:54:33,250
one thing we could do we could go on download a manual for for linux

831
00:54:33,280 --> 00:54:37,560
and this is how they like i was very religious about operating systems

832
00:54:38,340 --> 00:54:39,650
this figure

833
00:54:39,690 --> 00:54:41,280
and he basically counted

834
00:54:41,290 --> 00:54:43,170
the number of times he so

835
00:54:43,180 --> 00:54:47,440
later in in the in the whole manual and he just divided by the total

836
00:54:47,440 --> 00:54:51,290
number of letters that there were right including and that includes the space

837
00:54:53,180 --> 00:54:56,210
all right so once once he did that

838
00:54:56,240 --> 00:54:59,310
he obtained some sort of empirical

839
00:54:59,340 --> 00:55:05,750
estimates as it were of what's the probability of encountering those letters is in general

840
00:55:05,760 --> 00:55:07,430
links manually

841
00:55:08,260 --> 00:55:10,950
this representation here we're going to see later again

842
00:55:10,970 --> 00:55:15,270
the size of the squares is proportional to the size of the numbers

843
00:55:16,240 --> 00:55:17,990
so what properties

844
00:55:18,040 --> 00:55:20,320
what properties do these probabilities

845
00:55:20,410 --> 00:55:22,740
need to have

846
00:55:22,750 --> 00:55:24,380
one of them is

847
00:55:24,390 --> 00:55:25,440
they always

848
00:55:28,520 --> 00:55:31,910
and the other one is a up to one

849
00:55:31,920 --> 00:55:34,870
right now when you go and read papers

850
00:55:34,910 --> 00:55:38,840
in the different communities you see that people use statistics are a bit more careful

851
00:55:38,840 --> 00:55:40,080
they actually right

852
00:55:40,090 --> 00:55:42,030
things more sort of

853
00:55:42,040 --> 00:55:45,880
correctly right in in other sort of

854
00:55:45,920 --> 00:55:48,510
papers in machine learning like the ones i right

855
00:55:48,520 --> 00:55:54,180
people tend to be very very sloppy actually with notation which sometimes causes great confusion

856
00:55:54,180 --> 00:55:56,680
because we have p of expert

857
00:55:58,490 --> 00:56:01,970
then you recycle x many times and then p is always p and you don't

858
00:56:01,970 --> 00:56:03,670
have the substrate that says

859
00:56:03,880 --> 00:56:08,220
but with respect to which distribution is that and stuff like that all right so

860
00:56:08,220 --> 00:56:09,930
people tend to sort of

861
00:56:09,970 --> 00:56:13,790
right very very minimalistic

862
00:56:13,810 --> 00:56:18,310
expressions here i'd like either p of the random variable or p of the outcome

863
00:56:18,310 --> 00:56:20,800
of the random variable takes but actually

864
00:56:20,820 --> 00:56:25,030
the way this thing should be written is actually

865
00:56:26,340 --> 00:56:29,290
but just to train new i'm actually not going to do that in the rest

866
00:56:29,290 --> 00:56:34,940
of the presentation

867
00:56:40,950 --> 00:56:43,410
the alphabet

868
00:56:43,450 --> 00:56:47,540
can also be can either be sort of a

869
00:56:47,550 --> 00:56:51,010
so here it just sort of say a set x actually so these are these

870
00:56:51,010 --> 00:56:56,930
this is like x taking values in the in the alphabet right

871
00:56:58,460 --> 00:57:00,020
but the alphabet could also be

872
00:57:00,040 --> 00:57:02,910
could also be continuous like like what is the distribution

873
00:57:02,930 --> 00:57:03,990
of the height

874
00:57:04,000 --> 00:57:07,750
of of the human rights so we can go on and measure the height of

875
00:57:07,750 --> 00:57:11,190
all of you but if we had if we had instruments for measuring the were

876
00:57:11,200 --> 00:57:12,360
precise enough

877
00:57:12,370 --> 00:57:13,900
then in serie

878
00:57:13,910 --> 00:57:17,610
the height could be considered as a a continuous

879
00:57:17,620 --> 00:57:22,140
random variable right or a random variable that takes values from continuous alphabet as it

880
00:57:22,140 --> 00:57:28,610
you find acceptable information the the information about orientations in the occipital cortex

881
00:57:29,510 --> 00:57:34,380
this is this is obtained by averaging at the level of the accuracy maps across

882
00:57:35,900 --> 00:57:40,260
so you basically war the the beauty of this is that now you can accumulate

883
00:57:40,260 --> 00:57:45,510
the information processing across a large number of subjects at the level of the storage

884
00:57:45,510 --> 00:57:46,550
of information

885
00:57:46,570 --> 00:57:50,700
so you basically you can look at the average information stored for each person and

886
00:57:50,700 --> 00:57:54,090
you walk the brains also the fit together and then you can do some second-level

887
00:57:54,090 --> 00:57:56,300
analysis on that as well

888
00:57:56,320 --> 00:58:01,700
and that way you can you can find things that you wouldn't be able to

889
00:58:01,700 --> 00:58:04,740
see the current classical analysis and you wouldn't be able to see if you just

890
00:58:04,740 --> 00:58:08,950
did according analysis on a single person because effects will be far too weak

891
00:58:09,010 --> 00:58:12,530
so now you can for example run fifty subjects and see things that you wouldn't

892
00:58:12,530 --> 00:58:13,930
normally see

893
00:58:17,360 --> 00:58:19,570
so what you see here again

894
00:58:19,590 --> 00:58:21,780
it's a matter thresholding as well

895
00:58:21,800 --> 00:58:27,970
this shows the temporal development of the information about task rules target stimuli and motor

896
00:58:29,470 --> 00:58:32,960
and what you can see is that the task rule first is instructive q has

897
00:58:32,960 --> 00:58:34,110
a or b

898
00:58:34,130 --> 00:58:39,200
instructive q first is in visual cortex we first see the information related to the

899
00:58:42,130 --> 00:58:43,010
and then

900
00:58:43,030 --> 00:58:47,240
you see information and pride cortex and then it goes into lateral prefrontal cortex which

901
00:58:47,240 --> 00:58:53,820
is quite compatible with with data from previous lesion studies and so on

902
00:58:53,820 --> 00:58:57,900
the target stimuli because they get the q first and then they see the stimulus

903
00:58:57,900 --> 00:58:59,510
and they have to respond

904
00:58:59,530 --> 00:59:02,240
but the target stimulus comes on later

905
00:59:02,260 --> 00:59:06,490
and it's in the visual system but interestingly and this is related to the talk

906
00:59:06,490 --> 00:59:15,490
by performing a professor from actually the this perspective information original spective information

907
00:59:15,530 --> 00:59:20,160
so the idea that here you see the target stimulus information is first in the

908
00:59:20,160 --> 00:59:26,050
visual system but there is the late coding of this visual stimulus after the response

909
00:59:26,050 --> 00:59:30,760
has already been made in the lateral prefrontal cortex which is not exactly what you

910
00:59:30,760 --> 00:59:36,070
saw but is set it again raises the question what is this information doing that

911
00:59:36,070 --> 00:59:39,300
what is being used for and the motor response as you can see is that

912
00:59:39,300 --> 00:59:43,410
they can also unfold these processes in time

913
00:59:43,450 --> 00:59:45,470
so next show you how

914
00:59:45,490 --> 00:59:49,860
we address the same issue with internally chosen goals

915
00:59:49,860 --> 00:59:51,050
and to do this

916
00:59:51,070 --> 00:59:56,180
i've done this same thing in the other talk the other day so i apologize

917
00:59:56,180 --> 01:00:00,490
for duplication but i'd like everyone to decide now

918
01:00:00,510 --> 01:00:05,090
if they want to add or subtract two numbers so you just do one of

919
01:00:05,090 --> 01:00:08,680
the two you can do one of the two tasks but just decide now freely

920
01:00:09,070 --> 01:00:11,910
if you want to add or subtract two numbers

921
01:00:11,930 --> 01:00:15,950
and now the question is do you think i could read out of your brain

922
01:00:15,950 --> 01:00:17,470
activity that you've

923
01:00:17,510 --> 01:00:19,530
covertly made this choice

924
01:00:19,550 --> 01:00:22,010
to either add or subtract two numbers

925
01:00:22,240 --> 01:00:24,470
and the way we started this was

926
01:00:24,490 --> 01:00:27,210
here are the two numbers by the way so he can i can do your

927
01:00:27,210 --> 01:00:29,990
calculations fifty six thirty three

928
01:00:31,300 --> 01:00:33,910
so we get people to freely select

929
01:00:33,910 --> 01:00:38,410
one of two tasks they can either add or subtract numbers the different variants of

930
01:00:39,380 --> 01:00:40,640
and then

931
01:00:40,660 --> 01:00:42,030
there is a delay

932
01:00:42,050 --> 01:00:46,820
until they actually see the numbers that they have to do is use the calculation

933
01:00:46,840 --> 01:00:49,110
and then during this domain

934
01:00:49,130 --> 01:00:54,610
the question is can we decode and from which brain areas can with code which

935
01:00:54,610 --> 01:00:58,010
covered intention the person currently holding

936
01:00:58,070 --> 01:01:02,760
and the reason to use adding and subtracting here was we wanted to use maximally

937
01:01:02,760 --> 01:01:08,550
symmetrical operations they're not fully symmetric but they're are kind of more symmetric and other

938
01:01:12,030 --> 01:01:14,780
so the question is now

939
01:01:14,780 --> 01:01:19,680
we train a decoder to read out of this variable delay period so the idea

940
01:01:19,680 --> 01:01:24,130
is between your choice and the time when you see the numbers there is a

941
01:01:24,130 --> 01:01:28,490
variable delay this variable delay can be between two and twelve seconds that keeps you

942
01:01:28,490 --> 01:01:33,130
on your toes you you kind of have to stay ready to quickly respond to

943
01:01:33,130 --> 01:01:37,110
the to stimulate into two numbers you can see so you don't just make up

944
01:01:37,110 --> 01:01:40,280
your mind then you can forget it again in ten seconds later you make up

945
01:01:40,280 --> 01:01:42,550
your mind again we have to kind of keep on your tongue

946
01:01:42,550 --> 01:01:44,890
you know blob this kind of thing OK

947
01:01:46,470 --> 01:01:48,910
now if you learn a few tricks

948
01:01:48,930 --> 01:01:50,280
in least squares

949
01:01:50,300 --> 01:01:53,800
or you can do well if you learn how to do weight twiddling and you

950
01:01:53,800 --> 01:01:59,820
learn about regularisation basically those two alone and you are now actually trained you already

951
01:01:59,840 --> 01:02:04,890
or you're ready to do battle with using least squares is tool and you will

952
01:02:05,570 --> 01:02:06,740
really well

953
01:02:06,740 --> 01:02:10,550
let me tell you i mean so that the weight is basically

954
01:02:10,610 --> 01:02:12,840
you know you're going to problem in some sense

955
01:02:12,890 --> 01:02:18,140
this is no limit on the sum of the squares you know the limits on

956
01:02:18,140 --> 01:02:21,450
this that the then you got no problem these weights around this you just you

957
01:02:21,450 --> 01:02:24,400
look at the least square solution you like what you see you change the weights

958
01:02:24,400 --> 01:02:25,530
and you do it again

959
01:02:26,320 --> 01:02:30,160
but we in bowen engineering we do this all times called we twiddling

960
01:02:30,160 --> 01:02:33,300
slightly derogatory term

961
01:02:33,620 --> 01:02:36,800
but not not that and i'm sure they do in statistics and so like but

962
01:02:36,800 --> 01:02:41,010
i haven't got i don't know that they don't have to be admitted

963
01:02:41,010 --> 01:02:41,950
right so

964
01:02:41,950 --> 01:02:43,990
because they like to have this

965
01:02:45,780 --> 01:02:48,140
you know better smiling

966
01:02:48,200 --> 01:02:51,030
you are a good because i like to have when making fun of the department

967
01:02:51,030 --> 01:02:56,070
i like to have base to look at

968
01:02:56,070 --> 01:02:57,950
i'd like to stick to that

969
01:02:57,970 --> 01:02:58,910
you know

970
01:02:58,950 --> 01:03:01,870
so think you would get if you doing real statistics you go back in and

971
01:03:01,890 --> 01:03:06,910
i might change the prior distribution now admits you do you do that

972
01:03:06,930 --> 01:03:15,350
that's right we have to warn you that this this the theory by of all

973
01:03:15,350 --> 01:03:16,530
of these are being

974
01:03:16,590 --> 01:03:19,320
put these lectures being put on on the web

975
01:03:20,910 --> 01:03:23,890
because it's it's weird and find so

976
01:03:23,890 --> 01:03:27,610
don't worry we'll find out your voice and and if the camera focus on you

977
01:03:27,610 --> 01:03:32,450
will put like that squares all the way we if you like we can obscure

978
01:03:32,450 --> 01:03:33,870
your department

979
01:03:33,890 --> 01:03:38,280
so we could be put out actually would be good

980
01:03:38,300 --> 01:03:41,550
ninety two two diplomatic to to say

981
01:03:41,570 --> 01:03:44,660
i know so i'm just going to get in statistics right to make a big

982
01:03:44,660 --> 01:03:47,870
story about like the prior distribution that i bet they don't like the way he

983
01:03:47,950 --> 01:03:51,120
comes out they go back and change the prior distribution

984
01:03:51,140 --> 01:03:54,120
and they cover up their tracks they don't say that they do this i'm just

985
01:03:54,930 --> 01:03:57,660
because i like you know we know what we do in engineering OK

986
01:03:57,700 --> 01:03:59,110
we're not embarrassed

987
01:03:59,120 --> 01:04:01,120
so i do it

988
01:04:02,140 --> 01:04:08,110
next topic is linear programming so some of you have seen

989
01:04:08,120 --> 01:04:10,870
this may be actually how many people here just from sampling of people who were

990
01:04:10,870 --> 01:04:12,660
single linear programming someone

991
01:04:12,660 --> 01:04:13,800
so about

992
01:04:16,390 --> 01:04:18,390
MST three ten

993
01:04:18,700 --> 01:04:22,990
the morals

994
01:04:23,280 --> 01:04:26,640
in the greek islands

995
01:04:26,660 --> 01:04:28,870
along with the class

996
01:04:28,870 --> 01:04:30,140
and weights

997
01:04:30,160 --> 01:04:33,570
that's a lot of moore's law going down ten weeks so OK

998
01:04:33,590 --> 01:04:34,740
OK so

999
01:04:34,760 --> 01:04:36,510
linear programming

1000
01:04:37,970 --> 01:04:42,850
and that in my opinion it's what people should learn immediately after the least squares

1001
01:04:42,850 --> 01:04:44,970
single already comes in linear programming

1002
01:04:44,970 --> 01:04:47,070
i mean if you're gonna stop somewhere

1003
01:04:47,220 --> 01:04:50,910
you know that's a very good point i mean if you want to do you

1004
01:04:51,010 --> 01:04:54,510
actually really want to go on and field algorithms to real to real stuff you

1005
01:04:55,220 --> 01:04:59,680
that's a great place to start everybody nobody gets lost in a lot about so

1006
01:04:59,910 --> 01:05:04,010
many programs families like this minimize a linear function subject to

1007
01:05:04,160 --> 01:05:07,950
a bunch of linear inequalities now we're going to talk about horrible detail later in

1008
01:05:07,950 --> 01:05:10,160
the class so i'm not going to talk and not going to go into too

1009
01:05:10,160 --> 01:05:11,490
much detail now

1010
01:05:11,510 --> 01:05:14,610
but let me i one of the attributes

1011
01:05:14,640 --> 01:05:19,280
a linear programming so the first is in general except for very special cases there's

1012
01:05:19,280 --> 01:05:21,470
no analytical formula for the solution

1013
01:05:22,660 --> 01:05:27,340
just there is no formula there's no a transposing inverse transpose b

1014
01:05:27,350 --> 01:05:29,660
by the way don't confuse that

1015
01:05:29,660 --> 01:05:32,370
with our inability we can you can say

1016
01:05:32,390 --> 01:05:36,140
a huge amount of qualitative intelligent beings

1017
01:05:36,140 --> 01:05:38,840
about a linear programme

1018
01:05:38,870 --> 01:05:42,320
what you can do is write down the formula for the solution

1019
01:05:42,970 --> 01:05:48,010
there are reliable and efficient algorithms and software back in when year hold it as

1020
01:05:48,010 --> 01:05:50,280
a homework exercise you will right

1021
01:05:50,760 --> 01:05:55,970
actually something kind of in the top-class linear will be fifty lines of matlab or

1022
01:05:55,970 --> 01:06:00,050
python and if you take up our offer on that and you'll write something that

1023
01:06:00,050 --> 01:06:03,010
will solve huge linear programs quite well

1024
01:06:04,110 --> 01:06:10,160
and it's no asterisk on unsolved i mean you get best solution period

1025
01:06:10,180 --> 01:06:13,030
the computation time by the way is proportional and squared and

1026
01:06:13,280 --> 01:06:16,410
o and by the way that's exactly the same as least squares

1027
01:06:17,370 --> 01:06:20,390
that if you equate rose to the least squares

1028
01:06:20,410 --> 01:06:26,370
objective with constraints is identical that's not an accident which will get yes

1029
01:06:31,780 --> 01:06:34,930
m is the number of constraints here and k

1030
01:06:35,030 --> 01:06:37,740
is the height of a

1031
01:06:37,890 --> 01:06:40,430
she che well that's the best i can do

1032
01:06:40,450 --> 01:06:48,640
i think that's why not so this case this and is still many rose in

1033
01:06:48,660 --> 01:06:49,930
a great

1034
01:06:49,990 --> 01:06:50,990
over here

1035
01:06:51,010 --> 01:06:54,140
yes if i write that is in matrix inequality x less than b

1036
01:06:54,160 --> 01:06:56,890
yes that would be this same would be that k

1037
01:06:57,570 --> 01:07:00,490
so we carefully

1038
01:07:00,510 --> 01:07:04,970
my co-author of that we we had we spent hours discussing whether should be amateur

1039
01:07:05,200 --> 01:07:07,590
finally we can make either

1040
01:07:07,610 --> 01:07:12,470
we should be OK what about c or the edge sea

1041
01:07:12,530 --> 01:07:14,410
OK this is an hour and

1042
01:07:14,430 --> 01:07:16,780
OK don't see is an hour into

1043
01:07:16,800 --> 01:07:18,910
OK so i

1044
01:07:20,550 --> 01:07:24,620
so actually linear programming by the way it's very old topic free a wrote a

1045
01:07:24,620 --> 01:07:28,890
paper about that to hit the modern area starts in the thirties where else but

1046
01:07:29,720 --> 01:07:32,620
but actually the modern era

1047
01:07:32,620 --> 01:07:34,320
traces back to stanford

1048
01:07:35,590 --> 01:07:40,470
george stands for the by the way it it LP was something that you just

1049
01:07:40,470 --> 01:07:41,530
talked about

1050
01:07:41,590 --> 01:07:45,530
until you have computers which point all of a sudden look a lot more interesting

1051
01:07:45,530 --> 01:07:47,320
i was nineteen forty eight so

1052
01:07:47,410 --> 01:07:50,030
you know i think a lot of fun women also knew about a lot of

1053
01:07:50,030 --> 01:07:53,490
people knew about linear programming we will talk more about this when we get

1054
01:07:54,930 --> 01:07:59,300
i i think there's something like this couple's computers it many huge that that's immature

1055
01:08:00,280 --> 01:08:04,350
a linear programming it looks like it would be easy

1056
01:08:04,370 --> 01:08:08,200
to recognise and in fact in some cases the problem really has this four really

1057
01:08:08,200 --> 01:08:11,390
comes you on a platter like this so someone comes to you and says i

1058
01:08:11,390 --> 01:08:16,620
help me solve this problem i want to minimize this weighted sum of the variables

1059
01:08:16,620 --> 01:08:20,430
and they have some budget constraints or whatever there's about three problems or whatever that

1060
01:08:20,430 --> 01:08:22,260
actually have exactly this form

1061
01:08:23,660 --> 01:08:26,680
here's the really cool part of a linear programme

1062
01:08:26,700 --> 01:08:30,490
you will be stunned later in the class when you see the kind of problems

1063
01:08:30,660 --> 01:08:33,090
that we can reduce the linear programming

1064
01:08:33,090 --> 01:08:35,240
you would just keep this as free parameter

1065
01:08:35,260 --> 01:08:38,340
and then you would just optimizer with without parameter to

1066
01:08:38,400 --> 01:08:42,010
and and after you've done optimisation you with then open the hood and say well

1067
01:08:42,010 --> 01:08:44,280
what value is what is the value of

1068
01:08:44,420 --> 01:08:45,430
and if i have

1069
01:08:45,450 --> 01:08:47,650
i was very large and they all

1070
01:08:47,660 --> 01:08:51,090
this looks so there's only really one one lengthscale play here

1071
01:08:51,110 --> 01:08:53,360
but it out with various policies all

1072
01:08:53,400 --> 01:08:54,010
you know

1073
01:08:54,010 --> 01:08:55,860
we can draw some conclusions here

1074
01:08:55,930 --> 01:08:59,200
the interesting thing is although we're not really parameterizing

1075
01:08:59,260 --> 01:09:00,300
the functions

1076
01:09:00,320 --> 01:09:03,070
we can still parameterise the properties of the function

1077
01:09:03,090 --> 01:09:09,510
which seems in practice to be much more interesting thing to be able to do

1078
01:09:09,550 --> 01:09:11,150
OK so here is another example

1079
01:09:11,450 --> 01:09:17,920
these are the so-called matan forms so the the actual expression here involves something which

1080
01:09:17,920 --> 01:09:20,450
is known as the set bessel function

1081
01:09:20,470 --> 01:09:23,340
i don't really know what that is but i don't care about that i can

1082
01:09:23,340 --> 01:09:24,510
use it

1083
01:09:24,510 --> 01:09:27,590
so an interesting thing is you can prove

1084
01:09:27,650 --> 01:09:32,800
that that's sample functions from these mutant forms extra parameter new here

1085
01:09:32,860 --> 01:09:36,570
and the sample functions on numerous times differentiable

1086
01:09:37,420 --> 01:09:38,760
so here's an example

1087
01:09:38,780 --> 01:09:41,260
functions that are

1088
01:09:41,430 --> 01:09:53,130
what's interesting

1089
01:09:53,220 --> 01:09:57,510
OK the last five minutes

1090
01:09:57,550 --> 01:10:03,800
i could talk to us

1091
01:10:03,800 --> 01:10:04,650
let me talk

1092
01:10:04,650 --> 01:10:07,030
so i think this is the

1093
01:10:09,090 --> 01:10:10,260
the town forms

1094
01:10:10,280 --> 01:10:13,450
have this property that the set of functions that you draw

1095
01:10:13,530 --> 01:10:16,590
are differentiable a certain number of times

1096
01:10:16,650 --> 01:10:22,280
i was the one from the squared exponential functions were actually infinitely often differentiable all

1097
01:10:22,280 --> 01:10:23,590
derivatives exist

1098
01:10:23,630 --> 01:10:26,430
but you can also specify distributions of functions where only

1099
01:10:26,430 --> 01:10:27,930
up to second order

1100
01:10:27,930 --> 01:10:30,240
the french troops success

1101
01:10:30,260 --> 01:10:35,090
and if the project come back on architecture shows pictures of functions so again you

1102
01:10:35,090 --> 01:10:36,280
can also say well

1103
01:10:36,340 --> 01:10:37,860
how differentiable

1104
01:10:37,860 --> 01:10:41,530
does not function look like maybe you know something about the different abilities so then

1105
01:10:41,530 --> 01:10:44,030
you can inject that information directly into the process

1106
01:10:44,150 --> 01:10:46,650
or it might be something that you're interested in finding out

1107
01:10:46,650 --> 01:10:47,990
so you can do inference

1108
01:10:47,990 --> 01:10:50,740
over the parameter

1109
01:10:52,090 --> 01:10:57,320
i'll show you those

1110
01:10:57,380 --> 01:10:58,860
pictures and then i'll show you

1111
01:10:58,880 --> 01:10:59,920
one more

1112
01:10:59,930 --> 01:11:03,630
one more construction

1113
01:11:03,680 --> 01:11:05,430
which would be useful in

1114
01:11:05,430 --> 01:11:08,360
the application which are which and on

1115
01:11:08,760 --> 01:11:13,220
it is the problem with showing colourful pictures

1116
01:11:13,280 --> 01:11:21,130
this talk about some criterion third

1117
01:11:26,680 --> 01:11:29,200
yes that's the next one

1118
01:11:42,590 --> 01:11:47,880
i think i can come back to that in in the example that i been

1119
01:11:47,880 --> 01:11:50,380
looking at basically you can write down to the

1120
01:11:50,470 --> 01:11:52,720
for the noise process so independent

1121
01:11:52,760 --> 01:11:55,590
no it's just corresponds to adding a diagonal

1122
01:11:55,630 --> 01:11:56,360
on the

1123
01:11:56,360 --> 01:11:58,220
in the covariance

1124
01:11:59,380 --> 01:12:03,610
if you want spatial correlations then you can sort of imagine that

1125
01:12:03,650 --> 01:12:05,720
so that you can order the data points

1126
01:12:05,740 --> 01:12:09,130
if your data points then you would have something which is bandlimited

1127
01:12:09,180 --> 01:12:11,680
the saying well the noise

1128
01:12:11,720 --> 01:12:15,970
it is colored like it depends on on on neighboring points

1129
01:12:15,970 --> 01:12:18,990
but not over the whole scale historically

1130
01:12:19,860 --> 01:12:22,720
and you can also you don't have to have a constant on the diagonal

1131
01:12:22,740 --> 01:12:25,490
you get different values and i say well the noise

1132
01:12:25,610 --> 01:12:28,010
is very hefty in this region but it's very

1133
01:12:28,030 --> 01:12:29,970
very small amount noise

1134
01:12:29,970 --> 01:12:30,880
in this region

1135
01:12:58,130 --> 01:13:02,550
OK so here was this user's midtown fall and again a special limiting cases of

1136
01:13:02,550 --> 01:13:06,970
the midtown farmers out of the gaussians with that corresponds to the infinitely differentiable one

1137
01:13:07,160 --> 01:13:08,780
and again the crash functions look

1138
01:13:08,840 --> 01:13:10,530
rather similar

1139
01:13:10,530 --> 01:13:14,780
it turns out that the question about differentiability is actually related to it exactly what

1140
01:13:14,780 --> 01:13:17,240
the covariance function looks like

1141
01:13:17,590 --> 01:13:20,740
zero so there's a lot of mathematical theory that will tell you

1142
01:13:20,740 --> 01:13:21,990
these things here

1143
01:13:21,990 --> 01:13:24,110
this is just an just an example

1144
01:13:24,150 --> 01:13:25,160
here is the

1145
01:13:25,180 --> 01:13:30,090
the calcium again this very smooth function may be too small for practical applications already

1146
01:13:30,090 --> 01:13:32,300
for some applications

1147
01:13:32,400 --> 01:13:36,450
here's black is one which is twice differentiable

1148
01:13:36,470 --> 01:13:39,030
so it's not exactly a small this this one

1149
01:13:39,030 --> 01:13:40,240
but still pretty small

1150
01:13:40,260 --> 01:13:41,550
here's one which is

1151
01:13:41,570 --> 01:13:43,570
only once differentiable

1152
01:13:43,610 --> 01:13:49,780
and here with new equal to half actually something honest brownian motion

1153
01:13:50,320 --> 01:13:53,400
which is not even continuous

1154
01:13:53,450 --> 01:13:56,550
it may be just continue

1155
01:13:56,570 --> 01:13:58,400
so the question about

1156
01:13:58,400 --> 01:14:02,860
less than equal to see can actually be written as equal to c

1157
01:14:03,820 --> 01:14:08,910
our properties are going to be positive rate and they're going to some some to

1158
01:14:08,910 --> 01:14:11,050
some c value

1159
01:14:11,060 --> 01:14:11,980
and then

1160
01:14:12,060 --> 01:14:16,880
given the structure we can actually interpolate this is the probable you know in a

1161
01:14:16,880 --> 01:14:19,440
probabilistic framework and

1162
01:14:19,510 --> 01:14:22,890
we can think of the consequences of that city

1163
01:14:22,910 --> 01:14:25,980
and then we can write marginal deals so

1164
01:14:26,000 --> 01:14:27,260
we're going to define

1165
01:14:27,270 --> 01:14:29,620
two sets of marginals now

1166
01:14:29,730 --> 01:14:31,160
the marginal

1167
01:14:31,210 --> 01:14:34,610
and i'm going to consider all for i

1168
01:14:34,610 --> 01:14:36,000
why pairs

1169
01:14:36,020 --> 01:14:40,380
that have at the t position that have label

1170
01:14:40,410 --> 01:14:42,880
some summing over all of these

1171
01:14:43,120 --> 01:14:46,900
o parameters and

1172
01:14:46,950 --> 01:14:50,260
the the other set of tools are going to be

1173
01:14:50,270 --> 01:14:51,630
the ones

1174
01:14:51,650 --> 01:14:56,540
that that take the label interaction into place so here i am presenting the

1175
01:14:56,550 --> 01:15:01,060
formulas in terms of the sequences they actually defining more in general but i guess

1176
01:15:01,060 --> 01:15:02,810
it's easier to see here

1177
01:15:02,860 --> 01:15:04,210
right now

1178
01:15:04,450 --> 01:15:10,510
so for sequences we will have two sets of pairs two sets of of marginals

1179
01:15:10,560 --> 01:15:12,340
the marginals over

1180
01:15:13,800 --> 01:15:15,590
at each position

1181
01:15:15,610 --> 01:15:19,160
what is the label and the marginal over each

1182
01:15:19,220 --> 01:15:21,880
each position what is the labelled pair

1183
01:15:21,890 --> 01:15:26,330
OK so i'm going to some or all of these are all

1184
01:15:26,330 --> 01:15:30,770
for parameters that have s as prime

1185
01:15:30,840 --> 01:15:35,140
as as bar pair at the teat position

1186
01:15:35,170 --> 01:15:36,940
all right so

1187
01:15:36,980 --> 01:15:40,980
it it must be are it must be obvious to all of you that the

1188
01:15:41,000 --> 01:15:43,270
number of parameters to do

1189
01:15:43,270 --> 01:15:46,360
new parameters are only polynomial

1190
01:15:46,410 --> 01:15:52,070
it's there not exponential now for for each position i look at the labelled pair

1191
01:15:52,070 --> 01:15:53,860
or late label positions

1192
01:15:56,720 --> 01:16:02,450
all the alpha values there in my objective function are actually over some sounds so

1193
01:16:02,450 --> 01:16:07,220
i can really go to my objective function and plug in these values and rewrite

1194
01:16:07,220 --> 01:16:08,070
the the

1195
01:16:08,080 --> 01:16:09,140
l l

1196
01:16:09,170 --> 01:16:12,040
alpha in terms of these marginal tools

1197
01:16:12,040 --> 01:16:15,330
so now this is great i used to have an exponential

1198
01:16:15,360 --> 01:16:20,130
i have used to have an image of optimisation problem which has exponential parameters and

1199
01:16:20,130 --> 01:16:25,000
now i have only a polynomial size optimisation problem

1200
01:16:25,020 --> 01:16:28,000
one problem

1201
01:16:28,000 --> 01:16:33,880
here though is that the marginals do depend on the density of right so they

1202
01:16:33,880 --> 01:16:35,880
need to be consistent

1203
01:16:35,900 --> 01:16:40,900
by that i mean that

1204
01:16:40,900 --> 01:16:45,630
so the the the muse here really have an alpha parameter here

1205
01:16:45,650 --> 01:16:50,980
so the set of all possible values are actually larger than the set of

1206
01:16:51,000 --> 01:16:55,580
i mean parameters we can consider for a a given set of office

1207
01:16:57,080 --> 01:16:59,770
and that's the that's what i mean by by

1208
01:17:03,290 --> 01:17:07,440
but but by that the demand from illegal density

1209
01:17:08,900 --> 01:17:13,850
in fact the the number of wires that satisfy this and the number of wires

1210
01:17:13,850 --> 01:17:17,350
that satisfy this must be equal and

1211
01:17:18,310 --> 01:17:23,170
and this introduces some complex coupling over the these new parameters

1212
01:17:23,190 --> 01:17:30,670
and because of these couplings are taskar et al proposed to optimize really over the

1213
01:17:30,690 --> 01:17:37,120
alpha parameters that the that we've seen before using in some more like technique

1214
01:17:37,150 --> 01:17:42,770
but OK i algorithmically maybe didn't help at all it helps i'm i'm tossing of

1215
01:17:42,770 --> 01:17:47,560
some details but but in general we still have a we're still operating over the

1216
01:17:47,560 --> 01:17:53,790
exponential parameter space but really this this new definition of the margin and this we

1217
01:17:53,790 --> 01:17:56,020
parameterisation allows to

1218
01:17:56,170 --> 01:18:01,060
approach to two even two

1219
01:18:01,060 --> 01:18:04,270
to analyse the problem theoretically

1220
01:18:04,460 --> 01:18:07,170
in in an improved way yes

1221
01:18:07,190 --> 01:18:16,120
the other thing

1222
01:18:16,120 --> 01:18:23,620
this is the

1223
01:18:26,040 --> 01:18:30,810
i did not make it all the clear that this person is in our problem

1224
01:18:30,810 --> 01:18:34,150
does not only come from the sparseness of the hinge loss

1225
01:18:34,170 --> 01:18:41,600
it also comes from from the fact that are compatibility score of of some assignment

1226
01:18:41,600 --> 01:18:47,250
of some observation labelled pair is actually a combination of parts right so

1227
01:18:47,330 --> 01:18:52,150
the same parts can be observed in many many labels

1228
01:18:52,170 --> 01:18:56,150
OK so so because of the structure of the are labels

1229
01:18:56,190 --> 01:19:00,460
the there is the the sum of the sparsity is coming from the structure of

1230
01:19:00,460 --> 01:19:06,400
labels so the question you asked was OK we have explanation parameters and or some

1231
01:19:06,400 --> 01:19:07,270
lean years

1232
01:19:07,290 --> 01:19:11,960
if this person is only coming from the sparseness of the hinge loss we would

1233
01:19:11,960 --> 01:19:14,480
have exponential

1234
01:19:14,520 --> 01:19:17,480
parameters but it is not the case

1235
01:19:17,480 --> 01:19:22,150
actually in in our observations we need is that we do see that the number

1236
01:19:22,150 --> 01:19:24,270
number of support vector machines are

1237
01:19:24,310 --> 01:19:26,270
really various parts

1238
01:19:37,620 --> 01:19:41,380
so of they do grow

1239
01:19:43,330 --> 01:19:47,810
so for each position i actually i think they took away the slide i have

1240
01:19:48,120 --> 01:19:54,000
an example for from of from an experiment where i was showing some support vectors

1241
01:19:54,000 --> 01:19:58,500
and you will see that you will take the support vectors you have just differ

1242
01:19:58,500 --> 01:20:02,330
only in one position

1243
01:20:02,330 --> 01:20:04,480
he he's saying you have a sequence

1244
01:20:04,480 --> 01:20:09,520
and the support vectors you have i got to be the same for all positions

1245
01:20:09,520 --> 01:20:11,630
in justifying one position

1246
01:20:11,630 --> 01:20:17,770
so you're not going to of all possible label assignments here you're just really taking

1247
01:20:17,770 --> 01:20:24,460
a sequence switching build the label at that position which is zero polynomial

1248
01:20:24,460 --> 01:20:28,100
well thank you all for staying the very end i know you

1249
01:20:28,150 --> 01:20:30,180
yourself too

1250
01:20:30,250 --> 01:20:34,400
many talks and you will suffer too unlikely is quite amazing

1251
01:20:34,410 --> 01:20:36,640
so mean if you are still here and i guess

1252
01:20:36,650 --> 01:20:38,610
but it has to do with the fact that you

1253
01:20:38,650 --> 01:20:40,160
can't escaped

1254
01:20:44,280 --> 01:20:48,580
i'm sitting there saying the computer science and engineering department university of michigan

1255
01:20:48,620 --> 01:20:52,760
and my understanding is only really been one

1256
01:20:52,780 --> 01:20:54,660
proper reinforcement learning

1257
01:20:56,700 --> 01:20:58,240
though chris

1258
01:20:58,260 --> 01:21:02,910
this allows reinforcement learning these days are how much is talked about

1259
01:21:03,230 --> 01:21:07,970
power they quickly called because i can accelerate to the first tutorial part depending on

1260
01:21:08,620 --> 01:21:12,080
the full tells me i mean if you know of

1261
01:21:12,120 --> 01:21:14,480
BP's value iteration in

1262
01:21:14,490 --> 01:21:18,450
q learning

1263
01:21:18,480 --> 01:21:19,840
the quite

1264
01:21:20,970 --> 01:21:23,220
maybe i can but i

1265
01:21:23,230 --> 01:21:27,120
i was told that there is a fair bit of a mix of cognitive science

1266
01:21:27,120 --> 01:21:32,400
and machine learning people is not everybody sees are also solution would have tutorial

1267
01:21:33,900 --> 01:21:35,380
and then get on

1268
01:21:35,490 --> 01:21:39,630
the sort of thing that occupies my interest these data which is rethinking state action

1269
01:21:39,630 --> 01:21:41,350
reward so i will go through

1270
01:21:41,540 --> 01:21:44,680
i have a fairly quickly into or out

1271
01:21:44,690 --> 01:21:49,040
so first the obligatory slide of pictures of all my

1272
01:21:49,050 --> 01:21:51,150
collaborators and people have influenced me

1273
01:21:51,380 --> 01:21:56,270
on the top you see people i've collaborated with

1274
01:21:56,290 --> 01:22:00,180
at the bottom here all the students have done most of the work that i'm

1275
01:22:00,180 --> 01:22:04,100
going to tell you about i want to say their names

1276
01:22:04,180 --> 01:22:07,380
so here's the deal world

1277
01:22:07,380 --> 01:22:12,660
structural topology tutorial very quickly define MEP's tell you what the policy evaluation problem is

1278
01:22:12,660 --> 01:22:16,430
with the optimal control problem is and then give you one

1279
01:22:16,440 --> 01:22:22,770
idea for rethinking actions one idea of rethinking states in one thinking rewards and then

1280
01:22:23,290 --> 01:22:25,540
finish with a random thought two

1281
01:22:25,600 --> 01:22:31,520
OK so here is the power problem you an agent interactive environment actions getting perception

1282
01:22:31,540 --> 01:22:34,430
towards it's complete agent temporarily

1283
01:22:34,580 --> 01:22:39,200
situated in its world continually learning and planning and of course the

1284
01:22:39,210 --> 01:22:43,980
object here as opposed to most of the talks you've heard about to focus on

1285
01:22:43,980 --> 01:22:49,080
inference is to affect the environment so decision-making and action and of course other talks

1286
01:22:49,080 --> 01:22:49,810
and decision making

1287
01:22:50,310 --> 01:22:54,960
the environment can be stochastic uncertain so in some sense rl is like life and

1288
01:22:54,960 --> 01:22:59,430
here is the more abstract picture of this and the goal of the agent is

1289
01:22:59,430 --> 01:23:02,000
to maximize payoff for some time horizon so

1290
01:23:02,560 --> 01:23:07,280
this view of life is that light is an optimal control

1291
01:23:09,810 --> 01:23:11,120
thank you status

1292
01:23:11,200 --> 01:23:15,770
supervised and unsupervised reinforcement learning three legs in machine learning i guess

1293
01:23:15,990 --> 01:23:20,240
one thing i'd like to say is that if you've never heard of RL here's

1294
01:23:20,240 --> 01:23:24,840
one way to characterize what makes are all different from supervised and unsupervised learning you

1295
01:23:24,840 --> 01:23:26,400
play a game of chess

1296
01:23:26,450 --> 01:23:28,720
you make a hundred moves you lose

1297
01:23:28,860 --> 01:23:30,460
you have to figure out

1298
01:23:30,470 --> 01:23:34,560
which of two hundred moves were good which one of those are bad and this

1299
01:23:34,560 --> 01:23:38,520
is called the temporal credit assignment problem this problem is not there in supervised learning

1300
01:23:38,520 --> 01:23:43,830
unsupervised learning is what distinguishes reinforcement learning about solving in some ways the ends of

1301
01:23:44,520 --> 01:23:49,650
ten chromosome problems of course subsumes unsupervised and supervised learning as well as

1302
01:23:50,090 --> 01:23:57,050
for the shoes generalization and density estimation also that every possible OK lots of applications

1303
01:23:57,060 --> 01:24:01,580
slash slate up here is an MDP

1304
01:24:01,590 --> 01:24:07,300
just notation as state space and action space transition probabilities the probability of seeing state

1305
01:24:07,300 --> 01:24:08,930
i am taking action a in state j

1306
01:24:09,520 --> 01:24:14,340
the reward function function of state could be more general than that ties is deterministic

1307
01:24:14,340 --> 01:24:21,020
policy the notion of the return or utility of executing policy behind state i

1308
01:24:21,340 --> 01:24:24,920
which is the expected value of the discounted sum of rewards you get when you

1309
01:24:24,920 --> 01:24:26,740
behave according to policy pi

1310
01:24:26,920 --> 01:24:30,200
in some start state icivi of pi

1311
01:24:30,210 --> 01:24:33,300
we super is the utility

1312
01:24:33,450 --> 01:24:39,200
the policy pi the value of the return you all equivalent words in our forced

1313
01:24:39,420 --> 01:24:44,210
four state and policy pi the policies of course the one that maximizes utility

1314
01:24:44,280 --> 01:24:48,320
this is just a quick introduction mdps if you haven't seen this

1315
01:24:48,390 --> 01:24:50,810
four hopefully this

1316
01:24:50,860 --> 01:24:53,560
will do something for you for the rest of the talk

1317
01:24:53,570 --> 01:24:59,890
policy evaluation problem given a fixed policy evaluated the markov assumption is the key thing

1318
01:24:59,890 --> 01:25:03,570
i want you to understand this equation is an equation in the middle of this

1319
01:25:03,570 --> 01:25:05,500
slide for all s

1320
01:25:05,510 --> 01:25:10,170
in state space the pilot as the value of thanksgiving policy pi state s is

1321
01:25:10,220 --> 01:25:15,300
recursive four is the immediate reward you get plus the discounted expected value the next

1322
01:25:15,300 --> 01:25:20,100
state is an equation that takes the first principal definition of the value of the

1323
01:25:20,940 --> 01:25:25,560
and transforms it into a system of equations one for each state and the

1324
01:25:25,570 --> 01:25:30,160
the problem of evaluating how good a particular policy is amounts to solving the system

1325
01:25:30,160 --> 01:25:31,820
of equations

1326
01:25:31,860 --> 01:25:36,850
yes policy evaluation of optimal control the problem of finding the optimal value function or

1327
01:25:36,850 --> 01:25:41,330
in that the optimal policy can be written in this q notation in state action

1328
01:25:41,330 --> 01:25:45,630
values with the best you can do from state action pair is the immediate reward

1329
01:25:45,630 --> 01:25:49,660
you get plus the discounted expected the best you can do in the state to

1330
01:25:49,660 --> 01:25:55,750
get to again the markov assumption the markov assumption leads you to these recursive bellman

1331
01:25:55,750 --> 01:25:58,540
optimality equation and because the best you can do

1332
01:25:58,650 --> 01:26:02,280
in terms of action the state of the optimal action the state parks star that's

1333
01:26:02,580 --> 01:26:04,390
is then just the optimal

1334
01:26:04,460 --> 01:26:05,270
it's just the

1335
01:26:05,280 --> 01:26:09,330
greedy action with respect to the optimal q values

1336
01:26:09,340 --> 01:26:11,060
can just very quickly

1337
01:26:11,440 --> 01:26:13,770
in policy evaluation

1338
01:26:13,780 --> 01:26:19,250
on october twenty problems there are is often concerned evaluating a particular policy of finding

1339
01:26:19,250 --> 01:26:23,520
the optimal policy

1340
01:26:23,540 --> 01:26:24,810
OK so

1341
01:26:24,850 --> 01:26:27,960
i'll give you one algorithm for each of these before i know that that we

1342
01:26:28,010 --> 01:26:28,630
need to talk

1343
01:26:29,280 --> 01:26:34,400
policy evaluation if you have the planning version of it is you have a model

1344
01:26:34,520 --> 01:26:38,700
you know the reward function you the transition poverty you know the policy you evaluated

1345
01:26:38,950 --> 01:26:42,920
you basically take the recursion i showed you before and you know it

1346
01:26:42,940 --> 01:26:47,940
on the left-hand side on the right-hand side you can to left and right

1347
01:26:47,960 --> 01:26:49,650
simulating writing

1348
01:26:49,650 --> 01:26:53,700
who here is an undergraduate

1349
01:26:53,710 --> 01:26:57,120
a handful three who here is a

1350
01:26:57,150 --> 01:26:59,300
masters student just say

1351
01:27:00,140 --> 01:27:03,180
starting this year

1352
01:27:03,190 --> 01:27:06,670
there are two years phd student

1353
01:27:06,720 --> 01:27:09,980
growing around here is a

1354
01:27:10,160 --> 01:27:12,290
this just i'm phd

1355
01:27:16,080 --> 01:27:18,840
OK so everyone else for this year

1356
01:27:21,490 --> 01:27:24,370
this talk about reinforcement learning

1357
01:27:24,380 --> 01:27:27,440
which is

1358
01:27:27,470 --> 01:27:31,280
service centre task of artificial intelligence that is how do you

1359
01:27:31,300 --> 01:27:35,420
learn to make sequential decisions when i can do is get experience the world so

1360
01:27:35,420 --> 01:27:38,780
the robot walking around this room i'm trying to get to the door

1361
01:27:38,830 --> 01:27:40,270
a bump in

1362
01:27:40,310 --> 01:27:41,370
the chair

1363
01:27:41,420 --> 01:27:42,560
abundant here

1364
01:27:42,740 --> 01:27:46,910
i don't exactly know how my muscles my legs how it actually made me from

1365
01:27:46,910 --> 01:27:48,610
here to there were learned

1366
01:27:48,660 --> 01:27:50,020
how does that

1367
01:27:53,530 --> 01:27:57,870
you know i think i i a senior nectar

1368
01:27:57,950 --> 01:28:00,110
OK so

1369
01:28:00,120 --> 01:28:03,250
if i go through december punches me

1370
01:28:03,260 --> 01:28:05,410
OK i know that i want to see what about

1371
01:28:05,420 --> 01:28:08,160
but avoid punches me i don't know that is hostile

1372
01:28:08,180 --> 01:28:10,690
because the thing is i mean the world i don't know how to react to

1373
01:28:10,690 --> 01:28:12,130
what i do

1374
01:28:12,180 --> 01:28:15,310
but it does not to learn about that and i had seen my goals knowing

1375
01:28:16,010 --> 01:28:17,920
my actions affect the world

1376
01:28:17,940 --> 01:28:19,310
based on what i observe

1377
01:28:19,320 --> 01:28:23,110
OK this is the task of having every first learning it is very much the

1378
01:28:24,150 --> 01:28:26,100
in my view of a i

1379
01:28:26,140 --> 01:28:30,460
OK so this talk so has two

1380
01:28:30,770 --> 01:28:34,260
two aspects the first is RL itself

1381
01:28:34,300 --> 01:28:37,260
which is the exact appearance towards an unknown

1382
01:28:37,280 --> 01:28:38,960
so gas environment

1383
01:28:38,970 --> 01:28:43,090
so action is crucial here we need to do things nation has to affect the

1384
01:28:44,090 --> 01:28:45,770
OK so action is crucial

1385
01:28:45,780 --> 01:28:50,230
i know it is crucial for all i don't very far away and i had

1386
01:28:50,230 --> 01:28:55,630
to sample get experience in actions based on one you always lives in the first

1387
01:28:55,630 --> 01:28:58,710
two sections slide is slightly different what you have in your

1388
01:28:58,720 --> 01:29:01,610
handouts probably after that of

1389
01:29:01,770 --> 01:29:03,150
the slide will

1390
01:29:03,160 --> 01:29:06,060
will will be the same as what you have in your hand

1391
01:29:06,070 --> 01:29:09,670
also like piston inside the web but they're pretty close

1392
01:29:09,680 --> 01:29:12,510
OK this is the first aspect is the learning

1393
01:29:12,560 --> 01:29:15,220
OK but what happens if i knew

1394
01:29:15,270 --> 01:29:17,250
the full model

1395
01:29:17,270 --> 01:29:22,280
well that only learn and i if i know how to react to my actions

1396
01:29:22,390 --> 01:29:24,180
there's a learn i just have to do

1397
01:29:25,740 --> 01:29:29,140
so there's another aspect of this company and uncertainty

1398
01:29:29,750 --> 01:29:33,570
is the sequence of actions again actions

1399
01:29:33,590 --> 01:29:38,660
to do the same thing here towards and in this time non stochastic environment

1400
01:29:38,680 --> 01:29:43,370
OK so it realized two aspects this talk one is reinforcement learning we learn because

1401
01:29:43,370 --> 01:29:45,540
you don't know the environment and the other is

1402
01:29:45,590 --> 01:29:48,610
planning because you know the environment the speed the search

1403
01:29:48,620 --> 01:29:52,610
this too might in very different but once just the sampling version

1404
01:29:52,650 --> 01:29:57,540
the exact inference in the other so the very closely related

1405
01:29:59,830 --> 01:30:02,110
OK so i'll trainings

1406
01:30:02,130 --> 01:30:04,160
covers many many areas

1407
01:30:04,170 --> 01:30:06,320
a research from psychology

1408
01:30:06,330 --> 01:30:07,870
how do humans act

1409
01:30:07,900 --> 01:30:10,190
sociology how humans act

1410
01:30:10,200 --> 01:30:12,600
in the presence of reinforcement from other

1411
01:30:14,030 --> 01:30:15,350
in other groups

1412
01:30:15,400 --> 01:30:18,190
economics which starts to look at

1413
01:30:18,240 --> 01:30:21,560
how we act to achieve monetary rewards

1414
01:30:21,610 --> 01:30:23,300
how societies act to achieve

1415
01:30:24,600 --> 01:30:29,910
but rewards whether you're micro or macro economists

1416
01:30:30,250 --> 01:30:33,160
we answer is a clear science

1417
01:30:33,170 --> 01:30:36,920
agents anything it has to act intelligently in the world

1418
01:30:36,960 --> 01:30:40,960
has to some aspect of moral planning investing more than one action

1419
01:30:41,040 --> 01:30:45,790
if you want the rules theoretician about this is given to statistics house and environment

1420
01:30:45,900 --> 01:30:50,300
and decision theory which tells you how you make out up all decisions based on

1421
01:30:50,300 --> 01:30:51,800
what you know

1422
01:30:54,360 --> 01:30:56,280
this is a continuing here

1423
01:30:56,320 --> 01:30:58,120
from descriptive

1424
01:30:58,130 --> 01:31:01,950
we're in psychology sociology trying to describe how humans act

1425
01:31:02,000 --> 01:31:03,510
two prescriptive in

1426
01:31:03,520 --> 01:31:05,970
AI and that's the decision theory where we say

1427
01:31:05,980 --> 01:31:09,230
here's how you should act optimally given what you know

1428
01:31:09,240 --> 01:31:14,590
OK so we focus on the bottom half year on the descriptive aspects

1429
01:31:14,980 --> 01:31:22,590
so especially is especially relevant to a i how does in in agent learn to

1430
01:31:22,590 --> 01:31:27,120
learn to act on the environment

1431
01:31:27,200 --> 01:31:30,050
OK so there are two aspects one covered in this lecture

1432
01:31:30,100 --> 01:31:32,750
one is how do you model these problems

1433
01:31:32,760 --> 01:31:35,570
OK maybe not obvious but hopefully by the end of the

1434
01:31:35,570 --> 01:31:37,600
is y equals one over text

1435
01:31:37,620 --> 01:31:39,800
it's perfectly reasonable to do

1436
01:31:42,100 --> 01:31:45,220
we're going to calculate the areas of the triangles and you could ask yourself in

1437
01:31:45,220 --> 01:31:46,540
terms of what

1438
01:31:46,590 --> 01:31:49,920
well we're going to have to pick up points and given the name and since

1439
01:31:49,920 --> 01:31:52,850
we numbering have to do more than geometry we're going to have to do some

1440
01:31:52,850 --> 01:31:57,430
of this analysis just as we've done before so i'm going to make a point

1441
01:31:57,430 --> 01:32:00,280
and consistent with the labeling we've done before

1442
01:32:00,350 --> 01:32:03,160
i'm going to can't

1443
01:32:03,220 --> 01:32:05,960
so that's almost half the battle

1444
01:32:05,980 --> 01:32:08,080
having notations

1445
01:32:08,080 --> 01:32:11,910
x and y for the variables x zero and y zero for the for the

1446
01:32:11,940 --> 01:32:13,900
specific point

1447
01:32:15,340 --> 01:32:18,850
once you see that you have these labelings

1448
01:32:18,900 --> 01:32:22,070
i hope it's reasonable

1449
01:32:22,110 --> 01:32:24,590
but to do the following so first of all

1450
01:32:24,730 --> 01:32:28,560
this is the point x zero and over here is the pointwise zero

1451
01:32:28,600 --> 01:32:29,940
that's something that were

1452
01:32:29,980 --> 01:32:30,920
used to

1453
01:32:30,940 --> 01:32:32,590
in graphs

1454
01:32:32,590 --> 01:32:36,310
and in order to figure out the area of the triangle it's pretty clear that

1455
01:32:36,310 --> 01:32:37,860
we should find the base

1456
01:32:37,880 --> 01:32:42,620
which is that we to find this location here and we should find the height

1457
01:32:42,620 --> 01:32:44,680
so we need to find that out

1458
01:32:44,880 --> 01:32:49,030
value there

1459
01:32:49,070 --> 01:32:50,470
right so it's

1460
01:32:50,530 --> 01:32:51,840
let's go ahead and

1461
01:32:51,850 --> 01:32:53,760
and do it

1462
01:32:53,780 --> 01:32:55,630
so how are we going to

1463
01:32:55,690 --> 01:32:57,330
how are we going to do this

1464
01:33:03,720 --> 01:33:04,610
so let's

1465
01:33:04,620 --> 01:33:09,910
let's just take a look

1466
01:33:09,920 --> 01:33:12,100
so what is it that we need to do

1467
01:33:12,170 --> 01:33:15,890
i claim that there's only one calculus step

1468
01:33:16,070 --> 01:33:18,460
i'm gonna put a star here

1469
01:33:18,510 --> 01:33:23,850
for this tangent line i have to understand what the tangent line

1470
01:33:23,890 --> 01:33:26,450
i want to figure out what the tangent line is the rest of the problem

1471
01:33:26,520 --> 01:33:29,510
is no longer calculus it's just that slope

1472
01:33:29,550 --> 01:33:30,830
that we need

1473
01:33:30,840 --> 01:33:35,830
so what's the formula for the tangent line that over here

1474
01:33:35,840 --> 01:33:37,100
it's going to be

1475
01:33:38,050 --> 01:33:39,840
minus y zero

1476
01:33:39,880 --> 01:33:43,590
is equal to and here is the magic number we already calculated it's in the

1477
01:33:44,590 --> 01:33:45,870
over there

1478
01:33:46,930 --> 01:33:50,720
minus one over x zero square

1479
01:33:50,820 --> 01:33:53,150
x minus zero

1480
01:33:53,210 --> 01:33:57,220
so this is the only bit of calculus in this problem

1481
01:34:07,640 --> 01:34:09,070
but now

1482
01:34:09,070 --> 01:34:10,440
we're not done

1483
01:34:10,490 --> 01:34:13,710
we have to finish it we have to figure out all the rest of these

1484
01:34:13,710 --> 01:34:18,390
quantities so we can figure out the area

1485
01:34:18,400 --> 01:34:23,650
all right

1486
01:34:23,710 --> 01:34:28,420
so how do we do that

1487
01:34:36,770 --> 01:34:38,270
to find this point

1488
01:34:38,290 --> 01:34:39,620
this has the name

1489
01:34:39,640 --> 01:34:41,590
we're going to find

1490
01:34:41,630 --> 01:34:44,530
the so-called interest

1491
01:34:45,720 --> 01:34:49,460
the first thing we're going to do

1492
01:34:49,530 --> 01:34:51,290
so to do that

1493
01:34:51,300 --> 01:34:52,720
what we need to do

1494
01:34:52,740 --> 01:34:57,370
it is to find where this horizontal line meets that diagonal line

1495
01:34:57,400 --> 01:35:00,010
and the equation for the x intercept

1496
01:35:00,620 --> 01:35:03,960
y equals zero

1497
01:35:03,980 --> 01:35:09,980
so we plug in y equals zero that's horizontal line we find this point

1498
01:35:10,100 --> 01:35:13,290
so let's do that and to start

1499
01:35:13,330 --> 01:35:15,720
so we get a zero minus

1500
01:35:15,810 --> 01:35:17,840
one other thing we need to know

1501
01:35:17,880 --> 01:35:19,880
we know that y zero

1502
01:35:19,930 --> 01:35:20,940
it is

1503
01:35:20,960 --> 01:35:24,830
f of x zero and f of x is one over at of this thing

1504
01:35:25,740 --> 01:35:28,660
one over here

1505
01:35:29,610 --> 01:35:33,580
that equal to minus one over zero square

1506
01:35:38,320 --> 01:35:40,370
so another to find this

1507
01:35:40,380 --> 01:35:41,610
x value

1508
01:35:41,610 --> 01:35:42,760
i have two

1509
01:35:42,810 --> 01:35:48,680
plug in one equation into the other

1510
01:35:48,740 --> 01:35:51,840
so this simplifies a bit

1511
01:35:51,950 --> 01:35:53,640
let's put

1512
01:35:53,690 --> 01:35:55,620
but see this is

1513
01:35:55,670 --> 01:35:58,470
minus tax over breaks zero squared

1514
01:35:58,480 --> 01:36:00,080
in this class

1515
01:36:00,120 --> 01:36:04,030
one over zero because the axial axial square

1516
01:36:04,070 --> 01:36:05,400
council somewhat

1517
01:36:05,420 --> 01:36:07,870
so if i put this on the other side

1518
01:36:07,880 --> 01:36:09,470
i guess

1519
01:36:09,480 --> 01:36:12,130
x divided by x square

1520
01:36:12,150 --> 01:36:13,300
is equal to

1521
01:36:13,310 --> 01:36:15,810
two over here

1522
01:36:15,860 --> 01:36:20,130
and if i then multiply through so that's what this implies and if i multiply

1523
01:36:23,080 --> 01:36:25,630
series where i get x equal to two

1524
01:36:34,910 --> 01:36:46,880
OK so i claim at this point we just calculated to x right

1525
01:36:50,080 --> 01:36:51,710
almost done

1526
01:36:51,770 --> 01:36:54,540
i need to get

1527
01:36:54,580 --> 01:36:58,230
the other one i need to get this one up here

1528
01:36:58,290 --> 01:37:01,540
now i'm going to use a very big shortcut to do that

1529
01:37:03,070 --> 01:37:04,990
so the shortcut

1530
01:37:05,090 --> 01:37:11,680
two the y intercept is are you going it

1531
01:37:16,010 --> 01:37:17,760
to use symmetry

1532
01:37:19,380 --> 01:37:24,890
and i claim i can stare at this

1533
01:37:25,030 --> 01:37:28,490
i can look at that and i know the formula for the y intercept

1534
01:37:28,510 --> 01:37:31,390
it's equal to

1535
01:37:31,450 --> 01:37:34,990
two eyes

1536
01:37:34,990 --> 01:37:37,070
that's what that one

1537
01:37:37,110 --> 01:37:39,240
so this one is true y zero

1538
01:37:39,300 --> 01:37:41,620
and the reason i know this

1539
01:37:41,660 --> 01:37:45,660
is the following so here's the symmetry of the situation which is

1540
01:37:45,680 --> 01:37:51,300
not completely correct it to kind of mirror symmetry around the diagonal

1541
01:37:51,380 --> 01:37:54,070
it involves the exchange

1542
01:37:54,110 --> 01:37:57,470
x y

1543
01:37:57,470 --> 01:38:00,340
with y

1544
01:38:00,340 --> 01:38:03,720
the trading the roles of x and y so the symmetry that i'm using is

1545
01:38:03,720 --> 01:38:06,720
that any formula get that involves access and wise

1546
01:38:06,720 --> 01:38:10,490
if i treat all the axes and replace them by wise and trade otherwise replaced

1547
01:38:10,490 --> 01:38:11,490
by axes

1548
01:38:11,490 --> 01:38:15,240
now having a correct formula on the other ways of everywhere i see why

1549
01:38:15,410 --> 01:38:21,200
making next everywhere cnxn naked why this which will take place so why is that

1550
01:38:21,260 --> 01:38:25,410
that's because the that's just an accident of this equation

1551
01:38:25,470 --> 01:38:30,550
that's because

1552
01:38:30,550 --> 01:38:34,200
so the cemetery explained

1553
01:38:34,430 --> 01:38:44,200
is the equation is y equals one over

1554
01:38:44,320 --> 01:38:49,390
but that's the same thing as x y equals one final by acts

1555
01:38:49,390 --> 01:38:53,100
how people moved around the network

1556
01:38:53,120 --> 01:38:54,530
so you

1557
01:38:58,260 --> 01:38:59,160
is that

1558
01:39:05,360 --> 01:39:09,550
we do this for the data graph

1559
01:39:10,080 --> 01:39:13,830
there's always i was so

1560
01:39:13,860 --> 01:39:16,190
we're in this

1561
01:39:16,240 --> 01:39:20,530
some very interesting problems and this is is one of the system is free of

1562
01:39:20,530 --> 01:39:22,320
those trying

1563
01:39:22,350 --> 01:39:24,390
to communicate back to korea

1564
01:39:25,870 --> 01:39:30,050
time problems this first

1565
01:39:32,410 --> 01:39:38,950
some of what the things that help us

1566
01:39:39,260 --> 01:39:41,310
they come to your

1567
01:39:41,480 --> 01:39:44,360
and when you want to go full

1568
01:39:45,520 --> 01:39:49,780
well i don't so

1569
01:39:49,790 --> 01:39:51,540
it is personal

1570
01:39:51,550 --> 01:39:53,660
more well first of all

1571
01:39:53,820 --> 01:39:57,970
of course here is the some of this

1572
01:39:58,000 --> 01:39:59,270
well i

1573
01:39:59,280 --> 01:40:01,920
talk about west africa on

1574
01:40:01,930 --> 01:40:06,550
people charge mobile phones without electricity

1575
01:40:10,680 --> 01:40:12,440
the central station

1576
01:40:12,450 --> 01:40:15,380
public buildings that

1577
01:40:18,030 --> 01:40:19,170
well it

1578
01:40:20,410 --> 01:40:24,930
so we're going to see what the intersection of these is to

1579
01:40:25,100 --> 01:40:26,380
the first place

1580
01:40:26,400 --> 01:40:27,360
the other

1581
01:40:27,380 --> 01:40:30,700
this is important to know

1582
01:40:30,710 --> 01:40:33,600
and when controversy

1583
01:40:36,550 --> 01:40:37,770
OK a

1584
01:40:39,470 --> 01:40:42,440
well in the two thousand six

1585
01:40:42,830 --> 01:40:53,670
so you want the american sociological review all social isolation america that one question was

1586
01:40:53,670 --> 01:40:55,460
discussed before and that's where

1587
01:40:55,480 --> 01:40:59,180
and are

1588
01:40:59,190 --> 01:41:01,810
i found that the number of people

1589
01:41:01,820 --> 01:41:04,390
had declined for me to point nine

1590
01:41:04,410 --> 01:41:08,820
nineteen forty two point one two thousand five

1591
01:41:08,840 --> 01:41:12,790
and twenty three percent of americans were socialists

1592
01:41:12,810 --> 01:41:17,570
and this created a great deal of media morning radio

1593
01:41:17,590 --> 01:41:19,320
actually i don't know

1594
01:41:19,340 --> 01:41:24,180
what i like about sociology journal

1595
01:41:24,200 --> 01:41:34,060
in fact current question social graces the of translation

1596
01:41:34,080 --> 01:41:44,720
different question for you personally i think the level of the same year is a

1597
01:41:44,730 --> 01:41:45,760
in seventeen

1598
01:41:45,770 --> 01:41:49,410
for more see just sort

1599
01:41:49,430 --> 01:41:51,210
the state

1600
01:41:51,220 --> 01:41:57,760
but question sometimes referred to as the are things falling apart from the decision to

1601
01:42:01,310 --> 01:42:09,770
capitalism machine socialisation socialism and to change and the reason is the use of this

1602
01:42:11,840 --> 01:42:14,460
frances cairncross

1603
01:42:14,480 --> 01:42:18,430
on nineteen ninety six experimental data this

1604
01:42:18,430 --> 01:42:21,060
i would say that i still matter

1605
01:42:21,070 --> 01:42:23,480
and this i actually like

1606
01:42:23,600 --> 01:42:26,090
on the data format

1607
01:42:26,110 --> 01:42:30,910
so we go in the english channel once more

1608
01:42:32,530 --> 01:42:39,140
and given stuff that's all the food is that this is like

1609
01:42:39,160 --> 01:42:44,970
and this is very in the government national made and you have a few people

1610
01:42:45,000 --> 01:42:49,300
he was the same sex in the movie but it's

1611
01:42:49,330 --> 01:42:54,920
i see that we usually don't use these processes

1612
01:42:59,800 --> 01:43:00,690
but what to do

1613
01:43:00,880 --> 01:43:06,930
this is part of the new york times and changed use my name

1614
01:43:06,930 --> 01:43:07,810
this is

1615
01:43:07,840 --> 01:43:09,310
really wants

1616
01:43:10,780 --> 01:43:14,350
ten years ago so what's going on

1617
01:43:14,400 --> 01:43:21,520
i just to times it is also this but we live something

1618
01:43:22,520 --> 01:43:26,860
so there's an more about what happening from the into the

1619
01:43:26,890 --> 01:43:32,680
all of being close to be isolated four the numbers before

1620
01:43:32,700 --> 01:43:33,980
following along

1621
01:43:34,000 --> 01:43:36,480
actually for the shape of the

1622
01:43:36,500 --> 01:43:38,560
well these two

1623
01:43:38,580 --> 01:43:41,320
but this is a one

1624
01:43:41,330 --> 01:43:47,260
you are i am making a density is really

1625
01:43:47,320 --> 01:43:49,770
is that in

1626
01:43:49,790 --> 01:43:52,050
and we are actually not just

1627
01:43:52,070 --> 01:43:57,000
the new social network which is more informative

1628
01:43:57,020 --> 01:43:59,090
that goes some of the

1629
01:43:59,110 --> 01:44:00,810
one year

1630
01:44:00,820 --> 01:44:04,110
are produced for the

1631
01:44:04,130 --> 01:44:05,560
change doesn't want to

1632
01:44:05,570 --> 01:44:08,390
it's a great way to work in the future

1633
01:44:08,430 --> 01:44:14,600
and i wonder using semantic analysis and stuff like that

1634
01:44:14,900 --> 01:44:19,700
but i work many chances and questions

1635
01:44:19,710 --> 01:44:25,880
to be extremely careful online because they were interested in the question of the same

1636
01:44:28,660 --> 01:44:29,960
so people

1637
01:44:29,970 --> 01:44:34,200
social networks the world they but it's a

1638
01:44:34,270 --> 01:44:36,430
first formats

1639
01:44:37,010 --> 01:44:42,900
and if you look at how many people have not seen

1640
01:44:43,530 --> 01:44:45,950
this all of

1641
01:44:46,100 --> 01:44:50,000
if you look at the social network is not a social network

1642
01:44:50,050 --> 01:44:51,800
it so that e

1643
01:44:52,090 --> 01:45:00,300
will only going to to grow through

1644
01:45:00,310 --> 01:45:05,210
so that social classes problems in part due to surgery

1645
01:45:05,410 --> 01:45:11,880
also about its the root of the network and

1646
01:45:15,770 --> 01:45:24,210
right so people want to force network individuals individuals organizing unit

1647
01:45:24,260 --> 01:45:27,100
for organisation

1648
01:45:28,090 --> 01:45:29,860
and that's what we have

1649
01:45:29,890 --> 01:45:32,310
as someone was it was carried

1650
01:45:32,320 --> 01:45:35,420
the first of those agencies that we

1651
01:45:36,640 --> 01:45:39,380
in person-to-person contact

1652
01:45:39,470 --> 01:45:41,890
so place to place

1653
01:45:41,910 --> 01:45:49,570
the twenty news at ten this question is that for

1654
01:45:49,580 --> 01:45:54,210
also we have to move around twenty one community

1655
01:45:54,210 --> 01:45:57,670
under the assumption that all all hypothesis are equally likely

1656
01:45:57,680 --> 01:45:59,480
to begin with ok

1657
01:46:01,120 --> 01:46:04,820
the other thing that's a little bit interesting about this formulation

1658
01:46:04,820 --> 01:46:08,050
is that it spells out exactly what your assumptions have been

1659
01:46:08,140 --> 01:46:12,730
ok the data drawn heidi we had gaussian noise

1660
01:46:12,880 --> 01:46:17,220
applied on top of the true some true hypothesis from the hypothesis

1661
01:46:17,230 --> 01:46:21,600
space and this gaussian noise has the same standard deviation for

1662
01:46:21,610 --> 01:46:25,670
all the examples ok so under these assumptions the mean squared

1663
01:46:25,680 --> 01:46:29,660
error is the right error function in the sense that it is

1664
01:46:29,720 --> 01:46:32,810
giving you is going to give you the same hypothesis smacks

1665
01:46:33,010 --> 01:46:37,810
likelihood notice also that if your data violates these assumptions

1666
01:46:38,180 --> 01:46:41,830
then in some sense using a the sum-squared error is not the right

1667
01:46:41,840 --> 01:46:43,970
thing to do okay in particular error

1668
01:46:44,320 --> 01:46:48,610
if you have if let's say standard deviations that depend on

1669
01:46:48,620 --> 01:46:52,970
the input ok so that some examples are affected by more noise

1670
01:46:52,970 --> 01:46:55,950
than others then using the squared error is the wrong thing

1671
01:46:55,950 --> 01:46:58,370
to do and what you should do instead of to do a little bit of

1672
01:46:58,370 --> 01:47:02,460
math to figure out quite easily is to weight the examples differently

1673
01:47:02,700 --> 01:47:06,890
ok and perhaps draw some examples that are affected by to

1674
01:47:06,900 --> 01:47:15,290
much noise in order to to fit ok so i'm

1675
01:47:18,110 --> 01:47:20,500
not going to show you a little picture here

1676
01:47:20,810 --> 01:47:25,830
ok q kind of represent this process ok

1677
01:47:25,910 --> 01:47:29,300
and this is going to establish a link to sort of a general class

1678
01:47:29,300 --> 01:47:32,570
of machine learning algorithms called graphical models

1679
01:47:32,830 --> 01:47:36,490
ok i don't know much to see this pictures i'm doing this week

1680
01:47:36,490 --> 01:47:38,960
but i figured that it might be a good idea for you to

1681
01:47:38,960 --> 01:47:41,890
actually visualize things this way least once

1682
01:47:42,060 --> 01:47:46,540
ok so this is a cartoon of how the data as being generated in the

1683
01:47:46,550 --> 01:47:51,110
case case of linear regression that we've talked about ok and there

1684
01:47:51,110 --> 01:47:54,390
are the assumption that we talked about so we have some inputs

1685
01:47:54,820 --> 01:47:59,510
there's a random variable here x ok that's drawn from some distribution

1686
01:47:59,520 --> 01:48:02,820
p of x so each note here is going to be

1687
01:48:03,230 --> 01:48:08,680
some kind of a random variable ok and the errors are going to show

1688
01:48:08,690 --> 01:48:11,840
sort of influences between these random variables

1689
01:48:12,220 --> 01:48:16,130
and at each of these nouns are going to have a conditional distribution

1690
01:48:16,140 --> 01:48:18,310
of that particular now given its parents

1691
01:48:18,520 --> 01:48:22,070
ok so that's not parents is just drawn from some distribution

1692
01:48:22,080 --> 01:48:26,960
which is unknown ok epsom on the noise

1693
01:48:27,290 --> 01:48:31,020
is also has no parents is drawn from a normal distribution of mean

1694
01:48:31,030 --> 01:48:34,760
zero and some variance sigma that we have some standard deviation

1695
01:48:34,770 --> 01:48:36,620
sigma that we don't really know ok

1696
01:48:38,260 --> 01:48:43,560
and w here under these assumptions that we use so far is

1697
01:48:43,570 --> 01:48:49,350
fixed but unknown ok and so we're going to try to use this model

1698
01:48:49,360 --> 01:48:54,190
to infer what w might be given the evidence as being present

1699
01:48:54,710 --> 01:49:00,580
ok and the output y is in fact the deterministic note

1700
01:49:00,820 --> 01:49:03,390
that does it operation in this case

1701
01:49:03,710 --> 01:49:07,850
where it takes the output of the hypothesis that depends on w

1702
01:49:07,860 --> 01:49:12,410
and x and that's the noise comes from maps ok

1703
01:49:15,390 --> 01:49:20,620
so in this case some of the variables are observed some of the

1704
01:49:20,630 --> 01:49:25,160
variables are on observe ok we can see x and we can see why

1705
01:49:25,830 --> 01:49:29,670
but we don't actually c epsilon nor do we see w

1706
01:49:30,370 --> 01:49:35,180
ok and we actually don't really know the parameter of the probability

1707
01:49:35,180 --> 01:49:37,300
distribution that governs epsilon either

1708
01:49:38,020 --> 01:49:41,500
so this is going to be a very typical case where you have data

1709
01:49:41,510 --> 01:49:44,800
you have some observed variables you kind of know there

1710
01:49:44,810 --> 01:49:47,120
are some other variables that are important

1711
01:49:47,270 --> 01:49:50,760
but you don't see them those are called hidden or latent variables

1712
01:49:50,760 --> 01:49:53,360
and a lot of the time we're going to try to build the model that

1713
01:49:53,360 --> 01:49:56,560
infers these variables the probability distributions and the

1714
01:49:56,570 --> 01:50:02,210
values from data ok the other thing that's interesting about this

1715
01:50:02,220 --> 01:50:06,170
model is that it points out places where we could make more interesting

1716
01:50:06,180 --> 01:50:10,650
assumptions ok so for example here we have assumed that w

1717
01:50:10,900 --> 01:50:14,440
is fixed by know ok because when you're maximum likelihood but

1718
01:50:14,450 --> 01:50:16,910
now let's assume that w f the random variable

1719
01:50:17,160 --> 01:50:21,120
case of w as a random variable then it might be drawn from some

1720
01:50:21,130 --> 01:50:24,710
probability distribution ok then we're moving towards beijing

1721
01:50:24,720 --> 01:50:28,770
setting where we actually have priors over the parameters

1722
01:50:28,910 --> 01:50:33,110
and we can use the data to infer a posterior probability distribution

1723
01:50:33,120 --> 01:50:35,140
over the parameters ok

1724
01:50:35,140 --> 01:50:36,300
when you see

1725
01:50:36,360 --> 01:50:40,270
but if you plug in the numbers he needs to be astonishingly close to flat

1726
01:50:40,530 --> 01:50:41,600
but not quite

1727
01:50:41,650 --> 01:50:44,690
because if it was exactly zero

1728
01:50:46,030 --> 01:50:50,620
then if not to be point one

1729
01:50:50,620 --> 01:50:53,110
so this is the flatness problem

1730
01:50:53,150 --> 01:50:57,400
you quickly find that very early on one minus some again to be less than

1731
01:50:57,400 --> 01:51:01,880
ten to the minus sixty this is like balancing paying ceylon its the

1732
01:51:01,890 --> 01:51:07,630
coming back years later in still finding defensive that is balancing on stick

1733
01:51:07,640 --> 01:51:10,390
it's obviously quite unlike

1734
01:51:10,440 --> 01:51:12,490
that's the flatness

1735
01:51:12,520 --> 01:51:16,620
the other problem is the eyes and problem remember there are different type of horizons

1736
01:51:16,620 --> 01:51:22,750
in cosmology since lights travel the final speed the universe is expanding you can define

1737
01:51:22,760 --> 01:51:29,550
the particle horizon which is the maximum ca moving distance light can have propagated between

1738
01:51:29,550 --> 01:51:34,440
some time initial to sometimes fine so to do that you need to write an

1739
01:51:34,450 --> 01:51:39,910
integral you put a of team here in these kind of equation counts for all

1740
01:51:39,910 --> 01:51:44,060
the past expansion history of the universe remember it's function is that it could be

1741
01:51:44,060 --> 01:51:48,880
we may not have been constant the time because different components at different times started

1742
01:51:48,880 --> 01:51:52,630
dominating so this is a of the may like different powers of t and can

1743
01:51:52,630 --> 01:51:57,150
even go exponentially if you are dominated by the cosmological constant

1744
01:51:57,450 --> 01:52:02,380
on the other hand you can define the hubble arise remember se over HRC over

1745
01:52:02,620 --> 01:52:03,940
zero which is the

1746
01:52:03,950 --> 01:52:05,360
the age of the

1747
01:52:05,390 --> 01:52:06,510
universe today

1748
01:52:06,530 --> 01:52:12,190
and for an ottoman expansion is thirty these quantity and see over dates

1749
01:52:12,200 --> 01:52:14,430
at about the same

1750
01:52:14,480 --> 01:52:17,920
but before we the expansion history thirty be worse

1751
01:52:17,960 --> 01:52:22,350
four where this function is very c of h may be very very different from

1752
01:52:22,350 --> 01:52:28,540
this one

1753
01:52:28,600 --> 01:52:30,300
so there is some problem

1754
01:52:30,320 --> 01:52:35,540
the universe is homogeneous and isotropic and very large scale considered twenty people points in

1755
01:52:35,540 --> 01:52:40,890
the cosmic microwave background there are separated by

1756
01:52:41,660 --> 01:52:43,160
times these

1757
01:52:43,170 --> 01:52:51,470
these how would this time basically fifty thousand men parts today

1758
01:52:52,810 --> 01:52:55,610
so what was the hubble radius

1759
01:52:55,610 --> 01:52:56,760
back then

1760
01:52:56,780 --> 01:53:02,040
and first and the expansion history there but reduces the problem arises

1761
01:53:02,050 --> 01:53:07,390
well if i do that i end up to this point to make about six

1762
01:53:07,390 --> 01:53:14,050
so all the points at point omega passing distance could have talk to each other

1763
01:53:15,440 --> 01:53:16,820
what is

1764
01:53:16,910 --> 01:53:22,250
these the size of the eyes and back then the cosmic background of the last

1765
01:53:22,250 --> 01:53:25,470
scattering surface can i see with my i

1766
01:53:26,410 --> 01:53:29,110
the size of the eyes and the CMB

1767
01:53:29,130 --> 01:53:32,260
it's the size of about one degree

1768
01:53:32,330 --> 01:53:34,430
in other words to look at this map

1769
01:53:34,430 --> 01:53:38,860
the typical size of the block is the size of the eyes and then it's

1770
01:53:38,860 --> 01:53:43,670
fine to that six and indeed it's more or less that

1771
01:53:43,720 --> 01:53:49,720
pedigree scale that they see this speak

1772
01:53:49,730 --> 01:53:53,210
OK and it makes sense because an upper division college took which are they could

1773
01:53:53,210 --> 01:53:58,610
have acoustic waves remember that this sound speed at that time was very close to

1774
01:53:58,610 --> 01:54:05,890
the speed of light difference by the square root of three

1775
01:54:05,920 --> 01:54:09,620
so how does this by now that is the three kelvin this point also should

1776
01:54:09,620 --> 01:54:11,040
not have to because

1777
01:54:11,060 --> 01:54:17,410
if they could not the doctor it

1778
01:54:17,420 --> 01:54:21,740
now today is different because it seems the universe has expanded and for most of

1779
01:54:21,740 --> 01:54:27,020
the life of the universe expansion was slowing down then our set of about patch

1780
01:54:27,230 --> 01:54:32,930
has grown compared to parts back then because the this function has been slowing down

1781
01:54:32,950 --> 01:54:35,470
and sold well today we can see

1782
01:54:35,490 --> 01:54:40,450
side two spots on the CMB that are the opposite end but back then the

1783
01:54:40,460 --> 01:54:42,890
could not have talked to each

1784
01:54:42,890 --> 01:54:47,650
so the last surface can be divided in forty thousand pages of degree size that

1785
01:54:47,650 --> 01:54:49,690
is forty thousand eyes

1786
01:54:49,700 --> 01:54:54,810
how could they will be about three kelvin imagine forty thousand students taking an example

1787
01:54:54,910 --> 01:55:03,100
quite complicated one returning the same example down to the commas and the mistakes

1788
01:55:03,110 --> 01:55:09,460
now you start thinking they must have talk to each other somehow right

1789
01:55:09,480 --> 01:55:13,210
then there is what is called the monopole problem in the very early universe a

1790
01:55:13,220 --> 01:55:17,100
lot of phase transition went on and you know that when you have phase transition

1791
01:55:19,650 --> 01:55:24,760
and there was the fact the model that you know if you have one that

1792
01:55:24,760 --> 01:55:29,740
was completely messed up all your reservation

1793
01:55:29,770 --> 01:55:35,360
so the figures out of the main will cause means three except accepted accepted

1794
01:55:35,360 --> 01:55:38,720
so the fact that we are here it means that the maximum there is only

1795
01:55:38,720 --> 01:55:45,110
one monopole in the entire observable universe if there's more than that were talks

1796
01:55:45,130 --> 01:55:49,850
another questions what the heck this perturbation then come from if this is the eyes

1797
01:55:49,850 --> 01:55:51,470
of OK

1798
01:55:51,520 --> 01:55:54,450
what the heck this perturbation come from

1799
01:55:54,490 --> 01:55:56,740
well created them

1800
01:55:57,050 --> 01:56:00,200
so the theory of inflation come to the rescue

1801
01:56:00,210 --> 01:56:04,330
inflation started in the eighties with all angles and is still very very active research

1802
01:56:04,330 --> 01:56:07,980
area is still a paradigm in the sense that we have proven

1803
01:56:07,990 --> 01:56:11,920
but you will see that you keep one there the sorry

