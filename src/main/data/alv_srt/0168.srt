1
00:00:00,000 --> 00:00:02,210
very strong levels of urgency

2
00:00:02,260 --> 00:00:04,650
this is the time when was

3
00:00:05,160 --> 00:00:12,550
for forty years and rediscovered in the twentieth century but dropped by team us around

4
00:00:12,550 --> 00:00:18,860
the world and people resources with the same genetic analyses on the fly that they

5
00:00:18,860 --> 00:00:24,650
want make the end of the cold fathers opportunities so what they do is basically

6
00:00:24,650 --> 00:00:26,720
the these flies and they

7
00:00:26,740 --> 00:00:28,580
this the definition

8
00:00:28,670 --> 00:00:30,310
the future b

9
00:00:30,330 --> 00:00:33,400
it how they

10
00:00:33,420 --> 00:00:36,020
and they also what makes them

11
00:00:36,410 --> 00:00:39,080
some to general be

12
00:00:39,090 --> 00:00:40,730
trees of this analysis

13
00:00:41,020 --> 00:00:44,960
it is think of him as got the call of the eyes and you can

14
00:00:44,960 --> 00:00:49,580
see that when you lies like is most likely true

15
00:00:49,740 --> 00:00:55,780
so this is the only thing i was basically assume that the organisms not just

16
00:00:56,060 --> 00:00:57,540
five years

17
00:00:57,560 --> 00:00:58,680
and then

18
00:00:58,740 --> 00:01:02,630
constraints we're going to go deeper

19
00:01:03,100 --> 00:01:04,360
one of the those

20
00:01:04,370 --> 00:01:06,800
so twenty four nineteen forty four

21
00:01:06,810 --> 00:01:08,340
someone called ivory

22
00:01:08,360 --> 00:01:10,520
with the second game show here

23
00:01:10,530 --> 00:01:14,860
in fact the instances of you

24
00:01:14,870 --> 00:01:18,550
link that is there is what is the support of

25
00:01:18,780 --> 00:01:24,570
apparently so this is a list about the DNA molecule with

26
00:01:24,580 --> 00:01:26,040
this is

27
00:01:26,070 --> 00:01:31,090
because of microscopic cancer cells so you can see that was able to prove that

28
00:01:31,090 --> 00:01:31,770
this morning

29
00:01:31,820 --> 00:01:35,640
this information is presented long duration

30
00:01:35,660 --> 00:01:38,130
and the information to to make something work

31
00:01:39,350 --> 00:01:41,760
then in nineteen eighty three

32
00:01:41,780 --> 00:01:48,110
so need more than these point

33
00:01:48,260 --> 00:01:55,170
the main goal is is really more like this is this is greater than fifty

34
00:01:55,860 --> 00:01:59,610
the guys this is one of the best teacher and then back to the old

35
00:01:59,610 --> 00:02:03,820
the vic what's discovery data

36
00:02:03,870 --> 00:02:09,150
and the structure of the article called DNA these are all the same as the

37
00:02:09,150 --> 00:02:16,310
police and was big big news because need to solve the mystery of the whole

38
00:02:16,310 --> 00:02:20,120
but not really clear what it was and we knew it was communication us here

39
00:02:20,290 --> 00:02:27,740
was made and it became more information it would start with money who sequence

40
00:02:27,750 --> 00:02:29,670
and i think we be

41
00:02:31,740 --> 00:02:35,710
it this is a particular

42
00:02:35,730 --> 00:02:40,720
the discovery binary called the genetic code so this explains

43
00:02:40,740 --> 00:02:45,590
we will show going later i would like to extend our information DNA

44
00:02:45,780 --> 00:02:49,510
it is used to make something because we knew that you know what is in

45
00:02:49,520 --> 00:02:55,880
store information transmitted generation to derive and the goal was to keep the article

46
00:02:55,930 --> 00:02:59,260
that your teacher to contact information

47
00:02:59,280 --> 00:03:01,750
these things called genes

48
00:03:01,770 --> 00:03:07,120
in seventy seven the centre was really good because you can is the number of

49
00:03:07,960 --> 00:03:17,020
discovery i mean crystallinity of sequencing sequencing is who used which is

50
00:03:17,040 --> 00:03:18,600
which refers to the

51
00:03:18,610 --> 00:03:20,690
the information given

52
00:03:20,700 --> 00:03:23,090
and it contains information

53
00:03:23,110 --> 00:03:24,800
but can be written sequence

54
00:03:24,820 --> 00:03:27,400
and sequencing is really

55
00:03:27,420 --> 00:03:32,720
discover the there something to use it to read the DNA of

56
00:03:32,730 --> 00:03:34,020
OK for

57
00:03:34,030 --> 00:03:40,140
short strands of DNA and computer started to twenty get because as early as nineteen

58
00:03:40,140 --> 00:03:47,570
eighty two that have been created to go to bed was created this this was

59
00:03:47,570 --> 00:03:49,110
pretty store

60
00:03:49,130 --> 00:03:55,570
the sequences of DNA that we query these sequencing technologies so the beginning it was

61
00:03:55,630 --> 00:03:58,000
a small database contains few

62
00:03:58,160 --> 00:04:04,160
hundreds of thousands of sequences in between

63
00:04:04,210 --> 00:04:06,760
in nineteen eighty in today

64
00:04:06,780 --> 00:04:10,100
the technologies to improve the and the number

65
00:04:10,110 --> 00:04:13,270
sequences dramatically improved increases

66
00:04:13,290 --> 00:04:16,490
so i don't know the that contains something right

67
00:04:16,500 --> 00:04:19,880
on the other fifty million sequences for all

68
00:04:19,900 --> 00:04:21,580
fifty billion

69
00:04:21,590 --> 00:04:27,350
that is used to increase pensions is increasing this is a

70
00:04:27,360 --> 00:04:28,690
one of the

71
00:04:28,700 --> 00:04:33,600
o point where countries are starting to to play a role in the so that

72
00:04:33,650 --> 00:04:35,120
we we could distort information

73
00:04:35,540 --> 00:04:37,670
it turns out

74
00:04:37,690 --> 00:04:39,700
books anymore

75
00:04:39,710 --> 00:04:42,520
then the way the

76
00:04:42,570 --> 00:04:45,170
the case of the twentieth century so you this

77
00:04:45,180 --> 00:04:48,230
one the sequencing of the human genome so what is that

78
00:04:48,250 --> 00:04:53,500
well we we knew that all organisms contain DNA contains some information to make the

79
00:04:53,500 --> 00:04:59,040
organisms and in the nineteen eighties there was a big part of saying well let's

80
00:04:59,040 --> 00:05:01,730
try to look at our own to sell to look at

81
00:05:01,750 --> 00:05:06,330
how you manage what human it's paid off to reserve units in

82
00:05:06,340 --> 00:05:11,020
and it was a big technical challenges because a lot of money but it was

83
00:05:11,400 --> 00:05:14,920
in two thousand three there was the addition of the human genome

84
00:05:14,940 --> 00:05:18,690
at some point this must be and his

85
00:05:18,700 --> 00:05:23,840
long period because you know since the beginning people discover that there is lots of

86
00:05:23,840 --> 00:05:28,410
charity there was some information in DNA and so know we we you can download

87
00:05:28,500 --> 00:05:29,730
neural letter

88
00:05:29,740 --> 00:05:34,590
the sequence of the human which is supposed to be so it was supposed to

89
00:05:34,590 --> 00:05:37,800
be installed all the energy in some sense if you have a single

90
00:05:38,260 --> 00:05:43,310
vision of as you like DNA is the programme and then

91
00:05:43,340 --> 00:05:45,300
this is a computer

92
00:05:45,700 --> 00:05:48,380
parameter something because of was that

93
00:05:48,390 --> 00:05:53,260
i received five course but you want to be the program director enough you

94
00:05:56,650 --> 00:06:00,800
so of course for is not so simple

95
00:06:00,930 --> 00:06:05,960
we have the program but was that everything is not written in the programming was

96
00:06:05,960 --> 00:06:07,800
computer is not so simple

97
00:06:07,820 --> 00:06:12,920
but these things because we have everything we wanted but you know we need to

98
00:06:12,920 --> 00:06:14,550
analyse it and this is where

99
00:06:14,560 --> 00:06:19,320
the mathematics computer science of the role to play because now there is one of

100
00:06:19,340 --> 00:06:21,800
the two sequences

101
00:06:22,410 --> 00:06:27,430
well it's not just what some some empirical knowledge is organising

102
00:06:27,490 --> 00:06:33,810
so today one thousand six i would say that the transition shop of course we

103
00:06:33,810 --> 00:06:39,380
are at the beginning of boston meteorite where you're right where we need to to

104
00:06:39,380 --> 00:06:47,390
analyse the data in which there are many more coming from this which is project

105
00:06:47,400 --> 00:06:53,260
OK so i want to make sure that some of you might not know about

106
00:06:53,260 --> 00:06:58,200
the DNA proteins genes know very little about the

107
00:06:59,750 --> 00:07:01,240
molecular biology

108
00:07:01,260 --> 00:07:05,130
just to give you a more precise picture of what these concepts are

109
00:07:05,140 --> 00:07:08,520
what is what one of the things that

110
00:07:08,540 --> 00:07:16,900
so i will start with very things first the first thing here that everything was

111
00:07:16,900 --> 00:07:22,220
be alright but only things he is very dangerous

112
00:07:22,220 --> 00:07:23,620
no relation

113
00:07:27,240 --> 00:07:29,520
i'm not

114
00:07:38,880 --> 00:07:43,930
there you know

115
00:07:44,010 --> 00:07:47,600
there are

116
00:07:53,880 --> 00:07:58,810
a lot too

117
00:08:00,680 --> 00:08:02,580
and is

118
00:08:02,690 --> 00:08:05,600
and we're all this

119
00:08:12,640 --> 00:08:15,550
our goal is

120
00:08:18,950 --> 00:08:19,830
of the

121
00:08:24,340 --> 00:08:28,500
and there

122
00:08:48,500 --> 00:08:55,510
is that easy

123
00:09:11,720 --> 00:09:16,900
same this is

124
00:09:16,920 --> 00:09:21,180
all of these

125
00:09:26,950 --> 00:09:28,070
for moment

126
00:09:36,460 --> 00:09:41,190
i mean field

127
00:10:08,530 --> 00:10:09,640
yes you

128
00:10:38,480 --> 00:10:40,400
in the

129
00:10:44,930 --> 00:10:48,200
we well

130
00:11:05,470 --> 00:11:10,870
i think is

131
00:11:11,920 --> 00:11:14,150
five points

132
00:11:23,400 --> 00:11:25,970
in the end

133
00:11:26,270 --> 00:11:28,450
why not

134
00:11:31,140 --> 00:11:35,100
you need make sure that

135
00:11:35,240 --> 00:11:41,780
it is we should not

136
00:11:41,800 --> 00:11:43,130
and a

137
00:11:55,410 --> 00:11:59,340
well known

138
00:12:05,970 --> 00:12:11,780
prior work

139
00:12:11,780 --> 00:12:15,870
because you want to do this estimation that's what you want to do it sequentially

140
00:12:15,870 --> 00:12:21,360
in time on this important for many applications in particle filtering like this kind of

141
00:12:21,360 --> 00:12:22,890
talk in public

142
00:12:22,900 --> 00:12:26,560
each time you and your acceleration of trying to approximate

143
00:12:27,280 --> 00:12:31,330
according to the net playing each time you have new measurements you want to add

144
00:12:31,330 --> 00:12:32,890
a new knowledge about

145
00:12:32,930 --> 00:12:35,670
position the time you don't want

146
00:12:35,710 --> 00:12:37,900
you're not interested in estimating

147
00:12:37,910 --> 00:12:40,390
according for the target for what

148
00:12:40,400 --> 00:12:44,610
except of acceleration you start giving you know if you want to take your knowledge

149
00:12:44,610 --> 00:12:46,720
about about

150
00:12:47,910 --> 00:12:51,160
so not mathematically also there's no problem

151
00:12:51,180 --> 00:12:55,090
well essentially there very simple

152
00:12:55,240 --> 00:12:57,460
relation between

153
00:12:57,510 --> 00:13:01,440
the distribution of it in price at time n minus one

154
00:13:01,480 --> 00:13:05,570
of the distribution of input by

155
00:13:05,630 --> 00:13:09,330
this is again given by the very

156
00:13:09,390 --> 00:13:14,080
very important to share the joint distribution of time and it's going to short time

157
00:13:14,080 --> 00:13:16,230
and and minus one

158
00:13:16,280 --> 00:13:18,770
times y

159
00:13:18,820 --> 00:13:21,720
of the state i and even my book

160
00:13:21,720 --> 00:13:24,400
but i like your salvation

161
00:13:24,500 --> 00:13:28,380
basically what i and divided by the predictive

162
00:13:28,400 --> 00:13:32,710
of your television while you posted nothing complex about

163
00:13:34,610 --> 00:13:36,370
all in parties

164
00:13:36,450 --> 00:13:41,590
i will often write in the following slide this fallacious photos we

165
00:13:41,620 --> 00:13:43,240
basically this

166
00:13:43,270 --> 00:13:48,450
which is independent of x and together with a set of first time

167
00:13:48,500 --> 00:13:52,260
is simply the proportional to the first time i just want

168
00:13:52,270 --> 00:13:53,710
but i do

169
00:13:54,890 --> 00:13:57,660
so is that these things

170
00:13:57,710 --> 00:14:00,470
where well i was going to do it is very simple

171
00:14:00,610 --> 00:14:12,020
a letter at the end of

172
00:14:12,040 --> 00:14:17,150
you're right really is to use the right thing penalty do which is that there

173
00:14:17,360 --> 00:14:21,150
not a call to a divided by b

174
00:14:21,150 --> 00:14:25,970
it not difficult then you know that

175
00:14:25,980 --> 00:14:30,890
he of the and it is the eighth divided by p of

176
00:14:30,900 --> 00:14:34,530
you that the numerator and the denominator

177
00:14:34,590 --> 00:14:36,450
is very simple

178
00:14:36,490 --> 00:14:42,680
then you remember that these guys the joint distribution of the state of water and

179
00:14:42,680 --> 00:14:45,460
on television while more than one

180
00:14:45,480 --> 00:14:46,380
is the

181
00:14:46,390 --> 00:14:48,800
you are kind likely

182
00:14:48,830 --> 00:14:51,740
they are these food

183
00:14:51,760 --> 00:14:54,780
well this but it's alright still

184
00:14:54,920 --> 00:15:00,460
because the the simple physical assumption about x y or

185
00:15:00,470 --> 00:15:02,890
these guys were

186
00:15:02,910 --> 00:15:04,200
that simply

187
00:15:04,630 --> 00:15:09,000
followed by that i don't think the only way get that this is simple expression

188
00:15:09,140 --> 00:15:10,760
this is the expression

189
00:15:10,810 --> 00:15:13,230
we're going to use all the time

190
00:15:13,280 --> 00:15:15,920
this guy is

191
00:15:16,090 --> 00:15:18,380
if you look at the literature

192
00:15:19,960 --> 00:15:23,010
the lead character of the

193
00:15:23,030 --> 00:15:28,140
it is very disturbing that that maybe ninety percent of the paper

194
00:15:28,190 --> 00:15:30,440
there is

195
00:15:30,490 --> 00:15:36,710
show you the recursion not for the joint distribution of the whole set x time

196
00:15:36,710 --> 00:15:42,700
one two but they always establishes the office every they look at the

197
00:15:42,700 --> 00:15:47,890
recursion saying well the joint distribution of x

198
00:15:47,900 --> 00:15:52,560
the marginal distributions story of x then given y one y

199
00:15:54,230 --> 00:15:55,570
you can do

200
00:15:55,580 --> 00:15:57,610
they decompose this

201
00:15:57,610 --> 00:15:58,440
basically the

202
00:15:58,450 --> 00:16:05,620
if the distribution of this thing is that they understand what that

203
00:16:05,640 --> 00:16:11,470
this is a direct consequence of the notion of train on the joint distribution you

204
00:16:11,470 --> 00:16:13,140
just need to integrate all

205
00:16:13,860 --> 00:16:18,930
latent bible x one select x minus one or what i'm saying is that you

206
00:16:18,930 --> 00:16:22,710
should think about all the particle method i'm going to describe

207
00:16:22,790 --> 00:16:25,160
you should anything

208
00:16:25,200 --> 00:16:27,720
two that you should be able to make the

209
00:16:27,840 --> 00:16:31,530
on the mountain all this now this is

210
00:16:31,540 --> 00:16:37,320
this the numerical methods are made to approximate the the joint distribution

211
00:16:37,370 --> 00:16:44,300
the state xk one one four

212
00:16:44,470 --> 00:16:50,010
but because

213
00:16:50,070 --> 00:16:52,050
there are many applications

214
00:16:52,050 --> 00:16:54,840
machine is probably fine

215
00:16:54,860 --> 00:16:58,270
and so the the idea of thing to do in many respects it is the

216
00:16:58,290 --> 00:16:59,730
print message saying thank

217
00:17:00,460 --> 00:17:04,150
but things happen seems to be okay we're going to carry on

218
00:17:04,210 --> 00:17:06,300
but to find the body

219
00:17:06,340 --> 00:17:09,210
what you really want crash is machine horror

220
00:17:09,270 --> 00:17:12,210
because then they would bother to report

221
00:17:12,270 --> 00:17:17,860
so this is tricky conflict and try to do the right thing

222
00:17:17,880 --> 00:17:19,790
we also want to check so

223
00:17:19,800 --> 00:17:22,000
nowadays if you've got this running

224
00:17:22,130 --> 00:17:27,020
pretty cheap to run fusion gets stuck in a kind of doing something

225
00:17:27,050 --> 00:17:29,110
he will actually use that trace back

226
00:17:29,130 --> 00:17:32,500
you're watching the stuff is where we started

227
00:17:33,440 --> 00:17:40,710
then sort you know where you go aged how long to wait a while

228
00:17:40,710 --> 00:17:44,020
and we have page ownership jackets for the memory management

229
00:17:44,020 --> 00:17:48,050
which again designed for cases where memory management

230
00:17:48,070 --> 00:17:50,210
so we try to put all of these things in

231
00:17:50,230 --> 00:17:51,920
to find bugs

232
00:17:51,920 --> 00:17:56,000
without going through the the traditional practices of the user reporting

233
00:17:56,020 --> 00:17:59,590
my machine crashed i don't know why

234
00:17:59,820 --> 00:18:05,360
we have scaling challenge

235
00:18:05,400 --> 00:18:09,860
it's fairly obvious that we had problems already with a large number of users

236
00:18:11,020 --> 00:18:13,250
but is not going to work

237
00:18:14,670 --> 00:18:17,440
incredibly large number of users

238
00:18:17,460 --> 00:18:21,630
so if we we suddenly beat microsoft on the desktop we're going to need a

239
00:18:21,630 --> 00:18:23,840
new bug reporting

240
00:18:23,900 --> 00:18:28,710
we should probably right it before the user right fist

241
00:18:28,730 --> 00:18:32,170
together they get to be screaming in stressed anyway the point with we need to

242
00:18:32,170 --> 00:18:36,980
get a nice friendly and without any problem

243
00:18:37,000 --> 00:18:38,570
statistical debugging

244
00:18:38,570 --> 00:18:40,150
we really need to do more

245
00:18:40,170 --> 00:18:42,110
as you get more and more users

246
00:18:42,150 --> 00:18:47,170
statistical statistical data becomes more and more violence

247
00:18:47,270 --> 00:18:52,110
so the distributions of shipping tools like small with that track your work on these

248
00:18:53,630 --> 00:18:54,670
and then you got

249
00:18:54,670 --> 00:18:57,590
a unique identifier the change

250
00:18:57,610 --> 00:19:02,480
i designed just to say this is this this machine specification

251
00:19:02,480 --> 00:19:05,730
this is my magic number not to contain a personal right

252
00:19:05,750 --> 00:19:07,840
again the conflict between

253
00:19:07,840 --> 00:19:09,190
personal privacy

254
00:19:09,210 --> 00:19:13,670
and knowing the order of machine note to go

255
00:19:13,690 --> 00:19:16,190
so the distribution tend to say

256
00:19:16,230 --> 00:19:20,290
do you want me to register your machine to encourage people to do

257
00:19:20,300 --> 00:19:26,960
don't do it automatically when possible to install but

258
00:19:26,960 --> 00:19:28,980
it's also getting more interconnected

259
00:19:29,030 --> 00:19:31,170
use the case that

260
00:19:31,210 --> 00:19:33,270
kind of this is space

261
00:19:33,300 --> 00:19:34,300
there you go

262
00:19:34,320 --> 00:19:38,800
he looked bargain kind of like that is by

263
00:19:38,800 --> 00:19:41,300
more and more these things are interlinked

264
00:19:41,320 --> 00:19:42,770
so today

265
00:19:42,800 --> 00:19:47,210
you take memory card your PC non-technical user

266
00:19:47,230 --> 00:19:49,170
and it doesn't work

267
00:19:49,190 --> 00:19:52,290
we get about what the kernel bugs acceleration just something like

268
00:19:52,300 --> 00:19:57,500
i'd it's memory stick for my calendar from my camera nothing happens

269
00:19:57,840 --> 00:20:00,550
could be a kind of

270
00:20:00,570 --> 00:20:04,380
could be about something like you which manages the

271
00:20:04,400 --> 00:20:08,710
kind of the low level operations device files and devices

272
00:20:08,770 --> 00:20:13,320
could be a bugging in device which handles getting it into the desktop

273
00:20:13,360 --> 00:20:16,440
could be back in the desktop software which i do

274
00:20:16,460 --> 00:20:18,920
it's not KB

275
00:20:18,980 --> 00:20:20,130
far manager

276
00:20:20,130 --> 00:20:22,770
one of the wind

277
00:20:22,770 --> 00:20:27,440
could the use of mister think it may be tick box they don't do that

278
00:20:27,500 --> 00:20:30,690
there's no much harder to figure out who's but something

279
00:20:30,710 --> 00:20:33,940
which again pulls facts with the bugs alan davies

280
00:20:33,940 --> 00:20:37,520
i can't look you can argue but still not enough

281
00:20:37,570 --> 00:20:42,110
and klein transferred to now

282
00:20:42,130 --> 00:20:42,820
we have

283
00:20:42,840 --> 00:20:46,050
drawing links between the kernel and the actor in the system

284
00:20:46,050 --> 00:20:50,150
because video video card used to be done entirely in user space

285
00:20:50,170 --> 00:20:54,300
very simple devices fans

286
00:20:54,300 --> 00:20:57,670
then video cards started to grow acceleration

287
00:20:57,690 --> 00:20:59,710
o very easy

288
00:20:59,750 --> 00:21:02,170
then they started to DM

289
00:21:02,190 --> 00:21:04,800
which made parallel

290
00:21:04,800 --> 00:21:10,230
today's video cards sort of memory and the memory management system

291
00:21:10,250 --> 00:21:16,300
they interact in a very complicated ways the rest of the logic of support

292
00:21:16,380 --> 00:21:18,460
there's a lot of media management support

293
00:21:18,500 --> 00:21:22,270
got got it because it you know

294
00:21:22,290 --> 00:21:25,380
one of the good sides that it should finally be able to report things like

295
00:21:25,500 --> 00:21:28,030
expected using graph

296
00:21:28,090 --> 00:21:30,900
so if your machine is an aircraft that crashes

297
00:21:30,920 --> 00:21:33,290
have to tell you why

298
00:21:33,290 --> 00:21:37,090
currently you get two red flashing lights value

299
00:21:37,290 --> 00:21:41,670
we have pack if you can read most tell you messages

300
00:21:41,710 --> 00:21:46,090
that's the most human value rather serious

301
00:21:46,110 --> 00:21:48,790
people have used

302
00:21:48,840 --> 00:21:52,590
i have had the report can be read journals

303
00:21:52,610 --> 00:21:55,900
there very very rare

304
00:21:55,960 --> 00:21:59,130
so we have three d graphics that also has this

305
00:22:03,650 --> 00:22:07,800
and we can all think is always somebody complained

306
00:22:08,520 --> 00:22:12,770
we still support the commodore amiga

307
00:22:12,800 --> 00:22:16,840
and that's the be using the people who will get upset we delete commodore amiga

308
00:22:19,290 --> 00:22:22,840
it would be cheaper to buy them both the new PC but apparently they consider

309
00:22:22,840 --> 00:22:26,790
their media is so important

310
00:22:26,800 --> 00:22:30,360
and we dropped three eight six four six all these

311
00:22:30,380 --> 00:22:33,570
questions we after answer

312
00:22:34,070 --> 00:22:35,690
that's the end of the

313
00:22:35,690 --> 00:22:38,150
bayesian learning or my fact

314
00:22:38,200 --> 00:22:43,300
might just represent some penalized likelihood if you're taking the more frequent this kind of

315
00:22:45,370 --> 00:22:49,270
and there are many ways to augment the the likelihood you can use cross validation

316
00:22:49,270 --> 00:22:54,740
approaches really fully bayesian estimators in which you try to to integrate over the parameters

317
00:22:54,930 --> 00:23:01,300
these asymptotic approximations like BIC AIC akaike information criterion but we're not going to cover

318
00:23:01,300 --> 00:23:05,770
those are now going to cover the maximum likelihood learning in the full knowledge that

319
00:23:05,770 --> 00:23:08,570
it's naive and dangerous but with the

320
00:23:08,580 --> 00:23:12,280
excuse that you have to crawl before you can want something like that

321
00:23:13,490 --> 00:23:15,060
let's now

322
00:23:15,090 --> 00:23:15,980
just see

323
00:23:15,980 --> 00:23:23,140
how maximum likelihood learning plays so you guys on this yesterday the IID assumption

324
00:23:23,160 --> 00:23:28,290
independent and identically distributed means that the likelihood of all of the data

325
00:23:28,350 --> 00:23:33,930
it's just a product over each individual deity because we see in the independent that's

326
00:23:33,930 --> 00:23:39,570
what makes products and identically distributed so they all from the same function

327
00:23:39,580 --> 00:23:40,220
i mean

328
00:23:40,220 --> 00:23:45,650
the belong likelihood maximizing the likelihood in the log likelihood are equivalent so we maximize

329
00:23:45,650 --> 00:23:48,240
the likelihood is turned into a song

330
00:23:50,520 --> 00:23:55,040
so this leads to the standard maximum likelihood estimator which just as think of the

331
00:23:55,040 --> 00:23:59,290
likelihood function now the black box there's some parameters which are not

332
00:23:59,310 --> 00:24:02,960
and that box is conditioned on fixing data set d

333
00:24:02,960 --> 00:24:06,330
which is the data that we actually have in front of the training data and

334
00:24:06,330 --> 00:24:10,030
we wanted turn the nob to maximize likely

335
00:24:10,180 --> 00:24:13,830
so this is a very commonly used in statistics and often leads to the sort

336
00:24:13,830 --> 00:24:20,840
of intuitive or appealing estimate maximum likelihood but you shouldn't be lulled into complacency by

337
00:24:20,850 --> 00:24:23,730
a beautiful test right

338
00:24:23,730 --> 00:24:28,640
just because something looks natural doesn't mean that it's the right thing to do

339
00:24:28,660 --> 00:24:34,860
and then we'll we'll see how that can cause problems in the second in particular

340
00:24:34,860 --> 00:24:40,210
we give some examples of the kind of estimators markets was was talking

341
00:24:40,230 --> 00:24:46,080
OK so let's let's just have a very simple example which is flipping coins OK

342
00:24:46,160 --> 00:24:50,750
so let's see how point which is biased

343
00:24:50,910 --> 00:24:54,930
this is an example that comes up all the time and statistics and nobody i

344
00:24:55,500 --> 00:25:01,010
has ever actually seen by course but just imagine for a moment that you have

345
00:25:01,010 --> 00:25:05,640
pointed by saying i don't exactly know how you build a course the idea is

346
00:25:05,640 --> 00:25:09,400
that when you look at some probability of coming up heads might be different than

347
00:25:09,570 --> 00:25:11,780
some probability of one minus

348
00:25:11,860 --> 00:25:18,010
and we want to estimate the bias and the way we estimate the by bias

349
00:25:18,010 --> 00:25:18,840
is by

350
00:25:18,910 --> 00:25:23,180
observing some coin heads heads tails heads heads and we take those conflicts

351
00:25:23,340 --> 00:25:25,120
the maximum likelihood estimate

352
00:25:25,970 --> 00:25:30,700
this may seem very pedantic but it's like you know your morning medicine good for

353
00:25:30,700 --> 00:25:36,120
you go through this because in more complicated situations exactly what you're doing equations apply

354
00:25:36,250 --> 00:25:41,970
everywhere and you be very concerned about your remember it's just like according to work

355
00:25:41,990 --> 00:25:46,040
so what do you write down the likelihood function as the log likelihood which is

356
00:25:46,040 --> 00:25:47,240
just start

357
00:25:47,250 --> 00:25:50,660
over the terms of the individual

358
00:25:51,690 --> 00:25:55,850
and you take the derivative of that with respect to the parameters here there's only

359
00:25:55,850 --> 00:26:01,170
one parameter which is the probability of coming an end AUC to zero

360
00:26:01,220 --> 00:26:02,990
and maximize

361
00:26:03,920 --> 00:26:07,620
that sounds easy right there are several things happening here

362
00:26:07,640 --> 00:26:11,440
in particular using the algebraic true

363
00:26:11,460 --> 00:26:17,490
which is to put the value of the observations in the exponent

364
00:26:17,510 --> 00:26:22,540
and to use all the value of the observation and one minus the value to

365
00:26:22,540 --> 00:26:24,940
select the terms that we want

366
00:26:24,960 --> 00:26:27,160
so you see what i've done that here right

367
00:26:27,170 --> 00:26:29,080
written this is a very weird way

368
00:26:29,110 --> 00:26:33,240
i've said this is to be at x is the observation either heads or tails

369
00:26:33,250 --> 00:26:35,270
wonders you

370
00:26:35,780 --> 00:26:39,900
at times one minus stated one minus x but it seems like kind of a

371
00:26:39,900 --> 00:26:41,650
ridiculous way to write

372
00:26:41,660 --> 00:26:45,730
you know but what i write this way because obviously only one of these terms

373
00:26:45,730 --> 00:26:47,210
is ever going to be

374
00:26:47,230 --> 00:26:51,790
operations if x is zero then this term is just one and this is the

375
00:26:51,790 --> 00:26:57,570
term active in one then this terms stated this is it's just one but i

376
00:26:57,810 --> 00:27:00,620
that way so that when you take a look

377
00:27:00,650 --> 00:27:02,750
you get this nice form here

378
00:27:02,770 --> 00:27:05,320
OK so this is a very common trick

379
00:27:05,330 --> 00:27:09,850
the right things in this weird notation with exponent so that when you take what

380
00:27:09,890 --> 00:27:10,920
you get these

381
00:27:10,930 --> 00:27:12,760
nice find

382
00:27:12,790 --> 00:27:13,570
OK so now

383
00:27:13,600 --> 00:27:15,430
thing to remember here

384
00:27:15,440 --> 00:27:17,040
this is the right

385
00:27:17,070 --> 00:27:18,990
we said this derivative is zero

386
00:27:19,000 --> 00:27:24,720
we get this problem estimated the maximum likelihood estimate of the bias the coil is

387
00:27:24,720 --> 00:27:30,090
the number had over the total number of clicks which is number sports number

388
00:27:31,590 --> 00:27:37,470
does anyone see a problem with this

389
00:27:37,470 --> 00:27:42,500
in particular i give you going to tell you this going by some bias away

390
00:27:43,770 --> 00:27:45,230
and if one

391
00:27:45,230 --> 00:27:48,050
you see i have

392
00:27:48,110 --> 00:27:53,430
what is the maximum likelihood estimate value the probability of heads

393
00:27:53,440 --> 00:27:55,250
one right

394
00:27:55,300 --> 00:28:00,050
but that you would be infinitely surprised if you look again starting with you bet

395
00:28:00,050 --> 00:28:03,620
one dollar an infinite odds that the next be itself

396
00:28:03,700 --> 00:28:08,590
obviously not because you know there in small numbers of samples you're bound to see

397
00:28:08,590 --> 00:28:13,310
some variation just because the number of samples is small in particular sample size of

398
00:28:13,310 --> 00:28:18,000
one you can do anything except all heads or all tails so

399
00:28:18,020 --> 00:28:23,990
the maximum likelihood estimator is clearly junk and should never be used under any circumstances

400
00:28:23,990 --> 00:28:28,140
but i just developed here to show the development of the maximum likelihood estimation of

401
00:28:28,140 --> 00:28:33,850
course you should really use something like markets talked about like look last like the

402
00:28:33,850 --> 00:28:35,340
number of hands

403
00:28:36,240 --> 00:28:37,710
you know over

404
00:28:37,730 --> 00:28:40,490
number had five hundred miles

405
00:28:40,530 --> 00:28:42,190
plus one

406
00:28:42,540 --> 00:28:47,190
something like this right sometimes estimated that some smoothing

407
00:28:47,200 --> 00:28:51,640
and the smoothing estimators like assuming that before you even started

408
00:28:51,660 --> 00:28:52,920
you start

409
00:28:52,930 --> 00:28:57,030
you know have a conflict of heads and half according to

410
00:28:57,050 --> 00:29:01,140
just to get you started before you can take any data that helps you avoid

411
00:29:02,340 --> 00:29:05,980
but that's idea of maximum likelihood estimation

412
00:29:06,010 --> 00:29:09,490
and you do the same thing with a multinomial distribution

413
00:29:09,500 --> 00:29:11,060
which is just about you

414
00:29:11,090 --> 00:29:15,020
i don't know that termites the distribution on instead of two and from zero one

415
00:29:15,320 --> 00:29:17,860
k possible outcomes of like rolling a die

416
00:29:17,870 --> 00:29:19,510
a biased

417
00:29:20,570 --> 00:29:23,250
and again the same thing this model

418
00:29:23,260 --> 00:29:26,660
you write down the likelihood you take the derivative but now

419
00:29:26,670 --> 00:29:31,980
this slide is here to remind you in a second important features during maximum likelihood

420
00:29:31,980 --> 00:29:33,380
hidden variables

421
00:29:33,400 --> 00:29:36,730
so i i will describe the math that you probably know most of them as

422
00:29:36,730 --> 00:29:39,130
you need to know to to be able to do that and i'll just show

423
00:29:39,130 --> 00:29:44,400
you the results of an experiment that we did recently for this where we gave

424
00:29:46,130 --> 00:29:49,000
different these different patterns and they had to judge on a scale of zero to

425
00:29:49,000 --> 00:29:53,040
ten so here the judgments again we don't think of the absolute values have being

426
00:29:53,040 --> 00:29:56,900
particularly meaningful and we're going to take the model predictions are normalized to have the

427
00:29:56,900 --> 00:29:59,630
same minimax

428
00:29:59,630 --> 00:30:04,650
so but but but the key thing is that we we first of all we

429
00:30:04,650 --> 00:30:08,000
give people a pretty small number of events so this is these are all weak

430
00:30:08,000 --> 00:30:14,820
signals to a traditional statistical analysis but people give you some meaningfully different answers that

431
00:30:14,960 --> 00:30:19,340
depend on the function of for example how many events we give them or in

432
00:30:19,340 --> 00:30:22,880
this case or varying is the ratio of the number of events that look like

433
00:30:22,880 --> 00:30:26,150
they in a tight cluster to those which are so here you've got this

434
00:30:26,230 --> 00:30:27,750
five time

435
00:30:27,790 --> 00:30:29,960
tightly clustered events here for three

436
00:30:29,980 --> 00:30:34,550
two you can barely notice it and one or the zero heroes doesn't mean anything

437
00:30:34,550 --> 00:30:39,230
if the clusters one but people across again across a large number of subjects they

438
00:30:39,230 --> 00:30:43,860
show very systematic effect here increasing the the judgement that there is a hidden cars

439
00:30:43,860 --> 00:30:44,940
as you go

440
00:30:44,960 --> 00:30:48,670
this way and similarly for some of these other things you lose your i mean

441
00:30:48,770 --> 00:30:52,250
here we wanted to see how much detail yet so this is a very very

442
00:30:52,250 --> 00:30:55,790
weak effect but there is a statistically significant effect of just bringing

443
00:30:55,900 --> 00:31:01,670
one of those dots over here anyway the model makes the same basic predictions again

444
00:31:01,670 --> 00:31:06,130
just showing that these these intuitions people have although there we can by any normal

445
00:31:06,130 --> 00:31:09,730
statistical standard you know shouldn't count as evidence in a court of law but they're

446
00:31:09,730 --> 00:31:16,150
all actually very sensible intuitions under a sensible bayesian model selection analysis given pretty sensible

447
00:31:16,150 --> 00:31:18,880
hypothesis space of possible hidden causes

448
00:31:21,040 --> 00:31:28,480
they were shown the plots and they were shown them in different random orders

449
00:31:33,150 --> 00:31:36,610
the visual system it doesn't interfere

450
00:31:36,670 --> 00:31:43,150
it means and so so

451
00:31:43,420 --> 00:31:48,630
know that's a very good question in that what one of the so

452
00:31:48,670 --> 00:31:51,670
one of the things you might think are you might question might ask is is

453
00:31:51,670 --> 00:31:55,290
this really kind generous more of perceptual judgement

454
00:31:57,670 --> 00:32:00,270
for this kind of thing i know i don't think it you can draw a

455
00:32:00,270 --> 00:32:05,270
principled line that in other work we've actually been interested in what we call much

456
00:32:05,270 --> 00:32:10,500
more perceptual judgments like you know modeling things like perceptual organisation using bayesian principles and

457
00:32:10,500 --> 00:32:14,360
that seems to work well too but it's a good it's a good question i

458
00:32:14,360 --> 00:32:16,340
mean here

459
00:32:17,290 --> 00:32:19,070
in other experiments that we did

460
00:32:19,130 --> 00:32:21,110
and you can you can see this paper

461
00:32:21,710 --> 00:32:25,500
i think there's a reference to hear this cognition two thousand seven paper we did

462
00:32:25,500 --> 00:32:28,190
versions of this kind of thing but we also did a number of more cognitive

463
00:32:28,190 --> 00:32:32,150
things were for example you know the famous birthday problem like how many people you

464
00:32:32,150 --> 00:32:35,750
have to have in the room such that the probability is greater than fifty percent

465
00:32:35,750 --> 00:32:39,420
that two of them have to have the same birthday it's not it's much less

466
00:32:39,420 --> 00:32:43,840
than the number we have here i think it's twenty three or something is our

467
00:32:45,520 --> 00:32:48,480
i recently heard a version of this work it says how many do you have

468
00:32:48,480 --> 00:32:51,540
to have in the room so that the probability is greater than fifty percent but

469
00:32:51,540 --> 00:32:54,980
two of them have a birthday in the same week

470
00:32:55,000 --> 00:32:57,570
i think it is and you know the answer to that is

471
00:32:57,570 --> 00:33:00,210
within seven days of each other

472
00:33:03,340 --> 00:33:05,130
i think it's six so

473
00:33:05,150 --> 00:33:11,160
you're right so the class the classic result is supposed to be very counterintuitive and

474
00:33:11,160 --> 00:33:15,520
it probably will maybe for the first time i heard about it was but but

475
00:33:15,520 --> 00:33:19,460
our sense was actually people are pretty well calibrated to how big six you know

476
00:33:19,460 --> 00:33:23,440
that the relative magnitude of these coincidences so it's obviously a big coincidence to have

477
00:33:23,440 --> 00:33:26,270
brought the same day than to be within seven days of each other and in

478
00:33:26,270 --> 00:33:29,860
the experiment we did is this before i heard about this other problem but what

479
00:33:29,860 --> 00:33:33,170
we do is we actually gay people patterns birthday so it was just like this

480
00:33:33,210 --> 00:33:35,590
but we said well suppose you go to a party and you meet people with

481
00:33:35,590 --> 00:33:37,960
the following birthday's how

482
00:33:37,960 --> 00:33:42,170
but the coincidences we just how you how strong acquaintances and

483
00:33:42,190 --> 00:33:45,420
not all the birthdays had pattern to them but there was some of the network

484
00:33:45,420 --> 00:33:48,070
you know they could be strong pattern like three people on the same day or

485
00:33:48,070 --> 00:33:52,020
week pattern like four people kind of within a month of each other and you

486
00:33:52,020 --> 00:33:54,860
get exactly the same kind of thing because you have to have a different hypothesis

487
00:33:54,860 --> 00:34:00,270
space of hidden causes their hidden causes are basically proximity in in space

488
00:34:00,290 --> 00:34:04,250
six million time and other kinds of things like you know the same day of

489
00:34:04,250 --> 00:34:09,000
the month but basically it's a space of alternative regularities and the bayes factor does

490
00:34:09,000 --> 00:34:12,190
just as well it's a good job of predicting judgments that so i think this

491
00:34:12,190 --> 00:34:17,900
is a general inability to detect hidden causes the coincidences in in

492
00:34:17,960 --> 00:34:22,070
continuous data whether it's spatial or temporal is something that the brain does

493
00:34:22,110 --> 00:34:26,110
all over the place and computational neuroscientists have proposed that is what learning in the

494
00:34:26,110 --> 00:34:30,320
brain is it's coincidence detection i i i think that goes way too far to

495
00:34:30,320 --> 00:34:32,860
say that's all there is to learning in the brain that sort of the whole

496
00:34:32,860 --> 00:34:35,850
point of the rest of these lectures is all the things that go beyond finding

497
00:34:35,850 --> 00:34:41,210
coincidences but it is the case that the patterns of coincidence in co occurrence in

498
00:34:41,210 --> 00:34:42,960
in data are

499
00:34:42,960 --> 00:34:47,690
you know what are good cues to hidden causal structure and i think across cognition

500
00:34:47,690 --> 00:34:50,340
and perception were good picking that

501
00:34:50,360 --> 00:34:53,420
here's one other example

502
00:34:53,440 --> 00:34:56,560
again this is this is maybe a little less familiar but this is this is

503
00:34:56,560 --> 00:35:01,070
actually a textbook problem from bayesian statistics if you you've heard of the taxicab problem

504
00:35:01,070 --> 00:35:04,170
how people heard that anyone

505
00:35:04,190 --> 00:35:07,020
i think that the probably the classic version this is written up in

506
00:35:07,020 --> 00:35:13,250
the task is to estimate the transition matrix perhaps its stationary distribution pi

507
00:35:13,260 --> 00:35:16,780
and perhaps test different data

508
00:35:17,030 --> 00:35:22,280
is a markov chain predicts predict the next day usually things that we do in

509
00:35:22,280 --> 00:35:29,310
statistics in the face of this kind of problems so that's the set up next

510
00:35:32,690 --> 00:35:39,260
i'd like to go back one step talk about i think there are still people

511
00:35:39,840 --> 00:35:46,400
probability seriously and i just think of it as some mathematical tools in in this

512
00:35:48,160 --> 00:35:54,140
how can you tell if for sequences markov chain so this is definitely a markov

513
00:35:54,140 --> 00:35:58,130
chains kind of classical part of flat

514
00:35:58,140 --> 00:36:00,770
philosophy if you write so

515
00:36:00,780 --> 00:36:06,570
suppose we observe a process x are taking that surround them up to

516
00:36:06,960 --> 00:36:08,810
and i want

517
00:36:09,270 --> 00:36:14,650
how can i tell if it's a markov chain or not

518
00:36:14,660 --> 00:36:23,190
the notion of exchangeable which is invariant under permutations is a generalization of exchangeability called

519
00:36:23,190 --> 00:36:25,750
partial exchangeability in

520
00:36:25,760 --> 00:36:30,020
it's an insult to switching blocks and think you can see

521
00:36:30,340 --> 00:36:34,100
definition just one example of a strong

522
00:36:34,140 --> 00:36:36,790
three states

523
00:36:36,810 --> 00:36:38,250
once in terms

524
00:36:38,260 --> 00:36:41,710
and you can see the the fact that first block

525
00:36:41,770 --> 00:36:44,570
it begins with what it does with this

526
00:36:45,120 --> 00:36:50,220
doctor right different size but that's okay begins with one and ends with two

527
00:36:50,250 --> 00:36:50,960
it's first

528
00:36:50,970 --> 00:36:55,340
this block is the same as well before the second block and lots of after

529
00:36:55,350 --> 00:36:59,450
the first rock is the same as what after the second rock

530
00:36:59,460 --> 00:37:05,140
OK so because you can make history by switching those two blocks put that one

531
00:37:05,210 --> 00:37:08,200
to the beginning of that one two one two

532
00:37:08,210 --> 00:37:11,450
that's the story

533
00:37:11,460 --> 00:37:18,780
stochastic process is partially exchangeable if it's in the kinds of blocks which transformations OK

534
00:37:19,340 --> 00:37:25,390
this set presentation can which blocks the over the size of its

535
00:37:25,390 --> 00:37:34,400
and then this could partially exchangeable indefinitely through a which was first by the french

536
00:37:34,430 --> 00:37:38,840
version of ten is out of service the following

537
00:37:38,930 --> 00:37:45,110
if you have a stochastic process in action on occurs infinitely often sorry that technical

538
00:37:45,110 --> 00:37:49,020
but it's first without a

539
00:37:49,090 --> 00:37:55,030
partially exchangeable for you if it's a mixture of markov chains that is nothing if

540
00:37:55,270 --> 00:38:00,530
not prior distribution on the transition matrices DP

541
00:38:00,550 --> 00:38:05,030
p is one of the transition matrices is the prior and

542
00:38:06,700 --> 00:38:10,780
the chance of observing the string x not x one up to xk

543
00:38:10,790 --> 00:38:11,780
it should be

544
00:38:11,790 --> 00:38:15,150
the bayesian average of the

545
00:38:15,150 --> 00:38:21,700
classical chance of observing that strange string averaging over the past so different

546
00:38:21,760 --> 00:38:26,350
for markov chains and they can you don't have to say that the transition state

547
00:38:26,350 --> 00:38:30,860
probability you can say well from partially exchangeable

548
00:38:30,920 --> 00:38:34,500
if you can cite that we try do

549
00:38:36,870 --> 00:38:37,990
get to work

550
00:38:38,000 --> 00:38:41,410
i am i like the next transparency place

551
00:38:41,500 --> 00:38:46,500
if you do computation

552
00:38:46,540 --> 00:38:48,010
and it's the curr

553
00:38:48,020 --> 00:38:55,020
is this best part of complicated group to have conjugate priors conjugate prior analysis for

554
00:38:56,100 --> 00:39:01,970
the calculations for markov chains is what kind of not

555
00:39:02,020 --> 00:39:05,520
so bad just say what it is so got this

556
00:39:05,620 --> 00:39:12,500
transition state matrix system in by in matrix i'm the priors on these matrices the

557
00:39:12,730 --> 00:39:15,790
conjugate prior is the following each

558
00:39:15,810 --> 00:39:21,840
is independent and there was light that's that that's the conjugate prior so we are

559
00:39:23,130 --> 00:39:26,280
with in the parameters alpha

560
00:39:26,300 --> 00:39:34,470
by the one j et cetera okay the dirichlet priors probably most people in this

561
00:39:34,470 --> 00:39:42,250
audience and its parameters have a useful interpretation there's sort of problem that's the the

562
00:39:42,250 --> 00:39:49,150
vector meson alpha and there some kind of indication of the notion about tightly people

563
00:39:49,280 --> 00:39:56,150
are the actually has the familiar property of being closed under sampling if you have

564
00:39:56,160 --> 00:39:58,600
if you go to sleep

565
00:39:58,650 --> 00:40:01,280
and if you see the markov chain

566
00:40:02,370 --> 00:40:07,090
the posterior distribution is easy to compute it's just the dirichlet a with the parameters

567
00:40:07,090 --> 00:40:12,630
are changed for i j is augmented by the number of times for cross them

568
00:40:12,630 --> 00:40:13,510
i j

569
00:40:13,550 --> 00:40:20,910
very nice if you like this problem but some prior can approximate any prior as

570
00:40:20,910 --> 00:40:24,900
a mixture of slice of makes use of greece loans are dense in the space

571
00:40:24,900 --> 00:40:26,320
of all priors and

572
00:40:26,350 --> 00:40:31,950
that's often not looking for this and is that the output of fifty fifty mixture

573
00:40:32,880 --> 00:40:35,630
two there no

574
00:40:35,860 --> 00:40:41,930
characterisation i find still surprising and this is what i call these kinds of things

575
00:40:41,930 --> 00:40:45,880
that the johnson type theorems and it

576
00:40:45,910 --> 00:40:51,720
also outside point transparency spam some

577
00:40:51,740 --> 00:40:57,600
stochastic processes the markov nothing about anything just to stochastic process

578
00:40:58,520 --> 00:41:01,270
i see the process

579
00:41:01,290 --> 00:41:03,770
i have for predicting

580
00:41:03,790 --> 00:41:05,570
the next day

581
00:41:05,590 --> 00:41:09,880
and that's allowed to depend on and in the past

582
00:41:09,920 --> 00:41:16,160
suppose i conditional probability of the next day given the past so my rules

583
00:41:16,270 --> 00:41:21,450
the probability that the next day doesn't xn plus one the whole past

584
00:41:21,630 --> 00:41:23,890
it depends on the past

585
00:41:23,900 --> 00:41:27,490
the number of transitions from

586
00:41:27,500 --> 00:41:30,770
xn to little n plus one

587
00:41:30,780 --> 00:41:33,630
the function should depend on

588
00:41:33,670 --> 00:41:38,650
what happened in the past but this time the past and i come times i

589
00:41:38,740 --> 00:41:45,060
from one to just one hundred function of that cell to go to

590
00:41:45,090 --> 00:41:47,200
it's necessary and sufficient

591
00:41:47,220 --> 00:41:52,390
to be a conjugate mixture of markov chains to to have your prediction of that

592
00:41:52,390 --> 00:41:53,740
type so

593
00:41:53,760 --> 00:41:56,890
that's right these conjugate priors two

594
00:41:56,890 --> 00:42:00,340
and that's because i didn't say markov chain

595
00:42:00,470 --> 00:42:04,300
capturing that matches the it says

596
00:42:04,310 --> 00:42:08,740
predicting that way if and only if the act as

597
00:42:08,810 --> 00:42:13,120
you're watching the markov the conjugate priors quite a striking and pleasing

598
00:42:13,130 --> 00:42:20,550
theorem this is is a theorem of seven it is about to the next

599
00:42:20,560 --> 00:42:24,120
transparency please disregard

600
00:42:24,130 --> 00:42:29,240
i have my of my in this line of work

601
00:42:29,250 --> 00:42:37,130
if you use a conjugate prior the predictive future

602
00:42:37,150 --> 00:42:39,890
having seen in the past is fine

603
00:42:39,900 --> 00:42:43,760
in the past that is the expected value

604
00:42:43,910 --> 00:42:46,270
the transition matrix p

605
00:42:48,520 --> 00:42:49,560
he is

606
00:42:49,570 --> 00:42:54,970
a convex combination of the prior distribution of class

607
00:42:56,200 --> 00:43:02,990
empirical maximum likelihood estimate of p just the transition counts normalized so that was a

608
00:43:02,990 --> 00:43:09,770
conjugate prior to predict the future is a a convex combination of original prior and

609
00:43:10,150 --> 00:43:16,120
and and usually empirical estimate in weighted by the elephants and that way

610
00:43:16,140 --> 00:43:18,620
and that is

611
00:43:18,640 --> 00:43:22,780
the converse holds that is predicted

612
00:43:22,780 --> 00:43:24,650
you know we can write like this

613
00:43:24,670 --> 00:43:25,720
you know the q

614
00:43:25,730 --> 00:43:27,360
if the you weren't there

615
00:43:27,380 --> 00:43:29,630
right it's basically what we had before

616
00:43:29,660 --> 00:43:34,600
only slightly different representation we had count here and now we have this other way

617
00:43:34,600 --> 00:43:38,670
of indexing we could just write the this q variables in here right they would

618
00:43:38,670 --> 00:43:39,920
cancel out

619
00:43:39,970 --> 00:43:43,310
but now we can appliances inequality we can say well we can all the average

620
00:43:43,310 --> 00:43:44,530
this part

621
00:43:44,550 --> 00:43:50,600
inside the log or outside right now we just you know exploit the concavity of

622
00:43:51,810 --> 00:43:55,130
and you know it then we know that be that we get of story is

623
00:43:55,130 --> 00:43:56,570
lower bound

624
00:43:56,930 --> 00:43:59,330
if we pull out the log here

625
00:44:01,060 --> 00:44:03,720
now what what's nice about this

626
00:44:03,730 --> 00:44:04,960
is that

627
00:44:04,970 --> 00:44:10,850
now we have the this farm outside of the logarithms and that makes things easier

628
00:44:11,020 --> 00:44:16,160
but anyway what you should know what that shows is basically that this expression here

629
00:44:16,180 --> 00:44:17,730
right for any q

630
00:44:18,630 --> 00:44:19,220
you know

631
00:44:19,250 --> 00:44:21,520
for every observation you can choose one of these

632
00:44:21,650 --> 00:44:24,430
q distributions

633
00:44:24,470 --> 00:44:28,840
for any choice of these q distributions we will get a lower bound

634
00:44:28,860 --> 00:44:30,300
on the likelihood

635
00:44:30,380 --> 00:44:33,930
OK now what we actually do in the model and that's also then the way

636
00:44:33,930 --> 00:44:37,650
to derive it is to say well look

637
00:44:37,660 --> 00:44:42,110
you know if we have the q parametrized family of bounds

638
00:44:42,130 --> 00:44:47,270
you know course ultimately we would like to have this tightest possible right so we

639
00:44:47,270 --> 00:44:53,880
could say well before the current parameters that we have what is the best bond

640
00:44:53,900 --> 00:44:58,980
as you know what this value for q that gives us the best part so

641
00:44:58,980 --> 00:45:04,290
we could just treat theta pi is being fixed to compute the gradient with respect

642
00:45:04,980 --> 00:45:07,100
rq subject to the

643
00:45:07,110 --> 00:45:12,220
constraints i nonnegativity being normalized and so on and so forth

644
00:45:12,230 --> 00:45:14,060
and if we do that

645
00:45:14,080 --> 00:45:15,930
we basically just get

646
00:45:15,940 --> 00:45:22,500
that the optimal choice for these q distributions is just posterior probabilities because if we

647
00:45:22,500 --> 00:45:23,700
plug in the

648
00:45:23,710 --> 00:45:26,710
posterior probabilities

649
00:45:26,730 --> 00:45:28,760
right well over here

650
00:45:30,210 --> 00:45:31,780
then we get the optimal

651
00:45:31,800 --> 00:45:34,030
but actually the optimal

652
00:45:34,040 --> 00:45:35,920
the problem

653
00:45:36,110 --> 00:45:38,710
and that's exactly what we do in the east step case so one way to

654
00:45:38,710 --> 00:45:44,180
think about the e step is to come up with the best

655
00:45:44,200 --> 00:45:48,330
i know about and then we can also see that if we treat the

656
00:45:48,410 --> 00:45:53,350
these q dispute is fixed and we compute the gradient with respect to theta pi

657
00:45:53,360 --> 00:45:58,220
but then you know what we get is nothing else but the m step equations

658
00:45:58,220 --> 00:46:00,590
in the model because so

659
00:46:00,700 --> 00:46:04,680
so basically you know we augment the likelihood function by

660
00:46:04,700 --> 00:46:06,410
you know we have the parameters

661
00:46:06,420 --> 00:46:10,950
and then we have this lower bound where they also these additional auxiliary parameters q

662
00:46:11,070 --> 00:46:13,310
and then we optimize with respect to both

663
00:46:13,330 --> 00:46:15,270
in an alternating fashion

664
00:46:15,290 --> 00:46:17,810
right and since the same function

665
00:46:17,830 --> 00:46:18,830
that we have

666
00:46:18,840 --> 00:46:21,430
maximizing every time

667
00:46:21,480 --> 00:46:25,320
right it's actually fairly easy to you know to use these types of arguments to

668
00:46:25,320 --> 00:46:27,140
be with convergence

669
00:46:27,150 --> 00:46:29,270
and and things like that

670
00:46:29,320 --> 00:46:32,050
OK so

671
00:46:32,130 --> 00:46:37,050
and i don't know i think chris bishop to t

672
00:46:37,070 --> 00:46:40,820
he gives the tutorial on latent variable models right i'm not sure

673
00:46:40,960 --> 00:46:42,670
whether he

674
00:46:46,230 --> 00:46:47,430
all right so

675
00:46:47,460 --> 00:46:51,150
you know is the machinery so again you know if you if you look at the

676
00:46:52,600 --> 00:46:55,780
you know

677
00:46:55,790 --> 00:46:57,250
they are actually

678
00:46:57,260 --> 00:46:59,430
you know fairly easy

679
00:46:59,650 --> 00:47:02,600
fairly easy to implement and

680
00:47:02,910 --> 00:47:05,430
and also

681
00:47:05,440 --> 00:47:09,680
four for moderate size documents collection this is can all be done quite efficiently so

682
00:47:09,680 --> 00:47:10,710
let's look at

683
00:47:10,770 --> 00:47:12,520
an example of

684
00:47:12,540 --> 00:47:16,100
you know what we extract here right

685
00:47:16,820 --> 00:47:18,770
one of the numbers here

686
00:47:18,790 --> 00:47:22,800
if you could just ignore the numbers on this slide i ix say something about

687
00:47:22,800 --> 00:47:28,000
the numbers on the other side basically what was done here is we took a

688
00:47:28,000 --> 00:47:31,930
couple of ten thousand news stories from AP news

689
00:47:31,950 --> 00:47:33,070
and then we

690
00:47:33,090 --> 00:47:37,540
learn to model with a hundred concepts OK just fixing that number one hundred right

691
00:47:37,550 --> 00:47:38,670
was just we have to

692
00:47:38,730 --> 00:47:39,390
you know

693
00:47:39,420 --> 00:47:43,980
we have to specify some how many concept we would like to extract case can

694
00:47:44,020 --> 00:47:47,680
also in clustering right we're often you need to specify a

695
00:47:47,690 --> 00:47:54,210
how many concepts you want to look for and then we we actually ordered

696
00:47:54,230 --> 00:47:55,770
the terms

697
00:47:55,810 --> 00:48:01,300
according to their probability of expressing this particular concept right so the terms that have

698
00:48:01,300 --> 00:48:05,180
very high probability the p of w given that's large

699
00:48:05,180 --> 00:48:08,780
you know they come to the top and of small you know what the answer

700
00:48:08,960 --> 00:48:10,050
this is the top

701
00:48:10,110 --> 00:48:12,820
the the twenty years of

702
00:48:12,860 --> 00:48:16,430
so if we look at you know cities concept number two

703
00:48:16,450 --> 00:48:21,940
we see that the township has a very high probability but we we also see

704
00:48:21,940 --> 00:48:24,080
that there are a lot of the

705
00:48:24,110 --> 00:48:30,420
sort of synonyms special types of ships you see here tankers and fairy and then

706
00:48:30,420 --> 00:48:34,500
you see morphological variations also these would look at all

707
00:48:34,510 --> 00:48:37,080
i have also have high probability

708
00:48:37,090 --> 00:48:42,820
in this concept you also see that you know related terms like c important that

709
00:48:42,820 --> 00:48:45,030
are not synonyms of ships anyway

710
00:48:45,350 --> 00:48:48,950
you also find here right so you can make

711
00:48:48,960 --> 00:48:51,360
you know he seaports

712
00:48:51,380 --> 00:48:55,380
you know that with someone tell you that it's likely then also in the context

713
00:48:55,380 --> 00:48:58,460
defined the term ship and vice versa so

714
00:48:58,510 --> 00:49:01,500
and you can also see the very specific

715
00:49:01,510 --> 00:49:04,720
o thing going on that have to do with the document collection for instance there

716
00:49:04,730 --> 00:49:05,730
was the

717
00:49:05,760 --> 00:49:08,810
this oil spill vx about this

718
00:49:08,830 --> 00:49:12,420
and you know that was just you know very dominant in the news so you

719
00:49:12,420 --> 00:49:16,630
also pick that has been seen in this concept

720
00:49:17,250 --> 00:49:22,680
OK and you can look at the other concept here so you know you look

721
00:49:22,680 --> 00:49:24,130
at these things

722
00:49:24,160 --> 00:49:25,210
you know they have

723
00:49:25,210 --> 00:49:29,280
they often make sense let's put it that way kept is another example

724
00:49:29,300 --> 00:49:32,220
here the the numbers are actually the real

725
00:49:32,530 --> 00:49:35,770
expression probabilities p w d t

726
00:49:35,840 --> 00:49:42,670
and you can see here things like in sequence sequences genome DNA genes chromosome regions

727
00:49:43,870 --> 00:49:44,520
you know

728
00:49:44,530 --> 00:49:47,190
we're here male female

729
00:49:47,200 --> 00:49:51,540
reproductive offspring sex sexual exercise was we can see

730
00:49:51,560 --> 00:49:54,800
you know that semantically related words are

731
00:49:54,810 --> 00:49:55,770
you know

732
00:49:56,890 --> 00:50:01,980
grouped together if you like although this is not really a good thing but

733
00:50:01,980 --> 00:50:07,010
and now if you in the right if you're in for instance here that the

734
00:50:07,010 --> 00:50:08,740
document deals with this

735
00:50:09,200 --> 00:50:12,540
concept right they might be terms like DNA

736
00:50:12,560 --> 00:50:19,150
and gino might be explicitly in the document right then also terms like chromosomes and

737
00:50:19,150 --> 00:50:24,040
genes and so on and so forth will also get the relatively high probability that

738
00:50:24,040 --> 00:50:29,990
they could occur in the document right and so i call this method actually semantic

739
00:50:29,990 --> 00:50:32,530
c one

740
00:50:32,570 --> 00:50:41,420
and maybe this is interesting through the one that i haven't seen before

741
00:50:41,670 --> 00:50:47,320
but this is correct so it's proof and it also generalizes to any points

742
00:50:47,440 --> 00:50:51,300
could as well have used in point if you want to so the way i

743
00:50:51,300 --> 00:50:55,780
usually do is just take one point but it's perfectly fine

744
00:50:55,920 --> 00:50:58,360
it's perfectly fine to use two points

745
00:50:58,380 --> 00:51:03,260
so if you so just for convenience if you take only one point because here

746
00:51:03,260 --> 00:51:08,360
OK these individuals but if you go back to the definition

747
00:51:08,360 --> 00:51:12,300
just as any set of training points so i can also choose the set of

748
00:51:12,300 --> 00:51:16,070
one point you chose one of two point is just one point

749
00:51:16,090 --> 00:51:18,720
and i also choose the coefficients to be one

750
00:51:18,740 --> 00:51:20,990
and it's another possible

751
00:51:21,010 --> 00:51:22,670
OK so

752
00:51:22,680 --> 00:51:27,320
it's great so we've done saying one that's another one

753
00:51:30,130 --> 00:51:33,920
which one

754
00:51:33,920 --> 00:51:35,050
you can approach

755
00:51:37,300 --> 00:51:42,700
OK the first one

756
00:51:46,900 --> 00:52:12,130
so we have

757
00:52:20,340 --> 00:52:30,470
the the

758
00:53:00,450 --> 00:53:03,710
that's so because because you we say

759
00:53:03,740 --> 00:53:07,280
it's not into the product space so this thing is that the product and we

760
00:53:07,280 --> 00:53:12,050
know that the products are positive definite and that's exactly what you are using so

761
00:53:12,050 --> 00:53:14,130
if you take a point with itself

762
00:53:14,150 --> 00:53:17,490
the result will be nonnegative so i think you have to

763
00:53:17,550 --> 00:53:18,780
so what's your name

764
00:53:21,170 --> 00:53:31,860
and maybe for those who can read the funds

765
00:53:31,880 --> 00:53:36,280
looks a bit like when i'm having latex problems such as it is very tiny

766
00:53:40,680 --> 00:53:44,320
OK so he's just moving the coefficients and the sums

767
00:53:44,340 --> 00:53:47,240
so sum of ion coefficient AI

768
00:53:47,260 --> 00:53:52,150
he moved into the first arguement and some of a coefficient a change the second

769
00:53:52,150 --> 00:53:54,280
arguement and it looks like that

770
00:53:54,300 --> 00:53:57,570
OK so

771
00:53:57,590 --> 00:53:59,740
how about the third one

772
00:53:59,740 --> 00:54:03,070
the fourth one OK

773
00:54:03,070 --> 00:54:04,320
four one

774
00:54:04,320 --> 00:54:08,010
o the fourth one should be done after the first one because it

775
00:54:08,180 --> 00:54:12,670
but maybe if someone else does the third one

776
00:54:26,900 --> 00:54:28,110
if you want you can

777
00:54:28,130 --> 00:54:31,760
it was one of those over there

778
00:54:31,780 --> 00:54:37,970
commission decisions joking has to be understood

779
00:54:47,840 --> 00:54:51,150
we need to

780
00:54:53,860 --> 00:55:01,440
all of these a

781
00:55:01,670 --> 00:55:03,970
you know the image

782
00:55:04,740 --> 00:55:10,280
this is a graph should this to be was

783
00:55:10,300 --> 00:55:14,530
so it is too was

784
00:55:15,010 --> 00:55:16,280
don't mention

785
00:55:16,280 --> 00:55:18,240
it is

786
00:55:26,110 --> 00:55:33,420
well there will be a because is too busy doing

787
00:55:33,450 --> 00:55:37,380
so one way to argue is determined as the product of the i values

788
00:55:37,400 --> 00:55:51,670
so so the that three his requiring minimal knowledge of some senior

789
00:55:51,700 --> 00:55:56,820
i brought back one thing that is a positive definite matrix of positive semidefinite in

790
00:55:57,670 --> 00:56:02,010
not a terminology has nonnegative values

791
00:56:02,070 --> 00:56:05,450
hence the product of the i value is also non negative and the product is

792
00:56:05,450 --> 00:56:06,820
the determinant

793
00:56:06,840 --> 00:56:12,650
just use it

794
00:56:12,670 --> 00:56:14,800
this is just one

795
00:56:15,010 --> 00:56:23,070
everybody every see that you wanted to work on the data may be right on

796
00:56:23,070 --> 00:56:24,400
the bottom and

797
00:56:24,400 --> 00:56:26,530
just so people see it

798
00:56:45,130 --> 00:56:51,090
this is just

799
00:56:52,130 --> 00:56:54,320
thank you

800
00:56:54,340 --> 00:56:58,680
BC and then it was the me

801
00:56:58,700 --> 00:57:00,110
OK thank you i mean

802
00:57:00,130 --> 00:57:05,110
any questions for me

803
00:57:05,130 --> 00:57:10,380
OK so now

804
00:57:10,720 --> 00:57:12,470
fourth one

805
00:57:12,490 --> 00:57:14,320
we have

806
00:57:14,320 --> 00:57:16,340
we have

807
00:57:20,400 --> 00:57:24,420
you know if you want to find you have to come up here

808
00:57:52,650 --> 00:58:12,200
OK so the the square of all elements is zero

809
00:58:12,220 --> 00:58:16,030
therefore they have to be zero is is bounded from above by zero

810
00:58:16,050 --> 00:58:20,200
in the squares also known from below by zero instances where so it has to

811
00:58:20,200 --> 00:58:21,400
be equal to zero

812
00:58:21,420 --> 00:58:23,470
the element has to be zero

813
00:58:23,490 --> 00:58:26,820
and this argument works for any x x prime

814
00:58:26,840 --> 00:58:30,570
OK thank you

815
00:58:33,670 --> 00:58:39,260
OK so we have to use all these properties

816
00:58:39,300 --> 00:58:42,820
and we can approve in

817
00:58:42,840 --> 00:58:47,130
all remaining time and maybe will finish tomorrow

818
00:58:47,130 --> 00:58:51,220
how to construct feature space so basically we prove the opposite direction of this one

819
00:58:51,220 --> 00:58:55,300
proved given positive come out constructive feature space

820
00:58:55,340 --> 00:58:58,490
maybe maybe will manage to

821
00:58:58,510 --> 00:59:03,990
to prove it so it's it's like very strict half past the

822
00:59:06,670 --> 00:59:10,570
a few minutes so let's see if maybe i can do it with the humans

823
00:59:10,590 --> 00:59:14,260
model was will continue will finish it tomorrow so

824
00:59:14,280 --> 00:59:16,280
we have one two

825
00:59:16,300 --> 00:59:21,570
five slides

826
00:59:21,590 --> 00:59:24,860
OK so

827
00:59:24,900 --> 00:59:28,150
the feature map will be defined as follows

828
00:59:28,200 --> 00:59:32,610
given input point we map it to a point two of function

829
00:59:32,630 --> 00:59:37,900
in this space are to the power x but this simply means functions mapping x

830
00:59:37,900 --> 00:59:38,970
two are

831
00:59:38,990 --> 00:59:42,150
and the mapping was such that

832
00:59:42,170 --> 00:59:45,650
the point x is assigned to the function that we get

833
00:59:45,650 --> 00:59:47,200
if we substitute x

834
00:59:47,200 --> 00:59:48,570
in two

835
00:59:48,590 --> 00:59:50,570
the second arguement of the kernel

836
00:59:50,590 --> 00:59:54,650
could also be the first one doesn't matter and leave the first arguement open so

837
00:59:54,650 --> 00:59:57,200
this is the function of the first arguement

838
00:59:57,220 --> 01:00:01,300
which given a point x then we substitute in here gives us the real number

839
01:00:01,360 --> 01:00:04,740
so for instance if we had a gaussian kernel function then

840
01:00:04,760 --> 01:00:09,530
o point x would be mapped into a gaussian centered on x

841
01:00:09,530 --> 01:00:12,630
o point prime will be mapped into those instead ex-prime

842
01:00:12,650 --> 01:00:15,860
so what we have to do is we have to turn this into a linear

843
01:00:15,860 --> 01:00:22,250
ignore the sum from moment the error times this the basis set

844
01:00:22,300 --> 01:00:24,990
the stochastic approximation

845
01:00:25,000 --> 01:00:29,900
is what we were actually using the stochastic approximation is to say

846
01:00:29,910 --> 01:00:31,200
to do that

847
01:00:31,220 --> 01:00:34,700
and to keep doing that so you could expand this summer out and you be

848
01:00:34,700 --> 01:00:38,410
subtracting one of those each time you have a massive some overall your data going

849
01:00:38,410 --> 01:00:43,040
out there but what we're going do instead of doing that whole expanded thing is

850
01:00:43,040 --> 01:00:45,240
just to this thing

851
01:00:45,250 --> 01:00:48,710
and then when we need to include the other term we do this thing again

852
01:00:48,730 --> 01:00:52,600
so we take the first data point for example then we take second data point

853
01:00:52,600 --> 01:00:57,270
for data points and apply this rule the difference between the two is that you're

854
01:00:57,530 --> 01:01:02,710
dating this parameters that every time step now it turns out the

855
01:01:02,740 --> 01:01:04,410
this algorithm

856
01:01:04,420 --> 01:01:06,310
i will follow approximately

857
01:01:06,310 --> 01:01:10,280
the gradient

858
01:01:10,310 --> 01:01:14,300
all the previous algorithms are going in the same direction using more iterations because each

859
01:01:14,300 --> 01:01:18,410
iteration look to one data point but for large data this turned out to be

860
01:01:18,410 --> 01:01:22,810
much faster what's interesting is that we can't make it out in this region here

861
01:01:22,810 --> 01:01:27,590
it's a diffusing around all over the place because it keeps on making small adjustments

862
01:01:27,590 --> 01:01:30,830
in the wrong direction it is quite fast

863
01:01:30,840 --> 01:01:35,740
if very large datasets with small data it will typically be slow and in fact

864
01:01:35,750 --> 01:01:38,420
if you've got a quadratic error service like this you can just jump straight to

865
01:01:38,420 --> 01:01:40,860
the minimum by solving the quadratic programme

866
01:01:40,910 --> 01:01:44,800
but the reason i wanted talk about this is the same although

867
01:01:44,830 --> 01:01:47,750
learning rules that we

868
01:01:47,770 --> 01:01:52,330
i talked about before they all have

869
01:01:52,330 --> 01:01:56,950
some underlying error function and one should think more about the error function potentially if

870
01:01:56,950 --> 01:02:00,690
you study the learning rule what you want the error function to be the about

871
01:02:00,720 --> 01:02:06,000
learning rule itself

872
01:02:06,000 --> 01:02:10,780
in particular as i said before this goes into direction so

873
01:02:10,780 --> 01:02:15,480
i think it took us away perhaps to work this out machine learning this is

874
01:02:15,480 --> 01:02:20,300
of historical perspective what i think the mindset was but once we want to do

875
01:02:20,300 --> 01:02:24,150
with was split in machine learning one one is to think that the error function

876
01:02:24,150 --> 01:02:29,520
has a probabilistic interpretation right and what you really doing it

877
01:02:29,540 --> 01:02:34,620
maximizing the likelihood or minimizing the negative log likelihood i mean that's what all the

878
01:02:34,830 --> 01:02:36,290
talk about more

879
01:02:36,450 --> 01:02:43,080
my next session but say other researchers would take the point of view this is

880
01:02:43,080 --> 01:02:47,640
an actual loss function so this is the cost you will pay

881
01:02:47,640 --> 01:02:49,700
for making the wrong decision

882
01:02:49,740 --> 01:02:50,740
and then

883
01:02:50,740 --> 01:02:52,240
this error function

884
01:02:52,320 --> 01:02:57,680
minimizing the error function is known as empirical risk minimization and there's lots of theory

885
01:02:57,700 --> 01:03:02,320
around well whether you should be measured minimizing the empirical risk or whether should minimize

886
01:03:02,320 --> 01:03:08,360
another is or how you can regularize that to ensure you avoid things like overfitting

887
01:03:08,370 --> 01:03:15,670
so these interpretations probability in optimisation theory become really important and they become increasingly important

888
01:03:15,770 --> 01:03:17,980
over the last

889
01:03:17,990 --> 01:03:22,610
fifteen years so much so that as i say you know i wouldn't be qualified

890
01:03:22,610 --> 01:03:24,920
to start machine learning phd today

891
01:03:24,930 --> 01:03:28,200
if i have the same qualifications i had when i did started in nineteen ninety

892
01:03:30,710 --> 01:03:35,610
much of the last fifteen years of machine learning research i think has focused on

893
01:03:35,870 --> 01:03:37,390
supervised learning

894
01:03:38,270 --> 01:03:44,520
and even supervised learning has focus on probabilistic interpretations of these algorithms working them out

895
01:03:44,540 --> 01:03:50,290
or clever relaxations the difficult objective functions so the problem with the objective function approaches

896
01:03:50,290 --> 01:03:53,890
often you write down the thing you really want to optimize but it turned out

897
01:03:53,890 --> 01:03:58,600
to be NP hard to optimize CPU relaxing from san l zero norm two and

898
01:03:58,600 --> 01:04:02,330
l one norm and you can maybe per things about how close country after that's

899
01:04:02,330 --> 01:04:04,990
not really my area but that's sort of

900
01:04:05,050 --> 01:04:09,170
i would say you can categorize a large amounts of machine learning work into one

901
01:04:09,170 --> 01:04:15,140
of those two areas dealing with probabilistic problems the probabilistic interpretation for us all dealing

902
01:04:15,140 --> 01:04:17,300
with the relaxation

903
01:04:17,300 --> 01:04:22,490
so things that haven't talked about which is really important in in this area for

904
01:04:23,240 --> 01:04:28,510
optimisation of things like second order methods for optimizing which are very important for quicker

905
01:04:29,860 --> 01:04:34,050
those include examples like conjugate gradient causing newton and newton

906
01:04:34,300 --> 01:04:40,610
effective heuristics such as momentum i algorithms this does this momentum idea which basically says

907
01:04:40,650 --> 01:04:42,090
when you're doing this

908
01:04:42,100 --> 01:04:46,860
you should always take your next partially in the direction you took your last step

909
01:04:47,050 --> 01:04:51,260
and that gets around some this diffusing around here really speeds up the solutions these

910
01:04:52,920 --> 01:04:59,320
these algorithms are coming back in a big way for large data sets and a

911
01:04:59,320 --> 01:05:04,160
day using things like momentum let's say we are for some large datasets fitting

912
01:05:08,360 --> 01:05:11,110
the next thing the last thing i want to talk about a little bit is

913
01:05:11,110 --> 01:05:14,600
sort of unsupervised learning

914
01:05:14,600 --> 01:05:16,800
we would like these

915
01:05:18,950 --> 01:05:20,060
converge to

916
01:05:20,070 --> 01:05:21,930
the best possible error

917
01:05:21,980 --> 01:05:23,370
which is that

918
01:05:23,390 --> 01:05:25,480
he star

919
01:05:25,530 --> 01:05:26,670
so this is

920
01:05:26,680 --> 01:05:32,080
the best possible error can reach the function g bar is the function that minimizes

921
01:05:32,080 --> 01:05:33,850
the expected error

922
01:05:34,030 --> 01:05:37,890
that's what is called usually the bayes classifier

923
01:05:37,930 --> 01:05:42,030
kind of confusing because they use a completely different meaning that

924
01:05:43,180 --> 01:05:45,670
but let's say this is

925
01:05:45,750 --> 01:05:46,550
you know

926
01:05:46,560 --> 01:05:47,600
basically the

927
01:05:47,610 --> 01:05:48,800
the classifier that

928
01:05:48,850 --> 01:05:50,390
as you the right thing

929
01:05:50,440 --> 01:05:53,560
this area not always zero

930
01:05:53,800 --> 01:06:01,070
it can be zero if for a given x there is a unique body label

931
01:06:01,080 --> 01:06:04,630
but maybe for a given x there can be several different levels because there are

932
01:06:04,630 --> 01:06:07,030
some errors in the measurement or things like that

933
01:06:07,080 --> 01:06:10,930
so when there is some error in the measurement is going to be zero

934
01:06:10,980 --> 01:06:13,340
but it can because you say

935
01:06:13,390 --> 01:06:16,510
so in general is not zero

936
01:06:16,560 --> 01:06:22,380
and ideally we want our algorithm to be as good as the best one

937
01:06:23,550 --> 01:06:26,540
these convergence here usually studied

938
01:06:26,550 --> 01:06:28,970
in the context of

939
01:06:29,020 --> 01:06:30,530
increasing sample size

940
01:06:31,530 --> 01:06:34,450
as we get more and more data more observations

941
01:06:34,500 --> 01:06:36,200
we would like our error

942
01:06:36,290 --> 01:06:38,020
conversely the best there

943
01:06:38,910 --> 01:06:42,170
the notion here when this convergence of offers

944
01:06:42,220 --> 01:06:45,670
we say that our algorithm is consistent

945
01:06:46,060 --> 01:06:52,530
because ultimately with enough data we will have identified the best although in the way

946
01:06:52,530 --> 01:06:54,930
or the right although we would have

947
01:06:55,520 --> 01:06:58,460
learn the problem

948
01:06:58,510 --> 01:07:05,250
so the question is whether it is is possible and under which conditions

949
01:07:05,300 --> 01:07:08,730
this will wheelock is consistent with orca

950
01:07:08,830 --> 01:07:10,570
there is this notion of

951
01:07:10,620 --> 01:07:16,020
universally consistent algorithm so universally consistent boundaries and is one of the reasons that

952
01:07:16,070 --> 01:07:17,160
no matter

953
01:07:17,210 --> 01:07:20,460
what is the underlying distribution here

954
01:07:20,500 --> 01:07:22,530
we have this property that

955
01:07:22,570 --> 01:07:25,530
ultimately was enough data it will

956
01:07:25,580 --> 01:07:27,920
with the red the writing

957
01:07:28,030 --> 01:07:31,530
the question is whether this is possible at all can we always learn

958
01:07:31,580 --> 01:07:34,930
no matter what the based on how the data is generated

959
01:07:34,980 --> 01:07:37,360
it turns out that it's possible

960
01:07:37,410 --> 01:07:40,160
and it's kind of it's quite easy actually

961
01:07:42,670 --> 01:07:45,960
it is all these or other questions

962
01:07:47,580 --> 01:07:55,260
so the question is what do i mean by x generated uniformly here

963
01:08:02,190 --> 01:08:06,680
i mean OK this is just an example of one possible way to generate the

964
01:08:06,680 --> 01:08:08,050
instances x

965
01:08:08,140 --> 01:08:14,760
uniformly here i meant to say consider q in a d dimensional space you with

966
01:08:14,760 --> 01:08:17,410
the uniform distribution and you say well the

967
01:08:17,730 --> 01:08:21,880
the probability of something each x from q is really

968
01:08:21,940 --> 01:08:23,840
that's one possible way

969
01:08:23,850 --> 01:08:27,030
because basically

970
01:08:27,040 --> 01:08:27,610
you know

971
01:08:27,620 --> 01:08:30,840
i have to assume that my data comes from somewhere

972
01:08:30,890 --> 01:08:33,380
so i can say well

973
01:08:33,390 --> 01:08:37,680
so maybe our data is some measurements about people

974
01:08:37,690 --> 01:08:39,930
like you know the height and the

975
01:08:39,980 --> 01:08:41,070
the weight

976
01:08:41,080 --> 01:08:45,730
and i brought pressure and so on so the height is between zero and maybe

977
01:08:45,730 --> 01:08:47,070
two meters

978
01:08:47,120 --> 01:08:51,610
and what parameters between you know something is something

979
01:08:51,660 --> 01:08:52,840
and so on

980
01:08:55,820 --> 01:08:57,530
i want find some kind of q

981
01:08:58,680 --> 01:09:00,790
i might assume that

982
01:09:00,840 --> 01:09:04,350
any possible viewing this cube is equally likely

983
01:09:04,430 --> 01:09:08,360
it's a phi don't know anything about how people are distributed then i can say

984
01:09:08,930 --> 01:09:10,400
just assume that

985
01:09:10,440 --> 01:09:12,000
this is uniform

986
01:09:12,050 --> 01:09:17,040
or i can have you know more knowledge and i say well people or activities

987
01:09:17,110 --> 01:09:19,210
and based on the

988
01:09:19,220 --> 01:09:20,990
hi scales

989
01:09:21,100 --> 01:09:25,570
and maybe on the blood pressure can scale is like this so i would find

990
01:09:25,570 --> 01:09:27,520
some kind of multidimensional

991
01:09:27,580 --> 01:09:31,370
distribution here and say OK let's assume that this is the right

992
01:09:31,370 --> 01:09:35,100
and combinatorics graph theory everything comes here

993
01:09:35,120 --> 01:09:39,950
and this is the one of the main objective of this course is to show

994
01:09:39,950 --> 01:09:42,840
you how these things are connected

995
01:09:42,900 --> 01:09:53,910
maybe we can stop here and

996
01:09:53,920 --> 01:09:55,100
the next

997
01:09:55,120 --> 01:10:00,170
lecture continuing to twenty minutes that OK

998
01:10:00,210 --> 01:10:14,570
as of now there is ideological

999
01:10:15,380 --> 01:10:18,770
so before we

1000
01:10:18,790 --> 01:10:22,380
start with the next

1001
01:10:23,640 --> 01:10:26,490
let me sign some homework for you

1002
01:10:27,960 --> 01:10:29,040
this will be

1003
01:10:29,070 --> 01:10:31,980
h four

1004
01:10:31,990 --> 01:10:33,260
eight five

1005
01:10:33,280 --> 01:10:35,030
that's six

1006
01:10:35,180 --> 01:10:37,740
so you have to write down

1007
01:10:37,790 --> 01:10:41,420
the configuration table for the mechanical configuration

1008
01:10:41,430 --> 01:10:46,460
then you have to write down the configuration table for a configuration defined by the

1009
01:10:46,460 --> 01:10:50,670
set of vertices and faces of the cube

1010
01:10:51,920 --> 01:10:54,030
show that the tables from h

1011
01:10:54,060 --> 01:10:56,950
four and aged five are equivalent

1012
01:10:56,990 --> 01:10:58,200
this means that

1013
01:10:58,210 --> 01:10:59,670
you have to also

1014
01:10:59,690 --> 01:11:01,210
think about this page one

1015
01:11:01,230 --> 01:11:03,080
what it means that two

1016
01:11:03,100 --> 01:11:10,710
these are the structure are isomorphic or when the two tables define the same configuration

1017
01:11:13,070 --> 01:11:16,120
maybe just the words about age five

1018
01:11:16,130 --> 01:11:20,050
if you have a h five what is the configuration table

1019
01:11:20,080 --> 01:11:25,900
for the configuration defined by a set of vertices and faces of the cube

1020
01:11:25,910 --> 01:11:28,470
this means that we have here

1021
01:11:28,490 --> 01:11:42,450
you have to think about q

1022
01:11:42,740 --> 01:11:44,430
so it has eight

1023
01:11:56,580 --> 01:11:58,710
it has

1024
01:11:58,750 --> 01:12:00,930
six faces

1025
01:12:01,850 --> 01:12:05,500
four around one on top and bottom

1026
01:12:06,550 --> 01:12:10,290
for instance one face let me again this first phase here

1027
01:12:10,300 --> 01:12:12,880
that that may the phase one

1028
01:12:15,750 --> 01:12:16,600
and five

1029
01:12:16,620 --> 01:12:18,500
then you have some other faces

1030
01:12:20,390 --> 01:12:22,620
here for instance one

1031
01:12:22,640 --> 01:12:23,890
vertex one

1032
01:12:23,910 --> 01:12:25,640
is incident with this

1033
01:12:25,660 --> 01:12:26,730
first phase

1034
01:12:26,750 --> 01:12:30,100
and the this text five and six

1035
01:12:30,140 --> 01:12:34,680
well the other four but this is not in the first phase

1036
01:12:42,580 --> 01:12:44,520
there was a question right

1037
01:13:08,120 --> 01:13:18,000
OK so for instance here

1038
01:13:18,020 --> 01:13:23,000
we have eight candidates and for instance they are one and two minutes

1039
01:13:23,000 --> 01:13:25,270
and one of eight minutes

1040
01:13:25,290 --> 01:13:27,470
and one in seven minutes and so on

1041
01:13:27,470 --> 01:13:34,290
so for instance here you can see that one

1042
01:13:34,310 --> 01:13:36,290
and five

1043
01:13:36,310 --> 01:13:39,000
they do not meet

1044
01:13:39,020 --> 01:13:41,040
because there is no line

1045
01:13:41,100 --> 01:13:45,270
that passed through one and five so the question is

1046
01:13:45,270 --> 01:13:46,600
how many pairs

1047
01:13:46,660 --> 01:13:48,020
never meet

1048
01:13:49,140 --> 01:13:53,390
i suggest that we

1049
01:13:56,290 --> 01:14:00,350
the graph of this

1050
01:14:00,370 --> 01:14:12,020
configuration the leading graph and i would write for you

1051
01:14:12,040 --> 01:14:14,810
this graph

1052
01:14:14,830 --> 01:14:16,730
has name

1053
01:14:16,790 --> 01:14:17,910
it's called

1054
01:14:17,950 --> 01:14:20,140
maybe it's kind of graph because

1055
01:14:20,160 --> 01:14:24,870
configurations itself is called country configurations

1056
01:14:24,870 --> 01:14:26,870
and this graph

1057
01:14:26,930 --> 01:14:28,520
is also

1058
01:14:28,520 --> 01:14:35,230
it belongs to the family of a generalized petersen graphs so this is a graph

1059
01:14:36,160 --> 01:14:37,410
if we have now

1060
01:14:39,220 --> 01:14:42,410
so that we can number one number two

1061
01:14:43,730 --> 01:14:45,970
number four

1062
01:14:46,000 --> 01:14:47,290
and then

1063
01:14:52,310 --> 01:14:54,020
and the rest are

1064
01:14:54,060 --> 01:14:59,160
TV debates so that everyone TV debates

1065
01:14:59,250 --> 01:15:00,680
second TV debate

1066
01:15:00,700 --> 01:15:02,100
thirty five eight

1067
01:15:08,750 --> 01:15:11,600
so now if we look at the

1068
01:15:14,200 --> 01:15:15,120
pairs of

1069
01:15:15,120 --> 01:15:16,600
candidates that meet

1070
01:15:16,620 --> 01:15:18,040
we see that

1071
01:15:18,060 --> 01:15:22,390
for instance this candidate here

1072
01:15:22,410 --> 01:15:24,390
meets with this

1073
01:15:24,390 --> 01:15:25,470
because here

1074
01:15:25,490 --> 01:15:26,350
in this step two

1075
01:15:26,370 --> 01:15:30,580
and then it needs

1076
01:15:30,620 --> 01:15:32,520
this guy also

1077
01:15:32,540 --> 01:15:34,040
this guy also

1078
01:15:34,060 --> 01:15:36,100
this guy also

1079
01:15:36,140 --> 01:15:37,600
and this guy

1080
01:15:37,770 --> 01:15:43,060
and this guy but it never miss this guy here

1081
01:15:43,060 --> 01:15:44,680
so there are two

1082
01:15:44,700 --> 01:15:46,000
there is exactly one

1083
01:15:46,020 --> 01:15:48,520
opposite to this one

1084
01:15:48,540 --> 01:15:52,390
and since we have eight candidates and because of the symmetry

1085
01:15:52,430 --> 01:15:59,250
we see that there are four pairs

1086
01:15:59,250 --> 01:16:01,730
candidates that never mind

1087
01:16:13,600 --> 01:16:14,560
this year

1088
01:16:14,580 --> 01:16:15,770
we have

1089
01:16:15,830 --> 01:16:17,910
seven candidates

1090
01:16:20,390 --> 01:16:23,180
four seven candidates

1091
01:16:23,200 --> 01:16:26,600
if they would ever want to

1092
01:16:26,620 --> 01:16:29,830
design the same thing

1093
01:16:29,850 --> 01:16:31,600
then this would be

1094
01:16:31,620 --> 01:16:33,560
the appropriate solution

1095
01:16:33,620 --> 01:16:36,040
and in this case each pair

1096
01:16:36,120 --> 01:16:37,350
of candidates

1097
01:16:37,370 --> 01:16:39,140
i it's exactly once

1098
01:16:43,120 --> 01:16:45,200
which has this property

1099
01:16:45,270 --> 01:16:48,720
so symmetry configuration with this property is then

1100
01:16:48,790 --> 01:16:50,500
equivalent to

1101
01:16:55,000 --> 01:16:57,000
thank you

1102
01:16:57,020 --> 01:17:11,620
so now we can move to the other part

1103
01:17:11,720 --> 01:17:19,790
mutations we will be meeting permutations everywhere so let's review a little bit limitations so

1104
01:17:19,870 --> 01:17:23,270
as we all know permutations bijective mapping from

1105
01:17:23,330 --> 01:17:25,850
given set to itself

1106
01:17:25,870 --> 01:17:30,230
and permutations may be multiplied and therefore

1107
01:17:30,250 --> 01:17:32,410
the symmetric group

1108
01:17:33,700 --> 01:17:37,180
called seem of a or s subway a

1109
01:17:37,200 --> 01:17:41,850
we all know that symmetry group has two elements were and

1110
01:17:41,850 --> 01:17:46,540
optimal value function and then extract the optimal policy out of this this is very

1111
01:17:46,540 --> 01:17:48,480
important because of the low

1112
01:17:48,630 --> 01:17:54,710
reinforcement learning problem solutions which is the value function algorithms examples of those are the

1113
01:17:54,710 --> 01:17:57,850
sorts of q learning and value iteration

1114
01:17:57,870 --> 01:18:03,830
the other class is are called the policy search algorithms in which we directly search

1115
01:18:03,830 --> 01:18:07,500
in the space of policies in order to find the best policy so there is

1116
01:18:07,500 --> 01:18:12,470
an overlap between these two methods called the actor critic algorithms in which

1117
01:18:12,500 --> 01:18:16,970
you directed search in the space of policy but at the same time you maintain

1118
01:18:16,970 --> 01:18:18,320
the value function

1119
01:18:18,400 --> 01:18:21,320
so good examples of

1120
01:18:21,330 --> 01:18:26,940
actor critic is the algorithm by starting bit by condon to cyclists by on peters

1121
01:18:27,130 --> 01:18:36,780
and new algorithm by sultan bodnar myself and i know what is policy gradient algorithms

1122
01:18:36,830 --> 01:18:39,690
policy gradient algorithms the subset

1123
01:18:39,790 --> 01:18:45,470
policy search algorithms in which we use the gradient

1124
01:18:45,480 --> 01:18:49,910
in order to find the optimal policy we're going to explain it in detail whole

1125
01:18:49,910 --> 01:18:54,650
basically the gradient of what and how it works but they there is a gradient

1126
01:18:54,710 --> 01:18:56,820
involved in finding the optimal policy

1127
01:18:57,210 --> 01:19:01,690
and mean you may ask what is the policy search algorithm which is not policy

1128
01:19:01,690 --> 01:19:07,140
gradient basically any policies the algorithm research directly in the space of policies without maintaining

1129
01:19:07,750 --> 01:19:13,830
a gradient examples are let's say pegasos algorithm and green or genetic algorithms or anything

1130
01:19:13,830 --> 01:19:18,830
which doesn't each certain state policy without using value function

1131
01:19:18,980 --> 01:19:20,920
you are using gradient

1132
01:19:20,940 --> 01:19:24,140
so now let's look at different learning more

1133
01:19:24,150 --> 01:19:26,320
we have offline learning

1134
01:19:26,870 --> 01:19:28,710
h in which learning

1135
01:19:29,450 --> 01:19:35,130
we learned while interacting with a simulator not really more

1136
01:19:35,140 --> 01:19:40,100
so in that case you don't worry about the samples basically infinite number of samples

1137
01:19:40,100 --> 01:19:46,530
is provided for you we don't evaluate human trafficking environment we just after this sometimes

1138
01:19:46,590 --> 01:19:49,920
OK now give me your answer given a good policy

1139
01:19:50,020 --> 01:19:55,180
but the other one which is more interesting is online learning when we learn when

1140
01:19:55,180 --> 01:19:59,630
we interact with the environment in that case we are not free to have an

1141
01:19:59,640 --> 01:20:03,670
infinite number of samples or just van wander around anymore

1142
01:20:03,690 --> 01:20:06,590
so in the learning just

1143
01:20:06,600 --> 01:20:12,560
summarize attracted to simulate or reward and cost basically your evaluation when you're interacting with

1144
01:20:12,560 --> 01:20:19,040
the environment is not important computational time between actions is not critical and you don't

1145
01:20:19,070 --> 01:20:22,840
worry about the number of samples and the main challenge is just how to minimize

1146
01:20:22,840 --> 01:20:27,230
time to converge to optimal policy because after some time people ask you to give

1147
01:20:27,230 --> 01:20:28,220
me an answer

1148
01:20:28,350 --> 01:20:35,260
but in online learning you're directly interacting with the environment you receive reward and cost

1149
01:20:35,430 --> 01:20:40,910
so basically you being evaluated in traffic environment so here the meaning of exploration and

1150
01:20:40,910 --> 01:20:45,720
exploitation trade-off which passed scholars going to talk about it comes to the picture which

1151
01:20:45,720 --> 01:20:52,900
means how much you trust whatever i have learned so far just follow that or

1152
01:20:52,910 --> 01:20:56,870
just give a little bit and explorer in order to

1153
01:20:56,890 --> 01:20:59,630
o point something may be something better

1154
01:21:00,700 --> 01:21:04,540
and also the rear end time execution of actions important and also

1155
01:21:04,550 --> 01:21:08,770
the number of samples because you don't have infinite number of of samples

1156
01:21:08,780 --> 01:21:14,250
OK now let's look at bayesian learning and what is the bayesian approach that was

1157
01:21:14,250 --> 01:21:17,010
a very brief summary of reinforcement learning

1158
01:21:18,010 --> 01:21:19,810
what is bayesian approach

1159
01:21:19,920 --> 01:21:24,820
so in the bayesian approach you have the process is even which is

1160
01:21:24,870 --> 01:21:26,770
and you have process y

1161
01:21:26,770 --> 01:21:28,260
which you can observe

1162
01:21:28,270 --> 01:21:29,980
so let's say this is the physical

1163
01:21:29,990 --> 01:21:32,890
quantity and this is the measurement of this

1164
01:21:32,920 --> 01:21:34,800
right so this is what you observe

1165
01:21:34,810 --> 01:21:37,900
what is normal you're going to be fair

1166
01:21:37,900 --> 01:21:40,690
the heating process from the measurements

1167
01:21:40,700 --> 01:21:42,820
you observe process

1168
01:21:42,840 --> 01:21:45,170
and what is known to you

1169
01:21:45,190 --> 01:21:49,000
is the statistical dependence between

1170
01:21:49,020 --> 01:21:50,000
the he then

1171
01:21:50,010 --> 01:21:52,000
and the observable which is there

1172
01:21:52,040 --> 01:21:55,620
the conditional distribution of y given z

1173
01:21:56,240 --> 01:21:57,910
let's hope bayesian

1174
01:21:58,280 --> 01:22:05,420
attacks this problem or approaches this problem you place prior over z over your he

1175
01:22:05,430 --> 01:22:08,790
which is which reflects your uncertainty about this

1176
01:22:08,800 --> 01:22:16,380
then you observe samples from europe observable are valuable and then compute the posterior of

1177
01:22:16,380 --> 01:22:21,560
the heat and given the measurements of your observable using bayes rule

1178
01:22:21,610 --> 01:22:24,030
that's the bayesian approach

1179
01:22:24,050 --> 01:22:28,650
so the main learning has this present can't so

1180
01:22:28,660 --> 01:22:33,970
the problems are it it looks at onset thinking of a principled way

1181
01:22:33,990 --> 01:22:38,920
conceptually simple it's just you need to understand these rules

1182
01:22:38,980 --> 01:22:44,540
it's a new overfitting because of prior plays the role of regularisation here

1183
01:22:44,550 --> 01:22:46,840
or a lion regularizer here

1184
01:22:46,840 --> 01:22:51,260
fighter than the others but we doing any heavy proving

1185
01:22:51,260 --> 01:22:53,410
for very much

1186
01:22:53,420 --> 01:22:56,490
here in

1187
01:22:57,220 --> 01:22:59,570
i would like to do some

1188
01:23:02,270 --> 01:23:05,450
this is some more sort of overview

1189
01:23:06,730 --> 01:23:10,320
i'll tell you but other substructural long before i do that the job that he

1190
01:23:10,320 --> 01:23:15,590
or she can do this sort embarrassing he did remind me and didn't find in

1191
01:23:15,590 --> 01:23:21,060
germany start talking about the rules i remember in fact these women congress still systems

1192
01:23:21,060 --> 01:23:23,390
that are on

1193
01:23:23,430 --> 01:23:29,320
work translated his work in this paper called general logic which is for

1194
01:23:29,430 --> 01:23:31,190
i think the mid nineteen

1195
01:23:31,250 --> 01:23:36,790
actually the the day my head right now i think it was nineteen eighty nine

1196
01:23:36,810 --> 01:23:39,940
of in the early

1197
01:23:39,960 --> 01:23:46,260
in the australasian what he comes up with this as well look you know

1198
01:23:46,530 --> 01:23:51,390
the people that say don't understand the true for logic is a nice way of

1199
01:23:51,390 --> 01:24:00,370
looking at the moment he develops the so sequence to hide and seek

1200
01:24:00,560 --> 01:24:08,610
natural deduction system that was really quite good is very very nice paper and greg

1201
01:24:08,950 --> 01:24:15,460
restall work greg is drawn credit but i should have as well

1202
01:24:15,650 --> 01:24:17,040
OK now

1203
01:24:19,420 --> 01:24:27,700
back around nineteen eighty was here a lot of mine

1204
01:24:27,820 --> 01:24:30,890
you may have met barbara the first part can appear

1205
01:24:30,950 --> 01:24:39,530
of those to johnson is momentous post-doc i was here is the first after job

1206
01:24:39,530 --> 01:24:43,220
of in our paper for conference

1207
01:24:43,340 --> 01:24:46,960
the conference was sort of research

1208
01:24:46,980 --> 01:24:49,010
the conference was

1209
01:24:51,510 --> 01:24:59,940
logics with reduced structural rules which was kind of an ugly

1210
01:24:59,960 --> 01:25:03,800
one of its organizers

1211
01:25:04,120 --> 01:25:11,440
costa doesn't mention is serbian who found all of this work he

1212
01:25:11,460 --> 01:25:16,620
renamed trouble or what does that mean it means that

1213
01:25:16,630 --> 01:25:22,150
you give a list of the usual structure rules all sets

1214
01:25:25,960 --> 01:25:30,000
religious are ones that reject what are these rules

1215
01:25:30,030 --> 01:25:32,090
so since k

1216
01:25:32,120 --> 01:25:43,470
in particular a rule larger sets the average xk it's substructural substructural logics

1217
01:25:43,490 --> 01:25:52,550
OK and some of them are weaker than we that that's what are relevant implication

1218
01:26:03,240 --> 01:26:21,680
why do we care what we saw the relatlon

1219
01:26:21,690 --> 01:26:24,020
this idea

1220
01:26:24,030 --> 01:26:27,440
so i mean case particularly bad for because

1221
01:26:27,490 --> 01:26:29,340
it's not just a

1222
01:26:29,340 --> 01:26:31,280
the problem of having

1223
01:26:31,300 --> 01:26:35,960
these paradoxes of implication should but just general found

1224
01:26:36,000 --> 01:26:41,830
now k was problem or just arrested because it says you can always add in

1225
01:26:41,840 --> 01:26:43,150
to present

1226
01:26:43,220 --> 01:26:46,650
what comes you want or premises

1227
01:26:46,680 --> 01:26:52,030
two and you still have valid and this is how can we can in premises

1228
01:26:52,030 --> 01:26:54,900
in the they might be irrelevant to the conclusions

1229
01:26:54,960 --> 01:26:59,810
we don't know why i had the number system feature that we have

1230
01:26:59,840 --> 01:27:04,550
that subscripts system i we share we had this

1231
01:27:04,570 --> 01:27:08,460
the fitch style system

1232
01:27:08,730 --> 01:27:13,220
so that's why it was rejected

1233
01:27:13,630 --> 01:27:14,660
we can

1234
01:27:16,880 --> 01:27:19,820
but larger role of text

1235
01:27:19,840 --> 01:27:22,650
another model logical systems

1236
01:27:22,780 --> 01:27:25,940
or for other reasons

1237
01:27:25,950 --> 01:27:29,530
or for those reasons plus other reasons

1238
01:27:32,030 --> 01:27:34,340
the turn of the twentieth century

1239
01:27:34,350 --> 01:27:35,650
thank you

1240
01:27:35,730 --> 01:27:39,850
ninety two ninety three

1241
01:27:39,900 --> 01:27:43,580
a few logicians that bertrand russell gets credit for that

1242
01:27:43,590 --> 01:27:46,070
russell's paradox

1243
01:27:47,210 --> 01:27:54,860
um earned similar to resume set there in discovered almost exactly the same time

1244
01:27:54,890 --> 01:27:58,780
discovering paradoxes box

1245
01:27:58,990 --> 01:28:01,300
some of you have seen before

1246
01:28:02,830 --> 01:28:04,630
so the

1247
01:28:04,690 --> 01:28:06,760
that suppose we can

1248
01:28:06,780 --> 01:28:14,760
what started as a general comprehension scheme

1249
01:28:16,780 --> 01:28:19,230
so on

1250
01:28:36,510 --> 01:28:38,710
this is wrong i can

1251
01:28:38,720 --> 01:28:41,440
for any formula a

1252
01:28:41,460 --> 01:28:42,650
there are some

1253
01:28:42,650 --> 01:28:46,380
here on this side we have a much more direct use we're actually learning the

1254
01:28:46,380 --> 01:28:50,340
thing we really want but the learning is much more indirect because we don't get

1255
01:28:50,340 --> 01:28:55,190
exact training examples from the environment on this side the learning itself is very direct

1256
01:28:55,390 --> 01:28:59,720
every time we take a step environment another training example but to use it now

1257
01:28:59,720 --> 01:29:03,200
requires a lot of grinding away in computation

1258
01:29:03,830 --> 01:29:08,750
depending on the kind of problem that you're working on where you fit on this

1259
01:29:09,020 --> 01:29:13,100
what is the appropriate method use it varies so in a lot of the robot

1260
01:29:13,100 --> 01:29:17,670
stuff that we've done computation is much faster than the robot getting from point a

1261
01:29:17,670 --> 01:29:21,040
to point b robot is very slow in the world

1262
01:29:21,090 --> 01:29:28,770
o had twice the world purple come in purple box there go so

1263
01:29:28,780 --> 01:29:32,080
apparently purple races were my context

1264
01:29:32,100 --> 01:29:36,940
right so in the robot examples data is really expensive it takes a very long

1265
01:29:36,940 --> 01:29:40,090
time for the robot to actually make it across the room but compared to that

1266
01:29:40,090 --> 01:29:43,740
computation is relatively cheap so a lot of our the stuff that we've done in

1267
01:29:43,740 --> 01:29:44,780
my lab

1268
01:29:44,800 --> 01:29:49,170
has focused on model based approaches on this i don't really great talk recently i

1269
01:29:49,170 --> 01:29:51,790
think it was microsoft guy two

1270
01:29:51,830 --> 01:29:54,780
and what he was working on was

1271
01:29:54,830 --> 01:29:59,830
low level architectural decisions in this in the circuit of computer

1272
01:29:59,880 --> 01:30:02,060
the memory circuit like deciding which

1273
01:30:02,070 --> 01:30:07,540
memory accesses to do at which time when you've got multi threaded architecture i say

1274
01:30:07,580 --> 01:30:11,930
something like that and what was in his case he had absolutely no computation time

1275
01:30:11,930 --> 01:30:16,580
and all every every learning every piece of learning had to happen at clock speed

1276
01:30:16,670 --> 01:30:19,980
right so you have no time but on the other hand in you know in

1277
01:30:19,980 --> 01:30:23,920
three seconds you get two million training examples or something like that right so tons

1278
01:30:23,920 --> 01:30:28,460
and tons of data data is basically free computations expensive not surprisingly he ended up

1279
01:30:28,460 --> 01:30:32,540
using a mix it wasn't even a policy search here the using of value function

1280
01:30:32,540 --> 01:30:36,490
based methods so he was able to do this max fast enough and he was

1281
01:30:36,490 --> 01:30:41,730
able to to back propagation information train fast enough and and we asked him questions

1282
01:30:41,730 --> 01:30:45,350
like well you know what happens if you when you retrain is like we don't

1283
01:30:45,350 --> 01:30:49,520
retrain it just constantly retraining itself because the data just flowing in such an incredible

1284
01:30:49,520 --> 01:30:53,540
rate so i don't believe anybody with depression that i'm here to bash

1285
01:30:53,550 --> 01:30:56,460
any methods on this side but i'm going to focus on methods on that site

1286
01:30:57,370 --> 01:30:59,130
because they are interesting

1287
01:30:59,150 --> 01:31:00,400
all right so

1288
01:31:00,440 --> 01:31:04,290
i'm going to tell you about two learning which is kind of the classic prototypical

1289
01:31:04,310 --> 01:31:08,440
method in the middle column value face function value function

1290
01:31:08,440 --> 01:31:13,210
based because it for two reasons one is it's almost hard not to go out

1291
01:31:13,250 --> 01:31:16,860
and try this because this is the algorithm is already on the two lines of

1292
01:31:16,860 --> 01:31:18,920
the slide

1293
01:31:18,940 --> 01:31:22,610
and it really does some some remarkable things it just it's very slow to learn

1294
01:31:22,610 --> 01:31:24,990
so how does this work

1295
01:31:25,000 --> 01:31:28,680
what's happening is we've got an aging etc etc at some state chooses an action

1296
01:31:28,680 --> 01:31:33,170
it finds itself in some next state with some rewards so we have that SARS

1297
01:31:33,190 --> 01:31:37,050
a piece of data and now the question is what do we do

1298
01:31:37,100 --> 01:31:40,170
well what the q learning algorithm does is it keeps the table or keep some

1299
01:31:40,170 --> 01:31:44,510
kind of function that maps states the q function maps states and actions to future

1300
01:31:44,510 --> 01:31:48,530
work and it's going to update that function in the face of this new

1301
01:31:49,620 --> 01:31:53,240
as follows as i have this sort of cartoon version of q because there's the

1302
01:31:53,240 --> 01:31:56,590
real q that i defined before that's the true if you actually knew what the

1303
01:31:56,590 --> 01:32:00,670
environment was that's the real key function this is some kind of approximation of some

1304
01:32:00,670 --> 01:32:02,780
cartoon and so on

1305
01:32:02,780 --> 01:32:05,680
we're going to update that by taking

1306
01:32:05,710 --> 01:32:10,690
the it's all value and adding a little bit learning rate amount of it too

1307
01:32:11,610 --> 01:32:16,380
what we have to it is the immediate reward plus the discounted approximation of what

1308
01:32:16,380 --> 01:32:19,560
the value the next state is stated plus one

1309
01:32:19,570 --> 01:32:23,240
minus with the old value so this is the difference in

1310
01:32:23,270 --> 01:32:26,940
if you look at is the concept of the temporal difference as how old is

1311
01:32:26,940 --> 01:32:28,260
my prediction

1312
01:32:28,280 --> 01:32:30,370
now differ from what is

1313
01:32:30,380 --> 01:32:32,580
it would be one step later

1314
01:32:33,680 --> 01:32:38,150
right so remember the value as the as a value the earliest at time t

1315
01:32:38,150 --> 01:32:42,050
as a value should be equal to the expected value of the reward plus the

1316
01:32:42,050 --> 01:32:44,780
discounted value of the future states so

1317
01:32:44,790 --> 01:32:47,140
what we're doing here is is trying to bring those two things back in line

1318
01:32:47,140 --> 01:32:48,770
with each other

1319
01:32:49,080 --> 01:32:52,960
so on that's the whole thing right that's the whole algorithm you just keep doing

1320
01:32:52,960 --> 01:32:56,360
that over and over and over and over again if it's the case that we

1321
01:32:56,360 --> 01:33:00,040
visit all states and actions infinitely often and we decay learning rate according to the

1322
01:33:00,040 --> 01:33:04,700
standard rules of decaying learning rates then this cartoon value version of q

1323
01:33:04,710 --> 01:33:06,730
actually converges to

1324
01:33:06,740 --> 01:33:08,630
the real q function

1325
01:33:10,210 --> 01:33:13,610
it solves the whole MDP the only way that we notice of mdps in polynomial

1326
01:33:13,610 --> 01:33:18,750
time is using linear programming to to fairly sophisticated computational procedure but really you can

1327
01:33:18,750 --> 01:33:22,730
get the exact same answer in the limit by just doing is located here

1328
01:33:22,800 --> 01:33:23,960
so it's kind of neat

1329
01:33:23,970 --> 01:33:30,630
so so one thing one thing that one affected that had because it's so easy

1330
01:33:30,630 --> 01:33:32,220
to implement there's

1331
01:33:32,220 --> 01:33:35,640
when i guess this continues to be a lot of people who just implement this

1332
01:33:35,640 --> 01:33:39,530
some of the

1333
01:33:39,560 --> 01:33:42,060
actually i

1334
01:33:42,080 --> 01:33:45,870
why you come

1335
01:33:45,890 --> 01:33:51,540
which have all kinds of one so strong non

1336
01:33:51,570 --> 01:33:53,440
not relevant at all

1337
01:33:53,450 --> 01:33:56,290
just one person

1338
01:33:57,570 --> 01:34:01,910
nine also have all kinds of

1339
01:34:01,940 --> 01:34:03,910
but at least the

1340
01:34:03,940 --> 01:34:05,510
i agree

1341
01:34:06,750 --> 01:34:10,430
so here we have example station that everyone

1342
01:34:10,450 --> 01:34:13,390
considers which is relevant at all

1343
01:34:16,310 --> 01:34:17,940
on market

1344
01:34:17,970 --> 01:34:22,330
well this is one of the

1345
01:34:22,350 --> 01:34:25,660
you should be the person who

1346
01:34:25,690 --> 01:34:27,040
they might

1347
01:34:30,520 --> 01:34:31,700
and is

1348
01:34:31,710 --> 01:34:37,870
had such distribution marks

1349
01:34:37,880 --> 01:34:40,510
so should

1350
01:34:40,630 --> 01:34:44,460
right as they

1351
01:34:50,140 --> 01:34:51,330
i mention

1352
01:34:51,430 --> 01:34:53,660
so the weight

1353
01:34:54,880 --> 01:35:00,850
so we the participants in the experiment

1354
01:35:00,880 --> 01:35:02,950
so we should

1355
01:35:03,140 --> 01:35:05,910
these just

1356
01:35:12,820 --> 01:35:16,370
strong grounds for

1357
01:35:16,390 --> 01:35:17,450
o two

1358
01:35:17,470 --> 01:35:18,340
you have

1359
01:35:19,440 --> 01:35:26,510
all this thing

1360
01:35:31,830 --> 01:35:36,320
mention the relevant says we

1361
01:35:36,500 --> 01:35:39,130
it is

1362
01:35:40,410 --> 01:35:43,720
he wasn't happy to participate

1363
01:35:44,200 --> 01:35:48,680
and we have to where

1364
01:35:48,690 --> 01:35:51,780
and here are some numbers

1365
01:35:51,790 --> 01:35:52,790
because one

1366
01:35:57,010 --> 01:35:59,810
so that that's just too much

1367
01:36:04,330 --> 01:36:06,770
i mean there's

1368
01:36:12,210 --> 01:36:15,060
we have this week

1369
01:36:15,830 --> 01:36:18,240
participants on the bottom

1370
01:36:18,280 --> 01:36:21,370
and in

1371
01:36:22,840 --> 01:36:27,260
mark this by these opposite suppose we have one

1372
01:36:31,270 --> 01:36:33,790
so i want

1373
01:36:39,680 --> 01:36:41,780
i have had

1374
01:36:45,320 --> 01:36:45,890
the band

1375
01:36:46,990 --> 01:36:49,640
which came to an end

1376
01:36:52,140 --> 01:36:54,710
so we want to do

1377
01:36:54,720 --> 01:36:57,870
on those

1378
01:36:57,880 --> 01:37:00,190
which might

1379
01:37:00,210 --> 01:37:01,320
like that

1380
01:37:01,600 --> 01:37:04,960
all right

1381
01:37:07,710 --> 01:37:12,250
the question how to compare these things because

1382
01:37:13,070 --> 01:37:16,270
people use

1383
01:37:16,380 --> 01:37:19,960
like the rest is just one

1384
01:37:20,120 --> 01:37:23,930
during the number man's which

1385
01:37:28,650 --> 01:37:31,210
even if you have one

1386
01:37:34,120 --> 01:37:36,300
also we have

1387
01:37:36,360 --> 01:37:42,390
ninety percent probability to win

1388
01:37:42,400 --> 01:37:43,790
at the end

1389
01:37:44,590 --> 01:37:45,720
this one

1390
01:37:49,430 --> 01:37:50,340
all right

1391
01:37:50,360 --> 01:37:55,370
women have much lower position because they use another one

1392
01:38:00,220 --> 01:38:03,060
i also related

1393
01:38:03,090 --> 01:38:07,070
the second thing is that we have also very

1394
01:38:07,100 --> 01:38:10,420
big difference i would like to call

1395
01:38:10,440 --> 01:38:13,290
people so some of the participants

1396
01:38:13,310 --> 01:38:16,770
i don't think it was just

1397
01:38:19,940 --> 01:38:20,880
two persons

1398
01:38:22,230 --> 01:38:24,040
you know

1399
01:38:25,030 --> 01:38:27,640
there you just right

1400
01:38:28,680 --> 01:38:33,590
and there was the was equal to zero

1401
01:38:33,640 --> 01:38:37,040
the image is marked as not well

1402
01:38:37,090 --> 01:38:38,140
the same

1403
01:38:38,160 --> 01:38:40,120
like what

1404
01:38:40,850 --> 01:38:42,980
using one

1405
01:38:43,000 --> 01:38:44,270
so the

1406
01:38:46,030 --> 01:38:48,590
actually already just

1407
01:38:48,640 --> 01:38:50,100
above zero

1408
01:38:50,170 --> 01:38:52,810
here's one

1409
01:38:52,840 --> 01:38:53,740
so that the

1410
01:38:59,340 --> 01:39:00,530
so it also

1411
01:39:00,570 --> 01:39:03,040
depends very much

1412
01:39:03,060 --> 01:39:04,390
so i

1413
01:39:04,400 --> 01:39:05,940
these guys

1414
01:39:07,180 --> 01:39:08,560
there there

1415
01:39:09,640 --> 01:39:13,220
this might be a problem

1416
01:39:13,230 --> 01:39:17,130
if you have one

1417
01:39:30,660 --> 01:39:33,250
the last

1418
01:39:35,220 --> 01:39:36,510
talk about

1419
01:39:48,940 --> 01:39:51,260
a lot

1420
01:39:57,380 --> 01:40:01,530
it's clear from ten books

1421
01:40:03,490 --> 01:40:06,020
there will again

1422
01:40:06,040 --> 01:40:09,410
without it they would be

1423
01:40:18,750 --> 01:40:21,640
but what i mentioned

1424
01:40:22,710 --> 01:40:24,940
and the about the

1425
01:40:30,480 --> 01:40:34,100
she has

1426
01:40:34,120 --> 01:40:36,710
so just to remind you why

1427
01:40:38,640 --> 01:40:40,770
in the

1428
01:40:40,810 --> 01:40:42,980
which is to have

1429
01:40:43,860 --> 01:40:51,530
for all feature extraction algorithms have been featured it could be several feature vectors which

1430
01:40:51,940 --> 01:40:54,460
are usually high dimensional

1431
01:40:56,010 --> 01:40:56,960
two index

1432
01:40:57,000 --> 01:40:59,550
somehow this one does

1433
01:40:59,560 --> 01:41:04,610
i mean you have no images and what they did

1434
01:41:05,760 --> 01:41:07,810
hundreds of not

1435
01:41:07,840 --> 01:41:11,000
and to compare

1436
01:41:11,090 --> 01:41:12,790
o two three

1437
01:41:12,810 --> 01:41:14,820
you want someone to work

1438
01:41:16,570 --> 01:41:21,170
this the way that was the same over the whole thing

1439
01:41:21,230 --> 01:41:22,210
the same

1440
01:41:22,230 --> 01:41:27,030
each of us and we need to come with to the site to compare

1441
01:41:27,050 --> 01:41:28,570
you never know

1442
01:41:29,530 --> 01:41:32,500
and they have one hundred

1443
01:41:32,510 --> 01:41:38,600
now are going to mention of course the

1444
01:41:38,610 --> 01:41:42,540
and the structures which

1445
01:41:42,540 --> 01:41:45,970
poor monte carlo representation of the density

1446
01:41:45,980 --> 01:41:49,930
so your estimates could be unreliable would be unreliable

1447
01:41:49,930 --> 01:41:54,730
so resampling kind fixes that no resampling bit

1448
01:41:54,750 --> 01:42:00,750
conceptual problem in

1449
01:42:00,770 --> 01:42:06,270
particle filtering you know the theorists have done analysis of this resampling thing then under

1450
01:42:06,270 --> 01:42:10,560
certain circumstances to find out how the errors accumulate over time

1451
01:42:10,660 --> 01:42:13,600
and the stuff you can do it you can do not for every type of

1452
01:42:13,700 --> 01:42:19,160
sampling scheme but it's a bit of black how widely it and how it works

1453
01:42:19,270 --> 01:42:23,250
indeed some people is rapidly you'll hear this

1454
01:42:23,280 --> 01:42:29,100
which radford had has had a long running debate about whether you should use resampling

1455
01:42:29,100 --> 01:42:34,770
or not in this type of these type of sequential importance sampling schemes because they

1456
01:42:34,770 --> 01:42:40,540
do in fact introduce monte carlo are nevertheless over time there absolutely essential for reducing

1457
01:42:40,600 --> 01:42:42,290
the accumulation of error

1458
01:42:42,310 --> 01:42:44,120
more about that later

1459
01:42:44,230 --> 01:42:49,560
but recently what is it actually it's basically

1460
01:42:49,560 --> 01:42:54,480
instead of proceeding through time with weights accumulating as you go

1461
01:42:54,560 --> 01:42:59,330
through time we can take the monte carlo search and resample it so they have

1462
01:42:59,330 --> 01:43:04,100
uniform weights and the way you do that is to take is to is very

1463
01:43:04,830 --> 01:43:09,790
in times square and the number of particles if i could have it and it

1464
01:43:09,910 --> 01:43:15,890
in the end times but some large number of times you simply choose for a

1465
01:43:15,890 --> 01:43:17,910
new particle ecstatic

1466
01:43:17,950 --> 01:43:22,790
to be equal to the previous any of the previous particles of i with probability

1467
01:43:22,790 --> 01:43:28,560
equal to their weight so to sampling with replacement from a multinomial distribution having weights

1468
01:43:28,560 --> 01:43:30,390
w having done that

1469
01:43:30,430 --> 01:43:33,520
released in the limit as the number of particles is very large you've got an

1470
01:43:33,520 --> 01:43:41,430
equivalent representation of n particles with weights set to one over the idea being that

1471
01:43:41,540 --> 01:43:47,750
any particles very tiny weights never or hardly ever get get chosen in this resampling

1472
01:43:47,750 --> 01:43:53,080
stage but any with high weight so presumably very important for the representation they get

1473
01:43:53,080 --> 01:43:58,100
selected the very often so you end up with a replenished set of samples that's

1474
01:43:58,100 --> 01:43:58,810
good for

1475
01:43:58,950 --> 01:44:01,120
propagating to the next time step

1476
01:44:01,120 --> 01:44:06,580
which you wouldn't have had otherwise with other resampling

1477
01:44:06,580 --> 01:44:10,520
we wouldn't want to do this every time step because it does introduce unwanted color

1478
01:44:10,910 --> 01:44:15,980
of its basically wait until the samples have become the impoverished

1479
01:44:16,000 --> 01:44:21,660
and degenerate but then you resample

1480
01:44:22,270 --> 01:44:24,560
so i think that's just text that summarizes

1481
01:44:24,580 --> 01:44:25,890
what i said

1482
01:44:25,930 --> 01:44:30,040
this is the this is the standard multinomial resampling schemes are much better ways you

1483
01:44:30,040 --> 01:44:34,930
can do this to give you a slightly better performance using ideas from monte carlo

1484
01:44:34,930 --> 01:44:39,850
stratification and so on

1485
01:44:39,870 --> 01:44:45,470
OK so we have a general scheme here for approximating the filtering density over time

1486
01:44:45,540 --> 01:44:48,140
would also expectations

1487
01:44:48,480 --> 01:44:51,710
but it doesn't quite work and in the sequential

1488
01:44:51,770 --> 01:44:57,540
context because we can't get calculate the importance weights so to get the importance weights

1489
01:44:57,540 --> 01:45:00,980
we needed

1490
01:45:01,000 --> 01:45:05,410
direct access to the filtering density to evaluate the weight

1491
01:45:05,470 --> 01:45:09,500
we may well be about evaluate here the proposal will almost certainly won't be able

1492
01:45:09,520 --> 01:45:14,350
to evaluate p the target filtering density thirty one more

1493
01:45:14,410 --> 01:45:16,870
step in here

1494
01:45:16,870 --> 01:45:22,430
and this takes then to the sequential monte carlo filter or the particle filter so

1495
01:45:22,430 --> 01:45:29,750
the generic solution to this involves the repeated importance sampling resampling sequentially through time

1496
01:45:30,450 --> 01:45:35,560
and it mimics the filtering recursions in monte carlo fashion so does exactly the same

1497
01:45:35,560 --> 01:45:40,100
thing as the colony cells of using monte carlo version

1498
01:45:40,120 --> 01:45:41,270
the filter

1499
01:45:41,310 --> 01:45:43,870
of the of the of the steps

1500
01:45:45,250 --> 01:45:47,660
so to outline the broader principle

1501
01:45:47,710 --> 01:45:48,980
behind it

1502
01:45:49,000 --> 01:45:53,310
suppose we have a collection of samples

1503
01:45:53,330 --> 01:45:55,600
again we solve the problem at times

1504
01:45:55,600 --> 01:46:00,450
take so we have an much says that means we've got a collection of samples

1505
01:46:00,450 --> 01:46:05,200
x i have to be drawn from the correct filtering distribution px t

1506
01:46:05,210 --> 01:46:07,160
given all the data up to t

1507
01:46:07,200 --> 01:46:12,600
again we can write that in terms of unweighted dirac functions if we like his

1508
01:46:12,710 --> 01:46:18,810
i switch notation i'm afraid to a different definition of the function centred upon

1509
01:46:18,830 --> 01:46:20,600
x y

1510
01:46:20,620 --> 01:46:23,230
the function of x

1511
01:46:23,250 --> 01:46:25,890
we substitute that into the prediction equation

1512
01:46:25,890 --> 01:46:27,270
so drops out

1513
01:46:27,290 --> 01:46:28,430
so we've got

1514
01:46:28,430 --> 01:46:31,730
this is what we needed to do for the first step

1515
01:46:31,730 --> 01:46:33,640
of the filtering

1516
01:46:33,660 --> 01:46:35,790
equations for the first step

1517
01:46:35,810 --> 01:46:40,890
involved taking the filtering distribution from the previous timestep which we've now got gotten monte

1518
01:46:40,890 --> 01:46:42,580
carlo representation for that

1519
01:46:43,970 --> 01:46:45,830
well the state transition density

1520
01:46:45,850 --> 01:46:49,100
and integrating out the previous state activity

1521
01:46:49,120 --> 01:46:50,910
we simply take this

1522
01:46:52,020 --> 01:46:52,970
the previous

1523
01:46:52,980 --> 01:46:55,770
times approximation to the filtering density

1524
01:46:55,830 --> 01:46:57,450
like so

1525
01:46:57,640 --> 01:47:00,450
these are samples obtained at the previous time

1526
01:47:00,510 --> 01:47:02,540
a t

1527
01:47:02,560 --> 01:47:05,660
and then do the integral using the sifting property again so we are not just

1528
01:47:05,660 --> 01:47:08,370
in the not like with the normal monte carlo integral

1529
01:47:08,390 --> 01:47:10,700
with arithmetic mean

1530
01:47:10,730 --> 01:47:17,200
of all the transition densities f evaluated with the previous samples locations

1531
01:47:17,270 --> 01:47:20,120
xt y

1532
01:47:20,120 --> 01:47:21,920
so want to specify to some

1533
01:47:21,980 --> 01:47:23,920
some precision

1534
01:47:23,960 --> 01:47:27,500
number of bits required to specify

1535
01:47:27,500 --> 01:47:29,520
it's going to be

1536
01:47:32,250 --> 01:47:33,580
that square

1537
01:47:35,580 --> 01:47:39,710
so the number of bits

1538
01:47:39,730 --> 01:47:41,350
quite by

1539
01:47:41,350 --> 01:47:44,750
the actual markov decision process

1540
01:47:44,750 --> 01:47:46,620
is order as per day

1541
01:47:46,620 --> 01:47:49,830
and then interaction we need with the world

1542
01:47:49,850 --> 01:47:51,940
are the behaviour optimally

1543
01:47:51,980 --> 01:47:54,810
it's only of SA

1544
01:47:54,850 --> 01:47:56,960
so the the tell here means the system

1545
01:47:56,980 --> 01:48:00,230
sometimes depend upon log of that's a

1546
01:48:00,250 --> 01:48:02,400
so this is is that we can

1547
01:48:02,420 --> 01:48:05,950
interact with the world and the amount

1548
01:48:06,000 --> 01:48:06,790
which is

1549
01:48:06,790 --> 01:48:09,290
less than the description of the world

1550
01:48:09,290 --> 01:48:12,600
in order to behave near optimal way

1551
01:48:15,560 --> 01:48:20,500
if this is most interesting to me because there have been debates in reinforcement learning

1552
01:48:20,500 --> 01:48:22,210
about what that it was necessary

1553
01:48:22,270 --> 01:48:25,640
ten our them which in turn build model of the world

1554
01:48:26,560 --> 01:48:28,080
this seven doesn't

1555
01:48:28,100 --> 01:48:32,020
and can because the amount of experience that uses

1556
01:48:32,060 --> 01:48:34,160
is less than that required

1557
01:48:34,210 --> 01:48:44,420
to actually build an accurate internal model

1558
01:48:54,650 --> 01:48:57,630
right to the overloading here

1559
01:48:57,730 --> 01:49:00,710
so the lower case s is the state

1560
01:49:02,000 --> 01:49:02,960
he there

1561
01:49:02,980 --> 01:49:05,310
the number of states like here

1562
01:49:05,310 --> 01:49:07,060
or the set of states

1563
01:49:07,420 --> 01:49:10,270
so this is the number of states

1564
01:49:10,330 --> 01:49:11,660
number of actions

1565
01:49:11,670 --> 01:49:14,330
so this is in our state times actions

1566
01:49:14,350 --> 01:49:21,770
first it

1567
01:49:26,580 --> 01:49:30,850
so i guess thing to which you can realise that this algorithm is is significantly

1568
01:49:30,850 --> 01:49:32,020
different from

1569
01:49:32,080 --> 01:49:33,980
and the hugh durrant because

1570
01:49:35,060 --> 01:49:38,770
this is strategy of trying to build a model of the world

1571
01:49:38,810 --> 01:49:41,390
can work with that same complexity

1572
01:49:49,230 --> 01:49:51,210
these are seven positive results

1573
01:49:51,230 --> 01:49:55,350
and then i want to go through a short section or

1574
01:49:55,460 --> 01:49:57,250
saw talk about

1575
01:49:57,310 --> 01:50:01,040
the limitations of simplicity approached reinforcement learning

1576
01:50:01,060 --> 01:50:09,500
and what people have done to try to address these limitations

1577
01:50:09,830 --> 01:50:15,020
so our first limitation is given to you a lower bound

1578
01:50:15,080 --> 01:50:16,700
so the claim is that

1579
01:50:16,710 --> 01:50:18,810
the rhythm a

1580
01:50:18,830 --> 01:50:21,000
which satisfies the statement

1581
01:50:21,020 --> 01:50:23,250
must use at least

1582
01:50:23,250 --> 01:50:25,060
t s a

1583
01:50:25,080 --> 01:50:27,420
actions to explore

1584
01:50:27,440 --> 01:50:30,290
so we have to allow ourselves

1585
01:50:30,730 --> 01:50:34,100
to markov at least ESA actions in order to

1586
01:50:34,190 --> 01:50:36,540
to get any cube like statement

1587
01:50:36,650 --> 01:50:42,790
and the somewhat stronger lower bounds which actually involves the on but we'll worry about

1588
01:50:43,500 --> 01:50:47,730
because this is sufficient in the the proof is actually very simple

1589
01:50:47,810 --> 01:50:49,460
the proof is just

1590
01:50:49,480 --> 01:50:52,620
here's an MDP here his family of mdps

1591
01:50:52,620 --> 01:50:55,230
the claim is you need TSA

1592
01:50:55,250 --> 01:50:58,060
actions have to learn to behave near optimally

1593
01:50:58,850 --> 01:51:03,550
one thing to do is here i have to agree with demonstrating this

1594
01:51:03,600 --> 01:51:06,350
so here we have

1595
01:51:06,400 --> 01:51:07,900
so for the pathway

1596
01:51:07,950 --> 01:51:10,140
the pathway kind of goes right

1597
01:51:11,980 --> 01:51:13,100
if i

1598
01:51:13,120 --> 01:51:16,810
behave if they behave

1599
01:51:16,810 --> 01:51:18,330
well in addition to

1600
01:51:20,730 --> 01:51:22,760
so we have to look at the way

1601
01:51:22,810 --> 01:51:25,440
and we have

1602
01:51:25,500 --> 01:51:28,080
maybe two actions

1603
01:51:28,120 --> 01:51:29,560
per state right

1604
01:51:29,600 --> 01:51:32,170
so we have a one and a two

1605
01:51:32,190 --> 01:51:35,290
o twenty three

1606
01:51:35,340 --> 01:51:37,430
yes very good reactions state

1607
01:51:37,440 --> 01:51:39,960
a one a two and a three

1608
01:51:39,980 --> 01:51:44,940
and i think about a family market decision processes

1609
01:51:45,690 --> 01:51:48,830
the action which leads to the next in the

1610
01:51:48,830 --> 01:51:51,790
is randomized it just independently

1611
01:51:52,690 --> 01:51:56,890
learning how to behave well here goes nothing about how to be will here so

1612
01:51:56,890 --> 01:51:58,600
maybe section eight two

1613
01:51:58,620 --> 01:52:03,690
the next to hear the sine one and section eight three in affectionate

1614
01:52:05,600 --> 01:52:07,410
markov decision process in this

1615
01:52:07,420 --> 01:52:08,930
in this family

1616
01:52:08,940 --> 01:52:13,860
know how to be a few does nothing about how be a few

1617
01:52:13,910 --> 01:52:17,910
and then all actions just take you back to the beginning

1618
01:52:17,960 --> 01:52:20,560
i think that we have to do in order to

1619
01:52:20,730 --> 01:52:22,960
figure out how to get the in the past

1620
01:52:23,000 --> 01:52:24,640
and receive the rewards

1621
01:52:25,290 --> 01:52:28,540
so in every state

1622
01:52:28,590 --> 01:52:30,200
you have to execute

1623
01:52:30,210 --> 01:52:31,870
about half the actions

1624
01:52:31,870 --> 01:52:34,690
in order to figure out which ones to to the next state

1625
01:52:34,770 --> 01:52:39,810
and every time you execute the wrong action you go back to the beginning

1626
01:52:41,850 --> 01:52:45,930
in every state the second half the action to test times a you go back

1627
01:52:45,930 --> 01:52:53,130
to the beginning which is where the team comes in

1628
01:52:54,980 --> 01:52:59,290
you can do better than TSA is distressing

1629
01:53:00,440 --> 01:53:07,660
the reason distressing because the real world the number of actions going talk to physicists

1630
01:53:07,670 --> 01:53:10,250
and you ask him how he states

1631
01:53:10,980 --> 01:53:12,890
well you know

1632
01:53:13,060 --> 01:53:14,350
maybe you can

1633
01:53:14,400 --> 01:53:16,280
you can kind of these things which

1634
01:53:17,870 --> 01:53:22,040
the same after that the playing scale just like the minus thirty four meters

1635
01:53:22,080 --> 01:53:24,600
the number of states

1636
01:53:24,770 --> 01:53:28,880
which specify your location with this room

1637
01:53:28,920 --> 01:53:30,250
is larger

1638
01:53:30,260 --> 01:53:32,350
and the memory of any computer

1639
01:53:32,370 --> 01:53:34,190
then we can ever hope to make

1640
01:53:34,240 --> 01:53:37,920
and this says that

1641
01:53:37,960 --> 01:53:38,880
you know

1642
01:53:38,890 --> 01:53:40,710
for general MDP

1643
01:53:40,750 --> 01:53:47,050
you can depend upon the number of states

1644
01:53:47,250 --> 01:53:53,840
that's a bit of a problem right

1645
01:53:54,020 --> 01:54:00,880
so lower bounds is it really big problems are not going to be solvable

1646
01:54:00,930 --> 01:54:02,510
but if you are

1647
01:54:04,990 --> 01:54:07,770
and then we have a sort of conundrum because

1648
01:54:07,790 --> 01:54:12,300
we know the problems that we want to solve a solvable because reinforcement learning

1649
01:54:12,340 --> 01:54:16,300
it is going to mathematical description of life you are an agent one thing in

1650
01:54:16,300 --> 01:54:17,840
the world

1651
01:54:18,760 --> 01:54:22,300
use all of your exploration exploitation problems every day

1652
01:54:23,480 --> 01:54:27,390
so somehow the method is failing to capture

1653
01:54:27,400 --> 01:54:29,560
what is actually do

1654
01:54:29,610 --> 01:54:35,630
so what that means mathematically is that we need to make more different assumptions

1655
01:54:35,650 --> 01:54:38,740
they have been several attempts to make more assumptions

1656
01:54:38,740 --> 01:54:40,280
use a jeep modeling context

1657
01:54:41,570 --> 01:54:45,180
also gives opportunity to draw an interesting connection between

1658
01:54:45,640 --> 01:54:47,350
between lesbians and engine piece

1659
01:54:50,200 --> 01:54:50,980
here's the cheapy

1660
01:54:51,430 --> 01:54:52,970
joint right so we would be

1661
01:54:53,770 --> 01:54:54,490
we look that they

1662
01:54:55,620 --> 01:54:57,120
joint distribution pty enough

1663
01:54:57,630 --> 01:54:59,470
i mean take the the minus log there

1664
01:55:00,000 --> 01:55:01,050
and what you see is

1665
01:55:02,500 --> 01:55:03,370
is this term here

1666
01:55:04,180 --> 01:55:06,020
so you get this term here here's the prior

1667
01:55:06,610 --> 01:55:08,610
and here's here's the data likelihood

1668
01:55:12,110 --> 01:55:14,630
if we look at the standard has been lost

1669
01:55:15,330 --> 01:55:16,140
four r

1670
01:55:17,280 --> 01:55:18,330
for linear spam

1671
01:55:18,890 --> 01:55:19,780
right then you have this

1672
01:55:20,230 --> 01:55:20,680
which is the

1673
01:55:21,080 --> 01:55:22,110
regularization term

1674
01:55:22,770 --> 01:55:25,140
and you've got this which is a taylor series so this is

1675
01:55:25,650 --> 01:55:30,080
the soft margin right so you've got this you've got the soft margin parameter see

1676
01:55:31,480 --> 01:55:35,820
now it's often done in a as you can you can you can kernelized this

1677
01:55:36,510 --> 01:55:38,860
so instead of having a linear function you can have

1678
01:55:39,500 --> 01:55:42,350
you can have some basis some kernel basis here

1679
01:55:43,600 --> 01:55:47,440
with at least two and this this this is this is this is standard stuff is

1680
01:55:48,100 --> 01:55:48,610
you get this

1681
01:55:49,490 --> 01:55:52,180
you can write you can write your your has been lost

1682
01:55:53,790 --> 01:55:55,780
that's false assertions should change that's

1683
01:55:57,270 --> 01:56:01,350
what this allows us to do is is rather nice connection when we look at this between

1684
01:56:02,340 --> 01:56:04,680
the minus log joint from a cheap e

1685
01:56:04,750 --> 01:56:05,970
andy has fear and loss

1686
01:56:06,590 --> 01:56:08,050
you see that you've got the same

1687
01:56:09,390 --> 01:56:10,480
data fit term here

1688
01:56:11,020 --> 01:56:12,300
and then you've got a penalty term

1689
01:56:15,320 --> 01:56:17,430
so what this allows to see is that both

1690
01:56:18,900 --> 01:56:19,940
and yes em

1691
01:56:20,570 --> 01:56:24,740
regularize on your prior in this certain way these are not the same function and

1692
01:56:24,740 --> 01:56:27,690
are not doing the same thing but what they both do is they try to

1693
01:56:27,690 --> 01:56:28,250
fit the data

1694
01:56:29,390 --> 01:56:30,380
and the regularized at

1695
01:56:30,800 --> 01:56:31,580
with with the prior

1696
01:56:32,130 --> 01:56:32,890
and so there's there's

1697
01:56:35,250 --> 01:56:35,990
there's there's there's

1698
01:56:37,100 --> 01:56:41,400
quite interesting that a work here one papers like really is is is one and

1699
01:56:41,540 --> 01:56:44,590
is wrote on the relationship between cheap easiness fans

1700
01:56:45,680 --> 01:56:47,470
and he and he talks about supplies as well

1701
01:56:49,880 --> 01:56:50,550
all right so

1702
01:56:52,550 --> 01:56:53,410
where do we stand

1703
01:56:53,790 --> 01:56:54,740
we've talked about

1704
01:56:55,170 --> 01:56:55,640
how to

1705
01:56:56,190 --> 01:56:58,770
change the kernel we talked about how change the likelihood

1706
01:56:59,930 --> 01:57:03,150
those the two objects that i think are are of most interest

1707
01:57:04,250 --> 01:57:05,600
i wanna talk briefly about

1708
01:57:06,160 --> 01:57:08,180
the shortcomings of cheapy

1709
01:57:08,710 --> 01:57:12,940
i think at this point it's it's it's it's very easy thing that cheap is

1710
01:57:12,940 --> 01:57:14,760
this all-purpose regression tool that can do

1711
01:57:15,510 --> 01:57:16,820
effectively anything we want

1712
01:57:17,610 --> 01:57:19,230
in some ways that's true

1713
01:57:20,270 --> 01:57:23,510
and we talked about some of the strengths and this so so well i think

1714
01:57:23,590 --> 01:57:25,820
that one of the weaknesses cheapy

1715
01:57:26,660 --> 01:57:28,090
grow out of the strengths

1716
01:57:30,020 --> 01:57:31,490
so we talked about how

1717
01:57:32,200 --> 01:57:35,860
everything is happily galaxy and how that's a really great thing about two pieces that

1718
01:57:36,100 --> 01:57:39,940
once everybody living inside this nice galcians framework it's all kind of easy

1719
01:57:41,900 --> 01:57:44,200
the weakness of course that

1720
01:57:44,690 --> 01:57:48,920
that's that's exactly true is that when things are guassian then we have this trouble

1721
01:57:50,650 --> 01:57:53,830
we have to bring in if we have a non normal likelihood like we did

1722
01:57:53,860 --> 01:57:57,710
the jeep classification case we have to bring in approximate inference technologies

1723
01:58:00,840 --> 01:58:03,240
when we're dealing with model selection it's

1724
01:58:04,100 --> 01:58:06,730
doing model selection over a number these hyperparameters

1725
01:58:07,470 --> 01:58:10,640
can be can be of the tricky because it's it's nongaussian so you have to

1726
01:58:10,640 --> 01:58:15,640
decide should i optimizes model parameters should i try to integrate over these model parameters

1727
01:58:15,640 --> 01:58:17,670
should properly integrate over the model parameters

1728
01:58:18,890 --> 01:58:19,440
that's it

1729
01:58:19,880 --> 01:58:22,570
that doesn't quite fit into this simple cheap e

1730
01:58:23,010 --> 01:58:24,650
simple gas modeling context

1731
01:58:26,790 --> 01:58:31,520
another great strength of gas processes are the nonparametric flexibility this

1732
01:58:31,960 --> 01:58:35,790
right so we talked about how you don't get some fixed parametric set but rather

1733
01:58:36,790 --> 01:58:39,090
your model grows with the complexity of the data

1734
01:58:41,980 --> 01:58:46,690
the downside of course is that we have to compute on all the data so here's this term

1735
01:58:47,220 --> 01:58:48,940
we've looked at a whole bunch of times

1736
01:58:48,940 --> 01:58:52,130
we look into the set of examples here

1737
01:58:52,130 --> 01:58:53,220
and here

1738
01:58:54,440 --> 01:58:58,770
if all the examples in this subset belong to the same class

1739
01:58:58,790 --> 01:59:00,870
we label this

1740
01:59:00,930 --> 01:59:03,740
with the class name and stop

1741
01:59:03,770 --> 01:59:04,920
decision tree

1742
01:59:06,720 --> 01:59:10,840
in this subset there are still examples of different classes

1743
01:59:10,880 --> 01:59:15,380
like soft hard not we would still continue splitting

1744
01:59:15,430 --> 01:59:19,000
and we which is the most informative that built at this point

1745
01:59:19,060 --> 01:59:21,390
which turns out to be astigmatism

1746
01:59:21,480 --> 01:59:24,240
we again split the set into two subsets

1747
01:59:24,260 --> 01:59:28,890
we can split the set into two subsets and that's how the algorithm basically works

1748
01:59:28,890 --> 01:59:31,210
at each

1749
01:59:32,080 --> 01:59:35,290
of the decision tree it has to decide

1750
01:59:36,330 --> 01:59:39,720
attribute is the most informative

1751
01:59:39,740 --> 01:59:40,860
and put it

1752
01:59:41,000 --> 01:59:42,820
two it names

1753
01:59:42,830 --> 01:59:47,480
here and then it's it's according to the value of that most informative after that

1754
01:59:47,790 --> 01:59:51,890
and then it also at each point it has to decide whether the algorithm should

1755
01:59:51,890 --> 01:59:55,070
continue splitting or it should stop

1756
01:59:55,080 --> 01:59:59,370
building the decision tree by simply labeling the leaf with the class names

1757
01:59:59,760 --> 02:00:03,880
so this will become a bit more clear in a minute

1758
02:00:03,880 --> 02:00:09,590
OK so we have automatically induced the decision tree out of twenty four training examples

1759
02:00:09,900 --> 02:00:13,450
and this decision tree is now a model

1760
02:00:13,470 --> 02:00:17,120
of all the data which we had available

1761
02:00:17,120 --> 02:00:22,740
it was automatically produced one from twenty four training examples and it can be read

1762
02:00:22,740 --> 02:00:26,260
as follows if the production is reduced then

1763
02:00:26,370 --> 02:00:29,880
the patient should be prescribed in contact lenses

1764
02:00:29,880 --> 02:00:33,290
which seems to be a reasonable so

1765
02:00:33,320 --> 02:00:39,020
you should have a normative production in order to get to the lenses subscribed and

1766
02:00:39,020 --> 02:00:40,620
and then the next

1767
02:00:40,810 --> 02:00:46,290
so if the two productions normal and you the patient does have astigmatism then

1768
02:00:46,920 --> 02:00:53,210
the patient may be prescribed soft lenses otherwise if the patient has astigmatism then decision

1769
02:00:53,230 --> 02:00:58,120
still depends on spectacle prescription if the person is myopic

1770
02:00:58,480 --> 02:01:04,970
he would typically be prescribed car glances if he's hypertrophic he wouldn't get any lenses

1771
02:01:05,270 --> 02:01:09,540
at least this was the case in the training examples available you should note is

1772
02:01:09,540 --> 02:01:11,410
that this rule

1773
02:01:11,460 --> 02:01:13,620
represents the

1774
02:01:14,250 --> 02:01:17,750
second is a condensed representation of this

1775
02:01:17,780 --> 02:01:19,560
decision table here

1776
02:01:19,600 --> 02:01:21,530
so let's now look cool weather

1777
02:01:21,560 --> 02:01:26,100
indeed let's say if their production is reduced

1778
02:01:26,110 --> 02:01:30,150
leads us to to a set of examples which are all labeled with the class

1779
02:01:30,150 --> 02:01:31,810
label none

1780
02:01:31,820 --> 02:01:36,130
tear production reduced

1781
02:01:36,180 --> 02:01:42,450
reduced number reduced to reduce the number reduced reduced NONE OK so

1782
02:01:42,500 --> 02:01:45,080
with this

1783
02:01:45,500 --> 02:01:47,060
this value

1784
02:01:47,080 --> 02:01:52,200
after production attribute which turned out to be the most informative directly leads us

1785
02:01:52,280 --> 02:02:01,200
the director describes the group of patients for which no let no lenses were prescribed

1786
02:02:01,220 --> 02:02:02,260
and so on

1787
02:02:02,280 --> 02:02:06,010
so if you would to transform a decision tree into a set of rules you

1788
02:02:06,020 --> 02:02:07,460
could say

1789
02:02:07,470 --> 02:02:13,620
the patient will not be prescribed card lenses will not be prescribed any lenses in

1790
02:02:13,620 --> 02:02:16,620
the two productions reduced

1791
02:02:16,630 --> 02:02:19,670
it will also not be prescribed lenses if

1792
02:02:19,680 --> 02:02:21,790
the TM productions normal

1793
02:02:21,840 --> 02:02:28,460
there is astigmatism and the patient is hypertrophic so we

1794
02:02:28,520 --> 02:02:29,560
we have to

1795
02:02:29,560 --> 02:02:32,130
if then rules for not

1796
02:02:32,140 --> 02:02:34,930
we have one rule for soft lenses

1797
02:02:35,150 --> 02:02:40,390
so will be prescribed if the production is normal and there is no astigmatism and

1798
02:02:40,390 --> 02:02:45,860
the patient will be prescribed hard lenses if the production is normal there is astigmatism

1799
02:02:46,130 --> 02:02:50,870
and the patient is myopic

1800
02:02:50,870 --> 02:02:54,000
another example we have customers

1801
02:02:54,120 --> 02:02:56,760
now the age of the customer

1802
02:02:56,810 --> 02:02:59,500
o is represented by the actual age

1803
02:02:59,530 --> 02:03:05,760
one of the customer will looking at customers who spend a lot in the shop

1804
02:03:05,780 --> 02:03:06,670
we could

1805
02:03:06,680 --> 02:03:11,170
i did look at how much they spend or we could to

1806
02:03:11,210 --> 02:03:16,160
also label them with the by the binary class value whether a person is the

1807
02:03:16,170 --> 02:03:17,990
big spender yes or no

1808
02:03:18,150 --> 02:03:23,020
so now we could have different sorts of tasks if we have a binary classification

1809
02:03:23,020 --> 02:03:29,590
problem describing the patients which are big spenders or not we can induce simple decision

1810
02:03:29,590 --> 02:03:34,280
trees and we would again have the most informative activity in the nodes of the

1811
02:03:34,280 --> 02:03:38,070
decision tree when we have

1812
02:03:38,740 --> 02:03:41,430
values which are numeric

1813
02:03:42,570 --> 02:03:48,320
learning algorithm will split we find will find the best splitting points to to split

1814
02:03:48,330 --> 02:03:50,340
the set of

1815
02:03:50,350 --> 02:03:54,370
customers into those which are big spenders and on expanders

