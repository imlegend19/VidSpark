1
00:00:00,000 --> 00:00:02,940
that four so if we want to learn trees we have hierarchical clustering if want

2
00:00:02,960 --> 00:00:06,400
to learn flat cluster structure we have k means mixture models

3
00:00:06,410 --> 00:00:09,330
if we want to learn some kind of dimensional structure we have both linear and

4
00:00:09,330 --> 00:00:14,280
nonlinear manifold techniques but maybe what we want is to put it somewhat ambitiously is

5
00:00:14,280 --> 00:00:21,550
something like this more universal framework replaces this this grab-bag toolbox of techniques with a

6
00:00:21,550 --> 00:00:25,100
more principle in general purpose approach that's able to not just learn all of these

7
00:00:25,100 --> 00:00:27,640
different forms of structured figure out which is the right one

8
00:00:27,660 --> 00:00:31,860
so charles kemp did some really work when he was in my lab on this

9
00:00:31,860 --> 00:00:35,690
project and this was published a year or two ago in PNAS and the basic

10
00:00:35,690 --> 00:00:39,270
idea a couple of ideas about what is to come up with a universal

11
00:00:39,280 --> 00:00:42,890
a language for representing all these different forms of structure in the same terms and

12
00:00:42,890 --> 00:00:44,850
for that we're using graphs

13
00:00:44,870 --> 00:00:49,580
the each of you know if you want to talk about dimensional structure manifold structure

14
00:00:49,580 --> 00:00:54,130
trees either latent hierarchies are observable trees if you want to talk about flat clusters

15
00:00:54,130 --> 00:00:57,870
those are all different forms of graphs and then we'll will define distributions on these

16
00:00:57,870 --> 00:01:01,820
graphs to fit to the data but another team key idea is that each of

17
00:01:01,820 --> 00:01:05,290
these different forms of structure can be generated by a simple process what you think

18
00:01:05,290 --> 00:01:08,210
of as a graph grammar like a rule for taking a node in the graph

19
00:01:08,210 --> 00:01:11,480
and its in and out links and replacing it with two nodes so growing a

20
00:01:11,480 --> 00:01:14,040
little bit of structure and the appropriate in and out

21
00:01:14,890 --> 00:01:19,280
a portion to those two new nodes and that's that that is a simple generative

22
00:01:19,280 --> 00:01:24,780
model for these forms structures different graph grammars different rules are we to parameterize these

23
00:01:24,780 --> 00:01:26,570
different structural forms

24
00:01:26,580 --> 00:01:30,320
so now we can cast this learning problem in a hierarchical framework like this with

25
00:01:30,320 --> 00:01:34,080
the lowest level is the observable data the typical matrix and then we have these

26
00:01:34,080 --> 00:01:37,090
two latent levels that were interested in there is the level of what we call

27
00:01:37,090 --> 00:01:41,270
the structure which is some graphs and it's a graph of particular form that generated

28
00:01:41,270 --> 00:01:45,310
by some simple graph grammar at the higher level and by taking single productions or

29
00:01:45,320 --> 00:01:48,470
cross products productions like say a cross product of two

30
00:01:48,480 --> 00:01:52,550
two chain rules gives you a two dimensional grid i skipped over performance like for

31
00:01:52,550 --> 00:01:57,590
example across down in the lower right across the chain ring gives us cylinder structure

32
00:01:57,600 --> 00:02:00,410
and so what we want to do is basically do inference at both levels of

33
00:02:00,490 --> 00:02:01,620
hierarchical model

34
00:02:01,640 --> 00:02:06,040
as a way of learning the right structure and or the right form of structure

35
00:02:06,040 --> 00:02:08,580
and the best structure that four or you could think of it as a kind

36
00:02:08,580 --> 00:02:12,740
of fun structural grammar induction like by analogy to language or you want to learn

37
00:02:12,740 --> 00:02:15,680
the grammar at the same time as you have to use the grammar to parse

38
00:02:15,730 --> 00:02:19,270
sentences in order to even know how well it fits so i don't have time

39
00:02:19,270 --> 00:02:22,080
to tell you about the details of our probabilistic model i just free to the

40
00:02:22,080 --> 00:02:24,300
paper but the the basic idea

41
00:02:24,310 --> 00:02:28,830
you should be again pretty much familiar to bayesian machine learning crowd there's

42
00:02:28,890 --> 00:02:30,160
sort of the the

43
00:02:31,150 --> 00:02:34,770
this kind of bayesian occam's razor in the probability of the of the graph structure

44
00:02:34,770 --> 00:02:38,830
given form which tends to prefer simpler forms like for example the chain structure well

45
00:02:39,360 --> 00:02:42,690
a flat cluster structure is the simplest change structure is is a little bit more

46
00:02:42,690 --> 00:02:44,630
complex but some for the tree and so on

47
00:02:44,760 --> 00:02:48,310
and then the the probability of the data given the structure for that we're using

48
00:02:48,310 --> 00:02:51,730
some really neat ideas that together now to sort of classics in the NIPS community

49
00:02:52,080 --> 00:02:56,530
on the idea that you can use a graph to parameterize the gaussianprocess just to

50
00:02:56,530 --> 00:03:00,290
capture the notion of a smooth distribution of data by something like that you know

51
00:03:00,290 --> 00:03:04,840
the covariance being be some kind of normalized inverse graph class in

52
00:03:04,880 --> 00:03:09,690
and in the particular close to this is the work injuries you and lafferty and

53
00:03:10,000 --> 00:03:14,930
ghahramani did but there's many probably equivalent approaches and what we're doing here is is

54
00:03:14,930 --> 00:03:19,420
basically searching over this this these two levels to find the best grammar the best

55
00:03:19,420 --> 00:03:21,160
graph that for

56
00:03:21,170 --> 00:03:24,740
i can tell you details about later but just to show you a few results

57
00:03:24,740 --> 00:03:28,580
of using this approach given a dataset of animals and their features and these are

58
00:03:28,690 --> 00:03:31,020
you know genetic features these are the kind of things the child would see if

59
00:03:31,020 --> 00:03:36,080
they walk around the zoo are real picture book like a walrus has flippers or

60
00:03:36,250 --> 00:03:42,050
dolphin has a tail or dog barks that sort of thing the model is able

61
00:03:42,050 --> 00:03:44,470
to is able to identify

62
00:03:44,520 --> 00:03:45,800
plausible tree

63
00:03:45,810 --> 00:03:50,430
the captured something of the hierarchical structure biological categories as the normal human child my

64
00:03:50,480 --> 00:03:54,010
understand them but also to identify that should be a tree as opposed to some

65
00:03:54,010 --> 00:03:57,850
other kind of structure so given say for example the data set of more political

66
00:03:57,850 --> 00:03:59,720
domain supreme court

67
00:03:59,730 --> 00:04:03,230
opinions these are all justices who served on the rehnquist court

68
00:04:03,240 --> 00:04:08,520
then it identifies not auditory but more chain structure capturing the left right or liberal-conservative

69
00:04:08,870 --> 00:04:13,400
spectrum if we if we give the model say for example on data that comes

70
00:04:13,400 --> 00:04:19,510
from space of faces these are realistically rendered synthetic faces where we tweak interface synthesizer

71
00:04:19,520 --> 00:04:25,830
two knobs corresponding to dimensions of basically masculinity and racial dimension so you can see

72
00:04:25,830 --> 00:04:28,650
if you go sort of from top to bottom here what looks like a white

73
00:04:28,650 --> 00:04:31,530
to black dimension and if you go from from left to right you can see

74
00:04:31,530 --> 00:04:34,930
a sort of subtle mass masculinity variation the ones on the left is supposed to

75
00:04:34,930 --> 00:04:38,410
be the more heavily masked faces it might help if you think that the ones

76
00:04:38,410 --> 00:04:40,820
on the left are like the football team and the ones on the right are

77
00:04:40,820 --> 00:04:42,820
more like the tennis team

78
00:04:42,880 --> 00:04:48,010
and our models able to identify those two underlying dimensions but there's dimensional structure and

79
00:04:48,010 --> 00:04:52,910
so obviously this is what we call fallacies of relevance or the paradoxes strict and

80
00:04:52,960 --> 00:04:58,240
material implication

81
00:04:58,290 --> 00:05:03,940
implication for us for relevant logicians should be a closer bind between the antecedent and

82
00:05:03,940 --> 00:05:10,050
consequent then you get just with material implication material occasionally look set is this it

83
00:05:10,060 --> 00:05:13,780
the truth values doesn't look at the content

84
00:05:15,040 --> 00:05:18,670
now because of that a lot of people have said

85
00:05:18,710 --> 00:05:20,050
what you want

86
00:05:20,090 --> 00:05:26,710
is a logic where we have what's called variable sharing

87
00:05:26,750 --> 00:05:29,710
i think this is all should be on you know some of the retina

88
00:05:29,760 --> 00:05:37,280
variable sharing says that the antecedent and consequent of any theorem should share least one

89
00:05:37,280 --> 00:05:39,870
non logical bit of information

90
00:05:39,900 --> 00:05:41,260
what's that some

91
00:05:41,270 --> 00:05:43,680
proposition letter columns in common

92
00:05:43,730 --> 00:05:46,610
so you don't ever get off topic completely

93
00:05:51,800 --> 00:05:56,720
that turns out to be OK variable sharing

94
00:05:58,150 --> 00:06:01,510
not that because we don't have very much sharing so that you do

95
00:06:01,560 --> 00:06:03,950
that you don't

96
00:06:04,900 --> 00:06:11,030
that doesn't eliminate this one is this problem things summer school positive paradox well

97
00:06:12,360 --> 00:06:14,370
what we really want

98
00:06:14,440 --> 00:06:18,650
isn't just variable share a lot of logicians have said well what you just say

99
00:06:19,400 --> 00:06:23,770
will take all classical logic but only those their rooms where we have variable sharing

100
00:06:23,780 --> 00:06:26,270
that's called something called

101
00:06:26,360 --> 00:06:31,270
relevance filter is one that was called turns out the logic is horrible

102
00:06:31,310 --> 00:06:37,050
OK that you can actually ties many normal way because

103
00:06:39,530 --> 00:06:43,930
because obviously there are versions of this and then this

104
00:06:43,930 --> 00:06:46,800
that will have variable sharing but

105
00:06:46,820 --> 00:06:50,290
it's still so how do you formulate how do how do you formalise it so

106
00:06:50,290 --> 00:06:54,860
you get that and so on i mean those cases are too hard but the

107
00:06:56,500 --> 00:06:57,950
OK so now

108
00:06:59,270 --> 00:07:02,630
why is this the problem is what we really want isn't just this variable sharing

109
00:07:02,910 --> 00:07:05,670
but vertebrate notion of implication

110
00:07:05,680 --> 00:07:10,050
what does it say something and find something else this tells us that p is

111
00:07:10,050 --> 00:07:11,590
true then

112
00:07:11,610 --> 00:07:15,720
anything implies that's really the sets

113
00:07:15,730 --> 00:07:17,210
and that really is

114
00:07:17,220 --> 00:07:19,370
what's behind what's wrong with this

115
00:07:19,400 --> 00:07:22,170
just because something is true even always true

116
00:07:22,180 --> 00:07:24,610
doesn't mean everything entails that everything

117
00:07:24,930 --> 00:07:27,350
anything fall it follows from anything

118
00:07:28,550 --> 00:07:30,910
just because something is true doesn't mean something

119
00:07:30,910 --> 00:07:33,340
implies that you've got to have

120
00:07:33,990 --> 00:07:37,840
in saying

121
00:07:37,900 --> 00:07:41,070
the moon is made of green is the moon is made of green cheese implies

122
00:07:42,000 --> 00:07:44,540
i'm standing here lecturing right now

123
00:07:44,570 --> 00:07:47,520
we know understanding and lecturing right now

124
00:07:48,600 --> 00:07:52,490
that's not a real national implication that's the notion that if someone says

125
00:07:52,670 --> 00:07:59,540
well there is that really implying this tuna in the outside a classroom and logic

126
00:07:59,600 --> 00:08:01,220
you would say no

127
00:08:01,270 --> 00:08:05,050
and that what you would you would say no because under any normal notion of

128
00:08:07,240 --> 00:08:09,820
any pre theoretic notion implication at

129
00:08:09,850 --> 00:08:14,310
anything you would think before you learn classical logic you would say no acumen class

130
00:08:14,310 --> 00:08:17,260
quality so you have the second just to the truth table

131
00:08:17,630 --> 00:08:20,440
but that's theory

132
00:08:20,440 --> 00:08:23,560
replacing our normal concepts

133
00:08:23,590 --> 00:08:27,520
but we want to represent and on the concept of implication we need something that

134
00:08:27,520 --> 00:08:29,590
so much closer bind them

135
00:08:29,610 --> 00:08:33,420
the finds much closer than classical

136
00:08:42,210 --> 00:08:43,200
now the key

137
00:08:43,210 --> 00:08:45,960
in my opinion to relevant logic

138
00:08:46,060 --> 00:08:50,510
comes from this notion of the notion of a real use of premise

139
00:08:51,130 --> 00:08:53,600
a derivation now

140
00:08:53,610 --> 00:08:57,590
let's think about the derivation of this

141
00:08:58,190 --> 00:09:00,800
what you'll do

142
00:09:00,820 --> 00:09:03,230
if you are asked to derive that

143
00:09:03,240 --> 00:09:04,700
in classical logic

144
00:09:04,720 --> 00:09:09,910
it is in in the natural deduction system is you probably do something like that

145
00:09:09,930 --> 00:09:14,740
in the same way

146
00:09:14,760 --> 00:09:18,550
and give

147
00:09:18,610 --> 00:09:28,310
some proof

148
00:09:31,830 --> 00:09:34,630
OK so she gives some provably you're not be

149
00:09:34,650 --> 00:09:36,720
he one use a

150
00:09:39,280 --> 00:09:41,850
you would probably this

151
00:09:41,940 --> 00:09:42,850
but still

152
00:09:50,430 --> 00:10:00,760
in the proof again before from and b i wanna

153
00:10:00,770 --> 00:10:03,030
i never mentioned in the whole thing

154
00:10:03,050 --> 00:10:06,900
you can bring an end if you wanted but it wouldn't be relevant right could

155
00:10:06,900 --> 00:10:08,970
bring it in for no good reason

156
00:10:09,010 --> 00:10:13,030
in the brain and then throw it away again we know what to do that

157
00:10:13,050 --> 00:10:15,320
OK so you do that that's your crew

158
00:10:15,340 --> 00:10:18,400
you have an undischarged hypotheses which is the premise

159
00:10:18,400 --> 00:10:21,130
and the conclusion

160
00:10:21,160 --> 00:10:23,820
a is really irrelevant here

161
00:10:23,840 --> 00:10:27,490
so let's do something to make sure you really use the premises

162
00:10:27,520 --> 00:10:31,060
now back in the late fifties early sixties

163
00:10:31,080 --> 00:10:34,730
alan anderson nobel

164
00:10:35,770 --> 00:10:37,900
an idea about how to do this

165
00:10:37,940 --> 00:10:41,330
people have formulated relevant on before but this is the first time it was done

166
00:10:41,330 --> 00:10:43,980
in in know in the natural deduction systems

167
00:10:44,030 --> 00:10:48,480
my them

168
00:10:48,660 --> 00:10:53,910
when you introduce the hypothesis

169
00:10:53,910 --> 00:11:02,500
you don't just introduce a hypothesis

170
00:11:02,500 --> 00:11:07,420
well thank you very kind the goal

171
00:11:07,470 --> 00:11:08,400
if you

172
00:11:08,410 --> 00:11:11,460
active brain cells are very heavy we

173
00:11:11,520 --> 00:11:17,330
i feel in many skulking in the corner where you can get but quickly with

174
00:11:17,400 --> 00:11:21,490
very long stick quite fast

175
00:11:23,060 --> 00:11:24,470
this is

176
00:11:24,600 --> 00:11:30,030
and then the whole which gave the summer school two billion

177
00:11:34,930 --> 00:11:38,900
if you happen all the unexpurgated they

178
00:11:39,800 --> 00:11:41,360
my website

179
00:11:41,400 --> 00:11:45,330
that is enough for you there's even the recall election

180
00:11:45,340 --> 00:11:48,330
four four

181
00:11:48,340 --> 00:11:53,190
we're we're going to do that if you want

182
00:11:53,250 --> 00:11:57,830
in and around that time down to us

183
00:11:57,840 --> 00:12:00,820
now we're going to get one

184
00:12:01,290 --> 00:12:04,640
this process continues

185
00:12:04,640 --> 00:12:07,860
the elevator pitch a lot

186
00:12:10,170 --> 00:12:11,890
this is all about

187
00:12:13,900 --> 00:12:16,390
second moment in which i think

188
00:12:16,400 --> 00:12:19,310
has been particularly exciting last

189
00:12:19,340 --> 00:12:21,750
seven or eight years really transformed

190
00:12:21,750 --> 00:12:23,590
the way

191
00:12:23,590 --> 00:12:26,280
all of that

192
00:12:27,170 --> 00:12:28,390
i guess

193
00:12:28,420 --> 00:12:33,290
he died because of the programme chunk of stuff on graphical models

194
00:12:33,340 --> 00:12:36,060
and i don't know so you will have had

195
00:12:36,090 --> 00:12:39,030
a whole stuff belief propagation

196
00:12:41,010 --> 00:12:42,230
for nineteen

197
00:12:42,230 --> 00:12:43,720
and what about

198
00:12:43,890 --> 00:12:47,060
the belief propagation that come up

199
00:12:47,060 --> 00:12:50,780
so you know belief propagation works that doesn't in

200
00:12:50,790 --> 00:12:51,890
in tree

201
00:12:52,560 --> 00:12:57,860
the graph has no cycles and once you start adding cycles in the graph

202
00:12:58,700 --> 00:13:01,890
it's been doesn't work anymore but and so

203
00:13:01,930 --> 00:13:03,370
and that was here

204
00:13:03,950 --> 00:13:08,290
so the problem but they are more likely to say well i'm sorry i you

205
00:13:08,290 --> 00:13:11,770
know i got involved in the graph have cycles in the nineteen some kind of

206
00:13:11,820 --> 00:13:16,330
a solution to problems i get about and it's not just that they should be

207
00:13:17,130 --> 00:13:19,080
and see what happens

208
00:13:19,130 --> 00:13:20,100
the story

209
00:13:22,830 --> 00:13:24,920
as the problem to talk about

210
00:13:24,920 --> 00:13:29,390
problems of inferences of inference on ground

211
00:13:29,410 --> 00:13:31,890
which are very much the kind of problem

212
00:13:32,480 --> 00:13:33,450
you may that

213
00:13:33,450 --> 00:13:37,260
when you have an obligation

214
00:13:37,270 --> 00:13:39,990
here in the most of the time

215
00:13:40,010 --> 00:13:43,850
the graph cycles in the back of the have very tight

216
00:13:43,860 --> 00:13:46,360
five very tightly so

217
00:13:46,510 --> 00:13:49,580
census rather bad case four

218
00:13:49,610 --> 00:13:50,930
belief propagation

219
00:13:50,950 --> 00:13:52,550
you know there is one school

220
00:13:52,570 --> 00:13:56,240
but anyway on the graph and the consequence

221
00:13:56,290 --> 00:14:00,380
what i'm going to talk about is is another way of doing it and graph

222
00:14:01,360 --> 00:14:04,170
graph the kind of my thing

223
00:14:05,830 --> 00:14:07,580
i think

224
00:14:08,790 --> 00:14:11,260
given that in the past four

225
00:14:11,290 --> 00:14:13,800
i'm not going to spend very much time

226
00:14:16,420 --> 00:14:19,170
inference problems on graphs very well that

227
00:14:19,390 --> 00:14:22,830
nobody present operating problem

228
00:14:22,860 --> 00:14:28,640
but here the when i give belong to all these problems but i like go

229
00:14:28,820 --> 00:14:31,670
through just express you know what the

230
00:14:32,600 --> 00:14:37,110
for each of these problems and try to convince you that it does indeed boil

231
00:14:37,170 --> 00:14:41,140
down doing inference on the ground

232
00:14:41,160 --> 00:14:44,950
and once again one hundred and one of the three

233
00:14:44,960 --> 00:14:46,070
which is

234
00:14:46,080 --> 00:14:47,270
the problem in the

235
00:14:47,300 --> 00:14:49,490
segmentation problem

236
00:14:49,670 --> 00:14:54,860
all these kinds of problems have cropped i

237
00:14:54,990 --> 00:14:59,170
the restoration for example one very wrong

238
00:14:59,200 --> 00:15:02,930
put it back

239
00:15:02,950 --> 00:15:04,230
that's we jump

240
00:15:04,640 --> 00:15:05,730
in the audience

241
00:15:05,850 --> 00:15:06,990
we start again

242
00:15:07,010 --> 00:15:17,420
so image restoration is a problem is easy to understand you you have an image

243
00:15:17,460 --> 00:15:21,220
which is an got messed up somehow maybe you could think of

244
00:15:21,690 --> 00:15:24,960
this and let's say in the medical x-ray domain that there's a known x-ray your

245
00:15:24,960 --> 00:15:26,420
chest and that somewhere

246
00:15:26,880 --> 00:15:29,680
you know in the first is an ideal

247
00:15:29,690 --> 00:15:34,020
x-ray your chest but you don't get the ideal x-ray you get a rather marked-up

248
00:15:34,020 --> 00:15:38,620
version with with blur and noise and so on you could think of that might

249
00:15:38,800 --> 00:15:44,970
version the real x-ray as being an ideal x-rayed has been corrupted and when you

250
00:15:44,970 --> 00:15:48,170
want to recover something closer to the ideal x-ray

251
00:15:49,900 --> 00:15:53,940
you need to think about two things first of all what is the corruption that

252
00:15:53,940 --> 00:15:58,480
happened in transforming the ideal x-ray into this marked one

253
00:15:58,490 --> 00:16:00,110
the second thing is

254
00:16:00,170 --> 00:16:05,020
generally tell me about the prior properties of x-rays and how can you express this

255
00:16:05,020 --> 00:16:09,320
prior properties and is expressing the prior properties of images where

256
00:16:09,350 --> 00:16:12,960
graphs turn out to be so

257
00:16:13,000 --> 00:16:16,380
effective and appropriate

258
00:16:18,160 --> 00:16:21,530
now as i say i'm going to spend most of the time talking about problems

259
00:16:21,850 --> 00:16:24,300
related to segmentation

260
00:16:24,350 --> 00:16:27,290
so here we have a

261
00:16:27,290 --> 00:16:29,550
common-or-garden and an enemy

262
00:16:29,550 --> 00:16:31,290
but i think that's an enemy

263
00:16:31,370 --> 00:16:32,480
and it is

264
00:16:32,550 --> 00:16:34,970
because the starfish and they came so

265
00:16:34,980 --> 00:16:36,860
sort of brightly coloured

266
00:16:36,870 --> 00:16:42,610
it does say starfish dot j tag on the on the so you are probably

267
00:16:45,610 --> 00:16:48,570
never mind about the biology the point is

268
00:16:48,620 --> 00:16:53,480
can we get the outline of this object to anything like automatic i mean you

269
00:16:53,480 --> 00:16:56,230
know what you really love to be able to do is just you know take

270
00:16:56,230 --> 00:17:01,690
your magic wand and way the starfish and no not turn it into an enemy

271
00:17:02,060 --> 00:17:05,720
just get out the outline without really saying anything more

272
00:17:05,820 --> 00:17:07,820
i mean do you think it will be feasible

273
00:17:07,840 --> 00:17:09,180
just to sort of

274
00:17:09,230 --> 00:17:13,610
tell the inference system no more than that there is one object in this

275
00:17:13,620 --> 00:17:15,550
in this scene and can you get

276
00:17:15,590 --> 00:17:19,070
the object well you know for me that's still the holy grail i haven't given

277
00:17:19,070 --> 00:17:23,690
up trying to do it but we haven't quite yet so something a little bit

278
00:17:23,690 --> 00:17:27,670
more reasonable at the moment in the way problem setting would be

279
00:17:27,680 --> 00:17:33,040
to get a an interactive graphics user to roll this try map

280
00:17:33,050 --> 00:17:34,920
you know with a thick piece of lipstick

281
00:17:34,920 --> 00:17:38,460
and the rules of the game are the lipstick is assumed to straddle the edge

282
00:17:38,460 --> 00:17:40,160
of the object

283
00:17:41,880 --> 00:17:45,050
so that's perfectly reasonable to give this is input to say the

284
00:17:45,090 --> 00:17:48,430
the boundary of because somewhere in then now tell exactly where it is and the

285
00:17:48,430 --> 00:17:52,020
inference system will tell you OK that's the object and in fact it can do

286
00:17:52,020 --> 00:17:55,860
even a little bit more than that sometimes which is also to tell you a

287
00:17:55,860 --> 00:17:59,820
little bit about transparency because the edges of objects when you look at them close

288
00:17:59,840 --> 00:18:03,790
in images are not clear-cut and they can't be because they would have to follow

289
00:18:04,050 --> 00:18:08,210
the pixel boundaries in pixel boundaries are weird staircase shape things of course don't occur

290
00:18:08,210 --> 00:18:11,850
in nature so at least at that level there must be some sort of mixing

291
00:18:11,850 --> 00:18:15,210
of colour and in fact there are other factors as well like the optics of

292
00:18:15,210 --> 00:18:20,540
the camera which tend to it tends to blur the outline and so actually the

293
00:18:20,540 --> 00:18:25,360
pixels near the outline are kind of mixture of foreground color background colour and believe

294
00:18:25,360 --> 00:18:30,790
it or not you can actually recover under favourable circumstances that mixture but i'm not

295
00:18:30,790 --> 00:18:33,860
going to think too much about that problem i'm going to think about

296
00:18:33,860 --> 00:18:37,020
and if you do that basically services

297
00:18:37,450 --> 00:18:40,740
standard statistical techniques then

298
00:18:40,750 --> 00:18:46,100
you you you then have two steps the e step

299
00:18:46,110 --> 00:18:50,230
what you do is you compute the posterior probability of causing the latent variable and

300
00:18:50,230 --> 00:18:53,380
what that means in that particular case here is that

301
00:18:53,390 --> 00:18:57,460
if you look at every word occurrence in particular document every

302
00:18:58,280 --> 00:19:00,510
dw here for those

303
00:19:00,520 --> 00:19:02,710
pairs that actually occurred for the other

304
00:19:04,030 --> 00:19:07,900
so you have this big matrix you only have to do this for

305
00:19:07,900 --> 00:19:08,860
you know those

306
00:19:08,870 --> 00:19:11,240
entries that are nonzero

307
00:19:11,260 --> 00:19:14,950
and you do that and that's basically the probability that the occurrence of a particular

308
00:19:14,950 --> 00:19:16,360
term in document

309
00:19:16,380 --> 00:19:18,710
can be explained if you like by

310
00:19:18,750 --> 00:19:21,030
this concept and can see

311
00:19:21,130 --> 00:19:22,740
very simple

312
00:19:22,790 --> 00:19:26,240
of creation here and then in the end step

313
00:19:26,290 --> 00:19:28,160
you just use these

314
00:19:29,150 --> 00:19:32,890
completed statistics if you like you know the

315
00:19:32,900 --> 00:19:38,380
the expectation of the sufficient statistics and use that as these

316
00:19:41,020 --> 00:19:43,760
so what and what you get then this basically

317
00:19:43,840 --> 00:19:45,060
over here

318
00:19:45,070 --> 00:19:49,770
you know how often a particular term associated with a particular concept you just can't

319
00:19:49,770 --> 00:19:52,300
that you add up the fractions

320
00:19:52,380 --> 00:19:56,370
and over here how often a particular document associated with a a particular concept and

321
00:19:56,370 --> 00:19:57,920
then you just normalize

322
00:19:57,970 --> 00:20:02,440
so that these things really become

323
00:20:03,500 --> 00:20:09,390
OK then of course what you do is you you reiterate and you do that

324
00:20:09,440 --> 00:20:10,960
for a while until

325
00:20:10,980 --> 00:20:12,230
you know you reach

326
00:20:12,240 --> 00:20:14,930
you get close to convergence

327
00:20:14,930 --> 00:20:20,050
or you have some stopping criterion maybe we stop the

328
00:20:20,070 --> 00:20:21,500
OK i don't want to

329
00:20:21,550 --> 00:20:24,920
talk a lot about

330
00:20:25,020 --> 00:20:28,950
derivation you really one way to do this is to

331
00:20:28,950 --> 00:20:33,200
we've seen is to introduce this variational distribution

332
00:20:33,220 --> 00:20:37,650
apply jensen's inequality the only thing i want to say here is

333
00:20:37,660 --> 00:20:40,070
that's what we've been doing actually

334
00:20:40,210 --> 00:20:47,410
to be able to improve the generalisation performance of to avoid overfitting

335
00:20:47,420 --> 00:20:48,720
is used this

336
00:20:48,970 --> 00:20:52,570
of what we call tempered version of

337
00:20:52,590 --> 00:20:57,280
where if if you if you look at this derivation with this variation q distribution

338
00:20:57,280 --> 00:21:00,120
you artificially so you have

339
00:21:00,240 --> 00:21:02,860
this term here which is the expected

340
00:21:02,880 --> 00:21:06,780
complete data log likelihood

341
00:21:06,790 --> 00:21:09,920
and you have this the entropy term here

342
00:21:09,970 --> 00:21:12,670
is the artificially increases and term

343
00:21:12,710 --> 00:21:14,690
so kind of year

344
00:21:14,700 --> 00:21:17,890
you know you try to make

345
00:21:17,940 --> 00:21:21,960
these distributions since this is just

346
00:21:21,980 --> 00:21:26,790
the distribution over finite state space you just try to make it more uniform

347
00:21:26,870 --> 00:21:31,020
i'm making the small

348
00:21:31,030 --> 00:21:35,930
so this is there are better ways of doing that again probably brave and then

349
00:21:35,930 --> 00:21:39,370
we'll talk more bayesian approaches

350
00:21:39,410 --> 00:21:43,200
the inference for these types of models but this is something that in practice

351
00:21:43,210 --> 00:21:45,690
it gives you a good compromise in terms of

352
00:21:45,740 --> 00:21:49,780
the efficiency of the whole procedure stays the same

353
00:21:49,790 --> 00:21:54,540
but you some examples here we see for instance you do and you do whatever

354
00:21:54,540 --> 00:21:57,610
twentysomething iterations and if you don't stop there

355
00:21:57,650 --> 00:22:00,090
by using some many the data set to do

356
00:22:01,350 --> 00:22:05,200
the generalisation performance goes up again right and if you introduce this

357
00:22:05,210 --> 00:22:09,860
the temperature parameter for different settings you can see that this is just

358
00:22:09,870 --> 00:22:11,810
perplexity number here

359
00:22:12,600 --> 00:22:14,810
log likelihood on test set

360
00:22:15,240 --> 00:22:17,670
you know you can get much

361
00:22:17,770 --> 00:22:24,410
one so

362
00:22:24,420 --> 00:22:31,660
so what you see is that the objective function as the basis indians to just

363
00:22:31,660 --> 00:22:35,240
optimizing basically the same function with

364
00:22:35,300 --> 00:22:38,190
you know with these variation q functions but if you when you compute and is

365
00:22:38,190 --> 00:22:40,710
that when you compute these these these these q

366
00:22:40,750 --> 00:22:43,920
distributions they are no longer the posteriors

367
00:22:43,980 --> 00:22:45,120
they become

368
00:22:45,130 --> 00:22:48,600
bias towards being more uniform

369
00:22:48,610 --> 00:22:51,910
this is also people do this influences in speech

370
00:22:51,920 --> 00:22:55,180
recognition it's very often

371
00:22:55,200 --> 00:22:58,970
you make certain independence assumptions right that are not true

372
00:22:58,990 --> 00:22:59,880
and then

373
00:22:59,880 --> 00:23:03,770
you're adding up evidence which means you know you're multiplying these probabilities that are not

374
00:23:03,770 --> 00:23:08,260
really independent and then you get to be overconfident in your estimates for the posterior

375
00:23:08,260 --> 00:23:10,860
probability and you know it's just very

376
00:23:10,920 --> 00:23:12,900
i'm not saying that this is you know

377
00:23:12,940 --> 00:23:16,570
something you should do but actually in practice often useful one way is just to

378
00:23:16,590 --> 00:23:20,920
discount for that double counting of evidence is just to rescale

379
00:23:20,940 --> 00:23:22,550
right and say

380
00:23:22,610 --> 00:23:25,720
you know i don't have i don't know if you think about things like morning

381
00:23:25,720 --> 00:23:29,550
naive bayes models or something like that right you know you have fifty observations and

382
00:23:29,590 --> 00:23:33,260
you know your hundred percent sure it belongs to this class right

383
00:23:33,280 --> 00:23:37,610
but you made independence assumption is wrong maybe aviation talk about you know fifty independent

384
00:23:37,610 --> 00:23:41,760
observations may be the only effectively ten it's a right so you know if you

385
00:23:41,760 --> 00:23:43,510
know that you could just

386
00:23:43,530 --> 00:23:46,050
basically rescale

387
00:23:46,170 --> 00:23:50,110
but then of course the writing to really do this is more bayesian approach where

388
00:23:50,110 --> 00:23:52,710
you you introduce priors on europe

389
00:23:52,710 --> 00:23:57,550
parameters say and then use that to avoid overfitting when i come back to

390
00:23:57,610 --> 00:23:58,940
just briefly

391
00:24:00,130 --> 00:24:01,630
so anyway so

392
00:24:01,650 --> 00:24:05,320
you know looking at the BMI algorithm is really

393
00:24:05,690 --> 00:24:09,170
really simple algorithm in the model is simply algorithms really simple

394
00:24:09,220 --> 00:24:13,490
and he is known example of the types of things that you find in this

395
00:24:13,490 --> 00:24:15,220
picture the draw

396
00:24:15,240 --> 00:24:19,360
well i'm showing you basically these vectors

397
00:24:19,400 --> 00:24:21,220
and what is that well

398
00:24:21,240 --> 00:24:26,130
we have many of these vectors as we have concepts social three comes here

399
00:24:26,130 --> 00:24:27,490
and then

400
00:24:28,110 --> 00:24:30,840
it these vectors will be will contain

401
00:24:30,840 --> 00:24:34,010
for each term probability which we

402
00:24:34,090 --> 00:24:35,800
rescaled here

403
00:24:35,840 --> 00:24:42,030
to make them look nicer and and then ordered the terms according to their probability

404
00:24:42,050 --> 00:24:43,530
OK so

405
00:24:43,530 --> 00:24:48,490
each concept with with each latent concept with associated these

406
00:24:48,510 --> 00:24:52,800
the vector of what i call expression probability right and this is what you see

407
00:24:52,820 --> 00:24:57,760
so for instance you if you look at this particular fuzzy concept number two here

408
00:24:58,170 --> 00:25:04,630
we would see that it corresponds to something to certain context where words like ship

409
00:25:04,630 --> 00:25:06,010
load tanker

410
00:25:06,010 --> 00:25:11,100
set up the structure of the graph in which you're obligated to pass a message

411
00:25:11,100 --> 00:25:15,940
to parents and receive a message from your parents we pass messages and then we're

412
00:25:15,940 --> 00:25:19,830
ready to answer any arbitrary query one on the graph

413
00:25:21,050 --> 00:25:24,090
when we take a break for about five minutes and then i will come back

414
00:25:24,090 --> 00:25:27,680
and try and do a specific case of estrangement

415
00:25:27,720 --> 00:25:29,880
moving forward and here

416
00:25:29,880 --> 00:25:34,560
so i've of try to convince you that there are only

417
00:25:34,630 --> 00:25:43,140
it is work is it is managed to compute these messages properly

418
00:25:46,670 --> 00:25:50,020
because it's not entirely clear

419
00:25:50,030 --> 00:25:54,500
it is clear that there are very many messages but it's not entirely clear what

420
00:25:54,890 --> 00:26:01,110
should compute these messages to get the job done because in a single elimination rounds

421
00:26:01,120 --> 00:26:05,490
it's very obvious how you compute the message you can only always sort of approach

422
00:26:05,490 --> 00:26:09,200
the tail the message here when one way from the leaves up to it and

423
00:26:09,200 --> 00:26:12,310
when you hit the tail you compute the message and pass it your parent that

424
00:26:12,760 --> 00:26:17,250
but it's not clear how we do that so we're going to just go through

425
00:26:17,940 --> 00:26:21,830
and then and then you have the full story

426
00:26:24,320 --> 00:26:29,500
how we can compute all the messages so the idea here is that you ask

427
00:26:29,500 --> 00:26:35,790
what conditions needed to be satisfied in a single round of eliminations before i could

428
00:26:35,790 --> 00:26:38,140
send a message to my parents

429
00:26:39,200 --> 00:26:40,880
and the answer was

430
00:26:40,900 --> 00:26:45,700
i must have received a message from my children

431
00:26:46,280 --> 00:26:48,750
so in a single round of eliminations university

432
00:26:48,760 --> 00:26:51,010
the message from the children

433
00:26:51,030 --> 00:26:52,100
you're ready to go

434
00:26:52,140 --> 00:26:55,290
you have all the information that's ever going to be relevant for you to send

435
00:26:55,330 --> 00:27:00,000
message to parents any time after that you could send the message to parent

436
00:27:01,020 --> 00:27:07,160
and so i'm going to try to do is to design a club for computing

437
00:27:07,160 --> 00:27:13,400
these messages so every message when we compute it it's true that no one has

438
00:27:13,400 --> 00:27:17,820
received messages from all of its children and still you don't have to do in

439
00:27:17,830 --> 00:27:18,930
this would work

440
00:27:18,950 --> 00:27:22,850
and it turns out that there is a clever order that achieve that and there

441
00:27:22,850 --> 00:27:28,680
are many orders and they have to just respect this message passing protocol so the

442
00:27:28,680 --> 00:27:35,230
idea is that you remember to compute the messages so forget about the directed graph

443
00:27:35,230 --> 00:27:38,240
just the undirected version of the graph

444
00:27:38,250 --> 00:27:40,980
let's go back to the start symbol

445
00:27:44,840 --> 00:27:47,860
here's the star example

446
00:27:47,880 --> 00:27:52,670
and i've drawn undirected and the idea is

447
00:27:52,720 --> 00:27:56,420
each edge is there's two messages we want to compute p

448
00:27:57,530 --> 00:28:01,260
we need to be able to ensure when we compute that we have all the

449
00:28:01,260 --> 00:28:05,880
information available so if we expect the following protocol

450
00:28:05,930 --> 00:28:09,930
i know can only send a message to its neighbour

451
00:28:09,940 --> 00:28:14,000
well received messages from all its other neighbours so

452
00:28:14,020 --> 00:28:17,540
this no can only send a message to this guy

453
00:28:17,550 --> 00:28:20,560
when received messages from the sky and the sky

454
00:28:20,570 --> 00:28:24,920
and so on that's true for all of the if we expect that

455
00:28:27,530 --> 00:28:30,040
every message we send will be correct

456
00:28:30,060 --> 00:28:33,650
it will be sent under the same circumstances it would have been sent if it

457
00:28:33,650 --> 00:28:36,060
were in the middle of a single round of elimination

458
00:28:36,070 --> 00:28:40,180
and furthermore this protocol is actually realizable

459
00:28:40,200 --> 00:28:46,480
that's the surprising thing there are schedules for computing the messages which which respect this

460
00:28:46,480 --> 00:28:49,600
protocol and on the tree

461
00:28:49,640 --> 00:28:54,630
impossible schedules and each one of them just consists of picking an arbitrarily is the

462
00:28:54,630 --> 00:29:01,540
root and passing messages inward to the right and then back outward away from so

463
00:29:01,540 --> 00:29:04,350
let's just drop in a different color here

464
00:29:04,360 --> 00:29:09,360
so let's say arbitrarily designate this node has been

465
00:29:09,370 --> 00:29:14,440
for the purposes of doing the computation of the messages right and that the schedule

466
00:29:14,440 --> 00:29:18,250
is to pass messages inward to the root

467
00:29:18,300 --> 00:29:22,100
that's the first pass messages that we would compute and then back

468
00:29:22,120 --> 00:29:24,250
distribute back away

469
00:29:24,410 --> 00:29:25,870
from the root

470
00:29:25,870 --> 00:29:29,760
yes number the formula number one number two number three and four

471
00:29:29,770 --> 00:29:35,110
now delta by plus one

472
00:29:35,140 --> 00:29:37,600
is what

473
00:29:37,610 --> 00:29:42,360
well there are two cases

474
00:29:42,410 --> 00:29:44,210
it's delta i

475
00:29:44,210 --> 00:29:48,050
the famous delta i in one of the cases

476
00:29:49,000 --> 00:29:52,040
that i

477
00:29:52,100 --> 00:29:54,370
together with gamma

478
00:29:54,380 --> 00:29:56,990
together with

479
00:30:01,220 --> 00:30:08,850
entails a contradiction

480
00:30:11,630 --> 00:30:13,880
and it's delta i

481
00:30:13,890 --> 00:30:15,990
together with

482
00:30:16,300 --> 00:30:21,460
i i plus one otherwise

483
00:30:21,610 --> 00:30:28,790
and also of course

484
00:30:28,800 --> 00:30:31,810
it's going to be the union

485
00:30:31,870 --> 00:30:39,490
the delta ice

486
00:30:55,750 --> 00:30:58,810
we going along collecting a set of formulae

487
00:30:58,850 --> 00:31:00,150
as we go

488
00:31:01,370 --> 00:31:05,750
we walk along this list here

489
00:31:05,750 --> 00:31:09,560
and as we come to each formula we either throw it in the delta or

490
00:31:09,570 --> 00:31:11,300
we leave it out

491
00:31:11,380 --> 00:31:13,510
we try what can

492
00:31:13,510 --> 00:31:16,420
without going inconsistent

493
00:31:16,480 --> 00:31:18,130
given gap

494
00:31:18,180 --> 00:31:26,720
all right if putting it in would make things inconsistent then we leave it out

495
00:31:26,800 --> 00:31:29,140
all right and the big delta that we

496
00:31:29,160 --> 00:31:32,920
that we're going to end up with is all the form consists of all the

497
00:31:32,920 --> 00:31:35,480
formula with fourteen

498
00:31:36,040 --> 00:31:40,060
the like this

499
00:31:41,270 --> 00:31:45,400
or that we need to observe

500
00:31:45,450 --> 00:31:49,080
a lot of

501
00:31:50,540 --> 00:31:55,720
about this construction

502
00:31:55,810 --> 00:32:00,070
first of all

503
00:32:00,120 --> 00:32:03,590
we observe that for every

504
00:32:03,600 --> 00:32:05,420
delta i

505
00:32:05,480 --> 00:32:06,740
is consistent

506
00:32:13,400 --> 00:32:17,600
in fact

507
00:32:17,620 --> 00:32:21,330
we can observe something stronger than that

508
00:32:21,480 --> 00:32:23,980
delta i

509
00:32:24,020 --> 00:32:28,500
together with gamma

510
00:32:28,540 --> 00:32:33,290
does not entail that guy

511
00:32:39,060 --> 00:32:43,230
union of delta i have ended up

512
00:32:43,250 --> 00:32:44,880
the little set here

513
00:32:44,980 --> 00:32:46,480
a way

514
00:32:46,490 --> 00:32:49,750
union of that with gamma doesn't enter the

515
00:32:49,790 --> 00:32:51,020
a contradiction

516
00:32:51,030 --> 00:32:52,980
why is that

517
00:32:53,040 --> 00:32:55,630
induction on i

518
00:32:55,650 --> 00:32:58,040
it holds for the base case

519
00:32:58,040 --> 00:33:00,230
because delta zero

520
00:33:00,280 --> 00:33:02,980
union gamma is just again

521
00:33:03,070 --> 00:33:05,520
and i'm going to buy

522
00:33:08,310 --> 00:33:12,110
that's what we started with was the consistent

523
00:33:12,120 --> 00:33:16,300
and if we suppose that delta i is consistent

524
00:33:18,880 --> 00:33:23,450
then delta i was one

525
00:33:24,830 --> 00:33:28,890
well outside the delta i in which case is consistent

526
00:33:28,910 --> 00:33:33,150
all its delta i

527
00:33:33,200 --> 00:33:36,760
together with one more formula

528
00:33:36,860 --> 00:33:41,310
well don't i mean this is

529
00:33:41,460 --> 00:33:45,570
with gamma rays

530
00:33:45,580 --> 00:33:50,510
or is it is not known but in order to get one more formula

531
00:33:50,510 --> 00:33:53,380
we have to be in this case

532
00:33:53,510 --> 00:33:55,500
this is the case

533
00:33:57,130 --> 00:33:59,000
is consistent

534
00:33:59,140 --> 00:34:05,130
consistent with gamma

535
00:34:06,680 --> 00:34:10,860
so by construction at every step

536
00:34:11,330 --> 00:34:17,520
we study exist with gamma

537
00:34:20,330 --> 00:34:23,420
that's that's that proof just induction on i

538
00:34:29,080 --> 00:34:30,960
crucial step

539
00:34:30,970 --> 00:34:33,220
in in the whole completeness

540
00:34:38,230 --> 00:34:40,430
well let's

541
00:34:40,490 --> 00:34:41,330
all right

542
00:34:46,260 --> 00:34:48,020
just before the crucial step

543
00:34:48,040 --> 00:34:54,060
the patient a subset of delta

544
00:34:56,570 --> 00:34:58,720
how can it not be

545
00:34:58,730 --> 00:35:02,080
take any formula in gamma it's all a

546
00:35:02,140 --> 00:35:04,550
a i plus one for some time

547
00:35:04,590 --> 00:35:08,720
right at some point we come like considering

548
00:35:08,800 --> 00:35:10,210
all right

549
00:35:10,220 --> 00:35:14,400
we consider delta i together with with

550
00:35:14,460 --> 00:35:15,930
plus one

551
00:35:15,980 --> 00:35:19,090
but i like was one gamma

552
00:35:19,110 --> 00:35:21,880
all right

553
00:35:25,800 --> 00:35:31,670
i mean that can't that can't be inconsistent inconsistent we just said that

554
00:35:31,720 --> 00:35:33,220
right i mean the death

555
00:35:33,260 --> 00:35:35,640
delta i together with gamma

556
00:35:35,720 --> 00:35:37,220
is consistent

557
00:35:37,310 --> 00:35:39,210
which would prove that

558
00:35:39,250 --> 00:35:44,240
but if i i was wondering gamma then then we're in this case

559
00:35:46,280 --> 00:35:50,560
therefore every formula in gamma gets added to delta

560
00:35:50,570 --> 00:35:54,790
sooner or later

561
00:35:55,000 --> 00:35:57,900
the observation

562
00:35:57,920 --> 00:35:59,560
is the delta

563
00:35:59,610 --> 00:36:02,000
is consistent

564
00:36:02,000 --> 00:36:09,330
there is no derivation

565
00:36:09,390 --> 00:36:10,810
of the bad guy

566
00:36:10,850 --> 00:36:14,880
a contradiction from delta

567
00:36:14,960 --> 00:36:20,510
how do we know that

568
00:36:20,510 --> 00:36:23,790
but one of the things they would ask is the referees to guess who could

569
00:36:23,790 --> 00:36:27,580
be the answer is if two of them gases then it is not going to

570
00:36:27,580 --> 00:36:28,910
be accepted

571
00:36:28,970 --> 00:36:32,870
so there is a danger as well so this kind of those things they have

572
00:36:32,870 --> 00:36:37,030
done the first thing they found is that in two thousand six onwards

573
00:36:37,040 --> 00:36:43,120
this axis rate of those established researchers dropped from sixty percent success rate two

574
00:36:43,130 --> 00:36:44,870
one forty

575
00:36:44,890 --> 00:36:46,610
so you can see that

576
00:36:46,620 --> 00:36:47,910
you know when you do

577
00:36:47,930 --> 00:36:49,820
a change to the model

578
00:36:49,830 --> 00:36:53,400
you see the effect that you wanted to study these kinds of things

579
00:36:53,430 --> 00:36:57,600
so you know that in the data mining conference every year

580
00:36:57,640 --> 00:36:59,120
one of the

581
00:36:59,210 --> 00:37:03,720
programme committee members would provide all these kinds of statistics you you know because we're

582
00:37:03,730 --> 00:37:07,410
in data mining they got the data collecting all these years

583
00:37:07,480 --> 00:37:10,830
they say you know they tell you if you have these words in the title

584
00:37:10,850 --> 00:37:14,440
the likelihood of your paper accepted is much much

585
00:37:14,520 --> 00:37:18,930
and similarly if your if your surname is one

586
00:37:18,980 --> 00:37:24,230
you have a high probability now these are meaningless but just statistically you find on

587
00:37:24,230 --> 00:37:29,340
these datasets and so you can generalize because samples which is very small in single

588
00:37:29,340 --> 00:37:34,900
short but it's clear to see some of these kinds of contrast coming

589
00:37:34,910 --> 00:37:40,380
so we may be interested in contrast in terms of spatial sense of for example

590
00:37:40,440 --> 00:37:41,240
let's say

591
00:37:41,240 --> 00:37:43,370
find the distinguishing features

592
00:37:43,380 --> 00:37:48,970
location x in human DNA versus location x in mossley DNA so it could be

593
00:37:48,970 --> 00:37:53,690
interesting from an evolutionary sense or you could also be interested in in the functional

594
00:37:55,860 --> 00:37:57,610
sometimes we are interested in

595
00:37:58,160 --> 00:37:59,850
you overfitting

596
00:37:59,860 --> 00:38:01,480
within the class itself

597
00:38:01,490 --> 00:38:07,080
are there any contest so for example find the difference between students versus science students

598
00:38:07,090 --> 00:38:10,440
you know there may be different kinds of interest they might go to different parts

599
00:38:10,440 --> 00:38:11,430
for example

600
00:38:11,480 --> 00:38:15,560
you know all kinds of things you may be interested in knowing

601
00:38:15,610 --> 00:38:19,730
so sometimes we are interested in knowing contrast between the class

602
00:38:19,770 --> 00:38:22,650
so for example it says within the academic profession

603
00:38:22,660 --> 00:38:26,130
there are few people older than eighty years old you know

604
00:38:26,310 --> 00:38:28,530
maybe dementia

605
00:38:28,590 --> 00:38:30,830
and other kinds of illnesses

606
00:38:30,840 --> 00:38:33,220
probably want a lot of them

607
00:38:33,230 --> 00:38:34,730
to be very productive

608
00:38:34,740 --> 00:38:36,630
be uncertain age

609
00:38:36,640 --> 00:38:40,690
and also you want the younger ones to be promoted to senior positions

610
00:38:40,700 --> 00:38:45,530
and then we can academic profession there are no rich people for this is kind

611
00:38:45,530 --> 00:38:48,320
of an absence of the of the concepts

612
00:38:48,330 --> 00:38:53,650
or absence of certain patterns all within computer science most of the papers tend to

613
00:38:53,650 --> 00:38:57,260
come from the US and europe which is not surprising

614
00:38:57,280 --> 00:39:03,020
OK objects within it we may be interested in object positions in a ranking sense

615
00:39:03,020 --> 00:39:04,090
so for example

616
00:39:04,140 --> 00:39:07,740
find the difference between high and low income earners

617
00:39:07,740 --> 00:39:10,990
you know these kinds of things we may be interested

618
00:39:11,000 --> 00:39:15,830
so sometimes we want to combine all these kinds of aspects in studying the

619
00:39:15,980 --> 00:39:18,950
this contrasts

620
00:39:19,350 --> 00:39:23,750
there are lots of names to be in the literature sometimes you know people write

621
00:39:23,750 --> 00:39:28,140
papers with different keywords for example they might use change

622
00:39:28,150 --> 00:39:33,180
sometimes difference discriminator classification rules and all these things

623
00:39:34,930 --> 00:39:40,480
the contest data mining is generally related to the topics

624
00:39:40,490 --> 00:39:43,390
you know for example change detection

625
00:39:43,410 --> 00:39:45,580
class based association rules

626
00:39:45,590 --> 00:39:47,070
contrast sets

627
00:39:47,080 --> 00:39:51,120
concept drift difference difference detection

628
00:39:51,140 --> 00:39:54,790
discriminative patterns and similarities dissimilarities

629
00:39:54,810 --> 00:40:02,280
emerging patterns high confidence patterns infrequent patterns top k patterns and so forth

630
00:40:02,930 --> 00:40:09,240
what are the characteristics of data mining and generally applied to multivariate data which means

631
00:40:09,240 --> 00:40:10,980
more than one dimension

632
00:40:11,480 --> 00:40:13,830
generally very large dimensions

633
00:40:13,870 --> 00:40:18,020
objects may be relational in the sense that the data you look at the have

634
00:40:18,020 --> 00:40:19,980
a fixed number of features

635
00:40:20,030 --> 00:40:23,600
each feature has a specific meaning

636
00:40:24,480 --> 00:40:26,720
i mean you could be sequential

637
00:40:26,750 --> 00:40:31,110
like DNA sequences that we made up in a finite set of of that

638
00:40:31,120 --> 00:40:32,750
but the sequences could be

639
00:40:32,860 --> 00:40:35,150
variable length

640
00:40:35,210 --> 00:40:38,710
there could be graphs some kind of connected components

641
00:40:38,740 --> 00:40:42,070
and they could be modeled so for example you might have

642
00:40:42,740 --> 00:40:44,560
kinds of models in our people

643
00:40:44,570 --> 00:40:47,940
propose real estate is for some observations

644
00:40:47,950 --> 00:40:52,490
and you wanted to contrast these these theories and how do they differ radically inside

645
00:40:52,630 --> 00:40:56,510
they they contradict each other so these are of interest to us

646
00:40:56,520 --> 00:41:00,390
or it could be two different classifiers you you build them and you want to

647
00:41:00,390 --> 00:41:03,610
contrast between these things these two express

648
00:41:03,620 --> 00:41:06,860
or you might come in our you might have datasets

649
00:41:06,900 --> 00:41:09,510
which might have all of these things you you know you might have

650
00:41:09,650 --> 00:41:12,110
you know the section you can think of

651
00:41:12,680 --> 00:41:14,740
heterogeneous systems

652
00:41:14,790 --> 00:41:16,840
it has a relational

653
00:41:17,240 --> 00:41:18,720
it's a very top level

654
00:41:19,230 --> 00:41:22,020
and each feature could be sequential

655
00:41:22,110 --> 00:41:26,510
are you could be a graph or you can start with the graph and then

656
00:41:26,510 --> 00:41:31,040
you have each node could be relational age could be a sequence and all kinds

657
00:41:31,040 --> 00:41:34,940
of these complications you can put them into the system

658
00:41:35,020 --> 00:41:37,090
users may want to

659
00:41:37,150 --> 00:41:40,480
to find multiple contrasts so in the sense that if you are given

660
00:41:41,220 --> 00:41:45,970
datasets you try to find contact between them and you may be interested in finding

661
00:41:45,970 --> 00:41:47,320
all of

662
00:41:47,360 --> 00:41:49,530
it could be that could be exponential in number

663
00:41:49,540 --> 00:41:52,030
or you're interested in top k

664
00:41:52,040 --> 00:41:53,740
the moment you set top k

665
00:41:53,740 --> 00:41:56,480
there is some kind of measure you given you know

666
00:41:56,710 --> 00:42:00,340
you know in order to compare the top to you need some kind of ranking

667
00:42:00,370 --> 00:42:05,720
so you have to introduce some kind of ranking to these these these patterns

668
00:42:05,750 --> 00:42:11,610
again the moment user ranking know there could be arbitrarily infinite number of functions

669
00:42:11,650 --> 00:42:13,450
one one could choose

670
00:42:13,460 --> 00:42:15,860
one could choose for ranking purposes

671
00:42:15,900 --> 00:42:19,830
you know that's why what happens is you know whenever you're not have you know

672
00:42:19,830 --> 00:42:23,570
the universe is actually play this game in their rank them if you're in the

673
00:42:23,570 --> 00:42:28,280
top rank nobody questions the ranking method your quite or even in the top five

674
00:42:28,280 --> 00:42:29,490
you don't question

675
00:42:29,590 --> 00:42:33,070
the moment you rank is low the first thing you do is the ranking function

676
00:42:33,070 --> 00:42:34,220
is meaningless

677
00:42:34,260 --> 00:42:38,300
and also in the such console in this election

678
00:42:38,310 --> 00:42:42,700
the next round applications they have to do the ranking and the year i get

679
00:42:43,290 --> 00:42:45,770
ground ranking is perfect

680
00:42:45,950 --> 00:42:47,640
next i miss it

681
00:42:47,640 --> 00:42:50,760
no the rankings terrible is meaningless

682
00:42:51,490 --> 00:42:55,470
but they have course we always want to have a ranking ranking makes

683
00:42:55,480 --> 00:42:57,930
like if an administrator very simple

684
00:42:58,060 --> 00:43:00,540
to promote or this

685
00:43:00,570 --> 00:43:03,750
and or to give an incomplete or not to give an incoming

686
00:43:03,870 --> 00:43:06,940
so you need to have some kind of a scoring function so that we can

687
00:43:09,870 --> 00:43:13,730
you know in an australian context if you don't know now this going to be

688
00:43:13,730 --> 00:43:15,000
huge ranking

689
00:43:15,020 --> 00:43:17,520
systems come for example

690
00:43:17,530 --> 00:43:20,660
you know the number of papers he published in the year maybe one

691
00:43:20,660 --> 00:43:22,320
one component

692
00:43:22,320 --> 00:43:25,130
and what can often help is to try running a bigger model

693
00:43:25,280 --> 00:43:27,960
which is a little bit like using more features in machine learning

694
00:43:28,670 --> 00:43:32,210
or maybe even just running optimizer longer asking if it's converged

695
00:43:33,120 --> 00:43:36,480
and finally if gap is large

696
00:43:36,690 --> 00:43:40,510
you're doing great on your training data but you do horribly on your testing data

697
00:43:40,870 --> 00:43:45,280
it could be that you need a lot more data you should think about things like reflecting images

698
00:43:45,780 --> 00:43:47,710
adding distortions to get some more data

699
00:43:48,010 --> 00:43:49,400
and this is a key trick

700
00:43:49,660 --> 00:43:51,650
and those imagenet results showed you

701
00:43:52,110 --> 00:43:55,880
you think about things like regular position or maybe even just using smaller model

702
00:43:57,720 --> 00:44:02,430
so we lost about twenty minutes so i want to switch gears

703
00:44:02,570 --> 00:44:04,470
to tell you a slightly different story

704
00:44:04,760 --> 00:44:06,680
which is maybe when you already heard

705
00:44:07,500 --> 00:44:08,760
of deep learning

706
00:44:09,070 --> 00:44:09,800
which is

707
00:44:09,960 --> 00:44:12,000
what i would say is unsupervised deep learning

708
00:44:12,180 --> 00:44:15,440
many of you will say this is representation learning

709
00:44:16,540 --> 00:44:18,060
and the idea here is that

710
00:44:18,360 --> 00:44:20,780
in supervised learning we were trying to train

711
00:44:21,010 --> 00:44:22,090
these different layers

712
00:44:23,090 --> 00:44:23,850
of features

713
00:44:24,400 --> 00:44:29,180
by essentially using some top-level objective so we have a bunch of labels that we want to fit

714
00:44:30,200 --> 00:44:31,680
and we were backpropagating

715
00:44:31,830 --> 00:44:35,610
are gradient through the network to try to fit all these parameters in the middle

716
00:44:35,840 --> 00:44:38,010
define not only the best classifier

717
00:44:38,140 --> 00:44:41,660
but also the best possible features to make a classification work well

718
00:44:42,820 --> 00:44:44,250
but it's worth asking

719
00:44:44,450 --> 00:44:46,110
you know the lines shares

720
00:44:46,320 --> 00:44:49,450
and the lines share all parameters are system tend to be

721
00:44:49,640 --> 00:44:50,740
in the lower layers

722
00:44:52,690 --> 00:44:56,220
what if we don't have enough labels to actually train all these parameters

723
00:44:57,170 --> 00:44:58,890
and for instance

724
00:44:59,900 --> 00:45:00,970
as possible

725
00:45:01,200 --> 00:45:07,280
to train representation actually train these features without using this supervised information

726
00:45:07,530 --> 00:45:08,760
from our labelled data

727
00:45:09,050 --> 00:45:14,690
and in particular can we learn good representations h or h times the higher-level representation

728
00:45:15,040 --> 00:45:17,360
directly using only unlabeled data

729
00:45:18,140 --> 00:45:20,540
so this is what we call representation learning

730
00:45:20,980 --> 00:45:23,110
what we want to understand is

731
00:45:23,420 --> 00:45:29,310
you know how how we accomplish this in general what is it that makes a good representation

732
00:45:29,510 --> 00:45:30,320
other than just

733
00:45:30,520 --> 00:45:32,950
making my supervised algorithm work much better

734
00:45:34,000 --> 00:45:36,120
so there are few things that we want

735
00:45:36,390 --> 00:45:41,080
and this is these three points kind of a nice summary from yoshua bengio

736
00:45:41,400 --> 00:45:43,400
and some recent papers is

737
00:45:43,970 --> 00:45:50,320
and the three points i think sort represent key things that we really want from a representation

738
00:45:50,780 --> 00:45:53,230
are we want to be distributed

739
00:45:53,400 --> 00:45:58,660
and what distributed means is that if we have k neurons k features coming out of our network

740
00:45:58,950 --> 00:46:01,170
we don't just want to end up representing

741
00:46:01,390 --> 00:46:02,460
k patterns

742
00:46:02,660 --> 00:46:07,440
we can only represent k patterns like with clustering model things that are mutually exclusive

743
00:46:07,630 --> 00:46:11,200
that we're going to need tons and tons of neurons and tons of parameters to do

744
00:46:11,220 --> 00:46:12,100
anything interesting

745
00:46:12,430 --> 00:46:16,080
so we really want our to representation to be distributed which means

746
00:46:16,310 --> 00:46:17,660
if we have k neurons

747
00:46:17,780 --> 00:46:21,040
we rather be able to represent something like to the k

748
00:46:21,220 --> 00:46:24,180
different patterns we'd like to be able to make it much more express

749
00:46:25,180 --> 00:46:27,470
we like representations to be invariant

750
00:46:28,780 --> 00:46:33,970
we talked about invariance that in terms of pooling units you a neuron can turn on

751
00:46:33,990 --> 00:46:38,770
and stayed turned on even when the pattern underneath changes when it's part of one concept

752
00:46:40,440 --> 00:46:43,230
which is generally we think these kinds of features are really useful

753
00:46:43,420 --> 00:46:43,980
and we want

754
00:46:44,120 --> 00:46:47,360
our representations to get more abstract as we go higher levels

755
00:46:47,750 --> 00:46:53,040
are so for example invariant feature we already know about things like pooled edge features so that when an

756
00:46:53,060 --> 00:46:54,500
edge moves left right

757
00:46:54,770 --> 00:46:57,050
we still have some feature that can detect it

758
00:46:57,390 --> 00:47:00,890
and know that there's an edge they're even though it's not always the same place

759
00:47:01,930 --> 00:47:02,890
and finally

760
00:47:03,140 --> 00:47:06,520
we want a good representation to disentangle factors

761
00:47:06,660 --> 00:47:09,500
we want to somehow takes separate concepts

762
00:47:09,950 --> 00:47:14,810
like the color of a patch and whether there's an edge in a patch we want to put

763
00:47:14,830 --> 00:47:16,020
them in separate features

764
00:47:16,580 --> 00:47:20,030
so this makes our representation more understandable

765
00:47:20,150 --> 00:47:22,460
because we don't have all these things tangled up

766
00:47:22,760 --> 00:47:28,460
but it also turns out to make a performance benefit for example when you're using these locally connected

767
00:47:30,420 --> 00:47:32,920
so those are the basic ideas high-level we'd like

768
00:47:33,140 --> 00:47:35,740
but boiling these things down into an algorithm

769
00:47:35,880 --> 00:47:37,320
so we can actually implement

770
00:47:37,570 --> 00:47:38,680
is the tricky part

771
00:47:38,890 --> 00:47:43,900
and so the field of unsupervised feature learning this technique is about

772
00:47:44,070 --> 00:47:47,950
trying to find algorithms trying to find representation learning methods

773
00:47:48,170 --> 00:47:51,540
that eq actually build representations with these kinds of properties

774
00:47:51,730 --> 00:47:53,800
from data that when we don't have labels

775
00:47:55,520 --> 00:47:56,570
the key idea

776
00:47:56,760 --> 00:48:01,010
is to take all the tools we had for training neural networks take backprop

777
00:48:02,340 --> 00:48:06,260
we take our neural network architectures with pooling in contrast normalization

778
00:48:06,480 --> 00:48:08,090
take all the stuff that we just did

779
00:48:08,610 --> 00:48:13,240
but to get rid of the supervised loss we had we had penalty function at the top

780
00:48:13,470 --> 00:48:14,960
that depended on labelled data

781
00:48:15,110 --> 00:48:19,800
and we're just going to remove that and replace it with an unsupervised training loss

782
00:48:20,750 --> 00:48:24,980
and the place that we get these losses from the place we get these optimization criteria from

783
00:48:25,250 --> 00:48:27,690
is usually based on generic priors about

784
00:48:27,940 --> 00:48:30,480
characteristics that good features should have

785
00:48:30,790 --> 00:48:34,800
and some this is born from experience sometimes they're theoretical

786
00:48:34,990 --> 00:48:38,400
considerations neuroscience is sometimes inspiration

787
00:48:38,590 --> 00:48:40,230
for the kinds of features we want to build

788
00:48:40,750 --> 00:48:44,980
so we try to come up with loss functions that sort of us tell us what

789
00:48:45,000 --> 00:48:46,510
kind of features we want to find

790
00:48:46,670 --> 00:48:48,840
and then we're going to train these neural networks

791
00:48:48,990 --> 00:48:52,380
to minimize the loss functions using all the tools we already talked about

792
00:48:53,230 --> 00:48:55,060
and then once we've done our training

793
00:48:55,350 --> 00:48:58,700
which hopefully tell us what to put for these weights w

794
00:48:59,020 --> 00:48:59,870
we can

795
00:49:00,150 --> 00:49:01,580
just use those features

796
00:49:01,730 --> 00:49:07,220
in for example for example some off the shelf supervised classifier that just stick on top

797
00:49:07,350 --> 00:49:09,000
and the sort of paradigm where

798
00:49:09,120 --> 00:49:11,070
we train up our representation

799
00:49:11,260 --> 00:49:14,150
from unlabeled data and then we use a small amount of

800
00:49:14,440 --> 00:49:18,120
labelled data to train to classify the very end sometimes called self-taught learning

801
00:49:20,340 --> 00:49:25,040
so the key idea for building these things is the greedy layer-wise strategy

802
00:49:26,150 --> 00:49:29,830
and the basic idea is that we're going to start from the very bottom our

803
00:49:29,830 --> 00:49:34,920
in some nontrivial cases like the one i discussed it's also it's necessary and sufficient

804
00:49:34,920 --> 00:49:39,290
and then in other cases it's still useful as an more tractable

805
00:49:39,290 --> 00:49:41,520
convex condition

806
00:49:41,580 --> 00:49:44,650
that guarantees the the polynomial is nonnegative

807
00:49:44,670 --> 00:49:50,150
and that's used in for non convex global optimisation of polynomial

808
00:49:50,250 --> 00:49:52,250
as a relaxation is again

809
00:49:52,250 --> 00:49:54,580
black station set

810
00:49:54,620 --> 00:49:57,380
another exactly equivalents but

811
00:49:57,420 --> 00:50:01,360
if you lower bounds and so on

812
00:50:01,380 --> 00:50:06,040
so this was just once example maybe maybe i should skip two

813
00:50:06,100 --> 00:50:11,060
it's more practical example to show that it is nonnegative polynomial constraints might appear this

814
00:50:11,060 --> 00:50:13,580
is an example from single processing

815
00:50:13,630 --> 00:50:16,670
here is a polynomial of course and polynomial

816
00:50:16,690 --> 00:50:19,690
and it's the transfer function of some filters

817
00:50:19,690 --> 00:50:23,580
and coefficients h will be the variables

818
00:50:23,580 --> 00:50:29,960
we're interested in minimizing the maximum value in the stop band of this field so

819
00:50:29,960 --> 00:50:32,600
minimizing the maximum value here above some

820
00:50:32,620 --> 00:50:34,290
frequency omega as

821
00:50:34,310 --> 00:50:39,210
so that's probably a could write like this seem in my city he will be

822
00:50:39,210 --> 00:50:41,690
an upper bound on this value in the

823
00:50:43,290 --> 00:50:45,290
subject to the constraint that h

824
00:50:45,290 --> 00:50:50,060
this course in polynomial is less empty and greater minus the on the interval

825
00:50:50,080 --> 00:50:56,000
so that's nonnegative constraint because this is really saying at t minus discourse in polynomial

826
00:50:56,060 --> 00:50:58,630
is non negative on an interval

827
00:50:58,650 --> 00:51:01,560
as a constraint on the coefficients h

828
00:51:01,620 --> 00:51:05,790
and that's something you can express their sums of squares theorems for these constraints for

829
00:51:05,850 --> 00:51:07,830
course in polynomials on intervals of

830
00:51:09,150 --> 00:51:13,540
and they turn into a semidefinite constraints and so you could solve this problem

831
00:51:13,580 --> 00:51:15,250
as an SDP

832
00:51:15,520 --> 00:51:19,000
that makes sense

833
00:51:19,350 --> 00:51:22,020
and it will be necessary and sufficient

834
00:51:22,040 --> 00:51:30,770
so maybe the last example i can do quickly it's also a class of

835
00:51:31,170 --> 00:51:33,360
applications of SDP

836
00:51:33,400 --> 00:51:38,170
i think everyone's familiar with the standard chebyshev inequalities one-sided or two-sided

837
00:51:38,190 --> 00:51:40,860
that give you bounds on the probability that

838
00:51:40,880 --> 00:51:43,380
for example a two sided chebyshev inequality

839
00:51:43,380 --> 00:51:46,630
that the absolute value of a random variable is less than one

840
00:51:46,670 --> 00:51:49,750
if you know the mean and the variance

841
00:51:49,770 --> 00:51:53,790
so if x is a random variable with zero mean variance sigma scary

842
00:51:53,830 --> 00:51:55,330
then this is true for all

843
00:51:55,350 --> 00:52:00,060
put this the lower bound on the probability that x is less than one in

844
00:52:00,060 --> 00:52:01,250
absolute value

845
00:52:01,330 --> 00:52:06,190
actually tied because there exist distributions that achieves this by

846
00:52:06,210 --> 00:52:08,540
so that you can

847
00:52:08,790 --> 00:52:16,100
think about extensions of this where the sets x could be a factor

848
00:52:16,120 --> 00:52:17,980
instead of just the skater

849
00:52:18,040 --> 00:52:21,580
and then instead of saying that values less than one year extended to some set

850
00:52:23,290 --> 00:52:26,380
anyone lower bounds on the probability that x is in c given

851
00:52:26,400 --> 00:52:27,210
some moments

852
00:52:27,230 --> 00:52:30,080
the first and second moments of c

853
00:52:30,190 --> 00:52:34,040
so we look at one case where actually turns exactly into an SDP and this

854
00:52:34,040 --> 00:52:36,130
is where x is a random vector

855
00:52:36,130 --> 00:52:40,600
with a given mean a given second moment as

856
00:52:40,600 --> 00:52:44,080
and the set is described by a quadratic inequalities

857
00:52:44,150 --> 00:52:46,810
so this set is a set or

858
00:52:46,860 --> 00:52:50,960
it's described by m quadratic inequalities

859
00:52:54,420 --> 00:53:01,290
check should be as strict inequality to be precise

860
00:53:01,310 --> 00:53:05,170
and the the quadratic inequalities have to be convex so the ice could be indefinite

861
00:53:07,000 --> 00:53:10,500
so that we can examine again compute lower bound on the probability that x is

862
00:53:10,500 --> 00:53:11,810
in s

863
00:53:11,860 --> 00:53:16,130
there is a lower bound over all possible distributions with these moments

864
00:53:16,900 --> 00:53:19,250
it turns out that that's an SDP

865
00:53:19,270 --> 00:53:24,690
it's quite complicated has variables p q r and scalar variables but that p

866
00:53:24,730 --> 00:53:28,730
and the optimal value of this problem is actually this lower bounds that x is

867
00:53:28,730 --> 00:53:30,000
in s

868
00:53:30,000 --> 00:53:32,210
and as

869
00:53:32,830 --> 00:53:37,480
product of solving this you also get the distribution of cheeses

870
00:53:37,540 --> 00:53:39,830
so this is one example

871
00:53:39,850 --> 00:53:46,210
set c will be defined by this group currently has three linear inequalities two

872
00:53:46,210 --> 00:53:50,250
in non convex quadratic inequalities

873
00:53:50,270 --> 00:53:53,480
that's the mean of the distribution the first moment

874
00:53:53,500 --> 00:53:58,250
the dashed ellipses here gives an idea of the covariance so that's this ellipse

875
00:53:58,250 --> 00:54:02,330
its centre and then made six years as is the covariance

876
00:54:02,350 --> 00:54:03,270
let's give

877
00:54:03,480 --> 00:54:08,860
so now given suppose we are given a the mean of the distribution given the

878
00:54:08,860 --> 00:54:10,620
second moment

879
00:54:10,670 --> 00:54:13,060
then this is the solution ready

880
00:54:13,060 --> 00:54:15,960
dots give you the distribution achieve

881
00:54:16,020 --> 00:54:17,960
that have these moments

882
00:54:17,980 --> 00:54:20,830
and actually

883
00:54:20,850 --> 00:54:24,630
minimize the probability that x is in c

884
00:54:24,630 --> 00:54:26,040
so this

885
00:54:26,040 --> 00:54:30,690
OK so based on that so you have decrease in one of which is good

886
00:54:31,300 --> 00:54:33,440
on you can also basically

887
00:54:33,450 --> 00:54:38,280
in focus on fully mature and some of other and then estimate which converts some

888
00:54:38,280 --> 00:54:41,430
particularly towards basically the

889
00:54:41,450 --> 00:54:42,890
computer interest

890
00:54:42,910 --> 00:54:47,490
it's unbiased which is not something really chemical invasion but also

891
00:54:47,500 --> 00:54:52,460
its ability to satisfy yourself or limit OK so that's good if you want to

892
00:54:52,460 --> 00:54:56,410
play a lot of you are doing like learning balls all so if you want

893
00:54:56,410 --> 00:55:00,490
to use finite bones basically you can also play with it on obtain some kind

894
00:55:00,490 --> 00:55:03,910
of finite bounds for this type of a spin at i will go for that

895
00:55:04,680 --> 00:55:11,280
so let's basically here is important illustration of the be my estimate

896
00:55:11,310 --> 00:55:17,700
basically the function of so this is basically expected obviously as n goes to increase

897
00:55:17,700 --> 00:55:22,820
to infinity is going accomplish is the basically the all

898
00:55:22,840 --> 00:55:27,460
as n minus by former estimate if they is going to converge was zero as

899
00:55:27,460 --> 00:55:31,090
n goes to infinity this is unsure about this ontology taught

900
00:55:32,410 --> 00:55:36,590
so why do i show something like that actually is already an example

901
00:55:36,620 --> 00:55:40,520
when you see this is a trivial problem with building trying to compute essentially the

902
00:55:40,520 --> 00:55:45,920
dowry of the cell called basically using using some of the calorimeter the way you

903
00:55:46,120 --> 00:55:51,210
can see that evaluation can be actually quite useful for basically low dimensional problems some

904
00:55:51,210 --> 00:55:54,020
of the calorimeter is method of last resort

905
00:55:54,040 --> 00:55:57,220
so what about basically so this is the convergence

906
00:55:57,250 --> 00:56:01,890
this is all the while the realization to be aware the walls on we see

907
00:56:01,890 --> 00:56:08,450
that you realisation we obviously as expected the valuation quite significant so

908
00:56:08,470 --> 00:56:12,770
you all this property that had this fight so OK so i just say that

909
00:56:12,770 --> 00:56:17,670
i would just basically discussing that for people who never heard of the calorimeter everything

910
00:56:17,670 --> 00:56:19,600
i've been doing here

911
00:56:19,620 --> 00:56:22,810
in the case where i think having the unit square

912
00:56:22,820 --> 00:56:25,280
almost a so-called within square

913
00:56:25,300 --> 00:56:31,570
can be generalised essentially to any domain any space so in particular you could have

914
00:56:31,570 --> 00:56:32,660
have essentially

915
00:56:32,810 --> 00:56:36,850
that instead of being say a square

916
00:56:36,910 --> 00:56:42,800
on the two d plan you could have essentially hypercube basically of dimension n x

917
00:56:42,800 --> 00:56:45,520
one and x could be about mentions say one part of

918
00:56:46,190 --> 00:56:48,070
similarly you might be

919
00:56:48,090 --> 00:56:55,280
interested in computing said the volume of the point belonging to the hyperbolic basically

920
00:56:55,290 --> 00:56:57,690
in the space of dimension n x

921
00:56:57,710 --> 00:57:03,380
OK on your mother carla everything i mean this is the same before that supply

922
00:57:03,410 --> 00:57:09,180
also in this context never use whatsoever at any time any time in the process

923
00:57:09,180 --> 00:57:15,030
argument the dimension of the original space OK so multicolour it will remain valid in

924
00:57:15,030 --> 00:57:21,270
this case if i have that can of night rain in the hypercube uniformly distributed

925
00:57:21,270 --> 00:57:27,040
on hypercube snx that can count the number of clubs which fall within the eye

926
00:57:27,040 --> 00:57:31,670
ball on basically by divided by the total number of docs will have an estimate

927
00:57:31,670 --> 00:57:35,780
of the planet of belonging to the i bought OK

928
00:57:35,790 --> 00:57:37,280
so in particular

929
00:57:37,290 --> 00:57:39,740
what is quite we want to calumet

930
00:57:39,760 --> 00:57:43,040
is that the rate of convergence of the estimated to look at the violence is

931
00:57:43,040 --> 00:57:47,830
to always going to be in one of the capital and whatever being the dimension

932
00:57:48,150 --> 00:57:53,030
of the original space OK so this is essentially the main argument for using with

933
00:57:53,030 --> 00:57:58,930
the calorimeters because you start with the government to begin its rate of convergence is

934
00:57:58,930 --> 00:58:03,760
dependent on the rate of convergence to zero or the estimate is dependent of the

935
00:58:03,760 --> 00:58:10,370
dimension of the original space on formal sophisticated methods on the regularity of the contour

936
00:58:10,580 --> 00:58:15,850
of basically the our you're trying to compute the volume OK so that's really what

937
00:58:15,850 --> 00:58:19,640
is really nice philharmonic our rate of convergence is independent of the dimension and this

938
00:58:19,640 --> 00:58:21,080
is why i really

939
00:58:22,160 --> 00:58:27,240
people are using multicolour methods

940
00:58:29,510 --> 00:58:35,700
basically one should be also be careful about what's what we are doing

941
00:58:35,720 --> 00:58:38,760
so if you look at the literature sometimes people claim that

942
00:58:38,770 --> 00:58:42,750
monte carlo beat the curse of dimensionality because the rate of convergence

943
00:58:42,770 --> 00:58:46,210
of your estimate is going to be one of the capital and then being in

944
00:58:46,220 --> 00:58:50,520
the most important one being the dimension of the space this is not true because

945
00:58:50,520 --> 00:58:52,300
you have to be very careful with that

946
00:58:52,310 --> 00:58:56,190
OK so let's go back to the problem of essentially

947
00:58:56,210 --> 00:59:01,090
looking at my problem where i mean hypercube OK

948
00:59:03,540 --> 00:59:06,260
all i'm looking at basically

949
00:59:06,270 --> 00:59:13,830
computing the probability of falling into the eye ball hypersphere of what use are called

950
00:59:13,830 --> 00:59:18,310
one OK using the calorimeter OK so i know

951
00:59:18,330 --> 00:59:20,540
in this case the analytically

952
00:59:20,550 --> 00:59:23,420
divide by the volume of the hypersphere

953
00:59:23,420 --> 00:59:24,880
all right this one

954
00:59:25,990 --> 00:59:31,280
he has the function of the dimension the space is given by these expressions particular

955
00:59:31,300 --> 00:59:36,590
it converges to zero as n goes the dimension of the space increases OK so

956
00:59:36,620 --> 00:59:40,680
don't try to kind of like what happens in low dimensions often can be a

957
00:59:40,680 --> 00:59:45,800
bit misleading financial mentioned so assume you're trying to compute

958
00:59:45,810 --> 00:59:48,610
the probability using monte-carlo

959
00:59:48,620 --> 00:59:51,830
falling within the atmosphere

960
00:59:51,840 --> 00:59:57,680
given that you have an outpouring of solving the cube OK so was the violence

961
00:59:57,680 --> 00:59:59,740
of the estimate in this case

962
00:59:59,740 --> 01:00:01,370
well essentially

963
01:00:01,390 --> 01:00:04,780
is equal to approximately

964
01:00:05,540 --> 01:00:07,180
the probability

965
01:00:07,190 --> 01:00:12,980
the original point i'm interested in of falling within the hypersphere

966
01:00:13,080 --> 01:00:17,800
divided by the total number of samples so you could feel right because this is

967
01:00:17,800 --> 01:00:22,980
very small this is a very small property but you should never forget

968
01:00:22,980 --> 01:00:27,090
that actually what matters when you're doing with the callow is not the violence of

969
01:00:27,090 --> 01:00:32,880
your estimate but typically what matters is the related violence of estimate OK

970
01:00:34,610 --> 01:00:37,890
once more careful when you're doing with the comment that you want to compute the

971
01:00:37,890 --> 01:00:39,800
if we

972
01:00:39,840 --> 01:00:41,950
choose the decay factor lambda

973
01:00:41,970 --> 01:00:43,910
in our geometric series here

974
01:00:43,970 --> 01:00:46,520
appropriately then

975
01:00:48,360 --> 01:00:52,020
graph kernel here converges to the following expression

976
01:00:52,050 --> 01:00:54,790
e is the vector of all ones

977
01:00:54,800 --> 01:00:57,170
both here and here

978
01:00:57,190 --> 01:01:00,450
and this is multiplied to a matrix which is the inverse

979
01:01:00,470 --> 01:01:08,350
of the identity matrix minus lambda times the adjacency matrix of the product graph

980
01:01:08,390 --> 01:01:11,300
as you have seen in this simple example before

981
01:01:11,300 --> 01:01:13,950
the product graph is typically of size

982
01:01:13,970 --> 01:01:15,440
n square

983
01:01:15,450 --> 01:01:19,430
if you to input graphs have n nodes each

984
01:01:19,510 --> 01:01:20,710
so it is

985
01:01:20,770 --> 01:01:22,550
adjacency matrix of the

986
01:01:22,560 --> 01:01:25,540
product graph will be of size N square by N square

987
01:01:25,550 --> 01:01:29,280
and in writing this expression this matrix

988
01:01:29,300 --> 01:01:30,110
is then

989
01:01:30,120 --> 01:01:32,450
cubic in the size

990
01:01:32,470 --> 01:01:37,140
of this adjacency matrix system which means we've got an O of n to six

991
01:01:37,140 --> 01:01:42,540
effort for computing the random walk kernel

992
01:01:43,330 --> 01:01:44,390
the trick

993
01:01:44,410 --> 01:01:45,940
by the Vishwanathan et al

994
01:01:45,950 --> 01:01:50,040
was to recast the computation of these random walk kernels

995
01:01:50,080 --> 01:01:53,010
in terms of sylvester equations

996
01:01:53,020 --> 01:01:56,500
and these sylvester equations can be solved in cubic

997
01:01:56,540 --> 01:02:01,550
run time rather than O of n to six

998
01:02:01,600 --> 01:02:03,630
in order to understand this trick

999
01:02:03,680 --> 01:02:04,700
we need two

1000
01:02:06,400 --> 01:02:09,720
first of them is the so-called Vec operator

1001
01:02:09,770 --> 01:02:12,350
which vectorize a given matrix

1002
01:02:12,360 --> 01:02:16,900
an N by N matrix A is turned into an N squared by one vector

1003
01:02:17,530 --> 01:02:20,510
well of length N squared

1004
01:02:20,540 --> 01:02:21,810
and what happens is

1005
01:02:21,830 --> 01:02:25,890
that this operators stacks two columns of the matrix on top of each other from

1006
01:02:25,890 --> 01:02:28,980
left to right

1007
01:02:29,180 --> 01:02:33,170
the first concept the second one is its concept of kronecker products

1008
01:02:34,360 --> 01:02:35,930
especially product of two

1009
01:02:35,940 --> 01:02:38,650
the matrices a and b

1010
01:02:38,660 --> 01:02:42,390
and it works by multiplying each element of a

1011
01:02:43,400 --> 01:02:45,850
the complete matrix b

1012
01:02:45,850 --> 01:02:49,890
and if a and b of size n by n each then your kronecker product

1013
01:02:49,890 --> 01:02:55,760
will be of size and N square by N square

1014
01:02:55,820 --> 01:02:56,690
so now

1015
01:02:56,710 --> 01:02:57,830
what are these

1016
01:02:57,830 --> 01:03:02,680
so the best equations that vishwanthan et al used to speed up graph kernel

1017
01:03:02,720 --> 01:03:08,080
computations these are equations of the form X = X S T plus X_0

1018
01:03:10,500 --> 01:03:12,630
S T and X_0

1019
01:03:13,470 --> 01:03:15,400
three N by N matrices

1020
01:03:15,410 --> 01:03:19,020
and we want to solve for X

1021
01:03:19,020 --> 01:03:20,820
in this equation

1022
01:03:20,830 --> 01:03:24,530
these as i mentioned before are solvable in cubic runtime

1023
01:03:24,570 --> 01:03:27,820
in the size of these matrices

1024
01:03:27,840 --> 01:03:31,820
and as i also mentioned before it's possible to turn Sylvester equations into graph kernels and

1025
01:03:31,830 --> 01:03:34,900
we'll do just that now

1026
01:03:34,920 --> 01:03:36,090
the first step

1027
01:03:37,070 --> 01:03:41,900
doing that is vectorizing the Sylvester equation so all these matrices

1028
01:03:41,920 --> 01:03:45,000
i just turned into a rect

1029
01:03:45,000 --> 01:03:46,090
and then

1030
01:03:46,100 --> 01:03:47,420
one exploits

1031
01:03:47,460 --> 01:03:48,280
a well

1032
01:03:48,310 --> 01:03:49,540
known fact

1033
01:03:49,550 --> 01:03:51,830
from matrix algebra

1034
01:03:53,920 --> 01:03:54,770
the Vec

1035
01:03:54,770 --> 01:03:59,990
the vectorized form of this matrix product here is identical to the kronecker product of

1036
01:03:59,990 --> 01:04:01,710
t transposed

1037
01:04:01,730 --> 01:04:03,040
times s

1038
01:04:03,080 --> 01:04:07,130
times the vectorized form of x

1039
01:04:07,230 --> 01:04:11,130
then we block this equality here

1040
01:04:11,210 --> 01:04:14,020
in here

1041
01:04:14,130 --> 01:04:17,100
so for its exterior first and then

1042
01:04:17,120 --> 01:04:22,690
for vec of x

1043
01:04:22,750 --> 01:04:26,520
the only thing that we do then as we multiply

1044
01:04:26,580 --> 01:04:28,940
both sides of this equation by vec of

1045
01:04:29,000 --> 01:04:31,860
x zero transposed

1046
01:04:31,880 --> 01:04:34,600
obtaining this expression

1047
01:04:34,620 --> 01:04:37,230
so just by plugging in this definition

1048
01:04:37,250 --> 01:04:40,080
and multiplying by

1049
01:04:40,100 --> 01:04:45,500
vec of x zero transposed we go from this equation to this equation

1050
01:04:45,500 --> 01:04:47,940
down here

1051
01:04:47,960 --> 01:04:52,400
now all you have to do is we substitute appropriately and that expression namely

1052
01:04:52,400 --> 01:04:55,290
x zero is a matrix of all ones

1053
01:04:56,420 --> 01:04:58,770
is the adjacency matrix of g

1054
01:04:58,790 --> 01:04:59,710
time city

1055
01:04:59,730 --> 01:05:03,630
transposed times to decaying factor lambda

1056
01:05:03,670 --> 01:05:05,880
and s is the adjacency matrix

1057
01:05:05,900 --> 01:05:08,770
of g prime

1058
01:05:08,770 --> 01:05:10,080
and we obtain

1059
01:05:10,100 --> 01:05:11,520
if we block that in

1060
01:05:15,040 --> 01:05:16,690
the term

1061
01:05:17,650 --> 01:05:22,350
it is the graph common value for the random walk on

1062
01:05:22,440 --> 01:05:27,150
so by solving an inverse a sort of st equation we can compute the graph

1063
01:05:28,040 --> 01:05:29,880
well value

1064
01:05:29,940 --> 01:05:33,940
obviously you in the end is to sum up the entries of the back of

1065
01:05:33,940 --> 01:05:36,600
x here that's why there's an additional

1066
01:05:36,600 --> 01:05:43,150
the vector of all ones here

1067
01:05:43,150 --> 01:05:45,620
one can use this idea here

1068
01:05:45,650 --> 01:05:50,650
four further speedups for sparse graphs

1069
01:05:50,690 --> 01:05:52,420
so we now assume that

1070
01:05:52,440 --> 01:05:56,460
the two matrices s and t to two adjacency matrices are sparse

1071
01:05:56,480 --> 01:06:01,670
and then we observe the the following facts which is

1072
01:06:03,620 --> 01:06:08,250
whenever we have to compute this term here the kronecker product of teachers post and

1073
01:06:09,000 --> 01:06:12,790
and they effects

1074
01:06:12,810 --> 01:06:15,920
we encounter and off into the four effort

1075
01:06:18,060 --> 01:06:23,730
quite apart contains into the four entries

1076
01:06:23,750 --> 01:06:26,790
however this is identical as we have seen before

1077
01:06:26,790 --> 01:06:30,100
two vectors of xt

1078
01:06:30,150 --> 01:06:31,060
which is

1079
01:06:31,100 --> 01:06:36,650
a product of three matrices which requires cubic runtime and then the vectorisation is a

1080
01:06:36,650 --> 01:06:39,770
quadratic effort so rather than

1081
01:06:39,810 --> 01:06:43,330
having and two to four effort for commuting this term

1082
01:06:43,350 --> 01:06:45,130
we could be evaluated

1083
01:06:45,980 --> 01:06:49,250
this term here which can be computed in

1084
01:06:49,310 --> 01:06:52,100
cubic granted

1085
01:06:52,120 --> 01:06:54,880
and you can rewrite

1086
01:06:54,900 --> 01:06:57,420
the formula for the

1087
01:06:57,630 --> 01:07:00,080
random walk across

1088
01:07:00,150 --> 01:07:02,690
both as a fixed point iteration

1089
01:07:03,810 --> 01:07:06,150
as a linear equation

1090
01:07:06,170 --> 01:07:10,850
and in solving both the fixed point iteration problem and this linear equations

1091
01:07:10,860 --> 01:07:14,170
you repeatedly have to evaluate this term here

1092
01:07:14,210 --> 01:07:15,940
and by plugging in

1093
01:07:15,960 --> 01:07:17,270
this term

1094
01:07:18,380 --> 01:07:22,880
which is faster to compute you can achieve an additional speedup

1095
01:07:22,880 --> 01:07:24,210
for computing

1096
01:07:24,350 --> 01:07:28,830
aircraft can that you

1097
01:07:28,850 --> 01:07:30,650
so what's the impact on

1098
01:07:30,690 --> 01:07:34,600
the cross kind of computation runtime of these different

1099
01:07:35,790 --> 01:07:37,250
these are four

1100
01:07:37,270 --> 01:07:39,540
graph datasets

1101
01:07:39,560 --> 01:07:41,600
having seventeen

1102
01:07:41,600 --> 01:07:42,750
up to

1103
01:07:42,810 --> 01:07:44,480
thirty five

1104
01:07:44,520 --> 01:07:46,960
notes on average

1105
01:07:47,100 --> 01:07:48,960
as you can see here

1106
01:07:50,900 --> 01:07:55,400
the y axis is the run time in seconds and the direct approach is much

1107
01:07:55,400 --> 01:07:57,520
slower than these

1108
01:07:58,250 --> 01:08:00,500
novel approaches by which one

1109
01:08:03,560 --> 01:08:05,540
the worst case it's even

1110
01:08:05,540 --> 01:08:08,170
more than ten two to five times slower

1111
01:08:08,170 --> 01:08:09,810
and questions on this

1112
01:08:13,180 --> 01:08:19,370
ok so we've done so far is get gradients with respect to activations

1113
01:08:19,800 --> 01:08:23,190
and pre-activation it's now we finally need grain as spectra are

1114
01:08:23,190 --> 01:08:26,110
parameters because those are the parameters that we will b updating

1115
01:08:26,110 --> 01:08:29,300
with gradient step so let's do that now

1116
01:08:31,000 --> 01:08:34,400
say i want to get the gradient or the start with the derivative

1117
01:08:34,400 --> 01:08:36,080
the partial derivative of my loss

1118
01:08:36,080 --> 01:08:38,320
with respect to the connection between the

1119
01:08:38,730 --> 01:08:43,150
i with a unit in layer k and j unit in the layer below

1120
01:08:44,360 --> 01:08:49,440
i can again use the chain rule so i can write it as

1121
01:08:49,640 --> 01:08:54,610
the derivative respect my loss of my loss sorry with respect to

1122
01:08:54,620 --> 01:08:57,790
the highest pre-activation at my layer k

1123
01:08:58,750 --> 01:09:04,110
times what is the derivative of this pre-activation at layer k for

1124
01:09:04,120 --> 01:09:06,820
the unit with respect to my weight

1125
01:09:07,150 --> 01:09:10,870
the weight between unit i layer k and unit j

1126
01:09:11,020 --> 01:09:15,280
at layer below layer k minus one and again because this

1127
01:09:15,330 --> 01:09:18,450
act pre-activation function is just a linear transformation

1128
01:09:18,760 --> 01:09:22,510
take computing that they're very simple so the bias does not

1129
01:09:22,520 --> 01:09:25,050
depend on the weight so this goes away

1130
01:09:25,240 --> 01:09:28,740
and then this whole some here here there's only the term involving

1131
01:09:28,870 --> 01:09:31,430
the unit j with respect to which i'm

1132
01:09:31,880 --> 01:09:33,840
computing the derivative that mathers

1133
01:09:34,150 --> 01:09:38,640
and in this case all you will get is so derivative of the weights

1134
01:09:38,870 --> 01:09:41,970
multiplied by the activation but just going to be

1135
01:09:42,440 --> 01:09:47,700
the activation itself and so this term here is just replaced by

1136
01:09:47,910 --> 01:09:52,110
what is the g if activation in my hidden layer came minus one

1137
01:09:53,660 --> 01:09:57,150
and so now if i wanted to take all these partial learns input them

1138
01:09:57,150 --> 01:10:00,640
into gradient format so get the full matrix

1139
01:10:00,920 --> 01:10:06,130
of all the partial derivatives so i can see that the elements at

1140
01:10:06,760 --> 01:10:12,120
row i and column j is essentially just well take the partial

1141
01:10:12,130 --> 01:10:15,820
derivative for the tf pre-activation or the

1142
01:10:15,990 --> 01:10:18,490
detective is a layer k district that

1143
01:10:18,660 --> 01:10:22,760
i with unit times the activation of the j unit at layer

1144
01:10:22,770 --> 01:10:27,310
k minus one so one thing to obtain this is just take

1145
01:10:27,630 --> 01:10:31,350
the gradient vector pre-activation layer k that's a

1146
01:10:31,990 --> 01:10:37,160
column vector and then do not product or essentially take the product

1147
01:10:37,170 --> 01:10:40,540
with the row vector of the activations at layer

1148
01:10:41,410 --> 01:10:45,530
ok minus one so that means that the entry at the row

1149
01:10:45,840 --> 01:10:52,740
i and column j is just going to be the highest element of this vector

1150
01:10:52,750 --> 01:10:56,190
times j element of that vector and that is

1151
01:10:56,570 --> 01:10:58,640
exactly the expression that i have here

1152
01:10:58,810 --> 01:11:00,770
the i have of element of that vector

1153
01:11:00,930 --> 01:11:03,850
and j of element of the activation of layer k minus one

1154
01:11:05,210 --> 01:11:10,420
so really getting gradient of my full weight matrix for the gift

1155
01:11:10,430 --> 01:11:13,210
layer just take my pre-activation gradient

1156
01:11:13,660 --> 01:11:15,820
and the other product with the activation

1157
01:11:15,820 --> 01:11:19,170
gradient sorry the actual the the activation of the gradient the

1158
01:11:19,170 --> 01:11:20,930
activation at the layer below

1159
01:11:23,100 --> 01:11:25,660
and next we can do the same thing with the bias

1160
01:11:26,620 --> 01:11:29,950
as we've seen we can interpret the bias i just weight vector on

1161
01:11:29,950 --> 01:11:33,220
a constant unit equals one so not surprisingly we actually get

1162
01:11:33,220 --> 01:11:38,670
the same formula as for the weights but instead of h here we

1163
01:11:38,680 --> 01:11:41,310
have one so we just have effectively

1164
01:11:41,480 --> 01:11:43,590
during which respectively pre-activation

1165
01:11:44,130 --> 01:11:47,200
so i hope i wanted to construct the full gradient

1166
01:11:48,100 --> 01:11:54,400
for my full bias vector it is essentially just the pre-activation

1167
01:11:54,410 --> 01:11:58,010
gradient respect my full loss ok just grayer of

1168
01:12:01,960 --> 01:12:04,920
ok so let's put this all together and to get

1169
01:12:05,140 --> 01:12:08,030
finally are algorithm the algorithm that will give us

1170
01:12:08,240 --> 01:12:13,130
for some given example x y what is the gradient of the loss function

1171
01:12:13,140 --> 01:12:16,280
with respect to all my parameters and

1172
01:12:16,280 --> 01:12:19,510
what we'll essentially do is go through all these different gradients

1173
01:12:19,510 --> 01:12:22,940
that we've seen in order such that i can always be used computation

1174
01:12:22,940 --> 01:12:26,100
i've done before to get my computations that i need next

1175
01:12:26,540 --> 01:12:30,450
and we would do this is we first compute what is the gradient

1176
01:12:30,710 --> 01:12:34,580
of my loss which respect to the we could do the activation will do

1177
01:12:34,590 --> 01:12:38,180
the pre-activation it's formalize simpler at the output layer

1178
01:12:38,260 --> 01:12:41,610
so a l plus one that's pre-activation the output layer and

1179
01:12:41,620 --> 01:12:44,990
we've seen that the gradient of the loss for this pre-activation

1180
01:12:45,320 --> 01:12:48,500
is just minus one vector minus the output

1181
01:12:49,020 --> 01:12:51,860
so after the softmax so compute this

1182
01:12:52,250 --> 01:12:56,100
and i'm going to go from the output layer towards the first hidden

1183
01:12:56,110 --> 01:12:59,960
layer so for k from l plus one down to one

1184
01:13:01,090 --> 01:13:07,160
and l one i'm at a given layer k well i've already so when i'm starting

1185
01:13:07,160 --> 01:13:10,660
this iteration i'm assuming that i've already computed the gradient

1186
01:13:10,660 --> 01:13:12,560
of the pre-activation for that layer

1187
01:13:12,560 --> 01:13:15,530
so in other words i have what is the gradient of my loss

1188
01:13:15,530 --> 01:13:18,620
just like my pre-activation so the first thing i'm going to do in

1189
01:13:18,620 --> 01:13:22,290
my iteration is get my gradient with respect to my parameters for

1190
01:13:22,300 --> 01:13:26,140
that layer for layer k well what is that just i take my

1191
01:13:26,290 --> 01:13:29,000
pre-activation gradient i do the other product

1192
01:13:29,240 --> 01:13:32,110
with the activation of the layer below

1193
01:13:32,450 --> 01:13:35,730
so here in this algorithm i assume that i've already performed

1194
01:13:35,730 --> 01:13:39,840
for propagation solid we went from x i'll to the output i've computed

1195
01:13:39,840 --> 01:13:42,640
all the activations all layer i've computed my output

1196
01:13:43,230 --> 01:13:47,060
so this simple operation now gives me gradient for the weights

1197
01:13:47,440 --> 01:13:51,260
and now i want to gradient for the bias that just have shown the

1198
01:13:51,260 --> 01:13:54,080
gradient of the loss which respect that pre-activation at the

1199
01:13:54,080 --> 01:13:59,680
layer k and the next i'm going to take the gradient on the activations

1200
01:13:59,690 --> 01:14:02,130
and push it down to the next layer k minus one

1201
01:14:02,210 --> 01:14:05,750
so that the next iteration i can perform again the same computation

1202
01:14:05,750 --> 01:14:07,810
to get gradients for the layers below

1203
01:14:08,510 --> 01:14:14,240
so i can go from the gradients of the loss with respect to my pre-activation

1204
01:14:14,630 --> 01:14:18,090
to get the gradient on the loss with respect to the activation at

1205
01:14:18,100 --> 01:14:22,680
the layer below as i said by jus just taking

1206
01:14:23,100 --> 01:14:26,300
the gradient of the pre-activation and multiplying that

1207
01:14:26,300 --> 01:14:28,260
by the transpose of the weight matrix

1208
01:14:28,790 --> 01:14:32,940
simple operation and next i take what i just compute gradient

1209
01:14:32,950 --> 01:14:37,330
on the activation is right here i do element-wise multiplication

1210
01:14:37,330 --> 01:14:39,760
with the derivative of the activation function

1211
01:14:40,200 --> 01:14:44,700
again other simple so computation and that gives me now

1212
01:14:44,700 --> 01:14:48,170
the gradient of the loss function with respect to the pre-activation

1213
01:14:48,170 --> 01:14:52,320
of the layer below and i continue my iteration so i k becomes

1214
01:14:52,330 --> 01:14:55,280
came and this one i continue like this until i reached

1215
01:14:55,280 --> 01:14:57,780
after no

1216
01:15:02,700 --> 01:15:04,830
that afternoon so

1217
01:15:04,840 --> 01:15:07,620
my my role here is try to

1218
01:15:07,680 --> 01:15:12,260
you've very introductory class on graphical model

1219
01:15:13,760 --> 01:15:18,640
the particles will be to the really the first exposure to graphical models although i

1220
01:15:18,640 --> 01:15:21,400
know that many of you have already

1221
01:15:21,460 --> 01:15:23,980
i am familiar with graphical models but

1222
01:15:23,980 --> 01:15:27,170
this is the summer school that is to give

1223
01:15:27,190 --> 01:15:32,690
the basics of graphical models from the very beginning

1224
01:15:33,360 --> 01:15:36,250
now to make it easier for me like to know

1225
01:15:36,260 --> 01:15:39,900
how many of you have heard of graphical model

1226
01:15:40,010 --> 01:15:42,010
how many of you

1227
01:15:42,160 --> 01:15:48,010
i understand a little bit or more about graphical models

1228
01:15:48,040 --> 01:15:52,550
and how many of you have worked with graphical models

1229
01:15:52,560 --> 01:15:57,260
and how many of you have published papers on graph

1230
01:15:57,290 --> 01:16:01,690
that's very useful because this is we can

1231
01:16:01,730 --> 01:16:03,000
calibrate the

1232
01:16:06,010 --> 01:16:08,230
for the presentation is

1233
01:16:08,250 --> 01:16:10,280
it is designed before the

1234
01:16:10,300 --> 01:16:12,620
these assessment so

1235
01:16:13,620 --> 01:16:17,510
i probably down calibrated here really going into the very basics

1236
01:16:17,590 --> 01:16:20,360
but this is probably not be a very typical

1237
01:16:20,420 --> 01:16:22,830
for the coarse and graphical models because

1238
01:16:23,700 --> 01:16:26,420
the main goal was to

1239
01:16:26,470 --> 01:16:28,260
the present graphical models

1240
01:16:28,280 --> 01:16:35,470
from a slightly different perspective that's usually presented so really focus on the basic

1241
01:16:35,500 --> 01:16:38,700
the idea is the basic concept and how we build

1242
01:16:38,720 --> 01:16:41,060
there is this whole is whole theory

1243
01:16:41,950 --> 01:16:43,670
and of course we'll talk about

1244
01:16:43,700 --> 01:16:49,220
especially our talk more about applications and and what you can actually do with graph

1245
01:16:51,750 --> 01:16:54,590
so the idea is that the goal for one hour and as usual

1246
01:16:54,610 --> 01:16:55,970
we stop for

1247
01:16:56,000 --> 01:16:58,170
two break breaks

1248
01:16:58,200 --> 01:17:02,560
go down

1249
01:17:02,560 --> 01:17:05,000
OK first before starting

1250
01:17:05,030 --> 01:17:06,310
would you prefer

1251
01:17:06,310 --> 01:17:08,030
like this or

1252
01:17:20,520 --> 01:17:24,310
so before i start let me just tell you what you can find out there

1253
01:17:24,310 --> 01:17:28,540
to learn about graphical models or to improve our existing knowledge of graphical models so

1254
01:17:28,540 --> 01:17:31,030
there are quite a few books on these

1255
01:17:31,090 --> 01:17:32,620
subject so

1256
01:17:32,620 --> 01:17:33,750
i would say

1257
01:17:35,250 --> 01:17:38,590
a very recent book which includes lots about graphical model

1258
01:17:38,630 --> 01:17:43,810
it's chris bishop's book pattern recognition machine learning that's really very nice book

1259
01:17:46,470 --> 01:17:49,530
there are some other classical works on graphical models

1260
01:17:49,570 --> 01:17:52,020
like work

1261
01:17:52,030 --> 01:17:54,160
laurie to work

1262
01:17:54,240 --> 01:17:56,850
which are very very good book

1263
01:17:56,870 --> 01:17:59,310
very good references well

1264
01:17:59,340 --> 01:18:03,460
in terms of unpublished material i should say that people have

1265
01:18:03,470 --> 01:18:07,340
i learned a lot from published material particularly from jordan's book

1266
01:18:07,400 --> 01:18:08,520
and also

1267
01:18:08,530 --> 01:18:11,620
more recently color fremantle

1268
01:18:11,630 --> 01:18:13,620
but this is not a publisher so

1269
01:18:13,710 --> 01:18:16,710
you need to find someone who has a high quality

1270
01:18:17,410 --> 01:18:19,650
i have a hard copy i for

1271
01:18:22,960 --> 01:18:27,570
commercial solution i mean you can you can have to have hard copy

1272
01:18:27,590 --> 01:18:30,870
and they're very good views on graphical models as well

1273
01:18:31,160 --> 01:18:36,430
in videolectures net you can see for example sam roweis videos

1274
01:18:36,470 --> 01:18:38,190
which are really very

1275
01:18:38,280 --> 01:18:39,590
extremely good

1276
01:18:39,590 --> 01:18:42,320
he doesn't graphical models

1277
01:18:42,370 --> 01:18:47,850
OK so after seeing some readers a thought well what should be talking about

1278
01:18:47,970 --> 01:18:50,780
i'll just try to give different

1279
01:18:50,840 --> 01:18:52,780
type of perspective

1280
01:18:56,220 --> 01:18:57,180
i mean

1281
01:18:57,220 --> 01:18:59,070
basically a graphical model

1282
01:18:59,090 --> 01:19:02,710
it consists of a bunch of tools that can be used to solve problems in

1283
01:19:02,710 --> 01:19:04,190
many different fields

1284
01:19:04,250 --> 01:19:08,620
i mean i've seen particularly applied to all sorts of problems in image processing speech

1285
01:19:09,840 --> 01:19:12,950
natural image processing looking for that by from

1286
01:19:12,970 --> 01:19:16,320
compute the canonical is sort of science i mean

1287
01:19:16,530 --> 01:19:20,470
and it's hard to believe mean there's is any area

1288
01:19:20,550 --> 01:19:22,990
of applied analysis

1289
01:19:23,070 --> 01:19:25,410
the graphical models have

1290
01:19:27,670 --> 01:19:28,900
if not

1291
01:19:29,200 --> 01:19:31,460
then you should be

1292
01:19:31,470 --> 01:19:32,470
thinking about

1293
01:19:32,520 --> 01:19:34,540
applying the

1294
01:19:35,420 --> 01:19:38,660
it's really a very generic type of

1295
01:19:38,710 --> 01:19:40,570
mathematical tools to model

1296
01:19:40,580 --> 01:19:43,820
real problems

1297
01:19:43,890 --> 01:19:46,780
so just a few examples here

1298
01:19:48,030 --> 01:19:51,280
and physics i would say that probably

1299
01:19:51,280 --> 01:19:56,400
much of what we know about graphical models started lot physics along time about a

1300
01:19:56,400 --> 01:19:57,470
hundred years ago

1301
01:19:59,050 --> 01:20:01,660
people started to try to understand

1302
01:20:03,340 --> 01:20:06,510
the marker structure of matter

1303
01:20:06,550 --> 01:20:08,650
basically murders from

1304
01:20:08,900 --> 01:20:11,460
the internal structure of matter

1305
01:20:11,470 --> 01:20:15,910
in particular how you have things like for example phase transitions

1306
01:20:15,970 --> 01:20:16,840
i mean

1307
01:20:16,840 --> 01:20:20,340
how does one other can so that

1308
01:20:20,510 --> 01:20:23,280
one hundred eighty degrees

1309
01:20:24,010 --> 01:20:27,320
i mean things like that i mean how can you explain these

1310
01:20:27,380 --> 01:20:29,800
strange phenomenon that you know

1311
01:20:29,850 --> 01:20:32,780
they are guided by local molecular behavior

1312
01:20:33,650 --> 01:20:35,660
they have a very

1313
01:20:36,100 --> 01:20:38,420
very clear macro

1314
01:20:38,420 --> 01:20:40,620
we were not

1315
01:20:40,680 --> 01:20:41,530
so four

1316
01:20:41,540 --> 01:20:43,600
finding clusters in the data

1317
01:20:43,620 --> 01:20:46,020
based on finding peak

1318
01:20:47,210 --> 01:20:51,700
nonparametric density estimation estimate

1319
01:20:51,720 --> 01:20:53,120
and that is

1320
01:20:53,140 --> 01:21:00,510
this function here

1321
01:21:00,860 --> 01:21:04,980
and one that does one have to do with level sets

1322
01:21:05,070 --> 01:21:09,440
now look at another matter instead of looking at the level that look at finding

1323
01:21:11,500 --> 01:21:13,420
and so to the gradient

1324
01:21:13,470 --> 01:21:17,060
after this

1325
01:21:17,120 --> 01:21:21,180
and before going further i would like to correct the typo the is

1326
01:21:22,760 --> 01:21:24,860
you so

1327
01:21:24,870 --> 01:21:32,140
if is expressed on page alone in the north that this is what the

1328
01:21:32,160 --> 01:21:33,390
so the

1329
01:21:33,400 --> 01:21:34,720
well to get

1330
01:21:35,330 --> 01:21:37,530
if you going to have a

1331
01:21:37,540 --> 01:21:39,300
to be

1332
01:21:39,430 --> 01:21:44,560
what in france some of k

1333
01:21:45,540 --> 01:21:50,940
x minus x i would change

1334
01:21:55,940 --> 01:22:00,020
and so if you have this function square inside

1335
01:22:01,150 --> 01:22:03,350
what you get used

1336
01:22:03,430 --> 01:22:06,200
actually maybe within a

1337
01:22:06,210 --> 01:22:07,880
but this is what does matter because

1338
01:22:07,930 --> 01:22:09,350
but the gradient of

1339
01:22:09,370 --> 01:22:11,660
so to get

1340
01:22:11,940 --> 01:22:16,800
you need to be a functional successful

1341
01:22:16,880 --> 01:22:22,830
so the idea of this kind of is to

1342
01:22:22,880 --> 01:22:27,600
find the peaks of the distributions are to the points where the gradient is

1343
01:22:27,640 --> 01:22:32,510
by the way the exact also called valley of the web

1344
01:22:34,680 --> 01:22:35,830
the most of the time

1345
01:22:35,840 --> 01:22:37,910
present to find all the high

1346
01:22:38,060 --> 01:22:40,510
the local maxima

1347
01:22:40,590 --> 01:22:45,140
and do you take the gradient which is this expression here right there

1348
01:22:47,140 --> 01:22:48,870
and some the region

1349
01:22:48,880 --> 01:22:53,980
gradient equal zero

1350
01:22:54,190 --> 01:22:55,880
by the way

1351
01:22:55,900 --> 01:23:00,750
it is not trivial to find a mix of kernel density estimation so you can

1352
01:23:00,750 --> 01:23:02,930
find these things in growth form

1353
01:23:03,000 --> 01:23:08,380
but what you can do what you can do is to take this right inside

1354
01:23:08,380 --> 01:23:10,180
equated to zero

1355
01:23:10,200 --> 01:23:14,440
and then you notice that there is this part some of k problem stems x

1356
01:23:14,440 --> 01:23:16,360
minus x i

1357
01:23:16,400 --> 01:23:17,960
and should take all the time

1358
01:23:18,190 --> 01:23:20,670
context x and put them on the left

1359
01:23:20,710 --> 01:23:21,530
and the the

1360
01:23:21,780 --> 01:23:23,970
this exercise the right

1361
01:23:23,980 --> 01:23:29,650
and this could be just on matter because the constant in this case doesn't

1362
01:23:29,750 --> 01:23:34,390
and then you can expression here which is multiplied with

1363
01:23:34,440 --> 01:23:37,060
the cell k primes

1364
01:23:37,280 --> 01:23:39,180
it equal to

1365
01:23:39,860 --> 01:23:44,370
the sum of excise weighted by the escape hatch

1366
01:23:44,420 --> 01:23:47,160
so the equation question for finding x peaks

1367
01:23:47,170 --> 01:23:48,510
or the next

1368
01:23:52,520 --> 01:23:55,480
but x is bordered on the left and on the right so you can solve

1369
01:23:55,490 --> 01:23:59,560
it follows four then the best you can do is to actually follow the gradient

1370
01:23:59,560 --> 01:24:04,860
which means move x to the point of which is the expression on the right

1371
01:24:04,870 --> 01:24:11,250
and if you keep taking x and replacing with you eventually going

1372
01:24:11,290 --> 01:24:17,720
they i think this is a little more prone actually locally increasing in the ocean

1373
01:24:17,730 --> 01:24:18,940
and so on

1374
01:24:20,020 --> 01:24:25,460
and this is expressed in the simple algorithm below

1375
01:24:25,510 --> 01:24:28,510
which says that

1376
01:24:28,520 --> 01:24:31,640
before i start data points

1377
01:24:31,650 --> 01:24:37,660
and the iterate

1378
01:24:39,010 --> 01:24:44,490
i iterate the mean shift equation so replace x with which she

1379
01:24:44,540 --> 01:24:47,790
until convergence

1380
01:24:50,450 --> 01:24:52,530
and because

1381
01:24:52,540 --> 01:24:55,380
i'm increasing the value of

1382
01:24:55,400 --> 01:24:56,940
at least it means that no matter

1383
01:24:56,980 --> 01:25:01,060
last that winter and the people

1384
01:25:01,120 --> 01:25:05,950
and now i can cluster is because i was going to say everybody on the

1385
01:25:05,950 --> 01:25:09,800
point x i who converge this peak will be put in the same class every

1386
01:25:09,800 --> 01:25:15,390
points over here which we call the probabilistic was reported in the middle east

1387
01:25:15,410 --> 01:25:18,310
so this is what everyone wants know

1388
01:25:18,460 --> 01:25:20,760
so have to be so

1389
01:25:20,810 --> 01:25:25,160
clusters with one point of possible like

1390
01:25:25,170 --> 01:25:28,960
it is clear what it does

1391
01:25:28,980 --> 01:25:34,250
OK now we also that by the way

1392
01:25:34,270 --> 01:25:39,010
there is proof that it does converge so this iteration will converge to max because

1393
01:25:40,760 --> 01:25:44,920
it's all it's fixed but that's what we would call the fixed point iteration

1394
01:25:44,970 --> 01:25:47,010
and it is started in this case to

1395
01:25:47,940 --> 01:25:51,950
we don't know much about the rate of course in this case

1396
01:25:52,030 --> 01:25:56,670
how to you understand how to do it you realize that very expensive

1397
01:25:56,770 --> 01:26:01,310
because these things involves all the elements

1398
01:26:01,360 --> 01:26:05,980
computing one mean shift volatility that was because some of k properties of

1399
01:26:07,650 --> 01:26:09,460
and of course you can

1400
01:26:09,470 --> 01:26:11,270
we can play

1401
01:26:11,280 --> 01:26:17,990
the three mentioned before which is not considered all the data points to consider only

1402
01:26:17,990 --> 01:26:22,270
the points which are in the neighborhood of x

1403
01:26:22,290 --> 01:26:23,930
that i'm looking at

1404
01:26:23,940 --> 01:26:25,710
which make it starts

1405
01:26:25,720 --> 01:26:29,600
but still i have to reiterate for every point in the data so maybe not

1406
01:26:29,700 --> 01:26:35,200
quadratic but it's still very very expensive

1407
01:26:35,210 --> 01:26:39,650
how deal with the idea and what

1408
01:26:39,670 --> 01:26:42,440
some people did was to make it faster

1409
01:26:42,530 --> 01:26:46,790
by eliminating most of the point

1410
01:26:48,030 --> 01:26:49,910
comment each and man the

1411
01:26:50,970 --> 01:26:57,320
there is nothing in the whole that doing the mean shift iteration for each point

1412
01:26:57,320 --> 01:26:57,890
in the

1413
01:26:57,950 --> 01:27:00,430
they select to and few

1414
01:27:00,450 --> 01:27:02,710
q four which

1415
01:27:02,760 --> 01:27:04,880
they say a common data

1416
01:27:07,500 --> 01:27:12,720
if you put serious around these cues selected points to cover all the range of

1417
01:27:12,730 --> 01:27:14,390
the data

1418
01:27:14,400 --> 01:27:19,570
this is really obvious in in one dimension because i just uploaded intervals and then

1419
01:27:19,580 --> 01:27:25,410
pick a point in the middle of each interval it's not as easy

1420
01:27:25,680 --> 01:27:31,450
in high dimensions because the exponential number of points to cover the whole data and

1421
01:27:31,450 --> 01:27:34,600
so another little question about

1422
01:27:34,620 --> 01:27:37,230
i how

1423
01:27:37,250 --> 01:27:40,170
how to find these initial set of points

1424
01:27:40,270 --> 01:27:45,910
however it however is algorithm is used and so people succeeded higher dimensions to do

1425
01:27:45,940 --> 01:27:47,680
to find by some

1426
01:27:47,830 --> 01:27:50,580
maybe heuristic methods

1427
01:27:50,590 --> 01:27:54,650
tools to find for company that was actually one of the ways of doing it

1428
01:27:54,650 --> 01:27:59,620
is to use them in order to cluster around that time and they discussed in

1429
01:27:59,640 --> 01:28:04,510
the first lecture which finds point as far as possible and so by

1430
01:28:04,530 --> 01:28:07,640
taking more and more sensors that algorithm

1431
01:28:07,660 --> 01:28:11,750
you control the distance the maximum distance between points

1432
01:28:11,760 --> 01:28:19,080
and when the maximum distance falls below a threshold then you can stop you've got

1433
01:28:19,270 --> 01:28:22,850
the question is how many points are used and

1434
01:28:22,870 --> 01:28:25,230
and how much computation

1435
01:28:25,400 --> 01:28:29,960
OK so you select a set of points at its diverse in

1436
01:28:30,000 --> 01:28:30,940
that we had

1437
01:28:30,940 --> 01:28:35,100
all physics

1438
01:28:35,110 --> 01:28:37,190
of the nineteenth century

1439
01:28:37,200 --> 01:28:39,940
and idea is called classical physics

1440
01:28:39,950 --> 01:28:44,680
examples are newtonian mechanics which we dealt with this whole term

1441
01:28:45,710 --> 01:28:47,840
electricity and magnetism

1442
01:28:47,860 --> 01:28:49,590
which you will

1443
01:28:49,640 --> 01:28:52,320
and counter the next term

1444
01:28:52,320 --> 01:28:54,430
the only part of this century

1445
01:28:54,430 --> 01:28:56,590
when we learn about the composition of

1446
01:28:57,480 --> 01:29:00,280
it became clear that classical physics

1447
01:29:00,340 --> 01:29:03,370
did not work on the very small scale

1448
01:29:03,440 --> 01:29:05,990
of the atoms the size of an atom

1449
01:29:06,050 --> 01:29:08,620
only ten to the minus ten meters

1450
01:29:08,620 --> 01:29:11,290
if you take two hundred fifty million of them

1451
01:29:11,300 --> 01:29:13,020
you line them up

1452
01:29:13,060 --> 01:29:15,590
that's only one it's

1453
01:29:15,630 --> 01:29:17,600
in nineteen eleven

1454
01:29:17,620 --> 01:29:18,540
the english

1455
01:29:18,550 --> 01:29:20,550
physicist rutherford

1456
01:29:20,590 --> 01:29:22,840
demonstrated that almost

1457
01:29:22,910 --> 01:29:25,660
all the mass of an atom is concentrated

1458
01:29:25,660 --> 01:29:29,300
in extreme small hole in the centre of the atom

1459
01:29:29,300 --> 01:29:32,480
recall that the nucleus is positively charged

1460
01:29:32,490 --> 01:29:35,740
and the electrons which are negatively charged

1461
01:29:36,430 --> 01:29:39,490
i orbit around the nucleus

1462
01:29:39,510 --> 01:29:43,020
and the typical distances from the nucleus to the electrons

1463
01:29:43,040 --> 01:29:46,730
it is about one hundred thousand times larger than the size

1464
01:29:46,740 --> 01:29:47,880
of the nucleus

1465
01:29:50,070 --> 01:29:55,100
as early as nineteen twenty four named the proton chadwick

1466
01:29:55,130 --> 01:29:56,760
discovered the neutron

1467
01:29:56,820 --> 01:30:01,350
in nineteen thirty two for which he received the nobel prize

1468
01:30:01,430 --> 01:30:03,770
now let's imagine that this lecture hall

1469
01:30:03,820 --> 01:30:05,730
it's another

1470
01:30:05,770 --> 01:30:09,460
and the size of an atom is defined by the orbits the outer orbits of

1471
01:30:09,460 --> 01:30:12,010
the electron

1472
01:30:12,050 --> 01:30:14,320
if i scale it properly now in this

1473
01:30:14,370 --> 01:30:16,960
ratio hundred thousand to one

1474
01:30:17,010 --> 01:30:19,130
and the size of the nucleus

1475
01:30:19,150 --> 01:30:21,100
will be even smaller

1476
01:30:21,100 --> 01:30:23,460
then the grain of sand

1477
01:30:23,510 --> 01:30:25,800
and it just so happens that yesterday

1478
01:30:25,850 --> 01:30:29,290
i went to plum island work for three hours on the beach

1479
01:30:29,320 --> 01:30:32,490
and i ended up with some sent in my pocket

1480
01:30:33,650 --> 01:30:36,460
i will donate to u one proton

1481
01:30:36,600 --> 01:30:38,660
actually hold on to it

1482
01:30:38,700 --> 01:30:41,920
what two protons that's too generous

1483
01:30:41,980 --> 01:30:43,140
to keep it there

1484
01:30:43,290 --> 01:30:45,010
one protein

1485
01:30:45,060 --> 01:30:48,480
and there would be an electron then also anywhere there

1486
01:30:48,540 --> 01:30:51,760
the other walls going around like mad in orbit

1487
01:30:51,790 --> 01:30:53,570
and that would then be

1488
01:30:53,630 --> 01:30:56,080
how hydrogen need to think about

1489
01:30:56,100 --> 01:30:58,160
one atom is an atom

1490
01:30:58,230 --> 01:30:59,360
is all

1491
01:31:01,600 --> 01:31:02,600
and i

1492
01:31:02,660 --> 01:31:03,630
all right

1493
01:31:04,570 --> 01:31:06,940
you think of yourself of being something

1494
01:31:06,940 --> 01:31:08,880
but we are nothing

1495
01:31:08,910 --> 01:31:12,350
you can ask yourself the question if you all vacuum

1496
01:31:12,390 --> 01:31:16,140
why is it then that i can move my hand

1497
01:31:16,230 --> 01:31:17,630
not other hand

1498
01:31:17,630 --> 01:31:21,250
like ghosts can walk through a wall

1499
01:31:21,290 --> 01:31:23,380
that's not so easy to answer

1500
01:31:23,450 --> 01:31:24,380
in fact

1501
01:31:24,390 --> 01:31:28,040
you can answer it was classical physics and i will not return to that but

1502
01:31:28,040 --> 01:31:30,780
you i all vacuum

1503
01:31:30,790 --> 01:31:32,510
according to maxwell

1504
01:31:32,540 --> 01:31:35,720
equations maxwell's laws of electricity and magnetism

1505
01:31:35,730 --> 01:31:37,510
an electron

1506
01:31:37,510 --> 01:31:41,380
because of the attractive force of the proton what's bile into the proton in a

1507
01:31:41,380 --> 01:31:43,040
minute fraction

1508
01:31:43,060 --> 01:31:46,140
of the second so adams could not exist

1509
01:31:46,160 --> 01:31:47,540
we know that's not true

1510
01:31:47,540 --> 01:31:49,380
we know that atoms do exist

1511
01:31:49,380 --> 01:31:51,100
so that created the problem

1512
01:31:51,100 --> 01:31:52,720
four physics and it was

1513
01:31:52,760 --> 01:31:55,200
danish physicist niels bohr

1514
01:31:55,260 --> 01:31:57,510
before in nineteen thirteen

1515
01:31:59,160 --> 01:32:03,230
that electrons move around the nucleus in well defined orbits

1516
01:32:03,250 --> 01:32:06,390
which i distinctly separated from each other

1517
01:32:06,410 --> 01:32:09,760
and that the spiraling of of the electrons into the nucleus does not occur for

1518
01:32:09,760 --> 01:32:10,640
two reasons

1519
01:32:10,700 --> 01:32:13,290
that an electron can not exist in between

1520
01:32:14,070 --> 01:32:15,820
allowed orbits conjunction

1521
01:32:15,850 --> 01:32:17,500
from one orbit to another

1522
01:32:17,600 --> 01:32:19,010
but it can not exist

1523
01:32:22,110 --> 01:32:23,850
of course suggestion

1524
01:32:23,910 --> 01:32:25,690
also shaking

1525
01:32:25,700 --> 01:32:29,580
because it would also implied the planet that goes around the sun

1526
01:32:29,600 --> 01:32:32,600
canada ordered to song just any distance

1527
01:32:32,610 --> 01:32:35,080
you could move it just a trifle

1528
01:32:35,130 --> 01:32:39,190
in or trifle far out it would also require

1529
01:32:39,220 --> 01:32:41,290
discrete orbits

1530
01:32:41,290 --> 01:32:42,530
it would also mean

1531
01:32:43,260 --> 01:32:45,380
if you had a tennis ball

1532
01:32:45,440 --> 01:32:47,610
and you would balance the tennis ball

1533
01:32:47,650 --> 01:32:48,750
up and down

1534
01:32:48,800 --> 01:32:50,120
the tennis ball

1535
01:32:50,130 --> 01:32:52,700
could not reach just any level

1536
01:32:52,750 --> 01:32:55,510
above the ground but it would only be discrete levels

1537
01:32:55,560 --> 01:32:57,380
that is very much against

1538
01:32:57,390 --> 01:33:01,170
our intuition we'd like to think that when you bounce a tennis ball

1539
01:33:01,210 --> 01:33:02,260
that it can reach

1540
01:33:02,280 --> 01:33:05,270
any level that you want to give it just a little bit more energy

1541
01:33:05,330 --> 01:33:06,300
and it will go

1542
01:33:06,310 --> 01:33:09,180
a little higher that according to quantum mechanics

1543
01:33:09,190 --> 01:33:10,710
would not be

1544
01:33:14,320 --> 01:33:16,800
now all this seems rather bizarre

1545
01:33:16,920 --> 01:33:20,430
as it goes against our daily experiences

1546
01:33:20,460 --> 01:33:25,630
but before we dismiss the idea of quantisation quantisation comes in when you talk about

1547
01:33:25,630 --> 01:33:27,670
discrete orbits

1548
01:33:27,760 --> 01:33:28,840
you have to

1549
01:33:28,940 --> 01:33:30,620
i realize that the

1550
01:33:30,680 --> 01:33:33,940
differences in the allowed heights of the tennis ball

1551
01:33:33,990 --> 01:33:38,500
and the differences between the allowed orbits of the planets around the sun

1552
01:33:38,520 --> 01:33:41,080
would be so infinitesimal this small

1553
01:33:41,090 --> 01:33:42,740
but we may never be able

1554
01:33:42,750 --> 01:33:46,320
to measure not always quantum mechanics we plays no role

1555
01:33:46,340 --> 01:33:49,270
in our macroscopic world

1556
01:33:49,320 --> 01:33:52,500
at very very small compared to tennis balls

1557
01:33:52,510 --> 01:33:54,370
and the quantisation effects

1558
01:33:54,380 --> 01:33:58,280
a much larger in the sub microscopic world of electrons

1559
01:33:58,320 --> 01:34:02,820
and adams then in our familiar world baseball's pots and pans

1560
01:34:02,870 --> 01:34:04,880
and plants

1561
01:34:04,940 --> 01:34:08,930
before we continue i would like to repeat to you one of the corner

1562
01:34:08,940 --> 01:34:11,360
stones of quantum mechanics

1563
01:34:11,400 --> 01:34:13,990
and it says that the electrons in atoms

1564
01:34:14,010 --> 01:34:15,880
can only exist

1565
01:34:15,930 --> 01:34:18,090
and well defined energy levels

1566
01:34:18,110 --> 01:34:20,200
i think of them as being orbits

1567
01:34:20,200 --> 01:34:24,840
around the nucleus and they can not exist in between

1568
01:34:24,860 --> 01:34:27,110
now when i e the substance

1569
01:34:27,120 --> 01:34:32,200
the electrons in the atoms can jump from in orbits two allowed out of words

1570
01:34:32,210 --> 01:34:34,180
and when they do so they can leave

1571
01:34:34,190 --> 01:34:37,700
a whole opening an empty space in the in orbit

1572
01:34:37,820 --> 01:34:39,920
but later on they can fall back

1573
01:34:40,000 --> 01:34:42,210
to fill the opening day can occupy

1574
01:34:42,240 --> 01:34:44,710
that place again

1575
01:34:44,810 --> 01:34:47,930
and when i keep this substance

1576
01:34:48,010 --> 01:34:51,060
there is some kind of musical chairs game going on

1577
01:34:51,070 --> 01:34:54,740
electrons will go to outer orbits they may spend some time

1578
01:34:54,760 --> 01:34:56,320
and then they may fall

1579
01:34:57,260 --> 01:35:02,610
lower orbits to in all its

1580
01:35:02,650 --> 01:35:04,520
you see here face

1581
01:35:04,560 --> 01:35:06,450
very precious face

1582
01:35:06,460 --> 01:35:08,950
and when i pick up this face

1583
01:35:08,960 --> 01:35:10,320
i have to do work

1584
01:35:10,360 --> 01:35:12,500
bring it further away from the center

1585
01:35:12,550 --> 01:35:13,420
of the earth

1586
01:35:13,430 --> 01:35:15,870
now is the energy lost in all

1587
01:35:15,920 --> 01:35:17,870
i could drop to face

1588
01:35:17,930 --> 01:35:22,010
and it will pick up kinetic energy i'll get that energy back gravitational potential energy

1589
01:35:22,010 --> 01:35:23,330
will be converted

1590
01:35:23,340 --> 01:35:24,940
two kinetic energy

1591
01:35:24,950 --> 01:35:26,610
it will crash to pieces

1592
01:35:26,620 --> 01:35:28,380
and it will generate some heat

1593
01:35:28,430 --> 01:35:30,780
effective breaking itself of this phase

1594
01:35:30,810 --> 01:35:31,570
will take

1595
01:35:31,570 --> 01:35:34,000
some energy

1596
01:35:34,010 --> 01:35:36,750
in a similar way

1597
01:35:36,900 --> 01:35:41,380
energy that you put into electrons when you bring into outer orbits is retrieved when

1598
01:35:41,390 --> 01:35:43,450
electrons fall back

1599
01:35:43,460 --> 01:35:44,990
that is apparent

1600
01:35:45,000 --> 01:35:46,610
dropping his face

1601
01:35:46,690 --> 01:35:50,580
getting the work back that i put it

1602
01:35:50,630 --> 01:35:54,200
i wouldn't be nice thing to do to this five hundred year-old face

1603
01:35:54,240 --> 01:35:57,800
but as far as i'm concerned perfectly reasonable to do with it so he ne

1604
01:35:59,180 --> 01:36:00,390
let that go

1605
01:36:00,400 --> 01:36:01,570
and the energy

1606
01:36:01,580 --> 01:36:02,760
we'll come out

1607
01:36:02,760 --> 01:36:07,930
in the form of heat and also in the form of perhaps noise

1608
01:36:07,970 --> 01:36:10,110
when electrons

1609
01:36:10,120 --> 01:36:11,750
four from an outer

1610
01:36:11,750 --> 01:36:13,650
it's just somebody

1611
01:36:13,730 --> 01:36:15,260
had a smart idea

1612
01:36:15,280 --> 01:36:19,740
in choosing the wavelet basis and inverting it

1613
01:36:19,760 --> 01:36:23,670
it has a nice inverse actually you can see why it has nice inverse

1614
01:36:23,710 --> 01:36:28,700
do you see any property of these eight basis vectors

1615
01:36:28,750 --> 01:36:31,730
well i'm only written five of them but if you

1616
01:36:31,740 --> 01:36:35,670
see that property for those five you'll see it for the three

1617
01:36:35,670 --> 01:36:39,920
remaining what if i give you those a vectors

1618
01:36:39,970 --> 01:36:43,070
and ask what nice property

1619
01:36:43,080 --> 01:36:47,990
well you'd say first they're all ones and minus ones and zeros

1620
01:36:48,010 --> 01:36:54,630
so every multiplication is very fast using just in binary

1621
01:36:54,940 --> 01:37:00,180
but what's the other great property those vectors in bayesian

1622
01:37:00,200 --> 01:37:01,940
so of course

1623
01:37:01,960 --> 01:37:06,390
when i think about basis one nice property i don't have to have it but

1624
01:37:06,390 --> 01:37:09,220
i'm happy if there is

1625
01:37:09,270 --> 01:37:11,290
there are five

1626
01:37:11,300 --> 01:37:14,270
if the basis vectors are orthogonal then

1627
01:37:14,280 --> 01:37:18,280
i mean good shape and these are you see that take the dot product that

1628
01:37:19,740 --> 01:37:23,370
you get four plus one thousand four minus one get zero

1629
01:37:23,380 --> 01:37:25,510
take the dot product that with that

1630
01:37:25,530 --> 01:37:28,300
you get two plus one to minus one

1631
01:37:28,320 --> 01:37:33,090
or the dot product that that two plus one in two minds

1632
01:37:33,100 --> 01:37:35,260
you can easily check that

1633
01:37:35,280 --> 01:37:38,280
orthogonal basis it's not

1634
01:37:40,620 --> 01:37:42,510
i could to fix it

1635
01:37:42,580 --> 01:37:48,460
i should divided by the way to make community was supposed to do that

1636
01:37:48,480 --> 01:37:51,330
so somewhere here i've got to account for the

1637
01:37:51,340 --> 01:37:52,590
in fact

1638
01:37:52,610 --> 01:37:56,830
this length square root of a that has links were to four

1639
01:37:56,920 --> 01:37:59,330
has links were the two

1640
01:37:59,340 --> 01:38:03,150
but that's just a constant factor that easy to

1641
01:38:03,200 --> 01:38:05,020
so suppose we've done that

1642
01:38:07,070 --> 01:38:09,840
tell me what's w inverse

1643
01:38:09,860 --> 01:38:11,300
that's what

1644
01:38:11,320 --> 01:38:13,130
chapter four

1645
01:38:13,190 --> 01:38:16,120
section four point four was about

1646
01:38:16,120 --> 01:38:17,850
if we have

1647
01:38:17,860 --> 01:38:22,940
the orthonormal columns what then the inverse is the same as

1648
01:38:22,990 --> 01:38:24,770
the transpose

1649
01:38:24,820 --> 01:38:29,430
so if we have a fast way to multiply by w which we do

1650
01:38:29,440 --> 01:38:32,550
the inverse is going to look just the same and will have a fast way

1651
01:38:32,550 --> 01:38:34,680
to do w inverse so

1652
01:38:35,710 --> 01:38:41,610
of the wavelet bases passes this requirement for facts

1653
01:38:41,650 --> 01:38:43,720
we can use it fast

1654
01:38:43,740 --> 01:38:46,960
but there's the second requirement is isn't any good

1655
01:38:47,010 --> 01:38:51,170
because the very fastest thing we could do would be not to change the basis

1656
01:38:51,170 --> 01:38:52,610
of all

1657
01:38:52,640 --> 01:38:57,380
right that's thing would be okay stay with the standard basis stay with eight pixel

1658
01:38:58,510 --> 01:39:01,080
but that was poor

1659
01:39:01,090 --> 01:39:03,760
from compression point of view right

1660
01:39:03,780 --> 01:39:08,340
those eight pixel values if i just took those eight numbers

1661
01:39:08,340 --> 01:39:10,470
i can throw some of those away

1662
01:39:10,520 --> 01:39:12,570
if i throw

1663
01:39:12,590 --> 01:39:14,940
away ninety percent

1664
01:39:15,000 --> 01:39:16,990
if i compress ten to one

1665
01:39:17,000 --> 01:39:23,000
throwaway ninety percent of my pixel values while my pictures just gone dark

1666
01:39:24,630 --> 01:39:29,330
the basis of was good the wavelet basis are the fourier basis if i throw

1667
01:39:31,280 --> 01:39:36,080
c five c six c seven c eight all i'm throwing away is a little

1668
01:39:37,350 --> 01:39:41,330
there are probably there in very small amounts

1669
01:39:41,340 --> 01:39:45,660
so the second property that we need is good compression

1670
01:39:46,290 --> 01:39:50,190
so first it has to be fast and secondly

1671
01:39:50,240 --> 01:39:52,630
has to be

1672
01:39:52,670 --> 01:39:55,170
a few basis vectors

1673
01:39:56,570 --> 01:39:58,180
come close to the sea

1674
01:39:59,290 --> 01:40:00,940
a few

1675
01:40:01,050 --> 01:40:02,440
if few is not

1676
01:40:02,540 --> 01:40:04,250
can i write it that way

1677
01:40:04,320 --> 01:40:10,460
a few basis vectors are enough to

1678
01:40:11,770 --> 01:40:12,680
the image

1679
01:40:12,690 --> 01:40:17,940
just exactly as on video of these eighteen o six lectures

1680
01:40:18,390 --> 01:40:23,220
i don't know what the compression rate as well as david who

1681
01:40:23,300 --> 01:40:28,020
who does the compression and by the way i trying to get

1682
01:40:28,040 --> 01:40:29,530
the lectures

1683
01:40:29,540 --> 01:40:34,050
that are relevant for the quiz out onto the web

1684
01:40:35,800 --> 01:40:37,800
so i'll send a message today

1685
01:40:39,570 --> 01:40:44,920
the point so he's using the fourier basis because the JPA so jerry pate two

1686
01:40:46,180 --> 01:40:49,780
which will be the next standard for image compression

1687
01:40:49,800 --> 01:40:51,970
it will include way

1688
01:40:52,030 --> 01:40:55,220
so i mean you're you're actually getting kind of up to date

1689
01:40:55,880 --> 01:40:59,150
picture of where this

1690
01:40:59,200 --> 01:41:03,610
the world of signal and image processing is

1691
01:41:05,200 --> 01:41:08,110
four a is what everybody knew

1692
01:41:08,150 --> 01:41:12,490
and what people automatically used and the new

1693
01:41:12,490 --> 01:41:15,030
the new one is wavelets

1694
01:41:15,130 --> 01:41:18,110
where this is the simplest set of wavelet

1695
01:41:19,130 --> 01:41:23,240
this is the one that the FBI uses by the way the FBI users are

1696
01:41:23,240 --> 01:41:29,630
more more smoother wavelet instead of jumping from one minus one it's smooth

1697
01:41:29,650 --> 01:41:31,050
cut off

1698
01:41:33,780 --> 01:41:37,380
that's what will be an injury-plagued two thousand

1699
01:41:37,400 --> 01:41:40,880
OK so that's the application now let me come

1700
01:41:40,900 --> 01:41:42,130
to my

1701
01:41:42,170 --> 01:41:48,150
the mass the linear algebra part of the lecture

1702
01:41:48,200 --> 01:41:52,380
well we've actually seen a change of basis

1703
01:41:52,400 --> 01:41:54,700
so let let me just review that

1704
01:41:54,720 --> 01:41:56,800
change of basis idea

1705
01:41:56,820 --> 01:42:01,380
and then the and then the transformation to a matrix OK so

1706
01:42:01,400 --> 01:42:03,130
so this

1707
01:42:03,180 --> 01:42:07,740
i i hope you see that these applications are really big

1708
01:42:08,510 --> 01:42:09,340
i have to

1709
01:42:09,360 --> 01:42:11,630
talk a little about change of basis

1710
01:42:11,650 --> 01:42:14,840
and a little bit about that the matrix

1711
01:42:19,030 --> 01:42:24,180
so change of basis

1712
01:42:25,610 --> 01:42:27,360
forget that

1713
01:42:29,220 --> 01:42:30,670
i have

1714
01:42:30,720 --> 01:42:35,170
i have i have my vector

1715
01:42:35,180 --> 01:42:37,240
in one basis

1716
01:42:37,260 --> 01:42:40,990
and i want to change to a different what actually use it for the wavelet

1717
01:42:42,050 --> 01:42:46,940
so i need that the main idea that the matrix w

1718
01:42:46,940 --> 01:42:50,650
the columns of w

1719
01:42:53,740 --> 01:42:55,450
the the

1720
01:42:55,450 --> 01:42:58,530
new basis vectors

1721
01:42:58,630 --> 01:43:08,440
then the change of basis involved just as the their w four

1722
01:43:08,450 --> 01:43:10,700
so we have the vector

1723
01:43:11,880 --> 01:43:13,440
say x

1724
01:43:13,450 --> 01:43:14,970
in the old basis

1725
01:43:14,970 --> 01:43:22,360
and we'll that converts to a vector space

1726
01:43:22,420 --> 01:43:24,900
no basis

1727
01:43:24,900 --> 01:43:26,650
you basically do

1728
01:43:27,530 --> 01:43:28,840
four things

1729
01:43:28,860 --> 01:43:33,000
before you even think about doing you should

1730
01:43:33,010 --> 01:43:36,460
try the nearest neighbour is set not continuous

1731
01:43:36,940 --> 01:43:39,340
probabilistic methods the

1732
01:43:39,440 --> 01:43:41,630
in my research

1733
01:43:41,690 --> 01:43:44,780
you should try naive bayes

1734
01:43:44,780 --> 01:43:47,130
you should try logistic regression

1735
01:43:47,300 --> 01:43:49,900
and then you should probably try overfitting

1736
01:43:50,010 --> 01:43:52,900
fisher discriminant or

1737
01:43:55,130 --> 01:43:57,940
is continuous on the other hand gaussian

1738
01:43:58,260 --> 01:44:05,980
so here's the interaction with rationalism always says the conditional probability of y

1739
01:44:06,000 --> 01:44:08,960
the class given the input x is

1740
01:44:08,980 --> 01:44:12,440
this log linear function this of mass

1741
01:44:12,460 --> 01:44:16,860
and i don't care about what the distribution of the

1742
01:44:17,210 --> 01:44:19,090
this is joint

1743
01:44:19,110 --> 01:44:20,670
we want to do

1744
01:44:20,780 --> 01:44:23,440
also say anything about the marginal distribution

1745
01:44:24,800 --> 01:44:30,650
it's not for us but that's OK train small

1746
01:44:31,400 --> 01:44:33,480
the previous suggestions

1747
01:44:33,500 --> 01:44:34,460
the training halls

1748
01:44:34,480 --> 01:44:36,760
that's why this isn't even though

1749
01:44:36,800 --> 01:44:40,340
full model to do so i think about that

1750
01:44:41,510 --> 01:44:44,530
we optimize called the conditional

1751
01:44:44,530 --> 01:44:49,420
so a new loss function now is the original one

1752
01:44:49,440 --> 01:44:53,000
the probability of class given him

1753
01:44:53,050 --> 01:45:00,440
this is the objective function use your training time and this is very different than

1754
01:45:01,460 --> 01:45:04,320
maximum likelihood penalized maximum one

1755
01:45:04,340 --> 01:45:09,480
so this is a very important point logistic regression and naive bayes

1756
01:45:09,500 --> 01:45:12,900
are actually identical models in terms

1757
01:45:14,480 --> 01:45:17,960
they specify exactly the same hypothesis class

1758
01:45:17,980 --> 01:45:24,530
but the difference is the naive bayes is estimated using maximum likelihood and logistic regression

1759
01:45:24,530 --> 01:45:26,690
estimation using conditional

1760
01:45:26,690 --> 01:45:29,090
so the difference is all in your loss

1761
01:45:29,150 --> 01:45:32,800
remember there were these three pieces representation of all

1762
01:45:32,820 --> 01:45:35,630
the hypothesis class and the loss function

1763
01:45:35,650 --> 01:45:37,980
here representations

1764
01:45:38,000 --> 01:45:42,050
we're assuming fixed by class between a and b bayesian is not just about exactly

1765
01:45:42,050 --> 01:45:47,280
the same but the loss function loss function is what conditional probability and in a

1766
01:45:47,630 --> 01:45:50,980
also was actually like

1767
01:45:51,000 --> 01:45:54,970
OK so but that we can take the same approach write down the small right

1768
01:45:54,970 --> 01:45:56,570
down the likelihood function and we take

1769
01:45:56,670 --> 01:46:01,260
the derivatives of the likelihood function with respect to each

1770
01:46:01,280 --> 01:46:05,000
so this is also streamline compilation but directed

1771
01:46:05,010 --> 01:46:12,510
they come here is very interesting for the challenge

1772
01:46:12,510 --> 01:46:18,760
this is this is the derivative of the loss function with respect to the i

1773
01:46:18,780 --> 01:46:25,550
so the parameters for how much weight on features i when we're designing

1774
01:46:27,070 --> 01:46:31,380
this is the feature

1775
01:46:31,400 --> 01:46:32,610
the gradient of

1776
01:46:32,630 --> 01:46:35,210
it's the difference between

1777
01:46:35,240 --> 01:46:37,170
the true

1778
01:46:38,380 --> 01:46:42,130
the true answer for the example

1779
01:46:42,150 --> 01:46:44,740
in our model's predictions

1780
01:46:44,760 --> 01:46:46,940
so the answer is

1781
01:46:48,000 --> 01:46:49,960
it has one in

1782
01:46:49,980 --> 01:46:54,400
it has one of the key position is class labels k is zero so this

1783
01:46:54,440 --> 01:46:59,090
true vector is zero or one in the correct position

1784
01:46:59,130 --> 01:47:04,360
and this is our model's prediction which is one of the world's one point

1785
01:47:05,530 --> 01:47:09,260
one point eight one point

1786
01:47:09,280 --> 01:47:14,400
so i think the difference between the correct vector which is zero everywhere except one

1787
01:47:14,420 --> 01:47:16,570
placed into predictive factor

1788
01:47:16,570 --> 01:47:19,260
some of you are somewhat yahoo

1789
01:47:19,270 --> 01:47:21,920
chief data officer

1790
01:47:21,970 --> 01:47:23,900
executive vice president of

1791
01:47:23,980 --> 01:47:27,590
research and strategic data solutions

1792
01:47:27,630 --> 01:47:32,790
topic of research while my general areas data mining i'm not

1793
01:47:32,810 --> 01:47:38,000
very active research these days although we're trying to do with your research and invented

1794
01:47:38,000 --> 01:47:40,540
new sciences underlying the internet

1795
01:47:40,590 --> 01:47:45,160
which is a much bigger topics and just data mining but we're doing what

1796
01:47:47,750 --> 01:47:49,190
in your talk you

1797
01:47:51,100 --> 01:47:52,810
distinction between

1798
01:47:52,860 --> 01:47:56,610
a possible distinction between researchers and

1799
01:47:56,630 --> 01:48:01,410
engineers in the data mining community so you research or in g or

1800
01:48:01,460 --> 01:48:03,730
a scientific businessman

1801
01:48:03,780 --> 01:48:07,350
scientific business and i like that term a lot to the first time i hear

1802
01:48:07,950 --> 01:48:12,870
i guess some definitely used to be a researcher i

1803
01:48:12,920 --> 01:48:15,130
started out as an engineer and

1804
01:48:15,140 --> 01:48:17,390
today i guess scientific businessman

1805
01:48:17,410 --> 01:48:21,030
probably spend more of my time on business and science these days but it's a

1806
01:48:21,030 --> 01:48:25,700
balance between the two

1807
01:48:27,150 --> 01:48:36,150
as we know yahoo has lots of data huge quantities

1808
01:48:36,160 --> 01:48:41,090
what is your opinion on privacy policy

1809
01:48:42,480 --> 01:48:44,750
privacy is very

1810
01:48:44,760 --> 01:48:46,860
important topic for yahoo

1811
01:48:46,980 --> 01:48:49,140
in fact if anything we are

1812
01:48:49,340 --> 01:48:52,300
traditionally more conservative than

1813
01:48:52,310 --> 01:48:54,750
most of those in this area we

1814
01:48:54,800 --> 01:48:57,960
i tried to balance

1815
01:48:58,010 --> 01:49:03,840
between the user experience and the monetization of the how much use the data to

1816
01:49:03,840 --> 01:49:05,200
do targeting

1817
01:49:05,210 --> 01:49:06,330
very carefully

1818
01:49:06,340 --> 01:49:12,330
and we often draw the line if anything to conservatively could make

1819
01:49:12,440 --> 01:49:14,530
ten times the revenue tomorrow

1820
01:49:14,540 --> 01:49:15,760
simply by

1821
01:49:15,770 --> 01:49:17,550
serving ten times more at

1822
01:49:17,610 --> 01:49:21,380
we choose not to do that because what we care about for the long term

1823
01:49:21,420 --> 01:49:22,510
is the value

1824
01:49:22,520 --> 01:49:29,100
of the long term which is basically represented primarily in the trust that users have

1825
01:49:29,360 --> 01:49:34,000
and in fact who is the safe place to be a place to respect your

1826
01:49:34,000 --> 01:49:37,510
privacy and your anonymity when you choose to

1827
01:49:37,560 --> 01:49:39,590
so as an example of that when

1828
01:49:39,610 --> 01:49:43,990
somebody has logged into your versus like that we treat them as two completely different

1829
01:49:44,780 --> 01:49:48,270
because they make the fundamental assumption that you're logged out reason

1830
01:49:48,470 --> 01:49:51,380
we also do not track any information

1831
01:49:51,440 --> 01:49:54,900
with the idea that can be tracked back to you

1832
01:49:54,920 --> 01:49:57,200
two personally identifiable information

1833
01:49:57,260 --> 01:50:02,460
and most recently we also know that policy for minimisation of search data

1834
01:50:02,470 --> 01:50:04,790
which actually we do it

1835
01:50:05,240 --> 01:50:11,380
the earliest and industry we minimize data after thirteen months others who are eighteen months

1836
01:50:11,380 --> 01:50:14,020
or longer

1837
01:50:14,030 --> 01:50:15,300
so there

1838
01:50:15,380 --> 01:50:17,230
maybe i could

1839
01:50:17,720 --> 01:50:19,600
because of the question

1840
01:50:20,770 --> 01:50:22,240
who is

1841
01:50:22,250 --> 01:50:23,750
that big

1842
01:50:24,430 --> 01:50:28,580
impose mustapha question what exactly it is you who what is it will be coming

1843
01:50:28,890 --> 01:50:32,740
on the largest scale

1844
01:50:32,750 --> 01:50:36,490
well i mean what you know is something that evolved over the years i mean

1845
01:50:36,490 --> 01:50:39,260
you started out as a directory of the web

1846
01:50:39,370 --> 01:50:41,810
from there it

1847
01:50:41,840 --> 01:50:46,310
added some services that we felt were essential to consumers like email

1848
01:50:46,360 --> 01:50:47,810
if the messenger

1849
01:50:48,310 --> 01:50:54,420
finance news and so forth and all of these properties attracted

1850
01:50:54,460 --> 01:50:57,670
millions hundreds of millions in some cases of users

1851
01:50:57,810 --> 01:51:03,320
it's basically a place to find

1852
01:51:03,360 --> 01:51:07,630
all the things that we think are applicable or are interesting to

1853
01:51:07,650 --> 01:51:09,710
people out there

1854
01:51:09,720 --> 01:51:17,260
including place to actually find your way on the internet through our yahoo search

1855
01:51:17,280 --> 01:51:21,530
academia and industry

1856
01:51:21,580 --> 01:51:23,230
do they go hand in hand

1857
01:51:25,250 --> 01:51:39,860
OK concerning academia and industry

1858
01:51:39,870 --> 01:51:41,880
what is it

1859
01:51:41,900 --> 01:51:46,080
it would be

1860
01:51:46,090 --> 01:51:51,180
i could imagine in this you to go hand-in-hand and how do you see

1861
01:51:51,190 --> 01:51:52,830
from the yahoo

1862
01:51:52,840 --> 01:51:58,600
consumer point of view the bridge between users in europe and america

1863
01:51:58,610 --> 01:52:02,190
that's industry and academia work more in america

1864
01:52:02,200 --> 01:52:04,780
or europe

1865
01:52:04,800 --> 01:52:10,630
so that's compounds and what do i know enough to answer the question

1866
01:52:11,490 --> 01:52:14,780
first of all let me say whether they go hand in hand i think in

1867
01:52:14,780 --> 01:52:17,970
some areas the connection is absolutely essential

1868
01:52:17,980 --> 01:52:22,340
so in some areas the academia is doing something that are highly interesting and

1869
01:52:22,550 --> 01:52:26,640
we ahead showing demand by consumers

1870
01:52:26,650 --> 01:52:29,730
so in an industry like the internet which is fast evolving

1871
01:52:29,740 --> 01:52:32,570
it's very important to home in on these very quickly and in that case we

1872
01:52:32,570 --> 01:52:34,710
actually worked very closely with the kids

1873
01:52:34,730 --> 01:52:39,090
so as an example here who actually does has a grant program has a fellowship

1874
01:52:39,090 --> 01:52:42,520
program we have a academic relations program where we

1875
01:52:43,760 --> 01:52:48,180
establish a presence we bring in a lot of interns somewhere

1876
01:52:48,230 --> 01:52:49,730
throughout the year

1877
01:52:49,940 --> 01:52:52,830
so at that level at which say they go hand in hand

1878
01:52:54,020 --> 01:52:57,740
on the other hand industry is much more focused on

1879
01:52:58,340 --> 01:53:01,510
working on projects you

1880
01:53:01,530 --> 01:53:03,170
results today

1881
01:53:03,220 --> 01:53:08,930
and addressing consumer needs today whereas academic typically is what much more about this theory

1882
01:53:08,930 --> 01:53:11,730
or you know investigation of norway is so

1883
01:53:11,740 --> 01:53:13,380
i think there is this energy

1884
01:53:13,440 --> 01:53:16,540
in some cases it is very nice and very tight in some cases they there

1885
01:53:16,550 --> 01:53:18,230
are very very far apart

1886
01:53:18,420 --> 01:53:22,580
typically in areas like machine learning data mining

1887
01:53:22,670 --> 01:53:33,370
information retrieval connection is very strong in areas like systems managing huge web forums

1888
01:53:33,420 --> 01:53:35,750
o managing big systems

1889
01:53:36,040 --> 01:53:38,730
the distance is pretty far

1890
01:53:39,340 --> 01:53:42,500
in terms of the us versus europe i really don't know the difference i

1891
01:53:42,760 --> 01:53:47,550
i would imagine that in the US just by the nature of things a bit

1892
01:53:47,550 --> 01:53:50,160
more vibrant connection

1893
01:53:50,170 --> 01:53:56,350
and i think in europe it's picking up no which more

1894
01:53:58,370 --> 01:54:05,030
which industry in the data mining field is the hottest at the moment

1895
01:54:10,480 --> 01:54:14,560
which industries in the data mining field is the hardest i think any of the

1896
01:54:14,580 --> 01:54:19,980
the data mining field that deal with mining unstructured data and text data

1897
01:54:19,990 --> 01:54:25,060
i don't particularly hard man now

1898
01:54:25,070 --> 01:54:30,290
any of the categories that deal with privacy preserving data mining

1899
01:54:30,310 --> 01:54:35,240
worrying about anonymity users and so forth is also in high demand

1900
01:54:37,490 --> 01:54:41,520
i want to give you the wrong impression because you know classical stuff like classification

1901
01:54:41,520 --> 01:54:46,540
clustering and so forth are still used every day and also in demand but they

1902
01:54:46,550 --> 01:54:52,090
seem to be the heart is simply because the mining unstructured data mining distributed data

1903
01:54:52,100 --> 01:54:57,670
and privacy preserving

1904
01:54:57,680 --> 01:55:06,540
i guess who's coming to to the end of

1905
01:55:06,640 --> 01:55:09,160
interviews so

1906
01:55:09,180 --> 01:55:15,020
vision and grand plan how see the future in doubt for the internet and maybe

1907
01:55:15,020 --> 01:55:19,030
you who and data mining to how you predict the next twenty to thirty forty

1908
01:55:19,830 --> 01:55:21,240
or is that too long

1909
01:55:21,240 --> 01:55:22,640
not zero

1910
01:55:23,110 --> 01:55:26,200
so you will be

1911
01:55:27,240 --> 01:55:28,490
this is presentation

1912
01:55:28,490 --> 01:55:33,050
true basically if you know value j

1913
01:55:33,050 --> 01:55:35,930
i think you know it

1914
01:55:35,950 --> 01:55:37,370
but in terms of liquid

1915
01:55:38,470 --> 01:55:42,760
so if i didn't

1916
01:55:42,900 --> 01:55:50,830
so far east we observed that it's

1917
01:55:50,890 --> 01:56:00,590
there is a sense

1918
01:56:00,600 --> 01:56:11,530
so the topic is going to cover is

1919
01:56:11,590 --> 01:56:15,490
sequential model part of

1920
01:56:16,430 --> 01:56:19,300
this also interesting because it's the

1921
01:56:19,320 --> 01:56:24,550
o model that i will talk about in this section course that has a large

1922
01:56:24,570 --> 01:56:27,260
number of unobserved latent variables

1923
01:56:27,260 --> 01:56:30,990
and we're going to see if we can train very similar

1924
01:56:31,240 --> 01:56:33,280
propagation to do

1925
01:56:33,280 --> 01:56:35,490
inference and learning

1926
01:56:35,510 --> 01:56:36,470
and so on

1927
01:56:37,600 --> 01:56:41,050
this small is a is very influential

1928
01:56:41,240 --> 01:56:44,030
computational biology is used to

1929
01:56:44,030 --> 01:56:44,970
o line

1930
01:56:45,200 --> 01:56:48,490
sequences is the core model in

1931
01:56:48,570 --> 01:56:51,280
speech recognition software

1932
01:56:51,300 --> 01:56:53,910
and it's used in

1933
01:56:53,910 --> 01:56:56,720
processing video or computer vision has a lot

1934
01:56:57,760 --> 01:56:59,700
is also

1935
01:57:00,470 --> 01:57:03,720
more basic all life so in

1936
01:57:04,050 --> 01:57:11,720
because text information for example although very small we have conditional

1937
01:57:11,740 --> 01:57:13,850
also our

1938
01:57:13,970 --> 01:57:17,850
but it's good enough to form the the

1939
01:57:17,870 --> 01:57:20,120
and if you have questions

1940
01:57:20,140 --> 01:57:21,680
it was also

1941
01:57:21,700 --> 01:57:26,300
it's something that you always comparing axes into two

1942
01:57:26,320 --> 01:57:31,760
so much all is really

1943
01:57:33,700 --> 01:57:38,490
markov chain just like the more yesterday

1944
01:57:38,510 --> 01:57:42,890
which we had some problems in the

1945
01:57:42,910 --> 01:57:45,620
so is each

1946
01:57:45,720 --> 01:57:49,410
more probabilistic functions of markov

1947
01:57:50,570 --> 01:57:56,640
we use the first order markov chain to generate hidden state sequence x

1948
01:57:56,640 --> 01:58:00,850
so this these variables x will be sees

1949
01:58:00,870 --> 01:58:04,300
generate according to just support or

1950
01:58:04,320 --> 01:58:08,090
so we have probability first c x y

1951
01:58:08,100 --> 01:58:15,850
and given x one to x two x two x three to more or less

1952
01:58:16,530 --> 01:58:17,950
but we don't get to

1953
01:58:17,970 --> 01:58:19,280
visited states

1954
01:58:20,640 --> 01:58:22,700
for each time t

1955
01:58:22,720 --> 01:58:25,350
we created output y g

1956
01:58:25,370 --> 01:58:29,970
which is a function of the state executed that

1957
01:58:30,120 --> 01:58:35,640
so this is a transition diagram many time t

1958
01:58:35,660 --> 01:58:41,990
we have to which is are not sufficient quantity we really to say

1959
01:58:42,620 --> 01:58:44,240
observations on the move tab

1960
01:58:44,240 --> 01:58:46,320
one two is

1961
01:58:46,490 --> 01:58:49,030
so you can

1962
01:58:49,100 --> 01:58:52,660
this year is

1963
01:58:52,660 --> 01:58:54,010
the only had

1964
01:58:54,140 --> 01:58:55,970
so your from

1965
01:58:55,970 --> 01:58:57,890
the response what

1966
01:58:58,320 --> 01:59:00,550
usually state

1967
01:59:04,350 --> 01:59:05,620
time one

1968
01:59:05,640 --> 01:59:08,800
you can source your initial condition

1969
01:59:09,450 --> 01:59:14,820
so the solution what does it tell you twelve

1970
01:59:14,870 --> 01:59:16,030
so you want to

1971
01:59:16,050 --> 01:59:21,120
well it's the not only had twelve is the transition or is the

1972
01:59:21,120 --> 01:59:27,320
twelve rows of the transition matrix as the transitions inherent state well

1973
01:59:27,330 --> 01:59:30,260
he was going to all this

1974
01:59:30,280 --> 01:59:34,280
so what i silicon transistor you don't know

1975
01:59:34,300 --> 01:59:41,030
so that's the markov process each day each time you and i will also very

1976
01:59:42,600 --> 01:59:44,910
the situation is generated by

1977
01:59:44,930 --> 01:59:47,620
the observation

1978
01:59:49,260 --> 01:59:54,220
so on the on the j these distribution

1979
01:59:54,240 --> 02:00:01,430
january observations to move around the status according to the traditions each time observation and

1980
02:00:01,430 --> 02:00:04,410
all we get to see is the observations we don't know

1981
02:00:05,820 --> 02:00:08,350
the markov models action

1982
02:00:08,350 --> 02:00:12,350
so this is going to make it is important for those of you who know

1983
02:00:13,220 --> 02:00:15,800
you can state sequences first

1984
02:00:15,800 --> 02:00:20,330
of the process is not more of any works

1985
02:00:21,120 --> 02:00:23,160
so there is no

1986
02:00:23,720 --> 02:00:29,090
the local markov models in order capture the

1987
02:00:29,100 --> 02:00:33,430
the distribution over sequences that came are of

1988
02:00:33,430 --> 02:00:42,050
so that's why i occasionally models in speech recognition language modeling simple information retrieval system

1989
02:00:42,070 --> 02:00:43,740
closed in this area

1990
02:00:43,740 --> 02:00:45,970
what's your models

1991
02:00:47,870 --> 02:00:50,350
so why like use

1992
02:00:50,490 --> 02:00:53,600
support from biology and

1993
02:00:55,120 --> 02:01:02,180
this is graphical model but you should be very nice so the first thing to

1994
02:01:02,180 --> 02:01:04,280
notice is is a tree

1995
02:01:04,300 --> 02:01:13,070
three voluntary so treat is going through rest business these things as you can see

1996
02:01:13,070 --> 02:01:20,300
some is why are observation and what this small town this man for your our

1997
02:01:20,300 --> 02:01:27,090
one factorized distribution be all on y is for you is this

1998
02:01:27,100 --> 02:01:30,510
right on the problem of s

1999
02:01:31,410 --> 02:01:33,680
given x minus one

2000
02:01:34,800 --> 02:01:36,990
what he did

2001
02:01:37,050 --> 02:01:38,470
the here

2002
02:01:38,510 --> 02:01:41,050
joint probability of a sequence of axes

2003
02:01:41,070 --> 02:01:48,640
instead of y factorizes as the conditional probability of x px

2004
02:01:49,370 --> 02:01:51,430
so this should be about

2005
02:01:51,430 --> 02:01:54,850
times the probability of y given x

2006
02:01:54,890 --> 02:02:01,760
so the data y one y two y three or not i d

2007
02:02:01,760 --> 02:02:04,950
there's certainly not independent press

2008
02:02:04,970 --> 02:02:14,510
the number plate notation there's no easy way to write this small weights

2009
02:02:14,530 --> 02:02:18,010
so that's why text and that

2010
02:02:19,950 --> 02:02:21,720
OK so

2011
02:02:21,720 --> 02:02:24,450
his just his solution to

2012
02:02:24,470 --> 02:02:26,140
so in a few

2013
02:02:27,050 --> 02:02:33,970
all right my either as a markov chain so is simple first order markov chain

2014
02:02:34,640 --> 02:02:40,910
the we're here by hiding in and giving stochastic operations to take this chain we

2015
02:02:41,800 --> 02:02:43,660
each time generator with you

2016
02:02:43,660 --> 02:02:46,430
observation which depends on the the value chain

2017
02:02:46,450 --> 02:02:50,470
you can see or you get mixture models

2018
02:02:50,470 --> 02:02:53,590
with the state coupled across

2019
02:02:53,600 --> 02:02:54,700
so this

2020
02:02:54,720 --> 02:02:56,700
for the last

2021
02:02:57,640 --> 02:02:59,640
depending about state

2022
02:02:59,640 --> 02:03:00,320
you generate

2023
02:03:00,330 --> 02:03:02,030
now what i from

2024
02:03:02,240 --> 02:03:04,010
one or two

2025
02:03:04,010 --> 02:03:05,350
for k

2026
02:03:05,370 --> 02:03:12,530
this is exactly what makes except that the choice of mixture component is called

2027
02:03:12,530 --> 02:03:16,230
outputs classifier or maybe best support vector machine

2028
02:03:16,380 --> 02:03:18,340
which gave the final output of the model

2029
02:03:18,630 --> 02:03:21,530
basically use gradient ascent or descent

2030
02:03:22,050 --> 02:03:25,080
it's dataset these because our it's a min-max problem

2031
02:03:27,200 --> 02:03:30,740
basically there's a big related multi rule multi-layer perceptrons

2032
02:03:30,950 --> 02:03:32,800
like the pastoral like perceptrons

2033
02:03:32,990 --> 02:03:37,930
and easy and they couldn't solve like to solve problems that means proper for show

2034
02:03:39,300 --> 02:03:41,580
having both ball those perceptrons

2035
02:03:41,930 --> 02:03:42,840
put together

2036
02:03:43,000 --> 02:03:44,420
we have the multi-layer perceptron

2037
02:03:44,580 --> 02:03:46,870
and you can each make the deep whatever you want

2038
02:03:48,300 --> 02:03:52,790
a support vector machine deep support vector machine base we do the same person each perceptron

2039
02:03:52,810 --> 02:03:54,550
is based support vector machine

2040
02:03:54,910 --> 02:03:56,390
so you can understand that

2041
02:03:56,620 --> 02:03:59,180
the real person face face much more space

2042
02:04:00,870 --> 02:04:02,820
food to get really big datasets

2043
02:04:03,060 --> 02:04:04,250
in the dual space

2044
02:04:04,460 --> 02:04:07,380
was sold also that all of professor bach

2045
02:04:07,670 --> 02:04:09,420
you have probably to

2046
02:04:09,650 --> 02:04:13,190
to do some kind sparse case like only kernels

2047
02:04:13,430 --> 02:04:15,430
which are necessary on input points

2048
02:04:15,560 --> 02:04:17,250
which fall away our partners

2049
02:04:17,390 --> 02:04:18,250
but there's

2050
02:04:18,480 --> 02:04:22,100
a lot yet the case here because we have problems

2051
02:04:22,270 --> 02:04:25,070
with at most like that thousand fifty examples

2052
02:04:25,330 --> 02:04:26,720
so that's fast enough

2053
02:04:27,950 --> 02:04:29,100
the architecture

2054
02:04:29,360 --> 02:04:30,340
uses follows

2055
02:04:31,050 --> 02:04:32,960
you have the inputs like given who

2056
02:04:33,140 --> 02:04:35,180
in this case we support vector machines

2057
02:04:35,460 --> 02:04:39,910
but in general we optimise them with some search algorithm

2058
02:04:40,260 --> 02:04:42,570
which is called particles solve myspace

2059
02:04:42,920 --> 02:04:47,460
and sometimes we get like even like how the support vector machine here

2060
02:04:48,160 --> 02:04:49,810
the psyche from

2061
02:04:50,010 --> 02:04:53,180
extracted each on one feature bigger support vector machine

2062
02:04:53,640 --> 02:04:54,460
can usually

2063
02:04:54,650 --> 02:04:55,830
and one feature

2064
02:04:56,170 --> 02:04:57,040
the same time

2065
02:04:57,170 --> 02:04:58,590
and promote other classes

2066
02:04:58,850 --> 02:05:00,980
we need to train all support vector machines

2067
02:05:01,290 --> 02:05:03,100
then this may support vector machine

2068
02:05:03,270 --> 02:05:06,530
and basically makes all feature and get the final output

2069
02:05:07,780 --> 02:05:10,810
so the work things of the system as follows

2070
02:05:11,410 --> 02:05:15,290
you want to get a few presentation just norm requesting

2071
02:05:15,580 --> 02:05:19,650
became get some kernel here we use radial basis function kernels

2072
02:05:19,800 --> 02:05:20,780
in both layers

2073
02:05:20,950 --> 02:05:25,400
so this is really basis function kernel and then we get on feature if you train

2074
02:05:25,540 --> 02:05:26,330
for each

2075
02:05:26,520 --> 02:05:31,060
support vector machine m one tool in this case for each one twenty three

2076
02:05:31,360 --> 02:05:35,370
we can like support vector machine support vector coefficients long term b

2077
02:05:35,820 --> 02:05:40,810
for while we get for each data example locals vector coefficients

2078
02:05:41,130 --> 02:05:42,550
and then we can compute features

2079
02:05:42,710 --> 02:05:45,470
for all data examples also called test examples

2080
02:05:45,950 --> 02:05:47,920
and then the main support vector machine

2081
02:05:48,050 --> 02:05:52,760
this is just the normal way but now it uses the kernel over the exact features

2082
02:05:52,780 --> 02:05:54,000
s inputs

2083
02:05:55,990 --> 02:05:58,000
so what we do to train the system

2084
02:05:58,180 --> 02:06:00,240
of first we had output function

2085
02:06:00,370 --> 02:06:01,990
which is usually like

2086
02:06:02,280 --> 02:06:04,080
the function is th

2087
02:06:04,250 --> 02:06:06,930
is now all the feature space first expected

2088
02:06:07,280 --> 02:06:10,150
for more objective function w is replaced by

2089
02:06:10,330 --> 02:06:15,330
a min-max problem but w want to minimise the features

2090
02:06:15,480 --> 02:06:17,710
monster marks five for all of patients

2091
02:06:18,110 --> 02:06:22,790
so this is still funny how this is going with the min-max optimization

2092
02:06:23,060 --> 02:06:25,180
because the same function open out

2093
02:06:25,460 --> 02:06:27,550
and also if you look at how learns

2094
02:06:27,710 --> 02:06:30,660
the world vector goes up and down all the time

2095
02:06:31,030 --> 02:06:35,740
yeah it's like something like cain theory way have opens

2096
02:06:36,140 --> 02:06:37,290
it's going up and

2097
02:06:37,680 --> 02:06:40,040
this known and into this

2098
02:06:40,210 --> 02:06:42,440
it could offer some seven point

2099
02:06:42,560 --> 02:06:43,420
but there are

2100
02:06:43,940 --> 02:06:45,430
some very often

2101
02:06:48,180 --> 02:06:51,300
the training procedure with the gradient ascent as follows

2102
02:06:52,540 --> 02:06:55,320
and we have like the main support vector machine

2103
02:06:55,570 --> 02:06:59,960
that from scratch simple because we are we've growth optimise the main support vector machine

2104
02:07:00,210 --> 02:07:03,430
so we add up all these alpha coefficients can create percent

2105
02:07:04,390 --> 02:07:06,380
this space this kernel year

2106
02:07:06,580 --> 02:07:08,160
so this is the function

2107
02:07:08,360 --> 02:07:12,290
that this is th the gradient ascent update step

2108
02:07:12,570 --> 02:07:15,040
both alpha and alpha stop

2109
02:07:16,210 --> 02:07:22,260
for the feature layer sensitive the more complicated but i feel like training multilayer set consists

2110
02:07:22,210 --> 02:07:23,790
source all back-prop case

2111
02:07:24,040 --> 02:07:24,990
so we have applied

2112
02:07:25,160 --> 02:07:27,350
the same way of use backpropagation

2113
02:07:27,620 --> 02:07:29,280
to get like datasets

2114
02:07:29,410 --> 02:07:31,510
for each feature extracting svm

2115
02:07:32,470 --> 02:07:35,120
so in this way we have really basis function

2116
02:07:35,270 --> 02:07:36,080
even here

2117
02:07:36,320 --> 02:07:38,240
then we have this is the

2118
02:07:38,620 --> 02:07:41,500
basically now to your letter to the features

2119
02:07:41,870 --> 02:07:43,250
then this can be written

2120
02:07:43,400 --> 02:07:44,200
as follows

2121
02:07:44,380 --> 02:07:46,180
because we know that kernel

2122
02:07:46,300 --> 02:07:47,800
we can compute

2123
02:07:47,940 --> 02:07:49,190
exactly like

2124
02:07:49,360 --> 02:07:51,020
how we won't who

2125
02:07:51,730 --> 02:07:53,830
what kind of features how this would be changed

2126
02:07:54,680 --> 02:07:57,050
instead of changing them immediately

2127
02:07:57,230 --> 02:08:00,450
basically mean we construct all dataset by

2128
02:08:00,830 --> 02:08:01,850
een did

2129
02:08:02,690 --> 02:08:03,980
to the previous feature

2130
02:08:04,170 --> 02:08:08,360
so we have a feature and subtract these becomes want to minimise it

2131
02:08:08,600 --> 02:08:10,380
best that dataset perspective

2132
02:08:10,770 --> 02:08:13,330
then we get for each point you get a new

2133
02:08:13,470 --> 02:08:15,910
feature vector which would be a pocket feature

2134
02:08:16,430 --> 02:08:19,400
and then be off getting this training dataset

2135
02:08:19,540 --> 02:08:23,110
we we can train each in payments indepen support vector machine

2136
02:08:23,600 --> 02:08:24,960
want to know who thinks

2137
02:08:25,590 --> 02:08:28,610
first of all you could do this with the multilayer perceptrons as well

2138
02:08:28,880 --> 02:08:31,090
by first training output

2139
02:08:31,450 --> 02:08:35,480
nolte and then giving errors all the other notes

2140
02:08:35,870 --> 02:08:41,230
would be a little bit similar i like best approach offers affect layers that phones

2141
02:08:41,550 --> 02:08:43,540
but in this way makes little sense

2142
02:08:43,980 --> 02:08:47,700
for the mob support vector machines initialized

2143
02:08:47,830 --> 02:08:49,870
we need some kind of symmetry bread ok

2144
02:08:50,250 --> 02:08:54,590
so otherwise would all expect the same features which is not what we want because

2145
02:08:54,610 --> 02:08:55,800
it turns out that the only

2146
02:08:56,040 --> 02:08:58,050
learn one feature source say

2147
02:08:58,370 --> 02:08:59,970
so what do we

2148
02:09:00,470 --> 02:09:02,720
we train support vector machines the first time

2149
02:09:02,970 --> 02:09:06,530
on like better outputs all that bach datasets

2150
02:09:06,920 --> 02:09:10,240
so that way it is already like an ensemble method a little bit

