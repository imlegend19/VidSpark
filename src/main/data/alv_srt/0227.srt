1
00:00:00,000 --> 00:00:05,320
and the real events cluster sort of like earthquakes with the aftershocks

2
00:00:05,340 --> 00:00:08,980
and in fact the more a lot of aftershocks in earthquakes has been used by

3
00:00:08,980 --> 00:00:12,560
the on line ten you to describe the aftershocks

4
00:00:12,650 --> 00:00:15,560
other big events such as black monday

5
00:00:15,700 --> 00:00:20,450
in fact if you look very carefully at this stage using equipment that i'm not

6
00:00:20,680 --> 00:00:25,690
going to say that mister my mother said be trying to contribute to eyes

7
00:00:25,750 --> 00:00:31,600
you can't count the number of times more than five standard deviations and you get

8
00:00:31,600 --> 00:00:32,880
sixty four

9
00:00:32,880 --> 00:00:37,940
you think of the number of events more than ten standard deviations and you get

10
00:00:37,940 --> 00:00:40,300
eight thirty eight times fewer

11
00:00:40,320 --> 00:00:41,910
and you can do a number of events

12
00:00:42,000 --> 00:00:47,010
twenty standard deviation again only one times fewer

13
00:00:47,030 --> 00:00:48,980
that's obviously a power law

14
00:00:48,990 --> 00:00:53,120
because each time we don't x axis we divided by age

15
00:00:53,120 --> 00:00:57,700
the y axis so that only a power law with exponent is

16
00:00:57,850 --> 00:00:59,930
two q minus three

17
00:01:00,010 --> 00:01:04,860
so you can see with your eyes the power law the inverse cube for these

18
00:01:04,860 --> 00:01:07,970
data but that doesn't convince anybody that the

19
00:01:08,290 --> 00:01:11,120
this is one way to look at real data in the real breakthrough in my

20
00:01:11,120 --> 00:01:13,420
opinion came when was a month

21
00:01:14,370 --> 00:01:17,190
had the courage to analyse

22
00:01:17,200 --> 00:01:18,380
so the data

23
00:01:18,380 --> 00:01:22,960
that existed at that time which was one and a half million data points of

24
00:01:22,960 --> 00:01:25,180
this as the p five hundred index

25
00:01:25,200 --> 00:01:30,350
and what they found was very interesting distribution is a distribution that was leading in

26
00:01:30,350 --> 00:01:31,460
the middle

27
00:01:31,490 --> 00:01:38,390
but after about five standard deviation was truncated by what was the public approximating exponential

28
00:01:38,440 --> 00:01:44,950
introduced the concept called truncated flight which has become popular not only physics but also

29
00:01:44,950 --> 00:01:46,790
in economics

30
00:01:46,980 --> 00:01:55,120
in this work of was followed up by coptic christian would be even more data

31
00:01:55,240 --> 00:02:00,310
instead of one million data of an index they looked at each and every individual

32
00:02:00,310 --> 00:02:05,370
stock in every transaction that so there going to particle physics where instead of looking

33
00:02:05,410 --> 00:02:11,630
molecules look at every constituent of the molecule nuclei some degree and so forth and

34
00:02:12,670 --> 00:02:17,750
a lot basis but the most relevant one is the in quotes database for and

35
00:02:17,750 --> 00:02:20,140
what they specifically looked at was the files

36
00:02:20,300 --> 00:02:25,670
larger stocks but every retreated so instead of going the two hundred million records that's

37
00:02:25,690 --> 00:02:31,020
a staggering amount of data and because the staggering amount of data they could study

38
00:02:31,030 --> 00:02:37,190
events that was rare is one hundred standard deviations attributes x axis is one hundred

39
00:02:37,190 --> 00:02:43,210
ten editions of what they found was truly remarkable they found that there were no

40
00:02:43,210 --> 00:02:44,960
outliers at all

41
00:02:45,000 --> 00:02:47,500
the concept of outliers

42
00:02:47,550 --> 00:02:49,240
the that exist

43
00:02:49,450 --> 00:02:53,760
that's one hundred standard deviations from the same curve as

44
00:02:53,780 --> 00:02:56,920
the events before

45
00:02:56,950 --> 00:02:58,940
of everyday events

46
00:02:58,960 --> 00:03:03,820
and and that line is a straight line on the paper so minus three is

47
00:03:03,820 --> 00:03:04,940
really just by

48
00:03:04,960 --> 00:03:07,670
using our i two minutes ago

49
00:03:07,670 --> 00:03:16,790
so this is the economics fluctuations law financial fluctuations has been already anticipated by the

50
00:03:16,790 --> 00:03:19,170
work of thomas looks

51
00:03:19,180 --> 00:03:23,960
the data set two years before we did our work and has been confirmed by

52
00:03:24,090 --> 00:03:26,370
by roughly fifty other

53
00:03:26,380 --> 00:03:31,500
calculations using looking at data from other markets and it seems to be quite universal

54
00:03:31,500 --> 00:03:34,460
with a few caveats which we can discuss privately

55
00:03:35,270 --> 00:03:40,950
before we and that that's why should this be like statistical physics problems and we're

56
00:03:40,950 --> 00:03:46,880
going to say now is not to be blamed by collaborators it's more intuition which

57
00:03:46,880 --> 00:03:50,890
is a little like religion something little personal but it is a personal that's all

58
00:03:50,890 --> 00:03:56,920
about my mother to say something a little bit of speculative

59
00:03:56,930 --> 00:03:59,540
how can we think of this financial

60
00:03:59,560 --> 00:04:05,460
the system is complex system about which we know so incredibly well

61
00:04:05,460 --> 00:04:07,790
this approach religion they

62
00:04:07,820 --> 00:04:14,290
fundamental background who we are what we experienced so this approach this very very awe-inspiring

63
00:04:14,290 --> 00:04:18,320
see when people say

64
00:04:22,170 --> 00:04:28,670
well there's a lot of you know that

65
00:04:28,690 --> 00:04:30,430
we have a u

66
00:04:33,620 --> 00:04:40,570
o thing is

67
00:04:42,350 --> 00:04:43,880
there were one

68
00:04:56,830 --> 00:05:01,460
well one thing

69
00:05:07,370 --> 00:05:10,340
this is the

70
00:05:10,360 --> 00:05:11,600
the game

71
00:05:12,270 --> 00:05:14,610
know the

72
00:05:48,490 --> 00:05:55,550
you know

73
00:05:55,550 --> 00:05:58,940
on right

74
00:06:04,400 --> 00:06:07,060
on the

75
00:06:07,110 --> 00:06:11,010
o five

76
00:06:40,160 --> 00:06:46,440
all of the protein

77
00:06:54,710 --> 00:06:57,310
on the product

78
00:06:57,410 --> 00:07:05,370
you are approved by

79
00:07:10,590 --> 00:07:12,860
the model

80
00:07:13,680 --> 00:07:18,100
he was

81
00:07:31,120 --> 00:07:34,540
or right

82
00:07:34,550 --> 00:07:36,320
we are

83
00:07:47,540 --> 00:07:53,180
the problem

84
00:07:59,280 --> 00:08:00,880
she was you know

85
00:08:07,250 --> 00:08:10,190
from from

86
00:08:10,220 --> 00:08:18,890
one new

87
00:08:22,050 --> 00:08:24,230
nothing wrong

88
00:08:47,270 --> 00:08:52,320
so all

89
00:08:52,520 --> 00:08:57,880
so you see

90
00:09:26,010 --> 00:09:27,820
and course

91
00:09:28,080 --> 00:09:30,390
all along

92
00:09:42,530 --> 00:09:44,610
are there

93
00:09:44,630 --> 00:09:48,180
are they

94
00:09:48,190 --> 00:09:53,920
all of

95
00:09:56,170 --> 00:09:59,380
after that

96
00:10:26,620 --> 00:10:38,360
so all this

97
00:10:52,760 --> 00:10:58,170
wrong was

98
00:11:03,590 --> 00:11:08,350
from the graph

99
00:11:24,380 --> 00:11:25,570
rob hall

100
00:11:25,570 --> 00:11:29,630
it turns out that you can do much better at test time

101
00:11:30,900 --> 00:11:34,720
if you have this additional modalities the training time because what happens is that it

102
00:11:34,720 --> 00:11:38,450
turns out that you can regularize model with the model is able to learn much

103
00:11:38,970 --> 00:11:43,610
more reasonable embedded images by looking different sources of views of the data

104
00:11:45,440 --> 00:11:48,570
and the missing the nice thing about these probabilistic models is that the way you

105
00:11:48,570 --> 00:11:52,380
deal with missing text is you basically do inference over the missing text

106
00:11:52,690 --> 00:11:57,110
so the model fills in the missing tax and then there is the recognition of what it sees

107
00:11:58,210 --> 00:11:58,800
in each

108
00:11:59,280 --> 00:12:00,590
you can also do things like

109
00:12:01,260 --> 00:12:02,150
winning from lips

110
00:12:03,150 --> 00:12:05,840
this is what can be done by number of groups wear

111
00:12:06,380 --> 00:12:08,970
basically look at you have been modality

112
00:12:09,380 --> 00:12:09,940
as well as

113
00:12:10,590 --> 00:12:14,970
o forty images as well as follows the sound you can feed them in in

114
00:12:15,300 --> 00:12:16,670
you can have a pretty reasonable

115
00:12:18,240 --> 00:12:21,550
this is particularly useful for doing speech recognition in noisy environments

116
00:12:23,280 --> 00:12:24,630
so in the last part

117
00:12:26,650 --> 00:12:27,220
the tutorial

118
00:12:27,800 --> 00:12:32,360
let me just go into another development that's been happening in the deep learning communities

119
00:12:32,940 --> 00:12:35,840
is trying to look at slightly more challenging problem

120
00:12:36,420 --> 00:12:42,320
and the challenging problem of actually describing complete description of images were trying to basically generates

121
00:12:42,920 --> 00:12:44,990
structured output like this sentence

122
00:12:47,360 --> 00:12:51,090
so ideally you know you look at the image like this and the system automatically

123
00:12:51,090 --> 00:12:55,260
selects a man skiing down snow-covered mountain reductions going back in the back

124
00:12:55,940 --> 00:12:57,510
with the dark sky and in the background

125
00:12:58,220 --> 00:12:59,670
right so how can we do not

126
00:13:00,800 --> 00:13:05,240
one of the ideas that have been evolving in the last couple of years is the idea of

127
00:13:05,720 --> 00:13:07,070
representing words

128
00:13:08,010 --> 00:13:08,530
in terms

129
00:13:09,220 --> 00:13:14,530
we are in the semantic space representing words as feature vectors in some dimensional space

130
00:13:15,400 --> 00:13:19,990
right so what you gonna do he's gonna be embedding each word in the dimensional

131
00:13:19,990 --> 00:13:24,170
real valued vector with real valued space in your represented as a vector so the

132
00:13:24,170 --> 00:13:24,780
hope is that

133
00:13:25,260 --> 00:13:28,090
you know table and chairs will get put together

134
00:13:28,940 --> 00:13:34,820
dolphins and whales because they semantically similar will get put together in november will get put somewhere else

135
00:13:35,920 --> 00:13:36,550
in that space

136
00:13:37,650 --> 00:13:41,030
right so representing words is basically points dimensional space

137
00:13:43,490 --> 00:13:44,550
one of um

138
00:13:46,740 --> 00:13:51,340
one of the most commonly used model for generating sentences is

139
00:13:51,820 --> 00:13:54,340
is will bring a lot by learning in your language model

140
00:13:55,130 --> 00:14:00,130
and the new language model was actually developed back in two thousand three by yoshua bengio his lab

141
00:14:00,840 --> 00:14:02,090
that's what i had a resurgence

142
00:14:02,630 --> 00:14:06,150
coming up in the last few years and it is very simple you basically say well

143
00:14:07,190 --> 00:14:09,440
you have a bunch of words

144
00:14:10,220 --> 00:14:10,650
and let's say

145
00:14:11,150 --> 00:14:13,820
walking on there and you're trying to predict the next one

146
00:14:15,070 --> 00:14:19,490
and the hope is that working on the street would have a high probability because that's

147
00:14:20,220 --> 00:14:22,840
a reasonable conclusion but something like

148
00:14:23,320 --> 00:14:27,740
are block will have very low probability and know what that's like the name of some

149
00:14:29,670 --> 00:14:32,800
um or watching on the zebra will have small probability right

150
00:14:33,420 --> 00:14:36,240
and the way you cannot do it if you're gonna say well there are words

151
00:14:38,720 --> 00:14:40,150
there are context matrices

152
00:14:40,670 --> 00:14:40,990
that u

153
00:14:42,920 --> 00:14:44,150
multiply these words by

154
00:14:45,210 --> 00:14:48,260
and then you get you add is a key by complex matrices and then you

155
00:14:48,260 --> 00:14:52,760
basically these view the prediction of the next word so basically get a linear combination

156
00:14:52,780 --> 00:14:54,590
of the previous works to predict the next word

157
00:14:55,260 --> 00:14:58,880
that's that's the idea so this is the predictive what a presentation and then if

158
00:14:58,880 --> 00:15:01,380
you want have a distribution over possible words

159
00:15:01,900 --> 00:15:04,150
that's going to be given by softmax model

160
00:15:06,050 --> 00:15:11,050
so you're essentially here you're looking at the dot products between feature vectors and trying

161
00:15:11,050 --> 00:15:13,590
to predict what's the next most probable worked

162
00:15:14,150 --> 00:15:15,380
that's the answer

163
00:15:16,920 --> 00:15:20,650
now the way to think about images and text is you can think of images

164
00:15:21,610 --> 00:15:22,990
as an encouraging decorum model

165
00:15:23,530 --> 00:15:25,030
so let's say i think this image

166
00:15:25,570 --> 00:15:27,170
and map it into some feature space

167
00:15:27,630 --> 00:15:33,510
right so sam using convolutional neural network i'm using deep belief network both machine one of the different models

168
00:15:34,010 --> 00:15:35,710
i did the semantic feature space

169
00:15:37,400 --> 00:15:40,920
and then based on the semantic feature space i decoded back to this sentence

170
00:15:42,450 --> 00:15:47,380
so people who work and translations for example is a very similar story when you have a sentence in english

171
00:15:47,900 --> 00:15:51,360
you're encoded in the some representation and then you decode it in a new language

172
00:15:52,360 --> 00:15:52,720
in view

173
00:15:53,280 --> 00:15:56,880
and in coder-decoder is the same the same way they can image called in some

174
00:15:56,880 --> 00:15:58,780
space decoded back into the sentence

175
00:15:59,860 --> 00:16:04,610
and this is one particular mall was done by ryan tears where r u in

176
00:16:04,610 --> 00:16:08,280
addition to the text you have a modality you have an image modality

177
00:16:09,860 --> 00:16:13,260
so you say steamship in and you have an image of a steamship and you

178
00:16:13,260 --> 00:16:14,610
can see in the city for example

179
00:16:15,400 --> 00:16:19,440
so these image itself can bias what was going to predict next

180
00:16:20,780 --> 00:16:24,780
but now what's been emerging in the last couple of years is this idea of

181
00:16:24,950 --> 00:16:29,170
actually what you can do is you can also about words in the same space

182
00:16:30,900 --> 00:16:34,110
you can embed images in one space between the words in the same space

183
00:16:34,900 --> 00:16:37,860
you can combine all conditional anything images word phrases

184
00:16:38,440 --> 00:16:40,860
that is a very natural definition of a scoring function

185
00:16:41,670 --> 00:16:45,510
which is just an inner product between image in between features she just looking at

186
00:16:45,920 --> 00:16:48,320
in the product so cosine similarity between features

187
00:16:49,240 --> 00:16:50,550
which is very easy to to

188
00:16:51,130 --> 00:16:51,380
to do

189
00:16:53,050 --> 00:16:56,970
and you can also use you know some kind of language models incorporate additional structure

190
00:16:57,450 --> 00:16:58,440
into the system here

191
00:16:59,030 --> 00:17:02,970
the key idea is that now you can model images and text the phrase you

192
00:17:02,970 --> 00:17:05,300
can embed them all the same semantic space

193
00:17:05,990 --> 00:17:07,210
they all become features and this

194
00:17:08,280 --> 00:17:09,400
joint semantic space

195
00:17:09,880 --> 00:17:12,400
right so for example how we train a model like this

196
00:17:13,260 --> 00:17:15,900
so one way of training and imagine that you have an image and you have

197
00:17:15,900 --> 00:17:19,170
a sentence u map sentence the space in the image space

198
00:17:19,840 --> 00:17:21,050
hopefully have another image

199
00:17:21,610 --> 00:17:24,340
and the sentence in both image and sends get mapped to these

200
00:17:24,860 --> 00:17:25,860
the same space as well

201
00:17:27,400 --> 00:17:29,170
so what you like to do in terms of

202
00:17:29,800 --> 00:17:33,530
finding the joint feature space is you minimize the following injective function

203
00:17:34,840 --> 00:17:36,070
let's look at this objective function

204
00:17:37,050 --> 00:17:41,240
here you have the scoring function which you can think of just defining a dot

205
00:17:41,240 --> 00:17:44,400
product between two vectors defining dot product between x and

206
00:17:46,300 --> 00:17:51,380
and the idea is that you want this scoring function this has to be as high as possible

207
00:17:52,800 --> 00:17:53,320
because let's say

208
00:17:53,800 --> 00:17:55,650
the dot product between these guys is ten

209
00:17:57,510 --> 00:18:00,440
what's the second time the second term sometimes called contrastive

210
00:18:01,150 --> 00:18:02,510
it essentially says well

211
00:18:02,950 --> 00:18:04,510
if i take the same image

212
00:18:04,900 --> 00:18:06,170
and different sentence

213
00:18:07,650 --> 00:18:10,030
i want the dot product between these two features

214
00:18:10,530 --> 00:18:11,170
to be very small

215
00:18:12,190 --> 00:18:14,280
on the distance between these two features to be very small

216
00:18:14,800 --> 00:18:15,880
so if this is ten

217
00:18:16,320 --> 00:18:17,630
and this is five and it's good

218
00:18:18,070 --> 00:18:19,130
i'm not suffering loss

219
00:18:19,650 --> 00:18:21,940
and you have to be better than your contrastive sentence by

220
00:18:22,400 --> 00:18:24,510
some some margin of

221
00:18:25,030 --> 00:18:27,970
you know the way to think about these models you basically say well i want

222
00:18:27,970 --> 00:18:29,510
the sentence to go along with this image

223
00:18:30,170 --> 00:18:35,010
but i don't want this sentence to go along with image that's the idea ranking like loss

224
00:18:36,530 --> 00:18:39,590
and you're trying to basically find what those embeddings should be

225
00:18:40,090 --> 00:18:41,900
in terms of that's the same story

226
00:18:42,940 --> 00:18:43,630
right you can

227
00:18:43,630 --> 00:18:47,700
are larger than the pairwise distances we're here if i keep

228
00:18:47,700 --> 00:18:49,680
if i keep the order like

229
00:18:49,700 --> 00:18:57,140
so just move points apart in in you don't get it right

230
00:18:57,140 --> 00:18:59,430
this just

231
00:19:00,320 --> 00:19:03,340
no it's not you it's

232
00:19:03,430 --> 00:19:05,470
just do something like

233
00:19:05,470 --> 00:19:06,240
i say

234
00:19:07,680 --> 00:19:11,860
thirty six so i say this this point moves here this point here this point

235
00:19:12,050 --> 00:19:16,970
here so it's not it's not the geometric transformation just making OK on a more

236
00:19:16,970 --> 00:19:20,780
abstract level i just say i a function which maps each of those points to

237
00:19:20,780 --> 00:19:23,160
one of those points

238
00:19:23,910 --> 00:19:29,200
and i'm sure that the distances before like before so measure the distance here and

239
00:19:29,200 --> 00:19:34,160
i measure the distance after i made the transformation and the distances increased

240
00:19:34,180 --> 00:19:38,990
that's the only so it's not a geometric thing you just it's really

241
00:19:39,970 --> 00:19:44,450
increasing saying said no but this

242
00:19:44,570 --> 00:19:46,890
the same is true those

243
00:19:46,890 --> 00:19:50,720
no that's not the same but but i mean the axiom was

244
00:19:50,740 --> 00:19:53,110
it didn't say anything about how you

245
00:19:53,140 --> 00:19:57,530
how you are allowed to shrink distances it just says if you expand distances between

246
00:19:57,530 --> 00:19:59,660
points which are in different clusters

247
00:19:59,680 --> 00:20:02,050
then the clustering does not change

248
00:20:02,050 --> 00:20:04,410
and the idea of course is that if you have

249
00:20:04,430 --> 00:20:09,050
that is it

250
00:20:09,070 --> 00:20:09,780
it's hard to say

251
00:20:09,780 --> 00:20:14,050
so so it's it's it's much anything it doesn't have anything to do with geometry

252
00:20:14,050 --> 00:20:17,260
it's it's just the the notion of

253
00:20:17,280 --> 00:20:20,240
of clustering someone says OK maybe we

254
00:20:20,260 --> 00:20:24,970
based on clustering distances and we want the points which are in different clusters

255
00:20:24,990 --> 00:20:30,390
clusters are similar to each other and then if i make similarity even larger than

256
00:20:30,410 --> 00:20:34,470
it's the should give it just in different ways

257
00:20:34,490 --> 00:20:37,050
well OK why we need sex and that's why OK

258
00:20:37,070 --> 00:20:40,630
of course that's not what that's not exactly the question of course you can start

259
00:20:40,640 --> 00:20:42,200
to remove accents

260
00:20:42,220 --> 00:20:45,840
actually what turned out to if you remove any of those axioms the rest of

261
00:20:45,840 --> 00:20:48,780
the two other ones are consistent season

262
00:20:52,010 --> 00:20:53,240
three four

263
00:20:53,280 --> 00:20:55,570
because forms or

264
00:20:55,590 --> 00:20:59,320
what is just one

265
00:21:01,800 --> 00:21:03,550
it was red

266
00:21:05,260 --> 00:21:10,780
OK so OK so there's one hundred and didn't really explain so he doesn't work

267
00:21:10,780 --> 00:21:15,430
actually and configuration of points he works on distance matrices so the input is i've

268
00:21:15,630 --> 00:21:19,610
miss the distance matrix you don't have any points you have just the distance matrix

269
00:21:20,890 --> 00:21:22,220
someone to give you

270
00:21:22,240 --> 00:21:27,140
and then you so you don't and then just i think the idea is just

271
00:21:27,530 --> 00:21:32,390
if you have a distance matrix and cluster like this

272
00:21:33,050 --> 00:21:38,180
and now you are you have made zero point one that's that's

273
00:21:38,200 --> 00:21:42,950
so there you have two points in different classes but they have some small similarity

274
00:21:43,160 --> 00:21:44,450
now if you if you

275
00:21:44,510 --> 00:21:48,820
lord this one to zero it should not change the clustering

276
00:21:48,820 --> 00:21:53,340
so essentially if you decrease in the similarity matrix if you decrease the off diagonal

277
00:21:53,340 --> 00:21:57,700
terms it should even be more obvious the secaucus that somehow would be what you

278
00:21:57,700 --> 00:21:59,280
want to do

279
00:21:59,300 --> 00:22:00,550
OK but of course

280
00:22:01,050 --> 00:22:05,200
you can you can you can start debating i mean this is also problematic acts

281
00:22:05,200 --> 00:22:09,410
because maybe there's this stupid clustering simply don't want to get or

282
00:22:09,430 --> 00:22:13,640
i mean in learning theory you always learn that you never should work with the

283
00:22:13,660 --> 00:22:18,030
function space which can generate any function because it would fit somehow why do you

284
00:22:18,130 --> 00:22:21,110
have this year of course can debate about

285
00:22:21,130 --> 00:22:24,530
the point remains here is i think he came up with axioms which all of

286
00:22:24,530 --> 00:22:27,970
them sound pretty harmless but then it doesn't work so well

287
00:22:28,050 --> 00:22:29,820
and of course and

288
00:22:29,840 --> 00:22:33,550
i think didn't say that those are the only extant to see justice there's the

289
00:22:33,550 --> 00:22:38,010
axioms and on the first glance all of the input and now it doesn't

290
00:22:48,070 --> 00:22:51,300
and i which happening

291
00:22:51,320 --> 00:22:57,110
o that's OK if you're interested in that

292
00:22:57,140 --> 00:23:00,880
that section of the paper to talk you can download from shows so he discusses

293
00:23:00,880 --> 00:23:04,640
all this stuff so it sits atop which i done workshop so

294
00:23:04,640 --> 00:23:06,610
if you are interested just look at it

295
00:23:07,910 --> 00:23:12,110
we i can i mean i don't want to spend day on axiomatic points of

296
00:23:12,110 --> 00:23:17,660
view because namely my point of view is that somatic point of view it's pretty

297
00:23:17,660 --> 00:23:20,740
problematic because

298
00:23:20,740 --> 00:23:24,340
it depends on your choice of axioms and first of all it sounds very fundamental

299
00:23:24,340 --> 00:23:29,530
you define accents greater mathematician now you solve the problem of clustering and it's in

300
00:23:30,320 --> 00:23:33,720
it's pretty around the right to come up with a few accents and then

301
00:23:33,740 --> 00:23:37,450
whatever and of course the second thing is i mean

302
00:23:37,470 --> 00:23:40,050
xim xim nice but in practice

303
00:23:40,050 --> 00:23:42,630
they don't really help you here

304
00:23:42,630 --> 00:23:46,990
but this is i i want to present wanted to present because it's really not

305
00:23:47,010 --> 00:23:51,640
so there was a time when everybody trying to define clustering such away because of

306
00:23:51,640 --> 00:23:55,840
the advantages some if it would work it would be elegant because you avoid to

307
00:23:55,840 --> 00:23:58,360
define explicitly what clustering is

308
00:23:58,370 --> 00:24:02,340
just properties and then we show that there is an idea which does what you

309
00:24:02,340 --> 00:24:05,180
want so

310
00:24:05,180 --> 00:24:06,830
by my hand

311
00:24:06,830 --> 00:24:10,220
i have to keep moving because otherwise this will go to pieces i must be

312
00:24:10,240 --> 00:24:11,330
doing this

313
00:24:11,350 --> 00:24:12,780
all the time

314
00:24:12,790 --> 00:24:15,290
at that frequency which is exactly

315
00:24:15,350 --> 00:24:19,140
the resonance frequency of the single pendulum theta omega zero

316
00:24:19,140 --> 00:24:20,550
i must keep doing this

317
00:24:20,560 --> 00:24:21,550
this one

318
00:24:21,560 --> 00:24:23,370
there's nothing this one

319
00:24:23,510 --> 00:24:25,240
double the swing of this

320
00:24:25,290 --> 00:24:28,370
and is out of phase

321
00:24:28,560 --> 00:24:30,660
then you go even higher

322
00:24:30,680 --> 00:24:33,060
then you will see that the two two

323
00:24:33,100 --> 00:24:37,010
object will go on to face

324
00:24:37,330 --> 00:24:39,580
one will be in phase with the driver

325
00:24:39,580 --> 00:24:42,220
and no one will be out of phase with the drive and then you get

326
00:24:42,220 --> 00:24:43,830
that second residence

327
00:24:43,850 --> 00:24:46,530
when things get out of hand

328
00:24:46,580 --> 00:24:48,760
and you get that ratio

329
00:24:48,760 --> 00:24:55,930
minus o point four two back of course

330
00:24:56,010 --> 00:24:59,290
i want to demonstrate to you

331
00:24:59,310 --> 00:25:01,080
this situation here

332
00:25:01,100 --> 00:25:02,660
in this situation

333
00:25:02,700 --> 00:25:04,200
to see whether

334
00:25:04,260 --> 00:25:07,060
they make sense

335
00:25:07,120 --> 00:25:08,930
so now i'm going to

336
00:25:08,970 --> 00:25:11,830
user double pendulum

337
00:25:11,870 --> 00:25:12,930
and drive it

338
00:25:12,930 --> 00:25:14,280
with my

339
00:25:14,330 --> 00:25:17,220
o frequency which i determine

340
00:25:17,220 --> 00:25:18,550
i'm the boss

341
00:25:18,580 --> 00:25:20,030
i determine omega

342
00:25:20,050 --> 00:25:22,240
not looking for normal mode solutions

343
00:25:22,260 --> 00:25:23,950
i determine omega

344
00:25:23,990 --> 00:25:26,530
and i'm going to first

345
00:25:26,580 --> 00:25:28,580
private with omega zero

346
00:25:28,640 --> 00:25:30,930
was only guys is about zero

347
00:25:30,970 --> 00:25:32,450
in other words

348
00:25:32,490 --> 00:25:35,330
i'm going drive it

349
00:25:38,030 --> 00:25:39,720
what you're going to see

350
00:25:39,760 --> 00:25:41,410
is of course

351
00:25:47,850 --> 00:25:50,350
it's hanging straight down o

352
00:25:50,370 --> 00:25:51,950
and i'm moving it

353
00:25:51,990 --> 00:25:53,550
over distance

354
00:25:53,560 --> 00:25:54,990
one foot

355
00:25:55,130 --> 00:25:57,220
at the zero is one foot

356
00:25:57,390 --> 00:25:59,080
c one is wonderful

357
00:25:59,100 --> 00:26:00,760
and c two is one foot

358
00:26:00,760 --> 00:26:03,060
and there are in phase with the right

359
00:26:07,160 --> 00:26:09,760
if i go a little higher up here

360
00:26:09,890 --> 00:26:11,660
i go somewhere here

361
00:26:11,720 --> 00:26:13,510
approaching resonance

362
00:26:13,580 --> 00:26:17,240
then you will see that too becomes larger than c one

363
00:26:17,260 --> 00:26:19,370
this is to see why

364
00:26:19,370 --> 00:26:24,240
and you get to see a picture which is very much like this

365
00:26:24,290 --> 00:26:29,640
now try that

366
00:26:29,680 --> 00:26:33,470
drive it below but

367
00:26:33,510 --> 00:26:35,450
not too far below

368
00:26:35,530 --> 00:26:37,580
there you see

369
00:26:37,600 --> 00:26:39,470
c c one

370
00:26:39,510 --> 00:26:40,410
it's more

371
00:26:40,490 --> 00:26:42,060
c two

372
00:26:42,080 --> 00:26:44,990
no longer one was one this was one

373
00:26:45,010 --> 00:26:46,910
you see one of the two

374
00:26:46,910 --> 00:26:47,790
this one

375
00:26:47,810 --> 00:26:49,470
there is no longer the case

376
00:26:49,510 --> 00:26:51,740
really city two is going to go ahead

377
00:26:51,760 --> 00:26:53,760
i had nothing to interface

378
00:26:53,760 --> 00:26:55,330
but in terms of them

379
00:26:58,910 --> 00:27:01,390
now i'm going to attempt to do the impossible

380
00:27:01,390 --> 00:27:04,490
and the impossible is

381
00:27:04,600 --> 00:27:06,950
try to hit this point

382
00:27:06,990 --> 00:27:09,890
the point value of one standstill

383
00:27:09,930 --> 00:27:11,560
and rabbi

384
00:27:11,560 --> 00:27:12,680
the lower

385
00:27:12,720 --> 00:27:16,790
we'll have an amplitude which is twice that of my hands

386
00:27:16,850 --> 00:27:19,430
but out of phase with my

387
00:27:19,470 --> 00:27:21,810
how on earth can i have

388
00:27:21,810 --> 00:27:23,330
drive this system

389
00:27:23,350 --> 00:27:25,700
reason that frequency omega zero

390
00:27:25,720 --> 00:27:29,930
which is the frequency of a single plant

391
00:27:31,410 --> 00:27:33,160
maybe i can't

392
00:27:33,160 --> 00:27:37,850
but i will try and the rain i'm going to try this is the following

393
00:27:37,850 --> 00:27:38,580
i know

394
00:27:38,620 --> 00:27:41,740
the resonance frequency of a single pendulum is

395
00:27:41,790 --> 00:27:44,870
that's this

396
00:27:44,950 --> 00:27:47,430
i can feel it in my hands i can feel it in my stomach i

397
00:27:47,430 --> 00:27:52,050
can feel it in my brains i feel it all over my body

398
00:27:52,060 --> 00:27:56,240
i can confirm this frequency into my chips here

399
00:27:56,260 --> 00:27:59,490
and then i can close my eyes

400
00:27:59,550 --> 00:28:01,430
well you are looking

401
00:28:01,510 --> 00:28:04,870
and generate that frequency which was burned here

402
00:28:04,910 --> 00:28:07,930
and drive the system is the double pendulum

403
00:28:07,930 --> 00:28:09,160
if i succeed

404
00:28:09,180 --> 00:28:11,220
you see the article on standstill

405
00:28:11,240 --> 00:28:14,370
in the bottom one will have twice the amplitude of my

406
00:28:14,430 --> 00:28:17,550
so the success of this depends exclusively

407
00:28:17,600 --> 00:28:21,060
on how accurately i can run this frequency

408
00:28:21,100 --> 00:28:24,470
into my chips

409
00:28:24,510 --> 00:28:25,780
we have to be quite

410
00:28:25,790 --> 00:28:28,220
i'm going to come one

411
00:28:29,700 --> 00:28:31,010
three i'm burning up

412
00:28:41,470 --> 00:28:44,450
four closing my eyes one

413
00:29:08,660 --> 00:29:11,700
let's say anything you see this one sentence

414
00:29:11,830 --> 00:29:15,810
it to

415
00:29:15,850 --> 00:29:18,580
but you also see that the other one

416
00:29:18,600 --> 00:29:22,240
at twice the amplitude of my hands and out of phase with my hand you

417
00:29:22,240 --> 00:29:25,200
can see the right image you didn't see it

418
00:29:25,200 --> 00:29:26,930
because you're not looking for it

419
00:29:26,930 --> 00:29:29,240
especially for you i'll do it again

420
00:29:29,290 --> 00:29:33,140
so you have you really have to see

421
00:29:33,180 --> 00:29:37,180
number one and this one practically standstill number two

422
00:29:37,220 --> 00:29:40,030
that this one is double the amplitude of my hand

423
00:29:40,080 --> 00:29:43,390
but out of phase

424
00:29:43,450 --> 00:29:45,640
OK final earning

425
00:29:45,680 --> 00:29:49,470
is only one minute so i have to burn it again

426
00:30:03,600 --> 00:30:06,620
one way

427
00:30:06,640 --> 00:30:12,310
these things happen you have to start all over with pruning

428
00:30:12,370 --> 00:30:14,290
there we go

429
00:30:39,930 --> 00:30:42,370
i see it

430
00:30:52,370 --> 00:30:55,280
is it ideal moment before the break

431
00:30:57,160 --> 00:30:59,370
many queries

432
00:30:59,390 --> 00:31:02,890
and we will reconvene i'll give you six minutes this time

433
00:31:02,910 --> 00:31:05,680
so we could even stretch your legs

434
00:31:05,720 --> 00:31:07,370
and i would like some help

435
00:31:07,450 --> 00:31:09,280
and in this out

436
00:31:09,390 --> 00:31:12,930
and then you bring it back and put box is out there

437
00:31:12,950 --> 00:31:15,180
so if you can help me

438
00:31:15,180 --> 00:31:18,180
finding similar items we have we have s

439
00:31:18,220 --> 00:31:19,700
small data it's very easy so

440
00:31:20,550 --> 00:31:21,510
find similar

441
00:31:21,930 --> 00:31:22,550
data record

442
00:31:23,260 --> 00:31:24,050
let's say if have

443
00:31:25,180 --> 00:31:30,070
a hundred thousand documents and we get a new one we try to find similar wrongly

444
00:31:30,740 --> 00:31:32,720
brute force would work and that's it

445
00:31:33,140 --> 00:31:35,780
we have billions and billions then this would work so

446
00:31:36,410 --> 00:31:41,800
multidimensional indexing all data this is very important to just to access and together similar

447
00:31:41,800 --> 00:31:44,510
items so there's a whole set of classifiers that

448
00:31:45,490 --> 00:31:48,640
incremental updating of models in the small that the world

449
00:31:49,450 --> 00:31:53,510
we took the data we build the model and that was it then when the data changes we

450
00:31:54,050 --> 00:31:57,340
just changed a little bit just ran the whole thing and was okay

451
00:31:57,890 --> 00:32:02,880
here with the big data we cannot afford it so we we need to be incremental so small

452
00:32:03,640 --> 00:32:04,680
changing data and

453
00:32:05,390 --> 00:32:09,450
we know that produces small change in the model and the algorithm is a support this

454
00:32:10,360 --> 00:32:11,760
so this is incremental updating

455
00:32:14,050 --> 00:32:18,110
it's very fundamental set of techniques so linear algebra which you'll probably all

456
00:32:18,990 --> 00:32:19,530
know from

457
00:32:20,740 --> 00:32:21,930
undergraduate courses

458
00:32:22,640 --> 00:32:24,010
so need to be properly

459
00:32:26,010 --> 00:32:27,820
you know this this uh

460
00:32:27,860 --> 00:32:32,050
and its surrounding this distributed fashion and this is this is what google has sought is big

461
00:32:33,280 --> 00:32:34,110
search engines

462
00:32:35,090 --> 00:32:37,780
many of these big the data providers

463
00:32:38,610 --> 00:32:39,180
so these are

464
00:32:39,700 --> 00:32:40,050
type of

465
00:32:40,950 --> 00:32:42,180
operators which we need to

466
00:32:42,720 --> 00:32:45,220
happened then of course then we can deal with

467
00:32:46,970 --> 00:32:52,160
a bit more like traditional way of dealing with the data so the public this previous operators we just

468
00:32:52,860 --> 00:32:57,110
i do the usual stuff that's in the data mining or machine learning sense this would be

469
00:32:57,950 --> 00:33:04,450
supervised learning and unsupervised or semi-supervised is that traditional machine learning classification regression and so on so

470
00:33:05,070 --> 00:33:08,970
if you have these operators properly implemented on distributed infrastructures them

471
00:33:09,410 --> 00:33:11,570
everything goes is then the same so this is

472
00:33:15,090 --> 00:33:16,360
that's why i said the beginning

473
00:33:18,430 --> 00:33:20,220
big data this almost the same as

474
00:33:20,740 --> 00:33:21,260
small that

475
00:33:21,890 --> 00:33:24,860
so the same intuitions that we need to be careful about the scale

476
00:33:32,280 --> 00:33:33,220
to learn about the

477
00:33:33,660 --> 00:33:35,430
these techniques which i just mentioned

478
00:33:36,880 --> 00:33:37,360
this book

479
00:33:37,840 --> 00:33:40,160
recently came out is downloadable from the vet

480
00:33:40,840 --> 00:33:43,180
mining of massive data sets so it's

481
00:33:43,640 --> 00:33:46,340
from geoffrey ullman ants coauthor

482
00:33:49,430 --> 00:33:50,950
it goes through all this

483
00:33:51,660 --> 00:33:53,360
segments and provides good

484
00:33:54,140 --> 00:33:58,450
good leads at least hard to design algorithms are to actually algorithms

485
00:33:59,930 --> 00:34:01,680
so we can learn a lot from here

486
00:34:02,890 --> 00:34:03,300
and this is

487
00:34:03,950 --> 00:34:04,300
by my

488
00:34:04,700 --> 00:34:05,840
knowledge the only book

489
00:34:07,010 --> 00:34:07,950
which is technical

490
00:34:08,550 --> 00:34:11,840
which is technical and the great thing sites nowadays on the market

491
00:34:15,090 --> 00:34:17,950
okay so we have just a few more minutes savages growth

492
00:34:18,410 --> 00:34:20,970
quickly through some up of applications which

493
00:34:21,450 --> 00:34:22,820
are happened in my

494
00:34:23,530 --> 00:34:25,140
circle are which we were

495
00:34:26,620 --> 00:34:29,030
princes one big data application which

496
00:34:29,530 --> 00:34:30,510
with it was

497
00:34:31,220 --> 00:34:36,700
so far bloomberg website recommendations so the open article on bloomberg com

498
00:34:37,430 --> 00:34:39,970
would get this set of recommendations which

499
00:34:40,590 --> 00:34:42,010
we generate based on

500
00:34:43,070 --> 00:34:47,110
who is reading what is reading what was history of this person and so on

501
00:34:47,110 --> 00:34:49,490
and this happens like two hundred times per second

502
00:34:54,430 --> 00:34:55,360
why recommendations

503
00:34:55,820 --> 00:34:58,990
important because good recommendations can be like

504
00:34:58,990 --> 00:35:01,020
and father on

505
00:35:01,070 --> 00:35:04,590
the final step

506
00:35:04,600 --> 00:35:06,200
it can be done

507
00:35:06,250 --> 00:35:09,680
two weeklies emotions the visual expression

508
00:35:09,700 --> 00:35:12,120
this would be the one

509
00:35:12,380 --> 00:35:15,910
he should step another told not to

510
00:35:15,930 --> 00:35:16,970
to this

511
00:35:16,980 --> 00:35:17,840
two tool

512
00:35:17,860 --> 00:35:23,950
two slide the the slidingwindow on all the possible locations some more some improvements can

513
00:35:23,950 --> 00:35:26,090
be the process of applying

514
00:35:26,110 --> 00:35:29,890
some of the point to point detectors

515
00:35:29,910 --> 00:35:34,920
according to the corner detectors like we use a commission for

516
00:35:34,930 --> 00:35:39,540
cases like

517
00:35:39,590 --> 00:35:46,050
and this is what what we actually use

518
00:35:46,320 --> 00:35:48,730
OK so

519
00:35:48,750 --> 00:35:50,100
this would be

520
00:35:50,140 --> 00:35:55,430
if i have more than two three minutes and i can also say something about

521
00:35:55,870 --> 00:35:59,700
the evolution of emotions from from the speech

522
00:35:59,710 --> 00:36:01,090
OK so i

523
00:36:01,100 --> 00:36:05,200
this is one of the approaches that we

524
00:36:05,560 --> 00:36:08,570
so far we took another two

525
00:36:11,220 --> 00:36:17,830
to create to develop such a system automatic system for for legalizing the emotions speech

526
00:36:25,870 --> 00:36:29,610
i wanted to find out which part of the

527
00:36:29,620 --> 00:36:31,520
one of the responses

528
00:36:31,540 --> 00:36:35,450
is the most important for four

529
00:36:36,800 --> 00:36:40,400
giving the dominant emotion from the ottomans

530
00:36:40,440 --> 00:36:42,830
that maybe

531
00:36:43,660 --> 00:36:44,970
we speak something

532
00:36:44,990 --> 00:36:48,350
maybe we say something about this

533
00:36:48,360 --> 00:36:51,310
we want to know exactly what you want

534
00:36:51,320 --> 00:36:52,030
what are the

535
00:36:52,040 --> 00:36:58,470
keep he or you what is the key information that gives the relevant information that

536
00:37:02,260 --> 00:37:05,320
their emotions from one of

537
00:37:05,330 --> 00:37:10,980
we initially had some of databases

538
00:37:11,030 --> 00:37:14,850
these buses we used building dataset

539
00:37:17,730 --> 00:37:25,480
one is for german language another one is for the image they were actually recorded

540
00:37:25,480 --> 00:37:28,350
and mating for this one

541
00:37:28,360 --> 00:37:31,190
they have this for us to be used for

542
00:37:31,200 --> 00:37:37,720
in this kind of research is sold their of the content contained

543
00:37:38,340 --> 00:37:42,860
o audio speech and those the labels for the emotions

544
00:37:42,870 --> 00:37:45,000
we use the gentleman

545
00:37:46,770 --> 00:37:52,280
with maximum two hundred training sets so it is something that at each step it

546
00:37:53,520 --> 00:37:57,720
the the most relevant features

547
00:37:58,870 --> 00:38:01,040
that that represented the

548
00:38:01,050 --> 00:38:03,130
that level

549
00:38:05,240 --> 00:38:06,720
it that

550
00:38:06,740 --> 00:38:10,870
not only do that can differentiate well

551
00:38:12,090 --> 00:38:13,760
emotional plants

552
00:38:14,830 --> 00:38:19,330
for evaluation we use ROC rock graphs

553
00:38:19,350 --> 00:38:23,510
i think this would be one very efficiently

554
00:38:24,550 --> 00:38:27,090
making this information was

555
00:38:27,110 --> 00:38:30,200
when the databases are

556
00:38:30,230 --> 00:38:31,840
on the side other

557
00:38:32,550 --> 00:38:35,810
the structure of the data set

558
00:38:36,760 --> 00:38:40,880
from the structure of the data see that for instance one class

559
00:38:40,910 --> 00:38:42,830
we have lists

560
00:38:42,830 --> 00:38:47,360
let's so much sort of the simplest form

561
00:38:49,120 --> 00:38:50,810
simplicity before

562
00:38:50,820 --> 00:38:52,340
for another class then

563
00:38:52,550 --> 00:38:57,010
it's good to make an in-depth analyses for

564
00:38:57,020 --> 00:38:59,710
in terms of evaluation

565
00:38:59,740 --> 00:39:00,780
so these are

566
00:39:04,310 --> 00:39:07,820
for one of the things we split

567
00:39:11,260 --> 00:39:17,690
of the ten different segments and make light of segmentation we want to see

568
00:39:17,710 --> 00:39:19,630
which frame gives

569
00:39:20,300 --> 00:39:24,890
is a representative for the motion of the all utterance

570
00:39:24,910 --> 00:39:29,460
so we speak in light of the interval and we took a so one

571
00:39:29,600 --> 00:39:31,710
the order of ten

572
00:39:31,730 --> 00:39:37,070
singh was split into three five and so on

573
00:39:37,070 --> 00:39:38,290
and in total

574
00:39:38,310 --> 00:39:43,880
for each such state each such a combination we

575
00:39:44,010 --> 00:39:45,780
this is the information

576
00:39:47,180 --> 00:39:49,900
specific database

577
00:39:49,910 --> 00:39:51,860
so we have in then the

578
00:39:51,860 --> 00:39:53,390
one thousand

579
00:39:53,410 --> 00:39:57,010
sixty four sixty five datasets

580
00:39:58,490 --> 00:40:00,310
i spent thirty and they take

581
00:40:03,490 --> 00:40:05,300
the values of the features

582
00:40:06,740 --> 00:40:09,370
only from the first and third

583
00:40:10,580 --> 00:40:15,770
for between three and they take information about the features one from the first and

584
00:40:15,780 --> 00:40:20,610
the third because if i find this to be

585
00:40:20,660 --> 00:40:23,910
the most relevant again among other

586
00:40:23,930 --> 00:40:29,570
combinations of spitting then i see the first and the third frame

587
00:40:29,580 --> 00:40:34,620
friends would give me the most eleven one four four

588
00:40:34,630 --> 00:40:41,080
it lies in the emotions we used some parameters

589
00:40:41,440 --> 00:40:42,480
we use

590
00:40:42,990 --> 00:40:45,990
and five cross validation they need

591
00:40:46,010 --> 00:40:48,880
we the again one to the rest

592
00:40:51,100 --> 00:40:53,170
and then you see some

593
00:40:53,190 --> 00:40:54,610
the result

594
00:40:54,610 --> 00:40:58,660
we use the so

595
00:40:58,670 --> 00:41:00,360
of gender-based

596
00:41:01,080 --> 00:41:02,820
o analyses i mean

597
00:41:03,060 --> 00:41:05,550
we took separately made

598
00:41:05,550 --> 00:41:07,550
lectures supplement which goes

599
00:41:07,590 --> 00:41:12,630
four innovation step by step which will convince you that indeed

600
00:41:12,630 --> 00:41:13,760
this is

601
00:41:13,780 --> 00:41:17,150
what is happening

602
00:41:17,150 --> 00:41:21,740
why we can't digest this so easily is we don't know how to handle non

603
00:41:21,740 --> 00:41:24,490
conservative fields

604
00:41:24,590 --> 00:41:27,450
if you have a non-conservative field

605
00:41:27,490 --> 00:41:31,510
then if you go from a to d

606
00:41:31,570 --> 00:41:33,050
of the adult yell

607
00:41:33,150 --> 00:41:38,430
or from the two a for that matter doesn't matter

608
00:41:38,490 --> 00:41:42,300
the answer depends on the path

609
00:41:42,380 --> 00:41:46,130
it's no longer independent of the path

610
00:41:46,220 --> 00:41:47,360
and so on

611
00:41:47,380 --> 00:41:48,840
if there is the

612
00:41:48,860 --> 00:41:51,030
and here is a

613
00:41:51,070 --> 00:41:52,510
you go this way

614
00:41:52,590 --> 00:41:55,300
you find o point nine four class

615
00:41:55,320 --> 00:41:56,780
if you go this way

616
00:41:56,800 --> 00:42:00,740
find is o point one four five they has no problem with that the is

617
00:42:00,740 --> 00:42:02,530
what has the problem with that

618
00:42:02,550 --> 00:42:04,450
who cares about his role

619
00:42:04,490 --> 00:42:06,920
five the more that matters

620
00:42:06,990 --> 00:42:11,700
because fire there is not always holds because if we find it zero

621
00:42:11,760 --> 00:42:16,690
then again he sells his house rule is simply a special case of fire this

622
00:42:16,690 --> 00:42:20,240
law and fire this will always holds

623
00:42:20,240 --> 00:42:22,280
the took each of is for the birds

624
00:42:22,360 --> 00:42:27,880
and and fire they is not

625
00:42:27,900 --> 00:42:29,880
suppose you go

626
00:42:29,880 --> 00:42:30,780
from the

627
00:42:30,800 --> 00:42:34,300
eight and back to b

628
00:42:35,380 --> 00:42:36,450
we know that

629
00:42:36,470 --> 00:42:38,380
if you do minus three eight

630
00:42:38,430 --> 00:42:40,800
if we go

631
00:42:42,570 --> 00:42:44,630
if we go this way

632
00:42:44,690 --> 00:42:46,570
two are two

633
00:42:46,650 --> 00:42:49,150
we note in three d minus the a

634
00:42:49,170 --> 00:42:52,300
plus o point nine holes

635
00:42:52,430 --> 00:42:54,970
now we are in a

636
00:42:54,970 --> 00:42:58,320
and we go through the left side back to d

637
00:42:58,320 --> 00:43:01,070
so we now have a minus b b

638
00:43:01,090 --> 00:43:05,880
that of course is now plus o point one four because remember

639
00:43:05,970 --> 00:43:08,900
if you do you mind a mine o point one

640
00:43:08,970 --> 00:43:11,610
NVA minus plus

641
00:43:11,610 --> 00:43:12,900
so we add them up

642
00:43:13,070 --> 00:43:15,950
and we find that the d

643
00:43:15,950 --> 00:43:17,510
mine in three d

644
00:43:17,550 --> 00:43:20,490
is plus-one vote

645
00:43:20,530 --> 00:43:22,970
this half said has to be zero

646
00:43:22,970 --> 00:43:26,510
because i'm back at the same potential where i was before

647
00:43:26,530 --> 00:43:28,200
fireplaces are

648
00:43:28,200 --> 00:43:29,840
i'm sorry

649
00:43:29,900 --> 00:43:31,150
you can do that

650
00:43:31,150 --> 00:43:34,900
that's one fold is exactly that EMF one

651
00:43:34,950 --> 00:43:39,030
that is the closed loop moving the goal of each LTL around the globe

652
00:43:39,050 --> 00:43:40,240
it's no longer

653
00:43:42,300 --> 00:43:45,840
and therefore whenever you define potential difference if you do that

654
00:43:45,860 --> 00:43:47,700
in the way of the

655
00:43:47,760 --> 00:43:49,550
the integral of e dot pl

656
00:43:49,570 --> 00:43:52,240
keep in mind that was non-conservative fields

657
00:43:52,260 --> 00:43:54,010
it depends on the path

658
00:43:54,030 --> 00:43:55,590
and that is very normal

659
00:43:57,320 --> 00:43:59,990
and i'm going to demonstrate is now to you

660
00:44:00,010 --> 00:44:03,260
i have a circuit which is exactly what you have here

661
00:44:03,260 --> 00:44:05,130
five nine hundred falls

662
00:44:05,190 --> 00:44:07,070
in a

663
00:44:08,420 --> 00:44:12,010
copper wire year nine hundred twelve UL here is the

664
00:44:12,010 --> 00:44:15,360
so all in all we can switch the current in the solenoid going to blast

665
00:44:17,570 --> 00:44:18,920
of coming up

666
00:44:18,930 --> 00:44:21,150
and the system is going to react

667
00:44:21,240 --> 00:44:23,130
by driving a current

668
00:44:23,130 --> 00:44:25,110
in the direction that you

669
00:44:25,130 --> 00:44:26,880
see there

670
00:44:26,930 --> 00:44:29,610
i'd like to be even a little bit more

671
00:44:29,630 --> 00:44:32,380
quantitative studies you get a little bit

672
00:44:32,420 --> 00:44:36,110
more for your money

673
00:44:36,110 --> 00:44:39,760
the magnetic field

674
00:44:39,780 --> 00:44:42,380
thanks for the element of time

675
00:44:42,400 --> 00:44:45,700
to reach the maximum value in this course

676
00:44:45,860 --> 00:44:48,590
we will be able to

677
00:44:48,800 --> 00:44:50,670
he made the time that it takes

678
00:44:50,670 --> 00:44:53,240
for the magnetic field to build up

679
00:44:53,260 --> 00:44:56,090
we didn't get to that you have to forget that part is not so important

680
00:44:56,260 --> 00:44:57,380
i just want you to

681
00:44:57,430 --> 00:44:59,220
i appreciate the fact

682
00:45:00,010 --> 00:45:01,490
the magnetic fields

683
00:45:01,530 --> 00:45:03,720
as a function of time

684
00:45:03,760 --> 00:45:05,700
will come up like this

685
00:45:05,740 --> 00:45:08,110
and will then reach maximum

686
00:45:08,130 --> 00:45:12,010
no longer changing is constant is the maximum value is very high seventy eight from

687
00:45:12,010 --> 00:45:14,490
the guy so for this unit

688
00:45:14,530 --> 00:45:18,070
we are not interested in the magnetic field we're interested in the change of the

689
00:45:18,070 --> 00:45:19,900
magnetic fields

690
00:45:19,970 --> 00:45:23,300
so the change of the magnetic field b

691
00:45:23,360 --> 00:45:26,300
it's going to be something like this

692
00:45:27,740 --> 00:45:29,380
of this curve

693
00:45:29,430 --> 00:45:31,190
and that is proportional

694
00:45:31,190 --> 00:45:33,130
with the induced EMF

695
00:45:33,150 --> 00:45:39,150
and that's important for proportional with the current to flow

696
00:45:39,320 --> 00:45:41,280
if we now plot

697
00:45:41,300 --> 00:45:43,050
the voltage

698
00:45:43,050 --> 00:45:45,950
this figure actually illustrates this kind of

699
00:45:45,950 --> 00:45:50,800
distribution some nodes are highly connected have so many edges as compared to other nodes

700
00:45:50,800 --> 00:45:56,390
which have very few edges

701
00:45:56,430 --> 00:46:01,140
and the next one is a hybrid model which is a mixture of preferential attachment

702
00:46:01,380 --> 00:46:03,830
and around the model so

703
00:46:03,850 --> 00:46:08,470
here we try to give other people some chance to get creature

704
00:46:08,500 --> 00:46:12,620
there's also tries to solve irreducibility which is physically strong connectedness

705
00:46:12,650 --> 00:46:18,670
with fewer are subgraphs by basically providing random walk and random jump to non connected

706
00:46:18,680 --> 00:46:22,670
or isolated nodes

707
00:46:22,720 --> 00:46:26,170
and then there are some other modern summer study temporal patterns

708
00:46:26,190 --> 00:46:31,730
like how often people create blog posts hopelessness and popular this blog is

709
00:46:31,790 --> 00:46:34,760
how these posts are linked and modelling density

710
00:46:34,770 --> 00:46:39,500
this is basically the develops SIS model which is susceptible

711
00:46:39,550 --> 00:46:44,780
on infected and susceptible models

712
00:46:44,790 --> 00:46:47,740
there's another model which uses etc

713
00:46:48,220 --> 00:46:55,560
generate network within blogs which is increasingly the blog have links have the articles blogs

714
00:46:55,560 --> 00:46:59,480
which are related to this blog so that's assumption and then they use this kind

715
00:46:59,480 --> 00:47:04,720
of assumption to create a network of blogs

716
00:47:04,850 --> 00:47:07,730
i don't want to block clustering

717
00:47:07,750 --> 00:47:10,580
as you can see in this figure it is basically

718
00:47:12,220 --> 00:47:16,670
the blogsphere for bush in politics and

719
00:47:16,740 --> 00:47:23,160
different things so you can see they have they have cluster different blogs and persian

720
00:47:23,160 --> 00:47:27,920
domain and you can see different clusters of blogs lay

721
00:47:28,800 --> 00:47:33,420
all it takes on different things and the kind of according

722
00:47:33,420 --> 00:47:36,330
o signifies the relatedness of these different topics

723
00:47:36,450 --> 00:47:39,830
so if you have you can see persian poetry and politics as far apart as

724
00:47:39,830 --> 00:47:41,040
compared to

725
00:47:41,100 --> 00:47:45,100
if reformist politics and conservative politics

726
00:47:45,120 --> 00:47:48,710
so it's a very good nice we to visualize different blogs

727
00:47:48,740 --> 00:47:54,200
and how did they at hopes so different there

728
00:47:54,220 --> 00:48:00,350
so basically you can use doppler strings that immediately and automatically organize the content it

729
00:48:00,350 --> 00:48:05,650
also helps inconvenient accessibility by optimizing search engines because you don't have to search the

730
00:48:05,650 --> 00:48:08,820
whole block just the cluster in that case

731
00:48:08,850 --> 00:48:14,010
you also can no focused crawling based on the different clusters you have and then

732
00:48:14,010 --> 00:48:17,980
it also helps in summarisation of different topics have different clusters you can also identify

733
00:48:18,100 --> 00:48:24,670
topics of these different clusters and this can also help in extraction analysis of trends

734
00:48:24,720 --> 00:48:30,790
one motivation for this is to use the information because there are so many blog

735
00:48:30,790 --> 00:48:32,480
posts appeared per second

736
00:48:32,510 --> 00:48:35,740
so we don't want you don't have that kind of thing that was region and

737
00:48:35,760 --> 00:48:40,110
blog posts so you just go through the relevant which are interested in then read

738
00:48:40,120 --> 00:48:44,610
the blogs

739
00:48:44,620 --> 00:48:46,520
some of the techniques which

740
00:48:46,550 --> 00:48:50,210
he string they use the PFID if

741
00:48:50,250 --> 00:48:52,220
they find

742
00:48:52,240 --> 00:48:56,600
three key words from the blogs and then the class of these models based on

743
00:48:56,600 --> 00:49:03,180
these keywords have compared this to david are fact based clustering and reported some improvements

744
00:49:03,190 --> 00:49:06,370
over the fact that it's really

745
00:49:06,370 --> 00:49:10,220
then some of the research they also try to to assign different weights to different

746
00:49:10,220 --> 00:49:15,460
components of blogs like different weights to title body and comments of blog posts

747
00:49:15,510 --> 00:49:19,220
but they need to address high dimensionality and sparsity to do

748
00:49:19,230 --> 00:49:21,870
keyword based approaches

749
00:49:21,880 --> 00:49:22,700
is it

750
00:49:22,710 --> 00:49:27,040
from group which focuses on collective based approach

751
00:49:27,120 --> 00:49:33,020
it uses generates category relation graphs based on the user assignments of tags and categories

752
00:49:33,020 --> 00:49:35,390
and then using that now we compute

753
00:49:35,390 --> 00:49:36,930
similarity matrix

754
00:49:36,950 --> 00:49:40,980
between the categories and then this result in a graph

755
00:49:41,000 --> 00:49:44,580
and then we use this graph to cluster different of this is recovered later in

756
00:49:44,580 --> 00:49:53,000
the second part of the tutorial in much needed

757
00:49:53,040 --> 00:49:55,180
now let's move on to

758
00:49:55,180 --> 00:49:56,620
blog mining

759
00:49:56,640 --> 00:49:58,410
it is basically

760
00:49:58,430 --> 00:50:01,460
the interaction between producers and consumers

761
00:50:01,480 --> 00:50:03,480
so nowadays

762
00:50:05,040 --> 00:50:10,810
not to mention that there are differences as from the two point o environment that

763
00:50:10,810 --> 00:50:13,790
you have so many are so much interaction between

764
00:50:13,810 --> 00:50:15,520
consumers so

765
00:50:15,520 --> 00:50:20,930
this has really changed the way of previous appeasement would used to be done for

766
00:50:20,930 --> 00:50:25,530
to all the matrix form and to the recursion and everything

767
00:50:25,550 --> 00:50:28,640
and and you can see that it's

768
00:50:28,660 --> 00:50:33,050
lots of applications are blocked right diagonal the new state

769
00:50:33,080 --> 00:50:37,970
comes from the old state but it doesn't come from ten old state

770
00:50:37,990 --> 00:50:41,550
or doesn't go all the way back to the very beginning state

771
00:50:41,580 --> 00:50:45,080
it's just it's connected to the previous

772
00:50:45,100 --> 00:50:49,800
it's like you know we have a differential equation or a difference equation connecting new

773
00:50:49,800 --> 00:50:53,030
state it will one i'll come back to that

774
00:50:53,060 --> 00:51:00,470
in the in the second lecture course when we really get that OK

775
00:51:00,470 --> 00:51:02,700
i just don't want

776
00:51:02,870 --> 00:51:05,890
all i can what i can try to do in these

777
00:51:05,890 --> 00:51:08,410
like his is the highlight

778
00:51:08,490 --> 00:51:13,140
what seems like to me the key points

779
00:51:13,140 --> 00:51:16,140
we're going to need we're going to have formulas and i will get them all

780
00:51:16,140 --> 00:51:17,410
on the board

781
00:51:21,140 --> 00:51:22,990
but those can be looked at

782
00:51:23,030 --> 00:51:26,890
those would be in this chapter the goes on the web and and there are

783
00:51:26,950 --> 00:51:30,370
many other descriptions of them in the references

784
00:51:30,390 --> 00:51:33,200
and i have begun to give all the references

785
00:51:33,240 --> 00:51:35,260
but the idea is

786
00:51:35,370 --> 00:51:37,180
are always not so many

787
00:51:37,200 --> 00:51:38,700
and one of them is

788
00:51:38,720 --> 00:51:41,120
blocked right diag

789
00:51:41,120 --> 00:51:45,160
there blocks because

790
00:51:47,200 --> 00:51:48,740
new the new

791
00:51:48,760 --> 00:51:50,510
the new

792
00:51:50,780 --> 00:51:53,240
in this recursive stuff

793
00:51:53,240 --> 00:51:57,160
this is a block of bees is another block b

794
00:51:57,280 --> 00:52:00,660
this is a block of a this is the rest of the

795
00:52:00,720 --> 00:52:03,330
this is the block picture

796
00:52:03,370 --> 00:52:09,240
you want me to write that as of what picture

797
00:52:10,550 --> 00:52:13,330
the block matrix is a all

798
00:52:13,350 --> 00:52:14,530
a new

799
00:52:15,680 --> 00:52:17,550
and it multiplies u

800
00:52:17,550 --> 00:52:18,950
and then you use

801
00:52:18,970 --> 00:52:24,720
not changing like that we have a newer and newer number and no block

802
00:52:24,740 --> 00:52:26,050
in the recursive

803
00:52:26,050 --> 00:52:26,890
set up

804
00:52:26,890 --> 00:52:29,200
we're so keeping the same you

805
00:52:29,260 --> 00:52:30,870
in the kalman set up

806
00:52:30,910 --> 00:52:35,470
we're also getting a new you and you are you on a brand new you

807
00:52:35,470 --> 00:52:36,320
so the

808
00:52:36,370 --> 00:52:39,080
that's the difference here the recursive

809
00:52:39,200 --> 00:52:42,240
if you could say that in matrix language

810
00:52:42,300 --> 00:52:47,700
recursively squares were adding new rose

811
00:52:47,720 --> 00:52:49,680
in dynamic problems

812
00:52:49,680 --> 00:52:53,560
we're adding new parameters or adding new columns too

813
00:52:53,620 --> 00:52:56,140
we adding new rows and columns

814
00:52:56,160 --> 00:53:00,830
so as i say i write that down to get on the board recursive

815
00:53:00,870 --> 00:53:03,330
this stuff is new rose

816
00:53:03,370 --> 00:53:04,890
new rose

817
00:53:06,720 --> 00:53:08,870
and the dynamics is new

818
00:53:08,910 --> 00:53:12,510
columns two

819
00:53:12,600 --> 00:53:13,680
new unknown

820
00:53:17,030 --> 00:53:21,120
but this lecture is about new roses about this problem

821
00:53:21,180 --> 00:53:23,430
is about this problem so now i'm ready to

822
00:53:23,450 --> 00:53:25,580
turn to that problem

823
00:53:25,640 --> 00:53:29,060
but first i didn't make the

824
00:53:29,080 --> 00:53:32,330
the most important comment about this

825
00:53:32,560 --> 00:53:35,600
never read such a long ways to work

826
00:53:35,660 --> 00:53:37,680
as this one for sigma

827
00:53:39,660 --> 00:53:45,510
which is turned out to be PE which turned out to have this need answer

828
00:53:45,550 --> 00:53:48,120
what's the

829
00:53:48,490 --> 00:53:56,140
the point i haven't made but the it's it's the established in the textbook

830
00:53:56,140 --> 00:53:58,950
is the reason for this choice

831
00:53:58,970 --> 00:54:00,760
the reason that choice

832
00:54:01,510 --> 00:54:03,350
the right one

833
00:54:03,350 --> 00:54:05,950
is that it makes

834
00:54:06,010 --> 00:54:08,080
this matrix p

835
00:54:08,080 --> 00:54:10,640
as small as possible

836
00:54:10,680 --> 00:54:13,660
we get the best estimate

837
00:54:13,680 --> 00:54:19,160
in the sense that the variance is is the the the the the variance covariance

838
00:54:19,160 --> 00:54:21,490
matrix is the smallest possible

839
00:54:21,510 --> 00:54:24,510
by making this choice of c

840
00:54:24,560 --> 00:54:30,620
this choice are this choice of c this by taking that particular weighting matrix it

841
00:54:30,620 --> 00:54:34,720
produced this answer and we couldn't have got smaller

842
00:54:34,720 --> 00:54:37,830
we couldn't have gotten smaller sigma u

843
00:54:37,870 --> 00:54:40,280
than that

844
00:54:40,300 --> 00:54:44,180
and that's that

845
00:54:44,200 --> 00:54:46,870
that's one way to say why it that

846
00:54:46,890 --> 00:54:49,430
so there is an optimal property of this

847
00:54:49,430 --> 00:54:50,890
this way

848
00:54:50,950 --> 00:54:53,030
and that's one way to

849
00:54:54,300 --> 00:54:55,780
the choice of that

850
00:54:55,800 --> 00:54:57,850
that is there waiting make

851
00:55:01,970 --> 00:55:05,780
i'm ready to tackle this

852
00:55:05,800 --> 00:55:07,680
this recursion

853
00:55:08,740 --> 00:55:10,320
so what's the idea

854
00:55:10,370 --> 00:55:12,550
we solve the old problem

855
00:55:12,600 --> 00:55:13,620
then we get

856
00:55:13,640 --> 00:55:15,870
new measurements

857
00:55:15,930 --> 00:55:18,180
we have

858
00:55:19,600 --> 00:55:23,780
part of the corresponding new part of the matrix a and i want to get

859
00:55:23,780 --> 00:55:26,620
i want to solve the problem again

860
00:55:26,660 --> 00:55:31,300
and the point is i want to solve the problem again

861
00:55:31,350 --> 00:55:34,410
i don't want to start from the beginning

862
00:55:34,470 --> 00:55:36,890
that's the that's the recursive idea

863
00:55:36,910 --> 00:55:39,370
i would like to be able to say

864
00:55:39,390 --> 00:55:41,600
i would like to be able to use my

865
00:55:41,620 --> 00:55:48,450
the calculation already made

866
00:55:48,490 --> 00:55:49,950
so i would like to

867
00:55:49,990 --> 00:55:52,800
have update formula of this time

868
00:55:52,850 --> 00:55:56,490
i would like to see the best u

869
00:55:56,510 --> 00:55:57,620
for that

870
00:55:57,660 --> 00:55:59,050
combined system

871
00:55:59,050 --> 00:56:00,640
as the best u

872
00:56:00,640 --> 00:56:02,510
for the original system

873
00:56:03,600 --> 00:56:06,740
some matrix and that's going to be

874
00:56:07,680 --> 00:56:09,800
the kalman gain matrix

875
00:56:09,830 --> 00:56:12,060
multiplying this quantity

876
00:56:12,120 --> 00:56:14,740
now there's that seem like

877
00:56:14,740 --> 00:56:16,240
reasonable goal

878
00:56:16,280 --> 00:56:17,990
before we

879
00:56:18,030 --> 00:56:19,950
reach let's see if it's

880
00:56:19,990 --> 00:56:22,510
let's see what we're aiming for

881
00:56:22,700 --> 00:56:24,060
you will

882
00:56:24,410 --> 00:56:28,700
what happen if

883
00:56:28,760 --> 00:56:30,640
what would happen if

884
00:56:31,740 --> 00:56:34,870
these new measurements were perfect repeat

885
00:56:34,930 --> 00:56:38,850
and this was the repeat if if you know is the same as a old

886
00:56:38,850 --> 00:56:41,370
and the new is the same as the old

887
00:56:41,430 --> 00:56:45,240
so we just ran experiment twice

888
00:56:45,300 --> 00:56:48,820
or just copy the answer and notice or something

889
00:56:48,820 --> 00:56:53,740
what would we expect you knew the perfect the best you to be for the

890
00:56:55,510 --> 00:56:57,370
maybe the same right

891
00:56:57,390 --> 00:57:00,620
and you see that that's what happened with this formula

892
00:57:00,640 --> 00:57:02,280
that you know would be you all

893
00:57:05,060 --> 00:57:06,410
would be

894
00:57:06,410 --> 00:57:09,380
if you still those results you can you can use them

895
00:57:10,370 --> 00:57:11,270
they are

896
00:57:11,280 --> 00:57:13,270
there's an efficient implementation

897
00:57:13,270 --> 00:57:18,020
as you know colorado university them so it's being used

898
00:57:18,030 --> 00:57:27,000
so people are using these things for solving in many NP hard problems like satisfiability

899
00:57:27,000 --> 00:57:32,160
problems and so forth this representation giving them phenomenal speed ups

900
00:57:32,580 --> 00:57:37,800
and scheduling is done using these representations in typical scheduling problems

901
00:57:37,860 --> 00:57:41,790
and they're using in computer vision that out and all that

902
00:57:41,830 --> 00:57:46,380
i mean it's data mining also they have not been used

903
00:57:46,430 --> 00:57:52,060
OK in the case of zero suppressed binary decision diagrams what you're trying to do

904
00:57:52,180 --> 00:57:53,330
is that

905
00:57:53,350 --> 00:57:57,100
it's a it's variance

906
00:57:57,110 --> 00:57:59,010
it's a variation of BDD

907
00:57:59,450 --> 00:58:01,500
and what you do is you simply

908
00:58:01,540 --> 00:58:06,120
ignore part of the negative log and we'll see how this happens

909
00:58:06,130 --> 00:58:09,690
so first thing is you have to impose an order in all of these things

910
00:58:09,770 --> 00:58:12,600
and the interesting thing is the order

911
00:58:12,610 --> 00:58:15,970
that means which is the root node which is the next node

912
00:58:16,010 --> 00:58:20,130
can can make a huge difference in terms of the complexity of the representation

913
00:58:20,180 --> 00:58:22,930
so like i told you here earlier right

914
00:58:22,940 --> 00:58:27,200
by having a certain or you can get a very compressed three so one of

915
00:58:27,200 --> 00:58:30,060
the things you want do is that you want to have a long speak and

916
00:58:30,060 --> 00:58:31,540
the brush like this

917
00:58:31,570 --> 00:58:32,930
like like a brush

918
00:58:32,930 --> 00:58:35,400
that means lord branching have like that

919
00:58:35,440 --> 00:58:37,860
that a very efficient representation

920
00:58:37,910 --> 00:58:42,010
and if it is brought together where then then you have a lot of stories

921
00:58:42,010 --> 00:58:47,850
required and not much action take place that's why choosing the right kind of frequencies

922
00:58:47,910 --> 00:58:52,430
can make it might make an interesting thing so my belief is low density estimation

923
00:58:52,430 --> 00:58:55,930
should be done quite nicely using these kinds of structures

924
00:58:55,980 --> 00:58:59,840
so here is a as the representation of from

925
00:58:59,870 --> 00:59:01,540
some data

926
00:59:01,560 --> 00:59:04,400
so here we have let's say

927
00:59:04,590 --> 00:59:07,260
one instance which has values

928
00:59:09,930 --> 00:59:15,020
ABD and so forth and let's assume we chose in order see the baby

929
00:59:15,070 --> 00:59:18,560
so first thing is that this ABC

930
00:59:18,570 --> 00:59:22,770
using this order would be c comes first because it has seen

931
00:59:22,970 --> 00:59:25,110
followed by a

932
00:59:25,170 --> 00:59:29,210
because that's a the next one in the sequence of with respect to this

933
00:59:29,220 --> 00:59:30,950
and then followed by

934
00:59:31,350 --> 00:59:35,430
he and then last thing is because is the last one so

935
00:59:35,480 --> 00:59:37,760
so initially this is the structure

936
00:59:37,770 --> 00:59:39,540
if you only given the structure

937
00:59:39,550 --> 00:59:40,510
this is the

938
00:59:40,560 --> 00:59:43,610
the representation of binary decision diagram

939
00:59:43,620 --> 00:59:45,440
to this one i'm adding

940
00:59:45,440 --> 00:59:48,900
and the one so from point of view this is there another

941
00:59:48,950 --> 00:59:52,800
truth to value all boolean expression

942
00:59:54,120 --> 00:59:59,100
so that i construct this way again using the same order the you know in

943
00:59:59,100 --> 01:00:00,770
this case the

944
01:00:00,810 --> 01:00:04,600
is the highest as far as far as this ranking goes so you build this

945
01:00:04,600 --> 01:00:07,180
one so now i can join them

946
01:00:07,180 --> 01:00:09,460
by using this sharing

947
01:00:09,470 --> 01:00:12,960
and then you end up this these two structures like this

948
01:00:12,990 --> 01:00:16,020
you have a nose CAE

949
01:00:16,040 --> 01:00:17,940
he be as before

950
01:00:17,940 --> 01:00:22,190
and if c is false because it doesn't have in which case this is the

951
01:00:22,190 --> 01:00:23,380
node you come here

952
01:00:23,440 --> 01:00:24,970
so this is about an hour

953
01:00:24,980 --> 01:00:25,800
and then you

954
01:00:25,820 --> 01:00:30,110
and then you see the rest of the structure is shared because c e b

955
01:00:30,120 --> 01:00:32,770
and then you have a very concise representation

956
01:00:32,810 --> 01:00:34,010
and then to that

957
01:00:34,010 --> 01:00:37,170
adding the third one which is BCD again

958
01:00:37,220 --> 01:00:40,700
we know that if c is a higher rank than

959
01:00:40,720 --> 01:00:41,940
then then

960
01:00:41,970 --> 01:00:42,880
then c

961
01:00:43,750 --> 01:00:48,220
sorry sees i and b therefore c comes first and then the

962
01:00:48,230 --> 01:00:50,010
and c then b

963
01:00:50,020 --> 01:00:52,040
so now when we do that

964
01:00:52,460 --> 01:00:55,500
joining of these things then this is the structure

965
01:00:55,540 --> 01:00:57,270
so so you can see that

966
01:00:57,600 --> 01:01:00,000
in all the nodes seems to end up

967
01:01:00,000 --> 01:01:02,220
like this and this gives us the

968
01:01:03,180 --> 01:01:04,710
compact representation

969
01:01:04,750 --> 01:01:08,770
so once these are presented then what happens is when you're trying to

970
01:01:08,850 --> 01:01:10,150
the pattern mining

971
01:01:10,260 --> 01:01:12,250
basically what the structure

972
01:01:12,280 --> 01:01:17,540
walking through the structure and looking at the other datasets with this pattern is present

973
01:01:17,580 --> 01:01:23,690
then you can generate very very quickly all these emerging patterns and because it is

974
01:01:23,700 --> 01:01:27,000
this disjunction as well

975
01:01:27,060 --> 01:01:31,500
you can do the distinction construction also quite quickly using these structures

976
01:01:32,090 --> 01:01:35,380
the actual algorithm is a bit more involved and we don't have time to go

977
01:01:35,380 --> 01:01:40,400
through that there is a paper reference if you're interested you should look at that

978
01:01:40,440 --> 01:01:44,130
so one thing i wanted to say is that there is a tool available if

979
01:01:44,130 --> 01:01:48,030
you're interested you can download from the university

980
01:01:49,650 --> 01:01:53,540
and recently we use this structure to study

981
01:01:53,570 --> 01:01:57,800
schizophrenia it is one of the mental disorders generally

982
01:01:57,810 --> 01:01:59,400
i think age group

983
01:01:59,400 --> 01:02:02,800
i think that was

984
01:02:02,810 --> 01:02:07,990
are and works here that the the gene

985
01:02:08,150 --> 01:02:09,490
it's a

986
01:02:10,530 --> 01:02:12,410
and of course

987
01:02:13,060 --> 01:02:14,440
or one

988
01:02:14,460 --> 01:02:19,590
right before you on tree

989
01:02:20,800 --> 01:02:22,820
really four

990
01:02:22,850 --> 01:02:24,700
three or

991
01:02:28,040 --> 01:02:31,880
and how do we decide to the truth

992
01:02:31,910 --> 01:02:35,740
we trust also

993
01:02:35,770 --> 01:02:37,710
you can

994
01:02:37,800 --> 01:02:41,830
i don't know know if the test from

995
01:02:41,850 --> 01:02:42,590
they are

996
01:02:45,030 --> 01:02:48,790
now decided to bring site

997
01:02:48,850 --> 01:02:54,520
francis when you get e to the test small proline rich

998
01:02:54,610 --> 01:02:59,710
one hundred and

999
01:02:59,720 --> 01:03:02,330
so we only

1000
01:03:02,540 --> 01:03:07,350
let's try terms your

1001
01:03:09,270 --> 01:03:11,780
very small

1002
01:03:20,390 --> 01:03:25,090
well the i is true for

1003
01:03:25,130 --> 01:03:27,290
how much

1004
01:03:27,290 --> 01:03:29,400
it's still you

1005
01:03:29,510 --> 01:03:33,750
the first three cm to use

1006
01:03:33,760 --> 01:03:36,030
he gets

1007
01:03:36,050 --> 01:03:38,570
the challenge for free

1008
01:03:38,580 --> 01:03:41,620
the only problem is that and

1009
01:03:41,640 --> 01:03:44,570
first british

1010
01:03:44,590 --> 01:03:46,350
it's is

1011
01:03:46,370 --> 01:03:48,460
so this was

1012
01:03:48,560 --> 01:03:50,360
that was one of the

1013
01:03:53,580 --> 01:03:56,310
three years

1014
01:04:00,830 --> 01:04:03,490
one of the current

1015
01:04:03,490 --> 01:04:06,800
one aspect of this

1016
01:04:10,440 --> 01:04:13,080
they think about it

1017
01:04:17,300 --> 01:04:19,990
the right

1018
01:04:20,010 --> 01:04:21,630
it's really using the

1019
01:04:28,060 --> 01:04:29,430
is doing

1020
01:04:29,440 --> 01:04:31,030
much higher here

1021
01:04:36,700 --> 01:04:40,300
right before really really

1022
01:04:50,300 --> 01:04:56,110
three suppose

1023
01:04:56,130 --> 01:04:58,240
we were yesterday

1024
01:05:02,220 --> 01:05:05,310
i've got a a

1025
01:05:12,800 --> 01:05:17,040
so i will

1026
01:05:19,870 --> 01:05:23,720
so when we were

1027
01:05:25,470 --> 01:05:27,890
eight year

1028
01:05:32,870 --> 01:05:39,580
you say well how would you were right

1029
01:05:39,600 --> 01:05:40,870
i mean

1030
01:05:43,260 --> 01:05:46,410
he said he

1031
01:05:46,430 --> 01:05:48,040
how many trials

1032
01:05:48,080 --> 01:05:49,470
what you

1033
01:05:50,220 --> 01:05:54,910
we're trying to

1034
01:05:54,950 --> 01:05:57,560
well how we have a well

1035
01:05:59,640 --> 01:06:01,060
so what was

1036
01:06:01,080 --> 01:06:02,780
you know what

1037
01:06:02,830 --> 01:06:06,030
for all

1038
01:06:06,080 --> 01:06:08,120
three four

1039
01:06:16,240 --> 01:06:17,350
what mean

1040
01:06:17,370 --> 01:06:22,910
so what

1041
01:06:26,740 --> 01:06:29,800
and we all

1042
01:06:30,950 --> 01:06:34,510
no well how to brain

1043
01:06:35,990 --> 01:06:43,240
mean what fraction on the

1044
01:06:48,350 --> 01:06:50,850
so all in all

1045
01:06:50,920 --> 01:06:54,030
they must

1046
01:06:54,040 --> 01:06:57,950
you all

1047
01:06:57,970 --> 01:07:00,390
you're one is also

1048
01:07:01,450 --> 01:07:07,890
one thousand so that you know you

1049
01:07:09,310 --> 01:07:13,830
well were

1050
01:07:13,850 --> 01:07:18,660
this model was well

1051
01:07:21,270 --> 01:07:24,450
here ten two

1052
01:07:28,950 --> 01:07:31,620
it's a one

1053
01:07:35,120 --> 01:07:37,160
generates one two

1054
01:07:37,180 --> 01:07:39,680
so it's

1055
01:07:39,700 --> 01:07:40,720
it's true

1056
01:07:47,490 --> 01:07:49,540
i'll show

1057
01:07:51,200 --> 01:07:57,800
of two one hundred results

1058
01:08:00,260 --> 01:08:01,680
there are two

1059
01:08:01,740 --> 01:08:03,370
it is

1060
01:08:03,370 --> 01:08:06,510
if we have

1061
01:08:06,530 --> 01:08:12,200
the roots of

1062
01:08:12,220 --> 01:08:14,060
you should do

1063
01:08:14,060 --> 01:08:19,520
are exactly the same as non negation rules for classical logic

1064
01:08:20,600 --> 01:08:25,930
classical logic and intuitionist logics do not have the same

1065
01:08:25,950 --> 01:08:29,740
negation free theorems

1066
01:08:31,830 --> 01:08:33,200
so what are the rules

1067
01:08:33,290 --> 01:08:36,810
well the usual sorts of rules for

1068
01:08:36,910 --> 01:08:44,310
so if we have a we have been online for a and lines are proof

1069
01:08:44,350 --> 01:08:47,270
within the same depth within the same subproof

1070
01:08:47,270 --> 01:08:48,160
we can

1071
01:08:48,160 --> 01:08:50,910
include a and b that's that

1072
01:08:50,930 --> 01:08:58,430
and the action

1073
01:09:07,660 --> 01:09:10,680
and our and elimination rules are if we have a and b

1074
01:09:10,720 --> 01:09:14,640
we can conclude a or b

1075
01:09:18,810 --> 01:09:38,560
introduction rules same as usual

1076
01:09:48,220 --> 01:09:51,290
search like this

1077
01:09:52,600 --> 01:09:56,290
OK so we've got a or b is the disjunction elimination

1078
01:09:56,620 --> 01:09:59,890
and we can get was subproof were

1079
01:10:00,770 --> 01:10:02,990
starting with a hypothesis

1080
01:10:02,990 --> 01:10:04,060
we get c

1081
01:10:04,080 --> 01:10:07,830
we start with being gets a we knew we had a or b we can

1082
01:10:07,830 --> 01:10:10,620
conclude c because either way

1083
01:10:10,640 --> 01:10:15,430
that's sort of proof by cases is not very non intuitionistic about that because we

1084
01:10:15,430 --> 01:10:19,180
know we have a or b we should know which one we have

1085
01:10:20,540 --> 01:10:25,390
let's let's face it intuition is a lot of natural deduction and buy into this

1086
01:10:26,240 --> 01:10:28,790
i'm not an intuition is that to defend them do doing

1087
01:10:32,180 --> 01:10:37,850
an implication rules of usual tens of the first one is modus ponens

1088
01:10:37,870 --> 01:10:42,540
error minimisation and the second one

1089
01:10:42,560 --> 01:10:49,810
as from

1090
01:10:50,140 --> 01:10:52,120
a subproof

1091
01:10:52,140 --> 01:10:57,850
that gives us the from hypothesis that a we can conclude we can discharge a

1092
01:10:57,970 --> 01:11:00,100
include aerobic

1093
01:11:00,660 --> 01:11:03,060
actually sorry

1094
01:11:03,080 --> 01:11:04,390
i'm using the arrow

1095
01:11:04,390 --> 01:11:06,240
i should be using the hough

1096
01:11:06,410 --> 01:11:08,850
throughout the

1097
01:11:08,870 --> 01:11:10,450
i do that that's

1098
01:11:10,450 --> 01:11:13,330
and the reason is because i like to reserve the arrow

1099
01:11:13,350 --> 01:11:15,870
four out of the area

1100
01:11:16,990 --> 01:11:19,790
so our precious that when relevant logicians

1101
01:11:31,760 --> 01:11:35,040
that makes me feel better and it doesn't matter you make me feel

1102
01:11:40,160 --> 01:11:44,180
we have our books and in place not so these are the the the same

1103
01:11:44,180 --> 01:11:46,180
as the classical rules

1104
01:11:46,200 --> 01:11:51,200
now the interesting thing is that in in intuitionist logic when we have the negation

1105
01:11:51,200 --> 01:11:55,180
rules they don't give us any new theorems

1106
01:11:55,270 --> 01:11:58,040
in the non occasion fragment

1107
01:11:58,060 --> 01:12:00,240
so they don't give us any new

1108
01:12:00,430 --> 01:12:04,790
formulas and theorems that we could state with the negation

1109
01:12:04,810 --> 01:12:08,020
in classical logic they do

1110
01:12:08,040 --> 01:12:13,290
it what it's called intuitionist logic or in way in intuitionist logic at indication is

1111
01:12:13,290 --> 01:12:16,080
conservative or conservative extensions

1112
01:12:16,540 --> 01:12:18,700
non negation fragments

1113
01:12:18,720 --> 01:12:23,350
in classical logic it's not and what you get

1114
01:12:48,310 --> 01:12:57,080
that it's the

1115
01:12:57,080 --> 01:13:00,560
yes in in base out course

1116
01:13:00,600 --> 01:13:02,900
when i went

1117
01:13:03,350 --> 01:13:07,370
but it can be computed

1118
01:13:07,370 --> 01:13:09,100
and have been copied

1119
01:13:09,120 --> 01:13:11,460
the brothers have been studied

1120
01:13:11,580 --> 01:13:16,810
OK so what we have to define a dirichlet process DP mixture injured

1121
01:13:16,830 --> 01:13:20,580
we have to have this family of fungi of the

1122
01:13:20,600 --> 01:13:25,900
basically for the mixture components which are parameters by peter

1123
01:13:27,690 --> 01:13:31,230
the cluster membership is sampled exactly like before

1124
01:13:31,270 --> 01:13:37,600
no matter what i have got of something else

1125
01:13:37,650 --> 01:13:44,460
and i have to have prior distribution for the parameters theta are

1126
01:13:44,570 --> 01:13:48,210
and this is the only have some parameters of it all

1127
01:13:48,370 --> 01:13:52,420
like that sigma

1128
01:13:52,870 --> 01:13:57,230
so the the the model the parameters of the model two

1129
01:13:57,290 --> 01:14:02,230
outside the controls cluster sizes and beta which controls

1130
01:14:02,250 --> 01:14:05,040
prior to the parameters

1131
01:14:05,080 --> 01:14:07,190
which can be set independently

1132
01:14:07,270 --> 01:14:13,250
very flexible and then sample like before i'm sorry for being to different pages basically

1133
01:14:13,250 --> 01:14:15,810
example for the clusters

1134
01:14:15,850 --> 01:14:17,460
labels that

1135
01:14:17,460 --> 01:14:20,980
it's a new cluster sample parameters for the new cluster

1136
01:14:21,020 --> 01:14:25,520
and once they have the cluster and the parameters i sample the data we can

1137
01:14:25,520 --> 01:14:27,750
plot the data

1138
01:14:27,770 --> 01:14:37,270
OK let me see that questions about this matter

1139
01:14:38,560 --> 01:14:39,270
all right

1140
01:14:40,380 --> 01:14:43,620
then i will remind you that this is just a mental model of the data

1141
01:14:43,850 --> 01:14:45,140
being produced

1142
01:14:45,150 --> 01:14:46,750
what want to do is now

1143
01:14:46,750 --> 01:14:48,190
now is two

1144
01:14:48,210 --> 01:14:52,650
that from the data and go back and find them all

1145
01:14:52,670 --> 01:14:57,690
so we want to invert this process

1146
01:14:57,940 --> 01:15:02,580
so the problem of clustering is

1147
01:15:02,580 --> 01:15:03,670
he i know

1148
01:15:03,690 --> 01:15:06,100
the following things i know the model

1149
01:15:06,100 --> 01:15:11,400
in the sense that i know what kind of clusters i have

1150
01:15:11,480 --> 01:15:14,040
and i know

1151
01:15:14,060 --> 01:15:16,020
or i assume

1152
01:15:16,060 --> 01:15:17,750
so all the assumptions

1153
01:15:17,850 --> 01:15:18,940
you have put in

1154
01:15:18,980 --> 01:15:22,810
but the assumption that come from europe

1155
01:15:23,580 --> 01:15:24,350
as much

1156
01:15:24,380 --> 01:15:27,940
from what you happen to know about this process before you actually try to fit

1157
01:15:27,940 --> 01:15:29,100
the parameters

1158
01:15:29,170 --> 01:15:32,560
so what this is actually what the clusters do you know that

1159
01:15:32,640 --> 01:15:37,710
distribution of the parameters together that are likely to appear

1160
01:15:37,730 --> 01:15:43,210
and just like any prior cell serves also for some kind of regularization

1161
01:15:43,270 --> 01:15:46,750
but for example i can tell you how separated clusters should be

1162
01:15:46,830 --> 01:15:48,880
or how o

1163
01:15:49,020 --> 01:15:52,150
what are the variances should be sort of

1164
01:15:52,210 --> 01:15:54,100
almost equal or you allow

1165
01:15:54,120 --> 01:15:58,310
clusters of different shapes and different variances if you think about nature is that that's

1166
01:15:58,310 --> 01:16:00,920
what you put in practice

1167
01:16:01,270 --> 01:16:04,810
and then you have to control the distribution of

1168
01:16:06,870 --> 01:16:07,830
all these

1169
01:16:07,850 --> 01:16:10,580
the input and output and you also have the data

1170
01:16:10,600 --> 01:16:15,520
and you want to get the clustering here is that the parameters the model data

1171
01:16:16,670 --> 01:16:19,440
and you know i they want to

1172
01:16:19,500 --> 01:16:23,670
because when you can do in in that each point is

1173
01:16:23,670 --> 01:16:25,580
you can think of findings of the

1174
01:16:25,580 --> 01:16:29,920
the except for the one that in the same cluster they have the same

1175
01:16:30,230 --> 01:16:36,060
but if you it is what would be the implicit

1176
01:16:38,500 --> 01:16:44,670
and so you ask for this and then get a soft clustering from the data

1177
01:16:44,670 --> 01:16:47,750
by repeating the model

1178
01:16:48,710 --> 01:16:55,980
which is here or you can get hard clustering by just taking the clusters

1179
01:16:58,900 --> 01:17:02,920
in my opinion one the problem the bad news is that there is no closed

1180
01:17:02,920 --> 01:17:04,420
form solution

1181
01:17:04,460 --> 01:17:09,920
and what we do is actually the sort by what is called markov chain monte

1182
01:17:09,920 --> 01:17:11,960
carlo sampling

1183
01:17:12,150 --> 01:17:13,250
how many of you

1184
01:17:14,040 --> 01:17:18,310
you know what this is

1185
01:17:18,400 --> 01:17:22,620
OK then we say so

1186
01:17:22,750 --> 01:17:26,500
because i'm going to explain very short

1187
01:17:26,880 --> 01:17:30,670
so there is monte-carlo and markov chain monte

1188
01:17:30,710 --> 01:17:34,640
one colour means you have a distribution you drop point from it and then you

1189
01:17:34,640 --> 01:17:37,040
told the next point independently and so on

1190
01:17:37,040 --> 01:17:39,420
and you get the large sample and then you do something with it i compute

1191
01:17:39,420 --> 01:17:41,640
an average

1192
01:17:42,060 --> 01:17:45,620
in markov chain monte carlo

1193
01:17:45,620 --> 01:17:49,580
you want to sample from a distribution but you can't grow independent samples because

1194
01:17:49,600 --> 01:17:53,250
actually you can do independent samples from any distribution

1195
01:17:53,270 --> 01:17:55,500
there some from which you can sample from it you can

1196
01:17:55,690 --> 01:17:58,270
and then you the following trick

1197
01:17:59,400 --> 01:18:02,080
because construct the example

1198
01:18:02,190 --> 01:18:07,080
from the distribution so and then you change it selected with random walk through the

1199
01:18:07,080 --> 01:18:12,400
and these additional moves compensate for the fact that he is the weaker player and

1200
01:18:12,660 --> 01:18:17,010
under japanese rules again these stones have to be placed on particular points of the

1201
01:18:17,010 --> 01:18:20,220
board and

1202
01:18:20,240 --> 01:18:22,650
those are the points that are markets

1203
01:18:23,020 --> 01:18:26,080
here are called stop points partially

1204
01:18:26,100 --> 01:18:33,440
and these handicap stones must be placed on these particular points in a particular order

1205
01:18:33,590 --> 01:18:41,380
so that's that's a very nice property because after you've practice for while i could

1206
01:18:41,380 --> 01:18:44,920
have a game with you and we could have a really exciting game if i

1207
01:18:44,920 --> 01:18:46,740
gave you nine stones

1208
01:18:46,750 --> 01:18:50,530
of handicapped because then it would be quite hard for me to win and you

1209
01:18:50,530 --> 01:18:53,650
would have this compensation if you're beginning for example

1210
01:18:55,060 --> 01:19:01,070
there's another interesting thing which i would like to mention its ranking system in

1211
01:19:01,090 --> 01:19:06,790
and it works in a way entirely differently from the low system for example in

1212
01:19:06,790 --> 01:19:13,130
chess and the idea the idea you have these hugh grades and don kind of

1213
01:19:13,270 --> 01:19:18,850
student and master grades and

1214
01:19:18,870 --> 01:19:24,210
the trick is that this system is calibrated at least for amateur players against the

1215
01:19:24,210 --> 01:19:30,380
number of handicap stones for two people so if for example a ten q

1216
01:19:30,390 --> 01:19:33,050
so let's higher humans

1217
01:19:33,070 --> 01:19:40,450
worst player ten q playing against the five hu then the five q would grant

1218
01:19:40,450 --> 01:19:45,560
five handicap stones to the ten q because the difference is ten minus five gives

1219
01:19:45,560 --> 01:19:47,010
five so

1220
01:19:47,030 --> 01:19:52,340
one rank is exactly equivalent to having one less or one more handicap stones that's

1221
01:19:52,350 --> 01:19:55,550
very nice system and if you meet someone you don't know their strengths you have

1222
01:19:55,550 --> 01:20:00,640
the game you can pretty easily calibrate the system and from then on play with

1223
01:20:00,640 --> 01:20:06,680
the correct number of handicap stones and have fun and open game

1224
01:20:06,690 --> 01:20:10,960
so let's talk about computer go on computer chess

1225
01:20:10,980 --> 01:20:18,390
in a certain sense you could argue that computer chess

1226
01:20:20,760 --> 01:20:24,830
i mean many people in computer chess wouldn't agree with that but i think it

1227
01:20:24,830 --> 01:20:30,670
was in nineteen ninety seven there was the match between this deep blue by IBM

1228
01:20:30,670 --> 01:20:32,830
and kasparov

1229
01:20:32,840 --> 01:20:38,910
and people who was able to pull off the victory now there were

1230
01:20:38,920 --> 01:20:45,440
various arguments so because was the world champion at the time there are various arguments

1231
01:20:45,440 --> 01:20:50,430
you might bring forward that's a maybe people who wasn't as good as kasparov or

1232
01:20:50,430 --> 01:20:55,230
he wasn't well prepared and that this is probably too you see it

1233
01:20:55,250 --> 01:21:00,490
the what we can really conclude from this is that computers are pretty much at

1234
01:21:00,490 --> 01:21:01,560
the level of the

1235
01:21:01,950 --> 01:21:06,220
world champion of chess of the human world champion of chess and if they're not

1236
01:21:06,220 --> 01:21:11,540
there now than in five years time with additional computation time they they will be

1237
01:21:11,540 --> 01:21:17,940
there now state in computer go is entirely different and that can serve as another

1238
01:21:17,940 --> 01:21:24,490
source of inspiration and motivation for playing this game because the best computer go programs

1239
01:21:24,490 --> 01:21:30,610
at the level of weak amateur players so if you practice for half a year

1240
01:21:30,610 --> 01:21:36,170
or year then you may well be able to be the strongest computer go programs

1241
01:21:36,170 --> 01:21:37,830
at this game

1242
01:21:37,840 --> 01:21:44,880
and the various reasons why this may be the case

1243
01:21:44,900 --> 01:21:46,840
one reason

1244
01:21:47,510 --> 01:21:52,050
it may be best to understand this if you understand how chess works how computer

1245
01:21:52,050 --> 01:21:58,020
chess works we've talked about these game trees so computer chess is all about enumerating

1246
01:21:58,040 --> 01:22:06,460
the possibilities for for each player's moves get all these leave positions evaluate them and

1247
01:22:06,460 --> 01:22:12,530
use minimax search over the street to find the best move now obviously that requires

1248
01:22:12,540 --> 01:22:13,840
two things

1249
01:22:13,860 --> 01:22:17,220
you need to be able to explore this tree

1250
01:22:17,440 --> 01:22:23,730
and in chess you typically have maybe twenty different moves per side of the good

1251
01:22:26,320 --> 01:22:31,300
but in goal of course the entire board available so you have more like two

1252
01:22:31,300 --> 01:22:36,080
hundred moves available at any any move so that's one reason why this is so

1253
01:22:36,080 --> 01:22:41,470
hard but the other problem is that of course in in go all the stones

1254
01:22:41,470 --> 01:22:44,880
look pretty much like i mean except for their black and white cells but the

1255
01:22:44,880 --> 01:22:50,410
blackstone's look alike they don't have different values per say not like night or a

1256
01:22:51,500 --> 01:22:57,050
in chess but they only derive their value from the position on the board and

1257
01:22:57,050 --> 01:23:02,700
from the complex interactions between stones and that's the reason why this much harder to

1258
01:23:02,700 --> 01:23:05,630
find a good evaluation function and the

1259
01:23:05,640 --> 01:23:09,590
the evaluation function is the second ingredient if you want to approach this problem like

1260
01:23:09,590 --> 01:23:14,910
computer chess because at the bottom you need to evaluate all the leaves so that

1261
01:23:14,910 --> 01:23:21,680
doesn't work really well and that's the reason computer go is struggling a lot

1262
01:23:23,440 --> 01:23:26,730
one interesting observation is

1263
01:23:26,750 --> 01:23:30,160
that's a good human players can make moves

1264
01:23:30,160 --> 01:23:30,420
the car

1265
01:23:32,310 --> 01:23:33,080
and there's a model

1266
01:23:34,750 --> 01:23:36,420
called the dependent dirichlet process

1267
01:23:36,930 --> 01:23:37,290
which is

1268
01:23:38,240 --> 01:23:41,410
extremely popular in nonparametric bayesian statistics

1269
01:23:41,950 --> 01:23:43,740
it's not used so much in machine learning but

1270
01:23:45,350 --> 01:23:47,060
i think that just as a matter of

1271
01:23:47,460 --> 01:23:48,750
topics that people focus on

1272
01:23:49,210 --> 01:23:49,860
machine learning

1273
01:23:50,710 --> 01:23:55,930
one big application of nonparametric bayesian modeling for example are topic models and natural language processing so

1274
01:23:56,600 --> 01:24:00,010
the through to the get used a lot of things like direct directly processes

1275
01:24:00,970 --> 01:24:04,730
because topic models in the pitman yor process because this power-law property and so on

1276
01:24:05,220 --> 01:24:06,940
whereas a nonparametric bayesian statistics

1277
01:24:07,460 --> 01:24:10,490
people are interested in in all kinds of applications where

1278
01:24:10,970 --> 01:24:12,400
you have this covariate dependence

1279
01:24:12,820 --> 01:24:13,740
and this model is

1280
01:24:14,270 --> 01:24:18,210
you know and in some sense is actually maybe a bit of a hack but it's it's just

1281
01:24:18,720 --> 01:24:20,540
you know it's just hit nerve and it's it's

1282
01:24:21,350 --> 01:24:25,750
maybe the most popular modification of of the iterative process nonparametric statistics

1283
01:24:28,200 --> 01:24:29,860
what we do is we have these

1284
01:24:33,410 --> 01:24:33,910
we had these

1285
01:24:36,280 --> 01:24:39,050
the cluster sizes and the cluster locations

1286
01:24:39,700 --> 01:24:43,240
and i said you know making them functions of the covariates right hand

1287
01:24:43,580 --> 01:24:44,310
we know that these

1288
01:24:44,840 --> 01:24:45,680
cluster weights

1289
01:24:46,250 --> 01:24:47,600
that they are between zero and one

1290
01:24:48,550 --> 01:24:53,280
and now they take as input the take a parameter they take a covariate value so say time

1291
01:24:54,060 --> 01:24:56,590
and so that becomes a function from

1292
01:24:57,430 --> 01:24:59,730
time from the time axis into zero one

1293
01:25:00,490 --> 01:25:01,250
can and we have to

1294
01:25:02,600 --> 01:25:04,160
think about how we can model that function

1295
01:25:04,780 --> 01:25:10,640
and similarly reported that the parameter values we have a function from time into the set of possible parameters

1296
01:25:11,810 --> 01:25:12,480
that's the parameters

1297
01:25:14,030 --> 01:25:16,460
okay and now the idea and the dependent dirichlet processes

1298
01:25:16,850 --> 01:25:17,760
if we have we have to

1299
01:25:19,200 --> 01:25:20,780
we have to model smooth functions here

1300
01:25:21,660 --> 01:25:23,220
so why do we use a gaussian process

1301
01:25:24,320 --> 01:25:25,830
and this

1302
01:25:29,900 --> 01:25:30,340
four hour

1303
01:25:32,400 --> 01:25:32,850
four these

1304
01:25:34,340 --> 01:25:38,440
cluster weights you that's a bit of a problem because as john told you this

1305
01:25:38,440 --> 01:25:40,010
morning when we use a gaussian process

1306
01:25:40,680 --> 01:25:40,940
and we

1307
01:25:41,360 --> 01:25:45,610
we look at one particular time point right the girls process gives us a random function

1308
01:25:46,170 --> 01:25:49,390
and now we fix one time point and we look at the function value at that time

1309
01:25:50,090 --> 01:25:51,750
what is the distribution of the functionality

1310
01:25:52,740 --> 01:25:55,560
the function is random so that is a random variable what is the distribution

1311
01:25:56,080 --> 01:25:56,490
it's a gas

1312
01:25:57,220 --> 01:25:58,300
because it's a process

1313
01:26:01,070 --> 01:26:03,710
galson is a distribution on the on the real line

1314
01:26:06,250 --> 01:26:08,490
what we want to hear something that is in zero one

1315
01:26:09,180 --> 01:26:11,410
right we don't want something that's on the entire real line

1316
01:26:12,000 --> 01:26:13,610
so we have to sum transformers

1317
01:26:13,990 --> 01:26:18,270
and also we want to have a specific distribution you if we want to have iterative process because we know

1318
01:26:18,670 --> 01:26:22,670
the process stick breaking construction these weights have to be the variables

1319
01:26:23,270 --> 01:26:24,830
otherwise something else is going to pop up

1320
01:26:28,200 --> 01:26:30,330
we have to transform this calcium to have

1321
01:26:30,810 --> 01:26:35,800
these these girls variables at each time point to have a marginal distribution that speed

1322
01:26:37,070 --> 01:26:40,320
so you have to and that's why i said it's a bit of a hack because you have to

1323
01:26:41,750 --> 01:26:44,670
sample from the gaussianprocess and then put it through a transformation

1324
01:26:45,710 --> 01:26:46,820
that turns it into a beta

1325
01:26:47,660 --> 01:26:50,740
that is possible by means of cumulative distribution functions but it's just a bit

1326
01:26:52,780 --> 01:26:53,750
computationally intensive

1327
01:26:57,320 --> 01:26:58,460
and then similarly

1328
01:27:00,630 --> 01:27:01,610
similar we look

1329
01:27:02,030 --> 01:27:03,110
so i

1330
01:27:08,430 --> 01:27:13,050
what we do in specifically is that we use the stick breaking construction we sample these

1331
01:27:13,610 --> 01:27:18,080
these proportions if you remember that we used to construct the weights we sample the

1332
01:27:18,080 --> 01:27:23,480
proportions on the gaussianprocess can be variables and then apply this formula for

1333
01:27:24,250 --> 01:27:26,290
for computing the weights from the stick breaking construction

1334
01:27:26,900 --> 01:27:27,860
and so the idea is

1335
01:27:28,320 --> 01:27:32,930
make iterative process dependent by looking at the components that occur in the stick breaking construction

1336
01:27:33,360 --> 01:27:35,630
and turn them into functions that vary smoothly and time

1337
01:27:36,830 --> 01:27:40,750
these functions have to have the right marginal distributions at each time point so that

1338
01:27:41,140 --> 01:27:42,390
directly process comes out

1339
01:27:45,690 --> 01:27:47,790
so if we do that what do we get we get

1340
01:27:48,730 --> 01:27:52,490
it we get a distribution that is if we fix time point

1341
01:27:53,830 --> 01:27:58,760
and looked at the margin clustering the distribution of the clustering that we get at that time point

1342
01:27:59,330 --> 01:28:00,760
say one frame in the video

1343
01:28:01,510 --> 01:28:03,490
the distribution is actually iterative process

1344
01:28:06,490 --> 01:28:09,250
these clustering solutions very smoothly in time

1345
01:28:09,660 --> 01:28:12,000
and house mostly depends on how we set

1346
01:28:12,560 --> 01:28:14,460
the covariance function of the girls in process

1347
01:28:17,170 --> 01:28:18,340
all right so the

1348
01:28:18,760 --> 01:28:19,060
kind of

1349
01:28:19,750 --> 01:28:24,030
this is specific to the during process of course right that uses this this very

1350
01:28:24,030 --> 01:28:26,690
specific construction definition of the stick breaking process

1351
01:28:27,790 --> 01:28:28,740
for the process

1352
01:28:30,800 --> 01:28:31,670
but but you can

1353
01:28:32,100 --> 01:28:34,110
if you just look at this idea you can really

1354
01:28:35,350 --> 01:28:37,860
you can generalize this and the general theme is that

1355
01:28:38,360 --> 01:28:38,630
we have

1356
01:28:42,000 --> 01:28:46,730
some some random object you that has a certain distribution and then we have a

1357
01:28:46,750 --> 01:28:51,860
covariance random object is say the discrete random measures then we have the covariate space

1358
01:28:51,860 --> 01:28:54,650
time space or whatever that is supposed to an extent

1359
01:28:55,170 --> 01:28:56,210
and we have to turn this

1360
01:28:56,630 --> 01:28:57,000
in two

1361
01:28:57,580 --> 01:28:59,350
a random function that maps into

1362
01:28:59,890 --> 01:29:04,040
the set of possible run amok you know this kind of a general the general idea

1363
01:29:15,470 --> 01:29:16,290
so now we have seen

1364
01:29:21,170 --> 01:29:23,190
my talk yesterday and john talk this morning

1365
01:29:23,860 --> 01:29:26,510
and what actually so far what we have seen as we have seen a bunch

1366
01:29:26,510 --> 01:29:29,350
of models we seen the dark process that you're process

1367
01:29:29,880 --> 01:29:30,860
the new buffet process

1368
01:29:31,340 --> 01:29:32,580
we've seen some process

1369
01:29:33,160 --> 01:29:36,750
and we have seen ways of how we can combine these models together in

1370
01:29:37,480 --> 01:29:38,680
and with that we can

1371
01:29:39,480 --> 01:29:40,920
that accounts for are actually

1372
01:29:42,070 --> 01:29:45,150
for most of the models that we have in bayesian nonparametrics in one way or another

1373
01:29:46,230 --> 01:29:47,160
end when i

1374
01:29:48,540 --> 01:29:52,300
when you attain idea even if the trio on this last last december

1375
01:29:52,780 --> 01:29:54,580
u i made this this list here

1376
01:29:55,220 --> 01:29:57,110
because he feared it would be nice to just you know

1377
01:29:57,910 --> 01:30:00,710
just have a list of all models and and relate them to

1378
01:30:01,270 --> 01:30:02,420
the applications and to

1379
01:30:04,390 --> 01:30:06,200
to what type of parameter we are using

1380
01:30:07,300 --> 01:30:08,580
so remember in the bayesian model

1381
01:30:09,110 --> 01:30:12,860
these kind of these intuition that try to convey yesterday is that the parameters some

1382
01:30:12,860 --> 01:30:14,650
kind of pattern that play explains the data

1383
01:30:15,510 --> 01:30:17,710
what type of pattern to use and what do we get

1384
01:30:18,410 --> 01:30:21,220
so we have a classification and regression models

1385
01:30:21,670 --> 01:30:23,260
in that case the parameters function

1386
01:30:23,880 --> 01:30:26,640
and the political model for the girls process right

1387
01:30:27,280 --> 01:30:31,890
then when we do clustering the pattern is a partition and so the model for that is

1388
01:30:33,170 --> 01:30:35,960
the chinese restaurant process are directed process but this is an

1389
01:30:37,750 --> 01:30:41,300
we can also use the director process for density estimation and so on and so

1390
01:30:41,300 --> 01:30:42,860
forth right you can do this hierarchical

1391
01:30:48,740 --> 01:30:50,590
what you can see is that the

1392
01:30:51,250 --> 01:30:53,030
if you go through this list there's a certain

1393
01:30:53,690 --> 01:30:55,040
there's a certain set of of

1394
01:30:56,900 --> 01:31:01,260
of basic models that we used to use to generate all of these these different models

1395
01:31:03,000 --> 01:31:03,430
i want to

1396
01:31:04,650 --> 01:31:06,570
what i want to do next as i want to talk forum

1397
01:31:07,990 --> 01:31:10,920
but first i want to use a few slides to talk about what we can

1398
01:31:10,920 --> 01:31:12,430
possibly do you want to go beyond

1399
01:31:13,160 --> 01:31:16,960
what would be involved in how difficult is that i can tell you it's not easy to do

1400
01:31:16,960 --> 01:31:20,080
so that's basically enough

1401
01:31:20,120 --> 01:31:22,860
now what i was trying to show we're trying to show

1402
01:31:22,870 --> 01:31:24,580
that this

1403
01:31:24,620 --> 01:31:27,350
is minimized by something in FS

1404
01:31:27,360 --> 01:31:31,520
right we were trying to show that the minimiser of this lies in this

1405
01:31:31,530 --> 01:31:34,590
some of bump functions on data points

1406
01:31:34,640 --> 01:31:36,650
i suppose it's not

1407
01:31:37,760 --> 01:31:38,690
there is

1408
01:31:38,710 --> 01:31:40,360
and this

1409
01:31:40,400 --> 01:31:43,550
so f study the solution to this right it as

1410
01:31:43,550 --> 01:31:47,400
after as plus f perpendicular

1411
01:31:47,460 --> 01:31:49,410
then for each point

1412
01:31:49,460 --> 01:31:54,840
after that affects is equal to f as of exile right because f as effect

1413
01:31:54,840 --> 01:31:59,160
size was so before has the same value f f star affects i hope you

1414
01:32:00,280 --> 01:32:01,960
this fact

1415
01:32:05,870 --> 01:32:07,970
then the loss function

1416
01:32:08,030 --> 01:32:12,000
is the same because it's only depends on this one

1417
01:32:12,100 --> 01:32:16,960
on the other hand we make that thing short for this

1418
01:32:16,980 --> 01:32:19,700
so this is the same for fast

1419
01:32:19,710 --> 01:32:21,120
and this is

1420
01:32:21,160 --> 01:32:22,350
at least

1421
01:32:22,390 --> 01:32:25,460
not long for short maybe maybe small

1422
01:32:25,520 --> 01:32:30,930
so by going from left to fester the decrees

1423
01:32:30,970 --> 01:32:35,710
this whole some by leaving this first part to be exactly the same as before

1424
01:32:35,720 --> 01:32:37,650
making this possibly short

1425
01:32:37,700 --> 01:32:44,670
so OK so that means that when for this thing became smaller but it cannot

1426
01:32:44,670 --> 01:32:48,670
be smaller because it's already the minimum

1427
01:32:48,680 --> 01:32:50,200
so that this

1428
01:32:50,250 --> 01:32:53,220
therefore the minimum has to lie in

1429
01:32:54,790 --> 01:32:56,870
this space

1430
01:32:56,950 --> 01:32:58,050
in s

1431
01:32:58,050 --> 01:33:01,920
so the minimum is actually the sum of the parts

1432
01:33:01,940 --> 01:33:03,550
again this is

1433
01:33:05,470 --> 01:33:12,520
important argument because this is actually what makes this algorithms work

1434
01:33:14,150 --> 01:33:19,850
so finally this is basically repeating it f stars and as the first time this

1435
01:33:19,850 --> 01:33:21,760
it's a linear combination of the pump

1436
01:33:28,210 --> 01:33:32,840
former fast so you can you can think of this as being gaussians e to

1437
01:33:32,840 --> 01:33:39,860
the next i minus x squared

1438
01:33:39,870 --> 01:33:42,920
o so you maybe that's a good point

1439
01:33:46,380 --> 01:33:48,830
i should say that directly

1440
01:33:48,870 --> 01:33:55,010
so for the gaussians it's OK

1441
01:33:55,070 --> 01:34:02,280
x y

1442
01:34:02,360 --> 01:34:04,280
he calls

1443
01:34:04,300 --> 01:34:16,410
let me just say it was the x minus y

1444
01:34:19,120 --> 01:34:21,410
and you can do you think about it being

1445
01:34:22,910 --> 01:34:24,950
or they can be vector does not

1446
01:34:24,970 --> 01:34:29,570
let me just make some simple real numbers

1447
01:34:29,580 --> 01:34:31,250
make it as simple as possible

1448
01:34:31,300 --> 01:34:35,680
now what is the norm so k

1449
01:34:37,170 --> 01:34:41,570
door so this is the function of the dot x is fixed

1450
01:34:41,570 --> 01:34:43,570
and k

1451
01:34:44,450 --> 01:34:48,780
again this is a function of the dot

1452
01:34:48,830 --> 01:34:53,500
not all wise fixi fix x and a fixed like this is the function now

1453
01:34:53,500 --> 01:34:56,580
i want to to compute the inner product

1454
01:34:56,700 --> 01:35:00,340
in the corresponding space and this is just k of x

1455
01:35:02,390 --> 01:35:07,520
so this is the definition of the product for this functions

1456
01:35:07,540 --> 01:35:08,460
that's what

1457
01:35:08,520 --> 01:35:10,380
the product

1458
01:35:11,090 --> 01:35:14,560
and you have to play a little bit of this to make sure that somehow

1459
01:35:14,570 --> 01:35:20,480
it makes sense

1460
01:35:20,500 --> 01:35:24,980
right so basically with when you have three guys what happens is that because they're

1461
01:35:25,000 --> 01:35:25,770
quite far

1462
01:35:26,040 --> 01:35:31,540
k of x minus lies close to zero so they're almost orthogonal then you just

1463
01:35:31,540 --> 01:35:34,020
have three of orthogonal that three

1464
01:35:34,060 --> 01:35:37,730
i the it's just the somewhere in theorem saying that this one plus one plus

1465
01:35:37,730 --> 01:35:42,250
one square so if you have an bombs and they're quite far from each other

1466
01:35:42,420 --> 01:35:45,500
users does get square root of n as you know

1467
01:35:46,850 --> 01:35:49,790
this is just an intuition you have to be but it's

1468
01:35:49,790 --> 01:35:52,830
i think it's a good

1469
01:35:53,000 --> 01:35:54,710
OK so

1470
01:35:54,750 --> 01:36:00,150
so this this this is the case for the gaussians

1471
01:36:04,330 --> 01:36:09,230
OK so

1472
01:36:09,310 --> 01:36:11,500
let's not talk a little bit

1473
01:36:11,500 --> 01:36:13,040
about algorithms

1474
01:36:13,100 --> 01:36:20,230
so here is the first algorithm

1475
01:36:20,230 --> 01:36:21,750
take f

1476
01:36:21,750 --> 01:36:23,880
to be this more specific thing

1477
01:36:23,900 --> 01:36:26,250
i just take least squares fit

1478
01:36:26,270 --> 01:36:31,040
so at this point i just feet f of x i minus y i

1479
01:36:31,060 --> 01:36:34,460
plus i had this penalty so it's basically the same as before i just to

1480
01:36:34,480 --> 01:36:37,940
the loss function to be square

1481
01:36:37,960 --> 01:36:41,730
i know that the solution is of this form

1482
01:36:41,770 --> 01:36:43,880
that that's what i shot

1483
01:36:43,880 --> 01:36:48,660
so that's really is a bit of the of the subject but this is something

1484
01:36:48,660 --> 01:36:53,100
we developed for fun but it ended up being put in microsoft

1485
01:36:53,150 --> 01:36:57,700
image editing product which is out on the shelf near you

1486
01:36:58,750 --> 01:37:01,390
so now let's

1487
01:37:01,430 --> 01:37:04,400
get onto the the real business which is

1488
01:37:04,490 --> 01:37:06,340
how to model consoles

1489
01:37:09,360 --> 01:37:11,020
this should all work

1490
01:37:11,030 --> 01:37:13,980
so first of all i just can give you a sort of brief pointed to

1491
01:37:14,360 --> 01:37:17,070
the history of contour tracking

1492
01:37:17,090 --> 01:37:21,420
then i'm going to talk about this business of how to make the state space

1493
01:37:21,420 --> 01:37:27,320
is that represent both the variations the expect because of rigid body motion and the

1494
01:37:27,320 --> 01:37:30,570
variations that you expect because of

1495
01:37:30,620 --> 01:37:36,190
various deformations and then finally i'm going to describe how a curve fitting algorithm with

1496
01:37:36,190 --> 01:37:40,520
these models might work on it in this section i'm going to talk about the

1497
01:37:40,520 --> 01:37:47,230
deterministic fitting model but then in the next section will start reinterpreting everything as a

1498
01:37:50,150 --> 01:37:54,530
and see how that works and i mean the benefits of doing things probabilistically is

1499
01:37:54,540 --> 01:37:59,770
that also gives you then a key into how to learn the parameters in the

1500
01:37:59,770 --> 01:38:01,580
models that

1501
01:38:01,590 --> 01:38:03,770
we need to use

1502
01:38:03,780 --> 01:38:08,970
so outline models why for one more application just for the fun of it here's

1503
01:38:08,970 --> 01:38:14,400
some work was done with medics in in oxford where they wanted to track the

1504
01:38:14,400 --> 01:38:16,540
outline of images of the heart

1505
01:38:16,550 --> 01:38:19,060
and if you've had a heart attack

1506
01:38:19,070 --> 01:38:26,000
then a segment of muscle on this on this contour effectively has died and it

1507
01:38:26,000 --> 01:38:29,540
it doesn't stop moving so it's not as easy as it just detecting fact we

1508
01:38:29,550 --> 01:38:31,870
start moving body moves in a kind of lazy way

1509
01:38:31,900 --> 01:38:37,250
but it's being carried by ninety is neighbours is not really doing anything itself and

1510
01:38:39,160 --> 01:38:42,620
built software to analyse which was the lazy

1511
01:38:42,630 --> 01:38:46,410
contour and that

1512
01:38:46,460 --> 01:38:49,550
OK but yes one thing i want to point out is that there are no

1513
01:38:49,550 --> 01:38:53,460
distinguished features on this contour also there are no corners will be so so much

1514
01:38:53,460 --> 01:38:58,340
easier to solve the tracking problem if part controls would go around with all you

1515
01:38:58,340 --> 01:39:02,840
know tags on them or if they would please agree to be polygonal or something

1516
01:39:02,840 --> 01:39:06,750
so that we could we could catch on the vertices of the contour and track

1517
01:39:06,760 --> 01:39:08,490
them as they are but they just won't do that

1518
01:39:08,500 --> 01:39:11,560
so we really have to try to make things that will

1519
01:39:11,980 --> 01:39:16,060
track tools so actually there was a revolutionary idea

1520
01:39:16,070 --> 01:39:20,500
i was very excited when i this in eighty eight from cast openness and we

1521
01:39:20,560 --> 01:39:25,060
can the idea of the snake which you know the time computer vision was little

1522
01:39:25,080 --> 01:39:33,310
bit bogged down trying to use two-dimensional signal processing methods image analysis methods to make

1523
01:39:33,310 --> 01:39:36,680
structure emerges from images so was very popular at the time to take an image

1524
01:39:36,950 --> 01:39:43,400
invent a filter perhaps non-linear filter that would pull out consoles from the image that

1525
01:39:43,400 --> 01:39:48,710
was supposed to correspond to important physical contortion because what happened was

1526
01:39:48,770 --> 01:39:54,180
either you were so stringent field so stringent that missed important contours in the image

1527
01:39:54,180 --> 01:39:56,310
and everyone was upset all the

1528
01:39:56,760 --> 01:40:00,110
the filter was so generous that pulled the consuls but also a whole host of

1529
01:40:00,110 --> 01:40:04,960
other junk which humans can see we're not important so it becomes very frustrating and

1530
01:40:04,980 --> 01:40:11,020
they work with paper after paper about optimal edge detectors each one slightly more optimal

1531
01:40:11,030 --> 01:40:14,940
to have that than than the than the last

1532
01:40:14,960 --> 01:40:19,390
so this was a breath of fresh air because these young chaps i suppose there

1533
01:40:19,390 --> 01:40:24,910
in in the fifties now but they were young chaps had invented this method that

1534
01:40:24,910 --> 01:40:26,230
you know smelt of

1535
01:40:26,240 --> 01:40:32,350
computer games kind of made this live curve and they would drop it on an

1536
01:40:32,350 --> 01:40:35,490
image and it was kind of move around in the image of a lot of

1537
01:40:35,540 --> 01:40:39,040
the things if i really felt like a lot of fun and they provide a

1538
01:40:39,040 --> 01:40:44,460
toolkit to help manipulate these curves so they work because the naturally attracted to object

1539
01:40:44,540 --> 01:40:47,610
to call contours in the image but also you could sort of place above the

1540
01:40:47,610 --> 01:40:51,360
next one to sort of saying no i don't want you to look over here

1541
01:40:53,600 --> 01:40:57,100
the o thing was refreshing because it seemed to do away not just because it

1542
01:40:57,100 --> 01:40:58,660
was fun because it did away

1543
01:40:58,670 --> 01:41:04,700
with this mindset about and analyse images from the bottom up only and expecting good

1544
01:41:04,700 --> 01:41:09,950
structure to work to appear and people have lost patience with that and just in

1545
01:41:09,950 --> 01:41:13,150
the nick of time these people quite not with this idea that in principle would

1546
01:41:13,150 --> 01:41:17,340
allow you to come in with the strong object hypothesis

1547
01:41:19,980 --> 01:41:24,520
and sort of apply this hypothesis to the world so if you're a squirrel knew

1548
01:41:24,540 --> 01:41:28,080
looking for nuts that you would have a snake

1549
01:41:28,120 --> 01:41:32,250
one of these active condors in the shape of the outline of not and you

1550
01:41:32,250 --> 01:41:36,700
can drop it into the world and sort of see where

1551
01:41:37,340 --> 01:41:42,760
not work you know maybe an extremely efficient but it certainly contrasts with the idea

1552
01:41:42,760 --> 01:41:47,580
of taking the entire scene and analyzing everything in the scene including the branches and

1553
01:41:47,580 --> 01:41:50,340
leaves of grass and in really spending a lot of time on things that we're

1554
01:41:50,340 --> 01:41:55,330
not going to be interesting so here's the formalism you have contour are best which

1555
01:41:55,420 --> 01:42:00,560
parameterize from one zero to one and the feature map f of r which is

1556
01:42:00,610 --> 01:42:06,940
built yes using signal processing i mean image processing applied to the image to a

1557
01:42:06,940 --> 01:42:08,540
to enhance

1558
01:42:08,540 --> 01:42:15,000
important features but there's a big difference between enhancing features and actually detecting and detecting

1559
01:42:15,000 --> 01:42:20,110
was to have been announcing show you pictures and it is perfectly reasonable and now

1560
01:42:20,110 --> 01:42:21,380
we define

1561
01:42:21,400 --> 01:42:23,810
they defined and energy

1562
01:42:23,830 --> 01:42:28,690
which was the integral of this feature map f of along the

1563
01:42:28,730 --> 01:42:34,040
hypothetical could are less so now you can imagine moving the curve of our best

1564
01:42:34,060 --> 01:42:39,920
around in order to score as highly as possible on this feature map function f

1565
01:42:39,940 --> 01:42:41,920
but that might not be enough so

1566
01:42:42,000 --> 01:42:46,040
the gradient of this this cost function or energy we can think of as being

1567
01:42:46,040 --> 01:42:50,920
forced that will pull the curve distributed force that will pull curve towards features in

1568
01:42:50,920 --> 01:42:52,770
the image

1569
01:42:52,880 --> 01:42:58,600
and but that might not be enough for you because you probably want to impose

1570
01:42:58,600 --> 01:43:05,380
or prefer some kind of regularity in the curve so how about including some

1571
01:43:05,400 --> 01:43:12,670
regularizers for length the first derivative and the curvature of the second derivative that will

1572
01:43:12,960 --> 01:43:19,400
the when you omit minimize balance all of these these forces will

1573
01:43:19,460 --> 01:43:25,330
give you a compromise between regularity of the curve and adherence to these features and

1574
01:43:25,420 --> 01:43:29,460
this can be computed in practice by approximating curve

1575
01:43:29,500 --> 01:43:36,610
the curve as as the finite element chain or using finite differences

1576
01:43:38,670 --> 01:43:43,260
differences for first of design in second derivatives and turning this whole thing into a

1577
01:43:43,260 --> 01:43:44,650
numerical scheme

1578
01:43:45,130 --> 01:43:47,750
here i said i j the feature maps here are

1579
01:43:47,770 --> 01:43:51,330
the sort of thing you might get feature maps is something the top right something

1580
01:43:51,330 --> 01:43:53,360
that is supposed to

1581
01:43:53,500 --> 01:43:56,580
he doesn't show up very well for you i can see that you can

1582
01:43:58,150 --> 01:44:00,980
supposed to enhance edges in the scene respond to

1583
01:44:01,000 --> 01:44:07,150
areas of high contrast and here are ridges points of maximum brightness so you see

1584
01:44:07,150 --> 01:44:11,270
this highlight here has a rich following it and then

1585
01:44:11,270 --> 01:44:13,010
right well thank you very much for the quick

1586
01:44:13,150 --> 01:44:15,570
thing here the organisers and

1587
01:44:15,570 --> 01:44:19,800
indeed talk about is to allow the his sorry

1588
01:44:20,170 --> 01:44:24,330
so you have to talk about human travel patterns and

1589
01:44:24,370 --> 01:44:28,700
there is no better place to start with monkeys tigers

1590
01:44:28,710 --> 01:44:32,370
and why do we care about these monkeys will of course one reason we care

1591
01:44:32,370 --> 01:44:36,550
about them is because there was a group of researchers from from mexico

1592
01:44:37,110 --> 01:44:42,210
who had been tracking very close their motion and it's kind of one of the

1593
01:44:42,210 --> 01:44:44,660
issues of course the trick that

1594
01:44:44,660 --> 01:44:49,520
becomes interesting is how do you tracker monkeys motion and probably the best guess we

1595
01:44:49,520 --> 01:44:53,150
all of us have is that you would actually put the GPS on that

1596
01:44:53,160 --> 01:44:57,800
and it's true it's all that actually gps involved in that but rather the GPS

1597
01:44:57,800 --> 01:45:00,810
was not in the monkeys but undergraduate students

1598
01:45:00,830 --> 01:45:05,010
so there was always a distant under each monk in the forest and they would

1599
01:45:05,010 --> 01:45:11,050
rather than twenty four hours essential that would change each other and the position was

1600
01:45:11,050 --> 01:45:14,130
tracked essentially and that's how do we know where the monkeys

1601
01:45:14,160 --> 01:45:19,650
the end however is that they ended up getting trajectories like that that indicate how

1602
01:45:19,650 --> 01:45:26,760
the monkeys moved around in in space in this big forest on different scales and

1603
01:45:26,760 --> 01:45:31,490
the conclusion was that the best way to describe this trajectory is not random motion

1604
01:45:31,490 --> 01:45:34,430
but the think that this is actually really what matters

1605
01:45:35,260 --> 01:45:40,150
in the way the fact that animals may finally before but was not

1606
01:45:40,150 --> 01:45:45,680
new idea at the moment but rather the idea came from a seminal paper by

1607
01:45:45,730 --> 01:45:52,790
richemont on it'll just group nineteen ninety six track trusses and they found that the

1608
01:45:52,990 --> 01:45:59,400
again the trajectory follows a kind of this lady pattern and then he two years

1609
01:45:59,400 --> 01:46:03,900
later they came up with an explanation that it's very much to the kind of

1610
01:46:03,980 --> 01:46:06,120
the community was interested an emotion

1611
01:46:06,160 --> 01:46:08,570
because they found that

1612
01:46:08,590 --> 01:46:15,490
the reason why animals follow such only pattern it's because it's actually optimized for finding

1613
01:46:15,490 --> 01:46:19,940
food so if the animal is tries to find hard to find patches of food

1614
01:46:20,310 --> 01:46:23,850
then the best way to do that is to not to kind of searches around

1615
01:46:23,850 --> 01:46:28,640
the one but rather to do big jumps occasionally and in many many small steps

1616
01:46:28,640 --> 01:46:34,320
so the ladyhawke actually is really an optimal strategy in that respect and that's pretty

1617
01:46:34,320 --> 01:46:38,510
much the problem and there lots of work and then in the community who have

1618
01:46:38,530 --> 01:46:44,410
from that type of trajectory really follow really kind of describes many different dimensions many

1619
01:46:44,410 --> 01:46:45,560
different species

1620
01:46:45,600 --> 01:46:50,260
then of course for those of you who followed some of the field this year

1621
01:46:50,260 --> 01:46:54,980
there was a big surprise that the group was actually done the original owners have

1622
01:46:54,980 --> 01:46:59,290
gone back and analyse the data and they said that not as much the sources

1623
01:46:59,290 --> 01:47:03,010
don't follow around the world the fly but it's just hard to conclude on the

1624
01:47:03,010 --> 01:47:07,140
existing data there was some problem with how the data was collected by the biologist

1625
01:47:07,350 --> 01:47:14,230
so therefore kind of the idea became somewhat shaky whether really animals follow that but

1626
01:47:14,230 --> 01:47:15,750
if you still there

1627
01:47:15,760 --> 01:47:20,320
and and kind of a few months after this paper indicated that may be of

1628
01:47:20,320 --> 01:47:26,010
course is that the floods came out the very detailed paper by seems that has

1629
01:47:26,010 --> 01:47:31,070
many many different sea animals and they found very strong evidence for this living type

1630
01:47:31,230 --> 01:47:35,700
of motion so what i'm showing here is something that formed from the same set

1631
01:47:35,750 --> 01:47:41,540
of people this is the basking shark is the trajectory they looked at other sharks

1632
01:47:41,540 --> 01:47:45,110
but they looked at sea turtles and stingrays and so on and for each of

1633
01:47:45,110 --> 01:47:49,630
them the best way to describe the trajectory was to assume that is actually on

1634
01:47:49,630 --> 01:47:51,130
the flight

1635
01:47:51,880 --> 01:47:56,140
so what are the only reason i'm telling you this is the kind of understand

1636
01:47:56,140 --> 01:47:59,790
that there is a very good growing body of literature now

1637
01:47:59,810 --> 01:48:06,000
in the emotional community and then biologists is very interested in particular subject that animals

1638
01:48:06,000 --> 01:48:10,880
do not simple around the world but rather actually follow this limit trajectories

1639
01:48:10,890 --> 01:48:15,100
now the question comes up is what do we do because obviously one of the

1640
01:48:15,100 --> 01:48:18,440
reason that animals do that is because they are searching for food

1641
01:48:18,570 --> 01:48:23,230
and just kind of a summary you know like do any necessary for flight or

1642
01:48:23,230 --> 01:48:27,270
not was just the conference a few weeks ago in stockholm which was focused on

1643
01:48:27,270 --> 01:48:31,210
this problem and i think the consensus in the community is that really there's very

1644
01:48:31,210 --> 01:48:32,630
strong evidence they do

1645
01:48:32,830 --> 01:48:35,950
but of course is yes or no and i think the question is not is

1646
01:48:35,960 --> 01:48:39,970
much more about data there are other they follow it or not but the question

1647
01:48:39,970 --> 01:48:44,500
is what humans do i mean obviously not searching for food so that the pattern

1648
01:48:44,500 --> 01:48:48,680
is not determined by the need to kind of go and find patches of food

1649
01:48:48,680 --> 01:48:50,680
that are hidden somewhere in the fits

1650
01:48:50,820 --> 01:48:55,490
so so therefore we would expect that perhaps there is no reason for us to

1651
01:48:55,490 --> 01:48:57,020
have nearly trajectory

1652
01:48:57,090 --> 01:49:01,810
the surprise came about two years ago when there was a study indicated that is

1653
01:49:01,810 --> 01:49:04,680
not so and and indeed we do follow the path

1654
01:49:04,720 --> 01:49:05,950
and if we go

1655
01:49:05,960 --> 01:49:09,620
this kind of summarise so what is the difference between the random walk on radio

1656
01:49:09,650 --> 01:49:15,760
four and the simple random walk everybody committee knows what is that a moving forward

1657
01:49:15,760 --> 01:49:20,320
the roughly the same distance in every time time frame

1658
01:49:20,530 --> 01:49:26,880
three something what you see over here this is nothing but around the world however

1659
01:49:26,880 --> 01:49:32,260
the difference between the two is that the the size distribution of the consecutive jumps

1660
01:49:33,260 --> 01:49:37,820
not gaussian that function as an around the world but rather comes from power law

1661
01:49:37,820 --> 01:49:43,990
distribution and you can talk about lady flights only works and there's a wonderful because

1662
01:49:44,070 --> 01:49:49,210
papers by actually discusses the differences between them but the point is that when you

1663
01:49:49,210 --> 01:49:50,690
look at the leaf

1664
01:49:50,740 --> 01:49:55,900
visually you can see very very different from around the world because did was striking

1665
01:49:55,920 --> 01:49:58,870
it is very very long jumps in the trajectory

1666
01:49:58,940 --> 01:50:02,760
so the question is what do we do humans do we do really reflect what

1667
01:50:02,760 --> 01:50:06,920
you see over here that we do and walk what we see here or something

1668
01:50:07,720 --> 01:50:08,650
and so so

1669
01:50:08,700 --> 01:50:11,870
the first thing back came about two two years ago on that

1670
01:50:11,910 --> 01:50:14,650
and can very interesting direction

1671
01:50:14,700 --> 01:50:20,440
there is there was there's about site which is based in boston that is meant

1672
01:50:20,440 --> 01:50:25,680
to attract money had so you think about the bill and you want to visual

1673
01:50:25,680 --> 01:50:30,100
i'm not telling the model that uses image specific priors images should look like this

1674
01:50:30,100 --> 01:50:33,290
insight is just the standard algorithm so if i played

1675
01:50:33,870 --> 01:50:35,290
apply exact same algorithm

1676
01:50:35,750 --> 01:50:37,500
two hundred character recognition

1677
01:50:38,120 --> 01:50:41,640
and at the local level he discovers these little strokes at higher level

1678
01:50:42,120 --> 01:50:48,480
it figures out high-level strokes and this is again just by looking at the data

1679
01:50:48,730 --> 01:50:52,540
which we think is the right representation four modeling handwritten characters

1680
01:50:53,000 --> 01:50:54,080
and if you look at the type of

1681
01:50:54,450 --> 01:50:56,560
structural classes that discovery

1682
01:50:57,000 --> 01:51:01,410
you know it's basically saying all these hidden characters get put together in each one

1683
01:51:01,410 --> 01:51:06,220
of them basically shares the same distribution high-level parts and each one of these had

1684
01:51:07,290 --> 01:51:09,120
defines a distribution over low-level parts

1685
01:51:09,540 --> 01:51:13,600
so it figures out on its own different alphabets if you if you

1686
01:51:14,080 --> 01:51:16,500
think about we see no groups together reasonable

1687
01:51:17,040 --> 01:51:21,700
characters in terms of performance again these models do much much better in terms of generalizing to new things

1688
01:51:22,390 --> 01:51:26,000
but one thing you can do is you can ask them all to generate new characters

1689
01:51:27,060 --> 01:51:29,500
now that understand something about characters let's say

1690
01:51:30,660 --> 01:51:33,460
this is the real data that's it one particular so cause

1691
01:51:34,140 --> 01:51:37,430
and i can ask the more can you actually generate new characters that have the

1692
01:51:37,430 --> 01:51:39,930
same statistical properties it is these characters

1693
01:51:40,850 --> 01:51:44,930
right hand because it's a generative model yes indeed we can simulate from this model

1694
01:51:46,910 --> 01:51:48,950
if we simulate from this is what it looks like

1695
01:51:50,250 --> 01:51:53,850
so these are completely housing it that is nothing like that in the training set

1696
01:51:54,460 --> 01:51:56,000
it just makes things up

1697
01:51:56,730 --> 01:51:59,450
you know if i give you this self about is what generates

1698
01:52:00,180 --> 01:52:01,430
causes and again

1699
01:52:02,120 --> 01:52:06,540
this is very different from what you see in the training data so really i generalizes

1700
01:52:07,000 --> 01:52:09,560
obviously something like that is much more difficult to generate

1701
01:52:10,520 --> 01:52:15,540
but the same model can be applied speech text video in in in any other high dimensional data

1702
01:52:16,230 --> 01:52:19,290
in fact if you apply the same model to modeling motion capture data

1703
01:52:19,810 --> 01:52:22,270
principles i showed these these two different

1704
01:52:24,560 --> 01:52:27,060
right so one is known as the work one of them is the drunken walk

1705
01:52:28,020 --> 01:52:30,370
right so the data here is just the time series each

1706
01:52:30,910 --> 01:52:33,560
motion capture is characterized by its different points

1707
01:52:34,560 --> 01:52:35,140
through time

1708
01:52:36,100 --> 01:52:39,430
now let's say you know you're looking at these different walks in now

1709
01:52:39,910 --> 01:52:44,640
i'm gonna show you a sexy work and i'm only gonna show you sixty frames of sexy work right

1710
01:52:45,160 --> 01:52:46,540
so this has sixty work looks like

1711
01:52:48,000 --> 01:52:48,350
i am

1712
01:52:50,540 --> 01:52:54,950
basically by just you don't need to look at you know thousands of examples of

1713
01:52:54,950 --> 01:52:57,480
sex work for you to figure out what sex workers right

1714
01:52:58,560 --> 01:53:02,950
and indeed if you apply this model to this kind of structure we're trying to

1715
01:53:03,480 --> 01:53:07,620
grouped together different works and figure out what's shared between those different walks

1716
01:53:08,160 --> 01:53:11,730
you can do much better in terms of recognizing sexy well from all the other works

1717
01:53:12,540 --> 01:53:16,700
right so this is again the same thing can be applied to sort of modeling time

1718
01:53:17,370 --> 01:53:19,680
time series data which is which is quite exciting

1719
01:53:20,930 --> 01:53:24,080
now in the last part of the tutorial so we've got so far about

1720
01:53:26,500 --> 01:53:29,430
both machines one shot learning learning structured models

1721
01:53:30,060 --> 01:53:33,850
so now we're gonna look at the related problem of multimodal learning

1722
01:53:35,230 --> 01:53:36,200
so if we could be

1723
01:53:37,220 --> 01:53:38,290
datasets today

1724
01:53:38,790 --> 01:53:39,520
is basically

1725
01:53:40,660 --> 01:53:45,540
you know it's a collection of modalities right images associated with text if you look

1726
01:53:45,540 --> 01:53:50,600
at robotics applications you have lots of different sensors how can integrate all these different

1727
01:53:50,600 --> 01:53:51,480
modalities together

1728
01:53:52,060 --> 01:53:55,480
so that's a very exciting research directions in

1729
01:53:56,700 --> 01:53:58,730
in the machine learning and deep learning communities

1730
01:53:59,390 --> 01:54:03,950
right which is like the is you'd like to get modality free representation given image

1731
01:54:03,950 --> 01:54:07,870
and text like that some semantic meaning from both of these modalities

1732
01:54:08,370 --> 01:54:13,100
and obviously this can be useful right so he can be useful for maybe one improve classification

1733
01:54:14,700 --> 01:54:15,290
the fact that

1734
01:54:16,230 --> 01:54:18,390
you know you have a word that says kangaroo island

1735
01:54:18,980 --> 01:54:19,370
might be

1736
01:54:19,870 --> 01:54:24,680
a very useful thing for you to actually recognise that you somewhere on this is somewhere in australia right

1737
01:54:25,290 --> 01:54:29,980
so it could be useful for classification it could be useful for filling missing modalities

1738
01:54:30,270 --> 01:54:33,450
given an image can you can describe what's going on in the image

1739
01:54:34,200 --> 01:54:38,810
it also useful forty true also if i show you a bunch of words actually related images

1740
01:54:39,500 --> 01:54:42,100
so if i can model these different modalities i can do a lot of

1741
01:54:42,750 --> 01:54:43,580
a lot of things together

1742
01:54:44,000 --> 01:54:47,230
so one thing you can do is you can cycle on the joint distribution of

1743
01:54:47,330 --> 01:54:50,660
images and text or tags in this case words

1744
01:54:53,040 --> 01:54:56,770
age here the latent representation will be diffused representation

1745
01:54:57,600 --> 01:54:59,850
the semantic representation of both of these modalities

