1
00:00:00,000 --> 00:00:02,050
OK welcome to

2
00:00:02,480 --> 00:00:05,140
september first

3
00:00:05,160 --> 00:00:06,740
my name is dave fly

4
00:00:06,750 --> 00:00:09,070
i'm going to talk about a topic models

5
00:00:09,090 --> 00:00:14,190
i'm from books and from princeton university

6
00:00:14,210 --> 00:00:15,280
in new jersey

7
00:00:15,290 --> 00:00:18,820
where it's four o'clock in the morning

8
00:00:18,890 --> 00:00:23,290
and so

9
00:00:23,340 --> 00:00:26,270
this is what it's going to look like to teach at four in the morning

10
00:00:28,980 --> 00:00:33,270
OK and please interrupt me a lot in this talk so that it will help

11
00:00:33,480 --> 00:00:35,550
stimulate me to wake me

12
00:00:38,470 --> 00:00:40,020
OK good so

13
00:00:40,050 --> 00:00:41,840
what topic modeling is about

14
00:00:41,840 --> 00:00:44,420
is that as

15
00:00:44,480 --> 00:00:50,590
you all know is almost redundant for more information becomes available to us it becomes

16
00:00:50,590 --> 00:00:53,840
more difficult for us to be able to quickly

17
00:00:53,850 --> 00:00:59,100
access it and search for things in it and understand it and basically get something

18
00:00:59,100 --> 00:00:59,920
out of it

19
00:00:59,940 --> 00:01:04,940
and so we need new tools new algorithmic tools to help us organize search and

20
00:01:04,940 --> 00:01:12,530
understand these vast amounts of information things like text archives and image archives and all

21
00:01:12,560 --> 00:01:18,270
different kinds of data that we're all just for creating and having access to constantly

22
00:01:19,940 --> 00:01:27,940
with a topic modelling provides are methods for automatically organizing understanding searching summarizing exploiting these

23
00:01:27,940 --> 00:01:32,140
large electronic archive and the basic idea

24
00:01:32,160 --> 00:01:37,940
are is is these three steps first we take a big corpus like wikipedia as

25
00:01:37,940 --> 00:01:43,310
a and uncover the hidden topical patterns that pervaded so

26
00:01:43,330 --> 00:01:49,030
wikipedia has got some number of millions of articles and there are about some things

27
00:01:49,050 --> 00:01:53,090
those things overlap and we want to uncover what those things are what of the

28
00:01:53,110 --> 00:01:55,640
different topics that pervade this collection

29
00:01:55,660 --> 00:02:00,470
then we we want to annotate the documents according to the topics so i understand

30
00:02:00,470 --> 00:02:04,410
what say you know two hundred and fifty topics in wikipedia are now if i

31
00:02:04,410 --> 00:02:08,960
plot the document out of wikipedia what topics is that article about how can i

32
00:02:09,040 --> 00:02:12,990
edited that article according to the topics they discovered in step one

33
00:02:13,000 --> 00:02:17,250
and finally we want to use these annotations to organise summarize and do whatever it

34
00:02:17,250 --> 00:02:20,840
is we want to do obviously actually but doing this in a vacuum is only

35
00:02:20,850 --> 00:02:24,940
so interesting we really want to say OK now that i've been able to my

36
00:02:24,940 --> 00:02:29,380
essentially markup my collection according to these automatically found topics i want to use that

37
00:02:29,380 --> 00:02:31,440
markup collection

38
00:02:31,470 --> 00:02:34,010
you can think of it as if i had

39
00:02:34,030 --> 00:02:38,650
ten million people to go through wikipedia and carefully organised it then what i do

40
00:02:38,650 --> 00:02:42,970
with their organisation since i don't have those ten million people i build algorithms to

41
00:02:42,990 --> 00:02:44,940
do that one

42
00:02:50,100 --> 00:02:52,660
this is backwards it's the same

43
00:02:52,690 --> 00:02:56,210
have almost hit by cars about six times already

44
00:02:56,220 --> 00:02:57,690
the same idea OK

45
00:02:57,710 --> 00:03:00,530
so we can do with it

46
00:03:00,530 --> 00:03:02,870
with topic models things like discovery

47
00:03:02,880 --> 00:03:06,370
topics from a corpus so here are some topic so a topic is going to

48
00:03:06,370 --> 00:03:10,530
be a distribution over terms in the vocabulary and these are the top most probable

49
00:03:10,530 --> 00:03:16,960
terms and for topics that were uncovered by analyzing the text in in the journal

50
00:03:16,960 --> 00:03:22,600
science so here's a topic human genome DNA genetic evolution evolutionary

51
00:03:22,620 --> 00:03:28,820
species organisms disease host bacterial diseases computer models information data computers these are words that

52
00:03:28,820 --> 00:03:33,280
seem to go together in some kind of semantic thematically coherent way but i should

53
00:03:33,280 --> 00:03:37,740
say that part of this talk probably on thursday we're going to discuss the pitfalls

54
00:03:37,740 --> 00:03:40,180
of overly interpreting these

55
00:03:40,190 --> 00:03:45,440
probability distributions over words but for now since we're learning about it we can pretend

56
00:03:45,440 --> 00:03:49,100
like those pitfalls don't exist and interpret them so

57
00:03:49,120 --> 00:03:53,260
these are for topics discovered from the journal science

58
00:03:55,680 --> 00:04:04,990
the there is no good reason not to stand

59
00:04:05,000 --> 00:04:12,410
so you know sometimes stemming i mean when i when i feel the topic models

60
00:04:12,410 --> 00:04:17,570
sometimes i use stemming it can be when you're looking at these topics if you're

61
00:04:17,570 --> 00:04:21,960
doing this for like a system for someone to be able to to quickly browser

62
00:04:21,980 --> 00:04:29,060
navigator documents some stemmers can be overly with word aggressive and you can't really recognise

63
00:04:29,060 --> 00:04:30,100
the rights that they

64
00:04:30,140 --> 00:04:36,260
but they get back but then there's one that i know my student likes

65
00:04:36,270 --> 00:04:39,850
i can remember what it's called but it's more conservative stanford just it will remove

66
00:04:39,850 --> 00:04:42,900
things like computer computers

67
00:04:42,920 --> 00:04:46,940
but now there's something is a good idea

68
00:04:46,960 --> 00:04:48,480
other questions

69
00:04:48,620 --> 00:04:56,390
things like so discovered topics discovering topics in understanding how the words change how within

70
00:04:56,390 --> 00:05:00,620
the topic through time so here are here is time from eighteen eighty until two

71
00:05:00,620 --> 00:05:05,880
thousand and here two different topics that i've named again ignoring the pitfalls of naming

72
00:05:05,880 --> 00:05:10,680
topics and here are some words on those topics and and see how their probability

73
00:05:10,680 --> 00:05:17,250
is changing over time can we can uncover this time changing topics with topic models

74
00:05:17,250 --> 00:05:20,460
we can model connections between topics so

75
00:05:20,510 --> 00:05:25,710
here again are lists of words that we are interpreting topics and again this is

76
00:05:25,710 --> 00:05:26,750
from science

77
00:05:26,750 --> 00:05:31,160
ask label for in order to learn the most about this database

78
00:05:31,240 --> 00:05:35,640
and this you is interfaces for search engines based on this

79
00:05:38,710 --> 00:05:43,140
now question i'm going to push the idea for because active learning when we reach

80
00:05:43,140 --> 00:05:46,640
active learning we start looking at decisions

81
00:05:46,660 --> 00:05:50,300
and in this case i was being myopic about my decision to not what i

82
00:05:50,300 --> 00:05:52,480
was only looking one step ahead

83
00:05:52,540 --> 00:05:56,930
to make this is sometimes i want to make plans look several steps ahead

84
00:05:56,990 --> 00:05:59,540
in order to make the best decision

85
00:05:59,540 --> 00:06:03,340
so if i have a clinical trial one to consider

86
00:06:03,390 --> 00:06:07,860
all the events that could happen when i bring to patients and to determine when

87
00:06:07,870 --> 00:06:11,580
to best administered drugs

88
00:06:11,900 --> 00:06:16,500
this problem tend to be actually some of the hardest character

89
00:06:16,560 --> 00:06:20,110
and chapel will be talking to you about reinforcement learning which is a way of

90
00:06:20,110 --> 00:06:21,400
doing this

91
00:06:21,440 --> 00:06:22,720
i will introduce you

92
00:06:24,110 --> 00:06:24,910
u two

93
00:06:24,940 --> 00:06:29,360
simulation to solve some of these problems so i'll just basically for the next few

94
00:06:29,360 --> 00:06:42,470
minutes just described two case studies

95
00:06:42,510 --> 00:06:48,570
so icon

96
00:06:48,610 --> 00:06:51,150
so can you pick up like

97
00:06:51,180 --> 00:06:55,170
hot coupling

98
00:07:15,030 --> 00:07:30,610
can europe the question i'm struggling to understand

99
00:07:38,670 --> 00:07:41,250
we get no repeat

100
00:07:41,460 --> 00:07:44,140
you know it

101
00:07:44,160 --> 00:07:48,120
this is such a radical neighbors

102
00:07:48,150 --> 00:07:52,540
i mean you see is that arise from

103
00:07:52,620 --> 00:07:57,390
if you want to write

104
00:07:59,230 --> 00:08:01,530
OK thanks for the question so

105
00:08:01,540 --> 00:08:02,840
the question is

106
00:08:02,850 --> 00:08:04,980
if we have the that graph

107
00:08:05,070 --> 00:08:09,760
he's asking do i start from the local interactions and

108
00:08:09,810 --> 00:08:11,570
and in particular

109
00:08:11,680 --> 00:08:14,870
the range and then move on to these

110
00:08:14,910 --> 00:08:16,470
the answer is no

111
00:08:16,480 --> 00:08:20,190
what you might do is to try to look at the strongest correlations in the

112
00:08:20,190 --> 00:08:25,480
potentials and try to build the spanning tree to weighted spanning tree for those

113
00:08:25,490 --> 00:08:26,990
and then you add

114
00:08:27,010 --> 00:08:32,340
edges that your questions are very good one because how you constructed graph

115
00:08:32,410 --> 00:08:36,670
you construct this graph the beautiful thing because you have a dynamic system is you

116
00:08:36,670 --> 00:08:41,060
can construct this graph by adding edges and when you encounter trouble

117
00:08:41,060 --> 00:08:45,870
the importance weights divergence something you can backtrack

118
00:08:45,920 --> 00:08:48,390
and you can grow some other different way

119
00:08:48,400 --> 00:08:51,390
and you can even do some sort of planning to decide how you're going to

120
00:08:51,390 --> 00:08:54,650
grow and you can have different heuristics and play with them

121
00:08:54,690 --> 00:08:57,080
and in fact that's what's done

122
00:08:57,120 --> 00:09:00,550
in the field of protein folding using monte carlo methods

123
00:09:00,590 --> 00:09:03,330
they do exactly that sort of thing

124
00:09:03,350 --> 00:09:07,710
how to construct the graph is tricky it's not there is an optimal way using

125
00:09:07,710 --> 00:09:11,500
one among the techniques and i'm going to describe the problem is that those techniques

126
00:09:11,500 --> 00:09:13,310
are too expensive

127
00:09:13,320 --> 00:09:15,880
so this is sort of a

128
00:09:15,930 --> 00:09:17,760
trade trade-off

129
00:09:19,580 --> 00:09:24,590
because you're coupling partners

130
00:09:24,600 --> 00:09:28,310
and we wanted to be the best porn stars on the web

131
00:09:28,780 --> 00:09:32,720
the title of the page

132
00:09:32,740 --> 00:09:37,560
well the reason is why we actually don't had to the the edges

133
00:09:37,560 --> 00:09:42,310
the details this i'm trying to just give you a taste of it is supposed

134
00:09:42,310 --> 00:09:45,840
to go into the details when we have an edge we just don't add the

135
00:09:45,840 --> 00:09:51,100
network to note but we do that by by raising the temperature

136
00:09:51,140 --> 00:09:53,410
we any of the agent

137
00:09:53,420 --> 00:09:56,410
that way that all the edges are here

138
00:09:56,460 --> 00:09:58,380
or some of the edges

139
00:09:58,410 --> 00:10:00,300
this different games we use

140
00:10:00,340 --> 00:10:10,550
the protein folding one is important actually there's a lot of work

141
00:10:11,080 --> 00:10:14,510
to be done there because the connect the idea which is how you grow the

142
00:10:14,550 --> 00:10:19,440
sequences and how you across right to self avoiding changed to concentrate on the prose

143
00:10:19,490 --> 00:10:22,530
regions of high probability

144
00:10:23,010 --> 00:10:26,380
it's quite it's quite an interesting research

145
00:10:26,390 --> 00:10:28,810
the area

146
00:10:28,890 --> 00:10:32,140
more to come soon

147
00:10:32,260 --> 00:10:37,510
OK so here's an example

148
00:10:37,570 --> 00:10:40,800
i wanted to some control examples of something before we go

149
00:10:40,800 --> 00:10:43,160
and slow down a bit

150
00:10:47,430 --> 00:10:48,570
you want to go

151
00:10:48,610 --> 00:10:50,030
the beach

152
00:10:50,050 --> 00:10:52,740
you have a rough map in your head

153
00:10:52,740 --> 00:10:55,740
of what that wooden bridge is

154
00:10:55,740 --> 00:11:00,000
so what i five and machine learning do actually need solve any of those problems

155
00:11:02,520 --> 00:11:05,450
i'm going to claim that will need any new type of machine learning

156
00:11:07,050 --> 00:11:08,390
i was showing before r

157
00:11:08,610 --> 00:11:10,040
low those problems are really

158
00:11:10,520 --> 00:11:12,150
known problems in some form

159
00:11:12,880 --> 00:11:15,260
however there is specificity which is

160
00:11:16,160 --> 00:11:19,110
pertinent to the three data and network traffic

161
00:11:19,550 --> 00:11:20,350
which unit cell

162
00:11:21,090 --> 00:11:22,810
and also there is the question about

163
00:11:23,960 --> 00:11:28,870
where these things are actually operating what is our infrastructure how do they sleep with then

164
00:11:30,930 --> 00:11:34,450
decisions on what to build what algorithms should be applied

165
00:11:34,930 --> 00:11:38,150
applying and there's also of a really dependent on

166
00:11:38,700 --> 00:11:41,160
this kind of infrastructure implementation issues

167
00:11:42,660 --> 00:11:43,890
the problems are trying to solve

168
00:11:47,240 --> 00:11:47,870
so missed out

169
00:11:48,650 --> 00:11:49,330
by saying we

170
00:11:49,920 --> 00:11:54,330
more let's try to apply machine learning the social networks that there is a social network

171
00:11:54,820 --> 00:11:55,130
you like

172
00:11:55,850 --> 00:11:59,990
and this a social network where lv information comes in the form of text

173
00:12:02,570 --> 00:12:03,370
what the really

174
00:12:04,300 --> 00:12:06,240
tell us i mean doesn't stop

175
00:12:07,640 --> 00:12:08,730
we probably can get away

176
00:12:09,390 --> 00:12:11,180
with using fairly simple models

177
00:12:12,030 --> 00:12:13,300
and what can i say about

178
00:12:13,840 --> 00:12:16,670
well is like a long long long line of of

179
00:12:17,340 --> 00:12:18,500
research papers

180
00:12:19,010 --> 00:12:19,500
both by

181
00:12:20,240 --> 00:12:21,730
people academia and industry

182
00:12:22,330 --> 00:12:25,740
which tend to show that formal problems involving text thanks

183
00:12:26,330 --> 00:12:28,240
and especially large quantities of text

184
00:12:28,740 --> 00:12:31,090
it pays off to use fairly simple models

185
00:12:32,040 --> 00:12:33,960
which can actually brought lot of data

186
00:12:34,450 --> 00:12:35,910
rather than concentrating on

187
00:12:36,520 --> 00:12:41,520
complex models which most likely really cannot handle data at all in the first one

188
00:12:42,490 --> 00:12:44,310
and you actually can tell they that they

189
00:12:45,060 --> 00:12:46,760
accuracy gaze up small

190
00:12:48,420 --> 00:12:49,530
they actually exist about

191
00:12:53,720 --> 00:12:54,650
the second plane

192
00:12:55,160 --> 00:12:56,920
which is probably more important is

193
00:12:57,980 --> 00:12:59,140
we want to use

194
00:12:59,820 --> 00:13:03,810
machine learning for social networks in a way which is really easy use

195
00:13:05,790 --> 00:13:10,360
not everybody is a machine learning experts and even not everybody's data sciences

196
00:13:11,220 --> 00:13:15,640
so if people want apply those models need to be able to apply them in

197
00:13:15,640 --> 00:13:17,610
a way which fits with their day-to-day

198
00:13:18,450 --> 00:13:19,390
dealing with data

199
00:13:21,180 --> 00:13:21,590
if you know

200
00:13:22,410 --> 00:13:25,370
user of some kind of baltic pipeline you probably have york

201
00:13:25,860 --> 00:13:28,610
o to settle things like today you know how to do

202
00:13:29,370 --> 00:13:31,590
and to the extent you would like to play with models

203
00:13:32,230 --> 00:13:34,380
you want to be able to integrate them very easily

204
00:13:35,370 --> 00:13:36,630
and last is the key

205
00:13:37,180 --> 00:13:38,850
requirement for making this happen

206
00:13:45,530 --> 00:13:48,190
machine learning for social network of course social networks

207
00:13:48,630 --> 00:13:50,090
as the name suggests graphs

208
00:13:52,270 --> 00:13:55,440
but is graph processing really requirement out say

209
00:13:56,260 --> 00:14:01,740
the existence of graphs really provide an asset whatever we do it necessarily hard requirement

210
00:14:01,740 --> 00:14:03,740
for whatever machine learning want to plot

211
00:14:06,630 --> 00:14:10,850
it really stems from the power of the social graph so the guys are connected

212
00:14:11,380 --> 00:14:12,670
by users usually

213
00:14:13,270 --> 00:14:16,100
for purpose and the signal propagation in the graph

214
00:14:16,610 --> 00:14:19,960
that's the bring relevance so when we apply simple models

215
00:14:20,570 --> 00:14:24,540
over a graph the aggregation over the graph increases the power those models

216
00:14:24,980 --> 00:14:26,030
making them more accurate

217
00:14:26,490 --> 00:14:27,210
so the plus

218
00:14:30,730 --> 00:14:35,140
now the real requirements scalability so we scale all the data we have

219
00:14:35,850 --> 00:14:37,410
and all that they were going out

220
00:14:37,910 --> 00:14:38,990
with the scale they

221
00:14:39,680 --> 00:14:40,780
time of the data

222
00:14:40,820 --> 00:14:42,930
arrivals suppressing over data streams

223
00:14:43,600 --> 00:14:45,550
and we need to update is mostly fast

224
00:14:52,840 --> 00:14:55,340
straightforward adaptation buildings together

225
00:14:55,970 --> 00:14:59,280
and you get a bunch of engineers and researchers and scientists in the room

226
00:15:00,550 --> 00:15:01,910
the natural tendency is to

227
00:15:03,740 --> 00:15:06,230
built a lot of stuff which is called the thing up

228
00:15:08,900 --> 00:15:11,460
yes i mean possible to build everything from scratch

229
00:15:12,100 --> 00:15:15,930
but what the real benefit in doing so we probably will actually be able to finish anything

230
00:15:17,810 --> 00:15:18,840
there are questions are

231
00:15:19,420 --> 00:15:21,240
i mean what do actually need to build

232
00:15:22,150 --> 00:15:24,240
in our infrastructure to make it happen

233
00:15:24,780 --> 00:15:26,430
and how can we possibly

234
00:15:27,100 --> 00:15:30,130
re-use existing staff from wherever it comes

235
00:15:31,490 --> 00:15:37,470
social buildings the link low-level machine learning libraries feature extraction pipelines

236
00:15:38,190 --> 00:15:40,970
maybe they distributed processing platforms

237
00:15:41,660 --> 00:15:42,770
maybe the good cold

238
00:15:43,270 --> 00:15:45,800
the whole visualization called so support

239
00:15:46,390 --> 00:15:50,240
so the the kind of the questions which naturally come to mind and it can be answered

240
00:15:50,770 --> 00:15:51,990
in order to build a real system

241
00:15:54,770 --> 00:15:55,520
so forcefully

242
00:15:56,770 --> 00:15:58,550
when we started looking at this problem

243
00:15:59,130 --> 00:16:00,880
we're not operating in a vacuum

244
00:16:01,410 --> 00:16:04,200
to some extent the answers are constrained by

245
00:16:04,860 --> 00:16:06,870
whatever infrastructure was already in place

246
00:16:08,930 --> 00:16:10,970
just the shows some of the things which are

247
00:16:11,460 --> 00:16:12,870
operating in our pipeline

248
00:16:15,010 --> 00:16:17,350
it's how apache hadoop essentially

249
00:16:17,930 --> 00:16:19,960
at with a lot of fun surrounded

250
00:16:20,550 --> 00:16:20,970
so they

251
00:16:23,210 --> 00:16:25,720
the languages such as scala and java and pick

252
00:16:27,170 --> 00:16:29,880
there's things which make it past the happen such as

253
00:16:31,160 --> 00:16:32,620
me cells and zookeeper

254
00:16:33,050 --> 00:16:36,980
and tell stories components such as founder and those based on full support

255
00:16:37,680 --> 00:16:39,420
so those things provide real

256
00:16:39,830 --> 00:16:41,680
constraints to how it gets built

257
00:16:45,230 --> 00:16:46,810
naturally we decided about

258
00:16:47,740 --> 00:16:50,410
we want to capitalize on the use of of the

259
00:16:51,380 --> 00:16:53,190
so they can be processes in

260
00:16:53,840 --> 00:16:55,270
in many different ways

261
00:16:55,850 --> 00:16:59,560
but because our analytics pipeline already produces everything do

262
00:17:00,740 --> 00:17:03,660
wanted really did everything had the best well especially is

263
00:17:04,420 --> 00:17:07,180
as fitting into the map reduce process

264
00:17:09,690 --> 00:17:11,480
was pipeline operates

265
00:17:11,930 --> 00:17:13,010
using one particular

266
00:17:13,900 --> 00:17:15,110
technology called

267
00:17:16,320 --> 00:17:17,660
so for those who don't know

268
00:17:18,400 --> 00:17:22,470
what about much about pages because they a high-level data-flow language

269
00:17:23,110 --> 00:17:25,550
the people right mapreduce jobs with

270
00:17:26,140 --> 00:17:27,170
it is much easier to

271
00:17:28,320 --> 00:17:29,170
work with big

272
00:17:29,650 --> 00:17:30,540
there were working

273
00:17:31,450 --> 00:17:33,310
with the low-level java code for example

274
00:17:34,020 --> 00:17:37,600
so wanted to integrate our pipeline as close as possible if they

275
00:17:38,590 --> 00:17:40,210
to make it easy for users take two years

276
00:17:43,380 --> 00:17:43,690
the end

277
00:17:44,290 --> 00:17:47,100
we wanted to avoid certain processes that

278
00:17:47,100 --> 00:17:48,740
from the height say

279
00:17:49,860 --> 00:17:53,010
and that's a hundred metres

280
00:17:53,020 --> 00:17:56,380
let's calculate how long it takes for example

281
00:17:56,410 --> 00:18:00,820
two hit which for you will be trail of course

282
00:18:00,880 --> 00:18:05,310
yes the

283
00:18:05,380 --> 00:18:07,650
the mass of the earth

284
00:18:07,680 --> 00:18:13,720
it's about six times ten to the twenty four kilogrammes

285
00:18:15,550 --> 00:18:17,360
you edit distance h

286
00:18:17,380 --> 00:18:20,190
which will take a hundred metres

287
00:18:20,240 --> 00:18:21,980
it is apple and

288
00:18:22,060 --> 00:18:23,010
it's a

289
00:18:23,060 --> 00:18:26,520
has amassed a half kilogram

290
00:18:26,570 --> 00:18:28,970
there is a force

291
00:18:28,970 --> 00:18:31,250
from the earth on three apples

292
00:18:31,280 --> 00:18:33,650
this is that forces

293
00:18:33,660 --> 00:18:37,600
the magnitude of that forces and g

294
00:18:37,660 --> 00:18:38,680
and that is

295
00:18:38,690 --> 00:18:41,230
five new

296
00:18:41,240 --> 00:18:45,560
i think you can strike of little

297
00:18:47,300 --> 00:18:48,090
how long

298
00:18:48,100 --> 00:18:49,310
does it take

299
00:18:49,480 --> 00:18:52,410
this object to it

300
00:18:52,450 --> 00:18:53,380
so we know

301
00:18:53,560 --> 00:18:55,160
one half

302
00:18:55,170 --> 00:18:56,880
GT squared

303
00:18:56,900 --> 00:18:59,250
because h

304
00:18:59,310 --> 00:19:01,150
doesn't start with an initial

305
00:19:02,220 --> 00:19:04,190
that's hundreds

306
00:19:04,200 --> 00:19:07,480
these standards five for career

307
00:19:10,680 --> 00:19:13,400
about four and a half

308
00:19:13,410 --> 00:19:17,940
so after four and a half seconds it's too so far so good

309
00:19:19,030 --> 00:19:21,150
according to the

310
00:19:21,220 --> 00:19:26,010
the earth must experience exactly the same force as the apple does

311
00:19:26,030 --> 00:19:28,180
but in opposite directions

312
00:19:28,250 --> 00:19:29,780
so therefore

313
00:19:29,820 --> 00:19:31,380
the first

314
00:19:33,800 --> 00:19:35,990
the same force f

315
00:19:35,990 --> 00:19:38,960
five newton in this direction

316
00:19:39,040 --> 00:19:40,990
what is the earth going to do

317
00:19:41,020 --> 00:19:43,210
well this is going to fall

318
00:19:43,210 --> 00:19:44,910
towards the apple

319
00:19:44,960 --> 00:19:47,330
as he calls them a

320
00:19:47,390 --> 00:19:48,490
the fourth

321
00:19:48,580 --> 00:19:50,240
the earth

322
00:19:50,240 --> 00:19:52,200
the mass of the earth

323
00:19:52,350 --> 00:19:55,690
i'm acceleration over here

324
00:19:55,860 --> 00:19:58,000
cause we know five

325
00:19:58,090 --> 00:20:01,020
know the mass six times ten to twenty four

326
00:20:04,950 --> 00:20:08,440
five i six times ten to twenty four

327
00:20:08,450 --> 00:20:11,940
it is about eight times ten to the minus twenty five

328
00:20:13,460 --> 00:20:17,650
the second square

329
00:20:17,710 --> 00:20:22,310
how long will the earth well there's was willful roughly four and a half seconds

330
00:20:23,160 --> 00:20:25,330
they collide

331
00:20:25,350 --> 00:20:29,070
how far was move into four and a half seconds

332
00:20:29,080 --> 00:20:31,940
well it moves one half

333
00:20:33,600 --> 00:20:35,640
the square

334
00:20:35,680 --> 00:20:37,860
the distance that it moves

335
00:20:37,880 --> 00:20:39,640
we know a

336
00:20:39,650 --> 00:20:44,450
and we notice create which is twenty one halftime twenty years ten

337
00:20:44,460 --> 00:20:46,450
so that means this distance

338
00:20:46,470 --> 00:20:48,880
becomes that number times ten is about eight

339
00:20:48,900 --> 00:20:52,700
times ten to the minus twenty four meters

340
00:20:52,740 --> 00:20:57,850
move eight times ten to the minus twenty four meters

341
00:20:57,940 --> 00:21:00,290
that of course is impossible

342
00:21:00,340 --> 00:21:02,390
to measure

343
00:21:02,420 --> 00:21:04,180
just imagine

344
00:21:04,190 --> 00:21:09,150
what a wonderful concept this is

345
00:21:09,160 --> 00:21:11,390
when this ball

346
00:21:11,400 --> 00:21:13,360
o back to factory

347
00:21:13,590 --> 00:21:18,530
and i m i t

348
00:21:18,550 --> 00:21:21,170
falling towards the ball

349
00:21:21,200 --> 00:21:23,720
every time the ball comes down

350
00:21:23,730 --> 00:21:29,860
we're falling towards all measure but i have overview an overview

351
00:21:30,750 --> 00:21:32,900
you may want to think about this

352
00:21:33,010 --> 00:21:35,190
if i throw the ball up

353
00:21:35,230 --> 00:21:38,090
going to be away from

354
00:21:38,110 --> 00:21:41,600
i bet you anything useful also go away from all

355
00:21:41,740 --> 00:21:43,240
so as i do this

356
00:21:45,280 --> 00:21:48,500
believe me that gordon was feeling it is

357
00:21:48,570 --> 00:21:51,180
this is going down is coming first the ball

358
00:21:51,210 --> 00:21:53,780
was going down and part of the earth

359
00:21:53,800 --> 00:21:56,060
i'm shaking of that

360
00:21:56,070 --> 00:21:57,240
i simply

361
00:21:57,950 --> 00:21:59,500
was this form

362
00:21:59,590 --> 00:22:01,070
that is the consequence

363
00:22:02,060 --> 00:22:04,260
newton's third law even though

364
00:22:04,320 --> 00:22:08,720
the amount by which the earth move is of course too small

365
00:22:08,720 --> 00:22:12,500
to be measured

366
00:22:12,550 --> 00:22:15,940
i now want to work out with you

367
00:22:16,000 --> 00:22:20,180
a rather detailed example

368
00:22:20,190 --> 00:22:22,580
of something in which we combined

369
00:22:22,670 --> 00:22:25,190
what we've learned today

370
00:22:25,270 --> 00:22:27,330
down to earth first problem

371
00:22:27,420 --> 00:22:28,990
the kind of the problem

372
00:22:29,010 --> 00:22:29,990
that you

373
00:22:30,000 --> 00:22:31,630
your exam

374
00:22:31,740 --> 00:22:34,260
or site

375
00:22:36,770 --> 00:22:38,100
an object

376
00:22:38,110 --> 00:22:42,540
on two strings

377
00:22:42,560 --> 00:22:45,010
in one string

378
00:22:45,090 --> 00:22:47,010
makes an

379
00:22:47,070 --> 00:22:50,590
of sixty degrees

380
00:22:50,600 --> 00:22:51,880
with the vertical

381
00:22:51,920 --> 00:22:55,890
and you are makes an angle of forty five degrees of food

382
00:22:55,940 --> 00:22:58,460
so this is the one

383
00:22:58,510 --> 00:23:01,000
that makes an angle

384
00:23:01,100 --> 00:23:03,480
sixty degrees with right

385
00:23:03,490 --> 00:23:05,330
thirty degrees with the vertical

386
00:23:05,350 --> 00:23:06,760
and this one

387
00:23:06,810 --> 00:23:10,230
forty five degrees

388
00:23:10,260 --> 00:23:14,240
let's assume that the strings of negligible mass

389
00:23:14,290 --> 00:23:16,730
so to test here to the ceiling

390
00:23:16,810 --> 00:23:20,450
and i have a new object

391
00:23:22,990 --> 00:23:25,280
there's an object

392
00:23:26,420 --> 00:23:28,770
there will be a force

393
00:23:30,690 --> 00:23:33,530
gravitational force

394
00:23:33,560 --> 00:23:36,630
this object is hanging there's not being accelerated

395
00:23:37,920 --> 00:23:38,560
the net

396
00:23:38,580 --> 00:23:40,700
acceleration must be zero

397
00:23:40,780 --> 00:23:43,800
and so one string must be pulling in in this direction and you string must

398
00:23:43,800 --> 00:23:50,790
be pulling in this direction so that the net force on the system is zero

399
00:23:50,800 --> 00:23:52,690
let's call this pool

400
00:23:52,740 --> 00:23:54,380
for now t one

401
00:23:54,390 --> 00:23:56,730
recall that the tension in the string

402
00:23:56,740 --> 00:23:59,470
and we call the attention industry

403
00:24:03,660 --> 00:24:05,780
the question now is the largest one

404
00:24:05,800 --> 00:24:07,040
and how large is

405
00:24:07,080 --> 00:24:09,290
t two

406
00:24:09,340 --> 00:24:12,020
various ways you can do this

407
00:24:12,070 --> 00:24:15,840
one way that's always works pretty safe

408
00:24:15,890 --> 00:24:17,020
you call this

409
00:24:17,040 --> 00:24:18,660
x direction

410
00:24:18,700 --> 00:24:23,640
you may choose which direction you call i call this plus

411
00:24:23,690 --> 00:24:25,100
i call these negative

412
00:24:25,190 --> 00:24:26,550
and you could call

413
00:24:27,270 --> 00:24:30,360
this why direction you make all this plus

414
00:24:30,400 --> 00:24:32,860
and is negative

415
00:24:32,950 --> 00:24:36,450
i know from newton's

416
00:24:36,500 --> 00:24:37,760
the second law

417
00:24:37,780 --> 00:24:40,230
f equals m a

418
00:24:41,890 --> 00:24:45,050
that there is no acceleration so this must be

419
00:24:46,130 --> 00:24:51,920
so to sum of all forces on that mass must be zero

420
00:24:51,930 --> 00:24:56,720
the three forces must each are up so to speak

421
00:24:56,730 --> 00:25:00,430
well if that's the case then the sum of all forces in the x direction

422
00:25:00,430 --> 00:25:03,830
must also be zero because there is no acceleration in x rection

423
00:25:03,850 --> 00:25:07,740
and the sum of all forces in the y direction must be

424
00:25:07,880 --> 00:25:10,920
so i'm going to decompose and something we've done

425
00:25:10,950 --> 00:25:13,880
before going to decompose the forces

426
00:25:13,900 --> 00:25:14,980
in two

427
00:25:15,000 --> 00:25:19,500
x and into a y direction

428
00:25:24,610 --> 00:25:26,860
the x component of t one

429
00:25:28,050 --> 00:25:29,420
its magnitude

430
00:25:29,440 --> 00:25:30,900
sixty one

431
00:25:30,940 --> 00:25:32,690
times the cosine

432
00:25:32,710 --> 00:25:37,630
of sixty degrees

433
00:25:43,960 --> 00:25:46,050
i want to know what this

434
00:25:46,050 --> 00:25:50,780
one is

435
00:25:50,840 --> 00:25:52,510
this one

436
00:25:52,530 --> 00:25:54,500
if you want

437
00:25:55,570 --> 00:26:01,480
of sixty degrees

438
00:26:01,530 --> 00:26:05,900
this projection

439
00:26:05,900 --> 00:26:08,700
first of all what they say is if you have

440
00:26:08,710 --> 00:26:10,330
a structure like this

441
00:26:10,350 --> 00:26:11,640
anywhere even in

442
00:26:27,730 --> 00:26:31,960
OK so this is the example so you can do this

443
00:26:31,980 --> 00:26:36,080
rule here left push tells us that this

444
00:26:37,250 --> 00:26:39,250
so it says that we can get this

445
00:26:39,270 --> 00:26:42,370
from this we those as well

446
00:26:42,410 --> 00:26:47,290
so the proof goes down in the end the way you construct the proof

447
00:26:47,350 --> 00:26:51,230
you start at the bottom and say how can i get this new to you

448
00:26:51,230 --> 00:26:54,870
build up and by the length of the the error

449
00:26:55,660 --> 00:26:58,680
left and right to left

450
00:26:58,680 --> 00:27:01,660
i know that you sometimes use terms

451
00:27:04,620 --> 00:27:08,120
right left and right arrows are what we are raised

452
00:27:20,390 --> 00:27:24,790
and i was looking down

453
00:27:28,430 --> 00:27:31,980
OK now

454
00:27:31,980 --> 00:27:34,350
roman to the

455
00:27:34,500 --> 00:27:36,020
reject k

456
00:27:36,080 --> 00:27:39,370
because k tells us that if we

457
00:27:41,310 --> 00:27:46,580
structure so we had just get

458
00:27:46,600 --> 00:27:48,120
from that we can get

459
00:27:48,140 --> 00:27:51,870
and what we want to build up to

460
00:27:54,310 --> 00:28:22,940
OK i know i posted to do working up but this is show you it

461
00:28:22,940 --> 00:28:27,890
shows us that if we have any here we need right here

462
00:28:27,910 --> 00:28:28,960
to get from

463
00:28:28,960 --> 00:28:31,270
i think it is different from that

464
00:28:31,310 --> 00:28:35,060
we can get this from there

465
00:28:35,520 --> 00:28:37,600
and end up with

466
00:28:37,600 --> 00:28:40,940
our the paradox

467
00:28:40,960 --> 00:28:46,230
it tells you bring your premise

468
00:28:46,290 --> 00:28:48,580
the real relevant and not

469
00:28:50,430 --> 00:28:53,160
and it can

470
00:28:53,500 --> 00:28:55,930
and implication

471
00:28:55,960 --> 00:29:00,370
right so k is something is ruled a relevant logic but intuitions

472
00:29:00,370 --> 00:29:06,000
as much effect on the big list everything that there are axes except to emissions

473
00:29:06,020 --> 00:29:09,390
so intuitionist logic

474
00:29:09,410 --> 00:29:14,810
or without again because we're not talking negation is the sort of maximal system we

475
00:29:14,810 --> 00:29:15,620
can look at

476
00:29:15,680 --> 00:29:21,460
not quite the logic because once again purses law we talked about is not derivable

477
00:29:28,640 --> 00:29:36,000
what is not derivable but

478
00:29:36,680 --> 00:29:38,210
so what

479
00:29:38,250 --> 00:29:39,520
how do we get

480
00:29:39,710 --> 00:29:44,640
relevance logic well we get relevant logic or something pretty close

481
00:29:49,810 --> 00:29:56,460
with this

482
00:29:56,620 --> 00:30:02,180
with control strong contraction

483
00:30:02,230 --> 00:30:04,710
it's too sensitive roles

484
00:30:06,410 --> 00:30:10,960
start coming to

485
00:30:12,060 --> 00:30:15,410
but it doesn't give us all rolled what's missing

486
00:30:15,430 --> 00:30:18,960
but once again this annoying thing distribution

487
00:30:19,110 --> 00:30:21,870
distribution is something we can prove

488
00:30:21,930 --> 00:30:23,390
with this

489
00:30:23,440 --> 00:30:26,890
and that's fairly not just talk about the system

490
00:30:26,930 --> 00:30:31,390
with the distribution and the proof i've given use for and once

491
00:30:33,460 --> 00:30:36,140
this system

492
00:30:37,730 --> 00:30:38,520
but without

493
00:30:38,870 --> 00:30:44,680
and the red star system with the distribution of and with negation

494
00:30:47,790 --> 00:30:53,310
distribution is kind of pain right i mean we've seen already the pain in the

495
00:30:53,310 --> 00:30:57,620
system and i showed you ross brady is way dealing with that with getting his

496
00:30:57,620 --> 00:31:01,200
streams approach is quite an elegant way of of dealing with

497
00:31:02,160 --> 00:31:04,620
back in the nineteen seventies

498
00:31:04,890 --> 00:31:07,540
in developing sequent calculus

499
00:31:09,060 --> 00:31:10,700
relevant law

500
00:31:10,710 --> 00:31:15,730
mike dunne and a russian mathematician named romance

501
00:31:15,750 --> 00:31:19,680
developed by the

502
00:31:20,730 --> 00:31:32,020
developed very similar

503
00:31:32,830 --> 00:31:35,850
so the one rascist and more recently

504
00:31:36,040 --> 00:31:40,270
but it's different interests

505
00:31:40,290 --> 00:31:46,140
what they did was they distinguish between the same and well

506
00:31:46,200 --> 00:31:48,350
connecting premises

507
00:31:48,350 --> 00:31:50,850
and karma

508
00:31:50,910 --> 00:31:55,440
and the comma is more like the conjunction you know not

509
00:31:55,460 --> 00:32:00,350
so they think of conjunctive premises and sort of

510
00:32:00,350 --> 00:32:01,870
rather it is

511
00:32:05,140 --> 00:32:09,500
or more intense and more intentional form of finding premises

512
00:32:09,540 --> 00:32:13,500
which we call fusion but i'm not going to fusion for that

513
00:32:13,520 --> 00:32:16,520
now called the common

514
00:32:26,730 --> 00:32:30,250
louise for extensional

515
00:32:30,270 --> 00:32:32,790
once again

516
00:32:49,430 --> 00:32:52,230
so that looks like just like the

517
00:32:52,250 --> 00:32:55,060
with a comoros replacing

518
00:32:55,060 --> 00:32:59,790
a semi called the same thing with

519
00:32:59,810 --> 00:33:15,390
now this is a weak form of commutativity one you used to

520
00:33:16,180 --> 00:33:18,230
living things around

521
00:33:22,000 --> 00:33:34,160
this is a weak form contraction

522
00:33:35,430 --> 00:33:37,100
most play

523
00:33:37,200 --> 00:33:39,350
as of the form k

524
00:33:41,640 --> 00:33:46,750
you can

525
00:33:46,770 --> 00:33:48,430
so does that for us

526
00:33:48,430 --> 00:33:51,730
well if we can so this tells us

527
00:33:51,750 --> 00:33:59,100
if we had actually like this embedded anywhere in the bigger structure or by itself

528
00:33:59,140 --> 00:34:01,060
we could

529
00:34:01,120 --> 00:34:04,580
replace it with just the same and so on

530
00:34:04,600 --> 00:34:08,910
OK that allows us to prove

531
00:34:11,140 --> 00:34:14,200
the contractual allows us to prove distribution

532
00:34:14,230 --> 00:34:16,410
and the proof is become long

533
00:34:16,430 --> 00:34:19,410
and i thought that are in you know it's

534
00:34:19,440 --> 00:34:22,100
now the interesting right

535
00:34:22,100 --> 00:34:24,730
it's not by accident the ross

536
00:34:24,730 --> 00:34:29,940
we have a complicated problems with belief probably session is terrible trying to find a

537
00:34:31,360 --> 00:34:35,610
and that's been well known for a long time

538
00:34:37,970 --> 00:34:39,280
they all scale

539
00:34:39,310 --> 00:34:40,790
pretty badly when

540
00:34:40,810 --> 00:34:44,930
when you really get lots of perturbation and of course that makes sense in the

541
00:34:44,930 --> 00:34:49,580
sense that what is the shape the shape when when when when i don't want

542
00:34:49,580 --> 00:34:53,830
to mess things well when the very different so i guess you would expect a

543
00:34:53,830 --> 00:35:00,510
performance to always be perfect when you have significant perturbations anyway

544
00:35:00,520 --> 00:35:05,410
so what we do is going just remind you we measure the percentage correct assignment

545
00:35:05,750 --> 00:35:07,920
as a function of the perturbation

546
00:35:07,930 --> 00:35:14,330
of the positions or orientations or whatever it into in this in the same

547
00:35:14,590 --> 00:35:15,980
is a practical one

548
00:35:15,990 --> 00:35:18,850
well actually to what we do a lot of work in that division

549
00:35:18,970 --> 00:35:22,790
i'm not if use some people to think about

550
00:35:22,800 --> 00:35:27,110
the mapping industry the cartography industry is a multi-billion dollar industry

551
00:35:27,160 --> 00:35:30,550
most countries have given up

552
00:35:30,640 --> 00:35:33,020
being able to update maps

553
00:35:33,040 --> 00:35:34,260
you can afford it

554
00:35:35,050 --> 00:35:37,240
was eighty percent of the cost in any

555
00:35:37,240 --> 00:35:40,740
making and that is not in the satellites it's not in the

556
00:35:40,750 --> 00:35:43,780
gps sinus camera sitting in place

557
00:35:43,800 --> 00:35:45,170
it's in human labor

558
00:35:48,000 --> 00:35:52,240
they need drastically really advanced techniques to help

559
00:35:52,300 --> 00:35:55,710
my computer writing techniques that will help cartographers

560
00:35:55,780 --> 00:36:00,370
do not happen quickly and a big problem mapping is corresponds to have already typically

561
00:36:01,480 --> 00:36:02,800
now training about data

562
00:36:02,820 --> 00:36:07,850
so you have to align what's there with with new data and then and on

563
00:36:08,690 --> 00:36:12,760
so it's a very good example we have one approach is going to play just

564
00:36:12,760 --> 00:36:18,320
go and this is the typical situation where trying to update roads

565
00:36:18,330 --> 00:36:21,770
i want to be do here is partition rajinder segments

566
00:36:21,780 --> 00:36:24,350
and then i'm trying to match those of and this is really the sort of

567
00:36:24,350 --> 00:36:30,360
performance to say that relaxation methods loopy belief or to strike legislation enabling from pretty

568
00:36:31,210 --> 00:36:34,190
single pass goes really well and i you expect

569
00:36:34,430 --> 00:36:40,610
i'm adding more neighborhood information more information about the clique potentials

570
00:36:40,630 --> 00:36:43,090
arm or what more

571
00:36:43,220 --> 00:36:48,710
more richer clicks and the potentials and performance goes up in this case ninety percent

572
00:36:48,710 --> 00:36:52,500
and this is quite a difficult task although there is no benchmarking on this because

573
00:36:52,500 --> 00:36:55,820
this the practical the practical problems

574
00:36:55,830 --> 00:36:57,180
it comes up

575
00:36:57,190 --> 00:37:00,790
but show you how powerful is exact

576
00:37:00,970 --> 00:37:03,850
so i don't want to develop the money

577
00:37:03,870 --> 00:37:06,350
go to more on this stuff

578
00:37:06,360 --> 00:37:11,010
what's the stuff is when you're trying to think of structural pattern recognition as one

579
00:37:11,010 --> 00:37:15,160
of matching discrete graphs

580
00:37:15,210 --> 00:37:17,130
with larger graphs

581
00:37:17,190 --> 00:37:18,470
attributes are not

582
00:37:18,520 --> 00:37:23,500
i think we've been we had two hours yesterday and now this morning and i

583
00:37:23,500 --> 00:37:25,140
have just convinced you

584
00:37:25,930 --> 00:37:31,030
that in fact that there are committed is going to be doing the same type

585
00:37:31,030 --> 00:37:33,270
of logic applies

586
00:37:34,250 --> 00:37:35,990
you know

587
00:37:36,890 --> 00:37:38,500
of all the spectral

588
00:37:38,500 --> 00:37:40,010
least squares

589
00:37:40,020 --> 00:37:42,380
and bayesian formulations

590
00:37:42,440 --> 00:37:47,100
that at the moment the junction tree approach seems to be the best approach

591
00:37:47,160 --> 00:37:52,620
not only because this provably optimal because because because it seems to be robust even

592
00:37:52,620 --> 00:37:57,180
when they get quite significant perturbations and in particular when we're trying to find small

593
00:37:57,180 --> 00:37:59,500
things in large things

594
00:37:59,520 --> 00:38:03,270
this is often the problem in computer vision

595
00:38:03,290 --> 00:38:06,580
and we you can't afford to say move the window

596
00:38:06,610 --> 00:38:08,780
the piece wise do this

597
00:38:08,790 --> 00:38:14,550
so we got a polynomial but there's also nothing you can say that this stuff

598
00:38:14,550 --> 00:38:17,010
is quite quite useful so what else

599
00:38:17,020 --> 00:38:18,610
so we go to

600
00:38:18,620 --> 00:38:20,100
the governments now

601
00:38:20,210 --> 00:38:24,670
well not really because this is in some sense of still being giving you a

602
00:38:24,670 --> 00:38:27,050
pretty simple problems

603
00:38:27,230 --> 00:38:33,690
i have also known explore here how we can use the same type of thinking

604
00:38:33,700 --> 00:38:39,490
in the context of the basic supplies for many different tasks in understanding spatial modeling

605
00:38:39,700 --> 00:38:41,290
not just matching

606
00:38:41,340 --> 00:38:44,620
so now i'm going to generalize what we spoke about

607
00:38:44,630 --> 00:38:47,290
two really some really case study

608
00:38:47,340 --> 00:38:48,700
one do

609
00:38:48,740 --> 00:38:51,830
this sort of comment on this case study

610
00:38:52,420 --> 00:38:53,810
the show you how

611
00:38:54,660 --> 00:39:00,700
a lot of modern computer vision people image understanding people GIS people spatial data people

612
00:39:00,750 --> 00:39:02,890
and they are thinking the bayesian way

613
00:39:02,910 --> 00:39:04,440
about inference

614
00:39:04,450 --> 00:39:05,940
right from

615
00:39:05,950 --> 00:39:07,180
image encoding

616
00:39:07,190 --> 00:39:11,600
o two matching sings to three d models

617
00:39:12,710 --> 00:39:14,700
and even computer graphics

618
00:39:14,710 --> 00:39:18,470
we'll have a little bit of a sub division into stochastical systems

619
00:39:18,520 --> 00:39:23,060
so that's my task now just to give you a sort of a final

620
00:39:23,090 --> 00:39:25,700
situatedness time situation

621
00:39:25,700 --> 00:39:27,990
you in this new literature

622
00:39:28,010 --> 00:39:31,620
but i think it's got tremendous

623
00:39:33,360 --> 00:39:37,810
so now go back to canada where it came from

624
00:39:37,860 --> 00:39:41,540
seven months ago in western canada

625
00:39:43,600 --> 00:39:48,030
we've been involved for the last four years in the project with the

626
00:39:48,040 --> 00:39:54,930
federal provincial government and the company i'm trying to improve forestry inventory

627
00:39:55,820 --> 00:39:59,510
well canada has one third of the world or forest

628
00:39:59,510 --> 00:40:03,760
but presumably you want to say anything about the color of

629
00:40:03,770 --> 00:40:06,550
you know this block one pick it up

630
00:40:06,620 --> 00:40:09,600
so if we don't say anything about the colour that means that we're i trying

631
00:40:09,630 --> 00:40:12,610
to figure out what the color the block is

632
00:40:12,630 --> 00:40:16,800
after doing a sequence of actions i won't be able to conclude whether it's white

633
00:40:16,800 --> 00:40:17,930
or not white

634
00:40:17,940 --> 00:40:21,040
can conclude anything about colourable

635
00:40:21,040 --> 00:40:25,040
in other words i really need to write some information then about how the colour

636
00:40:25,050 --> 00:40:28,900
the block is affected by each of these actions

637
00:40:29,190 --> 00:40:33,500
so the frame problem really boils down to

638
00:40:33,540 --> 00:40:37,310
describing those parts of the state

639
00:40:37,360 --> 00:40:41,830
that changed by any enemy reaction

640
00:40:41,870 --> 00:40:43,100
OK now

641
00:40:43,110 --> 00:40:51,310
does that sound counterintuitive

642
00:40:57,890 --> 00:41:01,120
that's absolutely right that's the exact and that's what we want

643
00:41:01,160 --> 00:41:04,350
but what of the planet trying to make is that if we don't write that

644
00:41:07,020 --> 00:41:08,500
in first order logic

645
00:41:08,550 --> 00:41:10,910
then we can draw that conclusion

646
00:41:10,930 --> 00:41:12,130
so the

647
00:41:12,140 --> 00:41:14,340
as someone trying to point out is that

648
00:41:14,390 --> 00:41:17,510
in a sense the problem with using first order logic of you don't want to

649
00:41:17,510 --> 00:41:20,650
view it is the problem it's something that you're going to have to take care

650
00:41:20,650 --> 00:41:22,800
of if you use first order logic

651
00:41:22,800 --> 00:41:24,820
so what you said makes perfect sense

652
00:41:24,840 --> 00:41:26,310
if i have an action

653
00:41:26,320 --> 00:41:28,900
and the action has absolutely no

654
00:41:29,420 --> 00:41:32,940
you know nothing to do with the columns so pick up has presumably them into

655
00:41:32,940 --> 00:41:34,590
the column

656
00:41:34,610 --> 00:41:37,480
then i would say i should have to say anything about i should just naturally

657
00:41:37,480 --> 00:41:40,360
assume that nothing stays the same help

658
00:41:40,370 --> 00:41:43,790
and that's exactly the problem that if we don't write something down about that in

659
00:41:43,790 --> 00:41:47,760
logic i cannot draw that conclusion because there's no way

660
00:41:48,850 --> 00:41:54,150
well is this why after i do these actions all have actually describe that

661
00:41:54,170 --> 00:41:59,150
how the whiteness of this object is affected by these actions are not affected so

662
00:41:59,150 --> 00:42:02,670
i can say anything about what is

663
00:42:02,730 --> 00:42:05,740
and that's the whole point of this lecture but you know this problem is essentially

664
00:42:05,740 --> 00:42:08,210
the thing the main thing to get out of this lecture is one thing is

665
00:42:08,240 --> 00:42:12,200
going to have to describe and sort of counterintuitive has said the reason i ask

666
00:42:12,200 --> 00:42:15,770
that question at the beginning is because i really do think that if you look

667
00:42:15,770 --> 00:42:18,740
at this set of things

668
00:42:20,030 --> 00:42:24,350
you know then i think it's really quite reasonable to assume that that should be

669
00:42:24,350 --> 00:42:28,240
sufficient to describe this problem

670
00:42:28,290 --> 00:42:33,490
and the thing is that that's not sufficient just writing stuff down the describes these

671
00:42:34,380 --> 00:42:36,950
is not sufficient for logic to be able to

672
00:42:36,980 --> 00:42:38,860
reason about

673
00:42:38,920 --> 00:42:41,890
OK now i'm going to fix this problem so how we can have think might

674
00:42:41,900 --> 00:42:45,270
fix this problem

675
00:42:45,310 --> 00:42:46,320
i think there are two

676
00:42:46,340 --> 00:42:48,210
ways of doing

677
00:42:48,230 --> 00:42:50,580
and i want to show you can get

678
00:42:50,590 --> 00:42:51,590
let's give it again

679
00:42:51,600 --> 00:42:58,950
anything he might fix this problem

680
00:43:02,950 --> 00:43:04,900
one way is on the board

681
00:43:04,950 --> 00:43:07,020
which is we write more stuff

682
00:43:07,230 --> 00:43:09,050
the right order

683
00:43:09,070 --> 00:43:12,400
and the other way to do it would be to change the logic that we're

684
00:43:12,400 --> 00:43:16,130
using sonar not perhaps use first order logic right

685
00:43:17,870 --> 00:43:22,260
if you remember back to what i said about nonmonotonic logics in the previous lecture

686
00:43:22,280 --> 00:43:26,650
so that trying to capture this notion of common sense reasoning and in particular

687
00:43:26,660 --> 00:43:32,030
they try and sort of characterizes topper this is this should normally be the case

688
00:43:32,060 --> 00:43:36,100
so he muses normally fly for sorry it birds normally flies i don't really know

689
00:43:36,100 --> 00:43:37,240
anything more

690
00:43:37,250 --> 00:43:40,900
then i'm going to assume they fly if i'd like something like the

691
00:43:40,950 --> 00:43:44,340
but in a new then i'd like to withdraw the conclusion and if i learn

692
00:43:44,350 --> 00:43:48,800
something for the like for instance extract jetpack to their back then i might even

693
00:43:48,800 --> 00:43:51,390
change the conclusion again

694
00:43:51,450 --> 00:43:54,570
so one possibility is to replace first-order logic with

695
00:43:54,610 --> 00:44:01,390
some four nonmonotonic logic which makes exactly this assumption that you said and it's often

696
00:44:01,390 --> 00:44:04,370
referred to as the common sense principle of inertia

697
00:44:04,420 --> 00:44:06,890
in the common sense principle inertia says that

698
00:44:06,940 --> 00:44:09,400
in the absence of any other information

699
00:44:10,880 --> 00:44:14,190
something had that's a particular color in one state

700
00:44:14,210 --> 00:44:15,990
if there is no reason for me to

701
00:44:16,010 --> 00:44:19,520
i suggest that changed i'm going to assume that stays the same

702
00:44:19,590 --> 00:44:23,570
so if the pick action doesn't say anything about the colour of the object

703
00:44:23,620 --> 00:44:26,540
i could try and use the nonmonotonic logic to say that the

704
00:44:26,540 --> 00:44:29,860
color is then carried over to the next day next next as long as the

705
00:44:29,860 --> 00:44:32,010
actions that are applied and not

706
00:44:32,060 --> 00:44:35,840
actions that can change the colour like a painting action or something like that

707
00:44:35,850 --> 00:44:39,880
OK so it's sort of two ways of doing things right more down

708
00:44:40,810 --> 00:44:42,560
change the logic

709
00:44:43,280 --> 00:44:45,840
now what i'm going to write more down but we're going to look at the

710
00:44:45,840 --> 00:44:47,860
way of

711
00:44:49,360 --> 00:44:52,520
coming up with the stuff i need to write down

712
00:44:52,520 --> 00:44:54,250
by making the this

713
00:44:54,300 --> 00:44:56,860
common sense principle of inertia assumption

714
00:44:56,960 --> 00:45:01,100
that if the effects don't say anything about a particular property so if the effects

715
00:45:01,100 --> 00:45:03,710
of an action that say anything about the color of the cap

716
00:45:03,800 --> 00:45:05,620
then i'm going to see the colour the cap

717
00:45:05,650 --> 00:45:08,420
remains the same in the next day

718
00:45:08,530 --> 00:45:11,500
OK now what we need these frame axioms and is that we're going to try

719
00:45:11,500 --> 00:45:15,760
to automatically come up with frame axioms but frame maxim just something that says

720
00:45:15,760 --> 00:45:19,770
a statement about things that stay the same so the first one he just says

721
00:45:19,830 --> 00:45:21,050
the colour effect

722
00:45:22,090 --> 00:45:26,110
what is the colour ecstasy in a particular situation if i did the point action

723
00:45:26,200 --> 00:45:32,790
i put the object somewhere in the colour remains c

724
00:45:38,000 --> 00:45:41,550
one problem is that of course you know you may have

725
00:45:41,600 --> 00:45:47,150
many actions and each action like any effective fuel the flaunts

726
00:45:47,200 --> 00:45:49,820
OK so the one problem is that we're going to need a lot of these

727
00:45:49,820 --> 00:45:56,590
frame axioms because there's a lot of stuff that doesn't get changed by by action

728
00:45:56,590 --> 00:46:01,060
OK let's just take just to slight digression

729
00:46:01,400 --> 00:46:05,420
the frame problem is anyone of the reasoning problems it's introduced when you first order

730
00:46:06,320 --> 00:46:10,770
there are two other ones one's called the classification problem and the other one is

731
00:46:10,770 --> 00:46:12,680
called the ramification problem

732
00:46:12,730 --> 00:46:16,340
so the qualification problem which is the the lower half of the slide is the

733
00:46:17,820 --> 00:46:21,040
what we said that we can pick up an object if

734
00:46:21,040 --> 00:46:25,050
changing the frequency of the green one and all

735
00:46:25,190 --> 00:46:28,090
and then of course the green one goal

736
00:46:28,100 --> 00:46:33,190
so you see also here apart from the fact that it's nice to

737
00:46:33,200 --> 00:46:36,200
he now the idea that there point standing still

738
00:46:36,200 --> 00:46:38,290
they have name we call the nodes

739
00:46:39,200 --> 00:46:42,260
and there is system is also leading now

740
00:46:42,280 --> 00:46:43,060
in the third

741
00:46:44,430 --> 00:46:46,280
the natural frequency

742
00:46:46,340 --> 00:46:48,370
we only excited in one

743
00:46:56,320 --> 00:46:58,990
i'm not sure i got all the lights on the

744
00:47:05,490 --> 00:47:09,410
when we have a violin string

745
00:47:09,420 --> 00:47:11,630
or another string on cello

746
00:47:11,640 --> 00:47:13,180
eight whatever

747
00:47:13,230 --> 00:47:17,110
there is no such thing like someone who is vibrating at the end so you

748
00:47:17,110 --> 00:47:19,460
have to make it vibrate

749
00:47:19,460 --> 00:47:21,470
and you do that by having the ball

750
00:47:21,480 --> 00:47:24,190
so effectively robs history

751
00:47:24,190 --> 00:47:26,090
and when you run it

752
00:47:26,100 --> 00:47:26,890
that is

753
00:47:26,900 --> 00:47:30,150
sort of the same as saying to this string

754
00:47:30,200 --> 00:47:32,400
do whatever you like

755
00:47:32,400 --> 00:47:34,340
and what the string like this

756
00:47:35,630 --> 00:47:36,540
and that

757
00:47:36,560 --> 00:47:40,790
and that doesn't like anything in between so what strange is now going to do

758
00:47:40,790 --> 00:47:42,350
if you just it

759
00:47:42,350 --> 00:47:43,670
or even if you

760
00:47:43,700 --> 00:47:46,980
like it or even if you would hit with a hammer which is what you

761
00:47:46,990 --> 00:47:50,250
do with the piano hammer comes down on the string that's all that happened was

762
00:47:50,250 --> 00:47:52,010
the piano it then

763
00:47:52,030 --> 00:47:56,280
it starts to oscillate simultaneously

764
00:47:56,450 --> 00:48:00,380
the combination of these harmonics in general

765
00:48:00,380 --> 00:48:02,350
the lowest

766
00:48:02,360 --> 00:48:04,850
we can see is the strongest

767
00:48:04,870 --> 00:48:08,130
the first time on in general but not always

768
00:48:08,230 --> 00:48:13,220
that depends on where you would block it

769
00:48:13,250 --> 00:48:14,680
now comes the question

770
00:48:14,680 --> 00:48:16,650
how do you tool in

771
00:48:16,660 --> 00:48:19,410
people have a violin maker one play

772
00:48:19,420 --> 00:48:22,430
you may have noticed that the it

773
00:48:22,510 --> 00:48:26,200
well there's nothing they can do about the weight the food of the strings four

774
00:48:26,200 --> 00:48:28,960
strings of the violin that's against

775
00:48:29,100 --> 00:48:31,730
so that they cannot change that

776
00:48:31,740 --> 00:48:33,820
the length is fixed

777
00:48:33,840 --> 00:48:36,980
a violin the length of the city so the only thing they can change his

778
00:48:37,960 --> 00:48:41,050
and that's exactly what they do you may have noticed that and when we see

779
00:48:41,050 --> 00:48:44,980
later the demonstration of the violin will ask the person to to

780
00:48:45,000 --> 00:48:46,980
they did they turn these knobs

781
00:48:47,020 --> 00:48:49,290
change the tension in the listen

782
00:48:49,300 --> 00:48:53,850
he he and they

783
00:48:53,860 --> 00:48:55,950
and they don't all four strings

784
00:48:56,000 --> 00:48:58,040
so you have to learn a string instrument

785
00:48:58,090 --> 00:48:59,970
by changing the tension

786
00:49:00,140 --> 00:49:02,250
was the and it would be hopeless

787
00:49:02,260 --> 00:49:10,290
so there you hope that it is properly to the piano is eighty eight keys

788
00:49:10,300 --> 00:49:12,860
how do you play an instrument

789
00:49:13,750 --> 00:49:18,020
was violin wanted properly tuned

790
00:49:18,070 --> 00:49:21,240
the only thing you can do now if you want to make every tone that

791
00:49:21,240 --> 00:49:22,660
you would like to

792
00:49:22,660 --> 00:49:25,690
you can only change the length and you change the links by putting your finger

793
00:49:25,700 --> 00:49:26,630
on the string

794
00:49:26,730 --> 00:49:31,470
and making a string shorter and longer if you take this train

795
00:49:31,480 --> 00:49:34,960
and you put your finger on here who supported here

796
00:49:34,970 --> 00:49:36,840
then the fundamental

797
00:49:36,840 --> 00:49:40,820
has a lower link and the lowlands remember

798
00:49:40,840 --> 00:49:44,980
higher frequency so you get a higher tone so as the violin player moves the

799
00:49:44,980 --> 00:49:48,290
finger in one and the same string will produce

800
00:49:48,350 --> 00:49:51,520
high tone and that's the way you play it it's an amazing thing when you

801
00:49:51,520 --> 00:49:54,230
come to think of it all the time you got to move the fingers in

802
00:49:54,230 --> 00:49:57,390
and out to always get that says that the right length

803
00:49:57,400 --> 00:50:01,920
and if you just off by a little bit it sounds awful

804
00:50:01,930 --> 00:50:05,220
you don't have the problem with the piano

805
00:50:05,240 --> 00:50:06,970
you go being and

806
00:50:06,990 --> 00:50:09,870
right just the right tone

807
00:50:09,870 --> 00:50:13,860
again being sold as a whole different ball game playing piano not saying the piano

808
00:50:13,860 --> 00:50:17,700
playing easier because you don't know which is the key to push but in any

809
00:50:17,700 --> 00:50:20,760
case you don't have to adjust all the time the length

810
00:50:20,780 --> 00:50:24,290
of the of the strings

811
00:50:24,320 --> 00:50:25,460
in principle

812
00:50:25,460 --> 00:50:26,540
you could

813
00:50:26,560 --> 00:50:27,720
design and

814
00:50:27,770 --> 00:50:31,220
string instrument whereby you do change the tension

815
00:50:31,230 --> 00:50:34,020
and there is such an instrument with one string

816
00:50:34,130 --> 00:50:36,150
the person place it

817
00:50:36,150 --> 00:50:37,730
with the ball

818
00:50:37,800 --> 00:50:40,130
and as the person plays it

819
00:50:40,160 --> 00:50:43,600
he or she changes the tension all the time

820
00:50:43,610 --> 00:50:46,980
and when my daughter was three years old which is a long time ago

821
00:50:47,010 --> 00:50:49,400
if you build such an instrument

822
00:50:49,400 --> 00:50:50,460
this is one

823
00:50:50,520 --> 00:50:52,060
this is the box

824
00:50:52,110 --> 00:50:54,030
and this is one three

825
00:50:54,060 --> 00:50:56,890
and when you put this on here

826
00:50:56,900 --> 00:51:00,970
i will not strike admissible we go through that only three hands now and i

827
00:51:00,970 --> 00:51:01,900
only have two

828
00:51:01,910 --> 00:51:03,760
but i will

829
00:51:03,780 --> 00:51:05,390
and then you will hear

830
00:51:05,930 --> 00:51:07,220
i can markets

831
00:51:07,280 --> 00:51:11,070
and change the tension by moving this to the right i increase the tension on

832
00:51:11,070 --> 00:51:15,460
the strings you hear high tone then when the tension is low

833
00:51:15,470 --> 00:51:17,010
link is not change

834
00:51:17,010 --> 00:51:20,900
the weight per foot does not change without attention is changing

835
00:51:20,920 --> 00:51:23,820
listen carefully

836
00:51:23,880 --> 00:51:40,340
so in principle you could design an instrument and they do exist the value chain

837
00:51:40,490 --> 00:51:41,940
tension all the time

838
00:51:41,940 --> 00:51:43,900
and that of course

839
00:51:44,960 --> 00:51:46,200
a lot of practising

840
00:51:46,220 --> 00:51:48,260
so instead of changing the link

841
00:51:48,300 --> 00:51:53,360
the chains attention

842
00:51:53,380 --> 00:51:54,220
all right

843
00:51:54,240 --> 00:51:55,440
so far

844
00:51:55,470 --> 00:51:56,820
so good

845
00:51:56,880 --> 00:51:59,820
on our string instruments

846
00:52:00,610 --> 00:52:02,760
it's going to be more difficult

847
00:52:02,800 --> 00:52:05,200
before we demonstrate the string instruments

848
00:52:05,260 --> 00:52:06,960
i want to discuss with you

849
00:52:06,970 --> 00:52:11,420
wind instruments

850
00:52:11,420 --> 00:52:16,460
these probability distribution is very general is actually the generalized

851
00:52:16,480 --> 00:52:20,560
it generalizes cost by no multinomial possible gamma ray

852
00:52:20,640 --> 00:52:24,460
we show and several other types of distributions

853
00:52:24,470 --> 00:52:30,320
so it's convenient to work with them in the very very convenient from the optimisation

854
00:52:30,320 --> 00:52:32,630
point because it looked like

855
00:52:32,630 --> 00:52:34,070
of those

856
00:52:35,130 --> 00:52:38,700
the function of probability distribution and the exponential families

857
00:52:38,710 --> 00:52:42,170
he's concave of the negative log likelihood is convex

858
00:52:42,170 --> 00:52:45,390
so you can actually solve that optimisation problem

859
00:52:45,440 --> 00:52:49,960
efficient because it can be a convex optimization problem

860
00:52:50,020 --> 00:52:51,690
which is the case here

861
00:52:51,690 --> 00:52:56,470
it's convex function so that the challenge is to help prove the design other size

862
00:52:56,550 --> 00:52:58,800
you can try to prove that these

863
00:52:58,850 --> 00:52:59,810
the the

864
00:52:59,860 --> 00:53:00,850
prostitution the

865
00:53:00,860 --> 00:53:02,440
partition function is

866
00:53:02,460 --> 00:53:06,480
is a convex function this log partition function g of it is the name of

867
00:53:06,490 --> 00:53:08,360
this object

868
00:53:08,410 --> 00:53:12,220
and the like

869
00:53:12,280 --> 00:53:14,550
the negative log like is also

870
00:53:20,750 --> 00:53:25,800
and the the same game to the optimisation OK take the gradient set to zero

871
00:53:25,850 --> 00:53:28,060
and this is what you think

872
00:53:28,190 --> 00:53:31,110
so let me tell you what is is

873
00:53:31,120 --> 00:53:32,970
this is essentially telling you that

874
00:53:32,990 --> 00:53:37,490
the gradient of the log partition function

875
00:53:37,530 --> 00:53:41,430
for the maximum likelihood estimate is equal to the

876
00:53:41,470 --> 00:53:44,510
expectation of the sufficient statistics

877
00:53:44,520 --> 00:53:46,750
with respect to the distribution

878
00:53:46,800 --> 00:53:49,150
you can be sure that

879
00:53:50,950 --> 00:53:52,940
what's the

880
00:53:52,950 --> 00:53:56,540
the big question here because if you wanted to optimisation you need to compute the

881
00:53:56,540 --> 00:53:58,340
gradient of function

882
00:53:58,350 --> 00:54:01,450
full the gradient do some type of gradient descent

883
00:54:01,470 --> 00:54:07,720
convex function you're going to compute gradient to reach the minimum of the function

884
00:54:07,780 --> 00:54:13,990
computing the gradient of the function involves computing the gradient of these g of heat

885
00:54:14,040 --> 00:54:17,270
which is the complicated part of the great because this is linear

886
00:54:17,320 --> 00:54:20,700
this is the linear function of theta so computing the gradient is just the function

887
00:54:20,700 --> 00:54:22,090
of ft

888
00:54:22,130 --> 00:54:26,480
but computing the gradient of these obvious to be more complicated and requires compute these

889
00:54:28,450 --> 00:54:33,540
what's in expectation its estimation

890
00:54:33,600 --> 00:54:37,710
this is the summation of of these objects

891
00:54:37,810 --> 00:54:40,940
of the sufficient statistic with respect to this peak

892
00:54:40,970 --> 00:54:46,770
of x and people so if this p of x these factorizes

893
00:54:46,820 --> 00:54:49,370
you have some of the factors

894
00:54:49,410 --> 00:54:50,820
guess what you get

895
00:54:50,840 --> 00:54:53,440
the distributive law again you can compute

896
00:54:53,490 --> 00:54:59,690
these expectations efficiently if you have three or you have a chordal graph well

897
00:54:59,690 --> 00:55:03,570
you can compute this is

898
00:55:04,210 --> 00:55:06,850
in other words

899
00:55:06,960 --> 00:55:09,720
the maximum likelihood estimate

900
00:55:09,780 --> 00:55:16,550
must be such that the expected value of the sufficient statistics this distribution

901
00:55:16,560 --> 00:55:21,680
for every clique has to match the sample average of

902
00:55:21,720 --> 00:55:23,510
it's a very intuitive notion

903
00:55:23,560 --> 00:55:27,120
the same way as what's the maximum likelihood estimate for

904
00:55:27,120 --> 00:55:35,450
the average of one gaussian distribution is just the average of the sample

905
00:55:35,500 --> 00:55:40,240
in this problem is a convex problem

906
00:55:40,290 --> 00:55:43,860
how the show is a convex problem essentially you need to show that the log

907
00:55:43,860 --> 00:55:46,680
partition function

908
00:55:46,720 --> 00:55:48,730
is convex

909
00:55:48,890 --> 00:55:54,360
we have g

910
00:55:54,440 --> 00:55:56,460
this court here g

911
00:56:01,660 --> 00:56:08,840
g of theta is the log of this function is convex

912
00:56:08,840 --> 00:56:11,280
it to show that

913
00:56:11,300 --> 00:56:15,220
only show that you just take the second derivative of this function

914
00:56:15,240 --> 00:56:19,300
with respect to the initial positive everywhere

915
00:56:19,360 --> 00:56:20,500
in particular issue

916
00:56:20,520 --> 00:56:22,240
people if you have

917
00:56:22,290 --> 00:56:26,090
back to if the thing in this case in the general case of factors who

918
00:56:26,990 --> 00:56:28,900
this second derivative

919
00:56:28,940 --> 00:56:33,050
here this will be a matrix right because the number first three with you will

920
00:56:33,050 --> 00:56:37,560
be the gradient vector second derivative will be the hessian matrix

921
00:56:37,670 --> 00:56:41,510
and that has to be positive semidefinite

922
00:56:41,570 --> 00:56:44,410
which is the equivalent of positive numbers for multi

923
00:56:44,410 --> 00:56:48,180
the mission objects for

924
00:56:48,260 --> 00:56:50,700
so basically you we need to show that

925
00:56:52,420 --> 00:56:53,430
the two

926
00:56:53,440 --> 00:56:55,710
g of to

927
00:56:55,770 --> 00:56:56,520
the a

928
00:57:07,130 --> 00:57:10,640
positive semi

929
00:57:10,690 --> 00:57:12,770
good exercise

930
00:57:12,860 --> 00:57:16,450
so this is very nice this function is convex

931
00:57:16,680 --> 00:57:22,820
and then you can solve exactly the optimisation problem OK

932
00:57:22,890 --> 00:57:29,610
the here i have written a particular instantiation four keyser have multinomial reliables what tool

933
00:57:29,620 --> 00:57:32,400
just keep these equations here because

934
00:57:32,450 --> 00:57:35,590
it's just more technical stuff in know this

935
00:57:35,690 --> 00:57:37,810
pull tarts

936
00:57:37,840 --> 00:57:38,840
so let's

937
00:57:41,380 --> 00:57:43,670
for the last part which is

938
00:57:43,690 --> 00:57:45,030
what we do when

939
00:57:45,070 --> 00:57:46,100
we have

940
00:57:46,190 --> 00:57:52,620
learning with older in the wild

941
00:57:52,660 --> 00:57:56,930
first how we learn the potential functions when we have observed the doctor for all

942
00:57:56,930 --> 00:57:59,240
divided into my

943
00:57:59,300 --> 00:58:02,970
and second rule see how do we learn the potential functions

944
00:58:02,980 --> 00:58:09,110
when you have lead-in fighter so he didn't like the

945
00:58:09,150 --> 00:58:10,370
first case

946
00:58:11,870 --> 00:58:15,540
i want to learn the model

947
00:58:15,610 --> 00:58:18,640
love parameters

948
00:58:18,690 --> 00:58:20,280
and i have

949
00:58:20,310 --> 00:58:23,770
observations of my in time

950
00:58:23,820 --> 00:58:25,230
so for example

951
00:58:25,240 --> 00:58:28,050
let's assume i have i want to learn a particular model

952
00:58:28,990 --> 00:58:31,460
are images

953
00:58:31,490 --> 00:58:33,980
i observe one sample of speech

954
00:58:33,990 --> 00:58:36,200
one observation of speech which is

955
00:58:36,210 --> 00:58:40,420
an entire set of nodes are being observed one old correspond to this part of

956
00:58:40,430 --> 00:58:47,360
speech part of speech the first sample o thing it's an entire observation of my

957
00:58:47,360 --> 00:58:49,070
down here the

958
00:58:49,120 --> 00:58:51,580
here the most frequent cascades

959
00:58:51,590 --> 00:58:56,370
we know that stars stars are more common than chains and seven out of the

960
00:58:56,370 --> 00:58:58,810
ten most frequent cascades and the

961
00:58:58,820 --> 00:59:02,150
in the original data match ours

962
00:59:02,170 --> 00:59:05,540
so we know so here's the data are represented by circles of the model is

963
00:59:05,540 --> 00:59:07,720
represented by the dashed line

964
00:59:07,730 --> 00:59:13,220
so we have real that cascade size distribution cascade in degree size stars in the

965
00:59:13,220 --> 00:59:18,310
side chains so this is pretty pretty surprising results

966
00:59:33,640 --> 00:59:36,570
this is a course

967
00:59:43,670 --> 00:59:49,490
is the last

968
00:59:49,510 --> 00:59:52,950
this means

969
01:00:01,580 --> 01:00:06,640
all this

970
01:00:06,660 --> 01:00:09,330
he was just this

971
01:00:25,480 --> 01:00:29,220
i my my guess is that most of the

972
01:00:29,240 --> 01:00:31,870
currently the pass along ones turn out be stars

973
01:00:31,940 --> 01:00:35,580
because if i see video i might not really add anything to it

974
01:00:35,640 --> 01:00:39,220
so if i link to it my friends is that they they might or might

975
01:00:39,220 --> 01:00:41,550
not actually link to my blog

976
01:00:41,570 --> 01:00:44,310
the i is that what happens in

977
01:00:44,320 --> 01:00:47,520
how much do or violates

978
01:00:52,310 --> 01:00:56,650
based on

979
01:01:04,240 --> 01:01:17,010
are you a thousand years ago people better at in our

980
01:01:31,160 --> 01:01:34,410
that's an interesting question and

981
01:01:36,060 --> 01:01:38,100
i'm not not sure that i a

982
01:01:38,110 --> 01:01:41,640
knowledgeable about the content some of people in this room might be so

983
01:01:43,180 --> 01:01:48,150
my background is in this these structure structural ideas and i know that it's very

984
01:01:49,700 --> 01:01:54,100
it's it's blind it's blind to a lot of the content and i think that

985
01:01:54,330 --> 01:01:57,890
too to really do a lot of these things we want to use both structure

986
01:01:57,890 --> 01:01:59,780
and content

987
01:02:06,720 --> 01:02:12,340
are we

988
01:03:06,540 --> 01:03:18,450
right that's kind of what we're going for is that we

989
01:03:18,470 --> 01:03:21,570
we realize that the drawback of this is that we're already assuming this

990
01:03:21,590 --> 01:03:24,810
built in the blog network we know who is linked to in the past

991
01:03:24,890 --> 01:03:29,100
so we make this assumption that will somebody's link somebody they probably read his blog

992
01:03:29,100 --> 01:03:31,820
a regular basis and so therefore we

993
01:03:31,910 --> 01:03:37,770
probably want to use that that knowledge when

994
01:03:37,810 --> 01:03:39,770
that knowledge we're actually

995
01:03:39,780 --> 01:03:44,580
generating our cascades

996
01:03:45,480 --> 01:03:49,150
as last

997
01:03:49,180 --> 01:03:51,590
so far

998
01:04:11,020 --> 01:04:13,060
right right

999
01:04:13,070 --> 01:04:18,150
this some early workers we're trying to identify some general patterns and then

1000
01:04:18,170 --> 01:04:23,280
and build upon

1001
01:04:23,280 --> 01:04:25,770
of higher order correlations

1002
01:04:25,780 --> 01:04:32,400
o or in other words nonlinear correlations they are

1003
01:04:32,430 --> 01:04:35,810
you can you can measure them by some good by looking at

1004
01:04:35,810 --> 01:04:39,420
looking at

1005
01:04:39,430 --> 01:04:50,360
correlations of covariances of someone non-linear transformations of these components

1006
01:04:50,520 --> 01:04:54,360
then the question is that what kind of changes would be useful here well we

1007
01:04:54,360 --> 01:04:58,000
will see in a couple of sites with michael

1008
01:04:58,010 --> 01:04:59,840
yes and

1009
01:05:00,030 --> 01:05:03,400
one way of using these dependencies there needs to

1010
01:05:03,490 --> 01:05:09,130
these are by is to define some kind of topographical order that something like in

1011
01:05:09,130 --> 01:05:10,880
a self organizing map

1012
01:05:10,930 --> 01:05:16,870
so that's you would say so that's or actually you could it smallest like defining

1013
01:05:16,870 --> 01:05:21,870
using these dependencies to define some kind of dissimilarity that you can then import to

1014
01:05:21,870 --> 01:05:26,780
any kind of visualization and well we have more details of this

1015
01:05:26,820 --> 01:05:29,040
so the idea is when we

1016
01:05:29,060 --> 01:05:34,510
if i use this idea of topography

1017
01:05:34,600 --> 01:05:36,660
the point is that we

1018
01:05:37,250 --> 01:05:43,090
that we assign each other components into into a location on the lattice which might

1019
01:05:43,090 --> 01:05:46,360
be two-dimensional in the the basic case

1020
01:05:46,370 --> 01:05:51,470
this is just in just the same way as as in a self organizing map

1021
01:05:51,690 --> 01:05:53,650
is alcohol

1022
01:05:55,340 --> 01:05:56,630
now the point is that

1023
01:05:56,650 --> 01:05:59,010
we define the model so

1024
01:05:59,010 --> 01:06:00,670
that's components

1025
01:06:00,690 --> 01:06:05,490
so that is the goal of sort of the components are dependent

1026
01:06:05,490 --> 01:06:10,070
the closer the other more dependent the closer they are on this topic

1027
01:06:10,120 --> 01:06:16,290
so companies that are far from each other on the topography or more-or-less independent so

1028
01:06:16,290 --> 01:06:20,530
which are close to each other on the topography creates dependent

1029
01:06:20,540 --> 01:06:25,880
so we have actually to we could actually use two different approaches either we

1030
01:06:25,900 --> 01:06:31,260
smallest like to ICA first and then we visualize the components by

1031
01:06:31,270 --> 01:06:34,270
ordering them on this kind of the topography

1032
01:06:34,280 --> 01:06:39,810
so now we would at each node here then we would then somehow not representation

1033
01:06:39,830 --> 01:06:41,410
of each component

1034
01:06:41,430 --> 01:06:43,600
but the but the more principled way

1035
01:06:43,610 --> 01:06:49,400
one is to define the probabilistic statistical model which has this property that

1036
01:06:49,670 --> 01:06:54,200
so far components which are far away from the top of the independent and then

1037
01:06:54,200 --> 01:06:59,150
there's not that close to each other somehow dependent

1038
01:06:59,170 --> 01:07:03,280
then we have again we have the problem that we can not we can never

1039
01:07:03,290 --> 01:07:08,630
talk about dependencies in general i mean well it is it's very difficult to define

1040
01:07:08,630 --> 01:07:13,130
model where we just says components and and and then we have to choose what

1041
01:07:13,130 --> 01:07:16,960
kind of dependencies we actually modelling

1042
01:07:16,990 --> 01:07:20,490
well we have some words that the more

1043
01:07:24,270 --> 01:07:29,150
no the son of the linear components

1044
01:07:29,170 --> 01:07:31,720
we don't know we don't have yet subspaces

1045
01:07:31,850 --> 01:07:36,460
i mean it just the components is not the subspace but just linear components

1046
01:07:37,770 --> 01:07:42,800
the intensity images

1047
01:07:43,690 --> 01:07:50,860
so in terms of these in terms of image image features

1048
01:07:50,870 --> 01:07:54,830
each other each other knowledge will be just one of these windows

1049
01:07:54,860 --> 01:07:59,620
which one each of these windows is one linear components one linear basis vector in

1050
01:07:59,620 --> 01:08:01,870
the linear decomposition

1051
01:08:02,470 --> 01:08:05,940
how that actually in moment

1052
01:08:10,320 --> 01:08:12,920
but the one thing that that remains from all

1053
01:08:12,940 --> 01:08:18,770
all these previous models of images well i'm talking about images again but this is

1054
01:08:19,050 --> 01:08:22,600
this framework can be applied to other kinds of data as well but in the

1055
01:08:22,600 --> 01:08:26,050
i assume smoothness each manifold

1056
01:08:26,060 --> 01:08:27,870
so that's what we wanted

1057
01:08:28,530 --> 01:08:31,810
so the building block is actually this

1058
01:08:31,830 --> 01:08:36,330
if you have a lot of unlabelled data we're zooming onto the dollar signs up

1059
01:08:36,410 --> 01:08:37,870
zooming on part of it

1060
01:08:37,890 --> 01:08:40,680
if you have a lot of unlabelled data

1061
01:08:40,780 --> 01:08:42,430
and if you look at

1062
01:08:42,640 --> 01:08:49,640
the neighborhood of any anybody to point and you compute the covariance matrix there

1063
01:08:49,680 --> 01:08:52,470
that shape actually tells you something about

1064
01:08:52,530 --> 01:08:56,160
the manifold structure so here you will see this

1065
01:08:56,180 --> 01:09:01,810
sort of flat pancakes but they gradually curve and this vertical pancakes somewhere in the

1066
01:09:01,810 --> 01:09:06,720
middle they will have because you have

1067
01:09:06,720 --> 01:09:12,930
points from both manifolds the covariance structure will be more likeable

1068
01:09:14,830 --> 01:09:20,720
so the idea is to say that we can measure this week actually see which

1069
01:09:20,740 --> 01:09:23,030
kind of pancakes go together

1070
01:09:23,030 --> 01:09:28,350
and one particular measure can use so-called hellinger distance

1071
01:09:28,370 --> 01:09:32,120
it's between two distributions p and q

1072
01:09:32,330 --> 01:09:36,240
it's it's a nice distribution it's kind of like

1073
01:09:36,240 --> 01:09:40,260
kl divergence but is symmetric and the actually lower bound scale

1074
01:09:40,260 --> 01:09:41,600
it's easier one

1075
01:09:41,620 --> 01:09:44,430
and well so

1076
01:09:44,490 --> 01:09:46,080
if you have two

1077
01:09:46,100 --> 01:09:48,200
covariance matrices you can

1078
01:09:49,560 --> 01:09:54,080
the hellinger distance there to be the distance from the

1079
01:09:54,100 --> 01:09:59,350
normal distribution with that covariance matrix that gives you a nice closed form solution which

1080
01:09:59,350 --> 01:10:02,810
only involves those covariance matrices

1081
01:10:02,830 --> 01:10:04,280
and here's the intuition

1082
01:10:04,330 --> 01:10:06,430
if you have two

1083
01:10:06,470 --> 01:10:08,470
the covariance matrices

1084
01:10:08,680 --> 01:10:11,580
and i'm showing the hellinger distance here

1085
01:10:11,640 --> 01:10:17,850
the distance here is small because these two of not identical they are somewhat similar

1086
01:10:17,890 --> 01:10:21,220
this one is bigger

1087
01:10:21,240 --> 01:10:25,790
and they actually have the same covariance except that this one has a larger variance

1088
01:10:26,050 --> 01:10:30,030
also it's more big

1089
01:10:30,080 --> 01:10:36,100
if the pancakes the covariance matrices differ by dimensionality you actually have a very large

1090
01:10:37,160 --> 01:10:42,660
as well as if you they have a slightly different orientation then you get that

1091
01:10:42,680 --> 01:10:45,200
very large distance

1092
01:10:45,220 --> 01:10:49,160
and what we can do now is to

1093
01:10:52,580 --> 01:10:54,530
instead of measuring

1094
01:10:54,550 --> 01:10:56,180
the graph

1095
01:10:56,390 --> 01:11:02,530
instead of connecting two unlabelled data points where the edge weights is

1096
01:11:02,550 --> 01:11:04,720
related to the euclidean distance

1097
01:11:04,740 --> 01:11:06,490
now let's do this

1098
01:11:06,530 --> 01:11:09,100
with the distance

1099
01:11:09,100 --> 01:11:12,280
between their corresponding covariance matrices

1100
01:11:12,310 --> 01:11:14,850
so that the intuition is if they

1101
01:11:17,760 --> 01:11:19,220
pancake around

1102
01:11:19,240 --> 01:11:21,700
they will be connected with a large

1103
01:11:21,700 --> 01:11:24,850
hatch weight

1104
01:11:24,890 --> 01:11:28,350
and if you do this for example the dollar sign

1105
01:11:28,370 --> 01:11:32,830
you have agianst this is the k nearest neighbour graph edges like this where red

1106
01:11:32,830 --> 01:11:36,850
means large actuate and yellow means small edge weight

1107
01:11:36,870 --> 01:11:38,910
as we hoped

1108
01:11:38,930 --> 01:11:44,390
this will give you more edge weights when the different types of manifolds cross each

1109
01:11:45,330 --> 01:11:46,280
and that's

1110
01:11:46,290 --> 01:11:53,280
good because then we can separate out different manifolds

1111
01:11:58,740 --> 01:12:00,370
this should be fun

1112
01:12:00,410 --> 01:12:05,910
the and machinery

1113
01:12:07,600 --> 01:12:11,700
why do we want to ask the question whether human performance so much was learning

1114
01:12:11,700 --> 01:12:13,640
or not

1115
01:12:13,640 --> 01:12:16,280
again as the example

1116
01:12:16,330 --> 01:12:17,910
we gave before

1117
01:12:18,470 --> 01:12:20,310
i think children really

1118
01:12:20,330 --> 01:12:21,930
learn by e

1119
01:12:21,990 --> 01:12:24,370
combine both supervised

1120
01:12:24,430 --> 01:12:29,560
teaching and unsupervised observation from the world and it would be really nice too

1121
01:12:29,640 --> 01:12:31,450
see whether humans can

1122
01:12:32,870 --> 01:12:36,280
make use of those kinds of information

1123
01:12:36,290 --> 01:12:39,160
so we'll be discussing two

1124
01:12:39,240 --> 01:12:40,720
psychology studies

1125
01:12:40,740 --> 01:12:44,100
that has some flavor of some unsupervised learning

1126
01:12:44,120 --> 01:12:47,740
one is in the setting of a single class

1127
01:12:49,100 --> 01:12:52,120
the is in binary classification

1128
01:12:55,330 --> 01:13:01,010
so the first study i want to connect this to others self training

1129
01:13:02,160 --> 01:13:04,740
and basically we want to

1130
01:13:04,780 --> 01:13:07,100
show that

1131
01:13:07,120 --> 01:13:09,260
test data

1132
01:13:09,280 --> 01:13:10,870
actually changes

1133
01:13:10,890 --> 01:13:13,930
you're underlying classifier

1134
01:13:14,510 --> 01:13:16,260
so instead of

1135
01:13:16,280 --> 01:13:21,890
and learning algorithm now we have participants there the ground by the way

1136
01:13:21,890 --> 01:13:24,830
that's how do psychologists study u

1137
01:13:24,850 --> 01:13:29,180
they have to take a class for credit part of the classes to participate in

1138
01:13:29,180 --> 01:13:30,830
some psychologists study

1139
01:13:32,620 --> 01:13:36,140
on the ground you think of them as learning

1140
01:13:36,260 --> 01:13:38,760
and we're going to make them

1141
01:13:39,530 --> 01:13:45,410
former task that's very similar or we can formulate that as learning task

1142
01:13:45,560 --> 01:13:48,530
so here's the first experiment

1143
01:13:48,550 --> 01:13:50,680
we're going to give

1144
01:13:50,810 --> 01:13:52,810
each participant

1145
01:13:52,830 --> 01:13:54,530
the training set

1146
01:13:54,550 --> 01:13:57,560
of l labelled data points

1147
01:13:57,580 --> 01:14:00,580
but these elderly points all come from

1148
01:14:00,620 --> 01:14:03,640
class one so it's a single class prob

1149
01:14:03,640 --> 01:14:05,680
it's just class one

1150
01:14:05,700 --> 01:14:07,060
and then

1151
01:14:07,970 --> 01:14:09,080
later on

1152
01:14:09,080 --> 01:14:12,340
if i tell you where the object is with the pendulum i gave you the

1153
01:14:12,340 --> 01:14:13,360
initial angle

1154
01:14:13,380 --> 01:14:16,210
my data zero i tell you ready object is

1155
01:14:16,240 --> 01:14:17,540
so you know actually b

1156
01:14:17,580 --> 01:14:20,630
i tell you what's been i gave it so you know this one

1157
01:14:20,640 --> 01:14:24,570
you can calculate what the maximum display exactly the same idea

1158
01:14:24,590 --> 01:14:27,610
and if you want to apply to work energy theorem of course

1159
01:14:27,630 --> 01:14:32,260
you'll get exactly the same results this is in fact the same thing

1160
01:14:32,290 --> 01:14:34,060
so let's now

1161
01:14:34,150 --> 01:14:35,610
thirty two

1162
01:14:36,860 --> 01:14:38,080
universal law

1163
01:14:38,090 --> 01:14:39,870
of gravity

1164
01:14:39,910 --> 01:14:41,700
and believe it or not

1165
01:14:41,750 --> 01:14:46,680
i think i even gave you only exams

1166
01:14:46,690 --> 01:14:48,790
the gravitational force

1167
01:14:48,790 --> 01:14:50,810
newton's universal law

1168
01:14:53,510 --> 01:14:57,630
i don't think you need to you will find it on the example

1169
01:14:57,690 --> 01:15:00,610
that's the idea is going around the sun

1170
01:15:00,620 --> 01:15:03,540
and that's approximate orbit by

1171
01:15:04,840 --> 01:15:06,370
here is the US

1172
01:15:06,380 --> 01:15:10,140
here's the son last summer

1173
01:15:10,150 --> 01:15:12,610
and were two

1174
01:15:12,610 --> 01:15:14,940
radius is capital are

1175
01:15:15,000 --> 01:15:18,190
the earth has an orbital

1176
01:15:19,800 --> 01:15:20,900
college v

1177
01:15:20,900 --> 01:15:22,650
all the time

1178
01:15:23,990 --> 01:15:27,720
there is on the object the angular velocity is a

1179
01:15:28,970 --> 01:15:31,150
goes around certain

1180
01:15:31,200 --> 01:15:32,310
thank you

1181
01:15:33,390 --> 01:15:34,600
and it has to be

1182
01:15:34,670 --> 01:15:36,420
the force on the system

1183
01:15:36,430 --> 01:15:37,930
the gravitational force

1184
01:15:37,940 --> 01:15:39,400
it is the same as the

1185
01:15:39,450 --> 01:15:43,590
centripetal force that's the only way he object to go around

1186
01:15:43,650 --> 01:15:46,560
in a circle holding gravitational force

1187
01:15:46,570 --> 01:15:50,740
called mass of the sun

1188
01:15:50,800 --> 01:15:53,300
mass of earth

1189
01:15:53,350 --> 01:15:54,330
times g

1190
01:15:54,370 --> 01:15:56,770
divided by our screens

1191
01:15:56,850 --> 01:15:58,640
the force of gravity

1192
01:15:58,650 --> 01:16:00,610
but it also must equal

1193
01:16:00,610 --> 01:16:02,250
so centripetal

1194
01:16:02,270 --> 01:16:04,250
acceleration and this year

1195
01:16:04,290 --> 01:16:05,920
the my

1196
01:16:05,960 --> 01:16:07,960
and so it also is

1197
01:16:07,960 --> 01:16:09,830
the mass of of the earth

1198
01:16:09,890 --> 01:16:11,810
i'm orbital speed

1199
01:16:11,890 --> 01:16:15,040
this grid divided by orbital radius

1200
01:16:15,140 --> 01:16:17,230
and if you prefer to write that

1201
01:16:17,370 --> 01:16:20,000
in terms of omega screen are

1202
01:16:20,060 --> 01:16:22,170
that's fine too

1203
01:16:22,210 --> 01:16:23,350
OK this year

1204
01:16:23,410 --> 01:16:26,500
you know this year

1205
01:16:26,500 --> 01:16:29,730
earth masses if you want to know what your little speedos

1206
01:16:29,730 --> 01:16:31,170
you lose one are

1207
01:16:31,210 --> 01:16:36,060
and so you find that you to be equals the mass of the the sun

1208
01:16:36,120 --> 01:16:37,790
times g

1209
01:16:37,850 --> 01:16:39,330
divided by or

1210
01:16:39,330 --> 01:16:40,500
the power

1211
01:16:40,540 --> 01:16:41,710
one half

1212
01:16:41,730 --> 01:16:43,060
and that is about

1213
01:16:43,100 --> 01:16:47,310
thirty kilometres per second

1214
01:16:47,440 --> 01:16:49,750
given in kilometres per second but of course

1215
01:16:49,810 --> 01:16:53,330
when you calculate it make sure you always the MKS before you make

1216
01:16:53,330 --> 01:16:55,960
conversion to other units

1217
01:16:56,040 --> 01:16:58,850
so you know the period is to go around

1218
01:16:58,890 --> 01:17:00,540
the sun

1219
01:17:00,540 --> 01:17:02,040
that is to are

1220
01:17:02,600 --> 01:17:04,310
divided by d orbital

1221
01:17:04,310 --> 01:17:07,830
and that's about three hundred sixty

1222
01:17:07,830 --> 01:17:11,850
five and a half days that's one year following takes us to go wrong

1223
01:17:11,850 --> 01:17:14,080
the sun

1224
01:17:14,100 --> 01:17:17,500
now i wonder what is the kinetic energy

1225
01:17:17,500 --> 01:17:19,370
the earth

1226
01:17:19,370 --> 01:17:21,410
in orbit around

1227
01:17:21,420 --> 01:17:22,500
the sun

1228
01:17:22,620 --> 01:17:26,170
well that's completely trivial you would say because

1229
01:17:26,170 --> 01:17:27,790
it is one of

1230
01:17:27,830 --> 01:17:29,620
the mass of the earth

1231
01:17:29,670 --> 01:17:32,440
times the orbitals spacecraft

1232
01:17:34,080 --> 01:17:36,060
that is not

1233
01:17:36,080 --> 01:17:37,960
very difficult

1234
01:17:38,000 --> 01:17:40,080
so the kinetic energy

1235
01:17:40,140 --> 01:17:42,460
of the earth in orbit

1236
01:17:43,310 --> 01:17:44,420
one half

1237
01:17:44,440 --> 01:17:46,520
last year

1238
01:17:46,650 --> 01:17:48,830
the orbits

1239
01:17:50,770 --> 01:17:54,580
so i take this equation and iswc i lose half their

1240
01:17:54,600 --> 01:17:56,310
so i get

1241
01:17:58,600 --> 01:17:59,710
and so on

1242
01:18:02,670 --> 01:18:04,100
i i

1243
01:18:04,150 --> 01:18:06,230
that is the kinetic energy

1244
01:18:06,230 --> 01:18:07,720
the commands

1245
01:18:07,730 --> 01:18:12,580
things in the object form i could write down changes will show you the changes

1246
01:18:12,580 --> 01:18:14,670
would be

1247
01:18:14,730 --> 01:18:18,240
the difference between

1248
01:18:19,910 --> 01:18:24,480
this is what i did constraints the constraints failed

1249
01:18:24,490 --> 01:18:25,830
the change

1250
01:18:26,220 --> 01:18:33,300
what's more distracting because the different individual between male and female

1251
01:18:33,480 --> 01:18:37,180
i can use this you roll things

1252
01:18:37,200 --> 01:18:39,390
so i could undo

1253
01:18:39,410 --> 01:18:40,600
that change

1254
01:18:40,610 --> 01:18:44,710
which would again return to the to the unsatisfiability

1255
01:18:44,730 --> 01:18:48,670
of course i can do it again sort of silly but a fun

1256
01:18:48,880 --> 01:18:53,580
and i can see these changes and i can search for the missing

1257
01:18:53,640 --> 01:18:55,400
and we all know about

1258
01:18:56,520 --> 01:19:03,470
revisit the whole ontology and again usually very comfortable making changes because it's important

1259
01:19:03,490 --> 01:19:08,350
he is the only thing that i would

1260
01:19:08,360 --> 01:19:12,050
we're not going to the of

1261
01:19:12,070 --> 01:19:15,010
many many times

1262
01:19:15,250 --> 01:19:19,820
explicitly and it was released under constraint hold

1263
01:19:19,840 --> 01:19:23,230
that is actually kind of interesting that they didn't bother to put in the

1264
01:19:23,400 --> 01:19:24,940
the to the

1265
01:19:24,990 --> 01:19:28,010
inequality between the two

1266
01:19:28,330 --> 01:19:33,880
so that was how we get there when we get

1267
01:19:34,050 --> 01:19:36,750
we got there by show reference

1268
01:19:37,970 --> 01:19:39,640
four again

1269
01:19:39,660 --> 01:19:42,940
to think about how you're navigating through things

1270
01:19:42,950 --> 01:19:46,540
and it's got a long way

1271
01:19:46,560 --> 01:19:50,470
you should be OK there's two ways

1272
01:19:50,510 --> 01:19:55,890
these is the most commonly used on the whole drama class in your right click

1273
01:19:55,890 --> 01:19:58,940
on the negative control like get

1274
01:20:00,170 --> 01:20:03,060
you can show this is

1275
01:20:04,570 --> 01:20:07,380
so this is how to do so

1276
01:20:07,400 --> 01:20:09,760
what is

1277
01:20:10,000 --> 01:20:12,700
of her

1278
01:20:15,670 --> 01:20:19,140
so this is what the

1279
01:20:21,070 --> 01:20:23,930
see how i was going to go

1280
01:20:24,350 --> 01:20:26,690
this is again

1281
01:20:27,760 --> 01:20:28,830
so the thing to

1282
01:20:28,840 --> 01:20:30,520
the class

1283
01:20:31,120 --> 01:20:33,740
and fact show

1284
01:20:33,870 --> 01:20:37,200
that immediately gives you some sort of structure we're trying to figure

1285
01:20:37,260 --> 01:20:39,410
part of relations

1286
01:20:39,460 --> 01:20:43,180
and everything is referred to

1287
01:20:59,450 --> 01:21:02,270
this is true

1288
01:21:02,320 --> 01:21:05,260
you can change

1289
01:21:10,440 --> 01:21:13,170
so started structure

1290
01:21:13,180 --> 01:21:16,800
you can also do a lookup returns just is very similar

1291
01:21:16,820 --> 01:21:19,030
in this case if i want to look for male

1292
01:21:19,040 --> 01:21:22,190
i can see every place that has this its an now the

1293
01:21:22,200 --> 01:21:23,900
when you have turned

1294
01:21:26,740 --> 01:21:31,520
this will be some more

1295
01:21:37,260 --> 01:21:39,240
or the

1296
01:21:41,140 --> 01:21:45,940
also being forced to some

1297
01:21:48,630 --> 01:21:49,550
first was

1298
01:21:49,570 --> 01:21:50,870
very high

1299
01:21:50,920 --> 01:21:55,100
well first of all you

1300
01:21:55,120 --> 01:21:58,440
to get some ideas about what the whole ontology

1301
01:21:58,450 --> 01:22:02,080
to go brought to find points

1302
01:22:02,090 --> 01:22:03,770
what's on

1303
01:22:09,950 --> 01:22:11,050
period of

1304
01:22:13,010 --> 01:22:18,200
we might not honest you can also do this is by iteratively exploring

1305
01:22:18,500 --> 01:22:20,050
the ontology

1306
01:22:21,310 --> 01:22:23,190
so here i

1307
01:22:23,200 --> 01:22:24,970
this is quite this is reasonable sort

1308
01:22:24,980 --> 01:22:29,600
this is too small make it something that

1309
01:22:29,620 --> 01:22:33,600
OK so here is to the wall

1310
01:22:33,790 --> 01:22:35,730
some of the other

1311
01:22:35,830 --> 01:22:40,160
so obviously this book exposes across subclasses student to soon it's going to be very

1312
01:22:41,310 --> 01:22:44,190
so i understand

1313
01:22:44,210 --> 01:22:44,880
a graduate student

1314
01:22:44,890 --> 01:22:48,140
we need to understand student and that's a reasonable thing to do

1315
01:22:48,160 --> 01:22:53,370
go look at seems see what with song and then go back

1316
01:22:53,530 --> 01:22:58,780
clearly graduate student has some sort of refinement and also has had a particular degree

1317
01:22:58,800 --> 01:23:00,270
so i don't understand how to

1318
01:23:01,050 --> 01:23:04,590
this kind of back-and-forth local exploration where

1319
01:23:04,720 --> 01:23:06,780
explore and information

1320
01:23:06,790 --> 01:23:07,860
for the class

1321
01:23:07,890 --> 01:23:11,460
the force

1322
01:23:13,890 --> 01:23:16,540
this is

1323
01:23:24,000 --> 01:23:26,290
this figure out what's going on

1324
01:23:26,310 --> 01:23:30,110
i find it helpful when we don't have a particular

1325
01:23:30,490 --> 01:23:35,360
on the four

1326
01:23:35,380 --> 01:23:41,200
the large majority of us is is more effective for me

1327
01:23:41,210 --> 01:23:44,290
these explain but that's it that's against the you

1328
01:23:44,400 --> 01:23:45,100
have to pick up

1329
01:23:51,720 --> 01:23:54,830
just to him you know

1330
01:23:54,840 --> 01:23:57,350
everything behaves pretty much like a normal

1331
01:23:57,360 --> 01:24:00,260
web browsers so you had here

1332
01:24:00,290 --> 01:24:05,460
history there can jump that is they have not just the fact that can jump

1333
01:24:05,460 --> 01:24:10,660
further you can bookmark individuals and entities to come back to look at now it

1334
01:24:10,670 --> 01:24:15,550
so it's all pretty pretty normal from the best perspective

1335
01:24:15,870 --> 01:24:18,120
so we do a little bit

1336
01:24:18,220 --> 01:24:22,160
so to get into editing

1337
01:24:22,170 --> 01:24:24,590
you need to click the edible

1338
01:24:24,600 --> 01:24:28,890
and you'll get a number nine but

1339
01:24:28,940 --> 01:24:34,340
the art of graph are going to

1340
01:24:35,710 --> 01:24:38,610
three just text worked

1341
01:24:40,210 --> 01:24:43,150
i recommend that strongly

1342
01:24:45,330 --> 01:24:49,430
you of you lost

1343
01:24:49,440 --> 01:24:54,760
all these

1344
01:24:56,740 --> 01:25:05,730
this is one

1345
01:25:05,750 --> 01:25:07,590
the right

1346
01:25:09,920 --> 01:25:13,230
so there's a fairly strong subclasses are weaker than

1347
01:25:16,080 --> 01:25:20,980
often what most people use or some sort of across becoming and you get

1348
01:25:21,010 --> 01:25:23,730
OK like this

1349
01:25:30,760 --> 01:25:33,980
so that

1350
01:25:34,000 --> 01:25:37,230
if you have a very limited time so restrictions

1351
01:25:37,250 --> 01:25:38,610
restrictions are

1352
01:25:38,610 --> 01:25:40,690
that's not a function

1353
01:25:41,490 --> 01:25:43,610
recall function is

1354
01:25:45,200 --> 01:25:47,870
mathematicians called generalized functions

1355
01:25:47,880 --> 01:25:49,790
but that's not a function

1356
01:25:49,800 --> 01:25:51,200
it's just a way

1357
01:25:51,250 --> 01:25:56,450
making you feel comfortable about talking about something which is a function

1358
01:25:57,370 --> 01:26:00,150
it has the name was given the name

1359
01:26:00,170 --> 01:26:01,910
introduce formally

1360
01:26:01,920 --> 01:26:04,690
the mathematics by

1361
01:26:05,850 --> 01:26:08,280
this is iraq and its

1362
01:26:10,100 --> 01:26:13,430
looking ahead to the future that many people do

1363
01:26:13,440 --> 01:26:18,330
if you do something similar to which the formula or function or something which they

1364
01:26:18,330 --> 01:26:21,530
think is going to be important

1365
01:26:21,700 --> 01:26:26,000
he was to they never named directly after themselves but they always

1366
01:26:26,030 --> 01:26:28,910
uses the symbol for the first letter of their name

1367
01:26:28,920 --> 01:26:30,050
i can tell you how

1368
01:26:31,910 --> 01:26:34,230
they needed oiler called e

1369
01:26:34,240 --> 01:26:36,270
and for that reason although it

1370
01:26:36,280 --> 01:26:39,330
claims it was because what because

1371
01:26:39,390 --> 01:26:41,080
so the exponentials

1372
01:26:41,100 --> 01:26:46,250
well this what we need OK so this is

1373
01:26:46,290 --> 01:26:48,540
that that's

1374
01:26:48,560 --> 01:26:49,800
paul dirac

1375
01:26:49,820 --> 01:26:53,570
delta function

1376
01:26:53,700 --> 01:26:58,780
i won't dignify by the name function by writing out by putting function were function

1377
01:26:59,190 --> 01:27:02,050
but that is called the delta function

1378
01:27:03,350 --> 01:27:09,400
from this point on the entire rest of the lecture is a slightly fictionalized

1379
01:27:09,440 --> 01:27:12,150
the entire rest lecture series in

1380
01:27:12,200 --> 01:27:14,680
negative quotation marks

1381
01:27:14,690 --> 01:27:18,150
i'm not entirely responsible for anything i say

1382
01:27:20,280 --> 01:27:23,960
this is non function but you put it in core function

1383
01:27:24,910 --> 01:27:26,680
and actually want to complete

1384
01:27:26,790 --> 01:27:29,940
it's a function that must have all plus transform

1385
01:27:29,990 --> 01:27:34,410
even though it doesn't so the diagram is completed

1386
01:27:35,210 --> 01:27:40,310
it's all plus transform is declared to be one

1387
01:27:40,700 --> 01:27:43,030
start making list properties of this

1388
01:27:43,030 --> 01:27:44,380
we are thing

1389
01:27:57,330 --> 01:27:58,850
delta function

1390
01:27:59,650 --> 01:28:07,670
this applies transform is one

1391
01:28:09,560 --> 01:28:12,100
one of the things are one

1392
01:28:12,150 --> 01:28:13,940
we haven't yet

1393
01:28:14,000 --> 01:28:16,850
express the fact that the unit impulse

1394
01:28:16,890 --> 01:28:18,850
in other words since areas

1395
01:28:18,860 --> 01:28:20,310
of all these

1396
01:28:20,320 --> 01:28:25,830
boxes we all have areas one has shrunk this way they get higher that way

1397
01:28:25,890 --> 01:28:29,300
by convention one says that the area under the

1398
01:28:29,310 --> 01:28:31,650
orange curve is also

1399
01:28:31,690 --> 01:28:33,650
remains one the limit

1400
01:28:33,660 --> 01:28:35,530
my god expressed as well

1401
01:28:35,530 --> 01:28:37,850
it's not by the following formula

1402
01:28:38,850 --> 01:28:41,920
the integral over the total impulse

1403
01:28:41,980 --> 01:28:44,140
of the delta function should be one

1404
01:28:44,910 --> 01:28:47,000
where y integrate well

1405
01:28:47,010 --> 01:28:49,770
many places zero

1406
01:28:49,780 --> 01:28:54,590
places here on the other side of the vertical vertical lines

1407
01:28:54,640 --> 01:28:59,400
but in order to avoid controversy people integrate from all the way from negative infinity

1408
01:28:59,400 --> 01:29:03,770
to infinity is zero for your time

1409
01:29:05,370 --> 01:29:08,270
this is the function class transformers one

1410
01:29:08,290 --> 01:29:15,050
it's integral from minus infinity to infinity is one

1411
01:29:15,130 --> 01:29:18,600
whilst we calculate for well i'd like to calculate it

1412
01:29:20,100 --> 01:29:21,240
o OK

1413
01:29:21,300 --> 01:29:23,460
is there for the

1414
01:29:29,010 --> 01:29:31,260
so what happens if i convoluted

1415
01:29:31,270 --> 01:29:34,150
with delta function

1416
01:29:36,410 --> 01:29:38,810
well if you go back to the definition

1417
01:29:38,850 --> 01:29:42,210
of the convolution you know it's funny integral

1418
01:29:43,360 --> 01:29:44,600
you get

1419
01:29:44,650 --> 01:29:48,910
do a lot of head-scratching because it's not really all that we are at great

1420
01:29:48,930 --> 01:29:50,660
with the delta function

1421
01:29:50,740 --> 01:29:53,670
instead of doing that yet let's

1422
01:29:53,690 --> 01:30:00,760
i assume that follows was transformed in that case it transform would be well

1423
01:30:00,810 --> 01:30:05,250
the whole thing and convolution is available as transform of convolution

1424
01:30:05,260 --> 01:30:10,560
it is the product of two separate transform that's going to be that

1425
01:30:10,560 --> 01:30:16,500
times lipgloss transform the delta function which is one

1426
01:30:22,560 --> 01:30:24,720
what was this thing well

1427
01:30:24,740 --> 01:30:28,920
there's some ambiguity is what it is for negative values t

1428
01:30:28,970 --> 01:30:32,750
but if we make a brute force decided he

1429
01:30:32,750 --> 01:30:34,770
does active learning

1430
01:30:34,780 --> 01:30:40,850
it should down and after sixty well posed queries learn the role perfectly OK

1431
01:30:40,880 --> 01:30:45,040
so this is an example of active learning working

1432
01:30:45,520 --> 01:30:49,820
there's a lot more to subject just say

1433
01:30:49,940 --> 01:30:54,520
what is the sort of context we might uses the state i got from google

1434
01:30:54,540 --> 01:31:00,260
right to so years ago but this is a drug discovery context you have the

1435
01:31:00,260 --> 01:31:06,390
vector of a large number of binary shape features and you want to know

1436
01:31:07,390 --> 01:31:12,860
chemical biologically active or not active so what i mean by this

1437
01:31:12,880 --> 01:31:17,380
it is you've got all these chemicals you don't know which ones

1438
01:31:17,390 --> 01:31:24,590
relatively small number are going to be active in the biological context considering so but

1439
01:31:24,780 --> 01:31:27,200
passive learning be simply pick these

1440
01:31:27,230 --> 01:31:30,030
chemicals random find a way out

1441
01:31:30,050 --> 01:31:31,860
active learning would be

1442
01:31:31,890 --> 01:31:36,660
you know the characteristics of a of these chemicals they have drug discovery companies have

1443
01:31:36,660 --> 01:31:45,090
huge libraries of such chemical chemicals and you know the characteristics and you can therefore

1444
01:31:45,090 --> 01:31:49,370
presenter molecules so what happens biologically active yes

1445
01:31:49,380 --> 01:31:51,250
and i love this way

1446
01:31:52,280 --> 01:31:56,160
and in this dataset consists of a thousand three hundred of which thirty nine with

1447
01:31:56,190 --> 01:31:59,760
positive biologically active if passive learning

1448
01:32:00,170 --> 01:32:02,490
i for the local here

1449
01:32:02,490 --> 01:32:04,440
whereas if active learning

1450
01:32:04,470 --> 01:32:09,240
with an SVM then rapidly declined and eventually

1451
01:32:09,250 --> 01:32:11,820
quite soon i actually

1452
01:32:11,840 --> 01:32:14,160
got nearly all the active

1453
01:32:14,180 --> 01:32:16,830
biological compounds in the dataset

1454
01:32:16,870 --> 01:32:19,470
but sensitive of active learning so

1455
01:32:19,470 --> 01:32:23,270
just pointing out the sort of variance on the SVM story

1456
01:32:23,280 --> 01:32:24,200
any questions

1457
01:32:32,530 --> 01:32:34,670
well passive just pick

1458
01:32:37,530 --> 01:32:46,310
well you can see actually

1459
01:32:46,330 --> 01:32:48,590
personal and then pick a random

1460
01:32:48,610 --> 01:32:53,330
but what i'm doing is i'm not exploiting the knowledge of what gained OK

1461
01:32:53,700 --> 01:32:54,910
active learning

1462
01:32:54,930 --> 01:32:57,090
i've learnt something about the problem

1463
01:32:57,100 --> 01:33:00,090
that constrain the space should be searching

1464
01:33:00,090 --> 01:33:06,090
so now i suppose the query is probably better than totally random improve hypothesis

1465
01:33:06,100 --> 01:33:10,050
which actually improves by getnext gets better and i have run away

1466
01:33:10,090 --> 01:33:16,590
shortly answer OK because i'm using the hypothesis to tell me

1467
01:33:16,590 --> 01:33:19,670
we should be looking next OK

1468
01:33:19,700 --> 01:33:22,840
two crew then you can actually come up with counterexamples

1469
01:33:22,850 --> 01:33:28,480
where you can sort of defeated query that on the average you have problem it's

1470
01:33:28,640 --> 01:33:31,020
a good strategy

1471
01:33:31,020 --> 01:33:32,850
right so it's in

1472
01:33:32,910 --> 01:33:38,380
training SVM sinco common what's in this section so far less because of their involvement

1473
01:33:38,380 --> 01:33:40,140
in the output of programming

1474
01:33:40,160 --> 01:33:46,660
linear programming is just comments about the method used column generation techniques

1475
01:33:46,680 --> 01:33:53,410
if using quadratic programming these comments been out-of-date now the packages like minnows loco use

1476
01:33:53,890 --> 01:33:55,300
i would go to the kernel

1477
01:33:55,300 --> 01:33:56,830
machines all

1478
01:33:56,870 --> 01:33:58,900
site we've got a whole lot of software

1479
01:33:59,510 --> 01:34:01,510
i think this is slightly out of date

1480
01:34:01,520 --> 01:34:08,570
you have three general categories of learning machines really all this has to do with

1481
01:34:08,570 --> 01:34:11,580
what you happens if you have a huge dataset

1482
01:34:11,580 --> 01:34:14,020
well you can't extract the kernel k

1483
01:34:14,030 --> 01:34:16,150
you can construct kernels

1484
01:34:16,170 --> 01:34:18,310
as you go along discard

1485
01:34:18,320 --> 01:34:20,530
you also working set methods

1486
01:34:20,540 --> 01:34:23,420
in which an evolving subset of data

1487
01:34:23,440 --> 01:34:29,090
and which is specifically supported structure the problem so these are sort of different ways

1488
01:34:29,090 --> 01:34:32,020
of doing this trend

1489
01:34:32,180 --> 01:34:37,360
so what i'm on about here let's suppose we go right back to my

1490
01:34:37,390 --> 01:34:39,280
svm scenario

1491
01:34:39,280 --> 01:34:44,850
the quadratic programming problem i mentioned in the beginning it's all very well but but

1492
01:34:44,850 --> 01:34:47,330
kernel matrix was k i j

1493
01:34:47,360 --> 01:34:52,760
well i ranges one number patterns in the data now suppose that a thousand patterns

1494
01:34:52,990 --> 01:34:57,090
that kernel matrix is a thousand one thousand seven million it's not all that difficult

1495
01:34:57,090 --> 01:34:58,940
for me to therefore is also memory

1496
01:34:59,320 --> 01:35:04,310
of the machine you will encounter this problem certainly was quite with quite a few

1497
01:35:04,310 --> 01:35:07,110
dates for the beginning become large

1498
01:35:07,450 --> 01:35:12,970
the several ways round this problem if you actually encountering one is

1499
01:35:12,980 --> 01:35:15,480
something are noted already

1500
01:35:16,740 --> 01:35:21,890
if i took a hundred data points that have a data a thousand but just

1501
01:35:21,890 --> 01:35:22,630
pick out

1502
01:35:22,690 --> 01:35:29,090
a hundred from that i train would actually find that twenty five are support vectors

1503
01:35:29,090 --> 01:35:32,590
and the other seventy five and on support vectors the idea of the sort of

1504
01:35:32,650 --> 01:35:37,130
evolving subset is if i know that they are not support vectors i just dropped

1505
01:35:37,130 --> 01:35:37,910
them out

1506
01:35:37,930 --> 01:35:40,070
OK and have got this twenty five

1507
01:35:40,200 --> 01:35:45,090
then pick another bunch and in the course of the support vectors is probably increases

1508
01:35:45,090 --> 01:35:46,300
i'm going along

1509
01:35:46,310 --> 01:35:47,820
but if i'm lucky

1510
01:35:47,830 --> 01:35:54,070
then and do by chunks then i can probably avoided problems

1511
01:35:56,490 --> 01:35:58,630
so i just want to say here

1512
01:35:58,660 --> 01:36:04,900
the work that the drive these chunk process until the margin exploits of course this

1513
01:36:04,900 --> 01:36:07,290
procedure may fail because actually

1514
01:36:07,290 --> 01:36:11,490
we've got ten thousand by ten thousand chunk is going to do for it so

1515
01:36:11,490 --> 01:36:17,790
decomposition methods provide a better approach these algorithms only use fixed subset of data

1516
01:36:17,830 --> 01:36:20,970
with the alpha i for the remainder kept fixed OK

1517
01:36:20,990 --> 01:36:23,510
so i just always use a fixed subsets

1518
01:36:23,520 --> 01:36:28,300
got ten thousand data points but only ever use the hundred of any one time

1519
01:36:28,320 --> 01:36:30,510
and indeed

1520
01:36:30,530 --> 01:36:37,050
there's a street named of this stream into is algorithm proposed by john plant which

1521
01:36:37,050 --> 01:36:43,800
is small of sequential minimal optimisation which the two to only consider two samples anyone

1522
01:36:43,800 --> 01:36:45,080
can OK

1523
01:36:45,100 --> 01:36:50,100
so this is extreme limit of decomposition in other words only two alpha i wish

1524
01:36:50,100 --> 01:36:55,120
optimise this is a small subset the parameters i can consider i can't do this

1525
01:36:55,120 --> 01:37:02,310
one must always consider two you can prove this because if from this criterion can't

1526
01:37:02,310 --> 01:37:06,340
we change one of to time because it violates whereas from data at the time

1527
01:37:06,410 --> 01:37:07,840
i can maintain that

1528
01:37:07,940 --> 01:37:13,300
criterion all along OK so this is the extreme case

1529
01:37:13,300 --> 01:37:17,340
but now

1530
01:37:17,790 --> 01:37:23,760
after all

1531
01:39:11,700 --> 01:39:17,770
so much

1532
01:40:18,700 --> 01:40:21,810
there's only one man

1533
01:41:04,530 --> 01:41:08,380
one is

1534
01:41:12,240 --> 01:41:17,550
well i would

1535
01:41:17,580 --> 01:41:24,590
of these are

1536
01:41:56,200 --> 01:42:15,580
or that he

1537
01:42:27,460 --> 01:42:30,300
ninety five percent

1538
01:42:39,670 --> 01:42:43,810
the other

1539
01:42:49,650 --> 01:42:51,690
and from all

1540
01:42:54,820 --> 01:43:01,220
one of them

1541
01:43:01,670 --> 01:43:06,510
he says that

1542
01:43:28,040 --> 01:43:30,140
i mean

1543
01:43:43,510 --> 01:43:47,900
she says

1544
01:44:33,290 --> 01:44:35,300
problem then

1545
01:44:37,010 --> 01:44:41,640
you me two

1546
01:44:43,120 --> 01:44:53,050
in here

1547
01:45:16,010 --> 01:45:19,070
so far

1548
01:45:19,070 --> 01:45:22,250
as apostle variable

1549
01:45:22,290 --> 01:45:23,420
OK so

1550
01:45:23,430 --> 01:45:25,170
the first two we talked about

1551
01:45:25,230 --> 01:45:31,750
were not strictly speaking probability density functions they were just probability distributions for discrete random

1552
01:45:31,750 --> 01:45:38,300
variables but now i want to turn to continuous random variables so we describe

1553
01:45:38,320 --> 01:45:41,990
using PDF probability density functions

1554
01:45:42,000 --> 01:45:44,010
so the first one i want to consider

1555
01:45:44,040 --> 01:45:45,930
is the uniform distribution

1556
01:45:45,950 --> 01:45:47,150
and the

1557
01:45:47,790 --> 01:45:52,510
the formula is given here but it's easy to graphically illustrate what that means you

1558
01:45:52,510 --> 01:45:56,840
simply consider two constants alpha and beta and you say that the uniform is a

1559
01:45:56,840 --> 01:46:01,180
constant value between those limits and zero everywhere else

1560
01:46:01,190 --> 01:46:03,180
and because the

1561
01:46:03,200 --> 01:46:06,310
distribution has to be normalized to have unit area

1562
01:46:06,320 --> 01:46:09,660
that means that the height of the curve has to be one minus

1563
01:46:09,670 --> 01:46:11,110
the width

1564
01:46:11,130 --> 01:46:12,890
so one divided by the

1565
01:46:12,910 --> 01:46:14,280
which is here

1566
01:46:14,280 --> 01:46:17,280
so the area under the thing is equal to one

1567
01:46:17,290 --> 01:46:21,560
and here because the model is so simple you could actually work out

1568
01:46:21,670 --> 01:46:23,840
the expectation value and and variance

1569
01:46:24,240 --> 01:46:29,060
by hand very easily so i've written in the way that title notes just before

1570
01:46:29,330 --> 01:46:33,200
so if you're looking at the lecture notes had you'll notice that square is missing

1571
01:46:33,200 --> 01:46:37,990
but it's obviously better myself squared

1572
01:46:38,800 --> 01:46:43,570
so what some applications of the uniform distribution it is easy to show

1573
01:46:43,590 --> 01:46:47,160
but if you consider a random variable x

1574
01:46:47,170 --> 01:46:53,180
the falls any can continuous pdf such that the cumulative distribution is capital f of

1575
01:46:54,800 --> 01:46:55,820
OK so so

1576
01:46:55,840 --> 01:47:00,390
that could be the cumulative corresponding to an exponential or calcium or anything you want

1577
01:47:00,540 --> 01:47:05,080
now with the that cumulative distribution simply as a function

1578
01:47:05,110 --> 01:47:11,190
of x and consider the random variable y equals capital f of x

1579
01:47:11,210 --> 01:47:13,040
that's a very

1580
01:47:13,060 --> 01:47:16,360
so i'm remarked yesterday that the function of a random variable

1581
01:47:16,380 --> 01:47:20,160
it's also a random variable and if i regret that is the random variable i

1582
01:47:20,160 --> 01:47:24,370
find it will be uniformly distributed between zero and one

1583
01:47:24,380 --> 01:47:28,240
that turns out to be very convenient property and we will return to that

1584
01:47:28,260 --> 01:47:32,450
in just a bit when we consider the monte carlo method

1585
01:47:32,520 --> 01:47:36,510
but there are so that's that's mathematical

1586
01:47:36,550 --> 01:47:41,520
property which is very important but there are also physical examples of the uniform distribution

1587
01:47:41,780 --> 01:47:46,160
of for example if you consider the decay of neutral pions

1588
01:47:46,170 --> 01:47:51,610
most of the time it decays into two photons if you have higher moving along

1589
01:47:51,620 --> 01:47:55,520
and in the case into the two photons and the energy of the pion will

1590
01:47:55,520 --> 01:47:56,400
be shared

1591
01:47:56,430 --> 01:48:00,870
between the two photons and the show will be different depending on the orientation of

1592
01:48:00,880 --> 01:48:03,160
the of the the k

1593
01:48:03,180 --> 01:48:06,790
so if you look at the distribution of energy

1594
01:48:06,840 --> 01:48:10,070
all the come out you find that it will be

1595
01:48:10,080 --> 01:48:15,070
distributed between a certain minimum and maximum values which i've written the formulas for here

1596
01:48:15,130 --> 01:48:17,530
and it will follow that a uniform distribution

1597
01:48:18,340 --> 01:48:20,450
the minimum and maximum

1598
01:48:20,460 --> 01:48:23,580
the data refers to the

1599
01:48:24,530 --> 01:48:27,390
of the of the pizer

1600
01:48:28,790 --> 01:48:29,820
so so

1601
01:48:29,840 --> 01:48:32,360
that's the uniform distribution

1602
01:48:32,370 --> 01:48:33,270
it's gone

1603
01:48:33,280 --> 01:48:35,280
here's another

1604
01:48:35,280 --> 01:48:36,590
very important

1605
01:48:36,620 --> 01:48:40,110
continuous pdf with the exponential distribution

1606
01:48:40,130 --> 01:48:44,300
it is defined by a formula that i have written here there is the exponential

1607
01:48:45,360 --> 01:48:50,650
as a function of this continuous variable x it is of course only positive

1608
01:48:50,660 --> 01:48:54,620
it contains the single parameter which are written here is the letter c

1609
01:48:54,660 --> 01:48:57,410
so there it is for different values of

1610
01:48:58,290 --> 01:49:01,680
if you look at the expectation value and variance we find that they are able

1611
01:49:01,690 --> 01:49:02,730
to succeed

1612
01:49:02,750 --> 01:49:05,550
and ixi square respectively

1613
01:49:06,370 --> 01:49:10,870
now i very often use this notation the use of a latin letter

1614
01:49:10,880 --> 01:49:17,320
for the random variable itself and sometimes for the corresponding expectation value i use the

1615
01:49:17,380 --> 01:49:19,060
the corresponding greek letters

1616
01:49:19,070 --> 01:49:24,410
right so the random variables x and its expectation value xxi

1617
01:49:24,420 --> 01:49:28,320
but one of the most important applications of the exponential distribution

1618
01:49:28,330 --> 01:49:33,180
in particle physics is to the decay time of an unstable particle

1619
01:49:33,190 --> 01:49:36,030
so the the proper decay time t

1620
01:49:36,040 --> 01:49:39,230
it is also described by an exponential distribution so therefore

1621
01:49:39,240 --> 01:49:43,040
for the expectation value i would use the corresponding greek letters tau

1622
01:49:43,050 --> 01:49:46,880
so we would like fifty with the parameter tau

1623
01:49:48,480 --> 01:49:51,180
the entire line corresponds to the mean

1624
01:49:51,250 --> 01:49:53,980
the lifetime of the particle

1625
01:49:53,990 --> 01:49:56,210
now the exponential distribution

1626
01:49:56,240 --> 01:49:58,960
has some very interesting properties one of them

1627
01:49:58,990 --> 01:50:01,620
is what is called lack of memory

1628
01:50:01,640 --> 01:50:04,130
and this can be expressed in the following way

1629
01:50:04,150 --> 01:50:08,400
if you have a collection of particles that are all present at some start time

1630
01:50:08,420 --> 01:50:10,160
t zero

1631
01:50:10,180 --> 01:50:11,810
and if you look at the

1632
01:50:11,840 --> 01:50:13,620
distribution of the quantity

1633
01:50:13,630 --> 01:50:17,760
t minus t zero where t is the decay time t is the time after

1634
01:50:17,760 --> 01:50:21,550
which the particle decays t zero start time

1635
01:50:21,570 --> 01:50:24,740
and this is the conditional probability given

1636
01:50:24,780 --> 01:50:27,170
the particles all existing

1637
01:50:27,190 --> 01:50:30,270
at this start time they had indicated beforehand

1638
01:50:30,610 --> 01:50:35,180
then you find that conditional probability is simply equal

1639
01:50:35,190 --> 01:50:37,910
to the original exponential distribution

1640
01:50:37,920 --> 01:50:40,230
independent of t zero

1641
01:50:40,250 --> 01:50:42,130
so that's what's called lack of memory

1642
01:50:42,130 --> 01:50:46,140
bias and some voxels very large bias and now what you can do is you

1643
01:50:46,140 --> 01:50:53,140
can use pattern recognition techniques to analyse the information contained subtle information contained in each

1644
01:50:53,140 --> 01:50:58,940
voxel but you can accumulate this information across a large number of voxels

1645
01:50:59,040 --> 01:51:00,630
this is just to show

1646
01:51:00,640 --> 01:51:06,610
project this is just side on the project work in magdeburg and leipzig with the

1647
01:51:06,610 --> 01:51:10,810
post-doc of mine so you can see here this is the human lgn so you

1648
01:51:10,810 --> 01:51:18,610
can get subcortical structures and what this shows is the subdivisions macmillan published subdivisions of

1649
01:51:18,610 --> 01:51:25,250
the LGN but you can obtain with high resolution high field imaging seven test flight

1650
01:51:25,260 --> 01:51:28,090
and you can see here

1651
01:51:28,100 --> 01:51:34,150
different types of voxels showing quite different types of parametric responses and there's a bimodal

1652
01:51:34,160 --> 01:51:39,420
distribution for example of the c fifty the value in the u i mean most

1653
01:51:39,440 --> 01:51:43,660
people i guess not visual so you might not know this but it's basically tells

1654
01:51:43,660 --> 01:51:48,850
you where the steep part of this curve here is that relates the response amplitude

1655
01:51:48,850 --> 01:51:51,060
to the contrast that you're stimulating with

1656
01:51:53,070 --> 01:51:57,240
the point is you can fit a parametric function to each voxel and then you

1657
01:51:57,240 --> 01:52:00,770
can for example do some kind of clustering and what you get out here is

1658
01:52:00,770 --> 01:52:04,840
you get magnum part of voxels out of this with very high resolution one point

1659
01:52:04,840 --> 01:52:06,980
one millimetre

1660
01:52:07,080 --> 01:52:11,420
so we can get this fine grained information but the information each voxel is rather

1661
01:52:11,420 --> 01:52:15,930
unreliable for example you see here that the information also looks rather noisy so these

1662
01:52:15,930 --> 01:52:23,490
patterns and all that kind of you can't reliably identify each individual voxel but you

1663
01:52:23,490 --> 01:52:28,050
can roughly get the topography and the topography is reproducible

1664
01:52:28,180 --> 01:52:30,200
so how do we analyse this

1665
01:52:30,210 --> 01:52:36,260
so what we do is we take root this is just an illustrative example so

1666
01:52:36,260 --> 01:52:40,230
you are interested in the pattern of three by three voxels here then what you

1667
01:52:40,230 --> 01:52:46,040
do is you turn this into a pattern vector were ultimately these different entries in

1668
01:52:46,040 --> 01:52:47,000
this vector

1669
01:52:47,050 --> 01:52:52,690
contains the from my activity in each of these works all positions here and then

1670
01:52:52,690 --> 01:52:57,400
use this as an index to appoint in an n dimensional space where the n

1671
01:52:57,420 --> 01:53:02,440
dimensions are just the number of voxels that you're introducing into analysis and you can

1672
01:53:02,440 --> 01:53:06,480
illustrate this if you just look at two dimensions so you're stimulating with two different

1673
01:53:06,480 --> 01:53:11,900
stimuli and you take the first two dimensions which you can use to just get

1674
01:53:12,120 --> 01:53:19,740
the euclidean x y coordinate system and the cartesian coordinates for a cartesian coordinate system

1675
01:53:19,740 --> 01:53:28,170
here and what you can see here is the different possible cases in which this

1676
01:53:28,170 --> 01:53:31,270
information can be distributed across these two voxels

1677
01:53:31,280 --> 01:53:33,840
so what you can see on the left is the case

1678
01:53:33,860 --> 01:53:39,070
where blue is the image two and read different measurements of image one

1679
01:53:39,090 --> 01:53:42,390
and this shows the activity values into voxels

1680
01:53:42,390 --> 01:53:46,250
and you can see here that get a high value invokes the one when you

1681
01:53:46,250 --> 01:53:52,110
showing image two but he had low value voxel two and for the other type

1682
01:53:52,110 --> 01:53:54,650
of image it's the other way round

1683
01:53:54,670 --> 01:53:57,520
now this is the case where you could separate

1684
01:53:57,570 --> 01:54:03,530
you could classify based on each of these individual voxels you don't need to voxels

1685
01:54:03,540 --> 01:54:08,170
to classify here you can just basically put the criterion somewhere here and then you

1686
01:54:08,170 --> 01:54:13,150
can separate the blue and the red simply based on books one or voxel two

1687
01:54:14,130 --> 01:54:16,730
now but they are difficult cases

1688
01:54:16,770 --> 01:54:19,710
and these more difficult cases you can see here

1689
01:54:19,720 --> 01:54:21,350
for example the second

1690
01:54:22,080 --> 01:54:26,840
it is where you don't have any separable information in

1691
01:54:26,880 --> 01:54:28,080
each voxel

1692
01:54:28,110 --> 01:54:34,510
but the discriminative information is carried by the covariation pattern between the two voxels

1693
01:54:34,520 --> 01:54:37,970
so you can see here if you just know voxel one or you just know

1694
01:54:37,980 --> 01:54:38,960
voxel two

1695
01:54:38,960 --> 01:54:43,300
you can hardly separate these two but if you know voxel one voxel two together

1696
01:54:43,300 --> 01:54:46,410
you can separate these very well you can put the decision boundary in there and

1697
01:54:46,410 --> 01:54:48,470
you can separate between these two

1698
01:54:48,650 --> 01:54:53,410
and this is very important for this is also the reason why

1699
01:54:53,410 --> 01:54:55,950
if you want to do multivariate according

1700
01:54:55,970 --> 01:55:03,540
you can't do this properly if you don't have simultaneous measurements for example of multiple

1701
01:55:03,750 --> 01:55:09,460
cells when you're doing invasive recordings because the covariation pattern is something that is incredibly

1702
01:55:09,460 --> 01:55:13,500
important and this covariation pattern if you first measure one cell

1703
01:55:13,510 --> 01:55:18,080
and then you measure another cell and then third so anyone use decoding techniques this

1704
01:55:18,080 --> 01:55:22,750
covariation pattern is lost you lose this fine grained cooperation and then you can apply

1705
01:55:22,750 --> 01:55:26,940
the coding techniques anymore you can get up to ninety percent accuracy

1706
01:55:26,940 --> 01:55:29,700
now this this post is like a virus

1707
01:55:29,820 --> 01:55:32,070
all that happens is that

1708
01:55:32,100 --> 01:55:36,120
particular blog we will infect other bloggers other blogs now

1709
01:55:36,130 --> 01:55:38,220
link to the particular post

1710
01:55:38,230 --> 01:55:42,710
the block can compute in the next time step so that we can have claimed

1711
01:55:42,710 --> 01:55:45,270
that you can have a case the

1712
01:55:45,280 --> 01:55:49,050
two blogs just goalkeeping influence each other and we would get like a long chain

1713
01:55:49,050 --> 01:55:50,500
of of influence

1714
01:55:50,550 --> 01:55:54,520
and so if you go and see a more like this and and compare it

1715
01:55:54,530 --> 01:55:56,860
to reality and this is what we do so

1716
01:55:58,700 --> 01:56:04,840
the particular details are not important here but what i'm showing here is some distributions

1717
01:56:04,840 --> 01:56:07,620
or some because some some

1718
01:56:07,630 --> 01:56:11,800
statistical properties of the cascades of for example would be a cascade size distribution the

1719
01:56:11,800 --> 01:56:12,770
dark side of the day

1720
01:56:13,160 --> 01:56:14,580
and the line

1721
01:56:14,590 --> 01:56:18,950
is what is produced by the more or for example i can go and we

1722
01:56:18,970 --> 01:56:23,550
take the star cascade we have set central but then has a bunch of nodes

1723
01:56:24,300 --> 01:56:28,600
and again we can compare the results from the model with the results from the

1724
01:56:28,600 --> 01:56:33,070
through the results and you can see it's it's very close to i can only

1725
01:56:33,090 --> 01:56:35,320
be changed and again measure the

1726
01:56:35,320 --> 01:56:39,690
the size distribution of chains and compare it to what more gives me

1727
01:56:39,710 --> 01:56:40,960
and also

1728
01:56:40,990 --> 01:56:44,980
if you remember from previous slides these are the most frequent cascades the

1729
01:56:44,990 --> 01:56:48,820
one is in there and they are very similar to what we saw before the

1730
01:56:50,760 --> 01:56:56,270
the larger cascades the the less frequented his and also there are some nice topological

1731
01:56:56,270 --> 01:56:57,730
things going

1732
01:57:01,210 --> 01:57:10,760
this is in relation this simulation

1733
01:57:23,880 --> 01:57:32,260
o it ensure it was it was the result of iteration so i guess would

1734
01:57:32,260 --> 01:57:34,010
be saying that i was overfitting

1735
01:57:34,070 --> 01:57:38,290
manually overfitting four

1736
01:57:39,060 --> 01:57:43,450
the developing any money to use an iteration tried you have many intuitions that you

1737
01:57:43,450 --> 01:57:55,320
want to exploit

1738
01:57:55,380 --> 01:57:58,780
o there's a lot of things missing here and there is no topicality that i

1739
01:57:58,780 --> 01:58:03,040
would be following there is for example here i'm just picking the end of the

1740
01:58:03,040 --> 01:58:07,970
blog that starts the thing uniformly at random but if you go and ask what

1741
01:58:08,000 --> 01:58:12,030
the distribution of the number of posts per blog again identity some heavy tailed distributions

1742
01:58:12,030 --> 01:58:15,880
like most of the blogs have like one post and there are few blogs have

1743
01:58:15,880 --> 01:58:17,910
a huge number for example

1744
01:58:17,950 --> 01:58:18,630
that are

1745
01:58:18,690 --> 01:58:21,880
and this is not but

1746
01:58:24,410 --> 01:58:27,510
now i want to move to the last part of the park and here i

1747
01:58:27,820 --> 01:58:29,390
skip if you think so

1748
01:58:29,410 --> 01:58:30,850
we saw that we saw

1749
01:58:30,850 --> 01:58:33,160
what are the models we saw how

1750
01:58:33,160 --> 01:58:37,700
you can go into the data and measured the cascades and find them so now

1751
01:58:37,700 --> 01:58:38,920
the question is

1752
01:58:38,950 --> 01:58:44,420
given this cascades how can i take quick so

1753
01:58:44,470 --> 01:58:47,940
what we are doing is a network and a set of cascades or set of

1754
01:58:47,940 --> 01:58:53,040
propagation patterns and the question is which nodes should be monitored to detect this cascades

1755
01:58:54,320 --> 01:58:57,890
and here are two motivating example so for example if you if we have a

1756
01:58:57,940 --> 01:59:02,200
water distribution network of city so we have these houses and let's say that there

1757
01:59:02,200 --> 01:59:08,440
is some was introduced this particular not now the infected water spread and infect the

1758
01:59:08,440 --> 01:59:10,030
rest of the network

1759
01:59:10,040 --> 01:59:12,600
and what we would like to do is we would like to put

1760
01:59:12,660 --> 01:59:14,730
a sensor at some locations

1761
01:59:14,750 --> 01:59:19,630
so that if if there is a there is an infection that spreads

1762
01:59:19,640 --> 01:59:25,080
when it hits over-sensitive we will raise an alarm rate we detect the of

1763
01:59:25,130 --> 01:59:29,390
and the question is where should we put the the sensors detect this breaks as

1764
01:59:29,390 --> 01:59:30,980
effectively as possible

1765
01:59:31,030 --> 01:59:34,920
and effectively or efficiently can mean many different things

1766
01:59:34,940 --> 01:59:38,380
similarly for blocks right you can say

1767
01:59:38,390 --> 01:59:42,510
i have i have blogs as a child before i have blogs that have posts

1768
01:59:42,540 --> 01:59:43,930
and i had this

1769
01:59:43,940 --> 01:59:48,590
the hyperlinks between posts so i can track how information propagates through the network

1770
01:59:48,640 --> 01:59:51,570
so the question i want to answer is

1771
01:59:51,580 --> 01:59:54,810
which blogs should we read to be most up to date right if there is

1772
01:59:54,810 --> 01:59:58,690
an important story going on i would like to to know about it as quickly

1773
01:59:58,690 --> 02:00:01,650
as possible so for example if i go back here

1774
02:00:01,670 --> 02:00:06,430
you could say well i mean these blocks it is this the the

1775
02:00:06,440 --> 02:00:10,880
this excited to be a big blogs because six posts so i have to read

1776
02:00:10,880 --> 02:00:15,280
a lot of posts but i i capture all the stories right i would get

1777
02:00:15,280 --> 02:00:18,560
to know the story i would get to know the story and i would get

1778
02:00:18,560 --> 02:00:22,070
to know the story but i would get to know them very low

1779
02:00:22,750 --> 02:00:27,190
the solution to the problem what could possibly be that i go and i read

1780
02:00:27,190 --> 02:00:32,210
block one and block c this would mean i i need to read posts

1781
02:00:32,260 --> 02:00:36,780
but i can get to know all the stories all the cascade and i captured

1782
02:00:36,780 --> 02:00:40,120
the very soon right as soon as they have i know of

1783
02:00:40,170 --> 02:00:45,880
so these are the types of trade offs between the try to explore this

1784
02:00:45,880 --> 02:00:47,820
line of work

1785
02:00:49,460 --> 02:00:53,860
what is the problem right we have some dynamic process spreading over the network and

1786
02:00:53,860 --> 02:00:57,270
we want to select a set of nodes to detect this process as effectively as

1787
02:00:57,270 --> 02:01:00,080
possible and before i was

1788
02:01:00,130 --> 02:01:04,200
then you about finding the most influential set of nodes right this is sort of

1789
02:01:04,210 --> 02:01:09,420
similar but different right then are trying to find people that but that are most

1790
02:01:09,420 --> 02:01:13,620
likely to influence others here we are trying to find people that are most likely

1791
02:01:13,630 --> 02:01:17,010
to get influence right it's like sort of but i want to find people who

1792
02:01:17,010 --> 02:01:20,420
are most likely to get it so that if there is an epidemic and they

1793
02:01:20,420 --> 02:01:24,760
become i know something is going on right and there are numerous applications of extracted

1794
02:01:24,770 --> 02:01:27,650
you can use it in epidemics network security and so on

1795
02:01:27,740 --> 02:01:31,960
and there are two parts to the problem the first one is the reward to

1796
02:01:31,960 --> 02:01:36,210
the objective function right so what is the function what is the criteria you want

1797
02:01:36,210 --> 02:01:40,580
to minimize or maximize for example in the water distribution you could say i want

1798
02:01:40,580 --> 02:01:44,760
to minimize time to detection whenever there is an academic i want to detect it

1799
02:01:44,760 --> 02:01:46,760
as quickly as possible so

1800
02:01:46,810 --> 02:01:52,140
the amount of time between between the introduction detection then the second thing that is

1801
02:01:52,140 --> 02:01:59,070
different you could say i want to maximize the number of detected outbreaks so whenever

1802
02:01:59,120 --> 02:02:01,940
the outbreak happens i want to i want to

1803
02:02:01,960 --> 02:02:04,470
be able to detect it i don't care howley

1804
02:02:04,570 --> 02:02:06,940
and so on i just want to detect

1805
02:02:06,990 --> 02:02:10,320
and then the the the last one is we want to minimize the number of

1806
02:02:10,320 --> 02:02:12,270
infected people that's so

1807
02:02:12,300 --> 02:02:15,760
here we we have basically paying more for the gulf exactly what to say

1808
02:02:15,770 --> 02:02:20,380
between the time the album was was introduced in the private detective on the smaller

1809
02:02:20,380 --> 02:02:24,860
number of people to drink was poisoned water or something so this is the first

1810
02:02:24,860 --> 02:02:28,530
part of the problem in the second part of the problem is

1811
02:02:28,550 --> 02:02:33,690
the cost is location dependent meaning that there is some cost of to read off

1812
02:02:33,690 --> 02:02:37,870
reading a blog so big reading a bigger blogs is more time consuming than leading

1813
02:02:37,870 --> 02:02:39,320
a small or

1814
02:02:39,320 --> 02:02:40,510
placing a sensor

1815
02:02:40,520 --> 02:02:44,450
at some remote rural location may be more expensive than placing the sounds of summer

1816
02:02:44,450 --> 02:02:48,070
in the city which is easily accessible or or vice versa so that there is

