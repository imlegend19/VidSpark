1
00:00:00,000 --> 00:00:01,810
technique for storing information

2
00:00:02,260 --> 00:00:03,940
so he was very optimistic

3
00:00:05,480 --> 00:00:06,430
computers even

4
00:00:07,280 --> 00:00:11,940
obviously scalably matches the computer of nineteen fifty one

5
00:00:13,150 --> 00:00:14,340
would be able to

6
00:00:15,260 --> 00:00:18,170
achieve quality with human beings

7
00:00:18,720 --> 00:00:19,930
well we can see that

8
00:00:20,630 --> 00:00:21,650
hasn't been achieved

9
00:00:23,610 --> 00:00:24,520
that it well

10
00:00:24,950 --> 00:00:25,790
during tests

11
00:00:26,280 --> 00:00:28,230
where you compare computers human beings

12
00:00:29,520 --> 00:00:33,250
so far i haven't got terribly far together there was a

13
00:00:33,320 --> 00:00:37,860
for the one of the most successful was recently i just heard but nevertheless they still

14
00:00:38,760 --> 00:00:40,060
don't uh

15
00:00:40,730 --> 00:00:43,440
they aren't able to imitate human beings convincingly

16
00:00:46,400 --> 00:00:51,100
and other point it's worth making because there is notion when you come to quantum

17
00:00:51,150 --> 00:00:53,210
mechanics you see i think during had the idea

18
00:00:53,720 --> 00:00:55,460
there might be random elements

19
00:00:56,190 --> 00:01:00,210
the quantum mechanics introduces and therefore it wouldn't be exactly

20
00:01:00,650 --> 00:01:03,800
the turing machine it will be something like a turing machine with a randomizer

21
00:01:04,210 --> 00:01:05,300
but the point is can hear

22
00:01:05,980 --> 00:01:10,180
it's not difficult to design machines behavior appears quite random

23
00:01:10,610 --> 00:01:16,360
to anyone who doesn't know the details of the construction so you can and this is a standard technique

24
00:01:16,870 --> 00:01:17,460
to produce

25
00:01:18,160 --> 00:01:19,930
random numbers if you like

26
00:01:20,550 --> 00:01:21,400
random outputs

27
00:01:22,060 --> 00:01:25,920
which of these not strictly speaking random because they are produced by some

28
00:01:26,470 --> 00:01:28,360
complicated computational procedure

29
00:01:28,900 --> 00:01:33,490
and if you don't have produced then they are effectively random so i think it is more saying here

30
00:01:33,900 --> 00:01:37,620
the randomness doesn't really make any difference you can pretty well simulate randomness

31
00:01:38,480 --> 00:01:39,290
okay so that's they

32
00:01:39,950 --> 00:01:41,240
there's various quotes

33
00:01:42,540 --> 00:01:43,630
i want to talk about

34
00:01:44,070 --> 00:01:44,590
what i think

35
00:01:45,060 --> 00:01:46,070
is the argument

36
00:01:51,080 --> 00:01:54,580
really nature and think he might need to go beyond the idea

37
00:01:55,090 --> 00:01:56,970
of a universal turing machine

38
00:01:58,600 --> 00:02:01,810
because if this thing zones i won't say great deal about ordinals

39
00:02:02,310 --> 00:02:03,900
i mentioned briefly at one point

40
00:02:05,020 --> 00:02:05,920
roughly speaking

41
00:02:06,360 --> 00:02:10,410
it's iterating this procedure which i'm going to indicate in just a moment

42
00:02:11,650 --> 00:02:15,500
that's the would those articles will play a role in what i was later on

43
00:02:16,850 --> 00:02:17,970
so while during

44
00:02:20,530 --> 00:02:22,120
actually what i'm worried if you like

45
00:02:22,670 --> 00:02:24,900
well where during this words is another question

46
00:02:25,500 --> 00:02:29,820
but it's a question about the perception of mathematical truth how do we judge

47
00:02:30,500 --> 00:02:32,130
whether a mathematical statement

48
00:02:32,590 --> 00:02:33,870
it is true or false

49
00:02:35,230 --> 00:02:35,700
and do

50
00:02:36,310 --> 00:02:38,250
i'm going to restrict my attention to

51
00:02:39,580 --> 00:02:41,100
what we might consider the be very

52
00:02:41,600 --> 00:02:47,450
straightforward statements these are statements about natural numbers are not geometry for instance we might

53
00:02:47,450 --> 00:02:49,460
worry about what kind of geometry are talking about

54
00:02:50,010 --> 00:02:53,280
i'm only going to talk about statements which are very very clear cut

55
00:02:53,870 --> 00:02:55,120
and they just things about

56
00:02:55,710 --> 00:02:59,390
the natural numbers zero one two three four so the non-negative whole numbers

57
00:03:02,230 --> 00:03:06,320
he idea that statement here is that the perception mathematical truth

58
00:03:06,840 --> 00:03:10,020
cannot be reduced to a set of mechanical rules

59
00:03:11,550 --> 00:03:12,470
this comes from

60
00:03:13,390 --> 00:03:14,730
the famous theorem due to go out

61
00:03:15,510 --> 00:03:17,830
hand during most certainly very familiar with this

62
00:03:18,330 --> 00:03:19,770
it features very much in his

63
00:03:21,730 --> 00:03:23,280
thirty six paper the original one

64
00:03:24,180 --> 00:03:27,540
because he was very much concerned with things that computers could do if you like

65
00:03:27,830 --> 00:03:30,580
that's with probability with trying to do and so say

66
00:03:31,100 --> 00:03:31,960
what you could do

67
00:03:32,340 --> 00:03:33,540
by mechanical procedure

68
00:03:34,040 --> 00:03:37,870
we had to define what is meant by a mechanical procedure and then show that

69
00:03:37,870 --> 00:03:40,930
there were certain things that you can't do with a mechanical procedure

70
00:03:41,530 --> 00:03:43,690
well i'm phrase ghost theorem

71
00:03:44,140 --> 00:03:46,810
in a way which is not normally phrase this is

72
00:03:47,740 --> 00:03:49,060
a perfectly correct

73
00:03:49,520 --> 00:03:50,580
deduction from his

74
00:03:51,120 --> 00:03:51,840
his theorem

75
00:03:52,650 --> 00:03:54,690
tells us that for any mathematical theorem

76
00:03:55,450 --> 00:03:58,020
this kind of thing i'm talking about here with natural numbers and so on

77
00:03:58,650 --> 00:03:59,440
any momentum

78
00:04:00,180 --> 00:04:00,510
for any

79
00:04:01,920 --> 00:04:03,830
it tells us for any set a mechanical

80
00:04:04,300 --> 00:04:05,710
theorem proving rules

81
00:04:06,580 --> 00:04:10,500
and when i say mechanical theorem very proving rules i mean you have a set

82
00:04:10,500 --> 00:04:15,910
of procedures which can be checked by a computer that's the crucial point u if

83
00:04:15,910 --> 00:04:17,180
you follow these rules

84
00:04:18,130 --> 00:04:19,680
hand you come to the conclusion

85
00:04:21,110 --> 00:04:24,500
it prove some mathematical statement that can be talking about

86
00:04:27,070 --> 00:04:28,920
could lead to prove can be

87
00:04:29,320 --> 00:04:30,540
checked by computer

88
00:04:32,300 --> 00:04:33,720
that's the kind will talking about

89
00:04:34,530 --> 00:04:38,460
then what you can construct is a particular mathematical statement

90
00:04:38,940 --> 00:04:40,280
which kernel shows you how to do

91
00:04:41,270 --> 00:04:42,530
and i'm calling the jaafar

92
00:04:43,810 --> 00:04:49,090
which if we trust the rules that we believe in the validity of the rules that say if we believe

93
00:04:49,590 --> 00:04:52,730
anything we derive using the rules must be true

94
00:04:53,720 --> 00:04:55,010
if you trust the rules

95
00:04:55,540 --> 00:05:00,050
then you must also believe that this particular statement geva is true

96
00:05:01,700 --> 00:05:07,560
it cannot be proved using the rules that's a very striking remarkable thing because it's

97
00:05:07,560 --> 00:05:08,850
what is what is telling us

98
00:05:09,290 --> 00:05:10,780
is that whatever these rules are

99
00:05:11,520 --> 00:05:14,440
if there are rules whose consequences we believe in

100
00:05:15,290 --> 00:05:19,730
then that's belief about trust extends beyond the rules themselves

101
00:05:20,520 --> 00:05:20,940
and that's

102
00:05:21,810 --> 00:05:22,400
you have to know

103
00:05:22,960 --> 00:05:28,210
the rules are trustworthy and that's belief that they are trustworthy enables you to extend

104
00:05:28,210 --> 00:05:33,830
if after sort of fifty-five minutes you sort of feel uncomfortable standing we do remind you that it is being recorded

105
00:05:34,110 --> 00:05:38,100
and can be looked on the web afterwards and I said for tomorrow we definitely

106
00:05:38,100 --> 00:05:43,420
get to try to get a bigger room

107
00:05:43,510 --> 00:05:50,940
ok thank you it's a pleasure to be here in sunny Geneva actually it's warm Geneva

108
00:05:51,390 --> 00:05:57,050
compared to Chicago it's been really cold in Chicago and in fact something really remarkable

109
00:05:57,070 --> 00:06:01,940
was seen last week there was a Chicago politician walking around with his hands in his

110
00:06:01,940 --> 00:06:10,610
own pockets so the program for this week is today i'm going to give

111
00:06:10,690 --> 00:06:19,020
an introduction concentrating more on observational astronomy and the basis of this standard model

112
00:06:19,020 --> 00:06:22,480
that leads us to the idea that there is dark matter

113
00:06:22,520 --> 00:06:24,520
and dark energy

114
00:06:24,570 --> 00:06:31,380
it is a talk about the universe observed from the viewpoint of a theorist

115
00:06:32,010 --> 00:06:37,210
then tomorrow I would like to talk about inflation because this also plays a role

116
00:06:37,210 --> 00:06:41,910
in the entire picture of the standard model of cosmology

117
00:06:42,210 --> 00:06:46,020
then on Wednesday I will concentrate on dark matter

118
00:06:46,070 --> 00:06:51,800
and Thursday concentrate on dark energy

119
00:06:51,850 --> 00:06:58,410
now what wasn't mentioned in the introduction is that I'm a cosmologist

120
00:06:58,570 --> 00:07:06,010
and I'm proud of it now one of the things that a cosmologist does is to

121
00:07:06,010 --> 00:07:07,270
look out

122
00:07:07,300 --> 00:07:13,180
In in the universe so not only to see what's there and how it moves and a

123
00:07:13,180 --> 00:07:19,660
description of it but to look beyond that and try to understand why

124
00:07:19,710 --> 00:07:23,740
it behaves as it does so we want to

125
00:07:23,990 --> 00:07:29,850
in ancient times it was you know mechanical gears crystalline spheres

126
00:07:29,880 --> 00:07:36,130
but today at least in the civilized world we understand the the workings of the

127
00:07:36,130 --> 00:07:41,010
universe on the basis of the laws of physics and of course particle physics plays

128
00:07:41,010 --> 00:07:46,310
a large role in that and that is what I would like to concentrate on

129
00:07:46,410 --> 00:07:48,860
now in part

130
00:07:48,910 --> 00:07:50,510
To understand

131
00:07:50,540 --> 00:07:55,640
this working of the universe we construct a cosmological model

132
00:07:55,690 --> 00:08:00,350
just as in particle physics there's a cosmological sti there is a

133
00:08:00,380 --> 00:08:02,790
particle physics standard model

134
00:08:02,850 --> 00:08:05,780
in cosmology there is a cosmological

135
00:08:05,820 --> 00:08:07,160
Standard model

136
00:08:07,200 --> 00:08:10,070
and let me give you the

137
00:08:10,590 --> 00:08:18,360
relevant aspects of this standard cosmological model it goes by the name of lambda CDM

138
00:08:18,410 --> 00:08:23,630
lambda stands for Einstein's cosmological constant or dark energy

139
00:08:23,660 --> 00:08:29,880
and cold dark matter stands for what CDM stands for cold dark matter

140
00:08:30,380 --> 00:08:34,790
one of the aspects of this cosmological standard model

141
00:08:34,970 --> 00:08:40,760
is the present constitution of the universe what is involved in the universe today what is it made

142
00:08:40,760 --> 00:08:48,540
of and this is the constitution of the universe according to the cosmological standard model

143
00:08:48,590 --> 00:08:55,440
although the early universe was radiation dominated at present the radiation in the universe

144
00:08:55,480 --> 00:09:01,590
contributes only point zero zero five per cent of the mass energy of the universe and

145
00:09:01,590 --> 00:09:08,130
almost all of the radiation in the universe is in the back the cosmic background radiation

146
00:09:08,410 --> 00:09:11,640
this pizza

147
00:09:11,690 --> 00:09:16,660
chart also illustrates why chemistry is not important

148
00:09:17,250 --> 00:09:24,180
the chemical elements elements other than hydrogen and helium only make up about point zero

149
00:09:24,180 --> 00:09:28,160
two five per cent of the mass energy of the universe

150
00:09:28,290 --> 00:09:34,420
there is much more mass energy in the universe in the form of neutrinos today

151
00:09:34,420 --> 00:09:36,040
about point one per cent

152
00:09:36,630 --> 00:09:42,480
point one seven point two per cent stars make up about less than half a per

153
00:09:42,480 --> 00:09:45,540
cent of the mass energy of the universe

154
00:09:46,120 --> 00:09:50,000
so there is most of the baryons are not in the form of

155
00:09:50,000 --> 00:09:52,970
stars but are in a hot gas

156
00:09:53,000 --> 00:09:56,000
that is to be found in clusters of galaxies

157
00:09:56,910 --> 00:10:01,000
so five per cent of the universe are the things that are observed and

158
00:10:01,000 --> 00:10:04,120
we understand and we know about

159
00:10:04,160 --> 00:10:09,230
but in this cosmological standard model there's ninety-five per cent of the universe that's

160
00:10:09,230 --> 00:10:10,330
a mystery

161
00:10:10,360 --> 00:10:15,770
and it's thought that twenty-five per cent of the current energy density of the universe

162
00:10:15,770 --> 00:10:21,450
is in the form of some type of cold dark matter and the remaining seventy

163
00:10:21,450 --> 00:10:26,600
per cent above is in dark energy or a cosmological constant

164
00:10:26,650 --> 00:10:32,740
this is the constitution of the universe in the standard cosmological model

165
00:10:33,300 --> 00:10:36,410
also in the standard cosmological model

166
00:10:36,450 --> 00:10:38,000
there is the need

167
00:10:38,040 --> 00:10:39,060
for some

168
00:10:39,090 --> 00:10:42,210
primordial sea perturbations

169
00:10:42,240 --> 00:10:47,920
that grow to become galaxies and large-scale structure that we see in the universe

170
00:10:47,970 --> 00:10:48,770
In this

171
00:10:48,800 --> 00:10:53,650
is in this model thought to arise from the very early universe

172
00:10:53,680 --> 00:10:56,360
In the epic known as inflation

173
00:10:56,470 --> 00:11:02,530
finally I won't talk about this in these lectures because I'm only giving four

174
00:11:02,560 --> 00:11:07,240
but the standard model has to have some idea or theory of the origin of

175
00:11:07,240 --> 00:11:13,770
the asymmetry between matter and anti-matter baryo Genesis or lepto Genesis

176
00:11:14,090 --> 00:11:19,590
and the relevant thing for high-energy physics is that cold dark matter

177
00:11:19,620 --> 00:11:20,890
dark energy

178
00:11:20,920 --> 00:11:27,730
inflationare inflation in baryo and lepto Genesis seem to require physics beyond the standard model of

179
00:11:27,730 --> 00:11:29,740
particle physics

180
00:11:29,790 --> 00:11:33,980
so this is in twenty five words or less

181
00:11:34,030 --> 00:11:38,850
the description of the standard cosmological model

182
00:11:38,890 --> 00:11:43,500
now when I go around giving public lectures and I talk to them about scientific

183
00:11:43,500 --> 00:11:47,970
models whether it is the standard model of particle physics

184
00:11:48,000 --> 00:11:51,040
or the Standard Model od cosmology

185
00:11:51,180 --> 00:11:53,410
I think that the general public

186
00:11:53,450 --> 00:11:58,530
including our colleagues at universities in fields other than science

187
00:11:58,660 --> 00:12:00,350
do not really know

188
00:12:00,390 --> 00:12:02,970
what we mean by a model

189
00:12:03,630 --> 00:12:10,210
for instance there was a very well-known economist in the United States who wrote

190
00:12:10,210 --> 00:12:16,080
about models and he said the construction of a model consist of snatching from the enormous and

191
00:12:16,080 --> 00:12:18,560
complex massive facts called reality

192
00:12:19,330 --> 00:12:22,540
a few simple easily managed key points

193
00:12:22,590 --> 00:12:28,160
which when put together in some cunning way becomes for certain purposes

194
00:12:28,210 --> 00:12:36,830
a substitute for reality itself now I don't think as scientists we view a model whether it is

195
00:12:36,900 --> 00:12:43,330
the Standard Model of particle physics of the standard model of cosmology as a substitute

196
00:12:43,360 --> 00:12:47,210
for reality itself we view it as a description

197
00:12:47,230 --> 00:12:49,090
of reality itself

198
00:12:49,530 --> 00:12:55,290
and I think when we talk to the public we have to make that that idea clear

199
00:12:57,560 --> 00:12:59,500
we want more

200
00:12:59,540 --> 00:13:03,800
from a model than be able to describe what we see

201
00:13:03,850 --> 00:13:05,910
and account for the data

202
00:13:05,950 --> 00:13:10,710
there is a long history in astronomy of introducing things that will allow us to

203
00:13:10,710 --> 00:13:12,470
account for the data

204
00:13:12,500 --> 00:13:14,660
these were known as epicycles

205
00:13:14,790 --> 00:13:19,270
and they were in of course the model of Ptolemy in the Almagest

206
00:13:19,300 --> 00:13:24,300
and they were also many more epicycles in Copernicus's book

207
00:13:24,300 --> 00:13:28,680
you know and also this represents the trial solution to the segmentation problem so this

208
00:13:28,680 --> 00:13:33,450
says x two is in the background and x one is in the foreground

209
00:13:33,490 --> 00:13:34,330
and so

210
00:13:34,330 --> 00:13:36,570
the cost of that configuration

211
00:13:36,570 --> 00:13:38,450
apparently is thirteen units

212
00:13:38,470 --> 00:13:41,760
now is this the lowest-cost that we could get in this graph

213
00:13:41,760 --> 00:13:45,510
to do that we would have to wriggle around with this red curve and try

214
00:13:45,510 --> 00:13:50,470
all possible configurations one across here one across here one going to the negative slope

215
00:13:50,640 --> 00:13:53,580
and that would be actually all of them for this particular network

216
00:13:55,490 --> 00:13:56,390
but of course

217
00:13:56,390 --> 00:14:00,510
you know how the number of cuts grows exponentially with the number of nodes which

218
00:14:00,510 --> 00:14:03,620
is one of the reasons why you know if anybody has smart you know the

219
00:14:03,660 --> 00:14:08,470
naive algorithm for this problem will be exponentially with doing exponential amount of work so

220
00:14:08,870 --> 00:14:13,310
this is really worthwhile if anybody have something better than a naive algorithm

221
00:14:13,330 --> 00:14:14,200
OK so now

222
00:14:14,200 --> 00:14:16,850
let's see what would you like to simulate

223
00:14:16,850 --> 00:14:21,050
a flow through this through the network by the way one thing i forgot to

224
00:14:21,050 --> 00:14:22,370
say the beginning

225
00:14:22,430 --> 00:14:24,430
earlier on was that

226
00:14:24,450 --> 00:14:26,120
it's all very well

227
00:14:26,120 --> 00:14:27,220
taking this

228
00:14:27,240 --> 00:14:29,740
cut and wriggling around to see which

229
00:14:29,760 --> 00:14:33,640
cut gives the minimum cost but was that we wanted to know does anybody want

230
00:14:33,640 --> 00:14:36,140
to know the minimum cost

231
00:14:36,160 --> 00:14:39,390
i mean if i tell you the minimum cost is is eight units will be

232
00:14:41,870 --> 00:14:44,760
they will help you know with the computer graphics problem with it i mean we

233
00:14:44,760 --> 00:14:48,510
didn't want was we want to know not the actual value of the minimum cost

234
00:14:50,390 --> 00:14:54,410
the big so we we actually need to know you know we've solved this for

235
00:14:54,410 --> 00:15:00,100
this minimum cost we better we better also know how to recover the configuration that

236
00:15:00,760 --> 00:15:05,120
that gives the minimum cost and

237
00:15:05,140 --> 00:15:09,430
so in other words we when we push and also when we're pushing for flow

238
00:15:09,430 --> 00:15:13,640
through the network we need to know which are the arcs with which this minimum

239
00:15:13,640 --> 00:15:18,140
cut will pass and it turns out this is part of the full focus on

240
00:15:19,970 --> 00:15:25,800
the cut passes through exactly those pipelines in the network which is saturated

241
00:15:25,800 --> 00:15:29,800
so all you have to do with the end is to read out which

242
00:15:29,850 --> 00:15:34,740
pipelines with the final flow that to achieve have reached their maximum capacity and the

243
00:15:35,470 --> 00:15:37,200
it passes through each other

244
00:15:37,240 --> 00:15:39,660
we will have to do with the end

245
00:15:39,720 --> 00:15:41,850
OK so now we're going to stop pushing flow

246
00:15:41,850 --> 00:15:42,890
the black

247
00:15:42,890 --> 00:15:45,570
figures are capacities

248
00:15:46,200 --> 00:15:49,330
i think the best is the red figures are flow

249
00:15:49,720 --> 00:15:54,300
initially we start with zero flow and so actually what you have to do is

250
00:15:54,490 --> 00:15:59,640
kind of search problem building a search tree starting from the source here searching for

251
00:15:59,640 --> 00:16:05,800
unsaturated box so the beginning it's not very difficult to find unsaturated ox you imagine

252
00:16:05,800 --> 00:16:11,260
starting a breadth first search for factory quarks you'd quickly come across that one that's

253
00:16:11,260 --> 00:16:13,450
a and you see

254
00:16:13,510 --> 00:16:14,600
OK how much

255
00:16:15,530 --> 00:16:16,200
can we

256
00:16:16,200 --> 00:16:19,950
pushed down here well it might be as much as two units

257
00:16:20,050 --> 00:16:24,810
and that will be fine provided and this is the next step in the search

258
00:16:25,080 --> 00:16:29,700
we can find arcs after this node will take two years to flow and thank

259
00:16:29,700 --> 00:16:34,080
goodness there is one here so provisionally we can push two units down here and

260
00:16:34,080 --> 00:16:36,640
now in principle we would have to look at any node there was down here

261
00:16:36,640 --> 00:16:38,260
to see if it had an exit

262
00:16:38,330 --> 00:16:43,810
that could take two units afloat but actually in the image processing problems typically you've

263
00:16:43,810 --> 00:16:45,510
reached the sink after

264
00:16:45,660 --> 00:16:50,600
after two steps on vertical arcs so now we know

265
00:16:50,640 --> 00:16:54,830
but getting into units through that through their we are going to be able to

266
00:16:54,830 --> 00:16:57,530
get two units through

267
00:16:57,550 --> 00:16:58,450
the entire

268
00:16:58,470 --> 00:17:02,620
network OK so now we repeat the process but with very now looking at the

269
00:17:02,620 --> 00:17:08,080
residual flows so now we're a residual capacities rather than now we examine each of

270
00:17:08,080 --> 00:17:13,510
the arcs again looking at the residual capacity this has zero residual capacity because it's

271
00:17:13,510 --> 00:17:15,050
flow already

272
00:17:15,080 --> 00:17:17,830
has some saturated the

273
00:17:17,870 --> 00:17:22,780
and so then we do breakfast search carry on examining arcsin x one is this

274
00:17:22,780 --> 00:17:25,200
one it hasn't saturated it's

275
00:17:25,280 --> 00:17:28,830
capacity so now we can look pushing for flow through there and so on

276
00:17:28,910 --> 00:17:32,120
and continuing the search

277
00:17:32,120 --> 00:17:34,310
we find that the maximum

278
00:17:34,390 --> 00:17:35,760
we find a route

279
00:17:35,810 --> 00:17:38,240
we can push three user flow down

280
00:17:38,280 --> 00:17:39,680
and we got there

281
00:17:39,680 --> 00:17:42,370
OK now we continue the process

282
00:17:42,390 --> 00:17:44,350
looking across here for

283
00:17:44,410 --> 00:17:46,950
arcs which still have residual flow

284
00:17:46,990 --> 00:17:47,800
are there any

285
00:17:47,810 --> 00:17:51,910
yes that's the one the same one again still has some residual capacity

286
00:17:51,910 --> 00:17:53,490
so we could try

287
00:17:53,510 --> 00:17:55,580
pushing will flow down here so we

288
00:17:55,600 --> 00:18:01,260
expand the search tree again actually pretty darn to have collapsed the also is true

289
00:18:01,280 --> 00:18:05,870
because the first part was useful so clever algorithms don't do that they maintain fragments

290
00:18:05,870 --> 00:18:08,030
circuitry for later use

291
00:18:08,080 --> 00:18:11,740
and we get this on we can see what we can no residual capacity down

292
00:18:12,330 --> 00:18:16,100
but is there any other on with residual capacity turns out this one has residual

293
00:18:16,100 --> 00:18:20,550
capacity so that but that's promising but only if we can exit from this node

294
00:18:20,710 --> 00:18:25,260
any know any here with residual capacity is this one and the

295
00:18:25,510 --> 00:18:26,970
so now we get

296
00:18:29,350 --> 00:18:30,550
the next

297
00:18:30,600 --> 00:18:32,120
increment flow

298
00:18:32,120 --> 00:18:38,100
and strictly speaking we haven't finished yet because now you'd say well NER NER carriage

299
00:18:38,100 --> 00:18:40,430
here with with residual capacity and

300
00:18:40,510 --> 00:18:42,200
and the first one

301
00:18:42,200 --> 00:18:43,160
the the

302
00:18:43,180 --> 00:18:47,200
the rightmost one does have residual capacity so you try again you get here is

303
00:18:47,220 --> 00:18:51,140
there anything i coming here to ask you know that it's true that saturated finished

304
00:18:51,470 --> 00:18:55,120
OK so now the algorithm is terminated because there are no

305
00:18:55,140 --> 00:19:00,430
and no roots left with residual capacity

306
00:19:00,510 --> 00:19:05,180
now this point we have to do what i promised which is to examine all

307
00:19:05,180 --> 00:19:10,850
of the arctic char residual capacity which have no residual capacity which is saturated and

308
00:19:11,760 --> 00:19:17,550
so all the one circle in yellow have zero residual capacity and so now

309
00:19:18,330 --> 00:19:23,700
that red line the separating surface should go through all of those yellow circle arcs

310
00:19:23,700 --> 00:19:26,220
and we found the solution to the problem

311
00:19:26,240 --> 00:19:32,120
and it turns out that this is the minimum cost six units which is much

312
00:19:32,120 --> 00:19:36,220
less than thirteen units which shows the cost of the the thing that i try

313
00:19:36,220 --> 00:19:40,470
to random initial is a good thing to have done some optimizations and we've cause

314
00:19:40,490 --> 00:19:44,300
we've got also our optimal labeling after this which is that

315
00:19:44,310 --> 00:19:50,050
this big so is is in the background in this because in the foreground

316
00:19:50,720 --> 00:19:55,910
and it turns out that there are there are many varieties of algorithm of this

317
00:19:55,910 --> 00:20:01,700
form the ditch algorithm is one of the earliest ones and thank goodness although this

318
00:20:02,390 --> 00:20:06,740
this problem appears to have the potential for exponential complexity it turns out that in

319
00:20:06,740 --> 00:20:12,620
each algorithm has only order nq complexity so that if you systematically direct search pushing

320
00:20:12,620 --> 00:20:18,470
flows you can show that you will have examined all over the possible flows all

321
00:20:18,720 --> 00:20:23,640
you need to examine in order q steps where n is the number of pixels

322
00:20:23,640 --> 00:20:27,540
across and q is the pretty large number for when n is one mega pixel

323
00:20:27,540 --> 00:20:32,140
so thank goodness heuristically things you know

324
00:20:32,140 --> 00:20:35,350
faster than NQP

325
00:20:35,390 --> 00:20:40,830
and there are special opportunities also in vision because of the very particular and you

326
00:20:40,830 --> 00:20:44,160
know you could do this on any shape graph with many intermediate nodes and so

327
00:20:44,160 --> 00:20:47,990
on but in vision we tend to have these very wide shallow graphs only two

328
00:20:47,990 --> 00:20:51,660
steps vertically but many steps

329
00:20:51,660 --> 00:20:55,830
in terms of angular distance is of course the same complete symmetry

330
00:20:55,890 --> 00:20:59,430
and so do here fighting which roughly

331
00:20:59,440 --> 00:21:01,300
without being very precise

332
00:21:01,320 --> 00:21:03,970
the weight of each one of those big

333
00:21:04,000 --> 00:21:05,880
must be roughly lambda

334
00:21:05,910 --> 00:21:08,020
divided by and

335
00:21:08,070 --> 00:21:10,760
so i take off this this

336
00:21:10,830 --> 00:21:13,920
think this is all these things are

337
00:21:13,930 --> 00:21:15,800
angles in radians

338
00:21:15,820 --> 00:21:17,610
because science there

339
00:21:17,620 --> 00:21:22,440
very much smaller than the data

340
00:21:23,050 --> 00:21:25,450
you can see that the larger and is

341
00:21:25,490 --> 00:21:29,360
the more of these openings you have

342
00:21:29,420 --> 00:21:33,850
the never these lines are going to be but i think of these being aligned

343
00:21:33,910 --> 00:21:36,090
that means you will be

344
00:21:36,140 --> 00:21:40,500
to distinguish two neighbouring frequencies from each other two different led us

345
00:21:41,680 --> 00:21:45,210
and and that's what we call spectral resolution

346
00:21:45,260 --> 00:21:47,190
so the larger and is

347
00:21:47,240 --> 00:21:50,540
the band spectral resolution you have

348
00:21:50,560 --> 00:21:58,450
the ability to separate two neighbouring frequencies than increases

349
00:21:58,490 --> 00:22:01,480
it's very easy way

350
00:22:01,530 --> 00:22:03,690
but i can convince you

351
00:22:03,700 --> 00:22:05,880
without any mass

352
00:22:05,940 --> 00:22:07,970
a little less

353
00:22:07,980 --> 00:22:10,730
why the width of these pics

354
00:22:10,780 --> 00:22:12,880
two weeks of these maxima maximum

355
00:22:12,930 --> 00:22:15,160
must be proportional to one

356
00:22:15,210 --> 00:22:16,610
over and

357
00:22:16,630 --> 00:22:22,770
and that during an energy conservation arguments follow me closely

358
00:22:22,810 --> 00:22:27,210
if i have enough of these openings in discrete label that and times more light

359
00:22:27,940 --> 00:22:31,520
then one opening that straightforward you can tell that you get from

360
00:22:31,600 --> 00:22:35,450
if you have an opening industry you get ten times more light through than if

361
00:22:35,450 --> 00:22:38,770
you had one openly that's non-negotiable right

362
00:22:38,770 --> 00:22:42,290
but if the maximum is and script times higher

363
00:22:42,350 --> 00:22:46,000
the only way that you can conserve energy is if you make the maximum and

364
00:22:46,000 --> 00:22:50,230
times smaller than you know that ten times more likely

365
00:22:50,240 --> 00:22:52,130
the argument once more

366
00:22:52,190 --> 00:22:54,360
you have an openings

367
00:22:54,420 --> 00:22:57,580
give you an times more energy than one opening

368
00:22:57,600 --> 00:22:59,270
but if the maximum

369
00:22:59,390 --> 00:23:02,610
if you light intensity with groceries and square

370
00:23:02,620 --> 00:23:06,840
the only way that you can conserve energy if you make the maximum time

371
00:23:06,880 --> 00:23:09,040
so it's very easy way to see

372
00:23:09,090 --> 00:23:11,360
that the which you must

373
00:23:11,370 --> 00:23:12,270
go down

374
00:23:12,310 --> 00:23:14,890
was increasing and

375
00:23:14,910 --> 00:23:17,530
very powerful i

376
00:23:17,680 --> 00:23:20,870
now i will make you see in another way

377
00:23:20,910 --> 00:23:22,810
if we have for all of these

378
00:23:22,830 --> 00:23:24,500
openings industry

379
00:23:24,500 --> 00:23:26,750
why there are only three

380
00:23:28,080 --> 00:23:32,310
and all these methods that i'm using in a way i complementary

381
00:23:32,320 --> 00:23:35,480
it's all the same thing but i just wanted to see it in different ways

382
00:23:35,480 --> 00:23:37,320
that helped me enormously

383
00:23:37,890 --> 00:23:41,520
look at it in different ways

384
00:23:41,520 --> 00:23:43,530
so we have an equal four

385
00:23:43,590 --> 00:23:48,520
and i started with delta equals zero

386
00:23:48,560 --> 00:23:54,430
so there's no phase difference between adjacent sources

387
00:23:54,470 --> 00:23:56,020
so this is the case

388
00:23:56,030 --> 00:24:00,430
that this is the vector of source number one this effective source number two

389
00:24:00,470 --> 00:24:04,820
number three and number four all four factors lined up in the same direction and

390
00:24:04,830 --> 00:24:07,300
i doubt that could not be zero

391
00:24:07,450 --> 00:24:11,740
therefore i get sixteen times the like because i square

392
00:24:12,600 --> 00:24:14,660
and i get sixteen this is your

393
00:24:14,660 --> 00:24:16,310
factor of sixteen

394
00:24:16,370 --> 00:24:19,350
you get maximum

395
00:24:19,370 --> 00:24:23,620
now go to build eagles while the

396
00:24:23,670 --> 00:24:24,990
ninety degrees

397
00:24:25,010 --> 00:24:27,580
i don't have to look at that equation

398
00:24:27,630 --> 00:24:31,450
i don't need that one i know ninety degrees is i went to high school

399
00:24:31,460 --> 00:24:33,530
and educated

400
00:24:33,570 --> 00:24:35,020
this is one factor

401
00:24:35,060 --> 00:24:38,580
this is ninety degrees that's the second factor this is ninety degrees at the third

402
00:24:38,590 --> 00:24:44,030
factor this is the force vector ninety degrees what do i do was

403
00:24:45,360 --> 00:24:47,080
so if all four

404
00:24:47,090 --> 00:24:49,560
relative to neighbors ninety degrees

405
00:24:49,570 --> 00:24:50,530
phase angle

406
00:24:50,560 --> 00:24:53,230
then clearly zero here

407
00:24:53,300 --> 00:24:55,750
now i go delta

408
00:24:55,830 --> 00:24:57,220
because by

409
00:24:57,220 --> 00:24:59,100
i know why is one

410
00:24:59,800 --> 00:25:01,410
three four

411
00:25:01,450 --> 00:25:04,420
what do i end up with

412
00:25:04,430 --> 00:25:07,840
i have four vectors under a degree flip flip flip flip what is the net

413
00:25:11,350 --> 00:25:14,530
this one

414
00:25:14,530 --> 00:25:17,270
is this one

415
00:25:17,270 --> 00:25:20,210
and the second one is this one not going to do this one for you

416
00:25:22,730 --> 00:25:24,180
equals three as five

417
00:25:24,300 --> 00:25:27,510
three houses two hundred seventy degrees

418
00:25:28,340 --> 00:25:32,200
this is one this is two hundred seventy degrees this is two hundred seventy degrees

419
00:25:32,200 --> 00:25:35,990
and that is two hundred seventy degrees what is the net result

420
00:25:36,710 --> 00:25:40,730
well let this one

421
00:25:40,780 --> 00:25:43,470
now i'm going to do to pi

422
00:25:43,510 --> 00:25:46,290
i mean i don't report back here

423
00:25:46,360 --> 00:25:48,130
get the next

424
00:25:48,180 --> 00:25:49,860
and that this one

425
00:25:49,880 --> 00:25:51,270
so you can see

426
00:25:51,270 --> 00:25:54,530
fury biplane vector is very simple high-school level

427
00:25:54,550 --> 00:25:56,190
you can see that there will be

428
00:25:56,190 --> 00:25:58,100
n minus one minimum

429
00:25:58,180 --> 00:26:00,390
exact minimum between

430
00:26:00,410 --> 00:26:04,670
the prime maximum

431
00:26:04,680 --> 00:26:06,180
well the first thing that

432
00:26:06,190 --> 00:26:08,750
i wanted to see

433
00:26:08,790 --> 00:26:12,700
is when i take the grading that you have

434
00:26:13,220 --> 00:26:14,580
and i use my

435
00:26:14,830 --> 00:26:20,040
laser pointer

436
00:26:20,080 --> 00:26:22,330
the first thing i wanted to see

437
00:26:22,340 --> 00:26:24,470
incredible impact

438
00:26:24,470 --> 00:26:25,610
of using

439
00:26:25,630 --> 00:26:28,230
many many lines

440
00:26:28,240 --> 00:26:31,110
why laser beam

441
00:26:31,180 --> 00:26:33,280
has about diameter

442
00:26:33,340 --> 00:26:36,040
of about three millimeters

443
00:26:36,050 --> 00:26:38,450
so here is this laser beam

444
00:26:38,490 --> 00:26:39,310
and this

445
00:26:39,330 --> 00:26:43,220
i estimated to be roughly three millimeters

446
00:26:43,220 --> 00:26:45,440
no grading believe it or not

447
00:26:45,450 --> 00:26:47,970
is a supergradient

448
00:26:48,050 --> 00:26:54,430
upgrading at thirteen thousand four hundred lines per inch imagine how anyone can pull groups

449
00:26:54,440 --> 00:26:55,750
in your plastic

450
00:26:55,760 --> 00:26:58,020
thirteen thousand four hundred

451
00:26:59,170 --> 00:27:01,520
that means that the separation the

452
00:27:01,560 --> 00:27:04,220
between two groups is about one point nine

453
00:27:04,580 --> 00:27:07,440
times ten to the minus six meters there is only

454
00:27:07,450 --> 00:27:09,080
two microns

455
00:27:09,170 --> 00:27:14,330
how anyone can do that beats me but it can be done

456
00:27:14,360 --> 00:27:17,700
so i can calculate know how many of those lines i have used into three

457
00:27:17,700 --> 00:27:20,260
millimeters and i end up with

458
00:27:20,300 --> 00:27:22,820
and is about sixteen hundred

459
00:27:22,860 --> 00:27:25,180
so when i shine my laser beam

460
00:27:25,260 --> 00:27:31,330
through migrating i effectively sixteen hundred of those lines

461
00:27:31,330 --> 00:27:33,370
brother triangles of course

462
00:27:33,450 --> 00:27:34,900
that's not true

463
00:27:34,910 --> 00:27:37,220
for the triangles it's not but for

464
00:27:37,240 --> 00:27:39,340
a right triangle somehow

465
00:27:39,350 --> 00:27:47,280
that fact should connect to that fact can we just make that connection

466
00:27:47,310 --> 00:27:49,950
what's the connection between this

467
00:27:49,970 --> 00:27:51,960
test for orthogonality

468
00:27:51,970 --> 00:27:53,120
and this

469
00:27:53,130 --> 00:27:56,850
statement of orthogonality well i guess i have to say

470
00:27:56,870 --> 00:27:59,320
what is the length square

471
00:27:59,330 --> 00:28:00,840
so let's

472
00:28:00,890 --> 00:28:02,890
continue on the

473
00:28:02,900 --> 00:28:06,080
board underneath without equation

474
00:28:06,130 --> 00:28:11,510
give me another way to express the length squared of vector

475
00:28:11,680 --> 00:28:13,700
let me just give your vector

476
00:28:13,770 --> 00:28:17,180
the vector one two three

477
00:28:17,200 --> 00:28:19,430
that's in three dimensions

478
00:28:19,460 --> 00:28:23,900
what is the length square of the vector x equal

479
00:28:23,930 --> 00:28:27,180
one two three

480
00:28:27,220 --> 00:28:30,530
so how do you find the link squared

481
00:28:30,560 --> 00:28:32,630
well really it

482
00:28:32,700 --> 00:28:36,160
you want the length of that vector that goes along on

483
00:28:36,180 --> 00:28:38,750
up to one out of three

484
00:28:38,760 --> 00:28:40,240
and will

485
00:28:40,250 --> 00:28:43,680
come come back to that right triangles stuff

486
00:28:43,720 --> 00:28:45,790
the length squared is

487
00:28:45,810 --> 00:28:48,320
this is exactly

488
00:28:48,330 --> 00:28:52,580
x transpose x

489
00:28:52,590 --> 00:28:56,580
whenever i cx transpose x i know i've got

490
00:28:56,720 --> 00:28:58,780
number that's positive

491
00:28:58,790 --> 00:29:00,830
it's length squared

492
00:29:00,830 --> 00:29:05,750
a lesser unless x happens to be the zero vector that's the one case where

493
00:29:05,750 --> 00:29:08,270
the length is zero

494
00:29:09,200 --> 00:29:14,400
right now this is just x one squared plus x two squared plus on perspex

495
00:29:14,400 --> 00:29:15,690
in square

496
00:29:15,700 --> 00:29:19,900
so one in the example i gave you what was the length squared of that

497
00:29:19,900 --> 00:29:23,010
vector one two three

498
00:29:23,020 --> 00:29:24,440
so you square

499
00:29:24,450 --> 00:29:28,130
one four in nine forty

500
00:29:28,140 --> 00:29:32,390
so the vector one two three has length fourteen

501
00:29:32,390 --> 00:29:35,350
so let me just put down the back

502
00:29:36,060 --> 00:29:38,200
ah that's the

503
00:29:38,220 --> 00:29:40,840
the vector one two three

504
00:29:40,850 --> 00:29:49,320
michal kovac orthogonal to it

505
00:29:49,330 --> 00:29:50,330
so what

506
00:29:50,330 --> 00:29:53,830
so the right here

507
00:29:53,840 --> 00:29:55,070
x where

508
00:29:55,080 --> 00:29:58,650
one four nine forty

509
00:29:58,750 --> 00:30:02,680
let me come back here that orthogonal to it

510
00:30:02,710 --> 00:30:05,180
i will get

511
00:30:05,190 --> 00:30:12,220
right that those two vectors are orthogonal

512
00:30:12,230 --> 00:30:15,270
the length of life where it is

513
00:30:19,640 --> 00:30:22,440
and that was why

514
00:30:24,460 --> 00:30:25,690
one and two making

515
00:30:26,370 --> 00:30:26,840
one month

516
00:30:26,890 --> 00:30:30,130
one making one really zero making three

517
00:30:30,520 --> 00:30:33,870
the length of the square

518
00:30:34,410 --> 00:30:39,510
o nine was one was nine ninety

519
00:30:39,530 --> 00:30:42,630
and sure enough

520
00:30:42,640 --> 00:30:44,610
i have improved

521
00:30:44,620 --> 00:30:46,890
just like

522
00:30:46,930 --> 00:30:48,260
check to see

523
00:30:51,390 --> 00:30:54,040
best friend of y equals zero

524
00:30:54,100 --> 00:30:59,240
which is for everybody the next round of y zero here

525
00:30:59,410 --> 00:31:01,540
that may be the main point

526
00:31:01,550 --> 00:31:02,440
you should

527
00:31:02,460 --> 00:31:05,820
you really quick doing extra y

528
00:31:05,970 --> 00:31:07,120
so if

529
00:31:07,160 --> 00:31:09,000
this was the first is that

530
00:31:09,010 --> 00:31:11,290
and sure enough

531
00:31:11,350 --> 00:31:13,430
that played with

532
00:31:13,890 --> 00:31:16,930
forty five agreeing with ninety

533
00:31:17,620 --> 00:31:20,040
let me just do that is

534
00:31:21,510 --> 00:31:23,970
that's why have called y

535
00:31:23,990 --> 00:31:26,820
and this

536
00:31:26,990 --> 00:31:28,670
well why

537
00:31:33,260 --> 00:31:37,850
so i'm looking again this is always true i repeat

538
00:31:37,890 --> 00:31:41,790
this is going to be true when we have a right angle

539
00:31:41,850 --> 00:31:43,170
and the does

540
00:31:43,180 --> 00:31:46,500
well of course i'm just going to simplify the here

541
00:31:46,610 --> 00:31:49,860
the next round of the air

542
00:31:49,870 --> 00:31:52,510
and there is a wide variety of why there

543
00:31:52,530 --> 00:31:58,220
and the friends of y

544
00:31:58,270 --> 00:32:05,390
and there is a wide range of

545
00:32:05,440 --> 00:32:06,590
i know like

546
00:32:06,610 --> 00:32:10,380
do that simplification because

547
00:32:10,420 --> 00:32:13,900
and matrix multiplication and to follow the rules

548
00:32:15,220 --> 00:32:17,370
extract from that can fall

549
00:32:17,390 --> 00:32:19,720
y four y

550
00:32:19,740 --> 00:32:25,190
and what about these guys what would you tell me about

551
00:32:25,240 --> 00:32:27,230
the inner product of x with y

552
00:32:27,240 --> 00:32:30,220
and the inner product of y with that

553
00:32:30,400 --> 00:32:33,390
is there a different

554
00:32:33,400 --> 00:32:38,390
i think we well we're doing real

555
00:32:38,400 --> 00:32:40,900
bacteria which is already doing now

556
00:32:40,910 --> 00:32:43,360
there is a difference is noted

557
00:32:43,380 --> 00:32:44,800
if i

558
00:32:44,810 --> 00:32:48,470
extract y given a zero quite

559
00:32:48,480 --> 00:32:51,730
why transpose x i would have the same

560
00:32:51,800 --> 00:32:56,520
x one y one x two two y two and x three y three

561
00:32:56,530 --> 00:32:58,900
it would be the same so this is

562
00:32:58,910 --> 00:32:59,760
this is

563
00:32:59,800 --> 00:33:07,520
this is the same as that really all not that guy out there too

564
00:33:07,530 --> 00:33:10,130
so i believe that the

565
00:33:10,800 --> 00:33:15,720
this is a quite boil down to the thing being

566
00:33:16,790 --> 00:33:18,510
everything else can

567
00:33:18,540 --> 00:33:21,040
and this equation boils down

568
00:33:21,060 --> 00:33:24,180
so that's really all i

569
00:33:24,210 --> 00:33:28,010
i wanted to check that

570
00:33:30,140 --> 00:33:32,290
four right triangle

571
00:33:32,350 --> 00:33:33,690
let me

572
00:33:33,710 --> 00:33:34,600
to there

573
00:33:34,640 --> 00:33:36,970
i thought i can for the two

574
00:33:36,990 --> 00:33:38,430
no problem

575
00:33:39,010 --> 00:33:42,670
and for like a zero at the

576
00:33:44,970 --> 00:33:46,920
you know what

577
00:33:46,930 --> 00:33:50,690
the dot product of orthogonal vectors is

578
00:33:50,740 --> 00:33:52,800
it says i want to say

579
00:33:52,860 --> 00:33:53,960
that's really

580
00:33:53,970 --> 00:33:54,790
it can

581
00:33:54,800 --> 00:33:56,460
it comes out the

582
00:33:56,510 --> 00:33:58,120
all right

583
00:33:59,400 --> 00:34:03,610
what about now i know it to back what it means when two vectors are

584
00:34:03,610 --> 00:34:05,640
orthogonal by the way

585
00:34:05,650 --> 00:34:09,660
what about if one of these guys is zero

586
00:34:09,670 --> 00:34:12,250
suppose x is the zero vector

587
00:34:12,270 --> 00:34:13,070
and why

588
00:34:13,090 --> 00:34:15,420
it is what

589
00:34:15,430 --> 00:34:18,680
are they are not

590
00:34:18,740 --> 00:34:21,010
she was

591
00:34:21,010 --> 00:34:23,050
favourite reason doesn't support

592
00:34:23,130 --> 00:34:25,050
this tracing

593
00:34:25,110 --> 00:34:30,780
compute justifications then you would need something else and we can use the black box

594
00:34:30,780 --> 00:34:35,260
technique so black box techniques do not depend on the particular reason that you can

595
00:34:35,260 --> 00:34:38,590
you should be able to use them with any reasonable

596
00:34:38,610 --> 00:34:42,500
and all that we require for this black box technique is done

597
00:34:42,510 --> 00:34:46,730
and the reason it can i tell us whether a class expression is satisfiable

598
00:34:46,730 --> 00:34:47,960
with respect to

599
00:34:48,030 --> 00:34:50,340
a ontology

600
00:34:52,090 --> 00:34:57,340
we using power in these demonstrations due to a couple of implementation issues factors plus

601
00:34:57,340 --> 00:35:03,320
doesn't work so well the justification were working to sort out the justification workbench working

602
00:35:03,320 --> 00:35:05,590
to sort out the

603
00:35:05,650 --> 00:35:09,380
so i say it just requires that a class

604
00:35:09,500 --> 00:35:13,760
we can ask for the satisfiability of the class in other classes satisfiability

605
00:35:13,780 --> 00:35:18,500
so satisfiable and the reason for this is that if we have an entailment ontology

606
00:35:18,500 --> 00:35:20,960
models ontology entails c

607
00:35:20,980 --> 00:35:23,170
this of course did

608
00:35:23,170 --> 00:35:24,050
in OWL

609
00:35:24,070 --> 00:35:26,860
there's always the corresponding class description

610
00:35:26,880 --> 00:35:30,800
that would be unsatisfiable with respect to ontology for a given entailment

611
00:35:30,820 --> 00:35:35,210
so here you see the phrases across the in the class here

612
00:35:35,230 --> 00:35:36,840
c and not the

613
00:35:36,840 --> 00:35:39,130
it's going to be unsatisfiable

614
00:35:39,150 --> 00:35:41,150
and for all

615
00:35:41,240 --> 00:35:46,170
we can always generate an unsatisfiable class entailments that we're interested in

616
00:35:46,300 --> 00:35:51,130
OK so if this entailment holds you see that question is satisfiable if intend doesn't

617
00:35:51,130 --> 00:35:53,690
hold is satisfiable

618
00:35:54,780 --> 00:36:04,340
just a little bit more about this black box strategy because this is the one

619
00:36:04,340 --> 00:36:06,260
that brought show uses

620
00:36:06,260 --> 00:36:08,670
so that it doesn't depend on the particular reason

621
00:36:08,690 --> 00:36:13,820
so these blackbox strategy typically use some kind of expanding contract strategy

622
00:36:13,860 --> 00:36:17,880
so we start off with the entailment that we're interested in itself if it was

623
00:36:17,940 --> 00:36:20,820
we wanted to explain why he was supposed to die

624
00:36:20,860 --> 00:36:23,110
we take this signature

625
00:36:23,380 --> 00:36:27,650
so c and d and we can then look vaccines from the ontology for that

626
00:36:27,650 --> 00:36:28,940
define c and d

627
00:36:28,960 --> 00:36:31,980
we start with an empty ontology and we

628
00:36:32,010 --> 00:36:36,440
stuff in axioms gradually doing some kind of structural expansion

629
00:36:36,460 --> 00:36:40,360
so the thing turns out is satisfy unsatisfiable

630
00:36:40,360 --> 00:36:46,570
so we have this ability quite quite a few satisfiability cheque city-states gradually expand our

631
00:36:46,610 --> 00:36:50,920
ontology and my actions in israel the force technique

632
00:36:50,940 --> 00:36:53,030
when we get an unsatisfiable class

633
00:36:53,050 --> 00:36:58,780
then we know that we've got a superset of the axioms required entailment to hold

634
00:36:58,800 --> 00:37:02,900
so we start to prune these off often until expression is satisfiable and now from

635
00:37:02,900 --> 00:37:07,610
this we can determine which axioms actually constitute justification

636
00:37:07,630 --> 00:37:13,860
and then optimize this is for slowing quite a lot of satisfiability checks but several

637
00:37:13,860 --> 00:37:21,010
optimizations in particular to do with the expansion of axioms are stimulate contraction of axioms

638
00:37:21,010 --> 00:37:26,550
in the second stage several and optimizations we can introduce so that in practice is

639
00:37:26,550 --> 00:37:28,210
not too bad

640
00:37:28,230 --> 00:37:32,420
and one of the nice things about these modularity stuff is that as well as

641
00:37:32,440 --> 00:37:37,260
in doing this we use scenario where we can pull out a small section of

642
00:37:37,260 --> 00:37:40,300
the ontology and use it in

643
00:37:40,300 --> 00:37:45,880
import into another ontology network we working on we can also use this more programmatically

644
00:37:45,880 --> 00:37:50,360
we can use it for optimisations so we want to know why something is satisfiable

645
00:37:50,360 --> 00:37:53,380
we can compute the module for the thing that's unsatisfiable

646
00:37:53,400 --> 00:37:57,690
i guess small portion of the axioms and then this sort of

647
00:37:58,260 --> 00:38:03,010
explanation this technique generates explanations really works quite well

648
00:38:05,690 --> 00:38:10,610
so the techniques that we use here is that we use this black box strategy

649
00:38:10,610 --> 00:38:16,570
or glass box strategy to find one justification so in this case we would use

650
00:38:16,570 --> 00:38:19,940
pallets to compute the what explanation

651
00:38:20,000 --> 00:38:25,300
all part of the computer with any reasonable what explanation we find one justification and

652
00:38:25,300 --> 00:38:32,240
then we use techniques from all diagnosis to find all justifications in the answer for

653
00:38:32,240 --> 00:38:34,320
a given entailment this uses

654
00:38:34,900 --> 00:38:37,980
this a sum up my diagnosis in particular

655
00:38:38,000 --> 00:38:45,460
right so algorithms for computing hitting centuries and the worst case this is album is

656
00:38:46,610 --> 00:38:51,150
with the number justification so if a lot of justifications is quite slow

657
00:38:52,150 --> 00:38:54,510
some key optimizations we can apply here

658
00:38:54,530 --> 00:39:01,230
so again it doesn't work so badly in practice

659
00:39:08,480 --> 00:39:13,710
OK so what i want to do now is just have look small justifications and

660
00:39:13,710 --> 00:39:15,210
i want to

661
00:39:15,340 --> 00:39:19,940
as such repairs ontology and i'll show you a way of doing this

662
00:39:19,960 --> 00:39:20,800
so i

663
00:39:20,820 --> 00:39:24,760
and we should focus on average is unsatisfiable classes if you want to move

664
00:39:24,760 --> 00:39:27,150
ten of these classes satisfiable

665
00:39:27,150 --> 00:39:31,860
so what we do is we select all of these is unsatisfiable classes here

666
00:39:31,880 --> 00:39:34,380
and then on the right-hand side

667
00:39:34,400 --> 00:39:37,210
for each time we select

668
00:39:37,230 --> 00:39:43,090
it will have the justifications for the entailment of the panel on the right-hand side

669
00:39:43,130 --> 00:39:49,440
so we get quite a few justifications here

670
00:39:49,440 --> 00:39:52,680
that's that's continuous so

671
00:39:52,760 --> 00:39:58,330
i had a couple of questions during the break may be repeat here so one

672
00:39:58,330 --> 00:40:00,420
question was related to this figure

673
00:40:00,810 --> 00:40:03,330
and the question was

674
00:40:03,790 --> 00:40:09,100
one of these noise free observations and indeed i i did use noise free observation

675
00:40:09,150 --> 00:40:10,360
is right actually

676
00:40:10,380 --> 00:40:13,660
actually require that the function should agree exactly

677
00:40:13,670 --> 00:40:18,880
with the data points here in the formula down here the noise level here which

678
00:40:18,880 --> 00:40:23,740
comes on like your functions in allows for noise in principle but active set sigma

679
00:40:23,740 --> 00:40:27,160
square the noise to be zero in this particular equation

680
00:40:27,490 --> 00:40:32,350
this is one of the interesting things about casting process you could actually fit them

681
00:40:32,350 --> 00:40:34,120
to to noise free data

682
00:40:34,130 --> 00:40:37,310
so for example you can use the prior metric models to right

683
00:40:37,310 --> 00:40:38,700
because it might be that

684
00:40:38,710 --> 00:40:40,500
for any set of parameters

685
00:40:40,520 --> 00:40:44,140
you can get an exact fit the data is typically the case you can just

686
00:40:45,150 --> 00:40:47,680
you have exactly today

687
00:40:48,010 --> 00:40:53,010
in one of the other comments was shows that shows the pretty pictures maybe some

688
00:40:53,010 --> 00:40:55,240
sort of very nice

689
00:40:55,260 --> 00:41:00,430
it is a very humble questions

690
00:41:00,760 --> 00:41:11,340
that's right so the question is exactly what is the shaded area

691
00:41:11,380 --> 00:41:14,540
in this area what i did was i just computer

692
00:41:14,560 --> 00:41:17,960
for every point x i computed the mean

693
00:41:18,010 --> 00:41:20,510
and the variance of the core of the corresponding

694
00:41:20,560 --> 00:41:23,760
garcia predictive distribution rights were just evaluated

695
00:41:23,770 --> 00:41:27,030
this thing here for all possible values of x star

696
00:41:27,090 --> 00:41:28,590
and the grey shaded area

697
00:41:28,600 --> 00:41:31,820
it's just the mean the middle of the gracious areas just the mean

698
00:41:31,840 --> 00:41:36,880
and goes up to plus and minus two salutations that corresponds so two and ninety

699
00:41:36,880 --> 00:41:38,670
five percent confidence intervals

700
00:41:38,680 --> 00:41:40,420
of where the function could be

701
00:41:41,010 --> 00:41:45,490
of course the core looking at this is that you have these are pointwise confidence

702
00:41:47,070 --> 00:41:50,400
so obviously the the function is the true function values up here then it can

703
00:41:50,400 --> 00:41:53,620
be the true function value for the for nearby points will be done here down

704
00:41:53,640 --> 00:41:54,740
here right

705
00:41:54,780 --> 00:41:56,250
so the joint

706
00:41:56,310 --> 00:41:57,930
confidence intervals

707
00:41:58,000 --> 00:41:59,610
you can also compute

708
00:41:59,620 --> 00:42:05,490
but this is this is a a big object this is a matrix of so

709
00:42:05,840 --> 00:42:07,210
these are marginal

710
00:42:07,710 --> 00:42:08,840
point wise

711
00:42:08,870 --> 00:42:14,460
pointwise confidence intervals so therefore i also plotted functions that are drawn like may of

712
00:42:14,460 --> 00:42:16,400
course are drawn from the joint

713
00:42:16,460 --> 00:42:18,180
predictive distribution

714
00:42:34,140 --> 00:42:36,020
you could also the right to

715
00:42:36,030 --> 00:42:38,280
so so what's happening here is just that the

716
00:42:38,300 --> 00:42:40,650
that the predictive distribution here

717
00:42:41,330 --> 00:42:43,430
the distribution from under the prior

718
00:42:43,440 --> 00:42:47,900
one from between minus two two actually of serving these things

719
00:42:47,930 --> 00:42:51,280
at the value five did not to change that

720
00:42:51,310 --> 00:42:52,770
very much or at all

721
00:42:52,770 --> 00:42:55,180
i just says that the that the

722
00:42:55,220 --> 00:42:57,280
that the

723
00:42:57,300 --> 00:43:01,370
the information about the data contains about what the function is doing out here

724
00:43:01,400 --> 00:43:04,120
is almost zero

725
00:43:06,430 --> 00:43:12,110
yes yes they just run drawn around

726
00:43:15,490 --> 00:43:20,520
that's right now

727
00:43:20,560 --> 00:43:22,120
OK so let's try to sum up

728
00:43:22,120 --> 00:43:25,270
we're we're where we've come to so far right so i i spent a lot

729
00:43:25,270 --> 00:43:29,500
of time to tell you what is the gaussianprocess trying to convince you that this

730
00:43:29,500 --> 00:43:33,210
is the distribution of functions and i've shown here in a graphical

731
00:43:33,240 --> 00:43:35,120
format and also in

732
00:43:35,250 --> 00:43:36,710
using a drug

733
00:43:36,720 --> 00:43:38,930
how you update

734
00:43:39,310 --> 00:43:42,930
prior process to the posterior process by conditioning

735
00:43:42,990 --> 00:43:44,810
on the data

736
00:43:44,840 --> 00:43:46,490
like so the things that

737
00:43:46,490 --> 00:43:48,560
so i think i haven't told you about it

738
00:43:48,580 --> 00:43:51,870
it is well what are the properties of these functions right

739
00:43:51,910 --> 00:43:56,020
because if these in this class of functions functions that look like this

740
00:43:56,060 --> 00:43:58,860
if this is in some sense is a useless class like if you're not interested

741
00:43:58,860 --> 00:44:00,420
in functions look like that

742
00:44:01,610 --> 00:44:03,670
it doesn't make much sense what i'm saying right

743
00:44:03,710 --> 00:44:05,200
so we should worry about

744
00:44:05,880 --> 00:44:08,950
what kind of properties can functions have

745
00:44:08,960 --> 00:44:10,960
from these process so that's what i'm going to

746
00:44:11,850 --> 00:44:13,530
tell you more about

747
00:44:13,880 --> 00:44:18,250
so the so so so it's important to understand how we got from this picture

748
00:44:18,250 --> 00:44:19,320
to this picture

749
00:44:19,360 --> 00:44:20,250
at least

750
00:44:20,260 --> 00:44:24,670
in in the concept away by saying well you just draw this function around and

751
00:44:24,700 --> 00:44:27,020
have this distribution over functions

752
00:44:27,420 --> 00:44:28,540
and then you

753
00:44:28,590 --> 00:44:31,920
you can select the ones that agree with the data points

754
00:44:31,940 --> 00:44:35,630
you can think of each of these are doing in graphical way it turns out

755
00:44:35,630 --> 00:44:37,290
that it's actually very easy to compute

756
00:44:37,300 --> 00:44:38,070
this thing

757
00:44:38,300 --> 00:44:41,210
let's look first a little bit more

758
00:44:41,220 --> 00:44:45,650
these these equations since there quite long

759
00:44:45,700 --> 00:44:53,360
so here first this is a graphical representation of what i constant process model looks

760
00:44:53,360 --> 00:44:57,160
like i got the process model involves a number of

761
00:44:57,190 --> 00:45:00,030
these are latent variables of these variables the

762
00:45:00,700 --> 00:45:06,230
tell you what are the on what the true underlying function values right and if

763
00:45:06,230 --> 00:45:09,770
you had noise in the process then you don't have certain effort so you only

764
00:45:09,770 --> 00:45:11,090
observe y

765
00:45:12,190 --> 00:45:15,270
we observe the the things around in in square here

766
00:45:15,350 --> 00:45:18,380
and these are these are latent variables

767
00:45:18,390 --> 00:45:21,270
and what you're interested in is the distribution of

768
00:45:21,320 --> 00:45:25,630
of wise otherwise star here and why stars are connected to the things that you

769
00:45:26,530 --> 00:45:28,460
by the fully connected network

770
00:45:29,340 --> 00:45:31,030
of latent variables

771
00:45:31,090 --> 00:45:33,580
what you need to do to do inference is you need to

772
00:45:33,590 --> 00:45:35,360
figure out what is the

773
00:45:35,380 --> 00:45:37,190
the conditional distribution of y one

774
00:45:38,030 --> 00:45:39,340
all the observed

775
00:45:44,080 --> 00:45:48,970
now relating back to students talk about graphical models because the fully connected network

776
00:45:48,980 --> 00:45:50,690
there are no shortcuts

777
00:45:51,350 --> 00:45:54,030
you actually need to take all those

778
00:45:54,080 --> 00:45:56,300
dependencies into account

779
00:45:56,400 --> 00:46:01,190
but because everything is jointly calcium you can actually compute without distribution

780
00:46:01,380 --> 00:46:05,590
so things like that that you only thing which is bad is that these these

781
00:46:05,590 --> 00:46:09,210
great measures actually grow very rapidly right so if you have

782
00:46:09,210 --> 00:46:12,840
a hundred latent variables and you have to work with one hundred five hundred grams

783
00:46:14,150 --> 00:46:18,500
if you have millions of observations that might be a problem

784
00:46:18,510 --> 00:46:21,770
and it also shows against the graphical models sort author

785
00:46:22,000 --> 00:46:23,220
tells you about

786
00:46:23,230 --> 00:46:24,600
this this

787
00:46:24,610 --> 00:46:29,550
this thing that you can do that if you add another random variable here

788
00:46:29,600 --> 00:46:33,440
but if you had a test point here for the added an unspecified test point

789
00:46:34,020 --> 00:46:36,400
so doing that if it's not observed

790
00:46:36,400 --> 00:46:39,760
then it doesn't change the distribution over the variables

791
00:46:39,800 --> 00:46:42,300
that's a graphical models way

792
00:46:42,320 --> 00:46:45,380
a thing this marginalisation property

793
00:46:45,420 --> 00:46:49,260
if you're only interested in one of the variables then it just has the covariance

794
00:46:49,260 --> 00:46:50,530
and mean

795
00:46:51,000 --> 00:46:53,270
by picking out those those elements

796
00:46:53,330 --> 00:46:55,300
one of the matrix so you can add in

797
00:46:55,390 --> 00:47:00,790
these test points without changing the distribution of these things

798
00:47:01,290 --> 00:47:06,230
so let's let's look at this look at this sort of this is the main

799
00:47:06,230 --> 00:47:07,160
result here

800
00:47:07,710 --> 00:47:11,020
so that is the predictive distribution now i look at the british precision of the

801
00:47:11,020 --> 00:47:13,150
corresponding latent variable

802
00:47:13,210 --> 00:47:17,190
the difference between the latent variable and the corresponding served variable is just discussed in

803
00:47:17,190 --> 00:47:20,950
that's really physics and actions a one all the way

804
00:47:20,960 --> 00:47:25,100
most really morris velocity when he hits the floor

805
00:47:25,830 --> 00:47:27,940
the fall believable

806
00:47:27,940 --> 00:47:29,840
the guy was bowled for one thing

807
00:47:29,860 --> 00:47:33,290
and so the impact time was very short

808
00:47:33,290 --> 00:47:37,070
and when in fact i'm assured even if you get the frozen modest speed

809
00:47:37,210 --> 00:47:38,860
exploration is high

810
00:47:38,870 --> 00:47:39,690
and that

811
00:47:39,690 --> 00:47:40,800
i was too much

812
00:47:40,820 --> 00:47:41,580
and so

813
00:47:41,590 --> 00:47:42,620
that's why

814
00:47:42,630 --> 00:47:44,000
this school

815
00:47:44,070 --> 00:47:46,290
what crushed

816
00:47:46,290 --> 00:47:47,930
so it what matters is this

817
00:47:47,950 --> 00:47:49,380
change in velocity

818
00:47:49,400 --> 00:47:52,120
and the impact time

819
00:47:53,180 --> 00:47:55,590
now i want to make one last

820
00:47:55,680 --> 00:47:58,080
step from average acceleration

821
00:47:58,100 --> 00:47:59,200
we want to go

822
00:48:01,260 --> 00:48:04,020
acceleration at any moment in time

823
00:48:04,110 --> 00:48:05,890
just the way we did that

824
00:48:07,910 --> 00:48:10,350
and that now is the natural step

825
00:48:10,430 --> 00:48:13,140
the acceleration at any moment in time

826
00:48:13,210 --> 00:48:14,210
it would be

827
00:48:14,270 --> 00:48:15,430
and then

828
00:48:15,440 --> 00:48:16,910
for delta team

829
00:48:17,010 --> 00:48:18,680
goes to zero

830
00:48:18,700 --> 00:48:20,210
for the

831
00:48:20,230 --> 00:48:22,090
national because delta t

832
00:48:22,150 --> 00:48:24,470
the minus is the

833
00:48:24,510 --> 00:48:25,680
divided by

834
00:48:29,050 --> 00:48:32,690
is the instantaneous acceleration this

835
00:48:32,740 --> 00:48:33,970
you will recognised

836
00:48:34,000 --> 00:48:37,290
is the first derivative of velocity versus time

837
00:48:37,460 --> 00:48:38,740
which is also

838
00:48:38,740 --> 00:48:42,130
the second derivative of position versus time

839
00:48:42,150 --> 00:48:44,290
so here comes the second equation

840
00:48:44,290 --> 00:48:46,550
i really want you to remember

841
00:48:46,550 --> 00:48:47,450
four ever

842
00:48:47,460 --> 00:48:49,220
and ever and ever

843
00:48:49,240 --> 00:48:51,030
that the acceleration

844
00:48:52,230 --> 00:48:54,790
which is also

845
00:48:54,820 --> 00:48:56,760
two x

846
00:48:56,800 --> 00:49:02,540
you can square

847
00:49:02,560 --> 00:49:03,490
we can

848
00:49:03,500 --> 00:49:05,260
go to plot

849
00:49:05,310 --> 00:49:07,750
and we can ask ourselves the question now

850
00:49:07,790 --> 00:49:10,430
where is the acceleration zero

851
00:49:10,440 --> 00:49:14,800
where larger than zero and where is it smaller than zero because this value can

852
00:49:15,360 --> 00:49:17,440
larger than zero equal to zero

853
00:49:17,450 --> 00:49:19,150
and smaller than zero

854
00:49:20,290 --> 00:49:21,760
you have to be

855
00:49:21,810 --> 00:49:25,480
very careful when you try to derive that from this plot

856
00:49:25,570 --> 00:49:27,620
you have to be very careful

857
00:49:27,680 --> 00:49:30,760
because you and i have no good feeling for second derivative

858
00:49:30,770 --> 00:49:34,280
velocity is easy all you have to do is looking at all far

859
00:49:34,290 --> 00:49:37,360
but when it comes to the second derivative you have to see how of is

860
00:49:41,630 --> 00:49:43,860
right here

861
00:49:43,950 --> 00:49:46,260
the velocity is not changing

862
00:49:46,290 --> 00:49:50,320
sorry acceleration everywhere you must be zero

863
00:49:50,370 --> 00:49:53,230
here the velocity is increasing

864
00:49:53,270 --> 00:49:55,910
thirty acceleration must be larger than

865
00:49:55,930 --> 00:49:58,200
zero here

866
00:49:59,260 --> 00:50:01,610
the velocity is almost constant

867
00:50:01,630 --> 00:50:03,910
is almost a straight line

868
00:50:03,960 --> 00:50:05,030
what does that mean

869
00:50:05,080 --> 00:50:07,080
forty acceleration

870
00:50:09,300 --> 00:50:11,910
here what makes this rounding curve

871
00:50:11,960 --> 00:50:15,250
the velocities positive feedback but on this side is negative

872
00:50:15,280 --> 00:50:17,810
what does it mean for the acceleration

873
00:50:17,830 --> 00:50:19,250
negative you got it

874
00:50:19,250 --> 00:50:20,270
so you can now

875
00:50:20,280 --> 00:50:21,530
roughly find

876
00:50:22,480 --> 00:50:26,050
the acceleration is positive whereas negative

877
00:50:26,060 --> 00:50:27,060
and where

878
00:50:27,070 --> 00:50:28,020
it is

879
00:50:31,820 --> 00:50:33,020
let's do

880
00:50:33,070 --> 00:50:34,780
straightforward example

881
00:50:34,800 --> 00:50:35,540
the way

882
00:50:35,540 --> 00:50:37,310
that you could expect it

883
00:50:37,330 --> 00:50:39,240
on an assignment

884
00:50:40,600 --> 00:50:42,820
if you are extraordinarily lucky

885
00:50:42,910 --> 00:50:45,250
you might even get something like that

886
00:50:45,280 --> 00:50:47,210
on exam

887
00:50:47,250 --> 00:50:49,800
very straightforward i'm going to give you

888
00:50:49,850 --> 00:50:51,460
the position x

889
00:50:51,500 --> 00:50:53,550
as a function of time

890
00:50:53,570 --> 00:50:56,090
and then ask you lots of questions

891
00:50:56,140 --> 00:50:57,640
about it

892
00:50:57,650 --> 00:51:00,220
so it is example is working example

893
00:51:01,760 --> 00:51:05,540
he calls eight

894
00:51:05,550 --> 00:51:08,240
minus sixty

895
00:51:08,240 --> 00:51:10,490
plus the square

896
00:51:12,050 --> 00:51:15,820
so this tells you whether the object is at any moment in time

897
00:51:15,870 --> 00:51:19,290
let this be in meters

898
00:51:19,320 --> 00:51:22,880
what now is the velocity at any moment in time

899
00:51:22,890 --> 00:51:24,780
well that's the

900
00:51:24,820 --> 00:51:27,110
the relative bxt t

901
00:51:27,190 --> 00:51:29,020
and i use

902
00:51:29,070 --> 00:51:31,060
the following

903
00:51:31,130 --> 00:51:33,230
x y equals

904
00:51:33,290 --> 00:51:34,910
two the power and

905
00:51:35,890 --> 00:51:37,240
most of you

906
00:51:37,270 --> 00:51:40,880
you know the xt is then and times t

907
00:51:40,880 --> 00:51:44,780
so that we can ensure that we can start behaving according to a new values

908
00:51:45,980 --> 00:51:51,050
and it was part of our strategy walking the dog tradition sharing and thirdly it

909
00:51:51,050 --> 00:51:53,100
was also for new innovations

910
00:51:54,740 --> 00:51:56,980
and as as the story

911
00:51:57,000 --> 00:52:01,840
and they well this is our nokia way jam looked like we were looking for

912
00:52:01,840 --> 00:52:06,500
creating new nokia how to role in consummated markets what does it mean to be

913
00:52:06,570 --> 00:52:12,700
leader leading and winning in devices and then accelerating business an option but in general

914
00:52:12,880 --> 00:52:17,600
and with adults course of IBM which was of course very important for us so

915
00:52:17,600 --> 00:52:22,380
we could engage more about i employees and what was you has been even more

916
00:52:22,380 --> 00:52:27,920
importantly to me and to us afterwards was that we started to use

917
00:52:27,960 --> 00:52:34,600
the nokia way jam content as the wealth of ideas so we had they're like

918
00:52:34,620 --> 00:52:37,860
seven thousand seven hundred posts

919
00:52:37,880 --> 00:52:42,100
so ideas all parts of dialogue and then we

920
00:52:42,140 --> 00:52:48,280
notice that twenty two present active posters against the readers these for some very high

921
00:52:48,280 --> 00:52:52,590
if you know that if you take an average discussion forum there is one idea

922
00:52:53,100 --> 00:52:58,100
it may mean that the one is posting that idea ten may comment and one

923
00:52:58,100 --> 00:53:02,420
hundred is following up the other thing is that the different these these the length

924
00:53:02,420 --> 00:53:06,220
of threads the dialogues were almost ten

925
00:53:06,360 --> 00:53:09,570
and that is again very good it means that we had to dial we just

926
00:53:09,840 --> 00:53:12,880
put up any any comments whatsoever

927
00:53:12,900 --> 00:53:18,080
and then what is also very nice that there was not the single proper unprepared

928
00:53:19,180 --> 00:53:24,640
but we were looking at that and our senior management was leading the way

929
00:53:24,720 --> 00:53:29,960
oh PK was spending seventeen hours during the seventy two hours i mean he has

930
00:53:29,960 --> 00:53:34,040
some of these things in his agenda plants that i think he had the privilege

931
00:53:34,040 --> 00:53:37,400
to sleep during those those hectic hours

932
00:53:37,760 --> 00:53:42,160
what we also did was that it it was it looked like a quite ordinary

933
00:53:42,200 --> 00:53:47,440
discussion forum and then we had our facilitators in each and every time zone we

934
00:53:47,440 --> 00:53:54,000
were monitoring the discussions and i called it positive manipulation so it is our expert

935
00:53:54,000 --> 00:53:59,460
presentations facilitators know it is very good idea they could eat more in from in

936
00:53:59,460 --> 00:54:03,700
the jam user interface so they were like the hot topics and they were not

937
00:54:03,700 --> 00:54:08,540
like like mechanism created what are the most on the core used conversations but it

938
00:54:08,540 --> 00:54:10,200
was really that hey

939
00:54:10,220 --> 00:54:15,840
let's put the wisdom of the crowds to the joints these topic and

940
00:54:15,860 --> 00:54:22,220
indeed pressure that it seems that it's like six months almost six months of the

941
00:54:22,220 --> 00:54:25,540
the jam i AM not

942
00:54:25,560 --> 00:54:29,840
honestly i'm not in the position now i'm not very open to tell the concrete

943
00:54:29,840 --> 00:54:36,260
results except that maybe one thing is something that would interest us so we noticed

944
00:54:36,260 --> 00:54:38,900
that people were a lot of discussing about what does it mean to be an

945
00:54:38,900 --> 00:54:40,380
internet company

946
00:54:40,420 --> 00:54:48,540
so we need like content analyses and created internet company star which i have eight

947
00:54:48,700 --> 00:54:53,900
arms or dimensions and this is how we are driving the change towards internet company

948
00:54:54,300 --> 00:54:59,240
we are using these for analysing not only our own attitudes but also the attitude

949
00:54:59,240 --> 00:55:05,300
of the team attitude of the group and open innovation is likely to do there

950
00:55:05,320 --> 00:55:07,960
a very strong clue indeed

951
00:55:08,260 --> 00:55:13,720
so this is basically that's what we succeeded in that part then the other one

952
00:55:13,720 --> 00:55:17,360
was the one on the case that i would like to share with you is

953
00:55:17,360 --> 00:55:22,780
our community forum nokia so when we launch this ovi service three months ago we

954
00:55:22,780 --> 00:55:25,040
had two point eight million

955
00:55:25,060 --> 00:55:27,420
believe that

956
00:55:27,540 --> 00:55:31,740
early birds and technical experts to really

957
00:55:31,760 --> 00:55:33,240
you know started

958
00:55:33,260 --> 00:55:37,340
developing and all the all the all the

959
00:55:37,380 --> 00:55:41,820
former not is our answer to open innovation

960
00:55:42,140 --> 00:55:48,140
and well our fifty six different ways of innovating

961
00:55:48,500 --> 00:55:53,180
inside the company and outside the company and to six ways of doing it a

962
00:55:53,180 --> 00:55:58,860
bit of a lot so there will be some open innovation strategies come soon so

963
00:55:58,860 --> 00:56:02,120
we've been released with spying on what do we have

964
00:56:02,160 --> 00:56:04,900
so it's very important for us

965
00:56:05,660 --> 00:56:11,500
leading the small communities and i was previously working for for a company which was

966
00:56:11,500 --> 00:56:12,700
in this to me

967
00:56:12,720 --> 00:56:16,660
in the field of multimedia e-learning insanity and

968
00:56:16,680 --> 00:56:22,060
danger is our biggest customer was not just far too big strategically for company very

969
00:56:23,100 --> 00:56:26,420
and i recall two thousand one two thousand two when we we had some kind

970
00:56:26,420 --> 00:56:28,120
of ideas in the field of content

971
00:56:29,300 --> 00:56:35,200
not find it interesting that they didn't really know how to approach it now in

972
00:56:35,200 --> 00:56:41,980
two thousand seven and colleague of mine learning and learning and e-learning community approached and

973
00:56:41,980 --> 00:56:48,340
asked that hey we wanna do some mobile learning in a in the n ninety

974
00:56:48,460 --> 00:56:54,820
in nineteen with the answer and n eighty which is our platform our tablet PC

975
00:56:55,440 --> 00:56:58,280
and i was kind of thing dr how should i help you know i know

976
00:56:58,280 --> 00:57:02,440
the answer mean we're not interested in content in forty five minutes i found the

977
00:57:02,440 --> 00:57:07,120
right people to have a meeting in ten days how to

978
00:57:07,140 --> 00:57:11,830
do how to do utilise with API how to find i mean the not first

979
00:57:11,830 --> 00:57:20,640
was offering different is you develop the the user interface statistical parts further and i

980
00:57:20,640 --> 00:57:26,080
was really amazed myself and all the links where they're all the help help for

981
00:57:26,160 --> 00:57:31,140
for the user of this being the multinational company training a lot of people all

982
00:57:31,640 --> 00:57:36,200
over the seas so that yet and they wanted to give each and every of

983
00:57:36,200 --> 00:57:41,560
their trainee for their distance-learning phase a tablet PC for doing it so i think

984
00:57:41,560 --> 00:57:47,260
we're really changing and we're really opening and this is how forum nokia feeds in

985
00:57:47,340 --> 00:57:54,860
ideas innovations incorporation so what happened to patterns what will happen with with PR's i

986
00:57:54,860 --> 00:57:56,940
don't know the answer about

987
00:57:57,260 --> 00:58:02,140
yes two minutes yes unfortunately i don't know the answer but i've seen this happening

988
00:58:02,140 --> 00:58:03,290
in practice

989
00:58:03,360 --> 00:58:06,300
and it's just fascinating

990
00:58:06,420 --> 00:58:10,380
to know this and to get the same enthusiasm and this is part of the

991
00:58:11,540 --> 00:58:13,660
that we are working internally now

992
00:58:13,660 --> 00:58:17,850
of finding good elimination ordering

993
00:58:17,910 --> 00:58:22,380
it is a complex question and i guess for arbitrary graphs finding the best elimination

994
00:58:22,380 --> 00:58:25,100
is in NP complete control

995
00:58:26,240 --> 00:58:32,880
those out the in practice usually

996
00:58:32,930 --> 00:58:37,520
you will see that you don't really use the elimination our as it is here

997
00:58:37,540 --> 00:58:41,180
user more sophisticated version

998
00:58:41,240 --> 00:58:43,000
the problem is that

999
00:58:43,060 --> 00:58:46,720
here i have a computer all these because they want to the a given query

1000
00:58:46,720 --> 00:58:47,880
p of x one

1001
00:58:47,970 --> 00:58:51,200
but what if i want to compute p of x sixty four p of x

1002
00:58:51,210 --> 00:58:52,570
three and p of x

1003
00:58:52,570 --> 00:58:54,540
we had to withdraw all this procedure

1004
00:58:56,850 --> 00:58:58,620
there's a nice little voice

1005
00:58:58,630 --> 00:59:01,210
that's called belief propagation

1006
00:59:01,300 --> 00:59:03,900
so basically

1007
00:59:03,970 --> 00:59:08,630
it is also called some problems with him sometimes

1008
00:59:08,630 --> 00:59:14,370
it does not repeat computations you see that if you don't do this european competition

1009
00:59:14,420 --> 00:59:16,660
for this list

1010
00:59:16,700 --> 00:59:22,600
and specifically designed for three structure graph

1011
00:59:22,670 --> 00:59:24,960
also see why that's the case

1012
00:59:26,020 --> 00:59:27,800
immediately here we have

1013
00:59:27,840 --> 00:59:28,820
more heavy

1014
00:59:28,900 --> 00:59:33,700
slide but let's try to understand what's going on

1015
00:59:33,740 --> 00:59:35,810
assume you want to do elimination

1016
00:59:38,510 --> 00:59:41,710
let's assume that we want to compute p of and

1017
00:59:41,720 --> 00:59:44,080
we have a changed here

1018
00:59:44,090 --> 00:59:47,230
this a very simple graphical model is just change

1019
00:59:48,160 --> 00:59:49,480
this is extant

1020
00:59:49,500 --> 00:59:53,400
we just want to compute the marginal that's OK

1021
00:59:53,410 --> 00:59:57,950
what's the marginal of xn is the sum of all the variables that are smaller

1022
00:59:57,950 --> 01:00:00,000
than xn

1023
01:00:00,050 --> 01:00:03,010
and the virus that are larger than xm

1024
01:00:03,050 --> 01:00:06,100
of the joint probability distribution

1025
01:00:06,110 --> 01:00:09,430
which is given by these according to the home sixty four

1026
01:00:09,480 --> 01:00:13,160
we just take the maximal cliques which are i and i was one

1027
01:00:15,900 --> 01:00:20,050
now we just separate these problems into two problem

1028
01:00:20,050 --> 01:00:21,810
one problem

1029
01:00:21,870 --> 01:00:23,420
for the left hand side

1030
01:00:23,440 --> 01:00:27,400
and the problem with the right hand side

1031
01:00:28,160 --> 01:00:30,680
and this

1032
01:00:30,740 --> 01:00:32,610
now we observed that

1033
01:00:32,650 --> 01:00:35,610
devi temples in here

1034
01:00:35,660 --> 01:00:40,430
and the violence and here can be separated in this

1035
01:00:40,440 --> 01:00:46,090
we bring the violence here to this violin

1036
01:00:49,270 --> 01:00:52,050
now immediately

1037
01:00:52,090 --> 01:00:54,000
i have to solve this

1038
01:00:54,060 --> 01:00:56,820
two problems here

1039
01:00:56,830 --> 01:01:00,520
in order to compute p of xn

1040
01:01:03,110 --> 01:01:05,670
how to compute these problem here well

1041
01:01:05,690 --> 01:01:08,470
we've already seen how to compute this problem

1042
01:01:08,510 --> 01:01:10,590
a very simple example

1043
01:01:10,650 --> 01:01:14,830
we just use the distributive law

1044
01:01:14,880 --> 01:01:16,170
and you push

1045
01:01:17,060 --> 01:01:20,890
since we summing up two x input minus one

1046
01:01:20,940 --> 01:01:24,140
we push this sums as far as it can go

1047
01:01:24,180 --> 01:01:27,970
for example push this in mind is one of the last

1048
01:01:28,060 --> 01:01:30,010
factor which is

1049
01:01:30,020 --> 01:01:32,620
x i minus

1050
01:01:32,660 --> 01:01:38,240
two and i n minus two and minus one

1051
01:01:38,260 --> 01:01:40,900
then to compute that some

1052
01:01:40,970 --> 01:01:43,210
and they come back and compute

1053
01:01:43,260 --> 01:01:46,900
take the problem to compute something present that something

1054
01:01:46,990 --> 01:01:50,220
so basically it will be only quadratic in the size of the table here if

1055
01:01:50,220 --> 01:01:53,820
i have a tabular representation for function

1056
01:01:53,820 --> 01:01:56,630
it will be just linear in the number of

1057
01:01:56,680 --> 01:01:59,450
factors because i have to compute

1058
01:01:59,580 --> 01:02:02,040
is many times

1059
01:02:02,090 --> 01:02:09,180
well i have the same computational complexity here because the problem is identical just magically

1060
01:02:09,970 --> 01:02:11,990
what's going on here is that

1061
01:02:12,060 --> 01:02:17,240
i actually in the process of computing p of xn

1062
01:02:17,330 --> 01:02:19,380
i compute these local

1063
01:02:19,390 --> 01:02:21,920
summations that they store

1064
01:02:22,010 --> 01:02:24,680
in a viable so they call

1065
01:02:24,690 --> 01:02:25,940
nu alpha

1066
01:02:25,990 --> 01:02:31,410
or music

1067
01:02:31,450 --> 01:02:33,770
and then if you think what's happening here

1068
01:02:33,820 --> 01:02:36,180
what p of x in these

1069
01:02:39,670 --> 01:02:44,140
it's just the product of this incoming message

1070
01:02:44,180 --> 01:02:45,540
as we call

1071
01:02:45,580 --> 01:02:48,660
two x and coming from the left

1072
01:02:48,710 --> 01:02:50,900
this is mu alpha

1073
01:02:50,910 --> 01:02:52,720
of xn

1074
01:02:52,730 --> 01:02:54,620
and this is mu beta effects

1075
01:02:54,620 --> 01:02:57,630
so we have constructed thing the way that

1076
01:02:57,680 --> 01:02:59,950
the marginal for these

1077
01:03:00,000 --> 01:03:02,130
but you know and

1078
01:03:02,190 --> 01:03:04,710
is just the product of this

1079
01:03:04,730 --> 01:03:06,140
with this

1080
01:03:06,180 --> 01:03:10,310
but in order to compute these any to compute first one

1081
01:03:10,350 --> 01:03:16,090
in order to compute using compute first one and this n is completely arbitrary right

1082
01:03:16,140 --> 01:03:20,420
so i can start with an in here if i started in here what we

1083
01:03:21,420 --> 01:03:26,900
well one of these terms immediately is of p

1084
01:03:26,910 --> 01:03:29,600
and they only have these problems so i started that

1085
01:03:29,610 --> 01:03:30,770
in other words

1086
01:03:30,770 --> 01:03:34,660
look at citation networks you can look at the co authorship networks you can look

1087
01:03:34,660 --> 01:03:39,560
at section that networks where an edge means that some section interaction or something and

1088
01:03:39,560 --> 01:03:42,100
you can look at also clickstream data

1089
01:03:45,720 --> 01:03:48,970
how do you create the network out of text

1090
01:03:56,360 --> 01:03:58,000
people also that

1091
01:04:02,420 --> 01:04:03,320
i think so

1092
01:04:03,350 --> 01:04:06,780
but just like any then you can just like you you can create a network

1093
01:04:06,780 --> 01:04:09,150
of of everything and then it's like

1094
01:04:09,190 --> 01:04:10,040
i don't know

1095
01:04:10,130 --> 01:04:13,460
i mean because because the think that

1096
01:04:13,590 --> 01:04:16,760
so that i could be good company

1097
01:04:17,010 --> 01:04:23,460
OK so right and there is a lot of also graph models that they also

1098
01:04:23,460 --> 01:04:28,070
i describe what is good and i described but preferential attachment this was developed for

1099
01:04:28,080 --> 01:04:34,040
power law degree distributions copying model is something that gives you the power law degree

1100
01:04:34,040 --> 01:04:40,000
distributions but also user communities we had also small world model i described and so

1101
01:04:40,000 --> 01:04:42,190
on and for example this one

1102
01:04:42,210 --> 01:04:43,340
i want to give you

1103
01:04:43,780 --> 01:04:48,840
we generated graph that there's a fight but the diameter is increasing and again you

1104
01:04:48,840 --> 01:04:52,380
just got from property but not the other and so on and so this is

1105
01:04:52,580 --> 01:04:55,370
the problem and here's here's one solution x o

1106
01:04:55,420 --> 01:04:57,050
you want to model the

1107
01:04:57,070 --> 01:05:00,580
for which can proven then fitted and does all these

1108
01:05:00,600 --> 01:05:03,770
the thing here is that this morning

1109
01:05:04,100 --> 01:05:06,510
have like a very nice

1110
01:05:06,550 --> 01:05:08,530
generative semantics OK

1111
01:05:09,240 --> 01:05:10,590
it will be like

1112
01:05:10,610 --> 01:05:14,570
something that you could say is happening in the nature in some sense

1113
01:05:15,110 --> 01:05:18,780
and let's let me define this chronicle model kronecker graphs

1114
01:05:18,820 --> 01:05:23,480
and what we basically do we will we will try to generate our graph recursively

1115
01:05:24,080 --> 01:05:27,800
let's say we we do it because you that so we start with some initial

1116
01:05:27,800 --> 01:05:33,500
graph and then somehow recursively expand nodes of this graph to get larger and larger

1117
01:05:33,500 --> 01:05:36,590
versions of the graph that and here is an example of what we doing so

1118
01:05:36,590 --> 01:05:40,910
basically the tensor product of graph adjacency matrix is that this is the graph adjacency

1119
01:05:40,910 --> 01:05:42,630
matrix of the following

1120
01:05:42,680 --> 01:05:46,780
i go and expand each node here we don't miniature copy of the graph itself

1121
01:05:47,050 --> 01:05:50,660
and then introducing some additional edges and this is exactly the process if i think

1122
01:05:50,660 --> 01:05:55,840
this matrix and tensor multiplied minutes which means that i think the constant here and

1123
01:05:55,840 --> 01:05:59,910
it and multiply the whole matrix so here i get g one and here i

1124
01:05:59,970 --> 01:06:03,770
get zero times g one which is zero OK and i'm just doing

1125
01:06:03,810 --> 01:06:10,270
one of the exactly nothing at this operation

1126
01:06:10,320 --> 01:06:11,790
would be

1127
01:06:13,360 --> 01:06:15,820
he met his vision

1128
01:06:15,840 --> 01:06:22,300
for the the the this is how it i mean this mathematicians have defined is

1129
01:06:22,300 --> 01:06:26,260
a long time ago i mean this is a kronecker product usually have two types

1130
01:06:26,260 --> 01:06:30,260
of products between graphs one is this that the product of kronecker product the other

1131
01:06:30,260 --> 01:06:33,430
the other one is cartesian product which doesn't really

1132
01:06:36,740 --> 01:06:41,240
right so here is i could do one that i then i have multiplied itself

1133
01:06:41,250 --> 01:06:45,260
gadget to if i would know which obliged to read itself and this is what

1134
01:06:45,260 --> 01:06:49,490
i would go so i will be getting some kind of nice i know similar

1135
01:06:49,510 --> 01:06:51,510
structure like

1136
01:06:51,570 --> 01:06:53,350
and so

1137
01:06:53,360 --> 01:06:57,250
right we start with g one on n one and you want and just recorded

1138
01:06:57,300 --> 01:06:59,780
initiator and then we get

1139
01:07:00,290 --> 01:07:04,650
sequence of growing graphs each one we have and one to the k

1140
01:07:04,700 --> 01:07:05,870
known structure

1141
01:07:06,690 --> 01:07:10,210
the size of the matrix article history is not nine

1142
01:07:11,110 --> 01:07:16,420
just how the product works and then four so this is the model

1143
01:07:16,420 --> 01:07:18,970
and here is where he is more precisely what

1144
01:07:19,510 --> 01:07:23,960
the tensor product or kronecker product of two matrices it is but if i have

1145
01:07:24,020 --> 01:07:28,680
a and b i just think the elements of faith and like speaking the the

1146
01:07:28,690 --> 01:07:30,730
whole the matrix OK

1147
01:07:30,770 --> 01:07:33,490
and here is that if if these are the sizes of a and b this

1148
01:07:33,490 --> 01:07:37,590
is the size of the of the problem so the the matrix is growing

1149
01:07:37,610 --> 01:07:42,080
i explained this right so we get case chronicle part the just

1150
01:07:42,180 --> 01:07:44,490
multiplying g one which itself

1151
01:07:44,510 --> 01:07:50,380
so the intuition you can have is basically having this because you've growth of communities

1152
01:07:50,380 --> 01:07:51,950
with every

1153
01:07:51,950 --> 01:07:58,200
we're right this is like my our building block and now every node in the

1154
01:07:58,220 --> 01:08:02,650
in the network gets like expand we we is building block right and this continues

1155
01:08:05,190 --> 01:08:06,910
the other thing is

1156
01:08:06,920 --> 01:08:10,320
what i want to show you here this is all deterministic right so i have

1157
01:08:11,470 --> 01:08:17,290
matrix here it is zero sum someone so it's completely deterministic process how i can

1158
01:08:17,300 --> 01:08:19,720
analyse it

1159
01:08:19,730 --> 01:08:23,760
is is here so this is the idea how to get nonstochastic were how to

1160
01:08:23,760 --> 01:08:28,230
get around the world of of the and the idea is that instead of starting

1161
01:08:28,230 --> 01:08:32,250
with zero one matrix i we start with some probability matrix were here now every

1162
01:08:33,130 --> 01:08:36,970
is is the probability so it's between zero and one in the sense the need

1163
01:08:36,970 --> 01:08:40,630
to sum to one that just going on what is an arbitrary number between zero

1164
01:08:40,630 --> 01:08:44,230
and one i can now kronecker multiplied this mean itself

1165
01:08:45,560 --> 01:08:50,150
i get some probability right so for example here i get get point five square

1166
01:08:50,150 --> 01:08:54,590
here get point five times point two and so on right now i can i

1167
01:08:54,590 --> 01:08:55,740
i will

1168
01:08:57,590 --> 01:08:59,360
the ability

1169
01:08:59,420 --> 01:09:01,480
office circuit

1170
01:09:01,540 --> 01:09:03,570
to fight the magnetic flux

1171
01:09:04,660 --> 01:09:06,560
it's produced by the

1172
01:09:06,570 --> 01:09:09,000
circuits themselves

1173
01:09:09,020 --> 01:09:11,950
if you have a circuit that you want to current through the circuit

1174
01:09:12,000 --> 01:09:14,380
you create magnetic field

1175
01:09:14,430 --> 01:09:18,280
and if the currents are changing the magnetic field to change changing

1176
01:09:18,320 --> 01:09:21,500
and so there will be an induced EMF in that circuit

1177
01:09:21,500 --> 01:09:22,640
that fights

1178
01:09:22,650 --> 01:09:24,040
the change

1179
01:09:24,050 --> 01:09:25,430
and we express that

1180
01:09:25,440 --> 01:09:30,320
in terms of a self inductance l

1181
01:09:30,330 --> 01:09:35,550
self inductances

1182
01:09:35,570 --> 01:09:38,610
and would sell speaks for itself

1183
01:09:38,610 --> 01:09:42,490
it's doing it's to itself

1184
01:09:42,500 --> 01:09:44,250
magnetic flux

1185
01:09:44,260 --> 01:09:45,900
that is produced by

1186
01:09:45,960 --> 01:09:51,380
the circuit is always proportional to the current u double the current magnetic flux doubles

1187
01:09:51,410 --> 01:09:53,880
so does the proportionality constant

1188
01:09:53,890 --> 01:09:55,050
that we call

1189
01:09:55,070 --> 01:09:56,960
that is the self inductance

1190
01:09:57,020 --> 01:09:58,630
and so therefore the

1191
01:09:58,680 --> 01:10:00,690
the induced EMF

1192
01:10:04,850 --> 01:10:07,070
the phi dt

1193
01:10:07,110 --> 01:10:09,290
that is found a floor

1194
01:10:09,300 --> 01:10:11,860
so that becomes minus al

1195
01:10:11,910 --> 01:10:13,660
the ITT

1196
01:10:17,710 --> 01:10:20,350
it's only a matter of geometry

1197
01:10:20,350 --> 01:10:24,290
l is not a function of the current itself

1198
01:10:24,330 --> 01:10:28,720
i will calculate for you are very simple case of the self inductance

1199
01:10:28,800 --> 01:10:32,990
a of the solenoid

1200
01:10:33,080 --> 01:10:36,630
this be solenoidal and this is a closed circuit

1201
01:10:36,680 --> 01:10:39,100
and we want the current i through the solenoid

1202
01:10:39,270 --> 01:10:41,360
and the radius

1203
01:10:41,410 --> 01:10:45,100
of these windings is or

1204
01:10:45,160 --> 01:10:47,740
let's say said and windings

1205
01:10:48,740 --> 01:10:52,460
the length of the solenoid is the well

1206
01:10:52,520 --> 01:10:54,090
have you remember

1207
01:10:54,150 --> 01:10:57,020
that we only have derived using

1208
01:10:57,200 --> 01:11:00,260
if the magnetic field inside the solenoid

1209
01:11:00,270 --> 01:11:02,560
his new zero

1210
01:11:02,580 --> 01:11:04,550
times i

1211
01:11:04,610 --> 01:11:11,310
i'm capital and divided by all this is the number of windings parameter

1212
01:11:11,330 --> 01:11:12,800
if we attached

1213
01:11:12,960 --> 01:11:14,900
open surface duties

1214
01:11:14,960 --> 01:11:16,020
closed loop

1215
01:11:16,060 --> 01:11:19,860
very difficult to imagine what that open surface looks like we we discussed it many

1216
01:11:20,860 --> 01:11:25,680
inside this solenoid you have sort of staircase like of surface

1217
01:11:25,730 --> 01:11:31,340
that magnetic field penetrates that surface and times because you have and groups

1218
01:11:31,360 --> 01:11:34,210
so the magnetic flux

1219
01:11:34,260 --> 01:11:35,830
if i being

1220
01:11:35,920 --> 01:11:37,280
is simply the

1221
01:11:37,300 --> 01:11:38,800
every year

1222
01:11:38,860 --> 01:11:42,390
by the laws craig which is the surface area of one little because i saw

1223
01:11:42,520 --> 01:11:45,770
the edit field is constant inside the solenoid

1224
01:11:45,800 --> 01:11:50,050
and i still that is zero outside which is a very good approximation

1225
01:11:50,080 --> 01:11:52,150
so we get by the are square

1226
01:11:52,150 --> 01:11:54,010
the surface area of one loop

1227
01:11:54,020 --> 01:11:55,860
but we have n loops

1228
01:11:55,890 --> 01:11:58,370
and then we have to multiply by a constant

1229
01:11:58,430 --> 01:12:02,080
magnetic field so ligands create because we have and here

1230
01:12:02,210 --> 01:12:03,890
zero i

1231
01:12:03,900 --> 01:12:07,580
divided by l

1232
01:12:07,620 --> 01:12:08,740
and this

1233
01:12:08,770 --> 01:12:09,860
we call

1234
01:12:09,920 --> 01:12:13,740
well i don't definition for self inductance

1235
01:12:13,780 --> 01:12:16,520
so to self inductance l

1236
01:12:16,700 --> 01:12:18,110
purely geometry

1237
01:12:18,170 --> 01:12:20,330
by the last created

1238
01:12:20,330 --> 01:12:22,020
capitol and square

1239
01:12:22,030 --> 01:12:23,650
divided by l

1240
01:12:23,680 --> 01:12:25,210
and zero

1241
01:12:25,230 --> 01:12:33,180
check this

1242
01:12:33,200 --> 01:12:34,670
while little was created

1243
01:12:34,680 --> 01:12:36,710
the capital and squared

1244
01:12:38,730 --> 01:12:40,490
that's correct

1245
01:12:40,550 --> 01:12:42,240
divided by the way

1246
01:12:42,240 --> 01:12:45,050
so we can calculate for instance what

1247
01:12:45,150 --> 01:12:47,150
the self inductances

1248
01:12:47,180 --> 01:12:48,700
four a

1249
01:12:48,740 --> 01:12:53,520
solenoid that we have used in class several times

1250
01:12:53,530 --> 01:12:57,960
we had one whereby we twenty eight hundred windings

1251
01:12:58,020 --> 01:13:01,460
are i think something like five centimetres

1252
01:13:01,520 --> 01:13:04,500
after work as i of course be careful

1253
01:13:05,900 --> 01:13:08,020
we had a length

1254
01:13:08,080 --> 01:13:11,330
o point six metres

1255
01:13:11,380 --> 01:13:13,620
we had several times on here

1256
01:13:13,650 --> 01:13:16,480
and if you substitute those numbers in there

1257
01:13:16,500 --> 01:13:18,690
you'll find that the self inductance

1258
01:13:18,710 --> 01:13:20,650
all that solenoid

1259
01:13:20,660 --> 01:13:22,140
is o point one

1260
01:13:23,330 --> 01:13:24,590
as i units

1261
01:13:24,640 --> 01:13:29,630
and we call the as i units henry capital h

1262
01:13:29,670 --> 01:13:32,390
it will be the same as for seconds

1263
01:13:32,450 --> 01:13:36,140
and here but no one would ever use that to call them

1264
01:13:40,480 --> 01:13:41,910
every circuit

1265
01:13:41,940 --> 01:13:47,020
has a finite value for the self inductance however small that may be sometimes it's

1266
01:13:47,020 --> 01:13:49,780
so small that we ignore it

1267
01:13:49,870 --> 01:13:52,410
but if you take the simple

1268
01:13:52,460 --> 01:13:56,670
the same simple current just won why it goes around whether it is

1269
01:13:56,700 --> 01:14:01,790
rectangle whether it circle doesn't make any difference it always produces a magnetic field

1270
01:14:01,890 --> 01:14:05,950
always for those magnetic flux through the surface and so it always has the finite

1271
01:14:05,980 --> 01:14:11,840
self inductance maybe only nine nano and may be only micro henry's but it's never

1272
01:14:16,100 --> 01:14:17,960
so now what i want to do

1273
01:14:17,980 --> 01:14:20,950
is to show you remarkable

1274
01:14:21,000 --> 01:14:24,370
consequences of the presence of

1275
01:14:24,500 --> 01:14:26,150
self inductor

1276
01:14:26,150 --> 01:14:27,880
in a circle

1277
01:14:27,910 --> 01:14:29,830
and i started very simple

1278
01:14:29,920 --> 01:14:31,650
i have your battery

1279
01:14:31,670 --> 01:14:32,830
which has

1280
01:14:35,250 --> 01:14:37,290
i have is which

1281
01:14:37,310 --> 01:14:39,350
and here i was helping doctors

1282
01:14:39,370 --> 01:14:40,700
we always

1283
01:14:40,800 --> 01:14:43,200
draw cell conductor in a circuit

1284
01:14:43,210 --> 01:14:45,620
with these corals

1285
01:14:45,670 --> 01:14:48,890
and we also have in series

1286
01:14:51,390 --> 01:14:53,780
which are always indicated with this

1287
01:14:53,830 --> 01:14:58,770
and i close

1288
01:14:58,780 --> 01:15:00,170
this which

1289
01:15:00,190 --> 01:15:03,150
when there is no current running

1290
01:15:03,160 --> 01:15:06,830
in other words at time t equal zero when i close to switch

1291
01:15:06,890 --> 01:15:09,460
there is no current

1292
01:15:09,520 --> 01:15:11,510
when i close this which

1293
01:15:11,540 --> 01:15:15,580
the current wants to increase the self inductances

1294
01:15:16,970 --> 01:15:18,960
easy lens law

1295
01:15:18,960 --> 01:15:20,630
i don't like the change

1296
01:15:20,640 --> 01:15:23,910
of such occurrence of the self inductances fighting

1297
01:15:23,960 --> 01:15:27,080
the current wants to go through it

1298
01:15:27,090 --> 01:15:29,130
the concert time

1299
01:15:29,160 --> 01:15:33,700
that the self inductance loses the fight if you wait long enough

1300
01:15:33,700 --> 01:15:35,710
and these are so these are separate

1301
01:15:35,800 --> 01:15:40,670
gas and some of the files to do all these things together it's again again

1302
01:15:40,730 --> 01:15:45,750
and then i need to normalize out the context and that gives me a new

1303
01:15:45,750 --> 01:15:47,980
gas distribution and then just replace

1304
01:15:48,000 --> 01:15:51,540
the factor here by that gas in distribution and move on to the next one

1305
01:15:51,670 --> 01:15:53,560
right ninety two

1306
01:15:55,250 --> 01:16:00,410
we and especially in a situation here because the because the the likelihood function which

1307
01:16:00,410 --> 01:16:05,170
we are having at having trouble with is chosen to be the cumulative gaussians and

1308
01:16:05,200 --> 01:16:09,470
the cumulative gas in his log concave or convex i always mix them up like

1309
01:16:09,580 --> 01:16:15,270
it falls down like that right so it means that when we doing the the

1310
01:16:15,270 --> 01:16:19,730
the this this step here where we multiplying by by by that distribution here we

1311
01:16:19,730 --> 01:16:25,470
don't have this negative variance problem because we know that the that the that the

1312
01:16:26,220 --> 01:16:29,750
likelihood function is always convex

1313
01:16:29,760 --> 01:16:33,730
OK so so so things go very nicely

1314
01:16:33,760 --> 01:16:38,440
so here have a plot of phi to work is that so that's that support

1315
01:16:38,460 --> 01:16:40,670
which is comparing running

1316
01:16:40,710 --> 01:16:46,050
the EP algorithm to try to find a posterior approximation and comparing that to using

1317
01:16:46,430 --> 01:16:48,800
the fastest method

1318
01:16:48,800 --> 01:16:51,490
OK i'm going to run out of time someone OK so that the three parts

1319
01:16:51,490 --> 01:16:57,340
here so the first part here is showing you the value of the marginal likelihood

1320
01:16:57,390 --> 01:17:01,420
so this so these are the two hyperparameters OK so sorry i'm new i'm doing

1321
01:17:01,420 --> 01:17:02,500
i'm using this for

1322
01:17:02,630 --> 01:17:07,380
did for handwritten digit classification OK i'm just using a very simple case where i'm

1323
01:17:07,380 --> 01:17:11,460
just trying to distinguish between three and five because so is very simple

1324
01:17:11,500 --> 01:17:17,090
classification problem binary classification and i have two parameters in my model so my gaussianprocess

1325
01:17:17,090 --> 01:17:20,970
two parameters as the lengthscale that says you know how close should things be before

1326
01:17:20,970 --> 01:17:23,170
there before the could which

1327
01:17:23,220 --> 01:17:28,380
and i have magnitude parameter saying is some if the if the underlying process that's

1328
01:17:28,380 --> 01:17:31,500
very extreme values in the probability get close to one right so you get very

1329
01:17:31,500 --> 01:17:35,730
confident predictions and if the manager is very small then you get predictions around fifty

1330
01:17:35,730 --> 01:17:40,660
fifty all the time right so the confidence is also encoded by a parameter

1331
01:17:40,670 --> 01:17:46,000
right so so here i show the value of the marginal likelihood so

1332
01:17:46,020 --> 01:17:51,170
that means when i showed this dataset have seven hundred training cases then i can

1333
01:17:51,170 --> 01:17:55,930
optimize with respect to the marginal likelihood and the marginal likelihood landscape i painstakingly computed

1334
01:17:55,930 --> 01:17:59,000
the marginal likelihood for every possible combination of the hyperparameters

1335
01:17:59,010 --> 01:18:01,920
OK and i show you landscape here so normally i would just go to the

1336
01:18:02,300 --> 01:18:05,670
to the most likely to the most likely model

1337
01:18:05,800 --> 01:18:06,890
and this is for

1338
01:18:06,920 --> 01:18:13,040
this is for you EP

1339
01:18:13,050 --> 01:18:20,340
and sorry this is for the light passes method this is EP

1340
01:18:20,340 --> 01:18:23,950
and this is based on the on mark marketeer monte carlo so here we tried

1341
01:18:23,950 --> 01:18:27,840
really really hard to run the markov chain for for very very long time to

1342
01:18:27,840 --> 01:18:32,870
compute the right answer to this intractable problems and you can see that some of

1343
01:18:32,870 --> 01:18:35,670
these things ought to be the same as the approximations were good

1344
01:18:35,790 --> 01:18:39,380
and you can see actually the EP approximation was very hard to read the numbers

1345
01:18:39,500 --> 01:18:43,130
the distance but the number of this is minus minus ninety two in his mind

1346
01:18:43,130 --> 01:18:46,550
and minus ninety two one of the same contours was in the same place and

1347
01:18:46,550 --> 01:18:50,630
you can see that these countries plots are almost exactly the same like this one

1348
01:18:50,630 --> 01:18:54,760
already has a little bit wobbly because they used MCMC so i'm only getting estimates

1349
01:18:54,760 --> 01:18:59,420
up to i guess about plus and minus one in the in the log domain

1350
01:18:59,460 --> 01:19:03,510
so but this does not tell us anything about whether whether this could be because

1351
01:19:03,510 --> 01:19:09,250
they this could be exactly the right answer that's been data perfectly agrees with this

1352
01:19:09,330 --> 01:19:14,160
whereas past method doesn't seem to be doing as well it doesn't which is high

1353
01:19:14,170 --> 01:19:18,800
the marginal likelihood and also the shape of the contours is different and the maximum

1354
01:19:18,800 --> 01:19:20,750
maximum is in a different place

1355
01:19:20,960 --> 01:19:25,750
the down here and looking at the predictive performance and actually looking at the

1356
01:19:26,330 --> 01:19:27,830
the amount of information

1357
01:19:27,840 --> 01:19:30,670
that the predictive distribution has about the right target

1358
01:19:30,710 --> 01:19:32,480
in this case i get up to

1359
01:19:32,550 --> 01:19:36,800
o point eight four bits of information so the maximum amount of information you could

1360
01:19:36,800 --> 01:19:37,880
have only one bit

1361
01:19:37,920 --> 01:19:43,910
and now the corresponding to if you're always exactly right and you always completely confident

1362
01:19:44,680 --> 01:19:47,430
and zero bits of information would be

1363
01:19:47,440 --> 01:19:49,560
if you always a fifty fifty

1364
01:19:49,840 --> 01:19:54,590
so here we get more information on the predictive distribution also agrees very well with

1365
01:19:54,620 --> 01:19:56,620
the with a markov chain

1366
01:19:56,630 --> 01:19:58,660
so now we can ask the question

1367
01:19:58,680 --> 01:20:03,620
you know what what do they want to the posterior distributions look like why is

1368
01:20:03,620 --> 01:20:08,330
it that that passes method comes up with with a bad approximation or worse approximation

1369
01:20:08,330 --> 01:20:12,130
why does EP come up with with a with a good approximation to not have

1370
01:20:12,130 --> 01:20:15,840
the problem that my gas distribution is seven hundred dimensional

1371
01:20:15,850 --> 01:20:18,590
right so i can i can support it

1372
01:20:18,600 --> 01:20:23,590
so what i've plotted instead of plotted uploaded to of the marginals

1373
01:20:23,620 --> 01:20:27,010
right so i have a seven hundred dimensional distribution and i just want to other

1374
01:20:27,020 --> 01:20:32,170
models i looked through all the models and found the marginals that with one module

1375
01:20:32,170 --> 01:20:34,210
which was the least gaussians

1376
01:20:34,260 --> 01:20:37,620
and the way i did that was by using the markov chain so in the

1377
01:20:37,620 --> 01:20:40,290
set we tried to

1378
01:20:40,300 --> 01:20:43,750
we tried to get several clusterings of our dataset and we don't really want to

1379
01:20:43,750 --> 01:20:46,280
choose one in the end so we just want to look at our data from

1380
01:20:46,330 --> 01:20:48,350
different point of view

1381
01:20:49,100 --> 01:20:54,870
and then we can be end up with a list of subspaces and corresponding clusters

1382
01:20:54,890 --> 01:20:57,350
the subspace is something like this is

1383
01:20:57,370 --> 01:21:01,780
the projection of the data to a few features some to do something like feature

1384
01:21:01,780 --> 01:21:03,020
selection and the new

1385
01:21:03,050 --> 01:21:07,050
cluster based on those features and there's no way to implement this is not the

1386
01:21:07,060 --> 01:21:10,970
crucial point and there are many implementation tricks so you can do that

1387
01:21:10,990 --> 01:21:13,250
again here references

1388
01:21:13,690 --> 01:21:21,990
OK it's something different class called clustering it's also called bi clustering

1389
01:21:22,000 --> 01:21:25,860
there you have yet again a different goal so what you want to do is

1390
01:21:26,360 --> 01:21:31,050
so in the case so i assume you have you have

1391
01:21:31,060 --> 01:21:35,160
two variables actually this is like we can take the text example we had before

1392
01:21:35,520 --> 01:21:39,530
so you have documents in your words in the document

1393
01:21:40,400 --> 01:21:44,290
you make the bag of words approach and you generate

1394
01:21:53,100 --> 01:21:57,100
you generate huge matrix

1395
01:21:57,110 --> 01:22:01,350
which contains throws it has document so you have document one document two and so

1396
01:22:02,040 --> 01:22:04,920
and you have all the words you one

1397
01:22:08,110 --> 01:22:12,730
and in this matrix is the term document matrix you just count how often does

1398
01:22:12,730 --> 01:22:14,910
word one current document one

1399
01:22:14,930 --> 01:22:17,930
three times maybe and were toolkit occurs

1400
01:22:17,940 --> 01:22:18,780
not at all

1401
01:22:18,790 --> 01:22:21,410
and so on to make this huge matrix

1402
01:22:21,420 --> 01:22:23,460
now i think

1403
01:22:23,480 --> 01:22:26,110
i want to cluster documents

1404
01:22:26,130 --> 01:22:29,940
based on the statistics which words they have so what in the end you would

1405
01:22:29,940 --> 01:22:34,320
get something like if i have documents which are about politics i don't know the

1406
01:22:34,320 --> 01:22:39,490
name of of certain politicians will appear very often so you can cluster the like

1407
01:22:39,500 --> 01:22:43,410
looking at the world politicians will help to just documents

1408
01:22:43,420 --> 01:22:46,900
and of course you could also ask the other way around so which are the

1409
01:22:46,900 --> 01:22:48,550
words which are really

1410
01:22:48,560 --> 01:22:50,740
the important ones if i want to figure out

1411
01:22:50,760 --> 01:22:52,750
which text is about politics

1412
01:22:52,770 --> 01:22:57,680
so we can so you either contested documents based on rafts or you can cluster

1413
01:22:57,680 --> 01:23:02,170
words based on documents and also the dual viewpoint on

1414
01:23:02,180 --> 01:23:05,400
now what co clustering actually tries to do is

1415
01:23:05,420 --> 01:23:08,210
he tries to do both at the same time so the goal is in the

1416
01:23:08,210 --> 01:23:10,330
end you want to have

1417
01:23:10,370 --> 01:23:13,410
you want to find clusters such that

1418
01:23:13,420 --> 01:23:18,220
like the clusters of documents and clusters of works that if you want to identify

1419
01:23:18,220 --> 01:23:22,370
groups of documents and groups of words which can be used to describe the school

1420
01:23:22,370 --> 01:23:23,570
of documents

1421
01:23:23,590 --> 01:23:27,220
so ideally what you get is to re order

1422
01:23:28,250 --> 01:23:31,810
after clustering if you really seems to have something

1423
01:23:31,890 --> 01:23:35,820
so long as i mean that's really ideal situation something that they are can also

1424
01:23:35,980 --> 01:23:37,750
here we have all words

1425
01:23:37,770 --> 01:23:41,820
appearing in this class of documents

1426
01:23:41,840 --> 01:23:45,290
and again that's

1427
01:23:45,310 --> 01:23:48,010
the problem of solving this is called co clustering

1428
01:23:48,060 --> 01:23:52,540
and usually what people do is they their heuristic again so he start

1429
01:23:52,550 --> 01:23:55,510
clustering say the documents

1430
01:23:55,530 --> 01:23:56,670
and you obtain

1431
01:23:57,620 --> 01:24:00,330
so you just starting to obtain that

1432
01:24:00,340 --> 01:24:03,660
all those documents should be in the same cluster

1433
01:24:03,670 --> 01:24:08,610
now you reorder the matrix according to this clustering and then cluster the workspace so

1434
01:24:08,610 --> 01:24:12,990
you will then take those clusters is features in

1435
01:24:13,010 --> 01:24:15,830
the word domain the new cluster the words based on

1436
01:24:15,850 --> 01:24:22,120
those documents and you you go back and forth until convergence

1437
01:24:22,130 --> 01:24:26,340
that's just a different approach to clustering and then of course you could also imagine

1438
01:24:26,610 --> 01:24:28,000
that may be

1439
01:24:28,010 --> 01:24:31,240
this is not just block diagonal thing but it could be that there are certain

1440
01:24:31,240 --> 01:24:36,060
words that on this thing you might have actually subclusters so might look like this

1441
01:24:36,060 --> 01:24:38,890
we might even have something like this so it's

1442
01:24:38,900 --> 01:24:43,630
so you could have more structure than just block their structure

1443
01:24:44,460 --> 01:24:50,880
you mean in the in the in

1444
01:24:52,300 --> 01:24:56,120
well actually you what you do is you change the order of so you first

1445
01:24:56,120 --> 01:24:58,730
cluster the documents and then obtain i don't

1446
01:24:58,740 --> 01:25:03,180
doc one doc eleven talk seven top ten

1447
01:25:03,230 --> 01:25:05,290
the the first first and then

1448
01:25:05,300 --> 01:25:08,870
the for documents three

1449
01:25:08,970 --> 01:25:10,920
five seconds

1450
01:25:10,930 --> 01:25:14,040
now what you do is you change

1451
01:25:14,710 --> 01:25:18,890
this is the key by but one two three

1452
01:25:18,910 --> 01:25:22,290
but now the numbers right into your matrix

1453
01:25:22,310 --> 01:25:24,390
you can't often does but

1454
01:25:24,410 --> 01:25:28,450
one occur in any of the documents of one so you make an aggregation sort

1455
01:25:28,460 --> 01:25:29,700
of information

1456
01:25:29,830 --> 01:25:31,690
which is in those rows

1457
01:25:31,800 --> 01:25:35,820
so maybe you get fifteen are because you have very documents

1458
01:25:35,880 --> 01:25:39,570
you do the same here so the matrix so you are that i forgot to

1459
01:25:39,570 --> 01:25:42,920
tell the story you are not but the aggregated information

1460
01:25:42,930 --> 01:25:45,100
and then you cluster the words

1461
01:25:45,120 --> 01:25:46,200
and then you

1462
01:25:46,260 --> 01:25:50,350
you do it i mean the other way round you have no clusters of france

1463
01:25:50,350 --> 01:25:53,690
one test the second test and then

1464
01:25:53,790 --> 01:25:59,830
you again take the the rows of this matrix individually and clustering based

1465
01:25:59,840 --> 01:26:07,630
so you always use if you want to cluster more than just one variable

1466
01:26:07,650 --> 01:26:08,990
with respect to each other

1467
01:26:09,010 --> 01:26:11,520
and of course you can think of doing this and more

1468
01:26:11,640 --> 01:26:19,140
then mentions but like having three different objects but it gets very complicated

1469
01:26:19,140 --> 01:26:20,070
OK so

1470
01:26:20,080 --> 01:26:22,430
the main point is i mean they really really

1471
01:26:22,450 --> 01:26:27,090
thousands of clustering algorithms and there are lots of different paradigms how you

1472
01:26:27,120 --> 01:26:28,710
you could cite it right

1473
01:26:29,650 --> 01:26:31,410
so now there's not really

1474
01:26:31,420 --> 01:26:35,450
it seems so far there is nothing which is really a common so you try

1475
01:26:35,500 --> 01:26:39,460
just to do something which call clustering

1476
01:26:39,480 --> 01:26:41,930
and it and releasing for about

1477
01:26:41,940 --> 01:26:46,590
so of course there's been lots of people who try to define what clustering is

1478
01:26:48,310 --> 01:26:52,170
most of it i mean it's really hard to define in the and what i

1479
01:26:52,170 --> 01:26:54,580
want to do now is i want to show different different definitions

1480
01:26:55,070 --> 01:26:58,520
the conclusion in the end will be that that it doesn't have very much because

1481
01:26:58,520 --> 01:26:59,420
we just have

1482
01:26:59,490 --> 01:27:04,660
different have different definitions for different settings but i think it's still helpful to see

1483
01:27:04,680 --> 01:27:09,900
give an impression what you could do here

1484
01:27:09,930 --> 01:27:16,730
OK so OK so now let's let's take a slightly larger viewpoint so why is

1485
01:27:16,730 --> 01:27:21,430
clustering actually so difficult and i really spend a lot of time thinking about this

1486
01:27:21,490 --> 01:27:26,780
because it's i don't find it's obvious why this also difficult missing there are several

1487
01:27:26,780 --> 01:27:30,230
reasons and i just wanted to put a few of them here

1488
01:27:30,280 --> 01:27:33,720
so i mean of course is an unsupervised learning technique okay

1489
01:27:33,750 --> 01:27:36,670
simple so we have data points we don't have labels and we want to do

1490
01:27:36,670 --> 01:27:40,480
something with it and we don't have any supervision

1491
01:27:40,530 --> 01:27:43,990
but some of this can be all because there are lots of unsupervised techniques which

1492
01:27:43,990 --> 01:27:48,300
today i spent being reused in front of different works so again another example of

1493
01:27:48,300 --> 01:27:49,610
it being cat

1494
01:27:49,860 --> 01:27:54,380
a bit of work to understand the aim that of the data link

1495
01:27:54,490 --> 01:27:55,610
before you do that

1496
01:27:55,660 --> 01:27:59,630
for the most sophisticated approach is

1497
01:27:59,720 --> 01:28:03,990
we actually properties in particular entities are being used as the basis for doing so

1498
01:28:04,260 --> 01:28:08,160
a good example of this is how has been linked to geonames

1499
01:28:08,300 --> 01:28:13,990
so in this case the the as far as i understand process started with matching

1500
01:28:13,990 --> 01:28:15,990
taking an entry in the pedia

1501
01:28:16,340 --> 01:28:21,050
that's an attitude had latitude longitude entry so therefore can be assumed to some kind

1502
01:28:21,050 --> 01:28:22,680
of location

1503
01:28:22,890 --> 01:28:26,820
looking looking for string matching geonames and then if the

1504
01:28:26,970 --> 01:28:32,280
the coordinates of the last year to match a certain range of the margin of

1505
01:28:32,280 --> 01:28:36,700
error then these two entities were said to be the same thing

1506
01:28:36,950 --> 01:28:39,090
so that's the

1507
01:28:39,180 --> 01:28:43,740
the situation with this pre the most sophisticated way of doing that we need to

1508
01:28:45,220 --> 01:28:49,880
i mean the doing the more reliable more robust talk

1509
01:28:50,430 --> 01:28:56,820
so the never refused to how he's going to talk about taking these things

1510
01:28:56,950 --> 01:28:59,860
with human blood

1511
01:29:00,070 --> 01:29:05,390
she is

1512
01:29:05,450 --> 01:29:14,160
OK so

1513
01:29:14,860 --> 01:29:16,300
just learned about

1514
01:29:16,300 --> 01:29:18,380
automatic interlinking

1515
01:29:18,450 --> 01:29:20,800
now with

1516
01:29:20,880 --> 01:29:21,530
a couple of

1517
01:29:21,550 --> 01:29:26,720
months ago we started about thinking using human power basically the same idea

1518
01:29:26,910 --> 01:29:31,530
behind the games with a purpose or what you know from wikis are tagging

1519
01:29:31,570 --> 01:29:32,990
to actually

1520
01:29:33,030 --> 01:29:35,450
make humans to create a kind of link

1521
01:29:35,570 --> 01:29:39,550
humans are especially good to create a high-quality links

1522
01:29:39,660 --> 01:29:45,130
and there are actually certain use cases or certain resources type

1523
01:29:45,260 --> 01:29:46,990
especially with multimedia

1524
01:29:47,180 --> 01:29:51,200
where you know you want to have a kind of fine grained description you look

1525
01:29:51,200 --> 01:29:55,820
inside it talk about not removing their whatever

1526
01:29:56,030 --> 01:30:00,470
and these are we found a good candidates for many interlinked as we call it

1527
01:30:00,680 --> 01:30:03,070
is contributed including

1528
01:30:06,380 --> 01:30:11,510
i might not be able to democrat now and i don't have the time and

1529
01:30:11,530 --> 01:30:17,800
all that is the first concept demonstrator with with using flickr so you might have

1530
01:30:17,800 --> 01:30:19,130
seen flickr

1531
01:30:19,360 --> 01:30:23,240
possible to put a note on an image in a certain region so we kind

1532
01:30:23,240 --> 01:30:28,860
of people that using that node facility in flickr under your right for person in

1533
01:30:28,860 --> 01:30:30,160
the world

1534
01:30:30,410 --> 01:30:32,590
is that the like

1535
01:30:32,930 --> 01:30:34,130
so here

1536
01:30:34,660 --> 01:30:38,930
the user would actually go to flickr could note on on the image

1537
01:30:39,160 --> 01:30:43,570
picture thing this person you're i whatever

1538
01:30:43,610 --> 01:30:49,470
in our two what kind of crawl that data and you could create to tell

1539
01:30:50,200 --> 01:30:53,510
the photos were this and this person is depicted

1540
01:30:53,720 --> 01:30:56,180
it works with self profiling

1541
01:30:56,260 --> 01:30:59,740
six for any kind of of document which is out there

1542
01:30:59,820 --> 01:31:02,220
and it also users

1543
01:31:02,280 --> 01:31:04,320
over here is in the g

1544
01:31:04,360 --> 01:31:07,840
two kind of extend the interlinking

1545
01:31:07,930 --> 01:31:09,160
you know the the

1546
01:31:10,800 --> 01:31:15,410
the images can be tagged you can look up at tagging sunday june find out

1547
01:31:15,530 --> 01:31:21,240
bit more and basically at some RDF see also links that is part is automatically

1548
01:31:21,450 --> 01:31:22,720
does it

1549
01:31:22,800 --> 01:31:25,360
exposed to RDF and

1550
01:31:25,410 --> 01:31:27,610
sparql endpoint as well

1551
01:31:27,700 --> 01:31:29,470
so multimedia basically

1552
01:31:29,470 --> 01:31:31,130
it's a good candidate for that

1553
01:31:31,200 --> 01:31:34,340
and this was possible

1554
01:31:34,430 --> 01:31:37,780
was that our children and i could definitely

1555
01:31:37,860 --> 01:31:39,010
that tool

1556
01:31:39,090 --> 01:31:39,990
i could

1557
01:31:40,010 --> 01:31:42,380
basically create kind of

1558
01:31:42,610 --> 01:31:48,340
triple or technically that kind of triple so the containing

1559
01:31:50,130 --> 01:31:55,200
i i mean you knew you were i would be the subject of the triple

1560
01:31:55,280 --> 01:31:59,650
and the implicit semantic is this region actually

1561
01:31:59,860 --> 01:32:01,130
fourth depicts

1562
01:32:01,160 --> 01:32:02,140
a certain

1563
01:32:02,220 --> 01:32:07,430
person so i went to the URI for our youngest child here and then i

1564
01:32:07,430 --> 01:32:08,660
could agree that

1565
01:32:08,740 --> 01:32:09,490
as well

1566
01:32:09,570 --> 01:32:11,990
you could imagine the same works with video

1567
01:32:12,610 --> 01:32:16,220
the temporal dimension as well which gets more complicated

1568
01:32:16,360 --> 01:32:21,130
and still the the infrastructure if you look at youtube whatever is there

1569
01:32:21,300 --> 01:32:23,610
it will work as well

1570
01:32:23,650 --> 01:32:28,280
from the fragment identification type as well

1571
01:32:28,510 --> 01:32:33,700
so to sum it up it's quite new linking paradigm and we don't have that

1572
01:32:33,700 --> 01:32:35,630
much experience with it yet

1573
01:32:37,300 --> 01:32:42,530
there are several issues around that so for example you expose the process should

1574
01:32:42,720 --> 01:32:48,260
the user be aware of the actually creates some interlinking there or should you hydrogen

1575
01:32:48,950 --> 01:32:53,550
build UI around the user doesn't actually know the building

1576
01:32:53,700 --> 01:33:00,380
triple their their provenance trust and privacy issues can i really believe that somebody somebody

1577
01:33:00,380 --> 01:33:02,630
human that's that

1578
01:33:02,780 --> 01:33:07,300
then somehow take track who actually provided the link

1579
01:33:07,860 --> 01:33:11,880
and the basic question here is this was all human

1580
01:33:12,070 --> 01:33:16,990
driven things what the motivation for the end user why should he or she should

1581
01:33:16,990 --> 01:33:23,300
provide the kind of link but i think that for certain resource types especially multimedia

1582
01:33:24,090 --> 01:33:27,470
from there are more out there is

1583
01:33:27,760 --> 01:33:29,680
what you can think of one

1584
01:33:29,720 --> 01:33:33,220
this might be a very good way to get high quality

1585
01:33:33,380 --> 01:33:36,490
including says well i think

1586
01:33:36,610 --> 01:33:40,840
i i can come back to your should from earlier with the web what is

1587
01:33:40,840 --> 01:33:41,760
we're actually going to

1588
01:33:42,150 --> 01:33:45,030
compute a lower bound on the marginal likelihood

1589
01:33:46,340 --> 01:33:48,320
that's nice because it gives us some

1590
01:33:49,400 --> 01:33:53,860
definite information about the marginal likelihood if we want to model comparison

1591
01:33:54,360 --> 01:33:55,880
what we're going to do is we're going to

1592
01:33:57,400 --> 01:33:59,470
these lower bounds which of course

1593
01:34:01,050 --> 01:34:06,190
is not like comparing the true thing but it's the best we can do with this method

1594
01:34:07,970 --> 01:34:11,110
so how how do we compute this lower bound well

1595
01:34:11,740 --> 01:34:15,470
the log of the marginal likelihood is the log of this integral

1596
01:34:16,170 --> 01:34:19,400
the way we're going to write there is we can multiply

1597
01:34:19,840 --> 01:34:21,550
the term inside this integral

1598
01:34:22,010 --> 01:34:24,710
by some arbitrary distribution q

1599
01:34:26,800 --> 01:34:29,530
the hidden variables x and the parameters theta

1600
01:34:30,090 --> 01:34:33,820
we multiply by the distribution and we divide by the same distribution

1601
01:34:34,320 --> 01:34:34,690
so we

1602
01:34:35,400 --> 01:34:39,530
pointwise multiply this thing by one something whatever dividing by zero

1603
01:34:41,570 --> 01:34:43,610
so clearly this is an equality here

1604
01:34:45,360 --> 01:34:48,070
now for any arbitrary distribution q

1605
01:34:49,070 --> 01:34:53,860
it turns out that the log only average with respect to q

1606
01:34:54,460 --> 01:34:55,570
of this ratio

1607
01:34:56,570 --> 01:34:58,190
is greater than or equal to

1608
01:34:58,760 --> 01:35:00,710
we average with respect to q

1609
01:35:01,170 --> 01:35:03,150
of the log of the ratio

1610
01:35:04,940 --> 01:35:05,780
and this comes

1611
01:35:06,280 --> 01:35:08,240
this is called jensen's inequality

1612
01:35:08,670 --> 01:35:11,360
and it comes from the fact that the log function

1613
01:35:11,840 --> 01:35:12,740
is concave

1614
01:35:13,880 --> 01:35:15,590
the log function is concave

1615
01:35:18,690 --> 01:35:19,170
of these

1616
01:35:22,380 --> 01:35:23,860
is less than

1617
01:35:24,960 --> 01:35:27,570
sorry is is greater than

1618
01:35:28,030 --> 01:35:31,650
we average of the log for any averaging distribution

1619
01:35:32,210 --> 01:35:32,990
you can see this by

1620
01:35:33,610 --> 01:35:34,400
you know drawing

1621
01:35:35,260 --> 01:35:36,420
a picture like this

1622
01:35:38,510 --> 01:35:39,690
that's a log function

1623
01:35:40,550 --> 01:35:41,880
these are two points

1624
01:35:43,820 --> 01:35:44,300
this is

1625
01:35:45,630 --> 01:35:47,190
point in between

1626
01:35:49,050 --> 01:35:49,820
this is

1627
01:35:50,970 --> 01:35:51,940
o lord

1628
01:35:54,010 --> 01:35:55,190
average point

1629
01:35:55,820 --> 01:35:57,570
and this is the average

1630
01:35:58,490 --> 01:35:59,030
the log

1631
01:36:00,240 --> 01:36:03,570
averaging not equally averaging with respect any distribution

1632
01:36:06,860 --> 01:36:08,300
this of course holds for any

1633
01:36:08,800 --> 01:36:12,130
concave function we use it for the log and that's called jensen's inequality

1634
01:36:14,510 --> 01:36:15,420
so here we have

1635
01:36:15,960 --> 01:36:21,300
a lower bound general lower bound on the marginal likelihood and now the game is going to be

1636
01:36:21,760 --> 01:36:23,650
this holds for any distribution q

1637
01:36:24,150 --> 01:36:26,960
let's find the distribution q

1638
01:36:27,380 --> 01:36:28,740
within some family

1639
01:36:29,210 --> 01:36:31,960
that's has the tightest lower bound

1640
01:36:32,530 --> 01:36:36,070
the tightest lower bound comes from maximizing

1641
01:36:37,820 --> 01:36:38,840
as a function of q

1642
01:36:41,380 --> 01:36:42,510
so in particular

1643
01:36:43,760 --> 01:36:45,800
you can do various things you could say

1644
01:36:46,300 --> 01:36:47,240
q is gonna be

1645
01:36:47,970 --> 01:36:51,260
this accuser galaxy and i'm gonna find the best galcians fit

1646
01:36:52,260 --> 01:36:52,960
i to

1647
01:36:53,420 --> 01:36:54,800
maximize this lower bound

1648
01:36:55,610 --> 01:36:59,940
well i'm gonna talk about is assumed that q factors in a particular way

1649
01:37:00,460 --> 01:37:01,900
assume the q factors

1650
01:37:04,150 --> 01:37:08,090
as a distribution over the hidden variables and a distribution over the parameters

1651
01:37:11,440 --> 01:37:16,230
and then optimize this thing here with respect to these two distributions

1652
01:37:19,150 --> 01:37:21,530
so we write this down as ef

1653
01:37:21,940 --> 01:37:25,690
so functional these two distributions q v x and q theta

1654
01:37:26,550 --> 01:37:27,690
why is fixed

1655
01:37:28,110 --> 01:37:30,300
we optimize over q x and q data

1656
01:37:32,650 --> 01:37:40,800
algorithmically what we can do that optimizes we can alternate between optimizing q x holding qx theta fixed

1657
01:37:41,460 --> 01:37:44,440
and then optimizing incubator data holding qx x fixed

1658
01:37:45,340 --> 01:37:45,900
back and forth

1659
01:38:03,260 --> 01:38:05,960
okay i could i could write these equations

1660
01:38:08,670 --> 01:38:10,710
with the fact that expression from the beginning

1661
01:38:11,130 --> 01:38:11,470
it's true

1662
01:38:14,380 --> 01:38:16,570
it's a choice in this sense there

1663
01:38:17,650 --> 01:38:19,360
this holds for any q

1664
01:38:19,940 --> 01:38:23,900
this is the most general expression of the variational framework

1665
01:38:24,880 --> 01:38:28,630
i'm saying in very commonly used choice

1666
01:38:29,320 --> 01:38:30,800
is a factorized q

1667
01:38:31,360 --> 01:38:34,420
but other choices might be a galaxy enjoyed q

1668
01:38:34,960 --> 01:38:36,760
are there could be other choices

1669
01:38:37,320 --> 01:38:39,570
as well like partially factorized q

1670
01:38:40,730 --> 01:38:43,360
the rest of these things are choices from here on

1671
01:38:43,800 --> 01:38:45,400
let me focusing on the choice

1672
01:38:45,880 --> 01:38:47,320
other factorized q

1673
01:38:47,920 --> 01:38:49,460
and then i get this expression here

1674
01:38:51,530 --> 01:38:51,990
and then

1675
01:38:55,440 --> 01:38:58,650
variational bayesian algorithm for the factorized case

1676
01:38:59,090 --> 01:39:00,110
corresponds to

1677
01:39:03,170 --> 01:39:08,110
yeah like algorithm we can actually formalize their relationship to yemen a second

1678
01:39:09,030 --> 01:39:14,320
and ian like algorithm where in the e step i compute a distribution over the hidden variables

1679
01:39:14,320 --> 01:39:17,070
we have

1680
01:39:17,100 --> 01:39:19,550
all go home was the

1681
01:39:19,800 --> 01:39:27,930
all that

1682
01:39:27,970 --> 01:39:32,800
the first artist to come shortly

1683
01:39:32,820 --> 01:39:35,390
come on this side studio

1684
01:39:35,450 --> 01:39:38,820
during your

1685
01:39:38,850 --> 01:39:48,180
maybe you to be nice to

1686
01:39:52,930 --> 01:40:00,010
what think have to

1687
01:40:04,200 --> 01:40:06,050
i think it out completely

1688
01:40:06,070 --> 01:40:09,910
if you want to

1689
01:40:10,410 --> 01:40:16,510
so i can demonstrate that the that's all you right

1690
01:40:16,530 --> 01:40:17,450
which is in

1691
01:40:21,140 --> 01:40:22,720
it was

1692
01:40:22,740 --> 01:40:26,350
so you should not

1693
01:40:26,410 --> 01:40:30,450
you also we show this don't

1694
01:40:30,470 --> 01:40:31,070
that's it

1695
01:40:32,430 --> 01:40:37,280
the only about those are the only way to resolve this

1696
01:40:37,330 --> 01:40:43,720
if you were all that

1697
01:40:44,370 --> 01:40:50,410
i was talking to one another

1698
01:40:51,870 --> 01:40:54,970
if i can if i can put

1699
01:40:55,010 --> 01:41:00,070
just one for them and one first come on when i wasn't going to use

1700
01:41:00,070 --> 01:41:01,430
the word fundamental

1701
01:41:01,490 --> 01:41:03,890
then it looks very boring

1702
01:41:03,930 --> 01:41:06,050
and i'm going to do that first by making you

1703
01:41:06,070 --> 01:41:07,780
listen to a year

1704
01:41:07,800 --> 01:41:10,830
twenty four michael's if we can get this up here

1705
01:41:10,890 --> 01:41:12,350
the first which

1706
01:41:12,370 --> 01:41:16,260
so it's important that you quite because all the sound you make you see

1707
01:41:16,320 --> 01:41:18,910
let's first it's boring

1708
01:41:18,950 --> 01:41:21,800
four hundred forty hertz was nothing else

1709
01:41:21,850 --> 01:41:25,870
no overtones only for some family

1710
01:41:25,890 --> 01:41:33,220
all the images of four forty five

1711
01:41:33,240 --> 01:41:36,370
to fifty six using

1712
01:41:36,430 --> 01:41:38,660
far apart or because

1713
01:41:43,760 --> 01:41:47,030
and new rule

1714
01:41:47,030 --> 01:41:50,240
do you

1715
01:41:50,320 --> 01:41:51,680
what i would like

1716
01:41:51,720 --> 01:41:53,800
whole the first

1717
01:41:53,820 --> 01:41:57,700
he's trying to do something very close to four forty

1718
01:41:57,700 --> 01:41:59,370
four forty

1719
01:41:59,390 --> 01:42:06,180
in this you will be different you five

1720
01:42:06,220 --> 01:42:10,300
and the story for

1721
01:42:10,450 --> 01:42:12,430
forty instruction

1722
01:42:12,470 --> 01:42:16,490
here's might try to make it for forty

1723
01:42:16,510 --> 01:42:18,390
you don't have to be exactly

1724
01:42:22,600 --> 01:42:24,800
o just

1725
01:42:26,320 --> 01:42:33,240
if you're like

1726
01:42:33,240 --> 01:42:37,110
i'm trying to identify something that's on the order of one part in ten to

1727
01:42:37,110 --> 01:42:39,670
the ninth century to less than one part in ten to the ninth of the

1728
01:42:39,670 --> 01:42:43,340
whole human genome carrying out purification is like that

1729
01:42:43,360 --> 01:42:44,700
really kind of

1730
01:42:44,750 --> 01:42:46,140
hard to imagine

1731
01:42:46,380 --> 01:42:49,950
but the way it was done was by the invention of cloning

1732
01:42:50,000 --> 01:42:54,770
let me briefly over here the idea of cloning and then will dive into the

1733
01:42:55,910 --> 01:43:00,990
the idea of cloning

1734
01:43:03,350 --> 01:43:08,290
the latest to purify individual molecules would just be to take the molecules and just

1735
01:43:08,330 --> 01:43:11,150
delete them so that there was only one

1736
01:43:11,160 --> 01:43:12,960
of each molecule

1737
01:43:13,010 --> 01:43:16,650
that's very stories and the problem is is not very much

1738
01:43:16,660 --> 01:43:20,560
so you need a way to take a single copy of the molecule

1739
01:43:20,610 --> 01:43:21,410
and then

1740
01:43:21,430 --> 01:43:23,940
make many copies of it

1741
01:43:23,990 --> 01:43:28,640
so purification is not hard just deluded down to work with single molecules but then

1742
01:43:28,680 --> 01:43:32,520
need to copy it back again and again and again and no biochemical technique involves

1743
01:43:32,520 --> 01:43:38,630
for fracturing cell and replicating some inside the copying siemens and copy since can copy

1744
01:43:39,840 --> 01:43:43,510
that was the basis so here's the the way it goes the basic over will

1745
01:43:43,510 --> 01:43:46,680
look at is taken DNA and cut

1746
01:43:46,730 --> 01:43:49,560
your DNA of interest maybe the human genome

1747
01:43:49,570 --> 01:43:51,460
into pieces

1748
01:43:51,470 --> 01:43:54,510
to find sites

1749
01:43:56,490 --> 01:43:59,400
paste your DNA

1750
01:43:59,420 --> 01:44:01,710
which is

1751
01:44:01,720 --> 01:44:03,570
more technically like it

1752
01:44:03,590 --> 01:44:06,280
the word we use case your DNA

1753
01:44:06,420 --> 01:44:10,010
to some other DNA

1754
01:44:10,120 --> 01:44:14,550
called a vector

1755
01:44:14,660 --> 01:44:18,800
so cut DNA case to DNA

1756
01:44:19,140 --> 01:44:24,930
each piece of of your say human DNA gets stuck to some piece of effect

1757
01:44:25,160 --> 01:44:26,830
insert this

1758
01:44:27,620 --> 01:44:33,150
in two

1759
01:44:37,700 --> 01:44:39,300
can replicated

1760
01:44:39,410 --> 01:44:41,940
in bacteria

1761
01:44:41,950 --> 01:44:50,260
so i'm going to actually take my piece of human DNA and not just like

1762
01:44:50,260 --> 01:44:51,930
it to any piece of DNA

1763
01:44:51,940 --> 01:44:53,840
i'm going to take my human DNA

1764
01:44:53,860 --> 01:44:56,360
and i'm going to like it a vector

1765
01:44:56,520 --> 01:44:58,420
it has all of the

1766
01:44:58,430 --> 01:45:03,150
the machinery all of the ability to to cut to be copied in the bacteria

1767
01:45:03,250 --> 01:45:08,070
that's what i'm going to do is i'm going to transform my DNA

1768
01:45:09,780 --> 01:45:10,990
in two

1769
01:45:11,000 --> 01:45:12,260
a host cell

1770
01:45:12,280 --> 01:45:19,280
host bacterial cell transform

1771
01:45:19,330 --> 01:45:21,330
means introduce

1772
01:45:21,340 --> 01:45:25,640
when we talk about transforming DNA when not talking about changing it's the word that's

1773
01:45:25,640 --> 01:45:27,940
used for taking my

1774
01:45:27,950 --> 01:45:33,620
DNA stuck into a vector introducing it into bacterial cells ideally each bacterial cell

1775
01:45:35,020 --> 01:45:38,730
would carry ones such a DNA molecule

1776
01:45:38,740 --> 01:45:40,210
and then what i want to do

1777
01:45:40,240 --> 01:45:42,190
is i want to plate

1778
01:45:42,200 --> 01:45:44,950
my cells

1779
01:45:45,410 --> 01:45:46,660
and select

1780
01:45:49,180 --> 01:45:52,720
the carry human DNA

1781
01:45:52,730 --> 01:45:59,920
my DNA DNA effort on so i put them on the poetry play

1782
01:45:59,970 --> 01:46:01,780
and i want only the

1783
01:46:02,830 --> 01:46:05,380
that happened to have picked up

1784
01:46:05,420 --> 01:46:08,460
an individual piece of human DNA to grow

1785
01:46:08,510 --> 01:46:12,980
that's the that's the trick is very simple trick take total human DNA cut up

1786
01:46:12,980 --> 01:46:15,420
into pieces glue it to a vector

1787
01:46:15,430 --> 01:46:19,200
that's able to copy it's able to be replicated back to report the vectors into

1788
01:46:19,200 --> 01:46:24,320
bacterial cells every bacterial cell picks up no more than one vector

1789
01:46:24,340 --> 01:46:29,020
he played out and you simply arranged so that the only cells that grow

1790
01:46:29,040 --> 01:46:32,830
are those picked up the piece of human DNA and every one of these colonies

1791
01:46:32,830 --> 01:46:35,270
is the descendant of a single

1792
01:46:35,280 --> 01:46:40,430
bacterial cell the picked up a single human molecule but is obligingly copy that molecule

1793
01:46:40,430 --> 01:46:43,370
for you again and again and again and again

1794
01:46:43,380 --> 01:46:48,300
and as you have what we refer to this whole collection here is called a

1795
01:46:48,300 --> 01:46:50,610
library of clones

1796
01:46:51,330 --> 01:46:57,450
this is called the common library because every piece of the human genome is somewhere

1797
01:46:57,450 --> 01:46:58,480
in here

1798
01:46:58,490 --> 01:46:59,910
you know this one here

1799
01:46:59,920 --> 01:47:01,910
probably is active

1800
01:47:01,960 --> 01:47:04,220
and maybe this one here

1801
01:47:04,240 --> 01:47:08,720
maybe it's college in eleven that one there might be others beta goal

1802
01:47:08,740 --> 01:47:12,540
OK actually when you look at the plate there's no way to tell but in

1803
01:47:12,540 --> 01:47:15,670
principle they all there so there will be this question how do we look at

1804
01:47:15,680 --> 01:47:19,390
the library and pull out what the right one is but somewhere in there should

1805
01:47:19,390 --> 01:47:22,160
be a bacterial colony that is pure beta globin

1806
01:47:22,250 --> 01:47:26,290
gene the DNA for better tomorrow the next lecture will be about how you actually

1807
01:47:26,290 --> 01:47:29,870
find the today let's just build this library so our goal is to be able

1808
01:47:29,870 --> 01:47:32,030
to build the library like this so

1809
01:47:34,110 --> 01:47:40,260
we have to cut DNA pace DNA vectors senator so that's what are subject will

1810
01:47:40,260 --> 01:47:44,300
think my information and fact extraction

1811
01:47:44,690 --> 01:47:47,420
and then look at machine vision

1812
01:47:48,480 --> 01:47:53,290
topic which we will continue tomorrow

1813
01:47:53,300 --> 01:48:00,170
well you have to do is really we have become symbolic techniques to craft items

1814
01:48:00,210 --> 01:48:02,720
to extract the information

1815
01:48:02,780 --> 01:48:09,080
and dr of court you have seen in the message understanding conference that it really

1816
01:48:10,960 --> 01:48:12,530
of course you

1817
01:48:12,530 --> 01:48:16,310
it to work that you have stick to the mean

1818
01:48:16,350 --> 01:48:19,030
very restricted subject area

1819
01:48:19,070 --> 01:48:22,600
when you deal somehow with have language

1820
01:48:22,600 --> 01:48:28,950
so when you can restricted all the time that expression

1821
01:48:33,140 --> 01:48:34,700
twenty years

1822
01:48:36,460 --> 01:48:37,640
can be

1823
01:48:37,670 --> 01:48:39,980
comment what

1824
01:48:42,610 --> 01:48:43,560
we be

1825
01:48:43,570 --> 01:48:49,010
many of the things natural language

1826
01:48:49,030 --> 01:48:53,820
very well you have a large variety of expression

1827
01:48:55,480 --> 01:48:58,570
i tell you this content only

1828
01:48:58,670 --> 01:49:00,680
think of a very similar

1829
01:49:00,730 --> 01:49:03,540
so we have many

1830
01:49:03,590 --> 01:49:07,140
to many

1831
01:49:08,240 --> 01:49:10,730
two to implement

1832
01:49:10,740 --> 01:49:15,480
and also the fact that you find might be very

1833
01:49:15,480 --> 01:49:18,340
and they might need other knowledge

1834
01:49:18,370 --> 01:49:21,230
contextual knowledge

1835
01:49:21,260 --> 01:49:28,150
many information to really understand what the graph the

1836
01:49:28,150 --> 01:49:32,380
and last but not least the fact that

1837
01:49:33,070 --> 01:49:35,210
i showed you have to be

1838
01:49:35,230 --> 01:49:38,290
this more of a large text

1839
01:49:38,290 --> 01:49:41,480
at first the french community

1840
01:49:41,490 --> 01:49:42,870
they are quite

1841
01:49:44,370 --> 01:49:45,480
i think

1842
01:49:45,510 --> 01:49:47,560
this languages

1843
01:49:48,630 --> 01:49:51,410
the real the language being used

1844
01:49:51,510 --> 01:49:54,100
the used change a lot

1845
01:49:54,110 --> 01:49:57,850
so you need somehow to adapt continued

1846
01:49:57,860 --> 01:49:59,630
the fact that you have

1847
01:50:03,200 --> 01:50:06,890
actually manually if you

1848
01:50:08,690 --> 01:50:10,270
you need all kinds of

1849
01:50:16,700 --> 01:50:17,880
and i told

1850
01:50:18,300 --> 01:50:21,600
we meanwhile want to nineteen ninety six onwards

1851
01:50:22,070 --> 01:50:26,520
love came up with the idea that maybe we should learn better

1852
01:50:26,570 --> 01:50:30,730
i mean she and she lived a

1853
01:50:32,040 --> 01:50:36,050
she developed a very simple machine learning

1854
01:50:36,140 --> 01:50:37,800
which would been know

1855
01:50:37,970 --> 01:50:40,790
della retreated that

1856
01:50:40,890 --> 01:50:42,970
actually will

1857
01:50:43,950 --> 01:50:47,730
based on very simple to these matters

1858
01:50:48,860 --> 01:50:53,440
it's almost as good as in terms of form

1859
01:50:53,450 --> 01:50:56,750
when you have

1860
01:50:56,760 --> 01:50:58,100
so she

1861
01:50:58,940 --> 01:51:00,720
kind of course

1862
01:51:04,470 --> 01:51:06,390
and what we

1863
01:51:06,410 --> 01:51:08,350
a lot of communication

1864
01:51:08,580 --> 01:51:11,860
all kinds of machine learning techniques

1865
01:51:11,920 --> 01:51:17,420
starting with induction of decision and the same thing

1866
01:51:17,440 --> 01:51:20,260
many have been

1867
01:51:20,270 --> 01:51:22,250
in this

1868
01:51:22,260 --> 01:51:27,350
more to the well to the sea more support vector machines

1869
01:51:27,360 --> 01:51:30,250
maximum entropy models hidden markov model

1870
01:51:31,830 --> 01:51:36,720
we also have unsupervised learning

1871
01:51:36,760 --> 01:51:39,290
i told you yesterday

1872
01:51:39,320 --> 01:51:45,940
the would like recognise the topics of the documents of text

1873
01:51:46,330 --> 01:51:47,790
the the

1874
01:51:47,830 --> 01:51:51,670
when you go to find information extraction

1875
01:51:52,200 --> 01:51:59,660
they are sometimes quite different but someone who knows what more provide that

1876
01:51:59,680 --> 01:52:04,560
like probabilistic topic models that could be useful

1877
01:52:04,570 --> 01:52:05,410
and to the

1878
01:52:05,440 --> 01:52:08,690
we'll get the key to provide

1879
01:52:08,700 --> 01:52:10,570
we use that

1880
01:52:10,590 --> 01:52:14,570
from and manually and i t and u

1881
01:52:17,570 --> 01:52:19,190
lot of

1882
01:52:19,200 --> 01:52:21,130
thank you

1883
01:52:21,180 --> 01:52:24,260
so what we looking at the

1884
01:52:24,260 --> 01:52:28,210
first of all i will

1885
01:52:28,710 --> 01:52:33,400
what kind of features we use

1886
01:52:34,990 --> 01:52:37,160
how do we select features

1887
01:52:37,180 --> 01:52:42,590
we also to support vector machine and kernel

1888
01:52:45,320 --> 01:52:47,890
thirty two probabilistic model

1889
01:52:47,950 --> 01:52:50,550
i think may be

1890
01:52:50,560 --> 01:52:53,260
and also the maximum entropy

1891
01:52:53,270 --> 01:52:57,220
tomorrow we will could people who

1892
01:52:57,240 --> 01:53:01,340
the markov models conditional random field

1893
01:53:01,360 --> 01:53:02,820
and then also the

1894
01:53:03,940 --> 01:53:07,280
and so i probabilistic topic models

1895
01:53:08,240 --> 01:53:15,090
where you actually have to take into account also the structure of your

1896
01:53:15,630 --> 01:53:17,050
it could be also the

1897
01:53:17,130 --> 01:53:19,930
four of no

1898
01:53:19,940 --> 01:53:22,710
and from time to time i

1899
01:53:22,960 --> 01:53:25,870
the figure

1900
01:53:26,410 --> 01:53:27,380
well it

1901
01:53:27,390 --> 01:53:29,110
just another

1902
01:53:32,550 --> 01:53:33,750
it's not to the

1903
01:53:33,760 --> 01:53:36,510
that's the that's all the time

1904
01:53:36,560 --> 01:53:38,380
very much detail

1905
01:53:38,700 --> 01:53:40,570
it would not be

1906
01:53:40,590 --> 01:53:44,830
we will not have time to do just one lecture

1907
01:53:45,950 --> 01:53:46,900
i want to

1908
01:53:46,910 --> 01:53:52,300
to focus the attention of some aspects of the model

1909
01:53:52,320 --> 01:53:52,960
the dog

1910
01:53:52,970 --> 01:53:56,140
when you the natural language

1911
01:53:56,150 --> 01:54:01,160
text to extract search

1912
01:54:01,210 --> 01:54:06,200
the other thing that we use we will speak about which

1913
01:54:07,640 --> 01:54:09,740
a feature vector

1914
01:54:09,780 --> 01:54:15,010
the object that you want to classify the piece of information

1915
01:54:15,010 --> 01:54:16,900
that you want to track

1916
01:54:16,930 --> 01:54:20,800
or to identify in the text

1917
01:54:20,950 --> 01:54:22,880
it represented as

1918
01:54:22,930 --> 01:54:29,080
effects of which he later that you can see some of the structure of the

1919
01:54:29,300 --> 01:54:32,120
object represents

1920
01:54:33,120 --> 01:54:34,250
we use

1921
01:54:34,950 --> 01:54:37,330
in the key is to keep class

1922
01:54:37,380 --> 01:54:39,090
that you want to find

1923
01:54:39,210 --> 01:54:46,000
remember we do some kind of classification pattern recognition so why is the class label

1924
01:54:46,000 --> 01:54:52,760
that you want to assign to object represents the

1925
01:54:52,770 --> 01:54:58,610
individuals you find the between june twenty first

1926
01:54:58,620 --> 01:55:01,080
discriminative classification

1927
01:55:01,090 --> 01:55:04,140
general classifier

1928
01:55:04,550 --> 01:55:09,000
you can learn a model of the joint probability px y

1929
01:55:09,050 --> 01:55:10,200
and make

1930
01:55:10,220 --> 01:55:15,770
in the county because it's more convenient

1931
01:55:15,800 --> 01:55:20,110
the conditional probability can actually fly

1932
01:55:20,190 --> 01:55:24,090
given the object x x

1933
01:55:24,140 --> 01:55:28,140
very often we use well

1934
01:55:28,690 --> 01:55:31,150
to make computations facts about

1935
01:55:38,300 --> 01:55:42,460
you would think is an example of me the way to use these

1936
01:55:47,250 --> 01:55:48,800
on the other hand

1937
01:55:48,840 --> 01:55:51,250
in natural language processing

1938
01:55:51,250 --> 01:55:55,750
right because of signal processing variations of the above that of the different com

1939
01:55:55,770 --> 01:56:02,940
so when you do face recognition assuming is of the late elimination condition and you

1940
01:56:02,940 --> 01:56:04,620
know frontal and so on

1941
01:56:04,680 --> 01:56:09,750
it's one thing it's only when you want to bring in illumination and variation invariance

1942
01:56:10,040 --> 01:56:13,830
and no robustness to pose and so on it becomes a very different problem right

1943
01:56:14,120 --> 01:56:19,790
so well the way they approach the elimination thing is that them to the training

1944
01:56:19,810 --> 01:56:23,140
set well if you are in the SVM more for example

1945
01:56:23,200 --> 01:56:25,330
we all know from

1946
01:56:25,350 --> 01:56:30,270
pattern recognition classes to the extent the training set looks like the testing set

1947
01:56:30,290 --> 01:56:34,330
you know we're in good shape but in face recognition what happens is you often

1948
01:56:34,330 --> 01:56:37,750
how only one galaxy per person if you look at the

1949
01:56:37,790 --> 01:56:41,410
FRGC the face recognition grand challenge

1950
01:56:41,430 --> 01:56:45,790
test and so forth there's only one gallery image per person so there's not a

1951
01:56:45,790 --> 01:56:51,330
whole lot to learn from that so if you have here in one hundred database

1952
01:56:51,350 --> 01:56:56,980
hundred person database you only have one hundred images to speak that that's how the

1953
01:56:57,980 --> 01:57:02,100
protocol is construct constructed right so if you

1954
01:57:02,120 --> 01:57:05,640
i don't have all these images then it's going be a problem

1955
01:57:05,810 --> 01:57:07,040
so that's the

1956
01:57:07,060 --> 01:57:12,020
think and we want to work on either somebody other what it is before but

1957
01:57:12,060 --> 01:57:14,290
we like this idea of sparsity

1958
01:57:14,310 --> 01:57:19,250
and if you look at i signatures there are quite a reasonably better controlled illumination

1959
01:57:19,250 --> 01:57:25,230
conditions than face face you acquired many meters and sometimes even tens of metres

1960
01:57:26,160 --> 01:57:29,870
the advantage is that you can't be wheel farther than one or two meters and

1961
01:57:29,870 --> 01:57:34,710
you can have been a good elimination and also i signatures to answer for the

1962
01:57:34,710 --> 01:57:38,480
usual out of plane rotation and so forth that face

1963
01:57:38,570 --> 01:57:40,910
suffers from right so

1964
01:57:40,910 --> 01:57:47,020
iris recognition problem can be view more as a signal classification problem then a face

1965
01:57:47,020 --> 01:57:51,680
recognition problem that's all i'm trying to see if it's being the object it produces

1966
01:57:51,680 --> 01:57:56,710
many many variations corresponding to pose and illumination that is you don't see that many

1967
01:57:56,710 --> 01:58:01,540
variations dies but i still creates problems due to occlusion and so on you know

1968
01:58:01,540 --> 01:58:05,540
i it's closing and things like that so we want to do

1969
01:58:05,620 --> 01:58:10,290
of course is blurred specular reflections segmentation which led to segment

1970
01:58:10,330 --> 01:58:13,150
you know i this image and so forth so we wanted to see if we

1971
01:58:13,150 --> 01:58:18,730
can get more out of this spark city methods that were developed berkeley university of

1972
01:58:18,730 --> 01:58:26,040
illinois for the idea signature and so we pretty much just reproduced the algorithm for

1973
01:58:26,040 --> 01:58:31,230
iris recognition and of course we didn't want to pick the for light is a

1974
01:58:31,230 --> 01:58:37,160
signature you know we did some modifications so called the sector processing and so on

1975
01:58:37,180 --> 01:58:42,230
we just divide the city into many sectors and have

1976
01:58:42,250 --> 01:58:47,810
you know we treat them independently and then take a joint work so you take

1977
01:58:48,000 --> 01:58:53,020
in classes and images per class so you believe these things and you then assume

1978
01:58:53,060 --> 01:58:58,640
that the training and testing training image can be written as a linear combination of

1979
01:58:58,660 --> 01:59:04,060
there is a and then you can not go through the same procedure and look

1980
01:59:04,060 --> 01:59:11,750
for a recovery the coefficients alpha corresponding to the i class using the basis pursuit

1981
01:59:11,790 --> 01:59:17,560
and a side benefit of this approach is that mean is also mentioned in the

1982
01:59:17,930 --> 01:59:19,080
family paper

1983
01:59:19,120 --> 01:59:22,960
his you have something known as a quality measure

1984
01:59:23,000 --> 01:59:26,850
it's OK this will come out of algorithm them and you can use this as

1985
01:59:26,850 --> 01:59:33,060
a measure to to reject quality it's images or already face images also hope so

1986
01:59:33,060 --> 01:59:35,250
basically you can

1987
01:59:35,960 --> 01:59:40,390
you know inability go through the procedure and the performance is pretty good

1988
01:59:40,430 --> 01:59:48,460
we have all this various image quality would segmentation and so forth and

1989
01:59:48,520 --> 01:59:51,810
the idea is you have to be submitted different world if you are not at

1990
01:59:51,810 --> 01:59:57,560
least ninety percent even the conference paper is not possible in phase eighty five percent

1991
01:59:57,560 --> 02:00:01,350
of the respective in ideas ninety five it's respective so you've got to do better

1992
02:00:01,350 --> 02:00:07,270
than that why because it is usually acquired in very good illumination conditions the signal

1993
02:00:07,270 --> 02:00:11,410
quality is very good and i was right compared to what you can get face

1994
02:00:11,500 --> 02:00:15,370
OK so you know this is going to be and biometrics confidence in two thousand

1995
02:00:15,370 --> 02:00:17,540
nine now

