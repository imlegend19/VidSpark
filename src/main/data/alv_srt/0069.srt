1
00:00:00,000 --> 00:00:03,370
cruel repulsive hideous creatures

2
00:00:03,530 --> 00:00:05,730
horrible monsters

3
00:00:05,790 --> 00:00:10,170
then he saw one of them creeping towards him

4
00:00:10,210 --> 00:00:12,350
he aimed his weapon

5
00:00:12,440 --> 00:00:14,700
and opened fire on it

6
00:00:14,730 --> 00:00:20,040
the enemy gave that strange horrible cry that all of them used

7
00:00:20,040 --> 00:00:21,440
to utter

8
00:00:21,480 --> 00:00:24,380
then a deathly silence

9
00:00:24,400 --> 00:00:26,140
it was dead

10
00:00:26,200 --> 00:00:30,580
the cry and the sight of the dead body made him shudder

11
00:00:30,620 --> 00:00:35,150
in the course of time many of them had become accustomed

12
00:00:35,200 --> 00:00:41,460
took no notice of that but not he they were horrible disgusting

13
00:00:41,460 --> 00:00:45,060
creatures with only two legs

14
00:00:45,090 --> 00:00:46,440
two arms

15
00:00:46,470 --> 00:00:52,900
two eyes that sickening white skin and without scales

16
00:00:53,440 --> 00:00:57,790
so while reading we are thinking of something like that and on the contrary

17
00:00:57,790 --> 00:00:59,010
to a something

18
00:00:59,200 --> 00:01:00,210
like that

19
00:01:00,510 --> 00:01:07,060
Karl Rosenkranz in his aesthetics of ugliness draws an analogy between ugliness and moral

20
00:01:08,520 --> 00:01:12,200
just as evil in seeing are the opposite of good

21
00:01:12,230 --> 00:01:18,530
whose hell they represent so his ugliness the hell of beauty

22
00:01:18,620 --> 00:01:24,000
I want only to show you many experiments made by an unknown American artists

23
00:01:24,000 --> 00:01:29,310
that artist that I found in internet suggesting what they say

24
00:01:29,360 --> 00:01:32,220
call uglifications

25
00:01:34,410 --> 00:01:40,790
ugly is not only the opposite of beauty because if we examine this synonyms of

26
00:01:40,790 --> 00:01:50,530
ugly we find repellent horrible horrendous disgusting disagreeable grotesque abominable repulsive oduse

27
00:01:50,540 --> 00:01:57,120
indecent fool dirty obscene repugnant frightening object monstrous

28
00:01:57,640 --> 00:02:04,840
horrid horrifying unpleasant terrible terrifying frightful nightmarish revolting sickening fetid

29
00:02:04,840 --> 00:02:11,150
fearsome ingnoble ungainly displeasing tiresome offensive deformed

30
00:02:11,150 --> 00:02:17,840
disfigured and so on so there is a variety a very rich phenomenology

31
00:02:18,000 --> 00:02:24,070
in general it seems that the experience of beauty arouses what can't defined as

32
00:02:24,070 --> 00:02:27,380
disinterested pleasure

33
00:02:27,410 --> 00:02:33,000
whereas we would like to have all that seems agreeable to us and to take

34
00:02:33,020 --> 00:02:39,100
part in all that seems good the judgment of taste the site of a

35
00:02:39,100 --> 00:02:47,430
flower procuse a pleasure that excludes any desire for possession or consumption we don't you

36
00:02:47,430 --> 00:02:51,310
don't want to eat the the flower you want to

37
00:02:52,810 --> 00:02:57,140
he has certain philosophers have a wonder whether it is possible to make an aesthetic

38
00:02:57,140 --> 00:02:58,950
judgment of ugliness

39
00:02:59,180 --> 00:03:06,240
given that ugliness arouses emotional reactions such as disgust and repulsion

40
00:03:06,620 --> 00:03:14,070
as a matter of fact we owe to distinguish between manifestation of ugliness in itself

41
00:03:14,070 --> 00:03:21,170
like the ugliness of an extrement decomposing carry on someone covered with sause who gives

42
00:03:21,170 --> 00:03:28,020
us nauseating stench or this animal which

43
00:03:28,020 --> 00:03:31,900
last year won the contest for the ugliest dog in the world

44
00:03:31,900 --> 00:03:38,570
and we have to distinguish it these sort of natural

45
00:03:38,620 --> 00:03:42,290
total ugliness from formal ugliness

46
00:03:42,620 --> 00:03:49,520
understood as a lack of equilibrium in the organic relationship between the parts of the

47
00:03:49,520 --> 00:03:58,030
hole besides these beautiful paintings by Ghirlandaio suggest has the tender attitude

48
00:03:58,030 --> 00:04:04,980
of the grandchild that shows that a person an animal or else can be ugly

49
00:04:04,980 --> 00:04:06,670
and at the same time

50
00:04:07,620 --> 00:04:08,860
and lovable

51
00:04:08,910 --> 00:04:10,880
as it happens to us with many

52
00:04:11,040 --> 00:04:14,360
actors they are ugly but we like them

53
00:04:14,400 --> 00:04:21,860
however these portrait what is also an example of beautiful artistic representation of

54
00:04:23,950 --> 00:04:26,520
which is another kind

55
00:04:26,550 --> 00:04:33,260
of concept because there is also the artistic ugliness let's see in the contrary

56
00:04:33,260 --> 00:04:38,410
these example of ugly artistic representation

57
00:04:38,450 --> 00:04:45,980
a real example of teach incidentally the author was Adolf Hitler and the

58
00:04:45,980 --> 00:04:51,270
fact that he decided later to shift to politics has been certainly a disaster

59
00:04:51,270 --> 00:04:55,760
for the history of the world but a happy event for the history of

60
00:04:55,760 --> 00:05:06,840
art the the Greek identify the beautiful with good

61
00:05:06,880 --> 00:05:12,410
kalos kai agathos and by consequent ugly with evil

62
00:05:12,450 --> 00:05:15,910
see for instance the description of Thersites in

63
00:05:17,410 --> 00:05:23,650
Thersites was the ugliest man that stood before Ilium bandy legs and lame in

64
00:05:23,650 --> 00:05:29,430
one foot he had round shoulders hunched over his chest

65
00:05:29,430 --> 00:05:32,400
he had a pointed head and sparse thin hair

66
00:05:32,760 --> 00:05:39,450
Achilles and Ulysses hated him the most for he often insulted them

67
00:05:39,450 --> 00:05:42,260
but at the same the Greek know that Socrates

68
00:05:42,550 --> 00:05:45,690
was as ugly as Silenus

69
00:05:45,790 --> 00:05:47,700
but he had a great soul

70
00:05:48,200 --> 00:05:49,930
and Aesop

71
00:05:49,980 --> 00:05:56,750
who according to the legend was repugnant to the sight disgusting fat belly bulching head

72
00:05:56,750 --> 00:06:04,450
pug nose gibbous he was swarthy and short with flat feet short arms bendy legs thick lips

73
00:06:04,450 --> 00:06:08,810
stammered and was quite unable to express himself

74
00:06:08,840 --> 00:06:11,400
what's defined as a

75
00:06:11,500 --> 00:06:13,930
benefactor of humanity

76
00:06:14,220 --> 00:06:20,400
we idealize the Greek culture and mainly focused around beauty and harmony

77
00:06:20,430 --> 00:06:28,570
we idealize the Greek culture as a culture mainly focused about around beauty and harmony but

78
00:06:28,570 --> 00:06:36,860
we frequently forget that the Greek world was also populated by terrifying creatures hybrids

79
00:06:36,860 --> 00:06:44,620
that violated the lows of natural forms like the harpies or the sirens were not

80
00:06:44,620 --> 00:06:47,180
right the correct probability distribution

81
00:06:47,240 --> 00:06:49,220
and now we have in

82
00:06:49,240 --> 00:06:50,890
and edge here

83
00:06:50,890 --> 00:06:54,910
then again rain sprinkler become dependent

84
00:06:54,930 --> 00:07:00,330
marginally which is also incorrect this is nothing i can do to represent

85
00:07:00,370 --> 00:07:02,750
the simple sets of

86
00:07:02,770 --> 00:07:08,720
dependencies that arise very naturally from causal relationships can cause variables and variables

87
00:07:08,720 --> 00:07:12,100
now there's another interesting aspect of

88
00:07:12,120 --> 00:07:14,250
graphs like this

89
00:07:14,270 --> 00:07:20,560
these particular structures exhibit something called explaining away

90
00:07:20,580 --> 00:07:22,640
which is

91
00:07:22,660 --> 00:07:25,890
imagine that we observe that the ground was wet

92
00:07:25,930 --> 00:07:29,290
now there are two possible explanations under this model

93
00:07:29,310 --> 00:07:32,740
either if all these arranged either range

94
00:07:32,740 --> 00:07:34,600
or the sprinklers on

95
00:07:35,160 --> 00:07:38,120
or with some very small probability both

96
00:07:38,180 --> 00:07:40,720
both rate and this was

97
00:07:40,740 --> 00:07:45,310
so we have to think of all the possible explanations for the ground being where

98
00:07:46,140 --> 00:07:47,200
if we

99
00:07:47,350 --> 00:07:48,950
look at

100
00:07:49,020 --> 00:07:53,810
the weather report and we find that in fact it did rain

101
00:07:54,180 --> 00:07:56,220
knowing this variable

102
00:07:56,220 --> 00:08:00,310
explains away the observation that the ground was wet

103
00:08:00,330 --> 00:08:05,350
and therefore we don't need to infer that the sprinkler was on

104
00:08:06,060 --> 00:08:10,350
these two variables by having a common child

105
00:08:10,350 --> 00:08:19,310
become dependent on each other in this particular way once we observed that

106
00:08:20,660 --> 00:08:25,890
sure way of saying that is that observing the the sprinkler is on would explain

107
00:08:25,890 --> 00:08:30,100
away the observation of the ground was wet making it less probable that it rained

108
00:08:33,490 --> 00:08:40,450
let's move on to directed graphical models are directed basically graphs these are sometimes called

109
00:08:40,450 --> 00:08:45,680
bayesian networks but i also think this is a great name for them in particular

110
00:08:45,680 --> 00:08:47,600
have a footnote here

111
00:08:47,660 --> 00:08:48,520
which says

112
00:08:48,560 --> 00:08:55,310
bayesian networks can and often are learned using non bayesian in other words frequentist method

113
00:08:58,720 --> 00:09:04,290
bayesian networks there's nothing about these networks the required parameters structure learning using bayesian methods

114
00:09:04,290 --> 00:09:06,680
and if you look in the prose book

115
00:09:07,290 --> 00:09:09,660
but it doesn't actually use this method

116
00:09:09,720 --> 00:09:14,020
for the most part for learning bayesian so let's call them more descriptively we can

117
00:09:14,020 --> 00:09:15,470
just call them directed

118
00:09:15,520 --> 00:09:19,220
graphical models are directed acyclic graphs

119
00:09:19,270 --> 00:09:23,040
and directed acyclic graph

120
00:09:23,100 --> 00:09:28,020
corresponds to a factorisation of the joint probability distribution in the following way

121
00:09:28,060 --> 00:09:28,890
we say

122
00:09:28,910 --> 00:09:33,140
that is we have this graph that means that the join probability of the

123
00:09:33,200 --> 00:09:36,810
variables factors into the probability distribution over a

124
00:09:36,850 --> 00:09:39,560
that is the probability distribution over b

125
00:09:39,740 --> 00:09:43,180
the probability distribution over the given a and b

126
00:09:43,200 --> 00:09:45,850
he would be in the

127
00:09:46,970 --> 00:09:48,850
given c and d

128
00:09:48,950 --> 00:09:54,020
in in general we have is that the joint distribution factors into the product

129
00:09:54,020 --> 00:09:56,870
the probability of each other variables

130
00:09:56,890 --> 00:10:00,720
given its parents in the graph

131
00:10:01,620 --> 00:10:06,930
the parents of via just the parents of node i that obviously the nodes that

132
00:10:07,620 --> 00:10:11,100
at the tail end of the arrows that point to that

133
00:10:14,700 --> 00:10:18,620
that's directed graphical models

134
00:10:18,640 --> 00:10:22,370
and like undirected graph and

135
00:10:22,410 --> 00:10:25,830
and factor graphs

136
00:10:25,870 --> 00:10:31,930
we can read off the conditional independence relationships between the variables from the directed graph

137
00:10:32,450 --> 00:10:37,100
now in undirected graphs and factor graphs is really easy we just want to have

138
00:10:37,140 --> 00:10:40,640
and we can tell whether things are conditionally independent or not

139
00:10:41,810 --> 00:10:45,750
now we have a slightly more complicated criterion which takes

140
00:10:46,810 --> 00:10:47,720
you know

141
00:10:47,810 --> 00:10:51,080
in our so to get you we don't have an hour the so

142
00:10:51,120 --> 00:10:55,700
if you live in your example work afterwards you want

143
00:10:57,540 --> 00:11:02,720
x is conditionally independent from y given the that variables v

144
00:11:02,770 --> 00:11:04,560
if v

145
00:11:04,580 --> 00:11:08,850
dependency separate that these separate x from y

146
00:11:08,870 --> 00:11:10,060
so think of

147
00:11:11,330 --> 00:11:16,540
dependency flowing through the graph and we're trying to see whether the set of variables

148
00:11:17,120 --> 00:11:19,200
separate x from y

149
00:11:19,200 --> 00:11:24,200
so that x and y become independent when we observed variables be

150
00:11:24,220 --> 00:11:26,350
and here is the definition

151
00:11:26,350 --> 00:11:29,220
the separation

152
00:11:29,930 --> 00:11:31,120
the set b

153
00:11:31,140 --> 00:11:33,930
the separate x from y

154
00:11:33,930 --> 00:11:39,330
if every undirected path that mean path when we actually

155
00:11:39,350 --> 00:11:40,580
i can go

156
00:11:41,580 --> 00:11:42,970
along the

157
00:11:43,020 --> 00:11:45,910
arrows and backwards through the

158
00:11:45,930 --> 00:11:51,140
if every undirected path between x and b x and y

159
00:11:51,140 --> 00:11:55,120
that that we can it we can boost the performance from

160
00:11:55,140 --> 00:11:59,520
the of pretty much when we reach the limit but we can do individual

161
00:11:59,520 --> 00:12:05,940
so i talk a little bit about the timeline and

162
00:12:05,950 --> 00:12:09,410
the tree methods date back

163
00:12:09,430 --> 00:12:13,750
twenty three years or so twenty four years two cards being able to actually go

164
00:12:13,750 --> 00:12:18,950
way beyond that because they've been invented multiple fields but probably the most influential

165
00:12:18,970 --> 00:12:23,780
and in many ways the best outline was

166
00:12:23,780 --> 00:12:25,290
but by

167
00:12:25,310 --> 00:12:28,630
the o'brien j freeman

168
00:12:28,650 --> 00:12:32,870
ocean stone and it's a commercial product now which is good one

169
00:12:32,880 --> 00:12:36,440
but there are trees in both pretty much every sweet

170
00:12:38,250 --> 00:12:40,700
by leo o'brien was

171
00:12:40,720 --> 00:12:42,140
i recently submitted

172
00:12:42,150 --> 00:12:44,020
two the statistics journal

173
00:12:44,060 --> 00:12:45,400
and was rejected

174
00:12:45,430 --> 00:12:50,100
and they got into the machine learning journal november nineteen six where he became a

175
00:12:50,100 --> 00:12:52,650
citation classic within a year two

176
00:12:52,660 --> 00:12:55,880
and made him a hero and that fields

177
00:12:55,880 --> 00:13:02,460
the statistics field of which i'm very fundament ingenuity by our engineers but still suspicions

178
00:13:02,520 --> 00:13:07,450
were just one of these statisticians but it has a habit sometimes leading its owner

179
00:13:07,450 --> 00:13:14,460
are not recognizing generating a lot of fields that become successful elsewhere but the idea

180
00:13:14,460 --> 00:13:16,230
was to

181
00:13:16,250 --> 00:13:19,590
two bootstrap aggregates of different models

182
00:13:19,590 --> 00:13:20,640
and this

183
00:13:20,650 --> 00:13:23,360
so somewhat both out of the blue

184
00:13:23,370 --> 00:13:28,940
adaboost came out in ninety seven was very strange and heuristic procedure

185
00:13:31,670 --> 00:13:32,290
the the

186
00:13:32,350 --> 00:13:35,230
started to do very well people excited

187
00:13:35,320 --> 00:13:40,140
and then it sort of cleanup and input into statistical framework for a few years

188
00:13:41,560 --> 00:13:46,600
importance sampling

189
00:13:46,650 --> 00:13:53,000
is much more recent starting to make things clear

190
00:13:53,020 --> 00:13:55,950
divided you want to cover some of the

191
00:13:56,090 --> 00:14:00,140
details here you want to get to one

192
00:14:00,180 --> 00:14:07,410
this year so so john when he hits a little more this

193
00:14:10,490 --> 00:14:17,850
parallel to the simple fact that they tried

194
00:14:20,130 --> 00:14:21,450
one one

195
00:14:28,330 --> 00:14:30,870
i formal station

196
00:14:33,340 --> 00:14:36,120
so are they saying

197
00:14:36,140 --> 00:14:43,560
a from this problem we have turned their home

198
00:14:43,620 --> 00:14:45,940
they were

199
00:14:45,960 --> 00:14:48,190
how rules

200
00:14:48,210 --> 00:14:50,800
so we're right

201
00:14:54,280 --> 00:14:57,860
so what

202
00:15:00,850 --> 00:15:03,730
they are

203
00:15:03,750 --> 00:15:08,170
so so

204
00:15:08,190 --> 00:15:10,350
my friend

205
00:15:13,940 --> 00:15:19,590
and then the more simply my polish

206
00:15:19,590 --> 00:15:24,360
the called decision that that's all

207
00:15:24,380 --> 00:15:26,840
two of i neural

208
00:15:26,950 --> 00:15:29,640
so what these were

209
00:15:33,400 --> 00:15:41,710
i said oh by the

210
00:15:42,970 --> 00:15:45,870
price caused by a fuzzy rules

211
00:15:45,880 --> 00:15:48,830
four we

212
00:15:48,880 --> 00:15:50,470
we've shown

213
00:15:54,830 --> 00:16:00,800
they are also going to say a few things that you can tell us

214
00:16:00,810 --> 00:16:05,050
so these are different

215
00:16:08,570 --> 00:16:11,500
how we call this rule is

216
00:16:11,530 --> 00:16:19,450
so we see some of the features of trees and why they is not all

217
00:16:19,470 --> 00:16:22,450
five stations they're all

218
00:16:22,610 --> 00:16:32,440
what is this song for this is a list of all of

219
00:16:32,490 --> 00:16:39,210
and when you assemble as you have say

220
00:16:39,290 --> 00:16:42,200
is the problem

221
00:16:42,230 --> 00:16:48,400
but we don't have what i look at one hundred three and so on to

222
00:16:48,450 --> 00:16:49,610
go away

223
00:16:49,620 --> 00:16:53,640
so what is that

224
00:16:53,650 --> 00:16:57,520
also we talk about that

225
00:16:57,520 --> 00:17:03,310
and you know this is an i i

226
00:17:03,360 --> 00:17:05,780
these were

227
00:17:05,790 --> 00:17:10,090
forty five years of finer

228
00:17:10,140 --> 00:17:11,390
because the

229
00:17:11,420 --> 00:17:14,780
i the

230
00:17:17,650 --> 00:17:21,590
no indication that we are all

231
00:17:21,610 --> 00:17:27,340
but five years later i think

232
00:17:28,820 --> 00:17:34,100
i think this see more

233
00:17:38,170 --> 00:17:40,110
thank you

234
00:17:40,140 --> 00:17:44,040
all right so we do a little bit of overview predictive learning and decision trees

235
00:17:45,300 --> 00:17:48,620
predictive learning just together are addition right we have to

236
00:17:48,670 --> 00:17:53,300
independent the labels are inputs t p in this example in response to the good

237
00:17:53,300 --> 00:17:54,160
or bad

238
00:17:54,210 --> 00:17:57,140
good is blue band screen

239
00:17:57,180 --> 00:17:57,970
you might

240
00:17:57,990 --> 00:18:02,480
i don't know how comes out of the scaling your nodes sorry about that we

241
00:18:02,480 --> 00:18:05,790
can think ahead completely

242
00:18:05,800 --> 00:18:08,930
so in the one of mark while looking at the green you know there's there's

243
00:18:10,130 --> 00:18:13,860
definitely of boundary then there's the one there in the middle there is the question

244
00:18:13,860 --> 00:18:16,770
mark what's what's that should only be labeled

245
00:18:16,790 --> 00:18:21,920
depending on what estimation technique you have a different label for that or you may

246
00:18:21,920 --> 00:18:28,060
even have and and certainly value for year determination

247
00:18:29,910 --> 00:18:33,230
we can be descriptive trying to summarize the data

248
00:18:33,240 --> 00:18:37,480
in some way to understand what actionable or predicting which is more likely what we

249
00:18:37,530 --> 00:18:38,440
focus on

250
00:18:38,460 --> 00:18:42,070
what is the response to the class of new point middle

251
00:18:43,480 --> 00:18:46,100
so ordinary linear regression will fit data

252
00:18:46,100 --> 00:18:50,100
both the interior points and on the boundary points

253
00:18:50,120 --> 00:18:53,820
the two creations are called the MIB creations

254
00:18:53,840 --> 00:18:55,540
and the you might be flow

255
00:18:55,580 --> 00:18:59,400
can lead to the solution to this might creations

256
00:18:59,440 --> 00:19:01,640
as in newport in fact

257
00:19:01,660 --> 00:19:03,320
yemeni flow

258
00:19:03,340 --> 00:19:10,220
it's essentially has the same computational power as the ricci flow for surfaces however this

259
00:19:10,220 --> 00:19:15,980
will sectors they hold for manifold

260
00:19:17,540 --> 00:19:22,150
as we have seen we have some results from the smoke setting but you are

261
00:19:22,210 --> 00:19:26,820
to apply this to the engineer feels we need to discard as everything

262
00:19:26,840 --> 00:19:32,040
now comes the discrete as asian

263
00:19:32,180 --> 00:19:33,920
in engineering fields

264
00:19:33,940 --> 00:19:37,920
surfaces are represented as piece that's linear

265
00:19:38,380 --> 00:19:40,420
triangular meshes

266
00:19:40,440 --> 00:19:45,260
and so it's kind of discrete models can be easily acquired using the scanners in

267
00:19:45,260 --> 00:19:48,260
real time these days

268
00:19:48,340 --> 00:19:54,660
on such kind of peace last linear constructions we can define the notion of metrics

269
00:19:54,820 --> 00:19:57,880
curvature is an exceptional

270
00:19:57,900 --> 00:20:02,620
the discrete metric is defined as the function over edges

271
00:20:02,820 --> 00:20:08,020
say just says the l one l two announce three in the first figure

272
00:20:08,180 --> 00:20:13,500
the function should satisfy in the triangle inequality

273
00:20:13,560 --> 00:20:17,120
the discrete curvature another hand

274
00:20:17,140 --> 00:20:20,530
it is also a function but is defined on the viruses

275
00:20:20,840 --> 00:20:22,600
and the worries

276
00:20:22,620 --> 00:20:27,280
footage each were is the curvature is defined as the

277
00:20:27,320 --> 00:20:29,520
difference between two pi

278
00:20:29,540 --> 00:20:35,780
and it's running angles for the interior minister and sort the boundary waters

279
00:20:35,820 --> 00:20:40,420
the curvature defined as pi minus the surrounding angles

280
00:20:40,440 --> 00:20:46,520
also we will we will like to know that the total curvature should satisfy the

281
00:20:46,520 --> 00:20:54,360
discrete gas with that theory

282
00:20:56,160 --> 00:20:57,940
the discrete metrics

283
00:20:57,960 --> 00:21:02,160
can determine the discrete curvature by using the cosine law

284
00:21:02,180 --> 00:21:06,660
here i show the cosine law in the euclidean case we also have the similar

285
00:21:06,660 --> 00:21:15,040
laws for hyperbolic and spherical cases

286
00:21:15,100 --> 00:21:18,220
as we have seen the ricci flow

287
00:21:18,240 --> 00:21:23,260
well the discrete curvature flows deform the metric can family

288
00:21:23,320 --> 00:21:29,580
and in the discrete setting we have two approaches to the discrete rising this process

289
00:21:29,700 --> 00:21:32,470
the first one is analogy call one

290
00:21:32,700 --> 00:21:35,140
this is by discrete you might be flow

291
00:21:35,180 --> 00:21:41,520
it uses the typical discrete notions of metric and the vertex and the come from

292
00:21:41,530 --> 00:21:48,580
factors a second one is john magic approach which is the discrete ricci flow

293
00:21:48,660 --> 00:21:53,680
this way use the so-called circle packing metric

294
00:21:53,740 --> 00:21:58,620
OK let's look at the discrete you might be flow first

295
00:21:58,860 --> 00:22:00,280
for the you have to be flow

296
00:22:00,320 --> 00:22:04,120
we can define the can formal factor well actually it's a lot of that come

297
00:22:04,120 --> 00:22:05,340
from the factor which is you

298
00:22:06,080 --> 00:22:08,340
on the various is directly

299
00:22:08,360 --> 00:22:10,880
and we use the

300
00:22:10,900 --> 00:22:14,420
excellent as the discrete metric

301
00:22:14,440 --> 00:22:19,000
in this way we can deform by multiplying with

302
00:22:19,080 --> 00:22:24,200
eight e to the u i and times e to the u j

303
00:22:24,220 --> 00:22:28,100
and the discrete you might be flow in this case can be defined using the

304
00:22:28,440 --> 00:22:38,620
following differential equation which is defined in terms of the your URI and k i

305
00:22:38,680 --> 00:22:41,200
on the other hand discrete ricci flow

306
00:22:41,700 --> 00:22:44,440
defines in another way

307
00:22:44,500 --> 00:22:47,640
it uses the so-called circle packing metric

308
00:22:47,680 --> 00:22:51,390
basically for everyone x on the surface

309
00:22:51,420 --> 00:22:56,680
we can associated with a circle of radius

310
00:22:56,760 --> 00:22:58,420
and for each edge

311
00:22:58,700 --> 00:23:03,260
the two circles can intersect at the end goal of

312
00:23:03,280 --> 00:23:04,400
i j

313
00:23:04,500 --> 00:23:09,600
and the edge length could be given using the cosine law

314
00:23:09,640 --> 00:23:14,620
again here i only you show the euclidean case you have similar results for hyperbolic

315
00:23:14,620 --> 00:23:16,940
and spherical one

316
00:23:17,800 --> 00:23:18,820
for the

317
00:23:18,860 --> 00:23:20,520
this could ricci flow

318
00:23:20,560 --> 00:23:25,000
we can define it in terms of the vertex really i

319
00:23:25,020 --> 00:23:35,320
that my and the vertex curvature k i

320
00:23:35,340 --> 00:23:43,100
as i have said discrete ricci flow can be defined for euclidean hyperbolic or spherical

321
00:23:43,100 --> 00:23:47,160
cases for example for euclidean cases

322
00:23:47,180 --> 00:23:50,640
we can define an edgy cut the entropy energy

323
00:23:50,780 --> 00:23:54,550
then is actually an integration

324
00:23:54,790 --> 00:23:56,120
the surfaces

325
00:23:57,180 --> 00:23:58,590
using this energy

326
00:23:58,620 --> 00:24:03,360
during them all these are not allowed to to show that

327
00:24:03,440 --> 00:24:09,840
the euclidean ricci energy is well defined and in this context that means there exists

328
00:24:09,840 --> 00:24:12,040
a unique global minimum

329
00:24:12,100 --> 00:24:19,920
and the discrete euclidean ricci flow converges to that global minimum

330
00:24:19,940 --> 00:24:28,120
guaranteed by that syria we can find some numerical methods so the discrete ricci flow

331
00:24:28,200 --> 00:24:33,120
the first method is a gradient descent method

332
00:24:33,140 --> 00:24:37,980
since ricci flow is just the gradient flow to minimise the ricci energy

333
00:24:38,000 --> 00:24:42,960
we can use the gradient descent method directly or in another way

334
00:24:43,000 --> 00:24:48,030
we can use the newton's method here we need to compute the hessian matrix of

335
00:24:48,030 --> 00:24:53,560
the ricci energy in the following way

336
00:24:53,760 --> 00:25:00,900
this figure shows the results of the discrete euclidean ricci flow

337
00:25:00,940 --> 00:25:05,900
the figure on the left is the original surface and the figure in the middle

338
00:25:05,940 --> 00:25:07,040
is the

339
00:25:07,060 --> 00:25:08,860
canonical metric

340
00:25:08,900 --> 00:25:13,700
wisconsin college actually hear the country's zero

341
00:25:13,720 --> 00:25:18,160
and the figure on the right is a universal covering space

342
00:25:18,200 --> 00:25:21,840
now into surface

343
00:25:21,880 --> 00:25:27,460
we can apply the discrete ricci flow for hyperbolic cases and here is the result

344
00:25:27,460 --> 00:25:34,640
from the hyperbolic discrete ricci flow the surface on the left can be double covered

345
00:25:35,500 --> 00:25:41,600
for the hygienist closed surface and then we can flatten the surface into the hyperbolic

346
00:25:46,720 --> 00:25:50,160
OK after discrete house everything

347
00:25:50,180 --> 00:25:53,780
we are allowed to apply the discrete curvature flows

348
00:25:53,820 --> 00:25:59,020
two sold some practical problems in engineering fields

349
00:25:59,060 --> 00:26:04,900
and actually many such kind of problems can be formulated as finding a special metric

350
00:26:04,900 --> 00:26:07,740
with certain curvature concerts

351
00:26:10,100 --> 00:26:11,780
service privatisation

352
00:26:12,260 --> 00:26:15,700
it is actually to find a flat metric

353
00:26:15,700 --> 00:26:18,550
it's about the general setting of the

354
00:26:18,960 --> 00:26:22,490
a structured learning problem and he

355
00:26:22,500 --> 00:26:26,290
because i asked him so he made a distinction between the gentleman at the end

356
00:26:26,290 --> 00:26:31,880
of this commitment to the he told us the advantage of discriminative method

357
00:26:31,940 --> 00:26:34,780
is the ability to t incorporates

358
00:26:34,850 --> 00:26:42,760
more information into the model such as sequences looking into the previous or next observations

359
00:26:43,120 --> 00:26:49,540
in order to predict the label and he talked about the dynamic programming for sequences

360
00:26:49,570 --> 00:26:53,070
two he said that every decoding is good for

361
00:26:53,080 --> 00:26:55,450
computing the most likely

362
00:26:55,470 --> 00:27:01,390
label assignment for sequences and forward backward algorithm is good for

363
00:27:01,400 --> 00:27:08,640
computing the expectations and we saw MEMM serious and perceptrons as

364
00:27:08,650 --> 00:27:15,970
as the discriminative methods now we're going to see if the ends in detail so

365
00:27:15,970 --> 00:27:20,030
all one thing he didn't explicitly mention is the perceptron we can think of the

366
00:27:20,030 --> 00:27:26,950
perceptron as as a as a classification method where we're trying to find find a

367
00:27:26,950 --> 00:27:30,620
hyperplane that separates the data

368
00:27:30,640 --> 00:27:36,760
and of a given that one obvious thing to do is to find a hyperplane

369
00:27:36,760 --> 00:27:41,310
that separates the data with a maximum margin as opposed to just finding a separation

370
00:27:41,310 --> 00:27:46,120
hyperplane and as we can do in st anne's and we're going to talk about

371
00:27:46,120 --> 00:27:49,610
it spans two instructional domain more in detail

372
00:27:49,640 --> 00:27:54,140
well because that is something that i have proposed with my advisor in a graduate

373
00:27:56,480 --> 00:27:57,490
and also

374
00:27:57,500 --> 00:28:03,000
there is there there was a follow-up paper by ben taskar and colleagues in and

375
00:28:03,020 --> 00:28:09,760
turns it seems to be the most cited paper in this field really structured learning

376
00:28:09,760 --> 00:28:13,630
field so i think that this is some crucial

377
00:28:13,640 --> 00:28:20,080
literature so for svms where we will first defines a margin and then look into

378
00:28:20,080 --> 00:28:26,080
the primal and dual of optimisation problems will see an algorithm and then david we'll

379
00:28:26,080 --> 00:28:28,990
talk about generalisation bounds

380
00:28:29,800 --> 00:28:33,970
so in the general setting binary classification problem what does this mean and do we

381
00:28:33,970 --> 00:28:36,210
have a binary class

382
00:28:36,220 --> 00:28:40,390
classification we have a lose lose squares

383
00:28:40,400 --> 00:28:44,840
and that does and we want to set fire to find a hyperplane and what

384
00:28:44,840 --> 00:28:48,150
is he says is to find a hyperplane

385
00:28:48,160 --> 00:28:53,410
that separates the data with maximum margin the hyperplane w that separates the data with

386
00:28:53,430 --> 00:28:55,510
maximum margin where the marginal

387
00:28:55,520 --> 00:28:58,070
gamma is defined to be the distance

388
00:28:58,080 --> 00:28:59,950
from each class

389
00:28:59,970 --> 00:29:03,450
that are closest to the separation hyperplane

390
00:29:03,470 --> 00:29:05,410
so given this hyperplane

391
00:29:05,430 --> 00:29:07,340
each state

392
00:29:07,360 --> 00:29:09,420
when we take the inner product

393
00:29:09,630 --> 00:29:15,580
in look its and we can assign label rate if it's less than the one

394
00:29:15,580 --> 00:29:17,410
that it is

395
00:29:17,460 --> 00:29:22,480
if it's sorry if this is this is the type of its name negative we

396
00:29:22,480 --> 00:29:26,580
assign it to the negative class and close to be assigned to the positive class

397
00:29:26,590 --> 00:29:33,980
and this optimisation problem can be as we have seen many times in this summer

398
00:29:33,980 --> 00:29:38,900
school can be stated in terms of in l two norm we try to minimize

399
00:29:38,900 --> 00:29:44,050
the l two norm of the separating hyperplane with respect to some constraints and the

400
00:29:44,050 --> 00:29:46,360
constraints tell us that for each

401
00:29:46,380 --> 00:29:47,710
data points

402
00:29:47,720 --> 00:29:50,270
the label we assign

403
00:29:50,290 --> 00:29:54,150
to that data point which is the sign of this

404
00:29:54,160 --> 00:29:58,120
in our product so much to the correct label

405
00:29:58,170 --> 00:30:03,660
and not only that the real value coming from the inner product gives us confidence

406
00:30:03,660 --> 00:30:06,120
score and we want to be

407
00:30:06,170 --> 00:30:11,620
larger than one we want that multiplied with the correct label to be larger than

408
00:30:13,010 --> 00:30:20,740
OK we need is already well and then in the real world problems in in

409
00:30:20,930 --> 00:30:24,330
in general it's not possible it is not possible to

410
00:30:24,730 --> 00:30:28,840
as for being a separation hyperplane so we're going to have some nice we we

411
00:30:29,230 --> 00:30:34,930
we say that not all the constraints will be key can be

412
00:30:35,630 --> 00:30:37,710
we can cannot

413
00:30:37,750 --> 00:30:41,640
we can achieve all the constraints so we're going to allow someone relations in those

414
00:30:41,950 --> 00:30:45,320
so there will be some data points like this which may not which may be

415
00:30:45,320 --> 00:30:50,590
classified incorrectly or they will be some data points like this which may not achieve

416
00:30:50,590 --> 00:30:58,370
some marginal so for those we're going to introduce some slack variable size

417
00:30:58,380 --> 00:31:04,770
so that we we achieve the margin constraints but we're gonna get penalised in our

418
00:31:04,770 --> 00:31:09,160
objective function for this innovation

419
00:31:09,210 --> 00:31:11,400
for the slack variables

420
00:31:12,160 --> 00:31:18,290
and the constraints that hold with equality we're going to call them active constraints

421
00:31:18,920 --> 00:31:24,160
and the data points that correspond to those constraints we're going to call them support

422
00:31:26,200 --> 00:31:29,200
and again we have seen this many times

423
00:31:29,210 --> 00:31:36,880
that in fact are separating hyperplane can be represented in terms of our support vectors

424
00:31:36,890 --> 00:31:40,340
so when we are given a new data point x

425
00:31:40,390 --> 00:31:45,740
the way we classify as we take the inner product with into one

426
00:31:45,750 --> 00:31:47,320
of this new data

427
00:31:47,330 --> 00:31:50,830
with the support vectors and combine them linearly with some

428
00:31:50,890 --> 00:31:53,410
alpha parameters

429
00:31:53,420 --> 00:31:57,180
so this was the standard binary class support vector

430
00:31:57,230 --> 00:31:58,580
machine setting

431
00:31:58,600 --> 00:31:59,510
and how

432
00:31:59,520 --> 00:32:02,380
can we generalize this to structured problems

433
00:32:02,420 --> 00:32:07,710
well first we need to define the margin what is the marginal

434
00:32:07,730 --> 00:32:11,270
i should have said in the previous slide that the margin

435
00:32:11,290 --> 00:32:13,010
is actually this value

436
00:32:13,020 --> 00:32:18,680
the real value the the compatibility score of

437
00:32:18,700 --> 00:32:20,080
i'm sorry to the

438
00:32:20,160 --> 00:32:22,890
the confidence of our assignment

439
00:32:24,020 --> 00:32:27,490
ten data point times it's correct labels

440
00:32:27,500 --> 00:32:29,200
and in this case

441
00:32:29,230 --> 00:32:31,410
what we say

442
00:32:31,420 --> 00:32:33,350
what they it from your hands

443
00:32:33,360 --> 00:32:39,180
previous work which he also presented here we say we define the separation had the

444
00:32:39,180 --> 00:32:45,460
separation margin as follows for each data point i will look at the value which

445
00:32:45,460 --> 00:32:49,520
i'm sure everyone here is familiar with the recommendation problem where

446
00:32:49,980 --> 00:32:52,980
we have some set of users and some set of items

447
00:32:53,000 --> 00:32:56,340
the goal is to predict which items are given user will like

448
00:32:56,470 --> 00:32:59,090
now we've got several sort of data

449
00:32:59,130 --> 00:33:03,630
which can help solve this problem so most importantly we've got ratings data

450
00:33:03,650 --> 00:33:09,770
which is some indication of how much user will let users like other items

451
00:33:09,810 --> 00:33:15,560
we might also have information which is specific about the items so variations and these

452
00:33:15,590 --> 00:33:21,670
might have cast lists or johnr information or use or items are actually going to

453
00:33:21,740 --> 00:33:24,340
the technical specifications for example

454
00:33:24,440 --> 00:33:28,410
we might also have information about a user so we could have demographic information it's

455
00:33:28,670 --> 00:33:35,640
very easy to where they live or information from social network data

456
00:33:35,760 --> 00:33:40,260
so there are two main approaches to recommendation problem in collaborative filtering and content based

457
00:33:41,500 --> 00:33:43,330
so collaborative filtering

458
00:33:43,330 --> 00:33:46,480
the idea is to use the information about which

459
00:33:46,530 --> 00:33:48,030
i sense

460
00:33:48,030 --> 00:33:52,580
like four to make predictions what i can say like future

461
00:33:53,550 --> 00:33:56,500
this method in general works really really well

462
00:33:57,830 --> 00:34:02,420
those who follow the netflix prize it doesn't work quite million dollars well yet but

463
00:34:02,500 --> 00:34:03,670
really close

464
00:34:03,740 --> 00:34:04,860
how to coaching

465
00:34:04,880 --> 00:34:06,270
generally works very well

466
00:34:06,280 --> 00:34:09,240
because it captures in general both the

467
00:34:09,250 --> 00:34:12,060
features of nice and so whether maybe

468
00:34:12,080 --> 00:34:15,470
it will recommend horror movies like armor these

469
00:34:15,640 --> 00:34:19,720
also differentiate between good horror movie about her

470
00:34:20,130 --> 00:34:25,450
however this has problems the main problems of what we do we got fourteen movie

471
00:34:25,470 --> 00:34:27,630
which we haven't which alternated for

472
00:34:27,670 --> 00:34:31,920
what we do need to come the meeting you get what you recommend in that

473
00:34:35,050 --> 00:34:39,470
in one sense is edge problems but quite important problems because

474
00:34:39,480 --> 00:34:44,640
if your business is recommending movies you really want to recommend new to people

475
00:34:44,670 --> 00:34:47,620
and if you got to website recommends things to people who want to make sure

476
00:34:47,620 --> 00:34:49,020
that new people

477
00:34:49,020 --> 00:34:53,520
stable system rather than the realise of the ratings of the recommendations of rubbish only

478
00:34:53,520 --> 00:34:56,520
want someone else's recommender system

479
00:34:56,610 --> 00:34:59,900
so this is quite a problem with current methods

480
00:34:59,920 --> 00:35:02,670
on the other hand we have content based methods

481
00:35:02,680 --> 00:35:07,800
and content based filtering we look at what items users like to recommend items that

482
00:35:07,800 --> 00:35:10,900
are similar to the ones that uses light and some

483
00:35:10,920 --> 00:35:14,110
measure of similarity based on the content of the items

484
00:35:14,150 --> 00:35:19,320
this has the advantage that center new items very easily doesn't matter enunciated by got

485
00:35:20,290 --> 00:35:22,770
that's what active in it all

486
00:35:22,800 --> 00:35:25,120
what genre it is o

487
00:35:25,230 --> 00:35:27,540
reviews on IMDB or whatever

488
00:35:27,580 --> 00:35:30,740
and in general content based filtering

489
00:35:31,110 --> 00:35:32,770
really good at capturing

490
00:35:32,790 --> 00:35:35,020
jonathan type information

491
00:35:43,400 --> 00:35:45,580
but there less good

492
00:35:46,050 --> 00:35:53,520
that with less good at identifying quality of an itemset telling thing that could hardly

493
00:35:53,540 --> 00:35:55,550
be invited her

494
00:35:55,600 --> 00:35:59,180
so it seems these two approaches have complementary

495
00:35:59,370 --> 00:36:00,980
advantages and problem

496
00:36:01,040 --> 00:36:04,010
so we were looking for a

497
00:36:04,050 --> 00:36:05,700
that combine these two

498
00:36:05,710 --> 00:36:11,170
approaches to recommendation

499
00:36:12,170 --> 00:36:16,070
matrix factorisation is quite commonly used approach in

500
00:36:16,080 --> 00:36:19,050
recommender systems are taken captive filtering

501
00:36:19,100 --> 00:36:22,550
and the basic idea here is we you are

502
00:36:22,550 --> 00:36:27,900
ratings data as a incomplete matrix containing the

503
00:36:27,920 --> 00:36:31,510
ratings given by the users to the end with movies

504
00:36:31,550 --> 00:36:36,830
which i that rises matrix product too low rank matrices here a and b

505
00:36:36,850 --> 00:36:38,770
where a is you by k

506
00:36:38,860 --> 00:36:41,420
b is k by and

507
00:36:41,480 --> 00:36:43,240
we some

508
00:36:43,580 --> 00:36:45,610
social fourteen

509
00:36:45,710 --> 00:36:47,640
right the ratings from this

510
00:36:48,320 --> 00:36:51,820
to make

511
00:36:53,540 --> 00:36:57,730
so we extend we can extend this idea to factor most places at the same

512
00:36:58,620 --> 00:37:01,100
so here i've demonstrated how we can

513
00:37:01,110 --> 00:37:04,750
factorize a partially observed

514
00:37:04,760 --> 00:37:07,300
we been things matrix are

515
00:37:07,310 --> 00:37:10,050
and the fully observed content matrix s

516
00:37:10,110 --> 00:37:11,050
and here

517
00:37:11,050 --> 00:37:16,620
are is addressed a b s is called the bayes factor is going to be

518
00:37:16,620 --> 00:37:17,680
in c

519
00:37:17,750 --> 00:37:22,720
and the latent matrix b is constrained to be common to both makes these

520
00:37:22,800 --> 00:37:26,480
now here shown to just two matches but if we had

521
00:37:26,490 --> 00:37:29,730
in addition to have contemplated as we also had

522
00:37:29,780 --> 00:37:34,990
demographic information because i not the matrix to the left of the diagram constraint that

523
00:37:37,240 --> 00:37:44,000
going backwards to me would easy by a matrix where is common between the

524
00:37:44,030 --> 00:37:51,610
make the user matrix and the ratings matrix are

525
00:37:53,880 --> 00:37:57,220
we can take this model and

526
00:37:57,260 --> 00:38:02,690
users are for for the matrix factorisation we can use a new calcium cases the

527
00:38:04,170 --> 00:38:06,170
approach can take with this model

528
00:38:06,170 --> 00:38:12,780
where we have the ratings matrix are the entries this or gas industries is given

529
00:38:12,780 --> 00:38:13,930
by the product of the

530
00:38:14,240 --> 00:38:17,030
the matrix and some variants the sweat

531
00:38:17,070 --> 00:38:22,240
and similarly the matrix given by justice peter means times b

532
00:38:22,300 --> 00:38:26,490
in order to perform bayesian inference in this we put priors on the

533
00:38:27,680 --> 00:38:30,930
this makes is a b and c which is this prior to be zero mean

534
00:38:33,730 --> 00:38:34,620
we can use

535
00:38:34,670 --> 00:38:38,050
variational bayes to perform inference in this model

536
00:38:40,240 --> 00:38:42,320
this works out very nicely simply means

537
00:38:42,340 --> 00:38:46,290
we introduce a variational distribution to have a b and c

538
00:38:46,340 --> 00:38:52,410
we use jensen's inequality to learn about the log likelihood on the marginal likelihood

539
00:38:52,480 --> 00:38:54,650
we then it is our

540
00:38:54,690 --> 00:38:59,970
one of the approximation which is we assume that our variational distribution can be factorized

541
00:38:59,970 --> 00:39:01,240
into the product of the

542
00:39:01,280 --> 00:39:03,880
individual distributions on a b and c

543
00:39:03,900 --> 00:39:09,000
if we do this we see that the variational distributions are also gaussians

544
00:39:09,410 --> 00:39:14,060
we can derive the updates to the means and variances these variational distributions on a

545
00:39:14,060 --> 00:39:15,290
b and c

546
00:39:15,320 --> 00:39:16,790
we hold

547
00:39:16,840 --> 00:39:25,880
two constant and updates a b and c in turn until convergence

548
00:39:26,040 --> 00:39:29,530
this is exactly what we did on the movie lens dataset

549
00:39:29,540 --> 00:39:33,680
so here are the movies dataset is the data set

550
00:39:33,740 --> 00:39:37,900
around thousand users and around thousand half movies

551
00:39:38,570 --> 00:39:42,970
we've got a a hundred thousand ratings in total and that's what we sell participate

552
00:39:43,000 --> 00:39:44,610
paychecks obviously

553
00:39:44,660 --> 00:39:48,070
the constant data included with the movie lens dataset

554
00:39:48,090 --> 00:39:51,500
there is a

555
00:39:53,590 --> 00:40:00,440
binary shorts genre information where each movie is associated with zero or more

556
00:40:01,320 --> 00:40:03,110
a set of genres

557
00:40:03,110 --> 00:40:04,820
in orbit

558
00:40:06,670 --> 00:40:08,710
o is the half

559
00:40:08,820 --> 00:40:10,710
that makes it always

560
00:40:11,800 --> 00:40:15,590
so is the a half year this is also

561
00:40:17,260 --> 00:40:18,550
one half

562
00:40:21,590 --> 00:40:22,920
and remember

563
00:40:23,010 --> 00:40:24,900
the potential energy

564
00:40:24,940 --> 00:40:27,380
equals minus

565
00:40:27,420 --> 00:40:28,800
and so

566
00:40:28,840 --> 00:40:30,550
and earth

567
00:40:32,690 --> 00:40:34,420
divided by our

568
00:40:34,480 --> 00:40:36,920
so here you see plus half

569
00:40:36,920 --> 00:40:38,400
you see minus one

570
00:40:38,420 --> 00:40:40,610
this is indeed minus

571
00:40:40,690 --> 00:40:43,380
after potential energy

572
00:40:43,420 --> 00:40:45,440
what is the total energy

573
00:40:45,570 --> 00:40:48,260
of the earth

574
00:40:48,320 --> 00:40:50,530
around the sun that means the sum

575
00:40:50,570 --> 00:40:52,400
of potential energy

576
00:40:53,710 --> 00:40:55,260
genetic and

577
00:40:57,980 --> 00:41:01,150
the kinetic energy is minus one

578
00:41:01,860 --> 00:41:04,250
and the potential energy is you

579
00:41:04,330 --> 00:41:07,630
so this is

580
00:41:07,800 --> 00:41:10,510
is it plus one half you

581
00:41:10,570 --> 00:41:14,090
that means it's negative

582
00:41:14,130 --> 00:41:16,900
because you itself is negative

583
00:41:16,920 --> 00:41:17,900
it is also

584
00:41:17,920 --> 00:41:19,360
equal to minus k

585
00:41:19,860 --> 00:41:25,440
i just calculated for fun

586
00:41:25,440 --> 00:41:29,250
how large that number is it's the negative number large that is

587
00:41:29,260 --> 00:41:32,150
and it turns out to be approximately minus

588
00:41:32,210 --> 00:41:33,670
o point seven

589
00:41:33,670 --> 00:41:34,880
i'm stand two to thirty

590
00:41:34,900 --> 00:41:36,650
we do this

591
00:41:40,420 --> 00:41:42,460
if we get bored and we want to

592
00:41:42,510 --> 00:41:45,150
leave the solar system

593
00:41:45,210 --> 00:41:48,820
then we can ask yourself the question if we model rocket to the earth and

594
00:41:48,820 --> 00:41:51,340
we fired rockets

595
00:41:51,360 --> 00:41:53,130
how much speed should we give it

596
00:41:53,150 --> 00:41:57,110
to get away from the sun once and for all and never come back

597
00:41:57,150 --> 00:42:02,250
well we would have to do is make the total energy of the or zero

598
00:42:02,260 --> 00:42:04,840
so it can cruise all the way to infinity

599
00:42:04,860 --> 00:42:07,980
and have zero speed when it gets there

600
00:42:08,030 --> 00:42:09,840
how can i make this zero

601
00:42:09,880 --> 00:42:11,840
well i have two heads

602
00:42:11,880 --> 00:42:14,670
the kinetic energy exactly k

603
00:42:14,730 --> 00:42:17,260
because my netscape was k is zero

604
00:42:17,320 --> 00:42:20,110
if i have to

605
00:42:20,170 --> 00:42:21,730
the kinetic energy k

606
00:42:21,760 --> 00:42:23,590
then the new kinetic energy

607
00:42:23,610 --> 00:42:25,110
after the rocket burn

608
00:42:25,110 --> 00:42:28,780
could be exactly two k potential energy is not changing

609
00:42:28,780 --> 00:42:31,250
because you fired a rocket when you there

610
00:42:31,250 --> 00:42:35,900
and so you the same position after rocket fire and so you potential energy hasn't

611
00:42:37,150 --> 00:42:42,960
but you can edit energy must have doubled it hasn't doubled this doesn't become zero

612
00:42:43,800 --> 00:42:45,960
but if the kinetic energy has doubled

613
00:42:45,990 --> 00:42:48,030
then the speed must have gone up

614
00:42:48,070 --> 00:42:52,440
by the square root of two because kinetic energy is proportional to the square

615
00:42:52,610 --> 00:42:54,400
so to escape velocity

616
00:42:54,420 --> 00:42:57,710
must be the square root of two

617
00:42:57,780 --> 00:43:00,280
i'm for the velocity

618
00:43:00,320 --> 00:43:03,460
and that would be forty earth the square root of two

619
00:43:03,460 --> 00:43:05,920
i'm thirty which is about forty two kilometres

620
00:43:05,920 --> 00:43:07,280
per second

621
00:43:07,960 --> 00:43:10,340
by looking at the energy considerations

622
00:43:10,380 --> 00:43:11,300
you can

623
00:43:13,280 --> 00:43:14,630
connection between

624
00:43:14,650 --> 00:43:15,900
total energy

625
00:43:15,900 --> 00:43:17,170
escape velocity

626
00:43:17,250 --> 00:43:19,320
make the whole picture internally

627
00:43:22,590 --> 00:43:24,860
we talked for one whole lecture

628
00:43:24,880 --> 00:43:27,210
about resistive forces

629
00:43:27,250 --> 00:43:30,960
i still remember the number of that lecture number twelve

630
00:43:31,010 --> 00:43:33,260
because it's the one lecture i work

631
00:43:33,280 --> 00:43:35,150
on it more than any other

632
00:43:35,170 --> 00:43:36,230
with the help

633
00:43:36,230 --> 00:43:39,230
my graduate students they fully with it

634
00:43:39,280 --> 00:43:40,710
these wonderful

635
00:43:40,820 --> 00:43:43,900
calculations numerical solution

636
00:43:43,920 --> 00:43:44,780
and so

637
00:43:44,800 --> 00:43:46,730
let's talk a little bit about

638
00:43:46,750 --> 00:43:50,030
they resist the forces

639
00:43:50,050 --> 00:43:53,920
if an object moves through the air or liquid

640
00:43:53,960 --> 00:43:55,590
there's a resistive force

641
00:43:58,420 --> 00:44:01,920
which all these opposes the velocity

642
00:44:01,920 --> 00:44:04,070
it has two terms

643
00:44:04,110 --> 00:44:08,190
k one times the this is the speed is always positive

644
00:44:08,300 --> 00:44:09,960
was a two

645
00:44:12,630 --> 00:44:15,110
and this is the unit vector

646
00:44:15,170 --> 00:44:18,360
of the velocity always opposing develop

647
00:44:18,380 --> 00:44:19,670
so this is

648
00:44:19,690 --> 00:44:23,320
is the magnitude of the resistive force

649
00:44:23,360 --> 00:44:26,460
we dealt exclusively with fears remember

650
00:44:26,550 --> 00:44:28,460
and we get some

651
00:44:28,590 --> 00:44:31,960
would experiment with spheres k one

652
00:44:32,010 --> 00:44:34,570
equal c one times are

653
00:44:34,650 --> 00:44:36,570
and k two

654
00:44:38,780 --> 00:44:41,630
times are square

655
00:44:41,650 --> 00:44:45,530
and we did some experiments with corn cereal

656
00:44:45,570 --> 00:44:46,610
and this was

657
00:44:46,650 --> 00:44:49,170
the search for which see one

658
00:44:49,250 --> 00:44:53,320
was approximately one hundred sixty

659
00:44:53,360 --> 00:44:55,030
and c two

660
00:44:55,170 --> 00:44:57,940
it's about one point two

661
00:44:57,980 --> 00:45:01,360
ten to thirty kilograms per cubic metres

662
00:45:01,380 --> 00:45:03,480
o memory units of this

663
00:45:03,480 --> 00:45:05,860
because that's always density

664
00:45:05,860 --> 00:45:09,760
in fact it's always a little smaller than the actual density of the

665
00:45:09,780 --> 00:45:13,230
the equator of the sphere itself this is more complicated

666
00:45:13,590 --> 00:45:16,920
unit but you can figure that one out for yourself and so we had ball

667
00:45:16,920 --> 00:45:20,460
bearings one-eighth of an inch diameter two quarter of an inch

668
00:45:20,460 --> 00:45:21,760
and we dropped in

669
00:45:23,340 --> 00:45:24,800
carol here

670
00:45:24,860 --> 00:45:26,530
if we take the one for now

671
00:45:26,550 --> 00:45:29,420
which is a quarter of an inch diameter

672
00:45:29,460 --> 00:45:30,940
and the mass of the water

673
00:45:30,960 --> 00:45:34,320
it's about o point one grams

674
00:45:34,360 --> 00:45:35,920
we know density

675
00:45:35,940 --> 00:45:39,150
of steel and so you can figure out the mass just take my word for

676
00:45:40,010 --> 00:45:41,940
and so the question now is

677
00:45:41,940 --> 00:45:42,750
if this

678
00:45:42,750 --> 00:45:46,250
well there starts to fall what is this speed that it will

679
00:45:46,320 --> 00:45:47,940
chief well

680
00:45:47,990 --> 00:45:49,280
in the beginning

681
00:45:49,360 --> 00:45:50,110
there is

682
00:45:50,130 --> 00:45:51,420
only MG

683
00:45:51,440 --> 00:45:52,460
on that ball

684
00:45:52,460 --> 00:45:55,320
and the teeny-weeny little resistive force

685
00:45:55,340 --> 00:45:57,400
as the speed built up

686
00:45:57,420 --> 00:45:59,550
the resisting force will grow

687
00:45:59,570 --> 00:46:01,030
and there comes a time

688
00:46:01,050 --> 00:46:02,400
that it's equal

689
00:46:02,400 --> 00:46:03,230
two mg

690
00:46:03,260 --> 00:46:06,460
then object will not be accelerated anymore

691
00:46:06,460 --> 00:46:09,780
and it will have what we call the for terminal velocity

692
00:46:09,780 --> 00:46:12,670
so what are the conditions for terminal velocity

693
00:46:13,420 --> 00:46:16,210
this term equals MC

694
00:46:16,250 --> 00:46:17,690
in other words that

695
00:46:17,710 --> 00:46:18,780
c one

696
00:46:21,800 --> 00:46:22,760
he two

697
00:46:24,570 --> 00:46:26,900
this graph becomes equal

698
00:46:26,940 --> 00:46:27,820
two mg

699
00:46:27,860 --> 00:46:29,260
and that's the case

700
00:46:29,280 --> 00:46:31,630
then we have reached the terminal

701
00:46:37,320 --> 00:46:39,230
this is a

702
00:46:39,260 --> 00:46:41,150
quadratic equation in

703
00:46:41,170 --> 00:46:43,340
view of t

704
00:46:43,380 --> 00:46:45,150
you know c one

705
00:46:45,190 --> 00:46:48,440
you know c to you know to radius of the ball bearings in order massive

706
00:46:48,510 --> 00:46:51,420
ball-bearing so we can solve for the terminal

707
00:46:51,460 --> 00:46:54,010
velocity you get two solutions it is

708
00:46:54,090 --> 00:46:58,510
quadratic equation one it will be non-physical you will see so you keep the one

709
00:46:58,510 --> 00:47:00,650
that is physical but if you did that

710
00:47:00,690 --> 00:47:01,460
that would be

711
00:47:01,480 --> 00:47:03,510
the city thing to do

712
00:47:03,530 --> 00:47:05,590
and the reason why that would be silly is

713
00:47:05,610 --> 00:47:07,650
but i claim that this term

714
00:47:07,710 --> 00:47:11,260
which by the way we call the pressure terms

715
00:47:11,420 --> 00:47:12,460
this term

716
00:47:12,460 --> 00:47:14,050
can be completely ignored

717
00:47:14,050 --> 00:47:15,570
the most beautiful

718
00:47:15,650 --> 00:47:19,290
we put it in the with we put it in there

719
00:47:19,400 --> 00:47:20,660
label set

720
00:47:21,910 --> 00:47:26,020
we will come up with the most support the process

721
00:47:26,070 --> 00:47:30,240
this is this is what we're trying to do also worked perfectly direction

722
00:47:30,410 --> 00:47:35,580
but we're trying to work applied in the framework of the los angeles

723
00:47:36,580 --> 00:47:39,810
so why you will be able to say and reason

724
00:47:39,820 --> 00:47:41,700
so all of

725
00:47:42,310 --> 00:47:45,790
don't have we represent our data to explain why we should be able to say

726
00:47:47,800 --> 00:47:49,450
the most difficult

727
00:47:49,470 --> 00:47:53,760
representation with a our documents which the work to create a table

728
00:47:53,900 --> 00:47:56,200
one have the documents so the terms

729
00:47:56,270 --> 00:47:59,320
and we have the characters of its terminates document

730
00:47:59,430 --> 00:48:04,120
so what we are endorsing the call caller utterances of the documents and the war

731
00:48:04,230 --> 00:48:11,480
the problem we have its war because what i've synonyms many water polysems

732
00:48:11,490 --> 00:48:15,700
and this can create a disconnection between the topics of the documents and words what

733
00:48:15,700 --> 00:48:17,420
i mean and example

734
00:48:17,460 --> 00:48:21,270
if we take the world if a document has to walk able inside

735
00:48:21,330 --> 00:48:26,930
we don't know if you do the topic its fruits or it's a computers max

736
00:48:26,980 --> 00:48:29,580
so here's where second

737
00:48:29,630 --> 00:48:31,110
and a he

738
00:48:31,120 --> 00:48:35,130
also try to give something behind there was something about the

739
00:48:35,150 --> 00:48:37,200
topics of the day

740
00:48:37,210 --> 00:48:38,750
of the document

741
00:48:38,860 --> 00:48:40,830
and how he does that

742
00:48:40,910 --> 00:48:45,530
he does that by introducing london viable which is the components

743
00:48:45,580 --> 00:48:46,900
actually the topic

744
00:48:48,600 --> 00:48:56,510
and with an organ model one model our our algorithm using the equation there were

745
00:48:56,510 --> 00:48:57,060
actually the

746
00:48:57,630 --> 00:49:01,200
we have the profile of the component of the final of the topic that ended

747
00:49:01,200 --> 00:49:02,130
the war

748
00:49:02,220 --> 00:49:06,070
from what's gone there consisted the topic

749
00:49:06,120 --> 00:49:07,670
and the topics

750
00:49:07,690 --> 00:49:10,390
with sorry in a document from the wood from

751
00:49:10,440 --> 00:49:13,350
four which set up its the document

752
00:49:13,400 --> 00:49:16,830
but just beginning we're just semisupervised

753
00:49:17,020 --> 00:49:18,710
say so one two

754
00:49:18,790 --> 00:49:21,400
top place in semi supervised learning

755
00:49:23,000 --> 00:49:26,910
we will use the framework organisms revise framework introduced by e

756
00:49:26,930 --> 00:49:33,050
the and a good last year two thousand five

757
00:49:34,640 --> 00:49:36,100
so let's see

758
00:49:36,120 --> 00:49:37,860
normally can

759
00:49:37,940 --> 00:49:42,840
can be used as we can extend the same this over my this was revised

760
00:49:42,950 --> 00:49:44,190
more easily

761
00:49:44,260 --> 00:49:48,840
but there's a problem that the curious it that the ratio of labeled unlabeled data

762
00:49:48,840 --> 00:49:50,230
is really low

763
00:49:50,350 --> 00:49:54,080
then many components with no knowledge data appears

764
00:49:54,270 --> 00:49:56,860
and that's great the problem of

765
00:49:57,810 --> 00:50:02,490
and that's the problem because then arbitrary probabilities will be assigned to the to these

766
00:50:04,420 --> 00:50:07,130
and this will also influence or

767
00:50:09,260 --> 00:50:15,080
because the components on the labeled data then even a real answer to probability

768
00:50:15,150 --> 00:50:16,860
we learn

769
00:50:16,880 --> 00:50:23,650
we'll assign the particular labelled the components and then we can alternative if said in

770
00:50:25,620 --> 00:50:31,230
so the idea is to introduce another viable with a fake label

771
00:50:31,280 --> 00:50:34,340
let's say zero lazio zero then

772
00:50:35,010 --> 00:50:40,960
the labelled data we keep their labor really and they labelled data we all get

773
00:50:41,040 --> 00:50:42,370
the labels so

774
00:50:42,390 --> 00:50:44,010
he was in the binary case

775
00:50:44,020 --> 00:50:45,470
for example

776
00:50:45,490 --> 00:50:49,890
the labelled examples keep their own labels and all the labels gets into

777
00:50:49,900 --> 00:50:52,690
into the new labels

778
00:50:52,740 --> 00:50:53,650
i'm using

779
00:50:53,660 --> 00:50:54,690
this matrix

780
00:50:54,710 --> 00:50:59,080
we can say we can train our model of course after the training

781
00:50:59,190 --> 00:50:59,990
we have

782
00:51:00,000 --> 00:51:02,980
somehow to assign these probabilities

783
00:51:03,100 --> 00:51:06,690
which are obtained for the fake label in the truly because this is what we

784
00:51:06,700 --> 00:51:07,960
are interesting in

785
00:51:08,060 --> 00:51:13,170
so using this equation what we're doing is downweighting the unlabelled example which is actually

786
00:51:14,740 --> 00:51:16,660
and to a

787
00:51:16,730 --> 00:51:20,900
but of course the influence but there there are not enforced and the label example

788
00:51:20,910 --> 00:51:24,230
which are the motion to set

789
00:51:24,310 --> 00:51:26,050
so using this

790
00:51:26,100 --> 00:51:27,710
this new viable

791
00:51:27,720 --> 00:51:29,160
our model

792
00:51:29,780 --> 00:51:31,470
it becomes

793
00:51:31,480 --> 00:51:33,030
it's like this

794
00:51:33,050 --> 00:51:37,000
and in order to train on more than we use a in my life has

795
00:51:37,000 --> 00:51:39,370
all have london bibles

796
00:51:40,790 --> 00:51:44,860
the log likelihood of all of our model is actually the sum of the documents

797
00:51:44,860 --> 00:51:48,380
is the sum of the degree of fulfilment so full words

798
00:51:48,450 --> 00:51:53,660
o thing concurrency of occurrences of the word in a document

799
00:51:53,670 --> 00:51:58,940
times the probability that john on the local food and probability of document the water

800
00:51:58,940 --> 00:52:05,420
and the label of this work including the fake

801
00:52:05,460 --> 00:52:08,250
here's the EM with my one day

802
00:52:08,260 --> 00:52:10,060
get the data we are now

803
00:52:10,510 --> 00:52:12,380
i'm not going a using

804
00:52:12,390 --> 00:52:13,880
using their

805
00:52:13,890 --> 00:52:17,680
the cloister naive brindley before

806
00:52:17,690 --> 00:52:20,160
we obtain their

807
00:52:20,170 --> 00:52:25,620
the probability so far of the classification probabilities

808
00:52:25,660 --> 00:52:30,510
so here is where active learning appeared

809
00:52:30,630 --> 00:52:35,050
want to apply stacked learning on the top of the same supervised learning

810
00:52:35,740 --> 00:52:38,580
now all labelled data have probability

811
00:52:39,380 --> 00:52:41,930
of being in a in class

812
00:52:42,030 --> 00:52:44,890
so which is the most ambiguous example

813
00:52:44,940 --> 00:52:46,730
where we use

814
00:52:46,740 --> 00:52:48,200
which is the most

815
00:52:48,210 --> 00:52:49,910
in this method with it

816
00:52:49,960 --> 00:52:54,230
the example which is which has the highest entropy

817
00:52:54,270 --> 00:52:58,750
in the multiclass case in the binary this can be supplied simplified there

818
00:52:58,910 --> 00:53:01,880
document was the probability closest to p o five

819
00:53:02,000 --> 00:53:06,270
with which is so this document will label will put it in a set of

820
00:53:06,270 --> 00:53:10,200
labelled examples and we train our models

821
00:53:10,320 --> 00:53:15,010
so this is a the simple as that

822
00:53:15,020 --> 00:53:18,090
so we experiment with it in order to

823
00:53:18,300 --> 00:53:24,140
to test our method and it's an ongoing work so we haven't done months we

824
00:53:24,140 --> 00:53:27,720
use three binary problems of the twenty news groups

825
00:53:27,800 --> 00:53:29,980
we try to find three examples

826
00:53:29,990 --> 00:53:34,670
the one more difficult than the other so the first one is visible this wiki

827
00:53:34,780 --> 00:53:38,190
see versus martin rees in the southeast

828
00:53:38,200 --> 00:53:42,660
so the first is quite easy to moderate with moderate and the other hard in

829
00:53:42,660 --> 00:53:46,020
the brain is is the number of examples of the set

830
00:53:46,680 --> 00:53:48,600
one of its category

831
00:53:48,680 --> 00:53:52,580
we used a eighty percent of the training set us there

832
00:53:52,590 --> 00:53:55,050
eighty percent of the day this training set

833
00:53:55,560 --> 00:54:00,330
from wheat that we use to labelled examples as a initial sec one of class

834
00:54:00,330 --> 00:54:02,270
and all the others

835
00:54:02,350 --> 00:54:07,700
and ten percent just to test the accuracy of our system

836
00:54:07,850 --> 00:54:13,670
the comparison who have done is say our method with uses supervised learning reversibility plus

837
00:54:13,670 --> 00:54:15,030
active learning

838
00:54:15,050 --> 00:54:18,510
and we tested with a semisupervised principle is random query

839
00:54:18,520 --> 00:54:22,290
where the idea is perform the same supervised PLSA

840
00:54:22,420 --> 00:54:26,600
and then instead of being active learning with just amount of a document from the

841
00:54:26,600 --> 00:54:27,660
labelled set

842
00:54:27,710 --> 00:54:29,390
and we live in it

843
00:54:29,550 --> 00:54:34,820
and as a as a baseline we to the SVM blast active learning which is

844
00:54:34,820 --> 00:54:37,650
the example closest to the margin

845
00:54:37,750 --> 00:54:41,160
but what we mostly interesting in the comparison of the two first because it's the

846
00:54:41,160 --> 00:54:42,490
active learning part

847
00:54:42,580 --> 00:54:45,890
i want to see if it helps or not

848
00:54:45,960 --> 00:54:53,100
so this year something some results from the first experiment from the first set there

849
00:54:53,590 --> 00:54:57,560
the black line the first in the top it's our algorithm which is the same

850
00:54:57,560 --> 00:55:00,410
as revised the prospect of learning

851
00:55:00,410 --> 00:55:01,400
and then

852
00:55:01,410 --> 00:55:03,890
it is several file but

853
00:55:03,900 --> 00:55:06,380
this quantity here

854
00:55:06,470 --> 00:55:08,450
is one minus

855
00:55:08,510 --> 00:55:11,720
that quantity

856
00:55:12,570 --> 00:55:28,640
get that

857
00:55:28,720 --> 00:55:32,390
so ever classifiers are perfect

858
00:55:32,430 --> 00:55:33,380
this summer

859
00:55:36,780 --> 00:55:42,380
that's so probability plus one

860
00:55:44,160 --> 00:55:49,940
and this is the issue of maybe they're imperfect right

861
00:55:49,950 --> 00:55:55,440
but are that's it's pretty easy to analyse

862
00:55:57,750 --> 00:55:58,910
OK so

863
00:55:58,930 --> 00:56:02,750
you can take the probably analysis in contrast originates

864
00:56:02,800 --> 00:56:07,580
o thing but the bushes to the expectation because of to inequality

865
00:56:07,630 --> 00:56:11,210
and you get this statement says the distance between

866
00:56:11,240 --> 00:56:12,370
your prediction in

867
00:56:12,370 --> 00:56:13,720
any the actual

868
00:56:14,620 --> 00:56:16,220
spent about

869
00:56:16,240 --> 00:56:20,250
you are binary regret spirited

870
00:56:20,300 --> 00:56:24,170
this is essentially worthless currency come from

871
00:56:24,190 --> 00:56:27,660
so these problems

872
00:56:27,660 --> 00:56:30,720
right so that we have this this

873
00:56:34,330 --> 00:56:35,920
which is defining

874
00:56:35,940 --> 00:56:37,670
how we predict

875
00:56:37,720 --> 00:56:39,630
what's news for an adversary

876
00:56:40,280 --> 00:56:43,140
again we're going to have an adversarial analysis

877
00:56:43,140 --> 00:56:44,450
we're saying

878
00:56:44,500 --> 00:56:48,130
suppose an adversary has some but it's too

879
00:56:52,330 --> 00:56:56,460
because when and

880
00:56:59,380 --> 00:57:02,840
that should probably be that

881
00:57:02,890 --> 00:57:05,150
as the most efficient way

882
00:57:05,930 --> 00:57:08,130
for him to shift the sum

883
00:57:08,840 --> 00:57:15,950
the distance to dig and sift the sum is to control the multiclass ar

884
00:57:16,010 --> 00:57:18,640
so the most efficient way to disturb the sum

885
00:57:18,640 --> 00:57:19,690
it's actually

886
00:57:19,710 --> 00:57:23,260
at the same disturbance for every element of the cell

887
00:57:23,440 --> 00:57:25,300
and that's because of this current

888
00:57:27,500 --> 00:57:31,130
in order to get a big disturbance you have to pay

889
00:57:31,170 --> 00:57:32,980
you have to pay the square

890
00:57:33,010 --> 00:57:35,190
of the regret

891
00:57:36,060 --> 00:57:46,870
so i

892
00:57:46,880 --> 00:57:48,080
that's correct

893
00:57:50,670 --> 00:57:55,840
this says that

894
00:57:56,420 --> 00:57:59,550
the size of the disturbance

895
00:57:59,590 --> 00:58:02,580
so it's thousand disturbances like the square root

896
00:58:02,630 --> 00:58:04,370
so the amount that you pay

897
00:58:04,380 --> 00:58:06,190
so far you go out

898
00:58:06,210 --> 00:58:09,150
the size the disturbances how far you go

899
00:58:09,150 --> 00:58:12,190
so this into the diminishing returns the further you go out

900
00:58:12,250 --> 00:58:14,960
the less disturbance you actually get

901
00:58:15,900 --> 00:58:19,870
he says that going disturb something that each element of the sum

902
00:58:19,950 --> 00:58:23,550
the same amount if you're never adversary

903
00:58:25,410 --> 00:58:28,670
OK so the most efficient way

904
00:58:28,760 --> 00:58:31,430
disturbs the sum

905
00:58:31,450 --> 00:58:34,100
by epsilon times the number

906
00:58:34,100 --> 00:58:37,100
binary classifiers

907
00:58:37,120 --> 00:58:41,130
is to disturb each individual elements that

908
00:58:48,980 --> 00:58:53,470
yes i was this problem but how you define convex to concave

909
00:58:53,510 --> 00:58:55,860
and so this is

910
00:58:55,860 --> 00:58:59,980
it's kind of this context but it can be concave in other contexts

911
00:59:00,490 --> 00:59:04,240
i guess i think of convex

912
00:59:07,340 --> 00:59:10,840
you can say things that

913
00:59:11,190 --> 00:59:13,820
all lines go through the region that

914
00:59:15,240 --> 00:59:18,360
yet there may be a parody issue

915
00:59:18,430 --> 00:59:20,690
is so the appropriate thing that's the graph

916
00:59:20,740 --> 00:59:22,780
the graph is clear

917
00:59:24,330 --> 00:59:27,870
OK so

918
00:59:27,880 --> 00:59:29,480
what this says is that

919
00:59:29,500 --> 00:59:33,440
if we change the l one sometimes by beta and so on

920
00:59:34,110 --> 00:59:38,750
we know choosing classes is no more than

921
00:59:38,810 --> 00:59:40,240
four epsilon

922
00:59:40,320 --> 00:59:43,170
worse than the minimum as far as the area

923
00:59:43,230 --> 00:59:46,270
how do we see that all case remember this

924
00:59:46,310 --> 00:59:48,050
beta and that's the one

925
00:59:48,060 --> 00:59:50,900
and then let's go back a page

926
00:59:50,910 --> 00:59:52,010
as i did this

927
00:59:52,030 --> 00:59:54,670
so if we had

928
00:59:54,730 --> 00:59:57,890
we have the upside on here

929
00:59:57,900 --> 01:00:00,670
then this factor of two means that we can

930
01:00:00,690 --> 01:00:03,380
overestimate probability

931
01:00:04,910 --> 01:00:07,470
two times as long

932
01:00:07,590 --> 01:00:09,440
so a b or two

933
01:00:09,460 --> 01:00:13,780
this is a plus b or b ten ships on which is close to tend

934
01:00:13,890 --> 01:00:16,240
to to be that's over two

935
01:00:17,180 --> 01:00:18,410
i think of it is

936
01:00:18,410 --> 01:00:22,320
in reality is a hard problem but this one looks relatively easy to ask for

937
01:00:22,320 --> 01:00:24,950
the past so that you can you can debate about that

938
01:00:25,590 --> 01:00:30,130
the this one looks is just because we've been trained on this kind of data for a long long time

939
01:00:31,330 --> 01:00:32,000
during our life

940
01:00:34,250 --> 01:00:35,210
this kind of thing chunks

941
01:00:35,850 --> 01:00:38,270
different looks looks hard frost were not used this data

942
01:00:39,140 --> 01:00:42,760
and which maybe makes it more interesting already makes it apparent that it's a hard

943
01:00:42,760 --> 01:00:44,920
problem and of course a lot of the problems that we are

944
01:00:45,560 --> 01:00:47,450
dealing with differences in bioinformatics

945
01:00:48,160 --> 01:00:49,210
is this type of that

946
01:00:50,110 --> 01:00:51,740
we have seen that kind of data flow

947
01:00:54,850 --> 01:00:58,260
so this is the brain has learned to solve problems

948
01:00:58,940 --> 01:01:00,460
we can see that this policy to us

949
01:01:01,110 --> 01:01:03,280
and this a famous neuroscientist horace barlow

950
01:01:03,910 --> 01:01:06,940
once said that brings nothing but statistical decision organ

951
01:01:09,080 --> 01:01:10,600
we have different organs in our body

952
01:01:11,290 --> 01:01:12,510
so the pump blood

953
01:01:13,110 --> 01:01:14,810
so them clean the blood and this one

954
01:01:15,530 --> 01:01:21,930
it's build four statistical decisions that's the brain so if you're we're studying empirical inference and learning since we also

955
01:01:22,650 --> 01:01:24,100
doing theoretical brain research

956
01:01:25,440 --> 01:01:29,120
not at the level of looking at neurons but at the level of tasks that the brain cells

957
01:01:30,350 --> 01:01:33,780
so now it is shown you to inference problems one look easy

958
01:01:35,620 --> 01:01:40,020
two dimensional problem where should directly see what's going on where we probably see in

959
01:01:40,020 --> 01:01:42,180
which case we can generalize which is we can't

960
01:01:42,770 --> 01:01:43,910
you want to task

961
01:01:45,940 --> 01:01:46,450
in the

962
01:01:47,460 --> 01:01:50,140
let's look at another hard inference problem or if you look at some

963
01:01:50,650 --> 01:01:53,430
if what are general properties of hard inference problem

964
01:01:54,270 --> 01:01:55,930
so this is a problem from bioinformatics

965
01:01:57,730 --> 01:02:01,810
the classifier that this is just a component of a larger system to find genes

966
01:02:02,610 --> 01:02:07,580
and this is the classifier that classifies d-day sequence locations i forgot to ask the

967
01:02:07,640 --> 01:02:10,560
other computational biologists in the audience also

968
01:02:11,850 --> 01:02:12,510
okay so

969
01:02:13,520 --> 01:02:13,860
i miss

970
01:02:14,300 --> 01:02:17,040
eight significant group before it my question

971
01:02:20,210 --> 01:02:23,260
classifying human any human being sequence locations

972
01:02:23,990 --> 01:02:24,510
in two three

973
01:02:25,040 --> 01:02:28,410
classes and it doesn't matter if you don't know about these topics

974
01:02:29,130 --> 01:02:32,770
it's just a three-class classification problem which is highly

975
01:02:34,590 --> 01:02:36,140
so the class that have the same size

976
01:02:36,970 --> 01:02:42,020
in the input data x these pieces of these sequence of a certain things

977
01:02:42,780 --> 01:02:43,740
window around the

978
01:02:44,340 --> 01:02:45,420
place that you want to classify

979
01:02:46,180 --> 01:02:47,520
this is a hundred forty one

980
01:02:48,290 --> 01:02:49,970
we have fifteen million training points

981
01:02:50,630 --> 01:02:51,560
and type of

982
01:02:52,280 --> 01:02:53,370
machine learning methods

983
01:02:54,130 --> 01:02:55,560
this is a certain

984
01:02:56,270 --> 01:02:59,360
type performance criteria because the precision recall curve

985
01:03:00,310 --> 01:03:01,450
it doesn't matter

986
01:03:02,080 --> 01:03:04,350
so what the details of the point that i want to make your own is

987
01:03:05,200 --> 01:03:09,950
but here to increase the number of training examples gets better so initially four small

988
01:03:10,070 --> 01:03:12,470
training sets you are more there's a chance level

989
01:03:13,020 --> 01:03:14,710
so the data set looks random to

990
01:03:16,650 --> 01:03:19,330
but as we increase the number of training points quite a bit

991
01:03:19,910 --> 01:03:21,480
here we are ten million

992
01:03:22,370 --> 01:03:23,650
we can do a very good job

993
01:03:24,050 --> 01:03:25,820
it's solving this classification problem

994
01:03:27,450 --> 01:03:30,250
maybe this looks like a trivial point but i think it's important to

995
01:03:31,430 --> 01:03:32,180
keep this in mind

996
01:03:32,890 --> 01:03:34,060
so when we think about machine learning

997
01:03:34,700 --> 01:03:39,370
and if we were human looking at this data set we would always be the regime somewhere

998
01:03:40,090 --> 01:03:42,160
down here we would look a small datasets

999
01:03:43,830 --> 01:03:45,690
we would maybe think that this is

1000
01:03:46,570 --> 01:03:48,120
completely random there's no structure

1001
01:03:48,940 --> 01:03:54,710
but if we have advanced machine learning methods that can handle large datasets and find their relationships

1002
01:03:55,850 --> 01:03:56,780
we can be over here

1003
01:03:57,300 --> 01:03:59,060
and we can certainly see a lot of structure

1004
01:03:59,560 --> 01:04:04,500
so all the stuff that maybe that might appear human random to us humans

1005
01:04:05,340 --> 01:04:08,970
it may not be so random and we might be able to find structure in machine learning

1006
01:04:09,800 --> 01:04:11,480
and i think that's only gradually

1007
01:04:11,940 --> 01:04:12,770
being acknowledged

1008
01:04:13,440 --> 01:04:18,100
this can be a real paradigm shift in how we interpret data in the world

1009
01:04:18,740 --> 01:04:21,870
nothing machine learning is going to play an important role in the cell

1010
01:04:23,650 --> 01:04:24,520
study the right topic

1011
01:04:26,750 --> 01:04:27,550
so what's there

1012
01:04:28,020 --> 01:04:29,280
characteristics such problems

1013
01:04:30,480 --> 01:04:31,150
and typically out

1014
01:04:33,010 --> 01:04:35,140
so you have to look at many simultaneously

1015
01:04:35,660 --> 01:04:35,970
they are

1016
01:04:36,700 --> 01:04:37,840
complex in various ways

1017
01:04:38,950 --> 01:04:39,340
we have

1018
01:04:40,480 --> 01:04:41,410
little prior knowledge

1019
01:04:42,790 --> 01:04:47,300
by which i mean the difference we don't have a full mechanistic model politicized we had that

1020
01:04:47,860 --> 01:04:50,230
what scientists had that and probably they wouldn't

1021
01:04:50,870 --> 01:04:54,790
but with the asking us for help they wouldn't bother with applying machine learning

1022
01:04:56,020 --> 01:04:56,480
and the

1023
01:04:56,920 --> 01:04:58,890
because as a consequence of these points

1024
01:04:59,480 --> 01:05:01,210
typically we need large datasets

1025
01:05:02,780 --> 01:05:05,030
and this kind of course only be processed if we have

1026
01:05:05,630 --> 01:05:06,780
computers which

1027
01:05:07,260 --> 01:05:09,670
was because an enabling technology for machine learning

1028
01:05:10,750 --> 01:05:13,040
the increase in computing power over the last decades

1029
01:05:13,750 --> 01:05:16,650
and if we have advanced inference methods

1030
01:05:20,710 --> 01:05:22,410
and yes i really made this point

1031
01:05:24,050 --> 01:05:26,500
the first one we can solve problems that humans can solve

1032
01:05:27,530 --> 01:05:27,840
and the

1033
01:05:29,080 --> 01:05:34,640
even though this is probably just because the datasets are even if it's just a data set size dimensionality

1034
01:05:35,190 --> 01:05:36,470
it's a major step

1035
01:05:38,880 --> 01:05:41,540
okay so i mean what is the main issue in machine learning

1036
01:05:43,030 --> 01:05:44,540
maybe main issues generalization

1037
01:05:45,780 --> 01:05:50,380
and in the end just to give you an idea what generalization about is about was problem

1038
01:05:51,220 --> 01:05:53,860
the suppose we see this number sequence

1039
01:05:54,520 --> 01:05:55,880
one two four seven

1040
01:05:56,810 --> 01:05:59,300
ask question what's the next number in the sequence

1041
01:06:01,560 --> 01:06:02,230
any guesses

1042
01:06:06,130 --> 01:06:06,520
many others

1043
01:06:07,920 --> 01:06:08,790
twelve okay

1044
01:06:09,430 --> 01:06:10,480
so alone is this

1045
01:06:10,650 --> 01:06:10,990
the first

1046
01:06:11,480 --> 01:06:12,170
good solution

1047
01:06:13,360 --> 01:06:14,890
it's the simple sequence

1048
01:06:15,510 --> 01:06:17,860
it's called the lazy caterer's sequence because it's the

1049
01:06:18,520 --> 01:06:22,670
maximum number of pieces into which you can cut a piece of cake with end caps

1050
01:06:23,740 --> 01:06:24,990
you just have to make sure that every

1051
01:06:25,730 --> 01:06:31,470
additional cut intersects all previous cuts in that case being cut gives you an additional pieces

1052
01:06:32,260 --> 01:06:34,350
and the same size but there are pieces

1053
01:06:34,980 --> 01:06:36,780
and so this is a simple sequence

1054
01:06:37,530 --> 01:06:38,420
twelve was also good

1055
01:06:41,080 --> 01:06:41,580
is this one

1056
01:06:42,320 --> 01:06:46,270
thirteen is the so-called triple matches sequence each numbers the some of the previous three

1057
01:06:47,480 --> 01:06:50,400
sixteen we're going to ask me in front explanation later on

1058
01:06:52,190 --> 01:06:53,100
said sixteen injured

1059
01:06:57,640 --> 01:06:59,720
and the well this one is nice one

1060
01:07:00,500 --> 01:07:02,330
this is a sequence that ends twenty eight

1061
01:07:03,040 --> 01:07:04,730
this is the set of divisors twenty eight

1062
01:07:07,620 --> 01:07:08,200
this is the

1063
01:07:08,730 --> 01:07:13,720
when the whisky came up with this that there is no expressions of pi and e interleaved

1064
01:07:14,550 --> 01:07:15,030
which is a

1065
01:07:15,490 --> 01:07:18,040
also a nice sequence a mathematician

1066
01:07:18,040 --> 01:07:24,240
so if you if you look at that and try to to perform the

1067
01:07:24,270 --> 01:07:26,570
it's this implementation we

1068
01:07:26,590 --> 01:07:33,530
cost you complexity of explosion exponentially in the length of the

1069
01:07:33,550 --> 01:07:35,500
of the sequence

1070
01:07:35,530 --> 01:07:38,230
it's too much

1071
01:07:39,490 --> 01:07:45,240
let's not is that the only factor in the joint distribution

1072
01:07:45,270 --> 01:07:51,110
the joint distribution of of the sequence that is really them depending on the

1073
01:07:51,120 --> 01:07:52,320
on the last

1074
01:07:52,800 --> 01:07:56,760
the last state to the last element in sequence

1075
01:07:56,790 --> 01:07:59,550
is the estimates in the sequence

1076
01:07:59,590 --> 01:08:01,920
not anyone this one doesn't depend

1077
01:08:02,080 --> 01:08:03,770
on qn

1078
01:08:05,220 --> 01:08:10,300
in fact as as as a show before in in the little example

1079
01:08:10,350 --> 01:08:14,170
and i can be factorized

1080
01:08:14,190 --> 01:08:15,640
all the rest of

1081
01:08:16,660 --> 01:08:18,070
of the sequence

1082
01:08:18,080 --> 01:08:19,890
all the other probabilities

1083
01:08:19,900 --> 01:08:20,920
remember that

1084
01:08:20,940 --> 01:08:23,270
joint probability is this product

1085
01:08:23,280 --> 01:08:25,700
so i take on the last element

1086
01:08:25,720 --> 01:08:28,030
and factorize all the rest

1087
01:08:28,070 --> 01:08:33,380
because it's not going to be some of our over two in

1088
01:08:35,270 --> 01:08:38,170
i'm simply applying

1089
01:08:40,470 --> 01:08:43,600
the stability of multiplication over addition

1090
01:08:43,630 --> 01:08:44,790
OK so

1091
01:08:44,820 --> 01:08:46,240
i'm just doing

1092
01:08:47,550 --> 01:08:51,550
let's proceed with this marginalization

1093
01:08:51,560 --> 01:08:53,710
if a sum over

1094
01:08:53,720 --> 01:08:56,020
sure enough since i have here

1095
01:08:56,190 --> 01:08:57,750
q n minus one

1096
01:08:57,770 --> 01:09:01,210
this will be a function of

1097
01:09:01,240 --> 01:09:02,060
of the

1098
01:09:02,760 --> 01:09:07,210
state in the sequence the human is one element in the sequence

1099
01:09:09,880 --> 01:09:13,250
if a sum over two minus one

1100
01:09:14,410 --> 01:09:17,150
again so first

1101
01:09:17,170 --> 01:09:19,210
this again can factorize

1102
01:09:19,310 --> 01:09:21,580
as they did previously

1103
01:09:21,590 --> 01:09:22,890
using the

1104
01:09:22,900 --> 01:09:28,720
the sequence until minus one and then the property of minus one so again

1105
01:09:28,780 --> 01:09:31,420
i can factorize

1106
01:09:31,420 --> 01:09:32,780
out of the sum

1107
01:09:34,970 --> 01:09:36,740
if a sum over

1108
01:09:36,780 --> 01:09:39,830
over that it will be a function of n minus two

1109
01:09:39,850 --> 01:09:41,670
if some over the

1110
01:09:41,830 --> 01:09:44,190
and minus one element in in the sequence

1111
01:09:44,220 --> 01:09:46,650
and so on

1112
01:09:47,630 --> 01:09:49,850
it's a c

1113
01:09:49,930 --> 01:09:51,710
t plus one

1114
01:09:52,320 --> 01:09:53,630
so i have now

1115
01:09:53,670 --> 01:09:57,530
the probability of my sequence from the first element to the

1116
01:09:57,540 --> 01:09:58,870
take two

1117
01:09:58,880 --> 01:10:00,920
t plus one element

1118
01:10:02,670 --> 01:10:07,180
but i have collected a better function which depend

1119
01:10:07,200 --> 01:10:09,250
ieong show

1120
01:10:09,270 --> 01:10:11,150
t plus one

1121
01:10:12,990 --> 01:10:16,150
if a sum over t plus one

1122
01:10:16,190 --> 01:10:17,150
we have

1123
01:10:17,620 --> 01:10:19,270
so here

1124
01:10:19,290 --> 01:10:21,690
this decomposes into

1125
01:10:21,710 --> 01:10:24,170
the sequence until t

1126
01:10:24,170 --> 01:10:28,030
and the probability of seeing given

1127
01:10:28,060 --> 01:10:32,740
of t plus one given t so if a sum over t plus one

1128
01:10:32,780 --> 01:10:34,510
and we have

1129
01:10:34,530 --> 01:10:36,170
the function of

1130
01:10:39,920 --> 01:10:44,490
OK so now i have to stop because the

1131
01:10:44,510 --> 01:10:47,670
because i can and some of it

1132
01:10:50,690 --> 01:10:54,320
i can begin from the other side of the of the sequence

1133
01:10:54,340 --> 01:10:58,020
i can do the same thing because i can factorize this time for from the

1134
01:10:58,020 --> 01:11:02,420
other side take one one in the join in the product of the joint distribution

1135
01:11:02,430 --> 01:11:05,040
take the first term instead of taking the last one

1136
01:11:09,510 --> 01:11:12,200
this again broke into

1137
01:11:12,290 --> 01:11:20,700
i can break into the sediments in the cell the prior probability times the probability

1138
01:11:21,310 --> 01:11:22,570
she and

1139
01:11:22,580 --> 01:11:24,790
now after two given to us

1140
01:11:24,810 --> 01:11:27,920
and this

1141
01:11:29,130 --> 01:11:30,880
if a sum

1142
01:11:30,900 --> 01:11:35,530
first of all could one it will be a functionof of q two and then

1143
01:11:35,550 --> 01:11:37,430
doing that repeatedly

1144
01:11:37,440 --> 01:11:38,420
we have

1145
01:11:38,430 --> 01:11:41,140
in the end

1146
01:11:41,220 --> 01:11:42,770
OK so the

1147
01:11:44,940 --> 01:11:50,240
almost the last step is the ice over two t plus one

1148
01:11:50,290 --> 01:11:56,310
and i have to have fit finish taking nine or the products mainly during probity

1149
01:11:56,310 --> 01:11:58,690
and i only have this last

1150
01:12:00,550 --> 01:12:04,850
have collected a function f phi of minus one

1151
01:12:04,870 --> 01:12:07,280
if a sum over t minus one

1152
01:12:07,290 --> 01:12:08,530
i have

1153
01:12:08,570 --> 01:12:11,670
this alpha qt

1154
01:12:13,050 --> 01:12:14,620
in the end i

1155
01:12:14,640 --> 01:12:17,280
the in the beginning i had this big

1156
01:12:18,950 --> 01:12:20,800
some asian problem

1157
01:12:20,880 --> 01:12:23,840
and then they just have these two products

1158
01:12:23,850 --> 01:12:31,940
we use it of course that i find beta that are computed recursively

1159
01:12:31,950 --> 01:12:35,700
and the complexity this time is the

1160
01:12:35,720 --> 01:12:40,420
it's a square of of of the links of my

1161
01:12:41,380 --> 01:12:42,500
my sequence

1162
01:12:43,210 --> 01:12:47,780
square of number of states which is much smaller than the

1163
01:12:47,870 --> 01:12:50,640
the naive approach

1164
01:12:50,950 --> 01:12:56,220
so having that

1165
01:12:56,290 --> 01:12:58,520
we can compute a lot of things

1166
01:12:58,540 --> 01:13:00,530
this too so if

1167
01:13:00,550 --> 01:13:02,740
it's easy to compute them once

1168
01:13:02,760 --> 01:13:03,850
and stop them

1169
01:13:03,880 --> 01:13:05,380
then did the

1170
01:13:05,390 --> 01:13:11,180
no you two to compute a lot of things in this in this graphical model

1171
01:13:11,190 --> 01:13:19,170
for example we see that we can compute the temperature of two consecutive state of

1172
01:13:19,170 --> 01:13:24,200
two consecutive sorry elements in a sequence

1173
01:13:24,210 --> 01:13:25,290
so you would be

1174
01:13:26,670 --> 01:13:31,000
as i was going through the chain and stopping

1175
01:13:31,580 --> 01:13:35,070
at the first amendment

1176
01:13:35,090 --> 01:13:42,170
something for the first moment from the other side stopping the second element

1177
01:13:45,000 --> 01:13:48,060
but that's that's an idea to two

1178
01:13:48,890 --> 01:13:52,470
so it's the easter fairly simple simple example

1179
01:13:52,490 --> 01:13:55,200
but it so those of you who know all

1180
01:13:55,220 --> 01:14:01,740
he then that given markov mothers you know that there is forward backward

1181
01:14:01,760 --> 01:14:12,560
i was algorithm who works like that just makes more difficult because the qtrs observed

1182
01:14:12,850 --> 01:14:15,710
so you have there are latent variables

1183
01:14:15,720 --> 01:14:18,250
so you will not do

1184
01:14:18,270 --> 01:14:25,310
directed inference like that you will have to find info about words using their expectation

1185
01:14:27,400 --> 01:14:28,660
where are you

1186
01:14:28,670 --> 01:14:33,480
you have this latent you estimate them then you used them to maxima lies and

1187
01:14:33,750 --> 01:14:36,890
in something like that in fact in the chain

1188
01:14:39,000 --> 01:14:40,680
hmm are

1189
01:14:40,700 --> 01:14:43,660
our journey generalisation of that

1190
01:14:45,500 --> 01:14:47,760
in fact what we need here is to be

1191
01:14:47,790 --> 01:14:49,340
in independent

1192
01:14:51,960 --> 01:14:56,600
conditionally independent have group of separated

1193
01:14:56,620 --> 01:14:58,370
separated the

1194
01:14:58,370 --> 01:14:59,600
the correct

1195
01:14:59,850 --> 01:15:01,980
three y one

1196
01:15:01,980 --> 01:15:06,730
i would have a high discriminant value then every incorrect three one

1197
01:15:06,780 --> 01:15:08,530
all right

1198
01:15:08,590 --> 01:15:12,330
i have a set of constraints like that for every training

1199
01:15:12,380 --> 01:15:16,740
and then i regularized and again all

1200
01:15:16,830 --> 01:15:19,010
if that problem has a solution

1201
01:15:19,020 --> 01:15:20,870
then i know i have

1202
01:15:20,890 --> 01:15:25,500
the weight vector that gives me zero training

1203
01:15:25,560 --> 01:15:26,820
probably not

1204
01:15:26,830 --> 01:15:30,000
possible in practice so we have to introduce a loss function

1205
01:15:31,460 --> 01:15:33,240
for example if i can

1206
01:15:34,520 --> 01:15:36,840
this one i can actually get

1207
01:15:36,900 --> 01:15:38,960
then i would want to pay a penalty

1208
01:15:39,010 --> 01:15:41,300
a large proportion of

1209
01:15:44,460 --> 01:15:45,880
and that's all

1210
01:15:45,930 --> 01:15:49,190
how bad this

1211
01:15:49,380 --> 01:15:51,420
new prediction

1212
01:15:51,480 --> 01:15:54,030
and there are multiple ways of you can see how we can achieve that in

1213
01:15:54,030 --> 01:15:55,370
the linear model

1214
01:15:56,520 --> 01:16:00,710
one called slack rescaling one margin scaling margin rescaling simply

1215
01:16:00,730 --> 01:16:03,990
he'll the distance that you want to have in the

1216
01:16:04,040 --> 01:16:07,240
correct one and each incorrect one

1217
01:16:07,280 --> 01:16:11,120
the love you have incorrect one over ten

1218
01:16:11,210 --> 01:16:17,710
we have a slack variable that takes the value that's that followed by the

1219
01:16:17,760 --> 01:16:23,100
so in terms of an optimisation problem which is likely to replace the mach one

1220
01:16:23,180 --> 01:16:26,350
with the loss function just trying to value

1221
01:16:26,360 --> 01:16:28,900
and then the slack variables here

1222
01:16:29,080 --> 01:16:32,550
into the objective again just like in regular SVM

1223
01:16:32,590 --> 01:16:34,340
and again it's easy to see

1224
01:16:35,840 --> 01:16:38,840
some of the slack variables gives you an upper bound on the training

1225
01:16:38,870 --> 01:16:42,520
just like for regular SVM is one

1226
01:16:42,530 --> 01:16:45,400
so by solving this optimisation problem

1227
01:16:46,660 --> 01:16:51,130
optimized train

1228
01:16:52,890 --> 01:16:54,200
so let's try that

1229
01:16:54,210 --> 01:16:56,130
actually for natural language parsing

1230
01:16:56,460 --> 01:17:00,860
what i've done here it is

1231
01:17:00,870 --> 01:17:02,490
they can depend treebank

1232
01:17:02,490 --> 01:17:04,860
that's the standard

1233
01:17:07,320 --> 01:17:11,480
use only short sentences to be more efficient one

1234
01:17:12,150 --> 01:17:14,710
efficiency is are actually

1235
01:17:14,760 --> 01:17:17,530
so i'm using mark johnson part

1236
01:17:17,540 --> 01:17:19,100
and a trained

1237
01:17:22,270 --> 01:17:27,290
probabilistic context free grammars in the generative and i just think about

1238
01:17:27,340 --> 01:17:29,120
in the same grammar

1239
01:17:29,180 --> 01:17:31,600
using the SVM approach

1240
01:17:31,650 --> 01:17:36,260
when using this f one for the loss that the typical kind of war that

1241
01:17:36,260 --> 01:17:38,230
people use to evaluate

1242
01:17:38,270 --> 01:17:41,100
quality of parts

1243
01:17:41,120 --> 01:17:42,800
and again just like four

1244
01:17:42,800 --> 01:17:44,460
text classification

1245
01:17:44,510 --> 01:17:46,820
we are actually doing exactly the same model

1246
01:17:46,830 --> 01:17:47,990
it is the

1247
01:17:48,040 --> 01:17:52,250
that's exactly the same parameters but once trained in action

1248
01:17:52,260 --> 01:17:55,370
kind of the naive bayes equivalent of grammar

1249
01:17:55,430 --> 01:17:57,890
and once the train is indiscriminate fashion

1250
01:17:57,910 --> 01:18:01,670
and what you see is both that the error rate the the number of correct

1251
01:18:03,010 --> 01:18:05,050
the accuracy of the network

1252
01:18:05,060 --> 01:18:06,550
he goes

1253
01:18:06,560 --> 01:18:08,980
and you want to work well

1254
01:18:09,080 --> 01:18:14,290
not as much as the text classification but is using

1255
01:18:15,840 --> 01:18:17,630
ask and others have

1256
01:18:17,760 --> 01:18:20,890
done therefore much more complicated creatures and i think

1257
01:18:21,390 --> 01:18:24,100
using essentially the same approach

1258
01:18:25,400 --> 01:18:28,310
what they were managed to get essentially a state of the art

1259
01:18:34,770 --> 01:18:37,360
let me wrap up with this general framework

1260
01:18:41,600 --> 01:18:44,020
i wanted to apply

1261
01:18:44,020 --> 01:18:49,310
this method to this general to two in the problem after all

1262
01:18:49,400 --> 01:18:53,150
yes if i think to specify loss function

1263
01:18:53,200 --> 01:18:57,140
so how bad is it to predict a particular label

1264
01:18:57,360 --> 01:18:59,800
number one is correct

1265
01:18:59,840 --> 01:19:01,910
i have to come up with the representation

1266
01:19:01,930 --> 01:19:07,410
and i can often build on generative models conditional random field

1267
01:19:08,830 --> 01:19:14,350
then to do prediction i solve this discriminant value discriminant problem over

1268
01:19:14,370 --> 01:19:17,580
this linear model

1269
01:19:17,580 --> 01:19:22,210
and for training i have to solve this product

1270
01:19:22,270 --> 01:19:25,300
and what have also in the rest of the talk is actually

1271
01:19:25,340 --> 01:19:31,020
quite a lot of structure prediction problems fit into this

1272
01:19:31,030 --> 01:19:36,740
so one thing that you may wonder was glossed over

1273
01:19:36,790 --> 01:19:38,600
small details right

1274
01:19:38,870 --> 01:19:42,940
actually solve the problem

1275
01:19:44,120 --> 01:19:45,050
if you look at the

1276
01:19:45,060 --> 01:19:47,730
so if the convex quadratic programs

1277
01:19:49,690 --> 01:19:52,550
it's a bit

1278
01:19:53,450 --> 01:19:55,480
what kind of

1279
01:19:55,510 --> 01:19:57,830
in the the notation here is

1280
01:19:57,840 --> 01:20:00,600
that i have a lot of constraints

1281
01:20:00,650 --> 01:20:01,710
in particular

1282
01:20:02,160 --> 01:20:05,490
in this for annotation here is

1283
01:20:05,650 --> 01:20:07,750
the constraint that

1284
01:20:07,760 --> 01:20:10,380
the correct course three has to have

1285
01:20:10,410 --> 01:20:12,730
high value any incorrect parts

1286
01:20:12,770 --> 01:20:15,280
the constraint for every

1287
01:20:15,300 --> 01:20:17,960
incorrect car possible

1288
01:20:18,060 --> 01:20:20,980
that's an exponential number of constraints

1289
01:20:21,030 --> 01:20:23,240
i wanted to find that

1290
01:20:23,740 --> 01:20:25,780
it's you

1291
01:20:25,820 --> 01:20:31,400
so typing this into map that is not possible even for small toy problems

1292
01:20:31,460 --> 01:20:37,200
so how do actually solve these programs is that actually tractable

1293
01:20:37,250 --> 01:20:38,600
so that

1294
01:20:38,610 --> 01:20:42,980
brings up the question of how well actually training was not just the formulation of

1295
01:20:42,980 --> 01:20:46,060
the problem but the training

1296
01:20:46,060 --> 01:20:47,950
so let's think about this

1297
01:20:48,040 --> 01:20:53,320
so here's the optimisation problem again

1298
01:20:53,330 --> 01:20:58,150
and despite the fact that i have a lot of constraints here

1299
01:21:02,360 --> 01:21:03,520
if you think about it

1300
01:21:03,520 --> 01:21:06,180
then the solution that i'm looking for

1301
01:21:06,200 --> 01:21:08,260
just lies in the corner

1302
01:21:14,600 --> 01:21:18,490
this is depiction where the shading is the objective value

1303
01:21:18,500 --> 01:21:20,610
and the lines of constraints

1304
01:21:21,520 --> 01:21:23,190
really solution

1305
01:21:23,340 --> 01:21:27,850
probably going to be only a few constraints that actually act

1306
01:21:27,910 --> 01:21:30,240
these other great of constraints here

1307
01:21:30,270 --> 01:21:35,530
well there in the problem but they don't actually i can just leave

1308
01:21:35,550 --> 01:21:37,230
and what they were able to show

1309
01:21:37,320 --> 01:21:41,450
initially was the fact you can

1310
01:21:41,460 --> 01:21:44,340
you only are interested in solution

1311
01:21:46,660 --> 01:21:48,680
correct up to some epsilon which

1312
01:21:48,690 --> 01:21:50,340
the arbitrary

1313
01:21:50,390 --> 01:21:55,930
then the number of constraints that you have to consider that after the solution is

1314
01:21:56,110 --> 01:21:57,950
polynomially bounded

1315
01:21:58,000 --> 01:21:59,780
in that

1316
01:21:59,830 --> 01:22:03,890
but despite the fact that you have an exponential number of constraints the number actually

1317
01:22:03,890 --> 01:22:07,130
need is small

1318
01:22:07,150 --> 01:22:09,820
white's polynomial

1319
01:22:11,180 --> 01:22:12,540
polynomial but it

1320
01:22:12,540 --> 01:22:16,210
i like to think that the k i squared distribution is a gamma because that's

1321
01:22:16,220 --> 01:22:19,960
where i don't have to remember lots of get different distributions but it's a gamma

1322
01:22:19,960 --> 01:22:24,990
distribution and the way i parameterize gamma which is this form here is this density

1323
01:22:25,690 --> 01:22:27,230
that's the normalizer the

1324
01:22:27,480 --> 01:22:31,130
many important part of actually i minus one e to the minus be x

1325
01:22:31,600 --> 01:22:33,750
that's the form of the distribution and x

1326
01:22:35,100 --> 01:22:38,870
in this form the mean of this distribution is over be sometimes people said it

1327
01:22:38,890 --> 01:22:42,440
so that the bees underneath x and the main is eight times be but this

1328
01:22:42,440 --> 01:22:43,480
is the way i like to do it

1329
01:22:44,150 --> 01:22:47,400
writing in the case where it is shown that it scale by sigma squared

1330
01:22:47,810 --> 01:22:50,170
yeah so this is actually a scale parameter

1331
01:22:50,690 --> 01:22:55,690
armies sigma squared appears underneath it yeah so you've got a gamma distribution

1332
01:22:56,650 --> 01:23:02,590
the square this value here is distributed as a gamma with any parameter the shape parameters at a half

1333
01:23:03,040 --> 01:23:05,210
and so what i think it was the rate parameter

1334
01:23:05,630 --> 01:23:07,420
sector one over two sigma squared

1335
01:23:08,810 --> 01:23:14,060
so the nice thing about this is what you're interested in is the distance from the actual origin all the

1336
01:23:15,280 --> 01:23:15,710
a sample

1337
01:23:16,980 --> 01:23:18,470
you increase the dimensions yeah

1338
01:23:19,060 --> 01:23:23,110
and what it turns out is the gamma rather nicely if we want to gamma

1339
01:23:23,110 --> 01:23:26,600
variables hear and ask what's inside the square root hair

1340
01:23:27,250 --> 01:23:28,780
that's also gamma distributed

1341
01:23:29,240 --> 01:23:31,580
as long as the scales these two gammas the same

1342
01:23:32,070 --> 01:23:37,560
which they are resuming the same variance is gamma distributed with shape parameter

1343
01:23:37,970 --> 01:23:41,310
which is the sum of the shape parameters these guys so in this case is one

1344
01:23:42,090 --> 01:23:45,030
and the scale rate parameter which is the same as before

1345
01:23:45,780 --> 01:23:50,200
that's it these guys are independent which i'm assuming they are so it that's important

1346
01:23:50,240 --> 01:23:55,160
we're assuming with sampling each about features about data in this calcium egg is being

1347
01:23:55,210 --> 01:23:58,270
independently sampled from gauss in distribution which is spherical

1348
01:23:59,730 --> 01:24:03,830
so we can compute this cumulative density function and indeed we can do it

1349
01:24:05,720 --> 01:24:06,690
on peak

1350
01:24:07,090 --> 01:24:07,900
dimensional data

1351
01:24:09,150 --> 01:24:13,130
so people dimensional data the sum over these why i squared

1352
01:24:13,630 --> 01:24:17,240
these like why i squared so this is the i th data points in each

1353
01:24:17,240 --> 01:24:21,550
dimension and it is distributed as a gamma with shape parameter p e over to

1354
01:24:22,580 --> 01:24:24,850
now that means that the expected value

1355
01:24:25,270 --> 01:24:28,640
on the this distribution is pete times sigma squared

1356
01:24:29,070 --> 01:24:33,510
so what we typically do is will ask about one over pete times expected value

1357
01:24:33,530 --> 01:24:38,380
so that we keep the expected value of the squared distances and peter mentions now

1358
01:24:38,700 --> 01:24:39,490
from the origin

1359
01:24:40,830 --> 01:24:44,600
to be sigma squared so that means that in the case that the standard normal

1360
01:24:44,820 --> 01:24:47,950
it's gonna be one yeah so we'll be working with standard normals

1361
01:24:48,850 --> 01:24:52,030
so the scaling and that's scales the rate parameter on

1362
01:24:52,680 --> 01:24:53,990
ants we've got now

1363
01:24:54,430 --> 01:24:59,110
i density formula that says what will that distance the squared distance how is the

1364
01:24:59,110 --> 01:25:04,660
squared distance from the mean distributed now that's nice because in our galaxy egg model

1365
01:25:04,920 --> 01:25:06,780
we can now ask the question how

1366
01:25:07,300 --> 01:25:13,080
that's square distance from the mean vary with the number of dimensions you see this density is a function p

1367
01:25:13,670 --> 01:25:15,220
the mean distance from the

1368
01:25:16,390 --> 01:25:17,670
so the mean distance from the origin

1369
01:25:19,330 --> 01:25:20,730
which is the mean in this case

1370
01:25:22,900 --> 01:25:27,050
that's the entire volume of the egg and you start out with a yoke which

1371
01:25:27,050 --> 01:25:28,930
is sixty five percent of the egg

1372
01:25:29,740 --> 01:25:35,210
hands a white which was i think twenty eight percent not initially the volume of the white portion goes up

1373
01:25:35,970 --> 01:25:37,650
but then actually it starts to drop down

1374
01:25:39,640 --> 01:25:44,580
i mean you see the yolk drops down as well until when you get two thousand dimensions

1375
01:25:45,340 --> 01:25:47,600
with what defined to be the aluminium

1376
01:25:48,270 --> 01:25:49,870
you basically have only

1377
01:25:51,350 --> 01:25:55,840
see points which are back distance from the mean all your data sets

1378
01:25:56,410 --> 01:25:58,160
and at shell design makes sense

1379
01:25:59,720 --> 01:26:04,190
so that's very counterintuitive but is what you think if you look at the calcium

1380
01:26:04,740 --> 01:26:07,760
because you can see the calcium the data is close to the mean right

1381
01:26:08,290 --> 01:26:10,660
you know the galaxy and the data is close to the mean

1382
01:26:11,170 --> 01:26:12,230
it's very obvious right

1383
01:26:13,100 --> 01:26:17,040
now there's a fun factor every model we assume is a calcium

1384
01:26:17,880 --> 01:26:21,620
so this is a two-dimensional joint calcium with variance one

1385
01:26:23,930 --> 01:26:29,380
but actually no it's not a three-dimensional graphs and i'm just showing you the marginal two points to the to

1386
01:26:30,070 --> 01:26:31,400
two variables in our gauss you

1387
01:26:32,300 --> 01:26:37,180
so a three-dimensional grousing about means that when you look in these two dimensions with

1388
01:26:37,180 --> 01:26:40,270
projecting the third dimension down on the first to

1389
01:26:41,230 --> 01:26:44,620
so the amount of data that appears close to the mean hit but really in

1390
01:26:44,620 --> 01:26:47,980
the third dimension it's sitting away from the mean because it's a three-dimensional graphs

1391
01:26:48,770 --> 01:26:50,710
we are not a one-dimensional calcium

1392
01:26:51,360 --> 01:26:52,720
so that dimensions

1393
01:26:53,350 --> 01:26:57,240
right being projected onto the so that they use it and it really

1394
01:26:57,870 --> 01:27:02,830
two other dimensions where the data is away from the mean and it's being projected down to the mean

1395
01:27:03,790 --> 01:27:06,050
no it was a five dimensional gaussier

1396
01:27:06,900 --> 01:27:10,630
okay i can go online for a while and we are to a thousand dimensions

1397
01:27:10,630 --> 01:27:15,930
that say with a thousand dimensions this is a marginal distribution from a thousand dimensional

1398
01:27:17,070 --> 01:27:22,040
nine hundred ninety i dimensions are not visualizing just seeing projected down to me

1399
01:27:23,830 --> 01:27:26,280
so that means that there is no data the me

1400
01:27:26,700 --> 01:27:31,320
if you go to here we can even accounting processes an infinite dimensional gaussians so

1401
01:27:31,320 --> 01:27:35,180
you can even talk about infinite dimensional gaussians and look at these two parts survey

1402
01:27:35,210 --> 01:27:38,600
yet so there is definitely no data because we've improved the

1403
01:27:39,430 --> 01:27:40,780
projected infinite density

1404
01:27:41,040 --> 01:27:42,330
dimensions down to the mean

1405
01:27:44,050 --> 01:27:48,660
when you look at a guassian well you could guassian is on the restating its

1406
01:27:48,660 --> 01:27:53,410
mean whatever people say about what's happening in high dimensional spaces i can see myself

1407
01:27:53,410 --> 01:27:56,420
that the state and the mean but remember you're looking at

1408
01:27:56,940 --> 01:28:03,290
potentially projection a very very high-dimensional calcium down to those two dimensions which means

1409
01:28:03,700 --> 01:28:08,000
the all this data in the high dimensional gaussians must be nowhere near the mean yeah

1410
01:28:09,180 --> 01:28:16,460
so in fact this approximation is so good that in many areas they will use a gas in distribution

1411
01:28:17,440 --> 01:28:20,460
taking it a highdimensions approximations for sphere

1412
01:28:22,240 --> 01:28:26,910
so in very high dimensions if you want to have a uniform data distributed over sphere

1413
01:28:27,480 --> 01:28:29,080
you can approximate with a calcium

1414
01:28:29,530 --> 01:28:31,280
and that's the sort of known physics trick

1415
01:28:31,820 --> 01:28:32,700
what very well

1416
01:28:33,290 --> 01:28:35,110
i dimensions going the thousands

1417
01:28:35,670 --> 01:28:37,200
so galcians are

1418
01:28:37,630 --> 01:28:38,940
bad distributions

1419
01:28:41,500 --> 01:28:42,330
half so

1420
01:28:44,680 --> 01:28:45,690
the other thing that happens

1421
01:28:46,250 --> 01:28:50,680
is interpoint distances so in one slide this is the shiny show that the same

1422
01:28:50,680 --> 01:28:54,930
thing happens with the distance between data points if the sample from this calcium density

1423
01:28:55,680 --> 01:28:59,220
for very similar reasons it comes out very quickly if you think about it so

1424
01:28:59,440 --> 01:29:02,470
if you've got to data points both samples from the same guassian

1425
01:29:03,190 --> 01:29:06,730
and then subtracting one of the other leads to another calcium density

1426
01:29:07,940 --> 01:29:10,510
and squaring it then again listed this gamma density

1427
01:29:12,090 --> 01:29:16,170
that's the only new step subtracting one from another and sigma squared comes to sigma

1428
01:29:16,170 --> 01:29:19,240
squared everything else stays the same and you can see

1429
01:29:19,820 --> 01:29:22,650
that these data points a gamma distributed

1430
01:29:23,750 --> 01:29:26,230
the interpoint distances gamma distributed

1431
01:29:27,300 --> 01:29:29,590
with the properties of the gamma being pe over to

1432
01:29:30,070 --> 01:29:33,440
appeared before sigma squared rather than to sigma squared as we saw before

1433
01:29:33,440 --> 01:29:38,960
immediately right down the corresponding fourier transforms on the homage homogeneous space so in the

1434
01:29:38,960 --> 01:29:39,960
case of the sphere

1435
01:29:40,380 --> 01:29:41,630
what you get

1436
01:29:41,630 --> 01:29:46,340
and that therefore it transform of

1437
01:29:46,350 --> 01:29:48,590
this function on the sphere

1438
01:29:48,590 --> 01:29:51,380
and the representation rho

1439
01:29:51,430 --> 01:29:52,870
was going to be

1440
01:29:52,880 --> 01:29:54,380
the integral

1441
01:29:57,680 --> 01:30:01,870
again representation matrices call

1442
01:30:01,880 --> 01:30:08,840
so the surface integral over the rotation group SO internal rotations of the representation matrices

1443
01:30:09,260 --> 01:30:13,840
multiplied by the function but it wasn't controversial you the function of rotation of function

1444
01:30:13,840 --> 01:30:18,710
is not on rotations of the sphere to function

1445
01:30:19,620 --> 01:30:21,230
function evaluated

1446
01:30:25,720 --> 01:30:31,210
o point which you get by rotating your starting element by r

1447
01:30:34,510 --> 01:30:37,750
homogeneous spaces are kind of natural construction

1448
01:30:37,800 --> 01:30:43,340
and once you know the theory of harmonic analysis on groups you automatically have a

1449
01:30:43,340 --> 01:30:44,290
theory of

1450
01:30:44,430 --> 01:30:48,730
i harmonic analysis in to mogeneous space is just by this formula

1451
01:30:50,180 --> 01:30:53,690
and translations on

1452
01:30:54,210 --> 01:30:59,080
on the homogeneous space corresponding natural way to translations of the original group

1453
01:31:00,340 --> 01:31:07,150
everything that i said before including the by spectrum and capture this result does carry

1454
01:31:08,120 --> 01:31:13,420
OK so we can construct rotation invariance on the sphere we have a function on

1455
01:31:13,430 --> 01:31:18,560
the sphere we've form in the fourier transform given by that

1456
01:31:18,590 --> 01:31:22,500
from that from the by spectrum and we get at a bunch of numbers

1457
01:31:22,510 --> 01:31:23,840
which are going to be

1458
01:31:23,850 --> 01:31:28,720
invariant rotating the sphere this is something that's potentially really useful in all sorts of

1459
01:31:28,720 --> 01:31:31,690
applications involving rotational invariance

1460
01:31:32,680 --> 01:31:36,680
there's is the catch and the catch is that in this sort of context when

1461
01:31:36,680 --> 01:31:39,880
you have homogeneous spaces the

1462
01:31:39,910 --> 01:31:43,520
singularity condition might be violated

1463
01:31:43,540 --> 01:31:46,380
so it seems at first sight the seems

1464
01:31:46,430 --> 01:31:52,690
not this business of matrices not being singular and the fourier transform it turns out

1465
01:31:52,690 --> 01:31:57,350
that when you do this sort of integral so when you can afford

1466
01:31:57,380 --> 01:32:02,840
more that by something and work on homogeneous spaces as opposed to groups

1467
01:32:02,850 --> 01:32:07,380
originally typically what happens is at least some of the companies to become singular

1468
01:32:07,620 --> 01:32:08,980
so things

1469
01:32:09,000 --> 01:32:11,940
become more complicated because

1470
01:32:13,330 --> 01:32:15,130
the real power of

1471
01:32:15,150 --> 01:32:16,960
the by spectrum

1472
01:32:16,960 --> 01:32:20,470
you have to do a bit more work to get

1473
01:32:20,480 --> 01:32:25,810
so these are the suggested graphical representation for the

1474
01:32:25,870 --> 01:32:28,940
rotations acting on the sphere

1475
01:32:28,970 --> 01:32:30,410
thank you

1476
01:32:31,380 --> 01:32:35,100
this is all nice and

1477
01:32:36,170 --> 01:32:41,120
let's get back to the problem of constructing rotation and translation invariant

1478
01:32:41,130 --> 01:32:43,040
how do we link up

1479
01:32:43,050 --> 01:32:44,480
what we have

1480
01:32:44,500 --> 01:32:45,930
you know about

1481
01:32:45,960 --> 01:32:51,230
invariance with respect to the action of combat groups and say something like the sphere

1482
01:32:51,250 --> 01:32:56,000
two original problem of invariance with respect to translation and rotation

1483
01:32:56,010 --> 01:32:58,290
in the plane

1484
01:32:58,410 --> 01:32:59,600
the reason we can't

1485
01:32:59,720 --> 01:33:01,330
directly apply

1486
01:33:01,370 --> 01:33:05,060
the by spectrum and characterize results two

1487
01:33:05,180 --> 01:33:07,520
to this case is that

1488
01:33:07,550 --> 01:33:11,230
the group of rigid body motions is not compact

1489
01:33:12,720 --> 01:33:16,010
that means that both the overall representation theory

1490
01:33:16,010 --> 01:33:20,090
becomes unpleasantly complicated

1491
01:33:20,100 --> 01:33:22,810
and also this completeness result

1492
01:33:22,840 --> 01:33:24,510
it's something that we can't prove

1493
01:33:24,540 --> 01:33:26,540
the idea

1494
01:33:26,550 --> 01:33:34,840
and that became up with about a year ago is to establish an isomorphism between

1495
01:33:34,840 --> 01:33:38,050
these two types of problems at least local isomorphism

1496
01:33:39,090 --> 01:33:44,120
to establish a connection between what we want which is translation and rotation invariance on

1497
01:33:44,130 --> 01:33:45,080
the plane

1498
01:33:45,090 --> 01:33:48,760
and what we have which is rotation invariance

1499
01:33:48,770 --> 01:33:51,510
on the two and on the sphere in

1500
01:33:51,510 --> 01:33:52,340
part three

1501
01:33:54,130 --> 01:33:58,430
connection is really not that complicated so if you think about what happens when you

1502
01:33:58,430 --> 01:34:01,630
project your image onto the sphere

1503
01:34:01,680 --> 01:34:06,420
and the anyway but made the most simple way that you can think of just

1504
01:34:06,510 --> 01:34:09,650
projecting vertically down by the z axis

1505
01:34:09,670 --> 01:34:11,680
and then you think about

1506
01:34:11,720 --> 01:34:19,090
what happens to this projected image when you subject the original image to transformations so

1507
01:34:19,130 --> 01:34:21,590
things like translation this way

1508
01:34:21,980 --> 01:34:24,180
and translation that way

1509
01:34:24,180 --> 01:34:26,420
and rotations

1510
01:34:26,430 --> 01:34:28,230
then you discover that

1511
01:34:28,260 --> 01:34:35,330
effectively what's going to be happening in the sphere is just three-dimensional rotations

1512
01:34:35,340 --> 01:34:37,670
in our three

1513
01:34:38,470 --> 01:34:40,300
so this is like modelling

1514
01:34:40,310 --> 01:34:43,760
noncompact problems locally

1515
01:34:43,840 --> 01:34:47,930
by compact problem in one high what in one dimension i

1516
01:34:47,940 --> 01:34:49,390
thank you

1517
01:34:50,790 --> 01:34:52,960
this is all that it takes to reduce

1518
01:34:52,980 --> 01:34:57,790
the original non-combat problem of rotations and translations

1519
01:34:57,880 --> 01:34:59,120
into the

1520
01:34:59,120 --> 01:35:01,170
classical problem of

1521
01:35:02,510 --> 01:35:05,930
symmetry on the sphere

1522
01:35:07,380 --> 01:35:11,000
we can push through the all the by spectral

1523
01:35:13,960 --> 01:35:15,260
fifty project

1524
01:35:15,290 --> 01:35:19,410
we have we can start with the initial image say h

1525
01:35:21,040 --> 01:35:23,550
we have project down the sphere

1526
01:35:23,580 --> 01:35:27,120
and this is the kind of simple minded way

1527
01:35:27,130 --> 01:35:31,800
or or

1528
01:35:31,810 --> 01:35:33,620
so this is just the

1529
01:35:33,620 --> 01:35:37,270
particular way in which we choose to present the simplest ways to express it in

1530
01:35:37,270 --> 01:35:40,970
polar coordinates

1531
01:35:41,010 --> 01:35:45,730
so we have to parameterize the sphere somehow in thesis with weight apprentices in terms

1532
01:35:45,730 --> 01:35:47,550
of spherical

1533
01:35:47,670 --> 01:35:51,040
many wanted project the

1534
01:35:51,050 --> 01:35:54,020
the easiest way to set up the correspondences if we start out with those in

1535
01:35:54,020 --> 01:35:54,940
the first place

1536
01:35:54,960 --> 01:35:57,670
so we start out with

1537
01:35:58,370 --> 01:36:03,980
image in order to permit rise concordance with projected onto the sphere and then we

1538
01:36:03,980 --> 01:36:05,120
take the

1539
01:36:05,230 --> 01:36:06,760
fourier transforms

1540
01:36:09,340 --> 01:36:11,810
if you think about

1541
01:36:11,830 --> 01:36:17,090
clearly if you look into a little bit in figure out what fourier transformation on

1542
01:36:17,090 --> 01:36:22,420
the sphere actually means it turns out that it all boils down to if taking

1543
01:36:22,420 --> 01:36:24,840
the spherical harmonic expansion

1544
01:36:25,800 --> 01:36:29,770
so these things these matrices coming out there

1545
01:36:30,040 --> 01:36:33,130
going to be put together from the ordinary

1546
01:36:33,260 --> 01:36:37,750
spherical harmonic coefficients of projected image

1547
01:36:37,750 --> 01:36:39,900
so there is this you know

1548
01:36:40,180 --> 01:36:42,030
picture of so that

1549
01:36:42,170 --> 01:36:43,540
the manifold

1550
01:36:44,790 --> 01:36:48,210
and some people agree with this people x from say that's kind of wrong picture to

1551
01:36:48,230 --> 01:36:48,800
think about

1552
01:36:49,160 --> 01:36:52,240
i think i agree with him but it's but still useful

1553
01:36:57,380 --> 01:37:02,040
what we like to do is you know turn ideally what an ideal feature extractor to

1554
01:37:02,060 --> 01:37:02,850
do is

1555
01:37:04,550 --> 01:37:05,390
a bunch of

1556
01:37:06,050 --> 01:37:07,170
samples data points

1557
01:37:07,980 --> 01:37:08,880
and tell you

1558
01:37:10,150 --> 01:37:13,150
if there is a manifold data tell you where your manifold

1559
01:37:13,570 --> 01:37:17,440
with a number of components and have a separate set of components that tell you

1560
01:37:19,320 --> 01:37:22,750
distance manifold in all the other dimensions in the space

1561
01:37:23,320 --> 01:37:26,040
ok so what you really want you want to kind of

1562
01:37:27,770 --> 01:37:32,090
two sets of of variables y is where you want manifold on the other one is

1563
01:37:32,100 --> 01:37:33,540
where you are away from the manifold

1564
01:37:35,200 --> 01:37:40,520
ok so if you have for example the set of manifold for possible faces all human faces

1565
01:37:40,650 --> 01:37:41,770
it's high-dimensional manifold

1566
01:37:42,540 --> 01:37:47,690
for a particular person is bounded by the number of muscles in your face and the

1567
01:37:47,700 --> 01:37:50,730
number of you know degrees of freedom you can move around six

1568
01:37:52,620 --> 01:37:56,450
that if you include everybody then there is some sort of you know number dimensions

1569
01:37:56,670 --> 01:38:00,670
basically bounded by the genome of you know how many different cases you can have

1570
01:38:01,530 --> 01:38:02,730
what do

1571
01:38:09,180 --> 01:38:10,100
and so

1572
01:38:10,260 --> 01:38:13,460
you know if we had this manifold then if if we had this kind of a

1573
01:38:13,490 --> 01:38:14,910
set features separation

1574
01:38:15,100 --> 01:38:16,040
you could use

1575
01:38:16,680 --> 01:38:18,370
the first part to tell

1576
01:38:18,510 --> 01:38:20,490
who you're looking at what expression making

1577
01:38:20,660 --> 01:38:22,080
and you can use the second half

1578
01:38:22,290 --> 01:38:25,770
the part tells you you know how far away you are from the manifold to

1579
01:38:25,790 --> 01:38:26,230
tell you

1580
01:38:26,570 --> 01:38:27,530
if it's a face on not

1581
01:38:28,210 --> 01:38:29,100
or something else

1582
01:38:31,420 --> 01:38:37,270
so you know disentangling the explanatory factors of variation is really what unsupervised learning should be that

1583
01:38:37,860 --> 01:38:41,270
there is a general idea which sort emerging a little bit there is no kind

1584
01:38:41,280 --> 01:38:46,310
of concrete our theoretical justification for business what people are doing in the end

1585
01:38:46,940 --> 01:38:50,870
which is that feature extractor should really be composed of to steps

1586
01:38:51,250 --> 01:38:53,500
a non-linear step basically embeds

1587
01:38:53,860 --> 01:38:54,840
the input

1588
01:38:55,050 --> 01:38:58,520
into a very high dimensional space on high dimensional space ok this is similar to the

1589
01:38:58,940 --> 01:39:03,770
kind of kernel kind of thing right to and things to high-dimensional space so that things are more easily separable that

1590
01:39:04,710 --> 01:39:05,500
same stuff

1591
01:39:05,780 --> 01:39:09,350
except you know not based on the kernel trick based on other things

1592
01:39:09,590 --> 01:39:13,260
f has to be non-linear because it's linear doesn't do anything useful for you

1593
01:39:15,120 --> 01:39:19,300
ok so and then your input into a high-dimensional space in some sort of non-linear way

1594
01:39:19,450 --> 01:39:23,290
but what to do here is that the breaking this space the the space part of

1595
01:39:23,300 --> 01:39:27,940
the things that are semantically similar or even identical will end up in different bins

1596
01:39:28,300 --> 01:39:30,610
this representation to be very far apart

1597
01:39:30,890 --> 01:39:36,390
so the second step which is sort of generalized pooling similar to the pooling doing commercial nets

1598
01:39:36,730 --> 01:39:37,830
is to regroup

1599
01:39:38,030 --> 01:39:40,940
the things here are supposed to be similar

1600
01:39:41,800 --> 01:39:43,430
ok so whatever

1601
01:39:44,220 --> 01:39:47,660
you get vectors here represent two faces and happen to be very different

1602
01:39:48,140 --> 01:39:52,030
you somehow encode then such a way that here and have been

1603
01:39:53,710 --> 01:39:59,610
yeah it may or may be linear or and sorry that encour you're

1604
01:40:01,080 --> 01:40:02,320
probably up or

1605
01:40:03,360 --> 01:40:06,210
what people in fact this is something like

1606
01:40:07,850 --> 01:40:08,570
i l

1607
01:40:08,700 --> 01:40:11,840
so score some squares some components

1608
01:40:12,060 --> 01:40:12,980
subset of components

1609
01:40:15,030 --> 01:40:26,890
yeah ok yeah yeah right ok so here's an example for very concrete example for the

1610
01:40:26,910 --> 01:40:30,580
those of mainstream approach to computer vision and to accomplish on its right so

1611
01:40:30,910 --> 01:40:34,000
as a nick are very much for the for example

1612
01:40:34,880 --> 01:40:38,750
so formidable features which do here is k-means so you can you take

1613
01:40:39,190 --> 01:40:40,250
a seat vector

1614
01:40:40,510 --> 01:40:42,970
your need through k-means algorithm k-means is going to

1615
01:40:43,190 --> 01:40:45,530
give you a binary vector with all zeros but

1616
01:40:45,670 --> 01:40:47,730
and one one at the location of the

1617
01:40:47,970 --> 01:40:49,880
prototype this close to the input ok

1618
01:40:50,480 --> 01:40:52,690
so just when i take cold kind of encoding if you want

1619
01:40:53,050 --> 01:40:55,360
word right visual word

1620
01:40:56,180 --> 01:41:00,410
the pulley here takes all those feature vectors for the entire image just represent

1621
01:41:00,880 --> 01:41:02,800
or you know combines and some

1622
01:41:03,730 --> 01:41:07,320
so knowledge it is a histogram of words and you can see that you have a classifier

1623
01:41:07,420 --> 01:41:11,030
and the the the goal of this is to basically give you shifted guyon so

1624
01:41:11,400 --> 01:41:16,040
once you do this segregation the position of feature doesn't matter anymore just the presence of that matters

1625
01:41:16,380 --> 01:41:18,920
ok to regroup things that were you know

1626
01:41:20,230 --> 01:41:23,910
now it turns out when you do something that sparse coding here works much better if you do k-means

1627
01:41:24,180 --> 01:41:25,020
sparse coding

1628
01:41:25,490 --> 01:41:27,450
preserves a bit of

1629
01:41:27,830 --> 01:41:30,030
of ce ity between things right

1630
01:41:30,500 --> 01:41:32,690
which k-means just completely

1631
01:41:35,000 --> 01:41:38,240
ok so we have this manifold picture a bunch of data points

1632
01:41:38,510 --> 01:41:39,910
and what we like is

1633
01:41:40,120 --> 01:41:43,980
to can learn function that gives us the dependency between x one x to here in this case

1634
01:41:44,930 --> 01:41:49,230
we're going to do is we're going to that this general framework i think that

1635
01:41:49,250 --> 01:41:50,640
is most appropriate for this

1636
01:41:51,090 --> 01:41:54,900
is sort energy-based framework which consisted

1637
01:41:55,380 --> 01:41:59,400
it's actually learning an energy function or think of it as a contrast function that tells you

1638
01:42:00,440 --> 01:42:04,130
i give you point of the contrast wanting to scalar that tells you whether you are

1639
01:42:04,140 --> 01:42:05,860
on the manifold not on the manifold

1640
01:42:07,250 --> 01:42:08,660
if function is

1641
01:42:09,660 --> 01:42:12,800
is able to do this internally has to be able to to kind of

1642
01:42:13,320 --> 01:42:16,250
do this disentangling features right has to be able to

1643
01:42:16,760 --> 01:42:22,640
tell which direction manifold is you know what's the closest point manifold and you know where i doing

1644
01:42:22,660 --> 01:42:24,110
manifold things on this right

1645
01:42:25,430 --> 01:42:28,450
that's all we can do we're going to train system

1646
01:42:28,990 --> 01:42:33,080
to produce a single scalar the scalar to sort of contrast function that tells you

1647
01:42:33,270 --> 01:42:35,020
how far we are fundamental data

1648
01:42:36,170 --> 01:42:39,940
so let's say we take samples coming from the spiral here

1649
01:42:40,350 --> 01:42:44,150
around p c k you can see with only one dimension so it's to

1650
01:42:44,470 --> 01:42:46,350
between put problem to a problem

1651
01:42:46,490 --> 01:42:48,750
and you can see why

1652
01:42:48,890 --> 01:42:49,760
principal component

1653
01:42:50,070 --> 01:42:51,110
piece here will find

1654
01:42:52,830 --> 01:42:55,120
this may axis here the point cloud

1655
01:42:55,750 --> 01:43:02,070
and the the reconstruction error would be zero for anything that's on the principal axis and

1656
01:43:02,170 --> 01:43:04,170
with quadratically as we move away from it

1657
01:43:05,120 --> 01:43:06,450
i if you sparse coding

1658
01:43:07,910 --> 01:43:09,950
you know it will sort of the latter that

1659
01:43:09,950 --> 01:43:12,150
i don't give you a tree

1660
01:43:12,170 --> 01:43:15,800
and we know that the elimination algorithm could work

1661
01:43:15,830 --> 01:43:19,880
but i want to sort of think about doing elimination cleverly i wanna do elimination

1662
01:43:19,880 --> 01:43:23,580
in parallel by message passing like i did before

1663
01:43:25,700 --> 01:43:29,180
a natural idea the probably many of you would have would be well i can

1664
01:43:29,180 --> 01:43:33,370
just sort of start clumping the nodes together i don't have to treat this would

1665
01:43:33,370 --> 01:43:37,150
be a simple example i have just a single cycle here

1666
01:43:37,170 --> 01:43:41,000
and i could sort of start clumping the notes together and then i could make

1667
01:43:41,000 --> 01:43:45,410
a tree on those clumps or clusters of nodes

1668
01:43:45,420 --> 01:43:50,070
so for instance one way to do the to put all nodes that are in

1669
01:43:50,070 --> 01:43:54,570
the same creature one and three are in the same clique put them in what

1670
01:43:54,570 --> 01:43:56,350
you might call supernodes

1671
01:43:56,360 --> 01:43:59,820
so this is a new node and it's got a copy of x one and

1672
01:43:59,820 --> 01:44:02,340
x three it's sort of new random

1673
01:44:03,710 --> 01:44:08,320
and then i could put another supernote here for one and two and another supernova

1674
01:44:08,330 --> 01:44:11,760
two and four and another one for three and four

1675
01:44:11,770 --> 01:44:16,380
right so i sort of taking my model and i've reduced it to a tree

1676
01:44:16,430 --> 01:44:19,670
and then you might think OK well done i can this is the tree and

1677
01:44:19,670 --> 01:44:26,070
i can run a message passing algorithm on this this graph

1678
01:44:26,200 --> 01:44:29,130
it is that quite skipping because some connection

1679
01:44:35,170 --> 01:44:40,700
is there any predictability to

1680
01:44:40,710 --> 01:44:42,690
i think my mike is going to

1681
01:44:44,790 --> 01:44:46,130
it's not like you

1682
01:44:46,140 --> 01:44:49,700
sir batteries

1683
01:44:49,910 --> 01:44:51,480
the battery is dead

1684
01:45:10,280 --> 01:45:14,230
OK so i

1685
01:45:16,840 --> 01:45:18,420
OK so tree here

1686
01:45:19,440 --> 01:45:20,640
that's annoying

1687
01:45:21,260 --> 01:45:25,820
so you might think that you're off to the races that you can just run

1688
01:45:25,820 --> 01:45:29,540
your message passing on the street but the problem here

1689
01:45:29,620 --> 01:45:32,270
so what's the problem here

1690
01:45:32,280 --> 01:45:40,320
is anyone sort of understand this that

1691
01:45:40,350 --> 01:45:44,500
this is useful to think about because this the starts to illustrate what it means

1692
01:45:44,500 --> 01:45:45,990
to be local

1693
01:45:46,750 --> 01:45:49,100
message passing algorithms are local

1694
01:45:49,110 --> 01:45:52,680
they do dumb things that nodes and they as sings along edges they don't know

1695
01:45:52,680 --> 01:45:56,070
what's going on globally in the grass they don't care

1696
01:45:56,090 --> 01:46:00,360
that's part of the beauty it's beautiful because it makes them fast and efficient and

1697
01:46:01,870 --> 01:46:05,040
but it actually causes problems here

1698
01:46:05,190 --> 01:46:09,030
so what have intuition for the problem is

1699
01:46:09,080 --> 01:46:15,650
so there's and you want to give you think of yourself as the local algorithm

1700
01:46:15,650 --> 01:46:19,260
think i'm sitting here and i'm sort of doing stuff with

1701
01:46:19,270 --> 01:46:22,970
x three and x four here and standing along this edge

1702
01:46:23,020 --> 01:46:27,170
but someone else's my friend is sitting here with x one x three as well

1703
01:46:27,170 --> 01:46:30,360
and sending stuff along this edge

1704
01:46:30,380 --> 01:46:32,910
we're not actually talking directly ever

1705
01:46:32,920 --> 01:46:37,710
any communication we have is getting relayed around i never speaking directly to my friend

1706
01:46:37,710 --> 01:46:40,950
in this graph

1707
01:46:40,960 --> 01:46:47,900
so any intuition

1708
01:46:47,910 --> 01:46:51,660
what would happen if i ran the sum product algorithm on this graph what answer

1709
01:46:51,660 --> 01:46:58,800
would give me

1710
01:47:01,410 --> 01:47:04,770
why why

1711
01:47:04,790 --> 01:47:08,720
one of the

1712
01:47:12,390 --> 01:47:17,230
o because i michael sort of you trying to make a tree

1713
01:47:22,660 --> 01:47:24,050
true i could

1714
01:47:25,780 --> 01:47:29,760
but there's a kind of symmetry here it actually wouldn't matter which edge i forgot

1715
01:47:29,770 --> 01:47:34,640
right it's there's the rotational symmetry but that's true

1716
01:47:34,650 --> 01:47:40,130
so you could certainly run the sum product algorithm on this graph there's nothing to

1717
01:47:40,130 --> 01:47:43,680
stop you from doing it but what you want to understand is that it would

1718
01:47:43,680 --> 01:47:45,230
compute something

1719
01:47:45,250 --> 01:47:46,540
it would be correct

1720
01:47:46,580 --> 01:47:50,440
but it would be correct for the wrong model it wouldn't compute the right thing

1721
01:47:50,440 --> 01:47:54,320
for this graph which is what you're interested in

1722
01:47:54,330 --> 01:47:58,170
so the reason is because you again have to think about low

1723
01:47:58,180 --> 01:48:02,180
locality there's there's really one copy of x three sitting here

1724
01:48:02,220 --> 01:48:05,920
and there's another copy of x three sitting here

1725
01:48:05,940 --> 01:48:09,390
but in this model there is no way that those two copies know that they

1726
01:48:09,390 --> 01:48:13,020
just to give you an an idea that is an application of what i've just

1727
01:48:13,020 --> 01:48:18,910
presented so first of all we talk about ontology and metadata ontology refers to the

1728
01:48:18,910 --> 01:48:24,330
schema level and metadata refers to the instance level so metadata to ontology is which

1729
01:48:24,330 --> 01:48:29,560
would be the instances so let's take that simple ontology have academic staff the disquiet

1730
01:48:29,580 --> 01:48:34,950
cooperating with each other so one academic staff can operate with another academic staff and

1731
01:48:34,950 --> 01:48:39,260
you know that phd students and assistant professors are academic staff

1732
01:48:39,280 --> 01:48:43,350
so now we have two homepages from two of my colleagues on the one hand

1733
01:48:43,350 --> 01:48:47,660
you have six the time that and she different stop and if you look now

1734
01:48:47,770 --> 01:48:51,660
this would be what your browser shows you this would be the web page you

1735
01:48:51,660 --> 01:48:57,300
have you know you will be uniquely identify the websites on the one hand and

1736
01:48:57,300 --> 01:48:58,970
on the other hand

1737
01:48:59,050 --> 01:49:05,010
and if you look now two the first quote this so-called metadata within the military

1738
01:49:05,010 --> 01:49:11,850
attacks html a technically spoken so can include additional information in the hands of those

1739
01:49:11,850 --> 01:49:18,660
html documents and she could for example add that secret hundred extra a phd student

1740
01:49:18,660 --> 01:49:22,550
which cooperates with assistant professor steffen stop

1741
01:49:22,560 --> 01:49:26,620
so he wouldn't stand exactly the schema that just provided

1742
01:49:26,810 --> 01:49:33,470
and what differs from html here if you know the so-called age and have which

1743
01:49:33,470 --> 01:49:38,490
left text in html then you also have links between pages but you don't have

1744
01:49:38,490 --> 01:49:43,470
actually meaning but if you if you were the link between the two persons that

1745
01:49:43,470 --> 01:49:48,370
cooperate with each other then you have a well-defined meaning of what these links actually

1746
01:49:48,640 --> 01:49:51,550
good for

1747
01:49:51,550 --> 01:49:57,970
when we actually use that but there exists the so-called interactive community portal which is

1748
01:49:58,660 --> 01:50:03,850
a perfect the thematic network web and if you are interested in topics and themes

1749
01:50:03,850 --> 01:50:07,430
around ontology and semantic web and this is a very good starting point because it

1750
01:50:07,560 --> 01:50:15,060
notes because it lists the important person that currently in the arena the european research

1751
01:50:15,350 --> 01:50:18,720
so this is a nice way of starting your to to find nice documents and

1752
01:50:19,970 --> 01:50:24,060
and how does it work technically so that you know that's not true

1753
01:50:24,080 --> 01:50:29,390
maintain the paper about ourselves because we had the burden of producing the paper but

1754
01:50:29,390 --> 01:50:34,620
we don't want to maintain the content for the next so the idea is that

1755
01:50:34,620 --> 01:50:41,720
each partner provides annotation within its own homepages and then we collect the metadata

1756
01:50:41,950 --> 01:50:47,350
from the participating partners and integrate it into our portal so that they do first

1757
01:50:47,390 --> 01:50:52,510
because they annotate web approach similar to what i've shown before and there exists also

1758
01:50:52,510 --> 01:50:59,260
tools that support this man this annotation tasks you typically have a manual annotation but

1759
01:50:59,260 --> 01:51:03,970
we don't know have also semi-automatic annotations but we will a bit more about that

1760
01:51:03,970 --> 01:51:09,640
in the second part of this tutorial then you integrate different sources different the sources

1761
01:51:09,640 --> 01:51:14,120
from different partners but you could also imagine that you have databases we have telephone

1762
01:51:14,120 --> 01:51:19,160
lists which you could also integrate then the common language the just that has been

1763
01:51:19,220 --> 01:51:22,080
living ontology so for example the common

1764
01:51:22,310 --> 01:51:27,580
commercial solution would be to map some databases into an ontology so could map different

1765
01:51:27,580 --> 01:51:32,150
data sources into one ontology and then query simply the ontology the thing the entropy

1766
01:51:32,150 --> 01:51:36,470
and for intelligence services so this is just an easy example for that and do

1767
01:51:36,470 --> 01:51:41,140
and what we do in the papers the first to generate the pattern of the

1768
01:51:41,140 --> 01:51:46,280
pre defined ontology so we provide the navigational structures so if you look now for

1769
01:51:46,280 --> 01:51:50,640
example at the top level navigational structures this is actually

1770
01:51:50,680 --> 01:51:53,220
you derived out of the ontology

1771
01:51:53,740 --> 01:51:59,430
we provided permanent search engine so you could ask you those against the ontology and

1772
01:51:59,430 --> 01:52:04,490
that as answers the combined results from the participating partners and last but not least

1773
01:52:04,490 --> 01:52:10,470
you can produce pre-defined forms to your data manually into the petals so this would

1774
01:52:10,470 --> 01:52:15,950
be quite common scenario for example for which is currently up and running

1775
01:52:15,950 --> 01:52:20,550
or are all the rage that

1776
01:52:20,610 --> 01:52:25,550
there are some things that

1777
01:52:25,560 --> 01:52:29,260
this is the test that that is an ongoing effort so currently if you look

1778
01:52:29,260 --> 01:52:35,390
you will mostly find the metadata from of course the institute and some partners so

1779
01:52:35,470 --> 01:52:40,370
so far we actually provided the basic infrastructure in the next step would be the

1780
01:52:40,590 --> 01:52:47,760
partners provide their metadata and the next step would be to make it more melodramatic

1781
01:52:47,760 --> 01:52:53,660
because nobody really wants to annotate all homepages automatically and this is one the topics

1782
01:52:53,660 --> 01:52:58,680
around that interact better for example we have also the semantic web to make paper

1783
01:52:58,700 --> 01:53:00,870
which we know which relies on

1784
01:53:00,870 --> 01:53:06,010
so the technology these papers can exchange content for example so you can add the

1785
01:53:06,010 --> 01:53:11,740
news events in one part and then it is interchanged with the other portal and

1786
01:53:11,740 --> 01:53:17,340
OK well good morning and a very warm welcome to cambridge from this surface typically

1787
01:53:17,340 --> 01:53:25,510
sunny day and you know it from in here

1788
01:53:25,530 --> 01:53:27,660
OK and over here i guess

1789
01:53:27,670 --> 01:53:33,320
so because this is the first talk of the summer school i thought is something

1790
01:53:33,320 --> 01:53:35,100
slightly different instead of

1791
01:53:35,120 --> 01:53:38,050
trying to teach a lot of technical detail which i think you get from a

1792
01:53:38,050 --> 01:53:39,410
lot of the later talks

1793
01:53:39,440 --> 01:53:42,750
i thought i'd give you sort of personal perspective on where i think we are

1794
01:53:42,750 --> 01:53:46,580
in the field of machine learning and in particular have come to believe that the

1795
01:53:46,580 --> 01:53:50,950
next ten years also going to be the most exciting in the entire history of

1796
01:53:50,950 --> 01:53:54,440
the field of machine learning because we have some very exciting new techniques which allow

1797
01:53:54,440 --> 01:53:55,730
us to address the whole

1798
01:53:55,770 --> 01:54:00,330
class of new kinds of applications such really what i want to talk about

1799
01:54:00,690 --> 01:54:04,480
in this first talk and it's very much a personal perspective of course of lectures

1800
01:54:04,480 --> 01:54:06,940
may have other views on this is well

1801
01:54:06,970 --> 01:54:13,060
let me begin with perhaps the most important slide that you'll see which is this

1802
01:54:13,720 --> 01:54:18,930
which is to encourage you to ask questions and to make the summer school interactive

1803
01:54:18,930 --> 01:54:21,830
now i'm going to stand here two lots of

1804
01:54:21,880 --> 01:54:25,380
ninety minutes that you're going to be sat there for twenty lots of ninety minutes

1805
01:54:25,380 --> 01:54:29,510
or something it's a it's a long two weeks and

1806
01:54:29,520 --> 01:54:31,740
you will get a lot more out of it if we if we make it

1807
01:54:31,740 --> 01:54:36,120
interactive so please just interrupt and ask questions as we go through

1808
01:54:36,160 --> 01:54:39,990
almost as important as the slide which is just to mention that

1809
01:54:39,990 --> 01:54:40,820
o p

1810
01:54:40,820 --> 01:54:44,790
taking material taken second talk from from this textbook and a copy here if it

1811
01:54:44,830 --> 01:54:46,460
if you like to take a look

1812
01:54:48,550 --> 01:54:51,930
i'm going to characterize what i think is happening in the field of machine learning

1813
01:54:51,930 --> 01:54:56,330
as a sort of third generation of machine learning and so better explain my terminology

1814
01:54:56,330 --> 01:54:57,900
by telling you what i mean by

1815
01:54:57,910 --> 01:54:59,980
the first and second generations

1816
01:54:59,990 --> 01:55:04,260
was really the first and second generations of machine intelligence i guess

1817
01:55:05,510 --> 01:55:10,040
what i call the first generation is is good old-fashioned AI artificial intelligence

1818
01:55:10,050 --> 01:55:14,910
and the nice cohen from minsk from nineteen sixty seven within a generation the problem

1819
01:55:14,910 --> 01:55:18,660
of creating artificial intelligence will largely be solved

1820
01:55:18,710 --> 01:55:20,900
and two years later

1821
01:55:20,950 --> 01:55:24,150
anybody recognise this is what this is

1822
01:55:24,210 --> 01:55:28,530
how great this is from the film two thousand one so this was i think

1823
01:55:28,550 --> 01:55:32,220
released in nineteen sixty eight or they are about the same era and depicted the

1824
01:55:32,220 --> 01:55:36,710
by the year two thousand would have the intelligent putative conversation about philosophy with all

1825
01:55:36,710 --> 01:55:38,270
sorts of things

1826
01:55:38,280 --> 01:55:41,220
and the sort of canonical

1827
01:55:41,240 --> 01:55:44,460
first generation technique was the expert system

1828
01:55:44,460 --> 01:55:49,020
based on knowledge extracted from humans

1829
01:55:50,740 --> 01:55:53,990
i had a number of successes and indeed even today there are some problems with

1830
01:55:53,990 --> 01:55:56,970
if you want to solve those problems an expert system is going to be the

1831
01:55:56,970 --> 01:55:58,090
best way to go

1832
01:55:59,310 --> 01:56:02,140
but in most applications the

1833
01:56:02,150 --> 01:56:06,740
technical expert system suffers from this combinatorial explosion of lots of rules there are exceptions

1834
01:56:06,740 --> 01:56:11,400
to the rules exceptions to exceptions and so on and so by and large expert

1835
01:56:11,400 --> 01:56:14,990
systems failed to deliver on the on the vision

1836
01:56:15,030 --> 01:56:22,420
so can first generation as handcrafted rules everything is built out of rules extracted from

1837
01:56:22,420 --> 01:56:26,960
human maybe probabilistic rules but the all hand crafted

1838
01:56:27,060 --> 01:56:32,500
so so-called second generation is characterized by things like neural networks support vector machines he

1839
01:56:32,500 --> 01:56:37,670
proved to be much more successful in terms of applications so his example handwriting recognition

1840
01:56:37,670 --> 01:56:42,050
and there are thousands of other applications now all of these techniques

1841
01:56:42,060 --> 01:56:47,390
and again even today they remain extremely valuable from a practical perspective

1842
01:56:47,400 --> 01:56:52,740
one of the difficulties though is that incorporating article complex domain knowledge is going give

1843
01:56:52,740 --> 01:56:54,120
you examples of this

1844
01:56:54,490 --> 01:56:56,620
as we go through

1845
01:56:58,530 --> 01:57:03,470
OK have tried second generation as black box statistical models the emphasis is very much

1846
01:57:03,470 --> 01:57:07,740
is on learning from data any prior knowledge is built in is is a rather

1847
01:57:07,740 --> 01:57:12,300
general nature like continuity or maybe some translation invariance or something it's hard to build

1848
01:57:12,420 --> 01:57:15,520
this is rich domain knowledge

1849
01:57:15,550 --> 01:57:19,680
so i think i'm very excited about what i'm going to call third-generation machine intelligence

1850
01:57:19,680 --> 01:57:21,490
that characterized by

1851
01:57:21,530 --> 01:57:27,900
deep integration of rich domain knowledge combined with statistical learning

1852
01:57:27,920 --> 01:57:32,000
that's really the sort of theme of this this first talk

1853
01:57:32,020 --> 01:57:33,710
so there are three

1854
01:57:34,150 --> 01:57:39,040
ingredients to this framework first is that it's based on a bayesian approach the second

1855
01:57:39,040 --> 01:57:44,790
is that it makes use of probabilistic graphical models in order to describe the the

1856
01:57:44,810 --> 01:57:49,830
the the approach to the problem the modelling of the problem and it makes use

1857
01:57:49,830 --> 01:57:55,040
of fast inference techniques which are based on passing local messages around these graphs in

1858
01:57:55,040 --> 01:58:00,090
order to find tractable solutions and in a way which which is very efficient and

1859
01:58:00,090 --> 01:58:03,200
scale to very large datasets

1860
01:58:03,210 --> 01:58:08,460
although called generation these ideas have their origins going back a very long way in

1861
01:58:08,460 --> 01:58:10,780
all sorts of fields like to school

1862
01:58:10,810 --> 01:58:16,640
physics and electrical engineering signal processing and so on so these sort of generations of

1863
01:58:16,640 --> 01:58:20,220
it in a sense up in parallel with this i think is for me one

1864
01:58:20,220 --> 01:58:24,970
of the sort of it's very exciting frontiers of machine learning right now

1865
01:58:25,100 --> 01:58:29,330
OK let's just begin then with talking a little bit about those three things in

1866
01:58:29,330 --> 01:58:34,700
turn i'm just going to skim lightly over these topics and you'll be familiar with

1867
01:58:34,700 --> 01:58:36,530
some of them perhaps others less so

1868
01:58:36,540 --> 01:58:40,530
but later talks will drill down into more detail i want to give you a

1869
01:58:40,530 --> 01:58:45,730
flavour for these different topics and hopefully sort of motivate you to to be excited

1870
01:58:45,730 --> 01:58:46,860
about them into

1871
01:58:46,860 --> 01:58:48,240
sorry about this

1872
01:58:50,660 --> 01:58:55,160
the material is that

1873
01:59:04,090 --> 01:59:10,510
it has to do with the web page analysis text categorisation position filtering and structuring

1874
01:59:10,510 --> 01:59:16,630
of factual information and all this connects also with the stronger for natural language processing

1875
01:59:16,960 --> 01:59:18,400
which is a separate area of

1876
01:59:18,900 --> 01:59:27,190
information processing and this is a prerequisite for successful text and web mining

1877
01:59:27,200 --> 01:59:34,710
visualization we've seen see some ways of visualizing the data as well as visualizing the

1878
01:59:35,130 --> 01:59:37,060
discovered knowledge

1879
01:59:38,090 --> 01:59:42,070
and of course machine learning that's where our research

1880
01:59:42,110 --> 01:59:43,350
is mainly in

1881
01:59:43,430 --> 01:59:49,220
and this as i explained before is an expression coming from artificial intelligence and we

1882
01:59:49,220 --> 01:59:56,210
are also very interested here in the relation of machine learning with statistics

1883
01:59:56,230 --> 02:00:00,440
if time allows i would have some slides on that relation later on

1884
02:00:01,520 --> 02:00:07,680
there is obviously a very strong connection between machine learning data mining and statistics so

1885
02:00:07,690 --> 02:00:12,500
all you have to do with inductive techniques for data analysis so you have data

1886
02:00:12,520 --> 02:00:17,890
start from the data and then you induce knowledge out of the data

1887
02:00:17,900 --> 02:00:20,460
so in this is induction

1888
02:00:20,480 --> 02:00:21,580
goes from

1889
02:00:21,590 --> 02:00:27,570
reasoning from proper properties of data or individuals to properties which hold on the population

1890
02:00:27,570 --> 02:00:31,620
of individuals so some people could say well

1891
02:00:31,670 --> 02:00:36,730
why do we need data mining and knowledge discovery in databases isn't this just statistics

1892
02:00:36,730 --> 02:00:41,930
and some good marketing around it we would say well it has much has been

1893
02:00:41,930 --> 02:00:48,490
done in machine learning separately from statistics you can see the representation formalisms for tuesday

1894
02:00:48,490 --> 02:00:49,410
data is

1895
02:00:49,430 --> 02:00:54,170
a much different also sometimes the emphasis is different

1896
02:00:54,190 --> 02:00:58,110
and if we were the very roughly some up

1897
02:00:58,150 --> 02:01:00,110
we went

1898
02:01:00,160 --> 02:01:07,020
possibly say that statistics is particularly appropriate for hypothesis testing and data analysis with certain

1899
02:01:07,020 --> 02:01:13,520
theoretical expectations about data distribution independent random sampling and so on are satisfied so there

1900
02:01:13,520 --> 02:01:19,820
is a strong machinery and artillery about what data should be like that we can

1901
02:01:19,820 --> 02:01:25,630
then use reasonable statistical

1902
02:01:25,650 --> 02:01:32,850
these rules which hold on for a population obviously you should start with this

1903
02:01:32,870 --> 02:01:34,700
for sufficiently large

1904
02:01:34,710 --> 02:01:41,220
data sample if the sample is not large enough statisticians will not touch the data

1905
02:01:41,260 --> 02:01:45,900
at all and i would say we can't do anything which has strong reasons of

1906
02:01:46,600 --> 02:01:49,840
well machine learning people are much more

1907
02:01:57,040 --> 02:02:02,880
occasionally we start with very few data and we dare to say something what is

1908
02:02:02,880 --> 02:02:08,680
hidden in the data now this is particularly interesting with microarray data analysis analysis in

1909
02:02:08,680 --> 02:02:15,410
bioinformatics were getting the samples is extremely expensive and there's a laborious process

1910
02:02:15,550 --> 02:02:21,760
before another sample is obtained and it turns out that actually he even with not

1911
02:02:21,760 --> 02:02:24,810
that many data available you can

1912
02:02:25,110 --> 02:02:33,060
induce some interesting piece of knowledge which can be of use for scientific purposes so

1913
02:02:33,070 --> 02:02:35,740
with machine learning we

1914
02:02:36,310 --> 02:02:42,290
there occasionally to induce some knowledge of the less data

1915
02:02:44,040 --> 02:02:46,860
our main aim is to

1916
02:02:47,220 --> 02:02:53,600
induce knowledge in the form which is understandable to humans and that's one of the

1917
02:02:53,600 --> 02:02:56,370
strong points of machine learning and data mining

1918
02:02:58,830 --> 02:03:04,500
what is the relation between the expressions data mining and knowledge discovery in databases

1919
02:03:04,510 --> 02:03:06,680
data mining

1920
02:03:06,690 --> 02:03:12,710
similar is machine learning is aimed at finding patterns revealing hidden regularities and relationships in

1921
02:03:12,710 --> 02:03:17,340
the data whereas knowledge discovery in databases is viewed as the process

1922
02:03:17,350 --> 02:03:20,440
which includes also

1923
02:03:20,980 --> 02:03:23,730
data cleaning selecting the

1924
02:03:23,740 --> 02:03:31,330
the data for the the and it includes the entire process of data analysis including

1925
02:03:31,330 --> 02:03:37,160
the statisticians sort of hypothesis selection and we view data mining is one of the

1926
02:03:37,170 --> 02:03:39,230
key elements in this process

1927
02:03:43,060 --> 02:03:44,010
if we

1928
02:03:44,030 --> 02:03:45,520
illustrate this process

1929
02:03:45,520 --> 02:03:48,270
graph most this is from third-party libraries

1930
02:03:48,280 --> 02:03:52,510
i do some when programming values for the winter two API which the graphics stuff

1931
02:03:52,660 --> 02:03:57,400
others across from ones like open GL you can do x system i think magazine

1932
02:03:57,400 --> 02:03:58,330
called carbon

1933
02:03:58,390 --> 02:04:00,310
i might be wrong about that

1934
02:04:00,360 --> 02:04:02,090
OK carbon

1935
02:04:02,140 --> 02:04:06,140
there's a lot to ensure you won't have trouble finding it just you want have

1936
02:04:06,310 --> 02:04:09,610
and whatever that does it for you shop around this and take a look

1937
02:04:09,620 --> 02:04:13,130
the ones we give you a pretty nice i'd like to tell you how they

1938
02:04:13,130 --> 02:04:17,990
work cetera i actually don't know because it's really really really hard so go take

1939
02:04:18,010 --> 02:04:21,510
a look around and you find the ones that are quite nice

1940
02:04:21,550 --> 02:04:25,120
now have to this point are simple things like how to read input how to

1941
02:04:25,120 --> 02:04:28,780
convert a string things that you know what the nice weather important not going to

1942
02:04:28,780 --> 02:04:30,400
make a break your program

1943
02:04:30,420 --> 02:04:32,570
the thing is really going to make a difference

1944
02:04:32,640 --> 02:04:33,810
is a

1945
02:04:33,820 --> 02:04:37,730
the vector the set the map this tactic you those are important

1946
02:04:37,730 --> 02:04:40,540
if you don't have map the number of things you can do in the program

1947
02:04:40,540 --> 02:04:45,810
kind of just drop exponentially it's really impressive how much you need data structures

1948
02:04:45,820 --> 02:04:48,630
the good news is that c plus plus not only has very good support for

1949
02:04:48,630 --> 02:04:53,640
these data structures that has probably the best library in any standard programming language that

1950
02:04:53,640 --> 02:04:55,120
does these things for you

1951
02:04:55,170 --> 02:04:58,070
it's something called the standard template library which is the here

1952
02:04:58,080 --> 02:05:02,780
now for every single container class you have seen in this class of eighty two

1953
02:05:02,780 --> 02:05:06,680
you will find many STL the name might be a little different syntax is that

1954
02:05:06,680 --> 02:05:10,240
we are but conceptually it's the same thing vector the vector no matter what language

1955
02:05:10,310 --> 02:05:15,970
write map is a map stack is still alive though he was still five etcetera

1956
02:05:15,980 --> 02:05:19,800
the on top of that gives you these things called algorithms i'll show all how

1957
02:05:20,070 --> 02:05:21,570
talk about this and all that

1958
02:05:21,590 --> 02:05:24,340
basically the best thing since sliced bread

1959
02:05:24,390 --> 02:05:27,740
i'm not kidding it's really wonderful is really amazing and talk about that a little

1960
02:05:28,500 --> 02:05:32,060
but the scale is one of the major features of the c plus plus library

1961
02:05:32,060 --> 02:05:37,070
it's brilliantly designed it's very meaning it's not so the most intuitive thing once you

1962
02:05:37,070 --> 02:05:41,590
get a handle on it's very powerful

1963
02:05:41,650 --> 02:05:44,930
but we can kind of visualizing this looks like it's kind of like this

1964
02:05:44,960 --> 02:05:47,230
somewhat human this structure

1965
02:05:47,240 --> 02:05:49,200
at the very bottom of the container

1966
02:05:49,210 --> 02:05:52,510
the data structures is your vector maps your set

1967
02:05:52,720 --> 02:05:55,580
iterators on top of that now you've all seen iterators in this class you've seen

1968
02:05:55,580 --> 02:05:57,780
map iterators fifteen set iterators

1969
02:05:57,790 --> 02:06:02,850
yes you know is basically all the iterators every single content class has interesting iterate

1970
02:06:02,850 --> 02:06:06,390
over vector generator mapping iterate over sets

1971
02:06:06,420 --> 02:06:10,030
and those working to build this thing called algorithms which are going to talk to

1972
02:06:10,030 --> 02:06:13,080
you later where the functions that operate on ranges of data

1973
02:06:13,090 --> 02:06:15,630
and it's really impressive what you can do with these things will show a little

1974
02:06:15,650 --> 02:06:19,450
bit of that later

1975
02:06:19,460 --> 02:06:22,920
and these contain classes of the good news is the names are pretty similar basically

1976
02:06:22,920 --> 02:06:26,250
take the one i six version make it case you've got your FTL so big

1977
02:06:26,250 --> 02:06:30,220
vector turns into little vector big into little map et cetera

1978
02:06:30,270 --> 02:06:35,180
the one exception is grid there is no grid class it's very easy to make

1979
02:06:35,180 --> 02:06:37,990
one i have to do is just take it back to induce the matter as

1980
02:06:38,000 --> 02:06:40,410
a two dimensional container in one dimensional one

1981
02:06:40,560 --> 02:06:43,710
the back that's how are great work so you have to go that one but

1982
02:06:43,710 --> 02:06:47,160
otherwise everything carefully and quite nice

1983
02:06:47,170 --> 02:06:50,700
again they have the same fundamental abstractions you know how to work with the fact

1984
02:06:50,700 --> 02:06:54,470
that you know how to use scale factor with the lectures syntax

1985
02:06:54,510 --> 02:06:58,410
the big thing i will say up front the emphasis in the STL is on

1986
02:06:58,410 --> 02:07:03,020
speed in fact they are designed to be really really fast and that means that

1987
02:07:03,020 --> 02:07:07,750
there are notion of safety are good one good one so what this means is

1988
02:07:07,750 --> 02:07:09,660
that you have to derive bounds checking

1989
02:07:09,680 --> 02:07:12,680
when iterating you have to make sure you don't want fit your container

1990
02:07:12,700 --> 02:07:14,150
and stuff like that

1991
02:07:14,170 --> 02:07:17,480
and one of the major reasons we didn't go over the st on the classes

1992
02:07:17,480 --> 02:07:20,560
that very it's just that which is if you're trying to figure out how to

1993
02:07:20,560 --> 02:07:23,720
work with vector it was really bad if you also have to worry about doing

1994
02:07:23,870 --> 02:07:26,950
found striking if you're working the map and you have to use the st map

1995
02:07:26,950 --> 02:07:29,580
you probably will never want to use another map for the rest of your life

1996
02:07:29,580 --> 02:07:31,070
it's pretty complicated

1997
02:07:31,110 --> 02:07:34,070
but the point is that this is what you'll see in a professional environment because

1998
02:07:34,070 --> 02:07:37,530
it's a very good library in very

1999
02:07:37,570 --> 02:07:40,810
i want to give you a quick example of what this might look like

2000
02:07:40,820 --> 02:07:44,500
so this is a program written using the CS one is expected to undergo changes

2001
02:07:44,500 --> 02:07:47,420
you have failed to show you it's the same thing with the will of the

2002
02:07:47,420 --> 02:07:48,880
difference in fact

2003
02:07:49,110 --> 02:07:53,420
the first thing to do is say well respected at age we gave you you

2004
02:07:53,420 --> 02:07:55,380
have to change that the standard vector

2005
02:07:55,390 --> 02:07:57,360
which is a vector in brackets

2006
02:07:57,370 --> 02:08:01,400
OK let's see number characters we changed over three characters

2007
02:08:01,460 --> 02:08:04,000
next thing the big vector becomes vector OK

2008
02:08:04,010 --> 02:08:06,230
four characters

2009
02:08:06,280 --> 02:08:10,110
the only really big differences this as a function of everyone to guess who is

2010
02:08:10,110 --> 02:08:11,700
in in my class

2011
02:08:11,710 --> 02:08:15,660
what you call the function to append something to the end of the vector

2012
02:08:15,720 --> 02:08:18,540
or take a guess

2013
02:08:19,480 --> 02:08:22,900
that would be nice it's actually called back

2014
02:08:22,960 --> 02:08:27,290
there is actually good reason for this means that all these different cannot have similar

2015
02:08:27,290 --> 02:08:28,370
function name

2016
02:08:28,390 --> 02:08:30,810
pushback and potato

2017
02:08:32,400 --> 02:08:36,370
OK anyway so that's probably about ten or eleven character changes

2018
02:08:36,380 --> 02:08:39,530
and to live over the thing is exactly what you've seen before

2019
02:08:39,600 --> 02:08:44,560
there is a side function there are bracketing go into a gun access things

2020
02:08:44,560 --> 02:08:47,580
the result is that you can see it's not that different

2021
02:08:47,600 --> 02:08:50,210
i mean there are couple subtleties that i didn't going to hear about how the

2022
02:08:50,210 --> 02:08:53,880
vector in the still behave differently from the vector you've seen but overall it is

2023
02:08:53,880 --> 02:08:57,810
the same content

2024
02:08:57,810 --> 02:09:01,110
the other one i want to call your attention to the film and so if

2025
02:09:01,110 --> 02:09:03,520
you work with the st all you need to know to containers and the rest

2026
02:09:03,520 --> 02:09:05,700
of all of you know that no map

2027
02:09:05,710 --> 02:09:07,740
you've got the whole thing down

2028
02:09:07,770 --> 02:09:11,620
yes your map is like our map itself instead of going string to some value

2029
02:09:11,620 --> 02:09:15,300
it's any key types you want to any value type one

2030
02:09:15,310 --> 02:09:18,590
so you can match strings to ends you can have in the strings you can

2031
02:09:18,590 --> 02:09:21,810
map stack of q of and two vector of double

2032
02:09:22,300 --> 02:09:25,480
i don't know why you would you had the ability to do so

2033
02:09:25,840 --> 02:09:30,300
the problem is that the interface on map was probably designed by someone who didn't

2034
02:09:30,300 --> 02:09:36,080
like people it's really hard to use on anyone who you will tell you that

2035
02:09:36,140 --> 02:09:39,150
what's the hang of it though it's pretty intuitive and the result is you have

2036
02:09:39,150 --> 02:09:43,600
a very fast container that rules can make it back any program you right it's

2037
02:09:43,600 --> 02:09:45,140
quite useful

2038
02:09:45,150 --> 02:09:48,070
so again i don't expect you to memorize every single bit of syntax here everything

2039
02:09:48,070 --> 02:09:51,390
i'm saying this has given back your mind is what you want to look into

2040
02:09:51,430 --> 02:09:53,970
the matters when you do need to know so could take a look at that

2041
02:09:53,980 --> 02:09:58,080
one pursue c plus plus

2042
02:09:58,100 --> 02:09:59,150
now iterators

2043
02:09:59,160 --> 02:10:03,140
so the interiors seen in this class are very nice very well behaved like warm

2044
02:10:03,140 --> 02:10:05,560
and fuzzy like oh you know has next

2045
02:10:05,570 --> 02:10:09,220
next hasnext next no no has and

2046
02:10:09,230 --> 02:10:12,090
i still generators are designed to look like pointers

2047
02:10:12,150 --> 02:10:15,590
now you might wonder why anyone voluntarily make something look like appointed the very good

2048
02:10:15,590 --> 02:10:19,420
reason what this means is that there are about is smart pointer which means they

2049
02:10:19,420 --> 02:10:20,530
know nothing

2050
02:10:20,570 --> 02:10:24,430
so if you want to iterate over range had have two different narrators states start

2051
02:10:25,260 --> 02:10:28,520
stop there people looking forward to you get

2052
02:10:28,540 --> 02:10:30,070
and admittedly

2053
02:10:30,090 --> 02:10:34,580
well it's more work but it's very useful because it means that if you want

2054
02:10:34,580 --> 02:10:38,650
to iterate over a twenty elements slice out of four hundred million on the container

2055
02:10:38,670 --> 02:10:41,370
you can do that try to another library if you have to go into all

2056
02:10:41,370 --> 02:10:42,570
the way to start

2057
02:10:42,580 --> 02:10:44,280
they keep iterating

2058
02:10:44,280 --> 02:10:47,730
the other thing you need to know about the iterators to the read right

2059
02:10:47,770 --> 02:10:51,230
so you can actually crawl over container and i think the values with the iterator

2060
02:10:51,570 --> 02:10:53,650
so you can do in our library

2061
02:10:53,670 --> 02:10:57,540
it's a bit more work the syntax basically looks like point reading and writing the

2062
02:10:57,550 --> 02:11:02,110
result is that there are much more powerful and a lot more flexible

2063
02:11:02,160 --> 02:11:04,780
so here's a quick example

2064
02:11:04,820 --> 02:11:07,950
i just wrote some code you user factor the thing take a look at this

2065
02:11:08,040 --> 02:11:09,270
right here

2066
02:11:11,040 --> 02:11:14,090
so basically there's a couple parts to point out i just want to show you

2067
02:11:14,090 --> 02:11:18,830
this you see this later on located get in everything

2068
02:11:18,890 --> 02:11:21,070
the first thing is that this big in function

2069
02:11:21,070 --> 02:11:25,000
we call into writer they call it began to get me started writer

2070
02:11:25,040 --> 02:11:28,740
we need to introduce fine range we keep going to look at the called and

2071
02:11:28,740 --> 02:11:30,470
which means stop

2072
02:11:30,480 --> 02:11:35,280
meera forward you know hasnext next nights with plus plus

2073
02:11:35,330 --> 02:11:38,750
and then to read from a narrator right to it it the reference

