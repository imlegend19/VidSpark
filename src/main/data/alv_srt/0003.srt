1
00:00:00,000 --> 00:00:05,690
so imagine for example that removing different edges

2
00:00:05,710 --> 00:00:09,300
i love the space of you know taking a given edge and removing it partitions

3
00:00:09,300 --> 00:00:13,340
it into two pieces and now i have another piece not remove et cetera my

4
00:00:13,340 --> 00:00:17,480
can end up getting a whole bunch of some problems that are similar in there

5
00:00:17,520 --> 00:00:19,590
i am

6
00:00:19,610 --> 00:00:23,210
OK you know if i take out this one then i take out say this

7
00:00:23,210 --> 00:00:24,790
one here

8
00:00:24,800 --> 00:00:27,920
and then i'll have another tree here here

9
00:00:27,960 --> 00:00:30,000
OK that would be

10
00:00:30,070 --> 00:00:33,020
that would be the same as if i had originally taken this out and then

11
00:00:33,020 --> 00:00:35,050
taken that one out

12
00:00:35,110 --> 00:00:38,920
so if by look at some wearing of taking out the the edges and we

13
00:00:38,920 --> 00:00:42,210
end up with a whole bunch of overlapping subproblems

14
00:00:46,050 --> 00:00:51,670
so what does that suggest we use as an approach

15
00:00:51,690 --> 00:00:54,590
dynamic programming

16
00:00:54,590 --> 00:00:57,770
what a surprise

17
00:01:05,710 --> 00:01:12,670
yes you could use the temporary but it turns out

18
00:01:12,690 --> 00:01:15,480
minimum spanning tree exhibits

19
00:01:15,510 --> 00:01:22,110
and even more powerful properties

20
00:01:22,130 --> 00:01:35,650
OK so dynamic so we got all the clues for dynamic programming

21
00:01:35,670 --> 00:01:38,190
it turns out there's even a bigger club

22
00:01:38,230 --> 00:01:42,360
it's going to help us to use an even more powerful technique

23
00:01:44,130 --> 00:01:50,130
and that

24
00:01:50,150 --> 00:01:52,420
we call the whole

25
00:01:52,420 --> 00:01:55,710
four greedy algorithms

26
00:01:55,710 --> 00:02:16,500
that is we have a thing called the greedy

27
00:02:22,300 --> 00:02:28,570
which says a locally optimal

28
00:02:33,630 --> 00:02:39,210
choice is globally after all

29
00:02:43,730 --> 00:02:47,690
cause all these hallmarks

30
00:02:47,730 --> 00:02:50,880
the kind of thing one boxes

31
00:02:50,940 --> 00:02:53,420
take these are the clues that you're going to be able to do do so

32
00:02:53,420 --> 00:02:55,480
we have this property

33
00:02:55,520 --> 00:02:57,170
we call them

34
00:02:57,190 --> 00:03:00,840
greedy choice problem show you how it works in this case

35
00:03:00,860 --> 00:03:04,460
we haven't really choice property it turns out you can

36
00:03:04,480 --> 00:03:08,110
do even better than dynamic programming

37
00:03:09,170 --> 00:03:14,840
so when you see the two dynamic programming properties

38
00:03:14,860 --> 00:03:18,550
is a clue that says dynamic programming yes

39
00:03:18,570 --> 00:03:21,460
but also it says let me see whether

40
00:03:21,460 --> 00:03:24,840
it also has this greedy property because if it does you going to come up

41
00:03:24,840 --> 00:03:27,790
with something is even better than dynamic programming

42
00:03:29,670 --> 00:03:34,070
so if you just have the two you can usually do dynamic programming if you

43
00:03:34,070 --> 00:03:35,270
only have

44
00:03:35,300 --> 00:03:37,770
but if you have this third one is like whoa

45
00:03:40,650 --> 00:03:44,030
so here's the theorem and proof

46
00:03:44,050 --> 00:03:47,900
to illustrate this idea

47
00:03:47,930 --> 00:03:49,670
c and these are not

48
00:03:49,730 --> 00:03:53,300
no these hallmarks are not things there's heuristics there

49
00:03:53,320 --> 00:03:57,250
you know they're not i can give an algorithm to say here's where dynamic programming

50
00:03:57,250 --> 00:04:00,190
work so here's where greedy algorithms work

51
00:04:00,250 --> 00:04:04,300
but i can certainly indicate when they work the kind of structure they have

52
00:04:05,420 --> 00:04:09,380
so here's the first term so let's let t

53
00:04:09,400 --> 00:04:11,130
the the MST

54
00:04:11,150 --> 00:04:17,130
of course paragraph

55
00:04:17,250 --> 00:04:25,400
and let's let a be any subset of the

56
00:04:25,420 --> 00:04:30,190
some subset of vertices

57
00:04:30,210 --> 00:04:32,630
and now let's suppose

58
00:04:32,650 --> 00:04:36,420
that is you

59
00:04:36,440 --> 00:04:39,860
he is

60
00:04:39,860 --> 00:04:46,020
is the least weight edge

61
00:04:52,940 --> 00:04:59,130
are set a two

62
00:04:59,130 --> 00:05:05,000
complement that has been my essay

63
00:05:08,360 --> 00:05:10,340
and the theorem says

64
00:05:13,000 --> 00:05:18,710
easy and the minimum spanning tree

65
00:05:19,840 --> 00:05:21,860
so let's just take a look at our

66
00:05:21,880 --> 00:05:26,230
graph over here and see if that in fact the case

67
00:05:26,900 --> 00:05:28,730
so let's take

68
00:05:28,730 --> 00:05:33,880
so one thing i could do for a is just take a singleton out

69
00:05:33,900 --> 00:05:37,230
so i think a singleton node let's say this guy here

70
00:05:37,250 --> 00:05:41,500
that can be my a and everything else is b minus a and i look

71
00:05:41,500 --> 00:05:44,070
at the least weight edge

72
00:05:44,110 --> 00:05:48,230
connecting this to everything else but there are only two edges that connected to everything

73
00:05:49,590 --> 00:05:54,650
theorem says that the lighter one is in the minimum spanning tree

74
00:05:56,190 --> 00:05:58,000
if you take a look at every

75
00:05:58,070 --> 00:06:01,920
vertex that i picked the lightest edge coming out of that vertex

76
00:06:01,960 --> 00:06:06,650
is in the minimum spanning tree

77
00:06:07,570 --> 00:06:12,340
the lightest weight vertex edge coming up but that's not all the edges that are

78
00:06:12,340 --> 00:06:16,500
in here

79
00:06:17,770 --> 00:06:24,250
or let's just say imagine let's let's take a look at these three vertices connected

80
00:06:24,270 --> 00:06:25,000
to to

81
00:06:25,090 --> 00:06:30,210
this set of vertices three edges going across the least weight one is five that's

82
00:06:30,210 --> 00:06:32,440
the minimum spanning tree

83
00:06:32,460 --> 00:06:35,840
OK i can cut it this way

84
00:06:35,880 --> 00:06:38,020
OK the ones above

85
00:06:38,050 --> 00:06:44,340
so the it is going down seven eight fourteen seven is the

86
00:06:44,400 --> 00:06:48,960
please wait it's in the minimum spanning tree

87
00:06:49,800 --> 00:06:53,790
so no matter how i choose i could make you know this one and this

88
00:06:53,790 --> 00:06:56,400
one out this one and this one out this one and this one this one

89
00:06:56,420 --> 00:06:57,520
and this one out

90
00:06:57,530 --> 00:07:01,340
take a look at all the edges are whichever one the least weight it's in

91
00:07:01,340 --> 00:07:04,520
the minimum spanning tree

92
00:07:07,300 --> 00:07:12,030
in some sense that's a local property

93
00:07:12,090 --> 00:07:14,800
so we have to look at what the rest of the tree is i'm just

94
00:07:14,800 --> 00:07:19,230
looking at some small set of vertices if i wish

95
00:07:19,270 --> 00:07:20,320
i say well

96
00:07:20,360 --> 00:07:23,750
i wanted to connect that set of vertices are as the world

97
00:07:23,750 --> 00:07:24,860
and so

98
00:07:24,880 --> 00:07:29,520
along all these patterns with the same number of black nodes and therefore every

99
00:07:29,530 --> 00:07:32,100
leaf will have the same that

100
00:07:32,130 --> 00:07:34,900
let me write down some of these properties

101
00:07:34,920 --> 00:07:38,210
so every

102
00:07:38,210 --> 00:07:40,300
in general

103
00:07:40,380 --> 00:07:46,430
has between two and four

104
00:07:53,000 --> 00:07:56,560
everybody has the same data

105
00:08:07,650 --> 00:08:09,920
the black eyed of the room

106
00:08:20,180 --> 00:08:22,110
this is by property property for

107
00:08:27,730 --> 00:08:29,520
this is telling us alive

108
00:08:29,570 --> 00:08:33,570
so essentially what this transformation is doing is ignoring the red nodes

109
00:08:33,590 --> 00:08:37,500
and then you just focus on the black nodes height equals black i

110
00:08:37,540 --> 00:08:38,430
and then

111
00:08:38,430 --> 00:08:42,030
like i was telling us that all the root to leaf pads

112
00:08:42,060 --> 00:08:43,510
i have the same

113
00:08:44,550 --> 00:08:47,830
for all these nodes are at the same level

114
00:08:47,850 --> 00:08:50,580
having leaves at the same levels of good things

115
00:08:50,600 --> 00:08:55,560
the museo trees pretty much balanced tree where all the nodes are branching

116
00:08:55,580 --> 00:08:58,530
so they all have at least two children

117
00:08:58,550 --> 00:09:01,040
and all these are at the same level

118
00:09:01,060 --> 00:09:03,040
that's pretty balanced cable proved

119
00:09:03,060 --> 00:09:05,680
some form that now

120
00:09:05,680 --> 00:09:08,480
i'm going to call

121
00:09:08,480 --> 00:09:10,230
the height of the tree

122
00:09:10,240 --> 00:09:11,840
h prime

123
00:09:11,870 --> 00:09:14,220
the height of the original tree

124
00:09:14,450 --> 00:09:16,080
this page

125
00:09:16,100 --> 00:09:17,560
that's what we want to bound

126
00:09:20,210 --> 00:09:23,790
so the first thing is to h prime and then we want to relate h

127
00:09:23,790 --> 00:09:26,930
and h right

128
00:09:31,210 --> 00:09:34,520
the first question is how many leaves are there

129
00:09:34,530 --> 00:09:38,790
in this tree and it doesn't really matter which stream looking at because i didn't

130
00:09:38,880 --> 00:09:43,030
do anything all these are black so leaves to change

131
00:09:43,060 --> 00:09:47,300
how many leaves other industry and therefore industry

132
00:09:47,320 --> 00:09:49,090
it's a

133
00:09:49,110 --> 00:09:52,800
nine into her nine but i mean in general search

134
00:09:52,820 --> 00:09:54,810
in this example there are nine

135
00:09:54,830 --> 00:09:57,840
how many keys are there

136
00:09:59,150 --> 00:10:00,600
so in general

137
00:10:00,620 --> 00:10:04,780
the right nine is the function of eight

138
00:10:04,810 --> 00:10:09,200
for large values of nine

139
00:10:09,220 --> 00:10:10,610
or it

140
00:10:13,980 --> 00:10:18,970
plus one good correct answer by guessing

141
00:10:19,060 --> 00:10:21,000
and was one

142
00:10:21,000 --> 00:10:23,030
and plus one

143
00:10:23,050 --> 00:10:26,580
because whenever we have let's look at the binary tree case where we sort of

144
00:10:26,590 --> 00:10:29,020
understand what's going on

145
00:10:29,060 --> 00:10:31,430
so where you have the key

146
00:10:31,430 --> 00:10:32,530
there are two

147
00:10:35,180 --> 00:10:38,460
that's not a very good argument but

148
00:10:38,490 --> 00:10:43,420
we have what he what is here called a branching binary tree every internal node

149
00:10:43,420 --> 00:10:46,040
has exactly two children

150
00:10:46,060 --> 00:10:49,870
and we're counting the number of leaves that you get from that process in terms

151
00:10:49,870 --> 00:10:52,880
of the number of internal nodes the number of leaves in the tree

152
00:10:53,040 --> 00:10:56,740
branching tree is always one plus the number internal nodes

153
00:10:56,790 --> 00:10:58,050
you should know that

154
00:10:58,060 --> 00:11:00,480
you can prove it by induction

155
00:11:00,530 --> 00:11:03,490
so the number of these is and plus-one one doesn't hold if you have a

156
00:11:03,490 --> 00:11:04,870
single child

157
00:11:04,890 --> 00:11:07,990
of every internal node has

158
00:11:08,050 --> 00:11:12,490
french infrastructure

159
00:11:12,550 --> 00:11:15,680
OK this is an archive three

160
00:11:17,380 --> 00:11:21,850
and now we want to pull out some relation between the number of leaves and

161
00:11:21,860 --> 00:11:24,300
the height of the tree

162
00:11:24,310 --> 00:11:26,010
so what's the good relations

163
00:11:26,010 --> 00:11:27,390
to use here

164
00:11:27,450 --> 00:11:31,290
we know exactly how many leaves are are that will somehow connected to an

165
00:11:31,310 --> 00:11:33,930
what we care about is the height in this look at the height of the

166
00:11:37,170 --> 00:11:38,590
so if i have

167
00:11:38,610 --> 00:11:40,070
a two three four tree

168
00:11:40,220 --> 00:11:41,970
hi h prime

169
00:11:41,970 --> 00:11:45,010
how many leaves could it have

170
00:11:45,040 --> 00:11:49,710
what's the minimum and maximum number of these it could have

171
00:11:49,740 --> 00:11:52,510
two to the age to forty HRH crown

172
00:11:52,520 --> 00:11:54,420
so we also know

173
00:11:54,510 --> 00:11:57,280
in the two three four tree

174
00:12:01,000 --> 00:12:02,650
it has to be

175
00:12:02,670 --> 00:12:07,800
between forty eight times as the most i can branch four ways in each node

176
00:12:07,830 --> 00:12:10,210
and it's it lists

177
00:12:10,210 --> 00:12:15,060
to the age prime because i know that every node branches at least two ways

178
00:12:15,080 --> 00:12:18,330
that's the key

179
00:12:18,340 --> 00:12:20,420
so i only care about one of these

180
00:12:20,420 --> 00:12:23,430
i think this one

181
00:12:23,480 --> 00:12:26,920
so i get that to the age prime

182
00:12:26,940 --> 00:12:28,020
is it most

183
00:12:28,030 --> 00:12:29,070
and plus one

184
00:12:29,090 --> 00:12:32,560
the number of these is and was one we know that exactly

185
00:12:32,580 --> 00:12:37,290
so we rewrite the take logs about science so h one

186
00:12:37,300 --> 00:12:42,270
is it must log and so on

187
00:12:42,270 --> 00:12:46,670
so we have a nice balance tree should be into it if i had every

188
00:12:46,670 --> 00:12:48,530
node branching two ways

189
00:12:48,590 --> 00:12:52,480
and all the leaves at the same level that's the perfect tree should be exactly

190
00:12:52,510 --> 00:12:56,050
log base two of n plus one it turns out not quite an

191
00:12:56,110 --> 00:12:59,360
that should be the height of the tree here i might have even more branching

192
00:12:59,360 --> 00:13:00,120
which is

193
00:13:00,140 --> 00:13:02,920
making things even shallower in some sense

194
00:13:02,920 --> 00:13:06,800
so i get more leaves at the same height but that's only better for me

195
00:13:06,800 --> 00:13:09,760
that will only decrease the height in terms of the number of these

196
00:13:09,780 --> 00:13:11,260
and plus one here is that

197
00:13:11,270 --> 00:13:12,470
the number of leaves

198
00:13:12,470 --> 00:13:16,300
why does it do that

199
00:13:24,020 --> 00:13:27,280
why does not immediately after

200
00:13:27,290 --> 00:13:29,440
it touches

201
00:13:29,540 --> 00:13:33,220
the board why does it immediately repel

202
00:14:03,900 --> 00:14:07,080
o thing

203
00:14:07,140 --> 00:14:11,490
that's interesting isn't it

204
00:14:11,580 --> 00:14:13,850
i can see the roles

205
00:14:13,900 --> 00:14:15,790
would very much also

206
00:14:17,270 --> 00:14:18,770
volume on

207
00:14:19,130 --> 00:14:23,880
and i would call it very well the should but don't you stand up front

208
00:14:23,920 --> 00:14:25,120
i will

209
00:14:25,210 --> 00:14:29,650
the nice for you don't know

210
00:14:29,840 --> 00:14:32,480
just just stand here come here

211
00:14:32,510 --> 00:14:36,590
look at all

212
00:14:51,000 --> 00:15:01,730
boy i have all i forgot this one this is a real big

213
00:15:01,780 --> 00:15:04,390
that's a nice one

214
00:15:04,390 --> 00:15:06,940
if i wrote this one

215
00:15:06,980 --> 00:15:09,380
it's to do that

216
00:15:09,460 --> 00:15:15,510
i think it will stick to the blackboard

217
00:15:15,530 --> 00:15:21,770
who says it will to the blackboard come up professor model of all of what

218
00:15:21,830 --> 00:15:25,440
you say irreducible postings

219
00:15:25,630 --> 00:15:29,340
professor watson was a ballistic

220
00:15:29,380 --> 00:15:30,420
he says no

221
00:15:30,430 --> 00:15:31,520
o o

222
00:15:33,760 --> 00:15:35,900
well stay

223
00:15:35,900 --> 00:15:38,390
who says it will not stay on the blackboard

224
00:15:47,030 --> 00:15:48,740
why is that

225
00:15:48,750 --> 00:15:51,350
why doesn't this one stay in the others do

226
00:16:00,240 --> 00:16:03,270
that's right it is so much heavier than the others

227
00:16:03,290 --> 00:16:07,660
they have no problem with this one does that's all it is it wants to

228
00:16:07,660 --> 00:16:09,510
stick but it's too heavy

229
00:16:10,480 --> 00:16:13,670
i'm very careful

230
00:16:14,400 --> 00:16:16,880
like for the little or maybe

231
00:16:24,150 --> 00:16:25,880
so you see those who said it works

232
00:16:25,940 --> 00:16:27,240
take a right

233
00:16:27,250 --> 00:16:31,580
and those who said it's not going to stick rosa right

234
00:16:31,620 --> 00:16:36,220
obviously not professor at MIT could make that mistake so that's why they will three

235
00:16:42,140 --> 00:16:44,880
i now

236
00:16:44,890 --> 00:16:45,940
one two

237
00:16:45,950 --> 00:16:48,030
explore further

238
00:16:48,080 --> 00:16:52,340
the idea of conservation of charge

239
00:16:52,410 --> 00:16:54,440
and what i want to do

240
00:16:54,460 --> 00:16:56,460
it is i want to

241
00:16:56,510 --> 00:16:58,410
charge of

242
00:16:58,420 --> 00:17:00,130
the student

243
00:17:01,350 --> 00:17:03,560
beating students

244
00:17:03,580 --> 00:17:05,280
was capture

245
00:17:05,340 --> 00:17:10,890
and then

246
00:17:10,940 --> 00:17:15,440
the story will get a particular chart plus or minus whatever that is

247
00:17:15,490 --> 00:17:17,710
when i went to be here

248
00:17:17,720 --> 00:17:20,540
we'll get the opposite charge

249
00:17:20,550 --> 00:17:23,240
and then we will see

250
00:17:23,270 --> 00:17:28,420
whether we can demonstrate to you that when we touch each other that indeed

251
00:17:28,420 --> 00:17:29,740
charts from him

252
00:17:29,810 --> 00:17:33,960
a former goes to me

253
00:17:34,000 --> 00:17:35,700
i need to volunteer for that

254
00:17:35,740 --> 00:17:38,810
someone who is very brave

255
00:17:38,940 --> 00:17:41,130
i want to really a youngster

256
00:17:41,170 --> 00:17:42,100
i know you've been

257
00:17:42,120 --> 00:17:45,190
being a student of me for many years

258
00:17:45,210 --> 00:17:49,230
when i was a youngster you're so close what's your name

259
00:17:49,890 --> 00:17:52,290
alex alex

260
00:17:52,380 --> 00:17:53,660
i have a little nervous

261
00:17:53,780 --> 00:17:55,340
you should be

262
00:17:55,360 --> 00:18:02,130
now you see this is called the right

263
00:18:02,170 --> 00:18:04,560
not good not good don't take it off

264
00:18:06,370 --> 00:18:08,830
but we need something more synthetic

265
00:18:08,850 --> 00:18:11,210
and this is what you're going to wear

266
00:18:13,770 --> 00:18:18,110
we're going to put you on this tool which is very well insulated

267
00:18:18,110 --> 00:18:22,480
yes what you can always been used this is quantity

268
00:18:22,500 --> 00:18:26,470
what i'm saying is that if you have the same value here

269
00:18:30,310 --> 00:18:32,570
four different values of x x

270
00:18:32,620 --> 00:18:34,510
next it is also

271
00:18:34,570 --> 00:18:36,100
the point is that

272
00:18:36,140 --> 00:18:40,720
in general function of x a b and c

273
00:18:41,500 --> 00:18:42,480
i have

274
00:18:42,530 --> 00:18:44,780
an arbitrary freedom

275
00:18:44,790 --> 00:18:46,320
to assign

276
00:18:47,690 --> 00:18:49,200
any value

277
00:18:49,250 --> 00:18:51,060
all those

278
00:18:51,070 --> 00:18:53,720
i was and still have to tell you

279
00:18:53,770 --> 00:18:56,660
when you don't

280
00:19:00,980 --> 00:19:03,970
so basically what the graphical model is is

281
00:19:03,980 --> 00:19:04,990
the following

282
00:19:05,010 --> 00:19:10,860
if you given a set of conditional independence statements for the fact

283
00:19:10,900 --> 00:19:13,780
something of this kind

284
00:19:13,850 --> 00:19:15,440
i have been impact

285
00:19:15,490 --> 00:19:19,670
which is of the original object image or whatever it is

286
00:19:19,800 --> 00:19:21,790
representation probabilistic

287
00:19:21,850 --> 00:19:24,800
just settle off

288
00:19:24,830 --> 00:19:29,700
all of the bibles and then we happen to

289
00:19:29,700 --> 00:19:31,100
to know

290
00:19:31,110 --> 00:19:33,160
conditional independence bowl

291
00:19:35,650 --> 00:19:37,450
in biology we know

292
00:19:37,870 --> 00:19:39,620
but conditions

293
00:19:39,620 --> 00:19:44,230
and construct of condition

294
00:19:45,140 --> 00:19:47,770
with the graphical model is

295
00:19:48,960 --> 00:19:52,290
the family of probability distributions

296
00:19:52,300 --> 00:19:53,950
where those

297
00:19:53,960 --> 00:19:56,290
conditional independence statements

298
00:19:58,620 --> 00:20:00,440
that's what the graph

299
00:20:00,490 --> 00:20:06,620
so any probability distribution that set of conditions have been told is an element of

300
00:20:06,620 --> 00:20:10,970
agra graphical model of a graphical model is not a single probability distribution is a

301
00:20:10,970 --> 00:20:13,630
family of probability distributions that

302
00:20:13,670 --> 00:20:17,180
i respect those conditional independence

303
00:20:17,260 --> 00:20:22,210
so basically if you go to take the product distribution actually compute

304
00:20:22,440 --> 00:20:32,850
compute the probability and compute the probability assuming this position get the same number

305
00:20:32,850 --> 00:20:36,500
this is an important question is the basis of the

306
00:20:37,970 --> 00:20:40,680
i will give a complete characterisation of

307
00:20:40,790 --> 00:20:50,500
what the class conditional of distribution defined by a set of conditional independence statements

308
00:20:51,980 --> 00:20:55,100
remember the questions we want to address

309
00:20:58,020 --> 00:21:01,950
we wanted to estimate parameters of the model

310
00:21:01,980 --> 00:21:03,430
given that

311
00:21:03,480 --> 00:21:06,910
want to compute probabilities of particular outcomes

312
00:21:06,980 --> 00:21:13,500
find particularly interesting realizations for example that site now stands between the most likely

313
00:21:13,540 --> 00:21:18,710
realisation after five years another names are most like

314
00:21:18,770 --> 00:21:23,520
joint realization

315
00:21:23,580 --> 00:21:28,250
but you know not to manipulate the probabilistic model because we want to compute these

316
00:21:28,250 --> 00:21:32,000
things we want to questions we

317
00:21:32,060 --> 00:21:33,000
we need to

318
00:21:33,020 --> 00:21:35,500
twenty member where we start this

319
00:21:35,910 --> 00:21:39,540
quite the first lecture we saw what we we saw the

320
00:21:39,600 --> 00:21:43,180
there are about four interesting question to me one way

321
00:21:43,250 --> 00:21:47,180
but general probabilistic approaches model

322
00:21:47,180 --> 00:21:49,370
and i

323
00:21:49,450 --> 00:21:50,680
in order to us

324
00:21:50,680 --> 00:21:53,000
to answer this question to manipulate

325
00:21:53,020 --> 00:21:54,470
these probabilities

326
00:21:56,750 --> 00:22:02,750
and to manipulate the probability function you to exploit the structures existing as it exists

327
00:22:02,750 --> 00:22:03,640
in the problem

328
00:22:06,200 --> 00:22:09,160
and that's what we went

329
00:22:09,160 --> 00:22:12,140
so we need to find ways of computing efficiently

330
00:22:12,200 --> 00:22:13,060
in that

331
00:22:22,330 --> 00:22:23,910
yes will

332
00:22:23,930 --> 00:22:25,500
this national department

333
00:22:25,520 --> 00:22:26,770
but also more

334
00:22:26,790 --> 00:22:31,060
OK and essentially learning by using this make the criteria

335
00:22:31,160 --> 00:22:35,020
o thing and parameter from a sample from about this

336
00:22:35,140 --> 00:22:38,330
you an estimate

337
00:22:38,350 --> 00:22:41,430
and then we'll see maximum like this which

338
00:22:42,080 --> 00:22:45,230
common types

339
00:22:45,250 --> 00:22:46,970
you know the criteria five

340
00:22:46,980 --> 00:22:52,250
if you have nothing meaningful to model the criteria for choose the

341
00:22:52,470 --> 00:22:56,370
that's called in this to me

342
00:22:56,500 --> 00:23:02,600
a function that maps the data to ground

343
00:23:02,620 --> 00:23:03,910
so how does

344
00:23:03,930 --> 00:23:05,910
p of x look like OK

345
00:23:07,890 --> 00:23:09,600
let's look at these

346
00:23:10,500 --> 00:23:13,890
probability distribution p of x one x two and x three

347
00:23:13,910 --> 00:23:17,330
let's assume that these conditions depend on hold

348
00:23:19,290 --> 00:23:22,370
let's work out

349
00:23:22,390 --> 00:23:24,350
however these

350
00:23:24,410 --> 00:23:27,160
prior distribution

351
00:23:28,560 --> 00:23:30,710
if we assume that these hold

352
00:23:30,730 --> 00:23:32,930
so if this holds

353
00:23:32,950 --> 00:23:35,560
but we have

354
00:23:35,580 --> 00:23:39,330
well the first thing we write just the conditional distribution of x one x three

355
00:23:39,330 --> 00:23:42,580
given x two

356
00:23:42,620 --> 00:23:43,470
this is

357
00:23:43,480 --> 00:23:46,040
x one given x extra

358
00:23:46,060 --> 00:23:53,040
times p of three given extra this conditional independence

359
00:23:53,140 --> 00:23:56,310
remember the initial condition

360
00:23:56,430 --> 00:23:59,020
this condition depends is exactly

361
00:23:59,020 --> 00:24:03,630
what we're interested in is the marginal distribution of the state house where we have

362
00:24:03,630 --> 00:24:05,320
integrated out g

363
00:24:05,340 --> 00:24:09,340
so this is called the polya urn scheme so that's

364
00:24:09,350 --> 00:24:12,750
in in bayesian nonparametrics there's lots of

365
00:24:12,760 --> 00:24:17,130
there's lots of metaphore so we have heard about the stick breaking construction this is

366
00:24:17,130 --> 00:24:18,640
called the polya urn scheme

367
00:24:21,970 --> 00:24:25,740
the idea of this scheme is is as follows so imagine that we have and

368
00:24:26,630 --> 00:24:29,750
OK so is a really large so

369
00:24:29,770 --> 00:24:31,580
ball think of it as a ball

370
00:24:31,590 --> 00:24:34,840
and we start with no balls in the urn

371
00:24:37,790 --> 00:24:39,900
well we're going to do this

372
00:24:39,910 --> 00:24:42,010
the following so at each iteration

373
00:24:42,030 --> 00:24:43,770
we're going to have

374
00:24:43,820 --> 00:24:49,920
this two possible events happening with probability proportional to alpha

375
00:24:50,630 --> 00:24:53,580
we're going to draw

376
00:24:54,030 --> 00:24:55,540
theta and

377
00:24:55,570 --> 00:25:00,710
so at iteration and with probability proportional to alpha we're going to draw theta and

378
00:25:00,710 --> 00:25:03,470
from from a base distribution h

379
00:25:03,490 --> 00:25:09,100
and if we think of the fate has the value of the fetus as

380
00:25:09,110 --> 00:25:10,430
as the color

381
00:25:11,290 --> 00:25:12,750
and that so

382
00:25:12,750 --> 00:25:13,930
so this tells us

383
00:25:13,960 --> 00:25:15,210
the colour of the ball

384
00:25:15,220 --> 00:25:19,150
and what we're going to have a ball that colour into our

385
00:25:19,160 --> 00:25:24,950
so at the first iteration you and then with probability proportional to n minus one

386
00:25:25,790 --> 00:25:29,370
region today and pick up a ball at random

387
00:25:29,370 --> 00:25:32,410
and then we're going to

388
00:25:32,420 --> 00:25:36,660
we're gonna rock record the colour of of the ball and returned two balls of

389
00:25:36,660 --> 00:25:38,660
the same colour back into the

390
00:25:38,670 --> 00:25:41,130
so the first iteration

391
00:25:41,210 --> 00:25:44,730
and minus one is zero so with probability one

392
00:25:44,740 --> 00:25:47,300
was simply cannot draw

393
00:25:47,340 --> 00:25:48,840
theta one from h

394
00:25:48,860 --> 00:25:53,760
and put the ball that colour into the so now has exactly one ball and

395
00:25:53,760 --> 00:25:55,750
its colour is theta one

396
00:25:55,760 --> 00:26:01,430
the next iteration with probability alpha over

397
00:26:01,490 --> 00:26:04,820
alpha plus and minus one for over one plus

398
00:26:04,830 --> 00:26:06,090
i alpha

399
00:26:06,100 --> 00:26:08,610
we're going to do the same thing going to draw

400
00:26:08,650 --> 00:26:09,850
the colour from each

401
00:26:09,870 --> 00:26:11,750
and then put the ball of the column

402
00:26:11,750 --> 00:26:15,020
and today and with probability one over

403
00:26:15,030 --> 00:26:18,790
one class of work in the region to the and then pick up ball in

404
00:26:18,790 --> 00:26:19,670
this case

405
00:26:19,700 --> 00:26:21,520
well it will be

406
00:26:21,790 --> 00:26:24,300
the theta one ball

407
00:26:24,330 --> 00:26:27,990
and then we're gonna record its colour and put two balls of the same colour

408
00:26:27,990 --> 00:26:33,500
back into the and which is going to repeat this in infinite and the colours

409
00:26:33,500 --> 00:26:37,040
of theta one theta two and so forth that is

410
00:26:37,050 --> 00:26:40,970
record and when we find this scheme is going to be turns out to be

411
00:26:43,100 --> 00:26:47,730
a draw from theta one theta two and so on with g integrated

412
00:26:48,820 --> 00:26:53,000
the conditional distributions are as follows so theta and

413
00:26:54,130 --> 00:26:56,050
one two and minus one

414
00:26:56,070 --> 00:26:58,550
so the colour of the and

415
00:26:58,570 --> 00:27:04,290
given the colours of the first n minus one balls withdrawn is going to be

416
00:27:04,320 --> 00:27:08,630
is the distribution of theta and here the conditional distribution is going to be

417
00:27:08,640 --> 00:27:12,650
given by the following so with probability alpha over

418
00:27:12,760 --> 00:27:14,640
and minus one plus alpha

419
00:27:14,700 --> 00:27:18,220
and this is going to be drawn from h

420
00:27:18,260 --> 00:27:21,140
and with probability one over

421
00:27:21,150 --> 00:27:24,950
and minus one plus alpha theta and it's going to on exactly the same value

422
00:27:25,800 --> 00:27:28,330
the the i

423
00:27:30,070 --> 00:27:33,470
so we can think of this polymer and give some sort of a representative for

424
00:27:33,470 --> 00:27:38,640
the dirichlet process because each of these conditional distributions is

425
00:27:38,700 --> 00:27:41,450
it's kind of a finite mixture model

426
00:27:41,470 --> 00:27:47,180
basically and we can think of this as some sort of a finite projection of

427
00:27:47,180 --> 00:27:49,780
the infinite dimensional object

428
00:27:50,700 --> 00:27:51,780
which is g

429
00:27:51,790 --> 00:27:57,870
any questions

430
00:28:04,730 --> 00:28:06,530
simple construction

431
00:28:07,430 --> 00:28:12,310
the next process the next representation of the dirichlet process is called the chinese restaurant

432
00:28:12,310 --> 00:28:16,290
process so this is yet another metaphor for the dirichlet process

433
00:28:18,310 --> 00:28:21,660
the chinese restaurant process goes as follows so

434
00:28:21,690 --> 00:28:23,460
if we look at

435
00:28:23,480 --> 00:28:28,080
theta one two theta and the set of values drawn from this polya urn scheme

436
00:28:30,720 --> 00:28:32,540
they will take on

437
00:28:35,010 --> 00:28:36,630
this thing to values

438
00:28:36,690 --> 00:28:42,070
so let's label them to test by one two three test talking

439
00:28:42,070 --> 00:28:44,930
and that is the probability that

440
00:28:44,950 --> 00:28:46,170
any two

441
00:28:46,190 --> 00:28:50,060
of the three ties in this list will take on exactly the same value

442
00:28:50,060 --> 00:28:52,020
so what this is is that

443
00:28:52,060 --> 00:28:55,810
we can partition the state i want to an into

444
00:28:56,970 --> 00:29:00,520
in two k different clusters each cluster given by

445
00:29:00,560 --> 00:29:01,940
by a value

446
00:29:01,960 --> 00:29:04,790
from this fate as stylist

447
00:29:04,800 --> 00:29:06,510
so for example

448
00:29:09,780 --> 00:29:11,960
they tell one is

449
00:29:13,280 --> 00:29:15,530
and think that way

450
00:29:15,540 --> 00:29:17,040
the rule

451
00:29:17,090 --> 00:29:23,610
it actually is that they have all this

452
00:29:23,630 --> 00:29:29,350
you know so

453
00:29:29,370 --> 00:29:32,560
so this is that we can partition this into

454
00:29:32,580 --> 00:29:36,120
theta one thousand three

455
00:29:39,730 --> 00:29:47,950
so this is a partition of

456
00:29:48,010 --> 00:29:51,360
of the sequence into

457
00:29:51,370 --> 00:29:55,460
in two different fate which which take on the same value

458
00:29:55,530 --> 00:30:00,930
and the chinese restaurant process is basically the distribution over partitions

459
00:30:01,020 --> 00:30:03,800
that we can study this

460
00:30:04,010 --> 00:30:08,610
this process so the nice thing with the chinese restaurant processes that

461
00:30:08,630 --> 00:30:10,850
it splits out the

462
00:30:10,900 --> 00:30:12,070
this kind of

463
00:30:12,120 --> 00:30:15,230
two different effect what's going on here one is that

464
00:30:15,290 --> 00:30:20,770
one is the actual value of the test is simply drawn from the base distribution

465
00:30:21,450 --> 00:30:25,370
and the and the second one is the clustering structure on the state s and

466
00:30:25,370 --> 00:30:30,920
the chinese restaurant process counter tells us about how about the clustering structure without referring

467
00:30:30,940 --> 00:30:31,700
to the

468
00:30:31,740 --> 00:30:34,580
base distribution h itself

469
00:30:34,610 --> 00:30:41,460
and then the generative process for this chinese restaurant process is is as follows

470
00:30:41,520 --> 00:30:45,070
so the first customer comes in and sits at the first table

471
00:30:45,080 --> 00:30:47,790
customer one here sits at table number one

472
00:30:47,810 --> 00:30:50,090
and in subsequent customers

473
00:30:50,110 --> 00:30:51,680
so customer and

474
00:30:51,700 --> 00:30:53,420
we'll see that table k

475
00:30:53,430 --> 00:30:54,710
with probability

476
00:30:54,730 --> 00:31:01,200
and k over n minus one class of so is proportional to n k where

477
00:31:01,200 --> 00:31:03,140
and and even

478
00:31:03,160 --> 00:31:09,360
even it's in but sparse so for example this does work fuse fuse although

479
00:31:09,510 --> 00:31:16,610
strings of length eight and fourteen a inspection from its works for this matrix for

480
00:31:16,700 --> 00:31:21,760
twenty of the feature space is ten to fourteen to mention a few some efficient

481
00:31:21,760 --> 00:31:23,390
sparse data structure

482
00:31:23,410 --> 00:31:25,760
and if is

483
00:31:25,780 --> 00:31:30,110
so then you visit get speedup of up to factor and

484
00:31:30,120 --> 00:31:34,160
OK so

485
00:31:34,180 --> 00:31:35,530
you have to

486
00:31:35,550 --> 00:31:38,880
two computers have to be to be able to

487
00:31:39,370 --> 00:31:46,250
users w must be able to to access each dimension of the w using some

488
00:31:46,250 --> 00:31:47,240
some index

489
00:31:47,260 --> 00:31:51,920
so for example you have to be able to enumerate all these came as and

490
00:31:51,920 --> 00:31:52,740
you need

491
00:31:52,880 --> 00:31:57,230
and it's been need only future operations between able the two cities w two zero

492
00:31:57,240 --> 00:32:00,810
in the to be able to have to eat some

493
00:32:00,830 --> 00:32:06,540
scalar value to one component of w and you must be able to to look

494
00:32:06,540 --> 00:32:08,600
up one component of w

495
00:32:08,620 --> 00:32:12,870
so this this is what the operation this

496
00:32:13,250 --> 00:32:15,430
that has to be quite efficient if you want to do

497
00:32:15,430 --> 00:32:17,690
like the three large data sets

498
00:32:17,850 --> 00:32:22,630
and quite quite important this use storage

499
00:32:22,660 --> 00:32:26,810
so you should use an explicit

500
00:32:26,820 --> 00:32:32,260
explicit that if you're dimensionality is it's quite a a lot of this to w

501
00:32:32,260 --> 00:32:37,110
also them because in the of course of basically just

502
00:32:37,120 --> 00:32:39,880
just and this one then we look up

503
00:32:39,930 --> 00:32:42,760
you could use sort of area so

504
00:32:42,790 --> 00:32:49,030
which is something else possible it contains and the look of course is quite high

505
00:32:49,110 --> 00:32:50,930
because if someone

506
00:32:50,930 --> 00:32:57,010
and there but it's still quite efficient if you if you if you string length

507
00:32:57,030 --> 00:32:58,780
UK becomes too large

508
00:32:58,800 --> 00:33:00,690
you want to look up then you should

509
00:33:00,700 --> 00:33:07,510
use for example the suffix tries trees is so

510
00:33:07,580 --> 00:33:11,180
so this this table just summarizes the

511
00:33:11,280 --> 00:33:13,680
costs that exists in new

512
00:33:13,680 --> 00:33:17,490
and you do use one of these data structures

513
00:33:17,490 --> 00:33:24,390
and it looks like the suffix tree is slightly more

514
00:33:24,440 --> 00:33:26,540
what the like the fastest one

515
00:33:26,580 --> 00:33:31,450
because it just has a linear it's linear time in

516
00:33:31,480 --> 00:33:33,370
look out for

517
00:33:33,370 --> 00:33:34,530
for the

518
00:33:34,610 --> 00:33:40,240
all the possible and in one sequence however

519
00:33:40,250 --> 00:33:44,640
it has like it has a huge overhead in memory storage so at least forty

520
00:33:44,640 --> 00:33:48,510
unit is forty by to store four four one

521
00:33:48,570 --> 00:33:55,530
i came here to destroy this there's almost almost no

522
00:33:55,540 --> 00:34:02,360
i mean you need to have w so it's not not really big memory we

523
00:34:02,360 --> 00:34:09,370
had an explicit map sorted areas some more you need from some more memory but

524
00:34:09,370 --> 00:34:13,810
not not that much more so you can you can draw the conclusion that if

525
00:34:13,810 --> 00:34:18,880
you have like a very very small forbid that this explicit that this

526
00:34:18,890 --> 00:34:21,890
they very efficient for like for the

527
00:34:21,920 --> 00:34:23,070
for example

528
00:34:23,190 --> 00:34:27,480
if you have sort to recent work much better for

529
00:34:27,490 --> 00:34:30,430
large alphabets and you have

530
00:34:30,620 --> 00:34:37,450
if you have like really low high order and large format and should use suffix

531
00:34:37,510 --> 00:34:41,290
OK so

532
00:34:41,300 --> 00:34:46,280
so this is not the only applied to that a spokesman classifier applied a huge

533
00:34:46,280 --> 00:34:50,150
speedups using this i mean you can be really apply to support vector machines on

534
00:34:50,150 --> 00:34:53,770
the whole human genome sort six billion data points

535
00:34:54,680 --> 00:34:56,350
it is this vegetable problems

536
00:34:56,380 --> 00:35:00,930
the only problem is of course that if you have many many support vectors w

537
00:35:00,930 --> 00:35:02,970
may be extremely huge

538
00:35:04,730 --> 00:35:07,060
and this does the trick two

539
00:35:07,100 --> 00:35:13,520
decomposers w into into some some subparts and to do do with this but i

540
00:35:13,520 --> 00:35:14,680
will continue to here

541
00:35:15,270 --> 00:35:16,810
the same

542
00:35:16,820 --> 00:35:20,310
so it doesn't tell you what it doesn't take a whole new home can speed

543
00:35:20,310 --> 00:35:25,050
up the support vector machine training so this just another number of tricks which is

544
00:35:25,050 --> 00:35:27,700
what you can do that

545
00:35:29,360 --> 00:35:35,480
the new and usually self-supporting machine you have like a sum decomposition i'd like chunking

546
00:35:35,480 --> 00:35:37,960
SMO and

547
00:35:39,090 --> 00:35:40,260
caching is

548
00:35:42,010 --> 00:35:43,450
fishing is

549
00:35:43,460 --> 00:35:51,300
quickly becomes quickly feasible if the number of data points that you consider this

550
00:35:51,310 --> 00:35:55,580
it's like one million or larger because in you just cannot store the all these

551
00:35:56,360 --> 00:35:58,660
discussion anymore memory

552
00:35:58,700 --> 00:36:05,700
so it actually where this district he also help because you don't come crashing at

553
00:36:05,700 --> 00:36:06,840
all anymore

554
00:36:06,860 --> 00:36:13,020
you can you can grow quite i mean you can go a lot lot for

555
00:36:13,040 --> 00:36:20,100
the so basically the future sometime consented the ideas

556
00:36:20,180 --> 00:36:26,570
i mean in in the senate ranking SVM you have to compute you just basically

557
00:36:26,570 --> 00:36:34,820
choose select some working set or which optimizes the put the problems and the thing

558
00:36:34,820 --> 00:36:38,770
is that at each iteration you have to compute the output for all the training

559
00:36:39,930 --> 00:36:43,450
which is exactly where we could use the net trying to get the good news

560
00:36:43,450 --> 00:36:50,040
is that only a few of these variables change in optimisation so we don't need

561
00:36:50,040 --> 00:36:55,760
that many of these at operations to sort make sure to to this

562
00:36:55,770 --> 00:36:57,850
to some kind of knowledge

563
00:36:57,860 --> 00:36:59,300
OK so

564
00:36:59,310 --> 00:37:01,860
so these are the the

565
00:37:02,330 --> 00:37:06,480
the updates this sort of this you have to compute at each iteration does not

566
00:37:07,650 --> 00:37:12,810
does not efficient at all it's quite often you suggest that you start with an

567
00:37:12,810 --> 00:37:14,290
f of zero and just

568
00:37:14,300 --> 00:37:20,300
just update the vectors which which which changed in the working

569
00:37:22,970 --> 00:37:23,810
and now we

570
00:37:23,840 --> 00:37:31,720
all have to do is just use this to that approach basically compute the w

571
00:37:31,720 --> 00:37:33,470
on on the working set

572
00:37:34,820 --> 00:37:35,970
which is

573
00:37:36,000 --> 00:37:39,890
this this quantity and just

574
00:37:39,900 --> 00:37:41,460
just apply

575
00:37:41,470 --> 00:37:44,520
this technique and this this way we

576
00:37:44,630 --> 00:37:49,440
we we can we can we don't need the competition anymore

577
00:37:51,180 --> 00:37:58,060
get get a slightly modified SVM optimise and

578
00:37:58,070 --> 00:38:04,980
first of all of course still if the number of examples highland and the most

579
00:38:04,980 --> 00:38:11,590
draw a series of normal lines to this curve and start doing one-dimensional signal processing

580
00:38:11,610 --> 00:38:14,700
for feature detection along these lines are now

581
00:38:14,710 --> 00:38:19,350
this is very economical of course compared with the snakes methodology next methodology in principle

582
00:38:19,350 --> 00:38:23,410
you have to compute feature maps over the entire image which is the reason why

583
00:38:23,520 --> 00:38:25,550
the work here

584
00:38:25,560 --> 00:38:29,300
if you have a good hypothesis you can restrict the work you do to simply

585
00:38:29,300 --> 00:38:34,640
one-dimensional prior image processing along this series occurs we have these sampled normals to be

586
00:38:34,910 --> 00:38:39,890
computed directly from a particular configuration of the contour and now we do the search

587
00:38:39,890 --> 00:38:43,030
along those normal lines

588
00:38:43,050 --> 00:38:46,950
now one one interesting thing is that

589
00:38:47,000 --> 00:38:48,640
if i

590
00:38:48,720 --> 00:38:52,540
you don't have this this is a little bit like a regression problem isn't it

591
00:38:52,540 --> 00:38:55,070
i mean if i had a graph

592
00:38:55,200 --> 00:38:58,610
one of x in some data points and i was in order to minimize the

593
00:38:58,610 --> 00:39:02,580
sum of squares error to that that data points that there's a point where you

594
00:39:02,580 --> 00:39:03,620
can imagine

595
00:39:03,670 --> 00:39:05,900
doing something like what i'm going to have to do here

596
00:39:05,910 --> 00:39:11,350
with the parameters x of that graph y of y of x but what we

597
00:39:11,350 --> 00:39:14,960
have here is a little bit more difficult because it's a parametric curve in two

598
00:39:14,960 --> 00:39:18,040
dimensions rather than the graph so now

599
00:39:18,090 --> 00:39:23,830
when i search for features along these normals and i find the particular point intersect

600
00:39:23,890 --> 00:39:24,780
the normal

601
00:39:24,800 --> 00:39:30,040
i'm not really entitled to regard the feature as a two dimensional point in spacetime

602
00:39:30,040 --> 00:39:32,680
not entitled to say that

603
00:39:32,690 --> 00:39:34,130
this point here

604
00:39:34,140 --> 00:39:39,550
is being mapped to or drawn towards this point here what i'm entitled to say

605
00:39:39,550 --> 00:39:45,920
is that this is the section of the true curve corresponding to this

606
00:39:45,930 --> 00:39:50,720
section of the initial curve is likely to pass through this point

607
00:39:50,740 --> 00:39:54,550
so in other words i know a lot about the displacement of the curve along

608
00:39:54,550 --> 00:39:54,990
the normal

609
00:39:55,360 --> 00:39:58,390
but i should take that as being information

610
00:40:00,120 --> 00:40:04,420
the position of the the curve is perpendicular to the normal

611
00:40:04,440 --> 00:40:08,140
so this is this blue dot here should be taken as a sort of ten

612
00:40:08,170 --> 00:40:14,000
observation of the tangent to the curve not of a particular point in particular parameter

613
00:40:14,590 --> 00:40:18,530
on the curve so you know we're thinking about is that i start with the

614
00:40:18,540 --> 00:40:24,620
initial curve rx best and they get a new curve rx of s and

615
00:40:25,420 --> 00:40:30,290
can be some slippage of the parameterisation of the nuclear relatively ok we shouldn't assume

616
00:40:30,290 --> 00:40:35,940
that these normal lines joined corresponding parameter values that would be

617
00:40:40,670 --> 00:40:42,310
so what we have to do

618
00:40:43,130 --> 00:40:47,950
to make the least squares measures which are sensitive only to the

619
00:40:48,090 --> 00:40:49,920
displacement in the normal direction

620
00:40:49,940 --> 00:40:52,990
so instead of having to

621
00:40:53,010 --> 00:40:56,880
tempted to cover it up on the screen and of course that won't work instead

622
00:40:56,880 --> 00:40:59,530
of having a term which emits

623
00:41:01,480 --> 00:41:06,470
and investors which would be the normal kind of sum squared error term

624
00:41:06,490 --> 00:41:07,390
we put the

625
00:41:07,440 --> 00:41:11,720
dot product with the and as in the estimated normal curve

626
00:41:13,680 --> 00:41:18,590
that will ensure that the influence of this particular data point is only felt along

627
00:41:18,590 --> 00:41:20,130
the normal as it were not

628
00:41:23,820 --> 00:41:28,760
so now what we have is

629
00:41:28,770 --> 00:41:33,590
because of our best which is the initial estimate where we are and then we

630
00:41:33,590 --> 00:41:40,160
can think of having an observed curve which was however only observed at particular points

631
00:41:40,200 --> 00:41:42,530
as set by along that curve

632
00:41:42,550 --> 00:41:43,550
and now

633
00:41:43,560 --> 00:41:48,290
it's useful to define innovations which is the things we square to get those

634
00:41:48,300 --> 00:41:52,370
error terms the innovation is the difference between the

635
00:41:52,380 --> 00:41:54,080
the predicted position of

636
00:41:54,090 --> 00:42:00,540
the point on the curve and the corresponding position on the new feature curve but

637
00:42:00,660 --> 00:42:02,780
with its influence restricted to

638
00:42:02,830 --> 00:42:05,130
the normal direction

639
00:42:05,150 --> 00:42:10,790
and finally the way that we can absorb this into a least squares principle efficiently

640
00:42:10,790 --> 00:42:16,770
is to define an observation matrix h of s little age which is like the

641
00:42:17,170 --> 00:42:21,140
original observation matrix h of capital HFS that we had before

642
00:42:21,160 --> 00:42:22,900
but dotted with this

643
00:42:22,930 --> 00:42:28,530
normal vector so that it gets turned from HMS would be let's say two by

644
00:42:28,530 --> 00:42:33,500
six matrix for the fine space is that the age of this is going to

645
00:42:33,500 --> 00:42:40,460
be won by six matrix with its influence restricted just to that normal curve

646
00:42:40,570 --> 00:42:45,940
i think

647
00:42:45,960 --> 00:42:51,510
it's been a while while i covariance

648
00:42:53,360 --> 00:42:58,610
not yet i mean that that's coming that's coming but quite a few slides down

649
00:42:58,610 --> 00:43:03,420
the line so eventually yes we will be interested in estimating variances along these along

650
00:43:03,420 --> 00:43:05,490
these girls but for the moment is just

651
00:43:05,510 --> 00:43:08,990
a geometrical construction that is based on the initial k

652
00:43:09,000 --> 00:43:15,050
OK so finally we could make ourselves a recursive least squares algorithm that would crawl

653
00:43:15,050 --> 00:43:18,630
along the curve from one end to the other

654
00:43:18,650 --> 00:43:22,970
this algorithm is in your notes and

655
00:43:24,350 --> 00:43:29,300
a partial solution to this curve fitting problem in fact you can stop at any

656
00:43:29,300 --> 00:43:33,400
intermediate point which is the virtue of recursive solutions to least squares problems i can

657
00:43:33,400 --> 00:43:40,460
stop at some intermediate and and get what is my current best estimate

658
00:43:42,760 --> 00:43:49,580
the position of the newly fitted curve to the data that i just got

659
00:43:49,600 --> 00:43:51,390
and you see a each

660
00:43:51,440 --> 00:43:58,070
step as he step i what happens is that this as matrix is accumulated with

661
00:43:58,190 --> 00:44:03,740
a product of those two h vectors so what's happening here is that this will

662
00:44:03,740 --> 00:44:08,540
see later can be interpreted as information matrix and the information associated with the latest

663
00:44:08,540 --> 00:44:11,310
measurement along the normal is

664
00:44:11,320 --> 00:44:14,300
so this is getting more like you're variance idea

665
00:44:14,300 --> 00:44:15,830
it is a product

666
00:44:15,870 --> 00:44:18,110
of the h vector with itself

667
00:44:18,110 --> 00:44:25,690
information component into the total information sites that the point i along the the curve

668
00:44:25,690 --> 00:44:29,720
it will turn out of the total statistical information with accumulated is as i

669
00:44:29,720 --> 00:44:32,640
i would like to point in me

670
00:44:32,680 --> 00:44:34,690
so what you do it

671
00:44:34,850 --> 00:44:39,480
following dictionaries which are the first is

672
00:44:39,490 --> 00:44:43,520
one of the things that you

673
00:44:43,630 --> 00:44:46,330
so what is it for

674
00:44:47,100 --> 00:44:48,040
one of

675
00:44:48,290 --> 00:44:50,970
we have some day

676
00:44:51,020 --> 00:44:53,900
somebody is going to

677
00:44:54,130 --> 00:44:59,740
and what we want to find the location of the data on to the shore

678
00:44:59,830 --> 00:45:05,440
in is going proposition exists then it means that the dictionary is one of the

679
00:45:05,440 --> 00:45:07,350
key to the data

680
00:45:07,410 --> 00:45:09,600
so for instance for images

681
00:45:09,600 --> 00:45:12,050
people have been using for years these

682
00:45:12,060 --> 00:45:17,320
then wavelets have been struggling vol twenty years to find the best dictionary which is

683
00:45:17,320 --> 00:45:19,700
that the natural images

684
00:45:20,960 --> 00:45:23,780
there have been some of the works which instead of

685
00:45:23,810 --> 00:45:26,810
trying to find the dictionary by hand

686
00:45:26,810 --> 00:45:28,480
have tried to learn it

687
00:45:28,490 --> 00:45:31,210
and this is the kind of world that we follow today

688
00:45:32,910 --> 00:45:36,910
it turns out that this disease

689
00:45:36,920 --> 00:45:39,390
can be formulated as a large scale

690
00:45:39,410 --> 00:45:41,220
the problem

691
00:45:41,250 --> 00:45:43,470
and what it mean

692
00:45:43,490 --> 00:45:47,880
intended to add these efficient

693
00:45:48,080 --> 00:45:53,190
so that if you are able to do this if you can make some large

694
00:45:53,190 --> 00:45:55,940
scale image poses problems tractable

695
00:45:55,950 --> 00:45:58,670
and also it makes sense to

696
00:45:58,690 --> 00:46:02,860
values matrix factorisation programs such as negative decisions

697
00:46:02,920 --> 00:46:06,660
and the formulation of PCA

698
00:46:06,670 --> 00:46:09,530
so we start by describing the

699
00:46:09,550 --> 00:46:11,490
you know

700
00:46:11,500 --> 00:46:16,190
and he thought that come back to from the first division which was not all

701
00:46:16,190 --> 00:46:19,310
on the problem and image processing tasks

702
00:46:19,360 --> 00:46:21,500
that is if you be nice

703
00:46:21,500 --> 00:46:23,360
no the image y

704
00:46:23,360 --> 00:46:26,580
how can we recover the original image ix

705
00:46:26,660 --> 00:46:29,670
so i can give you a very simple algorithm

706
00:46:29,690 --> 00:46:32,920
we can do it in three days decide

707
00:46:32,970 --> 00:46:36,560
we almost state-of-the-art results

708
00:46:38,970 --> 00:46:44,160
here is what you consider image you extract all overlapping

709
00:46:44,190 --> 00:46:45,480
of the small size

710
00:46:45,500 --> 00:46:49,370
o point for instance by itself

711
00:46:49,380 --> 00:46:52,320
so that you have a large number of

712
00:46:52,340 --> 00:46:53,720
but she

713
00:46:53,780 --> 00:46:56,710
for instance if you have one mil mi eight

714
00:46:56,720 --> 00:47:00,660
you you have about one million of small but the one that and that if

715
00:47:02,160 --> 00:47:07,180
then you find a dictionary which is identical to the set of patches

716
00:47:07,190 --> 00:47:10,560
so we know know which is a metric the here

717
00:47:10,630 --> 00:47:17,250
which is composed of about a few number of it in one thousand two hundred

718
00:47:17,310 --> 00:47:23,630
you want to there the matter with all of your

719
00:47:23,760 --> 00:47:28,720
i want to i was born

720
00:47:28,840 --> 00:47:30,650
so the point here is the

721
00:47:30,880 --> 00:47:35,630
we are in the town the stuff the city where you have a single but

722
00:47:35,630 --> 00:47:41,160
laughs about the problem you have we we have here they should of small sparse

723
00:47:41,160 --> 00:47:42,220
coding problems

724
00:47:42,430 --> 00:47:47,880
so in a that so that when you take the feature we may want to

725
00:47:47,880 --> 00:47:52,220
do that again i will have between them to get the image of about twenty

726
00:47:53,870 --> 00:47:56,660
then you have to solve these

727
00:47:56,710 --> 00:47:57,910
but each

728
00:47:57,940 --> 00:48:04,410
batch you obtain approximation the alpha i so that each you as many teammates

729
00:48:04,440 --> 00:48:06,630
as such belongs to

730
00:48:06,650 --> 00:48:11,940
and so that you can just outside the city limits of ten find ticket image

731
00:48:12,030 --> 00:48:17,340
so here is an example of what you can get the same middle

732
00:48:17,350 --> 00:48:21,660
which is called the state of the art in image thing

733
00:48:21,690 --> 00:48:23,940
for image denoising this

734
00:48:23,940 --> 00:48:28,590
so on the left hand been those emails i'm going to talk about

735
00:48:28,600 --> 00:48:30,900
for the kind of applications images

736
00:48:30,910 --> 00:48:32,250
for instance if you

737
00:48:32,250 --> 00:48:34,840
that's the whole image i have to

738
00:48:34,840 --> 00:48:36,560
you can do image compression

739
00:48:36,570 --> 00:48:40,070
so here what we have done we have this image on the left leg

740
00:48:40,220 --> 00:48:42,780
we have no

741
00:48:42,810 --> 00:48:44,840
on the damaged image

742
00:48:44,880 --> 00:48:47,220
knowing where the pixels are missing

743
00:48:47,230 --> 00:48:49,510
and which are the said which

744
00:48:49,520 --> 00:48:50,530
the missing

745
00:48:50,650 --> 00:48:54,180
and we are able to pull

746
00:48:54,200 --> 00:48:59,120
well we are able to recover some textual the house which is even visible

747
00:48:59,800 --> 00:49:02,490
on the left

748
00:49:03,220 --> 00:49:04,870
if you want to know how the

749
00:49:04,900 --> 00:49:07,400
the looks like an example of

750
00:49:07,890 --> 00:49:10,050
from the nineteen h

751
00:49:10,070 --> 00:49:15,410
it has here to fifty six it was known the that on

752
00:49:15,430 --> 00:49:19,910
that's the size a a and i think there is not much more to say

753
00:49:19,910 --> 00:49:21,320
about this

754
00:49:21,800 --> 00:49:25,200
so let's come back to the formulation of the training that

755
00:49:25,210 --> 00:49:27,410
presented and give a little more detail

756
00:49:27,470 --> 00:49:29,340
so i what presented

757
00:49:30,390 --> 00:49:33,160
the c

758
00:49:33,370 --> 00:49:34,850
we just

759
00:49:36,010 --> 00:49:37,050
or you don't

760
00:49:37,120 --> 00:49:38,210
two d

761
00:49:38,220 --> 00:49:43,150
because if you look at the first is that he said

762
00:49:46,470 --> 00:49:49,870
he could be arbitrary not we don't realize it

763
00:49:50,030 --> 00:49:52,070
so that's what people usually do it just

764
00:49:52,090 --> 00:49:56,030
the whole trained number of the columns of the to be less than one

765
00:49:56,080 --> 00:50:00,520
so now we have here non nonconvex optimisation problem but

766
00:50:00,530 --> 00:50:02,260
if you fix it

767
00:50:02,280 --> 00:50:07,490
and you can make with the EU then the convex optimisation problem and should easy

768
00:50:07,510 --> 00:50:12,160
you obtain a large number of small convex optimisation one that can solve the classical

769
00:50:13,170 --> 00:50:16,240
just as a between the musicians on the

770
00:50:16,260 --> 00:50:21,600
and the musicians of alpha so he quality that was all but there is one

771
00:50:21,600 --> 00:50:22,400
this is

772
00:50:22,410 --> 00:50:25,030
the other is thinking

773
00:50:25,030 --> 00:50:29,440
and so you look at the first index i wear as you increase their one

774
00:50:29,440 --> 00:50:32,870
variable x this be minus eight times the column with

775
00:50:33,740 --> 00:50:34,470
be minus

776
00:50:34,910 --> 00:50:36,160
that's called matrix

777
00:50:37,850 --> 00:50:39,240
zero you pick out by

778
00:50:40,120 --> 00:50:41,630
and that's are leaving variable

779
00:50:42,900 --> 00:50:47,140
and now we just do algebraically originally equations that the variable x j on the

780
00:50:47,140 --> 00:50:49,300
left and the variable w i on the right

781
00:50:50,670 --> 00:50:55,750
and i will show you how to do this uh i mean show some examples

782
00:50:55,750 --> 00:50:57,170
in just a few seconds' here

783
00:50:58,410 --> 00:51:04,350
but it is trivial algebra it's just exactly like gas in elimination where you have

784
00:51:05,920 --> 00:51:10,800
standard tyre girl which we zeroing one column in used in your process

785
00:51:11,570 --> 00:51:13,600
inverting a matrix using gauss and elimination

786
00:51:14,260 --> 00:51:15,630
and you'll see what i mean in this

787
00:51:18,170 --> 00:51:21,400
we get this rearrangement now the y axis and the w is a kind kind

788
00:51:21,430 --> 00:51:23,160
mixed up and they get more and more mixed up

789
00:51:23,650 --> 00:51:28,240
as we do this several times and so in general the notation will be

790
00:51:29,640 --> 00:51:34,620
like this that i have some nonbasic variables which are denoted by x subscript and

791
00:51:35,230 --> 00:51:38,260
and that's the next so the original x variables and the and the and the

792
00:51:38,610 --> 00:51:41,050
original w variables they get all mixed up

793
00:51:42,200 --> 00:51:44,110
and our basic variables x be

794
00:51:44,720 --> 00:51:48,520
and that's what the rest of the variables that are in the x and

795
00:51:49,400 --> 00:51:56,890
can be a coefficients multiplayer it will change its orbital till period indicate that this the numbers here

796
00:51:57,300 --> 00:51:59,850
are different because we don't discuss the elimination type step

797
00:52:00,520 --> 00:52:02,010
which we call a pivot by the way

798
00:52:03,080 --> 00:52:06,240
can be in any also get change so i put all those on top

799
00:52:07,580 --> 00:52:12,220
come but all the variables are still not negative and this is a completely equivalent

800
00:52:12,220 --> 00:52:14,060
way of writing the problem is this

801
00:52:14,710 --> 00:52:19,390
and that's the whole point we go from this to this just by algebraic rearranging

802
00:52:19,390 --> 00:52:23,920
the equations but we could undo the rearrangement and go back from this back to

803
00:52:24,690 --> 00:52:27,420
so we have a much changed the problem with only

804
00:52:27,940 --> 00:52:29,410
represented in a different way

805
00:52:29,850 --> 00:52:30,540
we've taken

806
00:52:31,110 --> 00:52:34,150
are dictionary and are defining different variables are taking

807
00:52:34,990 --> 00:52:40,130
different variables are given fundamental variables and defining a different set variables x being

808
00:52:41,690 --> 00:52:46,610
and the winners are all there but as we do these iterations we get a constant appearing here

809
00:52:48,650 --> 00:52:51,290
is the current value of the objective function

810
00:52:52,090 --> 00:52:56,830
and not will go up and up because we're maximizing so we started zero and just are all

811
00:52:57,260 --> 00:52:58,830
go higher and higher as we iterate

812
00:53:00,510 --> 00:53:03,820
so you find on the internet i can actually demo this

813
00:53:07,520 --> 00:53:08,060
but i'm not

814
00:53:08,620 --> 00:53:10,680
but i also have a local version so

815
00:53:11,860 --> 00:53:13,160
let's get going bad

816
00:53:15,560 --> 00:53:16,330
demo here

817
00:53:18,420 --> 00:53:19,760
let me make sure it's feasible

818
00:53:20,370 --> 00:53:20,820
the problem

819
00:53:21,880 --> 00:53:24,390
tool here doesn't always generate feasible problems

820
00:53:25,450 --> 00:53:27,150
so there's a feasible problem right here

821
00:53:30,640 --> 00:53:32,250
so in this particular problem

822
00:53:33,630 --> 00:53:34,230
i have to

823
00:53:35,670 --> 00:53:40,360
objective coefficients are positive like a you one them as the entering variable you x two x form

824
00:53:44,730 --> 00:53:45,810
we have to make a decision so

825
00:53:46,240 --> 00:53:46,830
i'll pick x two

826
00:53:47,530 --> 00:53:49,710
i picked it because it has the larger coefficient but

827
00:53:50,330 --> 00:53:50,690
there's no

828
00:53:51,270 --> 00:53:53,110
guaranteed benefit that's just

829
00:53:53,990 --> 00:53:56,110
tempting so i'm tempted topic

830
00:53:57,520 --> 00:53:57,920
x two

831
00:53:59,120 --> 00:54:00,890
now as i looked down the column here so

832
00:54:01,850 --> 00:54:06,890
at all these variables are equal to zero in the dictionary solution associated with this dictionary

833
00:54:07,310 --> 00:54:09,670
and now thinking about improving the solution

834
00:54:10,760 --> 00:54:14,180
o one x two which is currently zero enter increase

835
00:54:14,800 --> 00:54:15,940
and is it increases

836
00:54:17,070 --> 00:54:22,360
this w one which is currently five starts to go up at a rate of two

837
00:54:24,540 --> 00:54:27,060
and so this one's gonna go up and i don't have to worry about this

838
00:54:27,230 --> 00:54:29,130
the w one equals zero however

839
00:54:30,380 --> 00:54:34,410
and similarly other ones that have a negative number inside this little box here and

840
00:54:34,750 --> 00:54:36,820
and the negative side there are going up so

841
00:54:37,240 --> 00:54:41,640
w one w three w five don't worry about but w two and w four

842
00:54:41,640 --> 00:54:44,340
morphologically complex nouns

843
00:54:44,430 --> 00:54:49,250
they are derived using the right kind of such suffix the verbal abstract noun producing

844
00:54:50,760 --> 00:54:54,700
is the class of suffixes including meant in ireland

845
00:54:59,660 --> 00:55:01,880
give semantics for those amounts

846
00:55:02,050 --> 00:55:07,620
and it's

847
00:55:07,660 --> 00:55:11,200
used in the derivation

848
00:55:11,250 --> 00:55:14,580
such a key

849
00:55:18,460 --> 00:55:19,780
this rule

850
00:55:19,960 --> 00:55:22,260
sensitive to weather

851
00:55:22,320 --> 00:55:23,890
a for

852
00:55:23,910 --> 00:55:26,830
as the verbsemtrans canonical solution

853
00:55:27,880 --> 00:55:32,070
it's as if they were not assertion that used

854
00:55:32,170 --> 00:55:34,680
to conclude

855
00:55:34,710 --> 00:55:37,920
the possessor is going to have the victims

856
00:55:39,040 --> 00:55:43,420
and we what i what we i want to expand the coverage of the

857
00:55:45,290 --> 00:55:48,040
in the future

858
00:55:48,040 --> 00:55:49,830
and there's lots more to do

859
00:55:49,910 --> 00:55:54,370
so i should stop talking and my there's ask questions but

860
00:55:55,130 --> 00:55:56,870
to summarize

861
00:55:56,880 --> 00:55:58,450
the cyc lexicon

862
00:55:59,500 --> 00:56:01,740
it it's represented in the cyc KB

863
00:56:01,750 --> 00:56:05,120
you have access to all the school ontological engineering stuff

864
00:56:05,130 --> 00:56:08,160
so we use a valueable predicates

865
00:56:08,210 --> 00:56:09,330
you can also have

866
00:56:09,330 --> 00:56:14,330
exceptions to the rules you can do it very very expressive

867
00:56:14,450 --> 00:56:16,750
means of representing lexicon

868
00:56:17,180 --> 00:56:21,250
and it's also generative so you can derive new

869
00:56:21,290 --> 00:56:22,740
lexical entries

870
00:56:22,750 --> 00:56:26,870
mappings between form and meaning

871
00:56:28,280 --> 00:56:30,260
inference rules

872
00:56:33,080 --> 00:56:35,630
it's kind of an interesting enterprise but

873
00:56:36,800 --> 00:56:42,490
innerstad copperhead forces you to come down on one side or another

874
00:56:42,490 --> 00:56:48,260
i don't think so

875
00:57:23,950 --> 00:57:26,120
one of the

876
00:58:57,310 --> 00:59:08,050
i don't know

877
00:59:11,160 --> 00:59:18,160
that can carry

878
00:59:18,160 --> 00:59:22,520
i mean i don't mind

879
00:59:37,080 --> 00:59:41,390
of course

880
00:59:54,970 --> 01:00:00,860
this area

881
01:00:01,070 --> 01:00:08,890
you know

882
01:00:11,870 --> 01:00:13,880
twenty years

883
01:00:13,890 --> 01:00:17,910
it's like

884
01:00:19,000 --> 01:00:20,790
o five

885
01:00:26,330 --> 01:00:29,690
don't want to find

886
01:00:29,860 --> 01:00:31,660
you know

887
01:00:51,720 --> 01:00:54,890
and you

888
01:01:04,680 --> 01:01:07,670
in the

889
01:01:14,750 --> 01:01:18,220
all known to

890
01:01:21,790 --> 01:01:26,210
here you and

891
01:01:29,610 --> 01:01:33,400
the rule

892
01:01:35,530 --> 01:01:37,360
four days

893
01:01:37,370 --> 01:01:38,550
x one

894
01:01:40,590 --> 01:01:45,300
and this one

895
01:01:45,320 --> 01:01:48,610
no of course

896
01:01:48,630 --> 01:01:51,740
no no

897
01:01:51,760 --> 01:01:56,570
the only

898
01:02:04,720 --> 01:02:09,780
the room one one

899
01:02:10,950 --> 01:02:13,760
you can see here

900
01:02:13,930 --> 01:02:18,340
very very very small

901
01:02:18,360 --> 01:02:23,030
o one

902
01:02:23,450 --> 01:02:29,840
he wrote was one of four

903
01:02:29,840 --> 01:02:40,980
this presentation is delivered by the stanford center for professional development

904
01:02:42,130 --> 01:02:49,760
so welcome back and what i want to do today is talk about his rap

905
01:02:49,770 --> 01:02:54,820
about discussion on factor analysis and in particular we want to do this

906
01:02:54,870 --> 01:02:56,170
step through

907
01:02:56,180 --> 01:03:00,380
on parts of the derivation for me and factor analysis because

908
01:03:00,430 --> 01:03:06,460
again there there few steps and in derivation they are particularly tricky and there specific

909
01:03:06,460 --> 01:03:12,460
mistakes people often make on deriving sounds like factor analysis so i want to show

910
01:03:12,460 --> 01:03:15,560
you how to do those that's right you can apply the same ideas of problems

911
01:03:15,560 --> 01:03:16,300
as well

912
01:03:16,310 --> 01:03:22,570
on and then in the second half also the structure of the principal component analysis

913
01:03:22,610 --> 01:03:28,130
which is on a very powerful out of four on the dimensionality reduction c that's

914
01:03:28,130 --> 01:03:29,700
what that is

915
01:03:30,450 --> 01:03:33,740
just to recap on

916
01:03:33,790 --> 01:03:39,560
in the previous lecture i just tried to the properties of gases distributions one was

917
01:03:39,560 --> 01:03:41,180
that of

918
01:03:41,200 --> 01:03:46,660
if you have random variable around the value back to x they can be partitioned

919
01:03:46,660 --> 01:03:48,830
into two portions x one x two

920
01:03:48,850 --> 01:03:50,940
on the axes

921
01:03:50,960 --> 01:03:54,540
now assume i mean and covariance sigma

922
01:03:55,510 --> 01:03:57,270
and you is

923
01:03:57,290 --> 01:04:03,400
itself the partition vector and sigma

924
01:04:03,440 --> 01:04:08,420
and so on

925
01:04:08,500 --> 01:04:15,650
so partition matrix that written like that this writing sink in terms of

926
01:04:15,660 --> 01:04:18,430
you know before some blocks

927
01:04:18,450 --> 01:04:23,190
then you can look at the distribution of facts and ask what is the marginal

928
01:04:24,840 --> 01:04:27,800
they have one

929
01:04:30,590 --> 01:04:35,820
the answer is we said last time was the x one the marginal distribution of

930
01:04:35,820 --> 01:04:42,770
x one calcium with mean mu and covariance sigma one sigma one one

931
01:04:42,800 --> 01:04:47,630
is the upper left block that covariance matrix sigma to this this was no surprise

932
01:04:47,740 --> 01:04:53,320
and also wrote down the formula for computing conditional distributions

933
01:04:53,340 --> 01:04:59,120
on the first one is given one given x two

934
01:04:59,130 --> 01:05:04,870
and last time i wrote down the distribution of x y given x which also

935
01:05:04,870 --> 01:05:06,380
be calcium

936
01:05:08,330 --> 01:05:14,320
the road has mu of one given to a sequence one given to

937
01:05:15,840 --> 01:05:19,200
we have one given to a

938
01:05:19,210 --> 01:05:21,490
this is just this one

939
01:05:42,550 --> 01:05:45,650
what these formulas will be able to look at

940
01:05:45,660 --> 01:05:51,730
on a pair of joint calcium random variables x one x example vectors and compute

941
01:05:51,730 --> 01:05:57,440
the marginal and the conditional distributions of the periods one of the for

942
01:05:58,210 --> 01:05:59,800
so i come back

943
01:05:59,820 --> 01:06:03,250
and derive the east that actually well

944
01:06:03,300 --> 01:06:06,810
o come back and use the marginal formula in the second and then when i

945
01:06:06,810 --> 01:06:08,260
come back and derive

946
01:06:08,280 --> 01:06:12,860
the e step in the lower of factor analysis

947
01:06:12,910 --> 01:06:15,370
well actually be using these the form of the game

948
01:06:15,510 --> 01:06:20,210
and again just to continue

949
01:06:20,220 --> 01:06:24,300
summarising what we did last time we said that

950
01:06:24,320 --> 01:06:28,700
no in factor analysis of

951
01:06:28,720 --> 01:06:31,070
our model

952
01:06:31,190 --> 01:06:35,530
see this is an unsupervised learning problem to give

953
01:06:35,540 --> 01:06:44,000
and unlabeled training set for each side is a vector in rn as usual

954
01:06:46,120 --> 01:06:49,250
we want to model

955
01:06:49,500 --> 01:06:51,050
the density of x

956
01:06:51,060 --> 01:06:52,380
and so on

957
01:06:52,400 --> 01:06:54,630
a model FOR acts OF that

958
01:06:54,660 --> 01:06:58,580
we imagine that the latent variables i generate

959
01:06:58,590 --> 01:07:05,770
well this this to zero mean and identity covariance as you will be some low

960
01:07:05,780 --> 01:07:07,400
dimensional thing

961
01:07:07,420 --> 01:07:12,470
and we want

962
01:07:12,570 --> 01:07:18,710
and we imagine the axis

963
01:07:18,720 --> 01:07:20,570
generated as mu plus

964
01:07:20,590 --> 01:07:25,820
lung disease was of ceylon where epsilon this accounts random variable with mean zero

965
01:07:25,830 --> 01:07:29,180
and the covariance matrix

966
01:07:29,190 --> 01:07:32,490
and so the parameters of the model

967
01:07:32,760 --> 01:07:34,600
are all

968
01:07:34,610 --> 01:07:37,700
which is an intentional vector

969
01:07:37,710 --> 01:07:42,170
london which is by dimensional vector

970
01:07:42,250 --> 01:07:45,590
the matrix and so which is

971
01:07:45,660 --> 01:07:49,600
and the diagonal

972
01:07:49,730 --> 01:07:54,310
is the diagonal and the diagonal on

973
01:07:54,360 --> 01:07:56,110
the matrix

974
01:07:57,090 --> 01:08:06,640
i guess the cartoon i drew lost time for factor analysis was set maybe that's

975
01:08:06,640 --> 01:08:13,470
the typical sample of data point xei if on this example i had this one

976
01:08:13,470 --> 01:08:17,800
so it's useful for analysis scale say ten thousand nodes

977
01:08:17,840 --> 01:08:21,150
twenty thousand fifty thousand nodes and say

978
01:08:21,260 --> 01:08:24,930
a billion is so communication events are two years is still open question actually how

979
01:08:24,930 --> 01:08:29,950
you do it was a hundred million something like that this question but in fact

980
01:08:29,950 --> 01:08:33,930
what accept the quadratic size structure is actually very fast way to update

981
01:08:34,050 --> 01:08:38,720
you simply process all communication events in one pass through the sorted sequence

982
01:08:38,810 --> 01:08:39,800
in time

983
01:08:39,810 --> 01:08:44,390
and with each communication of any simply update the vector clock city is let's consider

984
01:08:44,400 --> 01:08:45,550
time six

985
01:08:45,560 --> 01:08:47,300
the message is about to be sent

986
01:08:47,310 --> 01:08:49,580
here's the current clock uses

987
01:08:49,680 --> 01:08:53,100
so when the census message how we process is very simply

988
01:08:53,110 --> 01:08:54,550
with the message comes

989
01:08:54,560 --> 01:08:58,550
the vector clock OK so this was all of these most recent news and maybe

990
01:08:58,550 --> 01:09:01,240
that contains the potential for fresh information

991
01:09:01,260 --> 01:09:04,400
think that we just take the coordinate wise max

992
01:09:04,420 --> 01:09:08,810
of these two that right so in fact by communication see gets a more recent

993
01:09:08,810 --> 01:09:10,140
potential viewers

994
01:09:10,300 --> 01:09:14,780
we don't know what they are going on to have more potential to information certainly

995
01:09:14,780 --> 01:09:18,840
more view these so we take this corner max that season

996
01:09:18,890 --> 01:09:20,950
very very simple so over

997
01:09:21,040 --> 01:09:23,940
billions of communication events we simply run forward in time

998
01:09:23,950 --> 01:09:27,510
always max into factors to get maintains representation

999
01:09:27,600 --> 01:09:31,630
so you have to talk about today is of course interesting it's basically the amount

1000
01:09:31,630 --> 01:09:35,310
of potential information gain you just got the communication

1001
01:09:35,320 --> 01:09:39,630
OK so with lots of mind was something remind let's think about what this tells

1002
01:09:39,630 --> 01:09:41,340
us about our

1003
01:09:41,400 --> 01:09:46,560
also ten thousand faculty and staff at the university of change email or use so

1004
01:09:46,560 --> 01:09:47,730
the first is that

1005
01:09:47,780 --> 01:09:52,100
it says we should think of people as residing in these balls different radii

1006
01:09:53,570 --> 01:09:54,460
around this

1007
01:09:54,480 --> 01:09:56,780
o point called now right so here's node

1008
01:09:56,790 --> 01:10:01,680
is now use everyone within twelve hours current within twenty four hours current everyone out

1009
01:10:01,680 --> 01:10:04,890
of here they don't know what's happened to them that right there was no information

1010
01:10:04,890 --> 01:10:05,890
can flow

1011
01:10:05,930 --> 01:10:10,560
and the reason emails are proxy for the full set of communication events

1012
01:10:10,580 --> 01:10:13,010
but the more data one has the more

1013
01:10:13,020 --> 01:10:16,470
precisely one can do so the the ball of radius tower and simply all the

1014
01:10:16,530 --> 01:10:19,400
information rate is twenty

1015
01:10:19,410 --> 01:10:23,840
what we find is that both sides growing sort of piecewise exponential fashion so quickly

1016
01:10:23,840 --> 01:10:29,930
first slowing down a bit but so much faster than polynomial efficient compared to what

1017
01:10:30,090 --> 01:10:31,430
call randomized

1018
01:10:31,440 --> 01:10:35,840
baseline what kept the timestamp in the centre of each email saying

1019
01:10:35,900 --> 01:10:39,580
but we allowed to spread like up at random epidemic so the recipient is completely

1020
01:10:39,580 --> 01:10:43,780
random so i keep exactly the same trace data but every time you post any

1021
01:10:43,790 --> 01:10:47,030
message and said he really sad to send someone shows around so i break all

1022
01:10:47,040 --> 01:10:48,290
social structure

1023
01:10:48,310 --> 01:10:52,680
this going spread information much much faster and have spread of the spheres around the

1024
01:10:54,200 --> 01:10:56,270
we find actually that

1025
01:10:56,320 --> 01:10:59,310
after about a day and a half the random epidemic

1026
01:10:59,320 --> 01:11:04,410
and the actual real social communication or growing the ball size about and about the

1027
01:11:04,410 --> 01:11:08,680
same speed well actually distinguishes the epidemic interestingly is this first

1028
01:11:08,700 --> 01:11:13,090
thirty six hours start there's very specific to you know this but in general one

1029
01:11:13,090 --> 01:11:14,590
might expect some started

1030
01:11:14,680 --> 01:11:18,170
now let's take much much faster rates so

1031
01:11:18,190 --> 01:11:21,630
in some sense in the real social communication this set on average you had about

1032
01:11:21,630 --> 01:11:25,020
twelve people within a day and after the final decision

1033
01:11:25,060 --> 01:11:26,230
here in this

1034
01:11:26,260 --> 01:11:28,640
of all of thirty six hours only really know

1035
01:11:28,660 --> 01:11:30,940
for people with randomized epidemic

1036
01:11:30,950 --> 01:11:35,360
bunches and merely that bubble reaching fifty people and that's that's that's the case

1037
01:11:35,440 --> 01:11:40,940
there's also gives way to think about the strength of weak ties this idea that

1038
01:11:40,990 --> 01:11:43,380
chance encounters with

1039
01:11:43,430 --> 01:11:48,320
this is the queen's can convey what information so the article in which mark granovetter

1040
01:11:48,440 --> 01:11:49,640
nineteen seventies

1041
01:11:49,650 --> 01:11:53,400
this was to think about the social networks and think about your

1042
01:11:53,410 --> 01:11:57,980
close connections with which are embedded in triangles and long range connections and by range

1043
01:11:57,980 --> 01:11:59,400
here we simply mean what's

1044
01:11:59,420 --> 01:12:03,030
the length of the alternative second shortest path the set

1045
01:12:03,130 --> 01:12:07,010
so is french b and c are embedded in this triangle was with d

1046
01:12:07,020 --> 01:12:08,770
it was somehow

1047
01:12:08,790 --> 01:12:12,390
so structurally essential because otherwise you have to go forth to solve

1048
01:12:12,400 --> 01:12:17,420
and the reason is that these long-range local bridges is like a d

1049
01:12:17,430 --> 01:12:20,150
when you do happen to talk to them they give you access to parts of

1050
01:12:20,150 --> 01:12:23,820
the network the otherwise couldn't see and therefore you operation

1051
01:12:23,880 --> 01:12:25,440
but this is sort of

1052
01:12:25,490 --> 01:12:28,510
interesting hard theory to test quantitatively because

1053
01:12:28,520 --> 01:12:33,040
was i to minimalist most of the studies have been based on interviews with people

1054
01:12:33,140 --> 01:12:35,270
really worried about the job opportunities

1055
01:12:35,330 --> 01:12:37,930
here we can try to formalize in terms of

1056
01:12:38,030 --> 01:12:41,030
there are are with the vector clocks uses that

1057
01:12:41,930 --> 01:12:43,130
you meet somebody

1058
01:12:43,140 --> 01:12:46,340
who's that long range from you who maybe that's a very frequently

1059
01:12:46,350 --> 01:12:51,470
they give you this gigantic vector clock brain to operate so they you clock with

1060
01:12:51,470 --> 01:12:55,490
all sorts of coordinates that really lacking in your clock really current and there's and

1061
01:12:55,490 --> 01:13:00,240
you get this massive cloud and that we can test so we have here is

1062
01:13:00,240 --> 01:13:02,230
a function that range the

1063
01:13:02,260 --> 01:13:06,300
in average advanced per message in the vector clock when you talk to somebody that

1064
01:13:06,300 --> 01:13:09,820
range we see actually that although the very sort of try capturing want to get

1065
01:13:09,820 --> 01:13:11,350
out to translate like four

1066
01:13:11,360 --> 01:13:16,160
things are really actually significantly more informative per message in terms of the amount the

1067
01:13:16,170 --> 01:13:18,130
advancing and all the information is

1068
01:13:18,180 --> 01:13:21,580
so in some sense it is a kind of large-scale wanted

1069
01:13:21,640 --> 01:13:23,560
support for this

1070
01:13:23,570 --> 01:13:25,300
originally called the theory of

1071
01:13:25,310 --> 01:13:29,510
the strength of weak ties as mentioned that reach infinity means is an actual bridge

1072
01:13:29,520 --> 01:13:32,730
right the disconnect the graph et cetera there

1073
01:13:32,790 --> 01:13:35,920
this only shows up in our data on one end of that is typically incredibly

1074
01:13:36,280 --> 01:13:39,740
incredibly tiny components and so you actually to start advanced

1075
01:13:41,580 --> 01:13:43,700
we talk about the network back

1076
01:13:45,640 --> 01:13:48,150
that information is flowing

1077
01:13:48,150 --> 01:13:50,030
your what you think about it

1078
01:13:50,040 --> 01:13:51,460
and sent to this guy

1079
01:13:51,470 --> 01:13:53,610
OK only cares about

1080
01:13:53,620 --> 01:13:57,890
the preference essentially oversee these two variables right so it doesn't care about which he

1081
01:13:57,890 --> 01:13:59,110
shared with them and so

1082
01:13:59,160 --> 01:14:01,690
that's what happens here when you some out

1083
01:14:01,720 --> 01:14:03,450
everything else that

1084
01:14:03,460 --> 01:14:04,870
this guy doesn't

1085
01:14:04,890 --> 01:14:09,110
doesn't care about this is the message from btcc some out

1086
01:14:11,550 --> 01:14:13,520
that b

1087
01:14:13,550 --> 01:14:17,010
that he doesn't have so everything in

1088
01:14:21,020 --> 01:14:26,590
pretty really just essentially multiply in your potential with the messages

1089
01:14:26,600 --> 01:14:29,360
except for you from the the message from you

1090
01:14:29,380 --> 01:14:32,850
and then i marginalise out and send it to you

1091
01:14:32,900 --> 01:14:37,560
so whenever i never send you can have your own message multiplied

1092
01:14:40,470 --> 01:14:45,640
so this is the well defined because there's really no

1093
01:14:45,660 --> 01:14:49,910
i wouldn't say OK

1094
01:14:49,920 --> 01:14:54,480
OK OK i'm about to wrap up so this is this is

1095
01:14:54,530 --> 01:14:58,900
this is one of the few slides since he was the all

1096
01:14:59,620 --> 01:15:04,690
you can show this well defined because the trees singly connected you're you're not gonna

1097
01:15:04,920 --> 01:15:10,120
get kind of conflicting evidence everything sort of comes in from all my things downstream

1098
01:15:10,120 --> 01:15:15,120
to me in the past upstream there's no way but i passed come back to

1099
01:15:15,120 --> 01:15:16,140
some cycle

1100
01:15:16,160 --> 01:15:17,080
right so

1101
01:15:17,080 --> 01:15:22,620
if i decided on something that you know all my children like and i say

1102
01:15:22,620 --> 01:15:26,950
that my parents there

1103
01:15:27,270 --> 01:15:29,540
then we can all agree and there's not going to be some kind of backflow

1104
01:15:29,560 --> 01:15:32,350
communication everything is bottleneck for me

1105
01:15:33,500 --> 01:15:39,270
you dismiss propagation and to be this one would see in in centralized version of

1106
01:15:39,270 --> 01:15:47,410
an down and then you have all these messages precomputed and what you do

1107
01:15:47,430 --> 01:15:51,100
in the end to get a possibilities you take your own potential

1108
01:15:51,100 --> 01:15:56,580
you multiply by everybody now every all the also the opinions from other neighbours

1109
01:15:56,600 --> 01:16:00,250
and the renormalized

1110
01:16:00,270 --> 01:16:05,890
OK if it's not normalized it's it's the your local of kind of opinion in

1111
01:16:05,890 --> 01:16:11,080
the opinion of others expresses as messages about your variables

1112
01:16:11,100 --> 01:16:16,140
and so we can show this is proportional to the probability as you just need

1113
01:16:16,140 --> 01:16:17,960
to normalize

1114
01:16:19,640 --> 01:16:26,310
you had do this but so

1115
01:16:26,330 --> 01:16:31,620
you can basically show that that was the algorithm is doing is running variable elimination

1116
01:16:31,620 --> 01:16:35,870
in all directions at at the same time right so there is a relation that

1117
01:16:35,930 --> 01:16:37,460
kind of sums out

1118
01:16:38,270 --> 01:16:42,910
one neighbouring and passes everything else to this one in some that name address this

1119
01:16:42,910 --> 01:16:47,460
one so this doing is passing this

1120
01:16:47,770 --> 01:16:50,480
but this information to everybody same time so

1121
01:16:51,770 --> 01:16:52,460
but the

1122
01:16:52,480 --> 01:17:00,330
and this could explain this

1123
01:17:01,980 --> 01:17:06,930
so you can i mean you can prove it by looking very information and

1124
01:17:06,930 --> 01:17:08,640
and seeing that

1125
01:17:08,660 --> 01:17:13,580
that they do not serve pushing pushing some

1126
01:17:13,600 --> 01:17:17,160
two four meaning that you you take into account all the

1127
01:17:17,210 --> 01:17:19,270
all of them

1128
01:17:19,290 --> 01:17:20,850
evidence it had to do with

1129
01:17:20,870 --> 01:17:21,640
and so

1130
01:17:21,660 --> 01:17:30,930
but the running intersection property guarantees the clusters containing i constitute

1131
01:17:30,930 --> 01:17:34,830
a connected subgraph right so what that means is that

1132
01:17:34,850 --> 01:17:39,080
all of these if there is a variable that share its shared among these all

1133
01:17:39,080 --> 01:17:43,330
of these have so woman pasaran messages between each other

1134
01:17:44,000 --> 01:17:47,710
what can happen is that always need to agree on what

1135
01:17:47,730 --> 01:17:50,190
now the the marginal of that variable is

1136
01:17:58,870 --> 01:17:59,560
you can you

1137
01:17:59,580 --> 01:18:05,270
sure there's core part that has a variable y in the core and part basically

1138
01:18:05,270 --> 01:18:10,500
doesn't have it all and then you when you computing

1139
01:18:10,500 --> 01:18:14,000
this message this guy you're something out

1140
01:18:14,020 --> 01:18:17,870
sensually i the whole cluster that contains so

1141
01:18:17,870 --> 01:18:20,040
by connecting to the variable elimination

1142
01:18:20,060 --> 01:18:22,060
procedure you can show that that

1143
01:18:22,080 --> 01:18:26,790
you compute the same kind of the same kind of thing that very relational compute

1144
01:18:26,830 --> 01:18:28,080
she ran it

1145
01:18:28,080 --> 01:18:30,790
you know in the way that eliminated everything here

1146
01:18:30,810 --> 01:18:34,190
and then created a factor with this

1147
01:18:35,480 --> 01:18:37,230
OK so

1148
01:18:37,310 --> 01:18:42,770
and if it proves because it takes too long but just to a summary of

1149
01:18:42,770 --> 01:18:45,100
the algorithm is the following is

1150
01:18:45,160 --> 01:18:46,770
the coronation order

1151
01:18:46,790 --> 01:18:48,830
you get to cliques u

1152
01:18:48,850 --> 01:18:52,230
build the cluster graph by pruning out the maximal ones

1153
01:18:52,290 --> 01:18:55,640
we find maximum weight spanning tree which is cheap

1154
01:18:55,690 --> 01:19:00,540
and then you can start this density that's on the junction tree

1155
01:19:00,620 --> 01:19:04,310
so the runtime here just really depends on

1156
01:19:04,310 --> 01:19:06,270
the size of these clicks

1157
01:19:06,290 --> 01:19:07,270
right because

1158
01:19:07,290 --> 01:19:11,230
the maximum weight spanning tree in this case is is trivial

1159
01:19:12,140 --> 01:19:13,810
compared to the

1160
01:19:13,830 --> 01:19:18,100
the thing to do and when you passing messages because pass messages your

1161
01:19:18,100 --> 01:19:19,910
you have

1162
01:19:19,930 --> 01:19:23,080
two days for a clique over

1163
01:19:23,080 --> 01:19:27,040
the whole set of variables that number is very large

1164
01:19:27,060 --> 01:19:29,350
you can efficiently so

1165
01:19:29,350 --> 01:19:35,000
so for example if you look at what i mentioned before grid

1166
01:19:35,020 --> 01:19:38,120
and if you look at what can you possibly called

1167
01:19:38,160 --> 01:19:41,250
two have when you do something like junction tree

1168
01:19:41,270 --> 01:19:43,180
for the junction tree so far

1169
01:19:43,190 --> 01:19:45,390
this kind of structure

1170
01:19:45,540 --> 01:19:53,620
he would be something like this structure start with this

1171
01:19:53,640 --> 01:19:57,710
and you would be something that basically

1172
01:19:57,790 --> 01:19:59,100
put together

1173
01:19:59,100 --> 01:20:00,060
these things

1174
01:20:00,080 --> 01:20:01,460
and also something that

1175
01:20:01,480 --> 01:20:03,290
the simplest one is just put

1176
01:20:03,330 --> 01:20:04,770
all of these guys

1177
01:20:04,770 --> 01:20:08,280
given point in all other points are sampled the points

1178
01:20:08,310 --> 01:20:09,420
are examined

1179
01:20:09,500 --> 01:20:10,570
is examined

1180
01:20:10,630 --> 01:20:14,930
and outliers are points that have a spectrum

1181
01:20:14,940 --> 01:20:17,520
future hyperinflation again

1182
01:20:17,540 --> 01:20:21,120
two more details later

1183
01:20:21,130 --> 01:20:26,610
so here is the outline we already survived the introduction and

1184
01:20:26,620 --> 01:20:31,330
in the rest of the tutorial i will follow the model of the

1185
01:20:31,350 --> 01:20:37,510
classification where we have a model based approach is omitted based approaches

1186
01:20:37,510 --> 01:20:39,770
and as the third

1187
01:20:39,780 --> 01:20:41,090
kind of last year

1188
01:20:41,100 --> 01:20:46,190
adoption adoption of different models with special problem just

1189
01:20:46,200 --> 01:20:52,360
picked out one special problem as the sample and i just a high dimensional points

1190
01:20:52,390 --> 01:20:54,130
because we are working in so

1191
01:20:54,140 --> 01:20:58,510
it's not an advertising for work but i will try to motivate why i think

1192
01:20:58,840 --> 01:20:59,830
high dimensional

1193
01:20:59,840 --> 01:21:02,090
data is still challenging for four

1194
01:21:02,100 --> 01:21:03,970
of the detection of

1195
01:21:06,290 --> 01:21:09,040
we will discuss these approaches

1196
01:21:09,090 --> 01:21:14,620
and give some sample models and sample rooms here

1197
01:21:14,640 --> 01:21:17,580
and we also try to keep in mind the other two

1198
01:21:17,590 --> 01:21:20,590
class classification schema

1199
01:21:21,790 --> 01:21:26,000
so the output and the rest the resolution of the reference set

1200
01:21:26,000 --> 01:21:28,530
are still considered

1201
01:21:28,650 --> 01:21:35,090
OK so first of all statistical tests of the general idea of this test for

1202
01:21:35,090 --> 01:21:36,760
outlier detection is

1203
01:21:36,770 --> 01:21:42,330
OK given a certain kind of statistical distribution for example gaussian distribution then we just

1204
01:21:42,330 --> 01:21:46,070
compute the parameters assuming all data points have been generated

1205
01:21:46,130 --> 01:21:52,730
by this statistical distribution in terms of the case of course you washington solution just

1206
01:21:52,730 --> 01:21:55,310
compute the mean and standard deviation

1207
01:21:55,360 --> 01:22:00,230
and then outliers are points that have low probability to be generated by this

1208
01:22:00,270 --> 01:22:03,500
distribution so this is very similar to that year

1209
01:22:03,520 --> 01:22:05,110
we haven't seen before

1210
01:22:05,130 --> 01:22:09,290
in barnet discussion of the case of the problem where we have a caution distribution

1211
01:22:09,490 --> 01:22:11,950
of the station periods

1212
01:22:11,960 --> 01:22:13,310
and the

1213
01:22:13,330 --> 01:22:17,310
observation that it's very much ten weeks

1214
01:22:17,330 --> 01:22:21,710
i had a very low probability to be generated by this mechanism

1215
01:22:21,720 --> 01:22:25,000
this would be considered as an online

1216
01:22:25,370 --> 01:22:29,370
the basic assumption obviously is that the normal data objects follow

1217
01:22:29,380 --> 01:22:32,280
a given distribution which should be known of course

1218
01:22:32,290 --> 01:22:37,530
and the core in high probability regions small schools

1219
01:22:37,550 --> 01:22:39,610
and the of lies on the other hand

1220
01:22:39,670 --> 01:22:42,240
deviate strongly from distribution then

1221
01:22:44,440 --> 01:22:47,840
to be generated by the switch distribution

1222
01:22:47,860 --> 01:22:54,540
and this basic idea here has been implemented in huge number of different tests

1223
01:22:54,590 --> 01:22:56,090
that are available

1224
01:22:56,140 --> 01:22:59,060
the most important difference of these tests are

1225
01:22:59,070 --> 01:23:03,050
which type of these data distribution

1226
01:23:03,070 --> 01:23:09,120
is the number of which number of variables univariate or multivariate data

1227
01:23:09,200 --> 01:23:11,320
dimension the data was

1228
01:23:11,340 --> 01:23:18,090
which number of of distributions only one normal distribution or one

1229
01:23:18,110 --> 01:23:21,020
distribution of one mechanism for

1230
01:23:21,070 --> 01:23:23,590
four normal objects or or several

1231
01:23:24,730 --> 01:23:26,280
and also

1232
01:23:26,290 --> 01:23:27,730
we have parameter

1233
01:23:27,740 --> 01:23:33,160
parametric and non parametric approach nonparametric approach histogram

1234
01:23:33,200 --> 01:23:38,600
or pragmatic approach be computing for example for caution distribution meaning

1235
01:23:38,700 --> 01:23:45,710
and yes i will try to give you a very short example which is very

1236
01:23:45,820 --> 01:23:47,770
similar to the one we saw

1237
01:23:48,060 --> 01:23:50,520
in honest discussion so

1238
01:23:50,520 --> 01:23:51,770
we just assume

1239
01:23:51,850 --> 01:23:53,540
gaussian distribution

1240
01:23:53,560 --> 01:23:55,520
multivariate data

1241
01:23:55,530 --> 01:23:56,860
just one model

1242
01:23:56,880 --> 01:23:59,360
and we use parametric approach

1243
01:23:59,370 --> 01:24:03,500
and what we can do is OK the probability density function of a multivariate normal

1244
01:24:04,760 --> 01:24:06,020
quite well known

1245
01:24:06,030 --> 01:24:07,280
formerly here

1246
01:24:07,300 --> 01:24:09,980
we have to compute the mean and

1247
01:24:09,990 --> 01:24:12,510
covariance matrix

1248
01:24:12,530 --> 01:24:18,970
and then we can compute the mahalanobis distance of point of any point to me

1249
01:24:19,010 --> 01:24:23,060
and interestingly this distance function the interesting

1250
01:24:23,080 --> 01:24:26,790
this distance function just follows the chi square distribution

1251
01:24:26,800 --> 01:24:28,690
of the degrees of freedom

1252
01:24:28,730 --> 01:24:30,830
he is the dimension of data space

1253
01:24:30,860 --> 01:24:32,830
and then

1254
01:24:32,840 --> 01:24:35,230
typical statistical test is

1255
01:24:35,260 --> 01:24:40,940
that points that have a a distance with distance

1256
01:24:40,950 --> 01:24:43,020
larger than a given threshold

1257
01:24:43,030 --> 01:24:45,770
i mean for all lines

1258
01:24:46,960 --> 01:24:51,480
and this is just the visualisation so if people compute the mean on the left-hand

1259
01:24:51,480 --> 01:24:55,780
side that crosses the mean of these observations here

1260
01:24:55,840 --> 01:24:57,790
and on the right-hand side you have the

1261
01:24:57,790 --> 01:25:01,580
so i summarized here the bounds that the

1262
01:25:01,600 --> 01:25:04,110
we're discussing this morning

1263
01:25:04,120 --> 01:25:07,720
so this is the standard of being bound for one function

1264
01:25:07,770 --> 01:25:08,890
the true

1265
01:25:08,900 --> 01:25:12,390
average so the expectation is less than the empirical plus

1266
01:25:12,410 --> 01:25:15,470
the system that now we have seen many times

1267
01:25:15,490 --> 01:25:17,770
one was called of of n

1268
01:25:18,580 --> 01:25:23,610
there is this refined version which includes the variance or

1269
01:25:23,630 --> 01:25:26,450
an upper bound on the variance which is in this case

1270
01:25:26,530 --> 01:25:28,710
the expectation itself

1271
01:25:28,720 --> 01:25:34,310
and which is slightly better when the expectation is small and as i told you

1272
01:25:34,330 --> 01:25:39,390
this morning what we want is to improve on this union bound and one way

1273
01:25:39,390 --> 01:25:40,320
was to

1274
01:25:40,390 --> 01:25:45,930
introduced this this prior if you want on the on the set of functions

1275
01:25:45,980 --> 01:25:50,210
and and also ways to use the idea of these variants and to use

1276
01:25:50,260 --> 01:25:51,930
this parameter here

1277
01:25:52,940 --> 01:25:57,000
get a better handle on how big is the class of functions because so far

1278
01:25:57,000 --> 01:25:59,160
the only measure of

1279
01:25:59,170 --> 01:26:03,120
the size of the class of functions we have seen is the the cardinality itself

1280
01:26:03,120 --> 01:26:04,620
of the of the class

1281
01:26:04,770 --> 01:26:07,650
now we see that with this trick we can

1282
01:26:08,130 --> 01:26:12,100
you know a little bit more precise

1283
01:26:12,130 --> 01:26:14,330
OK so

1284
01:26:14,430 --> 01:26:17,580
let's let's take a simple example

1285
01:26:17,590 --> 01:26:19,580
where we we have a set of functions

1286
01:26:19,620 --> 01:26:21,700
finite may be large

1287
01:26:22,440 --> 01:26:28,480
but which have the property that they all behave smaller similarly which means they all

1288
01:26:28,480 --> 01:26:32,400
make small is the same predictions or they make

1289
01:26:32,410 --> 01:26:35,590
all small is the same type of errors

1290
01:26:35,610 --> 01:26:36,520
right so

1291
01:26:37,050 --> 01:26:38,920
this can be quantified let's say

1292
01:26:38,930 --> 01:26:41,520
let's say we even know how different they are

1293
01:26:41,540 --> 01:26:44,930
let's say we know that if we take two function f i and j

1294
01:26:44,940 --> 01:26:45,860
in this

1295
01:26:45,880 --> 01:26:47,790
set of function that we have f

1296
01:26:47,800 --> 01:26:49,050
and we look at

1297
01:26:49,070 --> 01:26:54,630
how many times or how frequently the predictions of the errors made by the sanctions

1298
01:26:54,630 --> 01:26:55,740
are different

1299
01:26:55,760 --> 01:26:59,750
this is small and we we know some of that that is an upper bound

1300
01:26:59,760 --> 01:27:01,190
on this

1301
01:27:01,200 --> 01:27:03,770
OK then we can exploit this fact

1302
01:27:03,780 --> 01:27:07,070
in the following way

1303
01:27:07,890 --> 01:27:10,690
we just rewriting

1304
01:27:10,700 --> 01:27:15,420
the difference between for any function in our class the difference between two an empirical

1305
01:27:17,750 --> 01:27:20,030
the same thing for

1306
01:27:20,050 --> 01:27:24,380
a fixed function any that we fix beforehand

1307
01:27:26,700 --> 01:27:30,010
the difference between two and empirical average of

1308
01:27:30,020 --> 01:27:32,680
somehow the difference between f and

1309
01:27:32,730 --> 01:27:33,700
and you

1310
01:27:34,740 --> 01:27:37,130
where it is

1311
01:27:37,140 --> 01:27:41,220
any function in set of zero is something i don't like the centre of the

1312
01:27:41,220 --> 01:27:44,320
class if you want or any function in class

1313
01:27:44,840 --> 01:27:47,010
now if you look at those two terms

1314
01:27:47,870 --> 01:27:51,560
we have something that we know how to bound we have things inequality because the

1315
01:27:51,560 --> 01:27:57,670
function is fixed and here is something we can bound using bernstein's inequality

1316
01:27:57,690 --> 01:28:01,190
and using the fact that now the variance

1317
01:28:01,200 --> 01:28:03,260
of this

1318
01:28:03,280 --> 01:28:04,590
run valuable

1319
01:28:04,600 --> 01:28:06,940
is actually controlled by

1320
01:28:07,510 --> 01:28:13,020
the frequency at which the two functions make different predictions so we can use then

1321
01:28:13,020 --> 01:28:16,350
this like an upper bound on the variance of this

1322
01:28:16,390 --> 01:28:18,210
quantity here is

1323
01:28:18,250 --> 01:28:19,820
actually i

1324
01:28:21,100 --> 01:28:24,200
essentially the result that you can get this form

1325
01:28:24,210 --> 01:28:27,630
this is the having to from

1326
01:28:27,670 --> 01:28:30,360
but the fixed function and this is

1327
01:28:30,370 --> 01:28:32,240
the bernstein

1328
01:28:32,690 --> 01:28:37,900
i've been sloppy about the constant but that's not important at this point

1329
01:28:37,910 --> 01:28:41,450
where now the log in which is the size of the class of functions is

1330
01:28:41,450 --> 01:28:44,810
multiplied by these alpha which was the upper bound on

1331
01:28:44,850 --> 01:28:47,900
the difference on of the function

1332
01:28:47,940 --> 01:28:51,500
OK so what we have gained compared to what we had before is that we

1333
01:28:51,500 --> 01:28:54,560
have been able to reduce the complexity

1334
01:28:54,570 --> 01:28:56,170
are the

1335
01:28:56,180 --> 01:29:02,340
additional term that we gain from looking at me functions by the fact that these

1336
01:29:02,340 --> 01:29:03,310
functions they are

1337
01:29:03,610 --> 01:29:04,700
there are not too

1338
01:29:04,710 --> 01:29:06,770
different from each other

1339
01:29:06,790 --> 01:29:08,830
so we went from log in

1340
01:29:08,850 --> 01:29:12,330
two and five things look

1341
01:29:13,320 --> 01:29:16,240
and this is the first the first point where we can

1342
01:29:19,270 --> 01:29:23,980
that's what i call the structure or the geometry of the class of functions so

1343
01:29:24,000 --> 01:29:26,190
and this is very important i think maybe that's

1344
01:29:26,200 --> 01:29:28,550
the most important

1345
01:29:29,650 --> 01:29:33,500
aspect of this lecture of the set of lectures is that

1346
01:29:33,550 --> 01:29:36,840
the complexity of the class of functions

1347
01:29:40,250 --> 01:29:42,150
in the very precise way

1348
01:29:42,170 --> 01:29:44,790
on how this function har

1349
01:29:44,800 --> 01:29:46,440
position in the space

1350
01:29:46,460 --> 01:29:49,670
so you you have to think about a set of function as

1351
01:29:49,680 --> 01:29:53,050
some kind of metric space where the functions are

1352
01:29:53,070 --> 01:29:57,580
at a certain distance from each other and the distance comes from

1353
01:29:57,600 --> 01:29:58,790
things like

1354
01:29:58,960 --> 01:30:04,880
like this right so this you can think of it as the distance between functions

1355
01:30:05,750 --> 01:30:07,500
and old always these distance

1356
01:30:07,510 --> 01:30:11,490
the set of function becomes a metric space which has a certain structure and the

1357
01:30:11,500 --> 01:30:15,610
structure is the thing that controls how

1358
01:30:15,630 --> 01:30:17,410
large will the deviation b

1359
01:30:17,430 --> 01:30:20,330
when you come consider for example the supremum of the

1360
01:30:20,350 --> 01:30:22,600
empirical process

1361
01:30:25,720 --> 01:30:28,760
this was the simple case where we have

1362
01:30:30,220 --> 01:30:35,500
knowledge on how dissimilar the function so but you can actually go much beyond and

1363
01:30:36,580 --> 01:30:37,940
more of the structure

1364
01:30:37,960 --> 01:30:43,250
so the first thing to do is to repeat this construction

1365
01:30:43,270 --> 01:30:44,530
for the whole set

1366
01:30:44,640 --> 01:30:47,250
so what i mean by this is

1367
01:30:47,770 --> 01:30:49,850
say you have

1368
01:30:49,900 --> 01:30:54,260
you're set of functions from one two capital and and from the set of function

1369
01:30:54,260 --> 01:30:56,510
if you slam the brakes

1370
01:30:56,520 --> 01:30:59,920
well what happens at will go

1371
01:30:59,930 --> 01:31:03,060
what do you think you slam the brakes at the local forward

1372
01:31:03,120 --> 01:31:06,540
well go backwards if you accelerate the car all of a sudden

1373
01:31:06,590 --> 01:31:09,420
apple will go backwards and and forwards

1374
01:31:09,450 --> 01:31:11,010
you can

1375
01:31:11,070 --> 01:31:12,360
you can

1376
01:31:12,370 --> 01:31:15,740
entertain you apparently thanksgiving

1377
01:31:15,780 --> 01:31:16,670
get some

1378
01:31:16,680 --> 01:31:24,510
of the twenty five thousand dollars tuition back

1379
01:31:24,570 --> 01:31:26,650
when fluids are moving

1380
01:31:26,700 --> 01:31:28,480
situations i

1381
01:31:28,530 --> 01:31:30,300
way more complicated

1382
01:31:30,340 --> 01:31:34,650
then when they are static

1383
01:31:34,650 --> 01:31:35,430
and this

1384
01:31:35,440 --> 01:31:36,770
leads to

1385
01:31:36,790 --> 01:31:38,110
again very

1386
01:31:38,120 --> 01:31:39,480
i nonintuitive

1387
01:31:40,550 --> 01:31:44,370
of fluids

1388
01:31:44,380 --> 01:31:46,780
i will

1389
01:31:46,850 --> 01:31:50,080
the rise in a shortcut way

1390
01:31:50,090 --> 01:31:51,690
very famous

1391
01:31:51,800 --> 01:31:55,680
equation which is called the new easy creation

1392
01:31:55,720 --> 01:31:58,240
which relates

1393
01:31:58,330 --> 01:32:01,310
kinetic energy is potential energy

1394
01:32:03,460 --> 01:32:07,820
suppose i have a

1395
01:32:07,830 --> 01:32:09,370
the fluid

1396
01:32:09,380 --> 01:32:11,560
non compressible

1397
01:32:11,610 --> 01:32:13,690
like so

1398
01:32:13,700 --> 01:32:15,550
this cross sectional area

1399
01:32:15,610 --> 01:32:17,530
it too

1400
01:32:17,570 --> 01:32:18,910
and the pressure here

1401
01:32:18,910 --> 01:32:21,460
the two

1402
01:32:21,470 --> 01:32:23,180
and i have a

1403
01:32:24,630 --> 01:32:26,810
of the liquid

1404
01:32:26,900 --> 01:32:29,020
which is the two

1405
01:32:29,040 --> 01:32:30,010
this level

1406
01:32:30,010 --> 01:32:31,960
is why two

1407
01:32:31,970 --> 01:32:35,940
you have a cross sectional area a one

1408
01:32:35,960 --> 01:32:38,720
i have pressure p one

1409
01:32:38,770 --> 01:32:39,850
my level

1410
01:32:39,860 --> 01:32:41,770
what i wanted increase

1411
01:32:43,270 --> 01:32:45,010
and i have a much larger

1412
01:32:45,030 --> 01:32:47,180
velocity because the cross section is

1413
01:32:47,220 --> 01:32:50,020
substantially smaller there

1414
01:32:50,200 --> 01:32:52,770
these fluid were completely static

1415
01:32:52,770 --> 01:32:54,770
if it were not moving

1416
01:32:54,790 --> 01:32:59,220
so forget about if you want to forget about the feature which is sitting still

1417
01:33:00,060 --> 01:33:02,570
one minus p two

1418
01:33:02,580 --> 01:33:04,200
would be

1419
01:33:05,220 --> 01:33:07,430
g and y two

1420
01:33:07,440 --> 01:33:10,060
why why why if rho easily

1421
01:33:10,090 --> 01:33:13,060
density of the fluid that's post-cold cultural

1422
01:33:13,110 --> 01:33:14,790
would just be sitting still

1423
01:33:14,840 --> 01:33:16,770
and we know that the pressure here

1424
01:33:16,880 --> 01:33:17,920
the lower

1425
01:33:17,920 --> 01:33:21,230
and the pressure there

1426
01:33:21,280 --> 01:33:24,490
this is also if you want to roll gh

1427
01:33:24,490 --> 01:33:26,670
if you call this difference

1428
01:33:29,320 --> 01:33:33,770
ro gh

1429
01:33:33,790 --> 01:33:38,230
that reminds me of MGH ngh gravitational potential energy

1430
01:33:38,270 --> 01:33:39,980
when i defined by

1431
01:33:39,980 --> 01:33:41,330
volume i get

1432
01:33:43,030 --> 01:33:44,520
so this is really

1433
01:33:45,720 --> 01:33:47,570
which is gravitational

1434
01:33:47,670 --> 01:33:49,360
potential energy

1435
01:33:50,970 --> 01:33:54,930
that makes the and divided by volume become density

1436
01:33:56,630 --> 01:33:58,580
pressure itself

1437
01:33:58,620 --> 01:33:59,970
must also

1438
01:33:59,980 --> 01:34:01,540
have the dimension

1439
01:34:01,560 --> 01:34:02,970
of energy

1440
01:34:05,820 --> 01:34:08,660
and if we now set this whole machine in motion

1441
01:34:08,720 --> 01:34:10,700
then there are three players

1442
01:34:10,830 --> 01:34:15,060
it is on the one hand kinetic energy of motion

1443
01:34:15,120 --> 01:34:16,950
kinetic energy

1444
01:34:16,950 --> 01:34:19,490
i take per unit volume

1445
01:34:19,550 --> 01:34:22,270
there is gravitational potential energy

1446
01:34:22,320 --> 01:34:25,830
i will take a unit volume

1447
01:34:25,840 --> 01:34:27,260
and then there was pressure

1448
01:34:27,300 --> 01:34:30,970
there are equal partners

1449
01:34:31,420 --> 01:34:33,610
and if i applied to conservation of

1450
01:34:33,630 --> 01:34:34,910
and g

1451
01:34:35,040 --> 01:34:39,280
the sum of these three should remain constant that's the idea behind

1452
01:34:39,310 --> 01:34:40,340
the new is low

1453
01:34:40,420 --> 01:34:42,360
noise equation

1454
01:34:42,370 --> 01:34:44,190
when i think of fluid element

1455
01:34:44,230 --> 01:34:47,730
and i move from one position in the two to another position

1456
01:34:47,770 --> 01:34:49,310
it traits b

1457
01:34:49,330 --> 01:34:50,830
four either heights

1458
01:34:51,490 --> 01:34:53,940
four pressure

1459
01:34:53,990 --> 01:34:56,380
one is the kinetic energy per unit volume

1460
01:34:56,400 --> 01:35:00,200
well the kinetic energy is one half MV squared i divided by volume i could

1461
01:35:00,200 --> 01:35:01,400
one have rome

1462
01:35:01,410 --> 01:35:03,250
the script

1463
01:35:03,300 --> 01:35:07,410
one is gravitational potential energy that is and why

1464
01:35:07,430 --> 01:35:08,990
i divide by volume

1465
01:35:09,010 --> 01:35:10,090
so i get

1466
01:35:10,090 --> 01:35:10,840
to date

1467
01:35:10,840 --> 01:35:12,490
i'd like to talk

1468
01:35:12,500 --> 01:35:14,250
with you about

1469
01:35:14,280 --> 01:35:16,340
my early days at MIT

1470
01:35:16,380 --> 01:35:18,620
and the research that i did here

1471
01:35:18,690 --> 01:35:21,390
is a long time ago

1472
01:35:21,400 --> 01:35:24,360
i got my phd in the netherlands in

1473
01:35:24,400 --> 01:35:27,600
nuclear physics and i came to MIT in january

1474
01:35:27,620 --> 01:35:31,240
nineteen sixty six almost thirty four years ago

1475
01:35:31,280 --> 01:35:34,720
and the idea was that i was only going to spend here one year on

1476
01:35:34,720 --> 01:35:36,540
the post position

1477
01:35:36,670 --> 01:35:38,380
i liked it so much

1478
01:35:38,400 --> 01:35:39,620
but i never left

1479
01:35:39,620 --> 01:35:41,690
and i don't regret it

1480
01:35:41,760 --> 01:35:42,930
i joint c

1481
01:35:42,940 --> 01:35:45,870
x-ray astronomy group here of professor ross

1482
01:35:45,910 --> 01:35:47,450
x-ray astronomy

1483
01:35:47,510 --> 01:35:48,800
has to be done

1484
01:35:48,860 --> 01:35:53,190
from above the earth's atmosphere we're at least near the top of the earth's atmosphere

1485
01:35:53,190 --> 01:35:54,450
because the x-rays

1486
01:35:54,470 --> 01:35:55,470
i'm sure

1487
01:35:55,480 --> 01:35:56,720
by air

1488
01:35:57,760 --> 01:36:00,190
optical astronomy and radio astronomy

1489
01:36:00,190 --> 01:36:01,400
which can be done

1490
01:36:02,340 --> 01:36:04,230
the ground

1491
01:36:04,250 --> 01:36:05,820
the kind of x-rays

1492
01:36:05,870 --> 01:36:08,620
that we measure

1493
01:36:08,640 --> 01:36:10,150
i'm not unlike those that

1494
01:36:10,160 --> 01:36:13,760
your dentist is using when takes an extra

1495
01:36:13,870 --> 01:36:17,470
energy range of these x-rays somewhere between one

1496
01:36:17,510 --> 01:36:20,970
and thirty forty kilo electron volts and if you don't know what to kill electron

1497
01:36:20,970 --> 01:36:23,920
volt is that's fine too which you never expressed

1498
01:36:23,980 --> 01:36:27,620
the energy of an x-ray in terms of joules because the number becomes so

1499
01:36:27,690 --> 01:36:32,170
ridiculously small

1500
01:36:32,220 --> 01:36:33,750
during world war two

1501
01:36:33,810 --> 01:36:35,560
on the heat germany

1502
01:36:35,610 --> 01:36:37,420
then from brown

1503
01:36:38,730 --> 01:36:40,260
the v two rockets

1504
01:36:40,280 --> 01:36:44,420
four destructive for purposes was developed in panama

1505
01:36:44,470 --> 01:36:46,670
and after the war

1506
01:36:46,730 --> 01:36:49,510
the americans use these v two rockets

1507
01:36:50,220 --> 01:36:52,230
scientific purposes

1508
01:36:52,250 --> 01:36:53,340
the first

1509
01:36:53,340 --> 01:36:54,750
rocket flights

1510
01:36:54,750 --> 01:36:57,610
to search for x-rays from the sun

1511
01:36:57,650 --> 01:37:00,690
two plays in nineteen forty eight

1512
01:37:00,730 --> 01:37:03,590
and x-rays were found from the sun

1513
01:37:03,640 --> 01:37:06,060
that was quite a surprise

1514
01:37:06,080 --> 01:37:08,390
and the power

1515
01:37:08,480 --> 01:37:12,780
energy per second that the sun puts out in x-rays

1516
01:37:12,830 --> 01:37:17,700
divided by the power in optical which is almost all the radiation of the sun

1517
01:37:17,760 --> 01:37:18,680
give it

1518
01:37:18,700 --> 01:37:20,570
the symbol of the sun

1519
01:37:20,640 --> 01:37:24,770
is approximately ten to the minus so only one ten millions

1520
01:37:24,790 --> 01:37:27,140
of all the energy comes out in extra

1521
01:37:27,200 --> 01:37:29,110
so from one energy point of view

1522
01:37:29,140 --> 01:37:30,390
it's very very little

1523
01:37:30,410 --> 01:37:32,170
it's very great deal two

1524
01:37:32,180 --> 01:37:33,050
but it is

1525
01:37:33,770 --> 01:37:35,760
very little

1526
01:37:35,760 --> 01:37:37,170
in nineteen

1527
01:37:37,230 --> 01:37:39,160
sixty two

1528
01:37:39,230 --> 01:37:44,480
scientist here in cambridge among them professor bruno rossi was a professor at MIT

1529
01:37:44,580 --> 01:37:46,330
and we can't check

1530
01:37:46,390 --> 01:37:52,020
and because q were working across the street at american science and engineering

1531
01:37:52,040 --> 01:37:53,200
i attempted

1532
01:37:54,640 --> 01:37:58,270
do an experiment to see whether they could detect x-rays

1533
01:37:58,290 --> 01:38:00,580
from object outside

1534
01:38:00,630 --> 01:38:02,480
our solar system

1535
01:38:02,480 --> 01:38:05,460
now the odds were very low that they were going to succeed

1536
01:38:05,540 --> 01:38:07,080
and the reason is very simple

1537
01:38:07,110 --> 01:38:08,770
if you take the song

1538
01:38:08,830 --> 01:38:12,700
you moved out of the nearest stars which is typically ten to one hundred light

1539
01:38:13,670 --> 01:38:15,700
wouldn't stand a chance to see

1540
01:38:15,740 --> 01:38:17,410
x-rays fact

1541
01:38:17,420 --> 01:38:20,910
the sensitivity of the detectors in these days was

1542
01:38:20,920 --> 01:38:22,450
too low by at least

1543
01:38:22,540 --> 01:38:26,230
nine orders of magnitude factor of one billion

1544
01:38:26,270 --> 01:38:27,550
the two everyone exp

1545
01:38:27,610 --> 01:38:29,920
surprised to everyone's

1546
01:38:30,290 --> 01:38:33,480
at least as far as i should say they succeeded

1547
01:38:33,540 --> 01:38:35,290
and they discover the object

1548
01:38:35,300 --> 01:38:40,820
which was later called it's one is in the constellation scorpio is extant to x-rays

1549
01:38:40,830 --> 01:38:41,630
and one

1550
01:38:41,640 --> 01:38:45,450
for the first x-ray source in the constellation

1551
01:38:45,490 --> 01:38:47,830
the total

1552
01:38:48,110 --> 01:38:52,550
output of that source was about ten thousand times more than the sum that didn't

1553
01:38:52,550 --> 01:38:56,080
make the source so special because many stars in the sky

1554
01:38:56,100 --> 01:38:57,790
that radiate way more

1555
01:38:58,580 --> 01:38:59,960
and then are summed us

1556
01:38:59,980 --> 01:39:01,610
but also very special

1557
01:39:01,610 --> 01:39:03,380
about go x y

1558
01:39:03,430 --> 01:39:04,480
that the

1559
01:39:04,490 --> 01:39:06,270
x-ray power

1560
01:39:06,320 --> 01:39:08,830
over the optical power

1561
01:39:09,300 --> 01:39:11,010
four scorex one

1562
01:39:11,510 --> 01:39:12,860
was approximately

1563
01:39:12,880 --> 01:39:14,760
one thousand

1564
01:39:14,890 --> 01:39:16,260
in other words

1565
01:39:16,330 --> 01:39:20,360
x-rays and dominate its source of energy

1566
01:39:20,430 --> 01:39:24,620
and the optical this sort of let's call it byproduct was the son

1567
01:39:24,670 --> 01:39:26,470
optical is the main thing

1568
01:39:26,480 --> 01:39:29,850
and the x-rays is sort of a by product

1569
01:39:29,860 --> 01:39:33,880
so to sixty four dollar question in those days was what can these objects be

1570
01:39:33,880 --> 01:39:37,570
they must be very different from the sun

1571
01:39:37,620 --> 01:39:38,870
and that's what i want to

1572
01:39:38,880 --> 01:39:41,480
discuss which when it came to MIT

1573
01:39:41,520 --> 01:39:44,480
in nineteen sixty six there were about six

1574
01:39:44,490 --> 01:39:47,480
of these x-ray sources known in the sky

1575
01:39:47,500 --> 01:39:49,440
today there are thousands no

1576
01:39:49,460 --> 01:39:50,920
there were six them

1577
01:39:50,970 --> 01:39:55,410
and they were discovered from rocket flights these rockets will be launched

1578
01:39:55,420 --> 01:39:57,000
typically from white sands

1579
01:39:57,020 --> 01:40:00,370
and they would spend about five minutes above the earth's atmosphere

1580
01:40:00,500 --> 01:40:03,100
during those five minutes they scanned the sky

1581
01:40:03,110 --> 01:40:05,090
and six sources were

1582
01:40:06,600 --> 01:40:11,240
i joined here the group of professor george clark who still professor at MIT

1583
01:40:11,270 --> 01:40:13,420
he was working on

1584
01:40:13,470 --> 01:40:14,810
observations made

1585
01:40:14,820 --> 01:40:19,070
to be made from very high flying balloons so we would build the telescope

1586
01:40:19,100 --> 01:40:20,800
we would launch it on the balloon

1587
01:40:20,900 --> 01:40:24,380
and go near the top of the atmosphere

1588
01:40:24,400 --> 01:40:25,730
it's not as good as it

1589
01:40:25,780 --> 01:40:27,310
rocket flight which

1590
01:40:27,360 --> 01:40:29,630
that's completely out of the atmosphere about

1591
01:40:29,670 --> 01:40:34,100
the flights on balloons can last longer than five minutes rocket flights we could fly

1592
01:40:34,880 --> 01:40:38,880
and if we're lucky even days with the price we pay for that is that

1593
01:40:38,910 --> 01:40:43,220
even though there was only very little atmosphere left by force

1594
01:40:44,660 --> 01:40:47,840
o point three percent of the atmosphere was left

1595
01:40:47,860 --> 01:40:49,320
still that

1596
01:40:49,340 --> 01:40:52,190
cause and effect of the absorption so we did lose

1597
01:40:52,220 --> 01:40:55,230
x-rays that the rocket flights did not

1598
01:40:56,600 --> 01:41:00,540
but we had the great advantage of many many hours

1599
01:41:00,540 --> 01:41:01,500
give you

1600
01:41:01,550 --> 01:41:02,740
a rough idea

1601
01:41:02,750 --> 01:41:04,750
of what it took in those days

1602
01:41:04,800 --> 01:41:09,070
i worked on this was graduate students and with many undergraduate students

1603
01:41:09,210 --> 01:41:13,130
telescope in those days to build the cost typically a million dollars

1604
01:41:13,180 --> 01:41:16,790
and it will take us two years to build one the balloons that we needed

1605
01:41:16,790 --> 01:41:19,810
to launch there were about a hundred thousand dollars in those days

1606
01:41:19,860 --> 01:41:20,920
and helium

1607
01:41:20,930 --> 01:41:23,150
that we needed to get it up was about

1608
01:41:23,230 --> 01:41:25,410
eighty thousand dollars

1609
01:41:25,410 --> 01:41:34,100
with the surroundings in terms of temperature at at thermal equilibrium equilibrium population of vacancies

1610
01:41:39,170 --> 01:41:44,080
and this is crystal and so what we're going to do is it's a comparison

1611
01:41:44,100 --> 01:41:49,340
of the binding energy comparison the binding energy

1612
01:41:49,340 --> 01:41:53,430
to the the energy of disruption

1613
01:41:53,450 --> 01:41:58,300
energy of destruction because you can imagine the binding energy of an atom is the

1614
01:41:58,300 --> 01:42:02,750
negative of the energy to form a vacancy how you form a vacancy can imagine

1615
01:42:02,760 --> 01:42:06,470
you go with atomic tweezers and you pull Adam out and what once you have

1616
01:42:06,470 --> 01:42:10,800
to break the bonds of bond that to others so this is the this is

1617
01:42:10,810 --> 01:42:16,210
minus the energy of vacancy formation of

1618
01:42:16,490 --> 01:42:22,450
and the disruptive energy is in our case thermal energy and so there is a

1619
01:42:22,450 --> 01:42:28,180
way of expressing this and we're going to say it's given by the following we're

1620
01:42:28,180 --> 01:42:32,230
gonna compare these 2 in an exponential model like the 1 you saw

1621
01:42:34,940 --> 01:42:41,580
course so I'm going to define the fraction the fraction of sites that are unoccupied

1622
01:42:41,760 --> 01:42:46,000
and which will be the ratio of the number of vacancies per unit volume to

1623
01:42:46,000 --> 01:42:51,660
the number of sites per unit volume so that in V is a number of

1624
01:42:51,660 --> 01:43:02,140
vacancies per unit volume and big M is the number of atomic sites number of

1625
01:43:02,140 --> 01:43:12,960
atomic sites per unit volume income and that's gonna be given by

1626
01:43:13,560 --> 01:43:19,580
the ratio of mediated through this exponential of

1627
01:43:19,670 --> 01:43:25,950
the energy and II loosely move energy and entropy back and forth this is the

1628
01:43:26,040 --> 01:43:39,100
enthalpy of vacancy formation of vacancy formation energy of the a vacancy formation of energy

1629
01:43:39,250 --> 01:43:44,130
over the thermal energy how do we get a rough measure of the thermal energy

1630
01:43:44,130 --> 01:43:50,340
the product of the Boltzmann constant an absolute temperature so what are we doing we're

1631
01:43:50,350 --> 01:43:52,800
taking this on to

1632
01:43:52,800 --> 01:44:00,120
the temperature distribution and we're saying where is this requires energy in relation to the

1633
01:44:00,120 --> 01:44:06,390
average energy of the system and in particular the high entail behind him so

1634
01:44:06,410 --> 01:44:18,010
and it has a prefactor a and is just an entropic factor and it's not

1635
01:44:18,010 --> 01:44:24,540
equal to 1 able to take values between the 10th of up to about here

1636
01:44:24,540 --> 01:44:30,410
and it's typically on the order of for 5 something like this but in some

1637
01:44:30,410 --> 01:44:35,250
in some cases it can even be less than 1 so this is a relationship

1638
01:44:35,250 --> 01:44:40,380
that tells us what the fraction of unoccupied sites as as a function of temperature

1639
01:44:40,470 --> 01:44:45,470
so that we could go in and and calculate what the effect bond energy because

1640
01:44:45,470 --> 01:44:49,050
we know what the thermal energy has to work backwards and calculate that side a

1641
01:44:49,050 --> 01:44:52,640
sample calculation was wondering you know which should be

1642
01:44:53,340 --> 01:45:01,450
the vacancy population a piece of copper so I said copper what happens that the

1643
01:45:01,800 --> 01:45:03,340
enthalpy of

1644
01:45:03,390 --> 01:45:09,580
the formation of vacancy is 1 . 2 9 electron volts per atom and the

1645
01:45:09,820 --> 01:45:13,580
factor is equal to 4 . 5 and look this up in tables this is

1646
01:45:13,580 --> 01:45:21,140
not from 1st principles calculations so I said OK let's look at room temperature and

1647
01:45:21,140 --> 01:45:25,410
room temperature upload these numbers in and I got on

1648
01:45:25,470 --> 01:45:31,320
the fraction of vacant sites as 10 to the minus 21

1649
01:45:31,320 --> 01:45:37,120
10 21 of you multiplied by the density of copper that turns out to be

1650
01:45:37,120 --> 01:45:45,650
85 vacancies 85 vacancies per cubic centimeter on this is vanishingly small

1651
01:45:45,670 --> 01:45:50,040
this is vanishingly small this is almost the inverse is almost none right because we

1652
01:45:50,040 --> 01:45:54,970
have roughly Avogadro's number of atoms per cubic centimeter for condensed matter so this is

1653
01:45:54,990 --> 01:46:02,040
really really low and 21 there's a hundred different insurance there is 85 there so

1654
01:46:02,040 --> 01:46:05,820
few you can't give each meaning of

1655
01:46:05,950 --> 01:46:09,730
and then so that simple whenever we go to the melting point at the melting

1656
01:46:09,730 --> 01:46:14,430
point of copper OK I'm not going to melt the copper like ice at 0

