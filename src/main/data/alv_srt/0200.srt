1
00:00:00,000 --> 00:00:05,260
which means that the maximum number of distinct rows is no more than

2
00:00:05,270 --> 00:00:07,760
which means the right most

3
00:00:07,780 --> 00:00:11,690
so clearly an MDP with n states

4
00:00:11,700 --> 00:00:17,030
cannot produce the system dynamics matrix was rank is more than

5
00:00:17,050 --> 00:00:18,380
yes because

6
00:00:18,430 --> 00:00:21,110
the only way be n distinct rows

7
00:00:21,130 --> 00:00:25,980
in every row every history that and in the same state where the same future

8
00:00:25,980 --> 00:00:27,190
the prediction yes

9
00:00:27,210 --> 00:00:30,320
the park

10
00:00:31,370 --> 00:00:32,140
the whole

11
00:00:32,280 --> 00:00:38,300
we have the highest might not want to use an MDP

12
00:00:38,300 --> 00:00:42,730
cannot produce even states can produce is made that one

13
00:00:42,740 --> 00:00:43,850
i'm going to show you my model

14
00:00:43,890 --> 00:00:44,800
two minutes

15
00:00:44,810 --> 00:00:46,960
but having

16
00:00:50,800 --> 00:00:53,360
so i just wanted to say

17
00:00:53,380 --> 00:00:59,260
one hundred years of pompey NDP except you don't get to see the states so

18
00:00:59,260 --> 00:01:00,890
if you ignore the last

19
00:01:00,940 --> 00:01:06,700
these these nodes you look at things about this as an MDP state at time

20
00:01:07,880 --> 00:01:11,730
state at time t plus one depends on the last state and the action you

21
00:01:11,730 --> 00:01:16,570
chose the conditional probability table to the transition matrix determines the distribution of the next

22
00:01:16,570 --> 00:01:20,590
state given the last state and the action so if i didn't have the edges

23
00:01:20,590 --> 00:01:24,730
coming out there will be a markov decision process is a graphical model representation of

24
00:01:24,730 --> 00:01:28,770
a markov decision process upon BP is just we don't get to see the state

25
00:01:28,990 --> 00:01:31,890
instead you get to see some other thing which is the observation

26
00:01:31,900 --> 00:01:36,590
which is determined stochastically from the state to make mark to make it upon the

27
00:01:36,590 --> 00:01:43,190
pond these are hugely richer class that MDP in fact most real world situations you

28
00:01:43,190 --> 00:01:46,600
have publicly because you never really get to see the state

29
00:01:46,620 --> 00:01:48,510
OK so you never get the states

30
00:01:48,590 --> 00:01:54,210
is the underlying hidden things like chairs and tables and things that generally get to

31
00:01:54,210 --> 00:01:57,400
observe what you get observe visual images or

32
00:01:57,410 --> 00:01:59,540
auditory signals things that's all

33
00:01:59,580 --> 00:02:00,710
right so

34
00:02:00,720 --> 00:02:03,910
all worlds are really pompey things

35
00:02:04,990 --> 00:02:07,500
OK well actually you know what i proved you now

36
00:02:09,580 --> 00:02:13,910
any palm TV with an underlying hidden states

37
00:02:13,950 --> 00:02:17,690
cannot produce a ceramics matrix of rank more than n and then i'll show you

38
00:02:17,690 --> 00:02:19,540
my model

39
00:02:20,600 --> 00:02:25,660
and this proof of i'm not going to hopefully only hopefully some of you get

40
00:02:25,710 --> 00:02:29,100
i don't think all of you here because be very fast over to how to

41
00:02:29,100 --> 00:02:32,930
pompey's work how HMM is one way to men's work is you don't get to

42
00:02:32,930 --> 00:02:34,950
see the state so what do you do instead

43
00:02:35,040 --> 00:02:41,210
instead what you do is you take your observations and compute posteriors over the underlying

44
00:02:43,510 --> 00:02:49,460
bayes rule basically by using the obvious bayesian updating procedure to say i don't get

45
00:02:49,460 --> 00:02:52,970
to see the state i can see observations so i'm just going to maintain the

46
00:02:52,990 --> 00:02:58,570
distribution over the underlying states and that's going to be my representation of state distribution

47
00:02:58,570 --> 00:02:59,570
over states

48
00:02:59,630 --> 00:03:05,140
the called belief states often in the in the punjabi literature groups of belief states

49
00:03:05,150 --> 00:03:08,840
are you representation of state in hmm upon

50
00:03:13,620 --> 00:03:14,530
the key

51
00:03:14,530 --> 00:03:16,270
he said need which to be

52
00:03:16,280 --> 00:03:20,010
and got very fast is the following

53
00:03:20,030 --> 00:03:24,300
if i want to make predictions about any possible future and upon DP give me

54
00:03:24,300 --> 00:03:28,790
the belief state it turns out the prediction for any future is a linear function

55
00:03:28,820 --> 00:03:31,580
of the belief state

56
00:03:31,600 --> 00:03:35,530
so if you give me any possible test and that's make predictions it turns out

57
00:03:35,860 --> 00:03:39,800
that the prediction for a test p of the given age is the belief state

58
00:03:41,260 --> 00:03:44,830
by age and history the belief state captures the history

59
00:03:44,850 --> 00:03:46,590
find some

60
00:03:47,120 --> 00:03:51,540
a linear linear weights like

61
00:03:51,570 --> 00:03:53,180
so what does that mean

62
00:03:53,180 --> 00:03:57,070
OK the proof of this is actually very straightforward is you know upon you're a

63
00:03:57,070 --> 00:04:01,610
tremendous sort of that mean that means the following if i consider

64
00:04:01,630 --> 00:04:04,000
and rose

65
00:04:04,050 --> 00:04:07,880
in this the belief states are unit basis vectors meets means in which the belief

66
00:04:07,880 --> 00:04:12,090
is that you and one specific state underlying state

67
00:04:12,110 --> 00:04:15,800
right so you basis vectors meaning one zero zero zero zero one zero zero zero

68
00:04:15,800 --> 00:04:17,650
zero zero ones are those just

69
00:04:17,690 --> 00:04:22,600
with all the probability mass in a particular state that underlying states and such belief

70
00:04:22,600 --> 00:04:24,740
factors consider rose

71
00:04:24,790 --> 00:04:26,930
in general corresponding to the end

72
00:04:26,940 --> 00:04:31,230
you can prove quite easily that any row

73
00:04:31,270 --> 00:04:36,310
is a linear combination of those and rose with the linear combination is exactly the

74
00:04:36,310 --> 00:04:38,540
linear content is exactly the belief

75
00:04:39,430 --> 00:04:41,770
that the corresponding history we said again

76
00:04:41,780 --> 00:04:47,980
support as how compute destroy the find this history this is really the corresponding belief

77
00:04:49,330 --> 00:04:54,210
so it assigns some probability of state one somebody state to somebody state three to

78
00:04:54,210 --> 00:04:57,740
then this row is that same linear combination

79
00:04:57,780 --> 00:05:01,290
the other rows that correspond to unit basis police station

80
00:05:01,300 --> 00:05:03,800
despite linear linear algebra

81
00:05:03,800 --> 00:05:04,580
which means

82
00:05:04,610 --> 00:05:09,510
the maximum rank maximum number of linearly independent rows is an

83
00:05:09,510 --> 00:05:13,150
hence the rank that

84
00:05:13,720 --> 00:05:14,510
so what

85
00:05:14,530 --> 00:05:18,790
i won't go to the point very much more

86
00:05:18,810 --> 00:05:20,060
let me now

87
00:05:20,080 --> 00:05:26,220
show you my representation now my alternative representation called predictive state representations you should be

88
00:05:28,050 --> 00:05:28,750
you know

89
00:05:28,750 --> 00:05:33,550
why this interesting cell come back in titus that explain why this might be might

90
00:05:33,550 --> 00:05:39,290
be interesting so here's my model here's my representation predictive state representation of this class

91
00:05:39,290 --> 00:05:42,380
of systems and it looks like this

92
00:05:43,770 --> 00:05:45,020
these core tests

93
00:05:45,060 --> 00:05:46,570
q one qn

94
00:05:46,620 --> 00:05:52,260
my state representation is going to be the predictions forties core tests

95
00:05:52,620 --> 00:05:57,310
the point is that of course given any history h these and numbers are sufficient

96
00:05:57,310 --> 00:06:02,540
statistics of history meaning i can make prediction for any test from the numbers

97
00:06:02,590 --> 00:06:06,150
so if i gave you need a number of my state

98
00:06:06,200 --> 00:06:08,300
and then i take an action a

99
00:06:08,390 --> 00:06:13,430
and get observation o so my new histories h a o

100
00:06:13,430 --> 00:06:18,010
the question to answer is how do i update my in numbers so that have

101
00:06:18,010 --> 00:06:20,130
these numbers

102
00:06:20,200 --> 00:06:23,090
if i can answer that question how models

103
00:06:23,180 --> 00:06:26,910
because i can maintain my state that know that from my state i can make

104
00:06:26,910 --> 00:06:29,140
any prediction i one

105
00:06:30,310 --> 00:06:31,210
are actually

106
00:06:31,220 --> 00:06:35,070
you understand the point i'm making so what i'm going to show you now is

107
00:06:35,110 --> 00:06:35,940
the following

108
00:06:35,960 --> 00:06:38,570
if you give me the numbers for my current history

109
00:06:38,590 --> 00:06:41,290
if i take an action a scene observation of

110
00:06:41,460 --> 00:06:43,380
and we show you how to compute it

111
00:06:43,400 --> 00:06:44,680
the this

112
00:06:44,680 --> 00:06:45,720
these are numbers

113
00:06:45,730 --> 00:06:49,290
if i could do that but i can maintain their numbers as i take action

114
00:06:49,490 --> 00:06:52,860
the observations and i know that one to have been a number that can predict

115
00:06:52,860 --> 00:06:54,440
anything you want

116
00:06:54,450 --> 00:06:56,790
four that seems yes

117
00:06:56,800 --> 00:07:00,430
that's my goal to show you how to update the numbers

118
00:07:00,470 --> 00:07:02,270
that's not to be quite straightforward

119
00:07:02,490 --> 00:07:05,560
so let's look at one particular number here

120
00:07:05,620 --> 00:07:08,230
corresponding to the past q survive

121
00:07:08,230 --> 00:07:09,800
let me show you update that

122
00:07:10,770 --> 00:07:16,380
the prediction fortescue survive in the new one step long history

123
00:07:16,400 --> 00:07:18,860
it's just by classic probability

124
00:07:18,910 --> 00:07:23,030
is the prediction of one step along the past in the old history

125
00:07:23,040 --> 00:07:24,200
divided by

126
00:07:24,200 --> 00:07:27,540
the probability of the one step past in old history

127
00:07:27,540 --> 00:07:32,170
good probabilistic model of document and we can plug in any

128
00:07:32,190 --> 00:07:37,320
probabilistic model of document in here and in principle be able to compute this kind

129
00:07:37,320 --> 00:07:38,530
of score

130
00:07:38,600 --> 00:07:45,610
OK if are items are images again we can go and buy a good model

131
00:07:45,610 --> 00:07:50,200
of features of images and plug in whatever that

132
00:07:50,260 --> 00:07:54,000
good model features of images is in the model

133
00:07:54,010 --> 00:07:58,610
the important thing is that whatever the model is that we're putting in has the

134
00:07:58,610 --> 00:07:59,900
three parameters

135
00:07:59,920 --> 00:08:01,650
that allows it to

136
00:08:01,670 --> 00:08:08,710
model different kinds of images or different kinds of documents etcetera

137
00:08:09,930 --> 00:08:12,090
that engendered work with

138
00:08:19,340 --> 00:08:21,470
well do that in a minute

139
00:08:27,450 --> 00:08:32,590
it's it's it looks alike mutual information except that

140
00:08:32,750 --> 00:08:36,190
it's not averaging with respect to x

141
00:08:36,250 --> 00:08:39,450
OK it's that some people have called

142
00:08:39,530 --> 00:08:42,840
such things pointwise mutual information

143
00:08:42,860 --> 00:08:47,270
but that is a kind of termites are occasionally

144
00:08:47,630 --> 00:08:59,960
yeah that's a very good question

145
00:09:00,020 --> 00:09:02,870
ideally what you might want to do is

146
00:09:02,890 --> 00:09:06,780
you might want to consider from our universe of items

147
00:09:06,850 --> 00:09:09,640
possible subsets

148
00:09:09,660 --> 00:09:14,170
you know in theory all possible subsets and see which one is best with our

149
00:09:14,220 --> 00:09:15,890
concept or category

150
00:09:17,480 --> 00:09:20,360
that's just much more computationally expensive

151
00:09:20,370 --> 00:09:24,110
so the cheapest thing to do is to rank the items but if you can

152
00:09:24,110 --> 00:09:26,730
afford a little bit more computation time

153
00:09:26,780 --> 00:09:29,690
then you might want to consider whether

154
00:09:29,710 --> 00:09:35,900
list of ranked items is actually cohen in with a single concept or not

155
00:09:35,910 --> 00:09:37,110
but i'm thinking

156
00:09:37,440 --> 00:09:42,110
here about a realistic information retrieval applications in which

157
00:09:42,110 --> 00:09:45,540
you send the query and you expect the answer back you know

158
00:09:45,570 --> 00:09:49,100
after your link

159
00:09:51,870 --> 00:09:55,540
so let's elaborate a little more on this criterion

160
00:09:55,590 --> 00:10:00,670
so we can write the score for items as the ratio

161
00:10:00,730 --> 00:10:02,660
of the probability of x

162
00:10:02,830 --> 00:10:06,700
given the query that divided by the probability of x

163
00:10:06,710 --> 00:10:11,640
now the probability of x given the query that we can decompose into

164
00:10:11,690 --> 00:10:14,900
the probability of x and the query that

165
00:10:14,950 --> 00:10:18,880
divided by the probability of the query that this makes it look even more like

166
00:10:18,880 --> 00:10:22,730
a mutual pointwise mutual information

167
00:10:22,780 --> 00:10:28,010
and now what we see is that the score is actually a ratio

168
00:10:28,460 --> 00:10:31,500
involving three marginal likelihood

169
00:10:32,360 --> 00:10:33,680
in the

170
00:10:33,700 --> 00:10:35,830
the numerator what we have

171
00:10:35,870 --> 00:10:40,320
is the probability of the item x and the queries

172
00:10:40,370 --> 00:10:46,260
which is a particular marginal likelihood where both the item and the queries that were

173
00:10:46,260 --> 00:10:49,350
generated from the parameters data

174
00:10:49,390 --> 00:10:52,920
and then in the denominator we have two terms one of them is that the

175
00:10:52,920 --> 00:10:54,530
marginal likelihood

176
00:10:54,540 --> 00:10:59,020
or the ACT and the other one is the marginal likelihood for the query

177
00:10:59,030 --> 00:11:01,360
but it turns out the computer core

178
00:11:01,370 --> 00:11:06,030
the only thing we need is to be able to compute marginal likelihoods for arbitrary

179
00:11:06,900 --> 00:11:09,510
from our model and given our prior

180
00:11:11,620 --> 00:11:15,190
and this is another perspective on this score

181
00:11:15,230 --> 00:11:19,140
which can be expressed through a graphical models

182
00:11:21,030 --> 00:11:23,400
what the score actually does

183
00:11:23,420 --> 00:11:26,030
it is the following

184
00:11:26,040 --> 00:11:27,800
it compares

185
00:11:29,080 --> 00:11:30,830
two hypotheses

186
00:11:32,680 --> 00:11:36,080
the data was generated

187
00:11:36,110 --> 00:11:37,810
in the numerator

188
00:11:37,870 --> 00:11:39,870
we have the hypothesis

189
00:11:42,090 --> 00:11:45,520
query set

190
00:11:45,520 --> 00:11:47,970
and the idea that we're trying to score

191
00:11:48,030 --> 00:11:49,810
were generated

192
00:11:49,860 --> 00:11:53,850
from a single setting of the parameters of our generative model

193
00:11:53,890 --> 00:11:55,290
but given that

194
00:11:55,330 --> 00:11:58,810
parameter is unknown we have integrate over possible

195
00:11:58,970 --> 00:12:01,470
adding that parameter could

196
00:12:03,010 --> 00:12:04,880
this is the generative model where

197
00:12:04,900 --> 00:12:06,280
r v

198
00:12:06,280 --> 00:12:14,600
queries that any item or scoring were generated from the same but unknown parameters data

199
00:12:14,600 --> 00:12:15,970
we're going to change

200
00:12:17,760 --> 00:12:19,130
to talk to you about

201
00:12:19,190 --> 00:12:20,270
so it's

202
00:12:20,270 --> 00:12:22,030
i also that added pressure

203
00:12:22,990 --> 00:12:26,580
barometric pressure

204
00:12:26,720 --> 00:12:28,630
if for now we

205
00:12:28,670 --> 00:12:31,150
forget gravity

206
00:12:31,200 --> 00:12:37,740
and i would have a

207
00:12:38,880 --> 00:12:40,330
closed off

208
00:12:40,350 --> 00:12:44,170
and filled with fluid gas so it could be

209
00:12:44,210 --> 00:12:47,020
a liquid

210
00:12:47,120 --> 00:12:50,240
has every a year

211
00:12:50,280 --> 00:12:54,170
and i apply force on it

212
00:12:57,020 --> 00:13:00,230
and i apply pressure

213
00:13:00,270 --> 00:13:01,880
pressure is defined

214
00:13:01,890 --> 00:13:04,420
force divided by area

215
00:13:04,460 --> 00:13:07,450
has units newton's discrimina

216
00:13:07,450 --> 00:13:08,700
it is also called

217
00:13:08,710 --> 00:13:10,090
of course

218
00:13:10,130 --> 00:13:13,680
one of the first i mean there's one

219
00:13:13,790 --> 00:13:16,120
in the absence of gravity

220
00:13:17,870 --> 00:13:19,920
is everywhere

221
00:13:19,930 --> 00:13:21,320
in this special

222
00:13:21,340 --> 00:13:22,600
the same

223
00:13:22,710 --> 00:13:24,560
and that's what's called

224
00:13:24,600 --> 00:13:27,490
articles principle

225
00:13:27,510 --> 00:13:39,540
cost principle says that the pressure applied to to an enclosed fluid

226
00:13:39,650 --> 00:13:42,540
is transmitted undiminished

227
00:13:42,650 --> 00:13:45,040
to every point in the fluid

228
00:13:45,040 --> 00:13:46,400
into the walls

229
00:13:46,430 --> 00:13:48,990
of the container

230
00:13:49,060 --> 00:13:52,090
keep in mind pressure is the scalar

231
00:13:52,100 --> 00:13:54,870
it has no direction

232
00:13:55,960 --> 00:13:58,650
has direction

233
00:13:58,680 --> 00:14:01,450
and the force exerted by the fluid

234
00:14:01,460 --> 00:14:04,590
on anything therefore also on the wall

235
00:14:04,590 --> 00:14:08,400
must be everywhere perpendicular to the wall

236
00:14:08,430 --> 00:14:11,480
because if there were any tangential component

237
00:14:11,540 --> 00:14:13,930
then the fluid would start to move

238
00:14:13,990 --> 00:14:16,810
actually equals minus reaction so it starts to move

239
00:14:16,870 --> 00:14:18,640
and we are talking about

240
00:14:18,680 --> 00:14:20,590
static fluid

241
00:14:20,620 --> 00:14:22,060
so if i take any

242
00:14:22,080 --> 00:14:24,990
element i take one year at the surface

243
00:14:25,050 --> 00:14:27,060
little element delta a

244
00:14:27,160 --> 00:14:31,810
and the force must be perpendicular to that surface

245
00:14:31,870 --> 00:14:34,250
delta s

246
00:14:34,340 --> 00:14:36,430
so delta have

247
00:14:36,470 --> 00:14:38,240
by delta a

248
00:14:38,300 --> 00:14:41,910
in the limiting case

249
00:14:41,960 --> 00:14:43,770
delta a goes to zero

250
00:14:43,810 --> 00:14:51,910
it is then that pressure

251
00:14:53,060 --> 00:14:55,300
has some truly amazing

252
00:14:55,310 --> 00:14:57,650
consequences which are by no means

253
00:14:57,650 --> 00:15:02,900
so intuitive

254
00:15:02,910 --> 00:15:05,360
this is the idea of an hydraulic

255
00:15:08,680 --> 00:15:16,300
i have here

256
00:15:16,340 --> 00:15:18,500
that's always has very

257
00:15:18,590 --> 00:15:25,470
curious shape

258
00:15:29,710 --> 00:15:31,970
and let be here

259
00:15:32,090 --> 00:15:33,490
based on on it

260
00:15:33,500 --> 00:15:34,770
area a one

261
00:15:39,220 --> 00:15:44,220
for every eight

262
00:15:44,240 --> 00:15:46,210
it's filled with liquid

263
00:15:50,080 --> 00:15:53,460
and i apply here force

264
00:15:53,500 --> 00:15:54,560
f one

265
00:15:57,090 --> 00:15:58,240
before after

266
00:15:58,440 --> 00:16:04,060
so the pressure that i apply here

267
00:16:04,110 --> 00:16:05,180
is f one

268
00:16:05,270 --> 00:16:10,120
divided by a one according to pass call everywhere in the fluid

269
00:16:10,140 --> 00:16:11,890
that pressure must be the same

270
00:16:11,910 --> 00:16:12,940
for now

271
00:16:12,960 --> 00:16:16,970
i just assume that the effect of gravity which i will discuss shortly

272
00:16:17,060 --> 00:16:18,340
doesn't change

273
00:16:18,400 --> 00:16:19,620
the situation

274
00:16:19,620 --> 00:16:21,460
very significantly

275
00:16:21,470 --> 00:16:24,180
i will address the gravity very shortly

276
00:16:24,220 --> 00:16:27,920
so the pressure then will be the same everywhere

277
00:16:28,610 --> 00:16:32,350
the pressure due to this site is f two divided by a two

278
00:16:32,400 --> 00:16:35,940
so too must be the same

279
00:16:35,960 --> 00:16:40,040
if the liquid is not moving

280
00:16:40,090 --> 00:16:42,450
so what that means is that is a two

281
00:16:42,480 --> 00:16:44,400
of a one

282
00:16:44,460 --> 00:16:46,540
there were one hundred

283
00:16:46,590 --> 00:16:48,400
it would means that this force

284
00:16:48,580 --> 00:16:51,090
three hundred times less than that one

285
00:16:51,160 --> 00:16:54,220
in other words i could put on here the weights

286
00:16:54,230 --> 00:16:56,360
the last ten kilograms

287
00:16:56,380 --> 00:16:59,140
and he i could put a thousand kilograms

288
00:16:59,150 --> 00:17:01,130
and it will be completing an equally well

289
00:17:01,160 --> 00:17:03,160
that's not so intuitive

290
00:17:03,210 --> 00:17:04,450
this is using

291
00:17:04,530 --> 00:17:06,710
all the garages

292
00:17:06,770 --> 00:17:09,100
what they do is they put on top of this

293
00:17:09,160 --> 00:17:11,940
i believe that up here

294
00:17:11,950 --> 00:17:13,790
so this is this platform

295
00:17:13,880 --> 00:17:15,530
there's around here

296
00:17:15,540 --> 00:17:20,090
and on top of it is car

297
00:17:20,200 --> 00:17:24,090
and someone pushes here

298
00:17:24,140 --> 00:17:25,140
and then

299
00:17:25,170 --> 00:17:26,840
this goes up

300
00:17:26,840 --> 00:17:29,170
the cargo so

301
00:17:29,270 --> 00:17:30,830
if i push here

302
00:17:30,880 --> 00:17:35,410
with a four-cylinder with more than ten kilograms so that would be hundred newtons

303
00:17:35,540 --> 00:17:37,910
this level will go up

304
00:17:38,010 --> 00:17:39,840
so your first thought maybe g

305
00:17:39,840 --> 00:17:42,800
isn't that a violation of the conservation of energy

306
00:17:42,820 --> 00:17:46,830
isn't it time and i'm not going to be getting something for nothing

307
00:17:46,880 --> 00:17:49,260
well not really

308
00:17:49,300 --> 00:17:53,300
suppose i push this down over the distance d one

309
00:17:53,320 --> 00:17:57,360
then the amount of fluid that i place is the volume

310
00:17:57,390 --> 00:17:59,610
it's a one times the one

311
00:17:59,660 --> 00:18:02,280
that through its ends up here

312
00:18:02,340 --> 00:18:03,840
so this one will go up

313
00:18:03,840 --> 00:18:06,840
over the distance d two

314
00:18:06,960 --> 00:18:09,030
but the same amount of through the

315
00:18:09,100 --> 00:18:10,160
these here

316
00:18:10,170 --> 00:18:11,350
and there

317
00:18:11,460 --> 00:18:12,700
in other words

318
00:18:12,710 --> 00:18:14,260
a one b one

319
00:18:14,270 --> 00:18:15,600
must be a true

320
00:18:15,780 --> 00:18:18,630
the two

321
00:18:20,260 --> 00:18:22,290
if the force here

322
00:18:22,450 --> 00:18:23,650
hundred times

323
00:18:23,700 --> 00:18:27,340
less than the force there

324
00:18:27,390 --> 00:18:29,020
the work

325
00:18:29,190 --> 00:18:31,860
i'm doing on the left side

326
00:18:32,010 --> 00:18:33,500
f one

327
00:18:33,510 --> 00:18:36,650
times the one

328
00:18:36,700 --> 00:18:39,760
if the force he was hundred times less than that

329
00:18:40,690 --> 00:18:43,040
distance that i move

330
00:18:43,070 --> 00:18:45,950
is hundred times larger than the two

331
00:18:45,960 --> 00:18:48,730
because a two over a one is one hundred

332
00:18:48,780 --> 00:18:50,510
so f one the one

333
00:18:50,530 --> 00:18:52,100
it will be the same

334
00:18:52,150 --> 00:18:54,840
as f two d two

335
00:18:54,840 --> 00:18:56,080
a hundred times

336
00:18:56,090 --> 00:18:57,340
lower force

337
00:18:57,390 --> 00:18:59,730
but over one hundred times larger distance

338
00:18:59,730 --> 00:19:02,830
it is maximizing the edge

339
00:19:02,850 --> 00:19:09,160
and depending on that we kind of smaller or larger much

340
00:19:09,180 --> 00:19:11,420
OK and that's actually a wide range i mean

341
00:19:11,440 --> 00:19:13,660
so it can be small can aligned

342
00:19:13,690 --> 00:19:19,000
however when you choose this parameter role

343
00:19:22,310 --> 00:19:22,980
you see

344
00:19:24,140 --> 00:19:27,850
this region becomes very small so i mean when you choose wrote i mean let's

345
00:19:27,850 --> 00:19:32,730
say the maximum margin is zero point four individuals role close to resume point four

346
00:19:32,730 --> 00:19:37,350
then this difference between the performance of the best in the worst

347
00:19:37,480 --> 00:19:40,370
case performance becomes really small

348
00:19:46,730 --> 00:19:50,640
so we can ensure that the most might not converge because essentially the base learner

349
00:19:50,960 --> 00:19:57,230
if it's really so it can go like first this i mean first be corporate

350
00:19:57,230 --> 00:19:58,000
if later

351
00:19:58,350 --> 00:20:04,210
b i mean performed first then goes back and forth between these two

352
00:20:04,230 --> 00:20:06,310
case so and the

353
00:20:06,440 --> 00:20:11,600
the function does not converge

354
00:20:11,620 --> 00:20:13,620
OK so

355
00:20:18,310 --> 00:20:19,620
OK so here's some

356
00:20:19,620 --> 00:20:27,600
results of how to get to the full of adaboost stop but thinking about it

357
00:20:27,750 --> 00:20:29,830
in the material

358
00:20:29,830 --> 00:20:31,190
i skip limit here

359
00:20:31,190 --> 00:20:34,680
just show you the final version of

360
00:20:34,680 --> 00:20:36,230
OK so

361
00:20:36,660 --> 00:20:40,480
that is again just a slight modification of adaboost so

362
00:20:40,500 --> 00:20:44,520
the main difference is that we don't have to specify this parameter role in the

363
00:20:44,520 --> 00:20:49,640
beginning but now this parameter always adapted during the iterations

364
00:20:49,660 --> 00:20:54,710
so essentially the look at the edge of the base learners so these gamma t

365
00:20:55,310 --> 00:20:57,480
take the minimum of all those actions

366
00:20:57,770 --> 00:20:59,620
subtract the limit

367
00:20:59,640 --> 00:21:03,500
and you see that in in that are of computation

368
00:21:03,520 --> 00:21:11,710
OK so the difference to all i did was throw is just that the adaptively

369
00:21:11,710 --> 00:21:13,540
choose this committee

370
00:21:13,600 --> 00:21:23,000
OK so now you might ask does large margin help

371
00:21:23,730 --> 00:21:25,940
and here is one example that helps so

372
00:21:25,960 --> 00:21:27,920
it really depends on the dataset

373
00:21:27,940 --> 00:21:31,370
OK so this is a hundred dimensional data set

374
00:21:31,390 --> 00:21:36,020
and we have only to discover that features and these two discriminative features look like

375
00:21:36,690 --> 00:21:39,060
and all the other dimensions are essentially noise

376
00:21:39,080 --> 00:21:42,750
so you see that there are separated they are not very well separated but they

377
00:21:45,190 --> 00:21:48,290
and then we used c four point five

378
00:21:48,290 --> 00:21:52,190
adaboost and precursor version of this article started by this

379
00:21:52,330 --> 00:21:53,830
doing essentially the same

380
00:21:53,850 --> 00:21:59,230
so when we run c four point five we measure the generalisation error seven point

381
00:21:59,230 --> 00:22:00,810
four percent

382
00:22:01,460 --> 00:22:03,910
the average in two hundred runs

383
00:22:03,960 --> 00:22:08,750
when you're on adaboost with c four point five then you get a generalisation error

384
00:22:08,770 --> 00:22:11,210
four percent

385
00:22:12,640 --> 00:22:16,810
the margin is as you point three which was achieved

386
00:22:17,080 --> 00:22:22,600
and when you run this extended better version of articles which achieves the larger margin

387
00:22:22,600 --> 00:22:25,410
and the at the margin of viewpoint point pi eight

388
00:22:25,460 --> 00:22:28,250
and we find that the genes asian error

389
00:22:28,480 --> 00:22:31,830
goes down to three point six percent is not a very big difference but it's

390
00:22:34,460 --> 00:22:37,980
so the large margin can improve the generalisation performance

391
00:22:38,000 --> 00:22:40,540
however many have noisy data

392
00:22:40,600 --> 00:22:45,830
then might not so empirically it i mean many if noisy data then you might

393
00:22:45,850 --> 00:22:49,520
do something else might want to do something else

394
00:22:53,500 --> 00:22:59,790
OK it essentially what what i was doing so this is tries to solve this

395
00:23:00,230 --> 00:23:06,330
this optimisation problem so we try to maximize this variable role such that the margin

396
00:23:06,330 --> 00:23:07,890
of all the examples

397
00:23:07,890 --> 00:23:09,520
are larger than role

398
00:23:11,350 --> 00:23:17,640
we have these the some of the output is this one and they critical because

399
00:23:19,120 --> 00:23:22,160
so this is the linear optimisation problem

400
00:23:22,160 --> 00:23:23,870
so this is

401
00:23:23,940 --> 00:23:30,250
some which goes over all hypotheses posts so here i assumed it's just fine the

402
00:23:30,250 --> 00:23:34,310
number of courses and as i said it's just j

403
00:23:34,420 --> 00:23:37,830
OK so this is normal linear programme

404
00:23:37,830 --> 00:23:41,790
so the dual optimisation problem just looks like this so we try to find some

405
00:23:42,020 --> 00:23:47,920
excellent such that the edge of all hypotheses is smaller than epsilon and we have

406
00:23:47,920 --> 00:23:51,640
to find the distribution d which is critical zero and the sum to one

407
00:23:51,660 --> 00:23:53,850
OK so this the dual optimisation problem

408
00:23:53,910 --> 00:23:55,940
you can compute this from this

409
00:23:55,960 --> 00:24:00,680
and we have a solution to this one then you get a solution

410
00:24:00,750 --> 00:24:05,600
so this is for finding fuzzy sets so you can generalize this now to infinite

411
00:24:05,600 --> 00:24:12,160
process it's adaboost stuff star and i actually be they all solve this

412
00:24:12,210 --> 00:24:14,290
even when the number of crosses

413
00:24:14,310 --> 00:24:16,020
it is infinite

414
00:24:16,040 --> 00:24:19,520
so here we have infinite sums

415
00:24:20,230 --> 00:24:24,350
so we can just use adaboost and other will actually be and all these other

416
00:24:24,350 --> 00:24:28,140
things we can use force holding such a linear programs

417
00:24:28,160 --> 00:24:35,460
so these seamy infinite linear programs because there infinitely many constraints or he has infinitely

418
00:24:35,460 --> 00:24:39,100
many constraints or infinitely many variables

419
00:24:39,160 --> 00:24:44,310
OK so this is just something interesting in itself

420
00:24:44,410 --> 00:24:49,690
so adaboost did not depend on the number of did not depend on the size

421
00:24:49,690 --> 00:24:51,190
of the force sent

422
00:24:51,210 --> 00:24:55,250
i really didn't care about about that is infinitely many constraints so the just go

423
00:24:55,270 --> 00:24:57,160
through everything works

424
00:24:57,210 --> 00:25:04,540
so this is a solution method for solving this optimisation problems

425
00:25:09,830 --> 00:25:21,020
so when you consider that most are so converges to the solution of this linear

426
00:25:21,020 --> 00:25:25,850
so what this does is that it gradually expands the network out from the query

427
00:25:25,850 --> 00:25:29,290
nodes until it bumps into evidence

428
00:25:29,320 --> 00:25:32,210
and of course in the worst case you know this will reduce the whole world

429
00:25:32,210 --> 00:25:36,520
but more often you know there will be something boundary evidence that separates the query

430
00:25:36,520 --> 00:25:39,920
from you know the vast majority of atoms out there and you get a big

431
00:25:39,920 --> 00:25:41,960
efficiency gains

432
00:25:42,160 --> 00:25:45,920
there no however in this case as well

433
00:25:45,940 --> 00:25:47,460
and the snake is the following

434
00:25:47,500 --> 00:25:52,400
is we want to be able to subsume both probabilistic and logical inference which means

435
00:25:52,400 --> 00:25:55,970
we have to be able to deal with deterministic dependencies

436
00:25:56,020 --> 00:26:01,220
and i was like MCMC and belief propagation can handle deterministic dependencies

437
00:26:01,250 --> 00:26:03,970
for example in the case of gibbs sampling is easy to see why it breaks

438
00:26:05,070 --> 00:26:08,940
if i deterministic dependencies what that means is that there are some possible worlds over

439
00:26:09,710 --> 00:26:13,010
and there some possible worlds over here but they don't communicate

440
00:26:13,020 --> 00:26:16,840
so i can start if i stuck my markov chain over here else sample over

441
00:26:16,840 --> 00:26:21,270
here forever and ever jump over there and it into their get the wrong probabilities

442
00:26:21,280 --> 00:26:24,350
even if all i have is very strong dependencies

443
00:26:24,390 --> 00:26:27,390
what that means is that yes i could go from here to here but i

444
00:26:27,390 --> 00:26:30,540
have to take on a bunch of unlikely steps to go from one to the

445
00:26:31,510 --> 00:26:35,880
you know the more steps i have to take the exponential less likely exponentially longer

446
00:26:35,880 --> 00:26:38,690
time to go from one to the other side in practice what happened

447
00:26:38,720 --> 00:26:43,770
and this is in fact what makes probabilistic inference five minutes long-standing problem

448
00:26:43,820 --> 00:26:46,320
and here again there's something that we can do

449
00:26:46,380 --> 00:26:51,890
by combining ideas from probability and logic which is to have an MCMC algorithm where

450
00:26:51,900 --> 00:26:57,100
the the next step is actually obtained by running a SAT solver

451
00:26:57,110 --> 00:27:01,650
sets of are very good at finding isolated you know non-zero probability regions of space

452
00:27:01,650 --> 00:27:03,490
that's what they're for

453
00:27:03,500 --> 00:27:06,650
so you can use the sets of and very quickly jump between modes whereas give

454
00:27:06,650 --> 00:27:08,680
sampling with basically take forever

455
00:27:08,740 --> 00:27:13,280
course there's some details in making this work correctly but they can be figured out

456
00:27:13,290 --> 00:27:18,600
so this the so called MC SAT that experimentally is just many orders of magnitude

457
00:27:18,600 --> 00:27:23,520
faster than gives an even you know more sophisticated MCMC algorithms designed to deal with

458
00:27:23,520 --> 00:27:26,320
this problem of multiple isolated nodes

459
00:27:26,350 --> 00:27:28,610
i won't go into the details of it here

460
00:27:28,650 --> 00:27:33,510
but it's certainly not itself with things like and cystatin said basically underlie our ability

461
00:27:33,770 --> 00:27:37,670
to do this type of modelling in domains of realistic size today

462
00:27:37,690 --> 00:27:40,930
OK what about learning

463
00:27:41,020 --> 00:27:45,950
well learning for learning in our data instead of the single relation is going to

464
00:27:45,950 --> 00:27:49,300
be a relational database with an arbitrary number of

465
00:27:49,360 --> 00:27:50,490
relations in

466
00:27:50,510 --> 00:27:55,890
i'm going to keep making the assumption that there is no missing data which you

467
00:27:55,890 --> 00:27:58,850
know here in this case what this means is that i'm going to make a

468
00:27:58,850 --> 00:28:00,350
closed world assumption

469
00:28:00,360 --> 00:28:04,540
on the database that i'm using for learning meaning any atom that is not in

470
00:28:04,540 --> 00:28:06,540
that it was assumed to be false

471
00:28:07,340 --> 00:28:11,020
in many cases this is inappropriate assumption one it's not that i need to use

472
00:28:11,020 --> 00:28:14,130
the and versions of these algorithms and again they are available but in this tutorial

473
00:28:14,150 --> 00:28:15,610
going ignore decision

474
00:28:16,510 --> 00:28:20,650
and of course there's going to be again learning parameters i e weights learning structure

475
00:28:20,650 --> 00:28:22,970
i e formulas as before

476
00:28:22,990 --> 00:28:25,090
so how do the weight learning

477
00:28:25,110 --> 00:28:28,150
just as we did for markov networks

478
00:28:28,160 --> 00:28:30,750
we have the same gradient descent procedure another the

479
00:28:30,760 --> 00:28:33,930
the gradient is just the difference between the number of times the clauses true in

480
00:28:33,930 --> 00:28:38,520
the data and the expected number of closed times it's true according to the MLE

481
00:28:38,560 --> 00:28:41,040
it is exactly the same way as before

482
00:28:41,100 --> 00:28:45,140
we're just doing it now on a non IID world forwarding it on an IB

483
00:28:45,140 --> 00:28:48,910
world but the beautiful thing is that the masterworks of the same

484
00:28:48,920 --> 00:28:53,980
so can actually handle this complicated dependencies with at some level no extra work conceptually

485
00:28:53,980 --> 00:28:57,070
of course computationally it could be a lot more expensive

486
00:28:57,100 --> 00:29:01,150
and not to the generative learning we can use the likelihood as before

487
00:29:01,160 --> 00:29:04,670
into the discriminative learning we can use conditional likelihood as before

488
00:29:04,680 --> 00:29:06,270
and now for inference

489
00:29:06,270 --> 00:29:07,310
we can use

490
00:29:07,320 --> 00:29:08,670
MC SAT

491
00:29:08,680 --> 00:29:10,790
to compute these expectations or

492
00:29:10,810 --> 00:29:14,390
we can use maxwalksat to compute the most likely state and then to the counts

493
00:29:14,390 --> 00:29:15,220
in there

494
00:29:15,520 --> 00:29:21,720
so very straightforward extension of the algorithms for markov networks what about structure learning

495
00:29:21,770 --> 00:29:24,820
well again this is a very straightforward the extension of feature

496
00:29:24,850 --> 00:29:27,000
induction for markov networks

497
00:29:27,010 --> 00:29:32,110
and it it's really kind of inductive logic programming like we're trying to learn clauses

498
00:29:32,130 --> 00:29:36,150
x so in principle you could use any for this

499
00:29:36,160 --> 00:29:37,970
except for a couple of things

500
00:29:37,990 --> 00:29:39,070
one is that

501
00:29:39,090 --> 00:29:42,930
most at the other end up for learning horn clauses and he won induce any

502
00:29:42,930 --> 00:29:44,620
clauses not just horn clauses

503
00:29:44,640 --> 00:29:48,630
so we need something a bit more general and the other one is that

504
00:29:48,640 --> 00:29:52,110
p with a simple have something like accuracy coverage

505
00:29:52,120 --> 00:29:54,890
no information gain is evaluation measure

506
00:29:54,900 --> 00:29:56,810
and that's not appropriate here

507
00:29:56,870 --> 00:30:00,200
because you are trying to model the probability distribution if you do if you use

508
00:30:00,200 --> 00:30:03,000
those criteria you get results

509
00:30:03,010 --> 00:30:06,910
so we actually one is something like the likelihood or you know some posterior probability

510
00:30:07,230 --> 00:30:10,490
as the evaluation function

511
00:30:10,500 --> 00:30:13,560
now this as

512
00:30:13,570 --> 00:30:17,380
so conceptually can take an LP with within generalize it to handle you know any

513
00:30:17,380 --> 00:30:22,310
clauses user likely because the russian function and you have an organ for learning alliance

514
00:30:22,360 --> 00:30:26,720
the problem though is that this is going to be hideously slow

515
00:30:26,730 --> 00:30:28,480
there's is going to be hideously slow

516
00:30:28,500 --> 00:30:29,360
is that

517
00:30:30,270 --> 00:30:31,340
as before

518
00:30:31,410 --> 00:30:35,430
we can little we need to learn weights for each candidate clauses any time i

519
00:30:35,430 --> 00:30:36,680
what is that

520
00:30:36,770 --> 00:30:42,910
that's sophie fy x that's right depends only on the variables in x of s

521
00:30:43,000 --> 00:30:46,710
what can we say about this guy here

522
00:30:46,770 --> 00:30:49,330
the term indexed by a

523
00:30:49,430 --> 00:30:53,450
right you can only depend on things in a and s

524
00:30:53,500 --> 00:30:58,020
right that's again using

525
00:30:58,040 --> 00:30:59,870
part of the cutset property

526
00:30:59,890 --> 00:31:05,190
right there cannot be any connection between a and b so there's no way that

527
00:31:05,480 --> 00:31:10,270
be term will somehow pull me and this guy can only depend on

528
00:31:11,140 --> 00:31:13,290
much of these different names this is as

529
00:31:13,330 --> 00:31:16,710
this is a s and this is fee

530
00:31:19,000 --> 00:31:21,290
so that depends only on bns

531
00:31:31,540 --> 00:31:32,660
OK so

532
00:31:32,680 --> 00:31:36,790
you can tell me why that's useful so what we sort of done is we've

533
00:31:36,790 --> 00:31:40,350
just taken our factorisation we've collapsed it into three terms

534
00:31:40,430 --> 00:31:44,950
one for a one for b and one for the overlap there

535
00:31:49,930 --> 00:31:54,600
right so if you remember what we're trying to prove right what we're trying to

536
00:31:54,600 --> 00:31:57,710
prove is this guy right here we're trying to prove that when we condition on

537
00:31:57,710 --> 00:31:59,120
x of s

538
00:31:59,120 --> 00:32:02,160
then it becomes the product of two terms

539
00:32:02,190 --> 00:32:03,350
so i won't

540
00:32:03,480 --> 00:32:07,120
right out the algebra leave that for you guys but essentially the proof is done

541
00:32:07,120 --> 00:32:12,600
because what happens now is if you take this new condition you're going fix

542
00:32:12,620 --> 00:32:16,230
this this become some constant that you don't care about

543
00:32:16,250 --> 00:32:18,560
you fix some of these variables in

544
00:32:18,580 --> 00:32:20,540
fix these variables here

545
00:32:20,560 --> 00:32:24,040
but after conditioning what you have is just to separate product of a

546
00:32:24,060 --> 00:32:29,060
in terms indexed by b so you can see exactly the splitting of factors conditioning

547
00:32:29,060 --> 00:32:33,660
you get exactly the splitting that you wanted to somehow that decomposition that was the

548
00:32:33,660 --> 00:32:40,160
key in this particular argument and of course the decomposition that was using this claim

549
00:32:40,160 --> 00:32:44,350
that but you should verify that we did in fact have disjoint union that was

550
00:32:44,350 --> 00:32:47,540
using the fact that we had a vertex cut set

551
00:32:47,640 --> 00:32:51,140
OK so that some

552
00:32:51,190 --> 00:32:57,350
that's all it shows is one direction that shows us the factorisation property

553
00:32:57,390 --> 00:33:00,000
implies markov so summarizing

554
00:33:01,180 --> 00:33:02,330
if we

555
00:33:03,500 --> 00:33:08,690
what you'll see is that px

556
00:33:08,710 --> 00:33:11,680
a x b if we condition on x of s

557
00:33:11,690 --> 00:33:13,100
i right

558
00:33:13,120 --> 00:33:16,000
proportional to number snowdrop things that

559
00:33:16,020 --> 00:33:17,290
depend only

560
00:33:17,290 --> 00:33:19,290
on as because those

561
00:33:19,420 --> 00:33:24,190
conditioning those are fixed i'm to get something that's basically proportional to

562
00:33:24,310 --> 00:33:27,120
fy a s

563
00:33:27,140 --> 00:33:29,770
x a

564
00:33:29,830 --> 00:33:33,890
put bar because we're conditioning on it and

565
00:33:35,190 --> 00:33:39,930
and vx p access

566
00:33:39,960 --> 00:33:44,770
i to be some constants floating around but we don't care about those

567
00:33:44,770 --> 00:33:50,290
that's enough to show is that this system breaks into the product we wanted

568
00:33:50,310 --> 00:33:59,100
OK so any questions about that

569
00:34:05,060 --> 00:34:07,180
OK so that actually was the

570
00:34:07,250 --> 00:34:10,680
the easier direction as you might have thought

571
00:34:13,100 --> 00:34:16,950
probably just in the interest of time i won't prove the other direction because it

572
00:34:16,950 --> 00:34:19,210
would take a fair bit of work but i can sort of tell you how

573
00:34:19,210 --> 00:34:20,330
it works

574
00:34:20,480 --> 00:34:24,680
the other direction is is a bit harder so if you look at the assumption

575
00:34:25,000 --> 00:34:31,500
the assumption of the theorem include that you have a strictly positive distribution

576
00:34:31,520 --> 00:34:32,870
but this argument

577
00:34:32,890 --> 00:34:36,540
i mean you might worry a little bit about dividing by zero when i condition

578
00:34:36,640 --> 00:34:40,960
but if you look carefully at this argument it actually holds for any distribution can

579
00:34:41,120 --> 00:34:44,730
you hold for distributions that might assign zero probability to some

580
00:34:44,750 --> 00:34:50,230
on configurations but the reverse that if you have a markov you have the markov

581
00:34:51,190 --> 00:34:56,600
this factorisation property doesn't always hold unless the distribution is strictly positive

582
00:34:56,620 --> 00:35:00,830
so if you look in the book by lawrence and it's book called graphical models

583
00:35:00,890 --> 00:35:05,870
you can find all these sort of weird kind of examples

584
00:35:05,890 --> 00:35:08,520
what the proof uses it uses

585
00:35:08,540 --> 00:35:12,190
something called the mobius inversion formula

586
00:35:12,210 --> 00:35:23,620
so this is what i said it certainly needs this assumption that p of x

587
00:35:23,620 --> 00:35:26,520
is bigger than zero

588
00:35:26,540 --> 00:35:31,960
it's it's a nice proof it's kind of combinatorial uses something called obvious inversion

589
00:35:31,980 --> 00:35:37,460
and i won't go into details on it here

590
00:35:39,540 --> 00:35:40,430
you can now

591
00:35:40,480 --> 00:35:43,000
you can read it in the proof of this

592
00:35:46,960 --> 00:35:49,160
and so in the

593
00:35:49,210 --> 00:35:52,930
the directory that i set up for this course on the first page of slides

594
00:35:53,060 --> 00:35:59,370
the web address i left a set of introductory lectures that actual go well

595
00:35:59,390 --> 00:36:01,770
review this proof that i just did but also

596
00:36:01,770 --> 00:36:05,950
state them obvious inversion and go for the backward direction too

597
00:36:06,140 --> 00:36:08,790
so if you're interested you can have a look at that if you have never

598
00:36:08,790 --> 00:36:11,790
seen it i i think it's worth looking at just once because

599
00:36:11,830 --> 00:36:16,620
while historically it's interesting and it's nice argument it's it's nice to understand why these

600
00:36:16,620 --> 00:36:18,890
two things are are equivalent

601
00:36:18,910 --> 00:36:23,180
OK so any questions

602
00:36:23,180 --> 00:36:47,430
OK so

603
00:36:47,520 --> 00:36:51,120
what i'd like to start talking about now and what will spend a lot of

604
00:36:51,120 --> 00:36:53,160
tomorrow morning talking about is

605
00:36:53,180 --> 00:37:00,460
more the the kinds of computational questions that arise when you apply a graphical model

606
00:37:00,520 --> 00:37:04,560
right we've already seen some instances of the kinds of computational problems that you'd like

607
00:37:04,560 --> 00:37:05,560
to solve

608
00:37:05,560 --> 00:37:12,790
one of them you'd like to solve is is basically a sort of simply stated

609
00:37:12,790 --> 00:37:17,230
it's basically just a giant summation or a giant in general

610
00:37:18,750 --> 00:37:23,370
i haven't said much so far about what this mysterious c quantity is

611
00:37:23,370 --> 00:37:26,600
the state space with the same size for example

612
00:37:28,890 --> 00:37:32,830
the complexity of computing this thing will be basically

613
00:37:32,870 --> 00:37:36,480
s times at times times at times s

614
00:37:36,540 --> 00:37:38,540
o two n

615
00:37:38,620 --> 00:37:41,370
we have asked again

616
00:37:41,370 --> 00:37:42,620
i mean there's no way

617
00:37:42,640 --> 00:37:47,350
in practice we were talking this thing the images and you have millions of

618
00:37:49,250 --> 00:37:51,710
or these things may be next

619
00:37:51,730 --> 00:37:53,270
all these things maybe

620
00:37:53,290 --> 00:37:55,350
speech signal

621
00:37:55,460 --> 00:37:57,040
all these things maybe

622
00:37:57,100 --> 00:38:02,600
particles of physical system there's no way you can actually compute anything

623
00:38:02,650 --> 00:38:03,870
if you

624
00:38:03,920 --> 00:38:06,620
just one

625
00:38:08,120 --> 00:38:13,730
every element of your sample space from your original representation k

626
00:38:13,770 --> 00:38:15,520
just giving an example

627
00:38:15,540 --> 00:38:17,870
if you wanted to

628
00:38:17,870 --> 00:38:23,000
ask both types of queries like some probabilistic calculations

629
00:38:23,020 --> 00:38:25,540
in the model

630
00:38:25,580 --> 00:38:28,390
in any realistic

631
00:38:28,410 --> 00:38:30,020
you be able to do it

632
00:38:30,140 --> 00:38:35,000
so you we need some type of compact representation for p

633
00:38:35,020 --> 00:38:38,020
right because if you going to compute these

634
00:38:38,040 --> 00:38:40,710
is not practical we need some

635
00:38:40,730 --> 00:38:46,250
some type of compact representation for in what graphical models is gives you is precisely

636
00:38:46,870 --> 00:38:49,560
that some of these you

637
00:38:50,710 --> 00:38:53,120
nice structure for p

638
00:38:53,120 --> 00:38:57,080
that allows you to perform this calculation

639
00:38:57,120 --> 00:39:00,790
in nice inefficient man ac

640
00:39:00,810 --> 00:39:08,540
OK for many types of graphical

641
00:39:12,540 --> 00:39:15,020
that's the critical observation again

642
00:39:15,040 --> 00:39:16,730
when the

643
00:39:16,790 --> 00:39:21,830
compactness of the model or when the structure of the model

644
00:39:21,870 --> 00:39:27,770
arises from conditional independence statements involving the viable

645
00:39:28,770 --> 00:39:30,460
graphical models

646
00:39:30,520 --> 00:39:32,440
these what you need

647
00:39:32,520 --> 00:39:34,170
to for the screen

648
00:39:35,460 --> 00:39:37,960
and and this is an important point

649
00:39:37,980 --> 00:39:42,390
graphical models have been used for many types of applications

650
00:39:42,440 --> 00:39:45,650
and they are extremely helpful extremely important

651
00:39:45,770 --> 00:39:49,150
but we need to understand their limitations as well

652
00:39:49,210 --> 00:39:51,150
i mean

653
00:39:51,170 --> 00:39:55,190
they are not the right model for any type of the probabilistic structure

654
00:39:56,170 --> 00:39:58,690
so graphical models are used

655
00:39:58,690 --> 00:40:00,790
in these types of cases

656
00:40:00,810 --> 00:40:12,290
when the structure of your probabilistic space is given by conditional independencies among the online

657
00:40:12,330 --> 00:40:13,750
there's not mean

658
00:40:13,750 --> 00:40:15,330
that's the only

659
00:40:15,540 --> 00:40:23,440
interesting structure that you may have and you probably

660
00:40:23,540 --> 00:40:25,230
for example you may have

661
00:40:25,230 --> 00:40:28,060
structure our probabilistic space that i

662
00:40:28,230 --> 00:40:31,210
probably six space maybe structural according to

663
00:40:31,230 --> 00:40:35,290
patterns between the limitations of the body

664
00:40:35,350 --> 00:40:38,520
she does not have to do with a particular

665
00:40:38,540 --> 00:40:40,330
viable stem cells but we

666
00:40:40,370 --> 00:40:43,420
all the valuables at the same time

667
00:40:43,440 --> 00:40:46,460
immediately graphical models but they don't seem

668
00:40:46,500 --> 00:40:47,350
to be

669
00:40:47,370 --> 00:40:51,640
the right tools to study these things in the present study

670
00:40:51,640 --> 00:40:52,770
o situation

671
00:40:52,770 --> 00:40:55,670
so i mean we should understand that

672
00:40:55,670 --> 00:41:02,500
graphical models basically really talking about structural problems structured probabilistic models when these

673
00:41:02,580 --> 00:41:04,540
structure arises from

674
00:41:04,560 --> 00:41:07,520
conditional independence

675
00:41:07,540 --> 00:41:11,940
of course been talking alot about conditional independence so had to convince you that

676
00:41:11,960 --> 00:41:16,600
this is a useful concept that actually realized the real problem

677
00:41:16,640 --> 00:41:19,120
many of them

678
00:41:20,210 --> 00:41:21,750
on the idea of

679
00:41:21,770 --> 00:41:24,060
conditional independence

680
00:41:25,920 --> 00:41:28,290
this is

681
00:41:28,310 --> 00:41:31,390
the shortest possible introduction i i could

682
00:41:31,670 --> 00:41:34,480
prepare for what's going to come

683
00:41:35,290 --> 00:41:36,830
i will start

684
00:41:37,600 --> 00:41:42,750
already before the break because we still have about twenty minutes before the first

685
00:41:42,750 --> 00:41:43,920
made some

686
00:41:43,940 --> 00:41:47,630
approximations and what was the approximation we use just the

687
00:41:47,670 --> 00:41:54,360
the class approximation

688
00:41:55,630 --> 00:41:57,940
someone as long the question

689
00:41:57,960 --> 00:42:03,730
but multiple classes had support vector machines deal with multiple classes

690
00:42:03,800 --> 00:42:07,270
and i'm sure you've all been wondering the same about this how do we deal

691
00:42:07,270 --> 00:42:10,670
with multiple classes and this sort of setting

692
00:42:10,670 --> 00:42:12,820
so what i'm going to do is i'm going to

693
00:42:14,270 --> 00:42:16,670
a way of doing gaussian process

694
00:42:18,130 --> 00:42:22,340
over multiple classes

695
00:42:22,420 --> 00:42:26,860
and i will introduce you to a couple of new concepts which hopefully should

696
00:42:26,860 --> 00:42:29,630
prove interesting

697
00:42:29,690 --> 00:42:31,900
so now we're going to do is we're going to

698
00:42:31,920 --> 00:42:34,110
the classification

699
00:42:35,670 --> 00:42:38,610
the target values when they take on

700
00:42:40,750 --> 00:42:44,340
values from one to k keeping the number of

701
00:42:44,340 --> 00:42:51,150
classes that you could be considering

702
00:42:55,840 --> 00:42:59,670
let's make some modelling assumptions here

703
00:42:59,670 --> 00:43:02,650
let's assume

704
00:43:04,630 --> 00:43:06,530
so for each class

705
00:43:06,550 --> 00:43:12,190
we want to discriminate we need to find the function which somehow or other

706
00:43:12,210 --> 00:43:16,460
describes it discriminates that class from all of others

707
00:43:16,500 --> 00:43:20,050
so what we will do is really soon

708
00:43:20,070 --> 00:43:22,880
that we have no real importance

709
00:43:22,900 --> 00:43:26,340
a gaussian process prior

710
00:43:27,190 --> 00:43:29,170
each class

711
00:43:29,190 --> 00:43:35,650
right so there's going to be a function for each class

712
00:43:35,750 --> 00:43:38,000
so that means that our prior

713
00:43:38,050 --> 00:43:39,570
is not going to be

714
00:43:40,690 --> 00:43:44,050
on the function values for class one

715
00:43:44,070 --> 00:43:47,800
the function values for class two the function values of two

716
00:43:48,030 --> 00:43:50,070
class k

717
00:43:52,690 --> 00:43:54,980
the covariance function parameters

718
00:43:56,300 --> 00:43:59,090
the GP for class one up to

719
00:43:59,110 --> 00:44:00,420
class k

720
00:44:00,440 --> 00:44:01,710
and given

721
00:44:01,980 --> 00:44:06,070
a set of attributes of covariates

722
00:44:08,500 --> 00:44:09,880
now also

723
00:44:09,880 --> 00:44:13,860
and impose the restriction that p on

724
00:44:15,070 --> 00:44:18,500
each of these gaussian processes for each class

725
00:44:18,520 --> 00:44:21,570
is independent

726
00:44:21,590 --> 00:44:26,820
and that's perfectly reasonable because quite well we haven't seen what the dependencies between the

727
00:44:27,860 --> 00:44:29,440
yeah it

728
00:44:31,480 --> 00:44:32,630
we'll see

729
00:44:32,630 --> 00:44:38,020
that posterior only the posterior over functions will be dependent

730
00:44:38,050 --> 00:44:41,460
so we can just assume

731
00:44:41,480 --> 00:44:45,610
statistical independence of each of these goes in processes

732
00:44:45,670 --> 00:44:48,400
so we take a product form over k

733
00:44:48,440 --> 00:44:50,920
all of our gaussian process priors

734
00:44:50,940 --> 00:44:54,590
for each class

735
00:44:54,820 --> 00:44:56,150
still with me

736
00:44:56,170 --> 00:44:59,770
are about thirty percent of

737
00:45:02,130 --> 00:45:05,130
what about the likelihood function

738
00:45:05,170 --> 00:45:14,000
what was the likelihood function is just to the for binary classification

739
00:45:14,030 --> 00:45:18,270
when you're always asking questions you should remember this was

740
00:45:18,290 --> 00:45:22,460
what was the the the likelihood function to be used

741
00:45:22,500 --> 00:45:24,300
for binary classification

742
00:45:28,460 --> 00:45:30,880
two classes

743
00:45:30,960 --> 00:45:36,300
it was a binomial and the function that we used was the logistic function sigmoid

744
00:45:36,300 --> 00:45:37,840
function remember

745
00:45:37,860 --> 00:45:40,980
so now we need to

746
00:45:41,030 --> 00:45:43,250
look at what the

747
00:45:43,290 --> 00:45:46,110
likelihood is going to be over two classes

748
00:45:46,110 --> 00:45:51,480
so over two classes we had a binomial function a binomial distribution

749
00:45:51,570 --> 00:45:54,460
over k classes

750
00:45:54,480 --> 00:45:59,250
we have a multinomial distribution might so we have to keep possible choices

751
00:45:59,340 --> 00:46:02,000
and this is for those of you who done

752
00:46:02,030 --> 00:46:08,710
so someone pick me up last night for was fitting to schoolboy algebra so and

753
00:46:08,710 --> 00:46:11,840
if anyone has done some schoolboy statistics

754
00:46:11,860 --> 00:46:13,820
and will recognise the

755
00:46:13,840 --> 00:46:15,550
multinomial distribution

756
00:46:15,570 --> 00:46:19,090
which is basically OK we'll see just short

757
00:46:19,110 --> 00:46:21,880
but if and

758
00:46:21,940 --> 00:46:24,730
if the class label for the instigator of point

759
00:46:24,750 --> 00:46:27,440
corresponds to the case class

760
00:46:27,500 --> 00:46:30,110
then this delta function returns of one

761
00:46:30,110 --> 00:46:31,740
why do that

762
00:46:31,760 --> 00:46:34,950
i'm just to show you that i can do right i'm just demonstrated that i

763
00:46:34,950 --> 00:46:36,840
can stick any denied any DNA

764
00:46:36,840 --> 00:46:40,070
remember the DNA doesn't want to have a piece of DNA it doesn't know whether

765
00:46:40,070 --> 00:46:43,450
it came from human or zebra it's just the molecule you can stick the molecules

766
00:46:44,530 --> 00:46:50,070
right but what i really want to attach my human DNA two

767
00:46:50,090 --> 00:46:54,380
i want to attach to to some other DNA

768
00:46:54,400 --> 00:46:58,800
that has the ability to grow on its own

769
00:46:58,840 --> 00:47:03,550
within bacteria

770
00:47:10,450 --> 00:47:14,570
i need to make

771
00:47:14,570 --> 00:47:16,180
here's what i would really like

772
00:47:16,380 --> 00:47:21,200
i would like to have a piece of DNA

773
00:47:21,240 --> 00:47:23,470
it has some sequences

774
00:47:28,760 --> 00:47:31,300
the recognition sites

775
00:47:31,300 --> 00:47:32,820
four replication

776
00:47:32,840 --> 00:47:42,700
i'd like have some replication initiation sites here

777
00:47:42,720 --> 00:47:44,760
so piece of DNA

778
00:47:45,940 --> 00:47:49,860
one of the bacterial chromosome itself

779
00:47:49,880 --> 00:47:51,820
here's my bacteria

780
00:47:51,880 --> 00:47:54,760
the bacteria is on chromosome

781
00:47:54,760 --> 00:47:56,320
replicates itself

782
00:47:56,320 --> 00:47:57,400
and it has

783
00:47:57,420 --> 00:48:01,440
the ability to start DNA replication at multiple sites

784
00:48:01,570 --> 00:48:03,200
called origins

785
00:48:04,670 --> 00:48:09,470
but what i would really like

786
00:48:09,490 --> 00:48:12,880
is to be able to construct in the laboratory

787
00:48:12,900 --> 00:48:16,010
a synthetic piece of DNA

788
00:48:16,110 --> 00:48:18,360
that also would function

789
00:48:18,510 --> 00:48:20,670
as an origin

790
00:48:20,700 --> 00:48:22,550
a replication

791
00:48:24,820 --> 00:48:27,590
because then what i could do

792
00:48:27,610 --> 00:48:31,680
is in vitro

793
00:48:31,700 --> 00:48:34,200
take my piece of DNA

794
00:48:34,220 --> 00:48:37,860
attach it to this effect

795
00:48:39,490 --> 00:48:41,070
it would now

796
00:48:41,090 --> 00:48:43,530
we have the ability to grow bacteria

797
00:48:43,610 --> 00:48:46,420
how like make

798
00:48:46,420 --> 00:48:50,010
a piece of DNA what kind of engineering tricks can do

799
00:48:50,050 --> 00:48:54,260
to create a small piece of DNA that has all the machinery

800
00:48:55,670 --> 00:48:59,550
to be able to be copied replicated just like bacterial chromosomes

801
00:48:59,550 --> 00:49:03,470
that's a pretty fancy feat of engineering

802
00:49:03,510 --> 00:49:06,220
i can do that

803
00:49:08,200 --> 00:49:10,840
risk because so we're ask

804
00:49:10,860 --> 00:49:14,970
if you want to do this you ask the experts who are the experts

805
00:49:15,010 --> 00:49:17,110
viruses or

806
00:49:17,130 --> 00:49:22,110
four or bacteriophage basically if you want to do anything the place to ask

807
00:49:22,170 --> 00:49:26,010
is the folks who have the most experience folks have the most experience are almost

808
00:49:27,090 --> 00:49:28,740
prokaryotic out organisms

809
00:49:28,760 --> 00:49:33,220
because they are by far the most evolved things on this planet anything that can

810
00:49:33,220 --> 00:49:37,670
replicate itself and grow every twenty minutes or something like that somewhat more generations of

811
00:49:37,670 --> 00:49:41,450
evolution and you have and therefore they are much more optimise than we are to

812
00:49:41,450 --> 00:49:42,550
go asking say

813
00:49:42,550 --> 00:49:46,610
has a bacteria worked out how to do this

814
00:49:46,630 --> 00:49:49,570
students had better in order to do this just fine

815
00:49:49,670 --> 00:49:51,900
in fact

816
00:49:51,940 --> 00:49:54,420
most bacteria

817
00:49:54,470 --> 00:49:56,240
least many bacteria

818
00:49:56,260 --> 00:49:59,860
contained within

819
00:49:59,880 --> 00:50:05,320
in addition to their own chromosome

820
00:50:05,360 --> 00:50:08,440
small circles of DNA

821
00:50:08,450 --> 00:50:09,700
these are called

822
00:50:09,800 --> 00:50:11,510
happy so

823
00:50:11,680 --> 00:50:17,280
this is the chromosome

824
00:50:18,320 --> 00:50:21,170
that means on top of or in addition to

825
00:50:21,240 --> 00:50:23,920
so in addition to the chromosome there's an episode

826
00:50:24,000 --> 00:50:25,420
the episode

827
00:50:25,450 --> 00:50:29,010
is in fact an autonomously replicating piece of DNA

828
00:50:29,010 --> 00:50:31,050
it has a large

829
00:50:31,700 --> 00:50:34,170
and markets

830
00:50:34,240 --> 00:50:38,340
why bacteria have episodes

831
00:50:38,510 --> 00:50:43,320
it turns out that the songs

832
00:50:43,340 --> 00:50:45,590
often contain genes

833
00:50:45,610 --> 00:50:47,760
one of the genes they contain

834
00:50:47,860 --> 00:50:53,050
or some of the types of genes they contain are resistance to

835
00:50:55,220 --> 00:50:58,280
there might be for example

836
00:50:58,320 --> 00:50:59,880
a penicillin

837
00:50:59,950 --> 00:51:02,050
resistance gene

838
00:51:02,070 --> 00:51:04,010
contains on episode

839
00:51:04,030 --> 00:51:07,220
or streptomyces reserve resistance

840
00:51:07,280 --> 00:51:09,360
it turns out the bacteria

841
00:51:09,360 --> 00:51:13,570
i have these episodes containing resistance genes

842
00:51:16,320 --> 00:51:18,740
they're not in the chromosome set

843
00:51:18,800 --> 00:51:22,320
why would they do that

844
00:51:22,320 --> 00:51:25,440
it turns out

845
00:51:25,450 --> 00:51:29,170
one a bacterium dies in the cell cracks open

846
00:51:29,200 --> 00:51:32,030
the DNA spills out

847
00:51:32,050 --> 00:51:35,180
the next door neighbour bacteria

848
00:51:35,240 --> 00:51:39,170
has mechanisms to suck up DNA from the environment

849
00:51:39,170 --> 00:51:41,700
never know might find something interesting out there

850
00:51:42,610 --> 00:51:48,590
it turns out that bacteria are rather promiscuously exchanging pieces of DNA

851
00:51:48,610 --> 00:51:50,200
all the time

852
00:51:50,260 --> 00:51:53,670
and so i bacteria that has a

853
00:51:53,740 --> 00:51:56,760
episode the has the penicillin resistance genes

854
00:51:56,820 --> 00:51:58,170
can spread it

855
00:51:58,180 --> 00:52:02,990
two other bacteria and and it's very nice it's compact on its own little episode

856
00:52:03,010 --> 00:52:05,680
autonomously replicating piece of DNA

857
00:52:05,680 --> 00:52:07,140
we are more likely to be in

858
00:52:11,450 --> 00:52:12,760
that are available at

859
00:52:12,860 --> 00:52:17,980
being laid down by a little bit you really haven't an initial conditions keep on going

860
00:52:18,600 --> 00:52:20,350
three hundred eighty five

861
00:52:22,330 --> 00:52:24,230
i know of of four hundred set

862
00:52:24,730 --> 00:52:26,760
you can hardly see on screen that

863
00:52:28,150 --> 00:52:29,580
the yellow and blue

864
00:52:30,310 --> 00:52:30,880
in absence

865
00:52:31,430 --> 00:52:35,560
almost all of your initial condition was in one hundred

866
00:52:37,170 --> 00:52:37,830
one day

867
00:52:38,370 --> 00:52:39,130
and that is that

868
00:52:39,790 --> 00:52:40,730
all and all

869
00:52:41,630 --> 00:52:45,070
five hundred in the fall

870
00:52:45,630 --> 00:52:51,810
i you it into them and the fact that the relation with it and that's where i think when we

871
00:52:54,070 --> 00:52:54,470
thank you

872
00:52:57,050 --> 00:52:58,070
any questions about

873
00:53:01,320 --> 00:53:03,480
so the general message we've got it

874
00:53:04,300 --> 00:53:06,970
you make that's all size at all

875
00:53:08,280 --> 00:53:11,980
and it the distribution in dealing with the characteristic length scale

876
00:53:12,590 --> 00:53:14,690
or al in the same units

877
00:53:15,670 --> 00:53:18,220
a random walk methods is going at least

878
00:53:18,630 --> 00:53:19,580
element belongs where

879
00:53:21,360 --> 00:53:25,150
to get around it may be worth because there might be other properties you know

880
00:53:25,160 --> 00:53:29,430
the registration might not be canyon in like that will get across

881
00:53:29,890 --> 00:53:31,550
then you have to write the even longer

882
00:53:32,300 --> 00:53:32,640
so this

883
00:53:33,280 --> 00:53:34,870
number we've got three years

884
00:53:35,290 --> 00:53:36,130
one on where

885
00:53:36,730 --> 00:53:39,980
is bound lower bound on how long you might have run

886
00:53:40,690 --> 00:53:42,270
and will be what

887
00:53:46,350 --> 00:53:50,570
everything i've shown here as in one dimension let's make clear how it relates to

888
00:53:51,760 --> 00:53:53,610
high-dimensional problems in the typical

889
00:53:55,790 --> 00:53:58,200
it they an inference problem that you're working on

890
00:53:58,620 --> 00:53:59,420
and if all of these

891
00:54:00,070 --> 00:54:04,630
things called acts are unknown parameters that are being determined by data

892
00:54:09,540 --> 00:54:12,490
in the real world typically you have quite a lot of data and you have quite a lot

893
00:54:15,570 --> 00:54:15,870
and so

894
00:54:16,610 --> 00:54:18,060
and we read distribution

895
00:54:20,380 --> 00:54:21,550
they look more like this

896
00:54:22,890 --> 00:54:25,900
maybe two dimensional maybe even two thousand dimensional

897
00:54:27,710 --> 00:54:31,190
and it will have some long lengthscale some lengthscale that are

898
00:54:31,580 --> 00:54:32,530
that's a bit shorter

899
00:54:37,720 --> 00:54:39,360
in general you might have

900
00:54:40,300 --> 00:54:41,300
this is again a cup to

901
00:54:41,990 --> 00:54:42,970
the real situation

902
00:54:43,890 --> 00:54:47,590
might be gamma dimensions gamma could yield good

903
00:54:48,320 --> 00:54:50,110
even well determined dimensions

904
00:54:50,570 --> 00:54:51,590
gamma dimensions

905
00:54:52,320 --> 00:54:54,740
where the length scale is less al

906
00:54:55,840 --> 00:54:58,740
and then there might be very minor gamma other dimensions

907
00:54:59,680 --> 00:55:01,930
in which the length scale is much larger

908
00:55:03,970 --> 00:55:10,090
and would be wholly determined relative the total number of grammar the damage you at the page

909
00:55:10,970 --> 00:55:13,820
minus gamma dimension that will be determined by the data you have

910
00:55:14,590 --> 00:55:15,200
and then you

911
00:55:16,110 --> 00:55:18,070
and gamma world determined by the data

912
00:55:19,160 --> 00:55:23,430
so this is your effective number of parameters the only number freedom

913
00:55:25,410 --> 00:55:27,030
number well parameters

914
00:55:29,550 --> 00:55:31,220
you know give yourself a step size

915
00:55:31,720 --> 00:55:32,300
that's long

916
00:55:36,140 --> 00:55:37,510
you could imagine all

917
00:55:40,140 --> 00:55:40,950
small out

918
00:55:41,660 --> 00:55:44,890
all smaller they could use just like problem

919
00:55:46,680 --> 00:55:47,430
something that

920
00:55:48,280 --> 00:55:49,090
finally let

921
00:55:50,510 --> 00:55:51,410
why can use

922
00:55:55,450 --> 00:55:56,450
are you something

923
00:55:57,930 --> 00:55:58,550
thank you

924
00:56:00,200 --> 00:56:01,860
even when you're designing a metropolis

925
00:56:02,280 --> 00:56:03,700
i think have a long

926
00:56:04,410 --> 00:56:06,780
he any size you want to know it back

927
00:56:07,570 --> 00:56:09,760
around what's gonna happen now well

928
00:56:10,700 --> 00:56:14,630
all in all your there are accepted and you don't do a random walk

929
00:56:15,260 --> 00:56:16,680
then it still the case

930
00:56:17,260 --> 00:56:19,890
at the typical distance u we'll gone

931
00:56:21,700 --> 00:56:23,740
he that's is going as the square root

932
00:56:26,970 --> 00:56:27,990
it's going to take you

933
00:56:34,430 --> 00:56:35,950
at least element belongs where

934
00:56:37,130 --> 00:56:40,220
get around from one independent points in to another

935
00:56:41,740 --> 00:56:43,990
and then what they need at least

936
00:56:46,320 --> 00:56:47,760
one of with that's

937
00:56:49,610 --> 00:56:49,910
you get

938
00:56:51,260 --> 00:56:52,590
pressure independent points

939
00:57:00,470 --> 00:57:05,970
so that's pushes you say well i want i want to be really big van allen at all

940
00:57:06,820 --> 00:57:08,450
small of this

941
00:57:09,280 --> 00:57:11,010
bacteria with group

942
00:57:13,890 --> 00:57:15,470
let's make it on unambiguously had

943
00:57:18,720 --> 00:57:22,260
if made at already beginning motivated alpha example

944
00:57:25,490 --> 00:57:27,550
then what actually happened

945
00:57:28,010 --> 00:57:31,220
well if you are interested in you make proposal

946
00:57:31,660 --> 00:57:32,700
and the green there

947
00:57:34,780 --> 00:57:35,430
which remember

948
00:57:37,680 --> 00:57:41,280
gamma dimensional effectively the whole place is a dimensional

949
00:57:41,950 --> 00:57:47,910
and gamma vote dimensions of the kind that in fact with with a little help

950
00:57:48,680 --> 00:57:51,140
and in so many the government will not and

951
00:57:52,490 --> 00:57:57,740
what about mean is the probability of making and accept the move it at one day

952
00:57:58,450 --> 00:57:59,930
it can be the ratio of

953
00:58:00,760 --> 00:58:01,360
the following

954
00:58:03,760 --> 00:58:04,280
the following

955
00:58:06,930 --> 00:58:07,630
and that's bad news

956
00:58:07,630 --> 00:58:09,480
and i think the metrics

957
00:58:09,480 --> 00:58:15,230
which is not symmetric the proposed an algorithm for performing grouping which is actually spectral

958
00:58:15,370 --> 00:58:20,960
and also are already already so that using mean field new for finding

959
00:58:21,020 --> 00:58:24,370
groupings using this method

960
00:58:24,400 --> 00:58:26,600
so we compared our results

961
00:58:26,650 --> 00:58:29,310
with the with those obtained using the original

962
00:58:30,560 --> 00:58:34,620
and then we also compare the results using the symmetrized version we wanted to see

963
00:58:34,620 --> 00:58:37,140
what happens if symmetrized that

964
00:58:38,850 --> 00:58:42,000
so we perform the few experiments on synthetic data

965
00:58:42,080 --> 00:58:43,000
we had

966
00:58:43,040 --> 00:58:45,210
for example circle here

967
00:58:45,270 --> 00:58:47,190
and a lot of nice

968
00:58:47,230 --> 00:58:49,460
this is what happens

969
00:58:49,710 --> 00:58:51,870
bias algorithms

970
00:58:51,920 --> 00:58:53,600
ours is

971
00:58:53,620 --> 00:58:54,830
this one

972
00:58:54,870 --> 00:58:57,290
williamson former USSR

973
00:58:57,290 --> 00:58:59,810
this is using the that mation

974
00:58:59,810 --> 00:59:05,580
for example compare with the original williams former of which is exactly the same similarity

975
00:59:06,560 --> 00:59:09,020
but using the spectral form

976
00:59:10,580 --> 00:59:14,480
so what happens is that the big picture which emerges actually that

977
00:59:15,290 --> 00:59:18,850
the replicator dynamics with the semantic matches

978
00:59:19,830 --> 00:59:23,920
provides a good compromise between false positive and false negative rate

979
00:59:23,980 --> 00:59:26,710
for example what happens is that usually

980
00:59:26,710 --> 00:59:28,230
williamson form

981
00:59:28,230 --> 00:59:29,830
very low

982
00:59:29,880 --> 00:59:31,370
false positive rate

983
00:59:31,420 --> 00:59:33,420
it's very conservative

984
00:59:34,370 --> 00:59:36,500
yes very greatly

985
00:59:36,520 --> 00:59:38,250
for example here you can see

986
00:59:38,270 --> 00:59:40,150
there is no false negative

987
00:59:40,190 --> 00:59:41,980
but you know the problem

988
00:59:42,080 --> 00:59:47,290
to capture all a very significant part of shape

989
00:59:47,290 --> 00:59:49,500
this is what we did

990
00:59:49,540 --> 00:59:52,480
of course we incorporate knowledge as well

991
00:59:52,500 --> 00:59:53,270
so we

992
00:59:54,330 --> 00:59:57,000
when measurements

993
00:59:57,040 --> 00:59:58,900
the big picture is that

994
00:59:58,940 --> 01:00:02,620
using our algorithm against the original outcomes

995
01:00:02,650 --> 01:00:07,600
is a good compromise between false positive rate false negative for example here we have

996
01:00:07,600 --> 01:00:15,230
some problems with your original argument your original answer mathematics very low false positive rate

997
01:00:15,290 --> 01:00:18,440
very low because it is a set this very conservative

998
01:00:18,480 --> 01:00:22,290
field of false false negative rate is very high

999
01:00:22,330 --> 01:00:24,150
while ours

1000
01:00:25,060 --> 01:00:29,420
the larger false positive rate and much more

1001
01:00:29,440 --> 01:00:34,060
false negative this is the big picture the

1002
01:00:34,140 --> 01:00:37,540
we also have to experiments in the early nineties

1003
01:00:37,560 --> 01:00:40,170
in this case the difference between

1004
01:00:41,290 --> 01:00:44,290
and we have some form

1005
01:00:44,310 --> 01:00:48,620
and even against against the symmetry this is what we got

1006
01:00:48,640 --> 01:00:56,050
i symmetrizing metrics so we got the radio asymmetric symmetrizing the replicator dynamics and you

1007
01:00:56,050 --> 01:00:58,100
can see the results are much worse

1008
01:00:58,120 --> 01:00:59,170
well actually

1009
01:00:59,170 --> 01:01:01,710
this confirms the intuition if i have

1010
01:01:01,730 --> 01:01:04,560
and regionalism that matters in this in the dry

1011
01:01:04,650 --> 01:01:08,870
just just using a lot of information

1012
01:01:08,920 --> 01:01:11,170
OK so let me go to the finals

1013
01:01:11,210 --> 01:01:13,460
part of this lecture

1014
01:01:13,480 --> 01:01:15,620
how can we use game theory

1015
01:01:16,600 --> 01:01:18,880
an interesting fact

1016
01:01:21,190 --> 01:01:27,140
the classical strategy you here for performing a partition of the input data is

1017
01:01:27,380 --> 01:01:28,960
belongs strategy

1018
01:01:28,980 --> 01:01:30,420
so i just

1019
01:01:30,460 --> 01:01:35,920
finding dominant sets remove it from graph theory applied research and so on

1020
01:01:35,980 --> 01:01:37,810
this is two problems

1021
01:01:37,830 --> 01:01:41,330
first this is not necessary for some months

1022
01:01:41,420 --> 01:01:44,120
it forces the hard partition

1023
01:01:44,270 --> 01:01:45,520
hard bound

1024
01:01:45,560 --> 01:01:48,250
the second problem that by

1025
01:01:48,270 --> 01:01:49,250
in this way

1026
01:01:49,270 --> 01:01:51,230
we are implicitly introducing

1027
01:01:51,230 --> 01:01:53,310
a change in this case

1028
01:01:53,540 --> 01:01:58,020
we're not change in the metrics using exactly the same edits four

1029
01:01:58,080 --> 01:01:59,460
but by removing

1030
01:01:59,580 --> 01:02:03,100
these are the ground right change this case

1031
01:02:03,120 --> 01:02:05,350
the data

1032
01:02:05,400 --> 01:02:07,690
so we can use game theory

1033
01:02:08,750 --> 01:02:11,670
perform an enumeration of the dominant set

1034
01:02:12,620 --> 01:02:16,190
to perform a partition of the data

1035
01:02:16,190 --> 01:02:18,520
which allows for overlapping last

1036
01:02:18,520 --> 01:02:19,370
this can be

1037
01:02:19,370 --> 01:02:22,650
can find some interesting application for example

1038
01:02:22,880 --> 01:02:25,670
consider the situation where

1039
01:02:25,710 --> 01:02:28,290
so there's is a transparent surface

1040
01:02:28,330 --> 01:02:29,540
that surface

1041
01:02:29,560 --> 01:02:31,190
makes perfect sense

1042
01:02:31,210 --> 01:02:32,460
it is classified

1043
01:02:32,460 --> 01:02:34,190
two different clusters

1044
01:02:34,250 --> 01:02:36,770
suppose case where we have

1045
01:02:36,810 --> 01:02:38,880
a very simple case

1046
01:02:42,900 --> 01:02:47,100
two clusters this is one cluster is another class

1047
01:02:47,100 --> 01:02:52,230
this point belongs both there's no way there's no sense to say this belongs to

1048
01:02:52,230 --> 01:02:54,150
this and not

1049
01:02:54,330 --> 01:02:57,980
are situations actually where it it is important to be able

1050
01:02:58,150 --> 01:03:00,900
get overlapping last

1051
01:03:00,920 --> 01:03:05,060
so let me show you the basic idea with a very simple case where we

1052
01:03:05,060 --> 01:03:07,400
don't have weights of edges

1053
01:03:07,500 --> 01:03:09,250
so we are working on

1054
01:03:09,290 --> 01:03:14,080
and directed undirected unweighted graphs

1055
01:03:14,120 --> 01:03:16,060
suppose that i found

1056
01:03:16,080 --> 01:03:19,560
these two dominant sets are to maximum clique

1057
01:03:19,650 --> 01:03:23,500
ABC on one side and a view

1058
01:03:23,520 --> 01:03:25,250
i want to

1059
01:03:26,270 --> 01:03:29,750
these two dominant set in the future so what i want to do is that

1060
01:03:29,750 --> 01:03:32,670
they want to run again the replicator dynamics

1061
01:03:32,690 --> 01:03:35,420
and i want to be sure that i will not get

1062
01:03:35,460 --> 01:03:38,750
these two guys here again how can i do that

1063
01:03:38,770 --> 01:03:40,710
OK to take is the following

1064
01:03:40,730 --> 01:03:45,920
for each maximum clique that you want to avoid just add an extra vertex

1065
01:03:45,960 --> 01:03:47,040
we are

1066
01:03:47,120 --> 01:03:49,600
but it's one here which is related to

1067
01:03:51,000 --> 01:03:54,020
and we have products to which is related to the

1068
01:03:54,040 --> 01:03:55,350
the greek

1069
01:03:55,370 --> 01:03:59,080
and you create new edges you could actually new

1070
01:03:59,190 --> 01:04:01,250
in this graph will be a directed one

1071
01:04:01,270 --> 01:04:05,920
so we are transforming an undirected graph the

1072
01:04:05,940 --> 01:04:07,650
you have directed edges

1073
01:04:07,670 --> 01:04:10,730
and the structure is the following

1074
01:04:10,730 --> 01:04:14,940
take for example this which is associated with this click here

1075
01:04:15,150 --> 01:04:17,100
to to right one

1076
01:04:18,020 --> 01:04:19,920
then you have edges which

1077
01:04:22,670 --> 01:04:25,440
you can mean edges and outgoing

1078
01:04:25,480 --> 01:04:30,080
you can't just start from that x one is doesn't one

1079
01:04:30,100 --> 01:04:33,210
and we reach all the vertices within

1080
01:04:33,230 --> 01:04:34,980
click associated two

1081
01:04:35,000 --> 01:04:38,480
all this just start from one to d

1082
01:04:38,540 --> 01:04:40,100
c into it

1083
01:04:40,120 --> 01:04:44,000
this is the parliament doesn't belong to directly

1084
01:04:44,060 --> 01:04:47,370
so you will have only just started from these

1085
01:04:47,460 --> 01:04:50,500
but this and ending up this

1086
01:04:51,190 --> 01:04:54,370
you do the same with this you have create new world

1087
01:04:54,370 --> 01:04:57,130
right so this is a lgn minus

1088
01:04:57,140 --> 01:04:59,030
the minimum

1089
01:04:59,040 --> 01:05:01,070
over virgin of energy

1090
01:05:01,080 --> 01:05:05,060
so if i take the minimum out then it becomes the maximum because there's a

1091
01:05:05,060 --> 01:05:07,230
minus sign so

1092
01:05:07,240 --> 01:05:12,200
n minus the minimum is the same as the maximum because of the difference between

1093
01:05:12,210 --> 01:05:14,410
a lgn lg

1094
01:05:16,790 --> 01:05:20,480
he's just comes from the definition of this quantity here

1095
01:05:20,530 --> 01:05:24,260
and this is called the estimation error i i like

1096
01:05:24,350 --> 01:05:25,850
to call it

1097
01:05:25,860 --> 01:05:30,840
better the maximum regret because it's the quantity that you can define even if you

1098
01:05:30,840 --> 01:05:32,930
don't have

1099
01:05:32,940 --> 01:05:37,550
this i framework even if you are not in the statistical framework and estimation is

1100
01:05:37,860 --> 01:05:41,450
a term that is usually used in the statistical for

1101
01:05:41,460 --> 01:05:43,690
OK and what did what it

1102
01:05:43,740 --> 01:05:47,090
it tells you is how much does the data

1103
01:05:47,100 --> 01:05:48,640
mislead you in

1104
01:05:48,900 --> 01:05:56,300
making you believe that gn is good while the the good function is g underscore

1105
01:06:01,100 --> 01:06:05,000
and this quantity of course depends on on the training data that you have

1106
01:06:05,050 --> 01:06:08,730
and the point is you can

1107
01:06:08,780 --> 01:06:10,550
quantify like get

1108
01:06:10,560 --> 01:06:13,050
upper bounds on this quantity

1109
01:06:13,100 --> 01:06:15,970
even if you don't make any assumptions about

1110
01:06:15,980 --> 01:06:17,220
you distribution

1111
01:06:17,240 --> 01:06:19,210
sorry about your target function

1112
01:06:21,090 --> 01:06:25,190
what is interesting when you do this decomposition is that you get term that

1113
01:06:25,200 --> 01:06:27,600
you cannot say anything about

1114
01:06:27,610 --> 01:06:29,250
and then another term

1115
01:06:29,310 --> 01:06:33,670
which you can control so the eighties now OK let's forget about this

1116
01:06:33,680 --> 01:06:37,670
let's say we will never be able to say anything about this

1117
01:06:37,720 --> 01:06:40,390
the party we make assumptions

1118
01:06:40,440 --> 01:06:41,840
so let's focus

1119
01:06:41,890 --> 01:06:43,220
on this term here

1120
01:06:43,230 --> 01:06:46,990
and we know anyway that the sum of those terms can be made arbitrarily large

1121
01:06:46,990 --> 01:06:51,760
because of the insurance and so let's see how small this can be a how

1122
01:06:51,760 --> 01:06:56,450
to control these these quantities here

1123
01:06:56,460 --> 01:06:57,250
because of

1124
01:06:57,260 --> 01:06:58,640
again rephrasing

1125
01:06:58,660 --> 01:07:02,470
so we want to focus on the estimation error or the maximum regret

1126
01:07:05,090 --> 01:07:07,110
somehow the message is that the

1127
01:07:07,120 --> 01:07:09,510
this learning theory will not tell you

1128
01:07:09,520 --> 01:07:13,270
how to justify your assumptions but it can tell you only

1129
01:07:13,300 --> 01:07:17,560
once you have put these assumptions what's once you've have made these restrictions

1130
01:07:17,600 --> 01:07:21,130
whether or not you believe that they are true is another problem but once you've

1131
01:07:21,130 --> 01:07:23,500
made them what can you do

1132
01:07:23,750 --> 01:07:29,380
to best exploit them

1133
01:07:30,110 --> 01:07:32,430
so now do a little bit of

1134
01:07:32,490 --> 01:07:34,540
mathematics finally

1135
01:07:35,080 --> 01:07:37,940
and i will introduce

1136
01:07:41,370 --> 01:07:44,580
selection bias variance decomposition

1137
01:07:44,590 --> 01:07:48,600
yes yes you can see that as a kind of a generalisation of the bias

1138
01:07:48,600 --> 01:07:51,970
variance decomposition sometimes

1139
01:07:52,000 --> 01:07:54,140
although it's not

1140
01:07:54,160 --> 01:07:59,120
always correct but sometimes people call these by using these different

1141
01:07:59,160 --> 01:08:03,440
but it's the same spirit

1142
01:08:03,560 --> 01:08:06,530
one is deterministic the other ones

1143
01:08:12,000 --> 01:08:13,650
so that's three

1144
01:08:13,660 --> 01:08:19,570
the main ingredient where you want to think about this quantity is maximum we wanted

1145
01:08:19,710 --> 01:08:23,370
one is deviation inequality

1146
01:08:23,390 --> 01:08:27,910
which are the basic probability inequalities about how

1147
01:08:27,930 --> 01:08:31,650
random variables deviates from verification

1148
01:08:31,650 --> 01:08:34,080
other one is called them

1149
01:08:34,160 --> 01:08:37,010
wiki so

1150
01:08:37,050 --> 01:08:38,170
i don't want

1151
01:08:39,820 --> 01:08:41,580
so which dates

1152
01:08:41,580 --> 01:08:43,630
the quantity we want to bound

1153
01:08:44,330 --> 01:08:46,500
something that we know how to do

1154
01:08:46,550 --> 01:08:52,290
so this estimation error can be related to the quantity that can be handled by

1155
01:08:52,290 --> 01:08:53,920
probability theory

1156
01:08:53,950 --> 01:08:56,330
and the

1157
01:08:56,390 --> 01:09:00,170
the gradient of interest actually

1158
01:09:01,290 --> 01:09:04,880
in my opinion this is the most important one is the union bound so it

1159
01:09:05,030 --> 01:09:06,050
how to

1160
01:09:06,610 --> 01:09:11,590
relates the deviation of a collection of random variables

1161
01:09:11,600 --> 01:09:12,800
once you know

1162
01:09:13,340 --> 01:09:15,220
each random variable

1163
01:09:15,250 --> 01:09:16,790
you see i mean flight

1164
01:09:16,790 --> 01:09:19,070
this is

1165
01:09:23,100 --> 01:09:24,300
i would say

1166
01:09:24,310 --> 01:09:26,580
the quantity of interest in this way

1167
01:09:26,600 --> 01:09:28,680
it is around one

1168
01:09:28,700 --> 01:09:29,890
so when you

1169
01:09:29,900 --> 01:09:31,400
so it's on the right

1170
01:09:31,410 --> 01:09:37,490
one of the random variable what can you i mean the kind of mathematical statement

1171
01:09:37,500 --> 01:09:41,470
you want to make about this from the variable is to characterize

1172
01:09:43,650 --> 01:09:45,710
how it actually right

1173
01:09:46,860 --> 01:09:50,760
the first thing can say is you want to compute the expectation of this from

1174
01:09:50,760 --> 01:09:56,420
the so again here expectation is with respect to the sampling of the training data

1175
01:09:56,420 --> 01:10:02,040
so showing you where to get repeated training samples of size n

1176
01:10:02,050 --> 01:10:03,700
and you have to compute

1177
01:10:03,720 --> 01:10:04,760
the average

1178
01:10:04,780 --> 01:10:09,290
error that that you make this these expectations in the

1179
01:10:09,410 --> 01:10:15,000
that's one quantity of interest

1180
01:10:15,130 --> 01:10:19,850
but sometimes people refer to quantify

1181
01:10:19,870 --> 01:10:23,400
one of the probabilities of the deviations

1182
01:10:24,650 --> 01:10:28,110
tell you what is the probability that these quantities larger than some

1183
01:10:28,110 --> 01:10:30,210
value epsilon

1184
01:10:30,470 --> 01:10:35,420
and why why is better it's because from this you can recover

1185
01:10:35,430 --> 01:10:40,030
all the properties of random variables with this is the distribution function of toronto bible

1186
01:10:40,030 --> 01:10:44,360
so it's completely characterize that characterizes it

1187
01:10:47,060 --> 01:10:48,950
quantity that some people

1188
01:10:48,980 --> 01:10:54,440
could be interested in is the worst case value of this from the bible

1189
01:10:55,920 --> 01:11:00,330
the maximum over all the possible training sample of this difference

1190
01:11:00,330 --> 01:11:04,720
OK this is usually not done in the framework and it's done more in the

1191
01:11:04,720 --> 01:11:08,400
on line framework but is just to tell you that

1192
01:11:08,610 --> 01:11:11,650
it's also something that you can get

1193
01:11:11,660 --> 01:11:13,510
a handle on

1194
01:11:13,530 --> 01:11:17,190
so so we will focus mainly on

1195
01:11:17,240 --> 01:11:21,680
the probability

1196
01:11:25,780 --> 01:11:30,460
what is a bit confusing here is that we have two layers of expectation one

1197
01:11:30,460 --> 01:11:34,680
layer is with respect to the training sample so we have the first kind of

1198
01:11:34,680 --> 01:11:38,420
random variable that we have is the training sample and then we have another random

1199
01:11:38,420 --> 01:11:40,900
variable which is the test data

1200
01:11:40,930 --> 01:11:42,120
so that's why

1201
01:11:42,500 --> 01:11:43,830
the loss

1202
01:11:43,880 --> 01:11:48,930
of the function was defined in the society setting as the expected value of the

1203
01:11:48,930 --> 01:11:50,200
the expectation of

1204
01:11:50,240 --> 01:11:55,620
the residual given the price and income is equal to zero so that means you

1205
01:11:55,620 --> 01:12:00,020
are in the case of the unity of the prices of course

1206
01:12:00,030 --> 01:12:05,440
so you have to to introduce some instrumental variables to solve and that leads to

1207
01:12:05,440 --> 01:12:11,900
the second case what happens when you the initial estimate of ten is no longer

1208
01:12:12,380 --> 01:12:18,800
estimation of the the conditional expectation the destination of instrument that regulation

1209
01:12:18,840 --> 01:12:25,040
and what happens then to the solution of the differential equation depending on the form

1210
01:12:25,040 --> 01:12:28,180
of the first estimate of

1211
01:12:28,190 --> 01:12:31,470
as the conditional expectation of something more complicated

1212
01:12:36,980 --> 01:12:40,500
what i'm going to do do this us focus on

1213
01:12:40,520 --> 01:12:46,830
this is more than where n is the conditional expectation derive some results for the

1214
01:12:46,830 --> 01:12:52,600
solution of the differential equation and using the same kind of methodology the same framework

1215
01:12:52,960 --> 01:12:55,320
we place the show

1216
01:12:55,370 --> 01:12:59,780
the conditional expectation by something complicated and see what happens

1217
01:12:59,810 --> 01:13:00,870
the solution

1218
01:13:13,210 --> 01:13:21,480
so in this

1219
01:13:21,500 --> 01:13:22,660
in this setting

1220
01:13:22,670 --> 01:13:29,780
i consider that this random vector x y z and they assume that my function

1221
01:13:29,780 --> 01:13:32,750
in which appears to be a functional equations

1222
01:13:32,760 --> 01:13:35,630
is exactly the conditional expectation is easy

1223
01:13:35,640 --> 01:13:37,150
even it's in

1224
01:13:37,160 --> 01:13:38,410
and i consider

1225
01:13:38,420 --> 01:13:40,200
both systems

1226
01:13:40,210 --> 01:13:43,090
lambda prime equal to m of explained

1227
01:13:43,100 --> 01:13:49,610
and london prime equal to an active pixel lambda and in or what follows i

1228
01:13:49,610 --> 01:13:55,890
will use some nonparametric can estimation to estimate the conditional expectation

1229
01:13:55,910 --> 01:13:59,660
and at so i will introduce some

1230
01:13:59,680 --> 01:14:04,070
cannot function capital gay and someone responded to each

1231
01:14:04,520 --> 01:14:08,290
two billion is basically

1232
01:14:08,300 --> 01:14:10,340
in the presentation

1233
01:14:10,350 --> 01:14:12,220
so the first step of

1234
01:14:12,230 --> 01:14:18,460
the methodology is to study the existence and uniqueness of both systems are true and

1235
01:14:18,460 --> 01:14:25,610
estimated one so the question the issues of identification of identification

1236
01:14:25,630 --> 01:14:32,040
and to study the issues i will i will i very well know

1237
01:14:32,050 --> 01:14:39,650
mathematical CERN about existence and uniqueness of solutions of ordinary differential equations it is the

1238
01:14:39,680 --> 01:14:40,810
because she she

1239
01:14:42,020 --> 01:14:47,420
and this year and there that you know

1240
01:14:47,430 --> 01:14:51,110
the neighborhood of the initial conditions so in the neighborhood of

1241
01:14:51,390 --> 01:14:55,280
nine point he was you will under some regularity property

1242
01:14:55,290 --> 01:14:57,310
on the functions

1243
01:14:57,350 --> 01:14:59,970
and in and

1244
01:14:59,990 --> 01:15:04,660
and if they satisfy in particular the kids conditions then i can define a unique

1245
01:15:05,650 --> 01:15:10,800
lambda and some associated based in london that

1246
01:15:10,810 --> 01:15:12,100
so if i assume

1247
01:15:13,130 --> 01:15:14,690
and satisfy

1248
01:15:14,910 --> 01:15:20,890
she condition that and then it also satisfies the conditions inside and able deal what

1249
01:15:20,890 --> 01:15:22,110
i can find

1250
01:15:22,170 --> 01:15:25,180
unique solutions formal systems

1251
01:15:25,220 --> 01:15:28,050
and then the second step to check

1252
01:15:28,090 --> 01:15:34,870
in this inverse problems mister the you are the solutions stable

1253
01:15:34,880 --> 01:15:41,990
that means if when i will replace a by and then we have the perturbation

1254
01:15:41,990 --> 01:15:47,040
involving is the solution to be confirmed as initial perturbations

1255
01:15:47,050 --> 01:15:48,810
and under

1256
01:15:48,830 --> 01:15:55,660
some regularity properties on the conventions of an actor and we have the stability of

1257
01:15:55,660 --> 01:16:00,480
the problem since the distance between london at

1258
01:16:00,490 --> 01:16:02,120
and then there is controlled

1259
01:16:02,160 --> 01:16:04,530
by distance between them and i

1260
01:16:04,540 --> 01:16:06,930
so i'm facing

1261
01:16:06,940 --> 01:16:12,270
well posed inverse problems there exists a unique solution to both systems

1262
01:16:12,270 --> 01:16:18,710
and the next step will be what kind of property can i prove ninety right

1263
01:16:19,130 --> 01:16:23,850
for the convergence of land at lambda i only have

1264
01:16:23,870 --> 01:16:25,780
you know with the consistency

1265
01:16:25,790 --> 01:16:32,420
what can i do what i can tell about the rate of convergence

1266
01:16:34,850 --> 01:16:37,060
so apt

1267
01:16:37,380 --> 01:16:43,400
what has been proved that the estimator

1268
01:16:43,420 --> 01:16:44,830
i consider

1269
01:16:46,280 --> 01:16:49,620
that means that i can in particular

1270
01:16:49,640 --> 01:16:52,070
right lambda as a function

1271
01:16:52,120 --> 01:16:53,810
he of n

1272
01:16:53,830 --> 01:16:57,350
and london at the function field an act

1273
01:16:57,430 --> 01:16:59,030
and the kind of assumptions

1274
01:16:59,040 --> 01:17:03,720
very roughly i will assume in the next

1275
01:17:03,720 --> 01:17:05,850
and what we what follows that

1276
01:17:05,870 --> 01:17:11,070
and will be more than it should satisfy the condition because since i will assume

1277
01:17:11,070 --> 01:17:13,590
that any continuous function of all the two

1278
01:17:13,610 --> 01:17:18,750
but to be effective helpful to derive some data expansions for the

1279
01:17:18,750 --> 01:17:20,710
i mean squared error

1280
01:17:20,730 --> 01:17:24,680
in this come in the neighborhood of he was you're going to combine neighborhood

1281
01:17:24,730 --> 01:17:28,440
i denoted by d is compact as was

1282
01:17:28,460 --> 01:17:33,080
i we assume that m and it is also continuously differentiable of all the two

1283
01:17:33,100 --> 01:17:37,040
in the same neighborhood and to have the ability of the solution i need to

1284
01:17:37,040 --> 01:17:41,210
assume that the derivative of

1285
01:17:41,210 --> 01:17:42,240
and then

1286
01:17:42,250 --> 01:17:44,800
the estimation of

1287
01:17:44,850 --> 01:17:49,570
the conditional expectation is that he respect the second arguement

1288
01:17:49,650 --> 01:17:55,640
converge uniformly to civility respect the content and

1289
01:17:56,840 --> 01:18:05,630
so that's the main the main assumption i need both on both function and that

1290
01:18:05,650 --> 01:18:07,530
now i know

1291
01:18:07,570 --> 01:18:13,090
i can write exactly lambda and london and at using this function and the next

1292
01:18:13,090 --> 01:18:14,100
step is

1293
01:18:14,120 --> 01:18:15,290
to study

1294
01:18:15,900 --> 01:18:19,020
rates of convergence of this function

1295
01:18:19,030 --> 01:18:24,840
lambda n two lambda and that leads to the following question

1296
01:18:24,880 --> 01:18:26,200
we know

1297
01:18:26,270 --> 01:18:32,830
one the weight of convergence of the estimation of the conditional expectation to the true

1298
01:18:32,850 --> 01:18:40,740
conditional expectation what happens when we apply some transformation he physicists nations and what kind

1299
01:18:40,740 --> 01:18:45,090
of information that we apply we have we can already have some intrusions of the

1300
01:18:45,090 --> 01:18:52,050
results we have defined lambda as the solution of differential equations that means you know

1301
01:18:52,070 --> 01:18:53,650
way that

1302
01:18:53,690 --> 01:18:59,260
what we have done is invest invest in with

1303
01:18:59,310 --> 01:19:01,750
differential operator so apply

1304
01:19:01,790 --> 01:19:04,240
very roughly some kind of internal world

1305
01:19:04,290 --> 01:19:12,180
later on the function and then at so we expect to prove that the convergence

1306
01:19:12,180 --> 01:19:14,720
of land and two lambda

1307
01:19:14,730 --> 01:19:18,380
is improved compared to the convergence and

1308
01:19:18,400 --> 01:19:20,840
that's the idea of what we

1309
01:19:20,890 --> 01:19:23,310
what to expect things

1310
01:19:23,330 --> 01:19:24,580
so now

1311
01:19:24,600 --> 01:19:28,200
study these conventions since we have a non linear

1312
01:19:28,200 --> 01:19:33,000
problems prevents we on the phone

1313
01:19:33,040 --> 01:19:37,130
of these the great he so

1314
01:19:37,230 --> 01:19:39,810
we have to realize

1315
01:19:39,840 --> 01:19:45,790
this system

1316
01:19:45,830 --> 01:19:50,110
to study the convention of london at london

1317
01:19:50,120 --> 01:19:54,770
so this is an intermediate

1318
01:19:54,770 --> 01:19:58,470
a very small step of the methodology

1319
01:19:58,470 --> 01:19:59,780
i think a you

1320
01:19:59,790 --> 01:20:03,070
any any prediction i make is likely to be wrong so it's better not to

1321
01:20:03,070 --> 01:20:05,800
predict but in terms of vision

1322
01:20:05,850 --> 01:20:10,690
i think it's useful to have a few guiding principles

1323
01:20:10,730 --> 01:20:13,500
four main areas of research

1324
01:20:13,550 --> 01:20:15,340
those include

1325
01:20:16,280 --> 01:20:18,360
prime guiding principles would be

1326
01:20:18,370 --> 01:20:19,600
number one

1327
01:20:19,610 --> 01:20:22,330
is understanding relevant so

1328
01:20:22,410 --> 01:20:25,240
for a company like you yahoo know how can we make it

1329
01:20:25,250 --> 01:20:27,140
so that are advertising

1330
01:20:27,150 --> 01:20:30,240
which is the product of actually drives money to the company

1331
01:20:30,280 --> 01:20:34,070
is as relevant to the user and the content they are consuming on the whole

1332
01:20:35,360 --> 01:20:40,640
another important principle is actually understanding a lot of these new phenomena that are emerging

1333
01:20:40,640 --> 01:20:44,890
on the web things that are enabled by web two point o like social media

1334
01:20:45,670 --> 01:20:51,870
sharing sites like you know photos on flickr and so forth all of these because

1335
01:20:51,870 --> 01:20:58,020
social media is understanding what the the appropriate dynamics appropriate incentives what makes these communities

1336
01:20:58,020 --> 01:21:00,390
grow and what sustains them

1337
01:21:00,430 --> 01:21:05,240
what is the equivalent of reputation online a lot of i would say

1338
01:21:05,290 --> 01:21:11,090
mapping a lot of the familiar online content offline concepts that we have evolved over

1339
01:21:11,090 --> 01:21:12,830
the last few thousand years

1340
01:21:12,840 --> 01:21:15,970
to have the equivalent online network

1341
01:21:15,980 --> 01:21:19,030
and make it into more secure and more reliable

1342
01:21:19,040 --> 01:21:20,590
safer for people

1343
01:21:20,830 --> 01:21:24,550
those are important guiding principles within them are

1344
01:21:24,600 --> 01:21:29,460
very deep open scientific problem i think is a lot of attention a in lot

1345
01:21:29,460 --> 01:21:32,570
of understanding

1346
01:21:32,650 --> 01:21:34,340
OK and first

1347
01:21:38,120 --> 01:21:49,690
so why research what what made me

1348
01:21:50,100 --> 01:21:53,640
one of the social when i was younger

1349
01:21:53,680 --> 01:21:57,150
on the personal side

1350
01:21:57,160 --> 01:22:00,210
probably two things i always wanted to be an engineer i think i was born

1351
01:22:00,210 --> 01:22:00,930
that way

1352
01:22:00,950 --> 01:22:05,150
my father happen to have a phd in engineering so there was a lot of

1353
01:22:05,150 --> 01:22:09,140
family pressure that you know one way or another graduate school is definitely something you

1354
01:22:09,140 --> 01:22:09,950
want to do

1355
01:22:10,000 --> 01:22:11,400
along the way

1356
01:22:12,940 --> 01:22:14,220
i discovered

1357
01:22:14,230 --> 01:22:15,920
passion for

1358
01:22:15,970 --> 01:22:18,590
four mathematics and more

1359
01:22:18,610 --> 01:22:23,170
modeling the world in an abstract language of mathematics

1360
01:22:23,190 --> 01:22:26,600
so realizing that you can actually theoretically model something

1361
01:22:26,640 --> 01:22:28,700
derive consequences of your model

1362
01:22:28,790 --> 01:22:30,680
and then actually see them

1363
01:22:30,730 --> 01:22:34,140
i realized when you make predictions about the real world

1364
01:22:34,150 --> 01:22:39,020
was a huge factor in turning me on to the whole world of research and

1365
01:22:39,020 --> 01:22:45,030
digging deeper beyond that i also had the good fortune of getting a couple of

1366
01:22:45,040 --> 01:22:47,010
summer internships during my

1367
01:22:47,020 --> 01:22:53,040
graduate career which really opened up my eyes one was with general motors research labs

1368
01:22:53,040 --> 01:22:55,930
where we worked on database of car repairs

1369
01:22:55,940 --> 01:23:01,900
millions of court cases that are difficult to solve and the idea was to mine

1370
01:23:01,900 --> 01:23:04,940
all this data and figure out the rules for

1371
01:23:04,950 --> 01:23:06,930
you know what causes some of these problems

1372
01:23:06,970 --> 01:23:11,940
that actually allowed me to discover that a lot of these things we do in

1373
01:23:11,940 --> 01:23:15,680
machine learning and induction in data mining actually have very practical

1374
01:23:16,830 --> 01:23:19,350
the other was GPL

1375
01:23:19,370 --> 01:23:21,940
we're realizing that these

1376
01:23:21,990 --> 01:23:25,570
machine learning and data mining techniques and actually solve problems that are very

1377
01:23:25,580 --> 01:23:28,140
very difficult to solve for

1378
01:23:28,190 --> 01:23:35,060
highly specialized groups of people like astronomers in very specialized scientists that was a big

1379
01:23:35,060 --> 01:23:41,990
eye-opening finding this is an area it's almost like

1380
01:23:42,080 --> 01:23:45,770
you know the analogy would be an car engine you know makes the cargo much

1381
01:23:45,770 --> 01:23:47,940
faster than any human can possibly run

1382
01:23:48,020 --> 01:23:54,160
with these algorithms you can actually extend intellect and inference capability over datasets that the

1383
01:23:54,160 --> 01:23:56,800
mind cannot fathom

1384
01:23:56,900 --> 01:24:00,700
so those those other things that sort of really influenced me in a big way

1385
01:24:00,770 --> 01:24:02,730
and after that you

1386
01:24:02,900 --> 01:24:06,650
you get you get addicted to learning in

1387
01:24:06,690 --> 01:24:08,480
figuring out things in

1388
01:24:08,490 --> 01:24:10,450
the joy of discovery in

1389
01:24:10,500 --> 01:24:11,570
good stuff

1390
01:24:11,760 --> 01:24:17,160
we do this

1391
01:24:17,160 --> 01:24:22,720
intuitively and very quickly and call it almost becomes second nature to make good moves

1392
01:24:22,720 --> 01:24:28,690
it seems to be intimately tied to the human visual system you when your experience

1393
01:24:28,690 --> 01:24:32,080
and you look at the borders immediately kind of points coming up to one to

1394
01:24:32,080 --> 01:24:37,370
be played they look like very natural moves now

1395
01:24:37,380 --> 01:24:42,220
the question is how how could the computer learned about this in one idea is

1396
01:24:42,360 --> 01:24:50,270
there exists a large databases of i guess today even millions of records of games

1397
01:24:52,460 --> 01:24:56,120
one idea is could we use these data

1398
01:24:56,130 --> 01:24:59,700
because it constitutes a very competent play

1399
01:24:59,720 --> 01:25:04,380
and could be somehow use this data to make the computer understand

1400
01:25:04,450 --> 01:25:07,340
so if you like to learn how to play go well

1401
01:25:07,530 --> 01:25:11,050
and that's the idea basically of applying machine learning

1402
01:25:11,060 --> 01:25:13,020
to go

1403
01:25:13,030 --> 01:25:17,410
in two tasks may come to mind in this context

1404
01:25:17,410 --> 01:25:19,810
one idea is

1405
01:25:19,840 --> 01:25:22,900
why don't we learn an evaluation function

1406
01:25:22,920 --> 01:25:26,100
so could be somehow learn

1407
01:25:26,150 --> 01:25:29,000
a mapping from a given position

1408
01:25:29,050 --> 01:25:33,750
two in number and the numbers somewhat indicates how good that position is for us

1409
01:25:33,810 --> 01:25:38,070
there would be a very natural units for that right this unit of one point

1410
01:25:38,070 --> 01:25:42,700
of territory and in those terms we would like to know how good position is

1411
01:25:42,700 --> 01:25:44,960
for example what's might expect that

1412
01:25:44,960 --> 01:25:48,350
winning score for example

1413
01:25:48,350 --> 01:25:53,000
and the interesting thing is that at the end of the game in fact we

1414
01:25:53,000 --> 01:25:57,520
know for every point on the board if it is black or if it is

1415
01:25:57,520 --> 01:26:01,610
why so we have a very detailed information at the end of the game that's

1416
01:26:01,630 --> 01:26:03,290
different from backgammon

1417
01:26:03,320 --> 01:26:07,300
wherever we only know that in the end we win or lose or points different

1418
01:26:07,300 --> 01:26:11,850
from chess where we only get this one bit of information here in goal we

1419
01:26:11,850 --> 01:26:16,360
have essentially three hundred sixty one bits of information that's something i learned from a

1420
01:26:16,520 --> 01:26:18,110
paper by nic schraudolph

1421
01:26:18,150 --> 01:26:20,390
the guy with the computer there

1422
01:26:20,400 --> 01:26:26,780
because he had realized that in the early nineties and i think that's a very

1423
01:26:26,840 --> 01:26:29,600
key thing for computer go i think

1424
01:26:29,610 --> 01:26:35,340
so another thing you might want to learn is how to select moves so evaluation

1425
01:26:35,340 --> 01:26:40,410
function learning was learn function that takes the position and see number that indicates the

1426
01:26:40,410 --> 01:26:44,870
position is now this would be for a given position

1427
01:26:44,890 --> 01:26:48,690
and all the possible moves to me how good each move is

1428
01:26:48,700 --> 01:26:52,600
and that's a different problem and of course here it might be more appropriate not

1429
01:26:52,600 --> 01:26:57,000
to look at the and positions but rather to look at the move actually made

1430
01:26:57,000 --> 01:27:01,650
by people and those we can get from the database and that's very interesting directions

1431
01:27:01,650 --> 01:27:03,250
forward i think

1432
01:27:03,280 --> 01:27:10,700
so the classical problem when approaching any learning problem is that of representation

1433
01:27:10,750 --> 01:27:16,460
and what we essentially requires that we have a certain degree of the and generality

1434
01:27:16,460 --> 01:27:20,940
in the representation because only then we can hope to generalize to larger

1435
01:27:21,100 --> 01:27:26,070
learn something more than just learning by heart the positions that we observe

1436
01:27:26,100 --> 01:27:31,090
yet we need sufficient information to actually draw the right conclusions

1437
01:27:31,100 --> 01:27:33,990
and of course we need learning algorithms that are able to

1438
01:27:34,490 --> 01:27:38,190
work on those representations

1439
01:27:39,380 --> 01:27:42,400
here's an example of a very recent work

1440
01:27:42,550 --> 01:27:49,820
by frank to grow out and this is actually a product that he's going to

1441
01:27:49,820 --> 01:27:57,800
commercialize and something i think would be a very good product it's called OIL and

1442
01:27:58,220 --> 01:27:59,670
frank's idea is

1443
01:27:59,710 --> 01:28:01,100
yes this huge

1444
01:28:01,100 --> 01:28:05,540
database of games half a million games i think he's working with a very strong

1445
01:28:05,550 --> 01:28:08,320
players and his idea is

1446
01:28:08,350 --> 01:28:13,320
to look at every single move made in within these games you can imagine this

1447
01:28:13,340 --> 01:28:15,570
kind of two hundred games

1448
01:28:15,590 --> 01:28:21,100
two and move per game and a half million games it's quite a few

1449
01:28:21,110 --> 01:28:25,880
quite a few moves and he looks at the surrounding patron of each of these

1450
01:28:26,880 --> 01:28:31,180
a bit like this in this diamond shape so to speak for this particular move

1451
01:28:31,450 --> 01:28:35,470
india's diamond shapes of different sizes of eight different sizes

1452
01:28:35,530 --> 01:28:38,440
so a small diamond shapes for

1453
01:28:38,440 --> 01:28:42,250
and those patterns that he would probably observe rather frequently

1454
01:28:42,280 --> 01:28:46,980
and he big diamond shapes that almost spend the entire board

1455
01:28:46,990 --> 01:28:52,050
and those would not be not be observed as frequently but if you observe them

1456
01:28:52,060 --> 01:28:55,050
then you have a very good idea that you may want to play there are

1457
01:28:55,660 --> 01:29:01,030
to play there and he's in this way he learning the value of different moves

1458
01:29:01,060 --> 01:29:03,690
and the amazing thing is this is not

1459
01:29:03,690 --> 01:29:09,700
really published the scientific results but he illustrate this on the game

1460
01:29:09,720 --> 01:29:14,870
he tried the system on the game that he wasn't in the database or that

1461
01:29:14,870 --> 01:29:20,840
he had used for training and he was able to achieve prediction of professional moves

1462
01:29:20,850 --> 01:29:23,370
of about forty five percent

1463
01:29:23,370 --> 01:29:29,270
shannon is is absolutely correct in i want to digitize everything to two he bandwidth

1464
01:29:29,400 --> 01:29:33,790
need for b and samples per second but that's assuming that i'm about to see

1465
01:29:33,790 --> 01:29:35,600
benjamin white noise

1466
01:29:35,620 --> 01:29:37,730
but in practice that's not what we see

1467
01:29:37,730 --> 01:29:41,650
in practice if you go into five to seven gigahertz band you can see little

1468
01:29:41,650 --> 01:29:44,650
pulses and things like this is not know is that all

1469
01:29:44,670 --> 01:29:50,920
so this seems to be about two requires actually extremely structure and extremely compressible and

1470
01:29:50,920 --> 01:29:55,190
that's where compressed sensing comes in and says if you're about to digitize things have

1471
01:29:55,290 --> 01:29:58,310
depend on the number of degrees of freedom which is much smaller

1472
01:29:58,350 --> 01:30:02,400
and what the shannon rate implies then you should be able in principle to sample

1473
01:30:02,400 --> 01:30:03,940
of a much reduced

1474
01:30:04,690 --> 01:30:06,650
without information loss

1475
01:30:06,670 --> 01:30:11,420
that's exactly what compressed sensing says and so we've seen an example of this would

1476
01:30:11,440 --> 01:30:12,850
say that the signal

1477
01:30:12,870 --> 01:30:15,620
has a sparse frequency spectrum

1478
01:30:15,620 --> 01:30:22,100
for example the GSM signaling if you know GSM protocols but GSM is allocating certain

1479
01:30:22,500 --> 01:30:28,620
bands for communications but what he does also that if you looking abandon next to

1480
01:30:28,620 --> 01:30:30,230
bands must be free

1481
01:30:30,250 --> 01:30:34,540
there must be emptied to avoid interferences and things like this and so when you

1482
01:30:34,540 --> 01:30:38,960
see GSM signal at any given time pretty sparse in the frequency domain lots of

1483
01:30:38,960 --> 01:30:43,190
signals are sparse in the frequency domain and just as an example of the toy

1484
01:30:44,170 --> 01:30:48,830
suppose we have a signal that has the spatial frequency spectrum well compressed sensing sense

1485
01:30:48,830 --> 01:30:54,620
the following it says if it has its consequences spectrum sample in a nonuniform locations

1486
01:30:54,620 --> 01:30:55,560
in time

1487
01:30:55,580 --> 01:30:57,500
we construct using

1488
01:30:57,520 --> 01:31:00,190
and linear programming

1489
01:31:00,190 --> 01:31:03,810
and what is guaranteed is that if the number of samples is above

1490
01:31:03,850 --> 01:31:08,330
the spectral occupancy times love factor then this reconstruction is

1491
01:31:08,350 --> 01:31:12,500
probably exact and so what you have by compressed sensing is that the shannon weight

1492
01:31:12,500 --> 01:31:18,540
is irrelevant what matters is information rate the amount of information contained per unit involved

1493
01:31:18,560 --> 01:31:21,250
and not the total bandwidth

1494
01:31:21,250 --> 01:31:23,000
OK so now

1495
01:31:25,080 --> 01:31:29,040
the US government agency has actually

1496
01:31:30,420 --> 01:31:31,520
created many

1497
01:31:31,600 --> 01:31:35,170
the US government has created many programs to actually leverage

1498
01:31:35,190 --> 01:31:41,500
compressed sensing in real hardware there's one associated with which is and that up two

1499
01:31:43,480 --> 01:31:49,790
and so is a circuit designers as the time me and others JPL we actually

1500
01:31:51,190 --> 01:31:58,170
new ABC's that will actually is sampled way below the shannon raids with hopefully without

1501
01:31:58,170 --> 01:32:00,020
much information loss

1502
01:32:00,020 --> 01:32:07,140
and so we are proposing to architecture going to propose show you most ambitious one

1503
01:32:07,140 --> 01:32:12,350
that does something like this so the incoming wave is coming i cannot sample it

1504
01:32:12,350 --> 01:32:14,370
that the shannon right it's just too high

1505
01:32:14,370 --> 01:32:19,750
it's three gigahertz there's no circuits that will let me sample signal six billion tons

1506
01:32:19,750 --> 01:32:24,830
per second what's interesting though is that i can change the polarity of the six

1507
01:32:24,830 --> 01:32:31,230
wave additional rate second multiplies single by plus or minus one additionally this i can

1508
01:32:31,230 --> 01:32:35,560
do because it has to do is changing the polarisation of the way so the

1509
01:32:35,560 --> 01:32:37,730
wave comes into its

1510
01:32:37,750 --> 01:32:40,460
it's the way as a function of time comes in

1511
01:32:40,480 --> 01:32:43,310
and enters a bank of channels

1512
01:32:43,330 --> 01:32:46,980
and each channel is actually essentially multiplying

1513
01:32:47,580 --> 01:32:53,370
incoming freak wave by random sequences of plus and minus once they're all difference

1514
01:32:53,400 --> 01:32:56,640
and of course i can digitize is the shannon way so what i'm doing is

1515
01:32:56,810 --> 01:33:01,040
integrating and i'm digitizing at a very slow rate

1516
01:33:01,040 --> 01:33:06,100
right so this is gets multiply the shannon rate then it gets integrated and then

1517
01:33:06,480 --> 01:33:09,670
we digitize the output at the shannon right

1518
01:33:09,670 --> 01:33:13,690
i do have a very slow rate is mathematically what one of these channels is

1519
01:33:14,870 --> 01:33:16,120
is essentially

1520
01:33:16,120 --> 01:33:21,650
taking an inner product between the incoming frequency with radio frequency wave and a random

1521
01:33:21,650 --> 01:33:23,850
sequence of plus and minus one

1522
01:33:23,870 --> 01:33:30,420
OK we actually spend an enormous amount of time designing this chip

1523
01:33:30,440 --> 01:33:34,710
and finally we have manufactured so now we're testing it

1524
01:33:34,740 --> 01:33:39,440
and so we have to build the ship was as time out be bright students

1525
01:33:39,440 --> 01:33:43,160
u two and u i must say that most of this work is by as

1526
01:33:43,160 --> 01:33:48,160
it and one and migrants steven bakker well we have actually put everything on the

1527
01:33:48,170 --> 01:33:52,480
chip it's very complex chip that has is a channel that you can see

1528
01:33:52,490 --> 01:33:55,480
one two three four five six seven eight

1529
01:33:55,570 --> 01:33:59,830
the uses little power at the moment six hundred twenty million watts

1530
01:33:59,860 --> 01:34:04,850
that is we believe can have an operation range of digitizing signal

1531
01:34:05,280 --> 01:34:09,610
in three gigahertz band it's a tiny chip to be made about to the minimum

1532
01:34:09,660 --> 01:34:15,040
and the dynamic range we hope to get about signals is about fifty db

1533
01:34:15,040 --> 01:34:15,920
OK so

1534
01:34:15,920 --> 01:34:19,210
problem actually going to think about now pretty much most of the time for the

1535
01:34:19,210 --> 01:34:23,750
rest of you know today and tomorrow is the problem of segmentation

1536
01:34:24,480 --> 01:34:26,880
some people believe that

1537
01:34:26,900 --> 01:34:31,150
a necessary precursor to making progress with

1538
01:34:31,210 --> 01:34:36,440
analyzing images you know try to build a general image processing system that could identify

1539
01:34:36,440 --> 01:34:42,100
components of images and was john winn would say explain every part of the everybody

1540
01:34:42,100 --> 01:34:45,290
in fact he takes it to an extreme and would say that the problem of

1541
01:34:45,350 --> 01:34:49,670
understanding images to explain the role of every pixel in an image of all humans

1542
01:34:49,670 --> 01:34:52,630
can do it by point topic so you know i choose one random you can

1543
01:34:52,630 --> 01:34:55,500
tell me what that pixel is doing you know it's part of the sky is

1544
01:34:55,500 --> 01:34:59,420
part of such so you know that i rather like that way of thinking about

1545
01:34:59,420 --> 01:35:04,420
the image understanding task explain every pixel and so as i say some people believe

1546
01:35:04,420 --> 01:35:08,870
the necessary precursor for that is to do some kind of segmentation

1547
01:35:08,900 --> 01:35:13,270
of the image into chunks that break the problem down without too much loss of

1548
01:35:14,350 --> 01:35:20,870
so there's been a program for some r one release ten years in berkeley two

1549
01:35:20,900 --> 01:35:23,750
both make this business of segmentation

1550
01:35:23,770 --> 01:35:26,610
but i have to say i'm skeptical because

1551
01:35:26,630 --> 01:35:27,480
what they do

1552
01:35:27,500 --> 01:35:34,440
to make progress on this problem is they get bank panel of humans to do

1553
01:35:34,460 --> 01:35:41,100
segmentations of images and and kind to use those as ground truth for evaluating the

1554
01:35:41,560 --> 01:35:47,750
the performance than of machine machine inference programs going to segmentation and

1555
01:35:49,270 --> 01:35:53,110
this annotated image is a map of all

1556
01:35:53,210 --> 01:35:57,770
what some some body of users said so bright line is the line that many

1557
01:35:57,770 --> 01:36:02,480
users selected the demo line is aligned only if you use the same brightness is

1558
01:36:02,480 --> 01:36:05,480
proportional to the number of years and what you see the as anything to me

1559
01:36:05,480 --> 01:36:11,130
anyway is the degree of disagreement amongst users and actually it may be an illusion

1560
01:36:11,130 --> 01:36:16,130
that you can come up to an image treated as a signal and say what

1561
01:36:16,130 --> 01:36:19,520
are the important politicians in the in the image to me this is going back

1562
01:36:19,520 --> 01:36:22,560
to that example i shown the beginning of the hand you know i said what

1563
01:36:22,560 --> 01:36:26,190
you would ideally like if you're naive easy is for the outline of the hand

1564
01:36:26,190 --> 01:36:30,830
just to come out but what you get with a good signal processing is something

1565
01:36:30,830 --> 01:36:35,850
much more arguable and ambiguous where actually a whole family of lines we you could

1566
01:36:35,850 --> 01:36:40,310
actually say that the lines in that in that piece signal processing are wrong it's

1567
01:36:40,310 --> 01:36:43,810
not the wrong is that there in some sense not fit for purpose and the

1568
01:36:43,810 --> 01:36:46,790
purpose is something that you have in your head it's not really something that you

1569
01:36:46,790 --> 01:36:53,290
can design define having been you know as as part of the problem spec

1570
01:36:53,350 --> 01:36:57,250
and here's another one in doing the same thing i suppose if you if you

1571
01:36:57,250 --> 01:37:01,440
came in with a very specific problem it may be you could do better and

1572
01:37:01,440 --> 01:37:07,710
define a low-level process that would pull out controls which were put into that process

1573
01:37:07,710 --> 01:37:11,060
and people are trying to do that kind of thing and definitely works but so

1574
01:37:11,060 --> 01:37:12,190
i i just want to say

1575
01:37:12,190 --> 01:37:17,130
i'm skeptical about segmentation the value of segmentation as a kind of module that you

1576
01:37:17,130 --> 01:37:21,880
can design build and test and then you forget about the the image after that

1577
01:37:21,880 --> 01:37:23,880
because you've you've simplify

1578
01:37:23,900 --> 01:37:26,100
on the other hand there are some

1579
01:37:26,100 --> 01:37:32,650
some less ambitious versions of the segmentation problem like the one that i mentioned before

1580
01:37:32,670 --> 01:37:36,400
which i think makes sense and you know they may not be the precursor for

1581
01:37:36,710 --> 01:37:40,290
vision vision systems that perform the level of animal vision

1582
01:37:41,170 --> 01:37:42,560
you can sell them

1583
01:37:42,580 --> 01:37:45,730
i'm gonna show

1584
01:37:45,730 --> 01:37:49,940
so here's the problem i'd like to be able to take this this foreground objects

1585
01:37:49,940 --> 01:37:54,690
and segmented here's the ground truth that segmentation and you know he is the kind

1586
01:37:54,690 --> 01:37:58,250
of evidence that i'm going to use all of a lot of foreground color and

1587
01:37:58,250 --> 01:38:03,290
background colour and it's well distinguished in this particular image so you can imagine that

1588
01:38:03,330 --> 01:38:06,400
in this particular case the job might not be that difficult although you know it's

1589
01:38:06,400 --> 01:38:10,040
not trivial because this is not this is not the kind of poster paint image

1590
01:38:10,040 --> 01:38:12,750
i mean if this was the blue star on a red background then we would

1591
01:38:12,750 --> 01:38:16,630
agree with trivial and you can all imagine the trivial matlab program that would do

1592
01:38:16,630 --> 01:38:21,580
the separation but here you know there's more interesting

1593
01:38:21,610 --> 01:38:25,000
the population of colours in the foreground and open the population in the background there

1594
01:38:25,000 --> 01:38:28,100
might be some overlap but i don't know what would you do you know what

1595
01:38:28,100 --> 01:38:33,460
you people are highly highly trained fighting unit now for a whole week plus the

1596
01:38:33,460 --> 01:38:37,750
additional years that you already spending a phd something i'd be interested in what people

1597
01:38:37,750 --> 01:38:41,580
you know your first reaction be if you want to make a separation program that

1598
01:38:41,580 --> 01:38:45,040
you know we use the colours to separate foreground from the background

1599
01:38:45,040 --> 01:38:46,420
in this image any

1600
01:38:49,170 --> 01:38:56,190
begun by gradient streams so then what property gradients would help people

1601
01:38:57,600 --> 01:38:58,100
and of course

1602
01:38:58,450 --> 01:39:00,940
o screen

1603
01:39:00,940 --> 01:39:06,920
do you think a straighter in the foreground and background by separation or background because

1604
01:39:06,920 --> 01:39:13,290
you mean that the gradients around there on the boundary yesterday i was not about

1605
01:39:13,290 --> 01:39:15,880
idea at all i think that would pull out the boundary of the star

1606
01:39:15,980 --> 01:39:18,500
i mean in the nature of things you know there are a few bumps here

1607
01:39:18,500 --> 01:39:22,460
so you know i worry that you might get a few leaks and that would

1608
01:39:22,460 --> 01:39:25,380
make it difficult for you i you're remember we're going to try and explain every

1609
01:39:25,380 --> 01:39:28,850
pixel all foreground pixels or the background is again a few leaks and then you

1610
01:39:28,850 --> 01:39:32,790
do flood filling or something to define the interior you're going to be in trouble

1611
01:39:32,790 --> 01:39:37,190
so i don't know if you have a good suggestion but i anybody got any

1612
01:39:37,190 --> 01:39:40,750
other any other ideas that would help characterize the interior

1613
01:39:41,210 --> 01:39:45,650
you know maybe just point to an interior pixel maybe the little clump of interior

1614
01:39:47,040 --> 01:39:53,130
to be used at all

1615
01:39:54,100 --> 01:40:02,670
so in

1616
01:40:02,750 --> 01:40:06,500
what we need to

1617
01:40:09,770 --> 01:40:15,480
that's a great answer that basically the rest of my lecture and tomorrow

1618
01:40:15,480 --> 01:40:19,190
this result take take

1619
01:40:19,230 --> 01:40:26,610
examples that require high things like the use of the link

1620
01:40:26,630 --> 01:40:28,310
two major

1621
01:40:30,210 --> 01:40:34,420
good idea so you know somehow biased towards the

1622
01:40:34,440 --> 01:40:39,540
colours are predominant in the foreground and away from the colours that from the background

1623
01:40:39,560 --> 01:40:42,440
i might get a bit of overlap between the colour palettes with your

1624
01:40:42,480 --> 01:40:44,500
your methods still be working

1625
01:40:44,560 --> 01:40:49,060
you know the purples i think is it's violence seem to be the foreground background

1626
01:40:49,060 --> 01:40:51,270
is going to cause trouble you think

1627
01:40:51,290 --> 01:40:53,810
cause the trouble

1628
01:40:54,600 --> 01:41:02,960
more speculation

1629
01:41:02,960 --> 01:41:05,850
so these are

1630
01:41:07,880 --> 01:41:13,080
i think it's a great suggestion when you take a photograph certain if it's sort

1631
01:41:13,080 --> 01:41:16,540
of you know deliberate photograph foreground object you try and get them focused on you

1632
01:41:16,540 --> 01:41:21,150
in the background is likely to be less focus so if you could somehow distinguish

1633
01:41:21,170 --> 01:41:23,830
you know the smoothness or lack of smoothness

1634
01:41:23,830 --> 01:41:51,100
the of you get all those on the test plus a couple more than what

1635
01:41:51,120 --> 01:41:57,590
do you want to those of basic formulas will plus transform it but if you

1636
01:41:57,590 --> 01:41:59,210
need anything else on

1637
01:41:59,230 --> 01:42:12,720
so I'm building those on board the basic task today is to see how will

1638
01:42:12,720 --> 01:42:18,820
plus transform so used to solve linear differential equation with constant coefficients

1639
01:42:18,930 --> 01:42:21,100
had to do

1640
01:42:21,250 --> 01:42:25,980
we're going to take the will plus transform of the derivative

1641
01:42:29,110 --> 01:42:31,940
In order to make sense of that procedure

1642
01:42:31,970 --> 01:42:38,720
will go laughter and I apologize if that's what it's slightly theoretical question namely we

1643
01:42:38,720 --> 01:42:44,300
have to have some guarantee in advance that the will plus transform is going to

1644
01:42:44,300 --> 01:42:48,950
exist now how people plus transform failed exist

1645
01:42:49,260 --> 01:42:54,750
can always calculate this and the answer is no you can't always calculate because it's

1646
01:42:54,750 --> 01:42:56,550
an improper integral

1647
01:42:56,660 --> 01:42:59,310
I'm integrating all the way up to infinity

1648
01:42:59,500 --> 01:43:03,880
and you know that improper integrals don't always converge

1649
01:43:04,110 --> 01:43:09,240
you know with the underground for example were just didn't have the exponential factor there

1650
01:43:09,240 --> 01:43:11,330
was we tdt that

1651
01:43:11,970 --> 01:43:17,770
it might look like it made sense but the integral of convergence infinity anyway there's

1652
01:43:17,770 --> 01:43:19,420
no value

1653
01:43:19,890 --> 01:43:28,080
so I mean conditions in advance which guarantees that people plus transforms will exist

1654
01:43:28,470 --> 01:43:32,650
only under those circumstances will formulas make any sense

1655
01:43:33,450 --> 01:43:38,610
there is a standard conditions in your book of a chance to talk about last

1656
01:43:38,610 --> 01:43:43,030
time so I thought I'd better than the 1st few minutes to talking about that

1657
01:43:43,030 --> 01:43:46,830
condition because that's what we're going to need in order to be able to solve

1658
01:43:46,830 --> 01:43:47,920
differential equations

1659
01:43:48,660 --> 01:43:51,960
the condition that makes plus transform

1660
01:43:51,980 --> 01:43:59,520
definitely exist for a function is that activity should grow to rapidly it can grow

1661
01:43:59,520 --> 01:44:01,590
rapidly on

1662
01:44:01,610 --> 01:44:07,080
because they can grow world because the minus st is pulling it down trying hard

1663
01:44:07,080 --> 01:44:12,700
to go down to 0 and the integral converges all we have to do is

1664
01:44:12,870 --> 01:44:18,390
to guarantee that it doesn't grow so rapidly that e the minus st is powerless

1665
01:44:18,390 --> 01:44:19,140
to pull it down

1666
01:44:20,260 --> 01:44:24,400
now the conditioned its growth what's called the growth conditions

1667
01:44:24,800 --> 01:44:29,380
this is a very important applications and this is the most important it's not always

1668
01:44:29,610 --> 01:44:34,880
it's always on 1801 but it's not always taught in high school calculus and

1669
01:44:35,580 --> 01:44:40,000
it's a question of how fast the function is allowed to grow and that the

1670
01:44:40,240 --> 01:44:47,100
condition is universally said this way should be of exponential types

1671
01:44:47,110 --> 01:44:56,200
so what I'm defining is the phrase all put quotation marks for that reason what

1672
01:44:56,200 --> 01:44:58,390
does this mean

1673
01:44:58,430 --> 01:45:04,980
it's a condition of growth condition on functions how fast the candidate of it said

1674
01:45:06,330 --> 01:45:12,870
activity inside synset team I get negatively very large that would hurt to make the

1675
01:45:12,870 --> 01:45:18,770
integral part to converge but not like the convergence of use the absolute value of

1676
01:45:18,870 --> 01:45:22,150
I'm getting is going up or going down very well

1677
01:45:22,300 --> 01:45:29,700
whichever way it goes it's you should not be bigger than

1678
01:45:29,740 --> 01:45:34,800
all rapidly growing exponentially and here a rapidly growing exponentially

1679
01:45:35,820 --> 01:45:36,960
of the

1680
01:45:37,020 --> 01:45:38,980
is some positive constants

1681
01:45:39,580 --> 01:45:46,500
for some positive constants and so

1682
01:45:46,560 --> 01:45:49,210
positive constant

1683
01:45:49,540 --> 01:45:54,540
and should be true for all values of the

1684
01:45:54,560 --> 01:46:00,530
greater equal to 0 I don't have to worry about negative values of because the

1685
01:46:00,530 --> 01:46:05,790
integral doesn't contain doesn't care about them into I'm only the integration as the runs

1686
01:46:05,790 --> 01:46:11,410
from 0 to infinity in other words activity could have been extremely while function of

1687
01:46:11,590 --> 01:46:18,620
the of levels so whatever functions of the negative values of T and we don't

1688
01:46:19,840 --> 01:46:25,140
it's only what's happening from now from time 0 onto effect of you know it

1689
01:46:25,260 --> 01:46:29,540
as long as it is now from now on it's OK alright so the way

1690
01:46:29,540 --> 01:46:31,820
to behave as by being an exponential

1691
01:46:35,120 --> 01:46:40,600
the charges some feeling for what this means these functions for example if K is

1692
01:46:40,600 --> 01:46:46,560
a hundred years and you with plot of the 100 looks like those straight of

1693
01:46:47,800 --> 01:46:52,040
on every computer plot on 180 go like that

1694
01:46:52,820 --> 01:46:57,540
unless of course you make the scale t equals 0 2 over here is 1

1695
01:46:59,380 --> 01:47:02,560
well you not to OK

1696
01:47:04,120 --> 01:47:07,500
so these functions really can go quite rapidly

1697
01:47:07,880 --> 01:47:12,450
but let's take an example and see what's of exponential type and then perhaps even

1698
01:47:12,450 --> 01:47:14,320
more interestingly what is it

1699
01:47:14,890 --> 01:47:21,330
so for instance the function of society is that of exponential time will

1700
01:47:21,380 --> 01:47:24,390
it's absolute value is always less than or equal to

1701
01:47:24,410 --> 01:47:29,970
so all right falls in this paradigm if takes equal to 1 of which I

1702
01:47:29,970 --> 01:47:32,080
think be

1703
01:47:33,000 --> 01:47:40,560
they can be 0 C equals 1 fact signed the basic conditions is 1 is

1704
01:47:40,590 --> 01:47:45,290
more interesting team

1705
01:47:46,120 --> 01:47:52,200
they get to power

1706
01:47:52,380 --> 01:47:56,700
is smaller than some exponential with maybe a constant out

1707
01:47:57,500 --> 01:48:01,890
well to 100 power go straight up also

1708
01:48:02,720 --> 01:48:09,130
well we feel that if we make the exponential big enough maybe you went out

1709
01:48:09,420 --> 01:48:10,160
in fact

1710
01:48:11,120 --> 01:48:15,480
you have to make the exponential big k equals 1 is good

1711
01:48:15,910 --> 01:48:19,150
in other words I say I don't have to put absolute value signs around the

1712
01:48:19,150 --> 01:48:22,850
deviance because I'm only thinking about the being a positive number anyway

1713
01:48:23,600 --> 01:48:29,760
I say that that's less than or equal to some constant and positive constant times

1714
01:48:29,950 --> 01:48:34,350
E to the T will be

1715
01:48:34,410 --> 01:48:35,910
for something

1716
01:48:36,380 --> 01:48:43,920
and 2 and that

1717
01:48:45,020 --> 01:48:51,580
the way you think of them so what this proves is that there for 2

1718
01:48:51,580 --> 01:48:55,460
years of exponential type which we could get because after all we were able to

1719
01:48:55,460 --> 01:48:57,180
calculate will plus transform

1720
01:48:57,180 --> 01:49:02,080
is no longer clear which haven't put down to be promised corner yet put down

1721
01:49:02,100 --> 01:49:03,590
a beeper

1722
01:49:03,600 --> 01:49:07,290
right and if we say that puppy often we close this often does so we

1723
01:49:07,290 --> 01:49:10,180
know that it's in fact running

1724
01:49:10,300 --> 01:49:13,840
to do we run our little program

1725
01:49:13,850 --> 01:49:15,030
and rock on

1726
01:49:15,030 --> 01:49:15,810
all right

1727
01:49:15,840 --> 01:49:19,230
no more of by one by life is good so any questions about the whole

1728
01:49:19,810 --> 01:49:21,330
of by one by

1729
01:49:21,340 --> 01:49:24,560
that's another common thing that the common couple common things i want to show you

1730
01:49:24,560 --> 01:49:27,220
so you know and if they come up in your program you don't feel like

1731
01:49:27,220 --> 01:49:30,610
all you're out there drift all along like these have been done millions of times

1732
01:49:30,610 --> 01:49:35,550
by other very qualified programmers don't worry it happens to every are some one of

1733
01:49:35,600 --> 01:49:39,410
things i just mentioned is this thing called the comments this thing up here in

1734
01:49:39,410 --> 01:49:41,300
green what's that all about

1735
01:49:41,410 --> 01:49:45,590
so as i mentioned last time one of the key software engineering principles is to

1736
01:49:45,590 --> 01:49:51,090
write programs that are understandable by people not just by machines and so what comment

1737
01:49:51,100 --> 01:49:56,280
is is always being able to put something in the program that another human being

1738
01:49:56,280 --> 01:50:01,140
reading your program can actually read and understand that actually has no impact on the

1739
01:50:01,140 --> 01:50:03,530
execution of the program at all

1740
01:50:03,540 --> 01:50:08,040
so the way the common works is it starts off with the splash star and

1741
01:50:08,040 --> 01:50:09,470
now everything you put

1742
01:50:09,490 --> 01:50:14,800
the main span multiple lines until you pointed star and slash is the commons it's

1743
01:50:14,800 --> 01:50:18,530
there just for the person to read it doesn't affect the program at all

1744
01:50:18,530 --> 01:50:21,500
and so what i would encourage you to do is part of good programming styles

1745
01:50:21,520 --> 01:50:25,200
all your programs up at the top should have a comment that says what the

1746
01:50:25,200 --> 01:50:28,370
name of the file is and has a little bit of an explanation about what

1747
01:50:28,370 --> 01:50:31,960
your program actually does and if you wonder how much of an explanation around at

1748
01:50:31,980 --> 01:50:35,050
the beginning be the end up as you want but in the hand that you've

1749
01:50:35,050 --> 01:50:38,680
got you see examples of major programs and they're all commented so you can see

1750
01:50:38,680 --> 01:50:42,420
examples of comments and that there's also short and you can do there's a comment

1751
01:50:42,420 --> 01:50:43,200
that's just

1752
01:50:43,230 --> 01:50:47,880
slash slash that means everything on the remainder of this line is the comment so

1753
01:50:47,880 --> 01:50:51,540
it actually has no close in some sense for the comment like this does explicitly

1754
01:50:51,800 --> 01:50:55,150
everything else the rest the line when you hit return that's how it was done

1755
01:50:55,150 --> 01:50:58,280
for comments sometimes if you want to have a one line comment somewhere you can

1756
01:50:58,280 --> 01:51:00,240
just put a slash slash question

1757
01:51:00,410 --> 01:51:18,520
yeah when you get assigned to a different section leader you'll actually put your assignments

1758
01:51:18,540 --> 01:51:22,180
in with your name and your section leader and that's how we get differentiated so

1759
01:51:22,180 --> 01:51:24,800
wait until you get your first action and you'll see how

1760
01:51:24,820 --> 01:51:28,170
all the submissions that works

1761
01:51:28,180 --> 01:51:37,370
well you when you write like other methods

1762
01:51:37,380 --> 01:51:41,080
yeah you should comment those as well so methods should also be commented on show

1763
01:51:41,080 --> 01:51:44,230
an example of that they actually so you can see an example of how we

1764
01:51:44,230 --> 01:51:48,680
do that so that's actually and i love what everyone just you know i like

1765
01:51:48,680 --> 01:51:51,960
it like you in the audience leading into the next topic and that's a perfect

1766
01:51:51,960 --> 01:51:56,040
example the say hey let's actually looking looking to program that we did last time

1767
01:51:56,040 --> 01:52:00,890
which was the steeplechase program now with comments OK so up at the top sometimes

1768
01:52:00,890 --> 01:52:03,840
you need to click this little plus sign to expand the common because the eclipse

1769
01:52:03,840 --> 01:52:08,160
has the tendency to try to minimize the comments automatically so just click the plus

1770
01:52:08,160 --> 01:52:11,790
sign will spend about steeplechaser we have and i still comment here at the top

1771
01:52:11,790 --> 01:52:16,530
of our programmes notice this is a job files i mentioned before and with job

1772
01:52:16,530 --> 01:52:21,170
because carols actually implemented in java but you shouldn't use no features of java other

1773
01:52:21,170 --> 01:52:23,940
than what is it in the carol book and if you like a and i

1774
01:52:23,940 --> 01:52:27,780
don't know job enough for you to unlearn you're good to go because you just

1775
01:52:27,780 --> 01:52:30,260
know carol right

1776
01:52:30,280 --> 01:52:33,940
the other thing that's going on here is you'll notice that inside the program we

1777
01:52:33,940 --> 01:52:37,080
have comments as well so this method to run method it has to run the

1778
01:52:37,080 --> 01:52:40,040
race that's nine avenues long we need to move forward john e

1779
01:52:40,050 --> 01:52:43,410
articles eight times and that makes it explicit that person right if we didn't have

1780
01:52:43,450 --> 01:52:46,840
a common someone come along and say hey the rate nine avenues wiry you only

1781
01:52:46,840 --> 01:52:50,570
iterating this eight times right that's a natural question to ask and they put in

1782
01:52:50,570 --> 01:52:55,450
comments to clarify things in the programme which are not obvious to a another thing

1783
01:52:55,450 --> 01:53:00,540
related to that is which refer to as preconditions and postconditions what did you expect

1784
01:53:00,540 --> 01:53:05,390
to be true before you call the particular method or before particular method is invoked

1785
01:53:05,590 --> 01:53:10,270
and what's going to be true afterwards storefront jump here remember jumper from last time

1786
01:53:10,270 --> 01:53:14,080
we ascend portal which means we go up we moved cross over the top of

1787
01:53:14,080 --> 01:53:18,330
the island we decentralised come back down the other side well what we needed for

1788
01:53:18,330 --> 01:53:19,500
that to be true

1789
01:53:19,520 --> 01:53:23,540
for this to actually work right is the precondition is you're facing east at the

1790
01:53:23,540 --> 01:53:26,690
bottom of the herd which means the was right in front of you and you're

1791
01:53:26,690 --> 01:53:30,300
facing east in front of it that way we know which way to turn on

1792
01:53:30,300 --> 01:53:33,050
how to ascend the model and how to get over it and when you're done

1793
01:53:33,050 --> 01:53:36,560
you're going to be facing east again so you can assume after this method is

1794
01:53:36,560 --> 01:53:41,190
done that you're facing east again and you're at the bottom of the world in

1795
01:53:41,190 --> 01:53:45,530
the next avenue after the so makes clear right because we don't know what all

1796
01:53:45,530 --> 01:53:50,390
stuff is going on inside on inside central in the central but programmer now designated

1797
01:53:50,390 --> 01:53:55,090
trace through the execution of your program to figure out what's going on this tells

1798
01:53:55,090 --> 01:53:58,270
them what needs to be true before and what needs to be trapped it helps

1799
01:53:58,270 --> 01:54:04,980
need for my specific applications and here is the clustering that will have those properties

1800
01:54:04,980 --> 01:54:08,070
the thing that we know from kleinberg is that you want to have the clustering

1801
01:54:08,320 --> 01:54:11,210
that satisfies all the possible properties

1802
01:54:11,970 --> 01:54:14,390
you know the same for many things in life

1803
01:54:14,430 --> 01:54:20,940
you can get the car which is beautiful and fast and convenient and cheap or

1804
01:54:23,160 --> 01:54:27,210
this is the ideal period i would like to say so here i have

1805
01:54:27,280 --> 01:54:29,700
lists of

1806
01:54:29,740 --> 01:54:31,950
things that are called axioms

1807
01:54:31,970 --> 01:54:38,060
and here we have a list of possible clustering paradigms and then can ask which

1808
01:54:38,060 --> 01:54:42,150
clustering power and satisfies which requirement now

1809
01:54:42,160 --> 01:54:46,560
the axioms should be satisfied by all clustering paradigms

1810
01:54:46,610 --> 01:54:50,940
the things which are properties like kind consistency clambers richness

1811
01:54:51,130 --> 01:54:56,420
satisfied by some paradigms but not by others and that should have been somehow

1812
01:54:59,740 --> 01:55:05,500
taxonomies the class different clustering methods by which properties are satisfied

1813
01:55:05,500 --> 01:55:10,080
two each clustering method will have a profile of which properties it's fine we should

1814
01:55:11,110 --> 01:55:15,810
the axioms should be satisfied all of them and if i have a function which

1815
01:55:15,810 --> 01:55:19,540
is not a clustering function it should at least fail one of those

1816
01:55:21,890 --> 01:55:26,600
the all clustering functions to satisfy all the the properties they call axioms

1817
01:55:26,660 --> 01:55:31,320
other properties distinguish between different clustering methods and

1818
01:55:31,350 --> 01:55:36,090
functions which are not clustering to each police force if i one of those

1819
01:55:36,110 --> 01:55:37,910
accept that

1820
01:55:37,910 --> 01:55:41,440
the idea deity only the and now i'm not going to give you such a

1821
01:55:41,440 --> 01:55:42,990
theory because

1822
01:55:43,020 --> 01:55:47,730
the they had difficulty here is that we don't have a clear definition of what

1823
01:55:47,730 --> 01:55:50,580
is not clustering i mean

1824
01:55:50,740 --> 01:55:54,710
i have this requirement every function which is not the clustering should forty five one

1825
01:55:54,710 --> 01:55:59,310
of the axioms but it's clear how should i defined it is not the clustering

1826
01:55:59,310 --> 01:56:03,410
by showing that it doesn't satisfy one of the properties that they want to do

1827
01:56:03,420 --> 01:56:08,360
so i i get into a side here but i can still come up with

1828
01:56:08,380 --> 01:56:13,460
some properties which are more natural to be axioms than others and is satisfied by

1829
01:56:13,480 --> 01:56:16,790
all the reasonable common clustering

1830
01:56:19,330 --> 01:56:25,060
i want to give you some example of such properties so it in general we

1831
01:56:25,060 --> 01:56:26,160
can say that

1832
01:56:27,690 --> 01:56:32,940
at that some of the requirements of the really partition into two types of

1833
01:56:32,950 --> 01:56:36,100
properties region it's properties

1834
01:56:36,130 --> 01:56:40,550
so one of the most the kind which is another one who became richness the

1835
01:56:40,550 --> 01:56:46,210
range of your partitioning is all possible partitioning into k that so

1836
01:56:46,230 --> 01:56:51,400
so we need some which is requirement and we need that answering back this

1837
01:56:52,730 --> 01:56:55,900
like saying scale invariance if you change the scale

1838
01:56:55,960 --> 01:56:58,980
the clustering were not change so these are two different

1839
01:56:59,000 --> 01:57:03,550
types of properties and for each of them we can come up with relaxations that

1840
01:57:03,550 --> 01:57:09,070
i really meant by all really common popular clustering paradigms

1841
01:57:09,090 --> 01:57:14,710
and so i'll just give you one example of

1842
01:57:14,730 --> 01:57:19,860
the relaxation of the property of clans that what you do this relaxation

1843
01:57:19,880 --> 01:57:20,980
you really

1844
01:57:21,010 --> 01:57:25,710
and i have it in all reasonable clustering functions so

1845
01:57:25,760 --> 01:57:30,110
the problem with the clan because it's so let me remind you what causes the

1846
01:57:30,110 --> 01:57:34,340
was because it was the requirements that if i pull together points in the same

1847
01:57:34,340 --> 01:57:38,170
cluster important class different clusters apart then

1848
01:57:38,210 --> 01:57:39,730
apply my

1849
01:57:39,740 --> 01:57:43,230
clustering function again and get the same partition

1850
01:57:43,230 --> 01:57:48,940
but the problem is this it could be that may be drawn picture here the

1851
01:57:48,940 --> 01:57:51,400
problem is this could be that assumed they have

1852
01:57:51,420 --> 01:57:53,070
this might

1853
01:57:53,090 --> 01:57:55,460
initially looks like this

1854
01:57:55,460 --> 01:57:58,150
so my clustering function would say

1855
01:57:58,150 --> 01:58:00,340
well here is my clustering

1856
01:58:00,340 --> 01:58:01,340
and now

1857
01:58:01,340 --> 01:58:08,500
i'm pulling together points in the same cluster and i get something like

1858
01:58:08,510 --> 01:58:13,050
so i just took some of the point in this class and put them together

1859
01:58:13,110 --> 01:58:16,730
and i don't touch the other or pull away

1860
01:58:16,740 --> 01:58:17,650
the other class

1861
01:58:19,000 --> 01:58:22,010
so i screamed this cluster

1862
01:58:22,070 --> 01:58:26,190
a little bit away half of those points will shrink to here and the other

1863
01:58:26,190 --> 01:58:31,610
half here so this is consistent change inside this class only

1864
01:58:31,730 --> 01:58:36,500
reduce the distances between the clusters i only increased distances but if i give you

1865
01:58:36,500 --> 01:58:42,940
this clustering you're probably give me not this clustering but something like this

1866
01:58:49,840 --> 01:58:54,820
you know you your wallet about the fact that i didn't fix scale again i'm

1867
01:58:54,820 --> 01:59:00,380
talking about and i would to find the correct number of clusters and then class

1868
01:59:00,440 --> 01:59:03,760
so i'm saying is that consistency requirements although it

1869
01:59:03,780 --> 01:59:09,510
on the first glance it looks much it has problems because the natural examples where

1870
01:59:09,510 --> 01:59:12,460
you want to refute it

1871
01:59:14,820 --> 01:59:19,530
because i don't know how to draw but i i could do it without increasing

1872
01:59:19,530 --> 01:59:23,980
any internal distances i could do it can i mean it doesn't even have to

1873
01:59:23,980 --> 01:59:30,880
be embedded in in say that all the distances here in the x y

1874
01:59:30,900 --> 01:59:33,920
equals one for every x and y

1875
01:59:33,960 --> 01:59:36,550
and between those two two

1876
01:59:36,570 --> 01:59:38,610
the x y

1877
01:59:38,670 --> 01:59:41,760
it was then if we pick point from here point from here

1878
01:59:41,800 --> 01:59:44,480
and now i make all the difference here

1879
01:59:44,500 --> 01:59:47,840
the x y here it was pointwise

1880
01:59:47,940 --> 01:59:49,780
and here the x y

1881
01:59:49,800 --> 01:59:51,590
it was point one

1882
01:59:51,610 --> 01:59:53,050
and between those two

1883
01:59:53,070 --> 01:59:55,550
i still have distance two

1884
01:59:55,570 --> 01:59:59,240
so i didn't extend any inside

1885
01:59:59,260 --> 02:00:01,650
i can it one

1886
02:00:01,710 --> 02:00:04,730
good to know the people of poland

1887
02:00:04,780 --> 02:00:10,340
so i still have distance one between these two groups but inside the group restricted

1888
02:00:10,340 --> 02:00:13,090
this is the point

1889
02:00:13,090 --> 02:00:16,650
and i'll get to stimuli and when i try to draw it of course i

1890
02:00:16,650 --> 02:00:19,880
i don't do precisely but you can easily

1891
02:00:19,880 --> 02:00:22,920
formalise such a situation

1892
02:00:25,960 --> 02:00:30,500
suggested remedy for this problem is to say that the consistency

1893
02:00:30,510 --> 02:00:35,940
should be not allow you to change the proportions which i within each class so

1894
02:00:35,940 --> 02:00:36,980
you have

1895
02:00:37,030 --> 02:00:39,610
a list of constant

1896
02:00:39,690 --> 02:00:42,460
a lambda i for every cluster

1897
02:00:42,480 --> 02:00:43,760
and the new

1898
02:00:43,820 --> 02:00:47,150
distance function should be

1899
02:00:47,170 --> 02:00:51,170
if a and b are in the same cluster is the oldest distance times the

1900
02:00:51,210 --> 02:00:52,420
constant lambda

1901
02:00:52,460 --> 02:00:57,690
and if they are in different clusters if the old distance times some london you

1902
02:00:57,800 --> 02:00:58,710
so i p

1903
02:00:58,730 --> 02:01:01,920
constants for every cluster have a shrinking constant

1904
02:01:02,010 --> 02:01:05,710
and for all the between distances i have some expansion constant

1905
02:01:05,730 --> 02:01:06,880
and if i now

1906
02:01:06,880 --> 02:01:10,090
do my shrinking and expansion

1907
02:01:10,090 --> 02:01:13,900
i'm running out of kind of counter examples and i can show

