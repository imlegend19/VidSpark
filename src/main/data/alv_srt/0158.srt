1
00:00:00,000 --> 00:00:10,600
all the same for all of

2
00:00:11,330 --> 00:00:13,870
so this is a little bit of

3
00:00:13,890 --> 00:00:16,230
this is just

4
00:00:16,240 --> 00:00:18,040
the feature space

5
00:00:18,050 --> 00:00:23,980
the station

6
00:01:03,820 --> 00:01:07,150
some of the theory

7
00:01:10,240 --> 00:01:12,090
it's called

8
00:02:01,680 --> 00:02:04,560
we see that

9
00:02:04,570 --> 00:02:09,210
so what is this

10
00:02:11,020 --> 00:02:15,500
one is that was the

11
00:02:15,510 --> 00:02:17,270
generate a new

12
00:02:19,560 --> 00:02:22,830
learning is some

13
00:02:22,990 --> 00:02:29,390
the chinese i is here

14
00:02:29,410 --> 00:02:31,250
one of the issues

15
00:02:41,700 --> 00:02:43,570
this is what she is

16
00:02:43,870 --> 00:02:49,840
this is in a way that you love it

17
00:03:01,990 --> 00:03:04,140
so here

18
00:03:04,430 --> 00:03:05,980
but due

19
00:03:05,980 --> 00:03:10,810
however would be made by

20
00:03:16,030 --> 00:03:21,840
what we want

21
00:03:26,900 --> 00:03:30,060
this image

22
00:03:33,840 --> 00:03:36,220
you have

23
00:03:43,100 --> 00:03:45,450
so yes

24
00:03:45,520 --> 00:03:52,530
this all this is something that i don't like that

25
00:03:54,050 --> 00:03:56,650
this is

26
00:03:58,880 --> 00:04:03,700
as the error average is unknown regularities my so this means

27
00:04:03,710 --> 00:04:06,100
i've only observed these five points but i'm

28
00:04:06,160 --> 00:04:08,670
assuming i could get more similar points

29
00:04:08,680 --> 00:04:10,710
and i want to estimate

30
00:04:10,780 --> 00:04:12,820
the mapping from x to y

31
00:04:12,880 --> 00:04:13,850
such that

32
00:04:13,860 --> 00:04:17,140
evolved from all points in the future is likely that my

33
00:04:17,150 --> 00:04:20,560
my estimate is good

34
00:04:20,570 --> 00:04:24,310
for instance if i estimator point between these two points

35
00:04:24,310 --> 00:04:29,710
so if differences and giving x and i want to estimate the corresponding y

36
00:04:29,760 --> 00:04:33,110
five hundred my estimation function at x

37
00:04:33,160 --> 00:04:37,040
i want to know whether the estimate is good or bad

38
00:04:37,040 --> 00:04:40,250
so given this five many points i could come up with various estimates i could

39
00:04:40,250 --> 00:04:44,080
say this is a straight line effectively underlying regularity

40
00:04:44,140 --> 00:04:47,050
it should be something like this but i would also say well this straight line

41
00:04:47,050 --> 00:04:49,310
doesn't explain my points for

42
00:04:49,350 --> 00:04:53,580
so why don't i use polynomial of sufficiently high degree and i can explain my

43
00:04:53,580 --> 00:04:55,530
point exactly

44
00:04:55,570 --> 00:04:59,110
otherwise the lies that that's always that

45
00:04:59,170 --> 00:05:01,290
maybe worse than the straight line

46
00:05:01,330 --> 00:05:05,860
since every physicist if you're resistant measure this quantity would you would probably say well

47
00:05:05,860 --> 00:05:10,620
the underlying dependency must be a straight line and all these minerals are this measurement

48
00:05:13,280 --> 00:05:16,870
and if you ask why is this a bit explanation and many would say she

49
00:05:16,870 --> 00:05:17,780
would say

50
00:05:17,800 --> 00:05:21,630
it's better because it's a simple explanation

51
00:05:21,650 --> 00:05:26,740
this notion of simplicity is exactly what's being studied by statistical learning theory so i

52
00:05:26,750 --> 00:05:27,590
have to

53
00:05:27,600 --> 00:05:29,760
trying to explain to you what does it mean for

54
00:05:29,760 --> 00:05:31,810
an explanation to be simple

55
00:05:31,820 --> 00:05:39,370
and what kind of consequences does have from a statistical point of view

56
00:05:42,150 --> 00:05:48,060
i think the next slide i have another example from pattern recognition and other

57
00:05:48,100 --> 00:05:54,280
it's toy example here we have a two class classification problem

58
00:05:55,680 --> 00:05:59,530
it is what point and the black ones and on the left we have a

59
00:05:59,530 --> 00:06:00,540
very simple

60
00:06:00,540 --> 00:06:01,970
decision rule

61
00:06:01,980 --> 00:06:03,760
it is so simple that it

62
00:06:04,050 --> 00:06:06,820
actually gets a lot of the points wrong

63
00:06:06,840 --> 00:06:10,530
here we have a very complex one which gets all the points right but

64
00:06:10,540 --> 00:06:14,280
maybe it's too complex and you could say this over here measurement error in this

65
00:06:14,280 --> 00:06:15,560
one as well

66
00:06:15,570 --> 00:06:19,570
and here in the middle you have a decision rule which is about halfway between

67
00:06:19,570 --> 00:06:23,100
simple and complex get some of the point light

68
00:06:23,120 --> 00:06:25,850
some of the points wrong but most of them right

69
00:06:25,860 --> 00:06:29,250
and you might want to say that this is the best ones within

70
00:06:30,120 --> 00:06:33,480
what is the reason for saying that

71
00:06:34,990 --> 00:06:38,590
so let's get a little bit more formal about things

72
00:06:38,610 --> 00:06:42,320
and right down the problem of pattern recognition

73
00:06:42,330 --> 00:06:45,090
same pattern recognition you want to learn a function

74
00:06:45,150 --> 00:06:48,150
taking our input domain two plus minus one

75
00:06:48,200 --> 00:06:50,460
from training examples

76
00:06:50,610 --> 00:06:55,300
was still making this is something that all training examples are generated i i d

77
00:06:55,310 --> 00:06:58,950
it's independently and identically distributed

78
00:06:59,000 --> 00:07:04,840
from some under no underlying joint probability distribution

79
00:07:04,870 --> 00:07:08,640
so the model is we're doing performing a random experiment

80
00:07:09,260 --> 00:07:11,490
we have some small

81
00:07:11,500 --> 00:07:18,890
of the data quality p of x y and we're generating are in the top

82
00:07:18,890 --> 00:07:24,550
pairs by performing m times around the experiment described by the distribution

83
00:07:24,560 --> 00:07:26,530
so for instance

84
00:07:26,540 --> 00:07:30,300
random experiment where you're tossing some kind

85
00:07:30,310 --> 00:07:34,650
only is the complicated here in that the random experiment every time you do it

86
00:07:34,650 --> 00:07:36,320
gives you a x and y

87
00:07:36,370 --> 00:07:39,500
every time you do the random experiment you such pair

88
00:07:39,540 --> 00:07:43,280
and if you do it in time exactly the same thing in the results of

89
00:07:43,320 --> 00:07:47,630
previous ones don't influence the results of the future ones

90
00:07:47,650 --> 00:07:51,290
so all these are independent

91
00:07:51,330 --> 00:07:53,490
so that's how you get your data

92
00:07:54,500 --> 00:07:56,020
the goal is that

93
00:07:56,040 --> 00:08:03,010
the classifier f but you should also work well on future data on the test

94
00:08:04,220 --> 00:08:06,290
more generally should also work well

95
00:08:06,310 --> 00:08:07,960
on average

96
00:08:09,130 --> 00:08:11,290
the whole distribution

97
00:08:11,340 --> 00:08:13,500
on average over the distribution means

98
00:08:13,580 --> 00:08:20,850
whatever you would get if you sample infinitely many computer points from the distribution

99
00:08:20,900 --> 00:08:25,070
so what you would like to be small is the expected misclassification error on the

100
00:08:25,070 --> 00:08:28,240
test set so i've written down this function here

101
00:08:28,250 --> 00:08:31,990
this is what people call the zero one loss functions

102
00:08:32,000 --> 00:08:35,220
so if you think about it for a second this

103
00:08:35,280 --> 00:08:36,300
labels y

104
00:08:36,310 --> 00:08:38,180
plus minus one

105
00:08:38,180 --> 00:08:39,300
otherwise here

106
00:08:39,390 --> 00:08:41,240
these are the class membership

107
00:08:41,260 --> 00:08:45,120
the effort to estimating takes values plus minus one

108
00:08:45,120 --> 00:08:49,080
so these two quantities both plus and minus one whenever they agree three

109
00:08:49,160 --> 00:08:50,700
the difference is zero

110
00:08:50,740 --> 00:08:54,740
whenever they disagree the difference has one genus tool

111
00:08:54,740 --> 00:08:58,490
differences plus two minus two we multiply with one of the two and we take

112
00:08:59,140 --> 00:09:00,850
what you here so

113
00:09:00,850 --> 00:09:02,600
whenever x and y

114
00:09:03,030 --> 00:09:05,870
don't fit together

115
00:09:07,370 --> 00:09:11,850
this quantity is plus-one one so whenever we make errors plus one

116
00:09:11,850 --> 00:09:14,910
therefore this quantity is just the average

117
00:09:14,930 --> 00:09:16,550
error if you like

118
00:09:16,570 --> 00:09:19,720
or if you mathematician and this is the expectation

119
00:09:19,780 --> 00:09:21,370
of the error

120
00:09:21,430 --> 00:09:24,220
expectation with respect to this

121
00:09:24,300 --> 00:09:27,700
probability distribution has generated the data

122
00:09:27,740 --> 00:09:32,890
now the assumption learning theory is that we don't know this probability distribution because if

123
00:09:32,890 --> 00:09:36,950
we knew this distribution then we will be in the room with probability theory then

124
00:09:36,950 --> 00:09:41,570
we could just derive everything from knowledge of the distribution

125
00:09:41,580 --> 00:09:43,410
in statistical learning theory

126
00:09:43,630 --> 00:09:47,010
assuming that you don't know the distribution and the only thing you know is this

127
00:09:47,010 --> 00:09:51,410
sample from distribution so here that's interesting difference between

128
00:09:51,470 --> 00:09:55,120
statistical learning theory and bayesian learning

129
00:09:55,200 --> 00:09:58,620
and maybe also classical statistics

130
00:09:58,660 --> 00:10:02,910
so in classical statistics of course people also solve this problem maybe maybe didn't write

131
00:10:02,910 --> 00:10:06,100
it down exactly the same way some certainly did

132
00:10:06,160 --> 00:10:09,240
this problem that you don't know the distribution

133
00:10:09,260 --> 00:10:13,260
he said well if you don't know it let's just assume it's of some parametric

134
00:10:13,260 --> 00:10:18,010
form so that that a about but we don't know the mean and variance

135
00:10:18,030 --> 00:10:21,580
and then we can write down things and somehow optimize for the parameters of the

136
00:10:21,580 --> 00:10:22,850
mean and variance

137
00:10:22,890 --> 00:10:27,070
so we can estimate this distribution from the data and then afterwards we can do

138
00:10:27,070 --> 00:10:28,890
whatever we want to

139
00:10:28,890 --> 00:10:34,020
in statistical learning theory maybe so little bit more radical people say well we don't

140
00:10:34,020 --> 00:10:36,100
the other announcements

141
00:10:36,120 --> 00:10:40,670
is that while it is allegedly working but please don't right now because i find

142
00:10:40,690 --> 00:10:43,840
route you can get your hands upstairs if you have a

143
00:10:44,540 --> 00:10:46,650
welcome to information theory part two

144
00:10:46,660 --> 00:10:48,750
we're going to come to this

145
00:10:49,170 --> 00:10:54,090
puzzle later on imminently first let's recap what we did

146
00:10:54,100 --> 00:10:55,320
last time

147
00:10:55,370 --> 00:11:01,070
we were talking about trying to communicate reliably over unreliable channels

148
00:11:01,090 --> 00:11:05,610
for example the binary symmetric channel with the flip probability of ten percent was favourite

149
00:11:05,820 --> 00:11:07,430
toy example

150
00:11:07,430 --> 00:11:12,750
and we discussed system solutions to this problem where we want to achieve reliable communication

151
00:11:12,750 --> 00:11:16,470
without changing the channel by adding a system in front and after to the encoder

152
00:11:16,470 --> 00:11:20,670
adds redundancy the decoder does inference

153
00:11:20,680 --> 00:11:26,180
and we discussed various ways of encoding we talk about repetition code to talk about

154
00:11:26,200 --> 00:11:29,680
the seven four hamming code and

155
00:11:29,740 --> 00:11:33,710
i showed you the performance of those codes and some others and we discussed what's

156
00:11:33,710 --> 00:11:36,800
achievable and this astonishing result of shannon's

157
00:11:36,850 --> 00:11:41,530
the noisy channel coding theorem that says you can get the error probability as small

158
00:11:41,530 --> 00:11:46,130
as you like so the good news was always we can get the error probably

159
00:11:46,130 --> 00:11:49,130
down but we lose in terms of right but the fact is no you don't

160
00:11:49,130 --> 00:11:52,470
have to lose in terms of rate you need to go down below the capacity

161
00:11:52,470 --> 00:11:55,880
which is the property of the particular channel you're working with

162
00:11:55,930 --> 00:12:00,990
and below that capacity there exist encoders and decoders which can get the error probability

163
00:12:00,990 --> 00:12:03,330
as small as you like which is stunning

164
00:12:03,360 --> 00:12:08,690
i define the capacity of the binary symmetric channel with low probability

165
00:12:08,690 --> 00:12:13,440
here using the binary entropy function and his a sketch of what the binary entropy

166
00:12:13,440 --> 00:12:16,800
function looks like and might think well where this

167
00:12:17,130 --> 00:12:21,430
all one over possible about whether that come from all that information theory and it

168
00:12:21,430 --> 00:12:26,800
would take about a lectures to completely explain everything about information theory and and fully

169
00:12:26,800 --> 00:12:30,530
prove the noisy channel coding theorem for any channel but today all hope to fill

170
00:12:30,530 --> 00:12:34,440
in a few of the highlights of that topic

171
00:12:38,190 --> 00:12:40,020
this theorem can be proved for

172
00:12:40,030 --> 00:12:44,890
any channel in a broad class of channels so every channel has got the capacity

173
00:12:44,890 --> 00:12:49,060
and we're going now is i'm going to tell you about source coding which is

174
00:12:49,060 --> 00:12:52,670
the other bits of information to reveal the main bit

175
00:12:52,710 --> 00:12:56,930
so we had encode and channel

176
00:12:59,080 --> 00:13:01,830
like this and

177
00:13:01,930 --> 00:13:06,090
the encoder was adding redundancy but in real life the things you want to send

178
00:13:06,090 --> 00:13:11,050
over your unreliable channel probably already have some redundancy in and it's very wasteful to

179
00:13:11,050 --> 00:13:18,300
not get rid of redundancy and so we have compression all source code have compressed

180
00:13:18,340 --> 00:13:27,170
and uncompressed and the first half of the lectures about the response to sparse

181
00:13:28,990 --> 00:13:33,800
so that's the big picture is conventional to divide the communication problem into these two

182
00:13:33,800 --> 00:13:38,330
steps that you first compress your redundant data then you add some very carefully to

183
00:13:38,670 --> 00:13:40,270
defined redundancy

184
00:13:40,270 --> 00:13:46,340
to help you with from the noisy channel whatever noisy channel you're dealing so this

185
00:13:46,340 --> 00:13:50,290
is called the noisy channel coding and this is called

186
00:13:51,360 --> 00:13:53,590
coding is also called data compression

187
00:13:55,890 --> 00:13:57,730
and it's also called machine learning

188
00:13:57,740 --> 00:14:02,770
as well it's like

189
00:14:02,820 --> 00:14:08,120
OK so i'll take you through some

190
00:14:08,140 --> 00:14:13,150
key ideas about source coding of describe how to make optimal compresses of a particular

191
00:14:13,150 --> 00:14:15,090
time called symbol codes

192
00:14:15,110 --> 00:14:18,090
and i'll tell you about another idea called arithmetic coding

193
00:14:18,110 --> 00:14:20,360
then we come back to noisy channel coding

194
00:14:20,460 --> 00:14:22,770
and see how much time we got left to sketch

195
00:14:22,950 --> 00:14:26,340
a few more of the final ideas that have come up in noisy channel coding

196
00:14:26,340 --> 00:14:31,360
and how it connects to what you've been hearing about factor graphs and message passing

197
00:14:31,360 --> 00:14:36,870
algorithms which you've already heard about in the last couple of days the lecture notes

198
00:14:36,870 --> 00:14:40,870
for all of this chapter four five six and forty seven of the book which

199
00:14:40,870 --> 00:14:41,930
is available

200
00:14:41,950 --> 00:14:49,290
in the bookshop bookshop is and it will be here until lunchtime

201
00:14:49,300 --> 00:14:52,610
and it's free online

202
00:14:52,620 --> 00:14:57,680
so offer you why are we doing this compression thing well here's an example of

203
00:14:57,680 --> 00:14:59,780
these are the kinds of results you get

204
00:14:59,870 --> 00:15:01,620
so of is are reasonable

205
00:15:01,700 --> 00:15:03,270
and some of these are not

206
00:15:04,100 --> 00:15:08,850
the the the problem here is that the data is extremely noisy

207
00:15:08,900 --> 00:15:11,530
so there's is a show

208
00:15:11,540 --> 00:15:14,470
what's called polysemy different meanings of that word

209
00:15:14,600 --> 00:15:19,160
forty percent of what you have is you could upload a collection of hundred photos

210
00:15:19,340 --> 00:15:20,660
and enable them all

211
00:15:20,660 --> 00:15:22,720
santa barbara channel

212
00:15:22,730 --> 00:15:26,800
it is not of much use when you're trying to get something out

213
00:15:27,470 --> 00:15:30,270
the argument is tags are not enough

214
00:15:30,280 --> 00:15:31,800
that's what we have to do

215
00:15:31,850 --> 00:15:35,600
i mean let's acknowledge the fact that this is currently how

216
00:15:35,700 --> 00:15:37,660
the work is done

217
00:15:37,720 --> 00:15:41,150
that's probably what it's going to be in the immediate future

218
00:15:41,200 --> 00:15:43,950
but there are some limitations

219
00:15:43,970 --> 00:15:47,720
we hope to go beyond

220
00:15:47,800 --> 00:15:49,990
OK so let's look at the field of

221
00:15:50,010 --> 00:15:54,670
so there was a field of content based image retrieval somehow invented in the

222
00:15:54,700 --> 00:15:56,290
in the nineteen nineties

223
00:15:56,320 --> 00:16:00,710
just ignore these things it's not quite important details and what

224
00:16:00,720 --> 00:16:02,120
and the

225
00:16:02,140 --> 00:16:06,200
find project was actually an idea the so called cubic project

226
00:16:06,210 --> 00:16:10,360
those words that we did at berkeley and this is from santa barbara

227
00:16:10,460 --> 00:16:14,490
mean is the project and mostly these projects what about

228
00:16:14,510 --> 00:16:17,020
using colour and texture

229
00:16:17,030 --> 00:16:17,910
and why

230
00:16:17,990 --> 00:16:19,620
use colour and texture

231
00:16:19,670 --> 00:16:22,550
well it's this all story of

232
00:16:23,400 --> 00:16:30,460
somebody went somebody looking and looking for somewhere first as down the ground and this

233
00:16:30,460 --> 00:16:32,270
person said what i'm looking for

234
00:16:32,280 --> 00:16:33,980
i'm looking for my keys

235
00:16:33,990 --> 00:16:38,570
and then the person said that it was the mrna with their by looking them

236
00:16:38,680 --> 00:16:40,620
this is where the light is

237
00:16:40,640 --> 00:16:44,360
OK so as i said to begin to look under the light

238
00:16:44,370 --> 00:16:45,780
and the light was

239
00:16:45,790 --> 00:16:49,390
the picture histograms because he knew how to model

240
00:16:49,400 --> 00:16:51,650
and here are the basic ideas

241
00:16:51,660 --> 00:16:55,740
so the color histograms are you can imagine divided up into blocks

242
00:16:55,830 --> 00:16:59,160
then you find the distribution of media scholars

243
00:16:59,170 --> 00:17:05,650
what percentage of blue green yellow red except get distribution that characterizes an image

244
00:17:05,670 --> 00:17:08,210
this is pretty good for finding sunsets

245
00:17:08,220 --> 00:17:12,240
if you try to find images with a lot not already get sunsets

246
00:17:12,370 --> 00:17:17,960
so the canonical example from any CBS from the nineteen nineties was showing the detection

247
00:17:17,960 --> 00:17:19,070
of sunsets

248
00:17:19,110 --> 00:17:22,540
the second example was a little bit tricky

249
00:17:22,590 --> 00:17:26,070
OK you can do something based on texture

250
00:17:26,080 --> 00:17:30,120
and take sheffield that's what we do is there's a bit more sophisticated

251
00:17:30,180 --> 00:17:35,590
i mean and some orientation sensitivity is constant it is about what

252
00:17:35,690 --> 00:17:41,210
and then the output of those filters we can again construct histograms and sort of

253
00:17:41,210 --> 00:17:43,750
county through the same idea

254
00:17:44,870 --> 00:17:46,070
but this

255
00:17:48,220 --> 00:17:50,920
works in very very limited settings

256
00:17:50,930 --> 00:17:54,040
and it took us a little while to figure out what we did we have

257
00:17:54,040 --> 00:17:56,630
figured out to give us credit for that

258
00:17:57,120 --> 00:18:01,340
and there is no slogan which multimedia community really

259
00:18:01,340 --> 00:18:04,840
fond of using it's called the semantic gap

260
00:18:04,860 --> 00:18:10,010
so the semantic gap is the first generation CBIR systems were based on colour and

261
00:18:10,930 --> 00:18:13,890
and these do not capture what users

262
00:18:13,950 --> 00:18:17,370
i'm interested in so they are interested in pictures

263
00:18:17,500 --> 00:18:24,590
monkees our kids are paris hilton what were not interested in picture which seventy percent

264
00:18:24,840 --> 00:18:26,270
twenty two percent three

265
00:18:26,300 --> 00:18:28,750
that's not what really is about

266
00:18:30,690 --> 00:18:34,820
we we have a lot of evidence from human perception of art

267
00:18:34,840 --> 00:18:39,380
what are the queues for categorisation and that most important cues shape

268
00:18:39,390 --> 00:18:45,190
but this is ignored in all this work because we addiction

269
00:18:46,390 --> 00:18:48,220
so what happened is that

270
00:18:48,250 --> 00:18:52,540
the kind of these two facts if you look at the ACM multimedia conference is

271
00:18:52,550 --> 00:18:58,140
by large their conclusion from this is OK this is hopeless the simple approach don't

272
00:18:58,840 --> 00:19:03,010
let's go off and try to develop good annotation tools of people we can bring

273
00:19:03,010 --> 00:19:05,150
the user in the loop and produced tonnes

274
00:19:05,180 --> 00:19:06,600
that's kind of what you

275
00:19:06,630 --> 00:19:09,400
caricaturing of it but that's what the

276
00:19:09,420 --> 00:19:12,060
multimedia community is sort of

277
00:19:12,970 --> 00:19:13,810
push forward

278
00:19:13,820 --> 00:19:18,120
are in the computer vision community this kind has been

279
00:19:18,360 --> 00:19:20,330
we are trying to fight the good fight

280
00:19:20,340 --> 00:19:21,840
will try to solve the

281
00:19:21,840 --> 00:19:25,290
the fundamental problem of recognition based on shape

282
00:19:25,310 --> 00:19:28,750
and now the question is what have we delivered in the last five to ten

283
00:19:29,540 --> 00:19:32,800
and i will argue that we have made substantial progress

284
00:19:32,840 --> 00:19:35,970
and it's not anywhere near human performance

285
00:19:36,010 --> 00:19:40,120
but it's good enough that it useful for some and the derivatives

286
00:19:40,140 --> 00:19:43,120
so this is the promissory notes

287
00:19:45,470 --> 00:19:49,640
so let's let's get some so here's the research

288
00:19:49,650 --> 00:19:52,980
so essentially we want to take images and analyse them

289
00:19:53,000 --> 00:19:59,550
and automatically generate annotations corresponding to object labels that activities in video

290
00:20:00,670 --> 00:20:04,220
if if you can automatically generate those annotations

291
00:20:04,220 --> 00:20:09,170
then of course these can be combined with other metadata and then these become the

292
00:20:09,180 --> 00:20:11,650
the signal to be used for searching

293
00:20:11,680 --> 00:20:18,190
because we believe that at at runtime analyzing analyzing images is very expensive so you

294
00:20:18,190 --> 00:20:23,250
don't want to be analyzing a billion images but if you you have more

295
00:20:23,310 --> 00:20:25,620
the content of the image into some

296
00:20:25,680 --> 00:20:30,980
something that you precomputed then at runtime things can go pretty fast

297
00:20:32,860 --> 00:20:34,960
that's surprising

298
00:20:34,970 --> 00:20:37,950
OK so let's start with some context what

299
00:20:37,960 --> 00:20:39,400
we know from

300
00:20:39,430 --> 00:20:40,400
human vision

301
00:20:40,400 --> 00:20:43,250
another constant cutting apache

302
00:20:43,330 --> 00:20:47,290
so then we look at the con straints all kinds of people give you very

303
00:20:47,290 --> 00:20:52,360
strange all kinds of constraints we found there are four kinds of constraints

304
00:20:52,400 --> 00:20:54,970
you actually can push quite deep

305
00:20:55,000 --> 00:20:57,350
to use it in very effectively

306
00:20:57,360 --> 00:20:59,020
while we call

307
00:20:59,080 --> 00:21:04,510
pattern search space means having constraints we can cut another data search space cut the

308
00:21:05,500 --> 00:21:13,270
the pattern while we we categorize into this peening spam pattern antimonotonicity

309
00:21:13,290 --> 00:21:17,540
OK so we have strong pattern and another fuzzy and week one

310
00:21:17,580 --> 00:21:24,940
we also for the search space because they had been monotonicity we have pattern inseparable

311
00:21:24,940 --> 00:21:26,630
and pattern inseparable

312
00:21:26,650 --> 00:21:30,130
what we can push so i give example you're c

313
00:21:30,170 --> 00:21:33,960
actually quite easy once you see the example OK

314
00:21:33,970 --> 00:21:36,590
for example i give you a constant say

315
00:21:36,590 --> 00:21:39,240
i one the pattern to be a sick

316
00:21:39,310 --> 00:21:44,510
i mean i wouldn't mind this chemical compound of finals basically patterns

317
00:21:44,560 --> 00:21:47,700
those form like rings or something and one

318
00:21:47,770 --> 00:21:49,390
if you say this

319
00:21:50,120 --> 00:21:54,550
very interesting thing is this one grow the pattern suppose you grow up and finally

320
00:21:54,550 --> 00:21:59,760
find this one's been benzene rings OK so this by arranging the pattern from this

321
00:21:59,760 --> 00:22:01,790
pairing you want to grow

322
00:22:01,800 --> 00:22:04,330
forget it because you got a ring

323
00:22:04,380 --> 00:22:09,130
you grow anything longer bigger and can still contain the strain you're never

324
00:22:09,180 --> 00:22:11,660
satisfied this basically

325
00:22:12,510 --> 00:22:16,680
there have so the whole pattern away without growing from there

326
00:22:16,700 --> 00:22:20,130
OK so that's the one we call strong

327
00:22:22,170 --> 00:22:24,020
what is a weak one

328
00:22:24,070 --> 00:22:27,060
i give you this constant k the consensus

329
00:22:27,140 --> 00:22:32,790
the density ratio of pattern suppose it should be greater than zero point one i

330
00:22:32,840 --> 00:22:37,120
list i give you the definition what is what do i mean the density ratio

331
00:22:37,120 --> 00:22:41,950
the ratio is the within for this pattern the number of edges

332
00:22:42,000 --> 00:22:43,360
divided by

333
00:22:43,370 --> 00:22:49,790
come possible combination of the that these are the vertices possibly come to construct all

334
00:22:49,800 --> 00:22:51,150
the possible edges

335
00:22:51,740 --> 00:22:53,150
so this is it

336
00:22:53,170 --> 00:22:58,470
available at this is the path branch OK so if the possible edge

337
00:22:58,500 --> 00:23:03,570
and available edges almost the same so it's almost one there is very that's

338
00:23:03,570 --> 00:23:05,510
otherwise is sparse like we

339
00:23:05,540 --> 00:23:08,450
assume we want density ratios that's OK

340
00:23:08,550 --> 00:23:11,620
this one interesting thing is

341
00:23:11,650 --> 00:23:12,710
it's not

342
00:23:12,730 --> 00:23:13,980
everywhere you grow

343
00:23:14,000 --> 00:23:15,820
back from here from here

344
00:23:15,820 --> 00:23:18,210
they were touch the wall again

345
00:23:18,230 --> 00:23:21,650
but the interesting thing you can see for this week

346
00:23:21,650 --> 00:23:26,170
antimonotonicity constraints is if you want a bigger

347
00:23:26,600 --> 00:23:29,110
graph satisfy this

348
00:23:30,710 --> 00:23:35,650
there's an interesting property is you can find a least award

349
00:23:35,650 --> 00:23:38,220
smaller one smaller means

350
00:23:38,240 --> 00:23:44,250
i have one vertices that's smaller want a least one is also that's if this

351
00:23:44,340 --> 00:23:47,560
you can not find any subset which stands

352
00:23:47,600 --> 00:23:51,450
there's no way of grow growing still can be that's OK that one that one

353
00:23:51,520 --> 00:23:52,990
we can prove it if you

354
00:23:53,020 --> 00:23:57,640
you carefully we can prove this one with this well with this property we can

355
00:23:57,640 --> 00:24:04,650
see this constraint actually is weak partial funds means you're not saying from here is

356
00:24:04,650 --> 00:24:10,110
useless but i find all the possible integral OK if they none of them

357
00:24:10,160 --> 00:24:11,440
can satisfy this

358
00:24:11,460 --> 00:24:12,640
this ratio

359
00:24:12,650 --> 00:24:17,490
and this one were dropped because we know further growth we never said this rich

360
00:24:17,490 --> 00:24:21,160
OK so this kind of constraints we call week one

361
00:24:21,170 --> 00:24:26,840
then our view the data and monotonicity constraints to see why this one is very

362
00:24:26,840 --> 00:24:32,560
useful in case for example you may have the constants like this the number of

363
00:24:32,820 --> 00:24:34,540
discrete or equal ten

364
00:24:34,550 --> 00:24:37,740
or the pattern contains vendor k

365
00:24:37,760 --> 00:24:42,910
suppose we look at this conference number of discrete and that

366
00:24:42,910 --> 00:24:46,180
at the very beginning we grow this monograph

367
00:24:46,200 --> 00:24:50,440
OK of course the number number of edges is not over ten

368
00:24:51,330 --> 00:24:52,650
you're not drop it

369
00:24:52,680 --> 00:24:56,860
OK why don't drop it because you see i grew further eager would be

370
00:24:56,910 --> 00:24:57,610
you know

371
00:24:57,620 --> 00:24:59,270
would be more than ten

372
00:24:59,360 --> 00:25:00,800
i still like to grow

373
00:25:00,810 --> 00:25:04,970
OK that's fine the perfect but the interesting thing is

374
00:25:04,990 --> 00:25:08,820
you go back to look looking dataset you suppose i grow

375
00:25:08,880 --> 00:25:11,120
i look in my graph in the middle

376
00:25:11,130 --> 00:25:16,420
i found this frequent pattern grow to understand the remaining part of graph actually were

377
00:25:16,420 --> 00:25:20,150
not even have more than ten years if i add them together so was this

378
00:25:20,150 --> 00:25:23,150
one size is five the remaining one

379
00:25:23,160 --> 00:25:27,650
without growing part size should be at least five if this one small

380
00:25:27,650 --> 00:25:29,950
this dataset is useless you can cut

381
00:25:29,970 --> 00:25:32,250
right so that's the idea

382
00:25:33,960 --> 00:25:39,440
cut any graph line integral find the remaining part has no way to satisfy it

383
00:25:39,480 --> 00:25:40,360
you cut

384
00:25:40,910 --> 00:25:44,960
then you may you may ask one question is why do cut it from the

385
00:25:44,960 --> 00:25:48,180
very beginning you cut it why have the weight entry here in the middle

386
00:25:48,180 --> 00:25:49,710
i can then

387
00:25:50,810 --> 00:25:53,580
the format of the

388
00:25:53,600 --> 00:25:56,790
regional groups so here in this case

389
00:25:56,830 --> 00:25:58,620
this data is just plain

390
00:25:58,640 --> 00:25:59,590
we some

391
00:26:00,580 --> 00:26:02,490
variance applied to it

392
00:26:02,490 --> 00:26:04,390
and this is the recovered

393
00:26:04,460 --> 00:26:09,480
great so it's actually denoised this manifold very well it's really easy example PCA we

394
00:26:09,510 --> 00:26:10,760
do it as well

395
00:26:10,770 --> 00:26:13,260
but there you go so he's got connectivity

396
00:26:13,300 --> 00:26:16,960
what we do is building activity matrix between all these points then we compute interpoint

397
00:26:16,960 --> 00:26:21,450
distances then we embed those distances and that's the embedding using the colours here match

398
00:26:21,460 --> 00:26:25,790
the colors it so it's been rotated ninety degrees

399
00:26:26,850 --> 00:26:28,570
the swiss roll

400
00:26:28,650 --> 00:26:30,210
so that's the swiss roll

401
00:26:30,290 --> 00:26:33,150
and then here's the uncovered embedding dennis

402
00:26:33,170 --> 00:26:37,270
it's got some slight flaws but it's basically a plane

403
00:26:37,280 --> 00:26:39,730
this is the trefoil three four o

404
00:26:40,410 --> 00:26:41,340
no not

405
00:26:41,360 --> 00:26:43,490
here's the data

406
00:26:43,530 --> 00:26:45,350
and here is is

407
00:26:45,370 --> 00:26:46,660
recovered in that

408
00:26:46,710 --> 00:26:52,760
so it's obviously can't split it's called to parliament topological constraint has been circles

409
00:26:52,770 --> 00:26:55,090
and there's there's another issue with this method

410
00:26:55,230 --> 00:27:00,030
is embedding in euclidean space what happens if the original character is you're trying to

411
00:27:00,030 --> 00:27:04,490
embed say someone walking which is somewhat periodic it's it's not really euclidean space some

412
00:27:04,490 --> 00:27:08,320
sort of a cylindrical space what they tend to do is not make use of

413
00:27:08,320 --> 00:27:12,350
them in the one dimensional structure basically living in two d space is really circle

414
00:27:12,760 --> 00:27:19,230
of political issue that has to do that

415
00:27:23,510 --> 00:27:28,220
so i'm not going to make sweeping statements about all of the spectral methods

416
00:27:28,260 --> 00:27:32,970
so from the covariance interpretation we think of the similarity matrix is a covariance

417
00:27:32,990 --> 00:27:37,600
each element of the covariance is a function of two data points

418
00:27:39,800 --> 00:27:44,490
it isn't isomap but in general it is we just going to specify the covariance

419
00:27:44,490 --> 00:27:46,970
matrix by similarity measure

420
00:27:47,080 --> 00:27:51,320
now another option is to specify the inverse covariance

421
00:27:51,350 --> 00:27:53,850
if we specify the inverse covariance

422
00:27:53,850 --> 00:27:58,950
if the inverse covariance between two points is zero those points are independent given all

423
00:27:58,950 --> 00:28:01,580
other points in not independent

424
00:28:02,430 --> 00:28:03,780
in the true sense

425
00:28:03,800 --> 00:28:06,990
the only independent if we observed all other points

426
00:28:07,050 --> 00:28:11,910
if you look at the covariance how they cope very they won't be independent

427
00:28:11,970 --> 00:28:14,350
now i'm not going to talk in detail about this but i just want him

428
00:28:14,350 --> 00:28:15,510
to this

429
00:28:15,550 --> 00:28:18,120
laplacian i can maps and LLE

430
00:28:18,180 --> 00:28:19,470
can be seen

431
00:28:19,530 --> 00:28:22,350
as specifying the inverse covariance

432
00:28:22,350 --> 00:28:25,200
in particular ways i'm not going to talk about

433
00:28:25,260 --> 00:28:30,510
by giving particular weights on the inverse covariance so the giving connectivity structure

434
00:28:30,510 --> 00:28:34,140
now that means when you look at the covariance the covariance each two points of

435
00:28:34,140 --> 00:28:39,620
the covariance depends on the entire dataset because it's the inverse covariance that specified only

436
00:28:39,620 --> 00:28:41,870
between two points in a pairwise manner

437
00:28:41,930 --> 00:28:45,950
so that's the way i wanted to i don't have time to prepare that's really

438
00:28:45,950 --> 00:28:50,620
the way i wanted to cover these guys the from the perspective of you specify

439
00:28:50,640 --> 00:28:52,410
the graph structure

440
00:28:52,430 --> 00:28:54,320
and they often it's called the plastic

441
00:28:54,760 --> 00:28:58,820
which is why it's called last item at is best inverse covariance now

442
00:28:59,450 --> 00:29:02,950
comment for both of these techniques see you create

443
00:29:02,970 --> 00:29:04,370
a structure

444
00:29:04,390 --> 00:29:06,990
and then you extract the smallest igon value

445
00:29:07,010 --> 00:29:12,160
so the largest actually discard the smallest i value and then you extract the second

446
00:29:13,140 --> 00:29:15,280
this complete within this framework

447
00:29:15,350 --> 00:29:20,260
it makes complete sense you're doing that because the largest eigen value of the covariance

448
00:29:20,260 --> 00:29:24,910
matrix is the smallest i value of the inverse covariance matrix

449
00:29:24,950 --> 00:29:29,450
with one little flying ointment they discard the smallest i can value the covariance matrix

450
00:29:29,450 --> 00:29:31,680
because it's all once well

451
00:29:31,760 --> 00:29:35,100
there's also an interpretation for that which comes back to centre in which i didn't

452
00:29:35,100 --> 00:29:36,490
talk about

453
00:29:36,510 --> 00:29:37,620
in detail

454
00:29:37,660 --> 00:29:42,390
if you send a command if you don't send a covariance matrix

455
00:29:42,430 --> 00:29:46,200
the first igon value will be all once

456
00:29:46,220 --> 00:29:51,550
so for example if you you've got data and you seem a zero mean

457
00:29:51,550 --> 00:29:55,800
and then you compute the covariance you confuse me computers covariance the first thing you

458
00:29:55,800 --> 00:29:58,240
get is like vector of ones

459
00:29:58,260 --> 00:30:01,740
o point what you're gonna get better ones which is associated with the fact you

460
00:30:01,740 --> 00:30:03,490
have removed the me

461
00:30:03,550 --> 00:30:08,240
so that the removal of the smallest igon vector has an interpretation as entering the

462
00:30:10,200 --> 00:30:13,910
when you're moving the smallest like vector from the inverse covariance so both these things

463
00:30:13,910 --> 00:30:19,240
make complete sense the of specifying an inverse covariance and extracting the largest eigen value

464
00:30:19,240 --> 00:30:23,240
of the covariance or the smallest value of the inverse square

465
00:30:23,280 --> 00:30:27,930
and they were quite nicely just to show you the early results for those things

466
00:30:27,990 --> 00:30:31,890
that really isn't quite nice as isomap i didn't spend a lot of time playing

467
00:30:31,890 --> 00:30:35,850
with parameters it's not really matters that i looked at the third dimension i would

468
00:30:35,850 --> 00:30:38,080
find it probably went along the line there

469
00:30:38,080 --> 00:30:42,160
it also shows you know they don't always work i just used seven neighbours you

470
00:30:42,160 --> 00:30:45,450
can go into my code and play with settings if you want to see how

471
00:30:45,580 --> 00:30:47,510
quickly can recover the thing

472
00:30:47,580 --> 00:30:53,490
it does something we're not here i'm not sure maybe even a bargain like in

473
00:30:55,600 --> 00:31:01,970
right so the final sort of thing

474
00:31:02,010 --> 00:31:03,320
what i

475
00:31:03,330 --> 00:31:06,890
i want to briefly talk about is more closely related to my research and i

476
00:31:06,890 --> 00:31:09,640
actually credit card for these slides because

477
00:31:09,700 --> 00:31:14,990
i work together with karl and his sort of illustrating some of the points here

478
00:31:15,050 --> 00:31:20,010
so everything we talked about so far is spectral methods distance matching let's trying to

479
00:31:20,010 --> 00:31:24,560
reconstruct distances in the latent space to match distances in the data space

480
00:31:24,620 --> 00:31:27,950
well the final thing i want to see is what happens if you do things

481
00:31:27,970 --> 00:31:30,640
in a slightly different way you try and say well

482
00:31:30,700 --> 00:31:34,430
all this going on is that i have functional mapping from a latent space to

483
00:31:34,430 --> 00:31:36,970
annotate space from x to y

484
00:31:36,970 --> 00:31:38,780
the model predicts

485
00:31:38,820 --> 00:31:41,130
so this is sort of what your data tells you and this was what your

486
00:31:41,130 --> 00:31:42,380
model tells you

487
00:31:42,420 --> 00:31:46,720
and in this special case it's very simple to tell you to do moment matching

488
00:31:46,720 --> 00:31:47,110
it says

489
00:31:47,490 --> 00:31:49,880
choose your model parameters

490
00:31:49,930 --> 00:31:55,360
change the status of the these guys end up matching what your data tells you

491
00:31:55,400 --> 00:31:59,670
so it's very intuitive in this this special case that's not always true for maximum

492
00:31:59,670 --> 00:32:02,950
likelihood but but here it is

493
00:32:02,950 --> 00:32:09,720
so there are many ways that you could imagine trying to solve these equations

494
00:32:09,760 --> 00:32:13,130
the naive way this is what i would suggest in practice but just just to

495
00:32:13,130 --> 00:32:18,570
give intuition the most naive way would just be gradient descent you would start with

496
00:32:18,570 --> 00:32:21,950
your current parameter and the way you update is by

497
00:32:21,990 --> 00:32:26,800
adding to that parameter you add the difference in the gradients in this sort of

498
00:32:26,800 --> 00:32:30,950
thing you think about this is the training error it's it's telling you

499
00:32:30,970 --> 00:32:33,130
this is what your data says

500
00:32:33,200 --> 00:32:37,110
this is what your model says and this is the difference it's the sort of

501
00:32:37,110 --> 00:32:41,420
air that your model this is currently making sort of pushing that to make the

502
00:32:42,200 --> 00:32:45,720
predict the data is special case

503
00:32:48,860 --> 00:32:51,720
this is the way that i would suggest solving it but what i would like

504
00:32:51,720 --> 00:32:53,900
to emphasise this is important

505
00:32:53,920 --> 00:32:59,490
remember i said that you can't escape this this inference problem this inference problem comes

506
00:32:59,490 --> 00:33:05,450
up again and what you want to think about this expectation is an expectation of

507
00:33:05,740 --> 00:33:07,740
an indicator function

508
00:33:07,780 --> 00:33:11,070
so if you sort of break this down analytically

509
00:33:11,070 --> 00:33:14,990
and what you find is it's like marginalization operation

510
00:33:15,030 --> 00:33:19,240
to do this you have to run something like the elimination or the junction tree

511
00:33:19,240 --> 00:33:21,400
algorithm to do exactly

512
00:33:21,400 --> 00:33:24,720
so there's a bit of what you might call the chicken and egg problem or

513
00:33:25,050 --> 00:33:30,700
of cart and horse problems to be able to estimate the parameters you have to

514
00:33:30,700 --> 00:33:32,760
be able to compute marginals

515
00:33:35,130 --> 00:33:38,030
computing marginals is tricky so it's

516
00:33:38,030 --> 00:33:41,550
the of circular larity here the

517
00:33:41,550 --> 00:33:45,030
both problems get wrapped up in one another because of this

518
00:33:45,090 --> 00:33:47,470
nice into the form of the updates

519
00:33:48,650 --> 00:33:54,550
let me now talk in detail about this because sam spoke about this the IPS

520
00:33:54,550 --> 00:33:59,630
cells this is slightly different algorithm it's not gradient ascent per say

521
00:33:59,670 --> 00:34:04,670
it's actually what's known as the coordinate ascent algorithm you sort of fixing all the

522
00:34:04,670 --> 00:34:06,860
other coordinates in ascending on one

523
00:34:06,920 --> 00:34:10,200
i won't go into details of this sort of key thing is it again you

524
00:34:10,200 --> 00:34:14,720
get quantities that involve the data what the data tells you and what your model

525
00:34:14,720 --> 00:34:18,530
tells you again you need need to do inference to be able to compute those

526
00:34:18,530 --> 00:34:21,030
those quantities

527
00:34:21,090 --> 00:34:27,360
again let me skip this because sam went into this is the issue of data

528
00:34:27,360 --> 00:34:33,150
augmentation and the algorithm which is sort of a very nice

529
00:34:33,200 --> 00:34:40,400
general purpose technique for solving problems which can be formulated in terms of exhilarating functions

530
00:34:40,420 --> 00:34:43,340
OK so

531
00:34:43,360 --> 00:34:48,320
in the last sort of twenty minutes i just want start on part two

532
00:34:48,360 --> 00:34:53,470
just again i think probably people are getting tired so try and stay high level

533
00:34:53,470 --> 00:34:58,130
this for the next fifteen or twenty minutes actually before i jump just any questions

534
00:34:58,130 --> 00:35:01,920
about issues of parameter estimation and model selection

535
00:35:03,400 --> 00:35:08,240
things like this

536
00:35:14,320 --> 00:35:15,010
so what

537
00:35:22,800 --> 00:35:24,720
for instance if

538
00:35:24,760 --> 00:35:28,700
going back to the simple example where i had an image model on a lattice

539
00:35:28,700 --> 00:35:30,840
had pairwise connections

540
00:35:30,840 --> 00:35:34,420
we wanted to estimate these four numbers here

541
00:35:34,420 --> 00:35:37,780
so for instance if if this was a very big relative to this it would

542
00:35:37,780 --> 00:35:40,430
be telling me that the image is quite smooth

543
00:35:40,450 --> 00:35:44,300
so i'd be sort of trying to figure out what the values of these numbers

544
00:35:44,630 --> 00:35:47,650
in that special case those would be the parameters

545
00:35:47,670 --> 00:35:52,900
more generally might functions over bigger cliques you'd have more parameters but it's the same

546
00:35:52,900 --> 00:36:00,220
kind of principle

547
00:36:00,260 --> 00:36:08,170
maybe one other comment the model selection problem is is is pretty tricky in general

548
00:36:08,170 --> 00:36:13,470
there are sort of sure-fire methods to do it there's some interesting theory

549
00:36:13,490 --> 00:36:20,300
in particular things like AIC BIC can be used to some extent to do selection

550
00:36:20,340 --> 00:36:21,950
but i

551
00:36:21,970 --> 00:36:25,320
a lot of these methods are computationally intractable

552
00:36:25,320 --> 00:36:29,510
so the current state of the art is that there are many methods that no

553
00:36:29,510 --> 00:36:31,590
theoretically are quite well understood

554
00:36:31,590 --> 00:36:35,350
one when i when i plug all cases in their own right

555
00:36:35,390 --> 00:36:38,640
so basically if i have a data point i what this is saying is when

556
00:36:38,690 --> 00:36:43,090
now compute the posterior probability for this data point was that what was the probability

557
00:36:43,090 --> 00:36:45,560
that it belongs to all the ten thousand that i have

558
00:36:45,600 --> 00:36:48,570
well it's just some numbers for each but they better add up to one right

559
00:36:48,570 --> 00:36:52,240
because the guy has to belong to someone after all right

560
00:36:52,260 --> 00:36:56,410
OK but that's easy because that's what i do is i simply make some here

561
00:36:56,510 --> 00:36:58,930
over j of all possible

562
00:36:58,990 --> 00:37:01,700
so this would be i j and then

563
00:37:01,740 --> 00:37:03,740
i j OK

564
00:37:03,760 --> 00:37:07,050
it's not you see the magic happens and actually

565
00:37:07,070 --> 00:37:08,300
i could i could

566
00:37:08,320 --> 00:37:09,110
i could

567
00:37:09,130 --> 00:37:11,820
change of the name and i could

568
00:37:11,840 --> 00:37:15,470
dig out this good all responsibilities and we had earlier right

569
00:37:15,490 --> 00:37:20,570
so what is this responsibility said to us earlier they said to us

570
00:37:20,570 --> 00:37:24,880
zero or one and the there were one if i is the i th data

571
00:37:24,880 --> 00:37:29,550
point belongs to cluster k right and here these guys are soft their numbers between

572
00:37:29,550 --> 00:37:31,800
zero and one OK

573
00:37:31,820 --> 00:37:36,860
and the posterior probabilities that a given y is going to belong to a given

574
00:37:40,740 --> 00:37:44,010
and because are defined like that quite trivially you can see that if i add

575
00:37:44,010 --> 00:37:44,760
them up

576
00:37:44,820 --> 00:37:48,030
over k then i should get one

577
00:37:51,740 --> 00:38:00,200
so now we have one missing thing to do is we need to compute derivatives

578
00:38:00,200 --> 00:38:05,970
with respect to the prior case right that's the that's missing thing we have

579
00:38:06,010 --> 00:38:09,220
so now what we need to do is we take the objective function we had

580
00:38:10,200 --> 00:38:10,930
and now

581
00:38:10,950 --> 00:38:13,700
we add a lagrange multiplier that enforces

582
00:38:13,720 --> 00:38:17,880
the fact that the prior probabilities need to add up to one as well

583
00:38:19,530 --> 00:38:22,280
so what implication does have

584
00:38:23,880 --> 00:38:25,550
i need to take derivatives now

585
00:38:25,550 --> 00:38:29,160
and make them to zero with respect to the parameters themselves

586
00:38:29,160 --> 00:38:32,680
but also with respect to the grande multipliers OK

587
00:38:33,450 --> 00:38:36,490
taking derivatives of the original

588
00:38:36,510 --> 00:38:38,880
cost function with respect to pi k

589
00:38:38,950 --> 00:38:42,530
is not very difficult and it's done in very similar fashion to what we did

590
00:38:42,530 --> 00:38:44,200
before right

591
00:38:44,300 --> 00:38:54,410
so now let me just go straight to the results so what is it that

592
00:38:54,410 --> 00:38:57,320
the pie case should be

593
00:38:57,340 --> 00:38:59,720
well interestingly

594
00:38:59,760 --> 00:39:02,380
it's sort of a very intuitive

595
00:39:02,410 --> 00:39:05,780
it's sort of the very intuitive equation OK

596
00:39:05,840 --> 00:39:09,030
remember earlier for k means

597
00:39:09,050 --> 00:39:12,680
what was the sum if a fixed k and i added overall

598
00:39:12,700 --> 00:39:15,570
and i added over all the points so far if i just had this thing

599
00:39:15,570 --> 00:39:16,840
here right

600
00:39:16,840 --> 00:39:21,380
that was the number of points that belong to the given class right

601
00:39:21,390 --> 00:39:22,220
so now

602
00:39:22,220 --> 00:39:25,990
i get the same thing except because it has to be a probability i divide

603
00:39:26,110 --> 00:39:27,890
need to divide by an right

604
00:39:27,930 --> 00:39:34,390
so this gives me it's sort of a very very intuitive equation right

605
00:39:34,470 --> 00:39:39,530
so what is what is the sum over all these guys as one one

606
00:39:45,150 --> 00:39:48,590
that's correct

607
00:39:48,650 --> 00:39:53,090
so when you implement mixtures of gaussians one thing you can keep is

608
00:39:53,150 --> 00:40:01,470
you can keep

609
00:40:01,510 --> 00:40:05,880
you can keep track of of of big matrix are

610
00:40:05,880 --> 00:40:07,660
that is of size k

611
00:40:08,910 --> 00:40:13,650
and right sort of contains for all data points how are they distributed among the

612
00:40:13,650 --> 00:40:17,300
classes right so if you were to plug to sort of do e major c

613
00:40:17,300 --> 00:40:19,660
with nice colors and stuff like that you would sort of c

614
00:40:20,030 --> 00:40:24,970
the different data points distributing themselves among classes

615
00:40:27,820 --> 00:40:30,530
OK so now i have to make a tough decision

616
00:40:30,530 --> 00:40:33,130
if they were a little bit

617
00:40:33,130 --> 00:40:37,700
we behind

618
00:40:37,820 --> 00:40:42,320
and we have this pattern so it is marked here is already or not

619
00:40:50,950 --> 00:40:52,880
actual OK this just

620
00:40:52,880 --> 00:40:56,240
you know give me a sign when you think it should be

621
00:40:56,470 --> 00:41:01,050
should be something

622
00:41:01,320 --> 00:41:04,840
the actual

623
00:41:04,860 --> 00:41:09,550
OK so in this slide here we only took we only took generic derivatives with

624
00:41:09,550 --> 00:41:14,860
respect to the parameters theta right but actually we want it to be the mean

625
00:41:14,860 --> 00:41:19,180
and the covariance so what we need to do is simply to come in here

626
00:41:19,430 --> 00:41:22,970
and now we know how to update the prior case we've just in it and

627
00:41:22,970 --> 00:41:26,240
now we just replace this thing but the actual derivative with respect to the mean

628
00:41:26,240 --> 00:41:28,550
so let's start

629
00:41:28,560 --> 00:41:35,960
i heard you had an exhaustive

630
00:41:35,980 --> 00:41:37,340
evening but

631
00:41:38,420 --> 00:41:40,440
i don't know what we can do

632
00:41:40,530 --> 00:41:44,660
we have quite a bit of things today

633
00:41:44,730 --> 00:41:47,820
so if i want to cover everything that i have to go a little bit

634
00:41:47,820 --> 00:41:49,940
faster than last time

635
00:41:49,960 --> 00:41:52,850
i hope that will be OK with here

636
00:41:55,130 --> 00:41:59,000
you know the last time we are talking about and the peace and then choose

637
00:41:59,020 --> 00:41:59,680
the basic

638
00:42:00,100 --> 00:42:04,030
constructions which just the basic framework

639
00:42:04,060 --> 00:42:07,520
and you're talking about

640
00:42:07,530 --> 00:42:12,420
stuff like the the existence of optimal policies what does it mean

641
00:42:13,600 --> 00:42:15,490
to be policy are

642
00:42:15,530 --> 00:42:18,200
what does it mean to be an optimal policy

643
00:42:18,250 --> 00:42:21,140
was the optimal value function

644
00:42:21,420 --> 00:42:26,880
given ultimate value function you can find an optimal policy in all you have to

645
00:42:26,880 --> 00:42:29,970
agree five suspected that the value function

646
00:42:30,020 --> 00:42:31,440
then introduced

647
00:42:31,440 --> 00:42:37,270
the action value functions and then the optimal action value function concluded that

648
00:42:37,280 --> 00:42:39,630
given ultimate action value function

649
00:42:39,640 --> 00:42:42,850
you don't have to be much computation just

650
00:42:43,440 --> 00:42:44,130
if your

651
00:42:44,140 --> 00:42:51,460
the given state you optimize respectively action to optimize the optimal action value function

652
00:42:51,470 --> 00:42:54,470
and you've got an optimal policy say

653
00:42:54,470 --> 00:42:56,850
this whole topic is

654
00:42:56,910 --> 00:43:00,240
board learning value functions like OK this

655
00:43:01,160 --> 00:43:03,130
in some approaches people

656
00:43:03,380 --> 00:43:07,770
try to avoid that and i don't know if you have time to cover that

657
00:43:07,780 --> 00:43:09,910
if the dog then

658
00:43:09,920 --> 00:43:12,520
please come to me and the breaks are

659
00:43:12,580 --> 00:43:17,520
two million excursion i'm very happy to talk about those approaches

660
00:43:17,520 --> 00:43:19,880
so in the MDP is there are three

661
00:43:19,890 --> 00:43:26,300
basic problems the planning problem the learning problem and and learning fast

662
00:43:27,570 --> 00:43:31,490
so five e manitoba structure things like

663
00:43:31,610 --> 00:43:36,270
you know this existence things how to construct the optimal policies how to think about

664
00:43:36,270 --> 00:43:37,890
other things

665
00:43:37,900 --> 00:43:42,260
and so today i'm going to talk more about

666
00:43:42,300 --> 00:43:45,980
like in the first part i am going to talk about planning and then we

667
00:43:46,870 --> 00:43:50,110
the learning an optimal learning stops

668
00:43:50,110 --> 00:43:53,490
so why is planning interesting on the first hand

669
00:43:53,510 --> 00:43:59,120
it's interesting because you could have this model of these approaches by your learning and

670
00:43:59,140 --> 00:44:02,770
and given the more than you want to find out an optimal policy right so

671
00:44:02,770 --> 00:44:06,490
then that's that's the planning problem you're given the model

672
00:44:06,560 --> 00:44:09,710
what was the optimal policy or how can i compute

673
00:44:09,760 --> 00:44:11,270
and efficient manner

674
00:44:11,310 --> 00:44:14,020
policy that's good enough

675
00:44:14,040 --> 00:44:16,480
OK so

676
00:44:16,970 --> 00:44:20,740
we left after this first present station

677
00:44:21,330 --> 00:44:25,710
at the end somewhere here after introducing action value functions

678
00:44:25,710 --> 00:44:29,020
so let me just briefly very briefly finish

679
00:44:29,080 --> 00:44:33,010
this mentioning

680
00:44:36,110 --> 00:44:41,040
me mentioning several variations of the basic team

681
00:44:41,050 --> 00:44:42,610
so first of all

682
00:44:42,620 --> 00:44:47,580
when you're doing this iterative computation of the optimal value function you don't have to

683
00:44:47,960 --> 00:44:51,460
be all the states at all times that

684
00:44:51,460 --> 00:44:54,840
and there are advantages of not doing that

685
00:44:54,890 --> 00:44:59,390
and there are many are giddens they're trying to organize this competition in such a

686
00:45:00,090 --> 00:45:05,390
they don't waste time on updating states which are very important

687
00:45:05,430 --> 00:45:07,240
depending on the context

688
00:45:07,260 --> 00:45:12,520
that you could have target and slightly lower is that are

689
00:45:12,590 --> 00:45:17,270
so there's deterministic ontoprise real time dynamic programming

690
00:45:17,290 --> 00:45:20,460
our good model to broken saying

691
00:45:20,480 --> 00:45:23,340
so that's the idea is that you want to

692
00:45:23,390 --> 00:45:29,180
make decisions faster and you are actually interacting with the environment

693
00:45:29,200 --> 00:45:33,760
and by interacting with the money you have the time constraints you don't have much

694
00:45:33,960 --> 00:45:35,650
time to sing

695
00:45:35,680 --> 00:45:38,040
and yet you want to learn

696
00:45:38,610 --> 00:45:41,550
good enough value functions so how do you do that

697
00:45:41,570 --> 00:45:43,420
on the other side is that

698
00:45:43,430 --> 00:45:45,310
as you go along the class

699
00:45:45,320 --> 00:45:50,500
c or x you're acting as well as you can see your knowledge here agree

700
00:45:50,530 --> 00:45:53,110
fine with respect to what you

701
00:45:53,110 --> 00:45:54,720
and as you do that

702
00:45:54,730 --> 00:45:59,820
you know you're travelling along the past maybe two hours ago

703
00:45:59,850 --> 00:46:05,200
and then you're visiting the states and the question is that once these you want

704
00:46:05,200 --> 00:46:06,510
to have a

705
00:46:06,590 --> 00:46:10,090
the idea here is that you want to update all of those things that you

706
00:46:10,090 --> 00:46:11,700
are visiting

707
00:46:11,720 --> 00:46:16,940
why because well may be optimal policies for is going to focus on a pretty

708
00:46:16,970 --> 00:46:19,180
small part of the state space

709
00:46:19,190 --> 00:46:23,870
and then there is no value of learning of all the values of the state

710
00:46:23,990 --> 00:46:27,800
of those part of the building never visited by the optimal policy so the idea

711
00:46:27,800 --> 00:46:29,590
is is pretty simple

712
00:46:29,600 --> 00:46:33,870
and the implementation is is is also pretty simple

713
00:46:34,280 --> 00:46:39,230
and evaluation of the idea is that well if you don't you're not in this

714
00:46:39,230 --> 00:46:41,070
real-time time situations

715
00:46:41,090 --> 00:46:44,920
but you're in a situation where you really want to

716
00:46:45,010 --> 00:46:48,760
have the value function pretty much everywhere

717
00:46:48,790 --> 00:46:51,900
so you know and i stress are

718
00:46:52,090 --> 00:46:56,280
you of the thing from the go backwards

719
00:46:56,320 --> 00:46:59,310
and it makes alot of sense because

720
00:46:59,340 --> 00:47:02,900
OK so i have to mention the by source target and is just a special

721
00:47:02,900 --> 00:47:04,670
case of a patient

722
00:47:04,680 --> 00:47:06,230
if you think about it

723
00:47:06,260 --> 00:47:09,490
and so they are out there think from the goal

724
00:47:09,500 --> 00:47:14,490
backwards and that makes a lot of sense because you are basically propagating information back

725
00:47:14,490 --> 00:47:19,840
first from the got see are propagating discourse to go information back from the war

726
00:47:19,850 --> 00:47:23,060
and if you do the synchronous value iteration so are

727
00:47:23,110 --> 00:47:28,340
during the iteration stuff on all state synchronously at all time steps

728
00:47:28,370 --> 00:47:32,730
i don't need to waste a lot of time thinking states

729
00:47:33,160 --> 00:47:36,550
that of course are not or the the value function is not going to change

730
00:47:36,560 --> 00:47:39,910
so if you have a chain of states

731
00:47:39,920 --> 00:47:44,070
i have to go from this state started to go state

732
00:47:44,120 --> 00:47:49,200
computing a value function so these may be i finite graphs unsinkable defined i

733
00:47:49,230 --> 00:47:50,970
graph like a change

734
00:47:50,980 --> 00:47:56,020
and i use value iteration and not every time step on come i'm doing updated

735
00:47:56,020 --> 00:47:57,800
every little state

736
00:47:57,800 --> 00:47:59,830
room temp the fraction

737
00:47:59,850 --> 00:48:02,910
the fraction in the conduction band

738
00:48:02,940 --> 00:48:06,540
is about ten to the minus nineteen because so few of them you can give

739
00:48:06,540 --> 00:48:10,010
each of them name so there's almost no conduction

740
00:48:10,040 --> 00:48:13,960
silicon at room temperature is an insulator so if we want to get some conductivity

741
00:48:13,960 --> 00:48:17,930
we got heater computers have to such a high temperature i don't think so so

742
00:48:17,930 --> 00:48:19,500
we need some other way

743
00:48:19,530 --> 00:48:22,890
populating the conduction band and we're going to do

744
00:48:22,920 --> 00:48:27,660
one other mode of promotion we look for work citations like citation in the third

745
00:48:27,660 --> 00:48:29,520
mode is kemal

746
00:48:31,380 --> 00:48:33,790
he mo x citation

747
00:48:33,800 --> 00:48:38,400
what we're going to do is we're going to get carrier

748
00:48:38,410 --> 00:48:42,570
the carrier injection

749
00:48:43,110 --> 00:48:45,860
introducing impurities

750
00:48:45,880 --> 00:48:50,400
by introducing empirical and i know you're sitting there thinking he is crazy we just

751
00:48:50,400 --> 00:48:55,350
spent all this time and money making silicon six nine purity

752
00:48:55,390 --> 00:48:58,310
and now we're going to start throwing impurities in anyway

753
00:48:58,320 --> 00:49:01,190
well there's two kinds of impurities in this world

754
00:49:01,200 --> 00:49:02,950
two kinds of impurities

755
00:49:02,960 --> 00:49:05,670
there are good impurities and beneficiaries

756
00:49:05,760 --> 00:49:08,500
all right the good impurities are called dopants

757
00:49:08,540 --> 00:49:10,910
these are the ones that are their desire

758
00:49:10,910 --> 00:49:14,460
OK so dopants are good guys and then there are other ones that are called

759
00:49:16,280 --> 00:49:20,600
contaminants and these are undesirable and purity

760
00:49:20,610 --> 00:49:23,280
so we don't want contaminants so we're going to look at

761
00:49:23,360 --> 00:49:27,630
opens and this is a really important lesson right now

762
00:49:27,660 --> 00:49:31,790
i'm teaching when i'm showing you for the first time in three nine one is

763
00:49:31,790 --> 00:49:36,650
how we can use chemistry to alter the performance of the material i want to

764
00:49:36,650 --> 00:49:38,130
take a plane

765
00:49:38,140 --> 00:49:43,670
high purity silicon which is an insulator room temperature and by operating on its chemical

766
00:49:43,670 --> 00:49:50,030
composition give it properties that it cannot possess otherwise so this is the paradox is

767
00:49:50,030 --> 00:49:54,450
the paradigm for solid state chemistry you want something has got a higher yield strength

768
00:49:54,580 --> 00:49:56,530
will change its composition

769
00:49:56,550 --> 00:50:00,650
you want somebody going to higher conductivity will change its composition

770
00:50:00,650 --> 00:50:01,850
that's different

771
00:50:01,860 --> 00:50:03,020
that's different

772
00:50:03,100 --> 00:50:06,400
OK so this is an engineered materials

773
00:50:06,460 --> 00:50:12,570
this will be engineered materials and is it engineered engineered material what we're getting is

774
00:50:14,310 --> 00:50:16,230
via chemical change

775
00:50:16,420 --> 00:50:20,120
biochemical change purposeful chemical change

776
00:50:20,190 --> 00:50:22,160
so we're getting if you like

777
00:50:22,230 --> 00:50:24,410
tailored composition

778
00:50:24,430 --> 00:50:28,520
tailored composition

779
00:50:28,530 --> 00:50:34,210
the composition to give targeted properties

780
00:50:34,230 --> 00:50:35,600
and this is the

781
00:50:35,680 --> 00:50:38,690
this is what underlies material science

782
00:50:38,760 --> 00:50:40,810
you don't see which you like

783
00:50:40,830 --> 00:50:42,110
make it

784
00:50:42,180 --> 00:50:47,330
how do you make it by understanding the relationship between electronic structure

785
00:50:47,360 --> 00:50:50,290
bonding and ultimately properties

786
00:50:50,310 --> 00:50:55,680
so here's what the gamma designed to talk only about elemental semiconductors so in the

787
00:50:55,680 --> 00:50:57,360
case of

788
00:50:57,400 --> 00:50:58,540
in case of

789
00:51:00,670 --> 00:51:05,500
and romania it just never do restricted data silicon germanium that's that's not such a

790
00:51:05,500 --> 00:51:10,520
bad restrictions since we're in the silicon age all your micro devices contain silica

791
00:51:10,580 --> 00:51:14,900
so this is pretty good so here's the game but you introduce a little village

792
00:51:14,900 --> 00:51:16,080
in puri

793
00:51:16,130 --> 00:51:18,370
introduce a little vaillant

794
00:51:20,520 --> 00:51:23,320
what i mean daily of a one

795
00:51:23,330 --> 00:51:26,540
a always the same as the latin word for alias

796
00:51:27,410 --> 00:51:31,900
so we are going to introduce impurities that have different valence from silicon and germanium

797
00:51:31,900 --> 00:51:35,740
silicon and germanium are group four or fourteen if you want to use you pack

798
00:51:35,740 --> 00:51:39,150
notation so what want to do is introduce something else

799
00:51:39,180 --> 00:51:41,290
give you an example so

800
00:51:41,330 --> 00:51:46,580
and the purpose of introducing a covalent impurities is to cause

801
00:51:46,590 --> 00:51:48,170
because electron

802
00:51:51,460 --> 00:51:54,880
electron consumption

803
00:51:54,890 --> 00:51:57,800
and how they're going to work well you see best in action so here's a

804
00:51:57,800 --> 00:52:03,550
simple example lets put phosphorus in silicon phosphorus is you know is group

805
00:52:03,600 --> 00:52:05,310
fifteen or group five

806
00:52:05,320 --> 00:52:06,150
so it's

807
00:52:06,170 --> 00:52:09,470
good five valence electrons let's first look at

808
00:52:09,480 --> 00:52:14,130
cartesian world so here's silicon sp three hybridized

809
00:52:14,180 --> 00:52:17,700
and it forms a diamond like

810
00:52:17,720 --> 00:52:20,750
diamond like solid right

811
00:52:20,760 --> 00:52:23,960
diamond like solid

812
00:52:23,970 --> 00:52:26,650
so it's crystal in its regular race

813
00:52:26,710 --> 00:52:29,550
silicon is

814
00:52:29,590 --> 00:52:32,390
at all the corners of the tetrahedra

815
00:52:32,440 --> 00:52:35,410
on and on it goes give about three of these

816
00:52:37,720 --> 00:52:45,130
so what happens if i introduce phosphorus introduce phosphorus the phosphorus actually goes in

817
00:52:45,220 --> 00:52:48,900
and substitutes for silicon solar phosphorus

818
00:52:48,900 --> 00:52:55,650
phosphorus substitutes it doesn't go and sit in some void space actually substituted

819
00:52:55,660 --> 00:53:01,570
substitutes for silicon graphics going to substitute for silicon has the form bonds agreed you

820
00:53:01,570 --> 00:53:05,520
can just sit there is no passive observer here if you want to play you

821
00:53:05,520 --> 00:53:10,670
gotta bond so fast comes in with five valence electrons so it goes one two

822
00:53:10,670 --> 00:53:15,830
three four this form of its valence electrons are born to the four adjacent silicon

823
00:53:15,830 --> 00:53:21,700
is what happens to the fifth valence electrons of phosphorus there's no empty orbital in

824
00:53:22,590 --> 00:53:24,970
that if the electron can sit

825
00:53:25,060 --> 00:53:27,650
seven of the electron sitting here kind of

826
00:53:27,660 --> 00:53:30,750
tapping his foot wondering what is going to do

827
00:53:30,760 --> 00:53:34,130
let's go over the energy level diagram

828
00:53:34,130 --> 00:53:37,140
here's the energy level diagram this is silica

829
00:53:37,150 --> 00:53:43,410
elemental silicon downstairs valence band upstairs the conduction band when we put the phosphorus into

830
00:53:43,410 --> 00:53:50,770
the silicon where in energy space whereas the only place that fifty electron can go

831
00:53:50,770 --> 00:53:56,840
and this is slightly more complex to bound but again it's very

832
00:53:57,520 --> 00:54:01,130
sort of each step is actually quite

833
00:54:02,060 --> 00:54:03,790
so here we have it the

834
00:54:03,790 --> 00:54:06,570
expectation over the sigma of the soup

835
00:54:06,590 --> 00:54:07,900
of the

836
00:54:08,110 --> 00:54:11,590
over the function class of this correlation

837
00:54:11,590 --> 00:54:12,770
the thing here

838
00:54:12,770 --> 00:54:15,270
this function now is a linear function

839
00:54:15,310 --> 00:54:17,000
so we can think of the

840
00:54:17,290 --> 00:54:19,920
you know this is w com o five x

841
00:54:19,960 --> 00:54:24,790
but some of sigma i of a linear function

842
00:54:24,840 --> 00:54:27,710
we can move the some inside the

843
00:54:29,380 --> 00:54:31,400
and we get w com

844
00:54:31,420 --> 00:54:33,830
all of this gubbins inside

845
00:54:33,840 --> 00:54:37,560
some of sigma i phi i five x i

846
00:54:38,670 --> 00:54:41,250
and therefore

847
00:54:41,290 --> 00:54:48,150
how is this maximized what is maximized if w is actually parallel to this

848
00:54:48,190 --> 00:54:49,710
o thing exactly

849
00:54:50,610 --> 00:54:52,750
is equal to be

850
00:54:52,750 --> 00:54:56,790
and so we actually end up with just the norm of this quantity

851
00:54:56,880 --> 00:54:58,830
the spectre here

852
00:54:58,840 --> 00:55:02,560
and we can take all the constant sound so to be over and come out

853
00:55:02,560 --> 00:55:04,190
so we actually get

854
00:55:04,380 --> 00:55:07,710
you see that i mean if you want to maximize this over w

855
00:55:07,750 --> 00:55:09,250
you just choose w

856
00:55:09,250 --> 00:55:10,790
parallel to this thing

857
00:55:10,790 --> 00:55:15,210
and of norm b and c get be times the norm of this thing

858
00:55:16,730 --> 00:55:19,810
so actually that isn't even an inequality here

859
00:55:19,840 --> 00:55:21,290
OK and now

860
00:55:21,330 --> 00:55:24,040
there's a little bit of trade

861
00:55:24,040 --> 00:55:27,150
what you have to do is to

862
00:55:27,180 --> 00:55:29,640
make this an inner product

863
00:55:29,640 --> 00:55:32,770
with itself and take the square root

864
00:55:32,830 --> 00:55:36,160
OK so this is the norm it's the inner product with itself to describe so

865
00:55:36,160 --> 00:55:38,150
that's just an inequality

866
00:55:38,180 --> 00:55:40,470
and now you move the square root

867
00:55:40,500 --> 00:55:44,260
through the expectation so jensen's inequality

868
00:55:44,350 --> 00:55:46,500
and if you do that so

869
00:55:46,640 --> 00:55:48,540
what's happening is your sort of

870
00:55:48,550 --> 00:55:52,180
rather than taking the expectation of the square root values

871
00:55:52,230 --> 00:55:54,460
square root values

872
00:55:54,470 --> 00:55:58,510
you're taking expectation here and then taking the square root and you can imagine what

873
00:55:58,510 --> 00:55:59,920
you get is bigger

874
00:56:00,720 --> 00:56:04,670
so this is actually bigger because you take expectation first here

875
00:56:04,890 --> 00:56:08,670
the input and then takes credit because the square root is concave

876
00:56:08,720 --> 00:56:10,340
you actually get bigger value

877
00:56:10,390 --> 00:56:16,190
so it's just jensen's inequality so now what do you do well you've got an

878
00:56:16,190 --> 00:56:17,920
expectation here

879
00:56:17,970 --> 00:56:19,640
of the sum

880
00:56:19,720 --> 00:56:23,040
and so you can take expectation inside

881
00:56:23,040 --> 00:56:24,210
the sum

882
00:56:24,300 --> 00:56:27,720
and the expectation of sigma i sigma j

883
00:56:27,760 --> 00:56:30,100
where i and j are different

884
00:56:30,130 --> 00:56:33,800
is is zero because they cancel each other

885
00:56:33,830 --> 00:56:35,090
out again

886
00:56:35,100 --> 00:56:36,140
plus one

887
00:56:36,140 --> 00:56:40,020
with probability half minus one probability half so they cancel

888
00:56:40,040 --> 00:56:42,810
so all of the cross terms here cancelled

889
00:56:42,890 --> 00:56:45,880
and you end up with just the diagonal terms and on the diagonal you get

890
00:56:45,880 --> 00:56:48,180
sigma i squared which is always one

891
00:56:48,220 --> 00:56:53,010
and so you end up with the sum of the diagonal terms in this

892
00:56:53,010 --> 00:56:55,430
kernel evaluation

893
00:56:55,480 --> 00:56:58,810
so that's just the trace of the kernel matrix

894
00:56:59,090 --> 00:57:02,230
square root of that times to be

895
00:57:02,290 --> 00:57:05,150
OK so this is the empirical rademacher complexity

896
00:57:05,170 --> 00:57:08,860
and therefore you know you can handle your case with the norms growing

897
00:57:09,930 --> 00:57:13,210
you just actually measure the norms on the sample

898
00:57:13,260 --> 00:57:15,470
and that's how you get actually that

899
00:57:16,890 --> 00:57:20,220
OK so that's the the rademacher complexity for

900
00:57:20,230 --> 00:57:22,140
a set of functions

901
00:57:22,190 --> 00:57:24,430
sorry for the

902
00:57:24,470 --> 00:57:28,600
so of functions empirical rademacher complexity on a particular sample

903
00:57:28,630 --> 00:57:34,670
of linear functions with norm at most b in in a kernel defined feature space

904
00:57:34,720 --> 00:57:38,330
OK so now applying that to ESPN's

905
00:57:40,270 --> 00:57:45,390
OK so there's a few steps here i've slightly read jaguar the SVM

906
00:57:45,470 --> 00:57:53,100
definition here to explicitly make an optimisation ghalib it's entirely equivalent to the standard method

907
00:57:53,140 --> 00:57:56,890
slide just slightly formulation with the norm

908
00:57:56,980 --> 00:57:59,560
if w squared constrained to be one

909
00:57:59,610 --> 00:58:02,010
and these are the slack variables

910
00:58:03,010 --> 00:58:04,510
what we need to do

911
00:58:04,550 --> 00:58:06,140
is to bound this

912
00:58:06,140 --> 00:58:11,840
expectation of the loss and this is the heaviside function here so zero and one

913
00:58:11,850 --> 00:58:13,590
of minus y

914
00:58:13,600 --> 00:58:17,570
g x so this is going to be plus one if there's an error and

915
00:58:17,570 --> 00:58:21,720
zero or minus one if it's correctly classified or negative

916
00:58:21,730 --> 00:58:26,170
and by the heaviside will be therefore zero every size just the function because zero

917
00:58:26,190 --> 00:58:27,420
and zero goes

918
00:58:27,430 --> 00:58:28,510
jumps to one

919
00:58:28,520 --> 00:58:33,300
so this is just the way of expressing the probability of misclassification as an expectation

920
00:58:33,300 --> 00:58:34,760
which is what we need

921
00:58:34,800 --> 00:58:35,930
to apply the

922
00:58:38,250 --> 00:58:39,180
and now

923
00:58:39,190 --> 00:58:42,730
the trick is to approach upper bound that h

924
00:58:42,770 --> 00:58:45,670
that function that we were looking at by

925
00:58:45,710 --> 00:58:50,590
a hinge loss function which just basically starts zero

926
00:58:50,600 --> 00:58:53,380
and then goes up to one

927
00:58:53,390 --> 00:58:55,930
with a gradient of one and gamma

928
00:58:55,930 --> 00:58:58,510
between zero and

929
00:58:58,720 --> 00:59:01,180
minus gamma so it's sort of the

930
00:59:01,220 --> 00:59:03,430
it's sort of

931
00:59:03,460 --> 00:59:06,130
goes the other way round it starts at one

932
00:59:06,640 --> 00:59:10,650
so that's right for a greater than zero is one

933
00:59:10,680 --> 00:59:15,760
and then it goes down to zero at minus gamma

934
00:59:16,550 --> 00:59:21,480
so that will be upper bounding the heaviside function which is zero all the way to

935
00:59:21,570 --> 00:59:24,040
zero and then jumps to one

936
00:59:24,050 --> 00:59:26,930
so we can take this function here

937
00:59:26,940 --> 00:59:29,470
and and upper bounded by this one

938
00:59:29,540 --> 00:59:33,350
i've added the minus one for technical reasons

939
00:59:33,390 --> 00:59:37,110
and now we apply the rademacher theorem this is the empirical value of that this

940
00:59:37,110 --> 00:59:38,800
is the rademacher complexity

941
00:59:38,850 --> 00:59:40,840
and this is the extra term

942
00:59:40,850 --> 00:59:43,250
so to plug-and-play here

943
00:59:43,260 --> 00:59:48,350
and now we need to bound the rademacher complexity this quantity here

944
00:59:50,420 --> 00:59:54,380
the well so first of all let's look at the empirical value of this

945
00:59:54,390 --> 00:59:58,720
quantity and that is just the sum of the slack variables divided by gamma

946
00:59:58,750 --> 01:00:02,010
so we actually getting the exactly the one norm

947
01:00:02,020 --> 01:00:06,380
of the slack variables that you optimize in the

948
01:00:06,390 --> 01:00:08,420
in the in the support vector

949
01:00:10,850 --> 01:00:13,430
you have this empirical rademacher complexity

950
01:00:13,430 --> 01:00:15,810
and this extra term

951
01:00:15,840 --> 01:00:19,310
and we have the final step is to bound this

952
01:00:19,360 --> 01:00:23,770
this empirical rademacher complexity in terms of

953
01:00:23,800 --> 01:00:28,130
the rademacher complexity that we've already computed of the underlying

954
01:00:28,170 --> 01:00:30,840
linear functions so this is this composition

955
01:00:30,850 --> 01:00:34,220
with the loss function which is now hinge loss

956
01:00:34,230 --> 01:00:40,300
OK now there's the a detailed which i'm going to skip the proof of the

957
01:00:40,600 --> 01:00:47,140
this basically results in a multiplication by the lipschitz cont constant of this

958
01:00:47,150 --> 01:00:49,250
loss function

959
01:00:49,250 --> 01:00:52,090
and there's the detailed proof of that which

960
01:00:52,100 --> 01:00:57,310
is quite involved nothing very serious but it just is a bit involves i'm going

961
01:00:57,310 --> 01:00:58,430
to skip over it

962
01:00:59,640 --> 01:01:00,510
the basic

963
01:01:00,520 --> 01:01:04,790
the result is that you get two times the lipschitz constant

964
01:01:04,790 --> 01:01:10,900
what is x values should be distributed according to some pdf f of x that

965
01:01:10,910 --> 01:01:12,420
you're interested in

966
01:01:12,430 --> 01:01:17,710
now you have access just a scalar quantity but in general text could be could

967
01:01:17,710 --> 01:01:21,490
be not a single number perhaps a collection of numbers x could be a vector

968
01:01:21,850 --> 01:01:27,980
but for the examples that show now of only consider x being a scalar quantity

969
01:01:28,570 --> 01:01:34,150
so the next step the third step is to use these x values two

970
01:01:34,150 --> 01:01:37,180
estimate some property of the pdf

971
01:01:37,190 --> 01:01:39,920
f of x that you're interested in

972
01:01:39,930 --> 01:01:43,030
a simple example would be that if you consider simply too

973
01:01:43,050 --> 01:01:45,570
fixed values from a to b

974
01:01:45,610 --> 01:01:48,490
and you simply calculate the fraction of

975
01:01:48,500 --> 01:01:52,200
the fraction of these x values that fall between a and b

976
01:01:52,330 --> 01:01:55,910
and that would give you what is that's an estimate of the probability for x

977
01:01:55,920 --> 01:01:56,830
to be

978
01:01:56,860 --> 01:02:00,460
between a and b and we know that simply the integral

979
01:02:00,460 --> 01:02:01,730
from a to b

980
01:02:01,760 --> 01:02:03,560
of the pdf

981
01:02:03,700 --> 01:02:09,850
so that that's a way of estimating this particular quantity and that is then given

982
01:02:09,850 --> 01:02:14,380
by the integral and that's generally what you find is the monte carlo calculations it

983
01:02:14,380 --> 01:02:18,860
is formally is equivalent to doing an integral

984
01:02:19,480 --> 01:02:24,720
you don't have to do it that way in fact sometimes we simply regard the

985
01:02:24,740 --> 01:02:30,450
jet generated data is generated sequence of points as simulated data

986
01:02:30,450 --> 01:02:35,010
rather than generating according to this algorithm we could actually do the experiment and these

987
01:02:35,010 --> 01:02:36,720
x values might actually

988
01:02:38,480 --> 01:02:41,680
refer to the outcomes of our measurements

989
01:02:41,690 --> 01:02:46,650
so we can use these generated values simulated data and use them for testing our

990
01:02:46,650 --> 01:02:53,470
statistical procedures or otherwise calibrating the measurement procedures that we would eventually then carry out

991
01:02:53,500 --> 01:02:56,910
with with real data

992
01:02:56,930 --> 01:03:01,410
OK so now we say a few words about each of the three steps are

993
01:03:01,440 --> 01:03:05,430
the first step was to generate this sequence of random numbers

994
01:03:05,450 --> 01:03:08,410
the followed a uniform distribution

995
01:03:08,420 --> 01:03:12,350
right now you can imagine how would you do that you're going to represent a

996
01:03:13,240 --> 01:03:14,380
in a computer

997
01:03:14,390 --> 01:03:19,460
say with a certain number of bits suppose to use thirty two bit to represent

998
01:03:19,460 --> 01:03:23,190
a number that would go from say zero up to some maximum value

999
01:03:23,210 --> 01:03:25,690
and then i could take that number and divide

1000
01:03:25,740 --> 01:03:29,530
by the maximum possible value that would give me a number somewhere between zero and

1001
01:03:30,850 --> 01:03:34,930
now how can i generate a number that would be random when you could simply

1002
01:03:34,930 --> 01:03:37,840
say well for every bit i'm going to toss the coin

1003
01:03:37,860 --> 01:03:40,510
and if it had so i don't know one and it's zero

1004
01:03:40,520 --> 01:03:42,710
head tail tales right down to zero

1005
01:03:42,710 --> 01:03:47,460
and then after thirty two point tosses i'll have generated random numbers

1006
01:03:47,470 --> 01:03:48,530
and that would work

1007
01:03:48,560 --> 01:03:50,270
actually and people have

1008
01:03:50,280 --> 01:03:53,470
i tried to use physical processes to

1009
01:03:53,470 --> 01:03:57,270
form the basis of random number generators and for certain very

1010
01:03:57,300 --> 01:04:00,240
specialized applications that may be something

1011
01:04:00,250 --> 01:04:04,050
the people want to do but is generally too much trouble and has its own

1012
01:04:04,050 --> 01:04:08,390
host of suite of problems and so on what we do is is we use

1013
01:04:08,390 --> 01:04:10,800
a deterministic computer algorithms

1014
01:04:10,810 --> 01:04:12,690
to generate these random numbers

1015
01:04:12,690 --> 01:04:14,770
now these algorithms have

1016
01:04:14,860 --> 01:04:18,920
the property that if you run them again you get exactly the same sequence of

1017
01:04:18,920 --> 01:04:23,880
random numbers so you can't really regarding this truly random sometimes called

1018
01:04:23,900 --> 01:04:25,420
pseudo random numbers

1019
01:04:25,440 --> 01:04:27,260
but for all

1020
01:04:27,300 --> 01:04:31,980
purposes that we use them for they are effectively as good as true random numbers

1021
01:04:31,980 --> 01:04:33,300
in fact better

1022
01:04:33,340 --> 01:04:37,480
because it can very often be very useful to be able to reproduce again the

1023
01:04:37,480 --> 01:04:42,920
exact same sequence of random numbers sometimes you see above your programmer see some unexpected

1024
01:04:42,920 --> 01:04:46,710
behavior and you would like to be able to reproduce that was true random numbers

1025
01:04:46,710 --> 01:04:47,420
you could

1026
01:04:47,460 --> 01:04:52,820
i guess never do that with this deterministic algorithm you can reproduce the sequence if

1027
01:04:52,820 --> 01:04:54,550
you desire

1028
01:04:54,550 --> 01:04:57,650
right so there there are a number of algorithms

1029
01:04:57,650 --> 01:05:01,820
but one can use and i want to describe very simple but quite effective algorithm

1030
01:05:01,820 --> 01:05:05,340
just to give you an idea of how they work and this is what's called

1031
01:05:05,340 --> 01:05:08,090
the multiplicative linear congruence rules

1032
01:05:08,110 --> 01:05:10,780
generator or g

1033
01:05:10,780 --> 01:05:15,070
and the idea behind this algorithm is the following is that you start with an

1034
01:05:15,070 --> 01:05:18,630
initial value and zero which is called the seed

1035
01:05:18,630 --> 01:05:21,170
and then i need to supply also

1036
01:05:21,230 --> 01:05:26,550
two constant values among supplier a and the modulus

1037
01:05:27,510 --> 01:05:32,300
and what this algorithm does it specifies the rule that tells you how to to

1038
01:05:32,300 --> 01:05:34,000
update this

1039
01:05:34,000 --> 01:05:38,190
value and so here's the rule that tells you how to go from its value

1040
01:05:38,230 --> 01:05:40,150
to the i plus one value

1041
01:05:40,170 --> 01:05:41,820
and i plus one

1042
01:05:41,840 --> 01:05:46,440
it is equal to the multiplier times and i modulo

1043
01:05:47,280 --> 01:05:48,530
and this model

1044
01:05:48,550 --> 01:05:53,460
the modulus operator says if you take the remainder when you take the the guide

1045
01:05:53,460 --> 01:05:56,510
to the left divided by the guide to the right and then you take the

1046
01:05:56,510 --> 01:06:01,130
remainder so for example twenty seven mod five would mean that you take twenty seven

1047
01:06:01,130 --> 01:06:02,590
divided by five

1048
01:06:02,610 --> 01:06:03,880
which then has

1049
01:06:03,900 --> 01:06:05,460
the remainder of two

1050
01:06:05,550 --> 01:06:08,110
right so twenty seven mod five would be two

1051
01:06:08,130 --> 01:06:12,210
so it's the remainder after integer division

1052
01:06:12,240 --> 01:06:14,590
OK so that's the rule it's a very simple rule

1053
01:06:14,590 --> 01:06:17,650
to program up in a computer

1054
01:06:17,670 --> 01:06:21,710
and it has a number of interesting properties

1055
01:06:21,730 --> 01:06:27,190
the most striking properties is the sequence of numbers that produces is periodic

1056
01:06:27,210 --> 01:06:31,670
that sounds crazy this post is supposed to look like random numbers that sort of

1057
01:06:31,670 --> 01:06:33,480
certainly there there are periodic

1058
01:06:33,500 --> 01:06:39,050
they could not regarded this as representing random quantities now the idea is that you

1059
01:06:39,050 --> 01:06:41,780
need to make this period very very long

1060
01:06:41,800 --> 01:06:46,380
and you're you're only allowed to use the sequence well within a single period if

1061
01:06:46,380 --> 01:06:50,630
this thing actually starts to repeat then there no good as random values

1062
01:06:50,650 --> 01:06:56,550
so here is just an example to illustrate how this periodicity can arise suppose we

1063
01:06:56,550 --> 01:06:57,550
were to take

1064
01:06:57,630 --> 01:07:01,650
a value for the multiplier of three in the model s seven

1065
01:07:01,650 --> 01:07:05,260
and we take as the initial value the seed one

1066
01:07:05,280 --> 01:07:09,570
this is crazy by the way the value the picture modulus is typically close to

1067
01:07:09,590 --> 01:07:14,000
the largest possible value that you could store in a computer with the number of

1068
01:07:14,000 --> 01:07:18,380
bits that you will allocate but just to illustrate this periodic feature i've chosen the

1069
01:07:18,380 --> 01:07:20,380
fairly low value for

1070
01:07:20,690 --> 01:07:24,240
the modulus this is actually an example from the book by plant that i mentioned

1071
01:07:24,610 --> 01:07:27,780
OK so let's see how it works and zero is one

1072
01:07:27,800 --> 01:07:32,960
and one is therefore the multiplier three times one month seven so let's see three

1073
01:07:32,960 --> 01:07:34,710
months seven that's three

1074
01:07:34,730 --> 01:07:36,650
so four and two

1075
01:07:36,670 --> 01:07:40,710
it is three times three that's nine months seven is too much

1076
01:07:40,710 --> 01:07:44,480
and so forth down the line and here i find finally in six is three

1077
01:07:44,480 --> 01:07:47,400
times and five fifteen months seven

1078
01:07:47,420 --> 01:07:48,480
is one

1079
01:07:48,500 --> 01:07:49,920
but that's where i started

1080
01:07:49,940 --> 01:07:52,760
OK so the sequence simply repeats

1081
01:07:52,780 --> 01:07:56,150
now you may think that was only a very simple algorithm how about some of

1082
01:07:56,150 --> 01:08:01,280
these more complicated algorithms they all repeatedly that a common feature of every

1083
01:08:01,330 --> 01:08:03,650
algorithms that i know

1084
01:08:03,650 --> 01:08:08,030
that is in use they all have some sort of periods

1085
01:08:08,050 --> 01:08:12,360
and in practice what you do is you try to make that period extremely long

1086
01:08:12,360 --> 01:08:14,150
we all

1087
01:08:14,170 --> 01:08:20,850
let me know how in

1088
01:08:27,750 --> 01:08:31,890
two or more

1089
01:08:31,960 --> 01:08:40,880
when we look

1090
01:10:00,160 --> 01:10:02,460
is this

1091
01:10:49,940 --> 01:10:59,270
you are

1092
01:12:22,660 --> 01:12:25,800
the model

1093
01:13:25,150 --> 01:13:30,230
well all

1094
01:13:35,860 --> 01:13:40,970
almost like about

1095
01:13:58,530 --> 01:14:03,820
so you can

1096
01:14:12,210 --> 01:14:15,210
one of

1097
01:14:39,190 --> 01:14:40,740
my work

1098
01:14:40,750 --> 01:14:43,510
the only

1099
01:14:43,530 --> 01:14:48,040
so the

1100
01:14:48,100 --> 01:14:51,210
little girl

1101
01:14:51,230 --> 01:14:58,280
but she would

1102
01:15:32,840 --> 01:15:37,620
if they no

1103
01:16:00,330 --> 01:16:04,930
at the end

1104
01:16:25,250 --> 01:16:29,290
no because

1105
01:16:37,090 --> 01:16:43,520
data where

1106
01:17:05,730 --> 01:17:09,770
one two

1107
01:17:59,460 --> 01:18:01,480
two men one

1108
01:18:01,480 --> 01:18:04,480
you smoke you're going to die early enough

1109
01:18:04,530 --> 01:18:07,110
but you won't drawing social security

1110
01:18:07,130 --> 01:18:10,980
and therefore the government actually saves money by smoking

1111
01:18:11,030 --> 01:18:14,030
because by the time they add up how much they get on the stick tobacco

1112
01:18:14,960 --> 01:18:19,730
and how much they earn by you're not living long enough to draw attention

1113
01:18:19,770 --> 01:18:23,510
it's much better it's much more money than how much it's going to cost

1114
01:18:24,480 --> 01:18:27,190
take care to wire died from emphysema

1115
01:18:27,210 --> 01:18:30,710
or bladder cancer or lung cancer or

1116
01:18:30,730 --> 01:18:32,110
heart disease

1117
01:18:32,230 --> 01:18:35,780
many more people die from heart attacks due to smoking then died from lung cancer

1118
01:18:35,780 --> 01:18:38,880
in fact so think about this

1119
01:18:39,000 --> 01:18:42,800
think about this if you smoke it's probably a good time to stop because if

1120
01:18:42,800 --> 01:18:45,650
you continue your age especially for women

1121
01:18:45,670 --> 01:18:49,270
which is for some reason women have a hard time stopping them and i can

1122
01:18:49,280 --> 01:18:54,250
see why it's probably some physiological things if you if you continue now you're it

1123
01:18:54,250 --> 01:18:56,630
will be almost impossible to stop

1124
01:18:56,670 --> 01:18:59,800
if you lived with smokers

1125
01:18:59,820 --> 01:19:01,630
ask them to leave

1126
01:19:01,710 --> 01:19:05,690
if you live in a home with your parents and they smoke

1127
01:19:05,690 --> 01:19:08,480
tell them it's time for them to leave

1128
01:19:08,500 --> 01:19:10,630
from house

1129
01:19:10,690 --> 01:19:16,800
smoke second-hand smoking killed probably between sixty and eighty thousand people last year in this

1130
01:19:17,960 --> 01:19:20,050
second smoke and by the way

1131
01:19:20,070 --> 01:19:24,610
if you want to see an interesting phenomenon goal veterinary hospital because there

1132
01:19:24,630 --> 01:19:28,980
they was great frequently frequency treat dogs with lung cancer

1133
01:19:29,000 --> 01:19:30,900
and why do they have lung cancer

1134
01:19:30,960 --> 01:19:35,150
in nineteen ninety nine percent of the cases in one hundred percent of the cases

1135
01:19:35,170 --> 01:19:36,510
these dogs

1136
01:19:37,420 --> 01:19:42,420
with all to smoke and average tobacco smoke goes through six or eight dogs in

1137
01:19:42,420 --> 01:19:44,380
his or her lifetime

1138
01:19:45,920 --> 01:19:49,070
it's absolutely true

1139
01:19:49,170 --> 01:19:52,480
so if you if you think the dogs

1140
01:19:52,530 --> 01:19:56,500
if that's the fact for the for the for the is to think about what's

1141
01:19:56,500 --> 01:19:58,670
happening to the inside of your lungs

1142
01:19:58,690 --> 01:20:02,090
so i'm going to take back what i said before before i told you that

1143
01:20:02,090 --> 01:20:05,340
the most important thing for you to do in this course

1144
01:20:05,360 --> 01:20:09,340
is to learn how to think clearly and to assess the still conceptual complexity of

1145
01:20:09,630 --> 01:20:12,820
there's actually one more thing that even more important to get out of this course

1146
01:20:12,860 --> 01:20:14,750
if you do

1147
01:20:14,770 --> 01:20:15,860
and that is

1148
01:20:15,920 --> 01:20:17,230
to stop smoking

1149
01:20:17,250 --> 01:20:18,840
if you do that

1150
01:20:18,840 --> 01:20:19,900
if you do that

1151
01:20:19,900 --> 01:20:23,380
it will be vastly more important for the rest of your life but anything you

1152
01:20:23,380 --> 01:20:25,610
learn here

1153
01:20:25,650 --> 01:20:28,440
so write that down vastly more important

1154
01:20:28,460 --> 01:20:32,730
you may think it's glamorous you may think it's exciting but keep in mind

1155
01:20:32,750 --> 01:20:34,900
people to stop smoking

1156
01:20:36,340 --> 01:20:42,130
vastly greater effect on reducing the morbidity and mortality

1157
01:20:42,190 --> 01:20:43,460
in this country

1158
01:20:43,630 --> 01:20:46,190
anything the cancer researchers can do

1159
01:20:46,230 --> 01:20:47,460
keep that in mind

1160
01:20:47,480 --> 01:20:49,480
and if you start smoking now

1161
01:20:49,500 --> 01:20:52,210
and you think that somehow the cancer research is to be able to come up

1162
01:20:52,210 --> 01:20:56,610
with some miracle cure by this time you start quite coughing and starts spitting up

1163
01:20:58,250 --> 01:20:59,630
don't be so certain

1164
01:20:59,690 --> 01:21:03,500
they may not be able to save the ball you're fat out of the fire

1165
01:21:03,500 --> 01:21:07,860
so i don't know whether i get this message in a very subtle way or

1166
01:21:07,880 --> 01:21:10,650
a picture of

1167
01:21:10,710 --> 01:21:13,460
but i think i think about that

1168
01:21:13,630 --> 01:21:20,150
now never going to focus on one cancer with focus on cancer because it's one

1169
01:21:20,150 --> 01:21:24,070
of the consequences of cigarette smoking but it's a disease we want to talk about

1170
01:21:24,090 --> 01:21:28,710
both this time next time and we want to relate here to the cell cycle

1171
01:21:28,730 --> 01:21:32,170
and how the growth of cell proliferation occurs

1172
01:21:32,170 --> 01:21:38,670
i told you last time the human tumor is roughly speaking about the human body

1173
01:21:38,670 --> 01:21:41,880
roughly carries three times ten to the thirteenth cells

1174
01:21:41,900 --> 01:21:44,590
so that's quite a few cells

1175
01:21:44,650 --> 01:21:48,000
that somebody cells are in the human body plus or minus

1176
01:21:48,000 --> 01:21:52,960
human tumor of one let's say one cubic centimeter

1177
01:21:52,980 --> 01:21:55,530
is roughly ten to the ninth cells

1178
01:21:55,530 --> 01:21:57,800
would quickly make a scan over the sky

1179
01:21:57,910 --> 01:22:00,250
five minutes that's all they had

1180
01:22:00,280 --> 01:22:02,880
and i joined the group of george clark

1181
01:22:02,900 --> 01:22:04,620
which come at MIT

1182
01:22:04,750 --> 01:22:08,660
he was doing actually astronomy from very high flying balloons

1183
01:22:08,690 --> 01:22:10,930
very close to the top of the atmosphere

1184
01:22:10,940 --> 01:22:15,490
and the advantage of balloons was that you could observe the sky for many many

1185
01:22:15,490 --> 01:22:19,070
hours if you're lucky sometimes even today or more

1186
01:22:19,120 --> 01:22:20,800
but on the other hand

1187
01:22:20,810 --> 01:22:24,620
since there is always a little bit of atmosphere left above you even though very

1188
01:22:24,620 --> 01:22:26,650
little there's still some left

1189
01:22:26,660 --> 01:22:29,690
he actually is i absorbs almost all

1190
01:22:29,700 --> 01:22:30,990
x-rays below

1191
01:22:30,990 --> 01:22:35,740
twenty killer that controls would be absorbed and we wouldn't be able to see them

1192
01:22:35,740 --> 01:22:37,220
but of course

1193
01:22:38,570 --> 01:22:41,150
compensation was that we could look at this guy

1194
01:22:41,220 --> 01:22:43,660
for many many many hours

1195
01:22:43,670 --> 01:22:46,970
nowadays no one is doing balloon observations anymore

1196
01:22:46,980 --> 01:22:50,160
no more rocket observations everything is done of course

1197
01:22:50,170 --> 01:22:52,250
from satellites

1198
01:22:52,300 --> 01:22:55,940
so when i came to MIT together with george clark i developed

1199
01:22:55,950 --> 01:22:58,950
new x-ray detectors for these balloon observations

1200
01:22:58,970 --> 01:23:01,120
many graduate students were involved

1201
01:23:01,130 --> 01:23:03,050
many undergraduates

1202
01:23:03,110 --> 01:23:05,280
will take about two years to build

1203
01:23:06,740 --> 01:23:08,680
give you a rough idea will take

1204
01:23:08,740 --> 01:23:10,130
million dollars

1205
01:23:10,150 --> 01:23:11,610
in terms of nineteen

1206
01:23:11,620 --> 01:23:12,750
sixty six

1207
01:23:12,790 --> 01:23:13,990
dollars and

1208
01:23:14,010 --> 01:23:17,070
the weight of such a telescope will be roughly

1209
01:23:17,090 --> 01:23:19,180
one thousand kilograms

1210
01:23:19,240 --> 01:23:22,730
the belongs in those days cost about two hundred thousand dollars

1211
01:23:22,740 --> 01:23:25,010
gets up to these high altitudes

1212
01:23:25,060 --> 01:23:27,870
and we would need about eighty thousand dollars of helium

1213
01:23:27,880 --> 01:23:30,200
you will see some slides of that

1214
01:23:30,220 --> 01:23:32,840
we have to go to an altitude of about two hundred forty

1215
01:23:32,850 --> 01:23:34,800
a thousand feet

1216
01:23:34,810 --> 01:23:36,750
really huge problems for that

1217
01:23:36,760 --> 01:23:38,150
you'll see one

1218
01:23:38,180 --> 01:23:40,350
they have diameters of about six

1219
01:23:40,360 --> 01:23:42,090
a hundred feet

1220
01:23:42,100 --> 01:23:43,200
and the material

1221
01:23:44,600 --> 01:23:48,290
extremely thin to make them light weight so that they can go i

1222
01:23:48,360 --> 01:23:50,470
the thickness of that polyethylene

1223
01:23:50,480 --> 01:23:53,950
was about half of one thousands of an inch which is then

1224
01:23:54,040 --> 01:23:56,550
then the surrender at that you have in the kitchen

1225
01:23:56,560 --> 01:23:59,430
it is thinner than cigarette paper

1226
01:23:59,440 --> 01:24:01,990
very risky business to fly these balloons

1227
01:24:01,990 --> 01:24:04,360
no guarantee of course they will work

1228
01:24:04,420 --> 01:24:08,060
you pay your money if they work that's great if they don't work that just

1229
01:24:08,060 --> 01:24:09,420
tough luck

1230
01:24:09,530 --> 01:24:12,950
there's a good chance that you have a failure when you want the balloon

1231
01:24:13,000 --> 01:24:14,550
very fragile

1232
01:24:14,600 --> 01:24:16,660
could be damaged right at the launch

1233
01:24:16,670 --> 01:24:19,480
but even if they make it up in the atmosphere they have to go through

1234
01:24:19,480 --> 01:24:20,910
the trouble course

1235
01:24:21,040 --> 01:24:23,880
one hundred thousand feet very very cold

1236
01:24:23,930 --> 01:24:25,260
balloons get brittle

1237
01:24:25,280 --> 01:24:27,470
and then they can burst

1238
01:24:27,490 --> 01:24:28,490
that of course

1239
01:24:28,500 --> 01:24:30,950
would be and then of the balloon flights

1240
01:24:31,010 --> 01:24:35,930
and that could also be the end of a phd thesis because all these

1241
01:24:35,980 --> 01:24:38,880
flights of course we're connected with research

1242
01:24:38,900 --> 01:24:40,820
and therefore his phd work

1243
01:24:40,850 --> 01:24:42,490
sort attention during these

1244
01:24:42,500 --> 01:24:44,490
early phases of the ones always

1245
01:24:44,500 --> 01:24:48,970
extremely high sometimes even unbearable

1246
01:24:48,980 --> 01:24:52,130
so now i would like to show you

1247
01:24:52,190 --> 01:24:55,200
some slides

1248
01:24:55,220 --> 01:24:56,750
which will give you

1249
01:24:56,870 --> 01:25:00,470
good idea of what

1250
01:25:00,490 --> 01:25:01,940
these expeditions

1251
01:25:01,950 --> 01:25:11,810
we're like

1252
01:25:11,880 --> 01:25:17,570
i am a classic problems

1253
01:25:19,090 --> 01:25:20,930
now it work

1254
01:25:21,060 --> 01:25:25,310
all right so if i can have the first slide

1255
01:25:25,340 --> 01:25:27,530
you see here

1256
01:25:28,950 --> 01:25:33,750
and that's who at the time were undergraduates they both phd

1257
01:25:33,760 --> 01:25:37,420
and they're working they're very tedious work trying to

1258
01:25:37,460 --> 01:25:40,420
putting electronics together

1259
01:25:40,460 --> 01:25:41,600
you may think that

1260
01:25:41,620 --> 01:25:44,520
science is not very romantic

1261
01:25:44,530 --> 01:25:47,140
but i can assure you it is they fell in love with a married to

1262
01:25:47,140 --> 01:25:48,500
have kids

1263
01:25:48,510 --> 01:25:58,560
that's the way it sometimes goes in life

1264
01:25:58,570 --> 01:26:04,260
here you see the plant in texas where these huge balloons were made

1265
01:26:04,300 --> 01:26:07,390
a lot support again sort of like

1266
01:26:07,430 --> 01:26:10,030
the way to the tangerine is put together

1267
01:26:10,070 --> 01:26:13,710
the surface see these of the balloon

1268
01:26:13,770 --> 01:26:16,280
and the ceiling of the gore through

1269
01:26:16,290 --> 01:26:17,710
make up the balloon with

1270
01:26:17,830 --> 01:26:21,270
only done by women only women were allowed to work there

1271
01:26:21,300 --> 01:26:24,480
nothing to do with sex discrimination of any kind

1272
01:26:24,500 --> 01:26:25,830
i just found out that

1273
01:26:25,890 --> 01:26:28,960
women were more patients they were better

1274
01:26:28,970 --> 01:26:32,930
make way fewer mistakes than men

1275
01:26:33,010 --> 01:26:38,710
that's the way it goes sometimes in life

1276
01:26:38,730 --> 01:26:43,000
you see people coming out of the box nicely techniques

1277
01:26:43,050 --> 01:26:44,670
plastic cover

1278
01:26:45,570 --> 01:26:47,480
we also have a year

1279
01:26:47,540 --> 01:26:49,560
well only

1280
01:26:49,580 --> 01:26:52,320
on the graph because of something

1281
01:26:52,340 --> 01:26:54,430
so only that

1282
01:26:54,480 --> 01:26:58,170
but the graph norms

1283
01:26:58,180 --> 01:27:00,540
this was not my

1284
01:27:00,550 --> 01:27:03,300
we were there was something wrong with them

1285
01:27:03,310 --> 01:27:05,150
can so

1286
01:27:05,170 --> 01:27:08,510
concerning the was holding the balloons

1287
01:27:09,320 --> 01:27:15,010
the whole is nothing more than that because the whole is almost always through many

1288
01:27:15,010 --> 01:27:20,960
many many of which are looking at all the layers that form together

1289
01:27:21,010 --> 01:27:23,900
that's my

1290
01:27:23,940 --> 01:27:26,680
story was never nice

1291
01:27:26,720 --> 01:27:28,800
if you see fail

1292
01:27:31,540 --> 01:27:33,720
now there

1293
01:27:36,890 --> 01:27:39,000
right heart failure

1294
01:27:39,010 --> 01:27:42,100
now you get a pretty good idea what it's like

1295
01:27:42,160 --> 01:27:43,520
you see them on

1296
01:27:43,560 --> 01:27:45,410
telescope is there

1297
01:27:45,470 --> 01:27:49,460
there you see it's enormously big blue or is and he is now in most

1298
01:27:49,460 --> 01:27:51,320
of these will stay empty

1299
01:27:51,340 --> 01:27:52,980
this is the role

1300
01:27:53,060 --> 01:27:54,390
which holds this

1301
01:27:54,430 --> 01:27:57,830
part down this the only part that will be inflated

1302
01:27:57,880 --> 01:27:59,750
you see the newly in proc

1303
01:27:59,800 --> 01:28:02,090
see inflation tubes

1304
01:28:02,150 --> 01:28:04,470
we will have the men from both sides

1305
01:28:04,500 --> 01:28:07,430
which will then gradually begin to feel this part

1306
01:28:07,450 --> 01:28:10,760
of the building

1307
01:28:10,780 --> 01:28:15,130
the role of arm detail armies very important

1308
01:28:15,150 --> 01:28:18,550
because when this part of the balloon is being filtered wants to lift wants to

1309
01:28:18,550 --> 01:28:19,960
go up

1310
01:28:19,970 --> 01:28:22,230
and of course you have to keep it down you have to keep it under

1311
01:28:23,390 --> 01:28:25,290
so these are all arm in this

1312
01:28:25,300 --> 01:28:26,640
his car

1313
01:28:26,680 --> 01:28:28,070
it's loaded down with

1314
01:28:28,130 --> 01:28:30,830
concrete is very heavy and then

1315
01:28:30,890 --> 01:28:36,930
just before the launch this role arms by command is flipped over and then as

1316
01:28:36,930 --> 01:28:39,650
you will see later that the balloon will

1317
01:28:39,660 --> 01:28:41,870
make it up

1318
01:28:41,930 --> 01:28:44,880
you see the early part of the inflation

1319
01:28:44,940 --> 01:28:47,300
it comes in from both sides

1320
01:28:47,340 --> 01:28:51,550
so we we fly these balloons almost always only morning because then the winds are

1321
01:28:53,570 --> 01:28:55,030
you need extremely

1322
01:28:55,040 --> 01:28:58,050
reliable winds you need to know the direction very well

1323
01:28:58,060 --> 01:28:59,290
and the wind should be

1324
01:28:59,510 --> 01:29:03,760
no more than something like three or four miles per hour if there stronger

1325
01:29:03,900 --> 01:29:07,780
louis blues CUDA cores that i mentioned earlier

1326
01:29:07,850 --> 01:29:09,460
the sun is behind

1327
01:29:09,470 --> 01:29:13,130
the balloon

1328
01:29:13,150 --> 01:29:17,320
here the bubble is nearly fully inflated now

1329
01:29:18,470 --> 01:29:19,840
still going on

1330
01:29:19,870 --> 01:29:22,580
still going on inflation would be very close

1331
01:29:22,590 --> 01:29:25,170
the end of inflation here the role

1332
01:29:25,190 --> 01:29:26,450
and then

1333
01:29:26,500 --> 01:29:28,020
in this direction here

1334
01:29:28,090 --> 01:29:30,150
five hundred feet or so down is the

1335
01:29:30,220 --> 01:29:36,200
the road which cross

1336
01:29:36,250 --> 01:29:39,420
we know very close to launch was still in alice springs this is one of

1337
01:29:39,420 --> 01:29:43,050
my graduate student jeff mcclintock at the time now

1338
01:29:43,090 --> 01:29:44,470
doctor mcclintock

1339
01:29:44,480 --> 01:29:47,680
you see radar reflectors which allows us to follow the

1340
01:29:47,680 --> 01:29:52,720
assume that's already been done so we're gonna look for the argmax of W

1341
01:29:52,720 --> 01:29:57,240
of this quantity here this outer product so this is the primal representation it's

1342
01:29:57,240 --> 01:30:03,300
equivalent to maximizing this Raleigh quotient which is just the ratio of that quantity divided

1343
01:30:03,300 --> 01:30:12,660
by the norm squared and this actually just picks the first principal component that W

1344
01:30:12,660 --> 01:30:17,170
what you would then do if you wanted to go for the second was orthogonalize

1345
01:30:17,180 --> 01:30:21,740
the data to that direction and look for now again for this maximum

1346
01:30:21,740 --> 01:30:25,380
where this direction has somehow been excluded from the data and then you get

1347
01:30:25,380 --> 01:30:32,080
the second and so on so now this actually just putting in Lagrange multipliers

1348
01:30:32,080 --> 01:30:37,480
for optimizing this quotient you end up with this quantity and if you take the

1349
01:30:37,480 --> 01:30:43,320
derivative of this vector W you get this equation which is an eigenvalue equation for the

1350
01:30:43,320 --> 01:30:51,500
covariance matrix X primed X so essentially the eigen the directions are the eigenvectors of

1351
01:30:51,500 --> 01:30:55,800
the covariance matrix and the ones that you want to choose are the ones corresponding to the

1352
01:30:55,800 --> 01:31:03,440
largest eigen values because if you look at this eigen value and compute W

1353
01:31:03,440 --> 01:31:08,700
primed X primed X W actually see is equal to lambda and so lambda measures the

1354
01:31:08,700 --> 01:31:16,180
variance in that particular eigen direction so the eigen values is actually are measuring the quality

1355
01:31:16,180 --> 01:31:21,700
if you like of the projection so clearly you want to pick the top eigen values and

1356
01:31:21,700 --> 01:31:27,300
choose those directions as your directions in which there is most variance and those are

1357
01:31:27,300 --> 01:31:35,340
the corrections you project into okay so so that's the primal principal components a very classical method

1358
01:31:35,340 --> 01:31:44,740
of data reduction of dimension notice that the there' this very nice property that the

1359
01:31:44,740 --> 01:31:50,280
amount of variance that you capture if you pick K eigenvectors are the sum of

1360
01:31:50,280 --> 01:31:56,840
the eigen values first K eigen values the total variance which is also the

1361
01:31:56,840 --> 01:32:03,800
trace of the kernel matrix is the sum of the norm squares

1362
01:32:03,900 --> 01:32:08,640
of the training data and that it is equal to the sum of the eigen

1363
01:32:08,640 --> 01:32:13,540
values so you can see a few eigen values start big and go down you can see

1364
01:32:13,540 --> 01:32:18,600
very clearly what you're losing by truncating at a particular point you're losing

1365
01:32:18,600 --> 01:32:22,580
the sum of the residual eigenvalues that you didn't choose and you're getting

1366
01:32:22,580 --> 01:32:27,240
the variance of the sum of the eigenvalues that you did choose so it's a very

1367
01:32:27,240 --> 01:32:33,300
nice way of reading of what's happening so what about kernel PCA okay well we

1368
01:32:33,300 --> 01:32:40,400
want to find a dual representation of those principle eigenvectors that's all there is to

1369
01:32:40,400 --> 01:32:48,180
it it's it's a very simple problem and the the trick that shows us

1370
01:32:48,250 --> 01:32:53,620
that we can do it is the following if we have a an eigen value

1371
01:32:53,620 --> 01:33:00,420
sorry eigenvector eigenvalue pair of that covariance matrix then it turns out

1372
01:33:00,420 --> 01:33:07,440
that if we take X W and lambda the same lamda but just multiply X

1373
01:33:07,530 --> 01:33:12,620
pre-multiply W by X then it actually turns out that that is an eigen

1374
01:33:12,620 --> 01:33:17,850
vector eigen value pair for the kernel matrix so here we are X X primed

1375
01:33:17,850 --> 01:33:23,400
the kernel matrix times X W that's X primed X just by changing the order of

1376
01:33:23,400 --> 01:33:28,940
the multiplication times W which of course is now lambda times W and so we

1377
01:33:28,940 --> 01:33:33,940
actually show that X W times X X primed is lambda times X W so this

1378
01:33:33,940 --> 01:33:40,100
is now and eigenvector of this matrix and vice versa if we take an eigen vector

1379
01:33:40,110 --> 01:33:46,640
which I'm now denoting suspiciously as alpha look and eigen

1380
01:33:46,640 --> 01:33:51,380
value lambda of the kernel matrix then I claim X primed alpha lambda is a

1381
01:33:51,380 --> 01:33:57,270
kernel is an eigen vector and eigen value of this covariance matrix and again

1382
01:33:57,310 --> 01:34:03,460
same computation shows us that very simply X primed X times X primed alpha change

1383
01:34:03,460 --> 01:34:08,080
the order X X primed times alpha that's the kernel matrix times alpha is lambda alpha and so we

1384
01:34:08,080 --> 01:34:15,080
end up with this vector being an eigenvector of the covariance matrix so there's a one-to-one correspondence

1385
01:34:15,080 --> 01:34:23,060
between the eigenvectors of the covariance matrix with non zero lambda and the eigenvectors of

1386
01:34:23,060 --> 01:34:27,520
the kernel matrix with nonzero alpha and indeed we can get between one and the

1387
01:34:27,520 --> 01:34:33,300
other by this multiplication and in particular if we have an Eigen vector of the

1388
01:34:33,300 --> 01:34:40,260
kernel matrix bingo we have a dual representation of the eigenvector of the covariance matrix

1389
01:34:40,260 --> 01:34:45,140
is X primed alpha that looks really familiar that is actually our way of expressing

1390
01:34:45,200 --> 01:34:50,480
a vector in a dual way it's a linear combination of the training data which

1391
01:34:50,480 --> 01:34:58,580
are in this as columns of this transpose matrix X so so here we

1392
01:34:58,580 --> 01:35:02,960
are all we need to know is what the norm of this thing is in

1393
01:35:02,960 --> 01:35:08,500
order to get the normalization right if we assume that we've normalized alpha to

1394
01:35:08,500 --> 01:35:12,640
be one which is the normal way you would if you applied a standard eigen

1395
01:35:12,640 --> 01:35:17,980
value routine to your kernel matrix you get back eigenvectors with norm one so

1396
01:35:17,980 --> 01:35:22,180
what is the norm of this thing well just multiply it out and you you find

1397
01:35:22,180 --> 01:35:25,860
it's lambda and so we just need to normalize by that one over the square

1398
01:35:25,860 --> 01:35:31,640
root of the lambda in order to make sure that this dual vector that we

1399
01:35:31,640 --> 01:35:36,880
sorry that the the vector that we get when we use the dual representation

1400
01:35:36,900 --> 01:35:45,320
is normalized correctly and so this is now the dual representation of the ith eigenvector

1401
01:35:45,320 --> 01:35:52,580
of the covariance matrix and it involves just the eigenvector the kernel matrix and divided

1402
01:35:52,580 --> 01:35:58,040
by one over the square root of the corresponding eigenvalue okay so it's as

1403
01:35:58,040 --> 01:36:05,820
simple as that it's a very very sort of natural thing again because we're measuring variance

1404
01:36:05,820 --> 01:36:11,580
and we're maximizing variances absolutely natural that the weight vectors in the in the in the

1405
01:36:11,980 --> 01:36:17,100
feature space will not have any component orthogonal to the span of the training data

1406
01:36:17,100 --> 01:36:21,400
there's no point in putting in a weight vector with some orthogonal component because you're

1407
01:36:21,400 --> 01:36:27,120
just wasting your varianceon a on a direction that has no no variance at all

1408
01:36:27,120 --> 01:36:28,100
this tube

1409
01:36:28,110 --> 01:36:30,020
and the sound comes out

1410
01:36:30,040 --> 01:36:31,480
on the other side of the two

1411
01:36:31,530 --> 01:36:34,270
and that sound is as of t

1412
01:36:36,120 --> 01:36:39,270
so you have to use the exhibition the puffs of air blow into the two

1413
01:36:39,270 --> 01:36:43,420
minutes of t is the sound the sound of the stu makes both u of

1414
01:36:43,420 --> 01:36:47,040
t and s of functions they live in some function spaces

1415
01:36:47,950 --> 01:36:53,150
let's assume for simplicity that these are actually periodic functions

1416
01:36:53,150 --> 01:36:57,780
so they have fully years series expansions still live little l two

1417
01:36:57,800 --> 01:37:00,230
and let's play the following game

1418
01:37:00,270 --> 01:37:02,140
every day you come to me

1419
01:37:02,150 --> 01:37:04,160
and collect the sound

1420
01:37:04,170 --> 01:37:07,950
that i make for you on that particular day

1421
01:37:08,930 --> 01:37:12,420
you don't know what i'm doing and just generating sounds for you every day you

1422
01:37:12,430 --> 01:37:12,890
come to me

1423
01:37:13,360 --> 01:37:14,170
come to me

1424
01:37:14,180 --> 01:37:18,500
and collect the sound the sound we just discussed lives in little tools of the

1425
01:37:18,500 --> 01:37:22,670
sound lives in an infinite dimension space so every day you go back with an

1426
01:37:22,670 --> 01:37:25,570
infinite dimensional data datapoints

1427
01:37:25,770 --> 01:37:27,890
but what i'm doing from day to day

1428
01:37:27,930 --> 01:37:29,970
OK unknown to you

1429
01:37:30,040 --> 01:37:35,360
is i'm actually changing one degree of freedom freedom in the physical system that is

1430
01:37:35,360 --> 01:37:37,430
generating the data for you

1431
01:37:37,430 --> 01:37:46,170
what i'm doing is i'm just changing the length l of the steel

1432
01:37:46,220 --> 01:37:48,760
so how do we reason about this

1433
01:37:48,850 --> 01:37:52,890
you would actually try to understand what is the set of all the sounds the

1434
01:37:52,890 --> 01:37:55,020
physical system can generate

1435
01:37:55,090 --> 01:37:58,730
where there is only one degree of freedom in the physical system and to understand

1436
01:37:58,730 --> 01:38:00,990
this you would actually have

1437
01:38:01,090 --> 01:38:03,120
to do something like this

1438
01:38:03,130 --> 01:38:07,260
so the equations of air flow in this deal

1439
01:38:07,320 --> 01:38:10,780
and these are the equations the complications leading to what is called the webster hall

1440
01:38:10,860 --> 01:38:12,920
equation ultimately v

1441
01:38:12,970 --> 01:38:16,730
p of x commodity is the pressure

1442
01:38:16,820 --> 01:38:18,890
as a function of time

1443
01:38:18,910 --> 01:38:20,900
t is time

1444
01:38:20,930 --> 01:38:23,270
x is location

1445
01:38:23,300 --> 01:38:26,510
in in this in the horizontal axis

1446
01:38:26,600 --> 01:38:29,450
so in other words sft

1447
01:38:29,500 --> 01:38:36,260
the sound is tube generate this still makes is really the pressure wave

1448
01:38:36,300 --> 01:38:41,180
had when x is equal to l

1449
01:38:42,920 --> 01:38:44,000
u of

1450
01:38:44,070 --> 01:38:45,790
is the pressure wave

1451
01:38:45,800 --> 01:38:53,640
when x is equal to zero at the other end of the two

1452
01:38:53,640 --> 01:38:55,080
so sort of this

1453
01:38:55,090 --> 01:38:59,100
and you get a nice closed form on so far how of is related to

1454
01:38:59,100 --> 01:39:00,780
s of

1455
01:39:00,830 --> 01:39:04,180
and if you have t looks like this it's a periodic function has a four-year

1456
01:39:04,200 --> 01:39:08,440
series expansion it lives in a little to the fourier coefficients of the alpha ends

1457
01:39:08,770 --> 01:39:14,740
so every sound you represent by computing its fourier coefficients and then it's the point

1458
01:39:14,760 --> 01:39:16,640
in this for years space

1459
01:39:17,840 --> 01:39:23,420
and because of the fourier coefficients of the s is and what i've plotted is

1460
01:39:23,420 --> 01:39:28,620
the set of all sounds in the space spanned by beta one

1461
01:39:28,630 --> 01:39:32,090
b three and b just seven for particular simulation

1462
01:39:33,050 --> 01:39:36,190
so basically the set of all sounds

1463
01:39:36,710 --> 01:39:42,070
generated by this deal with just one degree of freedom would lie on

1464
01:39:42,300 --> 01:39:44,380
a one-dimensional curve

1465
01:39:45,400 --> 01:39:46,800
in little l two

1466
01:39:46,820 --> 01:39:48,130
and this picture

1467
01:39:48,140 --> 01:39:50,860
looks more or less exactly like the picture

1468
01:39:50,880 --> 01:39:52,690
that i made out

1469
01:39:52,720 --> 01:39:58,370
a few slides ago

1470
01:40:01,000 --> 01:40:03,930
you might think even this is artificial when does

1471
01:40:03,940 --> 01:40:04,860
you know when we

1472
01:40:04,860 --> 01:40:08,150
in the situation where analyzing just single tubes

1473
01:40:08,160 --> 01:40:11,820
but actually there is a long tradition and some of you who

1474
01:40:11,860 --> 01:40:18,170
come from BT's in acoustics and its application to speech analysis would be aware of

1475
01:40:18,820 --> 01:40:26,710
there's a long tradition in modelling the book the set of speech sounds as those

1476
01:40:28,400 --> 01:40:31,410
by a non uniform tube

1477
01:40:31,420 --> 01:40:33,230
the vocal tract

1478
01:40:33,250 --> 01:40:38,200
as this nonuniform two was excited by puffs of air

1479
01:40:38,290 --> 01:40:40,200
that you blow into this tube

1480
01:40:40,210 --> 01:40:41,820
by moving along

1481
01:40:41,820 --> 01:40:43,470
and that's how you speak

1482
01:40:43,490 --> 01:40:47,700
and this is essentially the acoustic model of speech production which is the classical model

1483
01:40:47,720 --> 01:40:48,930
developed in the

1484
01:40:48,930 --> 01:40:51,040
the fifties and sixties and seventies and

1485
01:40:51,050 --> 01:40:54,550
sort of elaborated over the last thirty or forty years

1486
01:40:54,600 --> 01:40:57,700
and so if you take this view

1487
01:40:57,720 --> 01:41:02,820
he would say that look at all the speech sounds that are being generated generated

1488
01:41:03,320 --> 01:41:05,350
biophysical systems

1489
01:41:05,390 --> 01:41:09,280
and the degrees of freedom in the physical system are not that many

1490
01:41:09,300 --> 01:41:13,200
you cannot move your vocal tract in arbitrary ways

1491
01:41:13,200 --> 01:41:16,010
you cannot generate any sound

1492
01:41:16,070 --> 01:41:18,420
any desired sound by

1493
01:41:18,420 --> 01:41:23,300
moving vocal tract you can only generate the set of speech sounds that are naturally

1494
01:41:23,300 --> 01:41:28,200
generated by vocal tract so it's a physical system with few degrees of freedom and

1495
01:41:28,200 --> 01:41:31,110
the set of all speech sounds with therefore

1496
01:41:32,340 --> 01:41:35,170
in some low dimensional subset

1497
01:41:35,220 --> 01:41:36,760
of the ambient space

1498
01:41:36,780 --> 01:41:38,260
of all signals

1499
01:41:38,260 --> 01:41:43,570
in which this is embedded

1500
01:41:43,870 --> 01:41:49,900
so that's an example from acoustic so let me give an example from images

1501
01:41:49,950 --> 01:41:52,110
so what's an image

1502
01:41:52,130 --> 01:41:55,470
an image more or less is something like this

1503
01:41:55,530 --> 01:41:58,010
a function from our tool to zero one

1504
01:41:58,200 --> 01:41:59,570
well any x y

1505
01:42:00,670 --> 01:42:02,220
f of x y

1506
01:42:02,240 --> 01:42:03,320
is just the

1507
01:42:03,360 --> 01:42:06,970
intensity of that image at that location

1508
01:42:08,280 --> 01:42:10,650
so every image is therefore

1509
01:42:10,670 --> 01:42:12,650
lives trivially

1510
01:42:12,670 --> 01:42:18,590
in some function space again just like acoustic signals live trivially in some function space

1511
01:42:18,630 --> 01:42:23,280
and now i wanted to consider the following class of images is very simple

1512
01:42:23,300 --> 01:42:26,720
class of images almost trivial class of images

1513
01:42:26,720 --> 01:42:30,950
these are images of of vertical bar on the translation

1514
01:42:30,970 --> 01:42:34,200
these are pictures of vertical bars

1515
01:42:36,450 --> 01:42:37,800
as i said

1516
01:42:37,860 --> 01:42:40,820
this is the set script if

1517
01:42:40,820 --> 01:42:44,170
so every element of this set script f

1518
01:42:44,190 --> 01:42:48,650
it is a particular function that corresponds to a particular image of vertical bar

1519
01:42:48,690 --> 01:42:53,380
and the way the set is generated is by taking a particular vertical bar which

1520
01:42:53,380 --> 01:42:54,550
we call the

1521
01:42:54,570 --> 01:43:00,500
that's the image of a particular vertical bar and keep translating this vertical bars so

1522
01:43:00,500 --> 01:43:02,000
water molecules

1523
01:43:02,010 --> 01:43:05,330
in a solution of water of all things

1524
01:43:05,370 --> 01:43:09,340
and the fact of the matter is if let's say we grow one hydrogen

1525
01:43:09,360 --> 01:43:14,520
one water molecule down here and one water water molecule down here

1526
01:43:14,540 --> 01:43:16,880
what will happen is that this

1527
01:43:16,930 --> 01:43:20,950
oxygen atom over here by virtue of its electronegativity

1528
01:43:21,110 --> 01:43:22,360
will have a certain

1529
01:43:24,150 --> 01:43:26,820
appalling this hydrogen atom

1530
01:43:26,840 --> 01:43:32,180
taught itself and in fact what actually happens in real life whatever that is at

1531
01:43:32,190 --> 01:43:33,420
the molecular level

1532
01:43:33,430 --> 01:43:37,210
is it this hydrogen atom may actually be bouncing back and forth between these two

1533
01:43:37,210 --> 01:43:42,240
oxygen is it may be rapidly interchange between them

1534
01:43:42,290 --> 01:43:43,580
this interchange

1535
01:43:43,600 --> 01:43:48,210
cause a strong association between two neighbouring water molecules

1536
01:43:48,230 --> 01:43:53,210
and indeed represents the reason why water does not vaporize at room temperature because the

1537
01:43:53,210 --> 01:43:58,670
water molecules have a strong affinity or or nobility from one another

1538
01:43:58,680 --> 01:44:02,730
and therefore just take some illustrations of the book

1539
01:44:02,780 --> 01:44:07,450
this is the way it illustrated in in the book

1540
01:44:07,580 --> 01:44:10,760
probably good to have

1541
01:44:10,970 --> 01:44:16,470
a screen down and here you can see the way the water molecules are actually

1542
01:44:17,670 --> 01:44:22,150
it in water this is the the lower illustration here

1543
01:44:22,160 --> 01:44:25,710
just to indicate to you that the hydrogen atoms are not really the

1544
01:44:25,720 --> 01:44:26,820
the possession

1545
01:44:26,830 --> 01:44:32,260
the ownership of one molecule of water that is constantly being exchanged back and forth

1546
01:44:32,270 --> 01:44:35,800
and this back-and-forth exchange the sharing of the hydrogen atom

1547
01:44:35,820 --> 01:44:39,800
it is what enables the hydrogen bond over roughly five killer calories

1548
01:44:43,820 --> 01:44:47,360
of energy per per mole

1549
01:44:47,370 --> 01:44:50,960
to hold things together by killer calories

1550
01:44:50,970 --> 01:44:52,210
it's not much

1551
01:44:52,230 --> 01:44:55,960
it's only one order of magnitude above point six rather than being two orders of

1552
01:44:55,960 --> 01:45:00,900
magnitude and therefore if one raises the temperature to the level of boiling one the

1553
01:45:00,900 --> 01:45:05,490
temperature is high enough the thermal energy is high enough to rip apart these kinds

1554
01:45:05,490 --> 01:45:07,040
of associations

1555
01:45:07,170 --> 01:45:10,780
now if we were to go back here to look at this carbonyl atom

1556
01:45:10,790 --> 01:45:13,700
we would find the following sort of situation

1557
01:45:13,720 --> 01:45:18,610
here we have this unequal sharing of electoral positive electronegative

1558
01:45:18,620 --> 01:45:25,570
o bonds let's put in the city group like this this is a carboxylic acid

1559
01:45:25,570 --> 01:45:26,450
right here

1560
01:45:26,470 --> 01:45:28,280
here we see two

1561
01:45:29,040 --> 01:45:34,000
a carbon bound to hydroxyl here and by the this oxygen atom

1562
01:45:34,050 --> 01:45:37,100
here once again we have an electronegative atom

1563
01:45:37,140 --> 01:45:39,970
and in fact if we talk about an ionized

1564
01:45:40,330 --> 01:45:46,620
as it normally in the absence of ionisation there will be a net zero charge

1565
01:45:46,620 --> 01:45:48,040
right here

1566
01:45:48,080 --> 01:45:51,960
but at neutral ph it may well be the case

1567
01:45:51,980 --> 01:45:56,450
that the association for various reasons between this oxygen in this hydrogen

1568
01:45:56,460 --> 01:45:57,270
i will

1569
01:45:57,360 --> 01:46:02,130
will allow the hydrogen or or rather the protons the nucleus of the hydrogen atom

1570
01:46:02,130 --> 01:46:03,560
to just wander away

1571
01:46:03,580 --> 01:46:08,330
and therefore we can imagine there could be a net negative charge here

1572
01:46:08,380 --> 01:46:09,300
a whole

1573
01:46:09,320 --> 01:46:14,120
this is has one full electron electrical negative charge and the charge of one electron

1574
01:46:14,200 --> 01:46:19,430
and this proton will have ionized will have left the carboxylic group

1575
01:46:19,440 --> 01:46:23,970
in which it originated and now we have an ionized to citigroup group

1576
01:46:24,020 --> 01:46:30,520
either before or even after this ionisation there's a strong affinity of the carboxyl group

1577
01:46:30,580 --> 01:46:34,650
with the water around it because let's let's look about what happened

1578
01:46:34,660 --> 01:46:37,720
let's look at what happened before the innovation occurred

1579
01:46:37,770 --> 01:46:41,800
this carboxy carbamoyl here is strong unicom electronegative

1580
01:46:41,810 --> 01:46:46,580
and therefore it will participate in hydrogen bonding to the water

1581
01:46:46,590 --> 01:46:48,380
solvent here

1582
01:46:48,390 --> 01:46:53,170
i e this proton will be shared between the oxygen of the water molecules and

1583
01:46:53,170 --> 01:46:55,990
the oxygen right here

1584
01:46:56,040 --> 01:47:00,850
similarly here this oxygen will be slightly next electronegative this for the reasons i just

1585
01:47:00,850 --> 01:47:05,890
described in here once again there may be some weak hydrogen bonding going on

1586
01:47:05,940 --> 01:47:09,880
although not as effective as over here where we have a double bond we have

1587
01:47:09,880 --> 01:47:13,280
a lot of concentration of a cloud of

1588
01:47:13,290 --> 01:47:16,930
electrons are pulled towards the oxygen atom

1589
01:47:16,940 --> 01:47:19,030
and this begins to give us clues

1590
01:47:19,050 --> 01:47:26,050
as to why certain molecules are solvable soluble in water and others are insoluble

1591
01:47:26,100 --> 01:47:31,870
for example if we look at aliphatic compounds let's look at a compound that structured

1592
01:47:31,870 --> 01:47:33,540
like this

1593
01:47:38,010 --> 01:47:38,710
i guess

1594
01:47:38,730 --> 01:47:40,370
most people would call this

1595
01:47:40,380 --> 01:47:43,260
and we can

1596
01:47:43,280 --> 01:47:44,490
called that

1597
01:47:44,510 --> 01:47:51,450
and this has no electronegativity are positively by virtually equal affinities of these two of

1598
01:47:51,450 --> 01:47:54,660
these two kinds of atoms that is the hydrocarbons

1599
01:47:54,710 --> 01:47:56,140
four electrons

1600
01:47:56,150 --> 01:47:59,130
and as a consequence

1601
01:47:59,150 --> 01:48:03,810
this will not be able to form any hydrogen bonds with the solvent around if

1602
01:48:03,810 --> 01:48:06,400
the solvent happens to be water

1603
01:48:07,040 --> 01:48:11,080
there's there's not good bonding here

1604
01:48:11,100 --> 01:48:15,980
and this will in fact also if one puts this in in a solution of

1605
01:48:15,980 --> 01:48:19,720
water this will cause all the water molecules to line up in a certain way

1606
01:48:21,070 --> 01:48:23,110
almost quasi crystals

1607
01:48:24,910 --> 01:48:27,330
the aliphatic molecules

1608
01:48:27,380 --> 01:48:32,910
will be ordered certainly around the elephant molecule without being able to form any strong

1609
01:48:32,910 --> 01:48:34,610
hydrogen bonds with them

1610
01:48:34,620 --> 01:48:39,940
and this ordering represents a loss of chaos a loss of

1611
01:48:41,530 --> 01:48:47,400
entropy is chaos it's disorder it's what happens let's say

1612
01:48:47,440 --> 01:48:52,410
nine ten fifty five and we'll leave the room the sudden order becomes

1613
01:48:52,420 --> 01:48:55,580
chaotic and here before

1614
01:48:55,590 --> 01:49:01,420
before this lining up occurred the water molecules were chaotically arranged throughout the solvent after

1615
01:49:01,420 --> 01:49:05,230
this lining up occurred there was a lot of entropy there was a lot of

1616
01:49:06,160 --> 01:49:08,220
and thermodynamics tells us

1617
01:49:08,230 --> 01:49:12,070
generally the ordering of molecules is disfavored

1618
01:49:12,110 --> 01:49:16,050
and consequently we now have two reasons why this molecule doesn't like to be in

1619
01:49:16,050 --> 01:49:19,720
the midst of water first of all it's unable to form hydrogen bonds with the

1620
01:49:20,640 --> 01:49:21,770
and second of all

1621
01:49:21,780 --> 01:49:26,460
there's a decrease in the entropy in the chaos that occurs when this molecule directly

1622
01:49:26,460 --> 01:49:29,870
i'm not used to being half an hour behind before you can start by all

1623
01:49:29,910 --> 01:49:34,520
try and try and get slightly back on schedule so

1624
01:49:34,530 --> 01:49:37,920
it's my job to give you an introduction to

1625
01:49:37,930 --> 01:49:39,780
the whole idea of machine learning

1626
01:49:39,790 --> 01:49:45,990
and and then to try and introduce one of the many different important theoretical

1627
01:49:46,010 --> 01:49:50,480
foundations of machine learning which is the probabilistic approach so

1628
01:49:51,420 --> 01:49:54,240
i think is this microphone just for the camera

1629
01:49:54,250 --> 01:49:57,270
OK so you use two microphones

1630
01:49:59,690 --> 01:50:05,510
thanks that's so i want to emphasise four-star that the probabilistic approach is one of

1631
01:50:05,540 --> 01:50:06,360
the core

1632
01:50:06,380 --> 01:50:10,140
the ideas in machine learning but it's not the only approach in your life or

1633
01:50:10,140 --> 01:50:11,420
during the rest of

1634
01:50:11,520 --> 01:50:16,550
the lectures about other approaches but whether or not use probabilistic approach in any particular

1635
01:50:16,550 --> 01:50:21,040
problem it's important to note that because it's very influential in how our ideas about

1636
01:50:21,080 --> 01:50:25,430
how you think it is often an important technique you should be using in your

1637
01:50:25,430 --> 01:50:27,380
problem is not the only ones

1638
01:50:27,390 --> 01:50:33,250
i want to start by showing you a video if i can find

1639
01:50:34,480 --> 01:50:37,360
and then be you

1640
01:50:37,380 --> 01:50:39,380
thinking about

1641
01:50:42,210 --> 01:50:43,930
although structure is

1642
01:50:43,990 --> 01:50:45,660
this is

1643
01:50:45,810 --> 01:50:50,690
produced by of you was microsoft research in seattle

1644
01:50:52,530 --> 01:50:55,510
it's easy if you look at the video for humans to figure out what what's

1645
01:50:55,510 --> 01:51:01,170
happening in this video right what's happening is the computers box around the base that

1646
01:51:02,690 --> 01:51:08,030
and this video is impressive for several reasons one is that you can actually run

1647
01:51:08,030 --> 01:51:12,480
this system in real time on laptop so it's not likely to be

1648
01:51:12,610 --> 01:51:16,220
and they computed on for six days and made this video actually

1649
01:51:16,420 --> 01:51:21,740
and the other thing that's impressive although you could argue that because system is busy

1650
01:51:21,740 --> 01:51:24,280
during analysis each frame

1651
01:51:24,300 --> 01:51:28,520
it's not taking advantage of the fact that french sequential circuits a single

1652
01:51:28,570 --> 01:51:30,390
performance would actually be

1653
01:51:30,400 --> 01:51:34,140
so the question that i want to try and get you to think about is

1654
01:51:34,140 --> 01:51:36,920
if you had to write computer program like this

1655
01:51:36,930 --> 01:51:39,140
let's assume don't know about village on

1656
01:51:39,280 --> 01:51:43,050
face detectors about how would you go about doing it

1657
01:51:43,060 --> 01:51:44,900
OK that's really

1658
01:51:44,910 --> 01:51:47,160
kind of core problem

1659
01:51:47,170 --> 01:51:53,550
machine is a task which is relatively well defined like detecting faces in images

1660
01:51:53,570 --> 01:51:57,140
and you need to write a programme but it's not really at all clear how

1661
01:51:57,140 --> 01:51:59,420
you should start writing the program so

1662
01:51:59,440 --> 01:52:04,670
the fact that i'm going to discourage you from taking is the approach trying to

1663
01:52:04,670 --> 01:52:07,480
just say well i'm smarter than the world

1664
01:52:07,500 --> 01:52:11,370
i know what this is the face has two eyes nose and mouth and i'm

1665
01:52:11,370 --> 01:52:14,420
just going to write to i detectors was attacked

1666
01:52:14,430 --> 01:52:15,820
detectors going around the money

1667
01:52:15,830 --> 01:52:20,540
image and then i'm going to you know specify my quote exactly what needs to

1668
01:52:20,540 --> 01:52:22,580
be highlighted something around the table

1669
01:52:22,910 --> 01:52:24,740
and that's going to work

1670
01:52:26,000 --> 01:52:30,160
what you should convince yourself before you do do anything in machine learning that is

1671
01:52:30,170 --> 01:52:31,750
never going to work

1672
01:52:31,760 --> 01:52:34,170
the world is too complicated for

1673
01:52:34,830 --> 01:52:36,310
the essential

1674
01:52:36,340 --> 01:52:38,070
approach two

1675
01:52:38,120 --> 01:52:44,150
two building intelligent systems which is codified in machine learning is the idea of using

1676
01:52:44,150 --> 01:52:50,420
examples from the real world to train systems which are then given as one

1677
01:52:51,160 --> 01:52:55,070
for example if you want to train system to do face identification to go cover

1678
01:52:55,140 --> 01:53:00,560
picture like this to named then again hand programming is not really possible here so

1679
01:53:00,560 --> 01:53:04,630
what's the solution the solution is to get the computer to program itself at some

1680
01:53:04,630 --> 01:53:08,320
level by showing examples of the kind of behaviour one

1681
01:53:08,340 --> 01:53:12,130
so in the face problem we would show the computer alot of images and say

1682
01:53:12,130 --> 01:53:14,440
these images have faces

1683
01:53:14,460 --> 01:53:18,450
and then we to computer what images and so these images don't have faces and

1684
01:53:18,450 --> 01:53:24,240
then we trying to automatically from data set learned what is about to face OK

1685
01:53:24,380 --> 01:53:27,550
so this is the learning approach the and i think that

1686
01:53:27,560 --> 01:53:32,190
you know it's very easy to make a very strong argument this approach essentially dominates

1687
01:53:32,190 --> 01:53:34,120
intelligent systems

1688
01:53:34,140 --> 01:53:36,140
in in the modern day

1689
01:53:36,150 --> 01:53:38,930
there were older approach is based on rules

1690
01:53:38,950 --> 01:53:43,260
they were coming back and expert systems and stuff but those have almost universally been

1691
01:53:43,260 --> 01:53:49,400
eclipsed by learning machines in huge range of application so we should think about this

1692
01:53:49,400 --> 01:53:55,520
it's not really that computers programming itself completely really what you're doing is you're writing

1693
01:53:55,520 --> 01:53:56,870
a program

1694
01:53:56,880 --> 01:54:03,000
that has thousands or millions or tens of millions of undefined constants

1695
01:54:03,010 --> 01:54:07,900
so imagine you know those or c programmers imagine you literally wrote program but at

1696
01:54:07,900 --> 01:54:12,000
the top of of the CAE program there were a number of define constant one

1697
01:54:12,020 --> 01:54:13,760
comes their like

1698
01:54:13,800 --> 01:54:18,390
a hundred thousand or million years ago and the only one way of studying those

1699
01:54:18,400 --> 01:54:24,060
constants but cost to control the degree program for setting of the concerts different program

1700
01:54:24,190 --> 01:54:27,900
runs how do you set the concept that's really what the game of machine learning

1701
01:54:27,910 --> 01:54:33,390
about tweaking the real numbers inside a computer program your road to make the computer

1702
01:54:33,390 --> 01:54:38,060
program behave the way you want so geometrically you can think of this as any

1703
01:54:38,060 --> 01:54:44,230
particular program you write with undefined constants defines a space of possible

1704
01:54:44,250 --> 01:54:47,360
executables right if a million parameters in new

1705
01:54:47,910 --> 01:54:51,160
based on the goal machine learning is to put you in the right spot that

1706
01:54:51,160 --> 01:54:56,620
knowledge dimensional space to solve your problem face detection spam email or what it is

1707
01:54:57,500 --> 01:54:59,840
our goal in life is to estimate

1708
01:54:59,900 --> 01:55:06,170
internal parameters of computer program structure is specified by using an existing set

1709
01:55:06,190 --> 01:55:11,220
and that is that somehow captures correct behavior we want

1710
01:55:11,320 --> 01:55:18,520
so you can think of statistical machine learning models as probabilistic databases so

1711
01:55:18,530 --> 01:55:23,690
traditional database technology can really answer any query about items were never look into the

1712
01:55:23,690 --> 01:55:28,960
dataset and the idea of a probabilistic model for statistical models of talk about for

1713
01:55:28,970 --> 01:55:33,080
the next two lectures is that they can build

1714
01:55:33,090 --> 01:55:38,470
probabilistic any query about the about the

1715
01:55:41,090 --> 01:55:43,740
OK so

1716
01:55:43,760 --> 01:55:47,790
the these models also a lot of other advantages they allow you to make decisions

1717
01:55:47,790 --> 01:55:52,980
given partial information by taking into account the uncertainty or an observability of one

1718
01:55:52,990 --> 01:55:58,700
i don't know they count for noisy sensors actuators because what measurements taken in world

1719
01:55:58,700 --> 01:56:01,830
are not accurate for physical reasons

1720
01:56:01,840 --> 01:56:05,200
they can explain things that are part of the model

1721
01:56:05,230 --> 01:56:11,820
explicitly because they allow for certain stochastic behaviour and so

1722
01:56:11,820 --> 01:56:14,620
to get a classification so you can't just go above the

1723
01:56:14,630 --> 01:56:19,040
threshold by small amount above by quite a bit and the the

1724
01:56:19,040 --> 01:56:28,040
amount in the so it's very analogous to the mention of and a light I don't wanna that each other's it gets to the

1725
01:56:28,040 --> 01:56:32,160
legacy and I don't think it's very very instructive um and

1726
01:56:32,160 --> 01:56:40,550
particularly the corresponding of proof that 2 Saleh's so given that you know all the fracturing

1727
01:56:40,560 --> 01:56:44,280
to mention is you want a computer about the demographic function in the

1728
01:56:44,280 --> 01:56:46,540
way that some was lemma computer bound on the growth

1729
01:56:46,540 --> 01:56:53,600
function in terms of the the EC dimension that is in the paper by at long at al long choose a banque

1730
01:56:53,600 --> 01:57:04,860
you have aren't somebody who can be found in and and that it is absolutely problem and that is a really

1731
01:57:04,860 --> 01:57:20,620
very tricky very tricky in the article monitorial and I certainly wouldn't want to you don't have to reproduce or even in the present and so on so I I I don't want to go into

1732
01:57:20,620 --> 01:57:26,220
those but I just want to highlight exist and uh you know there's a machine are if

1733
01:57:26,220 --> 01:57:29,460
you like the bounding fracturing dimensions of various

1734
01:57:29,460 --> 01:57:32,420
function classes you can do it for the new classes and for

1735
01:57:32,420 --> 01:57:35,580
other function all of his theory works in general see

1736
01:57:35,580 --> 01:57:39,760
doesn't just apply to the functions of socio metallic so

1737
01:57:39,760 --> 01:57:45,360
very general results from the full all be bound it is

1738
01:57:45,350 --> 01:57:49,700
somewhat similar to those hours that most some of the major

1739
01:57:49,700 --> 01:57:59,120
global of the of the uh the growth function you get in this I was lemma cases the d you Lord here and nobody if you remember

1740
01:57:59,120 --> 01:58:05,380
that in the of alone italicus you get died most where so there's a an

1741
01:58:05,380 --> 01:58:08,460
extra log factor comes in the but you know a lot of work to

1742
01:58:08,450 --> 01:58:22,800
between learning there is used in lot of problem on so that it it all work through e 1 but I think I uh but I would like to indicate that this is

1743
01:58:22,790 --> 01:58:32,370
a more direct route and I think it's quite a nice a little of the idea that you can use to get covering numbers for

1744
01:58:32,580 --> 01:58:35,540
linear functions and I going to restrict down to looking at

1745
01:58:35,540 --> 01:58:40,840
the case of linear functions that says the answer and I'm going to look at how we can actually bound those

1746
01:58:40,850 --> 01:58:45,310
covering numbers for linear functions in a direct way of

1747
01:58:45,420 --> 01:58:47,800
this will be the last thing that we have to talk about and

1748
01:58:47,800 --> 01:58:53,300
actually I'm going to uh up suggest you might look at the details of this is an

1749
01:58:53,300 --> 01:59:01,260
exercise overnights and will have some reward points around so I'll talk about that at the end but it's a

1750
01:59:01,260 --> 01:59:05,440
difficult time in the of 1st are indicated you know at a

1751
01:59:05,440 --> 01:59:08,820
sort of the brush programmable how I think this mean how

1752
01:59:08,820 --> 01:59:14,140
this should be done on uh and uh uh that you know that in the book and the

1753
01:59:14,140 --> 01:59:19,760
details of what the hell you purchases and so on so we doing this week and

1754
01:59:19,760 --> 01:59:25,560
the this is a problem of up essentially remember 1 it's

1755
01:59:25,570 --> 01:59:29,480
trying to we have some function as a more going to try and

1756
01:59:29,480 --> 01:59:38,250
make sure that we're going to find a elements in the cover that is how this similar performance to add on the training set so on all

1757
01:59:38,260 --> 01:59:42,140
the training points its outputs should be willing gamma of

1758
01:59:42,140 --> 01:59:48,840
the output of the function of the start of on the training set on the trial make sure that this function comes from some

1759
01:59:48,840 --> 01:59:54,440
simple small said that a couple of things so that we don't

1760
01:59:54,440 --> 01:59:58,340
think of it is not converting that problem of finding that

1761
01:59:58,350 --> 02:00:06,640
approximately function to learning problem a classification problems that on and we're going to do

1762
02:00:06,630 --> 02:00:19,400
that by effectively making in each of those approximation so for each training point we can think of book are which is the output of the functions that I got I want to make sure that the output of the functioning my

1763
02:00:19,400 --> 02:00:23,060
cover is going to be with a gun and Denmark over of that

1764
02:00:23,060 --> 02:00:27,280
value so I get out So imagine putting a threshold or a sort

1765
02:00:27,280 --> 02:00:31,120
of a type of a universal threshold act gamma and I'm going

1766
02:00:31,120 --> 02:00:36,960
to say that this a function is correct shall correctly classified it is on

1767
02:00:36,960 --> 02:00:39,820
the right side of the threshold and you don't think that

1768
02:00:39,840 --> 02:00:43,860
making that in to a classification problems by effectively

1769
02:00:43,860 --> 02:00:48,860
adding 1 extra dimension in which is the output value of

1770
02:00:48,860 --> 02:00:52,960
the function of that so you can make it to the

1771
02:00:52,950 --> 02:01:00,020
classification function in fact it has 2 in the training points all if you go and you're trying

1772
02:01:00,040 --> 02:01:05,600
to cover and points because you you have to have 1 and classification problems was

1773
02:01:05,600 --> 02:01:09,460
saying I want to be a no warbling gamma above the output

1774
02:01:09,460 --> 02:01:22,840
value of the function and given and I need another 1 was saying I'm not woman gamma below soberer to sort of classification is that you have to make it but you can help by doing this extra to mention convert that been to

1775
02:01:22,960 --> 02:01:32,620
essentially a a yes no classification answer and furthermore of the actual weight vector that you start

1776
02:01:32,620 --> 02:01:35,540
it where the actual functioning started with satisfies

1777
02:01:35,540 --> 02:01:45,020
there's constraints clearly because its output actually is buried in the centre of that it's not you know it's it's it's the value that was not gamma bubble gum Belloso it's going in the

1778
02:01:45,020 --> 02:01:53,620
centre and it has a large in which is gonna paying so we have a situation group of a learning problem

1779
02:01:54,720 --> 02:01:57,560
and we've got a solution which has a large Indiana on that

1780
02:01:57,560 --> 02:02:10,020
many problems that thing know we should we would do apply the perceptron algorithm to find a classifier to solve that learning problem out I

1781
02:02:10,220 --> 02:02:16,120
know who is not familiar with the perceptron algorithm in 1

1782
02:02:16,120 --> 02:02:23,050
more the of the of the of the project we so should spends a

1783
02:02:23,020 --> 02:02:26,220
very brief it's a very simple algorithm and all just

1784
02:02:26,220 --> 02:02:29,440
which features are important for your problem

1785
02:02:29,450 --> 02:02:30,320
and so

1786
02:02:30,320 --> 02:02:33,540
in the remainder of the talk and going to bridge should tell you

1787
02:02:33,570 --> 02:02:40,120
what of the tools to discover is called the relationship

1788
02:02:40,150 --> 02:02:45,020
let me first define what i mean by a local feature relevance

1789
02:02:45,030 --> 02:02:47,750
shares a rather complex graph itself

1790
02:02:47,770 --> 02:02:52,110
when the allies completely but like to hear the sounds of all the things that

1791
02:02:52,110 --> 02:02:56,190
can happen is relatively elaborate in some cases

1792
02:02:56,290 --> 02:03:01,530
imagine that what you want to predict is like cancer

1793
02:03:01,530 --> 02:03:06,270
and there are some variables that might be causing lung cancer

1794
02:03:06,310 --> 02:03:11,680
for example smoking some genetic factors and some variables that may be a consequence of

1795
02:03:11,860 --> 02:03:16,830
cancer like of alive having test is or some other

1796
02:03:16,860 --> 02:03:22,750
symptoms that people some sometimes call you know biomarkers

1797
02:03:22,780 --> 02:03:27,280
now there be indicted causes like anxiety might be causing smoking

1798
02:03:27,290 --> 02:03:31,100
and they might be also consequences of causes

1799
02:03:31,100 --> 02:03:36,220
that are called in the jargon confounding factors like other cancers

1800
02:03:36,230 --> 02:03:38,480
there may be hidden variables

1801
02:03:38,530 --> 02:03:43,280
for example it may be that like tar lines that are

1802
02:03:43,330 --> 02:03:46,060
more that causes lung smoking

1803
02:03:46,070 --> 02:03:49,360
lung cancer but unfortunately you have access to them

1804
02:03:49,440 --> 02:03:51,190
then maybe also

1805
02:03:51,190 --> 02:03:54,350
hidden confounders so for example

1806
02:03:54,360 --> 02:03:58,740
imagine that has been you know a recurrent theme in the tobacco settlement problem in

1807
02:03:58,750 --> 02:04:00,190
the united states

1808
02:04:00,240 --> 02:04:06,610
there has been an by now a lot of health policies not to smoke in

1809
02:04:06,610 --> 02:04:13,110
public places and restrictions on selling cigarettes and tobacco companies have been heavily complaining that

1810
02:04:13,110 --> 02:04:17,640
there is no real evidence that smoking causes lung cancer that only correlation has been

1811
02:04:17,640 --> 02:04:23,480
observed and that so far there hasn't been any real prove that there is no

1812
02:04:23,480 --> 02:04:25,070
genetic factor

1813
02:04:25,070 --> 02:04:28,640
that would both cause cramping smoking

1814
02:04:28,650 --> 02:04:31,360
and cancer

1815
02:04:32,080 --> 02:04:35,180
imagining that there is such a genetic factor

1816
02:04:35,190 --> 02:04:42,620
then the school out the hypothesis that smoking causes lung cancer

1817
02:04:45,020 --> 02:04:47,360
there are other interesting things happening

1818
02:04:47,560 --> 02:04:53,720
imagine that you would like to predict what counts on the basis of coffee

1819
02:04:53,740 --> 02:04:59,020
indeed the coughing might be a predictor cancer but imagine that you have another cause

1820
02:04:59,020 --> 02:05:02,110
of casting molecular biology

1821
02:05:02,110 --> 02:05:06,410
if you don't know about our cheap then coffee becomes very poor predictor of lung

1822
02:05:06,410 --> 02:05:10,410
cancer for example if you are during the hay fever season

1823
02:05:10,440 --> 02:05:17,910
and becomes virtually useless to monitor coughing to predict cancer

1824
02:05:17,940 --> 02:05:21,360
and you know i have shown you other cases you know is the story of

1825
02:05:21,360 --> 02:05:27,160
the systematic knowledge this is the example i gave you with the baseline problem

1826
02:05:27,190 --> 02:05:31,610
all in all you have you know all these different cases here

1827
02:05:31,620 --> 02:05:35,570
and people working on causal future discovery

1828
02:05:35,610 --> 02:05:40,290
of playing this game of trying to find those variables that are closest to the

1829
02:05:41,620 --> 02:05:43,960
in the sense that this shield

1830
02:05:43,980 --> 02:05:48,860
the target from all the other variables and that's what people call the markov blanket

1831
02:05:48,870 --> 02:05:52,360
the markov blanket is the set of variables that are now in the

1832
02:05:52,370 --> 02:05:54,610
o shaded area

1833
02:05:54,690 --> 02:05:56,240
such that when you

1834
02:05:56,320 --> 02:05:58,940
you consider these variables then

1835
02:05:58,960 --> 02:06:02,440
the variables outside the shaded area become independent

1836
02:06:02,450 --> 02:06:04,940
of the target

1837
02:06:04,940 --> 02:06:08,530
and as you can see this is not as simple as it may look

1838
02:06:08,620 --> 02:06:15,020
at least as the people working markov blanket telling because if you have a hidden

1839
02:06:15,020 --> 02:06:15,950
variables or

1840
02:06:16,360 --> 02:06:20,200
those may be the ones that are really in the markov blanket

1841
02:06:20,240 --> 02:06:24,310
and in particular in the case of systematic knowledge is

1842
02:06:24,360 --> 02:06:28,110
the nice maybe the thing that is really in the markov blanket but you don't

1843
02:06:28,120 --> 02:06:29,070
know about it

1844
02:06:29,080 --> 02:06:32,770
and you may be thinking that you know certain biomarker

1845
02:06:32,770 --> 02:06:37,480
is relevant for example this sort position in my mass spectra you may think it's

1846
02:06:37,480 --> 02:06:40,870
a well it's relevant but in reality it's not relevant

1847
02:06:40,900 --> 02:06:44,820
the nice was what's was what

1848
02:06:46,410 --> 02:06:49,410
nevertheless i think these tools of

1849
02:06:49,560 --> 02:06:53,540
of causal discovery of very useful because they allow you to in some of the

1850
02:06:53,540 --> 02:07:00,950
arts instead of just detecting correlations you can detect causality

1851
02:07:00,980 --> 02:07:05,160
and that might become very important particularly if you want to

1852
02:07:05,190 --> 02:07:06,360
the grass

1853
02:07:06,370 --> 02:07:08,030
between random variables

1854
02:07:08,030 --> 02:07:12,060
we're dependencies are presented by edges and

1855
02:07:12,070 --> 02:07:20,030
the graph allows you to compute various distributions between subsets of variables

1856
02:07:20,070 --> 02:07:27,860
what is given is the probability of a given variable given its parents and from

1857
02:07:28,570 --> 02:07:32,570
you can compute the joint probability of all the variables and all sorts of

1858
02:07:32,580 --> 02:07:38,930
marginals that are using subsets of the virus

1859
02:07:38,940 --> 02:07:43,310
the fraction in the original vision entries have no meaning but in causal bayesian networks

1860
02:07:43,310 --> 02:07:46,280
they indicate causality

1861
02:07:46,310 --> 02:07:49,280
so here's an example of causal discovery algorithm

1862
02:07:49,290 --> 02:07:53,230
bias to send clear more and giving this example

1863
02:07:53,230 --> 02:07:57,060
not hoping that he will memorize it immediately but to show you that it's not

1864
02:07:57,060 --> 02:07:58,570
that complicated

1865
02:07:59,790 --> 02:08:03,280
let's consider a b and c

1866
02:08:03,320 --> 02:08:06,780
random variables that belong to the set x

1867
02:08:06,790 --> 02:08:13,560
and be a subset of x initialize with a fully connected un oriented graph representing

1868
02:08:13,560 --> 02:08:17,570
the causal relationships between the variables

1869
02:08:17,570 --> 02:08:21,150
then find an oriented edges

1870
02:08:21,180 --> 02:08:27,490
by using the criterion that variable a shares the dark ages bartleby

1871
02:08:27,520 --> 02:08:30,230
if and only if no subset

1872
02:08:30,240 --> 02:08:34,370
of all the variables we can render them conditionally independent so this is what is

1873
02:08:34,370 --> 02:08:37,520
written a conditionally independent of b

1874
02:08:37,520 --> 02:08:38,290
given the

1875
02:08:38,320 --> 02:08:44,190
so all these algorithms for detecting causal dependencies rely heavily upon

1876
02:08:44,240 --> 02:08:50,070
tests of conditional independence between variables

1877
02:08:50,070 --> 02:08:56,220
and then the orient edges in so-called colliders of triplets of collider is

1878
02:08:56,790 --> 02:08:59,490
is that when two arrows

1879
02:08:59,490 --> 02:09:00,950
o point

1880
02:09:01,030 --> 02:09:03,560
to a given variable

1881
02:09:04,640 --> 02:09:07,180
they collide on to c

1882
02:09:07,200 --> 02:09:10,910
so your ages and this is colored using the criterion that if there is a

1883
02:09:10,910 --> 02:09:13,810
dark age between a and c

1884
02:09:13,820 --> 02:09:15,460
and between c and b

1885
02:09:15,490 --> 02:09:17,240
but not between a and b

1886
02:09:18,060 --> 02:09:19,120
you have this

1887
02:09:19,140 --> 02:09:21,330
the pattern of connections

1888
02:09:21,360 --> 02:09:26,460
if and only if there is no such containing c such that independent given given

1889
02:09:26,660 --> 02:09:30,490
OK so just to say that you need to know to do another another test

1890
02:09:30,490 --> 02:09:32,650
of conditional independence to determine

1891
02:09:33,400 --> 02:09:38,730
and then there are some heuristic to further orient edges

1892
02:09:40,110 --> 02:09:42,560
these algorithms are

1893
02:09:43,480 --> 02:09:48,750
expensive computationally and also statistically so they are very data hungry and take a lot

1894
02:09:48,750 --> 02:09:49,600
of time

1895
02:09:49,640 --> 02:09:54,990
so usually people resort to simplify them

1896
02:09:55,020 --> 02:09:59,350
and compromised for example by

1897
02:09:59,440 --> 02:10:06,150
abandoning the estimation of the full graph but caring only about those links that between

1898
02:10:06,150 --> 02:10:08,270
the target variable and the

1899
02:10:08,310 --> 02:10:10,080
other variables

1900
02:10:10,140 --> 02:10:14,690
or abandoning the idea of fully oriented to orient in the graph or anything only

1901
02:10:14,860 --> 02:10:18,860
a subset of the edges

1902
02:10:19,620 --> 02:10:22,530
i'll be talking to the political

1903
02:10:22,570 --> 02:10:25,610
algorithm for discovering the markov blankets

1904
02:10:25,610 --> 02:10:28,940
which is the case in which you care only about the given target

1905
02:10:28,940 --> 02:10:32,030
and you want to find those variable that shield

1906
02:10:32,250 --> 02:10:37,770
the target from all the other variables

1907
02:10:37,820 --> 02:10:42,540
so what you do first is to identify the the parents and children

1908
02:10:42,560 --> 02:10:44,980
in the following way

1909
02:10:45,000 --> 02:10:47,240
that's the first iteration you add

1910
02:10:47,280 --> 02:10:50,310
and even though let's call it a

1911
02:10:50,460 --> 02:10:54,740
the second iteration you add another one that's colin b

1912
02:10:54,770 --> 02:10:57,110
and then the third iteration

1913
02:10:57,160 --> 02:11:02,570
you will be looking at conditional independence so if a is conditionally independent of y

1914
02:11:02,570 --> 02:11:04,140
given b

1915
02:11:04,190 --> 02:11:07,330
then remove then you don't need a

1916
02:11:07,350 --> 02:11:08,910
and keep only

