1
00:00:00,000 --> 00:00:04,990
subset of variables so that we can kind of propagate

2
00:00:05,850 --> 00:00:10,630
one one message in one way and propagate message in another way too

3
00:00:10,650 --> 00:00:12,740
to marginalize one one of the

4
00:00:12,750 --> 00:00:17,490
one of the element of a group of of of elements in the graph group

5
00:00:17,490 --> 00:00:18,960
of nodes in the graph

6
00:00:18,970 --> 00:00:23,100
OK i'm not going to have to go further into debt into the data was

7
00:00:23,100 --> 00:00:28,070
just to give you a taste of what is it

8
00:00:30,990 --> 00:00:32,050
it was also

9
00:00:32,070 --> 00:00:33,920
so it was simple but

10
00:00:33,940 --> 00:00:36,370
in fact we haven't yet

11
00:00:36,450 --> 00:00:38,920
talked about what's

12
00:00:39,020 --> 00:00:41,770
we have this nodes which are really

13
00:00:44,790 --> 00:00:50,740
model of a full of of probabilities what's happened when now we we we

14
00:00:51,500 --> 00:00:53,390
but true

15
00:00:53,400 --> 00:00:57,640
actually probabilities of these isn't and or in the conditional links

16
00:00:57,660 --> 00:00:59,840
nineteen two

17
00:01:00,130 --> 00:01:05,950
depending on what kind of probability distribution you choose

18
00:01:06,230 --> 00:01:09,920
the exact inference is not always possible

19
00:01:10,090 --> 00:01:14,500
so in this case there are severe

20
00:01:17,270 --> 00:01:19,240
so i approx

21
00:01:19,260 --> 00:01:23,130
we get approximation for inference

22
00:01:23,150 --> 00:01:25,470
most most

23
00:01:25,480 --> 00:01:27,970
common well versed emitters

24
00:01:27,990 --> 00:01:34,130
in which instead of of taking this particular one you will take one was less

25
00:01:35,010 --> 00:01:43,240
and various other parameters which we make the

26
00:01:43,250 --> 00:01:48,300
the inference to be lower or upper bound of that

27
00:01:48,320 --> 00:01:50,300
the more that you actually want

28
00:01:51,310 --> 00:01:56,660
and or you can use sampling method for inferring the

29
00:01:58,060 --> 00:02:06,100
approximating the value of one probability of one event in the your change here

30
00:02:06,170 --> 00:02:08,980
new graph

31
00:02:09,790 --> 00:02:11,490
so let's say that now

32
00:02:11,670 --> 00:02:14,320
what we want to do is to

33
00:02:14,350 --> 00:02:15,330
to learn

34
00:02:15,350 --> 00:02:16,390
the the

35
00:02:16,400 --> 00:02:17,820
parameters of the

36
00:02:17,850 --> 00:02:19,790
conditional probabilities

37
00:02:19,810 --> 00:02:21,570
and in the graph

38
00:02:21,590 --> 00:02:27,800
so we have a couple of examples which come which are generated from from which

39
00:02:27,980 --> 00:02:30,210
we assume are generated from

40
00:02:30,630 --> 00:02:32,160
these graphs

41
00:02:32,200 --> 00:02:35,140
in this graphical models

42
00:02:35,190 --> 00:02:40,170
OK if if all the random variables in a graph

43
00:02:40,180 --> 00:02:43,900
our of nineteen for example in the markov chain

44
00:02:43,910 --> 00:02:49,000
we see have examples of of sequences of states

45
00:02:51,440 --> 00:02:57,550
we can simply use maximum likelihood estimates for estimating the parameters

46
00:02:57,570 --> 00:03:03,420
which in the case of the chain will be only county count and taking the

47
00:03:03,420 --> 00:03:05,630
frequencies of of its

48
00:03:06,570 --> 00:03:12,440
when on some of the random variables are observed and the other

49
00:03:12,490 --> 00:03:13,570
latin or

50
00:03:14,260 --> 00:03:16,130
so i called him

51
00:03:17,380 --> 00:03:18,530
we will do

52
00:03:18,550 --> 00:03:21,500
it's the station maximisation

53
00:03:21,580 --> 00:03:22,820
or also

54
00:03:22,840 --> 00:03:29,040
when when inference a in the graph it's not it's not

55
00:03:30,190 --> 00:03:33,030
very similar the

56
00:03:36,040 --> 00:03:37,980
i'm going to two

57
00:03:38,000 --> 00:03:39,390
to talk about

58
00:03:40,000 --> 00:03:42,050
true graph

59
00:03:42,150 --> 00:03:44,360
so graphic and others

60
00:03:46,050 --> 00:03:50,230
traffic analysis and particularly

61
00:03:50,240 --> 00:03:51,930
what happened when you

62
00:03:52,610 --> 00:03:54,500
actually to use

63
00:03:54,500 --> 00:03:56,400
conditional probabilities

64
00:03:56,410 --> 00:03:59,510
in the in the graph

65
00:03:59,570 --> 00:04:01,810
so for that i'm going to do

66
00:04:01,990 --> 00:04:03,650
to prevent the true problems

67
00:04:03,650 --> 00:04:04,300
which is

68
00:04:04,320 --> 00:04:10,200
it was the one of my is it is the text representation and three different

69
00:04:10,200 --> 00:04:11,610
mothers that are

70
00:04:11,630 --> 00:04:16,110
use for decision for solving this problem

71
00:04:16,760 --> 00:04:18,500
so what is it

72
00:04:18,510 --> 00:04:26,460
text representation in which sitting the need for representing text right so usually really in

73
00:04:26,500 --> 00:04:29,630
text processing

74
00:04:29,680 --> 00:04:37,290
automatic text processing applications were given database of n documents which after some

75
00:04:37,320 --> 00:04:40,240
so we have this and the command

76
00:04:40,290 --> 00:04:43,790
we do some preprocessing like

77
00:04:43,840 --> 00:04:46,750
OK for those flaws stemming stopping

78
00:04:46,760 --> 00:04:50,230
and we extract from this database also

79
00:04:50,230 --> 00:04:51,760
dictionary of

80
00:04:51,780 --> 00:04:53,700
and if we

81
00:04:53,740 --> 00:04:56,550
that appear in this and the command

82
00:04:57,770 --> 00:05:00,970
we would like to find a good representation of

83
00:05:00,970 --> 00:05:03,690
document words

84
00:05:04,410 --> 00:05:08,020
four four for example

85
00:05:08,090 --> 00:05:10,000
document retrieval

86
00:05:10,200 --> 00:05:12,830
text categorisation text filtering

87
00:05:12,850 --> 00:05:15,400
text sentiment there's a lot of

88
00:05:17,340 --> 00:05:20,330
text processing tasks

89
00:05:21,600 --> 00:05:23,790
so the usual way of of

90
00:05:27,090 --> 00:05:29,320
the relation

91
00:05:29,380 --> 00:05:34,700
word document is by the is using the vector space model

92
00:05:34,710 --> 00:05:35,450
in which

93
00:05:35,460 --> 00:05:38,460
document is represented as a bag of words

94
00:05:38,500 --> 00:05:42,700
which is the vector of the size of the dictionary

95
00:05:42,710 --> 00:05:45,790
was a lot of zero and

96
00:05:47,470 --> 00:05:49,340
turn you

97
00:05:49,360 --> 00:05:52,730
are a function of the frequency of the

98
00:05:52,740 --> 00:05:55,550
word number and in the dictionary

99
00:05:55,550 --> 00:05:57,970
publishers was equivalent

100
00:05:58,030 --> 00:06:03,200
your application offline in and online and what is actually make it work in the

101
00:06:03,200 --> 00:06:04,340
state action

102
00:06:04,350 --> 00:06:06,530
what are the incentive

103
00:06:06,550 --> 00:06:10,880
to have people participate what makes a community grow what makes community that

104
00:06:11,010 --> 00:06:13,190
all of these fundamental questions

105
00:06:13,210 --> 00:06:15,580
we don't know the the scientific

106
00:06:15,590 --> 00:06:18,320
methodology to even know how to approach

107
00:06:18,340 --> 00:06:20,350
how do these things involved in the

108
00:06:20,360 --> 00:06:22,600
this section like you yesterday from

109
00:06:22,610 --> 00:06:24,790
jon kleinberg

110
00:06:24,810 --> 00:06:29,790
examples of you know tend to study the structure of these networks and trying to

111
00:06:29,830 --> 00:06:31,030
get a feel for what

112
00:06:35,080 --> 00:06:38,530
huge activity today big business now

113
00:06:38,720 --> 00:06:41,080
fourteen billion dollars c

114
00:06:45,940 --> 00:06:47,930
i refuse to believe

115
00:06:47,950 --> 00:06:51,740
we would like to interact with the world knowledge is by issuing to point eight

116
00:06:53,090 --> 00:06:57,220
looking at the top ten results of six hundred thousand different

117
00:06:57,220 --> 00:07:01,170
if any of you stands up and says that if i were you know that

118
00:07:01,260 --> 00:07:03,070
would like to

119
00:07:03,110 --> 00:07:04,530
i believe actually removing

120
00:07:04,570 --> 00:07:06,930
very very fundamentally different directions

121
00:07:06,950 --> 00:07:10,950
and the game still wide right over

122
00:07:11,050 --> 00:07:14,050
nobody knows how to do it right

123
00:07:14,410 --> 00:07:21,650
so areas community microeconomics which i want to talk about a lot today but you

124
00:07:21,650 --> 00:07:25,930
know whether you know i to search businesses options business there's a lot of people

125
00:07:26,390 --> 00:07:30,890
was of the underneath there's lots examples in our products well

126
00:07:30,890 --> 00:07:34,910
when we started writing some of these economists

127
00:07:34,970 --> 00:07:37,120
you know they ten product meetings

128
00:07:37,140 --> 00:07:38,240
and then we shot

129
00:07:38,260 --> 00:07:40,260
decisions made by engineers

130
00:07:40,320 --> 00:07:47,840
d profoundly deep economic decisions are made by an engineer doesn't even understand basic economics

131
00:07:50,370 --> 00:07:55,120
actually i could use the example of that one of the one of them

132
00:07:55,120 --> 00:07:59,120
our are stars and yahoo research michael schwarz

133
00:07:59,970 --> 00:08:02,780
comes from harvard berkeley

134
00:08:04,430 --> 00:08:08,490
his early meetings with their personal which is a bit

135
00:08:08,590 --> 00:08:10,550
service on the internet

136
00:08:13,350 --> 00:08:14,840
he said OK

137
00:08:14,850 --> 00:08:16,550
tell me what your problems are

138
00:08:16,640 --> 00:08:19,030
so started talking about the fact that it is

139
00:08:19,200 --> 00:08:22,140
problem is essentially stop right because you have

140
00:08:22,200 --> 00:08:27,950
they have fewer women than men and typically each woman gets proposed by

141
00:08:27,950 --> 00:08:29,280
a lot of men

142
00:08:29,300 --> 00:08:32,200
let's focus on the female side of the

143
00:08:32,220 --> 00:08:36,640
that actually makes experience

144
00:08:36,680 --> 00:08:40,740
but for me is a few minutes because for the you should get a whole

145
00:08:40,740 --> 00:08:44,030
bunch of them is gonna get ignored most of them

146
00:08:44,050 --> 00:08:46,390
she does not which was to pay attention to attacks

147
00:08:47,120 --> 00:08:49,910
it is as i understand the problem

148
00:08:49,930 --> 00:08:51,890
so they all look at him he said

149
00:08:51,890 --> 00:08:54,300
you know scarcity in the system

150
00:08:54,320 --> 00:08:55,390
and they are

151
00:08:55,430 --> 00:08:58,620
essentially blank stares what the house scarce

152
00:08:58,820 --> 00:09:02,280
well is that web concept

153
00:09:02,300 --> 00:09:04,610
and the scarcity means

154
00:09:04,610 --> 00:09:06,740
there's no cost you just posting

155
00:09:06,760 --> 00:09:11,410
you're bit everyone so we actually came up with a very very simple solution to

156
00:09:11,490 --> 00:09:16,530
about it he said what are you introduce something that actually makes a message special

157
00:09:16,550 --> 00:09:17,450
so what

158
00:09:17,470 --> 00:09:18,340
you give

159
00:09:18,370 --> 00:09:21,680
somebody a budget of i don't know ten digital roses

160
00:09:21,680 --> 00:09:23,220
four months

161
00:09:24,490 --> 00:09:28,160
four special messages natasha digital rules to

162
00:09:28,910 --> 00:09:32,550
so what does that mean that means you only get one of these search messages

163
00:09:32,550 --> 00:09:37,280
as you know important very few so have to think hard about what is best

164
00:09:37,300 --> 00:09:38,510
and that's it

165
00:09:38,510 --> 00:09:43,280
we know that at least you can do to put something special message right so

166
00:09:43,300 --> 00:09:46,450
hopefully she would pay more attention to it

167
00:09:46,620 --> 00:09:49,430
the very very simple doesn't cost anything

168
00:09:50,370 --> 00:09:51,990
it's all the virtual

169
00:09:52,010 --> 00:09:54,700
but change dynamically

170
00:09:54,700 --> 00:09:59,220
i can actually think about the very quickly they love actually resulted in some amazing

171
00:09:59,220 --> 00:10:01,470
user feedback

172
00:10:01,490 --> 00:10:06,660
advertising sciences we we wanted to do too much that's so

173
00:10:07,530 --> 00:10:11,840
and for this is two

174
00:10:11,870 --> 00:10:16,950
invent the future of the internet basically by developing the sciences underlying these areas i

175
00:10:16,970 --> 00:10:17,700
talked about

176
00:10:17,800 --> 00:10:20,450
and our philosophy is just a very focused

177
00:10:20,450 --> 00:10:22,950
areas that are not covered by others

178
00:10:22,970 --> 00:10:25,090
new and emerging

179
00:10:25,120 --> 00:10:28,450
but be very even though they have the best people in in the world there

180
00:10:30,370 --> 00:10:34,660
my first job was to hire person was the leading the higher prabhakar raghavan for

181
00:10:34,660 --> 00:10:37,160
that he did an amazing job

182
00:10:37,180 --> 00:10:42,260
started search area hired some of these folks by the way

183
00:10:42,280 --> 00:10:45,930
it's actually very hard to just two random sampling right

184
00:10:45,950 --> 00:10:49,180
the people i left off this list

185
00:10:49,240 --> 00:10:56,220
it just me everyone in fact to today i urge you to

186
00:10:57,680 --> 00:10:58,820
invited the

187
00:11:01,570 --> 00:11:05,930
one of the best research paper award also from research at three fifty PM

188
00:11:06,050 --> 00:11:09,280
by environments which are

189
00:11:09,350 --> 00:11:11,030
in the area of

190
00:11:11,120 --> 00:11:13,300
online communities

191
00:11:13,370 --> 00:11:15,700
got like all

192
00:11:15,720 --> 00:11:17,570
duncan watts martin slater

193
00:11:17,590 --> 00:11:18,700
many others

194
00:11:18,760 --> 00:11:23,090
i mentioned microeconomics person might be from text

195
00:11:23,090 --> 00:11:24,200
i'm sure

196
00:11:28,160 --> 00:11:29,490
i talked about

197
00:11:29,510 --> 00:11:35,510
the blog friendship graph study so i'm going to skip it today cover

198
00:11:35,530 --> 00:11:38,070
but that's an example of essentially

199
00:11:38,090 --> 00:11:40,180
trying to understand

200
00:11:40,240 --> 00:11:41,450
sort of a

201
00:11:42,300 --> 00:11:47,320
online and you heard about that get

202
00:11:47,620 --> 00:11:50,050
but the basic

203
00:11:51,140 --> 00:11:53,470
the observation that they came up

204
00:11:54,120 --> 00:11:55,340
first of all the people

205
00:11:55,350 --> 00:11:57,430
observation by climate himself

206
00:11:57,430 --> 00:11:58,450
is that

207
00:11:58,450 --> 00:12:01,030
surprisingly short pasta

208
00:12:01,050 --> 00:12:04,010
exist in these graphs in these huge

209
00:12:04,030 --> 00:12:05,820
and doubly surprising

210
00:12:05,840 --> 00:12:09,280
there are efficient algorithms for finding the short path so it's not quite so that

211
00:12:09,300 --> 00:12:11,490
you can actually stumbled on them very quickly

212
00:12:11,510 --> 00:12:13,320
and what the reason that price

213
00:12:13,340 --> 00:12:17,760
this is over and over this that notion of distance to be one over number

214
00:12:17,760 --> 00:12:19,780
of people the intuition by

215
00:12:19,820 --> 00:12:20,620
you know

216
00:12:20,640 --> 00:12:22,680
if you live one mile apart

217
00:12:22,700 --> 00:12:23,870
it's like

218
00:12:23,890 --> 00:12:25,720
you probably know anybody intimately

219
00:12:25,740 --> 00:12:29,720
if you are a mile apart place like new york city probably have no idea

220
00:12:29,720 --> 00:12:33,110
who person is is right so the idea is that it's not geography is the

221
00:12:33,340 --> 00:12:35,410
number of the people who are close to you

222
00:12:35,780 --> 00:12:37,140
one of them

223
00:12:37,160 --> 00:12:37,640
and this

224
00:12:37,990 --> 00:12:41,850
and that actually shows that there are small short pass all over the place and

225
00:12:41,850 --> 00:12:45,910
it's very easy to find the right

226
00:12:45,930 --> 00:12:46,680
the to

227
00:12:46,680 --> 00:12:47,910
social search

228
00:12:49,950 --> 00:12:52,450
show something about the wisdom of the crowds

229
00:12:52,550 --> 00:12:56,450
computer vision

230
00:12:56,470 --> 00:12:58,160
very hard problem

231
00:12:59,510 --> 00:13:01,160
get image

232
00:13:02,140 --> 00:13:04,640
segment to understand what it's saying

233
00:13:04,660 --> 00:13:07,820
and come back and say this is what it's about

234
00:13:07,820 --> 00:13:13,660
well done for a long time so mistaken for the talk about this game was

235
00:13:13,660 --> 00:13:15,840
rolled out at CMU

236
00:13:15,850 --> 00:13:18,490
a couple of a couple of years

237
00:13:18,510 --> 00:13:21,340
what is the game

238
00:13:21,340 --> 00:13:22,530
it's again

239
00:13:22,620 --> 00:13:26,140
you're going to get matched with random

240
00:13:26,610 --> 00:13:29,720
but for two minutes only

241
00:13:29,740 --> 00:13:31,050
and you get your images

242
00:13:31,070 --> 00:13:32,090
this was the

243
00:13:32,320 --> 00:13:33,780
getting images

244
00:13:34,850 --> 00:13:36,680
and if you want to see

245
00:13:37,700 --> 00:13:38,870
you move to the next image

246
00:13:38,870 --> 00:13:40,470
you get some points

247
00:13:44,260 --> 00:13:45,620
and you go to school

248
00:13:45,680 --> 00:13:47,200
and your score is published

249
00:13:47,200 --> 00:13:51,220
and the reason the linear programme is exact in this case is the linear programme

250
00:13:51,220 --> 00:13:54,120
that just has these local constraints

251
00:13:54,160 --> 00:13:59,220
you have this local so that the reason it's exact is because trees have this

252
00:13:59,220 --> 00:14:04,200
local consistency implies global consistency property

253
00:14:06,060 --> 00:14:10,350
sort of pictorially what i've shown you with this example is that if you instead

254
00:14:10,390 --> 00:14:14,580
think about only in you now the graph has cycles

255
00:14:14,600 --> 00:14:19,290
and you only impose local constraints that's all that you do

256
00:14:19,370 --> 00:14:24,020
what you're actually doing is you're you're doing a linear programming relaxation of the general

257
00:14:25,350 --> 00:14:28,740
so your answer is not going to be exact anymore and the reason it won't

258
00:14:28,740 --> 00:14:30,640
be exact is because

259
00:14:30,640 --> 00:14:35,310
you should have been optimizing over the smaller set with blue vertices

260
00:14:35,350 --> 00:14:39,810
and what you're doing when you only enforces constraints are somehow relaxing the set to

261
00:14:39,830 --> 00:14:43,560
making it a little bit larger so that means that there are things that fall

262
00:14:43,950 --> 00:14:46,970
outside the the set and that you're interested in

263
00:14:46,990 --> 00:14:48,870
that's what you'd like to optimize

264
00:14:48,910 --> 00:14:52,970
and they fall inside the set l l means locally consistent

265
00:14:53,020 --> 00:14:57,680
and this factor is exactly the set of sort of the marginals anymore there's sort

266
00:14:57,680 --> 00:15:01,890
of pseudomarginals you might call them some people call them pseudo beliefs

267
00:15:01,930 --> 00:15:06,910
that's exactly something that falls in this gap between the exact set in the relaxed

268
00:15:11,770 --> 00:15:22,060
OK so

269
00:15:23,020 --> 00:15:35,520
yes i haven't defined vertex accenture let's let me explain a little bit so the

270
00:15:35,520 --> 00:15:36,450
the vertices

271
00:15:36,490 --> 00:15:40,560
the sets that we're looking at are defined by linear constraints that all the constraints

272
00:15:40,560 --> 00:15:41,790
are linear

273
00:15:41,930 --> 00:15:45,270
there in very high dimension so they're hard to picture that this picture is very

274
00:15:45,270 --> 00:15:50,600
misleading that's partially because i'm about artist is partially because it's just impossible to

275
00:15:50,620 --> 00:15:56,720
you know high dimensional things behave very differently than what low dimensions would suggest but

276
00:15:56,720 --> 00:16:01,020
what sort of important when whenever polytopes you cut set by human set by making

277
00:16:01,020 --> 00:16:08,040
cuts with half halfspaces that's what we're doing you make extreme points of vertices

278
00:16:08,080 --> 00:16:12,390
now when you solve linear programme it has very nice geometry i'm not sure if

279
00:16:12,620 --> 00:16:16,270
even show this to but essentially linear programme says

280
00:16:16,290 --> 00:16:18,350
you've got a certain direction in space

281
00:16:18,370 --> 00:16:23,450
that that data is specified by a compatibility function so whatever your data

282
00:16:23,560 --> 00:16:24,700
whenever you have

283
00:16:24,700 --> 00:16:27,450
the village model to have a set of status

284
00:16:27,470 --> 00:16:31,160
and it says walk in the direction stade as far as you can and try

285
00:16:31,160 --> 00:16:32,200
and make that

286
00:16:32,260 --> 00:16:35,680
goes far in that direction and maximize this inner product

287
00:16:35,700 --> 00:16:38,410
so you have a direction state and what you want to do is walk in

288
00:16:38,410 --> 00:16:41,020
that direction as far as you can

289
00:16:41,060 --> 00:16:43,830
so if you think about that walking procedure

290
00:16:43,890 --> 00:16:46,770
if you walk you're always going to be able to

291
00:16:46,790 --> 00:16:49,830
hit a maximum one of these vertices

292
00:16:49,830 --> 00:16:53,370
so that's an important fact about linear programs they always obtain

293
00:16:53,390 --> 00:16:57,930
their optimum might be able to attain that many vertices more than one but you

294
00:16:57,930 --> 00:17:02,810
can always find at least one vertex one extreme point where you get the maximum

295
00:17:02,830 --> 00:17:05,040
so what are these blue things

296
00:17:05,120 --> 00:17:11,700
these blue things are exactly the probability distributions of the marginals that correspond to delta

297
00:17:14,640 --> 00:17:19,790
if i took my original distribution i put all of its mass at one point

298
00:17:19,830 --> 00:17:22,140
i could compute the marginals

299
00:17:22,160 --> 00:17:24,470
and i would get a set of zeros and ones

300
00:17:24,680 --> 00:17:29,080
right so in this example suppose i took a probability distribution that puts all its

301
00:17:29,080 --> 00:17:32,330
mass on the configuration one one one

302
00:17:32,370 --> 00:17:36,810
if i compute the marginal zone get the vector zero one i would get the

303
00:17:37,770 --> 00:17:39,830
zero zero zero one

304
00:17:39,850 --> 00:17:41,140
zero one

305
00:17:41,310 --> 00:17:42,950
zero zero zero one

306
00:17:42,970 --> 00:17:47,680
you can sort of read off what you get so those are called integral vertices

307
00:17:47,720 --> 00:17:52,810
called integral because everything would be zero or one right it's sort of the delta

308
00:17:52,810 --> 00:17:57,220
function point those are good vertices if you had that vertex and you've solved the

309
00:17:57,220 --> 00:17:59,260
original map problem exactly

310
00:17:59,450 --> 00:18:05,500
when you relax what happens is you introduce other vertices that don't have zeros or

311
00:18:05,500 --> 00:18:10,520
ones they might have in the binary case that have things like as

312
00:18:10,580 --> 00:18:16,350
so the vertex of the polytope a fractional vertex if you make if you take

313
00:18:16,350 --> 00:18:19,600
the extreme limit of this you make these two guys zero

314
00:18:19,600 --> 00:18:22,040
and you make these two guys i have

315
00:18:22,040 --> 00:18:25,350
and you do that on every one of these you take the point four new

316
00:18:25,350 --> 00:18:28,310
pushed up to have point ones down to zero

317
00:18:28,330 --> 00:18:33,140
what that becomes is it becomes a different vertex of the relaxed polytope it becomes

318
00:18:33,140 --> 00:18:34,790
one of these red vertices

319
00:18:34,890 --> 00:18:39,180
right so the problem is that if you choose the bad direction in a certain

320
00:18:39,180 --> 00:18:43,040
direction if you maximize you might hit that red vertex

321
00:18:43,060 --> 00:18:46,220
and if you had the red vertex then you kind of lost you know it

322
00:18:46,220 --> 00:18:51,430
doesn't mean anything it doesn't give you configuration or solution to the original problem

323
00:18:51,490 --> 00:18:57,270
right so that's the sort of geometry of of what a relaxation is doing

324
00:19:03,910 --> 00:19:10,580
so that's a great question so

325
00:19:10,700 --> 00:19:14,720
what i just said so far is let's just do the relaxation that uses the

326
00:19:14,720 --> 00:19:16,810
obvious sort of tree constraints

327
00:19:16,890 --> 00:19:20,760
and he's pointing to the fact that there exist other constraints for instance things are

328
00:19:20,760 --> 00:19:26,260
known to cycle constraints that sort of check how things behave around individual cycles

329
00:19:26,310 --> 00:19:31,160
it depends on the graph where the cycle constraints will tighten it or not

330
00:19:31,220 --> 00:19:35,910
obviously for a single cycle you the cycle constraints then you'd get the exact set

331
00:19:36,700 --> 00:19:49,760
over the cycle constraints are are one way of trying to enumerate the true set

332
00:19:49,770 --> 00:19:54,430
the true said in general is extremely hard to characterise

333
00:19:54,540 --> 00:19:58,060
it if you don't have the tree

334
00:20:03,060 --> 00:20:05,160
the picture should have is that

335
00:20:05,160 --> 00:20:08,390
you have the original set which is very complex

336
00:20:08,520 --> 00:20:12,540
this tree relaxation this our set one relaxation

337
00:20:12,620 --> 00:20:15,040
if you add some of the cycle constraints

338
00:20:15,080 --> 00:20:20,040
what you're doing geometrically is writing extra hyperplanes will start shaving parts of the polytope

339
00:20:20,040 --> 00:20:24,270
office get something that's in between the dark gray thing and the

340
00:20:24,320 --> 00:20:27,540
the tree set which is light grey my picture

341
00:20:27,560 --> 00:20:31,020
so there's actually a whole sort of sequence of relaxations that you can imagine a

342
00:20:31,020 --> 00:20:35,310
lot of people have studied how to sort of titan LP relaxations that's that's one

343
00:20:35,310 --> 00:20:37,680
reason why this perspective is interesting that

344
00:20:37,680 --> 00:20:41,970
to understand the max product or reweighted max product as we'll see is actually doing

345
00:20:41,970 --> 00:20:46,470
an LP relaxation

346
00:20:46,470 --> 00:20:48,740
animalistic way

347
00:20:48,750 --> 00:20:54,120
three input output associations as a shortcut or they can be done through these higher

348
00:20:57,470 --> 00:20:59,070
mechanisms of

349
00:21:01,210 --> 00:21:03,480
let's apply that now

350
00:21:03,500 --> 00:21:05,230
once again to this example

351
00:21:05,240 --> 00:21:10,320
and throw away the directions on the arcs throw away their actions on the arcs

352
00:21:10,320 --> 00:21:13,590
and what you have you have the connection this representation

353
00:21:13,610 --> 00:21:17,570
of the mind

354
00:21:17,700 --> 00:21:24,090
all of our beliefs have conditional form if condition then conclusion or conclusion of conditions

355
00:21:24,480 --> 00:21:26,840
are goal similarly have that forms

356
00:21:26,850 --> 00:21:28,700
the big difference however

357
00:21:28,720 --> 00:21:33,500
if you like is that goals can be expressed in full first order logic

358
00:21:33,520 --> 00:21:37,900
so the conclusion here with the disjunction which gives rise to the search space

359
00:21:37,930 --> 00:21:41,000
of alternatives takes far outside

360
00:21:41,010 --> 00:21:42,370
the realm of

361
00:21:42,380 --> 00:21:46,800
logic programming and takes us into the world of first order logic

362
00:21:47,110 --> 00:21:49,250
expressed in closed form

363
00:21:49,260 --> 00:21:53,080
so here we have connections between conclusions and conditions

364
00:21:53,120 --> 00:21:54,560
and these connections

365
00:21:54,570 --> 00:21:59,350
as you'll see later can be activated in any way we choose there's no reason

366
00:21:59,350 --> 00:22:02,310
to activate them as i've described

367
00:22:02,350 --> 00:22:04,940
first forward from observations

368
00:22:04,950 --> 00:22:07,960
and then finally old work towards actions

369
00:22:07,970 --> 00:22:10,560
we can compile shortcuts

370
00:22:10,570 --> 00:22:17,070
the right heuristics and we can as you'll see later employed any number of alternative

371
00:22:17,080 --> 00:22:21,350
ways of activating the relationships between our beliefs

372
00:22:21,360 --> 00:22:25,180
and our goals

373
00:22:25,220 --> 00:22:27,760
so for the record the syntax of beliefs

374
00:22:27,850 --> 00:22:33,540
in abductive logic programming is extremely simple you all know it probably by now atomic

375
00:22:33,540 --> 00:22:36,810
conclusions and conjunctions of conditions

376
00:22:36,850 --> 00:22:43,040
most of which are are probably atomic others of which may be negative

377
00:22:43,050 --> 00:22:49,180
all variables are universally quantified so we typically do not write universal quantifiers

378
00:22:49,250 --> 00:22:54,510
goals and a LP agents maintenance schools are like production rules and production systems like

379
00:22:54,510 --> 00:22:57,220
plans and BDI agents systems

380
00:22:57,490 --> 00:23:01,120
they generalize first-order logic clauses

381
00:23:01,170 --> 00:23:08,640
they too are universally quantified they look like logic programs except they have disjunctive conclusions

382
00:23:08,650 --> 00:23:16,310
it suffices to restrict these clauses to universally quantified but as it turns out

383
00:23:16,360 --> 00:23:18,460
when the conditions hold

384
00:23:18,470 --> 00:23:23,850
because of the state of the environment it's convenient to have existential quantifiers and the

385
00:23:23,850 --> 00:23:29,010
conclusion you want to find the state of the world in which you're you achieve

386
00:23:29,010 --> 00:23:31,970
your your alternate in

387
00:23:32,000 --> 00:23:33,560
one of your alternative

388
00:23:35,980 --> 00:23:39,800
now this is i think philosophically

389
00:23:42,430 --> 00:23:48,090
which is the minimal model semantics associated with abductive logic programming

390
00:23:48,170 --> 00:23:52,480
the beliefs are logic programs and they don't describe the world as it is but

391
00:23:52,480 --> 00:23:55,900
as the world the world as the agent believes it to be

392
00:23:56,680 --> 00:24:01,430
these are descriptive in nature on the other hand goals also in logic but not

393
00:24:01,500 --> 00:24:05,650
first order logic clausal form described the world as the agent would like it to

394
00:24:07,050 --> 00:24:10,540
there's no modalities associated with this

395
00:24:10,560 --> 00:24:12,990
separation between goals and beliefs

396
00:24:13,000 --> 00:24:19,520
conceptually it's like database integrity constraints the integrity constraints constrain the data

397
00:24:19,550 --> 00:24:22,800
which are like leaves to those which

398
00:24:22,850 --> 00:24:27,090
the database to two so that police conform to them

399
00:24:27,130 --> 00:24:29,930
observations are the only point of contact

400
00:24:31,040 --> 00:24:35,370
that the agent has with the world other than its actions which may or may

401
00:24:35,370 --> 00:24:38,600
not achieve their intended effects

402
00:24:38,620 --> 00:24:42,490
so the agent's task is to generate a set of actions and assumptions about the

403
00:24:42,490 --> 00:24:45,170
world so that schools are true

404
00:24:45,180 --> 00:24:47,690
the observations are true

405
00:24:47,730 --> 00:24:51,180
in this world which is made up

406
00:24:51,190 --> 00:24:54,970
by means of its actions by means of its assumptions and by means of its

407
00:24:55,790 --> 00:24:59,050
so it's a made-up world that the agent lives in

408
00:24:59,110 --> 00:25:04,140
the world is mainly of the agent's own creation the only point of contact is

409
00:25:04,140 --> 00:25:06,620
the observations that the world

410
00:25:06,680 --> 00:25:12,230
throws at the agent

411
00:25:12,240 --> 00:25:16,080
so let's look at that same example again and see how the semantics works we

412
00:25:16,080 --> 00:25:20,450
saw how the proof the so-called operational semantics proof theory works

413
00:25:20,460 --> 00:25:21,740
more or less

414
00:25:21,850 --> 00:25:23,310
here's some

415
00:25:23,410 --> 00:25:26,370
the way the semantics works

416
00:25:26,430 --> 00:25:30,000
we have a goal whenever there is an emergency i should deal with it

417
00:25:30,010 --> 00:25:32,160
either alone by myself

418
00:25:32,190 --> 00:25:33,300
or with help

419
00:25:33,310 --> 00:25:35,590
or just run away

420
00:25:35,600 --> 00:25:38,590
we observe for example that is smoke

421
00:25:38,610 --> 00:25:40,350
smoke so what

422
00:25:40,350 --> 00:25:42,640
suppose i believe that

423
00:25:42,660 --> 00:25:46,030
if there is a fire then there's smoke

424
00:25:46,030 --> 00:25:50,030
so suppose i believe that if there is a fire then there is an emergency

425
00:25:50,040 --> 00:25:52,850
so the smoke might be assigned to fire

426
00:25:52,870 --> 00:25:55,650
if it is assigned the fire then there is an emergency

427
00:25:55,660 --> 00:25:58,620
but i have the goal that says whenever there is an emergency i should do

428
00:25:59,640 --> 00:26:00,620
one of

429
00:26:00,630 --> 00:26:04,790
three all alternatives at the level here

430
00:26:04,830 --> 00:26:05,900
so what is

431
00:26:05,920 --> 00:26:09,570
the agent got to do the agent has got to

432
00:26:09,570 --> 00:26:11,340
make this goal truth

433
00:26:11,360 --> 00:26:14,660
the agent has got to make this school true

434
00:26:14,680 --> 00:26:16,230
and it does so

435
00:26:16,270 --> 00:26:21,330
by generating assumptions in this case an assumption that there is a fire

436
00:26:23,240 --> 00:26:26,300
which explains the observation that there's smoke

437
00:26:26,320 --> 00:26:29,030
so this observe this

438
00:26:29,080 --> 00:26:34,580
assumption explains the observation that makes the observation true

439
00:26:34,580 --> 00:26:42,370
now remember that I mentioned before this shattering coefficient in the worst case the function is maximally

440
00:26:42,370 --> 00:26:48,580
rich and will grow exponentially the growth function is logarithm so if we translate this into

441
00:26:48,580 --> 00:26:53,490
growth function language that means the growth function will grow linearly

442
00:26:53,890 --> 00:26:58,850
which means just to remind you briefly because we need it on the next slide which means that

443
00:26:58,850 --> 00:27:03,850
we can generate all loss vectors or in other words we can we can

444
00:27:03,850 --> 00:27:09,970
find points no matter what is M we can find M points such that

445
00:27:09,970 --> 00:27:13,640
by using functions of the learning machine we can generate all those vectors or that

446
00:27:13,740 --> 00:27:21,450
by using functionl learning machine we can separate them in all two to the M possible ways

447
00:27:21,450 --> 00:27:28,910
now there's this surprising result also due to Vapnik and Chervonenkis but also this

448
00:27:28,910 --> 00:27:36,060
has been proven by others by Sauer a little bit later and by Scherlach

449
00:27:36,630 --> 00:27:44,040
so this is sometimes called Sauer's lemma eventhough it was first proven by Vapnik and Chervonenkis

450
00:27:44,040 --> 00:27:48,700
it's a cominatorial lemma about the structure of this growth function

451
00:27:48,720 --> 00:27:53,240
and the surprising thing is that it turns out that the growth function so I told

452
00:27:53,240 --> 00:27:54,390
you before in

453
00:27:54,390 --> 00:27:58,490
the worst possible case or in the case where the function class has maximum richness

454
00:27:58,490 --> 00:28:01,720
the growth function grows linearly

455
00:28:01,740 --> 00:28:09,060
even in the limit so for all M but it turns out that if this doesn't happen

456
00:28:09,060 --> 00:28:15,130
then we have this nice upper bou this nice logarithmic upper bound so there's nothing

457
00:28:15,130 --> 00:28:17,120
in between linear and

458
00:28:17,140 --> 00:28:21,910
logarithmic it can only take one of these two behaviours so either it has full richness

459
00:28:22,140 --> 00:28:26,260
or this function grows linearly for a while

460
00:28:26,350 --> 00:28:28,850
so after some maximum M

461
00:28:28,890 --> 00:28:34,530
this number is called the VC-dimension and then afterwards suddenly we have this

462
00:28:34,540 --> 00:28:41,370
this very slow growth afterwards so suddenly after that it's only logarithmic in M so it starts

463
00:28:41,870 --> 00:28:47,740
linear in M and then becomes logarithmic so this growth function will typically indicates that

464
00:28:47,760 --> 00:28:51,010
we are interested in of course it indicates where we don't have

465
00:28:51,010 --> 00:28:57,330
maximum complexity because if we have maximum complexity we cannot guarantee generalazation so we are interest in this second case

466
00:28:57,740 --> 00:29:02,620
and the surprising thing is that in this second case the growth function grows

467
00:29:02,620 --> 00:29:05,330
linearly up to certain point and then suddenly

468
00:29:05,330 --> 00:29:09,900
we have this logarithmic upper bound so then the complexity of

469
00:29:09,900 --> 00:29:13,120
the function class grows very slowly suddenly

470
00:29:13,200 --> 00:29:19,830
so that's kind of surprising so up to a certain size of the training set it

471
00:29:19,830 --> 00:29:24,030
looks like the function class is very rich and afterwards it's suddenly small

472
00:29:24,030 --> 00:29:27,990
and that's the regime in which we can generalize

473
00:29:28,100 --> 00:29:30,290
so let me give you an example of this

474
00:29:30,450 --> 00:29:33,390
so-called VC-dimension

475
00:29:34,010 --> 00:29:39,870
so remember I said VC-dimension needs a point up to which the growth function grows linearly so up to

476
00:29:39,870 --> 00:29:46,830
which the complexity of function class is maximum so let's take this function class which is

477
00:29:46,830 --> 00:29:50,640
the which are half spaces in R two

478
00:29:50,720 --> 00:29:53,490
so they separate the space into

479
00:29:53,540 --> 00:29:59,100
an area where the value is a plus one and area where the value is minus one

480
00:29:59,100 --> 00:30:02,370
to specify this last functions we need three parameters

481
00:30:02,370 --> 00:30:08,620
and we can ask the question how rich is this function class so up to which number of

482
00:30:09,370 --> 00:30:18,290
is this maximal average maybe I'll drink a bit while you meditate on this issue so let's let's try it out

483
00:30:18,490 --> 00:30:20,600
let's take a three points

484
00:30:20,680 --> 00:30:23,160
so if we take three points

485
00:30:23,200 --> 00:30:24,950
we put them in general position

486
00:30:24,970 --> 00:30:30,600
now we try to realize different separations of these points so we we can

487
00:30:30,600 --> 00:30:35,850
assign these points to different classes three points we have two times two times

488
00:30:35,850 --> 00:30:38,120
circular orbits

489
00:30:38,210 --> 00:30:39,820
the centre of mass

490
00:30:39,820 --> 00:30:42,670
it means that and one are one

491
00:30:42,690 --> 00:30:45,600
it is and two r two

492
00:30:45,680 --> 00:30:49,620
i don't remember go back to a to one nodes

493
00:30:49,660 --> 00:30:53,340
and now you observing here on earth

494
00:30:53,430 --> 00:30:55,160
optical spectra

495
00:30:55,170 --> 00:30:59,640
you see the absorption lines in doppler shift you see them moving like this

496
00:30:59,690 --> 00:31:01,170
and so you will find

497
00:31:01,180 --> 00:31:03,680
from star number one

498
00:31:03,690 --> 00:31:05,390
sustainable one

499
00:31:05,440 --> 00:31:07,780
you get the period of the orbit

500
00:31:07,840 --> 00:31:12,270
you get it velocity in orbit and you get one just like we got from

501
00:31:12,280 --> 00:31:14,070
the sound is no different

502
00:31:14,080 --> 00:31:16,820
now we have star number two

503
00:31:16,830 --> 00:31:19,510
you get the clearly or it's nice

504
00:31:19,510 --> 00:31:22,040
you can check now to get the right answer

505
00:31:22,050 --> 00:31:23,850
you have the two

506
00:31:23,880 --> 00:31:25,790
and you have two

507
00:31:25,830 --> 00:31:29,340
all of that comes out of the doppler shift measurements

508
00:31:29,360 --> 00:31:31,020
but there is more

509
00:31:31,020 --> 00:31:34,160
you know newton's law of gravity

510
00:31:34,220 --> 00:31:37,440
which leads to capture capra's third law

511
00:31:37,500 --> 00:31:39,960
which is that the squared

512
00:31:40,080 --> 00:31:43,120
equals four pi squared

513
00:31:43,170 --> 00:31:47,170
times are one plays are two to the power three

514
00:31:47,220 --> 00:31:49,680
divided by one class and two

515
00:31:49,870 --> 00:31:58,010
times g which is the gravitational constant

516
00:31:58,080 --> 00:32:01,690
now look

517
00:32:01,750 --> 00:32:05,500
you know what you know are two from the doppler shift measurements so you know

518
00:32:07,650 --> 00:32:11,620
you know the period from the doppler shift measurements so you know this

519
00:32:11,630 --> 00:32:13,180
so what you don't know

520
00:32:13,210 --> 00:32:15,680
is what and one class and two

521
00:32:15,770 --> 00:32:19,280
but you know one or one is and to actually

522
00:32:19,320 --> 00:32:24,820
so we have two equations with two unknowns you find n one and n two

523
00:32:24,820 --> 00:32:27,290
think about this for a minute

524
00:32:27,370 --> 00:32:30,860
out of the doppler shift of this binary system

525
00:32:30,900 --> 00:32:34,320
you get their orbital radii you get their velocities

526
00:32:34,320 --> 00:32:36,510
you get to play with in order to produce

527
00:32:36,520 --> 00:32:42,050
we get the masses of the individual objects

528
00:32:42,060 --> 00:32:45,350
now when you observe from earth

529
00:32:45,410 --> 00:32:49,090
you are probably not in the plane of the orbit notice what i did very

530
00:32:49,090 --> 00:32:52,170
cleverly i put you in the plane of the orbit i said you you which

531
00:32:52,170 --> 00:32:54,880
is the plane of the blackboard

532
00:32:54,930 --> 00:32:58,370
in reality you will probably not be in that plane you will not see the

533
00:32:58,370 --> 00:33:01,670
orbital period and john like this is also and so on

534
00:33:01,760 --> 00:33:04,100
but it will be tilted a little

535
00:33:04,210 --> 00:33:08,010
and if it is still states that the radial velocity which is the only one

536
00:33:08,010 --> 00:33:12,900
you will measure will be lower than the two

537
00:33:12,930 --> 00:33:16,480
in fact you can easily see that supposedly orbitals like this

538
00:33:16,520 --> 00:33:19,820
so you on earth and they go around each other like this

539
00:33:19,880 --> 00:33:21,430
just like this

540
00:33:21,490 --> 00:33:26,090
then there is no radial velocity at all so inclination of the orbit is very

541
00:33:27,260 --> 00:33:30,740
but i will not for to address this today as it doesn't affect the basic

542
00:33:30,740 --> 00:33:35,250
principle behind doppler shift but you can imagine of course it is a key issue

543
00:33:35,260 --> 00:33:37,960
for astronomers to get a handle on the

544
00:33:41,880 --> 00:33:44,240
in our galaxy

545
00:33:44,280 --> 00:33:47,650
there are a few hundred

546
00:33:47,710 --> 00:33:50,770
i low this and get it back up later

547
00:33:50,790 --> 00:33:54,540
there are a few hundred very special binaries

548
00:33:55,840 --> 00:33:57,800
one stories

549
00:33:57,800 --> 00:34:00,510
more or less like the sun

550
00:34:00,520 --> 00:34:02,780
pretty common pretty boring

551
00:34:02,870 --> 00:34:05,180
but there is another one

552
00:34:05,240 --> 00:34:09,870
very small in size and neutron star or black hole you'll see very shortly

553
00:34:09,980 --> 00:34:13,590
which is very close

554
00:34:13,600 --> 00:34:17,330
so it's a very weak of close binary not because they close to us but

555
00:34:17,340 --> 00:34:19,410
the two objects are close together

556
00:34:19,460 --> 00:34:21,760
so this one could be a neutron star

557
00:34:21,840 --> 00:34:24,820
as you will see very shortly black hole

558
00:34:24,850 --> 00:34:28,840
and they go around a common centre of mass saying the blackboard the plane go

559
00:34:28,840 --> 00:34:31,590
around like this

560
00:34:34,610 --> 00:34:37,360
there is the point here between the two

561
00:34:37,370 --> 00:34:42,070
where the gravitational pull in one direction towards this star is the same as the

562
00:34:42,070 --> 00:34:46,410
gravitational pull in this direction has name we call it in the lagrangian point

563
00:34:46,430 --> 00:34:50,320
there is also such a point somewhere between earth and the moon is very close

564
00:34:50,320 --> 00:34:52,780
to the moon but there is such a point

565
00:34:52,830 --> 00:34:55,800
and so if you in a lagrangian point lies here

566
00:34:55,810 --> 00:34:59,220
so that the force in this direction on a test mass

567
00:34:59,250 --> 00:35:03,100
is the same as the forces that direction if it lies on the surface of

568
00:35:03,100 --> 00:35:04,350
that star

569
00:35:04,380 --> 00:35:08,590
then the matter that is on this side here wants to flow towards the report

570
00:35:08,610 --> 00:35:10,650
it's more energetically more favorable

571
00:35:10,660 --> 00:35:11,820
because the force

572
00:35:11,850 --> 00:35:13,860
in this direction is large

573
00:35:13,920 --> 00:35:17,980
now since they go around each other it cannot fall radially and so what will

574
00:35:17,980 --> 00:35:20,570
happen is that this matter from the star

575
00:35:20,630 --> 00:35:22,610
is going to spiral in

576
00:35:22,650 --> 00:35:23,680
for this

577
00:35:23,690 --> 00:35:26,380
and finally finds its way onto neutral state

578
00:35:26,430 --> 00:35:28,380
so it's unsurprising therefore

579
00:35:28,400 --> 00:35:31,210
this star is called the donor

580
00:35:31,250 --> 00:35:32,940
it provides the mass

581
00:35:32,960 --> 00:35:35,840
and this small object

582
00:35:35,880 --> 00:35:37,580
could be a neutron star

583
00:35:37,620 --> 00:35:38,890
it's called

584
00:35:39,150 --> 00:35:40,240
o creator

585
00:35:40,250 --> 00:35:45,800
and this this is called the accretion disk

586
00:35:45,850 --> 00:35:49,380
let's take a closer look at these neutron star

587
00:35:49,430 --> 00:35:52,300
i'm going to make it blow up here

588
00:35:52,340 --> 00:35:55,580
this is a neutron star which has mass and

589
00:35:55,710 --> 00:35:59,080
and it has radius are

590
00:35:59,090 --> 00:36:02,310
and i take a little bit of matter little test mass

591
00:36:02,350 --> 00:36:05,260
from very large distance

592
00:36:05,320 --> 00:36:08,060
and i let it fall onto the neutron star

593
00:36:08,160 --> 00:36:10,500
now the question is was what speed

594
00:36:10,500 --> 00:36:12,990
really to reach the surface of the neutron star

595
00:36:13,000 --> 00:36:16,580
that's an a one question that all of you should be able to do in

596
00:36:16,580 --> 00:36:18,470
no time whatsoever

597
00:36:18,520 --> 00:36:22,390
gravitational potential energy is converted to kinetic energy

598
00:36:22,440 --> 00:36:24,630
and when you object which is here

599
00:36:24,650 --> 00:36:25,890
the amount of

600
00:36:25,900 --> 00:36:28,820
gravitational potential energy that is released

601
00:36:28,870 --> 00:36:31,940
is MG divided by these are

602
00:36:31,990 --> 00:36:33,710
and that's all converted

603
00:36:33,810 --> 00:36:36,090
one half MV squared

604
00:36:36,120 --> 00:36:39,160
this is the speed with which it hits the neutron star

605
00:36:39,210 --> 00:36:43,160
so the little and cancels it doesn't make any difference whether this is a large

606
00:36:43,160 --> 00:36:47,090
mass so small as it always reach the neutron star start with the same speed

607
00:36:47,110 --> 00:36:49,260
and you'll see that speed

608
00:36:49,270 --> 00:36:50,640
is that the square root

609
00:36:50,660 --> 00:36:52,270
of two mg

610
00:36:52,320 --> 00:36:55,580
divided by or

611
00:36:55,640 --> 00:36:56,710
you've seen this

612
00:36:56,860 --> 00:36:58,640
a one

613
00:36:58,660 --> 00:37:03,350
you may also have seen it in terms of what we call escape velocity

614
00:37:03,460 --> 00:37:07,290
if you ask yourself the question if you were on the surface of the neutron

615
00:37:07,290 --> 00:37:12,300
star what is that you need to make it out to infinity

616
00:37:12,350 --> 00:37:16,480
in other words to break away from the gravitational pull of the neutron star that's

617
00:37:16,480 --> 00:37:18,440
exactly the same speed of course

618
00:37:18,490 --> 00:37:21,400
it has to do with the conservation of energy so you could also think of

619
00:37:21,400 --> 00:37:22,640
this as escape

620
00:37:24,940 --> 00:37:27,180
now comes the amazing things

621
00:37:27,190 --> 00:37:28,990
if you take a neutron star

622
00:37:29,020 --> 00:37:30,910
the mass of a neutron star

623
00:37:31,020 --> 00:37:32,330
is very roughly

624
00:37:32,350 --> 00:37:34,820
one and a half times the mass of the sun

625
00:37:34,860 --> 00:37:36,820
this is the symbol for some

626
00:37:36,900 --> 00:37:42,130
and the radius is for neutron star is about ten kilometres

627
00:37:42,210 --> 00:37:45,080
if you substitute those numbers in there

628
00:37:45,130 --> 00:37:50,460
you get the phenomenal speed you get two hundred thousand kilometres per second

629
00:37:50,470 --> 00:37:53,810
which is seventy percent of the speed of light

630
00:37:53,870 --> 00:37:57,820
so whenever antimatter falls onto the neutron star from a large distance

631
00:37:57,830 --> 00:38:02,090
it's the neutron star with seventy percent of the speed of light this is kinetic

632
00:38:02,940 --> 00:38:06,150
it's the surface is all converted to heat

633
00:38:06,210 --> 00:38:07,050
and so

634
00:38:07,060 --> 00:38:11,780
if enough kilograms per second fall onto the neutron star

635
00:38:11,830 --> 00:38:14,990
the surface of the neutron star gets very hot

636
00:38:15,030 --> 00:38:17,970
and we know there are cases where you can get as hot as about ten

637
00:38:17,970 --> 00:38:19,560
million degrees

638
00:38:19,600 --> 00:38:24,930
and the ten million degrees this neutron star will radiate almost all its energy in

639
00:38:24,930 --> 00:38:26,070
the x-rays

640
00:38:26,100 --> 00:38:31,080
very little in the optical almost all in the x-rays and so becomes strong x-ray

641
00:38:33,940 --> 00:38:36,610
make you appreciate

642
00:38:36,620 --> 00:38:42,060
the energy released when something hits neutron star if you take a marshmallow

643
00:38:42,140 --> 00:38:46,240
you throw marshmallow from a large distance on a neutron star when it hits the

644
00:38:48,260 --> 00:38:52,190
the energy that is released that means explosion that is caused by the impact is

645
00:38:53,690 --> 00:38:58,240
through the energy that was released on the atomic bomb was thrown hiroshima

646
00:38:58,280 --> 00:39:00,050
just shortly after

647
00:39:00,120 --> 00:39:02,550
so at the end of the second world war

648
00:39:02,610 --> 00:39:04,800
that enormous amount of energy

649
00:39:04,840 --> 00:39:06,090
because this

650
00:39:06,100 --> 00:39:07,770
incredible explosion

651
00:39:07,770 --> 00:39:11,900
it's comparable to the energy that is released when marshmallow

652
00:39:11,920 --> 00:39:17,970
hits the surface of a neutron star

653
00:39:17,970 --> 00:39:20,470
of where these electrodes or in three-dimensional space

654
00:39:20,510 --> 00:39:23,370
before this it was done with pictures

655
00:39:23,380 --> 00:39:27,700
we're sitting on neurosurgeons remarking photos that were taken with a hand-held camera in the

656
00:39:28,430 --> 00:39:30,310
to try to mark the location of these electrodes

657
00:39:30,820 --> 00:39:31,700
you know i think

658
00:39:31,720 --> 00:39:36,340
in brain surgery when you're trying to localize seizure focus the three most important things

659
00:39:36,500 --> 00:39:38,500
are location location location

660
00:39:38,510 --> 00:39:39,240
right so

661
00:39:39,250 --> 00:39:42,860
i think we have a small improvement here just by having an algorithm to detect

662
00:39:42,860 --> 00:39:47,150
where exactly in the brain we measuring this electricity

663
00:39:47,220 --> 00:39:50,530
and then we went on to do sort of you know fancier color mappings of

664
00:39:50,530 --> 00:39:57,450
power spectra across the brain during different states and seizure states how phase in different

665
00:39:57,450 --> 00:40:02,210
frequency bands is transmitted in this is a screenshot of the graphical user interface applications

666
00:40:02,520 --> 00:40:03,860
written in gtk

667
00:40:03,870 --> 00:40:07,210
for doing this kind of spectral analysis of epilepsy and this is part of the

668
00:40:07,210 --> 00:40:12,370
neural from the neuroimaging sweden and pi is called and i one i PI

669
00:40:12,660 --> 00:40:14,960
so some components that i wrote

670
00:40:16,580 --> 00:40:21,530
the three d component was pretty easy there's really powerful three d library in c

671
00:40:21,530 --> 00:40:24,490
plus plus called BTK that

672
00:40:24,500 --> 00:40:27,590
you know does more with everything you could need to do in three d the

673
00:40:27,590 --> 00:40:32,210
situation for two d graphics was different there were probably thirty or forty two d

674
00:40:32,210 --> 00:40:33,790
graphics library

675
00:40:33,800 --> 00:40:36,780
all of which were good all of which had features

676
00:40:36,830 --> 00:40:39,780
none of which we could do everything that i needed to do

677
00:40:39,800 --> 00:40:42,460
so you know the situation is a little bit like this seems to be in

678
00:40:42,460 --> 00:40:47,200
machine learning everybody does machine learning has a machine learning package everybody

679
00:40:47,250 --> 00:40:51,240
plotting in python have their own plotting package and so i figured i would join

680
00:40:51,240 --> 00:40:54,450
the club rolled up my sleeves and wrote my own

681
00:40:54,480 --> 00:40:56,560
client library

682
00:40:56,580 --> 00:41:00,830
but i had some fairly specific requirements and one of the most important requirements as

683
00:41:00,830 --> 00:41:04,590
i was writing the user interface applications so it is something that can be embedded

684
00:41:04,590 --> 00:41:07,680
into a growing and a lot of the tools

685
00:41:07,700 --> 00:41:09,650
for example the new plant

686
00:41:09,660 --> 00:41:12,900
i don't i hesitate to say can be but i don't think they can be

687
00:41:12,900 --> 00:41:15,240
embedded into google is in a lot of

688
00:41:15,340 --> 00:41:19,810
the plotting solutions had limitations on is something that can be embedded into growing

689
00:41:20,070 --> 00:41:24,590
as well as you know ideally you would like to you can interactively from the

690
00:41:25,350 --> 00:41:29,180
and figures made public location quality plot so those are the kind of three things

691
00:41:29,180 --> 00:41:34,170
i was looking for a user interface applications could scripting good interactive shell used in

692
00:41:34,170 --> 00:41:40,140
public publication quality graphics couldn't find anything that all those components and that's basically what

693
00:41:40,140 --> 00:41:43,220
matplotlib was born

694
00:41:43,240 --> 00:41:47,570
the an interesting story the least funny to me

695
00:41:47,720 --> 00:41:53,400
fernando perez who i python which has the potential of using a lot today

696
00:41:53,570 --> 00:41:57,030
support for new newport nice sentiment patch

697
00:41:58,260 --> 00:42:02,530
i added some features to the ipod interface to point to make it look more

698
00:42:02,530 --> 00:42:03,730
like matlab

699
00:42:03,770 --> 00:42:06,980
and she wrote me back and said you know i'm really busy right now and

700
00:42:06,980 --> 00:42:09,740
that can be able to look at this for six months i can deal with

701
00:42:10,120 --> 00:42:12,400
from my phd so

702
00:42:13,410 --> 00:42:18,490
natalie was born as the patch rejection two two i pi

703
00:42:18,500 --> 00:42:19,630
so this is the first

704
00:42:19,640 --> 00:42:24,050
the picture the of the epilepsy viewing application we roadmap to live showing

705
00:42:24,130 --> 00:42:28,680
you know the time series of EEG on this axis and the time dependent spectra

706
00:42:28,680 --> 00:42:35,000
histogram spectral density plot on this exercise procedures you go starting to seizure activity you

707
00:42:35,000 --> 00:42:37,690
see these high frequency bands start to dominate

708
00:42:37,790 --> 00:42:43,490
no harmonics and then they become very regular in this region procedure properly

709
00:42:47,350 --> 00:42:54,440
so many is a

710
00:42:54,490 --> 00:42:58,490
class library that was designed to be embedded in gtk in in a

711
00:42:58,510 --> 00:43:01,330
user interface application initially was gtk

712
00:43:01,340 --> 00:43:03,960
now is we support other toolkits as well

713
00:43:03,970 --> 00:43:07,720
but for the shell component we know what we want to scripting initial component as

714
00:43:07,720 --> 00:43:10,620
well and i was matlab user so

715
00:43:10,640 --> 00:43:15,270
i decided to write a sort of an interface to the library so basically procedure

716
00:43:15,270 --> 00:43:20,150
interface that was stateful in the way the matlab stay for you know when you

717
00:43:20,150 --> 00:43:25,070
say supply you graph into that and

718
00:43:25,080 --> 00:43:28,830
essentially the same magic going on on under the hood is managing the state of

719
00:43:28,830 --> 00:43:31,020
your process

720
00:43:31,030 --> 00:43:32,080
and so

721
00:43:32,100 --> 00:43:35,420
matlab worked for me is as an environment and i just decided to emulate matt

722
00:43:35,600 --> 00:43:41,710
for procedural interface so we set out to be a matlab clone and the

723
00:43:41,720 --> 00:43:45,230
i don't really strive to be a matlab clone but essentially at the

724
00:43:45,240 --> 00:43:49,780
coarsest layer we provide a lot of the matlab functionality

725
00:43:49,790 --> 00:43:53,950
as you drill deeper and deeper we depart further and further and we try to

726
00:43:53,950 --> 00:43:57,430
do things in the iconic way as opposed to a matlab way

727
00:43:57,510 --> 00:44:01,800
and most of our users appreciate that because people using matlab

728
00:44:01,810 --> 00:44:05,430
i mean should be used by the user because they like the language they like

729
00:44:05,430 --> 00:44:10,150
its features like object oriented programming and they don't find the you know the matlab

730
00:44:10,150 --> 00:44:14,680
language terribly rich but you can see if you're coming from matlab

731
00:44:14,700 --> 00:44:18,720
there's a fairly close correspondence that you know you can learn one or the other

732
00:44:18,740 --> 00:44:20,520
fairly quickly

733
00:44:20,530 --> 00:44:24,170
and we produced in fairly similar outputs

734
00:44:24,230 --> 00:44:30,260
so edward tufte whose vision has worked on the visual display of quantitative information

735
00:44:30,270 --> 00:44:35,070
has this phrase copy the great architectures and

736
00:44:35,070 --> 00:44:36,100
what to do

737
00:44:36,130 --> 00:44:39,570
and this was the general idea of based approaches so

738
00:44:40,080 --> 00:44:47,110
you just search for outliers at the border of the data space but independent of

739
00:44:47,220 --> 00:44:49,050
the data distribution

740
00:44:49,050 --> 00:44:53,680
i think you should have to use the microphone because of the sorry

741
00:44:55,480 --> 00:45:00,350
all that all sorry i know i can repeat it just it just go

742
00:45:00,430 --> 00:45:11,180
that's right that seems to cut so the question was that seems to contradict to

743
00:45:11,180 --> 00:45:16,950
the cause of dimensionality because it's completely right in high dimensional spaces the points tend

744
00:45:16,950 --> 00:45:21,620
to move at the borders of the of the one of the of the data

745
00:45:21,620 --> 00:45:29,780
space usually these that based approaches are only applicable to lower dimensional points not only

746
00:45:29,780 --> 00:45:35,200
because of the model but you will see that these points rely on on convex

747
00:45:35,200 --> 00:45:37,760
hull computation and again

748
00:45:37,780 --> 00:45:45,430
those are the only have a efficient algorithms for convex hull computational some but you're

749
00:45:45,430 --> 00:45:48,390
completely right also from the model

750
00:45:49,070 --> 00:45:51,280
a little bit contradiction to the cause

751
00:45:51,310 --> 00:45:54,200
dimensionality but in low low dimensional spaces

752
00:45:54,970 --> 00:45:56,140
it might work

753
00:45:56,990 --> 00:46:03,970
anyway i'm presenting it trees so i'm neutral about all these approaches but of course

754
00:46:03,970 --> 00:46:06,800
you can ask my personal opinion

755
00:46:06,810 --> 00:46:09,910
i would say this is just for small

756
00:46:09,930 --> 00:46:11,490
four low for low dimensional

757
00:46:14,180 --> 00:46:18,780
so yeah the that's what i said the basic idea is to organize data objects

758
00:46:18,780 --> 00:46:20,570
in convex hull layers

759
00:46:20,580 --> 00:46:23,080
and objects on how to

760
00:46:23,140 --> 00:46:25,470
players are outliers and

761
00:46:25,490 --> 00:46:26,780
on inner layers

762
00:46:26,800 --> 00:46:28,600
are the outliers of the normal

763
00:46:28,620 --> 00:46:33,430
points and obviously basic assumption is that the others are at the border of the

764
00:46:33,430 --> 00:46:37,720
data space and the normal objects are insights in the in the middle of the

765
00:46:37,720 --> 00:46:40,390
day in the centre of the data space

766
00:46:40,410 --> 00:46:43,390
yeah some

767
00:46:43,410 --> 00:46:46,700
things to the model year how are these convex hull layers

768
00:46:46,700 --> 00:46:48,510
find well

769
00:46:48,530 --> 00:46:52,600
you just compute the convex hull of the full data space and all points on

770
00:46:52,600 --> 00:46:57,430
these kind of this convex hull for example take this picture here so this is

771
00:46:57,430 --> 00:46:58,780
the convex hull

772
00:46:58,830 --> 00:47:03,620
for all the objects and all the objects on this on this whole have that

773
00:47:05,490 --> 00:47:06,430
layer one

774
00:47:06,450 --> 00:47:10,800
then you remove these points and compute again the convex hull

775
00:47:10,890 --> 00:47:15,300
which end up ends up in this convex hull here all the points in this

776
00:47:15,310 --> 00:47:20,450
second convex hull have two or layer two and so on and so became

777
00:47:20,490 --> 00:47:23,310
and then you can say OK points having a depth of

778
00:47:23,330 --> 00:47:25,450
at most k

779
00:47:25,470 --> 00:47:28,680
are the outliers and the rest of

780
00:47:28,740 --> 00:47:34,300
so this is the basic here and there are some simple algorithms that

781
00:47:35,720 --> 00:47:37,450
differ in how to

782
00:47:37,450 --> 00:47:40,390
efficiently compute convex hulls

783
00:47:40,410 --> 00:47:44,050
in terms of not too

784
00:47:45,830 --> 00:47:49,260
the data all the times four four four new

785
00:47:49,280 --> 00:47:51,160
convex hull layers

786
00:47:51,180 --> 00:47:53,160
but to do it in one

787
00:47:53,220 --> 00:47:58,120
this can all in one run so here is for example is of that but

788
00:47:58,120 --> 00:48:00,010
also the seed

789
00:48:00,030 --> 00:48:03,280
maybe there are other about but

790
00:48:03,310 --> 00:48:05,140
the some

791
00:48:05,160 --> 00:48:07,030
potentially the most

792
00:48:07,050 --> 00:48:13,850
one of the social discussion the idea is very similar to the classical statistical approaches

793
00:48:14,350 --> 00:48:15,930
for one distribution

794
00:48:15,950 --> 00:48:18,390
one distribution of normal objects

795
00:48:18,410 --> 00:48:22,330
but independent from the from the type of distribution

796
00:48:22,430 --> 00:48:28,390
convex hull computation of course is only efficient dispersants only in

797
00:48:28,430 --> 00:48:30,100
low dimensional spaces

798
00:48:30,120 --> 00:48:34,830
really efficient in low dimensional spaces and well the output

799
00:48:34,850 --> 00:48:37,600
it is usually a label but can easily be

800
00:48:38,720 --> 00:48:44,260
two scoring output for example we take that as as the scoring well you done

801
00:48:44,280 --> 00:48:47,280
you can think of course even

802
00:48:47,300 --> 00:48:51,890
more sophisticated scoring space that can be done very

803
00:48:51,890 --> 00:48:57,050
and obviously the scope resolution is global so to global

804
00:48:57,330 --> 00:48:59,030
approach to my actions

805
00:48:59,080 --> 00:49:01,160
because it considers all

806
00:49:06,890 --> 00:49:14,430
obviously i it that so the question is would it be would remove some normal

807
00:49:14,430 --> 00:49:20,620
objects and obviously yes so again the basic assumption here is outliers are at the

808
00:49:20,620 --> 00:49:23,450
border in liars are in the centre

809
00:49:23,470 --> 00:49:25,570
if this assumption holds then

810
00:49:26,410 --> 00:49:27,410
good work

811
00:49:27,430 --> 00:49:28,370
if not

812
00:49:28,370 --> 00:49:30,930
i strongly doubt

813
00:49:32,030 --> 00:49:35,580
you are completely right

814
00:49:39,890 --> 00:49:44,180
so then we already approach deviation based approaches so

815
00:49:44,620 --> 00:49:48,800
the idea here is that

816
00:49:48,990 --> 00:49:53,200
we go a set of data points of course local group or the global sense

817
00:49:53,220 --> 00:49:55,280
i don't care about it

818
00:49:55,330 --> 00:50:00,490
and then outliers of those points that do not fit to the general current characteristics

819
00:50:00,510 --> 00:50:01,760
of this

820
00:50:01,950 --> 00:50:06,370
reference OK for example brands of the set is minimized when removing

821
00:50:06,390 --> 00:50:08,550
the outliers that would be one

822
00:50:09,760 --> 00:50:12,330
characteristics the variance of the of

823
00:50:12,720 --> 00:50:14,180
this set

824
00:50:14,180 --> 00:50:16,910
and if you remove the out less than the variance

825
00:50:16,970 --> 00:50:18,800
significantly decreased

826
00:50:18,810 --> 00:50:21,220
and this is meant by

827
00:50:21,240 --> 00:50:24,780
the let's do not fit to the to the strength of characters

828
00:50:24,780 --> 00:50:31,120
and the basic assumption again is that the outliers of the outermost points

829
00:50:31,160 --> 00:50:32,490
the dataset

830
00:50:32,490 --> 00:50:34,070
it's taken

831
00:50:34,080 --> 00:50:38,780
a little bit more closer look at the model so usually you are given you

832
00:50:38,780 --> 00:50:41,470
have a smoothing factor

833
00:50:41,530 --> 00:50:45,620
and the smoothing factor just computes for any

834
00:50:45,620 --> 00:50:50,030
from the time you have use a model

835
00:50:50,040 --> 00:50:53,450
and for example for this model here dpfp model

836
00:50:53,460 --> 00:50:54,540
we see that

837
00:50:54,550 --> 00:50:58,090
performance was done by about ten percent

838
00:50:58,140 --> 00:51:01,400
it as you increased from two point two to one

839
00:51:02,540 --> 00:51:05,890
these are models you should tune for four

840
00:51:05,910 --> 00:51:09,350
to get good performance

841
00:51:09,370 --> 00:51:14,440
his comparison in one experiment from one of the original papers which i have the

842
00:51:15,460 --> 00:51:18,370
between vector spaces and l

843
00:51:18,390 --> 00:51:20,020
and you can see

844
00:51:20,050 --> 00:51:24,020
this is a different of recall so this means at the very top of the

845
00:51:24,020 --> 00:51:25,340
retrieved list

846
00:51:25,350 --> 00:51:27,460
the top ranked documents

847
00:51:27,470 --> 00:51:31,260
what happened to them this kind of means

848
00:51:32,880 --> 00:51:34,310
four very

849
00:51:34,350 --> 00:51:37,420
larger prefix of the rank this what happened for it

850
00:51:37,430 --> 00:51:40,300
and so you can see that the top of the range

851
00:51:40,350 --> 00:51:44,360
the improvement of language model compared to vector space

852
00:51:44,380 --> 00:51:47,160
are these but small then

853
00:51:47,200 --> 00:51:49,290
for higher because they

854
00:51:49,300 --> 00:51:51,280
they are quite substantial

855
00:51:51,330 --> 00:51:53,700
and that seems to be the case in general

856
00:51:53,750 --> 00:51:56,150
that language models two

857
00:51:56,160 --> 00:52:01,160
don't hurt for precision but they do much better than recall on vector spaces they

858
00:52:01,160 --> 00:52:07,720
are tuned by

859
00:52:07,730 --> 00:52:09,160
OK summary

860
00:52:10,810 --> 00:52:13,010
the language model approach to IR

861
00:52:13,600 --> 00:52:16,070
view the document as a generative model

862
00:52:16,080 --> 00:52:18,850
that generates the query

863
00:52:18,890 --> 00:52:22,500
we define the precise generative model we want to use

864
00:52:22,500 --> 00:52:26,070
estimate parameters different from parameters for each document model

865
00:52:26,870 --> 00:52:29,050
small to avoid zeros

866
00:52:29,080 --> 00:52:33,170
and then apply to query and find document most likely to to have generated the

867
00:52:34,900 --> 00:52:40,370
present the most likely documents to the user

868
00:52:40,390 --> 00:52:45,310
OK so now a comparison of the

869
00:52:45,330 --> 00:52:48,100
the two probabilistic models and also the vector space model

870
00:52:48,320 --> 00:52:50,940
that have covered today

871
00:52:50,960 --> 00:52:56,150
and actually i also want to compare to naive space because i'm assuming that many

872
00:52:56,150 --> 00:52:58,680
of us who are somewhat familiar with naive bayes

873
00:52:59,430 --> 00:53:02,380
the models i actually name space so i want to

874
00:53:02,500 --> 00:53:08,370
connected back to space so quick recap of what nine based on the model is

875
00:53:08,380 --> 00:53:11,500
in particular the text classification

876
00:53:11,510 --> 00:53:15,540
so let's say i want to use my face for text classification

877
00:53:15,580 --> 00:53:17,690
so text classification you want

878
00:53:17,710 --> 00:53:21,730
if you use naive bayes for that we want to classify document d

879
00:53:21,790 --> 00:53:25,460
human divine assassins

880
00:53:26,040 --> 00:53:29,990
maybe classes like politics economics sports

881
00:53:30,010 --> 00:53:33,300
and you assume that d was generated by

882
00:53:33,320 --> 00:53:36,540
the generative model that corresponds to one of these classes

883
00:53:36,540 --> 00:53:40,880
the question is which of the classes class models which of these class models

884
00:53:40,900 --> 00:53:44,100
is most likely to have generated the document

885
00:53:44,120 --> 00:53:46,580
also which class to have the most evidence

886
00:53:46,600 --> 00:53:49,000
does that equivalent this framework

887
00:53:51,020 --> 00:53:54,820
language model and the language model approach to IR

888
00:53:55,560 --> 00:53:59,590
the object you want to testify is actually the creator is another document

889
00:54:00,400 --> 00:54:03,410
in each document in the collection is a different class so we have a lot

890
00:54:03,410 --> 00:54:07,520
of different classes because each document is the class

891
00:54:08,610 --> 00:54:11,830
we assume that q was generated by the generative model

892
00:54:11,850 --> 00:54:13,970
and the question is which document

893
00:54:13,980 --> 00:54:16,670
is most likely to have generated here

894
00:54:16,770 --> 00:54:20,370
and then offer which document we have the most evidence

895
00:54:20,390 --> 00:54:25,330
so in terms of formal framework is exactly the same but the mapping from

896
00:54:25,340 --> 00:54:28,180
the formal concepts two

897
00:54:28,480 --> 00:54:33,120
the objects in information retrieval system

898
00:54:33,270 --> 00:54:39,310
another important distinction is that two different naive bayes models actually and they correspond to

899
00:54:39,320 --> 00:54:42,320
the two information retrieval models that we've looked at today

900
00:54:42,330 --> 00:54:45,830
one of them is multinomial model

901
00:54:45,850 --> 00:54:50,240
in the multinomial model and simplifying somewhere here but in the multinomial model we generating

902
00:54:50,260 --> 00:54:51,940
tokens one by one

903
00:54:52,720 --> 00:54:57,880
let's say we have to five a class and once we have chosen the class

904
00:54:57,890 --> 00:55:00,110
we generate tokens for this class and

905
00:55:00,130 --> 00:55:05,580
so the first token in the document we generate might be beijing and taipei

906
00:55:05,590 --> 00:55:06,920
and then joined the

907
00:55:06,930 --> 00:55:10,420
she so we generate talking but talking in this model

908
00:55:10,440 --> 00:55:14,050
model is the binary independence model

909
00:55:14,270 --> 00:55:17,350
which corresponds to the naive bayes manually models

910
00:55:17,460 --> 00:55:21,540
and that is what we generate is not trying to tokens buy

911
00:55:21,550 --> 00:55:25,820
we make a decision for each term in the vocabulary the decision is does this

912
00:55:25,820 --> 00:55:27,770
protocol the document yes or no

913
00:55:28,230 --> 00:55:32,170
so we're generating a term incidence vectors

914
00:55:32,230 --> 00:55:32,940
and the

915
00:55:32,960 --> 00:55:36,750
tom indicators and the term incidence vectors so in this case you might choose china

916
00:55:37,440 --> 00:55:43,360
for this class generate document by first making decision does the alaska could this document

917
00:55:43,440 --> 00:55:44,590
and so no

918
00:55:44,600 --> 00:55:46,330
the celebrity apprentice

919
00:55:46,340 --> 00:55:47,990
document that says yes

920
00:55:48,000 --> 00:55:51,330
and so on and so that's how we generate document in the

921
00:55:51,420 --> 00:55:52,120
in the

922
00:55:52,610 --> 00:55:54,340
finally my

923
00:55:54,360 --> 00:55:58,550
and so these two models are exactly the two models that were chosen for the

924
00:55:58,550 --> 00:56:02,870
two competing probabilistic framework here this was chosen for language models

925
00:56:02,960 --> 00:56:06,230
and this was chosen for the binary independence model

926
00:56:06,230 --> 00:56:07,910
so they are they model

927
00:56:07,940 --> 00:56:11,780
documents differently and and the the key difference is that

928
00:56:11,820 --> 00:56:16,070
this is a binary model binary independence model and this is a token based model

929
00:56:16,070 --> 00:56:18,410
that takes term frequency to con

930
00:56:18,460 --> 00:56:22,410
and different numbers tokens

931
00:56:22,430 --> 00:56:24,320
his comparison of the two

932
00:56:24,330 --> 00:56:27,770
i think most of us have already said

933
00:56:27,780 --> 00:56:36,660
and because the bernoulli model does not take token token count call it works best

934
00:56:36,660 --> 00:56:39,380
for short documents that's what i said earlier that

935
00:56:39,430 --> 00:56:40,530
it works

936
00:56:40,570 --> 00:56:45,310
well for titles and abstracts but not for long enough for long documents use

937
00:56:45,320 --> 00:56:47,670
are better off using something like

938
00:56:47,710 --> 00:56:50,050
and i language models

939
00:56:50,080 --> 00:56:55,730
one very nice factoid to bring home the difference between these two models is if

940
00:56:55,730 --> 00:56:57,810
you look at the parameters for the

941
00:56:57,820 --> 00:57:01,850
the parameter for the in in the multinomial model is

942
00:57:01,870 --> 00:57:06,500
what's the probability and going to generate the ad for particular token in the document

943
00:57:06,530 --> 00:57:09,310
and the problem zero one five because in english

944
00:57:09,320 --> 00:57:12,360
about one in twenty words system

945
00:57:12,390 --> 00:57:13,730
and eventually models

946
00:57:13,730 --> 00:57:19,210
how to choose here and that is a very important problem in machine learning is

947
00:57:19,220 --> 00:57:24,290
accidentally or for answering this question

948
00:57:24,330 --> 00:57:28,570
there are two extreme for choosing p and l

949
00:57:28,580 --> 00:57:33,180
OK with the length of the nearby if he is one

950
00:57:33,230 --> 00:57:35,080
and the

951
00:57:36,290 --> 00:57:38,790
one thousand or four and

952
00:57:38,870 --> 00:57:43,110
it is that after what the average means that are rock which

953
00:57:43,120 --> 00:57:44,830
if k is small

954
00:57:44,880 --> 00:57:47,120
and this are a large

955
00:57:47,170 --> 00:57:50,380
then you have many matches

956
00:57:50,420 --> 00:57:54,030
in the body

957
00:57:56,640 --> 00:57:58,410
the variance will be

958
00:58:00,300 --> 00:58:03,020
many similarities in the

959
00:58:03,030 --> 00:58:04,290
in the average

960
00:58:05,460 --> 00:58:06,390
but the

961
00:58:07,120 --> 00:58:09,250
could be large for example

962
00:58:09,260 --> 00:58:15,120
you can imagine that this the market sequence has the memory

963
00:58:15,620 --> 00:58:22,110
longer than k equal to one maybe the court case is is three

964
00:58:22,160 --> 00:58:27,610
there would be a good and is the function of the sample

965
00:58:27,660 --> 00:58:29,350
for the one that the

966
00:58:29,360 --> 00:58:31,650
that you have many matches

967
00:58:31,660 --> 00:58:32,980
the other extreme

968
00:58:34,810 --> 00:58:36,530
that you have few matches

969
00:58:36,710 --> 00:58:38,320
is large

970
00:58:38,370 --> 00:58:42,490
you the quantizer is find xs are small

971
00:58:42,810 --> 00:58:45,990
there there is no but thought

972
00:58:46,050 --> 00:58:49,100
in this case the bias is it more

973
00:58:49,120 --> 00:58:52,060
but the variance is extremely large

974
00:58:52,210 --> 00:58:57,050
the question is

975
00:58:57,100 --> 00:58:59,270
how sell hall could chew

976
00:58:59,280 --> 00:59:01,080
he and al

977
00:59:01,090 --> 00:59:04,930
we depends on the problem the good choices

978
00:59:04,940 --> 00:59:08,130
and the band on on on and

979
00:59:08,180 --> 00:59:10,170
but the sample size

980
00:59:10,220 --> 00:59:15,820
and here comes the machine learning guy i am very grateful who

981
00:59:15,860 --> 00:59:20,130
nicolo and cobbled was she

982
00:59:20,140 --> 00:59:27,150
i i learned this idea from from his book

983
00:59:27,160 --> 00:59:30,960
they do the following imagine that

984
00:59:32,120 --> 00:59:36,670
i somehow these these choices of the and the

985
00:59:36,680 --> 00:59:38,730
among u

986
00:59:39,800 --> 00:59:45,060
here you the first row corresponds to k equal to one

987
00:59:45,070 --> 00:59:46,410
the second row key

988
00:59:46,810 --> 00:59:48,850
but so and so on

989
00:59:48,890 --> 00:59:52,730
and within a here and is equal to one two

990
00:59:52,760 --> 00:59:55,150
three and so on

991
00:59:56,060 --> 01:00:00,010
all of fuel make a portfolio selection

992
01:00:00,020 --> 01:00:04,390
you need that you follow the the algorithm

993
01:00:04,400 --> 01:00:08,500
at each step you calculate your amount of money

994
01:00:08,520 --> 01:00:10,420
starting with one dollar

995
01:00:10,430 --> 01:00:14,410
at the time you and there

996
01:00:14,420 --> 01:00:15,700
and you are my

997
01:00:15,720 --> 01:00:18,090
my place

998
01:00:18,140 --> 01:00:22,820
i see your actions your choice of portfolio

999
01:00:22,830 --> 01:00:29,040
and in each step i see how much money you have if you make an

1000
01:00:29,040 --> 01:00:29,630
on the

1001
01:00:33,410 --> 01:00:37,120
and then

1002
01:00:37,180 --> 01:00:38,660
the question is

1003
01:00:38,710 --> 01:00:42,860
hard to aggregate these portfolio selection

1004
01:00:42,940 --> 01:00:45,400
and i'm actually learning curve

1005
01:00:48,340 --> 01:00:50,360
there are not so many

1006
01:00:50,380 --> 01:00:55,610
coming up with the idea but i don't know what the were the best

1007
01:00:55,660 --> 01:01:00,780
maybe a duty can you if you are interested in

1008
01:01:00,790 --> 01:01:03,970
the aggregation presupposes is the following

1009
01:01:04,020 --> 01:01:07,290
it's again a simple principle

1010
01:01:08,170 --> 01:01:13,440
i aggregate portfolio selection all of your

1011
01:01:13,510 --> 01:01:15,900
a portfolio selection

1012
01:01:15,910 --> 01:01:17,430
such that

1013
01:01:17,790 --> 01:01:24,400
it's a linear combination of your portfolio that or b

1014
01:01:24,450 --> 01:01:27,140
we're in this linear combination

1015
01:01:27,150 --> 01:01:29,640
somebody has all the way

1016
01:01:29,650 --> 01:01:32,570
if on field and minus one

1017
01:01:32,590 --> 01:01:37,200
he has a big amount of money in it

1018
01:01:38,200 --> 01:01:46,090
if somebody is has the less money than in the aggregation this linear combination has

1019
01:01:46,210 --> 01:01:53,860
a he or she has much less weight especially if their way

1020
01:01:54,790 --> 01:02:00,440
it it'll be was but how to make these the weight

1021
01:02:00,490 --> 01:02:01,540
five each

1022
01:02:01,550 --> 01:02:05,400
we make an aggregation

1023
01:02:05,470 --> 01:02:07,600
but this means that the

1024
01:02:07,610 --> 01:02:13,900
combination of experts i advertised the good book maybe that and that our

1025
01:02:13,950 --> 01:02:16,380
and and the

1026
01:02:16,430 --> 01:02:18,610
i is the

1027
01:02:24,450 --> 01:02:25,510
we have

1028
01:02:25,570 --> 01:02:31,270
for any k and we have a sequence of four

1029
01:02:32,480 --> 01:02:34,290
in fact forests

1030
01:02:36,780 --> 01:02:41,880
choose an initial distribution over the expert we have infinitely many experts in in the

1031
01:02:41,880 --> 01:02:45,390
you're and in practice we have finite dimension

1032
01:02:45,420 --> 01:02:52,970
the chosen arbitrary probability distribution or in and out every tree

1033
01:02:52,980 --> 01:02:56,380
it doesn't come

1034
01:02:56,380 --> 01:03:02,680
the word is more important if it appears in less it several times in a

1035
01:03:02,680 --> 01:03:07,790
target document so this is said by this first factor and the second factor was

1036
01:03:07,790 --> 01:03:11,930
saying that the word is more important if it appears in less documents so if

1037
01:03:11,940 --> 01:03:13,120
we have a word

1038
01:03:13,140 --> 01:03:17,130
some words which appear in all possible documents than most likely there are not too

1039
01:03:17,130 --> 01:03:23,130
informative like the would be such a which appears there and certainly is not too

1040
01:03:23,130 --> 01:03:28,290
informative but so we are normally so in this former like this

1041
01:03:28,500 --> 01:03:36,780
which produces one real number physically captures this to concepts and usually have some kind

1042
01:03:36,780 --> 01:03:39,090
of balance between this

1043
01:03:39,140 --> 01:03:44,090
two factors are written in this weight vectors

1044
01:03:44,100 --> 01:03:46,050
so it's called TFIDF

1045
01:03:46,100 --> 01:03:49,990
so this is an example of such as TFIDF vector so if this is the

1046
01:03:51,170 --> 01:03:58,260
then this document would get transport transform in such vector and this relates behind here

1047
01:03:58,270 --> 01:04:01,050
would be TFIDF weights

1048
01:04:01,060 --> 01:04:06,150
this idea came from the information through the

1049
01:04:06,180 --> 01:04:09,380
from the beginning of eighties i think

1050
01:04:09,400 --> 01:04:13,070
another important thing which

1051
01:04:13,080 --> 01:04:20,640
appears everywhere but in text mining also information she will similarity how to measure similarity

1052
01:04:20,640 --> 01:04:22,360
between two

1053
01:04:22,380 --> 01:04:27,920
the document vectors as we said so each document in this vector space model is

1054
01:04:27,920 --> 01:04:31,740
represented as a vector of weights

1055
01:04:31,940 --> 01:04:33,070
and now

1056
01:04:33,430 --> 01:04:39,060
what similar what is similarity now between two documents are presented

1057
01:04:39,070 --> 01:04:40,330
in such a way

1058
01:04:40,340 --> 01:04:41,760
it's simply to

1059
01:04:41,780 --> 01:04:43,140
cause i'm

1060
01:04:43,160 --> 01:04:48,370
core zone of the angle between these two factors which is calculated with a simple

1061
01:04:49,950 --> 01:04:53,480
formula which we all know from

1062
01:04:53,480 --> 01:04:54,570
in high school

1063
01:04:54,590 --> 01:04:59,110
so there's no big magic in this and this works very well

1064
01:04:59,140 --> 01:05:01,610
and this is used in

1065
01:05:01,630 --> 01:05:07,680
i would say most of the products you can buy in the market this

1066
01:05:07,700 --> 01:05:14,790
why is this simple formula so interesting because it can be calculated very efficiently and

1067
01:05:14,800 --> 01:05:17,280
has a couple of other very

1068
01:05:17,290 --> 01:05:19,760
nice properties

1069
01:05:19,990 --> 01:05:28,860
next presentation is so-called language models just briefly mention it so this language model representation

1070
01:05:28,940 --> 01:05:38,790
is basically this formula and it captures this the fact that language modeling can be

1071
01:05:39,190 --> 01:05:44,580
also about the probability of a sequence of words so basically what everything gets reduced

1072
01:05:44,860 --> 01:05:49,920
to the estimating probabilities of a next word given one or two

1073
01:05:51,020 --> 01:05:56,330
so this would be called three try model and this is this simple formula so

1074
01:05:56,330 --> 01:06:02,920
we have two frequencies and we can estimate this probability and having this

1075
01:06:04,980 --> 01:06:07,810
three calculated and this can be very

1076
01:06:07,830 --> 01:06:11,710
efficiently used for speech recognition OCR

1077
01:06:11,710 --> 01:06:18,610
handwriting recognition machine translation and spelling correction as well so this very simple formula actually

1078
01:06:19,670 --> 01:06:25,840
it's also a lot of problems and this is also quite quite old already in

1079
01:06:25,840 --> 01:06:31,150
text mining is not used so often but let's in this other related fields would

1080
01:06:31,150 --> 01:06:32,250
be a little bit more

1081
01:06:32,310 --> 01:06:41,420
full parsing also just briefly tragedies so what is full parsing surpassing of

1082
01:06:41,440 --> 01:06:46,380
text basically provides the maximum structural information

1083
01:06:46,540 --> 01:06:51,670
one of the sentence was the scenario so on the input we get this put

1084
01:06:51,670 --> 01:06:56,880
the sentence and on the output together past three so this would be an example

1085
01:06:56,880 --> 01:06:58,360
of past three so

1086
01:06:58,380 --> 01:06:59,960
john hit the ball

1087
01:06:59,980 --> 01:07:06,560
and the possibility was these three in the structure of this sentence is nounphrase were

1088
01:07:06,580 --> 01:07:10,980
in nounphrase we determine and now

1089
01:07:11,000 --> 01:07:14,960
now what's interesting is that this is a lot of structure which we get a

1090
01:07:15,130 --> 01:07:18,150
which we get to the top of the text is actually quite hard to use

1091
01:07:18,150 --> 01:07:19,960
for most of the tasks

1092
01:07:19,980 --> 01:07:24,330
later on we see some examples where we can use this information but this is

1093
01:07:24,330 --> 01:07:28,190
that this was a lot of discussion is still a lot of discussion how to

1094
01:07:28,190 --> 01:07:34,270
use this additional structure information usually we just remove it to be on ninety we

1095
01:07:34,270 --> 01:07:36,270
just made this

1096
01:07:36,270 --> 01:07:38,270
some some of the labels

1097
01:07:41,190 --> 01:07:46,170
last syntactic level representation will be something which

1098
01:07:46,190 --> 01:07:48,560
usually not even mentioned but i think it's quite

1099
01:07:48,580 --> 01:07:53,100
what's relevant is called i would call it cross modality so

1100
01:07:53,310 --> 01:07:59,230
the data in which we can get out of the

1101
01:07:59,250 --> 01:08:06,290
let's say up on the web for for our application can be represented in different

1102
01:08:06,290 --> 01:08:08,540
modalities so here we talk

1103
01:08:08,630 --> 01:08:09,730
the main about it

1104
01:08:09,750 --> 01:08:14,110
textual documents and the most common with the english text in some kind of textual

1105
01:08:14,110 --> 01:08:20,330
documents but that text can be also multilingual information can be provided also in images

1106
01:08:20,330 --> 01:08:22,400
and video in social networks

1107
01:08:22,420 --> 01:08:26,980
sensor networks are getting very popular nowadays so

1108
01:08:27,000 --> 01:08:33,270
the question here is now if you have the same object described with several modalities

1109
01:08:33,270 --> 01:08:40,340
of the same time its ability flickr would have image photos of keywords maybe some

1110
01:08:40,340 --> 01:08:46,440
additional text dump with no question is can we exploit all these different information

1111
01:08:46,460 --> 01:08:53,650
to get better results so one this is one example and say if we have

1112
01:08:53,670 --> 01:08:54,860
the word by

1113
01:08:56,210 --> 01:09:01,810
they can have several representations for this work by just actual work bias it written

1114
01:09:01,810 --> 01:09:03,420
here multilingual

1115
01:09:03,440 --> 01:09:08,480
so in different languages we have to be able to get different representations then audio

1116
01:09:08,500 --> 01:09:14,360
suddenly have just audio files with the work by this is another representation for the

1117
01:09:14,360 --> 01:09:21,170
same object or image or even you don't like here so these are alternative representations

1118
01:09:21,170 --> 01:09:22,380
now the question is

1119
01:09:22,400 --> 01:09:26,650
can we benefit out it so i want to go here much into the details

1120
01:09:26,650 --> 01:09:33,020
about this is method which we use of but often when we try to relate

1121
01:09:33,020 --> 01:09:35,670
images that image text or

1122
01:09:35,670 --> 01:09:38,630
actually presentation in different languages

1123
01:09:38,630 --> 01:09:41,010
this property influences the future outcomes

1124
01:09:41,040 --> 01:09:43,610
so what you do with the TED so with this model

1125
01:09:43,610 --> 01:09:46,310
this is what i explained before so that was the induction step

1126
01:09:46,370 --> 01:09:49,980
what you do with this model later you want to use it for prediction

1127
01:09:50,040 --> 01:09:52,910
so why don't use it directly for prediction

1128
01:09:52,910 --> 01:09:56,210
so what is the probability that the next outcome is one

1129
01:09:56,230 --> 01:09:57,980
given our past outcomes

1130
01:09:58,060 --> 01:10:00,190
i mean this is what we really want to do we want to predict the

1131
01:10:01,400 --> 01:10:02,730
of future events

1132
01:10:03,780 --> 01:10:06,760
and this conditional probability you can either

1133
01:10:07,020 --> 01:10:09,790
computed from the posterior

1134
01:10:09,800 --> 01:10:13,160
or directly from the ratio of evidence

1135
01:10:13,160 --> 01:10:15,660
because i mean you just use the definition

1136
01:10:15,870 --> 01:10:18,630
and then you features this ratio of two evidences

1137
01:10:18,670 --> 01:10:20,420
so you see

1138
01:10:20,460 --> 01:10:23,710
how they can be used in the answers to her

1139
01:10:23,720 --> 01:10:27,310
which is actually more reasonable than the three four but i will not go into

1140
01:10:27,310 --> 01:10:29,370
this now

1141
01:10:29,450 --> 01:10:34,490
expectations of you all know i mean expectation of functions just the function times the

1142
01:10:34,490 --> 01:10:36,490
probability and then summed over all

1143
01:10:36,700 --> 01:10:38,520
all the sample space

1144
01:10:38,810 --> 01:10:43,810
so here the expected value of ten because one problem the posterior is that it's

1145
01:10:43,820 --> 01:10:46,670
a highly complicated object i mean this is the function here

1146
01:10:46,670 --> 01:10:49,230
but if you have many parameters

1147
01:10:49,270 --> 01:10:52,650
then i mean it's not the answer but what do you do with it to

1148
01:10:52,650 --> 01:10:54,270
begin summaries of the

1149
01:10:54,280 --> 01:10:58,420
and so in one summary is to compute either the maximum

1150
01:10:58,420 --> 01:10:59,920
so sum up estimator

1151
01:10:59,940 --> 01:11:01,880
or you compute the mean

1152
01:11:01,890 --> 01:11:04,120
and this would be true for this

1153
01:11:04,370 --> 01:11:09,340
if you want an estimate of the accuracy

1154
01:11:09,380 --> 01:11:15,660
over estimation then you often variance which is defined in this way

1155
01:11:17,530 --> 01:11:22,460
then you know this is things with probabilities of probability densities

1156
01:11:22,700 --> 01:11:25,480
just i mean you take the probability of a small small interrupted by by the

1157
01:11:25,480 --> 01:11:28,670
length and let me go to zero

1158
01:11:28,910 --> 01:11:33,500
i mean that's not the most important to the density

1159
01:11:33,510 --> 01:11:35,720
OK OK i'm running late

1160
01:11:37,530 --> 01:11:41,870
time for coffee break now so we will always have the first coffee break in

1161
01:11:41,870 --> 01:11:43,390
the morning will be some

1162
01:11:43,410 --> 01:11:45,060
real coffee break

1163
01:11:45,090 --> 01:11:48,420
you can go down there is coffee and tea

1164
01:11:48,430 --> 01:11:51,010
please be that in time so we

1165
01:11:51,060 --> 01:11:54,170
even if some run late

1166
01:11:56,420 --> 01:11:58,480
you should have stopped

1167
01:11:58,500 --> 01:12:06,470
we started even if you're starting time

1168
01:12:06,490 --> 01:12:08,570
and the second break

1169
01:12:08,620 --> 01:12:12,220
is the smaller i mean the times the same but you know the only instant

1170
01:12:12,220 --> 01:12:14,290
coffee and so on in the same in the afternoon

1171
01:12:14,320 --> 01:12:15,730
they you back in

1172
01:12:15,800 --> 01:12:18,420
ten minutes

1173
01:12:20,540 --> 01:12:22,130
there a bunch of

1174
01:12:24,530 --> 01:12:28,430
and most of them will be explained in detail by the lecture so if you

1175
01:12:28,430 --> 01:12:31,250
if you can't follow one no problem

1176
01:12:31,320 --> 01:12:34,690
you we get mostly of full lecture about them anyway later

1177
01:12:35,310 --> 01:12:37,090
the metaphoric ration

1178
01:12:37,090 --> 01:12:39,490
already mentioned

1179
01:12:39,520 --> 01:12:42,810
this is a very exciting to be the line through data

1180
01:12:42,860 --> 01:12:47,190
but it's really really important and it's more powerful than you think of

1181
01:12:47,240 --> 01:12:49,910
some methods for regularisation

1182
01:12:50,690 --> 01:12:54,790
linear methods for classification and linear basis function regression and this is the reason why

1183
01:12:54,790 --> 01:12:56,760
they are so important

1184
01:12:57,050 --> 01:13:05,620
for instance blind faith let's they're all linear methods for regression

1185
01:13:06,220 --> 01:13:09,380
kernel methods local small thing

1186
01:13:14,060 --> 01:13:15,550
let's start so the

1187
01:13:15,620 --> 01:13:17,490
the classical

1188
01:13:17,510 --> 01:13:21,440
classic problem regression problem

1189
01:13:21,460 --> 01:13:23,930
yes some input feature vector x

1190
01:13:23,930 --> 01:13:26,100
in some d dimensional space

1191
01:13:26,140 --> 01:13:29,760
so you have the components x one up to the

1192
01:13:29,780 --> 01:13:34,720
the particularly you add another component because and you said that identical one that this

1193
01:13:34,720 --> 01:13:36,860
just to make matters easier

1194
01:13:36,960 --> 01:13:39,890
you have a response value

1195
01:13:40,250 --> 01:13:42,310
it is often noisy

1196
01:13:42,640 --> 01:13:46,790
and you want to infer a relation between the input variables the features and the

1197
01:13:46,790 --> 01:13:48,750
output variable

1198
01:13:52,960 --> 01:13:56,040
and if the relation is linear or you assume this

1199
01:13:56,060 --> 01:14:00,280
relations linear in the most general linear function is this year

1200
01:14:00,430 --> 01:14:04,460
justice coefficients w zero after to

1201
01:14:04,530 --> 01:14:07,380
so remember makes zero is just one

1202
01:14:07,470 --> 01:14:09,510
OK so now we have data

1203
01:14:09,560 --> 01:14:15,780
so the appears of features of feature vectors and labels say and

1204
01:14:17,870 --> 01:14:20,090
o thing you need is the loss function

1205
01:14:20,100 --> 01:14:24,240
if you don't specify then it's often implicit and often quadratic

1206
01:14:24,890 --> 01:14:26,600
if you look for the

1207
01:14:27,960 --> 01:14:28,960
the real

1208
01:14:28,960 --> 01:14:33,800
and i'm sort of conduct continuing this because the process until a particular cell so

1209
01:14:33,800 --> 01:14:38,170
what's the probability of meeting the cell is exactly the product of these decisions i

1210
01:14:38,170 --> 01:14:42,960
made two to descend into this and that's exactly the sort of thinking continues is

1211
01:14:42,970 --> 01:14:44,040
of some kind of

1212
01:14:44,080 --> 01:14:50,010
compatibility matrix thinking of you as a sort of you have some kind of coordinating

1213
01:14:50,010 --> 01:14:54,160
is binary space or in this binary tree and then just narrowing down down

1214
01:14:54,170 --> 01:14:57,790
and the probability of hitting the cell is exactly the

1215
01:14:57,880 --> 01:14:59,630
the product of this element

1216
01:14:59,670 --> 01:15:03,620
so that's another view how you could think of this kronecker graphs

1217
01:15:05,160 --> 01:15:06,260
what is nice

1218
01:15:06,290 --> 01:15:10,580
is that you can go and estimated from data so the idea is used in

1219
01:15:10,580 --> 01:15:12,460
the UK g

1220
01:15:12,500 --> 01:15:15,880
the graph you ask how should i say this four numbers

1221
01:15:15,920 --> 01:15:20,820
so that you know that was that i would get something similar like this

1222
01:15:20,870 --> 01:15:24,380
and there are many different ways to do this so for example one way to

1223
01:15:24,380 --> 01:15:25,530
do this is to

1224
01:15:25,550 --> 01:15:30,540
use the method of moments where the idea is that if you if you set

1225
01:15:30,580 --> 01:15:32,700
this is the values of these guys

1226
01:15:32,750 --> 01:15:34,660
then you can analytically workout

1227
01:15:34,660 --> 01:15:36,450
how many edges

1228
01:15:36,500 --> 01:15:41,220
open tryouts close brian's riots and this is what is called the things will be

1229
01:15:41,240 --> 01:15:45,450
there in the network so that you can work out analytically what the expected frequencies

1230
01:15:45,450 --> 01:15:46,280
of these

1231
01:15:46,290 --> 01:15:48,670
of these of the subgraphs

1232
01:15:48,720 --> 01:15:52,420
given that this is the value of this part of this made values of this

1233
01:15:52,420 --> 01:15:56,950
matrix so we can see seem you can simply take the network count the frequencies

1234
01:15:56,950 --> 01:16:01,570
of these subgraphs and figure out what is the what is the

1235
01:16:01,620 --> 01:16:05,640
what are the values of these elements are given sort of the same subgraph comes

1236
01:16:05,700 --> 01:16:07,700
so so that's the first

1237
01:16:07,750 --> 01:16:12,410
one the method the second method is to apply maximum likelihood

1238
01:16:12,500 --> 01:16:14,450
the idea is i started

1239
01:16:14,460 --> 01:16:15,640
k one

1240
01:16:15,660 --> 01:16:18,830
i kronecker multiplied something to to get something bigger

1241
01:16:18,840 --> 01:16:22,970
and when i get this stochastic adjacency matrix i want to say these guys so

1242
01:16:22,970 --> 01:16:27,240
that when i sleep is going to get this sequence it this now the challenge

1243
01:16:27,420 --> 01:16:29,710
semantics i get this sequence of

1244
01:16:29,750 --> 01:16:35,300
coins sort of force of heads and thanks for this particular realization of edges appearing

1245
01:16:35,300 --> 01:16:38,640
in the network in this way i can sort of find the most likely thing

1246
01:16:39,120 --> 01:16:42,540
to do that would give rise to myself my observed data

1247
01:16:42,550 --> 01:16:46,830
and then the last way to to go about this is to use SVD

1248
01:16:46,830 --> 01:16:52,090
well because it turns out that you can minimises froebenius norm so basically can minimize

1249
01:16:52,330 --> 01:16:55,070
you can solve g minus

1250
01:16:55,680 --> 01:17:00,530
OK chronic OK right so you can you can you can figure out what is

1251
01:17:00,530 --> 01:17:01,380
what is

1252
01:17:01,420 --> 01:17:04,030
OK one that minimizes this expression

1253
01:17:04,080 --> 01:17:07,110
right so basically you are minimizing the

1254
01:17:07,120 --> 01:17:12,080
some kind of euclidean distance between this matrix and the adjacency matrix and now we

1255
01:17:12,080 --> 01:17:15,370
can do this recursively and you begin and you get a solution

1256
01:17:15,790 --> 01:17:19,680
and if you do that so let's keep this

1257
01:17:19,710 --> 01:17:21,250
and then ask

1258
01:17:21,250 --> 01:17:21,910
you know

1259
01:17:21,920 --> 01:17:25,340
this is for example if you take network and seventy five thousand miles and about

1260
01:17:25,340 --> 01:17:29,080
half a million edges and you know these are the four numbers you find and

1261
01:17:29,080 --> 01:17:34,090
now we are in now generate a kronecker graph using this initiator matrix

1262
01:17:34,110 --> 01:17:37,790
and now we compare this to the video

1263
01:17:37,840 --> 01:17:42,260
opinions networks it is like a big who trusts whom network and you for example

1264
01:17:42,260 --> 01:17:47,910
compare in the distribution of degree distribution triangle counts

1265
01:17:47,920 --> 01:17:54,040
shortest path distribution this is the first eigen vector this is that these are the

1266
01:17:54,040 --> 01:17:55,420
ideal values

1267
01:17:55,420 --> 01:17:58,120
you can see that you get things very close

1268
01:17:58,120 --> 01:18:04,550
so basically you are able to generate the big network that this somehow

1269
01:18:04,570 --> 01:18:09,390
using this microscopic measures very close to the network

1270
01:18:11,830 --> 01:18:14,550
and using just four four parameters

1271
01:18:14,800 --> 01:18:19,490
which is kind of surprising that setting four numbers correctly you can sort of get

1272
01:18:19,490 --> 01:18:23,950
something that's at least in the ballpark very similar to the to the end

1273
01:18:24,000 --> 01:18:27,830
and you can do this efficiently like a laptop in two hours so

1274
01:18:27,880 --> 01:18:30,580
it's not expensive so much

1275
01:18:30,620 --> 01:18:32,380
OK so

1276
01:18:32,380 --> 01:18:38,720
why what what consequences that now we have a model that can generate such networks

1277
01:18:38,720 --> 01:18:43,830
right i can use this for example is a very good not model for simulations

1278
01:18:43,830 --> 01:18:47,580
where i have new algorithms or something that i would like like to test on

1279
01:18:47,580 --> 01:18:51,070
the network and i would like to figure out how does it perform as i

1280
01:18:51,070 --> 01:18:56,410
as a very the structure of mine and i can use this for hypothesis testing

1281
01:18:56,410 --> 01:19:00,620
for sort of predicting how the network it will evolve over time i can take

1282
01:19:00,630 --> 01:19:05,080
and then it generate smaller or larger versions of the network using sort of the

1283
01:19:05,080 --> 01:19:08,990
same category and i can use this for

1284
01:19:09,010 --> 01:19:10,660
anomaly detection

1285
01:19:13,080 --> 01:19:16,290
you can make a break or i can go on

1286
01:19:16,340 --> 01:19:17,850
any thoughts

1287
01:19:17,870 --> 01:19:23,840
so so so far what i was talking about was this sort of more physics

1288
01:19:24,280 --> 01:19:29,000
type models of how to generate this landscape structure what they want to get into

1289
01:19:29,010 --> 01:19:31,620
now is how to

1290
01:19:31,640 --> 01:19:37,240
think about leaks so basically what we focus now is more statistical models on how

1291
01:19:37,750 --> 01:19:41,550
one of the network of the structure of links and here i probably have to

1292
01:19:41,550 --> 01:19:44,350
skip lot because a lot behind

1293
01:19:44,380 --> 01:19:46,490
so the idea is

1294
01:19:46,500 --> 01:19:50,790
i want to predict missing links in the network and then sort of two types

1295
01:19:50,790 --> 01:19:56,290
of approaches for some shows something that's a bit more heuristic method based on how

1296
01:19:56,290 --> 01:20:00,710
to estimate some kind of distance between the nodes in the network and then the

1297
01:20:00,710 --> 01:20:04,800
second one i'll talk about is how to sort of become model and somehow model

1298
01:20:04,830 --> 01:20:06,760
the links in a statistical way

1299
01:20:08,250 --> 01:20:11,010
come to think of link prediction you can think of is in the fall in

1300
01:20:11,010 --> 01:20:16,640
the following way you have an evolving networks that you observe up to time so

1301
01:20:16,840 --> 01:20:21,240
some time and then what is like to present we predict so given the network

1302
01:20:21,240 --> 01:20:24,330
up to particular point in time you would like to predict what new links will

1303
01:20:24,330 --> 01:20:26,890
appear in the network

1304
01:20:26,910 --> 01:20:30,780
in some other time call subsequent binding

1305
01:20:30,790 --> 01:20:36,640
and then evaluation that you're doing is basically you predict this number of new edges

1306
01:20:36,640 --> 01:20:39,790
that actually appear in the network and asking how many of them did i guess

1307
01:20:39,790 --> 01:20:42,160
correctly and how many of them to this

1308
01:20:42,160 --> 01:20:45,140
so this this particular work

1309
01:20:45,630 --> 01:20:49,660
the time that he was done by david liben nowell and jon kleinberg you know

1310
01:20:49,710 --> 01:20:54,630
three they looked at the evolution of co author citation and recorded

1311
01:20:54,640 --> 01:21:01,930
or basically collaboration networks in physics between different areas of physics and the other thing

1312
01:21:01,930 --> 01:21:06,200
they did the only consider edges between the people that have at least three other

1313
01:21:06,200 --> 01:21:08,700
edges just because of sparsity

1314
01:21:09,990 --> 01:21:15,070
and what they they they they decided to do was basically become pair of nodes

1315
01:21:15,070 --> 01:21:16,120
that somehow

1316
01:21:16,130 --> 01:21:22,920
quantify the distance between them rank the rank or ranking do not seem the in

1317
01:21:22,920 --> 01:21:27,670
the increasing distance take top n and consider this is your prediction right for example

1318
01:21:27,670 --> 01:21:31,540
what do i mean by distance you can use the shortest path distance

1319
01:21:31,610 --> 01:21:37,250
you can use the distance between the between the nodes as the number of common

1320
01:21:37,250 --> 01:21:41,210
neighbors they have the number of shared friends you can use the same thing but

1321
01:21:41,410 --> 01:21:43,540
do it as a coefficient

1322
01:21:43,720 --> 01:21:48,030
you can use a measure that was proposed by damage not and you can sort

1323
01:21:48,040 --> 01:21:51,620
of think of it as some kind of IDF right you're going over all the

1324
01:21:51,620 --> 01:21:56,340
shared neighbours and you're taking one over the log of the degree of that should

1325
01:21:56,380 --> 01:21:59,350
neighbours you can sort of considered you know

1326
01:21:59,350 --> 01:22:02,960
three sets of size and the size

1327
01:22:02,980 --> 01:22:04,800
we get a small

1328
01:22:04,810 --> 01:22:05,810
so this is

1329
01:22:13,940 --> 01:22:15,750
in addition to

1330
01:22:15,900 --> 01:22:19,970
that is all that you feel

1331
01:22:19,980 --> 01:22:21,990
so you

1332
01:22:24,250 --> 01:22:25,980
with support vector machine

1333
01:22:27,240 --> 01:22:34,830
necessary you the whole thing is one line is

1334
01:22:34,850 --> 01:22:37,820
that's why

1335
01:22:38,000 --> 01:22:42,350
features of cells to the site

1336
01:22:42,360 --> 01:22:43,910
the these history

1337
01:22:43,990 --> 01:22:46,520
the problem is that we try

1338
01:22:46,540 --> 01:22:47,470
so now

1339
01:22:47,960 --> 01:22:51,930
training and testing on the side

1340
01:22:53,180 --> 01:22:55,180
so that they

1341
01:22:55,200 --> 01:23:04,970
and are course to estimate how to keep the eyes sometimes point one

1342
01:23:04,990 --> 01:23:08,440
on the second half sometimes point two

1343
01:23:11,440 --> 01:23:14,620
expected same

1344
01:23:15,840 --> 01:23:17,130
this is very

1345
01:23:17,250 --> 01:23:19,660
five years

1346
01:23:20,500 --> 01:23:22,220
you can see the

1347
01:23:22,270 --> 01:23:23,870
get one

1348
01:23:26,100 --> 01:23:30,630
the same progressively deteriorated

1349
01:23:33,210 --> 01:23:34,380
five years

1350
01:23:34,420 --> 01:23:35,930
o three

1351
01:23:36,070 --> 01:23:39,990
this is the average generalisation about one

1352
01:23:40,010 --> 01:23:43,840
o point one five sixty five

1353
01:23:43,960 --> 01:23:46,270
red light

1354
01:23:46,580 --> 01:23:50,010
in this particular article

1355
01:24:00,440 --> 01:24:04,710
small you mention

1356
01:24:04,780 --> 01:24:05,480
this is

1357
01:24:05,490 --> 01:24:07,110
what about

1358
01:24:07,410 --> 01:24:09,740
to the right

1359
01:24:10,450 --> 01:24:11,440
the problem

1360
01:24:14,410 --> 01:24:18,590
sounds like my work just

1361
01:24:20,190 --> 01:24:22,600
and mean that picture

1362
01:24:27,580 --> 01:24:34,300
he left behind you by space shuttle

1363
01:24:35,370 --> 01:24:38,090
and we use machine

1364
01:24:39,990 --> 01:24:42,210
also a large march for

1365
01:24:42,240 --> 01:24:43,630
so if you imagine this

1366
01:24:48,100 --> 01:24:50,990
what does not just pick

1367
01:24:51,080 --> 01:24:52,640
one of them

1368
01:24:52,770 --> 01:24:57,430
which is the the one that maximizes the marginal

1369
01:24:58,310 --> 01:25:01,580
intuition is that you

1370
01:25:01,590 --> 01:25:04,750
and this is the just as

1371
01:25:04,760 --> 01:25:07,090
the whole examples might be

1372
01:25:08,290 --> 01:25:13,340
then we may not be possible that one

1373
01:25:13,410 --> 01:25:14,990
what they are

1374
01:25:15,000 --> 01:25:21,780
i've left my heart actually signed that the usually

1375
01:25:21,790 --> 01:25:23,540
the tradition

1376
01:25:23,860 --> 01:25:28,290
you don't know

1377
01:25:28,290 --> 01:25:30,770
and space is still separation

1378
01:25:31,030 --> 01:25:35,020
hard to see that these

1379
01:25:35,130 --> 01:25:37,340
no it's time

1380
01:25:39,560 --> 01:25:41,500
and also

1381
01:25:42,130 --> 01:25:43,180
should be

1382
01:25:45,160 --> 01:25:49,100
you can also

1383
01:25:49,100 --> 01:25:56,780
the solution is that you want to minimize the way back home

1384
01:25:59,130 --> 01:26:01,500
and know

1385
01:26:01,550 --> 01:26:02,890
you will

1386
01:26:06,850 --> 01:26:08,170
and one to

1387
01:26:08,190 --> 01:26:11,580
and i think that the resolution

1388
01:26:11,770 --> 01:26:15,950
actually by the way

1389
01:26:16,000 --> 01:26:21,010
it's not a good

1390
01:26:21,030 --> 01:26:25,840
the station is actually not quite a lot

1391
01:26:25,910 --> 01:26:29,890
they said they had to measure

1392
01:26:32,100 --> 01:26:35,210
and what was going

1393
01:26:35,220 --> 01:26:37,250
she was saying the

1394
01:26:37,360 --> 01:26:41,000
the same input does

1395
01:26:41,100 --> 01:26:44,360
he in as you know

1396
01:26:44,390 --> 01:26:48,170
the standard of using exactly the same

1397
01:26:48,190 --> 01:26:49,490
you know the

1398
01:26:49,490 --> 01:26:50,810
actually power

1399
01:26:50,820 --> 01:26:57,880
we seem what to have on generalisation of this histogram we should

1400
01:27:00,640 --> 01:27:04,100
see already decided to find

1401
01:27:04,110 --> 01:27:06,190
i mean should

1402
01:27:06,240 --> 01:27:08,430
we will not detail

1403
01:27:08,690 --> 01:27:13,850
this for things you find those

1404
01:27:13,870 --> 01:27:16,800
he was

1405
01:27:16,820 --> 01:27:19,530
the same is happening all

1406
01:27:19,700 --> 01:27:22,080
nasty tales

1407
01:27:22,080 --> 01:27:25,570
he be there in the hope is that for the one two one thirty part

1408
01:27:25,570 --> 01:27:30,090
of the lab which is different setting things right and after that the lab begins

1409
01:27:30,130 --> 01:27:33,880
but that's enough about administrative stuff you need to say a few things

1410
01:27:33,910 --> 01:27:39,170
OK so i'll give money and after that i will introduce the next big because

1411
01:27:39,290 --> 01:27:41,820
he doesn't actually

1412
01:27:41,960 --> 01:27:50,320
just one thing watch welcome to seals somewhere chap money with money father-and-son administrator here

1413
01:27:50,320 --> 01:27:52,560
since intelligent edges

1414
01:27:52,580 --> 01:27:56,000
if you have any questions and problems and we are not there for three twenty

1415
01:27:56,000 --> 01:28:00,950
four this some people would get some you still need to fell out we needed

1416
01:28:00,950 --> 01:28:03,830
to come to the office when you get a chance maybe in between the break

1417
01:28:03,830 --> 01:28:07,570
or right before you go to lines some user need to go over to like

1418
01:28:07,570 --> 01:28:11,790
student employment or international of some of the people work

1419
01:28:12,440 --> 01:28:18,940
so if you have any questions you may have any questions for me

1420
01:28:18,980 --> 01:28:23,830
no questions no questions and more

1421
01:28:26,760 --> 01:28:28,980
you see

1422
01:28:31,270 --> 01:28:35,030
so if you you

1423
01:28:35,040 --> 01:28:40,250
what think is going

1424
01:28:45,850 --> 01:28:52,390
you can hear the good OK so no time productions because we've got a virus

1425
01:28:52,390 --> 01:28:55,780
to sticking your neck out

1426
01:28:55,830 --> 01:29:01,150
i want to tell you first this morning in first half of the morning of

1427
01:29:01,280 --> 01:29:04,230
the things that i wish somebody had sat me down and explained to me at

1428
01:29:04,250 --> 01:29:10,460
the beginning grad school fifteen years ago or so clyde figure them out myself and

1429
01:29:10,460 --> 01:29:13,130
try to boil them down to lower forty

1430
01:29:13,150 --> 01:29:18,090
i want to start with a little is that this is something that people this

1431
01:29:18,210 --> 01:29:22,530
early program people often right when they start realising the probabilities and language might have

1432
01:29:22,530 --> 01:29:26,120
something to do with one another so here are generated some text

1433
01:29:26,380 --> 01:29:36,730
see what this say of arrange key article or safety limited fool zeros the estrogen

1434
01:29:36,810 --> 01:29:39,320
so had no idea how to do this well i think the latter to start

1435
01:29:39,320 --> 01:29:43,770
with let's say we start over here with and i took out my trusty owned

1436
01:29:43,770 --> 01:29:45,300
guy by

1437
01:29:45,320 --> 01:29:46,380
and role

1438
01:29:46,390 --> 01:29:50,270
to find out what the next letter would be of the next letter was an

1439
01:29:50,270 --> 01:29:54,070
OK so i took my and i next on world to find out what the

1440
01:29:54,070 --> 01:29:57,500
next letter would be i see in the next letter space and then i took

1441
01:29:57,500 --> 01:30:02,930
out the space that every letter is being chosen depending on what what the previous

1442
01:30:04,130 --> 01:30:09,880
so how do you think i think the weights for these days

1443
01:30:09,910 --> 01:30:13,010
what matters usually followed

1444
01:30:13,020 --> 01:30:16,440
more likely to be constant yet so you can't quite see here because some of

1445
01:30:16,440 --> 01:30:20,070
the slides are very small but this is actually like twenty seven sided dies the

1446
01:30:20,100 --> 01:30:25,650
twenty six letters in space of a few bits of punctuation as well and the

1447
01:30:25,650 --> 01:30:31,070
probability of landing on a particular side is proportional to the fraction of letters

1448
01:30:31,510 --> 01:30:34,650
the fraction of times the fraction of those

1449
01:30:34,740 --> 01:30:38,720
in a sample of english text that are followed by that letter so this is

1450
01:30:38,720 --> 01:30:39,590
the ODE

1451
01:30:39,610 --> 01:30:42,920
it tends to land on letters

1452
01:30:42,930 --> 01:30:47,040
so we generate you know this is kind of pronounce evolved because you say constant

1453
01:30:47,050 --> 01:30:50,200
and bells and alternate we can do better

1454
01:30:50,230 --> 01:30:54,160
so this is what is called a two parameter or by graph model

1455
01:30:54,210 --> 01:30:59,190
because it takes about two hundred times when it takes the letter conditions that whatever

1456
01:30:59,190 --> 01:31:00,770
the previous lecture what

1457
01:31:00,780 --> 01:31:02,150
so let's go up to

1458
01:31:02,160 --> 01:31:05,230
three grams or tri grams

1459
01:31:05,240 --> 01:31:06,240
so here

1460
01:31:06,400 --> 01:31:10,110
i picked two letters to start with

1461
01:31:10,130 --> 01:31:11,760
let's start over here

1462
01:31:14,360 --> 01:31:15,720
is so we start with

1463
01:31:15,730 --> 01:31:20,920
o t h i n decided well since it's t h

1464
01:31:20,940 --> 01:31:24,900
let's roll the t h di and we pick the next letter being and is

1465
01:31:24,910 --> 01:31:27,230
actually quite common following t h

1466
01:31:27,250 --> 01:31:28,930
so what policy age

1467
01:31:28,940 --> 01:31:33,370
well we got a lot of via this that

1468
01:31:33,380 --> 01:31:36,850
these so it's almost always valves

1469
01:31:37,040 --> 01:31:39,530
the valves are almost never you

1470
01:31:39,540 --> 01:31:44,650
in the following h eighty is pretty common in following eighty space is pretty common

1471
01:31:45,100 --> 01:31:47,720
cause as a good way to work

1472
01:31:47,740 --> 01:31:50,840
and this is starting to look a little more like english

1473
01:31:51,630 --> 01:31:53,530
this is called a

1474
01:31:54,190 --> 01:31:55,750
i'm get try to to model

1475
01:31:55,770 --> 01:32:00,280
and we can do a little better if we got the foreground

1476
01:32:00,290 --> 01:32:04,020
so now this is starting to look like

1477
01:32:04,020 --> 01:32:07,190
it's rather interesting

1478
01:32:07,230 --> 01:32:11,570
market by that in the vocal it choosing between created from those not present up

1479
01:32:11,570 --> 01:32:15,220
in class inherently days lovely voice or the words i mean we're getting a lot

1480
01:32:15,220 --> 01:32:18,050
of actual english words

1481
01:32:18,070 --> 01:32:23,150
we can even go little bit further this point you may be able to see

1482
01:32:23,150 --> 01:32:26,480
what the text was from which i chose the probability

1483
01:32:26,500 --> 01:32:42,240
that someone would like to guess

1484
01:32:45,140 --> 01:32:52,410
let's see the breaks out of oppressor of all which the proletarian coalesced into germany

1485
01:32:54,910 --> 01:33:03,250
will give you one point it generated the work reactionary

1486
01:33:03,250 --> 01:33:07,060
to leave you with that sort of thought-provoking idea

1487
01:33:07,060 --> 01:33:12,150
and let me o

1488
01:33:12,170 --> 01:33:16,270
oh sorry i have memories of my slides are everywhere so one more example then

1489
01:33:16,270 --> 01:33:17,830
i get to my was just so

1490
01:33:17,860 --> 01:33:21,520
this is this is another good example of what i meant by the

1491
01:33:21,520 --> 01:33:25,650
understanding the precision and recall problem

1492
01:33:26,360 --> 01:33:29,100
here's a typical kind of thing you might want to do with the knowledge based

1493
01:33:29,100 --> 01:33:33,190
system you have a question like this the answers warren harding

1494
01:33:33,230 --> 01:33:38,150
and we're not getting the answer with search because you know the right words are

1495
01:33:38,150 --> 01:33:41,000
in in the document that actually expresses this

1496
01:33:41,060 --> 01:33:46,020
so we use some kind of query expansion knowledge query expansion it says OK president

1497
01:33:46,020 --> 01:33:47,480
is the kind of leader

1498
01:33:47,520 --> 01:33:51,350
and the speech you know what kind of talk to someone gives presentation if someone

1499
01:33:51,350 --> 01:33:55,630
gives an now you kind of have a wider net to cast and you might

1500
01:33:55,630 --> 01:34:01,160
find something that doesn't explicitly say speech and president in it but may mention the

1501
01:34:02,020 --> 01:34:05,420
and all we get the

1502
01:34:05,480 --> 01:34:08,900
his data

1503
01:34:08,920 --> 01:34:12,330
o thing here it is are going to live that down the

1504
01:34:14,330 --> 01:34:17,620
that was actually my

1505
01:34:17,670 --> 01:34:20,290
closing joke so we need to

1506
01:34:20,380 --> 01:34:25,630
we need to measure these things i mean does improving recall actually improve the quality

1507
01:34:25,630 --> 01:34:30,920
analysis search the returns status name is wrong but the the idea is is that

1508
01:34:30,920 --> 01:34:35,940
when you open these things up by improving recall when you limit them by improving

1509
01:34:35,940 --> 01:34:42,850
precision with these knowledge based methods are you actually making an improvement

1510
01:34:42,850 --> 01:34:44,440
thank you

1511
01:34:47,310 --> 01:34:49,710
i can hear what he said

1512
01:34:53,130 --> 01:34:55,620
turn the microphone i can't it's

1513
01:34:55,630 --> 01:35:00,080
i can hear what you're saying because there's a bad girl

1514
01:35:00,120 --> 01:35:01,650
that's better

1515
01:35:01,710 --> 01:35:04,120
all right i'm making this slide

1516
01:35:04,130 --> 01:35:05,360
for that one

1517
01:35:05,770 --> 01:35:12,460
so it's not just about credit OK

1518
01:35:12,460 --> 01:35:17,080
got it

1519
01:35:17,080 --> 01:35:20,440
OK thank you

1520
01:35:20,460 --> 01:35:26,960
so i guess is then just showed it doesn't have to be a question it

1521
01:35:26,960 --> 01:35:29,190
can be a correction

1522
01:35:30,940 --> 01:35:34,000
as as then you just showed you don't have to ask a question you could

1523
01:35:34,000 --> 01:35:40,770
also correct some

1524
01:35:40,790 --> 01:35:47,190
there's no questions i do have a section on ontology called t

1525
01:35:47,190 --> 01:35:52,100
where was this

1526
01:35:52,630 --> 01:35:56,150
i e

1527
01:35:57,790 --> 01:35:59,540
was my slides anyway

1528
01:36:00,040 --> 01:36:06,150
so this is about reasoning what i said about reasoning having having difficulty selling and

1529
01:36:06,150 --> 01:36:11,290
i have a hard you know when i started doing this stuff i my first

1530
01:36:11,290 --> 01:36:16,420
reasoning system was something called knowledge crafts which was developed by mark fox at CMU

1531
01:36:16,420 --> 01:36:21,920
in the eighties it's one of these expert system shells around was machine

1532
01:36:24,850 --> 01:36:30,460
and i used to love your writing will rules and building of hierarchies and then

1533
01:36:30,460 --> 01:36:32,060
just watching the system

1534
01:36:32,060 --> 01:36:36,400
you know show me all the things that can be computed automatically from what i'd

1535
01:36:36,420 --> 01:36:42,210
written that i had a explicitly said i thought it was great so i started

1536
01:36:42,210 --> 01:36:46,190
building knowledge systems just to see those things happen

1537
01:36:46,230 --> 01:36:50,770
the the result lot of people spend hours tweaking their their web pages just you

1538
01:36:50,770 --> 01:36:54,860
know to make this font goal of half an inch

1539
01:36:54,900 --> 01:36:58,480
and and i love doing that and when

1540
01:36:58,520 --> 01:37:02,520
i'm certainly convinced that knowledge and reasoning is important but on how i'm saying is

1541
01:37:02,520 --> 01:37:04,130
i'm having a hard time

1542
01:37:04,210 --> 01:37:08,960
convincing people who want to adopt who want to know whether or not they should

1543
01:37:08,960 --> 01:37:13,980
adopt semantic web technology or semantic technology in general

1544
01:37:14,020 --> 01:37:18,000
when they ask me what do i need reasoning for

1545
01:37:18,130 --> 01:37:21,350
you know i try to give them some examples high how you can check the

1546
01:37:21,350 --> 01:37:25,960
consistency of the ontology and i try to give them examples of how they might

1547
01:37:25,960 --> 01:37:29,690
find errors in their data

1548
01:37:29,730 --> 01:37:33,940
you know but it's there they're not swept off their feet and most of the

1549
01:37:33,940 --> 01:37:39,880
people i talk to are interested ontologies for expressing the semantics of the data

1550
01:37:39,960 --> 01:37:43,690
but they just don't seem to be motivated by having any kind of inference or

1551
01:37:43,690 --> 01:37:46,880
any kind of automatic automated constraint checking

1552
01:37:46,940 --> 01:37:49,250
you know they want to be able to express

1553
01:37:49,250 --> 01:37:53,440
their semantics so that other people can see what this stuff means

1554
01:37:53,480 --> 01:38:00,330
and if we can automatically some query engine can automatically process changes so that their

1555
01:38:00,330 --> 01:38:05,270
maintenance their applications goes down they seem to be excited by this but

1556
01:38:05,290 --> 01:38:07,920
you know there are there are in

1557
01:38:07,920 --> 01:38:11,250
people interested in reason and i don't mean to say there's not

1558
01:38:11,290 --> 01:38:13,420
at all i mean there's certainly are

1559
01:38:13,420 --> 01:38:16,830
especially the medical community seems gung-ho

1560
01:38:16,960 --> 01:38:22,580
but outside of their it's really tough sell and and i don't i need help

1561
01:38:22,580 --> 01:38:27,400
these people to tell me or are we just start selling reasoning because it's school

1562
01:38:27,440 --> 01:38:31,290
or do we really think there are problems out there that cannot be solved any

1563
01:38:31,290 --> 01:38:33,040
other way

1564
01:38:33,170 --> 01:38:38,630
that's information i'd love to have

1565
01:38:38,630 --> 01:38:43,310
yeah i guess i should add that to my slide two i think that's

1566
01:38:44,060 --> 01:38:48,960
is that problem arose a trend i think i think that's more trend

1567
01:38:49,070 --> 01:38:52,960
i see people starting to put a lot of different things together and i think

1568
01:38:52,960 --> 01:38:57,980
that's great the work that we're doing in the rules working group

1569
01:38:58,040 --> 01:39:01,560
you know we started with this basic logic dialect and then

1570
01:39:02,080 --> 01:39:03,500
you know the

1571
01:39:03,520 --> 01:39:07,000
it as a companion to we we felt it was important to publish how you

1572
01:39:07,000 --> 01:39:10,150
would use this with some of the existing technology that's out there i think that's

1573
01:39:10,900 --> 01:39:19,750
what DBP in freebase and you know IMDB in wordnet don't actually real me wordnets

1574
01:39:19,750 --> 01:39:21,630
coming around but IMDB is

1575
01:39:21,650 --> 01:39:28,630
not really in the semantic web from the perspective of using the technology but there's

1576
01:39:28,630 --> 01:39:32,330
a lot of ways that you could combine some of the things that we have

1577
01:39:32,400 --> 01:39:37,500
to to make these things work better and and i think that's incredibly promising

1578
01:39:37,500 --> 01:39:39,440
i saw about eight months ago

1579
01:39:39,460 --> 01:39:41,230
i think the message from

1580
01:39:41,270 --> 01:39:44,600
one of the DBP you guys announcing that they were gonna

1581
01:39:44,630 --> 01:39:48,650
try to integrate the cyc ontology which by the way i think is really an

1582
01:39:48,650 --> 01:39:50,280
in order to do that i have to talk about

1583
01:39:53,120 --> 01:39:54,700
about representations for a little bit

1584
01:40:03,780 --> 01:40:05,940
usually when we in parametric statistics

1585
01:40:06,650 --> 01:40:07,660
want to build a new model

1586
01:40:08,370 --> 01:40:09,270
and what do is

1587
01:40:09,930 --> 01:40:10,850
we write down the density

1588
01:40:11,920 --> 01:40:14,570
that is how we define a probability model right on the density

1589
01:40:17,090 --> 01:40:18,660
one of the you know one over the year

1590
01:40:19,610 --> 01:40:23,910
the real obstacles in designing nonparametric bayesian models is that that doesn't work

1591
01:40:24,840 --> 01:40:26,400
you can't just write on the density

1592
01:40:27,790 --> 01:40:32,400
and the reason for that is is the following what what is the density what is the density mean

1593
01:40:32,860 --> 01:40:33,710
density is

1594
01:40:34,380 --> 01:40:35,810
representation of some

1595
01:40:36,680 --> 01:40:40,210
probability measure on measure in general but let's say of some probability measure

1596
01:40:40,790 --> 01:40:41,920
with respect to

1597
01:40:42,690 --> 01:40:43,360
and other measures

1598
01:40:45,220 --> 01:40:48,100
and what this is what does this formula mean so this here for example

1599
01:40:48,780 --> 01:40:52,050
imagine that this you say is girls in distribution on the line

1600
01:40:53,930 --> 01:40:54,850
and this year is

1601
01:40:55,950 --> 01:40:59,660
let back measure which means uniform distribution on the line something flat flat

1602
01:41:00,990 --> 01:41:03,080
and this year then is a ghost density

1603
01:41:04,390 --> 01:41:05,690
so what this density mean

1604
01:41:06,360 --> 01:41:06,960
it tells us

1605
01:41:07,760 --> 01:41:08,250
how we can

1606
01:41:09,860 --> 01:41:11,850
this distribution here at each point

1607
01:41:12,450 --> 01:41:14,200
it's a pointwise reweighting formula

1608
01:41:15,760 --> 01:41:17,830
to turn it into a girls in distribution

1609
01:41:20,200 --> 01:41:20,660
and it's very

1610
01:41:21,030 --> 01:41:23,790
we need to work with density density is very convenient

1611
01:41:24,410 --> 01:41:26,470
because they they are defined pointwise

1612
01:41:27,270 --> 01:41:31,100
and if you've ever done if ever seen any measure theory then you know that

1613
01:41:31,100 --> 01:41:32,990
measures are functions that are defined on sets

1614
01:41:33,400 --> 01:41:34,760
so we have to take the real line

1615
01:41:35,570 --> 01:41:35,790
and now

1616
01:41:37,040 --> 01:41:38,810
takes realign put a measure on it brought

1617
01:41:39,200 --> 01:41:39,610
how do you do

1618
01:41:40,830 --> 01:41:42,880
you can actually draw because what you would have to

1619
01:41:43,430 --> 01:41:46,590
what you have to draw near x axes are all possible subsets

1620
01:41:47,720 --> 01:41:50,260
of the real line and then you would have for each of them you would

1621
01:41:50,260 --> 01:41:52,140
have to draw value you can't really great

1622
01:41:54,170 --> 01:41:57,760
this representation here is something that actually is is a function on

1623
01:41:58,170 --> 01:42:00,820
the domain on which measurements so that's very convenient

1624
01:42:03,200 --> 01:42:05,010
okay now the but the problem is that

1625
01:42:05,680 --> 01:42:06,800
such a nice that

1626
01:42:07,360 --> 01:42:10,890
in order for this representation to be useful we need this distribution here

1627
01:42:13,060 --> 01:42:14,970
so the first problem is if we have some

1628
01:42:16,860 --> 01:42:19,940
parameter space and bayesian nonparametrics some infinite dimensional space

1629
01:42:20,530 --> 01:42:21,260
and we want to put

1630
01:42:22,380 --> 01:42:24,400
we want to define this distribution on that space

1631
01:42:25,180 --> 01:42:27,930
then we first need some other distribution on that space to

1632
01:42:28,770 --> 01:42:29,690
to write down density

1633
01:42:30,150 --> 01:42:32,340
right some reference distribution which is usually

1634
01:42:32,740 --> 01:42:34,590
so usually that's called a carrier measure

1635
01:42:35,030 --> 01:42:36,290
sometimes just a reference methods

1636
01:42:37,960 --> 01:42:40,890
so we first have to you know be

1637
01:42:41,320 --> 01:42:44,700
we first have to solve the problem of output solution on that space anyway

1638
01:42:46,290 --> 01:42:47,110
that's the first problem

1639
01:42:47,710 --> 01:42:48,690
the second problem is

1640
01:42:52,120 --> 01:42:54,100
this this measure you should have certain

1641
01:42:56,210 --> 01:42:58,160
so if you represent the gauss in distribution

1642
01:42:58,830 --> 01:42:59,660
what do use

1643
01:43:00,940 --> 01:43:02,400
you're always use something uniform

1644
01:43:03,120 --> 01:43:06,240
right and for good reason why did you not why do not use and other calcium

1645
01:43:07,900 --> 01:43:09,210
because we use another calcium

1646
01:43:09,670 --> 01:43:11,690
then this density is much harder to interpret

1647
01:43:12,770 --> 01:43:16,890
can imagine we take to go since which are located which have peaks at different locations

1648
01:43:17,570 --> 01:43:18,930
now we could represent one of them

1649
01:43:19,360 --> 01:43:21,670
has a density with respect to the other one it's possible

1650
01:43:23,260 --> 01:43:26,400
but what with this density look like or what would it mean if the density is

1651
01:43:26,910 --> 01:43:29,020
is very small and some at some point

1652
01:43:29,590 --> 01:43:32,430
what does it mean it could mean in this case you always means

1653
01:43:32,910 --> 01:43:35,990
this probability measure has small probability mass in the region

1654
01:43:37,410 --> 01:43:40,010
if we use another girl you with another location

1655
01:43:40,880 --> 01:43:42,620
it could you know mean that's could mean

1656
01:43:43,530 --> 01:43:45,170
this one is very large in that region

1657
01:43:48,900 --> 01:43:49,660
doesn't make any sense

1658
01:43:51,330 --> 01:43:53,840
if we do something if this distribution is not

1659
01:43:54,290 --> 01:43:55,100
is not flat

1660
01:43:56,870 --> 01:43:57,650
if the density

1661
01:43:58,380 --> 01:44:00,210
increases somewhere put you mean

1662
01:44:00,590 --> 01:44:02,120
the distribution becomes larger

1663
01:44:02,620 --> 01:44:04,200
but it could also mean that this one becomes small

1664
01:44:05,590 --> 01:44:08,160
so would like this to be to be flat neutral

1665
01:44:09,790 --> 01:44:12,560
just to get some sense of interpretability

1666
01:44:16,770 --> 01:44:21,960
such such uniform distribution turns out mathematics that such uniform distributions only

1667
01:44:22,400 --> 01:44:23,840
exist on on certain

1668
01:44:24,490 --> 01:44:27,370
spaces and roughly speaking is a finite dimensional spaces

1669
01:44:28,260 --> 01:44:31,350
so that's exactly result the parameter space and nonparametrics

1670
01:44:33,770 --> 01:44:36,540
that the technical term for this flat distributions are you

1671
01:44:37,020 --> 01:44:42,190
uniformly ones is translation invariant if you just take it and shifted around on your space

1672
01:44:42,950 --> 01:44:44,100
and that doesn't change the value

1673
01:44:46,940 --> 01:44:49,360
and that just doesn't work on infinite dimensional spaces

1674
01:44:49,940 --> 01:44:52,920
yeah but remember the parameter spaces and bayesian nonparametrics

1675
01:44:53,580 --> 01:44:54,820
are always infinite-dimensional

1676
01:44:55,360 --> 01:44:57,110
so we can do something like that

1677
01:44:57,180 --> 01:44:59,140
representing the prior and the posterior

1678
01:44:59,890 --> 01:45:01,740
has a density with respect to something nice

1679
01:45:03,360 --> 01:45:04,340
and the second problem is

1680
01:45:05,150 --> 01:45:08,890
in order to do that we first have to define some probability distribution on that

1681
01:45:08,930 --> 01:45:11,950
anyway with respect to which we can we can represent this kind of a

1682
01:45:13,580 --> 01:45:14,590
its own tail problem

1683
01:45:16,200 --> 01:45:19,830
okay so that's the first the first representation problem that we have to cope with

1684
01:45:21,180 --> 01:45:21,660
and so

1685
01:45:22,790 --> 01:45:23,550
there's another one

1686
01:45:24,990 --> 01:45:25,920
and the second one is

1687
01:45:28,660 --> 01:45:29,530
so i affirm

1688
01:45:30,370 --> 01:45:31,660
yesterday i talked about

1689
01:45:32,590 --> 01:45:33,970
about all this clustering models

1690
01:45:34,430 --> 01:45:35,490
general bayesian models

1691
01:45:36,140 --> 01:45:40,260
but you may have noticed that throughout this like this whole part and clustering

1692
01:45:40,660 --> 01:45:42,050
we didn't see using bayes equation

1693
01:45:43,680 --> 01:45:48,010
now usually when you define a bayesian model when you see the definition of a bayesian model in the textbook

1694
01:45:48,490 --> 01:45:50,160
it's defined in terms of the basic relation

1695
01:45:50,680 --> 01:45:51,950
right bayesian model is like

1696
01:45:52,820 --> 01:45:55,370
the basic question the different parts of the basic creation other based on

1697
01:45:58,950 --> 01:46:02,490
the reason why why didn't show you any basic operations yesterday is because

1698
01:46:03,120 --> 01:46:06,160
four these nonparametric models based theorem is actually

1699
01:46:06,760 --> 01:46:10,510
not applicable in most cases and so they don't have a basic question this the

1700
01:46:10,510 --> 01:46:12,370
bayesian models but they don't have a basic reaction

1701
01:46:13,160 --> 01:46:14,320
let me try to explain what

1702
01:46:14,930 --> 01:46:16,140
what i mean that's okay

1703
01:46:18,120 --> 01:46:19,620
in general the bayesian model is

1704
01:46:20,200 --> 01:46:20,780
is given by

1705
01:46:20,780 --> 01:46:24,340
statistical learning theory approaches

1706
01:46:24,360 --> 01:46:27,050
so here we are part a OK so

1707
01:46:27,060 --> 01:46:31,380
the set of start with some thoughts about what theory should be doing for some

1708
01:46:31,380 --> 01:46:34,170
why we're doing theory in you know i don't

1709
01:46:34,190 --> 01:46:35,520
i think i

1710
01:46:35,570 --> 01:46:38,530
and sort of pushing an open door i think most people appreciate the need for

1711
01:46:38,550 --> 01:46:42,830
theory but but sometimes it's good to sort of reflect on why what are we

1712
01:46:42,830 --> 01:46:45,060
trying to achieve with this is it

1713
01:46:45,110 --> 01:46:48,010
is it really useful and what's the what's the purpose

1714
01:46:48,020 --> 01:46:54,930
so in statistical learning theory the main idea is to view learning from a statistical

1715
01:46:57,140 --> 01:46:59,120
and by that i mean that

1716
01:46:59,150 --> 01:47:03,920
you know the core of much of learning is about trying to find out

1717
01:47:03,930 --> 01:47:08,780
some truths about the world or about the phenomenon you're observing

1718
01:47:08,810 --> 01:47:13,320
from a finite sample i mean that's the key sort of ingredient here you're taking

1719
01:47:13,320 --> 01:47:14,780
some observations

1720
01:47:14,960 --> 01:47:20,250
a finite sample of observations and your hoping from that finite sample to learn something

1721
01:47:20,250 --> 01:47:26,090
that's actually true about the generating process that gave you that sample

1722
01:47:28,690 --> 01:47:32,590
so you sort of going from particular to general in that sense

1723
01:47:32,610 --> 01:47:38,500
and of course the danger is that you misinterpret some feature of the sample

1724
01:47:38,510 --> 01:47:44,220
as something that's a fundamental and true whereas actually is just a coincidence that particular

1725
01:47:44,220 --> 01:47:46,560
sample that you had that particular

1726
01:47:46,640 --> 01:47:50,420
feature present so the

1727
01:47:50,430 --> 01:47:52,930
the aim of learning theory is to try and

1728
01:47:52,950 --> 01:47:57,380
you know gauge that likelihood that you'll be misled in some sense by the sample

1729
01:47:57,380 --> 01:48:00,070
to conclude something that's actually not true

1730
01:48:00,070 --> 01:48:05,510
from you know some specific configuration of the data that you happen to observe

1731
01:48:05,590 --> 01:48:09,180
because you could take a second sample and do the same thing but you're

1732
01:48:09,220 --> 01:48:11,360
you that would just check for one

1733
01:48:11,380 --> 01:48:15,570
one sort of thing you might be interested in the

1734
01:48:15,580 --> 01:48:18,320
in learning were generally interested in checking for

1735
01:48:18,370 --> 01:48:21,820
a whole swathe of properties at the same time

1736
01:48:21,880 --> 01:48:23,630
you know trying to fit the whole

1737
01:48:23,630 --> 01:48:27,300
the function class and picking out the best function in the whole class

1738
01:48:27,320 --> 01:48:31,050
after that data so we need to be able to check if you like how

1739
01:48:31,070 --> 01:48:34,680
we're being misled on any of these functions not just on one of them

1740
01:48:34,750 --> 01:48:39,240
and that's the key behind much of the analysis is trying to see how much

1741
01:48:39,240 --> 01:48:43,320
information we can actually squeeze out of a finite sample

1742
01:48:43,340 --> 01:48:48,820
that will allow us to be confident about our conclusions we make over a whole

1743
01:48:48,820 --> 01:48:50,620
swathe of

1744
01:48:52,810 --> 01:48:55,320
functions or analysis that we might make

1745
01:48:55,330 --> 01:48:59,870
so in many cases in many ways it's connected to multiple hypothesis testing if you're

1746
01:48:59,920 --> 01:49:03,920
familiar with statistics of hypothesis testing where

1747
01:49:03,950 --> 01:49:08,370
you might have a no hypothesis you want to test and you observe sample and

1748
01:49:08,380 --> 01:49:12,050
you see whether it's likely that hypothesis will be true

1749
01:49:12,060 --> 01:49:13,530
given that sample

1750
01:49:13,570 --> 01:49:18,280
and you might do multiple hypothesis testing away machine learning can be seen as doing

1751
01:49:18,280 --> 01:49:19,620
very very many

1752
01:49:19,630 --> 01:49:23,150
multiple hypothesis tests and trying to do that in an efficient

1753
01:49:23,180 --> 01:49:25,960
and reliable way

1754
01:49:25,970 --> 01:49:31,110
OK so that's the that's just point one one point two aim of any theory

1755
01:49:31,110 --> 01:49:33,330
is to model real world

1756
01:49:33,360 --> 01:49:37,690
an artificial phenomena because we want to exploit them we want to understand

1757
01:49:37,700 --> 01:49:39,570
why things are

1758
01:49:39,580 --> 01:49:43,030
the way they are in order to leave the that

1759
01:49:43,080 --> 01:49:45,330
knowledge so for instance if

1760
01:49:45,340 --> 01:49:50,380
we can be confident that this inference that we're making from a finite sample is

1761
01:49:51,330 --> 01:49:52,400
then we can be

1762
01:49:52,420 --> 01:49:55,630
exploit that to analyse new data

1763
01:49:55,670 --> 01:49:58,680
and you get accurate predictions on new data

1764
01:49:58,720 --> 01:50:03,920
until we have that confidence then we're in the risk of not being able to

1765
01:50:03,930 --> 01:50:06,740
in accurately predict data

1766
01:50:07,720 --> 01:50:13,020
statistical learning theory is just one approach to understanding predicting exploiting learning systems

1767
01:50:13,090 --> 01:50:18,330
others include is i've already mentioned bayesian inference inductive inference is another

1768
01:50:18,340 --> 01:50:23,020
statistical physics has been used in this connection in scottish traditional statistical analysis there are

1769
01:50:23,020 --> 01:50:24,890
many competing theories here

1770
01:50:24,900 --> 01:50:30,650
and i'm certainly not trying to imply statistical learning theory is the best

1771
01:50:30,670 --> 01:50:33,920
in any sort of absolute sense it has

1772
01:50:33,950 --> 01:50:37,400
each theory will will have its own

1773
01:50:37,410 --> 01:50:39,340
strengths and weaknesses

1774
01:50:39,350 --> 01:50:44,540
so each theory will make some assumptions about the phenomenon of learning and based on

1775
01:50:44,540 --> 01:50:47,910
these derives predictions of behaviour

1776
01:50:47,930 --> 01:50:53,500
and as i have already sort of alluded to every predictive theory automatically implies the

1777
01:50:53,520 --> 01:50:56,070
possible algorithm because if you know

1778
01:50:56,110 --> 01:50:57,660
about your prediction

1779
01:50:57,690 --> 01:51:01,820
then you can simply optimise the quantities that improve the predictions

1780
01:51:01,830 --> 01:51:05,910
so if you know for instance that the number of features will impact on the

1781
01:51:05,910 --> 01:51:08,210
quality of your learning

1782
01:51:08,290 --> 01:51:11,950
then you can try the smaller the better than you can try and keep the

1783
01:51:11,950 --> 01:51:13,470
number of features small

1784
01:51:13,480 --> 01:51:15,960
and survive algorithms that do that

1785
01:51:15,970 --> 01:51:19,860
or if you you know if you know your VC dimension is is something

1786
01:51:19,890 --> 01:51:24,070
critical feature you try keep that small while obviously getting a good fit on the

1787
01:51:25,000 --> 01:51:29,640
so these these sort of natural consequences of any theory

1788
01:51:29,800 --> 01:51:34,070
and this would be my argument about why theories good here is good if it

1789
01:51:34,710 --> 01:51:38,370
drive you to new and better algorithms that will perform

1790
01:51:38,430 --> 01:51:44,350
in more efficient and more of efficiency isn't dealt with here but more and more

1791
01:51:44,370 --> 01:51:48,640
with high predictive accuracy

1792
01:51:48,660 --> 01:51:52,770
so the theory is have strengths and weaknesses of already mentioned

1793
01:51:53,070 --> 01:51:58,430
generally speaking the more assumptions you make the more accurate predictions all the more sort

1794
01:51:58,430 --> 01:52:00,870
of refine predictions you can make

1795
01:52:00,930 --> 01:52:02,150
but of course

1796
01:52:04,230 --> 01:52:10,820
predictions will be dependent on those assumptions being correct or approximately correct mean most assumptions

1797
01:52:10,820 --> 01:52:12,450
that you make can never be

1798
01:52:12,460 --> 01:52:14,120
exactly verified

1799
01:52:14,150 --> 01:52:16,000
and probably untrue

1800
01:52:16,030 --> 01:52:20,530
so the question is how few abstracted some assumptions that are reasonably good

1801
01:52:20,570 --> 01:52:26,190
and actually capture the key phenomenon what's going on what have you put in some

1802
01:52:26,200 --> 01:52:29,900
if you like poison into the mix that actually is completely miss

1803
01:52:29,920 --> 01:52:32,070
skewing your analysis

1804
01:52:32,080 --> 01:52:35,590
and making your predictions are inaccurate

1805
01:52:35,610 --> 01:52:40,060
so it depends on capturing the right aspects of the phenomenon

1806
01:52:40,070 --> 01:52:45,030
and obviously you want to try and keep it as general as possible but if

1807
01:52:45,030 --> 01:52:48,670
it's too general then you won't be able to say very much and

1808
01:52:48,710 --> 01:52:50,270
in the way you know the the

1809
01:52:50,280 --> 01:52:53,970
maybe the distinction between the

1810
01:52:53,980 --> 01:53:00,030
bayesian and perhaps statistical models the bayesian typically will make more assumptions about some prior

1811
01:53:00,030 --> 01:53:02,570
distributions and noise models

1812
01:53:02,590 --> 01:53:05,840
but in many cases they seem to be

1813
01:53:05,900 --> 01:53:10,570
maybe not exactly right but good enough and you get good results out statistical

1814
01:53:11,220 --> 01:53:17,320
learning theory makes the rather less assumptions in that it just assumes an underlying distribution

1815
01:53:17,370 --> 01:53:23,400
and the training data is drawn i i d independently identically from that distribution

1816
01:53:23,410 --> 01:53:27,470
so it seems like a weaker assumption and

1817
01:53:27,490 --> 01:53:32,680
typically you may get weaker results for that reason but

1818
01:53:32,720 --> 01:53:35,370
in fact you do get surprisingly strong

1819
01:53:35,390 --> 01:53:39,850
results given the weakness of those assumptions but even those assumptions are not necessarily going

1820
01:53:39,850 --> 01:53:43,530
to be true in any practical application it may not be the case that your

1821
01:53:43,530 --> 01:53:46,370
training data is independently generated

1822
01:53:46,370 --> 01:53:48,250
true beliefs he roughly seems to know where the red

1823
01:53:50,140 --> 01:53:54,620
here's another video which is much more famous that's a classic in social psychology etc etc

1824
01:53:56,220 --> 01:53:59,800
two psychologist heider and simmel in this study that was published in the nineteen forties

1825
01:54:00,050 --> 01:54:02,670
just curious how many people are familiar with the site and some of it

1826
01:54:03,210 --> 01:54:04,790
i mean it's it's it's so striking

1827
01:54:05,590 --> 01:54:08,950
if you haven't seen it before r and all you remember from this talk is this video great

1828
01:54:10,450 --> 01:54:15,120
so again you could pass this has two triangles moving in a circle moving

1829
01:54:15,880 --> 01:54:17,670
and some other lines but you don't see it that way you see

1830
01:54:18,190 --> 01:54:21,410
objects that are beginning to each other in any again you feel like they are

1831
01:54:21,410 --> 01:54:25,110
intentional agents with stories and desires it looks like the big triangles kind

1832
01:54:26,160 --> 01:54:28,970
was displayed again kind of maybe billing the little triangle

1833
01:54:30,150 --> 01:54:33,250
we are bringing in him and then is gonna be sort of the

1834
01:54:33,660 --> 01:54:36,910
the circle kinda seems to observe and now is going to hide inside and once

1835
01:54:37,110 --> 01:54:39,470
the big triangles kinda scared of the little triangle there

1836
01:54:39,950 --> 01:54:43,400
he goes back and goes in there and sort key ominous scary music

1837
01:54:44,820 --> 01:54:48,350
if you haven't seen this video you can watch it on youtube and now we

1838
01:54:48,350 --> 01:54:51,240
are left a kind of moment suspense don't worry it ends happily

1839
01:54:51,760 --> 01:54:53,880
these are the two and three characters

1840
01:54:54,380 --> 01:54:56,040
right so what's going on here again

1841
01:54:56,430 --> 01:54:56,680
you know

1842
01:54:57,130 --> 01:55:00,800
from just what what you can think of is you just a few numbers in

1843
01:55:00,800 --> 01:55:04,520
time these are low dimensional time series if you think about how many what how

1844
01:55:04,520 --> 01:55:07,800
much information is required to describe these movies are not nearly as rich as the

1845
01:55:07,820 --> 01:55:09,240
full natural scene contrast

1846
01:55:09,700 --> 01:55:14,190
strikingly with what sebastian's services the really massive data coming in

1847
01:55:14,780 --> 01:55:17,440
you know to it from a street scene to google's self driving car

1848
01:55:18,050 --> 01:55:20,550
here you have very little data but look at how much you see you see

1849
01:55:20,880 --> 01:55:27,370
real physics objects with forced dynamics you see real psychology beliefs desires intentions emotions and

1850
01:55:27,370 --> 01:55:31,450
even more what you might call things like sort of morality in sociology like who's

1851
01:55:31,450 --> 01:55:35,640
on who's team who the good guys bad guys most people here see the big

1852
01:55:35,640 --> 01:55:38,740
triangle is kind a bad guy and the other two being on a team so

1853
01:55:38,740 --> 01:55:40,790
how do you do that how do you get so much from so little

1854
01:55:41,490 --> 01:55:43,950
this is the kind of thing we want to understand and i think if we

1855
01:55:43,950 --> 01:55:45,850
can even make progress on just those last

1856
01:55:46,290 --> 01:55:48,270
steps and then building up the things like

1857
01:55:48,670 --> 01:55:52,170
you know kids with air with their blocks and building up to real common sense

1858
01:55:52,170 --> 01:55:54,250
physics and psychology in natural scenes

1859
01:55:55,200 --> 01:55:55,750
would be somewhere

1860
01:55:56,850 --> 01:55:58,730
OK so the approach that we've been pursuing

1861
01:55:59,570 --> 01:56:02,590
well i would say you know over the last ten or fifteen years or so

1862
01:56:02,590 --> 01:56:07,560
in computational cognitive science is very much one i think many people recognise here is

1863
01:56:07,670 --> 01:56:09,960
is in common with a lot of the state art

1864
01:56:11,560 --> 01:56:16,440
a clear recognition the importance statistics both in data entry inference but the really the

1865
01:56:16,440 --> 01:56:20,530
heart things is about the interaction between statistics and knowledge in all these cases the

1866
01:56:20,530 --> 01:56:23,780
data on its own it's observed isn't enough to make the difference is he makes

1867
01:56:23,850 --> 01:56:27,400
something else have to fill in the gaps some other kind of abstract background knowledge

1868
01:56:27,640 --> 01:56:31,020
and the key questions have to do with the interaction between us how does background

1869
01:56:31,020 --> 01:56:35,420
knowledge guide these inferences from such sparse data what forms that knowledge take across

1870
01:56:35,840 --> 01:56:39,240
we all these different domains and tasks and maybe most interestingly how like how many

1871
01:56:39,240 --> 01:56:43,050
get in there if the brain is you know doesn't isn't programmed by a smart

1872
01:56:43,050 --> 01:56:47,900
engineer r in some sense has the program itself learning about technology is absolutely critical

1873
01:56:48,630 --> 01:56:50,510
so the kind of ideas that we've been pursuing

1874
01:56:51,060 --> 01:56:52,610
are shown here in red

1875
01:56:53,240 --> 01:56:57,810
i broadly were taking a bayesian approach you know probabilistic inference framework wear

1876
01:56:58,200 --> 01:57:01,790
the heart of things is some kind of a generative model some kind of hypothesis

1877
01:57:01,790 --> 01:57:03,930
space ways to to account for the data

1878
01:57:04,960 --> 01:57:09,410
and some kind prior over those but you will go well beyond anything that looks just like bayes rule

1879
01:57:09,810 --> 01:57:14,220
work hours and they i would not talking about exact bayesian inference we're talking about

1880
01:57:14,220 --> 01:57:18,120
some kind of approximation because these computations are going to be intractable to do in

1881
01:57:18,120 --> 01:57:19,270
any exact way

1882
01:57:19,920 --> 01:57:22,500
on end again has has pretty much now

1883
01:57:23,050 --> 01:57:23,730
i think you know

1884
01:57:24,160 --> 01:57:25,740
well recognized in this community

1885
01:57:26,170 --> 01:57:29,810
but it's one that say the statistical machine learning community has only more recently coming

1886
01:57:29,810 --> 01:57:31,230
to and sometimes not even at all

1887
01:57:32,090 --> 01:57:35,810
and is and is also still quite new in the cognitive science and neuroscience community

1888
01:57:36,180 --> 01:57:39,500
statistics is not just something you do with numbers it's not just something you do

1889
01:57:39,500 --> 01:57:43,410
in high dimensional vector spaces but it's something that you can do with real knowledge

1890
01:57:43,590 --> 01:57:48,260
with representations of the world and data that look much more like these were earlier

1891
01:57:48,260 --> 01:57:49,600
classic i approaches

1892
01:57:50,030 --> 01:57:54,810
what various kinds of symbolic knowledge representations graphs grammars schemas predicate logic

1893
01:57:55,470 --> 01:57:59,180
end i think in in in common with a number of people in this community

1894
01:57:59,500 --> 01:58:03,790
but the number one thing we need to do to understand build tools that allow

1895
01:58:03,790 --> 01:58:09,110
us to understand this kind of these aspects of human intelligence is understand how statistical

1896
01:58:09,110 --> 01:58:13,080
inference and learning can operate with these kinds of representations and that's that takes us

1897
01:58:13,080 --> 01:58:18,060
beyond what is an unfortunate common legacy of these two fields which which is you

1898
01:58:18,060 --> 01:58:23,410
know decades fighting over statistics versus structured representations it's nice that you know in the

1899
01:58:23,410 --> 01:58:27,590
last decade we've seen how these approaches can interweaving really deep in interesting ways

1900
01:58:28,310 --> 01:58:31,270
maybe most exciting is this idea which i'll try to get to by the end

1901
01:58:31,270 --> 01:58:35,560
of what we call probabilistic programs so we're doing probabilistic reasoning

1902
01:58:36,240 --> 01:58:41,490
not just over things like grammars predicate logic but actual programs in particular be looking

1903
01:58:41,490 --> 01:58:44,860
at functional programs so list is is come back and our work in a certain

1904
01:58:45,820 --> 01:58:47,330
and don't try to argue why this is

1905
01:58:47,760 --> 01:58:50,240
i think an approach that subsumes a lot of what we and others have been

1906
01:58:50,240 --> 01:58:54,330
doing with other kinds approach structured probabilistic models and in a powerful way for common

1907
01:58:54,330 --> 01:58:54,900
sense reasoning

1908
01:58:55,440 --> 01:59:00,310
and this this last maybe most compelling question how much knowledge itself acquired well here

1909
01:59:00,310 --> 01:59:03,100
we've been drawing on ideas have become very popular in these

1910
01:59:03,590 --> 01:59:08,060
bayesian machine learning community and themselves to run the business statistics community what are called

1911
01:59:08,060 --> 01:59:13,390
hierarchical models hierarchical bayes we don't just have one level of the hypotheses generating data

1912
01:59:14,250 --> 01:59:19,620
prior there but but multiple levels hypothesis space of hypotheses and priors on priors and

1913
01:59:19,620 --> 01:59:23,550
by doing inference at all levels of the hierarchy you can explain how some piece

1914
01:59:23,680 --> 01:59:27,810
abstract knowledge in a certain level isn't just useful in guiding inference lower level but

1915
01:59:27,810 --> 01:59:31,830
itself might be learned over say broader timescale are proper sources of data

1916
01:59:32,590 --> 01:59:36,620
up by inference at higher levels of abstraction and when you put all these things

1917
01:59:36,620 --> 01:59:40,970
together you put hierarchical bayes together with the idea of a probabilistic programs as a

1918
01:59:40,970 --> 01:59:46,810
basic knowledge representation then you're talking about these hierarchies programs that generate other programs again

1919
01:59:46,810 --> 01:59:51,110
an idea that has has very important early precursors if people remember the work of

1920
01:59:51,110 --> 01:59:52,090
people like feldman

1921
01:59:52,630 --> 01:59:56,330
warning on grammar grammars the earliest probabilistic grammar induction approaches

1922
01:59:57,390 --> 02:00:01,640
and it's sort of a kind of a hierarchical bayesian approach to learning as programme induction

1923
02:00:02,040 --> 02:00:04,740
this is just something we can glimpse at this point and maybe at the end

1924
02:00:04,750 --> 02:00:08,110
of show you a little bit of demo going without but it's that's just

1925
02:00:08,590 --> 02:00:09,550
you know that's mostly and

1926
02:00:10,020 --> 02:00:13,580
as sebastian so you invite me back in two thousand twenty eight or something and

1927
02:00:13,780 --> 02:00:15,330
maybe will actually have that's all

1928
02:00:16,130 --> 02:00:17,010
OK so

1929
02:00:17,560 --> 02:00:21,540
i let at what i wanna do here is in in this talk is to

1930
02:00:21,540 --> 02:00:24,800
give you a bit of the tour through the way we've been using these ideas

1931
02:00:25,020 --> 02:00:25,700
to understand

1932
02:00:26,150 --> 02:00:28,090
these these hard problems of human cognition

1933
02:00:28,650 --> 02:00:33,260
of things like learning from very few examples or common sense intuitive physics and psychology

1934
02:00:33,680 --> 02:00:37,720
i want take one step back just because you know you're coming from a different

1935
02:00:37,720 --> 02:00:41,380
field and just a little bit again in the context still see a parallel with

1936
02:00:41,580 --> 02:00:45,830
with i feel when i got into cognitive science was in the mid nineties basically

1937
02:00:45,830 --> 02:00:46,370
in grad school

1938
02:00:46,880 --> 02:00:50,780
i'm just as just as in any i there was a lot of skepticism over

1939
02:00:50,780 --> 02:00:55,670
whether statistics and and probabilistic inference was even relevant probably many people are familiar with

1940
02:00:55,670 --> 02:00:59,840
the work comment averse key very important work justly deserving of the nobel prize the

1941
02:00:59,860 --> 02:01:01,780
column and one in economics in two thousand two

1942
02:01:02,300 --> 02:01:06,530
but the headline this work for psychologists was basically people aren't bayesian

1943
02:01:06,950 --> 02:01:10,930
they gave people and a lot of other researchers in the field of judgment decision-making

1944
02:01:11,560 --> 02:01:16,540
around and following them people very very basic bayesian stats questions like these things simpler

1945
02:01:16,600 --> 02:01:21,150
problems like you've drawn a certain sample red-and-white ships had this surname veteran and they

1946
02:01:21,150 --> 02:01:25,220
the battle scale region average force of about twenty militants

1947
02:01:25,270 --> 02:01:27,680
four times five

1948
02:01:27,740 --> 02:01:30,790
if you want to males but if they were super bowls which would bounce up

1949
02:01:30,800 --> 02:01:35,680
then the momentum change would be doubled and so the bathroom scales would indicate

1950
02:01:35,690 --> 02:01:36,810
forty newtons

1951
02:01:36,810 --> 02:01:42,600
so this is a real thing it's a real force that you can record

1952
02:01:42,660 --> 02:01:44,100
now i'm going to be

1953
02:01:44,160 --> 02:01:46,630
a little bit unpleasant to you

1954
02:01:46,660 --> 02:01:48,100
i'm going to throw

1955
02:01:48,130 --> 02:01:49,780
rotten tomatoes you

1956
02:01:50,880 --> 02:01:53,980
u i

1957
02:01:54,040 --> 02:01:55,560
and i have here

1958
02:01:55,570 --> 02:01:57,760
the tomato

1959
02:01:57,800 --> 02:01:59,310
and this tomato

1960
02:01:59,360 --> 02:02:02,080
has zero speed to start with

1961
02:02:02,100 --> 02:02:07,280
let me make your little bigger otherwise i won't even

1962
02:02:07,450 --> 02:02:09,560
and even get you

1963
02:02:11,460 --> 02:02:12,450
so this issue

1964
02:02:12,650 --> 02:02:17,870
and so i give this domain always certain velocity

1965
02:02:17,950 --> 02:02:21,020
the of x

1966
02:02:21,020 --> 02:02:24,530
the tornado hits you

1967
02:02:24,550 --> 02:02:27,080
maybe it stays there is possible

1968
02:02:28,230 --> 02:02:29,760
it will drip down

1969
02:02:29,760 --> 02:02:31,130
but in any case

1970
02:02:31,150 --> 02:02:34,540
the velocity actor action is gone

1971
02:02:34,540 --> 02:02:36,490
so it's you

1972
02:02:36,610 --> 02:02:38,910
with the velocity vx

1973
02:02:38,970 --> 02:02:40,110
and then

1974
02:02:40,150 --> 02:02:41,110
the effects

1975
02:02:41,190 --> 02:02:43,690
equals zero

1976
02:02:43,730 --> 02:02:46,930
and may make a mess

1977
02:02:46,980 --> 02:02:52,270
you will experience a force if i keep throwing tomatoes at you all the time

1978
02:02:52,320 --> 02:02:54,540
and this is the force you will experience

1979
02:02:54,580 --> 02:02:55,980
and that force

1980
02:02:56,050 --> 02:02:57,120
is of course

1981
02:02:57,130 --> 02:03:01,080
in this direction

1982
02:03:01,220 --> 02:03:05,410
all these tomatoes

1983
02:03:05,520 --> 02:03:07,370
you feel that as the fourth

1984
02:03:07,420 --> 02:03:10,240
but now look at the symmetry of the problem

1985
02:03:10,290 --> 02:03:14,020
here the velocity goes from px two zero

1986
02:03:14,130 --> 02:03:17,490
but i throwing tomatoes

1987
02:03:17,550 --> 02:03:20,910
as to increase the velocity from zero to three x

1988
02:03:20,970 --> 02:03:22,560
so for obvious reasons

1989
02:03:22,570 --> 02:03:27,460
i must then few force this direction think of it as record

1990
02:03:27,550 --> 02:03:29,340
you fired a bullet

1991
02:03:29,350 --> 02:03:31,670
so i experience exactly the same force

1992
02:03:31,720 --> 02:03:32,960
but in this direction

1993
02:03:33,070 --> 02:03:34,030
and that no

1994
02:03:34,040 --> 02:03:35,120
is the idea

1995
02:03:35,130 --> 02:03:37,280
behind a rock

1996
02:03:37,350 --> 02:03:38,520
a rocket

1997
02:03:38,540 --> 02:03:43,360
is spewing out tomatoes well not quite tomatoes spewing out hot gas

1998
02:03:43,480 --> 02:03:45,870
in this direction

1999
02:03:45,870 --> 02:03:49,380
and then the rocket will experience a force in that direction

2000
02:03:49,390 --> 02:03:51,130
that is the basic concept

2001
02:03:54,350 --> 02:03:57,080
and the higher the speed of the gas

2002
02:03:57,090 --> 02:03:58,610
that it throws out

2003
02:03:58,660 --> 02:04:02,050
the higher the velocity

2004
02:04:02,120 --> 02:04:05,430
the more kilograms per second it spits out

2005
02:04:05,490 --> 02:04:07,410
the higher the MDT

2006
02:04:07,460 --> 02:04:09,640
hi will be the force on the rocket

2007
02:04:09,700 --> 02:04:11,920
and this force on the rocket is called

2008
02:04:11,970 --> 02:04:13,330
the trust

2009
02:04:13,430 --> 02:04:17,280
of the rocket

2010
02:04:17,320 --> 02:04:21,370
so if we have a rocket in space

2011
02:04:21,440 --> 02:04:27,330
use the rocket

2012
02:04:27,400 --> 02:04:31,730
and the rocket is spewing out gas

2013
02:04:31,770 --> 02:04:33,320
with a velocity

2014
02:04:34,650 --> 02:04:37,060
which is fixed relative

2015
02:04:39,060 --> 02:04:41,660
the burning of chemical energy

2016
02:04:41,710 --> 02:04:44,790
chemicals are burnt it comes out with a certain speed

2017
02:04:44,790 --> 02:04:47,560
and the rocket will then experience

2018
02:04:47,610 --> 02:04:49,020
a force

2019
02:04:49,040 --> 02:04:50,260
which we call

2020
02:04:50,310 --> 02:04:51,630
the trust

2021
02:04:51,680 --> 02:04:53,520
and that is given by this equation

2022
02:04:53,520 --> 02:04:56,450
if you know how many kilograms per second that spewed out

2023
02:04:56,470 --> 02:04:59,630
and you know what the velocities which i've called you you

2024
02:04:59,630 --> 02:05:03,960
this will tell you about the thrust this of that rocket

2025
02:05:04,010 --> 02:05:05,450
if we take

2026
02:05:05,470 --> 02:05:07,370
the case of

2027
02:05:07,450 --> 02:05:10,480
the centre rockets that were used

2028
02:05:10,560 --> 02:05:13,300
for the landing on the moon

2029
02:05:18,120 --> 02:05:19,700
for the saturn rocket the

2030
02:05:19,720 --> 02:05:21,910
speed you

