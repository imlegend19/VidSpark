1
00:00:00,000 --> 00:00:05,570
it happened first with the aesthetics of the sublime which imposed the radical change

2
00:00:06,120 --> 00:00:11,730
the way people saw the ugly by analyzing our reaction to natural phenomena dominated

3
00:00:11,740 --> 00:00:15,880
by the formless the painful and the terrifying

4
00:00:16,200 --> 00:00:19,180
thus we sense the sublime when seeing

5
00:00:19,220 --> 00:00:28,750
storm rough see rugged clifs glaciers abyss boundless stretches of land caves and waterfalls when

6
00:00:28,750 --> 00:00:33,400
we can appreciate emptiness darkness solitude silence

7
00:00:33,440 --> 00:00:41,640
and the storm only pressure that can prove delightful when we feel horror for something

8
00:00:41,640 --> 00:00:46,290
that cannot possess us and cannot harm us

9
00:00:46,490 --> 00:00:52,180
on his essay on tragic art schiller observed that it is a general

10
00:00:52,180 --> 00:00:58,120
phenomenon of our nature that sad terrible even horrific things

11
00:00:58,120 --> 00:01:01,420
are irresistibly attractive to us

12
00:01:01,640 --> 00:01:06,990
and that scenes of suffering and terror repel and attract us with equal power

13
00:01:07,040 --> 00:01:13,990
and that we greedily devour those stories that make our hair stand on end

14
00:01:15,830 --> 00:01:17,990
it was that spirit that led

15
00:01:18,070 --> 00:01:19,970
a few decades

16
00:01:20,140 --> 00:01:28,250
previously to the Gothic novel which with its ruined castles and monasteries terrifying bones of

17
00:01:28,250 --> 00:01:35,200
bloody crimes diabolical operations and decomposed bodies

18
00:01:35,270 --> 00:01:41,030
thus among the protagonist of the romantic drama we see the damned hero

19
00:01:41,400 --> 00:01:48,170
like in Byron the villains be found in sue balzac emily bronte

20
00:01:50,730 --> 00:01:57,660
but the most ardent romantic eulogy of ugliness came with victor hugo's

21
00:01:57,660 --> 00:02:00,790
preface to his play cromwell

22
00:02:01,120 --> 00:02:06,140
the ugliness that Hugo saw as typical of the new aesthetics was the

23
00:02:06,140 --> 00:02:14,030
grotesque the deformed horrible repellant thing transported with truth and poetry to the

24
00:02:14,400 --> 00:02:15,530
realm of art

25
00:02:15,770 --> 00:02:22,880
and the grotesque was the most fecund of the sources that nature makes available

26
00:02:22,880 --> 00:02:25,250
to artistic creation

27
00:02:25,720 --> 00:02:34,460
as Remo Bodei observed hugo makes beauty turn full circle thus leading it to

28
00:02:34,460 --> 00:02:37,720
coincide with ugliness

29
00:02:38,060 --> 00:02:41,160
see for instance the the description of

30
00:02:41,270 --> 00:02:43,700
quasimodo the hunchback

31
00:02:43,730 --> 00:02:48,330
of notre dame we shall not give to the reader an idea of that tetrahedral

32
00:02:48,890 --> 00:02:56,300
that nose that horse shoe mouth that little left eye obstructed with a red bristling eyebrow

33
00:02:56,300 --> 00:03:01,270
with the right eye disappearing entirely beneath the enormous wart of those

34
00:03:02,160 --> 00:03:07,750
teeth in disarray broken here and there like the embattled parapet of the fortress of that

35
00:03:07,750 --> 00:03:13,170
callous lip upon which one of these teeth encroached like the tusk of an elephant

36
00:03:13,170 --> 00:03:18,660
of that forked chin and above all the expression spread over the whole of

37
00:03:18,660 --> 00:03:23,720
that mixture of malice amazement and sadness and so on and so and so forth

38
00:03:23,920 --> 00:03:29,750
and there is the description of l'homme qui rit the man who laughs gwynplaine

39
00:03:30,820 --> 00:03:37,070
has been prodigal over kindness to gwynplaine she had bestowed on him

40
00:03:37,070 --> 00:03:45,440
a mouth opening to his ears ears holding over to his eyes shapeless nose to support the spectacles

41
00:03:45,440 --> 00:03:51,030
of the grimase maker and the face that no one could look up

42
00:03:51,070 --> 00:03:53,050
without laughing

43
00:03:53,090 --> 00:03:54,550
but was it nature

44
00:03:55,230 --> 00:03:56,990
had she not been assisted

45
00:03:57,310 --> 00:04:02,220
two slits for eyes a hiatus for a mouth a

46
00:04:02,480 --> 00:04:06,440
snub protuberance with two holes for nostrilis

47
00:04:07,900 --> 00:04:14,290
a flattened face all having for the result an appearance of laughter

48
00:04:14,330 --> 00:04:20,940
it is certain that nature never produces such perfection single handed

49
00:04:20,940 --> 00:04:23,480
but is laughter synonym of joy

50
00:04:23,660 --> 00:04:30,250
such a face could never have been created by chance it must have resulted from intention

51
00:04:30,650 --> 00:04:34,720
had Gwynplaine when a child been so worthy of attention that his face

52
00:04:34,720 --> 00:04:38,510
had been subjected to transmutation

53
00:04:38,550 --> 00:04:39,420
why not

54
00:04:39,550 --> 00:04:46,810
industrious manipulators of children had worked upon his face it's inevident

55
00:04:46,830 --> 00:04:52,750
that a mysterious and probably occult science which was to surgery what alchemy

56
00:04:52,750 --> 00:05:00,850
was to chemistry has chiselled that flesh evidently at a very tender age and manufactured his

57
00:05:00,850 --> 00:05:07,380
countenance with premeditation that science clever with the knife skilled in

58
00:05:07,380 --> 00:05:09,650
any the crew

59
00:05:09,720 --> 00:05:14,690
so as do you would want to avoid having several minima

60
00:05:18,000 --> 00:05:21,580
there's no way you're going to your function is going to go up and down

61
00:05:21,580 --> 00:05:25,650
and up and down because if you go up and down then you're going to

62
00:05:25,650 --> 00:05:29,940
have maximise the potential use of minimal

63
00:05:32,580 --> 00:05:36,130
so it looks like

64
00:05:36,160 --> 00:05:39,400
the derivative of your functions then

65
00:05:39,460 --> 00:05:41,850
should have always the same sign

66
00:05:41,870 --> 00:05:45,070
if you can if you go up and down

67
00:05:45,080 --> 00:05:49,100
then the derivative of your of your function is going

68
00:05:49,120 --> 00:05:51,380
it when you go down it's going to be

69
00:05:51,390 --> 00:05:54,370
sorry when you go

70
00:05:58,870 --> 00:06:00,710
and change once

71
00:06:04,460 --> 00:06:07,230
in it is if you think about it the more

72
00:06:07,250 --> 00:06:11,630
then the only way that you have four

73
00:06:11,670 --> 00:06:16,010
to be sure that the function as i only one minimum is to have it

74
00:06:17,120 --> 00:06:19,100
which is

75
00:06:19,280 --> 00:06:22,120
those kinds of functions there

76
00:06:22,160 --> 00:06:26,920
and what's the property of a convex function it's the property that

77
00:06:26,950 --> 00:06:33,830
she would take to any two points

78
00:06:33,830 --> 00:06:41,830
and if you are the segment between f of x in f of y x

79
00:06:41,830 --> 00:06:45,540
and y then this segment is always of above the

80
00:06:45,570 --> 00:06:47,210
the curvature of function

81
00:06:48,230 --> 00:06:50,780
that's the definition of a convex function

82
00:06:50,790 --> 00:06:55,820
and it happens that it's equivalent if you know in advance that f is twice

83
00:06:57,530 --> 00:07:03,030
then being convex is just that the second derivative is is

84
00:07:03,040 --> 00:07:04,480
was it

85
00:07:06,870 --> 00:07:13,220
of comics functions when you find the point where the derivative is zero

86
00:07:13,270 --> 00:07:17,960
then you are guaranteed its minimum

87
00:07:17,960 --> 00:07:21,810
i think pretty much everything that's on this that is what we said about

88
00:07:21,880 --> 00:07:25,460
functions when when we don't know there are convex

89
00:07:25,570 --> 00:07:29,500
except that the only thing we haven't talked about it is

90
00:07:32,380 --> 00:07:34,810
if this continues to grow

91
00:07:35,440 --> 00:07:40,760
so the minimum necessary and sufficient condition for

92
00:07:40,810 --> 00:07:46,830
having only one global minimum for a convex function is is that it is corrosive

93
00:07:46,830 --> 00:07:52,390
which is the definition of their which means basically that if if you have

94
00:07:52,450 --> 00:07:57,450
a continuous function in your sure that in the

95
00:07:57,580 --> 00:08:03,560
when when x goes to minus operators infinity then the function is going to plus

96
00:08:05,510 --> 00:08:08,180
since the function is continuous there's

97
00:08:08,240 --> 00:08:10,150
there's going to be some

98
00:08:10,150 --> 00:08:12,530
some points here

99
00:08:12,590 --> 00:08:15,220
words where there is a minimum

100
00:08:16,230 --> 00:08:19,860
so if you have a continuous function and you know that the infinity goes to

101
00:08:19,860 --> 00:08:23,520
infinity then there's a i fixed one meaning

102
00:08:25,250 --> 00:08:26,570
what this is there

103
00:08:26,590 --> 00:08:32,840
so a minimizer is the point where the derivative is zero

104
00:08:32,850 --> 00:08:37,340
if it's square differentiable then you'd have to have

105
00:08:37,390 --> 00:08:44,940
the gradient being zero and the hessian be non negative semidefinite quantitative metrics again you

106
00:08:44,940 --> 00:08:48,850
can forget about that so i mean this is a positive matrix

107
00:08:48,860 --> 00:08:50,020
so what

108
00:08:50,030 --> 00:08:53,640
here instead of considering only functions

109
00:08:53,650 --> 00:08:58,620
that go to and i have read values are considered functions that maybe you have

110
00:08:58,850 --> 00:09:00,190
a vector values

111
00:09:00,250 --> 00:09:06,910
so your derivative this is is replaced by the gradient and your second derivative is

112
00:09:06,910 --> 00:09:12,390
if i get the signs wrong then will come back and fix

113
00:09:12,450 --> 00:09:15,390
it's about that's my average energy that's the entropy

114
00:09:15,430 --> 00:09:18,640
and what i'm going to do is i'm going to maximise its subject to the

115
00:09:18,640 --> 00:09:23,700
constraint these are marginal song to make every one of them nonnegative that's obvious

116
00:09:23,700 --> 00:09:27,720
and to make them some to one that sort of another obvious constraint

117
00:09:27,770 --> 00:09:30,100
those are not the interesting constraints

118
00:09:30,120 --> 00:09:34,970
interesting constraints are these ones that we've seen before going to make the joint marginals

119
00:09:34,970 --> 00:09:40,560
on the edges though should collapse down to the marginals on the nodes

120
00:09:40,560 --> 00:09:53,520
those are marginalisation constraints that's where the messages are going to pop out

121
00:10:00,020 --> 00:10:02,750
so that's an optimisation problem

122
00:10:02,770 --> 00:10:04,140
fourteen my

123
00:10:04,160 --> 00:10:06,930
energy go

124
00:10:06,970 --> 00:10:12,910
so we sort of remember it's an optimisation problem that involves these two terms

125
00:10:13,060 --> 00:10:16,430
better entropy in an average energy

126
00:10:16,450 --> 00:10:19,250
to constrained optimisation problem

127
00:10:19,290 --> 00:10:25,120
but the constraints are simple and nonnegativity constraints normalisation constraints and have our old friend

128
00:10:25,120 --> 00:10:27,930
the marginalisation constraints

129
00:10:28,120 --> 00:10:33,220
OK so the reason this is an important optimisation problem is because

130
00:10:33,290 --> 00:10:35,700
what the sum product algorithm is doing

131
00:10:35,770 --> 00:10:37,000
we're trying to do

132
00:10:37,020 --> 00:10:40,720
it is it's trying to solve this constrained optimisation problem

133
00:10:40,730 --> 00:10:45,930
so going to drive that are going to see the messages are lagrange multipliers

134
00:10:45,930 --> 00:10:51,770
right so it's trying to solve it because in general as i said that's not

135
00:10:51,770 --> 00:10:56,330
going to be a concave maximisation problem so there could be multiple optima and when

136
00:10:56,330 --> 00:10:59,970
you have multiple optima you can't really say anything about an algorithm that you don't

137
00:10:59,970 --> 00:11:03,770
know if you're getting caught in the valley are not so that's one reason why

138
00:11:03,770 --> 00:11:07,060
some products without reweighting can be problematic

139
00:11:07,060 --> 00:11:12,370
when we make the entropy is concave function then

140
00:11:12,410 --> 00:11:15,640
if we look at the energy the energy is just a linear function

141
00:11:16,580 --> 00:11:22,080
the marginals right it's linear because when you take expectations that linear function

142
00:11:22,100 --> 00:11:28,580
taking expectations linear that sort of an easy part of function

143
00:11:28,600 --> 00:11:32,290
so as long as we choose these weights so we make this a concave function

144
00:11:32,290 --> 00:11:33,830
or concave entropy

145
00:11:33,870 --> 00:11:38,390
then when we add them together concave plus linear is still concave

146
00:11:38,540 --> 00:11:42,140
we're going to get a nice well-behaved optimisation problem

147
00:11:42,220 --> 00:11:46,430
so that's the advantage of choosing the weights in good ways that you you'll get

148
00:11:46,430 --> 00:11:50,850
problems that have unique optimum and there's no issues about

149
00:11:50,850 --> 00:11:55,430
flailing and not not sure which optimum you go to

150
00:11:55,490 --> 00:12:03,620
OK so let's let's try this

151
00:12:03,620 --> 00:12:09,060
any questions before we that's one work through the lagrangian calculations because we haven't seen

152
00:12:09,060 --> 00:12:12,950
it it's not too hard and i think it's it's instructive you have a different

153
00:12:12,950 --> 00:12:18,250
perspective on where the sum product algorithm came from

154
00:12:18,310 --> 00:12:24,580
any questions before we we do that

155
00:12:29,580 --> 00:12:32,700
so e

156
00:12:32,850 --> 00:12:38,080
queries e

157
00:12:38,080 --> 00:12:45,750
right so that's

158
00:12:45,750 --> 00:12:47,810
fix those you're problem parameters

159
00:12:47,810 --> 00:12:52,200
if the marginal every node and you take an expectation of the problem parameters so

160
00:12:52,200 --> 00:12:55,370
it's a linear function of the marginals

161
00:13:07,430 --> 00:13:08,730
i guess

162
00:13:08,990 --> 00:13:12,890
this is why the derivation i'm doing that sort of obscure parts of the bigger

163
00:13:12,890 --> 00:13:18,310
picture so that the bigger picture which is covered more in the slides is that

164
00:13:18,330 --> 00:13:23,220
this function that have defined

165
00:13:27,620 --> 00:13:32,930
where we define it is given in sec

166
00:13:33,180 --> 00:13:44,040
right so this function this is what we're trying to compute the log likelihood that

167
00:13:44,040 --> 00:13:47,770
function is a convex function of the parameters that we use that when i sort

168
00:13:47,770 --> 00:13:49,640
of motivated the upper bound

169
00:13:49,680 --> 00:13:53,370
but whenever you have a convex function

170
00:13:53,390 --> 00:14:00,770
well if you're a convex optimisation kind of person be a convex function one thing

171
00:14:00,770 --> 00:14:03,200
you might do is take its dual

172
00:14:03,850 --> 00:14:07,500
so little even talk about conjugate tools

173
00:14:11,000 --> 00:14:13,890
what i could do is just to give you some sort of intuition where this

174
00:14:13,890 --> 00:14:15,390
is coming from

175
00:14:15,520 --> 00:14:19,640
i'm trying to compute the log likelihood that some function

176
00:14:19,640 --> 00:14:20,890
any function

177
00:14:20,910 --> 00:14:28,540
any convex function closed convex function closed is just a technical condition can be written

178
00:14:28,540 --> 00:14:33,140
as an optimisation problem that involves its

179
00:14:35,720 --> 00:14:37,970
it's a function in some dimension d

180
00:14:37,970 --> 00:14:41,890
it can be written as an optimisation problem that involves

181
00:14:41,930 --> 00:14:44,620
the conjugate dual function

182
00:14:44,640 --> 00:14:50,640
that's probably leaving went through that with you

183
00:14:52,060 --> 00:14:54,290
this is sort of saying that the dual of the dual

184
00:14:54,290 --> 00:14:57,470
under good conditions if you take the dual of the dual you get back the

185
00:14:58,370 --> 00:14:59,560
right cause

186
00:14:59,580 --> 00:15:02,020
that's the dual and i take the dual of it

187
00:15:02,080 --> 00:15:03,370
i get the double tool

188
00:15:03,370 --> 00:15:06,540
under good conditions that's the same thing

189
00:15:06,640 --> 00:15:11,120
good conditions hold here if you take the log likelihood is equal to this

190
00:15:11,120 --> 00:15:14,270
where this is some conjugate function

191
00:15:14,330 --> 00:15:18,540
so the question is what is the conjugate function

192
00:15:18,560 --> 00:15:21,620
the conjugate function in general is an entropy

193
00:15:22,640 --> 00:15:24,890
i for for any graphical model

194
00:15:24,910 --> 00:15:27,430
it's the distribution it has some entropy

195
00:15:27,450 --> 00:15:29,870
and that conjugate function is

196
00:15:29,890 --> 00:15:34,850
it's not the entropy it's the negative entropy because conjugate functions are always convex

197
00:15:34,910 --> 00:15:37,680
so because this guy is the entropy that's why

198
00:15:37,700 --> 00:15:41,250
in our variational problem that's where the better entropy comes from

199
00:15:41,290 --> 00:15:44,730
if i'd done this on the tree if i took the conjugate dual tree i

200
00:15:44,730 --> 00:15:49,060
would have gone tree entropy and it would have been better thing would be exact

201
00:15:49,560 --> 00:15:56,700
so that's why it's coming out because we're doing an approximation on the general graph

202
00:16:01,200 --> 00:16:07,790
actually little let me just let me just walk through this because this kind of

203
00:16:07,790 --> 00:16:13,180
relation what i'm saying here is that this becomes negative entropy

204
00:16:13,230 --> 00:16:16,100
if we do a simple example

205
00:16:16,120 --> 00:16:17,910
a toy example but

206
00:16:17,910 --> 00:16:22,160
it sort of good practice in computing tools and we'll see why entropy pops out

207
00:16:22,160 --> 00:16:25,540
so let's do that because the we see entropy come out in the simple case

208
00:16:25,540 --> 00:16:28,490
then i think it should make a bit more sense you why entropy comes out

209
00:16:28,490 --> 00:16:31,080
in the more complicated case

210
00:16:31,390 --> 00:16:41,540
right so let's some where are we

211
00:16:41,540 --> 00:16:44,750
so i put it she i just had this

212
00:16:44,750 --> 00:16:46,040
kind of

213
00:16:46,060 --> 00:16:51,330
simple thing but it's nice results and what the result well if you think about

214
00:16:52,130 --> 00:16:58,380
so this is actually is you with right you construct this like from the point

215
00:16:58,380 --> 00:17:03,650
clouds from unlabelled actually both labeled and and unlabeled and you get this

216
00:17:03,710 --> 00:17:06,110
she had this

217
00:17:06,130 --> 00:17:12,000
here it's like the sum over all labels class label what

218
00:17:12,000 --> 00:17:16,210
so before we had the represent theorem

219
00:17:16,230 --> 00:17:21,750
and then we also have representer theorems and basically you can think of keeping ghost

220
00:17:22,540 --> 00:17:25,810
you have a sum over all labeled and unlabeled point

221
00:17:25,960 --> 00:17:31,360
of this is this is the solution to to this problem and more of the

222
00:17:31,360 --> 00:17:34,670
interesting thing is you know you get the same

223
00:17:34,690 --> 00:17:36,480
basically the same thing

224
00:17:36,670 --> 00:17:41,810
you have an explicit form for the coefficients so for the case when the loss

225
00:17:41,810 --> 00:17:43,460
is quadratic

226
00:17:43,460 --> 00:17:46,230
this is just the matrix inversion

227
00:17:46,270 --> 00:17:49,270
so you have to to solve the system of linear equations

228
00:17:49,290 --> 00:17:53,060
now of course the typical one sentence labeled unlabeled points

229
00:17:53,110 --> 00:17:56,380
if you want to change was like in SVM

230
00:17:56,380 --> 00:18:03,770
then it's a simple problem the problem words quoted from same

231
00:18:03,790 --> 00:18:07,090
from the problem and support vector machines

232
00:18:07,110 --> 00:18:12,420
but now with this additional controls little bit

233
00:18:12,540 --> 00:18:16,270
so basically we have a support vector machine but we do not

234
00:18:22,710 --> 00:18:27,150
basically where you can sort of look at it works pretty well

235
00:18:27,250 --> 00:18:34,980
so there is some comparisons here but shares interesting to this is the thing that

236
00:18:34,980 --> 00:18:36,420
if you use

237
00:18:37,940 --> 00:18:42,070
there are some of the error is so are some labelled and unlabelled pork

238
00:18:42,090 --> 00:18:46,520
so suppose we just use only labeled points this is there

239
00:18:46,540 --> 00:18:51,090
so that you can ignore this if unlabeled point also

240
00:18:51,110 --> 00:18:56,090
so this is basically a little point should we get a little bit

241
00:18:56,090 --> 00:19:00,840
so this is the difference you can see the difference in some cases very dramatic

242
00:19:00,840 --> 00:19:02,840
twenty four four

243
00:19:02,840 --> 00:19:04,810
so definitely

244
00:19:04,830 --> 00:19:07,070
in all cases with some

245
00:19:07,090 --> 00:19:13,290
so definitely that unlabelled actually tells us the geometry of unlabeled data that's something interesting

246
00:19:14,170 --> 00:19:14,980
there is

247
00:19:15,000 --> 00:19:16,960
there is something

248
00:19:16,980 --> 00:19:22,330
utility which can be captured by using the using this graph

249
00:19:22,340 --> 00:19:27,770
now sort of moving to the next thing and the next thing you have to

250
00:19:28,630 --> 00:19:34,190
theoretical questions surrounding well remember there were two objects so this graph

251
00:19:34,210 --> 00:19:36,090
based things which is

252
00:19:36,090 --> 00:19:41,380
data dependent on there was another thing which is you know and alignment so now

253
00:19:41,420 --> 00:19:43,150
the question is

254
00:19:43,170 --> 00:19:44,110
you know it

255
00:19:44,110 --> 00:19:47,480
construction based on unlabeled

256
00:19:47,500 --> 00:19:55,250
but based on simple how does collective suppose it is really from the manifold suppose

257
00:19:55,270 --> 00:20:00,480
we know that the sample more and more data from with the question is actually

258
00:20:00,590 --> 00:20:02,170
convergence the this

259
00:20:02,250 --> 00:20:07,880
you know that this procedure faithful representation of the object claim to be

260
00:20:07,920 --> 00:20:11,090
it turns out that this is actually true

261
00:20:11,090 --> 00:20:15,500
and in fact what you control

262
00:20:16,650 --> 00:20:18,290
you can show that

263
00:20:19,730 --> 00:20:21,540
so here is the thing

264
00:20:23,460 --> 00:20:28,960
support a simple more and more points from an underlying probability distribution and the constructed

265
00:20:28,960 --> 00:20:33,290
graph as before and i need to choose the parameter remember there was this guy

266
00:20:33,290 --> 00:20:35,270
was in which i need choose this

267
00:20:35,290 --> 00:20:37,150
the width of the gaussians

268
00:20:37,170 --> 00:20:39,310
the function of

269
00:20:39,340 --> 00:20:44,480
in a way that it becomes more and more narrow as the number of questions

270
00:20:44,540 --> 00:20:49,250
so it very important regardless of course you don't get

271
00:20:49,270 --> 00:20:52,400
then i can vectors of

272
00:20:52,560 --> 00:20:55,040
so graph of cluster

273
00:20:55,060 --> 00:21:00,310
converge to again functions as

274
00:21:00,330 --> 00:21:02,650
so that's the problem main

275
00:21:02,710 --> 00:21:06,250
theoretical result

276
00:21:06,270 --> 00:21:11,310
and the previous work there is basically the

277
00:21:11,380 --> 00:21:14,710
there are some results pointwise convergence

278
00:21:14,730 --> 00:21:19,920
so uniform but this is you can actually show that investors themselves

279
00:21:20,830 --> 00:21:25,210
and let me just give you an idea not just an idea of the proof

280
00:21:25,210 --> 00:21:29,940
but also an idea of why this is the right and it turns out that

281
00:21:30,000 --> 00:21:32,690
is it's actually kind of simple

282
00:21:32,710 --> 00:21:35,090
it's basically the heat equation

283
00:21:37,790 --> 00:21:40,480
what is the semantic web

284
00:21:40,500 --> 00:21:45,520
five minutes discussion specifically what is typically

285
00:21:45,650 --> 00:21:50,650
if you questions the phone i let's say this surface for

286
00:21:50,730 --> 00:21:54,670
i have a huge distribution of the surface

287
00:21:54,690 --> 00:21:59,920
in the beginning this is my initial distribution so suppose there is nothing else

288
00:21:59,940 --> 00:22:06,690
and then what happens i just live my run for

289
00:22:06,750 --> 00:22:07,980
and you know

290
00:22:08,000 --> 00:22:10,210
distribution changes as time goes on

291
00:22:10,230 --> 00:22:13,210
and you know so for those of us feel right and there is in the

292
00:22:13,210 --> 00:22:18,640
vacuum because this joke about the physical force in vacuum

293
00:22:25,110 --> 00:22:26,210
then i mean

294
00:22:26,210 --> 00:22:31,210
in the early phase from distribution in the beginning right on this in the and

295
00:22:31,210 --> 00:22:35,380
like twenty goes to infinity for every out to become cost

296
00:22:39,590 --> 00:22:40,340
and i

297
00:22:40,340 --> 00:22:44,210
but in the end it's very short period of time right if i will concede

298
00:22:44,210 --> 00:22:48,670
to be you know zero point zero zero one second this distribution doesn't change well

299
00:22:48,670 --> 00:22:50,400
ten very low

300
00:22:50,440 --> 00:22:55,380
so now how does this work well imagine i just have a few minutes

301
00:22:55,460 --> 00:22:58,310
the source of its support

302
00:22:58,420 --> 00:23:03,340
so i have so everything else is zero here this is my one sort of

303
00:23:03,590 --> 00:23:05,230
what happens to the well

304
00:23:05,730 --> 00:23:08,650
for a short time it become a sort of go

305
00:23:08,650 --> 00:23:10,770
spread out over the

306
00:23:10,810 --> 00:23:13,520
more the result of point of

307
00:23:13,520 --> 00:23:15,270
and there just completely flat

308
00:23:15,400 --> 00:23:20,860
so if it's if anything is going to propose to a xerox if it's surface

309
00:23:20,880 --> 00:23:22,900
and just come

310
00:23:24,420 --> 00:23:28,550
know what happens if i have so many sources of what i to some of

311
00:23:29,250 --> 00:23:34,420
so if have infinitely many sources of how much she would like to him point

312
00:23:34,480 --> 00:23:39,370
and see if i have to integrate over the set of all the points writes

313
00:23:39,370 --> 00:23:43,750
like an integral over all of the point of how much she just point from

314
00:23:43,750 --> 00:23:45,770
all other point

315
00:23:45,830 --> 00:23:48,900
and i know that from each point to its go

316
00:23:51,040 --> 00:23:52,250
well they

317
00:23:52,270 --> 00:23:57,480
the total amount is going to be in convolution to discuss is is biggest from

318
00:23:57,650 --> 00:23:58,920
the point of how much he

319
00:23:59,230 --> 00:24:01,730
so this is the form of

320
00:24:01,750 --> 00:24:07,420
it's actually a very natural right is just group is believed to point to go

321
00:24:07,420 --> 00:24:09,790
normal distributions well this

322
00:24:09,810 --> 00:24:13,830
a large body of work in the statistics literature just about what you can do

323
00:24:13,830 --> 00:24:15,870
with the normal distribution

324
00:24:15,890 --> 00:24:19,330
for instance you could have gauss markov random fields

325
00:24:19,390 --> 00:24:21,120
people have written books on that

326
00:24:21,140 --> 00:24:23,520
actually even very recently

327
00:24:23,520 --> 00:24:28,060
and has a lot to do being with for sparse matrices and so will get

328
00:24:28,060 --> 00:24:30,620
to this in a bit more detail later on

329
00:24:31,370 --> 00:24:36,440
so basically don't underestimate the power of interest the single one of those distributions

330
00:24:36,460 --> 00:24:41,290
you can build a large body of work of statistical inference devices on top of

331
00:24:42,100 --> 00:24:43,370
but what i'm saying is

332
00:24:43,560 --> 00:24:45,250
you know rather than

333
00:24:45,290 --> 00:24:47,420
simeon on this specific one

334
00:24:47,420 --> 00:24:48,960
we can play with all of them

335
00:24:48,980 --> 00:24:50,540
and do it efficiently

336
00:24:50,730 --> 00:24:55,290
all the gotta to do is pick one we read of the corresponding normalisation

337
00:24:56,080 --> 00:24:58,210
we've made life much easier

338
00:25:02,040 --> 00:25:04,330
now why is this useful

339
00:25:04,330 --> 00:25:09,920
well let's just i mean after at the end if they want to perform inference

340
00:25:11,920 --> 00:25:15,940
it's just look at again what we started out so i said well let's consider

341
00:25:15,940 --> 00:25:20,870
all distributions which looked like it within approach between some profits in theta

342
00:25:23,890 --> 00:25:32,870
and these are the sufficient statistics will get to that the later that's normalisation

343
00:25:34,420 --> 00:25:37,730
one of the really good things is that this function g of data

344
00:25:37,790 --> 00:25:39,670
is a nice function

345
00:25:41,500 --> 00:25:43,440
it generates the cumulants

346
00:25:43,560 --> 00:25:48,250
so sometimes people actually use the mechanism very similar to this as

347
00:25:48,640 --> 00:25:54,980
cumulant generating mechanism just to get those for any arbitrary distribution

348
00:25:55,040 --> 00:25:56,850
and the math actually

349
00:25:56,850 --> 00:25:59,520
it does look quite similar

350
00:26:00,440 --> 00:26:03,230
let's recall this is our

351
00:26:03,270 --> 00:26:04,540
g of data

352
00:26:04,560 --> 00:26:09,480
now let's take to the relative with respect to of g of data

353
00:26:09,560 --> 00:26:11,580
and while ignoring

354
00:26:11,620 --> 00:26:16,190
issues of convergence and all of that well all all we do is we just

355
00:26:16,190 --> 00:26:20,390
apply the chain rule and push the narrative through the integral

356
00:26:20,420 --> 00:26:22,810
so the log of an integral

357
00:26:22,810 --> 00:26:24,850
is one of the integral

358
00:26:27,310 --> 00:26:30,920
the derivative pushed into the integral

359
00:26:30,920 --> 00:26:32,170
now e to the

360
00:26:32,170 --> 00:26:38,080
something will be safe i just pulled my father fix out of you

361
00:26:39,480 --> 00:26:41,730
now what get

362
00:26:41,750 --> 00:26:43,290
i get five fixed

363
00:26:44,230 --> 00:26:46,350
he e this expression here

364
00:26:47,350 --> 00:26:48,980
the log of that expression

365
00:26:49,000 --> 00:26:52,540
so basically if i wanted to squeeze then here this in here i would have

366
00:26:52,540 --> 00:26:55,170
to take months to lock

367
00:26:55,190 --> 00:26:56,730
now that

368
00:26:56,770 --> 00:27:01,290
if you recall is exactly to of data

369
00:27:01,350 --> 00:27:02,370
in other words

370
00:27:02,370 --> 00:27:03,020
what i

371
00:27:03,040 --> 00:27:04,140
got sitting here

372
00:27:04,150 --> 00:27:10,830
this entire expression there is p of x parameterized by theta

373
00:27:11,480 --> 00:27:14,230
the integral of four fix

374
00:27:14,330 --> 00:27:16,810
p of express interest by caesar

375
00:27:16,810 --> 00:27:20,500
it's just the expected value for fixed

376
00:27:21,440 --> 00:27:23,020
well that's great

377
00:27:23,070 --> 00:27:26,980
it this is what i've done is i've now once have done the hard work

378
00:27:27,020 --> 00:27:31,420
of looking up in some statistics book the book what your data is

379
00:27:31,480 --> 00:27:34,810
i can compute expected values

380
00:27:34,830 --> 00:27:36,290
o five x

381
00:27:36,310 --> 00:27:38,920
just by taking derivatives

382
00:27:38,940 --> 00:27:43,830
so rather than me having to actually solve this integral here i just take maple

383
00:27:43,850 --> 00:27:47,140
tell it take the derivative with respect to those parameters

384
00:27:47,150 --> 00:27:49,690
another comes with an answer

385
00:27:49,730 --> 00:27:52,460
so now this means that they have an automatic way

386
00:27:52,480 --> 00:27:56,920
of generating all those expectations

387
00:27:56,940 --> 00:28:01,600
so this makes my life really really much easier

388
00:28:04,210 --> 00:28:05,980
let's get ambitious

389
00:28:06,000 --> 00:28:09,350
let's take the second derivative of g theta

390
00:28:10,500 --> 00:28:13,040
you could now go through all the details in

391
00:28:13,060 --> 00:28:16,170
i encourage you to do so at your leisure at some point

392
00:28:16,210 --> 00:28:20,620
you probably want to do that exactly once in your life and the never again

393
00:28:20,620 --> 00:28:25,390
new five but you really should do it once it's it's really really worth it

394
00:28:25,390 --> 00:28:26,930
to the other state here

395
00:28:26,950 --> 00:28:30,800
you have the load your on it means that from this date knowing that you

396
00:28:30,800 --> 00:28:32,310
had this input

397
00:28:37,300 --> 00:28:40,720
i conform to what the transition relation

398
00:28:42,680 --> 00:28:46,990
which is that it's conformed to the behaviour of the automaton OK

399
00:28:48,930 --> 00:28:51,390
that was the definition here

400
00:28:51,450 --> 00:28:53,480
knowing that

401
00:28:55,290 --> 00:28:59,390
labeling of the full binary tree by states of the automaton

402
00:29:02,730 --> 00:29:04,360
of the

403
00:29:04,420 --> 00:29:06,990
you labelled with the initial state

404
00:29:08,830 --> 00:29:13,390
and in our example it was qy should

405
00:29:18,170 --> 00:29:21,590
i have copied it i think my branch here

406
00:29:23,460 --> 00:29:24,880
branch say

407
00:29:29,550 --> 00:29:30,910
this branch

408
00:29:33,580 --> 00:29:37,250
o a q a q and the bottom at the bottom of them

409
00:29:37,360 --> 00:29:39,500
in fact all the colours

410
00:29:41,250 --> 00:29:46,220
so the columns of w the human has got one pascal of zero

411
00:29:48,270 --> 00:29:51,240
so that means if you have a look at this branch and b

412
00:29:53,290 --> 00:29:57,080
one one one and he was he was he was he was he was

413
00:29:59,230 --> 00:30:02,220
you can have branches here

414
00:30:02,240 --> 00:30:03,740
and imagine that you

415
00:30:05,830 --> 00:30:08,900
after this be in this input after the

416
00:30:08,920 --> 00:30:12,520
i have that found is we have like here

417
00:30:14,510 --> 00:30:18,750
an infinite subtree contain

418
00:30:20,580 --> 00:30:26,540
but even this have to complete the full battery with the left branch

419
00:30:31,650 --> 00:30:34,170
always labelled baby

420
00:30:35,250 --> 00:30:39,910
on the left edge

421
00:30:39,950 --> 00:30:41,960
the computation

422
00:30:43,960 --> 00:30:46,710
according to the transition relation

423
00:30:46,720 --> 00:30:48,470
the computational

424
00:30:50,880 --> 00:30:54,660
that's the only

425
00:30:57,240 --> 00:30:58,900
if i have be

426
00:31:00,230 --> 00:31:03,770
so you have to be here to q and q

427
00:31:08,660 --> 00:31:11,860
q bates's OK so the colours

428
00:31:12,200 --> 00:31:15,470
it was one of the leftmost most rush

429
00:31:18,040 --> 00:31:22,010
of this competition

430
00:31:24,190 --> 00:31:26,940
my way to contradiction or to declare

431
00:31:29,150 --> 00:31:31,280
the competition is successful

432
00:31:31,340 --> 00:31:33,410
one of them i might

433
00:31:35,680 --> 00:31:37,800
so what i do is that

434
00:31:41,960 --> 00:31:43,670
and they look at

435
00:31:43,720 --> 00:31:46,390
the current of from

436
00:31:48,440 --> 00:31:50,130
along the branch

437
00:31:54,180 --> 00:31:55,700
so it's

438
00:31:55,770 --> 00:31:57,280
the full binary tree

439
00:31:59,500 --> 00:32:01,450
i choose the branch

440
00:32:05,600 --> 00:32:06,830
this tree

441
00:32:06,850 --> 00:32:12,400
and instead of looking at the states that just because of of the generalisation of

442
00:32:14,500 --> 00:32:16,330
being financed

443
00:32:18,790 --> 00:32:24,460
being an accepting state of being statistic zero one zero would be accepting one would

444
00:32:24,460 --> 00:32:26,000
be accepting

445
00:32:28,290 --> 00:32:30,740
but just to itself

446
00:32:32,650 --> 00:32:35,150
imagine i have now the way

447
00:32:35,150 --> 00:32:39,750
describing our posterior and then we put them into this equation which we can

448
00:32:39,770 --> 00:32:42,350
analytically compute now if we want to be

449
00:32:42,370 --> 00:32:46,150
even a little more robust we might we might want to say well i don't

450
00:32:46,810 --> 00:32:50,230
what these offers were we put a prior on those

451
00:32:50,250 --> 00:32:53,500
and try to integrate over that they would be called the hyper

452
00:32:53,520 --> 00:32:54,580
i per

453
00:32:54,600 --> 00:32:59,390
parameter or hyper hyper prior and you don't need to go at infinitum some people

454
00:32:59,390 --> 00:33:05,330
think of this process never ends that supply of bayesian methods that's completely bogus basically

455
00:33:05,600 --> 00:33:07,080
you u

456
00:33:07,100 --> 00:33:13,520
you only need to consider adding levels of parameters and hyperparameters et cetera it's your

457
00:33:13,520 --> 00:33:17,620
model up to the point where there aren't any two things that you want to

458
00:33:17,620 --> 00:33:19,080
couple together

459
00:33:19,140 --> 00:33:23,770
because then at some point if you have a whole bunch of hyper parameters and

460
00:33:23,770 --> 00:33:25,980
hyper hyper parameters et cetera

461
00:33:25,980 --> 00:33:30,750
then you could collapse the whole hierarchy to just some distribution over

462
00:33:30,810 --> 00:33:33,250
points at some point in the graph

463
00:33:33,270 --> 00:33:37,120
in the hierarchy anyway so

464
00:33:37,120 --> 00:33:44,870
for these models people just usually choose some simple hyperparameters here maybe even all ones

465
00:33:44,890 --> 00:33:49,540
you know like in the lab classes rule the uniform dirichlet distribution they might use

466
00:33:49,540 --> 00:33:51,790
it all to be one and then do inference

467
00:33:51,810 --> 00:33:55,940
and then you can question whether that's good enough

468
00:34:09,080 --> 00:34:21,290
right so the they days question was when you do maximum likelihood

469
00:34:21,310 --> 00:34:23,250
the maximum likelihood for

470
00:34:23,270 --> 00:34:30,040
more complicated models always has a higher likelihood than the maximum likelihood for strictly simpler

471
00:34:30,940 --> 00:34:33,500
whereas when you do the bayesian model averaging

472
00:34:33,520 --> 00:34:36,850
you don't necessarily get that

473
00:34:36,850 --> 00:34:40,480
that's coming in and

474
00:34:40,500 --> 00:34:45,850
so chris bishop kind of already described this in one of his lectures where

475
00:34:45,870 --> 00:34:50,790
really what's happening is when you integrating over the parameters of the model

476
00:34:50,810 --> 00:34:55,390
you're not fitting the model to data in the in the maximisation sensor fitting

477
00:34:55,390 --> 00:35:01,040
so it doesn't matter if that model has more and more parameters it doesn't make

478
00:35:01,040 --> 00:35:06,730
this marginal likelihood higher and higher in fact a more complicated model is just the

479
00:35:06,730 --> 00:35:09,850
model that puts its probability mass

480
00:35:10,770 --> 00:35:12,870
more possible datasets

481
00:35:13,620 --> 00:35:15,580
so i think of any model

482
00:35:15,620 --> 00:35:21,310
class of models as making bets about possible datasets that could be observed and some

483
00:35:21,310 --> 00:35:26,020
simple models will put their bets on some datasets and

484
00:35:26,040 --> 00:35:27,920
not on anything else

485
00:35:28,070 --> 00:35:33,230
the other models will spread their bets out on a wider range of possible datasets

486
00:35:33,230 --> 00:35:38,060
and that's what we usually think of as more complicated models but each model has

487
00:35:38,060 --> 00:35:44,140
this has a unique currency that can spread out unit you know one dollar pound

488
00:35:44,140 --> 00:35:49,690
or euro of probability mass that can spread out over possible datasets

489
00:35:49,710 --> 00:35:53,370
so because of that more complicated models

490
00:35:53,390 --> 00:35:57,440
i will not necessarily have higher marginal likelihood

491
00:35:58,060 --> 00:36:08,850
still have a problem with the next week

492
00:36:14,420 --> 00:36:17,000
three years

493
00:36:17,020 --> 00:36:24,210
it is hard to believe that

494
00:36:24,460 --> 00:36:28,870
yeah so the question was a different way of doing this would be to start

495
00:36:28,870 --> 00:36:30,890
out with the fully connected graph

496
00:36:30,940 --> 00:36:35,230
then do some estimation or learning or something and if some of the parameters are

497
00:36:35,230 --> 00:36:40,260
starting to go to zero in some way then then you could prove them out

498
00:36:40,260 --> 00:36:45,840
and get simpler model out yet there actually are good ways of of using exactly

499
00:36:45,840 --> 00:36:50,790
that intuition that you have in certain cases for example with linear models

500
00:36:50,800 --> 00:36:55,850
like linear calcium models then when the parameter goes to zero something becomes independent of

501
00:36:55,850 --> 00:36:59,840
something else here it's a little more subtle to think about that because these are

502
00:36:59,840 --> 00:37:05,590
conditional probability tables so things are really going to zero they're going towards independence and

503
00:37:05,590 --> 00:37:08,970
as you said yourself one of the problems with

504
00:37:08,980 --> 00:37:11,080
well what you're sort of

505
00:37:11,120 --> 00:37:17,630
thinking about is that it might be computationally costly air and indeed you know it's

506
00:37:17,630 --> 00:37:20,370
it's simpler to start with if you're graph is sparse

507
00:37:20,830 --> 00:37:24,600
you don't want to start with the fully connected graph because then you have a

508
00:37:24,650 --> 00:37:28,720
hell of a big probability distribution to deal with

509
00:37:28,720 --> 00:37:30,430
OK let me just move on

510
00:37:30,470 --> 00:37:34,970
and try to get to some of the sort of fun researching thing at the

511
00:37:34,980 --> 00:37:36,040
end as well

512
00:37:38,600 --> 00:37:42,000
here we can take this score

513
00:37:42,000 --> 00:37:45,170
which was the marginal likelihood we can obviously

514
00:37:45,190 --> 00:37:50,600
i use prior information if we prefer certain graphs to other graphs may be we

515
00:37:50,600 --> 00:37:55,220
have domain knowledge is the biological problem in these genes and we read in the

516
00:37:55,220 --> 00:37:59,910
literature that this gene affects that genes so we want to put that in well

517
00:37:59,910 --> 00:38:04,810
we can put a prior over models are graph structures in here as well

518
00:38:04,830 --> 00:38:09,930
and then the score would just be the log marginal likelihood plus lot prior

519
00:38:10,080 --> 00:38:16,510
a greedy search algorithm we just take graphs and consider operations on the graphs and

520
00:38:16,510 --> 00:38:19,330
try to find one that has the highest score

521
00:38:21,140 --> 00:38:24,960
the bayesian method would try to do

522
00:38:25,000 --> 00:38:30,920
inference over model structures by running markov chain monte carlo for example over the graph

523
00:38:30,920 --> 00:38:37,430
structures so proposing graphs accepting or rejecting exactly in the same way as in mary

524
00:38:38,590 --> 00:38:41,430
four metropolis for example

525
00:38:41,430 --> 00:38:42,260
all right

526
00:38:43,630 --> 00:38:47,680
now what do you do when you have incomplete data so this is kind of

527
00:38:47,680 --> 00:38:50,100
you know the hardest cases in this square

528
00:38:50,130 --> 00:38:56,060
there is an algorithm by near freedman called bayesian structural yeah

529
00:38:57,210 --> 00:39:03,480
is using some of these marginal likelihood computation ideas and the idea behind the EM

530
00:39:03,480 --> 00:39:05,840
algorithm to try to

531
00:39:05,870 --> 00:39:10,770
phil in settings of the hidden variables and then score

532
00:39:10,800 --> 00:39:16,590
structures based on those filled in settings and then move around the space until it

533
00:39:16,590 --> 00:39:21,000
finds a graph with the highest score as possible

534
00:39:25,310 --> 00:39:28,910
let me have a couple of remarks and then let me just give you a

535
00:39:28,910 --> 00:39:31,030
i would like really

536
00:39:34,700 --> 00:39:36,590
this is all

537
00:39:36,610 --> 00:39:40,350
that is the sum fancy mathematics is associated with

538
00:39:40,370 --> 00:39:42,840
it can be seen that sounds

539
00:39:44,280 --> 00:39:47,760
we already have the new function is the same

540
00:39:48,530 --> 00:39:50,060
illustration that shows you

541
00:39:50,100 --> 00:39:52,520
where these two kinds of different

542
00:39:52,530 --> 00:39:55,320
so one one-dimensional regression problems

543
00:39:55,330 --> 00:39:57,980
the difference between here

544
00:39:58,000 --> 00:40:02,170
i have three thousand miles

545
00:40:04,880 --> 00:40:06,080
from here

546
00:40:06,100 --> 00:40:10,630
because of the weights of the posterior

547
00:40:10,810 --> 00:40:13,780
always just run around

548
00:40:13,820 --> 00:40:17,110
three three car right

549
00:40:17,120 --> 00:40:18,540
you get something

550
00:40:18,580 --> 00:40:19,980
if fifty

551
00:40:19,990 --> 00:40:21,000
a few

552
00:40:21,050 --> 00:40:25,110
on new examples posterior the weights and you would imagine the

553
00:40:25,120 --> 00:40:28,020
but i would like to run

554
00:40:28,950 --> 00:40:30,990
right these

555
00:40:34,780 --> 00:40:36,040
the question is

556
00:40:36,060 --> 00:40:37,640
the songs

557
00:40:37,650 --> 00:40:38,580
but we

558
00:40:40,060 --> 00:40:43,330
the question is what happens if you want to make region here

559
00:40:43,350 --> 00:40:45,550
how to make predictions from the model

560
00:40:45,560 --> 00:40:46,350
well we

561
00:40:46,360 --> 00:40:49,070
look what was to

562
00:40:49,090 --> 00:40:53,430
and many more on the weights according to you

563
00:40:53,440 --> 00:40:55,090
what happens here

564
00:41:02,090 --> 00:41:04,650
in addition is always

565
00:41:04,660 --> 00:41:05,790
so we integrate over

566
00:41:05,800 --> 00:41:08,270
the posterior way

567
00:41:08,280 --> 00:41:09,920
we always here

568
00:41:09,940 --> 00:41:11,720
zero maybe not

569
00:41:11,740 --> 00:41:14,010
as for the mean

570
00:41:14,230 --> 00:41:16,030
but it turns us

571
00:41:16,050 --> 00:41:19,790
same again zero with the same model

572
00:41:19,810 --> 00:41:23,110
at the start

573
00:41:23,130 --> 00:41:24,550
that's because

574
00:41:24,570 --> 00:41:26,690
what that does not

575
00:41:26,710 --> 00:41:29,420
if we had just here

576
00:41:29,430 --> 00:41:30,400
and also

577
00:41:30,410 --> 00:41:31,690
our first

578
00:41:31,740 --> 00:41:34,690
and they will tell you

579
00:41:36,300 --> 00:41:37,540
the posterior

580
00:41:37,550 --> 00:41:38,770
the reason why

581
00:41:39,000 --> 00:41:42,250
the model that would be telling

582
00:41:46,260 --> 00:41:47,320
o thing

583
00:41:47,340 --> 00:41:48,540
to worry about

584
00:41:48,560 --> 00:41:52,880
these are the more to worry about whether prior

585
00:41:52,900 --> 00:41:54,930
actually allows you to have

586
00:42:00,850 --> 00:42:04,850
so i'm going to

587
00:42:04,870 --> 00:42:08,020
more than functions so

588
00:42:08,030 --> 00:42:09,640
for most of the rest

589
00:42:11,170 --> 00:42:11,950
like this

590
00:42:11,960 --> 00:42:14,250
actually wish

591
00:42:14,270 --> 00:42:15,620
the we constructed ten

592
00:42:15,630 --> 00:42:18,890
well that's very function

593
00:42:20,000 --> 00:42:22,980
this is year

594
00:42:22,990 --> 00:42:24,410
tell the

595
00:42:24,590 --> 00:42:26,680
inverse square scale

596
00:42:26,700 --> 00:42:29,710
so you you basically just to

597
00:42:30,230 --> 00:42:35,990
gas laws for children functions with different things

598
00:42:36,010 --> 00:42:38,070
and it turns out if you choose the

599
00:42:38,090 --> 00:42:39,880
the distribution of less

600
00:42:40,040 --> 00:42:40,860
and this

601
00:42:40,880 --> 00:42:42,570
gamma distribution

602
00:42:42,580 --> 00:42:45,100
and actually you know it

603
00:42:49,470 --> 00:42:51,230
all the rest

604
00:42:51,250 --> 00:42:53,870
see what looks like

605
00:42:55,270 --> 00:42:57,790
this shows what the actually

606
00:42:58,900 --> 00:43:04,500
the question is all stationary only depend on the distance between

607
00:43:05,280 --> 00:43:09,190
if if you go to infinity

608
00:43:09,210 --> 00:43:10,120
then the

609
00:43:10,130 --> 00:43:12,570
distribution of over the main scale

610
00:43:12,630 --> 00:43:14,480
becomes very high

611
00:43:14,500 --> 00:43:15,740
that means that the

612
00:43:15,760 --> 00:43:17,430
to infinity

613
00:43:17,450 --> 00:43:19,870
you get that calcium is residential

614
00:43:21,410 --> 00:43:28,130
two smaller and smaller you get a mixture which has a different kind of link

615
00:43:28,150 --> 00:43:31,360
let's get real functions

616
00:43:31,380 --> 00:43:34,530
two of

617
00:43:34,580 --> 00:43:36,750
we're not going run

618
00:43:36,770 --> 00:43:42,200
you can see that output is a quite natural in lot

619
00:43:43,230 --> 00:43:46,080
you get a function of the boy

620
00:43:46,090 --> 00:43:49,480
perhaps not structure on was scale

621
00:43:50,510 --> 00:43:51,500
he has

622
00:43:51,520 --> 00:43:55,100
this full-scale destruction cells structure doesn't do it

623
00:43:55,110 --> 00:43:59,240
this exactly what we

624
00:43:59,290 --> 00:44:01,520
the characteristic length scale

625
00:44:01,670 --> 00:44:05,970
by mixing together in functions by

626
00:44:05,980 --> 00:44:07,650
we can get

627
00:44:07,740 --> 00:44:09,970
it gives rise to point

628
00:44:09,980 --> 00:44:12,880
so for example you

629
00:44:14,100 --> 00:44:15,300
so small

630
00:44:15,310 --> 00:44:18,250
skin effect and also artist

631
00:44:18,260 --> 00:44:20,360
small and

632
00:44:20,380 --> 00:44:27,220
this is this is different kind of dimension of different ways

633
00:44:28,460 --> 00:44:30,740
ideas about

634
00:44:31,930 --> 00:44:34,570
one of the characteristic

635
00:44:34,580 --> 00:44:40,750
this example is called the ten commandments

636
00:44:40,900 --> 00:44:42,690
ten growth function

637
00:44:42,710 --> 00:44:44,800
also actually expression

638
00:44:44,820 --> 00:44:47,620
the last of this function

639
00:44:49,440 --> 00:44:51,260
properties are

640
00:44:51,450 --> 00:44:53,260
is similar to

641
00:44:53,280 --> 00:44:54,320
should be

642
00:44:56,320 --> 00:44:57,840
and the next

643
00:44:57,850 --> 00:44:59,980
right there

644
00:45:00,950 --> 00:45:02,710
situation where

645
00:45:02,720 --> 00:45:03,910
from this case

646
00:45:04,640 --> 00:45:06,340
that's why

647
00:45:07,360 --> 00:45:08,720
small values

648
00:45:08,740 --> 00:45:10,090
good functions

649
00:45:10,090 --> 00:45:11,630
all of these cores

650
00:45:11,760 --> 00:45:14,490
process by my brains in such a way

651
00:45:14,490 --> 00:45:16,850
that they think or they well

652
00:45:16,910 --> 00:45:22,310
they actually make you see white light is as real as you can have it

653
00:45:22,350 --> 00:45:24,690
so this raises the subject of

654
00:45:24,730 --> 00:45:28,000
the illusion of course which is what i want to discuss with you

655
00:45:28,030 --> 00:45:30,950
for the remaining part of this lecture

656
00:45:30,970 --> 00:45:33,470
if you ask a physicist

657
00:45:33,520 --> 00:45:38,980
when we see certain colors when we see red when see blue or green

658
00:45:39,020 --> 00:45:40,510
and chances are

659
00:45:40,560 --> 00:45:42,640
that he would give you standard answer

660
00:45:42,670 --> 00:45:43,980
and he would say well

661
00:45:43,990 --> 00:45:45,630
that depends on them

662
00:45:45,740 --> 00:45:49,590
on the wavelength in air if you tell me what wavelengths

663
00:45:49,610 --> 00:45:51,260
you're wrong and i will tell you

664
00:45:51,260 --> 00:45:53,470
what causes you'll see

665
00:45:53,480 --> 00:45:55,140
and i have here a

666
00:45:55,200 --> 00:45:57,730
transparency which

667
00:45:57,800 --> 00:46:01,580
makes the connection between wavelength and these colors

668
00:46:01,590 --> 00:46:06,740
it's also on the web you can also download this

669
00:46:06,750 --> 00:46:07,840
and so if

670
00:46:07,860 --> 00:46:09,550
the wavelength

671
00:46:09,590 --> 00:46:12,030
of light

672
00:46:12,100 --> 00:46:13,590
in areas about

673
00:46:13,630 --> 00:46:17,250
in this range one angstrom is ten to the minus ten meters

674
00:46:17,920 --> 00:46:20,440
you will probably say that's red lights

675
00:46:20,490 --> 00:46:24,980
when the wavelength gets shorter this range you would say i disagree light

676
00:46:25,000 --> 00:46:27,330
and when the wavelength gets even shorter

677
00:46:27,380 --> 00:46:29,790
you would say others blue light

678
00:46:29,790 --> 00:46:31,320
you can see

679
00:46:31,360 --> 00:46:34,860
any wavelength shorter than this you get into the ultraviolet here

680
00:46:34,900 --> 00:46:37,560
you can see any wavelengths longer than this

681
00:46:37,570 --> 00:46:39,800
that's infrared analyse

682
00:46:40,610 --> 00:46:41,820
sensitive for

683
00:46:41,820 --> 00:46:45,310
those wavelengths

684
00:46:45,350 --> 00:46:47,600
so this is all nice and then the

685
00:46:47,610 --> 00:46:49,470
but we still have the problem

686
00:46:49,470 --> 00:46:50,410
of the

687
00:46:50,470 --> 00:46:52,690
the fact that if you mix all the

688
00:46:52,740 --> 00:46:57,730
course together like the rotating disk might looking at the light bulb that our brains

689
00:46:57,730 --> 00:47:02,430
still tell us that we're seeing white light

690
00:47:02,450 --> 00:47:06,020
so maybe that is i'm not really is simple

691
00:47:06,060 --> 00:47:07,360
as we

692
00:47:07,380 --> 00:47:09,360
i think

693
00:47:10,590 --> 00:47:13,830
this scheme

694
00:47:15,230 --> 00:47:17,390
callers has been worked out in

695
00:47:17,440 --> 00:47:20,310
quite some detail already in the seventeenth

696
00:47:20,320 --> 00:47:23,340
and in the eighteenth century

697
00:47:23,380 --> 00:47:25,660
when it was discovered

698
00:47:25,700 --> 00:47:31,480
that there is such a thing as primary colours

699
00:47:31,570 --> 00:47:33,870
maxwell did some research on that

700
00:47:33,930 --> 00:47:36,630
and handles even the

701
00:47:36,640 --> 00:47:37,770
the poet

702
00:47:37,770 --> 00:47:38,590
going to

703
00:47:38,610 --> 00:47:40,360
did work on this

704
00:47:40,410 --> 00:47:43,250
they discover that their primary colours

705
00:47:43,280 --> 00:47:45,890
and when you mix lights

706
00:47:45,920 --> 00:47:48,590
we call these additive mixing

707
00:47:48,660 --> 00:47:52,230
the three primary colours are green violet and red

708
00:47:52,280 --> 00:47:54,030
and where you mix paint

709
00:47:54,130 --> 00:47:56,080
the three primary colours

710
00:47:56,090 --> 00:47:58,160
i yellow blue and red

711
00:47:58,240 --> 00:48:00,250
and the idea behind this

712
00:48:00,330 --> 00:48:02,820
is that if you mix the

713
00:48:02,820 --> 00:48:05,680
the three primary colors in the right proportions

714
00:48:05,700 --> 00:48:07,850
you can make many colors

715
00:48:07,860 --> 00:48:10,180
i want to show you a

716
00:48:10,220 --> 00:48:13,090
color triangle which is the recipe

717
00:48:13,130 --> 00:48:15,900
tells you how you have to make this course

718
00:48:15,950 --> 00:48:18,500
and in order to do that

719
00:48:18,590 --> 00:48:21,490
going to make it dark

720
00:48:21,760 --> 00:48:22,900
all the way dark

721
00:48:23,010 --> 00:48:26,400
you see the color triangle

722
00:48:26,450 --> 00:48:30,540
and the color triangle has in the three colours three corners

723
00:48:30,550 --> 00:48:32,620
the color red

724
00:48:32,640 --> 00:48:34,350
one primary color

725
00:48:34,410 --> 00:48:35,470
and then

726
00:48:35,470 --> 00:48:37,070
here his violence

727
00:48:37,090 --> 00:48:39,330
and then of their it has

728
00:48:39,400 --> 00:48:41,280
it has green

729
00:48:41,320 --> 00:48:42,870
and i'll tell you how this

730
00:48:42,890 --> 00:48:45,400
the recipe has to be used

731
00:48:45,440 --> 00:48:47,690
so i'm going to draw year

732
00:48:47,760 --> 00:48:50,020
color triangle

733
00:48:50,190 --> 00:48:55,670
and we have already here

734
00:48:56,600 --> 00:49:01,850
and we have a green there

735
00:49:01,860 --> 00:49:03,670
and we have here violated

736
00:49:03,780 --> 00:49:08,240
if you know that is called the triangle

737
00:49:08,290 --> 00:49:11,390
you see all the cause that you can imagine sort of

738
00:49:11,410 --> 00:49:12,720
you see yellow

739
00:49:12,780 --> 00:49:14,970
and you see here purple

740
00:49:15,010 --> 00:49:16,570
and you see orange

741
00:49:16,620 --> 00:49:19,010
you can see why

742
00:49:19,070 --> 00:49:21,620
so how do we make these calls now

743
00:49:21,680 --> 00:49:24,990
well suppose i wanna make this call here

744
00:49:25,050 --> 00:49:27,070
so that this color

745
00:49:28,600 --> 00:49:31,390
then you draw three lines

746
00:49:38,010 --> 00:49:40,160
from the three corners

747
00:49:40,200 --> 00:49:42,350
and this is the amount of threads

748
00:49:42,350 --> 00:49:44,570
you have to put in

749
00:49:44,620 --> 00:49:47,050
and this is the amount of green

750
00:49:47,100 --> 00:49:49,220
but you have to put in in the light

751
00:49:49,260 --> 00:49:53,800
and this is the amount of violent like if you do that in the ratio

752
00:49:53,850 --> 00:49:55,180
you would get

753
00:49:55,350 --> 00:49:56,570
the core

754
00:49:56,640 --> 00:49:58,820
which is here

755
00:49:58,850 --> 00:50:01,620
if you want to make very nice yellow

756
00:50:01,640 --> 00:50:05,120
so let's see this makes it very nice yellow which is all the way you

757
00:50:05,280 --> 00:50:06,180
the edge

758
00:50:06,220 --> 00:50:10,430
you don't need any violence you can do to exclusively with green

759
00:50:10,470 --> 00:50:13,970
and with red so let's go to this point here

760
00:50:14,010 --> 00:50:16,510
which is yellow

761
00:50:16,510 --> 00:50:17,740
it would mean then

762
00:50:17,780 --> 00:50:20,450
but i have to put in this much read

763
00:50:20,470 --> 00:50:22,430
and i have to put in

764
00:50:22,490 --> 00:50:25,430
this much ring

765
00:50:25,490 --> 00:50:29,160
and if i had more and more red

766
00:50:29,200 --> 00:50:31,120
i go along this line

767
00:50:31,180 --> 00:50:33,570
and i end up here and i get orange

768
00:50:33,600 --> 00:50:35,740
so i could make audience here

769
00:50:35,800 --> 00:50:39,030
by simply increasing the amount of red o

770
00:50:39,050 --> 00:50:41,950
you should not be violent right

771
00:50:42,010 --> 00:50:44,550
would green

772
00:50:44,600 --> 00:50:45,590
well you should have

773
00:50:45,640 --> 00:50:47,760
well the this is green

774
00:50:47,760 --> 00:50:51,070
i did off on measures of image similarity

775
00:50:51,130 --> 00:50:52,580
so like one

776
00:50:52,590 --> 00:50:57,210
the measurement similarity of is i guess they can make sense of the images in

777
00:50:57,210 --> 00:50:59,560
the same thing not to make sense of this noise

778
00:50:59,620 --> 00:51:03,320
so with correlations

779
00:51:04,050 --> 00:51:09,160
they tend to fail more easily but they tend to suffer from problems with outliers

780
00:51:09,160 --> 00:51:10,220
and things like that

781
00:51:10,260 --> 00:51:16,370
but the big advantage of it is basically in terms of the complexity the the

782
00:51:16,370 --> 00:51:18,970
reason that we kind of tend in vision

783
00:51:19,010 --> 00:51:22,670
to deal with some square difference is all

784
00:51:22,680 --> 00:51:27,530
basically linear classifiers is complexity because all of the time when not just worried about

785
00:51:27,560 --> 00:51:32,320
performance of the accuracy of the classification decision were also worried about how fast we

786
00:51:32,320 --> 00:51:34,810
can do it and so

787
00:51:34,820 --> 00:51:40,520
obviously things like this is the linear classifiers because there's been some kind of signal

788
00:51:40,520 --> 00:51:46,060
processing inland over the years we can do we can evaluate classifiers very quickly now

789
00:51:46,060 --> 00:51:52,740
one exception to this rule going to talk about this is cascaded classifier and classic

790
00:51:52,740 --> 00:51:55,100
cascaded classifier like

791
00:51:55,120 --> 00:52:00,900
boosting can out of this type of topic classifier we got like a stub tree

792
00:52:00,950 --> 00:52:06,670
and instead of having to evaluate that every pixel in an image you might only

793
00:52:06,670 --> 00:52:11,310
have to evaluate small groupings of them such that before you can start making decision

794
00:52:11,320 --> 00:52:14,040
that can actually speed things up a lot so so you can actually come

795
00:52:14,090 --> 00:52:17,320
make your question for complex but generally

796
00:52:17,480 --> 00:52:21,420
we try to adhere to these sort of things

797
00:52:21,490 --> 00:52:23,820
and just to that

798
00:52:23,870 --> 00:52:27,590
this is the linear classifier with weighting matrix by

799
00:52:27,600 --> 00:52:28,400
and here

800
00:52:28,420 --> 00:52:30,180
the quadratic classifier

801
00:52:30,200 --> 00:52:35,040
this is this is a symmetric matrix b then

802
00:52:35,230 --> 00:52:40,850
see the bias and sometimes for i we need to have some constraints and i

803
00:52:40,850 --> 00:52:43,590
again we get into that

804
00:52:44,820 --> 00:52:48,100
so if you can

805
00:52:49,650 --> 00:52:54,040
i have got all these questions now that i will have my sample i've got

806
00:52:54,230 --> 00:52:55,700
go to find the speed

807
00:52:55,700 --> 00:52:59,730
what this implies tutorial is going to be about finding the p so

808
00:52:59,820 --> 00:53:05,110
i know the piece continues so i know that i can settle continuously so

809
00:53:05,140 --> 00:53:10,380
how should i say what's the strategy should have the sample and

810
00:53:10,410 --> 00:53:15,640
the first strategy in the nice strategy i think it's just basically operating on what

811
00:53:15,640 --> 00:53:18,850
what's commonly referred to as the pixel coherence assumption

812
00:53:18,890 --> 00:53:22,890
it essentially the pixel coherence assumption is something really simple

813
00:53:22,920 --> 00:53:28,480
it essentially goes and you have the template at the ground truth the registration point

814
00:53:28,480 --> 00:53:33,420
you move the blue box all around basically every possible x y position in terms

815
00:53:33,420 --> 00:53:34,640
of the temple

816
00:53:34,660 --> 00:53:38,880
and you get what's essentially it's an autocorrelation function into the

817
00:53:38,880 --> 00:53:40,200
this and this

818
00:53:40,200 --> 00:53:44,720
and now the interesting thing and this is where the main knowledge that that's coming

819
00:53:45,230 --> 00:53:50,070
now for any signal to produce is that any kind to the signal

820
00:53:50,080 --> 00:53:54,080
or i could get any sort of behavior you so i might have a big

821
00:53:54,080 --> 00:54:00,610
fight here and everything else flat or for another synthetic sigma whatever i might have

822
00:54:00,640 --> 00:54:04,730
a completely flat all the way through because every other position for every other every

823
00:54:04,730 --> 00:54:09,330
other image but the interesting thing for natural images is that we tend to have

824
00:54:09,440 --> 00:54:10,510
this lovely

825
00:54:10,520 --> 00:54:12,190
graceful degradation

826
00:54:12,220 --> 00:54:14,050
if you actually play with nearly

827
00:54:14,070 --> 00:54:16,380
a natural image by susan

828
00:54:16,450 --> 00:54:18,450
seems whatever

829
00:54:18,470 --> 00:54:20,010
this pixel coherence

830
00:54:20,010 --> 00:54:25,350
this this domain specific knowledge this idea that pixels that are relatively so

831
00:54:25,420 --> 00:54:30,850
it is everyone can understanding what this is representative of basically image shifted with compared

832
00:54:30,850 --> 00:54:32,070
to itself

833
00:54:32,110 --> 00:54:34,690
why translation text translation

834
00:54:34,760 --> 00:54:38,540
and the thing is that with this is i think this is the kind of

835
00:54:38,720 --> 00:54:43,320
inside because you can actually explain a lot of what's going on in computer vision

836
00:54:43,320 --> 00:54:48,820
and image in the image creating count how people were saying jpg images and things

837
00:54:48,820 --> 00:54:51,880
like that based purely on this pixel coherence assumption

838
00:54:51,920 --> 00:54:54,040
in city the coherence assumption is it

839
00:54:54,040 --> 00:54:58,600
is that pixels in close spatial proximity to each other very highly correlated

840
00:54:58,640 --> 00:55:04,130
but that's not something that applies universally so you could have a random signal noise

841
00:55:04,130 --> 00:55:09,260
and things that that doesn't always apply this is something very specific provisions

842
00:55:09,270 --> 00:55:12,460
and the cool thing is is that you can do a lot of great things

843
00:55:12,460 --> 00:55:17,570
with the so if you think about it will touch upon a couple j coding

844
00:55:17,610 --> 00:55:20,320
j coding essentially takes advantage of this

845
00:55:20,330 --> 00:55:24,540
so if the cutting takes advantage that pixels are highly correlated with each other and

846
00:55:24,540 --> 00:55:29,380
so based on this correlation we can have a big inroads in compression and it's

847
00:55:29,380 --> 00:55:33,080
going to change the world people can transfer images very quickly now

848
00:55:33,090 --> 00:55:35,220
and also there also

849
00:55:35,320 --> 00:55:39,970
and we'll touch upon two i could also be something like this x y in

850
00:55:39,970 --> 00:55:41,570
time t

851
00:55:41,580 --> 00:55:45,320
and pixels tend to be quite correlated over time as well which has made big

852
00:55:45,320 --> 00:55:47,340
inroads into safe

853
00:55:47,360 --> 00:55:51,200
the very thing that's recording at the moment makes it possible to record the sort

854
00:55:51,200 --> 00:55:53,940
of image sequences in the in the compressed fashion

855
00:55:53,990 --> 00:55:59,190
and you can do that for any single visual signals the natural natural world of

856
00:55:59,360 --> 00:56:03,690
have this very domain specific knowledge and the cool thing is is that the following

857
00:56:03,700 --> 00:56:07,590
it straight from machine learning perspective not taking into account

858
00:56:07,650 --> 00:56:12,310
all i do a good job but i can do a much better job going

859
00:56:12,310 --> 00:56:14,260
on this programme

860
00:56:14,310 --> 00:56:19,810
yes learner you could which is the which is which is a good which is

861
00:56:19,810 --> 00:56:23,450
a good so it's it's it depends on that's a very good point so so

862
00:56:23,460 --> 00:56:27,080
it depends on how deeply deeply you want to go so like a good general

863
00:56:27,080 --> 00:56:30,140
rule of thumb is say like

864
00:56:30,150 --> 00:56:35,130
natural images things like the discrete cosine transform something so that very representative of town

865
00:56:35,140 --> 00:56:40,450
pixels redundancy in pixels things but you could for specific domains you can learn those

866
00:56:40,460 --> 00:56:44,010
redundancy and that's a very important

867
00:56:44,560 --> 00:56:50,140
now this is like an example of an unnatural image should but actually image up

868
00:56:50,160 --> 00:56:55,070
there but it's essentially got an image and i is

869
00:56:55,130 --> 00:57:01,460
yes yes this is the origin this is basically where where it is in cause

870
00:57:01,560 --> 00:57:05,580
if if there is any information specific and they're never going to get it to

871
00:57:05,720 --> 00:57:09,690
not all the time of the images complete zero flat all the way through but

872
00:57:09,690 --> 00:57:14,570
if there is some sort of specific information to pete heller generated this was that

873
00:57:14,690 --> 00:57:18,770
this actually had to face image and i put into the for duty free domain

874
00:57:18,820 --> 00:57:21,320
and i normalized to the spectral power

875
00:57:21,320 --> 00:57:25,030
talk about that and when you look at these images so i normalized the spectral

876
00:57:25,030 --> 00:57:30,010
energy in the image you look at the image looks extremely edgy looks really unnatural

877
00:57:30,140 --> 00:57:34,900
things like and that's because every frequency spatial frequency in the image is being given

878
00:57:34,900 --> 00:57:39,630
the same amount of power so like edges if everything was in natural images what

879
00:57:39,630 --> 00:57:43,760
tends to happen is that most of the power is concentrated in the lower frequencies

880
00:57:43,760 --> 00:57:47,360
and things so that that's almost the energy use and so on because it's comes

881
00:57:47,720 --> 00:57:51,240
low frequencies that's what we get is graceful degradation

882
00:57:51,250 --> 00:57:55,640
so this is an example of an and a natural image

883
00:57:55,690 --> 00:57:59,140
now you can all be deal that's

884
00:57:59,220 --> 00:58:03,440
that's realise i can i can watch high-definition TV but how does this apply to

885
00:58:03,440 --> 00:58:04,640
computer vision

886
00:58:04,650 --> 00:58:08,680
the reason this is useful is that even though thinking about this pixel elements into

887
00:58:08,680 --> 00:58:15,910
and in in particular you might come across jeffery's priors and reference prior the

888
00:58:15,910 --> 00:58:20,950
another maximum entropy bayes priors so these are all attempts to try and write down

889
00:58:20,950 --> 00:58:27,890
priors that don't particularly quantify anything anybody actually thinks but nevertheless have this property of being

890
00:58:27,890 --> 00:58:30,950
kind of like vanilla they don't they don't sort of

891
00:58:31,340 --> 00:58:38,720
influence inference inference too much and in particular popular to use things like this when

892
00:58:38,720 --> 00:58:40,930
you're trying to compare models for example

893
00:58:42,350 --> 00:58:47,620
but it's still an ongoing quest I mean this this is stock sorry yes

894
00:58:54,520 --> 00:58:59,870
do I have any intuition

895
00:58:59,890 --> 00:59:05,660
well the particular question the jeffery's prior is trying to to deal with is the question mentioned

896
00:59:05,660 --> 00:59:07,910
here if

897
00:59:08,040 --> 00:59:12,370
you know if you and I think the same thing about something let's but you think actually

898
00:59:12,370 --> 00:59:17,140
in terms of the standard deviation of a distribution and I think naturally about the variants

899
00:59:17,660 --> 00:59:21,310
there ought to be a way that you could write down your relative ignorance about sigma

900
00:59:21,410 --> 00:59:22,660
I can write down my

901
00:59:22,700 --> 00:59:26,290
ignorance about sigma squared and it wouldn't matter

902
00:59:26,310 --> 00:59:30,430
you know that the the answer you get in the end would would would be the same you know mines would be the square of

903
00:59:30,430 --> 00:59:36,560
yours and that's a very that's quite a strong condition actually doesn't sound much but it's quite a strong condition of what a prior

904
00:59:36,560 --> 00:59:43,520
could be and that's where jeffery's prior comes from so that's why it uses the the inference the the the information

905
00:59:44,810 --> 00:59:49,500
the problem with all of these things is the pro problem

906
00:59:49,540 --> 00:59:53,270
down at the bottom is that these are all priors that depend on the likelihood

907
00:59:53,290 --> 00:59:54,080
in some way

908
00:59:56,980 --> 01:00:00,330
if we if we want to do think of the prior information

909
01:00:00,370 --> 01:00:01,770
as being what we knew

910
01:00:03,250 --> 01:00:04,850
about theta

911
01:00:04,890 --> 01:00:06,830
before we had any data

912
01:00:06,870 --> 01:00:09,580
OK that's what prior means

913
01:00:09,580 --> 01:00:14,220
surely it also ought to be what we think about theta before we know what

914
01:00:14,220 --> 01:00:18,930
kind of data we're gonna get

915
01:00:20,060 --> 01:00:23,790
the theta is a real thing I think is a better tomorrow today tomorrow I'm going

916
01:00:23,790 --> 01:00:29,060
to find out if I'm gonna have a normally distributed observation minus theta

917
01:00:29,060 --> 01:00:33,830
or a prior distributed observation minus theta I maybe gonna find that out tomorrow but what do I think

918
01:00:33,830 --> 01:00:35,270
today about theta

919
01:00:35,640 --> 01:00:38,890
so these these views of setting priors don't allow you to have a prior in that

920
01:00:38,890 --> 01:00:41,560
situation that's quite

921
01:00:41,560 --> 01:00:46,410
troubling thought so this is this is really an active area particularly we we don't know

922
01:00:46,410 --> 01:00:47,890
how to do

923
01:00:47,910 --> 01:00:51,140
objective priors properly in large emissional problems

924
01:00:51,140 --> 01:00:55,770
it's an active research area and there isn't a clear answer at the moment

925
01:01:11,660 --> 01:01:16,120
that's my understanding of that yeah now these priors are often improper

926
01:01:19,620 --> 01:01:24,450
improper an improper distribution in probability is one that's you know its positive and non-negative

927
01:01:24,730 --> 01:01:26,660
its integral is infinite

928
01:01:26,700 --> 01:01:27,620
not one

929
01:01:30,980 --> 01:01:33,410
like a normal distribution in different variants

930
01:01:35,620 --> 01:01:38,980
are you allowed to use those well

931
01:01:39,000 --> 01:01:40,810
yes you are but be careful

932
01:01:40,830 --> 01:01:48,250
that's what this slide is improper priors are only safe to use if this have this nice property that there

933
01:01:48,290 --> 01:01:51,230
they can be viewed as limits of proper priors

934
01:01:51,410 --> 01:01:54,640
and the the posterior you get there is

935
01:01:54,730 --> 01:02:00,040
correctly the limit of the corresponding it to the posteriors

936
01:02:00,180 --> 01:02:03,390
so this is a this is a bit of advice is not not taken by anybody

937
01:02:03,390 --> 01:02:07,410
but nevertheless I'd give it you know that's the that's the test you have to apply

938
01:02:07,410 --> 01:02:11,290
every time you think about using an improper prior does it does it behave properly in

939
01:02:11,290 --> 01:02:15,080
this sense cause it doesn't always that's not always true

940
01:02:15,120 --> 01:02:21,090
OK I'm gonna use the rest of this first session to talk about some

941
01:02:21,090 --> 01:02:23,020
principles of modelling

942
01:02:23,120 --> 01:02:29,500
how do you model well it's a little difficult to do this in a

943
01:02:29,500 --> 01:02:33,140
kind of generic way right I don't have a particular thing in front of me

944
01:02:33,180 --> 01:02:38,310
a nuclear power station or financial system or something so

945
01:02:38,390 --> 01:02:40,390
but I want to keep it general so what I'm gonna do is talk about

946
01:02:40,390 --> 01:02:43,020
some themes that you see quite a lot

947
01:02:43,020 --> 01:02:44,670
when people think about modelling

948
01:02:44,830 --> 01:02:48,310
so these are quite generic ideas in

949
01:02:48,350 --> 01:02:52,560
in that people use when building models

950
01:02:52,790 --> 01:02:56,270
whether they're statisticians or machine learners I think

951
01:02:56,790 --> 01:03:00,020
and the first of these is to do is called hierarchical modelling

952
01:03:00,160 --> 01:03:06,520
and just in introducing it very informally if you've a bunch of related things

953
01:03:07,840 --> 01:03:13,320
are there rules about how you should think about

954
01:03:13,450 --> 01:03:18,060
modelling them jointly now that's something concrete

955
01:03:18,280 --> 01:03:23,340
so this is a a little sort of medical stats type example

956
01:03:25,040 --> 01:03:27,310
so you've got twelve hospitals carrying out

957
01:03:28,580 --> 01:03:30,910
kind of operation on

958
01:03:30,960 --> 01:03:37,200
it's actually this is based on data on inference cardiac surgery so these little babies

959
01:03:37,230 --> 01:03:39,270
very ill with heart defects

960
01:03:39,270 --> 01:03:43,130
times the expectation

961
01:03:43,150 --> 01:03:44,790
you have to a

962
01:03:44,920 --> 01:03:58,170
so why is that true

963
01:03:58,190 --> 01:04:03,580
so i've done is i've split the ice the expectation of the product

964
01:04:03,630 --> 01:04:09,250
it is the product of the expectations

965
01:04:09,270 --> 01:04:17,850
that's because an independent so what's independent of what

966
01:04:18,790 --> 01:04:22,040
px OK here

967
01:04:22,040 --> 01:04:23,420
random variable

968
01:04:23,440 --> 01:04:28,790
are independent of any of the other partitionings

969
01:04:28,790 --> 01:04:32,880
and the if you will the xnk that would exist for any of the other

970
01:04:32,900 --> 01:04:35,560
recursive calls

971
01:04:36,350 --> 01:04:38,500
so whatever happens in here

972
01:04:38,520 --> 01:04:41,250
is independent of what happened there

973
01:04:41,270 --> 01:04:44,020
OK so we're actually hides have a recurrence

974
01:04:44,040 --> 01:04:47,330
we're using the same we're not partition the same way each time we have a

975
01:04:47,330 --> 01:04:51,080
different one to actually have something going on underneath the mathematics you have to pay

976
01:04:51,080 --> 01:04:52,270
attention to

977
01:04:52,270 --> 01:04:56,650
that the mathematics alone is really showing which is that in t

978
01:04:56,690 --> 01:05:02,020
OK there is actually a set of random choices that are being made

979
01:05:02,020 --> 01:05:04,440
if you were OK

980
01:05:04,440 --> 01:05:08,020
so you have to understand those are independent of those in which case we can

981
01:05:08,020 --> 01:05:11,440
multiply the probabilities of their expectations

982
01:05:11,460 --> 01:05:13,630
OK everybody with me

983
01:05:13,670 --> 01:05:16,750
OK so that was the that's a big one independent of x and k from

984
01:05:16,750 --> 01:05:18,290
other random choices

985
01:05:18,310 --> 01:05:20,230
OK that's equal to

986
01:05:23,790 --> 01:05:26,850
well first of all this is nice

987
01:05:27,000 --> 01:05:30,710
what's expectation of x and k

988
01:05:30,710 --> 01:05:32,560
one already

989
01:05:32,580 --> 01:05:36,850
that's actually doesn't even belong in the summation just pop it outside

990
01:05:36,850 --> 01:05:39,040
OK so i get one over and

991
01:05:41,110 --> 01:05:42,440
times the sum

992
01:05:42,440 --> 01:05:44,150
k equal zero

993
01:05:44,290 --> 01:05:46,150
one minus one

994
01:05:46,170 --> 01:05:48,900
the expectation of the of

995
01:05:49,880 --> 01:05:52,380
i was wondering

996
01:05:52,980 --> 01:05:55,690
summation can zero

997
01:05:57,170 --> 01:06:00,330
expectation of

998
01:06:00,370 --> 01:06:02,080
t of

999
01:06:02,110 --> 01:06:05,880
and minus k minus one

1000
01:06:07,170 --> 01:06:11,670
and so should a people's hero

1001
01:06:11,710 --> 01:06:13,350
like this one

1002
01:06:17,080 --> 01:06:23,150
so that's once again using linearity of explic expectation there this time to split up

1003
01:06:23,150 --> 01:06:25,770
these pieces

1004
01:06:25,770 --> 01:06:27,940
and just factoring out

1005
01:06:27,960 --> 01:06:29,480
the expectation

1006
01:06:29,540 --> 01:06:34,610
OK is being one over and

1007
01:06:34,630 --> 01:06:37,440
everybody with me still

1008
01:06:39,270 --> 01:06:43,980
so all this is elementary just it's one of these things it's hard just because

1009
01:06:43,980 --> 01:06:46,100
there are so many steps

1010
01:06:48,460 --> 01:06:51,250
it takes that you've seen some of this before

1011
01:06:51,290 --> 01:06:58,020
OK so now the next observation is that these two summations are in fact identical

1012
01:06:58,040 --> 01:07:03,230
the same information just in a different order this is going

1013
01:07:03,250 --> 01:07:06,900
t zero t one t two t three up to t

1014
01:07:06,920 --> 01:07:08,400
i mean minus one

1015
01:07:08,460 --> 01:07:13,000
this one is going to n minus one t of n minus two t minus

1016
01:07:13,900 --> 01:07:15,100
down to

1017
01:07:15,130 --> 01:07:18,270
t is there are

1018
01:07:18,290 --> 01:07:21,730
so these are in fact equal

1019
01:07:21,770 --> 01:07:23,900
so therefore i two of them

1020
01:07:23,980 --> 01:07:51,900
and then what's this term equal to

1021
01:07:51,960 --> 01:08:00,790
so i want equal to

1022
01:08:03,560 --> 01:08:05,520
so but to see why

1023
01:08:05,540 --> 01:08:07,960
so the summation of zero

1024
01:08:07,980 --> 01:08:11,710
of DNA stadium square

1025
01:08:11,730 --> 01:08:13,940
divided by n

1026
01:08:13,960 --> 01:08:17,380
or if i want to bring the the an i have one times the summation

1027
01:08:17,380 --> 01:08:19,900
of cause one and they don't want

1028
01:08:19,900 --> 01:08:21,940
or one

1029
01:08:21,960 --> 01:08:24,980
so once again get and in the way of doing it

1030
01:08:29,540 --> 01:08:34,270
OK so this is in some sense the so this this is because the summations

1031
01:08:34,270 --> 01:08:34,790
of the

1032
01:08:34,790 --> 01:08:38,220
identical terms and this is just algebra

1033
01:08:39,180 --> 01:08:42,890
so now we're going to do is do something for technical convenience

1034
01:08:42,890 --> 01:08:45,670
is it sore

1035
01:08:45,680 --> 01:08:50,260
the k equals zero and one terms

1036
01:08:50,270 --> 01:08:54,050
into the thing in

1037
01:08:54,060 --> 01:08:59,720
four technical convenience

1038
01:09:03,640 --> 01:09:05,380
we have a recurrence here

1039
01:09:05,380 --> 01:09:07,640
private water and

1040
01:09:07,690 --> 01:09:12,640
and if i look at the cases where k equal zero or cables one i

1041
01:09:12,640 --> 01:09:16,090
know what the expectation is per year one it's

1042
01:09:16,100 --> 01:09:17,960
the expected cost is

1043
01:09:17,970 --> 01:09:21,880
the worst case cost most the worst case cost which is constant

1044
01:09:21,880 --> 01:09:23,330
OK because it's only

1045
01:09:23,340 --> 01:09:25,190
i'm only running at four

1046
01:09:25,210 --> 01:09:31,300
ah i'm only solving the problem for constant size and we know that for any

1047
01:09:31,930 --> 01:09:37,040
of the boundary cases that are solutions current subsumption constantine

1048
01:09:37,090 --> 01:09:40,420
so i can basically just take those two terms out of

1049
01:09:40,420 --> 01:09:43,840
models that they don't need high voltage to work and so of course it has

1050
01:09:43,840 --> 01:09:44,400
to be

1051
01:09:44,430 --> 01:09:46,810
to be there

1052
01:09:48,110 --> 01:09:51,850
you went to the scene of first thing to do that you could say all

1053
01:09:51,850 --> 01:09:56,670
the good depict octopus don't so i just put it on fire with the input

1054
01:09:56,670 --> 01:09:59,260
impedance have single

1055
01:09:59,270 --> 01:10:02,850
child detector capacitance and they will look at it

1056
01:10:02,880 --> 01:10:05,070
so why not

1057
01:10:05,080 --> 01:10:07,300
you will get the voltage which is

1058
01:10:07,310 --> 01:10:10,800
a function of the child and the detector capacitance

1059
01:10:10,800 --> 01:10:11,560
and again

1060
01:10:11,620 --> 01:10:15,250
have to one million people often took also then you can add

1061
01:10:15,260 --> 01:10:18,070
but again you wish

1062
01:10:18,100 --> 01:10:19,730
the that

1063
01:10:19,740 --> 01:10:20,770
you you have to

1064
01:10:20,780 --> 01:10:21,540
o five

1065
01:10:21,550 --> 01:10:23,150
the i think you know

1066
01:10:23,220 --> 01:10:29,010
the opened we also have an input capacitance of which means that you still be

1067
01:10:29,290 --> 01:10:33,760
this is to detect octopus capacitance in addition to that the fact that you and

1068
01:10:33,770 --> 01:10:37,330
i don't mean that you are more sensitive to course book i will come back

1069
01:10:37,370 --> 01:10:38,850
to that

1070
01:10:38,940 --> 01:10:42,400
unless you have to do and then we'll have twenty

1071
01:10:42,410 --> 01:10:48,040
this is the second best otherwise assisting stay constant

1072
01:10:48,130 --> 01:10:53,890
so if we do that we make what is called a child on

1073
01:10:53,990 --> 01:10:57,010
and i don't have to fail is shown here

1074
01:10:57,020 --> 01:11:00,680
so you have to do that the councils and then you

1075
01:11:00,700 --> 01:11:03,900
and all the feedback element is

1076
01:11:03,960 --> 01:11:06,410
if the capacitance if you look at

1077
01:11:06,570 --> 01:11:11,870
if you saw the equation i mean as as i have done before

1078
01:11:12,070 --> 01:11:15,210
for a few minutes ago you will find

1079
01:11:15,220 --> 01:11:18,950
what action should between the output and input is just

1080
01:11:18,970 --> 01:11:22,860
the change in the voltage output is is

1081
01:11:22,880 --> 01:11:25,300
it is a division of the child

1082
01:11:25,310 --> 01:11:31,730
by this feedback capacitance also the gain of the system is the feedback capacitance

1083
01:11:31,740 --> 01:11:33,160
i'm going to

1084
01:11:33,250 --> 01:11:37,420
its feedback capacitance value it will have to be determined by by the amount of

1085
01:11:37,420 --> 01:11:41,800
signals that you want to know that you want to to detect so

1086
01:11:41,820 --> 01:11:47,800
so a small capacitance high of again but of course the the voltage and so

1087
01:11:47,800 --> 01:11:50,130
we get four four new venture

1088
01:11:50,450 --> 01:11:52,760
and you have some it's your for you

1089
01:11:52,810 --> 01:11:58,330
you if i i suppose impossible i cannot believe among other things and that you

1090
01:11:58,340 --> 01:11:59,550
can use it

1091
01:11:59,570 --> 01:12:04,250
computer is optimal in the end you can

1092
01:12:05,240 --> 01:12:10,760
a very simple thing it turns a short signal into a long one so you

1093
01:12:10,760 --> 01:12:12,700
get the idea

1094
01:12:12,790 --> 01:12:14,990
and that's what we did before them

1095
01:12:15,000 --> 01:12:18,380
this is the end of ninety percent of particle physics detectors

1096
01:12:18,460 --> 01:12:21,910
but it's always been by this custom circuits

1097
01:12:22,850 --> 01:12:25,420
a lot of parameters after

1098
01:12:25,780 --> 01:12:28,340
adapted to this particular

1099
01:12:28,360 --> 01:12:30,410
two particular depicted

1100
01:12:30,420 --> 01:12:31,820
so that

1101
01:12:32,510 --> 01:12:34,790
so if the formulas on in

1102
01:12:34,880 --> 01:12:40,420
the game is not find for these is not confined to infinity so you will

1103
01:12:42,890 --> 01:12:48,060
films on a with respect to what

1104
01:12:48,070 --> 01:12:51,170
to what was which which means that we have a bit of a

1105
01:12:51,200 --> 01:12:55,860
of diffuse it's of detector capacitance played a small role

1106
01:12:55,920 --> 01:12:57,230
the thing is

1107
01:12:57,240 --> 01:12:59,880
you will you have

1108
01:12:59,900 --> 01:13:05,950
the don't one is not infinite and so it was designed it holds off

1109
01:13:05,960 --> 01:13:07,640
and because

1110
01:13:07,670 --> 01:13:10,720
it introduces

1111
01:13:10,730 --> 01:13:17,250
the frequency of the pension and and also it makes the input impedance is not

1112
01:13:17,260 --> 01:13:19,520
zille was it it should be used

1113
01:13:19,570 --> 01:13:21,230
mp fail was

1114
01:13:21,240 --> 01:13:22,740
it was perfect

1115
01:13:22,760 --> 01:13:24,660
and because of that

1116
01:13:24,680 --> 01:13:29,290
the output signal got as good diamonds i stand

1117
01:13:29,300 --> 01:13:32,270
it is given by

1118
01:13:32,290 --> 01:13:40,710
but by this formula in which the capacitance the picture is introduced and this

1119
01:13:40,730 --> 01:13:45,900
jews you will images you also have to deal with the gain of the amplifier

1120
01:13:45,920 --> 01:13:55,450
we give you all these frequencies this is doing input impedance of the amplifier

1121
01:13:55,460 --> 01:13:59,910
so again if you want to do

1122
01:13:59,930 --> 01:14:03,910
on the website you should you should be able to see that it's the way

1123
01:14:04,120 --> 01:14:08,850
it's to showing how you compute the input impedance so you would have to

1124
01:14:08,860 --> 01:14:10,760
to be

1125
01:14:11,320 --> 01:14:13,140
so the

1126
01:14:13,150 --> 01:14:20,040
the input impedance for an idea if we have an ideal amplifier will be because

1127
01:14:20,040 --> 01:14:23,400
it is just a the user feedback be don't divided by the gain of the

1128
01:14:23,400 --> 01:14:28,970
amplifier so this is very i suppose yourself so that's why we call this input

1129
01:14:28,970 --> 01:14:33,800
virtual on so it's not on the but in fact the don't solos you could

1130
01:14:33,800 --> 01:14:35,450
assume it's it's home

1131
01:14:36,380 --> 01:14:39,150
minimizes sensitivity to detector

1132
01:14:39,200 --> 01:14:40,610
in don't say

1133
01:14:40,610 --> 01:14:45,470
or training during a triangular matrices you just

1134
01:14:45,510 --> 01:14:49,050
we need to to know if the product of the day

1135
01:14:49,090 --> 01:14:51,760
the diagonal elements is nonzero

1136
01:14:51,780 --> 01:14:59,280
and the direction generalisation to that is that is that

1137
01:15:00,300 --> 01:15:04,320
let's see for dinner matrix a out two

1138
01:15:04,320 --> 01:15:09,090
it's two lines in two columns is a b c

1139
01:15:09,670 --> 01:15:14,800
then you can easily take that is invertible if and only if might be seen

1140
01:15:14,800 --> 01:15:19,030
as a hero in the universe is the the one that they give you there

1141
01:15:19,070 --> 01:15:22,490
in general

1142
01:15:22,550 --> 01:15:28,450
if you want to know if there is matrix debating matrix is invertible you know

1143
01:15:28,550 --> 01:15:31,780
you have to find to determine which is written this way

1144
01:15:31,800 --> 01:15:38,510
the determinant maybe for example defined recursively by saying that

1145
01:15:38,570 --> 01:15:41,170
the determinant of a matrix

1146
01:15:41,170 --> 01:15:48,920
it is the sum of the determinant of the mine the ninth of

1147
01:15:48,930 --> 01:15:53,900
these are some of the smaller determinants

1148
01:15:53,920 --> 01:15:56,720
so that's what's written there

1149
01:15:56,740 --> 01:16:02,880
thing we think changes equal to one then this formula for the determinant is the

1150
01:16:02,880 --> 01:16:04,970
sum of a i one

1151
01:16:04,990 --> 01:16:07,630
times these coefficients there

1152
01:16:07,650 --> 01:16:10,930
he i one then

1153
01:16:10,950 --> 01:16:14,300
is doing the coefficients of the

1154
01:16:14,570 --> 01:16:20,070
the first column of the matrix and the coefficient that your input here is actually

1155
01:16:20,090 --> 01:16:26,490
the the determinant of the matrix that is obtained from a without the first column

1156
01:16:26,610 --> 01:16:29,550
and we without the rule number i

1157
01:16:29,550 --> 01:16:30,900
OK so

1158
01:16:30,900 --> 01:16:35,300
this thing here is the determinant of a d minus one by d minus one

1159
01:16:35,300 --> 01:16:38,400
matrix of that is obtained from a

1160
01:16:38,420 --> 01:16:42,110
by subtracting two lines and columns

1161
01:16:46,650 --> 01:16:56,030
features affirmative from genes for the inverse of a matrix using the determinants

1162
01:16:56,030 --> 01:17:00,300
but you never use it twice a ways that because you have to find all

1163
01:17:01,380 --> 01:17:04,840
these these matrices

1164
01:17:04,900 --> 01:17:07,280
this matrix of determined there

1165
01:17:07,300 --> 01:17:09,400
and it's not really practical

1166
01:17:12,610 --> 01:17:17,070
the next thing you want to know is

1167
01:17:17,360 --> 01:17:20,380
it is

1168
01:17:22,130 --> 01:17:28,920
other ways to find and i will show how to invert the matrix

1169
01:17:29,010 --> 01:17:34,470
OK to do so then we look inside the matrix and we try to decompose

1170
01:17:34,470 --> 01:17:36,970
it into an easier matrix

1171
01:17:36,970 --> 01:17:43,650
or maybe what i should say that when you look for given values

1172
01:17:43,700 --> 01:17:49,530
then you would be looking for

1173
01:17:49,550 --> 01:17:54,380
an equivalent diagonal matrix to your matrix a

1174
01:17:56,170 --> 01:18:00,570
in terms of an open if you think of it in terms of an operator

1175
01:18:00,610 --> 01:18:06,320
there is a year dividing matrix is an operator that goes from rd two rd

1176
01:18:06,340 --> 01:18:11,820
and you may find nice business were this operator is actually diagonals

1177
01:18:11,860 --> 01:18:17,570
if you do so it's really easy to invert the matrix on this basis right

1178
01:18:17,590 --> 01:18:25,380
well it's not always possible so the operator is it doesn't necessarily have

1179
01:18:25,420 --> 01:18:32,700
diagonal form on some basis but you may want to to get used to get

1180
01:18:32,700 --> 01:18:34,200
close to that

1181
01:18:34,260 --> 01:18:38,280
so the diagonal values of the matrix or

1182
01:18:38,280 --> 01:18:42,970
this carries a message that they are there is a vector

1183
01:18:42,990 --> 01:18:47,320
a non-zero vectors and that along the AV

1184
01:18:47,360 --> 01:18:49,200
is equal to lambda v

1185
01:18:49,340 --> 01:18:54,400
the eigen values are going to be the values that you put on the the

1186
01:18:54,450 --> 01:18:58,780
the they are going to all of the equivalent of period after that is diagonal

1187
01:18:58,800 --> 01:19:00,550
OK and the

1188
01:19:00,570 --> 01:19:02,050
the these

1189
01:19:02,070 --> 01:19:05,920
so the eigen vector are going to be the vector that you put in the

1190
01:19:05,990 --> 01:19:07,490
in the new basis

1191
01:19:07,530 --> 01:19:10,450
on which the operator is local

1192
01:19:10,450 --> 01:19:15,680
whole regulate the view yourself on we also have an mid-term which rule which is

1193
01:19:15,680 --> 01:19:21,240
scheduled for the this the eighth of november at six PM so please keep that

1194
01:19:22,080 --> 01:19:24,140
so please keep that evening free

1195
01:19:24,160 --> 01:19:26,000
and so on

1196
01:19:26,020 --> 01:19:35,060
let's see

1197
01:19:35,070 --> 01:19:39,990
and one more at one one most mysterious thing i want to say is about

1198
01:19:39,990 --> 01:19:42,500
a class project on

1199
01:19:43,850 --> 01:19:48,120
the goal of this course is so the few well equipped to apply machine learning

1200
01:19:48,120 --> 01:19:53,150
algorithms to the problem or to do research in machine learning and so as part

1201
01:19:53,150 --> 01:19:58,620
of this scores so asks you to execute a small research projects a small

1202
01:19:58,710 --> 01:20:00,030
term projects

1203
01:20:00,840 --> 01:20:04,870
and what most of us do for this is easy to apply machine learning to

1204
01:20:04,880 --> 01:20:06,040
problem that

1205
01:20:06,090 --> 01:20:12,060
you find interesting or investigate some aspects of machine learning on so i don't see

1206
01:20:12,060 --> 01:20:15,860
that you are already doing research to those of you who are industry are taking

1207
01:20:15,860 --> 01:20:22,590
this different from from the company on one fantastic way to do course project would

1208
01:20:22,590 --> 01:20:27,620
be if you apply machine learning algorithms to the problem very interested in onto problem

1209
01:20:27,620 --> 01:20:32,340
that you already working on you whether it be in science research problem also the

1210
01:20:32,340 --> 01:20:36,480
problem in this you're trying to get the system to work using the learning algorithm

1211
01:20:36,540 --> 01:20:42,640
to those of you that are not currently doing research in this space

1212
01:20:43,860 --> 01:20:48,480
i one great way to do a project would be if you apply learning algorithms

1213
01:20:48,490 --> 01:20:53,160
to just pick problem the care about figure out that you find interesting and apply

1214
01:20:53,160 --> 01:20:56,180
the learning outcomes so that the idea is to see what happens

1215
01:21:02,670 --> 01:21:08,140
and the goal of the project should really be for you to do a publishable

1216
01:21:08,140 --> 01:21:11,840
piece of research in machine learning and take on

1217
01:21:11,860 --> 01:21:17,480
and because the course website you actually find a list of the projects that students

1218
01:21:17,480 --> 01:21:20,890
had done last year and so all the the semantic and you can go go

1219
01:21:20,970 --> 01:21:25,120
later and to look alike but look reading down this list i see that last

1220
01:21:25,120 --> 01:21:29,620
year there was that is that some of learning algorithms to control the snake robots

1221
01:21:29,820 --> 01:21:31,290
are fewer

1222
01:21:31,310 --> 01:21:35,090
projects on improving learning algorithms on desert

1223
01:21:35,140 --> 01:21:41,280
project on flying autonomous aircraft there was a project that he done by lt paul

1224
01:21:41,280 --> 01:21:42,480
on on this

1225
01:21:42,620 --> 01:21:49,110
improving computer vision algorithms using machine learning other cup approaches on netflix rankings using learning

1226
01:21:49,110 --> 01:21:55,630
algorithms on a few medical robotics ones and segmenting iota segmenting pieces of the body

1227
01:21:55,630 --> 01:21:59,700
using learning algorithms on one the musical instrument detection

1228
01:21:59,730 --> 01:22:02,060
another on our in a sequence alignment

1229
01:22:02,160 --> 01:22:07,710
few algorithms on understanding the brain neuroscience

1230
01:22:07,730 --> 01:22:15,330
actually quite a few approaches neuroscience on projects and understanding FMRI data brain scans on

1231
01:22:15,530 --> 01:22:22,570
and so on another project and market making the financial trading project on trying to

1232
01:22:22,570 --> 01:22:28,460
use learning algorithms to decide what is it that makes the person's face physically attractive

1233
01:22:28,500 --> 01:22:32,130
was the only album optical illusions and so on it goes to fund lots of

1234
01:22:32,130 --> 01:22:34,530
fun projects take look on

1235
01:22:34,540 --> 01:22:38,810
then you come come of your own ideas but whatever you find cool and interesting

1236
01:22:38,810 --> 01:22:40,790
i hope you feel make learning

1237
01:22:40,810 --> 01:22:45,220
the machine learning approach to questions

1238
01:22:45,250 --> 01:22:47,480
oh yes thank you

1239
01:22:47,500 --> 01:22:51,640
so precious to done in groups of up to people

1240
01:22:51,660 --> 01:22:56,700
so on as part of forming study groups on your later today is to get

1241
01:22:56,700 --> 01:23:00,930
to know cause phase side if we also encourage you to grab to other people

1242
01:23:00,930 --> 01:23:07,270
in form group up to three people for project just start brainstorming ideas for analysis

1243
01:23:07,270 --> 01:23:08,820
cells on

1244
01:23:08,860 --> 01:23:13,110
you can also contact immunity is if you want to bring some ideas with us

1245
01:23:19,590 --> 01:23:22,600
one organisation questions

1246
01:23:22,640 --> 01:23:27,290
here is how many of you know how many of you know matlab

1247
01:23:27,330 --> 01:23:28,900
well call quite a lot

1248
01:23:28,990 --> 01:23:34,530
on OK so as part of the

1249
01:23:34,570 --> 01:23:36,420
china know octave

1250
01:23:36,480 --> 01:23:37,840
i have use after

1251
01:23:37,920 --> 01:23:41,300
it was small enough so

1252
01:23:41,300 --> 01:23:45,740
as part of the scores on especially in the whole world wants to implement a

1253
01:23:45,740 --> 01:23:51,310
few programs a few machine learning algorithms as part of the whole words on and

1254
01:23:51,310 --> 01:23:56,290
most of those whole worlds will be done in either matlab or octave which is

1255
01:23:56,290 --> 01:24:00,920
sort of and some people call the free version of that allowed which is sort

1256
01:24:00,920 --> 01:24:03,270
of it sort of isn't so

1257
01:24:03,320 --> 01:24:05,810
because for those who have seen that before

1258
01:24:05,820 --> 01:24:09,270
on the intimacy have matlab is programming

1259
01:24:09,280 --> 01:24:15,980
here's the proof language that makes it very easy to write code using matrices on

1260
01:24:15,980 --> 01:24:21,160
to form new to write code numerical routines to move data around to plot data

1261
01:24:21,200 --> 01:24:27,800
on and is an extremely easy to learn tool to use for implementing learning algorithms

1262
01:24:28,970 --> 01:24:32,610
and for those of you in case somebody wants to work you know on your

1263
01:24:32,610 --> 01:24:37,110
own home computer or something we don't have the matlab licence on for the purposes

1264
01:24:37,110 --> 01:24:39,100
of this course there's also

1265
01:24:40,580 --> 01:24:46,790
there's also a software package called octave that you can download for free on the

1266
01:24:48,050 --> 01:24:51,670
it has some of fewer features and that that but is free and for the

1267
01:24:51,670 --> 01:24:57,080
purposes of this class still work for just about everything on

1268
01:24:57,090 --> 01:24:59,410
so actually i

1269
01:24:59,440 --> 01:25:05,660
so just to psychology for those you have seen that before i guess on one

1270
01:25:06,160 --> 01:25:11,210
a colleague of mine and the different university knowledge that they actually teaches teachers and

1271
01:25:11,210 --> 01:25:13,330
other machine learning class on

1272
01:25:13,330 --> 01:25:17,410
he sort of many years so one day on

1273
01:25:17,440 --> 01:25:21,810
he was in his office and posted and to his for about ten years ago

1274
01:25:21,810 --> 01:25:26,080
came into his office and he said oh professor professor thank you so much for

1275
01:25:26,100 --> 01:25:30,550
machine learning class i learned so much from it on the stuff that i don't

1276
01:25:30,550 --> 01:25:32,260
it's the particular

1277
01:25:32,320 --> 01:25:33,480
question that

1278
01:25:33,490 --> 01:25:37,170
i may ask i have a probabilistic model that relates the weather today

1279
01:25:37,220 --> 01:25:42,700
with the temperature today with the pressure during the with all the available for example

1280
01:25:42,700 --> 01:25:44,910
what happened yesterday at this time

1281
01:25:44,920 --> 01:25:47,730
if you have a bunch of random variables and how they depend on each other

1282
01:25:47,730 --> 01:25:51,800
i can ask particular query about one of them

1283
01:25:53,320 --> 01:25:54,880
two of them

1284
01:25:54,880 --> 01:26:01,050
examples of computing probabilities of particular outcomes

1285
01:26:01,070 --> 01:26:02,770
and also i make

1286
01:26:04,210 --> 01:26:06,510
particularly interesting

1287
01:26:06,560 --> 01:26:13,550
outcomes i mean particularly interesting configurations of my probabilistic model for example

1288
01:26:13,600 --> 01:26:16,820
finding the most likely outcome

1289
01:26:18,050 --> 01:26:24,410
have a given probabilistic model like for example a noisy remember the example of

1290
01:26:24,450 --> 01:26:26,270
of image processing

1291
01:26:26,320 --> 01:26:28,420
i have a noisy image

1292
01:26:28,420 --> 01:26:34,790
we could ask the following question what's the most likely corrections for his entire image

1293
01:26:34,860 --> 01:26:38,910
given how images in general became

1294
01:26:38,960 --> 01:26:42,520
and given the pattern of these lights

1295
01:26:44,340 --> 01:26:48,840
given the image in terms of service

1296
01:26:48,850 --> 01:26:51,000
so an interesting question

1297
01:26:53,030 --> 01:26:55,600
solving this problem for example with

1298
01:26:55,610 --> 01:27:00,630
in that context would give us a solution to the noise problem

1299
01:27:00,640 --> 01:27:04,110
so i mean in graphical models will be concerned with

1300
01:27:04,120 --> 01:27:09,200
asking this very same questions that we as with any probabilistic model

1301
01:27:09,210 --> 01:27:14,540
it's just that seems to have a particular type of structure given by conditional

1302
01:27:14,590 --> 01:27:16,570
independence statements

1303
01:27:16,590 --> 01:27:20,400
the way we answer for these different questions

1304
01:27:20,410 --> 01:27:22,780
has a particular

1305
01:27:23,150 --> 01:27:24,970
i just think

1306
01:27:24,990 --> 01:27:25,780
you know

1307
01:27:25,820 --> 01:27:30,380
we need to learn specific algorithms to answer for this particular question

1308
01:27:30,430 --> 01:27:31,260
well this

1309
01:27:31,280 --> 01:27:33,700
another unified problems

1310
01:27:33,710 --> 01:27:36,520
answering these questions is much more trivial

1311
01:27:37,890 --> 01:27:40,640
for example

1312
01:27:40,680 --> 01:27:42,290
you have

1313
01:27:44,540 --> 01:27:47,590
it's just univariate gaussian distribution

1314
01:27:49,670 --> 01:27:53,920
i want to ask how many people have seen this

1315
01:28:02,930 --> 01:28:06,250
well this is something extremely simple

1316
01:28:06,370 --> 01:28:09,820
it's the preferred model

1317
01:28:10,340 --> 01:28:12,390
for many

1318
01:28:12,410 --> 01:28:14,870
for many things in life

1319
01:28:15,050 --> 01:28:19,430
let's see how those things happen basically

1320
01:28:19,480 --> 01:28:20,410
we have

1321
01:28:20,410 --> 01:28:28,410
a unified galson distribution here we have two parameters have the means and if i

1322
01:28:28,490 --> 01:28:30,250
and the

1323
01:28:30,270 --> 01:28:34,310
the first question to be to estimate the mean and the violence giving

1324
01:28:34,340 --> 01:28:36,620
observation right

1325
01:28:37,140 --> 01:28:39,340
that's one question you can ask

1326
01:28:39,390 --> 01:28:40,920
i mean

1327
01:28:40,960 --> 01:28:43,450
very difficult to find this in

1328
01:28:43,500 --> 01:28:46,140
solutions for this problem here

1329
01:28:46,140 --> 01:28:47,740
i mean you can find them

1330
01:28:47,800 --> 01:28:52,200
and the text books have been written more than fifty years ago

1331
01:28:52,240 --> 01:28:56,860
you can also sample from the distribution very easy

1332
01:28:56,860 --> 01:28:59,250
we have a single viable

1333
01:28:59,270 --> 01:29:00,690
i mean

1334
01:29:00,870 --> 01:29:04,220
papers written more than fifty years about fifty years ago

1335
01:29:04,320 --> 01:29:08,210
he was some nice ways to sample from mcgovern distribution efficiently

1336
01:29:08,220 --> 01:29:09,770
and africa

1337
01:29:09,790 --> 01:29:12,360
you can compute areas under these

1338
01:29:12,360 --> 01:29:13,860
core right

1339
01:29:13,880 --> 01:29:15,860
which is compute the probability

1340
01:29:15,870 --> 01:29:18,200
also given

1341
01:29:20,730 --> 01:29:22,980
given elements of samples

1342
01:29:23,030 --> 01:29:24,440
to compute areas

1343
01:29:24,450 --> 01:29:26,890
we just need to integrate this quantity

1344
01:29:26,900 --> 01:29:28,700
in particular

1345
01:29:30,310 --> 01:29:36,480
well you can also find what's the most likely value of this probability distribution

1346
01:29:37,530 --> 01:29:39,460
well you can work on

1347
01:29:39,470 --> 01:29:41,950
you can compute derivatives for example

1348
01:29:42,000 --> 01:29:43,320
if you go out

1349
01:29:43,360 --> 01:29:46,350
when the derivative is going to be easier

1350
01:29:46,370 --> 01:29:50,840
and you see that the only find value for going zero is exactly this fellow

1351
01:29:50,840 --> 01:29:52,790
this morning we looked at

1352
01:29:52,800 --> 01:29:54,940
maximum likelihood

1353
01:29:54,950 --> 01:30:00,300
and we look at the algorithm for maximizing the likelihood function

1354
01:30:00,350 --> 01:30:01,340
what i want to do

1355
01:30:01,380 --> 01:30:02,270
in this

1356
01:30:02,280 --> 01:30:05,690
lecture is now to look at the bayesian approach

1357
01:30:05,730 --> 01:30:09,270
and talk about variational inference in the context of

1358
01:30:09,320 --> 01:30:12,410
bayesian learning

1359
01:30:12,430 --> 01:30:14,010
so in the bayesian

1360
01:30:14,060 --> 01:30:18,620
framework we introduce prior distributions over the parameters

1361
01:30:19,560 --> 01:30:21,740
much of the discussion will be very general

1362
01:30:21,750 --> 01:30:24,090
but i will again illustrates

1363
01:30:24,110 --> 01:30:43,290
the idea is simple examples including things like the gas mixture model

1364
01:30:45,140 --> 01:30:47,740
all OK with everybody is a new feature

1365
01:30:47,750 --> 01:30:57,240
we're doing user studies at the moment to see

1366
01:31:04,850 --> 01:31:08,600
so we introduce prior distributions over all the parameters

1367
01:31:08,610 --> 01:31:10,770
and so this is just

1368
01:31:10,780 --> 01:31:12,750
this can be represented graphically

1369
01:31:13,040 --> 01:31:18,530
by simply adding extra nodes to the graph representing these prior distributions

1370
01:31:18,630 --> 01:31:23,930
so just have an expander graph so learning in a bayesian setting of course corresponds

1371
01:31:23,930 --> 01:31:29,990
to finding posterior distributions over parameters and then using those posterior distributions to make predictions

1372
01:31:29,990 --> 01:31:31,130
for new

1373
01:31:31,390 --> 01:31:33,950
test points

1374
01:31:33,960 --> 01:31:37,710
because this is an expander graph learning is really just in front

1375
01:31:37,960 --> 01:31:40,140
just inference on this expander graph

1376
01:31:40,150 --> 01:31:42,050
so we at this rather nice

1377
01:31:42,100 --> 01:31:43,430
results that

1378
01:31:43,440 --> 01:31:49,300
learning in a bayesian setting is just just inference problem again

1379
01:31:49,350 --> 01:31:53,480
and in particular there's no fundamental distinction between

1380
01:31:54,170 --> 01:31:57,490
latent variables hidden variables

1381
01:31:57,500 --> 01:31:59,990
and parameters

1382
01:32:00,100 --> 01:32:02,760
in one sense a sort of the distinction if we have a whole set of

1383
01:32:02,760 --> 01:32:07,720
data points we have one latent variable per data point may be a shared parameter

1384
01:32:07,760 --> 01:32:10,300
so in that sense they can be a distinction between

1385
01:32:10,310 --> 01:32:12,180
latent variables and parameters

1386
01:32:12,310 --> 01:32:16,160
fundamentally in this framework there is no difference they just

1387
01:32:17,500 --> 01:32:21,350
variables stochastic variables in some big complicated graph

1388
01:32:21,360 --> 01:32:24,930
and we're going to observe some of the variables that sort of data

1389
01:32:24,940 --> 01:32:26,940
and then what inference two

1390
01:32:26,960 --> 01:32:31,970
find posterior distributions of other variables which called trying to predict

1391
01:32:31,980 --> 01:32:38,550
so for example we can take the mixture of gaussians model and we can make

1392
01:32:38,550 --> 01:32:40,800
it a bayesian model

1393
01:32:40,810 --> 01:32:43,460
so this is the structure that we had before

1394
01:32:43,470 --> 01:32:45,490
these are the observations

1395
01:32:45,530 --> 01:32:49,020
capital and for each observation each observed

1396
01:32:49,040 --> 01:32:50,300
data vector

1397
01:32:50,310 --> 01:32:51,940
the binary hidden

1398
01:32:51,960 --> 01:32:54,540
variable z which is the indicator variable

1399
01:32:54,560 --> 01:32:57,160
yes which component generated each of the

1400
01:32:57,180 --> 01:32:58,550
data points

1401
01:32:58,570 --> 01:33:04,090
so the prior distribution of z is governed by some mixing coefficients

1402
01:33:04,140 --> 01:33:06,180
and so we can put a prior distribution

1403
01:33:06,390 --> 01:33:08,770
those shown by this node

1404
01:33:08,780 --> 01:33:12,240
and this is the a guassian so we have a conjugate prior

1405
01:33:12,280 --> 01:33:14,130
over the mean

1406
01:33:14,140 --> 01:33:18,300
and the precision of the gas in

1407
01:33:18,350 --> 01:33:27,410
and so conjugacy in this model

1408
01:33:27,460 --> 01:33:28,790
so p of pi

1409
01:33:28,810 --> 01:33:30,500
it is

1410
01:33:30,550 --> 01:33:32,590
just to dirichlet which is

1411
01:33:32,650 --> 01:33:35,280
products on k

1412
01:33:35,330 --> 01:33:38,510
i k and is usually written alpha k minus one

1413
01:33:38,630 --> 01:33:39,950
the some

1414
01:33:39,970 --> 01:33:44,550
normalisation constant at the front and you can see that conjugate because

1415
01:33:44,630 --> 01:33:47,240
recall p of x given pi

1416
01:33:47,250 --> 01:33:50,420
it was the products on k

1417
01:33:50,710 --> 01:33:53,160
k to set k

1418
01:33:53,170 --> 01:33:55,670
so when we multiply this by this

1419
01:33:55,680 --> 01:33:58,770
the posterior distribution of pilot can be high

1420
01:33:58,780 --> 01:34:00,300
raised to some power

1421
01:34:00,320 --> 01:34:02,100
alpha tilde minus one

1422
01:34:02,200 --> 01:34:05,460
the posterior distribution has the same form as prior

1423
01:34:05,520 --> 01:34:07,840
we can certainly write down the

1424
01:34:09,010 --> 01:34:11,140
prior distribution for the mean

1425
01:34:11,150 --> 01:34:13,860
and precision which is the inverse covariance

1426
01:34:13,920 --> 01:34:16,860
of the garrison

1427
01:34:16,870 --> 01:34:20,940
and that's the thing called normal wishart

1428
01:34:20,990 --> 01:34:22,390
so we have the prior

1429
01:34:22,400 --> 01:34:24,210
on land

1430
01:34:24,230 --> 01:34:28,020
which is a wishart distribution

1431
01:34:28,070 --> 01:34:32,350
w is governed by matrix and the parameter

1432
01:34:32,400 --> 01:34:34,210
it's essentially

1433
01:34:34,260 --> 01:34:39,010
the first exact form is something like the

1434
01:34:39,020 --> 01:34:40,430
well involves the trace

1435
01:34:41,740 --> 01:34:46,680
and some matrix which can write w inverse to w matrix

1436
01:34:47,730 --> 01:34:49,740
the parameters of the distribution

1437
01:34:49,750 --> 01:34:53,740
and again you can see the controversy structure because the

1438
01:34:54,090 --> 01:34:56,470
in the gas in the exponential

1439
01:34:56,480 --> 01:35:00,960
of the trade of lambda times something with the something is x minus mu x

1440
01:35:00,960 --> 01:35:02,940
minus mu transpose

1441
01:35:02,960 --> 01:35:05,460
then he is

1442
01:35:05,510 --> 01:35:12,740
new prior for mu depends on lambda in order to have conjugacy

1443
01:35:12,760 --> 01:35:15,770
and this is the normal distribution

1444
01:35:15,780 --> 01:35:16,810
which is

1445
01:35:17,090 --> 01:35:19,380
has some of the

1446
01:35:19,430 --> 01:35:20,350
but it's

1447
01:35:20,430 --> 01:35:22,660
variant three precision rather

1448
01:35:26,990 --> 01:35:28,640
proportional to

1449
01:35:28,690 --> 01:35:32,340
the so my notation this is the main the covariance

1450
01:35:32,360 --> 01:35:33,320
so the

1451
01:35:33,330 --> 01:35:37,800
the inverse covariance the precision to the precision of gas prior is proportional to the

1452
01:35:39,980 --> 01:35:42,480
of the original guassian lambda

1453
01:35:42,490 --> 01:35:47,230
so again it's sort of an exercise five minutes associated paper to verify that the

1454
01:35:47,230 --> 01:35:51,220
product of these which is joint distribution of immune lambda

1455
01:35:51,230 --> 01:35:59,540
is the conjugate prior for the gaps in

1456
01:35:59,580 --> 01:36:02,900
is an intuition why depends on lambda i don't have an intuition but if you

1457
01:36:02,900 --> 01:36:05,690
look through them as you can see that it's you know a few minutes the

1458
01:36:05,690 --> 01:36:07,170
and equals two

1459
01:36:07,180 --> 01:36:12,350
raise a siouxsie in this example is one one-dimensional one and so

1460
01:36:13,930 --> 01:36:20,090
it was later map consists of course disease

1461
01:36:20,100 --> 01:36:24,110
and that we use some

1462
01:36:24,210 --> 01:36:27,530
so the point is there

1463
01:36:27,550 --> 01:36:33,460
and lastly the small was visually that a place along calcium but rather we should

1464
01:36:33,460 --> 01:36:35,220
be say

1465
01:36:35,340 --> 01:36:38,090
and sample

1466
01:36:38,100 --> 01:36:40,340
change of colors

1467
01:36:41,580 --> 01:36:49,550
the axis of the maybe that's that that would be a typical sample of the

1468
01:36:49,550 --> 01:36:52,050
axis under this model

1469
01:37:03,820 --> 01:37:07,720
so how do i fit the parameters of the small

1470
01:37:11,540 --> 01:37:15,750
the joint distribution

1471
01:37:18,010 --> 01:37:19,820
xeon x

1472
01:37:19,830 --> 01:37:27,580
you know is actually calcium what parameters given by some vector they realize meex z

1473
01:37:27,620 --> 01:37:29,590
and some covariance matrix sigma

1474
01:37:29,640 --> 01:37:33,880
on the right to what those things are

1475
01:37:33,900 --> 01:37:39,160
this vector may be as effective zeroes appended to the vector mu

1476
01:37:39,300 --> 01:37:43,390
and the matrix a square

1477
01:37:43,400 --> 01:37:44,480
is the

1478
01:37:44,590 --> 01:37:49,330
partitions made

1479
01:37:49,880 --> 01:37:53,020
so what is the last time

1480
01:37:53,040 --> 01:37:59,750
so you can also what the distribution of x under this model

1481
01:37:59,770 --> 01:38:01,750
and the answer is

1482
01:38:01,770 --> 01:38:05,360
well under this model excess calcium

1483
01:38:05,410 --> 01:38:08,620
i mean you

1484
01:38:08,670 --> 01:38:16,850
and covariance lambda lambda the transposable side it's just the second block the mean vector

1485
01:38:16,890 --> 01:38:18,930
and take that law

1486
01:38:19,030 --> 01:38:25,110
the the right-hand corner block of the covariance matrix and so this is this is

1487
01:38:25,860 --> 01:38:31,460
my formula for computing the marginal distribution of the gaussians that that computing the marginal

1488
01:38:31,460 --> 01:38:36,400
distribution of you know the second half of the vector of the first

1489
01:38:36,420 --> 01:38:40,300
so this is the marginal distribution of x under my model

1490
01:38:42,020 --> 01:38:45,150
and so if you want to learn

1491
01:38:51,010 --> 01:38:55,860
let's see here this one writing down the

1492
01:38:55,920 --> 01:39:01,790
right this is really specifying the conditional distribution of x given z right so on

1493
01:39:01,790 --> 01:39:04,390
the conditional distribution of x given the

1494
01:39:04,460 --> 01:39:06,850
this is gaussians

1495
01:39:06,860 --> 01:39:09,160
well i mean on

1496
01:39:09,170 --> 01:39:15,560
the from the covariance side as well that specified

1497
01:39:28,760 --> 01:39:33,490
this is the marginal distribution of x given my training sets of

1498
01:39:33,510 --> 01:39:38,270
and unlabelled examples i can actually write down the log likelihood of a training set

1499
01:39:38,270 --> 01:39:44,170
right so the likelihood of a training set is actually this right on the likelihood

1500
01:39:44,170 --> 01:39:49,690
of the likelihood of my pseudo likelihood of parameters given my training set is the

1501
01:39:49,690 --> 01:39:57,520
product and i just want to things i given parameters

1502
01:39:57,540 --> 01:40:00,940
and on

1503
01:40:00,950 --> 01:40:07,650
i actually write down what others because x i is calcium with random you

1504
01:40:07,660 --> 01:40:13,520
and variance lambda lambda transposed times site savings rate is down as you know going

1505
01:40:13,520 --> 01:40:17,470
over on the

1506
01:40:17,600 --> 01:40:26,240
so i'm going to right the times and minus one

1507
01:40:43,620 --> 01:40:45,630
so that's why

1508
01:40:47,600 --> 01:40:50,810
for the density of the calcium that has mean mu

1509
01:40:50,860 --> 01:40:53,020
and covariance

1510
01:40:53,040 --> 01:40:55,360
lambda lambda transpose possi

1511
01:40:56,900 --> 01:41:01,110
so this is my likelihood of the parameters given a training set

1512
01:41:01,120 --> 01:41:05,520
and one thing you could do is actually take this likelihood and try to maximize

1513
01:41:05,520 --> 01:41:10,060
in terms of the parameters to try to find them as likely as the parameters

1514
01:41:10,920 --> 01:41:15,390
but if you do that you find that tried you take well to give the

1515
01:41:15,390 --> 01:41:20,260
log likelihood to characters that there is a zero you find that you be unable

1516
01:41:20,270 --> 01:41:22,510
to solve for the maximum of this

1517
01:41:22,550 --> 01:41:24,660
on analytically

1518
01:41:24,680 --> 01:41:26,150
so news

1519
01:41:26,160 --> 01:41:29,750
you won't be able to solve this by like this patient problem if you take

1520
01:41:29,750 --> 01:41:34,790
all right so

1521
01:41:34,860 --> 01:41:38,540
we have got until

1522
01:41:38,540 --> 01:41:43,040
nine thirty and questions

1523
01:41:44,860 --> 01:41:46,730
one copy

1524
01:41:46,750 --> 01:41:49,130
so i'm going to start is with the movie

1525
01:41:49,160 --> 01:41:53,820
the o thanks

1526
01:41:54,050 --> 01:41:56,550
the people in the back and here we have

1527
01:41:57,020 --> 01:42:04,270
they ransacked

1528
01:42:04,280 --> 01:42:07,350
so i don't know what we've seen is an introduction

1529
01:42:07,730 --> 01:42:12,520
to machine learning there was this whirlwind tour markets yesterday

1530
01:42:12,560 --> 01:42:16,770
and then there was a good analysis of the different kinds of data

1531
01:42:16,870 --> 01:42:19,970
with the rise in practice italics showed you

1532
01:42:21,630 --> 01:42:25,150
part of this summer school is going to be more computation

1533
01:42:25,160 --> 01:42:30,200
computation first specific kind of models and as as we go for the next few

1534
01:42:30,200 --> 01:42:34,300
days will be more clear what kind of model this type of competition is useful

1535
01:42:34,300 --> 01:42:36,690
or it's not useful for everything

1536
01:42:36,730 --> 01:42:37,480
and so

1537
01:42:37,530 --> 01:42:41,110
part of this will be that you get an idea when you can use these

1538
01:42:41,300 --> 01:42:45,340
method and when she you shouldn't try

1539
01:42:45,420 --> 01:42:52,870
all going to be based on sampling and simulation and so on so quite often

1540
01:42:52,870 --> 01:42:54,690
the goal of that do

1541
01:42:54,690 --> 01:42:58,810
it's obviously to do machine learning it's obviously to compute parameters

1542
01:42:58,860 --> 01:43:00,800
solve integrals compute

1543
01:43:00,840 --> 01:43:03,610
this normalisation constants the markers

1544
01:43:03,630 --> 01:43:05,190
talking about yesterday

1545
01:43:05,190 --> 01:43:07,110
but quite often

1546
01:43:07,130 --> 01:43:10,720
it will be just as the simulation device

1547
01:43:10,800 --> 01:43:14,030
simulation is important because

1548
01:43:14,080 --> 01:43:17,280
if i want to kind of

1549
01:43:17,300 --> 01:43:19,310
do it task like

1550
01:43:19,310 --> 01:43:20,670
go to sydney

1551
01:43:20,780 --> 01:43:24,750
i'm trying to decide what to do this we can kind of simulated my head

1552
01:43:24,750 --> 01:43:28,050
how final will be one from the sydney mardi gras and all that

1553
01:43:30,170 --> 01:43:34,110
and that kind of gives me that allows me to make decisions

1554
01:43:34,110 --> 01:43:35,230
and that's actually how

1555
01:43:35,530 --> 01:43:38,410
in fact to come into the sort of thinking

1556
01:43:38,530 --> 01:43:40,620
i want to make a decision in the future

1557
01:43:40,630 --> 01:43:44,340
i have some prior knowledge my head of state of knowledge in my head and

1558
01:43:44,340 --> 01:43:47,570
from that around simulations into the future

1559
01:43:47,620 --> 01:43:50,030
so we're going to see a bit of that and that sort of

1560
01:43:50,060 --> 01:43:54,490
o be a preamble for worksop is going to talk about later when he

1561
01:43:54,500 --> 01:43:58,870
starts talking about reinforcement learning

1562
01:43:58,900 --> 01:44:01,190
OK so

1563
01:44:03,880 --> 01:44:05,990
a clear indication of we sample

1564
01:44:06,720 --> 01:44:09,880
we've seen this before

1565
01:44:09,940 --> 01:44:12,290
if you have forced climb

1566
01:44:13,280 --> 01:44:14,620
i have a video

1567
01:44:14,630 --> 01:44:15,820
it's this tube

1568
01:44:15,840 --> 01:44:20,060
pictures in this video for sort of flashing between one and the other

1569
01:44:20,120 --> 01:44:23,070
and there is a difference between the two

1570
01:44:23,130 --> 01:44:27,660
how many people have seen the difference of and once you see it please don't

1571
01:44:27,760 --> 01:44:29,340
and also it

1572
01:44:29,440 --> 01:44:31,260
the rest three people

1573
01:44:31,590 --> 01:44:33,750
only human

1574
01:44:33,750 --> 01:44:36,870
it's quite a lot of number of pixels

1575
01:44:36,940 --> 01:44:41,320
respect to the image

1576
01:44:41,340 --> 01:44:46,690
how many people have it

1577
01:44:46,710 --> 01:44:49,750
it's like a because of waking up

1578
01:44:49,760 --> 01:44:56,630
it is that getting up in the morning

1579
01:44:58,310 --> 01:45:02,720
that motion seems to be developing

1580
01:45:02,720 --> 01:45:03,870
many people

1581
01:45:03,870 --> 01:45:06,650
i have it now

1582
01:45:06,660 --> 01:45:09,440
for those who haven't seen it

1583
01:45:09,440 --> 01:45:13,510
what do airplanes need in order to fly

1584
01:45:21,840 --> 01:45:28,760
the end to gives you haven't quite figured out

1585
01:45:28,810 --> 01:45:30,130
OK so

1586
01:45:30,150 --> 01:45:34,220
this is a phenomenon called change blindness in psychology and its

1587
01:45:34,280 --> 01:45:41,470
it happens with images that happens in the visual cortex in the visual field

1588
01:45:41,490 --> 01:45:43,840
starts in the visual cortex

1589
01:45:43,850 --> 01:45:50,690
but it's it's characteristic of human brain mind and brain how we work

1590
01:45:50,730 --> 01:45:52,430
we sample the world

1591
01:45:52,480 --> 01:45:55,880
the reason why you don't see it is because we never see this image

1592
01:45:55,940 --> 01:46:00,310
this is an illusion we believe we sing it but we don't we sampling

1593
01:46:00,370 --> 01:46:03,950
and we only extract from it what we need and we do our reconstruction or

1594
01:46:05,060 --> 01:46:07,540
so when you believe using the whole thing

1595
01:46:07,570 --> 01:46:11,080
and the same is true for all types of arguments if you're in trouble

1596
01:46:11,170 --> 01:46:15,200
unified with your boyfriend on think you write and sign you might just not be

1597
01:46:15,200 --> 01:46:16,690
seeing the whole picture

1598
01:46:17,700 --> 01:46:23,130
and that's probably more familiar

1599
01:46:23,190 --> 01:46:29,630
you know it's some theories of cognition now it's about there being conscious into being

1600
01:46:29,630 --> 01:46:33,850
subconscious processes and you just basically covering this

1601
01:46:33,870 --> 01:46:36,210
subconscious processes so that

1602
01:46:36,230 --> 01:46:38,180
the crisis seems to have

1603
01:46:39,230 --> 01:46:41,790
and a good understanding of what's going on

1604
01:46:41,820 --> 01:46:44,680
this something mechanism keeping things control

1605
01:46:44,790 --> 01:46:49,980
this is again like alex said yesterday please don't take this into projecting that any

1606
01:46:49,980 --> 01:46:54,110
of the methods of talk about here described how the brain works it's more

1607
01:46:54,120 --> 01:46:56,520
you something is important

1608
01:46:56,560 --> 01:46:59,890
it's useful for humans whenever you have limited capacity

1609
01:46:59,900 --> 01:47:02,290
to process something we humans don't have

1610
01:47:02,350 --> 01:47:06,620
or problems for that matter the capacity to process the whole visual field

1611
01:47:07,810 --> 01:47:22,130
focus on certain areas of the space

1612
01:47:22,140 --> 01:47:28,130
right so that's the end result

1613
01:47:28,130 --> 01:47:30,140
all right so i think this

1614
01:47:30,190 --> 01:47:36,200
lastly i kind of thought halfway through the deletion process section so it is just

1615
01:47:36,230 --> 01:47:37,670
remind you

1616
01:47:38,950 --> 01:47:45,240
dirichlet process is basically a random probability measure that satisfies this conditions for so for

1617
01:47:45,250 --> 01:47:46,880
any set partitions

1618
01:47:46,900 --> 01:47:51,650
of four space so if it is our space partitioning to us to find the

1619
01:47:51,650 --> 01:47:54,010
number of subsets that

1620
01:47:56,810 --> 01:48:00,680
it is random because g is random input to measure

1621
01:48:01,150 --> 01:48:05,360
and is the vector sum to one and each entry is nonnegative because each entry

1622
01:48:05,360 --> 01:48:10,080
here is the mass assigned to each subset

1623
01:48:10,220 --> 01:48:15,350
and what we see is that this gene is the dirichlet process if for every

1624
01:48:15,350 --> 01:48:17,530
such finite partition this factor here

1625
01:48:17,960 --> 01:48:21,970
is distributed according to a dirichlet distribution

1626
01:48:21,980 --> 01:48:26,110
with the with the parameters given by

1627
01:48:26,290 --> 01:48:30,010
base base measure evaluated the same subset

1628
01:48:30,010 --> 01:48:34,250
and as we saw this set of

1629
01:48:34,990 --> 01:48:41,800
the set conditions is is consistent or in peter's work is projective right so we

1630
01:48:41,800 --> 01:48:48,240
can use common graphs theorem to show that there exists a

1631
01:48:49,940 --> 01:48:51,910
a random probability measure over

1632
01:48:52,270 --> 01:48:54,850
that satisfies all

1633
01:48:54,850 --> 01:48:57,100
all of these conditions

1634
01:48:58,900 --> 01:49:01,510
o thing is this

1635
01:49:03,750 --> 01:49:08,180
OK so

1636
01:49:08,190 --> 01:49:10,850
if we look at say

1637
01:49:11,570 --> 01:49:14,430
this set of

1638
01:49:15,830 --> 01:49:22,660
distribution problems of probability distributions we have dirichlet alpha and then we take this mess

1639
01:49:22,660 --> 01:49:27,240
and then we split into two forward to offer over to take this one split

1640
01:49:27,240 --> 01:49:27,990
into two

1641
01:49:28,020 --> 01:49:33,080
this one split into two iteration of awful awful awful awful awful and we can

1642
01:49:33,910 --> 01:49:39,890
doing this and what we see from here is that this that islam is consistent

1643
01:49:41,630 --> 01:49:45,470
the usually of a single parameter alpha is simply going to be

1644
01:49:46,480 --> 01:49:48,790
it's just gonna put all its mass and one

1645
01:49:52,320 --> 01:49:54,320
this distribution

1646
01:49:54,320 --> 01:49:58,810
a draw some of it are random vectors of of length two that sums to

1647
01:49:59,420 --> 01:50:02,280
so we can visualize this as count tool

1648
01:50:02,290 --> 01:50:05,980
two bars that whose total area sums to one

1649
01:50:06,010 --> 01:50:07,570
and similarly for

1650
01:50:07,590 --> 01:50:11,910
for this thing can visualize a random draw from it as

1651
01:50:11,970 --> 01:50:13,790
four bars was

1652
01:50:13,810 --> 01:50:16,410
it was totally has to sum to one

1653
01:50:20,890 --> 01:50:24,450
this is the demo to show you so we started with

1654
01:50:24,470 --> 01:50:25,810
this one right

1655
01:50:25,820 --> 01:50:31,160
so the degree area here is the total area is one

1656
01:50:31,220 --> 01:50:33,970
so we split this into two and

1657
01:50:33,980 --> 01:50:37,980
that's kind of to that the high some or less the same

1658
01:50:38,000 --> 01:50:41,720
this slide is a little different so this is a draw from dirichlet of over

1659
01:50:41,720 --> 01:50:46,870
two of over two and we can keep on repeating this beating each each of

1660
01:50:46,870 --> 01:50:48,510
these two bus into two

1661
01:50:48,510 --> 01:50:53,350
so make this this is draw from a dirichlet alpha alpha before of over four

1662
01:50:53,350 --> 01:50:55,170
for over four and a half

1663
01:50:56,310 --> 01:50:58,090
and we can keep on doing

1664
01:50:58,130 --> 01:51:02,750
doing this with english by the two and having to draw from the corresponding to

1665
01:51:03,530 --> 01:51:06,880
and if you do this lot of times

1666
01:51:06,940 --> 01:51:08,640
then we see that

1667
01:51:08,790 --> 01:51:11,530
the resulting

1668
01:51:11,540 --> 01:51:13,440
this basically looks like a random

1669
01:51:13,560 --> 01:51:15,060
probability measure

1670
01:51:15,070 --> 01:51:17,660
a random distribution that

1671
01:51:17,670 --> 01:51:19,810
such that such that

1672
01:51:20,220 --> 01:51:24,970
this is a random probability measure such that for any draw

1673
01:51:24,980 --> 01:51:27,790
you can look like this is an infinite sum

1674
01:51:27,810 --> 01:51:31,190
of of the functions of points passes

1675
01:51:36,690 --> 01:51:41,320
we see that it kind looks like the following so g random draw from a

1676
01:51:41,320 --> 01:51:45,630
dirichlet process turns out to be always going to be a infinite sum

1677
01:51:45,690 --> 01:51:51,690
of point masses this weighted so the case point mass is going to be located

1678
01:51:51,690 --> 01:51:54,140
at theta stocky

1679
01:51:54,170 --> 01:51:56,590
and has a mass of pikey

1680
01:51:56,630 --> 01:51:59,750
of course the pages have to be nonnegative and sum to one

1681
01:51:59,820 --> 01:52:03,990
so that this thing actually defines a probability measure

1682
01:52:04,010 --> 01:52:06,650
and if a cases atoms

1683
01:52:06,680 --> 01:52:07,690
in our

1684
01:52:07,700 --> 01:52:09,170
a probability space

1685
01:52:10,880 --> 01:52:14,620
so this two questions that we can ask one is

1686
01:52:14,700 --> 01:52:16,690
what is that

1687
01:52:16,710 --> 01:52:19,210
joint distribution over

1688
01:52:19,300 --> 01:52:20,730
of the weights

1689
01:52:20,740 --> 01:52:24,690
and over the locations of the atoms

1690
01:52:24,710 --> 01:52:28,420
so if we can understand what's the joint is a distribution over both of this

1691
01:52:28,420 --> 01:52:29,150
then we

1692
01:52:29,170 --> 01:52:32,190
i have a pretty good idea of how does she look like

1693
01:52:33,980 --> 01:52:38,800
so these a kind of different representations of dirichlet process that gives us some understanding

1694
01:52:39,400 --> 01:52:44,760
what is this strange beast the second thing which we we might do is since

1695
01:52:44,760 --> 01:52:48,450
g is a probability measure then we can treat it as

1696
01:52:48,460 --> 01:52:50,510
a distribution that we can draw from

