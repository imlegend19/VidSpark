1
00:00:00,000 --> 00:00:01,090
so let's start

2
00:00:05,220 --> 00:00:09,060
what we're going to do we are going to follow part of the

3
00:00:09,870 --> 00:00:13,050
configurations from graph theoretical viewpoint

4
00:00:13,070 --> 00:00:20,090
that is being prepared by my me my myself by big disadvantages and can sort

5
00:00:20,090 --> 00:00:21,210
of answers

6
00:00:21,230 --> 00:00:23,770
the book

7
00:00:23,930 --> 00:00:26,390
five chapters

8
00:00:26,390 --> 00:00:29,340
actually six

9
00:00:29,340 --> 00:00:33,490
and today we're going to cover introduction

10
00:00:33,600 --> 00:00:35,930
next chapter one graphs

11
00:00:35,940 --> 00:00:37,100
chapter two

12
00:00:37,130 --> 00:00:39,220
groups and symmetry

13
00:00:39,250 --> 00:00:40,250
chapter three

14
00:00:40,290 --> 00:00:42,930
surfaces and maps

15
00:00:42,940 --> 00:00:51,790
chapter four incidence geometries and combinatorial configurations chapter five geometric configurations

16
00:00:54,150 --> 00:00:56,430
we designed two courses

17
00:00:56,470 --> 00:00:59,400
two one semester courses out of this

18
00:00:59,440 --> 00:01:01,120
the first course

19
00:01:01,970 --> 00:01:03,280
let's say

20
00:01:03,340 --> 00:01:08,530
algebraic and topological graph theory or introduction to algebraic and topological graph theory that it

21
00:01:08,530 --> 00:01:10,980
covers the first three chapters of the book

22
00:01:11,010 --> 00:01:16,260
so introduction graphs and symmetry surface maps and this is what we are in

23
00:01:16,530 --> 00:01:18,280
and then the next semester

24
00:01:18,290 --> 00:01:20,360
if everything goes OK

25
00:01:20,370 --> 00:01:22,250
the river the second course

26
00:01:24,310 --> 00:01:26,200
and geometric configurations

27
00:01:26,210 --> 00:01:27,570
composed of these two

28
00:01:29,340 --> 00:01:34,150
chapters but we're going to talk about that

29
00:01:34,170 --> 00:01:35,840
next semester so

30
00:01:35,900 --> 00:01:41,730
let's focus on the first course algebraic and topological graph

31
00:01:41,760 --> 00:01:44,840
as you see

32
00:01:50,420 --> 00:01:52,710
delivering this course in english

33
00:01:52,920 --> 00:01:54,650
and there is more

34
00:01:56,170 --> 00:01:57,250
on our

35
00:01:57,530 --> 00:02:00,120
he which in

36
00:02:00,170 --> 00:02:02,760
the lecture on

37
00:02:02,810 --> 00:02:05,450
and here is the

38
00:02:07,050 --> 00:02:10,380
how to get to this

39
00:02:12,090 --> 00:02:14,400
you have to register their

40
00:02:14,490 --> 00:02:17,210
but more information how to do that

41
00:02:17,210 --> 00:02:22,680
you will be given by alan or manage is the system for this cause

42
00:02:22,680 --> 00:02:26,060
and if you have any questions please ask him and he

43
00:02:26,090 --> 00:02:26,830
give you

44
00:02:26,840 --> 00:02:28,310
information about the

45
00:02:28,370 --> 00:02:30,250
i would like you to

46
00:02:33,620 --> 00:02:35,930
for this course as soon as possible

47
00:02:35,940 --> 00:02:37,400
if possible today

48
00:02:39,720 --> 00:02:42,410
definitely within a week because

49
00:02:43,180 --> 00:02:45,440
much much more difficult to

50
00:02:45,960 --> 00:02:50,580
to follow the course if you're not registered for the course and if you do

51
00:02:50,580 --> 00:02:52,680
not have access to this

52
00:02:52,690 --> 00:02:56,220
web page

53
00:02:56,370 --> 00:02:58,660
the web page will be protected

54
00:02:58,680 --> 00:03:00,160
this means that only you

55
00:03:00,180 --> 00:03:02,470
those of you who are registered

56
00:03:02,520 --> 00:03:04,120
will be able to

57
00:03:04,150 --> 00:03:06,490
view the contents of this page

58
00:03:06,500 --> 00:03:08,530
but that doesn't mean that

59
00:03:08,560 --> 00:03:10,590
you must be present here

60
00:03:10,630 --> 00:03:15,180
maybe some people will register from the united states or from some other

61
00:03:15,190 --> 00:03:19,780
countries and they may register but we know exactly who they are

62
00:03:19,810 --> 00:03:24,810
and we welcome everybody who could register for this course

63
00:03:24,810 --> 00:03:30,960
so as i said the first

64
00:03:32,340 --> 00:03:33,900
we're going to do

65
00:03:33,900 --> 00:03:38,220
very broad introduction to algebraic and topological graph theory

66
00:03:38,280 --> 00:03:40,620
and it is possible

67
00:03:41,590 --> 00:03:43,910
it's pretty much torture he's going to

68
00:03:45,440 --> 00:03:47,930
basically the

69
00:03:47,970 --> 00:03:49,880
the the lectures

70
00:03:51,940 --> 00:03:54,210
what is new

71
00:03:54,240 --> 00:03:57,560
to this type of of course is

72
00:03:58,990 --> 00:04:03,600
will have as i say exercises and homework and alan alan or manage is going

73
00:04:03,600 --> 00:04:05,180
to take care of that

74
00:04:05,190 --> 00:04:06,870
everybody is going to do

75
00:04:06,870 --> 00:04:09,530
to have to solve some problems

76
00:04:09,530 --> 00:04:10,840
that will be given

77
00:04:11,990 --> 00:04:15,280
and during the election period and they will appear

78
00:04:15,290 --> 00:04:16,940
on the web

79
00:04:16,960 --> 00:04:19,090
and you have about

80
00:04:19,110 --> 00:04:20,330
one week

81
00:04:20,340 --> 00:04:21,520
to solve them

82
00:04:21,580 --> 00:04:23,780
and then you will

83
00:04:23,780 --> 00:04:25,820
upload your solutions

84
00:04:25,830 --> 00:04:27,570
to the web page

85
00:04:27,590 --> 00:04:31,800
maybe the first we can be a little tricky but alan is here to help

86
00:04:31,800 --> 00:04:33,240
you how to do that

87
00:04:33,270 --> 00:04:35,290
but you have to have access to the

88
00:04:35,300 --> 00:04:40,140
to the computer to the internet in order to do so

89
00:04:40,150 --> 00:04:44,360
the other feature is that we are trying to get as many

90
00:04:44,380 --> 00:04:46,650
guest lecturers as possible

91
00:04:46,670 --> 00:04:48,340
so we try to get

92
00:04:50,380 --> 00:04:53,380
and they will give lectures

93
00:04:53,390 --> 00:04:55,850
i know one lectures already taped

94
00:04:55,860 --> 00:04:59,640
so the lecture is not going to be performed here

95
00:04:59,650 --> 00:05:02,230
but we just want lecture

96
00:05:02,340 --> 00:05:04,110
on the screen

97
00:05:04,160 --> 00:05:07,470
and then comment on it

98
00:05:09,330 --> 00:05:12,230
the last features that we are going to use

99
00:05:12,270 --> 00:05:13,480
very much these

100
00:05:13,650 --> 00:05:14,900
lecture room

101
00:05:15,280 --> 00:05:19,910
so that would be the first advanced use of e lecture this means that this

102
00:05:19,910 --> 00:05:21,400
that tape

103
00:05:21,420 --> 00:05:24,590
and as soon as possible

104
00:05:25,590 --> 00:05:26,580
the tapes

105
00:05:26,590 --> 00:05:28,010
will be available

106
00:05:28,030 --> 00:05:28,710
in there

107
00:05:29,690 --> 00:05:32,650
so in case you catch a cold or something like that

108
00:05:32,660 --> 00:05:35,090
you may watch later lecture

109
00:05:36,580 --> 00:05:39,540
on the internet

110
00:05:39,550 --> 00:05:42,740
any questions

111
00:05:46,540 --> 00:05:49,020
what the objective of this course

112
00:05:49,030 --> 00:05:53,730
as i said earlier it's bread introduction to the subject

113
00:05:53,780 --> 00:05:57,070
and we try to get some selected deep

114
00:05:57,070 --> 00:05:59,090
how likely you are

115
00:05:59,100 --> 00:06:01,900
so you draw new candidate text

116
00:06:01,910 --> 00:06:05,260
from you put it through the dynamics

117
00:06:05,290 --> 00:06:09,370
for example if the dynamics is just not regressive process then what you do is

118
00:06:09,370 --> 00:06:11,240
you just say that xt

119
00:06:11,250 --> 00:06:14,900
until that is sixty minus one plus some noise

120
00:06:14,960 --> 00:06:17,000
in that case the text is still there

121
00:06:17,010 --> 00:06:21,230
you have an expression for the model the likelihood is evaluated

122
00:06:21,250 --> 00:06:26,060
normalized importance weights because we don't know the normalisation constants we had to do this

123
00:06:27,030 --> 00:06:30,300
and then we do this black box resampling routines

124
00:06:30,310 --> 00:06:33,750
we looked at it yesterday

125
00:06:38,090 --> 00:06:46,620
i think

126
00:06:46,640 --> 00:06:48,250
so let's look at an example

127
00:06:48,290 --> 00:06:50,040
actually of this

128
00:06:50,240 --> 00:06:52,810
so this is a very simple model

129
00:06:52,930 --> 00:06:55,140
kind of illustrates this

130
00:06:55,160 --> 00:07:01,470
basically have the dynamical system with the new state depends on the previous state

131
00:07:01,530 --> 00:07:06,500
for some linear transformations the some nonstationarity embedded in it

132
00:07:06,520 --> 00:07:09,570
and then there are some noise process which is scouts

133
00:07:11,320 --> 00:07:13,500
the observations are generated

134
00:07:13,530 --> 00:07:17,770
depending on the state and also according to some noise gas

135
00:07:17,790 --> 00:07:22,160
now first you would think of as discussed in the last but it's not because

136
00:07:22,160 --> 00:07:24,950
the posterior because of this

137
00:07:25,000 --> 00:07:27,040
square would be by month

138
00:07:27,090 --> 00:07:31,290
you try to convert x from y and y for all x squared up two

139
00:07:31,290 --> 00:07:34,410
possible answers

140
00:07:34,980 --> 00:07:43,220
what with the code look for the this

141
00:08:14,640 --> 00:08:31,070
we just get the right of

142
00:08:31,080 --> 00:08:32,070
all right so

143
00:08:32,070 --> 00:08:34,310
that's what the code is going to look for

144
00:08:34,310 --> 00:08:38,490
particle filter to to solve the problem just some parameters

145
00:08:38,520 --> 00:08:44,080
i'm just going to generate the data synthetically and this is the particle falls

146
00:08:46,120 --> 00:08:50,060
OK first i'm going to generate some particles

147
00:08:50,070 --> 00:08:55,950
i'm going to use two names x particle spread is the extent the particle production

148
00:08:55,960 --> 00:08:58,910
x part english x

149
00:08:58,930 --> 00:09:02,770
and initially draw them from lagos in distribution

150
00:09:02,810 --> 00:09:06,110
which has

151
00:09:06,140 --> 00:09:08,430
covariance sigma chi square

152
00:09:08,480 --> 00:09:10,710
and now wants

153
00:09:10,720 --> 00:09:13,790
capital t the number of time samples

154
00:09:13,810 --> 00:09:15,020
and so on

155
00:09:15,060 --> 00:09:19,370
some just initializing these guys matlab

156
00:09:19,390 --> 00:09:20,810
and then the algorithm

157
00:09:20,840 --> 00:09:23,300
recursively goes like this

158
00:09:23,310 --> 00:09:29,650
i can get rid of the different french they fought

159
00:09:29,730 --> 00:09:31,730
so first we need to do prediction

160
00:09:31,750 --> 00:09:34,430
that's that's w zero from the transition prior

161
00:09:34,490 --> 00:09:37,110
and in this case i would transition prior was just

162
00:09:37,170 --> 00:09:41,100
if you recall was half the previous particle that nonlinearity

163
00:09:41,650 --> 00:09:45,090
but the venture for the terms that go went that way

164
00:09:45,100 --> 00:09:48,020
and that these are the new particles at time t

165
00:09:48,030 --> 00:09:50,620
so you start with the when the time t minus one

166
00:09:50,680 --> 00:09:57,640
article lifetimes minus one and that gives dx still the antiparticle particle and that's it

167
00:09:57,650 --> 00:10:00,660
and the noise that you have to it is just gas in noise which was

168
00:10:00,660 --> 00:10:02,490
the noise of the model

169
00:10:02,490 --> 00:10:05,160
once you do that

170
00:10:05,240 --> 00:10:06,860
we moved down to

171
00:10:06,880 --> 00:10:13,330
evaluate importance weights

172
00:10:13,370 --> 00:10:16,980
so your computer why predictions according to the particle

173
00:10:17,020 --> 00:10:20,380
and he basically just compute the likelihood which is that counts

174
00:10:20,410 --> 00:10:24,950
e to the minus zero point five with covariance and this is why

175
00:10:26,720 --> 00:10:30,510
x squared over twenty

176
00:10:32,310 --> 00:10:36,400
and you do that for each article you compute the likelihood for each particle and

177
00:10:36,400 --> 00:10:38,520
the new normalise the weights

178
00:10:38,520 --> 00:10:40,750
and that's it and then

179
00:10:40,790 --> 00:10:45,270
actually we only need one line of code just was trying to make sense here

180
00:10:45,290 --> 00:10:46,110
so that idea

181
00:10:46,130 --> 00:10:50,970
different people with use different kind of sampling resampling schemes and you can play with

182
00:10:50,970 --> 00:10:56,590
them but basically what it does just calls wonder sampling schemes they receive sampling

183
00:10:56,640 --> 00:11:00,060
you pass in in this is outcome the new industries

184
00:11:00,220 --> 00:11:03,890
that's a black box routine you never need to change the is of course

185
00:11:03,950 --> 00:11:06,760
all you need to change over to implement this thing

186
00:11:06,810 --> 00:11:10,070
using matlab or unicode byte order

187
00:11:10,110 --> 00:11:13,200
is you need to change the expression for the weights and you need to change

188
00:11:13,230 --> 00:11:16,620
the expression for the model

189
00:11:16,720 --> 00:11:20,300
it's a very simple algorithm and actually know i think that's the main reason for

190
00:11:27,280 --> 00:11:29,680
actually in the interest of time

191
00:11:31,950 --> 00:11:36,180
i actually slide

192
00:11:36,200 --> 00:11:38,720
so does exactly that

193
00:11:39,450 --> 00:11:44,930
here is the same algorithm but not in life generate and samples from the prior

194
00:11:44,950 --> 00:11:48,550
but each of the samples you look at the transition model

195
00:11:48,650 --> 00:11:55,530
evaluate importance weights normalized weights and result

196
00:11:55,550 --> 00:11:58,880
and if you do that then you get this

197
00:11:59,080 --> 00:12:02,630
you look at the distribution that evolves over time

198
00:12:02,650 --> 00:12:05,700
and yes you are able to capture the multimodality

199
00:12:05,740 --> 00:12:10,740
if you run a suboptimal filter like an extended focus on here what happens

200
00:12:10,750 --> 00:12:14,070
the fourth chooses one of the most states with one of the most and then

201
00:12:14,070 --> 00:12:15,500
it diverges

202
00:12:15,520 --> 00:12:18,540
because when one of the most disappears

203
00:12:18,560 --> 00:12:21,260
like if you choose this route

204
00:12:21,310 --> 00:12:23,310
and this guy kind of disappears

205
00:12:23,360 --> 00:12:26,660
then this guy is caught

206
00:12:26,680 --> 00:12:37,070
you can get lost

207
00:12:37,710 --> 00:12:45,510
i'm going to try to so that's sort of basic algorithm and

208
00:12:45,520 --> 00:12:48,860
with that knowledge you can go into one of the particle filter for a lot

209
00:12:48,860 --> 00:12:50,100
of these problems

210
00:12:50,160 --> 00:12:52,310
now when try to like this

211
00:12:52,320 --> 00:12:55,500
presented some sort of more advanced ideas

212
00:12:55,530 --> 00:13:01,210
at this stage i hope you have first i would like to know

213
00:13:01,210 --> 00:13:04,660
who would be able to now go home and implement the particle filter for those

214
00:13:04,660 --> 00:13:06,210
equations that showed

215
00:13:06,260 --> 00:13:09,330
on the board

216
00:13:12,140 --> 00:13:15,010
in my copy

217
00:13:16,420 --> 00:13:17,880
if you feel they can't

218
00:13:19,280 --> 00:13:21,610
you know what to find ways or

219
00:13:21,730 --> 00:13:23,540
and the evening in malaysia

220
00:13:23,590 --> 00:13:27,610
i be very happy to help you could one by one

221
00:13:32,790 --> 00:13:37,470
now more generally i'm going to discuss some more advanced ideas and i'm going to

222
00:13:37,470 --> 00:13:39,740
increase the pace of election

223
00:13:40,000 --> 00:13:43,140
because some people here to know this stuff before

224
00:13:43,160 --> 00:13:45,370
and more

225
00:13:45,410 --> 00:13:47,910
my intention i will be more to give you a taste of the things you

226
00:13:47,910 --> 00:13:49,490
two graph kernels

227
00:13:54,640 --> 00:13:57,890
and they are based on the idea to link graph comes

228
00:13:57,910 --> 00:13:59,510
with other fields

229
00:13:59,550 --> 00:14:01,370
of machine learning

230
00:14:01,490 --> 00:14:02,700
the first one

231
00:14:02,700 --> 00:14:04,820
by by

232
00:14:04,840 --> 00:14:08,510
combines graph comes with graphical models it's

233
00:14:08,510 --> 00:14:09,930
quite similar

234
00:14:09,930 --> 00:14:10,780
two days

235
00:14:10,800 --> 00:14:13,570
colors using saab three like patterns

236
00:14:13,570 --> 00:14:19,610
and those using box but they use especially this specific factorized form for the local

237
00:14:19,610 --> 00:14:22,130
economy between subtrees

238
00:14:23,370 --> 00:14:25,320
user factorizations similar to

239
00:14:25,320 --> 00:14:29,110
those used in graphical models to compute across com

240
00:14:29,160 --> 00:14:29,840
and that's

241
00:14:29,860 --> 00:14:31,700
definitely an interesting area

242
00:14:32,140 --> 00:14:35,780
this combination of graph kernels and graphical models

243
00:14:36,360 --> 00:14:39,820
and i i'm sure see further work in that area

244
00:14:39,820 --> 00:14:44,320
and this is the second idea is to combine graph comes with group theory

245
00:14:45,090 --> 00:14:48,590
you do have a new set of feature vectors for graphs

246
00:14:48,950 --> 00:14:53,550
by representing a graph is the function over the symmetric group over the group of

247
00:14:53,550 --> 00:14:56,910
permutations of the nodes in the graph

248
00:14:56,930 --> 00:15:00,720
and then you derive invariants followed function

249
00:15:00,720 --> 00:15:03,390
called discuss spectrum

250
00:15:04,950 --> 00:15:06,890
this comes spectrum of features

251
00:15:06,890 --> 00:15:08,130
it includes

252
00:15:08,140 --> 00:15:09,240
as it of features

253
00:15:09,260 --> 00:15:13,240
there are computable in cubic runtime use this these

254
00:15:13,240 --> 00:15:14,840
it's invariant

255
00:15:14,890 --> 00:15:17,360
as features to represent your graphs

256
00:15:17,360 --> 00:15:19,680
as the feature vector so that somehow it

257
00:15:19,720 --> 00:15:22,510
a graph a crude theory

258
00:15:22,510 --> 00:15:24,950
inspired way of deriving

259
00:15:25,010 --> 00:15:27,160
the feature vector representation

260
00:15:27,160 --> 00:15:28,800
of graphs etc

261
00:15:28,840 --> 00:15:33,990
also an intermediate thing between graph comes in these topological descriptors

262
00:15:34,220 --> 00:15:39,180
but as mentioned twice before

263
00:15:39,200 --> 00:15:41,200
so these other recent trends in

264
00:15:41,220 --> 00:15:45,360
i similar to those night

265
00:15:45,360 --> 00:15:49,160
before concluding i want to give you

266
00:15:49,200 --> 00:15:53,590
three examples of rare people have applied these different

267
00:15:53,680 --> 00:15:55,320
graph kernels one

268
00:15:57,720 --> 00:15:58,660
paper from

269
00:15:58,660 --> 00:16:00,090
two thousand five

270
00:16:01,160 --> 00:16:02,930
rather like bowler

271
00:16:03,510 --> 00:16:05,220
by the end of

272
00:16:05,240 --> 00:16:11,950
uses graph comments compare chemical compounds to each other the final three newcomers

273
00:16:11,970 --> 00:16:12,990
the trains

274
00:16:12,990 --> 00:16:18,610
inspired by similarity measures defined in the game chemoinformatics community

275
00:16:18,840 --> 00:16:22,660
based on the idea of molecular fingerprints

276
00:16:23,780 --> 00:16:25,590
which which in turn

277
00:16:25,610 --> 00:16:29,780
are based on counting labour costs of that's up to the

278
00:16:29,780 --> 00:16:32,090
using a depth first search

279
00:16:32,110 --> 00:16:33,300
from each

280
00:16:33,300 --> 00:16:34,970
no in your given

281
00:16:35,030 --> 00:16:37,410
graphs so basically

282
00:16:37,490 --> 00:16:39,840
these accounts based on graphs

283
00:16:39,840 --> 00:16:44,510
of depth up to based on powers of that

284
00:16:44,530 --> 00:16:49,470
up to deep

285
00:16:50,450 --> 00:16:53,390
contiki for chemoinformatics tada for applications

286
00:16:53,450 --> 00:16:58,820
in chemoinformatics obviously that because they exploit this small size and low average degree of

287
00:16:58,820 --> 00:17:04,060
these molecular graphs if you do a depth first search in graph that has a

288
00:17:04,060 --> 00:17:05,510
really large

289
00:17:05,550 --> 00:17:10,550
average degree then you might run into serious runtime problems but

290
00:17:10,550 --> 00:17:15,260
as to use it on a small chemical compounds here they don't encounter

291
00:17:15,260 --> 00:17:18,130
this problem

292
00:17:18,140 --> 00:17:24,180
another paper that studied chemical compound classification using colors which is very interesting is that

293
00:17:24,200 --> 00:17:27,450
by rail from ICDM two thousand six

294
00:17:29,140 --> 00:17:30,550
the final

295
00:17:30,610 --> 00:17:33,070
a new type of colour well

296
00:17:33,130 --> 00:17:36,220
basins are called graph recommends these subgraphs

297
00:17:36,240 --> 00:17:41,200
the maximum of l edges so that somehow the counterpart to the graph let's i

298
00:17:41,200 --> 00:17:43,780
mentioned before you would call it is the actual it's

299
00:17:43,820 --> 00:17:46,160
so subgraphs that contain up to l

300
00:17:47,860 --> 00:17:49,300
and what they observe

301
00:17:49,370 --> 00:17:50,970
is that these

302
00:17:51,010 --> 00:17:54,070
fragment based comments

303
00:17:54,070 --> 00:17:55,680
and outperforms

304
00:17:55,700 --> 00:17:57,950
in their experimental evaluation

305
00:17:59,510 --> 00:18:04,180
that use frequent subgraphs as features to compare two graphs and

306
00:18:05,470 --> 00:18:07,390
comment sort of talked about here

307
00:18:07,410 --> 00:18:10,450
which boxed compared to to each other

308
00:18:10,490 --> 00:18:13,200
so that's really interesting findings

309
00:18:16,090 --> 00:18:19,070
i think we'll see also more work in this area of

310
00:18:19,090 --> 00:18:22,530
comparing these different

311
00:18:22,590 --> 00:18:24,910
ideas of how to

312
00:18:24,950 --> 00:18:27,340
the final graph cut using different

313
00:18:27,360 --> 00:18:28,240
types of

314
00:18:28,260 --> 00:18:29,570
features such as

315
00:18:29,590 --> 00:18:31,220
fragments graphlets

316
00:18:31,240 --> 00:18:32,570
five inside as

317
00:18:32,610 --> 00:18:34,490
all walks

318
00:18:34,510 --> 00:18:35,470
and they also

319
00:18:35,890 --> 00:18:37,570
talk about

320
00:18:37,700 --> 00:18:42,180
four interesting choices that you have to make in common design for chemical compounds with

321
00:18:42,260 --> 00:18:45,140
which i want to quickly cover here

322
00:18:45,140 --> 00:18:49,890
learning theory you show you consider like the class of all quantum states as a

323
00:18:49,890 --> 00:18:56,250
hypothesis class news quantum information ideas again based on quantum mechanical linearity the show to

324
00:18:56,250 --> 00:19:00,520
upper bound the fat shattering dimension of quantum states you know and this could have

325
00:19:00,520 --> 00:19:02,160
some actual applications in

326
00:19:02,640 --> 00:19:07,810
quantum state demography because this gives you another is characterizing an unknown quantum state given

327
00:19:07,810 --> 00:19:11,680
a lot of independent copies of it because this is if you only care about

328
00:19:11,680 --> 00:19:16,160
predicting the outcomes of most measurements then you can reduce the amount of sample data

329
00:19:16,160 --> 00:19:18,430
that you need from exponential to linear

330
00:19:20,910 --> 00:19:21,930
you know this was just you know

331
00:19:22,430 --> 00:19:28,250
theorem that a few years ago i had to work with the student yelled actors

332
00:19:28,250 --> 00:19:34,240
here end he actually implemented this quantum state you know learning process in matlab and

333
00:19:34,240 --> 00:19:38,930
we tried it out on various simulated examples and you know the result of these

334
00:19:39,100 --> 00:19:43,160
numerical experiments is my theorem appears to be true so

335
00:19:43,910 --> 00:19:49,290
they you know indeed the sample complexity only grows linearly with the e the not with this they

336
00:19:49,700 --> 00:19:53,350
number of qubits and very the constants are perfectly reasonable as well

337
00:19:53,980 --> 00:19:58,970
okay one last dimension is the no cloning theorem this is the principle that says

338
00:19:58,970 --> 00:20:03,810
that there is no physical procedure to copy an unknown quantum state and the reason

339
00:20:03,810 --> 00:20:08,020
for that is simply that if you write out algebraically what cloning of a quantum

340
00:20:08,020 --> 00:20:13,080
state would mean you find that acts quadratically in the amplitude go but that's not

341
00:20:13,080 --> 00:20:15,890
allowed unitary transformations have to be linear

342
00:20:16,430 --> 00:20:17,680
okay end on

343
00:20:18,620 --> 00:20:24,540
so this makes quantum information hugely different from classical information you know you've heard that information wants to be free

344
00:20:25,060 --> 00:20:27,850
but quantum information wants to be private and so that's okay

345
00:20:28,470 --> 00:20:33,290
you know it's closely related to the uncertainty principle says you know you can simultaneously

346
00:20:33,290 --> 00:20:38,120
measure about the position and the momentum of a particle to unlimited precision you know

347
00:20:38,120 --> 00:20:43,910
that to imply eachother actually you know there's been some amazing applications over no-cloning theorem

348
00:20:44,310 --> 00:20:50,040
to quantum information a we one is quantum key distribution so alice and bob can

349
00:20:50,040 --> 00:20:56,310
do cryptography by sending photons you know sending qubits down a fiber-optic cable and if

350
00:20:56,410 --> 00:21:01,870
eavesdroppers if you know tries to measure the qubits housing bubble actually be able to

351
00:21:01,870 --> 00:21:07,160
detect thattheir qubits had been tampered with and therefore aboard the protocol this is already

352
00:21:07,160 --> 00:21:09,330
practical you can buy devices now

353
00:21:09,750 --> 00:21:13,600
that will implement quantum key distribution and how much market but you know

354
00:21:14,600 --> 00:21:19,290
so so you know more speculative thing is quantum money where you would have some

355
00:21:19,290 --> 00:21:22,290
qubits attached to each dollar bill let's say

356
00:21:23,040 --> 00:21:25,060
you know such that you know they are

357
00:21:25,600 --> 00:21:31,540
the bank the printed be bills could authenticate state as genuine by measuring the qubits

358
00:21:31,770 --> 00:21:36,270
okay by someone who didn't know how the qubits were prepared would be physically unable

359
00:21:36,270 --> 00:21:37,140
to copy a bill

360
00:21:37,600 --> 00:21:41,810
okay because you know again because the no cloning theorem very recently with paul cristiano

361
00:21:41,850 --> 00:21:46,580
we show that under some cryptographic assumptions you can even get quantum money that can

362
00:21:46,580 --> 00:21:48,930
be verified by anyone not only the bank

363
00:21:49,410 --> 00:21:53,420
okay and then one another thing that cristiano and i are now working on its

364
00:21:53,420 --> 00:22:00,330
quantum copy protected software quantum theorem so you would get some states sorry sorry so

365
00:22:00,330 --> 00:22:04,330
state about the you know but you know is not good are not gonna be

366
00:22:04,330 --> 00:22:05,850
practical for a long time

367
00:22:07,930 --> 00:22:13,250
stateside about what to evaluate the function f on inputs x if your choice but

368
00:22:13,250 --> 00:22:17,580
that you wouldn't be able to efficiently use to learn the function f or indeed

369
00:22:17,580 --> 00:22:22,000
to prepare any more states with which have to be evaluated and we actually think

370
00:22:22,000 --> 00:22:23,410
that this can be done in general now

371
00:22:23,980 --> 00:22:30,270
okay so i only asking my remaining few minutes is quantum mechanics relevant to biology

372
00:22:30,700 --> 00:22:34,560
okay well at small enough scale you know we now know the answer is certainly

373
00:22:34,560 --> 00:22:37,930
yes right that should be such a surprise because you had a small enough scale

374
00:22:38,040 --> 00:22:39,450
everything is quantum mechanical

375
00:22:39,830 --> 00:22:43,750
okay but there's been some really you know call recent papers saying that for example

376
00:22:44,040 --> 00:22:48,770
green plant photosynthesis right the reason why it's able to be as efficient as it

377
00:22:48,770 --> 00:22:54,020
is at harvesting photons you know fundamentally has to do with a coherent quantum optical

378
00:22:54,020 --> 00:22:58,390
effect and people are now working on reverse engineering that's so that you know maybe

379
00:22:58,390 --> 00:23:01,810
they could eventually designs solar cells that would have similar efficiencies

380
00:23:03,330 --> 00:23:09,660
the european robins and various other birds have this amazing you know ability to navigate

381
00:23:09,660 --> 00:23:13,850
by internal compass is okay and it's you know we over the last decade has

382
00:23:13,850 --> 00:23:18,640
been discovered that you know the way that the internal compasses work is by measuring

383
00:23:18,640 --> 00:23:21,680
now the problem seems to be

384
00:23:21,700 --> 00:23:22,890
as i say we've got us

385
00:23:22,910 --> 00:23:27,060
we've got this set k of formulas it's closer the action and the original idea

386
00:23:27,060 --> 00:23:29,120
was you trying take away

387
00:23:29,140 --> 00:23:30,660
so it's possible he

388
00:23:30,660 --> 00:23:32,950
so that we just eliminated five

389
00:23:32,950 --> 00:23:36,220
and we said there are many possibilities here we're going to choose one

390
00:23:36,240 --> 00:23:39,970
but if we just choose one we end up with this problem

391
00:23:39,990 --> 00:23:43,600
you know suddenly become opinion

392
00:23:43,660 --> 00:23:47,240
OK so the giving up as little as possible idea seems to be a bad

393
00:23:47,950 --> 00:23:50,660
leads to opinion and revision

394
00:23:50,700 --> 00:23:55,260
so we're going to try and do something else

395
00:23:55,310 --> 00:23:58,410
and something else is let's go to the opposite end of the spectrum

396
00:23:58,490 --> 00:24:02,080
instead of just taking one of these possible sets we're going to look at all

397
00:24:03,040 --> 00:24:06,600
and we'll take what's coming toward them

398
00:24:06,620 --> 00:24:09,910
and that's called the formerly contraction where you just take watkins

399
00:24:09,910 --> 00:24:13,270
what's in all the possible maximal

400
00:24:13,270 --> 00:24:15,620
not implying subset

401
00:24:15,700 --> 00:24:19,270
now it turns out if you do the same thing in the case of formerly

402
00:24:20,470 --> 00:24:24,080
then you end up with the following also equally bad property

403
00:24:24,100 --> 00:24:28,080
after you do this all the belief that you have in case stuff i just

404
00:24:28,080 --> 00:24:30,200
the consequences of five so

405
00:24:30,240 --> 00:24:33,080
so if you did have beliefs about other things that had nothing to do with

406
00:24:33,660 --> 00:24:37,020
you've lost all of those beliefs now and the only thing you have an opinion

407
00:24:37,020 --> 00:24:39,790
on is just stuff about five

408
00:24:39,830 --> 00:24:42,310
necessarily lost too much

409
00:24:42,430 --> 00:24:45,830
OK that's probably not so surprising you know the fact that if you look take

410
00:24:45,830 --> 00:24:47,620
what's coming toward them again

411
00:24:47,660 --> 00:24:51,200
throw away too much

412
00:24:51,240 --> 00:24:53,890
OK so the compromise just to take a few of days and you have to

413
00:24:53,890 --> 00:24:59,290
select the mouse and again i delayed this choice by just assuming this abstract selection

414
00:24:59,290 --> 00:25:02,080
function take account can take a couple of the

415
00:25:02,120 --> 00:25:06,720
max implying subset somewhere between one and the whole

416
00:25:06,740 --> 00:25:08,850
and if i do that then i get exact

417
00:25:08,910 --> 00:25:11,390
a function that exactly characterizes

418
00:25:11,390 --> 00:25:15,910
those properties now it's fairly obvious that i could have many

419
00:25:15,950 --> 00:25:20,100
such functions because you got me choices here in each different choice will give me

420
00:25:20,100 --> 00:25:21,720
a different one of these functions

421
00:25:21,770 --> 00:25:25,080
so there is no unique contraction function like there was an expansion there are many

422
00:25:29,620 --> 00:25:33,560
so if you want to go into the detail of like trying to provide more

423
00:25:33,620 --> 00:25:37,770
you know trying to make more concrete what this selection function might look like in

424
00:25:37,770 --> 00:25:39,020
properties hayes

425
00:25:39,100 --> 00:25:42,540
then you can start to get a reason for getting this supplementary possible

426
00:25:42,560 --> 00:25:44,620
again i'll just leave it for

427
00:25:44,660 --> 00:25:48,290
the benefit

428
00:25:48,350 --> 00:25:52,270
OK let's just take a little bit of a look at the recovery property because

429
00:25:52,270 --> 00:25:56,330
as i said before it's ten tends to be the contentious one

430
00:25:56,370 --> 00:25:59,200
so the following example comes from

431
00:25:59,200 --> 00:26:01,540
and since this is the following so as i said

432
00:26:01,600 --> 00:26:04,540
from k minus one k minus four you get the

433
00:26:06,100 --> 00:26:09,930
the opposite inclusion to the k minus five properties so when we put all that

434
00:26:11,720 --> 00:26:15,620
we end up with this property which is called the recovery properties

435
00:26:15,700 --> 00:26:18,370
and it says that if i is in k

436
00:26:18,430 --> 00:26:22,310
then if you want to find that back you get what you started again as

437
00:26:22,310 --> 00:26:25,950
i said what it's trying to capture the side dish this idea sorry of minimal

438
00:26:25,950 --> 00:26:28,540
change giving up as little as possible

439
00:26:28,540 --> 00:26:30,430
so this was a counterexample

440
00:26:30,450 --> 00:26:32,180
i offered by hanson

441
00:26:32,240 --> 00:26:36,180
so suppose the proposition m stands for george's murder

442
00:26:36,240 --> 00:26:37,450
the that

443
00:26:37,520 --> 00:26:39,450
you know george's law right

444
00:26:39,490 --> 00:26:42,890
and g is the tax evasion so we see that if

445
00:26:42,910 --> 00:26:48,290
you know george's murder the george lawbreaker in georgia tech savannah more georgia the

446
00:26:49,810 --> 00:26:52,500
suppose initially that we believe that

447
00:26:52,560 --> 00:26:54,770
george is both the murderer

448
00:26:54,770 --> 00:26:56,290
and a

449
00:26:56,310 --> 00:26:58,270
a little break

450
00:26:58,290 --> 00:27:02,350
now suddenly someone tells me all the current to show the so you want to

451
00:27:02,350 --> 00:27:06,310
give up the fact that you know george's so i suppose you want to give

452
00:27:06,310 --> 00:27:09,950
up the fact that george is a little break

453
00:27:09,970 --> 00:27:13,260
and as i say you're forced to give up the fact that george's murderer because

454
00:27:13,260 --> 00:27:17,240
george bingham murderer means that he is also lawbreaker so you got give us the

455
00:27:17,240 --> 00:27:21,020
fact that some

456
00:27:21,040 --> 00:27:26,350
now for the more you know by the partial as we know that you know

457
00:27:26,350 --> 00:27:27,830
this contraction of

458
00:27:27,890 --> 00:27:32,180
OK i want to give up the fact that george's

459
00:27:32,560 --> 00:27:34,140
a lawbreaker

460
00:27:34,180 --> 00:27:36,040
that's more than k

461
00:27:36,100 --> 00:27:38,430
so we also know that

462
00:27:38,430 --> 00:27:39,310
you know that

463
00:27:39,350 --> 00:27:42,020
george stopping attacks break is not

464
00:27:42,020 --> 00:27:47,370
tax evaders nineteen years well

465
00:27:47,430 --> 00:27:50,060
just by the fact that it wasn't mentioned in

466
00:27:50,060 --> 00:27:51,270
initially so we

467
00:27:51,290 --> 00:27:54,200
you also we can not

468
00:27:54,220 --> 00:27:58,600
express that takes

469
00:27:58,680 --> 00:28:02,600
OK now let's suppose what happens when we start adding the back you

470
00:28:03,760 --> 00:28:05,660
if i try and add

471
00:28:06,180 --> 00:28:10,500
so let's look at the situation where we want to

472
00:28:11,720 --> 00:28:14,330
into this set

473
00:28:15,080 --> 00:28:18,500
it consists consistent for me and t this set because it so we can conclude

474
00:28:18,500 --> 00:28:20,350
that not to in this because it

475
00:28:20,450 --> 00:28:25,600
wasn't mentioned in the first set and when i contract something it's more than what

476
00:28:25,600 --> 00:28:26,760
i started

477
00:28:26,770 --> 00:28:30,450
so it's only consistent for me to add to this case

478
00:28:30,470 --> 00:28:31,870
so far and

479
00:28:31,870 --> 00:28:37,870
the fact that joy is now tax evader into my beliefs after having removed that

480
00:28:37,910 --> 00:28:40,490
the fact that he was a lawbreaker

481
00:28:40,540 --> 00:28:42,200
well that

482
00:28:42,240 --> 00:28:48,390
tax evasion implies is lawbreaker solo break it has to be added back to

483
00:28:48,450 --> 00:28:52,580
so this is certainly going to be a set of just adding

484
00:28:52,620 --> 00:28:53,540
the fact that

485
00:28:53,560 --> 00:28:54,490
george was

486
00:28:54,500 --> 00:28:57,310
it was a little break

487
00:28:57,350 --> 00:29:01,870
but from the love you know the recovery property for and back the fact that

488
00:29:02,160 --> 00:29:03,160
george was

489
00:29:03,270 --> 00:29:04,790
a little break

490
00:29:04,850 --> 00:29:08,160
that means i get back what i started with one i started what we did

491
00:29:08,160 --> 00:29:11,700
was that george was murdered so that means i'm going to have to and also

492
00:29:11,700 --> 00:29:13,790
the fact that he was murdered

493
00:29:13,830 --> 00:29:15,270
so suddenly when i j

494
00:29:15,310 --> 00:29:18,540
start adding back the law that you know changed my mind i'm giving up his

495
00:29:19,580 --> 00:29:22,930
then i added that he was the tax evaders somebody forces me not only to

496
00:29:22,930 --> 00:29:27,540
add that george was lawbreaker but also means i have to add that he was

497
00:29:27,580 --> 00:29:31,310
motor recovery properties forcing them back

498
00:29:31,410 --> 00:29:34,240
so i mean that with the fact that if i now want to and expand

499
00:29:34,240 --> 00:29:38,830
this contract and stayed with the fact that george is a tax evader got also

500
00:29:38,830 --> 00:29:42,910
believe it but the tax evaders murdering

501
00:29:42,970 --> 00:29:47,680
and that seems a little bit counterintuitive so perhaps recovery properties of little bit strong

502
00:29:47,770 --> 00:29:50,830
so you see a lot of people sort of argue for the forms of belief

503
00:29:50,830 --> 00:29:53,290
revision don't include this recovery properties

504
00:29:53,390 --> 00:29:58,200
other properties tend to be fairly sort of accepted widely accepted but this is perhaps

505
00:29:58,200 --> 00:30:04,060
the property that's mostly the most contentious

506
00:30:04,120 --> 00:30:05,870
OK now

507
00:30:05,890 --> 00:30:08,430
let's look at all the functions

508
00:30:08,490 --> 00:30:13,100
then that satisfy the AGM postulates k minus one

509
00:30:13,100 --> 00:30:16,540
to k minus six so the basic policies that we have not came under and

510
00:30:16,540 --> 00:30:18,410
came on

511
00:30:18,470 --> 00:30:20,450
but don't satisfy

512
00:30:20,450 --> 00:30:26,080
this recovery properties given that we think the recovery is a contentious properties

513
00:30:26,120 --> 00:30:29,520
so these functions that satisfy k minus one to k minus four

514
00:30:29,540 --> 00:30:33,500
then came under six we usually turn withdrawal functions

515
00:30:33,500 --> 00:30:39,950
so i can already said this so we need a similarity function and we will

516
00:30:41,000 --> 00:30:44,550
put the value the similarity value between the data points in the matrix which is

517
00:30:44,550 --> 00:30:48,770
called capital s so that similarity matrix

518
00:30:49,250 --> 00:30:52,740
then we need to know what the degree of vertex is

519
00:30:52,760 --> 00:30:56,300
so in a graph the degree of a vertex is just

520
00:30:56,370 --> 00:31:00,420
the some of the traits of the edges which are attached to the vertex which

521
00:31:00,420 --> 00:31:04,810
simply shows you see some similarities of all the data points which are used to

522
00:31:05,730 --> 00:31:08,610
vertex which are attached to the vertex

523
00:31:08,640 --> 00:31:11,220
that's the degree of vertex

524
00:31:11,260 --> 00:31:14,990
and then we will put those into the matrix which require degree matrix it's a

525
00:31:14,990 --> 00:31:20,450
diagonal matrix and we just put the different degrees on the diagonal

526
00:31:21,100 --> 00:31:26,630
OK then also need later is we need to measure

527
00:31:26,650 --> 00:31:28,680
how large different parts of the of

528
00:31:28,800 --> 00:31:33,100
of graph so we say later on for example we would assume this

529
00:31:33,130 --> 00:31:37,380
guy should be closer we need to know how large the clusters

530
00:31:37,450 --> 00:31:41,260
there are two different generally two different ways so you or tool

531
00:31:41,340 --> 00:31:44,130
i don't think about how you would measure the size of the cluster there are

532
00:31:44,130 --> 00:31:48,390
two ways could come up with so i simply can't how many vertices and the

533
00:31:49,730 --> 00:31:53,730
that's what i been not by the absolute value of a

534
00:31:53,740 --> 00:31:58,400
so is this is the absolute value is just five

535
00:31:58,410 --> 00:32:00,100
or we can do

536
00:32:00,130 --> 00:32:03,590
which is called the volume of a instead of counting the vertices we simply can't

537
00:32:03,590 --> 00:32:07,540
the edges so what we do is fall that vertices which are in the subset

538
00:32:08,160 --> 00:32:13,810
we sum up the weights of all edges which are capable to vertices

539
00:32:16,990 --> 00:32:22,800
you know it makes it means well OK

540
00:32:22,830 --> 00:32:26,020
it means that the similarity zero essential

541
00:32:26,030 --> 00:32:29,540
so in the end i will i will tell letter later on how we achieve

542
00:32:29,550 --> 00:32:34,610
that the similarities are so is essentially what plot here is sparse similarity matrix so

543
00:32:34,610 --> 00:32:38,460
you know that certain points for example what you can do is

544
00:32:38,500 --> 00:32:42,180
take it just computer calls comments on your data

545
00:32:42,200 --> 00:32:46,520
but then only connect points in the similarity is higher than zero point nine

546
00:32:46,580 --> 00:32:49,140
and the other one set to zero

547
00:32:49,160 --> 00:32:54,490
so it's so essentially if there's no it's we really assume that there similarities zero

548
00:32:54,600 --> 00:32:56,880
however which is that to

549
00:33:03,060 --> 00:33:07,760
any more questions so far about the notation

550
00:33:07,770 --> 00:33:11,030
OK so you need to you need intuition what this means because if you don't

551
00:33:11,030 --> 00:33:14,150
know it then you will get lost from the next slide

552
00:33:17,590 --> 00:33:19,490
OK one more thing which

553
00:33:19,500 --> 00:33:24,500
you probably can't because it's just on the bottom of the screen

554
00:33:24,510 --> 00:33:28,800
so assume that such a graph and let only one i mean clustering in the

555
00:33:28,800 --> 00:33:32,540
end will be we want to give labels to data points within the cluster one

556
00:33:32,620 --> 00:33:37,370
cluster two cluster five so we want to attach vertices to those data points more

557
00:33:37,390 --> 00:33:38,900
less in each vertex get the

558
00:33:38,920 --> 00:33:40,280
gets the label of its

559
00:33:40,300 --> 00:33:41,730
corresponding cluster

560
00:33:41,750 --> 00:33:47,670
and the way you can express is now is you can just write the vertices

561
00:33:47,670 --> 00:33:48,700
is a vector

562
00:33:48,710 --> 00:33:53,280
so if you meaning that the clustering the

563
00:33:53,290 --> 00:33:55,840
index of the clustering of these data points

564
00:33:55,860 --> 00:33:59,710
and we will often write so that's where in my notation often called because essentially

565
00:33:59,890 --> 00:34:02,980
the function values so if we have a vector

566
00:34:02,990 --> 00:34:08,130
of n data points in this setting we usually mean this vector contains entries which

567
00:34:08,130 --> 00:34:11,910
but show you which cluster the data point t

568
00:34:23,590 --> 00:34:25,800
the standard object

569
00:34:25,860 --> 00:34:29,650
we would always use inspector accessories the graph laplacian

570
00:34:29,710 --> 00:34:32,510
actually i just think

571
00:34:32,580 --> 00:34:37,230
two so the graph facets matrix and

572
00:34:37,240 --> 00:34:38,280
it is the so

573
00:34:38,300 --> 00:34:41,710
given across can define this matrix which is called the graph laplacian

574
00:34:41,910 --> 00:34:47,280
and it is simply the difference between the degree matrix in the similarity matrix

575
00:34:47,340 --> 00:34:51,180
OK just taken as the definition for now we will see why why this makes

576
00:34:51,180 --> 00:34:53,330
sense later on

577
00:34:53,890 --> 00:34:59,020
and the most important property of this matrix is the one which i have on

578
00:34:59,030 --> 00:35:03,220
the slide so everything else is so what we will do is the consequence of

579
00:35:03,980 --> 00:35:06,510
and property that the properties

580
00:35:06,560 --> 00:35:08,880
if you take some vector in rn

581
00:35:08,890 --> 00:35:09,550
and you

582
00:35:09,580 --> 00:35:13,180
multiplied from the left and right to your matrix

583
00:35:13,200 --> 00:35:14,260
what you get

584
00:35:14,400 --> 00:35:16,470
this expression you see down here

585
00:35:17,210 --> 00:35:18,770
it is essentially

586
00:35:18,780 --> 00:35:20,740
the sum over all data points

587
00:35:20,750 --> 00:35:24,560
so all pairs of data points the sum runs over i and j

588
00:35:24,580 --> 00:35:29,650
so for all pairs of data points is some the similarity times the difference of

589
00:35:29,650 --> 00:35:32,230
the function square

590
00:35:32,260 --> 00:35:36,990
it's a

591
00:35:44,230 --> 00:35:45,960
what's the question

592
00:35:49,020 --> 00:35:53,010
you may varied over this this should be here

593
00:35:53,880 --> 00:35:59,720
that's the type i then when so forget about this

594
00:36:00,120 --> 00:36:04,260
so maybe write definitions again that was it so the

595
00:36:04,260 --> 00:36:05,580
this the matrix

596
00:36:05,600 --> 00:36:08,420
which contains the degrees

597
00:36:08,450 --> 00:36:11,340
on the diagonal otherwise zero

598
00:36:11,360 --> 00:36:17,350
this is just the similarity matrix which contains entries of similarity functions

599
00:36:17,580 --> 00:36:21,750
OK i again the degree to which you have from this matrix d i

600
00:36:21,760 --> 00:36:23,000
is some more

601
00:36:23,000 --> 00:36:29,910
our kernels may the main kernel equality so we went product to be such that

602
00:36:30,480 --> 00:36:35,100
the kernel represents taking the dot product in that space

603
00:36:35,140 --> 00:36:39,260
because the at the end we can get away without mapping into that space we

604
00:36:39,260 --> 00:36:44,370
can just formulate everything in terms of dot products and computer products in terms of

605
00:36:45,750 --> 00:36:49,440
so we want to come to represent the dot product in that space and by

606
00:36:49,440 --> 00:36:55,300
definition of ahmed that means the kernel has to have this so called reproducing property

607
00:36:55,490 --> 00:37:00,990
all the dot product together with the kernel has had this problem this property so

608
00:37:00,990 --> 00:37:06,120
we'll have to be constructed dot products such that it satisfies the quality here

609
00:37:06,140 --> 00:37:11,250
and there's only one way to do it and we have to define that the

610
00:37:11,250 --> 00:37:14,490
dot product takes this value here for

611
00:37:14,500 --> 00:37:19,690
two such elements but that space contains also other functions because we're just taking the

612
00:37:19,690 --> 00:37:23,520
linear completion so we also have to have to say what's the value of the

613
00:37:23,520 --> 00:37:27,570
dot product if we take linear combinations of such kind of functions

614
00:37:27,580 --> 00:37:31,370
and since the products have to be by linear functions there's only one way of

615
00:37:31,370 --> 00:37:37,540
doing it we have to do the linear extension of this definition and that's that

616
00:37:37,540 --> 00:37:42,920
this equation here so you just imagine substituting linear combinations back here

617
00:37:42,940 --> 00:37:44,000
we already know

618
00:37:44,020 --> 00:37:49,620
the individual kernels the product gives us something like this and sums in the coefficients

619
00:37:49,620 --> 00:37:53,440
we have to move out to make sure that this becomes by linear

620
00:37:53,450 --> 00:37:58,190
so then this becomes our definition for the dot product

621
00:38:00,770 --> 00:38:04,250
and then we we prove that this is the product in the proof was kind

622
00:38:04,250 --> 00:38:10,690
of nice because it was proceeding by first consul showing that it's a positive definite

623
00:38:10,690 --> 00:38:16,980
kernel and then we use the properties of the positive definite kernels to show that

624
00:38:16,980 --> 00:38:21,800
this is strictly positive definite by linear form

625
00:38:21,810 --> 00:38:25,320
in other words a dot product

626
00:38:25,350 --> 00:38:28,750
so now we have a way of thinking about our feature space of feature space

627
00:38:28,750 --> 00:38:33,510
contains functions that look like the kernel function together with a special kind of the

628
00:38:33,510 --> 00:38:39,430
product and maybe i'm sure i would say a little bit more about this today

629
00:38:39,430 --> 00:38:42,000
during the lecture to give you a bit more of an idea

630
00:38:42,000 --> 00:38:43,310
what's going on

631
00:38:46,830 --> 00:38:48,870
now we've seen we can in no way

632
00:38:48,880 --> 00:38:50,990
yes if we map each point two

633
00:38:53,230 --> 00:38:59,100
so we said the kernel represents similarity measure we now map points to the kernel

634
00:38:59,100 --> 00:39:04,040
since centered on that point so that's all representation of some of the feature space

635
00:39:04,560 --> 00:39:08,000
so we can also think of this as a representation of data which is more

636
00:39:08,000 --> 00:39:14,560
convenient all representation in which we will construct i wouldn't know if this nothing takes

637
00:39:14,560 --> 00:39:19,830
the point that into the kernel function it actually means we represent each data point

638
00:39:20,010 --> 00:39:25,250
by its similarity to all other possible points because the kernel function is not this

639
00:39:25,250 --> 00:39:29,810
thing here is nothing else but a function which given some other point tells us

640
00:39:29,810 --> 00:39:34,940
how similar it is to x so we x is represented by its similarities to

641
00:39:34,940 --> 00:39:37,160
all possible points

642
00:39:37,180 --> 00:39:39,380
and then with a suitable product

643
00:39:39,410 --> 00:39:44,560
so that's one way to think about it and some of also makes it plausible

644
00:39:44,560 --> 00:39:45,910
that one can do

645
00:39:45,930 --> 00:39:50,200
some kind of empirical approximation of this if one wants to

646
00:39:50,740 --> 00:39:56,000
by robert rather than representing each point by similarity to all other points we might

647
00:39:56,000 --> 00:40:00,870
want to represent it's just by its similarity to a sample of points but sometimes

648
00:40:02,460 --> 00:40:03,430
and so

649
00:40:04,250 --> 00:40:09,420
again map point to the kernel function but not this and we restrict the kernel

650
00:40:09,420 --> 00:40:11,810
function to a set of points

651
00:40:11,830 --> 00:40:15,480
evaluation points so that's like

652
00:40:15,490 --> 00:40:19,580
so then the point is mapped o input point is not actually to effect because

653
00:40:19,580 --> 00:40:23,700
the function evaluated at a finite set of points is the vector of this is

654
00:40:23,700 --> 00:40:25,010
all vector here

655
00:40:25,020 --> 00:40:30,620
and we can not think about well if if the original kernel such that

656
00:40:30,640 --> 00:40:34,390
the kernel trick holds true for original mapping phi

657
00:40:34,410 --> 00:40:39,130
so we we can't compute the dot product after taking the mapping phi how about

658
00:40:39,130 --> 00:40:43,440
if we take the mapping phi and then just use the standard m dimensional dot

659
00:40:43,440 --> 00:40:46,880
product we get the same or we get something different

660
00:40:46,900 --> 00:40:49,860
so it turns out we get something slightly different but we can make it the

661
00:40:50,980 --> 00:40:56,060
and we can make the same exactly if we substitute

662
00:40:56,100 --> 00:40:59,830
the kernel matrix to the power of one minus one over two

663
00:40:59,840 --> 00:41:01,160
in here

664
00:41:01,170 --> 00:41:03,550
it's some kind of whitening procedure

665
00:41:03,560 --> 00:41:05,410
and i'm not going to go

666
00:41:05,430 --> 00:41:10,320
please infer the algebra but basically if you if you look at this thing here

667
00:41:10,320 --> 00:41:14,360
and i imagine taking the product of this with itself

668
00:41:14,560 --> 00:41:19,150
then you can imagine you've function twice and the then the dot product matrix of

669
00:41:19,150 --> 00:41:24,490
this thing itself actually ends up being case square

670
00:41:24,510 --> 00:41:27,580
and to make sure that we get k which is what we wanted to we

671
00:41:27,580 --> 00:41:34,010
have to put in this whitening procedure so that sometimes convenient if we want to

672
00:41:34,010 --> 00:41:37,030
kernelize migrants so we have some grim you want to do it in the feature

673
00:41:37,030 --> 00:41:41,690
space but somehow find to complicated you don't manage to write it terms of the

674
00:41:41,710 --> 00:41:46,810
product then you can do something like this which effectively means you are doing things

675
00:41:46,810 --> 00:41:52,280
in accordance you are choosing some basis of a subspace of your feature space that

676
00:41:52,280 --> 00:41:57,230
you're interested in the subspace spanned by the kernel functions centered on those points x

677
00:41:57,230 --> 00:42:00,860
one through x and then you can work on a subspace so if you don't

678
00:42:00,860 --> 00:42:04,480
like infinite dimensional space that something sometimes

679
00:42:04,510 --> 00:42:09,440
so before i move on to a kernel i agree that i just want to

680
00:42:09,440 --> 00:42:13,960
say a few more things about kernels that are nice library for

681
00:42:14,070 --> 00:42:20,060
constructing similarity measures because they have certain closure properties you can multiply kernels by nonnegative

682
00:42:20,060 --> 00:42:24,720
numbers you can and then you can take the product of kernels that

683
00:42:24,720 --> 00:42:27,570
without telling you that that distribution

684
00:42:27,590 --> 00:42:28,740
but just to me

685
00:42:29,020 --> 00:42:31,750
but if i ten years some p

686
00:42:31,750 --> 00:42:34,250
then by optimizing the beta

687
00:42:34,350 --> 00:42:37,720
i can recover q

688
00:42:37,750 --> 00:42:43,510
they can recover distribution where everything came from

689
00:42:43,520 --> 00:42:46,060
and this is not problem

690
00:42:46,070 --> 00:42:47,940
it's convex

691
00:42:47,940 --> 00:42:50,100
it has a unique solution

692
00:42:50,870 --> 00:42:54,650
well as i said the suffixes is q of pixel appear fix

693
00:42:54,660 --> 00:42:57,250
just achieves the minimum value

694
00:42:58,120 --> 00:43:01,740
there's really nicely behaved

695
00:43:01,860 --> 00:43:03,610
now here's the catch

696
00:43:03,640 --> 00:43:05,750
this is all in terms of expectation

697
00:43:05,800 --> 00:43:10,720
and obviously we don't have access the distribution but we just have an empirical average

698
00:43:11,010 --> 00:43:13,440
so what would you do in this case

699
00:43:13,450 --> 00:43:17,030
what you suggest

700
00:43:17,120 --> 00:43:21,250
but as a suggestion

701
00:43:21,250 --> 00:43:22,220
come one

702
00:43:24,480 --> 00:43:28,400
i cannot compute expectation but i have some data

703
00:43:28,450 --> 00:43:31,720
what would i do

704
00:43:31,810 --> 00:43:33,110
i just

705
00:43:33,170 --> 00:43:40,010
take empirical averages over my father and try solving the same problem

706
00:43:40,050 --> 00:43:41,570
so i just

707
00:43:41,620 --> 00:43:43,250
take some average

708
00:43:43,260 --> 00:43:45,640
over the training data reweighted

709
00:43:45,670 --> 00:43:48,640
and the average over the test starter unweighted

710
00:43:48,650 --> 00:43:55,800
now somehow will try to make that distance small

711
00:43:55,850 --> 00:43:57,090
and ensure that

712
00:43:57,100 --> 00:44:00,190
well the ratings prob

713
00:44:01,560 --> 00:44:05,250
that's the problem

714
00:44:05,300 --> 00:44:08,280
exactly what i had in the previous slide

715
00:44:08,330 --> 00:44:10,360
so here i've replaced

716
00:44:11,260 --> 00:44:14,030
average body parker mean

717
00:44:14,050 --> 00:44:15,690
same here

718
00:44:15,840 --> 00:44:19,620
and i only need to estimate m of those coefficients beta i

719
00:44:19,680 --> 00:44:21,750
and for convenience of also

720
00:44:21,800 --> 00:44:24,350
required that those beta is bounded by some

721
00:44:24,350 --> 00:44:28,120
upper bound b

722
00:44:29,060 --> 00:44:34,120
then you just have to be optimized away

723
00:44:34,140 --> 00:44:36,010
so at the moment this looks like a hack

724
00:44:36,870 --> 00:44:40,200
like why should this work at all

725
00:44:40,220 --> 00:44:42,680
as it turns out you can actually prove that this is

726
00:44:42,680 --> 00:44:45,790
the this gives you a consistent estimator

727
00:44:45,810 --> 00:44:49,290
but if what you care about is algorithms or you do now is you can

728
00:44:49,290 --> 00:44:54,120
not fall asleep and just remember that problem you feed into QP solver

729
00:44:54,250 --> 00:44:56,810
and you'll get some weights use them

730
00:44:56,910 --> 00:45:02,290
used for his support vector machine and you'll get very good results

731
00:45:06,510 --> 00:45:10,080
actually what we can show is and i'm going to stay in the theory is

732
00:45:10,080 --> 00:45:14,080
that this reweighted set of observations will behave like one which is drawn from the

733
00:45:14,080 --> 00:45:17,140
correct distribution but you pay a little bit

734
00:45:17,200 --> 00:45:23,810
if a basically by m over the squared norm of those coefficients

735
00:45:23,810 --> 00:45:24,660
so they

736
00:45:24,680 --> 00:45:28,580
effective sample size will be a little bit smaller

737
00:45:29,560 --> 00:45:30,790
not too bad

738
00:45:30,810 --> 00:45:31,890
of course you pay

739
00:45:31,890 --> 00:45:32,870
and then

740
00:45:32,910 --> 00:45:34,830
you have a little bit of bias in it

741
00:45:34,890 --> 00:45:39,080
and that's just like how well you will manage to approximate

742
00:45:39,100 --> 00:45:40,470
this from here

743
00:45:40,580 --> 00:45:43,470
if that term is zero then the box disappears

744
00:45:43,490 --> 00:45:48,140
but that might cost you rather deal in the beat

745
00:46:05,990 --> 00:46:11,140
now you actually well actually want to be close to n because you want your

746
00:46:11,140 --> 00:46:15,290
writing to be such that it corresponds to a proper probability distribution again

747
00:46:15,330 --> 00:46:19,220
actually what you would really like to have is that

748
00:46:19,250 --> 00:46:23,970
in minus the sum is less equal in uppsala

749
00:46:25,020 --> 00:46:28,310
we're going to be a bit sloppy interested in this epsilon to sierra because it

750
00:46:28,310 --> 00:46:31,220
doesn't really matter very much

751
00:46:31,240 --> 00:46:36,040
and all the theory goes through regardless

752
00:46:36,600 --> 00:46:38,200
and the other thing is

753
00:46:38,200 --> 00:46:40,430
imposing this constraint

754
00:46:40,470 --> 00:46:45,770
actually led to make a very nice connection to single class SVM

755
00:46:45,770 --> 00:46:47,430
that's not published yet

756
00:46:47,450 --> 00:46:49,310
so you have to

757
00:46:49,310 --> 00:46:53,200
well stay put until this goes online time

758
00:46:53,220 --> 00:46:56,560
but the nice thing is this is just the standard quadratic problem

759
00:46:56,580 --> 00:47:00,270
you can use we could tweak to chance code a little bit

760
00:47:00,290 --> 00:47:02,330
and you could just happily it

761
00:47:09,160 --> 00:47:11,240
how do we prove such results

762
00:47:11,270 --> 00:47:15,040
basically what you do is you argue that for smooth functions with the loss doesn't

763
00:47:15,040 --> 00:47:16,950
change much applications

764
00:47:17,870 --> 00:47:19,180
the empirical

765
00:47:19,200 --> 00:47:24,450
the two losses are reasonably close if i just take care of my min operators

766
00:47:25,790 --> 00:47:29,720
the new show that the empirical loss is close to the expected loss

767
00:47:29,770 --> 00:47:32,100
so basically have to work

768
00:47:32,120 --> 00:47:33,970
for the approximation term

769
00:47:33,990 --> 00:47:38,060
we trust try to map different distributions and then you have to worry about the

770
00:47:38,060 --> 00:47:39,140
fact that well

771
00:47:39,200 --> 00:47:41,950
actually don't have

772
00:47:41,970 --> 00:47:47,600
the right expectation but you just have an empirical observation of the labels

773
00:47:47,600 --> 00:47:51,700
so the first statement essentially is concerned with the x depend in part

774
00:47:51,700 --> 00:47:57,140
and that's like the the y dependence part we condition on x

775
00:47:57,180 --> 00:48:01,600
but the tools are very standard

776
00:48:03,080 --> 00:48:07,490
let's have a look at some actual but i mean that that's the reason why

777
00:48:07,490 --> 00:48:08,390
does it

778
00:48:08,470 --> 00:48:12,060
so this is actually from a paper by shimodaira

779
00:48:12,120 --> 00:48:15,930
and the blue surface in the right ones are

780
00:48:15,990 --> 00:48:17,770
the dot is drawn from

781
00:48:17,810 --> 00:48:19,890
the solent blue line

782
00:48:21,290 --> 00:48:23,640
one would assume that the blue dots

783
00:48:23,660 --> 00:48:27,720
are what is drawn from the training data

784
00:48:27,740 --> 00:48:31,180
and the red dots are the ones drawn from the test

785
00:48:31,250 --> 00:48:32,560
sorry the other way around

786
00:48:32,560 --> 00:48:34,120
that's the training data

787
00:48:34,220 --> 00:48:36,810
and that's it is

788
00:48:38,950 --> 00:48:40,540
and then

789
00:48:40,560 --> 00:48:43,700
the idea is well you would like to find some linear model which fits the

790
00:48:43,720 --> 00:48:46,290
started fairly well

791
00:48:48,220 --> 00:48:50,290
so if you don't do any rewriting

792
00:48:50,310 --> 00:48:54,970
this is pretty much what you're gonna get it

793
00:48:54,990 --> 00:48:56,370
if you

794
00:48:57,430 --> 00:48:58,870
well the proper

795
00:48:58,890 --> 00:49:00,450
importance sampling

796
00:49:00,450 --> 00:49:03,790
that's what you're looking

797
00:49:04,540 --> 00:49:08,540
well this is what you will get if you use shimodaira method which does the

798
00:49:08,540 --> 00:49:11,930
importance sampling then if you act on top of that

799
00:49:12,120 --> 00:49:13,720
this is what you get

800
00:49:13,770 --> 00:49:15,410
if you use

801
00:49:17,490 --> 00:49:20,620
re weighting method that just six days before

802
00:49:20,660 --> 00:49:25,100
and that would be the optimal answer

803
00:49:27,390 --> 00:49:29,430
you can see that actually works fairly well

804
00:49:29,470 --> 00:49:34,970
and if you look at the very

805
00:49:35,360 --> 00:49:37,520
what is this thing doing

806
00:49:37,580 --> 00:49:45,270
so this is what you get three importance sampling

807
00:49:45,290 --> 00:49:48,560
this is get this is what you get to the moment matching we what explain

808
00:49:51,240 --> 00:49:53,700
that's what she our method with two

809
00:49:53,700 --> 00:49:55,330
so these are all

810
00:49:55,330 --> 00:49:57,240
this mysterious

811
00:49:57,330 --> 00:49:59,830
and this is of course what you do if you just do it if you

812
00:49:59,830 --> 00:50:01,790
train and test it

813
00:50:01,890 --> 00:50:03,910
the variance is lower

814
00:50:03,930 --> 00:50:06,700
and the mean is also considerably lower

815
00:50:06,720 --> 00:50:09,740
i mean but obviously that's just toy example

816
00:50:09,750 --> 00:50:13,450
let's look at some real data

817
00:50:13,470 --> 00:50:14,890
that's on

818
00:50:14,910 --> 00:50:19,620
the breast cancer dataset and well unfortunately it's really hard to find such five starters

819
00:50:19,620 --> 00:50:21,140
it's where you can

820
00:50:21,160 --> 00:50:22,660
effective taste

821
00:50:22,680 --> 00:50:24,750
so we constructed in

822
00:50:24,770 --> 00:50:27,740
and the what we did is basically took

823
00:50:27,790 --> 00:50:29,660
dataset which was all the same

824
00:50:29,700 --> 00:50:32,290
and the bias our training set

825
00:50:32,290 --> 00:50:36,770
he wanted to do we can therefore the classifier because it's just so equal just

826
00:50:36,770 --> 00:50:38,730
had about seven events for

827
00:50:38,740 --> 00:50:43,060
failures and thousands of thousands

828
00:50:43,120 --> 00:50:47,080
normal operation we can let's build a model of you know all

829
00:50:47,090 --> 00:50:51,440
and then detect hopefully any divergence from norm normality

830
00:51:00,380 --> 00:51:06,360
there's actually two things ways to approach the problem is the same as in there

831
00:51:06,460 --> 00:51:12,100
one approach is to model was called the support of the distribution

832
00:51:12,110 --> 00:51:13,740
which is actually what i do here

833
00:51:13,760 --> 00:51:17,860
and the idea is you something i want to know where the data is located

834
00:51:18,020 --> 00:51:22,600
OK then have boundary around and outside the country is assumed to be novel

835
00:51:22,610 --> 00:51:25,960
the alternative which is suggested here which is also

836
00:51:25,980 --> 00:51:30,400
two triumph it's some sort of a mixture of gaussians or some sort of distribution

837
00:51:30,400 --> 00:51:31,930
function of your data

838
00:51:31,950 --> 00:51:33,820
and do it that way OK

839
00:51:33,840 --> 00:51:36,400
so that's alternative which might do

840
00:51:36,420 --> 00:51:39,390
here i'm just going to do the support of the distribution

841
00:51:39,400 --> 00:51:42,410
and as say the more subject and giving him

842
00:51:42,430 --> 00:51:46,300
so if just wanted to model support of the distribution then this is a binary

843
00:51:46,300 --> 00:51:51,060
valued function which is positive in the region beams put space weather data lies and

844
00:51:51,060 --> 00:51:52,430
negative elsewhere

845
00:51:52,450 --> 00:51:56,980
you can do this using linear programming and the method i give is with christine

846
00:51:56,980 --> 00:52:00,790
better just to illustrate the idea but i think about this book it tends to

847
00:52:00,790 --> 00:52:06,060
do his own quadratic programming type things subjective is defined as a surface in input

848
00:52:06,060 --> 00:52:08,090
space which wraps around the data

849
00:52:08,100 --> 00:52:11,010
and as i say anything outside is abnormal

850
00:52:11,640 --> 00:52:14,810
what you do is use a gaussian kernel

851
00:52:17,090 --> 00:52:19,740
the reason for the choice get kind

852
00:52:19,760 --> 00:52:22,780
comes about from just have a quick look at

853
00:52:22,800 --> 00:52:28,870
i introduced last lecture six months ex-prime square

854
00:52:28,880 --> 00:52:35,520
o two sigma square led to actually quite ex-prime words just considering the point itself

855
00:52:35,570 --> 00:52:36,890
and what i see

856
00:52:37,030 --> 00:52:41,620
is that this is going to be easier to just one OK

857
00:52:41,670 --> 00:52:44,240
now with the gas kernel therefore

858
00:52:44,260 --> 00:52:46,340
all the data points must lie

859
00:52:46,350 --> 00:52:47,720
on a hypersphere

860
00:52:47,730 --> 00:52:51,540
of unit radius in feature space for this reason OK

861
00:52:51,550 --> 00:52:55,130
and indeed there full-time using a gas kernel

862
00:52:55,140 --> 00:52:56,720
my domain data

863
00:52:56,730 --> 00:53:00,730
lies like this in in this feature space

864
00:53:00,740 --> 00:53:06,550
now the idea that get a give you now christine invent is that we saw

865
00:53:06,560 --> 00:53:10,360
a hyperplane in the feature space on the data

866
00:53:10,380 --> 00:53:16,890
OK provided the data points became negative then i just stopped it on the data

867
00:53:18,150 --> 00:53:20,030
doing that is a simple

868
00:53:20,040 --> 00:53:23,140
linear programming talk problem case alpha here

869
00:53:23,150 --> 00:53:25,670
you can do it using quadratic programming as well

870
00:53:25,690 --> 00:53:30,060
OK again we're seeing this linear programming quadratic programming choice

871
00:53:30,070 --> 00:53:36,430
so that's the objective function subject to constraints these with the constraints and you can

872
00:53:36,430 --> 00:53:40,550
also do using soft margins and the bias and

873
00:53:40,570 --> 00:53:46,110
you got various choices just given the basic idea is to cleanse the data and

874
00:53:46,960 --> 00:53:51,930
and it's not the the best way of running but it's boundary around the data

875
00:53:51,940 --> 00:53:53,570
OK and so something like here

876
00:53:53,670 --> 00:53:55,520
will be viewed as normal

877
00:53:55,530 --> 00:53:57,790
whereas here would be viewed as normal

878
00:53:57,830 --> 00:54:02,980
OK so it's just means of estimating the support we can elaborate it uses soft

879
00:54:03,950 --> 00:54:06,850
these guys here i really like the boundary

880
00:54:06,860 --> 00:54:09,070
so i can use the soft margin variant

881
00:54:09,080 --> 00:54:11,170
and drop them out OK

882
00:54:11,190 --> 00:54:12,690
after all

883
00:54:12,910 --> 00:54:17,200
if i got data which i i said just now my unable detect tries to

884
00:54:17,200 --> 00:54:20,230
get a function which covers the data

885
00:54:20,970 --> 00:54:27,900
and then detect abnormal events unfortunately in my trace and they also have normal events

886
00:54:27,900 --> 00:54:32,280
OK there outliers have to account for the possibility so that's why you can do

887
00:54:32,280 --> 00:54:38,020
this all this with the soft margin with not modify kernel is my data

888
00:54:38,040 --> 00:54:41,540
it was an nature only data like so

889
00:54:41,550 --> 00:54:45,840
you'll notice some data points set right on the edge that doesn't look too good

890
00:54:45,880 --> 00:54:50,120
you can do a variant on its way you introduce virtual data or various other

891
00:54:50,120 --> 00:54:55,230
variants which pushes the boundary as you wish OK so you have various on the

892
00:54:55,230 --> 00:54:58,760
same story but it's sort of greasy c

893
00:54:58,770 --> 00:54:59,680
it's actually

894
00:54:59,700 --> 00:55:02,780
estimating the support of the distribution and

895
00:55:02,780 --> 00:55:05,050
i have here very special

896
00:55:05,090 --> 00:55:09,840
violin string or yarn or string whatever you want to call this

897
00:55:09,870 --> 00:55:12,270
and we can generate

898
00:55:12,270 --> 00:55:14,560
these resonance frequencies by

899
00:55:14,570 --> 00:55:17,530
searching for them and i would need assistance from

900
00:55:17,590 --> 00:55:20,410
student which you be willing to help me

901
00:55:20,430 --> 00:55:22,420
so your hand and one

902
00:55:22,470 --> 00:55:24,540
hold one end of the

903
00:55:24,650 --> 00:55:27,370
piano string in your hand

904
00:55:27,380 --> 00:55:30,560
don't let it go please don't let go

905
00:55:30,560 --> 00:55:32,970
i promise you i will let it go either

906
00:55:33,080 --> 00:55:35,380
OK so i put a certain tension on it

907
00:55:35,430 --> 00:55:39,390
and i started to shake at very low frequencies

908
00:55:39,440 --> 00:55:41,360
the columns

909
00:55:41,420 --> 00:55:44,620
and look are happy this string is nothing

910
00:55:44,700 --> 00:55:47,430
just last me ignores me

911
00:55:47,440 --> 00:55:48,810
it doesn't like me

912
00:55:48,810 --> 00:55:54,220
now i going increase the frequency

913
00:55:54,230 --> 00:55:56,550
hitting the first coming up

914
00:55:56,580 --> 00:55:57,740
and is

915
00:55:57,790 --> 00:56:01,830
that's exactly the shape that you see on the blackboard

916
00:56:01,880 --> 00:56:03,240
very clear

917
00:56:03,270 --> 00:56:04,890
shake it here

918
00:56:04,920 --> 00:56:08,540
have exactly the resonance

919
00:56:08,540 --> 00:56:09,990
f one

920
00:56:10,000 --> 00:56:11,750
let me know try to

921
00:56:11,800 --> 00:56:12,860
it's the

922
00:56:12,890 --> 00:56:15,420
second harmonic

923
00:56:15,470 --> 00:56:17,550
if i got a little over the

924
00:56:17,590 --> 00:56:18,720
f one then

925
00:56:18,720 --> 00:56:24,160
nothing much happens

926
00:56:24,230 --> 00:56:26,270
apart from e to c is this

927
00:56:26,320 --> 00:56:29,340
this is the second one was already the third

928
00:56:29,360 --> 00:56:30,720
so very difficult

929
00:56:30,730 --> 00:56:38,120
the this year i can get the second

930
00:56:38,180 --> 00:56:40,010
i think i got it now

931
00:56:42,150 --> 00:56:45,020
OK so you see the second harmonic

932
00:56:45,160 --> 00:56:48,760
he needed no that point standing still

933
00:56:48,810 --> 00:56:50,420
the amplitude is you know

934
00:56:50,450 --> 00:56:52,290
that's the characteristic for

935
00:56:52,330 --> 00:56:57,360
resonance frequency we call it also normal mode frequency or natural frequencies all the same

936
00:56:59,630 --> 00:57:02,500
let me know try to generate one that is very high as high as i

937
00:57:02,500 --> 00:57:03,760
possibly can

938
00:57:03,810 --> 00:57:05,000
you tell me which

939
00:57:05,010 --> 00:57:06,430
monarch it is

940
00:57:06,450 --> 00:57:09,650
all you have to do is count how many nodes there are not counting my

941
00:57:09,650 --> 00:57:11,310
hand on his hand

942
00:57:11,340 --> 00:57:12,630
you add one

943
00:57:12,650 --> 00:57:13,650
and that is the

944
00:57:13,660 --> 00:57:19,430
monica that generates

945
00:57:19,540 --> 00:57:22,180
the system is not too happy to you feel it sort of when it comes

946
00:57:24,570 --> 00:57:32,320
i see no i'm of residents

947
00:57:32,480 --> 00:57:35,610
very hard for me to add resonance but i will get it

948
00:57:35,630 --> 00:57:37,870
comes the first question

949
00:57:38,020 --> 00:57:40,980
residents now look it

950
00:57:41,040 --> 00:57:43,320
william resonance

951
00:57:43,370 --> 00:57:46,010
so how many to come

952
00:57:46,040 --> 00:57:47,570
six harmonic

953
00:57:47,800 --> 00:57:50,690
twelve to me but OK you can better than i can

954
00:57:50,740 --> 00:57:53,070
OK thank you very much so you see here

955
00:57:53,080 --> 00:57:55,260
our was system response

956
00:57:55,290 --> 00:57:57,120
two driver

957
00:57:57,150 --> 00:57:59,130
and complicated system like this

958
00:58:00,360 --> 00:58:01,600
has many many

959
00:58:01,650 --> 00:58:03,980
of these resonance frequencies

960
00:58:04,020 --> 00:58:06,580
which we call the normal modes

961
00:58:06,640 --> 00:58:08,130
so when you have a

962
00:58:10,240 --> 00:58:11,860
four strings

963
00:58:11,870 --> 00:58:16,220
they all have the same like their musical instruments like the piano by links is

964
00:58:17,380 --> 00:58:20,340
finally for strings all the same length

965
00:58:20,420 --> 00:58:21,340
you can

966
00:58:21,380 --> 00:58:22,280
set b

967
00:58:22,280 --> 00:58:24,590
tensions so you have something to play with

968
00:58:24,700 --> 00:58:28,590
tension is change normally when you tony instrument before you start playing

969
00:58:28,610 --> 00:58:31,560
but these four strings all have different mass

970
00:58:31,580 --> 00:58:35,780
that gives them a very different frequencies

971
00:58:35,830 --> 00:58:39,400
now to played the violin you cannot change the tension of course during the play

972
00:58:39,580 --> 00:58:42,970
that would be a little difficult although the instruments exist

973
00:58:42,980 --> 00:58:43,990
we're actually

974
00:58:44,030 --> 00:58:46,280
the playing depends on the tension of the strings

975
00:58:46,330 --> 00:58:48,660
the violin that's not the case with violin

976
00:58:48,690 --> 00:58:50,980
the only option value playing it

977
00:58:51,000 --> 00:58:52,140
is to make the

978
00:58:52,220 --> 00:58:53,950
string length shorter

979
00:58:53,970 --> 00:58:56,980
you do that by moving you finger along the string

980
00:58:57,030 --> 00:58:58,260
by making it shorter

981
00:58:58,270 --> 00:59:01,180
the frequency goes up and by making it longer

982
00:59:01,200 --> 00:59:02,260
the frequency

983
00:59:02,310 --> 00:59:03,950
goes down

984
00:59:04,010 --> 00:59:05,950
now how do you excite

985
00:59:05,970 --> 00:59:11,010
musical instrument like a piano string or violin string

986
00:59:11,050 --> 00:59:14,840
there's nothing that is driving it exactly at the resonance frequencies if you want to

987
00:59:14,840 --> 00:59:16,310
get to four hundred forty

988
00:59:16,360 --> 00:59:18,590
words out of the violin string

989
00:59:18,640 --> 00:59:21,940
then you know not driving it exactly the four hundred forty it's like we are

990
00:59:23,070 --> 00:59:25,050
well that's true

991
00:59:25,090 --> 00:59:26,940
but if you take a bow

992
00:59:26,990 --> 00:59:29,220
and you registering with the ball

993
00:59:29,260 --> 00:59:33,020
that in a way which you're doing is you're exposing that string

994
00:59:33,030 --> 00:59:36,080
to a lot of possible frequencies not just one

995
00:59:36,110 --> 00:59:37,840
it is a running action

996
00:59:37,840 --> 00:59:41,840
i could also robert was my finger i could market what i could kick it

997
00:59:41,930 --> 00:59:43,160
and what does it

998
00:59:43,180 --> 00:59:48,060
it ignores all the frequencies which are not resonance but it picks out the ones

999
00:59:48,060 --> 00:59:49,530
which i addressed

1000
00:59:49,560 --> 00:59:53,660
so striking it was the bow is effectively exposing it to to a whole art

1001
00:59:53,690 --> 00:59:57,810
spectrum of frequencies and it picks out the ones that likes

1002
00:59:57,820 --> 01:00:01,540
in fact if you strike a violin string with the ball

1003
01:00:01,550 --> 01:00:06,080
you may excited simultaneously in the first harmonic and in the second even in the

1004
01:00:06,080 --> 01:00:08,640
third harmonic or even higher harmonics

1005
01:00:08,660 --> 01:00:09,640
and that makes

1006
01:00:09,650 --> 01:00:12,720
the difference between the various in some that gives special

1007
01:00:12,720 --> 01:00:14,090
don't quality

1008
01:00:14,150 --> 01:00:16,410
depends upon the cocktail the combination

1009
01:00:16,420 --> 01:00:17,910
of the various

1010
01:00:17,920 --> 01:00:20,770
harmonix that you excite

1011
01:00:20,840 --> 01:00:24,530
now if you go to a woodwind instruments

1012
01:00:24,630 --> 01:00:25,980
the situation

1013
01:00:26,930 --> 01:00:29,450
quite different

1014
01:00:29,490 --> 01:00:31,120
i have here

1015
01:00:31,190 --> 01:00:32,570
some cavity

1016
01:00:33,670 --> 01:00:35,700
it has length l

1017
01:00:35,750 --> 01:00:37,810
and there's an inside you

1018
01:00:37,860 --> 01:00:39,390
and i want to see whether this

1019
01:00:39,400 --> 01:00:41,720
system has resonance frequencies

1020
01:00:41,760 --> 01:00:44,550
so i put it here on the loudspeaker

1021
01:00:44,750 --> 01:00:50,990
generate sound different frequencies and search for residences

1022
01:00:50,990 --> 01:00:52,780
and there are

1023
01:00:52,830 --> 01:00:57,580
the residences of this system however a different from this string in that sense it's

1024
01:00:57,580 --> 01:00:59,750
not the box that will resonate here

1025
01:00:59,800 --> 01:01:04,700
this is the area itself starts to resonate area x like which way

1026
01:01:04,720 --> 01:01:08,360
in fact in that sense it is very parallel to or spring system

1027
01:01:08,410 --> 01:01:10,790
it is also a longitudinal

1028
01:01:10,830 --> 01:01:15,880
oscillation was that is the transverse oscillations so you make pressure waves inside

1029
01:01:15,930 --> 01:01:17,990
and if you do that the right frequency

1030
01:01:17,990 --> 01:01:20,740
air acts like a spring and then you get

1031
01:01:20,800 --> 01:01:22,810
strong reactions which i

1032
01:01:22,860 --> 01:01:24,860
the residences

1033
01:01:24,870 --> 01:01:25,840
in this case

1034
01:01:27,650 --> 01:01:29,560
and smote

1035
01:01:29,660 --> 01:01:31,160
and harmonic

1036
01:01:31,180 --> 01:01:32,230
is given by

1037
01:01:33,340 --> 01:01:35,240
times the velocity of sound

1038
01:01:35,280 --> 01:01:37,140
divided by two l

1039
01:01:37,200 --> 01:01:38,990
and v

1040
01:01:39,000 --> 01:01:42,250
it's about three hundred forty meters per second

1041
01:01:42,300 --> 01:01:43,640
the temperature

1042
01:01:43,700 --> 01:01:45,950
so it sounds

1043
01:01:46,030 --> 01:01:50,920
nor is it again the inherent

1044
01:01:50,970 --> 01:01:52,870
in other words if an instrument

1045
01:01:52,970 --> 01:01:55,510
but uses a certain

1046
01:01:57,340 --> 01:01:59,160
let's say that l

1047
01:01:59,230 --> 01:02:02,540
because twenty five centimetres

1048
01:02:02,540 --> 01:02:10,370
OK i'm going to explain think being and a this strategy so we include like

1049
01:02:10,370 --> 01:02:12,980
a huge number of people

1050
01:02:13,800 --> 01:02:17,570
let me show you how we do

1051
01:02:17,580 --> 01:02:21,160
the general idea has been proposed by leslie in two thousand twelve

1052
01:02:21,180 --> 01:02:23,810
other questions

1053
01:02:24,020 --> 01:02:30,090
the general idea has been proposed by christina leslie the two

1054
01:02:30,110 --> 01:02:33,870
and the idea is that we consider

1055
01:02:33,920 --> 01:02:39,730
OK now OK could be like for something i i mean some or some small

1056
01:02:39,730 --> 01:02:45,870
number of nucleotides or amino acids which are like in sequence

1057
01:02:45,920 --> 01:02:47,740
so like next to each other

1058
01:02:47,760 --> 01:02:52,810
we consider came in and for every possible came which is a bit size to

1059
01:02:52,810 --> 01:02:53,900
the power of k

1060
01:02:53,920 --> 01:02:57,140
for every possible came out we have one feature

1061
01:02:57,240 --> 01:03:06,460
so it essentially created feature that every example every second week we record came appears

1062
01:03:07,060 --> 01:03:08,130
the sequence

1063
01:03:08,140 --> 01:03:13,270
this is what is defined here to the indicate spectral

1064
01:03:13,290 --> 01:03:16,090
at the time this one

1065
01:03:17,350 --> 01:03:20,970
this is defined as either one when the

1066
01:03:20,990 --> 01:03:22,890
heyman s

1067
01:03:22,900 --> 01:03:24,830
it's appearing in the sequence x

1068
01:03:26,520 --> 01:03:34,210
what's your what's actually actually counting all the sequence x appearing in the

1069
01:03:34,280 --> 01:03:41,460
and we have no effect on where the sprawl this information called him

1070
01:03:41,480 --> 01:03:48,050
the dimensionality of x the exponential in k to be about the they are thing

1071
01:03:48,060 --> 01:03:52,760
the spectrum kernel to be discovered product of these two features

1072
01:03:52,780 --> 01:03:57,570
so this might be very inefficient but there are actually ways of people eating the

1073
01:03:58,680 --> 01:04:02,900
so and maybe just to give you an example

1074
01:04:02,980 --> 01:04:09,450
the second point is to the protein sequence if you put a b

1075
01:04:09,500 --> 01:04:11,660
and now you look at all

1076
01:04:11,670 --> 01:04:13,630
what when you looking at one man

1077
01:04:14,230 --> 01:04:15,440
just let

1078
01:04:15,490 --> 01:04:17,470
you can identify all the

1079
01:04:17,490 --> 01:04:21,840
one man all the letters PN sequences quite efficient

1080
01:04:21,890 --> 01:04:27,730
you don't need to compute exponential exponentially dimensional feature vectors and the kind of product

1081
01:04:27,920 --> 01:04:31,570
you just need to see which led us to here and here and here

1082
01:04:31,620 --> 01:04:34,820
the l p c

1083
01:04:34,830 --> 01:04:37,890
the same kind of the tumor with three minutes

1084
01:04:37,910 --> 01:04:42,990
and you know this and the number of they appear then you can compute this

1085
01:04:42,990 --> 01:04:44,980
kind of product without really

1086
01:04:44,990 --> 01:04:47,350
computing the kind of the city

1087
01:04:47,370 --> 01:04:50,690
without computing the to the text

1088
01:04:50,740 --> 01:04:55,520
it is important

1089
01:04:59,590 --> 01:05:02,370
i choose which

1090
01:05:02,470 --> 01:05:05,740
OK premiered a new rituals

1091
01:05:07,590 --> 01:05:13,000
typically depending on the alphabet so like in nucleotide you're looking at six million eighty

1092
01:05:13,220 --> 01:05:17,000
six proteins usually but left the

1093
01:05:17,040 --> 01:05:22,550
but you really i twenty two the par five already item

1094
01:05:22,590 --> 01:05:24,520
so but

1095
01:05:24,800 --> 01:05:27,490
so you have choose to this k just by

1096
01:05:27,540 --> 01:05:30,440
what you can do you can already look at

1097
01:05:31,590 --> 01:05:33,370
a different case at the same time

1098
01:05:33,420 --> 01:05:37,810
it's actually a good idea so you look at one man who may be on

1099
01:05:37,810 --> 01:05:42,090
you have to that if you usually one

1100
01:05:42,190 --> 01:05:48,940
at that point no i

1101
01:05:48,950 --> 01:05:55,050
are better than others

1102
01:06:03,580 --> 01:06:10,190
OK so it may be the best thing that could be just period any position

1103
01:06:10,230 --> 01:06:13,100
so it's not important that they appear

1104
01:06:13,140 --> 01:06:16,000
come to kernel which are position dependent

1105
01:06:16,010 --> 01:06:20,050
but for now i just want to do

1106
01:06:36,740 --> 01:06:40,200
can you know when you have several kernels you take the sum of kernels

1107
01:06:40,220 --> 01:06:43,170
it's just going to nuclear

1108
01:06:48,520 --> 01:06:54,720
if you you think of what what it means in feature space you have to

1109
01:06:55,110 --> 01:06:59,100
search maybe one can we take this kind of product in one subspace

1110
01:06:59,150 --> 01:07:04,360
and now you adding to kernel means essentially that you take the two pictures basis

1111
01:07:04,360 --> 01:07:07,730
and augment the i mean concatenated each other

1112
01:07:07,760 --> 01:07:11,060
OK because then it kind of a product

1113
01:07:11,180 --> 01:07:14,790
taken independently of each other

1114
01:07:14,840 --> 01:07:16,710
i mean this kind of product

1115
01:07:18,230 --> 01:07:19,710
so that's

1116
01:07:20,920 --> 01:07:27,010
considering one man to man be this independently and then

1117
01:07:27,050 --> 01:07:31,130
OK so important part of this

1118
01:07:31,160 --> 01:07:33,420
i have x only had very few

1119
01:07:33,430 --> 01:07:35,020
non zero element

1120
01:07:35,620 --> 01:07:39,200
only those payments which appear in sequence

1121
01:07:39,210 --> 01:07:40,770
they will be non-zero

1122
01:07:40,770 --> 01:07:45,670
and this allows very efficient computation what about the later

1123
01:07:45,690 --> 01:07:46,780
i think one

1124
01:07:46,780 --> 01:07:53,980
so the fact that this is an example of

1125
01:07:54,030 --> 01:07:57,010
so called substring

1126
01:07:57,170 --> 01:08:02,030
and idea is that the count common substrings in two strings

1127
01:08:02,070 --> 01:08:07,170
and i think that are deemed to be more similar than they have more up

1128
01:08:07,410 --> 01:08:10,510
walk on that subject

1129
01:08:10,550 --> 01:08:15,610
so now there are variations and this one will allow

1130
01:08:15,630 --> 01:08:18,600
and then in the card

1131
01:08:18,600 --> 01:08:24,600
allow mismatches and only to look at the motives of what you have wait

1132
01:08:24,620 --> 01:08:33,030
i'm only going to get old

1133
01:08:33,690 --> 01:08:38,840
OK so let me explain the so-called gettering so

1134
01:08:38,890 --> 01:08:41,060
the idea is that if we

1135
01:08:41,060 --> 01:08:43,750
look at this spectral sequences which are

1136
01:08:43,800 --> 01:08:50,670
like its sequence select and then we look at all different letter combinations which are

1137
01:08:50,670 --> 01:08:54,360
in the middle so points all too many which are

1138
01:08:54,420 --> 01:08:58,130
for all combinations two letters which are within the man

1139
01:08:58,140 --> 01:09:02,410
so there was no need to be next to each

1140
01:09:02,410 --> 01:09:05,210
the problem is designed in such a way that

1141
01:09:05,210 --> 01:09:07,900
it can be a scan can just go find or

1142
01:09:07,920 --> 01:09:11,090
sit there just keep eating one

1143
01:09:11,110 --> 01:09:14,200
is the inexhaustible supply one

1144
01:09:14,210 --> 01:09:17,960
or you can do much more challenging thing you can go to war

1145
01:09:17,970 --> 01:09:19,210
pick it up

1146
01:09:19,230 --> 01:09:20,980
the men were not needed

1147
01:09:21,010 --> 01:09:22,170
because hungry

1148
01:09:22,190 --> 01:09:23,830
what all the fish

1149
01:09:23,830 --> 01:09:27,780
you get one fish it now has one

1150
01:09:27,830 --> 01:09:29,890
it has to go back to the war

1151
01:09:29,900 --> 01:09:31,710
the couple are remember

1152
01:09:31,710 --> 01:09:34,310
much more challenging to learn to fish

1153
01:09:34,320 --> 01:09:37,360
that just need to work

1154
01:09:38,770 --> 01:09:42,560
so what is the fitness level is just witness increments by one for each fish

1155
01:09:42,960 --> 01:09:44,730
and points for for each

1156
01:09:44,760 --> 01:09:46,590
for each data sets case

1157
01:09:46,590 --> 01:09:49,560
i just define an arbitrary notion of of

1158
01:09:51,090 --> 01:09:53,960
OK so now what do i basically defined

1159
01:09:54,170 --> 01:09:58,320
his experience some experimental results

1160
01:09:58,330 --> 01:09:59,720
the x axis

1161
01:10:00,510 --> 01:10:03,330
possible lifetimes of an agent

1162
01:10:03,530 --> 01:10:07,090
so each point on the x axis is a completely different age

1163
01:10:07,100 --> 01:10:11,630
here the list for ten thousand time steps here is in the list eleven thousand

1164
01:10:11,630 --> 01:10:16,850
time steps here is a journalist for told that so what changes is the lifetime

1165
01:10:16,850 --> 01:10:19,130
of the nation

1166
01:10:19,380 --> 01:10:21,570
and basically searching

1167
01:10:21,590 --> 01:10:23,490
o were

1168
01:10:23,500 --> 01:10:25,660
internal reward functions

1169
01:10:25,680 --> 01:10:28,640
which are functions of internal states

1170
01:10:28,670 --> 01:10:36,030
what features of internal states the amount of hungary let's remember something it is hungry

1171
01:10:36,050 --> 01:10:37,130
or is it

1172
01:10:37,140 --> 01:10:39,780
not hungry for eating fish

1173
01:10:39,790 --> 01:10:43,230
or its medium revisiting the war

1174
01:10:44,480 --> 01:10:48,100
the internal reward function can decide how much reward

1175
01:10:48,120 --> 01:10:51,380
internal rewards ascribed to each of these two situations

1176
01:10:51,410 --> 01:10:55,610
and searching the what three-dimensional reward function space

1177
01:10:55,630 --> 01:11:00,210
so for each agent i search for the best reward function

1178
01:11:00,380 --> 01:11:05,800
internal reward and ask how much to that agent actually

1179
01:11:05,970 --> 01:11:07,290
the red line

1180
01:11:07,300 --> 01:11:08,120
it is the

1181
01:11:08,140 --> 01:11:11,920
thickness of paint by the best internal reward

1182
01:11:11,960 --> 01:11:17,220
and the blue line is the thickness obtained by the best by the

1183
01:11:17,240 --> 01:11:22,800
confounded reward where the fitness function is used in war

1184
01:11:22,910 --> 01:11:25,990
no search just little is the fitness

1185
01:11:26,710 --> 01:11:30,490
so the first thing to observe is that it does not surprisingly because the fitness

1186
01:11:30,490 --> 01:11:34,830
award is is in the search space of internal rewards they don't reward always better

1187
01:11:34,890 --> 01:11:39,630
you can see the difference between the lifetime of twenty five thousand because the difference

1188
01:11:39,640 --> 01:11:42,730
three as much more much more late so

1189
01:11:43,730 --> 01:11:48,090
let's keep going here somebody noticed this is about the life about twenty five thousand

1190
01:11:48,090 --> 01:11:50,610
steps is a lifetime

1191
01:11:50,620 --> 01:11:51,840
beyond which

1192
01:11:51,860 --> 01:11:55,710
it and this is going to live longer than that it can learn to fish

1193
01:11:55,710 --> 01:11:59,110
and exploit that explains the biggest slow

1194
01:11:59,160 --> 01:12:01,470
it was lifetimes shorter than that

1195
01:12:01,600 --> 01:12:06,420
the best thing you can do is just eight one

1196
01:12:06,430 --> 01:12:08,290
never just

1197
01:12:08,310 --> 01:12:12,160
not bothered about eating fish

1198
01:12:12,320 --> 01:12:16,490
it would get confounded reward it takes much longer

1199
01:12:17,720 --> 01:12:21,740
to learn to eat fish the media is about forty

1200
01:12:21,760 --> 01:12:26,330
OK let's keep punishment state many many things about this so let's

1201
01:12:26,330 --> 01:12:30,090
if i can go on

1202
01:12:31,830 --> 01:12:34,590
so the phi let's just do this this

1203
01:12:35,740 --> 01:12:38,810
so the point is the best internal reward can help

1204
01:12:38,850 --> 01:12:42,830
because mitigation value y mitigation a few minutes and then there's much

1205
01:12:44,600 --> 01:12:48,350
for horizon for a lifetime that short enough that you can't learn to eat fish

1206
01:12:48,350 --> 01:12:50,820
we can't learn to eat fish so

1207
01:12:50,830 --> 01:12:52,480
you know that don't know what to better

1208
01:12:52,490 --> 01:12:53,830
it is but i much

1209
01:12:53,910 --> 01:12:59,030
this is mid-life it's really interesting because of course the best internal reward is much

1210
01:12:59,030 --> 01:13:02,640
better teaching to fish short enough

1211
01:13:03,170 --> 01:13:06,330
it is long enough live long enough that both

1212
01:13:06,550 --> 01:13:07,850
and efficiency of the

1213
01:13:07,860 --> 01:13:11,800
the best internal reward agent can get get there much faster

1214
01:13:12,880 --> 01:13:17,970
so now here again different view in which we're looking at the proportion of fitness

1215
01:13:18,260 --> 01:13:19,740
that comes from eating

1216
01:13:20,610 --> 01:13:21,550
in the war

1217
01:13:21,570 --> 01:13:25,260
and again this is saying what i said before the for lifetime of twenty five

1218
01:13:25,260 --> 01:13:29,930
thousand it's all this comes from from eating not

1219
01:13:29,950 --> 01:13:33,410
one thing to keep in in mind is the learning curve for each agent is

1220
01:13:33,410 --> 01:13:35,040
completely separate

1221
01:13:35,040 --> 01:13:38,360
bayes classifier is this without the data

1222
01:13:39,140 --> 01:13:40,990
so the bayes classifier is

1223
01:13:40,990 --> 01:13:45,810
minimize expected loss under the true distribution of the labels given the input assuming you

1224
01:13:45,810 --> 01:13:47,890
knew everything about the world

1225
01:13:47,950 --> 01:13:52,860
so the bayes classifier is the best possible classifier that you could possibly do if

1226
01:13:52,860 --> 01:13:54,140
you knew everything

1227
01:13:54,200 --> 01:13:58,540
so it's sort of theoretical objects that people compared to analysis

1228
01:13:58,570 --> 01:14:02,240
it has no practical application because you can never know everything

1229
01:14:02,260 --> 01:14:03,350
bayesian inference

1230
01:14:03,370 --> 01:14:08,100
doesn't assume you know the truth there is uncertainty and conditions from the data set

1231
01:14:08,110 --> 01:14:13,640
and this this is the predictive distribution given the data rather than given knowing everything

1232
01:14:13,730 --> 01:14:15,780
OK that was on the site

1233
01:14:15,850 --> 01:14:18,060
this is what we're trying to do

1234
01:14:18,100 --> 01:14:21,690
we need to now know how and how we come up with this distribution how

1235
01:14:21,690 --> 01:14:25,240
do we predict new labels given a new input given the data set

1236
01:14:25,270 --> 01:14:30,480
and if you've done lots of manipulations probabilities you would write down immediately the surface

1237
01:14:30,480 --> 01:14:34,960
integral if you have and then maybe after tomorrow you will

1238
01:14:34,970 --> 01:14:38,580
this probability is just equal to the integral the idea is

1239
01:14:39,380 --> 01:14:43,510
it's the average of the predictions we make for each possible weight vector

1240
01:14:43,510 --> 01:14:44,920
average by

1241
01:14:44,950 --> 01:14:47,210
their posterior plausibility

1242
01:14:47,940 --> 01:14:51,380
you make predictions using whole bunch of different weight vectors

1243
01:14:51,400 --> 01:14:57,640
and you wait those predictions by how probable work those parameters are

1244
01:14:59,050 --> 01:15:06,900
OK so

1245
01:15:06,920 --> 01:15:09,550
here's a picture that

1246
01:15:09,570 --> 01:15:14,560
might make this a bit more concrete the logistic regressions so we have a classifier

1247
01:15:15,880 --> 01:15:17,210
in weight space

1248
01:15:17,210 --> 01:15:20,190
so there were two products for the class of the classifier

1249
01:15:20,250 --> 01:15:25,410
there is some history distribution is the probability distribution list in that space the parameters

1250
01:15:25,410 --> 01:15:29,300
of the distribution and we came up with after seeing the data

1251
01:15:29,380 --> 01:15:31,800
metal has the maximum somewhere

1252
01:15:32,490 --> 01:15:35,800
now one thing we could do is we could maximize

1253
01:15:35,820 --> 01:15:38,120
that probability of way

1254
01:15:39,190 --> 01:15:42,580
that would give a single weight vector that we could use the prediction

1255
01:15:42,610 --> 01:15:44,060
so we could do that

1256
01:15:45,310 --> 01:15:48,850
that might look like a regularizer fit that we did before

1257
01:15:50,010 --> 01:15:53,760
we would come up with a particular class they the decision boundary and is dotted

1258
01:15:53,760 --> 01:15:56,470
line shows some sort of confidence interval that's where

1259
01:15:56,560 --> 01:16:00,310
b and they started lines are actually pretty certain what the classes

1260
01:16:00,800 --> 01:16:04,290
but of course we don't know

1261
01:16:04,330 --> 01:16:06,130
what the weight vector is

1262
01:16:06,140 --> 01:16:09,270
we've seen only a limited amount of data and that's not enough to pin down

1263
01:16:09,270 --> 01:16:12,340
what the true underlying decision boundary is

1264
01:16:14,600 --> 01:16:17,870
what we should do is what was on the slide we average over possible decision

1265
01:16:17,870 --> 01:16:21,360
boundaries may be the decision boundary points out like this

1266
01:16:21,370 --> 01:16:24,360
maybe point over like this we're not really sure

1267
01:16:24,500 --> 01:16:28,030
and if you average the predictions under all of those possibilities you end up with

1268
01:16:28,030 --> 01:16:29,460
this decision

1269
01:16:29,470 --> 01:16:32,270
the decision boundary is in a similar place

1270
01:16:34,060 --> 01:16:36,150
now when you draw

1271
01:16:36,190 --> 01:16:40,150
contour showing sort of where that region is beyond which you certain

1272
01:16:40,220 --> 01:16:42,190
they can because

1273
01:16:42,210 --> 01:16:45,590
it's averaging a whole bunch of lines

1274
01:16:45,620 --> 01:16:49,350
like that you end up with kit kat decisions

1275
01:16:49,370 --> 01:16:51,510
so the result of this is

1276
01:16:51,620 --> 01:16:54,060
near where you've seen data end up

1277
01:16:54,110 --> 01:16:55,280
pretty certain

1278
01:16:55,280 --> 01:16:57,880
these confidence bands are quite married

1279
01:16:57,900 --> 01:17:01,580
but it turns out that because the plane can vary a lot more out here

1280
01:17:01,590 --> 01:17:03,690
we have a decision boundary that

1281
01:17:03,730 --> 01:17:05,550
and very like this

1282
01:17:05,560 --> 01:17:09,080
you end up very uncertain away from the data

1283
01:17:09,090 --> 01:17:10,130
so now

1284
01:17:11,580 --> 01:17:12,800
i'm reading the check

1285
01:17:13,610 --> 01:17:17,490
i see some really weekly handwriting that sort of not really like anything that i

1286
01:17:17,490 --> 01:17:18,800
saw my training set

1287
01:17:18,830 --> 01:17:24,980
maybe the model notices that that input in x the representation of this wiki handwriting

1288
01:17:24,980 --> 01:17:26,670
isn't near anything else

1289
01:17:26,750 --> 01:17:28,160
and so

1290
01:17:28,190 --> 01:17:32,240
although i think that this might be in nine actually have no idea and i

1291
01:17:32,240 --> 01:17:37,310
should make a decision so this formalism gives you a natural way of dealing with

1292
01:17:37,310 --> 01:17:42,390
uncertainty and it uses the fact that you don't know these parameters to tell you

1293
01:17:42,390 --> 01:17:51,480
that you are not certain predictions you can make either

1294
01:17:56,020 --> 01:17:57,690
once you come up with this

1295
01:17:57,700 --> 01:18:00,700
the formalism you can then sort of look back in

1296
01:18:00,710 --> 01:18:03,530
my college connections with things that we've already seen

1297
01:18:03,540 --> 01:18:04,310
so a

1298
01:18:04,330 --> 01:18:07,610
that has to use this one is too

1299
01:18:07,620 --> 01:18:10,070
i tell you that this is a sensible thing to do and the other is

1300
01:18:10,070 --> 01:18:13,510
that it can tell you how you might do better and make better method

1301
01:18:13,560 --> 01:18:15,520
say same

1302
01:18:16,240 --> 01:18:21,220
i apologize for people on videolectures

1303
01:18:21,230 --> 01:18:24,600
companies like terms

1304
01:18:24,650 --> 01:18:26,640
we're solving

1305
01:18:26,660 --> 01:18:30,270
this problem we optimizing liveliness integral here

1306
01:18:30,270 --> 01:18:31,340
over the way

1307
01:18:31,410 --> 01:18:34,800
we don't know what the correct parameter vector so averaging

1308
01:18:34,810 --> 01:18:37,660
under the posterior distribution over the parameters

1309
01:18:37,710 --> 01:18:40,340
and that can be hard to do so may be something we could do is

1310
01:18:41,270 --> 01:18:43,260
well that maximizes thing

1311
01:18:43,270 --> 01:18:45,300
let's find the most probable way

1312
01:18:45,310 --> 01:18:49,280
that was one in the centre of the posterior that gave reasonable decision boundary

1313
01:18:49,280 --> 01:18:53,900
and just use that so if the averaging predictions of distribution will find the best

1314
01:18:53,900 --> 01:18:56,940
way and then predict using just the best

1315
01:18:57,000 --> 01:19:00,380
setting of the parameters

1316
01:19:00,410 --> 01:19:01,780
now if we do that

1317
01:19:01,810 --> 01:19:05,490
what we end up doing is we maximize the posterior distribution of the weights given

1318
01:19:05,490 --> 01:19:10,690
data and so we really maximize the log probability of the weights because again

1319
01:19:10,740 --> 01:19:12,470
probabilities get very small

1320
01:19:12,480 --> 01:19:17,520
a variety of reasons dealing with log probabilities numerically is a lot better

1321
01:19:17,530 --> 01:19:19,550
so the log probability of the way

1322
01:19:19,560 --> 01:19:21,020
it turns out to be

1323
01:19:21,030 --> 01:19:24,920
the log likelihood log probability of the data given the way

1324
01:19:24,950 --> 01:19:30,700
plus the logprior plus constant that you often do they need to work out

1325
01:19:30,720 --> 01:19:34,490
so now we have a log likelihood and the time

1326
01:19:34,490 --> 01:19:37,030
and so noise

1327
01:19:37,080 --> 01:19:41,400
four between those two options we have two which is somewhere between these things and

1328
01:19:41,400 --> 01:19:45,720
i guess that makes the science of engineering but you you have an expressive enough

1329
01:19:46,730 --> 01:19:52,360
b not too expensive to to to allow you to prove optimal inference

1330
01:19:52,380 --> 01:19:54,680
two of them in my area

1331
01:19:54,690 --> 01:19:57,160
people come up with a good models

1332
01:19:57,180 --> 01:19:58,770
but no runs

1333
01:19:58,780 --> 01:20:01,190
numerical optimisation

1334
01:20:01,270 --> 01:20:07,660
now some numerical optimisation problems to more in the most for the local minima

1335
01:20:07,680 --> 01:20:10,810
but i wanted to happen we want something it doesn't

1336
01:20:10,860 --> 01:20:13,450
probably doesn't fall into my comment

1337
01:20:13,470 --> 01:20:17,190
but that's what i mean by exact inference

1338
01:20:17,240 --> 01:20:21,840
obviously its relative to how much change

1339
01:20:21,880 --> 01:20:23,680
on the approximation

1340
01:20:23,700 --> 01:20:27,500
and that's what i call inside come inside of

1341
01:20:27,520 --> 01:20:30,350
getting the best of both worlds

1342
01:20:30,370 --> 01:20:32,580
so the graph matching

1343
01:20:32,600 --> 01:20:36,540
we show that the problem is equivalent to another problem which should not in be

1344
01:20:36,570 --> 01:20:42,200
this actually shows the front of the guys that it is it

1345
01:20:42,220 --> 01:20:46,020
they completely missus its exponential

1346
01:20:46,040 --> 01:20:53,790
thank you for coming to the point

1347
01:20:56,200 --> 01:21:01,260
we want to use graphical models to try and make it more tractable

1348
01:21:01,480 --> 01:21:04,350
well i think you know i'm talking about its

1349
01:21:04,420 --> 01:21:07,830
you same many examples i tried to give you earlier

1350
01:21:07,850 --> 01:21:11,080
you have the template trying to find the complex scenes

1351
01:21:11,140 --> 01:21:14,570
was the pattern

1352
01:21:15,170 --> 01:21:17,700
the problem is to find an optimal match

1353
01:21:17,750 --> 01:21:20,320
now words we're trying to find this

1354
01:21:20,350 --> 01:21:21,740
this matching function

1355
01:21:21,760 --> 01:21:22,950
that will

1356
01:21:24,720 --> 01:21:31,110
points in the main point in the range

1357
01:21:31,130 --> 01:21:32,950
and as

1358
01:21:32,960 --> 01:21:34,660
we are

1359
01:21:34,670 --> 01:21:40,060
typically in this talk today focusing particularly all measures pairwise

1360
01:21:40,080 --> 01:21:45,150
although we out out extensions to set particularly because we're interested in certain types of

1361
01:21:45,220 --> 01:21:47,700
geometric invariant matching

1362
01:21:47,720 --> 01:21:48,690
we have

1363
01:21:48,720 --> 01:21:54,830
higher order points like cost ratios points things this of course the whole points missus

1364
01:21:54,830 --> 01:21:59,230
four points and of course it's complexity

1365
01:21:59,240 --> 01:22:00,470
well first

1366
01:22:00,480 --> 01:22:01,710
good thing

1367
01:22:01,760 --> 01:22:06,770
about playing geometrically with graphs graphs embedded in some way

1368
01:22:06,820 --> 01:22:10,160
a metric space

1369
01:22:10,170 --> 01:22:13,260
that you can reduce

1370
01:22:14,740 --> 01:22:17,130
the matrix representations

1371
01:22:17,140 --> 01:22:19,670
of the data if you like

1372
01:22:19,690 --> 01:22:20,890
he calls

1373
01:22:20,910 --> 01:22:25,960
if we couldn't would have many more satellites in the sky wouldn't

1374
01:22:26,010 --> 01:22:29,740
and to the national try the notion of triangulations

1375
01:22:29,760 --> 01:22:34,030
is it enables us to have satellite communication

1376
01:22:34,050 --> 01:22:36,170
without having the full satellites

1377
01:22:36,190 --> 01:22:38,820
guys satellites because we know

1378
01:22:38,840 --> 01:22:41,330
if you have for example in the plane

1379
01:22:41,350 --> 01:22:46,540
if we have just the distance around here the distance around here

1380
01:22:46,590 --> 01:22:52,840
the sorry the distance from this point is uniquely defined by its distance from these

1381
01:22:52,840 --> 01:22:56,530
reports for all very point in the distance

1382
01:22:56,540 --> 01:22:59,400
to uniquely localized more

1383
01:22:59,450 --> 01:23:03,890
so that means that we move to encode

1384
01:23:04,310 --> 01:23:08,730
distance from every other point in the game

1385
01:23:08,750 --> 01:23:11,360
i can triangular kind of

1386
01:23:11,370 --> 01:23:16,390
so that we can increase the sparsity

1387
01:23:16,440 --> 01:23:19,990
the because it can make the graph much more sparse

1388
01:23:20,040 --> 01:23:21,370
so for example

1389
01:23:21,460 --> 01:23:25,770
here's is a graph on one two three four five six seven eight points

1390
01:23:25,790 --> 01:23:30,230
r men desert and this really is to one

1391
01:23:30,240 --> 01:23:34,800
one is that it has connections to fully to fully now everything

1392
01:23:34,870 --> 01:23:38,920
so this is very important very important result

1393
01:23:39,140 --> 01:23:40,890
we really only need

1394
01:23:40,900 --> 01:23:42,920
on the plane

1395
01:23:44,540 --> 01:23:50,100
triangular reference if you like to define everything

1396
01:23:52,000 --> 01:23:54,950
so we use is built on that

1397
01:24:03,570 --> 01:24:09,410
this in the sense that embeddable in image

1398
01:24:09,440 --> 01:24:13,810
metric space with distance metric many years is that

1399
01:24:13,940 --> 01:24:18,400
in the interest in the individual

1400
01:24:18,680 --> 01:24:23,860
when you get to be attributed graph matching and hidden markov random field stuff

1401
01:24:23,880 --> 01:24:28,480
not so much jigsaw accepted to pre-empt we're going to have to make a graph

1402
01:24:28,480 --> 01:24:29,710
is chordal

1403
01:24:29,770 --> 01:24:30,840
and you're right i

1404
01:24:31,980 --> 01:24:33,950
invited to be done to ensure

1405
01:24:33,970 --> 01:24:35,110
and that's all we don't

1406
01:24:35,110 --> 01:24:37,700
in the context of the points that i have

1407
01:24:37,700 --> 01:24:40,180
p so far

1408
01:24:40,200 --> 01:24:44,550
so it is not just by itself but in the context of the points i

1409
01:24:44,550 --> 01:24:45,640
have big so far

1410
01:24:47,740 --> 01:24:53,470
this in importance measured is characterised like like every sample distribution is characterized by by

1411
01:24:53,470 --> 01:24:55,360
the centre and with

1412
01:24:55,390 --> 01:24:58,700
and and so let's let's look into what that means

1413
01:24:58,700 --> 01:25:00,030
so again

1414
01:25:00,410 --> 01:25:04,590
the the little white one-dimensional problem that we have this is the function that we're

1415
01:25:04,590 --> 01:25:07,450
trying to integrate this is the

1416
01:25:07,530 --> 01:25:13,450
a PS space where we are drawing point four and so picture this sampling distribution

1417
01:25:13,470 --> 01:25:14,890
that a

1418
01:25:14,910 --> 01:25:17,160
most of the points here have

1419
01:25:17,200 --> 01:25:22,220
see already low importance and of course

1420
01:25:22,430 --> 01:25:24,490
star which is the

1421
01:25:24,490 --> 01:25:27,110
the single point

1422
01:25:27,590 --> 01:25:34,450
that that will minimize risk in the context of finding the best single tree of

1423
01:25:34,450 --> 01:25:39,760
the best single that that's what appears to be will mean

1424
01:25:39,780 --> 01:25:43,660
and so this template is usually centre around p star

1425
01:25:43,660 --> 01:25:48,510
and in this case we have a broad distribution so this is OK you you

1426
01:25:48,510 --> 01:25:49,800
are allowed to pick

1427
01:25:49,800 --> 01:25:54,930
poems from everywhere but mostly but if you pick points from the center those are

1428
01:25:54,930 --> 01:26:00,890
more important to get a good approximation to this function

1429
01:26:00,970 --> 01:26:07,930
and so if our sampling distribution is narrow like this narrow centred around the star

1430
01:26:07,930 --> 01:26:10,180
which is the best single

1431
01:26:11,360 --> 01:26:13,240
then you have

1432
01:26:13,260 --> 01:26:15,200
what is going on in sample of

1433
01:26:15,220 --> 01:26:19,930
many a strong base learners then we do you have an error

1434
01:26:20,260 --> 01:26:24,030
close to that of the best

1435
01:26:24,050 --> 01:26:25,340
on the other hand

1436
01:26:25,360 --> 01:26:29,780
if we have a wide distribution what like this we're going to have a diverse

1437
01:26:29,780 --> 01:26:36,760
and symbols the predictions are not highly correlated with each other

1438
01:26:39,280 --> 01:26:41,030
so here

1439
01:26:41,070 --> 01:26:49,320
in the narrow distribution case again they the friend that if talking about recent different

1440
01:26:49,320 --> 01:26:56,590
trees which gene similar highly correlated predictions so when to take that combination and you

1441
01:26:56,590 --> 01:27:02,010
know don't necessarily get an unexceptional improvement

1442
01:27:02,010 --> 01:27:06,990
on the other hand if we have a very simple with many weak

1443
01:27:08,050 --> 01:27:12,950
so again means the risk of of that particular

1444
01:27:12,970 --> 01:27:18,110
it's much higher than than the release of the single optimum one then we have

1445
01:27:18,140 --> 01:27:21,700
good performance and this is this is the

1446
01:27:22,410 --> 01:27:23,450
so there

1447
01:27:24,010 --> 01:27:25,640
the empirical

1448
01:27:26,120 --> 01:27:30,930
observe trade-off between this trend

1449
01:27:30,950 --> 01:27:32,510
all the

1450
01:27:32,530 --> 01:27:37,990
individual base learners and the correlation some of them can be understood here in the

1451
01:27:37,990 --> 01:27:49,510
context of their with of the sampling distribution

1452
01:27:53,880 --> 01:27:56,450
of course they optimally

1453
01:27:56,470 --> 01:28:02,260
the optimal sampling distribution depends on the unknown target functions so so far is a

1454
01:28:02,260 --> 01:28:04,680
little bit of theoretical thing

1455
01:28:04,680 --> 01:28:06,970
right so we can not really

1456
01:28:07,360 --> 01:28:13,760
get like analytical form from the sampling distribution but but what we're going to do

1457
01:28:13,760 --> 01:28:16,660
is to come up with on are packed

1458
01:28:16,680 --> 01:28:21,490
which is an artistic approximation to the process of sampling from that

1459
01:28:21,510 --> 01:28:24,700
the importance distribution

1460
01:28:25,430 --> 01:28:32,470
they do is take is based on a technique that is called perturbation samples which

1461
01:28:32,470 --> 01:28:38,950
is unknown three using monte carlo integration to simulate the process of growing from from

1462
01:28:38,950 --> 01:28:40,550
now on

1463
01:28:40,570 --> 01:28:46,180
they down on sampling distribution and what it is is so we're gonna

1464
01:28:50,120 --> 01:28:56,860
so let's see how perturbations sampling goes in the in the algorithm so remember we

1465
01:28:57,610 --> 01:29:03,370
we're going to build a simple in enough for what stagewise manner adding one symbol

1466
01:29:03,370 --> 01:29:06,030
at a time to the

1467
01:29:06,070 --> 01:29:08,320
problem right

1468
01:29:09,890 --> 01:29:15,010
this expression here if for a moment you ignore there were perturbed

1469
01:29:15,030 --> 01:29:17,740
it's your saying give me the best tree

1470
01:29:17,760 --> 01:29:22,470
one of the best known their work at this time right

1471
01:29:22,490 --> 01:29:24,030
so that that is

1472
01:29:24,030 --> 01:29:28,390
just with this expression is aimed at minimizing the expected loss which is the risk

1473
01:29:28,390 --> 01:29:29,880
of the

1474
01:29:29,970 --> 01:29:33,360
now so perturbation sample says

1475
01:29:33,380 --> 01:29:39,820
simulate the process of growing this point from the sampling distribution by perturbing an aspect

1476
01:29:39,820 --> 01:29:41,140
of the problem

1477
01:29:41,160 --> 01:29:47,590
and so in the context of an symbols perturb is a random modification so all

1478
01:29:48,140 --> 01:29:55,950
let's see what what can we perturb in in our situation we can perturb the

1479
01:29:55,950 --> 01:29:57,910
distribution here

1480
01:29:59,070 --> 01:30:02,740
so again whenever we drop point we can

1481
01:30:02,760 --> 01:30:09,030
somehow alter the data distribution for instance by re weighting the observations and that happens

1482
01:30:09,030 --> 01:30:11,300
to be what enables us

1483
01:30:11,340 --> 01:30:14,430
we can

1484
01:30:14,450 --> 01:30:16,860
mess around with the loss function

1485
01:30:16,890 --> 01:30:22,780
we can perturb the loss function and for instance by modifying doing something to eat

1486
01:30:22,780 --> 01:30:27,260
argument either randomly or deterministically

1487
01:30:27,320 --> 01:30:33,240
and so we can see that that as much as we for what goes on

1488
01:30:33,240 --> 01:30:35,990
in gradient boosting later does

1489
01:30:35,990 --> 01:30:38,200
and we'll talk about that

1490
01:30:38,290 --> 01:30:40,680
later on in the week

1491
01:30:42,040 --> 01:30:46,760
so one thing we could use for example is could look at a very very

1492
01:30:46,760 --> 01:30:49,250
simple cubic polynomial

1493
01:30:49,970 --> 01:30:51,470
our function x

1494
01:30:51,490 --> 01:30:54,330
is just a polynomial expansions

1495
01:30:54,350 --> 01:30:56,590
of of our

1496
01:30:56,950 --> 01:30:58,240
a lot of time

1497
01:30:58,950 --> 01:31:01,740
so now we have four parameters

1498
01:31:02,330 --> 01:31:07,070
to think about and more generally speaking we could use the case or the

1499
01:31:11,400 --> 01:31:16,240
results as far as least squares are concerned still hold

1500
01:31:17,110 --> 01:31:21,570
for the simple polynomial expansions matrixx

1501
01:31:21,600 --> 01:31:26,430
still of course consist of a column of ones and then each other column will

1502
01:31:26,430 --> 01:31:28,700
correspond to the

1503
01:31:28,700 --> 01:31:32,280
the polynomial ten that we've used up too

1504
01:31:32,300 --> 01:31:33,800
whatever case order

1505
01:31:34,020 --> 01:31:35,600
so no

1506
01:31:37,810 --> 01:31:39,470
our data matrix

1507
01:31:39,470 --> 01:31:45,150
the design matrix will be an n by t plus one dimensional matrix work is

1508
01:31:45,150 --> 01:31:46,020
obviously the

1509
01:31:46,080 --> 01:31:47,000
or death

1510
01:31:47,000 --> 01:31:49,750
all of the polynomial that we're using

1511
01:31:49,770 --> 01:31:52,500
so that means that the least squares

1512
01:31:52,510 --> 01:31:54,640
machinery still holds

1513
01:31:54,640 --> 01:31:55,410
well now

1514
01:31:55,750 --> 01:31:58,200
w hat

1515
01:31:58,210 --> 01:32:00,980
rather than being in the simple case to by one is not going to be

1516
01:32:01,290 --> 01:32:03,460
a t plus one

1517
01:32:03,510 --> 01:32:06,690
column vector

1518
01:32:06,780 --> 01:32:11,150
so i well let let's

1519
01:32:11,160 --> 01:32:13,250
let's look at

1520
01:32:13,270 --> 01:32:20,580
very highly nonlinear model of order k equals nine

1521
01:32:20,590 --> 01:32:23,080
and here is what we get

1522
01:32:23,090 --> 01:32:26,200
now it's

1523
01:32:26,240 --> 01:32:27,280
this model

1524
01:32:27,300 --> 01:32:31,660
nine sort of polynomials

1525
01:32:31,670 --> 01:32:39,710
but still captures the ongoing trend but also seems to capture these oscillations

1526
01:32:39,810 --> 01:32:42,320
the question is

1527
01:32:42,340 --> 01:32:44,540
is this a better model and not

1528
01:32:44,710 --> 01:32:47,080
the linear model

1529
01:32:47,130 --> 01:32:49,750
would you be prepared to go to the

1530
01:32:49,810 --> 01:32:54,400
the book is the diffusion the keys that's not european

1531
01:32:56,340 --> 01:33:00,740
i put some money on any predictions this ninth order polynomial

1532
01:33:00,760 --> 01:33:03,000
model would use

1533
01:33:03,010 --> 01:33:09,500
is this a better model

1534
01:33:09,540 --> 01:33:12,440
so much shaking their heads on what basis the

1535
01:33:12,440 --> 01:33:15,600
make that judgement

1536
01:33:16,470 --> 01:33:20,260
what we're going to do is we're not going to

1537
01:33:20,330 --> 01:33:24,060
but i'm going to do well potentially finish

1538
01:33:29,860 --> 01:33:33,920
ten fifty three OK so we could planet thing

1539
01:33:36,040 --> 01:33:41,560
to answer that question we need to look at the whole notion of generalizations

1540
01:33:42,510 --> 01:33:45,010
and i have one lecture

1541
01:33:45,510 --> 01:33:47,180
looking at how the

1542
01:33:47,180 --> 01:33:53,050
the loss function can be decomposed into bias and sort of residual

1543
01:33:53,330 --> 01:33:56,030
eight of terror

1544
01:33:56,080 --> 01:33:58,410
but what i'm not going to do is move on

1545
01:33:58,780 --> 01:33:59,990
and what time

1546
01:34:00,000 --> 01:34:03,060
this whole problem of creating models like this

1547
01:34:03,170 --> 01:34:06,690
from october probabilistic basis

1548
01:34:06,690 --> 01:34:10,740
so this has been rather

1549
01:34:10,750 --> 01:34:13,700
well one could argue it's been rather arbitrary

1550
01:34:13,740 --> 01:34:16,960
that we have used

1551
01:34:34,360 --> 01:34:37,560
OK so

1552
01:34:37,570 --> 01:34:39,480
we we started off

1553
01:34:39,480 --> 01:34:42,920
and we should we can identify a linear model

1554
01:34:42,940 --> 01:34:47,480
using this squared error loss empirical squared error loss

1555
01:34:47,530 --> 01:34:51,600
but that so that we can really do it because we cannot really judge

1556
01:34:51,610 --> 01:34:56,650
how good the model as a whole confident should be in the estimates that we've

1557
01:34:59,080 --> 01:35:02,180
what we're not going to do

1558
01:35:06,480 --> 01:35:11,710
is look at this problem of modelling

1559
01:35:11,730 --> 01:35:14,390
linear linear models

1560
01:35:14,440 --> 01:35:16,320
within a probabilistic framework

1561
01:35:16,340 --> 01:35:20,440
and you've all been exposed to that and the number of times that were raised

1562
01:35:20,440 --> 01:35:26,630
when you were asked if he knew about hidden markov models suggest that probabilistic view

1563
01:35:26,630 --> 01:35:29,360
is something which a lot of you are familiar with

1564
01:35:29,360 --> 01:35:32,510
so we're going to

1565
01:35:32,610 --> 01:35:35,570
look specifically the likelihood principle

1566
01:35:35,570 --> 01:35:37,710
and maximizing the likelihood

1567
01:35:37,730 --> 01:35:41,340
as a method of parameter estimation

1568
01:35:43,610 --> 01:35:46,420
but we also obtain by employing this

1569
01:35:46,670 --> 01:35:50,070
probabilistic perspective is we know

1570
01:35:50,110 --> 01:35:52,010
can i say this

1571
01:35:52,050 --> 01:35:57,120
the levels of certainty of confidence that we can place and our estimates and in

1572
01:35:57,120 --> 01:35:59,280
our subsequent productions

1573
01:36:00,050 --> 01:36:07,610
there's no moves is much more forward anyway that allows us to rationally consider the

1574
01:36:07,610 --> 01:36:09,400
validity of our models and the

1575
01:36:09,860 --> 01:36:14,880
the validity of the predictions that our model will make

1576
01:36:14,920 --> 01:36:20,300
so basically what we've done is we have

1577
01:36:20,300 --> 01:36:22,840
define the very simple model

1578
01:36:22,880 --> 01:36:26,960
potentially some linear some non linear function

1579
01:36:27,480 --> 01:36:29,230
over attributes

1580
01:36:30,210 --> 01:36:33,050
some set of parameters associated with this function

1581
01:36:35,110 --> 01:36:39,050
which maps two are are actual target values

1582
01:36:39,050 --> 01:36:40,840
and there's also some

1583
01:36:40,840 --> 01:36:44,320
and of ten and associated with this

1584
01:36:44,320 --> 01:36:45,900
some noise ten

1585
01:36:47,030 --> 01:36:50,320
the model is based on a deterministic function

1586
01:36:50,360 --> 01:36:53,260
all that outputs

1587
01:36:53,280 --> 01:36:54,780
so about

1588
01:36:54,780 --> 01:36:58,230
and that deterministic function

1589
01:36:58,280 --> 01:36:59,530
it is then

1590
01:36:59,550 --> 01:37:01,500
contaminated in some way

1591
01:37:01,510 --> 01:37:03,460
you can view this

1592
01:37:03,460 --> 01:37:06,000
the shapes

1593
01:37:06,110 --> 01:37:09,720
the problem is that large point clouds

1594
01:37:09,760 --> 01:37:10,960
we would like somehow how

1595
01:37:10,960 --> 01:37:11,770
to compare

1596
01:37:11,820 --> 01:37:14,000
so something much more compact

1597
01:37:14,010 --> 01:37:17,850
so the main problem i give you points and the point i want to know

1598
01:37:18,750 --> 01:37:20,430
just to point

1599
01:37:20,670 --> 01:37:22,540
how much overlap there

1600
01:37:22,670 --> 01:37:24,770
i don't want to look at all the point

1601
01:37:24,780 --> 01:37:28,630
OK take it on the sample well if you just take random samples from the

1602
01:37:28,630 --> 01:37:31,050
the blue you might not be consistently

1603
01:37:32,370 --> 01:37:37,520
i think it's hard to be consistently point from that the blue set

1604
01:37:37,570 --> 01:37:41,800
and this is a you like

1605
01:37:41,820 --> 01:37:43,080
call me actually

1606
01:37:43,140 --> 01:37:45,330
introduced by bruno

1607
01:37:45,350 --> 01:37:47,230
document similarity

1608
01:37:47,230 --> 01:37:51,000
which is to say we invoke especially around the next round of expertise nothing but

1609
01:37:54,220 --> 01:37:55,180
one of the

1610
01:37:55,190 --> 01:37:57,220
or you take up space in the blocks

1611
01:37:57,230 --> 01:37:59,740
he began to occasionally brought space

1612
01:37:59,770 --> 01:38:02,960
and you simply according to your preference for each

1613
01:38:02,970 --> 01:38:05,300
o point for that point according

1614
01:38:05,320 --> 01:38:08,390
the ordering of this voxels

1615
01:38:08,730 --> 01:38:12,660
so maybe this experts

1616
01:38:12,660 --> 01:38:15,980
fair the self

1617
01:38:16,980 --> 01:38:18,240
it yourself

1618
01:38:18,450 --> 01:38:21,740
the best and there's nothing there and fairness

1619
01:38:21,750 --> 01:38:27,040
the ones that it's not in their preferred to select how that point there fifty

1620
01:38:27,080 --> 01:38:31,790
seven that blue point and so on

1621
01:38:31,810 --> 01:38:34,910
so here is the best point according to the best

1622
01:38:34,910 --> 01:38:37,490
o point according to the fact that

1623
01:38:37,640 --> 01:38:41,880
he is best report according to sex

1624
01:38:41,990 --> 01:38:43,790
what about

1625
01:38:43,810 --> 01:38:46,000
and expert different underpin

1626
01:38:46,000 --> 01:38:47,500
the notion of voxels

1627
01:38:47,520 --> 01:38:49,390
and for the fact

1628
01:38:49,430 --> 01:38:50,740
the law

1629
01:38:50,800 --> 01:38:51,590
you know

1630
01:38:51,600 --> 01:38:53,400
this point here

1631
01:38:53,400 --> 01:38:54,380
he is best

1632
01:38:54,400 --> 01:38:56,130
before the blue

1633
01:38:57,450 --> 01:38:58,560
yes according to

1634
01:38:58,580 --> 01:39:00,460
how many of the random experts

1635
01:39:00,460 --> 01:39:01,700
the first choice

1636
01:39:01,820 --> 01:39:03,030
the same or not

1637
01:39:03,040 --> 01:39:05,740
part is the same or not

1638
01:39:05,750 --> 01:39:06,750
and it turns out

1639
01:39:06,790 --> 01:39:08,380
you it

1640
01:39:08,410 --> 01:39:11,710
estimate of how many of you have said

1641
01:39:11,950 --> 01:39:13,920
at the close by back

1642
01:39:14,010 --> 01:39:14,830
the ocean

1643
01:39:14,850 --> 01:39:19,490
so the line here is can

1644
01:39:20,560 --> 01:39:22,230
broken up into shingles

1645
01:39:22,310 --> 01:39:25,000
the computer signatures for signals

1646
01:39:25,050 --> 01:39:27,970
from that moment on compact descriptor and then

1647
01:39:28,000 --> 01:39:29,440
this idea

1648
01:39:29,450 --> 01:39:34,660
we were really very very compact we used a hundred random experts was selected the

1649
01:39:34,660 --> 01:39:42,210
descriptors of favourite actions in that sense they are missing

1650
01:39:42,230 --> 01:39:44,590
and here is some

1651
01:39:44,630 --> 01:39:48,940
so the results using based on comparing partial scans of his

1652
01:39:49,010 --> 01:39:52,040
using geometry

1653
01:39:52,090 --> 01:39:53,240
on the black

1654
01:39:53,250 --> 01:39:54,940
and this a fingerprint

1655
01:39:54,980 --> 01:39:56,190
on the right

1656
01:39:56,240 --> 01:39:59,340
this comparing large point clouds comparing

1657
01:39:59,360 --> 01:40:00,060
you know

1658
01:40:00,250 --> 01:40:04,510
a few sites k it's very close enough

1659
01:40:04,550 --> 01:40:07,300
the same but quite close

1660
01:40:07,320 --> 01:40:13,260
in one can use is also find a market features got together

1661
01:40:13,280 --> 01:40:18,750
aligning multiple times without ever going to get decide for example a discount

1662
01:40:21,070 --> 01:40:24,830
four the holes just by looking at this

1663
01:40:24,880 --> 01:40:25,490
and again

1664
01:40:25,500 --> 01:40:26,690
standard databases

1665
01:40:26,700 --> 01:40:28,480
clustering of shapes and so on

1666
01:40:28,500 --> 01:40:29,890
or based on this

1667
01:40:29,900 --> 01:40:31,550
they compact disc

1668
01:40:32,130 --> 01:40:33,320
so to wrap this up

1669
01:40:33,340 --> 01:40:35,770
he was that

1670
01:40:37,200 --> 01:40:39,670
decided not to look for

1671
01:40:39,690 --> 01:40:42,260
so the human significant features of shape

1672
01:40:42,320 --> 01:40:45,030
but let's think of the children at random

1673
01:40:45,040 --> 01:40:47,460
because if two shapes overlap enough

1674
01:40:48,740 --> 01:40:53,610
the probability of finding some randomly chosen feature that appears in what that is high

1675
01:40:53,840 --> 01:40:56,020
in the future

1676
01:40:56,050 --> 01:41:01,030
it's a very simple method the parameters you have

1677
01:41:01,100 --> 01:41:02,310
for the last

1678
01:41:02,470 --> 01:41:03,610
my last

1679
01:41:03,670 --> 01:41:04,610
i think that

1680
01:41:05,600 --> 01:41:08,950
the bottoms

1681
01:41:09,040 --> 01:41:10,310
this game

1682
01:41:10,330 --> 01:41:13,610
not so much for but actually

1683
01:41:13,660 --> 01:41:18,020
and the structure of the human themselves that is said to be

1684
01:41:18,080 --> 01:41:19,200
there are we

1685
01:41:19,240 --> 01:41:21,780
if if have windows so much

1686
01:41:21,830 --> 01:41:24,820
but if you you look at the task involved in this election

1687
01:41:24,830 --> 01:41:26,700
therefore the above

1688
01:41:26,710 --> 01:41:28,310
so for first of the building

1689
01:41:28,360 --> 01:41:30,390
like for me to be good

1690
01:41:30,390 --> 01:41:34,860
the question is can we detect the structure without knowing

1691
01:41:34,880 --> 01:41:36,430
what the problem is

1692
01:41:36,480 --> 01:41:38,680
well what the elements

1693
01:41:38,690 --> 01:41:39,670
and you

1694
01:41:39,680 --> 01:41:44,960
the but quickly and algorithm based on the stages again this is an will show

1695
01:41:45,010 --> 01:41:48,470
the aggregation state

1696
01:41:49,540 --> 01:41:53,150
so if stages we take the input

1697
01:41:53,160 --> 01:41:55,540
we analyse the contrast show you how

1698
01:41:56,330 --> 01:42:00,810
then based on that we estimate what be the bottom and then we go back

1699
01:42:00,810 --> 01:42:02,730
in time that

1700
01:42:04,140 --> 01:42:05,600
the human

1701
01:42:05,630 --> 01:42:09,500
like before start going on in the spatial domain

1702
01:42:09,540 --> 01:42:12,660
it's going on in the main body

1703
01:42:12,670 --> 01:42:15,740
close couple

1704
01:42:15,830 --> 01:42:17,900
in this work we discover

1705
01:42:17,900 --> 01:42:20,700
he said structure based on

1706
01:42:20,710 --> 01:42:24,230
the community guided subgroup consisting of

1707
01:42:24,240 --> 01:42:26,680
rotations translations scaling

1708
01:42:26,690 --> 01:42:29,560
show here is actually the only possible

1709
01:42:30,820 --> 01:42:33,430
that you see here

1710
01:42:33,430 --> 01:42:35,110
then the function value

1711
01:42:35,190 --> 01:42:37,360
is always less than the

1712
01:42:37,370 --> 01:42:42,250
the same linear combination applied to the function values of x y so the core

1713
01:42:42,250 --> 01:42:45,130
defined by these two points on the graph of the function

1714
01:42:45,150 --> 01:42:48,070
is above the graph of the

1715
01:42:48,080 --> 01:42:51,100
and that has to be true for all points x y in the

1716
01:42:51,350 --> 01:42:54,940
all possible points x and y

1717
01:42:54,960 --> 01:42:59,220
is actually another condition that sometimes important of the domain of the function of the

1718
01:43:02,610 --> 01:43:05,360
meaning it's defines the

1719
01:43:05,370 --> 01:43:08,780
a set of values of x for which is defined as a convex set and

1720
01:43:08,790 --> 01:43:11,780
if you pick two points on the domain then it satisfies

1721
01:43:12,000 --> 01:43:14,710
and concave means minors

1722
01:43:16,460 --> 01:43:21,970
so these are some simple examples of one variable we have a linear or affine

1723
01:43:21,970 --> 01:43:23,700
function exponential

1724
01:43:23,720 --> 01:43:25,590
is convex

1725
01:43:25,650 --> 01:43:32,110
powers are convex if for positive x if all size greater than one negative

1726
01:43:32,110 --> 01:43:36,530
for example one over x is convex for positive

1727
01:43:37,880 --> 01:43:39,800
or power of an absolute value

1728
01:43:41,190 --> 01:43:45,600
convex on all of are if he is greater than one

1729
01:43:45,670 --> 01:43:48,920
negative entropy is convex

1730
01:43:49,850 --> 01:43:52,810
the logarithm is concave

1731
01:43:56,420 --> 01:43:58,980
then i will also look at function

1732
01:43:59,290 --> 01:44:02,630
functions of obviously and functions of made

1733
01:44:04,580 --> 01:44:07,040
again any find function is convex

1734
01:44:07,060 --> 01:44:09,650
you can see from the definition

1735
01:44:09,770 --> 01:44:12,980
and general expression for the function of the fact that this

1736
01:44:12,980 --> 01:44:17,250
four matrix we use this station x is an m by n matrix

1737
01:44:17,340 --> 01:44:20,580
then the trace of the product it's politics

1738
01:44:20,630 --> 01:44:22,060
b is the channel

1739
01:44:22,190 --> 01:44:24,120
the station of anaphora

1740
01:44:25,830 --> 01:44:30,940
basically it is a complicated or compact notation

1741
01:44:30,960 --> 01:44:34,120
the just in the product of the next face if you look at the face

1742
01:44:34,120 --> 01:44:35,670
of it will take

1743
01:44:35,710 --> 01:44:41,310
then it just some of the products AIJ XIT overall change it's like taking the

1744
01:44:41,310 --> 01:44:45,900
inner product of factors over all the elements of

1745
01:44:45,940 --> 01:44:50,120
and we write it like this phase of a perfect because that's all

1746
01:44:50,170 --> 01:44:55,310
more compact but it basically means the same thing as the inner product of those

1747
01:44:57,560 --> 01:44:59,310
nor is always convex

1748
01:44:59,310 --> 01:45:03,080
i mean if if you know this is context on this property

1749
01:45:03,130 --> 01:45:06,480
and that's also true for matrix norms obviously so for example if you take the

1750
01:45:06,480 --> 01:45:08,980
maximum singular value norm of a matrix

1751
01:45:09,000 --> 01:45:13,500
then because norms always convex you know this is convex

1752
01:45:13,500 --> 01:45:17,520
although from this mission for example it's not obvious that the square root of the

1753
01:45:17,520 --> 01:45:19,770
maximise value of examples x

1754
01:45:19,790 --> 01:45:23,810
that's a convex function of the elements in x

1755
01:45:24,080 --> 01:45:27,020
it has to be a convex function because it is used as an normal

1756
01:45:27,040 --> 01:45:29,920
but it's not that easy to show it directly

1757
01:45:31,600 --> 01:45:32,750
from the definition

1758
01:45:38,980 --> 01:45:44,420
now it's very non-standard

1759
01:45:44,480 --> 01:45:48,730
if you have to start optimizing it's very non symmetrical

1760
01:45:48,940 --> 01:45:57,000
the same so maximizing we'll talk about minimizing convex functions

1761
01:45:57,020 --> 01:45:59,310
and that's easy

1762
01:45:59,310 --> 01:46:02,380
and that's a good to maximizing

1763
01:46:04,380 --> 01:46:08,940
minimizing concave functions actually extremely hard

1764
01:46:09,000 --> 01:46:12,080
that's the current maximizing

1765
01:46:12,190 --> 01:46:20,880
but then you switch quality of the minimize and maximize

1766
01:46:20,900 --> 01:46:26,650
so you can get actually very problems that look very similar and actually have a

1767
01:46:26,730 --> 01:46:31,650
completely different complexion on this very easy on

1768
01:46:31,670 --> 01:46:38,560
like finding closest point sets or in its order followed point that can be very

1769
01:46:41,400 --> 01:46:51,710
therefore differentiable functions you can actually so many convex functions that you can impact is

1770
01:46:51,710 --> 01:46:53,330
not differentiable

1771
01:46:53,350 --> 01:46:58,040
but did eventually gets a little more have occurred conditions are

1772
01:46:58,060 --> 01:47:01,830
actually better and especially this one it's function is twice differentiable

1773
01:47:01,850 --> 01:47:06,860
then the passion matrix is non negative semidefinite for all x in that the best

1774
01:47:07,920 --> 01:47:10,230
the true definition of convex function

1775
01:47:10,250 --> 01:47:14,290
it's always the easiest one to apply to show the physical

1776
01:47:14,290 --> 01:47:20,690
first find the derivative and prove that is positive semi here are often easier ways

1777
01:47:20,690 --> 01:47:22,600
to function is called

1778
01:47:22,670 --> 01:47:26,210
if the function is

1779
01:47:26,230 --> 01:47:31,040
there's also a condition that in terms of the first derivative it's also interesting

1780
01:47:31,100 --> 01:47:35,650
it says that if f is differentiable so the gradient is o point x the

1781
01:47:35,650 --> 01:47:37,650
gradient vector of first

1782
01:47:37,710 --> 01:47:40,130
and you always have this inequality

1783
01:47:40,170 --> 01:47:44,080
for any y and x in the domain

1784
01:47:44,120 --> 01:47:47,130
basically this means if you look at the right hand side here

1785
01:47:47,190 --> 01:47:52,080
and this is the first order taylor series expansion of f

1786
01:47:52,080 --> 01:47:53,460
around the point x

1787
01:47:53,480 --> 01:47:55,440
to be valid in f of x

1788
01:47:55,500 --> 01:47:57,380
if elevator gradients

1789
01:47:57,540 --> 01:48:01,540
of project and i give you a local linear approximation to the graph of the

1790
01:48:01,560 --> 01:48:04,980
function of y that's true in general for any

1791
01:48:06,480 --> 01:48:12,170
but for convex functions we actually have the property that the approximation

1792
01:48:12,190 --> 01:48:17,880
is always less than the function of lower bound on the function value everywhere

1793
01:48:17,900 --> 01:48:19,960
it's not just local

1794
01:48:20,000 --> 01:48:21,560
linear or local

1795
01:48:21,580 --> 01:48:26,250
good approximation to the function value around x it's also less than this for quite

1796
01:48:26,290 --> 01:48:29,060
a few lower bound on every

1797
01:48:29,170 --> 01:48:37,420
and that's not true for non convex functions that are convex functions potential through

1798
01:48:37,440 --> 01:48:40,040
it's actually one of the basic reasons why convex

1799
01:48:40,040 --> 01:48:45,480
usually really often in which we want to to organize the data saying

1800
01:48:45,490 --> 01:48:49,780
someway for applying then although kind of algorithms

1801
01:48:51,460 --> 01:48:54,770
so here i have an example of it's really

1802
01:48:55,420 --> 01:48:58,710
traditional example of a geyser

1803
01:48:59,130 --> 01:49:02,770
who is very popular because he the shots real from

1804
01:49:04,770 --> 01:49:08,290
we want to to understand if they're correlation putting

1805
01:49:09,090 --> 01:49:14,980
the time the eruptions last and the time between two

1806
01:49:16,790 --> 01:49:17,530
two eruptions

1807
01:49:19,550 --> 01:49:22,330
from doing density estimation we see that there is a

1808
01:49:22,700 --> 01:49:27,340
kind of to two patterns so if we here we have i think

1809
01:49:28,450 --> 01:49:35,810
time between two eruptions and he not here is how how long th eruption

1810
01:49:35,810 --> 01:49:38,300
last and here is the time between two eruptions

1811
01:49:38,450 --> 01:49:43,960
so a frenzy we see that there's two kind of of patterns in here

1812
01:49:44,640 --> 01:49:50,290
which i don't know can be used for modelizing this data

1813
01:49:51,420 --> 01:49:56,950
ok so what wants to show by this example is that most problems

1814
01:49:56,960 --> 01:50:00,140
in machine learning that the one i have shown

1815
01:50:00,730 --> 01:50:05,600
and the reformulating into curve or surface to be discovered

1816
01:50:06,400 --> 01:50:13,030
and that's it's usually modelize as system of equation

1817
01:50:13,420 --> 01:50:17,910
with unknowns to be solved which it's really easily don with

1818
01:50:17,920 --> 01:50:23,520
much as metrics much relation so that's that's what's is in larger

1819
01:50:23,530 --> 01:50:28,610
useful in machine the second thing that comes out from example

1820
01:50:28,620 --> 01:50:32,070
is that there's diverse sources of uncertainty

1821
01:50:32,700 --> 01:50:36,160
one comes from the fact that we have a limited amount of of data

1822
01:50:37,740 --> 01:50:42,160
and others can come from the noise in the misery muntz

1823
01:50:42,500 --> 01:50:46,880
and also of course of the randomness inherent to the observed

1824
01:50:46,890 --> 01:50:53,170
phenomena what is really a can be something run from happening there

1825
01:50:53,620 --> 01:50:57,150
so that's the second which i'm going to talk about tomorrow which

1826
01:50:57,160 --> 01:51:00,670
is probability theory we we again really

1827
01:51:01,890 --> 01:51:07,290
a basic step from a introduction to to probability

1828
01:51:09,030 --> 01:51:14,340
ok let's go to you are algebra so as has been already discussed

1829
01:51:14,350 --> 01:51:18,020
today there is a different kind of data vector we

1830
01:51:19,050 --> 01:51:22,580
most often or least what have been done moston machine learning

1831
01:51:22,950 --> 01:51:26,290
it's data are represented as vectors

1832
01:51:27,920 --> 01:51:32,590
so vectors which m features of course i think i didn't take the

1833
01:51:32,600 --> 01:51:35,320
same convention of is a bit because we didn't

1834
01:51:36,150 --> 01:51:42,310
so for me i have but n vectors of n components so the f

1835
01:51:44,570 --> 01:51:48,730
and so

1836
01:51:50,750 --> 01:51:55,830
de inner product toro which is in some condition don't in some cases

1837
01:51:55,840 --> 01:51:59,120
called look for the doors carpet that it's more is the same

1838
01:52:00,640 --> 01:52:05,600
the finance that so line time column

1839
01:52:07,320 --> 01:52:10,690
that's important then for matrix multiplication not that

1840
01:52:12,700 --> 01:52:16,600
two vectors are say to be orthogonal if there's carpet that

1841
01:52:16,870 --> 01:52:24,620
is zero and as calling was saying just a few minutes ago

1842
01:52:26,310 --> 01:52:32,040
it's as often interesting to to have a metric so in in a usual

1843
01:52:32,740 --> 01:52:37,060
he genspace like where matrix and vectors like

1844
01:52:37,770 --> 01:52:41,250
most of and it's we have norm which is

1845
01:52:41,650 --> 01:52:45,980
the square root of the sky are for two of the vector by itself

1846
01:52:46,830 --> 01:52:51,840
and usually the distance is defined with these norm as

1847
01:52:52,220 --> 01:52:55,360
or the norm of the difference of two vectors

1848
01:52:57,290 --> 01:53:01,340
and as you can see here in the definition of the distance in fact

1849
01:53:01,350 --> 01:53:04,040
there's scarpa looked of the two vectors

1850
01:53:04,500 --> 01:53:06,520
which is included and that's why

1851
01:53:08,560 --> 01:53:12,110
also we can see these there's carpet that the similarity or

1852
01:53:12,110 --> 01:53:16,290
dipole moment and you have to have to spend articles

1853
01:53:16,370 --> 01:53:19,360
some of these very general out one electron

1854
01:53:19,370 --> 01:53:20,310
good habits

1855
01:53:20,330 --> 01:53:25,730
dipole moment this direction and the in this direction and vectorial sum is zero

1856
01:53:25,770 --> 01:53:29,230
the net result is that most atoms and molecules

1857
01:53:29,260 --> 01:53:31,510
i have a dipole moments

1858
01:53:31,520 --> 01:53:34,140
which are either one bohr magneton

1859
01:53:34,150 --> 01:53:37,310
or two were making that is very common

1860
01:53:37,320 --> 01:53:39,650
and that's what i will need today

1861
01:53:39,710 --> 01:53:41,140
to discuss the issue

1862
01:53:41,150 --> 01:53:42,440
how strong

1863
01:53:42,450 --> 01:53:44,470
field we can create

1864
01:53:44,480 --> 01:53:47,060
if we align all those

1865
01:53:52,860 --> 01:53:55,280
the magnetic field that is produced

1866
01:53:56,820 --> 01:53:59,150
the material when i exposed to in

1867
01:53:59,200 --> 01:54:00,910
external field

1868
01:54:00,960 --> 01:54:03,510
magnetic field b

1869
01:54:03,570 --> 01:54:06,620
o is the vacuum field

1870
01:54:06,700 --> 01:54:10,630
that i can create a solenoid we will discuss the further today

1871
01:54:11,510 --> 01:54:13,830
the film which i will call b prime

1872
01:54:14,020 --> 01:54:17,260
which is that the magnetic field that is the result of the fact that we're

1873
01:54:17,260 --> 01:54:18,780
going to align

1874
01:54:18,840 --> 01:54:20,580
these dipoles

1875
01:54:20,600 --> 01:54:23,040
external field wants to align

1876
01:54:23,090 --> 01:54:27,290
these dipoles and the degree of success depends on the strength of the external field

1877
01:54:27,350 --> 01:54:29,980
and of course on the temperature of the temperature is low

1878
01:54:29,990 --> 01:54:33,060
it's easier to align them because there's less thermal

1879
01:54:36,990 --> 01:54:38,460
and that's a big if

1880
01:54:38,470 --> 01:54:41,570
they you will see why it's a big if

1881
01:54:41,580 --> 01:54:43,290
if b prime

1882
01:54:43,960 --> 01:54:46,740
is linearly proportional

1883
01:54:46,830 --> 01:54:49,180
to be vacuum

1884
01:54:49,260 --> 01:54:51,890
if that case

1885
01:54:51,970 --> 01:54:56,130
there you will see that there situation where that's not the case

1886
01:54:56,160 --> 01:54:59,260
then i can write down the b prime

1887
01:54:59,320 --> 01:55:02,010
the equals sign of and we call that

1888
01:55:02,070 --> 01:55:04,980
last lecture the magnetic susceptibility

1889
01:55:05,030 --> 01:55:06,720
time the vacuum

1890
01:55:06,760 --> 01:55:14,590
the linear proportionality constant if i can do that then of course be is also

1891
01:55:14,630 --> 01:55:16,770
proportional to be vacuous

1892
01:55:16,780 --> 01:55:20,370
because now i can write down the b is one class

1893
01:55:21,500 --> 01:55:24,100
and the vacuum

1894
01:55:24,140 --> 01:55:29,180
and that core that we write

1895
01:55:29,230 --> 01:55:32,660
and in fact

1896
01:55:32,680 --> 01:55:36,480
which is the equation that i started out with today

1897
01:55:36,580 --> 01:55:39,140
and so that only meaningful equation

1898
01:55:40,350 --> 01:55:43,640
the some of the alignment of all the dipoles

1899
01:55:43,710 --> 01:55:44,790
can be written

1900
01:55:44,810 --> 01:55:49,010
as being linearly proportional with the external field and this is is what i want

1901
01:55:49,010 --> 01:55:50,500
to explore today

1902
01:55:51,370 --> 01:55:53,960
more detail

1903
01:55:54,090 --> 01:55:56,370
paramagnetic materials

1904
01:55:56,460 --> 01:55:58,450
there is never any worry

1905
01:56:00,690 --> 01:56:03,360
the linearity doesn't hold

1906
01:56:03,410 --> 01:56:06,740
but with ferromagnetic material that is not the case

1907
01:56:06,740 --> 01:56:09,160
because the ferromagnetic material

1908
01:56:09,180 --> 01:56:12,830
it is relatively easy to align the dipoles because they

1909
01:56:12,860 --> 01:56:17,100
already group in domains as we discussed last time in domain split

1910
01:56:17,140 --> 01:56:18,790
in unison

1911
01:56:18,820 --> 01:56:20,770
it's always ferromagnetic materials

1912
01:56:20,790 --> 01:56:22,040
as you will see today

1913
01:56:22,060 --> 01:56:25,830
we can actually go into what we call saturation that all

1914
01:56:25,850 --> 01:56:27,710
the dipoles are

1915
01:56:27,740 --> 01:56:30,090
the lines in the same direction

1916
01:56:30,110 --> 01:56:31,680
now the question is

1917
01:56:31,690 --> 01:56:32,930
how strong

1918
01:56:32,970 --> 01:56:36,370
would that field b

1919
01:56:36,480 --> 01:56:38,280
but to make a rough calculation

1920
01:56:38,340 --> 01:56:40,350
but if you have a pretty good feeling

1921
01:56:40,410 --> 01:56:41,610
for the numbers

1922
01:56:41,610 --> 01:56:43,740
it depends on what material you have

1923
01:56:44,710 --> 01:56:46,070
using material

1924
01:56:46,080 --> 01:56:49,830
well by the magnetic dipole moment is two

1925
01:56:51,890 --> 01:56:53,150
so this is

1926
01:56:53,230 --> 01:56:56,030
i told you to one two or three i think one

1927
01:56:56,040 --> 01:57:00,360
for which it is too

1928
01:57:00,370 --> 01:57:02,190
i have them all lines

1929
01:57:02,200 --> 01:57:05,320
so i think the situation that all aligned

1930
01:57:05,330 --> 01:57:07,200
so here

1931
01:57:07,240 --> 01:57:09,480
the current going around

1932
01:57:09,490 --> 01:57:11,750
the nucleus is another one

1933
01:57:11,760 --> 01:57:13,110
here's another one

1934
01:57:13,120 --> 01:57:16,240
here's another one this is solid material

1935
01:57:16,240 --> 01:57:19,040
so these atoms and molecules

1936
01:57:19,100 --> 01:57:20,470
nicely packed

1937
01:57:20,480 --> 01:57:25,490
you see all these currents going around and all these magnetic dipole moment

1938
01:57:25,510 --> 01:57:27,750
nicely aligned

1939
01:57:27,960 --> 01:57:31,820
so the magnetic fields supporting each other

1940
01:57:31,830 --> 01:57:33,070
and the question now

1941
01:57:33,080 --> 01:57:36,810
is one of the magnetic field inside here

1942
01:57:36,850 --> 01:57:38,740
well that's an easy calculation

1943
01:57:38,780 --> 01:57:42,480
because this really looks like a solenoid like you have windings

1944
01:57:42,480 --> 01:57:44,910
you have occurred going around

1945
01:57:44,950 --> 01:57:47,260
you remember or should remember

1946
01:57:47,270 --> 01:57:49,990
if we have the solenoid

1947
01:57:50,000 --> 01:57:52,260
and we run a current through the solenoid

