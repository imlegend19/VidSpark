1
00:00:00,000 --> 00:00:00,750
and sometimes

2
00:00:01,250 --> 00:00:02,810
bringing in the ideas of intervention

3
00:00:04,460 --> 00:00:10,770
because if we can interpret it we can overlay on estimated factors in volatility matrices

4
00:00:10,770 --> 00:00:12,040
as we track them over time

5
00:00:12,710 --> 00:00:15,790
if we can overlay interpretation understanding link back

6
00:00:16,310 --> 00:00:20,270
the fundamentals in this case fundamentals in the market economic circumstances

7
00:00:20,770 --> 00:00:24,730
and that allows us to think about how defeat at learning into the next

8
00:00:25,610 --> 00:00:27,880
a period of time into into modifying the model

9
00:00:28,770 --> 00:00:30,500
to potentially improve forecasts

10
00:00:31,320 --> 00:00:35,730
and these these kinds of methods are being used now for as i say forever twenty five years and

11
00:00:36,270 --> 00:00:38,610
a substantial number of variables

12
00:00:39,440 --> 00:00:43,000
as components bigger models that are used in management investment

13
00:00:52,040 --> 00:00:52,770
what a my time

14
00:01:05,440 --> 00:01:06,060
well on

15
00:01:06,880 --> 00:01:07,860
everything i've said

16
00:01:10,380 --> 00:01:11,630
could've been set twenty years ago

17
00:01:12,060 --> 00:01:13,130
so there was the tutorial

18
00:01:14,710 --> 00:01:16,040
and now can fast forward a little bit

19
00:01:16,770 --> 00:01:18,580
and just touch on just a couple of vignettes

20
00:01:19,500 --> 00:01:21,380
on some some more recent areas

21
00:01:22,170 --> 00:01:22,940
in dynamic modelling

22
00:01:27,060 --> 00:01:27,730
no surprise

23
00:01:30,190 --> 00:01:32,380
datasets are getting bigger models are getting bigger

24
00:01:33,040 --> 00:01:36,900
parameter dimension is getting bigger boy if the parameter domain if the parameters are varying

25
00:01:36,900 --> 00:01:40,190
in time it all its infinite dimension by definition

26
00:01:42,500 --> 00:01:43,270
we need more structure

27
00:01:44,210 --> 00:01:48,360
if we're going to make sense bigger problems we need more highly structured models

28
00:01:48,980 --> 00:01:49,900
small modular models

29
00:01:51,060 --> 00:01:55,690
ann that's one dimensional sparsity we need sparsity in all its dimensions so these are

30
00:01:55,690 --> 00:01:59,130
really you know it's no surprise for anybody else these are the three hot buttons

31
00:02:01,500 --> 00:02:03,360
what i learned in tokyo

32
00:02:04,440 --> 00:02:05,380
at theare

33
00:02:05,730 --> 00:02:09,110
the wonderful workshop that some view attended in tokyo last week

34
00:02:10,020 --> 00:02:11,690
at the institute of statistical mathematics

35
00:02:12,980 --> 00:02:13,440
is that

36
00:02:14,340 --> 00:02:16,730
big data is the fourth paradigm in science

37
00:02:19,630 --> 00:02:20,130
i heard that

38
00:02:20,630 --> 00:02:22,560
from cherokee our

39
00:02:23,210 --> 00:02:24,170
in this presentation

40
00:02:25,840 --> 00:02:26,980
president of the institute

41
00:02:27,840 --> 00:02:30,440
and i had that from tommy goochie current president who's who

42
00:02:30,980 --> 00:02:31,520
here i think

43
00:02:33,880 --> 00:02:36,190
i don't think we're in the fourth paradigm yet

44
00:02:36,980 --> 00:02:38,540
the fourth paradigm is big data

45
00:02:39,520 --> 00:02:42,540
that's not a paradigm science it's what we do with the big data how we

46
00:02:42,940 --> 00:02:46,170
how we model and how we how we store it how we communicate it

47
00:02:46,610 --> 00:02:47,460
how we model it

48
00:02:48,340 --> 00:02:50,110
and use that is the fourth paradigm

49
00:02:52,560 --> 00:02:54,650
we're just opening the door when not not even

50
00:02:55,440 --> 00:02:55,690
and even

51
00:02:56,090 --> 00:02:58,770
starting to think about it had in store yet

52
00:02:59,810 --> 00:03:01,520
i was starting to think that most at

53
00:03:02,730 --> 00:03:03,940
so this is the fourth paradigm

54
00:03:06,730 --> 00:03:08,540
i'm gonna give you a few slides just a show

55
00:03:09,000 --> 00:03:14,540
just some snapshots of some other kinds of application areas where bayesian dynamic modeling

56
00:03:16,320 --> 00:03:18,170
dealing with bigger datasets

57
00:03:18,590 --> 00:03:20,040
bigger models bigger structure

58
00:03:20,540 --> 00:03:21,860
where the needfor where

59
00:03:22,290 --> 00:03:24,090
and sparsity and structure

60
00:03:24,610 --> 00:03:26,420
is ever more pressing so here

61
00:03:26,940 --> 00:03:31,520
i haven't talked about spatial structure so much in dynamic models is a context where spatial structure

62
00:03:32,060 --> 00:03:32,790
is fundamental

63
00:03:34,040 --> 00:03:40,290
so a high-rise satellite data where on a weekly basis if not a daily basis one is receiving

64
00:03:40,710 --> 00:03:43,520
on thousands of grid cells on a lattice around there

65
00:03:45,110 --> 00:03:49,630
various measures various aerosols in this case datasets these pictures come from

66
00:03:50,090 --> 00:03:51,090
carbon monoxide

67
00:03:51,690 --> 00:03:52,710
from various sources on

68
00:03:53,980 --> 00:03:55,420
i huge datasets

69
00:03:56,380 --> 00:03:59,840
really being gathered and uh uh an incredibly fast rate

70
00:04:01,400 --> 00:04:06,630
end available and accessible and the problems that have a bearing on a hard interesting questions

71
00:04:07,710 --> 00:04:10,250
so the kinds of dynamic models one is interested in here

72
00:04:11,310 --> 00:04:11,920
would then

73
00:04:12,820 --> 00:04:14,420
think about the danger has

74
00:04:15,900 --> 00:04:19,230
on a lattice in very high dimension are as a vectorised version

75
00:04:19,960 --> 00:04:23,170
in this context this is the context computer modeling

76
00:04:23,650 --> 00:04:28,230
work physical models of predicting fluxes sio from their from various sources

77
00:04:29,040 --> 00:04:31,750
and so that's if you like a set prior information

78
00:04:32,270 --> 00:04:36,480
that feeds in you can treat it in different ways we like to treat it as a covariate information

79
00:04:37,440 --> 00:04:38,420
because we don't believe it

80
00:04:39,000 --> 00:04:41,840
we want to understand it and how it relates to the data we actually see

81
00:04:42,210 --> 00:04:44,130
so will map through to the data through

82
00:04:44,690 --> 00:04:45,790
dynamic parameters

83
00:04:46,590 --> 00:04:50,790
and they will be modeled in various ways state space models random walks or other forms and models

84
00:04:51,500 --> 00:04:53,000
because time is a covariance

85
00:04:53,540 --> 00:04:56,690
andrew but lots of stuff that we don't have in this model we know has

86
00:04:56,690 --> 00:04:58,420
a role in influencing the data

87
00:04:59,840 --> 00:05:00,880
and then they other piece here

88
00:05:01,690 --> 00:05:04,170
is everything that's left apart from measurement error

89
00:05:05,420 --> 00:05:06,630
and that's a hugely

90
00:05:07,500 --> 00:05:09,440
this is a highly spatially structured problem

91
00:05:10,400 --> 00:05:14,710
and if you ignore that's a new u fit the model you find is a lot of spatial structure left

92
00:05:15,230 --> 00:05:17,060
so this is where there is a need for

93
00:05:17,750 --> 00:05:20,860
really highly structured and highly sparse models

94
00:05:22,520 --> 00:05:23,310
one one isn't

95
00:05:23,750 --> 00:05:28,190
going to be able to use guassian processes for spatial analysis on these kinds of scales

96
00:05:29,040 --> 00:05:31,040
without lots of approximations

97
00:05:31,670 --> 00:05:32,420
structure them

98
00:05:33,540 --> 00:05:38,290
and make them more sparse and turn them into really different models so i think different kinds of models

99
00:05:39,040 --> 00:05:40,320
graphically structured models

100
00:05:40,980 --> 00:05:44,590
markov random field models which have lots of conditional independencies in them

101
00:05:45,230 --> 00:05:50,210
as way to go there so integrating those kinds of models into into dynamic models

102
00:05:50,540 --> 00:05:51,900
is a big part a v

103
00:05:52,460 --> 00:05:54,150
aware in recent times

104
00:05:54,630 --> 00:05:55,610
based in dynamic model

105
00:05:56,520 --> 00:05:58,630
based in dynamic modeling has been going

106
00:06:01,400 --> 00:06:05,380
backing up to a more general context and thinking about this idea a sparsity than

107
00:06:05,690 --> 00:06:08,400
and again uh looking over thee

108
00:06:08,900 --> 00:06:13,710
thee the last two or three years and where things are at right now in terms of applications

109
00:06:14,310 --> 00:06:17,020
particularly in t in time series dynamic models

110
00:06:18,310 --> 00:06:23,000
on a generic model for a vector a vector time series with some predictive information

111
00:06:23,630 --> 00:06:27,170
andy all kinds of parameters that may or may not very in time

112
00:06:27,580 --> 00:06:31,270
i flip my volatility matrix now to to its inverse to its precision

113
00:06:32,520 --> 00:06:35,590
the natural parameter of the normal distribution zero mean normal

114
00:06:37,340 --> 00:06:39,560
her for reasons which will become obvious

115
00:06:40,320 --> 00:06:40,710
and if

116
00:06:41,130 --> 00:06:41,440
could be

117
00:06:41,980 --> 00:06:45,250
anyone in a number of things that could be something that we don't know that

118
00:06:45,250 --> 00:06:48,570
so for blogs maybe i should put this slide on the right we have a

119
00:06:49,570 --> 00:06:52,520
this is called a blog blocks the posts

120
00:06:52,520 --> 00:06:57,520
and each blog post has timestamp and hyperlinks

121
00:06:57,530 --> 00:07:01,550
right so this way because we have timestamps and we have hyperlinks we can go

122
00:07:01,550 --> 00:07:06,270
and follow hyperlinks to other posts that are older than this post in this way

123
00:07:06,270 --> 00:07:09,860
we can detect how how things were propagating

124
00:07:09,880 --> 00:07:15,100
because every time every postbox talks about something and links relevant sources and now we

125
00:07:15,100 --> 00:07:19,030
have the time stamp of the relevant source because you can follow the hyperlinks in

126
00:07:19,030 --> 00:07:22,120
time and did and get to get the

127
00:07:22,170 --> 00:07:23,460
get the cascades

128
00:07:24,600 --> 00:07:29,930
we're going to follow the hyperlink so here's a different picture of the blogosphere right

129
00:07:30,140 --> 00:07:31,570
the squares are blogs

130
00:07:31,580 --> 00:07:34,840
blogs are composed of posts in this post

131
00:07:34,850 --> 00:07:39,610
timestamp there are there are so we know the time between

132
00:07:40,140 --> 00:07:45,270
so there are hyperlinks between the posts and we know the time of the posts

133
00:07:45,270 --> 00:07:49,190
so we can go and follow these hyperlinks back in time to do to find

134
00:07:49,190 --> 00:07:54,050
how the propagation how the information was propagating so for example if this was something

135
00:07:54,050 --> 00:07:59,820
that i know the blog said that somebody referred to somebody else later referred to

136
00:07:59,820 --> 00:08:00,970
it and so on

137
00:08:00,980 --> 00:08:05,780
and this in this way we can follow o thing spreads and this now this

138
00:08:05,780 --> 00:08:11,500
propagation graph is called the information cascade arc escape and

139
00:08:11,540 --> 00:08:15,310
as i said the data we are using is this one year we were going

140
00:08:15,310 --> 00:08:19,750
forty five thousand blogs that gives us ten million posts and from this we identified

141
00:08:19,750 --> 00:08:22,350
three hundred fifty thousand cascades

142
00:08:22,400 --> 00:08:27,750
some of those reasons

143
00:08:27,820 --> 00:08:29,310
rather than for

144
00:08:29,420 --> 00:08:31,010
it is

145
00:08:31,040 --> 00:08:33,040
it is

146
00:08:34,960 --> 00:08:40,170
questions because this is what is

147
00:08:41,830 --> 00:08:45,450
but but this is this is the cleanest right i mean i don't have to

148
00:08:45,450 --> 00:08:49,750
do any so the reason that i want to do like topics i want to

149
00:08:49,750 --> 00:08:53,960
sort of fall topics right and one way to do this sort of measure the

150
00:08:53,960 --> 00:08:57,940
similarity between between posts and say all this because some of them are on the

151
00:08:57,940 --> 00:09:00,440
same topic but through hyperlinks right

152
00:09:00,440 --> 00:09:05,030
it's really obvious that the person who's refers to someone else and that someone else

153
00:09:05,030 --> 00:09:11,960
existed before they have to have some relevant reason for saying they said that

154
00:09:11,980 --> 00:09:13,750
most of

155
00:09:13,770 --> 00:09:16,700
son who was

156
00:09:19,990 --> 00:09:23,770
sure i i i could be missing a lot but this way

157
00:09:23,780 --> 00:09:27,310
it's very clean what's going on so that's why why we decided just to follow

158
00:09:27,310 --> 00:09:30,870
hyperlinks and of course there are other ways helping spread but then you get you

159
00:09:30,870 --> 00:09:35,550
can get in various say tracks on how to to deal with

160
00:09:35,580 --> 00:09:39,530
OK so this is this is blocked

161
00:09:39,540 --> 00:09:44,790
so the first thing is now we basically asked which commonly blogs in three which

162
00:09:44,790 --> 00:09:50,250
blogs should we read to to detect these big stories effective right and again so

163
00:09:50,250 --> 00:09:54,210
i'm plotting the number of blogs we select and this is the

164
00:09:54,620 --> 00:09:57,790
the population affected right so we want to be the first to know so we're

165
00:09:57,800 --> 00:10:02,830
asking how many people get infected after after after US right so we want to

166
00:10:02,830 --> 00:10:07,540
be the first to detect the story that's what you are optimizing and what you

167
00:10:07,540 --> 00:10:10,990
see from here is that this is this is the quality of our solution

168
00:10:12,340 --> 00:10:13,310
this would be

169
00:10:13,330 --> 00:10:14,420
and the

170
00:10:14,420 --> 00:10:19,240
the old traditional bounded by this tells us is that there on the solution the

171
00:10:19,240 --> 00:10:24,060
optimal solution that we can't compute is somewhere in between the red and blue and

172
00:10:24,060 --> 00:10:29,060
this is our new results which again which just says that no the optimal solution

173
00:10:29,060 --> 00:10:32,930
is in between here right so instead of thinking that we are i know thirty

174
00:10:32,930 --> 00:10:37,100
six o eight thirty six percent away from from the solution now we know that

175
00:10:37,100 --> 00:10:40,750
we only ten percent away from the solution so this this this this this is

176
00:10:40,750 --> 00:10:42,750
the same be thing much better

177
00:10:42,760 --> 00:10:43,950
if nothing else

178
00:10:43,970 --> 00:10:45,900
was run

179
00:10:45,930 --> 00:10:49,860
society decided to use it

180
00:10:54,840 --> 00:10:59,620
OK so so as i said instead of being is thinking that we have thirty

181
00:10:59,620 --> 00:11:02,530
seven percent the way you know we are at most t

182
00:11:02,530 --> 00:11:04,240
OK so this is good news

183
00:11:04,250 --> 00:11:09,240
the second thing is about using the cost right so for example if you say

184
00:11:09,520 --> 00:11:14,470
every got blocked because the same then we go and we are picking this like

185
00:11:14,520 --> 00:11:19,370
big big room summarizing blogs and what the result is that

186
00:11:19,390 --> 00:11:21,350
if you find now lot

187
00:11:21,400 --> 00:11:24,330
the cost in the number of posts here

188
00:11:24,380 --> 00:11:28,580
and just ignore the cost i get i get the following solution quality but if

189
00:11:28,580 --> 00:11:32,010
and if you believe that labels are produced from images

190
00:11:32,070 --> 00:11:34,810
so the label is conditionally independent of the

191
00:11:34,820 --> 00:11:39,850
stuff given the image then the right way to do machine learning to discriminate objects

192
00:11:39,850 --> 00:11:44,130
in images for example is to try and learn the function from images to labels

193
00:11:44,180 --> 00:11:46,370
but almost nobody believes that

194
00:11:46,370 --> 00:11:48,380
what you want to believe

195
00:11:48,430 --> 00:11:52,620
is that stuff creates images of the same stuff creates labels

196
00:11:52,630 --> 00:11:55,970
what's more the bandwidth between stuff in images is hard

197
00:11:55,990 --> 00:11:58,420
that is what show you name you can tell a lot about the stuff that

198
00:11:58,420 --> 00:12:01,630
created it but it is the dog you can tell me how big dog is

199
00:12:01,640 --> 00:12:05,420
more colour it is and what direction it is facing whether jumping up in the

200
00:12:05,490 --> 00:12:08,520
if i give you label like don't you can tell me hardly anything about the

201
00:12:08,530 --> 00:12:10,050
stuff that created it

202
00:12:10,070 --> 00:12:12,070
so that's a very low bandwidth

203
00:12:12,090 --> 00:12:16,400
so now if you want to get permission labels you completely crazy to try a

204
00:12:16,400 --> 00:12:18,160
direct mapping from installation

205
00:12:18,200 --> 00:12:19,400
what you want to do

206
00:12:19,410 --> 00:12:20,940
is learning

207
00:12:20,960 --> 00:12:25,600
how to get stuff from the image height the high bandwidth pathway because image specifies

208
00:12:25,610 --> 00:12:26,960
the stuff quite well

209
00:12:26,980 --> 00:12:31,320
once you've learned that then to get the label from the stuff and maybe slightly

210
00:12:31,320 --> 00:12:34,550
change the way in which you get stuff from images

211
00:12:34,760 --> 00:12:37,480
that's a different view of how you're going to do

212
00:12:37,500 --> 00:12:42,080
discriminative machine learning you first going to figure out what's going on and then you're

213
00:12:42,080 --> 00:12:45,910
going to do the learning and the interesting bit is figuring out what's going on

214
00:12:48,450 --> 00:12:50,460
so i want describe one application

215
00:12:50,470 --> 00:12:53,390
which demonstrates that approach works

216
00:12:53,470 --> 00:13:00,240
my contribution to this was to thoroughly indoctrinated one very smart student

217
00:13:00,250 --> 00:13:04,500
he met another student in your about speech and until about the timit phone recognition

218
00:13:05,320 --> 00:13:07,630
and he said OK what are used as input

219
00:13:07,690 --> 00:13:10,160
and and sort of simplifying this would be the best you can see here so

220
00:13:10,160 --> 00:13:12,520
they can correct it later

221
00:13:12,540 --> 00:13:17,570
use the standard input which is thirty nine coefficients for framing eleven frames and you

222
00:13:17,570 --> 00:13:19,860
have to predict the final label of the middle frame

223
00:13:19,900 --> 00:13:24,030
once you predicted those phone labels they can go into post process so the user

224
00:13:24,030 --> 00:13:27,270
by model of firms to predict what's really going on

225
00:13:27,290 --> 00:13:34,570
so the students said well why don't i take these thirty nine times eleven input

226
00:13:34,570 --> 00:13:37,290
variables and use two thousand units

227
00:13:37,350 --> 00:13:43,220
and that unsupervised and another two thousand ten units and amount supervised and another two

228
00:13:43,220 --> 00:13:46,010
thousand units around that unsupervised

229
00:13:46,020 --> 00:13:49,700
if you told me i children to stop there because then another two thousand units

230
00:13:49,740 --> 00:13:53,030
about unsupervised this companies units on top

231
00:13:53,040 --> 00:13:57,780
and from this hunt or learn all unsupervised and then go a hundred and eighty

232
00:13:57,780 --> 00:14:02,940
three context specific for labels which is very standard and back to fine-tune

233
00:14:02,960 --> 00:14:07,600
fine tuning takes a few days on a very fast GPU board

234
00:14:07,610 --> 00:14:11,610
but it turns out that just wipes out the record on the TIMIT recognition problem

235
00:14:11,620 --> 00:14:17,290
the previous record was twenty four point four percent and this gets twenty three percent

236
00:14:17,350 --> 00:14:22,370
more importantly all variations he tries along these lines get between twenty three and twenty

237
00:14:22,370 --> 00:14:25,670
four percent so there will be record

238
00:14:25,700 --> 00:14:27,280
and during

239
00:14:27,290 --> 00:14:32,270
as a student called honglak lee has recently applied similar techniques to the classification task

240
00:14:32,420 --> 00:14:36,080
where you're given the boundaries of the phone you told all these things are the

241
00:14:36,090 --> 00:14:37,840
same for can you classify it

242
00:14:37,850 --> 00:14:40,400
and he's round about the

243
00:14:40,410 --> 00:14:43,460
the record for the classification task although i think michael collins has something a little

244
00:14:43,460 --> 00:14:46,140
bit better

245
00:14:46,150 --> 00:14:48,040
so this is the trend

246
00:14:48,050 --> 00:14:53,340
it could be a benefit to the ideology you just take the input you have

247
00:14:53,340 --> 00:14:56,530
lots and lots of unless you need to big data set of course

248
00:14:56,630 --> 00:15:01,300
only the last letter weights is not pretrial and and that's why the penultimate then

249
00:15:01,300 --> 00:15:03,760
needs to be small to keep the number small

250
00:15:03,780 --> 00:15:07,220
and then you just train the hell out of it with backprop

251
00:15:07,240 --> 00:15:10,250
and it wins

252
00:15:10,260 --> 00:15:13,480
now in order to do that they had to be able to model the input

253
00:15:13,480 --> 00:15:17,140
which was real valued melchett from coefficients

254
00:15:17,190 --> 00:15:21,040
and so the both machine i told you about has binary visible units in binary

255
00:15:21,040 --> 00:15:23,740
hidden units you can advise that have

256
00:15:23,750 --> 00:15:29,910
real valued visible units and binary hidden units we sometimes call the gas in RBM

257
00:15:29,920 --> 00:15:34,660
you make a very naive assumption which is that given the states of the hidden

258
00:15:34,660 --> 00:15:39,830
units the visible units are conditionally independent so it's not good for data with high

259
00:15:41,370 --> 00:15:45,360
but the milk catching coefficients are designed to get rid of the covariance is so

260
00:15:45,360 --> 00:15:47,370
it's not about that later

261
00:15:47,390 --> 00:15:49,260
you can write an energy function

262
00:15:49,280 --> 00:15:50,450
the picture is better

263
00:15:50,460 --> 00:15:55,050
around each visible unit you put a parabolic containment function you say is energy to

264
00:15:55,050 --> 00:15:57,520
move away from where the visible unit would like to be

265
00:15:57,540 --> 00:16:00,470
and then the effect of the hidden units

266
00:16:00,480 --> 00:16:01,550
is two

267
00:16:01,570 --> 00:16:05,410
impose an energy gradient this trying to move it away

268
00:16:05,440 --> 00:16:09,000
and so the lowest energy point will be where the gradient of the blue line

269
00:16:09,000 --> 00:16:13,690
is equal to gradient into the red army and so just move the problem sideways

270
00:16:14,280 --> 00:16:19,090
that's the simple model and that's not a very good model data covariance is the

271
00:16:19,090 --> 00:16:23,510
end of the talk i come back come to much better

272
00:16:23,520 --> 00:16:25,360
so the new idea is that

273
00:16:25,370 --> 00:16:27,160
the basic module

274
00:16:27,170 --> 00:16:28,280
that has either

275
00:16:28,340 --> 00:16:33,670
binary or real valued visible units binary hidden units has something deeply wrong with which

276
00:16:33,670 --> 00:16:39,040
is it can't deal with multiplicative interactions and the world is full of multiplicative interactions

277
00:16:40,130 --> 00:16:45,090
freeman ten by a long time ago had a paper on style and content which

278
00:16:45,090 --> 00:16:47,190
is about a multiplicative interactions

279
00:16:47,820 --> 00:16:52,360
well all the contents of faces about multiplicative interactions

280
00:16:52,370 --> 00:16:56,880
whenever you get two guys in things in the world to get distributed very

281
00:16:56,890 --> 00:16:58,580
and you multiply them together

282
00:16:58,590 --> 00:17:03,120
you get a variable is very nongaussian distributed go very heavy tails and that's probably

283
00:17:03,120 --> 00:17:04,950
why the world full of heavy tailed things

284
00:17:04,970 --> 00:17:08,760
even if you believe in ghosts in distributed things

285
00:17:08,770 --> 00:17:13,450
i want to give you one example of why what is multiplicative interactions it's from

286
00:17:13,450 --> 00:17:17,480
a generative perspective if you're building generative models of multi layer and you want to

287
00:17:17,480 --> 00:17:19,370
generate some interesting data

288
00:17:19,380 --> 00:17:23,760
let's consider the problem of generating an image of the shape when i just tell

289
00:17:23,760 --> 00:17:25,300
you that this is square

290
00:17:25,320 --> 00:17:29,020
and it's got particular pose parameters where it is more orientations

291
00:17:29,800 --> 00:17:34,490
you could generate whether parts on the edges of the square very accurately if you

292
00:17:34,490 --> 00:17:36,120
had actually pose parameters

293
00:17:36,170 --> 00:17:39,740
and then maybe you can maintain the constraints between the parts

294
00:17:39,740 --> 00:17:42,760
but if you do any kind of sloppy generation of the parts

295
00:17:42,770 --> 00:17:46,590
then the ages were quite meet and the calls were quite the right angles and

296
00:17:46,590 --> 00:17:48,410
you get a very bad looking square

297
00:17:48,420 --> 00:17:50,620
so if user talk time graphical model

298
00:17:51,370 --> 00:17:57,080
basically getting modelling neutron generator me praying like this

299
00:17:57,100 --> 00:18:01,300
it's more likely configuration but they are allowed to generate something like this or like

300
00:18:02,120 --> 00:18:05,650
it unless you are very very accurate generation at each stage

301
00:18:05,670 --> 00:18:11,170
the alternative is too sloppy generation

302
00:18:11,190 --> 00:18:14,090
but also specify how to clean things up

303
00:18:14,100 --> 00:18:16,210
and that's a much better way to go

304
00:18:16,230 --> 00:18:18,300
so here's the picture

305
00:18:18,330 --> 00:18:23,950
you know the shape in the pose you do sloppy generation probably redundant parts

306
00:18:23,970 --> 00:18:27,000
and then you specify how the party should ensure

307
00:18:27,010 --> 00:18:28,770
and i that things settle down

308
00:18:28,780 --> 00:18:32,080
so in generation is going to be slow is going to settle and i get

309
00:18:32,080 --> 00:18:34,200
a very clean structure

310
00:18:34,200 --> 00:18:36,940
outer product of the vector x itself

311
00:18:36,960 --> 00:18:41,980
so it means for example that x matrix has to be rank one

312
00:18:41,980 --> 00:18:44,510
so it's nonlinear nonconvex

313
00:18:44,530 --> 00:18:48,440
it should be because it's still equivalent to the original problem

314
00:18:49,580 --> 00:18:54,460
so then the relaxation consistent actually simplifying this problem and making it convex by replacing

315
00:18:54,460 --> 00:18:57,870
the difficult constrained by weaker constraint

316
00:18:57,910 --> 00:19:01,920
i one way to do this is to replace the equality inequality

317
00:19:01,930 --> 00:19:05,340
so if you say x is greater than or equal to x x extensible suppose

318
00:19:05,370 --> 00:19:08,970
then that turns into a convex constraints

319
00:19:09,800 --> 00:19:13,490
nonlinear because of the product of x and x transpose but you can write it

320
00:19:13,490 --> 00:19:17,170
as an LMI by using should components

321
00:19:17,210 --> 00:19:20,220
so that's a very important but this is now a complex problem and you can

322
00:19:20,220 --> 00:19:24,890
solve it as a semidefinite programme so can solve very efficiently in polynomial time

323
00:19:24,900 --> 00:19:29,130
and that's several exhibition it's

324
00:19:29,130 --> 00:19:32,310
we replaced the feasible set of the original problem

325
00:19:32,340 --> 00:19:38,000
my large set because we we can loosen the constraint that we had here

326
00:19:39,000 --> 00:19:43,920
it's useful because this gives us a convex problem that will provide a lower bound

327
00:19:43,920 --> 00:19:44,550
on the

328
00:19:44,560 --> 00:19:47,190
the original value original optimal value

329
00:19:47,190 --> 00:19:52,670
because they optimize the same objective over a larger set of ten and lower bound

330
00:20:00,130 --> 00:20:02,980
so in general the also something

331
00:20:03,000 --> 00:20:04,070
and sort of

332
00:20:04,080 --> 00:20:08,460
later in the lecture so complexity of and is the p

333
00:20:08,470 --> 00:20:10,470
if you saw it using interior point method

334
00:20:10,490 --> 00:20:13,250
it's typically

335
00:20:13,270 --> 00:20:18,420
the worst-case polynomial bounds is depends on the dimensions and it's this kind of the

336
00:20:18,420 --> 00:20:21,020
order of the matrix inequality

337
00:20:21,030 --> 00:20:24,900
so in this case this will turn into matrix inequality of size n plus one

338
00:20:24,900 --> 00:20:26,210
if you like linearly

339
00:20:26,210 --> 00:20:28,380
and so we kind of

340
00:20:28,400 --> 00:20:33,200
that's a bound on the iterations an interior point method method

341
00:20:33,220 --> 00:20:36,320
and then you have to multiply with cost of one iteration

342
00:20:36,330 --> 00:20:40,530
linear algebra in one iteration of an interior point method and that will be a

343
00:20:40,530 --> 00:20:42,370
polynomial function of

344
00:20:42,370 --> 00:20:46,520
all the variables the number of people so that could be quite expensive but it's

345
00:20:46,520 --> 00:20:47,980
still polynomials

346
00:20:48,000 --> 00:20:51,220
because it's just the linear algebra of computing research directions

347
00:20:51,220 --> 00:20:53,690
and you can try to look for

348
00:20:53,760 --> 00:20:59,060
more efficient ways to do this by exploiting the structure

349
00:20:59,080 --> 00:21:04,910
in the world

350
00:21:05,040 --> 00:21:11,040
it's difficult immediately should be

351
00:21:11,090 --> 00:21:15,890
so the original set would be this this unit cube to the corners the feasible

352
00:21:17,070 --> 00:21:17,900
now we

353
00:21:17,920 --> 00:21:20,640
lifted this to higher dimensions

354
00:21:20,650 --> 00:21:21,570
so the

355
00:21:22,790 --> 00:21:23,830
the dr

356
00:21:24,210 --> 00:21:29,170
the in terms of x it would really be the projection

357
00:21:29,230 --> 00:21:32,180
of course the semidefinite cone in high dimension

358
00:21:33,230 --> 00:21:35,440
are and

359
00:21:35,480 --> 00:21:36,920
so it's difficult to

360
00:21:36,940 --> 00:21:38,190
people i think

361
00:21:38,210 --> 00:21:40,700
you can find some pictures and so

362
00:21:40,710 --> 00:21:41,990
papers book

363
00:21:42,020 --> 00:21:46,230
but is sort of a given a convex set that includes the

364
00:21:46,240 --> 00:21:49,800
the corners of this unit cube

365
00:21:49,910 --> 00:21:52,620
a really big because of this listing

366
00:21:52,640 --> 00:21:57,730
you it through the projection of the positive semidefinite cone in high dimensions

367
00:21:57,780 --> 00:22:00,070
projected onto x x space

368
00:22:04,090 --> 00:22:07,890
so the first use of this will be just to compute lower bounds

369
00:22:07,890 --> 00:22:11,480
by convex optimisation very NP hard problem

370
00:22:11,500 --> 00:22:17,010
the second one is if by accident at the optimum of this is the p

371
00:22:17,020 --> 00:22:20,000
x happens to be equal to x x ten suppose

372
00:22:20,040 --> 00:22:23,140
then you found the optimal actually

373
00:22:23,240 --> 00:22:26,980
back since we solve the in NP hard problem

374
00:22:27,940 --> 00:22:32,700
just to convex optimisation problem

375
00:22:32,700 --> 00:22:37,630
and then in jolly one b two of course because this is just a convex

376
00:22:37,630 --> 00:22:41,570
optimization problem then you can try to also try to use the answer of the

377
00:22:41,570 --> 00:22:43,510
optimal solution of this is the p

378
00:22:45,860 --> 00:22:53,030
suboptimal solutions for x might be doing some kind of around

379
00:22:53,080 --> 00:22:56,840
and one of the many ways of rounding x two

380
00:22:56,870 --> 00:23:00,380
plus or minus one because all we need to find a feasible point is effective

381
00:23:00,380 --> 00:23:02,740
vector it plus minus one entries

382
00:23:02,760 --> 00:23:10,030
the one method is suggested by this constraint because you can actually interpret this as

383
00:23:10,080 --> 00:23:14,000
the vector x in the matrix XS moments of the distribution because say x the

384
00:23:14,000 --> 00:23:16,730
vector is the mean of the distribution

385
00:23:16,780 --> 00:23:20,490
and then x would be the second moment the correlation matrix

386
00:23:24,120 --> 00:23:28,900
the mean and the second moment always that satisfy this inequality

387
00:23:28,940 --> 00:23:32,540
so that would be one interpretation of the solution is computed here maybe

388
00:23:32,560 --> 00:23:34,960
you can interpret it as a distribution

389
00:23:34,960 --> 00:23:39,260
and then generate random samples x from the distribution and random

390
00:23:39,290 --> 00:23:43,480
so that's called randomized rounding so the simple choice would be to use a normal

391
00:23:43,480 --> 00:23:45,570
distribution with this mean x

392
00:23:45,620 --> 00:23:47,990
covariance expires xx transpose

393
00:23:48,020 --> 00:23:50,760
generate random vectors from the distribution

394
00:23:50,870 --> 00:23:52,660
round two plus minus one

395
00:23:52,740 --> 00:23:56,380
and then maybe repeat is and keep the best ones

396
00:23:57,700 --> 00:23:59,910
so if you do this you get something like this

397
00:23:59,930 --> 00:24:03,060
so this is for a problem it's an equal to one hundred so it's certainly

398
00:24:03,060 --> 00:24:09,270
too many feasible point of possible points to consider by enumeration

399
00:24:09,330 --> 00:24:10,520
and this is the

400
00:24:10,810 --> 00:24:14,770
so the dashed line here is the is the people out the of the relaxation

401
00:24:14,790 --> 00:24:16,480
normalized to be one

402
00:24:16,510 --> 00:24:22,210
so that's the lower bound on the optimal value of this bullion optimisation problems

403
00:24:23,190 --> 00:24:25,640
the reality optimal value is exactly

404
00:24:25,730 --> 00:24:29,200
and this is an histogram of

405
00:24:29,220 --> 00:24:31,060
suboptimal solutions

406
00:24:31,080 --> 00:24:34,400
obtained by the randomized rounding using the solution

407
00:24:34,440 --> 00:24:35,780
of the SDP

408
00:24:37,650 --> 00:24:40,260
sample suboptimal values c

409
00:24:40,260 --> 00:24:41,060
but now

410
00:24:41,070 --> 00:24:44,340
i made this cancellation i can pass the limit

411
00:24:44,350 --> 00:24:47,440
and all that happens is a surface delta x equal to zero and i get

412
00:24:47,450 --> 00:24:49,380
minus one over here

413
00:24:50,750 --> 00:24:52,230
right so that's the answer

414
00:24:52,250 --> 00:24:59,310
so in other words what i've shown

415
00:24:59,350 --> 00:25:01,880
we put it up here is the last time

416
00:25:03,140 --> 00:25:04,860
is a minus one

417
00:25:13,540 --> 00:25:22,290
now let's let's look at the graph just a little bit to check this for

418
00:25:22,290 --> 00:25:25,290
plausibility right

419
00:25:25,330 --> 00:25:29,980
what's happening here is first of all it's negative

420
00:25:29,980 --> 00:25:32,300
i a it's less than zero

421
00:25:32,320 --> 00:25:36,800
which is a good thing you see that slope there is negative

422
00:25:36,820 --> 00:25:43,260
that's the simplest

423
00:25:43,310 --> 00:25:45,050
check if you could

424
00:25:45,070 --> 00:25:46,590
make and

425
00:25:46,610 --> 00:25:48,120
the second thing

426
00:25:48,170 --> 00:25:50,590
then i would just like to point out is that

427
00:25:51,540 --> 00:25:55,980
x goes to infinity it as we go farther to the right it gets less

428
00:25:55,980 --> 00:25:57,590
and less steep

429
00:26:01,200 --> 00:26:05,340
and what's xk it goes to infinity not zero

430
00:26:05,340 --> 00:26:08,770
sexier goes to infinity less and less

431
00:26:10,980 --> 00:26:16,240
so that's also consistent here is one the very large is smaller and smaller numbers

432
00:26:16,350 --> 00:26:25,810
in magnitude although it's always negative so slipping down

433
00:26:27,010 --> 00:26:31,040
so i managed to fill the board so maybe i should stop for question two

434
00:26:38,950 --> 00:26:43,610
so the question is to explain again

435
00:26:45,050 --> 00:26:47,170
limiting process

436
00:26:47,230 --> 00:26:51,580
so the formula here is we have basically two numbers

437
00:26:51,590 --> 00:26:55,090
so in other words why is it that this expression when delta x tends to

438
00:26:56,280 --> 00:26:58,850
is equal to minus one of squared

439
00:26:58,910 --> 00:27:03,340
let me let me illustrated by sticking in number for x zero to make it

440
00:27:03,340 --> 00:27:04,590
more explicit

441
00:27:04,620 --> 00:27:07,060
all right so for instance

442
00:27:07,100 --> 00:27:11,110
mister and here for x zero the number three

443
00:27:11,120 --> 00:27:13,700
and it's minus one over three

444
00:27:13,750 --> 00:27:17,320
what's delta x times three

445
00:27:17,410 --> 00:27:21,800
that's the situation that we've got another question is what happens is this number gets

446
00:27:21,800 --> 00:27:23,690
smaller and smaller and smaller

447
00:27:23,700 --> 00:27:25,840
and gets to be practically

448
00:27:27,390 --> 00:27:30,020
literally what we can do is just plug ins are there then you get three

449
00:27:30,020 --> 00:27:34,800
plus zero times three in the denominator minus one in the numerator so this tends

450
00:27:35,430 --> 00:27:41,040
tends to minus one over and over three square

451
00:27:41,060 --> 00:27:45,660
and that's what i'm saying in general with this with this extra

452
00:27:45,680 --> 00:27:50,560
number here other other questions

453
00:28:02,560 --> 00:28:06,860
the question is how what happened between this step

454
00:28:06,950 --> 00:28:10,690
and this step right it could explain this step here

455
00:28:10,700 --> 00:28:14,440
right so there were two parts to that the first is

456
00:28:15,340 --> 00:28:18,340
delta x was sitting in the denominator

457
00:28:18,350 --> 00:28:20,980
i that all the way out front

458
00:28:20,990 --> 00:28:24,610
so what's in the parentheses is supposed to be the same

459
00:28:24,630 --> 00:28:26,850
is what's in the numerator

460
00:28:26,890 --> 00:28:29,110
this other expression and then

461
00:28:29,160 --> 00:28:32,270
at the same time is doing that i put

462
00:28:32,280 --> 00:28:34,850
that expression which is the difference of two fractions

463
00:28:34,870 --> 00:28:39,440
express it with the common denominator so in the denominator using the product of the

464
00:28:40,740 --> 00:28:44,690
of the two fractions and then i just figured out the numerator had to be

465
00:28:44,710 --> 00:28:47,390
without really

466
00:28:47,430 --> 00:28:52,590
other questions

467
00:28:54,000 --> 00:28:56,590
so now

468
00:28:57,810 --> 00:28:59,740
so i claim

469
00:29:00,650 --> 00:29:02,510
on the whole

470
00:29:02,610 --> 00:29:06,250
calculus is gets a bad rap

471
00:29:06,250 --> 00:29:09,320
it's actually easier

472
00:29:09,330 --> 00:29:10,930
then then most things

473
00:29:10,950 --> 00:29:15,580
but it has there's a perception that it's that's

474
00:29:15,590 --> 00:29:16,900
it's harder

475
00:29:16,920 --> 00:29:20,950
and so i really have a duty to to give you the

476
00:29:21,000 --> 00:29:23,110
calculus made harder

477
00:29:23,130 --> 00:29:25,060
the story here so we

478
00:29:25,080 --> 00:29:28,440
we have to make things harder because that's that's our job

479
00:29:28,460 --> 00:29:32,540
and this is what actually what most people doing calculus and the reason why calculus

480
00:29:33,160 --> 00:29:35,330
that reputation so the

481
00:29:35,330 --> 00:29:36,550
the secret

482
00:29:36,590 --> 00:29:40,010
is that when people

483
00:29:40,060 --> 00:29:44,200
as problems in calculus they generally as in in context

484
00:29:44,260 --> 00:29:47,600
and there are many many other things going on

485
00:29:47,660 --> 00:29:52,140
so the little piece of the problem which is calculus is actually fairly routine

486
00:29:52,210 --> 00:29:55,550
and has to be isolated and gone through all the rest of the relies on

487
00:29:55,550 --> 00:29:56,960
everything else you weren't

488
00:29:57,010 --> 00:29:59,120
in mathematics up to the stage

489
00:29:59,140 --> 00:30:01,120
from grade school to

490
00:30:01,130 --> 00:30:03,060
through high school so

491
00:30:03,060 --> 00:30:04,140
so that's

492
00:30:04,170 --> 00:30:12,680
the complication so now we're going to do a little bit of calculus made hard

493
00:30:12,850 --> 00:30:17,800
by talking about word prob

494
00:30:17,810 --> 00:30:18,880
now we

495
00:30:18,930 --> 00:30:21,790
we only have one sort of work from that we can

496
00:30:23,870 --> 00:30:26,340
are we talked about is this geometry

497
00:30:26,380 --> 00:30:30,180
o point of view so so far those are the only kinds of word problems

498
00:30:30,180 --> 00:30:34,980
we can post so what we're going to do is just post prior to find

499
00:30:34,980 --> 00:30:36,420
the area

500
00:30:41,440 --> 00:30:46,520
and close

501
00:30:49,510 --> 00:30:51,570
the axon

502
00:30:55,040 --> 00:30:57,850
the tangent

503
00:31:00,570 --> 00:31:04,760
y equals one

504
00:31:05,750 --> 00:31:08,590
so that's the geometry problem

505
00:31:09,240 --> 00:31:11,460
let me drop picture of it

506
00:31:11,530 --> 00:31:16,130
it's practically the same as the picture for example one of course

507
00:31:16,190 --> 00:31:17,560
so here's

508
00:31:17,590 --> 00:31:19,790
we're only consider the first quadrant

509
00:31:19,830 --> 00:31:22,050
here's our shape

510
00:31:22,090 --> 00:31:23,520
right it's the

511
00:31:25,050 --> 00:31:27,610
here's maybe one of our tangent lines

512
00:31:27,640 --> 00:31:28,270
which is

513
00:31:28,280 --> 00:31:30,090
coming in like this

514
00:31:30,100 --> 00:31:31,300
and then

515
00:31:31,390 --> 00:31:37,500
we're trying to find this area here

516
00:31:37,510 --> 00:31:41,110
right so there's a problem so why does it have to do with calculus it

517
00:31:41,110 --> 00:31:45,030
has to do with calculus because there's a tangent line and so remained need to

518
00:31:45,030 --> 00:31:46,550
do some calculus

519
00:31:46,570 --> 00:31:48,980
to to answer this question

520
00:31:48,990 --> 00:31:51,190
but as you'll see

521
00:31:51,250 --> 00:31:53,950
the calculus is the easy part

522
00:31:55,510 --> 00:31:58,940
so let's get started with this problem

523
00:31:58,960 --> 00:32:00,330
first of all

524
00:32:00,330 --> 00:32:05,540
and the label of few things and one important thing to remember courses the curve

525
00:32:05,540 --> 00:32:09,460
thank you

526
00:32:18,810 --> 00:32:24,190
this is what you do this one

527
00:32:27,480 --> 00:32:31,520
this is

528
00:32:31,730 --> 00:32:33,850
this graph

529
00:32:33,860 --> 00:32:38,860
i can tell you is that due

530
00:32:43,040 --> 00:32:47,480
about this

531
00:33:33,810 --> 00:33:37,670
well one

532
00:33:52,460 --> 00:33:55,400
not really

533
00:34:02,940 --> 00:34:07,210
that sort of you know

534
00:34:14,540 --> 00:34:18,940
il gran

535
00:34:30,750 --> 00:34:34,730
it's not all ships

536
00:34:42,060 --> 00:34:43,650
we can do

537
00:34:49,980 --> 00:34:52,830
mean this

538
00:35:26,480 --> 00:35:32,150
so from

539
00:35:36,790 --> 00:35:40,060
so if you want to

540
00:35:47,480 --> 00:35:50,560
and so on

541
00:35:53,330 --> 00:35:55,270
the it

542
00:36:21,670 --> 00:36:24,210
and this is true

543
00:36:31,770 --> 00:36:32,960
it is not

544
00:36:34,060 --> 00:36:39,830
so when

545
00:36:42,000 --> 00:36:45,350
is it just me or is it

546
00:36:51,830 --> 00:36:56,650
it would be

547
00:36:58,330 --> 00:37:02,170
let me

548
00:37:02,230 --> 00:37:04,960
this is wrong

549
00:37:06,880 --> 00:37:08,670
the one

550
00:37:11,460 --> 00:37:13,940
is going to

551
00:37:32,500 --> 00:37:34,440
right to

552
00:37:34,440 --> 00:37:36,810
well to be

553
00:37:36,810 --> 00:37:37,920
the the

554
00:37:46,250 --> 00:37:51,540
so let me

555
00:37:51,560 --> 00:37:57,360
you could reach

556
00:38:02,360 --> 00:38:07,670
and so

557
00:38:12,580 --> 00:38:20,690
the school is

558
00:38:34,960 --> 00:38:39,360
we're not

559
00:38:47,880 --> 00:38:50,920
this species somewhere right

560
00:38:50,940 --> 00:38:55,210
along that route

561
00:38:55,230 --> 00:38:56,670
this species

562
00:39:15,920 --> 00:39:17,880
the problem

563
00:39:17,880 --> 00:39:20,740
i'm going to show the same blue curve on all these slides i'm gonna move

564
00:39:20,740 --> 00:39:25,300
the green curve around and that's all that changes the green curve showing the resulting

565
00:39:25,300 --> 00:39:28,360
red curves to the red curve is a deterministic function of the green curve in

566
00:39:28,800 --> 00:39:29,920
the car

567
00:39:29,930 --> 00:39:33,400
i'm just showing what is the what is the red approximation get out from different

568
00:39:33,400 --> 00:39:36,400
combinations of factor and context

569
00:39:36,450 --> 00:39:40,240
and this is the the property that that you want to have happen is that

570
00:39:40,380 --> 00:39:43,110
the context tells you where you need to be accurate into so the red curve

571
00:39:43,110 --> 00:39:47,280
should always be good in the neighbourhood of the context

572
00:39:47,280 --> 00:39:51,550
so this just showing you intuitively that has the baby we want so on the

573
00:39:51,550 --> 00:39:52,610
right-hand case

574
00:39:52,650 --> 00:39:57,110
it matches the the curve exactly want to match and what it does other than

575
00:39:57,110 --> 00:40:01,670
that sure about approximation away from the context but doesn't really matter because be multiplying

576
00:40:01,670 --> 00:40:03,970
by the context anyway

577
00:40:06,860 --> 00:40:11,200
here's an interesting case so the context is right here

578
00:40:11,260 --> 00:40:13,780
then we have to approximate this

579
00:40:13,800 --> 00:40:17,220
convex curving up part of the likelihood function

580
00:40:17,220 --> 00:40:18,840
and the only way to do that

581
00:40:18,860 --> 00:40:21,630
it is to use the galaxy in with the negative variance in this is what

582
00:40:21,630 --> 00:40:27,630
accounts negative looks like if you're wondering and as you can see these are very

583
00:40:27,630 --> 00:40:31,280
useful actually from modeling certain like that that so if you likely to with with

584
00:40:31,280 --> 00:40:36,030
the concave part you want to use constant winning inference model

585
00:40:37,470 --> 00:40:41,740
that's what happened here so we get very nice approximation that like

586
00:40:41,760 --> 00:40:44,030
in this in this region

587
00:40:44,050 --> 00:40:50,760
OK so if we make the context wider

588
00:40:50,820 --> 00:40:54,490
so before a very very narrow streets of the model one tiny part likely if

589
00:40:54,490 --> 00:40:57,780
the context is a bit wider meaning you have as much data

590
00:40:58,780 --> 00:41:02,280
then what you find is that it sort of it sort of takes more and

591
00:41:02,280 --> 00:41:07,030
more averaged approximation citizens isn't locally specific anymore

592
00:41:07,090 --> 00:41:09,150
so in this case

593
00:41:09,280 --> 00:41:12,420
we we still get negative variance but it's it's sort of tries to be good

594
00:41:12,420 --> 00:41:16,110
in bigger area in the early on the right

595
00:41:16,150 --> 00:41:17,840
and if we make the context

596
00:41:17,860 --> 00:41:19,220
very very wide

597
00:41:19,220 --> 00:41:22,800
then we end up which is sort of one global overall constant approximation which is

598
00:41:22,800 --> 00:41:26,450
what you would have expected in the beginning surviving but in say anything about what

599
00:41:26,450 --> 00:41:29,590
is going multiply with the best you can do it just to make one big

600
00:41:29,840 --> 00:41:34,490
moment last approximation

601
00:41:38,030 --> 00:41:39,920
we make use medical

602
00:41:41,780 --> 00:41:46,610
the reason there are some companies

603
00:41:52,010 --> 00:41:55,340
well the the

604
00:41:55,360 --> 00:41:56,650
OK so the question is

605
00:41:56,670 --> 00:42:01,820
how can we understand the air that were making the approximations so you could locally

606
00:42:01,820 --> 00:42:05,720
measure the air that you've made so after you've done after you made from the

607
00:42:05,720 --> 00:42:09,260
red curve you could measure the air to the curve

608
00:42:09,280 --> 00:42:11,820
and you could you could measure that when you can out of those approximations and

609
00:42:11,820 --> 00:42:14,690
some people have done so without estimating there

610
00:42:16,200 --> 00:42:18,800
that's certainly one thing to do

611
00:42:18,800 --> 00:42:23,670
but i but i think what really happens is you run a busy you start

612
00:42:23,670 --> 00:42:27,280
out with all of their approximations being very flat very broad and so you end

613
00:42:27,280 --> 00:42:32,360
up in this case so everyone to make sort of broad averaged approximation but as

614
00:42:32,360 --> 00:42:36,610
the as the algorithm iterates you get more and more information flowing in from all

615
00:42:36,610 --> 00:42:40,320
these other factors and you end up getting narrower and narrower context and you get

616
00:42:40,320 --> 00:42:42,970
more and more specific information

617
00:42:46,240 --> 00:42:50,320
what is it

618
00:42:50,360 --> 00:42:53,110
is the computational

619
00:42:53,110 --> 00:42:58,780
i mean look a there's a policy interpretation the probabilistic interpretation is

620
00:42:59,780 --> 00:43:02,240
if this blue curve was your prior

621
00:43:02,240 --> 00:43:06,220
and this so if the green curve was your prior x and the blue curve

622
00:43:06,220 --> 00:43:07,760
was likelihood

623
00:43:07,840 --> 00:43:12,860
then you're posterior distribution will have a bigger variance then your prior to

624
00:43:12,920 --> 00:43:15,610
that's essentially what this thread curve saying

625
00:43:15,630 --> 00:43:18,950
so saying you need to you actually need to reduce the variance

626
00:43:18,970 --> 00:43:22,170
of your of your prior and the posterior

627
00:43:22,220 --> 00:43:29,110
so you can interpret this approximation disgusting approximations is being essentially what would be the

628
00:43:29,110 --> 00:43:31,030
klingons and observation

629
00:43:31,050 --> 00:43:35,510
as the likelihood as you're likely it will be will be so the klingons in

630
00:43:35,510 --> 00:43:39,570
a thing that could observe and what this is saying is that the the closest

631
00:43:39,570 --> 00:43:43,700
equivalent in the gaussian observation one negative variance meaning one that actually makes you more

632
00:43:43,700 --> 00:43:48,110
uncertain observed that when one makes a lot more search

633
00:43:48,110 --> 00:43:50,860
and that certainly does happen in some cases

634
00:43:52,900 --> 00:43:54,800
i mean it is

635
00:43:55,070 --> 00:43:59,700
please go all the old man

636
00:44:02,720 --> 00:44:09,340
but now even more reasonable than to exclude the measurement

637
00:44:09,990 --> 00:44:12,110
so the difference

638
00:44:13,340 --> 00:44:15,610
given information about reasons for

639
00:44:16,900 --> 00:44:20,320
OK so the question is should we just should running out to converge is known

640
00:44:20,320 --> 00:44:23,970
throughout all of the messages that are negative variances and the answers i would say

641
00:44:23,970 --> 00:44:28,840
no because you want your posture to have the correct amount of uncertainty in form

642
00:44:28,860 --> 00:44:32,010
of your data point says that you should increase the amount of uncertainty then you

643
00:44:32,030 --> 00:44:35,740
do that you don't should just ignore it because that's what has do

644
00:44:35,760 --> 00:44:38,280
in particular this

645
00:44:38,300 --> 00:44:39,630
this approximation

646
00:44:39,650 --> 00:44:44,880
the reason that as negative right because you're likely does have that shape in that

647
00:44:44,880 --> 00:44:46,010
region and so

648
00:44:46,030 --> 00:44:47,170
just by

649
00:44:47,240 --> 00:44:49,920
just by throwing out is likely because the shape you don't like it it doesn't

650
00:44:49,920 --> 00:44:52,580
international resource sites

651
00:44:52,620 --> 00:44:57,450
or resource platform which are managing from liver with all team of

652
00:44:57,580 --> 00:45:02,050
european north american and japanese scholars in particular

653
00:45:02,160 --> 00:45:04,560
we tried to put together

654
00:45:04,620 --> 00:45:07,050
whatever is useful for q

655
00:45:07,060 --> 00:45:08,570
oriented scholars

656
00:45:08,970 --> 00:45:10,930
and you can find

657
00:45:10,970 --> 00:45:14,920
but i think we can have a look at this later on in the course

658
00:45:14,930 --> 00:45:17,150
including a

659
00:45:17,170 --> 00:45:18,950
exhaustive bibliography

660
00:45:19,020 --> 00:45:21,780
with about three thousand references

661
00:45:21,820 --> 00:45:26,600
and some recent publications and working papers

662
00:45:26,750 --> 00:45:28,320
and many other things

663
00:45:28,380 --> 00:45:30,380
so please have a look at

664
00:45:30,430 --> 00:45:32,990
comparison with three assists please

665
00:45:33,000 --> 00:45:37,440
but or if you type in combat with two axes dot org

666
00:45:37,550 --> 00:45:39,410
it is an american

667
00:45:41,310 --> 00:45:43,650
religious group or sect

668
00:45:43,770 --> 00:45:48,160
and you can pay a few thousand dollars and go to heaven

669
00:45:48,200 --> 00:45:55,150
that's not what we offer with compressed

670
00:45:56,650 --> 00:46:00,120
this is basically why we had to invent combats with three s right because was

671
00:46:00,120 --> 00:46:00,610
already the

672
00:46:00,950 --> 00:46:05,360
the main aim was already taken

673
00:46:05,420 --> 00:46:10,850
right there is also some news about the application about software developments

674
00:46:10,950 --> 00:46:13,820
and i want to stress that quite important that is going to be a new

675
00:46:13,820 --> 00:46:15,080
feature on the site

676
00:46:15,100 --> 00:46:17,450
it's just been launched

677
00:46:17,450 --> 00:46:21,790
and damien ball with my sister in law is here in this room

678
00:46:21,830 --> 00:46:24,530
yes answer recently a

679
00:46:24,550 --> 00:46:29,320
former section which i mean the section in which people can interact and share experiences

680
00:46:29,320 --> 00:46:30,450
and problems

681
00:46:30,530 --> 00:46:34,570
we might want to use the course to begin to feed four actually and make

682
00:46:34,570 --> 00:46:39,270
it available to the broader community

683
00:46:39,320 --> 00:46:43,180
all right i have said enough in terms of very general introduction

684
00:46:43,200 --> 00:46:47,770
and let me tell you what you see is about as my main point today

685
00:46:47,790 --> 00:46:53,430
to start with

686
00:46:53,490 --> 00:46:57,170
QCA which stands for qualitative comparative analysis

687
00:46:57,270 --> 00:47:00,270
as of label

688
00:47:00,330 --> 00:47:02,700
both methodology

689
00:47:02,700 --> 00:47:05,520
and the set of techniques

690
00:47:05,530 --> 00:47:07,380
by methodology i mean

691
00:47:07,440 --> 00:47:09,700
a way to envisage

692
00:47:09,810 --> 00:47:13,990
the dialogue as chosen put it between ideas and evidence

693
00:47:15,310 --> 00:47:16,990
theory and data

694
00:47:17,050 --> 00:47:21,170
four hypotheses and daytime evidence

695
00:47:21,180 --> 00:47:23,210
the empirical world

696
00:47:23,240 --> 00:47:26,180
also more about this later

697
00:47:26,220 --> 00:47:28,370
it's also said techniques

698
00:47:28,410 --> 00:47:29,710
data analysis

699
00:47:31,240 --> 00:47:33,200
data processing techniques

700
00:47:33,280 --> 00:47:35,730
i say the family

701
00:47:35,820 --> 00:47:38,250
because it's like also

702
00:47:38,290 --> 00:47:39,900
almost like paradigm

703
00:47:39,910 --> 00:47:42,830
because they're all there's a whole set of assumptions

704
00:47:42,880 --> 00:47:44,430
behind QCA

705
00:47:44,440 --> 00:47:50,590
which need to be taken on board before you enter the techniques

706
00:47:50,650 --> 00:47:52,960
what can be you

707
00:47:53,020 --> 00:47:56,170
we can use them for different purposes

708
00:47:56,200 --> 00:47:59,780
in very short i would say that the main use of these techniques

709
00:47:59,860 --> 00:48:04,280
is to test models or theories of course you've got many other

710
00:48:04,330 --> 00:48:06,350
needs to do that like

711
00:48:06,410 --> 00:48:10,070
mainstream quantitative techniques

712
00:48:12,000 --> 00:48:13,050
what is

713
00:48:13,060 --> 00:48:17,130
quite specific but uses first that you you can do this

714
00:48:17,190 --> 00:48:22,740
in a systematic way on the quite low number of cases

715
00:48:24,630 --> 00:48:26,060
will see

716
00:48:26,070 --> 00:48:31,580
in the course what we mean by this ready low number of cases

717
00:48:31,590 --> 00:48:32,690
what is also

718
00:48:32,700 --> 00:48:36,350
probably even more specific but

719
00:48:36,360 --> 00:48:38,290
is that this

720
00:48:38,420 --> 00:48:41,030
for the test series and

721
00:48:43,310 --> 00:48:46,180
remains of keeps

722
00:48:46,230 --> 00:48:49,200
core focus on complexity

723
00:48:49,210 --> 00:48:52,880
because usually when you use quantitative methods

724
00:48:52,940 --> 00:48:55,150
the main ones at least

725
00:48:55,230 --> 00:48:58,150
there is a price to be paid for

726
00:48:58,200 --> 00:49:00,330
when you test model of theories

727
00:49:00,450 --> 00:49:03,580
the price is simplification

728
00:49:03,660 --> 00:49:06,380
it is for instance when you regression analysis

729
00:49:07,410 --> 00:49:09,790
take out the outliers right

730
00:49:09,860 --> 00:49:12,550
we try to find a general tendency

731
00:49:12,590 --> 00:49:16,560
what would you say what you do is quite the contrary

732
00:49:16,600 --> 00:49:18,370
you keep the complexity

733
00:49:18,450 --> 00:49:21,200
within each case

734
00:49:21,250 --> 00:49:24,690
and you also keep the idea of diversity

735
00:49:25,350 --> 00:49:28,870
diversity goes along with complexity

736
00:49:28,880 --> 00:49:32,140
in particular we have one core

737
00:49:33,190 --> 00:49:37,500
or can recall multiple congenital causation

738
00:49:37,530 --> 00:49:40,020
what is the idea

739
00:49:40,030 --> 00:49:43,830
in a non-technical way the idea is that if you are an ionizing that's a

740
00:49:43,830 --> 00:49:46,570
twenty twenty five thirty cases

741
00:49:46,590 --> 00:49:50,900
and you want to explain some outcome of interest

742
00:49:50,980 --> 00:49:52,290
the outcome will be

743
00:49:52,300 --> 00:49:53,760
what you would call

744
00:49:53,830 --> 00:49:57,570
dependent variables right

745
00:49:57,580 --> 00:50:01,770
well it may well be that there might be given explanation for three four five

746
00:50:03,590 --> 00:50:04,830
that's one back

747
00:50:04,840 --> 00:50:09,640
link the outcome then you may want you may have another

748
00:50:09,650 --> 00:50:10,860
path for

749
00:50:10,870 --> 00:50:14,060
three or four other cases and so on and so forth

750
00:50:14,510 --> 00:50:15,650
each path

751
00:50:15,670 --> 00:50:18,020
has its own logic

752
00:50:18,050 --> 00:50:19,100
and each path

753
00:50:19,970 --> 00:50:21,330
consists of

754
00:50:21,330 --> 00:50:23,870
a different combination of factors

755
00:50:23,930 --> 00:50:25,830
that's what we mean by

756
00:50:25,830 --> 00:50:28,980
in very short by multiple contemporary causation

757
00:50:29,020 --> 00:50:30,330
there's much more to it

758
00:50:30,340 --> 00:50:32,570
but i cannot go into here

759
00:50:32,570 --> 00:50:33,250
this is

760
00:50:33,260 --> 00:50:37,620
technically qualified interest you

761
00:50:37,690 --> 00:50:40,600
while we are attempting to

762
00:50:40,650 --> 00:50:42,570
accomplish certain level

763
00:50:43,060 --> 00:50:48,170
of text in models and trying to keep view complexity

764
00:50:48,180 --> 00:50:52,220
so we think it's complexity complexity across the cases in terms of

765
00:50:52,220 --> 00:50:56,100
the diversity of causal patterns

766
00:50:56,100 --> 00:51:00,700
on it

767
00:51:00,720 --> 00:51:05,930
so the question is is that effectively because you can produce any moment this way

768
00:51:05,930 --> 00:51:07,930
and the answers effectively

769
00:51:10,330 --> 00:51:14,180
if you have a sufficiently rich class linearity is you can

770
00:51:14,180 --> 00:51:19,780
and that's exactly what i mean mappings are doing what we mapping into a space

771
00:51:19,800 --> 00:51:20,890
which we

772
00:51:20,890 --> 00:51:25,780
taxes are spent by linear functions of all the time and that's why every moment

773
00:51:26,530 --> 00:51:30,780
becomes i mean you want so

774
00:51:30,780 --> 00:51:32,870
that's what this theorem says

775
00:51:34,280 --> 00:51:36,390
now remember from the last slide

776
00:51:36,890 --> 00:51:40,280
this thing here looks familiar to us so we have some we've seen something like

777
00:51:41,330 --> 00:51:45,220
we know that this quantity year so this is not the supremum over all could

778
00:51:45,220 --> 00:51:49,810
all continuous functions it's the supremum over all functions in the unit ball of our

779
00:51:49,810 --> 00:51:54,490
approach is countable space but it's also the supremum over pretty large class of functions

780
00:51:54,530 --> 00:51:59,830
if outcome is sufficiently linearly so we have this supreme

781
00:51:59,870 --> 00:52:03,910
and we know the supremum simply corresponds to the distance of the means

782
00:52:03,930 --> 00:52:07,400
now let's look at this supremum is almost the same only that we have a

783
00:52:07,400 --> 00:52:11,060
different from cost so first

784
00:52:11,080 --> 00:52:12,560
let's use

785
00:52:12,620 --> 00:52:17,830
our quality from the last slide and actually we going to replace this function class

786
00:52:17,830 --> 00:52:23,180
by the unit ball in repertory company space special reproducing space we will which choose

787
00:52:23,180 --> 00:52:28,260
one which is dense in the set of continuous functions that means every continuous function

788
00:52:28,260 --> 00:52:31,260
can be approximated arbitrarily closely

789
00:52:31,260 --> 00:52:36,950
by a function from our can space

790
00:52:36,990 --> 00:52:40,780
it turns out we can also the the scaling here doesn't matter it's just the

791
00:52:40,800 --> 00:52:44,260
scaling factor so of course all

792
00:52:44,330 --> 00:52:52,010
functions you wanted link but it's sufficient if we can approximate every continuous function after

793
00:52:52,010 --> 00:52:56,120
scan scaling factors so that doesn't matter and the whole proof

794
00:52:56,140 --> 00:53:01,760
of the injectivity of all mapping can then be reduced to this classical result simply

795
00:53:01,760 --> 00:53:07,060
with this observation by choosing kernel which has this property

796
00:53:07,080 --> 00:53:12,010
but the reproducing kernel can space is dense in the set of continuous functions

797
00:53:12,010 --> 00:53:13,450
in such kernels

798
00:53:13,450 --> 00:53:21,510
i have been called universal before by who proved to results about approximation properties of

799
00:53:21,760 --> 00:53:27,720
support vector machines and consistency properties of support vector machines so he defined this class

800
00:53:27,720 --> 00:53:28,810
of kernels which

801
00:53:28,830 --> 00:53:33,800
this is very rich hilbert spaces where every continuous function can be approximated in terms

802
00:53:33,800 --> 00:53:37,510
of this class of kernels also does the job for us here i actually we

803
00:53:37,510 --> 00:53:42,540
can use a slightly larger one we noticed later but i don't want to confuse

804
00:53:43,780 --> 00:53:47,310
it turns out one example of such accounts goes into the ghost is always a

805
00:53:47,310 --> 00:53:51,310
nice example of the kernel and the final statement is that

806
00:53:51,310 --> 00:53:54,580
the vodka is universal in that sense

807
00:53:56,430 --> 00:54:02,850
the mapping is injective in other words two distributions input points of the mappings are

808
00:54:02,850 --> 00:54:05,220
identical if and only if

809
00:54:05,240 --> 00:54:11,280
the means of the distribution the expectations the output of the mapping is identical

810
00:54:11,350 --> 00:54:16,030
and this be identical is the same as that distance being zero because this is

811
00:54:16,030 --> 00:54:16,970
a norm

812
00:54:16,970 --> 00:54:20,720
OK so that's

813
00:54:20,780 --> 00:54:24,990
quite nice and maybe just as too short side note

814
00:54:25,010 --> 00:54:29,160
two observations

815
00:54:29,310 --> 00:54:34,580
the made with that have been exploited it and maybe that be interesting for someone

816
00:54:34,580 --> 00:54:38,780
to think about a bit more is

817
00:54:38,830 --> 00:54:45,040
this image here so the mapping being injective means is invertible on its image

818
00:54:45,100 --> 00:54:46,720
it doesn't have to be

819
00:54:46,720 --> 00:54:50,300
he surjective doesn't have to map to the whole space but whatever it maps to

820
00:54:50,760 --> 00:54:57,180
is invertible the the image of the map is something that's somehow called the marginal

821
00:54:58,430 --> 00:55:04,870
in the second observation is of the second note is that this whole concept of

822
00:55:04,870 --> 00:55:07,310
mapping distributions to

823
00:55:07,350 --> 00:55:09,280
their means

824
00:55:09,740 --> 00:55:15,930
sorry mapping distributions to the means of kernel functions

825
00:55:15,950 --> 00:55:21,700
under the distribution is a generalization of what statisticians call the moment generating function of

826
00:55:21,700 --> 00:55:26,470
a random variable so the moment generating function is defined like this so we take

827
00:55:26,470 --> 00:55:32,470
expectation with respect to the distribution of the random variable

828
00:55:32,660 --> 00:55:36,080
all of this special quantity here

829
00:55:36,100 --> 00:55:40,100
and it turns out that decisions they have because this is the moment generating function

830
00:55:40,100 --> 00:55:44,430
because from this thing you can compute all moments of the distribution of the random

831
00:55:46,580 --> 00:55:50,530
which is not surprising france because if we look at this we see are that's

832
00:55:50,580 --> 00:55:54,930
a special type of kernel it turns out this is a universal kernel so it

833
00:55:54,930 --> 00:55:56,510
makes sense that this

834
00:55:56,530 --> 00:55:59,510
these meaning it contains all the information that we can do it more generally we

835
00:55:59,510 --> 00:56:04,560
can use any universal kernel and you we construct other types of

836
00:56:04,580 --> 00:56:07,180
and we're generating functions

837
00:56:07,200 --> 00:56:11,370
so this is just as i note above the main point for us is that

838
00:56:11,410 --> 00:56:17,120
this thing gives us a convenient metric and probability distributions because now we know that

839
00:56:17,120 --> 00:56:20,470
these distributions are the same if and only if their distance in that space is

840
00:56:20,470 --> 00:56:21,350
the same

841
00:56:21,370 --> 00:56:27,040
so we can use this distance as a metric to to test whether distributions are

842
00:56:27,040 --> 00:56:29,470
somewhat different questions

843
00:56:31,040 --> 00:56:37,660
so the image not for example was

844
00:56:37,660 --> 00:56:40,700
yeah i mean positive measure should be no problem

845
00:56:47,470 --> 00:56:49,030
OK so

846
00:56:50,740 --> 00:56:52,410
it would be

847
00:56:52,470 --> 00:56:56,060
a good point to take a short break people want

848
00:56:56,080 --> 00:56:58,660
and then we'll try to meet again so

849
00:56:58,660 --> 00:57:01,180
so what is that transform

850
00:57:01,190 --> 00:57:04,590
several x depends on frequency

851
00:57:04,610 --> 00:57:08,210
little x depends on time

852
00:57:10,040 --> 00:57:12,870
these are the coefficients of

853
00:57:12,890 --> 00:57:14,450
two pi periodic

854
00:57:14,490 --> 00:57:18,240
series e to the minus side chain

855
00:57:22,580 --> 00:57:25,060
and i

856
00:57:25,080 --> 00:57:26,920
and all land in our

857
00:57:26,930 --> 00:57:30,700
my signals are infinite

858
00:57:32,740 --> 00:57:38,190
that is the discrete time for a transformer i started with a discrete time signal

859
00:57:38,200 --> 00:57:39,720
what kind of

860
00:57:39,750 --> 00:57:42,680
the result i got here what what

861
00:57:42,690 --> 00:57:45,590
what kind of an object is this

862
00:57:45,650 --> 00:57:48,580
fourier transform

863
00:57:48,680 --> 00:57:51,360
the function

864
00:57:51,370 --> 00:57:52,720
so all

865
00:57:52,790 --> 00:57:54,510
a function of frequency

866
00:57:54,530 --> 00:57:58,400
and what's the range of frequencies

867
00:57:59,720 --> 00:58:01,310
this thing is two pi

868
00:58:01,320 --> 00:58:05,400
it's has period two pi

869
00:58:05,410 --> 00:58:06,920
so i it

870
00:58:06,940 --> 00:58:11,900
normal take the range of frequencies b-minus pilot high

871
00:58:11,910 --> 00:58:13,720
so let me do that

872
00:58:13,780 --> 00:58:14,400
i mean

873
00:58:14,540 --> 00:58:22,200
so it makes the figures nicer if i go from minus from this frequency here

874
00:58:22,260 --> 00:58:25,130
pi to pi

875
00:58:26,010 --> 00:58:28,220
somehow what i wanna plot

876
00:58:29,190 --> 00:58:30,810
for this example

877
00:58:30,830 --> 00:58:33,870
is that maybe was there are somehow

878
00:58:34,970 --> 00:58:37,640
multiplayer the game the

879
00:58:37,690 --> 00:58:43,680
frequency response function was just one

880
00:58:43,740 --> 00:58:45,700
and i'm a was

881
00:58:45,760 --> 00:58:47,370
this is the amazing

882
00:58:47,470 --> 00:58:51,160
frequency but it's going to be periodic so

883
00:58:51,170 --> 00:58:54,000
this is the highest frequency

884
00:58:56,500 --> 00:58:58,510
everything happens on the

885
00:58:58,520 --> 00:58:59,560
two pi

886
00:59:02,060 --> 00:59:06,470
it's telling me that the frequency with the response to this frequency

887
00:59:06,570 --> 00:59:07,780
is there zero

888
00:59:07,790 --> 00:59:10,770
so maybe i used a lot

889
00:59:10,790 --> 00:59:19,550
plot those key points

890
00:59:19,570 --> 00:59:24,630
i'm talking here about low pass is going to be a low pass

891
00:59:24,650 --> 00:59:27,220
there's averaging

892
00:59:27,300 --> 00:59:30,490
is running at

893
00:59:30,500 --> 00:59:33,130
running and

894
00:59:34,270 --> 00:59:35,750
what's up

895
00:59:35,770 --> 00:59:39,010
at the other frequencies what what's

896
00:59:39,200 --> 00:59:41,380
what's the input

897
00:59:41,400 --> 00:59:44,080
and what's the output

898
00:59:44,110 --> 00:59:46,330
at other frequencies

899
00:59:46,340 --> 00:59:49,440
OK i'm looking for

900
00:59:49,460 --> 00:59:52,800
i guess i'm looking for and i can vector

901
00:59:52,820 --> 00:59:55,900
this was an organ vector religon value one

902
00:59:55,910 --> 00:59:57,280
i just came right up

903
00:59:57,310 --> 01:00:01,310
this was an idea vector with i value

904
01:00:02,560 --> 01:00:04,790
right is actually an old vector

905
01:00:04,900 --> 01:00:08,380
notice that this is telling us that that we

906
01:00:08,390 --> 01:00:10,300
cannot invert

907
01:00:10,320 --> 01:00:12,070
this filter

908
01:00:12,120 --> 01:00:13,790
this matrix

909
01:00:13,810 --> 01:00:17,080
this matrix now which is which is

910
01:00:17,090 --> 01:00:18,950
one half square league

911
01:00:19,090 --> 01:00:21,020
those one half there

912
01:00:22,630 --> 01:00:25,700
it's a very simple matrix but it isn't

913
01:00:25,720 --> 01:00:28,410
it's not invertible

914
01:00:28,450 --> 01:00:31,110
so what happens sitting on the diagonal

915
01:00:31,110 --> 01:00:33,810
all the way

916
01:00:33,840 --> 01:00:36,160
and one half is sitting

917
01:00:36,180 --> 01:00:38,740
below the that

918
01:00:38,750 --> 01:00:40,190
all the way

919
01:00:40,240 --> 01:00:41,910
otherwise there

920
01:00:42,420 --> 01:00:44,330
that's the main diagonal

921
01:00:44,340 --> 01:00:48,560
and that matrix doesn't hurt

922
01:00:48,570 --> 01:00:50,800
the bounded inverse because

923
01:00:50,820 --> 01:00:54,390
we found the vector of skills

924
01:00:56,890 --> 01:01:00,480
so what are the other one of the i can vectors

925
01:01:01,250 --> 01:01:03,440
i mean this sort of weird to be using

926
01:01:03,550 --> 01:01:08,170
we estimate i in vectors here but maybe that

927
01:01:08,210 --> 01:01:16,000
maybe the main point is that when we have constant coefficient linear operation

928
01:01:16,050 --> 01:01:19,550
the i can vectors are always the same guys

929
01:01:21,860 --> 01:01:24,210
and therefore that's the way the study

930
01:01:24,280 --> 01:01:28,170
every constant coefficient linear operation you have to study it

931
01:01:28,180 --> 01:01:31,400
because by those i can vectors which are always

932
01:01:32,460 --> 01:01:34,190
pure frequencies

933
01:01:34,210 --> 01:01:35,730
pure harmonics

934
01:01:35,740 --> 01:01:38,630
so let x

935
01:01:38,630 --> 01:01:41,300
omega of and

936
01:01:45,350 --> 01:01:47,200
this pure frequency

937
01:01:53,320 --> 01:01:55,920
so so this is the

938
01:01:55,950 --> 01:01:59,780
the there is

939
01:01:59,830 --> 01:02:01,810
there's the pure

940
01:02:01,820 --> 01:02:03,880
so these are the components of

941
01:02:05,690 --> 01:02:10,410
there are there some sort harmonic discrete harmonic because it's these

942
01:02:10,450 --> 01:02:13,410
and they were

943
01:02:14,160 --> 01:02:16,400
is discrete time

944
01:02:17,430 --> 01:02:18,670
so that's

945
01:02:18,670 --> 01:02:21,630
so this was the main equals zero

946
01:02:21,670 --> 01:02:24,620
that all the components were one

947
01:02:24,630 --> 01:02:28,710
this was amazing part what we're all water all the components of this thing on

948
01:02:28,710 --> 01:02:30,820
omega is pi

949
01:02:30,980 --> 01:02:34,790
you see that this is the right thing for me is pi one one of

950
01:02:34,790 --> 01:02:36,300
i got there

951
01:02:36,350 --> 01:02:37,830
link is pi

952
01:02:37,910 --> 01:02:45,160
he the i omega and is what

953
01:02:45,170 --> 01:02:51,750
for filmmakers pi i have either applied which is

954
01:02:51,760 --> 01:02:55,020
which is minus one to the

955
01:02:55,030 --> 01:02:58,990
which is exactly are alternating guy

956
01:02:59,000 --> 01:03:05,330
and in between for these other magazine between may is we've got well but

957
01:03:05,370 --> 01:03:11,160
a little problem here what's my little problem on the graph

958
01:03:11,200 --> 01:03:14,170
i've got complex numbers

959
01:03:15,480 --> 01:03:16,980
for these two

960
01:03:16,990 --> 01:03:22,670
cases all ones and pure alternating then i got real numbers so i started off

961
01:03:22,670 --> 01:03:26,430
with this graph so happily but yes

962
01:03:28,410 --> 01:03:34,520
well that's what i have to that's what i have to tell you now i

963
01:03:34,520 --> 01:03:39,640
have to you know it's fine it's exactly what so i graph

964
01:03:39,640 --> 01:03:45,230
these these why what comes out is is are

965
01:03:45,250 --> 01:03:48,350
i sensible and

966
01:03:48,400 --> 01:03:52,350
OK so we've heard so many times before yesterday the marginal likelihood on the upper

967
01:04:01,420 --> 01:04:03,680
now let's start with

968
01:04:04,480 --> 01:04:07,350
the gas distribution so it

969
01:04:07,370 --> 01:04:09,060
in the talk today

970
01:04:09,070 --> 01:04:12,060
the manipulating out in distribution all the time

971
01:04:13,500 --> 01:04:16,130
gas distribution

972
01:04:16,180 --> 01:04:18,900
in one dimension that regions of little

973
01:04:19,510 --> 01:04:26,170
but like this has location has a certain which is distribution in which the one

974
01:04:26,390 --> 01:04:28,510
june two apples here

975
01:04:28,590 --> 01:04:34,990
in two dimensions i can show the content part of this is the the the

976
01:04:35,000 --> 01:04:40,280
joint distribution in in the two dimensional space and the lines cross here is indicated

977
01:04:40,290 --> 01:04:41,040
that the

978
01:04:41,090 --> 01:04:44,740
the mean of the distribution and the contour lines

979
01:04:44,790 --> 01:04:49,200
are equal probability contours using a particular value of the

980
01:04:49,480 --> 01:04:55,150
of the probability of drawing a line with that

981
01:04:56,040 --> 01:04:58,920
so what we talk about what i'm going to use that they are

982
01:04:58,930 --> 01:05:02,900
much higher dimensional graphs here but i can draw the i can only draw three-dimensional

983
01:05:03,960 --> 01:05:06,110
we have to sort of imagine

984
01:05:06,130 --> 01:05:12,740
these sort of things in much higher dimension than the and here's the

985
01:05:13,000 --> 01:05:15,500
expression for thousand going to use the notation

986
01:05:15,550 --> 01:05:19,780
and four girls as mean

987
01:05:19,830 --> 01:05:23,280
that here and the covariance matrix doing things

988
01:05:23,290 --> 01:05:26,680
this is the actual expression and so it e to the minus

989
01:05:26,760 --> 01:05:30,340
the distance between x and the mean i'm regret

990
01:05:30,710 --> 01:05:33,620
what i mean

991
01:05:33,670 --> 01:05:37,130
so the mean vector in this part mean vectors the

992
01:05:37,150 --> 01:05:39,210
in the past there

993
01:05:39,230 --> 01:05:40,280
and the

994
01:05:40,300 --> 01:05:42,180
the covariance matrix

995
01:05:42,230 --> 01:05:45,110
actually gives you the

996
01:05:45,130 --> 01:05:47,240
something about what the

997
01:05:47,280 --> 01:05:50,560
directions of their in particular

998
01:05:51,290 --> 01:05:55,680
i mean values of the covariance matrix of the square of the

999
01:05:55,730 --> 01:05:57,110
of the standard deviation

1000
01:05:57,200 --> 01:05:59,930
in the in the direction of the idea

1001
01:05:59,950 --> 01:06:06,780
that means the matrix find what the thing looks like

1002
01:06:06,790 --> 01:06:10,270
all right so i hope seen

1003
01:06:10,290 --> 01:06:11,490
now before

1004
01:06:11,670 --> 01:06:14,960
so the two things i need to the two ways only to manipulate got in

1005
01:06:15,020 --> 01:06:18,820
the i need to do conditioning of gaussians

1006
01:06:18,830 --> 01:06:25,360
and i need to do much of missionary couple to take a multi dimensional gaussians

1007
01:06:25,720 --> 01:06:28,870
with the government in two dimensions again

1008
01:06:28,890 --> 01:06:30,360
now i condition

1009
01:06:31,400 --> 01:06:33,480
on the on the

1010
01:06:33,490 --> 01:06:39,150
on this variable having one particular value and the value given by the blue line

1011
01:06:39,200 --> 01:06:40,740
because there well no i want to

1012
01:06:40,750 --> 01:06:43,520
look at what the conditional distribution of the

1013
01:06:43,690 --> 01:06:47,530
one variable given a particular value for the other one

1014
01:06:47,550 --> 01:06:50,800
i think get that by by like slicing through the

1015
01:06:51,500 --> 01:06:52,920
probability distribution

1016
01:06:52,940 --> 01:06:56,770
the nice thing about got in the that the conditional distribution for

1017
01:06:56,860 --> 01:06:58,620
are again

1018
01:06:59,290 --> 01:07:04,990
so in this case conditioning on that particular value which is out which has a

1019
01:07:06,240 --> 01:07:07,440
OK here

1020
01:07:08,020 --> 01:07:09,740
that's all seems reasonable enough

1021
01:07:09,750 --> 01:07:11,790
but means to be

1022
01:07:13,080 --> 01:07:16,560
on the other hand the conditioned on value which is much higher than the mean

1023
01:07:16,560 --> 01:07:17,440
will be

1024
01:07:17,920 --> 01:07:21,540
is much lower the of the conditional distribution

1025
01:07:21,590 --> 01:07:24,360
they could conditioning one operation then it

1026
01:07:24,370 --> 01:07:27,150
and if use and modernization one

1027
01:07:27,200 --> 01:07:30,060
the modernization means simply integrating out

1028
01:07:30,110 --> 01:07:32,830
some variables or something else memorable

1029
01:07:32,880 --> 01:07:35,690
again here the joint gaus distribution

1030
01:07:35,740 --> 01:07:37,150
now in some

1031
01:07:37,820 --> 01:07:43,920
this is the variable is is that with some go everything and again i can

1032
01:07:45,230 --> 01:07:46,790
the marginal distribution

1033
01:07:47,660 --> 01:07:50,880
other single variable is again accounted

1034
01:07:56,410 --> 01:08:01,320
OK so i won't try not to worry too much about exactly

1035
01:08:01,330 --> 01:08:02,180
what the

1036
01:08:02,580 --> 01:08:04,620
what the expression of these things like

1037
01:08:04,630 --> 01:08:05,510
because of it

1038
01:08:05,570 --> 01:08:07,500
but so that in the brain may

1039
01:08:07,510 --> 01:08:09,950
if you're interested

1040
01:08:10,070 --> 01:08:12,170
now so what's gas process

1041
01:08:12,220 --> 01:08:15,010
what i talk about some of our gas distribution

1042
01:08:15,060 --> 01:08:19,060
the the process is just generalization

1043
01:08:19,080 --> 01:08:20,620
the graphs and distribution

1044
01:08:20,670 --> 01:08:22,910
to infinitely many variables

1045
01:08:25,710 --> 01:08:26,950
so informally

1046
01:08:26,960 --> 01:08:29,940
it would be useful to think of a function

1047
01:08:29,990 --> 01:08:33,070
that's just being an infinitely long vector

1048
01:08:33,080 --> 01:08:37,160
this seems like a very impractical way of thinking about things it turns out to

1049
01:08:37,160 --> 01:08:41,940
to give you exactly the right intuition here we don't need to you need to

1050
01:08:41,960 --> 01:08:45,790
any fancy mathematical concept just think of the function if you think of the one

1051
01:08:45,790 --> 01:08:50,470
dimensional function just say well if i just specify what the function is

1052
01:08:50,520 --> 01:08:53,990
the function f of x if you just specify what f of x is for

1053
01:08:53,990 --> 01:08:55,350
every x

1054
01:08:55,390 --> 01:08:58,550
then you specify the function

1055
01:08:58,560 --> 01:09:00,620
so it seems a pretty

1056
01:09:00,630 --> 01:09:02,670
simple idea what is actually

1057
01:09:02,680 --> 01:09:06,420
the idea that we need to a function is just the vector

1058
01:09:06,430 --> 01:09:08,360
which is infinitely long because it has

1059
01:09:08,410 --> 01:09:13,080
it specifies what f of x is for every possible

1060
01:09:15,040 --> 01:09:21,060
OK so now now we want to try and look at what happens with the

1061
01:09:21,110 --> 01:09:22,830
for these distributions when we

1062
01:09:22,850 --> 01:09:26,170
when when when their distributions of these in a long

1063
01:09:27,280 --> 01:09:30,850
mathematically it is the definition of the golf course that

1064
01:09:30,960 --> 01:09:35,000
it's a collection of random variables any finite number of which have

1065
01:09:35,010 --> 01:09:37,480
gaston distribution so this is that

1066
01:09:37,490 --> 01:09:43,930
this about mathematicians definition which which avoids them

1067
01:09:45,220 --> 01:09:49,570
i'm taking this has to do with with infinity but they so shy away from

1068
01:09:49,570 --> 01:09:50,870
that and say well

1069
01:09:50,920 --> 01:09:57,510
any finite number of these have gotten distribution maybe maybe you avoid some technicalities

1070
01:09:57,780 --> 01:10:02,320
all right so

1071
01:10:02,330 --> 01:10:06,930
whereas the the the the multivariate doubts in distribution

1072
01:10:06,950 --> 01:10:09,390
what's best was fully specified by

1073
01:10:09,400 --> 01:10:10,840
he the mean vector

1074
01:10:10,850 --> 01:10:13,930
and the covariance matrix

1075
01:10:13,980 --> 01:10:17,900
the gas and process is now specified by

1076
01:10:18,070 --> 01:10:20,570
the infinite dimensional counterpart the

1077
01:10:20,630 --> 01:10:24,550
so the mean vector here now becomes an infinitely long mean

1078
01:10:24,660 --> 01:10:27,360
and since infinitely long vector function

1079
01:10:27,410 --> 01:10:28,810
the mean

1080
01:10:28,820 --> 01:10:31,650
out of the and the

1081
01:10:31,660 --> 01:10:38,600
right in the way right the function has distribution which is

1082
01:10:38,650 --> 01:10:46,480
i have the mean which i mean function mx and similarly for the covariance matrix

1083
01:10:46,480 --> 01:10:47,230
up here

1084
01:10:47,280 --> 01:10:52,300
we would now the matrix is in general and n by n matrix and dimensional

1085
01:10:53,690 --> 01:10:54,820
and we just

1086
01:10:55,840 --> 01:10:57,320
generalizes this to be

1087
01:10:57,450 --> 01:10:59,920
the function of two arguments

1088
01:10:59,970 --> 01:11:01,110
so this is

1089
01:11:01,220 --> 01:11:06,250
roughly speaking corresponds to an infinite plane in the matrix

1090
01:11:06,990 --> 01:11:09,230
and similarly

1091
01:11:09,280 --> 01:11:12,400
one thing which is useful to keep in mind is that

1092
01:11:12,400 --> 01:11:14,600
just such very simple world

1093
01:11:14,650 --> 01:11:17,830
i'm just saying

1094
01:11:17,900 --> 01:11:21,560
there's only two possibilities it's very simple world

1095
01:11:21,560 --> 01:11:26,500
and it's only one period between now and exercise so it's very simple

1096
01:11:26,500 --> 01:11:33,120
now i would say it's gonna development arbitrage theory of options going to say that

1097
01:11:33,120 --> 01:11:35,000
you want to

1098
01:11:37,170 --> 01:11:40,080
you take any profit opportunities restless

1099
01:11:40,170 --> 01:11:44,000
not to be possible to get a riskless profit opportunity here

1100
01:11:44,020 --> 01:11:49,130
by investing both in the stock in the option because there's only two possible values

1101
01:11:49,130 --> 01:11:50,310
for the star

1102
01:11:50,360 --> 01:11:54,400
and you've got both the stock and option there must be a riskless portfolio

1103
01:11:55,100 --> 01:11:59,100
because the price of the option depends only on the price of the stock

1104
01:11:59,200 --> 01:12:02,310
on the exercise date

1105
01:12:02,350 --> 01:12:06,730
so what we do is give optimal head ratio eight

1106
01:12:08,690 --> 01:12:13,230
that makes my portfolio i'm going to form a portfolio of the stock and the

1107
01:12:14,480 --> 01:12:16,310
i'm going to put them together

1108
01:12:16,310 --> 01:12:19,170
so that i have addressed this portfolio

1109
01:12:19,230 --> 01:12:21,380
all right and that's what i want to do

1110
01:12:21,400 --> 01:12:23,250
and out of that is going to fall

1111
01:12:23,290 --> 01:12:26,150
value for the price of the option

1112
01:12:26,150 --> 01:12:27,500
so this is what you want to do

1113
01:12:28,250 --> 01:12:31,940
we're looking for riskless profit up this consider this

1114
01:12:31,960 --> 01:12:34,460
we're going to write one call

1115
01:12:34,500 --> 01:12:37,120
and by h shares

1116
01:12:38,120 --> 01:12:40,120
and i going to pick h

1117
01:12:40,130 --> 01:12:42,860
so that i have no risk at all

1118
01:12:43,040 --> 01:12:45,560
it's easy to see how you do that

1119
01:12:45,600 --> 01:12:47,190
because we already know

1120
01:12:47,210 --> 01:12:49,420
before the exercise date

1121
01:12:49,480 --> 01:12:52,040
we know that if the price goes up

1122
01:12:52,100 --> 01:12:55,000
it will be worth u h

1123
01:12:55,830 --> 01:12:57,130
my portfolio

1124
01:12:57,150 --> 01:12:58,960
but h shares

1125
01:12:59,020 --> 01:13:01,790
the shares will be worth UHF

1126
01:13:02,670 --> 01:13:04,960
use us is the price

1127
01:13:04,960 --> 01:13:08,290
they share be were few a test

1128
01:13:08,330 --> 01:13:12,580
OK but i've written one call

1129
01:13:12,600 --> 01:13:17,850
so this will be worth UAH has minus the

1130
01:13:17,920 --> 01:13:20,270
price of call

1131
01:13:20,290 --> 01:13:24,460
similarly if the price stock if the stock price goes down

1132
01:13:25,290 --> 01:13:28,440
this is the value intrinsic value of the call

1133
01:13:28,960 --> 01:13:31,790
the next period on exercise date

1134
01:13:31,850 --> 01:13:36,190
the portfolio will be worth dhs minus

1135
01:13:36,420 --> 01:13:38,730
c so

1136
01:13:39,460 --> 01:13:40,520
so let's

1137
01:13:40,520 --> 01:13:44,420
choose h so that the two are the same

1138
01:13:44,470 --> 01:13:49,480
and all have to do is set this equal to this solve for h

1139
01:13:49,520 --> 01:13:51,150
that gives me the optimal

1140
01:13:52,750 --> 01:13:54,960
so it is equal to c

1141
01:13:54,960 --> 01:13:57,290
the view minus is the

1142
01:13:57,330 --> 01:13:58,980
however you minus the

1143
01:13:58,980 --> 01:14:00,920
times as

1144
01:14:01,020 --> 01:14:02,290
and now

1145
01:14:02,290 --> 01:14:05,210
it's very simple to get to option pricing

1146
01:14:05,230 --> 01:14:06,770
is that if i can

1147
01:14:06,790 --> 01:14:09,020
form this portfolio with

1148
01:14:09,080 --> 01:14:11,560
where i have

1149
01:14:11,580 --> 01:14:16,690
one call and it shares in the portfolio

1150
01:14:16,710 --> 01:14:18,830
it's a riskless portfolio

1151
01:14:18,850 --> 01:14:22,270
and so it has to run the risk this rate of interest

1152
01:14:22,270 --> 01:14:24,130
and that's what no arbitrage

1153
01:14:24,150 --> 01:14:28,440
sure as it can be possible to get a riskless portfolio

1154
01:14:28,440 --> 01:14:31,880
that means either more or less than the risk was right

1155
01:14:31,880 --> 01:14:35,060
because if if that did happen i would have

1156
01:14:35,080 --> 01:14:38,100
i would have a risk this opportunity

1157
01:14:38,120 --> 01:14:42,960
the ability to earn more than the rest straight with no rest

1158
01:14:43,020 --> 01:14:45,940
that that's contrary to our

1159
01:14:46,170 --> 01:14:47,360
are try

1160
01:14:48,040 --> 01:14:53,150
the return on since you you invested HS minus c

1161
01:14:53,170 --> 01:14:54,960
in the portfolio

1162
01:14:55,020 --> 01:14:59,130
the return of the total value of it has to equal one plus the response

1163
01:15:00,020 --> 01:15:02,560
and hs minus c

1164
01:15:02,580 --> 01:15:04,270
if so if you substitute in

1165
01:15:04,480 --> 01:15:06,830
for a test minus c

1166
01:15:06,830 --> 01:15:09,100
you find out that it equals

1167
01:15:10,650 --> 01:15:16,130
and substitute for h into this and you get the price of the call

1168
01:15:16,190 --> 01:15:18,000
today and that's

1169
01:15:18,020 --> 01:15:20,150
it's simple algebra

1170
01:15:20,170 --> 01:15:21,380
but there it is

1171
01:15:21,400 --> 01:15:22,540
so that's the

1172
01:15:22,540 --> 01:15:24,250
arbitrage theory

1173
01:15:24,270 --> 01:15:26,500
call option price

1174
01:15:28,400 --> 01:15:31,770
that might be less than intuitive to you but

1175
01:15:31,810 --> 01:15:33,290
you see that was very simple

1176
01:15:33,310 --> 01:15:35,060
arguing that got us there

1177
01:15:35,080 --> 01:15:36,620
we merely said

1178
01:15:36,630 --> 01:15:42,290
the way to think about options is that options move with the stock price

1179
01:15:44,190 --> 01:15:47,920
there are there are perfectly correlated with stock price over

1180
01:15:47,920 --> 01:15:49,210
this interval because

1181
01:15:49,230 --> 01:15:52,830
if the stock price goes up you know you've got c so this is the

1182
01:15:52,830 --> 01:15:56,060
view of the stock price goes down you know you gets said

1183
01:15:56,080 --> 01:15:59,770
so you have only one source of uncertainty but you have to assets

1184
01:15:59,810 --> 01:16:02,380
so you can put them together to eliminate risk

1185
01:16:02,400 --> 01:16:05,650
and if you put them together where they have to and the rest was right

1186
01:16:05,690 --> 01:16:10,460
and you just solve for and you get this value the call

1187
01:16:10,520 --> 01:16:14,380
this is the ins inherent inside the black and shoals

1188
01:16:14,400 --> 01:16:16,060
i came up with in there

1189
01:16:16,060 --> 01:16:18,020
classic nineteen

1190
01:16:18,040 --> 01:16:19,830
seventy three paper

1191
01:16:19,860 --> 01:16:23,190
an option pricing which i'll come to you but

1192
01:16:23,500 --> 01:16:28,400
this has to be the price of the car option the simple world

1193
01:16:28,400 --> 01:16:31,230
otherwise be arbitrage

1194
01:16:31,270 --> 01:16:36,690
the interesting thing about this is that there are no probabilities in this formula

1195
01:16:36,730 --> 01:16:39,040
what's in this form i've got

1196
01:16:39,040 --> 01:16:44,290
and these variables here which you need the covariance matrix by correlations between the variables

1197
01:16:44,290 --> 01:16:45,490
and you have me

1198
01:16:45,490 --> 01:16:47,910
and so on and so forth OK

1199
01:16:47,930 --> 01:16:52,220
so this allows you to describe joint distributions over

1200
01:16:52,270 --> 01:16:54,650
multiple outputs

1201
01:16:55,700 --> 01:16:59,220
so the other thing i want to mention it was which

1202
01:16:59,270 --> 01:17:01,760
we're not going to be used a lot

1203
01:17:01,760 --> 01:17:07,430
but it's there is very important in probability sample based approximations very often you can't

1204
01:17:07,430 --> 01:17:10,440
compute expectations that you're interested in exactly

1205
01:17:10,490 --> 01:17:13,440
but what you can do is you can have samples from

1206
01:17:13,450 --> 01:17:16,500
the density you're interested in and then you can have a sample in the law

1207
01:17:16,500 --> 01:17:20,070
of large numbers of and i was mentioning you can have a sample based approximations

1208
01:17:20,070 --> 01:17:22,520
to the expectation you're interested in

1209
01:17:22,550 --> 01:17:26,750
so that used to compute orbit because of course the sample mean is is we

1210
01:17:26,750 --> 01:17:30,020
always talk about means what we think of sample means but in some ways that's

1211
01:17:30,020 --> 01:17:33,120
just an approximation to the true mean but we say both of these things are

1212
01:17:33,120 --> 01:17:37,280
means but what we really mean when saying this is the sample mean i tend

1213
01:17:37,280 --> 01:17:39,230
to think of that being true mean

1214
01:17:40,350 --> 01:17:42,320
anything can be computed in that way

1215
01:17:42,340 --> 01:17:47,020
because when we're looking at data that's what we effectively have have some unknown distribution

1216
01:17:47,030 --> 01:17:49,970
and we have samples from it and the sort of thing but i was talking

1217
01:17:49,970 --> 01:17:53,570
about in terms of law of large numbers and convergence is making the sample based

1218
01:17:53,570 --> 01:17:57,590
approximations you've got samples from some unknown distribution

1219
01:18:00,710 --> 01:18:02,410
so that's as much as i wanted

1220
01:18:02,420 --> 01:18:04,670
talk about the review

1221
01:18:04,760 --> 01:18:07,240
probability hopefully it wasn't

1222
01:18:07,250 --> 01:18:08,960
well if it something in there

1223
01:18:08,980 --> 01:18:12,370
so this is what we had before so i'm not something to talk about error

1224
01:18:13,470 --> 01:18:15,020
and we said OK

1225
01:18:15,040 --> 01:18:18,270
this is how we measure the quality of our regression and we said we we're

1226
01:18:18,270 --> 01:18:22,570
get some across some output from some functions so this is the set of bases

1227
01:18:22,570 --> 01:18:24,770
that we had follow bumps

1228
01:18:24,820 --> 01:18:29,600
and these weighting of these bonds minus the actual target so this was like delta

1229
01:18:29,600 --> 01:18:31,150
y i square

1230
01:18:31,170 --> 01:18:35,000
and the sum of that for every data point we observed

1231
01:18:35,720 --> 01:18:40,200
the quadratic error function can be seen as the gaussians noise models so the way

1232
01:18:40,200 --> 01:18:44,870
that works is to imagine this is the data generating process and we often talk

1233
01:18:44,870 --> 01:18:49,480
about generative models and this is sort what we mean that x is inside the

1234
01:18:49,480 --> 01:18:52,160
basis function vector basis functions here

1235
01:18:52,240 --> 01:18:55,340
and what we are observing is why would the function of x which is a

1236
01:18:55,340 --> 01:18:59,600
linear weighted set of basis functions plus now this is the new this is what

1237
01:18:59,600 --> 01:19:00,440
we had before

1238
01:19:00,460 --> 01:19:04,320
x long long is calcium noise standard deviation sigma

1239
01:19:05,100 --> 01:19:09,900
this noise model assumption is still critical something you didn't see what bernard was talking

1240
01:19:09,900 --> 01:19:13,740
about because when he's generating a loss function

1241
01:19:13,760 --> 01:19:17,250
you're not actually considering the noise what you're considering is the penalty will pay for

1242
01:19:17,250 --> 01:19:21,240
getting things incorrect this is nothing to do with the penalty you'll pay for getting

1243
01:19:21,240 --> 01:19:25,820
things incorrect so let's say i'm ordering lumber

1244
01:19:27,580 --> 01:19:31,450
because i got supply and demand side i need to buy in london

1245
01:19:31,670 --> 01:19:33,870
to deliver to my customers

1246
01:19:33,930 --> 01:19:37,600
now these two things that can happen right if i don't have the right amount

1247
01:19:37,610 --> 01:19:42,920
of lumber i might have cost for having incorrect by running a pentecostal missiles if

1248
01:19:42,920 --> 01:19:46,930
i've got too much rots so there's an amount of money i can say that's

1249
01:19:46,940 --> 01:19:49,290
my cost right so that's one thing

1250
01:19:49,350 --> 01:19:52,530
but another thing is the demand for lumber

1251
01:19:52,550 --> 01:19:55,560
so the cost might be i don't know linear

1252
01:19:55,570 --> 01:19:59,760
in the amount of the incorrect this is my number predictions

1253
01:19:59,760 --> 01:20:01,680
but the demand for lumber

1254
01:20:01,680 --> 01:20:06,340
might be coming from gauss distribution so might be quadratic so these things are these

1255
01:20:06,340 --> 01:20:08,690
two things are important and different

1256
01:20:09,810 --> 01:20:14,610
in some ways actually i would say that what the the frequentist approach the but

1257
01:20:14,640 --> 01:20:20,260
i was describing is more correct because you're trying to take into account your cost

1258
01:20:20,270 --> 01:20:22,420
at the time the fitting your model

1259
01:20:22,430 --> 01:20:26,220
in the bayesian approach you do that and there's a reason why or in this

1260
01:20:26,220 --> 01:20:30,060
maximum likelihood approach you want to do that the reason why you don't do that

1261
01:20:30,090 --> 01:20:31,390
is you know

1262
01:20:31,400 --> 01:20:35,700
if your model is correct you don't need to worry about that you can prove

1263
01:20:35,700 --> 01:20:37,650
that you can say if the thing

1264
01:20:37,870 --> 01:20:41,550
the model you've got the system is correct you don't have to worry about the

1265
01:20:41,550 --> 01:20:43,200
cost until later

1266
01:20:43,210 --> 01:20:47,520
so that's really nice separation you can do these two things separately what about inference

1267
01:20:47,530 --> 01:20:50,500
as i recall it then you worry about your cost of course in practice that's

1268
01:20:50,500 --> 01:20:53,940
never really true never know you've got the correct models you can't really do that

1269
01:20:53,940 --> 01:20:58,100
separation but the nice thing about the separation is it makes everything simpler make model

1270
01:20:58,100 --> 01:21:00,290
construction simpler so

1271
01:21:00,350 --> 01:21:01,580
you have to be

1272
01:21:01,620 --> 01:21:05,220
more intelligent to be frequentist i would say

1273
01:21:05,230 --> 01:21:09,860
so i'm very because it's mechanical and i don't have to think so so i

1274
01:21:09,860 --> 01:21:12,180
can do more with my limited brain

1275
01:21:12,230 --> 01:21:15,270
OK so

1276
01:21:15,290 --> 01:21:19,220
this is calcium noise writing on here but is the product of the generating system

1277
01:21:19,220 --> 01:21:21,560
nothing to do with the cost

1278
01:21:21,890 --> 01:21:25,530
so once we got that this implies that we can write this down and this

1279
01:21:25,530 --> 01:21:29,240
is the sort of way we describe the likelihood in this case that implies that

1280
01:21:29,250 --> 01:21:32,610
y is drawn from the guassian distribution

1281
01:21:32,620 --> 01:21:35,140
with the meaning given by that

1282
01:21:35,140 --> 01:21:37,260
and since the noise is zero

1283
01:21:37,280 --> 01:21:38,190
i mean

1284
01:21:38,200 --> 01:21:39,610
when you add

1285
01:21:39,670 --> 01:21:43,450
noise gas in you just adding new effectively making that the mean of the distribution

1286
01:21:43,450 --> 01:21:49,870
writing constant function to zero mean gaussians which just means mean new thing is that

1287
01:21:49,900 --> 01:21:54,060
so we can also write i would write that that that this the sort of

1288
01:21:54,180 --> 01:21:59,420
very statistical notation this still the thing here means y is sampled from this

1289
01:21:59,550 --> 01:22:03,550
another way of writing that is that the probability of y given

1290
01:22:03,570 --> 01:22:05,520
w and sigma

1291
01:22:05,560 --> 01:22:08,050
is this calcium distributions so

1292
01:22:09,950 --> 01:22:12,070
my wife depend on these parameters

1293
01:22:12,090 --> 01:22:17,050
and its dependent the gaussians vision so i might interchange between those two styles but

1294
01:22:17,210 --> 01:22:23,000
basically identical sister wives missing of that

1295
01:22:24,860 --> 01:22:29,190
one of the things i think is quite confusing is is i i d assumptions

1296
01:22:29,190 --> 01:22:31,820
the way i think of i i assumptions

1297
01:22:31,830 --> 01:22:33,960
which you mention all the time

1298
01:22:34,070 --> 01:22:38,450
is this so if the noise is sampled independently for each data point from the

1299
01:22:38,450 --> 01:22:43,740
same density so this noise we this corrupting influence is independent every time we get

1300
01:22:43,740 --> 01:22:47,440
a data point and coming from the same density each time that's why i think

1301
01:22:47,440 --> 01:22:51,820
it was an idea assumptions i don't think of the model is being IID assumption

1302
01:22:51,820 --> 01:22:54,450
your body can help get it here it's kind of funny when you with these

1303
01:22:54,450 --> 01:22:58,540
things i want to give it away

1304
01:22:58,630 --> 01:23:03,460
that's all i know it's really it's actually when you build is very integrated models

1305
01:23:03,500 --> 01:23:06,700
kind of behaviors which might not necessarily anticipated

1306
01:23:06,730 --> 01:23:07,480
all right

1307
01:23:07,500 --> 01:23:11,890
and here's another example looking at the learning scenario

1308
01:23:11,890 --> 01:23:13,710
you know

1309
01:23:13,730 --> 01:23:18,390
just looking at integrating different sort of social building so this is map from the

1310
01:23:18,400 --> 01:23:21,480
i this is the possibility of this is big bird

1311
01:23:21,520 --> 01:23:26,490
so one year of age human children able to find the very effective appraisals of

1312
01:23:26,490 --> 01:23:30,010
novel objects environmentally socially reference

1313
01:23:30,030 --> 01:23:32,610
their parents people they trust

1314
01:23:32,700 --> 01:23:37,980
you're welcome big brother things that you don't have to themselves

1315
01:23:38,000 --> 01:23:39,480
if you look at the learning

1316
01:23:39,610 --> 01:23:43,000
learning ability of the of the matter is that the very interesting

1317
01:23:43,070 --> 01:23:47,360
early birds sharing attention about it you start to think that fact from his tone

1318
01:23:47,360 --> 01:23:48,220
of voice

1319
01:23:48,550 --> 01:23:50,080
getting are yellow

1320
01:23:50,120 --> 01:23:54,090
this is the reference to the interaction that happening around

1321
01:23:54,100 --> 01:23:55,400
this object

1322
01:23:55,420 --> 01:23:59,130
starting to pick up on that his own internal state

1323
01:23:59,140 --> 01:24:03,630
are starting to come into line with that is to learn essentially an effective tag

1324
01:24:03,630 --> 01:24:07,540
helio vocabularies in world characters actually the thing that you want so that's what he's

1325
01:24:07,540 --> 01:24:14,660
reaching out be a good thing i want i want to be

1326
01:24:14,710 --> 01:24:19,740
is i want to give it to the police are ineffective presumably the positive things

1327
01:24:19,830 --> 01:24:22,980
they might want to learn the next for about

1328
01:24:23,130 --> 01:24:27,120
this is showed basically that the you know the answer the interaction can happen as

1329
01:24:28,630 --> 01:24:32,930
was also a novel object can you find cookie monster introducing making sure that you

1330
01:24:32,930 --> 01:24:34,450
know this where it is

1331
01:24:34,480 --> 01:24:37,450
so we understand this cookie monster thing

1332
01:24:37,470 --> 01:24:43,970
nobody found in the world complete monsters appearing in the opposite

1333
01:24:46,440 --> 01:24:54,140
speaking up on that monsters very very

1334
01:24:54,210 --> 01:24:59,430
scary monster wants to disregard these

1335
01:24:59,630 --> 01:25:08,380
and that's OK you that you can share now

1336
01:25:08,390 --> 01:25:11,890
in this but surely actually has the memory now you can read about these subject

1337
01:25:11,900 --> 01:25:13,760
in prison

1338
01:25:13,830 --> 01:25:26,170
is that we still want to predict what

1339
01:25:26,230 --> 01:25:30,650
you want to do that you want to so

1340
01:25:30,700 --> 01:25:37,900
so people sometimes get very concerned about this video were teaching leo cookie monster

1341
01:25:37,910 --> 01:25:43,480
it's really not trying to show in the video

1342
01:25:43,620 --> 01:25:48,070
really want to make the point that know robots in the world you know most

1343
01:25:48,070 --> 01:25:50,730
about the encounter is going to be notable to the brain

1344
01:25:50,750 --> 01:25:53,630
and the way that the public and how have to learn about things that are

1345
01:25:53,660 --> 01:25:57,130
safe to explore that to explore the problem that to pick up from the people

1346
01:25:57,130 --> 01:26:03,040
around them so endowing about these social learning abilities imitation demonstration for thing all these

1347
01:26:03,040 --> 01:26:05,930
things that we do in nature i think are going to be very important for

1348
01:26:05,940 --> 01:26:08,690
the success so the idea is that by

1349
01:26:08,700 --> 01:26:13,000
communicating to things that are going to explore versus maybe not good might

1350
01:26:13,050 --> 01:26:16,820
o gauge how we choose to explore the weight of but isn't necessarily matching his

1351
01:26:16,820 --> 01:26:21,100
memory these forever going forever that we get to keep the bigger picture open mind

1352
01:26:21,170 --> 01:26:24,070
and why we're motivated to look at this is what learning

1353
01:26:24,120 --> 01:26:27,360
OK so so the next thing i want to talk about then is if we

1354
01:26:27,360 --> 01:26:32,480
can build machines that can elicit the set of social understanding from people can and

1355
01:26:32,480 --> 01:26:37,970
should be little sister elongation for social understanding what about relationship what about long-term interaction

1356
01:26:37,990 --> 01:26:41,240
relationship and why might that be interesting for robots

1357
01:26:41,310 --> 01:26:45,660
so we wanted to do a study exploring this in terms of long-term human robot

1358
01:26:46,840 --> 01:26:50,790
and the domain that we wanted to look at was weight management and we chose

1359
01:26:50,790 --> 01:26:53,960
that for a number of reasons the first of course is that this is this

1360
01:26:53,960 --> 01:26:57,410
is an african issue in the united states sixty five percent of the united states

1361
01:26:57,410 --> 01:26:58,810
are overweight or obese

1362
01:26:58,830 --> 01:27:02,040
and we know that as we get older many of chronic diseases are tied to

1363
01:27:02,040 --> 01:27:05,320
overweight and obesity so even just for human quality of life it's important for us

1364
01:27:05,320 --> 01:27:08,290
to think about how we can help people get this under control

1365
01:27:09,110 --> 01:27:13,510
it also turns out that from you know successful progress to achieve success is often

1366
01:27:13,510 --> 01:27:15,140
attributed to a number of things so

1367
01:27:15,170 --> 01:27:17,860
the first of course is long-term motivation

1368
01:27:17,860 --> 01:27:20,130
two your diet exercise program

1369
01:27:20,140 --> 01:27:25,190
social support is often found to be very positive interventions of friends family also trying

1370
01:27:25,200 --> 01:27:25,640
to do

1371
01:27:25,650 --> 01:27:30,480
you are encouraged to lose weight is also very important just logistics like accuracy of

1372
01:27:30,480 --> 01:27:31,560
collection of

1373
01:27:31,560 --> 01:27:37,690
in calories expended calories through exercise and perhaps if using adaptive using training being able

1374
01:27:37,700 --> 01:27:41,210
to share your progress with members of a social support network so

1375
01:27:41,730 --> 01:27:43,350
given these factors

1376
01:27:43,350 --> 01:27:47,320
it is intriguing to think about how a robot might introduce a new element a

1377
01:27:47,320 --> 01:27:51,560
new kind of technology to help people address this problem with the key ideas this

1378
01:27:51,560 --> 01:27:57,210
robot can not only kind of either the social support a long-term engagement between the

1379
01:27:57,210 --> 01:27:59,040
the person themselves

1380
01:27:59,060 --> 01:28:01,760
the patient so to speak

1381
01:28:01,780 --> 01:28:06,720
are to maintain a way that you can imagine the robot can also help manager

1382
01:28:06,780 --> 01:28:10,960
device network we have a bluetooth pedometer bluetooth scale just to help with the logistics

1383
01:28:10,960 --> 01:28:15,930
of keeping track of your calorie expenditure and so forth and also if you have

1384
01:28:15,930 --> 01:28:19,380
a little bit every day and keeping track of those interactions it can store the

1385
01:28:19,380 --> 01:28:24,160
information that you could print out and share with your training your doctor your family

1386
01:28:24,180 --> 01:28:26,700
so to empower your actual social network

1387
01:28:26,720 --> 01:28:27,650
as well

1388
01:28:27,650 --> 01:28:29,590
we find this structure

1389
01:28:29,590 --> 01:28:32,270
in profound way melts away

1390
01:28:32,280 --> 01:28:33,890
leaving the very

1391
01:28:33,900 --> 01:28:39,670
he symmetry behind which are symbolized by these grains of sand on this picture and

1392
01:28:39,670 --> 01:28:42,790
what this means i'll explain in the fourth lecture

1393
01:28:42,800 --> 01:28:44,320
but the general idea

1394
01:28:44,340 --> 01:28:48,860
of there being a profound symmetry in hot conditions

1395
01:28:48,890 --> 01:28:53,410
which freezes into structures and patterns in cold conditions

1396
01:28:53,420 --> 01:28:58,940
it's something which permeates all of science and is at the root of this desire

1397
01:28:58,940 --> 01:29:03,150
to find the higgs those on all that stuff the large hadron collider is about

1398
01:29:03,160 --> 01:29:06,580
hopefully by the end of these four lectures you'll have a sense

1399
01:29:06,610 --> 01:29:08,820
what this is

1400
01:29:08,900 --> 01:29:12,840
and here of course is a particular example of snowflakes

1401
01:29:14,360 --> 01:29:20,160
the freezing point of water show beautiful six-fold pattern

1402
01:29:20,170 --> 01:29:25,590
melt snowflake by raising the temperature that six-fold pattern disappears and you get a completely

1403
01:29:25,590 --> 01:29:32,570
uniform symmetry this is one of many examples of patterns disappearing and different symmetry is

1404
01:29:32,570 --> 01:29:34,450
emerging as you heat things up

1405
01:29:34,530 --> 01:29:36,880
and we will see in the universe as a whole

1406
01:29:36,890 --> 01:29:40,240
we believe exhibit this sort of phenomenon you go from

1407
01:29:40,260 --> 01:29:45,330
present temperatures extreme edges we'll measure the LHC

1408
01:29:45,340 --> 01:29:47,840
so that's how things began with nothing at all

1409
01:29:47,850 --> 01:29:52,140
and then out of it out of this big bang emerged

1410
01:29:52,150 --> 01:29:55,710
matter and antimatter in perfect balance

1411
01:29:55,780 --> 01:29:58,580
the best experiments we can do

1412
01:29:58,590 --> 01:30:01,600
and we all know from star trek and even experiment

1413
01:30:01,630 --> 01:30:05,900
the when matter and antimatter meet they annihilate one another

1414
01:30:05,990 --> 01:30:09,490
so that then raises the big question which we still don't know the answer to

1415
01:30:09,490 --> 01:30:10,760
why is it

1416
01:30:10,840 --> 01:30:13,040
the beginning a second later

1417
01:30:13,130 --> 01:30:17,490
the newly born matter and antimatter had mutually annihilate to one another and destroy the

1418
01:30:18,540 --> 01:30:21,050
so how is that fourteen billion years later

1419
01:30:21,060 --> 01:30:24,470
the universe appears to be made primarily of matter

1420
01:30:24,490 --> 01:30:27,600
and you have to invoke we find no evidence for it all

1421
01:30:27,620 --> 01:30:29,620
this asymmetry

1422
01:30:29,630 --> 01:30:32,630
is critical to us being here

1423
01:30:32,640 --> 01:30:35,980
the origin of it is one of the great unsolved problems which we hope will

1424
01:30:36,010 --> 01:30:37,570
get the answers to

1425
01:30:37,590 --> 01:30:41,640
the LHC but we'll talk about this again you'll hear more about this in other

1426
01:30:44,500 --> 01:30:47,590
so to give an idea of the scale of things that we can do in

1427
01:30:47,590 --> 01:30:50,850
science and what we know about the universe

1428
01:30:51,930 --> 01:30:57,740
i have been able to explore the universe over forty orders of magnitude distance scale

1429
01:30:57,800 --> 01:30:58,920
that the

1430
01:30:58,940 --> 01:31:01,990
observable universe about ten to twenty six metres

1431
01:31:02,000 --> 01:31:07,300
of course we build instruments to expand our senses beyond the immediate once we have

1432
01:31:07,340 --> 01:31:09,140
telescope shows the space

1433
01:31:09,600 --> 01:31:11,190
if you want to start looking

1434
01:31:11,210 --> 01:31:15,000
at the smaller structures of things you need to build microscopes

1435
01:31:15,010 --> 01:31:18,910
and if you want to study the very small structures of things that the levels

1436
01:31:18,910 --> 01:31:22,750
of ten to the minus fifteen meters or even smaller

1437
01:31:22,770 --> 01:31:25,250
you have very special microscopes which is what

1438
01:31:25,270 --> 01:31:27,220
particle accelerators are

1439
01:31:27,230 --> 01:31:30,880
what i do and it is actually show you how these numbers

1440
01:31:31,540 --> 01:31:38,720
things all merged together why is the particle accelerators microscopes why they probe distance scales

1441
01:31:38,720 --> 01:31:42,970
while you need energy you need

1442
01:31:42,980 --> 01:31:47,790
so the basic game that started many many years ago is what is matter made

1443
01:31:49,450 --> 01:31:52,640
and there are three broad ways that we know how to do that you can

1444
01:31:52,640 --> 01:31:56,230
look at it you can smash it up which is obviously more fun when you

1445
01:31:56,230 --> 01:31:58,090
can heat up and see what happens

1446
01:31:58,120 --> 01:32:00,910
as we will see all of these three are doing the same thing in different

1447
01:32:00,910 --> 01:32:03,050
ways but let's take the one at the time

1448
01:32:03,060 --> 01:32:04,960
looking at things will see me

1449
01:32:04,960 --> 01:32:06,680
because light is shining on me

1450
01:32:06,690 --> 01:32:08,660
and you see it in your archives

1451
01:32:08,790 --> 01:32:11,320
the light source and object in your eye

1452
01:32:11,370 --> 01:32:15,380
two international jargon of particle physics

1453
01:32:15,440 --> 01:32:18,680
this is a particular example of what we do all the time

1454
01:32:18,700 --> 01:32:22,300
in this case the light is a beam of radiation

1455
01:32:22,310 --> 01:32:23,930
you have been

1456
01:32:24,000 --> 01:32:25,180
which you

1457
01:32:25,220 --> 01:32:27,410
fire target

1458
01:32:27,540 --> 01:32:30,350
and then you detect the scattered beam

1459
01:32:30,370 --> 01:32:34,180
and from that you work out what the target is made of

1460
01:32:34,190 --> 01:32:38,140
so why don't we do it just by looking well there's a limit to what

1461
01:32:38,140 --> 01:32:40,260
you can see with your eyes the limits

1462
01:32:40,320 --> 01:32:41,610
of the

1463
01:32:41,630 --> 01:32:43,240
vision with

1464
01:32:43,300 --> 01:32:44,250
the rainbow

1465
01:32:46,180 --> 01:32:50,630
ten months for me to i bacteria are even smaller than that

1466
01:32:50,660 --> 01:32:54,680
after much smaller than atomic nucleus quarks electrons yet small still

1467
01:32:54,700 --> 01:32:59,930
so it's not a problem of you have the powerful enough the magnifying glass there's

1468
01:32:59,940 --> 01:33:01,580
an inherent problem

1469
01:33:02,890 --> 01:33:04,410
i'll show you know i mean it

1470
01:33:04,610 --> 01:33:07,560
what i mean by that and how we can't

1471
01:33:07,570 --> 01:33:12,560
so to look at things smaller than bacteria we need to have special instruments to

1472
01:33:12,580 --> 01:33:14,320
be able to withstand

1473
01:33:14,360 --> 01:33:16,720
our vision

1474
01:33:16,720 --> 01:33:20,910
and the problem is the fact that light at least the wavelength of light that

1475
01:33:20,910 --> 01:33:23,200
our eyes respond to

1476
01:33:23,240 --> 01:33:25,550
are very large on the scale

1477
01:33:25,590 --> 01:33:28,340
after a little over small things

1478
01:33:28,570 --> 01:33:29,840
five scale

1479
01:33:31,020 --> 01:33:34,800
ica wavelengths greenlight right there in the middle of the rainbow

1480
01:33:34,960 --> 01:33:37,770
compared size that and you see the atoms

1481
01:33:37,790 --> 01:33:40,190
incredibly small compared with

1482
01:33:40,250 --> 01:33:43,800
the light just passes by without being disturbed

1483
01:33:43,910 --> 01:33:44,990
in order

1484
01:33:45,020 --> 01:33:48,770
two c and at some level and resolve its internal structure

1485
01:33:48,830 --> 01:33:53,440
you need to have a wavelength which is comparable to or smaller than the object

1486
01:33:53,440 --> 01:33:57,310
you want to look at when those of you who go sailing you where the

1487
01:33:57,310 --> 01:33:59,090
fact if you've got a nice

1488
01:33:59,100 --> 01:34:01,200
rolling waves going along

1489
01:34:01,220 --> 01:34:05,350
a small fish small yachts will just pop up and down the waves without disturbing

1490
01:34:06,220 --> 01:34:09,560
is a large oil tanker will scatter the waves

1491
01:34:09,590 --> 01:34:13,250
and in principle you can imagine detecting the scattered waves and working out there was

1492
01:34:13,250 --> 01:34:18,670
an oil tanker there so the moment the wavelength is smaller than the object that

1493
01:34:18,670 --> 01:34:25,840
object will be resolved so we need to have waves that are smaller than atoms

1494
01:34:25,850 --> 01:34:30,160
one of the great things that was discovered in the nineteen twenties

1495
01:34:30,270 --> 01:34:35,160
is that not just like in the way that all particles have wavelike characteristics as

1496
01:34:35,170 --> 01:34:37,740
one of the basics of quantum mechanics

1497
01:34:37,750 --> 01:34:40,980
and the wavelength for example of an electron

1498
01:34:40,990 --> 01:34:44,740
the wavelength is inversely proportional to the momentum so

1499
01:34:45,570 --> 01:34:50,750
high speed the higher the energy the higher the momentum the smaller the wavelength

1500
01:34:50,770 --> 01:34:54,840
and hence the better the resolution you can receive

1501
01:34:54,840 --> 01:35:00,250
and rather than comparing walk walks into graphs they compare some three like patterns

1502
01:35:00,380 --> 01:35:04,440
in in two graphs what's up three like patterns

1503
01:35:04,440 --> 01:35:06,860
it's not exactly the same as up tree

1504
01:35:06,900 --> 01:35:08,190
of the graph

1505
01:35:10,020 --> 01:35:15,440
subtree that allows for repetitions of nodes so you start from one node in your

1506
01:35:15,440 --> 01:35:18,340
grasp you visit its neighbours

1507
01:35:18,400 --> 01:35:20,550
and then you visit the neighbours

1508
01:35:20,570 --> 01:35:22,210
one of these neighbors

1509
01:35:22,230 --> 01:35:24,340
which means that you might go back

1510
01:35:24,380 --> 01:35:25,960
two years starting note

1511
01:35:26,020 --> 01:35:29,230
and that's the difference between this up a tree like pattern

1512
01:35:29,270 --> 01:35:31,570
and the up tree

1513
01:35:31,630 --> 01:35:34,860
and the common works by first comparing

1514
01:35:34,960 --> 01:35:37,190
the starting notes to each other

1515
01:35:37,230 --> 01:35:39,310
in year two graphs so started to

1516
01:35:39,320 --> 01:35:42,940
nodes in u two graphs and then you compared to neighborhoods

1517
01:35:42,980 --> 01:35:44,550
because if you to each other

1518
01:35:44,570 --> 01:35:48,650
if you think about it it's quite related to this paper decomposition idea

1519
01:35:49,670 --> 01:35:52,610
these nodes from which is up three

1520
01:35:52,670 --> 01:35:57,750
like patterns are grown as the selectors

1521
01:35:57,750 --> 01:36:01,520
and the advantage here is that this gives you a richer representation of the graph

1522
01:36:01,520 --> 01:36:06,070
structure than the walk based approach but of course the runtime grows exponentially with the

1523
01:36:07,570 --> 01:36:10,690
depth of the subtree like patterns

1524
01:36:10,690 --> 01:36:16,820
so this gives you a richer representation but does not here neuron timbre

1525
01:36:16,880 --> 01:36:20,690
another approach was presented at KDD two thousand four

1526
01:36:20,730 --> 01:36:23,400
is the so-called cyclic pattern occurrence

1527
01:36:25,020 --> 01:36:27,710
i want to compare simple cycles

1528
01:36:27,710 --> 01:36:30,340
in two graphs and these are

1529
01:36:30,380 --> 01:36:33,320
pass through the start node equals and note

1530
01:36:33,340 --> 01:36:38,320
so cycles cycles in europe graph the problem is that the number of simple cycles

1531
01:36:38,360 --> 01:36:46,590
is exponential in the number n of vertices in the worst case

1532
01:36:46,630 --> 01:36:47,540
but still

1533
01:36:47,540 --> 01:36:51,750
in order to find a problem based on the simple cycles

1534
01:36:51,750 --> 01:36:52,960
palmer et

1535
01:36:53,020 --> 01:36:58,230
defined the canonical string representation of each simple cycles so that they be find an

1536
01:36:58,230 --> 01:37:03,320
ordering over the nodes in your graph and the canonical string representation of the cyclist

1537
01:37:03,380 --> 01:37:06,860
the one that is first in alphabetical order

1538
01:37:07,040 --> 01:37:10,940
according to one needs to be in a sabbatical order according to the

1539
01:37:11,020 --> 01:37:13,090
the order about is that they

1540
01:37:13,130 --> 01:37:15,090
defined in the first place

1541
01:37:15,150 --> 01:37:19,590
and that's an interesting alternative to walk based comments but it suffers from the problem

1542
01:37:19,590 --> 01:37:23,880
that in general the sickly pattern con is NP hard to compute

1543
01:37:23,900 --> 01:37:26,770
and so what i restrict attention

1544
01:37:26,820 --> 01:37:30,000
two scenarios but the number of simple cycle

1545
01:37:30,050 --> 01:37:35,020
in the graph dataset is bounded by a constant you then they can prove

1546
01:37:35,070 --> 01:37:36,940
that the runtime effort

1547
01:37:37,540 --> 01:37:43,250
it's only polynomial in the size of the graph otherwise is NP hard

1548
01:37:43,290 --> 01:37:50,840
to compute this suit cyclic pattern comes

1549
01:37:50,840 --> 01:37:52,340
a third approach

1550
01:37:52,380 --> 01:37:54,500
which is based on the idea to look

1551
01:37:54,520 --> 01:37:56,150
and subgraph

1552
01:37:56,210 --> 01:37:58,420
class sub graphs other than walks

1553
01:37:59,040 --> 01:38:01,070
the so-called graph can

1554
01:38:01,130 --> 01:38:06,750
here one counts subgraphs of of of limited size k into two input graph

1555
01:38:06,790 --> 01:38:11,320
and these subgraphs of limited size are referred to as graph it's because that's the

1556
01:38:11,690 --> 01:38:14,340
term that has been used in the bound for medical literature

1557
01:38:14,380 --> 01:38:17,900
as before

1558
01:38:17,960 --> 01:38:20,690
and you can't value is then

1559
01:38:20,730 --> 01:38:25,980
the number of isomorphic graphlets into graphs

1560
01:38:25,980 --> 01:38:27,130
however this

1561
01:38:27,130 --> 01:38:31,090
because of runtime problems the pairwise hasadvisorx morphism

1562
01:38:31,320 --> 01:38:33,020
for these graphlets

1563
01:38:34,210 --> 01:38:35,650
is expensive

1564
01:38:35,670 --> 01:38:40,480
and the number of graphlets lipschitz off into the so enumerating them is already very

1565
01:38:40,480 --> 01:38:41,820
expensive k

1566
01:38:41,840 --> 01:38:43,570
as k grows

1567
01:38:43,610 --> 01:38:46,920
as k increases

1568
01:38:46,940 --> 01:38:50,110
there are however two solutions

1569
01:38:50,110 --> 01:38:53,570
to speed up this kind of unlabelled graphs

1570
01:38:54,460 --> 01:38:58,000
two drinks the first one is precompute

1571
01:38:58,070 --> 01:39:00,320
the isomorphism relationships

1572
01:39:00,360 --> 01:39:03,170
between the graph let's here you exploited

1573
01:39:03,210 --> 01:39:05,040
the property the fact

1574
01:39:05,090 --> 01:39:09,860
that if you cross are unlabelled then there can be only a limited number of

1575
01:39:10,900 --> 01:39:14,810
graph of size k so you can precompute the isomorphisms

1576
01:39:14,820 --> 01:39:16,290
among these

1577
01:39:16,310 --> 01:39:19,000
subgraphs of size k

1578
01:39:19,050 --> 01:39:23,020
and then rather than enumerating all the

1579
01:39:23,070 --> 01:39:25,500
the graphlets in off into the k

1580
01:39:25,520 --> 01:39:28,590
you can sample graph from your graphs

1581
01:39:29,480 --> 01:39:31,880
to speed up your computer

1582
01:39:31,880 --> 01:39:33,380
this each here

1583
01:39:34,210 --> 01:39:37,590
there are obviously the same solutions do not hold

1584
01:39:37,610 --> 01:39:39,210
for labeled graphs

1585
01:39:39,270 --> 01:39:40,940
so far this only

1586
01:39:41,840 --> 01:39:45,650
on labeled unlabeled graph

1587
01:39:45,670 --> 01:39:47,090
these are the

1588
01:39:48,050 --> 01:39:49,980
graph of size four

1589
01:39:50,040 --> 01:39:51,630
you encounter

1590
01:39:51,670 --> 01:39:57,880
in and they would an undirected graphs

1591
01:39:57,940 --> 01:40:01,270
in ICML two thousand eight two new

1592
01:40:01,270 --> 01:40:04,900
i will start today was very interesting phenomenon

1593
01:40:04,900 --> 01:40:08,110
which is known as beat phenomenon

1594
01:40:08,180 --> 01:40:11,070
suppose you have two simple harmonic motions

1595
01:40:11,150 --> 01:40:14,930
with the same amplitude but with different frequencies

1596
01:40:14,960 --> 01:40:17,500
so we have x one

1597
01:40:17,510 --> 01:40:19,540
is a

1598
01:40:19,540 --> 01:40:21,250
times cosine

1599
01:40:21,290 --> 01:40:23,620
only got one t

1600
01:40:23,650 --> 01:40:25,180
and you have access to

1601
01:40:25,190 --> 01:40:27,710
which is a

1602
01:40:27,720 --> 01:40:30,680
i'm gonna go only got two and the two are different

1603
01:40:30,730 --> 01:40:32,940
same amplitude

1604
01:40:32,960 --> 01:40:34,170
if you some them

1605
01:40:35,790 --> 01:40:38,300
one class x two

1606
01:40:38,360 --> 01:40:40,200
you get to a

1607
01:40:40,260 --> 01:40:42,700
times the cosine of half the sun

1608
01:40:42,830 --> 01:40:44,620
omega one

1609
01:40:44,670 --> 01:40:47,260
but only get divided by two

1610
01:40:47,260 --> 01:40:48,540
times t

1611
01:40:48,590 --> 01:40:49,920
times the cosine

1612
01:40:49,930 --> 01:40:53,330
of half the difference

1613
01:40:53,340 --> 01:40:57,700
and when you look at this

1614
01:40:57,730 --> 01:40:59,370
you have

1615
01:41:02,030 --> 01:41:05,750
a frequency which is high compared to

1616
01:41:05,760 --> 01:41:08,610
this frequency you can best see that

1617
01:41:08,620 --> 01:41:10,180
you only get one look

1618
01:41:10,200 --> 01:41:12,260
it's very close to omega two

1619
01:41:12,310 --> 01:41:15,620
you could simply replaced by omega

1620
01:41:15,620 --> 01:41:19,450
then this is simply saying cosine omega t

1621
01:41:19,450 --> 01:41:20,970
and so if you

1622
01:41:20,980 --> 01:41:25,180
chains from the notation of omega in you had to

1623
01:41:25,200 --> 01:41:28,890
frequency in hertz which is a lot easier to think of

1624
01:41:30,420 --> 01:41:31,370
two quite

1625
01:41:31,390 --> 01:41:33,560
time frequency f

1626
01:41:33,570 --> 01:41:35,290
this is the words

1627
01:41:35,340 --> 01:41:38,400
this is in radians per second suppose you have

1628
01:41:38,470 --> 01:41:39,640
and f one

1629
01:41:39,650 --> 01:41:43,030
which is two two hundred and fifty six per

1630
01:41:43,040 --> 01:41:45,190
and you have an f two

1631
01:41:45,230 --> 01:41:48,330
which is two hundred and fifty four

1632
01:41:48,380 --> 01:41:52,230
you take these numbers now is working example

1633
01:41:53,070 --> 01:41:58,640
the period of this oscillation is one two hundred fifty six of the second

1634
01:41:58,760 --> 01:42:02,610
but the period of this oscillation is only one second

1635
01:42:02,660 --> 01:42:06,760
because it's the difference between the two divided by two

1636
01:42:06,800 --> 01:42:08,450
so what happens is

1637
01:42:08,460 --> 01:42:10,570
that if you

1638
01:42:10,610 --> 01:42:12,660
make a plot

1639
01:42:12,710 --> 01:42:13,760
of the

1640
01:42:13,830 --> 01:42:17,040
of x as a function of time

1641
01:42:17,080 --> 01:42:19,140
i will first sketch

1642
01:42:19,140 --> 01:42:20,980
this is very slow

1643
01:42:22,290 --> 01:42:23,730
so this

1644
01:42:23,790 --> 01:42:25,460
is very slow

1645
01:42:25,510 --> 01:42:27,190
compared to this one

1646
01:42:27,200 --> 01:42:29,640
which is very fast

1647
01:42:29,690 --> 01:42:32,940
i will first make a sketch of of the very

1648
01:42:32,950 --> 01:42:38,420
slow one

1649
01:42:38,490 --> 01:42:40,360
this is the slower

1650
01:42:40,570 --> 01:42:45,070
and i will make also

1651
01:42:46,360 --> 01:42:47,880
the line here

1652
01:42:47,950 --> 01:42:51,140
guide my hand when i'm going to make it a lot

1653
01:42:51,960 --> 01:42:54,450
so what you see now is that the

1654
01:42:54,460 --> 01:42:57,450
the first rule she

1655
01:42:57,490 --> 01:43:02,040
this flow on goes like this

1656
01:43:02,100 --> 01:43:07,940
and the net was old is this

1657
01:43:07,950 --> 01:43:10,800
this is the fast one

1658
01:43:10,880 --> 01:43:12,550
fast one

1659
01:43:12,570 --> 01:43:15,520
and so you see right here

1660
01:43:15,570 --> 01:43:17,640
is when the slow one is zero

1661
01:43:17,700 --> 01:43:23,570
so this long kills the amplitude you can think of this as an amplitude modulation

1662
01:43:23,630 --> 01:43:25,070
that he h

1663
01:43:25,140 --> 01:43:26,850
is multiplied by that

1664
01:43:26,860 --> 01:43:29,880
cosine function

1665
01:43:29,890 --> 01:43:35,360
we defined as just a matter of definition we define this as the beat

1666
01:43:36,640 --> 01:43:38,610
when the two are

1667
01:43:38,610 --> 01:43:39,640
in phase

1668
01:43:39,640 --> 01:43:41,330
and here they are again

1669
01:43:41,380 --> 01:43:42,540
in face

1670
01:43:42,550 --> 01:43:46,990
the two a hundred eighty degrees out of phase we define this as the bit

1671
01:43:47,760 --> 01:43:52,350
for the working example that we have on the blackboard and two fifty six per

1672
01:43:52,350 --> 01:43:54,680
cent to fifty four

1673
01:43:54,770 --> 01:43:56,150
from here to here

1674
01:43:56,160 --> 01:44:00,000
it would be one second

1675
01:44:00,010 --> 01:44:03,350
that's the time for this slow one

1676
01:44:03,370 --> 01:44:06,320
go three hundred sixty degrees and so the bit

1677
01:44:06,350 --> 01:44:08,590
here we would in that case

1678
01:44:08,600 --> 01:44:10,840
the harvest second

1679
01:44:10,890 --> 01:44:13,890
so if you want to see in your head what is happening

1680
01:44:13,900 --> 01:44:16,880
then if one oscillation

1681
01:44:16,900 --> 01:44:19,600
is like this

1682
01:44:19,700 --> 01:44:21,900
then when they are in face

1683
01:44:21,900 --> 01:44:22,340
we should

1684
01:44:24,090 --> 01:44:24,900
give evidence before

1685
01:44:25,400 --> 01:44:26,340
is this really is

1686
01:44:26,940 --> 01:44:28,130
almost optimal

1687
01:44:29,940 --> 01:44:30,190
let me

1688
01:44:32,170 --> 01:44:33,530
right down the theorem forty

1689
01:44:36,400 --> 01:44:38,090
the length of the binary string

1690
01:44:41,150 --> 01:44:45,880
but i just mentioned a few minutes ago the one that we send the length about binary string

1691
01:44:48,440 --> 01:44:51,130
this article is the length of the encoding

1692
01:44:51,800 --> 01:44:54,400
if x one to x and its lesser equal to

1693
01:44:55,630 --> 01:44:57,000
the shannon information content

1694
01:44:57,880 --> 01:44:59,650
which is what shannon says ought to be

1695
01:45:01,250 --> 01:45:04,920
but absolutely perfect compression is less than or equal to the last two

1696
01:45:05,940 --> 01:45:11,780
so the entire length of the whole compressed file is only two bit longer and the optimal

1697
01:45:12,550 --> 01:45:14,510
length that shannon would like you have

1698
01:45:15,530 --> 01:45:18,940
and something to really emphasise there is when you compare that with what we could

1699
01:45:18,940 --> 01:45:20,710
do with simple codes for symbol codes

1700
01:45:21,190 --> 01:45:22,110
the length that you've got

1701
01:45:22,530 --> 01:45:25,210
character was with in one of the entropy

1702
01:45:25,960 --> 01:45:29,940
and so that means the possible overhead on a file with an characters in it

1703
01:45:30,170 --> 01:45:31,230
could be as big as an

1704
01:45:31,900 --> 01:45:32,610
and overhead event

1705
01:45:33,110 --> 01:45:33,920
which could be enormous

1706
01:45:34,460 --> 01:45:38,230
this is an overhead only two bits the entire file so it's a factory end

1707
01:45:38,230 --> 01:45:40,840
smaller than the overhead you get with simple codes

1708
01:45:43,210 --> 01:45:47,280
why is this the case what why is it that we end up with three

1709
01:45:47,280 --> 01:45:49,750
of length well if you just think about

1710
01:45:50,150 --> 01:45:52,230
the subdivision process that we're going through

1711
01:45:53,070 --> 01:45:53,650
the first

1712
01:45:54,900 --> 01:45:58,480
interval has size equal to the probability of the first character

1713
01:45:58,860 --> 01:45:59,400
given your model

1714
01:46:00,190 --> 01:46:02,780
then you subdivided in proportion to

1715
01:46:03,420 --> 01:46:05,480
the probability of the second character

1716
01:46:05,940 --> 01:46:07,050
given the first character

1717
01:46:07,590 --> 01:46:07,920
so his

1718
01:46:09,110 --> 01:46:10,780
the alphabet of possibilities

1719
01:46:11,210 --> 01:46:12,440
and the product of those two

1720
01:46:12,940 --> 01:46:17,340
is equal to the joint probability of x one and x two just by trivial

1721
01:46:18,170 --> 01:46:19,270
probability theory you

1722
01:46:20,320 --> 01:46:24,500
and then when you subdivided and you take the important size

1723
01:46:24,980 --> 01:46:25,690
is x one

1724
01:46:27,090 --> 01:46:29,070
and next to sell off its hair

1725
01:46:29,550 --> 01:46:30,360
you subdivide

1726
01:46:32,320 --> 01:46:34,460
and you say this is the probability of

1727
01:46:36,030 --> 01:46:40,070
divided in proportion to the probability vector given x one x two

1728
01:46:40,730 --> 01:46:44,500
if you multiply this length by that's fraction you get

1729
01:46:46,710 --> 01:46:49,280
the probability of x one x two x three

1730
01:46:49,800 --> 01:46:55,070
and so forth so these signs of the interval that you get on the left-hand side in the alphabet world

1731
01:46:56,150 --> 01:46:58,420
exactly the probability of the string

1732
01:47:00,670 --> 01:47:04,650
now when you find a binary interval that lines up with at interval and that

1733
01:47:04,800 --> 01:47:07,860
fits snugly inside it fitted perfectly snugly

1734
01:47:09,030 --> 01:47:11,750
then how many bits need for the binary

1735
01:47:12,210 --> 01:47:16,730
strength on each time you start with one and a half then a quarter and

1736
01:47:16,730 --> 01:47:19,590
so forth so the number of halving you go through is the log base two

1737
01:47:20,170 --> 01:47:24,820
of the size of the binary interval you end up with so if it fits exactly snugly

1738
01:47:26,710 --> 01:47:29,980
the length of the binary string will be it be exactly equal to

1739
01:47:30,670 --> 01:47:31,730
the number factors are to

1740
01:47:33,130 --> 01:47:33,570
but this

1741
01:47:35,010 --> 01:47:36,300
number here is small

1742
01:47:37,940 --> 01:47:39,460
which is log base two or whatever peak

1743
01:47:41,090 --> 01:47:41,900
first not fit

1744
01:47:42,960 --> 01:47:45,440
you will end up with a binary string and exactly the optimal length

1745
01:47:46,590 --> 01:47:48,500
in practice you won't quite have a snug fit

1746
01:47:48,960 --> 01:47:53,300
but it's an exercise in the book you can do the exercise and prove that

1747
01:47:53,500 --> 01:47:57,820
in the worst case the worst norms not fit you can end up with will

1748
01:47:57,820 --> 01:47:58,500
be that he

1749
01:47:59,270 --> 01:48:01,340
binary string is a factor of four

1750
01:48:02,980 --> 01:48:05,980
the alphabet string so you only need an extra two bits

1751
01:48:06,820 --> 01:48:08,320
to be guaranteed that you've found

1752
01:48:08,820 --> 01:48:10,270
a binary string lifts inside

1753
01:48:12,380 --> 01:48:15,010
so that's fact one

1754
01:48:17,880 --> 01:48:18,610
which is brilliant

1755
01:48:19,480 --> 01:48:21,070
andy node number two

1756
01:48:22,960 --> 01:48:27,110
the other thing i want you to know is how much work is it to do all this

1757
01:48:27,670 --> 01:48:32,190
what did we need to do did we need to think about all conceivable strings

1758
01:48:32,190 --> 01:48:33,570
the hub scores

1759
01:48:33,620 --> 01:48:37,930
for over here gets translated into authority scores over here and then is propagated back

1760
01:48:37,930 --> 01:48:42,760
so that for instance the hub score of that node has changed

1761
01:48:42,810 --> 01:48:46,390
and they have squad that no had some impact on what the hub score that

1762
01:48:46,390 --> 01:48:50,440
and that node will be as we do the propagation can in this back and

1763
01:48:50,440 --> 01:48:52,380
forth over the links

1764
01:48:55,170 --> 01:49:00,030
OK and we can also write it again in matrix notation just by

1765
01:49:00,060 --> 01:49:05,770
this time the matrix that we're looking at remembering the in the pagerank we really

1766
01:49:05,770 --> 01:49:12,210
had a markov chain so we had a markovian matrix is the hostage matrix here

1767
01:49:12,230 --> 01:49:16,240
we just use the adjacency matrix

1768
01:49:16,270 --> 01:49:21,150
it's a it's just the adjacency matrix and so on what we then can see

1769
01:49:21,150 --> 01:49:23,940
is that

1770
01:49:23,950 --> 01:49:28,660
the authority scores if multiply by the adjacency matrix we get something that should be

1771
01:49:28,660 --> 01:49:33,480
proportional to the hub scores and if we take perhaps causing multiply with the transpose

1772
01:49:33,480 --> 01:49:37,490
right because we are reversing the direction of the of the links is just taking

1773
01:49:37,490 --> 01:49:42,220
the transpose of the adjacency matrix we have that relationship

1774
01:49:44,260 --> 01:49:47,370
and now what we what we can do is

1775
01:49:47,370 --> 01:49:53,970
basically eliminate to analyse this further we can instance now eliminate

1776
01:49:54,930 --> 01:49:57,340
the hub scores in that scheme

1777
01:49:58,350 --> 01:50:00,670
and condense it in a single equation

1778
01:50:00,690 --> 01:50:05,860
which means right we take the authority score time step t minus one multiplied by

1779
01:50:05,870 --> 01:50:09,930
a that gives us the hub score we take the hub score multiplied by transpose

1780
01:50:09,940 --> 01:50:11,870
that gives us the new authorities

1781
01:50:11,870 --> 01:50:14,110
so we have the two steps in one

1782
01:50:14,130 --> 01:50:15,820
and this is now

1783
01:50:18,300 --> 01:50:23,820
almost and i can vector equation only that is just the proportionality we don't actually

1784
01:50:23,820 --> 01:50:29,140
know what the what i can value is and whether it's the same for for

1785
01:50:29,140 --> 01:50:33,260
every time step t we haven't really specified

1786
01:50:33,340 --> 01:50:34,690
OK but

1787
01:50:34,710 --> 01:50:38,260
what we can do for instance if we can ask ourselves well if we would

1788
01:50:38,310 --> 01:50:39,940
somehow normalise

1789
01:50:40,000 --> 01:50:42,590
let's say these scores so that the norm

1790
01:50:42,600 --> 01:50:45,830
it is always equal to one

1791
01:50:45,880 --> 01:50:49,140
you know does this scheme then converge toward some

1792
01:50:49,280 --> 01:50:52,250
the score vector for the authorities

1793
01:50:53,150 --> 01:50:55,430
for instance if we would start with

1794
01:50:55,450 --> 01:50:59,500
all the hub scores being initialize to one

1795
01:50:59,520 --> 01:51:02,150
OK and

1796
01:51:03,240 --> 01:51:05,410
so here's a little example

1797
01:51:05,940 --> 01:51:09,640
i don't know how much that helps but it's you know small graph with six

1798
01:51:12,050 --> 01:51:16,330
you see them over here is the adjacency matrix but you already see that in

1799
01:51:16,330 --> 01:51:21,380
that picture this is how a transpose and a transport a looks like and if

1800
01:51:21,380 --> 01:51:22,730
you don't do the

1801
01:51:22,760 --> 01:51:26,360
the perhaps another thirty computation

1802
01:51:26,370 --> 01:51:28,320
until convergence

1803
01:51:28,340 --> 01:51:32,730
you get for instance the scores for the authorities so you see that page one

1804
01:51:32,730 --> 01:51:34,050
and two

1805
01:51:39,070 --> 01:51:42,320
because you know page two

1806
01:51:42,340 --> 01:51:43,590
then we will

1807
01:51:44,450 --> 01:51:50,240
here web page to actually happens to get a zero hub scores well so it

1808
01:51:50,240 --> 01:51:54,670
doesn't inherit anything here and this is the only incoming edges both but actually end

1809
01:51:54,670 --> 01:52:00,510
up with an authority of zero the highest authority is page five here

1810
01:52:00,530 --> 01:52:03,700
we see this here in the second highest age three

1811
01:52:03,700 --> 01:52:08,740
and in terms of the authority scores on these are the authority scores page one

1812
01:52:08,740 --> 01:52:10,430
has the highest

1813
01:52:11,280 --> 01:52:15,050
you know it's kind of consistent with that because three and five have the highest

1814
01:52:15,050 --> 01:52:19,470
authority scores and one points to both of them right so one in that case

1815
01:52:19,470 --> 01:52:26,860
is good hop note although it has zero authority weight

1816
01:52:28,050 --> 01:52:35,010
OK let's just like two

1817
01:52:35,070 --> 01:52:41,650
maybe just skip over that so basically you can show that this that indeed

1818
01:52:41,670 --> 01:52:46,410
this makes infinity vector defined by these

1819
01:52:46,430 --> 01:52:56,970
by this recurrence relation is ultimately concerned giving converging and it's converging towards the dominant

1820
01:52:57,380 --> 01:53:01,050
i can vector of this matrix transpose a

1821
01:53:01,070 --> 01:53:09,180
under the assumption that basically the dominant igon value has the multiplicity of one

1822
01:53:09,200 --> 01:53:10,240
OK but

1823
01:53:11,470 --> 01:53:15,030
if we just move on here

1824
01:53:15,030 --> 01:53:19,650
and talk a little bit about how you know actually going to use that in

1825
01:53:19,650 --> 01:53:22,610
the in the context of of web search

1826
01:53:22,610 --> 01:53:25,900
OK now you could just

1827
01:53:25,930 --> 01:53:30,610
you could just say well why that's just use this perhaps an authority on them

1828
01:53:30,630 --> 01:53:32,280
on the web as a whole

1829
01:53:32,280 --> 01:53:33,970
but that was not really

1830
01:53:34,490 --> 01:53:39,590
the idea that time work was pursuing because he was interested remember we had initially

1831
01:53:39,590 --> 01:53:41,570
we're interested in this brought

1832
01:53:45,320 --> 01:53:50,570
and then we might be able to find a result set for these broad queries

1833
01:53:50,570 --> 01:53:53,610
and the results it is just you know very large

1834
01:53:53,610 --> 01:53:56,720
a lot of pages and we would like to find a way to rank these

1835
01:53:56,720 --> 01:54:02,360
pages so the idea that the strategy that he was suggesting is that we first

1836
01:54:02,550 --> 01:54:05,090
determine a subgraph of the web

1837
01:54:05,110 --> 01:54:09,900
and then run the hopson authority propagation algorithm on that subgraph

1838
01:54:09,930 --> 01:54:16,430
how's this subgraph determined well the way that determine is based on a specific query

1839
01:54:16,430 --> 01:54:21,470
OK so let me talk about just one more thing we did on this spoken

1840
01:54:21,470 --> 01:54:23,080
dialogue task

1841
01:54:23,080 --> 01:54:25,200
another challenge that we had is

1842
01:54:25,220 --> 01:54:27,720
o which comes up in a lot of applications

1843
01:54:27,790 --> 01:54:33,240
is that we have a huge source of unlabelled examples that getting the labels is

1844
01:54:33,240 --> 01:54:37,910
very expensive so in a single day might be able to gather thousands even tens

1845
01:54:37,910 --> 01:54:39,810
of thousands of

1846
01:54:39,850 --> 01:54:41,600
telephone calls

1847
01:54:41,640 --> 01:54:43,540
which are unlabelled examples

1848
01:54:43,560 --> 01:54:49,510
but then you have to have humans actually annotate those examples so getting those examples

1849
01:54:49,510 --> 01:54:53,810
getting those labels throughout rather is very expensive you can only afford to do a

1850
01:54:53,810 --> 01:54:55,740
few hundred every day

1851
01:54:55,760 --> 01:55:00,370
so the question is how to get the most information from the human how can

1852
01:55:00,370 --> 01:55:02,490
we make the best use

1853
01:55:02,510 --> 01:55:06,660
the human annotators that we have how can we choose

1854
01:55:07,910 --> 01:55:12,270
that are most informative for labeling

1855
01:55:13,120 --> 01:55:16,910
this is the problem often called active learning which comes up in a lot of

1856
01:55:18,060 --> 01:55:23,160
and we use the very standard technique here we use something called selective sampling which

1857
01:55:23,160 --> 01:55:26,160
means that two to select examples

1858
01:55:26,180 --> 01:55:31,660
in particular what we did is we focused on the least confident examples

1859
01:55:32,410 --> 01:55:34,060
but we need to do is we

1860
01:55:34,260 --> 01:55:37,680
we need to find the unlabelled examples

1861
01:55:37,700 --> 01:55:43,390
on which are classifier which we built is least confident and ask for the labels

1862
01:55:43,390 --> 01:55:45,220
on those examples

1863
01:55:45,240 --> 01:55:48,430
and this is the technique that goes back to lewis and gail

1864
01:55:48,450 --> 01:55:52,230
and so for how do you measure confidence while for boosting we already have a

1865
01:55:52,230 --> 01:55:56,660
way of measuring confidence we can just use the margins around the absolute value of

1866
01:55:56,660 --> 01:55:57,720
the margin

1867
01:55:57,740 --> 01:56:00,700
as the natural confidence measure

1868
01:56:00,740 --> 01:56:04,890
so here's the type of experiment that we did so we started out with a

1869
01:56:04,890 --> 01:56:06,930
pool of unlabeled examples

1870
01:56:06,990 --> 01:56:10,850
and then we choose maybe five hundred examples at ran for labeling just get the

1871
01:56:10,850 --> 01:56:12,530
whole thing started

1872
01:56:12,540 --> 01:56:15,240
and then we what we do is we run boosting

1873
01:56:15,290 --> 01:56:17,430
and all of the labelled examples

1874
01:56:17,490 --> 01:56:21,220
to get the combined classifier as and then we pick

1875
01:56:21,260 --> 01:56:24,700
maybe two hundred fifty additional examples from the pool

1876
01:56:24,760 --> 01:56:25,990
for labeling

1877
01:56:26,010 --> 01:56:30,090
and the way we choose the messages the examples that were least confident about the

1878
01:56:30,090 --> 01:56:31,950
ones with minimum

1879
01:56:32,890 --> 01:56:35,350
this turns out to be the margin

1880
01:56:35,410 --> 01:56:37,390
and then we repeat which is another

1881
01:56:37,390 --> 01:56:39,200
two hundred fifty examples

1882
01:56:39,220 --> 01:56:42,740
we train and so on

1883
01:56:42,740 --> 01:56:46,010
so here's the kind of performance they get doing that

1884
01:56:46,010 --> 01:56:49,560
so this is plotting the error rate the test error rate

1885
01:56:49,560 --> 01:56:55,080
as a function of the number of labelled examples that we have

1886
01:56:56,260 --> 01:57:01,220
and the red curve is showing what happens if you just use those examples random

1887
01:57:02,310 --> 01:57:06,600
and the blue curve is showing what happens the beaches those examples for labeling

1888
01:57:06,620 --> 01:57:10,240
using this kind of active technique that i talk about

1889
01:57:10,290 --> 01:57:11,990
so again you see this

1890
01:57:12,040 --> 01:57:17,240
huge speedups this huge improvement in terms of number of examples that you actually need

1891
01:57:17,260 --> 01:57:18,930
so for instance to get

1892
01:57:18,950 --> 01:57:21,200
narrator twenty six percent

1893
01:57:21,200 --> 01:57:25,740
if you just using examples are random you need about twenty two thousand examples

1894
01:57:25,790 --> 01:57:29,560
with boosting you only need about ninety five hundred examples so it's like a two

1895
01:57:29,560 --> 01:57:31,620
to one speed up

1896
01:57:31,640 --> 01:57:33,830
what's also interesting about this curve

1897
01:57:33,870 --> 01:57:37,430
is the fact that this blue curve is active curves

1898
01:57:37,450 --> 01:57:40,700
goes down and then comes back up again

1899
01:57:40,760 --> 01:57:45,890
you would never expect to see this with random examples with random examples more data

1900
01:57:45,890 --> 01:57:47,430
is always better

1901
01:57:47,450 --> 01:57:51,010
but this is showing that more data

1902
01:57:51,100 --> 01:57:53,740
it's actually worse than last data

1903
01:57:53,790 --> 01:57:57,060
that somehow these examples that were added out here

1904
01:57:57,080 --> 01:57:58,540
are actually

1905
01:57:58,560 --> 01:58:07,160
destructive there actually dis informative and you actually do better by ignoring those examples

1906
01:58:07,220 --> 01:58:10,260
OK and this is just another dataset

1907
01:58:10,270 --> 01:58:16,720
OK so let me just talk about one final application o o you question

1908
01:58:17,910 --> 01:58:31,470
you try to get to the point that we saw

1909
01:58:42,910 --> 01:58:43,870
oh i see

1910
01:58:43,890 --> 01:58:47,530
so so you need to know something about the experiment that was actually done here

1911
01:58:47,530 --> 01:58:49,120
which is that we had

1912
01:58:49,140 --> 01:58:53,490
for this experiment we were starting with a fixed pool of forty thousand

1913
01:58:54,760 --> 01:58:58,240
so but you could do something similar to what you're talking about so we could

1914
01:58:58,240 --> 01:58:59,310
do is you could

1915
01:59:00,390 --> 01:59:02,530
try to predict somehow

1916
01:59:02,530 --> 01:59:04,850
where this minimum point is

1917
01:59:04,870 --> 01:59:08,310
and then you could just stop at that point

1918
01:59:08,330 --> 01:59:13,100
you could just stop at that point and people calling conan so and had a

1919
01:59:13,100 --> 01:59:18,870
paper using support vector machines rather than boosting but they observed this exact same thing

1920
01:59:19,680 --> 01:59:23,740
with active learning you can actually do better with less examples and they try to

1921
01:59:23,740 --> 01:59:25,240
use techniques to

1922
01:59:25,270 --> 01:59:29,220
try to predict where the lowest point was if i'm remembering correctly

1923
01:59:29,260 --> 01:59:33,160
where the lowest point wasn't stopping at that point

1924
01:59:33,220 --> 01:59:34,720
so if you have an

1925
01:59:34,720 --> 01:59:38,310
infinite amount of data and presumably would not

1926
01:59:38,390 --> 01:59:42,390
see this type of phenomenon even with active learning you would see things just dropping

1927
01:59:42,390 --> 01:59:44,390
dropping dropping but

1928
01:59:44,410 --> 01:59:47,720
i can't do the experiment because i don't have an infinite amount of data

1929
01:59:52,350 --> 01:59:54,970
this is out of sample this is tester

1930
01:59:55,030 --> 01:59:57,120
this is all tester

1931
02:00:03,010 --> 02:00:09,140
i don't completely know i wouldn't call it overfitting

1932
02:00:09,140 --> 02:00:11,660
it's not really overfitting because

1933
02:00:12,910 --> 02:00:15,760
so each of these is the different run

1934
02:00:15,770 --> 02:00:20,970
a boosting every two hundred fifty examples getting differ run boosting

1935
02:00:22,080 --> 02:00:25,640
what's being plotted here is the number of examples

