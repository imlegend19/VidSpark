1
00:00:00,000 --> 00:00:02,500
in the direction of k

2
00:00:02,550 --> 00:00:06,400
and this line happens to be the line where the field at this moment in

3
00:00:06,400 --> 00:00:08,840
time is the maximum and pointing

4
00:00:08,880 --> 00:00:11,020
in new directions

5
00:00:11,040 --> 00:00:12,820
so it is crests

6
00:00:12,860 --> 00:00:15,130
in e

7
00:00:15,130 --> 00:00:22,360
the mountain coming out of the board maximum value

8
00:00:24,210 --> 00:00:29,480
i draw not aligned perpendicular to are

9
00:00:29,480 --> 00:00:33,460
so you can build are

10
00:00:36,020 --> 00:00:38,070
the dot product is zero

11
00:00:38,130 --> 00:00:41,540
are is perpendicular to k

12
00:00:41,540 --> 00:00:43,880
for this point

13
00:00:43,900 --> 00:00:46,960
i will also assume that the effect here

14
00:00:47,000 --> 00:00:54,690
is the maximum pointing a new direction and then there is one here

15
00:00:54,710 --> 00:00:57,480
OK there are here

16
00:00:57,520 --> 00:01:00,670
is therefore by

17
00:01:00,710 --> 00:01:03,590
and cables are used for

18
00:01:03,630 --> 00:01:06,610
because this now represents a wave

19
00:01:06,610 --> 00:01:07,880
this is the form e

20
00:01:09,360 --> 00:01:11,980
of the wave where he is maximum

21
00:01:12,070 --> 00:01:19,270
new direction is the maximum new direction is maximum you interaction

22
00:01:19,400 --> 00:01:22,480
in other words this here

23
00:01:22,520 --> 00:01:24,050
is by definition

24
00:01:24,090 --> 00:01:25,900
of labor

25
00:01:25,920 --> 00:01:28,290
and this whole thing moves out

26
00:01:28,340 --> 00:01:30,130
in space was

27
00:01:30,190 --> 00:01:36,000
speed c

28
00:01:36,040 --> 00:01:41,110
so you see the lines can build are perpendicular to k

29
00:01:43,250 --> 00:01:45,840
the maximum of the effect this

30
00:01:45,860 --> 00:01:50,210
a moment in time and all that starts to move out

31
00:01:50,360 --> 00:01:52,000
now you go to the

32
00:01:52,050 --> 00:01:54,070
the third dimension

33
00:01:54,150 --> 00:01:58,230
then came the world are is constant i now no longer lines but they are

34
00:02:00,090 --> 00:02:03,630
perpendicular to the vector k

35
00:02:03,690 --> 00:02:07,630
and this whole plane perpendicular to the vector k

36
00:02:07,670 --> 00:02:12,900
in the whole plane at the moment in time the vector is everywhere the same

37
00:02:13,000 --> 00:02:19,340
what it is linearly polarized whether this surgery polarized when it's elliptical polarized is irrelevant

38
00:02:19,340 --> 00:02:21,880
it's everywhere the same

39
00:02:21,940 --> 00:02:26,550
and the whole plane then moved out which this speed of light

40
00:02:26,610 --> 00:02:28,190
in that direction

41
00:02:28,210 --> 00:02:30,880
of k

42
00:02:30,920 --> 00:02:34,540
so can build are in three dimensions i all

43
00:02:34,550 --> 00:02:37,320
planes perpendicular to the the vector

44
00:02:40,590 --> 00:02:43,820
so if you stand anywhere in space

45
00:02:43,880 --> 00:02:49,130
and you look in the direction where the radiation is coming from

46
00:02:49,170 --> 00:02:52,480
that in any plane that is perpendicular to k

47
00:02:52,540 --> 00:02:55,320
go to infinity there there to there

48
00:02:55,320 --> 00:02:59,000
at any moment in time the vector is the same and the vector is the

49
00:03:00,400 --> 00:03:04,860
and i said that is linearly polarized weighty issues or circular or elliptical

50
00:03:05,900 --> 00:03:07,610
is a different matter it could be

51
00:03:07,630 --> 00:03:11,150
either one of those

52
00:03:11,190 --> 00:03:14,070
so this is the best way that you can think of the

53
00:03:17,050 --> 00:03:18,770
forum of a

54
00:03:18,840 --> 00:03:20,290
the vector going in

55
00:03:20,310 --> 00:03:22,070
three dimensions

56
00:03:22,090 --> 00:03:25,790
these and in planes perpendicular to the direction of propagation

57
00:03:25,880 --> 00:03:28,250
in our first case

58
00:03:28,270 --> 00:03:30,820
which is also very simple

59
00:03:30,860 --> 00:03:33,920
because i wanted to warm you up slowly

60
00:03:33,940 --> 00:03:35,480
OK are

61
00:03:35,500 --> 00:03:37,590
it became simply casey

62
00:03:37,710 --> 00:03:40,920
o OK x will zero

63
00:03:42,290 --> 00:03:43,630
OK while zero

64
00:03:43,690 --> 00:03:48,290
because it was only going in the direction casey was the only one which was

65
00:03:48,290 --> 00:03:50,540
not zero which was k

66
00:03:50,570 --> 00:03:56,000
so my dot product k delta are collapses into a casey

67
00:03:56,040 --> 00:04:01,110
and so the wavelength lambda is two pi divided by this case

68
00:04:01,190 --> 00:04:04,770
but if of course if you have a three-dimensional case the the situation

69
00:04:04,790 --> 00:04:06,750
is a little bit more complicated

70
00:04:06,770 --> 00:04:08,130
because then k

71
00:04:08,150 --> 00:04:10,050
it is the

72
00:04:10,070 --> 00:04:11,380
square root

73
00:04:11,460 --> 00:04:12,650
as you see here

74
00:04:12,650 --> 00:04:16,840
of xkx scrapers carey's purpose cases career

75
00:04:16,880 --> 00:04:18,500
so our first case

76
00:04:19,340 --> 00:04:22,440
to make it simple for you

77
00:04:22,440 --> 00:04:25,770
this is the right moment to stop

78
00:04:25,790 --> 00:04:26,750
after the

79
00:04:26,790 --> 00:04:28,570
break we can

80
00:04:28,630 --> 00:04:29,840
look at some

81
00:04:29,860 --> 00:04:31,500
demonstrations also

82
00:04:31,520 --> 00:04:34,440
so you have to relax a little to digest all this

83
00:04:34,440 --> 00:04:36,210
this is not easy

84
00:04:36,270 --> 00:04:39,420
so let's start handing out this many quiz

85
00:04:39,440 --> 00:04:42,820
to make you feel good about yourself made it easy this time

86
00:04:43,000 --> 00:04:48,650
don't start yet

87
00:04:48,670 --> 00:04:57,290
you start handing out

88
00:04:57,290 --> 00:05:01,040
i put here on the blackboard

89
00:05:01,070 --> 00:05:02,110
x y

90
00:05:05,630 --> 00:05:08,320
this is an electromagnetic wave

91
00:05:08,380 --> 00:05:13,880
going in this direction perpendicular to the direction of k

92
00:05:13,900 --> 00:05:15,630
so this is

93
00:05:15,690 --> 00:05:17,880
definition of wavelengths

94
00:05:17,880 --> 00:05:20,630
this is where e is the maximum in your direction and this is where the

95
00:05:20,630 --> 00:05:22,380
maximum in your direction

96
00:05:22,400 --> 00:05:27,540
and so this is the wavelength

97
00:05:27,630 --> 00:05:28,710
now look

98
00:05:28,710 --> 00:05:30,420
at the intersection

99
00:05:30,460 --> 00:05:32,040
of these wave

100
00:05:32,050 --> 00:05:34,570
in the y axis

101
00:05:34,630 --> 00:05:36,040
this wave

102
00:05:37,980 --> 00:05:41,380
and it intersects there

103
00:05:41,570 --> 00:05:44,360
so the distance from here to here

104
00:05:44,400 --> 00:05:45,610
is l y

105
00:05:45,610 --> 00:05:48,550
which is way larger than life

106
00:05:48,670 --> 00:05:52,790
and the same is true for the distance in the x direction is also larger

107
00:05:52,790 --> 00:05:54,190
than that

108
00:05:54,210 --> 00:05:57,150
and in the z direction in general it would also

109
00:05:57,150 --> 00:06:00,130
the larger than lambda

110
00:06:00,150 --> 00:06:04,130
now it is wave moves with the speed of c

111
00:06:04,320 --> 00:06:06,500
and and when it has moved

112
00:06:06,540 --> 00:06:08,380
the distance

113
00:06:10,590 --> 00:06:12,940
in the UK direction

114
00:06:12,940 --> 00:06:14,880
this crest

115
00:06:14,880 --> 00:06:18,810
is here

116
00:06:18,810 --> 00:06:22,500
how far has moved in the y direction

117
00:06:22,540 --> 00:06:24,860
all the way from here

118
00:06:24,880 --> 00:06:26,900
two there

119
00:06:26,900 --> 00:06:32,710
so it's a brief in the y direction is larger than c

120
00:06:32,820 --> 00:06:36,520
so that's been which we call the face

121
00:06:38,320 --> 00:06:39,790
the phase velocity

122
00:06:39,790 --> 00:06:41,290
in the y direction

123
00:06:41,320 --> 00:06:43,540
it's very simple is l

124
00:06:43,550 --> 00:06:45,070
divided by lab

125
00:06:45,070 --> 00:06:46,670
time sink

126
00:06:46,690 --> 00:06:49,520
and that is larger than c

127
00:06:49,540 --> 00:06:52,170
it follows immediately from the geometry

128
00:06:52,190 --> 00:06:55,940
because this angle here is the same as this angle there

129
00:06:55,960 --> 00:06:58,360
this is also OK

130
00:06:58,400 --> 00:07:00,880
divided by KY

131
00:07:00,900 --> 00:07:01,920
five c

132
00:07:02,840 --> 00:07:05,270
it's not only larger than c

133
00:07:05,270 --> 00:07:06,360
but it can be

134
00:07:06,380 --> 00:07:10,590
when larger than c

135
00:07:13,150 --> 00:07:15,130
two pi

136
00:07:15,170 --> 00:07:17,500
divided by l of y

137
00:07:17,520 --> 00:07:19,770
i refuse to call and of y

138
00:07:19,770 --> 00:07:22,320
let the y

139
00:07:22,320 --> 00:07:27,520
i don't want to have to think in terms of the wavelength this direction wavelength

140
00:07:27,540 --> 00:07:30,860
in this direction and of the wavelength in that direction for me

141
00:07:30,880 --> 00:07:37,750
there is only one wavelength and that wavelength is two pi divided by k

142
00:07:37,860 --> 00:07:43,670
so i refuse to call this labor of y

143
00:07:43,710 --> 00:07:48,570
the k lies two pi divided by that of y

144
00:07:48,630 --> 00:07:52,380
and you can do the same of course in the x in this he direction

145
00:07:52,440 --> 00:07:53,670
and you will find

146
00:07:53,670 --> 00:07:55,250
that the phase velocity

147
00:07:55,270 --> 00:07:57,210
in the same direction

148
00:07:59,360 --> 00:08:00,500
divided by

149
00:08:00,500 --> 00:08:02,460
kx time c

150
00:08:02,520 --> 00:08:03,980
and the phase direction

151
00:08:04,000 --> 00:08:05,380
in the same direction

152
00:08:05,380 --> 00:08:06,210
in this case

153
00:08:06,230 --> 00:08:07,770
divided by k

154
00:08:07,790 --> 00:08:09,710
c five c

155
00:08:09,730 --> 00:08:15,090
and what is the phase direction phase velocity in the direction k

156
00:08:15,150 --> 00:08:16,250
that is k

157
00:08:16,250 --> 00:08:17,820
divided by k

158
00:08:17,820 --> 00:08:18,770
time c

159
00:08:18,790 --> 00:08:19,710
that is c

160
00:08:19,710 --> 00:08:30,700
it is

161
00:08:40,240 --> 00:08:42,840
on the wall

162
00:09:04,520 --> 00:09:06,630
one of the

163
00:09:06,680 --> 00:09:11,090
before that

164
00:09:33,980 --> 00:09:36,030
this is something

165
00:09:36,520 --> 00:09:45,440
you get what you have

166
00:09:49,610 --> 00:09:52,180
a one of

167
00:09:52,200 --> 00:09:54,610
three of these

168
00:10:17,450 --> 00:10:19,970
you know nothing

169
00:10:20,310 --> 00:10:22,730
one of them

170
00:10:22,780 --> 00:10:25,450
o thirty five

171
00:10:25,470 --> 00:10:32,440
one of these different

172
00:10:32,450 --> 00:10:37,360
that's why

173
00:10:42,240 --> 00:10:46,820
so the government

174
00:10:48,320 --> 00:10:51,130
on the way to

175
00:11:29,270 --> 00:11:32,160
all of

176
00:11:32,160 --> 00:11:33,530
you know

177
00:11:33,540 --> 00:11:37,770
cancer patients of different stages right so

178
00:11:38,340 --> 00:11:39,580
we might

179
00:11:40,480 --> 00:11:45,950
interested in summarizing data so we might use the hierarchical clustering as a way of

180
00:11:46,000 --> 00:11:48,330
finding the clusters in the data

181
00:11:48,380 --> 00:11:51,220
and that is still representing the data itself

182
00:11:51,240 --> 00:11:52,500
we simply

183
00:11:52,540 --> 00:11:56,540
represent each data by which cluster it belongs to

184
00:11:56,570 --> 00:12:00,040
so we don't really sometimes they don't really care about the

185
00:12:02,460 --> 00:12:04,700
i love the detailed information about

186
00:12:04,750 --> 00:12:06,030
about the data

187
00:12:06,040 --> 00:12:12,270
about the data points by simply where it is in generally space

188
00:12:13,590 --> 00:12:17,190
the reason why why why we might be interesting hierarchical clustering is

189
00:12:17,260 --> 00:12:19,090
that we don't believe that

190
00:12:19,620 --> 00:12:22,640
they come came from some sort of hierarchical

191
00:12:23,040 --> 00:12:25,270
underlying tree structure

192
00:12:25,280 --> 00:12:26,200
and we

193
00:12:26,200 --> 00:12:30,350
actually want to we actually interested in recovering what is

194
00:12:30,370 --> 00:12:34,120
the underlying tree structure which give rise to the data so this is

195
00:12:34,560 --> 00:12:36,780
this is an example

196
00:12:36,830 --> 00:12:39,250
examples of this things like

197
00:12:39,290 --> 00:12:42,890
the the language tree example and phylogenetic tree

198
00:12:44,030 --> 00:12:46,160
oh by the way if there's any questions to

199
00:12:46,210 --> 00:12:47,970
ask questions

200
00:12:51,490 --> 00:12:52,650
so this

201
00:12:52,700 --> 00:12:54,080
a number of

202
00:12:54,140 --> 00:12:55,830
jet that's of

203
00:12:56,080 --> 00:12:59,830
so there are many different types of hierarchical clustering algorithms

204
00:12:59,850 --> 00:13:06,370
and they can generally be split into a different sort of approaches they could this

205
00:13:06,410 --> 00:13:09,310
described in terms of different dimensions

206
00:13:09,390 --> 00:13:13,720
to describe the algorithms so one dimension is in terms of how you construct your

207
00:13:16,580 --> 00:13:18,200
that's true

208
00:13:18,200 --> 00:13:20,830
general approach is one is the top down approach

209
00:13:20,830 --> 00:13:23,010
and one is the bottom up approach

210
00:13:23,040 --> 00:13:27,470
in the top down approach we start off by saying that all of data points

211
00:13:27,580 --> 00:13:29,400
belong to one single cluster

212
00:13:29,450 --> 00:13:30,500
and then we're gonna

213
00:13:30,520 --> 00:13:36,290
we consider recursively split this cluster into smaller and smaller clusters until we decide

214
00:13:36,310 --> 00:13:38,190
until we did decide to stop

215
00:13:40,220 --> 00:13:42,820
the in the bottom-up approach which is

216
00:13:43,080 --> 00:13:46,120
also called agglomerative

217
00:13:46,520 --> 00:13:48,630
we start off with one

218
00:13:48,640 --> 00:13:52,250
cluster for every data point so we start with the clusters

219
00:13:52,260 --> 00:13:55,890
and we iteratively find two clusters too much

220
00:13:55,890 --> 00:13:59,410
and we can't keep on merging the clusters until we get one big cluster for

221
00:13:59,410 --> 00:14:00,340
the whole data

222
00:14:01,140 --> 00:14:03,390
the whole dataset

223
00:14:03,440 --> 00:14:04,680
so it turns out to the

224
00:14:04,690 --> 00:14:06,850
dominant approach is bottom one

225
00:14:06,890 --> 00:14:12,340
because it turns out to be much easier to construct to find good trees

226
00:14:12,350 --> 00:14:15,940
when you start off with the bottom-up rather than top-down

227
00:14:18,990 --> 00:14:22,780
i guess one way to think about this is that

228
00:14:22,840 --> 00:14:25,150
when you start off with one big cluster

229
00:14:25,210 --> 00:14:26,660
and you want to

230
00:14:26,720 --> 00:14:29,270
two split into two smaller clusters

231
00:14:29,290 --> 00:14:35,020
without knowing the internal structure of the clusters is very hard to decide how to

232
00:14:35,020 --> 00:14:38,060
split that one the cluster into smaller clusters

233
00:14:38,110 --> 00:14:42,470
another way of looking at is as follows if you have one cluster with an

234
00:14:42,500 --> 00:14:43,640
data points

235
00:14:44,500 --> 00:14:47,710
to find the optimal split into two sub clusters

236
00:14:47,730 --> 00:14:50,070
you need to search through

237
00:14:50,630 --> 00:14:51,890
how many

238
00:14:51,940 --> 00:14:54,940
how many possible splits things exponential

239
00:14:55,290 --> 00:14:57,680
two today and number of possible splits

240
00:14:59,350 --> 00:15:03,560
one cluster into two smaller clusters

241
00:15:03,620 --> 00:15:04,500
but in the

242
00:15:04,510 --> 00:15:05,350
OK so

243
00:15:05,750 --> 00:15:07,520
the bottom-up approach

244
00:15:07,530 --> 00:15:11,710
if you have an clusters and you want to decide which two cluster too much

245
00:15:11,710 --> 00:15:15,350
there's only a quadratic number of them so is much easier

246
00:15:15,390 --> 00:15:17,510
it's much easier to search for the

247
00:15:17,550 --> 00:15:20,300
locally optimal health clusters too much

248
00:15:20,310 --> 00:15:22,120
then to search for

249
00:15:22,140 --> 00:15:23,650
the locally optimal

250
00:15:23,660 --> 00:15:27,430
it's split of one cluster into two

251
00:15:32,250 --> 00:15:38,710
yes i will come to that in fact you could

252
00:15:41,390 --> 00:15:43,600
many of the self agglomerative

253
00:15:43,610 --> 00:15:45,190
clustering approaches

254
00:15:45,760 --> 00:15:50,940
the way they differ is in terms of what distance measure used

255
00:15:50,960 --> 00:15:54,530
and how you define distances on clusters

256
00:15:54,690 --> 00:15:57,390
given distances on data points

257
00:16:03,690 --> 00:16:07,790
it doesn't have to be split one to two but could be split one into

258
00:16:11,030 --> 00:16:13,570
choosing the number is

259
00:16:15,100 --> 00:16:21,390
well defence still so many different ways

260
00:16:21,390 --> 00:16:24,000
so choosing the i guess you could

261
00:16:26,530 --> 00:16:30,990
if you are bayesian like me then you use things like dirichlet process mixtures

262
00:16:35,100 --> 00:16:35,940
when you

263
00:16:35,960 --> 00:16:40,100
split data point to multiple clusters and you don't use

264
00:16:40,140 --> 00:16:43,730
some penalty for me if you have more clusters than in the sense you have

265
00:16:43,730 --> 00:16:46,760
more use more parameters to describe the data

266
00:16:46,760 --> 00:16:52,450
and if you don't use something some sort of penalty to penalize for a larger

267
00:16:52,450 --> 00:16:53,440
number of

268
00:16:53,720 --> 00:16:55,620
of clusters then

269
00:16:55,630 --> 00:16:57,270
the best way of

270
00:16:57,310 --> 00:17:01,260
describing the data is simply to every data point being its on cost

271
00:17:01,260 --> 00:17:04,470
so then that's that's different ways of of

272
00:17:06,840 --> 00:17:10,000
for the number of parameters they've gotten one way is to

273
00:17:10,010 --> 00:17:11,530
these businesses

274
00:17:11,940 --> 00:17:14,230
i think tomorrow

275
00:17:14,270 --> 00:17:16,300
maybe was so soon

276
00:17:18,190 --> 00:17:21,350
it called some

277
00:17:21,350 --> 00:17:29,020
good so much for linguistic values variables and because can of course now only want

278
00:17:29,020 --> 00:17:34,730
to model those types of membership functions in in the computer in some sense system

279
00:17:34,730 --> 00:17:39,130
that derives them automatically from data we want to manipulate them using some clever algorithms

280
00:17:39,660 --> 00:17:43,770
usually don't want to model arbitrary functions of arbitrary shape so the couple of classical

281
00:17:43,770 --> 00:17:49,310
examples that people often use the most commonly used is the so-called property which is

282
00:17:49,310 --> 00:17:52,270
defined by four numbers so all you need to do is to store these four

283
00:17:52,270 --> 00:17:55,890
numbers ABC which sort of the left

284
00:17:55,910 --> 00:18:01,330
left corner office repetetive the degree of membership starts being larger than zero then be

285
00:18:01,350 --> 00:18:05,210
used there are sort of the area starts where the degree of membership is one

286
00:18:05,250 --> 00:18:11,510
and then it declines until c and then it declines it's going to property it

287
00:18:11,510 --> 00:18:15,570
that's nice it's very easy to manipulate it's enormously easy to calculate

288
00:18:15,590 --> 00:18:19,370
degrees of membership but if you wanted to use algorithms i'll get back to that

289
00:18:19,370 --> 00:18:23,470
later but if you wanted to modify these things sort of fit data that a

290
00:18:23,470 --> 00:18:27,190
little bit like the ones i just mentioned networks of for example using something like

291
00:18:27,190 --> 00:18:31,910
gradient descent they need to compute a derivative this is really not it would made

292
00:18:32,270 --> 00:18:36,810
to model things so you may want to do something like that goes costs bell-shaped

293
00:18:36,810 --> 00:18:42,110
curves they you model the degree of membership using such for fear of being standard

294
00:18:42,210 --> 00:18:46,930
the to sort of describe rates it's a wider this and you could actually use

295
00:18:46,930 --> 00:18:51,250
compute derivatives do is sort a gradient descent try to minimise are so in cases

296
00:18:51,250 --> 00:18:56,090
where you want to use sort of mathematical optimisation methods this may be a better

297
00:18:57,590 --> 00:19:02,270
still reasonably easy to compute a special case of the property it will get come

298
00:19:02,270 --> 00:19:05,350
back to that when we talk about fuzzy numbers are the ones that have no

299
00:19:05,350 --> 00:19:12,310
plateau but just the peak you have eighty indians it's sort of like property price

300
00:19:12,350 --> 00:19:15,310
and then of course we have the case singletons instead of just talking about the

301
00:19:15,310 --> 00:19:20,490
degree of membership of one particular that it's like that with a belonging

302
00:19:20,510 --> 00:19:24,710
to this set to a degree of one just listing sort of the the elements

303
00:19:24,710 --> 00:19:28,870
and how much what kind of degree of membership to have don't usually use that

304
00:19:28,870 --> 00:19:30,860
if you don't have a continuous domain

305
00:19:31,050 --> 00:19:36,950
OK so this are some of the most commonly used methods to model fuzzy membership

306
00:19:38,210 --> 00:19:43,090
if you wanted to manipulate them including to invent anything yourself of course

307
00:19:43,110 --> 00:19:50,890
OK just a few definitions because it will pop up more often later they are

308
00:19:50,930 --> 00:19:55,390
two things how you can describe the membership function

309
00:19:55,570 --> 00:19:58,630
are very peaks and so the very fact about which are the support of the

310
00:19:58,630 --> 00:20:04,570
core the support of all elements having non-zero degree of membership the support essentially all

311
00:20:04,570 --> 00:20:05,970
the way from here

312
00:20:06,070 --> 00:20:11,470
so if you like that sort of elements all the elements that belong just beat

313
00:20:11,470 --> 00:20:16,390
to the set are in this the core of the sets of elements that actually

314
00:20:16,390 --> 00:20:18,890
have a degree of one so they need to be some and you lose all

315
00:20:18,900 --> 00:20:23,830
the elements that we definitely belong to that set and then things like alpha cuts

316
00:20:24,010 --> 00:20:28,970
which are all elements that have a degree of membership at least alpha above all

317
00:20:28,970 --> 00:20:32,810
subsequent sort of define an alpha somewhere here and then literally cut through that's the

318
00:20:32,990 --> 00:20:34,570
biggest name comes from and then the

319
00:20:35,150 --> 00:20:39,030
all the elements would be all the elements on this line and then the height

320
00:20:39,030 --> 00:20:42,950
which is the maximum degree of membership for this particular fuzzy sets fuzzy sets don't

321
00:20:42,950 --> 00:20:47,370
need to necessarily have elements that belong to this question

322
00:20:48,060 --> 00:20:52,780
i want to you could also imagine the separatists' all the elements that have the

323
00:20:52,790 --> 00:20:56,320
two but none really belongs to to a degree of one so the height doesn't

324
00:20:56,320 --> 00:20:57,960
necessarily have to be

325
00:20:59,410 --> 00:21:04,330
so i'll be talking about support and cornell did more so the question is to

326
00:21:04,330 --> 00:21:08,450
draw the traditional dress remembering

327
00:21:08,490 --> 00:21:14,470
in the core disappeared from alpha cuts specific alpha

328
00:21:15,180 --> 00:21:18,340
and then of course the height is in this case it's one that could be

329
00:21:18,360 --> 00:21:24,320
and he OK so now the interesting question is if you have fuzzy sets a

330
00:21:24,360 --> 00:21:30,100
fuzzy set that describes old people who has has describes tall people and the trying

331
00:21:30,100 --> 00:21:31,200
to figure out so

332
00:21:31,220 --> 00:21:34,880
an element that has a degree of membership to the set of old people of

333
00:21:34,880 --> 00:21:38,480
point seven and has a degree of the same element has a degree of membership

334
00:21:38,480 --> 00:21:42,160
to the government to the set of tall people to a degree of five what's

335
00:21:42,170 --> 00:21:46,140
the degree of membership to the set of old and tall people were trying to

336
00:21:46,140 --> 00:21:52,420
do things like conjunctions fuzzy distractions normal set operators so what would that be i

337
00:21:52,420 --> 00:21:53,900
mean you can of course

338
00:21:53,940 --> 00:21:58,300
some rule of thumb say well i mean in that case maybe sort of the

339
00:21:58,300 --> 00:22:00,700
average point six may make sense

340
00:22:00,740 --> 00:22:05,260
then you can all point six cents successfully and so it should be higher than

341
00:22:05,260 --> 00:22:06,900
any of these little

342
00:22:06,920 --> 00:22:11,680
degrees of membership probably selection from not be not more than point five percent of

343
00:22:11,740 --> 00:22:15,640
advice you add constraints you want not only

344
00:22:15,920 --> 00:22:19,840
the tall people but if you want to talk and all people in the seventies

345
00:22:19,840 --> 00:22:23,680
with belong to the degree to the set of old and tall people to a

346
00:22:23,680 --> 00:22:25,320
goes to zero

347
00:22:25,370 --> 00:22:27,090
as you approach

348
00:22:27,110 --> 00:22:29,050
this is for you to be bold

349
00:22:30,420 --> 00:22:31,760
you can see it

350
00:22:31,820 --> 00:22:35,990
now actually as opposed to zero of the square the gradient

351
00:22:36,010 --> 00:22:40,720
it becomes more shallow it into multiplying

352
00:22:40,730 --> 00:22:44,630
if you want to play by the gradient it means that as because emergence the

353
00:22:44,630 --> 00:22:48,480
green gives zero here because the average degree does not zero

354
00:22:48,510 --> 00:22:54,210
so just from the properties of optimizing it is easier to optimize the

355
00:22:54,260 --> 00:22:57,720
squared error screen descent than absolute here

356
00:22:57,760 --> 00:23:04,490
great usually women are absolute area use something we use a linear programming solver or

357
00:23:04,510 --> 00:23:07,630
use specialized algorithms that can actually handle the up there

358
00:23:10,130 --> 00:23:15,930
i year

359
00:23:16,400 --> 00:23:20,780
it is the problem here but it's just it's it's

360
00:23:28,220 --> 00:23:29,360
it's just that the

361
00:23:29,360 --> 00:23:33,310
the gradient can only be larger you want to close to the out there more

362
00:23:33,330 --> 00:23:36,970
the range is because here because you can you get more

363
00:23:37,040 --> 00:23:40,110
so convergence was still a lot less

364
00:23:40,150 --> 00:23:44,200
here which is good because if you're if you if you're running online you you

365
00:23:44,200 --> 00:23:44,950
don't want to

366
00:23:45,510 --> 00:23:49,010
jump around with values too much

367
00:23:50,730 --> 00:23:55,060
yes purely for come purely for the competition properties do we use these square

368
00:23:56,290 --> 00:23:59,050
i should point out if you know anything about

369
00:23:59,060 --> 00:24:03,930
least recent literature in regression classification

370
00:24:06,140 --> 00:24:07,610
one of the absolute error

371
00:24:07,620 --> 00:24:11,480
it has a nice properties with respect to the sparsity of the solution so if

372
00:24:11,480 --> 00:24:15,630
you use an error function like this you get many more zero weight

373
00:24:15,640 --> 00:24:20,910
in your status and so if you want an explanatory explanatory solution this is just

374
00:24:20,910 --> 00:24:22,570
the basis functions matter

375
00:24:22,620 --> 00:24:27,160
then you actually are given will give you that solution whereas in square almost every

376
00:24:27,390 --> 00:24:28,810
will be non-zero

377
00:24:31,760 --> 00:24:33,550
but for simplicity

378
00:24:33,560 --> 00:24:38,370
we we use greater

379
00:24:38,370 --> 00:24:40,220
OK so

380
00:24:40,280 --> 00:24:43,380
i think it is explained by

381
00:24:43,380 --> 00:24:48,090
very well in the book if you want to know how you ppl ability to

382
00:24:48,190 --> 00:24:49,890
to the

383
00:24:49,900 --> 00:24:51,710
function approximation case

384
00:24:53,150 --> 00:24:58,020
you can check therefore for for better justification but the idea is simply showing that

385
00:24:58,020 --> 00:24:59,470
this update

386
00:24:59,520 --> 00:25:02,790
it's exactly that the previous update under the exact

387
00:25:02,830 --> 00:25:06,510
function blocks on under the exact same thing

388
00:25:06,530 --> 00:25:11,690
OK so now we have to keep out we keep eligibility is on each parameter

389
00:25:11,730 --> 00:25:13,000
and we update

390
00:25:13,010 --> 00:25:14,530
with the land

391
00:25:14,580 --> 00:25:18,480
the air times the old parameter to be updated

392
00:25:18,500 --> 00:25:22,220
OK so before we had the of states in the states now we have two

393
00:25:23,380 --> 00:25:27,880
with better there'll there'll ability NASA way to implement very efficiently in a few lines

394
00:25:27,880 --> 00:25:29,300
in the gradient sense

395
00:25:29,320 --> 00:25:35,140
for td lambda

396
00:25:35,940 --> 00:25:38,900
are covered the

397
00:25:44,300 --> 00:25:47,020
a bit too detailed i think for what i want to go into that but

398
00:25:47,020 --> 00:26:00,070
that this more information slide actually use the stuff

399
00:26:00,080 --> 00:26:02,550
OK now you have to use

400
00:26:02,590 --> 00:26:04,540
functions again

401
00:26:04,590 --> 00:26:07,970
which are relevant to predicting value

402
00:26:08,000 --> 00:26:13,720
so you might example limiting factor and imagine that processing of my life functions

403
00:26:13,890 --> 00:26:16,990
being in a state

404
00:26:20,480 --> 00:26:25,670
each feature fi is there's an island box one or next box

405
00:26:25,720 --> 00:26:27,860
so i had exactly

406
00:26:28,200 --> 00:26:30,510
i see

407
00:26:30,570 --> 00:26:36,910
i went solo

408
00:26:36,950 --> 00:26:38,050
a really bad

409
00:26:38,070 --> 00:26:40,720
really that we do this would be to have

410
00:26:40,740 --> 00:26:44,020
twenty seven features i think

411
00:26:44,080 --> 00:26:50,080
twenty seven boeing features

412
00:26:50,090 --> 00:26:57,780
which would be

413
00:26:58,750 --> 00:27:02,250
o and acts or mt

414
00:27:05,250 --> 00:27:10,510
positions one what one nine nine to towards three properties here was here

415
00:27:10,550 --> 00:27:12,650
twenty seven features

416
00:27:12,690 --> 00:27:14,770
let's say there's decision one

417
00:27:14,820 --> 00:27:18,910
there's an exhibition nine years in division three and so on

418
00:27:19,940 --> 00:27:23,250
now if you know tic-tac-toe

419
00:27:23,310 --> 00:27:24,660
can someone explain why

420
00:27:24,660 --> 00:27:27,420
this value function approximation

421
00:27:27,460 --> 00:27:41,220
o limit although maybe making sense would never learned the opera policy

422
00:27:41,240 --> 00:27:44,860
think about what values

423
00:27:44,860 --> 00:27:47,830
that this method can can can approximate

424
00:27:48,710 --> 00:28:00,970
so it it all a

425
00:28:00,980 --> 00:28:05,150
i can value this

426
00:28:05,220 --> 00:28:09,940
and i can tell you see next year

427
00:28:09,980 --> 00:28:14,510
but only ever looks at the at each feature individually

428
00:28:14,570 --> 00:28:18,280
what really matters to that are not the individual features and no matter how it

429
00:28:18,300 --> 00:28:21,700
together that i have an axe here and now here

430
00:28:21,700 --> 00:28:25,170
or an exit i i've outlined three axis here

431
00:28:25,180 --> 00:28:28,120
clearly you need that to represent

432
00:28:30,470 --> 00:28:33,590
so weighting these sorts of pages individually

433
00:28:33,610 --> 00:28:36,880
doesn't mean anything about the global picture what you really need

434
00:28:36,910 --> 00:28:38,980
are conjunctions of market positions

435
00:28:39,040 --> 00:28:40,450
i have

436
00:28:40,450 --> 00:28:41,910
features such as

437
00:28:42,080 --> 00:28:46,050
two axes in

438
00:28:48,690 --> 00:28:51,130
two cases where there are two axes in the world

439
00:28:51,170 --> 00:28:53,620
by features like this and we do better in

440
00:28:53,630 --> 00:28:55,770
in tic-tac-toe

441
00:28:55,800 --> 00:28:59,940
but so people often use the nearby from approximations

442
00:28:59,980 --> 00:29:03,110
they're upset when they don't work but i think the reason that work because you

443
00:29:03,110 --> 00:29:06,720
didn't use expressed feature set

444
00:29:08,260 --> 00:29:12,490
one way around this

445
00:29:12,590 --> 00:29:15,260
is that

446
00:29:15,310 --> 00:29:17,720
you can add in

447
00:29:17,740 --> 00:29:21,220
you start with simple features and if you have a function not going well and

448
00:29:21,220 --> 00:29:22,980
more complex features to start to conjoin

449
00:29:23,320 --> 00:29:25,400
cases in your initial features

450
00:29:25,530 --> 00:29:27,980
if you know things like decision trees

451
00:29:28,020 --> 00:29:32,870
decision trees would be a good i functionapproximator here or a regression tree

452
00:29:32,900 --> 00:29:38,840
the biggest trees again here we take into account conjunctions of features

453
00:29:38,860 --> 00:29:42,360
so you may not seem decision trees you have

454
00:29:42,410 --> 00:29:49,920
thank you for example

455
00:29:49,980 --> 00:29:54,680
so we want a century you say

456
00:29:54,690 --> 00:29:57,390
there's an axe in

457
00:29:59,240 --> 00:30:01,010
true false

458
00:30:01,850 --> 00:30:04,380
maybe there is

459
00:30:04,460 --> 00:30:05,900
annexin a

460
00:30:06,010 --> 00:30:08,640
true or false

461
00:30:08,800 --> 00:30:12,020
some point

462
00:30:12,070 --> 00:30:16,110
you get them to leave any say OK i write this has value three point

463
00:30:18,020 --> 00:30:23,120
here i i had two axes are are probably saying high somewhere else another better

464
00:30:23,120 --> 00:30:24,870
here each point

465
00:30:24,880 --> 00:30:26,930
will we

466
00:30:26,950 --> 00:30:27,860
give me

467
00:30:27,900 --> 00:30:29,710
high probability for

468
00:30:29,720 --> 00:30:31,710
one of the faces of my dice

469
00:30:31,710 --> 00:30:35,960
and zero probability for the other each each point of this channel

470
00:30:38,460 --> 00:30:45,710
very very close to what we had in the in the precise this one

471
00:30:45,730 --> 00:30:49,950
if i gave a really little value for alpha

472
00:30:49,960 --> 00:30:51,610
that we make that

473
00:30:51,620 --> 00:30:55,640
i have the chance of falling in one of history

474
00:30:55,750 --> 00:30:58,040
but it's like i'm saying

475
00:30:58,100 --> 00:31:02,010
the chances that i see one of the faces of any of these it's really

476
00:31:02,010 --> 00:31:04,450
low so indian one we

477
00:31:04,460 --> 00:31:05,650
falls but

478
00:31:05,670 --> 00:31:08,440
we don't know which one

479
00:31:11,010 --> 00:31:17,050
if AC one is that having no prior with a uniform prior over the tree

480
00:31:17,050 --> 00:31:19,650
of the

481
00:31:21,800 --> 00:31:25,460
tree possible faces of my days

482
00:31:25,510 --> 00:31:26,850
and if i

483
00:31:26,860 --> 00:31:28,170
could be

484
00:31:29,170 --> 00:31:30,630
four the tree

485
00:31:31,180 --> 00:31:34,230
so far it's like having saying that

486
00:31:34,250 --> 00:31:36,170
they're all

487
00:31:36,170 --> 00:31:38,580
equiprobable they have the same

488
00:31:43,920 --> 00:31:45,980
it is in the middle of the tree

489
00:31:52,030 --> 00:31:53,670
and of course

490
00:31:54,070 --> 00:31:58,500
the classical distributions to caution distribution that we all know

491
00:31:59,980 --> 00:32:04,130
was parameters nu and sigma

492
00:32:04,150 --> 00:32:08,180
which generalizes to the very caution distribution

493
00:32:08,230 --> 00:32:09,910
in which

494
00:32:09,920 --> 00:32:10,890
this time

495
00:32:10,900 --> 00:32:15,700
it's not i don't have here this colour value but a victory

496
00:32:16,830 --> 00:32:21,790
so the the mean is also picture and the

497
00:32:21,820 --> 00:32:25,320
the variance of my question distribution is symmetric

498
00:32:25,400 --> 00:32:29,700
a square matrix

499
00:32:29,750 --> 00:32:34,330
and this distribution it's really important in

500
00:32:34,350 --> 00:32:36,250
in statistics

501
00:32:36,280 --> 00:32:40,210
because of the central limit theorem most of the time

502
00:32:40,250 --> 00:32:42,710
because if i have

503
00:32:42,760 --> 00:32:46,530
a set of random variables

504
00:32:46,540 --> 00:32:47,760
which are

505
00:32:47,770 --> 00:32:48,660
i i d

506
00:32:48,660 --> 00:32:52,030
which i'm going to to define after but what i mean is that they all

507
00:32:52,030 --> 00:32:53,700
have the same distribution

508
00:32:53,710 --> 00:32:55,980
and very

509
00:32:56,060 --> 00:33:00,000
independence between each for each other's

510
00:33:00,010 --> 00:33:04,770
and so the the distribution of this the mean

511
00:33:04,790 --> 00:33:06,460
of these variables

512
00:33:06,460 --> 00:33:10,130
then the words a gaussian distribution

513
00:33:10,180 --> 00:33:14,060
so any time you considering the sum of random variables if

514
00:33:14,080 --> 00:33:18,170
you always in the output is is that if you have enough data

515
00:33:18,180 --> 00:33:21,680
you will have in fact a gaussian distribution

516
00:33:21,700 --> 00:33:26,260
so here is these these central limit theorem is initiated by

517
00:33:26,320 --> 00:33:33,950
by these histograms well what is shown is that imagine they have a uniform distribution

518
00:33:33,950 --> 00:33:34,910
so the

519
00:33:34,920 --> 00:33:39,710
between zero and one the so the probability of having one value between zero and

520
00:33:39,710 --> 00:33:43,020
one is the same for each of these values

521
00:33:43,040 --> 00:33:47,500
and i repeat i have made the

522
00:33:47,550 --> 00:33:49,780
i repeat i draw

523
00:33:49,790 --> 00:33:52,390
somevaluesfrom this uniform distribution

524
00:33:52,400 --> 00:33:58,460
and then computed histogram of these repeated experiment

525
00:34:00,170 --> 00:34:01,970
when i have only one

526
00:34:02,800 --> 00:34:04,200
random variable

527
00:34:04,210 --> 00:34:05,390
we have

528
00:34:05,450 --> 00:34:10,700
our uniform distribution between zero and one of the probability of having each of these

529
00:34:11,400 --> 00:34:14,490
it's quite the same but if i have

530
00:34:15,500 --> 00:34:17,410
if they have two variables

531
00:34:17,420 --> 00:34:18,870
and i take to mean

532
00:34:18,890 --> 00:34:20,580
so we see that now

533
00:34:21,570 --> 00:34:24,520
we have a greater probability two

534
00:34:24,530 --> 00:34:25,980
the sum

535
00:34:26,000 --> 00:34:27,310
the mean is

536
00:34:27,540 --> 00:34:30,190
one half

537
00:34:30,200 --> 00:34:31,310
and if

538
00:34:31,330 --> 00:34:32,350
i have

539
00:34:32,350 --> 00:34:33,290
and search

540
00:34:33,310 --> 00:34:35,600
variables and i take them in

541
00:34:35,600 --> 00:34:41,770
we so we see what's the characteristic form of gaussian distribution can appear

542
00:34:50,660 --> 00:34:55,410
he is the same for me

543
00:34:55,480 --> 00:34:59,270
well for me that's that's something

544
00:34:59,520 --> 00:35:02,210
one is defined as as

545
00:35:02,220 --> 00:35:05,640
the ones like the samples

546
00:35:09,370 --> 00:35:11,640
the the

547
00:35:11,660 --> 00:35:14,120
examples for this

548
00:35:14,180 --> 00:35:18,910
you can give me so that's

549
00:35:18,930 --> 00:35:21,820
and also this and is the same as this

550
00:35:21,850 --> 00:35:23,480
should have

551
00:35:23,550 --> 00:35:25,140
had this in addition

552
00:35:25,180 --> 00:35:27,910
no it's so i take

553
00:35:27,920 --> 00:35:32,750
one sample for example in this in this case i think one sample

554
00:35:32,830 --> 00:35:34,720
of my

555
00:35:34,740 --> 00:35:36,170
uniform distribution

556
00:35:36,230 --> 00:35:37,690
i take one hundred

557
00:35:37,710 --> 00:35:41,610
sample but that's for only one variable

558
00:35:41,660 --> 00:35:43,580
what what ten c

559
00:35:43,640 --> 00:35:46,960
let's say for two because once it's misleading

560
00:35:47,010 --> 00:35:48,540
no i take two

561
00:35:48,560 --> 00:35:51,920
one sample one of one of the other thing to me

562
00:35:51,930 --> 00:35:56,010
one of one variable one after the viability to me and i i d do

563
00:35:56,010 --> 00:35:58,050
that kind of thing

564
00:35:58,140 --> 00:36:00,200
so that that's the mean of two

565
00:36:00,230 --> 00:36:01,900
of taking twice

566
00:36:01,900 --> 00:36:04,000
the uniform

567
00:36:05,990 --> 00:36:09,040
so that's what you call center

568
00:36:09,150 --> 00:36:14,720
always it's one that's what

569
00:36:14,750 --> 00:36:21,620
in one it's the standards

570
00:36:24,100 --> 00:36:25,850
all that was

571
00:36:25,900 --> 00:36:29,440
let's consider

572
00:36:29,450 --> 00:36:33,450
let's consider the

573
00:36:33,480 --> 00:36:38,240
probabilities is that it's usually use in and statistics

574
00:36:38,820 --> 00:36:40,410
statistic it's

575
00:36:40,420 --> 00:36:44,050
more specific around what's the random sample

576
00:36:45,310 --> 00:36:49,450
so a random sample is the number of independent observations taken

577
00:36:49,450 --> 00:36:52,450
well if i have another view with the mother

578
00:36:52,510 --> 00:36:55,550
which is not observed

579
00:36:55,560 --> 00:36:58,240
then i have some additional information

580
00:36:58,290 --> 00:37:00,290
right OK

581
00:37:01,790 --> 00:37:04,330
let's assume that the nodes

582
00:37:07,250 --> 00:37:09,160
the same

583
00:37:15,320 --> 00:37:17,620
clearly is independent

584
00:37:17,670 --> 00:37:21,200
this is information here completely blocks

585
00:37:21,230 --> 00:37:26,320
a user of knowing information about these violent or confer bishop

586
00:37:26,390 --> 00:37:29,880
so there are conditionally independent

587
00:37:29,890 --> 00:37:32,000
a position-by-position

588
00:37:32,090 --> 00:37:34,210
now let's see if they are

589
00:37:34,210 --> 00:37:37,400
we are all of the world will assume that

590
00:37:37,420 --> 00:37:42,810
in the air conditioning panel c let's see if you obtain factorizations

591
00:37:45,560 --> 00:37:51,000
let's just assume that is conditions can all be given c

592
00:37:51,020 --> 00:37:52,300
i assume this is true

593
00:37:52,330 --> 00:37:54,780
so we assume this is true

594
00:37:54,790 --> 00:37:56,890
because this means

595
00:38:04,990 --> 00:38:08,290
just write the joint

596
00:38:08,300 --> 00:38:12,280
as p of the given c of c

597
00:38:12,340 --> 00:38:16,030
and now we know that p of a b and c

598
00:38:16,030 --> 00:38:19,190
is that thing

599
00:38:19,200 --> 00:38:24,090
full we just something these times you see

600
00:38:24,110 --> 00:38:26,580
isn't that precisely

601
00:38:26,770 --> 00:38:31,760
the factorisation of these graphs

602
00:38:31,830 --> 00:38:34,220
you see

603
00:38:34,220 --> 00:38:37,020
you see all sorts of things that i mean we're not talking about the same

604
00:38:37,020 --> 00:38:39,610
things we're over talk about different things

605
00:38:42,850 --> 00:38:46,290
and we are proving that

606
00:38:47,010 --> 00:38:51,210
something that happening in the domain of the graph is implied something

607
00:38:51,230 --> 00:38:52,560
we're proving some

608
00:38:52,580 --> 00:38:54,360
some some mappings

609
00:38:55,600 --> 00:38:58,330
the universe of the probability distribution

610
00:38:58,390 --> 00:39:00,800
in the universe of the graph

611
00:39:00,850 --> 00:39:04,620
when you're interpreting just by looking at the graph

612
00:39:04,640 --> 00:39:06,270
you reading

613
00:39:06,310 --> 00:39:10,130
the set of conditional independence statements and there

614
00:39:10,140 --> 00:39:12,370
factorisation of the distribution

615
00:39:12,380 --> 00:39:17,280
so what this means so far we need to check still for the

616
00:39:17,280 --> 00:39:21,950
and of course you to make an induction argument for everyone but if these equivalence

617
00:39:21,960 --> 00:39:24,130
really hold

618
00:39:24,210 --> 00:39:25,510
in general

619
00:39:25,560 --> 00:39:27,300
what does this tell you

620
00:39:27,320 --> 00:39:28,640
is that

621
00:39:28,650 --> 00:39:33,150
we may completely forget about the universal

622
00:39:33,160 --> 00:39:37,350
o ja break representation in our minds and complete think about

623
00:39:37,400 --> 00:39:39,130
graphical objects

624
00:39:39,140 --> 00:39:42,870
of course and we implemented in the computer will be writing

625
00:39:42,920 --> 00:39:44,710
i mean

626
00:39:44,750 --> 00:39:46,120
behind everything there

627
00:39:46,150 --> 00:39:47,820
probability of course

628
00:39:49,000 --> 00:39:53,420
working with graphs is much easier for

629
00:39:53,440 --> 00:39:56,760
if we have this direct mapping between the universe

630
00:39:56,800 --> 00:39:58,740
you can just work with the graph

631
00:39:58,820 --> 00:40:01,760
and not everything that will be applied

632
00:40:01,770 --> 00:40:02,760
in the real

633
00:40:07,160 --> 00:40:10,470
will be completely under control

634
00:40:10,480 --> 00:40:14,330
so we need to prove this and that is the

635
00:40:16,740 --> 00:40:18,710
there's one

636
00:40:18,720 --> 00:40:20,660
graph left

637
00:40:20,720 --> 00:40:24,060
which is the graph where we have

638
00:40:25,170 --> 00:40:26,700
and c

639
00:40:26,700 --> 00:40:29,550
so we have two parents and one child

640
00:40:30,230 --> 00:40:31,730
now the same question

641
00:40:31,770 --> 00:40:34,040
this a and b

642
00:40:34,050 --> 00:40:36,510
in the independent

643
00:40:36,530 --> 00:40:40,060
OK if is said to have an observation i just

644
00:40:40,110 --> 00:40:45,040
i'm just asking whether a and b independent we have seen the previous UK is

645
00:40:45,040 --> 00:40:47,760
the business does not hold

646
00:40:49,570 --> 00:40:52,060
so let's check if this is true so basically

647
00:40:52,070 --> 00:40:53,980
i feel a b

648
00:40:53,980 --> 00:40:57,330
is equal to some oversea of p of the BBC

649
00:40:58,910 --> 00:41:01,650
but what's the factorisation of the ground

650
00:41:01,670 --> 00:41:04,220
his appeal for a b

651
00:41:04,270 --> 00:41:06,720
and p of the given name

652
00:41:07,650 --> 00:41:11,100
well feel faintly of constant force

653
00:41:11,150 --> 00:41:13,050
we can pull them all

654
00:41:13,060 --> 00:41:18,270
and they only have someone seal you'll see you

655
00:41:18,310 --> 00:41:23,330
and how is how much is the someone still feels he given a b

656
00:41:23,340 --> 00:41:24,530
it's one

657
00:41:24,540 --> 00:41:27,620
so the result here is p of three times p of the

658
00:41:27,670 --> 00:41:31,790
and this is p of b

659
00:41:31,840 --> 00:41:34,060
so look what's happening here

660
00:41:34,070 --> 00:41:36,730
we just proven that can be is

661
00:41:36,730 --> 00:41:40,780
the random variables in the act and the

662
00:41:43,350 --> 00:41:44,760
there is that here

663
00:41:44,760 --> 00:41:47,980
connecting to survive

664
00:41:48,640 --> 00:41:50,160
both the mistaken

665
00:41:50,250 --> 00:41:52,270
that does not mean

666
00:41:53,400 --> 00:41:59,780
the electorate and then

667
00:42:02,070 --> 00:42:05,250
they are actually independent

668
00:42:05,280 --> 00:42:06,880
so this is the

669
00:42:06,940 --> 00:42:09,490
different type of

670
00:42:09,660 --> 00:42:13,440
let's atomic bayesian network that has different

671
00:42:13,450 --> 00:42:16,300
i guess in cracked

672
00:42:16,390 --> 00:42:18,510
we have no observations and this two

673
00:42:19,420 --> 00:42:22,230
it actually makes a lot of sense to me

674
00:42:22,390 --> 00:42:30,760
if this work and to have a child i mean

675
00:42:32,290 --> 00:42:37,010
mary someone i mean it's independent women computing the pelican area

676
00:42:37,060 --> 00:42:39,750
and then if i hope

677
00:42:40,970 --> 00:42:42,050
chicken but

678
00:42:42,110 --> 00:42:43,890
i mean

679
00:42:43,920 --> 00:42:45,100
i'm doing this

680
00:42:45,130 --> 00:42:47,690
let's assume subset the subset of

681
00:42:48,900 --> 00:42:54,300
and it's independence in

682
00:42:56,440 --> 00:42:58,870
now let's see

683
00:42:58,920 --> 00:43:00,500
if i hope so

684
00:43:00,540 --> 00:43:02,600
the child

685
00:43:02,620 --> 00:43:04,640
if we observe the child

686
00:43:04,690 --> 00:43:07,240
if i see something about the track

687
00:43:07,250 --> 00:43:11,910
now the question is will factorizations imply conditional independence

688
00:43:11,970 --> 00:43:15,280
we've seen previously that has been applied math

689
00:43:16,290 --> 00:43:21,380
so a lot let's assume justin factorizations p of a few p of seeing a

690
00:43:24,280 --> 00:43:26,040
conditional independence will

691
00:43:26,090 --> 00:43:29,990
happen if we have that p of the given c

692
00:43:29,990 --> 00:43:33,010
is it good to p of a difference if p of the given c so

693
00:43:33,010 --> 00:43:35,300
let's just what is true

694
00:43:35,390 --> 00:43:39,320
so have here the joint right to join us in

695
00:43:39,370 --> 00:43:42,460
but suddenly we're stuck because

696
00:43:42,520 --> 00:43:44,590
we're looking for these

697
00:43:44,660 --> 00:43:48,120
quality p of the given c of the given c

698
00:43:51,300 --> 00:43:55,790
so two factors in each factor has two terms there's no way you're going to

699
00:43:55,790 --> 00:43:57,060
get to read

700
00:43:57,110 --> 00:43:58,390
there's no way

701
00:43:58,400 --> 00:44:00,890
you're going to get rid of these three

702
00:44:00,890 --> 00:44:04,500
you can make toy model universes which are definitely not computable

703
00:44:05,000 --> 00:44:08,210
even though they're completely deterministic and one way of doing it

704
00:44:08,520 --> 00:44:09,500
is depend on

705
00:44:10,540 --> 00:44:11,750
timing properties of

706
00:44:12,460 --> 00:44:17,250
of what a called polyominoes as squares glued together and if they tile plane or don't

707
00:44:17,730 --> 00:44:20,560
they if it does one thing the universe is one thing if it doesn't other

708
00:44:22,250 --> 00:44:24,870
and then this happens to be one of these non computable

709
00:44:27,190 --> 00:44:29,480
this is tower playing these examples which do

710
00:44:30,730 --> 00:44:32,250
here's another example which doesn't

711
00:44:32,660 --> 00:44:36,520
the fact that is a difficult problem you can see from the example because it's

712
00:44:36,520 --> 00:44:38,480
pretty difficult to see how that tiles the plane

713
00:44:38,890 --> 00:44:42,920
even though doesn't actually extend to infinity but let me not waste time on here the

714
00:44:43,810 --> 00:44:44,620
other things i would say

715
00:44:45,100 --> 00:44:47,390
i just want to show that they are perfectly

716
00:44:48,330 --> 00:44:53,060
but not physically plausible but mathematically plausible models of toy universes

717
00:44:53,670 --> 00:44:57,420
which are not computable which you can not simulate on the computer

718
00:44:57,870 --> 00:45:00,400
so it's only a genuine question that's one thing

719
00:45:00,790 --> 00:45:02,770
that that could be non sit

720
00:45:03,370 --> 00:45:05,060
possible not possible to simulate

721
00:45:05,600 --> 00:45:07,290
but i do want to work on mechanics

722
00:45:08,310 --> 00:45:10,640
because as we saw during himself already

723
00:45:11,080 --> 00:45:13,330
but quantum mechanics might have have relevance

724
00:45:14,080 --> 00:45:16,330
so i have to tell you something about quantum mechanics

725
00:45:16,750 --> 00:45:19,920
in the last remaining minutes that have which is not really enough time

726
00:45:20,420 --> 00:45:21,120
this have occurred

727
00:45:22,480 --> 00:45:23,270
here i have to

728
00:45:23,960 --> 00:45:25,560
idealized experiments

729
00:45:26,540 --> 00:45:27,770
which illustrates he

730
00:45:28,690 --> 00:45:31,190
conflict the basic concepts in quantum mechanics

731
00:45:31,640 --> 00:45:33,330
between particle-like behavior

732
00:45:33,810 --> 00:45:34,980
and wave-like behavior

733
00:45:35,390 --> 00:45:38,400
so here we have a laser so which is the source of photons

734
00:45:39,830 --> 00:45:41,400
do we have what's called eh

735
00:45:41,980 --> 00:45:43,830
beamsplitter or a half-silvered mirror

736
00:45:44,580 --> 00:45:46,310
and half the light goes through it

737
00:45:46,830 --> 00:45:48,120
after this detector over here

738
00:45:48,660 --> 00:45:50,290
the other half figures reflected

739
00:45:50,730 --> 00:45:51,480
the detector

740
00:45:52,670 --> 00:45:58,100
now for a single photon what you find is for an idealized experiment is either

741
00:45:58,140 --> 00:45:59,640
because this way all the way

742
00:46:00,270 --> 00:46:03,900
it's an exclusive if this is registered this world if this

743
00:46:04,330 --> 00:46:05,020
doesn't register

744
00:46:05,440 --> 00:46:07,520
the stars and vice versa on the other one

745
00:46:09,170 --> 00:46:10,890
it's an exclusive or

746
00:46:11,140 --> 00:46:13,310
this all this but not both or neither

747
00:46:14,100 --> 00:46:16,080
that's a very particle-like behavior

748
00:46:16,620 --> 00:46:19,870
but you could slightly modify this property to problem areas here

749
00:46:20,350 --> 00:46:22,270
having now other beings for the

750
00:46:22,730 --> 00:46:24,100
and all these path lengths equal

751
00:46:24,560 --> 00:46:30,020
detectors here and here and then miraculously find every time the photon comes along because

752
00:46:30,140 --> 00:46:31,480
this way and never that way

753
00:46:32,350 --> 00:46:35,310
if these particles like what we are trying to do here

754
00:46:35,830 --> 00:46:39,290
then it might go this way like this way because this way like this with

755
00:46:39,290 --> 00:46:42,540
this way because this would make with respect to it's equally likely to come here

756
00:46:43,890 --> 00:46:44,690
but you don't find

757
00:46:45,060 --> 00:46:46,370
we find that the possibilities

758
00:46:46,960 --> 00:46:47,540
coming here

759
00:46:48,060 --> 00:46:50,080
in some mysterious way cancel each other out

760
00:46:50,940 --> 00:46:53,230
so now this is more like a wavelike behavior

761
00:46:54,310 --> 00:46:55,810
this is particle-like its wave-like

762
00:46:56,580 --> 00:46:57,100
and the way we

763
00:46:57,830 --> 00:46:59,580
make sense that's introducing

764
00:47:00,920 --> 00:47:04,230
the formalism of quantum mechanics and we're gonna tell you too much about it

765
00:47:04,770 --> 00:47:05,790
the crucial point is

766
00:47:06,230 --> 00:47:07,750
that you've these things cause states

767
00:47:08,350 --> 00:47:10,060
hand that alternatives

768
00:47:11,440 --> 00:47:13,520
in superposition so they can happen at once

769
00:47:13,980 --> 00:47:15,500
so let's go back to this case here

770
00:47:16,040 --> 00:47:17,160
it means that the photon

771
00:47:17,770 --> 00:47:19,520
my do this forum i do this

772
00:47:20,210 --> 00:47:23,660
and you have to understand that in a sense it actually does both once

773
00:47:24,100 --> 00:47:27,120
otherwise you couldn't get interference which stops going this way

774
00:47:27,580 --> 00:47:30,870
the photon fields at least both groups

775
00:47:31,270 --> 00:47:32,480
it goes both ways ones

776
00:47:32,890 --> 00:47:35,440
so today is one of the alternative is either

777
00:47:35,770 --> 00:47:37,850
and you have to consider that they both happen once

778
00:47:38,830 --> 00:47:40,160
it's even more mysterious

779
00:47:40,670 --> 00:47:43,940
because we have weighting functions for put weighting factors here

780
00:47:44,460 --> 00:47:46,020
you might think these probabilities

781
00:47:46,690 --> 00:47:49,170
not these are complex numbers

782
00:47:49,560 --> 00:47:49,980
yeah i got

783
00:47:50,140 --> 00:47:53,980
complex plane complex means involves the square root of minus one

784
00:47:54,460 --> 00:47:56,270
you can plot those numbers on the plane

785
00:47:57,000 --> 00:47:59,020
and these numbers w and z

786
00:47:59,580 --> 00:48:01,420
two different numbers in this complex play

787
00:48:01,420 --> 00:48:05,690
well thought corpus for this one hundred and his going twelve

788
00:48:06,510 --> 00:48:10,300
now i was on the selection committee and forty one

789
00:48:10,300 --> 00:48:14,360
there's and also won the award selection committee in each of the images

790
00:48:14,370 --> 00:48:20,470
it is very independent i ask because of his oldest and record what does and

791
00:48:20,470 --> 00:48:22,360
he was selected twice for

792
00:48:22,370 --> 00:48:29,430
this site and also one piece of information i found old since i

793
00:48:29,440 --> 00:48:32,240
i came here to this conference is his academic

794
00:48:32,240 --> 00:48:38,020
you moment OK and everyone knows that he has been running the data mining companies

795
00:48:38,020 --> 00:48:41,690
is microsoft is chief error is now you

796
00:48:41,700 --> 00:48:44,180
and also too small data mining companies

797
00:48:44,190 --> 00:48:47,080
probably not a small data mining companies here

798
00:48:47,170 --> 00:48:52,920
what could be and actually has also extensive and then these experiences

799
00:48:52,930 --> 00:48:55,800
this different universities who was an

800
00:48:55,820 --> 00:48:59,430
gender professor anthony it was still southern california

801
00:48:59,430 --> 00:49:01,890
for several years and his in

802
00:49:01,940 --> 00:49:06,410
and adjunct professor at the university of technology in sicily

803
00:49:06,620 --> 00:49:12,990
and he is also an adjunct professor at the hong kong polytechnic university

804
00:49:12,990 --> 00:49:19,530
OK most part model from the mold after i saw his prison just lies on

805
00:49:19,530 --> 00:49:26,560
sunday had some academic on and i was wondering what colleagues used OK

806
00:49:27,300 --> 00:49:28,310
given his

807
00:49:28,320 --> 00:49:33,120
researcher called and also he has been a great speaker for

808
00:49:33,140 --> 00:49:37,410
many countries is i actually want him to ICDM o three when i was one

809
00:49:37,410 --> 00:49:45,490
of the program committee chair for ICT also in florida and eventually you two cells

810
00:49:45,490 --> 00:49:47,750
and industry

811
00:49:47,760 --> 00:49:51,890
we had two formulations for any security

812
00:49:52,410 --> 00:49:58,330
so what and one was somerfield OK and the other was myself

813
00:49:58,540 --> 00:50:02,230
and then when i heard that i was cover feature was the use of and

814
00:50:02,230 --> 00:50:08,420
i told my mother say that you've always if i was allowed to vote i

815
00:50:08,450 --> 00:50:12,490
would vote for someone as well as for his land at that time i if

816
00:50:12,500 --> 00:50:15,410
his name was given on a one

817
00:50:15,420 --> 00:50:19,570
that would seem to me to need a lot more attractive and the union he

818
00:50:19,570 --> 00:50:24,720
won the silver so what in the two thousand three and i want to use

819
00:50:24,720 --> 00:50:27,580
it so what informally which which is two thousand four

820
00:50:27,600 --> 00:50:28,970
and it's less

821
00:50:28,980 --> 00:50:33,490
one competition story and after any competition so now

822
00:50:33,490 --> 00:50:39,170
we actually that colleagues as i mentioned earlier in his agenda professor at the hong

823
00:50:39,170 --> 00:50:45,280
kong polytechnic university i haven't been able to you spend my sabbatical in the same

824
00:50:45,280 --> 00:50:51,620
university for one year so we actually working together for the now not enough although

825
00:50:52,260 --> 00:50:54,040
two delays that's

826
00:50:54,080 --> 00:50:56,560
welcome to some of the other four

827
00:50:56,560 --> 00:51:00,130
because they want to together

828
00:51:07,880 --> 00:51:10,810
thank you very much for her

829
00:51:10,840 --> 00:51:12,730
very kind introduction

830
00:51:12,750 --> 00:51:18,250
for me that was almost lost control the first two two-minute limit

831
00:51:18,300 --> 00:51:22,040
but i was conflicted

832
00:51:24,350 --> 00:51:26,750
given time

833
00:51:26,760 --> 00:51:29,550
i think i'll skip the first part of my talk

834
00:51:32,570 --> 00:51:34,330
what i wanted to

835
00:51:34,800 --> 00:51:39,400
this talk more about the technical challenges but then

836
00:51:39,420 --> 00:51:41,710
this morning i decided to

837
00:51:41,730 --> 00:51:44,680
focus more on the

838
00:51:44,730 --> 00:51:46,180
on the

839
00:51:46,190 --> 00:51:47,680
and really

840
00:51:47,690 --> 00:51:49,300
more and

841
00:51:49,310 --> 00:51:53,450
what's new here because i think there plenty of data mining

842
00:51:53,470 --> 00:51:55,430
opportunity cost

843
00:51:57,260 --> 00:52:02,180
to be you start thinking about this whole area of social media people interacting with

844
00:52:02,180 --> 00:52:03,800
each other on the web

845
00:52:03,820 --> 00:52:07,470
i'll explain what i mean by that so with

846
00:52:07,480 --> 00:52:10,410
skim i like challenges and

847
00:52:12,690 --> 00:52:13,540
we sort of covered

848
00:52:14,260 --> 00:52:19,340
time and i apologize for not going into the technical side

849
00:52:19,430 --> 00:52:21,730
let me

850
00:52:21,760 --> 00:52:26,230
no i will start with a generic introduction

851
00:52:26,260 --> 00:52:31,500
the goal is to sort of walking for some reason i went through as i

852
00:52:31,520 --> 00:52:37,650
all of the internet advertising business search so forth and show you how to shift

853
00:52:37,650 --> 00:52:39,210
to strategy

854
00:52:39,220 --> 00:52:42,640
and then i'm going to do something completely different which is

855
00:52:42,660 --> 00:52:49,890
should he start talking about some of these very fundamental very important

856
00:52:50,140 --> 00:52:55,070
extremely poorly understood and tell you about research why we formed

857
00:52:55,080 --> 00:52:56,730
and its mission

858
00:52:56,760 --> 00:53:01,160
so i don't need say much about the internet

859
00:53:01,220 --> 00:53:02,780
it's growing fast

860
00:53:03,060 --> 00:53:06,560
thing to be about million users online

861
00:53:11,300 --> 00:53:12,480
happens to be

862
00:53:12,500 --> 00:53:17,250
the number one destination on the internet reaching seventy percent of the US population actually

863
00:53:17,250 --> 00:53:19,250
about seventy percent of the world

864
00:53:19,260 --> 00:53:24,710
for more than five hundred million users to thirteen million users

865
00:53:24,720 --> 00:53:27,700
it offers a lot of properties

866
00:53:27,710 --> 00:53:30,340
by properties things like male

867
00:53:30,360 --> 00:53:34,730
when new finance i am

868
00:53:34,800 --> 00:53:37,310
first of all sorts of

869
00:53:37,450 --> 00:53:39,430
functions that would like to do

870
00:53:39,430 --> 00:53:41,670
one which actually attracts lots of

871
00:53:41,680 --> 00:53:43,560
people keep coming back

872
00:53:43,560 --> 00:53:45,720
and that brings the project

873
00:53:46,140 --> 00:53:51,030
as well as research on the things that makes up the opportunity to

874
00:53:51,040 --> 00:53:56,560
to serve advertising which is how the model is one

875
00:53:56,560 --> 00:53:57,720
i mentioned

876
00:53:57,720 --> 00:53:59,610
we get at the moment the

877
00:53:59,680 --> 00:54:02,740
the term inside looks like just longer

878
00:54:02,810 --> 00:54:05,980
you fix science some reference density

879
00:54:06,080 --> 00:54:11,040
and i'm only interested in the max of this is zero so only

880
00:54:11,950 --> 00:54:14,960
he effects on given theta

881
00:54:14,980 --> 00:54:17,550
it is less than my reference density

882
00:54:17,590 --> 00:54:19,380
when i see something here

883
00:54:19,400 --> 00:54:22,390
otherwise the term will vanish

884
00:54:27,110 --> 00:54:31,000
let's say this is lace and that expression here

885
00:54:31,210 --> 00:54:33,820
and this term will be less than one

886
00:54:33,840 --> 00:54:35,960
so the negative log is going to be

887
00:54:35,970 --> 00:54:38,030
greater than zero

888
00:54:38,070 --> 00:54:42,120
so that his appears

889
00:54:42,130 --> 00:54:46,850
so therefore only if i'm in a low density region

890
00:54:46,860 --> 00:54:49,530
here there are

891
00:54:49,800 --> 00:54:53,050
well actually see something happening

892
00:54:54,880 --> 00:54:56,260
that's what the maps

893
00:54:56,670 --> 00:54:58,620
is good for

894
00:54:59,580 --> 00:55:01,090
for the next thing

895
00:55:01,100 --> 00:55:04,560
so each other over there will be some reference density but actually i want to

896
00:55:04,560 --> 00:55:06,730
get rid of this normalization

897
00:55:06,780 --> 00:55:10,890
just to pay in the wrong place and doesn't really help so

898
00:55:10,900 --> 00:55:15,120
let's just some subtract that the normalisation automatically

899
00:55:15,130 --> 00:55:17,590
it doesn't really change my estimation procedure

900
00:55:17,600 --> 00:55:19,540
it just makes it easier

901
00:55:19,640 --> 00:55:22,650
and then

902
00:55:22,700 --> 00:55:28,740
if you actually what makes the exponential families modelling two p of x given theta

903
00:55:28,750 --> 00:55:32,030
will get this expression here

904
00:55:32,050 --> 00:55:34,640
now this is something you should know from bernard

905
00:55:34,650 --> 00:55:37,890
that's exactly the same same class

906
00:55:38,100 --> 00:55:42,470
the classification will see the first noted addiction

907
00:55:42,480 --> 00:55:45,080
loss function that not was talking about

908
00:55:45,130 --> 00:55:50,420
it was probably talking in terms of slack variables and constraints but if you

909
00:55:50,470 --> 00:55:54,060
go through the math it's the exact same thing

910
00:55:54,890 --> 00:55:55,670
there two

911
00:55:55,700 --> 00:55:58,430
key advantages for it

912
00:55:58,440 --> 00:56:02,470
i don't care about the normalisation so even if g if theta is

913
00:56:02,520 --> 00:56:04,380
impossible to compute

914
00:56:04,390 --> 00:56:07,160
or really expensive

915
00:56:07,870 --> 00:56:09,750
i don't need it

916
00:56:09,760 --> 00:56:12,880
i don't need to do particularly well so i don't need to waste lot of

917
00:56:12,890 --> 00:56:17,220
capacity on the high density regions

918
00:56:17,230 --> 00:56:22,000
if i just care about normal observations this is where you should try getting a

919
00:56:22,000 --> 00:56:25,210
fairly actually density estimate and for the rest they just need to ensure that it's

920
00:56:25,500 --> 00:56:29,440
either in whatever threshold might want to pick

921
00:56:29,490 --> 00:56:33,880
so this is fairly complicated optimisation problem here

922
00:56:33,900 --> 00:56:41,020
which is some general convex programs now becomes the simple quadratic programme

923
00:56:41,100 --> 00:56:44,990
possibly quite a bit here are

924
00:56:45,000 --> 00:56:50,480
so one in several regards it's easy to compute it does what i want allows

925
00:56:50,480 --> 00:56:57,720
me to do so even in situations where otherwise it couldn't do it

926
00:56:57,730 --> 00:57:03,600
and this is probably what you've seen from finite the geometric interpretation

927
00:57:03,610 --> 00:57:05,970
trying to find one perhaps play

928
00:57:05,980 --> 00:57:09,620
that has the maximum distance from the origin yet it's still closer to the origin

929
00:57:09,620 --> 00:57:15,340
than the observations

930
00:57:17,180 --> 00:57:21,720
you guys in the dual optimisation problem to sing the classes being

931
00:57:21,850 --> 00:57:25,980
seen it

932
00:57:26,610 --> 00:57:30,020
who hasn't seen it

933
00:57:30,170 --> 00:57:35,820
so but i didn't cover that

934
00:57:37,270 --> 00:57:40,770
OK so

935
00:57:40,830 --> 00:57:42,690
similar to our entropy

936
00:57:45,980 --> 00:57:48,720
proud promise to minimize

937
00:57:48,730 --> 00:57:52,750
one half the data square some of the slack variables

938
00:57:52,820 --> 00:57:55,860
the condition that theta delta x

939
00:57:55,880 --> 00:57:56,530
six i

940
00:57:56,550 --> 00:57:58,070
and just

941
00:57:58,080 --> 00:58:03,580
i'm using the non-credible version at the moment just because otherwise notation gets messier was

942
00:58:03,590 --> 00:58:05,610
one sequence here

943
00:58:05,670 --> 00:58:08,280
thanks i critical in here

944
00:58:08,310 --> 00:58:09,730
so what we do is

945
00:58:09,750 --> 00:58:10,720
in order to

946
00:58:10,720 --> 00:58:16,430
compute the dual problem similar to the maximum entropy notation we have to get telegrams

947
00:58:17,350 --> 00:58:21,320
by subtracting the constraints multiplied grande multipliers

948
00:58:21,580 --> 00:58:24,310
from the from the objective function

949
00:58:24,320 --> 00:58:25,400
and then

950
00:58:25,420 --> 00:58:27,620
get rid of the problem variables

951
00:58:27,630 --> 00:58:29,980
we need to compute the saddle point

952
00:58:30,170 --> 00:58:33,140
then i can express the problem is that the saddle point in terms of the

953
00:58:33,140 --> 00:58:37,870
deal one's negative dual optimisation problem

954
00:58:37,960 --> 00:58:42,810
again the advances but to go from a possibly infinite dimensional space

955
00:58:42,860 --> 00:58:47,440
one of the proverb into finite dimensional all but

956
00:58:47,480 --> 00:58:48,900
probably high dimensional but

957
00:58:48,920 --> 00:58:53,190
five dimensional space non the less the dual variables

958
00:58:53,990 --> 00:58:55,370
how do we do

959
00:58:55,390 --> 00:59:01,210
well it's just like a risk we take the problem objective function

960
00:59:01,220 --> 00:59:02,600
write it down

961
00:59:04,380 --> 00:59:08,700
lagrange multiplier times constraint

962
00:59:08,700 --> 00:59:10,740
also the green constraint there just here

963
00:59:10,860 --> 00:59:15,220
constraints so that

964
00:59:15,260 --> 00:59:19,130
in the office and the test because the french multipliers have two great equals europe

965
00:59:19,180 --> 00:59:20,260
extend match

966
00:59:23,840 --> 00:59:29,440
and and then what you do is you take the derivative with respect to theta

967
00:59:29,450 --> 00:59:31,230
new soulful

968
00:59:31,300 --> 00:59:33,570
you get another problem

969
00:59:33,590 --> 00:59:39,170
what comes out of it is what you then might be able to solve directly

970
00:59:40,720 --> 00:59:42,470
the seat of a

971
00:59:42,580 --> 00:59:44,560
gives me this expression here

972
00:59:44,570 --> 00:59:48,160
to get the data is some of our colleagues or

973
00:59:48,160 --> 00:59:54,200
remember similar to the kernel expansion had before the representer theorem so i had before

974
00:59:54,200 --> 00:59:59,460
some theta equals some of alpha i y phi alpha x i and y

975
00:59:59,500 --> 01:00:03,360
OK i would have thought topics i here if they want to the kind of

976
01:00:03,360 --> 01:00:07,260
way but at the same time expansion

977
01:00:07,270 --> 01:00:09,030
dix i have

978
01:00:09,040 --> 01:00:10,930
gives me this expression here

979
01:00:10,950 --> 01:00:13,810
see myself one minus the title sierra

980
01:00:13,840 --> 01:00:18,060
now i know that the does it tell a great sequence here

981
01:00:18,070 --> 01:00:20,250
that means it out twice in this interval

982
01:00:20,260 --> 01:00:23,070
you can get rid of the test

983
01:00:23,080 --> 01:00:24,480
and then i go

984
01:00:25,390 --> 01:00:32,200
these things but expressions back into the ground function centre this beast

985
01:00:34,790 --> 01:00:37,480
basically a plug all the values for theta

986
01:00:37,500 --> 01:00:38,280
thanks i

987
01:00:38,290 --> 01:00:41,470
and from what i had

988
01:00:41,530 --> 01:00:46,360
and this allows me to eliminate all the primal variables

989
01:00:49,720 --> 01:00:54,160
and get this dual problem

990
01:00:54,170 --> 01:00:58,160
this is probative seen before

991
01:00:58,170 --> 01:01:00,050
and convexity ensures

992
01:01:00,110 --> 01:01:01,620
at the minimum value

993
01:01:01,630 --> 01:01:02,900
this unique

994
01:01:02,910 --> 01:01:04,680
it does not guarantee

995
01:01:04,700 --> 01:01:10,440
that the minimise itself is unique just the minimum value is unique

996
01:01:10,480 --> 01:01:15,970
something that often people confuse

997
01:01:20,030 --> 01:01:24,450
the problem is that well depending on how we choose the margin rho

998
01:01:24,500 --> 01:01:32,280
we might get more or fewer novel observations began picking some arbitrary threshold for those

999
01:01:32,330 --> 01:01:37,640
no observations and it doesn't really mean much

1000
01:01:37,650 --> 01:01:41,720
so i would like to get something that's slightly more meaningful

1001
01:01:41,720 --> 01:01:43,660
which hopefully you can calculate

1002
01:01:43,680 --> 01:01:45,490
more precisely

1003
01:01:45,500 --> 01:01:49,080
because it can express is quite neat expression for the

1004
01:01:49,080 --> 01:01:51,260
generating function of the degree

1005
01:01:51,310 --> 01:01:52,950
in terms of that

1006
01:01:53,390 --> 01:01:54,930
he began

1007
01:01:55,160 --> 01:01:57,770
and we can we can we actually

1008
01:01:57,770 --> 01:02:01,180
drive quite quite quite a lot from that fact

1009
01:02:01,290 --> 01:02:06,930
in particular one thing to note is that is that became more constant and i

1010
01:02:06,930 --> 01:02:11,290
was actually wasn't random at all so they know to be doesn't depend on location

1011
01:02:11,290 --> 01:02:12,140
at all

1012
01:02:12,160 --> 01:02:17,990
then you actually end up with a degree structure in the degree distribution which is

1013
01:02:17,990 --> 01:02:20,220
possibly the same as described

1014
01:02:20,310 --> 01:02:25,240
now that doesn't mean that the network is structurally equivalent to register a new network

1015
01:02:25,240 --> 01:02:31,310
with other statistics may not be same particular things like clustering all clustering coefficient degree

1016
01:02:31,310 --> 01:02:35,830
correlations are not going to be necessary it seems to have just read the model

1017
01:02:37,260 --> 01:02:41,620
one one interesting conclusion is that

1018
01:02:41,660 --> 01:02:47,080
if your space is very symmetric homogeneous meaning that you can basically some symmetry which

1019
01:02:47,080 --> 01:02:50,370
takes any point to any other things i think it's something like the surface of

1020
01:02:50,370 --> 01:02:52,970
the sphere for instance

1021
01:02:52,990 --> 01:02:56,600
and in addition you're your to create your network

1022
01:02:56,890 --> 01:02:59,830
note placement x is

1023
01:02:59,830 --> 01:03:03,870
it's actually uniform in your space the the outcome will be constants and you will

1024
01:03:03,870 --> 01:03:05,200
have a process of

1025
01:03:05,220 --> 01:03:06,500
nineteen ninety degree

1026
01:03:06,520 --> 01:03:07,700
one consequence of

1027
01:03:07,720 --> 01:03:12,680
that is quite nice example of a very strong constraining effect of in this case

1028
01:03:12,680 --> 01:03:18,270
the spatial symmetry on on the actual structure of the network structure

1029
01:03:19,990 --> 01:03:24,560
the other thing you can say is that the decrease distribution of the scale free

1030
01:03:25,450 --> 01:03:29,770
if the moments of the random variable fail to exist for some

1031
01:03:29,810 --> 01:03:31,160
essentially the government

1032
01:03:31,180 --> 01:03:35,120
the game itself follows some kind of power law

1033
01:03:35,140 --> 01:03:39,870
the meaning that in fact if you had this fully symmetric space and uniform distribution

1034
01:03:39,890 --> 01:03:43,140
you cannot have scale free networks will not be scale free

1035
01:03:44,740 --> 01:03:48,810
now it all says that it's not too hard to into kind of an engineer

1036
01:03:49,120 --> 01:03:53,080
scale free networks by appropriate choice of

1037
01:03:53,100 --> 01:03:57,660
no distribution x as in that had one model which we saw earlier that's precisely

1038
01:03:57,660 --> 01:03:59,500
what they do

1039
01:03:59,520 --> 01:04:02,540
this artificial pos people come to that

1040
01:04:02,580 --> 01:04:08,450
interesting statistics is a statistic is the variance of that

1041
01:04:08,470 --> 01:04:13,180
one of the big and which may not exist in a particular may not exist

1042
01:04:13,180 --> 01:04:14,990
in the in the scale free case

1043
01:04:15,040 --> 01:04:18,370
it measures if you like the departure from the kind of course on this of

1044
01:04:18,410 --> 01:04:19,720
the degree structure

1045
01:04:20,620 --> 01:04:26,770
statistics clustering coefficient and degree correlations and so on

1046
01:04:26,790 --> 01:04:28,970
as i say in principle

1047
01:04:29,450 --> 01:04:35,720
computable cult calculator all that you can express terms kind of higher

1048
01:04:35,770 --> 01:04:42,160
higher moments if you like so long so this these variants of connectivity

1049
01:04:42,160 --> 01:04:47,890
technically difficult to calculate because you end up with this on riemannian manifold you end

1050
01:04:47,890 --> 01:04:50,540
up with some very nasty integral

1051
01:04:51,540 --> 01:04:57,100
how do we calculate things like component sizes phase transitions and so on well there's

1052
01:04:57,100 --> 01:05:00,740
a problem with using the generating function formalism which is how you might want to

1053
01:05:00,740 --> 01:05:01,810
do it because

1054
01:05:01,890 --> 01:05:05,890
that's very powerful technique unfortunately this model does violate

1055
01:05:05,910 --> 01:05:10,770
some of the assumptions of the model which is that the degree distributions at individual

1056
01:05:10,770 --> 01:05:16,640
nodes on not identically distributed not independent what you can do is you can you

1057
01:05:16,640 --> 01:05:20,310
can come up with some kind of if you like statistical mechanics is called the

1058
01:05:20,450 --> 01:05:26,100
mean field approximation which is actually to ignore the fluctuations correlations of node degree

1059
01:05:26,100 --> 01:05:28,140
five model

1060
01:05:28,160 --> 01:05:31,770
i try this it doesn't work too well as you might expect is that there

1061
01:05:31,770 --> 01:05:36,720
is a lot of variance autocorrelation amongst the degrees with a very

1062
01:05:36,770 --> 01:05:39,040
the simple bunched up

1063
01:05:39,220 --> 01:05:42,720
spatial distributions for instance

1064
01:05:42,740 --> 01:05:47,370
in the mean field approximation you actually find small world scale again that may not

1065
01:05:48,160 --> 01:05:50,870
is it is only an approximation

1066
01:05:50,990 --> 01:05:56,080
this shouldn't be too surprising as i say in fact the original inclination was to

1067
01:05:56,080 --> 01:06:01,080
call this model the small world spatially embedded model i decided not to go on

1068
01:06:01,080 --> 01:06:02,160
the grounds that

1069
01:06:02,160 --> 01:06:06,700
this in fact may not and what you may not be the the case that

1070
01:06:06,700 --> 01:06:11,950
you can fly apply kind of exact version of the generating function for the formalism

1071
01:06:12,040 --> 01:06:16,850
by telling it to you have to do is go through several stages of of

1072
01:06:16,850 --> 01:06:22,330
of averaging of conditioning first on the given node placement so the technical

1073
01:06:22,330 --> 01:06:25,120
and that's difficult in fact probably intractable

1074
01:06:25,310 --> 01:06:28,790
it's about this a bit better than that

1075
01:06:29,020 --> 01:06:33,120
possibly can be done so we could be looking at and practices

1076
01:06:33,220 --> 01:06:37,600
sometimes perturbation of the mean field approximation which again is one statistical

1077
01:06:37,600 --> 01:06:39,700
physicists would be inclined to

1078
01:06:39,740 --> 01:06:45,160
i'm not of statistical physicists to get some sleep problems for anyway

1079
01:06:45,160 --> 01:06:49,720
so now another side effects point is we saw in the i israeli model it's

1080
01:06:49,720 --> 01:06:53,870
quite it's quite clear how you're going to scale the connection probability has you

1081
01:06:53,890 --> 01:06:55,770
network increases in size

1082
01:06:55,790 --> 01:07:00,100
not clear how to do it for this model because your connection probability is is

1083
01:07:00,100 --> 01:07:01,450
that hold k functions now

1084
01:07:01,890 --> 01:07:05,290
so how you actually going to scale so well

1085
01:07:05,330 --> 01:07:09,430
the shape of the like is significant so you might want to scale like this

1086
01:07:09,430 --> 01:07:12,180
is my camera this to is how

1087
01:07:12,220 --> 01:07:14,990
connection probability tales of distance

1088
01:07:15,020 --> 01:07:17,040
now i can just kind of squash that i

1089
01:07:17,160 --> 01:07:19,330
or perhaps

1090
01:07:19,490 --> 01:07:22,020
this way perhaps to squash the whole thing down

1091
01:07:23,240 --> 01:07:28,290
this corresponds more to kind of just read time just simply scaled

1092
01:07:28,310 --> 01:07:34,450
uniformly scale whole probability across the entire distance this one corresponds more to if you

1093
01:07:34,450 --> 01:07:35,930
have the truncation

1094
01:07:37,410 --> 01:07:39,430
can squashing up truncation

1095
01:07:39,520 --> 01:07:44,220
i would make the distances which connections happens more and more that those things turn

1096
01:07:44,220 --> 01:07:45,810
out to be

1097
01:07:45,830 --> 01:07:50,850
to make a rather crucial difference to the statistical properties of the network so

1098
01:07:50,870 --> 01:07:54,660
it's again it's not clear how one should do that necessarily

1099
01:07:55,220 --> 01:07:57,140
so here's some pictures them anyway

1100
01:07:58,700 --> 01:08:03,390
what might look like this is this is a uniform this the space is uniform

1101
01:08:03,390 --> 01:08:06,620
discontinued dimensions the k

1102
01:08:06,640 --> 01:08:08,850
his thresholds so basically

1103
01:08:08,850 --> 01:08:13,260
nodes are connected if less than a certain distance is probably something like that

1104
01:08:13,620 --> 01:08:17,770
so like these are not connected to too far apart

1105
01:08:17,790 --> 01:08:20,680
let's continue to look like

1106
01:08:22,020 --> 01:08:26,580
the you can spatial distribution of nodes kind of radially exponential

1107
01:08:26,600 --> 01:08:30,520
so now it's much more likely to appear near the origin

1108
01:08:31,140 --> 01:08:36,640
and and also the connection probability drops off exponentially with distance to get very dense

1109
01:08:36,640 --> 01:08:39,120
conductivity structure close to the origin

1110
01:08:39,160 --> 01:08:43,700
and it gets much more sparsity move away from the origin

1111
01:08:43,700 --> 01:08:47,160
it will

1112
01:08:51,850 --> 01:08:54,450
and here is

1113
01:09:39,480 --> 01:09:42,710
he was

1114
01:10:01,920 --> 01:10:04,470
you mean

1115
01:10:10,500 --> 01:10:12,450
let me

1116
01:10:27,590 --> 01:10:34,230
you i it

1117
01:10:46,430 --> 01:10:54,440
can anyone

1118
01:10:59,600 --> 01:11:02,250
and in began

1119
01:11:41,780 --> 01:11:44,650
know we

1120
01:12:14,250 --> 01:12:21,670
so get

1121
01:12:29,410 --> 01:12:34,050
given that

1122
01:13:22,790 --> 01:13:27,100
i mean

1123
01:13:47,700 --> 01:13:48,960
i mean

1124
01:14:04,540 --> 01:14:07,250
i mean

1125
01:14:18,380 --> 01:14:20,950
thank you

1126
01:14:20,950 --> 01:14:23,290
on the limits of zeros for the

1127
01:14:23,320 --> 01:14:25,970
for the point in the family

1128
01:14:25,990 --> 01:14:30,670
all right so how does this apply to our case OK so far this polynomial

1129
01:14:30,670 --> 01:14:32,830
family identify

1130
01:14:32,860 --> 01:14:39,440
with i kept by the set of all polynomials that appear here that are nonzero

1131
01:14:40,180 --> 01:14:42,170
and we will denote by m

1132
01:14:42,490 --> 01:14:45,040
the cardinal of this matrix

1133
01:14:46,190 --> 01:14:49,680
this will be the minimum recurrence degree you can prove that

1134
01:14:49,730 --> 01:14:52,060
OK then the linear recurrence

1135
01:14:53,290 --> 01:14:56,420
it can be written like that and we can actually compute

1136
01:14:56,430 --> 01:14:59,110
what these polynomials are

1137
01:14:59,130 --> 01:15:02,640
and we note that the recurrence

1138
01:15:02,720 --> 01:15:05,440
doesn't depend at all on

1139
01:15:06,050 --> 01:15:09,270
on the actual

1140
01:15:09,300 --> 01:15:15,310
the values of the actual court polynomials that appear here only on the set of

1141
01:15:15,520 --> 01:15:18,590
those that are zero

1142
01:15:20,980 --> 01:15:22,930
now the characteristic equation

1143
01:15:23,650 --> 01:15:27,780
can be the like that but for these fjz problem here

1144
01:15:27,790 --> 01:15:30,780
the characteristic equation looks exactly

1145
01:15:30,980 --> 01:15:32,870
like here so x

1146
01:15:32,900 --> 01:15:38,060
appears to all the power that having this is in i

1147
01:15:38,110 --> 01:15:42,330
and these are all the routes OK these are the london

1148
01:15:42,340 --> 01:15:47,640
OK and to compare the absolute values of the land as this just to comparing

1149
01:15:47,690 --> 01:15:52,950
it is easy to some power different parts OK and we see here why the

1150
01:15:52,970 --> 01:15:57,260
the unit circle is so important so if you have

1151
01:15:58,150 --> 01:16:00,830
routes that are equally model that means

1152
01:16:01,890 --> 01:16:05,490
that c has to belong to the unit circle

1153
01:16:05,580 --> 01:16:08,760
if one of them is dominating the others

1154
01:16:08,770 --> 01:16:13,790
then that has to that xe has to be outside

1155
01:16:13,810 --> 01:16:17,000
and of course that will be

1156
01:16:17,040 --> 01:16:22,540
x to the largest element in i which is exactly

1157
01:16:24,380 --> 01:16:26,980
if there are

1158
01:16:27,010 --> 01:16:32,750
limits of zeros outside the unit circle they can only be roots of this polynomial

1159
01:16:32,800 --> 01:16:39,480
and again if there are millions of zero inside only the roots of the sport

1160
01:16:39,510 --> 01:16:43,280
all right so this character is completely characterizes completely the

1161
01:16:43,290 --> 01:16:46,340
the limits of zero and therefore

1162
01:16:46,360 --> 01:16:48,490
the spectrum of the

1163
01:16:48,490 --> 01:16:53,170
polynomial transform that we are designed so we are free to choose

1164
01:16:53,180 --> 01:16:57,350
these AK polynomials anyway we like

1165
01:16:57,370 --> 01:17:02,050
we only need to be careful that the roots of the first and the last

1166
01:17:02,050 --> 01:17:03,070
in the first

1167
01:17:03,080 --> 01:17:07,270
and we done a week we get a polynomial transform

1168
01:17:07,790 --> 01:17:11,550
that approximates the escape africa

1169
01:17:13,040 --> 01:17:17,830
so just show you some examples of how does this

1170
01:17:19,540 --> 01:17:22,800
how do these routes

1171
01:17:22,840 --> 01:17:25,250
approximate the unit circle

1172
01:17:25,260 --> 01:17:30,720
OK so here is the classic example that i've had throughout the talk

1173
01:17:30,740 --> 01:17:35,420
xn minus one we see the periodic boundary conditions

1174
01:17:35,460 --> 01:17:37,110
for n equal twenty

1175
01:17:37,850 --> 01:17:42,940
well sample units are going to call of places the

1176
01:17:42,950 --> 01:17:44,260
influence of unity

1177
01:17:44,270 --> 01:17:47,850
for and equal fifty the units finally grew eighty

1178
01:17:47,890 --> 01:17:49,870
again you could almost see that

1179
01:17:50,210 --> 01:17:53,880
this article is completely covered almost

1180
01:17:53,900 --> 01:17:55,000
OK for

1181
01:17:55,040 --> 01:17:57,700
another polynomial

1182
01:17:57,770 --> 01:18:00,700
this one here

1183
01:18:01,570 --> 01:18:03,310
routes that

1184
01:18:03,320 --> 01:18:04,680
i have

1185
01:18:04,710 --> 01:18:05,850
the same

1186
01:18:05,870 --> 01:18:09,200
experiments that have the same argument and they look like that

1187
01:18:09,290 --> 01:18:10,900
and as you increase

1188
01:18:10,950 --> 01:18:12,180
the degree

1189
01:18:12,190 --> 01:18:15,520
they get closer and closer

1190
01:18:15,530 --> 01:18:16,690
to each other

1191
01:18:17,630 --> 01:18:20,110
and of course they will sample

1192
01:18:20,200 --> 01:18:22,090
they will get two

1193
01:18:22,850 --> 01:18:24,450
sample the

1194
01:18:24,490 --> 01:18:26,520
orientation space

1195
01:18:26,530 --> 01:18:28,710
finer and finer

1196
01:18:28,720 --> 01:18:30,510
OK so it fits

1197
01:18:32,350 --> 01:18:33,440
our goal

1198
01:18:33,440 --> 01:18:36,290
OK some more interesting examples

1199
01:18:36,300 --> 01:18:39,510
for instance a polynomial here that

1200
01:18:41,290 --> 01:18:42,090
he has

1201
01:18:44,250 --> 01:18:45,800
the freedom

1202
01:18:45,800 --> 01:18:46,880
and so

1203
01:18:46,910 --> 01:18:49,740
how would we use this in the natural system

1204
01:18:49,780 --> 01:18:54,280
well i kind of like in stevens analysis by synthesis models

1205
01:18:55,240 --> 01:18:58,950
well basically lattice rescoring his so but

1206
01:19:00,240 --> 01:19:03,220
the problem here is is that this

1207
01:19:03,240 --> 01:19:07,410
model here we've got these multi dimensional articulatory

1208
01:19:10,700 --> 01:19:13,380
the models themselves will be obtained

1209
01:19:13,380 --> 01:19:15,470
as the cartesian product

1210
01:19:15,490 --> 01:19:24,090
of each articulator dimension and this results in enormous computational complexity during search

1211
01:19:24,930 --> 01:19:29,180
well so what do we do we we sort of punt actually decoding and this

1212
01:19:29,180 --> 01:19:30,110
to me

1213
01:19:30,270 --> 01:19:31,660
punters in we

1214
01:19:31,660 --> 01:19:33,130
we we kick the ball in

1215
01:19:33,180 --> 01:19:34,240
something else

1216
01:19:35,280 --> 01:19:36,860
well we can do though

1217
01:19:36,880 --> 01:19:39,840
we can start with our our our

1218
01:19:39,890 --> 01:19:49,010
our article acoustic features we could generate some hypothesized phonetic transcriptions from our generic off-the-shelf

1219
01:19:49,010 --> 01:19:50,450
is our system

1220
01:19:50,510 --> 01:19:51,760
and so

1221
01:19:51,780 --> 01:19:58,340
we will assume that we've got these hypothesized transcriptions here from some generic off-the-shelf system

1222
01:19:59,910 --> 01:20:02,030
articulatory model then

1223
01:20:02,530 --> 01:20:09,110
we ask that to generate these hypothesized acoustics

1224
01:20:10,320 --> 01:20:18,380
for each different hypothesise phone segmentation will generate different set of hypothesized acoustics

1225
01:20:18,610 --> 01:20:20,320
acoustic sequence

1226
01:20:20,340 --> 01:20:23,910
and they will choose the best phonetictranscription

1227
01:20:23,930 --> 01:20:28,880
as that which gives us that which is most plausible according to

1228
01:20:28,880 --> 01:20:30,860
i take the tree model

1229
01:20:31,160 --> 01:20:33,780
here in the acoustic space

1230
01:20:35,490 --> 01:20:37,470
i would like to

1231
01:20:37,970 --> 01:20:42,650
defined some kind of distance between are

1232
01:20:42,660 --> 01:20:48,160
will call these are target acoustic features and the hypothesis ones and will pick the

1233
01:20:49,320 --> 01:20:54,590
sequence as that which is somehow generates the acoustics is closest to the original

1234
01:20:54,610 --> 01:20:57,410
and we've

1235
01:20:57,450 --> 01:21:01,820
seen as i mentioned before we see in the number of different efforts to that

1236
01:21:03,090 --> 01:21:08,950
the bark is himself to model these these dynamics

1237
01:21:08,950 --> 01:21:14,220
the design some basically this empirically designs and fire filters

1238
01:21:14,240 --> 01:21:16,160
john riedl

1239
01:21:16,660 --> 01:21:18,150
so looked at using

1240
01:21:18,160 --> 01:21:21,800
deterministic a deterministic hidden dynamic model

1241
01:21:23,820 --> 01:21:29,970
explored this vocal tract resonance dynamics both of these are these last two are something

1242
01:21:29,970 --> 01:21:33,280
like common filters and you like like you would expect some

1243
01:21:33,320 --> 01:21:34,650
something that

1244
01:21:35,410 --> 01:21:38,700
describe the dynamics here of

1245
01:21:38,820 --> 01:21:44,360
given that we've we've made some assumptions about the form of our model we can

1246
01:21:44,360 --> 01:21:48,470
describe its dynamics in terms of something like a call cells are

1247
01:21:50,450 --> 01:21:54,660
the the acoustic articulatory to acoustic mapping

1248
01:21:54,680 --> 01:21:58,970
has generally been in the form of some sort of a of a static mapping

1249
01:21:58,970 --> 01:22:00,470
radial basis function

1250
01:22:00,490 --> 01:22:04,510
classifiers multilayer perceptron

1251
01:22:05,630 --> 01:22:11,280
most of the evaluations of these have been mostly sort of anecdotal utterances and so

1252
01:22:11,280 --> 01:22:17,010
you have your say we're live on these handful of utterances i'm seeing reasonable acoustics

1253
01:22:17,050 --> 01:22:21,840
they really are my hypotheses and reasonable way but they not these have not

1254
01:22:24,910 --> 01:22:27,070
we've we've got plenty of time right

1255
01:22:27,760 --> 01:22:29,650
it's fact

1256
01:22:29,660 --> 01:22:33,930
it's OK if you're approaching the table there are thirty

1257
01:22:34,070 --> 01:22:40,860
to be the next the second speaker who

1258
01:22:42,910 --> 01:22:44,880
four so

1259
01:22:44,910 --> 01:22:48,470
right so again this is another technique that but this is the holy grail if

1260
01:22:48,470 --> 01:22:51,380
you could come up with a

1261
01:22:51,390 --> 01:22:56,200
an accurate and robust model like this you could use it for so much

1262
01:22:56,280 --> 01:23:01,720
and you could use it for sentences you could use it for recognition and and

1263
01:23:03,570 --> 01:23:05,410
it's not quite there

1264
01:23:05,430 --> 01:23:10,510
but it's a but it's quite a fascinating research topic you can find

1265
01:23:10,530 --> 01:23:18,240
people were when i was about lads we had people who are doing collaborative projects

1266
01:23:18,240 --> 01:23:23,450
with the group at general dynamics to try and actually use their their wind tunnels

1267
01:23:24,220 --> 01:23:26,760
you might come up with full

1268
01:23:26,780 --> 01:23:32,430
completes articulatory dynamic models which incorporated the airflow and everything else

1269
01:23:32,450 --> 01:23:35,510
and it's those kinds of people that are

1270
01:23:35,550 --> 01:23:39,160
taking on the tough problems like this

1271
01:23:41,680 --> 01:23:43,090
finally the so

1272
01:23:43,110 --> 01:23:48,240
to conclude our time our sort of second thing here and how our exploiting speech

1273
01:23:48,240 --> 01:23:51,430
of the individual weight vectors if you want to think about it in these terms

1274
01:23:51,430 --> 01:23:53,320
because it's all stick together

1275
01:23:53,450 --> 01:24:00,010
and you add a term for the slack variables OK there should be actually see

1276
01:24:00,010 --> 01:24:03,550
year like usual you can you can wait this differently

1277
01:24:03,550 --> 01:24:07,030
and the margin constraint the way you would be funded knows he would just say

1278
01:24:07,030 --> 01:24:13,260
OK i want to minimize that with regard to w excited such that the margin

1279
01:24:13,260 --> 01:24:17,470
that obtain on a particular example is at least one or whatever

1280
01:24:17,490 --> 01:24:22,430
i've chosen the slack variables to be lower the modern it right so in the

1281
01:24:22,450 --> 01:24:24,090
in the

1282
01:24:24,110 --> 01:24:28,340
the separable case right where you wouldn't have a soft margin this would go away

1283
01:24:28,340 --> 01:24:32,280
here and then this would go away you would actually require the gamma i has

1284
01:24:32,280 --> 01:24:36,660
to be great equal to one as one of the constraints and then in the

1285
01:24:36,660 --> 01:24:40,990
soft margin case you expect variables that need to be nonnegative and you have a

1286
01:24:40,990 --> 01:24:42,130
penalty term here

1287
01:24:42,180 --> 01:24:47,140
so this is not a this is not a quadratic programme

1288
01:24:47,160 --> 01:24:49,380
and that becomes important

1289
01:24:49,400 --> 01:24:53,300
not so much for multiclass problem but it will become much more important when we

1290
01:24:53,300 --> 01:24:59,430
talk about structured classification structured prediction is that if we look at this now here

1291
01:24:59,760 --> 01:25:03,240
right this constraint is actually

1292
01:25:03,260 --> 01:25:08,340
constraint that involves a maximum right because we had defined the constrained in the way

1293
01:25:08,340 --> 01:25:13,740
that we said on this score of the correct class and what is the score

1294
01:25:13,760 --> 01:25:16,320
of the second best sorry the

1295
01:25:16,360 --> 01:25:21,880
the one that gets the highest score among the incorrect one

1296
01:25:21,910 --> 01:25:27,010
right and so we had this maximum but we can basically expand this right so

1297
01:25:27,010 --> 01:25:28,990
this one constraint

1298
01:25:29,030 --> 01:25:32,340
because basically as we've already seen if we get the

1299
01:25:32,360 --> 01:25:34,130
you get a certain margin

1300
01:25:34,140 --> 01:25:37,570
right separation margin between the correct one and the

1301
01:25:37,570 --> 01:25:41,760
best incorrect one then clearly we get at least as much and also for the

1302
01:25:41,760 --> 01:25:47,160
other incorrect ones right because by definition right they are not the ones among the

1303
01:25:47,160 --> 01:25:52,220
incorrect one that get the highest score so basically we can just expand it follows

1304
01:25:52,220 --> 01:25:58,180
that the difference between that we simply require that the difference between the score we

1305
01:25:58,180 --> 01:26:03,320
are getting for correct class and what we were getting for any y that is

1306
01:26:03,320 --> 01:26:08,300
different from the correct class that that difference in score needs to be at least

1307
01:26:08,300 --> 01:26:14,860
one or one minus something that we subtract using this library books i can

1308
01:26:14,860 --> 01:26:17,860
so is this equivalence is that you

1309
01:26:17,910 --> 01:26:21,320
so if we you know basically if we go to this picture here

1310
01:26:21,380 --> 01:26:25,660
right just basically means that now we saying OK we want the difference in score

1311
01:26:25,680 --> 01:26:30,740
between these two to be greater equal one and these two in these two

1312
01:26:30,740 --> 01:26:35,110
and these two right and we have many more than we get any more

1313
01:26:35,130 --> 01:26:37,990
the creation so the thing is that here

1314
01:26:37,990 --> 01:26:42,140
you know we just have one equation that's not nonlinear because it involves the max

1315
01:26:42,180 --> 01:26:43,260
we can just

1316
01:26:44,820 --> 01:26:48,840
and rolled this maximum and then we get lots of linear constraints namely we get

1317
01:26:48,840 --> 01:26:53,090
as many constraints as we have classes basically minus one and then we get this

1318
01:26:53,090 --> 01:26:56,680
constraint here so the total number of constraints

1319
01:26:56,700 --> 01:27:01,910
that we would be getting in our in our pocket drastic program in the end

1320
01:27:01,950 --> 01:27:06,490
would be and the number of samples that we have times the cardinality of the

1321
01:27:07,550 --> 01:27:09,110
OK so

1322
01:27:09,130 --> 01:27:12,510
if we have a relatively small number of classes then

1323
01:27:12,720 --> 01:27:15,930
we might be able to handle this if it

1324
01:27:16,630 --> 01:27:22,030
the cardinality of wire several ten thousand or so as you know could be in

1325
01:27:22,070 --> 01:27:26,090
in the problem with large taxonomy then already this becomes a bit nasty to deal

1326
01:27:32,530 --> 01:27:33,590
so now

1327
01:27:33,610 --> 01:27:35,090
i'd like to

1328
01:27:35,090 --> 01:27:40,340
move from multiclass classification two

1329
01:27:40,360 --> 01:27:46,760
really the core of this talk which is too strong to do structured prediction and

1330
01:27:46,760 --> 01:27:51,010
the reason why i talked about multiclass classification in the first place is as i

1331
01:27:51,010 --> 01:27:54,900
said before that it can be you can think of as an evil way of

1332
01:27:54,900 --> 01:27:59,650
you can decide to one dish ten days of two thousand images something

1333
01:27:59,650 --> 01:28:03,570
you really picks them up

1334
01:28:03,570 --> 01:28:05,840
and subsequent customer

1335
01:28:06,170 --> 01:28:09,800
subsequent customer that comes in the customer i

1336
01:28:09,840 --> 01:28:12,840
we will have two things to do

1337
01:28:12,860 --> 01:28:16,490
among the dishes that previous customers has tried

1338
01:28:16,490 --> 01:28:22,300
so let's say dish k customer i will try dish k with probability and k

1339
01:28:22,320 --> 01:28:27,720
divided by i where mk is the number of previous customers that have tried dish

1340
01:28:27,780 --> 01:28:31,760
so at this stage is very popular has been tried by lots of different customers

1341
01:28:31,760 --> 01:28:35,110
in the past the next customer will will

1342
01:28:35,130 --> 01:28:38,920
we want to try the dish as well as the popular dish

1343
01:28:38,920 --> 01:28:43,450
and in addition to that

1344
01:28:43,470 --> 01:28:45,590
the customer will also try

1345
01:28:45,610 --> 01:28:50,630
cross on alfalfa i shows that no other customers have tried

1346
01:28:50,760 --> 01:28:55,510
so in this case customer one decides to try six dishes

1347
01:28:55,530 --> 01:28:59,110
and out of the six dishes customer two

1348
01:28:59,110 --> 01:29:02,690
i will try each of this fish with probability one over

1349
01:29:03,780 --> 01:29:06,450
because is one and i is two

1350
01:29:06,470 --> 01:29:07,360
right so

1351
01:29:07,380 --> 01:29:11,950
with probability half the second customer we try each of the six dishes

1352
01:29:11,950 --> 01:29:16,240
and then this customer we also try a so the number of dishes that customer

1353
01:29:16,240 --> 01:29:17,900
one has tried before

1354
01:29:17,920 --> 01:29:21,300
so in this case because the two decided to try treat issues the customer one

1355
01:29:21,300 --> 01:29:22,530
did not try

1356
01:29:22,670 --> 01:29:24,970
and similarly customer tree

1357
01:29:24,990 --> 01:29:28,220
comes i will try each dish with probability

1358
01:29:28,220 --> 01:29:31,220
so we tried dish

1359
01:29:31,220 --> 01:29:35,170
we tried this tree with probability one over tree

1360
01:29:35,190 --> 01:29:39,070
we tried dish full with probability to the tree

1361
01:29:39,240 --> 01:29:42,880
and this goes on to fish like

1362
01:29:42,900 --> 01:29:45,630
and if a customer will also try

1363
01:29:45,670 --> 01:29:49,450
at a number of dishes that previous customers did not like before

1364
01:29:49,470 --> 01:29:50,970
and this can repeats itself

1365
01:29:53,990 --> 01:29:55,110
and we can

1366
01:29:55,130 --> 01:29:59,340
in fact derive this from this model where k goes to infinity and you can

1367
01:29:59,340 --> 01:30:01,630
kind of see this because

1368
01:30:01,650 --> 01:30:07,030
let's look at the first customer right the first customer we try each

1369
01:30:07,050 --> 01:30:07,650
the che

1370
01:30:08,590 --> 01:30:09,880
with probability

1371
01:30:09,880 --> 01:30:14,420
me OK but if we integrate me OK this probability is going to work out

1372
01:30:14,760 --> 01:30:16,340
to be alfalfa k

1373
01:30:16,380 --> 01:30:19,510
the book divided by one plus of of wiki

1374
01:30:19,530 --> 01:30:25,030
so this is this to privatise normalized

1375
01:30:25,070 --> 01:30:28,700
and that's basically going to be more or less equal to alpha over k

1376
01:30:28,760 --> 01:30:31,590
if k is very large

1377
01:30:34,260 --> 01:30:39,970
the customer will try each of this vicious independently so this is kind of input

1378
01:30:39,970 --> 01:30:41,970
and independence

1379
01:30:41,990 --> 01:30:46,050
a set of independent the new lease each with probability that approaches zero and you

1380
01:30:46,050 --> 01:30:47,400
can can work out that

1381
01:30:48,110 --> 01:30:49,780
the number of the new release

1382
01:30:49,820 --> 01:30:55,550
goes to infinity this the number of that picked by the customer is this can

1383
01:30:55,550 --> 01:30:59,990
be passed on distributed and the parameter was set to be puzzle

1384
01:31:00,010 --> 01:31:01,610
for some of alpha

1385
01:31:03,450 --> 01:31:06,530
anyway so it's kind of the exercise two to

1386
01:31:06,550 --> 01:31:08,880
to derive distinct from the previous page

1387
01:31:09,760 --> 01:31:15,260
know small all forms

1388
01:31:21,700 --> 01:31:25,240
this thing actually has

1389
01:31:25,280 --> 01:31:27,450
except for this of

1390
01:31:27,470 --> 01:31:32,420
representation it doesn't really have a close connection to dirichlet process it does have some

1391
01:31:32,420 --> 01:31:35,590
connections to the judicial process as we'll will see later but it doesn't have that

1392
01:31:35,590 --> 01:31:37,190
much of a connection

1393
01:31:41,920 --> 01:31:47,780
so it turns out that the IBP as described here is also infinitely exchangeable and

1394
01:31:47,780 --> 01:31:52,200
we can now see that it has to be infinitely exchangeable because this finite model

1395
01:31:52,200 --> 01:31:58,570
is infinitely exchangeable right we're treating each customer

1396
01:31:59,260 --> 01:32:01,200
independence given our

1397
01:32:01,220 --> 01:32:02,930
parameters mu

1398
01:32:03,740 --> 01:32:08,220
we can permute the customers around and it actually doesn't affect the probability

1399
01:32:09,860 --> 01:32:14,950
you can also show that this process is also affiliation but the tricky bit here

1400
01:32:14,950 --> 01:32:18,950
is that we cannot have to forget about the ordering of the

1401
01:32:18,970 --> 01:32:22,820
like if you look at this one it will always be tried by customer one

1402
01:32:22,820 --> 01:32:26,150
well unless customer one decided not to try any additional

1403
01:32:26,150 --> 01:32:31,230
do which is to use what's called a log linear model

1404
01:32:31,240 --> 01:32:35,520
in the new models that represent the probability of the state as a product of

1405
01:32:35,520 --> 01:32:39,430
potential functions we represent as an exponentiated so

1406
01:32:39,610 --> 01:32:42,700
and of terms which after all they can always convert from one to the other

1407
01:32:42,800 --> 01:32:47,290
so we still have the normalisation constant now we have have the exponentially some of

1408
01:32:47,520 --> 01:32:53,440
is a sign of a set of features times the weights

1409
01:32:53,450 --> 01:32:58,750
now in one extreme i can just have one feature for each state as each

1410
01:32:58,750 --> 01:33:04,380
clique and so i can we implement any potential function model in this way

1411
01:33:04,430 --> 01:33:06,010
the nice thing though is that

1412
01:33:06,190 --> 01:33:07,580
i could have

1413
01:33:07,600 --> 01:33:11,590
many fewer features than there are states of the potential functions

1414
01:33:11,690 --> 01:33:15,410
if i have a very large fleet with a huge number of states but i

1415
01:33:15,410 --> 01:33:18,060
know that there's only ten important features

1416
01:33:18,070 --> 01:33:20,730
then i can just represent those and their weights

1417
01:33:20,780 --> 01:33:23,760
so my model stay compact even

1418
01:33:23,770 --> 01:33:28,020
if the potential function with be very large and we're going to take full advantage

1419
01:33:28,020 --> 01:33:28,810
of this

1420
01:33:28,820 --> 01:33:32,470
for example the potential function that we saw before

1421
01:33:32,480 --> 01:33:35,590
could be represented simply by one feature

1422
01:33:35,610 --> 01:33:41,450
of the smoking cancer which is one if you don't have cancer and zero otherwise

1423
01:33:41,450 --> 01:33:45,340
and if you give one point five weights to that teacher you get exact potential

1424
01:33:45,340 --> 01:33:46,850
function that we saw

1425
01:33:48,850 --> 01:33:53,150
so there's famous mysterious in markov networks called the hammersley clifford theorem

1426
01:33:53,170 --> 01:33:54,610
which is the following

1427
01:33:54,660 --> 01:33:56,960
suppose we have a distribution

1428
01:33:57,060 --> 01:34:02,390
there is always nonnegative so there's always strictly positive

1429
01:34:02,400 --> 01:34:06,150
and graph encodes conditional independences

1430
01:34:06,170 --> 01:34:08,730
in your in your domain then

1431
01:34:08,740 --> 01:34:12,700
the distribution is a product the potentials of the cliques of the graph

1432
01:34:12,710 --> 01:34:17,130
so is that i i told me two things here may know a proof that

1433
01:34:17,130 --> 01:34:21,970
they were equivalent it turns out that they are equivalent the conditional independencies and the

1434
01:34:21,970 --> 01:34:27,270
potential for functions as long as every state has non-zero probability and the inverse of

1435
01:34:27,270 --> 01:34:28,590
this is also true

1436
01:34:28,600 --> 01:34:31,910
it's as its successful also a lot easier to prove

1437
01:34:31,950 --> 01:34:36,530
and you can think of this theorem in in in short form as a markov

1438
01:34:36,530 --> 01:34:39,000
network is the same as it gives distribution

1439
01:34:39,010 --> 01:34:42,870
it gives distribution is is this thing that i just showed here

1440
01:34:42,920 --> 01:34:48,230
young statisticians call it along models physicists call it gives distribution and what the hammersley

1441
01:34:48,230 --> 01:34:52,810
clifford theorem and even say is the the markov network meaning the graph with the

1442
01:34:52,810 --> 01:34:57,370
conditional independencies i described in the gibbs distribution are in the in effect the same

1443
01:34:58,170 --> 01:35:02,160
OK so we know we can we can use one or the other as as

1444
01:35:02,160 --> 01:35:04,980
appropriate and that's we're taking advantage

1445
01:35:04,990 --> 01:35:08,520
so how the markov networks compared to bayesian networks

1446
01:35:10,860 --> 01:35:14,710
they both have the same for both in the form of products potential so that

1447
01:35:14,910 --> 01:35:16,360
they are very similar

1448
01:35:16,650 --> 01:35:21,610
the big difference is that in markov networks the potentials are arbitrary functions

1449
01:35:21,870 --> 01:35:26,150
the bayes nets the potentials are forced to be only conditional probabilities of some variables

1450
01:35:26,150 --> 01:35:27,290
given this

1451
01:35:27,420 --> 01:35:32,030
so in this way markov networks are better than bayes nets because the more flexible

1452
01:35:32,030 --> 01:35:33,780
also in bayes nets

1453
01:35:33,790 --> 01:35:35,780
we allow cycles

1454
01:35:35,800 --> 01:35:39,770
whereas in india is not so can tell cycles a cycle in in in the

1455
01:35:39,800 --> 01:35:44,470
a directed graph does not correspond to any distribution and this lexical is gonna come

1456
01:35:44,470 --> 01:35:47,810
in very handy when you try to model relational domains

1457
01:35:47,820 --> 01:35:53,360
to offset these two advantages of markov networks the disadvantage that that they have is

1458
01:35:53,360 --> 01:35:58,250
that the partition function z the normalising constant can be very hard to compute whereas

1459
01:35:58,260 --> 01:36:01,960
in bayes networks is completely trivial to compute is just one

1460
01:36:01,970 --> 01:36:03,190
in fact if you say

1461
01:36:03,200 --> 01:36:06,940
i'm going to have a lot linear model the partition function is one where you

1462
01:36:06,940 --> 01:36:11,440
end up with necessarily going to be a bayesian network OK so those are the

1463
01:36:11,440 --> 01:36:12,730
pros and cons

1464
01:36:12,750 --> 01:36:17,100
here are some more interesting differences how the czech independence in markov networks just grab

1465
01:36:18,290 --> 01:36:22,150
in bayes nets the separation so much more complicated criteria

1466
01:36:22,170 --> 01:36:25,980
you know i was the graph basically you know says what it seems to be

1467
01:36:25,980 --> 01:36:32,200
saying was in bayesian networks the graph is actually somewhat misleading tonight is the lack

1468
01:36:32,200 --> 01:36:37,440
of medicine business are incomparable in the sense that each one of them can represent

1469
01:36:37,450 --> 01:36:41,800
the graph can represent some independence properties that the other one count

1470
01:36:41,830 --> 01:36:45,440
so some things are called markov network with a lot of competition network and vice

1471
01:36:46,310 --> 01:36:51,560
however the most important thing to remember is that the mathematical form of log linear

1472
01:36:51,560 --> 01:36:54,170
models be contacted either case

1473
01:36:54,180 --> 01:36:57,530
so if we can represent as a log linear things you know anything that's compact

1474
01:36:57,530 --> 01:37:02,470
bayesian networks is a compact linear model and anything that compact markov network is a

1475
01:37:02,470 --> 01:37:04,710
compact linear models

1476
01:37:04,720 --> 01:37:05,480
and now

1477
01:37:05,490 --> 01:37:10,420
lastly how do you do inference well we're going to look more that shortly followed

1478
01:37:10,420 --> 01:37:14,580
governments this things like markov chain monte carlo belief propagation and others for bees and

1479
01:37:14,580 --> 01:37:17,410
so i want

1480
01:37:17,410 --> 01:37:20,580
this property

1481
01:37:20,580 --> 01:37:23,520
to be true and i want to be in this set

1482
01:37:23,520 --> 01:37:26,310
and this corresponds to

1483
01:37:26,350 --> 01:37:30,500
the fat

1484
01:37:30,580 --> 01:37:33,080
the time could talking about so why

1485
01:37:33,100 --> 01:37:35,960
will represent the best

1486
01:37:36,020 --> 01:37:37,040
the good one

1487
01:37:37,060 --> 01:37:42,160
so what i want is that if i mean this that one needs be true

1488
01:37:44,180 --> 01:37:47,460
and i also want

1489
01:37:54,600 --> 01:37:56,850
let me

1490
01:37:56,870 --> 01:37:59,080
i try to explain so

1491
01:37:59,100 --> 01:38:02,390
i have way

1492
01:38:05,580 --> 01:38:09,080
to be really here

1493
01:38:09,080 --> 01:38:11,540
so actually this is not the type

1494
01:38:11,600 --> 01:38:13,060
this is

1495
01:38:13,080 --> 01:38:16,620
the nodes

1496
01:38:16,710 --> 01:38:18,330
along this path

1497
01:38:18,350 --> 01:38:21,520
with the whole

1498
01:38:21,520 --> 01:38:26,640
OK and as i have infinitely often b

1499
01:38:26,690 --> 01:38:31,460
this why is set which is infinitely long

1500
01:38:31,480 --> 01:38:33,910
so while represents the set

1501
01:38:33,920 --> 01:38:37,730
of nodes along this path

1502
01:38:37,730 --> 01:38:39,660
will be holds

1503
01:38:41,190 --> 01:38:43,710
so and so

1504
01:38:45,980 --> 01:38:47,040
the now

1505
01:38:47,040 --> 01:38:51,100
and i mean is that i mean i mean the sets of those nodes along

1506
01:38:51,100 --> 01:38:54,730
this very person talking about the one supposed to

1507
01:38:55,810 --> 01:39:00,730
so i'm along this path them on on the position along this path will be

1508
01:39:03,000 --> 01:39:05,730
i'm still in the past but

1509
01:39:05,750 --> 01:39:11,100
i mean i know where i promise you that in the future

1510
01:39:11,120 --> 01:39:13,580
i would which

1511
01:39:13,600 --> 01:39:15,330
some position would be whole

1512
01:39:16,100 --> 01:39:18,750
i will be in way

1513
01:39:20,000 --> 01:39:21,440
let me show you

1514
01:39:21,460 --> 01:39:24,480
this time formula

1515
01:39:24,500 --> 01:39:26,140
so here what have

1516
01:39:26,140 --> 01:39:31,310
if i forget this one

1517
01:39:31,310 --> 01:39:34,910
so how would you first compute this one you have to do iterations

1518
01:39:34,910 --> 01:39:40,140
two things have one which says me OK i take a set of nodes

1519
01:39:40,160 --> 01:39:42,270
and the associated to it

1520
01:39:46,390 --> 01:39:49,250
this thing here

1521
01:39:51,250 --> 01:40:13,410
this the definition

1522
01:40:16,560 --> 01:40:21,350
i want to compute the greatest fix point of this one

1523
01:40:21,350 --> 01:40:23,350
so how have to iterate

1524
01:40:23,370 --> 01:40:26,080
from the entire set

1525
01:40:26,140 --> 01:40:29,520
however this descending change

1526
01:40:29,540 --> 01:40:34,060
but it's time i have to compute so i start with these are zero one

1527
01:40:34,100 --> 01:40:35,480
so star

1528
01:40:35,500 --> 01:40:40,980
i have to compute this so let me give the name to this thing here

1529
01:40:41,040 --> 01:40:43,640
that we call f of a

1530
01:40:43,730 --> 01:40:48,690
so i have to compute f of

1531
01:40:49,180 --> 01:40:50,770
but what is the f of ten

1532
01:40:50,790 --> 01:40:55,660
if n is at least fixpoint of somebody

1533
01:40:55,750 --> 01:40:58,520
f of

1534
01:41:03,040 --> 01:41:07,330
to compute this one has to iterate for

1535
01:41:07,330 --> 01:41:09,790
the empty set

1536
01:41:09,810 --> 01:41:14,890
and now to compute

1537
01:41:14,940 --> 01:41:19,480
this thing here

1538
01:41:19,500 --> 01:41:23,680
which is

1539
01:41:25,940 --> 01:41:27,870
of n

1540
01:41:27,890 --> 01:41:30,330
and said

1541
01:41:32,330 --> 01:41:34,890
of n

1542
01:41:36,480 --> 01:41:38,330
of n

1543
01:41:38,370 --> 01:41:40,310
and set

1544
01:41:40,350 --> 01:41:48,160
and so on

1545
01:41:48,180 --> 01:41:50,870
so this set

1546
01:41:50,960 --> 01:41:58,310
will be

1547
01:41:58,330 --> 01:42:01,690
so the set of nodes so the this is the empty set so you replace

1548
01:42:02,500 --> 01:42:03,790
by the end to set

1549
01:42:03,810 --> 01:42:05,330
and and

1550
01:42:05,350 --> 01:42:09,540
and and y is replaced it is replaced by y

1551
01:42:09,580 --> 01:42:14,100
well it's always replaced by and so it means that

1552
01:42:20,870 --> 01:42:25,000
the empty set so either i mean the empty set

1553
01:42:26,270 --> 01:42:28,020
unable so sorry

1554
01:42:28,080 --> 01:42:30,940
i don't have the success of which is in the empty set

1555
01:42:34,000 --> 01:42:37,540
or have a successor which is

1556
01:42:37,560 --> 01:42:43,020
label baby

1557
01:42:43,060 --> 01:42:45,410
this is the thing

1558
01:42:47,910 --> 01:42:51,100
i keep iterating to get the least fixpoint

1559
01:42:51,120 --> 01:42:55,350
so first of all what do a catch on the first set

1560
01:42:55,370 --> 01:42:57,020
i catch

1561
01:42:57,020 --> 01:42:59,230
note that the labelled data

1562
01:42:59,250 --> 01:43:00,850
the second step of the

1563
01:43:00,850 --> 01:43:03,770
computation of this looks six point

1564
01:43:05,940 --> 01:43:07,770
i have a success

1565
01:43:07,810 --> 01:43:10,810
was able b

1566
01:43:16,210 --> 01:43:17,790
the success

1567
01:43:17,870 --> 01:43:20,910
was labelled by b

1568
01:43:20,910 --> 01:43:26,250
on the scene and

1569
01:43:30,500 --> 01:43:33,290
what i have

1570
01:43:33,310 --> 01:43:37,140
is that basically in one step

1571
01:43:37,160 --> 01:43:40,040
this convergence

1572
01:43:40,060 --> 01:43:41,390
and i have

1573
01:43:42,060 --> 01:43:46,910
computation of this f of n which is the set of nodes

1574
01:43:46,910 --> 01:43:50,850
we're not the full than today right

1575
01:43:51,000 --> 01:43:56,940
after iterations here sorry should have told me that this and is my iteration of

1576
01:43:56,940 --> 01:44:00,250
my greatest fixpoint here

1577
01:44:01,270 --> 01:44:03,640
this and is

1578
01:44:03,690 --> 01:44:07,060
zero and stuff

1579
01:44:07,060 --> 01:44:13,890
here i see that my but still because it's getting very difficult to understand OK

1580
01:44:13,890 --> 01:44:16,420
so i will not do it on the board what you need to do is

1581
01:44:16,420 --> 01:44:18,140
that i have

1582
01:44:18,310 --> 01:44:20,440
greatest fixpoint to compute

1583
01:44:20,460 --> 01:44:21,770
and it's time

1584
01:44:21,790 --> 01:44:26,350
so my greatest fixpoint is computed on the basis of some iterations

1585
01:44:26,370 --> 01:44:28,330
at each iteration

1586
01:44:28,350 --> 01:44:32,790
the meaning of this great the meaning of my function

1587
01:44:32,810 --> 01:44:34,410
is itself

1588
01:44:34,410 --> 01:44:36,520
so the way to compute this one

1589
01:44:37,660 --> 01:44:40,500
the computation of least fixpoint

1590
01:44:40,560 --> 01:44:42,540
so you would put it on your

1591
01:44:42,560 --> 01:44:44,710
on your paper here with the

1592
01:44:44,810 --> 01:44:47,810
you penn and you will remain quietly

1593
01:44:48,410 --> 01:44:49,600
what was done

1594
01:44:49,660 --> 01:44:51,350
for just one

1595
01:44:51,370 --> 01:44:52,870
fixpoint operator

1596
01:44:52,910 --> 01:44:56,390
but what you have to do is that for each iteration of the greatest fix

1597
01:44:56,390 --> 01:44:59,210
point you have to iterate

1598
01:44:59,270 --> 01:45:04,410
but this fix point until civilization when the stabilisers you you you have achieved the

1599
01:45:04,410 --> 01:45:05,870
computation of this

1600
01:45:05,870 --> 01:45:09,850
the right hand side and since there epsilon and over two of the probability is

1601
01:45:09,850 --> 01:45:13,030
to the minimize epsilon two that's basically

1602
01:45:13,050 --> 01:45:18,290
but now we're really the case so this is the kind of thing we like

1603
01:45:18,290 --> 01:45:21,040
an exponential decay in probability

1604
01:45:21,060 --> 01:45:23,720
with the sample size

1605
01:45:23,730 --> 01:45:25,410
OK this is looking

1606
01:45:25,430 --> 01:45:30,230
pretty good and so now what we can do is set this equal to

1607
01:45:30,710 --> 01:45:33,860
delta over two BH two and

1608
01:45:33,870 --> 01:45:34,690
which was

1609
01:45:34,750 --> 01:45:39,640
come from we're now back to putting the whole thing together effectively

1610
01:45:39,660 --> 01:45:42,150
that to be achieved two and is here

1611
01:45:42,170 --> 01:45:44,990
what we want to do is to make this less than delta

1612
01:45:44,990 --> 01:45:49,000
so going to make this quantity less than delta divided by two h

1613
01:45:52,990 --> 01:45:54,850
then we can invert

1614
01:45:55,820 --> 01:45:57,940
to give a bound on epsilon

1615
01:45:59,940 --> 01:46:01,370
if you

1616
01:46:01,370 --> 01:46:02,820
i want to spend just

1617
01:46:02,830 --> 01:46:07,690
two minutes trying to do that so just said this is equal to that and

1618
01:46:08,720 --> 01:46:14,170
where we know what is the age of two enemies from sours lemma

1619
01:46:18,340 --> 01:46:20,020
just got that down

1620
01:46:20,030 --> 01:46:22,380
OK that sounds like

1621
01:46:22,400 --> 01:46:24,290
this cells

1622
01:46:24,300 --> 01:46:28,400
so h and is less than or equal to this quantity

1623
01:46:28,430 --> 01:46:33,210
these dimension d

1624
01:46:39,790 --> 01:46:41,230
so you can plug that in

1625
01:46:41,250 --> 01:46:42,580
in place of this

1626
01:46:42,590 --> 01:46:44,450
i said that equal to

1627
01:46:44,570 --> 01:46:47,770
that and solve for

1628
01:46:47,800 --> 01:46:49,820
and so on

1629
01:46:49,840 --> 01:46:56,230
this could be too

1630
01:48:19,470 --> 01:48:31,290
well it's the expectation of the other probability so that's why you say anything

1631
01:48:31,300 --> 01:48:35,370
is that it's the thing i'm not sure is the right

1632
01:48:37,880 --> 01:48:39,310
i mean that was

1633
01:48:40,780 --> 01:48:44,360
i mean that's essentially this

1634
01:48:44,380 --> 01:48:46,490
this thing here right OK

1635
01:48:49,280 --> 01:48:50,670
i mean

1636
01:48:52,860 --> 01:48:59,880
yes because i mean this is just another way of writing this essentially it's not

1637
01:48:59,880 --> 01:49:03,040
as a means to notation is just saying

1638
01:49:03,060 --> 01:49:05,500
what we're doing is working at the probability

1639
01:49:12,120 --> 01:49:20,840
i mean it's the expectation of

1640
01:49:51,880 --> 01:49:54,140
here here

1641
01:49:56,660 --> 01:49:59,620
can you

1642
01:50:10,700 --> 01:50:11,870
i don't see i do

1643
01:50:12,810 --> 01:50:16,180
so the question was can you go directly from here to

1644
01:50:16,200 --> 01:50:17,620
two here right was

1645
01:50:18,820 --> 01:50:20,870
or rather

1646
01:50:23,110 --> 01:50:24,610
from here to here

1647
01:50:24,610 --> 01:50:30,880
useful technique which allows you to simultaneously diagonalizable matrix a

1648
01:50:30,910 --> 01:50:34,210
and you here's an orthogonal matrix x

1649
01:50:34,220 --> 01:50:38,290
is this just a set of vectors

1650
01:50:38,300 --> 01:50:40,650
which has some special properties

1651
01:50:40,740 --> 01:50:46,460
and then c is again diagonalized the ratio alpha to gamma those are what's called

1652
01:50:46,460 --> 01:50:52,040
the generalized singular values they come up in lots of different applications and that's what

1653
01:50:52,040 --> 01:50:54,400
i've done here is shown the manipulations

1654
01:50:54,600 --> 01:50:59,980
that the as new goes to infinity then you get convergence so it's really a

1655
01:50:59,980 --> 01:51:01,690
nice way of the

1656
01:51:01,800 --> 01:51:06,430
approaching this problem if you have one piece of software for solving least squares problem

1657
01:51:06,490 --> 01:51:07,330
then you can

1658
01:51:08,810 --> 01:51:14,290
convert to a problem where you have constrained by just adding in these constraints in

1659
01:51:14,290 --> 01:51:16,070
the simple way

1660
01:51:16,080 --> 01:51:21,100
but the techniques and we just mentioned this is the final method

1661
01:51:21,540 --> 01:51:25,200
and i like this approach

1662
01:51:25,210 --> 01:51:28,540
because it reduces the dimensionality of the problem

1663
01:51:28,650 --> 01:51:34,640
so here again we have minimize say x and we have linear constraint and now

1664
01:51:34,990 --> 01:51:41,580
we have what we do with QR factorizations of the matrix c member c transpose

1665
01:51:41,590 --> 01:51:42,300
is that

1666
01:51:42,300 --> 01:51:43,380
a lot

1667
01:51:43,450 --> 01:51:49,390
matrix or i mean rectangular matrix with more roles and more columns and rows if

1668
01:51:49,390 --> 01:51:51,620
you do now the QR factorizations aunts

1669
01:51:53,300 --> 01:51:56,790
you get an upper triangular matrix p by p

1670
01:51:57,810 --> 01:51:59,290
from from the

1671
01:52:00,530 --> 01:52:03,370
we transform a

1672
01:52:03,400 --> 01:52:08,850
the same killer was applied to a so we have a one and a two

1673
01:52:08,870 --> 01:52:15,020
and we transform x by the same transformation q only this time from the left

1674
01:52:15,020 --> 01:52:18,350
and lo and behold well will do this

1675
01:52:18,380 --> 01:52:21,640
transformation being minus a one y

1676
01:52:21,650 --> 01:52:24,390
mine is a to see what we've done is just

1677
01:52:24,400 --> 01:52:29,150
stuck in q transposing killing two in between a and x

1678
01:52:29,160 --> 01:52:33,840
alright minimizes so that are transpose y equals zero

1679
01:52:33,850 --> 01:52:35,260
in other words we have

1680
01:52:35,510 --> 01:52:38,200
b-minus a to z equals the minimum

1681
01:52:38,250 --> 01:52:41,840
and then you can use any method you want to sign this least squares problem

1682
01:52:41,880 --> 01:52:44,680
and here this gives the solution to the

1683
01:52:44,800 --> 01:52:47,720
constrained least squares problem

1684
01:52:47,730 --> 01:52:53,120
if you can and if the data structures from that there is a good idea

1685
01:52:53,120 --> 01:52:57,260
to eliminate the constraints that makes the problem better conditions

1686
01:52:57,280 --> 01:53:00,930
you can prove easily in this matrix a two

1687
01:53:00,950 --> 01:53:06,010
it is at least as well conditioned as the original problem and often

1688
01:53:06,020 --> 01:53:10,820
the original problem may be singular imposing the constraints may be make it non singular

1689
01:53:10,820 --> 01:53:15,020
so if you can use the constraints it's a good idea

1690
01:53:15,040 --> 01:53:20,140
now what about quadratic constraints

1691
01:53:20,160 --> 01:53:21,870
and that comes up very often

1692
01:53:21,880 --> 01:53:24,040
there lots of different applications

1693
01:53:24,050 --> 01:53:26,910
i mean does anybody hear ever see that kind of problem i know you see

1694
01:53:26,910 --> 01:53:29,000
regularisation problems

1695
01:53:29,000 --> 01:53:35,610
has anybody seen quadratic constraints OK thank so here's the quadratic constraint again we use

1696
01:53:35,610 --> 01:53:37,620
the grounds multipliers

1697
01:53:37,650 --> 01:53:39,010
and you see

1698
01:53:39,070 --> 01:53:40,770
you get the system

1699
01:53:40,790 --> 01:53:46,900
which statisticians sometimes called ridge regression this problem here only now

1700
01:53:46,930 --> 01:53:51,370
the mu is determined from this constraint on the length of the solution the length

1701
01:53:51,370 --> 01:53:53,800
of the solution must be equal to the square

1702
01:53:53,810 --> 01:53:58,910
if you substitute in what x is equal into this equation to here you get

1703
01:53:58,910 --> 01:54:01,390
what i call the secular equation

1704
01:54:01,470 --> 01:54:05,460
so it's a new is this unknown quantity here

1705
01:54:05,820 --> 01:54:11,300
it's actually the hardest part of this holy squares problem is finding new correctly

1706
01:54:12,300 --> 01:54:17,980
you don't need to know muta fifteen significant places was and south square

1707
01:54:18,010 --> 01:54:18,630
could be

1708
01:54:18,650 --> 01:54:23,660
one point one or one point o one or o point nine six

1709
01:54:23,680 --> 01:54:25,660
he really as we say in america

1710
01:54:26,330 --> 01:54:29,450
we just want a ballpark estimate of mu

1711
01:54:29,460 --> 01:54:34,650
if you use that expression at all ballpark breast cancer ballpark estimate that we need

1712
01:54:34,930 --> 01:54:38,540
so but this was not always in these equations

1713
01:54:38,560 --> 01:54:42,160
the south

1714
01:54:42,240 --> 01:54:50,360
if you use the singular value decomposition then you can die analyse your matrix a

1715
01:54:50,400 --> 01:54:52,760
and you have this equation to some

1716
01:54:52,780 --> 01:54:55,780
there are at least two there are two n

1717
01:54:55,800 --> 01:55:00,830
possible solutions to this but there's a particular one that we need to solve for

1718
01:55:00,870 --> 01:55:06,040
and then once you sign up for the new year on the way

1719
01:55:06,070 --> 01:55:08,570
another idea is the following

1720
01:55:08,580 --> 01:55:10,810
you have this problem here

1721
01:55:10,830 --> 01:55:15,640
i'm just rewriting this as a new matrix problem and if you eliminate

1722
01:55:15,650 --> 01:55:16,540
you know

1723
01:55:16,550 --> 01:55:18,480
you get the equation i wrote about

1724
01:55:18,490 --> 01:55:20,210
but if you eliminate c

1725
01:55:20,220 --> 01:55:21,940
you get this equation here

1726
01:55:21,950 --> 01:55:25,510
so this equation this is the square of the matrix

1727
01:55:25,560 --> 01:55:28,250
a transfer is a plus mu i square

1728
01:55:28,270 --> 01:55:31,860
minus one over alpha square and the matrix of rank one

1729
01:55:31,880 --> 01:55:37,080
and what you need to do is find a particular value of this quadratic i

1730
01:55:37,190 --> 01:55:39,100
value problems so

1731
01:55:41,080 --> 01:55:44,240
the lagrange multiplier is equivalent

1732
01:55:44,250 --> 01:55:47,830
to solving a certain quadratic i can be a problem if you can solve the

1733
01:55:47,840 --> 01:55:51,800
quadratic i can be a problem then you have the lagrange multiplier and then you

1734
01:55:51,800 --> 01:55:56,500
can sort of once you have a large multiply that is easy enough to to

1735
01:55:56,500 --> 01:55:58,040
solve for the solution

1736
01:55:58,090 --> 01:56:03,500
so the parameter mu is generally positive but not always

1737
01:56:03,500 --> 01:56:07,050
so this is something that's been of interest to me

1738
01:56:07,070 --> 01:56:10,930
and let me just mention how this can be done

1739
01:56:10,930 --> 01:56:14,470
so we need to solve this equation here

1740
01:56:14,730 --> 01:56:17,840
and earlier using the singular value decomposition

1741
01:56:17,860 --> 01:56:21,110
i wrote that in this fashion here

1742
01:56:21,120 --> 01:56:27,460
that is after i diagonalizes a and of course if you have a some

1743
01:56:27,490 --> 01:56:32,500
of non negative quantities you can think of this as the still choose integral

1744
01:56:33,700 --> 01:56:35,830
so computing f of mu

1745
01:56:35,850 --> 01:56:39,570
this matrix equation here

1746
01:56:39,680 --> 01:56:46,040
which is really a quadratic equation quadratic form is equivalent to computing integrals

1747
01:56:46,060 --> 01:56:51,990
can we don't know what this distribution function is because that depends upon the singular

1748
01:56:51,990 --> 01:56:53,200
vectors of a

1749
01:56:53,210 --> 01:56:55,270
for a

1750
01:56:55,320 --> 01:56:59,800
any time you see a quadratic forms think in terms of an integral

1751
01:57:00,040 --> 01:57:04,190
so this integral may be officially bounded by applying what is called the land shows

1752
01:57:05,270 --> 01:57:07,930
and then there's

1753
01:57:07,950 --> 01:57:13,090
the gauss rather quadrature rule i won't go into all the details but the interesting

1754
01:57:13,090 --> 01:57:14,260
thing is you can

1755
01:57:16,210 --> 01:57:21,840
making approximation to this integral by using was called ln shows scheme and it has

1756
01:57:21,840 --> 01:57:26,950
a certain side benefit not only do you get a good estimate of this parameter

1757
01:57:27,750 --> 01:57:34,390
but in the use of the land shows algorithm you construct and not vectors

1758
01:57:34,410 --> 01:57:39,590
so that you can actually that approximate the solution so like getting a free lunch

1759
01:57:39,720 --> 01:57:44,390
you get not only the regularisation parameter out of the whole process but you also

1760
01:57:44,390 --> 01:57:50,890
get an approximation to the solution and there is an algorithm that's very effective for

1761
01:57:50,890 --> 01:57:53,540
solving sparse least squares problems

1762
01:57:53,590 --> 01:57:57,590
it's based in part on this line shows the algorithm

1763
01:57:57,630 --> 01:58:01,070
and it it was it's called LSQR

1764
01:58:01,090 --> 01:58:03,060
so method devised by

1765
01:58:03,190 --> 01:58:08,510
chris page and mike saunders is freely available so if you have sparse least squares

