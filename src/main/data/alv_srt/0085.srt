1
00:00:00,000 --> 00:00:02,330
and the point is that

2
00:00:02,380 --> 00:00:09,090
throughout this subject of optimisation

3
00:00:10,360 --> 00:00:16,740
20 problem that uses the same input ABC and and what looks a completely different

4
00:00:18,100 --> 00:00:23,880
and yet there to work closely related it's it's this is 1 of the more

5
00:00:23,880 --> 00:00:27,980
subtle aspects of

6
00:00:28,010 --> 00:00:33,620
linear programming and all of optimization is this presence of dual variables

7
00:00:33,680 --> 00:00:36,210
well actually

8
00:00:36,390 --> 00:00:41,970
that theme of variables x is unwise

9
00:00:42,000 --> 00:00:44,630
has been fundamental in this whole

10
00:00:44,650 --> 00:00:46,890
right from the start of these lectures

11
00:00:47,500 --> 00:00:54,660
we are are you member in our network flow problems we had potential and currents

12
00:00:54,660 --> 00:00:59,500
W and there was a certain duality between those

13
00:00:59,520 --> 00:01:05,520
and in fact we had M O N and potential than M currents

14
00:01:05,590 --> 00:01:11,470
and nodes and m edges so actually graphs there would nodes and edges were going

15
00:01:11,480 --> 00:01:13,640
to be the central limit theorem

16
00:01:13,880 --> 00:01:15,320
2nd lecture

17
00:01:16,180 --> 00:01:22,820
so all we want not totally is not only news to us that there that

18
00:01:22,820 --> 00:01:23,860
there's some

19
00:01:24,160 --> 00:01:31,340
reverse problem which turns out to be a maximization also linear problems of course and

20
00:01:31,350 --> 00:01:34,350
now it'll be the be transposed Y

21
00:01:35,660 --> 00:01:39,640
so the the that within the constraint now popping into the cost

22
00:01:40,770 --> 00:01:43,060
if I call this cost

23
00:01:43,480 --> 00:01:45,680
or whatever income maybe

24
00:01:46,640 --> 00:01:53,340
and what the constraints on this uh well not to everybody's surprise A transpose is

25
00:01:53,340 --> 00:01:54,160
going show

26
00:01:54,640 --> 00:01:57,440
and this vector

27
00:01:57,450 --> 00:02:01,290
C is going to throw up on the right hand side of the age

28
00:02:01,320 --> 00:02:03,440
transpose what

29
00:02:03,470 --> 00:02:05,580
equals state

30
00:02:07,140 --> 00:02:11,020
OK have I got that rated where want greater than equal and that I want

31
00:02:11,020 --> 00:02:11,880
greater than equal

32
00:02:13,540 --> 00:02:15,080
yeah if you're right that that

33
00:02:15,290 --> 00:02:19,350
yeah at about it is checked do I want less or equal or greater equal

34
00:02:19,620 --> 00:02:20,730
I want last

35
00:02:23,040 --> 00:02:26,120
it's not surprising that

36
00:02:26,380 --> 00:02:31,020
we have a greater than or equal than we in the primal problem well let's

37
00:02:31,020 --> 00:02:34,710
report shows up here but the main point is a transpose shows up again

38
00:02:35,200 --> 00:02:38,280
so we really uh I could

39
00:02:38,290 --> 00:02:40,270
stretch our little framework

40
00:02:41,200 --> 00:02:49,100
quite ways a little our framework of 3 steps to include linear programming shall I

41
00:02:49,100 --> 00:02:52,940
do that uh men

42
00:02:53,020 --> 00:02:59,210
it was make some comments about this and do that yeah yeah

43
00:03:00,500 --> 00:03:03,780
well at the moment user to

44
00:03:03,790 --> 00:03:04,830
independent problem

45
00:03:05,620 --> 00:03:11,060
that I mean I I could hide the dual problem never tell never tell you

46
00:03:11,060 --> 00:03:14,860
about it and just discussed the primal how solve

47
00:03:16,590 --> 00:03:19,090
and revealing that there's a 2nd problem

48
00:03:19,640 --> 00:03:25,460
that which if I had 5 lectures on program naturally probably wouldn't reveals so rarely

49
00:03:25,510 --> 00:03:32,180
that there is a 2nd problem hiding around with the same inputs and also another

50
00:03:32,180 --> 00:03:38,740
linear programming this is evidence that the linear programming problem to linear programming is linear

51
00:03:38,740 --> 00:03:41,200
cost linear

52
00:03:41,210 --> 00:03:49,140
equality and inequality constraints however that I can play you can massage 1 problem to

53
00:03:49,140 --> 00:03:53,540
have sit another 1 I could do I could do that I could have a

54
00:03:53,540 --> 00:03:59,450
x lesser rate will be there and play with it and and introduce some slack

55
00:03:59,450 --> 00:04:04,850
2 unknowns to make up the equality sign and they would have to be nonnegative

56
00:04:04,850 --> 00:04:10,160
and input impediment to the axes and all I want to say is that

57
00:04:10,300 --> 00:04:17,130
1st this fits the general form but others that don't look like they're quite fit

58
00:04:18,420 --> 00:04:21,420
as long as they have linear

59
00:04:21,450 --> 00:04:25,180
costs and linear

60
00:04:26,610 --> 00:04:35,370
and inequality is no no conditional on y yeah it's it's it's interesting

61
00:04:35,780 --> 00:04:42,210
In a that's why you girlfriend correct I have to think every time OK it

62
00:04:42,220 --> 00:04:47,920
is a condition that they let me tell you about duality so duality

63
00:04:48,440 --> 00:04:53,560
some really jumping dear to the heart of the

64
00:04:53,580 --> 00:05:00,800
of of the underlying says that the maximum work in the dual problem equals the

65
00:05:00,800 --> 00:05:03,710
minimum in the prime

66
00:05:03,780 --> 00:05:10,090
that the answer to this problem that the maximum whatever is the man is equal

67
00:05:10,090 --> 00:05:12,160
to the minimum cost

68
00:05:12,200 --> 00:05:14,130
so that

69
00:05:14,160 --> 00:05:21,340
in a way this is a big help because it tells us when to stop

70
00:05:21,370 --> 00:05:26,300
if we have a suppose we have an algorithm that gradually finds a better and

71
00:05:26,300 --> 00:05:27,520
better exits

72
00:05:27,540 --> 00:05:30,210
this is what you're really doing reality

73
00:05:30,830 --> 00:05:35,700
you get an approximate X and you say that the best I can do you

74
00:05:35,700 --> 00:05:40,940
look around it as calculus always does you say it up I can go off

75
00:05:41,400 --> 00:05:45,280
I can change that x 2 is doing to reduce the cost

76
00:05:46,440 --> 00:05:53,060
and so you go that way and then you don't know when the stop well

77
00:05:53,110 --> 00:05:57,990
there are several ways to know when to stop what they have when you get

78
00:05:57,990 --> 00:06:01,230
to a point x star let's say

79
00:06:01,230 --> 00:06:03,920
serving galaxies in trying to trace

80
00:06:03,940 --> 00:06:05,920
the rotation curve

81
00:06:05,940 --> 00:06:07,650
and his conclusion

82
00:06:07,660 --> 00:06:12,010
he's in her own words in spiral galaxy the ratio of dark to light market

83
00:06:12,010 --> 00:06:13,750
is about a factor of ten

84
00:06:13,800 --> 00:06:19,260
this probably have good number for the ratio of our ignorance to knowledge

85
00:06:19,340 --> 00:06:23,910
OK so the master iteration for galaxies of all the

86
00:06:23,930 --> 00:06:26,540
OK let's keep on going

87
00:06:26,590 --> 00:06:28,340
so we know that galaxies

88
00:06:28,840 --> 00:06:31,040
don't leave isolated

89
00:06:31,050 --> 00:06:34,400
there are a lot of galaxy clusters out there and we've seen some picture of

90
00:06:34,410 --> 00:06:40,130
them so for a group of objects that interact gravitational e there must always be

91
00:06:40,130 --> 00:06:46,190
a balance between gravity and velocity gravity and kinetic energy to little velocity and then

92
00:06:46,200 --> 00:06:51,310
gravity takes over and then the clusters should collapse too much velocity and the objects

93
00:06:51,310 --> 00:06:56,660
should escape and the clusters should not just not explode and everything fly away and

94
00:06:56,660 --> 00:07:00,950
evaporate the fact that clusters of there they are not operating

95
00:07:00,960 --> 00:07:04,160
must be telling you something that we tell you know there is a balance between

96
00:07:05,350 --> 00:07:07,860
potential energy and kinetic energy

97
00:07:07,880 --> 00:07:11,980
so by measuring the doppler shift of galaxies in clusters you can measure the the

98
00:07:11,980 --> 00:07:13,260
cluster gravity

99
00:07:14,060 --> 00:07:18,030
and from that you can get that last month

100
00:07:18,040 --> 00:07:21,380
and so to do that you write down the video DRM you say that the

101
00:07:21,380 --> 00:07:28,010
kinetic energy is equal minus one half potential energy and these are four here it's

102
00:07:28,010 --> 00:07:31,210
due to the fact that you don't really know exactly what the radius of a

103
00:07:31,210 --> 00:07:35,600
cluster because the radius of a cluster is not aspect the sphere we are you

104
00:07:35,600 --> 00:07:40,920
with the straight edge so depending on how you define the edge then these are

105
00:07:40,930 --> 00:07:45,230
for parameter will take value may be slightly different from unity but here we're talking

106
00:07:45,230 --> 00:07:49,650
orders of magnitude so let's not worry about this speed there's just yet so let's

107
00:07:49,650 --> 00:07:55,150
consider the common class you can measure the dispersion velocity along the line of sight

108
00:07:55,610 --> 00:07:59,610
and you know that the dispersion velocity is actually that is not just the one

109
00:07:59,610 --> 00:08:02,900
along the line of sight it's three times

110
00:08:03,230 --> 00:08:09,480
that of the line of sight this half-light radius of the cluster is about one

111
00:08:09,480 --> 00:08:13,460
point five million pounds six black the number in

112
00:08:13,500 --> 00:08:17,030
and you end up with some us that is two

113
00:08:17,090 --> 00:08:20,550
time ten to fifteen solar masses

114
00:08:20,590 --> 00:08:25,650
remember solar masses two times ten to the thirty kilograms

115
00:08:25,670 --> 00:08:27,820
that amongst

116
00:08:27,840 --> 00:08:31,130
and much more stuff there to meet the

117
00:08:32,070 --> 00:08:38,030
four clusters must light ratio is from hundreds to three hundred

118
00:08:38,050 --> 00:08:41,710
there is a lot more there that china

119
00:08:41,730 --> 00:08:48,770
and so i couldn't resist of put in the picture of swiss astronomy and squeaky

120
00:08:48,900 --> 00:08:53,440
because we can not is that in nineteen thirty three the galaxies in cluster move

121
00:08:53,440 --> 00:08:56,550
much too fast for the amount of money that we see

122
00:08:56,570 --> 00:09:02,780
so the cluster must contain a lot of unseen matter that provide sex

123
00:09:02,800 --> 00:09:10,800
and more recently we have some other indication that come from the extreme case the

124
00:09:10,800 --> 00:09:14,250
exterior is simply due to the fact that there is a lot of gas in

125
00:09:14,250 --> 00:09:16,800
galaxy clusters that gets completed

126
00:09:16,820 --> 00:09:21,780
and that's another way to actually measure the kinetic energy of the stuff that is

127
00:09:21,780 --> 00:09:26,890
contained in the clusters and if you repeat the calculation by looking at this is

128
00:09:26,890 --> 00:09:31,380
simply the kinetic energy to get from the temperature they come from lakes strays again

129
00:09:31,380 --> 00:09:35,090
you get the same mass to light ratio that depending from cluster to cluster is

130
00:09:35,090 --> 00:09:37,510
of the order of hundreds of your

131
00:09:37,530 --> 00:09:43,000
another indication

132
00:09:43,050 --> 00:09:46,840
i explained all this the mast bend space-time

133
00:09:46,860 --> 00:09:49,750
and now i need to make a distinction here

134
00:09:49,800 --> 00:09:53,960
global geometry is different from local geometry OK

135
00:09:53,980 --> 00:09:55,800
so he may well be

136
00:09:55,820 --> 00:10:00,370
that the universe on average in the very large scale c well described by a

137
00:10:00,370 --> 00:10:05,630
flat geometry but if you're going to see the nearer block all arise the geometry

138
00:10:05,630 --> 00:10:08,280
that is not going to be flat

139
00:10:09,340 --> 00:10:10,300
and so

140
00:10:10,320 --> 00:10:12,940
if you sit very close to

141
00:10:12,960 --> 00:10:19,270
concentration of mass the local geometry is not going to be flat

142
00:10:19,300 --> 00:10:25,880
but this general relativity describe as gravity warps the spacetime around a massive object the

143
00:10:25,880 --> 00:10:29,610
stronger the gravity the more space times what

144
00:10:29,610 --> 00:10:30,770
and we know

145
00:10:30,800 --> 00:10:33,750
that light follows a geodesic

146
00:10:33,800 --> 00:10:37,980
and i geodesic in space that is not blocked

147
00:10:38,030 --> 00:10:40,800
it's not straight

148
00:10:40,820 --> 00:10:45,130
and so you can imagine that a lot of strange thing can happen if

149
00:10:45,170 --> 00:10:48,590
you have the light rays if you're sitting here and you have light rays going

150
00:10:48,590 --> 00:10:49,550
to that

151
00:10:49,650 --> 00:10:51,090
what is

152
00:10:51,130 --> 00:10:55,300
and so this is what happens so imagine you have an intervening galaxy or a

153
00:10:55,300 --> 00:11:00,550
galaxy cluster you're are hero serving this is the distortion of the space time and

154
00:11:00,550 --> 00:11:05,130
you have a distant galaxy here the light from the galaxy come follow different kind

155
00:11:05,130 --> 00:11:08,920
of part and then when you look at it from their your brain or your

156
00:11:08,920 --> 00:11:13,300
telescope says well you know about the light rays should go straight and so what

157
00:11:13,300 --> 00:11:13,670
you see

158
00:11:14,070 --> 00:11:17,730
you see for example two different

159
00:11:17,780 --> 00:11:24,820
position of the same background objects this is an extreme case if the geometry is

160
00:11:24,820 --> 00:11:29,290
not that great to get two different position what you end up seeing is simply

161
00:11:29,290 --> 00:11:34,000
a distortions so the same galaxy veins stretched and distorted

162
00:11:35,630 --> 00:11:39,190
this kind of effect is called gravitational lensing

163
00:11:39,210 --> 00:11:41,400
and it's an extremely powerful tool

164
00:11:41,400 --> 00:11:45,920
two men should not just the mass of the intervening objects a galaxy of plaster

165
00:11:46,130 --> 00:11:51,230
but is a powerful tool to measure the intervening mass on cosmological scales

166
00:11:51,280 --> 00:11:55,980
that is the stuff that is even between clusters of galaxies that galaxies clusters of

167
00:11:58,650 --> 00:12:04,330
so this is an animation of what will happen if you have background galaxies and

168
00:12:04,330 --> 00:12:08,050
in for foreground over density of matter

169
00:12:08,070 --> 00:12:12,880
and this is an in the foreground of the density of matter moves

170
00:12:12,880 --> 00:12:16,590
this is gravitational lensing play

171
00:12:16,610 --> 00:12:18,770
and this is an actual image

172
00:12:18,780 --> 00:12:22,480
mutation you at the very beginning and i told you know these things are not

173
00:12:22,480 --> 00:12:30,250
artifacts of the hubble thinking about picture this and background images that have been distorted

174
00:12:30,550 --> 00:12:36,280
and then they replicated by the sheer amount of money that is in this clusters

175
00:12:36,280 --> 00:12:38,110
of four

176
00:12:40,050 --> 00:12:47,900
OK so very briefly the mathematical gravitational lensing in case you are interested

177
00:12:47,920 --> 00:12:50,820
so the deflection angle which is

178
00:12:50,840 --> 00:12:56,130
the object should be there but it appears he is related to gravity

179
00:12:56,170 --> 00:12:59,750
the amount of mass that is in the intervening lens

180
00:12:59,930 --> 00:13:02,980
the impact parameter is your geometry

181
00:13:03,000 --> 00:13:05,210
and the

182
00:13:05,210 --> 00:13:08,770
i'm going to see from here and then is also related to the geometry that

183
00:13:08,770 --> 00:13:12,730
is the distance from you to the source and the distance from you to the

184
00:13:12,730 --> 00:13:15,960
length and the distance from the lens to the source

185
00:13:15,980 --> 00:13:21,550
basically if you write down the equation you end up with something like this this

186
00:13:21,550 --> 00:13:26,550
angle you can measure and in this angle you have the mass of the intervening

187
00:13:26,550 --> 00:13:29,470
what he every time

188
00:13:32,570 --> 00:13:38,070
one of the u we want to

189
00:13:40,550 --> 00:13:51,180
so all all in all the training and supply serious focused all

190
00:13:51,200 --> 00:13:53,620
although he rule

191
00:13:53,640 --> 00:13:56,850
and were here on the wings

192
00:13:56,850 --> 00:14:00,490
tell us why is

193
00:14:02,320 --> 00:14:04,070
all right

194
00:14:04,850 --> 00:14:07,950
one is that the

195
00:14:07,970 --> 00:14:10,720
what more

196
00:14:13,890 --> 00:14:15,990
so that

197
00:14:32,780 --> 00:14:35,180
here's one

198
00:14:37,700 --> 00:14:39,620
four eight

199
00:14:49,570 --> 00:14:52,410
the probability that all

200
00:15:06,140 --> 00:15:09,410
in addition of the test

201
00:15:16,070 --> 00:15:23,870
that's why

202
00:15:31,760 --> 00:15:36,780
i'm sure

203
00:15:45,350 --> 00:15:48,740
so those who may wish to refer to

204
00:15:49,370 --> 00:15:50,390
what your

205
00:16:11,620 --> 00:16:15,510
it's not for us

206
00:16:15,530 --> 00:16:24,950
lo or loh

207
00:16:24,970 --> 00:16:27,580
or a lot of

208
00:16:35,370 --> 00:16:37,950
he also solutions

209
00:16:42,050 --> 00:16:49,300
what and so forth should try using that you

210
00:16:51,570 --> 00:16:55,800
this is your

211
00:16:56,720 --> 00:17:01,140
the eurovision song expect

212
00:17:07,030 --> 00:17:11,320
changes in the last season you right

213
00:17:12,350 --> 00:17:15,490
heard those things you

214
00:17:16,760 --> 00:17:20,490
so the problem is he has of

215
00:17:25,070 --> 00:17:28,450
o is one

216
00:17:28,470 --> 00:17:33,340
the only one

217
00:17:33,350 --> 00:17:39,450
you seem to be on

218
00:17:39,470 --> 00:17:43,030
it is possible that the

219
00:17:43,850 --> 00:17:49,490
stanford it is that you know

220
00:17:51,010 --> 00:17:52,600
so the question is

221
00:17:56,320 --> 00:17:57,680
so this

222
00:17:57,700 --> 00:18:03,160
the first one to hear a lot more

223
00:18:11,530 --> 00:18:13,050
so should

224
00:18:27,300 --> 00:18:30,950
and we

225
00:18:30,950 --> 00:18:32,490
as an SP

226
00:18:32,510 --> 00:18:36,240
this is nineteen thirteen pound in london

227
00:18:36,250 --> 00:18:38,370
styling himself

228
00:18:38,390 --> 00:18:39,410
isn t

229
00:18:39,580 --> 00:18:49,420
after this renaissance artists and poets whom he would write about translate in this period

230
00:18:49,460 --> 00:18:54,240
it could be in miniature war by a province all

231
00:18:54,260 --> 00:18:55,820
damsel no

232
00:18:57,920 --> 00:19:02,360
a little later pound after the war

233
00:19:02,420 --> 00:19:05,920
in nineteen twenty three

234
00:19:06,240 --> 00:19:15,540
member of the full flower of modernism still young man but got that came and

235
00:19:15,760 --> 00:19:18,240
he's in paris

236
00:19:18,650 --> 00:19:20,300
we're we're

237
00:19:20,350 --> 00:19:21,570
he would

238
00:19:21,620 --> 00:19:25,760
meet elliot and work on the wasteland with them

239
00:19:27,030 --> 00:19:30,310
fast forward twenty years

240
00:19:30,360 --> 00:19:32,180
this is pounds

241
00:19:32,190 --> 00:19:34,910
pound accused of treason

242
00:19:34,930 --> 00:19:37,890
pound accused of treason of his by his country

243
00:19:37,900 --> 00:19:41,230
accused of treason as he tries to bend

244
00:19:41,240 --> 00:19:42,810
the world to his

245
00:19:42,830 --> 00:19:44,260
the vision of it

246
00:19:44,340 --> 00:19:46,270
and he escapes trial

247
00:19:46,300 --> 00:19:47,750
only by

248
00:19:47,750 --> 00:19:50,710
the reason of insanity when he is brought from

249
00:19:51,780 --> 00:19:53,790
under charges of

250
00:19:53,800 --> 00:20:00,090
having made broadcast on fascist radio back to the united states after an or deal

251
00:20:00,090 --> 00:20:05,270
in a cage in pisa

252
00:20:09,770 --> 00:20:11,410
poses for this photo

253
00:20:11,410 --> 00:20:17,390
it has an intake photo as he enters saint elizabeth's hospital

254
00:20:17,400 --> 00:20:21,590
for the insane in washington

255
00:20:21,620 --> 00:20:23,970
in this final photo

256
00:20:24,010 --> 00:20:28,030
photo from nineteen seventy one back in italy in pollen

257
00:20:28,040 --> 00:20:35,620
well here's pounds in presenting us with an image of something that would have seemed

258
00:20:35,620 --> 00:20:43,570
impossible when he began which is an image of modernism grown old old and last

259
00:20:45,340 --> 00:20:47,440
many sensors

260
00:20:47,450 --> 00:20:52,760
contrast this this career encapsulated in those images

261
00:20:54,810 --> 00:20:55,760
this one

262
00:20:55,800 --> 00:20:59,520
who is this this is the author of proof rock in fact this is the

263
00:20:59,520 --> 00:21:07,250
harvard student who wrote proprietary elliot wrote proof rock largely when still at harvard and

264
00:21:07,340 --> 00:21:11,050
in the years immediately following

265
00:21:12,120 --> 00:21:14,670
a little maybe

266
00:21:14,670 --> 00:21:18,270
the whole slightly parted lips

267
00:21:18,300 --> 00:21:21,680
windswept hair the

268
00:21:21,680 --> 00:21:24,230
technology a crew that

269
00:21:24,250 --> 00:21:29,160
now notice the handkerchief

270
00:21:32,190 --> 00:21:39,010
here is the editor of gay editor of the publisher faber and faber

271
00:21:40,430 --> 00:21:48,100
thirty years later or more surrounded by books the cultural arbiter of the english speaking

272
00:21:49,500 --> 00:21:51,720
he only sixty

273
00:21:52,090 --> 00:21:54,520
that here is now slicked down

274
00:21:54,540 --> 00:21:57,530
their glasses between him and us

275
00:21:58,800 --> 00:22:03,690
this is the young man has become a monument but really the customers the same

276
00:22:03,690 --> 00:22:04,760
one right

277
00:22:07,260 --> 00:22:13,760
pounds descent into infamy and sanity and dignity

278
00:22:13,760 --> 00:22:20,300
and it's rise to the extraordinary cultural power and prestige

279
00:22:20,620 --> 00:22:24,830
that he occupied that is represented by this and many other photos well these are

280
00:22:24,860 --> 00:22:30,150
key stories in modern poetry in their interestingly interlocking just as there are two lives

281
00:22:34,600 --> 00:22:36,090
another modern poet

282
00:22:36,100 --> 00:22:37,000
this is

283
00:22:37,000 --> 00:22:40,500
this is an old woman called mary and more

284
00:22:40,510 --> 00:22:43,040
who became

285
00:22:43,050 --> 00:22:51,480
a kind of civic icon who became a celebrity even as an eccentric new yorkers

286
00:22:51,480 --> 00:22:56,430
who walked right corner hats and went to baseball games in the zoo

287
00:22:56,430 --> 00:23:01,250
excess risk and excess fire

288
00:23:01,300 --> 00:23:06,470
so what i mean by that well we don't look at the risk of function

289
00:23:06,780 --> 00:23:10,460
and related to the fire risk we look at how much the risk exceeds the

290
00:23:10,460 --> 00:23:11,490
bayes risk

291
00:23:11,510 --> 00:23:14,820
right so how much worse f is than the optimum

292
00:23:14,830 --> 00:23:19,190
in terms of the risk and in terms of the fire escape

293
00:23:21,320 --> 00:23:25,760
so for any probability distribution in any f this function psi

294
00:23:25,790 --> 00:23:29,680
gives us an upper bound on the relationship between the access virus can the excess

295
00:23:31,510 --> 00:23:33,510
right we saw in this case

296
00:23:33,520 --> 00:23:37,000
so i was just an increasing in quadratic was actually

297
00:23:37,020 --> 00:23:40,450
sia theta was the squared

298
00:23:40,460 --> 00:23:41,920
right so

299
00:23:41,950 --> 00:23:44,920
you know we can invert that we get the in

300
00:23:44,930 --> 00:23:49,140
in that case on the previous slide the excess risk is no more than the

301
00:23:49,140 --> 00:23:51,630
square root of the access virus

302
00:23:51,650 --> 00:23:54,220
right for that example

303
00:23:54,230 --> 00:23:56,640
and that's that's true in general for

304
00:23:56,700 --> 00:24:01,480
for any any phi we can we can come up with this corresponding function psi

305
00:24:01,480 --> 00:24:06,040
and it gives this relationship between the excess risk and excess

306
00:24:06,090 --> 00:24:09,800
virus turns out that the optimal

307
00:24:09,830 --> 00:24:14,500
found that you can't do any better than that given some value of the argument

308
00:24:15,170 --> 00:24:17,090
the excess risk

309
00:24:17,850 --> 00:24:23,770
you can always find the probability distribution function so that the the probability distribution

310
00:24:23,790 --> 00:24:26,910
and that function the excess risk is equal to

311
00:24:28,060 --> 00:24:29,930
right so this inequality

312
00:24:29,960 --> 00:24:31,960
is this inequality

313
00:24:31,980 --> 00:24:35,020
right and there's a matching inequality on the other side

314
00:24:35,130 --> 00:24:38,150
as close as you like to matching on the other side

315
00:24:38,160 --> 00:24:41,030
right so so you can have

316
00:24:41,080 --> 00:24:45,590
any better inequality you can have an inequality that any better than any

317
00:24:45,610 --> 00:24:47,620
any value this one

318
00:24:47,840 --> 00:24:51,750
if you want to hold uniformly over distributions in a function

319
00:24:51,770 --> 00:24:57,120
OK so this is the right the right relating excess risk and excess fires

320
00:24:57,470 --> 00:24:59,280
this fall

321
00:24:59,600 --> 00:25:01,340
and thirdly

322
00:25:01,720 --> 00:25:06,520
when we ask the question when the minimisation of phi risk correspond to minimisation of

323
00:25:07,970 --> 00:25:09,000
right so

324
00:25:11,700 --> 00:25:14,140
that's equivalent to saying well when does this

325
00:25:14,150 --> 00:25:17,240
when is this upper bound

326
00:25:17,250 --> 00:25:18,890
non vacuous

327
00:25:18,900 --> 00:25:23,130
right when when does driving the right-hand side is zero

328
00:25:23,140 --> 00:25:24,550
you imply that

329
00:25:24,560 --> 00:25:26,470
the excess risk

330
00:25:26,480 --> 00:25:28,440
goes to zero

331
00:25:28,470 --> 00:25:32,020
right well we can we can precisely characterize

332
00:25:32,060 --> 00:25:34,300
that kind of implication

333
00:25:34,310 --> 00:25:37,470
OK it depends just on this

334
00:25:37,480 --> 00:25:44,170
weakest possible condition the obviously necessary condition that the function phi is classification calibrated

335
00:25:44,260 --> 00:25:47,010
OK so if this function

336
00:25:47,100 --> 00:25:52,170
satisfies this condition that h minus is strictly worse than h

337
00:25:52,190 --> 00:25:53,490
that's equivalent to

338
00:25:56,080 --> 00:26:01,200
the value of sin go to zero implies that the argument goes to zero

339
00:26:01,540 --> 00:26:05,450
is equivalent to the argument goes to zero

340
00:26:05,470 --> 00:26:10,490
right but the case for interested in nontrivial cases the full implication here

341
00:26:10,530 --> 00:26:15,510
right and that of course and that is equivalent to if we can find the

342
00:26:15,910 --> 00:26:18,040
following implication if we can find

343
00:26:18,080 --> 00:26:22,640
a sequence of functions for which the fire risk approaches the optimum

344
00:26:22,690 --> 00:26:24,240
then for that sequence

345
00:26:24,260 --> 00:26:29,250
the risk approaches the optimum right so this inequality is not vacuous in the sense

346
00:26:29,250 --> 00:26:32,540
that when we when we need to send the right-hand side is zero

347
00:26:32,560 --> 00:26:37,630
then the argument on left-hand side also goes to zero

348
00:26:37,650 --> 00:26:39,440
OK so

349
00:26:39,450 --> 00:26:44,360
so this is a fairly complete answer this question when minimisation of virus scanner the

350
00:26:44,360 --> 00:26:45,370
right thing to do

351
00:26:45,420 --> 00:26:51,060
right it's it's precisely when the when the function is classification calibrated

352
00:26:51,070 --> 00:26:55,910
and as i said this theorem is true for arbitrary bounded from below

353
00:26:56,810 --> 00:26:59,050
with the with the slide

354
00:26:59,090 --> 00:27:01,870
change the definition of a sign

355
00:27:01,890 --> 00:27:07,800
everything here is is true turns out in the in the case of convex phi

356
00:27:07,820 --> 00:27:13,120
it's so there's there's also an easy conditions to assess whether our cost function is

357
00:27:13,120 --> 00:27:18,140
classification calibrated and we'll see that animal

358
00:27:18,150 --> 00:27:21,780
OK so

359
00:27:21,790 --> 00:27:24,200
one other observation if we have this

360
00:27:24,220 --> 00:27:26,870
classification calibrated

361
00:27:26,880 --> 00:27:28,430
condition applying

362
00:27:28,440 --> 00:27:29,820
right that is

363
00:27:29,940 --> 00:27:34,110
sending this thing to zero implies that thing

364
00:27:34,130 --> 00:27:39,700
the argument of psychosis that tells us because this function size convex

365
00:27:39,730 --> 00:27:43,800
it's always connected to constant minus the concave function

366
00:27:43,810 --> 00:27:45,420
right then

367
00:27:45,430 --> 00:27:47,500
that means that psi

368
00:27:47,540 --> 00:27:52,190
is strictly increasing and so it has an inverse

369
00:27:52,200 --> 00:27:55,610
right and so we can rewrite that expression in terms of you know whatever the

370
00:27:55,610 --> 00:28:00,180
functional in this is that the excess risk is no more than some function

371
00:28:00,190 --> 00:28:01,500
the axis

372
00:28:01,510 --> 00:28:04,010
prior risk

373
00:28:04,020 --> 00:28:11,320
i which is some some strictly increasing bandwidth which is more natural form ideas right

374
00:28:11,320 --> 00:28:14,940
this is the thing that our algorithms are working to to make small and this

375
00:28:14,940 --> 00:28:16,710
is the thing that we'd like to

376
00:28:16,720 --> 00:28:18,380
to to have small

377
00:28:18,440 --> 00:28:23,940
OK this is the

378
00:28:23,960 --> 00:28:26,740
the condition i mentioned

379
00:28:27,750 --> 00:28:31,940
i alluded to earlier so in the case of the convex

380
00:28:33,090 --> 00:28:36,730
the classification calibration condition is particularly easy to cheque

381
00:28:36,750 --> 00:28:39,350
right turns at all that you that you care about

382
00:28:39,370 --> 00:28:41,050
is what happens is zero

383
00:28:41,390 --> 00:28:44,030
the function just needs to be differentiable zero

384
00:28:44,080 --> 00:28:45,870
and having negative gradient

385
00:28:48,150 --> 00:28:53,750
and that's equivalent to the classification calibration condition for convex functions

386
00:28:55,010 --> 00:28:57,770
you know all those functions that we saw earlier

387
00:28:57,790 --> 00:29:00,640
right satisfy that that condition

388
00:29:00,650 --> 00:29:04,910
OK and zero all differentiable and decreasing

389
00:29:04,920 --> 00:29:09,730
i have a negative gradient zero so the negative gradient is a very natural thing

390
00:29:09,730 --> 00:29:12,550
right i mean it might seem strange to have things going up to the right

391
00:29:12,550 --> 00:29:16,550
of zero but that's fine from the point of view of optimizing the

392
00:29:16,560 --> 00:29:20,500
the fire risk leading to a leading to an optimal value of the risk

393
00:29:22,360 --> 00:29:24,110
the differentiability

394
00:29:24,130 --> 00:29:28,040
might seem a little strange at first but it's not too hard to see why

395
00:29:28,040 --> 00:29:29,890
that's essential

396
00:29:29,910 --> 00:29:32,940
OK if you think about the the case of the cost function that is not

397
00:29:32,940 --> 00:29:36,740
differentiable at zero a convex cost function is not differentiable at zero

398
00:29:36,750 --> 00:29:40,270
right then this some sort of point zero

399
00:29:40,290 --> 00:29:42,700
OK so if you take a mixture

400
00:29:42,710 --> 00:29:45,300
of the cost function with the point is zero

401
00:29:45,310 --> 00:29:48,210
right and the cost function flipped

402
00:29:48,240 --> 00:29:52,770
then you're going to and take fifty fifty mixture then the minimum is going to

403
00:29:52,770 --> 00:29:56,230
occur zero but is going to have a point

404
00:29:57,050 --> 00:29:58,530
and so

405
00:29:58,610 --> 00:30:02,280
if you if you take something a little bit bigger than fifty percent

406
00:30:02,290 --> 00:30:05,220
it should be classifying that's a plus one but it's still going to have a

407
00:30:05,220 --> 00:30:06,590
point zero

408
00:30:06,610 --> 00:30:09,640
right a little bit less than fifty percent should be classifiers minus one is still

409
00:30:09,640 --> 00:30:14,690
going to have its minimum zero because not able to to correctly classify those cases

410
00:30:14,690 --> 00:30:17,860
that have conditional probability nearer half

411
00:30:17,870 --> 00:30:21,460
if you don't have different abilities zero

412
00:30:21,500 --> 00:30:25,800
it turns out you know you can you can kind of exploit this in

413
00:30:25,870 --> 00:30:29,400
in other situations so for instance if you're interested in

414
00:30:29,410 --> 00:30:33,320
in coming up with

415
00:30:33,320 --> 00:30:35,460
i minus p one c

416
00:30:35,490 --> 00:30:36,800
i get this area

417
00:30:36,920 --> 00:30:41,690
and instead of evaluating every time evaluating all those pixels all i need to do

418
00:30:41,690 --> 00:30:46,500
is about compute the integral images of free look values and that's it

419
00:30:46,560 --> 00:30:51,610
so instead of me having the sum is twenty four twenty four and every time

420
00:30:51,620 --> 00:30:53,690
five pre compute the integral image

421
00:30:53,710 --> 00:30:57,110
on the image beforehand but three lookup values

422
00:30:57,140 --> 00:30:59,560
that's it that's all have to do

423
00:30:59,670 --> 00:31:05,840
and there's some

424
00:31:05,890 --> 00:31:07,760
thank you something attention

425
00:31:10,410 --> 00:31:14,380
so essentially this is a list of experience

426
00:31:14,400 --> 00:31:15,220
and so

427
00:31:15,560 --> 00:31:19,770
so a three box filter takes only lookups

428
00:31:19,780 --> 00:31:20,570
and so

429
00:31:20,580 --> 00:31:22,180
they evaluated

430
00:31:22,210 --> 00:31:26,120
and and get very similar performance

431
00:31:26,120 --> 00:31:30,550
two rally rally not work

432
00:31:30,560 --> 00:31:32,530
in fact many instances they get better

433
00:31:33,680 --> 00:31:35,800
the big thing to is that

434
00:31:35,800 --> 00:31:41,280
with the linear classifier it actually would take five hundred seventy six lookups whereas as

435
00:31:41,300 --> 00:31:44,700
we this despite that on the previous slide took eight lookups to do the same

436
00:31:47,440 --> 00:31:52,160
so these examples of the defections and so i guess the big thing here i

437
00:31:52,160 --> 00:31:56,320
just kind of wanted emphasise again is that special detection in vision

438
00:31:56,320 --> 00:32:01,200
it's not just coming up with in the real temptation i think computer science to

439
00:32:01,200 --> 00:32:05,210
sustain theory to say what i can make this work better to make this work

440
00:32:05,300 --> 00:32:09,420
i mean in terms of better saying like in terms of my object detector detection

441
00:32:09,420 --> 00:32:15,650
accuracy is really important i think especially in vision algorithms to think about the computational

442
00:32:15,650 --> 00:32:20,420
complexity at the same time because really want success successful people to in the way

443
00:32:20,420 --> 00:32:24,990
that measure success all over time isn't so much in the paper it's how ubiquitously

444
00:32:25,030 --> 00:32:26,300
method is applied

445
00:32:26,300 --> 00:32:30,090
this technique is kind of the the combination of what kind of well known machine

446
00:32:30,090 --> 00:32:33,000
learning ideas but so applied extremely well

447
00:32:33,440 --> 00:32:37,700
it's essentially changed a lot of what people doing because people don't know anything about

448
00:32:37,700 --> 00:32:41,530
machine learning taking these tools now and applying the price and it's really changed the

449
00:32:41,530 --> 00:32:43,880
landscape of the computer vision work

450
00:32:43,880 --> 00:32:49,000
so this is kind of what i think apologize especially really good lesson four had

451
00:32:49,020 --> 00:32:50,270
kind of two

452
00:32:50,330 --> 00:32:53,910
good machine learning research that has a lot of practical value

453
00:32:53,950 --> 00:32:59,200
and another thing that it too is i spoke open cv recently they basically

454
00:33:00,010 --> 00:33:04,970
what wasn't them so another one one hundred he basically counted up

455
00:33:05,940 --> 00:33:10,890
detector that was very similar to this input open sea essentially because it so ubiquitously

456
00:33:10,890 --> 00:33:15,080
easy to use and it was applied to well known computer vision framework

457
00:33:15,080 --> 00:33:19,260
everyone's using everyone can is has been around for years now and people still using

458
00:33:19,340 --> 00:33:22,690
things and so there speedups

459
00:33:22,750 --> 00:33:26,070
mister kind are

460
00:33:26,100 --> 00:33:30,370
a colleague of mine in this tournament he came up with the way he can

461
00:33:30,440 --> 00:33:35,320
again heuser all school of graphical models in machine learning things and so got the

462
00:33:35,320 --> 00:33:39,240
idea of again taking this interval image idea

463
00:33:39,260 --> 00:33:45,680
and basically box filters instead of calling boosting he can applied mutual information for techniques

464
00:33:45,770 --> 00:33:48,810
step essentially getting histogram at each value

465
00:33:48,810 --> 00:33:55,370
and trying to so she would not a naive bayes classifier c kind word this

466
00:33:55,400 --> 00:33:56,470
who were which

467
00:33:56,490 --> 00:34:00,780
box values were heavily correlated with each other in terms of mutual information and then

468
00:34:00,820 --> 00:34:02,350
will and which ones were

469
00:34:02,360 --> 00:34:07,340
and essentially if you can assume conditional independence between the more can essentially does we're

470
00:34:07,340 --> 00:34:12,920
not these quite easy to learn histograms of the positive and negative classes and actually

471
00:34:12,920 --> 00:34:16,450
got a lot better performance and is a couple reasons why

472
00:34:16,480 --> 00:34:22,710
henry still have page and why that actually the case i think that's a boosting

473
00:34:22,710 --> 00:34:27,590
essentially especially business that classifier to skip you single threshold whereas like kind of using

474
00:34:27,590 --> 00:34:33,910
histogram can actually maybe about a much nicer or more advanced kind of decision boundary

475
00:34:33,990 --> 00:34:37,780
and i think that all it's a lot more elegant to the mutual information strategy

476
00:34:37,780 --> 00:34:41,240
they use but it's not paper and i encourage people to check it out as

477
00:34:41,240 --> 00:34:43,120
in CVPR two thousand four

478
00:34:43,130 --> 00:34:50,330
and is using mutual information and so and he actually launched a spin-off company called

479
00:34:50,330 --> 00:34:55,860
that which is very enough based on pittsburgh both pattern recognition and so much

480
00:34:56,240 --> 00:35:01,260
and so you so in his cuts packages are available online things and that's actually

481
00:35:01,260 --> 00:35:05,300
a lot better than they are to be face detector but it's not really about

482
00:35:05,320 --> 00:35:07,340
you have to pay some money for

483
00:35:07,930 --> 00:35:14,030
so the can my whole thing on exhaustive local search

484
00:35:14,360 --> 00:35:17,930
but as you can kind of see is that we kind of pushing because it

485
00:35:17,930 --> 00:35:19,180
essentially is

486
00:35:19,200 --> 00:35:22,840
and if you have a problem in terms of what we kind of what kind

487
00:35:22,840 --> 00:35:27,860
of exhaustive research in these areas even though discreetly as the dimensionality my will expand

488
00:35:28,030 --> 00:35:31,280
on i'm always going to be in trouble so made three or four

489
00:35:31,800 --> 00:35:34,650
dimensional what a moron this is like i say

490
00:35:34,700 --> 00:35:39,550
to face mesh here that's twenty twenty five dimensions and there's no way that i

491
00:35:39,550 --> 00:35:41,880
can i can actually actually

492
00:35:41,880 --> 00:35:45,200
so what happens if we want finer one

493
00:35:46,280 --> 00:35:49,950
so there is an an alternative to be exhaustively search

494
00:35:49,950 --> 00:35:54,240
and that's where i think how we go

495
00:35:54,300 --> 00:35:57,300
another five minutes

496
00:35:57,320 --> 00:36:00,490
twenty minutes i can source that on on the other the other stuff

497
00:36:00,570 --> 00:36:07,880
so perhaps relationship and just to give you guys an trying to work

498
00:36:07,930 --> 00:36:13,610
what do i do that

499
00:36:13,610 --> 00:36:18,110
this guy going to give you guys an trying to

500
00:36:18,110 --> 00:36:21,840
one of the talk about that essentially

501
00:36:21,840 --> 00:36:26,110
we all know machine learning we can use regression to court to could use

502
00:36:26,150 --> 00:36:32,570
is that is that perhaps some sort of learning i to play between what displacement

503
00:36:32,630 --> 00:36:37,150
and appearances by its essentially if i have two images so i've got a source

504
00:36:37,150 --> 00:36:42,800
image in my template image and the displaced in terms of image intensities

505
00:36:42,820 --> 00:36:46,010
can i perhaps feed into some sort of regression

506
00:36:46,070 --> 00:36:51,860
regressor or some sort of other kind of a linear relationship with the actual warp

507
00:36:51,860 --> 00:36:55,760
displacement so it's kind of like is their relationship between

508
00:36:55,780 --> 00:37:00,340
the motion field in the optical field between between between two patterns and it turns

509
00:37:00,340 --> 00:37:02,780
out actually for natural images there is it so

510
00:37:02,780 --> 00:37:04,840
that's what we're going to talk about next

511
00:37:04,900 --> 00:37:08,090
to bring up

512
00:37:08,110 --> 00:37:10,700
the next thing

513
00:37:15,490 --> 00:37:18,800
this is the topic there are really like i really like because i think it's

514
00:37:18,800 --> 00:37:23,590
really neat and elegant how well this works so CV

515
00:37:23,650 --> 00:37:29,760
exhaustive search detectors worked really really well but still pretty much a pattern computational aspects

516
00:37:29,760 --> 00:37:33,110
the pretty much this kind of high school in the kind of like OK we're

517
00:37:33,110 --> 00:37:36,570
going to take a lot of good writing the gradient descent work that in the

518
00:37:36,570 --> 00:37:37,340
talk about now

519
00:37:37,400 --> 00:37:38,400
it's really

520
00:37:38,420 --> 00:37:42,220
nice in the sense that the really talk about the pixel coherence

521
00:37:42,320 --> 00:37:46,160
economists that they really do take that on board and they will happen we use

522
00:37:46,260 --> 00:37:49,900
there should be a better way for us to kind the searching in this sort

523
00:37:49,900 --> 00:37:52,720
of course looking everywhere

524
00:37:54,130 --> 00:37:56,950
we skip ahead

525
00:37:58,070 --> 00:38:01,800
as you said if you're in my person for machine learning exhaustive search is essentially

526
00:38:01,800 --> 00:38:06,200
i discreetly search of the image and i

527
00:38:06,200 --> 00:38:09,990
basically get these vectorize values of the of the object

528
00:38:10,070 --> 00:38:12,970
and if been into classifier and say

529
00:38:12,970 --> 00:38:17,570
you are not necessarily it's fundamentally con computationally expensive i can do lots of things

530
00:38:17,570 --> 00:38:20,630
the kind of limit but it's fundamentally

531
00:38:21,490 --> 00:38:23,160
so from that perspective

532
00:38:23,220 --> 00:38:27,950
and another kind of problem with that is that this is the face detector run

533
00:38:27,950 --> 00:38:30,840
across portion of the multiply face database

534
00:38:30,860 --> 00:38:35,820
what i did was that i got i had the ground truth with the eyes

535
00:38:35,820 --> 00:38:37,340
another distribution

536
00:38:37,400 --> 00:38:39,550
which is not from the list

537
00:38:42,880 --> 00:38:44,860
and then what to do

538
00:38:44,880 --> 00:38:48,110
because the way of computing the mean of a gaussian which is the sum of

539
00:38:48,110 --> 00:38:49,000
all the data

540
00:38:49,090 --> 00:38:52,440
it's actually very sensitive to

541
00:38:52,460 --> 00:38:54,880
one of us

542
00:38:54,900 --> 00:38:58,520
and there are books written robust statistic

543
00:38:58,610 --> 00:39:01,550
how to make another that is not

544
00:39:01,590 --> 00:39:06,300
it's almost optimal when the data come from the core from the distribution but it

545
00:39:06,300 --> 00:39:12,070
doesn't agree but doesn't agree badly when the data are corrupted

546
00:39:12,210 --> 00:39:14,800
and one of the simplest ways of doing it is by the what's called the

547
00:39:14,820 --> 00:39:16,780
trend in the three very

548
00:39:16,900 --> 00:39:19,670
and that's the following basically your

549
00:39:19,690 --> 00:39:22,900
so the the data in any dimension and then remove the highest

550
00:39:22,920 --> 00:39:28,440
two percent and the lowest two percent the highest one percent the lowest one person

551
00:39:28,460 --> 00:39:32,710
and of course that happen to come from the galaxy and then you are disturbing

552
00:39:32,710 --> 00:39:33,920
the results

553
00:39:33,940 --> 00:39:39,650
but you can show that in on average to there are introducing a small

554
00:39:40,020 --> 00:39:44,880
and the potential of removing actually those outlets don't come from the data is is

555
00:39:44,880 --> 00:39:50,170
is that the benefits greater when actually you have to rule it

556
00:39:50,270 --> 00:39:54,380
and that's what remained because just to train the top and the bottom of of

557
00:39:54,420 --> 00:39:56,270
the court

558
00:39:56,280 --> 00:40:00,300
and similar model for the variance and so on

559
00:40:00,300 --> 00:40:03,610
very intuitive

560
00:40:03,670 --> 00:40:10,130
and that can be done at every step of reestimation nine years the year already

561
00:40:10,230 --> 00:40:12,340
or in the game

562
00:40:12,380 --> 00:40:15,320
however i mean is to me

563
00:40:15,610 --> 00:40:17,130
there is another committee

564
00:40:17,150 --> 00:40:21,190
which is that in fact is the to change the cost function

565
00:40:21,400 --> 00:40:27,340
basically the but cost is more sensitive to outliers than the same cost without the

566
00:40:30,800 --> 00:40:33,270
and so if i want to

567
00:40:33,360 --> 00:40:38,650
and that is called the k medians algorithms because this is

568
00:40:38,690 --> 00:40:42,650
because the minimizer of that cost is the median of the

569
00:40:42,980 --> 00:40:45,670
let me make it to draw traditional

570
00:41:01,880 --> 00:41:04,300
so if the data on this line

571
00:41:04,300 --> 00:41:08,300
here is the

572
00:41:11,980 --> 00:41:20,610
this is i mean

573
00:41:20,630 --> 00:41:23,000
the median minimizes the cost

574
00:41:23,050 --> 00:41:24,820
x minus mu

575
00:41:26,630 --> 00:41:28,980
some of the text

576
00:41:29,020 --> 00:41:35,340
and i believe this square and they want to minimize some or x of

577
00:41:35,420 --> 00:41:37,730
i don't know

578
00:41:37,750 --> 00:41:42,820
just x minus

579
00:41:43,020 --> 00:41:48,280
the minimizer of this course is the media

580
00:41:48,300 --> 00:41:49,590
which would be

581
00:41:49,610 --> 00:41:51,400
one two three

582
00:41:53,420 --> 00:42:01,400
move actually problem

583
00:42:01,570 --> 00:42:10,050
OK and that's what i algorithm is called the k median

584
00:42:10,070 --> 00:42:14,270
now the media the nice property with respect to outliers

585
00:42:14,360 --> 00:42:17,540
the second thing is going to move it far away out of the room

586
00:42:17,610 --> 00:42:21,130
the median will not change at all

587
00:42:21,880 --> 00:42:24,940
the median is sensitive to the media that the

588
00:42:25,000 --> 00:42:29,360
it's not make a change in the middle of the data the median change all

589
00:42:30,090 --> 00:42:32,520
so much more robust to outliers

590
00:42:32,550 --> 00:42:37,190
and therefore is this cost will automatically be robust

591
00:42:37,210 --> 00:42:42,150
what i can say about this cost is that we don't know the minimum so

592
00:42:42,150 --> 00:42:44,520
it can be hard as well

593
00:42:44,570 --> 00:42:47,020
and in some way

594
00:42:47,040 --> 00:42:57,400
licenses but there are some computer science algorithms that can give some very complicated but

595
00:42:57,400 --> 00:43:03,500
i didn't include them in the the handouts actually give some guarantees these as for

596
00:43:03,500 --> 00:43:08,170
example the i think a factor of four approximation

597
00:43:09,300 --> 00:43:10,730
this means again

598
00:43:10,750 --> 00:43:14,840
there are four approximation means that

599
00:43:14,860 --> 00:43:18,400
the algorithm guarantees to get

600
00:43:18,420 --> 00:43:21,800
at most four times the true cost

601
00:43:21,820 --> 00:43:28,190
so will close to the true cost is small then that's not too bad

602
00:43:28,190 --> 00:43:33,570
or is it really then it's not too bad if they are clustered but

603
00:43:33,670 --> 00:43:39,040
not very compactly then it's also

604
00:43:39,050 --> 00:43:47,230
another that are applied the two single linkage or

605
00:43:47,270 --> 00:43:52,380
other algorithms that are really sensitive to outliers

606
00:43:52,400 --> 00:43:56,130
i have to eliminate small clusters

607
00:43:56,150 --> 00:43:59,630
yes so if have a cluster with two or three point

608
00:44:00,520 --> 00:44:02,020
that's close to that

609
00:44:02,020 --> 00:44:05,090
or one point which can come out in

610
00:44:05,570 --> 00:44:13,440
in single linkage just eliminated enemy that point and counter clusters without that

611
00:44:16,380 --> 00:44:19,710
before he quit the subject of let's let's think of the following

612
00:44:22,540 --> 00:44:28,310
they they should be there and what outlier is some instances the philosophical problem or

613
00:44:28,310 --> 00:44:31,500
it's a problem that

614
00:44:31,520 --> 00:44:35,110
this fellow cannot solve you have to think what you really want

615
00:44:35,130 --> 00:44:39,090
and one of the problems that the race is that if you have a bias

616
00:44:39,090 --> 00:44:43,180
why don't we make a start

617
00:44:44,810 --> 00:44:46,910
so last week

618
00:44:46,930 --> 00:44:51,120
so unless we yesterday i didn't quite finish with our discussion

619
00:44:51,130 --> 00:44:56,300
of the expectation value so this is still from the first set of lecture notes

620
00:44:56,300 --> 00:44:58,530
but what we had gone through with this idea

621
00:44:58,550 --> 00:45:03,480
but you have a probability density function that it's convenient to be able to summarize

622
00:45:03,480 --> 00:45:04,510
it with some

623
00:45:04,570 --> 00:45:09,620
numerical values that characterize its position in width and so forth and so that's why

624
00:45:09,640 --> 00:45:12,300
we define this expectation value

625
00:45:13,080 --> 00:45:19,440
expectation values can be used not only for probability density functions that are of function

626
00:45:19,480 --> 00:45:23,560
of a single random variable but it can also be that the events that you're

627
00:45:23,560 --> 00:45:28,910
observing are characterized by same more than one random variable so you have some joint

628
00:45:28,920 --> 00:45:33,020
probability density function and in that case

629
00:45:33,050 --> 00:45:37,770
it's useful to give a measure of the expectation value of

630
00:45:37,790 --> 00:45:42,850
products of pairs of the random variables so suppose that we're observing i don't know

631
00:45:42,850 --> 00:45:43,660
we're observing people

632
00:45:44,080 --> 00:45:48,830
and each person is characterized by a certain height in a certain way

633
00:45:48,950 --> 00:45:52,390
so you can measure those two quantities for each person

634
00:45:52,440 --> 00:45:56,190
and you would like to know something about the expectation value of the product of

635
00:45:56,190 --> 00:46:01,750
those two quantities now typically what you do is you take the expectation value of

636
00:46:01,750 --> 00:46:06,450
the product and it is convenient to subtract off from that the product of the

637
00:46:06,450 --> 00:46:12,080
two individual expectation values so here i have used the notation new setbacks to simply

638
00:46:12,720 --> 00:46:15,140
the expectation value of x alone

639
00:46:15,140 --> 00:46:19,870
and mu so y to mean the expectation station value of y alone

640
00:46:19,950 --> 00:46:20,830
OK so

641
00:46:20,860 --> 00:46:23,720
that combination expectation value of x y

642
00:46:23,720 --> 00:46:28,970
minus the product of the two corresponding expectation values that's what we call the covariance

643
00:46:29,050 --> 00:46:33,200
of two random variables x and y so usually written with some sort of notation

644
00:46:33,200 --> 00:46:34,470
like that

645
00:46:34,480 --> 00:46:38,000
if you if you multiply this out you can see the x the expression on

646
00:46:38,000 --> 00:46:43,780
the right is is equivalent so it's the expectation value of the product of x

647
00:46:43,780 --> 00:46:47,450
minus mu x times y minus mu y

648
00:46:48,180 --> 00:46:52,170
OK so so that gives you information on how large on average the

649
00:46:52,180 --> 00:46:56,010
the product of x and y are now if x and y have some sort

650
00:46:56,010 --> 00:47:01,270
of units like axes in centimetres and y is in horsepower then

651
00:47:01,300 --> 00:47:03,980
the covariance you can see i will have

652
00:47:04,040 --> 00:47:06,270
the product of those two dimensions

653
00:47:06,320 --> 00:47:07,580
but it is often

654
00:47:07,600 --> 00:47:11,930
useful to provide a measure of the covariance which is dimensionless

655
00:47:12,010 --> 00:47:16,890
so what we do is we take the covariance and divided by the product of

656
00:47:16,890 --> 00:47:17,640
the two

657
00:47:17,650 --> 00:47:24,130
corresponding standard deviations so remember that if a variable x has some units than the

658
00:47:24,130 --> 00:47:27,130
standard deviation of x has the same units

659
00:47:27,200 --> 00:47:28,700
so therefore this

660
00:47:28,710 --> 00:47:35,080
combination here is is dimensionless and it is what's called the correlation coefficient

661
00:47:35,080 --> 00:47:40,950
it is dimensionless and furthermore you can show that its value lies necessarily between minus

662
00:47:40,950 --> 00:47:43,700
one n one

663
00:47:44,550 --> 00:47:48,040
before i i'm going to show graph just on the next overhead to show how

664
00:47:48,040 --> 00:47:53,170
to interpret that intuitively but let me just make first the following comment and that

665
00:47:53,170 --> 00:47:55,850
is that if the two random variables

666
00:47:55,860 --> 00:47:58,270
in question are independent

667
00:47:58,300 --> 00:48:01,080
so recall from yesterday what we define that to mean

668
00:48:01,100 --> 00:48:05,850
if two random variables are independent it means that the joint probability density for those

669
00:48:05,850 --> 00:48:11,380
two variables factorizes into a function of one times the function of the other

670
00:48:11,390 --> 00:48:16,460
and those two functions are in fact the marginal pdf for the corresponding variables

671
00:48:16,510 --> 00:48:19,360
OK well if that's true then we can work out

672
00:48:20,230 --> 00:48:23,510
the expectation value of the product is

673
00:48:23,510 --> 00:48:28,800
OK what is that by definition that's just the integral over both of my variables

674
00:48:28,820 --> 00:48:30,420
x times y

675
00:48:30,430 --> 00:48:32,890
times the joint pdf for x and y

676
00:48:34,420 --> 00:48:36,230
if this condition holds

677
00:48:36,290 --> 00:48:41,490
that means that the variable will factorize into a very into an integral over x

678
00:48:41,490 --> 00:48:43,760
times an integral over y

679
00:48:43,800 --> 00:48:46,770
and what you get if you just stare at this for five seconds you realize

680
00:48:46,790 --> 00:48:50,450
that that will simply be equal to the expectation value of x

681
00:48:50,460 --> 00:48:52,890
times the expectation value of y

682
00:48:52,950 --> 00:48:56,640
so if the two variables are independent the covariance

683
00:48:56,690 --> 00:48:58,600
necessarily vanishes

684
00:48:58,640 --> 00:49:01,340
so that's that's important fact to remember

685
00:49:01,400 --> 00:49:07,080
the converse is not necessarily true you can have situations where you have variables that

686
00:49:07,080 --> 00:49:09,270
it is absolutely critical

687
00:49:09,300 --> 00:49:11,840
first of all because our customers

688
00:49:11,850 --> 00:49:15,140
really want to have i think solutions which work

689
00:49:15,150 --> 00:49:20,480
well on future data you know in all of the industrial projects i pursued so

690
00:49:20,480 --> 00:49:27,900
far the the robustness of of the methods was a key ingredient the engineers asked

691
00:49:28,670 --> 00:49:36,950
and that actually cuts down their their their their development time so if you have

692
00:49:36,950 --> 00:49:43,100
a better understanding what it means i have a robust solution for image interpretation task

693
00:49:43,320 --> 00:49:49,150
then i think we learn a lot about the generalisation in hindi context there are

694
00:49:49,150 --> 00:49:53,300
a couple of approaches for clustering in particular and i will focus on the last

695
00:49:53,300 --> 00:49:54,510
one because it comes

696
00:49:54,580 --> 00:49:58,800
from my group its stability analysis of solutions

697
00:49:58,820 --> 00:50:02,040
and who does someone have stable solutions

698
00:50:03,930 --> 00:50:08,160
so they don't types you will find

699
00:50:08,170 --> 00:50:14,370
clustering approaches for quite different data types of rectal data are probably used in more

700
00:50:14,370 --> 00:50:19,190
than ninety percent of the applications if you are sure that you know in which

701
00:50:19,190 --> 00:50:24,050
space you have to represent your objects then just below the vector space for these

702
00:50:24,050 --> 00:50:31,880
objects and then start doing you grouping there another approach very successful and pursued by

703
00:50:33,060 --> 00:50:38,700
tally tishby and his group in israel and also by my group is to use

704
00:50:38,700 --> 00:50:44,450
the histogram data to find a good groups and

705
00:50:44,480 --> 00:50:50,340
a third approach which i want to go into is the proximity the data approach

706
00:50:50,350 --> 00:50:57,200
where you no longer represent individual objects either by vectors by histograms but you characterize

707
00:50:57,200 --> 00:51:03,850
objects by their pairwise relations proximity data dissimilarity doesn't really matter if you keep track

708
00:51:03,850 --> 00:51:07,530
of the sign obviously the the the

709
00:51:07,590 --> 00:51:10,230
the last representation

710
00:51:10,250 --> 00:51:17,920
the the proximity data representation here is is the harder problem because the structure now

711
00:51:17,920 --> 00:51:23,610
is hidden in pairwise relations between the object to have a square matrix which might

712
00:51:23,610 --> 00:51:25,820
actually have missing values and so on

713
00:51:29,090 --> 00:51:31,840
there's a lot of

714
00:51:31,850 --> 00:51:32,880
so all of

715
00:51:32,920 --> 00:51:39,520
classification of the different clustering principles in the literature

716
00:51:39,580 --> 00:51:43,680
i would like on the meta level sort of two

717
00:51:43,700 --> 00:51:51,110
two point q two two some criteria which you could actually use to see if

718
00:51:51,110 --> 00:51:54,870
sort of several methods basically do the same thing despite the fact that they are

719
00:51:55,380 --> 00:51:58,110
written up in completely different ways so

720
00:51:58,120 --> 00:52:03,680
compactness is one of the criteria which allows you to find groups of objects and

721
00:52:03,680 --> 00:52:08,930
depending on how you represent your object in which underlying space to represent them you

722
00:52:08,930 --> 00:52:14,080
might actually use a method called k means come to that about probably most of

723
00:52:14,080 --> 00:52:19,190
you know that already if you represent your objects by histograms then you could use

724
00:52:19,190 --> 00:52:24,940
a method called histogram clustering any does roughly the same as k means except in

725
00:52:24,950 --> 00:52:29,170
a different space is the space of probability distributions see and here is the space

726
00:52:29,170 --> 00:52:30,010
of vector

727
00:52:30,080 --> 00:52:40,320
pairwise data clustering for for relational data for proximity data that she also a compact

728
00:52:40,360 --> 00:52:49,180
this type of of grouping of your objects average association is a different name for

729
00:52:49,180 --> 00:53:00,350
pairwise data clustering for cold and and people in my group has looked at a

730
00:53:00,350 --> 00:53:05,670
connection between pairwise data clustering and k means and it's called the constant shift embedding

731
00:53:05,670 --> 00:53:12,360
paradigm which really makes explicit how you go from relational data to vectorial data and

732
00:53:12,360 --> 00:53:16,940
we'll be hearing people be drifting and thank you all for interrupting your skiing to

733
00:53:16,940 --> 00:53:21,100
come so i just a quick show of hands how many people have heard either

734
00:53:21,100 --> 00:53:25,170
jacob are myself talk about neighbourhood components analysis before

735
00:53:25,200 --> 00:53:28,880
OK so maybe about half so i'll try and goal of the quickly through that

736
00:53:28,880 --> 00:53:33,510
and then get to some of the newer work with with so feel free to

737
00:53:33,860 --> 00:53:36,360
to interrupt me at any time

738
00:53:36,550 --> 00:53:39,700
and ask questions or

739
00:53:39,780 --> 00:53:43,450
make insulting comments or whatever starting argument

740
00:53:44,150 --> 00:53:49,810
OK so as you all know the the sort of thing for today's distance metric

741
00:53:49,810 --> 00:53:53,680
learning and the idea is that there's a lot of times in the course of

742
00:53:53,680 --> 00:53:57,810
an algorithm when we need some kind of distance measure often we need to have

743
00:53:57,830 --> 00:54:02,910
the properties of the metric but maybe not always which compares examples of scalar function

744
00:54:02,910 --> 00:54:07,260
which eats two examples and tells you how different are similar they are

745
00:54:07,270 --> 00:54:13,590
and unless the structure strongly specifies this metric or maybe some constraints on that metric

746
00:54:13,590 --> 00:54:17,330
then it seems like the preferred approach is to learn the metric as part of

747
00:54:17,330 --> 00:54:18,220
your problem

748
00:54:18,240 --> 00:54:21,540
so in other words you have a lot of data often the data has labels

749
00:54:21,540 --> 00:54:25,590
which implicitly tell you what the right metric is anyone who performed the learning along

750
00:54:25,590 --> 00:54:28,710
with the rest of the parameters based on the training set so today i'm going

751
00:54:28,710 --> 00:54:33,670
to focus on something very very simple semiparametric classifiers like k nearest neighbour augustine kernel

752
00:54:33,670 --> 00:54:39,390
support vector machines and gaussian processes you know very local machines

753
00:54:39,440 --> 00:54:41,930
and the idea is given labelset

754
00:54:41,940 --> 00:54:46,780
so we have the training set of inputs so use extra inputs and labels your

755
00:54:46,780 --> 00:54:52,210
why sometimes all use how should we learn a metric that will give good generalisation

756
00:54:52,210 --> 00:54:56,820
when used in such a classifier OK so what's the idea of these classifiers they

757
00:54:56,820 --> 00:55:02,530
just look locally in the neighborhood of the test example at nearby training examples and

758
00:55:02,530 --> 00:55:05,070
they sort of average labels are there's

759
00:55:05,170 --> 00:55:09,060
regression outputs or whatever and the idea is to make this measurement of what is

760
00:55:09,060 --> 00:55:15,490
local mean good for whatever you're trying to do classification or prediction intuitively should think

761
00:55:15,490 --> 00:55:21,430
that our goal is to make directions of the space which are relevant for classification

762
00:55:21,450 --> 00:55:22,840
we want to shrink them

763
00:55:22,850 --> 00:55:26,340
and directions of the space which are very informative we want to structure that lets

764
00:55:26,340 --> 00:55:29,080
the that's the job of the metric

765
00:55:31,340 --> 00:55:35,490
there's this sort of at this point in my talk usually

766
00:55:35,500 --> 00:55:38,170
someone put up their hand and says

767
00:55:38,180 --> 00:55:43,270
but you have a line of research is totally stupid because local if classifiers are

768
00:55:45,090 --> 00:55:48,900
they need an exponential amount of training data to just memorize the space and they

769
00:55:48,900 --> 00:55:50,990
don't generalize in any meaningful way

770
00:55:51,000 --> 00:55:55,380
OK so that's actually a somewhat valid criticism but it's not a criticism i'm going

771
00:55:55,380 --> 00:55:57,450
to get into a battle about today

772
00:55:57,530 --> 00:56:03,570
i'm going to defer the criticism to this you know all paper by rob hold

773
00:56:03,580 --> 00:56:05,850
which i think is still somewhat true today

774
00:56:05,900 --> 00:56:09,390
which is that there a lot of situations in which very simple classifiers tend to

775
00:56:09,390 --> 00:56:10,640
do surprisingly well

776
00:56:10,690 --> 00:56:15,050
OK so you should think of this is the conditional talk if you're interested in

777
00:56:15,050 --> 00:56:19,900
some kind of the local method like nearest neighbour gaussian kernel support vector machine or

778
00:56:19,900 --> 00:56:23,920
whatever then i have something interesting to tell you about learning the distance if you're

779
00:56:23,920 --> 00:56:25,340
not interested

780
00:56:25,390 --> 00:56:27,560
there's a wireless connection in this room

781
00:56:27,580 --> 00:56:32,170
and i'll be done in forty five minutes so OK

782
00:56:32,180 --> 00:56:37,230
so just you know to remind you of the set for instance based on memory

783
00:56:37,230 --> 00:56:42,530
based or non parametric classifiers the idea is that you just have the training set

784
00:56:42,540 --> 00:56:45,550
and you take say the k nearest neighbors of the training set you take the

785
00:56:45,550 --> 00:56:49,900
majority vote in that class labels so people to make fun of k nearest neighbour

786
00:56:49,900 --> 00:56:53,060
but it's pretty interesting the decision surfaces are nonlinear

787
00:56:53,120 --> 00:56:59,090
it's semi parametric and nonparametric so it has high capacity without having really to train

788
00:56:59,090 --> 00:57:00,350
a lot of parameters

789
00:57:00,400 --> 00:57:02,450
so what is this point i think this a very

790
00:57:02,460 --> 00:57:08,090
the important point that underappreciated if you want your a classifier to do something complicated

791
00:57:08,090 --> 00:57:10,890
it so in some sense it has to have a lot of bits inside its

792
00:57:11,710 --> 00:57:15,910
it has to have some work has the reasonable capacity but we're not very good

793
00:57:15,910 --> 00:57:18,270
at setting huge numbers of parameters

794
00:57:19,140 --> 00:57:22,390
what we can do we're going to say well will make the training set itself

795
00:57:22,390 --> 00:57:26,030
in some sense all those bits because we're pretty sure we didn't screw up those

796
00:57:26,030 --> 00:57:29,490
bits and then we're only going to say you know one number k or one

797
00:57:29,490 --> 00:57:31,250
bandwith parameter alpha

798
00:57:31,270 --> 00:57:36,460
OK and the quality of the predictions of automatically improves with more data these results

799
00:57:36,460 --> 00:57:41,040
about it being asymptotically close to optimal which you may or may not be interested

800
00:57:41,040 --> 00:57:46,120
in because of the word asymptotic but some people think that this word is just

801
00:57:46,120 --> 00:57:49,950
the gateway to lots of very interesting theoretical results and some people think that this

802
00:57:49,950 --> 00:57:52,740
word means you should ignore everything that comes after

803
00:57:52,910 --> 00:57:57,720
OK there's only a few parameters to tune usually you can do it by cross

804
00:57:57,720 --> 00:58:01,920
validation or some kind of simple optimisation you know gradient on leave one out of

805
00:58:03,020 --> 00:58:04,500
OK so what's the problem

806
00:58:04,560 --> 00:58:08,210
the problem is what is nearest mean you need to specify some kind of distance

807
00:58:08,210 --> 00:58:09,640
metric on the input

808
00:58:09,660 --> 00:58:13,990
so that's one problem with these classifiers in the second problem is computational cost you

809
00:58:13,990 --> 00:58:17,320
have to sort of story all training set and carried around in your suitcase with

810
00:58:17,320 --> 00:58:19,790
you every time you want to do classification so

811
00:58:20,210 --> 00:58:23,780
we'd like to sort of try and solve both of these problems

812
00:58:23,820 --> 00:58:26,780
so today i'm going to talk mostly about the first problem how do you want

813
00:58:26,790 --> 00:58:28,230
to distance metric

814
00:58:28,280 --> 00:58:31,520
and also to show you how if you need to you can make this distance

815
00:58:31,520 --> 00:58:32,880
metric low rank

816
00:58:32,880 --> 00:58:36,190
we have to square pads coming to it

817
00:58:36,210 --> 00:58:41,160
and we have two to the thirtieth pattern going away from it

818
00:58:41,160 --> 00:58:43,980
so that's two to the thirty two paths going through it

819
00:58:43,990 --> 00:58:47,400
so let's look at this past year we have to do the two patterns

820
00:58:47,400 --> 00:58:49,220
i want show all of them

821
00:58:49,250 --> 00:58:52,380
going from start to it

822
00:58:52,390 --> 00:58:54,160
and to to the

823
00:58:54,180 --> 00:58:56,410
thirty pounds going from it to the end

824
00:58:56,560 --> 00:58:59,940
so let's label these patterns with names just to see what's going on this is

825
00:58:59,940 --> 00:59:03,480
x y z and p q are

826
00:59:03,530 --> 00:59:07,910
so we're interested in

827
00:59:07,920 --> 00:59:12,690
well so one path that goes through the state is x times p

828
00:59:12,760 --> 00:59:14,930
we can come in by actually by p

829
00:59:14,940 --> 00:59:21,710
and another path is xq

830
00:59:21,740 --> 00:59:23,510
another path is x are

831
00:59:23,550 --> 00:59:26,230
another pattern is why p

832
00:59:26,270 --> 00:59:27,650
why q

833
00:59:27,670 --> 00:59:29,500
why are

834
00:59:30,920 --> 00:59:32,340
zq q

835
00:59:32,390 --> 00:59:36,410
xe are and for those of you who can see behind here it's exactly what

836
00:59:36,410 --> 00:59:38,000
you'd expect

837
00:59:38,050 --> 00:59:41,610
OK so can we factor this

838
00:59:41,620 --> 00:59:47,700
this factoring trick is the whole idea here

839
00:59:47,760 --> 01:00:01,700
so we've got nine had the well two square times to the thirtieth panzer

840
01:00:01,740 --> 01:00:05,360
this is just going to be x plus y fuzzy

841
01:00:05,370 --> 01:00:07,850
times p plus q plus are

842
01:00:07,920 --> 01:00:11,830
there's always getting into another ways of getting out

843
01:00:11,840 --> 01:00:16,460
well this is the this is the sum of two squares things up on the

844
01:00:16,460 --> 01:00:18,380
shore three of them

845
01:00:18,400 --> 01:00:23,920
and this is some two to thirtieth things but we computed right self

846
01:00:23,940 --> 01:00:27,160
total number of ways of getting to the airport

847
01:00:27,180 --> 01:00:31,820
to the thirtieth number ways of getting into her sorry total probability of paths getting

848
01:00:31,820 --> 01:00:35,700
away from that airport if we multiply these we have the total probability will pass

849
01:00:35,710 --> 01:00:39,250
through the airport so it's just tell the time spent school

850
01:00:39,310 --> 01:00:42,480
so let's see how that comes out of the spreadsheet

851
01:00:42,500 --> 01:00:47,250
so here we have the alpha color

852
01:00:47,270 --> 01:00:51,260
the columns for the going through the cold state going through the heart state better

853
01:00:51,260 --> 01:00:53,480
column similarly

854
01:00:53,490 --> 01:00:58,490
and here's alpha times better so this is the total probability of all paths going

855
01:00:58,490 --> 01:01:03,650
through the cold state day one two three four total probability of all paths going

856
01:01:03,650 --> 01:01:06,550
through the hot state it is one two three four now if you look at

857
01:01:06,550 --> 01:01:07,730
the alphas

858
01:01:07,750 --> 01:01:12,400
these decrease tend to decrease as you go down the color right

859
01:01:12,410 --> 01:01:16,360
if you look at the baiters these ten two

860
01:01:16,410 --> 01:01:21,110
interesting well working backwards so it's not too surprising

861
01:01:21,160 --> 01:01:24,260
they tend to decrease as you go up the

862
01:01:24,320 --> 01:01:28,490
because the padgett as you go up the column the paths from day thirteen and

863
01:01:28,490 --> 01:01:31,820
twelve eleven to the end get longer and longer

864
01:01:31,830 --> 01:01:36,710
the story explaining more and more ice cream when we multiply alpha by that works

865
01:01:36,710 --> 01:01:40,660
explaining all the groups

866
01:01:40,680 --> 01:01:41,590
so now

867
01:01:41,660 --> 01:01:43,600
this is

868
01:01:43,650 --> 01:01:45,420
say all the paths

869
01:01:45,430 --> 01:01:47,220
they go through

870
01:01:47,770 --> 01:01:52,210
the cold stayed on day five to the thirty second pass

871
01:01:52,260 --> 01:01:55,180
here's another two to the thirty second pass go through the heart stayed on day

872
01:01:55,190 --> 01:01:59,120
five let's add those together one of the total probability all

873
01:01:59,120 --> 01:02:01,160
two the thirty third perhaps

874
01:02:03,430 --> 01:02:10,090
what's going on in this column

875
01:02:10,100 --> 01:02:13,880
total probability vol two to thirty third has always the same right it's just a

876
01:02:13,880 --> 01:02:18,160
question how we divide it up

877
01:02:18,180 --> 01:02:20,180
so here

878
01:02:20,330 --> 01:02:25,320
we can look at two to thirty third go through this state versus that state

879
01:02:25,420 --> 01:02:28,300
or they go through the state versus that

880
01:02:28,310 --> 01:02:30,720
different divisions of the past

881
01:02:30,740 --> 01:02:31,860
so here

882
01:02:31,860 --> 01:02:35,380
you know we've taken say pads a and b and c and d here it's

883
01:02:35,380 --> 01:02:37,320
going to be a and c and b and

884
01:02:37,330 --> 01:02:39,740
OK but are we divide it up into two

885
01:02:39,760 --> 01:02:44,440
into two groups the total probability of all the paths will be the same

886
01:02:45,310 --> 01:02:47,200
what do you think it means

887
01:02:47,230 --> 01:02:49,950
that this number

888
01:02:49,960 --> 01:02:53,530
is about one hundred times more likely than that number

889
01:02:53,580 --> 01:02:57,160
a hundred times bigger than the numbers are all unlikely right actually what is this

890
01:02:57,160 --> 01:03:03,170
sort of ten r nine times ten to the minus nineteen twenties this number would

891
01:03:05,490 --> 01:03:09,600
the likelihood of that is retiring

892
01:03:09,630 --> 01:03:13,280
the likelihood of that sequence of choices

893
01:03:14,860 --> 01:03:17,060
what do you think it means

894
01:03:17,110 --> 01:03:18,890
but that's about one hundred times more

895
01:03:18,900 --> 01:03:22,350
then that

896
01:03:22,360 --> 01:03:27,420
so i hard what it means is that there is a lot more probability on

897
01:03:27,420 --> 01:03:30,330
has to go through the hot stayed on day three than on the cold and

898
01:03:30,490 --> 01:03:34,400
three so all of all the to the thirty third ways to explain how a

899
01:03:34,400 --> 01:03:38,050
those ice-creams the happen which is a hot day and a three

900
01:03:38,100 --> 01:03:42,480
have much more total probability than half of which is cold and three

901
01:03:42,500 --> 01:03:45,520
and that is how we got the graph

902
01:03:46,330 --> 01:03:49,700
if you look at this is the probability that we're going to called state in

903
01:03:49,700 --> 01:03:52,930
other words it was called a and that is

904
01:03:52,950 --> 01:03:55,660
the ratio of this to the total

905
01:03:55,740 --> 01:03:57,360
and over here

906
01:03:57,360 --> 01:03:59,180
we have the ratio of that to the total

907
01:03:59,200 --> 01:04:01,840
so these are these two numbers sum to one

908
01:04:01,970 --> 01:04:06,320
because we're just asking about

909
01:04:06,330 --> 01:04:07,140
of the

910
01:04:07,150 --> 01:04:09,920
the total of these two numbers

911
01:04:09,970 --> 01:04:13,100
here's the total what fraction of those to the called state what fraction goes to

912
01:04:13,100 --> 01:04:16,970
the heart of it so it's about one percent versus ninety nine percent

913
01:04:16,980 --> 01:04:20,300
and if we look at this picture we see that's what we get here

914
01:04:20,300 --> 01:04:22,680
the three it's ninety nine percent

915
01:04:22,710 --> 01:04:25,590
OK so this that's exactly where the graph came from

916
01:04:25,600 --> 01:04:27,650
all right

917
01:04:31,050 --> 01:04:35,330
so let me just ask a little question here it is we know now why

918
01:04:35,330 --> 01:04:37,020
this state here

919
01:04:37,110 --> 01:04:40,280
was tagged as

920
01:04:40,530 --> 01:04:43,720
was was tagged as a hot day

921
01:04:43,750 --> 01:04:45,110
despite having

922
01:04:45,140 --> 01:04:49,310
and apparently uncertain two ice-creams it was because the data probability right

923
01:04:49,350 --> 01:04:52,770
because the better probability allows it to look for if we decide to make this

924
01:04:52,770 --> 01:04:56,220
high is more pleasant to explore so you know it's it's not more pleasant to

925
01:04:56,220 --> 01:05:00,740
explain this to extremes here but it's more pleasant to predict the weather that would

926
01:05:00,740 --> 01:05:03,910
explain the subsequent

927
01:05:04,000 --> 01:05:08,730
however over here we've got tired eleven damon is the one i screamed and yet

928
01:05:08,730 --> 01:05:11,400
we predicted that it would be high

929
01:05:11,420 --> 01:05:17,990
so what are we predict that

930
01:05:18,020 --> 01:05:20,260
why should one i screamed behind

931
01:05:20,280 --> 01:05:24,610
i think it's probably right by the way

932
01:05:24,610 --> 01:05:32,250
you not just so there are three extra days after there's three into icelandic before

933
01:05:32,280 --> 01:05:35,680
i mean we have a lot of evidence that there's hard this before and after

934
01:05:35,680 --> 01:05:39,030
so we we if we already believe based on information from the left this is

935
01:05:39,030 --> 01:05:43,090
probably day and we already believe based on information from the right this is probably

936
01:05:43,090 --> 01:05:46,200
a hot day what what what does it do for us if we predict the

937
01:05:46,200 --> 01:05:51,380
cold here well mixed seven times more likely to seven times more likely to predict

938
01:05:51,420 --> 01:05:54,780
the data that we actually observe so we're doing a better job if we made

939
01:05:54,780 --> 01:05:57,660
the so-called they would be doing a better job

940
01:05:57,700 --> 01:06:00,070
by a factor of seven

941
01:06:00,180 --> 01:06:02,780
predicting a single ice

942
01:06:03,300 --> 01:06:07,820
so what's the cost for it's terrible cost because

943
01:06:07,840 --> 01:06:10,950
in order to given that this is probably hard and that's probably how what we

944
01:06:10,950 --> 01:06:13,920
have to do we go from hot to cold back to hot

945
01:06:13,930 --> 01:06:16,470
well because of factor eight

946
01:06:17,930 --> 01:06:18,590
if we can

947
01:06:18,590 --> 01:06:19,660
ha ha ha

948
01:06:19,660 --> 01:06:22,730
this displaced water is about here

949
01:06:22,780 --> 01:06:25,050
now we have the we enforce up

950
01:06:25,080 --> 01:06:26,900
and now you see what's going to happen

951
01:06:26,940 --> 01:06:28,680
i told to site

952
01:06:28,780 --> 01:06:32,140
it will rotate even further this talk will drive it away

953
01:06:32,180 --> 01:06:33,300
from the vertical

954
01:06:33,310 --> 01:06:35,740
and that's very important therefore we ships

955
01:06:35,760 --> 01:06:38,970
that you always build the ships such that the centre of mass of the ship

956
01:06:39,020 --> 01:06:41,960
is as low as you can get it to give you the most stable

957
01:06:43,130 --> 01:06:45,810
if you bring the centre of mass of ships very high

958
01:06:45,820 --> 01:06:50,220
in the seventeenth century that is very massive cannons which were very high on the

959
01:06:51,150 --> 01:06:53,950
the ship can capsize and has happened many times

960
01:06:54,010 --> 01:06:56,460
the centre of mass which is too high

961
01:06:58,640 --> 01:07:00,610
the centre of mass is somewhere here

962
01:07:00,630 --> 01:07:02,130
very heavy this part

963
01:07:02,180 --> 01:07:03,300
so now

964
01:07:03,310 --> 01:07:06,290
if i is lower than the water notice it goes into the water to the

965
01:07:06,290 --> 01:07:07,940
same depth

966
01:07:07,970 --> 01:07:10,180
because the we enforce

967
01:07:10,260 --> 01:07:12,700
of course the same so the amount of displaced

968
01:07:12,800 --> 01:07:14,970
what is the same

969
01:07:15,030 --> 01:07:16,420
in both cases

970
01:07:16,430 --> 01:07:18,710
but now the centre of mass is high

971
01:07:18,720 --> 01:07:21,140
and this is very unstable when i let go

972
01:07:22,580 --> 01:07:25,150
centre of mass of the object was higher

973
01:07:25,170 --> 01:07:26,570
the centre of mass

974
01:07:26,580 --> 01:07:29,350
of the displaced fluid

975
01:07:29,360 --> 01:07:31,030
so we ship you have to

976
01:07:31,070 --> 01:07:36,810
a very careful about that

977
01:07:36,810 --> 01:07:43,890
let's talk a little bit about balloons

978
01:07:43,900 --> 01:07:47,890
available only

979
01:07:47,930 --> 01:07:50,790
the situation is not too dissimilar

980
01:07:50,800 --> 01:07:52,720
from having an object

981
01:07:53,580 --> 01:07:54,670
in a liquid

982
01:07:54,710 --> 01:07:57,390
but the balloon never mass

983
01:07:57,440 --> 01:08:00,860
that is the mass of the gas in the balloon

984
01:08:00,940 --> 01:08:04,530
all the rest

985
01:08:04,590 --> 01:08:08,040
and what i mean by all the rest that is the material of the balloon

986
01:08:08,040 --> 01:08:12,080
industry everything else that makes up the mess

987
01:08:12,090 --> 01:08:13,920
it has a certain volume v

988
01:08:14,100 --> 01:08:18,600
and so on a certain role of the gas inside

989
01:08:18,640 --> 01:08:22,380
and there's role of air outside

990
01:08:22,390 --> 01:08:26,480
and i want to evaluate what the criterion is for balloon to rise

991
01:08:26,540 --> 01:08:32,110
well for it to rise to reinforce have to be larger energy

992
01:08:33,000 --> 01:08:34,670
what is the buoyant force

993
01:08:34,670 --> 01:08:35,890
that's the weight

994
01:08:35,900 --> 01:08:40,020
of the displaced fluid the fluid in this case is air

995
01:08:40,040 --> 01:08:43,930
so the weight of the displaced fluid is the volume

996
01:08:43,970 --> 01:08:46,870
times the density of the air

997
01:08:46,940 --> 01:08:49,820
that's the fluid in which it is now

998
01:08:49,820 --> 01:08:51,390
times g

999
01:08:51,470 --> 01:08:54,960
that's the way we enforce that's the weight of the displaced fluid

1000
01:08:55,000 --> 01:08:56,450
has to be larger

1001
01:08:56,500 --> 01:08:58,190
and ng

1002
01:08:58,200 --> 01:09:00,790
ng is the mass of the gas

1003
01:09:00,870 --> 01:09:02,980
which is the volume of the gas

1004
01:09:05,590 --> 01:09:08,000
the density of the gas

1005
01:09:08,020 --> 01:09:09,180
that's the mass

1006
01:09:09,180 --> 01:09:09,980
times g

1007
01:09:09,990 --> 01:09:12,250
you have to convert it to force

1008
01:09:12,290 --> 01:09:15,620
plus all the rest

1009
01:09:17,630 --> 01:09:20,230
i lose my g

1010
01:09:20,330 --> 01:09:22,280
and what you see

1011
01:09:23,410 --> 01:09:25,200
this of course

1012
01:09:25,410 --> 01:09:26,940
o is larger than zero

1013
01:09:26,990 --> 01:09:30,280
there's always some mass associated with the skin

1014
01:09:30,330 --> 01:09:33,640
and in this case was the string

1015
01:09:33,710 --> 01:09:35,840
but you see the only way that this

1016
01:09:35,840 --> 01:09:37,720
well and can rise

1017
01:09:37,790 --> 01:09:38,900
is that the

1018
01:09:38,900 --> 01:09:43,130
the density of the gas must be smaller than the density of air

1019
01:09:43,150 --> 01:09:46,770
density of the gas must be less than the density of the air

1020
01:09:46,850 --> 01:09:50,450
this is a necessary condition

1021
01:09:50,510 --> 01:09:53,840
for this to hold it is not a sufficient condition

1022
01:09:53,910 --> 01:09:55,840
because i can take a balloon

1023
01:09:55,890 --> 01:09:57,700
what little bit of in there

1024
01:09:57,730 --> 01:10:00,740
so the density of the gas is lower than the density of air but it

1025
01:10:00,740 --> 01:10:02,540
may not arise

1026
01:10:02,550 --> 01:10:05,930
and that's because of this term

1027
01:10:05,930 --> 01:10:07,740
but it is a necessary condition

1028
01:10:07,760 --> 01:10:08,670
but not

1029
01:10:08,690 --> 01:10:11,490
a sufficient condition

1030
01:10:11,640 --> 01:10:13,730
now i'm going to

1031
01:10:13,790 --> 01:10:17,360
make you see the demonstration which is extremely nonintuitive

1032
01:10:17,370 --> 01:10:19,940
and i will try step by step

1033
01:10:19,970 --> 01:10:21,420
to explain to you

1034
01:10:21,460 --> 01:10:23,300
why you see what you're see

1035
01:10:23,410 --> 01:10:25,200
we're going to see

1036
01:10:25,290 --> 01:10:26,940
very nonintuitive

1037
01:10:27,080 --> 01:10:29,040
try to follow closely

1038
01:10:29,200 --> 01:10:32,910
what i see what you will see

1039
01:10:32,950 --> 01:10:35,080
i feel pendulum

1040
01:10:35,220 --> 01:10:36,570
an apple

1041
01:10:36,580 --> 01:10:38,620
you blue

1042
01:10:38,680 --> 01:10:39,800
so this

1043
01:10:40,520 --> 01:10:43,580
i got this story and i cut the string gravity in this direction

1044
01:10:43,660 --> 01:10:46,020
we have over four

1045
01:10:46,040 --> 01:10:47,890
will rise

1046
01:10:48,890 --> 01:10:50,590
because in the opposite direction

1047
01:10:50,600 --> 01:10:52,850
the gravitational acceleration

1048
01:10:52,860 --> 01:10:55,360
if there were no gravity

1049
01:10:55,370 --> 01:10:58,390
this one would not arise

1050
01:10:58,430 --> 01:11:01,590
and you yet will not fall

1051
01:11:01,600 --> 01:11:02,890
i agree so far

1052
01:11:02,940 --> 01:11:04,130
without gravity

1053
01:11:04,140 --> 01:11:05,420
it would not fall

1054
01:11:05,430 --> 01:11:07,330
problem would not arise

1055
01:11:07,350 --> 01:11:10,580
now we go in outer space

1056
01:11:14,940 --> 01:11:17,440
in compartment

1057
01:11:17,490 --> 01:11:22,680
here's an apple

1058
01:11:25,400 --> 01:11:28,480
none of us great

1059
01:11:28,570 --> 01:11:29,680
no gravity

1060
01:11:29,760 --> 01:11:31,640
and here is a meeting field

1061
01:11:31,640 --> 01:11:34,680
of blue and there's an air inside

1062
01:11:34,790 --> 01:11:37,830
not spaces no gravity

1063
01:11:38,910 --> 01:11:41,230
has any weight

1064
01:11:41,320 --> 01:11:44,990
well floating

1065
01:11:46,490 --> 01:11:48,230
i'm going to accelerate

1066
01:11:48,240 --> 01:11:49,520
i rocket

1067
01:11:49,800 --> 01:11:54,400
two accelerated in this direction with acceleration a

1068
01:11:54,410 --> 01:11:57,550
we all perceive now

1069
01:11:57,600 --> 01:12:00,010
perceive gravity in this direction

1070
01:12:00,110 --> 01:12:03,600
i call it g

1071
01:12:03,640 --> 01:12:05,100
so apple will fall

1072
01:12:05,150 --> 01:12:09,610
i'm standing there i see example for i'm in this compartment close for ICT up

1073
01:12:09,610 --> 01:12:12,780
to go down a little later you will be here

1074
01:12:12,870 --> 01:12:16,070
i myself or little later and there

1075
01:12:16,120 --> 01:12:20,300
but i can put it bathroom scale even way myself on the bathroom scales

1076
01:12:20,350 --> 01:12:25,130
my way below and times his a and being my mass a being acceleration i

1077
01:12:25,130 --> 01:12:28,440
really think that is gravity in this direction

1078
01:12:28,500 --> 01:12:33,020
the air once to form

1079
01:12:33,030 --> 01:12:34,930
but the below

1080
01:12:34,950 --> 01:12:37,070
wants to go against gravity

1081
01:12:37,130 --> 01:12:43,530
will rise

1082
01:12:43,740 --> 01:12:45,660
and once to fall

1083
01:12:45,720 --> 01:12:47,180
so inside here

1084
01:12:47,190 --> 01:12:51,520
you create the differential pressure between the bottom

1085
01:12:53,230 --> 01:12:54,940
and the top of the air

1086
01:12:55,200 --> 01:12:57,610
two inside here

1087
01:12:57,610 --> 01:13:00,000
just like the atmosphere on earth

1088
01:13:00,050 --> 01:13:02,410
the atmosphere is pushing down on us

1089
01:13:02,420 --> 01:13:04,850
preciously higher than their

1090
01:13:04,900 --> 01:13:06,550
so we get p one

1091
01:13:06,570 --> 01:13:11,680
it's higher than p two

1092
01:13:11,770 --> 01:13:15,200
so you create yourself an atmosphere

1093
01:13:15,210 --> 01:13:18,740
and the building rise balloon goes in the opposite direction

1094
01:13:21,440 --> 01:13:23,930
if there were no air in there

1095
01:13:23,970 --> 01:13:27,870
then clearly all of us will fall four i would fall

1096
01:13:27,890 --> 01:13:29,470
and balloon form

1097
01:13:29,530 --> 01:13:32,400
the only reason why helium balloon rises

1098
01:13:32,470 --> 01:13:34,280
is because the air

1099
01:13:34,330 --> 01:13:38,010
because you build the differential pressure

1100
01:13:38,030 --> 01:13:41,530
now comes my question to you

1101
01:13:41,580 --> 01:13:44,440
instead of accelerating upwards

1102
01:13:44,440 --> 01:13:48,130
he had the absolute value of logo

1103
01:13:48,150 --> 01:13:52,130
it has to

1104
01:13:52,230 --> 01:13:58,190
so the value is the maximum of logan minus law

1105
01:13:59,710 --> 01:14:00,860
and then i can

1106
01:14:01,270 --> 01:14:06,770
have the maximum log minus the log

1107
01:14:06,810 --> 01:14:10,440
so i can move the log outside the maximum value

1108
01:14:10,460 --> 01:14:12,000
fact that the value

1109
01:14:12,020 --> 01:14:13,310
you can also

1110
01:14:13,310 --> 01:14:18,230
in addition to give this the right could he moved located outside

1111
01:14:18,270 --> 01:14:21,340
but the problem like this to minimize the maximum

1112
01:14:21,360 --> 01:14:26,710
of a cave that both he and one over it would be

1113
01:14:27,270 --> 01:14:32,250
objective about it

1114
01:14:32,290 --> 01:14:35,730
so i did actually compared to the previous problem is

1115
01:14:36,610 --> 01:14:39,150
move the log outside

1116
01:14:39,170 --> 01:14:44,420
thinking and that this

1117
01:14:44,420 --> 01:14:50,110
so it is now it's clear it's convex because we've seen that a maximum of

1118
01:14:50,110 --> 01:14:52,000
convex functions is convex

1119
01:14:52,040 --> 01:14:57,690
so the next few and one over you is convex for positive values of

1120
01:14:57,810 --> 01:15:03,480
so each of these functions h is a convex function in x two

1121
01:15:03,520 --> 01:15:07,810
convex function of it will be and then at the back

1122
01:15:09,380 --> 01:15:11,340
the convex

1123
01:15:11,440 --> 01:15:17,190
so this is actually the only important part here

1124
01:15:17,380 --> 01:15:20,400
this is a convex function

1125
01:15:20,400 --> 01:15:25,230
and then if you get that cpxm up to the code of practice what you

1126
01:15:25,230 --> 01:15:27,310
want to minimize the action so

1127
01:15:27,360 --> 01:15:33,500
matlab commands that the separate your code for modelling tool from

1128
01:15:33,590 --> 01:15:35,810
the rest of the mop up

1129
01:15:35,820 --> 01:15:40,540
and within the scope of the text again and again because the variable x

1130
01:15:40,610 --> 01:15:43,420
of land in europe asia terrible

1131
01:15:43,440 --> 01:15:48,840
he stated that get the function you want to minimize the risk minimized the maximum

1132
01:15:48,880 --> 01:15:52,690
and we write it in a lot of information age times sex

1133
01:15:52,770 --> 01:15:55,840
that's a factor but the value is a key for the

1134
01:15:55,880 --> 01:15:57,670
as a component

1135
01:15:57,690 --> 01:16:03,900
and this is the function in policies one over a x componentwise we at fact

1136
01:16:03,900 --> 01:16:08,710
and then you minimize the max of this

1137
01:16:08,770 --> 01:16:10,980
that's the same

1138
01:16:11,110 --> 01:16:12,940
the this objective

1139
01:16:13,110 --> 01:16:17,820
and they also and the three to expose the fact that one

1140
01:16:17,840 --> 01:16:21,690
and then you enter civics and and then called actually said of the problem of

1141
01:16:21,690 --> 01:16:27,590
it for you and the next

1142
01:16:30,500 --> 01:16:35,590
one thing you might wonder about why today define a function in both an open

1143
01:16:35,590 --> 01:16:40,630
up the page one over it or one of

1144
01:16:40,730 --> 01:16:41,860
mean not

1145
01:16:41,860 --> 01:16:54,980
the be doing here is defined componentwise com componentwise inverse effect

1146
01:16:55,060 --> 01:16:59,190
the reason why the standard model patient doesn't work one of x

1147
01:16:59,230 --> 01:17:03,810
is that wiki that section of the convex function of x

1148
01:17:03,940 --> 01:17:09,000
one over x is convex for positive x but not before i relax

1149
01:17:09,040 --> 01:17:13,110
one of over is the convex function your you and this

1150
01:17:13,130 --> 01:17:17,520
programme but you right one book slash x

1151
01:17:17,560 --> 01:17:20,810
cvx will complain about the conduct problem

1152
01:17:20,860 --> 01:17:24,090
because it's not intelligent enough to know that a x

1153
01:17:24,110 --> 01:17:27,610
it is always positive because the coefficients of a happen to be

1154
01:17:27,650 --> 01:17:29,860
positive and x is positive

1155
01:17:30,060 --> 01:17:35,630
eight positive actually equivalent to only considering the positive convex half of one of the

1156
01:17:37,230 --> 01:17:42,380
just say it that can establish convexity from

1157
01:17:42,430 --> 01:17:44,810
the the context of

1158
01:17:44,810 --> 01:17:48,650
this function is actually a different one and one of x one over x one

1159
01:17:48,650 --> 01:17:50,400
positive x

1160
01:17:50,460 --> 01:17:54,670
and it's not defined for negative

1161
01:17:54,770 --> 01:17:59,230
that's a convex function and then the problem

1162
01:17:59,310 --> 01:18:04,690
so this just to illustrate these modelling tools

1163
01:18:05,170 --> 01:18:06,940
the question

1164
01:18:06,960 --> 01:18:09,420
so there's two things still recognise this

1165
01:18:09,420 --> 01:18:11,840
you excessive variables

1166
01:18:11,880 --> 01:18:14,860
that that's a convex function of x

1167
01:18:14,880 --> 01:18:18,420
and then the second thing is it was set up in some standard for

1168
01:18:18,480 --> 01:18:19,710
and solve for

1169
01:18:19,840 --> 01:18:22,460
and so you don't have to know what sort

1170
01:18:23,770 --> 01:18:34,980
and the second one is to continue

1171
01:18:35,000 --> 01:18:40,460
this on part of different types of convex optimization problems

1172
01:18:40,460 --> 01:18:44,660
same thing for the sake back and forth between the two venues

1173
01:18:44,720 --> 01:18:51,580
it means a might cause as a means they could have a hidden common cause

1174
01:18:51,620 --> 01:18:55,160
the two headed arrow means as causes cell

1175
01:18:55,810 --> 01:19:00,060
i would fact learning that smoking causes lung cancer

1176
01:19:00,110 --> 01:19:02,620
and you can see what i was talking about at the beginning of the talk

1177
01:19:02,620 --> 01:19:07,400
to that there is value in having other variables because once

1178
01:19:07,450 --> 01:19:10,680
i'm able to directly search is this way

1179
01:19:10,690 --> 01:19:14,710
i can direct this one this way and i can that one smoking causes lung

1180
01:19:14,710 --> 01:19:21,370
cancer something i can never learned from data and these two variables alone

1181
01:19:21,410 --> 01:19:22,490
here is

1182
01:19:22,510 --> 01:19:28,400
a real study that actually affect the senator from from carnegie mellon

1183
01:19:28,440 --> 01:19:32,220
CMU but university pittsburgh which

1184
01:19:32,270 --> 01:19:37,450
it's not so interested in the this study done by some followers at CMU and

1185
01:19:37,450 --> 01:19:39,590
actually affected the way CMU

1186
01:19:40,160 --> 01:19:43,310
evaluate students afterwards

1187
01:19:43,360 --> 01:19:47,970
every university is worried about student retention rate

1188
01:19:48,740 --> 01:19:51,660
soon retention means of all the students who who

1189
01:19:51,710 --> 01:19:54,940
university what percentage graduate

1190
01:19:54,950 --> 01:19:56,680
and universities one

1191
01:19:56,700 --> 01:20:00,700
that to be high so they want to make changes to make student retention rate

1192
01:20:00,700 --> 01:20:02,300
is high as possible

1193
01:20:03,420 --> 01:20:05,460
drugs to we more

1194
01:20:05,480 --> 01:20:07,030
the study

1195
01:20:07,110 --> 01:20:10,330
and originally had more variables than others

1196
01:20:10,340 --> 01:20:11,700
based on data

1197
01:20:13,200 --> 01:20:16,010
thousands of university of trees

1198
01:20:16,060 --> 01:20:17,690
to see what

1199
01:20:17,720 --> 01:20:20,270
variables affected with

1200
01:20:20,280 --> 01:20:21,970
one we care about this one

1201
01:20:22,850 --> 01:20:25,590
this is that this is the variable which are about

1202
01:20:25,600 --> 01:20:26,600
they looked at

1203
01:20:26,610 --> 01:20:30,330
the fraction of the applicants were not offered admission

1204
01:20:30,340 --> 01:20:35,720
this that and then look at the average standardized test scores you're SAT's scores

1205
01:20:35,770 --> 01:20:38,690
if you look at the fraction of incoming students the top ten percent of the

1206
01:20:38,690 --> 01:20:40,430
high school class

1207
01:20:40,480 --> 01:20:44,390
fraction of students who accept the offer

1208
01:20:44,390 --> 01:20:49,520
the educational general expenses per student the student faculty ratio is all things you think

1209
01:20:49,520 --> 01:20:53,130
might be take a small student faculty ratio might help

1210
01:20:53,170 --> 01:20:57,910
certainly the professors were hoping this would be the important variable rates so they can

1211
01:20:57,910 --> 01:20:59,860
live for a

1212
01:20:59,900 --> 01:21:02,920
one when they find

1213
01:21:02,960 --> 01:21:08,270
this is a causal DAG they learned court that actually might cause causal graphs they

1214
01:21:09,060 --> 01:21:11,250
using my notation were

1215
01:21:11,270 --> 01:21:17,450
two hundred zero means cause an error like this means the hidden common cause

1216
01:21:17,480 --> 01:21:21,030
it was on one hand there are none here

1217
01:21:21,090 --> 01:21:25,620
there's no one hundred one idea would mean what i told the problem is

1218
01:21:25,640 --> 01:21:29,090
that's right

1219
01:21:29,130 --> 01:21:30,890
well i don't see it

1220
01:21:30,900 --> 01:21:33,910
that that's an rather than just due japan

1221
01:21:36,810 --> 01:21:41,010
and so what is alpha here

1222
01:21:41,070 --> 01:21:45,520
alpha is when you're on the right side we assume you know the conditional independencies

1223
01:21:45,690 --> 01:21:47,150
alpha is used to

1224
01:21:47,160 --> 01:21:52,800
determine the conditional independencies in determine whether to reject the conditional dependency

1225
01:21:52,860 --> 01:21:56,470
this is the significance level at which you rejected

1226
01:21:56,530 --> 01:21:58,480
so help is point

1227
01:21:59,420 --> 01:22:04,280
it means you reject conditional independency very readily all you need is to have an

1228
01:22:04,360 --> 01:22:06,360
and result

1229
01:22:06,380 --> 01:22:10,350
the point two range to reject alpha point zero one

1230
01:22:10,470 --> 01:22:14,250
if the conditional independences present very rare event

1231
01:22:14,300 --> 01:22:16,720
it's happened so you don't reject it

1232
01:22:17,810 --> 01:22:22,750
so for the smaller alpha in the more sparse graph is because

1233
01:22:23,890 --> 01:22:29,920
you're you're not rejecting conditional independencies is easily rejecting them very easily so there's lot

1234
01:22:29,920 --> 01:22:31,080
of edges here

1235
01:22:31,130 --> 01:22:35,000
with a lot you to reject them so there aren't as many edges to make

1236
01:22:37,340 --> 01:22:41,590
called what you learn is more reliable right because

1237
01:22:41,770 --> 01:22:46,320
i noticed when i was trying to learning what variables affect the size another break

1238
01:22:46,320 --> 01:22:48,770
with variables affected the

1239
01:22:48,780 --> 01:22:50,360
stock markets

1240
01:22:52,640 --> 01:22:56,530
they are i can learn a lot was making up really small is hard time

1241
01:22:56,550 --> 01:22:59,220
learning anything

1242
01:22:59,240 --> 01:23:02,980
anyway so at all levels of alpha

1243
01:23:02,990 --> 01:23:03,990
the only

1244
01:23:04,010 --> 01:23:09,210
variable that they found it had in the fact that graduation rate was test score

1245
01:23:09,220 --> 01:23:11,410
these are hidden common causes

1246
01:23:11,450 --> 01:23:12,420
you know it's

1247
01:23:12,430 --> 01:23:15,310
it's always the test score

1248
01:23:15,350 --> 01:23:20,280
in other words what this study indicate is that good in good out

1249
01:23:20,340 --> 01:23:24,330
you bring in students with high SAT's cause more gradually you bring students of lower

1250
01:23:24,330 --> 01:23:26,340
city schools they will gradually

1251
01:23:26,390 --> 01:23:29,470
it's about university wants here right they want to hear that you know they can

1252
01:23:30,230 --> 01:23:32,560
university better by

1253
01:23:32,570 --> 01:23:36,500
i think there will be more and the this seems to be about the study

1254
01:23:36,500 --> 01:23:41,510
indicates after that kind of don't change their this financial aid policy and they get

1255
01:23:41,510 --> 01:23:43,900
more financially based on

1256
01:23:43,950 --> 01:23:48,810
city square rather than need and in fact the graduation rate went up

1257
01:23:48,860 --> 01:23:51,050
after that

1258
01:23:51,060 --> 01:23:55,070
so i guess say can't be sure of causal influences but they

1259
01:23:55,120 --> 01:23:58,300
this is a reasonable results so it helps

1260
01:23:58,360 --> 01:24:01,470
people kind of mound believe that this

1261
01:24:01,490 --> 01:24:06,810
well in fact really important variable in this is the one they changed

1262
01:24:06,820 --> 01:24:11,030
now here's the study i talked about earlier with myself and more we're trying to

1263
01:24:12,980 --> 01:24:16,250
what variables said and the fact that

1264
01:24:16,300 --> 01:24:20,680
and racial racial harassment and we originally had more than five variables just like there

1265
01:24:20,690 --> 01:24:22,700
are more than eight but we know these

1266
01:24:22,760 --> 01:24:27,590
i mean say dominated by we know these five to be the important one

1267
01:24:29,140 --> 01:24:32,390
respondents races based on survey data of

1268
01:24:32,400 --> 01:24:35,190
about six thousand individuals in the military

1269
01:24:35,250 --> 01:24:38,220
respondents years of military service

1270
01:24:38,270 --> 01:24:39,990
whether respondent

1271
01:24:41,510 --> 01:24:44,360
he or she experienced a racial incident

1272
01:24:44,380 --> 01:24:47,900
whether the incident was reported to military

1273
01:24:47,920 --> 01:24:53,080
and whether the respondent held the military responsible for the incident

1274
01:24:53,630 --> 01:25:07,470
no i don't know i never heard

1275
01:25:07,500 --> 01:25:09,170
why should a

1276
01:25:10,640 --> 01:25:18,990
based on that indicate hidden common cause these two don't indicate anything at all

1277
01:25:19,050 --> 01:25:22,330
i showed this to the dean my own institution that you don't want to hear

1278
01:25:22,330 --> 01:25:26,820
that lawmakers should justify so why do we have teams in colleges i mean i

1279
01:25:26,820 --> 01:25:32,900
always felt that we can save money by not having justify their existence by looking

1280
01:25:32,900 --> 01:25:40,400
for solutions to these problems other than other than this one

1281
01:25:40,420 --> 01:25:41,550
here is

1282
01:25:41,560 --> 01:25:45,380
here is

1283
01:25:45,490 --> 01:25:46,890
what we want

1284
01:25:46,900 --> 01:25:47,800
all right

1285
01:25:47,850 --> 01:25:52,590
we learn that the only variable that has a causal effect on race

1286
01:25:52,640 --> 01:25:54,110
this is only doubled

1287
01:25:54,120 --> 01:25:55,670
that's there was

1288
01:25:55,700 --> 01:25:59,950
whether respondent held the military responsible one was we learn this at all levels of

1289
01:25:59,950 --> 01:26:03,830
significance just like the green one

1290
01:26:03,870 --> 01:26:07,730
as i said before and it doesn't make sense to hold the military responsible can

1291
01:26:07,730 --> 01:26:09,840
change your right

1292
01:26:09,850 --> 01:26:13,470
now why did we learn the first try and has tried to how this could

1293
01:26:13,470 --> 01:26:15,260
happen and i just

1294
01:26:15,310 --> 01:26:20,830
i came up with the possible scenario for why it happened

1295
01:26:20,920 --> 01:26:23,180
the causal influence of

1296
01:26:23,190 --> 01:26:27,510
responsible on race would not be learned if there was a direct causal influence of

1297
01:26:27,510 --> 01:26:29,500
race and incident

1298
01:26:29,510 --> 01:26:33,030
which is why we were looking for right that's what you suspect in the domain

1299
01:26:33,030 --> 01:26:36,340
i was

1300
01:26:54,190 --> 01:27:00,150
find little that they

1301
01:27:01,660 --> 01:27:05,380
there are

1302
01:27:09,040 --> 01:27:12,850
or is

1303
01:27:25,450 --> 01:27:27,120
there are

1304
01:27:28,460 --> 01:27:33,750
here all

1305
01:27:48,820 --> 01:27:54,180
it may in more

1306
01:27:57,520 --> 01:27:58,920
well you

1307
01:28:11,230 --> 01:28:14,380
rather than

1308
01:28:31,860 --> 01:28:39,010
as we well

1309
01:28:48,300 --> 01:28:52,420
we call this

1310
01:29:02,380 --> 01:29:06,170
well is is

1311
01:29:12,980 --> 01:29:15,640
and we

1312
01:29:21,910 --> 01:29:24,660
all the

1313
01:29:37,070 --> 01:29:41,270
you have express who

1314
01:29:43,350 --> 01:29:46,120
it is

1315
01:29:46,270 --> 01:29:50,570
and if you don't have

1316
01:30:08,250 --> 01:30:12,410
one hundred

1317
01:30:32,490 --> 01:30:35,950
i think we have

1318
01:30:43,800 --> 01:30:45,140
what you

1319
01:30:45,150 --> 01:30:46,390
all right

1320
01:30:55,070 --> 01:31:00,680
i mean it

1321
01:31:01,110 --> 01:31:03,420
all the

1322
01:31:03,440 --> 01:31:07,390
he is that you know

1323
01:31:07,650 --> 01:31:10,850
all right

1324
01:31:13,910 --> 01:31:17,210
one theory

1325
01:31:18,870 --> 01:31:21,130
when very well

1326
01:31:21,150 --> 01:31:25,590
but how of the whole

1327
01:31:30,650 --> 01:31:36,530
he will be

1328
01:31:42,910 --> 01:31:48,880
one of the

1329
01:31:53,800 --> 01:31:55,930
you want your

1330
01:31:56,020 --> 01:31:59,220
i more

1331
01:32:00,920 --> 01:32:03,980
you need to

1332
01:32:03,980 --> 01:32:06,330
distribution of building material

1333
01:32:06,350 --> 01:32:07,740
the over time

1334
01:32:07,810 --> 01:32:11,850
you end up with this kind of waif-like distribution which is supposed to correspond to

1335
01:32:12,980 --> 01:32:16,550
OK equally spaced pillars

1336
01:32:16,620 --> 01:32:22,720
and that's just through a one that's measuring the one dimensional space and the diffusion

1337
01:32:22,720 --> 01:32:23,560
of pheromone

1338
01:32:24,000 --> 01:32:30,020
termites and building material and another pheromone and how those the coupling of those partial

1339
01:32:30,020 --> 01:32:35,480
differential equations leads to this kind of equity-based colour building

1340
01:32:35,680 --> 01:32:42,200
here's a model in two dimensions the queen is sitting in the middle and she's

1341
01:32:42,200 --> 01:32:48,500
giving off a pheromone at some concentration that pheromone the termites like to build so

1342
01:32:48,500 --> 01:32:51,700
they're building a kind of roughly circular

1343
01:32:51,800 --> 01:32:54,140
wall around the queen

1344
01:32:54,220 --> 01:33:01,280
earlier this situation where that is happening on top of it you've got a stream

1345
01:33:01,280 --> 01:33:04,050
of termites crossing the world

1346
01:33:04,160 --> 01:33:08,120
in this direction and a stream of termites crossing the world in this dimension

1347
01:33:08,130 --> 01:33:14,550
and they're leaving they're laying down a trail pheromone which inhibits building activity

1348
01:33:14,590 --> 01:33:18,550
so now what this is supposed to indicate is too

1349
01:33:18,560 --> 01:33:21,380
two tunnels meeting at an intersection

1350
01:33:22,200 --> 01:33:27,880
because they're not explicitly modelling the third dimension they can't actually model the

1351
01:33:27,920 --> 01:33:29,880
the become cavity

1352
01:33:33,340 --> 01:33:37,660
we've got these models were were unsatisfactory in several respects

1353
01:33:38,060 --> 01:33:41,980
the the the fact that they didn't model the third dimension was was one of

1354
01:33:42,870 --> 01:33:47,500
the the main thing is the basic model model the physicality of the building material

1355
01:33:47,570 --> 01:33:54,530
so it's in this situation here although they've built a ring of building material termites

1356
01:33:54,530 --> 01:33:57,350
and pheromones are free to diffuse through it

1357
01:33:57,370 --> 01:34:03,780
OK it doesn't obstruct anything surely the the complication of the situation faced by the

1358
01:34:03,780 --> 01:34:06,150
termites as they

1359
01:34:06,160 --> 01:34:08,120
carry out the building activity

1360
01:34:08,180 --> 01:34:13,220
that's going to influence the flow of pheromone which is suppose to be guiding them

1361
01:34:13,220 --> 01:34:16,530
that's going to influence subsequent building activity which is gonna feed back on the flow

1362
01:34:16,540 --> 01:34:18,480
of pheromone that's where the

1363
01:34:18,610 --> 01:34:19,980
problem is right

1364
01:34:19,980 --> 01:34:23,780
it's relatively easy to show that a diffusing pheromone is going to result in a

1365
01:34:23,780 --> 01:34:29,740
circular structure if the if the building of the structure doesn't impede the pheromone

1366
01:34:29,760 --> 01:34:32,570
so in order to look at that and that's so that's what i mean by

1367
01:34:32,570 --> 01:34:34,310
logistics OK

1368
01:34:34,390 --> 01:34:37,720
what i mean is there are things that get in the way of this coming

1369
01:34:37,720 --> 01:34:38,810
from here to here

1370
01:34:39,020 --> 01:34:43,280
these individuals are over here and so they didn't get this message

1371
01:34:43,280 --> 01:34:45,580
etcetera etcetera local

1372
01:34:48,800 --> 01:34:52,080
so our model looked like this the simple three d lattice

1373
01:34:52,280 --> 01:34:54,770
just about rectangle rectangular lattice

1374
01:34:54,800 --> 01:34:57,720
a finite number of termites each modelled individually

1375
01:34:57,870 --> 01:34:59,980
pheromone diffusion

1376
01:35:00,200 --> 01:35:06,830
and termite movement were physically constrained modelled pheromone diffusion just using a standard sort of

1377
01:35:06,830 --> 01:35:09,670
fluid mechanics type model

1378
01:35:09,840 --> 01:35:15,360
and we also imposed a kind of crude physics that in which was supposed to

1379
01:35:15,360 --> 01:35:20,600
report prevent impossible structures so

1380
01:35:20,610 --> 01:35:24,310
termites were not allowed to build unsupported horizontal

1381
01:35:24,460 --> 01:35:26,240
structures for example

1382
01:35:26,320 --> 01:35:30,040
those who assumed this fall fall down

1383
01:35:30,160 --> 01:35:34,220
but we do that in any particularly sophisticated way so here in the middle of

1384
01:35:34,220 --> 01:35:36,100
the world is the queen

1385
01:35:36,380 --> 01:35:39,570
she gives off a pheromone which diffuses away from

1386
01:35:39,980 --> 01:35:46,000
and around that at some level of pheromone concentration but the termites

1387
01:35:48,360 --> 01:35:54,260
but they're compelled to build the probability building is increased at this particular intensity of

1388
01:35:54,260 --> 01:35:58,800
pheromone and you can see that the which started to build towers

1389
01:35:58,840 --> 01:36:04,960
pillars sort of characteristic distance from the queen and the shape

1390
01:36:05,000 --> 01:36:06,210
but i don't know

1391
01:36:06,500 --> 01:36:11,880
line these pillars it reflects the for the shape of the queen

1392
01:36:12,250 --> 01:36:17,840
so the reason that they build pillars is that when they deposit a piece of

1393
01:36:17,840 --> 01:36:21,210
building material they basically have chewed it up

1394
01:36:21,230 --> 01:36:23,210
and most and they spit out

1395
01:36:23,230 --> 01:36:25,020
and stick it to the floor

1396
01:36:25,020 --> 01:36:28,670
and the but they use gives off a pheromone

1397
01:36:28,690 --> 01:36:35,270
a building sort of building material pheromone which attracts termites

1398
01:36:35,280 --> 01:36:36,500
towards it

1399
01:36:36,630 --> 01:36:41,720
so as they start building they recruit termites to that building sites

1400
01:36:41,730 --> 01:36:43,380
and because the

1401
01:36:43,380 --> 01:36:46,280
that was a good reason to put a building block there in the first place

1402
01:36:46,720 --> 01:36:50,380
there's also a good reason to build on it and you find the termites compelled

1403
01:36:50,380 --> 01:36:51,740
to add two

1404
01:36:51,750 --> 01:36:55,150
existing building activity

1405
01:36:55,190 --> 01:36:57,630
and in fact the the

1406
01:36:57,640 --> 01:37:04,720
when you run this with larger grid sizes the relatively equal spacing of the pillars

1407
01:37:04,730 --> 01:37:09,400
you can see that you can sort of understand the pillars is competing for termites

1408
01:37:09,400 --> 01:37:11,720
but the bigger pillars give off more

1409
01:37:11,730 --> 01:37:18,360
building pheromone recruit more termites and the positive feedback there so any pillar that starts

1410
01:37:19,040 --> 01:37:25,090
sort of a small pillar which is it's impossible or

1411
01:37:25,100 --> 01:37:32,740
that inhibits other new pillars from starting close to existing pillars sort

1412
01:37:32,780 --> 01:37:39,960
so over time you see that the low wall form

1413
01:37:40,000 --> 01:37:44,730
and indeed eventually becomes the think enclosed concave dome

1414
01:37:44,860 --> 01:37:46,980
which had been demonstrated in the

1415
01:37:47,040 --> 01:37:48,780
in the math model

1416
01:37:48,840 --> 01:37:52,960
and the fact that had this is happening in the pheromone diffusion is being influenced

1417
01:37:52,960 --> 01:37:56,760
by the building activity doesn't prevent this from occurring

1418
01:37:57,650 --> 01:38:02,670
in the math model they also simulate situation where wind blows across the plain so

1419
01:38:02,670 --> 01:38:03,880
we did that too

1420
01:38:03,880 --> 01:38:05,230
let's assume

1421
01:38:05,270 --> 01:38:06,940
the posterior

1422
01:38:06,960 --> 01:38:11,690
is a multivariate gaussian so it takes at a specific functional for

1423
01:38:11,750 --> 01:38:13,560
you know its approximate

1424
01:38:14,670 --> 01:38:18,340
but it's the form that week it's an analytic form is one that we

1425
01:38:18,440 --> 01:38:20,920
you know we know how to handle it

1426
01:38:21,020 --> 01:38:24,040
and this is called the

1427
01:38:24,090 --> 01:38:27,500
the laplace approximation of the saddle point approximation

1428
01:38:27,610 --> 01:38:29,540
basically what we do

1429
01:38:29,580 --> 01:38:30,940
israel assume the

1430
01:38:30,960 --> 01:38:33,810
posterior this knowledge to posterior

1431
01:38:33,900 --> 01:38:38,400
actually takes an analytic form which is a multivariate gaussian

1432
01:38:38,400 --> 01:38:42,610
and a multivariate gaussian with the mean value

1433
01:38:42,630 --> 01:38:47,190
it is located at the maximum of the actual posterior

1434
01:38:47,310 --> 01:38:49,520
and the covariance matrix

1435
01:38:49,540 --> 01:38:51,630
there's not going to be proportional

1436
01:38:51,670 --> 01:38:54,360
to the curvature of the posterior

1437
01:38:54,440 --> 01:39:02,790
at the maximum point

1438
01:39:02,790 --> 01:39:04,110
so in other words

1439
01:39:04,110 --> 01:39:05,460
but will do

1440
01:39:05,520 --> 01:39:07,460
it is we find

1441
01:39:07,480 --> 01:39:09,540
the set of parameters w

1442
01:39:09,560 --> 01:39:12,860
that maximises the posterior

1443
01:39:12,880 --> 01:39:13,920
and we use

1444
01:39:14,400 --> 01:39:18,560
a covariance matrix as i said which is proportional to the curvature

1445
01:39:18,560 --> 01:39:19,880
of the posterior

1446
01:39:19,900 --> 01:39:22,340
or the curvature of the joint likelihood

1447
01:39:22,380 --> 01:39:24,320
now this should be familiar to you

1448
01:39:24,670 --> 01:39:26,690
from yesterday

1449
01:39:26,750 --> 01:39:29,940
so that's basically all that we have to do we need to do some sort

1450
01:39:29,940 --> 01:39:31,880
of search

1451
01:39:31,900 --> 01:39:33,020
one of the

1452
01:39:34,380 --> 01:39:36,520
this smart value

1453
01:39:36,610 --> 01:39:39,940
and once we have that we need to find an analytic four

1454
01:39:41,150 --> 01:39:44,580
for this covariance matrix here

1455
01:39:44,590 --> 01:39:48,400
and then we're done well we're not really done because we still have to make

1456
01:39:48,400 --> 01:39:52,040
predictions and we'll see this still doesn't solve the problem

1457
01:39:52,110 --> 01:39:56,150
but basically what we're doing is we're seeing our posterior

1458
01:39:57,150 --> 01:40:01,130
so we have this non gaussian likelihood goes in

1459
01:40:01,820 --> 01:40:05,900
and this no ngos in marginal likelihood which is non analytic it's all going to

1460
01:40:05,900 --> 01:40:08,150
be approximated by goes in

1461
01:40:08,170 --> 01:40:09,980
center the wmap

1462
01:40:10,000 --> 01:40:13,630
with the covariance c which we've previously defined

1463
01:40:13,630 --> 01:40:15,980
so how do we estimate the maximum of the

1464
01:40:17,520 --> 01:40:19,380
and also compute the curvature

1465
01:40:19,420 --> 01:40:22,400
of the posterior at that point

1466
01:40:24,460 --> 01:40:28,460
we need to find the maximum of the posterior

1467
01:40:28,480 --> 01:40:32,480
so to find the maximum of the posterior although we need to do is find

1468
01:40:32,480 --> 01:40:35,000
the maximum of the joint likelihood y

1469
01:40:35,040 --> 01:40:41,040
because the marginal distribution is independent of the parameters that we're interested in

1470
01:40:41,090 --> 01:40:42,940
so again if we take

1471
01:40:43,150 --> 01:40:47,060
joint distribution and take the log of that then we end up with this this

1472
01:40:47,080 --> 01:40:48,480
thing here OK

1473
01:40:48,540 --> 01:40:52,580
so here's the component from the likelihood he's the component from the

1474
01:40:52,580 --> 01:40:53,650
the prior

1475
01:40:53,670 --> 01:40:58,460
again those of you familiar with regularisation methods you see this is a loss function

1476
01:40:58,500 --> 01:41:01,980
and this is the the regularisation term

1477
01:41:02,000 --> 01:41:05,580
so we do simple optimisation because i'm running out of time

1478
01:41:06,040 --> 01:41:09,250
for those of you who are really into the the the the details

1479
01:41:09,650 --> 01:41:15,730
the to the first derivative of of the likelihood committed in this form here

1480
01:41:15,770 --> 01:41:20,880
so these are target values p is a vector of the probability

1481
01:41:20,900 --> 01:41:26,110
of the class label taking on the value one given of attributes

1482
01:41:26,130 --> 01:41:29,670
and this is just the matrix of the basis

1483
01:41:32,610 --> 01:41:35,230
and this is the residual from the

1484
01:41:35,290 --> 01:41:37,270
the prior

1485
01:41:37,290 --> 01:41:39,250
now of course we also need

1486
01:41:39,270 --> 01:41:44,500
the curvature so we also need the second the matrix of second derivatives

1487
01:41:44,500 --> 01:41:46,590
to define the covariance

1488
01:41:47,610 --> 01:41:51,690
if we just do some multivariate action because i'm running out of time

1489
01:41:52,210 --> 01:41:56,080
for those of you who are really into the the the details

1490
01:41:56,110 --> 01:42:02,210
the to the first derivative of the likelihood can be written in this form here

1491
01:42:02,230 --> 01:42:07,340
so these are target values p is a vector of the probability

1492
01:42:07,360 --> 01:42:12,590
of the class label taking on the value one given of attributes

1493
01:42:12,610 --> 01:42:15,290
and this is just the matrix of the

1494
01:42:15,310 --> 01:42:16,170
the basis

1495
01:42:19,060 --> 01:42:21,710
and this is the residual from the

1496
01:42:21,750 --> 01:42:23,730
the prior

1497
01:42:23,750 --> 01:42:25,730
now of course we also need

1498
01:42:25,750 --> 01:42:30,940
the curvature so we also need the second the matrix of second derivatives

1499
01:42:30,960 --> 01:42:33,060
to define the covariance

1500
01:42:34,080 --> 01:42:41,520
if we just do some multivariate differentiation we then find that our

1501
01:42:41,540 --> 01:42:44,460
matrix of second derivatives look something like this

1502
01:42:44,480 --> 01:42:47,560
for this matrix here is diagonal

1503
01:42:48,020 --> 01:42:50,040
and the diagonal components

1504
01:42:50,610 --> 01:42:55,770
basically the data in terms of the of the binomial

1505
01:42:55,790 --> 01:42:59,250
OK so is the product of PC because one times the product

1506
01:42:59,310 --> 01:43:03,000
the probability of sequels zero one minus people

1507
01:43:04,460 --> 01:43:06,900
of sequels one

1508
01:43:06,900 --> 01:43:08,340
and of course we have this

1509
01:43:08,380 --> 01:43:12,980
regularizing ten here so this residual component due to the

1510
01:43:13,020 --> 01:43:13,960
the prior

1511
01:43:13,980 --> 01:43:15,710
so we are done

1512
01:43:15,750 --> 01:43:19,210
on the other we need to do is used so so there is the covariance

1513
01:43:20,670 --> 01:43:22,150
of approximation

1514
01:43:22,230 --> 01:43:27,190
we started with don and again and again

1515
01:43:27,210 --> 01:43:29,380
so it

1516
01:43:29,420 --> 01:43:33,360
the MAP values well we need to use some sort of

1517
01:43:33,360 --> 01:43:36,210
optimisation methods

1518
01:43:38,190 --> 01:43:42,520
we all remember newton's root finding methods from school

1519
01:43:42,540 --> 01:43:44,190
the multivariate version

1520
01:43:44,210 --> 01:43:45,420
is basically

1521
01:43:45,440 --> 01:43:48,360
we want to find the roots of

1522
01:43:48,420 --> 01:43:52,880
the stationary points of

1523
01:43:52,880 --> 01:43:54,940
the score function here

1524
01:43:54,960 --> 01:43:56,190
and so

1525
01:43:56,210 --> 01:43:58,860
we just use

1526
01:43:58,860 --> 01:44:00,770
the second order

1527
01:44:00,820 --> 01:44:03,080
the message here

1528
01:44:03,090 --> 01:44:04,610
so we can plug in

1529
01:44:04,610 --> 01:44:07,540
the values of the gradient and the hessian

1530
01:44:07,590 --> 01:44:09,730
and again do some

1531
01:44:09,820 --> 01:44:12,040
some simple algebra and you end up

1532
01:44:12,060 --> 01:44:14,150
with far and iteration

1533
01:44:14,170 --> 01:44:17,250
so we don't have this is not in closed form

1534
01:44:17,270 --> 01:44:19,460
because w appears

1535
01:44:20,290 --> 01:44:21,420
and also

1536
01:44:21,460 --> 01:44:26,190
appears in the elements of p and it appears in the elements of e

1537
01:44:26,230 --> 01:44:28,690
so we basically have this

1538
01:44:28,690 --> 01:44:33,060
is which you can think of the compelled compatibility score

1539
01:44:33,080 --> 01:44:36,470
of a given observation and labels

1540
01:44:36,490 --> 01:44:40,370
so we're going to look at the compatibility score of the correct

1541
01:44:40,380 --> 01:44:43,150
correct observation labelled pairs

1542
01:44:43,200 --> 01:44:48,290
and we're going to look at the the labelled pair the deal label

1543
01:44:48,300 --> 01:44:52,360
that is not the correct one but this it is the maximum score with respect

1544
01:44:52,360 --> 01:44:53,010
to the

1545
01:44:54,850 --> 01:44:59,340
we suspect our current classifier so we define the margin to be the distance between

1546
01:44:59,340 --> 01:45:04,110
the case and n is that is before we're we're going to try to maximize

1547
01:45:04,110 --> 01:45:06,870
the minimum of these margins

1548
01:45:06,890 --> 01:45:09,180
so if we look at the graphical

1549
01:45:09,190 --> 01:45:14,420
representation of this problem what do we have let's look at one of the things

1550
01:45:14,420 --> 01:45:15,850
that x i

1551
01:45:15,900 --> 01:45:21,780
and for each x i we have many labels many possible label assignments

1552
01:45:21,800 --> 01:45:27,600
and here with the rank tells us that the are confident

1553
01:45:27,620 --> 01:45:29,950
our confidence on each pair

1554
01:45:30,930 --> 01:45:34,500
if x x i y i which is the correct pair

1555
01:45:34,510 --> 01:45:39,530
which is the correct label for example it is it a higher score

1556
01:45:39,570 --> 01:45:44,150
then all the other possible labels we have the correct classifications

1557
01:45:44,170 --> 01:45:48,670
and the the the margin is given by this and if this margin is larger

1558
01:45:48,670 --> 01:45:53,000
than what we have satisfied the margin constraints and if it is not the case

1559
01:45:53,000 --> 01:45:54,180
we have

1560
01:45:54,190 --> 01:45:56,480
satisfied the margin constraints

1561
01:45:56,490 --> 01:46:02,950
an alternative example is an alternative situation is when the correct label to do correct

1562
01:46:02,950 --> 01:46:05,710
label for for an observation

1563
01:46:05,970 --> 01:46:10,360
it is the lower confidence score than then some other

1564
01:46:10,610 --> 01:46:16,200
some other label assignments for the instance then we have the negative margin and we

1565
01:46:16,200 --> 01:46:19,850
have we need to do something about this

1566
01:46:21,960 --> 01:46:23,910
so we have the margin

1567
01:46:24,090 --> 01:46:30,190
the definition again and the confidence scores again the inner product between some feature representation

1568
01:46:31,020 --> 01:46:35,310
x y parents which they talked about in detail

1569
01:46:35,320 --> 01:46:36,650
and as before

1570
01:46:36,660 --> 01:46:42,230
are the this this problem can can be stated in terms of the quadratic programme

1571
01:46:42,250 --> 01:46:44,300
so and l two norms

1572
01:46:44,310 --> 01:46:48,980
we we try to minimize the l two norm of our hyperplane plus some slack

1573
01:46:49,980 --> 01:46:51,420
that allows us to

1574
01:46:51,430 --> 01:46:56,290
violate our constraints margin constraints which is given here

1575
01:46:56,300 --> 01:46:59,120
and this is exactly equivalent to here

1576
01:46:59,200 --> 01:47:04,330
so for each data instance we want the most compatible want to be larger than

1577
01:47:04,330 --> 01:47:06,730
one plus some slack

1578
01:47:07,020 --> 01:47:14,450
and if we want to look at this this problem or

1579
01:47:14,800 --> 01:47:20,610
and again in a graphical setting with the slack variables

1580
01:47:21,750 --> 01:47:24,200
the slack variables are going to be

1581
01:47:24,210 --> 01:47:27,830
just because of the definition of hard margin constraints

1582
01:47:27,850 --> 01:47:30,130
we're going to have a slack variable

1583
01:47:30,140 --> 01:47:35,230
four for a label that is that is that that is the most competitive to

1584
01:47:35,230 --> 01:47:39,900
the correct one so the one that achieves the maximum score

1585
01:47:39,950 --> 01:47:43,890
or that achieves the minimum of this distance

1586
01:47:45,740 --> 01:47:47,010
so let's see how

1587
01:47:47,020 --> 01:47:47,950
we can

1588
01:47:47,960 --> 01:47:49,710
visualize the

1589
01:47:49,730 --> 01:47:56,300
r compatibility score is an inner product of our feature representation in our hyperplane are

1590
01:47:56,340 --> 01:47:57,740
weight vectors

1591
01:47:57,750 --> 01:48:03,870
and the slides are slack variables can in fact be stated like this it is

1592
01:48:03,890 --> 01:48:07,000
it is either zero it is the max of

1593
01:48:08,160 --> 01:48:09,940
or one minus

1594
01:48:09,950 --> 01:48:11,610
our separation

1595
01:48:14,550 --> 01:48:16,520
here i'm showing

1596
01:48:16,810 --> 01:48:22,570
i have two examples here x i two training instances x i x j

1597
01:48:22,580 --> 01:48:25,080
so for example i am using green

1598
01:48:25,750 --> 01:48:28,890
the color coding for sixty red one

1599
01:48:28,900 --> 01:48:30,750
and for each observation

1600
01:48:32,240 --> 01:48:37,740
this is a structured observation obviously we can have many many possible labels that y

1601
01:48:37,740 --> 01:48:40,650
is the correct one of four

1602
01:48:40,890 --> 01:48:46,560
and we can have all these other incorrect labels so i'm going to use a

1603
01:48:48,560 --> 01:48:54,200
representation for the correct pairs in the circle for the incorrect one

1604
01:48:55,160 --> 01:48:57,850
i can think of this operation is the projection

1605
01:48:57,870 --> 01:49:05,920
after the feature representation and onto the onto my hyperplane and if

1606
01:49:05,940 --> 01:49:10,360
if the score is larger than all the correct once again

1607
01:49:11,390 --> 01:49:12,120
i have

1608
01:49:12,140 --> 01:49:13,540
i have no

1609
01:49:13,590 --> 01:49:18,760
correct classifications but this is not enough for me i want the most competitive the

1610
01:49:18,760 --> 01:49:22,340
difference between the most competitive want to be larger than one

1611
01:49:22,360 --> 01:49:24,420
right so in this case

1612
01:49:24,440 --> 01:49:29,220
these slack variables xj is this difference is larger than one

1613
01:49:29,300 --> 01:49:33,380
them also the slack variable is going to be zero if it's not larger than

1614
01:49:33,380 --> 01:49:37,250
one then i'll have to be penalized in my loss function

1615
01:49:37,270 --> 01:49:43,750
in i the situation is different because the correct one is ranked actually lower than

1616
01:49:43,750 --> 01:49:45,020
than the

1617
01:49:46,530 --> 01:49:48,400
then some other labels

1618
01:49:48,470 --> 01:49:51,600
but i'm going to be only analyse for the one

1619
01:49:51,620 --> 01:49:53,450
that use the max

1620
01:49:53,450 --> 01:49:54,520
of misery

1621
01:49:54,570 --> 01:49:59,550
well what you perceive is orange or yellow depends very much on context just as

1622
01:49:59,560 --> 01:50:02,550
what you see in the face depends on whether it's upside down and so it's

1623
01:50:02,550 --> 01:50:06,520
actually pretty hard to division system and i don't want to get so what i

1624
01:50:06,520 --> 01:50:10,950
did instead was i went to the supermarket also later oranges and lemons and i

1625
01:50:10,950 --> 01:50:13,160
measured them with the ruler so

1626
01:50:13,220 --> 01:50:17,480
i i did actually do this i sat down with the real and i measure

1627
01:50:17,480 --> 01:50:20,980
the width and height of a whole bunch of oranges and then we can we

1628
01:50:20,980 --> 01:50:24,980
can but now we turn it into numbers numbers is something i computer can deal

1629
01:50:24,980 --> 01:50:26,090
with say

1630
01:50:26,100 --> 01:50:27,590
in particular

1631
01:50:27,630 --> 01:50:29,500
i've got

1632
01:50:30,870 --> 01:50:32,660
the matrix text

1633
01:50:35,190 --> 01:50:38,550
one night in the street is going to have a height

1634
01:50:38,560 --> 01:50:40,980
and with the

1635
01:50:40,990 --> 01:50:48,700
and then i'll have to that one item for another one would have another

1636
01:50:48,750 --> 01:50:52,250
so i can have an

1637
01:50:52,350 --> 01:50:55,610
and then i'm going to have a bunch of columns for different features are measured

1638
01:50:55,620 --> 01:50:57,930
and i just happened to measure two features

1639
01:50:58,000 --> 01:51:02,410
OK and then as well as these input x and has the outputs y so

1640
01:51:02,410 --> 01:51:06,080
why is going to be a factor which will feel like and that will have

1641
01:51:06,080 --> 01:51:09,560
some labels that will be the first ones and minus one

1642
01:51:09,580 --> 01:51:13,960
saying whether it's an orange whether it's true so then once again the matrix and

1643
01:51:13,960 --> 01:51:17,250
vector that's something i can shove into a computer

1644
01:51:17,360 --> 01:51:18,880
and i can also

1645
01:51:18,940 --> 01:51:23,100
obviously draw plot so we can look at this and we can see that there's

1646
01:51:23,100 --> 01:51:27,520
obviously some structure in the data

1647
01:51:31,310 --> 01:51:36,960
not liking this example if it doesn't excite you a very similar example is saying

1648
01:51:36,960 --> 01:51:39,610
i've got an image from a

1649
01:51:39,690 --> 01:51:44,130
telescope and it's got stars and galaxies and and galaxies tend to be a bit

1650
01:51:44,130 --> 01:51:49,270
more elliptical than stars and you might plot similar plot for the height and weight

1651
01:51:49,300 --> 01:51:52,900
of those objects and ask whether the stars and galaxies

1652
01:51:54,590 --> 01:51:59,720
it's immensely problem so i'm going to back off back to my oranges and lemons

1653
01:52:00,470 --> 01:52:04,700
so the first thing i propose we going to do is build what's called a

1654
01:52:04,700 --> 01:52:06,500
linear classifier say

1655
01:52:06,560 --> 01:52:09,180
what we're going to try and get a computer to do

1656
01:52:10,140 --> 01:52:12,950
let down the picture down the middle of the black line is what's called the

1657
01:52:12,950 --> 01:52:15,720
decision boundary and to the right

1658
01:52:15,730 --> 01:52:17,940
if we were to see new item fruit

1659
01:52:17,950 --> 01:52:19,400
and it landed here

1660
01:52:19,450 --> 01:52:23,230
to the right of the line anything that we would machine with them say

1661
01:52:23,230 --> 01:52:24,980
that's an orange and similarly

1662
01:52:24,990 --> 01:52:28,570
to the other side of machine would say that so i goal is going to

1663
01:52:28,570 --> 01:52:33,970
be to come up with a computer program that can draw that line four

1664
01:52:35,710 --> 01:52:38,940
just as we needed to represent as free with numbers we're going to need to

1665
01:52:38,940 --> 01:52:41,740
represent the line with numbers as well and so

1666
01:52:41,800 --> 01:52:45,950
this is why your linear algebra needs to be pretty good to deal with the

1667
01:52:45,950 --> 01:52:47,810
los machine learning because

1668
01:52:47,820 --> 01:52:52,240
dealing with vectors and matrices comes up lot and understanding sort of the geometry that

1669
01:52:52,300 --> 01:52:54,740
associated with that can be very useful

1670
01:52:56,300 --> 01:52:59,180
if i want to represent a plane

1671
01:52:59,230 --> 01:53:02,400
the standard way to do it is to represent it with the vector which i'm

1672
01:53:02,400 --> 01:53:06,360
going to call w which lies perpendicular to that plane

1673
01:53:07,240 --> 01:53:10,350
if i wanted to classify

1674
01:53:10,360 --> 01:53:14,130
this side of the rain from the side of the room i could have affected

1675
01:53:14,130 --> 01:53:15,400
the points this way

1676
01:53:15,450 --> 01:53:17,930
and then the plane will be perpendicular to the

1677
01:53:17,980 --> 01:53:20,190
the vector and i will split you can

1678
01:53:20,230 --> 01:53:22,600
now that might seem a bit of a funny thing to do why am i

1679
01:53:23,390 --> 01:53:25,780
pointing down the middle i wanted to divide

1680
01:53:25,840 --> 01:53:29,380
the dividing line here why not just point that

1681
01:53:29,390 --> 01:53:30,710
and that

1682
01:53:30,720 --> 01:53:33,870
if i have got to two d picture like the oranges and lemons that's a

1683
01:53:33,870 --> 01:53:36,820
fair point but in machine learning we're interested in

1684
01:53:36,850 --> 01:53:39,200
things that are less trivial and what you can plot

1685
01:53:39,230 --> 01:53:41,870
in two d on this slide there were interested in

1686
01:53:41,960 --> 01:53:47,260
not just decision lines but decision planes and these planes could be of high dimension

1687
01:53:47,280 --> 01:53:51,260
so let's say i want to define a plane which isn't just here

1688
01:53:51,270 --> 01:53:54,350
but it's up to the ceiling as well so now

1689
01:53:54,400 --> 01:53:57,050
if i needed to point out down the middle of the room i don't have

1690
01:53:57,050 --> 01:53:59,720
to point up and say this is the plane i'm interested in

1691
01:53:59,730 --> 01:54:02,370
now we need two vectors to do that

1692
01:54:02,380 --> 01:54:08,240
and indeed dimensions i would need a d minus one planes vectors to say these

1693
01:54:08,480 --> 01:54:11,380
this is the plane i want all these vectors lie in the plane

1694
01:54:11,440 --> 01:54:14,890
there's only one factor that doesn't lie in the plane so much easier to specify

1695
01:54:14,890 --> 01:54:15,980
that one

1696
01:54:15,990 --> 01:54:17,480
all the others

1697
01:54:17,520 --> 01:54:20,730
OK so

1698
01:54:20,770 --> 01:54:24,770
if this sort of geometry isn't something you've seen before it's well worth drawings and

1699
01:54:24,770 --> 01:54:27,110
pictures trying to work out

1700
01:54:27,140 --> 01:54:29,010
how these planes work

1701
01:54:30,460 --> 01:54:35,000
the basic take-home message for the moment is that you can define a plane by

1702
01:54:35,000 --> 01:54:36,530
just saying

1703
01:54:36,550 --> 01:54:42,260
the inner product of the weight vector with the feature is greater than zero

1704
01:54:44,060 --> 01:54:49,450
we can think about that intuitive like maybe y is y values w

1705
01:54:49,490 --> 01:54:52,730
that vector is often called the weight vector

1706
01:54:52,780 --> 01:54:54,340
and the reason is that it

1707
01:54:54,350 --> 01:54:57,550
giving weight sort of importance to the features say

1708
01:54:57,560 --> 01:55:02,690
i'm going to define something is plus one or an orange if w transpose x

1709
01:55:02,700 --> 01:55:04,090
begins airing

1710
01:55:04,130 --> 01:55:05,370
OK so

1711
01:55:06,350 --> 01:55:08,350
let's imagine that

1712
01:55:08,360 --> 01:55:12,800
w won the first elements of this vector is positive

1713
01:55:13,460 --> 01:55:17,610
now in order to make this inequality holds in order to make the transfer effects

1714
01:55:17,650 --> 01:55:22,140
because than theory and if w one is positive we'd encourage that happen if x

1715
01:55:22,140 --> 01:55:24,450
one was also positive so

1716
01:55:24,480 --> 01:55:27,120
the meaning of that weight vector that is

1717
01:55:27,180 --> 01:55:31,760
i am saying with these features contribute positively or negatively to

1718
01:55:31,780 --> 01:55:35,820
the expected classes and thinking about so that sort of importance is attached to the

1719
01:55:39,050 --> 01:55:46,210
this in the bias here the necessary for general plane and i'm going to skip

1720
01:55:46,210 --> 01:55:48,730
the moment so i don't know if you can see down here

1721
01:55:48,830 --> 01:55:52,510
i'm just going to talk about planes to go through the origin and their weight

1722
01:55:52,510 --> 01:55:57,940
w transpose x begins airing

1723
01:56:01,200 --> 01:56:04,650
i didn't say at the beginning but i'm really keen for to interrupt me and

1724
01:56:04,650 --> 01:56:08,240
ask questions all the way through the afternoon in case any of this is unclear

1725
01:56:09,000 --> 01:56:13,450
a lot of if you've done any machine learning before this is basic introductory material

1726
01:56:13,450 --> 01:56:16,110
a lot of people haven't and it's

1727
01:56:16,150 --> 01:56:18,350
you know there's no point in marching through matter

1728
01:56:24,530 --> 01:56:29,610
i just wanted to mention in fight which is

1729
01:56:29,650 --> 01:56:33,140
that you if you don't already know one you really need to go away and

1730
01:56:33,140 --> 01:56:39,110
learn a numerical package like matlab or python numerical package or some piece of software

1731
01:56:39,110 --> 01:56:42,170
that lets you deal with matrices and vectors easily

1732
01:56:42,190 --> 01:56:45,700
and here the reason in one line

1733
01:56:45,720 --> 01:56:49,750
so i had a program this is given me one of these weight vector w

1734
01:56:49,760 --> 01:56:51,330
and i had now

1735
01:56:51,340 --> 01:56:55,410
a new bunch of examples that i wanted to classify say

1736
01:56:55,910 --> 01:57:00,500
i've got a matrix which contains a whole bunch of numbers associated with new apples

1737
01:57:00,500 --> 01:57:06,000
and oranges on new stars and galaxies here is the code to make prediction so

1738
01:57:06,000 --> 01:57:08,090
this one line says

1739
01:57:08,130 --> 01:57:10,440
combine the weights and the data

1740
01:57:10,450 --> 01:57:14,520
see whether the result is positive and negative and use that to make predictions spit

1741
01:57:14,530 --> 01:57:18,630
out the galaxy of laurent one line of code to make predictions

1742
01:57:20,130 --> 01:57:23,320
it might not be that much code to do this and see if you're just

1743
01:57:23,320 --> 01:57:26,520
going to sort of leap over these things and do this and it'll be a

1744
01:57:26,520 --> 01:57:28,240
lot harder to see what's going on

1745
01:57:28,250 --> 01:57:30,960
and we're going to move on to the things that are less

1746
01:57:30,990 --> 01:57:33,400
trivial and that's this sort of

1747
01:57:33,420 --> 01:57:35,900
six thinking this is really important

1748
01:57:35,940 --> 01:57:39,350
it's not just sort of need this will also be a lot more efficient than

1749
01:57:39,350 --> 01:57:42,490
any c program you go away and write because in the background this will be

1750
01:57:42,490 --> 01:57:46,540
solved by high optimize linear algebra routines are going to be better than any c

1751
01:57:46,540 --> 01:57:47,870
program you right

1752
01:57:49,320 --> 01:57:53,120
learn how to use these packages

1753
01:57:58,680 --> 01:58:02,210
once you've got away and learn how to use the package this is an implementation

1754
01:58:02,220 --> 01:58:04,600
of a complete program for

1755
01:58:04,600 --> 01:58:07,780
learning one of these weight vector vectors given some data

1756
01:58:08,750 --> 01:58:11,330
it's not important you follow all the details right now

1757
01:58:11,330 --> 01:58:15,620
two pieces but no segment which spanned both the object and the background so this

1758
01:58:15,620 --> 01:58:19,650
is so much in practice so here we go back from the big image to

1759
01:58:19,650 --> 01:58:21,930
a set of twenty six months

1760
01:58:21,940 --> 01:58:24,660
the will behind the conundrum about that

1761
01:58:24,700 --> 01:58:28,570
so here the goal is not to create a new way of image is this

1762
01:58:28,600 --> 01:58:32,300
a good idea because these people use that allow the vision of is in addition

1763
01:58:32,350 --> 01:58:36,240
the goal is to take this is the way people use images and put it

1764
01:58:36,240 --> 01:58:39,300
into the kind of work to get the most out of it and being able

1765
01:58:39,300 --> 01:58:41,180
to use the full can imagery

1766
01:58:41,200 --> 01:58:46,010
if it can be can only images things that technique for example then automatically you

1767
01:58:46,010 --> 01:58:48,220
can get out of four

1768
01:58:48,230 --> 01:58:51,660
image retrieval classification clustering and so on so here

1769
01:58:51,670 --> 01:58:55,240
if you can sort zone a potential

1770
01:58:55,260 --> 01:58:58,300
the problem for images you get a lot out of it

1771
01:58:58,320 --> 01:59:03,070
so you see how it works will be taken image that segments using night to

1772
01:59:03,160 --> 01:59:05,820
energy from economy important role

1773
01:59:05,830 --> 01:59:07,150
well you've been a gradient

1774
01:59:07,160 --> 01:59:09,100
and your computer what action

1775
01:59:09,230 --> 01:59:12,990
and with different number of segments that you can be considered

1776
01:59:13,350 --> 01:59:16,210
you see that is the same as to get your your tools is so this

1777
01:59:16,210 --> 01:59:17,340
is not the case

1778
01:59:17,380 --> 01:59:18,240
the case where

1779
01:59:18,260 --> 01:59:20,260
the semantic get out of it

1780
01:59:20,270 --> 01:59:22,180
but if you take the

1781
01:59:22,180 --> 01:59:28,880
then we started saying that get stuck with the job to make with the background

1782
01:59:28,990 --> 01:59:32,370
and there is no way you can really discover anything but if you go to

1783
01:59:32,390 --> 01:59:33,330
sixty four

1784
01:59:33,330 --> 01:59:34,870
higher number of segments

1785
01:59:34,890 --> 01:59:40,160
so you just stuck it into pieces produced to get something for the same segmentation

1786
01:59:40,180 --> 01:59:45,200
so what we do is go from one image to what they called the segmentation

1787
01:59:45,220 --> 01:59:50,820
which is just a graph where each one of vertex per region

1788
01:59:51,660 --> 01:59:53,350
and it is a labeled graph

1789
01:59:53,400 --> 01:59:56,520
we need to label the text with the set of the set of colors that

1790
01:59:56,520 --> 01:59:59,450
you have on your region and connect

1791
01:59:59,470 --> 02:00:02,220
regions which are spatial neighbors

1792
02:00:02,230 --> 02:00:06,460
this is a very common with condition and in vision

1793
02:00:06,710 --> 02:00:10,660
essentially what work on that

1794
02:00:10,700 --> 02:00:14,580
so it is difficult for many reasons and because it essentially

1795
02:00:14,610 --> 02:00:17,590
you have a planar graph that tree

1796
02:00:17,600 --> 02:00:19,930
and of course you wanted to inexact matching

1797
02:00:19,950 --> 02:00:26,380
i think we'll we'll but we did is simply to go back to bioinformatics so

1798
02:00:26,490 --> 02:00:32,870
quite interesting to see the exchanges between mathematics and vision of different OK don't look

1799
02:00:32,870 --> 02:00:34,100
at all the same problems

1800
02:00:34,200 --> 02:00:35,940
in terms of the

1801
02:00:35,950 --> 02:00:40,260
in many cases the same type of problems for example in biology

1802
02:00:40,270 --> 02:00:45,400
the mathematics put these got but it comes across a lot of work trying to

1803
02:00:46,620 --> 02:00:52,150
to to design a kernel for graphs and essentially what we did is extend this

1804
02:00:52,150 --> 02:00:56,760
block kernels that were used for about six in the vision

1805
02:00:58,120 --> 02:01:02,930
so what is it the principle is that you want to contact us about essentially

1806
02:01:02,930 --> 02:01:04,730
enumerating all the support

1807
02:01:04,760 --> 02:01:08,380
so if you go back to this high dimensional feature space there are other possible

1808
02:01:08,380 --> 02:01:09,750
subgraphs in like

1809
02:01:10,540 --> 02:01:14,150
by using recursivity factorisation can compute

1810
02:01:14,190 --> 02:01:19,740
don't part it in all possible substrings also lives in nineteen ninety nine it

1811
02:01:19,750 --> 02:01:21,040
so here

1812
02:01:21,140 --> 02:01:22,340
if you to the

1813
02:01:22,350 --> 02:01:26,100
and i should allow for details the principle is very

1814
02:01:26,140 --> 02:01:31,350
very important which is very large feature space here all possible some rocks and we

1815
02:01:31,350 --> 02:01:36,010
make sure that we can compute the product very efficiently

1816
02:01:36,040 --> 02:01:42,030
so this work so stickers people database of the one thousand natural images with fourteen

1817
02:01:42,980 --> 02:01:44,890
and see how it works

1818
02:01:44,890 --> 02:01:49,510
so it so that you get simply is what you call what you do when

1819
02:01:49,510 --> 02:01:52,860
you only consider one big segment of the image and compare

1820
02:01:52,960 --> 02:01:54,390
histogram of kernels

1821
02:01:54,400 --> 02:01:57,980
so have the images of i so you would get a lot out of the

