1
00:00:00,000 --> 00:00:32,120
like were like a a a a a a a a right now the other people who are low on following a with

2
00:00:33,090 --> 00:00:46,400
all over the world but also catalogue acting but also did

3
00:00:46,400 --> 00:00:51,580
not have the time and he had a a offered its head of each

4
00:00:51,580 --> 00:02:25,550
other but I don't think it is just ps pp the Howard and given that the wrong sitting better exact algorithms

5
00:02:25,550 --> 00:02:28,690
we need to result approximation so many many applications

6
00:02:28,690 --> 00:02:36,810
we need to consider the possibility of schemes so idea is simply to ignore the

7
00:02:36,810 --> 00:02:41,550
fact that we have a graph general for of tree training just apply that

8
00:02:41,550 --> 00:02:44,460
belief propagation algorithm that some products algorithm

9
00:02:44,590 --> 00:02:48,990
to the general graphs so just keep propagating these local messages around and

10
00:02:48,990 --> 00:02:56,820
see what happens the problem that is that it might not converge um make

11
00:02:56,830 --> 00:02:59,530
converges make version of that all set on the other hand we

12
00:02:59,530 --> 00:03:02,050
may just the updating these things forever nothing city

13
00:03:02,050 --> 00:03:04,990
interesting happens and this is very much open research

14
00:03:04,990 --> 00:03:12,010
issue nobody really I'm to a lot of research on this topic at the moment to try to

15
00:03:12,170 --> 00:03:14,970
prove theorems all understand the circumstances you might

16
00:03:14,970 --> 00:03:17,490
get convergence and so on some progress has been made that

17
00:03:17,500 --> 00:03:22,050
is still some of the problems that is a very interesting problem because the wrong

18
00:03:22,050 --> 00:03:25,870
examples of an example of company decoding algorithms

19
00:03:25,870 --> 00:03:29,970
interoperating codes in which the state of the art

20
00:03:29,970 --> 00:03:34,610
algorithms corresponds to this this procedure is just

21
00:03:34,750 --> 00:03:37,570
really believe propagation you believe propagation to

22
00:03:37,570 --> 00:03:42,050
prosecutors so all the sum of all the say that all of these

23
00:03:42,050 --> 00:03:45,150
ones situation in which is the best known of the algorithm

24
00:03:46,050 --> 00:03:50,650
and of course at the track of interest so it's easy to

25
00:03:50,650 --> 00:03:56,390
apply it may or may not work so it might be worth trying to find the most difficult to say much more massive and

26
00:03:56,390 --> 00:04:02,270
that I'm a decided not to go through the junction tree

27
00:04:02,290 --> 00:04:05,530
algorithm in detail pot because even a chance to starting

28
00:04:05,530 --> 00:04:08,130
to look very a short and I think of the Socialist I find

29
00:04:08,130 --> 00:04:17,830
more interesting overall talk about the notes I think a recently self contained again the Renault priest in here pretty the user theorems

30
00:04:18,410 --> 00:04:22,050
news news is somewhat time consuming laborious so that the

31
00:04:22,050 --> 00:04:26,050
idea is a fairly simple you just read viewgraph military

32
00:04:26,050 --> 00:04:35,230
the flavour of the geometry of of summary of the focus on working towards from thinking about variation inference as

33
00:04:35,220 --> 00:04:47,410
a as a as approximate inference framework so that's so I'm stocks in that direction so let's just think a little bit less

34
00:04:47,410 --> 00:04:50,710
than 1st kind of abstract to sort of do these across the initial

35
00:04:50,710 --> 00:04:53,450
independent properties all very abstract it's all seeing

36
00:04:53,450 --> 00:04:58,570
very far away from the machine learning so analysis not

37
00:04:58,590 --> 00:05:00,810
make some links between the things the familiar from

38
00:05:00,810 --> 00:05:03,510
machine learning some of the techniques for the graphic

39
00:05:03,520 --> 00:05:06,450
models and then that will take on world thinking about

40
00:05:06,310 --> 00:05:11,510
restoration methods so according this this is just going to be a revision so

41
00:05:11,590 --> 00:05:16,110
that we can go through this reasonably quickly so let's

42
00:05:16,110 --> 00:05:19,660
suppose that X is all because the policy we're measuring

43
00:05:19,810 --> 00:05:23,430
and we have a capital and observations index of the set of

44
00:05:23,430 --> 00:05:27,370
all these observations local D. that's all data set of

45
00:05:27,370 --> 00:05:30,710
training data set out in the simplest case these

46
00:05:30,710 --> 00:05:35,490
observations all independent and so we can write down the

47
00:05:35,490 --> 00:05:46,930
probability of the data set and you in terms of some with and the song parametric model so if we have a a parametric

48
00:05:46,930 --> 00:05:52,750
model for x which depends on some parameters dataset than the

49
00:05:52,750 --> 00:05:55,270
probability of the data set is the product of the data

50
00:05:55,270 --> 00:05:59,330
points of the probability for each observation given data

51
00:05:59,930 --> 00:06:02,830
and we take this quantity if we view this as a function of

52
00:06:02,850 --> 00:06:09,310
theta but it's called the likelihood function and the the likelihood of a word and used and in all sorts

53
00:06:09,310 --> 00:06:13,210
of ways in this is the regret definition of likelihood that is the

54
00:06:13,210 --> 00:06:15,710
probability that a given the premises viewed as a function

55
00:06:15,710 --> 00:06:18,250
of the premises and notice that is not a probability

56
00:06:18,250 --> 00:06:21,890
distribution of the data this just a function of the you

57
00:06:24,490 --> 00:06:27,290
know the simplest procedures in very widely used which

58
00:06:27,260 --> 00:06:30,430
using these parameters based on the training data is the

59
00:06:30,430 --> 00:06:32,670
list of learning the parameters of the model is full

60
00:06:32,680 --> 00:06:37,810
maximum likelihood that it just as it will assess the premises to specific

61
00:06:37,810 --> 00:06:43,210
them to such that it maximizes the probability of the data set and

62
00:06:43,210 --> 00:06:57,620
practices convenient and thank the logarithm and published in America reasons but also converts this for brought into some and uh can simplify the these expression sometimes but it is completely equivalent because law as a moment of

63
00:06:57,630 --> 00:07:00,830
functions the maximum likelihood solution is just obtained

64
00:07:00,830 --> 00:07:03,530
by a maximum maximizing the love of the likelihood function

65
00:07:03,950 --> 00:07:06,350
and then given that solution we don't want to to but the

66
00:07:06,350 --> 00:07:09,390
training phase now we want to make predictions that was the

67
00:07:09,390 --> 00:07:12,460
it's quite instructive to look at

68
00:07:12,470 --> 00:07:14,110
what different

69
00:07:14,680 --> 00:07:18,930
results these two learning algorithms could have

70
00:07:18,950 --> 00:07:24,580
unfortunately it didn't the ten factor lecture that he talk about an example for the

71
00:07:24,580 --> 00:07:27,640
difference between off policy and on policy

72
00:07:27,660 --> 00:07:28,970
alex where you there

73
00:07:29,710 --> 00:07:31,760
did explain it quite well or

74
00:07:34,930 --> 00:07:40,990
that could help so this very nice example also by certain and

75
00:07:41,000 --> 00:07:43,310
by sutton and barto

76
00:07:43,330 --> 00:07:50,510
which is a relatively simple problems

77
00:07:50,550 --> 00:07:56,500
you know most of these reinforcement learning algorithms are never really tested but only tested

78
00:07:56,510 --> 00:08:03,710
in so-called great world but the favourite toy of reinforcement learning researchers so

79
00:08:03,720 --> 00:08:06,690
this is the particularly

80
00:08:07,690 --> 00:08:09,420
great gridworld example

81
00:08:09,430 --> 00:08:13,860
in which we have to start state and the goal state

82
00:08:13,880 --> 00:08:18,100
and the agent is required to move from the start state to the goal state

83
00:08:18,350 --> 00:08:19,520
and whenever

84
00:08:19,540 --> 00:08:21,960
the agent moves

85
00:08:21,980 --> 00:08:24,280
he gets the reward of minus one

86
00:08:24,290 --> 00:08:27,270
because that cost something so to speak to move

87
00:08:29,700 --> 00:08:35,540
the goal state is absorbing so the episode is over once the goal state is

88
00:08:35,540 --> 00:08:42,270
reached and if the agent runs into the so-called in this class

89
00:08:42,290 --> 00:08:45,580
if you like when he is taken back

90
00:08:45,630 --> 00:08:51,080
to the start state and incurs a loss of minus one hundred i think that's

91
00:08:52,250 --> 00:08:56,170
from this thing to incur if you fall off the cliff

92
00:08:58,040 --> 00:09:03,470
well it does the trick and no one can i ask how what q learning

93
00:09:03,470 --> 00:09:05,850
and sarsa behave differently

94
00:09:05,860 --> 00:09:09,970
if we use an exploratory

95
00:09:09,980 --> 00:09:11,380
absolutely greedy

96
00:09:13,120 --> 00:09:18,290
say with absolute to set to ten percent so in ten percent of the cases

97
00:09:20,510 --> 00:09:25,200
the agent would actually not the best thing or what he believes is the best

98
00:09:25,200 --> 00:09:29,660
thing but would choose randomly among the other options of actions

99
00:09:29,960 --> 00:09:32,950
and it turns out that

100
00:09:34,500 --> 00:09:37,220
so is an on policy

101
00:09:39,700 --> 00:09:44,040
it uses as its policy

102
00:09:46,720 --> 00:09:49,250
something that looks

103
00:09:49,270 --> 00:09:52,310
like this it takes the

104
00:09:52,330 --> 00:09:55,900
it takes the safe route to the gold

105
00:09:55,910 --> 00:09:58,670
now why is that

106
00:09:58,680 --> 00:10:01,580
it is because every now and then

107
00:10:01,600 --> 00:10:04,400
the policy that's being evaluated

108
00:10:04,420 --> 00:10:08,290
in ten percent of the cases chooses not the optimal things

109
00:10:09,980 --> 00:10:15,940
is just shows this variant here

110
00:10:15,960 --> 00:10:19,690
then it would fall in ten percent of the cases of the cliff

111
00:10:19,710 --> 00:10:22,190
and would incur huge loss

112
00:10:22,260 --> 00:10:24,100
so is the consequent

113
00:10:24,150 --> 00:10:29,030
as a consequence the values that have been in the state action values at the

114
00:10:29,030 --> 00:10:35,120
end recommend this policy here which is carefully and totally avoid falling of the of

115
00:10:35,120 --> 00:10:37,860
the edge because it stays away from it

116
00:10:37,880 --> 00:10:41,190
now in contrast hue learning

117
00:10:42,320 --> 00:10:44,730
a different policy

118
00:10:44,750 --> 00:10:46,370
q learning

119
00:10:46,380 --> 00:10:48,680
is not permanent

120
00:10:49,270 --> 00:10:56,100
it's not terminator is at the classical mistake that policy

121
00:10:56,110 --> 00:10:58,990
so you're learning

122
00:10:59,010 --> 00:11:02,370
finds this policy

123
00:11:02,380 --> 00:11:07,590
which of course is the shorter way but somewhat more dangerous and if you look

124
00:11:07,590 --> 00:11:07,920
at the

125
00:11:08,390 --> 00:11:09,970
reward for the two

126
00:11:09,990 --> 00:11:14,520
you'll find that

127
00:11:14,530 --> 00:11:15,610
you'll find that

128
00:11:15,620 --> 00:11:20,780
so plotting the reward

129
00:11:20,800 --> 00:11:23,730
over the time of learning you for that

130
00:11:23,790 --> 00:11:25,330
so size

131
00:11:25,410 --> 00:11:29,110
actually does better

132
00:11:29,130 --> 00:11:32,850
then q learning

133
00:11:32,870 --> 00:11:34,780
because q learning false

134
00:11:34,800 --> 00:11:40,350
in the learning period falls down this cliff quite often because he chooses which is

135
00:11:40,350 --> 00:11:45,900
this risky policy if you like but of course that doesn't really matter because once

136
00:11:45,940 --> 00:11:49,800
we've learned we switch off the exploration

137
00:11:49,820 --> 00:11:52,690
we then go to have an equal to zero

138
00:11:52,700 --> 00:11:56,700
and then this policy of course is the better one because it is shorter in

139
00:11:56,700 --> 00:12:03,160
christmas loss if you like and you never fall down here because you don't have

140
00:12:03,160 --> 00:12:08,230
this random exploration factor anymore so what you see here is that there's really the

141
00:12:08,260 --> 00:12:14,890
difference between what source and q learning can do and of course if you you

142
00:12:14,980 --> 00:12:19,160
down the epsilon and union down the learning rate

143
00:12:19,180 --> 00:12:23,670
the learning rate alpha here they will eventually converge to the same thing but in

144
00:12:23,670 --> 00:12:28,840
practice you may never get there and so you could expect different policies to emerge

145
00:12:29,640 --> 00:12:32,230
from the two learning algorithm

146
00:12:32,240 --> 00:12:36,270
this is the principal difference between off policy and non-policy learning

147
00:12:36,280 --> 00:12:39,940
OK so here's an example of

148
00:12:39,960 --> 00:12:48,420
of whole learning proceeds i'm choosing for illustration purposes here tabular representation of the action

149
00:12:48,420 --> 00:12:51,320
value function so this is the q table

150
00:12:51,320 --> 00:12:53,770
OK now

151
00:12:53,780 --> 00:12:57,540
why is it so useful because posterior distributions often have the form rights you often

152
00:12:57,540 --> 00:12:59,410
have some sort of

153
00:13:02,030 --> 00:13:05,890
which has some prior distribution and you're going to observe a bunch of data

154
00:13:06,030 --> 00:13:09,750
why i given x

155
00:13:09,840 --> 00:13:10,650
and this will be

156
00:13:11,950 --> 00:13:14,890
you're going distribution of y and x

157
00:13:18,650 --> 00:13:22,080
so so this sort of factor product and actually arise and that's why that's my

158
00:13:22,080 --> 00:13:23,820
photographs is useful

159
00:13:23,830 --> 00:13:27,210
OK so let me just show an example of of fascist fashion graph that arises

160
00:13:27,210 --> 00:13:33,460
in probability theory or machine learning suppose we just have to girls observations of some

161
00:13:33,460 --> 00:13:37,220
unknown means so we have this a variable which is unknown and we made observations

162
00:13:37,220 --> 00:13:38,520
of that

163
00:13:38,520 --> 00:13:41,140
but suppose we make idea observations that so

164
00:13:41,150 --> 00:13:45,840
now you all know how to write it down algebraically you write that the posterior

165
00:13:45,840 --> 00:13:47,270
distribution of

166
00:13:47,290 --> 00:13:50,690
given a bunch of axes

167
00:13:50,700 --> 00:13:54,340
it is going to be proportional to first of all the prior on

168
00:13:54,350 --> 00:13:55,820
and the product

169
00:13:55,830 --> 00:13:59,690
of the likelihood of all the axes

170
00:14:00,310 --> 00:14:02,980
that's second nature two guys now

171
00:14:03,000 --> 00:14:06,810
but what is the factor graph for this problem

172
00:14:07,580 --> 00:14:11,150
i've given you the posterior distribution of showing you how factors so should be quite

173
00:14:11,150 --> 00:14:19,980
straightforward to manufacture have can anyone give me the factor of this problem

174
00:14:20,020 --> 00:14:22,020
so effective

175
00:14:22,020 --> 00:14:26,500
and all the axes

176
00:14:26,540 --> 00:14:28,010
one factor

177
00:14:28,020 --> 00:14:31,790
for each x and what does it do

178
00:14:31,840 --> 00:14:34,010
next to each one

179
00:14:38,040 --> 00:14:39,960
are we done

180
00:14:39,970 --> 00:14:44,320
yes one single factor for the prior

181
00:14:44,330 --> 00:14:50,890
OK let me give you another brain teaser suppose we have a markov chain

182
00:14:50,970 --> 00:14:55,080
so what's the formula for markov chain so i have a bunch of

183
00:14:56,160 --> 00:15:01,600
and we have a distribution for the first one

184
00:15:01,630 --> 00:15:04,770
and then we we'll have a distribution for each x

185
00:15:04,840 --> 00:15:09,270
conditional on the previous acts

186
00:15:09,390 --> 00:15:13,160
so that's the that's the joint distribution for markov chains

187
00:15:13,230 --> 00:15:16,250
while familiar with the now what is the factor graph

188
00:15:16,270 --> 00:15:18,570
this market change

189
00:15:19,620 --> 00:15:25,060
a chain of squares and circles OK

190
00:15:28,500 --> 00:15:32,120
and there's the prior years

191
00:15:32,140 --> 00:15:34,540
so x one

192
00:15:34,570 --> 00:15:35,560
x two

193
00:15:35,580 --> 00:15:37,030
x three

194
00:15:37,130 --> 00:15:41,830
OK and this so this box here just to be clear is the prior x

195
00:15:41,850 --> 00:15:44,090
one this box here is the

196
00:15:44,120 --> 00:15:46,920
conditional distribution of x two given x one

197
00:15:46,970 --> 00:15:52,900
in this boxes x three community

198
00:15:54,910 --> 00:15:57,720
and i on the order of ten factor graphs you're halfway there

199
00:15:59,790 --> 00:16:02,410
yes so just to review

200
00:16:03,950 --> 00:16:07,270
one so there's two sort of big problems one is what graph actually use a

201
00:16:07,270 --> 00:16:10,710
particular dataset i'm not convinced that all assuming that you figured out how to model

202
00:16:10,710 --> 00:16:11,840
the data already

203
00:16:12,100 --> 00:16:15,590
then there's the second question is given the graph and the data

204
00:16:15,600 --> 00:16:19,540
what does it mean for example what is the past what's the posterior distribution and

205
00:16:19,770 --> 00:16:22,730
there's a very different organs you can choose from and the other and i'm going

206
00:16:22,730 --> 00:16:23,870
to be looking into their

207
00:16:23,880 --> 00:16:30,120
what are called message passing type algorithms in particular expectation propagation variational bayes

208
00:16:30,140 --> 00:16:35,270
OK so i'm going to illustrate his arguments with a specific country problem

209
00:16:35,330 --> 00:16:39,150
which is seemingly very difficult problem if you look at it from exact inference but

210
00:16:39,150 --> 00:16:42,580
actually quite easy if we look at it from approximate inference five u

211
00:16:42,630 --> 00:16:44,040
OK so

212
00:16:44,090 --> 00:16:46,950
this problem is one that i have used a lot in my papers which is

213
00:16:46,950 --> 00:16:48,450
called clutter problems

214
00:16:48,480 --> 00:16:54,280
so in this problem you have this unknown x scalar variable which has gaussian prior

215
00:16:54,290 --> 00:16:58,270
and you have a bunch of observations of this axis but the observations are corrupted

216
00:16:58,280 --> 00:17:00,230
some probability so half the time

217
00:17:00,250 --> 00:17:04,780
the observations are discussed in observations two minutes act

218
00:17:04,800 --> 00:17:06,120
said before

219
00:17:06,130 --> 00:17:10,360
but half the time the observation just plain noise avenue that's all it is why

220
00:17:10,360 --> 00:17:13,520
i distributed from zero to normal distribution

221
00:17:13,530 --> 00:17:17,320
so what is so it's sort of like a problem we have background noise some

222
00:17:17,320 --> 00:17:20,750
of your points are actually meaningful the the foreground points to the ones i actually

223
00:17:20,750 --> 00:17:23,680
tell you what we try trying estimate and a whole bunch of data that just

224
00:17:23,680 --> 00:17:28,890
says nothing about what you're trying to just random stuff that was run

225
00:17:28,970 --> 00:17:32,200
we don't know which points in which OK so

226
00:17:32,260 --> 00:17:34,800
what is the an organ we're organ observed

227
00:17:34,810 --> 00:17:38,670
the point of these why is want try to estimate x OK so this point

228
00:17:38,670 --> 00:17:42,030
you guys can buy tell me what the pressure of his for this problem

229
00:17:44,030 --> 00:17:50,940
factor graph this problem

230
00:17:50,950 --> 00:17:53,780
so we have a bunch of wise

231
00:17:53,800 --> 00:17:58,670
we have access

232
00:17:58,730 --> 00:18:01,510
and have

233
00:18:01,630 --> 00:18:05,410
likelihood contracts the prior acts

234
00:18:05,450 --> 00:18:07,270
OK now

235
00:18:07,280 --> 00:18:08,150
what is the

236
00:18:08,150 --> 00:18:10,910
x what is the form of the exact

237
00:18:10,930 --> 00:18:13,150
posterior next to this problem

238
00:18:13,150 --> 00:18:16,420
so we give you an data points

239
00:18:19,380 --> 00:18:22,370
what form does have world as we know it's going to be

240
00:18:22,390 --> 00:18:26,850
if x is approached by y given x

241
00:18:26,860 --> 00:18:28,220
OK now

242
00:18:28,240 --> 00:18:34,030
here is again why i given x is some weighted sum of two gaussians

243
00:18:35,370 --> 00:18:39,630
now what's what's going to turn into our thesis of i have

244
00:18:39,670 --> 00:18:41,680
if i have seen

245
00:18:41,690 --> 00:18:45,890
two data points

246
00:18:46,120 --> 00:18:47,280
and i'm going to get

247
00:18:47,290 --> 00:18:50,860
girls in times the product of two terms and another product in terms of expand

248
00:18:50,860 --> 00:18:53,340
that i'm going to get four terms

249
00:18:53,350 --> 00:18:56,810
my some of the gaussians are gonna multiplied to give gaussians org seems to have

250
00:18:56,810 --> 00:18:58,380
a mixture of four gaussians

251
00:18:58,390 --> 00:19:02,600
in my posterior distribution is that clear to everyone

252
00:19:02,650 --> 00:19:06,800
let's of i two points

253
00:19:06,820 --> 00:19:09,390
i get for gaussians

254
00:19:09,400 --> 00:19:13,950
doesn't sound too bad overweight five points

255
00:19:13,970 --> 00:19:18,490
how many gaussians i have in my posterior distributions

256
00:19:18,500 --> 00:19:20,550
two to the and like

257
00:19:22,120 --> 00:19:23,750
so that doesn't seem very tractable

258
00:19:23,760 --> 00:19:27,730
so if i gave you a hundred points are already carry out of luck with

259
00:19:27,730 --> 00:19:32,380
an exact inference here so yes you can enumerate all those gaussians computer giant mixture

260
00:19:33,020 --> 00:19:33,920
but it's going to be

261
00:19:33,920 --> 00:19:39,280
disagreement is going to be replaced with

262
00:19:39,300 --> 00:19:41,130
the difference in losses

263
00:19:41,170 --> 00:19:42,920
next over y

264
00:19:43,130 --> 00:19:49,300
and an asian each prime here but they're both run it are somewhat triangular inequality

265
00:19:49,300 --> 00:19:51,490
miles choose one h

266
00:19:51,490 --> 00:19:53,190
around the

267
00:19:53,220 --> 00:19:54,990
the rich

268
00:19:55,010 --> 00:19:57,400
so only managed by a factor of two in

269
00:19:57,420 --> 00:20:02,860
we're doing analysis were factors of two don't really matter that much

270
00:20:02,900 --> 00:20:07,650
OK so a big disagreement coefficient a big generalizes coefficients

271
00:20:07,650 --> 00:20:09,510
it means that there are

272
00:20:09,550 --> 00:20:11,760
near optimal hypotheses which

273
00:20:11,840 --> 00:20:14,800
substantially disagree in terms of the loss

274
00:20:14,800 --> 00:20:16,800
this incurred

275
00:20:21,510 --> 00:20:22,340
OK so

276
00:20:22,400 --> 00:20:25,570
given all that

277
00:20:25,590 --> 00:20:28,300
we have the minimum loss

278
00:20:28,720 --> 00:20:31,510
known as the loss rate

279
00:20:32,210 --> 00:20:35,490
we can prove about like this so we have

280
00:20:35,510 --> 00:20:38,470
his new team which is under removable

281
00:20:38,490 --> 00:20:40,590
two lower bound

282
00:20:40,610 --> 00:20:43,110
right so we we have some

283
00:20:43,130 --> 00:20:44,220
the minimum

284
00:20:44,740 --> 00:20:45,860
amount of

285
00:20:45,880 --> 00:20:50,630
labels we must ask for because our minimum error rate is not zero or minimal

286
00:20:50,630 --> 00:20:53,470
loss rate is not here

287
00:20:53,490 --> 00:20:55,360
and then we have

288
00:20:55,400 --> 00:20:57,090
this coverage of t

289
00:20:57,840 --> 00:20:59,860
many of these waters so

290
00:20:59,880 --> 00:21:03,920
this is this is not quite as good as well as it substantially worse than

291
00:21:04,010 --> 00:21:06,340
we got a zero on this

292
00:21:06,340 --> 00:21:10,970
but still less than t right so we used a lot here

293
00:21:11,010 --> 00:21:12,780
and now we have

294
00:21:12,780 --> 00:21:15,530
a reality

295
00:21:17,050 --> 00:21:25,300
we lost a little bit in generalizing to other loss functions

296
00:21:25,440 --> 00:21:30,800
so i should mention that i'm not at all positive this analysis canonical

297
00:21:30,970 --> 00:21:34,220
this is the notion that it's possible to

298
00:21:36,320 --> 00:21:41,740
active learning to work with other loss functions it using these importance weighted approach

299
00:21:41,740 --> 00:21:55,880
so the answer is yes

300
00:21:55,900 --> 00:21:59,420
but all the loss functions where i know how to achieve lower than

301
00:21:59,510 --> 00:22:01,470
i have a very special structure

302
00:22:01,490 --> 00:22:04,510
where the losses are very discreet

303
00:22:04,510 --> 00:22:05,650
which means that

304
00:22:05,670 --> 00:22:09,630
for variations in the parameters of your bosses

305
00:22:09,650 --> 00:22:12,800
the loss don't change very much

306
00:22:21,170 --> 00:22:26,550
so this is a slightly different analysis of the

307
00:22:26,570 --> 00:22:35,320
the claim is that you're going to compete with supervised learning algorithm which has t

308
00:22:35,320 --> 00:22:37,280
labelled examples

309
00:22:37,300 --> 00:22:43,240
so the have on a set implicitly by the supervised learning our

310
00:22:44,260 --> 00:22:50,780
so this is talking about the number of labelled samples used in the number of

311
00:22:50,780 --> 00:22:53,010
labelled samples used simultaneously

312
00:22:53,030 --> 00:22:57,300
which is which is why this maybe a better form of analysis

313
00:22:57,320 --> 00:23:00,380
but you have to think about where the epsilon

314
00:23:00,450 --> 00:23:03,240
OK so

315
00:23:03,490 --> 00:23:05,610
the reason why

316
00:23:05,880 --> 00:23:09,010
i felt like it was the time to inaccurate tutorial

317
00:23:09,070 --> 00:23:12,820
it is because we have addressed all of these cancers theoretically with this actually has

318
00:23:12,820 --> 00:23:16,550
in fact experimental is now possible to

319
00:23:18,190 --> 00:23:20,470
a active learning out the

320
00:23:20,470 --> 00:23:21,840
which is consistent

321
00:23:21,840 --> 00:23:24,300
in the nonseparable case

322
00:23:24,360 --> 00:23:27,860
and get label complexity improvements in practice

323
00:23:27,920 --> 00:23:31,940
so the several ways you could do this one way is

324
00:23:32,490 --> 00:23:34,570
if if you look at if you think about the

325
00:23:34,570 --> 00:23:35,710
loss waiting

326
00:23:35,720 --> 00:23:37,880
rejection threshold subroutine

327
00:23:37,880 --> 00:23:40,720
there a special case where things are easy so

328
00:23:40,740 --> 00:23:45,110
if the loss function is convex like five like logistic loss

329
00:23:45,130 --> 00:23:48,920
and the representation is linear breaking w dot x

330
00:23:48,940 --> 00:23:53,550
the competition is is easy

331
00:23:53,630 --> 00:23:59,840
so is the caveat to that but basically you can use

332
00:23:59,940 --> 00:24:01,610
a simple approach to

333
00:24:01,630 --> 00:24:04,760
to keep track of what the version spaces are near

334
00:24:05,070 --> 00:24:08,380
attractor with the version spaces

335
00:24:08,400 --> 00:24:09,650
another things you can do

336
00:24:09,690 --> 00:24:13,400
you can go back to the initial skeleton and you can think about

337
00:24:13,420 --> 00:24:18,220
that's pretty much said about the same for the rejection threshold says that the probabilities

338
00:24:18,220 --> 00:24:19,670
above some minimum

339
00:24:19,710 --> 00:24:21,800
and you're going to be consistent

340
00:24:21,820 --> 00:24:25,090
in this this opens up a whole bag of

341
00:24:25,150 --> 00:24:29,800
possible our rhythms to solve the problem if if you as a human know that

342
00:24:29,800 --> 00:24:30,760
this region

343
00:24:30,860 --> 00:24:34,130
it is particularly problematic in any more samples

344
00:24:34,150 --> 00:24:38,490
and you can design your rejection threshold subroutines to go there and settle there more

345
00:24:38,490 --> 00:24:40,510
often than elsewhere

346
00:24:40,510 --> 00:24:44,150
this is going to come to point about make so we really have no black

347
00:24:44,150 --> 00:24:48,790
box view of machine learning if we have to think about each problem domains separately

348
00:24:48,870 --> 00:24:52,940
and then i think a nice way of knowing our beliefs to generate data from

349
00:24:52,940 --> 00:24:57,780
the prior season matches our expectations if it doesn't try to figure out how to

350
00:24:57,780 --> 00:25:02,600
improve our prior so play around with the prior for a while before you actually

351
00:25:02,600 --> 00:25:07,160
exposed to the data i think that's seems rational way of doing things that i

352
00:25:07,160 --> 00:25:11,120
feel that even vaguely cinema you may be shy about using them but they will

353
00:25:11,120 --> 00:25:13,780
actually be very useful

354
00:25:14,950 --> 00:25:19,910
so let me talk about this sort of black box view of machine learning

355
00:25:19,960 --> 00:25:24,620
and i think you know there really are two different at least two different schools

356
00:25:24,620 --> 00:25:27,180
of machine learning out there

357
00:25:27,190 --> 00:25:33,280
most of the field the machine learning out characterizes being in this black box view

358
00:25:33,370 --> 00:25:37,830
so the goal of machine learning is to produce general purpose

359
00:25:37,840 --> 00:25:40,300
algorithms for learning

360
00:25:40,340 --> 00:25:43,290
i should be able to put my algorithm online

361
00:25:43,300 --> 00:25:47,340
you know many of us do this so lots of people can download it

362
00:25:47,390 --> 00:25:50,540
if people want to apply it to problems a b c and d then it

363
00:25:50,540 --> 00:25:54,830
should work regardless of the problem basically like it should be robust to

364
00:25:55,850 --> 00:25:58,470
you know classification or regression are

365
00:25:58,500 --> 00:26:01,620
clustering problem you throw at it because we don't know what people are going to

366
00:26:02,310 --> 00:26:06,580
any user should not have to think too much right we provided the tool

367
00:26:06,630 --> 00:26:09,650
the user downloads it runs it on the data

368
00:26:09,670 --> 00:26:13,670
and this is why male most machine learning papers

369
00:26:13,680 --> 00:26:17,540
i have a table of results at the end of the where they take

370
00:26:17,580 --> 00:26:21,600
some algorithm and they apply it in fact to problems a b c and d

371
00:26:22,220 --> 00:26:26,460
and they show that it works better problems a b c and d then some

372
00:26:27,760 --> 00:26:29,790
you know

373
00:26:30,770 --> 00:26:32,790
ex-prime which is whatever the

374
00:26:33,180 --> 00:26:34,470
you know

375
00:26:34,500 --> 00:26:36,650
an algorithm is out there

376
00:26:37,410 --> 00:26:39,650
so this is how

377
00:26:39,660 --> 00:26:42,450
most machine learning papers that

378
00:26:42,500 --> 00:26:45,340
now most statistics papers

379
00:26:46,850 --> 00:26:50,960
i feel least follow this approach the more case studies

380
00:26:51,020 --> 00:26:53,360
right and so the idea here

381
00:26:53,380 --> 00:26:56,650
is if i want to solve problem a

382
00:26:56,660 --> 00:27:02,060
it seems silly to use some general purpose method that was never designed for a

383
00:27:02,060 --> 00:27:05,570
so i should really try to understand what problem a is

384
00:27:05,570 --> 00:27:08,080
i learn about the properties of the data

385
00:27:08,120 --> 00:27:12,010
use as much expert knowledge as i can only then should i think of designing

386
00:27:12,010 --> 00:27:14,050
a method to solve

387
00:27:15,540 --> 00:27:19,850
so here's the question that we can think about

388
00:27:20,390 --> 00:27:22,600
you know maybe this is

389
00:27:26,110 --> 00:27:29,100
they are actually

390
00:27:33,260 --> 00:27:37,690
actually realistic a lot of real problem cases

391
00:27:37,760 --> 00:27:43,740
they're in you talk about this with me thinking about what

392
00:27:43,740 --> 00:27:45,400
what's the problem

393
00:27:49,460 --> 00:27:51,600
a lot of time

394
00:27:52,030 --> 00:27:53,820
and then after as

395
00:27:53,910 --> 00:27:58,150
after you get to know and then you develop a model and you say you

396
00:27:58,490 --> 00:28:01,140
behold i can model my data

397
00:28:01,360 --> 00:28:07,610
that's true i think it is even true for bayesian straight you know i i

398
00:28:07,710 --> 00:28:13,820
would be impressed to find bayesian who has the disc full of data and you

399
00:28:13,820 --> 00:28:20,460
know it is strong enough not to peak at the data for you if you're

400
00:28:20,460 --> 00:28:22,670
that bayesian raise your hand right now

401
00:28:22,680 --> 00:28:26,580
i think most of us speak at the data but you know we we try

402
00:28:26,580 --> 00:28:28,680
to quickly forget what we see

403
00:28:28,820 --> 00:28:36,260
that it right i mean that we do not

404
00:28:37,330 --> 00:28:41,240
it's probably you know if we want to solve the problem well

405
00:28:41,250 --> 00:28:44,100
then you know peaking at the data

406
00:28:44,160 --> 00:28:45,500
you know probably helps

407
00:28:45,500 --> 00:28:50,380
right i might point out things that are grossly wrong about our prior

408
00:28:51,280 --> 00:29:00,610
and i think the problem is that is one of the things that you can

409
00:29:04,280 --> 00:29:11,180
and one of the things i want you to be bold

410
00:29:11,210 --> 00:29:13,600
well let's not

411
00:29:15,870 --> 00:29:17,130
so there

412
00:29:17,160 --> 00:29:19,700
we can do that well

413
00:29:19,710 --> 00:29:25,830
i think more philosophical and i think you should always believed that they want to

414
00:29:25,830 --> 00:29:28,520
be in business rates they don't want to be you know they don't want to

415
00:29:28,520 --> 00:29:30,170
be replaced by machines

416
00:29:30,190 --> 00:29:36,590
right whereas we are trying to build machines to replace statisticians and so that's why

417
00:29:36,590 --> 00:29:42,440
the black box view if we can get tony o'hagan inside the black box then

418
00:29:42,440 --> 00:29:45,360
we have a pretty good black box out there

419
00:29:45,430 --> 00:29:46,200
you know

420
00:29:46,210 --> 00:29:53,540
you know but you know in that sense you can see where you know these

421
00:29:54,350 --> 00:29:59,100
views can get blurred right in the if we have a clever enough black box

422
00:30:00,300 --> 00:30:03,070
it should be

423
00:30:03,110 --> 00:30:09,100
you know it should be able to come up with original set of models

424
00:30:09,120 --> 00:30:12,580
that you might be able to work on lots of different problems

425
00:30:12,620 --> 00:30:14,430
but you know that's like

426
00:30:14,440 --> 00:30:17,110
back to the AI problem

427
00:30:17,350 --> 00:30:22,410
and also it's hard to put in domain knowledge into the black box records usually

428
00:30:22,420 --> 00:30:26,870
people to supply the dataset to some matrix of data whatever

429
00:30:27,170 --> 00:30:32,820
but this is interesting i mean i think about this quite a lot because

430
00:30:32,860 --> 00:30:35,720
you know if i'm really trying to solve the problem and not just going to

431
00:30:35,720 --> 00:30:39,690
take it you know i really care about the results have acquired or somebody who

432
00:30:39,690 --> 00:30:41,530
really means

433
00:30:41,530 --> 00:30:43,640
no good answers

434
00:30:43,660 --> 00:30:48,080
i don't really want to apply some just standard algorithm unless that you know unless

435
00:30:48,080 --> 00:30:51,970
i don't really want to get you know i don't care about getting really good

436
00:30:51,970 --> 00:30:55,320
answers i just want to get answers quickly that person

437
00:30:55,320 --> 00:30:57,570
this sparse terminal a grant multipliers

438
00:30:59,810 --> 00:31:02,110
hand you can use the trick the i

439
00:31:02,420 --> 00:31:04,320
outlined to convert that's

440
00:31:04,980 --> 00:31:05,440
in from

441
00:31:06,200 --> 00:31:09,010
it's really difficult when you start dealing with the ground and the class

442
00:31:09,480 --> 00:31:11,610
so this is a low class e and f

443
00:31:11,670 --> 00:31:13,120
which changed it is

444
00:31:13,860 --> 00:31:15,690
contains below ground multipliers

445
00:31:17,510 --> 00:31:18,640
this is the same trick before

446
00:31:19,360 --> 00:31:20,400
we keep using actually

447
00:31:20,830 --> 00:31:21,810
to move between

448
00:31:22,300 --> 00:31:26,640
the trace multiplication between the distance and a sparse connectivity matrix

449
00:31:27,760 --> 00:31:30,830
hands they look classy and and in the product matrix

450
00:31:31,260 --> 00:31:32,620
so i briefly outlined at

451
00:31:33,780 --> 00:31:34,520
on the board there

452
00:31:35,340 --> 00:31:41,110
and the reason you can do that's because the laplacian has this null space now this classy and

453
00:31:41,720 --> 00:31:43,440
is the same as the previous l class you

454
00:31:45,440 --> 00:31:53,290
of diagonals are the ground multipliers now rather than these adjacency elements which were set constant or set

455
00:31:53,700 --> 00:31:54,340
according to

456
00:31:55,040 --> 00:31:58,970
some calcium function that we had before and the diagonal is just the sum of

457
00:31:59,020 --> 00:32:01,640
the off-diagonals giving us the standard plus in full

458
00:32:02,390 --> 00:32:05,870
you also get no you also get this term appearing yes so this

459
00:32:08,300 --> 00:32:09,850
needs to be combined in here

460
00:32:10,080 --> 00:32:11,840
and there's a gamma i appear in there

461
00:32:12,280 --> 00:32:14,770
that's effect and making a proper probabilistic

462
00:32:16,200 --> 00:32:19,210
because the question how the null space and the constant i can value

463
00:32:19,780 --> 00:32:22,440
so in the constant i can vector so it's not

464
00:32:22,950 --> 00:32:23,380
they on

465
00:32:24,970 --> 00:32:27,050
valid cover invest well valley

466
00:32:28,000 --> 00:32:30,850
as this will turn out to be the covariance so this is why the base

467
00:32:30,850 --> 00:32:35,330
distribution is needed but you can consider the limit the gamma goes to zero and

468
00:32:35,330 --> 00:32:38,890
also gamma doesn't affect the i can vectors so it doesn't affect and the later

469
00:32:39,380 --> 00:32:42,220
analysis but this is the maximum entropy distribution

470
00:32:43,520 --> 00:32:47,930
and this is in fact a guassian random field so it's a guassian random field

471
00:32:48,440 --> 00:32:53,480
the covariance this thing is the inverse of this sparse model this sparse class yeah

472
00:32:55,350 --> 00:32:57,000
almonds determine their

473
00:32:57,690 --> 00:32:58,480
to normalise

474
00:33:01,710 --> 00:33:05,330
okay hits the review how you move between those two representations

475
00:33:05,810 --> 00:33:10,780
minus traceability grant multipliers times the distance is equal to

476
00:33:11,610 --> 00:33:17,400
in our definition because the diagonal the distance is zero trace out is unaffected by diagonal all

477
00:33:18,320 --> 00:33:22,120
it is equal to the trace civil times ti because we've set elements are well

478
00:33:22,410 --> 00:33:24,890
off diagonal three negatively grant multipliers

479
00:33:26,870 --> 00:33:29,130
and this is just this little proof i was doing before

480
00:33:31,710 --> 00:33:32,800
because that's the case

481
00:33:33,630 --> 00:33:36,200
you can cancel those two terms and recover that's

482
00:33:38,980 --> 00:33:40,750
the maximum entropy probability distribution

483
00:33:41,610 --> 00:33:45,400
on the these distance constraints between neighbors is a calcium random field

484
00:33:46,420 --> 00:33:48,950
gauss markov random field with this covariance hit

485
00:33:49,500 --> 00:33:53,800
but the one thing i want you to notice about it is its independence across data features

486
00:33:54,370 --> 00:33:55,520
not data points

487
00:33:56,820 --> 00:33:58,060
i love models like this

488
00:33:58,520 --> 00:34:03,220
because they just blow away all the standard statistical analysis that people tend to do

489
00:34:03,800 --> 00:34:06,180
and show that a lot of it is nonsensical

490
00:34:06,800 --> 00:34:09,830
because it's just about model definition it's a funny thing to do

491
00:34:10,310 --> 00:34:13,820
because you're saying instead saying independence across data points

492
00:34:14,980 --> 00:34:16,820
saying independence across data features

493
00:34:17,220 --> 00:34:22,530
but this is emerging from maximum entropy i haven't made this up i said please give me the distribution

494
00:34:23,570 --> 00:34:25,470
and it just gives me independent across features

495
00:34:26,280 --> 00:34:27,120
so i'm not so

496
00:34:27,520 --> 00:34:29,440
uh this is emerging just from the mass

497
00:34:31,590 --> 00:34:36,080
so at laplacian matrix associated with the neighborhood graph that we've already seen before and

498
00:34:36,080 --> 00:34:40,150
the off-diagonal elements this laplacian within the grant multipliers from moment constraints

499
00:34:43,780 --> 00:34:50,730
the gas metastability specifying is independent across features but most applications against models are applied independently across data points so

500
00:34:51,300 --> 00:34:56,370
people have done this differently this is as you and ghahramani and lafferty who looked

501
00:34:56,470 --> 00:35:00,930
semi supervised learning myself with the detail view and will talk about next time and

502
00:35:01,660 --> 00:35:07,310
charles kemp and josh bound with a relatively sparse gas random fields in the context of

503
00:35:07,690 --> 00:35:08,640
cognitive science

504
00:35:11,010 --> 00:35:12,560
the the point about maximum entropy

505
00:35:13,110 --> 00:35:16,270
if you're not familiar with it is that if you want to know the parameters

506
00:35:16,400 --> 00:35:20,190
the ground multipliers it always equivalent to maximum likelihood

507
00:35:20,690 --> 00:35:21,660
so these two things the

508
00:35:23,360 --> 00:35:27,530
maximum entropy knowing that the ground multipliers is equivalent to maximum likelihood in this model

509
00:35:27,720 --> 00:35:32,620
but there's something funny about maximum likelihood interact in this case because the something i

510
00:35:32,620 --> 00:35:38,300
call the blessing of dimensionality so everyone knows that if you have a large peak

511
00:35:38,350 --> 00:35:39,940
small and that's very bad

512
00:35:40,550 --> 00:35:45,820
because what happens is that your parameters become badly determined you've got all the people

513
00:35:45,840 --> 00:35:48,540
parameters and only in data to determine them

514
00:35:48,540 --> 00:35:58,050
that thank you thank you

515
00:35:58,150 --> 00:36:01,260
OK here's lecture 10 in linear algebra

516
00:36:02,040 --> 00:36:05,580
add 2 important things to do in this lecture

517
00:36:05,780 --> 00:36:10,260
what is the correct an error from lecture 9

518
00:36:10,280 --> 00:36:13,510
so the blackboard with that awful

519
00:36:13,580 --> 00:36:15,100
it is still with us

520
00:36:15,360 --> 00:36:19,620
and the 2nd the big thing to do is to tell you

521
00:36:19,640 --> 00:36:22,400
about the forest subspaces

522
00:36:22,440 --> 00:36:24,330
that come with a matrix

523
00:36:24,360 --> 00:36:29,870
we've seen to subspaces the column space in the null space there's studio

524
00:36:30,000 --> 00:36:32,630
OK 1st cannot

525
00:36:32,720 --> 00:36:39,080
and this is a great way to recapture and correct the previous lecture

526
00:36:39,090 --> 00:36:40,580
so you Members

527
00:36:40,630 --> 00:36:46,980
I was just doing 3 I couldn't have taken a simpler example than our 3

528
00:36:47,120 --> 00:36:53,840
as I wrote down the standard basis but that's the standard basis

529
00:36:53,860 --> 00:36:58,280
the basis the obvious space

530
00:36:58,300 --> 00:36:59,340
for example

531
00:36:59,390 --> 00:37:01,250
three-dimensional space

532
00:37:01,310 --> 00:37:02,170
and then

533
00:37:02,940 --> 00:37:08,390
I wanted to make the point that there was nothing special

534
00:37:10,150 --> 00:37:15,510
about that basis that another basis couldn't have it could have a linear independence it

535
00:37:15,510 --> 00:37:20,510
could span a space there's lots of other bases so I started with these vectors

536
00:37:20,530 --> 00:37:25,200
1 1 2 1 2 2 5 and those were independent

537
00:37:25,250 --> 00:37:28,750
and then I said 3 3 7 would do

538
00:37:28,800 --> 00:37:34,090
because 3 3 7 is the sum of those cells in my and I put

539
00:37:34,090 --> 00:37:36,340
in 3 3 8

540
00:37:36,360 --> 00:37:37,730
I figured

541
00:37:37,750 --> 00:37:43,090
probably have 3 3 7 is on the plane is which I know it's in

542
00:37:43,090 --> 00:37:44,780
the plane with these 2

543
00:37:44,800 --> 00:37:48,700
and probably 3 3 takes a little bit out of the plane

544
00:37:48,750 --> 00:37:53,840
and its independent and gives the basis but after class

545
00:37:53,890 --> 00:37:57,350
to my sorrow

546
00:37:57,370 --> 00:38:03,180
student the way that break that 3rd 2 3 3 8 is not independent

547
00:38:03,250 --> 00:38:06,020
and why did she say that

548
00:38:06,060 --> 00:38:08,630
she didn't actually

549
00:38:08,650 --> 00:38:11,090
take the time didn't have to

550
00:38:11,130 --> 00:38:12,570
the flying

551
00:38:12,620 --> 00:38:18,280
what combinations of this 1 and this 1 gives 3 3 8

552
00:38:18,460 --> 00:38:21,560
she did something else

553
00:38:21,620 --> 00:38:25,690
In other words he looked ahead because she said wait a minute if I look

554
00:38:25,690 --> 00:38:29,470
at that matrix

555
00:38:29,520 --> 00:38:31,850
it's not invertible

556
00:38:31,870 --> 00:38:33,500
that 3rd column can be

557
00:38:34,000 --> 00:38:38,750
independent of the 1st 2 because when I look at that matrix it's got 2

558
00:38:38,750 --> 00:38:41,530
identical roads

559
00:38:41,560 --> 00:38:44,960
I was square matrix

560
00:38:44,970 --> 00:38:46,410
it's rose

561
00:38:46,460 --> 00:38:49,590
are obviously

562
00:38:51,470 --> 00:38:55,070
that makes the columns

563
00:38:55,780 --> 00:39:00,590
there's my when I look at the matrix A that has those

564
00:39:00,620 --> 00:39:01,370
3 columns

565
00:39:02,460 --> 00:39:07,860
0 3 columns can be independent because that matrix is not invertible because it's got

566
00:39:07,870 --> 00:39:09,340
to equal rose

567
00:39:10,470 --> 00:39:13,810
and today's lecture will

568
00:39:13,840 --> 00:39:18,810
can reach the conclusion the great conclusions

569
00:39:18,850 --> 00:39:20,960
that connects

570
00:39:20,970 --> 00:39:22,460
the column space

571
00:39:22,470 --> 00:39:26,440
with the rose space

572
00:39:26,470 --> 00:39:30,620
so all those the row space is now going to be

573
00:39:30,650 --> 00:39:33,840
another 1 of my fundamental subspaces

574
00:39:33,870 --> 00:39:36,650
the row space of this matrix

575
00:39:36,680 --> 00:39:41,440
or this 1 well the row space of this 1 is OK

576
00:39:41,460 --> 00:39:45,290
but the row space of this 1 I'm looking at the rows of the matrix

577
00:39:45,290 --> 00:39:48,900
L anyway I'll have to equal rows

578
00:39:48,940 --> 00:39:52,310
and the row space will be only two-dimensional

579
00:39:52,440 --> 00:39:56,750
the rank of the matrix with these columns will only be 2

580
00:39:56,780 --> 00:40:01,120
so only 2 of those columns columns can be independent

581
00:40:01,760 --> 00:40:06,230
the rows tell me something about the columns in other words

582
00:40:06,320 --> 00:40:08,960
something that I should have noticed and I did

583
00:40:08,990 --> 00:40:10,670
OK so now

584
00:40:10,840 --> 00:40:15,900
let me spend down the 4 fundamental subspaces

585
00:40:15,910 --> 00:40:17,850
so here the floor

586
00:40:17,910 --> 00:40:20,110
fundamental subspace

587
00:40:21,590 --> 00:40:29,990
this is related the heart of this approach to linear algebra to see these 4

588
00:40:30,030 --> 00:40:32,850
subspaces how they're related

589
00:40:32,930 --> 00:40:36,230
so what are they the column space

590
00:40:39,350 --> 00:40:43,260
the vector of the null space

591
00:40:46,110 --> 00:40:52,580
and now comes the rows space something new

592
00:40:52,580 --> 00:40:56,320
when we find the i going to add it

593
00:40:58,840 --> 00:41:03,530
create a list of concepts of we don't have one or going to set the

594
00:41:03,530 --> 00:41:08,300
concept this is to consider the because it's so the of this process was the

595
00:41:09,110 --> 00:41:10,670
which takes a string in l

596
00:41:10,680 --> 00:41:12,340
and encapsulate

597
00:41:12,380 --> 00:41:17,340
series of cyc concepts which this string refused to

598
00:41:17,360 --> 00:41:18,650
so you can see the

599
00:41:18,660 --> 00:41:21,120
reuse that if we run

600
00:41:21,200 --> 00:41:23,520
on a piece of text

601
00:41:23,520 --> 00:41:25,030
it does

602
00:41:25,050 --> 00:41:27,030
something like this so

603
00:41:27,050 --> 00:41:31,070
it's going through for each of these strings at the top

604
00:41:31,080 --> 00:41:38,180
so just one two grams from some brief piece of text and finds these concepts

605
00:41:38,480 --> 00:41:42,930
not all of which are the two meanings of the words like i guess close

606
00:41:42,940 --> 00:41:46,570
to it and i happen to know didn't come up in the piece of text

607
00:41:46,570 --> 00:41:49,840
is talking about the somehow somehow can

608
00:41:49,890 --> 00:41:54,850
the interpretation something which could the text so talk about that later

609
00:41:54,900 --> 00:41:57,860
see the

610
00:41:57,910 --> 00:42:02,090
so now we have there's another class in there

611
00:42:02,110 --> 00:42:06,780
you've got thousand which takes an annotated document which is the document broken up into

612
00:42:06,780 --> 00:42:08,400
ingram's with this

613
00:42:08,490 --> 00:42:15,840
concepts and at its concepts to work it's them both in this form into the

614
00:42:17,290 --> 00:42:20,880
and the cycle form like this for the purposes of

615
00:42:23,820 --> 00:42:28,940
was that it was over the easy

616
00:42:34,100 --> 00:42:36,370
so we're going through the strings

617
00:42:36,420 --> 00:42:39,660
in the document or the ingrams and the

618
00:42:39,680 --> 00:42:41,980
by the side of the grammar

619
00:42:42,020 --> 00:42:45,950
getty and then getting all the concert

620
00:42:46,000 --> 00:42:48,840
i'm going through that and those concepts two

621
00:42:48,860 --> 00:42:51,830
a concept for the document

622
00:42:51,840 --> 00:42:56,180
so it's like a tag found a new with meetings

623
00:42:56,220 --> 00:42:57,690
this is

624
00:42:57,740 --> 00:43:02,720
the result of doing this for this brief article cycles from the news

625
00:43:02,730 --> 00:43:04,540
two or three days ago

626
00:43:06,830 --> 00:43:10,990
these are the results that you see that some of these concepts

627
00:43:11,030 --> 00:43:20,740
if group for sure is relevant first-order administrative free agent summer meeting leadership something friday

628
00:43:20,750 --> 00:43:22,790
sitting of paging

629
00:43:22,810 --> 00:43:24,580
being social learning

630
00:43:24,630 --> 00:43:28,890
beijing municipality OK clearly relevant

631
00:43:28,940 --> 00:43:35,080
things which are relevant to this quote is not doing any disambiguation what's

632
00:43:35,090 --> 00:43:40,720
in terms of doing all the possible meanings disambiguation is this too as an exercise

633
00:43:40,720 --> 00:43:44,270
for the reader or for me when i'm actually doing my job

634
00:43:44,310 --> 00:43:51,510
so we're really simple but we've got some pretty useful tags somewhere but what's even

635
00:43:51,510 --> 00:43:53,580
more useful is it

636
00:43:53,600 --> 00:43:56,300
o things like what sort of thing

637
00:43:56,300 --> 00:43:59,600
beijing is because then we could say does this

638
00:43:59,680 --> 00:44:03,390
article mentions in the cities we can say that about because we don't know if

639
00:44:03,390 --> 00:44:05,850
it's going be cities but we generally

640
00:44:06,080 --> 00:44:07,040
we can

641
00:44:07,050 --> 00:44:10,940
so give me all the news articles which mention cities will give me all the

642
00:44:10,940 --> 00:44:13,440
news at ten

643
00:44:15,170 --> 00:44:18,070
we we don't have that we don't have that in this way we can

644
00:44:19,930 --> 00:44:25,340
we aware of them and this is for those of you who are not a

645
00:44:25,340 --> 00:44:26,050
i e

646
00:44:26,050 --> 00:44:29,760
in particular it means you see something like this

647
00:44:29,770 --> 00:44:32,070
i looked at

648
00:44:32,070 --> 00:44:38,630
this representation of a concept cloud revealed the ottomans this is just some document were

649
00:44:38,630 --> 00:44:42,480
for ingrams broken out of it and concepts additional is grams

650
00:44:42,540 --> 00:44:48,050
we've looked at a site we look in the right side with service which is

651
00:44:48,760 --> 00:44:51,500
as of this find we service

652
00:44:51,500 --> 00:44:53,140
and of course we have language

653
00:44:53,150 --> 00:44:56,550
like semantic networks and so on

654
00:44:56,580 --> 00:44:59,230
and also in softer we can find

655
00:44:59,310 --> 00:45:01,230
a lot of i mean that's a

656
00:45:01,240 --> 00:45:06,760
projectile of cleanup these exactly with this experts how how to help

657
00:45:06,770 --> 00:45:14,080
view to softer is this huge graph huge changes through time

658
00:45:18,630 --> 00:45:21,000
what kind of what types of network we

659
00:45:21,010 --> 00:45:26,090
so we have the before mentioned already directed networks and we have also on

660
00:45:27,890 --> 00:45:32,830
then with the graph this would mean that multiple edges are between two nodes this

661
00:45:32,830 --> 00:45:38,130
can also be an interesting aspect

662
00:45:38,150 --> 00:45:42,470
then hypergraphs edges connecting with people so that is not just

663
00:45:43,410 --> 00:45:46,470
rdf movement usually connect just two notes but

664
00:45:46,480 --> 00:45:51,350
relationship can be a bit more complex than just connecting two nodes

665
00:45:51,390 --> 00:45:53,860
bipartite graph so this would be

666
00:45:53,880 --> 00:45:55,840
this type of networks where we have

667
00:45:55,850 --> 00:45:58,770
two types of objects which are interlinked

668
00:45:58,840 --> 00:46:00,260
weighted network so that

669
00:46:00,270 --> 00:46:05,960
weights on top of them and eventually we have also this evolving door networks which

670
00:46:06,500 --> 00:46:11,350
we are discussing here in the context of eq two there

671
00:46:11,370 --> 00:46:13,830
nodes and edges are

672
00:46:14,100 --> 00:46:19,220
it are even the weights would be basically over the whole network is change for

673
00:46:19,230 --> 00:46:20,130
this would be

674
00:46:22,420 --> 00:46:26,650
quick selection selection of types of networks we are dealing with

675
00:46:27,740 --> 00:46:32,290
because network which i mean you could you have endless

676
00:46:32,340 --> 00:46:35,920
types of just we didn't put this

677
00:46:35,930 --> 00:46:38,140
the slides

678
00:46:38,200 --> 00:46:39,770
process or

679
00:46:39,830 --> 00:46:45,540
the practices with sort of course it is also

680
00:46:45,600 --> 00:46:46,990
the first option

681
00:46:47,630 --> 00:46:53,580
a little bit on the historical sites so traditional approaches would

682
00:46:54,430 --> 00:46:57,990
the ones from sociologists

683
00:46:58,000 --> 00:47:03,230
which were first studying networks in the past and the idea was basically tools to

684
00:47:03,230 --> 00:47:09,750
study connections between people to understand the functioning of the society usually these societies were

685
00:47:11,020 --> 00:47:17,610
so in this presentation people certain people are nodes interactions are edges with the interaction

686
00:47:17,620 --> 00:47:18,770
this was

687
00:47:22,910 --> 00:47:27,120
in the past and it was very hard to get this this kind of data

688
00:47:27,130 --> 00:47:32,640
and typical questions were actually centrality and connec tivity so

689
00:47:32,650 --> 00:47:37,890
between the nodes of centrality in the race important also nowadays they drink and with

690
00:47:37,890 --> 00:47:43,350
all this algorithm to estimate importance of the web pages but some are now

691
00:47:43,370 --> 00:47:47,160
this notion of centrality went far beyond this old

692
00:47:48,740 --> 00:47:53,660
so in the past this was limited to small graphs like in the range of

693
00:47:54,400 --> 00:48:02,000
notes and people are studying properties of individual nodes and edges now

694
00:48:02,010 --> 00:48:08,260
this a new approach so they what is interesting nowadays for the research community

695
00:48:08,270 --> 00:48:14,540
it is actually studying the large networks like that internet online social networks we'd really

696
00:48:14,540 --> 00:48:21,770
millions and hundreds of millions of nodes later on you show examples of

697
00:48:23,150 --> 00:48:24,700
the old questions are

698
00:48:24,720 --> 00:48:28,540
not really useful anymore so let's see if in the past we ask the question

699
00:48:28,550 --> 00:48:29,640
what happens

700
00:48:29,700 --> 00:48:32,280
if a node u is removed now

701
00:48:32,290 --> 00:48:37,260
the relevant question would be what percentage of nodes we need to remove tools

702
00:48:37,270 --> 00:48:40,230
that's a fact some connected so it's

703
00:48:40,280 --> 00:48:46,910
seen similar kind of related but subtly and far from being the same question

704
00:48:46,960 --> 00:48:50,450
and actually the focus is moving

705
00:48:50,500 --> 00:48:54,080
from the single note to the statistical properties of the whole set

706
00:48:54,090 --> 00:48:59,270
sub networks and description of the whole of the whole

707
00:48:59,280 --> 00:49:01,230
but the set b

708
00:49:01,250 --> 00:49:06,990
and certainly we cannot draw any more small networks so this is really an object

709
00:49:06,990 --> 00:49:11,670
which we can just study and we cannot visualize easily actually we are dealing with

710
00:49:11,670 --> 00:49:13,620
hundreds of millions

711
00:49:13,620 --> 00:49:20,470
as many parameters as we let us you need help as index into the

712
00:49:20,490 --> 00:49:21,620
OK this

713
00:49:21,630 --> 00:49:25,190
assume independence between the position

714
00:49:25,440 --> 00:49:29,880
this zero order markov

715
00:49:29,900 --> 00:49:30,880
OK so

716
00:49:31,360 --> 00:49:37,490
o can estimate the parameters vary he from training set you can simply look at

717
00:49:37,500 --> 00:49:40,810
whole lot here at each position

718
00:49:40,860 --> 00:49:46,080
three b c i mean you can't which an example

719
00:49:46,720 --> 00:49:50,660
we look all the positions led here

720
00:49:50,860 --> 00:49:55,650
and then the entry for each of the

721
00:49:57,740 --> 00:50:02,620
and for classification you would estimate this

722
00:50:02,620 --> 00:50:08,140
four years for the positive class and for the negative class separate

723
00:50:08,160 --> 00:50:10,190
OK then you take the

724
00:50:10,200 --> 00:50:15,340
look what ratio for discrimination three take the probability for the positive class divided by

725
00:50:15,340 --> 00:50:19,590
the probability of the negative log pick the lock this is something which is positive

726
00:50:19,590 --> 00:50:26,620
or negative right depending on whether positive or negative you can classify all positive one

727
00:50:27,220 --> 00:50:31,840
this is a very simple example of the probabilistic one

728
00:50:31,880 --> 00:50:35,220
so i mean that what much more complex models

729
00:50:35,300 --> 00:50:37,800
just assume we have an

730
00:50:37,810 --> 00:50:44,430
how can we use these kind of model for defining a

731
00:50:44,490 --> 00:50:47,910
i actually other questions for this you

732
00:50:47,950 --> 00:50:50,320
for every

733
00:50:50,320 --> 00:50:55,700
OK so the idea is that the the second of the idea is that the

734
00:50:55,930 --> 00:51:03,310
takes some parameters so we estimate theta point by the simple estimation technique for you

735
00:51:03,320 --> 00:51:08,130
so we have some maximum likelihood estimate for this problem

736
00:51:08,890 --> 00:51:10,210
it's the

737
00:51:10,220 --> 00:51:17,780
and then for every teacher every week reading we compute the so called fisher score

738
00:51:17,790 --> 00:51:23,150
this is the derivative of the log p theta x

739
00:51:23,230 --> 00:51:25,910
at the position you take it

740
00:51:25,960 --> 00:51:27,600
OK so we need

741
00:51:27,600 --> 00:51:29,900
i mean you had the probability

742
00:51:29,920 --> 00:51:31,410
we have the property

743
00:51:33,210 --> 00:51:34,610
the probabilistic model here

744
00:51:34,640 --> 00:51:38,360
we take the derivative with respect to all the time which we have in the

745
00:51:39,600 --> 00:51:41,230
at position

746
00:51:41,360 --> 00:51:44,070
well i like

747
00:51:44,430 --> 00:51:50,070
this is a vector which is as long as we have the parameters in the

748
00:51:51,330 --> 00:51:53,300
the use of this feature that

749
00:51:55,710 --> 00:52:00,140
i mean this feature for discriminating

750
00:52:01,490 --> 00:52:02,560
OK so

751
00:52:02,560 --> 00:52:04,380
from this we can define

752
00:52:04,390 --> 00:52:08,990
so we take this feature that you have some additional metric here because of the

753
00:52:08,990 --> 00:52:13,430
fisher information metric and the future of the other

754
00:52:13,440 --> 00:52:17,870
essentially what you think

755
00:52:17,870 --> 00:52:26,360
this derivative this computing some kind of picture describing how the particular example is using

756
00:52:26,370 --> 00:52:27,480
the term

757
00:52:29,030 --> 00:52:32,820
this feature vector is then used to define the the

758
00:52:32,890 --> 00:52:38,900
typically the fisher information metric is very important and i think of the garden and

759
00:52:38,900 --> 00:52:39,850
this is

760
00:52:41,500 --> 00:52:45,440
whitening formation

761
00:52:45,920 --> 00:52:51,720
so this is escort describes how each parameter contributes to the process of generating a

762
00:52:51,720 --> 00:52:55,350
particular data the intuitive idea behind

763
00:52:55,380 --> 00:53:00,690
so the project can be reused the fisher information metric is invariant under change of

764
00:53:00,690 --> 00:53:05,550
parameterisation of the model you can come up with a the different privatisation of a

765
00:53:05,550 --> 00:53:09,270
probabilistic model and

766
00:53:09,310 --> 00:53:15,040
and the kernel classifier going to protect problem model that contains the label of the

767
00:53:15,040 --> 00:53:16,910
latent variable is

768
00:53:16,940 --> 00:53:22,220
at least as good as function of days but the probabilistic model

769
00:53:22,260 --> 00:53:29,540
any hope of course that might be

770
00:53:29,540 --> 00:53:34,530
OK so in practice we can just compute the features

771
00:53:34,550 --> 00:53:36,590
i can just replace

772
00:53:36,600 --> 00:53:40,440
fisher information metric with the identity matrix

773
00:53:40,460 --> 00:53:42,120
so we don't need to

774
00:53:42,210 --> 00:53:49,670
so let me show you want a particular example where we compute the fisher kernel

775
00:53:50,630 --> 00:53:55,170
one particular probabilistic model which is that it was his position the brain

776
00:53:55,270 --> 00:53:59,780
and it turns out of out at end up with a great degree

777
00:53:59,820 --> 00:54:01,560
i think we want

778
00:54:03,380 --> 00:54:07,960
i don't think that we have a sequence of fixed length link

779
00:54:08,000 --> 00:54:11,090
we have the position specific going activities like

780
00:54:11,090 --> 00:54:13,920
so this is the probability

781
00:54:13,940 --> 00:54:16,210
even with

782
00:54:16,300 --> 00:54:18,110
so fisher score

783
00:54:18,110 --> 00:54:19,840
and carry constantly

784
00:54:19,850 --> 00:54:22,670
CNF of be this university and

785
00:54:22,690 --> 00:54:26,720
this district is one of the

786
00:54:27,140 --> 00:54:35,900
so this is the procedure for computing disjunctive sorry distribution the distribution of conjunctive forms

787
00:54:36,040 --> 00:54:40,550
assume that the input that two

788
00:54:40,720 --> 00:54:45,000
former which already conjunctive form

789
00:54:47,500 --> 00:54:50,140
a a c and d

790
00:54:51,500 --> 00:54:53,040
you don't have to do any

791
00:54:53,040 --> 00:54:55,100
so you just be

792
00:54:55,160 --> 00:54:57,140
b over c and p

793
00:54:57,160 --> 00:54:58,790
it's just that

794
00:54:58,850 --> 00:55:02,120
corresponds to to in

795
00:55:02,180 --> 00:55:05,440
this debate

796
00:55:05,540 --> 00:55:06,660
it says that

797
00:55:08,040 --> 00:55:15,910
of the conjunction though

798
00:55:16,250 --> 00:55:18,660
what you do is you

799
00:55:18,660 --> 00:55:21,560
from the conjunction calling for me

800
00:55:22,830 --> 00:55:24,940
one of the key

801
00:55:26,310 --> 00:55:28,520
and symmetrically in b

802
00:55:28,540 --> 00:55:30,960
the conjunction new distributed

803
00:55:30,980 --> 00:55:32,710
eighty or

804
00:55:32,750 --> 00:55:34,870
if there are

805
00:55:39,660 --> 00:55:42,230
we already have

806
00:55:42,230 --> 00:55:45,330
the class disjunctions

807
00:55:53,830 --> 00:56:03,200
for example when we apply the procedure interested me

808
00:56:03,230 --> 00:56:05,660
to say this

809
00:56:05,660 --> 00:56:09,960
simple example

810
00:56:10,070 --> 00:56:13,680
so you just check case by case

811
00:56:13,680 --> 00:56:15,580
if you calculate

812
00:56:15,580 --> 00:56:17,850
never is

813
00:56:17,870 --> 00:56:19,580
we call it

814
00:56:19,620 --> 00:56:21,750
call recursively

815
00:56:21,930 --> 00:56:24,390
because it is a question

816
00:56:26,520 --> 00:56:30,060
distributed on

817
00:56:30,080 --> 00:56:31,980
you know

818
00:56:31,980 --> 00:56:34,480
x y

819
00:56:39,310 --> 00:56:42,250
can try to calculate yourself

820
00:56:42,310 --> 00:56:44,310
see that

821
00:56:44,330 --> 00:56:46,520
this is actually

822
00:56:46,640 --> 00:56:51,310
x x

823
00:56:51,390 --> 00:56:53,830
and then you just distribute

824
00:56:53,830 --> 00:56:59,600
first distribute this of this

825
00:57:03,250 --> 00:57:06,410
x x

826
00:57:06,430 --> 00:57:10,140
y x

827
00:57:10,180 --> 00:57:14,890
and remember the disjunction is associated to

828
00:57:15,600 --> 00:57:18,580
quite right but this does is x

829
00:57:18,710 --> 00:57:21,160
x y

830
00:57:21,160 --> 00:57:28,330
i think it's very simple operation

831
00:57:28,560 --> 00:57:33,980
and you can also

832
00:57:33,980 --> 00:57:37,180
so disjunctive normal form

833
00:57:37,200 --> 00:57:39,770
really the dual conjunctive normal form

834
00:57:39,790 --> 00:57:43,000
which is the disjunction conjunction

835
00:57:46,160 --> 00:57:49,620
so it is the conjunction which is disjunction

836
00:57:49,640 --> 00:57:53,560
sorry it took the conjunction inside the disjunction is

837
00:57:53,620 --> 00:57:55,980
oppose from the tools

838
00:57:56,020 --> 00:57:58,620
and again you can just

839
00:57:58,660 --> 00:58:03,500
the transformation instead of distributing

840
00:58:03,580 --> 00:58:06,390
disjunction over conjunction

841
00:58:06,460 --> 00:58:09,080
we used to be

842
00:58:09,080 --> 00:58:12,310
also distribution

843
00:58:14,270 --> 00:58:19,080
and the algorithm is that exercise

844
00:58:19,210 --> 00:58:27,310
for any questions so far

845
00:58:30,080 --> 00:58:34,290
now i'm going to look at the proof procedure called the rest

846
00:58:36,370 --> 00:58:39,730
so what does it you if you give it

847
00:58:44,370 --> 00:58:50,560
well this is what this formula is unsatisfiable

848
00:58:52,370 --> 00:58:53,910
so remember that

849
00:58:53,930 --> 00:58:56,390
satisfiability is two

850
00:58:57,770 --> 00:59:00,410
this means that this procedure can be used to

851
00:59:00,430 --> 00:59:03,120
for that

852
00:59:04,500 --> 00:59:06,250
taking the negation

853
00:59:07,540 --> 00:59:12,060
so it is said by

854
00:59:14,620 --> 00:59:16,560
to simplify

855
00:59:16,560 --> 00:59:18,120
president and

856
00:59:18,160 --> 00:59:21,230
we usually represent

857
00:59:21,230 --> 00:59:25,060
conjunctive normal form as a set of sets

858
00:59:25,060 --> 00:59:27,700
remember that conjunction is

859
00:59:27,710 --> 00:59:29,770
commutative and associative

860
00:59:29,850 --> 00:59:33,390
the order in which don't

861
00:59:33,410 --> 00:59:37,540
parent is is around which can jump

862
00:59:39,520 --> 00:59:41,520
this means that can represent

863
00:59:41,540 --> 00:59:44,140
the conjunction has just said

864
00:59:44,160 --> 00:59:45,700
for example

865
00:59:45,710 --> 00:59:47,640
x y

866
00:59:48,620 --> 00:59:52,600
six people and x

867
00:59:54,410 --> 00:59:56,040
six people into

868
00:59:59,160 --> 01:00:00,960
the order and

869
01:00:01,020 --> 01:00:02,910
so our

870
01:00:02,930 --> 01:00:05,460
and the way in which you write down

871
01:00:05,460 --> 01:00:06,700
one doesn't

872
01:00:07,600 --> 01:00:10,500
this means that

873
01:00:10,540 --> 01:00:14,040
this actually the same as the set because in the set

874
01:00:14,040 --> 01:00:16,670
xe axis so that's one

875
01:00:17,840 --> 01:00:19,580
perhaps this

876
01:00:19,590 --> 01:00:21,290
that's about the last time in the

877
01:00:21,370 --> 01:00:22,990
i want to talk about these things

878
01:00:23,000 --> 01:00:27,320
it's more interesting to talk about why self-reliance i'm going to say let's just fall

879
01:00:27,320 --> 01:00:29,200
on the slide the slide

880
01:00:29,230 --> 01:00:31,750
imitates what's going on here

881
01:00:31,750 --> 01:00:34,170
so let's see if got a couple of them i want to try let's look

882
01:00:34,170 --> 01:00:37,090
at for starters all b

883
01:00:37,130 --> 01:00:41,760
well look at all be someone started the origin and come out to the point

884
01:00:41,760 --> 01:00:45,180
b and i've got a vector on this it's emanating from the origin out to

885
01:00:46,040 --> 01:00:48,570
and what does it say to do it says

886
01:00:48,670 --> 01:00:51,990
i define vector from the origin to the point of the line

887
01:00:52,040 --> 01:00:56,460
she is the smallest set of integers and said oh no comment so i put

888
01:00:56,460 --> 01:01:00,080
the origin which conveniently here is the origin

889
01:01:00,130 --> 01:01:05,820
out to the point along that direction so the vector all be as written we'll

890
01:01:05,820 --> 01:01:06,950
go from there

891
01:01:06,960 --> 01:01:09,690
how to what is it goes to one

892
01:01:09,700 --> 01:01:11,340
zero i have

893
01:01:11,360 --> 01:01:13,490
but then it says on

894
01:01:13,570 --> 01:01:18,430
let's see no commas enclose in brackets and clear the fractions they don't like to

895
01:01:18,430 --> 01:01:23,380
have fractions when you're talking about lines so clear the fractions multiply through

896
01:01:23,810 --> 01:01:25,960
so the natl just double two

897
01:01:25,980 --> 01:01:29,820
zero one and you could say what are they doing actually if you can imagine

898
01:01:29,820 --> 01:01:31,430
going out one more

899
01:01:31,440 --> 01:01:35,130
one more unit cell this line would actually go through the point

900
01:01:35,170 --> 01:01:39,890
two zero one so you're just looking at the projection along here just multiplying and

901
01:01:39,890 --> 01:01:42,830
then it says no commas enclose in brackets

902
01:01:42,950 --> 01:01:45,620
OK this is called the brackets

903
01:01:45,630 --> 01:01:49,520
the stable like things are called brackets

904
01:01:49,570 --> 01:01:52,440
so that's the direction to the direction or b

905
01:01:52,440 --> 01:01:53,730
let's do the

906
01:01:53,750 --> 01:01:55,440
see what's the other going to do

907
01:01:55,450 --> 01:01:57,290
was that a all

908
01:01:57,330 --> 01:01:59,330
they all

909
01:01:59,380 --> 01:02:02,570
all right so it says you have to move the origin to the basis vectors

910
01:02:02,570 --> 01:02:04,750
of i want to do a all

911
01:02:04,770 --> 01:02:05,930
i have to

912
01:02:05,980 --> 01:02:08,810
i have to put the origin of here

913
01:02:08,810 --> 01:02:13,000
at the tail of vectors of this is the origin i'm moving actually what i'm

914
01:02:13,000 --> 01:02:15,250
not moving anywhere in the

915
01:02:15,260 --> 01:02:19,310
x axis but i'm moving minus one and the y axis and minus one and

916
01:02:19,310 --> 01:02:22,130
the easy axis in order to get this direction

917
01:02:22,210 --> 01:02:26,050
so really going zero on the axis i'm going minus one in the y

918
01:02:26,150 --> 01:02:28,050
one minus one in the z

919
01:02:28,060 --> 01:02:31,880
and you can see since we don't use commas putting minus signs would look

920
01:02:31,940 --> 01:02:33,070
cumbersome so

921
01:02:33,080 --> 01:02:39,520
in crystallographic notation the minus sign is represented as a macro macro align over so

922
01:02:39,520 --> 01:02:45,980
this is zero one bar one bar that's equal to minus one but crystal crystallography

923
01:02:46,060 --> 01:02:51,290
use the term one bar so this is all one bar one bar for this

924
01:02:53,880 --> 01:02:55,770
a down to towards

925
01:02:55,790 --> 01:02:57,250
so that's

926
01:02:57,300 --> 01:03:01,860
that's all that stuff and then it says i assume we can denote entire family

927
01:03:01,860 --> 01:03:03,310
of directions by

928
01:03:04,250 --> 01:03:05,940
so let's do that

929
01:03:05,940 --> 01:03:07,840
i want to suppose i wanted to say

930
01:03:08,000 --> 01:03:13,180
when i got there all of the body diagonals how i get the body diagonals

931
01:03:13,230 --> 01:03:14,930
body diagonals

932
01:03:14,940 --> 01:03:20,840
this is the origin i want to go up the body diagonal it would be

933
01:03:20,840 --> 01:03:25,400
out one unit one unit of one unit so it would be one one one

934
01:03:25,400 --> 01:03:29,820
would be a body diagonal right but suppose i want to say all body diagonals

935
01:03:29,820 --> 01:03:34,340
that's one one one this direction one one one that direction i want to get

936
01:03:34,340 --> 01:03:37,940
this model diagonal going to get that body day how do i do that i

937
01:03:37,940 --> 01:03:40,450
can do that by putting carrots

938
01:03:40,460 --> 01:03:44,070
these are called carrots those of you who have done some editing proofreading all these

939
01:03:45,060 --> 01:03:49,740
OK so care so this is all body diagonals

940
01:03:49,860 --> 01:03:51,690
this is all body diagonals

941
01:03:56,050 --> 01:03:58,860
we could do a few others well-read suppose i wanted to do

942
01:03:58,870 --> 01:04:01,640
all the face diagonals

943
01:04:01,690 --> 01:04:04,880
all the phase diagram for like this like this

944
01:04:04,880 --> 01:04:06,020
and then over here

945
01:04:06,050 --> 01:04:09,180
like this like this well what's face diagonals

946
01:04:09,190 --> 01:04:10,690
zero one

947
01:04:10,690 --> 01:04:14,560
zero one so all face diagonals would be

948
01:04:16,140 --> 01:04:20,040
face diagonals

949
01:04:20,770 --> 01:04:22,720
what about all cube edges

950
01:04:22,780 --> 01:04:26,480
what's the cube edge cube edge could be this one

951
01:04:26,540 --> 01:04:28,170
zero zero one

952
01:04:28,210 --> 01:04:30,100
so this would be all

953
01:04:34,480 --> 01:04:38,270
so what does it mean it means write this out in full so that's all

954
01:04:38,270 --> 01:04:40,780
part partly all one

955
01:04:41,410 --> 01:04:44,830
all one bar

956
01:04:44,920 --> 01:04:48,410
all one o c and using alpha zero

957
01:04:48,570 --> 01:04:53,020
i'm clue when you enter crystallography first talk that's how they talk all one bar

958
01:04:53,030 --> 01:04:56,310
so can be really happy if you end up in an elevator with a bunch

959
01:04:56,310 --> 01:05:00,920
of crystallography first OK and there's two others here one o o

960
01:05:00,940 --> 01:05:04,260
and one bar all o

961
01:05:04,270 --> 01:05:06,170
well how many faces are the

962
01:05:06,180 --> 01:05:07,480
six how many

963
01:05:07,540 --> 01:05:13,210
face edges six and we're just going to the the permutations and combinations gives you

964
01:05:13,210 --> 01:05:18,310
exactly the number that you should have this is mathematics imitating reality

965
01:05:18,310 --> 01:05:19,730
that's good

966
01:05:19,790 --> 01:05:25,040
that's good and likewise here if you go through all permutations combinations of zero one

967
01:05:25,060 --> 01:05:30,240
and one bar you'll end up with all the possibilities for face diagonals all the

968
01:05:30,240 --> 01:05:33,890
possibility for body diagonals i think i've got

969
01:05:33,910 --> 01:05:36,640
cartoon here that shows there is

970
01:05:36,690 --> 01:05:38,830
it shows all the different

971
01:05:38,890 --> 01:05:43,480
diagonals that are possible when you mix one and one bar

972
01:05:43,540 --> 01:05:45,220
OK that's good

973
01:05:45,240 --> 01:05:48,990
so that so far so good that's pretty simple now let's look at planes

974
01:05:49,040 --> 01:05:54,260
let's look at plans for planes we use something called the miller indices

975
01:05:54,310 --> 01:05:57,940
miller indices for describing planes

976
01:05:57,940 --> 01:05:59,370
and they're named after

977
01:06:00,270 --> 01:06:02,690
british mineralogist

978
01:06:02,710 --> 01:06:04,850
william howells miller

979
01:06:04,850 --> 01:06:07,250
the dangerous tool because it's only true

980
01:06:08,060 --> 01:06:09,090
the force

981
01:06:12,100 --> 01:06:14,680
spring forces are also conservative

982
01:06:14,720 --> 01:06:17,190
but for instance friction

983
01:06:17,240 --> 01:06:19,450
it's not a conservative force

984
01:06:19,550 --> 01:06:21,500
if i move an object

985
01:06:21,510 --> 01:06:23,890
from here

986
01:06:23,920 --> 01:06:26,910
two here

987
01:06:26,950 --> 01:06:30,000
let's suppose i moved this object

988
01:06:30,010 --> 01:06:32,440
and i go along the straight line

989
01:06:32,450 --> 01:06:35,620
and friction is doing negative work i'm doing positive work

990
01:06:35,680 --> 01:06:36,780
now suppose

991
01:06:36,800 --> 01:06:41,150
i go from here to here for this routing

992
01:06:41,190 --> 01:06:43,300
you can see that the work i have to do

993
01:06:43,330 --> 01:06:47,420
is much more

994
01:06:47,420 --> 01:06:48,930
friction is not

995
01:06:48,980 --> 01:06:52,740
the conservative for the frictional force remains constant

996
01:06:52,750 --> 01:06:57,300
dependent on the friction the kinetic friction coefficient is always the same

997
01:06:57,320 --> 01:07:00,420
the frictional force that i have to overcome as i move

998
01:07:00,470 --> 01:07:03,940
so if i go all the way here and then all the way back

999
01:07:03,950 --> 01:07:06,740
to this point where i wanted to be then i have done a lot more

1000
01:07:06,740 --> 01:07:08,140
work than if i go

1001
01:07:08,160 --> 01:07:11,690
along the shortest distance friction is a classic example

1002
01:07:11,710 --> 01:07:15,160
of the force is not conservative

1003
01:07:15,910 --> 01:07:16,780
you look

1004
01:07:19,710 --> 01:07:25,590
the sum of gravitational potential energy and kinetic energy is conserved for gravitational force

1005
01:07:25,620 --> 01:07:27,570
then it is immediately obvious

1006
01:07:27,580 --> 01:07:30,970
where we put the zero of kinetic energy

1007
01:07:30,970 --> 01:07:33,800
the zero of kinetic energy is when

1008
01:07:33,930 --> 01:07:36,960
the object has no velocity because kinetic energy

1009
01:07:37,010 --> 01:07:39,010
equals one half and

1010
01:07:39,060 --> 01:07:40,030
three squared

1011
01:07:40,050 --> 01:07:41,940
if you object has no velocity

1012
01:07:41,980 --> 01:07:44,240
then there is no kinetic energy

1013
01:07:44,260 --> 01:07:45,510
how about

1014
01:07:45,620 --> 01:07:47,350
potential energy

1015
01:07:47,370 --> 01:07:50,060
well you say sure potential energy

1016
01:07:50,090 --> 01:07:53,770
must be zero when y is zero because that's the way

1017
01:07:53,810 --> 01:07:55,070
that we defined it

1018
01:07:55,080 --> 01:07:56,540
you see MG y

1019
01:07:56,560 --> 01:07:59,950
its gravitational potential energy so you would think that

1020
01:08:00,700 --> 01:08:01,700
this is zero

1021
01:08:01,710 --> 01:08:02,520
when one

1022
01:08:02,540 --> 01:08:05,280
is there not unreasonable thing to think

1023
01:08:05,290 --> 01:08:06,950
where is y is zero

1024
01:08:06,990 --> 01:08:10,390
is why zero at the surface of the earth

1025
01:08:10,430 --> 01:08:13,550
which is why zero at the floor of twenty six one hundred

1026
01:08:13,580 --> 01:08:17,310
which is why zero here which is why zero roof

1027
01:08:18,680 --> 01:08:21,810
completely free to choose where

1028
01:08:21,820 --> 01:08:24,240
you put you equals zero

1029
01:08:24,240 --> 01:08:25,410
it doesn't matter

1030
01:08:25,460 --> 01:08:26,490
as long

1031
01:08:26,500 --> 01:08:29,840
as point a and point b are close enough together

1032
01:08:29,960 --> 01:08:32,810
that the gravitational acceleration g

1033
01:08:33,010 --> 01:08:35,760
very closely the same for both points

1034
01:08:35,890 --> 01:08:41,030
the only thing that matters then is how far they are separated vertically the only

1035
01:08:41,030 --> 01:08:43,290
thing that matters is that you be

1036
01:08:44,250 --> 01:08:46,010
you a

1037
01:08:46,010 --> 01:08:46,890
you be

1038
01:08:46,900 --> 01:08:50,590
you a would-be be gh

1039
01:08:50,680 --> 01:08:52,920
is only the age that matters

1040
01:08:52,960 --> 01:08:53,890
and so

1041
01:08:53,900 --> 01:08:55,440
you can then

1042
01:08:55,480 --> 01:08:57,350
i simply choose you zero

1043
01:08:59,300 --> 01:09:00,310
one two

1044
01:09:00,320 --> 01:09:01,910
it's easy to see

1045
01:09:03,540 --> 01:09:05,520
i he point eight

1046
01:09:05,660 --> 01:09:07,960
and i viewpoint b

1047
01:09:08,120 --> 01:09:10,680
and suppose this separation

1048
01:09:10,730 --> 01:09:12,890
was a ge

1049
01:09:14,290 --> 01:09:15,900
if you prefer to call

1050
01:09:15,910 --> 01:09:17,760
zero potential energy

1051
01:09:19,200 --> 01:09:21,330
i have no problem with that

1052
01:09:21,350 --> 01:09:22,780
so we can call this

1053
01:09:22,800 --> 01:09:25,590
you equal zero here

1054
01:09:25,600 --> 01:09:27,680
then you would have to call this you

1055
01:09:27,690 --> 01:09:30,510
you have to call it plus and gh

1056
01:09:30,660 --> 01:09:33,560
if you say no i don't want to do that i want to call this

1057
01:09:34,270 --> 01:09:35,800
that's fine

1058
01:09:35,860 --> 01:09:39,330
then this becomes minus ngh

1059
01:09:39,370 --> 01:09:41,250
if you prefer to call this zero

1060
01:09:41,250 --> 01:09:42,510
that's fine too

1061
01:09:42,560 --> 01:09:44,360
and this will have a positive

1062
01:09:44,370 --> 01:09:48,910
gravitational potential energy and this will have one that is higher than this one by

1063
01:09:48,910 --> 01:09:50,060
this amount

1064
01:09:50,120 --> 01:09:52,560
if you say really i like to call this hero

1065
01:09:52,570 --> 01:09:54,350
of course the same holds

1066
01:09:54,370 --> 01:09:57,010
what matters is what the difference between

1067
01:09:57,010 --> 01:09:59,010
potential energy is that

1068
01:09:59,020 --> 01:10:03,620
is what we need when we apply the conservation of mechanical energy that is what

1069
01:10:03,620 --> 01:10:05,800
we need in order to evaluate

1070
01:10:05,930 --> 01:10:11,740
do object changes its kinetic energy so where you choose your zero is completely up

1071
01:10:11,740 --> 01:10:13,920
to you

1072
01:10:13,920 --> 01:10:14,910
as long

1073
01:10:14,920 --> 01:10:16,840
as a and b are close enough

1074
01:10:16,860 --> 01:10:18,410
so that there is no

1075
01:10:18,410 --> 01:10:22,320
a noticeable difference in the gravitational acceleration g

1076
01:10:22,440 --> 01:10:25,460
before the end of this our

1077
01:10:25,510 --> 01:10:27,820
also evaluate the situation

1078
01:10:27,840 --> 01:10:33,010
that g is changing when you go far away from the earth g is changing

1079
01:10:33,010 --> 01:10:37,580
12 . 0 1 1 as the atomic mass so we have here they all

1080
01:10:37,580 --> 01:10:46,560
have the same the same proton number but OK but different a different name which

1081
01:10:46,560 --> 01:10:54,400
is means a number of neutrons vary right so that means they're found on the

1082
01:10:54,400 --> 01:11:00,400
same place on the periodic table so far from the same place it's so Topol

1083
01:11:00,420 --> 01:11:08,080
so these are called isotropic isotopes have the same chemical identity but different nuclear

1084
01:11:08,660 --> 01:11:10,600
the properties

1085
01:11:11,200 --> 01:11:15,660
1 of the units here we write 12 . 0 1 1 writing 12 .

1086
01:11:15,660 --> 01:11:21,780
12 1 1 it's either grams per mole and I'm not making a mistake here

1087
01:11:21,780 --> 01:11:27,680
most of chemistry has gone side so every man should be in kilograms but occasionally

1088
01:11:27,680 --> 01:11:33,560
there is this 1 little remnant of the old CGS systems so cm grams seconds

1089
01:11:33,820 --> 01:11:37,040
and this is 1 so this is 4 . 0 1 1 grams per mole

1090
01:11:37,050 --> 01:11:47,770
Ward 12 point bold 1 1 atomic mass units atomic mass units and and you

1091
01:11:47,780 --> 01:11:53,900
want you is defined by the end user defined as

1092
01:11:53,930 --> 01:12:02,810
of the mass of the mass of 1 12 of a carbon 12 atoms the

1093
01:12:02,810 --> 01:12:06,910
mass of 1 12 of

1094
01:12:07,430 --> 01:12:15,020
so I talk about this this concept of moral here what is the more moles

1095
01:12:15,030 --> 01:12:21,060
more practical value something that we can handle tangibly and how we get to the

1096
01:12:21,060 --> 01:12:25,920
value of the moldable is something that was also defined in terms of carbon and

1097
01:12:26,120 --> 01:12:29,080
the mole is defined as the

1098
01:12:29,600 --> 01:12:40,560
the mass of it's the amount the amount of carbon 12 waiting

1099
01:12:41,220 --> 01:12:46,200
exactly 12 .

1100
01:12:47,450 --> 01:12:52,430
programs so you can see the atomic mass units of all

1101
01:12:52,680 --> 01:13:02,400
the view that the atomic mass unit would be 1 point a weeks at differently

1102
01:13:02,500 --> 01:13:07,280
this'll be the definition of the mole as the amount of carbon way exactly 12

1103
01:13:07,280 --> 01:13:12,080
grammes so I'd like to know now how many particles from carbons are there in

1104
01:13:12,080 --> 01:13:15,960
that mold and 1 way we can get to that is by looking at 2

1105
01:13:15,960 --> 01:13:26,840
pieces of data in the first one again Faraday Verde who conducted electrolysis of silver

1106
01:13:26,840 --> 01:13:34,920
electrolysis of silver but in aqueous solution he passes electric currents because a silver alliance

1107
01:13:34,920 --> 01:13:42,770
to deposit and for metallic silver and he measures how much charge is required he

1108
01:13:42,770 --> 01:13:48,210
has current right we know that charge is equal to the integral of the current

1109
01:13:48,300 --> 01:13:52,140
times the time and he knows the current you know so much time in many

1110
01:13:52,140 --> 01:13:59,200
ways so he's able to weigh the silver and compare it to carbon and against

1111
01:13:59,200 --> 01:14:06,500
that scale defines the solar ways a 170 grams pursues 12 grammes

1112
01:14:06,600 --> 01:14:14,100
for carbon so that's the ratio now the only knew what the electron brought him

1113
01:14:14,230 --> 01:14:18,450
he'd be able to divide through and calculate how many species there are in a

1114
01:14:18,450 --> 01:14:24,180
mall in order to determine that we had to wait for a few years until

1115
01:14:24,180 --> 01:14:33,950
1909 1909 Robert Millikan doing experiments at the University of Chicago conducted this interesting

1116
01:14:35,080 --> 01:14:43,250
effort where he took atomizer filled it with oil and then squirted fine droplets of

1117
01:14:43,250 --> 01:14:50,750
oil minister in between 2 plates that could be electrically charged the further

1118
01:14:50,790 --> 01:14:58,900
charged the droplets by radiating here showing actually used x-rays use various forms of high-energy

1119
01:14:58,900 --> 01:15:03,800
radiation in order to make the droplets bear a charge so now you have the

1120
01:15:03,820 --> 01:15:06,160
situation where

1121
01:15:07,060 --> 01:15:12,560
you have a droplets that is net negative or perhaps a net positive and it

1122
01:15:12,560 --> 01:15:14,850
lies suspended

1123
01:15:14,870 --> 01:15:20,430
between 2 plates and I'm able to vary the voltage on the plates

1124
01:15:20,800 --> 01:15:22,460
making this

1125
01:15:22,520 --> 01:15:27,560
for argument's sake negative this for argument's sake positive so if there's no charge on

1126
01:15:27,560 --> 01:15:32,960
essentially what you can do it you can update independently does the state component at

1127
01:15:32,960 --> 01:15:37,540
each each compare each comparator fires because that's actually quite nice

1128
01:15:37,810 --> 01:15:39,810
so that was basically

1129
01:15:39,830 --> 01:15:45,230
for MCMC so it's very quashed course introduction we might have been too fast

1130
01:15:45,250 --> 01:15:48,520
so just to point out there is a lot

1131
01:15:48,540 --> 01:15:52,360
mcmc is feel that action is very old

1132
01:15:52,380 --> 01:15:55,560
OK this has been going on for fifty years

1133
01:15:55,560 --> 01:15:59,600
people used to this news actually far too often

1134
01:15:59,620 --> 01:16:04,940
still and so that gives samplerank metropolis that actually do not scale do not take

1135
01:16:05,060 --> 01:16:09,540
hold for the rights to model which are used basically in the context of machine

1136
01:16:09,540 --> 01:16:14,130
learning you have to be careful when you're basically dealing with complicated models

1137
01:16:14,150 --> 01:16:18,710
there's another way and almost all of the MCMC using some kind of more flexible

1138
01:16:18,710 --> 01:16:24,000
phisticated idea like i'm doing an MCMC and something so i think the

1139
01:16:24,000 --> 01:16:25,080
kind of

1140
01:16:25,100 --> 01:16:30,000
because of the recent like increase in computational power folks like due to GPU

1141
01:16:30,020 --> 01:16:34,040
i think there's a lot of things that could be used as well but comparing

1142
01:16:34,040 --> 01:16:36,880
is not new sixty idea any more

1143
01:16:36,880 --> 01:16:42,190
but basically because one of the central computer it actually used to take ages knowledge

1144
01:16:42,210 --> 01:16:43,380
we have GPU

1145
01:16:43,400 --> 01:16:49,670
basically some of the techniques that i apologize basically should be basically consider on i

1146
01:16:49,670 --> 01:16:54,980
think i'll have a lot of potential for actually complex machine learning type application so

1147
01:16:54,980 --> 01:16:57,210
that's it for MCMC

1148
01:16:57,230 --> 01:16:58,980
or after MCMC

1149
01:16:59,000 --> 01:17:02,650
basically look at SMC so i thought i we do everything honestly one in one

1150
01:17:02,650 --> 01:17:04,210
of the was impossible

1151
01:17:06,960 --> 01:17:08,630
well we discuss

1152
01:17:08,650 --> 01:17:10,520
four basically

1153
01:17:10,540 --> 01:17:14,290
the remaining offline on

1154
01:17:14,310 --> 01:17:16,730
tomorrow i will discuss

1155
01:17:16,750 --> 01:17:18,350
in nineteen eighty

1156
01:17:18,350 --> 01:17:23,920
to mark him of calorimeter which is essentially the so called large class of numerical

1157
01:17:23,920 --> 01:17:28,460
techniques have been developed in in the statistics and machine learning

1158
01:17:28,480 --> 01:17:31,330
this the closest sequential multicolour methods

1159
01:17:31,350 --> 01:17:36,830
so we know where you actually want this techniques are doing on also so

1160
01:17:36,850 --> 01:17:41,830
i will have time show you can combine those two powerful ideas to come up

1161
01:17:41,830 --> 01:17:46,400
with MCMC type algorithm which for example or

1162
01:17:46,420 --> 01:17:51,770
come in which are basically quote for example come up with like i water that

1163
01:17:51,790 --> 01:17:56,480
simultaneously one thousand highly correlated components OK so

1164
01:17:56,480 --> 01:18:01,210
that's basically what the the story i'm going to tell you

1165
01:18:04,540 --> 01:18:06,880
here is the point i'm going to consider

1166
01:18:06,900 --> 01:18:08,630
so what we've been doing

1167
01:18:09,440 --> 01:18:11,540
for the first two hours

1168
01:18:11,560 --> 01:18:15,210
on off we've been looking at you give me

1169
01:18:15,250 --> 01:18:18,900
the target distribution pi x

1170
01:18:18,920 --> 01:18:21,150
no not a normalizing constant

1171
01:18:21,230 --> 01:18:24,920
on what i want to do i want an iterative algorithm

1172
01:18:24,920 --> 01:18:29,560
OK this is going to generate approximate samples from it

1173
01:18:29,580 --> 01:18:32,330
so that's what i've been looking at this MCMC

1174
01:18:32,350 --> 01:18:35,900
not i'm going to use to look at the pollen which is

1175
01:18:35,900 --> 01:18:38,230
closely related so different

1176
01:18:38,330 --> 01:18:43,480
which is basically the problem we're now instead of being you know one single target

1177
01:18:43,480 --> 01:18:44,790
distribution pi x

1178
01:18:45,580 --> 01:18:48,040
you given sequence of

1179
01:18:48,080 --> 01:18:51,500
pi and so and is going to be kind of time indexed so if you

1180
01:18:51,500 --> 01:18:54,850
want algorithmic time index OK

1181
01:18:54,860 --> 01:18:58,560
all these target distributions

1182
01:18:59,540 --> 01:19:02,980
of increasing dimension so essentially

1183
01:19:03,000 --> 01:19:06,920
the argument of target distribution pi one time one be x one

1184
01:19:06,940 --> 01:19:10,330
the argument of target distribution pi two at time to me

1185
01:19:10,350 --> 01:19:11,440
x one x two

1186
01:19:11,460 --> 01:19:16,580
the argument of target distribution of pi in time and be x one x two

1187
01:19:16,580 --> 01:19:20,250
xn on i used the matlab notation OK x one

1188
01:19:20,250 --> 01:19:24,830
similarly like an MCMC i only assume

1189
01:19:24,840 --> 01:19:26,940
that each target distribution

1190
01:19:26,970 --> 01:19:29,830
is known up to a normalizing constant OK so

1191
01:19:29,840 --> 01:19:31,360
i'm going to assume that

1192
01:19:31,380 --> 01:19:35,620
prior to put together was zn where these guys to give me x one x

1193
01:19:35,620 --> 01:19:37,230
two xn i can evaluate it

1194
01:19:37,260 --> 01:19:42,090
OK i carried on numerical value it but i don't want to compute the integral

1195
01:19:42,090 --> 01:19:45,970
of gamma and all the x one x two xn that is the normalizing constant

1196
01:19:48,510 --> 01:19:50,810
what i want to do

1197
01:19:50,840 --> 01:19:54,270
OK i want to come up essentially

1198
01:19:54,290 --> 01:19:57,810
we've approximation by monte carlo approximation

1199
01:19:57,840 --> 01:19:59,890
of both

1200
01:19:59,900 --> 01:20:03,140
actually the target distribution pi and OK

1201
01:20:03,150 --> 01:20:08,600
so i pi one i will approximation problems at all approximation of the normalizing constant

1202
01:20:08,600 --> 01:20:10,300
which i wasn't doing before

1203
01:20:10,300 --> 01:20:14,290
before and just sampling sampled from prior never presented you

1204
01:20:14,580 --> 01:20:20,400
methods to approximate the normalizing constant they're quite different in the MCMC called OK

1205
01:20:20,430 --> 01:20:22,430
so i want to do that

1206
01:20:22,440 --> 01:20:26,050
on the additional constraint i wanted that sequential

1207
01:20:26,050 --> 01:20:28,000
OK so

1208
01:20:30,140 --> 01:20:33,890
i want a time want to go an approximation of pi one on its assisted

1209
01:20:33,890 --> 01:20:40,100
normalizing constant time two by two unnecessary normalising constants also force that's what i wanted

1210
01:20:40,130 --> 01:20:41,790
OK so that's the big

1211
01:20:41,800 --> 01:20:45,380
i mean we've actually widened discover quite plastic only

1212
01:20:45,470 --> 01:20:50,590
OK not only what they want to go to distribution of we are actually moving

1213
01:20:50,590 --> 01:20:55,710
target essentially all its dimension is increasing over time

1214
01:20:55,810 --> 01:21:00,430
so you need obviously numerical methods obviously

1215
01:21:00,440 --> 01:21:04,550
we can use MCMC OK so for each target distribution

1216
01:21:04,550 --> 01:21:07,420
you could run MCMC egoism

1217
01:21:07,430 --> 01:21:10,180
that's basically target pioneer

1218
01:21:10,190 --> 01:21:11,840
so you can do that

1219
01:21:11,850 --> 01:21:15,920
but for most applications going to deal with it's actually quite slow

1220
01:21:15,930 --> 01:21:20,850
on top of that it doesn't imply the river this debate the normalizing constants are

1221
01:21:20,880 --> 01:21:23,340
quite happy with that is not already

1222
01:21:23,340 --> 01:21:24,800
what i'm interested in

1223
01:21:24,900 --> 01:21:26,640
so instead

1224
01:21:26,650 --> 01:21:29,680
basically we gonna basically pause

1225
01:21:29,690 --> 01:21:34,500
discuss and class of techniques which on the interacting particle better although sequential monte carlo

1226
01:21:34,500 --> 01:21:38,500
methods or indigenous particle filters

1227
01:21:38,510 --> 01:21:41,880
so the key idea what we're going to do

1228
01:21:41,930 --> 01:21:46,000
is actually fairly nice thing actually is to say well all the

1229
01:21:46,010 --> 01:21:49,440
following on look at on the going to do some examples later on

1230
01:21:49,470 --> 01:21:53,310
is that you have a sequence of target distributions like pi one of x one

1231
01:21:53,310 --> 01:21:55,850
y two x one x two by free of x one x two x three

1232
01:21:55,970 --> 01:22:01,580
on the time distribution they're not so different from each other OK so essentially

1233
01:22:01,580 --> 01:22:07,850
what are we're talking about today is my dissertation dynamics of large networks ITV that

1234
01:22:07,850 --> 01:22:09,430
current i was right

1235
01:22:09,490 --> 01:22:11,040
quite useful

1236
01:22:11,060 --> 01:22:12,590
so basically talk

1237
01:22:12,610 --> 01:22:14,340
go about

1238
01:22:14,360 --> 01:22:20,800
hartson about three things dynamics how things change over time

1239
01:22:20,840 --> 01:22:21,980
large scale

1240
01:22:22,000 --> 01:22:24,230
and networks in the US

1241
01:22:24,230 --> 01:22:30,390
let me start so that's what is the web today we have this wonderful they

1242
01:22:30,820 --> 01:22:33,750
on line computing applications so for example

1243
01:22:33,770 --> 01:22:38,210
we have a social networking website for sharing websites

1244
01:22:39,960 --> 01:22:45,860
one recommendation size things like that we also have online social media

1245
01:22:45,880 --> 01:22:47,720
so mean block

1246
01:22:47,740 --> 01:22:50,100
we lots of people want

1247
01:22:50,320 --> 01:22:51,420
so we have like

1248
01:22:51,460 --> 01:22:52,560
keep the

1249
01:22:53,180 --> 01:22:55,460
and and so

1250
01:22:55,470 --> 01:23:00,070
we also have virtual worlds like one of warcraft EVE online so

1251
01:23:00,080 --> 01:23:06,680
and i have a lot of online collaboration and communication tools applications like google

1252
01:23:06,770 --> 01:23:07,710
so so

1253
01:23:08,330 --> 01:23:11,660
the question is whether these things gone

1254
01:23:11,690 --> 01:23:13,770
so the first thing the

1255
01:23:13,780 --> 01:23:19,660
large scale like millions or hundreds of millions of users are using these applications

1256
01:23:19,690 --> 01:23:21,720
the second one is that they

1257
01:23:21,770 --> 01:23:27,350
so coming from this point of philosophy right a lot a lot of it is

1258
01:23:27,360 --> 01:23:33,130
user generated content so it's not users just can't consume content that actually interact the

1259
01:23:33,130 --> 01:23:35,490
website content

1260
01:23:35,500 --> 01:23:42,880
and the thing is this content is high speed constantly updated and changed over time

1261
01:23:42,890 --> 01:23:48,630
and because is going on computers so you get this message

1262
01:23:48,690 --> 01:23:49,770
and then the

1263
01:23:49,920 --> 01:23:52,460
set of human social activity

1264
01:23:52,530 --> 01:23:54,080
he also collective

1265
01:23:54,110 --> 01:23:56,610
so what is this

1266
01:23:56,690 --> 01:24:02,550
each interactions between users and content they create and these interactions can actually be model

1267
01:24:03,270 --> 01:24:07,310
i mean i have not which is present humans of some kind of entities and

1268
01:24:07,310 --> 01:24:13,110
the edges between these networks to prove the present interaction communication and things like that

1269
01:24:13,480 --> 01:24:17,140
and this is basically the object to make this was

1270
01:24:17,150 --> 01:24:18,430
network as a

1271
01:24:19,830 --> 01:24:24,390
so what want to bring system today when is like the first

1272
01:24:24,640 --> 01:24:28,010
hundreds of millions of people people at once so

1273
01:24:28,050 --> 01:24:33,210
for example what you can do is you best traditional social science opposite so for

1274
01:24:33,210 --> 01:24:38,860
example like the degrees of separation of the small world hypothesis way today you

1275
01:24:39,150 --> 01:24:44,120
you can around the experiment of on the basic water for millions of people who

1276
01:24:44,170 --> 01:24:50,580
were actively using microsoft instant messenger and perhaps surprisingly you find that the average we

1277
01:24:50,580 --> 01:24:55,550
are separated by six point six halls and the like you can reach ninety percent

1278
01:24:55,620 --> 01:25:01,300
of the messenger population which can be bought on this map of the world basically

1279
01:25:01,490 --> 01:25:04,260
represents an x y coordinates of the use

1280
01:25:04,270 --> 01:25:09,310
and the density of dots represents the number of users location basically you can reach

1281
01:25:09,310 --> 01:25:13,770
ninety percent of the users in a cost from any given node that network which

1282
01:25:13,770 --> 01:25:15,810
is kind of surprising

1283
01:25:16,140 --> 01:25:20,990
so one question is why should we study that networks and i think there are

1284
01:25:20,990 --> 01:25:22,150
two reasons why to do this

1285
01:25:22,270 --> 01:25:27,890
the first is to be understand and theory of online content is being created because

1286
01:25:27,890 --> 01:25:32,110
consumed so basically the question is how do users create content and how they interact

1287
01:25:32,670 --> 01:25:35,650
and how they also interact with one another through this

1288
01:25:35,670 --> 01:25:37,740
like online let's say computing

1289
01:25:40,120 --> 01:25:43,460
the question that comes up is that can be used this

1290
01:25:43,890 --> 01:25:49,840
understanding to be better conditions so we can design better and better services and can

1291
01:25:49,840 --> 01:25:50,730
be developed

1292
01:25:50,740 --> 01:25:55,960
better algorithms that take advantage of the structure of the underlying data

1293
01:25:55,980 --> 01:25:59,180
so that's basically what what we want to do this

1294
01:25:59,330 --> 01:26:04,230
so to talk about what has talking been the topic was exactly metadata data and

1295
01:26:04,230 --> 01:26:07,890
dynamic network and there are two

1296
01:26:07,900 --> 01:26:09,740
aspects of of the network

1297
01:26:09,770 --> 01:26:14,360
first network evolution right so if have a graph or network that wasn't changes all

1298
01:26:14,990 --> 01:26:20,350
the question how the structure change some of the structure itself facebook social network change

1299
01:26:20,350 --> 01:26:22,780
over time face gets that

1300
01:26:22,800 --> 01:26:27,260
the second question is a bit different so you can sort of think of a

1301
01:26:27,260 --> 01:26:31,950
social network or network has been expecting but now have some processes that are also

1302
01:26:31,950 --> 01:26:37,300
released that so i think of like a wide spread like a virus disease spread

1303
01:26:37,310 --> 01:26:42,050
in the network on the web what is spreading misinformation so instead of having disease

1304
01:26:42,050 --> 01:26:43,410
epidemics we now having

1305
01:26:43,420 --> 01:26:46,880
information feedbacks and trying to study and quantify those

1306
01:26:48,860 --> 01:26:51,440
the go more into this city

1307
01:26:52,360 --> 01:26:55,720
in each of the two parts in the in the slides and and what you

1308
01:26:55,880 --> 01:27:01,290
want to also see this three parts which which parts of the question is you

1309
01:27:01,300 --> 01:27:02,880
know how can i mentioned the data

1310
01:27:02,900 --> 01:27:06,560
how can i developed some intuition what is going on so for example on the

1311
01:27:06,560 --> 01:27:12,310
network evolution the basic question is how does network structure changed as networks evolve and

1312
01:27:12,350 --> 01:27:17,850
what we found is suspect all five is the network specify which basically means the

1313
01:27:17,860 --> 01:27:21,750
number of edges the number of nodes of time follows this particular

1314
01:27:21,850 --> 01:27:26,780
all relations which basically says that we should is increasing or time and many perhaps

1315
01:27:26,780 --> 01:27:32,290
surprisingly what you also find is the diameter of networks trees which basically says that

1316
01:27:32,320 --> 01:27:37,490
the graph is large distances tend to be small which is kind of counterintuitive so

1317
01:27:37,490 --> 01:27:41,430
as you go see measure something like that then you can ask what to

1318
01:27:42,130 --> 01:27:44,110
how could i have some information

1319
01:27:44,160 --> 01:27:49,130
and for example the model developed is called the forest fire because of a simple

1320
01:27:49,180 --> 01:27:54,550
genetic mechanism so you find node you want to create edges it to create

1321
01:27:54,560 --> 01:27:55,850
it is located the network

1322
01:27:55,940 --> 01:27:57,490
but soon followed

1323
01:27:57,490 --> 01:28:00,030
or one of the system

1324
01:28:00,040 --> 01:28:01,360
the network

1325
01:28:01,370 --> 01:28:02,860
you know because way

1326
01:28:02,870 --> 01:28:05,440
and then attaching to

1327
01:28:05,470 --> 01:28:07,240
you know what is

1328
01:28:07,260 --> 01:28:13,540
and what is what is interesting about this process you both network densify and that

1329
01:28:13,540 --> 01:28:18,800
would have shrinking diameter property OK so now we know what's going on in the

1330
01:28:18,800 --> 01:28:21,750
channel so what what you can do

1331
01:28:21,780 --> 01:28:25,150
so for example one thing that we can do is say OK how can i

1332
01:28:25,150 --> 01:28:28,560
generate realistic synthetic graphs right so the

1333
01:28:28,570 --> 01:28:36,240
goal is basically given the network how can we generate something synthetic scene and and

1334
01:28:36,250 --> 01:28:40,380
it's sort of the solution or what is what we need here is becoming a

1335
01:28:40,380 --> 01:28:43,340
sort of embed this interface to do some sort of

1336
01:28:43,360 --> 01:28:47,910
composition of these of those two approaches and you get a method that has the

1337
01:28:47,960 --> 01:28:51,310
the first to convergence time behaviour

1338
01:28:51,320 --> 01:28:54,330
OK so

1339
01:28:54,340 --> 01:28:57,000
one more thing i mentioned it because it's going to come up again at the

1340
01:28:57,000 --> 01:29:03,440
moment is ultimately what you can show is that if these methods are converging OK

1341
01:29:03,480 --> 01:29:06,870
they the xk is all that the iterates xk all end up on some sort

1342
01:29:06,870 --> 01:29:09,220
of reduced manifold

1343
01:29:09,240 --> 01:29:13,740
so for instance if your regularizer result is the one norm and after you've taken

1344
01:29:13,740 --> 01:29:17,610
in a whole bunch of steps and getting pretty close the solution what you're going

1345
01:29:17,610 --> 01:29:22,500
to find is that most of the components of the xk going to be zero

1346
01:29:22,560 --> 01:29:25,690
and some are going to be negative and some are going to be positive but

1347
01:29:26,730 --> 01:29:31,120
after a while you can identify which ones are nonzero and which ones zero you

1348
01:29:31,120 --> 01:29:34,830
going get a pretty good idea of what those two sets

1349
01:29:35,090 --> 01:29:37,990
and so if you know if you want you got that estimate of what the

1350
01:29:37,990 --> 01:29:40,910
optimal manifold it is

1351
01:29:40,960 --> 01:29:43,180
then you can do something fancier

1352
01:29:43,230 --> 01:29:47,080
you might for example just ditch the the slide

1353
01:29:47,130 --> 01:29:52,760
and restrictions of explicitly to whatever manifold decided is close to optimal

1354
01:29:52,780 --> 01:29:56,790
and start doing something else with their flag some sort of newton step requires step

1355
01:29:56,810 --> 01:30:00,170
something not talk about methods like that in my mind

1356
01:30:00,210 --> 01:30:03,460
and i can really help the the convergence

1357
01:30:03,550 --> 01:30:05,430
of these approaches

1358
01:30:05,450 --> 01:30:09,950
now as i said as i said number times you often are interested in the

1359
01:30:09,950 --> 01:30:13,980
solution of this problem for a range of values of the parameter tau

1360
01:30:14,000 --> 01:30:18,710
and it turns out that in many settings these problems easier to solve when the

1361
01:30:18,710 --> 01:30:24,830
towers began in other words when you're trying to impose more regularisation on the solution

1362
01:30:24,860 --> 01:30:28,130
the problems tend to be easier to solve because they tend to give you a

1363
01:30:28,130 --> 01:30:29,910
solution lying in

1364
01:30:29,920 --> 01:30:31,900
small dimensional space

1365
01:30:32,050 --> 01:30:36,780
so that's known to be true surname compressed sensing that when this is the one

1366
01:30:37,510 --> 01:30:41,850
and the towers large the solution only has a few nonzero components and it's pretty

1367
01:30:41,850 --> 01:30:45,180
easy to find very rapid convergence

1368
01:30:45,200 --> 01:30:49,010
on the other hand when the towers smaller the opposite is true you often have

1369
01:30:49,010 --> 01:30:51,790
solutions are more likely unconstrained solution

1370
01:30:51,800 --> 01:30:55,000
got more degrees of freedom in a sense

1371
01:30:55,010 --> 01:30:59,820
and they tend to need many more iterations of this kind of methods i described

1372
01:30:59,830 --> 01:31:02,980
so what can we do about this well it turns out that first which is

1373
01:31:02,980 --> 01:31:06,190
one of the accelerated methods i just described

1374
01:31:06,220 --> 01:31:11,240
it's actually pretty robust to this effect in other words it the number of iterations

1375
01:31:11,240 --> 01:31:13,330
it takes is not that sense

1376
01:31:13,350 --> 01:31:14,840
the choice of ten

1377
01:31:14,850 --> 01:31:16,280
whereas the basic

1378
01:31:16,330 --> 01:31:22,080
gradient type methods for consumer meaning method is pretty sensitive OK

1379
01:31:22,170 --> 01:31:27,380
but not all is lost because you can speed up but it's incredibly naive continuation

1380
01:31:27,380 --> 01:31:32,610
strategy you can start with the big value of tau say tell zero

1381
01:31:32,620 --> 01:31:35,790
you can solve the problem for the big value of ten as i said usually

1382
01:31:35,790 --> 01:31:40,250
it's pretty easy in that case then you can just decrease tell by some factor

1383
01:31:41,750 --> 01:31:45,340
use the solution i just found was the starting point for the new problem with

1384
01:31:45,350 --> 01:31:47,600
the new reduced value of tau

1385
01:31:47,630 --> 01:31:51,700
so it is in essence you can just sort of chase a sequence of solutions

1386
01:31:51,700 --> 01:31:53,680
and you keep decreasing tell

1387
01:31:53,700 --> 01:31:56,340
until you reach the target you looking for

1388
01:31:56,380 --> 01:32:01,570
so this is a very kind of dumb strategy has been analyzed very much but

1389
01:32:01,580 --> 01:32:07,150
but is very dependent on the heuristic here how much do choose to decrease tabloid

1390
01:32:07,150 --> 01:32:08,340
each step

1391
01:32:08,430 --> 01:32:13,680
but fundamentally it works pretty well and really rescues the behaviour the poor behaviour these

1392
01:32:13,680 --> 01:32:15,370
methods for small town

1393
01:32:15,390 --> 01:32:18,900
it really tends to fix them up pretty well

1394
01:32:18,920 --> 01:32:24,350
there's more work to be done in figuring out why this works

1395
01:32:24,360 --> 01:32:30,380
OK stochastic gradient so there's this pulls together a couple of things that i mentioned

1396
01:32:30,410 --> 01:32:34,160
namely the case where you don't have access to the great which have been assuming

1397
01:32:34,160 --> 01:32:38,190
for the last few slides you only have access to an estimate

1398
01:32:38,200 --> 01:32:41,690
but we just talked in the second part of the tour right before the break

1399
01:32:41,850 --> 01:32:47,040
about what you can do with stochastic approximation methods we can take as much of

1400
01:32:47,040 --> 01:32:50,010
the great you know maybe averages of all the estimate you've come up with so

1401
01:32:51,540 --> 01:32:54,620
you can do that here even when you've got a regularisation term

1402
01:32:54,640 --> 01:32:56,980
so you can former subproblem here with

1403
01:32:56,980 --> 01:33:01,930
the average values of the gradients or subgradients you found so far

1404
01:33:01,950 --> 01:33:05,070
and the regularizer just appears here explicitly

1405
01:33:05,070 --> 01:33:06,300
no change at all

1406
01:33:06,310 --> 01:33:07,470
and then you've got this

1407
01:33:07,480 --> 01:33:12,260
so it shows up in primal dual averaging it sort of is approx that analyzes

1408
01:33:12,260 --> 01:33:14,830
how far you move from x

1409
01:33:14,850 --> 01:33:18,830
so you can put all these ingredients together as a recent paper by showery does

1410
01:33:18,830 --> 01:33:23,160
this and refers to the astros region results in

1411
01:33:23,180 --> 01:33:27,780
a number of other algorithms like DJ singer one thing from two thousand nine which

1412
01:33:27,780 --> 01:33:29,390
is also related

1413
01:33:29,410 --> 01:33:32,660
and what you can show that this is the in expectation

1414
01:33:32,680 --> 01:33:34,850
the value of this objective

1415
01:33:35,400 --> 01:33:39,640
approaches the optimal value out one of the square of k right

1416
01:33:39,650 --> 01:33:43,320
OK so this is the kind of right you see in the primal dual approach

1417
01:33:43,320 --> 01:33:48,540
even when you've got access to deterministic exact gradients so they don't seem to pay

1418
01:33:48,540 --> 01:33:53,080
much price for having an average grade

1419
01:33:53,080 --> 01:33:57,390
so here are some references there's really only scratch the surface is a huge area

1420
01:33:57,390 --> 01:34:02,850
of investigation hey designer regularizes to get the properties that you want to design an

1421
01:34:02,850 --> 01:34:04,820
so this is our

1422
01:34:06,310 --> 01:34:10,800
morning after dark as very happy to have structure theory

1423
01:34:12,420 --> 01:34:13,800
and dark about

1424
01:34:14,920 --> 01:34:21,710
OK thanks for

1425
01:34:22,300 --> 01:34:23,990
right well good morning

1426
01:34:25,460 --> 01:34:29,650
thank you for getting up early to see tutorial and boosting

1427
01:34:29,670 --> 01:34:33,740
so this is a theory and applications of boosting a little bit more theory than

1428
01:34:33,740 --> 01:34:35,470
applications but they are both

1429
01:34:37,510 --> 01:34:40,510
this is a tutorial for people who know

1430
01:34:40,520 --> 01:34:45,300
little or nothing about boosting so let me start at the beginning with typical kind

1431
01:34:45,300 --> 01:34:46,870
of problem

1432
01:34:46,870 --> 01:34:48,690
this is the problem i worked on

1433
01:34:48,690 --> 01:34:53,080
when i was at eighteen t so

1434
01:34:53,320 --> 01:34:57,830
the problem here is to automatically categorize the type of call

1435
01:34:57,830 --> 01:35:02,120
requested by the customer so the idea is that somebody would call up eighteen c

1436
01:35:02,150 --> 01:35:06,760
and maybe they want to make a collect call our third number call or whatever

1437
01:35:06,760 --> 01:35:09,600
it is and

1438
01:35:09,630 --> 01:35:13,070
the idea was to build a system where people could just speak naturally to the

1439
01:35:13,080 --> 01:35:15,410
system rather than if you want

1440
01:35:15,410 --> 01:35:19,150
this first one if you want that just two and so on

1441
01:35:19,290 --> 01:35:23,540
so when the person says something we need the computer needs to be able to

1442
01:35:23,540 --> 01:35:28,600
categorize what kind of request the person is making so if the person says yes

1443
01:35:28,600 --> 01:35:30,600
i'd like to place a collect call

1444
01:35:30,650 --> 01:35:33,520
long-distance please that's a collect call

1445
01:35:33,540 --> 01:35:35,690
i i discovered recently that

1446
01:35:35,930 --> 01:35:38,800
since everybody is cell phones now there are a lot of

1447
01:35:38,820 --> 01:35:43,150
people younger than me who don't know what to collect call and third number call

1448
01:35:43,150 --> 01:35:47,660
and so on so these are just kinds of calls so you need to know

1449
01:35:47,740 --> 01:35:52,990
in the old days you make a collect call third number call so person says

1450
01:35:52,990 --> 01:35:56,570
operator need to make a call but i need to belittle my office is being

1451
01:35:56,570 --> 01:36:00,820
built with third numbers so that's called the third number call and so on

1452
01:36:00,820 --> 01:36:02,370
OK so if you think about it

1453
01:36:03,570 --> 01:36:08,080
for a problem like this if you just look at examples like this then

1454
01:36:08,130 --> 01:36:10,550
immediately start to realize that there

1455
01:36:10,620 --> 01:36:12,630
rules of thumb that will

1456
01:36:12,630 --> 01:36:16,890
often be correct so for instance if you just look at that very first example

1457
01:36:17,130 --> 01:36:19,040
i have point

1458
01:36:19,090 --> 01:36:22,890
so strong enough OK if you just look at the very first example

1459
01:36:22,910 --> 01:36:28,890
you know you can immediately think well of the word collect appears in what was

1460
01:36:28,890 --> 01:36:34,570
then predict that to collect call or the word car appears in what was predicted

1461
01:36:34,570 --> 01:36:39,630
to calling card call and so on so you can immediately start thinking about these

1462
01:36:39,630 --> 01:36:41,070
rules of thumb

1463
01:36:41,090 --> 01:36:45,660
which will be pretty good they're not going to be perfect but they should be

1464
01:36:45,660 --> 01:36:48,540
better than just guessing at random

1465
01:36:48,540 --> 01:36:51,480
what's harder is defined just the single

1466
01:36:51,500 --> 01:36:55,970
prediction rule which will be very highly accurate so the idea of boosting is is

1467
01:36:55,970 --> 01:36:59,480
to somehow use these weak rules of thumb

1468
01:36:59,500 --> 01:37:03,340
in combination somehow so that when you put them together you end up with the

1469
01:37:03,350 --> 01:37:07,140
prediction rule which is very highly accurate

1470
01:37:07,160 --> 01:37:11,730
OK so here's the boosting approach at a very high level will certainly be going

1471
01:37:11,730 --> 01:37:16,470
into more detail that had a very high level here's the idea of boosting so

1472
01:37:16,470 --> 01:37:18,500
since these rules of thumb

1473
01:37:18,510 --> 01:37:21,410
are not so hard to find

1474
01:37:21,420 --> 01:37:26,640
we could think about coming up with a computer program that could look at data

1475
01:37:26,850 --> 01:37:30,040
and derive rules of thumb from the data

1476
01:37:30,090 --> 01:37:34,910
and we can apply that procedure to a subset of the examples

1477
01:37:34,920 --> 01:37:37,340
that would give us the first rule of thumb

1478
01:37:37,380 --> 01:37:41,790
then we could do this again to the second subset of the examples get second

1479
01:37:41,790 --> 01:37:42,910
rule of thumb

1480
01:37:42,940 --> 01:37:47,590
and you can imagine doing this over and over again repeating this city times

1481
01:37:47,660 --> 01:37:48,760
so there are some

1482
01:37:48,790 --> 01:37:51,290
important details there are being left out here

1483
01:37:51,310 --> 01:37:55,170
so first of all as we choose the subset of examples on each one of

1484
01:37:55,170 --> 01:37:56,820
these rounds

1485
01:37:56,820 --> 01:37:58,040
and secondly

1486
01:37:58,060 --> 01:37:59,260
once we've

1487
01:37:59,280 --> 01:38:03,410
gathered all these rules of thumb how do we combine them into a single prediction

1488
01:38:03,410 --> 01:38:07,280
rule that will be very highly accurate

1489
01:38:07,290 --> 01:38:11,220
OK so again those two questions to the first question again is how should we

1490
01:38:11,220 --> 01:38:16,660
choose the examples on each round to train this procedure on

1491
01:38:17,010 --> 01:38:22,610
and here the main idea of boosting is to concentrate on the hardest examples

1492
01:38:22,640 --> 01:38:25,990
and every round we want to concentrate on the hard examples

1493
01:38:26,020 --> 01:38:29,850
and what are the hard examples with the hard examples are the one

1494
01:38:30,540 --> 01:38:35,590
are most often misclassified by the previous rules of thumb ones that we got wrong

1495
01:38:35,590 --> 01:38:36,660
the most

1496
01:38:36,660 --> 01:38:40,060
on the previous rounds up until that point

1497
01:38:40,090 --> 01:38:41,350
so that's the first time

1498
01:38:41,810 --> 01:38:46,000
the main idea in the first question we have to answer and the second question

1499
01:38:46,000 --> 01:38:46,850
again is how do we

1500
01:38:46,850 --> 01:38:48,100
and learn this model

1501
01:38:48,100 --> 01:38:52,440
and learn this model and using the gas in units here just so we can

1502
01:38:52,440 --> 01:38:56,600
compare with PCA which is also used as units

1503
01:38:56,620 --> 01:38:58,390
the new units

1504
01:38:58,410 --> 01:39:00,730
and you do this

1505
01:39:00,770 --> 01:39:05,140
you learn for models unsupervised you put the transpose weights there

1506
01:39:05,160 --> 01:39:09,040
and one i learned these models and just using cd one that is i'm going

1507
01:39:09,080 --> 01:39:14,100
down reconstruct the to reconstruct and then activating the hidden units again so we know

1508
01:39:14,200 --> 01:39:17,040
these things we construct those things quite well

1509
01:39:17,060 --> 01:39:20,660
and and therefore we know that we get quite a good reconstruction and there

1510
01:39:20,680 --> 01:39:24,040
so what comes out or to look something like what goes in

1511
01:39:24,100 --> 01:39:26,750
of course is subject to the constraint has gets really thirty

1512
01:39:27,310 --> 01:39:28,350
this year

1513
01:39:28,350 --> 01:39:31,600
and indeed

1514
01:39:31,640 --> 01:39:33,180
it does

1515
01:39:33,180 --> 01:39:34,520
so if you do

1516
01:39:36,120 --> 01:39:39,080
so we can take now the same set of digits

1517
01:39:39,100 --> 01:39:44,310
we can use all classes of digits using PCA and go down to thirty dimensions

1518
01:39:44,560 --> 01:39:48,220
these are the reconstructions you get from the PCA

1519
01:39:48,390 --> 01:39:54,180
it's not really fair to do that because we know the pixels of between zero

1520
01:39:54,180 --> 01:39:55,140
and one

1521
01:39:55,140 --> 01:39:57,390
so you can do logistic PCA

1522
01:39:57,410 --> 01:40:01,270
which we did by stochastic gradient descent because we have the code and that does

1523
01:40:01,270 --> 01:40:03,080
considerably better than PCA

1524
01:40:03,100 --> 01:40:06,330
but nowhere near as good as using this deep autoencoders

1525
01:40:06,430 --> 01:40:10,910
the deep autoencoder actually produce reconstructions that are better than the data

1526
01:40:10,910 --> 01:40:15,770
well you know it does look at the site you see is slightly flaky in

1527
01:40:15,770 --> 01:40:18,180
the data and now we've got something better

1528
01:40:18,180 --> 01:40:24,200
that means of course i should be on learning things are always move but in

1529
01:40:24,200 --> 01:40:28,490
and in some cases not the data but it's pretty damn good

1530
01:40:32,430 --> 01:40:35,640
we can now apply the same techniques to get little codes for

1531
01:40:37,330 --> 01:40:39,790
so we start by

1532
01:40:39,870 --> 01:40:42,120
converting documents into a bag of words

1533
01:40:42,140 --> 01:40:43,540
as usual

1534
01:40:43,970 --> 01:40:48,080
and then we use a bunch of hidden units to model what's going on in

1535
01:40:48,080 --> 01:40:50,250
that bag of words binary hidden units

1536
01:40:50,250 --> 01:40:52,790
and we use lot less like that

1537
01:40:52,810 --> 01:40:59,160
i should just mention the result has just been accepted by nets which is

1538
01:40:59,220 --> 01:41:04,000
in marion ross figured out how to compute the partition function for these boltzmann machines

1539
01:41:04,000 --> 01:41:05,080
pretty well

1540
01:41:05,120 --> 01:41:08,020
if after you finish learning you spend a few hours doing it

1541
01:41:08,060 --> 01:41:10,720
so now we can measure the density of held out data in one of these

1542
01:41:10,720 --> 01:41:12,180
balls machines

1543
01:41:12,200 --> 01:41:16,470
and if you just learn a restricted boltzmann machine with one american units the simple

1544
01:41:17,520 --> 01:41:19,850
and you compare the density that you get

1545
01:41:19,870 --> 01:41:21,770
with topic models

1546
01:41:21,770 --> 01:41:26,410
it gives much better density for the bags of words for holding bags of words

1547
01:41:26,870 --> 01:41:31,470
it's not surprising because the much more powerful representation topic models really saying each document

1548
01:41:31,470 --> 01:41:34,770
has to be generated by one topic

1549
01:41:34,830 --> 01:41:35,830
so each

1550
01:41:36,000 --> 01:41:40,060
so each word has to be generated by picking random topic that word generation

1551
01:41:40,060 --> 01:41:44,910
OK here we are concerned just not to model the documents well but they get

1552
01:41:44,950 --> 01:41:48,200
down to short description

1553
01:41:48,350 --> 01:41:50,490
so this was the model he learned

1554
01:41:50,500 --> 01:41:51,450
one model

1555
01:41:51,470 --> 01:41:53,450
we have ten numbers in the middle

1556
01:41:53,450 --> 01:41:57,160
and he showed these ten numbers do much better

1557
01:41:57,220 --> 01:41:58,770
then the ten numbers

1558
01:41:58,790 --> 01:42:04,350
if you do latent semantic analysis but is if you speak essentially PCA on local

1559
01:42:04,350 --> 01:42:06,080
one plus the word count

1560
01:42:06,100 --> 01:42:11,970
in fact ten of our numbers i think about a hundred of them person

1561
01:42:12,020 --> 01:42:13,200
the curve

1562
01:42:13,220 --> 01:42:18,330
he did lot of big data set

1563
01:42:18,350 --> 01:42:23,970
the first time this paper got rejected one of the referees complained that we'd use

1564
01:42:23,970 --> 01:42:27,830
data data set was too big so it can be compared with other methods because

1565
01:42:27,830 --> 01:42:31,370
they didn't work on datasets is big

1566
01:42:36,750 --> 01:42:41,770
here's an essay ten years that a hundred your code is one hundred real values

1567
01:42:41,870 --> 01:42:46,350
and here's your autoencoder ten real values and it does better

1568
01:42:46,680 --> 01:42:53,000
another thing you could do is actually take you down two units

1569
01:42:53,060 --> 01:42:56,140
to see what's going on so we can take it down to just two real

1570
01:42:56,140 --> 01:42:57,470
values here

1571
01:42:57,540 --> 01:43:00,390
and that's going to be good for visualization

1572
01:43:00,430 --> 01:43:04,390
we're saying give me two real values that contain as much information as possible

1573
01:43:04,410 --> 01:43:07,350
about the world can factor

1574
01:43:07,390 --> 01:43:09,350
and then we can lay this too

1575
01:43:09,350 --> 01:43:11,270
two values out into d

1576
01:43:11,290 --> 01:43:16,120
and we can unsupervised layer bunch documents out in two d

1577
01:43:16,160 --> 01:43:20,100
and then in somerset county provided the labels we can come up

1578
01:43:20,120 --> 01:43:21,410
by the labels

1579
01:43:21,430 --> 01:43:23,810
and see if we found the structure of the

1580
01:43:23,870 --> 01:43:25,220
class structure

1581
01:43:25,220 --> 01:43:29,370
so here's what happens if you use PCA

1582
01:43:29,370 --> 01:43:33,770
there is some class structure here obviously

1583
01:43:33,870 --> 01:43:38,020
here's what happens if you use the deep autoencoders

1584
01:43:38,060 --> 01:43:40,310
this better class structure

1585
01:43:40,330 --> 01:43:45,470
this is for a subset of the main categories in the reuters dataset

1586
01:43:45,470 --> 01:43:49,490
so when you look at these green ones initially you think

1587
01:43:49,490 --> 01:43:53,910
boys discovered lots of very complicated structures that actually need to bear in mind that

1588
01:43:53,910 --> 01:43:57,140
this isn't showing all of the data points and so these gaps you may well

1589
01:43:57,140 --> 01:43:59,060
be filled by other classes right

1590
01:43:59,310 --> 01:44:02,660
it's not that it chose to put these long way away

1591
01:44:03,200 --> 01:44:09,700
even though it could put in place in these repelled from other classes in here

1592
01:44:10,430 --> 01:44:13,180
anyway is found the cluster too much better

1593
01:44:13,180 --> 01:44:15,380
and this is then the hilbert space representation

1594
01:44:16,020 --> 01:44:16,780
as a distance

1595
01:44:17,350 --> 01:44:21,290
and it turns out this kind of representation works for a slightly larger class of kernels

1596
01:44:21,870 --> 01:44:23,860
which are called conditionally positive definite

1597
01:44:26,060 --> 01:44:26,660
so it turns out

1598
01:44:27,720 --> 01:44:30,550
the all algorithms that are translation invariant

1599
01:44:31,470 --> 01:44:34,330
such as compass ieee or are lesbians

1600
01:44:35,070 --> 01:44:38,200
can actually be also be done with conditionally positive definite kernels

1601
01:44:39,970 --> 01:44:43,280
some examples of kernels that are conditionally positive definite but not

1602
01:44:43,930 --> 01:44:46,520
positive definite so one example for instances

1603
01:44:47,080 --> 01:44:48,990
well just approximate example

1604
01:44:49,790 --> 01:44:51,780
in the early days of support vector machines

1605
01:44:52,410 --> 01:44:53,920
people were quite interested in

1606
01:44:54,490 --> 01:44:55,110
this kernel

1607
01:44:55,870 --> 01:44:56,420
which was the

1608
01:44:57,410 --> 01:44:58,740
hyperbolic tangent of

1609
01:44:59,780 --> 01:45:03,640
uh the dot product between x and x prime plus some

1610
01:45:06,800 --> 01:45:09,340
and that this is actually not a positive definite kernel

1611
01:45:10,170 --> 01:45:13,600
if you compute the kernel matrix with this and you know the i can values

1612
01:45:14,440 --> 01:45:17,190
typically find a negative igon value people work right

1613
01:45:17,670 --> 01:45:18,490
bother with that but

1614
01:45:19,000 --> 01:45:22,730
it like that because it looks like the activation function of neural network

1615
01:45:24,590 --> 01:45:25,710
in those days was still

1616
01:45:26,240 --> 01:45:29,400
when the when when we wrote of his papers about support vector machines

1617
01:45:29,900 --> 01:45:32,590
the reviewers would ask us to compare against neural networks

1618
01:45:33,310 --> 01:45:34,130
and then at some point

1619
01:45:34,870 --> 01:45:36,170
maybe six or seven years later

1620
01:45:36,920 --> 01:45:40,700
they would ask everybody to compare them methods against support vectors

1621
01:45:41,490 --> 01:45:42,130
and there are now

1622
01:45:42,560 --> 01:45:46,270
i don't know maybe the wave is swinging back again to neural networks so maybe

1623
01:45:46,880 --> 01:45:47,190
we have

1624
01:45:48,230 --> 01:45:49,440
mary mixture

1625
01:45:51,420 --> 01:45:52,390
all sorts of methods

1626
01:45:53,070 --> 01:45:55,200
anyway so this is the thing was a bit funny

1627
01:45:55,890 --> 01:45:58,950
but i removed by at some point that i was a student

1628
01:45:59,360 --> 01:46:02,150
i looked at the values for which this actually worked

1629
01:46:02,620 --> 01:46:04,800
it didn't work for all values are being

1630
01:46:05,280 --> 01:46:08,790
if i remember correctly so the hyperbolic tangent is not something like this

1631
01:46:10,090 --> 01:46:13,690
for me it only works if i had a negative be

1632
01:46:14,090 --> 01:46:16,990
which means shifting this curve to the right so i actually had the curve

1633
01:46:17,750 --> 01:46:18,550
a bit like this

1634
01:46:19,990 --> 01:46:23,910
end and if i looked at how best where it also had to

1635
01:46:24,470 --> 01:46:26,970
make sure the data is normalized in a certain range

1636
01:46:27,620 --> 01:46:29,050
and then at some point i noticed that

1637
01:46:29,570 --> 01:46:36,030
they range i was operating with looked suspiciously like a polynomial kernel that sometimes use just shifted a little bit

1638
01:46:37,130 --> 01:46:40,250
now it turns out if you take it if case a positive definite kernel

1639
01:46:42,350 --> 01:46:43,450
and then actually came

1640
01:46:46,850 --> 01:46:48,520
let's see could also be a negative number

1641
01:46:49,090 --> 01:46:51,390
is always at least conditionally positive different

1642
01:46:55,020 --> 01:46:58,440
so therefore the even though the hypertension is not positive definite

1643
01:47:00,790 --> 01:47:05,970
can be well approximated in a certain range and maybe the range where the data lived in our experiments

1644
01:47:06,590 --> 01:47:08,780
but conditionally positive definite kernel and that's why

1645
01:47:09,300 --> 01:47:11,470
it it worked also with support vector machines

1646
01:47:13,090 --> 01:47:14,180
but then it was forgotten

1647
01:47:15,120 --> 01:47:17,520
and people didn't worry about neural networks anymore

1648
01:47:19,460 --> 01:47:20,130
until recently

1649
01:47:22,550 --> 01:47:23,250
then they returned

1650
01:47:26,150 --> 01:47:31,110
this is an example of a non trivial kernel that we were playing around with at the time

1651
01:47:31,880 --> 01:47:32,340
and we were

1652
01:47:32,780 --> 01:47:36,240
interested in handwritten digit recognition mostly was mostly

1653
01:47:37,400 --> 01:47:42,390
about images are very simple images in inferences we have these kernels that we found

1654
01:47:42,830 --> 01:47:45,590
work a be better than standard polynomial kernels that

1655
01:47:46,430 --> 01:47:47,980
because pyramid kernels

1656
01:47:49,470 --> 01:47:49,830
and our

1657
01:47:50,630 --> 01:47:52,190
in computer vision people are using

1658
01:47:52,560 --> 01:47:54,710
as the standard model that works based on

1659
01:47:55,630 --> 01:48:01,020
so classification problems is a permanent quality pyramid match kernel i think it's not exactly

1660
01:48:01,020 --> 01:48:03,040
the same thing as this but this sort of a similar idea

1661
01:48:04,740 --> 01:48:05,450
but i think this

1662
01:48:06,290 --> 01:48:10,190
they have reinvented everything and maybe it's appropriate that are different

1663
01:48:10,910 --> 01:48:13,020
but this is one the idea here was to

