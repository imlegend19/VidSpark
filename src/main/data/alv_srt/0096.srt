1
00:00:00,000 --> 00:00:00,860
and the three

2
00:00:00,880 --> 00:00:02,960
contains ship

3
00:00:02,980 --> 00:00:05,810
seven both synonyms of each other so

4
00:00:05,860 --> 00:00:08,790
the document should have some semantic similarity even though

5
00:00:08,810 --> 00:00:11,130
they don't have any terms in common

6
00:00:11,130 --> 00:00:15,980
and that's reflected in the reduced representation because we have

7
00:00:16,040 --> 00:00:19,580
limited details like the same concept

8
00:00:20,790 --> 00:00:22,060
expressed by one

9
00:00:22,850 --> 00:00:27,960
by one more than one document and by another another document is omitted those details

10
00:00:27,960 --> 00:00:28,690
are now

11
00:00:28,690 --> 00:00:29,420
in this

12
00:00:29,420 --> 00:00:31,480
improved representation dissimilarity

13
00:00:31,480 --> 00:00:36,350
becomes directly visible

14
00:01:00,750 --> 00:01:03,150
yes i think

15
00:01:03,170 --> 00:01:08,150
that should work because i mean effectively what you're doing is you're compute your

16
00:01:08,670 --> 00:01:13,860
i think the debate regarding the matter is that you compute the probability

17
00:01:13,880 --> 00:01:18,040
argument continued the which in the document containing the robot

18
00:01:18,380 --> 00:01:21,690
and that should be similar

19
00:01:21,690 --> 00:01:26,290
in the reduced representation

20
00:01:26,380 --> 00:01:31,940
see all these

21
00:01:39,150 --> 00:01:50,290
i mean the same way of doing that what is suggesting which is is a

22
00:01:50,290 --> 00:01:52,580
good idea to actually

23
00:01:52,650 --> 00:01:53,690
do that

24
00:01:56,330 --> 00:01:57,460
i mean here

25
00:01:57,480 --> 00:02:01,460
in the reduced use so if your

26
00:02:01,460 --> 00:02:05,330
the clean way of doing that person you're suggesting is to throw away these numbers

27
00:02:05,330 --> 00:02:10,710
and directly compute here the similarity between ship and boat and you can see that

28
00:02:10,710 --> 00:02:11,440
it is

29
00:02:11,440 --> 00:02:15,040
quite high vital role the point

30
00:02:15,060 --> 00:02:19,270
your faultless your point

31
00:02:19,290 --> 00:02:20,230
well it's not

32
00:02:21,150 --> 00:02:21,850
o point

33
00:02:21,860 --> 00:02:23,670
o point one or

34
00:02:23,670 --> 00:02:25,500
o point two i guess

35
00:02:25,540 --> 00:02:27,650
well have to normalize so

36
00:02:27,670 --> 00:02:32,330
but i think it probably would have to be around point five

37
00:02:32,360 --> 00:02:34,580
so in this matrix you can

38
00:02:34,630 --> 00:02:36,670
compute the product the similarity

39
00:02:36,710 --> 00:02:40,980
comes directly

40
00:02:42,400 --> 00:02:49,020
OK so now we have everything for latent semantic indexing

41
00:02:49,020 --> 00:02:55,730
and i takes documents that are semantically similar talk about the same topics

42
00:02:55,770 --> 00:03:00,210
but are not similar in the original vector space because they use different words

43
00:03:00,230 --> 00:03:06,000
and really represents the in the reduced space in which they have higher similarity

44
00:03:06,020 --> 00:03:11,650
thus LSI addresses the problems of synonymy and semantic relatedness

45
00:03:11,670 --> 00:03:15,600
and the sun vector space and contribute nothing to documents and

46
00:03:15,620 --> 00:03:21,940
the desired effect of LSI circuits and contribute strongly to documents from

47
00:03:23,270 --> 00:03:24,270
and so on

48
00:03:24,290 --> 00:03:31,850
the white house hostess related to dimension reduction because dimensionality reduction for system in a

49
00:03:31,850 --> 00:03:33,540
lot of detail

50
00:03:33,560 --> 00:03:37,190
and we have to map different words different dimensions of the full space to the

51
00:03:38,000 --> 00:03:40,850
underlying dimension in the reduced space

52
00:03:40,850 --> 00:03:43,750
the cost of mapping synonyms to the same dimension

53
00:03:43,790 --> 00:03:44,880
is less

54
00:03:46,710 --> 00:03:51,190
the cost of mapping synonyms to the same dimension is less than the cost of

55
00:03:51,190 --> 00:03:52,960
collapsing unrelated words

56
00:03:53,000 --> 00:03:56,350
because their distribution of happen system

57
00:03:56,350 --> 00:04:01,710
svd selects the least costly method that's why it's a good tool for doing dimensionality

58
00:04:07,500 --> 00:04:12,520
and i would say something about the optimality in a few slides

59
00:04:12,540 --> 00:04:13,710
now this may

60
00:04:13,730 --> 00:04:15,920
sound familiar to some of you know

61
00:04:15,980 --> 00:04:21,900
what information retrieval two relevance feedback and query expansion

62
00:04:21,920 --> 00:04:23,730
because those are the

63
00:04:23,750 --> 00:04:27,600
techniques that can be used if we have a mismatch between

64
00:04:27,650 --> 00:04:31,690
in this match in

65
00:04:31,710 --> 00:04:37,060
which terms are used in creating a document that said

66
00:04:38,230 --> 00:04:39,230
and so on

67
00:04:39,230 --> 00:04:43,730
rather than expansion in those cases increase recall

68
00:04:44,900 --> 00:04:45,960
and as i

69
00:04:45,960 --> 00:04:51,310
in principle does the same it increases recall but it can decision

70
00:04:51,330 --> 00:04:55,560
thus it addresses the same problems as relevance feedback in expansion and it has the

71
00:04:55,560 --> 00:05:00,360
same problems so we can see it as is something that most closely related

72
00:05:00,440 --> 00:05:05,420
as far as other techniques are concerned to relevance feedback and query expansion

73
00:05:05,420 --> 00:05:10,130
although it is also different i can maybe comment about that if you have questions

74
00:05:12,520 --> 00:05:16,310
first we compute the SVD of the term document matrix

75
00:05:16,310 --> 00:05:20,480
then you reduce the space and compute reduced document representations

76
00:05:20,500 --> 00:05:23,480
here we have yet another parameter that we have to deal with in this case

77
00:05:23,480 --> 00:05:26,710
the parameters the dimensionality that we want to keep

78
00:05:26,730 --> 00:05:28,580
in my example was to

79
00:05:28,580 --> 00:05:35,380
in practice people usually choose numbers like one hundred or two hundred to three hundred

80
00:05:35,380 --> 00:05:38,830
that means that the query into the reduced space so

81
00:05:38,880 --> 00:05:40,790
we we need a way of

82
00:05:40,790 --> 00:05:44,060
i mean we don't know what the queries on that's so

83
00:05:44,080 --> 00:05:45,960
when a new query comes in

84
00:05:45,980 --> 00:05:48,960
we need to be able to map it into the new space so that's why

85
00:05:48,960 --> 00:05:50,400
we need this formula

86
00:05:50,420 --> 00:05:53,650
and this formula directly follows from the decomposition so

87
00:05:53,670 --> 00:05:57,020
if you take the decomposition formula into multiply by

88
00:05:58,150 --> 00:06:01,270
first on the left and then

89
00:06:02,330 --> 00:06:04,900
sigma inverse

90
00:06:05,170 --> 00:06:07,040
then get the k

91
00:06:07,830 --> 00:06:09,730
the query is like document

92
00:06:10,060 --> 00:06:12,150
in the sense bases apartments o

93
00:06:12,170 --> 00:06:15,650
if you multiply with this problem then

94
00:06:15,750 --> 00:06:18,420
you get the theory in the reduced space

95
00:06:18,440 --> 00:06:21,980
because of course the original query is in the original space and we have to

96
00:06:21,980 --> 00:06:25,120
transform it into the reduced space

97
00:06:25,170 --> 00:06:27,630
then we compute the similarity of two k

98
00:06:27,670 --> 00:06:29,540
reduced documents OK

99
00:06:29,540 --> 00:06:34,130
and output a ranked list of documents is huge

100
00:06:34,150 --> 00:06:40,980
what's the fundamental problem because there is a big problem here that i haven't talked

101
00:06:41,690 --> 00:06:44,770
does anybody see what that is

102
00:06:44,790 --> 00:06:48,960
in the sea

103
00:06:48,980 --> 00:06:50,860
what's the problem of the index

104
00:06:50,880 --> 00:06:57,290
right there are no zero values

105
00:06:57,310 --> 00:06:58,810
and so on

106
00:06:58,860 --> 00:07:00,250
the matrix

107
00:07:00,250 --> 00:07:04,420
before i had a matrix that i could transform into and out in the next

108
00:07:04,420 --> 00:07:08,100
month and it would be very efficient and now i have a dense matrix but

109
00:07:08,100 --> 00:07:09,440
no zeros

110
00:07:09,480 --> 00:07:16,040
the way i put it is that we actually can use an inverted index i

111
00:07:16,040 --> 00:07:20,650
mean this equivalent to what is that we actually have to

112
00:07:20,670 --> 00:07:25,520
do this year to compute similarity of concave all reduced documents in k

113
00:07:26,250 --> 00:07:29,310
actually means we have to go through all documents and compute

114
00:07:29,350 --> 00:07:32,670
the similarity between the query and document for each

115
00:07:32,690 --> 00:07:35,400
and that's much more expensive than what we usually do

116
00:07:35,400 --> 00:07:37,710
in information retrieval system so

117
00:07:38,330 --> 00:07:45,940
it's very difficult to make analysis system as efficient as as normal

118
00:07:47,920 --> 00:07:52,060
information is retrieval system is

119
00:07:52,130 --> 00:07:55,900
OK optimality SVD is optimal in the following sense

120
00:07:55,920 --> 00:08:00,330
keeping the k largest singular values and setting all others to zero gives you the

121
00:08:00,330 --> 00:08:04,330
optimal approximation of the original matrix c

122
00:08:04,350 --> 00:08:08,100
optimal in the following sense no other matrix of the same rank with the same

123
00:08:08,100 --> 00:08:11,580
underlying dimensionality approximates the better

124
00:08:11,630 --> 00:08:14,880
the message here is the frobenius norm where you take all

125
00:08:14,880 --> 00:08:20,030
this function here which is associated with that

126
00:08:20,050 --> 00:08:21,780
so it's the same thing

127
00:08:21,820 --> 00:08:24,400
lambda he's one of partitions

128
00:08:24,450 --> 00:08:26,240
of n elements

129
00:08:26,240 --> 00:08:27,610
that index

130
00:08:27,630 --> 00:08:30,130
there were that representation

131
00:08:31,400 --> 00:08:33,780
what do you want to do this

132
00:08:33,820 --> 00:08:35,700
you can

133
00:08:35,740 --> 00:08:42,130
that the key to to efficient here is to use recursion

134
00:08:42,190 --> 00:08:43,420
and you have

135
00:08:43,470 --> 00:08:47,950
the the essential element regression is to use concept

136
00:08:47,970 --> 00:08:53,530
so you have because you know that core sets the they either composed the group

137
00:08:53,550 --> 00:08:54,860
in two us some

138
00:08:54,880 --> 00:08:57,220
union of course sets

139
00:08:57,970 --> 00:08:58,820
there is

140
00:08:58,860 --> 00:09:04,400
nine representation theory you have a relation to construct the representation of

141
00:09:04,420 --> 00:09:05,780
as n

142
00:09:05,780 --> 00:09:11,200
you can use bits of representations of the smaller subgroups

143
00:09:11,200 --> 00:09:14,840
so this will be able speed up computation

144
00:09:14,860 --> 00:09:17,530
so you need you can break this

145
00:09:17,550 --> 00:09:21,800
in two you know small example in

146
00:09:21,840 --> 00:09:22,610
in the

147
00:09:22,610 --> 00:09:23,300
and the

148
00:09:23,320 --> 00:09:26,530
in the subgroup is this

149
00:09:30,800 --> 00:09:36,130
this is actually result from representation theory that you can

150
00:09:36,220 --> 00:09:42,950
ah you know you relate the representations of the supergroup with the representations of the

151
00:09:42,970 --> 00:09:44,670
of the remaining

152
00:09:44,720 --> 00:09:45,860
that's why this

153
00:09:45,900 --> 00:09:51,320
these are all down here means the representation restricted to that sort of

154
00:09:51,340 --> 00:09:52,550
anyway you can

155
00:09:52,550 --> 00:09:53,820
coke later this

156
00:09:54,670 --> 00:09:56,260
and q

157
00:09:56,320 --> 00:09:58,860
time this was result from regime

158
00:09:58,860 --> 00:10:02,240
last year

159
00:10:02,360 --> 00:10:05,260
and here you can do that

160
00:10:05,300 --> 00:10:08,990
graph correlation can compare two graphs

161
00:10:09,630 --> 00:10:16,880
the key here is that all this lots of those mattresses representing

162
00:10:18,190 --> 00:10:19,470
the group

163
00:10:19,490 --> 00:10:22,050
of permutation of n elements

164
00:10:22,050 --> 00:10:24,240
they may be orthogonal

165
00:10:24,950 --> 00:10:27,470
the were then results box

166
00:10:27,510 --> 00:10:31,670
so many of the representations you don't even have to

167
00:10:31,760 --> 00:10:33,450
performed the product

168
00:10:33,450 --> 00:10:36,840
so just have to perform in this case here

169
00:10:36,880 --> 00:10:39,400
you just take four

170
00:10:39,550 --> 00:10:44,110
representations to do this product

171
00:10:44,150 --> 00:10:46,420
to do this property

172
00:10:46,470 --> 00:10:49,010
and this you can do in the

173
00:10:49,030 --> 00:10:50,510
and four

174
00:10:50,550 --> 00:10:52,130
scholar operation

175
00:10:52,190 --> 00:10:54,200
so this thing is faster do

176
00:10:54,220 --> 00:10:56,340
because you don't have to do

177
00:10:58,170 --> 00:11:00,900
you would have to do normally all

178
00:11:00,930 --> 00:11:02,510
count all terms

179
00:11:02,510 --> 00:11:06,510
but many of them with is zero anyway so if you know that before and

180
00:11:06,510 --> 00:11:07,740
you can just

181
00:11:07,740 --> 00:11:13,050
you can just speed up the calculation

182
00:11:16,260 --> 00:11:17,340
the the

183
00:11:18,200 --> 00:11:25,090
the traveling salesman salesman problem it's the version of that of that problem that can

184
00:11:25,090 --> 00:11:27,400
be written in terms of

185
00:11:27,400 --> 00:11:29,840
finding the the

186
00:11:29,950 --> 00:11:33,200
the permutation that minimize the correlation of

187
00:11:33,260 --> 00:11:37,650
this graph with this particular those divisions metrics

188
00:11:37,650 --> 00:11:40,050
with these are the graph here

189
00:11:40,090 --> 00:11:44,090
so if you can rewrite you can rewrite some of the classical problems in graph

190
00:11:44,090 --> 00:11:47,630
theory using using the

191
00:11:47,630 --> 00:11:51,110
the language of harmonic analysis

192
00:11:51,110 --> 00:11:56,300
and speedup calculations

193
00:11:57,170 --> 00:11:59,970
o japanese parts or the

194
00:12:01,550 --> 00:12:03,550
i want to go back to

195
00:12:03,590 --> 00:12:05,900
he two

196
00:12:05,920 --> 00:12:09,240
two convolution again

197
00:12:09,260 --> 00:12:12,090
this these are

198
00:12:12,110 --> 00:12:18,930
probably can't see anything here

199
00:12:24,760 --> 00:12:26,990
get it

200
00:12:32,090 --> 00:12:35,170
sometimes the most of work

201
00:12:54,590 --> 00:12:58,780
it's more

202
00:12:58,780 --> 00:13:03,170
well you cannot see right can you see there's this green line

203
00:13:03,190 --> 00:13:06,260
c and the green line

204
00:13:07,570 --> 00:13:13,550
so this is actually what the college

205
00:13:20,700 --> 00:13:27,300
so this is for us you know we're doing the convolution of these

206
00:13:27,320 --> 00:13:29,380
this will function

207
00:13:29,420 --> 00:13:31,280
with the

208
00:13:31,340 --> 00:13:33,030
red line

209
00:13:33,030 --> 00:13:39,780
and they called the result of the convolution is this green one

210
00:13:39,800 --> 00:13:43,690
as it grows when it's not touching each other when it gets in the center

211
00:13:43,690 --> 00:13:46,090
here gets the maximum value

212
00:13:46,110 --> 00:13:49,650
he was out of order of phase again it was

213
00:13:49,670 --> 00:13:51,650
the value diminish

214
00:13:51,670 --> 00:13:54,670
and here is the same thing with graphs

215
00:13:54,990 --> 00:13:57,320
the convolution of the green and

216
00:13:58,820 --> 00:14:01,530
the function is centered on this

217
00:14:01,550 --> 00:14:03,670
there's a green line here

218
00:14:03,720 --> 00:14:06,340
and this is the result of the call

219
00:14:06,400 --> 00:14:07,840
the convolution here

220
00:14:07,840 --> 00:14:09,550
is maximal

221
00:14:09,550 --> 00:14:13,280
when you have these group operation here

222
00:14:13,320 --> 00:14:16,240
in this case the group operation is a translation

223
00:14:16,990 --> 00:14:19,490
so i'm just saying this show

224
00:14:19,510 --> 00:14:21,760
it to find his mother child

225
00:14:26,360 --> 00:14:29,820
in showing this because of

226
00:14:29,840 --> 00:14:34,820
i would not have much time to to go through all the applications that

227
00:14:34,840 --> 00:14:38,130
but this is of this form here

228
00:14:38,130 --> 00:14:41,360
OK so now we have this additional information

229
00:14:41,380 --> 00:14:46,360
and then we can have a two-step process in in translating sentence

230
00:14:46,380 --> 00:14:48,650
so the first step is to segment

231
00:14:48,710 --> 00:14:50,270
german sentence

232
00:14:50,300 --> 00:14:52,710
and again to choose the set of the entries

233
00:14:52,730 --> 00:14:57,520
and then have a set of these or sequence of these syntactic fragments

234
00:14:57,540 --> 00:15:01,190
because we have a sequence of this tree fragments

235
00:15:02,060 --> 00:15:04,790
the second step is to somehow assemble

236
00:15:04,800 --> 00:15:07,750
these tree fragments into a full parse tree

237
00:15:07,750 --> 00:15:09,650
so that essentially involves

238
00:15:09,670 --> 00:15:14,320
something very similar to the junction operations i showed you on the talk we start

239
00:15:14,320 --> 00:15:15,400
to see these

240
00:15:15,460 --> 00:15:17,750
tree structures together with some costs

241
00:15:17,770 --> 00:15:20,980
and we cover a an english parse tree

242
00:15:21,110 --> 00:15:24,750
and some is allowed so that you have to move around the ring of these

243
00:15:24,750 --> 00:15:27,840
different fragments so finally we get the translation

244
00:15:27,840 --> 00:15:30,790
the output of the system is distinguished past

245
00:15:30,800 --> 00:15:32,770
so that's the sketch of this model

246
00:15:32,790 --> 00:15:34,570
a couple of things about this

247
00:15:34,590 --> 00:15:38,490
so one is that the search process again we're going have some process the first

248
00:15:38,490 --> 00:15:40,130
searches for these fragments

249
00:15:40,170 --> 00:15:42,460
and looks at how to combine them

250
00:15:42,500 --> 00:15:46,570
this can be implemented using essentially extensions of

251
00:15:46,590 --> 00:15:48,630
algorithms used regular

252
00:15:48,630 --> 00:15:53,000
so this is what i mean by translation as parsing problem was essentially

253
00:15:53,230 --> 00:15:58,820
the translation problems decoding under one of these parsing models

254
00:15:58,820 --> 00:16:02,570
and there's some potential advantages which have been shown in this work one is that

255
00:16:02,570 --> 00:16:08,320
you build an english parse tree and have some control over grammaticality of the tree

256
00:16:08,320 --> 00:16:11,540
so you you have some control over the fact that they need to subject and

257
00:16:11,540 --> 00:16:13,520
object in all these different things

258
00:16:13,570 --> 00:16:15,920
the second thing is that in capturing

259
00:16:16,040 --> 00:16:18,880
the difference in between two languages

260
00:16:18,900 --> 00:16:20,590
for example if we don't care

261
00:16:20,630 --> 00:16:24,960
the fact that the subject comes after the before it in english

262
00:16:25,000 --> 00:16:28,980
we can capture those kind of facts by reordering on these polls trees which is

263
00:16:28,980 --> 00:16:35,500
called the natural way to capture real reordering in the two languages

264
00:16:35,500 --> 00:16:40,130
so let's transition let not to inference of these models

265
00:16:40,150 --> 00:16:41,770
so one question

266
00:16:41,790 --> 00:16:42,860
left open

267
00:16:42,880 --> 00:16:46,130
how we find the most likely parse tree under the model

268
00:16:46,130 --> 00:16:47,000
i showed you

269
00:16:47,020 --> 00:16:49,380
so here again actually the model form

270
00:16:49,480 --> 00:16:55,270
a parse tree is scored by the sum of scores of these different junction operations

271
00:16:55,380 --> 00:16:57,730
so the main point of this part of the talk

272
00:16:57,750 --> 00:17:01,040
so dynamic programming algorithms exist

273
00:17:01,060 --> 00:17:02,610
for TAG grammars

274
00:17:02,630 --> 00:17:05,920
which actually rules for the TAG grammars i've shown you

275
00:17:05,940 --> 00:17:10,980
i have very similar runtime two algorithms for PCFG

276
00:17:10,980 --> 00:17:12,920
so for example cubic time

277
00:17:13,130 --> 00:17:15,670
time in the length of the sentence

278
00:17:15,940 --> 00:17:17,500
so that's some promising

279
00:17:17,520 --> 00:17:21,770
unfortunately exact inference under these models are still very expensive

280
00:17:21,790 --> 00:17:26,750
that is prohibitively expensive you can easily spend minutes pausing assumptions

281
00:17:26,750 --> 00:17:31,040
and inferences about both applying the model to new sentences

282
00:17:31,040 --> 00:17:33,300
and also during training of the model

283
00:17:33,320 --> 00:17:39,440
OK because the training algorithms will see for conditional random fields generally require decoding the

284
00:17:39,440 --> 00:17:42,060
training examples multiple times

285
00:17:42,070 --> 00:17:45,090
as inferences about american training

286
00:17:45,090 --> 00:17:50,090
so one solution which has been found effective many times in the parsing lecture

287
00:17:50,130 --> 00:17:53,320
it is to use medical coarse to fine dynamic programming

288
00:17:53,320 --> 00:17:57,520
for example charniak has been using this for a long time and parsing models

289
00:17:57,520 --> 00:18:01,690
the basic idea here is to use the first past model

290
00:18:01,710 --> 00:18:05,090
which is a simple computationally cheap model

291
00:18:05,110 --> 00:18:08,690
to restrict the search space of the full parsing model

292
00:18:08,880 --> 00:18:11,730
and this can actually be remarkably effective

293
00:18:11,790 --> 00:18:17,380
and the models will see so let me describe this idea

294
00:18:17,400 --> 00:18:22,400
so to understand these algorithms will actually go to another syntactic formalisms we've seen context

295
00:18:22,400 --> 00:18:24,670
free grammars syntaxes

296
00:18:24,730 --> 00:18:29,730
number token little bit about dependency grammars with dependency representations

297
00:18:29,770 --> 00:18:33,440
so the top of the slide is the dependency structure

298
00:18:33,440 --> 00:18:39,000
which can be thought of as an alternative representation to context free trees

299
00:18:39,040 --> 00:18:42,670
so basically is formed bidirected graph

300
00:18:42,730 --> 00:18:44,290
which forms a tree

301
00:18:44,300 --> 00:18:48,960
this special node called star the nodes are about to star

302
00:18:49,000 --> 00:18:51,250
at the root of the tree

303
00:18:51,300 --> 00:18:53,690
and we have talks between

304
00:18:53,980 --> 00:18:56,060
basically heads and modifiers

305
00:18:56,060 --> 00:18:59,820
so as the dependency for example between germans or

306
00:18:59,840 --> 00:19:03,960
basically corresponding to the subject dependency

307
00:19:03,980 --> 00:19:06,860
the simplest form of this dependency structures

308
00:19:06,880 --> 00:19:09,210
just as i've shown you there

309
00:19:09,230 --> 00:19:13,210
in a slightly more fine for me might have labels on these socks might actually

310
00:19:13,210 --> 00:19:14,820
have label same subject

311
00:19:14,840 --> 00:19:17,590
saying that he is the object of saw

312
00:19:17,650 --> 00:19:19,790
and so on and so on

313
00:19:19,840 --> 00:19:22,360
so this can be thought of as an alternative

314
00:19:22,440 --> 00:19:27,980
representation is again quite widely used in natural natural language community another way of representing

315
00:19:27,980 --> 00:19:30,610
syntactic structures

316
00:19:31,360 --> 00:19:33,460
a few years ago when mcdonald's

317
00:19:33,480 --> 00:19:35,130
and collaborators

318
00:19:35,150 --> 00:19:37,800
i came up with a CRF style

319
00:19:37,960 --> 00:19:40,040
models for dependency parsing

320
00:19:40,060 --> 00:19:43,110
so now we can score these structures

321
00:19:43,270 --> 00:19:46,020
using again the serious star model

322
00:19:46,860 --> 00:19:50,940
the school semantic structure is going to be a sum of the dependency on that

323
00:19:50,940 --> 00:19:55,540
structure and that dependency gets school which is just another in product

324
00:19:55,590 --> 00:20:01,480
features now track dependencies essentially any information about the input sentence

325
00:20:01,650 --> 00:20:10,130
so one thing which is remarkable remarkable about these dependency structures

326
00:20:10,170 --> 00:20:13,190
is that they have very efficient decoding algorithms

327
00:20:13,190 --> 00:20:14,820
james joyce meissner

328
00:20:14,840 --> 00:20:19,290
so for the structures to show you which have no labels the most probable

329
00:20:19,320 --> 00:20:22,730
well cost dependency structure under this kind of model

330
00:20:22,750 --> 00:20:26,460
can be found found in cubic time in the length of the sentence

331
00:20:26,480 --> 00:20:28,610
very low constants

332
00:20:29,500 --> 00:20:35,650
and the eisna came up with the really quite beautiful algorithms the dynamic programming algorithms

333
00:20:35,670 --> 00:20:40,980
and they did compose dependency structures into subparts and quite ingenious way that kind of

334
00:20:40,980 --> 00:20:43,070
dynamic programming bottom up search

335
00:20:43,090 --> 00:20:47,210
so it looks rather different from say of context free grammar but the complexity is

336
00:20:47,210 --> 00:20:48,750
actually very similar

337
00:20:48,770 --> 00:20:52,670
to see if which also cubic and the length of the sentence

338
00:20:52,690 --> 00:20:53,820
and was

339
00:20:53,840 --> 00:20:56,670
this to great effect in training

340
00:20:56,690 --> 00:20:58,460
discriminative models

341
00:20:58,500 --> 00:21:01,190
based on these dependency representations

342
00:21:01,210 --> 00:21:07,020
in fact these representations of fission enough exhausted parsing is really quite quite plausible quite

343
00:21:09,570 --> 00:21:12,920
so what does this fit with the tax structures that i've shown you

344
00:21:12,920 --> 00:21:14,770
so if you think about is

345
00:21:14,790 --> 00:21:16,750
tag structures

346
00:21:16,750 --> 00:21:20,090
they look very much like dependency structures

347
00:21:20,380 --> 00:21:22,070
some additional information

348
00:21:22,090 --> 00:21:26,070
so essentially dependency structures augmented with these spines

349
00:21:26,090 --> 00:21:30,060
and also augmented with these positions of these the junction points and these points

350
00:21:30,110 --> 00:21:33,360
so for example there is essentially a dependency

351
00:21:33,420 --> 00:21:35,630
between the with environments

352
00:21:35,840 --> 00:21:39,800
now it's mediated by these two spines in this junction position

353
00:21:39,820 --> 00:21:43,670
so you can think of tank structures as being

354
00:21:43,790 --> 00:21:49,800
more elaborate dependency structures essentially does the close connection between the two

355
00:21:49,820 --> 00:21:54,400
and actually means that iciness algorithms can also be applied to this formalism

356
00:21:56,020 --> 00:21:59,250
the most probable lowest cost house

357
00:21:59,250 --> 00:22:00,870
the brain

358
00:22:00,880 --> 00:22:03,240
this is the brain

359
00:22:03,250 --> 00:22:07,510
in fact it's a specific person's brain

360
00:22:07,560 --> 00:22:12,800
and what's interesting about the brain is that white mark there

361
00:22:12,870 --> 00:22:14,550
it's her brain

362
00:22:14,560 --> 00:22:16,860
it's terry child's brain

363
00:22:16,880 --> 00:22:20,480
you recognise or more from pictures like that

364
00:22:20,530 --> 00:22:23,180
and what a case like this where somebody

365
00:22:25,360 --> 00:22:28,130
in the home is without consciousness

366
00:22:28,180 --> 00:22:32,770
as a result the damage to the brain is the start illustration of the physical

367
00:22:32,770 --> 00:22:34,830
nature of mental life

368
00:22:34,850 --> 00:22:38,330
the physical basis for everything that we normally hold dear

369
00:22:38,340 --> 00:22:39,880
like free will

370
00:22:41,270 --> 00:22:43,500
more ali and and emotions

371
00:22:43,520 --> 00:22:47,900
and that's will be in the course talking about how a physical things

372
00:22:47,920 --> 00:22:52,050
can give rise to mental life

373
00:22:52,060 --> 00:22:54,060
we'll talk a lot about children

374
00:22:54,780 --> 00:22:57,070
this is actually a specific child

375
00:22:57,080 --> 00:23:00,160
it's my son's akari younger son

376
00:23:00,180 --> 00:23:05,850
dressed up as superman spider-man but it is how it knows not how it

377
00:23:08,760 --> 00:23:11,120
it's more to say about that

378
00:23:11,140 --> 00:23:14,830
i study child development for living

379
00:23:14,840 --> 00:23:19,530
and i'm interested in several questions so one question is just the question of development

380
00:23:19,530 --> 00:23:21,520
everybody in this room

381
00:23:21,560 --> 00:23:24,070
i can speak and understand english

382
00:23:24,080 --> 00:23:28,600
everybody in this room has some understanding of how the world works how physical things

383
00:23:28,930 --> 00:23:32,370
behave everybody in this room has some understanding of other people

384
00:23:32,420 --> 00:23:33,470
and how people

385
00:23:35,680 --> 00:23:36,890
and the question

386
00:23:36,900 --> 00:23:42,190
that preoccupies developmental psychologists is how do we come to have the knowledge

387
00:23:42,200 --> 00:23:47,650
and in particular how much of it is hard-wired built in in a

388
00:23:47,700 --> 00:23:50,150
and how much of it is the product of culture

389
00:23:50,230 --> 00:23:53,520
of of language of schooling

390
00:23:53,590 --> 00:23:56,080
and developmental psychologists use

391
00:23:56,090 --> 00:23:59,680
many ingenious methods to try to pull these apart

392
00:23:59,700 --> 00:24:02,980
and try to figure out what what what's

393
00:24:03,070 --> 00:24:07,200
what are the basic components of human nature

394
00:24:07,220 --> 00:24:11,480
there's also the question of continuity

395
00:24:11,490 --> 00:24:12,940
to what extent

396
00:24:12,980 --> 00:24:15,430
is that during that age

397
00:24:15,450 --> 00:24:17,410
going to be

398
00:24:17,430 --> 00:24:19,120
that way forever

399
00:24:19,130 --> 00:24:21,730
to what extent is your fate sealed

400
00:24:21,780 --> 00:24:26,270
to what extent could if i were to me when you were five years old

401
00:24:26,270 --> 00:24:29,000
i could describe the way you are now

402
00:24:29,100 --> 00:24:32,010
the poet

403
00:24:32,030 --> 00:24:33,540
william wordsworth

404
00:24:35,630 --> 00:24:38,340
the child is father to the man

405
00:24:38,390 --> 00:24:40,010
and what this means is

406
00:24:40,020 --> 00:24:41,280
but you can see

407
00:24:41,290 --> 00:24:43,000
within every child

408
00:24:43,010 --> 00:24:45,260
the adult here she will become

409
00:24:45,270 --> 00:24:50,660
we will look in and ask the question whether this is true is it true

410
00:24:50,660 --> 00:24:57,110
for your personality is it true for your interest is it true for your intelligence

411
00:24:57,120 --> 00:25:01,140
another question having to do with development

412
00:25:02,140 --> 00:25:04,930
what makes us the way we are

413
00:25:04,970 --> 00:25:07,470
we're different into waterways

414
00:25:07,520 --> 00:25:10,170
the people in this room differ according to

415
00:25:10,220 --> 00:25:14,320
their taste in food they differ according to IQ's whether there

416
00:25:14,330 --> 00:25:16,210
aggressive or or shy

417
00:25:16,250 --> 00:25:19,890
whether they are attracted to males females both or neither

418
00:25:21,010 --> 00:25:22,940
whether they good music

419
00:25:24,370 --> 00:25:27,270
there politically liberal or conservative

420
00:25:27,290 --> 00:25:28,790
one week different

421
00:25:28,810 --> 00:25:31,650
what is the explanation for why were different

422
00:25:31,660 --> 00:25:33,030
and again

423
00:25:33,040 --> 00:25:37,190
this could be translated in terms of the question of genes and environment to what

424
00:25:37,190 --> 00:25:41,720
extent are things the result of the genes we possess to what extent

425
00:25:41,810 --> 00:25:46,190
are are are individual majors is the result of how we were raised and to

426
00:25:46,190 --> 00:25:52,110
what extent are the best explained in terms of the interaction one common theory for

427
00:25:52,110 --> 00:25:55,490
instance is that we are shaped by our parents

428
00:25:55,540 --> 00:26:02,370
this was best summarized most famously by the british poet philip larkin

429
00:26:02,500 --> 00:26:04,390
who wrote

430
00:26:04,450 --> 00:26:07,000
they mess you up

431
00:26:07,020 --> 00:26:09,280
your mum and dad

432
00:26:09,290 --> 00:26:12,170
they may not mean to but they do not

433
00:26:12,210 --> 00:26:14,680
they fill you with the faults they had

434
00:26:14,690 --> 00:26:16,800
and that some extra just for you

435
00:26:16,820 --> 00:26:19,560
the right

436
00:26:19,580 --> 00:26:25,140
this very controversial you it's been a a series of a huge controversy in popular

437
00:26:25,140 --> 00:26:28,570
culture the extent to which parents matter

438
00:26:28,590 --> 00:26:31,050
and this is an issue which will preoccupy us

439
00:26:31,060 --> 00:26:33,790
for much of the course

440
00:26:33,870 --> 00:26:35,700
a different question

441
00:26:35,710 --> 00:26:37,980
what makes somebody attractor

442
00:26:37,980 --> 00:26:39,560
in the second part of the

443
00:26:40,040 --> 00:26:44,440
tutorial we going to discuss compressed sensing so in some sense

444
00:26:45,280 --> 00:26:49,470
we're going to jump forward in time by a few years by about twenty years

445
00:26:49,580 --> 00:26:52,620
and so on

446
00:26:53,120 --> 00:26:57,800
maybe that everybody comes back

447
00:27:01,620 --> 00:27:09,070
right so we're going talk about compressed sensing

448
00:27:09,170 --> 00:27:14,140
so the first thing i like to say about this field compressed sensing is these

449
00:27:14,140 --> 00:27:16,820
days it's it's an enormous field

450
00:27:17,460 --> 00:27:22,960
weighs thousands of research papers literally being published yearly

451
00:27:22,990 --> 00:27:28,320
addressing all aspects of the theory great ranging from theory

452
00:27:28,330 --> 00:27:34,130
two lots of algorithmic papers these days people exploiting applications it's an enormous field

453
00:27:36,640 --> 00:27:40,610
perhaps a bit for additional so they have several

454
00:27:41,390 --> 00:27:48,960
scientific journals published special issues on compressed sensing lots of tripoli journals publishing special issues

455
00:27:49,450 --> 00:27:55,650
if you go to major international conferences and mass in in electrical engineering in signal

456
00:27:55,650 --> 00:28:02,840
processing information theory also special sessions on this topic but there are several scientific conferences

457
00:28:03,200 --> 00:28:06,260
discussed this devoted to this topic

458
00:28:06,790 --> 00:28:12,060
an interesting thing is that the US government things to think that great thing that's

459
00:28:12,060 --> 00:28:14,560
is investing a lot of money into

460
00:28:14,570 --> 00:28:17,620
trying to realised in hardware promise

461
00:28:17,650 --> 00:28:21,950
of compressed sensing if you go on the world wide web you have

462
00:28:22,100 --> 00:28:26,390
several blogs dedicated to this topic

463
00:28:26,420 --> 00:28:29,060
and so on and so forth

464
00:28:29,070 --> 00:28:30,040
OK so

465
00:28:30,060 --> 00:28:34,400
i'm just not saying this is this is is a very impressive of course there

466
00:28:34,400 --> 00:28:38,200
is the danger that it's a bit vanish so time will tell whether it's

467
00:28:38,290 --> 00:28:39,790
is that it should not

468
00:28:39,810 --> 00:28:43,870
but it's a very big field were lots of people contribute

469
00:28:43,870 --> 00:28:50,040
as we have seen these lectures people contribute hardware people contribute reasons people contribute theory

470
00:28:50,040 --> 00:28:51,610
that people contribute lots of

471
00:28:51,640 --> 00:28:57,700
very different things and because it's an enormous field with several thousands of papers already

472
00:28:57,700 --> 00:29:01,270
i cannot give you an overview of this field what i will do it i

473
00:29:01,270 --> 00:29:06,200
will focus on the early ideas that were at the premise of the has developed

474
00:29:06,200 --> 00:29:10,140
by a group and to some extent by by david

475
00:29:11,240 --> 00:29:15,700
so the motivation underlying compressed sensing is extremely simple

476
00:29:18,360 --> 00:29:20,540
it is it possible

477
00:29:20,570 --> 00:29:22,730
to acquire information efficient

478
00:29:22,760 --> 00:29:24,360
that is

479
00:29:24,380 --> 00:29:27,520
there is i wish to make an image and so

480
00:29:27,570 --> 00:29:31,580
we can talk again about that you have which leads in r and so we

481
00:29:31,580 --> 00:29:36,680
could think about it now has a digital picture i wish to acquire digital pictures

482
00:29:36,680 --> 00:29:42,510
say that has and pixels introduces i'm going to make measurements about these objects and

483
00:29:42,510 --> 00:29:44,480
so i'm going to measure

484
00:29:44,850 --> 00:29:48,100
correlations between the object i wish to acquire

485
00:29:49,200 --> 00:29:51,260
away waveforms phi k

486
00:29:51,290 --> 00:29:53,860
and so i'm going to have a number of

487
00:29:54,230 --> 00:29:58,100
measurements which is equal to m as before

488
00:29:58,290 --> 00:30:02,830
so there of this form just linear measurements are going to be interested in situations

489
00:30:03,600 --> 00:30:06,790
we have very few measurements before i do this

490
00:30:06,790 --> 00:30:09,980
i just want to show that for example if i k then indicator function of

491
00:30:09,980 --> 00:30:11,170
a little square

492
00:30:11,190 --> 00:30:13,690
then y k will be pixel intensity

493
00:30:13,700 --> 00:30:17,490
at pixel number k because it will just count the number of photons hitting a

494
00:30:17,490 --> 00:30:19,980
little square sharing

495
00:30:20,820 --> 00:30:24,860
we cannot be interested in situations where is number of measurements we can take about

496
00:30:24,860 --> 00:30:26,760
an pixel image

497
00:30:27,610 --> 00:30:28,860
are very small

498
00:30:28,890 --> 00:30:30,410
and this

499
00:30:30,770 --> 00:30:35,040
comes up in a lot of applications for example i may have you're few sensors

500
00:30:35,230 --> 00:30:39,570
so in some fields sensors are very expensive to manufacture so for example if you

501
00:30:39,570 --> 00:30:40,440
try to

502
00:30:40,450 --> 00:30:43,760
build an infrared camera for example

503
00:30:43,770 --> 00:30:47,200
a camera operating invisible

504
00:30:47,200 --> 00:30:48,640
i have some entity called be

505
00:30:49,080 --> 00:30:49,720
well yes i do

506
00:30:50,310 --> 00:30:52,830
it happens to be in in three so prince three

507
00:30:56,870 --> 00:30:59,000
if you ask what is saying

508
00:31:00,430 --> 00:31:03,520
i can't says okay in my environment do i have some

509
00:31:05,080 --> 00:31:05,560
the name

510
00:31:08,000 --> 00:31:10,640
it doesn't find it so it brings out this cryptic message

511
00:31:11,930 --> 00:31:16,040
basically says sorry guys i can't find something called eh

512
00:31:16,660 --> 00:31:17,970
in the current environment

513
00:31:19,640 --> 00:31:23,290
that's the key to the way python does all name binding

514
00:31:26,200 --> 00:31:28,970
in general there's a global environment

515
00:31:30,140 --> 00:31:31,600
you start typing the on

516
00:31:32,600 --> 00:31:35,220
it just start adding and modifying

517
00:31:35,970 --> 00:31:36,910
the bindings

518
00:31:37,310 --> 00:31:38,540
in the binding environment

519
00:31:39,370 --> 00:31:41,370
so if you type eight with three in taipei

520
00:31:41,910 --> 00:31:42,560
know find three

521
00:31:43,720 --> 00:31:45,680
if you don't i'd be was eight a-plus two

522
00:31:46,720 --> 00:31:48,910
it evaluates the right-hand side

523
00:31:50,080 --> 00:31:52,120
relative to the current environment

524
00:31:53,700 --> 00:31:55,120
so it first looks here

525
00:31:56,040 --> 00:31:58,600
and says do i have something called it

526
00:32:00,700 --> 00:32:01,810
i it's an integer three

527
00:32:02,830 --> 00:32:03,700
substitute back

528
00:32:05,040 --> 00:32:09,680
do i know what to it as an end pluses area that's the thing that combines to it

529
00:32:10,390 --> 00:32:12,790
so it decides that plus two

530
00:32:13,560 --> 00:32:16,350
it evaluates a-plus in the current environment gets five

531
00:32:17,140 --> 00:32:18,660
and is i'm trying to do a new

532
00:32:20,410 --> 00:32:21,970
a new association new new variable

533
00:32:22,740 --> 00:32:23,640
make thee

534
00:32:24,270 --> 00:32:25,060
name be

535
00:32:26,000 --> 00:32:26,620
o point two

536
00:32:27,290 --> 00:32:29,520
this is evaluated in the current environment

537
00:32:30,000 --> 00:32:31,910
so be is associated with int

538
00:32:34,870 --> 00:32:36,540
then if i do this line it

539
00:32:37,680 --> 00:32:40,350
evaluates me plus one in the current environment

540
00:32:43,250 --> 00:32:46,910
these five in the current environment it adds wanna get six

541
00:32:48,080 --> 00:32:50,850
and it says associate this thing

542
00:32:52,580 --> 00:32:52,930
with me

543
00:32:54,000 --> 00:32:55,620
so it overrides the be

544
00:32:56,060 --> 00:32:57,850
which had been bound to five

545
00:32:58,830 --> 00:33:00,620
and if be is now bound sex

546
00:33:02,520 --> 00:33:04,790
so the whole thing the waitress variables

547
00:33:06,600 --> 00:33:10,450
the way associates in the weight height and associate a name with value in a variable

548
00:33:10,930 --> 00:33:14,250
is evaluate the right hand side according to the current environment

549
00:33:15,020 --> 00:33:16,790
then change the current environment

550
00:33:17,350 --> 00:33:19,180
to reflect the new binding

551
00:33:21,240 --> 00:33:23,790
what it doesn't cases subroutines is very similar

552
00:33:26,020 --> 00:33:26,540
when you say

553
00:33:26,980 --> 00:33:29,870
so here's an illustration only in the local environment

554
00:33:31,140 --> 00:33:33,020
is generated by this piece of code

555
00:33:33,640 --> 00:33:34,890
when i say it was to

556
00:33:35,330 --> 00:33:37,540
generates a name in the local environment eh

557
00:33:38,870 --> 00:33:41,390
it evaluates the right-hand side and finds tool

558
00:33:42,700 --> 00:33:47,750
so it makes a binding in the local environment where the name is associated with integer tool

559
00:33:50,910 --> 00:33:53,580
i say define that's to be return x squared

560
00:33:56,790 --> 00:33:58,020
that's more complicated

561
00:33:58,970 --> 00:33:59,620
by says

562
00:34:00,930 --> 00:34:02,660
and defining a new operation

563
00:34:06,660 --> 00:34:07,500
it's a procedure

564
00:34:09,000 --> 00:34:12,060
the procedure has a formal argument x

565
00:34:14,200 --> 00:34:15,240
it has a body

566
00:34:16,100 --> 00:34:17,830
return x times x

567
00:34:18,950 --> 00:34:20,870
i'm gonna have to remember all that's stuff

568
00:34:24,040 --> 00:34:29,450
i'm trying to define new procedure called square it's gonna make a binding for square

569
00:34:31,080 --> 00:34:37,640
so in the future if somebody says the words quare it'll find out elsewhere i remembered our squaring

570
00:34:38,750 --> 00:34:39,790
it's a procedure

571
00:34:42,180 --> 00:34:44,310
just like the binding free variable

572
00:34:44,830 --> 00:34:49,390
might be int the binding fore eight procedure

573
00:34:50,470 --> 00:34:52,200
is the name of the procedure

574
00:34:53,790 --> 00:34:58,600
then in the procedure which is some other data structure outside the environment

575
00:34:59,480 --> 00:35:03,040
it's gotta remember the formal parameters in this case acts

576
00:35:04,290 --> 00:35:05,000
and the body

577
00:35:07,950 --> 00:35:09,750
and for the purpose of resolving

578
00:35:10,580 --> 00:35:19,640
what do the variables mean it needs to remember what was these binding environment in which this subroutine was defined

579
00:35:20,790 --> 00:35:22,350
so that's really that's the sour

580
00:35:23,910 --> 00:35:29,790
this this sequence says make a new binding square points to procedure in the procedure has the formal

581
00:35:30,350 --> 00:35:31,410
argument acts

582
00:35:32,720 --> 00:35:35,270
it has the body return x times x

583
00:35:36,830 --> 00:35:38,350
and you has the binding

584
00:35:38,970 --> 00:35:42,910
it came from the environment you want the current remember

585
00:35:45,270 --> 00:35:50,970
okay is everybody clear so the idea is that's he environment associates names with things

586
00:35:50,970 --> 00:35:54,060
the thing could be a data item or could be a procedure

587
00:35:57,390 --> 00:35:59,080
then when you call procedure

588
00:36:01,600 --> 00:36:02,500
it makes a new environment

589
00:36:04,160 --> 00:36:07,470
so what happens then when i try to evaluate a form

590
00:36:07,870 --> 00:36:09,520
so whereof a-plus two

591
00:36:12,120 --> 00:36:13,870
what i found us it says okay

592
00:36:14,740 --> 00:36:16,750
i need the figure out what's queries

593
00:36:18,310 --> 00:36:21,810
so it looks it up in environment and finds out that squares a procedure

594
00:36:23,040 --> 00:36:24,970
five i know how to deal with procedures

595
00:36:26,450 --> 00:36:27,520
so then it

596
00:36:28,080 --> 00:36:31,040
three years out this procedure has a formal argument acts

597
00:36:31,850 --> 00:36:35,060
okay from gonna around this procedure and what have know xmins

598
00:36:36,910 --> 00:36:39,970
so makes a new environment here is labeled e to

599
00:36:40,620 --> 00:36:43,810
separate from the the global environment e one

600
00:36:44,540 --> 00:36:45,910
it makes a new environment

601
00:36:46,470 --> 00:36:48,350
that will associate x with something

602
00:36:49,470 --> 00:36:50,370
doesn't know what it is yet

603
00:36:50,930 --> 00:36:51,850
it just knows there

604
00:36:52,450 --> 00:36:55,910
this queries a procedure that takes a four argument x

605
00:36:57,370 --> 00:37:00,950
so makes a new environment e to with exploiting the something

606
00:37:01,830 --> 00:37:07,200
then pi on evaluates theargument a-plus to in the environment you want

607
00:37:10,000 --> 00:37:14,330
you called square error plus two in the environment you one so it figures out

608
00:37:14,330 --> 00:37:16,220
what did you mean by a-plus

609
00:37:16,220 --> 00:37:18,930
and when i get on the right was

610
00:37:19,740 --> 00:37:21,220
i'll just write down

611
00:37:23,740 --> 00:37:27,010
this very page domain over two

612
00:37:27,090 --> 00:37:30,470
he made the truth

613
00:37:30,570 --> 00:37:33,820
and the factor to disappear results

614
00:37:35,470 --> 00:37:40,110
in the transform domain

615
00:37:40,160 --> 00:37:44,590
well this was accomplished right this was that this was really a convolution

616
00:37:44,630 --> 00:37:46,130
followed by

617
00:37:46,150 --> 00:37:49,530
the change of scale

618
00:37:49,590 --> 00:37:50,900
and of course

619
00:37:50,920 --> 00:37:55,380
in the frequency domain becomes the multiplication with the change of scale

620
00:37:55,400 --> 00:37:57,630
change in the opposite direction

621
00:37:57,680 --> 00:38:03,610
it was two t there will be only over to here i really recommend that

622
00:38:03,610 --> 00:38:05,550
now there's a serious

623
00:38:05,590 --> 00:38:10,880
suggested exercises that take the foragers it's just terrific practice

624
00:38:11,070 --> 00:38:15,470
take the fourier transforms and say they get so you see that

625
00:38:15,490 --> 00:38:19,590
we have may made every time we

626
00:38:20,530 --> 00:38:26,610
let's take one more step this would be a major over two

627
00:38:26,660 --> 00:38:28,590
now what could i substitute for

628
00:38:28,590 --> 00:38:34,720
and i will make two

629
00:38:34,800 --> 00:38:36,360
using the

630
00:38:36,380 --> 00:38:37,430
the same

631
00:38:37,490 --> 00:38:40,490
this recursively it would be a

632
00:38:40,530 --> 00:38:44,300
over four times the

633
00:38:44,320 --> 00:38:46,990
from four

634
00:38:51,160 --> 00:38:54,570
pauses second because it's only

635
00:38:55,470 --> 00:38:58,380
so nice in the frequency domain

636
00:38:58,430 --> 00:39:02,110
but there are two routes nice but it's not

637
00:39:02,130 --> 00:39:04,700
like self evident because it still is

638
00:39:04,700 --> 00:39:07,200
change of scale

639
00:39:07,300 --> 00:39:09,430
course i carry on

640
00:39:09,430 --> 00:39:10,530
the nature of

641
00:39:10,550 --> 00:39:11,800
two times a

642
00:39:11,820 --> 00:39:13,030
four times age

643
00:39:13,030 --> 00:39:15,760
eight and you can see that in the limit

644
00:39:15,780 --> 00:39:18,220
this is a picture me or two

645
00:39:21,180 --> 00:39:26,050
that's my formula

646
00:39:26,150 --> 00:39:29,970
what this guy the tail end

647
00:39:29,990 --> 00:39:33,300
OK i was what you know good question

648
00:39:33,360 --> 00:39:35,280
so it became

649
00:39:35,320 --> 00:39:37,990
became goes away some goes to one

650
00:39:38,050 --> 00:39:40,110
why does it go to one

651
00:39:40,160 --> 00:39:45,220
so there should be a fair share of omega or so i'm finding that this

652
00:39:45,220 --> 00:39:48,050
this guy

653
00:39:48,070 --> 00:39:53,220
no because if have zero right

654
00:39:53,240 --> 00:39:57,050
because i can dividing by two

655
00:39:59,260 --> 00:40:01,880
this is approaching

656
00:40:01,880 --> 00:40:07,400
factors that the tail and so on so that about seventy seven terms in product

657
00:40:07,780 --> 00:40:10,070
for over two hundred seventy seven

658
00:40:10,090 --> 00:40:15,780
and it's made over to the surface so it's very near the speed of zero

659
00:40:15,800 --> 00:40:17,050
and that's what

660
00:40:28,920 --> 00:40:30,530
so but no

661
00:40:30,550 --> 00:40:35,470
i don't know why why not

662
00:40:35,510 --> 00:40:43,200
maybe it's because we're in the frequency domain

663
00:40:43,220 --> 00:40:47,680
it it bears checking of course

664
00:40:47,820 --> 00:40:56,820
so let's see

665
00:40:57,120 --> 00:41:01,380
first of all why is fear zero one well that's the same as i the

666
00:41:01,570 --> 00:41:02,970
convention mentioned

667
00:41:02,990 --> 00:41:04,360
that was that

668
00:41:04,380 --> 00:41:07,530
that's the

669
00:41:07,550 --> 00:41:12,610
that's the right

670
00:41:12,660 --> 00:41:18,340
so somewhere in here

671
00:41:18,360 --> 00:41:21,320
let's see

672
00:41:21,590 --> 00:41:24,720
i know it does approach one

673
00:41:26,930 --> 00:41:28,990
i don't know the right

674
00:41:29,180 --> 00:41:32,320
explanation to give you her

675
00:41:32,340 --> 00:41:34,340
we're seeing

676
00:41:34,400 --> 00:41:35,510
so this is

677
00:41:35,510 --> 00:41:39,450
if i take j terms here so i

678
00:41:39,470 --> 00:41:44,740
going along stopping to do the change eventually and this would be

679
00:41:44,740 --> 00:41:48,200
this would be the only for each element

680
00:41:48,200 --> 00:41:49,820
three jamaica

681
00:41:50,450 --> 00:41:52,630
this approach feed of zero

682
00:41:53,300 --> 00:42:00,380
maybe this is the point this gives the same answer good answer for each omega

683
00:42:00,400 --> 00:42:04,300
well good answer this weekend product well

684
00:42:04,340 --> 00:42:11,260
the world's is answered and then if we want to ask the question what we

685
00:42:11,380 --> 00:42:14,950
have an inverse fourier transform still do

686
00:42:14,970 --> 00:42:21,110
we like to do that so you can see that it's extremely lucky

687
00:42:22,340 --> 00:42:26,920
if you access to get have an explicit solution not happen

688
00:42:26,970 --> 00:42:27,990
and maybe

689
00:42:28,180 --> 00:42:31,530
the question is this form would

690
00:42:31,590 --> 00:42:32,760
you are

691
00:42:33,780 --> 00:42:35,840
a way to tackle my

692
00:42:35,840 --> 00:42:39,070
little convolution question

693
00:42:39,070 --> 00:42:42,840
what was the question that we started with that

694
00:42:42,840 --> 00:42:44,550
if i had

695
00:42:44,590 --> 00:42:46,510
if i had the feeling e

696
00:42:46,570 --> 00:42:49,050
would solve this equation

697
00:42:49,760 --> 00:42:52,800
now if i can involve a chart with itself

698
00:42:52,800 --> 00:42:54,900
and i look at equation

699
00:42:54,930 --> 00:42:57,110
i'm trying to answer would be

700
00:42:57,130 --> 00:42:59,630
the involved with itself

701
00:42:59,650 --> 00:43:01,660
what works and why

702
00:43:01,680 --> 00:43:04,280
because the transfer this domain

703
00:43:04,300 --> 00:43:07,200
in the frequency domain one i

704
00:43:07,300 --> 00:43:11,070
all this with itself what to do to capital h

705
00:43:11,070 --> 00:43:13,570
we multiply by itself right

706
00:43:13,590 --> 00:43:20,660
so then there would be a great square and feet times three

707
00:43:20,680 --> 00:43:21,610
i have

708
00:43:21,930 --> 00:43:27,360
that's where the frequency domain to go back i have

709
00:43:28,320 --> 00:43:30,420
of cells

710
00:43:33,820 --> 00:43:35,400
the convolution

711
00:43:35,470 --> 00:43:39,110
up there

712
00:43:39,150 --> 00:43:43,530
but this a little bit

713
00:43:43,550 --> 00:43:45,400
tangential to the

714
00:43:45,450 --> 00:43:46,880
central well

715
00:43:46,900 --> 00:43:50,300
because the what we discover is

716
00:43:50,360 --> 00:43:53,240
we've got an answer here

717
00:43:53,260 --> 00:43:55,820
at each omega

718
00:43:55,880 --> 00:43:57,300
so why do i say

719
00:43:57,340 --> 00:44:00,420
the solution is in doubt

720
00:44:00,420 --> 00:44:03,300
why do i say not always one i

721
00:44:03,320 --> 00:44:04,900
produced and

722
00:44:04,950 --> 00:44:10,300
and it does this product does converge is free to make

723
00:44:10,320 --> 00:44:14,150
because the ages of if this is a low pass filter so i really need

724
00:44:14,150 --> 00:44:16,510
to build the fact some

725
00:44:16,510 --> 00:44:19,300
one of them is one

726
00:44:19,340 --> 00:44:22,220
that's that's also part of my page

727
00:44:22,240 --> 00:44:23,530
and this

728
00:44:23,530 --> 00:44:25,700
these things approach one

729
00:44:27,150 --> 00:44:30,700
product is converges but

730
00:44:30,700 --> 00:44:32,740
so can i ask again

731
00:44:32,740 --> 00:44:40,840
i can answer why should i say there isn't always an answer

732
00:44:44,030 --> 00:44:48,590
that's right it can answer freecell many but now if i want to go back

733
00:44:48,590 --> 00:44:51,740
and find the function

734
00:44:52,900 --> 00:44:56,240
well by the way what was the answer when went for the lead

735
00:44:56,240 --> 00:44:57,430
1 and group 2

736
00:44:57,810 --> 00:44:59,330
and over here

737
00:45:00,560 --> 00:45:05,100
the group 6 and 7 so those are the ones that are most likely form

738
00:45:05,100 --> 00:45:13,390
ionic bonds OK not only did he proposes the quantified and what he did something

739
00:45:13,390 --> 00:45:17,700
even even better but still

740
00:45:21,100 --> 00:45:36,100
he defines

741
00:45:36,380 --> 00:45:44,660
something called certainly partial charges

742
00:45:45,350 --> 00:45:49,160
right now this is what government is what got the Nobel Prize

743
00:45:49,740 --> 00:45:56,660
and partial charge and I'll give you the formula of X so this is in

744
00:45:56,660 --> 00:46:01,740
your text and so

745
00:46:02,620 --> 00:46:11,220
x y bond right

746
00:46:11,580 --> 00:46:18,160
because that's what I tried to where

747
00:46:18,960 --> 00:46:23,920
that is the group number

748
00:46:23,960 --> 00:46:39,880
and so that is the number of non bonding nonbonding electron

749
00:46:40,140 --> 00:46:53,560
and of that is the number of bonding electrons OK now let's take this and

750
00:46:53,560 --> 00:46:58,960
look at what what we get of try chips right we know this is what

751
00:46:58,960 --> 00:47:01,940
we this is what comes out of this

752
00:47:02,930 --> 00:47:06,060
that 1

753
00:47:06,400 --> 00:47:13,580
we know that this is going to be my measurement there's going to be plus

754
00:47:13,580 --> 00:47:18,340
invisibility minus is going to be done it's going to be a dipole this processes

755
00:47:19,140 --> 00:47:25,830
and we know that kind fluorine is 4 .

756
00:47:26,480 --> 00:47:27,960
1 of

757
00:47:28,580 --> 00:47:33,560
and size of hydrogen is equal to

758
00:47:33,660 --> 00:47:35,990
2 . 3 0

759
00:47:37,330 --> 00:47:43,120
well the plug that in promises to 1 of these before just do the fluorine

760
00:47:43,240 --> 00:47:47,290
and by the way that if you do this for the hydrogen about the same

761
00:47:47,290 --> 00:47:49,850
that's equal to 7 minus 6

762
00:47:49,960 --> 00:47:51,420
minus 2

763
00:47:51,430 --> 00:47:58,260
times 4 . 1 9 over 4 . 1

764
00:47:58,440 --> 00:48:03,560
+ 2 . 3 and that gives you a number

765
00:48:03,980 --> 00:48:11,530
for the fluorine minus 0 . 2 9 right now if you do the same

766
00:48:11,530 --> 00:48:12,980
do the same for the

767
00:48:13,030 --> 00:48:14,990
for the for the hydrogen

768
00:48:15,860 --> 00:48:18,780
you can have the same number but it'll be plus

769
00:48:18,790 --> 00:48:22,910
all right but the plus and so

770
00:48:23,240 --> 00:48:30,300
then that what that tells us a chapters polar region of the image of this

771
00:48:32,310 --> 00:48:38,830
OK so we did that and but now let's go back to what the running

772
00:48:38,830 --> 00:48:44,330
of space must go back to let's go back to my and see what happens

773
00:48:44,350 --> 00:48:45,680
see what happens there

774
00:48:49,340 --> 00:49:08,580
let's go back to back nothing well the carbon is equal to 2 . 5

775
00:49:10,020 --> 00:49:16,340
I the hydrogen again what 2 . 3 and so on

776
00:49:17,880 --> 00:49:23,800
so what this tells us since these are people that tells us that the carbon

777
00:49:23,860 --> 00:49:25,720
hydrogen bonds

778
00:49:25,960 --> 00:49:31,520
is not well there would

779
00:49:32,040 --> 00:49:37,680
this poll right hydrogen bond is polar

780
00:49:38,100 --> 00:49:41,440
and so if we go in we draw this again this is the carbon here

781
00:49:42,140 --> 00:49:51,320
hydrogen hydrogen hydrogen hydrogen like this then this is going to be a little bit

782
00:49:52,760 --> 00:50:02,720
will work for us and this and is going to be negative right so we

783
00:50:02,730 --> 00:50:08,080
know from our calculations that we can do over here that this this bond

784
00:50:08,480 --> 00:50:16,840
is polar and this bond is polar but again as I alluded to earlier because

785
00:50:16,840 --> 00:50:22,620
the carbon is centered in the text regions because the S P 3 hybridization the

786
00:50:22,620 --> 00:50:25,800
molecule itself is symmetric and non polar

787
00:50:26,320 --> 00:50:30,600
OK and for this he got a the Nobel

788
00:50:30,810 --> 00:50:36,500
and I think 19 fifties

789
00:50:36,560 --> 00:50:43,080
1954 OK not 1 last thing needed is that OK

790
00:50:43,180 --> 00:50:44,380
what's going on here

791
00:50:45,440 --> 00:50:49,680
well on the 1 hand we have ionic bonding where we have complete sharing

792
00:50:50,080 --> 00:50:55,460
on the other hand if we have a perfect covalent bonds we have identical apps

793
00:50:55,460 --> 00:51:00,160
equal shares but we know that that just by this calculation that we don't have

794
00:51:00,160 --> 00:51:05,200
equal sharing so somewhere in between the bonds are we have a range of bonds

795
00:51:05,200 --> 00:51:10,340
between ionic on the 1 hand and perfectly covalent on the other hand but there's

796
00:51:10,340 --> 00:51:18,540
a whole range in between so what's going on there well he point also suggested

797
00:51:18,540 --> 00:51:24,660
that for that case that the bonds are partly ionic and partially covalent and he

798
00:51:24,660 --> 00:51:26,500
came up with a relationship

799
00:51:26,680 --> 00:51:38,390
calculate the % ionic character right and I'll just give it to you

800
00:51:44,600 --> 00:51:54,460
this is squared

801
00:51:56,980 --> 00:52:04,080
you % right so there's a relationship and so you can see where qi comes

802
00:52:04,080 --> 00:52:09,060
at work I comes in here and this is the the difference in electronegativity between

803
00:52:09,060 --> 00:52:11,380
the 2 between the 2 the 2 elements

804
00:52:12,600 --> 00:52:14,200
was time

805
00:52:15,100 --> 00:52:23,040
OK he also that we see here also lastly of the that closes out he

806
00:52:24,280 --> 00:52:29,000
developed an analytical expression for the energy that covalent bond which is really what we

807
00:52:29,000 --> 00:52:34,840
want right so that the type of

808
00:52:34,850 --> 00:52:39,430
on the bond energies that we can go and look up in the in tables

809
00:52:39,430 --> 00:52:41,420
another group with the saying well

810
00:52:41,430 --> 00:52:43,260
sorry that in this example

811
00:52:43,310 --> 00:52:46,160
the group for the sake is only the sake

812
00:52:47,240 --> 00:52:55,240
nevertheless it's it's cut and what's what's that cut down

813
00:52:55,250 --> 00:52:57,010
that cut has

814
00:52:57,030 --> 00:52:58,870
separated the source

815
00:52:58,880 --> 00:53:02,990
the same it's it's literally cut the two apart

816
00:53:03,050 --> 00:53:05,700
so that

817
00:53:05,870 --> 00:53:08,220
the capacity of course is

818
00:53:08,260 --> 00:53:15,260
they crossed the cut is the number of

819
00:53:15,280 --> 00:53:20,360
times we click the image so we are trying trying to say the maximum number

820
00:53:20,360 --> 00:53:27,940
of disjoint paths that measure how much flow we could do equals the minimum number

821
00:53:27,990 --> 00:53:32,920
of cut of a kind of cut edges

822
00:53:39,110 --> 00:53:43,310
you may use is the stronger there to split s

823
00:53:47,910 --> 00:53:50,800
the minimum number of edges that talk to

824
00:53:53,700 --> 00:53:57,430
separate this from this to this

825
00:53:57,490 --> 00:54:01,110
to say that this is

826
00:54:01,120 --> 00:54:03,930
a restatement maybe we could say why

827
00:54:03,950 --> 00:54:05,740
why i mean this is

828
00:54:05,950 --> 00:54:08,900
as usual

829
00:54:08,920 --> 00:54:13,750
we should see a weak duality so what what did we could well the mean

830
00:54:14,070 --> 00:54:19,930
weak duality which was usually the easy thing to to to see of just made

831
00:54:19,930 --> 00:54:22,810
suppose i just made this statement

832
00:54:22,860 --> 00:54:24,590
but i draw network

833
00:54:24,620 --> 00:54:29,630
and i look at how many separate paths there are they don't don't share any

834
00:54:29,630 --> 00:54:32,000
edges that's with disjoint means

835
00:54:32,030 --> 00:54:34,320
from source to sink how many

836
00:54:36,420 --> 00:54:41,360
and i looked at the minimum number of edges of cuts i had to make

837
00:54:41,380 --> 00:54:45,440
to separate the two now it's so we should be

838
00:54:45,550 --> 00:54:48,820
maximum less or equal minimum

839
00:54:48,850 --> 00:54:50,630
should be easy

840
00:54:50,650 --> 00:54:52,150
and remember it's

841
00:54:53,200 --> 00:54:58,750
so strange that it's that direction that easy course it's the maximum of one thing

842
00:54:58,750 --> 00:55:03,690
and the minimum of another why is that easy how do i know that

843
00:55:06,620 --> 00:55:08,060
but if i have

844
00:55:12,610 --> 00:55:17,490
the minimum number of edges to if i find if i find some images that

845
00:55:17,490 --> 00:55:19,910
split the how do i know

846
00:55:19,920 --> 00:55:23,520
then if i if i find any cut

847
00:55:23,560 --> 00:55:25,090
any any any

848
00:55:25,100 --> 00:55:27,600
a bunch of edges i could snap through

849
00:55:27,670 --> 00:55:32,300
that would separate source from saying i count those edges and i get k

850
00:55:32,340 --> 00:55:37,300
how do i know that the number of passes cannot be more than k

851
00:55:40,430 --> 00:55:42,050
if f

852
00:55:42,060 --> 00:55:43,390
exactly if

853
00:55:43,400 --> 00:55:46,230
each edge if

854
00:55:46,250 --> 00:55:51,560
if i manage to cut the same with k

855
00:55:51,570 --> 00:55:57,030
you could be more than k destroyed has because i wouldn't cut one of them

856
00:55:57,050 --> 00:56:02,090
when we say that again this is suppose

857
00:56:11,690 --> 00:56:14,210
from this track

858
00:56:14,220 --> 00:56:19,200
and can be

859
00:56:20,890 --> 00:56:22,800
can be greater than k

860
00:56:22,810 --> 00:56:28,150
this joint task

861
00:56:28,260 --> 00:56:36,450
because there are more than k this tripe as an i didn't cut them all

862
00:56:36,460 --> 00:56:38,190
and i have been separated

863
00:56:38,210 --> 00:56:40,270
the source from the sink

864
00:56:40,290 --> 00:56:46,800
so that would be their reasoning of the i take any cut that works

865
00:56:46,850 --> 00:56:48,920
say OK maybe it's OK

866
00:56:49,030 --> 00:56:52,190
separate edges to clip

867
00:56:53,040 --> 00:56:57,820
than the number of disjoint paths could be larger than k because

868
00:56:57,890 --> 00:57:04,970
and if it were one of those has isn't getting cut and and

869
00:57:05,000 --> 00:57:08,300
we haven't we haven't separated from as prime

870
00:57:08,310 --> 00:57:11,280
so that says that any

871
00:57:11,290 --> 00:57:14,640
any cut that's acceptable

872
00:57:14,660 --> 00:57:21,050
is large is at least as large as any counter destroyed past so we increase

873
00:57:21,050 --> 00:57:25,920
the counter this trend has up to its maximum we decrease the number of cuts

874
00:57:25,920 --> 00:57:31,140
to the do the best cutting we can and we have less or equal and

875
00:57:31,140 --> 00:57:34,580
the point is we actually have an equal

876
00:57:34,600 --> 00:57:36,130
maybe here's the way to

877
00:57:36,150 --> 00:57:39,310
here's the proof of equal

878
00:57:39,320 --> 00:57:41,390
i suppose

879
00:57:41,430 --> 00:57:46,770
we have well the the idea of the proof is that if you have

880
00:57:49,310 --> 00:57:51,370
suppose we have

881
00:57:51,450 --> 00:57:53,940
a maximum flow

882
00:57:53,950 --> 00:57:59,710
this is in the book and it's hard to take in verbally but if we

883
00:57:59,710 --> 00:58:02,020
had a maximum flow

884
00:58:02,060 --> 00:58:05,040
because we had the flow as large as possible

885
00:58:06,750 --> 00:58:10,980
i want to see twice some kind of that size is holding this up

886
00:58:11,000 --> 00:58:13,870
so we have max flow biggest possible

887
00:58:13,880 --> 00:58:15,770
that night

888
00:58:15,780 --> 00:58:18,000
keep with as some edges

889
00:58:18,210 --> 00:58:20,680
some somewhere in s

890
00:58:20,690 --> 00:58:25,050
some edges i might be able to send more troops

891
00:58:25,050 --> 00:58:30,750
so long some edges i might be able to send more for example along the

892
00:58:30,750 --> 00:58:33,570
edge i could say more

893
00:58:33,690 --> 00:58:38,410
it's interesting OK now i'm going i may find another cut here

894
00:58:38,450 --> 00:58:41,440
so i i keep my group with the s

895
00:58:41,450 --> 00:58:47,380
all the nodes to which i could still send one more OK

896
00:58:47,420 --> 00:58:51,940
the we see i'm the only users so with as i keep this note because

897
00:58:51,940 --> 00:58:56,800
i could still send one more to that

898
00:58:58,680 --> 00:59:04,100
now this is going to be tricky because i could now send backwards on this

899
00:59:04,100 --> 00:59:06,880
square and we spend the entire lecture

900
00:59:06,920 --> 00:59:09,850
on dealing with this age

901
00:59:09,860 --> 00:59:12,140
changed only

902
00:59:12,190 --> 00:59:14,710
we evaluated omega equals zero

903
00:59:14,720 --> 00:59:16,350
oh my god resonance

904
00:59:16,360 --> 00:59:18,850
omega very high values

905
00:59:18,950 --> 00:59:20,110
there is no

906
00:59:21,210 --> 00:59:23,530
constant in this solution

907
00:59:23,590 --> 00:59:24,830
but there was one

908
00:59:25,690 --> 00:59:29,710
that solution

909
00:59:29,720 --> 00:59:31,770
now let's look at the

910
00:59:31,800 --> 00:59:33,940
differential equations

911
00:59:33,980 --> 00:59:36,340
that we were sold

912
00:59:36,350 --> 00:59:38,600
so first we go

913
00:59:38,710 --> 00:59:40,780
only under even system

914
00:59:40,790 --> 00:59:43,950
under the names it equal zero you give given kicking and this led to do

915
00:59:43,950 --> 00:59:46,210
his own thing

916
00:59:46,210 --> 00:59:50,630
the differential equation that we then had was x double dot

917
00:59:50,640 --> 00:59:52,450
as gamma x dot

918
00:59:52,460 --> 00:59:55,660
because omega zero korean x

919
00:59:55,670 --> 00:59:56,900
equals zero

920
00:59:56,910 --> 00:59:58,720
that's the one we have

921
00:59:58,730 --> 01:00:00,000
we solve it

922
01:00:00,040 --> 01:00:03,530
and we found that solution there

923
01:00:03,570 --> 01:00:06,020
then we were driving it

924
01:00:06,080 --> 01:00:08,520
so now we drive it

925
01:00:08,570 --> 01:00:11,100
what now was the differential equation

926
01:00:11,150 --> 01:00:13,710
well we had annexed double but

927
01:00:13,770 --> 01:00:15,710
karma x dot

928
01:00:15,720 --> 01:00:18,710
was omega zero career times x

929
01:00:18,760 --> 01:00:20,110
and now we had

930
01:00:21,230 --> 01:00:23,400
this driving for

931
01:00:23,400 --> 01:00:24,980
in the case that we

932
01:00:26,200 --> 01:00:28,770
put the force on the object

933
01:00:28,780 --> 01:00:30,460
then we had here

934
01:00:30,470 --> 01:00:32,960
zero divided by

935
01:00:33,010 --> 01:00:36,980
times cosine omega t because the and comes in because you divide by an right

936
01:00:36,980 --> 01:00:39,880
yet newton's second law is made

937
01:00:39,940 --> 01:00:42,650
but in the case that you shake the left side

938
01:00:42,700 --> 01:00:45,530
raise your hands you're always is at the zero

939
01:00:45,540 --> 01:00:48,210
cosine omega team another way

940
01:00:48,210 --> 01:00:49,840
of effectively

941
01:00:49,840 --> 01:00:51,770
driving object

942
01:00:51,780 --> 01:00:54,200
we do work that out then we have here

943
01:00:54,250 --> 01:00:55,790
at the zero

944
01:00:55,800 --> 01:00:57,950
times omega zero squared

945
01:00:57,960 --> 01:01:00,150
times cosine

946
01:01:00,200 --> 01:01:01,450
but in any case

947
01:01:01,500 --> 01:01:02,570
you see here

948
01:01:02,580 --> 01:01:04,590
a driving term

949
01:01:04,650 --> 01:01:05,500
and we

950
01:01:05,510 --> 01:01:09,210
solve our equations and if you take this case

951
01:01:09,210 --> 01:01:10,590
this is the solution

952
01:01:10,600 --> 01:01:13,400
if you take this case than this

953
01:01:13,460 --> 01:01:14,840
take place

954
01:01:14,880 --> 01:01:18,380
of that

955
01:01:18,460 --> 01:01:19,750
suppose now

956
01:01:19,760 --> 01:01:22,440
i take this solution

957
01:01:22,640 --> 01:01:29,140
and i substitute that solution in this differential equation

958
01:01:29,150 --> 01:01:30,030
then i get

959
01:01:30,040 --> 01:01:31,510
the result is zero

960
01:01:31,540 --> 01:01:32,730
because look

961
01:01:32,750 --> 01:01:33,650
to get this

962
01:01:33,650 --> 01:01:35,400
so i put it in this part

963
01:01:35,510 --> 01:01:39,630
get zero because if fits this differential equation

964
01:01:40,880 --> 01:01:44,340
what is wrong with adding zero

965
01:01:45,570 --> 01:01:48,130
if i had to these two solutions

966
01:01:48,170 --> 01:01:52,960
it must be a solution to this differential equation

967
01:01:53,040 --> 01:01:54,780
because you just add zero

968
01:01:54,840 --> 01:01:57,080
and if you take eighteen o three

969
01:01:57,130 --> 01:01:59,100
and they use very

970
01:01:59,110 --> 01:02:01,750
nice terms they say yes of course

971
01:02:01,760 --> 01:02:04,260
if you have this special solution

972
01:02:04,300 --> 01:02:05,650
which is this one

973
01:02:05,690 --> 01:02:09,140
you have to add the homoge neous solution

974
01:02:09,150 --> 01:02:13,140
and these were homogeneous means that you put zero

975
01:02:13,230 --> 01:02:15,390
and so the general solution

976
01:02:15,400 --> 01:02:16,550
is really

977
01:02:16,570 --> 01:02:19,300
the sum of the two

978
01:02:19,340 --> 01:02:21,300
by adding this one

979
01:02:21,320 --> 01:02:23,790
you effectively and zero

980
01:02:23,840 --> 01:02:25,140
so you have nothing

981
01:02:25,150 --> 01:02:27,410
but you get something in return

982
01:02:27,520 --> 01:02:29,790
what you get in return you to

983
01:02:29,850 --> 01:02:31,520
adjustable constant

984
01:02:32,470 --> 01:02:34,540
you can deal with this situation

985
01:02:34,550 --> 01:02:36,140
that t equals zero

986
01:02:36,150 --> 01:02:38,200
you know exactly where

987
01:02:38,260 --> 01:02:41,360
that object is and what its velocity is

988
01:02:41,410 --> 01:02:43,370
so i will write down now

989
01:02:43,380 --> 01:02:45,580
the general solution

990
01:02:45,600 --> 01:02:46,810
which is the one

991
01:02:50,110 --> 01:02:52,910
these are part of the election today

992
01:02:52,920 --> 01:02:54,660
so x no

993
01:02:54,730 --> 01:02:56,760
the function of time

994
01:02:57,780 --> 01:02:59,580
the steady state solution

995
01:02:59,700 --> 01:03:02,280
eight cosine

996
01:03:02,360 --> 01:03:03,570
omega t

997
01:03:05,180 --> 01:03:06,390
this is my

998
01:03:07,870 --> 01:03:09,370
why i

999
01:03:09,400 --> 01:03:11,910
that's waterloo elements omega

1000
01:03:15,790 --> 01:03:17,700
times including minus

1001
01:03:17,720 --> 01:03:20,030
come over two times t

1002
01:03:20,040 --> 01:03:21,290
times cosine

1003
01:03:21,300 --> 01:03:23,540
omega prime she

1004
01:03:23,550 --> 01:03:24,900
because of

1005
01:03:24,950 --> 01:03:27,170
and this is the real

1006
01:03:27,180 --> 01:03:28,900
of the oscillator

1007
01:03:29,000 --> 01:03:30,480
and this is my will

1008
01:03:30,730 --> 01:03:31,920
two different

1009
01:03:31,930 --> 01:03:36,270
omega as

1010
01:03:36,270 --> 01:03:40,270
now you can see what happens

1011
01:03:40,350 --> 01:03:44,490
you see that this tour will never die out as well as for ever and

1012
01:03:44,490 --> 01:03:45,800
ever and ever

1013
01:03:45,860 --> 01:03:48,070
but this one is going to die

1014
01:03:48,150 --> 01:03:52,740
so one of e decay time of two alpha gamma

1015
01:03:52,780 --> 01:03:56,000
and so if to overcome i happens to be ten hours

1016
01:03:56,010 --> 01:03:59,210
there you have to wait ten hours for this one can be done by a

1017
01:03:59,210 --> 01:04:00,640
factor of e

1018
01:04:00,680 --> 01:04:04,250
but if you overcome i happens to be one millisecond then all you have to

1019
01:04:04,250 --> 01:04:06,290
do is what weight one many seconds

1020
01:04:06,300 --> 01:04:11,030
four that were to go down by a factor of so this is the one

1021
01:04:11,070 --> 01:04:12,810
that will die out

1022
01:04:12,870 --> 01:04:14,520
that's why we have to wait

1023
01:04:14,570 --> 01:04:16,020
so this is called

1024
01:04:16,030 --> 01:04:19,130
the trenchant

1025
01:04:19,170 --> 01:04:21,040
and it will die out faster

1026
01:04:21,050 --> 01:04:22,310
the higher

1027
01:04:23,250 --> 01:04:24,760
and this is called then

1028
01:04:24,770 --> 01:04:26,290
the steady state solution

1029
01:04:26,300 --> 01:04:27,560
which ultimately

1030
01:04:31,660 --> 01:04:32,910
i told you

1031
01:04:32,920 --> 01:04:35,880
that a teen equals zero

1032
01:04:35,890 --> 01:04:37,640
x equals zero

1033
01:04:37,690 --> 01:04:40,870
and actually dot also equal zero

1034
01:04:40,900 --> 01:04:42,610
and i

1035
01:04:42,620 --> 01:04:47,260
was so nasty to say oh by the way i don't use all four x

1036
01:04:47,260 --> 01:04:48,810
and of

1037
01:04:48,870 --> 01:04:50,870
it will be very nice thing to do

1038
01:04:50,980 --> 01:04:54,420
will take your fifteen minutes of grinding

1039
01:04:54,480 --> 01:04:56,720
not so fast because remember

1040
01:04:56,740 --> 01:04:58,130
if you have an expert

1041
01:04:58,170 --> 01:05:02,040
you have to take the time derivative of this entire functions

1042
01:05:02,080 --> 01:05:06,380
you have here you have two there you have to substitute in there

1043
01:05:06,390 --> 01:05:09,890
time t equals zero and they you have to make that equal zero

1044
01:05:09,930 --> 01:05:14,190
and the q fifteen minutes and out pops in a value for x and the

1045
01:05:14,200 --> 01:05:16,260
value for all

1046
01:05:16,360 --> 01:05:19,750
haven't learned much physics when you do that you to algebra

1047
01:05:19,850 --> 01:05:21,910
so i've decided not to

1048
01:05:21,970 --> 01:05:25,970
i spend my time on doing that but in principle

1049
01:05:26,010 --> 01:05:31,100
you must agree with me now the device especially for specified the initial conditions

1050
01:05:31,150 --> 01:05:34,600
i don't have to call this zero i can call this axis here i can

1051
01:05:34,600 --> 01:05:38,370
do anything i want to i can give x any value i want to

1052
01:05:38,380 --> 01:05:40,800
then i get unique values for

1053
01:05:40,810 --> 01:05:42,780
x and for all five

1054
01:05:43,560 --> 01:05:45,800
that is ultimately

1055
01:05:45,890 --> 01:05:47,150
then what

1056
01:05:47,230 --> 01:05:49,310
the solution is i must

1057
01:05:49,320 --> 01:05:51,640
at the two

1058
01:05:51,680 --> 01:05:54,900
the bottom line and that's really where the physics is

1059
01:05:54,910 --> 01:05:58,170
and that has to do with problem two five that you have this week on

1060
01:05:58,170 --> 01:05:59,640
your plate

1061
01:05:59,670 --> 01:06:01,780
is then the following

1062
01:06:01,790 --> 01:06:04,770
if i make a plot

1063
01:06:04,800 --> 01:06:09,220
of x as a function of time

1064
01:06:11,830 --> 01:06:14,280
the solution

1065
01:06:14,310 --> 01:06:17,780
is really the some of these two

1066
01:06:17,780 --> 01:06:24,300
on the segmentation step but with the graph representation of the two images then we

1067
01:06:24,300 --> 01:06:30,810
define a recursive weighted graph matching legacy because we have to do the matching we

1068
01:06:31,560 --> 01:06:39,610
want to understand if the matching is OK with if it is reliability function is

1069
01:06:39,610 --> 01:06:45,970
it OK and so all we can as we can have some donkey the idea

1070
01:06:46,000 --> 01:06:48,140
that we we can process again

1071
01:06:48,190 --> 01:06:55,530
the segmentation step is simple and very fast that we

1072
01:06:55,580 --> 01:07:02,360
we want to that this step doesn't influence the rest of the grid and it's

1073
01:07:03,040 --> 01:07:08,480
it's possible because we have a stereo if you still image so we we have

1074
01:07:08,500 --> 01:07:13,140
this same scene looking from two different we point to point

1075
01:07:13,150 --> 01:07:19,100
then there's segmentation step is adopted that adaptative

1076
01:07:19,640 --> 01:07:22,110
for each major accordingly

1077
01:07:23,620 --> 01:07:27,360
lighting conditions of this period

1078
01:07:27,980 --> 01:07:29,620
graph representation

1079
01:07:29,630 --> 01:07:31,910
it is

1080
01:07:31,960 --> 01:07:42,270
is based on the we select the segmented regions from the previous step and we

1081
01:07:42,270 --> 01:07:44,020
identify the

1082
01:07:44,060 --> 01:07:45,790
i mean value of

1083
01:07:45,800 --> 01:07:51,820
for each original decides called the need to run a mask and we define this

1084
01:07:51,820 --> 01:07:54,100
way the two graph

1085
01:07:54,110 --> 01:07:57,280
for less than five each

1086
01:07:57,290 --> 01:08:06,870
we define some cost functions that depending on the call of dimension position and the

1087
01:08:07,460 --> 01:08:08,700
we apply

1088
01:08:08,720 --> 01:08:10,420
we so we

1089
01:08:10,450 --> 01:08:16,200
we will we want to we look for the best solution that is the solution

1090
01:08:16,200 --> 01:08:20,350
with maximum cardinality and with mean

1091
01:08:20,380 --> 01:08:24,800
lowest cost lowest cost

1092
01:08:24,810 --> 01:08:28,680
of course the ability of nature can be

1093
01:08:29,210 --> 01:08:35,800
but generally can be time-consuming and for these reasons we

1094
01:08:35,810 --> 01:08:41,870
apply some constraints for i want to be part of the graph matching

1095
01:08:42,000 --> 01:08:46,710
our constraints is to define a searching i because we know that

1096
01:08:47,160 --> 01:08:54,790
and adjourned on the left image and can have a maximum vertical displacement and and

1097
01:08:54,790 --> 01:09:03,650
the maximum horizontal displacement in this way we can use spots information and the color

1098
01:09:03,650 --> 01:09:12,090
information and so we can improve our matching in in order to have a low

1099
01:09:12,780 --> 01:09:16,410
execution time

1100
01:09:16,420 --> 01:09:19,810
it's the end of the graph matching

1101
01:09:19,860 --> 01:09:26,130
as i said before we have here is a list of mitch died and the

1102
01:09:26,130 --> 01:09:28,130
list of donkey idea

1103
01:09:28,180 --> 01:09:38,320
those ideas don't care don't care about this can be groper is a in addition

1104
01:09:38,480 --> 01:09:45,110
blobs so we can try to put together some blobs in the left image and

1105
01:09:45,520 --> 01:09:47,640
the right to major and two rules

1106
01:09:47,660 --> 01:09:49,190
five to two

1107
01:09:49,200 --> 01:09:56,850
to look for another graph matching for these combination of addition to blocks in this

1108
01:09:56,850 --> 01:10:04,490
way we want to use split and merge artifacts of the segmentation step

1109
01:10:04,510 --> 01:10:13,150
despite the competition is maximal call coverage of the corresponding correspondent regions

1110
01:10:13,190 --> 01:10:17,250
and so the disparity value is the horizontal displacement of

1111
01:10:17,820 --> 01:10:22,650
of those regions

1112
01:10:22,670 --> 01:10:26,980
the output of the algorithm is that it's better than that of course

1113
01:10:27,530 --> 01:10:33,350
but also a graphic also the performance this is a graphical representation of these performances

1114
01:10:33,350 --> 01:10:36,720
so we have brighter value for

1115
01:10:36,730 --> 01:10:45,540
one of four original it is related to higher performance venue at the end we

1116
01:10:45,540 --> 01:10:47,660
can apply some positive

1117
01:10:47,670 --> 01:10:52,030
two to improve our is

1118
01:10:52,040 --> 01:10:55,890
so what is the advantages of the approaches

1119
01:10:55,910 --> 01:11:00,850
all our approach to give these results

1120
01:11:00,860 --> 01:11:05,920
we don't have a problem in a uniform manner

1121
01:11:06,040 --> 01:11:13,790
we stable more of the in the case of vibration of a camera in this

1122
01:11:13,790 --> 01:11:20,570
case we move one two or three peaks we impose two forty pixels of vertical

1123
01:11:20,570 --> 01:11:25,410
misalignment misalignment between the left and right images and our results on the right

1124
01:11:25,420 --> 01:11:33,670
is enough stable in comparison with the other that can that have some problems

1125
01:11:33,680 --> 01:11:34,900
has some problems

1126
01:11:34,910 --> 01:11:39,940
the execution time is going to a ball with the other methods in this case

1127
01:11:39,940 --> 01:11:48,270
we have compiled composing elements only with dynamic programming and city because other approach that

1128
01:11:48,280 --> 01:11:55,570
is not good for her us because it's about five hundred seconds or

1129
01:11:55,590 --> 01:12:01,060
one hundred sixty but is not good for our framework at the end of the

1130
01:12:01,920 --> 01:12:07,500
day cost function because the graph matching is good for local and global perturbation of

1131
01:12:07,510 --> 01:12:09,200
the tunes

1132
01:12:09,250 --> 01:12:11,030
you other

1133
01:12:11,050 --> 01:12:18,270
kind of results that we want to people who want to test is a quantitative

1134
01:12:18,360 --> 01:12:27,790
results we have tested our algorithm and we have combined the with and this is

1135
01:12:27,790 --> 01:12:33,620
what to scale that are the most used in the literature it and we

1136
01:12:33,630 --> 01:12:36,560
we tested on their

1137
01:12:36,600 --> 01:12:39,450
realistic comedian that we have acquired from

1138
01:12:40,160 --> 01:12:45,340
mobile platform that is the movie that a show within to show it to you

1139
01:12:46,980 --> 01:12:57,130
in these newly movie we have a camera vibration like changing uniform obstacles obstacles then

1140
01:12:57,130 --> 01:13:03,590
we have defined the sound performance index because in the literature we have only a

1141
01:13:03,590 --> 01:13:08,010
also discriminated one i can reduce at least two ottoman

1142
01:13:08,030 --> 01:13:09,360
you can think of this

1143
01:13:10,670 --> 01:13:15,920
then if we think about this the interesting thing is how can we

1144
01:13:16,650 --> 01:13:20,170
graph mining to find property index structure

1145
01:13:22,070 --> 01:13:25,710
the first interesting thing i would say is

1146
01:13:26,530 --> 01:13:32,960
besides frequent subgraph mining we needed the discriminant my what's discriminant mining

1147
01:13:33,010 --> 01:13:38,030
i should apologize here this

1148
01:13:38,070 --> 01:13:39,730
this is three that's OK

1149
01:13:39,740 --> 01:13:44,630
i think different machine recognised this should we should blame know

1150
01:13:44,650 --> 01:13:45,510
bill gates

1151
01:13:45,530 --> 01:13:46,920
OK what

1152
01:13:46,940 --> 01:13:50,980
the original this year and this is just so

1153
01:13:51,000 --> 01:13:53,650
the general philosophy like this

1154
01:13:53,670 --> 01:13:58,320
suppose you are professor you want to select students suppose we already have five students

1155
01:13:58,340 --> 01:14:03,480
in europe you get a new applicant cup pass they want join group

1156
01:14:03,530 --> 01:14:04,860
what you're think

1157
01:14:04,940 --> 01:14:07,360
you would think is it's OK

1158
01:14:07,400 --> 01:14:11,730
suppose this f one f two f and is the current students current feature means

1159
01:14:11,730 --> 01:14:14,030
current students OK

1160
01:14:14,030 --> 01:14:16,260
if you get a new one comes

1161
01:14:16,280 --> 01:14:17,610
if this one

1162
01:14:17,630 --> 01:14:21,340
you ask him what do what do know what is your skin

1163
01:14:21,360 --> 01:14:23,760
OK if you find this guy scale

1164
01:14:23,780 --> 01:14:27,170
OK will be fully covered by your existing students

1165
01:14:27,190 --> 01:14:31,590
using this guy joined the group for not contributing new skills

1166
01:14:31,710 --> 01:14:35,420
he was oh i got enough people OK i don't i don't need you anymore

1167
01:14:35,420 --> 01:14:37,570
so you you're not be selected

1168
01:14:37,610 --> 01:14:40,880
but if the student got some unix scale OK

1169
01:14:40,900 --> 01:14:44,300
can be cover was the existing students you all

1170
01:14:44,320 --> 01:14:47,360
this is a very good addition to my group had better include u

1171
01:14:47,380 --> 01:14:51,960
that's the same idea same philosophy that means you will find discriminant structure means you

1172
01:14:51,960 --> 01:14:58,610
won't find your new features so if i got the problem i suppose

1173
01:14:58,610 --> 01:15:00,380
i got this act

1174
01:15:00,400 --> 01:15:04,230
i want find this access the common features if

1175
01:15:04,280 --> 01:15:06,170
my existing feature

1176
01:15:06,190 --> 01:15:12,030
OK can predict the acts simply says this axis is sensory or not but they

1177
01:15:12,230 --> 01:15:14,820
not you can get in so then

1178
01:15:14,840 --> 01:15:16,340
i would thinking

1179
01:15:16,360 --> 01:15:21,300
you know if this one is small enough simply says i can barely predicting your

1180
01:15:21,300 --> 01:15:27,840
probability predicting your skills then x is discriminative our include you so basically based on

1181
01:15:27,840 --> 01:15:28,860
this idea

1182
01:15:28,880 --> 01:15:32,190
we work out the frequent discriminant

1183
01:15:32,210 --> 01:15:36,900
structure my pattern mining and using this one as are index entries

1184
01:15:36,920 --> 01:15:40,960
so the interesting thing another interesting thing is

1185
01:15:41,010 --> 01:15:44,760
if you think the frequent graph mining you say oh i set up a stable

1186
01:15:44,760 --> 01:15:50,500
structurally actually this way it may not work well we really use is we use

1187
01:15:50,500 --> 01:15:53,420
a growing support structure

1188
01:15:53,460 --> 01:15:57,510
you may say oh you could be wrong you grew one use shrinking lines that

1189
01:15:57,610 --> 01:16:02,550
growing more OK so the first idea is to try to argue this this curve

1190
01:16:02,550 --> 01:16:05,860
is right this is when you're size is really small

1191
01:16:05,880 --> 01:16:08,820
like the size only one single or not

1192
01:16:08,840 --> 01:16:11,170
you political everyone

1193
01:16:11,280 --> 01:16:16,820
including some rare elements is very in your data set but as long as they

1194
01:16:16,860 --> 01:16:19,320
p appear used don't want to miss them

1195
01:16:19,360 --> 01:16:24,300
that time every single one suppose you're everything one would become or be indexed

1196
01:16:24,320 --> 01:16:26,230
OK then when the

1197
01:16:26,260 --> 01:16:28,480
when you start to become bigger and bigger

1198
01:16:31,340 --> 01:16:33,010
the real frequency

1199
01:16:33,010 --> 01:16:36,630
the rear explination appliances the pakistan come

1200
01:16:36,650 --> 01:16:41,880
two in combinatorial OK was stuck on combining to win three and four you really

1201
01:16:41,880 --> 01:16:47,010
get the star explodes and that's the part we become pick and pick OK we

1202
01:16:47,010 --> 01:16:51,280
try to raise the suppose that said if you're bar

1203
01:16:51,340 --> 01:16:54,590
i raised the bar if you're not frequent enough

1204
01:16:54,610 --> 01:16:56,900
OK i didn't even want to check

1205
01:16:56,920 --> 01:17:02,230
whether you're discriminant just thinking about this suppose i have a rare element this rare

1206
01:17:02,230 --> 01:17:03,760
element here already

1207
01:17:04,940 --> 01:17:08,190
they it but index only one node all or one edge

1208
01:17:08,210 --> 01:17:13,610
OK now you start combining with many many other things OK of course combine make

1209
01:17:14,380 --> 01:17:16,000
but since you are so rare

1210
01:17:16,010 --> 01:17:20,010
i don't need to index the very low product already ten radical

1211
01:17:20,440 --> 01:17:25,690
in the bigger and better reserved this index entry for those more popular because that's

1212
01:17:25,690 --> 01:17:28,760
the general philosophy so that's why we raise the bar here

1213
01:17:29,460 --> 01:17:34,460
of course this one if you really see the whole explosion curve is going this

1214
01:17:34,460 --> 01:17:38,940
way and then going down but we don't have to index that big structure that's

1215
01:17:38,940 --> 01:17:41,320
why we go up to certain state was done

1216
01:17:41,340 --> 01:17:48,110
so based on this we actually look at the same this aids antiviral structure

1217
01:17:48,150 --> 01:17:52,780
so we found the first interesting thing is if you use

1218
01:17:52,820 --> 01:17:59,460
past like a daylight or you know graph the graph grab them by dennis shasha

1219
01:17:59,460 --> 01:18:01,130
using this one

1220
01:18:01,170 --> 01:18:05,800
comparing to using this one you actually can see once the database size keep going

1221
01:18:05,840 --> 01:18:09,400
this structure this frequent structure

1222
01:18:09,420 --> 01:18:11,050
it's really stable

1223
01:18:11,070 --> 01:18:16,840
actually the frequent discriminative structure because you can further only the slow

1224
01:18:16,940 --> 01:18:21,130
so what you really need you can you can look at this you say this

1225
01:18:21,130 --> 01:18:24,150
is very efficient because you don't need them in index

1226
01:18:24,150 --> 01:18:26,960
then we look at how effective it could be

1227
01:18:27,710 --> 01:18:30,480
we use the very small index structures

1228
01:18:30,500 --> 01:18:35,500
but it would be as effective as you know exactly what you have

1229
01:18:35,710 --> 01:18:39,840
so this one you can see this is our g index

1230
01:18:39,860 --> 01:18:43,400
this is ideal case the actual match and inside

1231
01:18:43,400 --> 01:18:45,530
i really had this one i really

1232
01:18:45,550 --> 01:18:49,000
you know PN point to this one is or is impossible because you were not

1233
01:18:49,000 --> 01:18:52,610
really keen on them so it's is very close to the ideal

1234
01:18:52,630 --> 01:18:54,730
but if you look at the graph graph

1235
01:18:54,780 --> 01:18:58,840
even to have a much bigger structure with a return much

1236
01:18:58,860 --> 01:19:00,050
much more

1237
01:19:00,070 --> 01:19:04,420
candidates and you have to do much more you know match

1238
01:19:04,440 --> 01:19:07,190
that's the reason this is more effective

1239
01:19:07,280 --> 01:19:10,150
and also it is quite

1240
01:19:10,190 --> 01:19:15,570
good for incremental updates means you keep adding new elements to average to

1241
01:19:15,590 --> 01:19:16,880
could operate

1242
01:19:16,900 --> 01:19:22,510
in the index structure based examples when he was sent to use very very stable

1243
01:19:22,590 --> 01:19:25,550
those the the thing for the index

1244
01:19:25,570 --> 01:19:27,980
and then we go back to do the

1245
01:19:28,000 --> 01:19:29,110
to the cameras

1246
01:19:29,130 --> 01:19:32,630
COI we construct index

1247
01:19:32,710 --> 01:19:37,260
and then we need to get one more problem they say you go index is

1248
01:19:37,300 --> 01:19:43,070
i don't find exactly what day of an approximate one is thinking about this OK

1249
01:19:43,070 --> 01:19:44,300
for example

1250
01:19:44,320 --> 01:19:45,280
this structure

1251
01:19:45,300 --> 01:19:49,920
if you go to these chemical compounds found exactly the structure probably were not find

1252
01:19:49,920 --> 01:19:55,070
it but it's a i just twist letter i had this one from the carbon

1253
01:19:55,070 --> 01:20:01,140
them cross one to con to computing the conditional distribution of some unobserved random variable

1254
01:20:01,140 --> 01:20:03,880
given the observed data

1255
01:20:03,910 --> 01:20:09,070
okay and I think this is quite important especially when you start looking to more

1256
01:20:09,070 --> 01:20:15,120
complex models and and for these models the there's often not much distinction between what

1257
01:20:15,140 --> 01:20:20,600
is a latent varibale and what is a parameter they're all simply random variables that you

1258
01:20:20,600 --> 01:20:21,910
did not observe

1259
01:20:25,140 --> 01:20:30,480
so when are some examples of probabilistic models

1260
01:20:30,550 --> 01:20:35,340
I guess the let there's a very large class of probabilistic models called graphical

1261
01:20:35,350 --> 01:20:40,360
models which are very useful in terms of reasoning about

1262
01:20:40,400 --> 01:20:42,240
uncertainty in

1263
01:20:42,260 --> 01:20:43,360
this in

1264
01:20:44,000 --> 01:20:47,870
in our data

1265
01:20:48,190 --> 01:20:51,530
and graphical models can be visualized in these

1266
01:20:53,240 --> 01:20:58,240
and basically each node of the graph corresponds to a random variable

1267
01:20:58,650 --> 01:21:04,500
and each edge basically corresponds to a direct dependence of one variable on

1268
01:21:04,520 --> 01:21:06,120
another variable okay

1269
01:21:06,740 --> 01:21:08,590
and importantly

1270
01:21:08,720 --> 01:21:09,740
if you don't

1271
01:21:11,060 --> 01:21:16,270
if there's no edge connecting to random variables that corresponds to actually conditional

1272
01:21:16,270 --> 01:21:20,220
independence between the random variables

1273
01:21:20,300 --> 01:21:25,520
so in this graph for example it's because there's no edge between earthquake and burglar

1274
01:21:25,910 --> 01:21:30,570
that means that whether an earthquake happens or not or whether burglar

1275
01:21:30,620 --> 01:21:32,310
comes into your house or not are

1276
01:21:32,360 --> 01:21:33,840
independent events okay

1277
01:21:37,720 --> 01:21:41,190
so this is the Asian network and this is the alarm network and I guess

1278
01:21:41,190 --> 01:21:42,650
they're pretty

1279
01:21:45,310 --> 01:21:49,070
simple graphical models to to illustrate

1280
01:21:49,120 --> 01:21:51,290
what these things are

1281
01:21:52,810 --> 01:21:58,880
so I'll show you a few more examples so this example for model based clustering

1282
01:21:58,880 --> 01:22:02,570
is pro will actually returned to this

1283
01:22:02,980 --> 01:22:09,760
a few times over both today and tomorrow and this is a model for clustering

1284
01:22:09,790 --> 01:22:15,310
so what is clustering clustering is the idea that given a data set which consists

1285
01:22:15,310 --> 01:22:18,260
of heterogenous of

1286
01:22:18,290 --> 01:22:24,270
data items basically you can think of the data items as coming from different sources then

1287
01:22:24,290 --> 01:22:29,150
we'd like to cluster the data items into the different clusters each of which are

1288
01:22:29,150 --> 01:22:31,480
homogenous within themselves and

1289
01:22:31,520 --> 01:22:37,670
and the different clusters are different okay so to visualize so he if the

1290
01:22:37,720 --> 01:22:43,310
blue dots here are our data items then we'll let to cluster them into three

1291
01:22:43,340 --> 01:22:46,380
different clusters which corresponds to well

1292
01:22:48,010 --> 01:22:53,100
three different types of data items that we observe right

1293
01:22:53,150 --> 01:22:54,910
and this is a model

1294
01:22:54,950 --> 01:23:00,770
and we can take a model based approach to clustering and it's model based in the

1295
01:23:00,770 --> 01:23:04,100
sense that we are gonna postulate a particle at model

1296
01:23:04,550 --> 01:23:07,220
for the data items in each cluster okay

1297
01:23:07,840 --> 01:23:11,020
so the model kind of goes as follows

1298
01:23:11,100 --> 01:23:12,260
we're gonna assume

1299
01:23:12,550 --> 01:23:16,100
so x i is gonna be data item i

1300
01:23:16,140 --> 01:23:21,860
we're gonna assume that there're some latent variabes z i which corresponds to

1301
01:23:22,290 --> 01:23:28,970
the unobserved indicator variable which tells us which cluster this data item belongs to

1302
01:23:29,500 --> 01:23:34,620
and we're gonna assume the following generative model so for every data item we're first

1303
01:23:34,620 --> 01:23:39,220
gonna decide which cluster that data item belongs to

1304
01:23:39,260 --> 01:23:45,420
by sampling from a dicrete distribution with a prior given by pi so pi here is basically a

1305
01:23:46,480 --> 01:23:49,650
pi one to pi k with pi k

1306
01:23:49,690 --> 01:23:51,690
being the prior probability

1307
01:23:51,740 --> 01:23:53,740
that cluster k is

1308
01:23:53,770 --> 01:23:59,770
respon is no is the prior probability that data item i belongs to

1309
01:23:59,770 --> 01:24:02,030
cluster k okay

1310
01:24:02,190 --> 01:24:06,310
and given that we've postulated that

1311
01:24:06,360 --> 01:24:09,070
data item i belongs to cluster

1312
01:24:09,120 --> 01:24:10,070
k say

1313
01:24:13,340 --> 01:24:16,770
the this sorry is that is that a question

1314
01:24:18,970 --> 01:24:21,430
oh it's just a discrete distribution

1315
01:24:22,260 --> 01:24:27,840
yeah well it's a discrete distribution with probabilities given by pi so

1316
01:24:27,880 --> 01:24:30,140
the probability that z i

1317
01:24:30,410 --> 01:24:34,550
takes on value one is gonna be pi one

1318
01:24:34,570 --> 01:24:38,650
and the probability that z i takes on value two is gonna be pi two

1319
01:24:38,900 --> 01:24:43,650
and the probability that z i takes on value k is gonna be pi k

1320
01:24:43,840 --> 01:24:47,060
I would

1321
01:24:47,100 --> 01:24:51,780
yeah so people typically call that multinomial in machine learning but it's actually a bit

1322
01:24:51,780 --> 01:24:59,900
confusing because a multinomial distribution actually is actually a richer set of distributions than this

1323
01:24:59,960 --> 01:25:01,430
discrete distributions

1324
01:25:01,430 --> 01:25:04,560
because from this analysis analysis

1325
01:25:04,560 --> 01:25:08,670
in particular from knowing what the entering angle with this

1326
01:25:08,680 --> 01:25:12,920
this is the quantity that the girls wanted to measure

1327
01:25:12,970 --> 01:25:16,970
writer were really interested in the scattering angle

1328
01:25:16,990 --> 01:25:20,180
which turns out to be fifty point seven degrees

1329
01:25:20,300 --> 01:25:24,210
this first order diffraction

1330
01:25:24,300 --> 01:25:26,820
we're going to use the

1331
01:25:26,820 --> 01:25:29,650
knowledge to calculate the the wavelength

1332
01:25:29,660 --> 01:25:31,240
of the electron

1333
01:25:32,560 --> 01:25:35,580
four here

1334
01:25:35,580 --> 01:25:37,270
and we're going to do they have

1335
01:25:37,300 --> 01:25:39,570
because of this known condition

1336
01:25:40,250 --> 01:25:46,210
constructive interference that the difference in the distance travelled by the two ways that gives

1337
01:25:46,210 --> 01:25:51,320
the constructive interference to be an integral multiple of the wavelength

1338
01:25:51,320 --> 01:25:54,330
that's where we're at

1339
01:25:54,340 --> 01:25:56,140
so now

1340
01:25:56,260 --> 01:25:57,310
the fly

1341
01:25:57,320 --> 01:25:59,940
from random number two to the screen

1342
01:26:00,000 --> 01:26:02,100
call it the two

1343
01:26:02,110 --> 01:26:03,460
the tool

1344
01:26:03,510 --> 01:26:06,960
is the length of that line it's the destination

1345
01:26:06,970 --> 01:26:08,510
that the wave

1346
01:26:08,560 --> 01:26:12,940
trammell from adam to to the screen

1347
01:26:12,960 --> 01:26:13,920
d one

1348
01:26:13,940 --> 01:26:17,930
is the distance that the waves scattering from one

1349
01:26:17,950 --> 01:26:19,370
has travelled

1350
01:26:19,370 --> 01:26:20,910
the screen

1351
01:26:24,170 --> 01:26:27,590
i'm going to draw a perpendicular right here

1352
01:26:27,610 --> 01:26:30,860
trying to draw perpendicular from one

1353
01:26:30,880 --> 01:26:32,590
to this line two

1354
01:26:32,610 --> 01:26:35,300
here's the right angle

1355
01:26:35,320 --> 01:26:37,100
that's really nice

1356
01:26:37,110 --> 01:26:41,360
because look at what this leg of the triangle is then

1357
01:26:41,380 --> 01:26:45,090
it d two minus the one he

1358
01:26:45,100 --> 01:26:48,890
at the quantity that is going to be interesting to watch

1359
01:26:49,220 --> 01:26:52,880
right now

1360
01:26:52,900 --> 01:26:54,880
what you have to do

1361
01:26:54,960 --> 01:27:00,380
you have to convince yourself that this angle right here in the triangle

1362
01:27:00,420 --> 01:27:03,370
it is equal to the ringing of data

1363
01:27:04,200 --> 01:27:07,440
if you're going to geometry considered immediately

1364
01:27:07,460 --> 01:27:10,560
or you might have to think about a little bit but it is equal to

1365
01:27:13,880 --> 01:27:17,230
well that's good because now i got triangle right here

1366
01:27:17,250 --> 01:27:19,920
and i know the length of one side

1367
01:27:20,110 --> 01:27:25,070
the distance the spacing between the two nickel atoms in the crystal

1368
01:27:26,450 --> 01:27:30,930
i know what they is that with the listening grammar measure

1369
01:27:30,940 --> 01:27:33,930
pretty good i can use that

1370
01:27:33,980 --> 01:27:36,390
i can use this geometry

1371
01:27:36,390 --> 01:27:38,720
say a signed data

1372
01:27:38,750 --> 01:27:41,150
is equal to the opposite

1373
01:27:41,160 --> 01:27:47,100
the link the outside the two minus the one over the hypothenuse a

1374
01:27:49,100 --> 01:27:50,030
and now

1375
01:27:50,050 --> 01:27:55,320
if this is the condition in to obtain the constructive interference

1376
01:27:55,340 --> 01:27:58,050
and there expression

1377
01:27:58,100 --> 01:28:03,380
is comes from the geometry of the problem that i'm solving

1378
01:28:03,400 --> 01:28:06,590
this is the theory that the geometry

1379
01:28:06,640 --> 01:28:08,530
the problem solving

1380
01:28:09,650 --> 01:28:16,900
then and lambda has to be equal to the sine paid

1381
01:28:16,920 --> 01:28:20,990
and and therefore i can rearrange their to get lambda

1382
01:28:21,010 --> 01:28:22,700
and is equal to

1383
01:28:22,740 --> 01:28:24,640
eighty nine data

1384
01:28:24,640 --> 01:28:27,750
divided by

1385
01:28:27,770 --> 01:28:33,080
and now i can use this information here to calculate land

1386
01:28:33,080 --> 01:28:38,310
right i know all the scattering angle fifty point seven degrees

1387
01:28:38,320 --> 01:28:43,940
i know what and and it's the first order diffraction feature because it is the

1388
01:28:43,970 --> 01:28:49,110
most intense but it is closest to the zero order zero order is always going

1389
01:28:49,110 --> 01:28:53,940
to be normal to your sample normal to the crystal

1390
01:28:53,980 --> 01:28:57,310
so therefore i can plug some numbers in there

1391
01:28:57,350 --> 01:29:01,640
and i get for the wavelength one point six six times ten to the minus

1392
01:29:01,640 --> 01:29:03,660
ten years

1393
01:29:03,670 --> 01:29:08,880
right so this experiment that if you have electrons there are fifty four you need

1394
01:29:08,880 --> 01:29:10,310
an energy

1395
01:29:10,320 --> 01:29:11,700
the wavelength

1396
01:29:11,730 --> 01:29:16,620
of those electrons one point six six times in ninety meters

1397
01:29:17,750 --> 01:29:20,750
davis grammar and

1398
01:29:21,060 --> 01:29:22,590
so the

1399
01:29:22,590 --> 01:29:27,880
not only did they demonstrate the interference phenomenon of particles

1400
01:29:28,830 --> 01:29:31,420
particles that can man

1401
01:29:31,450 --> 01:29:39,410
but they were able to calculate from an experiment the wavelength the corresponding wavelength

1402
01:29:39,420 --> 01:29:43,720
right but now i just want to take a moment to site

1403
01:29:43,730 --> 01:29:51,660
to tell you that geometry that i illustrated right here is the identical geometry

1404
01:29:51,670 --> 01:29:56,080
we have is used in x-ray diffraction

1405
01:29:56,080 --> 01:30:01,990
x-ray diffraction is the interference the diffraction of x-rays photo

1406
01:30:02,780 --> 01:30:05,420
this is the electron diffraction

1407
01:30:05,500 --> 01:30:09,350
the geometry is the same the concepts are the same

1408
01:30:09,390 --> 01:30:12,660
the reason i bring up x-ray diffraction

1409
01:30:12,670 --> 01:30:15,910
is because it's very important technique

1410
01:30:15,940 --> 01:30:20,700
for you in your future whether you're going to be looking at material

1411
01:30:20,750 --> 01:30:23,270
get structure material

1412
01:30:23,320 --> 01:30:29,710
or whether you're going to be doing some kind of biological chemistry because x-ray diffraction

1413
01:30:29,730 --> 01:30:31,080
it is

1414
01:30:31,090 --> 01:30:37,860
used to get structures crystal structures particularly of proteins

1415
01:30:37,870 --> 01:30:42,100
and the reason why you want to know the structure of the protein

1416
01:30:42,150 --> 01:30:48,300
because their structure does give you some hints as to what the function of the

1417
01:30:48,300 --> 01:30:51,700
protein yet

1418
01:30:51,760 --> 01:30:55,850
and actually professor drennan who was going to watch in the second half of the

1419
01:30:55,850 --> 01:30:58,050
course that's specialty

1420
01:30:58,080 --> 01:31:00,260
is x-ray diffraction of

1421
01:31:00,280 --> 01:31:03,270
protein crystal protein

1422
01:31:03,470 --> 01:31:08,230
and i'm sure she'll tell you much more about it

1423
01:31:08,320 --> 01:31:11,880
right but in x-ray diffraction

1424
01:31:11,890 --> 01:31:12,730
what they

1425
01:31:13,840 --> 01:31:19,420
is they use those results not to get the wavelength

1426
01:31:19,440 --> 01:31:23,530
but they use those results to get this distance here eight

1427
01:31:23,580 --> 01:31:26,950
the distance between the atoms that's what i mean by

1428
01:31:26,970 --> 01:31:29,420
getting out the structure

1429
01:31:29,440 --> 01:31:31,160
in x-ray diffraction

1430
01:31:31,170 --> 01:31:34,190
are coming in with the x-ray beam

1431
01:31:34,220 --> 01:31:38,830
and they know what the wavelength is of their x-ray beam

1432
01:31:38,860 --> 01:31:41,630
so they know what lambda is

1433
01:31:41,650 --> 01:31:46,170
and then they go and measure what data is the scattering angle

1434
01:31:47,170 --> 01:31:49,940
and from those two quantities

1435
01:31:49,960 --> 01:31:53,160
lambda and data they figure out what it is

1436
01:31:53,160 --> 01:31:55,250
the distance between the atoms

1437
01:31:55,250 --> 01:32:00,110
and girls in probability measure and new one here is the measurement so the measure

1438
01:32:00,130 --> 01:32:02,540
which assigns volume on the real line

1439
01:32:02,540 --> 01:32:05,480
then this would be the gas intensity

1440
01:32:09,540 --> 01:32:12,070
that brings us back to

1441
01:32:12,130 --> 01:32:14,360
to what i said in the beginning there

1442
01:32:14,360 --> 01:32:18,520
one reason why we have to use measures in infinite dimensions is because many infinite

1443
01:32:18,520 --> 01:32:21,630
dimension distributions don't have a useful density

1444
01:32:22,040 --> 01:32:24,110
the problem here is

1445
01:32:24,130 --> 01:32:26,210
that the density

1446
01:32:26,230 --> 01:32:30,750
the density is always referred defined with respect to some measure

1447
01:32:30,750 --> 01:32:35,570
density is not standalone object you have to defined with respect to some reference measure

1448
01:32:35,670 --> 01:32:39,360
in this case here for the girls in the reference measure the standard girth density

1449
01:32:39,770 --> 01:32:42,420
the reference measure is this this flat back matter

1450
01:32:42,420 --> 01:32:44,190
and this kind of uniform

1451
01:32:44,190 --> 01:32:45,980
measure on the real line

1452
01:32:47,540 --> 01:32:50,610
in order for this to be meaningful

1453
01:32:50,880 --> 01:32:54,060
two two burke was this density in the way we usually

1454
01:32:54,090 --> 01:32:57,000
i like to work with this measure you has to be it has to be

1455
01:32:57,900 --> 01:32:59,900
it has to be flat

1456
01:32:59,920 --> 01:33:01,290
if you think of

1457
01:33:01,310 --> 01:33:03,650
if you you could very well right

1458
01:33:03,650 --> 01:33:10,290
one gaussians represented as density with respect to another gaussian say another goes measure with

1459
01:33:10,360 --> 01:33:12,710
different location parameter

1460
01:33:12,730 --> 01:33:17,670
but then interpreting interpreting this density becomes really hard

1461
01:33:17,670 --> 01:33:18,960
because it means that

1462
01:33:19,000 --> 01:33:22,230
when at the point where this goes in here has its mode

1463
01:33:22,250 --> 01:33:24,250
this density has to decrease

1464
01:33:24,270 --> 01:33:26,090
to make up for the

1465
01:33:26,520 --> 01:33:30,230
so you always have to when you want to interpret what large or small density

1466
01:33:30,230 --> 01:33:33,730
means would always have to look back at the other measures check is large in

1467
01:33:33,730 --> 01:33:35,000
that region or not

1468
01:33:35,040 --> 01:33:38,840
so in order for the for the density to be to be interpretable you wanted

1469
01:33:38,840 --> 01:33:41,110
to be defined with respect to some

1470
01:33:41,110 --> 01:33:42,670
some neutral

1471
01:33:43,380 --> 01:33:49,570
and this neutrality is is formalized as translation invariant so the back to measure of

1472
01:33:49,570 --> 01:33:52,670
the vision of the which assigns volume you can show it around on the on

1473
01:33:52,670 --> 01:33:53,710
the real line

1474
01:33:53,730 --> 01:33:57,400
and it never changes with respect to location if you have one you must the

1475
01:33:57,400 --> 01:34:01,270
same size at different points on the real line euclidean space it always has the

1476
01:34:01,270 --> 01:34:04,920
same volume assigned right there was a question

1477
01:34:05,020 --> 01:34:09,420
the case using

1478
01:34:11,710 --> 01:34:17,860
well almost so the question is is to measure the cumulative distribution

1479
01:34:19,960 --> 01:34:22,900
not quite so the cumulative distribution is

1480
01:34:22,920 --> 01:34:25,380
you start at minus infinity

1481
01:34:25,420 --> 01:34:28,750
and integrate all the way to your current point

1482
01:34:29,130 --> 01:34:32,520
whereas the

1483
01:34:32,520 --> 01:34:35,020
the measure can be

1484
01:34:35,020 --> 01:34:39,190
can be an arbitrary set so the cumulative distribution in that case would be is

1485
01:34:39,210 --> 01:34:44,090
a measure of the interval the open interval minus infinity up to your point

1486
01:34:46,040 --> 01:34:47,790
one of

1487
01:34:56,380 --> 01:34:57,520
yes so

1488
01:34:59,110 --> 01:35:01,520
the problem with the translation invariance is the following

1489
01:35:01,980 --> 01:35:06,060
i haven't finished it yet

1490
01:35:06,070 --> 01:35:07,400
OK so

1491
01:35:07,400 --> 01:35:12,540
in in in basically in euclidean space to the translation invariance is the property which

1492
01:35:12,540 --> 01:35:16,900
makes which makes the density the translation invariance of this measure measures property which makes

1493
01:35:16,900 --> 01:35:18,690
the density useful

1494
01:35:18,710 --> 01:35:22,830
now when you go to infinite dimensional space a hilbert space

1495
01:35:22,830 --> 01:35:29,020
then we don't there's there's no translation invariant measure on hilbert space

1496
01:35:29,060 --> 01:35:32,400
infinite dimensional spaces don't have translation invariant measure

1497
01:35:32,440 --> 01:35:37,110
and that's even even hilbert space even though he was space kind of the most

1498
01:35:37,110 --> 01:35:41,310
the nicest and most problematic infinite dimensional space

1499
01:35:46,250 --> 01:35:51,420
so there's there's no analogue of the olympic measure in infinite dimensional spaces

1500
01:35:51,460 --> 01:35:53,060
which is

1501
01:35:53,130 --> 01:36:00,090
basically the whole notion of the volume in infinite dimensions becomes a bit flaky

1502
01:36:02,130 --> 01:36:03,060
OK so

1503
01:36:03,060 --> 01:36:05,460
and that means that that in order to

1504
01:36:05,460 --> 01:36:09,960
in order to to work on infinite dimensional spaces densities are often not an option

1505
01:36:09,960 --> 01:36:14,540
so you can have something like a ghost in probability on infinite dimensional space and

1506
01:36:14,540 --> 01:36:16,190
that's because the process

1507
01:36:16,540 --> 01:36:21,190
and it does have density so you can write the density of one gaussian process

1508
01:36:21,190 --> 01:36:22,400
gives you

1509
01:36:23,200 --> 01:36:24,420
the interval

1510
01:36:24,420 --> 01:36:26,290
the right where

1511
01:36:26,400 --> 01:36:28,240
right after this

1512
01:36:28,280 --> 01:36:30,030
and we

1513
01:36:30,050 --> 01:36:31,780
also law

1514
01:36:31,780 --> 01:36:37,260
in most of the existing process of the law some kind of a random mutation

1515
01:36:37,430 --> 01:36:40,710
there is a small probability of

1516
01:36:45,300 --> 01:36:49,700
somebody's somebody that i

1517
01:36:49,700 --> 01:36:51,500
this summer

1518
01:36:51,520 --> 01:36:54,630
the size of his

1519
01:36:54,690 --> 01:36:57,220
strong the bands

1520
01:36:57,290 --> 01:36:59,960
on the ever dealt

1521
01:37:00,290 --> 01:37:03,380
if you want to

1522
01:37:03,660 --> 01:37:05,970
more precise

1523
01:37:06,930 --> 01:37:07,460
we need

1524
01:37:07,480 --> 01:37:10,580
the space of some other people

1525
01:37:10,610 --> 01:37:11,910
if you do too

1526
01:37:15,290 --> 01:37:17,600
i i get probabilities

1527
01:37:17,630 --> 01:37:19,290
displays of

1528
01:37:19,370 --> 01:37:20,930
i think it's

1529
01:37:22,470 --> 01:37:25,110
so sound

1530
01:37:25,130 --> 01:37:28,090
basic streaming format

1531
01:37:28,140 --> 01:37:30,430
the first one

1532
01:37:30,440 --> 01:37:33,060
his son

1533
01:37:35,260 --> 01:37:37,200
is useful

1534
01:37:37,220 --> 01:37:39,000
too slow by

1535
01:37:41,610 --> 01:37:43,860
the problem is

1536
01:37:43,910 --> 01:37:47,710
if you look for statistical theory of something

1537
01:37:48,740 --> 01:37:52,060
and by a sample of those

1538
01:37:52,110 --> 01:37:54,060
we need to know

1539
01:37:54,100 --> 01:37:55,760
the sun

1540
01:37:55,760 --> 01:37:58,790
number of observations

1541
01:38:00,730 --> 01:38:02,080
in the case of the history

1542
01:38:02,110 --> 01:38:04,020
this is not possible

1543
01:38:07,200 --> 01:38:09,060
number of iterations is

1544
01:38:13,480 --> 01:38:17,830
in data streams so we the sampling step

1545
01:38:17,850 --> 01:38:19,160
must be

1546
01:38:19,540 --> 01:38:23,720
different from standard

1547
01:38:23,730 --> 01:38:24,930
and are some

1548
01:38:24,940 --> 01:38:26,460
known problems

1549
01:38:26,510 --> 01:38:28,250
we with the

1550
01:38:34,870 --> 01:38:36,790
tasks that involve

1551
01:38:36,830 --> 01:38:40,200
the money starting distribution of string

1552
01:38:40,200 --> 01:38:42,660
for example we know that

1553
01:38:42,730 --> 01:38:47,050
sampling strategy has low probability of error

1554
01:38:48,090 --> 01:38:52,950
are anomalies

1555
01:38:56,330 --> 01:38:59,090
take this is an illustrative tech

1556
01:39:00,030 --> 01:39:02,250
sampling from data streams

1557
01:39:02,410 --> 01:39:04,290
there's been percent

1558
01:39:04,340 --> 01:39:05,890
eighty five five

1559
01:39:05,900 --> 01:39:07,560
he is the

1560
01:39:08,990 --> 01:39:10,230
so what sample

1561
01:39:10,810 --> 01:39:12,550
because he is

1562
01:39:12,550 --> 01:39:14,550
we want to

1563
01:39:14,900 --> 01:39:17,630
only for example of six

1564
01:39:17,690 --> 01:39:19,550
five k

1565
01:39:21,280 --> 01:39:22,160
the case

1566
01:39:22,180 --> 01:39:25,430
the size of the reservoir

1567
01:39:26,370 --> 01:39:29,490
this at first scale many elements

1568
01:39:30,650 --> 01:39:32,570
and after that

1569
01:39:34,780 --> 01:39:36,950
a new

1570
01:39:39,050 --> 01:39:40,010
that is

1571
01:39:40,460 --> 01:39:45,220
it is inset into this one with these probability

1572
01:39:45,260 --> 01:39:46,790
and when we said

1573
01:39:46,820 --> 01:39:50,870
but the point we must remove

1574
01:39:51,060 --> 01:39:53,510
another point in the

1575
01:39:53,520 --> 01:39:56,850
one so the with the at night

1576
01:39:56,870 --> 01:39:59,860
one of the things that's the

1577
01:39:59,870 --> 01:40:04,180
in the way the this is a very simple of

1578
01:40:04,180 --> 01:40:09,900
knowledge base that might be dissolved it's been proven that are much more complex that

1579
01:40:10,160 --> 01:40:11,020
is all

1580
01:40:11,030 --> 01:40:14,110
but the basic idea is that

1581
01:40:14,130 --> 01:40:16,440
we can prove that this is

1582
01:40:16,480 --> 01:40:22,300
four only for example by easily for example flat metric

1583
01:40:24,560 --> 01:40:30,550
one of the tasks or

1584
01:40:30,560 --> 01:40:31,510
in most

1585
01:40:31,540 --> 01:40:33,250
machine learning approach

1586
01:40:33,290 --> 01:40:35,020
one of the

1587
01:40:36,660 --> 01:40:38,220
not relations that is

1588
01:40:38,220 --> 01:40:41,600
very common is to come

1589
01:40:41,790 --> 01:40:43,500
we need to come from

1590
01:40:44,110 --> 01:40:49,350
number of observations really of the class and so on

1591
01:40:49,520 --> 01:40:51,570
a real concern

1592
01:40:53,990 --> 01:40:56,130
illustrative county

1593
01:40:56,150 --> 01:40:59,720
the problem in the case of string

1594
01:41:04,030 --> 01:41:05,720
the first one

1595
01:41:05,730 --> 01:41:07,550
is that

1596
01:41:09,290 --> 01:41:10,620
the number of

1597
01:41:10,740 --> 01:41:12,680
this thing follow

1598
01:41:12,720 --> 01:41:14,640
of a random variable

1599
01:41:14,660 --> 01:41:17,860
suppose that we

1600
01:41:17,900 --> 01:41:22,590
the domain of that variable is very large

1601
01:41:22,640 --> 01:41:27,210
for example in TCP IP address

1602
01:41:27,450 --> 01:41:29,840
suppose that you want to know

1603
01:41:29,900 --> 01:41:32,900
on many different parents

1604
01:41:32,910 --> 01:41:34,770
the IP

1605
01:41:40,980 --> 01:41:43,490
this is the trivial problem

1606
01:41:43,510 --> 01:41:45,580
if you do we don't have

1607
01:41:46,100 --> 01:41:47,820
memory problem

1608
01:41:50,590 --> 01:41:52,680
of the domain

1609
01:41:53,830 --> 01:41:56,020
you can see that there is very large

1610
01:41:56,030 --> 01:41:57,690
it is not possible

1611
01:41:57,700 --> 01:41:59,950
two of them

1612
01:42:01,980 --> 01:42:05,540
we have space linear up to two

1613
01:42:07,430 --> 01:42:09,070
what we want

1614
01:42:09,130 --> 01:42:10,250
is an

1615
01:42:10,250 --> 01:42:12,450
approximate solutions

1616
01:42:13,430 --> 01:42:14,290
look at it

1617
01:42:15,890 --> 01:42:18,270
most of the problems in

1618
01:42:18,320 --> 01:42:19,590
data streams

1619
01:42:19,610 --> 01:42:21,810
out of this

1620
01:42:21,820 --> 01:42:27,030
you have include the the wall that is very large

1621
01:42:27,070 --> 01:42:28,510
and you want

1622
01:42:28,550 --> 01:42:29,600
that is

1623
01:42:29,600 --> 01:42:31,400
and i multiply that

1624
01:42:31,420 --> 01:42:34,580
by the difference between my current p

1625
01:42:34,580 --> 01:42:35,990
this is my current guess

1626
01:42:36,010 --> 01:42:38,990
and the template the template that either

1627
01:42:39,050 --> 01:42:44,040
but the have and what i've been doing this i update my car p

1628
01:42:44,070 --> 01:42:45,670
with this delta p

1629
01:42:45,690 --> 01:42:48,200
and then i keep on in writing until

1630
01:42:49,610 --> 01:42:54,110
actually should BP converges not delta p my p convergence estimate p

1631
01:42:54,130 --> 01:42:58,880
essentially states that normally we just use a set number iterations so fifteen or twenty

1632
01:42:58,880 --> 01:43:02,950
but you can you can actually do a couple of things to profile things and

1633
01:43:02,950 --> 01:43:07,880
things like that but that's essentially the in natural very simple it is an application

1634
01:43:07,880 --> 01:43:10,320
of the gas and now we can kind of see here

1635
01:43:10,320 --> 01:43:14,480
but the magic and happens in how you framed the problem kind of framing this

1636
01:43:14,560 --> 01:43:16,980
as one big linear generative model

1637
01:43:17,010 --> 01:43:20,160
of how things to spice and then i converted

1638
01:43:20,220 --> 01:43:23,550
to try and work out the war

1639
01:43:23,590 --> 01:43:30,180
and as i said OK we'll use local patches something so that this is an

1640
01:43:30,180 --> 01:43:32,360
image of a scene from one perspective

1641
01:43:32,370 --> 01:43:35,370
another from another perspective and if we do

1642
01:43:35,390 --> 01:43:40,870
OK so translations are enough patches we can get disparity map

1643
01:43:40,870 --> 01:43:45,610
and so that useful for working at distances and depth and things like that

1644
01:43:45,620 --> 01:43:47,450
so this is one stereo vision

1645
01:43:47,500 --> 01:43:52,610
what it's also useful in

1646
01:43:52,610 --> 01:43:54,600
is along

1647
01:43:54,670 --> 01:43:58,080
so a couple of examples here of the of the learning for and this is

1648
01:43:58,080 --> 01:43:59,160
just right OK

1649
01:43:59,210 --> 01:44:03,560
and and if you guys could look this up in a couple onto map but

1650
01:44:03,580 --> 01:44:05,350
amazing how it works

1651
01:44:05,560 --> 01:44:06,910
so that

1652
01:44:06,920 --> 01:44:08,460
this is the final

1653
01:44:08,510 --> 01:44:11,090
so can be massively off and you can kind of see here

1654
01:44:11,100 --> 01:44:14,840
this is the wall and keeps refining refining

1655
01:44:19,110 --> 01:44:22,620
yes yes at the beginning so we can go and we can go over this

1656
01:44:22,620 --> 01:44:27,450
sort of so you can actually make this stuff would have have a bigger field

1657
01:44:27,450 --> 01:44:28,140
field of

1658
01:44:28,930 --> 01:44:34,510
reach if you take a coarse to fine strategy so actually if you subsample this

1659
01:44:34,510 --> 01:44:35,920
image down

1660
01:44:36,010 --> 01:44:39,040
you take gradients over over subsampled image

1661
01:44:39,080 --> 01:44:43,700
you can then use the pixel coherence assumption over that subsampled image rather enough on

1662
01:44:43,720 --> 01:44:47,230
image so if you've got a very again there's limit to how far you can

1663
01:44:47,230 --> 01:44:50,870
take that but this is for illustration purposes just to come to show that OK

1664
01:44:50,870 --> 01:44:54,350
this is my template with the image this is the thing but it is conical

1665
01:44:54,350 --> 01:44:58,460
cool the thing it to convert stream how tightly work so if you if you

1666
01:44:58,460 --> 01:45:03,480
do it well enough it really fails a big thing but here is how you

1667
01:45:03,480 --> 01:45:08,530
calculate gradients and i was talking about yesterday on that so if you can arbitrarily

1668
01:45:08,530 --> 01:45:09,960
calculate gradients

1669
01:45:10,020 --> 01:45:14,590
this can this can file because the grains might be noisy if you're going the

1670
01:45:14,590 --> 01:45:21,390
gradient function in matlab or i i i i prefer so using more regression based

1671
01:45:21,390 --> 01:45:24,790
techniques to do the to the to the gradients because it ends up being a

1672
01:45:24,790 --> 01:45:28,420
much better prediction because if you think about what you're doing you just have a

1673
01:45:29,170 --> 01:45:30,830
model of how

1674
01:45:30,840 --> 01:45:33,940
the template image there is a function of delta p

1675
01:45:33,960 --> 01:45:36,840
and then when i trying to work out what delta p is on this inventing

1676
01:45:36,840 --> 01:45:40,640
that that's all i'm doing so i want to make sure that linear model linear

1677
01:45:40,640 --> 01:45:43,750
generative model is a good one

1678
01:45:43,790 --> 01:45:46,290
so is every kind of fun with

1679
01:45:46,590 --> 01:45:50,090
so it's it's pretty simple but i think it's a nice illustration

1680
01:45:50,100 --> 01:45:52,040
i talking about

1681
01:45:52,320 --> 01:45:57,620
there's a lot of different extensions and if an changes whatever to the LK algorithm

1682
01:45:57,620 --> 01:46:03,280
have been proposed very good a colleague of mine was that same year he says

1683
01:46:03,380 --> 01:46:08,000
microsoft research simon baker he wrote actually a suite of

1684
01:46:09,180 --> 01:46:15,270
quite long but very interesting papers called OK twenty lucas kanade a lucas kanade have

1685
01:46:15,280 --> 01:46:20,580
the the twenty years on and they're kind of five five installments in the in

1686
01:46:20,580 --> 01:46:21,580
the papers and

1687
01:46:21,620 --> 01:46:22,600
i guess all of

1688
01:46:22,720 --> 01:46:28,190
about two hundred pages things and it basically it's all variants of that

1689
01:46:28,210 --> 01:46:33,900
so basically like how can i e thirty in his first first volume whatever he

1690
01:46:33,900 --> 01:46:38,040
goes through world in this is the least squares iterative problem one of the best

1691
01:46:38,040 --> 01:46:42,590
ways of doing for four different images whatever is the hessian always will right because

1692
01:46:43,090 --> 01:46:44,380
market bombing

1693
01:46:44,400 --> 01:46:49,800
should be actually tackling the phone instead of showing the forecasting rather than the pseudo

1694
01:46:49,800 --> 01:46:54,380
science and is there any kind of benefit and perhaps doing steepest descent where on

1695
01:46:54,390 --> 01:46:59,090
this basically the gradients and sitting and doing some sort of long search instead of

1696
01:46:59,840 --> 01:47:05,750
trying to do a new talk to make but generally for lucas kanade it's well-established

1697
01:47:05,750 --> 01:47:08,900
another gasoline approach one that are kind of

1698
01:47:08,920 --> 01:47:14,420
been talking about here is the best approach are unfamiliar with casting everything

1699
01:47:14,700 --> 01:47:17,170
that are so

1700
01:47:17,180 --> 01:47:20,350
and in in bakers baker's work whenever he showed

1701
01:47:20,370 --> 01:47:24,260
baker and a couple of other people he showed that for large

1702
01:47:24,270 --> 01:47:29,870
a lot of image domains in image top object sort of gasoline it the best

1703
01:47:29,870 --> 01:47:35,210
so that's something else you guys can read up on but with decision gasoline here

1704
01:47:35,320 --> 01:47:41,170
so now there are computational concerns so the lucas is going and then worked really

1705
01:47:41,860 --> 01:47:46,930
for calculating optical flow because most of the time i was dealing with small image

1706
01:47:46,930 --> 01:47:52,170
patches on my coherence my what was this translation was really doing anything too fancy

1707
01:47:52,170 --> 01:47:52,830
at all

1708
01:47:52,940 --> 01:47:56,140
but as soon as i even when get into like in the final

1709
01:47:56,180 --> 01:47:58,710
i start getting into some computational concerns

1710
01:47:58,720 --> 01:48:03,620
because if you can see here this j which is very important in my is

1711
01:48:03,620 --> 01:48:09,830
about actually has to be recomputed at every iteration because i'm updating this page

1712
01:48:09,870 --> 01:48:14,310
and i think this p every iteration so i have to keep only keep repeating

1713
01:48:14,310 --> 01:48:18,040
this j tackling this has taking the inversion

1714
01:48:18,060 --> 01:48:22,560
and and on the unreasonable fast computers is that it's not about when you're talking

1715
01:48:22,560 --> 01:48:28,330
over like couple of thousand hundreds of thousands of pixels with quite competitive awards this

1716
01:48:28,330 --> 01:48:33,400
can really slow things down so and in vision were all about doing things but

1717
01:48:33,400 --> 01:48:38,470
what about doing things they in real time close to real time and even on

1718
01:48:38,470 --> 01:48:40,960
the fastest computers at the moment if want account

1719
01:48:41,230 --> 01:48:46,670
do complicated war with lucas kanade we can do real time there might be some

1720
01:48:46,670 --> 01:48:50,850
hope perhaps integrating some of stuff with GPS and things like that but at the

1721
01:48:50,850 --> 01:48:53,210
moment it's still real problem

1722
01:48:53,220 --> 01:48:56,610
so is there any way we can precompute

1723
01:48:56,610 --> 01:48:58,680
so that's also can be done by companies he

1724
01:49:00,120 --> 01:49:00,690
in that case

1725
01:49:01,570 --> 01:49:05,630
i like to tell me that multidimensional scaling is a special case appreciate you just

1726
01:49:05,630 --> 01:49:07,770
like because because multidimensional scaling is over

1727
01:49:09,650 --> 01:49:12,250
anyway logically is a special case of kernel piece e eight

1728
01:49:13,020 --> 01:49:13,410
in the

1729
01:49:15,770 --> 01:49:16,170
and the

1730
01:49:16,820 --> 01:49:17,280
and can

1731
01:49:17,520 --> 01:49:21,750
it can be done with all positive definite kernels may be the same is true for much initial scaling

1732
01:49:22,210 --> 01:49:28,900
although whenever i see is multidimensional scaling is done before uh stationary kernels that depend only on the distance

1733
01:49:30,610 --> 01:49:31,650
history said

1734
01:49:34,960 --> 01:49:40,800
okay so logically it subsumes multidimensional scaling and in some some some other methods and

1735
01:49:40,800 --> 01:49:41,990
we have a paper about that

1736
01:49:42,540 --> 01:49:44,950
and we show that it contains uh

1737
01:49:45,220 --> 01:49:47,100
locally linear embedding in the

1738
01:49:48,200 --> 01:49:50,970
the last and i can map end in some limit

1739
01:49:52,760 --> 01:49:57,750
isomap doesn't always correspond to a positive definite matrix in some limit also isomap so

1740
01:49:57,750 --> 01:49:59,750
if you're interested in that had data

1741
01:50:00,530 --> 01:50:03,500
in dimensionality reduction and that's interesting

1742
01:50:04,280 --> 01:50:05,420
connection to look at

1743
01:50:08,640 --> 01:50:09,880
so much about his you

1744
01:50:10,490 --> 01:50:12,580
and we applied it to some problems but

1745
01:50:13,680 --> 01:50:16,700
in most other applications with by others i think has been applied in the

1746
01:50:17,080 --> 01:50:19,120
and i worked in the computer vision

1747
01:50:19,620 --> 01:50:20,930
so i'm not gonna go into that

1748
01:50:22,080 --> 01:50:24,690
well maybe just i think you don't see much anyway so this is

1749
01:50:25,800 --> 01:50:28,510
this was an application to super resolution where we

1750
01:50:29,190 --> 01:50:30,750
trained piece e eight on some

1751
01:50:31,180 --> 01:50:32,290
high resolution images

1752
01:50:33,030 --> 01:50:37,870
and then we took low resolution images projected onto to our learned piece basis

1753
01:50:38,680 --> 01:50:42,210
and this we could reconstruct higher resolutions but i think

1754
01:50:42,900 --> 01:50:45,110
on this so basically what you should be seeing

1755
01:50:46,270 --> 01:50:48,740
is that this reconstruction is sort of similar to the

1756
01:50:49,400 --> 01:50:54,290
original and it's better to these other methods better than these other methods but maybe

1757
01:50:54,290 --> 01:50:55,110
you don't see it on this

1758
01:50:58,810 --> 01:51:01,760
so now support vector machines any questions

1759
01:51:02,750 --> 01:51:03,320
but this part

1760
01:51:05,900 --> 01:51:08,390
i think we have enough time left for support vector machines

1761
01:51:10,570 --> 01:51:13,050
so you've heard about them several times the basic idea

1762
01:51:14,040 --> 01:51:20,200
not that enter the feature space compute separating hyperplane which correspond to a non-linear separation in the input domain

1763
01:51:21,060 --> 01:51:21,540
so i'll

1764
01:51:22,990 --> 01:51:26,710
in detail how the separating hyperplane is computed and hopefully i can also show you

1765
01:51:27,550 --> 01:51:28,430
prove a bound on the

1766
01:51:29,190 --> 01:51:30,480
we dimension or something

1767
01:51:30,940 --> 01:51:31,830
something like it

1768
01:51:33,830 --> 01:51:37,080
of same optimum modern or large margin separating hyperplanes

1769
01:51:38,410 --> 01:51:39,380
welcome to the programme

1770
01:51:41,510 --> 01:51:42,560
i was thinking written

1771
01:51:43,490 --> 01:51:44,500
by such an equation

1772
01:51:45,320 --> 01:51:49,440
hyperplane is a set of points satisfying this kind of equation where w is a vector of

1773
01:51:49,880 --> 01:51:52,800
it's not equal to zero w is the normal vectorp hyperplane

1774
01:51:53,230 --> 01:51:54,320
and bees and offset

1775
01:51:55,170 --> 01:51:57,970
and this hyperplane can be thought of as a separating one

1776
01:51:58,470 --> 01:52:02,280
because this quantity here takes positive values on one side

1777
01:52:03,810 --> 01:52:08,200
so in one half space and it takes negative values on the other side of the hyperplane

1778
01:52:13,250 --> 01:52:16,370
we can also define an optimal separating hyperplane based on

1779
01:52:17,010 --> 01:52:17,960
two datasets

1780
01:52:18,520 --> 01:52:21,080
and by this we mean the hyperplane which has

1781
01:52:21,740 --> 01:52:26,720
the largest distance to any of the points which separates the points correctly this one class the other class

1782
01:52:27,410 --> 01:52:30,740
and while satisfying the constraint that the closest point to the hyperplane

1783
01:52:31,250 --> 01:52:34,760
has maximal distance so the distance of the closest point is maximized

1784
01:52:35,510 --> 01:52:38,830
one way to get this is to take the convex hulls of these sets of points

1785
01:52:39,460 --> 01:52:43,470
and then there's the problem the shortest connection between these two convex hulls and that

1786
01:52:43,470 --> 01:52:44,580
will give us the normal vector

1787
01:52:50,000 --> 01:52:52,470
note that there is a scaling degree of freedom

1788
01:52:53,030 --> 01:52:53,940
in this whole business

1789
01:52:55,590 --> 01:52:58,020
we have a hyperplane defined by this equation

1790
01:52:58,980 --> 01:53:02,520
we can multiply this equation by some number that's non-zero

1791
01:53:03,040 --> 01:53:05,810
which gives us a different weight vector and the different offset

1792
01:53:06,510 --> 01:53:09,390
but this is the set of points so this equation is still the same

1793
01:53:10,780 --> 01:53:15,300
so there is a degree of freedom is like a cage degree in physics

1794
01:53:16,070 --> 01:53:18,570
and this thing here describes the same hyperplane is this thing

1795
01:53:19,500 --> 01:53:24,810
and then when k one way to remove this uh was the definition reuptake who said that

1796
01:53:25,320 --> 01:53:29,860
uh recall hyperplane being canonical form with respect to a given dataset

1797
01:53:30,840 --> 01:53:35,420
if the skating is such that for the closest points of the data set to the hyperplane

1798
01:53:35,990 --> 01:53:38,050
this thing here has more to this one

1799
01:53:38,050 --> 01:53:42,090
and then i'll talk about how to analyse the training year

1800
01:53:43,880 --> 01:53:49,130
and then finally all talk about how to analyse the test error the generalisation error

1801
01:53:49,210 --> 01:53:54,800
based on something called the margins theory

1802
01:53:54,860 --> 01:53:57,900
OK so

1803
01:53:57,920 --> 01:54:00,300
so let me go back to

1804
01:54:00,350 --> 01:54:04,120
this description of boosting that i gave at the beginning and give a little bit

1805
01:54:04,120 --> 01:54:08,310
more formal description of what the problem is

1806
01:54:08,330 --> 01:54:13,080
so we're starting out with the training set as usual with learning problems

1807
01:54:13,090 --> 01:54:17,060
so the training set consists of examples

1808
01:54:17,070 --> 01:54:21,080
each example is a pair x i y i

1809
01:54:21,130 --> 01:54:25,880
OK if one is the pair x i y i the size of the instances

1810
01:54:25,900 --> 01:54:29,100
so those are the things we're trying to make predictions on so

1811
01:54:29,780 --> 01:54:35,570
like in this call classification example the instances would be

1812
01:54:35,620 --> 01:54:39,990
the actual utterances that was set by the telephone caller

1813
01:54:40,000 --> 01:54:42,650
and the y axis are the labels

1814
01:54:43,550 --> 01:54:46,820
that would be the label that we're trying to predict collect call

1815
01:54:46,920 --> 01:54:49,730
a credit card call and so on

1816
01:54:49,750 --> 01:54:53,380
so in that problem there are many possible labels

1817
01:54:53,400 --> 01:54:57,150
the first part of the talking and assume that there are only two labels

1818
01:54:57,200 --> 01:55:01,250
and it's usually convenient to assume that those two labels are just plus one to

1819
01:55:01,250 --> 01:55:03,090
minus one

1820
01:55:03,140 --> 01:55:04,620
makes the as a little

1821
01:55:06,820 --> 01:55:09,740
OK so that's what we're given

1822
01:55:09,780 --> 01:55:13,310
and now we want to do is OK so

1823
01:55:13,360 --> 01:55:17,060
what i said before is that boosting works in rounds so we're going to have

1824
01:55:17,060 --> 01:55:17,930
a capital

1825
01:55:17,940 --> 01:55:19,590
eighty rounds

1826
01:55:19,610 --> 01:55:22,260
OK so capital t is the number of rounds

1827
01:55:22,310 --> 01:55:24,650
and what do we do on each round

1828
01:55:25,520 --> 01:55:29,720
what we do with the boosting algorithm does is it starts out by constructing a

1829
01:55:31,320 --> 01:55:32,440
the team

1830
01:55:32,450 --> 01:55:36,100
over the training examples so these are the indices

1831
01:55:36,120 --> 01:55:40,160
the training examples so here and changing things

1832
01:55:40,170 --> 01:55:43,770
a little bit earlier i said that on every round of boosting

1833
01:55:43,810 --> 01:55:47,020
which is a subset of the examples

1834
01:55:47,070 --> 01:55:50,270
and now i'm changing things a little bit and saying instead of choosing a subset

1835
01:55:50,270 --> 01:55:54,030
of the examples which is the distribution over the examples

1836
01:55:54,050 --> 01:55:57,620
so there sort of the same thing i mean distribution is sort of just a

1837
01:55:57,650 --> 01:56:02,350
softer version of choosing a subset and you can take a distribution in subsample from

1838
01:56:02,350 --> 01:56:05,690
and and created this a subset and so on

1839
01:56:05,730 --> 01:56:09,270
so what we're doing is we're constructing this distribution

1840
01:56:09,310 --> 01:56:12,470
and the idea is that we're going to put the most weight on the examples

1841
01:56:12,470 --> 01:56:14,740
that we care the most about

1842
01:56:14,760 --> 01:56:19,610
so the weight under the distribution is going to reflect how much we care about

1843
01:56:19,610 --> 01:56:21,920
each one of those training examples

1844
01:56:22,000 --> 01:56:26,070
for this particular round of boosting

1845
01:56:26,740 --> 01:56:30,710
so we construct this distribution dt somehow

1846
01:56:30,730 --> 01:56:34,810
and then we passed the distribution after the weak learning algorithm

1847
01:56:34,830 --> 01:56:37,470
the weak learning algorithm gives us back

1848
01:56:37,480 --> 01:56:41,890
a weak classifier what i was calling before rule of thumb

1849
01:56:41,910 --> 01:56:47,890
and we classifiers like any classifiers just for you know that each t

1850
01:56:47,900 --> 01:56:52,530
and all it is is the prediction rule so it says for any instance

1851
01:56:52,540 --> 01:56:55,600
in our space

1852
01:56:55,620 --> 01:57:00,550
the classifiers giving us the prediction which is either plus one to minus one

1853
01:57:00,560 --> 01:57:02,070
and what we want

1854
01:57:02,080 --> 01:57:07,280
from the weak learners we want the weak learner to find a weak classifier with

1855
01:57:07,330 --> 01:57:09,200
small area

1856
01:57:09,210 --> 01:57:12,360
OK so we're measuring a better way of saying it is that we are measuring

1857
01:57:12,360 --> 01:57:13,450
the goodness

1858
01:57:13,500 --> 01:57:16,290
of each of these weak classifiers

1859
01:57:16,300 --> 01:57:17,860
in terms of

1860
01:57:17,880 --> 01:57:19,510
it's error rate

1861
01:57:19,560 --> 01:57:24,680
so it's air raid with respect to the distribution dt in which it was trained

1862
01:57:24,690 --> 01:57:28,340
so we measure the goodness of one of these weak classifiers

1863
01:57:28,360 --> 01:57:34,890
as the probability of choosing one of these training examples under the distribution dt

1864
01:57:34,910 --> 01:57:38,490
such that training examples misclassified

1865
01:57:38,500 --> 01:57:42,440
such that the prediction of that we classifier around

1866
01:57:42,450 --> 01:57:43,630
example outside

1867
01:57:43,680 --> 01:57:46,840
is different from the correct label y i

1868
01:57:46,890 --> 01:57:49,680
kennedy noting that mary epsilon two

1869
01:57:49,730 --> 01:57:52,770
they should think about twenty is a fairly big number

1870
01:57:52,790 --> 01:57:55,910
like forty percent

1871
01:57:57,740 --> 01:58:00,010
so we do that for capital t rounds

1872
01:58:00,020 --> 01:58:03,910
and then finally we take all of these weak classifiers we combine them into a

1873
01:58:03,910 --> 01:58:07,680
single combined or final classifier each five

1874
01:58:08,330 --> 01:58:09,450
so there are two

1875
01:58:09,480 --> 01:58:13,180
really important details that i'm leaving out from the slide

1876
01:58:13,190 --> 01:58:14,850
the first one is

1877
01:58:14,890 --> 01:58:17,590
how do you construct the distribution dt

1878
01:58:17,610 --> 01:58:21,850
and secondly once we found all the weak classifiers how do we combine them into

1879
01:58:21,850 --> 01:58:22,860
the combined

1880
01:58:22,870 --> 01:58:26,420
class for the final combine classifiers

1881
01:58:26,470 --> 01:58:30,970
so here's how adaboost answers those questions

1882
01:58:30,980 --> 01:58:34,410
so first of how do we construct the distribution dt

1883
01:58:34,420 --> 01:58:37,130
how do we construct this distribution dt

1884
01:58:37,150 --> 01:58:41,110
well on the very first round and round one

1885
01:58:41,120 --> 01:58:42,130
we don't have any

1886
01:58:42,140 --> 01:58:47,200
information of any kind so just uniform distribution over all the examples will give equal

1887
01:58:47,200 --> 01:58:50,330
weight or importance to all of the training examples

1888
01:58:50,340 --> 01:58:51,890
so they each get way

1889
01:58:51,900 --> 01:58:54,730
one over and since there

1890
01:58:54,740 --> 01:58:58,440
so now how do we update this distribution

1891
01:58:58,460 --> 01:59:00,300
well suppose on round t

1892
01:59:00,320 --> 01:59:02,770
we have the distribution dt

1893
01:59:02,810 --> 01:59:06,570
we have the weak classifier ht that we just computed

1894
01:59:06,590 --> 01:59:10,390
and now we want to do is we want to update this distribution

1895
01:59:10,510 --> 01:59:13,610
so adaboost is something very very simple

1896
01:59:13,610 --> 01:59:16,630
are we going to do is we're going to increase the weight of examples which

1897
01:59:17,350 --> 01:59:21,740
incorrectly classified because we're trying to focus on those hard examples

1898
01:59:21,790 --> 01:59:25,730
we're going to cut the weight of the examples which were correctly classified

1899
01:59:25,730 --> 01:59:28,900
because we want to give them less importance because they work

1900
01:59:28,960 --> 01:59:30,960
just correctly classified

1901
01:59:30,980 --> 01:59:35,530
so for each example i saw trying to compute the new distribution

1902
01:59:35,660 --> 01:59:37,880
t plus one for the following round

1903
01:59:37,930 --> 01:59:40,100
so for each example by i

1904
01:59:40,100 --> 01:59:44,450
things off i think what we need to do

1905
01:59:44,500 --> 01:59:45,990
is consider

1906
01:59:45,990 --> 01:59:49,550
a definition i mean to define what love was

1907
01:59:49,620 --> 01:59:52,670
but then most of the experiments i'm going to talk about are

1908
01:59:52,730 --> 01:59:58,110
really focused more on attraction and one who finds each other

1909
01:59:58,110 --> 02:00:03,720
all of romantic interest that might develop into

1910
02:00:03,760 --> 02:00:04,690
i love

1911
02:00:04,740 --> 02:00:10,050
relationship but let's start with the definition of love and i'm going to pick a

1912
02:00:10,120 --> 02:00:14,630
definition from a former colleague robert sternberg

1913
02:00:14,650 --> 02:00:16,050
o is now the dean

1914
02:00:16,060 --> 02:00:20,700
at tufts university but was here on right faculty yellow

1915
02:00:20,720 --> 02:00:23,060
for nearly thirty years or so

1916
02:00:23,100 --> 02:00:27,350
and here is the theory of love that are that it's made up of three

1917
02:00:27,860 --> 02:00:30,160
components intimacy

1918
02:00:31,630 --> 02:00:34,860
and commitment to what is sometimes called decision

1919
02:00:34,920 --> 02:00:41,310
commitment and these are relatively straightforward he argued that you don't have a lot if

1920
02:00:41,310 --> 02:00:45,930
you don't have all three of these elements intimacy

1921
02:00:45,940 --> 02:00:48,130
is the feeling of closeness

1922
02:00:48,200 --> 02:00:52,300
of connectedness with someone of bonding

1923
02:00:52,310 --> 02:00:55,760
operational you could think of intimacy as you share

1924
02:00:57,130 --> 02:00:58,680
share information

1925
02:00:58,690 --> 02:01:02,420
with this person that you don't share with anybody else

1926
02:01:02,430 --> 02:01:07,700
that's really what intimacy is the bottom and that comes from sharing information that is

1927
02:01:08,660 --> 02:01:13,070
with other with many other people

1928
02:01:13,090 --> 02:01:15,130
the second element

1929
02:01:15,150 --> 02:01:19,620
it's passion passion is what you think it it does

1930
02:01:19,630 --> 02:01:25,300
passion is the dry we would say the drive leads to romance you can think

1931
02:01:25,300 --> 02:01:29,990
of it as physical attraction or sex and

1932
02:01:30,120 --> 02:01:36,120
sternberg argues that this is a required component

1933
02:01:36,150 --> 02:01:42,350
of a love relationship it is not however require component of taking a shower in

1934
02:01:42,350 --> 02:01:47,490
calvin college before

1935
02:01:47,540 --> 02:01:52,790
the third element of luck in instead theories is

1936
02:01:53,610 --> 02:01:57,090
what he calls decision or commitment the

1937
02:01:58,940 --> 02:02:05,500
that one is in a love relationship the willingness to label it as such commitment

1938
02:02:05,550 --> 02:02:11,150
to maintain their relationship at least for some period of time starring would argue it's

1939
02:02:11,150 --> 02:02:15,360
not love if you don't call it love and if you don't have some desire

1940
02:02:15,360 --> 02:02:17,700
to maintain the relationship

1941
02:02:18,300 --> 02:02:20,380
if you have all three of these

1942
02:02:21,510 --> 02:02:25,290
passion and commitment installments the

1943
02:02:25,300 --> 02:02:27,300
you have one

1944
02:02:28,900 --> 02:02:33,800
what's interesting about the theory is what you have if you only have one to

1945
02:02:34,760 --> 02:02:36,900
or two three

1946
02:02:36,930 --> 02:02:40,920
what do you have and how is it different if you have a different two

1947
02:02:40,920 --> 02:02:42,010
out of three

1948
02:02:42,220 --> 02:02:49,300
these are what's interesting about this kind of theorizing is it gives it gives rise

1949
02:02:49,600 --> 02:02:54,920
to many different permutations that when you break them down and start to look at

1950
02:02:54,920 --> 02:02:56,110
them carefully

1951
02:02:56,120 --> 02:03:00,410
could be quite interesting so what i've done is i've taken

1952
02:03:00,450 --> 02:03:05,090
sturm graz three elements of love intimacy passionate commitment

1953
02:03:05,150 --> 02:03:10,840
and have listed out the different kinds of relationships one might have

1954
02:03:10,900 --> 02:03:12,120
if you had

1955
02:03:12,130 --> 02:03:14,860
zero one two or three

1956
02:03:14,870 --> 02:03:20,820
out of the three elements that i'm using names or types that stauffenberg uses in

1957
02:03:20,820 --> 02:03:25,430
his theory these are really from some of these are pretty obvious

1958
02:03:25,470 --> 02:03:30,110
if you don't have intimacy if you don't have passion if you don't have commitment

1959
02:03:30,120 --> 02:03:34,530
you don't have love story recalls this non love

1960
02:03:34,550 --> 02:03:41,090
that's the technical terms and essentially what he's saying is the relationship you now have

1961
02:03:41,090 --> 02:03:44,560
to the person sitting next to you presuming that is sitting next to a random

1962
02:03:44,560 --> 02:03:46,050
person that you didn't know

1963
02:03:47,040 --> 02:03:52,670
the college is probably not a lot

1964
02:03:52,680 --> 02:03:56,090
if it's something else we can talk about it at the end of the lecture

1965
02:03:56,090 --> 02:04:00,460
or perhaps when i get to it in moment now let's start to an

1966
02:04:01,580 --> 02:04:05,780
let's and intimacy this is sharing

1967
02:04:07,520 --> 02:04:11,370
a feeling of closeness connectedness bonding

1968
02:04:11,420 --> 02:04:15,510
let's say we have that was someone but we don't have passion there is no

1969
02:04:15,510 --> 02:04:18,950
sexual arousal and no commitment to maintain

1970
02:04:18,960 --> 02:04:21,900
the relationship this is like kingston recalls it

1971
02:04:22,900 --> 02:04:24,250
and like

1972
02:04:24,260 --> 02:04:34,000
is really what is happening in most typical friendships not close friendships but friendships of

1973
02:04:34,000 --> 02:04:35,670
other casual kind

1974
02:04:35,720 --> 02:04:40,500
you feel close to share information with that person that you don't share with other

1975
02:04:40,500 --> 02:04:42,140
many other people

1976
02:04:42,150 --> 02:04:47,140
which are not physically attractive and there's no particular commitment to maintaining this

1977
02:04:47,210 --> 02:04:50,200
for a long period of time

1978
02:04:50,260 --> 02:04:52,300
now what if you're not in it

1979
02:04:52,320 --> 02:04:55,410
you're not committed but your passion

1980
02:04:55,450 --> 02:04:57,510
you feel that sexual arousal

1981
02:04:58,510 --> 02:05:04,930
is what story would call infatuation and that term probably works for you too infatuated

1982
02:05:05,800 --> 02:05:08,790
and so this is love at first sight

1983
02:05:08,800 --> 02:05:10,080
i don't know

1984
02:05:10,090 --> 02:05:11,510
no you

1985
02:05:11,520 --> 02:05:15,000
we've never shared any secrets because i don't know you

1986
02:05:15,020 --> 02:05:17,080
i'm not can not committed to

1987
02:05:17,090 --> 02:05:21,370
defining this is anything i'm not committed to the future effect not thinking about the

1988
02:05:21,370 --> 02:05:23,080
future but right now

1989
02:05:23,090 --> 02:05:25,140
but why am i attractor

1990
02:05:25,160 --> 02:05:30,000
that's that's that's infatuation and that's what what's removed by

1991
02:05:30,010 --> 02:05:34,930
infatuated the third kind of it's one element

1992
02:05:34,930 --> 02:05:40,380
relationship if there's no intimacy right no binding no cause there's no secrets

1993
02:05:40,430 --> 02:05:42,300
no physical attraction

1994
02:05:42,310 --> 02:05:44,260
no sexual arousal

1995
02:05:46,230 --> 02:05:52,000
gosh we are going to maintain this relationship we are committed to it for all

1996
02:05:53,670 --> 02:05:57,250
so called the empty love to love is kind of interesting

