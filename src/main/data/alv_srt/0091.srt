1
00:00:00,000 --> 00:00:03,460
I guess in the next two slides

2
00:00:03,480 --> 00:00:06,500
actually in

3
00:00:06,540 --> 00:00:08,040
not in the next two slides

4
00:00:08,080 --> 00:00:12,980
a little bit further down I'll be telling you what are measures before I can

5
00:00:12,980 --> 00:00:14,380
tell you what are

6
00:00:14,440 --> 00:00:16,540
distributions over measures okay

7
00:00:21,020 --> 00:00:23,770
and Dirichlet process being kind of this very

8
00:00:23,790 --> 00:00:30,060
basically a canonical example of a nonparametric model it that's kind of like di different ways in which we

9
00:00:30,060 --> 00:00:32,690
could derive it

10
00:00:32,730 --> 00:00:36,000
and it's kind of a special case of lots of different

11
00:00:36,040 --> 00:00:37,460
processes okay

12
00:00:37,730 --> 00:00:43,530
and we'll be looking at we'll be deriving the Dirichlet process actually in three different ways

13
00:00:45,130 --> 00:00:50,880
the as the infinit limit of Gibbs sampler for finite mixture model and using the Chinese

14
00:00:50,880 --> 00:00:53,630
restaurant process and using the stick breaking construction

15
00:00:54,170 --> 00:00:59,730
okay so let's start off with this infinite limit of finite mixture models okay so this is I guess the

16
00:00:59,730 --> 00:01:02,650
simplest recall that we have this

17
00:01:02,730 --> 00:01:10,670
model this model for model based clustering which is typically called a finite mixture model

18
00:01:10,670 --> 00:01:14,960
and it's finite in the sense that there's a finite number of

19
00:01:15,000 --> 00:01:18,520
of mixture components or clusters

20
00:01:18,730 --> 00:01:23,040
and it's a mixture model in that you assume that the data that you observe

21
00:01:23,040 --> 00:01:31,830
is a mixture of of different basically heterogenous populations

22
00:01:31,940 --> 00:01:37,460
right and so just to recall for you the the generative model is that we

23
00:01:37,460 --> 00:01:42,210
have some indicated variables that i which tells us which cluster

24
00:01:42,230 --> 00:01:49,600
x i belongs to and it can take on one of k values k goes to one to big k

25
00:01:50,170 --> 00:01:55,210
the mixing proportion is pi with a prior given by a Dirichlet okay

26
00:01:55,600 --> 00:02:00,580
and again we have for each cluster k we have a parameter theta

27
00:02:00,580 --> 00:02:01,670
star k

28
00:02:01,690 --> 00:02:06,440
and we have a prior over theta star k given by h

29
00:02:08,060 --> 00:02:11,360
are pe have people here

30
00:02:11,400 --> 00:02:16,610
who has seen a Dirichlet distribution before

31
00:02:16,690 --> 00:02:20,600
who has not seen Dirichlet distributions

32
00:02:20,640 --> 00:02:24,270
quite a lot okay so

33
00:02:24,290 --> 00:02:27,650
I'll spend a few slides on them

34
00:02:31,900 --> 00:02:37,330
by h what I mean is a prior over the parameters of that cluster so

35
00:02:37,380 --> 00:02:39,110
f is gonna be your

36
00:02:39,130 --> 00:02:40,790
your cluster distribution

37
00:02:40,830 --> 00:02:47,130
and h is your prior so in the case of a mixture of Gaussians okay f

38
00:02:47,130 --> 00:02:51,560
would be a Gaussian parameterized by a mean and a variance

39
00:02:51,750 --> 00:02:53,900
and that theta will be

40
00:02:53,980 --> 00:02:58,690
vector of lenght too is will be the mean and the variance and h is gonna

41
00:02:58,690 --> 00:03:02,830
be a prior over your mean and your variance so you might think that maybe

42
00:03:02,830 --> 00:03:04,190
the mean

43
00:03:04,210 --> 00:03:10,690
I expect it to be lying close to zero okay and the variance will be approximately one okay

44
00:03:10,710 --> 00:03:15,440
so those are counter priors on your mean and your varinace

45
00:03:16,580 --> 00:03:21,940
so Dirichlet distribution is basically a

46
00:03:21,980 --> 00:03:23,630
distribution on the

47
00:03:23,650 --> 00:03:28,020
k dimensional probability simplex so this is basically the set of all vectors

48
00:03:28,020 --> 00:03:30,100
pi such that

49
00:03:30,130 --> 00:03:34,560
each entry of the vector is nonnegative and it sums to one okay

50
00:03:34,560 --> 00:03:39,290
typically you can think of this as a

51
00:03:39,500 --> 00:03:40,650
maybe I should draw this

52
00:03:41,270 --> 00:03:46,630
if you

53
00:03:46,670 --> 00:03:48,600
have a three dimensional space

54
00:03:49,120 --> 00:03:51,330
okay so this is your

55
00:03:53,540 --> 00:03:55,080
pi one

56
00:03:55,110 --> 00:03:56,100
pi two

57
00:03:56,360 --> 00:03:57,400
pi three okay

58
00:03:57,600 --> 00:04:04,960
and this is the space which basically corresponds to something like this basically it's

59
00:04:05,000 --> 00:04:07,880
this plain which passes through

60
00:04:07,980 --> 00:04:09,860
one zero zero

61
00:04:09,900 --> 00:04:10,960
zero one zero

62
00:04:12,250 --> 00:04:14,040
and zero zero one

63
00:04:14,100 --> 00:04:16,920
okay so it's a plain in that

64
00:04:18,610 --> 00:04:24,060
every point of this probability simplex is gonna be a vector of lenght three

65
00:04:24,130 --> 00:04:29,380
each of which is each entry is positive or is nonnegative and they have to sum to

66
00:04:29,380 --> 00:04:30,380
one okay

67
00:04:30,380 --> 00:04:37,520
so it spans by these three vectors basically it's all convex combinations of these three vectors okay

68
00:04:40,140 --> 00:04:46,610
and a Dirichlet distribution is a distribution on this space it has a density given

69
00:04:46,610 --> 00:04:51,650
by this thing okay so this here is gonna be the basically is the normalization

70
00:04:51,650 --> 00:04:56,860
constant for the density and over here is the part of the density which does

71
00:04:56,860 --> 00:04:58,080
depend on pi okay

72
00:04:58,210 --> 00:05:02,940
and is basically a products over k goes to one to big k of

73
00:05:02,940 --> 00:05:04,770
pi k raise to the

74
00:05:04,810 --> 00:05:06,690
some exponents minus one okay

75
00:05:06,770 --> 00:05:11,980
so these exponents are the parameters of the Dirichlet distribution okay

76
00:05:11,980 --> 00:05:15,710
useful when evaluation of each

77
00:05:15,720 --> 00:05:20,710
individual it takes a lot of time so this can be distributed

78
00:05:20,730 --> 00:05:23,880
o specific topic here is

79
00:05:23,890 --> 00:05:26,790
genetic algorithms have been subject to

80
00:05:26,810 --> 00:05:28,900
evaluation of solutions

81
00:05:28,910 --> 00:05:32,990
so far we we assumed we have some measure that can be

82
00:05:33,010 --> 00:05:41,200
calculated that can be checked on a computer so this is the most traditional way

83
00:05:41,200 --> 00:05:44,650
of problem solving with computers now

84
00:05:44,660 --> 00:05:46,900
in this case

85
00:05:46,910 --> 00:05:53,720
i subjective evaluation means interact way of using the genetic algorithms there of course some

86
00:05:53,720 --> 00:05:58,960
problems with the solutions cannot be measured using some numeric

87
00:05:59,420 --> 00:06:07,720
function like a way to minimum cost or the minimum error or whatever

88
00:06:07,740 --> 00:06:10,990
imagine for example

89
00:06:11,000 --> 00:06:19,960
some designs for example where one needs to decide about how these colors are nice

90
00:06:19,960 --> 00:06:27,710
how come how to evaluate this subjectively in this sense or maybe to evaluate taste

91
00:06:27,720 --> 00:06:29,490
or whatever so

92
00:06:30,910 --> 00:06:34,720
this is a way of doing it again in a sort of

93
00:06:34,720 --> 00:06:37,850
evolutionary experimentation computer

94
00:06:37,860 --> 00:06:41,150
or the algorithm offers a solution

95
00:06:41,200 --> 00:06:45,830
and the human expert tell us how good it is

96
00:06:45,850 --> 00:06:48,800
and then this is used as a measure

97
00:06:48,880 --> 00:06:52,950
to perform or to guide the search further

98
00:06:52,960 --> 00:06:56,460
there's also a very popular topic now

99
00:06:56,490 --> 00:07:00,420
the multi object to g is when there is

100
00:07:00,460 --> 00:07:03,110
more than one object you

101
00:07:03,160 --> 00:07:10,980
to be optimized within a certain problem this also requires extensions of these techniques

102
00:07:11,000 --> 00:07:17,990
OK this is already the next technique before going to that i would like to

103
00:07:19,150 --> 00:07:24,780
an example again of a which would be

104
00:07:24,790 --> 00:07:36,610
the support that explanation of GA's

105
00:07:36,620 --> 00:07:44,350
OK can you see these simplest colors in the back rows

106
00:07:44,960 --> 00:07:52,150
OK this is really a simple example but i must say that students use really

107
00:07:52,150 --> 00:07:57,280
like it because it illustrates in the way how the the genetic algorithm works

108
00:07:57,290 --> 00:08:00,220
first of all about the problem

109
00:08:00,240 --> 00:08:01,780
the task is

110
00:08:04,780 --> 00:08:07,870
ten digit number

111
00:08:07,990 --> 00:08:14,670
so to get the right ten digit number and the the information you get for

112
00:08:14,670 --> 00:08:20,520
that for every number you offer you suggest is the number of digits that are

113
00:08:22,320 --> 00:08:26,620
for example here is one solution seven six eight and so on

114
00:08:26,630 --> 00:08:28,480
and the blue

115
00:08:28,510 --> 00:08:34,270
background here indicates that this value is the correct one

116
00:08:34,290 --> 00:08:37,450
all the others are not

117
00:08:39,310 --> 00:08:44,210
actually you can say the fitness or the quality of the solution is one because

118
00:08:44,210 --> 00:08:47,000
only one digit is the correct one

119
00:08:47,010 --> 00:08:53,010
how this can be this computer program operates first it generates a sequence of ten

120
00:08:53,010 --> 00:09:00,390
digits it keeps it aside and then the solutions in this search is evaluated according

121
00:09:00,390 --> 00:09:06,270
to that solution of course according to that ten digit number

122
00:09:07,810 --> 00:09:11,770
so here we see a population of ten

123
00:09:11,790 --> 00:09:16,860
such solutions generated randomly and we see that actually

124
00:09:16,860 --> 00:09:18,670
there's different number of

125
00:09:18,680 --> 00:09:23,410
right digits varying from zero to five so

126
00:09:23,480 --> 00:09:25,270
more or less blood

127
00:09:26,690 --> 00:09:31,950
let's say we to solve this problem or try to solve it by genetic algorithms

128
00:09:32,210 --> 00:09:40,100
this is the population of ten individuals and this is what happens after that come

129
00:09:40,100 --> 00:09:41,770
the variation phase

130
00:09:42,670 --> 00:09:47,170
two strings these we first and the second one

131
00:09:47,220 --> 00:09:48,900
produced these two

132
00:09:48,910 --> 00:09:52,370
offspring by recombination

133
00:09:52,390 --> 00:09:53,600
so we can see

134
00:09:53,620 --> 00:09:58,170
seven six eight zero nineteen sixty five

135
00:09:59,180 --> 00:10:00,300
and then

136
00:10:00,330 --> 00:10:01,700
here is six eight

137
00:10:01,700 --> 00:10:06,660
but heuristics eight in the second so this was the crossing site and these two

138
00:10:06,660 --> 00:10:10,740
substrings were exchanged OK

139
00:10:11,450 --> 00:10:16,320
it's actually performing in the same way as on the binary string we use to

140
00:10:16,320 --> 00:10:22,540
parents and this defined crossing site and exchange the second part

141
00:10:22,570 --> 00:10:26,360
now this grey zone here are mutated values

142
00:10:26,370 --> 00:10:31,660
mutation is something that happens with low probability so

143
00:10:31,780 --> 00:10:37,620
these zero here you can see originally we had

144
00:10:37,620 --> 00:10:39,270
eight here OK

145
00:10:39,290 --> 00:10:46,110
so now after mutation this great period indicates the mutation this became zero

146
00:10:46,110 --> 00:10:53,410
so again this goes on according to the principles of variation and selection so we

147
00:10:53,410 --> 00:10:55,180
can run these

148
00:10:55,780 --> 00:11:01,160
it's a full one population for one generation so this continues in steps so we

149
00:11:01,160 --> 00:11:07,440
have some statistics here generation eight mean fitness average fitness and maximum fitness

150
00:11:07,530 --> 00:11:13,480
and best solutions from a solution found so far so we can run this

151
00:11:13,490 --> 00:11:17,950
not too wait for step and what you can see here is that

152
00:11:17,960 --> 00:11:20,900
OK ten digits correct here

153
00:11:20,920 --> 00:11:26,960
and this was found after one hundred and and eighty seven generations

154
00:11:28,210 --> 00:11:34,610
we can run it again

155
00:11:34,660 --> 00:11:38,670
this time we were lucky only thirty two generations

156
00:11:38,690 --> 00:11:45,680
as do it again

157
00:11:45,700 --> 00:11:51,760
it takes us to be more

158
00:11:51,890 --> 00:11:54,360
seven hundred and something

159
00:11:54,370 --> 00:12:02,640
maybe once again

160
00:12:02,640 --> 00:12:03,720
OK now

161
00:12:03,730 --> 00:12:05,940
four hundred and four

162
00:12:05,940 --> 00:12:09,790
so this is to illustrate the stochastic nature of this group

163
00:12:09,820 --> 00:12:11,310
you can see that

164
00:12:16,330 --> 00:12:18,510
in different rounds

165
00:12:18,560 --> 00:12:24,430
depending on of course the random values that are used and the seed is

166
00:12:24,460 --> 00:12:29,270
said differently in every round and the idea is

167
00:12:29,280 --> 00:12:34,030
with stochastic algorithms and this will be and a message from this demo

168
00:12:34,040 --> 00:12:38,470
a single run is just a single run if you want to

169
00:12:38,490 --> 00:12:43,450
evaluate the performance of the stochastic algorithm this should be done

170
00:12:44,240 --> 00:12:45,860
sequence of france

171
00:12:45,900 --> 00:12:50,500
with some statistical evaluation why this is important

172
00:12:52,130 --> 00:12:54,720
somebody may be interested

173
00:12:54,770 --> 00:12:59,930
what is let's say the average number of iterations one needs to run in order

174
00:12:59,940 --> 00:13:01,760
to obtain the solution

175
00:13:01,810 --> 00:13:04,850
if you're on the with once this is

176
00:13:04,920 --> 00:13:10,240
not not really informative you know what you need to provide some information you can

177
00:13:10,240 --> 00:13:15,760
only get after doing statistics or the number of rounds

178
00:13:15,780 --> 00:13:16,720
and so on

179
00:13:17,680 --> 00:13:21,190
this is a very good

180
00:13:21,200 --> 00:13:25,970
right this is this is just the next point i wanted to illustrate yes

181
00:13:26,000 --> 00:13:29,000
the role of the operators

182
00:13:29,430 --> 00:13:35,280
in particular mutation here y mutation is important

183
00:13:35,290 --> 00:13:41,370
we said this is a random changes of certain bits of certain genes

184
00:13:41,370 --> 00:13:43,580
two distributions p and q

185
00:13:43,660 --> 00:13:46,020
and is an epsilon p

186
00:13:46,040 --> 00:13:50,660
this is a little neighborhood of the distribution

187
00:13:51,370 --> 00:13:55,680
so while

188
00:13:55,750 --> 00:13:56,680
think of

189
00:13:56,700 --> 00:14:00,680
these y one through y n

190
00:14:00,830 --> 00:14:06,430
all right that's just the realization of say and i i d random variables and

191
00:14:06,430 --> 00:14:09,980
what you could do is take the empirical distribution

192
00:14:10,000 --> 00:14:15,310
of these vector of random variables you take these empirical distribution

193
00:14:17,460 --> 00:14:19,520
the true distribution is key

194
00:14:19,540 --> 00:14:21,940
that's why i have these bq here

195
00:14:22,850 --> 00:14:24,230
right so

196
00:14:24,250 --> 00:14:27,500
we know from the law of large numbers that

197
00:14:30,430 --> 00:14:33,200
eventually these empirical distribution

198
00:14:33,210 --> 00:14:35,850
is going to converge to q

199
00:14:36,580 --> 00:14:39,120
so now this is telling us

200
00:14:39,200 --> 00:14:41,930
what is going to be the probability

201
00:14:41,960 --> 00:14:47,480
then this empirical distribution is going to be in the little neighborhood of p

202
00:14:47,500 --> 00:14:51,270
it's going to be visiting a little neighborhood of p

203
00:14:51,310 --> 00:14:54,790
after and samples

204
00:14:54,810 --> 00:14:57,580
so if n is very small three or four

205
00:14:57,660 --> 00:15:01,700
well maybe there is a chance that u will be impersonating p

206
00:15:01,710 --> 00:15:05,430
by the sequence but it is going to be very unlikely that you are going

207
00:15:05,430 --> 00:15:07,160
to be in the neighborhood of p

208
00:15:08,160 --> 00:15:11,830
n is large because you should really be close to q

209
00:15:13,390 --> 00:15:17,390
the the relative entropy is actually gauging dot

210
00:15:17,410 --> 00:15:22,180
that the probability that road is going to be exponentially decreasing in n

211
00:15:22,200 --> 00:15:25,810
and the exponent is really the relative entropy

212
00:15:25,830 --> 00:15:28,350
so that's the difficulty

213
00:15:28,370 --> 00:15:32,680
of impersonating p by q

214
00:15:32,700 --> 00:15:35,390
now imagine that hue

215
00:15:35,410 --> 00:15:37,160
imagine that there are some

216
00:15:39,520 --> 00:15:41,680
some letters in this alphabet

217
00:15:41,790 --> 00:15:45,410
that have zero probability according to q

218
00:15:45,410 --> 00:15:50,370
and positive probability airport according to p

219
00:15:50,430 --> 00:15:57,350
well there's no way that you can impersonate a distribution

220
00:15:57,350 --> 00:15:58,870
the doing in in

221
00:15:58,980 --> 00:16:01,180
in person p with q

222
00:16:01,200 --> 00:16:05,660
because no matter what you do that the empirical distribution of y one through y

223
00:16:05,660 --> 00:16:10,250
and will never have positive probability in those letters so you will never be able

224
00:16:10,270 --> 00:16:15,730
to impersonate people you'll never be able to to be close to be and that's

225
00:16:16,580 --> 00:16:20,370
relative entropy in that case is infinity

226
00:16:20,700 --> 00:16:26,080
you cannot do it was the other way around around you may very well be

227
00:16:26,080 --> 00:16:32,640
able to do it maybe may very well be that if the true distribution is

228
00:16:33,620 --> 00:16:39,370
then after a while you're still a very very close to q

229
00:16:39,390 --> 00:16:40,430
in that case

230
00:16:41,600 --> 00:16:44,910
is is a finite number

231
00:16:44,930 --> 00:16:50,200
OK so this result is in our paper chernoff

232
00:16:50,270 --> 00:16:52,830
in fact it's interesting because

233
00:16:52,890 --> 00:16:55,180
he attributes the result

234
00:17:02,460 --> 00:17:04,390
and it turns out there

235
00:17:04,390 --> 00:17:08,940
the story is that then they are cysteine about this and he said i have

236
00:17:08,960 --> 00:17:15,040
no clue if never seen these results and so was called this time this times

237
00:17:15,960 --> 00:17:20,770
and so on but but then it turned out that what happened was the chairman

238
00:17:20,770 --> 00:17:22,290
of submitted the paper

239
00:17:22,290 --> 00:17:27,080
and then one of the reviewers said all you know i've seen this results in

240
00:17:27,080 --> 00:17:30,080
an unpublished paper by this time

241
00:17:30,230 --> 00:17:36,600
so then chairman of just reference this this thing it without even looking at it

242
00:17:36,600 --> 00:17:42,680
because it had not been published and so it's back like stein's lemma three

243
00:17:42,750 --> 00:17:47,330
the to turn off and is that the turn of in this paper

244
00:17:47,330 --> 00:17:48,930
usually people

245
00:17:49,000 --> 00:17:53,160
and reference to the wrong paper might of

246
00:17:53,160 --> 00:17:55,660
in order to reference this results

247
00:17:55,710 --> 00:18:01,390
now this one is the law of large numbers type of

248
00:18:02,700 --> 00:18:04,930
all of relative entropy

249
00:18:07,560 --> 00:18:09,210
this is kind of like invasion

250
00:18:10,290 --> 00:18:11,410
the type of

251
00:18:11,910 --> 00:18:15,750
analysis say after you have

252
00:18:16,850 --> 00:18:19,290
several observations

253
00:18:19,350 --> 00:18:25,660
in the observations come from p one is the reliability of rejecting q when p

254
00:18:25,660 --> 00:18:27,580
is true

255
00:18:29,270 --> 00:18:33,580
what you're going to do is we're going to find the posterior probability of both

256
00:18:35,210 --> 00:18:38,710
and then decide

257
00:18:38,730 --> 00:18:41,250
according to which one is larger

258
00:18:41,270 --> 00:18:44,330
so if you were to take

259
00:18:44,330 --> 00:18:45,620
that number

260
00:18:45,620 --> 00:18:46,270
there there

261
00:18:46,290 --> 00:18:50,680
the ratio of the two of posterior probabilities they belong them and then divide by

262
00:18:50,700 --> 00:18:53,580
and then that actually is going to

263
00:18:53,660 --> 00:18:58,540
to convert to the relative entropy of p and q

264
00:18:58,560 --> 00:19:02,410
so in fact this is the kind of based results

265
00:19:02,460 --> 00:19:05,810
then on patients can live with because

266
00:19:05,830 --> 00:19:10,730
it actually does not depend on the priority probability of the hypothesis even though

267
00:19:10,730 --> 00:19:12,560
unless there is that

268
00:19:12,580 --> 00:19:15,770
a priori probability you cannot define the quantities here

269
00:19:15,770 --> 00:19:19,640
at the end of the day it washes out

270
00:19:19,640 --> 00:19:25,230
OK so if this is large then you would expect that is going to be

271
00:19:25,250 --> 00:19:26,230
a likely

272
00:19:26,250 --> 00:19:30,620
that you're going to decide the all hypotheses

273
00:19:30,640 --> 00:19:32,330
you're going to the two

274
00:19:32,350 --> 00:19:39,750
be able to reject q when p is true with high reliability

275
00:19:39,770 --> 00:19:41,640
now in information theory

276
00:19:41,780 --> 00:19:49,580
when we do lossless data compression for example we do i huffman coding we ever

277
00:19:49,600 --> 00:19:53,710
huffman called then selects valuable

278
00:19:53,870 --> 00:19:59,910
length representations for different elements in the source

279
00:19:59,930 --> 00:20:02,500
and that

280
00:20:02,520 --> 00:20:06,330
code is designed to minimize the average length

281
00:20:06,330 --> 00:20:09,810
so it was designed for some distribution

282
00:20:11,620 --> 00:20:14,770
but then i suppose that the true distribution is

283
00:20:14,790 --> 00:20:16,460
these people

284
00:20:18,250 --> 00:20:21,890
well then you have to pay a penalty

285
00:20:21,940 --> 00:20:24,710
and that penalty is

286
00:20:24,730 --> 00:20:27,620
dpq the relative entropy of

287
00:20:28,160 --> 00:20:30,540
p with respect to q

288
00:20:32,180 --> 00:20:33,770
note is that what i said

289
00:20:33,810 --> 00:20:36,560
actually is not quite what i have in the slide in the slide i say

290
00:20:36,560 --> 00:20:38,910
the asymptotic ex is right

291
00:20:38,930 --> 00:20:43,410
because to be precise that result is not true

292
00:20:43,460 --> 00:20:45,330
is not true for any length

293
00:20:45,350 --> 00:20:50,410
it's only two asymptotically so the fact that this is the that the penalty

294
00:20:50,410 --> 00:20:54,120
for the mismatch in data compression is

295
00:20:54,160 --> 00:20:56,850
is true in the limit as n goes to infinity

296
00:20:56,850 --> 00:21:03,370
for kirk

297
00:21:03,390 --> 00:21:07,240
the first speaker is the chairman the head of the department of brain and cognitive

298
00:21:07,240 --> 00:21:09,910
sciences maracas who has

299
00:21:09,970 --> 00:21:15,530
been a fantastic leader in this department for the last five years six years

300
00:21:15,580 --> 00:21:19,690
and he's going to lead off the other speakers for today are

301
00:21:19,830 --> 00:21:24,170
l in and i once in

302
00:21:25,170 --> 00:21:27,920
i think it's been in the news lately because he wrote

303
00:21:27,920 --> 00:21:29,880
the smallest book

304
00:21:29,940 --> 00:21:32,940
was the bible was the bible bible

305
00:21:32,970 --> 00:21:38,620
on the smallest piece of metal known to man has been a new york times

306
00:21:38,620 --> 00:21:40,000
and there's

307
00:21:40,120 --> 00:21:41,970
the things but there's no reason

308
00:21:41,970 --> 00:21:43,730
anyway regardless are

309
00:21:43,800 --> 00:21:48,550
a our leader in brain and cognitive sciences

310
00:21:48,610 --> 00:21:50,750
they're not going to be any exams

311
00:21:50,800 --> 00:21:52,400
i've been promised

312
00:22:55,710 --> 00:23:00,630
drawing on

313
00:23:00,650 --> 00:23:06,230
that is

314
00:23:20,190 --> 00:23:23,350
but have

315
00:23:53,380 --> 00:23:54,530
it is

316
00:24:10,410 --> 00:24:16,470
three you

317
00:24:31,880 --> 00:24:42,630
the reason

318
00:25:47,790 --> 00:25:52,180
the range

319
00:26:07,240 --> 00:26:14,710
he want to bring

320
00:26:14,720 --> 00:26:20,090
there are a few

321
00:26:20,210 --> 00:26:23,890
you read the article

322
00:26:26,660 --> 00:26:36,990
three three

323
00:26:37,040 --> 00:26:44,590
two hundred people

324
00:26:57,110 --> 00:26:59,610
the only way

325
00:27:12,680 --> 00:27:17,160
can do this

326
00:27:17,250 --> 00:27:19,500
this is one of the batteries

327
00:27:19,530 --> 00:27:23,250
we have that it ourselves

328
00:27:23,270 --> 00:27:34,410
one reason why

329
00:27:35,810 --> 00:27:38,230
can better computers

330
00:27:38,290 --> 00:27:39,600
has to do with it

331
00:27:39,660 --> 00:27:41,230
the way they are why

332
00:27:41,230 --> 00:27:47,370
the way that brain cells while themselves into networks and can create nonlinear output from

333
00:27:47,370 --> 00:27:53,350
simple inputs and therefore emergent out parts that are not simply predicted by the courts

334
00:27:53,470 --> 00:27:56,610
there's another reason and you'll have to wait till the end of my talk

335
00:27:56,620 --> 00:27:59,470
the figure out why that has to do with the deep structure of the brain

336
00:27:59,710 --> 00:28:02,330
and cognition so you to stay away

337
00:28:07,680 --> 00:28:10,390
the brain is made up of cells

338
00:28:10,400 --> 00:28:14,220
and there are two ways by which brain cells differ from any other cell in

339
00:28:14,220 --> 00:28:15,530
our body

340
00:28:15,540 --> 00:28:19,920
and one of those ways has to do with the fact that brain cell has

341
00:28:19,920 --> 00:28:22,470
what is called an excitable membrane

342
00:28:22,500 --> 00:28:25,540
a brain cells firing impulses

343
00:28:25,590 --> 00:28:29,100
and electrical spikes impulses are

344
00:28:29,150 --> 00:28:32,830
the language of the brain all information in the brain

345
00:28:32,850 --> 00:28:35,150
it is conveyed the

346
00:28:35,160 --> 00:28:39,460
the interval between spikes there is no information in the height of the spikes but

347
00:28:39,460 --> 00:28:42,230
all information is contained in there

348
00:28:42,240 --> 00:28:44,730
the time interval between spikes with the formal

349
00:28:44,740 --> 00:28:49,490
all scored modulation and the reason brain cells can fire spikes is because they have

350
00:28:49,500 --> 00:28:54,660
what is called a semi permeable membrane that keeps sodium out and keep potassium in

351
00:28:54,840 --> 00:29:00,290
r and and and a brain cell five despite when sodium rushes then

352
00:29:00,350 --> 00:29:02,160
and then put us in russia

353
00:29:02,160 --> 00:29:04,860
and the nobel prize was given for this discovery

354
00:29:05,810 --> 00:29:07,620
hodgkin and huxley in

355
00:29:07,650 --> 00:29:11,420
in the early nineteen sixties for the discovery in the fifteenth of the mechanism for

356
00:29:11,420 --> 00:29:13,290
generating action potentials

357
00:29:15,200 --> 00:29:20,040
the second way by which brain cells differ from any other cell in the body

358
00:29:20,040 --> 00:29:22,890
it has to do with how they connect to each other

359
00:29:22,970 --> 00:29:29,130
and brain cells connect to other cells at specialized junctions called synapses and the sign

360
00:29:29,130 --> 00:29:34,740
up is not of physical contact that still makes it with another cell but it

361
00:29:34,750 --> 00:29:36,960
contact with the gas

362
00:29:37,000 --> 00:29:38,940
and so the brain cells

363
00:29:38,970 --> 00:29:42,770
makes between one thousand ten thousand sign-ups is

364
00:29:42,780 --> 00:29:44,950
with hundreds of others

365
00:29:44,990 --> 00:29:47,120
so if you add it all up

366
00:29:47,180 --> 00:29:52,510
ten to the ten or ten to the eleventh cells hundred billion cells each with

367
00:29:52,510 --> 00:29:54,350
ten to the four sign-ups is

368
00:29:55,580 --> 00:30:00,530
a hundred trillion two thousand trillion synapses

369
00:30:00,580 --> 00:30:05,720
these numbers are the essence of complexity in the brain is because have such large

370
00:30:05,720 --> 00:30:10,110
numbers of the maps and such large numbers of brain cells

371
00:30:10,170 --> 00:30:14,770
you can do things that you cannot predict from a trillion times one hundred trillion

372
00:30:14,770 --> 00:30:20,290
OK so what i was saying by the way is is that was in the

373
00:30:20,290 --> 00:30:25,120
part of the talk that was about the contextual annotation so in that case it

374
00:30:25,120 --> 00:30:28,270
was negative but i like your idea that can amplify

375
00:30:28,290 --> 00:30:30,250
it's an intensive fire

376
00:30:30,580 --> 00:30:32,310
that's nice

377
00:30:32,620 --> 00:30:36,960
OK lexicon development

378
00:30:37,000 --> 00:30:44,520
what who could do it humans do it and that's valuable right

379
00:30:44,890 --> 00:30:50,830
have someone sit down so we can get a lot of knowledge out so o

380
00:30:50,830 --> 00:30:56,040
and what what i mean by mexican i mean the lexicons of words and phrases

381
00:30:56,040 --> 00:30:58,310
in constructions and senses now

382
00:30:58,420 --> 00:31:04,040
that of things that can be used for subjectivity and of course you can refine

383
00:31:04,040 --> 00:31:07,540
that by positive negative sentiment arguing whatever

384
00:31:07,560 --> 00:31:09,500
OK and

385
00:31:09,520 --> 00:31:13,250
especially when maybe not the document level

386
00:31:13,350 --> 00:31:17,100
i analysis because that's more in text categorisation but when you're at the lower level

387
00:31:17,540 --> 00:31:21,290
most groups use some kind of lexicon

388
00:31:21,290 --> 00:31:24,170
so it's an important part of this area

389
00:31:24,210 --> 00:31:28,890
so humans can do it you can have semi-automatic methods and you can have fully

390
00:31:28,980 --> 00:31:31,270
automatic methods

391
00:31:31,290 --> 00:31:36,690
so we want to find relevant words phrases patterns that can be used to express

392
00:31:36,690 --> 00:31:37,770
its activity

393
00:31:37,790 --> 00:31:41,420
we want to determine the polarity of subjective expressions as well as people want to

394
00:31:41,420 --> 00:31:42,920
do that

395
00:31:42,940 --> 00:31:47,020
when i'm not going to get to it all is an interesting other sub area

396
00:31:47,350 --> 00:31:50,770
which is determining domain dependent expressions

397
00:31:50,810 --> 00:31:55,520
was on domain dependent expressions right that hot nightclub is a good thing but i

398
00:31:55,520 --> 00:32:00,330
have concerns about things like just will get to that

399
00:32:02,460 --> 00:32:05,730
i think that kind of moving forward in this work

400
00:32:08,120 --> 00:32:12,790
our kind of the clearest case we and others have found they usually the least

401
00:32:12,790 --> 00:32:16,350
ambiguous the clearest part of speech in english

402
00:32:17,440 --> 00:32:19,310
for subjectivity

403
00:32:19,330 --> 00:32:24,710
so we can have positive adjectives honest important feature large patient

404
00:32:25,640 --> 00:32:31,730
yes it says these i wonder if it is overturned

405
00:32:31,810 --> 00:32:38,810
and then of course negative harmful hypocritical inefficient insecure and chris we want to point

406
00:32:38,810 --> 00:32:43,640
out that you can have subjectivity that's not positive and negative sentiments you can have

407
00:32:43,640 --> 00:32:47,270
things that are serious are likely or probable

408
00:32:47,270 --> 00:32:53,190
that does it you know there's not pull polar by their nature so too as

409
00:32:53,190 --> 00:32:59,560
his probable successor the two speeches species are likely to flower at different times

410
00:32:59,650 --> 00:33:03,960
but people have worked to another part of speech as well and again this is

411
00:33:03,980 --> 00:33:10,020
to sampling there so you can have verbs like positive praise love

412
00:33:10,020 --> 00:33:12,080
the negative blame criticize

413
00:33:12,100 --> 00:33:19,980
something that subjective but neither positive and negative is predict the same thing for nouns

414
00:33:20,120 --> 00:33:22,940
these these two

415
00:33:25,620 --> 00:33:30,340
do phrases containing adjectives and adverbs in that they do others as well but i

416
00:33:30,340 --> 00:33:35,270
was pointing out that that's something that people is that it's really useful thing to

417
00:33:35,270 --> 00:33:40,150
do in english is to look at the faces containing entities another set english but

418
00:33:40,150 --> 00:33:44,620
this is actually about japanese so something like in japanese was was useful as well

419
00:33:44,620 --> 00:33:49,830
so high intelligence low-cost little variation many troubles and that's kind of getting to the

420
00:33:49,830 --> 00:33:52,230
point i was just talking about which is that

421
00:33:52,250 --> 00:33:54,120
you you can build up

422
00:33:54,120 --> 00:34:00,210
collocations or phrases that seems like a promising thing to do

423
00:34:00,230 --> 00:34:08,810
so i mentioning my because i know myself better and i could speak about easier

424
00:34:08,810 --> 00:34:14,750
but there's other people that were kind and lexico syntactic phrases to so

425
00:34:14,770 --> 00:34:20,210
we use extraction patterns representation and these are these are patterns that were developed for

426
00:34:20,210 --> 00:34:22,390
the purpose of information extraction

427
00:34:22,410 --> 00:34:29,420
and then just mixtures of words select scenes and used in particular syntactic ways so

428
00:34:29,460 --> 00:34:32,750
the way with and three NP

429
00:34:32,770 --> 00:34:39,690
you would never put away by itself subjectivity that second because so many instances of

430
00:34:40,500 --> 00:34:41,290
the word way

431
00:34:41,310 --> 00:34:46,250
our objective so it's too ambiguous it just make your system

432
00:34:46,270 --> 00:34:48,690
over predict subjectivity

433
00:34:48,710 --> 00:34:52,020
but if you have a way with NP a lot of those are going to

434
00:34:52,020 --> 00:34:57,890
be subjective everybody signed his first to have its way with

435
00:34:57,890 --> 00:35:02,940
i should tell you the corpus we use is

436
00:35:03,000 --> 00:35:07,890
seven hundred eighty seven different news sources there are english translations of articles from the

437
00:35:07,890 --> 00:35:11,440
world press i don't mean to be taking in china led to really good juicy

438
00:35:11,440 --> 00:35:13,850
examples from the chinese press

439
00:35:13,870 --> 00:35:18,270
but there are very good for us because the US whether the US issues human

440
00:35:18,270 --> 00:35:23,270
the second question was only converges to a local optimum voice

441
00:35:23,340 --> 00:35:30,300
and know we we start here we get here is there any we here or

442
00:35:30,790 --> 00:35:34,420
so in general there is no way to find the global optimum

443
00:35:34,420 --> 00:35:38,320
you can prove that this is computationally hard some sense

444
00:35:38,420 --> 00:35:41,270
that let's to say you can always create a data set for which is huge

445
00:35:41,270 --> 00:35:45,170
amount to find the optimal parameter as much work essentially

446
00:35:45,400 --> 00:35:46,570
parameter space

447
00:35:47,860 --> 00:35:50,520
there are some things you can do so

448
00:35:50,550 --> 00:35:56,940
there's one set of things which is the way to go from this local optima

449
00:35:56,940 --> 00:36:01,360
two very different parts the parameter space that we expect will have like

450
00:36:02,300 --> 00:36:07,190
these are also emerged so essentially what you do is you take two models that

451
00:36:07,190 --> 00:36:11,960
representing sacrifices herself into one and you one

452
00:36:11,980 --> 00:36:13,070
and split

453
00:36:13,070 --> 00:36:14,800
so the total number

454
00:36:14,800 --> 00:36:22,070
the number of clusters but wildly nonlocal movement parameter space and because the intuition these

455
00:36:22,070 --> 00:36:27,030
clusters of you know what to do if split cluster into two

456
00:36:27,050 --> 00:36:30,030
nearby blocks emerge junior by two

457
00:36:30,050 --> 00:36:33,290
the expected value of the parameter space

458
00:36:33,300 --> 00:36:37,790
one the things you can do you can intentionally

459
00:36:37,800 --> 00:36:41,250
construct a model that has a convex lens

460
00:36:41,270 --> 00:36:45,790
and then you can reach a maximum just because it's convex

461
00:36:45,800 --> 00:36:50,890
and sometimes even though that might be less flexible it's work because you know that

462
00:36:50,890 --> 00:36:52,190
you have to worry about

463
00:36:52,960 --> 00:36:55,170
so again trade

464
00:36:55,190 --> 00:37:00,500
but in a more like this there's no way to to resolve loss

465
00:37:00,500 --> 00:37:08,300
in any reasonable amount of time

466
00:37:08,360 --> 00:37:12,420
other than that the

467
00:37:12,440 --> 00:37:18,320
OK i want to move on and talk about continuously variable so

468
00:37:18,770 --> 00:37:22,710
the example i gave you mixture models had a discrete latent variables there was just

469
00:37:30,650 --> 00:37:32,340
so for example the

470
00:37:32,360 --> 00:37:34,290
the rest of the talk today

471
00:37:34,290 --> 00:37:36,480
conditional likelihood

472
00:37:36,500 --> 00:37:39,520
the rest is a convex function as

473
00:37:39,570 --> 00:37:44,690
so there we have to worry about a lot when you're just in time

474
00:37:44,710 --> 00:37:51,550
the rest we knew for data that we transform our labels for that data

475
00:37:51,590 --> 00:37:55,210
the the likelihood function which is convex if just descended

476
00:37:55,230 --> 00:37:59,710
and just like when you get to the that's all

477
00:38:00,840 --> 00:38:05,460
stable up to point in that so

478
00:38:05,480 --> 00:38:07,270
in general

479
00:38:07,290 --> 00:38:08,460
you know

480
00:38:10,590 --> 00:38:13,960
it depends on how you start

481
00:38:13,980 --> 00:38:21,880
in general don't want to model the variables are are convex but there are also

482
00:38:21,880 --> 00:38:22,960
meant that

483
00:38:23,020 --> 00:38:25,440
that's the kind of accent

484
00:38:25,440 --> 00:38:30,380
the model which we can talk about which has

485
00:38:30,670 --> 00:38:31,770
many many many things

486
00:38:31,820 --> 00:38:36,320
there's also a non convex by

487
00:38:37,800 --> 00:38:38,860
a lot

488
00:38:38,860 --> 00:38:43,210
that's easier to do with symmetry don't care so on the next

489
00:38:43,250 --> 00:38:44,750
case two

490
00:38:44,750 --> 00:38:47,340
clusters slot

491
00:38:47,360 --> 00:38:49,880
different parts of the space

492
00:38:50,960 --> 00:38:52,360
it's really

493
00:38:53,960 --> 00:38:57,570
so the likelihood function as these weird signature moves

494
00:38:57,790 --> 00:39:03,270
parameters here with some shape and has an identically she part of parameters based on

495
00:39:03,520 --> 00:39:08,730
the clusters so this better things all the and so

496
00:39:08,750 --> 00:39:15,590
when you analyse mathematically of course there a lot because is to me you here

497
00:39:15,590 --> 00:39:18,130
between symmetries which you end up

498
00:39:18,190 --> 00:39:19,670
so the fact that

499
00:39:19,690 --> 00:39:25,340
there are multiple stable is in itself a cause for concern we really

500
00:39:25,340 --> 00:39:27,250
so the answer experimentally

501
00:39:27,590 --> 00:39:29,480
problems abound to

502
00:39:29,550 --> 00:39:30,840
during the day

503
00:39:45,050 --> 00:39:45,900
right so

504
00:39:46,150 --> 00:39:47,610
simulated annealing

505
00:39:47,610 --> 00:39:53,500
simulated annealing is this unfortunate result from optimisation

506
00:39:53,520 --> 00:39:56,050
on the basis that looks very promising it says

507
00:39:56,050 --> 00:39:59,340
if you do random exploration

508
00:39:59,340 --> 00:40:04,400
and you reduce the amount of randomness gradually over time that with high probability theory

509
00:40:04,610 --> 00:40:05,840
and in

510
00:40:06,020 --> 00:40:08,670
in a lot of work

511
00:40:09,150 --> 00:40:11,380
just as about that

512
00:40:11,400 --> 00:40:13,440
results that

513
00:40:13,440 --> 00:40:15,400
the speed with which need to

514
00:40:15,400 --> 00:40:18,500
use the search is so slow

515
00:40:18,520 --> 00:40:22,710
there you might as well just read the entire universe in search

516
00:40:22,730 --> 00:40:23,960
the french

517
00:40:24,130 --> 00:40:27,300
four so it's kind of like you

518
00:40:27,320 --> 00:40:28,940
the result is interesting

519
00:40:28,980 --> 00:40:31,360
again but it is basically useless

520
00:40:31,380 --> 00:40:34,460
so really there is no serious

521
00:40:38,020 --> 00:40:43,210
then you're much better off with you know with aggressive inference techniques to do approximate

522
00:40:43,210 --> 00:40:44,020
inference and

523
00:40:44,230 --> 00:40:49,880
and you see that the parameter space

524
00:40:53,440 --> 00:40:55,000
it's good

525
00:41:04,650 --> 00:41:07,400
here is a gradient descent

526
00:41:09,230 --> 00:41:12,190
roughly speaking e

527
00:41:12,210 --> 00:41:16,250
t and potentially take a very large space

528
00:41:16,270 --> 00:41:18,500
guaranteeing each time

529
00:41:18,610 --> 00:41:24,670
one so that it seems like we to bring great methods in order to guarantee

530
00:41:24,670 --> 00:41:26,960
you things to take into

531
00:41:28,360 --> 00:41:33,690
and by the you can sometimes

532
00:41:33,770 --> 00:41:35,840
from initial

533
00:41:36,570 --> 00:41:38,800
parameters which is not in the business

534
00:41:39,530 --> 00:41:43,480
you can skip over all is convex

535
00:41:43,500 --> 00:41:46,550
you know the better

536
00:41:48,400 --> 00:41:55,300
however in practice you can be much slower than an impressive green so the intuition

537
00:41:55,300 --> 00:41:57,320
here is this in

538
00:41:58,420 --> 00:42:00,590
a reasonable parameters

539
00:42:00,590 --> 00:42:04,630
you have very little uncertainty variables

540
00:42:04,650 --> 00:42:06,860
in fact

541
00:42:06,880 --> 00:42:12,480
is given to the parameters you still have a huge amount of uncertainty about the

542
00:42:12,500 --> 00:42:14,230
variables that you will be

543
00:42:15,300 --> 00:42:18,440
so in this case for example

544
00:42:18,480 --> 00:42:22,820
what is in this case we expect you to be very fast

545
00:42:22,840 --> 00:42:25,000
because you know the parameters

546
00:42:25,020 --> 00:42:27,530
variables are completely different

547
00:42:30,190 --> 00:42:36,170
however if you have a situation where it was very ambiguous where very hard to

548
00:42:37,480 --> 00:42:40,940
two friends and then you expect him to be very well because it has to

549
00:42:40,940 --> 00:42:47,100
be very conservative about semantic difference was conjugate doesn't care that really doesn't have to

550
00:42:47,100 --> 00:42:51,590
that can only take steps to live search

551
00:42:51,590 --> 00:42:56,700
so there is over there is a relation between the two hundred and ripple height

552
00:42:58,920 --> 00:43:03,160
i mean that

553
00:43:03,200 --> 00:43:04,680
you can get now

554
00:43:04,700 --> 00:43:07,630
you can get involved so so somehow

555
00:43:07,650 --> 00:43:12,030
the there is a relation between this like all this degree and

556
00:43:12,050 --> 00:43:16,130
the the relation between and

557
00:43:16,180 --> 00:43:18,450
number of professions i'm allowing

558
00:43:19,320 --> 00:43:22,050
may get as minor something p

559
00:43:22,160 --> 00:43:24,860
we're trying to do this and

560
00:43:24,880 --> 00:43:26,990
and the delta

561
00:43:31,780 --> 00:43:33,490
this student mine

562
00:43:33,650 --> 00:43:36,320
and that down i think more

563
00:43:36,380 --> 00:43:40,360
closely it was known people did

564
00:43:40,360 --> 00:43:42,380
matlab experiments

565
00:43:42,430 --> 00:43:45,110
to get a graph of this relationship

566
00:43:45,260 --> 00:43:49,010
but the theory is possible here

567
00:43:49,070 --> 00:43:52,800
i just mentioned that of oppenheim sitting here

568
00:43:52,860 --> 00:43:55,570
he would say

569
00:43:55,590 --> 00:43:58,090
he would wait

570
00:43:58,110 --> 00:44:03,990
he would sit with allowances here is that he would not want smaller ripples

571
00:44:03,990 --> 00:44:07,240
in the stopband than in the past

572
00:44:07,260 --> 00:44:10,950
so he would introduce away so so

573
00:44:10,990 --> 00:44:12,110
i will

574
00:44:12,130 --> 00:44:13,740
w oppenheim

575
00:44:13,740 --> 00:44:15,780
four for for weight

576
00:44:15,800 --> 00:44:20,070
that the weights that allows you to unequally

577
00:44:20,090 --> 00:44:22,680
have unequal height ripples in the

578
00:44:22,680 --> 00:44:24,700
as and stuff

579
00:44:24,720 --> 00:44:26,070
but let's take

580
00:44:26,220 --> 00:44:29,430
mean that's that's set up nineteen one

581
00:44:33,130 --> 00:44:37,010
so what was my point

582
00:44:37,010 --> 00:44:41,700
that that these equal ripple filters are the best

583
00:44:41,720 --> 00:44:45,240
optimal error sense

584
00:44:45,260 --> 00:44:47,430
and also that we have to

585
00:44:47,450 --> 00:44:50,470
we have to go to matlab to

586
00:44:50,530 --> 00:44:51,900
so just

587
00:44:52,130 --> 00:44:58,400
so what will matlab output if we if we give matlab that what we want

588
00:44:58,470 --> 00:45:00,570
the coefficients except

589
00:45:00,590 --> 00:45:01,650
that level of

590
00:45:02,630 --> 00:45:04,150
and it'll plot the graph

591
00:45:04,180 --> 00:45:06,130
and it was it was like that

592
00:45:06,150 --> 00:45:08,530
like this

593
00:45:14,450 --> 00:45:20,630
and we we know relation between these so that if we know what

594
00:45:20,680 --> 00:45:24,360
as stop and has been we want we know how the

595
00:45:24,380 --> 00:45:27,650
what do we we have to take to get certain

596
00:45:27,660 --> 00:45:29,010
error reduction

597
00:45:29,010 --> 00:45:31,470
OK that's the a discussion of

598
00:45:31,530 --> 00:45:35,070
filter design

599
00:45:36,820 --> 00:45:40,990
now it's not a final discussion filter

600
00:45:41,090 --> 00:45:46,380
i mean this often oftentimes course could could could ask other things you might like

601
00:45:48,930 --> 00:45:54,450
the UK is so we've got to two choices here l two least squares

602
00:45:54,470 --> 00:45:55,930
and max

603
00:45:55,930 --> 00:45:59,490
but that's not the only two choices we could

604
00:45:59,490 --> 00:46:03,450
we could average between those we could introduce other

605
00:46:03,450 --> 00:46:07,510
error measures all sorts of possibilities

606
00:46:08,780 --> 00:46:11,380
maybe at some point two

607
00:46:11,400 --> 00:46:13,740
go to high pass

608
00:46:13,760 --> 00:46:15,720
let's look at the other extreme

609
00:46:15,740 --> 00:46:19,490
what would be a high pass filter what its graph look like

610
00:46:22,010 --> 00:46:25,900
let's give me an example of a high pass filter let me give me an

611
00:46:25,900 --> 00:46:28,930
example of a high pass filter that kills

612
00:46:28,950 --> 00:46:31,130
zero frequency

613
00:46:31,180 --> 00:46:36,490
so let's let's do a high pass filter and dotted lines

614
00:46:36,510 --> 00:46:39,470
i guess i'm going to look for a filter that

615
00:46:41,070 --> 00:46:44,550
it's of omega

616
00:46:44,570 --> 00:46:49,380
it is probably sign

617
00:46:49,400 --> 00:46:52,240
of all male over two

618
00:46:52,260 --> 00:46:56,430
what what what are or what would be an natural how can i fix this

619
00:46:59,820 --> 00:47:02,990
change one of the side right

620
00:47:02,990 --> 00:47:05,280
in computer vision

621
00:47:05,420 --> 00:47:10,680
literature there are several ways of just breaking down

622
00:47:10,740 --> 00:47:15,530
text pictures and the simplest you can think of is to break it down into

623
00:47:15,540 --> 00:47:21,700
regular grid just like this believer not this this technique is an even more powerful

624
00:47:21,700 --> 00:47:25,880
than some of the fancier ones so we we can break the images down to

625
00:47:25,880 --> 00:47:33,630
great or we can take the fancier technique of so-called interest point detectors where were

626
00:47:33,630 --> 00:47:39,410
we have tools to find the interesting spots

627
00:47:41,430 --> 00:47:46,880
interesting is defined by some kind of metric but in any case interesting spots of

628
00:47:46,880 --> 00:47:50,840
same it so we can break it down into smaller pieces

629
00:47:52,690 --> 00:47:54,410
OK now

630
00:47:54,420 --> 00:48:01,080
let's take one interesting regions or one grid region it doesn't really matter how do

631
00:48:01,080 --> 00:48:07,260
we put the take this image and take this path to break represented as numbers

632
00:48:07,260 --> 00:48:13,030
because eventually we have to tell the computer that with the what what this patch

633
00:48:13,040 --> 00:48:18,920
being so normally people we we tend to do is take this patch normalized probably

634
00:48:18,920 --> 00:48:22,420
according to the intensity or saw and then

635
00:48:22,460 --> 00:48:27,900
and describe it using some kind of space in computer vision we have we have

636
00:48:27,900 --> 00:48:33,760
a very powerful descriptive concept sometimes people just also

637
00:48:33,760 --> 00:48:35,150
use score

638
00:48:35,180 --> 00:48:42,860
wavelets are sometimes people use just pixel intensity value it doesn't matter in any case

639
00:48:43,210 --> 00:48:49,940
all these all these regions are expressed as vectors of numbers so you get thousands

640
00:48:49,940 --> 00:48:50,770
of them

641
00:48:50,780 --> 00:48:51,850
and now

642
00:48:51,860 --> 00:48:53,550
the issue is

643
00:48:53,560 --> 00:48:58,020
OK we we got thousands of these points but we want to have a building

644
00:48:58,020 --> 00:49:05,580
block that that this can be used universally used for all the images so

645
00:49:05,590 --> 00:49:07,760
the simplest way to do it is

646
00:49:07,770 --> 00:49:10,690
to cluster them and then we have very

647
00:49:10,750 --> 00:49:12,920
oh sorry

648
00:49:22,460 --> 00:49:30,240
what you are talking about very the second thing is very top-down attention what if

649
00:49:30,250 --> 00:49:36,110
you look at the human visual pathway we start basically from the primary visual cortex

650
00:49:36,110 --> 00:49:43,760
v one and v one receptors are very similar to bars oriented bars and edges

651
00:49:43,760 --> 00:49:46,430
and then images

652
00:49:46,450 --> 00:49:52,350
comes into our human visual system is sets of these oriented bars and edges

653
00:49:52,430 --> 00:49:58,570
and then starting building on top of the the one we have layers clean these

654
00:49:58,570 --> 00:50:02,970
bars and edges and making more complex

655
00:50:02,970 --> 00:50:09,030
receptive field so this level what what computer vision is doing

656
00:50:09,030 --> 00:50:11,390
by detecting these

657
00:50:11,550 --> 00:50:20,300
busier or interesting textual patches we can also approximated to to the v four or

658
00:50:20,300 --> 00:50:25,890
inferior temporal cortex of the human visual system which is about two or three layers

659
00:50:25,950 --> 00:50:31,030
up from the ones so that in terms of seconding

660
00:50:31,070 --> 00:50:34,280
that's all already be on feature is

661
00:50:34,300 --> 00:50:36,990
it's guided by m

662
00:50:37,010 --> 00:50:40,030
it's a

663
00:50:40,050 --> 00:50:45,050
i mean it's not in this domain already it's more data by saliency and other

664
00:50:48,340 --> 00:50:51,780
some context on the bottom up saliency

665
00:50:52,180 --> 00:50:56,050
so that's a whole different lecture and

666
00:50:56,140 --> 00:50:58,550
OK so we got thousands of these

667
00:50:58,780 --> 00:50:59,990
that is

668
00:51:00,010 --> 00:51:04,280
patches and we want to make it dictionary out of it so that we have

669
00:51:04,280 --> 00:51:06,550
a building block and

670
00:51:06,590 --> 00:51:09,340
vector quantization or

671
00:51:09,390 --> 00:51:14,700
some kind of clustering algorithm would do it here we're just showing many papers so

672
00:51:14,700 --> 00:51:20,180
far just using k means clustering with some kind of smart pruning and so on

673
00:51:20,240 --> 00:51:26,410
so so what we end up with this as an example is pulling together

674
00:51:26,430 --> 00:51:31,660
tens of thousands of millions of patches and ending up with let's say three hundred

675
00:51:31,660 --> 00:51:37,470
and and so here is how the scheme works very roughly i'm going to

676
00:51:37,470 --> 00:51:40,530
just look at this in a very high level

677
00:51:41,570 --> 00:51:45,270
you start with a bunch of data points and then you build the neighborhood graph

678
00:51:45,270 --> 00:51:49,270
on them so you have a node for each data point

679
00:51:49,280 --> 00:51:53,140
and then you put an edge between points that are close together

680
00:51:53,160 --> 00:51:56,470
OK so for instance within some radius

681
00:51:56,470 --> 00:52:00,750
or points that are k nearest neighbors of each other or sometimes just put an

682
00:52:00,750 --> 00:52:05,170
edge between every two points and you wait the edges according to distance there are

683
00:52:05,170 --> 00:52:06,850
there are several ways to do this

684
00:52:06,870 --> 00:52:10,410
OK but you do this you build neighborhood it build neighborhood graph

685
00:52:10,430 --> 00:52:13,820
and the new query some points eleventh in this case let's just say we have

686
00:52:13,820 --> 00:52:15,390
a binary problem

687
00:52:15,410 --> 00:52:18,370
it's very some important thing we query these two points that turns out to be

688
00:52:18,390 --> 00:52:22,230
zero this one over here turns out to be one

689
00:52:23,600 --> 00:52:25,970
you propagate labels

690
00:52:28,150 --> 00:52:31,810
you look at the data you have so far i propagated to the rest of

691
00:52:31,810 --> 00:52:35,220
the graph and it turns out that there are some really nice ways to do

692
00:52:35,220 --> 00:52:39,870
this basically by solving an i can value problem and one way to think about

693
00:52:39,870 --> 00:52:44,600
it is to think about the semantics of this number is suppose you started this

694
00:52:44,600 --> 00:52:46,970
node i needed a random walk

695
00:52:47,040 --> 00:52:52,100
what fraction of time which you first have to one what fraction of time which

696
00:52:52,200 --> 00:52:56,790
first it is zero OK so you can think of the semantics of these numbers

697
00:52:56,800 --> 00:52:58,630
in that form roughly

698
00:53:00,670 --> 00:53:04,730
once you have these you can use this as the basis for deciding which point

699
00:53:04,730 --> 00:53:07,730
to query and so for instance one thing you might want to do is to

700
00:53:07,730 --> 00:53:09,970
query the most uncertain point which

701
00:53:09,970 --> 00:53:15,590
which would be this point five but then this seems like to point that a

702
00:53:15,590 --> 00:53:18,930
little bit of an outlier is kind of sticking out from the graph

703
00:53:19,100 --> 00:53:21,060
so it might be better to

704
00:53:21,070 --> 00:53:22,820
to query this because

705
00:53:22,840 --> 00:53:24,120
it's likely

706
00:53:24,130 --> 00:53:29,310
two lead to the biggest reduction in overall uncertainty and one can put in nice

707
00:53:29,320 --> 00:53:36,430
probabilistic semantics on top of these kinds of graphs in which all these all these

708
00:53:36,430 --> 00:53:43,070
analyses are very meaningful and you can actually quantify the amount of residual uncertainty after

709
00:53:43,070 --> 00:53:46,880
finding a label can superior point and then you go back

710
00:53:48,850 --> 00:53:53,660
so why is this in a sense cluster based active learning

711
00:53:55,500 --> 00:54:00,160
if you have a clearly defined clusters in the data

712
00:54:00,220 --> 00:54:06,620
you might have the clusters might be something like these three and the remaining points

713
00:54:06,650 --> 00:54:11,790
basically when the clearly defined clusters and the corresponding nodes will have cut between them

714
00:54:11,820 --> 00:54:18,370
through which which don't have too many edges and through which influence can propagate only

715
00:54:19,280 --> 00:54:22,720
OK so this is an example of

716
00:54:22,730 --> 00:54:30,330
very beautiful scheme for doing active learning that that seems to be motivated in part

717
00:54:30,330 --> 00:54:33,950
by a cluster learning but i sort of a cluster model

718
00:54:33,970 --> 00:54:36,720
and so

719
00:54:36,740 --> 00:54:43,140
i'm not i'm not clear on what the what the consistency

720
00:54:43,160 --> 00:54:47,930
status of this is you know what one can say about statistical consistency

721
00:54:47,930 --> 00:54:54,120
but i can tell you that but but but i'll tell you briefly about some

722
00:54:54,120 --> 00:54:58,830
work that basically uses this is motivation to come up with a different scheme in

723
00:54:58,830 --> 00:55:04,010
which you can actually prove things about consistency and that give some idea of the

724
00:55:04,010 --> 00:55:06,120
kind of analysis that need to be done

725
00:55:06,140 --> 00:55:08,970
OK so here's

726
00:55:08,970 --> 00:55:12,080
here's another cluster based

727
00:55:12,180 --> 00:55:16,390
which attempts to use some of the same intuitions from the previous one

728
00:55:16,410 --> 00:55:22,390
while while achieving consistency and so the idea is to look at the data

729
00:55:22,390 --> 00:55:27,030
you find a bunch of clusters you sample a few points in each cluster

730
00:55:27,080 --> 00:55:30,680
and then you assign each cluster its majority label

731
00:55:30,760 --> 00:55:33,830
and you use that label set to build a classifier

732
00:55:33,850 --> 00:55:34,910
so that

733
00:55:34,910 --> 00:55:38,180
kind of a high level of what's going to be done but let's look at

734
00:55:38,180 --> 00:55:39,240
the reality

735
00:55:39,260 --> 00:55:43,080
OK to start with some unlabelled data

736
00:55:43,100 --> 00:55:46,780
you find clusters

737
00:55:46,780 --> 00:55:48,830
you also some labelled

738
00:55:48,850 --> 00:55:53,490
o point in this cluster is looking pretty good because they feel

739
00:55:53,510 --> 00:55:56,470
it's relatively pure you have

740
00:55:56,490 --> 00:56:00,930
six points you've chosen six random points and that in this cluster that gives you

741
00:56:01,180 --> 00:56:05,450
a certain confidence interval on on how pure this cluster is

742
00:56:05,450 --> 00:56:09,300
and because it's a random sample you can use you can come up with this

743
00:56:09,300 --> 00:56:12,350
sort of confidence intervals so feeling good about this one

744
00:56:12,370 --> 00:56:14,680
this one is looking like trouble

745
00:56:14,700 --> 00:56:17,120
OK so what you do that

746
00:56:17,140 --> 00:56:18,080
is that you

747
00:56:18,120 --> 00:56:21,200
then we find the clustering

748
00:56:26,930 --> 00:56:31,070
so the the overall overall a nice way to do this is to just use

749
00:56:31,070 --> 00:56:32,760
a hierarchical clustering

750
00:56:32,760 --> 00:56:37,100
indicator variables between zero and minus one in each case no matter how the interest

751
00:56:37,100 --> 00:56:41,250
comes out exactly one of them will be one all the others will be zero

752
00:56:41,270 --> 00:56:42,940
so now we can

753
00:56:42,940 --> 00:56:45,490
divide out the running time

754
00:56:45,570 --> 00:56:48,710
of this algorithm based in which case where

755
00:56:48,750 --> 00:56:59,750
that sort of unified this

756
00:56:59,800 --> 00:57:01,570
intuition that we did

757
00:57:01,580 --> 00:57:07,050
and get all the cases and then we can look at the expectation

758
00:57:08,370 --> 00:57:09,970
so to

759
00:57:10,120 --> 00:57:16,680
in if we just let out by cases

760
00:57:16,810 --> 00:57:19,390
we have

761
00:57:19,500 --> 00:57:20,900
upper bound

762
00:57:20,920 --> 00:57:24,740
like this

763
00:57:31,680 --> 00:57:41,030
so if we have a zero ten minus one split the worst is we have

764
00:57:41,060 --> 00:57:45,190
n minus one we have to recursion problem size n minus one

765
00:57:45,200 --> 00:57:48,710
in fact pretty hard recursion from the size zero

766
00:57:48,720 --> 00:57:56,040
if we have one to n minus two split then we take the mass of

767
00:57:56,040 --> 00:57:57,460
the two sides that's the

768
00:57:57,490 --> 00:58:00,400
certainly going to give us an upper bound

769
00:58:00,510 --> 00:58:08,200
and so on

770
00:58:10,040 --> 00:58:15,550
at the bottom and minus one to zero

771
00:58:19,950 --> 00:58:26,450
so this is now sort of conditioning on various events but we have run an

772
00:58:26,510 --> 00:58:30,800
indicator random variables to tell us when these events happened so we can just multiply

773
00:58:30,800 --> 00:58:31,760
each of these

774
00:58:31,950 --> 00:58:35,950
values by the indicator random variable and will come and zero if that's not the

775
00:58:35,950 --> 00:58:39,380
case come out one and you give us the value of that happens to be

776
00:58:39,490 --> 00:58:42,020
split so if we add up all those

777
00:58:42,100 --> 00:58:43,450
will get the same thing

778
00:58:43,530 --> 00:58:47,570
this is equal to

779
00:58:47,830 --> 00:58:55,680
the sum over all k of indicator variable times

780
00:58:55,690 --> 00:58:59,960
the cost in that case which is max OK

781
00:58:59,990 --> 00:59:03,470
and the other side dishes and minus k minus one

782
00:59:03,620 --> 00:59:10,030
plus there

783
00:59:10,180 --> 00:59:13,880
OK so this is our

784
00:59:13,880 --> 00:59:19,510
recurrence in some sense for the random variable representing running time that depends on

785
00:59:19,530 --> 00:59:21,720
i mean the value will depend in which case

786
00:59:21,720 --> 00:59:23,110
we come into

787
00:59:23,120 --> 00:59:27,160
we know the probability of each of these events happening is the same

788
00:59:27,170 --> 00:59:28,410
because we're choosing

789
00:59:28,420 --> 00:59:31,540
the partition element uniformly random

790
00:59:31,610 --> 00:59:34,420
but we can simplify much beyond this

791
00:59:34,750 --> 00:59:36,990
until we take expectations

792
00:59:37,000 --> 00:59:40,620
we know this random variable could be as big as n squares

793
00:59:40,660 --> 00:59:43,260
hopefully it's usually linear

794
00:59:43,280 --> 00:59:45,690
will take expectations of both sides

795
00:59:45,700 --> 00:59:46,940
and get

796
00:59:46,980 --> 00:59:49,600
what we want

797
01:00:01,460 --> 01:00:04,260
so let's look at

798
01:00:04,270 --> 01:00:07,950
the expectation of this random variable

799
01:00:07,950 --> 01:00:11,430
which is the expectation of copy of

800
01:00:11,450 --> 01:00:13,300
summation we have here

801
01:00:13,320 --> 01:00:16,940
so i can work on this board

802
01:00:38,010 --> 01:00:41,460
so i want to compute the expectation of the summation

803
01:00:42,220 --> 01:00:44,870
property and expectations should i use

804
01:00:44,880 --> 01:00:47,320
linearity good

805
01:00:47,350 --> 01:00:49,830
so we can bring the summation outside

806
01:00:58,620 --> 01:01:21,850
so now i have some of expectations so let's look at each expectation individually it's

807
01:01:21,850 --> 01:01:23,670
the product of two

808
01:01:23,680 --> 01:01:25,230
random variables

809
01:01:26,460 --> 01:01:28,310
this is an indicator random variable

810
01:01:28,320 --> 01:01:29,510
and this is

811
01:01:29,580 --> 01:01:31,720
some more complicated function

812
01:01:31,740 --> 01:01:33,670
more complicated random variable

813
01:01:33,680 --> 01:01:35,690
the set representing some running time

814
01:01:35,700 --> 01:01:41,160
which depends on what random choices are made in the recursive call

815
01:01:41,180 --> 01:01:48,310
now which should i do

816
01:01:48,360 --> 01:01:52,030
expectation of the product of two random variables

817
01:01:52,060 --> 01:01:55,730
independence exactly

818
01:01:55,760 --> 01:01:59,660
if i find that these two random variables are independent then i know that the

819
01:01:59,660 --> 01:02:02,720
expectation the product is the product of the expectations

820
01:02:02,760 --> 01:02:08,140
now we have to cheque are they independent

821
01:02:08,140 --> 01:02:08,990
i hope so

822
01:02:09,010 --> 01:02:11,650
because otherwise there's much else i can there

823
01:02:11,660 --> 01:02:14,050
so why are they in

824
01:02:15,980 --> 01:02:20,210
because we stated that they are because of

825
01:02:20,220 --> 01:02:21,440
this assumption

826
01:02:21,450 --> 01:02:24,720
the reason that all the random numbers are chosen independently

827
01:02:25,510 --> 01:02:27,680
we need to sort interpolate that here

828
01:02:27,680 --> 01:02:29,150
these xk is

829
01:02:29,160 --> 01:02:32,420
all the xk is x zero up to x and minus one all the ones

830
01:02:32,420 --> 01:02:37,360
appearing in the summation are dependent on a single random choice

831
01:02:37,370 --> 01:02:41,370
of this particular call to random partition

832
01:02:41,370 --> 01:02:46,350
evans that or for which the intersection is empty then

833
01:02:46,370 --> 01:02:50,770
the probability of the union of these and even is to discern the sum of

834
01:02:50,770 --> 01:02:54,050
the probabilities can

835
01:02:54,130 --> 01:02:55,420
and the

836
01:02:55,620 --> 01:03:00,120
you can also see that

837
01:03:00,130 --> 01:03:02,130
the probability of the

838
01:03:02,160 --> 01:03:08,510
union press the probability of the intersection is the sum of the two probabilities

839
01:03:08,620 --> 01:03:18,670
and we'll to to measure and study these random events will be able to be

840
01:03:18,670 --> 01:03:23,690
able to see them or to measure them through random variables so

841
01:03:23,690 --> 01:03:29,560
for your kindness and then you define the random variable x

842
01:03:29,570 --> 01:03:34,690
goes from meager two

843
01:03:34,690 --> 01:03:37,810
for example two zero one

844
01:03:37,870 --> 01:03:42,930
and we say that x is equal x of media

845
01:03:43,020 --> 01:03:46,110
is equal to zero for example when

846
01:03:46,120 --> 01:03:54,570
and the result of the time because it is it is that he is

847
01:03:54,580 --> 01:04:01,420
and acts of is equal to one and the result is as

848
01:04:12,690 --> 01:04:18,570
the proof the the reason we use a random variable is to transport the probability

849
01:04:18,660 --> 01:04:21,640
from your space of random you can still me again

850
01:04:21,670 --> 01:04:25,600
something that is measurable for example think of

851
01:04:25,620 --> 01:04:28,560
capital being our

852
01:04:28,560 --> 01:04:33,440
for example so random variables on omega two r

853
01:04:33,460 --> 01:04:38,690
we transport measurable sets seen in two measurable set in rd

854
01:04:38,700 --> 01:04:39,290
that's what

855
01:04:39,330 --> 01:04:41,700
basically what's written there

856
01:04:47,560 --> 01:04:51,810
image of your random events so

857
01:04:51,870 --> 01:04:58,560
actually extra random variable may take values in different kind of said

858
01:04:58,560 --> 01:05:02,620
i said for example

859
01:05:02,620 --> 01:05:06,250
x maybe maybe taking values in rd

860
01:05:07,060 --> 01:05:11,350
are you know already said that would be difficult i there may have so it

861
01:05:11,350 --> 01:05:16,230
may have one or several viable but it could also take values in in in

862
01:05:16,260 --> 01:05:19,910
finite set such says you one then that would be

863
01:05:20,750 --> 01:05:22,710
the discrete

864
01:05:22,730 --> 01:05:24,500
random valuable

865
01:05:27,360 --> 01:05:28,940
it may take

866
01:05:28,960 --> 01:05:34,830
here so we basically making the distinction between

867
01:05:34,880 --> 01:05:42,930
the discrete and the continuous firewall so these three firewalls discrete random variables

868
01:05:42,940 --> 01:05:48,800
take values in the discrete set one continuous random variables may take values in rd

869
01:05:48,810 --> 01:05:49,980
for example

870
01:05:53,190 --> 01:06:01,190
eurandom firewall is actually several coordinates would call that the random vector

871
01:06:03,360 --> 01:06:05,240
so i used

872
01:06:05,440 --> 01:06:08,190
it is the notation px

873
01:06:08,220 --> 01:06:15,300
when i look at events that are in the measure in the measure

874
01:06:15,310 --> 01:06:22,660
we which are measurable quantity of observable quantities and and p is the probability in

875
01:06:23,270 --> 01:06:36,100
initial random set of media

876
01:06:36,170 --> 01:06:39,280
again if you consider again the the

877
01:06:39,290 --> 01:06:40,700
a coin toss

878
01:06:40,720 --> 01:06:46,270
we the media with x that goes from all media you one

879
01:06:46,760 --> 01:06:50,190
we write

880
01:06:50,420 --> 01:06:54,590
a small p as the probability of x being ads

881
01:06:54,600 --> 01:06:56,190
are being one

882
01:06:56,200 --> 01:07:02,340
and the random variable that you obtain here from observing single going to this is

883
01:07:02,340 --> 01:07:04,730
called a bernoulli variables

884
01:07:04,780 --> 01:07:07,310
with the parameter p

885
01:07:07,430 --> 01:07:10,020
and it's it's

886
01:07:10,030 --> 01:07:16,180
the law of the random variables with the way it either axes only

887
01:07:16,810 --> 01:07:19,540
is defined only by p

888
01:07:19,580 --> 01:07:21,260
if you have several

889
01:07:21,270 --> 01:07:28,080
and independent going to tosses if you if you observe an independent going to

890
01:07:29,230 --> 01:07:33,780
let's say that y is the random variable you observe why is this sum the

891
01:07:35,100 --> 01:07:40,330
then the probability of life being k exactly this one

892
01:07:40,350 --> 01:07:47,710
where this is the binomial coefficient for kitchens and

893
01:07:47,790 --> 01:07:51,040
and so this will decent useful

894
01:07:51,060 --> 01:08:01,270
random viable is said to have a binomial distribution with parameters n and p

895
01:08:05,610 --> 01:08:13,810
so if you want to

896
01:08:13,850 --> 01:08:18,020
OK now if you have a measurable random variable the new me

897
01:08:18,040 --> 01:08:21,590
i wonder what is the mean it's me its expectation

898
01:08:21,610 --> 01:08:27,130
it is defined as above on the set omega of random events

899
01:08:27,270 --> 01:08:33,350
and so if x is discrete takes values in into to a discrete set x

900
01:08:33,350 --> 01:08:39,070
one to x and then the expectation is the sum of the x IPI where

901
01:08:39,080 --> 01:08:45,910
p a is the probability of x being excited intermediary can define the which is

902
01:08:46,300 --> 01:08:48,210
the deviation of the random variable

903
01:08:48,520 --> 01:08:50,290
two from its mean

904
01:08:50,300 --> 01:08:57,660
so it's the expectation of x minus its eggs

905
01:08:57,670 --> 01:09:01,730
i'm sorry the expectation switch from this similar to

906
01:09:01,760 --> 01:09:06,130
this implies that is it is it is it is exactly exactly the same thing

907
01:09:08,710 --> 01:09:14,730
the variance is the expectation of x minus think is its expectation square then you

908
01:09:14,730 --> 01:09:15,780
can show

909
01:09:15,810 --> 01:09:21,310
actually that is equivalent to this definition here which may be more easy to use

910
01:09:21,330 --> 01:09:26,980
in any case if x is that is a discrete random variable you find that

911
01:09:26,980 --> 01:09:35,800
the valiant is has this form there

912
01:09:40,280 --> 01:09:47,610
modern or you can defend the expectation of any function of your

913
01:09:47,610 --> 01:09:51,090
OK maybe i just have to try to try to address draw this by hand

914
01:09:51,090 --> 01:09:55,860
it's not completely impossible

915
01:09:55,880 --> 01:09:58,700
because well it would take a long time to try to get the new version

916
01:09:58,700 --> 01:10:03,270
of this in another version of these slides i'm afraid

917
01:10:37,100 --> 01:10:48,500
OK so the point here is that

918
01:10:48,510 --> 01:10:52,920
we have two independent components with uniform distributions

919
01:10:52,940 --> 01:10:56,830
inside certain intervals so let's say

920
01:10:56,830 --> 01:10:57,890
from here to here

921
01:10:58,090 --> 01:11:01,400
the mean is zero so it has to be something like that and because the

922
01:11:01,450 --> 01:11:07,790
independent the independent from each other then the scatterplot is simply that kind of square

923
01:11:07,850 --> 01:11:11,250
so this is the edges of the square simple but so we will have like

924
01:11:11,270 --> 01:11:15,760
points inside that square everything quite uniform

925
01:11:15,770 --> 01:11:19,220
so this is the distribution of s s one

926
01:11:19,270 --> 01:11:21,610
and it's two

927
01:11:21,640 --> 01:11:26,260
so this is this is the latent variables that we don't observe what we is

928
01:11:26,680 --> 01:11:28,320
x one

929
01:11:28,330 --> 01:11:30,150
and x two

930
01:11:30,200 --> 01:11:31,970
o scatterplot is now

931
01:11:31,970 --> 01:11:39,490
some kind of really distorted vision of this thing something like for example like this

932
01:11:39,870 --> 01:11:42,360
with a uniform distribution inside

933
01:11:44,250 --> 01:11:46,000
geometric form

934
01:11:46,010 --> 01:11:48,790
so the point is

935
01:11:48,860 --> 01:11:52,600
well first of all from this kind of the distributions what you can see

936
01:11:53,160 --> 01:11:56,000
is you can see that

937
01:11:57,570 --> 01:12:01,860
you could possibly estimates that kind of a distribution as the one who could possibly

938
01:12:01,860 --> 01:12:05,030
estimate the independent components because obviously

939
01:12:05,040 --> 01:12:10,360
but the directions of these edges here of this distribution have something to do

940
01:12:10,390 --> 01:12:13,070
with the coefficients in the mixing matrix

941
01:12:13,220 --> 01:12:18,990
actually the directions of these edges will be something like the

942
01:12:19,000 --> 01:12:26,500
will be almost the same thing as being the columns of the mixing matrix shows

943
01:12:26,790 --> 01:12:29,850
so now what happens is that when we do i see what the story when

944
01:12:29,850 --> 01:12:31,150
we do what

945
01:12:31,160 --> 01:12:32,490
so let's

946
01:12:32,510 --> 01:12:36,860
how do know what i did i don't seem to be not widening anyway so

947
01:12:37,330 --> 01:12:39,730
let's say xt lies previous

948
01:12:39,820 --> 01:12:45,340
so when we do the right thing what you see is that we get the

949
01:12:45,340 --> 01:12:48,410
same kind of square as in the beginning

950
01:12:49,240 --> 01:12:54,160
it will not it will be

951
01:12:54,650 --> 01:12:57,630
in some direction i don't know to do this the right way but in any

952
01:12:57,630 --> 01:13:00,230
case you will be rotated somehow

953
01:13:00,420 --> 01:13:06,680
so we will not have a uniform distribution inside this kind of this way

954
01:13:10,330 --> 01:13:12,670
this illustrates just the fact that

955
01:13:12,690 --> 01:13:16,050
after whitening the mixing matrices is also going

956
01:13:17,280 --> 01:13:22,620
what was going on with an orthogonal transformation is basically is a rotation in space

957
01:13:22,990 --> 01:13:26,600
in the case of two dimensions well there is only you know one way or

958
01:13:26,600 --> 01:13:32,370
rotating a square that is you know rotating by this centre axis so we will

959
01:13:32,370 --> 01:13:38,380
know that after we had done whitening we basically have only one parameter left so

960
01:13:38,380 --> 01:13:42,050
in the beginning we had four parameters left because we have two times two mixing

961
01:13:42,050 --> 01:13:48,090
matrix but after whitening we actually only one parameter left which is the rotation angle

962
01:13:48,090 --> 01:13:49,400
in this direction

963
01:13:49,430 --> 01:13:53,430
so we can see that we see that whitening order correlation is quite a useful

964
01:13:53,430 --> 01:13:54,310
thing to do

965
01:13:54,310 --> 01:14:00,130
but certainly doesn't solve the problem because this we with this orientation here will be

966
01:14:00,130 --> 01:14:02,810
completely arbitrary and it will not be

967
01:14:02,830 --> 01:14:04,740
what these

968
01:14:04,770 --> 01:14:08,810
basic orientation that we wanted to have in the first place

969
01:14:11,180 --> 01:14:15,390
so this should be compared with with a cake in the case of of gaussian

970
01:14:16,700 --> 01:14:18,580
so i got some

971
01:14:18,620 --> 01:14:23,630
so as i wrote previously was the PDF was

972
01:14:23,690 --> 01:14:25,860
PDF was basically

973
01:14:25,870 --> 01:14:30,140
proportional to the exponential of

974
01:14:30,150 --> 01:14:31,170
well it's a

975
01:14:31,220 --> 01:14:33,410
well it's xtrans posts

976
01:14:34,110 --> 01:14:36,820
inverse space x so now

977
01:14:36,830 --> 01:14:40,780
if we have in the data then c will be identity and c minus the

978
01:14:40,790 --> 01:14:45,860
inverse will be identity so what this will be is simply a function of the

979
01:14:45,860 --> 01:14:47,070
norm of x

980
01:14:47,070 --> 01:14:48,950
so the contour plots

981
01:14:48,980 --> 01:14:49,900
for this

982
01:14:49,950 --> 01:14:53,370
distribution will be simply spherical so

983
01:14:53,390 --> 01:14:55,290
you can call it spherically symmetric

984
01:14:55,290 --> 01:14:59,790
this kind of distribution if you like so from this distribution you would say that

985
01:14:59,790 --> 01:15:05,240
well there is basically no way of determining what is what is the right orientation

986
01:15:05,240 --> 01:15:06,850
for training data

987
01:15:09,930 --> 01:15:11,280
in this case

988
01:15:11,280 --> 01:15:16,310
if we want really the preimage problem if we look at the

989
01:15:16,370 --> 01:15:18,220
his next mission

990
01:15:18,240 --> 01:15:23,010
you have to make some new version we see here as is this function we

991
01:15:23,010 --> 01:15:26,530
see that we are not going to sicily

992
01:15:26,530 --> 01:15:31,010
so this is just a reminder to to tell you that also the competition

993
01:15:31,060 --> 01:15:32,510
can be done

994
01:15:33,430 --> 01:15:36,100
OK six using the canon creek as usual

995
01:15:36,140 --> 01:15:40,200
that's in the space

996
01:15:40,220 --> 01:15:45,080
and there was this problem of prem of squeamish program so i'm just doing just

997
01:15:45,080 --> 01:15:49,560
to what about it because in fact you know problem of network completion

998
01:15:49,720 --> 01:15:52,330
we are not using the preimage

999
01:15:52,350 --> 01:15:58,790
we are still working on the future space and we are going to make this

1000
01:15:59,140 --> 01:16:01,100
inner product of output

1001
01:16:01,120 --> 01:16:03,410
so so

1002
01:16:03,430 --> 01:16:08,680
and known moments will be you really need to make a free image problem but

1003
01:16:08,680 --> 01:16:13,640
they just give you the answer because it can be useful

1004
01:16:13,660 --> 01:16:16,850
so in fact what we want to do

1005
01:16:16,870 --> 01:16:21,140
we have dozens of work in the future not s

1006
01:16:21,260 --> 01:16:26,640
and we deal with the outputs as they chose this feature map

1007
01:16:26,660 --> 01:16:29,100
but always using canada

1008
01:16:29,120 --> 01:16:30,100
can value

1009
01:16:30,120 --> 01:16:33,550
and now you want for instance for one

1010
01:16:33,550 --> 01:16:34,370
that actual

1011
01:16:34,390 --> 01:16:36,060
o point blue here

1012
01:16:36,060 --> 01:16:37,970
we want to

1013
01:16:37,990 --> 01:16:39,530
guest predict

1014
01:16:43,260 --> 01:16:47,470
in the output space always in a good place the first

1015
01:16:47,530 --> 01:16:48,600
open space

1016
01:16:50,350 --> 01:16:57,580
we propose a very simple and general methods that can be in fact improve these

1017
01:16:57,780 --> 01:17:03,510
if you can is well adapted but this is what is the most general solution

1018
01:17:03,510 --> 01:17:05,120
that we can have

1019
01:17:05,160 --> 01:17:07,180
we are going to say well

1020
01:17:07,410 --> 01:17:09,870
we are looking for we are going to look

1021
01:17:09,890 --> 01:17:11,220
and the set

1022
01:17:11,280 --> 01:17:17,870
of training to the set of output and the the way to solve the preimage

1023
01:17:17,930 --> 01:17:19,680
is is to find out

1024
01:17:19,680 --> 01:17:22,180
so why you apply

1025
01:17:22,220 --> 01:17:27,450
which is the closest to the true solution to this one motion so it is

1026
01:17:27,450 --> 01:17:28,640
the feature map

1027
01:17:28,680 --> 01:17:34,990
and the idea is to find the best ones so said quite how the other

1028
01:17:34,990 --> 01:17:37,080
day you usually why is huge

1029
01:17:37,100 --> 01:17:42,180
can i can do computations so what we're going to do is we're going just

1030
01:17:42,180 --> 01:17:43,200
to use the

1031
01:17:43,220 --> 01:17:46,600
how the training data which is of course

1032
01:17:46,600 --> 01:17:51,890
the construction and we are going to say well i'm going to set

1033
01:17:51,910 --> 01:17:54,510
to find the y

1034
01:17:54,530 --> 01:17:56,600
prime such that

1035
01:17:56,620 --> 01:18:01,200
it is the closest in terms of of can then

1036
01:18:01,310 --> 01:18:03,890
OK so in terms

1037
01:18:04,140 --> 01:18:10,120
five of why prime using the euclidean distance in in this space

1038
01:18:10,550 --> 01:18:11,930
i'm going to see

1039
01:18:12,990 --> 01:18:15,260
which white from either close this

1040
01:18:15,310 --> 01:18:16,990
was my help

1041
01:18:17,010 --> 01:18:19,910
OK remember the output of the regression trees

1042
01:18:19,930 --> 01:18:21,680
it just as which

1043
01:18:21,780 --> 01:18:24,100
the output falling leaf

1044
01:18:24,120 --> 01:18:26,100
OK so as i have

1045
01:18:26,120 --> 01:18:29,450
why if we were to say mass

1046
01:18:30,700 --> 01:18:35,390
all this competition again can be done we can

1047
01:18:36,120 --> 01:18:39,660
what you can see it at this stage it surrounds approximation

1048
01:18:39,680 --> 01:18:41,680
it seems to to be too rough

1049
01:18:41,720 --> 01:18:46,220
but in fact i think that the tree will be used in some combination

1050
01:18:46,220 --> 01:18:47,930
it's possible to make sense

1051
01:18:47,950 --> 01:18:50,660
and then we have the better

1052
01:18:50,660 --> 01:18:58,450
bit accuracy because the responses would be combined with those instances

1053
01:18:58,470 --> 01:19:02,870
so this is strictly a single tree the output kernel trees

1054
01:19:02,890 --> 01:19:05,530
and so we could this

1055
01:19:05,550 --> 01:19:08,200
OK three

1056
01:19:08,790 --> 01:19:11,760
by regions of the one of the first

1057
01:19:12,140 --> 01:19:15,740
treaty proposed by queen anne

1058
01:19:16,180 --> 01:19:22,430
eighty six which is called the index decision tree

1059
01:19:22,450 --> 01:19:23,260
and now

1060
01:19:23,280 --> 01:19:26,310
i'm going to tell you short world

1061
01:19:26,330 --> 01:19:29,310
about ensemble methods so

1062
01:19:29,330 --> 01:19:33,470
i have let's say fifteen minutes again you

1063
01:19:36,310 --> 01:19:37,870
so four b we

1064
01:19:37,870 --> 01:19:41,780
we began at a quarter past nine

1065
01:19:41,810 --> 01:19:44,530
so they tell me that it was possible

1066
01:19:44,560 --> 01:19:47,310
OK so

1067
01:19:47,370 --> 01:19:49,850
OK so women

1068
01:19:49,870 --> 01:19:55,280
it's that we want to be in this function

1069
01:19:57,330 --> 01:20:01,120
the output can entry is one way to be such a function

1070
01:20:01,160 --> 01:20:03,310
OK it's quite simple way

1071
01:20:03,310 --> 01:20:04,720
it's quite

1072
01:20:04,720 --> 01:20:07,100
let's say

1073
01:20:07,120 --> 01:20:12,690
of course as as we wish and trees in the classic classic space it's a

1074
01:20:12,690 --> 01:20:15,100
rough approximation of the function that

1075
01:20:15,120 --> 01:20:16,680
that's the first solution

1076
01:20:16,740 --> 01:20:20,240
OK we're going to try to improve this function

1077
01:20:20,260 --> 01:20:31,260
by using ensemble methods

1078
01:20:31,260 --> 01:20:35,620
which is essentially a thinner tails than the target

1079
01:20:35,650 --> 01:20:41,530
what happens if you have things when you look at this is simulated sampled across

1080
01:20:41,530 --> 01:20:46,220
iteration number so this is like basically the possible market change so what happened is

1081
01:20:46,380 --> 01:20:47,420
that you see

1082
01:20:47,470 --> 01:20:51,730
at some stage basically when the absolute value

1083
01:20:51,740 --> 01:20:53,680
of the state is quite high

1084
01:20:53,700 --> 01:20:55,770
basically there's a lot of rejection

1085
01:20:55,780 --> 01:21:00,900
so the markov chain they quite long time OK so this is a as well

1086
01:21:00,910 --> 01:21:05,220
so basically market share reject reject make many proposals rejected quite a few times on

1087
01:21:05,590 --> 01:21:09,030
it was back and so it's coming from his behavior

1088
01:21:09,090 --> 01:21:14,150
well simply coming from basically if you look at the metropolis acceptance ratio

1089
01:21:14,160 --> 01:21:16,140
OK you get these guys

1090
01:21:16,150 --> 01:21:18,660
so assume basically

1091
01:21:18,680 --> 01:21:20,390
that the current value

1092
01:21:20,400 --> 01:21:25,350
you have the value of x the absolute value of x is actually quite large

1093
01:21:25,360 --> 01:21:31,160
it is that essentially the ratio gamma over q is very large OK that is

1094
01:21:31,160 --> 01:21:36,520
then the acceptance probability of image processing is going to be very small so essentially

1095
01:21:38,050 --> 01:21:42,720
the condition when you have a demo that x is not bounded on that essentially

1096
01:21:42,770 --> 01:21:48,460
the racial gamma x can take arbitrary values then essentially you can be it's you

1097
01:21:48,460 --> 01:21:55,040
going to be in scenarios where sometimes essentially your algorithms can be stacked essentially in

1098
01:21:55,040 --> 01:21:59,090
the tails some OK the distribution so that's why you have this actually kind of

1099
01:21:59,110 --> 01:22:00,520
pretty bad behaviour

1100
01:22:00,530 --> 01:22:04,970
if you look at least one zero of the simulated so important you would like

1101
01:22:05,020 --> 01:22:10,840
to look like an ocean distribution pretty bad because essentially because of rejection that once

1102
01:22:10,840 --> 01:22:14,770
you basically you to change in the tales basically then it does a lot of

1103
01:22:14,770 --> 01:22:18,570
time it takes a long time to escape then basically you got like quite a

1104
01:22:18,570 --> 01:22:21,640
few samples in details you that compensate for

1105
01:22:21,700 --> 01:22:27,360
for the for the fact that he agrees that support oppose doesn't explore the support

1106
01:22:27,370 --> 01:22:30,860
of distribution popular OK as that's about

1107
01:22:30,870 --> 01:22:32,950
simon says there's no magic

1108
01:22:32,970 --> 01:22:34,890
if rejection sampling

1109
01:22:34,940 --> 01:22:37,070
essentially it is

1110
01:22:37,090 --> 01:22:43,590
not applicable then you can use metropolis testing egories but it's been exhibited bad behavior

1111
01:22:43,590 --> 01:22:47,850
well actually particle in means that you can show that actually this algorithm can be

1112
01:22:47,850 --> 01:22:53,090
geometrically ergodic this is famous paperback between on mendelson publishing that

1113
01:22:53,140 --> 01:22:57,590
OK so now if you consider case where essentially q

1114
01:22:57,640 --> 01:23:03,400
is actually has seen as figure tails than the target distribution you be able to

1115
01:23:03,400 --> 01:23:07,320
hold on essentially the same policy like this would this thing that you have a

1116
01:23:07,350 --> 01:23:11,180
huge amount of rejection at some given in some given part of the space on

1117
01:23:11,180 --> 01:23:17,170
you can basically do is to go on simulated sample merchandising approximation of the oceans

1118
01:23:17,440 --> 01:23:20,480
the gaussian distribution of interest OK

1119
01:23:22,760 --> 01:23:26,380
if you do that also also that system that for the time being without much

1120
01:23:26,380 --> 01:23:31,630
like benefits so if you also in i mention so assume basically

1121
01:23:31,650 --> 01:23:37,620
you're trying to solve important from target distribution which is dominated by the notion

1122
01:23:37,730 --> 01:23:43,520
in dimension of dimension nx so this is zero mean and unit identity covariance matrix

1123
01:23:43,830 --> 01:23:45,510
or that you use the

1124
01:23:45,520 --> 01:23:48,090
similarly basically proposal

1125
01:23:48,100 --> 01:23:53,620
distribution which is normal which is a little bit shifted OK we've mean that is

1126
01:23:53,620 --> 01:23:59,600
the mean vector which has epsilon well basically component on the same covariance structure

1127
01:23:59,620 --> 01:24:02,310
if you look at at one small basically

1128
01:24:02,330 --> 01:24:04,560
basically the ratio

1129
01:24:04,590 --> 01:24:09,100
essentially private q which is in component of the metal processing factories and you can

1130
01:24:09,110 --> 01:24:13,770
see that this race to actually exhibit a lot of violation of lot of viability

1131
01:24:13,770 --> 01:24:18,560
as an an effect increases so maybe that essentially if you try to use the

1132
01:24:18,560 --> 01:24:25,090
similarly this independent metropolis hastings and i mentioned is similar to what's happening fall

1133
01:24:25,090 --> 01:24:30,580
basically rejection basic is not going to mix very well so in this case if

1134
01:24:30,580 --> 01:24:36,830
you this kind of proposal distribution in ten dimensional news that we image processing area

1135
01:24:36,830 --> 01:24:42,600
is that acceptance rate is like o point three percent is like ridiculously bad

1136
01:24:42,620 --> 01:24:49,150
OK so that's basically independent metropolis hastings uris until c is not really

1137
01:24:49,170 --> 01:24:54,480
actually good algorithm is low it says it's in the more flexible than the accept

1138
01:24:54,480 --> 01:24:59,000
project because you can use it when the tailwind this race was approach is not

1139
01:24:59,000 --> 01:25:04,230
of bonded that's the kind of deceiving because then the MCMC algorithm doesn't work well

1140
01:25:04,240 --> 01:25:05,470
in this case

1141
01:25:06,140 --> 01:25:11,240
so i could you what's the benefits of using this kind of independent proposals

1142
01:25:11,290 --> 01:25:17,200
OK for the reassessed things he basically it's as bad as the rejection sampling

1143
01:25:17,210 --> 01:25:19,610
what is the advantage of basically

1144
01:25:20,150 --> 01:25:25,860
mcmc of rejection sampling is in MCMC you get is this idea that you can

1145
01:25:26,700 --> 01:25:33,850
essentially the initial something problem into some kind of subproblems so instead basically of trying

1146
01:25:33,850 --> 01:25:39,850
to update using metropolis testing all the components at the same time you could try

1147
01:25:39,850 --> 01:25:45,150
to and they don't like the gibbs sampler one at the time OK so is

1148
01:25:45,150 --> 01:25:49,340
the same thing you can have the same features of the s four gibbssampling don't

1149
01:25:49,340 --> 01:25:54,240
have the right to date everything simultaneously the six and essentially trying to do something

1150
01:25:54,240 --> 01:25:56,720
a little bit toward so what you do

1151
01:25:56,720 --> 01:25:57,940
it would be

1152
01:25:57,960 --> 01:26:01,020
that wave could go straight through the slot

1153
01:26:01,080 --> 01:26:03,890
there would be no problem at all

1154
01:26:03,940 --> 01:26:07,600
you be that's what

1155
01:26:07,610 --> 01:26:10,610
in this way

1156
01:26:10,670 --> 01:26:12,070
three through

1157
01:26:12,090 --> 01:26:15,950
because it moves up and down in the

1158
01:26:15,960 --> 01:26:17,460
now imagine

1159
01:26:17,560 --> 01:26:18,910
that i have lights

1160
01:26:18,920 --> 01:26:21,820
there also made in this direction spring

1161
01:26:21,870 --> 01:26:24,450
oscillating in this direction so now

1162
01:26:24,450 --> 01:26:26,340
also it's like this

1163
01:26:26,350 --> 01:26:30,020
i'm trying to make you see the three-dimensional way it comes here

1164
01:26:30,040 --> 01:26:31,450
now it is

1165
01:26:31,460 --> 01:26:33,690
this narrow and it can go through

1166
01:26:33,730 --> 01:26:34,870
get stuck

1167
01:26:34,970 --> 01:26:37,380
so you see that was this morning

1168
01:26:37,420 --> 01:26:38,650
you can easily see

1169
01:26:38,660 --> 01:26:41,990
why this light which is polarized in this direction

1170
01:26:42,050 --> 01:26:43,360
goes through

1171
01:26:43,450 --> 01:26:48,150
but light which is polarized in that direction cannot be true because the

1172
01:26:48,200 --> 01:26:50,170
this what is to know

1173
01:26:50,180 --> 01:26:52,850
and in that sense you can perhaps now imagine

1174
01:26:52,890 --> 01:26:56,520
why unpolarized light can be changed

1175
01:26:56,610 --> 01:26:58,170
it to linearly

1176
01:26:58,180 --> 01:27:03,300
polarized light i'm just mentioning this so that you have some idea of on what

1177
01:27:03,300 --> 01:27:04,570
may be going on

1178
01:27:04,600 --> 01:27:05,290
in that

1179
01:27:05,320 --> 01:27:07,910
plate even though that idea is not

1180
01:27:07,950 --> 01:27:12,510
absolutely perfect

1181
01:27:13,120 --> 01:27:16,070
there is something that i have to teach some of you

1182
01:27:16,130 --> 01:27:20,220
although many of you may know this what comes but i'm going to need this

1183
01:27:20,220 --> 01:27:22,740
for the rest of my lectures

1184
01:27:22,800 --> 01:27:24,140
that's the idea

1185
01:27:24,140 --> 01:27:29,220
of an angle

1186
01:27:29,300 --> 01:27:31,720
what is an angle

1187
01:27:31,720 --> 01:27:35,850
and how do we express anger

1188
01:27:35,900 --> 01:27:39,900
if i have here circle

1189
01:27:39,950 --> 01:27:42,520
and i draw the line

1190
01:27:42,610 --> 01:27:45,110
i draw the the line

1191
01:27:45,140 --> 01:27:51,220
then we call this an angle

1192
01:27:51,320 --> 01:27:54,580
and how do we measure an angle

1193
01:27:54,660 --> 01:27:57,320
in mathematics

1194
01:27:57,380 --> 01:27:58,850
we have

1195
01:27:58,900 --> 01:28:04,570
adopted that's the definition there's nothing that you can understand about it it's just the

1196
01:28:05,800 --> 01:28:07,130
name things

1197
01:28:07,130 --> 01:28:09,410
we call an angle

1198
01:28:09,600 --> 01:28:12,210
it goes all the way around

1199
01:28:12,210 --> 01:28:16,390
so it is what we call a form rotation

1200
01:28:16,510 --> 01:28:22,710
so this like going all the way around this angle then

1201
01:28:23,610 --> 01:28:27,070
three hundred sixty degree

1202
01:28:27,150 --> 01:28:28,690
is it logic

1203
01:28:28,700 --> 01:28:31,720
no i would prefer on on the degrees but we call it three sixty can

1204
01:28:32,670 --> 01:28:37,720
that's the way it's called it's called three hundred sixty degree

1205
01:28:37,810 --> 01:28:39,580
you think of it

1206
01:28:39,650 --> 01:28:42,530
there are sixty minutes in an hour

1207
01:28:42,630 --> 01:28:46,240
equally absurd line of the hundred final ten

1208
01:28:46,300 --> 01:28:50,320
equally absurd is that there are sixty seconds in a minute

1209
01:28:50,370 --> 01:28:52,160
makes no sense

1210
01:28:52,220 --> 01:28:56,280
the very absurd twelve inches in a foot final ten

1211
01:28:56,360 --> 01:28:58,680
i don't know all the things are historical

1212
01:28:58,690 --> 01:29:01,650
and we have to live with it

1213
01:29:01,670 --> 01:29:06,480
so once you define this angle three hundred sixty three

1214
01:29:07,430 --> 01:29:12,880
you know that one quarter of rotation

1215
01:29:12,920 --> 01:29:15,000
it's going to be ninety degrees

1216
01:29:15,030 --> 01:29:17,720
so i'm going to make their picture

1217
01:29:17,720 --> 01:29:19,710
of ninety degrees

1218
01:29:19,740 --> 01:29:22,600
since we're going to need that

1219
01:29:22,600 --> 01:29:26,640
very badly doing this lecture

1220
01:29:26,650 --> 01:29:29,770
so if i rotate this flag

1221
01:29:29,830 --> 01:29:32,300
over one quarter of the rotation

1222
01:29:32,320 --> 01:29:34,290
then it will be like this

1223
01:29:34,310 --> 01:29:37,870
and this now

1224
01:29:37,950 --> 01:29:39,950
ninety degrees

1225
01:29:40,000 --> 01:29:43,950
it's one quarter of the rotation

1226
01:29:43,980 --> 01:29:46,710
and this ninety degrees

1227
01:29:46,710 --> 01:29:50,380
we have another word for that matter recall at right angles

1228
01:29:50,380 --> 01:29:53,400
it's like when officer once in a bunch of soldiers on a parade ground to

1229
01:29:53,400 --> 01:29:58,550
stand on the rectangles he could give every soldier is GPS coordinates down to within

1230
01:29:58,550 --> 01:29:59,590
a few inches

1231
01:29:59,610 --> 01:30:01,410
and that will be kind of hard work

1232
01:30:01,420 --> 01:30:05,970
or you could tell roughly where to stand and then tell each soldier what distances

1233
01:30:05,970 --> 01:30:07,340
should be from his name

1234
01:30:07,370 --> 01:30:12,070
that's a much better thing to do

1235
01:30:12,090 --> 01:30:15,800
so we're going to try and make a more powerful module for doing this deep

1236
01:30:16,720 --> 01:30:18,670
and this more powerful module

1237
01:30:18,680 --> 01:30:20,940
we'd like the units in one layer

1238
01:30:20,950 --> 01:30:25,170
to modify to modulate the interactions in the layer below

1239
01:30:25,180 --> 01:30:27,940
so in the level or something like markov random field and we will be in

1240
01:30:27,940 --> 01:30:31,740
a similar but to specify the weights in the markov random field

1241
01:30:31,760 --> 01:30:35,180
some previous work in a few years ago assignments in there

1242
01:30:35,190 --> 01:30:39,920
i had a fixed markov random field with units in the lower low-budget specifying the

1243
01:30:39,920 --> 01:30:42,180
buses and that's already with

1244
01:30:42,210 --> 01:30:46,660
but after something more ambitious to actually modulated markov random field

1245
01:30:46,800 --> 01:30:51,040
to do that we need higher order boltzmann machines

1246
01:30:51,130 --> 01:30:53,540
so a higher order boltzmann machines

1247
01:30:53,540 --> 01:30:57,820
and asking first thought about these in nineteen eighty six i think or thereabouts

1248
01:31:01,290 --> 01:31:04,110
interaction terms that are more than quadratic

1249
01:31:04,130 --> 01:31:07,610
so the second equation that is for three way one that's all we're going use

1250
01:31:09,040 --> 01:31:14,030
and if you think about this the state of you can see that SK second

1251
01:31:15,170 --> 01:31:17,540
when that's all of

1252
01:31:17,540 --> 01:31:19,630
the the that doesn't apply

1253
01:31:19,650 --> 01:31:24,040
when is all one has got state award now you got pairwise interaction between i

1254
01:31:24,040 --> 01:31:26,120
and j so the state of sk

1255
01:31:26,140 --> 01:31:29,170
is gating pairwise interactions between i and j

1256
01:31:29,190 --> 01:31:37,420
and each unit can be viewed like skating pairwise interactions between the other guys

1257
01:31:37,440 --> 01:31:43,080
so one thing we could use higher order boltzmann machine for is modelling image transformations

1258
01:31:43,080 --> 01:31:45,400
i could give you pairs of images

1259
01:31:45,410 --> 01:31:49,670
the simple cases when there's a global transformation to more interesting cases when there's more

1260
01:31:49,670 --> 01:31:54,190
local transformations and several transformations going once in the same image and we can cope

1261
01:31:54,190 --> 01:31:57,670
with just fine so i give you pairs of images

1262
01:31:57,690 --> 01:31:59,670
and the hidden units

1263
01:31:59,670 --> 01:32:03,570
and i'm going to get interactions between pixels

1264
01:32:03,570 --> 01:32:07,090
so when there is little triangle cymbals i will have a three-way weight

1265
01:32:09,390 --> 01:32:11,970
gives you an energy is a function of the states of the two pixels in

1266
01:32:11,970 --> 01:32:13,810
the state of the union

1267
01:32:13,820 --> 01:32:15,710
and red unit

1268
01:32:15,740 --> 01:32:20,970
likes the to the two transformations of pixels this is because they consistently correspond to

1269
01:32:20,970 --> 01:32:22,760
consistent translation

1270
01:32:22,900 --> 01:32:26,030
so if you have the image at time t in the image at time t

1271
01:32:26,030 --> 01:32:27,870
plus one

1272
01:32:27,880 --> 01:32:31,670
both those parents will provide evidence that he knew unit should be on

1273
01:32:31,720 --> 01:32:35,010
so tend to say that a consistent transformation

1274
01:32:36,260 --> 01:32:39,530
if you have the image at time t and you turn on in unit it

1275
01:32:39,530 --> 01:32:43,920
will tell you whether pixels should translate to an image time t plus one

1276
01:32:45,420 --> 01:32:48,510
one of the most savage and i had a paper that uses this and has

1277
01:32:48,510 --> 01:32:53,480
a huge number weights because you see this triangle cymbals have three indices they connect

1278
01:32:53,480 --> 01:32:55,060
three different units

1279
01:32:55,070 --> 01:33:00,340
and so this cubicle in many

1280
01:33:00,360 --> 01:33:04,320
so the sort of central trick to this talk is going to take that to

1281
01:33:04,320 --> 01:33:05,410
be many

1282
01:33:06,760 --> 01:33:11,580
and we just use factorizations because we don't actually believe that we want hidden units

1283
01:33:11,620 --> 01:33:16,700
be able to cause arbitrary permutations of the pixels we believe this regular structure so

1284
01:33:16,700 --> 01:33:20,310
we don't really need that many parameters

1285
01:33:20,320 --> 01:33:21,550
so we can

1286
01:33:21,570 --> 01:33:22,570
take the

1287
01:33:22,580 --> 01:33:24,490
three way weight w i j

1288
01:33:25,410 --> 01:33:29,090
and we can factor it as a product

1289
01:33:29,130 --> 01:33:34,390
of the three weights WIF wjfk w h f

1290
01:33:34,840 --> 01:33:38,360
per factor we're gonna have a bunch of factors

1291
01:33:38,380 --> 01:33:42,550
so it looks like we've got four indices that's true but notice we haven't got

1292
01:33:42,550 --> 01:33:47,320
anything that has more than two indices so basically turned the cube into

1293
01:33:47,330 --> 01:33:48,840
three pairwise things

1294
01:33:48,910 --> 01:33:50,020
so we

1295
01:33:50,050 --> 01:33:53,900
we only get many many parameters per factor

1296
01:33:53,920 --> 01:33:58,070
and we have about as many factors as hidden units are visible units so we

1297
01:33:58,070 --> 01:34:03,900
basically turns on this cube wilkinson's replicate quadratic

1298
01:34:03,900 --> 01:34:06,530
here's a picture of what we're doing

1299
01:34:07,090 --> 01:34:08,630
each factor

1300
01:34:08,640 --> 01:34:10,410
corresponds to

1301
01:34:10,420 --> 01:34:12,110
tensor one it's

1302
01:34:12,110 --> 01:34:16,690
much what is a typical fraction of hyphens in word

1303
01:34:19,190 --> 01:34:24,750
one one high out of seven is probably below average and if this and having

1304
01:34:24,750 --> 01:34:28,050
a hyphen after every second letter is probably above average

1305
01:34:28,070 --> 01:34:30,460
and there's some intermediate

1306
01:34:30,460 --> 01:34:32,900
density of hyphens which would give

1307
01:34:32,960 --> 01:34:35,690
this is the y string the highest probability

1308
01:34:48,340 --> 01:34:51,170
for linear chain crfs

1309
01:34:51,170 --> 01:34:56,630
there is a restriction on which feature functions i'm allowed

1310
01:35:01,820 --> 01:35:04,770
this is that

1311
01:35:04,780 --> 01:35:06,320
the feature function j

1312
01:35:06,320 --> 01:35:07,690
for any j

1313
01:35:09,630 --> 01:35:13,050
it's a function of x exploring wipe

1314
01:35:13,070 --> 01:35:17,590
and now i'm going to use the pontification to mean sequences

1315
01:35:19,780 --> 01:35:21,340
it's going to be the

1316
01:35:25,280 --> 01:35:27,480
so i

1317
01:35:27,570 --> 01:35:29,960
ranges over

1318
01:35:31,530 --> 01:35:32,840
the sequence

1319
01:35:32,860 --> 01:35:36,360
so from i was one to the length of the sequence

1320
01:35:38,820 --> 01:35:40,900
little fj j

1321
01:35:42,050 --> 01:35:45,710
why i minus one y i

1322
01:35:45,750 --> 01:35:49,110
exp are high

1323
01:36:06,460 --> 01:36:12,670
of feature functions so feature function eighteen little feature function eighteen

1324
01:36:12,690 --> 01:36:13,710
five b

1325
01:36:13,730 --> 01:36:16,070
one of

1326
01:36:16,090 --> 01:36:19,780
why i minus one y i

1327
01:36:22,460 --> 01:36:23,610
might be

1328
01:36:24,590 --> 01:36:27,000
i equals

1329
01:36:29,570 --> 01:36:32,840
and why i minus one

1330
01:36:37,420 --> 01:36:40,750
and y equals one

1331
01:36:42,000 --> 01:36:43,570
x one x two

1332
01:36:47,420 --> 01:36:51,000
they have

1333
01:36:57,090 --> 01:37:01,480
so forget this for a minute and just think about this is an example another

1334
01:37:01,480 --> 01:37:03,360
example of feature function

1335
01:37:03,380 --> 01:37:05,030
march so

1336
01:37:05,050 --> 01:37:09,530
paying attention to the position i corps two

1337
01:37:09,590 --> 01:37:13,550
so then why i minus one will be y one y i would be y

1338
01:37:14,420 --> 01:37:18,210
so i pay attention to the first two labels in the string

1339
01:37:18,230 --> 01:37:20,250
first first two tags

1340
01:37:21,210 --> 01:37:26,380
those first two tags are zero and one according to this feature function

1341
01:37:29,510 --> 01:37:32,550
OK let me make that p

1342
01:37:32,590 --> 01:37:34,010
one and zero

1343
01:37:35,130 --> 01:37:37,270
then move

1344
01:37:37,400 --> 01:37:38,820
and paying attention

1345
01:37:38,840 --> 01:37:44,230
so the first two letters in the word in those first two letters and

1346
01:37:44,280 --> 01:37:48,920
and more so

1347
01:37:52,300 --> 01:37:55,050
we have the first two letters a and

1348
01:37:57,630 --> 01:38:08,860
one zero is that plausible tags for the first of those living and

1349
01:38:08,860 --> 01:38:14,840
well you think of words that begin with a there is a moral

1350
01:38:15,980 --> 01:38:18,650
you probably can hyphenate like this

1351
01:38:18,650 --> 01:38:21,650
and then there's some

1352
01:38:21,670 --> 01:38:28,300
but you immediately get high and then there one which i can think of where

1353
01:38:28,300 --> 01:38:34,510
you wouldn't have a hyphen think amateurism word means to do with love

1354
01:38:36,730 --> 01:38:41,360
depending so sometimes are allowed to hyphen after the a and sometimes you're not allowed

1355
01:38:41,380 --> 01:38:43,150
hyphen after the a

1356
01:38:43,190 --> 01:38:46,650
and but probably mostly

1357
01:38:46,690 --> 01:38:48,550
you are allowed

1358
01:38:50,400 --> 01:38:52,420
hyphen after the a

1359
01:38:54,980 --> 01:38:57,150
with this feature function

1360
01:38:57,190 --> 01:38:58,650
then you learn

1361
01:38:58,650 --> 01:39:00,900
wait for that was positive

1362
01:39:00,920 --> 01:39:06,840
and what that positive weight would be meaning would be that you know any

1363
01:39:06,880 --> 01:39:09,630
any tag sequence y

1364
01:39:09,630 --> 01:39:12,860
that it starts out one zero

1365
01:39:14,570 --> 01:39:16,340
everything else being equal

1366
01:39:16,400 --> 01:39:18,940
has a high probability

1367
01:39:18,960 --> 01:39:20,940
so every x y pair

1368
01:39:20,940 --> 01:39:24,690
where x starts out and y starts out

1369
01:39:24,710 --> 01:39:28,150
hi for no hyphen

1370
01:39:28,150 --> 01:39:29,900
is a higher probability

1371
01:39:29,920 --> 01:39:31,270
x y pair

1372
01:39:49,900 --> 01:39:51,940
this is

1373
01:39:52,150 --> 01:39:57,710
the user is exactly right and the the idea of feature function is an awful

1374
01:39:57,710 --> 01:40:01,530
idea for machine learning and is an idea that it can be hard to get

1375
01:40:01,530 --> 01:40:07,170
your head around the first of month because we're so used to thinking of features

1376
01:40:07,170 --> 01:40:10,510
as being properties of the input x

1377
01:40:10,530 --> 01:40:14,750
but when we have a very large output space y

1378
01:40:16,550 --> 01:40:20,480
i really can't we really need to talk about properties of the out space y

1379
01:40:20,710 --> 01:40:27,280
output space y also and rather than thinking of the mapping from x to y

1380
01:40:27,280 --> 01:40:29,300
we should think of

1381
01:40:29,320 --> 01:40:31,400
x y pairs

1382
01:40:31,400 --> 01:40:32,860
and then

1383
01:40:32,880 --> 01:40:36,860
map and then working out whether the XY pair is good pair

1384
01:40:36,880 --> 01:40:39,420
or bad

1385
01:40:40,440 --> 01:40:41,340
let me

1386
01:40:42,920 --> 01:40:47,360
try to sketch yet another example and this will be an example of a log

1387
01:40:47,360 --> 01:40:52,750
linear model that is not a conditional random field

1388
01:40:56,850 --> 01:40:58,820
consider the problem of

1389
01:40:58,820 --> 01:41:00,420
multi label

1390
01:41:00,460 --> 01:41:07,840
supervised learning

1391
01:41:07,900 --> 01:41:11,750
who knows what multilabel

1392
01:41:11,750 --> 01:41:18,090
for multilabel problem is it's different from the multiclass problems

1393
01:41:20,090 --> 01:41:23,010
the multiclass problems when you have an input x

1394
01:41:23,980 --> 01:41:25,840
you want to predict the label y

1395
01:41:25,860 --> 01:41:30,150
and the y comes from a set that is bigger than the binary set

1396
01:41:30,920 --> 01:41:31,900
if you want

1397
01:41:31,920 --> 01:41:34,630
classify somebody is

1398
01:41:34,800 --> 01:41:37,480
freshman sophomore junior senior

1399
01:41:38,420 --> 01:41:39,820
multi class

1400
01:41:39,920 --> 01:41:43,250
but multi label is when you want to

1401
01:41:43,270 --> 01:41:45,230
classify all tax

1402
01:41:45,250 --> 01:41:47,030
as having

1403
01:41:47,050 --> 01:41:48,650
zero or more

1404
01:41:51,420 --> 01:41:55,270
so each tax

1405
01:41:55,280 --> 01:41:57,400
has zero or more

1406
01:41:59,940 --> 01:42:01,460
and it so

1407
01:42:01,480 --> 01:42:05,170
for each document

1408
01:42:06,050 --> 01:42:09,090
you want to map it onto my

1409
01:42:12,210 --> 01:42:15,170
of course

1410
01:42:28,400 --> 01:42:30,360
so you you have some

1411
01:42:30,380 --> 01:42:34,510
you have some classic document categorisation problem

1412
01:42:34,510 --> 01:42:39,030
so this is kind of an exploration exploitation kind of problem you pick something it

1413
01:42:39,030 --> 01:42:42,950
did pretty well is there something else better i don't know try and see you

1414
01:42:42,950 --> 01:42:47,040
need an algorithm that can kind of mixed these exploring exploiting stuff

1415
01:42:47,110 --> 01:42:50,530
it turns out this kind of thing you can do in a reasonably natural way

1416
01:42:50,590 --> 01:42:54,030
the bounds of being works but the ideas are fairly straightforward when you want to

1417
01:42:54,040 --> 01:42:56,950
do here or this one thing you can do

1418
01:42:57,180 --> 01:43:02,170
you realize that all the algorithms need to work is an estimate of the cost

1419
01:43:02,390 --> 01:43:06,290
to the sum of all day one day two hundred eighty minus one that's something

1420
01:43:06,290 --> 01:43:09,840
that's the only thing you're using and even if you get a pretty close that's

1421
01:43:09,840 --> 01:43:13,170
good enough because you're going to be noise to anyone c and noise it's pretty

1422
01:43:13,170 --> 01:43:14,570
close to the right answers

1423
01:43:14,850 --> 01:43:16,770
the same thing is i is the right answer

1424
01:43:16,780 --> 01:43:18,590
that might help you get

1425
01:43:20,500 --> 01:43:23,120
OK you see an estimate of the

1426
01:43:23,180 --> 01:43:24,250
some of the

1427
01:43:24,270 --> 01:43:27,780
cost factors and today's the day one day two day

1428
01:43:27,840 --> 01:43:30,920
so you see that estimate how do you do that well what you can do

1429
01:43:30,920 --> 01:43:34,510
is you pick some sort of basis which would in this case look like the

1430
01:43:34,520 --> 01:43:37,680
fixed collection of routers that together

1431
01:43:37,730 --> 01:43:42,230
every other row can be written as a linear combination of those

1432
01:43:42,240 --> 01:43:46,220
and everyone said well your sample something random

1433
01:43:46,270 --> 01:43:51,100
and you do you stop products with your basis vectors to reconstruct an estimate of

1434
01:43:51,100 --> 01:43:55,070
the final that year so everyone so i take some basis vector input the dot

1435
01:43:55,070 --> 01:43:56,120
product of you

1436
01:43:56,170 --> 01:43:58,470
and you can

1437
01:43:58,480 --> 01:44:01,860
use that to reconstruct an estimate of what the actual cost

1438
01:44:01,890 --> 01:44:04,770
the healthier basis to be is orthogonal is possible

1439
01:44:04,770 --> 01:44:07,400
whether you can do that depends on the kind of problem if you have a

1440
01:44:07,400 --> 01:44:12,180
problem like this figure what's of passes it that is orthogonal possible in this in

1441
01:44:12,180 --> 01:44:16,310
this space

1442
01:44:16,320 --> 01:44:19,390
and you can show that even if the world's adaptive knows that you know how

1443
01:44:19,440 --> 01:44:23,870
to get you if it's still campfires to estimate how much he was right

1444
01:44:23,970 --> 01:44:26,480
so that that's on this say about the band

1445
01:44:26,550 --> 01:44:31,020
but if not once you have the other thing it's not too hard to be

1446
01:44:31,020 --> 01:44:36,310
extended to this case we do a little bit x exploration in your merge

1447
01:44:38,240 --> 01:44:42,470
so let me and this part of this is the end of stuff one before

1448
01:44:42,470 --> 01:44:47,470
we go game theory part with just one natural generalisation of this problem to something

1449
01:44:47,470 --> 01:44:49,700
that makes sense for

1450
01:44:49,770 --> 01:44:54,040
problems like dealing with text and other sort of learning problem

1451
01:44:54,730 --> 01:44:58,910
let me start by thinking this from the point of view drive so

1452
01:44:58,960 --> 01:45:02,010
we were just trying to be nearly as well as the best fixed route

1453
01:45:02,020 --> 01:45:06,990
that's fine but a more ambitious goal might be to say that but we also

1454
01:45:06,990 --> 01:45:11,740
want to rainy days we do nearly as well as the best route for a

1455
01:45:11,780 --> 01:45:14,780
so maybe in general this is the best way to go but rainy days or

1456
01:45:14,780 --> 01:45:15,810
some other better

1457
01:45:15,860 --> 01:45:19,240
and we would like a strategy that has the property that not only does it

1458
01:45:19,240 --> 01:45:20,950
overall time do as well

1459
01:45:20,960 --> 01:45:24,000
nearly as well as the best round time rainy days

1460
01:45:24,050 --> 01:45:26,570
it really is one of the best referee days

1461
01:45:26,610 --> 01:45:30,860
on mondays does nearly as well as best monday's traffic

1462
01:45:30,880 --> 01:45:35,320
about it so traffic where i live is a lot less on mondays

1463
01:45:36,270 --> 01:45:40,440
even if ten percent of people who is not gonna work the boy that's that

1464
01:45:41,920 --> 01:45:44,860
pay them to knock OK

1465
01:45:44,920 --> 01:45:48,700
so more generally what if you have an

1466
01:45:48,700 --> 01:45:50,460
prediction rules

1467
01:45:50,480 --> 01:45:55,390
your prediction rules could be things like on monday use happy

1468
01:45:55,710 --> 01:46:00,440
all simultaneously for each of these rules that we want to guarantee do nearly as

1469
01:46:00,440 --> 01:46:06,800
well as that rule does in the time steps from which several fires

1470
01:46:06,820 --> 01:46:09,560
OK and

1471
01:46:09,610 --> 01:46:14,340
because the for all rules i expected cost of our algorithm on time steps in

1472
01:46:14,340 --> 01:46:18,430
which we live wires should be not too much more than expected and the cost

1473
01:46:18,430 --> 01:46:19,610
of the rule i

1474
01:46:19,700 --> 01:46:23,330
on the time which fires classes

1475
01:46:23,390 --> 01:46:27,390
it turns out you can get this

1476
01:46:27,510 --> 01:46:31,440
OK and what's nice about this is that many places in machine learning we would

1477
01:46:31,440 --> 01:46:32,930
like to combine

1478
01:46:32,940 --> 01:46:34,300
if then rules

1479
01:46:34,320 --> 01:46:38,320
in fact in the data mining lectures we saw that there you no

1480
01:46:38,330 --> 01:46:43,700
there are these strategies for pulling out this the high specificity rules don't quite often

1481
01:46:43,700 --> 01:46:47,120
but when they apply they can be very good so imagine though the someone's hand

1482
01:46:47,120 --> 01:46:50,540
you a whole bunch of rules some of them may be very good some of

1483
01:46:50,540 --> 01:46:53,680
them may not be very good if you don't know in advance which one you

1484
01:46:53,680 --> 01:46:55,090
want to combine

1485
01:46:55,180 --> 01:46:58,270
so think of the document classification so

1486
01:46:58,320 --> 01:47:04,310
a rule like a four x appears in document predict label wiser has football sports

1487
01:47:04,410 --> 01:47:06,000
that's probably a good

1488
01:47:06,040 --> 01:47:11,030
if football then classifies economics prior that give be lots of rules in advance you

1489
01:47:11,030 --> 01:47:13,170
don't know which are good and which are bad

1490
01:47:13,170 --> 01:47:16,380
we would like to have the property that if it really care out the ninety

1491
01:47:16,380 --> 01:47:21,230
percent of documents with the word football really are about support and you should do

1492
01:47:21,240 --> 01:47:25,530
nearly as well as the word for all those under so football is there ten

1493
01:47:25,530 --> 01:47:28,680
percent and then you should have only a little bit more than ten percent

1494
01:47:28,730 --> 01:47:32,220
but that that kind of an goal corresponds

1495
01:47:32,300 --> 01:47:37,580
and this is called the special problem sleeping experts problem so this rule like an

1496
01:47:37,580 --> 01:47:41,960
expert but sleep it only wakes up and the doctor has football football football i

1497
01:47:41,960 --> 01:47:44,850
know about sports there goes back to

1498
01:47:44,870 --> 01:47:48,560
i know people like that

1499
01:47:57,110 --> 01:47:58,490
when i was in italy

1500
01:47:58,490 --> 01:48:02,270
during the world cup the whole country was sort of like that it's likely that

1501
01:48:02,270 --> 01:48:05,820
they are so once the game was on everything shut down this is what you

1502
01:48:05,820 --> 01:48:08,820
can walk into someone's house is an account what

1503
01:48:08,840 --> 01:48:10,020
he went even though

1504
01:48:10,120 --> 01:48:15,500
it's great actually really great anyway

1505
01:48:15,860 --> 01:48:21,260
OK this has been studied theoretically and also experimentally in a number of papers

1506
01:48:24,210 --> 01:48:27,300
well i just give a feel of how some of the other things talked about

1507
01:48:27,300 --> 01:48:29,620
could be extended to this case i will go through the

1508
01:48:29,780 --> 01:48:33,670
the arguments not hard i will go through it just we've done after

1509
01:48:33,750 --> 01:48:35,580
i want to go into the game

1510
01:48:36,590 --> 01:48:40,910
everything analysis part but here's here's the so just like before we start everybody with

1511
01:48:40,920 --> 01:48:42,580
weight one

1512
01:48:42,600 --> 01:48:44,060
and just like before

1513
01:48:44,080 --> 01:48:47,030
every time step now some of the rules fire

1514
01:48:47,040 --> 01:48:48,390
some of

1515
01:48:48,590 --> 01:48:52,960
will do is will pick one with probability proportional to its weight just like we

1516
01:48:53,040 --> 01:48:54,340
doing before

1517
01:48:54,350 --> 01:48:57,770
the with the first we had all the guys with the weights and we cut

1518
01:48:57,770 --> 01:49:00,570
the waste and we picked on the probably proportional to its weight so do the

1519
01:49:00,570 --> 01:49:02,470
exact same

1520
01:49:02,480 --> 01:49:06,490
but now we can update the weights differently so before we penalize things for getting

1521
01:49:06,490 --> 01:49:07,270
it wrong

1522
01:49:07,330 --> 01:49:09,090
now we don't want to just

1523
01:49:09,100 --> 01:49:12,430
now we mean the problem is if you do that kind of penalizing rules for

1524
01:49:12,430 --> 01:49:17,200
writing and one highest weight ones never is and you want to penalize raising anyone

1525
01:49:17,200 --> 01:49:18,810
also reward them to

1526
01:49:18,820 --> 01:49:20,240
the user and do

1527
01:49:20,240 --> 01:49:23,210
if they didn't fire would be the way loss

1528
01:49:23,220 --> 01:49:25,530
if they did fire

1529
01:49:25,540 --> 01:49:27,910
OK listen some to ignore them matter second

1530
01:49:27,910 --> 01:49:31,560
so if they did fire we're gonna raise their way or lower their way

1531
01:49:31,610 --> 01:49:36,840
depending on how well they do compared to the weighted average of the rules of

1532
01:49:36,850 --> 01:49:41,010
in particular what turns out to be something that you can still proves thing about

1533
01:49:41,050 --> 01:49:44,040
in practice you may not need some of the hedging but

1534
01:49:44,090 --> 01:49:45,940
it proved to work to do

1535
01:49:45,980 --> 01:49:47,800
is it the rule

1536
01:49:47,820 --> 01:49:53,510
raise its makes the predictions and it doesn't exactly as well as the weighted average

1537
01:49:53,510 --> 01:49:56,350
of the rules of fire by weighted average weighted by the

1538
01:49:57,360 --> 01:49:59,330
but rule fires

1539
01:49:59,340 --> 01:50:01,570
if someone is exactly as well

1540
01:50:01,580 --> 01:50:03,910
as the weighted average of the rules of fire

1541
01:50:03,920 --> 01:50:06,950
you penalizes a little bit

1542
01:50:06,970 --> 01:50:08,570
if it does better

1543
01:50:08,570 --> 01:50:15,050
and there can the point will be that 1 of these corners is the winner

1544
01:50:15,090 --> 01:50:19,050
that that if I don't know if to think why

1545
01:50:19,110 --> 01:50:24,480
but but let me emphasize that point the winner is at a corner

1546
01:50:24,480 --> 01:50:26,250
the winner is

1547
01:50:26,420 --> 01:50:29,310
at corner

1548
01:50:29,350 --> 01:50:32,480
so anything with so what that mean ordinary words

1549
01:50:33,080 --> 01:50:35,420
that means that the winter of

1550
01:50:35,420 --> 01:50:37,290
the only 1 of these

1551
01:50:39,420 --> 01:50:42,330
at the optimal X stock

1552
01:50:42,350 --> 01:50:43,720
it is not

1553
01:50:43,720 --> 01:50:46,570
is working is not 0

1554
01:50:46,590 --> 01:50:51,790
it is and I only need like 1 to just because I only have 1

1555
01:50:53,200 --> 01:50:57,250
so so at the corners of the sources say what a corner

1556
01:50:57,480 --> 01:50:59,480
but the

1557
01:50:59,740 --> 01:51:01,820
corner is have em

1558
01:51:03,140 --> 01:51:11,220
nonzero Texas and that and that gives me enough to solve the M equations x

1559
01:51:13,070 --> 01:51:15,220
and the n minus

1560
01:51:18,020 --> 01:51:20,680
the house

1561
01:51:21,340 --> 01:51:28,670
because I'm adequate so that in that it's from it we can visualize this thing

1562
01:51:28,670 --> 01:51:31,070
in this 3 the picture

1563
01:51:31,180 --> 01:51:35,530
the visualization gets quite

1564
01:51:35,550 --> 01:51:38,750
uh harder

1565
01:51:38,790 --> 01:51:45,180
in a so that in M and then dimensions in so my point was that

1566
01:51:45,180 --> 01:51:50,510
the winner X star

1567
01:51:50,530 --> 01:51:52,660
it is a corner

1568
01:51:52,980 --> 01:51:54,940
some courses

1569
01:51:55,400 --> 01:51:59,850
this thing is so you might think OK just try all the corners

1570
01:51:59,900 --> 01:52:04,740
evaluate the cost every corner once you know the winner a corner

1571
01:52:07,440 --> 01:52:08,870
but the thing is

1572
01:52:08,900 --> 01:52:12,450
there are too many corners the number of corners is

1573
01:52:12,970 --> 01:52:17,620
N choose them or something which is a very big number

1574
01:52:18,660 --> 01:52:20,620
it's 3 here

1575
01:52:20,780 --> 01:52:23,980
so the 3 corners and we can check a

1576
01:52:24,020 --> 01:52:29,560
but generally you have to imagine sorry imagining n-dimensional space

1577
01:52:29,580 --> 01:52:32,080
like this where n was 3

1578
01:52:32,120 --> 01:52:33,250
and then

1579
01:52:33,270 --> 01:52:36,200
what is the constraints give us

1580
01:52:36,250 --> 01:52:40,060
think of n-dimensional space X 1 to X N

1581
01:52:40,180 --> 01:52:42,910
and then we have equations

1582
01:52:44,040 --> 01:52:46,920
the any equations 1 of the idea of

1583
01:52:47,920 --> 01:52:49,380
in this space

1584
01:52:49,560 --> 01:52:52,930
well every equation is linear so it's a

1585
01:52:53,950 --> 01:52:56,770
Soviet on planes

1586
01:52:56,870 --> 01:53:01,270
and forget the optimum because octant

1587
01:53:01,310 --> 01:53:07,410
positive direction in an n-dimensional space of and website because I can't utilize here

1588
01:53:08,120 --> 01:53:11,900
but acceptance conservative important to realize

1589
01:53:11,910 --> 01:53:15,930
that the feasible set is

1590
01:53:16,020 --> 01:53:20,180
a polyhedron it's the setting curve boundaries

1591
01:53:20,250 --> 01:53:27,930
that everything is is there they based planes which dropout here

1592
01:53:28,820 --> 01:53:36,020
coming from x greater equal 0 chop up the positive part or the constraint planes

1593
01:53:36,080 --> 01:53:42,870
coming from the constraints that cut through in some way so you get somehow region

1594
01:53:42,890 --> 01:53:47,180
that's created out of N plus planes

1595
01:53:47,180 --> 01:53:48,540
it's it's

1596
01:53:48,560 --> 01:53:52,790
but it can be quite complicated

1597
01:53:52,810 --> 01:53:59,060
and less is pretty big number you're just in areas they we have all mean

1598
01:53:59,060 --> 01:54:05,200
this would be big 20 equations 10 constraint so we're in 20 dimensional space got

1599
01:54:05,220 --> 01:54:09,810
30 planes all things through and you can see that

1600
01:54:10,890 --> 01:54:15,780
the feasible set could be empty for example how often I look how can I

1601
01:54:15,780 --> 01:54:21,840
change this prior I create a problem where the where there were no feasible X

1602
01:54:21,840 --> 01:54:23,400
it's not out of this example

1603
01:54:25,930 --> 01:54:29,200
well I could make for a minus

1604
01:54:29,290 --> 01:54:34,450
if I may therefore minus 4 then there wouldn't be any non-negative axes that would

1605
01:54:34,450 --> 01:54:37,250
solve minus 4 but of course that

1606
01:54:37,640 --> 01:54:42,180
problem with with minus 4 our 0 marks to do

1607
01:54:43,460 --> 01:54:50,560
well often have no homework but I've never signed number of homework problem so anyway

1608
01:54:55,770 --> 01:54:59,660
OK which is the winner

1609
01:54:59,660 --> 01:55:04,250
which is the best point well in this case I could just check the cost

1610
01:55:04,250 --> 01:55:05,220
of every

1611
01:55:05,290 --> 01:55:07,040
at every corner

1612
01:55:07,040 --> 01:55:11,520
at all and I have to say why is the winner adequate

1613
01:55:11,620 --> 01:55:14,950
the fact that the winners of the corner is is a big help

1614
01:55:15,920 --> 01:55:21,660
and it somehow depends on linear narrative forever linear functions you know you

1615
01:55:22,830 --> 01:55:28,100
if it's increasing it continues to increase until you bump up against

1616
01:55:31,600 --> 01:55:35,680
or continues to decrease over time to make something decreases

1617
01:55:35,700 --> 01:55:40,060
like like suppose you said well maybe that's the winner in the middle

1618
01:55:40,080 --> 01:55:42,450
all 3 working away well

1619
01:55:42,540 --> 01:55:47,450
but if I discovered that the move from around from there

1620
01:55:47,560 --> 01:55:50,120
in some direction is better

1621
01:55:50,160 --> 01:55:55,060
than more is better more better more and better until finally maybe I just make

1622
01:55:56,020 --> 01:56:00,020
edge at that point

1623
01:56:00,040 --> 01:56:01,430
it can't

1624
01:56:01,520 --> 01:56:06,270
it's not legal to go further because Annex would get negative so at that point

1625
01:56:07,950 --> 01:56:10,740
what again

1626
01:56:10,740 --> 01:56:12,270
OK but now

1627
01:56:12,290 --> 01:56:15,970
my point is that we only have to consider corners

1628
01:56:16,290 --> 01:56:18,710
and why is that

1629
01:56:18,950 --> 01:56:28,950
that's right the drive to a corner that's just that's the fundamental reason you what

1630
01:56:29,100 --> 01:56:31,200
1 way to see it is

1631
01:56:33,010 --> 01:56:39,240
is now just think of these cost if we got the answer is a good

1632
01:56:39,240 --> 01:56:44,500
way to see not only that it's corner but but what what picks out that

1633
01:56:44,500 --> 01:56:50,500
corner so the cost suppose I'm trying to do this job for a total cost

1634
01:56:50,500 --> 01:57:00,510
of a million dollars it's not costly million dollars for the area personal workers and

1635
01:57:00,510 --> 01:57:07,040
only forward problem but suppose I look at the equation 5 Examples 3 extemporization

1636
01:57:07,440 --> 01:57:11,740
0 I don't even know if I could spend a million dollars

1637
01:57:11,750 --> 01:57:13,060
yes I could

1638
01:57:16,540 --> 01:57:22,210
I don't well at an even know but anyway suppose I look at the axis

1639
01:57:22,210 --> 01:57:26,850
that's all that give me a cost a million dollars

1640
01:57:27,060 --> 01:57:35,200
yeah I don't think I could spend up there not feasible right is that this

1641
01:57:35,200 --> 01:57:40,380
we have is that the top the slide off two a one we want to

1642
01:57:40,640 --> 01:57:42,860
show their faces

1643
01:57:42,860 --> 01:57:46,770
i came up with the covalent here here's island

1644
01:57:47,000 --> 01:57:55,350
to change the examples so far there is a great deal of corporate and government

1645
01:57:55,350 --> 01:57:57,090
surveillance of people

1646
01:57:57,110 --> 01:58:00,620
all they want to get be example of

1647
01:58:00,630 --> 01:58:06,850
we want to do what we do what's going on cities experiences to want be

1648
01:58:07,120 --> 01:58:08,240
within the

1649
01:58:08,270 --> 01:58:13,730
there's a lot of those in some ways part that you villages where you start

1650
01:58:13,780 --> 01:58:16,430
at two in the morning to have fair

1651
01:58:16,450 --> 01:58:20,110
they used friend of

1652
01:58:20,110 --> 01:58:24,580
it might be pretty visible involvement was

1653
01:58:24,860 --> 01:58:30,400
this is a different individual the family the

1654
01:58:30,400 --> 01:58:35,100
he is the american examples that apologize for the

1655
01:58:35,110 --> 01:58:38,070
that was the type the

1656
01:58:39,170 --> 01:58:41,200
radio and and television shows

1657
01:58:41,220 --> 01:58:43,890
and when the images were there

1658
01:58:43,890 --> 01:58:45,510
every school

1659
01:58:45,540 --> 01:58:48,340
my later learned to read

1660
01:58:48,350 --> 01:58:51,680
master home two kids play

1661
01:58:51,740 --> 01:58:57,890
and one of more things that of the many TV shows like this has been

1662
01:58:57,890 --> 01:59:01,390
shown on one user

1663
01:59:01,700 --> 01:59:03,740
this is changing

1664
01:59:03,750 --> 01:59:05,840
but this is

1665
01:59:05,850 --> 01:59:09,920
what your children and so on

1666
01:59:09,940 --> 01:59:12,830
and by the the show itself

1667
01:59:12,840 --> 01:59:15,970
is it

1668
01:59:19,610 --> 01:59:20,720
that's is not possible

1669
01:59:20,740 --> 01:59:22,900
the reason that describe

1670
01:59:22,900 --> 01:59:35,580
because he was playing very easy to do

1671
01:59:35,580 --> 01:59:42,160
so when we it was not going to be

1672
01:59:43,150 --> 01:59:48,480
kids what it's

1673
01:59:48,490 --> 01:59:53,380
those long all nodes in the PC

1674
01:59:53,410 --> 01:59:57,570
i wish know where where you are told by accident

1675
01:59:57,600 --> 02:00:00,660
examples that you

1676
02:00:12,770 --> 02:00:20,770
call and say we're doing now you know he's going to get from background noise

1677
02:00:20,770 --> 02:00:25,880
with the

1678
02:00:25,940 --> 02:00:28,630
his parents were

1679
02:00:28,700 --> 02:00:35,670
and we know that this is a paper based on a few days before the

1680
02:00:35,670 --> 02:00:36,890
network also

1681
02:00:36,950 --> 02:00:40,760
that's also chapter six per

1682
02:00:41,660 --> 02:00:42,470
call network

1683
02:00:42,490 --> 02:00:48,970
families of which is a lot of communication and this is why or how can

1684
02:00:48,980 --> 02:00:53,580
this work addresses the president's policies related to each other

1685
02:00:55,090 --> 02:01:00,260
you can see that are related to each other all the time using a mobile

1686
02:01:00,420 --> 02:01:05,850
phone call is the way forward so you know that's a whole

1687
02:01:05,870 --> 02:01:09,480
as far is for is still alive

1688
02:01:13,850 --> 02:01:15,790
there has been

1689
02:01:15,810 --> 02:01:19,220
the networks

1690
02:01:19,240 --> 02:01:23,120
at the same time so this is a very good

1691
02:01:24,370 --> 02:01:30,330
has changed in is a that few households with children the american association words from

1692
02:01:30,470 --> 02:01:35,070
parents he has to say that are

1693
02:01:35,090 --> 02:01:37,170
we had to that

1694
02:01:37,270 --> 02:01:41,770
parameters and therefore is the only american data

1695
02:01:41,820 --> 02:01:42,990
on the

1696
02:01:43,010 --> 02:01:48,360
two things maybe not all the samples will pertain to europe

1697
02:01:51,360 --> 02:01:53,320
perhaps in parts of the

1698
02:01:55,420 --> 02:01:57,410
we know how to use the wiki

1699
02:01:57,420 --> 02:01:59,880
all let's find together

1700
02:01:59,880 --> 02:02:04,340
they keep separate works of the conductor mobile

1701
02:02:04,380 --> 02:02:12,010
and the more interesting and i are just published a book about two sets of

1702
02:02:13,620 --> 02:02:17,290
there are many kinds of doing work at home the first of all the people

1703
02:02:17,290 --> 02:02:19,300
who say it all the time

1704
02:02:19,320 --> 02:02:23,160
two were based on these three to four days we were

1705
02:02:24,810 --> 02:02:26,860
they really really small

