1
00:00:00,000 --> 00:00:05,290
now my pleasure to introduce you miller from carnegie mellon

2
00:00:05,330 --> 00:00:07,750
thanks very much for a

3
00:00:07,750 --> 00:00:09,270
thank you

4
00:00:09,610 --> 00:00:14,440
thank you for inviting me to give the talk

5
00:00:14,460 --> 00:00:20,840
so hopefully this will be understandable so again please ask questions this workshop

6
00:00:25,020 --> 00:00:29,070
so today what i'd like to talk about is this area which is called

7
00:00:29,120 --> 00:00:33,510
now spectral graph theory right is the idea that you think about graph is the

8
00:00:33,510 --> 00:00:37,730
matrix and looking outside investors into using matrix of the graph

9
00:00:37,760 --> 00:00:43,220
and graph theory to the matrix OK and particular going to look at questions about

10
00:00:43,220 --> 00:00:45,590
how do you quickly solve linear systems

11
00:00:45,590 --> 00:00:50,380
and again that's going to reduce to graph graph questions and we also show applications

12
00:00:50,380 --> 00:00:53,900
where we used spectral graph theory to solve some of these questions

13
00:00:55,810 --> 00:00:59,880
so far the simplest of all constraints systems that has been around for hundreds of

14
00:00:59,880 --> 00:01:05,980
years is the notion of a linear system and this is very fundamental in all

15
00:01:05,980 --> 00:01:11,060
the workers everyone every talk have seen so far under the hood somewhere solving linear

16
00:01:11,060 --> 00:01:14,840
systems so it's critical to be able to solve these these systems quickly

17
00:01:15,000 --> 00:01:18,580
so it's obvious thing to do is write is the matrix i guess it's interesting

18
00:01:18,580 --> 00:01:22,340
point i believe that gauss did not understand this notation so that's one of the

19
00:01:22,340 --> 00:01:25,500
reasons i have never been able to read a lot of his original work that

20
00:01:25,500 --> 00:01:29,390
he didn't know about matrices with much later discovered but very

21
00:01:29,410 --> 00:01:32,170
the beautiful way to think of the world rise matrices

22
00:01:32,190 --> 00:01:38,140
so what's interesting is that is theoretically anyway if you want to solve dense matrices

23
00:01:38,220 --> 00:01:42,310
you can do that and q time stress then showed how to do that and

24
00:01:42,310 --> 00:01:47,030
the two point eight and then time and improved most of the centre course way

25
00:01:47,030 --> 00:01:50,080
too large for the kind of system we'd like to solve today

26
00:01:50,090 --> 00:01:53,420
and so it's still a very big number right

27
00:01:53,420 --> 00:01:57,230
so what's interesting is that is not if it's just general sparse system we don't

28
00:01:57,230 --> 00:02:01,610
have a better theory worst case analysis to say much better bounds

29
00:02:03,300 --> 00:02:07,560
so there are easy cases for the simplest of all these cases the upper triangular

30
00:02:07,560 --> 00:02:11,990
lower triangular we can do back substitution and when we do this then we get

31
00:02:12,000 --> 00:02:16,720
to solve the number of nonzero right so the goal one goal then would be

32
00:02:16,720 --> 00:02:18,340
to find a bunch of other

33
00:02:18,380 --> 00:02:21,750
linear systems all of which we can then solve as fast as we can solve

34
00:02:21,750 --> 00:02:23,340
an upper triangular system

35
00:02:23,340 --> 00:02:25,060
offensive that's our goal

36
00:02:26,600 --> 00:02:31,680
so the first obvious constraint people put on that symmetric so throughout the rest of

37
00:02:31,680 --> 00:02:35,210
this talk them with some matrix system that is symmetric

38
00:02:35,220 --> 00:02:38,690
army corps is the graph that means our graph is undirected

39
00:02:38,690 --> 00:02:41,630
it be nice to get rid of this constraint i don't how

40
00:02:41,750 --> 00:02:44,440
tonight but anyway so i'll use that

41
00:02:44,500 --> 00:02:50,320
so another assumption throughout this talk is going to be the start with positive definite

42
00:02:50,320 --> 00:02:55,150
right namely that all the eigen values values are

43
00:02:55,210 --> 00:02:58,160
it strictly positive or that

44
00:02:58,210 --> 00:03:01,470
the rayleigh quotient is always positive

45
00:03:04,410 --> 00:03:08,470
so the first approach that people started doing i guess i just decided to do

46
00:03:08,470 --> 00:03:13,710
a history of some of the standard computer science literature and solving systems that selected

47
00:03:13,710 --> 00:03:18,190
direct methods gauss elimination this is still a matter to be uniform if you only

48
00:03:18,190 --> 00:03:22,350
have a few thousand variables so that's where we're going to start because in fact

49
00:03:22,350 --> 00:03:26,670
what happens in all modern systems is what you're going to do that is to

50
00:03:26,670 --> 00:03:30,370
reduce the problem to smaller size in this in this fall size goes below a

51
00:03:30,370 --> 00:03:34,660
certain level you're going to go to whatever the method that best for that size

52
00:03:34,710 --> 00:03:36,560
in particular

53
00:03:36,570 --> 00:03:41,880
let's look at the direct OK so in the seventies

54
00:03:42,060 --> 00:03:48,290
graph theoretic interpretation gauss elimination was popularized probably was known before that but that's what

55
00:03:48,290 --> 00:03:52,560
you do is the view every nonzero off diagonal edge

56
00:03:52,690 --> 00:03:54,680
value is undirected edge

57
00:03:56,180 --> 00:03:59,600
you view pivoting as a graph operation OK

58
00:03:59,620 --> 00:04:03,910
so in particular what is pivoting for

59
00:04:03,910 --> 00:04:08,150
from the graph theoretic point of view is you have some brass free original matrix

60
00:04:08,150 --> 00:04:12,290
and you have a vertex and you'd like to pivot on the vertex so what

61
00:04:12,290 --> 00:04:15,040
you end up doing is you make a clique out of the neighborhood of that

62
00:04:16,920 --> 00:04:20,860
you then remove the that vertex and its edges from the graph and you repeat

63
00:04:20,860 --> 00:04:22,770
until you have eliminated the graph

64
00:04:22,770 --> 00:04:25,250
come up heads or tails that's the thing you have to learn

65
00:04:25,250 --> 00:04:28,100
but given m trials

66
00:04:28,140 --> 00:04:32,890
and we observe x successes x times the coin came up heads we can estimate

67
00:04:32,890 --> 00:04:37,560
the probability is x and there's very nice found very standard testing downstream of found

68
00:04:37,690 --> 00:04:39,000
things that say

69
00:04:39,000 --> 00:04:42,890
how many times you have to flip a coin before this estimate this empirical estimate

70
00:04:43,060 --> 00:04:46,910
is really close with high probability to the true value

71
00:04:47,060 --> 00:04:49,520
so this is this you can you can think of this as a quick learning

72
00:04:49,520 --> 00:04:53,190
algorithm in that somebody gives you coin you have to predict what's the probability comes

73
00:04:53,190 --> 00:04:55,370
up heads and you have to be right

74
00:04:55,390 --> 00:04:59,500
there were said no but only no smaller times

75
00:04:59,520 --> 00:05:02,440
so we can quickly is probably by just saying i don't know

76
00:05:02,460 --> 00:05:06,560
up until we've flipped it enough times that this bound kicks in and we can

77
00:05:06,560 --> 00:05:09,290
be epsilon accurate with high probability

78
00:05:09,310 --> 00:05:12,440
so quickly learning the point is no big deal people know how to do this

79
00:05:12,440 --> 00:05:15,170
this is the standard building block

80
00:05:15,190 --> 00:05:19,000
but the things in addition to the coin probability that we can quickly and so

81
00:05:19,020 --> 00:05:23,790
we anything that we can quickly learn we can learn

82
00:05:23,790 --> 00:05:29,770
the hypothesis class that says that the output is

83
00:05:29,810 --> 00:05:31,910
the output of some quick learners

84
00:05:31,930 --> 00:05:36,590
so so what to well in this in particular case you can think of it

85
00:05:36,590 --> 00:05:39,350
as being that the dice learning problem so imagine that we're going to try to

86
00:05:39,350 --> 00:05:42,920
estimate that instead of always calling we've already died then we're gonna roll it it

87
00:05:42,920 --> 00:05:47,270
comes up one two three four five or six with unknown probabilities well we can

88
00:05:47,270 --> 00:05:50,370
treat them that as being six separate points

89
00:05:50,460 --> 00:05:53,560
and what we're doing is on each trial predicting the output of each of those

90
00:05:53,560 --> 00:05:55,790
points simultaneously so

91
00:05:55,790 --> 00:05:59,120
so we can quickly to the queen probabilities and what this is saying is that

92
00:05:59,120 --> 00:06:01,910
if we have a vector of something that we can quickly learn then we can

93
00:06:01,910 --> 00:06:03,500
quickly that hold back to

94
00:06:03,520 --> 00:06:06,310
so we could do dice learning is actually better ways of doing it and thinking

95
00:06:06,310 --> 00:06:10,140
of it is bunch of coins but that's one concrete example here another one is

96
00:06:10,140 --> 00:06:14,230
a forum for learning a map from some input space to some output space

97
00:06:14,250 --> 00:06:19,640
and the input is partitioned into regions where each of those is kwik learnable

98
00:06:19,660 --> 00:06:23,230
then we can quickly in the whole space and that's a really easy result as

99
00:06:23,230 --> 00:06:26,790
well it just says that we run a separate quick learner for each of the

100
00:06:26,790 --> 00:06:29,000
partitions of the input

101
00:06:29,000 --> 00:06:32,600
and then when it comes in we just as the corresponding one for the answer

102
00:06:32,810 --> 00:06:33,730
so that's

103
00:06:33,750 --> 00:06:35,790
this seems sort of two

104
00:06:35,810 --> 00:06:41,350
simple to be worth mentioning but combined this and that the coin probability and you

105
00:06:41,350 --> 00:06:45,350
can learn a standard transition function in an MDP because what is the transition function

106
00:06:45,350 --> 00:06:48,750
it's mapping the state the choice to stay in the choice of action that bus

107
00:06:48,750 --> 00:06:54,040
of the input space into into n times k separate problems to learn in each

108
00:06:54,040 --> 00:06:57,730
of those you have to learn the vector of probabilities which are the state transition

109
00:06:57,730 --> 00:07:03,540
probabilities to each possible next each of those values itself is just bernoulli trials

110
00:07:04,270 --> 00:07:10,790
by combining these three levels which can actually quickly learn entire transition function and therefore

111
00:07:10,810 --> 00:07:17,750
quick we can we can solve report learning problems efficiently in this in this

112
00:07:17,770 --> 00:07:21,690
flat setting where each state action pair is considered to be a separate problem so

113
00:07:21,710 --> 00:07:24,040
the hand

114
00:07:24,080 --> 00:07:34,660
now when you see cardinality

115
00:07:34,710 --> 00:07:37,930
it's like the partitions of the input space that was partitions here

116
00:07:37,940 --> 00:07:40,870
we don't have the numbers state

117
00:07:46,020 --> 00:07:50,000
this was

118
00:07:50,850 --> 00:07:52,410
i see i see

119
00:07:52,430 --> 00:07:56,790
and we want to bound grows with unknown number of states say yes these these

120
00:07:56,790 --> 00:07:58,290
algorithms do that as well they

121
00:07:58,310 --> 00:08:03,100
ten not actually need to know necessarily the number of states

122
00:08:03,120 --> 00:08:06,370
but they behave well with respect to what the number of states actually is the

123
00:08:06,370 --> 00:08:10,350
the other question is what are you talking about known input partition what if it's

124
00:08:10,350 --> 00:08:13,390
unknown what if i want to state act act like each other but we don't

125
00:08:13,390 --> 00:08:16,350
know which ones we have to discover which one is correct which i will get

126
00:08:16,350 --> 00:08:17,230
to that

127
00:08:17,250 --> 00:08:20,580
but this is kind of the the lowest level basic is just a regular MDP

128
00:08:20,580 --> 00:08:26,000
the way bellman talked about them we can now efficiently learn in that setting how

129
00:08:26,000 --> 00:08:30,480
to behave with unknown initially unknown transition and reward functions

130
00:08:33,250 --> 00:08:38,210
i think

131
00:08:46,370 --> 00:08:49,520
so if it were

132
00:08:49,600 --> 00:08:54,410
OK so so if if we have a deterministic system where you can actually choose

133
00:08:54,410 --> 00:08:57,890
you know when you take an action always goes to the same next state exploring

134
00:08:57,890 --> 00:09:01,790
that graph getting the same kind of bounds is easy you just get his basically

135
00:09:01,790 --> 00:09:05,390
like that for sir thomas you just go to any any transition have visited yet

136
00:09:05,390 --> 00:09:09,120
you try which the reason that this isn't just completely trivial

137
00:09:09,160 --> 00:09:13,730
is in the MDP setting there's probabilities involved it may be hard to get yourself

138
00:09:13,730 --> 00:09:15,310
to state to learn about it

139
00:09:15,330 --> 00:09:18,540
so you can't really just reason about saying OK we'll just i can quickly visit

140
00:09:18,540 --> 00:09:21,810
all the states and learn about all the states because you can't force yourself to

141
00:09:21,810 --> 00:09:25,100
a can put itself in interstate necessarily

142
00:09:25,120 --> 00:09:26,560
so you need you need more

143
00:09:26,580 --> 00:09:30,910
flexible notion of what optimality means they can account for the case where

144
00:09:30,940 --> 00:09:33,790
maybe the state you can get two or maybe can only get away with really

145
00:09:33,810 --> 00:09:35,390
low probability

146
00:09:35,410 --> 00:09:37,710
you don't count that against the learner

147
00:09:37,740 --> 00:09:43,810
that that's the question about his

148
00:09:43,830 --> 00:09:46,460
where it is

149
00:09:46,480 --> 00:09:48,460
this is

150
00:09:48,480 --> 00:09:49,660
it is the

151
00:09:54,370 --> 00:09:57,540
what i

152
00:09:57,600 --> 00:10:01,020
so so the question is what happens if you say there are parts of the

153
00:10:01,020 --> 00:10:04,510
state space that are very very difficult to learn it's not the right sort of

154
00:10:04,510 --> 00:10:09,560
rapidly mixing markov chain kind of situation and that's OK these these results will still

155
00:10:09,560 --> 00:10:13,040
apply they tell us that the stuff that we do get too often enough we're

156
00:10:13,040 --> 00:10:16,710
get a we're getting high reward if things are very hard to get to there

157
00:10:16,710 --> 00:10:19,690
may be very high we were there but it can have a big impact on

158
00:10:19,690 --> 00:10:20,960
our expected value

159
00:10:20,980 --> 00:10:24,100
because it's very hard to get there

160
00:10:24,120 --> 00:10:28,120
so the reward those are the sorts of issues that make the analysis tricky but

161
00:10:28,160 --> 00:10:31,580
think the top of story things are actually sort of nice and clean as long

162
00:10:31,580 --> 00:10:37,080
as we can quickly learn the transition function how horrible the state space is we

163
00:10:37,080 --> 00:10:42,790
get higher would with high probability with only a polynomial number mistakes

164
00:10:42,790 --> 00:10:47,830
OK so idea that these particular mention is quite notable classes because it gets up

165
00:10:47,830 --> 00:10:51,350
to transition probabilities there are the sort of things the union of two kwik learnable

166
00:10:51,350 --> 00:10:53,000
classes kwik learnable

167
00:10:53,020 --> 00:10:57,770
now mentioned a couple others as we go

168
00:10:57,830 --> 00:11:00,410
all right so just in case

169
00:11:00,430 --> 00:11:04,770
there were too many of songs and dances from it here's here's video of robot

170
00:11:04,770 --> 00:11:08,560
again this is not the robot in our lab is trapped in a four sided

171
00:11:08,560 --> 00:11:12,600
box but there's a door one side has to escape through the door to door

172
00:11:12,600 --> 00:11:14,640
over here you can see it

173
00:11:14,660 --> 00:11:19,890
and it's got there's that or it's got forward backward left right slide last slide

174
00:11:19,890 --> 00:11:24,370
right next step without turning left and right which have to be important not get

175
00:11:24,390 --> 00:11:30,060
stuck with your shoulder on the door it's if we describe discretize the state space

176
00:11:30,060 --> 00:11:36,790
into four thousand fifty states and the number of positions and orientations and then give

177
00:11:36,810 --> 00:11:43,210
one of these models base learners this armax words quick learning the transition probabilities

178
00:11:43,230 --> 00:11:46,560
to the system to learn it can actually learn this fairly effectively and the dog

179
00:11:46,560 --> 00:11:50,160
can escape from the box all is well in this case the dog is being

180
00:11:50,160 --> 00:11:52,690
tracked you can see it's got a little green

181
00:11:52,710 --> 00:11:57,340
the raspberry beret here in green park and those are being tracked from above mounted

182
00:11:57,340 --> 00:11:59,480
camera so that's how it knows where it is

183
00:11:59,500 --> 00:12:03,600
so you know they can make it work

184
00:12:03,620 --> 00:12:06,850
but i think it's a natural thing to say this point as well

185
00:12:06,890 --> 00:12:10,250
it's growing with the size of the state space if you have to learn about

186
00:12:10,250 --> 00:12:13,730
all the states in the environment most problems that we're thinking about the we care

187
00:12:13,730 --> 00:12:16,060
about have a very large number of states

188
00:12:16,940 --> 00:12:21,830
if we really view each of the states completely independent this transition knowledge doesn't transfer

189
00:12:21,830 --> 00:12:23,410
learning is going to be really slow

190
00:12:23,440 --> 00:12:27,140
these algorithms can actually even the really smart ones can do really stupid things under

191
00:12:27,140 --> 00:12:29,850
then you can again write that as an integral representation form

192
00:12:32,280 --> 00:12:36,190
so you can see what these what the underlying theme here is the underlying theme is that we

193
00:12:39,020 --> 00:12:39,640
if we want to do

194
00:12:40,410 --> 00:12:41,780
bayesian modeling and we want to

195
00:12:43,160 --> 00:12:46,120
we need a conditional independence like this to justify

196
00:12:46,930 --> 00:12:48,200
this assumption that we have

197
00:12:49,020 --> 00:12:54,490
that we have an independent random this part and some form of common information joint information that we can extract

198
00:12:55,070 --> 00:12:56,780
by statistical inference right

199
00:12:57,510 --> 00:12:58,660
and we need some kind of

200
00:12:59,360 --> 00:13:01,660
so and the way we can get that is

201
00:13:02,250 --> 00:13:05,600
by looking at some form of invariance in this case and the permutations

202
00:13:06,450 --> 00:13:08,620
and then finding a representation like this

203
00:13:09,780 --> 00:13:13,790
i can tell you it's not easy to find these representations but fortunately probabilities are

204
00:13:13,790 --> 00:13:16,510
doing all the work for us now so we don't actually have to do it

205
00:13:19,980 --> 00:13:21,110
there's a whole bunch of these

206
00:13:21,680 --> 00:13:25,980
these theorems already around that we can use four different types of data and so if we have

207
00:13:26,480 --> 00:13:29,040
based sequences of points and we have definitive theorem

208
00:13:29,560 --> 00:13:29,960
if we have

209
00:13:32,010 --> 00:13:34,300
if we have sequences in the sense of time series

210
00:13:34,700 --> 00:13:36,090
which in some sense evolve over time

211
00:13:37,360 --> 00:13:37,740
that's the

212
00:13:38,330 --> 00:13:40,490
here by diaconis and freedman which says that

213
00:13:43,490 --> 00:13:48,190
such sequences under certain invariance condition on mixtures of markov chains right

214
00:13:48,820 --> 00:13:51,280
here we have mixtures of eighty sequences and mixtures

215
00:13:51,710 --> 00:13:53,410
change because here we have a markov chains

216
00:13:54,560 --> 00:13:57,450
the case of partitions we have here making and which says we have

217
00:13:58,290 --> 00:14:03,760
this a copy books partition that is working and call them but it's basically sampling from these cluster weights

218
00:14:05,010 --> 00:14:05,670
if we um

219
00:14:06,260 --> 00:14:07,580
if you want to look at graphs are

220
00:14:08,490 --> 00:14:12,210
and then then there is also a theorem this is actually more generally applicable to

221
00:14:12,210 --> 00:14:13,980
erase and the idea was racist

222
00:14:15,060 --> 00:14:19,440
you think of you can think of a graph as a matrix right has the adjacency matrix

223
00:14:19,940 --> 00:14:21,740
and now you can think of making this matrix

224
00:14:22,470 --> 00:14:25,700
which is a two way array you can make that a three way array and so on yeah

225
00:14:27,550 --> 00:14:28,370
okay end

226
00:14:29,240 --> 00:14:29,740
and just to

227
00:14:30,230 --> 00:14:33,200
to close up this topic there's the there's kind of like

228
00:14:37,030 --> 00:14:37,690
the underlying

229
00:14:38,260 --> 00:14:41,820
mathematical theory which is the theory of regarding decompositions and this is

230
00:14:42,320 --> 00:14:44,810
really one of the most in my opinion one of the most

231
00:14:45,220 --> 00:14:47,430
stunningly beautiful topics in probability

232
00:14:48,010 --> 00:14:49,420
and what it basically gives you is

233
00:14:50,040 --> 00:14:52,610
you have those are the basic the basic common theme is

234
00:14:53,100 --> 00:14:54,350
you have a probability measure

235
00:14:55,680 --> 00:15:00,420
and it has invariance property that was what we saw was exchangeability you different forms

236
00:15:00,420 --> 00:15:02,680
of exchangeability of partitions on graphs and so on

237
00:15:04,200 --> 00:15:07,810
it can be expressed as exchangeability by saying is invariant under permutations

238
00:15:08,720 --> 00:15:10,080
permutations are group

239
00:15:10,920 --> 00:15:11,930
in mathematical terms

240
00:15:13,000 --> 00:15:14,040
and there's a lot of groups

241
00:15:14,730 --> 00:15:19,200
and what these groups what groups press expressed in mathematics of different forms of symmetry

242
00:15:19,690 --> 00:15:22,540
so we have a permutation invariance which is a form of symmetry you can have

243
00:15:22,590 --> 00:15:25,810
rotation invariance which is spatial symmetry and so on and so forth

244
00:15:27,590 --> 00:15:30,800
thee the basic theme is you have a probability distribution

245
00:15:31,580 --> 00:15:35,020
end it is invariant under some form of symmetry

246
00:15:35,760 --> 00:15:38,610
which is formalized by a group and that gives you an integral decomposition

247
00:15:40,220 --> 00:15:41,710
it's truly awesome okay

248
00:15:44,890 --> 00:15:45,480
so much for their

249
00:15:46,000 --> 00:15:46,890
so this is like

250
00:15:48,110 --> 00:15:51,820
basically if we want to construct new bayesian models on new types of data

251
00:15:53,120 --> 00:15:55,840
four for a given type of data we first have to

252
00:15:57,320 --> 00:15:58,190
solve some

253
00:15:58,880 --> 00:16:02,940
fundamental questions before we can really start modelling was designing specific models

254
00:16:03,440 --> 00:16:07,220
but we don't have to do this over and over again for each model we want to design right

255
00:16:07,780 --> 00:16:11,980
just have to do this once more this time after graphs of all permutations

256
00:16:12,530 --> 00:16:14,110
four four partitions and so on

257
00:16:27,770 --> 00:16:28,370
okay after

258
00:16:30,240 --> 00:16:33,010
after all this applied topics let me get a something more theoretical

259
00:16:41,580 --> 00:16:41,990
all right so

260
00:16:42,660 --> 00:16:43,810
well i've talked about so far

261
00:16:45,640 --> 00:16:50,060
different forms of modeling right modeling end a little bit yesterday about applications

262
00:16:50,780 --> 00:16:51,940
and there are some

263
00:16:52,690 --> 00:16:53,940
there's a different line of work

264
00:16:54,490 --> 00:16:56,990
that that's happening in in mathematical statistics

265
00:16:57,500 --> 00:16:58,930
great don't ask the question

266
00:16:59,870 --> 00:17:03,850
how can we build new models but rather ask if before the models that we already have

267
00:17:04,430 --> 00:17:05,280
how they behave

268
00:17:05,920 --> 00:17:09,890
and and what statisticians always want to know is if i if i see more and more samples

269
00:17:10,510 --> 00:17:15,080
that's a model how quickly doesn't converge to the right parameter values right how how

270
00:17:15,080 --> 00:17:19,170
quickly can i how much data do i need to estimate my parameters in order

271
00:17:19,170 --> 00:17:19,740
to get a good

272
00:17:21,230 --> 00:17:22,270
good posterior

273
00:17:27,950 --> 00:17:29,870
about the last ten years some

274
00:17:31,380 --> 00:17:37,830
very good mathematical statisticians have started to applying this idea that are rather ask this question about bayesian nonparametric models

275
00:17:37,830 --> 00:17:39,680
using logistic units

276
00:17:39,740 --> 00:17:41,940
but with this gaussian model of the pixels

277
00:17:41,950 --> 00:17:43,790
on small colour images

278
00:17:43,830 --> 00:17:46,340
and you can't see a thing unless we turn the lights turned can we just

279
00:17:46,340 --> 00:17:48,920
turn the lights off for this one thing?

280
00:17:48,980 --> 00:17:51,190
OK what you can see is

281
00:17:51,200 --> 00:17:53,550
when we train on a million color images

282
00:17:53,550 --> 00:17:57,800
each of which is three 32 by 32, just the RGB

283
00:17:59,550 --> 00:18:04,070
we take those weights, so we've got three sets of 32 by 32 weights,

284
00:18:04,150 --> 00:18:09,050
and we display them by treating those three sets of weights from a binary hidden unit

285
00:18:09,060 --> 00:18:11,690
as the RGB channels

286
00:18:11,710 --> 00:18:14,070
and what you notice is that almost all of them

287
00:18:14,110 --> 00:18:19,200
are completely uncoloured, which means they have exactly balanced weights to the r g and

288
00:18:19,200 --> 00:18:20,840
b channels

289
00:18:20,880 --> 00:18:24,480
in other words they deliberately learned not to be color-tuned,

290
00:18:24,510 --> 00:18:29,320
and they tried very hard not to be color-tuned

291
00:18:29,330 --> 00:18:31,860
and they're all the high frequency ones

292
00:18:31,910 --> 00:18:35,410
and then there's complete separation, almost complete

293
00:18:36,830 --> 00:18:40,570
and there's a few non color-tuned

294
00:18:40,620 --> 00:18:42,130
low frequency ones

295
00:18:42,140 --> 00:18:45,020
but basically it's high-frequency non color-tuned

296
00:18:45,090 --> 00:18:47,830
and then low-frequency color tuned,

297
00:18:47,840 --> 00:18:49,560
and they're

298
00:18:49,580 --> 00:18:52,400
they're basically two colours

299
00:18:53,100 --> 00:18:56,090
they're sort of purple green just because of the funny

300
00:18:56,100 --> 00:18:58,120
colour balance here. they're purple green

301
00:18:58,140 --> 00:19:00,200
and red blue

302
00:19:05,210 --> 00:19:06,520
actually, no, they're

303
00:19:06,520 --> 00:19:11,580
that's, that's an odd one. that's is the other color you get. you get purple green and you get

304
00:19:12,260 --> 00:19:14,630
sort of yellow

305
00:19:14,640 --> 00:19:19,230
and these are low frequency

306
00:19:19,280 --> 00:19:22,970
so that's really weird because the human visual system's like that. you've got low frequency

307
00:19:23,760 --> 00:19:26,410
things and high-frequency black-and-white things

308
00:19:26,420 --> 00:19:28,410
and it's an efficient way to do business

309
00:19:28,430 --> 00:19:31,740
jpeg in fact uses ideas like this to do compression. but this isn't to do

310
00:19:31,740 --> 00:19:33,140
with jpeg artifacts.

311
00:19:34,540 --> 00:19:37,980
but it's there in the data, right? what this is saying is in the data

312
00:19:37,980 --> 00:19:39,560
from the visual world

313
00:19:39,570 --> 00:19:41,400
there's lots of evidence for

314
00:19:41,420 --> 00:19:44,780
high frequency edges that don't have any colour contrast

315
00:19:44,810 --> 00:19:47,870
and i think that's internal edges in an object

316
00:19:47,880 --> 00:19:50,530
like sort of folds in my cheek. the colour of the surface is the same

317
00:19:50,530 --> 00:19:51,580
both sides

318
00:19:51,710 --> 00:19:55,910
because illumination hasn't changed and the material hasn't changed

319
00:19:55,980 --> 00:19:58,240
of course, in an occluding image,

320
00:19:58,290 --> 00:20:00,580
there's different colors on either side

321
00:20:00,650 --> 00:20:03,520
but there's something about the data that says this is a really good idea

322
00:20:03,560 --> 00:20:06,080
and the colour you would handle in a low frequency channel

323
00:20:06,130 --> 00:20:10,350
but the main thing is to show this learns nice filters.

324
00:20:10,400 --> 00:20:15,960
now i want to go on to

325
00:20:17,180 --> 00:20:20,770
can you remember which ones to do?

326
00:20:20,810 --> 00:20:25,260
was it?

327
00:20:29,710 --> 00:20:32,160
now i want to talk about

328
00:20:33,540 --> 00:20:35,370
these deep belief net ideas

329
00:20:35,440 --> 00:20:37,510
with ideas about

330
00:20:37,510 --> 00:20:38,790
gaussian processes

331
00:20:38,800 --> 00:20:41,270
that's a sort of kernel method for doing regression

332
00:20:42,840 --> 00:20:44,140
deep belief nets do

333
00:20:44,170 --> 00:20:47,390
really well when most of data's unlabelled, because they can learn these layers of

334
00:20:47,390 --> 00:20:49,420
features without needing labels

335
00:20:49,470 --> 00:20:52,970
they can learn appropriate features that are going to be good for discrimination

336
00:20:53,020 --> 00:20:56,860
because they've learned how to get the stuff, and then all we have to do is get from

337
00:20:56,860 --> 00:20:58,560
the stuff to the labels,

338
00:20:58,580 --> 00:21:01,470
uses fine tuning.

339
00:21:01,490 --> 00:21:04,230
if you take gaussian processes and you're

340
00:21:04,300 --> 00:21:08,830
naive about them because you don't really believe in them, then

341
00:21:08,840 --> 00:21:11,340
they don't scale very well

342
00:21:11,390 --> 00:21:15,070
but they're very good if you don't have much data, much labelled data

343
00:21:15,080 --> 00:21:19,530
and they're very good at not seeing--not seeing structure where there's noise

344
00:21:23,760 --> 00:21:25,550
so you can combine the two

345
00:21:25,560 --> 00:21:28,320
what we do is we learn deep belief nets

346
00:21:28,330 --> 00:21:30,160
using no labels

347
00:21:30,260 --> 00:21:32,100
then we take

348
00:21:32,100 --> 00:21:33,810
the top level of features

349
00:21:33,860 --> 00:21:38,060
and we apply a gaussian process to it, try and predict the labels

350
00:21:38,080 --> 00:21:39,920
and that already does much better than

351
00:21:39,940 --> 00:21:42,620
just applying the gaussian process to the raw data

352
00:21:42,660 --> 00:21:45,360
'cause we made use of all this unlabelled data, we get nice features

353
00:21:45,580 --> 00:21:47,190
but then

354
00:21:47,210 --> 00:21:52,170
even though gaussian processes are slow, if we don't have much labelled data, we can back propagate derivatives through

355
00:21:52,170 --> 00:21:53,730
the deep belief net

356
00:21:53,740 --> 00:21:57,880
so what you're doing is now learning a much better kernel for the gaussian process.

357
00:21:57,880 --> 00:22:00,570
that involves a lot of iterations of the gaussian process, but if you don't have

358
00:22:00,570 --> 00:22:01,890
much labelled data that's fine

359
00:22:01,960 --> 00:22:05,360
so this is really good for the regime of lots of unlabeled data and very

360
00:22:05,360 --> 00:22:08,070
little labelled data and doing regression

361
00:22:09,800 --> 00:22:12,620
so here's the example that rus did

362
00:22:13,980 --> 00:22:17,420
you take a patch of a face that might--and you're trying to figure out what orientation it's

363
00:22:18,900 --> 00:22:20,400
you learn

364
00:22:20,680 --> 00:22:22,120
gaussian units here

365
00:22:22,150 --> 00:22:24,600
binary units here, learn an RBM

366
00:22:24,600 --> 00:22:27,790
take this as data for learning the next RBM, take this as data for the next

367
00:22:27,790 --> 00:22:33,620
RBM, so we've learned one two three layers of a thousand features

368
00:22:33,640 --> 00:22:35,760
then stick it all together

369
00:22:35,760 --> 00:22:44,530
and apply GP on top that's trying to predict the orientation of the face

370
00:22:46,140 --> 00:22:52,740
OK, rus tried various things. i--fairly sure it's just--it's the

371
00:22:52,820 --> 00:22:57,290
negative squared exponential

372
00:22:57,490 --> 00:23:05,130
just so as not to be confusing

373
00:23:05,140 --> 00:23:11,450
the exponential of minus x squared.

374
00:23:11,460 --> 00:23:13,560
so here's what

375
00:23:13,560 --> 00:23:14,870
training data looks like

376
00:23:14,880 --> 00:23:17,720
here's the orientations of these face patches

377
00:23:19,960 --> 00:23:24,080
unlabeled test data

378
00:23:24,150 --> 00:23:28,890
no, here's unlabeled training cases, and then here's the test data

379
00:23:28,900 --> 00:23:35,130
and he tries three different under numbers of labelled cases. when you get to about a

380
00:23:35,130 --> 00:23:37,420
thousand, the gaussian process is labouring

381
00:23:37,450 --> 00:23:39,420
because you have to do it many times

382
00:23:39,520 --> 00:23:40,910
because you want a derivative from it

383
00:23:41,380 --> 00:23:46,520
and here's the results

384
00:23:46,560 --> 00:23:50,070
if you apply a gaussian process to the raw pixels

385
00:23:50,090 --> 00:23:55,650
you get 22% error--22% mean squared error because it's regression--with hundred labels,

386
00:23:55,650 --> 00:23:57,870
and sixteen percent with a thousand labels

387
00:23:57,930 --> 00:24:04,120
with only a hundred labels i can beat this

388
00:24:04,210 --> 00:24:05,340
over here

389
00:24:07,600 --> 00:24:08,930
you apply the GP

390
00:24:08,950 --> 00:24:11,440
on top of this deep network

391
00:24:11,480 --> 00:24:15,520
and you also back propagate derivatives from the GP to fine-tune the kernel

392
00:24:15,520 --> 00:24:18,960
and so you get something with a hundred labels that's better than what you got here

393
00:24:18,970 --> 00:24:20,440
with a thousand labels

394
00:24:20,460 --> 00:24:23,550
alternatively you can get something which has

395
00:24:23,600 --> 00:24:26,010
much less than half the error

396
00:24:26,020 --> 00:24:28,590
mean squared error

397
00:24:28,590 --> 00:24:30,020
was that people got

398
00:24:30,070 --> 00:24:32,630
confused even even skilled programmers

399
00:24:32,650 --> 00:24:34,650
we're confused when they had to

400
00:24:34,670 --> 00:24:35,580
so class

401
00:24:35,610 --> 00:24:39,890
things to in order to use the software the logic programming

402
00:24:39,930 --> 00:24:41,680
this is something that people are familiar with

403
00:24:41,820 --> 00:24:44,200
but it's

404
00:24:44,240 --> 00:24:48,890
it's still not completely obvious it's still it's still of kernel

405
00:24:48,910 --> 00:24:50,230
but it's

406
00:24:50,240 --> 00:24:52,890
the users of software would prefer not to have to

407
00:24:52,970 --> 00:24:54,640
to go over just said

408
00:24:54,670 --> 00:24:57,950
some software music this was the least my experience

409
00:24:57,960 --> 00:24:59,690
in the in the

410
00:24:59,740 --> 00:25:02,740
early nineties

411
00:25:02,760 --> 00:25:08,100
i think that's still the case of this something to them

412
00:25:09,070 --> 00:25:15,540
i left school and worked excite for three years

413
00:25:15,570 --> 00:25:21,610
running the writing software there was originally written by another fellow not taken over early

414
00:25:23,730 --> 00:25:29,930
it was the indexing here was actually performed pretty well was based purely on on

415
00:25:29,940 --> 00:25:34,620
merging of indexes you may make indexes in memory writing to disk

416
00:25:34,780 --> 00:25:39,710
and then you when you start the process of hierarchically merging indexes into larger indexes

417
00:25:43,250 --> 00:25:48,900
looks like we we have four levels of merging with fixed that process

418
00:25:48,970 --> 00:25:52,200
you can skip a lot of this

419
00:25:52,210 --> 00:25:55,350
we got to be a fairly large collection for the day

420
00:25:55,370 --> 00:26:01,210
and for a while we were the largest search engine on the web at the

421
00:26:01,340 --> 00:26:05,400
at the picnic site had two hundred fifty million pages indexed

422
00:26:05,440 --> 00:26:09,580
and was i i think around and i forget exactly

423
00:26:09,590 --> 00:26:11,350
what the traffic volume ones

424
00:26:11,360 --> 00:26:13,530
and it was always one of these

425
00:26:13,640 --> 00:26:15,890
secrets not supposed to talk about

426
00:26:15,910 --> 00:26:18,200
so i don't have anything written down anywhere about

427
00:26:18,450 --> 00:26:24,100
traffic but it was around a thousand miles per second at peak of the data

428
00:26:24,180 --> 00:26:27,800
so things i learned here was that

429
00:26:27,900 --> 00:26:29,310
merging indexes

430
00:26:29,320 --> 00:26:30,970
it's something that

431
00:26:31,030 --> 00:26:33,140
scales very well

432
00:26:33,190 --> 00:26:36,180
i also learned that

433
00:26:36,190 --> 00:26:39,990
writing software that runs on the server is is a very nice

434
00:26:40,030 --> 00:26:45,020
way to develop software as opposed to writing software that

435
00:26:45,030 --> 00:26:47,350
people then installed on their on their

436
00:26:47,360 --> 00:26:52,910
the computers and you have to maintain installations in the field in lots of different

437
00:26:52,910 --> 00:26:58,700
environments if you can control the environment that is delivered on and even in solid

438
00:26:58,700 --> 00:26:59,800
yourself in the

439
00:26:59,850 --> 00:27:04,110
i mean it's much easier to do new releases daily or weekly

440
00:27:04,130 --> 00:27:05,420
as often as you like

441
00:27:05,430 --> 00:27:07,590
and you can also have

442
00:27:07,640 --> 00:27:12,430
a good sense of what the what's hardware platform is going to be on and

443
00:27:12,430 --> 00:27:15,480
therefore guaranteed certain performance properties

444
00:27:15,530 --> 00:27:18,740
and so that was that was really nice is first someone worked on server side

445
00:27:20,250 --> 00:27:25,790
and most preferred that lovely me to focus on things i was interested in c

446
00:27:25,790 --> 00:27:27,070
plus plus

447
00:27:28,210 --> 00:27:33,040
really fragile we have a lot of problems a lot of reliability problems when we're

448
00:27:33,040 --> 00:27:35,370
doing rapid development using c plus plus

449
00:27:37,070 --> 00:27:43,500
and i actually started while i was still working excite working job and that that

450
00:27:43,510 --> 00:27:49,240
say there's a lot of thought has also worked work very well as a replacement

451
00:27:49,250 --> 00:27:52,360
the last lesson that i learned excite was that

452
00:27:52,440 --> 00:27:53,960
when the company went bankrupt

453
00:27:53,970 --> 00:27:59,620
in figure around two thousand one out business all the software that was written there

454
00:27:59,670 --> 00:28:01,680
and there was a lot was in early

455
00:28:01,790 --> 00:28:03,650
fairly large company that live for

456
00:28:03,670 --> 00:28:05,150
six or so years

457
00:28:06,670 --> 00:28:07,600
it's gone

458
00:28:07,640 --> 00:28:09,490
was it's

459
00:28:10,500 --> 00:28:17,060
owned by some someone is the result of some bankruptcy-court proceedings but legally no one

460
00:28:17,090 --> 00:28:22,130
but i know i can access the software it's disappeared effectively from from the face

461
00:28:22,130 --> 00:28:23,830
of the earth which is

462
00:28:23,840 --> 00:28:26,580
so i mean we

463
00:28:26,850 --> 00:28:31,750
now if you think of software has written knowledge we this sort of stuff that

464
00:28:31,750 --> 00:28:32,830
we keep it

465
00:28:32,840 --> 00:28:36,790
libraries universities history may may not be great literature but

466
00:28:36,800 --> 00:28:39,030
it is something that we would typically

467
00:28:39,040 --> 00:28:42,580
i don't like the throwaway stuff like that the people who worked hard to create

468
00:28:42,620 --> 00:28:43,800
and yet

469
00:28:43,850 --> 00:28:45,230
in this context

470
00:28:45,240 --> 00:28:49,360
proprietary software can simply disappear

471
00:28:49,400 --> 00:28:53,700
and not everything that was worked on their effectively harnessed

472
00:28:57,130 --> 00:28:58,450
i'm going to digress

473
00:28:58,520 --> 00:29:00,270
before i come back to

474
00:29:00,350 --> 00:29:01,230
the seen

475
00:29:01,540 --> 00:29:08,230
i mentioned several times this issue of

476
00:29:08,250 --> 00:29:09,650
index merging

477
00:29:10,020 --> 00:29:14,310
and i one i want to go into this little more detail it's sort of

478
00:29:14,310 --> 00:29:15,630
basic computer science

479
00:29:15,730 --> 00:29:18,340
but it's it's a confusion a lot of people

480
00:29:18,390 --> 00:29:21,890
seem to seem to make the trees are the

481
00:29:21,910 --> 00:29:23,090
building block of

482
00:29:23,130 --> 00:29:25,380
relational databases primarily

483
00:29:25,430 --> 00:29:27,470
that's that's that's so the

484
00:29:27,480 --> 00:29:32,480
well as far as i know every major relational database at its core is is

485
00:29:32,480 --> 00:29:35,640
using the trees for four indexes on the table

486
00:29:35,720 --> 00:29:40,300
it's a it's a very useful data structure

487
00:29:40,310 --> 00:29:42,490
it in general case

488
00:29:42,540 --> 00:29:43,880
requires login

489
00:29:43,890 --> 00:29:47,470
six or access for either read or write

490
00:29:47,560 --> 00:29:52,200
but in general you can optimize things so you only have to do one seek

491
00:29:53,630 --> 00:29:54,410
to right

492
00:29:54,420 --> 00:29:56,980
it and

493
00:29:57,080 --> 00:29:58,210
if you

494
00:29:58,260 --> 00:30:03,840
take advantage of axes and new for them and sort then you can actually

495
00:30:03,910 --> 00:30:06,380
go optimizing things somewhat more

496
00:30:07,520 --> 00:30:12,870
so the only access each page once per update so the two works as this

497
00:30:12,870 --> 00:30:16,750
top level index is a simplified version of it to little bit rate

498
00:30:16,760 --> 00:30:19,040
it tells you that everything

499
00:30:19,060 --> 00:30:20,340
the before g

500
00:30:20,350 --> 00:30:21,380
is over here

501
00:30:21,390 --> 00:30:23,230
between GNO is here

502
00:30:23,240 --> 00:30:25,350
between o and use over here

503
00:30:25,360 --> 00:30:29,550
so the the top level things so you divide things up and

504
00:30:29,570 --> 00:30:35,270
works because we see how blocks of data we do you see the disk your

505
00:30:35,280 --> 00:30:37,560
block of data

506
00:30:37,630 --> 00:30:40,460
but the problem is that you can actually build the tree

507
00:30:40,470 --> 00:30:45,640
and then sorted accesses and go through and by access in sequential order but over

508
00:30:45,640 --> 00:30:50,180
time you have to end up serving as as work is getting started inserting another

509
00:30:50,180 --> 00:30:51,540
page between here

510
00:30:51,550 --> 00:30:54,510
and you'll end up setting another one in here

511
00:30:54,520 --> 00:30:58,300
and a new chapel things around i'm going to go into too much detail but

512
00:30:58,300 --> 00:31:00,420
that's like

513
00:31:02,590 --> 00:31:05,560
and the most popular

514
00:31:05,570 --> 00:31:09,340
actually the most popular features

515
00:31:09,350 --> 00:31:11,550
texture features used

516
00:31:14,620 --> 00:31:17,550
also have a special way

517
00:31:17,580 --> 00:31:19,050
special kind of

518
00:31:19,810 --> 00:31:26,770
we can make it possible to be like mother like and we can think about

519
00:31:28,400 --> 00:31:29,570
he was

520
00:31:31,310 --> 00:31:33,050
one of the ways

521
00:31:33,100 --> 00:31:35,840
and like can

522
00:31:35,950 --> 00:31:37,060
that year

523
00:31:40,090 --> 00:31:43,160
five which is not right

524
00:31:43,210 --> 00:31:45,480
the presentation

525
00:31:45,500 --> 00:31:46,790
we so

526
00:31:46,800 --> 00:31:48,960
you see that

527
00:31:51,060 --> 00:31:52,780
there is no

528
00:31:52,790 --> 00:31:56,310
he was the

529
00:31:56,330 --> 00:31:59,270
while the full scale so

530
00:31:59,270 --> 00:32:00,890
as before

531
00:32:00,950 --> 00:32:06,080
when we change the parameters have differing functions

532
00:32:06,100 --> 00:32:09,520
which is actually composed of bank

533
00:32:09,530 --> 00:32:11,540
so for every value

534
00:32:11,550 --> 00:32:19,260
this problem one see one for in this particular case we have

535
00:32:24,230 --> 00:32:25,140
we don't

536
00:32:25,160 --> 00:32:26,480
which it was

537
00:32:26,530 --> 00:32:27,560
he that

538
00:32:28,860 --> 00:32:29,920
and the way

539
00:32:29,940 --> 00:32:32,400
somewhere in just of before

540
00:32:34,640 --> 00:32:38,860
that's the way sections

541
00:32:38,860 --> 00:32:42,750
actually the same idea from

542
00:32:42,770 --> 00:32:45,400
and you want changes very

543
00:32:45,440 --> 00:32:49,380
independent component internet so this also

544
00:32:49,380 --> 00:32:54,650
a kind of the competition but instead of looking at the data

545
00:32:54,670 --> 00:32:58,150
that way we do could

546
00:32:58,480 --> 00:33:00,040
big problem

547
00:33:00,090 --> 00:33:02,860
which i think the real images

548
00:33:02,880 --> 00:33:03,940
by one

549
00:33:03,960 --> 00:33:05,820
depend on one another

550
00:33:06,590 --> 00:33:10,070
we know a lot of pages of some images

551
00:33:10,110 --> 00:33:12,400
and we try to understand what the

552
00:33:12,440 --> 00:33:15,460
the same way that this image is it

553
00:33:17,810 --> 00:33:19,230
so tried to

554
00:33:20,770 --> 00:33:22,420
i really like

555
00:33:22,440 --> 00:33:24,090
article which is

556
00:33:24,090 --> 00:33:25,340
so we

557
00:33:25,340 --> 00:33:28,790
try to find the most independent

558
00:33:28,820 --> 00:33:31,020
this debate we have

559
00:33:31,040 --> 00:33:32,420
you waited

560
00:33:33,880 --> 00:33:35,460
this training

561
00:33:35,480 --> 00:33:36,520
when we have the

562
00:33:37,360 --> 00:33:41,770
bank of that they can do exactly the same so we

563
00:33:41,810 --> 00:33:43,000
two pages

564
00:33:43,040 --> 00:33:44,690
we can it

565
00:33:46,670 --> 00:33:51,190
and was

566
00:33:51,320 --> 00:33:55,000
well a picture no single energy but he

567
00:33:55,040 --> 00:33:59,360
of the responses so that for example and

568
00:33:59,380 --> 00:34:02,310
so the result in histogram

569
00:34:02,320 --> 00:34:05,570
and and we know where you ever there

570
00:34:05,610 --> 00:34:06,670
it's a

571
00:34:06,730 --> 00:34:10,570
which is one of the image of the particle

572
00:34:10,570 --> 00:34:13,320
using all but

573
00:34:13,340 --> 00:34:16,170
division actually to measure

574
00:34:16,290 --> 00:34:17,070
the five

575
00:34:17,090 --> 00:34:21,960
this between two images you simply like just south

576
00:34:22,900 --> 00:34:25,310
o barkley individual

577
00:34:25,320 --> 00:34:27,000
all right

578
00:34:31,320 --> 00:34:32,550
the application

579
00:34:32,570 --> 00:34:34,090
so does

580
00:34:36,070 --> 00:34:38,690
the result

581
00:35:20,190 --> 00:35:22,840
development which

582
00:35:22,860 --> 00:35:25,860
i don't have to be able to measure

583
00:35:31,980 --> 00:35:34,090
the the

584
00:35:34,180 --> 00:35:41,190
so i

585
00:35:42,820 --> 00:35:45,380
so you can see why

586
00:35:45,400 --> 00:35:48,110
independent of

587
00:35:50,980 --> 00:35:54,230
so there is no such as like that

588
00:35:54,250 --> 00:35:55,750
this one

589
00:35:55,770 --> 00:35:57,590
to the book to describes

590
00:35:59,320 --> 00:36:03,440
in the right direction

591
00:36:03,460 --> 00:36:09,190
which is to try to class high

592
00:36:09,250 --> 00:36:11,290
that particular direction

593
00:36:11,310 --> 00:36:14,920
and i think for just

594
00:36:14,940 --> 00:36:19,320
when wiring

595
00:36:19,340 --> 00:36:23,210
a straight line like horizontal and vertical

596
00:36:23,270 --> 00:36:26,670
i this was very

597
00:36:26,810 --> 00:36:29,670
on the

598
00:36:29,690 --> 00:36:31,630
for this one

599
00:36:31,630 --> 00:36:35,300
you want to test that this is why it often a better strategies

600
00:36:35,310 --> 00:36:39,170
to use random search so random search instead of specifying a finite

601
00:36:39,170 --> 00:36:41,270
number of values for each hyperparameter

602
00:36:41,270 --> 00:36:44,080
you specify some distribution over some range

603
00:36:44,280 --> 00:36:46,390
for the hyperparameters maybe the number

604
00:36:46,390 --> 00:36:49,580
of hidden layer could be just a uniform distribution over one two

605
00:36:49,580 --> 00:36:52,160
and three but for the learning rate maybe

606
00:36:52,550 --> 00:36:55,860
i would have some log uniform distribution

607
00:36:56,470 --> 00:37:02,730
over some range and then i specify a in advance instead of obtaining

608
00:37:02,740 --> 00:37:06,430
from these choices a number expense any to run i can also separately

609
00:37:06,430 --> 00:37:10,200
determine how many experiments of i'm going to run how many configurations

610
00:37:10,200 --> 00:37:14,090
hyperparameter i'm going to use run a grid in this and with these

611
00:37:14,090 --> 00:37:17,000
choices and then ultimately compare between experiments

612
00:37:17,800 --> 00:37:21,490
and to get a configuration you just sample independently

613
00:37:21,650 --> 00:37:25,390
from each distribution for each parameter run that experiment

614
00:37:25,490 --> 00:37:28,820
and then report the number of validation and store that for all

615
00:37:28,820 --> 00:37:32,040
my experiments so what's really nice about this is that there's no

616
00:37:32,040 --> 00:37:35,530
relationship between the distribution you choose for the potential

617
00:37:35,530 --> 00:37:38,590
values hyperparameters and the number of experiments you have to

618
00:37:38,590 --> 00:37:42,730
do to sort of completely explore the space sufficiently explore

619
00:37:42,740 --> 00:37:47,660
the space one in to like one particularly back case for grid search

620
00:37:47,670 --> 00:37:51,630
is that if for one and that's often typical of the learning rate

621
00:37:51,640 --> 00:37:55,010
for one choice of the value of say the learning rate

622
00:37:55,360 --> 00:37:57,910
you for some reason get very bad results so

623
00:37:58,050 --> 00:38:01,130
that is quite typical the learning rate if it's too high a

624
00:38:01,810 --> 00:38:04,940
the training might divergen give you a very bad results

625
00:38:05,290 --> 00:38:08,720
no matter what other choices of values with the other parameters

626
00:38:08,720 --> 00:38:11,950
you've made so that means you've wasted a whole lot of experiments

627
00:38:11,950 --> 00:38:14,700
because you have a big chunk of experiments that are

628
00:38:15,370 --> 00:38:18,700
committing to this one choice of the parameter for the learning

629
00:38:18,700 --> 00:38:23,520
rate whereas in this case all hyperparameter are chosen independent

630
00:38:23,530 --> 00:38:26,920
the across all experiments so less likely to be

631
00:38:27,650 --> 00:38:32,060
a victim of this that situation and there are

632
00:38:32,250 --> 00:38:36,400
us smarter ways of also doing exploration hyperparameters

633
00:38:36,400 --> 00:38:39,940
there's a whole field on vision of the zation or sequential model-based

634
00:38:39,940 --> 00:38:42,830
optimization where which are trying to do here is

635
00:38:43,020 --> 00:38:47,330
actually have a model whose job is to learn a relationship between

636
00:38:47,610 --> 00:38:52,210
values of your hyperparameters and some prediction on what would

637
00:38:52,210 --> 00:38:54,240
be the performance on the validation set

638
00:38:54,240 --> 00:38:56,720
so if you doing experiments iteratively

639
00:38:56,720 --> 00:38:59,850
you can imagine i can use previous experiments to orient what

640
00:38:59,850 --> 00:39:02,530
kind of values on try out for my x experiments

641
00:39:02,650 --> 00:39:05,670
in fact as people doing research this is exactly where we

642
00:39:05,670 --> 00:39:10,070
do we often even in the random search case you do a bunch of experiments

643
00:39:10,080 --> 00:39:13,980
will be noticed that it seems like maybe you know the maximum value

644
00:39:13,980 --> 00:39:16,800
was allowing for some hyperparameter was too small i'm

645
00:39:16,800 --> 00:39:20,410
going to increase it there's still some iterative procedure area

646
00:39:20,420 --> 00:39:22,450
in this model explore version-space

647
00:39:22,610 --> 00:39:25,520
and vision optimization or sequential base optimization

648
00:39:25,730 --> 00:39:28,440
is trying to automate this and make this you know

649
00:39:28,700 --> 00:39:32,430
machine learnable i'm going to tell you do this but that you should

650
00:39:32,440 --> 00:39:36,100
be aware that this is another approach that can be better

651
00:39:38,200 --> 00:39:41,770
ok so yeah yes yes

652
00:40:23,210 --> 00:40:26,860
yes so so i guess the question is you know if you have a model making

653
00:40:26,860 --> 00:40:30,420
predictions s to try to model the relationship between hyperparameters

654
00:40:30,420 --> 00:40:34,330
selection choices and the prediction well doesn't that models

655
00:40:34,330 --> 00:40:37,360
have hyperparameters for which you need to select hyperparameters

656
00:40:37,360 --> 00:40:40,480
and that you know yet stack overflow essentially

657
00:40:41,250 --> 00:40:47,540
so in this case it it actually is a really good motivation for the

658
00:40:47,550 --> 00:40:50,830
asian approach of machine learning because in this case you

659
00:40:50,840 --> 00:40:54,220
can try to integrate your uncertainty over the hyperparameters

660
00:40:54,230 --> 00:40:57,330
of this model making predictions of what could be good

661
00:40:57,340 --> 00:41:01,620
choices hyperparameters and this is partly why yes some a lot

662
00:41:01,620 --> 00:41:03,910
of literature is actually using gaussian processes

663
00:41:03,910 --> 00:41:06,230
for which you can actually put priors on

664
00:41:06,330 --> 00:41:09,770
the hyperparameters of that gaussian process and try to integrate

665
00:41:09,770 --> 00:41:14,210
that uncertainty and there has been problems gaussian processes

666
00:41:14,210 --> 00:41:17,320
that are not superscalar also if you do a lot of experiments you're

667
00:41:17,320 --> 00:41:19,290
going to have a large training set of

668
00:41:19,530 --> 00:41:24,170
validation set hyperparameters and performance and then beige neural

669
00:41:24,180 --> 00:41:27,380
nets can be then like an alternative again to be visions or

670
00:41:27,390 --> 00:41:30,150
integrate over some of the uncertainty in your

671
00:41:30,410 --> 00:41:34,980
choices hyperparameters there's another reason why the beijing cases

672
00:41:35,100 --> 00:41:39,380
convenient here is that typically you don't just want a prediction

673
00:41:39,380 --> 00:41:42,370
of what you think the expected performance is going to be but you

674
00:41:42,370 --> 00:41:46,370
want to notion of uncertainty over what's your i guess

675
00:41:46,660 --> 00:41:49,900
confidence interval over the range of values you would expect

676
00:41:50,010 --> 00:41:53,340
so just the expectation is not as useful as also getting a full

677
00:41:53,340 --> 00:41:55,720
distribution on that conditional distribution

678
00:41:55,770 --> 00:41:58,420
because if you you think the expectation is fairly

679
00:41:58,630 --> 00:42:02,160
bad but there's a lot of variance might still be worth exploring

680
00:42:02,410 --> 00:42:05,580
so that's another motivation for using a gaussian or

681
00:42:05,850 --> 00:42:09,370
the patient approach here but you do have to be careful you could

682
00:42:09,380 --> 00:42:14,550
imagine maybe training a a model that performs hyperparameters

683
00:42:14,560 --> 00:42:18,740
selection and over multiple problems and there's

684
00:42:19,170 --> 00:42:23,020
work on using multitask vision optimization which tries to justice

685
00:42:23,020 --> 00:42:25,930
so there are solutions but it's true you have to be careful about

686
00:42:25,930 --> 00:42:27,800
that yeah yes

687
00:42:39,020 --> 00:42:42,880
yeah it's so this is something people do quite a bit in practice

688
00:42:42,880 --> 00:42:45,840
so the question is should do what makes sense to start by just

689
00:42:45,840 --> 00:42:48,260
tuning the learning rate keeping everything else fixed

690
00:42:48,260 --> 00:42:51,250
and what you did that maybe move on other hyperparameters and

691
00:42:51,250 --> 00:42:53,950
that is indeed like strategy people use quite a bit

692
00:42:54,050 --> 00:42:57,480
in practice because indeed it tends to be one of the most

693
00:42:57,720 --> 00:42:59,990
influential hyperparameter within all

694
00:43:00,130 --> 00:43:02,370
hyperparameters that's definitely a good

695
00:43:02,580 --> 00:43:05,570
thing to do in practice yeah and are actually

696
00:43:05,810 --> 00:43:09,550
i'm going to cover a partial some of them are algorithms that

697
00:43:10,200 --> 00:43:13,530
are fairly rule more more robust to the choices of the learning

698
00:43:13,530 --> 00:43:16,550
rate because they have effectively adaptive learning rate

699
00:43:16,550 --> 00:43:19,730
that might be other solution to rely such optimizers instead

700
00:43:21,290 --> 00:43:22,080
other questions

701
00:43:29,130 --> 00:43:32,880
a think might be were not superfine earlier with that literature

702
00:43:32,890 --> 00:43:37,990
the one of the one of the things that makes genetic algorithms

703
00:43:37,990 --> 00:43:41,150
to be less interesting is that typically here you don't want to

704
00:43:41,150 --> 00:43:44,050
run like thousands and thousands of experiments

705
00:43:44,240 --> 00:43:48,030
and one nice property of things like vision optimization sequential

706
00:43:48,110 --> 00:43:49,810
model-based optimization is that

707
00:43:50,390 --> 00:43:54,240
there are usually considered as approaches that's

708
00:43:54,420 --> 00:44:00,000
require fewer guesses to eventually find a good solution to

709
00:44:00,010 --> 00:44:04,010
your problem whereas depending on you set up your genetic algorithm

710
00:44:04,020 --> 00:44:06,100
it might make a lot more while guesses

711
00:44:06,360 --> 00:44:08,690
in terms of which hyperparameters to try out

712
00:44:09,330 --> 00:44:13,550
so that would be my you know warning about using such an approach

713
00:44:13,560 --> 00:44:16,470
in setting but there might be good i'm that first into

714
00:44:16,710 --> 00:44:19,030
these algorithms so there that could be

715
00:44:19,500 --> 00:44:21,930
interesting approach in certain yeah situation

716
00:44:29,340 --> 00:44:33,150
i mean yeah this requires a pheromone of like you know you need

717
00:44:33,160 --> 00:44:36,880
to implement that's so that's pretty good like vision optimization

718
00:44:36,880 --> 00:44:38,870
for the first experiments you're going to do

719
00:44:38,870 --> 00:44:41,430
is not going to do much better than random search

720
00:44:41,430 --> 00:44:47,060
and so it might be for certain problems are of fairmount of hyperparameters

721
00:44:47,070 --> 00:44:51,710
are good enough but this is a good direction to explore if you

722
00:44:51,710 --> 00:44:58,820
so let me remind you of the program which and I believe these look at

723
00:44:58,820 --> 00:45:04,070
the slides are on the web if people checked and they're not there please let me know

724
00:45:04,070 --> 00:45:11,770
first day I talked about up a little bit about observations of the universe yesterday

725
00:45:11,770 --> 00:45:16,790
I talked about inflation today in the third room of the lecture I will

726
00:45:16,810 --> 00:45:21,990
talk about dark matter and tomorrow we hear also alright so two days in a  row

727
00:45:21,990 --> 00:45:26,930
in the same room I don't know how that is going to work so

728
00:45:26,930 --> 00:45:32,670
this is the cosmic pizza that you've seen every day and I'm finish for a

729
00:45:32,670 --> 00:45:37,980
while talking about the things we see and  now I`ll talk about the things we don't see

730
00:45:37,990 --> 00:45:46,810
about dark matter the I believe the simplest evidence for dark matter comes from looking

731
00:45:46,810 --> 00:45:55,130
at rotation curves of spiral galaxies this is a procedure is a technique that

732
00:45:55,130 --> 00:46:01,770
was pioneered in the late sixties and up through the middle seventies by Vera Rubin

733
00:46:01,770 --> 00:46:07,990
who is also continues to be involved in this subject and I think that a

734
00:46:07,990 --> 00:46:14,530
one day she will be remembered as the person who really brought dark matter into the

735
00:46:14,530 --> 00:46:22,110
public consciousness at least the consciousness of physics physicists really the evidence that it exist

736
00:46:22,250 --> 00:46:26,670
so what she did in in the early seventies is start on a program

737
00:46:26,670 --> 00:46:33,510
of measuring the rotation curves of galaxies so this is an example of her data

738
00:46:33,510 --> 00:46:41,510
she`s looks at a nearby galaxy m thirty-three so a couple of megaparsec away and

739
00:46:41,510 --> 00:46:48,970
by combinations of various techniques she can measure the rotation curves either of stars the rotational

740
00:46:48,970 --> 00:46:54,690
velocity of some objects about the center of the galaxy either stars or in the outer

741
00:46:54,720 --> 00:46:59,830
parts of the galaxy where there's hydrogen you can measure the redshift or blueshift

742
00:46:59,830 --> 00:47:07,110
of twenty of the twenty-one centimetre emission and so she plots the rotation curve here in

743
00:47:07,110 --> 00:47:12,750
given in kilometers per second as a function of distance from the center of the galaxy

744
00:47:12,760 --> 00:47:17,730
and to orient to about sizes of a galaxy we are in the Milky Way

745
00:47:17,730 --> 00:47:23,270
about eight point five kiloparsec from the center of the galaxy

746
00:47:23,270 --> 00:47:29,340
a parsec in the secret language of astronomy is three point two six light years

747
00:47:29,340 --> 00:47:33,660
so sort of this size is a visible parts of galaxies or

748
00:47:33,660 --> 00:47:38,630
the size of a galaxy is a little bit arbitrary to define but eight ten or so

749
00:47:38,630 --> 00:47:45,750
kiloparsec is a good picture so this is the observations of the rotation

750
00:47:45,750 --> 00:47:51,410
curve as a function of radius and you can try to model the rotation curve

751
00:47:51,410 --> 00:47:56,350
as a function of radius that you would expect just on the basis of the

752
00:47:56,870 --> 00:48:01,190
stars that you see of the light that you see with the idea that there`s where

753
00:48:01,190 --> 00:48:07,690
there is more light there's more mass and this is what's expected when you go

754
00:48:07,690 --> 00:48:12,330
out to the out of regions of the galaxy where there is not very much mass

755
00:48:12,330 --> 00:48:18,530
interior to this circu interior to the orbit you would expect the rotational

756
00:48:18,530 --> 00:48:26,050
velocity decreases radius as Kepler's third law right this isn`t rocket science it's Kepler's third law

757
00:48:26,050 --> 00:48:30,710
so this is what would be expected on the basis of what is seen

758
00:48:30,710 --> 00:48:36,230
in terms of the stars and this is absurd and the observations of the

759
00:48:36,230 --> 00:48:41,810
rotation curves generally at large distance or flat they do not decrease as you would

760
00:48:41,810 --> 00:48:48,830
expect on the basis of Kepler's laws it's not just our galaxy or m thirty-one

761
00:48:48,830 --> 00:48:53,930
or m thirty-three or andromeda when we look at many many many spiral galaxies we see

762
00:48:53,930 --> 00:49:00,210
the same type of behavior that most of the rotation curves increased reach a maximum

763
00:49:00,210 --> 00:49:06,990
bounce around here but out here the rotation becomes flat and this is evidence that

764
00:49:06,990 --> 00:49:13,910
most of the mass is not in the form of stars this is a remarkable

765
00:49:13,910 --> 00:49:20,870
result in again we need other conferring observations before we really believe this remarkable

766
00:49:20,870 --> 00:49:30,130
result and one confirming observation comes from gravitational lensing a gravitational lensing is based upon

767
00:49:30,130 --> 00:49:37,230
the idea in Einstein's theory of gravity the light path is modified by the

768
00:49:37,230 --> 00:49:43,000
existence of a massive object and back in the nineteen-twenties and nineteen-thirties Einstein

769
00:49:43,410 --> 00:49:51,210
and    worked out the distortion to an image that would be caused by a massive

770
00:49:51,210 --> 00:49:57,310
object known as a gravitational lens so this is an example of having a source

771
00:49:57,310 --> 00:50:05,550
a of distance source and having a gravitational lens in this case along the

772
00:50:05,550 --> 00:50:12,030
exactly the optical axis so if we imagine we're in the lucky situation where we

773
00:50:12,030 --> 00:50:19,050
have an observer here in Geneva looking at a source and exactly on the optical

774
00:50:19,050 --> 00:50:25,250
axis is a gravitational lens and if you do a simple calculation assuming point like

775
00:50:25,250 --> 00:50:31,310
source point light lenss exactly on the optical axis what you discover is that the

776
00:50:31,310 --> 00:50:37,510
image you would see would not be deployed like source but would be a ring

777
00:50:37,510 --> 00:50:46,210
of actually of infinite surface brightness and by measuring the this is known

778
00:50:46,210 --> 00:50:52,410
as the Einstein ring and by measuring the radius of the Einstein ring you can deduce

779
00:50:52,410 --> 00:50:59,720
the mass of the lensing object so in real life we do not have

780
00:50:59,730 --> 00:51:04,770
exactly point like lenses we do not have exactly point light sources and we

781
00:51:04,770 --> 00:51:12,130
never find something exactly on the optical axis but we do observe gravity lensing

782
00:51:12,130 --> 00:51:18,310
we do observe Einstein's ring in strong lensing so this is an image taken by

783
00:51:18,310 --> 00:51:26,430
the Hubble Space Telescope of a cluster of galaxies Abel twenty-two eighteen it was fifty years ago

784
00:51:26,430 --> 00:51:32,730
that George Abel put together his cluster excuse me his catalog of

785
00:51:32,730 --> 00:51:41,050
galaxy clusters this is a two thousand two hundred and eighty entry in that catalog and

786
00:51:41,050 --> 00:51:45,570
this is you can see the cluster of galaxies and what you can make

787
00:51:45,570 --> 00:51:53,350
out with the exquisite resolution of the space telescope is the objects that are distorted so

788
00:51:53,350 --> 00:52:01,540
these objects or images that are behind the cluster of galaxies and it's not perfectly

789
00:52:01,550 --> 00:52:06,010
on the optical axis as you can see the lens is not a perfect point

790
00:52:06,010 --> 00:52:11,430
source so it's not a perfect ring but it forms art and by

791
00:52:11,430 --> 00:52:16,790
deducing the properties of the ox or the radius of the Einstein ring you can deduce

792
00:52:16,790 --> 00:52:22,810
the mass of the cluster of galaxies and again you come to the conclusion that

793
00:52:22,810 --> 00:52:29,490
there is a much more mass associated with galaxies and clusters of galaxies then you

794
00:52:29,490 --> 00:52:34,470
how does the space of all possible choices that you could get look

795
00:52:34,490 --> 00:52:40,740
let me just she tried to figure out what i'm going to say

796
00:52:40,870 --> 00:52:45,120
right so

797
00:52:45,140 --> 00:52:48,680
what's true about this set is

798
00:52:48,680 --> 00:52:51,850
in the case of trees wire trees easy

799
00:52:51,910 --> 00:52:56,830
trees are easy because this is the deep fact i'm not going to be able

800
00:52:56,830 --> 00:53:01,370
to go into it but it turns out for trees if you just impose the

801
00:53:01,370 --> 00:53:06,080
obvious constraints if you just impose the marginalisation constraints that they should collapse down to

802
00:53:07,140 --> 00:53:11,910
and also the normalisation constraints that that somebody else mentioned so those are very local

803
00:53:11,910 --> 00:53:14,850
and obvious constraints so easy to enforce

804
00:53:16,290 --> 00:53:20,830
that's enough that's all that you need so you only need to do local consistency

805
00:53:20,830 --> 00:53:22,640
operations on the tree

806
00:53:22,700 --> 00:53:26,810
and if you do local consistency you'll get global consistency

807
00:53:26,830 --> 00:53:31,260
this is sort of the essence of something known as the junction tree theorem which

808
00:53:31,260 --> 00:53:35,030
i'm not going to go into but it's it's a general technique for understanding how

809
00:53:35,030 --> 00:53:37,470
trees behave

810
00:53:40,810 --> 00:53:43,310
right so

811
00:53:43,310 --> 00:53:47,080
what's nice about trees is that the obvious thing to do is the correct thing

812
00:53:47,080 --> 00:53:47,970
to do

813
00:53:48,100 --> 00:53:51,810
that's not always the case in life right off the obvious thing you do this

814
00:53:51,810 --> 00:53:54,350
sort of greedy algorithm or the dumb algorithm

815
00:53:55,180 --> 00:53:59,430
not the best thing to do but somehow trees life is good

816
00:53:59,450 --> 00:54:02,330
easy thing local thing is the correct thing

817
00:54:07,490 --> 00:54:10,930
i'm not going to derive this but sort of giving you the intuitions what you

818
00:54:10,930 --> 00:54:16,490
can show is that you have a constraint here and for this direction of the

819
00:54:16,490 --> 00:54:20,850
edges you have a constraint for every value that acts of s can take

820
00:54:20,910 --> 00:54:24,560
so what you do when you derive max product is an algorithm for solving the

821
00:54:24,560 --> 00:54:27,060
LP is you put all the grounds multiplayer

822
00:54:27,160 --> 00:54:32,790
one of them let's call lambda as of access one of them for every constraint

823
00:54:32,810 --> 00:54:34,180
and so have

824
00:54:34,200 --> 00:54:39,430
a vector of multipliers that sort of flow from node t to note as

825
00:54:39,430 --> 00:54:43,850
and that vector multipliers is exactly like the log of the messages

826
00:54:43,850 --> 00:54:46,770
so if you work through the derivation and you have to use some duality see

827
00:54:46,770 --> 00:54:49,890
here theory here things that live and probably would have mentioned if you work through

828
00:54:49,890 --> 00:54:54,660
the the details but you'll find our work to it later for the sum product

829
00:54:54,660 --> 00:55:00,080
algorithm you find that the lagrange multipliers exactly enforcing this constraint the messages are lagrange

830
00:55:01,790 --> 00:55:05,560
right so this gives us another view it gives us to view the

831
00:55:05,560 --> 00:55:08,810
what the max product algorithm is doing on a tree or on a chain or

832
00:55:08,810 --> 00:55:13,580
what the viterbi algorithm is doing its solving a linear programme and it's a very

833
00:55:13,580 --> 00:55:17,830
efficient way of solving a linear programme

834
00:55:17,870 --> 00:55:26,790
OK so the reason that i went into this i mean what we've learned

835
00:55:26,810 --> 00:55:31,160
essentially we just learned a rather different perspective on why trees are easy

836
00:55:31,220 --> 00:55:33,490
trees are easy because

837
00:55:33,530 --> 00:55:38,140
these marginal distributions are easy to characterize that's that's what we've learned

838
00:55:38,200 --> 00:55:42,390
but this gives us a way of understanding what is max product doing when we

839
00:55:42,390 --> 00:55:45,450
apply to graphs with cycles

840
00:55:45,970 --> 00:55:50,100
well will see max product is not actually doing what we'd like it to do

841
00:55:50,100 --> 00:55:53,180
but we'll see how we can modify it to do good thing

842
00:55:53,220 --> 00:55:57,640
and this leads to what are known as convexified forms of max product these are

843
00:55:57,640 --> 00:56:03,060
algorithms that are actually solving linear programming relaxation so they have some exact guarantees the

844
00:56:03,080 --> 00:56:09,180
max product does not have on general graphs with cycles

845
00:56:10,140 --> 00:56:18,810
during the

846
00:56:18,810 --> 00:56:20,740
so here

847
00:56:31,120 --> 00:56:38,640
OK so

848
00:56:38,680 --> 00:56:45,390
this is the probability distribution and for a discrete probability distribution by definition of expectation

849
00:56:45,390 --> 00:56:46,580
if i sort of

850
00:56:46,600 --> 00:56:48,580
take whatever function

851
00:56:48,850 --> 00:56:52,220
i weighted by the probabilities and i sum over everything

852
00:56:52,270 --> 00:56:56,830
that's the the definition of an expectation for discrete thing so i'm just using either

853
00:56:56,850 --> 00:57:04,370
q is a shorthand for saying i'm just taking expectations under that distribution

854
00:57:04,760 --> 00:57:08,580
oh sorry i see this yes OK that was your question yes sorry the squiggly

855
00:57:08,580 --> 00:57:12,720
brackets everything in here is the china expectations including the some yes

856
00:57:25,040 --> 00:57:26,470
sure so

857
00:57:26,470 --> 00:57:29,330
the sums it came back

858
00:57:29,350 --> 00:57:33,450
initially had some over the vertices and the sum over the edges

859
00:57:33,470 --> 00:57:37,140
right that was my cost function my or loss function my l

860
00:57:37,160 --> 00:57:41,910
so it decomposed across the grass so it sums over vertices and edges

861
00:57:41,970 --> 00:57:47,240
i introduced an additional sum over x when i did my expectations

862
00:57:47,330 --> 00:57:48,890
the l

863
00:57:49,280 --> 00:57:55,240
so i bl includes everything here l my entire cost function so this l includes

864
00:57:55,240 --> 00:57:59,140
that some and that some so when i come down here and i break l

865
00:57:59,160 --> 00:58:03,080
out again i get back the two sums that i had four vertices and three

866
00:58:16,740 --> 00:58:20,930
OK so you can read the proof of this in in the book with mike

867
00:58:20,930 --> 00:58:22,740
jordan or

868
00:58:22,790 --> 00:58:24,450
proof sketch perhaps

869
00:58:24,470 --> 00:58:29,720
the junction tree theorem is covered in the introductory lectures

870
00:58:29,740 --> 00:58:34,890
what i'd like to sort of do duties to continue building intuition is we sort

871
00:58:34,890 --> 00:58:38,530
of said that these are exact for tree

872
00:58:41,180 --> 00:58:45,530
they must not be exact for graphs with cycles because we know somehow that you

873
00:58:45,530 --> 00:58:47,740
know graphs with cycles are hard

874
00:58:47,850 --> 00:58:52,330
if we think about the problem that we're trying to solve but down over three

875
00:58:52,330 --> 00:58:54,700
variables so it is easy whatever you do

876
00:58:54,720 --> 00:58:56,830
but in general if you have an variables

877
00:58:57,260 --> 00:59:01,790
and you don't have the tree this is conclusive very broad class of problems includes

878
00:59:01,790 --> 00:59:05,740
many problems that are known to be NP complete things like the max cut problem

879
00:59:05,740 --> 00:59:06,830
on the graph

880
00:59:06,910 --> 00:59:12,110
many kinds of satisfiability problems case at three SAT maxsat all of these are a

881
00:59:12,110 --> 00:59:14,290
special case is not exactly of this but of

882
00:59:14,760 --> 00:59:18,400
graphical models so there's no way we can expect that we're going to get a

883
00:59:18,510 --> 00:59:22,450
sort of generic solution on any any graph with cycles

884
00:59:22,540 --> 00:59:27,080
so we sort of have to understand from this perspective whereas the complexity

885
00:59:28,680 --> 00:59:33,390
the complexity lies exactly in in this step the trees it turned out there was

886
00:59:33,390 --> 00:59:37,660
enough just to do this constraint but if you're the many you have even a

887
00:59:37,660 --> 00:59:39,140
single cycle

888
00:59:39,180 --> 00:59:44,410
then this is not sufficient anymore so let's let's try and explore that why is

889
00:59:44,410 --> 00:59:49,760
it that cycles make our life difficult

890
00:59:49,850 --> 00:59:55,180
OK so we're going to do is we're gonna play a little game

891
00:59:55,220 --> 01:00:00,390
and you can sort of tell me whether you think

892
01:00:00,450 --> 01:00:03,510
the this is valid what i'm doing

893
01:00:05,970 --> 01:00:11,720
number of pieces of paper some from somebody just

894
01:00:11,720 --> 01:00:13,370
one minus y

895
01:00:14,130 --> 01:00:18,990
now i and replacing them with this expression here i have

896
01:00:19,000 --> 01:00:21,050
it is the form of my solution

897
01:00:21,130 --> 01:00:31,920
and then OK and then have x

898
01:00:32,520 --> 01:00:37,120
so this is how

899
01:00:37,370 --> 01:00:38,410
but this

900
01:00:41,550 --> 01:00:42,710
OK now

901
01:00:42,730 --> 01:00:45,070
let me get this right

902
01:00:45,330 --> 01:00:47,920
i have all the

903
01:00:47,930 --> 01:00:49,580
one minus

904
01:00:49,590 --> 01:00:52,130
these guys here like by this

905
01:00:52,180 --> 01:00:55,180
which is something we all

906
01:01:00,300 --> 01:01:03,710
and there never remaining thing which is

907
01:01:06,510 --> 01:01:08,010
but in this

908
01:01:08,020 --> 01:01:09,130
and this

909
01:01:10,200 --> 01:01:13,380
so the go away

910
01:01:13,420 --> 01:01:15,390
in i get out of the

911
01:01:15,440 --> 01:01:17,450
x is created

912
01:01:18,640 --> 01:01:21,990
my my my nose

913
01:01:22,040 --> 01:01:29,600
but is rare that x being spread it

914
01:01:29,700 --> 01:01:33,860
i just think that this one and wrote way

915
01:01:33,910 --> 01:01:35,850
so now

916
01:01:35,860 --> 01:01:39,010
i know this is single

917
01:01:41,580 --> 01:01:44,200
but critics at x

918
01:01:46,530 --> 01:01:50,580
lots of and now i can write this

919
01:01:50,600 --> 01:01:55,470
again because i'm making up the only one a lot of negative

920
01:01:56,230 --> 01:01:58,710
i can write this as the hinge loss

921
01:01:58,980 --> 01:02:01,700
o seventeen one

922
01:02:01,920 --> 01:02:07,370
this is the wall around function or the

923
01:02:07,380 --> 01:02:10,200
but still i have i have my

924
01:02:10,280 --> 01:02:15,040
what's called friend

925
01:02:15,710 --> 01:02:18,390
know if i e

926
01:02:18,440 --> 01:02:20,610
so all i can now also for

927
01:02:21,990 --> 01:02:24,270
and they get solution which is

928
01:02:24,570 --> 01:02:28,130
but if it well

929
01:02:28,150 --> 01:02:29,330
it's obvious

930
01:02:29,460 --> 01:02:32,790
the effectiveness of people there are

931
01:02:32,840 --> 01:02:36,050
get out of it was

932
01:02:36,130 --> 01:02:38,560
l w

933
01:02:38,570 --> 01:02:40,640
w b

934
01:02:40,690 --> 01:02:43,410
minus one

935
01:02:43,450 --> 01:02:45,330
by the by

936
01:02:45,800 --> 01:02:48,820
x squared

937
01:02:50,820 --> 01:02:55,440
now i to take into account also the addition

938
01:02:55,530 --> 01:02:58,510
but can't get the a

939
01:02:58,520 --> 01:03:03,010
i can write the update like like this

940
01:03:04,750 --> 01:03:07,460
they can also that account so

941
01:03:07,470 --> 01:03:09,610
WP one started that

942
01:03:09,920 --> 01:03:14,030
but that you might not want blast it but

943
01:03:15,210 --> 01:03:17,340
thing where it but

944
01:03:17,360 --> 01:03:20,230
it but the minimal

945
01:03:20,250 --> 01:03:22,480
if see

946
01:03:22,500 --> 01:03:25,160
something like this

947
01:03:25,200 --> 01:03:27,110
and that

948
01:03:27,120 --> 01:03:31,140
that's the main one that

949
01:03:35,240 --> 01:03:36,470
OK so

950
01:03:36,480 --> 01:03:37,580
this is the

951
01:03:37,590 --> 01:03:39,570
the four here

952
01:03:39,590 --> 01:03:41,930
one one

953
01:03:43,600 --> 01:03:49,360
this is a very simple so is just like modification on the on the solution

954
01:03:50,100 --> 01:03:54,290
i mean i can just of musician

955
01:03:54,310 --> 01:03:57,810
functions they can get an analytical solution

956
01:03:57,830 --> 01:04:01,920
it depends again on the loss of the previous

957
01:04:01,970 --> 01:04:03,950
OK now

958
01:04:04,000 --> 01:04:08,320
and this is something and how do how can we go about line

959
01:04:08,340 --> 01:04:10,420
the performance of

960
01:04:10,480 --> 01:04:15,020
number this is something similar the because one but this using this different form of

961
01:04:23,980 --> 01:04:27,900
one possibility is to exploit the quality

962
01:04:27,910 --> 01:04:29,460
in the following way

963
01:04:29,650 --> 01:04:33,910
so i start

964
01:04:37,970 --> 01:04:39,600
so i go back to the

965
01:04:42,000 --> 01:04:45,040
to the SVM functional

966
01:04:46,920 --> 01:04:49,720
a given string of examples

967
01:04:53,250 --> 01:04:54,680
so i have

968
01:04:55,780 --> 01:05:04,960
in fact in the bit background

969
01:05:04,970 --> 01:05:06,840
and then look at the

970
01:05:07,030 --> 01:05:14,480
and then look at the full SVM function

971
01:05:55,440 --> 01:06:00,210
what do i

972
01:06:00,290 --> 01:06:05,970
do the same idea for the k case so i compute the URL

973
01:06:05,980 --> 01:06:11,620
will look and function i'm not doing it so that the one show my ignorance

974
01:06:11,640 --> 01:06:14,600
again but the just right

975
01:06:14,610 --> 01:06:17,580
hoping i copied correctly

976
01:06:17,580 --> 01:06:19,590
it is not

977
01:06:22,790 --> 01:06:26,940
i mean i can write the the

978
01:06:27,000 --> 01:06:28,550
now i one

979
01:06:28,600 --> 01:06:34,110
so this is the one for and we can write it will you

980
01:06:34,120 --> 01:06:36,200
for completeness

981
01:06:37,530 --> 01:06:41,170
this is going to be equal to minus

982
01:06:41,220 --> 01:06:44,250
my heart

983
01:06:44,280 --> 01:06:46,610
the square of the sum of

984
01:06:51,060 --> 01:06:54,700
x being read the

985
01:06:58,320 --> 01:07:01,750
some over

986
01:07:03,860 --> 01:07:06,650
under the constraint

987
01:07:06,700 --> 01:07:08,990
box constant

988
01:07:15,110 --> 01:07:16,630
i know

989
01:07:18,860 --> 01:07:20,640
from what i think

990
01:07:20,650 --> 01:07:22,090
i know that

991
01:07:26,150 --> 01:07:34,570
optimal by the value of a descendant of the out that maximizes the likelihood function

992
01:07:34,620 --> 01:07:37,840
equals the value of the primal objective

993
01:07:37,840 --> 01:07:40,380
that could be a possible

994
01:07:40,400 --> 01:07:45,400
much better do this morning but as you can see these two charts for property

995
01:07:45,660 --> 01:07:47,260
for two different

996
01:07:47,430 --> 01:07:52,400
so influential blog post and both of them have very different

997
01:07:52,990 --> 01:07:56,110
comment reaction one has very spiky comments reaction

998
01:07:56,130 --> 01:07:58,060
intuition you can always

999
01:07:58,080 --> 01:08:00,520
i think that's very spiky comments reaction

1000
01:08:00,530 --> 01:08:01,500
could be

1001
01:08:01,510 --> 01:08:04,960
two of very influential blog post but on other NBC flat comments reaction to an

1002
01:08:04,960 --> 01:08:06,730
influential blog post as well

1003
01:08:06,780 --> 01:08:14,640
so this is not exactly very good differentiating between conventional and go

1004
01:08:14,680 --> 01:08:20,340
in this study you can see the different temporal patterns of influential bloggers

1005
01:08:21,540 --> 01:08:22,690
in the rows

1006
01:08:22,710 --> 01:08:24,150
or you can see

1007
01:08:24,160 --> 01:08:25,840
the of

1008
01:08:25,890 --> 01:08:30,080
by the months starting from february two thousand four january two thousand seven

1009
01:08:30,090 --> 01:08:34,500
and on the columns who have different influential bloggers who have been influential it's important

1010
01:08:37,620 --> 01:08:40,960
and the kind of media and shows the influence level

1011
01:08:40,980 --> 01:08:43,280
the darker the colour is the higher

1012
01:08:43,290 --> 01:08:48,380
the more influential so in this chart can observe long there are

1013
01:08:48,400 --> 01:08:52,980
different kinds of influential bloggers somewhere along all influential bloggers

1014
01:08:53,000 --> 01:08:54,510
like this

1015
01:08:55,620 --> 01:09:00,860
who has been quite influential belong to the same and some of them are

1016
01:09:00,880 --> 01:09:07,130
very short on order the average influential bloggers who have been influential for number of

1017
01:09:07,130 --> 01:09:12,430
the transient influential bloggers who have been selected for very small and then burgeoning influentials

1018
01:09:13,870 --> 01:09:18,980
i just started to become influential so depending on different different modelling applications you can

1019
01:09:18,980 --> 01:09:22,260
use these bloggers for different

1020
01:09:25,010 --> 01:09:28,160
OK so far we have

1021
01:09:28,170 --> 01:09:31,420
the addressed questions like all these

1022
01:09:31,570 --> 01:09:36,520
so influential bloggers are different from active bloggers on

1023
01:09:36,640 --> 01:09:41,340
what are the different types of bloggers but we haven't

1024
01:09:41,360 --> 01:09:45,130
that is the question how you can read this

1025
01:09:45,140 --> 01:09:47,060
this morning

1026
01:09:47,080 --> 01:09:52,030
the influential bloggers should we propose i created in french and art and specially when

1027
01:09:52,030 --> 01:09:56,740
you have no ground truth how we can find out these things

1028
01:09:56,780 --> 01:10:00,240
one obvious solution is to

1029
01:10:00,340 --> 01:10:03,410
use human so it again

1030
01:10:03,410 --> 01:10:04,630
as the people do

1031
01:10:04,650 --> 01:10:08,410
rank for audio findings and one the thing

1032
01:10:08,450 --> 01:10:12,910
but it involves a lot of cost effort and resources and beyond that it's not

1033
01:10:12,910 --> 01:10:14,140
always guaranteed to the

1034
01:10:14,590 --> 01:10:15,870
a evaluation

1035
01:10:16,840 --> 01:10:20,020
we need to think of something

1036
01:10:20,040 --> 01:10:21,440
very differently

1037
01:10:21,460 --> 01:10:25,910
it has to be are different alternate reference point for the ground truth in this

1038
01:10:25,910 --> 01:10:26,880
case we use another

1039
01:10:27,410 --> 01:10:29,550
the one website which is today

1040
01:10:29,620 --> 01:10:31,770
because all of these part content

1041
01:10:31,800 --> 01:10:36,180
everything is submitted and voted on by the digg community share discover bookmark and also

1042
01:10:36,180 --> 01:10:40,490
that part you visit big is a huge user base

1043
01:10:40,540 --> 01:10:44,250
we have a lot of collective is people for what they see and if

1044
01:10:44,420 --> 01:10:49,000
an article is really important problem keeps on acquiring another big score and it keeps

1045
01:10:49,000 --> 01:10:50,780
on getting problem

1046
01:10:50,990 --> 01:10:53,490
so the higher the digg score for a blog post is the more it is

1047
01:10:55,630 --> 01:10:59,670
blog post is not on the data that means that it's not like odd

1048
01:10:59,680 --> 01:11:02,290
it was not about symmetry

1049
01:11:02,300 --> 01:11:08,030
so you have this wonderful website digg as a reference point two

1050
01:11:08,040 --> 01:11:11,300
two modifications

1051
01:11:11,310 --> 01:11:14,340
the center for this experiment is

1052
01:11:14,370 --> 01:11:16,820
big record stop hundred blog posts

1053
01:11:18,380 --> 01:11:22,620
we have to find mention as well as five active bloggers

1054
01:11:22,640 --> 01:11:25,560
which we used to construct four categories

1055
01:11:25,580 --> 01:11:28,070
forty two of these four categories of bloggers

1056
01:11:28,130 --> 01:11:31,320
we collect top twenty blog posts from our model and compare them

1057
01:11:31,380 --> 01:11:33,570
but at the top the

1058
01:11:33,690 --> 01:11:37,110
so as you can see in this first stable

1059
01:11:37,370 --> 01:11:42,960
there are four categories of these bloggers influential active influential inactive non-intervention

1060
01:11:43,020 --> 01:11:44,890
active and so on

1061
01:11:44,910 --> 01:11:48,570
one influential and active bloggers the cell is one

1062
01:11:48,590 --> 01:11:51,090
you can see there are

1063
01:11:51,150 --> 01:11:57,520
seventeen o seventeen locals that overlapping between the top twenty blog post office

1064
01:11:57,700 --> 01:12:02,840
influential active as ants compared to the hundreds the blog posts

1065
01:12:02,870 --> 01:12:08,920
seven of the top twenty influential blog posts from influential and active bloggers fall on

1066
01:12:08,920 --> 01:12:11,610
the top and of the

1067
01:12:11,620 --> 01:12:13,840
similarly in this three you can see

1068
01:12:13,870 --> 01:12:15,910
only three of

1069
01:12:15,910 --> 01:12:20,670
the block top twenty blog posts some interactive bloggers

1070
01:12:20,680 --> 01:12:23,040
appearing in the opening of

1071
01:12:25,140 --> 01:12:28,280
if you compare this to an s three you can

1072
01:12:29,060 --> 01:12:30,670
make of that

1073
01:12:30,680 --> 01:12:36,190
being influential is more important than if you want more information

1074
01:12:36,320 --> 01:12:38,050
then you have

1075
01:12:38,110 --> 01:12:41,660
more likely to appear on they are more likely to be popular with the people

1076
01:12:41,750 --> 01:12:42,960
as compared to

1077
01:12:42,980 --> 01:12:45,160
being active

1078
01:12:45,180 --> 01:12:46,460
in the next two

1079
01:12:46,480 --> 01:12:47,750
charts me

1080
01:12:47,770 --> 01:12:48,390
give the

1081
01:12:48,440 --> 01:12:50,150
distribution for

1082
01:12:50,160 --> 01:12:51,050
one hundred

1083
01:12:51,070 --> 01:12:57,400
blog posts and big and the five thirty five blog posts collected from the unofficial

1084
01:12:57,400 --> 01:12:59,210
apple logo

1085
01:12:59,240 --> 01:13:03,750
so in this case if you compare this to stand here and it's too here

1086
01:13:03,750 --> 01:13:05,960
you can see around the

1087
01:13:05,990 --> 01:13:07,410
forty percent of

1088
01:13:07,430 --> 01:13:09,400
there are blog posts

1089
01:13:10,740 --> 01:13:14,750
on big big as compared to just six percent of

1090
01:13:14,770 --> 01:13:16,880
these dollars

1091
01:13:16,930 --> 01:13:22,710
this is quite significant and sticks

1092
01:13:22,760 --> 01:13:28,440
next we study the

1093
01:13:28,450 --> 01:13:29,910
that importance

1094
01:13:29,910 --> 01:13:34,020
what's the importance of these parameters in the morning

1095
01:13:34,020 --> 01:13:38,820
used up and the influential blog posts from our model and top twenty

1096
01:13:38,840 --> 01:13:42,200
blog was that appeared on their and then we tried to do and all that

1097
01:13:42,220 --> 01:13:45,310
house and try to compute the overlap between these two

1098
01:13:45,360 --> 01:13:46,850
not going to list

1099
01:13:46,870 --> 01:13:50,740
four six different months starting from january two thousand two

1100
01:13:50,740 --> 01:13:52,320
june two thousand seven

1101
01:13:52,340 --> 01:13:56,080
so again the sea use those five configurations

1102
01:13:56,100 --> 01:14:01,260
for all in no links no comments no planes and no blog post length

1103
01:14:01,260 --> 01:14:04,610
as you can see for all and then you have all the parameters in the

1104
01:14:04,610 --> 01:14:09,900
model that that's when you get the maximum water level

1105
01:14:09,950 --> 01:14:13,690
and when you remove all in links from the model that's when you get the

1106
01:14:13,690 --> 01:14:14,870
minimum overlap

1107
01:14:14,870 --> 01:14:18,790
and this is a bound on the amount of information we've lost

1108
01:14:18,840 --> 01:14:21,060
and we can bound in terms of

1109
01:14:21,070 --> 01:14:26,580
the some of the eigen values from k plus one up to all the

1110
01:14:27,200 --> 01:14:29,930
training samples so these are the

1111
01:14:29,950 --> 01:14:32,220
i can demands on the training set

1112
01:14:33,310 --> 01:14:36,340
their average size divided by one whatever and

1113
01:14:36,390 --> 01:14:38,630
some of the last k

1114
01:14:38,650 --> 01:14:41,260
and plus this

1115
01:14:41,320 --> 01:14:43,380
OK so the term here that is

1116
01:14:43,400 --> 01:14:46,040
really just related to the

1117
01:14:46,070 --> 01:14:50,660
the radius of the ball containing the data and has won over and

1118
01:14:50,670 --> 01:14:54,930
turns out that should be quite controllable the log one delta

1119
01:14:54,980 --> 01:14:57,050
and this term here

1120
01:14:57,070 --> 01:15:00,960
which has the rademacher flavour to it and it involves the square root of the

1121
01:15:01,860 --> 01:15:03,640
divided by the square root of n

1122
01:15:03,660 --> 01:15:04,570
plus this

1123
01:15:04,590 --> 01:15:08,340
sort of normal rademacher term which is roughly as well as well

1124
01:15:08,360 --> 01:15:13,500
because he got some of basically terms the bounded by r squared divided by and

1125
01:15:13,560 --> 01:15:15,040
this is roughly

1126
01:15:15,060 --> 01:15:17,980
sorry roughly square

1127
01:15:17,990 --> 01:15:19,970
OK so

1128
01:15:20,020 --> 01:15:24,310
what this is telling us is yes if we've got igon values that fall off

1129
01:15:24,310 --> 01:15:25,730
quite sharply

1130
01:15:25,740 --> 01:15:29,890
and we don't have to use too many dimensions

1131
01:15:30,970 --> 01:15:33,890
we are going to get a good representation of the data

1132
01:15:33,910 --> 01:15:36,860
so this is the kind of results were able to get

1133
01:15:37,260 --> 01:15:40,820
using this rademacher complexity what i wanted to do is try and show you how

1134
01:15:40,820 --> 01:15:44,970
we arrive at this result i wanted to motivate first results show you what we're

1135
01:15:44,970 --> 01:15:46,520
trying to achieve

1136
01:15:46,830 --> 01:15:51,510
it's slightly unusual to use statistical analysis for

1137
01:15:51,540 --> 01:15:58,170
analyzing essentially as unsupervised learning techniques it's slightly strange-looking but hopefully

1138
01:15:58,170 --> 01:16:02,750
i've given you a picture of what's going on the stage

1139
01:16:02,780 --> 01:16:06,130
so we can sort of think of it is in the sense learning the subspace

1140
01:16:06,130 --> 01:16:09,670
how well it would space is one way of doing it

1141
01:16:10,570 --> 01:16:13,820
so it does tell us i mean this is telling us i think stuff we

1142
01:16:13,820 --> 01:16:17,750
didn't know i mean we know from practice people applied kernel PCA we've seen it

1143
01:16:17,750 --> 01:16:20,650
work but it is telling us that actually there is

1144
01:16:20,650 --> 01:16:24,180
you know statistical reasons to believe why that's the sensible thing to do

1145
01:16:24,200 --> 01:16:25,650
provided that the

1146
01:16:25,670 --> 01:16:29,180
mention the production is significantly lower than the sample size

1147
01:16:29,220 --> 01:16:35,400
and the the igon values on the sample fall off reasonably sharply

1148
01:16:37,160 --> 01:16:40,180
here's an outline of the proof and

1149
01:16:40,660 --> 01:16:41,610
hopefully this

1150
01:16:41,630 --> 01:16:44,000
you know will will

1151
01:16:44,020 --> 01:16:45,310
you know make

1152
01:16:45,330 --> 01:16:50,150
i think this is nice tricks i think in here so

1153
01:16:50,160 --> 01:16:55,270
imagine what we can do with the data matrix x is now the data matrix

1154
01:16:55,980 --> 01:17:00,250
if we perform the singular value decomposition of the matrix

1155
01:17:00,270 --> 01:17:02,670
then the columns of u

1156
01:17:02,670 --> 01:17:05,610
are the eigen vectors in the feature space

1157
01:17:05,630 --> 01:17:07,650
of the correlation matrix

1158
01:17:07,660 --> 01:17:11,120
i want to verify that this

1159
01:17:11,130 --> 01:17:13,510
a lot of fairly straightforward show

1160
01:17:13,540 --> 01:17:14,270
and me

1161
01:17:14,700 --> 01:17:15,940
columns of the

1162
01:17:15,950 --> 01:17:19,650
the eigen vectors of the the kernel matrix of this

1163
01:17:19,670 --> 01:17:23,800
decomposition singular value decomposition actually size together

1164
01:17:23,820 --> 01:17:28,530
the kernel the i can that is the correlation matrix which is where we're projecting

1165
01:17:28,530 --> 01:17:30,580
and the i can by values of the

1166
01:17:30,610 --> 01:17:33,000
kernel matrix which gives the dual

1167
01:17:37,320 --> 01:17:41,750
what we can see is that when we project into the space of the that's

1168
01:17:42,000 --> 01:17:43,700
what we're actually doing

1169
01:17:43,730 --> 01:17:48,860
is taking five x and applying the matrix UK

1170
01:17:48,880 --> 01:17:53,340
that's the new is the matrix with the columns equal to the

1171
01:17:53,410 --> 01:17:56,190
i think that is UK is just taking the

1172
01:17:56,200 --> 01:17:57,780
matrix where we throw away

1173
01:17:57,800 --> 01:18:02,470
all but the first k columns of the matrix which is projected onto those

1174
01:18:04,170 --> 01:18:08,530
so that is computing the inner product and therefore the projection onto that

1175
01:18:08,560 --> 01:18:10,190
value and this is

1176
01:18:10,230 --> 01:18:11,570
reducing the

1177
01:18:11,590 --> 01:18:15,990
taking the norm of that we just take that times itself transpose

1178
01:18:16,920 --> 01:18:20,510
so this gives us the norm squared of that projection

1179
01:18:21,950 --> 01:18:26,190
that's how much data how much norm is captured by that

1180
01:18:27,990 --> 01:18:30,240
now we can write that

1181
01:18:30,260 --> 01:18:31,990
in the following way

1182
01:18:33,140 --> 01:18:34,890
because of the way the

1183
01:18:34,940 --> 01:18:37,190
let me just go back to the previous slide

1184
01:18:37,200 --> 01:18:41,230
because of the way this is written you can see that essentially

1185
01:18:41,260 --> 01:18:45,010
each element of this matrix UK UK primed

1186
01:18:45,030 --> 01:18:50,770
it corresponds to multiplication of five x i five jason take the i j entry

1187
01:18:50,770 --> 01:18:52,210
of this matrix

1188
01:18:52,210 --> 01:18:57,050
it is multiplied from the side by five excited by the side by five xj

1189
01:18:57,070 --> 01:19:00,070
so we can think of this as the sum

1190
01:19:00,080 --> 01:19:02,670
weights w i j

1191
01:19:02,670 --> 01:19:05,430
o five x i five x j

1192
01:19:05,450 --> 01:19:07,970
where the weight w i j is just the

1193
01:19:07,990 --> 01:19:11,150
i j th entry of the matrix new prime

1194
01:19:11,150 --> 01:19:13,070
UK UK prime

1195
01:19:14,420 --> 01:19:19,700
and the nice thing about this is we can this is actually the feature space

1196
01:19:19,700 --> 01:19:20,910
the features

1197
01:19:20,920 --> 01:19:22,490
corresponding to the

1198
01:19:22,960 --> 01:19:25,250
polynomial kernel of degree

1199
01:19:26,210 --> 01:19:29,470
where we take all pairs of features

1200
01:19:29,690 --> 01:19:32,120
of the base kernel

1201
01:19:32,900 --> 01:19:37,690
i'm going to call at phi that i have that i j if i

1202
01:19:37,710 --> 01:19:38,810
is the projection

1203
01:19:38,830 --> 01:19:41,040
into a feature space f

1204
01:19:41,050 --> 01:19:44,450
and w i j is indicated was the i j entry

1205
01:19:44,490 --> 01:19:47,660
and and so we can write this now as a linear function

1206
01:19:47,660 --> 01:19:48,710
in this

1207
01:19:48,760 --> 01:19:53,660
extended speak feature space

1208
01:19:54,230 --> 01:19:58,780
and this tells us that that is actually the feature space we're looking at

1209
01:19:59,970 --> 01:20:04,730
if i had of x y z in this feature space at

1210
01:20:04,740 --> 01:20:06,630
is given by the kernel

1211
01:20:06,640 --> 01:20:09,280
OK which is just the base kernel

1212
01:20:10,550 --> 01:20:16,330
and that's just basically this is just a rewrite of the standard polynomial kernel of

1213
01:20:16,330 --> 01:20:18,490
degree two construction

1214
01:20:18,510 --> 01:20:19,720
so essentially

1215
01:20:19,720 --> 01:20:22,940
very long standing issue in physics

1216
01:20:22,980 --> 01:20:24,220
and i will show you

1217
01:20:24,230 --> 01:20:27,140
i think the next week maybe after the exam

1218
01:20:27,170 --> 01:20:29,290
o young eighteen o one

1219
01:20:29,290 --> 01:20:31,400
conclusively demonstrated

1220
01:20:31,450 --> 01:20:33,310
that light

1221
01:20:33,370 --> 01:20:35,030
a way

1222
01:20:35,060 --> 01:20:38,190
so it looks like organs we're going to be the winner

1223
01:20:38,190 --> 01:20:41,040
on the other hand i showed you last lecture

1224
01:20:41,770 --> 01:20:43,400
light can behave

1225
01:20:43,460 --> 01:20:45,020
like particles

1226
01:20:48,600 --> 01:20:50,150
radiation pressure

1227
01:20:50,160 --> 01:20:51,500
that's particles

1228
01:20:52,370 --> 01:20:53,880
the whole thing

1229
01:20:53,890 --> 01:20:55,480
the whole ball of wax and so

1230
01:20:55,480 --> 01:21:00,400
maybe newton was right maybe they are particles

1231
01:21:01,060 --> 01:21:02,300
they both right

1232
01:21:02,300 --> 01:21:04,820
there are times that you can actually

1233
01:21:04,870 --> 01:21:08,290
interpret what you see best by assuming their ways

1234
01:21:08,300 --> 01:21:11,950
and that there are times that is much better to assume that they are particles

1235
01:21:11,950 --> 01:21:13,800
with mass like in the case of

1236
01:21:13,840 --> 01:21:15,710
the radiation pressure

1237
01:21:15,710 --> 01:21:19,390
but of course the key question now is was right in terms of the speed

1238
01:21:19,390 --> 01:21:20,670
of light

1239
01:21:20,720 --> 01:21:23,960
is the light going faster in water then

1240
01:21:23,980 --> 01:21:25,920
norton was right is the light

1241
01:21:25,970 --> 01:21:27,920
going slower in water

1242
01:21:27,970 --> 01:21:31,840
an organ was right needless to say that the dutchman was right

1243
01:21:31,900 --> 01:21:34,320
the speed of light in water is lower

1244
01:21:34,330 --> 01:21:36,090
then the speed of light

1245
01:21:36,100 --> 01:21:39,600
in air

1246
01:21:39,650 --> 01:21:41,800
when we arrived the

1247
01:21:41,800 --> 01:21:43,850
the speed of light

1248
01:21:43,900 --> 01:21:45,560
in vacuum

1249
01:21:45,570 --> 01:21:48,580
we use maxwell's equations

1250
01:21:48,590 --> 01:21:51,500
and they allowed us to

1251
01:21:51,520 --> 01:21:54,310
conclude that the speed of light

1252
01:21:54,400 --> 01:21:57,230
much to everyone's surprise

1253
01:21:57,300 --> 01:22:00,150
depends on that film zero and you zero

1254
01:22:00,210 --> 01:22:02,050
in a very simple way

1255
01:22:02,090 --> 01:22:05,830
i will call it for no v

1256
01:22:06,340 --> 01:22:07,720
that was one over

1257
01:22:07,720 --> 01:22:09,550
the square roots

1258
01:22:09,600 --> 01:22:11,530
of actually non-zero

1259
01:22:11,670 --> 01:22:14,510
zero and we call that c

1260
01:22:14,580 --> 01:22:18,990
if you had used maxwell's equations as they

1261
01:22:19,110 --> 01:22:20,620
valentin material

1262
01:22:20,730 --> 01:22:25,800
in dielectrics and also in materials that have magnetic properties

1263
01:22:25,850 --> 01:22:29,370
then it would be exactly the same derivation which you would have seen the cap

1264
01:22:29,390 --> 01:22:32,770
here the dielectric constant and you will see here

1265
01:22:32,800 --> 01:22:35,550
the magnetic permeability

1266
01:22:35,560 --> 01:22:36,830
car five

1267
01:22:36,850 --> 01:22:38,370
if you don't care

1268
01:22:38,420 --> 01:22:40,040
in glass and water

1269
01:22:40,080 --> 01:22:43,150
o is larger than one so you see from the view that the speed of

1270
01:22:43,150 --> 01:22:45,400
light and water is lower

1271
01:22:45,410 --> 01:22:48,170
than the speed of light in air

1272
01:22:48,220 --> 01:22:50,950
this can also be written as e divided by

1273
01:22:50,970 --> 01:22:52,770
the square root of kappa

1274
01:22:52,790 --> 01:22:58,460
divided by caplan and we will nowadays simply right c divided by and so the

1275
01:22:58,460 --> 01:23:00,070
index of refraction

1276
01:23:00,100 --> 01:23:01,650
it is really the square root

1277
01:23:01,660 --> 01:23:04,410
of the product of the dielectric constant

1278
01:23:04,460 --> 01:23:05,860
and the

1279
01:23:05,880 --> 01:23:08,480
magnetic permeability

1280
01:23:08,490 --> 01:23:10,200
no cap on

1281
01:23:10,250 --> 01:23:13,850
and cat out of a very strong functions of frequency

1282
01:23:13,870 --> 01:23:15,820
and that's not so surprising

1283
01:23:15,900 --> 01:23:18,300
because at very high frequency

1284
01:23:18,340 --> 01:23:22,050
the intrinsic electric enigmatic dipoles

1285
01:23:22,070 --> 01:23:26,520
which being aligned by the alternating external fields

1286
01:23:26,570 --> 01:23:29,080
i cannot follow quickly enough

1287
01:23:29,090 --> 01:23:32,380
if you want to drive them in this direction and wants to drive them back

1288
01:23:32,380 --> 01:23:35,740
and forth and back and there's just not enough time to do that

1289
01:23:35,780 --> 01:23:38,340
and so you expect that high frequencies

1290
01:23:38,390 --> 01:23:43,050
the values for kappa lower than at low frequencies which is exactly what you see

1291
01:23:43,080 --> 01:23:47,980
in the case of kappa and that's only important when you do with ferromagnetic materials

1292
01:23:47,990 --> 01:23:49,630
because is paramagnetic

1293
01:23:49,650 --> 01:23:52,050
and diamagnetic materials kappa

1294
01:23:52,090 --> 01:23:55,710
it is always one anyhow are very close to one

1295
01:23:55,780 --> 01:23:57,210
i have chosen

1296
01:23:57,220 --> 01:23:59,250
one as an example

1297
01:23:59,300 --> 01:24:01,210
to show you the dependence

1298
01:24:03,310 --> 01:24:08,110
kappa on the frequency

1299
01:24:08,240 --> 01:24:12,960
this on the web so you can download it and make yourself a copy

1300
01:24:13,010 --> 01:24:16,400
so if you look here is for water

1301
01:24:18,570 --> 01:24:21,810
you see there the

1302
01:24:21,860 --> 01:24:26,100
at low frequencies it's zero even at radio frequencies

1303
01:24:26,120 --> 01:24:27,500
at a hundred

1304
01:24:29,720 --> 01:24:32,560
a hundred megahertz this is radio

1305
01:24:32,620 --> 01:24:34,240
radio waves

1306
01:24:34,240 --> 01:24:38,990
notice that the dielectric constant in one is about eighty

1307
01:24:40,460 --> 01:24:45,770
at visible light the set frequencies of visible light it's way or

1308
01:24:45,820 --> 01:24:47,420
we just discussed that

1309
01:24:47,490 --> 01:24:52,120
the oscillations go too fast electric dipoles can follow it

1310
01:24:52,140 --> 01:24:53,350
and so the

1311
01:24:53,360 --> 01:24:55,370
the index of refraction then

1312
01:24:56,170 --> 01:24:58,300
radio waves hundreds

1313
01:24:59,570 --> 01:25:01,640
roughly nine and so to speak

1314
01:25:01,660 --> 01:25:04,800
of those ways in water is nine times lower than the

1315
01:25:04,820 --> 01:25:06,060
the speed of light in

1316
01:25:07,410 --> 01:25:10,560
call it speed of light but the speed of course of the radio waves and

1317
01:25:10,560 --> 01:25:12,650
and in the case of visible light

1318
01:25:12,700 --> 01:25:15,080
you see that visible light in water

1319
01:25:15,140 --> 01:25:17,730
this speed is only one point three times slower

1320
01:25:17,760 --> 01:25:19,760
then what it would be in air

1321
01:25:19,770 --> 01:25:27,370
or of course in vacuum

1322
01:25:29,630 --> 01:25:31,700
frequency effect

1323
01:25:31,730 --> 01:25:34,510
is very noticeable

1324
01:25:34,520 --> 01:25:37,270
if you take a red lights

1325
01:25:37,350 --> 01:25:40,300
and blue light they have different frequencies

1326
01:25:40,300 --> 01:25:43,190
and therefore the index of refraction is different

1327
01:25:43,200 --> 01:25:44,560
four red light

1328
01:25:44,620 --> 01:25:46,580
and blue light if i take water

1329
01:25:46,600 --> 01:25:48,190
numbers i'm going to give you

1330
01:25:48,190 --> 01:25:49,800
f of water

1331
01:25:49,860 --> 01:25:52,970
in that's of refraction for red light in water

1332
01:25:52,990 --> 01:25:54,100
is one point

1333
01:25:54,120 --> 01:25:55,820
three three one

1334
01:25:55,870 --> 01:25:57,970
but the index of refraction

1335
01:25:57,970 --> 01:25:59,310
four blue light

1336
01:25:59,340 --> 01:26:03,560
in water is one point three four three

1337
01:26:03,600 --> 01:26:06,310
we're going to use these numbers shortly

1338
01:26:06,310 --> 01:26:09,290
this is an asked to select

1339
01:26:09,310 --> 01:26:13,410
x days select the correct splitting

1340
01:26:13,540 --> 01:26:15,200
so that

1341
01:26:15,220 --> 01:26:19,620
collect sufficient statistics from a small set of examples

1342
01:26:19,640 --> 01:26:21,140
this debate many

1343
01:26:22,060 --> 01:26:25,680
it at

1344
01:26:25,720 --> 01:26:28,470
and i used often involved

1345
01:26:28,520 --> 01:26:32,200
two one and the best attribute is really

1346
01:26:33,890 --> 01:26:39,220
so the basic color

1347
01:26:40,180 --> 01:26:41,270
given the

1348
01:26:41,470 --> 01:26:42,930
the game

1349
01:26:42,970 --> 01:26:45,250
like to follow

1350
01:26:45,270 --> 01:26:46,970
one example

1351
01:26:47,070 --> 01:26:49,600
they statistics

1352
01:26:52,430 --> 01:26:55,060
next example and so on

1353
01:26:55,250 --> 01:27:00,620
thanks to barry

1354
01:27:01,140 --> 01:27:04,040
even with many of the test

1355
01:27:04,040 --> 01:27:07,200
thank you get caught to him

1356
01:27:07,200 --> 01:27:09,620
select the best two work through

1357
01:27:09,620 --> 01:27:11,430
and the phi

1358
01:27:13,830 --> 01:27:20,350
to get the difference and engage with both satisfies the offenbach

1359
01:27:21,220 --> 01:27:22,470
is expressed

1360
01:27:23,580 --> 01:27:25,700
so this means that

1361
01:27:27,870 --> 01:27:31,040
but they have used to to compute

1362
01:27:31,770 --> 01:27:33,470
featured the piece

1363
01:27:33,500 --> 01:27:34,330
it is

1364
01:27:34,350 --> 01:27:36,750
so if you use large enough

1365
01:27:37,330 --> 01:27:39,310
in favour two

1366
01:27:39,330 --> 01:27:40,830
this split that

1367
01:27:40,930 --> 01:27:42,830
so i can fix

1368
01:27:49,350 --> 01:27:52,600
so later on the cells inventory

1369
01:27:52,660 --> 01:27:55,290
we expand is

1370
01:27:55,770 --> 01:28:00,450
take into account the views

1371
01:28:00,450 --> 01:28:01,060
o point

1372
01:28:06,180 --> 01:28:13,220
the album create an example propagate through the three deal the example below it

1373
01:28:13,270 --> 01:28:16,410
they sufficient statistic

1374
01:28:16,520 --> 01:28:17,680
and and when

1375
01:28:17,700 --> 01:28:19,970
so that it's really

1376
01:28:20,020 --> 01:28:23,540
we the kind of a small dataset

1377
01:28:23,770 --> 01:28:28,740
yes statistics from a small

1378
01:28:28,740 --> 01:28:30,040
this is in part

1379
01:28:31,330 --> 01:28:34,990
these fine

1380
01:28:35,640 --> 01:28:36,950
in standard

1381
01:28:39,620 --> 01:28:42,450
the two classifier test examples

1382
01:28:42,450 --> 01:28:43,540
the example

1383
01:28:43,620 --> 01:28:45,040
that's the

1384
01:28:45,040 --> 01:28:46,750
from the field

1385
01:28:47,850 --> 01:28:52,310
and it is classified using the majority class

1386
01:28:52,360 --> 01:28:54,790
of the examples of the

1387
01:28:54,790 --> 01:28:56,930
so this means that

1388
01:28:56,970 --> 01:28:59,470
you are only using

1389
01:28:59,500 --> 01:29:01,100
it's not for me

1390
01:29:01,100 --> 01:29:03,620
probability of

1391
01:29:06,180 --> 01:29:08,410
in the FPT

1392
01:29:08,470 --> 01:29:09,600
we have a

1393
01:29:09,620 --> 01:29:10,660
the leaf

1394
01:29:10,680 --> 01:29:13,540
the thoughts much more information

1395
01:29:13,850 --> 01:29:19,040
they start information required by the splitting criteria

1396
01:29:19,120 --> 01:29:22,560
and the information required by splitting correctly

1397
01:29:22,620 --> 01:29:25,740
is most of the times of this form

1398
01:29:27,290 --> 01:29:29,870
of of setting class given

1399
01:29:30,330 --> 01:29:32,540
he was born

1400
01:29:34,850 --> 01:29:35,720
we have

1401
01:29:35,740 --> 01:29:40,100
lists we have also that information

1402
01:29:40,120 --> 01:29:42,220
so you can use

1403
01:29:42,250 --> 01:29:44,890
much more

1404
01:29:44,950 --> 01:29:47,930
if you strategies to classify

1405
01:29:47,980 --> 01:29:49,600
best example

1406
01:29:50,870 --> 01:29:52,390
that information

1407
01:29:52,410 --> 01:29:53,830
it's is exactly

1408
01:29:53,850 --> 01:29:57,790
the one that is required by the naive bayes classifier

1409
01:29:57,790 --> 01:29:58,580
that is

1410
01:30:01,270 --> 01:30:06,430
no need to model computation we have that information

1411
01:30:06,970 --> 01:30:09,450
we can classify example

1412
01:30:10,430 --> 01:30:14,160
and i five classifier it

1413
01:30:15,100 --> 01:30:16,080
only two

1414
01:30:16,080 --> 01:30:17,980
this is illustrative

1415
01:30:19,720 --> 01:30:21,160
using four

1416
01:30:23,720 --> 01:30:28,470
this is the four different number of examples

1417
01:30:28,830 --> 01:30:30,100
we are going to

1418
01:30:31,770 --> 01:30:33,850
millions and off

1419
01:30:34,390 --> 01:30:35,770
after million

1420
01:30:35,790 --> 01:30:39,700
and we compare

1421
01:30:41,680 --> 01:30:45,330
the standard approach classify majority class

1422
01:30:46,540 --> 01:30:51,250
in peru is using and i five classifier leave

1423
01:30:51,500 --> 01:30:53,080
and yellow

1424
01:30:53,120 --> 01:30:54,660
is c four point

1425
01:30:54,680 --> 01:30:56,310
c four five

1426
01:30:56,350 --> 01:30:59,810
standard that's classifier what kind of

1427
01:30:59,850 --> 01:31:03,250
is that you might find classifier

1428
01:31:03,290 --> 01:31:08,040
using ninety five forty five of ten

1429
01:31:10,370 --> 01:31:12,620
almost equivalent

1430
01:31:12,660 --> 01:31:14,910
two c four point five

1431
01:31:15,870 --> 01:31:17,500
much less disorder

1432
01:31:17,520 --> 01:31:20,290
that is before

1433
01:31:26,520 --> 01:31:31,080
live on characteristics of fifty years of

1434
01:31:31,120 --> 01:31:32,700
the first one

1435
01:31:34,640 --> 01:31:39,540
while standard decision to the last that season will

1436
01:31:39,580 --> 01:31:43,160
that is based on greedy decision

1437
01:31:45,140 --> 01:31:48,330
i have a lot of

1438
01:31:48,370 --> 01:31:50,470
two i variance

1439
01:31:50,470 --> 01:31:53,000
the FPT like health and

1440
01:31:53,100 --> 01:31:56,640
i could have low variance model

1441
01:31:57,390 --> 01:32:03,640
and most independent from the author of the examples

1442
01:32:06,290 --> 01:32:09,430
we don't have fitting in this type of all

1443
01:32:09,430 --> 01:32:13,270
basically we basically overfitting is

1444
01:32:13,370 --> 01:32:17,080
too much room for example

1445
01:32:17,120 --> 01:32:19,470
and this is due to

1446
01:32:19,520 --> 01:32:22,660
it's several pass over the training set

1447
01:32:22,680 --> 01:32:26,160
and in that case we process it examples

1448
01:32:26,200 --> 01:32:28,080
only one

1449
01:32:28,390 --> 01:32:30,560
but also

1450
01:32:30,580 --> 01:32:34,200
it's possible that mean prove proved that

1451
01:32:34,220 --> 01:32:36,120
the FDP

1452
01:32:36,120 --> 01:32:38,040
let's talk confirm

1453
01:32:38,100 --> 01:32:41,240
two to see season three that will be used in the late

1454
01:32:41,240 --> 01:32:42,950
using all

1455
01:32:42,980 --> 01:32:44,540
the information

1456
01:32:44,540 --> 01:32:46,540
here but i will explain it there

1457
01:32:46,550 --> 01:32:52,340
the black dots means that that is what is called a week density

1458
01:32:54,580 --> 01:32:59,710
the dn denotes the empty circles mean that is the only the strong station

1459
01:32:59,750 --> 01:33:03,710
so that means that the

1460
01:33:03,790 --> 01:33:05,790
d in the second but

1461
01:33:05,850 --> 01:33:11,220
creates attention that is immediately after it is it is resolved access when it comes

1462
01:33:11,270 --> 01:33:14,800
the second day of the the this CD right

1463
01:33:14,820 --> 01:33:16,690
in the second

1464
01:33:16,690 --> 01:33:21,270
it is called weak because it dissolve immediately

1465
01:33:21,290 --> 01:33:23,090
and this will strong

1466
01:33:23,200 --> 01:33:28,250
DS when it is of much later so depending on how how much time it

1467
01:33:28,990 --> 01:33:33,790
take to resolve is called strong or weak as w so all these that i

1468
01:33:33,790 --> 01:33:34,850
was finding u

1469
01:33:34,870 --> 01:33:42,240
is it is automatically inferred analyse by the system and is incorporated into the into

1470
01:33:42,240 --> 01:33:46,080
the case space associated to each of the nodes of the

1471
01:33:46,100 --> 01:33:48,250
one of the examples

1472
01:33:48,250 --> 01:33:51,540
so you can see here is a little bit of everything down here is the

1473
01:33:51,550 --> 01:33:55,000
score when they have this is the different notes which is the next which is

1474
01:33:55,000 --> 01:34:02,350
the underlying underlying caught any medical strength is is this is love mental has value

1475
01:34:02,750 --> 01:34:05,270
that has been with the GTTM theory

1476
01:34:05,290 --> 01:34:11,050
and belongs to a hundred and fifty so has been the parts of the using

1477
01:34:11,050 --> 01:34:17,260
the IRMA implicated anonymous system or theory we can we can deduce all this information

1478
01:34:17,270 --> 01:34:21,080
so all these is there in the case base and is ready to be taken

1479
01:34:21,080 --> 01:34:25,580
into account in order to figure out whether or not to

1480
01:34:25,600 --> 01:34:27,340
so phrases

1481
01:34:27,390 --> 01:34:30,330
the motives of music are similar

1482
01:34:30,340 --> 01:34:35,780
and obviously because we have is is the case which is the president we also

1483
01:34:35,780 --> 01:34:40,310
have the performance information so as you can see for each one of the five

1484
01:34:40,830 --> 01:34:43,380
expressive dimensions you have

1485
01:34:43,400 --> 01:34:47,130
the level of the value high and low high

1486
01:34:47,170 --> 01:34:51,770
very low density and in fact i understand explosive

1487
01:34:51,880 --> 01:34:56,610
so this a sequence of events are related to each which note to each node

1488
01:34:56,610 --> 01:34:58,300
there is a that is related

1489
01:34:58,320 --> 01:35:03,160
this is the overall picture no of how complex is the structure of the case

1490
01:35:03,160 --> 01:35:07,960
in this system of the complexity of the of this presentation is is one

1491
01:35:08,000 --> 01:35:11,620
what obliges us to to think of

1492
01:35:11,780 --> 01:35:17,900
new ways of new retrieval audience of the existing retrieval algorithms in in CBIR that

1493
01:35:18,050 --> 01:35:19,730
that the time

1494
01:35:19,730 --> 01:35:23,400
i couldn't deal with cities such a complicated situation

1495
01:35:23,420 --> 01:35:27,650
you cannot you don't have lead approximate values what you can compute some sort of

1496
01:35:27,650 --> 01:35:33,380
the euclidean distance or something like this some very simple distance he unit what is

1497
01:35:33,380 --> 01:35:38,530
what we call knowledge intensive retrieval and we see what a little bit about is

1498
01:35:38,530 --> 01:35:41,340
so what is knowledge adventure people

1499
01:35:41,570 --> 01:35:45,420
but before we provide some indexes in the system

1500
01:35:45,440 --> 01:35:48,770
and the index is provided by default in this case you can see in this

1501
01:35:48,770 --> 01:35:50,610
case is

1502
01:35:50,630 --> 01:35:52,360
not more part

1503
01:35:52,420 --> 01:35:53,790
so the first thing

1504
01:35:53,800 --> 01:35:59,880
two that the system those when you look for similar similar interpretation similar performances in

1505
01:35:59,880 --> 01:36:03,460
that case memory is to look for

1506
01:36:03,500 --> 01:36:08,690
now more patterns nodes that belong to the same monomer more as the node that

1507
01:36:08,690 --> 01:36:12,960
has to be decided how has to be this is also say how to play

1508
01:36:12,960 --> 01:36:15,400
OK so they would know the input problem

1509
01:36:15,420 --> 01:36:20,190
right this is an example if you have ideas on the left you can see

1510
01:36:20,230 --> 01:36:26,860
these motifs this race and the system has to decide has to four

1511
01:36:26,880 --> 01:36:30,210
how to play the this version of the b

1512
01:36:32,030 --> 01:36:35,360
seems by the followers system looks for

1513
01:36:35,380 --> 01:36:40,730
nodes that belong to people turns it leaves what you have here in the middle

1514
01:36:40,750 --> 01:36:45,050
it is all this budget information because it forms indeed note

1515
01:36:45,190 --> 01:36:48,300
that is the first note of the pattern

1516
01:36:48,330 --> 01:36:53,090
so this is the first step that the retrieval algorithm those

1517
01:36:53,090 --> 01:36:59,030
the system allows the user interface that the user can decide that instead of paying

1518
01:36:59,030 --> 01:37:01,050
attention first to fall two

1519
01:37:01,550 --> 01:37:08,380
the similarity between the the patterns could say no medical strength or ten of another

1520
01:37:08,380 --> 01:37:13,670
tends to ensure normalisation of the note or any other any other

1521
01:37:13,920 --> 01:37:19,960
aspect of the two theories of of music that we are dealing with

1522
01:37:19,980 --> 01:37:23,070
so but will be always provided by the full what we

1523
01:37:23,300 --> 01:37:28,460
what is our personal opinion believe it is it is more reasonable to do

1524
01:37:28,530 --> 01:37:32,090
but you know this is not enough because

1525
01:37:32,130 --> 01:37:37,540
OK then this first not is is the first to be but if by default

1526
01:37:37,540 --> 01:37:42,480
la what is important is is to look for nodes that belong to be patterns

1527
01:37:42,500 --> 01:37:45,840
in may find more than one if it finds one that's that's we have one

1528
01:37:45,860 --> 01:37:51,440
that's perfect obviously general it well it will find a bunch of them because there

1529
01:37:51,440 --> 01:37:54,360
are there are a huge number of

1530
01:37:54,380 --> 01:37:57,800
note in our memory right in the case memory

1531
01:37:57,840 --> 01:38:03,520
these are very simplified a simplified example of usually assume that it found only two

1532
01:38:03,520 --> 01:38:07,040
and what it came down to was

1533
01:38:07,150 --> 01:38:08,290
the top that

1534
01:38:08,870 --> 01:38:11,190
and combined the more general view

1535
01:38:12,830 --> 01:38:17,250
these kinds of large margin methods for structured prediction

1536
01:38:17,310 --> 01:38:22,060
combined with applications that i'm going to introduce

1537
01:38:23,520 --> 01:38:28,020
this talk is going to be about supervised learning

1538
01:38:28,020 --> 01:38:29,410
in a very

1539
01:38:29,450 --> 01:38:31,070
additional i mean

1540
01:38:31,090 --> 01:38:33,750
in the normal setting that everybody knows

1541
01:38:33,950 --> 01:38:35,730
so we have

1542
01:38:35,750 --> 01:38:38,180
data comes from the distribution of x y

1543
01:38:39,610 --> 01:38:40,890
given the sample

1544
01:38:41,690 --> 01:38:42,750
and here

1545
01:38:42,760 --> 01:38:45,190
just regular training example

1546
01:38:45,230 --> 01:38:49,600
when learning function that maps x to y so that some

1547
01:38:50,800 --> 01:38:55,180
with this loss function delta gets many

1548
01:38:55,250 --> 01:38:58,170
so from that perspective it's very standard

1549
01:38:58,180 --> 01:39:00,610
and we have a lot of methods for

1550
01:39:00,670 --> 01:39:02,900
no classification regression

1551
01:39:02,930 --> 01:39:04,980
to the supervised learning

1552
01:39:06,260 --> 01:39:08,840
what's going to be different in this talk was

1553
01:39:09,950 --> 01:39:12,260
not standard part is

1554
01:39:12,310 --> 01:39:14,980
considering these problems where

1555
01:39:15,030 --> 01:39:16,430
the prediction y

1556
01:39:16,450 --> 01:39:21,040
it is not just like a binary label multiclass label

1557
01:39:21,060 --> 01:39:24,570
but some structured and complex out

1558
01:39:24,590 --> 01:39:33,370
so both x and y will typically be some discrete object like a graph

1559
01:39:34,060 --> 01:39:37,040
what other types of problems that i'm thinking of so

1560
01:39:37,500 --> 01:39:39,070
for example

1561
01:39:39,120 --> 01:39:41,110
natural language parts right

1562
01:39:41,140 --> 01:39:42,950
that's the problem where

1563
01:39:43,000 --> 01:39:44,540
the prediction

1564
01:39:44,590 --> 01:39:46,250
is the tree

1565
01:39:46,290 --> 01:39:48,390
so the

1566
01:39:51,810 --> 01:39:53,220
given a sentence

1567
01:39:53,230 --> 01:39:55,340
like a sequence of words

1568
01:39:55,360 --> 01:39:58,310
we want to predict what the correct posture

1569
01:39:58,340 --> 01:40:02,860
and if you think about this it's not easy to break the problem down into

1570
01:40:02,870 --> 01:40:06,900
that's a multiple binary classification problem right

1571
01:40:06,920 --> 01:40:08,500
it's kind of hard to say

1572
01:40:08,510 --> 01:40:09,730
you know

1573
01:40:09,750 --> 01:40:12,460
the first words have you know phrase

1574
01:40:12,510 --> 01:40:18,740
because everything is connected everything depends first and eventually the whole thing up for grabs

1575
01:40:18,760 --> 01:40:25,170
so it's not clear how we could actually break down to binary classification problem

1576
01:40:25,190 --> 01:40:30,450
so here the prediction why is the tree

1577
01:40:30,580 --> 01:40:34,070
here's another problem

1578
01:40:34,100 --> 01:40:39,950
sequence alignment one of the kind of people techniques in bioinformatics i given two sequences

1579
01:40:39,950 --> 01:40:41,580
s and t

1580
01:40:41,640 --> 01:40:46,670
we want to predict is what's the correct alignment these two c

1581
01:40:47,570 --> 01:40:48,830
and here

1582
01:40:48,830 --> 01:40:53,200
well the prediction again is on the line and that depends on the output gap

1583
01:40:53,200 --> 01:40:56,450
here in align these two here then there are certain other

1584
01:40:57,880 --> 01:41:04,080
that are eliminated five

1585
01:41:04,100 --> 01:41:06,700
here's another one information retrieval

1586
01:41:07,070 --> 01:41:10,980
you want to predict the ranking

1587
01:41:10,990 --> 01:41:15,550
and so there are two reasons for why i think it's beneficial to think about

1588
01:41:15,550 --> 01:41:18,480
the structured prediction problem one is

1589
01:41:18,540 --> 01:41:23,580
typically in information retrieval we think about loss functions that are

1590
01:41:23,610 --> 01:41:26,140
and function of the ranking

1591
01:41:27,080 --> 01:41:28,760
for example average precision

1592
01:41:28,820 --> 01:41:33,890
is a typical loss function that people use performance measure the people use make people

1593
01:41:33,910 --> 01:41:35,960
really you can

1594
01:41:36,010 --> 01:41:40,890
every transition is not defined on kind of binary labels but in the final ranking

1595
01:41:40,920 --> 01:41:42,450
thirty one to

1596
01:41:42,460 --> 01:41:47,110
optimise performance measure we actually have to think about the problem of ranking

1597
01:41:47,160 --> 01:41:48,920
the second one is that

1598
01:41:48,920 --> 01:41:52,920
i would actually like to model the dependency between the results

1599
01:41:52,980 --> 01:41:54,260
in particular

1600
01:41:54,760 --> 01:41:59,300
we wouldn't want only present results that are

1601
01:41:59,330 --> 01:42:01,980
table SPM software here

1602
01:42:03,070 --> 01:42:07,820
it's only a very kind of narrow interpretation of re as the

1603
01:42:07,830 --> 01:42:13,170
so we actually want to among model dependencies and then becomes structured prediction problem or

1604
01:42:13,230 --> 01:42:16,330
i think the problem of noun phrase coreference resolution there

1605
01:42:16,360 --> 01:42:17,790
the problem is

1606
01:42:17,790 --> 01:42:19,880
given a set of noun phrases

1607
01:42:19,930 --> 01:42:25,100
predict equivalence classes of noun phrases that belong to that refer to the same entity

1608
01:42:25,100 --> 01:42:26,030
in the world

1609
01:42:26,050 --> 01:42:30,150
so essentially were predicting equivalence relationship here

1610
01:42:30,200 --> 01:42:32,660
so all of these problems where

1611
01:42:32,740 --> 01:42:34,070
we given

1612
01:42:35,500 --> 01:42:41,820
more less discrete or structured input but in particular were predicting structured output ranking an

1613
01:42:41,820 --> 01:42:44,140
equivalence relation three

1614
01:42:48,330 --> 01:42:51,140
and there are many many other problems as sequence labeling

1615
01:42:51,140 --> 01:42:54,500
collective classification multilabel classification

1616
01:42:54,510 --> 01:42:57,250
any kind of opposition non

1617
01:42:57,300 --> 01:42:59,160
linear performance measures

1618
01:43:00,060 --> 01:43:00,910
and also

1619
01:43:00,940 --> 01:43:06,770
like planning for inverse reinforcement all problems that fall into this category

1620
01:43:07,520 --> 01:43:09,960
what i want do in this talk is the following

1621
01:43:09,990 --> 01:43:12,180
on the

1622
01:43:13,220 --> 01:43:16,400
give an overview of

1623
01:43:18,780 --> 01:43:21,470
essentially an SVM style algorithm

1624
01:43:21,520 --> 01:43:25,880
four tackling all of these problems that i mentioned

1625
01:43:25,910 --> 01:43:31,030
and what this case this is kind of a general way of formulating these learning

1626
01:43:33,510 --> 01:43:34,740
this leads to

1627
01:43:34,750 --> 01:43:36,330
an optimisation problem

1628
01:43:36,800 --> 01:43:39,700
and i'm going to talk a little bit about training algorithms

1629
01:43:39,710 --> 01:43:42,270
guarantees for these algorithms

1630
01:43:44,370 --> 01:43:45,430
and then

1631
01:43:46,890 --> 01:43:48,760
outline how these

1632
01:43:48,770 --> 01:43:54,680
method for this framework can be applied to these applications that particular sequence alignment for

1633
01:43:55,470 --> 01:43:56,990
after prediction

1634
01:43:57,550 --> 01:44:01,510
the first application of retrieval results in the ranking

1635
01:44:01,530 --> 01:44:04,340
and to supervise

1636
01:44:04,390 --> 01:44:08,560
so predicting improvement

1637
01:44:08,570 --> 01:44:09,390
all right

1638
01:44:10,440 --> 01:44:12,260
why do i think this is an

1639
01:44:12,320 --> 01:44:15,890
important interesting line of research well

1640
01:44:15,930 --> 01:44:19,530
for these from any of these problems that mentioned for example the information which is

1641
01:44:19,560 --> 01:44:22,890
the problem i don't think we currently have

1642
01:44:22,910 --> 01:44:24,710
that solves the problem

1643
01:44:24,760 --> 01:44:26,330
this gives us a way of

1644
01:44:28,640 --> 01:44:31,300
learning problem

1645
01:44:31,340 --> 01:44:33,450
but some of the problems that have mentioned

1646
01:44:33,460 --> 01:44:37,640
methods already exist for example the noun phrase coreference resolution problem

1647
01:44:37,650 --> 01:44:40,500
people often do first pairwise classification

1648
01:44:40,510 --> 01:44:42,050
and then do of clustering

1649
01:44:42,060 --> 01:44:44,020
to come clean things up

1650
01:44:44,030 --> 01:44:47,090
and get an equivalence relation but that

1651
01:44:47,140 --> 01:44:50,980
can for complicated process right you have to get the classification right and then the

1652
01:44:50,980 --> 01:44:53,430
two things that clustering works well

1653
01:44:53,450 --> 01:44:57,510
much easier to just do that all of that one that

1654
01:44:57,520 --> 01:45:00,060
optimized things directly

1655
01:45:00,130 --> 01:45:06,310
we get other problems like the natural language parsing problem we already have enough to

1656
01:45:06,310 --> 01:45:07,820
this kind of one step

1657
01:45:07,870 --> 01:45:10,200
but in many cases they are generally

1658
01:45:11,840 --> 01:45:13,410
the hope is that

1659
01:45:13,430 --> 01:45:19,160
this kind of methods like the support vector machine or conditions like

1660
01:45:19,260 --> 01:45:22,330
will allow us to get better predictive performance for example

1661
01:45:22,330 --> 01:45:24,680
my experience from text classification

1662
01:45:25,830 --> 01:45:27,370
we went from naive bayes

1663
01:45:27,370 --> 01:45:30,290
well first i would like to thank the organisers

1664
01:45:30,300 --> 01:45:36,270
for this opportunity to actually to present some of my work on this very beautiful

1665
01:45:36,270 --> 01:45:44,390
framework of information geometry are has been pioneered by professor mary what i

1666
01:45:44,440 --> 01:45:46,010
going to do is

1667
01:45:46,130 --> 01:45:49,120
to start out from

1668
01:45:49,130 --> 01:45:55,060
bregman divergence i think this community is has a lot about and dan

1669
01:45:55,100 --> 01:45:58,290
trying to give a generalization

1670
01:45:58,350 --> 01:46:05,610
of bregman divergence which hopefully will give a deeper understanding about vision how it arises

1671
01:46:05,610 --> 01:46:08,390
and its connection to complex analysis in general

1672
01:46:08,410 --> 01:46:11,420
and that these two are kind of geometry

1673
01:46:12,210 --> 01:46:17,610
we call sort of hessian geometry which contains two flat space but has very nice

1674
01:46:17,610 --> 01:46:23,920
properties and then they're going to go into the infinite dimensional function space

1675
01:46:23,960 --> 01:46:25,410
trying to

1676
01:46:25,420 --> 01:46:27,920
work on the exact same kind of structure

1677
01:46:27,930 --> 01:46:30,840
that's present in international space

1678
01:46:30,980 --> 01:46:37,790
and finally i will give the fishing the metric the fisher metric and connection in

1679
01:46:37,790 --> 01:46:43,290
this information about space assuming topology now i think the high point of this

1680
01:46:43,340 --> 01:46:47,260
analysis is want clarify two kinds of

1681
01:46:48,170 --> 01:46:53,620
in information geometry one is a call kind of article called referential duality that is

1682
01:46:53,620 --> 01:46:53,850
to say

1683
01:46:54,260 --> 01:46:58,090
when you are doing comparison when you see captain the divergence

1684
01:46:58,100 --> 01:46:59,370
the point

1685
01:46:59,390 --> 01:47:04,590
or the function you use as the basis for comparison and the point of the

1686
01:47:04,590 --> 01:47:09,090
function you use as the comparing point the asymmetric but there's tool

1687
01:47:09,130 --> 01:47:13,060
relationship between the two so this is called the reference should work and it is

1688
01:47:13,060 --> 01:47:19,960
the representational duality basically you can present some convex functions are as and subject to

1689
01:47:20,390 --> 01:47:27,180
freedom of monotone transformation and the embedding would be a class of that kind of

1690
01:47:27,370 --> 01:47:27,730
me so

1691
01:47:28,230 --> 01:47:32,360
these two duality actually are quite different kind of two entities

1692
01:47:32,420 --> 01:47:34,790
but in the classic framework

1693
01:47:34,820 --> 01:47:37,980
the two two entities so mixed together

1694
01:47:38,000 --> 01:47:42,570
and without so so that's why you see a lots of labour nice results but

1695
01:47:42,580 --> 01:47:45,260
we really want to have a deep understanding about

1696
01:47:45,290 --> 01:47:49,500
whether it is related to a refreshing twenty or embedding twenty

1697
01:47:49,880 --> 01:47:54,760
and i hope this framework what i'm going to present is going to very much

1698
01:47:54,760 --> 01:47:57,480
give you a clarification of that

1699
01:47:57,500 --> 01:47:59,440
so let's start with the bregman divergence

1700
01:47:59,450 --> 01:48:01,550
we have a strictly convex function

1701
01:48:01,570 --> 01:48:03,500
define on the subset of

1702
01:48:03,520 --> 01:48:07,100
i and and what i want to point out that

1703
01:48:07,320 --> 01:48:08,300
this property

1704
01:48:08,320 --> 01:48:14,690
so quadrilateral relation it's an extension of the triangular relation that everybody here probably wouldn't

1705
01:48:14,690 --> 01:48:18,910
know that easy for any four points actually twenty four point

1706
01:48:18,920 --> 01:48:21,170
they don't want to be the three three to four

1707
01:48:21,260 --> 01:48:23,010
you have this

1708
01:48:23,020 --> 01:48:25,920
this very nice relation so that

1709
01:48:25,970 --> 01:48:30,540
the right hand side basically the difference between these two and so here's the period

1710
01:48:30,540 --> 01:48:33,610
of the two space so the triangulation is

1711
01:48:33,630 --> 01:48:35,570
in degenerate case

1712
01:48:35,600 --> 01:48:39,770
OK where you take say theta one equals that two

1713
01:48:39,790 --> 01:48:40,770
many of the two

1714
01:48:40,790 --> 01:48:45,600
o point will be the same so that we generate two triangulation but eugen in

1715
01:48:45,600 --> 01:48:50,450
general have quadrilateral relational what does it imply i'm still thinking about the implication of

1716
01:48:50,470 --> 01:48:55,230
i think it contains very significant

1717
01:48:55,270 --> 01:48:59,580
the information here in terms of this kind of space that we are dealing with

1718
01:48:59,820 --> 01:49:02,940
the second thing i want to point out is this is this

1719
01:49:02,950 --> 01:49:06,130
reference representation by twenty in the

1720
01:49:06,170 --> 01:49:10,450
bregman divergence in the following sense if you look at the bregman divergence between theta

1721
01:49:10,450 --> 01:49:11,570
one and theta two

1722
01:49:11,630 --> 01:49:15,830
by the way this is the particular notation i use a particular definition of the

1723
01:49:15,830 --> 01:49:20,190
problem that which could be reversed OK but some books but in this case i

1724
01:49:20,190 --> 01:49:22,260
just used this so

1725
01:49:22,320 --> 01:49:26,020
if we look at the convex conjugate function

1726
01:49:26,690 --> 01:49:27,450
this five

1727
01:49:27,480 --> 01:49:28,480
and then

1728
01:49:28,500 --> 01:49:31,630
you can exchange the theta one theta two

1729
01:49:31,670 --> 01:49:38,240
after the embedding after you a transformation of this and lines see the variables so

1730
01:49:38,450 --> 01:49:39,380
the so

1731
01:49:40,720 --> 01:49:44,380
see that one is true is chosen as the reference point the the two is

1732
01:49:44,380 --> 01:49:49,720
accompanied by comparing the divergence of the two with respect to the one

1733
01:49:49,860 --> 01:49:53,450
one is the base for but then it's equivalent of comparing

1734
01:49:53,510 --> 01:49:54,840
the divergence

1735
01:49:54,860 --> 01:49:59,710
we using the phi as the inducing convex function

1736
01:49:59,760 --> 01:50:04,490
and treating theta to what is essentially the two transmission of the two as the

1737
01:50:04,510 --> 01:50:10,620
base point so just reference representation by twenty it's easy to verify by the formulas

1738
01:50:10,620 --> 01:50:13,900
of the intelligence and these are the things i just want to point out which

1739
01:50:14,120 --> 01:50:18,430
may not be so well known well recognised i think it's important properties forty eight

1740
01:50:18,430 --> 01:50:20,120
bregman divergence and we shall see

1741
01:50:20,130 --> 01:50:22,610
with the rise in my following

1742
01:50:24,030 --> 01:50:25,620
so no

1743
01:50:25,630 --> 01:50:30,390
it's about the money has pointed out and averages is intimately related to the so

1744
01:50:30,390 --> 01:50:34,300
called the canonical divergence canonical divergence what can i do that i would just is

1745
01:50:34,300 --> 01:50:38,720
is that not only have the two points that i wanted to

1746
01:50:38,750 --> 01:50:40,260
you can see the divergence

1747
01:50:41,260 --> 01:50:42,210
if i

1748
01:50:42,260 --> 01:50:46,320
get effective transformation on the second thing to say

1749
01:50:46,330 --> 01:50:50,450
OK so now in this case i call it's easy to all it time that's

1750
01:50:50,470 --> 01:50:55,690
basically a one point transformation of the second compelling point and then define the divergence

1751
01:50:55,690 --> 01:50:59,860
in this case it has this very nice form

1752
01:50:59,880 --> 01:51:05,270
which will hear star is there is a continuous function you have this kind of

1753
01:51:05,270 --> 01:51:06,570
tool is symmetric

1754
01:51:06,630 --> 01:51:10,430
kind of expression for this bregman divergence functions

1755
01:51:10,510 --> 01:51:13,700
and that's the bregman divergence canonical form because

1756
01:51:13,750 --> 01:51:15,210
between theta

1757
01:51:16,340 --> 01:51:19,820
here is that it's easy to see in the same form is used it i

1758
01:51:19,820 --> 01:51:22,970
think we would defeat uses if you stop

1759
01:51:24,490 --> 01:51:26,060
as you change

1760
01:51:26,070 --> 01:51:31,080
the comparison reference point change that what at the end if you the same to

1761
01:51:31,180 --> 01:51:32,800
also change the

1762
01:51:32,810 --> 01:51:36,570
convex in using functions from theta on this

1763
01:51:36,580 --> 01:51:41,570
really is a reiteration of the state and really of the this referenced

1764
01:51:41,580 --> 01:51:46,460
the representation by twenty which is deeply significant and the individual's

1765
01:51:46,630 --> 01:51:52,080
i think that once you write in this case what it's obvious that the

1766
01:51:52,160 --> 01:51:55,160
actually it's related to the french

1767
01:51:55,180 --> 01:51:58,700
and that the basic guarantees the nonnegativity

1768
01:51:58,750 --> 01:52:00,900
of this canonical divergence

1769
01:52:01,190 --> 01:52:07,260
eventually quality stands and that guarantees a basically is subtracted the right hand side from

1770
01:52:07,260 --> 01:52:08,890
the left hand side

1771
01:52:08,900 --> 01:52:10,700
so from this perspective

1772
01:52:11,530 --> 01:52:14,940
immediately that leads to the following intuition

1773
01:52:14,970 --> 01:52:17,070
this kind of divergence

1774
01:52:17,120 --> 01:52:22,440
computational this kind that which is geometry junction general by that which somehow related to

1775
01:52:22,440 --> 01:52:24,880
this particular complex inequality

1776
01:52:24,930 --> 01:52:30,570
because the equality when you move the right hand side telephony minus and you get

1777
01:52:30,570 --> 01:52:31,820
the divergence functions so

1778
01:52:31,830 --> 01:52:35,570
what in the most general case

1779
01:52:35,590 --> 01:52:37,190
so let's see

1780
01:52:37,200 --> 01:52:38,630
in general

1781
01:52:38,640 --> 01:52:40,750
for a strictly convex function fy

1782
01:52:40,760 --> 01:52:42,190
we have this

1783
01:52:43,510 --> 01:52:48,870
quality which basically the definition of kind of a strictly convex function so the inequality

1784
01:52:49,680 --> 01:52:52,610
for all theta one theta two

1785
01:52:53,890 --> 01:52:58,120
it end the equality holds if and only if beta one is equal to that

1786
01:52:58,130 --> 01:53:01,390
so that's the definition of a strictly convex function

1787
01:53:01,440 --> 01:53:06,580
OK so this is if i hope people can recognise here is restricted between minus

1788
01:53:06,580 --> 01:53:08,050
one positive one

1789
01:53:08,070 --> 01:53:10,450
this the definition of ft

1790
01:53:10,460 --> 01:53:12,340
as a strictly convex function not

1791
01:53:12,360 --> 01:53:13,570
if we know

1792
01:53:13,620 --> 01:53:16,460
move the right hand side to the left

1793
01:53:16,470 --> 01:53:19,360
so this becomes the things in the curly bracket

1794
01:53:19,360 --> 01:53:23,590
then we expand the the square the square of there

1795
01:53:23,600 --> 01:53:29,400
and we just observe because wt made mistakes on the example then this must be

1796
01:53:29,410 --> 01:53:31,600
negative this is the model

1797
01:53:31,620 --> 01:53:34,540
and because it was mistake madness to be negative

1798
01:53:34,550 --> 01:53:37,060
so we can drop this get an upper bound

1799
01:53:37,950 --> 01:53:42,800
we've seen these guys are all the more we get this relationship you so again

1800
01:53:42,800 --> 01:53:47,940
we have an occurrence we can work out the recurrence and then we get this

1801
01:53:48,880 --> 01:53:54,070
well again these things sum up to the number of mistakes

1802
01:53:54,080 --> 01:53:58,840
and so we can be combined using because she shorts inequality

1803
01:53:58,850 --> 01:54:00,830
to recover

1804
01:54:00,850 --> 01:54:02,120
two relate to this

1805
01:54:02,140 --> 01:54:03,430
with the

1806
01:54:03,450 --> 01:54:07,110
quantity that we used to prove the the other part

1807
01:54:08,090 --> 01:54:10,290
we can just solve the for this

1808
01:54:10,310 --> 01:54:15,180
which we happen to have been both sign in both inequalities this one in the

1809
01:54:15,180 --> 01:54:16,070
previous ones

1810
01:54:16,120 --> 01:54:17,610
so we can solve for it

1811
01:54:17,620 --> 01:54:21,380
and we can get this which is you can think of it as a generalized

1812
01:54:21,380 --> 01:54:27,170
perceptron convergence theorem says this does number of mistakes made by the perceptron on any

1813
01:54:27,170 --> 01:54:31,170
arbitrary string is bounded by the

1814
01:54:31,180 --> 01:54:39,040
the best trade the infimum of the hinge loss of the fixed linear classifier u

1815
01:54:39,040 --> 01:54:45,240
plus this could normal blasts across to that involves both for both things

1816
01:54:45,330 --> 01:54:48,900
so this is what you see this is

1817
01:54:48,930 --> 01:54:54,340
as interesting properties it's an oracle inequality because it basically

1818
01:54:54,350 --> 01:54:59,350
without knowing anything about this you is able you're able to compare the number of

1819
01:54:59,350 --> 01:55:01,130
mistakes with the best

1820
01:55:01,160 --> 01:55:04,260
trade between hinge loss and more

1821
01:55:04,260 --> 01:55:09,020
OK so for those of who know some of you who support vector machine is

1822
01:55:09,020 --> 01:55:12,090
you know that if we should drop this part here

1823
01:55:12,130 --> 01:55:16,770
this is actually the function which is minimized by

1824
01:55:16,780 --> 01:55:19,330
the support vector machine solution

1825
01:55:20,390 --> 01:55:23,400
sure there is some kind of relationship even though

1826
01:55:23,620 --> 01:55:27,150
if you have information across to here

1827
01:55:27,160 --> 01:55:31,920
and now you can get of course the special case of a separable streams so

1828
01:55:31,920 --> 01:55:33,120
if you cycle

1829
01:55:33,120 --> 01:55:36,390
the the perceptron the linearly separable streams

1830
01:55:36,400 --> 01:55:39,950
then that you have that

1831
01:55:40,810 --> 01:55:42,300
i will

1832
01:55:42,310 --> 01:55:44,710
then this term disappear

1833
01:55:44,780 --> 01:55:49,040
this is here because the hinge loss of the best guide

1834
01:55:49,260 --> 01:55:52,740
you want to linearly separable three is going to be the right

1835
01:55:52,760 --> 01:55:55,340
and then you're left with b

1836
01:55:55,420 --> 01:55:57,310
mean for you

1837
01:55:57,330 --> 01:56:01,110
the infimum of the new unknown new square

1838
01:56:01,160 --> 01:56:03,240
where you square is

1839
01:56:04,410 --> 01:56:08,860
separator achieving zillions losses in particular you can take the

1840
01:56:08,860 --> 01:56:14,350
support vector machines solution which is the shortest linear separator achieving zero loss

1841
01:56:14,400 --> 01:56:15,770
so basically

1842
01:56:15,770 --> 01:56:16,730
shows that

1843
01:56:16,760 --> 01:56:20,550
the number of mistakes made the perceptron on elizabeth stream as

1844
01:56:21,570 --> 01:56:27,600
bounded by the squared norm of the support vector machine solution for the same stream

1845
01:56:27,620 --> 01:56:30,280
and another consequence is there

1846
01:56:30,290 --> 01:56:33,550
you know if you have a separable sitting always exists

1847
01:56:33,560 --> 01:56:39,140
a linear separator expressible as a linear combination of at most these many support

1848
01:56:39,160 --> 01:56:41,050
this is because

1849
01:56:41,050 --> 01:56:43,880
the the update rule of the perceptron

1850
01:56:43,900 --> 01:56:45,620
you can view it

1851
01:56:45,620 --> 01:56:48,410
as anything support

1852
01:56:48,430 --> 01:56:51,670
which is mistaken example every time a mistake is made

1853
01:56:51,720 --> 01:56:54,320
so you see you added is going to some

1854
01:56:54,410 --> 01:56:57,040
so after and mistakes

1855
01:56:57,040 --> 01:57:02,260
this guy here will be a linear combination of m mistaken instances

1856
01:57:02,270 --> 01:57:05,370
which you can view as supports

1857
01:57:05,380 --> 01:57:09,380
so that's why you can see is the

1858
01:57:09,390 --> 01:57:13,280
you make economic and most this many mistakes gonna make you don't have that most

1859
01:57:13,280 --> 01:57:17,010
of these many supporting yourself in your solution

1860
01:57:17,020 --> 01:57:17,880
so i

1861
01:57:17,960 --> 01:57:22,860
so there's a number of interesting properties of this is a very basic example of

1862
01:57:23,190 --> 01:57:27,490
online mistake bound for the simplest

1863
01:57:27,510 --> 01:57:30,150
binary classification i agree

1864
01:57:30,180 --> 01:57:31,370
OK so

1865
01:57:31,400 --> 01:57:34,680
let me move on

1866
01:57:34,680 --> 01:57:40,090
two special cases of separable saying i want to say something more about here

1867
01:57:40,110 --> 01:57:42,760
and the first thing is

1868
01:57:45,760 --> 01:57:51,390
one thing that you could you can be asking now is OK

1869
01:57:51,430 --> 01:57:58,220
we saw the perceptron doesn't doesn't have the correcting up update

1870
01:57:58,260 --> 01:58:05,860
so the the margin after every up the margin increases by a constant additive term

1871
01:58:05,860 --> 01:58:08,800
which is the square the number of distance so now

1872
01:58:08,850 --> 01:58:12,990
what you might want to do is that you might want to modify the presenter

1873
01:58:12,990 --> 01:58:16,970
also that sought to enforce a large margin so you want to come up with

1874
01:58:16,970 --> 01:58:18,610
the corrective update

1875
01:58:18,610 --> 01:58:22,380
and now we can wonder whether corrective update might be

1876
01:58:22,410 --> 01:58:25,740
might work better than the perceptron update

1877
01:58:29,240 --> 01:58:33,550
so in particularly what you can you might find corrective update

1878
01:58:33,570 --> 01:58:35,760
as follows

1879
01:58:35,780 --> 01:58:39,220
as an update that enforces a zero hinge loss

1880
01:58:39,700 --> 01:58:46,410
zero hinge loss its weight

1881
01:58:46,450 --> 01:58:47,840
after the update

1882
01:58:48,510 --> 01:58:49,800
OK so

1883
01:58:49,840 --> 01:58:50,950
we want to be

1884
01:58:50,970 --> 01:58:55,410
we want to modify the participants also to make it more aggressive in responding to

1885
01:58:55,450 --> 01:58:59,390
the two mistakes and see what happens in terms of the solution

1886
01:58:59,410 --> 01:59:01,220
so now

1887
01:59:02,050 --> 01:59:05,200
again you admit you made

1888
01:59:05,200 --> 01:59:10,030
you made a now you're looking at the hinge loss of the

1889
01:59:10,050 --> 01:59:15,910
of the online so whenever they are learning curves something to loss then you make

1890
01:59:15,910 --> 01:59:20,860
an update so that the the updated weight the will have no no hinge loss

1891
01:59:20,860 --> 01:59:22,930
of missing some example

1892
01:59:22,990 --> 01:59:25,430
so this is a

1893
01:59:27,490 --> 01:59:32,260
a different and more aggressive update so so we now can try to analyse the

1894
01:59:32,260 --> 01:59:36,680
properties of this is a different topic here and

1895
01:59:36,680 --> 01:59:38,010
and this

1896
01:59:38,030 --> 01:59:40,120
this thing this this

1897
01:59:40,140 --> 01:59:46,770
what does say in the moment is strongly related to another in proposed by his

1898
01:59:46,770 --> 01:59:52,390
hildreth in convex optimisation in the fifties

1899
01:59:52,470 --> 01:59:54,510
that has

1900
01:59:54,970 --> 01:59:58,720
i mean you can do this is very special case of the general in general

1901
01:59:58,720 --> 02:00:00,300
algorithm by hundreds

1902
02:00:00,320 --> 02:00:06,530
i'm not going to tell more about this relationships that to start to see in

1903
02:00:06,780 --> 02:00:10,590
in learning and convex optimization

1904
02:00:10,610 --> 02:00:12,320
so now

1905
02:00:12,340 --> 02:00:13,610
let's get this

1906
02:00:13,620 --> 02:00:15,030
that the solution

1907
02:00:15,030 --> 02:00:19,340
all of this is the solution of the weight will have this form which is

1908
02:00:19,340 --> 02:00:23,820
just the perceptron update with the learning rate which are going to determine

1909
02:00:23,840 --> 02:00:25,410
now we want to

1910
02:00:25,430 --> 02:00:31,050
choose it that is such that the updated with margin big enough so we can

1911
02:00:31,050 --> 02:00:36,640
just to solve and find the solution here

1912
02:00:37,530 --> 02:00:39,590
since his roommate made an update

1913
02:00:39,610 --> 02:00:45,050
this is this is a positive and so we can just the right

1914
02:00:45,490 --> 02:00:46,340
hinge loss

1915
02:00:47,410 --> 02:00:51,740
this is actually equivalent to the hinge loss against the positive part of this thesis

1916
02:00:51,740 --> 02:00:53,450
we made enough

1917
02:00:53,530 --> 02:00:58,200
this is we making up whenever this is positive

1918
02:00:58,200 --> 02:01:00,550
the probability that the couple's

1919
02:01:00,600 --> 02:01:04,490
the value of the of the couple falls into this rectangle

1920
02:01:04,500 --> 02:01:06,910
again it is symmetric and

1921
02:01:07,280 --> 02:01:08,790
if a integrate

1922
02:01:09,590 --> 02:01:11,120
over the two domains

1923
02:01:11,130 --> 02:01:13,090
it's equal to one

1924
02:01:13,140 --> 02:01:21,240
i have like to previously

1925
02:01:22,410 --> 02:01:28,360
all the same rules and so the distance d some rules and not for the

1926
02:01:28,530 --> 02:01:33,620
properties mission that for the probability density function

1927
02:01:33,640 --> 02:01:36,250
so again if a sum over the

1928
02:01:36,250 --> 02:01:41,250
the domain of one of the variable they have the margin of the other variables

1929
02:01:44,340 --> 02:01:47,690
trying to everything and can be decomposed into

1930
02:01:48,790 --> 02:01:50,040
probability density

1931
02:01:50,060 --> 02:01:53,400
of the conditional tense the marginal

1932
02:01:53,410 --> 02:01:56,560
the bayes rules applies and

1933
02:01:57,600 --> 02:02:01,810
two random to continuous random variable it to be independent

1934
02:02:01,820 --> 02:02:03,750
if there

1935
02:02:03,760 --> 02:02:05,850
china probability density

1936
02:02:05,880 --> 02:02:06,850
can be

1937
02:02:06,870 --> 02:02:09,140
decompose into the product of the

1938
02:02:09,150 --> 02:02:11,060
two probability densities

1939
02:02:11,840 --> 02:02:14,770
and the definition of the expectation of

1940
02:02:14,780 --> 02:02:16,010
the function x

1941
02:02:16,030 --> 02:02:17,090
this time

1942
02:02:17,110 --> 02:02:19,790
is the sum of the domain of

1943
02:02:20,560 --> 02:02:24,750
continuous from a random variable of the function

1944
02:02:24,770 --> 02:02:27,510
always it again weighted by

1945
02:02:27,520 --> 02:02:31,390
the probability density

1946
02:02:33,370 --> 02:02:36,310
some classical distributions continuous

1947
02:02:36,320 --> 02:02:38,450
the version of maybe

1948
02:02:38,460 --> 02:02:41,680
let's classical i didn't know them

1949
02:02:41,950 --> 02:02:45,080
before my phd

1950
02:02:47,130 --> 02:02:48,520
remember the

1951
02:02:48,570 --> 02:02:51,540
binomial distribution so how many

1952
02:02:51,550 --> 02:02:56,710
and if i toss my kind if i keep my kind several times

1953
02:02:56,730 --> 02:03:00,630
what's what's the number of times i have have a head

1954
02:03:02,240 --> 02:03:08,340
so the prior distribution on the parameter u which was that the number of

1955
02:03:08,360 --> 02:03:12,240
of the probability of t

1956
02:03:12,240 --> 02:03:16,270
so it's it's very off in the in the minimum distribution

1957
02:03:16,280 --> 02:03:17,480
is usually

1958
02:03:17,520 --> 02:03:19,590
you take a better distribution

1959
02:03:19,610 --> 02:03:23,300
was store which has two parameters a and b

1960
02:03:23,310 --> 02:03:24,250
and as you see

1961
02:03:24,250 --> 02:03:29,210
we find here that this product that we had before in the in the binomial

1962
02:03:31,510 --> 02:03:39,220
and OK got my these strange function that's for four normalize this scene

1963
02:03:39,260 --> 02:03:40,550
but what was

1964
02:03:40,570 --> 02:03:43,910
what is interesting is that some giving

1965
02:03:43,930 --> 02:03:45,290
the density to the

1966
02:03:45,300 --> 02:03:47,410
to the

1967
02:03:47,460 --> 02:03:52,660
the parameters to this the to the probability of having a head

1968
02:03:52,680 --> 02:03:58,450
but they and what you see here is that these two values a and b

1969
02:03:58,460 --> 02:04:03,790
which are made to parameters in fact something like that

1970
02:04:03,820 --> 02:04:05,250
the effective

1971
02:04:05,260 --> 02:04:08,010
number of times i see

1972
02:04:08,030 --> 02:04:10,070
days or hit

1973
02:04:11,440 --> 02:04:17,140
so it's my prior on i'm pretty sure that for example

1974
02:04:17,990 --> 02:04:21,580
these two are another good example

1975
02:04:21,640 --> 02:04:24,550
i'm pretty sure that i'm going to see

1976
02:04:26,760 --> 02:04:28,910
water is than hits

1977
02:04:28,970 --> 02:04:31,830
so i'd say i put eight two eight

1978
02:04:31,890 --> 02:04:33,610
and beta four

1979
02:04:33,630 --> 02:04:36,930
and what they have is that my the prior on my

1980
02:04:36,960 --> 02:04:37,570
on my

1981
02:04:37,660 --> 02:04:40,250
coefficient on my parameter

1982
02:04:40,250 --> 02:04:41,940
is it

1983
02:04:41,960 --> 02:04:43,960
closer to one down to zero

1984
02:04:44,000 --> 02:04:46,750
it's a little bit the maldives

1985
02:04:46,790 --> 02:04:48,660
the other side of

1986
02:04:48,680 --> 02:04:49,860
i have

1987
02:04:51,390 --> 02:04:56,870
the other way round if i put i have a prior that maybe and see

1988
02:04:57,900 --> 02:04:59,540
i had done is

1989
02:04:59,550 --> 02:05:04,270
and so i have put eight two one two three and the

1990
02:05:04,830 --> 02:05:05,940
prior is

1991
02:05:05,940 --> 02:05:09,620
in this way

1992
02:05:09,650 --> 02:05:12,180
and if i have no idea

1993
02:05:12,190 --> 02:05:15,160
i said that it would have looked prior on bus

1994
02:05:15,180 --> 02:05:16,330
d a and b

1995
02:05:16,350 --> 02:05:18,700
and what they will have is that

1996
02:05:18,760 --> 02:05:20,060
any chance

1997
02:05:20,120 --> 02:05:23,480
you will give me one zero and the other is zero

1998
02:05:24,600 --> 02:05:30,320
and i understand my is it or you know it's it's completely

1999
02:05:30,340 --> 02:05:32,280
the reaction

2000
02:05:36,460 --> 02:05:38,030
OK this

2001
02:05:38,320 --> 02:05:44,960
it's a candidate it's also generalize to the case of the multinomial distribution

2002
02:05:45,710 --> 02:05:48,240
the classical prior for the

2003
02:05:48,260 --> 02:05:51,070
parameter u which is the vector

2004
02:05:51,790 --> 02:05:54,550
a multinomial is taken as

2005
02:05:54,550 --> 02:05:58,060
as being do the outlet distribution

2006
02:05:58,090 --> 02:05:59,620
with parameters

2007
02:06:01,700 --> 02:06:03,430
being the vector

2008
02:06:03,450 --> 02:06:06,780
and with this kind of

2009
02:06:06,790 --> 02:06:08,340
of probability

2010
02:06:08,350 --> 02:06:10,070
density functions

2011
02:06:13,120 --> 02:06:18,000
if i take for me to nominate of capello's favour days with

2012
02:06:18,040 --> 02:06:18,950
true face

2013
02:06:19,000 --> 02:06:22,330
it's not possible

2014
02:06:23,370 --> 02:06:24,690
so the

2015
02:06:24,690 --> 02:06:29,900
so you have lots of things like you know the ball and

2016
02:06:30,120 --> 02:06:33,690
are a and you know and

2017
02:06:33,940 --> 02:06:37,360
and then you have always because it's it's about the model you have these all

2018
02:06:37,360 --> 02:06:41,040
these different word probability distributions

2019
02:06:41,480 --> 02:06:46,020
and then you also have another sort of large state for organizations

2020
02:06:46,040 --> 02:06:52,860
and what sort of conceptual transitions from every state here to every state in here

2021
02:06:52,880 --> 02:06:58,520
they all these transitions share the same set of parameters

2022
02:06:58,540 --> 02:07:01,730
so you didn't so basically did sort of completely

2023
02:07:01,750 --> 02:07:04,150
exacerbate the premier estimation problem

2024
02:07:04,210 --> 02:07:08,230
i mean this is this is actually a pretty reasonably common technique think it's called

2025
02:07:08,360 --> 02:07:10,310
like parameter tying

2026
02:07:10,330 --> 02:07:15,060
and there are other cases of this parameter tying things in the research literature

2027
02:07:15,080 --> 02:07:17,500
the other really interesting thing that they did

2028
02:07:17,520 --> 02:07:22,350
and what really gets the core of why humans are

2029
02:07:22,360 --> 02:07:25,520
not completely satisfactory for information extraction

2030
02:07:25,560 --> 02:07:30,290
is that they wanted to use various types of capitalization

2031
02:07:30,310 --> 02:07:32,310
and features about this

2032
02:07:32,330 --> 02:07:33,750
format of numbers

2033
02:07:33,800 --> 02:07:36,710
but if you remember one thing that should in the early on the talk with

2034
02:07:36,710 --> 02:07:38,710
this list the features

2035
02:07:38,790 --> 02:07:42,500
they had nothing to do with actually the identity of the word itself

2036
02:07:42,560 --> 02:07:45,310
it included in any of the world but basically had like

2037
02:07:45,330 --> 02:07:49,190
fifty other types of features based on form adding capitalization

2038
02:07:49,290 --> 02:07:53,480
i mean other sorts of things that are not being used here by hidden markov

2039
02:07:53,480 --> 02:07:56,670
models right sort of no hidden markov models just looking at

2040
02:07:56,690 --> 02:08:00,920
the identity of the world so you know it's really sad about hidden markov models

2041
02:08:00,920 --> 02:08:02,380
is that they

2042
02:08:02,400 --> 02:08:05,670
by using this rich source of information

2043
02:08:05,730 --> 02:08:09,560
so let's try to hack around it including

2044
02:08:09,650 --> 02:08:13,730
this small set of perhaps a dozen features

2045
02:08:13,750 --> 02:08:17,150
in each word was really not a word

2046
02:08:17,210 --> 02:08:20,900
distribution but it was paired with one of these features so for example

2047
02:08:21,060 --> 02:08:25,580
most of these are various flavours of trying to recognise digits and money at the

2048
02:08:26,900 --> 02:08:30,920
and then always wanted the bottom are various types of capitals asian

2049
02:08:30,940 --> 02:08:36,350
sort of features and so the word abraham would actually be a pair of the

2050
02:08:36,350 --> 02:08:38,630
world abraham and

2051
02:08:38,650 --> 02:08:40,710
initial caps

2052
02:08:40,730 --> 02:08:42,630
the cell

2053
02:08:42,810 --> 02:08:46,900
and so this

2054
02:08:46,920 --> 02:08:53,270
one of the factors that this really expands the parameter space right fully by creating

2055
02:08:53,630 --> 02:08:55,440
ten different things

2056
02:08:55,480 --> 02:08:59,350
you almost never quite be really always increase the number of parameters by an order

2057
02:08:59,350 --> 02:09:00,670
of magnitude

2058
02:09:00,730 --> 02:09:04,190
so quite true because the word abrahams never going to be a two-digit number but

2059
02:09:04,190 --> 02:09:08,380
it could be any kind of a ranking of any kind of this capitalization

2060
02:09:08,440 --> 02:09:13,060
right so that's there's definitely tension there are you can really

2061
02:09:13,080 --> 02:09:14,750
creating that many premier should be

2062
02:09:14,790 --> 02:09:17,350
somewhat scary

2063
02:09:17,560 --> 02:09:20,330
good OK so

2064
02:09:20,630 --> 02:09:25,350
eight interns works pretty well in practice but why should we not be completely satisfied

2065
02:09:27,920 --> 02:09:31,290
in markov models and actually have identities

2066
02:09:31,350 --> 02:09:36,710
and this sort of don't allow for these features to be handled so so nicely

2067
02:09:36,730 --> 02:09:40,440
i mean the other thing that

2068
02:09:40,460 --> 02:09:43,100
we should be said about is it

2069
02:09:43,150 --> 02:09:46,830
it turns out that the core generative technique writer there

2070
02:09:46,830 --> 02:09:50,980
what actually written down mathematically is the way that we believe that the document was

2071
02:09:51,750 --> 02:09:56,140
ramsay approximation is sort depending on this fact and we know that to sort of

2072
02:09:56,140 --> 02:09:58,290
be blatantly not true

2073
02:09:58,290 --> 02:10:04,230
right and in particular the number of actually spending quite a bit of parameter estimation

2074
02:10:05,230 --> 02:10:08,460
trying to figure out how it is that people

2075
02:10:08,460 --> 02:10:10,730
what the words and things like that

2076
02:10:12,170 --> 02:10:14,100
well you really wish is that they

2077
02:10:14,400 --> 02:10:15,730
the technique wouldn't actually is

2078
02:10:15,770 --> 02:10:19,670
spend that time and we really focus in on

2079
02:10:19,670 --> 02:10:23,020
what we really care about right which is what's the label of the word right

2080
02:10:23,040 --> 02:10:25,560
to the land was given to us and so

2081
02:10:25,560 --> 02:10:28,230
we have to worry about generating was already given to us we would have no

2082
02:10:28,230 --> 02:10:31,380
choice we really want the model to account for

2083
02:10:31,460 --> 02:10:34,420
and really pay attention to is trying to do a great job

2084
02:10:34,480 --> 02:10:36,100
predicting that label

2085
02:10:36,120 --> 02:10:40,810
and there's nothing in this generative model that tells that yes what we're really interested

2086
02:10:40,810 --> 02:10:43,520
in is predicting the model

2087
02:10:43,540 --> 02:10:45,420
so this brings us to

2088
02:10:45,420 --> 02:10:47,560
yes question

2089
02:10:47,620 --> 02:10:50,230
most problem

2090
02:10:50,310 --> 02:10:54,190
what you see is that when people

2091
02:10:56,290 --> 02:11:02,480
o and know that are some

2092
02:11:06,440 --> 02:11:11,120
so for example when you actually

2093
02:11:11,250 --> 02:11:17,580
so what you want these things how exactly

2094
02:11:20,480 --> 02:11:24,190
inside the building

2095
02:11:25,650 --> 02:11:33,520
so i wouldn't say that hidden markov models are particularly

2096
02:11:33,520 --> 02:11:35,460
understandable models

2097
02:11:35,530 --> 02:11:38,920
like it's not very easy to look at of HMM in

2098
02:11:38,980 --> 02:11:41,980
see what's going on under the hood

2099
02:11:43,130 --> 02:11:47,540
i don't think i have any strong suggestions for how to in no

2100
02:11:47,540 --> 02:11:50,250
when your models good

2101
02:11:50,270 --> 02:11:53,770
i think there's a lot of room for domain expertise to play a role in

2102
02:11:53,770 --> 02:11:55,210
crafting the

2103
02:11:55,230 --> 02:11:57,400
state structure

2104
02:11:57,460 --> 02:12:01,350
and i highly recommend that but i don't have any

2105
02:12:01,420 --> 02:12:05,830
guidelines other than just be clever

2106
02:12:10,560 --> 02:12:17,630
conditional random fields but the heart what the model is the probability distribution of y

2107
02:12:17,630 --> 02:12:21,980
which are the output labels given x which are the words are actually occurring

2108
02:12:22,040 --> 02:12:23,710
and sort of

2109
02:12:23,790 --> 02:12:27,330
give you some sense for the mathematics of conditional random fields

2110
02:12:27,380 --> 02:12:31,500
i wanted to show you the hidden markov models are special cases of conditional random

2111
02:12:31,500 --> 02:12:35,440
fields i think this will help your understanding of what's really going on in canada

2112
02:12:35,500 --> 02:12:37,190
and conditional random fields

2113
02:12:37,210 --> 02:12:38,350
so i'm going to sort of

2114
02:12:38,360 --> 02:12:41,420
take you from this initial formal

2115
02:12:41,500 --> 02:12:45,440
formulation of hidden markov models and sort of wacky through

2116
02:12:45,460 --> 02:12:47,060
to show that

2117
02:12:48,650 --> 02:12:53,900
she took arrive at this sort of functional form of the conditional random field

2118
02:12:54,190 --> 02:12:57,420
one thing to keep in mind that the conditional and feel how the enterpri amateurs

2119
02:12:57,420 --> 02:12:58,790
and need to be estimated

2120
02:12:58,850 --> 02:13:02,560
if we set the parameters in some strange way will

2121
02:13:02,580 --> 02:13:05,020
and that's exactly as had hidden markov model

2122
02:13:06,250 --> 02:13:09,880
well the training for real concern feel actually end up with quite different parameters and

2123
02:13:09,880 --> 02:13:11,350
we'll talk about that in a minute

2124
02:13:12,400 --> 02:13:15,000
so here we have this generative distribution

2125
02:13:15,100 --> 02:13:19,730
over first generating a next state and then generate a word for that state

2126
02:13:19,750 --> 02:13:21,540
i'm going to basically

2127
02:13:21,540 --> 02:13:24,830
do this EXP exponentiation and long hair

2128
02:13:24,850 --> 02:13:27,650
so in that will convert this product into some

2129
02:13:27,670 --> 02:13:29,940
it allows get pushed n

2130
02:13:30,130 --> 02:13:35,150
and to separate out the addition of these two log probabilities

2131
02:13:35,420 --> 02:13:38,650
then i want to try to make this a conditional right so i want to

2132
02:13:38,670 --> 02:13:44,790
derive its sort of probability y given x and for some this joint y

2133
02:13:44,830 --> 02:13:46,290
this is just a little

2134
02:13:47,150 --> 02:13:50,850
that's probably why next if you sort of divided by all the other all possibilities

2135
02:13:50,850 --> 02:13:54,920
of x y pairs there is so i've done the same thing here

2136
02:13:54,980 --> 02:13:59,000
which is basically just introduce this ze which is the some

2137
02:13:59,170 --> 02:14:03,650
the normalizer that depends only on x so x is the only thing that matters

2138
02:14:03,650 --> 02:14:07,170
here because y is taking care about the

2139
02:14:07,190 --> 02:14:11,120
and don't worry too much about that but but busy barley for now

