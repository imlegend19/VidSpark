1
00:00:00,000 --> 00:00:02,380
the last ten years

2
00:00:02,400 --> 00:00:04,740
having this

3
00:00:04,770 --> 00:00:09,640
are we going to

4
00:00:23,820 --> 00:00:27,550
absolutely not

5
00:00:46,020 --> 00:00:49,130
theory of the

6
00:01:18,700 --> 00:01:23,820
i mean turn

7
00:01:23,850 --> 00:01:27,400
and my

8
00:01:41,270 --> 00:01:44,360
at the time of

9
00:01:48,770 --> 00:01:55,610
one year

10
00:02:03,430 --> 00:02:08,440
it would be

11
00:02:16,310 --> 00:02:18,720
make your

12
00:02:56,320 --> 00:02:58,820
is all the more

13
00:02:59,390 --> 00:03:02,210
we can already

14
00:03:12,280 --> 00:03:16,700
so they can play

15
00:03:21,600 --> 00:03:24,720
o five

16
00:03:24,750 --> 00:03:26,530
all the here

17
00:03:47,430 --> 00:03:50,940
it be or

18
00:04:02,430 --> 00:04:06,350
we have

19
00:04:06,430 --> 00:04:09,720
he is on a

20
00:04:25,500 --> 00:04:27,460
might be

21
00:04:36,160 --> 00:04:37,030
you are

22
00:04:37,050 --> 00:04:40,030
early on

23
00:04:40,060 --> 00:04:43,000
OK the world

24
00:04:46,880 --> 00:04:52,010
that year

25
00:04:52,040 --> 00:04:54,540
he will be

26
00:04:57,580 --> 00:05:02,850
a lot

27
00:05:02,870 --> 00:05:03,970
it is

28
00:05:07,200 --> 00:05:13,770
there are

29
00:05:13,790 --> 00:05:15,950
you know

30
00:06:11,490 --> 00:06:13,570
the way

31
00:06:13,570 --> 00:06:17,020
is not a clique so if you think there are two

32
00:06:17,090 --> 00:06:19,410
nolte the that are not connected

33
00:06:20,400 --> 00:06:22,830
otherwise it would be

34
00:06:22,890 --> 00:06:28,550
so let's try to conditional independence according to graph separation between these two guys

35
00:06:28,670 --> 00:06:32,290
we get exactly this because this is the lowest p

36
00:06:32,330 --> 00:06:33,530
she was only the

37
00:06:33,880 --> 00:06:36,150
p of x one cm x two

38
00:06:36,170 --> 00:06:40,660
is equal to p of cx one times your sixty divided what you see

39
00:06:40,710 --> 00:06:43,470
since we applied with this

40
00:06:43,530 --> 00:06:45,320
so these red

41
00:06:45,680 --> 00:06:47,830
equation holds

42
00:06:49,240 --> 00:06:52,000
is true

43
00:06:53,440 --> 00:06:54,980
what we do

44
00:06:55,050 --> 00:06:58,870
we essentially compute

45
00:06:58,920 --> 00:07:02,670
these in their expression

46
00:07:02,720 --> 00:07:05,810
this expression is over

47
00:07:05,860 --> 00:07:10,270
all the elements see that are hoping it be

48
00:07:12,600 --> 00:07:14,020
and now

49
00:07:14,070 --> 00:07:17,600
we can separate these some into four

50
00:07:18,650 --> 00:07:20,080
which together

51
00:07:20,090 --> 00:07:25,110
formed in times so you can you can see from this four parts

52
00:07:25,230 --> 00:07:29,880
in the first part x one and x two are not in c

53
00:07:29,930 --> 00:07:33,460
in the second part only extreme norms

54
00:07:35,130 --> 00:07:37,720
only x

55
00:07:41,600 --> 00:07:42,850
now this is

56
00:07:42,920 --> 00:07:48,020
major just see

57
00:07:51,630 --> 00:07:52,710
now this right

58
00:07:52,710 --> 00:07:55,080
the way it's written is correct

59
00:07:55,110 --> 00:07:57,960
we are going to perform this song

60
00:07:59,860 --> 00:08:01,230
but we will

61
00:08:01,230 --> 00:08:05,100
the set si not included

62
00:08:05,120 --> 00:08:07,480
x one or x

63
00:08:07,510 --> 00:08:10,730
we only include the notable right

64
00:08:12,280 --> 00:08:15,400
if i put here only c

65
00:08:15,420 --> 00:08:18,070
the set will be this i will

66
00:08:18,100 --> 00:08:21,600
the range of this image will be all the seas

67
00:08:21,610 --> 00:08:24,570
but do not include x one

68
00:08:24,570 --> 00:08:27,570
and not include xb

69
00:08:27,630 --> 00:08:30,880
now let's assume that they want to include x y

70
00:08:30,890 --> 00:08:34,640
now i just take the union of x one with c

71
00:08:35,570 --> 00:08:39,170
but they keep something over the same range

72
00:08:39,760 --> 00:08:42,720
i want to include only x two

73
00:08:42,730 --> 00:08:47,040
they include x two you would see but keep some of the same original range

74
00:08:47,040 --> 00:08:50,720
of c which was not x one marks

75
00:08:50,760 --> 00:08:54,420
and finally for wanting to both explain the extreme estimation

76
00:08:54,440 --> 00:08:59,330
i some of the original set but they include x x

77
00:08:59,370 --> 00:09:03,020
these exhaust all the possibilities for all the subsets

78
00:09:04,650 --> 00:09:06,880
now the basic because the following

79
00:09:06,930 --> 00:09:10,070
since the cardinality of the set

80
00:09:10,120 --> 00:09:12,570
the party of these

81
00:09:15,740 --> 00:09:19,260
the same as the party of physics one

82
00:09:19,290 --> 00:09:22,300
but is the opposite of the parity of these exponent

83
00:09:23,480 --> 00:09:25,150
these cardinality here

84
00:09:25,170 --> 00:09:26,570
it is

85
00:09:27,330 --> 00:09:32,410
well if this is all is even if is even this or rather because i'm

86
00:09:32,410 --> 00:09:34,650
just adding one single line

87
00:09:34,670 --> 00:09:37,190
since he had two elements

88
00:09:37,210 --> 00:09:41,580
the fact of this expression is the same but your original expression because

89
00:09:41,640 --> 00:09:43,710
i had to live

90
00:09:43,780 --> 00:09:46,360
not one

91
00:09:46,420 --> 00:09:48,390
forty five had two

92
00:09:49,680 --> 00:09:53,630
six it's a which is still in effect

93
00:09:54,990 --> 00:10:00,090
nine which is still which governments so busy

94
00:10:00,100 --> 00:10:02,680
these kind is the same as

95
00:10:02,720 --> 00:10:04,210
and this is the same

96
00:10:04,260 --> 00:10:08,380
so i can suppress all these x one x two

97
00:10:08,400 --> 00:10:10,470
and just change the signs

98
00:10:10,470 --> 00:10:13,410
of these this expression

99
00:10:13,430 --> 00:10:15,170
so i'll change

100
00:10:16,530 --> 00:10:19,520
all suppressed is x one the mines here

101
00:10:19,560 --> 00:10:24,420
suppressed physics to the mines here as suppressed x one x two and they put

102
00:10:24,420 --> 00:10:26,790
two models that which was one

103
00:10:26,840 --> 00:10:29,020
so there's

104
00:10:29,060 --> 00:10:31,020
when you put all the missing evidence

105
00:10:31,040 --> 00:10:35,930
i guess what you think you think exactly that expression

106
00:10:35,990 --> 00:10:40,300
the side monday that

107
00:10:40,350 --> 00:10:44,000
but side on the side here because of this

108
00:10:44,000 --> 00:10:46,630
these things you know

109
00:10:46,650 --> 00:10:51,350
basically things that will never be is not a clique

110
00:10:51,360 --> 00:10:52,840
this is

111
00:10:52,850 --> 00:10:57,410
so the final result we just our function or the entire set

112
00:10:57,440 --> 00:11:00,380
will be just some of these

113
00:11:01,100 --> 00:11:03,000
are cliques

114
00:11:03,120 --> 00:11:07,060
these which represent leaks

115
00:11:07,070 --> 00:11:08,410
that's decision proved

116
00:11:09,930 --> 00:11:12,070
you can try to cool

117
00:11:12,120 --> 00:11:17,550
we can to one of the most celebrated theorem

118
00:11:17,590 --> 00:11:20,290
graph model

119
00:11:21,350 --> 00:11:23,920
slide here is really important

120
00:11:23,940 --> 00:11:29,600
y four

121
00:11:29,650 --> 00:11:35,070
conditional independence statements are usually what you can see from problem

122
00:11:35,070 --> 00:11:38,510
once we've gone through training period we're not going to make mistakes with high probability

123
00:11:38,510 --> 00:11:42,280
we're not going to make big mistakes for any future things drawn from the distribution

124
00:11:42,390 --> 00:11:46,970
f of the PAC guarantee holds so graphically i drew it this way imagine each

125
00:11:46,970 --> 00:11:50,280
each row here is is another training example coming through

126
00:11:50,300 --> 00:11:54,030
the first block of them are blue those training examples and after that it's a

127
00:11:54,030 --> 00:11:57,760
bunch of things where we were the algorithm makes no mistakes it correctly classifies

128
00:11:57,780 --> 00:11:58,970
four outputs

129
00:11:59,030 --> 00:12:01,070
after the initial training period

130
00:12:01,090 --> 00:12:02,090
OK so

131
00:12:02,110 --> 00:12:04,680
that seems like a good kind of guarantee it seems like a useful thing to

132
00:12:04,680 --> 00:12:09,620
have which you can imagine that the learner the robots scrambling around the world collecting

133
00:12:09,620 --> 00:12:12,560
data about oh i tried this in this state i tried this in this state

134
00:12:12,820 --> 00:12:17,700
building up the training set and then predicting things correctly after that point

135
00:12:17,720 --> 00:12:21,490
that would be really great so here's the first thing that goes wrong building

136
00:12:21,490 --> 00:12:26,590
putting the PAC learner as are transition function learner in reinforcement learning algorithm isn't going

137
00:12:26,590 --> 00:12:31,160
to give us a PAC MDP guarantee and the reason for this is

138
00:12:31,180 --> 00:12:35,740
i find it somewhat amusing the idea something that so critical for getting kind of

139
00:12:35,740 --> 00:12:39,870
these classic results to go through a implies that the learner can improve

140
00:12:39,870 --> 00:12:42,030
and changing anyway it's behaviour

141
00:12:42,070 --> 00:12:44,010
right so these these examples

142
00:12:44,070 --> 00:12:47,410
whereas the robot getting his examples from his wandering around the world and collecting them

143
00:12:47,670 --> 00:12:52,030
it's what's the distribution of examples what is a function of how wanders around the

144
00:12:52,030 --> 00:12:55,700
world so if it collects for a while and this is all about how things

145
00:12:55,700 --> 00:12:59,950
work now start to behave differently the distribution changes

146
00:12:59,950 --> 00:13:03,800
so really the only way they can use this great learning that it is that

147
00:13:03,800 --> 00:13:07,760
it just it just it just made so it makes perfect predictions now and wanders

148
00:13:07,760 --> 00:13:10,890
around the world the same way that it's always wanted the world saying you know

149
00:13:10,890 --> 00:13:14,240
i know how i would have optimized that i know how would have optimise that

150
00:13:14,350 --> 00:13:16,950
they can make use of that because the learner goes away

151
00:13:16,950 --> 00:13:19,450
you can make accurate predictions anymore

152
00:13:19,490 --> 00:13:23,470
so that's unfortunate OK so we have another model that there's a better on the

153
00:13:23,470 --> 00:13:26,200
mistake bound model of of

154
00:13:26,220 --> 00:13:30,160
computational learning theory says that we're not going to have any depends on the distribution

155
00:13:30,180 --> 00:13:34,610
inputs are going to be presented online perhaps by an adversary and for each one

156
00:13:34,950 --> 00:13:37,490
the learner is supposed to predict the output

157
00:13:37,510 --> 00:13:40,590
if it makes a mistake get to observe with the correct label is

158
00:13:40,610 --> 00:13:43,740
and it's not allowed over the course of its lifetime to make more than m

159
00:13:43,740 --> 00:13:51,370
mistakes so it's sort of a victim of the adversary adversary can can not show

160
00:13:51,390 --> 00:13:54,180
some example for arbitrarily long into the future

161
00:13:55,890 --> 00:13:59,470
it when it gets an example might make a mistake on it but the whole

162
00:13:59,490 --> 00:14:02,070
numbers takes over its lifetime has been found

163
00:14:02,070 --> 00:14:05,640
so that seems good right we got rid of this notion of depending

164
00:14:05,660 --> 00:14:08,990
desperately on the distribution of inputs

165
00:14:09,030 --> 00:14:12,450
the problem with this one though if you stick it into a reinforcement learning algorithm

166
00:14:12,450 --> 00:14:16,660
is again it's not PAC MDP and that's because it can make mistakes the predictor

167
00:14:16,680 --> 00:14:20,070
of next state can make mistakes and as i showed in that little two by

168
00:14:20,070 --> 00:14:25,070
two example that a mistake could mean that there's some place out there that has

169
00:14:25,070 --> 00:14:29,510
high reward that are currently function predicts has low reward and therefore we're not going

170
00:14:29,510 --> 00:14:32,030
to learn what i'm going to go visit when i can learn and we can

171
00:14:32,030 --> 00:14:36,970
be permanently suboptimal so we will satisfy the PAC MDP guarantee

172
00:14:36,970 --> 00:14:38,840
getting near optimal reward

173
00:14:38,870 --> 00:14:42,010
after some number of mistakes so are so we said okay well maybe we can

174
00:14:42,010 --> 00:14:44,720
get the best of both worlds here and we sort of

175
00:14:44,740 --> 00:14:47,800
did we have developed

176
00:14:47,820 --> 00:14:50,780
a learning framework to sound like paco quick

177
00:14:50,840 --> 00:14:52,950
which stands for knows what it knows

178
00:14:53,010 --> 00:14:55,140
so here's the basic idea

179
00:14:55,180 --> 00:14:59,180
is the inputs are presented online like in the mistake bound model for each

180
00:14:59,220 --> 00:15:03,340
the learner can actually predict the output but has to be accurate or it's allowed

181
00:15:03,340 --> 00:15:06,470
to say i don't know right we can expect to be accurate with

182
00:15:06,490 --> 00:15:10,870
learning anything so it's allowed to say i don't know and observe the label it's

183
00:15:10,870 --> 00:15:12,390
not allowed to make mistakes

184
00:15:12,390 --> 00:15:15,300
and up to that point it's actually really easy to do this right you just

185
00:15:15,300 --> 00:15:17,890
say i don't know forever so we have to put it down on the total

186
00:15:17,890 --> 00:15:20,430
number of times it's allowed to say i don't know

187
00:15:20,450 --> 00:15:24,410
OK so that means it's allowed to get training examples it's allowed to be cool

188
00:15:24,410 --> 00:15:27,470
is for little while but it can be closed forever so

189
00:15:27,530 --> 00:15:31,990
graphically awaiting i didn't point the graphical representation the mistake bound k so let's take

190
00:15:31,990 --> 00:15:35,870
the case it's getting examples some of them are wrong read and some of the

191
00:15:35,890 --> 00:15:39,590
right green but the total number of red bars has to be found near the

192
00:15:39,590 --> 00:15:42,800
top of the total number of blue bars was found this is now the quick

193
00:15:42,800 --> 00:15:46,470
model the total number of blue bars is bounded so what's happening is it's it's

194
00:15:46,470 --> 00:15:49,610
getting in input making prediction it

195
00:15:49,610 --> 00:15:51,970
if at all but only if it's sure that's right

196
00:15:51,990 --> 00:15:55,660
otherwise you can say i don't know and request an example

197
00:15:55,680 --> 00:15:59,930
and the total number of blue bars has to be relatively small

198
00:15:59,930 --> 00:16:01,870
so that's

199
00:16:01,870 --> 00:16:04,450
quick quickly

200
00:16:04,510 --> 00:16:08,010
and this this can we can plug this into reinforcement learning algorithm and we can

201
00:16:08,010 --> 00:16:10,840
get our PAC MDP guarantee y

202
00:16:10,870 --> 00:16:12,930
why why

203
00:16:14,620 --> 00:16:18,530
the problem with

204
00:16:21,640 --> 00:16:30,610
right worst mistake with the mistake bound algorithms do is if it's not sure

205
00:16:30,660 --> 00:16:32,720
it just it gases

206
00:16:32,740 --> 00:16:35,990
and some of the really affect mistake bound algorithms what they do is they not

207
00:16:35,990 --> 00:16:39,950
playing the odds but they play the hypothesis space they say basically if more thing

208
00:16:39,950 --> 00:16:44,070
many things in my hypothesis space agree on go with that and if i'm wrong

209
00:16:44,070 --> 00:16:49,200
then i learn a lot and it makes very calm very dramatic progress towards learning

210
00:16:49,200 --> 00:16:53,490
the real hypothesis but but the quick recovery can do that if it's not sure

211
00:16:53,490 --> 00:16:55,510
it has to say i don't know

212
00:17:01,180 --> 00:17:03,530
if it's too confident

213
00:17:03,680 --> 00:17:07,340
o it's not allowed to be wrong

214
00:17:07,370 --> 00:17:13,070
it's simply not OK well in its impact has the pack like has an epsilon

215
00:17:13,070 --> 00:17:14,800
delta sort of thing so

216
00:17:14,800 --> 00:17:18,610
a quick algorithm you can guarantee that any problem with any amount of stochasticity is

217
00:17:18,610 --> 00:17:21,890
going to be a hundred percent guarantee correct so we have to hold out some

218
00:17:21,890 --> 00:17:26,260
probability that the whole thing could just fail but when it's not failing it has

219
00:17:26,260 --> 00:17:27,620
to be right

220
00:17:27,620 --> 00:17:29,820
otherwise cyclic

221
00:17:29,820 --> 00:17:31,850
little section on

222
00:17:31,870 --> 00:17:35,270
so some practical details and

223
00:17:35,280 --> 00:17:37,640
in the rest of the lecturing and get into sort of

224
00:17:37,650 --> 00:17:38,600
some more

225
00:17:38,610 --> 00:17:43,740
technical details of estimated to construct different sorts of markov chains harder problems so we

226
00:17:43,750 --> 00:17:46,080
can solve so if at this point

227
00:17:46,120 --> 00:17:52,400
things on claire good times questions

228
00:17:53,260 --> 00:17:55,520
world class

229
00:17:55,530 --> 00:18:02,470
one of the things that seems a bit unfortunate about one colour with that it's

230
00:18:02,470 --> 00:18:06,080
a noisy i started off with this example of trying to compute pi and it's

231
00:18:06,080 --> 00:18:10,190
embarrassing bad right when you can it's huge amount of work just to get a

232
00:18:10,190 --> 00:18:11,560
few significant figures

233
00:18:13,990 --> 00:18:18,080
when sometimes it's not true that all we can do is evaluate how distribution up

234
00:18:18,080 --> 00:18:22,680
to a constant we've actually got some mathematical ability to

235
00:18:22,740 --> 00:18:27,990
do some analytical work and maybe some things out exactly they would be nice if

236
00:18:27,990 --> 00:18:32,050
sometimes we could use some of the mathematical ability to remove the noise from something

237
00:18:32,050 --> 00:18:33,390
get better estimates

238
00:18:33,400 --> 00:18:35,340
here's an example of the problem the

239
00:18:35,790 --> 00:18:38,890
multicolored find very hard to imagine something from

240
00:18:38,890 --> 00:18:44,280
distribution and you're interested in the tail probability just some extreme actually looking for that

241
00:18:44,280 --> 00:18:46,940
red area

242
00:18:47,000 --> 00:18:51,050
if you just draw samples then hardly any of them are going to fit

243
00:18:51,110 --> 00:18:54,500
into the red area and taken as the variance is the sort of binomial thing

244
00:18:54,640 --> 00:18:57,520
whether you end up in the air and then you can get very very noisy

245
00:18:57,520 --> 00:19:00,520
estimates because you're going to be averaging a whole string is there is no i'm

246
00:19:00,520 --> 00:19:05,010
not in that area with the old one is something necessary

247
00:19:05,050 --> 00:19:08,100
so we like methods that can maybe do that one way is to just change

248
00:19:08,100 --> 00:19:11,830
the distribution of sampling from that some sort of importance sampling trick where you actually

249
00:19:11,830 --> 00:19:15,900
sample from something focused on that are

250
00:19:15,930 --> 00:19:19,570
here's another way of reducing some noise so

251
00:19:19,590 --> 00:19:21,850
imagine we are interested in

252
00:19:21,860 --> 00:19:27,600
very simple problem what's the probability of a single binary variable being turned on

253
00:19:27,680 --> 00:19:31,890
so we've got a big probability distribution which just interested in one of the variables

254
00:19:31,890 --> 00:19:36,650
and whether it's turned onto its marginal probability and it might be that marginal probabilities

255
00:19:36,650 --> 00:19:38,150
are small

256
00:19:39,130 --> 00:19:41,180
if the standard thing you say well

257
00:19:41,190 --> 00:19:43,460
this probability is

258
00:19:43,500 --> 00:19:48,100
in expectation that's what we always do we write an expectation that you are on

259
00:19:48,150 --> 00:19:51,950
drawn from the distribution and so you just average and your samples how many times

260
00:19:51,950 --> 00:19:55,360
it was turned on doing the thing averaging together all these areas and the old

261
00:19:58,000 --> 00:19:59,610
is the alternative

262
00:19:59,630 --> 00:20:03,510
so you're doing gibbs sampling you've already computed

263
00:20:03,570 --> 00:20:06,290
the conditional probability of it being turned on

264
00:20:06,310 --> 00:20:09,500
given all the others because that's what you computed in order to resample it and

265
00:20:09,500 --> 00:20:12,590
you get something algorithms

266
00:20:12,640 --> 00:20:16,110
so we're going to rewrite the probability in a different way

267
00:20:16,130 --> 00:20:18,030
this probability is just

268
00:20:18,050 --> 00:20:21,000
the joint probability is the probability of all work

269
00:20:21,010 --> 00:20:23,430
summed over all of the others

270
00:20:23,440 --> 00:20:26,880
and now this is an average of the conditional probability

271
00:20:26,930 --> 00:20:29,880
which we've already computed so we can have a free

272
00:20:29,930 --> 00:20:31,420
average under

273
00:20:31,440 --> 00:20:33,950
the samples of all the other variables

274
00:20:34,000 --> 00:20:36,380
so what we can do is this

275
00:20:36,380 --> 00:20:39,800
we're interested in one of the variables and none of the others

276
00:20:39,860 --> 00:20:42,270
and the obvious thing to do was to sample from all of them and then

277
00:20:42,270 --> 00:20:43,990
you look at this one

278
00:20:44,020 --> 00:20:47,900
instead we cannot sample from all of them throw away the one we're interested in

279
00:20:47,900 --> 00:20:49,800
and only look at the rest

280
00:20:49,810 --> 00:20:51,640
and the rest are going to say

281
00:20:51,640 --> 00:20:55,300
given out setting this is how much we think that one variable you're interested and

282
00:20:55,300 --> 00:20:58,500
wants to be turned on that nice real number which could be the sort of

283
00:20:58,500 --> 00:21:03,240
very small things close what we're estimating and we average a

284
00:21:03,290 --> 00:21:06,600
so we're going to run exactly the same MCMC algorithm and we're going to be

285
00:21:06,600 --> 00:21:10,760
processing samples in a different way is the whole the monte carlo estimator we just

286
00:21:10,760 --> 00:21:15,410
played around with some of the

287
00:21:15,470 --> 00:21:16,820
this thing is

288
00:21:16,840 --> 00:21:22,350
called by a lot of statisticians around isation i have what can only be described

289
00:21:22,350 --> 00:21:28,080
as ransom my thesis why that's stupid doesn't really apply but only that interested readers

290
00:21:28,080 --> 00:21:29,240
is also

291
00:21:29,260 --> 00:21:32,520
called waste recycling by some communities because

292
00:21:32,570 --> 00:21:39,720
they generalize to other transition operators that imagine moving into the state you're interested in

293
00:21:39,770 --> 00:21:43,470
so in this if you're doing something like metropolis and you're proposing lots state they

294
00:21:43,470 --> 00:21:47,280
say makes states to propose the weight but we're going to recycle waste by looking

295
00:21:47,280 --> 00:21:51,490
at the probability of going there there's lots of funky names out this very simple

296
00:21:51,490 --> 00:21:57,090
trick which has been around for many decades

297
00:21:57,150 --> 00:22:00,540
it's an example of a more general trick

298
00:22:01,560 --> 00:22:06,250
involves this idea of you're interested in some variables and instead of keeping just a

299
00:22:06,250 --> 00:22:08,830
being one feels the same amount attention

300
00:22:09,310 --> 00:22:12,140
i mean one has its unit vector pointing in the opposite direction

301
00:22:13,730 --> 00:22:17,830
i guess that's written here is the unit vector pointing from jade i is the

302
00:22:17,830 --> 00:22:20,120
negative of the unit vector pointing from writer jay

303
00:22:24,210 --> 00:22:24,850
but but the

304
00:22:25,180 --> 00:22:25,750
under tension

305
00:22:26,160 --> 00:22:26,850
and so the x

306
00:22:27,770 --> 00:22:29,450
it doesn't get a negation

307
00:22:29,910 --> 00:22:35,140
it's the same number but the unit vectors that give me the direction that is about these forces

308
00:22:35,580 --> 00:22:37,480
are acting there opposite each other

309
00:22:38,980 --> 00:22:42,200
and so this is the force on node to pulling it this way

310
00:22:42,730 --> 00:22:46,410
this is the force i know to pulling up and to the left this is

311
00:22:46,410 --> 00:22:48,600
the force on to pulling it straight up

312
00:22:49,020 --> 00:22:51,730
i write down these equations and all the five nodes

313
00:22:52,200 --> 00:22:57,370
that's fine vector equations my vectors are two-dimensional in this picture so it's ten equations

314
00:22:58,950 --> 00:22:59,910
in a bunch of unknowns

315
00:23:02,730 --> 00:23:05,950
just as a little bit of notation here we have a coordinate system

316
00:23:06,700 --> 00:23:10,350
introduced say put a court according system like this may be

317
00:23:10,850 --> 00:23:12,410
then we can give coordinates to these

318
00:23:13,180 --> 00:23:18,140
nodes and if we don't know piece if i is the position vector with the court if you like

319
00:23:18,560 --> 00:23:20,710
i've no died in the unit vectors u i jay

320
00:23:21,390 --> 00:23:24,270
are just peachy my speech i divided by the length of the difference

321
00:23:24,980 --> 00:23:28,080
so you have explicit the concrete formula here

322
00:23:28,730 --> 00:23:33,930
and so in general and one specific one but in general are constraints look like this

323
00:23:34,430 --> 00:23:35,180
this the sum

324
00:23:36,040 --> 00:23:39,180
for each node i is the sum overall jay

325
00:23:40,430 --> 00:23:43,580
such that i j's one these beams one arcs

326
00:23:44,770 --> 00:23:49,710
the unit vector pointing from my deejay times the tension variable which i don't know

327
00:23:49,710 --> 00:23:53,620
yet excideuil that's my optimization variable should be equal to

328
00:23:54,140 --> 00:23:55,370
minus be survive

329
00:23:56,350 --> 00:23:57,970
the force node

330
00:23:59,370 --> 00:24:02,180
so it was really equal equations that have to be satisfied

331
00:24:02,890 --> 00:24:04,350
and is written out in

332
00:24:04,730 --> 00:24:10,390
and in matrix form for that particular example we've got one two three four five six seven

333
00:24:10,830 --> 00:24:13,500
eight variables and we've got ten constraints

334
00:24:19,710 --> 00:24:20,480
let's see here

335
00:24:24,930 --> 00:24:29,020
but there are some redundancies in these constraints because as i said these forces have to add up to zero

336
00:24:29,790 --> 00:24:33,750
so the x component these four set up after zero but why can the

337
00:24:34,180 --> 00:24:37,060
vertical components these first attempt at a zero so there are

338
00:24:37,390 --> 00:24:41,730
two k to redundancies actually there's a third redundancy that the talk

339
00:24:42,140 --> 00:24:43,980
the total torque has to be zero so

340
00:24:44,790 --> 00:24:49,770
well going into it because it would take us too far field there are three redundancies in this system here

341
00:24:51,060 --> 00:24:56,450
and and so even though it looks like this is overdetermined it's actually underdetermined i

342
00:24:56,450 --> 00:25:01,620
only have seven constraints and eight variables so this is an underdetermined system

343
00:25:03,270 --> 00:25:04,120
by the way it

344
00:25:04,620 --> 00:25:05,600
for those of you who know

345
00:25:06,180 --> 00:25:10,430
network flow problems that looks a bit like a network flow problem because every column

346
00:25:10,640 --> 00:25:12,290
has a plus one minus one

347
00:25:12,680 --> 00:25:15,480
this that's what happens in network flow problems for those of you know

348
00:25:16,080 --> 00:25:18,120
recall a plus one minus one

349
00:25:18,750 --> 00:25:21,100
here every column has a unit vector

350
00:25:22,970 --> 00:25:24,470
and the negative the unit vector

351
00:25:26,180 --> 00:25:30,060
in two dimensions are if we did in three dimensions are being will unitary three vectors

352
00:25:33,870 --> 00:25:36,100
this class problems is very similar network flows

353
00:25:37,040 --> 00:25:39,430
and so you can see that if i add up all the columns

354
00:25:40,040 --> 00:25:42,160
all right so we get zero equals zero

355
00:25:42,700 --> 00:25:43,810
even if i ended up being

356
00:25:45,580 --> 00:25:49,410
rose associate with the x variables you'll see with horizontal constraint

357
00:25:50,310 --> 00:25:51,970
with the horizontal direction and force

358
00:25:52,370 --> 00:25:54,270
i we get zero if i add up to be

359
00:25:55,520 --> 00:25:58,660
even rose i would also get zero so the redundancies are clearer

360
00:25:59,680 --> 00:26:02,540
okay was the optimization problem the optimization problem is

361
00:26:04,200 --> 00:26:10,560
minimize the total weight of this structure because we're gonna put on some spacecraft and fly it to

362
00:26:12,080 --> 00:26:17,850
end it's expensive and the hardest this thing is the more it costs in fuel to launch this thing

363
00:26:18,410 --> 00:26:22,540
and so we wanna make this the lightest structure possible that will handle the forces

364
00:26:22,850 --> 00:26:24,450
the forces that can be applied to it

365
00:26:25,000 --> 00:26:26,620
and so on and minimize the weight

366
00:26:27,520 --> 00:26:28,140
end now

367
00:26:28,560 --> 00:26:29,390
this problem is

368
00:26:29,930 --> 00:26:33,810
the original model you'd have to be a little more careful here but i put

369
00:26:33,810 --> 00:26:36,040
something simple just make it easy to explain

370
00:26:36,660 --> 00:26:37,350
the weights

371
00:26:37,700 --> 00:26:39,210
is the sum over all arcs

372
00:26:40,470 --> 00:26:43,770
the length of the article which are doing biology that's data

373
00:26:44,660 --> 00:26:47,930
times the cross sectional area of the arc

374
00:26:48,560 --> 00:26:49,950
and their cross sectional area

375
00:26:50,430 --> 00:26:53,370
ought to be proportional to the tension or compression

376
00:26:53,750 --> 00:26:54,200
he met

377
00:26:56,450 --> 00:26:58,950
and so they had been is under tension or compression

378
00:27:00,370 --> 00:27:04,560
a lot of it then i did a big number here and so put the absolute value x i jay

379
00:27:05,100 --> 00:27:05,410
as my

380
00:27:06,640 --> 00:27:07,390
measuring how

381
00:27:07,910 --> 00:27:10,390
much cross sectional area i need for this being

382
00:27:10,890 --> 00:27:12,080
and that's correct

383
00:27:12,700 --> 00:27:14,640
and then this will give me the total

384
00:27:15,230 --> 00:27:16,450
volume and the course

385
00:27:17,040 --> 00:27:19,480
the way is just the density the material times by

386
00:27:21,310 --> 00:27:25,640
so this is the optimization problem we want to solve the same constraints i showed

387
00:27:25,640 --> 00:27:28,700
you in detail before r i just added this objective function here

388
00:27:29,250 --> 00:27:33,310
but it's not a linear programming problem has written because these absolute value signs

389
00:27:34,450 --> 00:27:39,040
there's a few common tricks and here's one of them and these tricks are good to know

390
00:27:43,140 --> 00:27:45,560
when you're an absolute value signs you can often

391
00:27:46,370 --> 00:27:47,930
reduce the problem to a linear programming

392
00:27:51,450 --> 00:27:52,230
here's the trick

393
00:27:53,410 --> 00:27:56,950
right x i jay is the difference to nonnegative variables

394
00:27:58,890 --> 00:28:01,430
now these now nonnegative variables that's nice

395
00:28:02,200 --> 00:28:06,140
so replace x i j with the difference of two variables

396
00:28:07,230 --> 00:28:09,060
x i j plus oxygen minus

397
00:28:10,980 --> 00:28:12,890
and all the next statement is not quite right

398
00:28:13,660 --> 00:28:15,310
be absolute value is not

399
00:28:15,730 --> 00:28:18,390
equal to the sum of these two variables

400
00:28:19,390 --> 00:28:22,520
it would be equal to the sum of those two variables if i knew

401
00:28:24,200 --> 00:28:25,100
this differences

402
00:28:25,540 --> 00:28:29,950
written what and with the extra stipulation that one of these two must be equal to zero

403
00:28:31,350 --> 00:28:32,180
positive number

404
00:28:32,710 --> 00:28:37,080
if x i j was positive it would be just this act the positive one

405
00:28:37,310 --> 00:28:39,850
effects idea was negative that would be just fact

406
00:28:40,520 --> 00:28:42,560
and then the absolute value would be

407
00:28:42,950 --> 00:28:46,390
e there just at or just at and there would be the correct thing

408
00:28:47,890 --> 00:28:49,710
if the if they are both can have

409
00:28:50,730 --> 00:28:55,060
can be positive at the same time then adding a little bit too much so

410
00:28:55,060 --> 00:28:56,810
this is actually an upper bound

411
00:28:57,290 --> 00:28:59,540
on the absolute value of the variable

412
00:29:01,040 --> 00:29:01,980
it's an upper bound

413
00:29:03,100 --> 00:29:05,600
but i'm trying to minimize this thing

414
00:29:06,700 --> 00:29:11,620
and i would never leave the slack left over in both of these things

415
00:29:12,270 --> 00:29:14,980
if i added three both these variables so that they can

416
00:29:15,770 --> 00:29:18,790
on both zero also what so the positive

417
00:29:18,790 --> 00:29:23,880
actually extra bass strings which were appointed by the challenge so you have to keep

418
00:29:23,880 --> 00:29:25,950
that in mind

419
00:29:25,960 --> 00:29:31,600
so much of which was same task quite complex task and

420
00:29:32,160 --> 00:29:35,380
if you want to solve a complex task you have been told true the complex

421
00:29:36,490 --> 00:29:41,080
so from is really two extreme choices to get a complex system

422
00:29:41,100 --> 00:29:43,920
the first one is what i call love scale engineering

423
00:29:43,940 --> 00:29:47,090
so i think we brainwashed you will get at the beginning about that

424
00:29:47,100 --> 00:29:50,280
you basically like design

425
00:29:51,020 --> 00:29:54,430
very good and christian features that you know

426
00:29:54,450 --> 00:29:56,830
good for your for your task

427
00:29:56,880 --> 00:30:02,800
and you see that two of very classical originally now simpler learning enterprise so that

428
00:30:02,800 --> 00:30:04,410
actually works very well

429
00:30:04,920 --> 00:30:08,950
but we're just not interested in that we prefer you know like

430
00:30:08,970 --> 00:30:10,030
the only way

431
00:30:10,040 --> 00:30:15,920
instead we use that simple features cities like features which are related to gently till

432
00:30:15,960 --> 00:30:18,770
task like in know because it would be text

433
00:30:18,820 --> 00:30:23,970
gently straight text and try to design this time quite complex system of this text

434
00:30:24,870 --> 00:30:28,470
i mean the specs walls and

435
00:30:28,480 --> 00:30:37,090
you hope that your system is going to implicitly down the right features

436
00:30:37,940 --> 00:30:43,090
existing cure which processing system we tend to use like you know the first the

437
00:30:45,370 --> 00:30:49,440
so the usually design and quality features and food

438
00:30:49,660 --> 00:30:53,220
if you don't do like shallow classifier like this public television

439
00:30:53,240 --> 00:30:57,960
sometimes because you got on top of something it's overnight

440
00:30:57,980 --> 00:31:00,980
so far just like part of speech thinking

441
00:31:01,000 --> 00:31:02,300
feature actually

442
00:31:02,310 --> 00:31:07,710
usually quite simple it's like a combination of false unlabelled so i mean it's OK

443
00:31:07,710 --> 00:31:10,400
but from the more complicated task

444
00:31:10,410 --> 00:31:11,710
it tends to be

445
00:31:11,770 --> 00:31:15,490
but meanwhile in particular for semantic role labeling

446
00:31:15,520 --> 00:31:16,400
and actually

447
00:31:16,410 --> 00:31:19,240
it's sometimes even more complicated than that

448
00:31:19,320 --> 00:31:25,630
people tend to like cascade features so far semantic colorings will like first handcrafted features

449
00:31:25,850 --> 00:31:30,320
get from is crafted features the part of speech tags

450
00:31:30,350 --> 00:31:34,930
from this part of speech tags they would like to be in the posterior

451
00:31:34,930 --> 00:31:39,570
and then from the sparse trees it would again handcrafted features

452
00:31:39,580 --> 00:31:43,270
and if it gains features to a new classifier

453
00:31:43,340 --> 00:31:46,170
and they would have been the tags are interested

454
00:31:51,220 --> 00:31:53,350
we think that you know

455
00:31:53,370 --> 00:31:58,250
trying to

456
00:31:58,270 --> 00:32:03,300
i do like some task specific engineering really like limits the natural language processing so

457
00:32:03,300 --> 00:32:06,570
called because it's time you have like a new task

458
00:32:06,580 --> 00:32:12,480
you need again to find new features and as tasks tend to be more and

459
00:32:12,480 --> 00:32:15,090
more complicated variables you need to

460
00:32:15,130 --> 00:32:15,860
get you know

461
00:32:15,880 --> 00:32:21,100
one more complicated features so instead we would like to find some kind of unified

462
00:32:21,100 --> 00:32:24,260
hidden representations of text which

463
00:32:24,260 --> 00:32:25,150
would you know

464
00:32:25,500 --> 00:32:29,990
represent very well text on wood which would work for any kind of natural language

465
00:32:29,990 --> 00:32:32,810
processing task which involves tagging

466
00:32:32,820 --> 00:32:37,020
and as you see the presentation we would be able then to

467
00:32:37,030 --> 00:32:41,070
build some kind of unified natural language processing architecture

468
00:32:41,090 --> 00:32:43,080
without any

469
00:32:43,090 --> 00:32:47,120
external and internal

470
00:32:47,130 --> 00:32:49,710
so we want to work towards the actions

471
00:32:50,370 --> 00:32:51,850
to achieve this goal

472
00:32:51,870 --> 00:32:55,820
we like going to start from scratch will forget

473
00:32:55,820 --> 00:32:58,940
most of the john which was a single knowledge

474
00:32:58,960 --> 00:33:04,250
we're going to keep actually then that is the natural language processing benchmarks to see

475
00:33:04,250 --> 00:33:05,070
if we do

476
00:33:05,090 --> 00:33:06,580
and you know any good

477
00:33:06,590 --> 00:33:07,620
but really

478
00:33:07,640 --> 00:33:12,230
in this talk to remember all of the which is avoid

479
00:33:12,240 --> 00:33:17,310
that's specific engineering

480
00:33:17,320 --> 00:33:20,140
so we are doing the job was processing

481
00:33:20,160 --> 00:33:22,010
we need some kind of

482
00:33:22,030 --> 00:33:24,030
complicated systems

483
00:33:24,040 --> 00:33:28,630
something which would scale because we have a lot of the

484
00:33:28,690 --> 00:33:33,990
something which would like to find some out he then all preservation so i don't

485
00:33:34,000 --> 00:33:34,580
know if

486
00:33:34,620 --> 00:33:38,860
you know it's true it's about in your head but for me the obvious choice

487
00:33:39,760 --> 00:33:41,050
and i let one

488
00:33:41,070 --> 00:33:46,370
so maybe you forgot about knowledge work because it's quite old

489
00:33:46,390 --> 00:33:51,100
but for me and i know it works just like a series of

490
00:33:51,180 --> 00:33:53,000
metrics based on computations

491
00:33:53,010 --> 00:33:55,850
followed by some some monday night

492
00:33:55,870 --> 00:34:00,010
so in all cases we we use like a simple

493
00:34:00,010 --> 00:34:05,440
i mean it's like the house version of the hyperbolic tangent but you can use

494
00:34:05,440 --> 00:34:06,550
whatever you want

495
00:34:06,590 --> 00:34:11,690
so you can you know stacks is layers

496
00:34:11,720 --> 00:34:13,510
and basically at it's clear

497
00:34:13,890 --> 00:34:16,300
you get

498
00:34:16,300 --> 00:34:20,630
an increasing level of abstraction of your input data

499
00:34:21,210 --> 00:34:22,760
the cool thing is

500
00:34:22,770 --> 00:34:25,810
that's the

501
00:34:26,080 --> 00:34:31,500
what we call the weights the parliament of each trails just trained and trained by

502
00:34:31,500 --> 00:34:33,750
on something like gradient descent

503
00:34:35,620 --> 00:34:40,750
it looks OK like that because we would legs and obtain like he did presentation

504
00:34:41,870 --> 00:34:44,630
it's like you know what the main point here is

505
00:34:44,640 --> 00:34:50,380
how can we fit you know text usually knowledge works sunlight just you know number

506
00:34:50,380 --> 00:34:52,540
of so i can we find the text

507
00:34:56,850 --> 00:35:00,660
it's actually quite simple you just like my simple

508
00:35:01,230 --> 00:35:05,720
next two like a feature space you embed them into a feature space

509
00:35:05,730 --> 00:35:11,100
so you want is that just like a fifty dimensional feature vector nothing else

510
00:35:11,130 --> 00:35:15,160
but this feature that you know is going to be trained

511
00:35:16,870 --> 00:35:21,720
well what happened before were

512
00:35:23,280 --> 00:35:28,510
no no no no no as you might them to like feature space and you

513
00:35:28,510 --> 00:35:32,710
trains you know by gradient descent you really like train them

514
00:35:32,730 --> 00:35:38,660
she was trained this is an important point that train

515
00:35:40,580 --> 00:35:44,020
the in the implementation is like you know empolis

516
00:35:44,030 --> 00:35:45,100
basically i

517
00:35:45,130 --> 00:35:49,580
a kind of lookup tables which has a lot of tasks like it's a big

518
00:35:49,580 --> 00:35:52,020
metrics of the size and the

519
00:35:52,030 --> 00:35:54,890
the size of the likelihood of feature size you want for

520
00:35:54,910 --> 00:35:57,110
there was five hundred fifty

521
00:35:58,370 --> 00:36:01,820
the number of false you have in the dictionary

522
00:36:01,820 --> 00:36:07,590
and basically the lookup table just return for one one which is an index in

523
00:36:07,590 --> 00:36:08,750
some dictionaries

524
00:36:08,760 --> 00:36:14,850
it returns back the corresponding column in the big metrics but it

525
00:36:14,870 --> 00:36:16,520
and train

526
00:36:16,540 --> 00:36:17,120
you know

527
00:36:18,970 --> 00:36:20,930
by gradient descent

528
00:36:20,930 --> 00:36:27,290
so we have this kind of approach assuming that sigma had the gram matrix has

529
00:36:27,290 --> 00:36:32,830
restricted l one i can value which is well behaved you get out fast convergence

530
00:36:32,830 --> 00:36:36,770
rates and l one estimation error bound

531
00:36:36,830 --> 00:36:39,870
OK so

532
00:36:39,910 --> 00:36:45,500
isabelle said that maybe there journal seemingly papers and so

533
00:36:45,540 --> 00:36:49,980
many people improve things but what's going on here also what

534
00:36:50,000 --> 00:36:53,410
this whole framework is is you just make

535
00:36:53,430 --> 00:36:57,770
the appropriate assumptions to prove what you would like to prove i mean it's a

536
00:36:57,770 --> 00:37:02,560
bit exaggerated but this is not this is in way not that hard i mean

537
00:37:02,560 --> 00:37:06,770
you just make the right definition to restrict the one i can value

538
00:37:06,830 --> 00:37:10,060
and then you crank it out and it's not too much work in way of

539
00:37:10,060 --> 00:37:14,730
course it took some time to see how this can be done but

540
00:37:14,750 --> 00:37:20,270
in a way the real question for me is

541
00:37:20,810 --> 00:37:28,680
how restrictive or such conditions how restrictive it is restricted to one i can condition

542
00:37:29,690 --> 00:37:34,370
and here is one and more technical answer well it's actually weaker than what most

543
00:37:34,370 --> 00:37:37,040
people use usually statistics so far

544
00:37:37,410 --> 00:37:43,230
this is the restricted l two wagon value assumptions due to be queried of tsybakov

545
00:37:43,230 --> 00:37:49,020
so this is what i just presented you before it is slightly weaker and here

546
00:37:49,040 --> 00:37:51,140
is kind of more general picture

547
00:37:51,140 --> 00:37:52,120
i don't know

548
00:37:52,160 --> 00:37:57,450
one imposed that you understand that in detail but the picture should give somehow the

549
00:37:57,450 --> 00:38:00,000
grand view of what's going on

550
00:38:00,020 --> 00:38:05,810
so there's lots a huge literature in high dimensional statistical inference and you see in

551
00:38:05,810 --> 00:38:11,640
this picture here are various assumptions and various implications k and there's a tendency from

552
00:38:11,640 --> 00:38:16,370
the left to the right so the left or strong assumptions on the right are

553
00:38:16,370 --> 00:38:20,540
the weak assumption just if you look at this graph right and way on the

554
00:38:20,540 --> 00:38:25,460
left is the restricted isometry property this is come this time i mean this is

555
00:38:25,460 --> 00:38:29,270
great work but it's very restrictive assumptions

556
00:38:29,290 --> 00:38:33,750
OK this is in compressed sensing i think many people work with the restricted isometry

557
00:38:33,750 --> 00:38:38,980
property but it's just much more restrictive than what people have worked out now for

558
00:38:38,980 --> 00:38:45,120
the lasso in linear models your other assumptions representable welcomed assumptions and then way out

559
00:38:45,120 --> 00:38:49,210
here is this compatibility restricted l one i can assumption

560
00:38:49,230 --> 00:38:53,540
so it seems to be a weak assumption but nevertheless i mean that's not really

561
00:38:53,540 --> 00:38:59,710
fully convincing you want to know i mean it's really what's going on in practice

562
00:38:59,710 --> 00:39:01,370
does it hold

563
00:39:01,390 --> 00:39:03,640
at least approximately in practice

564
00:39:03,660 --> 00:39:08,580
and the first thing is it's not checkerboard it's impossible to check it you would

565
00:39:08,580 --> 00:39:09,060
need to go

566
00:39:09,560 --> 00:39:14,540
you would need to know your true active safety issue we can take the assumptions

567
00:39:14,560 --> 00:39:18,640
in statistics i mean you cannot check with a linear model is correct or not

568
00:39:19,350 --> 00:39:24,160
i mean some people quite interesting like you did skinnier offski to come up with

569
00:39:24,160 --> 00:39:29,460
checkable assumptions is nice so you have the design matrix lexicon shake the assumptions are

570
00:39:29,460 --> 00:39:31,830
fulfilled but

571
00:39:31,870 --> 00:39:36,930
the checklist sumptions are substantially stronger non technical assumptions so there's is a trade-off here

572
00:39:38,120 --> 00:39:44,080
and here is kind of justification why we think this restricted i one i value

573
00:39:44,080 --> 00:39:49,690
assumption is kind of reasonable in practice and in the following scenario you say OK

574
00:39:49,690 --> 00:39:51,460
you xis

575
00:39:51,480 --> 00:39:56,370
in your matrix they have been sampled as i i d roads

576
00:39:57,190 --> 00:40:01,060
so you say x one xn the rose in your matrix they are i i

577
00:40:01,830 --> 00:40:06,060
have mean zero that's not the restriction to have the covariance structure sigma

578
00:40:06,080 --> 00:40:12,270
so sick most of population covariance structure of you're IID sampling for the matrix

579
00:40:12,870 --> 00:40:14,370
and now you assume

580
00:40:14,370 --> 00:40:19,390
that the l one restricted i value holds for u sigma for the population covariance

581
00:40:19,790 --> 00:40:21,080
that's an issue

582
00:40:21,080 --> 00:40:23,730
but here the end has

583
00:40:23,750 --> 00:40:28,020
the end is not the parent anymore this is there's no sample size involved in

584
00:40:28,020 --> 00:40:29,100
this anymore

585
00:40:29,120 --> 00:40:30,430
it's just the p

586
00:40:30,450 --> 00:40:35,420
times p matrix so we don't know with high dimensional not the relation into pieces

587
00:40:35,420 --> 00:40:39,960
lost it peter by p matrix and i just want to know whether it p

588
00:40:40,180 --> 00:40:42,310
by p matrix is reasonable

589
00:40:42,330 --> 00:40:48,230
and so we assume actually it has a well behaved restricted one i can values

590
00:40:48,250 --> 00:40:51,520
the specic the population six

591
00:40:51,540 --> 00:40:56,140
and that's OK i mean sometimes even you could and p matrix has well behaved

592
00:40:56,140 --> 00:40:58,930
i can about is if you take it templates matrix

593
00:40:58,930 --> 00:41:03,210
it has well behaved i can values even if p he goes to a million

594
00:41:03,230 --> 00:41:07,640
if you take a correlation it has a well behaved i can so we kind

595
00:41:07,640 --> 00:41:13,290
of thing it's not too unreasonable to say that the population sigma has reasonably behaved

596
00:41:13,290 --> 00:41:15,230
restricted in one i could use

597
00:41:15,250 --> 00:41:19,450
and then you need some moment conditions its for example including the gauss in case

598
00:41:19,460 --> 00:41:20,870
and then again

599
00:41:20,890 --> 00:41:22,890
the sparsity principle

600
00:41:22,890 --> 00:41:25,830
if we assume that the number of

601
00:41:25,850 --> 00:41:32,120
active variables in the linear model is in this regime not growing faster in square

602
00:41:32,120 --> 00:41:33,950
over log p

603
00:41:33,950 --> 00:41:38,040
then you can come up with this bound with high probability so then

604
00:41:38,060 --> 00:41:44,180
you're empirical drama matrix has also well behaved restricted i one i can values it's

605
00:41:44,180 --> 00:41:47,270
just loosing factor here

606
00:41:47,950 --> 00:41:52,960
from my view this is maybe the best justification why this is restricted eigenvalue assumption

607
00:41:52,980 --> 00:41:57,480
kind of holes in practice if you believe this kind of scenario in this part

608
00:41:57,480 --> 00:41:58,890
of the regime

609
00:41:58,910 --> 00:42:01,770
it holds with high probability

610
00:42:01,870 --> 00:42:04,680
OK so this was kind of my theory block

611
00:42:04,680 --> 00:42:05,690
a summary

612
00:42:05,710 --> 00:42:07,060
for the last so

613
00:42:07,080 --> 00:42:09,960
so for fixed design linear models

614
00:42:10,000 --> 00:42:14,140
if you make known design assumptions and all they can as crazy as possible

615
00:42:14,180 --> 00:42:17,520
and you have a mild assumption on your ear at it's long

616
00:42:17,560 --> 00:42:18,410
you get

617
00:42:18,410 --> 00:42:22,960
and if the slow rate of convergence for prediction so for prediction you can do

618
00:42:22,960 --> 00:42:28,390
the job and you can actually get consistent prediction if it is sparse

619
00:42:28,410 --> 00:42:34,430
the fact to murder property two is if discomfort ability conditional is restricted one idea

620
00:42:34,460 --> 00:42:36,230
about condition holds

621
00:42:36,230 --> 00:42:38,060
four year gram matrix

622
00:42:38,080 --> 00:42:40,180
and mild assumption on here

623
00:42:40,190 --> 00:42:41,960
that's really not a big issue

624
00:42:41,960 --> 00:42:45,290
and you get fast rate for prediction

625
00:42:45,290 --> 00:42:48,730
so i'm going to talk about

626
00:42:48,750 --> 00:42:53,730
alternating direction method of multipliers and i just to give you a rough idea of

627
00:42:53,730 --> 00:42:55,330
what you what

628
00:42:55,350 --> 00:42:58,660
what i'm going to use the title what the talk is going to be

629
00:42:58,670 --> 00:43:01,250
i'm going to kind of like a tutorial

630
00:43:01,250 --> 00:43:04,700
it's going to be sort of the background i know there are people in this

631
00:43:04,700 --> 00:43:05,950
room you know

632
00:43:05,970 --> 00:43:08,970
actually way more than

633
00:43:08,970 --> 00:43:10,770
what what i'm going to talk about

634
00:43:10,790 --> 00:43:13,630
in this talk so i apologize to them

635
00:43:13,680 --> 00:43:14,740
some of them

636
00:43:14,790 --> 00:43:19,420
actually even came willingly i don't know why but there there are here

637
00:43:19,450 --> 00:43:24,910
but not for the others is just supposed to be an introduction to

638
00:43:24,910 --> 00:43:27,490
two one two one method for

639
00:43:27,490 --> 00:43:29,020
for doing

640
00:43:29,050 --> 00:43:33,750
distributed optimisation and we will get that now to show the simplest of all of

641
00:43:33,750 --> 00:43:37,700
this is taken from something that's much longer but still kind of tutorial in nature

642
00:43:37,700 --> 00:43:42,810
and it's one of these foundations and trends of booklets it's everything is available online

643
00:43:42,990 --> 00:43:46,780
i mean including source code for every example you'll see and things like that

644
00:43:49,730 --> 00:43:54,870
o and this is a joint work with with with the bunch of co-authors and

645
00:43:54,870 --> 00:43:56,070
in fact i should say

646
00:43:56,070 --> 00:43:59,260
this is mostly work from the seventies

647
00:43:59,320 --> 00:44:03,150
and in fact it could be argued to have come from in front of you

648
00:44:03,150 --> 00:44:06,870
could trace it to the fifties so so this is also

649
00:44:07,770 --> 00:44:08,810
so what's the goal

650
00:44:08,820 --> 00:44:13,570
the goal is i mean you really want to do something like arbitrary scale optimisation

651
00:44:13,590 --> 00:44:16,670
so and you know the most of them in the most obvious example

652
00:44:16,680 --> 00:44:18,490
certainly for everyone here is to do

653
00:44:18,520 --> 00:44:23,340
machine learning and statistics with huge datasets i mean that's that's that's obvious

654
00:44:23,370 --> 00:44:27,180
but actually there are many other applications so

655
00:44:27,200 --> 00:44:32,430
not all optimisation problems are machine learning once an example of another one would be

656
00:44:32,450 --> 00:44:35,230
dynamic optimisation on large scale network

657
00:44:35,230 --> 00:44:37,020
to take one example

658
00:44:37,090 --> 00:44:41,480
yes you can take of electricity dispatch i mean just to take one so in

659
00:44:42,570 --> 00:44:45,100
maybe like thirty one hundred generators

660
00:44:45,120 --> 00:44:48,180
the schedule in fifteen minute intervals for twenty four hours so

661
00:44:48,570 --> 00:44:51,270
for generator you have to work out one hundred numbers

662
00:44:51,290 --> 00:44:54,850
that's still that's not bad i don't know what we're up to that's like thirty

663
00:44:54,850 --> 00:44:57,200
thousand variable it's it's still

664
00:44:58,230 --> 00:45:02,470
i mean it it's not a small problem but it's not huge however all of

665
00:45:02,470 --> 00:45:07,760
that has to happen over network with about five thousand nodes and maybe fifteen thousand

666
00:45:10,540 --> 00:45:16,380
there are variables for the power flow every edge that network is capacity aided and

667
00:45:16,380 --> 00:45:19,510
guess what now you have a really big problem

668
00:45:19,530 --> 00:45:22,720
i mean bigger than in fact anyone can handle by the way the lights to

669
00:45:22,720 --> 00:45:26,570
come on in california so you might ask how sets off in itself in an

670
00:45:26,570 --> 00:45:27,580
ad-hoc way

671
00:45:27,600 --> 00:45:33,580
they they alternate between going back and forth between sort of network optimization scheduling generators

672
00:45:33,580 --> 00:45:37,890
and they go back and forth a couple of times and usually works and the

673
00:45:37,890 --> 00:45:42,880
lights come on the next california so much mostly i should say OK

674
00:45:44,880 --> 00:45:48,760
now the approach is going to be decentralised optimisation and so the idea is that

675
00:45:48,780 --> 00:45:55,670
you have different devices processes agents workers threads whatever you want to call it different

676
00:45:55,670 --> 00:45:57,360
entities computing entities

677
00:45:57,470 --> 00:46:02,910
the according to solve large problem by passing relatively small messages in fact that's actually

678
00:46:02,910 --> 00:46:06,110
an important point here is that this is this is not going to be something

679
00:46:06,110 --> 00:46:11,100
like implementing stochastic gradient descent on

680
00:46:11,450 --> 00:46:16,640
a data set or something like that where lots of data is flowing all over

681
00:46:16,640 --> 00:46:17,390
the place

682
00:46:17,410 --> 00:46:18,380
i mean

683
00:46:18,410 --> 00:46:22,080
that these things are optimized for that so that's fine this is given this is

684
00:46:22,080 --> 00:46:25,630
in contrast to the idea is to get a relatively small messages and a fair

685
00:46:25,630 --> 00:46:29,290
amount of heavy lifting is going to be done at each worker node and relatively

686
00:46:29,290 --> 00:46:34,230
small messages are gonna fly that's going to be the the idea and obviously this

687
00:46:34,230 --> 00:46:38,140
it's gonna the math doesn't matter but it did operate at many levels right so

688
00:46:38,140 --> 00:46:39,200
this could be

689
00:46:39,230 --> 00:46:41,780
this could be something like this could just be

690
00:46:41,790 --> 00:46:45,300
how to take you can have multiple threads and just take advantage of different causes

691
00:46:45,300 --> 00:46:49,610
could be something operating data centre could be g seven you could target GP using

692
00:46:49,610 --> 00:46:53,670
electric stuff now i i won't even talk about that but there are lots of

693
00:46:55,700 --> 00:46:57,200
so here is the outline

694
00:46:57,230 --> 00:47:02,040
so i'll go away back in history and actually the first two sections are history

695
00:47:02,270 --> 00:47:08,480
well maybe history and helps a little bit this is just the set up for

696
00:47:08,480 --> 00:47:12,980
the method we're going to talk about so it should be maybe review for most

697
00:47:14,710 --> 00:47:17,480
OK so start with dual decomposition

698
00:47:18,410 --> 00:47:21,360
that's that's what you want to solve this

699
00:47:21,390 --> 00:47:26,800
convex optimisation really minimize the convex function subject to some equality constraints and see if

700
00:47:26,800 --> 00:47:28,160
all the lagrangian

701
00:47:28,170 --> 00:47:30,860
which basically says you're going to allow

702
00:47:30,910 --> 00:47:34,390
the equality constraint to be violated and you have a residual

703
00:47:34,400 --> 00:47:36,770
and you have dual vector price vector

704
00:47:36,780 --> 00:47:42,160
and you can interpret this is something like a a payment either to or from

705
00:47:42,160 --> 00:47:45,650
it either like a charge or subsidy depending on the sign

706
00:47:46,350 --> 00:47:49,970
four what happy what happens if you violate the the constraints

707
00:47:49,980 --> 00:47:54,420
if you minimize in this in this framework over x

708
00:47:54,440 --> 00:47:58,000
not sure what you like yourself to violate the constraints but you either pay or

709
00:47:58,000 --> 00:48:02,640
are paid for through this dual variable y to to get the dual function

710
00:48:04,720 --> 00:48:08,600
you then i mean this is the dual function that's always concave and if you

711
00:48:08,600 --> 00:48:13,310
maximize the dual function of all goes well the maximizer is in fact the solution

712
00:48:13,310 --> 00:48:17,480
of this that they're like they're actually details there and not just little baby technical

713
00:48:17,480 --> 00:48:21,360
details because in fact for some of the problems you might like to solve like

714
00:48:21,360 --> 00:48:24,830
linear programs this scheme fails utterly

715
00:48:24,850 --> 00:48:29,000
right so these are not just sort of little mathematical details the things that make

716
00:48:29,000 --> 00:48:31,170
this not work but you with enough

717
00:48:31,220 --> 00:48:36,350
assumptions you know one being like f is strong is you know has minimum

718
00:48:36,400 --> 00:48:39,870
curvature something like that this method works

719
00:48:41,610 --> 00:48:43,860
so how would you

720
00:48:43,890 --> 00:48:45,970
how do you how to maximize the

721
00:48:45,980 --> 00:48:47,360
the dual function

722
00:48:48,080 --> 00:48:50,120
you reduce the gradient method

723
00:48:50,180 --> 00:48:55,580
or in the case when differentiable be subgradient methods and so you would calculate gradient

724
00:48:55,580 --> 00:49:00,660
and make a step in that direction these are positive steplengths well the gradient

725
00:49:00,670 --> 00:49:01,590
of that

726
00:49:01,600 --> 00:49:04,240
the dual function is nothing but the residual

727
00:49:04,270 --> 00:49:07,780
so the algorithm that looks like this is called dual as and and this is

728
00:49:07,780 --> 00:49:12,650
maybe if maybe from the sixties but probably people knew it in in moscow in

729
00:49:12,650 --> 00:49:15,340
the fifties or something like that and

730
00:49:15,350 --> 00:49:18,490
so i mean that's where all that's role this comes from and to the dual

731
00:49:18,490 --> 00:49:21,920
ascent method looks like this and this should remember because you be seeing this for

732
00:49:21,920 --> 00:49:26,350
the rest of us our OK so it looks like this you take this that

733
00:49:26,350 --> 00:49:32,050
lagrangian you fix the price vector dual vector new minimize over x

734
00:49:32,100 --> 00:49:36,180
the minimizer x you calculate the residual but if the residuals are you quit because

735
00:49:36,180 --> 00:49:38,190
you your optimal you stop

736
00:49:38,220 --> 00:49:43,230
OK if it's not zero you update the price the dual vector in fact by

737
00:49:43,230 --> 00:49:46,480
some positive number times the residual and that's it

738
00:49:46,490 --> 00:49:50,150
so that's it then it's got other names in economics you call this the like

739
00:49:50,150 --> 00:49:55,420
attached on more procedure something like that or price adjustment procedure and the idea is

740
00:49:55,740 --> 00:50:00,090
the a x equals b constraint there would be something like a market clearing

741
00:50:00,100 --> 00:50:02,980
and then this is the price adjustment mechanism

742
00:50:03,280 --> 00:50:07,030
and to that so it's got other names and long traditions in in lots of

743
00:50:07,030 --> 00:50:08,900
fields OK

744
00:50:08,910 --> 00:50:12,190
you first of you might ask you know why right why why should you solve

745
00:50:12,190 --> 00:50:14,840
the dual instead of the prime or something like that

746
00:50:16,040 --> 00:50:19,540
and the reason has to do with the composition so

747
00:50:19,540 --> 00:50:28,040
and in a

748
00:50:44,670 --> 00:50:51,320
at all

749
00:51:42,480 --> 00:51:53,550
i mean

750
00:52:54,610 --> 00:52:57,780
i mean it

751
00:53:58,170 --> 00:54:05,620
you are

752
00:54:06,520 --> 00:54:07,560
when you these

753
00:54:09,720 --> 00:54:11,820
this way

754
00:54:18,190 --> 00:54:25,430
right and i mean

755
00:54:25,440 --> 00:54:28,780
and one can

756
00:54:28,800 --> 00:54:31,470
and i

757
00:54:56,690 --> 00:54:59,100
no no

758
00:54:59,170 --> 00:55:07,290
well right

759
00:55:09,490 --> 00:55:14,010
o are

760
00:55:53,980 --> 00:55:59,240
you are you know

761
00:56:27,110 --> 00:56:31,020
and you

762
00:56:31,020 --> 00:56:34,670
i don't want to create a few things to say about single like one tools

763
00:56:34,670 --> 00:56:37,070
of the expected avoid slide

764
00:56:37,230 --> 00:56:38,530
well i agree

765
00:56:38,560 --> 00:56:43,120
what they mean what they say they want to simulated nearly in instantly needing effectively

766
00:56:43,140 --> 00:56:44,480
you just switch

767
00:56:44,500 --> 00:56:46,380
one time

768
00:56:47,980 --> 00:56:49,460
six times

769
00:56:51,250 --> 00:56:54,750
in fact you do it according to some statistical

770
00:56:54,830 --> 00:56:59,310
measure which surround not those around us

771
00:56:59,460 --> 00:57:00,750
in the

772
00:57:00,850 --> 00:57:02,300
what you need to

773
00:57:02,410 --> 00:57:07,460
so-called cooling schedule supposed to simulate what happens when you need a piece of metal

774
00:57:07,710 --> 00:57:14,830
but the theoretical result the simulated annealing always find the absolute optimal solutions

775
00:57:14,840 --> 00:57:16,420
given enough

776
00:57:16,430 --> 00:57:17,420
the words

777
00:57:18,460 --> 00:57:22,160
time but in fact

778
00:57:22,170 --> 00:57:26,620
it really does take a long time so it is not really work although it

779
00:57:27,630 --> 00:57:30,060
it effectively will always

780
00:57:31,060 --> 00:57:37,620
theoretically we always choose alpha expansion model which is similar to the recent rise well

781
00:57:37,620 --> 00:57:39,530
for special better and in the sense

782
00:57:39,590 --> 00:57:41,100
it is because

783
00:57:41,120 --> 00:57:42,900
the following table idea

784
00:57:42,950 --> 00:57:45,450
if you

785
00:57:45,500 --> 00:57:48,810
you have

786
00:57:55,390 --> 00:57:57,450
a bunch of pixels

787
00:57:57,510 --> 00:58:00,630
and they all have some labels around

788
00:58:02,900 --> 00:58:04,090
nine eight

789
00:58:04,100 --> 00:58:06,780
like this one as well

790
00:58:06,800 --> 00:58:09,850
users say OK this time this

791
00:58:09,860 --> 00:58:12,450
pixels chance to sort of switch

792
00:58:12,470 --> 00:58:14,350
so do you want to switch

793
00:58:14,350 --> 00:58:15,660
maybe these all

794
00:58:15,670 --> 00:58:17,140
wrongly labeled

795
00:58:17,170 --> 00:58:19,420
they're all wrong labels together

796
00:58:19,480 --> 00:58:20,620
right so

797
00:58:20,620 --> 00:58:23,760
if you give this pixel chance to switch

798
00:58:23,780 --> 00:58:25,660
people say well

799
00:58:25,660 --> 00:58:28,780
i prefer stay where i am my friends are right

800
00:58:28,830 --> 00:58:31,750
i don't see the film i have

801
00:58:31,820 --> 00:58:34,490
that will be

802
00:58:35,240 --> 00:58:39,380
in fact you increase the cost is one to to move

803
00:58:39,380 --> 00:58:43,020
right because if you want to zero that a lot of weight between all these

804
00:58:43,130 --> 00:58:45,110
functions right but

805
00:58:45,160 --> 00:58:46,660
you know for expansion

806
00:58:46,680 --> 00:58:49,050
these all have the opportunity

807
00:58:49,100 --> 00:58:50,740
switching to get

808
00:58:52,030 --> 00:58:53,950
the whole area of image

809
00:58:53,990 --> 00:58:57,250
the terms that really has been wrongly

810
00:58:57,460 --> 00:59:01,780
label then it'll switch in fact this which was released in the early stages after

811
00:59:03,180 --> 00:59:06,100
large numbers of pixels tend to switch

812
00:59:07,110 --> 00:59:10,060
just try to do them one by one they all held back

813
00:59:10,070 --> 00:59:12,490
by the neighbouring pixels is only the

814
00:59:12,590 --> 00:59:14,030
it's only the

815
00:59:15,660 --> 00:59:19,860
in the process simulated annealing which allows to to give us is not really

816
00:59:21,230 --> 00:59:24,010
very useful these days

817
00:59:28,410 --> 00:59:30,100
final office

818
00:59:30,150 --> 00:59:36,390
function computationally intensive graph cuts but that's not really too bad

819
00:59:36,970 --> 00:59:47,420
after expansion the one pixel is doesn't get linear

820
00:59:47,430 --> 00:59:50,670
the same is also such a thing called

821
00:59:53,750 --> 00:59:55,580
which also doesn't work

822
00:59:55,640 --> 01:00:00,450
taking well

823
01:00:01,490 --> 01:00:04,700
yes it does because i mean you saw that the thing

824
01:00:04,740 --> 01:00:10,410
the land in the image of the behavior of the lambda which is quite narrow

825
01:00:10,410 --> 01:00:11,850
stuff after that

826
01:00:11,930 --> 01:00:14,070
was sufficiently

827
01:00:16,120 --> 01:00:18,840
background going

828
01:00:24,200 --> 01:00:25,660
so he offered

829
01:00:25,670 --> 01:00:29,430
simulated annealing nineteen hours

830
01:00:31,410 --> 01:00:33,860
after expansion ninety six

831
01:00:33,870 --> 01:00:36,580
the better results

832
01:00:38,030 --> 01:00:40,810
what happened just

833
01:00:40,810 --> 01:00:44,120
this for all time to make it

834
01:00:44,230 --> 01:00:45,530
if you could use

835
01:00:45,630 --> 01:00:48,060
cooling schedule wrong when they get

836
01:00:51,200 --> 01:00:57,510
i think this is a million ways

837
01:00:57,520 --> 01:01:02,660
you see that we see is that today was to be i layout

838
01:01:02,660 --> 01:01:06,660
yes or something like

839
01:01:06,770 --> 01:01:11,630
genetic out alternative to things

840
01:01:16,620 --> 01:01:22,170
alpha expansion practice results depend on initialisation very much

841
01:01:22,200 --> 01:01:28,660
this slide initialisation important practice not in theory

842
01:01:34,450 --> 01:01:40,670
just one thing about this with simulated annealing with the cell alpha expansion rather

843
01:01:40,840 --> 01:01:43,140
built for me

844
01:01:44,050 --> 01:01:44,980
first in the middle

845
01:01:45,000 --> 01:01:48,900
the people who did this dynamic graph cuts you can use dynamic graph cuts in

846
01:01:48,900 --> 01:01:50,350
this world

847
01:01:50,390 --> 01:01:51,410
right by

848
01:01:51,430 --> 01:01:55,110
so let's have a look at this is an example of simulated annealing

849
01:01:55,110 --> 01:01:57,400
this is an image which people use a lot

850
01:01:57,420 --> 01:02:01,120
to segment of tree in the garden house

851
01:02:01,130 --> 01:02:05,690
it is given labeled tree got house

852
01:02:05,750 --> 01:02:07,890
it's not

853
01:02:07,940 --> 01:02:11,960
the semantic meaning give to computer

854
01:02:12,010 --> 01:02:15,180
the various steps to really offer

855
01:02:15,200 --> 01:02:17,520
a lot of expansion

856
01:02:17,570 --> 01:02:19,230
often only

857
01:02:19,310 --> 01:02:20,880
one by one

858
01:02:28,420 --> 01:02:30,090
if you do

859
01:02:30,130 --> 01:02:34,670
if you do this often expansion then you're doing alpha alpha equals one two three

860
01:02:34,670 --> 01:02:35,660
et cetera

861
01:02:35,710 --> 01:02:40,260
you've got all these different graphs which is setting up every time

862
01:02:40,400 --> 01:02:44,080
called one iteration through the

863
01:02:44,120 --> 01:02:48,900
the next time you try to think particularly as you approaching final solution

864
01:02:48,910 --> 01:02:50,950
graphs can be very similar

865
01:02:52,420 --> 01:02:56,990
nothing much has changed from one hundred thirteen years techniques

866
01:02:57,010 --> 01:02:59,250
reusing graphs

867
01:02:59,330 --> 01:03:01,670
because they have to change such that

868
01:03:01,690 --> 01:03:06,050
i don't believe that range expansion

869
01:03:06,120 --> 01:03:09,410
and i also think recycle in iteration

870
01:03:09,420 --> 01:03:12,160
so these type improvements to get

871
01:03:12,180 --> 01:03:13,060
using that

872
01:03:13,860 --> 01:03:14,930
more about that

873
01:03:15,000 --> 01:03:16,860
but it does allow you

874
01:03:16,860 --> 01:03:18,000
two years

875
01:03:18,020 --> 01:03:21,130
the self expansion technique in close to real time

876
01:03:21,130 --> 01:03:23,180
on things like the

877
01:03:23,230 --> 01:03:25,190
i think they're getting about six

878
01:03:25,240 --> 01:03:26,510
one frame

879
01:03:26,550 --> 01:03:28,420
every sixty minutes seconds

880
01:03:28,430 --> 01:03:30,560
so it's not like

881
01:03:30,670 --> 01:03:33,430
the second from right

882
01:03:33,490 --> 01:03:39,480
on on his browsing power this is not a very difficult problem browser can write

883
01:03:39,480 --> 01:03:43,920
space a all these solid lines that was the lines

884
01:03:43,930 --> 01:03:49,040
correspond to two things which are parallel to the plane so we have very high

885
01:03:49,220 --> 01:03:50,330
plains here

886
01:03:50,350 --> 01:03:55,220
in some feature space which is non linearly related to this in domain and this

887
01:03:55,220 --> 01:03:57,250
solid lines

888
01:03:57,260 --> 01:04:02,140
on the traces of these have constraints on the domain

889
01:04:02,150 --> 01:04:03,760
so you can see in this situation

890
01:04:03,780 --> 01:04:06,080
almost all the points become support vectors

891
01:04:09,070 --> 01:04:11,580
five gave

892
01:04:21,360 --> 01:04:26,650
there can be situations where even in high dimensional spaces the problem is not separable

893
01:04:26,690 --> 01:04:31,930
and in situations where even if the problem is separable you don't want to understand

894
01:04:31,810 --> 01:04:33,590
separate it precisely

895
01:04:33,650 --> 01:04:35,450
so sometimes you don't want to

896
01:04:35,490 --> 01:04:38,920
you want to a lot of possibilities use training error

897
01:04:39,220 --> 01:04:42,100
so is that it allows the optimisation problem

898
01:04:42,120 --> 01:04:46,740
and there are ways you should tell you about it because if you apply support

899
01:04:46,750 --> 01:04:50,480
the tutorial program you are likely to use

900
01:04:50,990 --> 01:04:56,250
how much he's to about and how it can be used

901
01:04:56,260 --> 01:04:58,040
of the machine

902
01:04:58,100 --> 01:05:00,700
so it is so much to protect the machine

903
01:05:01,680 --> 01:05:03,370
if we a constrained

904
01:05:03,400 --> 01:05:07,950
this can be satisfied so this

905
01:05:07,970 --> 01:05:10,340
after this last year

906
01:05:11,420 --> 01:05:14,070
the slide about the optimisation problem

907
01:05:14,100 --> 01:05:18,990
suppose there is a point which no matter what we do we cannot correctly classified

908
01:05:19,020 --> 01:05:21,600
as an economist correctly classified points

909
01:05:21,760 --> 01:05:25,970
suppose that we cannot change such that this is avoided

910
01:05:25,970 --> 01:05:32,160
this was all this is always negative that means the alpha i would actually increase

911
01:05:32,480 --> 01:05:34,960
without bound so will go to infinity

912
01:05:35,130 --> 01:05:39,270
so that's bad the optimisation problem does not converge

913
01:05:39,300 --> 01:05:45,090
what do we do in this case a naive solution would be what and was

914
01:05:45,090 --> 01:05:49,540
simply place upper bound on the phi just stop going to infinity it would sound

915
01:05:49,540 --> 01:05:54,350
like cheap hack but it turns out that's one of the solutions

916
01:05:54,370 --> 01:06:00,670
only that solution can be motivated slightly differently so you can motivate the solution as

917
01:06:00,670 --> 01:06:06,850
follows you can modify the constraint it OK i can't satisfy this constraint

918
01:06:06,860 --> 01:06:11,490
so i think the constrained limits simpler by subtracting something from the right hand side

919
01:06:11,490 --> 01:06:17,850
this something is XIIA what people call a slack variable but then i don't want

920
01:06:17,850 --> 01:06:21,020
to make excessive use of the slack variables and we want to use them which

921
01:06:21,030 --> 01:06:26,140
is really necessary so i would penalize the slack variables in the objective function so

922
01:06:26,140 --> 01:06:29,520
i will have to turn like this the some of all the slack variables

923
01:06:29,530 --> 01:06:33,260
time some trade constant to the objective function

924
01:06:33,260 --> 01:06:35,060
so it turns out if you

925
01:06:35,080 --> 01:06:39,230
do this and you go through the whole culture value derive you do problems but

926
01:06:39,230 --> 01:06:43,230
you end up with is exactly that you get an upper bound on this

927
01:06:43,230 --> 01:06:47,680
o five variables and the upper bound will be this consistency here

928
01:06:47,700 --> 01:06:55,780
so that's the just the bodies called the c support vector machine

929
01:06:55,790 --> 01:07:02,300
that's the first soft margin SVM was proposed that a slightly more elegant way of

930
01:07:02,300 --> 01:07:07,450
doing soft margin which is called the new support vector machines

931
01:07:07,490 --> 01:07:13,290
in that case the parameter that we use for the trade off is called new

932
01:07:13,310 --> 01:07:15,890
and it has some physical meaning that i'll tell you about

933
01:07:15,970 --> 01:07:20,950
in that case the objective function so the sum of six II's it does not

934
01:07:20,950 --> 01:07:24,260
into the objective function via multiplication of c

935
01:07:24,280 --> 01:07:31,790
but it and has like this sort of like with a constant multiplier and in

936
01:07:31,790 --> 01:07:35,640
addition we have this term here minus new times rule

937
01:07:35,650 --> 01:07:36,420
where rho

938
01:07:36,430 --> 01:07:41,930
it is the variable of the optimisation problem of the primal problem and rho also

939
01:07:41,930 --> 01:07:44,940
enters the constraints in here

940
01:07:44,950 --> 01:07:48,000
so i'll

941
01:07:48,030 --> 01:07:52,320
probably come back to this a little bit later but first well i'll tell you

942
01:07:52,330 --> 01:07:57,620
something more about this from ten new and later on try to explain why this

943
01:07:57,620 --> 01:08:01,800
is the case of explained in the case of regression may lead to a tiny

944
01:08:02,050 --> 01:08:03,280
bit easier to see

945
01:08:03,360 --> 01:08:09,650
but effectively is the same property so what does this parameter new meaning

946
01:08:09,650 --> 01:08:12,220
sorry man remember

947
01:08:12,240 --> 01:08:17,430
we can we can define support vectors as points which have positive i

948
01:08:17,440 --> 01:08:20,050
some being a little bit sloppy here

949
01:08:20,070 --> 01:08:24,250
before i was saying support vectors of points that lines

950
01:08:24,260 --> 01:08:31,440
supporters of points for which the constraints are met as equalities

951
01:08:31,510 --> 01:08:36,210
and the consequent tucker conditions not tell us if a constraint is medicine inequality than

952
01:08:36,210 --> 01:08:39,570
the phi can be nonzero but they can be

953
01:08:39,590 --> 01:08:44,990
in principle that can be support vectors which exactly meet the constraint but which have

954
01:08:44,990 --> 01:08:49,020
a zero five but that would be very coincidental situation

955
01:08:49,020 --> 01:08:54,850
so anyway let's let's consider those points are support vectors from which we have positive

956
01:08:54,850 --> 01:08:55,990
nk is

957
01:08:56,010 --> 01:08:58,280
that is the number of customers

958
01:08:58,310 --> 01:09:01,030
currently sitting at table k

959
01:09:01,210 --> 01:09:05,970
and then the customer and will sit at a new table less labeled as k

960
01:09:05,990 --> 01:09:07,700
plus one with probability

961
01:09:07,700 --> 01:09:10,570
out proportional to alpha

962
01:09:10,590 --> 01:09:15,950
so customer one comes in and says here customer two can either sit at the

963
01:09:15,950 --> 01:09:21,780
first table with probability one of one class alpha all the second table with probability

964
01:09:21,780 --> 01:09:23,480
alpha one class of

965
01:09:23,480 --> 01:09:27,620
so in this case the second customer sits decides to sit with the first customer

966
01:09:27,620 --> 01:09:31,100
and the customer comes in and can decide between sitting at the first table the

967
01:09:31,100 --> 01:09:32,230
second table

968
01:09:32,310 --> 01:09:37,030
and if a customer sits here both customers here fifty six seven eight nine and

969
01:09:37,050 --> 01:09:38,890
so on and what this

970
01:09:38,910 --> 01:09:43,560
yes this is the partitions of the integers one to nine

971
01:09:43,590 --> 01:09:47,280
that simply says that one two and four belong to the same

972
01:09:48,510 --> 01:09:52,990
five in tree belong to the same cluster six belong to its own cluster seven

973
01:09:53,220 --> 01:09:54,450
belong to

974
01:09:54,460 --> 01:09:58,050
to a cluster and line belong to its own cluster

975
01:09:58,110 --> 01:10:01,490
so this basically corresponds to saying that

976
01:10:01,520 --> 01:10:05,560
theta one theta two and three to four takes on the same value

977
01:10:05,580 --> 01:10:08,610
the tree in figure five takes on the same value and so on

978
01:10:08,620 --> 01:10:09,880
so this

979
01:10:09,890 --> 01:10:14,020
this chinese restaurant process basically tells us the clustering structure

980
01:10:15,090 --> 01:10:20,040
on the fetus

981
01:10:20,060 --> 01:10:26,610
if you look at the clustering structure the chinese restaurant process it exhibits count two

982
01:10:26,670 --> 01:10:30,550
properties i guess the first one is rich gets richer

983
01:10:30,610 --> 01:10:33,190
in fact right because if you look at

984
01:10:33,280 --> 01:10:34,540
a class the

985
01:10:34,620 --> 01:10:36,740
that really has lots of

986
01:10:36,740 --> 01:10:40,020
a table that has that really has lots of customers

987
01:10:40,050 --> 01:10:42,520
then the chance that future customers

988
01:10:42,530 --> 01:10:46,360
i will sit on a on the same table is much higher than table that

989
01:10:46,360 --> 01:10:50,730
has very few customers so table that's rich gets richer

990
01:10:52,780 --> 01:10:54,420
so so

991
01:10:54,520 --> 01:11:00,440
what this effect is that there will be a few tables with lots of customers

992
01:11:00,450 --> 01:11:01,830
and then

993
01:11:01,850 --> 01:11:04,370
the the other tables we have

994
01:11:04,380 --> 01:11:05,740
much fewer customers

995
01:11:07,410 --> 01:11:11,460
the second effect is that

996
01:11:11,470 --> 01:11:15,730
as the number of customers goes to infinity the number of tables that used by

997
01:11:16,690 --> 01:11:18,860
customers also goes to infinity

998
01:11:18,880 --> 01:11:23,850
in fact you can show that the expected number of clusters is going to be

999
01:11:24,460 --> 01:11:27,250
on the order of alpha timeslot and

1000
01:11:27,260 --> 01:11:32,210
so of course as an goes to infinity and also goes to infinity so over

1001
01:11:32,210 --> 01:11:36,800
here we see that the concept the number of

1002
01:11:36,830 --> 01:11:40,120
the number of tables depends on the concentration parameter alpha

1003
01:11:40,130 --> 01:11:45,860
basically the the larger alpha is the more tables we expect to see

1004
01:11:45,870 --> 01:11:47,550
and this is of course

1005
01:11:47,610 --> 01:11:52,870
the result of of this thing right so

1006
01:11:52,880 --> 01:11:54,730
alpha here is basically

1007
01:11:54,740 --> 01:11:59,880
governs the probability that a new customers come in with the new tables and that's

1008
01:11:59,880 --> 01:12:03,780
going to create more tables if if alpha is larger

1009
01:12:07,640 --> 01:12:09,660
so the

1010
01:12:09,670 --> 01:12:12,260
sometimes people call

1011
01:12:12,310 --> 01:12:14,910
the the infinite mixture model

1012
01:12:14,910 --> 01:12:15,740
that we saw

1013
01:12:16,600 --> 01:12:22,760
on saturday a chinese restaurant process mixture model so basically

1014
01:12:22,760 --> 01:12:24,270
we can think of this

1015
01:12:24,280 --> 01:12:26,210
china the clustering

1016
01:12:26,260 --> 01:12:28,160
of the customers into

1017
01:12:30,820 --> 01:12:32,180
corresponding to

1018
01:12:32,190 --> 01:12:35,890
clustering of data items in two different clusters

1019
01:12:35,990 --> 01:12:40,790
in a mixture model and every table is going to correspond to a mixture component

1020
01:12:40,790 --> 01:12:43,100
in our model

1021
01:12:43,130 --> 01:12:46,840
any questions

1022
01:12:47,850 --> 01:12:52,780
it means that

1023
01:12:58,350 --> 01:13:01,580
i mean yes but

1024
01:13:03,580 --> 01:13:05,740
that there is a difference

1025
01:13:05,790 --> 01:13:08,600
a process called the pitman yor process we shall come to at the end of

1026
01:13:08,600 --> 01:13:13,800
this lecture where the scaling is not a lot and bus and to the

1027
01:13:13,820 --> 01:13:17,370
one parameter d which is between zero and one

1028
01:13:19,770 --> 01:13:25,030
we can see actually just use it is willing to use any so any probably

1029
01:13:25,070 --> 01:13:30,850
any scheme to sit customers at tables it has to satisfy a property called exchangeability

1030
01:13:30,870 --> 01:13:32,550
we shall come to in few slides

1031
01:13:33,850 --> 01:13:42,700
yes that's right it should be here less than or equal to n but typically

1032
01:13:42,700 --> 01:13:47,540
k is less than that because case on the other lot yes

1033
01:13:55,470 --> 01:13:59,470
this different interpretations actually lead to different

1034
01:13:59,520 --> 01:14:01,550
inference algorithms

1035
01:14:01,790 --> 01:14:04,890
and there are for example

1036
01:14:04,910 --> 01:14:09,190
gibbs sampling algorithms based on the chinese restaurant process is that so this is in

1037
01:14:09,190 --> 01:14:13,560
fact exactly the same give something other than that we derived last week

1038
01:14:13,580 --> 01:14:19,110
but there are also like gibbs something algorithms based on the stick breaking construction

1039
01:14:20,820 --> 01:14:21,960
once we have

1040
01:14:21,960 --> 01:14:23,990
written down each of the

1041
01:14:24,020 --> 01:14:27,960
parameters in g in the following way then we can just say let's forget about

1042
01:14:27,960 --> 01:14:32,270
g let's try to do inference where this is going to be are latent variables

1043
01:14:32,290 --> 01:14:38,180
and because of the different inference algorithms

1044
01:14:38,190 --> 01:14:39,330
can have different

1045
01:14:41,430 --> 01:14:42,930
so for example

1046
01:14:42,940 --> 01:14:48,470
with the stick stick breaking construction we can derive a variational inference algorithm pretty straightforwardly

1047
01:14:48,510 --> 01:14:50,220
by this

1048
01:14:50,230 --> 01:14:53,200
i don't know of anybody who has the right to variational inference camp for the

1049
01:14:53,200 --> 01:14:54,700
chinese restaurant process

1050
01:14:56,780 --> 01:14:59,040
i mean that

1051
01:14:59,160 --> 01:15:01,820
because we don't have a problem with people

1052
01:15:01,890 --> 01:15:08,520
for use by the way of understanding

1053
01:15:08,520 --> 01:15:16,920
the people who who say well this things don't actually give you a density for

1054
01:15:16,920 --> 01:15:19,520
the whole g i say features

1055
01:15:19,540 --> 01:15:23,830
if you just write down what the density of pi one until pi

1056
01:15:26,300 --> 01:15:31,040
OK and also we can also write down the density of the town

1057
01:15:31,070 --> 01:15:32,810
the test i want to use it

1058
01:15:32,820 --> 01:15:37,120
the test wiki we can write the density but as big k goes to infinity

1059
01:15:37,120 --> 01:15:38,850
the is simply gonna

1060
01:15:38,860 --> 01:15:40,240
go to what's

1061
01:15:40,270 --> 01:15:42,530
zero so

1062
01:15:42,560 --> 01:15:46,860
we can't actually read all those the four g itself because there this into objects

1063
01:15:46,860 --> 01:15:48,280
is infinite dimensional

1064
01:15:54,310 --> 01:15:56,160
six weeks

1065
01:15:56,170 --> 01:15:57,600
the polya urn scheme

1066
01:15:57,630 --> 01:16:00,590
in fact the first

1067
01:16:00,610 --> 01:16:06,760
the first successful MCMC schemes for the dirichlet process is based on the polya scheme

1068
01:16:06,760 --> 01:16:12,670
but nowadays people don't actually use it the problem is that it can have

1069
01:16:12,720 --> 01:16:14,620
mixes together

1070
01:16:14,700 --> 01:16:21,130
two things it makes together the clustering structure on the dirichlet process and the values

1071
01:16:21,130 --> 01:16:22,100
assigned to

1072
01:16:22,110 --> 01:16:24,310
to each cluster

1073
01:16:24,320 --> 01:16:28,970
and that can mixed inference slower so for both the chinese restaurant process and the

1074
01:16:28,970 --> 01:16:33,240
so the representation is going to be

1075
01:16:33,280 --> 01:16:36,170
and serves two questions one can ask in the world

1076
01:16:36,220 --> 01:16:39,300
so which questions what kinds of questions so here's

1077
01:16:39,420 --> 01:16:43,130
i mean make precise what's the question

1078
01:16:43,170 --> 01:16:46,520
so we can think about uncontrolled system to control systems i mean i think about

1079
01:16:46,530 --> 01:16:50,290
control systems for stop and think about controlled system the control system is one that

1080
01:16:50,290 --> 01:16:53,640
actually so a question

1081
01:16:53,660 --> 01:16:55,280
is essentially of this form

1082
01:16:55,290 --> 01:16:59,780
it's about what they use the word future some of the questions and the would

1083
01:16:59,780 --> 01:17:05,000
has still mean the same thing request the form action observation action observation action observation

1084
01:17:05,000 --> 01:17:06,040
action observation

1085
01:17:06,090 --> 01:17:08,180
the sequence of actions and observations

1086
01:17:08,240 --> 01:17:13,150
what an answer to a question the answer to the question is

1087
01:17:13,380 --> 01:17:15,230
the conditional probability

1088
01:17:15,250 --> 01:17:19,200
but you would see the sequence of observations and that question if you take the

1089
01:17:19,210 --> 01:17:21,370
sequence of actions in the question

1090
01:17:21,380 --> 01:17:24,780
so if i behave according to what the question asks me

1091
01:17:24,790 --> 01:17:27,860
what's the probability that see the actions in that

1092
01:17:27,880 --> 01:17:29,480
it is part of the question

1093
01:17:29,520 --> 01:17:33,880
that's the answer to a question

1094
01:17:33,920 --> 01:17:35,390
what i'm going to show you

1095
01:17:35,450 --> 01:17:39,640
as of this class of questions contains within it a subset

1096
01:17:39,650 --> 01:17:44,830
so define was and is sufficient to model the state of many many interesting dynamical

1097
01:17:45,980 --> 01:17:50,310
including the class system model of mdps and pomdps

1098
01:17:50,340 --> 01:17:51,960
that's where i'm going

1099
01:17:51,970 --> 01:17:57,100
so these questions are going to be entirely expression in observable terms i'm going to

1100
01:17:57,100 --> 01:17:59,530
show you that in fact

1101
01:17:59,540 --> 01:18:05,120
there is a small number of questions was answers are

1102
01:18:05,130 --> 01:18:07,800
a good representation of knowledge about the world

1103
01:18:07,860 --> 01:18:11,870
that's michael classes system i'm going to limit myself to the most of the talk

1104
01:18:11,870 --> 01:18:13,500
is going to be discrete time

1105
01:18:13,500 --> 01:18:15,270
the discrete observations

1106
01:18:15,330 --> 01:18:16,990
and finite action systems

1107
01:18:17,990 --> 01:18:21,380
MTV's comedy like system OK

1108
01:18:21,390 --> 01:18:24,260
so let me what to extract that and we define what i call the system

1109
01:18:24,260 --> 01:18:26,760
dynamics vector

1110
01:18:26,770 --> 01:18:29,010
so here's the system dynamics vector

1111
01:18:29,060 --> 01:18:31,810
basically it has

1112
01:18:31,830 --> 01:18:33,370
later on top

1113
01:18:33,380 --> 01:18:35,350
all possible questions

1114
01:18:35,360 --> 01:18:37,500
i can ask for dynamical system

1115
01:18:37,510 --> 01:18:41,540
king case is not clear why using the phrase dynamical system an agent environment interaction

1116
01:18:41,540 --> 01:18:45,340
is a dynamical system like it takes an action get an observation takes an action

1117
01:18:45,350 --> 01:18:49,670
get an observation of the dynamical system so it's fairly abstract general

1118
01:18:49,840 --> 01:18:52,950
what what i have the columns

1119
01:18:52,960 --> 01:18:53,740
as the

1120
01:18:53,750 --> 01:18:59,660
as the indices to this that there are questions so here's the length one question

1121
01:18:59,660 --> 01:19:05,230
all length one questions for bowling two questions for bowling three questions on infinity so

1122
01:19:05,230 --> 01:19:06,700
this is an infinite vector

1123
01:19:06,720 --> 01:19:11,020
and the answer to the question what are in effect this is the probability the

1124
01:19:11,020 --> 01:19:17,110
answer to question one and question to remember questions alternating sequence of actions and observations

1125
01:19:17,130 --> 01:19:18,410
so laid out

1126
01:19:18,420 --> 01:19:20,720
in this form in exhaustive

1127
01:19:20,730 --> 01:19:23,880
explicit conceptual form

1128
01:19:23,920 --> 01:19:28,150
all the answers to all questions about the dynamical system

1129
01:19:28,160 --> 01:19:31,280
it should be confused as hell of the point of this is the point you

1130
01:19:31,280 --> 01:19:34,420
ask me a question or two that i can try and address

1131
01:19:34,470 --> 01:19:36,950
so what i'm asserting is that

1132
01:19:36,970 --> 01:19:38,750
any dynamical system

1133
01:19:38,760 --> 01:19:45,460
will have answers to these questions or equivalently and answering this these filling in this

1134
01:19:46,470 --> 01:19:53,090
this conceptual vector specifies the dynamical system

1135
01:19:53,100 --> 01:19:57,200
right it's assigning a probability all possible futures

1136
01:19:57,210 --> 01:20:01,880
condition making actions

1137
01:20:01,900 --> 01:20:07,580
so this is a this is an exhaustive representation of

1138
01:20:07,590 --> 01:20:10,980
any discrete time every discrete time

1139
01:20:11,010 --> 01:20:15,120
finite action five observation system has this conceptual representations

1140
01:20:21,600 --> 01:20:26,720
any dynamical system there's no i'm not placing any constraint system other than finite observation

1141
01:20:26,760 --> 01:20:28,890
finite action discrete

1142
01:20:29,930 --> 01:20:31,990
any such system will have

1143
01:20:32,000 --> 01:20:38,380
and to these questions and i made an exhaustive listing here and it's a one-to-one

1144
01:20:38,380 --> 01:20:44,720
so the way the one-to-one mapping between these vectors and dynamical systems

1145
01:20:44,760 --> 01:20:46,830
why i am doing this is not going to be clear to a few more

1146
01:20:48,610 --> 01:20:51,720
OK so it's one of the make sure you understand what this vector is because

1147
01:20:51,720 --> 01:20:55,060
otherwise the next fifteen minutes will be meaningless

1148
01:20:56,000 --> 01:20:59,540
please ask me if this is not or actually whenever you feel confused about these

1149
01:20:59,540 --> 01:21:04,150
things so all possible futures they may be zero in here

1150
01:21:04,170 --> 01:21:07,780
lots of constraints on the entries i'd like to think that the sum to one

1151
01:21:07,790 --> 01:21:08,850
and so on

1152
01:21:08,850 --> 01:21:12,760
right this is a mathematical construct that IS the system not a model of the

1153
01:21:12,760 --> 01:21:15,610
system it is the system OK

1154
01:21:15,630 --> 01:21:20,560
or equivalently any exact model of the system should be able to produce or generate

1155
01:21:20,560 --> 01:21:21,790
the system dynamics

1156
01:21:22,800 --> 01:21:26,890
so what i'm going to do now is to show you a modelling this this

1157
01:21:28,070 --> 01:21:32,120
using a pretty representation of states which only one more if you would like to

1158
01:21:32,120 --> 01:21:36,140
develop now i'm going to make that into vector

1159
01:21:36,150 --> 01:21:38,610
in two infinity by affinity matrix

1160
01:21:38,790 --> 01:21:40,740
which goes like this

1161
01:21:40,760 --> 01:21:44,420
these are all possible futures

1162
01:21:44,520 --> 01:21:47,850
the answer is in the first row to the first row the that you already

1163
01:21:47,850 --> 01:21:52,250
seen in the vectors on the previous slide and the first row corresponds to the

1164
01:21:52,250 --> 01:21:58,100
number history or the initial conditions for all possible futures on the initial conditions

1165
01:21:58,110 --> 01:21:59,260
so what i'm gonna do

1166
01:21:59,260 --> 01:22:02,090
is expand this vector matrix by

1167
01:22:02,710 --> 01:22:06,050
making the rows all possible pasts

1168
01:22:06,070 --> 01:22:08,240
so future the columns

1169
01:22:08,240 --> 01:22:10,670
and past are the rose

1170
01:22:10,680 --> 01:22:12,970
or histories past history

1171
01:22:13,150 --> 01:22:18,220
what's the history history is just the trajectory of action observation that happened so far

1172
01:22:18,240 --> 01:22:23,450
right so they are both action observation sequences future and

1173
01:22:24,500 --> 01:22:27,930
OK now let me tell you to fill out this matrix

1174
01:22:27,930 --> 01:22:33,900
so here's the entry corresponding to the i future and the jet history

1175
01:22:33,950 --> 01:22:35,090
this is the

1176
01:22:35,120 --> 01:22:37,500
and so the following question what's the probability

1177
01:22:37,510 --> 01:22:40,710
of this future given this history

1178
01:22:40,720 --> 01:22:41,530
so let's

1179
01:22:42,690 --> 01:22:47,180
so the property of its future given the genetic history is probably conditional probability of

1180
01:22:47,180 --> 01:22:51,760
seeing the observation sequence in that future given that you take the action sequence in

1181
01:22:51,760 --> 01:22:56,480
that future from that history

1182
01:22:56,530 --> 01:23:00,460
notice that all the rows after the first or redundant in the sense that they

1183
01:23:00,460 --> 01:23:05,680
all computable from the first round by the first row is already a sufficient characterisation

1184
01:23:05,680 --> 01:23:09,610
of the dynamical system redundant information but i'm doing this for a very specific reason

1185
01:23:09,610 --> 01:23:14,690
that hopefully you'll see just two more slide thirty much like

1186
01:23:14,690 --> 01:23:16,750
OK so any dynamical system

1187
01:23:16,760 --> 01:23:20,830
and the corresponding system dynamics matrix

1188
01:23:20,830 --> 01:23:24,920
different features are different scales so if on

1189
01:23:24,980 --> 01:23:29,120
now if you take measures the people and so you know one one one feature

1190
01:23:29,120 --> 01:23:34,000
maybe the height another maybe the way to another maybe the strength another maybe

1191
01:23:34,020 --> 01:23:38,020
and how the age or whatever all of these you all these quantities are very

1192
01:23:38,020 --> 01:23:40,710
different scales and so on normally

1193
01:23:40,730 --> 01:23:42,270
norris the variance

1194
01:23:42,290 --> 01:23:47,370
sometimes if all the excise the called same type of thing on so for example

1195
01:23:47,370 --> 01:23:52,600
if you're taking your work with images and x ij is all the pixels of

1196
01:23:52,600 --> 01:23:57,750
different users are on the same scale because they're all pixel intensity values ranging from

1197
01:23:57,750 --> 01:24:00,830
zero to twenty five to get the way of this

1198
01:24:03,560 --> 01:24:05,770
so after preprocessing

1199
01:24:05,790 --> 01:24:12,210
let's talk about how we would find the main axes along which the data there

1200
01:24:13,290 --> 01:24:20,190
how we find the principal axes of

1201
01:24:20,330 --> 01:24:26,020
so to do that

1202
01:24:35,350 --> 01:24:42,120
let me just try to just go one specific example and we formalize the our

1203
01:24:50,750 --> 01:24:58,120
here's the training set comprising five examples and has roughly zero mean and variance on

1204
01:24:58,120 --> 01:25:04,690
the x one and x two axes of the same as the one

1205
01:25:04,710 --> 01:25:05,690
and so

1206
01:25:05,690 --> 01:25:10,210
it depends where axes the variation of the data is roughly the the positive forty

1207
01:25:10,210 --> 01:25:11,350
five it reacts

1208
01:25:11,370 --> 01:25:16,890
so like to do is have my out of can concluded this direction to draw

1209
01:25:16,920 --> 01:25:18,250
with arrow u

1210
01:25:18,270 --> 01:25:24,230
is this the best direction onto which the projected data so that the axis i

1211
01:25:24,230 --> 01:25:26,810
wish my data really varies

1212
01:25:27,830 --> 01:25:30,420
so let's think about how we formalise as well

1213
01:25:30,420 --> 01:25:35,670
one thing we want to look at is supposed to take axis

1214
01:25:35,710 --> 01:25:39,140
this is an easy access onto which i want to project on the quantock to

1215
01:25:39,210 --> 01:25:42,370
actually used to capture both the variational data

1216
01:25:42,420 --> 01:25:48,000
and which to training set and projected onto this line to get

1217
01:25:48,020 --> 01:25:51,170
that said points

1218
01:25:55,410 --> 01:25:59,810
you notice that those dots the projections of my training set on the this axis

1219
01:26:00,040 --> 01:26:02,270
has very large variance

1220
01:26:02,290 --> 01:26:06,620
in contrast if i were to choose on

1221
01:26:06,620 --> 01:26:09,850
a different direction let's this is really

1222
01:26:09,980 --> 01:26:15,020
falcon the worst direction onto west my data

1223
01:26:15,060 --> 01:26:18,940
if i project on my data trying to this access

1224
01:26:18,940 --> 01:26:31,870
right then i find that the projections of my data onto the

1225
01:26:31,890 --> 01:26:35,690
purple line and it is on the line has much smaller variance than my points

1226
01:26:35,690 --> 01:26:38,350
are clustered together much more tightly

1227
01:26:40,500 --> 01:26:43,560
one way to formalize this notion of finding the

1228
01:26:43,560 --> 01:26:49,170
that the main axis variations of data is a to find a vector you will

1229
01:26:49,170 --> 01:26:51,600
like to find a direction you

1230
01:26:51,620 --> 01:26:55,290
so that when i project my data into that direction

1231
01:26:56,390 --> 01:27:00,100
the projected points very as much as possible so in other words

1232
01:27:00,100 --> 01:27:04,920
find finer direction so the one project data onto it projections are

1233
01:27:04,940 --> 01:27:08,600
largely are widely spaced out lines

1234
01:27:11,070 --> 01:27:18,870
let's see

1235
01:27:34,210 --> 01:27:36,370
so i want to find the directions

1236
01:27:36,420 --> 01:27:43,480
and on this so reminder never have vector you often one

1237
01:27:43,500 --> 01:27:50,500
then the length of a vector x i

1238
01:27:50,520 --> 01:27:53,830
project well

1239
01:27:53,850 --> 01:28:00,020
the vector x i projected onto you

1240
01:28:00,120 --> 01:28:03,290
has length

1241
01:28:03,310 --> 01:28:09,670
of course transpose u is the

1242
01:28:09,670 --> 01:28:12,930
so the talk is some feature selection in machine learning

1243
01:28:12,950 --> 01:28:14,270
i give

1244
01:28:14,280 --> 01:28:19,130
basically overview of what's happening in there

1245
01:28:21,470 --> 01:28:23,480
and some reasons and so on so

1246
01:28:23,500 --> 01:28:28,390
why we do feature selection by dimensionality reduction in general

1247
01:28:28,400 --> 01:28:32,260
we can have different reasons we want to improve the performance

1248
01:28:32,310 --> 01:28:38,310
to improve the efficiency of the to provide faster predictors that request list information of

1249
01:28:38,310 --> 01:28:43,480
the original data which can really be important in some domains were obtaining features is

1250
01:28:44,800 --> 01:28:49,720
maybe medical demands so when you when you're getting new examples to test

1251
01:28:49,760 --> 01:28:54,530
you would maybe prefer to have less features than the complete set of features

1252
01:28:54,540 --> 01:28:59,710
maybe we want to do with dimensionality reduction in order to reduce reduce complexity

1253
01:28:59,760 --> 01:29:01,880
of the eleven results

1254
01:29:01,900 --> 01:29:06,610
enabling better understanding of what's happening behind there

1255
01:29:06,630 --> 01:29:08,240
OK so what are

1256
01:29:09,280 --> 01:29:11,210
the common approach is to

1257
01:29:11,220 --> 01:29:14,100
that means that reduction in machine learning what we do

1258
01:29:14,110 --> 01:29:17,390
we met the original feature space into some other

1259
01:29:17,410 --> 01:29:18,770
feature space

1260
01:29:18,850 --> 01:29:22,180
and how we get this other feature space in different ways

1261
01:29:22,230 --> 01:29:27,840
one way is to select simply select a subset of the original features

1262
01:29:27,860 --> 01:29:32,030
and this stock is name is talking about is just the beginning i give bigger

1263
01:29:34,490 --> 01:29:37,660
the other ways of course to construct new features

1264
01:29:37,700 --> 01:29:42,230
to replace the original features and we heard a lot about that yesterday

1265
01:29:42,310 --> 01:29:47,790
we can however also construct new features using some background knowledge

1266
01:29:47,840 --> 01:29:49,060
which can be

1267
01:29:50,130 --> 01:29:55,480
general background knowledge like no numeric data you can do some of the two features

1268
01:29:55,490 --> 01:30:00,060
you can do x or two features make apply some general functions

1269
01:30:00,110 --> 01:30:03,190
ignoring the domain of the problem

1270
01:30:03,200 --> 01:30:05,750
or you can use the background knowledge

1271
01:30:05,760 --> 01:30:08,020
which is connected to the domain

1272
01:30:08,030 --> 01:30:12,540
it can be possibly also provided by the user as a function you know saying

1273
01:30:12,540 --> 01:30:14,960
this is how the function works

1274
01:30:15,010 --> 01:30:19,570
or it can be domain specific for instance on text documents

1275
01:30:19,630 --> 01:30:23,260
what you can do is you know it's text you can use language parser talked

1276
01:30:23,290 --> 01:30:27,320
a noun phrases or some other kind of constructed features

1277
01:30:27,380 --> 01:30:30,090
or you can toward clustering

1278
01:30:30,140 --> 01:30:32,660
based on the data and then use clusters

1279
01:30:32,670 --> 01:30:33,970
as new features

1280
01:30:34,010 --> 01:30:36,580
and these new features constructed features

1281
01:30:36,600 --> 01:30:42,030
can be used either in this instead of the original features so just use that

1282
01:30:42,050 --> 01:30:47,800
or they can be used as additional features so you actually in larger feature set

1283
01:30:47,810 --> 01:30:48,960
and then of course

1284
01:30:48,980 --> 01:30:54,870
after that enlargement what you can do is apply feature subset selection averaging features generate

1285
01:30:54,870 --> 01:30:56,050
more features

1286
01:30:56,070 --> 01:30:58,000
then the feature subset selection

1287
01:30:58,020 --> 01:30:59,720
and hope

1288
01:31:00,670 --> 01:31:04,330
of the new features are going to get in this subset and helping

1289
01:31:04,350 --> 01:31:06,900
solving the problem

1290
01:31:06,920 --> 01:31:11,230
OK so a simple example

1291
01:31:11,240 --> 01:31:15,020
what we do with feature subset selection so the rest is about

1292
01:31:15,040 --> 01:31:16,960
feature subset selection

1293
01:31:17,000 --> 01:31:20,810
if we have a simple domain five will the features

1294
01:31:20,830 --> 01:31:22,280
and assume that

1295
01:31:22,290 --> 01:31:25,380
the classes correlated directly with the two

1296
01:31:25,390 --> 01:31:28,980
features is or f one or f two

1297
01:31:29,000 --> 01:31:30,540
and after three

1298
01:31:30,550 --> 01:31:33,280
feature trees negation of two

1299
01:31:33,290 --> 01:31:37,590
and the five thousand four also related by negation

1300
01:31:37,610 --> 01:31:42,210
so what would be the optimal feature subset if we do that

1301
01:31:42,260 --> 01:31:46,410
based on this knowledge which of course in real situations we don't have about the

1302
01:31:47,300 --> 01:31:48,170
we would say

1303
01:31:48,200 --> 01:31:51,060
optimal subsidies either f one f two

1304
01:31:51,070 --> 01:31:54,900
which completely describe the class or f one f three

1305
01:31:54,950 --> 01:31:57,240
since f threes negation of f two

1306
01:31:57,250 --> 01:32:01,510
so you can get an idea of the optimisation space for finding the feature subset

1307
01:32:01,510 --> 01:32:02,850
is actually

1308
01:32:02,870 --> 01:32:05,100
all the possible feature subsets

1309
01:32:05,300 --> 01:32:07,100
two IDF

1310
01:32:07,120 --> 01:32:12,930
and we have to search through the space is some clever way of course

1311
01:32:13,710 --> 01:32:17,860
this is an example if have more features our search space how it looks in

1312
01:32:17,870 --> 01:32:20,960
the movie between feature subsets when searching

1313
01:32:20,970 --> 01:32:25,830
and there are different approaches common approach is to search in the space

1314
01:32:25,840 --> 01:32:31,990
the most well-known two are either former called for selection start from an empty set

1315
01:32:31,990 --> 01:32:32,880
of features

1316
01:32:32,930 --> 01:32:34,000
and move

1317
01:32:34,040 --> 01:32:35,950
through the space

1318
01:32:35,960 --> 01:32:40,800
adding more features or backward elimination when you start with a complete set of features

1319
01:32:40,930 --> 01:32:45,050
then remove some features in hopefully smarter way

1320
01:32:45,090 --> 01:32:49,210
so for selection and there is the variant forward stepwise selection

1321
01:32:49,230 --> 01:32:53,810
well you move by adding or removing the feature one at the time so that

1322
01:32:53,810 --> 01:32:55,750
they are really going to miss

1323
01:32:55,800 --> 01:32:57,620
backward elimination

1324
01:32:57,630 --> 01:32:59,140
removing features

1325
01:32:59,150 --> 01:33:04,310
one at the time or you can also enable adding or removing at each step

1326
01:33:04,330 --> 01:33:06,650
and then random mutation that

1327
01:33:06,670 --> 01:33:11,060
you start with the random data set a random subset of feature actually from the

1328
01:33:11,060 --> 01:33:12,640
previous picture

1329
01:33:12,650 --> 01:33:13,490
and then

1330
01:33:13,500 --> 01:33:17,960
you make a random change or some other kind of adding removing features

1331
01:33:18,000 --> 01:33:21,780
and of course we move to the search space and evaluate

1332
01:33:21,830 --> 01:33:25,770
so that's the the big stories now how we about it

1333
01:33:25,780 --> 01:33:29,450
so there are again different approaches to evaluation

1334
01:33:29,460 --> 01:33:32,260
there is a

1335
01:33:32,280 --> 01:33:34,510
simple family of filters

1336
01:33:34,550 --> 01:33:36,800
use for feature subset selection

1337
01:33:36,810 --> 01:33:42,580
well developed asian function is independent of the learning algorithm that is going later to

1338
01:33:42,580 --> 01:33:45,090
use this feature subsets

1339
01:33:45,110 --> 01:33:47,670
there are so-called wrappers

1340
01:33:47,710 --> 01:33:49,330
thirty evaluations

1341
01:33:49,350 --> 01:33:54,780
the function of the feature subset is actually using the machine learning algorithm

1342
01:33:54,790 --> 01:33:55,850
that is

1343
01:33:55,860 --> 01:33:59,170
to evaluate the subset and and once we find

1344
01:33:59,190 --> 01:34:00,820
feature subset

1345
01:34:00,830 --> 01:34:04,650
it's going to be used by the same machine learning algorithm so it's kind of

1346
01:34:04,650 --> 01:34:08,550
taking machine learning group them into little wrapping around it

1347
01:34:08,560 --> 01:34:10,660
there are embedded approaches

1348
01:34:11,910 --> 01:34:16,920
have feature selection happening inside the learning algorithm during learning

1349
01:34:17,410 --> 01:34:21,280
and there is a variant of scene of lucas

1350
01:34:21,330 --> 01:34:23,640
which i called sympathy filters there's

1351
01:34:23,640 --> 01:34:26,580
the growth inhibitory factor

1352
01:34:26,650 --> 01:34:33,320
so an important growth inhibitory factor is for example tgfb

1353
01:34:33,340 --> 01:34:38,300
and teacher better works exactly the opposite

1354
01:34:38,970 --> 01:34:44,740
ptgs because it is the signal which is present extracellular space and tells the cell

1355
01:34:44,750 --> 01:34:46,870
it should stop proliferating

1356
01:34:47,110 --> 01:34:50,930
and therefore TGF beta if it's present in large amounts in this part of the

1357
01:34:50,930 --> 01:34:51,920
cell cycle

1358
01:34:51,960 --> 01:34:56,830
if the cell experiences in large amounts that will influence the cell not to move

1359
01:34:56,830 --> 01:35:03,870
through the restriction point conversely if it's absent then obviously ptgs can have the undiluted

1360
01:35:04,130 --> 01:35:05,590
tensions of the cell

1361
01:35:05,600 --> 01:35:08,790
and therefore what i'm trying to convey by this is to tell you that the

1362
01:35:08,790 --> 01:35:14,860
cell balances both its growth stimulate tree and growth inhibitory signals that is receiving from

1363
01:35:14,870 --> 01:35:16,620
extracellular space

1364
01:35:17,730 --> 01:35:20,530
decisions are weighed and finally down here

1365
01:35:20,540 --> 01:35:25,320
but still make the the binary decision to go ahead or not to go ahead

1366
01:35:25,470 --> 01:35:30,910
depending on historically how many of these factors it recruited in this specific window of

1367
01:35:32,570 --> 01:35:37,480
recall as we said last time that once a growth factor like pgf goes to

1368
01:35:37,480 --> 01:35:43,470
the plasma membrane it encounters a receptor on the surface

1369
01:35:44,340 --> 01:35:46,730
let's say we call the pgf receptor

1370
01:35:46,740 --> 01:35:49,890
and i'll just like this for the moment

1371
01:35:49,940 --> 01:35:54,800
it's a transmembrane proteins the extracellular domain is on the outside

1372
01:35:54,810 --> 01:35:58,390
and i'm trying to copy the EGF receptor here

1373
01:35:58,400 --> 01:36:01,830
for reasons that will become apparent in moment

1374
01:36:01,970 --> 01:36:08,030
and what happens is that ptgs which for example can be itself a dimer it

1375
01:36:08,080 --> 01:36:12,910
can be a dimeric growth factor so it has two distinct subunits in

1376
01:36:12,960 --> 01:36:17,040
they're both essentially equivalent to one another but it is time eric

1377
01:36:17,080 --> 01:36:25,910
and this dynamic structure PDF allows it to bind to growth factor receptors simultaneously

1378
01:36:25,930 --> 01:36:27,820
well why is that interesting

1379
01:36:27,870 --> 01:36:30,750
it's interesting for the following reasons

1380
01:36:31,990 --> 01:36:36,620
transmembrane pgf receptors like the ones i've indicated right here

1381
01:36:36,640 --> 01:36:41,790
there are anchored in the plasma membrane because there's a portion of their sequence writin

1382
01:36:41,790 --> 01:36:45,720
here in the transmembrane domain indicating here in the orange

1383
01:36:45,780 --> 01:36:47,580
which contains highly

1384
01:36:47,600 --> 01:36:49,690
hydrophobic amino acids

1385
01:36:49,700 --> 01:36:54,350
and those hydrophobic amino acids obviously love to be in this hydrophobic environment of the

1386
01:36:54,350 --> 01:36:58,610
lipid bilayer and they but they don't they have no effect at all on whether

1387
01:36:58,620 --> 01:37:04,370
pgf receptors can move can traverse in the plane of the plasma membrane so the

1388
01:37:04,370 --> 01:37:08,720
pgf receptors can move across the face of the plasma membrane these ones can move

1389
01:37:08,720 --> 01:37:11,370
to the right and left but they're not going to come in or out

1390
01:37:11,420 --> 01:37:16,540
because they anchored in this lipid bilayer by the structure of hydrophobic amino acids

1391
01:37:16,550 --> 01:37:19,620
now what happens interestingly

1392
01:37:19,630 --> 01:37:26,370
g the dimeric receptor binds to two of these pgf receptor molecules which as i

1393
01:37:26,370 --> 01:37:31,880
told you have lateral the freedom to translate lateral and clean the plasma membrane

1394
01:37:31,970 --> 01:37:37,230
what happens is that it will bind to these receptors and in so doing it

1395
01:37:37,230 --> 01:37:40,120
it will pull the two receptor molecules

1396
01:37:40,140 --> 01:37:44,820
next to one another previously they were just floating around the plane plasma membrane having

1397
01:37:44,820 --> 01:37:46,490
bound their ligand

1398
01:37:46,500 --> 01:37:50,980
recall that pdf is considered elegant for the pdf receptor

1399
01:37:51,000 --> 01:37:55,070
having it will cause these two

1400
01:37:55,090 --> 01:37:56,490
receptor molecules

1401
01:37:56,500 --> 01:37:57,660
two now becomes

1402
01:37:57,680 --> 01:38:02,600
the become very close to one another so read right now like this

1403
01:38:02,740 --> 01:38:09,000
now these two receptor molecules that look like this they're right the cheek-by-jowl

1404
01:38:09,680 --> 01:38:11,720
they're right next to one another

1405
01:38:11,810 --> 01:38:14,250
and this has an interesting consequences

1406
01:38:14,270 --> 01:38:19,790
of great interest here is the effect this has on the ability of the PD

1407
01:38:19,790 --> 01:38:25,380
jeff receptor to emit signals into the cytoplasm because we call that the endgame here

1408
01:38:25,390 --> 01:38:27,400
is always how to

1409
01:38:27,420 --> 01:38:31,700
how does the intracellular part of the cell know that this there's been an encounter

1410
01:38:31,700 --> 01:38:35,030
with the growth factors in the extracellular space

1411
01:38:35,040 --> 01:38:38,820
and this signal emitting power pdf receptor

1412
01:38:38,950 --> 01:38:41,490
comes from the fact that once

1413
01:38:41,540 --> 01:38:42,890
these two

1414
01:38:43,080 --> 01:38:45,310
domains are brought together

1415
01:38:45,350 --> 01:38:50,940
each of these two domains is able to modify the other and change the other

1416
01:38:50,960 --> 01:38:52,110
i e

1417
01:38:52,120 --> 01:38:54,620
so a modified subunit b

1418
01:38:54,650 --> 01:38:58,960
subunit b modify subunit in how is this modification achieves

1419
01:38:58,970 --> 01:39:02,890
for the for it is achieved in the following way

1420
01:39:03,720 --> 01:39:08,160
the this domain which i've been calling i've just writing like this is a rectangle

1421
01:39:08,210 --> 01:39:14,170
is actually a catalytic agent it's actually a tire seen

1422
01:39:14,220 --> 01:39:17,350
kind of certain enzymes

1423
01:39:17,400 --> 01:39:23,370
and the terracing kind is is an enzyme that takes the gamma phosphate from ATP

1424
01:39:23,370 --> 01:39:26,840
stress syndrome and if you talk to these guys when the lights go off they

1425
01:39:26,840 --> 01:39:28,670
think of you know i'm now

1426
01:39:28,700 --> 01:39:32,890
vulnerable back on my guard post and i don't know what's going to happen and

1427
01:39:32,910 --> 01:39:37,990
is much larger and people post-traumatic stress syndrome so he was the phenomenon that independent

1428
01:39:37,990 --> 01:39:42,220
condition the light was on for a very long time and the questioners will be

1429
01:39:42,220 --> 01:39:44,850
dependent on the image below the bed nucleus

1430
01:39:44,880 --> 01:39:49,540
the punchline is the for the first time we found the bed nucleus was critical

1431
01:39:49,540 --> 01:39:51,430
for this for now

1432
01:39:51,460 --> 01:39:55,300
so this is the most complicated slide you can see

1433
01:39:55,380 --> 01:39:59,080
there are three groups of animals they have little can so little tubes to allow

1434
01:39:59,080 --> 01:40:02,100
us to infuse drugs directly into the brain

1435
01:40:02,120 --> 01:40:06,000
and now we're looking at the magnitude of started going from the dark to the

1436
01:40:06,000 --> 01:40:11,030
light so that's the magnitude of light lighthouse startled the yellow curves are the control

1437
01:40:11,030 --> 01:40:15,180
condition and so the different groups of animals that have cameos in either the bayes

1438
01:40:15,220 --> 01:40:20,330
lateral amygdala projects to both the bed nucleus and the central the central nucleus or

1439
01:40:20,330 --> 01:40:21,690
the bed nucleus

1440
01:40:21,720 --> 01:40:25,400
and what we found is that in activation in the basal lateral block this light

1441
01:40:25,400 --> 01:40:26,850
enhanced are

1442
01:40:26,860 --> 01:40:32,330
to our amazement inactivation of the central nucleus had no effect and more to our

1443
01:40:32,330 --> 01:40:37,580
amazement when we activated the bed nucleus it totally blocked this effect

1444
01:40:37,600 --> 01:40:41,050
so we wondered well maybe they can also just in the wrong place so we

1445
01:40:41,050 --> 01:40:45,120
actually took the same animals and now we took a short light three second light

1446
01:40:45,410 --> 01:40:47,230
and we repair that with the shock

1447
01:40:47,250 --> 01:40:49,940
and we asked what happens when we activate them

1448
01:40:49,960 --> 01:40:54,500
and here is now fear potentiated startle that is the increase in starling the presence

1449
01:40:54,500 --> 01:40:59,100
versus the absence of the light that previously been paired with a shock to we

1450
01:40:59,100 --> 01:41:04,200
activate the bayes ll we block that like lesion work only activate the central nucleus

1451
01:41:04,450 --> 01:41:09,060
we block that but like deletion work and inactivated the bed nucleus we didn't so

1452
01:41:09,060 --> 01:41:15,210
we get this double dissociation between inactivation the central nucleus walking fear but not this

1453
01:41:15,210 --> 01:41:17,060
longer duration effect

1454
01:41:17,060 --> 01:41:20,470
and vice versa for the bed nucleus of the stretch miles

1455
01:41:20,500 --> 01:41:23,630
so this fact number one

1456
01:41:23,630 --> 01:41:25,280
fact number also

1457
01:41:25,290 --> 01:41:27,130
in this sort of schema than

1458
01:41:27,150 --> 01:41:31,160
we don't understand it at least at that time but the difference here

1459
01:41:31,200 --> 01:41:35,560
is we have very explicit q three second like here to diffuse q it's too

1460
01:41:35,690 --> 01:41:41,530
late some twenty minutes its long-term versus short-term it's on condition worsens condition and this

1461
01:41:41,530 --> 01:41:47,700
is sort of an inherently less predictable compared to highly predictable situations

1462
01:41:47,940 --> 01:41:54,300
well it turns out that there's a peptide called corticotropin releasing hormone that course is

1463
01:41:54,300 --> 01:42:00,000
known for activity in the hypothalamus pituitary axis but is also found in very high

1464
01:42:00,000 --> 01:42:04,020
concentrations in the amygdala the bed nucleus of the street remarks

1465
01:42:04,030 --> 01:42:10,270
and importantly if you confuse this directly into the brain it produces a whole constellation

1466
01:42:10,270 --> 01:42:15,040
of behaviors that look very much like fear anxiety so there's an increase in plasma

1467
01:42:15,040 --> 01:42:19,590
norepinephrine happen after an increase in heart rate blood pressure and respiration

1468
01:42:19,650 --> 01:42:24,430
they're angiogenic effects anxiety like effects in these animal tests

1469
01:42:24,490 --> 01:42:29,890
there's a decrease in feeding and sexual behavior pain sensitivity there's a big increase in

1470
01:42:29,890 --> 01:42:36,780
startle and other anxiety like effects so the question is is where CRF acting

1471
01:42:36,780 --> 01:42:41,120
so is a really important question because you have a single peptide that can produce

1472
01:42:41,120 --> 01:42:45,310
this whole pattern behaviors and the question is where active

1473
01:42:45,330 --> 01:42:48,890
and the punchline once again is attacked in the bed nucleus of the street from

1474
01:42:48,900 --> 01:42:54,600
mouse anatomy amygdala or least the way we've been doing these experiments technical point that

1475
01:42:54,630 --> 01:42:59,160
get into later on just one piece of evidence for groups of animals here

1476
01:42:59,670 --> 01:43:03,800
we implant them with the cannula to allow us to directly put this chemical into

1477
01:43:03,800 --> 01:43:04,800
the brain

1478
01:43:04,820 --> 01:43:07,960
and that we give them a pre-test so this is just start all amplitude there

1479
01:43:07,960 --> 01:43:09,400
now for example

1480
01:43:09,410 --> 01:43:11,330
atomic propositions

1481
01:43:11,340 --> 01:43:13,450
it is hard to do a or

1482
01:43:13,460 --> 01:43:15,260
it's going to rain tomorrow

1483
01:43:15,290 --> 01:43:19,250
and x less than zero x equals you and so on

1484
01:43:19,460 --> 01:43:22,710
now in propositional logic we

1485
01:43:22,750 --> 01:43:28,080
i don't really look at the particle individuals in state like it for example

1486
01:43:28,140 --> 01:43:29,970
so we not

1487
01:43:30,020 --> 01:43:31,920
formalise x

1488
01:43:31,960 --> 01:43:33,000
this is

1489
01:43:33,040 --> 01:43:34,340
we just take

1490
01:43:34,400 --> 01:43:35,710
the statement x

1491
01:43:35,720 --> 01:43:37,630
equal zero as

1492
01:43:37,640 --> 01:43:41,060
atomic state which we can analyse

1493
01:43:41,660 --> 01:43:45,340
that's why we're going to abstract abstract is

1494
01:43:45,380 --> 01:43:47,600
atomic propositions yes you

1495
01:43:47,650 --> 01:43:49,250
well that

1496
01:43:49,260 --> 01:43:53,400
so when i say property propositional variables i mean

1497
01:43:53,540 --> 01:43:58,600
it is an abstract and of an atomic propositional

1498
01:43:58,610 --> 01:44:01,820
now we what we really case just how

1499
01:44:03,230 --> 01:44:10,320
propositional composing how we calculate the truth value of the compass

1500
01:44:11,150 --> 01:44:14,140
basically problem logic is just starting

1501
01:44:14,160 --> 01:44:15,770
composite truth

1502
01:44:15,830 --> 01:44:17,960
i understand the logical

1503
01:44:18,010 --> 01:44:20,410
connect you have to standard

1504
01:44:20,420 --> 01:44:23,530
the called and for y and so on

1505
01:44:25,560 --> 01:44:27,390
and then

1506
01:44:27,450 --> 01:44:29,340
one of the important problems in

1507
01:44:29,350 --> 01:44:33,100
propositional logic is the so-called satisfiability problem

1508
01:44:33,210 --> 01:44:36,470
so if i gave you a complex formula

1509
01:44:37,300 --> 01:44:42,790
assign truth values to the atomic propositions are true

1510
01:44:42,920 --> 01:44:47,460
they composed an industry

1511
01:44:48,960 --> 01:44:50,450
so this is the

1512
01:44:50,460 --> 01:44:52,460
the language or

1513
01:44:52,470 --> 01:44:57,410
usually called the syntax of propositional logic

1514
01:44:57,460 --> 01:45:01,700
now we defined inductively

1515
01:45:01,830 --> 01:45:05,320
the class of all propositional

1516
01:45:05,360 --> 01:45:07,910
so we've seen that proposition five

1517
01:45:07,960 --> 01:45:10,660
this is an abstract in tonic statements

1518
01:45:10,700 --> 01:45:12,890
from that

1519
01:45:12,990 --> 01:45:17,660
and i will introduce

1520
01:45:18,340 --> 01:45:21,640
logical connectives

1521
01:45:21,710 --> 01:45:24,130
actually six come true and false

1522
01:45:24,160 --> 01:45:28,610
so there is another e connected true and false

1523
01:45:28,670 --> 01:45:32,870
the traditional symbol is as you you can see that it

1524
01:45:32,890 --> 01:45:37,940
shows symbolises called top part is called but

1525
01:45:37,950 --> 01:45:40,010
and and then you have this

1526
01:45:40,030 --> 01:45:42,060
wait symbol

1527
01:45:42,100 --> 01:45:43,870
three percent

1528
01:45:43,920 --> 01:45:45,710
the conjunction

1529
01:45:45,720 --> 01:45:49,940
and this is the symbol represents you know

1530
01:45:50,460 --> 01:45:53,480
its meaning is obvious in the

1531
01:45:53,570 --> 01:45:55,470
and then you have this arrow

1532
01:45:56,850 --> 01:45:59,080
three percent implication

1533
01:45:59,090 --> 01:46:02,250
and then the formula is just composed by

1534
01:46:02,730 --> 01:46:06,340
propositional variables and connected

1535
01:46:06,580 --> 01:46:09,290
on the context

1536
01:46:09,300 --> 01:46:10,960
so you are familiar with

1537
01:46:10,980 --> 01:46:14,430
he met notation of the

1538
01:46:14,460 --> 01:46:16,210
you probably learned

1539
01:46:16,250 --> 01:46:17,910
automata theory

1540
01:46:17,950 --> 01:46:19,920
i can just right

1541
01:46:20,500 --> 01:46:24,130
the inductive definition is one line which says that

1542
01:46:24,160 --> 01:46:29,910
after is the classifier that is the probability that two of negation

1543
01:46:29,950 --> 01:46:31,160
and or

1544
01:46:40,170 --> 01:46:44,690
legends always struggle with this to the concept of syntactic semantic

1545
01:46:44,730 --> 01:46:45,870
the syntactic

1546
01:46:45,880 --> 01:46:50,050
describe the form of state this is how write down

1547
01:46:50,090 --> 01:46:52,220
semantics describe to me

1548
01:46:55,100 --> 01:46:57,580
let's say you are interested in

1549
01:46:58,940 --> 01:47:02,000
know what can infer from the form of the state

1550
01:47:02,060 --> 01:47:06,190
so is valid in all possible patients

1551
01:47:06,480 --> 01:47:09,430
for example if i write

1552
01:47:12,960 --> 01:47:16,070
first two miles three

1553
01:47:16,080 --> 01:47:19,830
you probably understand immediately what is

1554
01:47:19,850 --> 01:47:24,580
so error

1555
01:47:32,190 --> 01:47:37,590
for most of you it's probably very obvious that describes the operation

1556
01:47:38,430 --> 01:47:40,430
there's really no

1557
01:47:40,470 --> 01:47:44,960
rule universal law that says you have to write this statement is

1558
01:47:44,970 --> 01:47:55,230
again for the same

1559
01:47:55,280 --> 01:47:58,000
represent a number

1560
01:47:58,010 --> 01:47:59,140
number of

1561
01:47:59,230 --> 01:48:01,380
alphabet a

1562
01:48:01,480 --> 01:48:03,570
one is free

1563
01:48:03,660 --> 01:48:06,290
in one and two is a two

1564
01:48:07,110 --> 01:48:07,840
so what

1565
01:48:08,830 --> 01:48:13,370
ladies tend to get syntax is very

1566
01:48:13,430 --> 01:48:16,670
relax we saw

1567
01:48:16,710 --> 01:48:19,990
particular symbols doesn't have fix

1568
01:48:20,050 --> 01:48:22,480
well apart from standard

1569
01:48:22,560 --> 01:48:23,680
thank you

1570
01:48:23,690 --> 01:48:25,600
unlike his investment

1571
01:48:26,390 --> 01:48:27,530
he died

1572
01:48:27,570 --> 01:48:30,710
right away i

1573
01:48:30,760 --> 01:48:32,700
so we'll see this

1574
01:48:33,240 --> 01:48:36,050
this is

1575
01:48:36,190 --> 01:48:37,940
two concepts

1576
01:48:37,970 --> 01:48:42,590
very often so soundness we're interested

1577
01:48:43,130 --> 01:48:48,580
this two questions cause soundness completeness sound just means that

1578
01:48:48,690 --> 01:48:53,010
you take the form of logical statements you do some operations at

1579
01:48:53,070 --> 01:48:54,240
you get another

1580
01:48:54,260 --> 01:48:56,740
logical state

1581
01:48:56,800 --> 01:48:59,410
for example i can confirm this

1582
01:48:59,450 --> 01:49:00,660
equation two

1583
01:49:00,710 --> 01:49:02,890
things like three three

1584
01:49:02,910 --> 01:49:04,200
by simply

1585
01:49:04,210 --> 01:49:05,560
this expression

1586
01:49:05,570 --> 01:49:11,010
how does this operation preserves the meaning that we suddenly assigned numbers

1587
01:49:11,070 --> 01:49:13,470
this is a sound

1588
01:49:14,310 --> 01:49:16,110
on the completeness is

1589
01:49:22,610 --> 01:49:26,030
semantically valid logical statements

1590
01:49:26,080 --> 01:49:30,810
the proof by just purely syntactic transformation this is

1591
01:49:30,820 --> 01:49:34,080
really at the heart of automated reasoning

1592
01:49:35,180 --> 01:49:38,730
found computer science

1593
01:49:38,850 --> 01:49:41,600
as i said earlier

1594
01:49:41,640 --> 01:49:43,630
we can assign for that two

1595
01:49:43,690 --> 01:49:46,540
complex formed by breaking down

1596
01:49:46,550 --> 01:49:49,310
the truth value of simple form

1597
01:49:49,450 --> 01:49:52,960
assuming that we know the truth value of atomic

1598
01:49:52,960 --> 01:49:58,060
rules so to compute the probability all the time

1599
01:49:58,060 --> 01:50:00,060
the marginal probability this

1600
01:50:00,570 --> 01:50:05,670
clock here are probably next so these quantities here

1601
01:50:07,510 --> 01:50:08,590
more generally

1602
01:50:08,590 --> 01:50:10,390
this is the

1603
01:50:10,410 --> 01:50:16,410
this calculation in which for the values of variables given that the prior is called

1604
01:50:16,410 --> 01:50:17,840
inference and when

1605
01:50:18,170 --> 01:50:20,980
many many variables inference can be called

1606
01:50:21,130 --> 01:50:27,410
which in itself requires an entire album to this misleading here is really just simple

1607
01:50:27,420 --> 01:50:35,640
o ratio using phrases just great britain but there's many many many things you're referring

1608
01:50:35,690 --> 01:50:43,360
the process of writing down is serious itself requires replication

1609
01:50:43,380 --> 01:50:44,280
so when we

1610
01:50:44,320 --> 01:50:45,480
talk about

1611
01:50:47,680 --> 01:50:52,770
in these cases we can solve it exactly

1612
01:50:54,420 --> 01:50:56,520
so let's go through the circle

1613
01:50:56,600 --> 01:51:02,650
example of gas mixtures as for piece of cake going to make specific

1614
01:51:02,720 --> 01:51:11,030
choices of gaussians and then you can see the responsibilities involved evaluating the density of

1615
01:51:11,030 --> 01:51:18,000
the point x under each models and compute the ratio of how likely is it

1616
01:51:18,920 --> 01:51:21,760
multi by how

1617
01:51:22,820 --> 01:51:31,070
so i think this model the mixture of gaussians as two separate things work

1618
01:51:31,090 --> 01:51:33,470
if you given data point

1619
01:51:33,500 --> 01:51:36,710
any way this marginal probability

1620
01:51:36,770 --> 01:51:38,820
don't want make sure that

1621
01:51:38,840 --> 01:51:40,740
i think is not going to

1622
01:51:41,980 --> 01:51:43,250
so here

1623
01:51:43,270 --> 01:51:47,310
in this picture i put it up here and knowledge is good

1624
01:51:47,310 --> 01:51:50,920
one of the best in sample data availability

1625
01:51:50,940 --> 01:51:54,650
here's what about minutes not

1626
01:51:54,720 --> 01:51:59,830
but you can also have a mixture of gaussians as a machine that converts data

1627
01:51:59,830 --> 01:52:02,350
points each point on the simplex

1628
01:52:02,360 --> 01:52:03,810
it data point two

1629
01:52:03,830 --> 01:52:10,100
distribution over clusters and in that case the posterior distribution is sort of like clustering

1630
01:52:11,510 --> 01:52:12,780
so the

1631
01:52:12,790 --> 01:52:14,150
i really like your

1632
01:52:15,520 --> 01:52:18,750
make sure that the engine converts that

1633
01:52:18,760 --> 01:52:21,420
four distribution which says nine on this

1634
01:52:22,570 --> 01:52:24,940
zero five boston to

1635
01:52:24,950 --> 01:52:26,100
his first

1636
01:52:27,340 --> 01:52:32,630
these models can be used in many ways one is to evaluate marginal probability that

1637
01:52:32,680 --> 01:52:36,450
in some sense often what's interesting about reading

1638
01:52:36,480 --> 01:52:43,490
this is serious problem later so the traditional view UN statistics is very important nuisance

1639
01:52:44,570 --> 01:52:48,120
and the only reason why it should be something that

1640
01:52:48,130 --> 01:52:50,620
in order to compute the relative margin that

1641
01:52:50,640 --> 01:52:53,430
but sometimes you actually care about variables

1642
01:52:53,460 --> 01:52:55,830
you want to do something like clusters

1643
01:52:55,840 --> 01:53:01,180
or for a copy like those so don't do

1644
01:53:01,180 --> 01:53:06,090
variables just irritating things is always want really should think about

1645
01:53:06,420 --> 01:53:09,740
model introduced intentionally because we

1646
01:53:09,760 --> 01:53:10,840
they like

1647
01:53:10,900 --> 01:53:16,500
and to assure that we will be true and where i some that cases really

1648
01:53:16,560 --> 01:53:17,850
want more

1649
01:53:18,750 --> 01:53:26,310
three cases reviewed university itself before so you should be looking for this is like

1650
01:53:26,310 --> 01:53:31,250
eating vegetables sometimes in the seventies

1651
01:53:31,440 --> 01:53:37,330
OK so just to continue in this example we can extend this to mixture of

1652
01:53:37,380 --> 01:53:39,400
linear regression experts

1653
01:53:39,400 --> 01:53:41,440
so here we have given y

1654
01:53:41,460 --> 01:53:43,190
we have an input x

1655
01:53:43,210 --> 01:53:50,660
n conditioned on x y has the distribution mixture of linear forms

1656
01:53:50,740 --> 01:53:51,870
you basically

1657
01:53:51,870 --> 01:53:55,970
in these mixing proportion of actually depends on x

1658
01:53:56,000 --> 01:53:58,840
so that means that which

1659
01:53:58,840 --> 01:54:02,670
component case select is a function of

1660
01:54:02,680 --> 01:54:05,250
the input variables x

1661
01:54:05,260 --> 01:54:07,310
you can think of this as

1662
01:54:08,160 --> 01:54:10,420
imagine that

1663
01:54:10,640 --> 01:54:12,250
a teacher for

1664
01:54:12,270 --> 01:54:16,160
and then the principal of the school in two thousand features

1665
01:54:17,270 --> 01:54:18,380
the teacher

1666
01:54:18,380 --> 01:54:21,820
i want to make it appear like classifiers

1667
01:54:21,830 --> 01:54:26,900
so what does it do well when principle as question the feature space

1668
01:54:26,920 --> 01:54:28,470
i with my students

1669
01:54:28,490 --> 01:54:30,840
you know the answer to that question

1670
01:54:30,840 --> 01:54:34,900
teachers college assume that they know the answer in this case audience

1671
01:54:35,010 --> 01:54:39,180
right so the teacher is playing the role of the scaling function l

1672
01:54:39,190 --> 01:54:44,150
the future works question which is input x and make a decision about the distribution

1673
01:54:44,150 --> 01:54:46,190
over students k

1674
01:54:46,210 --> 01:54:49,180
we're going to be able to get the right answer and

1675
01:54:49,190 --> 01:54:50,260
you one

1676
01:54:50,270 --> 01:54:54,860
and the reason that so cleverly select students

1677
01:54:54,870 --> 01:54:57,110
and here's the classes

1678
01:54:57,130 --> 01:54:57,810
the is

1679
01:54:57,840 --> 01:55:00,560
you know of anyone in

1680
01:55:00,580 --> 01:55:06,560
question think this is gating function is designed to select the expert that is best

1681
01:55:06,560 --> 01:55:08,470
suited for that

1682
01:55:08,500 --> 01:55:10,860
the scaling function

1683
01:55:10,900 --> 01:55:16,840
many many forms the simplest form is to just put in year just rational softmax

1684
01:55:16,840 --> 01:55:18,610
regression with

1685
01:55:18,630 --> 01:55:20,350
so the conditional

1686
01:55:20,490 --> 01:55:22,040
distribution is

1687
01:55:22,100 --> 01:55:24,100
you what the next mixture

1688
01:55:24,140 --> 01:55:25,910
and this is

1689
01:55:25,920 --> 01:55:28,160
this is of the k two

1690
01:55:28,170 --> 01:55:32,670
probability of getting closer to a given the input x which small has a log

1691
01:55:32,670 --> 01:55:34,240
linear on

1692
01:55:34,260 --> 01:55:35,630
so that's exactly

1693
01:55:35,720 --> 01:55:39,670
here this is just the start lines show

1694
01:55:40,700 --> 01:55:47,920
your probabilities of selecting model zero one max and in the one-dimensional case because it

1695
01:55:49,130 --> 01:55:50,150
this is what

1696
01:55:50,160 --> 01:55:52,220
for any given

1697
01:55:52,220 --> 01:55:54,640
so that you should really be using that

1698
01:55:55,600 --> 01:55:59,060
try and apply the machine learning for instead of using a box we should be

1699
01:55:59,060 --> 01:56:04,110
trying to get more information should be getting better registration and hopefully

1700
01:56:04,120 --> 01:56:06,970
a lot of stuff to talk about was for you guys to have a look

1701
01:56:06,970 --> 01:56:08,090
at that

1702
01:56:08,090 --> 01:56:09,480
so i'm moving on

1703
01:56:09,500 --> 01:56:13,020
another thing that really difficult is identical

1704
01:56:13,020 --> 01:56:17,280
so that in this is the real real top one it's called

1705
01:56:17,300 --> 01:56:19,380
we refer to this as the

1706
01:56:19,430 --> 01:56:23,800
generic one problem and essentially it's

1707
01:56:23,830 --> 01:56:26,660
it's very very difficult at the moment

1708
01:56:27,860 --> 01:56:32,800
i will discuss footfall which topped things when we just wanted all boxes that what

1709
01:56:32,810 --> 01:56:36,250
you can boxer interface you know pretty good job you can come to get good

1710
01:56:36,250 --> 01:56:41,650
job generically across crosses places like this guy alexander downer x

1711
01:56:41,680 --> 01:56:46,620
foreign minister yes and

1712
01:56:46,670 --> 01:56:50,640
and madeleine albright and then are more

1713
01:56:50,660 --> 01:56:58,100
but anyway so these are collected generic places and things and we can do a

1714
01:56:58,100 --> 01:57:02,430
good job sometimes box detectors but if we have something that's benson so something that

1715
01:57:02,430 --> 01:57:08,070
we can exhaustively search of already introduced this term it becomes a lot harder and

1716
01:57:08,070 --> 01:57:11,640
we've been actually a lot of research in this slightly and we've got all kind

1717
01:57:11,640 --> 01:57:15,110
of some really exciting results we can actually do a lot better now

1718
01:57:19,030 --> 01:57:25,010
city in from for the linear perspective there are two problems in registration

1719
01:57:25,030 --> 01:57:27,580
the first problem is essentially the length

1720
01:57:27,590 --> 01:57:32,050
so how do i learn an object model that satisfactorily discriminate between

1721
01:57:32,070 --> 01:57:36,330
the object and the image background so that's that's what this problem and of course

1722
01:57:36,330 --> 01:57:39,320
i mean you guys have been sitting through all these courses and a lot of

1723
01:57:39,320 --> 01:57:43,360
really interesting information you has been gathering as well

1724
01:57:43,970 --> 01:57:47,310
kind idea how to do that so far not to do that

1725
01:57:47,320 --> 01:57:53,020
why are you here and sometimes asked myself the same question but the with the

1726
01:57:53,060 --> 01:57:59,320
with the three things apart whatever this is an important part of how to effectively

1727
01:57:59,630 --> 01:58:03,560
evaluate the compliance across all possible warp

1728
01:58:03,620 --> 01:58:05,600
and as you come to go through

1729
01:58:05,610 --> 01:58:09,220
this question and this question is inherently tied together

1730
01:58:09,250 --> 01:58:11,510
so it seems to make some assumptions here

1731
01:58:11,560 --> 01:58:14,980
you constrain yourself here seems to make some assumptions here

1732
01:58:15,010 --> 01:58:17,150
we constrain yourself here so

1733
01:58:17,170 --> 01:58:19,420
being ignorant of one

1734
01:58:19,440 --> 01:58:20,670
or the other

1735
01:58:20,690 --> 01:58:26,560
severely limits you also severely limits research into and the idea is afford so

1736
01:58:26,570 --> 01:58:30,210
if anything else you think about registration that's what i want you to go because

1737
01:58:30,210 --> 01:58:35,020
the code come to go home with the basically winning everything are inherently hard and

1738
01:58:35,020 --> 01:58:41,940
i can't separate the two are going to consider both questions at the same time

1739
01:58:42,790 --> 01:58:46,860
we will come to cover the landing landing kind of going to go the feeding

1740
01:58:46,860 --> 01:58:49,850
because i think you guys have a lot of learning already will go back to

1741
01:58:49,850 --> 01:58:53,190
the learning is as i go through

1742
01:58:55,660 --> 01:58:58,110
in the end

1743
01:58:58,200 --> 01:58:59,090
you will

1744
01:58:59,090 --> 01:59:05,410
to that we will actually get challenge one was so happy we deal with all

1745
01:59:05,410 --> 01:59:13,220
the variations on an object can undergo a set lighting pose clusions and even now

1746
01:59:13,220 --> 01:59:18,220
i've been talking about pose lighting things this so all of the time in vs

1747
01:59:18,220 --> 01:59:21,800
video sequences people and not just going to talk places so

1748
01:59:21,810 --> 01:59:26,000
faces are coming to my heart but cushions happen all the time and other objects

1749
01:59:26,000 --> 01:59:28,510
like if you come to my talk on the whole

1750
01:59:28,530 --> 01:59:33,480
if you got someone walking in the walking behind the house things get it all

1751
01:59:33,500 --> 01:59:38,030
the time or for everyone knows my face gets included nuisance variables so things like

1752
01:59:38,030 --> 01:59:44,040
care like we're actually putting a a demo together for a face tracking system in

1753
01:59:44,040 --> 01:59:45,960
the just for japanese car

1754
01:59:46,010 --> 01:59:49,640
and it's it's weird because we were training at all these places of often online

1755
01:59:49,640 --> 01:59:50,900
database that we have

1756
01:59:50,920 --> 01:59:54,370
a different kind of his come back it's kind of like to talk about it

1757
01:59:54,370 --> 01:59:58,950
seems like the current histone japan is going to have people's here over the topic

1758
01:59:58,950 --> 02:00:03,700
and its playing havoc with the system because all these people what a like people

1759
02:00:03,700 --> 02:00:08,360
have this this coming over here you really have to kind of know how to

1760
02:00:08,360 --> 02:00:14,340
deal with this and have a good time to do this so but it's often

1761
02:00:14,340 --> 02:00:18,010
difficult to define the nuisance variables two so you have to have a good way

1762
02:00:18,010 --> 02:00:22,950
of generalizing things and also place of

1763
02:00:22,970 --> 02:00:27,940
if you have any questions if you guys something is unclear or from that this

1764
02:00:27,940 --> 02:00:30,070
makes sense to stop me

1765
02:00:30,070 --> 02:00:31,540
ask me

1766
02:00:31,550 --> 02:00:33,120
the more

1767
02:00:33,170 --> 02:00:37,340
and the challenge to use the fitting

1768
02:00:37,340 --> 02:00:40,690
i'm just going to show an example here so if you have a six forty

1769
02:00:40,690 --> 02:00:45,030
four eighty image and this so i'm just searching my what function this translation and

1770
02:00:46,010 --> 02:00:51,560
and i'm searching say different scales so far different scales here

1771
02:00:51,780 --> 02:00:55,120
one twenty four twenty four thirty six forty eight sixty seventy two

1772
02:00:55,140 --> 02:01:00,820
and sliding every pixel synonymous that every pixel i still just for single image need

1773
02:01:00,820 --> 02:01:04,170
to classify twenty four thousand subwindows to classify

