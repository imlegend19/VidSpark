1
00:00:00,000 --> 00:00:05,570
that measures how far apart these two lists are

2
00:00:07,020 --> 00:00:08,670
and the idea is that if

3
00:00:09,250 --> 00:00:12,830
so let's let's define that then we'll take a look at it so we can

4
00:00:14,080 --> 00:00:21,080
potential function

5
00:00:29,370 --> 00:00:32,630
mapping the set of

6
00:00:32,650 --> 00:00:37,740
MTF's list in the real numbers

7
00:00:41,600 --> 00:00:44,570
feel i

8
00:00:44,580 --> 00:00:46,570
it's going twice

9
00:00:46,690 --> 00:00:51,850
the cardinality of the set

10
00:01:11,700 --> 00:01:13,200
so this is a

11
00:01:13,240 --> 00:01:17,140
this is the preceeds operation in list i i

12
00:01:17,280 --> 00:01:20,490
we can define a relationship between any two elements

13
00:01:20,530 --> 00:01:21,720
it says

14
00:01:21,780 --> 00:01:23,850
x precedes

15
00:01:23,860 --> 00:01:24,840
l by

16
00:01:24,850 --> 00:01:27,360
x precedes y YNL also by

17
00:01:28,060 --> 00:01:31,710
as an actor singer from head i hit expert first

18
00:01:31,920 --> 00:01:34,960
so what i'm interested in here are

19
00:01:35,010 --> 00:01:38,380
in some sense the disagreements between the two lists

20
00:01:38,400 --> 00:01:40,340
this is where

21
00:01:40,340 --> 00:01:42,180
x proceeds y

22
00:01:42,200 --> 00:01:47,280
in MTF's list why perceived x in obsolete

23
00:01:47,280 --> 00:01:50,190
they disagree

24
00:01:51,030 --> 00:01:54,450
and what we're interested in is the cardinality of the set

25
00:01:54,470 --> 00:01:57,060
multiply by two

26
00:01:58,150 --> 00:02:01,820
so that equal to two times

27
00:02:01,830 --> 00:02:05,580
so there's a name for this type of thing we saw that when we're doing

28
00:02:05,600 --> 00:02:08,020
sorting but remember the name

29
00:02:08,040 --> 00:02:12,660
briefly very briefly i expect anybody remembers somebody might

30
00:02:12,720 --> 00:02:14,380
conversions goode

31
00:02:14,480 --> 00:02:18,240
twice the number of inversions

32
00:02:18,240 --> 00:02:22,650
it was just do an example

33
00:02:22,760 --> 00:02:28,620
so let's say also by is the list

34
00:02:31,370 --> 00:02:36,550
OK let's see it

35
00:02:36,600 --> 00:02:38,190
i used

36
00:02:38,220 --> 00:02:40,350
characters for us

37
00:02:40,410 --> 00:02:41,430
the order

38
00:02:41,450 --> 00:02:44,750
just to keep things simple

39
00:03:05,030 --> 00:03:06,150
in this case

40
00:03:06,170 --> 00:03:08,050
the of l i

41
00:03:09,260 --> 00:03:10,260
it's going to be

42
00:03:12,050 --> 00:03:14,710
the cardinality of the set

43
00:03:14,740 --> 00:03:18,390
so we want to do is look to see what things are out of order

44
00:03:18,390 --> 00:03:20,750
here i look at e

45
00:03:20,760 --> 00:03:22,880
and c are in this war

46
00:03:22,900 --> 00:03:24,580
but c

47
00:03:24,630 --> 00:03:27,560
and he in that order so those are out of work

48
00:03:27,570 --> 00:03:32,490
so that counts as one of my element c

49
00:03:32,510 --> 00:03:35,710
and then ian a

50
00:03:37,210 --> 00:03:41,060
OK so those are out of order

51
00:03:41,070 --> 00:03:45,610
and then you die

52
00:03:45,650 --> 00:03:47,690
d out of water

53
00:03:47,700 --> 00:03:55,600
and then e

54
00:03:57,310 --> 00:03:58,880
those are out of work

55
00:04:01,820 --> 00:04:03,720
and now i go see

56
00:04:05,020 --> 00:04:08,080
see a those are in order so that doesn't count

57
00:04:11,690 --> 00:04:13,950
the so nothing with c

58
00:04:14,010 --> 00:04:15,410
and a

59
00:04:16,160 --> 00:04:18,410
eighty those or a b

60
00:04:18,430 --> 00:04:20,570
maybe those are in order

61
00:04:20,590 --> 00:04:22,300
so then DB

62
00:04:22,320 --> 00:04:24,870
PD so PD

63
00:04:24,930 --> 00:04:27,350
that's the last one

64
00:04:30,610 --> 00:04:32,350
that's my potential function

65
00:04:32,370 --> 00:04:33,780
it is equal to

66
00:04:34,800 --> 00:04:37,030
therefore ten

67
00:04:37,030 --> 00:04:42,030
the cardinality of the set is five five inversions

68
00:04:42,890 --> 00:04:45,930
between the two lists

69
00:04:50,450 --> 00:04:51,950
OK so

70
00:04:51,990 --> 00:04:55,890
so let's just cheque some properties of this potential function

71
00:04:59,340 --> 00:05:03,340
the first one is the notice that p of l i

72
00:05:03,410 --> 00:05:07,120
is critical to zero for all i

73
00:05:07,180 --> 00:05:11,820
number of inversions might be zero but is never less than zero

74
00:05:11,820 --> 00:05:13,870
OK it's always at least zero

75
00:05:13,910 --> 00:05:17,220
so that's one of the properties that we normally happen we're doing

76
00:05:17,240 --> 00:05:19,220
with potential functions

77
00:05:19,240 --> 00:05:21,090
the other thing is

78
00:05:21,200 --> 00:05:23,280
what about female zero

79
00:05:23,370 --> 00:05:29,620
z equals zero

80
00:05:29,640 --> 00:05:33,430
that depends powerless they start with

81
00:05:34,530 --> 00:05:38,550
so you know what's the initial were so zero if they start with the same

82
00:05:40,030 --> 00:05:41,870
and there are no inversions

83
00:05:41,930 --> 00:05:45,430
they might start with different we'll talk about different this later on but let's say

84
00:05:45,430 --> 00:05:46,570
for now

85
00:05:46,570 --> 00:05:48,760
it is zero because

86
00:05:48,930 --> 00:05:51,910
they start with the same list that seems like a fair

87
00:06:09,300 --> 00:06:11,570
so we have this potential function now

88
00:06:11,570 --> 00:06:15,760
deconvolution problems so once you work out the math and if you're interested the masses

89
00:06:15,760 --> 00:06:17,200
in the slide

90
00:06:17,200 --> 00:06:22,050
so when you were out as you've got that you have an unknown g which

91
00:06:22,050 --> 00:06:23,430
is essentially

92
00:06:23,440 --> 00:06:27,920
the derivative of the picture well with field at the surface which is just is

93
00:06:27,920 --> 00:06:29,220
this is something that you

94
00:06:29,250 --> 00:06:31,670
i get directed to

95
00:06:31,670 --> 00:06:33,000
to measure

96
00:06:33,020 --> 00:06:37,740
and the beautiful thing is that this kind of measurement g is actually related to

97
00:06:37,740 --> 00:06:38,950
what you put in

98
00:06:39,060 --> 00:06:41,400
the as simple convolution equation

99
00:06:42,150 --> 00:06:43,610
right so

100
00:06:43,630 --> 00:06:46,110
what you get to here is

101
00:06:46,170 --> 00:06:51,530
the system higher excited the system convolve with the derivative

102
00:06:51,590 --> 00:06:52,880
of alpha

103
00:06:52,900 --> 00:06:55,690
which is this function alpha prime time two

104
00:06:55,700 --> 00:07:01,670
so to recover a or equivalently to recover outside its will be sufficient to recover

105
00:07:01,670 --> 00:07:02,960
its derivative

106
00:07:02,960 --> 00:07:03,800
and so

107
00:07:03,860 --> 00:07:06,860
what you have to solve then is deconvolution problem

108
00:07:08,240 --> 00:07:11,820
you have the excite asian you have what you hear

109
00:07:11,900 --> 00:07:15,010
and they're related buys this convolution function

110
00:07:15,030 --> 00:07:20,170
the convolution by alpha prime and so you just need to deconvolve this product to

111
00:07:20,170 --> 00:07:22,280
find out what alpha is

112
00:07:22,280 --> 00:07:27,670
OK so we've got a very simple problem to solve now since now everything is

113
00:07:28,530 --> 00:07:34,960
well we have what seismologists calls the reflectivity function which is prime of tower two

114
00:07:35,130 --> 00:07:39,570
so we have the deconvolution problem to solve where if hours of prime of tower

115
00:07:39,570 --> 00:07:43,800
two i need to solve f was are equal to

116
00:07:43,840 --> 00:07:45,570
OK so now the

117
00:07:46,700 --> 00:07:48,550
this is the

118
00:07:48,630 --> 00:07:53,090
so the basic point here is that the earth is irregularly layer

119
00:07:53,090 --> 00:07:54,050
and so

120
00:07:54,070 --> 00:07:58,190
if we think about a of the first things this profile for primacy

121
00:07:58,880 --> 00:08:04,650
fuzzy well it's centralizes minus an offset so alpha prime it's going to be zero

122
00:08:04,650 --> 00:08:07,880
except at the locations of discontinuities

123
00:08:10,380 --> 00:08:16,240
right so the earth is the real services irregular layer as a consequence is

124
00:08:16,280 --> 00:08:21,150
that the vector are that i wish to recover is an irregular series of spikes

125
00:08:21,220 --> 00:08:24,280
what people call the sparse spike trains

126
00:08:28,030 --> 00:08:33,200
characteristics of seismic experiment is that the wavelength of the thing to do

127
00:08:33,220 --> 00:08:37,650
the wave function i used to probe the system is bandpass

128
00:08:37,670 --> 00:08:42,940
that is it's it is only supported in frequency in an interval because economic cycle

129
00:08:42,940 --> 00:08:45,760
frequencies india's it's physically impossible

130
00:08:45,760 --> 00:08:51,300
so the expectation is is bandpass which mean that this convolution equation kills a lot

131
00:08:51,300 --> 00:08:53,470
of frequencies it kills low

132
00:08:53,470 --> 00:08:55,130
and high frequencies

133
00:08:55,150 --> 00:08:58,530
and so as the a consequence of this is that the seismic experiment is missing

134
00:08:58,530 --> 00:09:02,650
a lot of low frequency or in other words the problem is extremely opposed this

135
00:09:02,650 --> 00:09:04,240
because you don't get to see

136
00:09:04,240 --> 00:09:07,800
the high frequency response of the reflectivity function

137
00:09:08,070 --> 00:09:11,260
OK so here's what i'm

138
00:09:11,280 --> 00:09:14,920
i am explaining you have the earth's

139
00:09:14,940 --> 00:09:17,130
the impedance that looks like this

140
00:09:17,130 --> 00:09:21,650
and of course the reflectivity function which is essentially the derivative of this thing looks

141
00:09:21,650 --> 00:09:24,150
something like

142
00:09:24,260 --> 00:09:29,700
OK now this is the kind of function that we used to probe the system

143
00:09:29,740 --> 00:09:31,150
so wavelength

144
00:09:31,190 --> 00:09:34,990
something that looks like this so this is fifty and this is what i get

145
00:09:34,990 --> 00:09:37,960
to years what we call the seismic traces the echo

146
00:09:37,970 --> 00:09:42,650
comes back after excited me

147
00:09:42,670 --> 00:09:47,440
so to explain how this thing is bandpass

148
00:09:47,460 --> 00:09:49,090
well this

149
00:09:49,110 --> 00:09:54,530
x citation function missus out in the frequency domain lots of components because it sharply

150
00:09:54,530 --> 00:09:59,110
concentrated around a frequency of interest

151
00:10:00,860 --> 00:10:01,690
all right so

152
00:10:01,700 --> 00:10:03,630
you can have

153
00:10:03,690 --> 00:10:07,110
very important paper so

154
00:10:07,150 --> 00:10:11,030
of course people have been interested in finding or for very long time

155
00:10:11,150 --> 00:10:15,990
it's so in very interesting paper john care about suggested for the first time in

156
00:10:15,990 --> 00:10:19,670
applied science the use of an one minimisation

157
00:10:19,690 --> 00:10:21,030
and so

158
00:10:21,090 --> 00:10:25,380
john about essentially formulated the problem exactly like this

159
00:10:25,400 --> 00:10:28,920
and then he said listen i mean what i need to kind of recovery something

160
00:10:28,920 --> 00:10:33,610
like this because i need to kind of reconstructing the discontinuities in

161
00:10:33,990 --> 00:10:36,360
in the first upper crust

162
00:10:36,380 --> 00:10:40,780
so here's what he wrote in nineteen seventy three paper is said in the convolving

163
00:10:40,780 --> 00:10:42,510
any observed seismic trace

164
00:10:42,530 --> 00:10:46,740
it is rather disappointing to discover that there is a non-zero spike at every point

165
00:10:46,860 --> 00:10:50,190
in time regardless of the data sampling rate

166
00:10:50,240 --> 00:10:53,840
one might hope to find facts only where real geological

167
00:10:53,880 --> 00:10:58,670
geologic discontinuities take place perhaps in one and can be utilized to give an output

168
00:10:58,670 --> 00:11:01,740
rates like figure thirteen

169
00:11:01,760 --> 00:11:02,690
and so

170
00:11:02,720 --> 00:11:06,570
essentially what john had to do is have to solve an underdetermined system of equation

171
00:11:06,570 --> 00:11:10,970
in new that because of the physics of the problem the right hand side with

172
00:11:10,970 --> 00:11:15,510
sparse any points although it doesn't do it in the paper he points out the

173
00:11:15,510 --> 00:11:18,300
use of l one norm to get

174
00:11:18,300 --> 00:11:22,070
a solution that looks like this and not something that would look like

175
00:11:22,090 --> 00:11:23,170
anything else

176
00:11:23,670 --> 00:11:27,530
because when you news minimum energy what you find is that there's a non-zero spike

177
00:11:27,530 --> 00:11:30,970
at every point in time regardless of the data something rate

178
00:11:30,990 --> 00:11:33,380
the course that is not physical

179
00:11:33,400 --> 00:11:35,570
OK and so

180
00:11:35,610 --> 00:11:43,030
a little later geophysicist started to crystallize these ideas so they were really crystallized in

181
00:11:43,030 --> 00:11:46,190
a paper by taleban macquarie which

182
00:11:46,240 --> 00:11:49,590
if i write my deconvolution problem matrix form

183
00:11:49,590 --> 00:11:53,070
g calls f are where f is the convolution matrix

184
00:11:53,070 --> 00:11:59,940
two banks in macau actually proposed to fit the data by an one minimisation

185
00:11:59,940 --> 00:12:03,760
and so they said OK what we should do is we should minimize the l

186
00:12:03,760 --> 00:12:04,720
one norm

187
00:12:04,720 --> 00:12:09,980
OK so that's something kind of an interesting point that explanations of success tend to

188
00:12:09,980 --> 00:12:11,920
be longer than expression

189
00:12:11,940 --> 00:12:16,290
now how do you explain i didn't mention it did in the case of planning

190
00:12:16,680 --> 00:12:22,170
the dome in the background knowledge in sense matalan instances that plan would be carried

191
00:12:22,320 --> 00:12:23,770
in everything

192
00:12:23,800 --> 00:12:26,800
goal is supported by somebody

193
00:12:26,820 --> 00:12:30,430
and that was what every step in the plan

194
00:12:30,490 --> 00:12:35,230
if it has any conditions somebody else is supporting them such that those support structures

195
00:12:35,420 --> 00:12:37,110
not violated

196
00:12:37,140 --> 00:12:39,180
OK so nobody else is coming in which

197
00:12:39,190 --> 00:12:41,200
so you actually true

198
00:12:41,210 --> 00:12:46,090
this explanation on your plan and which are features of the plan take part in

199
00:12:46,090 --> 00:12:48,760
that explanation those are the only relevant features

200
00:12:48,830 --> 00:12:53,300
that explain why defectors and the rest can be removed through sort of

201
00:12:54,850 --> 00:12:57,640
a subset of the region

202
00:12:58,230 --> 00:13:03,730
so there's been a lot of work in exploration research controlled rule learning pre nineteen

203
00:13:03,730 --> 00:13:06,060
ninety five what i said there was a big

204
00:13:06,160 --> 00:13:11,850
the increase in performance of domain independent planning staff nineteen ninety five there have been

205
00:13:11,880 --> 00:13:16,240
so before that there was a lot of work which showed that the the planets

206
00:13:16,240 --> 00:13:22,030
could be improved significantly using UBL analysis and more recently i think as i said

207
00:13:22,220 --> 00:13:28,030
in general the interest in speeding up using learning techniques over the used and are

208
00:13:28,040 --> 00:13:32,290
but there's been some examples that is where i think for example that the planet

209
00:13:32,380 --> 00:13:35,320
which i want to discuss the plan and invite now

210
00:13:35,390 --> 00:13:39,700
most implementations of grassland lined up using some sort of explanation based

211
00:13:39,710 --> 00:13:41,510
learning from failures

212
00:13:41,550 --> 00:13:47,220
improve performance and that seems to work quite well but are it basically uses EBL

213
00:13:47,220 --> 00:13:51,920
almost as explanations are correct and complete explanations of

214
00:13:51,940 --> 00:13:56,070
what i want you to understand that in general if you have background knowledge it's

215
00:13:56,070 --> 00:13:57,030
sort of

216
00:13:57,060 --> 00:14:00,820
it focuses you as to what part of your big example

217
00:14:00,870 --> 00:14:04,730
i can be blamed for its label can be created for

218
00:14:04,760 --> 00:14:07,090
OK and if you wrong it's still be

219
00:14:07,170 --> 00:14:11,420
it's like saying the feature selection that is on that space

220
00:14:12,370 --> 00:14:15,420
and i spend as much time talking what be allowed to say a few things

221
00:14:17,710 --> 00:14:22,880
which i might have rather misconceptions to you in your mind you know you if

222
00:14:22,880 --> 00:14:26,800
you read about this in literature or the literature of this sort of stuff comes

223
00:14:26,800 --> 00:14:29,470
up so one is a

224
00:14:29,520 --> 00:14:34,410
you know the idea that maybe explanation based learning on the complete and correct background

225
00:14:35,300 --> 00:14:38,390
as i mentioned in my card example

226
00:14:38,440 --> 00:14:43,060
OK so you explanation that as brazil background knowledge and to the extent you have

227
00:14:43,060 --> 00:14:48,460
any background knowledge that you believe in it introduces it focuses your learning of what

228
00:14:51,460 --> 00:14:55,850
in fact i think that sort of components the in that he was this analytical

229
00:14:55,850 --> 00:15:00,030
arguments and with knowledge of what knowledge four dimensional

230
00:15:00,070 --> 00:15:02,970
OK so if you have a background knowledge you want to use it in general

231
00:15:02,970 --> 00:15:08,140
not and unless you're going to be completely but why would you not want to

232
00:15:09,790 --> 00:15:15,030
so if you do have completely acknowledged that the learned knowledge will be the deductive

233
00:15:15,030 --> 00:15:18,580
closure of the original image so if you just had to get them to work

234
00:15:18,700 --> 00:15:22,980
i just kept spewing out of that effectively

235
00:15:22,980 --> 00:15:29,530
and the knowledge from the existing set of knowledge if you had complete correct explanations

236
00:15:29,530 --> 00:15:32,710
originally then you think that would be that detective was

237
00:15:32,730 --> 00:15:34,290
it's not for me that

238
00:15:34,540 --> 00:15:39,260
so it's more likely that the learning where you always making jumps from what you

239
00:15:39,260 --> 00:15:41,230
know what you

240
00:15:41,240 --> 00:15:45,660
OK so again i think the idea of thinking of these other the feature selection

241
00:15:45,660 --> 00:15:51,250
step is useful one the second thing is that maybe elizabeth competing connected and i

242
00:15:51,250 --> 00:15:55,110
don't think it's that much in vogue right now but there was the a time

243
00:15:55,110 --> 00:15:58,530
when people would actually think about this and i would say

244
00:15:58,550 --> 00:16:01,980
i think of another feature selection step twenty learning algorithm and you don't have a

245
00:16:03,320 --> 00:16:08,370
the third thing which actually comes in several description fabio if you look at any

246
00:16:08,370 --> 00:16:13,860
textbook level descriptions the is some mile mileage is a bad thing because knowledge was

247
00:16:13,860 --> 00:16:15,580
used in the lndl had do

248
00:16:15,940 --> 00:16:19,960
the problem what was the problem it turns out that if if you learn a

249
00:16:19,960 --> 00:16:21,880
whole bunch of rules

250
00:16:25,670 --> 00:16:29,700
if you learn a whole bunch of rules of this farm saying don't exist right

251
00:16:29,820 --> 00:16:33,010
the following is still the biggest rival the following is true

252
00:16:33,030 --> 00:16:37,420
and then you have knowledge among library you know can be things don't do this

253
00:16:37,420 --> 00:16:40,040
don't do this don't do don't do that and then i will you have a

254
00:16:40,040 --> 00:16:43,570
new plan you are looking at you a large library of all the do's and

255
00:16:43,570 --> 00:16:46,760
don'ts to see whether or not he should take a step OK and if there

256
00:16:46,760 --> 00:16:50,640
are too many of these rules you could be spending your lifetime matching

257
00:16:51,730 --> 00:16:55,560
the left hand side of the rule just to see whether it is applicable

258
00:16:55,590 --> 00:16:56,550
OK now

259
00:16:56,560 --> 00:17:01,330
this has nothing to do with explanation based learning policy in general search control knowledge

260
00:17:01,330 --> 00:17:02,590
which tries to

261
00:17:02,600 --> 00:17:05,910
which is organised in terms of a new bunches of rules you know you have

262
00:17:05,910 --> 00:17:09,590
to match those rules if you have a large number of plans that you stored

263
00:17:09,600 --> 00:17:13,750
in a library so that you can use them to solve new problems if the

264
00:17:13,770 --> 00:17:18,540
family becomes very large then you know you can give you a very small problem

265
00:17:18,540 --> 00:17:22,600
to solve your be spending time looking into this very large library trying to see

266
00:17:22,600 --> 00:17:23,770
whether or not

267
00:17:23,820 --> 00:17:26,730
another that i plan for the day in the life

268
00:17:26,770 --> 00:17:31,000
OK so i think that the problem is the real one but it's real one

269
00:17:31,000 --> 00:17:34,630
but in general it is absurd control knowledge it has nothing to do

270
00:17:35,800 --> 00:17:38,800
in a knowledge base was a small amount this one

271
00:17:38,840 --> 00:17:45,200
OK so that's basically what i wanted to say and on the part one other

272
00:17:45,200 --> 00:17:48,720
thing that actually i think something that we hopefully will learn that john as a

273
00:17:48,720 --> 00:17:53,740
and this lecture today is one thing that would be useful if you want to

274
00:17:53,740 --> 00:17:55,000
learn complex

275
00:17:55,010 --> 00:17:58,600
OK with a few examples examples

276
00:17:58,630 --> 00:18:02,420
you just have to use that knowledge i mean the classic what we did this

277
00:18:02,420 --> 00:18:06,720
for that right now i think it was in the audience to me i will

278
00:18:08,230 --> 00:18:12,760
we have in the humans basically only look learn those things that we are the

279
00:18:12,760 --> 00:18:14,560
most of any normal

280
00:18:14,620 --> 00:18:18,040
OK this is exactly the reason why you are not our own methods and you're

281
00:18:18,040 --> 00:18:22,510
the shorter the wavelength the higher the probability ever take a three

282
00:18:22,540 --> 00:18:25,760
you going to see the location of the qualitative division

283
00:18:25,810 --> 00:18:28,740
blue light is ten times higher probability

284
00:18:28,760 --> 00:18:31,520
to be scarier than red light

285
00:18:31,640 --> 00:18:34,060
and so whenever i going to do my

286
00:18:34,100 --> 00:18:39,490
scattering experiments on very fine particles you will see that light is going to be

287
00:18:40,370 --> 00:18:42,410
you can miss that

288
00:18:42,430 --> 00:18:46,560
now if the particles of which i scatter larger than the tens of micron say

289
00:18:46,560 --> 00:18:48,390
one micron this

290
00:18:48,470 --> 00:18:51,890
effect of color on probability of scattering is

291
00:18:51,910 --> 00:18:53,280
highly reduced

292
00:18:53,290 --> 00:18:57,520
and if i scan of very large particles and microns or so

293
00:18:57,600 --> 00:19:00,990
and there is no dependence anymore at all

294
00:19:01,010 --> 00:19:02,410
and in this lies the

295
00:19:02,450 --> 00:19:04,700
secret why the sky is blue

296
00:19:04,700 --> 00:19:06,790
i'll get back to that later today

297
00:19:06,830 --> 00:19:10,600
the reason why cigarette smoke can be blue if they smoke particles

298
00:19:10,620 --> 00:19:14,280
a very small and it's the reason why clouds of white

299
00:19:14,330 --> 00:19:18,350
because the sunlight hits the clouds the light scattering but the water drops in the

300
00:19:18,350 --> 00:19:19,680
clouds and not

301
00:19:19,740 --> 00:19:23,140
o point one microns but they are much larger they are more like ten microns

302
00:19:23,140 --> 00:19:24,410
enough so

303
00:19:24,410 --> 00:19:30,740
there is no preferred wavelength that scatters and so the clouds look white

304
00:19:30,760 --> 00:19:33,510
and so the first demonstration that i want to do

305
00:19:33,510 --> 00:19:36,100
it is very much like what you see here

306
00:19:36,140 --> 00:19:39,520
i'm going to send unpolarized light up here

307
00:19:39,560 --> 00:19:41,330
straight up

308
00:19:41,330 --> 00:19:45,450
we have bright spotlights there and the light goes straight up

309
00:19:45,490 --> 00:19:49,510
and here i'm going to put a very small dust particles

310
00:19:49,560 --> 00:19:53,960
and i decided to do that smoke simply cigarette smoke

311
00:19:54,010 --> 00:19:57,490
so i'm going to hold cigarette smoke in these beings

312
00:19:57,510 --> 00:20:01,520
and the light that will come to you no matter where you see it must

313
00:20:01,520 --> 00:20:05,470
have scattered closely over ninety degrees right it comes up like this but if you

314
00:20:05,470 --> 00:20:08,470
see it almost for everyone in the audience

315
00:20:08,490 --> 00:20:09,560
ninety degrees

316
00:20:09,580 --> 00:20:10,780
angle scattering

317
00:20:10,790 --> 00:20:12,850
so we should many polarizes

318
00:20:12,850 --> 00:20:14,310
you will be able to see

319
00:20:14,330 --> 00:20:15,350
that that light

320
00:20:15,370 --> 00:20:16,600
is polarized

321
00:20:16,640 --> 00:20:19,740
it's going to be polarized in this direction

322
00:20:19,780 --> 00:20:23,180
which is the direction that i have here

323
00:20:23,240 --> 00:20:25,600
that's the first thing i'm going to do

324
00:20:25,660 --> 00:20:27,290
for this demonstration

325
00:20:27,290 --> 00:20:28,640
so i need

326
00:20:30,140 --> 00:20:32,290
and i need to smoke

327
00:20:32,350 --> 00:20:40,700
as much as i hate to this

328
00:21:15,950 --> 00:21:18,890
so the light

329
00:21:18,950 --> 00:21:20,080
it's like an

330
00:21:20,100 --> 00:21:21,700
presumably you

331
00:21:30,870 --> 00:21:34,540
for this

332
00:21:34,740 --> 00:21:45,700
OK but i really

333
00:21:52,160 --> 00:21:53,870
well you do this

334
00:21:53,890 --> 00:21:55,430
number one

335
00:21:55,430 --> 00:21:56,910
that the light

336
00:21:56,930 --> 00:21:58,740
is bluish

337
00:21:58,740 --> 00:22:00,160
number two to this

338
00:22:01,830 --> 00:22:03,870
take your time for that

339
00:22:03,930 --> 00:22:06,080
if you don't see it is blue

340
00:22:06,140 --> 00:22:09,240
and the reason for that is that at low light intensities

341
00:22:09,260 --> 00:22:13,560
your eyes are not very sensitive full-color anymore

342
00:22:13,600 --> 00:22:21,100
looks quite close to me though

343
00:22:21,220 --> 00:22:23,260
now i want to do

344
00:22:23,290 --> 00:22:25,410
something in addition

345
00:22:25,470 --> 00:22:26,640
i mentioned

346
00:22:26,700 --> 00:22:28,510
that if the particles

347
00:22:28,560 --> 00:22:30,540
grow in size

348
00:22:30,580 --> 00:22:34,600
that the scattering is no longer preferred in the blue

349
00:22:34,620 --> 00:22:36,140
and i can demonstrate that

350
00:22:36,140 --> 00:22:39,040
i can kill two birds with one stone

351
00:22:39,080 --> 00:22:40,620
what i can do

352
00:22:40,680 --> 00:22:42,970
it is i can hold to smoke

353
00:22:42,970 --> 00:22:45,260
in my mind for a while

354
00:22:45,310 --> 00:22:49,040
and when i do that the

355
00:22:49,080 --> 00:22:50,720
the water vapor in my

356
00:22:57,140 --> 00:23:00,680
will precipitate on these dust particles

357
00:23:00,740 --> 00:23:03,370
and they will form small world graphs

358
00:23:03,430 --> 00:23:05,660
and when i post that out

359
00:23:05,700 --> 00:23:09,680
you will see a distinct difference in color between what you see now

360
00:23:09,720 --> 00:23:12,370
and the smoke that comes out of my lungs

361
00:23:12,430 --> 00:23:17,240
when these particles can microns and even larger you'll see then that the light is

362
00:23:19,040 --> 00:23:19,970
so this is

363
00:23:19,990 --> 00:23:23,850
this comes extract over and above come for free

364
00:23:23,850 --> 00:23:28,560
in order to make you see the difference shortly before i profile to smoke in

365
00:23:29,330 --> 00:23:32,390
i will again show you this smoke as it is now

366
00:23:32,390 --> 00:23:34,180
so you can compare the cause

367
00:23:34,220 --> 00:23:37,100
and you will see that there is a difference even though it doesn't it may

368
00:23:37,100 --> 00:23:38,810
not look very blue is to you

369
00:23:38,850 --> 00:23:41,060
the reason that i mentioned in darkness

370
00:23:41,120 --> 00:23:43,450
you don't have very good sensitive for calling

371
00:23:43,470 --> 00:23:45,160
so i'm going to hold is

372
00:23:45,160 --> 00:23:46,180
smoke now

373
00:23:46,200 --> 00:23:49,970
as much as i hate it is one of the worst demonstrations that i have

374
00:23:49,970 --> 00:23:55,850
to hold it in my lungs for a while

375
00:24:22,700 --> 00:24:25,350
i see you get very close

376
00:24:25,370 --> 00:24:26,240
the second

377
00:24:27,240 --> 00:24:32,280
i was very wide compared to the first one

378
00:24:32,290 --> 00:24:46,740
so we were able to catch two birds with one stone here

379
00:24:46,740 --> 00:24:50,850
the sky is blue because of this phenomenon

380
00:24:52,060 --> 00:24:53,790
you're standing

381
00:24:54,790 --> 00:24:56,350
on the earth

382
00:24:56,390 --> 00:24:58,930
and so light is coming in

383
00:24:58,950 --> 00:25:01,470
onto the earth's atmosphere

384
00:25:01,520 --> 00:25:02,990
the sun is there

385
00:25:03,040 --> 00:25:04,310
so light comes in

386
00:25:04,330 --> 00:25:06,410
and the lights cameras

387
00:25:06,430 --> 00:25:08,200
and the light that reaches you

388
00:25:08,200 --> 00:25:11,200
its canons of these extremely fine dust particles

389
00:25:11,200 --> 00:25:14,850
and also scanners of the air molecules themselves

390
00:25:14,890 --> 00:25:19,640
there are thermal fluctuations that go on all the time which causes density fluctuations in

391
00:25:19,640 --> 00:25:23,450
the air and they are sufficient to act as catalyst

392
00:25:23,510 --> 00:25:26,740
and so if light from here comes through

393
00:25:26,810 --> 00:25:28,140
the chances are

394
00:25:28,140 --> 00:25:29,370
that is below

395
00:25:29,410 --> 00:25:31,790
because there's a higher probability than red

396
00:25:31,830 --> 00:25:33,930
and this is also likely to be blue

397
00:25:33,970 --> 00:25:35,890
so when you look at the sky

398
00:25:35,950 --> 00:25:37,040
this guy

399
00:25:37,100 --> 00:25:41,160
looks road that's the reason has to do with this strong preference

400
00:25:41,220 --> 00:25:43,540
four color to be

401
00:25:43,540 --> 00:25:45,120
scan when it is

402
00:25:45,220 --> 00:25:46,660
blue light

403
00:25:46,680 --> 00:25:47,850
if you look

404
00:25:47,850 --> 00:25:49,910
in the direction of the sky

405
00:25:49,930 --> 00:25:52,260
an angle of ninety degrees

406
00:25:52,260 --> 00:25:54,120
the direction of the sun

407
00:25:54,160 --> 00:25:58,620
this guy is also linearly polarized for the reason that we now understand because the

408
00:25:58,620 --> 00:26:04,000
the first of the economic transition system of lectures on a string theory for pedestrians

409
00:26:04,090 --> 00:26:08,820
and with a promise of course this podcast was director of research at CNRS

410
00:26:08,840 --> 00:26:11,560
and the theoretical physics lab difficult moments montpellier

411
00:26:11,680 --> 00:26:16,420
because that's not the same thing is that is the imprisoning

412
00:26:16,480 --> 00:26:18,460
nineteen eighty then you would

413
00:26:18,480 --> 00:26:20,370
he went most of the slack

414
00:26:20,390 --> 00:26:23,690
is member of senate since nineteen eighty eight

415
00:26:23,690 --> 00:26:30,790
he has been member incorporated mechanical my has also visited from nineteen ninety two

416
00:26:30,820 --> 00:26:35,290
it's an expert in theoretical aspects of particle physics condensed matter and also a leading

417
00:26:35,290 --> 00:26:37,820
expert in knitting theory

418
00:26:37,850 --> 00:26:42,170
for this reason he has been awarded several prizes including two thousand two is better

419
00:26:42,210 --> 00:26:43,670
prize of the french

420
00:26:43,670 --> 00:26:49,520
the second society so very fortunate to have him introducing here first in these

421
00:26:50,340 --> 00:26:51,570
thank you very much

422
00:26:51,670 --> 00:26:57,530
has held thank you for the invitation to give these lectures here

423
00:26:57,540 --> 00:27:02,700
i understand this is a non-specialist audience now some of you may know a lot

424
00:27:02,700 --> 00:27:07,310
more of course from what i'm going to tell you about what i will try

425
00:27:07,340 --> 00:27:12,390
to do in history lectures is to try to give you a basic idea of

426
00:27:12,390 --> 00:27:14,120
what string theorists

427
00:27:14,200 --> 00:27:17,170
i have done are doing and what we hope

428
00:27:17,180 --> 00:27:20,930
four with trying to give you a particular

429
00:27:20,950 --> 00:27:22,480
there a the

430
00:27:22,480 --> 00:27:25,870
o point of view of the particle theories namely what

431
00:27:25,890 --> 00:27:30,030
one can hope to learn about particle physics in this place this is of course

432
00:27:30,030 --> 00:27:35,570
very appropriate so in the first lecture this will be

433
00:27:35,590 --> 00:27:39,960
an extremely elementary one of the first one i just want to get across to

434
00:27:39,960 --> 00:27:47,100
you the basic ideas of the basic tools of string theory in elliptic and up

435
00:27:47,310 --> 00:27:52,540
way and then the next two lectures our move on to the two main stream

436
00:27:52,540 --> 00:27:58,140
activities of string theory over the last twenty years almost a on the the one

437
00:27:58,140 --> 00:28:03,560
hand the unification of particle interactions and on the other hand QCD

438
00:28:03,650 --> 00:28:10,160
the strong force this as you will see the crests along different routes but which

439
00:28:10,160 --> 00:28:14,830
meet at various places in interesting ways

440
00:28:15,560 --> 00:28:16,900
to go back

441
00:28:16,910 --> 00:28:21,900
forty years of forty five years in the sense that the whole story one may

442
00:28:21,900 --> 00:28:29,440
say started with one of his prophetic paper so the iraq in nineteen sixty two

443
00:28:29,460 --> 00:28:34,550
it did not as you may know the didn't likely normalisation he thought it should

444
00:28:34,550 --> 00:28:39,710
be some sort of effective theory effective calculation

445
00:28:39,710 --> 00:28:43,680
and that nature in the sense with finite had some optional

446
00:28:43,690 --> 00:28:49,680
that of scale and may be motivated by this it's not clear in the paper

447
00:28:49,780 --> 00:28:54,080
he thought that maybe he could try to understand why there is both an electron

448
00:28:54,080 --> 00:28:59,030
and a new one in nature which after all are very similar particles except for

449
00:28:59,110 --> 00:29:03,330
different mass by just thinking of both of them

450
00:29:03,340 --> 00:29:09,060
as quantum states of one extended object in this case the charged membrane

451
00:29:09,090 --> 00:29:13,720
actually if you go back and look at this paper the actor it qualifies the

452
00:29:13,720 --> 00:29:18,800
membrane fortunately he did it in a cheaper way doing what we want to make

453
00:29:18,800 --> 00:29:21,030
all bore bored quantization

454
00:29:21,030 --> 00:29:25,180
because otherwise he would have discovered that it's not possible

455
00:29:25,210 --> 00:29:29,860
with our new techniques to quantify the membrane and actually quite

456
00:29:29,900 --> 00:29:36,550
surprisingly he even found the mass ratio of about fifty miles that's of course not

457
00:29:36,550 --> 00:29:39,910
correct but it's a large number of which

458
00:29:39,930 --> 00:29:46,100
comes out of a very simple calculation and this was in a sense the first

459
00:29:46,100 --> 00:29:51,060
study of extended quantum objects in modern physics

460
00:29:51,120 --> 00:29:58,000
now if you years later in a totally different to decouple development people were starting

461
00:29:58,000 --> 00:30:05,250
at that time met resonances in hadronic collisions and they realize that these resonances were

462
00:30:05,250 --> 00:30:12,520
actually arranging themselves in this plot where you put mass squared on the horizontal axis

463
00:30:12,520 --> 00:30:19,420
and you put angular momentum or have to get to know how to use this

464
00:30:19,430 --> 00:30:30,550
they get this could

465
00:30:30,690 --> 00:30:38,480
so one portsmouth square the the horizontal axis and one outputs a angular momentum or

466
00:30:38,480 --> 00:30:44,730
spin on the vertical axis and if you look at the experiment method brought these

467
00:30:44,730 --> 00:30:51,540
are probably still the most impressive experimental evidence for the existence of relativistic quantum strings

468
00:30:51,540 --> 00:30:56,600
in nature this approach are absolutely straight lines as you can see

469
00:30:56,640 --> 00:31:02,830
over about a decade in energy from half GV to about five or ten TV

470
00:31:03,230 --> 00:31:05,300
and this is exactly what

471
00:31:05,350 --> 00:31:10,740
the theory of quantum relativistic strings predicts as i say more in a minute is

472
00:31:10,740 --> 00:31:13,560
that sometimes called the the chief around shiploads

473
00:31:13,790 --> 00:31:19,680
and the edge of trajectories that describe this vision resonances

474
00:31:21,190 --> 00:31:23,540
this was forty years ago

475
00:31:23,570 --> 00:31:28,680
however precisely these two ideas are allowed to live today they are still alive but

476
00:31:28,680 --> 00:31:34,250
they have gone numerous transmutation almost unrecognizable

477
00:31:34,310 --> 00:31:38,450
but still very out of

478
00:31:38,570 --> 00:31:41,680
the first idea namely the fact that

479
00:31:41,680 --> 00:31:47,250
all the particles could be may be unified by an extended object in particular the

480
00:31:47,250 --> 00:31:52,870
quarks leptons and gauge bosons of the standard model in this

481
00:31:52,880 --> 00:31:57,310
as you will see is indeed very much what string theories have been trying to

482
00:31:57,310 --> 00:31:59,830
today we going talk about a new

483
00:31:59,840 --> 00:32:06,320
balanced search structures data structure that maintains the dynamics that subject insertion deletion and search

484
00:32:06,330 --> 00:32:08,390
called skip lists

485
00:32:08,440 --> 00:32:16,020
so called this the dynamic search structure because of the data structure and support search

486
00:32:16,420 --> 00:32:19,280
dynamic meaning experimentally

487
00:32:19,300 --> 00:32:26,190
so what other dynamics are structures so we know just for the sake of comparison

488
00:32:26,680 --> 00:32:29,830
weak everyone of

489
00:32:30,260 --> 00:32:33,420
efficient i should say

490
00:32:36,140 --> 00:32:41,130
time per operation

491
00:32:43,570 --> 00:32:45,300
so this is really easy

492
00:32:45,310 --> 00:32:47,440
question to get us

493
00:32:50,220 --> 00:32:52,290
so all the last we

494
00:32:52,330 --> 00:32:55,370
we shouldn't be so

495
00:32:55,480 --> 00:33:00,030
three go the problems that with troops that in some sense

496
00:33:00,070 --> 00:33:05,240
this is the simplest dynamics can get from first principles because all we needed was

497
00:33:06,290 --> 00:33:11,060
the bound on randomly constructed binary search tree trips did well

498
00:33:11,110 --> 00:33:14,730
so that was sort of first one is sort ending when you get your problem

499
00:33:15,520 --> 00:33:23,290
what else

500
00:33:24,620 --> 00:33:27,580
red black trees good answer

501
00:33:30,810 --> 00:33:32,700
that was exactly one week ago

502
00:33:32,710 --> 00:33:37,880
because the they have guaranteed logan performance of this was the

503
00:33:37,900 --> 00:33:41,320
expected bounds this was the worst case order log

504
00:33:41,330 --> 00:33:44,650
for operation insert delete search

505
00:33:44,660 --> 00:33:46,630
and there is one more

506
00:33:46,640 --> 00:33:53,700
this what recitation on friday

507
00:33:54,990 --> 00:33:56,570
the trees good

508
00:33:56,590 --> 00:34:01,690
and by the trees i also include two three trees to three report reason of

509
00:34:02,370 --> 00:34:06,690
so these are constant or if you made your between a little bit

510
00:34:08,190 --> 00:34:09,140
and these have

511
00:34:09,160 --> 00:34:13,850
guaranteed or log performance the worst case order log

512
00:34:13,870 --> 00:34:15,890
so you should know this these are all

513
00:34:17,400 --> 00:34:24,100
search structures that dynamically support insertions deletions they support search is finally given

514
00:34:24,140 --> 00:34:28,290
and if you don't find the key finders predecessor and successor pretty easily in all

515
00:34:28,290 --> 00:34:29,440
of these structures

516
00:34:29,480 --> 00:34:32,770
if you want arguments some data structure you should think about which one of these

517
00:34:32,770 --> 00:34:34,800
diseases to augment

518
00:34:34,820 --> 00:34:37,050
as in monday's lecture

519
00:34:38,430 --> 00:34:40,730
the question i want to post here

520
00:34:40,750 --> 00:34:41,940
is suppose

521
00:34:41,950 --> 00:34:44,560
i give you all right now

522
00:34:44,570 --> 00:34:45,570
which would be great

523
00:34:45,590 --> 00:34:49,330
but then i asked you to in order to keep this laptop you have to

524
00:34:49,330 --> 00:34:52,440
implement one of these data structures

525
00:34:52,450 --> 00:34:54,000
let's say within

526
00:34:54,010 --> 00:34:56,360
this class our

527
00:34:56,370 --> 00:35:00,780
do you think you could do it how many people think you could go

528
00:35:00,790 --> 00:35:03,280
what a couple people fifty k

529
00:35:03,350 --> 00:35:05,470
all four of people

530
00:35:05,520 --> 00:35:09,330
i could probably do my preference would be trees there's sort of the

531
00:35:09,410 --> 00:35:10,990
simplest in my mind

532
00:35:11,000 --> 00:35:14,190
this without using the textbook to be closed book

533
00:35:14,210 --> 00:35:19,170
example i don't have an about us to do unfortunately

534
00:35:19,190 --> 00:35:24,060
so the trees are pretty reasonable deletion you have to remember this stealing from the

535
00:35:24,060 --> 00:35:28,200
sibling and what not to do which is a bit tricky red black trees i

536
00:35:28,200 --> 00:35:32,010
can never remember how to look it up or are we derive the three places

537
00:35:33,160 --> 00:35:35,610
are a bit messy so that would be

538
00:35:35,690 --> 00:35:39,900
take a little while remember exactly how this work you'd have to solve your problem

539
00:35:39,900 --> 00:35:42,310
set again if you don't have to memorize

540
00:35:42,320 --> 00:35:47,090
at skip lists on the other hand are data structure you will never forget

541
00:35:47,140 --> 00:35:48,820
and something you can implement

542
00:35:48,830 --> 00:35:49,940
within an hour

543
00:35:49,960 --> 00:35:53,040
o problem i've made this claim

544
00:35:53,110 --> 00:35:57,610
a couple times before and i always felt bad because i've never actually done it

545
00:35:57,610 --> 00:36:00,480
so this morning i implemented skip lists

546
00:36:02,230 --> 00:36:04,510
took me ten minutes to implement link

547
00:36:04,560 --> 00:36:07,230
thirty minutes to implement skip lists

548
00:36:07,280 --> 00:36:09,600
another thirty minutes debug

549
00:36:09,610 --> 00:36:11,960
there you go

550
00:36:12,010 --> 00:36:13,190
it can be done

551
00:36:13,200 --> 00:36:16,540
skip lists are really simple and at no point writing the code that i have

552
00:36:16,540 --> 00:36:17,670
to make

553
00:36:17,680 --> 00:36:22,290
whereas every other structure i would have to think there's one moment when

554
00:36:22,290 --> 00:36:23,600
my name was get or

555
00:36:23,600 --> 00:36:28,680
and i'm going to be talking about statistical relational learning

556
00:36:28,680 --> 00:36:33,180
and the way that i want that this tutorial is like two

557
00:36:34,960 --> 00:36:36,120
about it

558
00:36:36,120 --> 00:36:37,960
an hour and a half or so

559
00:36:37,980 --> 00:36:39,100
a little bit more

560
00:36:39,120 --> 00:36:39,980
and then

561
00:36:40,000 --> 00:36:44,870
break a little bit early for line so you can all get in line early

562
00:36:44,910 --> 00:36:47,050
and then come back at the regular time

563
00:36:48,160 --> 00:36:50,840
and what i'm going to be presenting

564
00:36:50,870 --> 00:36:53,290
is really

565
00:36:53,300 --> 00:36:54,660
an attempt

566
00:36:56,810 --> 00:37:00,010
i have a lot of ideas for a lot of different people

567
00:37:00,060 --> 00:37:04,410
that have been involved and

568
00:37:04,410 --> 00:37:09,990
various statistical relational learning workshop had over the years

569
00:37:10,040 --> 00:37:18,010
and various other workshops like the related multirelational data mining workshop couple dog still workshops

570
00:37:18,010 --> 00:37:20,400
and so on

571
00:37:22,730 --> 00:37:25,750
so it's an attempt at synthesis

572
00:37:25,800 --> 00:37:27,200
but it's also

573
00:37:27,200 --> 00:37:28,110
you know

574
00:37:28,200 --> 00:37:31,810
even at three and a half hours seems like a long time it still cannot

575
00:37:31,810 --> 00:37:35,690
show it's not covering everything

576
00:37:35,730 --> 00:37:38,470
if you do want to see everything

577
00:37:38,500 --> 00:37:42,500
so we just recently i have the book at which

578
00:37:42,530 --> 00:37:44,860
i didn't bring because

579
00:37:45,800 --> 00:37:48,140
like six hundred pages in

580
00:37:48,160 --> 00:37:50,870
we use too much in my luggage it would have been able to take it

581
00:37:50,870 --> 00:37:56,560
carry on but this is available from MIT press

582
00:37:56,580 --> 00:38:00,310
and it has a lot more detail a lot more

583
00:38:00,340 --> 00:38:03,580
kind of tutorial chapters on

584
00:38:03,590 --> 00:38:07,860
a number of the different approaches and i'm going to talk about

585
00:38:15,560 --> 00:38:22,730
the general outline of the tutorial i'm going to spend up twenty minutes during and

586
00:38:22,730 --> 00:38:24,110
then the core

587
00:38:24,120 --> 00:38:26,860
it's going to be talking about

588
00:38:26,880 --> 00:38:29,270
four different approaches to

589
00:38:29,270 --> 00:38:31,450
statistical relational learning

590
00:38:32,790 --> 00:38:33,770
and then

591
00:38:33,800 --> 00:38:37,880
can interact with applications and future directions

592
00:38:37,920 --> 00:38:39,950
and definitely

593
00:38:39,960 --> 00:38:44,420
you know make this is interactive is possible if you guys have questions

594
00:38:44,480 --> 00:38:46,050
sure an interactive

595
00:38:46,050 --> 00:38:50,330
ask questions and so on and clarifications

596
00:38:56,360 --> 00:38:58,580
what is SRL

597
00:38:58,630 --> 00:39:01,050
there's a question will why

598
00:39:03,330 --> 00:39:06,320
and i kind of simplistic view

599
00:39:06,330 --> 00:39:09,390
i have zero is

600
00:39:09,450 --> 00:39:13,480
well if you think has that traditional

601
00:39:13,520 --> 00:39:17,990
statistical machine learning approaches all of them make

602
00:39:18,170 --> 00:39:21,130
really strong assumptions about

603
00:39:21,140 --> 00:39:25,740
what kind of sample you have that you have an IID random sample

604
00:39:25,800 --> 00:39:30,920
each of instances has the same kind of structure there are homogeneous objects from single

605
00:39:30,920 --> 00:39:33,020
relation so that's one

606
00:39:34,640 --> 00:39:36,230
an area

607
00:39:36,240 --> 00:39:38,230
of machine learning

608
00:39:40,240 --> 00:39:46,980
areas machine learning that has always dealt with structure relational learning inductive logic programming again

609
00:39:47,010 --> 00:39:50,680
caricature of that is traditionally

610
00:39:50,700 --> 00:39:58,710
are at least initially it didn't deal very much with noise and that examples so

611
00:39:58,770 --> 00:39:59,890
we have these two

612
00:40:01,730 --> 00:40:03,480
machine learning settings

613
00:40:03,490 --> 00:40:04,800
but in reality

614
00:40:04,800 --> 00:40:06,740
most of the data

615
00:40:06,760 --> 00:40:09,800
so we wanted to know what is for stuff

616
00:40:09,850 --> 00:40:16,430
multi relational and heterogeneous even sometimes family structure

617
00:40:17,210 --> 00:40:19,010
it's noisy and uncertain

618
00:40:19,070 --> 00:40:20,800
and so

619
00:40:20,820 --> 00:40:23,460
statistical relational learning kind of

620
00:40:24,640 --> 00:40:26,230
an emerging area

621
00:40:30,110 --> 00:40:35,400
exemplified also by work and say social network link analysis

622
00:40:35,410 --> 00:40:38,770
hypertext and web mining graph mining

623
00:40:38,770 --> 00:40:42,030
and relational learning and inductive logic programming always

624
00:40:42,120 --> 00:40:47,700
fields and areas are trying to deal with this can make

625
00:40:47,750 --> 00:40:49,520
logical structure

626
00:40:49,530 --> 00:40:51,510
and uncertainty

627
00:40:53,740 --> 00:40:55,220
part of the reason

628
00:40:55,240 --> 00:40:57,970
of y is coming to the forefront now

629
00:40:57,990 --> 00:41:04,060
it's so many other domains and it that they were getting really exemplified so you

630
00:41:04,060 --> 00:41:06,600
know if you think of web data OK that

631
00:41:06,620 --> 00:41:07,890
can the classic

632
00:41:07,910 --> 00:41:12,490
up where you have some some kind of link structure and you structure inside the

633
00:41:12,490 --> 00:41:18,340
pages and so on the graphical data where you have citations you have authors to

634
00:41:18,470 --> 00:41:20,510
other kinds of attributes

635
00:41:20,880 --> 00:41:24,200
many kinds of biological data so

636
00:41:24,390 --> 00:41:29,310
epidemiological data so disease spread but then also

637
00:41:29,330 --> 00:41:35,370
things like protein protein interaction networks all of these things have a kind of natural

638
00:41:35,390 --> 00:41:41,120
graph relational structure but then also natural noisy component

639
00:41:41,140 --> 00:41:45,090
you know a lot of the work in natural language processing these days is trying

640
00:41:45,090 --> 00:41:52,880
to capture both the structure semantic structure together with the co occurrence information

641
00:41:52,890 --> 00:41:54,450
computer vision

642
00:41:54,510 --> 00:41:57,160
there's lots of examples

643
00:42:01,510 --> 00:42:02,140
i have

644
00:42:02,150 --> 00:42:08,380
let me start with kind of caricature of what is a zero

645
00:42:08,390 --> 00:42:12,470
this is always my favourite slide sometimes they have these all animated

646
00:42:14,900 --> 00:42:16,970
well certainly

647
00:42:17,020 --> 00:42:20,020
one way of doing it kind of alphabet soup

648
00:42:20,570 --> 00:42:22,640
all these different

649
00:42:22,650 --> 00:42:28,630
representations and systems proposed for combining together

650
00:42:29,750 --> 00:42:35,770
probability and i can kind of cheese about this because i'm responsible for at least

651
00:42:35,770 --> 00:42:40,080
a couple of these collections of alphabet

652
00:42:41,010 --> 00:42:45,200
actually one of the things that i hope i'm going to be able to accomplish

653
00:42:47,510 --> 00:42:53,270
tutorials can demystify the model that so try and pull out you know

654
00:42:53,380 --> 00:42:54,490
what are

655
00:42:54,500 --> 00:42:55,880
the comment

656
00:42:55,890 --> 00:42:58,430
commonality is and what are they

657
00:42:58,440 --> 00:43:00,770
kind of ways of characterizing

658
00:43:00,780 --> 00:43:03,440
these different approaches

659
00:43:03,450 --> 00:43:07,830
kind of second view

660
00:43:08,950 --> 00:43:10,830
a little bit more

661
00:43:12,090 --> 00:43:13,910
representations two

662
00:43:15,040 --> 00:43:16,540
this is kind of

663
00:43:17,560 --> 00:43:21,470
where these approaches come from well

664
00:43:21,490 --> 00:43:27,070
there's certainly you can identify two different camps there's kind of people that started off

665
00:43:28,240 --> 00:43:31,540
a logical

666
00:43:31,580 --> 00:43:34,070
representation that did capture

667
00:43:34,080 --> 00:43:36,580
the relational structure and then

668
00:43:36,590 --> 00:43:38,890
added in probability

669
00:43:39,930 --> 00:43:46,820
defectors started with some sort of probabilistic representation better propositional

670
00:43:48,260 --> 00:43:50,640
and then added in structure

671
00:43:50,640 --> 00:43:51,030
and would

672
00:43:52,400 --> 00:43:54,630
the exact numbers are all sins he served under

673
00:43:57,740 --> 00:44:01,180
this is a very general properties common to all analytical functions

674
00:44:07,200 --> 00:44:09,410
but in the case of the function we can do

675
00:44:10,320 --> 00:44:11,410
something very special

676
00:44:14,180 --> 00:44:18,200
we can consider that the fact that function which is a multiplicative function

677
00:44:18,990 --> 00:44:21,530
by some factor which is explained session function

678
00:44:22,740 --> 00:44:24,200
its position is never zero

679
00:44:24,980 --> 00:44:27,090
so we do so if you do not have any new

680
00:44:30,560 --> 00:44:32,290
this function can be easily computed

681
00:44:33,420 --> 00:44:35,030
and it was it is important

682
00:44:36,810 --> 00:44:37,910
which changes the argument

683
00:44:40,660 --> 00:44:42,740
when teenage range already all numbers

684
00:44:43,350 --> 00:44:45,650
the argument over the years range or

685
00:44:45,860 --> 00:44:46,830
along the critical line

686
00:44:48,970 --> 00:44:52,780
this function turned out to be a deal forty awareness of it

687
00:44:53,900 --> 00:44:54,760
and so we can plot

688
00:44:56,180 --> 00:44:56,720
o twenty

689
00:44:57,510 --> 00:44:58,240
goes here

690
00:45:00,110 --> 00:45:02,670
this argument goes along the critical line and

691
00:45:04,040 --> 00:45:04,910
see evidence of

692
00:45:06,820 --> 00:45:07,410
functions that

693
00:45:09,710 --> 00:45:11,480
so now we can try to do the following

694
00:45:12,690 --> 00:45:13,010
let us

695
00:45:14,460 --> 00:45:15,290
some important

696
00:45:15,740 --> 00:45:17,590
he green point six invisible

697
00:45:21,240 --> 00:45:23,000
our functions at this point

698
00:45:27,070 --> 00:45:28,850
we don't need to find exact values

699
00:45:29,840 --> 00:45:30,880
it would be sufficient

700
00:45:31,330 --> 00:45:33,240
to find the evidence for some possible errors

701
00:45:35,060 --> 00:45:36,240
as long as we assure

702
00:45:37,040 --> 00:45:40,670
that we can judge what is the sign of our function these points

703
00:45:41,910 --> 00:45:43,260
if you know that this point

704
00:45:43,810 --> 00:45:44,900
the function is negative

705
00:45:45,800 --> 00:45:46,910
at this point

706
00:45:47,440 --> 00:45:48,160
it is positive

707
00:45:49,020 --> 00:45:50,990
then we know for sure that somewhere in between

708
00:45:51,630 --> 00:45:52,600
there should be zero

709
00:45:55,740 --> 00:45:56,390
and in this way

710
00:45:57,200 --> 00:45:59,250
we can count the number of zeros

711
00:45:59,890 --> 00:46:01,160
exactly on the critical line

712
00:46:03,660 --> 00:46:05,340
it remains the company's numbers

713
00:46:07,650 --> 00:46:09,890
and so the classical methods long before during

714
00:46:10,920 --> 00:46:11,700
but as follows

715
00:46:12,770 --> 00:46:14,400
it consisted of two stages

716
00:46:15,760 --> 00:46:16,580
the first stage

717
00:46:18,830 --> 00:46:21,240
this number of people concluded this integral

718
00:46:22,930 --> 00:46:23,270
and then

719
00:46:24,000 --> 00:46:24,770
try to find

720
00:46:25,690 --> 00:46:27,680
such many numbers green points

721
00:46:28,990 --> 00:46:30,600
go going increase in order

722
00:46:31,480 --> 00:46:32,720
and until that each time

723
00:46:33,390 --> 00:46:34,330
we get the changes sign

724
00:46:36,990 --> 00:46:40,010
if succeeded then they would know for sure that all zeros

725
00:46:40,790 --> 00:46:42,170
the lie on the critical line

726
00:46:46,480 --> 00:46:47,300
this is principle

727
00:46:48,860 --> 00:46:49,720
how could you find

728
00:46:52,320 --> 00:46:54,020
but this is the sign of the function

729
00:46:56,600 --> 00:46:57,740
luckily gram

730
00:46:58,240 --> 00:46:59,930
was the first person to publish

731
00:47:00,850 --> 00:47:01,210
proof of

732
00:47:02,070 --> 00:47:03,730
about initial that of the function

733
00:47:04,550 --> 00:47:05,310
gave the

734
00:47:06,140 --> 00:47:08,320
heuristic rules for selecting such points

735
00:47:10,850 --> 00:47:12,780
so he refuses to the function

736
00:47:14,040 --> 00:47:16,010
it grows almost as a linear function

737
00:47:17,110 --> 00:47:18,480
because this factor grows

738
00:47:19,040 --> 00:47:19,820
very slowly

739
00:47:21,330 --> 00:47:22,550
sit comes here

740
00:47:23,100 --> 00:47:23,780
in the exponent

741
00:47:24,490 --> 00:47:25,200
in the exponent

742
00:47:26,660 --> 00:47:29,510
this can be written as the real part and imaginary part

743
00:47:30,280 --> 00:47:33,090
and this function dimension but is of interest for us

744
00:47:37,800 --> 00:47:39,590
it looks like a sine function

745
00:47:43,110 --> 00:47:44,860
and there are also this function

746
00:47:46,010 --> 00:47:46,490
on our

747
00:47:47,610 --> 00:47:48,360
gram points

748
00:47:50,530 --> 00:47:54,390
the traditional unnumbered in such a way that the the function at point

749
00:47:54,840 --> 00:47:57,000
is indexed families that book by multiplied by

750
00:48:00,580 --> 00:48:01,100
if you look

751
00:48:03,110 --> 00:48:05,200
the have that the plot of the

752
00:48:05,930 --> 00:48:07,490
and the plot of the sine function

753
00:48:08,280 --> 00:48:08,780
we'll see

754
00:48:09,780 --> 00:48:10,580
they behave

755
00:48:11,290 --> 00:48:13,930
similar to sine function and cosine function

756
00:48:15,270 --> 00:48:15,920
in other ways

757
00:48:16,650 --> 00:48:18,850
zero so functions function

758
00:48:19,670 --> 00:48:22,510
respond approximately those axioms as the function

759
00:48:24,500 --> 00:48:24,760
and so

760
00:48:25,470 --> 00:48:26,520
grams points are

761
00:48:30,840 --> 00:48:31,940
four be a good one

762
00:48:33,790 --> 00:48:35,000
joke this i know that

763
00:48:46,300 --> 00:48:47,020
grand made this

764
00:48:48,030 --> 00:48:48,860
observation on

765
00:48:50,090 --> 00:48:51,230
looking at the initial

766
00:48:51,930 --> 00:48:54,510
below so that and that the the functions

767
00:48:55,440 --> 00:48:56,080
but later

768
00:48:56,650 --> 00:48:59,410
hodkinson extended it to all possible values of

769
00:49:00,150 --> 00:49:01,430
and called ground floor

770
00:49:03,720 --> 00:49:05,360
so grammar can be stated that

771
00:49:06,680 --> 00:49:10,200
itself that function alternating sign is a growth of

772
00:49:13,690 --> 00:49:14,610
but ironically

773
00:49:15,310 --> 00:49:16,360
atkinson himself

774
00:49:16,900 --> 00:49:19,240
following that can alone is not a lot of

775
00:49:21,470 --> 00:49:26,570
following the first violations of this long and nowadays know that there are infinitely many violations

776
00:49:28,970 --> 00:49:29,780
look at this picture

777
00:49:31,160 --> 00:49:32,490
he is the negative

778
00:49:33,360 --> 00:49:34,550
here's a negative

779
00:49:35,220 --> 00:49:37,560
but we can judge from the picture what happens here

780
00:49:38,540 --> 00:49:42,580
and fact the middle is also negative so we have city negative images

781
00:49:43,040 --> 00:49:44,810
in three consecutive grand points

782
00:49:47,000 --> 00:49:47,990
so we can distinguish

783
00:49:48,440 --> 00:49:49,750
between good ground points

784
00:49:50,480 --> 00:49:51,460
and background words

785
00:49:56,160 --> 00:49:56,990
what would you do

786
00:49:57,820 --> 00:49:59,550
and we need to find a place where

787
00:50:00,470 --> 00:50:01,660
that function is positive

788
00:50:05,700 --> 00:50:08,140
and then just make some shift from this point

789
00:50:09,360 --> 00:50:11,130
all that we need this you would be

790
00:50:11,990 --> 00:50:14,320
would keep the point between the consecutive grand points

791
00:50:16,140 --> 00:50:17,510
in this case is rather small

792
00:50:19,800 --> 00:50:20,080
and so

793
00:50:22,500 --> 00:50:23,650
the classical methods was

794
00:50:24,200 --> 00:50:26,150
none before argument was as follows

795
00:50:27,170 --> 00:50:28,510
the first stage company

796
00:50:29,180 --> 00:50:29,800
this integral

797
00:50:32,030 --> 00:50:35,790
try to find a small numbers age minus one and so on

798
00:50:36,600 --> 00:50:39,530
go on sex in some schools in increasing order

799
00:50:40,310 --> 00:50:41,560
and verify that the change

800
00:50:42,230 --> 00:50:43,590
of science at this point

801
00:50:45,560 --> 00:50:46,260
he succeeded

802
00:50:46,980 --> 00:50:48,210
you know that all zeros

803
00:50:49,100 --> 00:50:50,010
held the critical like

804
00:50:56,800 --> 00:50:58,480
and t why would not succeed

805
00:51:01,000 --> 00:51:03,210
and very very simplified situation a bit

806
00:51:04,910 --> 00:51:07,270
when we speak about the number of zeros

807
00:51:08,250 --> 00:51:08,860
we can count

808
00:51:09,240 --> 00:51:10,240
them into ways

809
00:51:10,240 --> 00:51:13,260
this is the basic

810
00:51:13,350 --> 00:51:16,070
object of interest

811
00:51:16,770 --> 00:51:17,880
i mean the

812
00:51:17,970 --> 00:51:21,840
basically to characterize a markov process

813
00:51:21,860 --> 00:51:24,500
what i'm telling you

814
00:51:24,510 --> 00:51:25,740
is that

815
00:51:25,780 --> 00:51:28,050
so the markov process

816
00:51:28,100 --> 00:51:30,860
i'm going to assume full second simplicity

817
00:51:30,880 --> 00:51:33,810
that is the emerging markov process

818
00:51:33,860 --> 00:51:36,990
on its own tightly defined essentially

819
00:51:37,010 --> 00:51:40,020
by the distribution

820
00:51:40,820 --> 00:51:43,540
the initial state x one

821
00:51:43,810 --> 00:51:45,830
but this is applied to the new

822
00:51:45,850 --> 00:51:48,340
o point you don't see

823
00:51:48,390 --> 00:51:50,500
all basically

824
00:51:50,550 --> 00:51:55,080
well i also need to know politician density

825
00:51:56,770 --> 00:51:58,970
markov process xk

826
00:51:58,980 --> 00:52:01,100
so what i would assume

827
00:52:01,120 --> 00:52:02,260
is that

828
00:52:02,270 --> 00:52:06,560
assuming xt i just wanted to put this quantity here

829
00:52:06,570 --> 00:52:12,980
then xk is distributed according to these conditional one of

830
00:52:13,000 --> 00:52:14,760
but when you don't

831
00:52:14,840 --> 00:52:16,420
on the

832
00:52:16,440 --> 00:52:17,690
the same thing

833
00:52:18,010 --> 00:52:19,760
the bits of people

834
00:52:19,780 --> 00:52:23,320
so this thing is nothing but the probability

835
00:52:24,720 --> 00:52:28,740
in human xk parliament twice but

836
00:52:30,270 --> 00:52:33,160
i'm sure you all know about the model

837
00:52:33,170 --> 00:52:36,240
when xk to finite

838
00:52:36,550 --> 00:52:40,150
a finite markov process is finite

839
00:52:40,160 --> 00:52:41,680
but the market change

840
00:52:41,730 --> 00:52:43,590
in the case of being twice

841
00:52:43,610 --> 00:52:46,120
i'm interested in the case where particularly

842
00:52:46,140 --> 00:52:47,800
the next day

843
00:52:47,810 --> 00:52:53,680
if x is already but also in my opinion this kind of detail state space

844
00:52:53,720 --> 00:52:55,270
markov process

845
00:52:55,280 --> 00:52:57,940
it doesn't have to to be only value depending on back

846
00:52:57,950 --> 00:52:59,880
no problem

847
00:53:00,120 --> 00:53:07,000
so this is the notation i'm going to use new initial distribution f transition density

848
00:53:07,040 --> 00:53:09,070
although we also use

849
00:53:09,120 --> 00:53:12,920
the following matlab notation i'm sure you're all familiar with

850
00:53:13,200 --> 00:53:16,160
i j which is a collection of state

851
00:53:16,170 --> 00:53:18,780
x one phi i phi j

852
00:53:19,730 --> 00:53:25,560
on the other side information because fine but also the market process

853
00:53:25,650 --> 00:53:29,470
i know that if i look at the joint distribution

854
00:53:29,480 --> 00:53:36,580
the first newspaper is x one x two x then obviously i can't always we're

855
00:53:36,580 --> 00:53:40,460
right that it is you have explored the next the x one and p of

856
00:53:40,620 --> 00:53:46,040
y one on and so forth all because of the markov assumption i can also

857
00:53:46,040 --> 00:53:51,500
arise these things as initial distribution nu x one time to hold

858
00:53:51,520 --> 00:53:58,020
from two two and the transition from time from experiment one

859
00:53:58,030 --> 00:53:58,840
but that's it

860
00:53:58,850 --> 00:53:59,800
this is

861
00:53:59,810 --> 00:54:01,750
these things five

862
00:54:03,190 --> 00:54:06,880
the prior distribution as stated the state

863
00:54:06,900 --> 00:54:09,420
x one to x

864
00:54:12,240 --> 00:54:18,110
those market all sets i'll try to convince you that they are ubiquitous in science

865
00:54:18,110 --> 00:54:23,740
and applied science so very popular very popular application of the thing is i'm going

866
00:54:23,740 --> 00:54:27,090
to describe it in the case where you have the right to some walking

867
00:54:27,220 --> 00:54:29,280
OK so you interesting talking

868
00:54:29,300 --> 00:54:30,870
whatever object

869
00:54:30,880 --> 00:54:35,120
and so the x y plane are

870
00:54:35,170 --> 00:54:37,000
you tried to put

871
00:54:37,030 --> 00:54:38,590
modelled on

872
00:54:38,600 --> 00:54:45,110
on this process on the way simple fold our would duties consist of introducing

873
00:54:45,160 --> 00:54:49,420
o state x which correspond to say

874
00:54:49,470 --> 00:54:52,100
coordinates on the excess axes

875
00:54:52,120 --> 00:54:56,400
coordinates of the y axis is the city on the x axis is the one

876
00:54:59,140 --> 00:55:03,050
model to describe the evolution of the target in plane

877
00:55:03,060 --> 00:55:06,140
it the following basically

878
00:55:06,150 --> 00:55:08,260
constant velocity model

879
00:55:08,280 --> 00:55:13,100
all this is simply covering everything in the simple

880
00:55:13,100 --> 00:55:15,480
this is

881
00:55:15,490 --> 00:55:18,500
thank you

882
00:55:27,270 --> 00:55:34,360
the is just

883
00:55:43,810 --> 00:55:50,330
so the problem is that the rate at which

884
00:55:53,610 --> 00:55:57,930
you can do that in and

885
00:55:58,010 --> 00:55:59,180
it is

886
00:55:59,200 --> 00:56:02,690
the point

887
00:56:07,600 --> 00:56:08,540
this is

888
00:56:08,570 --> 00:56:12,470
this is the solution

889
00:56:12,480 --> 00:56:17,110
the point

890
00:56:20,870 --> 00:56:24,620
it also

891
00:56:24,640 --> 00:56:27,090
she is

892
00:56:32,760 --> 00:56:37,010
let's see what he

893
00:56:38,540 --> 00:56:40,190
it's the one

894
00:56:40,200 --> 00:56:42,950
in the case of

895
00:56:47,430 --> 00:56:49,820
and c

896
00:56:53,130 --> 00:56:53,980
so so

897
00:56:53,990 --> 00:56:55,870
this year

898
00:57:00,340 --> 00:57:02,350
eighty percent

899
00:57:04,690 --> 00:57:10,100
what he saw

900
00:57:10,140 --> 00:57:15,360
one of my vision

901
00:57:29,240 --> 00:57:30,980
the second approach

902
00:57:31,000 --> 00:57:35,950
you be able to give

903
00:57:50,720 --> 00:57:54,450
OK go

904
00:57:58,090 --> 00:58:01,320
that's what

905
00:58:02,760 --> 00:58:06,000
that means that all

906
00:58:26,310 --> 00:58:28,340
the show

907
00:58:35,290 --> 00:58:37,360
we have a

908
00:58:37,480 --> 00:58:39,050
one hundred years

909
00:58:39,250 --> 00:58:46,490
set c

910
00:58:56,450 --> 00:59:00,660
the national you to do

911
00:59:00,750 --> 00:59:09,440
born into this world

912
00:59:09,450 --> 00:59:10,570
in the image

913
00:59:10,630 --> 00:59:12,980
he said the school

914
00:59:35,810 --> 00:59:38,380
so this is all

915
00:59:38,540 --> 00:59:40,460
this is

916
00:59:53,780 --> 00:59:59,310
this is just the

917
01:00:01,330 --> 01:00:06,920
the next radio show

918
01:00:06,920 --> 01:00:09,060
the station could

919
01:00:10,950 --> 01:00:13,260
these are

920
01:00:35,040 --> 01:00:38,840
so i think that they should

921
01:00:38,890 --> 01:00:40,620
i call this

922
01:00:40,620 --> 01:00:41,690
all right it

923
01:00:41,880 --> 01:00:46,870
right so you would start again with now

924
01:00:46,890 --> 01:00:47,970
you come up

925
01:00:47,980 --> 01:00:52,130
with a similar expression alpha densities say the likelihood

926
01:00:52,140 --> 01:00:54,800
function for new

927
01:00:54,840 --> 01:01:01,180
and sigma squared would be again if i assume that the set of data

928
01:01:01,610 --> 01:01:05,960
x one to x is generated independently at random

929
01:01:05,980 --> 01:01:09,050
so i would form the likelihood

930
01:01:09,050 --> 01:01:11,230
of these data

931
01:01:12,760 --> 01:01:15,000
right then this would be

932
01:01:16,340 --> 01:01:21,330
the something it's clear two phi sigma squared

933
01:01:21,350 --> 01:01:23,390
e to the minus

934
01:01:24,390 --> 01:01:28,150
two sigma squared x minus

935
01:01:28,200 --> 01:01:30,380
square so this would be

936
01:01:30,390 --> 01:01:32,850
the likelihood

937
01:01:32,870 --> 01:01:35,700
and then was always

938
01:01:35,760 --> 01:01:41,550
so the people with specific background i always love to minimize things rather than maximise

939
01:01:41,550 --> 01:01:46,010
i think economists try to maximize sort of other people like i mean i have

940
01:01:46,010 --> 01:01:50,390
an education is a theoretical physicist like this is always kind of energy should be

941
01:01:50,390 --> 01:01:54,620
minimized things like that although we have entropy such that maximizes

942
01:01:54,620 --> 01:01:58,010
and it's a different kind of the risk should be minimized

943
01:01:58,090 --> 01:01:59,570
right so

944
01:01:59,640 --> 01:02:05,590
so often it's people put a minus log to the likelihood and you want to

945
01:02:05,590 --> 01:02:08,430
minimize that is just a matter of taste

946
01:02:08,630 --> 01:02:15,510
no big deal so just to the negative effect take the log of this you

947
01:02:15,510 --> 01:02:20,830
get the sum of logs and since i have an exponential here then the nice

948
01:02:20,830 --> 01:02:24,630
thing is exponential in the log of the exponential gets this

949
01:02:24,720 --> 01:02:28,820
o thing in the exponent down and we have this

950
01:02:28,840 --> 01:02:30,900
and here is also belong

951
01:02:31,110 --> 01:02:34,640
two pi sigma square and so this is what we have to minimize now

952
01:02:35,090 --> 01:02:39,670
we can do the same trick is before we differentiate for instance if you're interested

953
01:02:41,450 --> 01:02:43,230
the estimate from you

954
01:02:43,380 --> 01:02:45,710
well you have to do

955
01:02:45,880 --> 01:02:49,580
so first of all there's always the part that doesn't interest you because it does

956
01:02:49,580 --> 01:02:51,970
not depend on get it

957
01:02:51,980 --> 01:02:54,630
when you differentiate so you take the

958
01:02:54,640 --> 01:02:56,220
the mu

959
01:02:58,100 --> 01:02:59,650
i from one to n

960
01:03:01,050 --> 01:03:04,210
x mu squared

961
01:03:04,250 --> 01:03:07,740
two sigma square you want to set it equal to zero but even if you

962
01:03:08,480 --> 01:03:10,700
because you set it equal to zero

963
01:03:10,710 --> 01:03:12,720
you can even forget about that

964
01:03:12,730 --> 01:03:15,550
because this is just the multiplicative thing

965
01:03:15,550 --> 01:03:17,040
well let's let's

966
01:03:17,050 --> 01:03:18,700
i believe the two years

967
01:03:18,790 --> 01:03:21,190
so we know this there should be zero

968
01:03:21,240 --> 01:03:25,080
then you get to from here

969
01:03:25,090 --> 01:03:26,670
you get a

970
01:03:26,690 --> 01:03:29,390
x y minus mu

971
01:03:29,400 --> 01:03:35,600
so i want to n of the minus sign in this should be zero

972
01:03:35,600 --> 01:03:40,790
now you have two parts of this you have some over x y

973
01:03:40,800 --> 01:03:42,760
i from one to n

974
01:03:42,860 --> 01:03:45,410
and you have the somewhere you sum

975
01:03:45,440 --> 01:03:47,650
in times the factor mu

976
01:03:47,660 --> 01:03:51,940
so you get equals you get a plus n times mu

977
01:03:51,990 --> 01:03:54,880
and this is zero so yourself from you

978
01:03:54,920 --> 01:03:56,710
and you get

979
01:03:56,760 --> 01:04:01,540
again one over and you take all your data some them up divided by and

980
01:04:01,800 --> 01:04:05,030
you get the mean of the data well again why bother i mean it seems

981
01:04:05,030 --> 01:04:09,400
like the obvious answer everybody would do that but

982
01:04:09,420 --> 01:04:13,840
as i said there are other models were maximum likelihood looks different

983
01:04:14,050 --> 01:04:20,640
and especially here is a specific class of models for which maximum likelihood looks terribly

984
01:04:20,640 --> 01:04:25,600
simple this is the class of exponential families and they are considered to be the

985
01:04:25,600 --> 01:04:30,300
simple things these are the building blocks for more complicated

986
01:04:30,310 --> 01:04:34,510
so let's look at on that

987
01:04:34,530 --> 01:04:36,510
i haven't done the second part

988
01:04:36,570 --> 01:04:37,960
you can do the same

989
01:04:37,980 --> 01:04:41,000
o thing then for the variance in which you would get

990
01:04:41,140 --> 01:04:44,570
it is an estimate of the variance squared

991
01:04:44,570 --> 01:04:48,350
with the similar trick so you have to do the deed by d sigma squared

992
01:04:48,350 --> 01:04:51,790
and you can also estimate for the variance

993
01:04:52,780 --> 01:04:53,850
so now

994
01:04:54,890 --> 01:04:58,180
let's relate this maximum likelihood

995
01:04:58,290 --> 01:05:01,260
well maybe i should just freeze have

996
01:05:01,320 --> 01:05:03,590
i have a glass of water

997
01:05:05,440 --> 01:05:07,160
ask me questions if

998
01:05:09,000 --> 01:05:10,050
embarrassed me

999
01:05:10,050 --> 01:05:11,070
yes please

1000
01:05:11,190 --> 01:05:16,700
this is not always assume that

1001
01:05:16,710 --> 01:05:22,570
you know what is going to be yes yes it's your model u s four

1002
01:05:22,580 --> 01:05:24,780
yes yes

1003
01:05:24,900 --> 01:05:26,380
what if you don't

1004
01:05:26,400 --> 01:05:29,110
we have one

1005
01:05:31,140 --> 01:05:34,010
the goal of the model to come up with something

1006
01:05:34,030 --> 01:05:37,210
if you don't know anything i mean you can make it more vague and they

1007
01:05:37,210 --> 01:05:38,630
got will

1008
01:05:38,640 --> 01:05:44,340
talk about probably tomorrow about girls processes where you have a very wake kind of

1009
01:05:44,340 --> 01:05:48,400
idea what you functions that you try to model they are so you could actually

1010
01:05:48,400 --> 01:05:52,530
say well you know they are just kind of well actually said

1011
01:05:52,540 --> 01:05:57,810
i believe that my data come from some function you know this and x and

1012
01:05:57,810 --> 01:06:02,480
y is some function and i believe there actually well is a continuous function is

1013
01:06:02,480 --> 01:06:08,390
differentiable many times and well consider the input of free parameter tells me how we

1014
01:06:08,390 --> 01:06:12,310
can see this is so it's very wake thing you can put in

1015
01:06:12,320 --> 01:06:18,110
the models that are the nonparametric model is parametric models got one single parameter

1016
01:06:18,160 --> 01:06:19,320
in it or two

1017
01:06:19,320 --> 01:06:23,900
then you can think about models that have sort of a number of parameters that

1018
01:06:23,900 --> 01:06:24,970
actually grow

1019
01:06:25,080 --> 01:06:31,170
with the number of data so you must come up with more parameters after this

1020
01:06:31,170 --> 01:06:34,870
is what you would call a nonparametric semiparametric problem

1021
01:06:34,900 --> 01:06:36,730
so we can do that as well

1022
01:06:36,750 --> 01:06:41,700
well there is probably one but

1023
01:06:47,900 --> 01:06:53,370
OK i can look it up if you want but OK well first of all

1024
01:06:53,370 --> 01:06:57,150
it's a clear thing whether what i mean by a parametric model i mean the

1025
01:06:57,150 --> 01:07:01,900
number of parameters are completely independent of the number of the data here so i

1026
01:07:01,900 --> 01:07:03,320
just fixed my

1027
01:07:07,130 --> 01:07:08,860
well if you know nothing

1028
01:07:08,880 --> 01:07:11,570
and it's a bit hard you always have to come up with something because you're

1029
01:07:11,570 --> 01:07:12,850
kind of

1030
01:07:12,870 --> 01:07:16,960
you have some domain knowledge always and then you can come up with some ideas

1031
01:07:16,960 --> 01:07:19,780
that might fit

1032
01:07:19,860 --> 01:07:23,040
that might be applicable

1033
01:07:23,530 --> 01:07:24,860
so now

1034
01:07:24,860 --> 01:07:29,180
using this good old friend that you also know from before and also from the

1035
01:07:29,180 --> 01:07:32,890
from johns lecture linear regression

1036
01:07:32,910 --> 01:07:34,410
well you try to fit

1037
01:07:34,420 --> 01:07:37,980
straight line through the cloud of points

1038
01:07:37,990 --> 01:07:43,310
and now we can do that again as a probabilistic from a probabilistic model point

1039
01:07:43,310 --> 01:07:44,230
of view

1040
01:07:44,230 --> 01:07:47,650
of that allows invariance as a special case and so if you prefer one of

1041
01:07:47,650 --> 01:07:51,420
those special cases think in those terms

1042
01:07:51,480 --> 01:07:56,080
now that's half the problem is is the the rolls on the other hand the

1043
01:07:56,080 --> 01:07:57,080
problem is the

1044
01:07:57,100 --> 01:08:02,350
discriminant function i the classifier so we have this family classifiers line some family

1045
01:08:02,480 --> 01:08:06,690
big gamma and it's probably going to be a large families were not symmetric family

1046
01:08:06,710 --> 01:08:10,650
and the problem is to find a decision like in decision theoretic framework at the

1047
01:08:10,650 --> 01:08:14,830
node incision this is now is two parts is to choose the quantizer q

1048
01:08:14,830 --> 01:08:16,900
and the two the discriminant functions

1049
01:08:16,900 --> 01:08:20,350
that's the output is this this this this to people

1050
01:08:20,360 --> 01:08:23,380
i mean what our loss function well the risk is going to be the problem

1051
01:08:23,380 --> 01:08:24,350
is here

1052
01:08:24,350 --> 01:08:29,750
so the y the point the quantized discriminant function value was not equal to the

1053
01:08:29,810 --> 01:08:35,070
the correct label that that is the way of one zero one loss function i

1054
01:08:35,070 --> 01:08:39,500
take expectation like get the probability that an article so this is the risk function

1055
01:08:39,770 --> 01:08:43,860
as a function of q and gamma i it's different notation but it is it's

1056
01:08:43,860 --> 01:08:46,460
my decision theoretic framework we talked about earlier

1057
01:08:46,520 --> 01:08:49,250
OK so there are many applications for this

1058
01:08:49,250 --> 01:08:52,810
OK so if you look at the existing literature there sort of help on two

1059
01:08:52,810 --> 01:08:57,310
sides of the equation but not both of them simultaneously the classical signal press processing

1060
01:08:57,310 --> 01:09:02,880
literature define this problem decentralised detection and it assume everything is known except for the

1061
01:09:04,830 --> 01:09:09,770
everything means that all the problems which are known the class conditional probability distributions the

1062
01:09:09,770 --> 01:09:12,810
class problem prior probabilities and so on

1063
01:09:12,830 --> 01:09:15,540
although it's not known is q

1064
01:09:16,130 --> 01:09:18,210
and so how did you find q

1065
01:09:18,250 --> 01:09:20,880
nine symmetrical drawing do

1066
01:09:20,900 --> 01:09:24,150
slide here is the trying light over here

1067
01:09:26,040 --> 01:09:31,290
i didn't come on the idea that wrong

1068
01:09:31,290 --> 01:09:36,310
no i didn't work

1069
01:09:36,310 --> 01:09:42,790
so in original space there is the next phase over here had class one

1070
01:09:42,810 --> 01:09:44,960
and class zero

1071
01:09:45,020 --> 01:09:46,880
maybe looking like that

1072
01:09:46,880 --> 01:09:50,580
and it's hard to be discriminate boundary among these two things one might want to

1073
01:09:50,580 --> 01:09:53,810
use them as you go over to space the

1074
01:09:53,810 --> 01:09:57,060
which places as far apart as possible

1075
01:09:57,060 --> 01:09:59,440
right that would be a good choice of q

1076
01:09:59,460 --> 01:10:02,420
and if i did a bad choice of q it would smash them together even

1077
01:10:04,000 --> 01:10:06,690
right so all i can do is measured some ways the

1078
01:10:06,710 --> 01:10:09,190
divergence among probability distributions

1079
01:10:09,210 --> 01:10:11,630
and then optimize q expected divergence

1080
01:10:11,650 --> 01:10:13,790
what version should use

1081
01:10:13,790 --> 01:10:19,150
i will try to maximize divergence here not with minimum divergence is the maximum divergence

1082
01:10:19,920 --> 01:10:24,250
so these guys said well what are some divergences you can maximize and so they

1083
01:10:24,250 --> 01:10:28,520
wrote down lot of kind of functions functionals on probability distributions and they found some

1084
01:10:28,520 --> 01:10:31,750
of them were easy to maximize some were not and so they pick those and

1085
01:10:31,750 --> 01:10:35,790
that's what they did so hellinger but i have a whole bunch of others kind

1086
01:10:35,810 --> 01:10:40,940
came out of the literature and became famous in other fields and they were set

1087
01:10:40,940 --> 01:10:44,850
up because of this problem of divergence lots of radar has been done this way

1088
01:10:45,400 --> 01:10:50,690
big literature or people have just picked the divergence a hellinger chair for something and

1089
01:10:50,690 --> 01:10:55,120
then it's function properties you should be using those properties are known

1090
01:10:55,120 --> 01:10:59,130
so just write down the expected divergence maximize q and pop up that you back

1091
01:10:59,130 --> 01:11:02,040
to the user and so can you put that in your radar radar quantizes in

1092
01:11:02,040 --> 01:11:05,480
that way called signal set selection

1093
01:11:05,540 --> 01:11:09,730
all right so that's the story i would be this is basically heuristic literature to

1094
01:11:09,730 --> 01:11:12,880
to turn the lights off or i could control

1095
01:11:13,020 --> 01:11:15,940
so it's

1096
01:11:15,960 --> 01:11:17,790
with the what they do

1097
01:11:21,850 --> 01:11:28,690
alright so it's basically using a plugin and they're not really worry about how well

1098
01:11:28,690 --> 01:11:30,020
it performs you put it in

1099
01:11:30,040 --> 01:11:33,650
and you don't then try to evaluate how well that does

1100
01:11:33,670 --> 01:11:38,440
all so the systems unit and the china has its focus on problems where the

1101
01:11:38,440 --> 01:11:42,640
whole problem to find the discriminant function and not all about experimental design so you

1102
01:11:42,640 --> 01:11:47,210
assume that that's known you try to find that and the way it's done is

1103
01:11:47,210 --> 01:11:50,980
is that by the time the certain loss function to boost indiscretions for the machines

1104
01:11:50,980 --> 01:11:55,670
are all based on surrogate loss functions and this is the kind of more rigorous

1105
01:11:55,670 --> 01:11:58,750
as a decision theoretic flavour discusses the results and so on and so forth but

1106
01:11:58,750 --> 01:12:01,710
it really facing the whole problem which is to find the q and the gamma

1107
01:12:03,400 --> 01:12:07,690
OK so it's unbelievable machinery this talk about these f divergences these are the guys

1108
01:12:07,690 --> 01:12:11,920
that have been discussed by the signal processing literature course then appear many other solutions

1109
01:12:11,920 --> 01:12:12,730
as well

1110
01:12:12,750 --> 01:12:15,290
a kind of part of part of this this talk is going to be to

1111
01:12:15,310 --> 01:12:18,960
somehow unify these things is that is the list of things and in minor there

1112
01:12:18,960 --> 01:12:21,750
on the predicates are not inside but

1113
01:12:21,790 --> 01:12:24,560
the actual critical state

1114
01:12:24,560 --> 01:12:29,290
of the predicates is in the apart from these ones and if you a few

1115
01:12:29,290 --> 01:12:35,410
of his council along path

1116
01:12:36,390 --> 01:12:38,250
there are about one million

1117
01:12:38,250 --> 01:12:40,670
the solutions in open source

1118
01:12:40,670 --> 01:12:47,000
i believe and five hundred ninety six thousand two hundred fifteen of them is a

1119
01:12:47,040 --> 01:12:50,750
relationship so relating individuals

1120
01:12:50,770 --> 01:12:54,150
including individual predicates the types of things they are so

1121
01:12:54,670 --> 01:12:56,000
the predicate is

1122
01:12:56,020 --> 01:13:00,910
is predicted so that's one hundred ninety six thousand two hundred fifty

1123
01:13:00,920 --> 01:13:04,640
o five hundred ninety six thousand two hundred fourteen more

1124
01:13:04,690 --> 01:13:08,420
i guess is the predicate for example that accounts for another one

1125
01:13:09,640 --> 01:13:16,350
ninety nine almost a hundred thousand generalisation relationships so saying that

1126
01:13:18,620 --> 01:13:23,080
was saying that US president generalizes to

1127
01:13:23,100 --> 01:13:29,270
politicians for example is one of those two generals relations it's saying that

1128
01:13:30,270 --> 01:13:32,290
so i saying that city in

1129
01:13:32,310 --> 01:13:36,460
country from german germany generalizes to city

1130
01:13:36,540 --> 01:13:39,440
there's another one of those generals relationships so have

1131
01:13:39,550 --> 01:13:43,920
the relationships between functional terms and

1132
01:13:43,940 --> 01:13:49,480
not functional terms so these are all the type relationships to have instantiation relationship type

1133
01:13:49,480 --> 01:13:50,940
relations and

1134
01:13:51,060 --> 01:13:54,580
between those two we have used up about seven hundred thousand of them

1135
01:13:54,730 --> 01:13:56,730
the rest of them

1136
01:13:56,770 --> 01:13:58,460
destroying this sort

1137
01:13:58,460 --> 01:14:00,580
for example you know

1138
01:14:00,600 --> 01:14:03,370
city is a piece of fruit

1139
01:14:03,420 --> 01:14:07,850
probably that hasn't decision the knowledge base directly but it's the thing which allows you

1140
01:14:07,850 --> 01:14:13,600
to infer that from the is generally relationships and thus destroying this relationship is in

1141
01:14:15,020 --> 01:14:21,890
being able to prove that i could is not a material being for example from

1142
01:14:21,890 --> 01:14:27,620
the fact that he's a that he is a son and therefore a human being

1143
01:14:27,620 --> 01:14:32,480
and therefore a hominid therefore a primate and therefore

1144
01:14:32,480 --> 01:14:39,790
a mammal and therefore it so that all objects and therefore a something is still

1145
01:14:39,960 --> 01:14:47,100
in something as this thing is disjoint with the material things and all the way

1146
01:14:48,120 --> 01:14:54,580
all the way down point of this was the ethereal being just stating the distance

1147
01:14:54,580 --> 01:14:59,730
of the top that allows you to prove for example that buffer zone is somewhat

1148
01:14:59,730 --> 01:15:04,940
of an ethereal beings with marko certainly

1149
01:15:04,940 --> 01:15:10,790
with similar relationships we improve the markers that are present our trusty and the

1150
01:15:10,830 --> 01:15:14,600
one of the largest not peaceable on furniture and so on and so forth so

1151
01:15:14,600 --> 01:15:18,350
these sets are disjoint efforts relationships are extremely

1152
01:15:20,500 --> 01:15:24,940
computers improving the sorts of things that grow

1153
01:15:25,000 --> 01:15:27,150
people take for granted

1154
01:15:27,230 --> 01:15:34,480
the result in all for the functions for example the relation which is used to

1155
01:15:34,480 --> 01:15:39,830
say that city in country from germany produces the city

1156
01:15:39,830 --> 01:15:43,830
so what if you put in whatever country you put into it

1157
01:15:43,850 --> 01:15:46,420
you give a collection of cities back

1158
01:15:46,440 --> 01:15:52,410
and he said that using a relation but this is

1159
01:15:52,410 --> 01:15:57,890
this is sort of an overview of these cover most of the these cover most

1160
01:15:57,890 --> 01:16:01,290
of the sorts of things that you can get out of

1161
01:16:01,330 --> 01:16:06,960
open in full sight of the relationship to use more widely because we're in the

1162
01:16:07,190 --> 01:16:15,080
ricci stating facts rather than its nomination

1163
01:16:18,120 --> 01:16:19,210
and so on

1164
01:16:19,230 --> 01:16:25,210
cruising at to a few thousand meters by the knowledge base where sort of look

1165
01:16:25,230 --> 01:16:26,810
look into some areas of

1166
01:16:28,390 --> 01:16:35,500
there's a good deal of geospatial knowledge there

1167
01:16:35,540 --> 01:16:43,770
there are seventeen thousand terms related to cut the fourteen hundred genes related apology

1168
01:16:43,770 --> 01:16:50,600
a thousand translated topology so that's the top quality of parts

1169
01:16:52,520 --> 01:16:59,520
information about the geographical region bodies award the graphical for underwater plants

1170
01:16:59,540 --> 01:17:07,920
and their relationship to the relationship between those types of geospatial entities and other represent

1171
01:17:08,020 --> 01:17:13,620
for those participation in these such as the USGS representations

1172
01:17:14,730 --> 01:17:19,870
these representations for the sorts of things which happened in

1173
01:17:20,890 --> 01:17:23,440
so for example

1174
01:17:23,460 --> 01:17:33,980
then an ecological system has the boundary a resource a resource and synthesizer released resource

1175
01:17:33,980 --> 01:17:40,620
synthesis inside a closed system is done by something is a providerofmotiveforce for that resource

1176
01:17:40,620 --> 01:17:43,190
in size

1177
01:17:43,190 --> 01:17:48,840
again these sorts of things are represented as systems inside like the vocabulary for representing

1178
01:17:48,840 --> 01:17:51,140
them is available

1179
01:17:51,190 --> 01:17:56,020
open inside and you get to site you can see examples of how these this

1180
01:17:56,020 --> 01:17:59,960
representational vocabulary is used

1181
01:17:59,960 --> 01:18:01,730
because of ecosystems

1182
01:18:01,790 --> 01:18:09,170
credit lives to inspire ions is a consistent problem rainforest ecosystems

1183
01:18:09,190 --> 01:18:15,290
hundreds of years whatever they are graphs the larger there are lots of the terms

1184
01:18:15,290 --> 01:18:20,140
of human knowledge this cycle is much more about it than i do

1185
01:18:20,170 --> 01:18:22,560
it was produced by people who liked

1186
01:18:22,580 --> 01:18:28,000
representing obscure things sometimes it seems to be the worst scenes

1187
01:18:28,170 --> 01:18:29,870
terribly down

1188
01:18:36,850 --> 01:18:42,270
the ecosystem so is

1189
01:18:42,390 --> 01:18:47,960
line for prospective application but parts of it have been used for previous applications for

1190
01:18:47,960 --> 01:18:56,980
example in these areas things like mediterranean skype between training scrub for example is used

1191
01:18:56,980 --> 01:19:01,980
for military applications to work things like traffic ability so the type of the type

1192
01:19:02,000 --> 01:19:05,960
of ecosystem that's in the region and as you know what sort of vehicles can

1193
01:19:05,960 --> 01:19:07,250
pass through

1194
01:19:11,500 --> 01:19:16,670
so again these are some examples of how to use the book relates things together

1195
01:19:16,940 --> 01:19:22,270
so the climate of h temporal ecosystem

1196
01:19:22,350 --> 01:19:28,750
a mediterranean climate type and the training of the climate is mediterranean scrub

1197
01:19:28,750 --> 01:19:32,190
and military discovers the type of true

1198
01:19:32,210 --> 01:19:37,060
geographical region and the territory of crab creek greece

1199
01:19:37,080 --> 01:19:41,770
i had the time high mediterranean climate cycle so these

1200
01:19:41,810 --> 01:19:46,670
the things on the predicates existing programs you can just find look inside and you

1201
01:19:46,670 --> 01:19:52,620
use them to quit represent the relationships between the

1202
01:19:52,620 --> 01:19:53,980
the land

1203
01:19:54,620 --> 01:19:57,850
violence the type of vegetation

1204
01:19:57,890 --> 01:20:03,420
the location and ecological system you have to start from scratch during this if you

1205
01:20:03,420 --> 01:20:08,370
didn't use for purposes but this would have the terms of existing is very useful

1206
01:20:08,370 --> 01:20:16,810
to have them all really connected the background knowledge available background knowledge

1207
01:20:19,000 --> 01:20:23,910
talking about transportation events we have hundreds of

1208
01:20:23,960 --> 01:20:26,670
for hundreds of

1209
01:20:26,670 --> 01:20:33,100
first of all possible lecture this you should consider going back to current just to

1210
01:20:33,110 --> 01:20:34,360
seriously the

1211
01:20:34,380 --> 01:20:37,830
in the early nineteen eighties and became part of the power law distributed

1212
01:20:39,060 --> 01:20:40,230
research group

1213
01:20:40,240 --> 01:20:44,260
he co-inventor boltzmann machines turns nasty in nineteen eighty three

1214
01:20:44,280 --> 01:20:48,760
members of the researchers who introduced back propagation algorithm

1215
01:20:48,780 --> 01:20:53,950
the training of new networks books in nineteen eighty six to an explosion in the

1216
01:20:53,950 --> 01:21:00,230
growth neural networks and actually the nips conference started in nineteen eighty seven american way

1217
01:21:00,330 --> 01:21:02,300
excitement around books

1218
01:21:02,320 --> 01:21:07,550
jeff has received many awards including the fellowship of royal society in nineteen ninety eight

1219
01:21:07,550 --> 01:21:10,410
the data david run in two thousand one

1220
01:21:10,470 --> 01:21:14,580
and this guy award for research excellence in two thousand five

1221
01:21:14,580 --> 01:21:17,720
since nineteen eighty just continue to be high

1222
01:21:17,870 --> 01:21:20,830
contribution such as variation learning

1223
01:21:20,830 --> 01:21:26,900
how does machine politics expert architecture most recently he has been

1224
01:21:27,010 --> 01:21:31,870
working on deep belief networks for unsupervised learning and they be

1225
01:21:31,890 --> 01:21:36,510
talking about deep learning multiplicative interactions thank you

1226
01:21:36,530 --> 01:21:37,640
thank you

1227
01:21:37,890 --> 01:21:47,890
so today i'm going to spend the first ten minutes also give you background about

1228
01:21:47,920 --> 01:21:51,200
the learning and for many of you know this stuff but i want to give

1229
01:21:51,200 --> 01:21:56,200
the general background and then i'll show you one recent example where the ideology of

1230
01:21:56,200 --> 01:21:59,760
deep learning worked perfectly

1231
01:21:59,780 --> 01:22:01,750
and then going to talk about how to make

1232
01:22:01,780 --> 01:22:08,450
the basic learning module works better by introducing multiplicative interactions into a restricted boltzmann machine

1233
01:22:08,560 --> 01:22:12,450
i must say why we need to do that and how we keep the number

1234
01:22:12,450 --> 01:22:14,790
of parameters under control

1235
01:22:14,810 --> 01:22:17,400
and show you how the inference and learning is still

1236
01:22:17,420 --> 01:22:19,550
structure relatively straightforward

1237
01:22:19,560 --> 01:22:22,220
and then describes three examples

1238
01:22:22,230 --> 01:22:24,890
from student in my lab using the

1239
01:22:24,890 --> 01:22:27,340
multiplicative interactions

1240
01:22:27,340 --> 01:22:31,510
one is learning transform images that is learning and just change over time

1241
01:22:31,530 --> 01:22:36,390
another is having a model of human motion and using style able to transform models

1242
01:22:36,390 --> 01:22:39,880
that can come in many different motions and the last is

1243
01:22:39,890 --> 01:22:46,710
modeling the covariance structure of images so that you can do better discrimination

1244
01:22:46,730 --> 01:22:50,800
so restricted boltzmann machines consists of two layers of

1245
01:22:50,880 --> 01:22:54,230
you're like units binary stochastic units

1246
01:22:54,260 --> 01:22:56,730
and there's no connections between hidden units

1247
01:22:56,730 --> 01:22:59,270
and also no connection between visible units

1248
01:22:59,290 --> 01:23:03,610
given the state of the visible units which is right for the data the hidden

1249
01:23:03,610 --> 01:23:07,040
units are conditionally independent so it's very easy to do inference in this kind of

1250
01:23:07,040 --> 01:23:09,130
machine and that's good news

1251
01:23:09,140 --> 01:23:14,040
the rule for turning on the hidden unit is simply you compute

1252
01:23:14,980 --> 01:23:18,790
input is getting from the visible units put through the logistic function and that tells

1253
01:23:18,790 --> 01:23:22,360
you the probability turning on and then make a stochastic decision about whether it should

1254
01:23:22,360 --> 01:23:28,430
be an independent stochastic decisions rolling units

1255
01:23:28,450 --> 01:23:31,450
this network obeys a hopfield energy function

1256
01:23:31,480 --> 01:23:34,980
where the energy if i tell you the states of the visible and hidden units

1257
01:23:35,070 --> 01:23:36,860
the energy of the network

1258
01:23:36,890 --> 01:23:38,800
is quadratic in the states

1259
01:23:38,830 --> 01:23:43,850
for all pairs of the visible and units both of you take the weight between

1260
01:23:43,860 --> 01:23:46,040
you add up all those weights

1261
01:23:46,050 --> 01:23:47,910
and that tells you

1262
01:23:48,760 --> 01:23:51,860
negative energy low energies good

1263
01:23:52,420 --> 01:23:55,420
the derivative of the energy function is nice and simple

1264
01:23:55,430 --> 01:23:59,930
it's just the product of the visible in activity that's the inspector way to achieve

1265
01:24:01,450 --> 01:24:08,510
you can use those energies to define probabilities over joint configurations invisible units

1266
01:24:08,540 --> 01:24:11,870
and the probability of a joint configuration is proportional to e to the minus its

1267
01:24:12,640 --> 01:24:17,110
but that's normalized by the probabilities of all possible configurations is called the partition function

1268
01:24:17,270 --> 01:24:18,890
makes learning tricky

1269
01:24:18,890 --> 01:24:22,420
to know the probability that you assign to visible vector v

1270
01:24:22,430 --> 01:24:26,200
you have to add up over all possible states of the hidden units

1271
01:24:26,200 --> 01:24:28,890
possible one

1272
01:24:28,890 --> 01:24:33,710
the maximum likelihood learning algorithm for restricted boltzmann machine is conceptually very simple

1273
01:24:33,730 --> 01:24:35,420
but takes a long time

1274
01:24:35,430 --> 01:24:38,910
you start with data on the visible units at time t zero

1275
01:24:38,920 --> 01:24:43,050
and then using whatever what you currently have you the hidden units

1276
01:24:44,380 --> 01:24:47,350
and then if you to do it right you then activate the visible units again

1277
01:24:48,670 --> 01:24:50,860
and you go backwards and forwards

1278
01:24:50,880 --> 01:24:55,290
this is the markov chain to alternating blocked gibbs sampling and when you run long

1279
01:24:55,290 --> 01:24:59,810
enough you forgot where you started in my sampling from the visible path is the

1280
01:24:59,810 --> 01:25:05,030
model believes in and for those visible patterns that distribution model beliefs you compute the

1281
01:25:05,030 --> 01:25:09,480
pairwise statistics you compute how often a pixel of tag together

1282
01:25:09,500 --> 01:25:12,050
the learning rule is just the difference between

1283
01:25:12,070 --> 01:25:14,440
the pairwise statistics with data

1284
01:25:14,460 --> 01:25:16,770
and we fantasies from the model

1285
01:25:16,790 --> 01:25:20,530
unfortunately had to run the markov chain for a long time

1286
01:25:21,380 --> 01:25:24,800
i discovered empirically that if you are markov chain for a short time still works

1287
01:25:24,800 --> 01:25:26,990
pretty well and it's much quicker

1288
01:25:27,070 --> 01:25:30,740
so what we're going to do is just activate hidden units

1289
01:25:30,750 --> 01:25:35,680
a lot of the hidden units to reconstruct the pattern on the visible units was

1290
01:25:35,680 --> 01:25:37,420
to be going be quite like the data

1291
01:25:37,440 --> 01:25:39,220
then activate in units again

1292
01:25:39,230 --> 01:25:43,310
and the difference of those two pairwise statistics will tell you how to change awake

1293
01:25:43,340 --> 01:25:47,570
and then we'll just do stochastic gradient descent to try and get a good set

1294
01:25:47,570 --> 01:25:50,060
of words

1295
01:25:50,180 --> 01:25:53,230
the main reason restricted boltzmann which is interesting

1296
01:25:53,240 --> 01:25:57,760
it's because you can use it recursively to learn deep that it turns out that

1297
01:25:57,760 --> 01:26:01,530
the the you end up learning is a directed network which is a big surprise

1298
01:26:01,530 --> 01:26:05,980
people in graphical models and your learning directed one layer at a time

1299
01:26:06,030 --> 01:26:08,300
i'm not going to go into that any further

1300
01:26:08,320 --> 01:26:09,810
i'll just give you the sort of

1301
01:26:09,840 --> 01:26:11,230
how to use it

1302
01:26:11,350 --> 01:26:15,400
you train one layer of features and then treat the activations of those features is

1303
01:26:16,170 --> 01:26:17,500
new train another

1304
01:26:17,520 --> 01:26:21,540
you just keep going like that for many less you can stand

1305
01:26:21,550 --> 01:26:26,210
and when he finished the top two layers are restricted boltzmann machines and the system

1306
01:26:26,210 --> 01:26:31,460
memory and all those below can be thought of as a directed graphical model director

1307
01:26:31,470 --> 01:26:36,170
so that you can generate data from the system memory via some transformations on the

1308
01:26:36,170 --> 01:26:38,410
other hand this

1309
01:26:38,420 --> 01:26:41,600
i would like to prove that every time you add another here unless you get

1310
01:26:41,600 --> 01:26:43,490
a better model of the data

1311
01:26:43,520 --> 01:26:47,020
we can't quite prove that we can almost prove every time you add another left

1312
01:26:47,050 --> 01:26:51,550
if you do it right you improve the variational bound on the log probability of

1313
01:26:51,550 --> 01:26:54,220
the model generates the training data

1314
01:26:54,230 --> 01:26:58,000
that's good enough to show you doing something sensible

1315
01:26:58,030 --> 01:27:00,540
after you've learned multiple less

1316
01:27:00,550 --> 01:27:04,030
you can then sticks label units on top

1317
01:27:04,040 --> 01:27:07,030
and do backpropagation

1318
01:27:07,050 --> 01:27:12,860
now back propagation is now not having to design features of the features already designed

1319
01:27:12,860 --> 01:27:15,670
by unsupervised learning the model is boltzmann machines

1320
01:27:15,680 --> 01:27:21,240
all back propagation is doing is slightly changing the way it hardly changes the tall

1321
01:27:21,290 --> 01:27:25,550
so the boundaries between classes are in the right places

1322
01:27:25,570 --> 01:27:28,720
so it's slightly adjusting the features to be better for discrimination but it's not having

1323
01:27:28,720 --> 01:27:30,210
to discover the features

1324
01:27:30,220 --> 01:27:34,630
so the unsupervised learning gets into lot of white space and then the

1325
01:27:34,650 --> 01:27:37,750
back propagation within a group of white space

1326
01:27:37,780 --> 01:27:42,290
does local myopic steepest descent to tune the weights discrimination

1327
01:27:42,300 --> 01:27:44,480
experiments in bengio slide show that

1328
01:27:44,490 --> 01:27:49,720
this wins in two ways the unsupervised learning get richer better part of white space

1329
01:27:49,840 --> 01:27:50,680
the new would

1330
01:27:50,690 --> 01:27:53,470
i've got to if you just in supervised learning

1331
01:27:53,480 --> 01:27:56,400
and you actually

1332
01:27:56,410 --> 01:28:00,730
then get better generalisation as well so you winning both on the optimisation on speed

1333
01:28:00,730 --> 01:28:03,680
and the generalization

1334
01:28:03,730 --> 01:28:06,210
this is probably the most important slide in the talk

1335
01:28:06,360 --> 01:28:10,300
because it's sort of challenge is what most people in machine learning and is used

1336
01:28:10,300 --> 01:28:12,160
to believe

1337
01:28:13,050 --> 01:28:16,180
it uses the technical concept that i got from

1338
01:28:16,190 --> 01:28:19,230
donald rumsfeld called stuff

1339
01:28:19,250 --> 01:28:22,880
and so the way the world works is the stuff that happens

1340
01:28:22,900 --> 01:28:25,120
and stuff produces images

1341
01:28:25,120 --> 01:28:28,230
or tall it won't change amount of water you have

1342
01:28:28,240 --> 01:28:31,040
if you take a bunch of pennies and you spread them out you don't get

1343
01:28:31,040 --> 01:28:32,750
more planets

1344
01:28:32,800 --> 01:28:35,960
but kids according to actually don't know that

1345
01:28:36,010 --> 01:28:39,860
and this is one of the real cool demonstrations any of you have access to

1346
01:28:39,860 --> 01:28:45,490
all four five year-old a sibling something not take one without permission but if you

1347
01:28:45,490 --> 01:28:48,860
have access to four five you can do this is this is what it looks

1348
01:28:49,640 --> 01:28:52,410
the first one has no sound the second one is going to be sent come

1349
01:28:52,410 --> 01:28:53,920
on it and

1350
01:28:53,940 --> 01:28:59,090
but there's two rows of checkers yes ticket

1351
01:28:59,110 --> 01:29:01,640
which one has more

1352
01:29:01,650 --> 01:29:03,860
users are saying that you have not yet

1353
01:29:03,910 --> 01:29:07,840
which one has more

1354
01:29:07,860 --> 01:29:09,910
now that

1355
01:29:09,930 --> 01:29:12,550
so that's really stupid

1356
01:29:13,980 --> 01:29:20,640
and it's amazing amazing finding his older than is the robust finding here's another example

1357
01:29:21,650 --> 01:29:22,990
the same

1358
01:29:23,130 --> 01:29:30,410
and the

1359
01:29:30,460 --> 01:29:35,930
so it's cool finding of that stage suggesting a limitation in how you deal and

1360
01:29:35,930 --> 01:29:38,360
make sense of the world

1361
01:29:38,400 --> 01:29:43,260
the next phase concrete operations from seven to twelve you can solve the conservation

1362
01:29:44,610 --> 01:29:45,880
but still

1363
01:29:45,890 --> 01:29:50,190
you're limited the extent you're capable of abstract reasoning

1364
01:29:50,910 --> 01:29:53,430
the mathematical notions of infinity

1365
01:29:53,450 --> 01:29:58,260
or logical notions like logical entailment are beyond the child of this age

1366
01:29:58,270 --> 01:30:02,080
the child is able to do a lot but still is to some extent stock

1367
01:30:02,080 --> 01:30:04,650
in the company world

1368
01:30:04,660 --> 01:30:07,820
and then finally around age twelve

1369
01:30:07,840 --> 01:30:11,360
you can get abstract and scientific reasoning

1370
01:30:11,380 --> 01:30:13,720
and this is the activity theory

1371
01:30:13,730 --> 01:30:15,760
in very brief forms

1372
01:30:17,350 --> 01:30:18,930
i j

1373
01:30:18,940 --> 01:30:20,610
fearing a lot better

1374
01:30:20,630 --> 01:30:23,050
then did freud or scanners

1375
01:30:23,070 --> 01:30:25,120
for several reasons

1376
01:30:25,140 --> 01:30:26,570
one reason is

1377
01:30:26,580 --> 01:30:32,090
these are interesting and falsifiable claims about child development so claims that

1378
01:30:32,140 --> 01:30:34,280
about failure conservation

1379
01:30:34,300 --> 01:30:35,490
in children

1380
01:30:35,500 --> 01:30:40,000
a different ages could be easily tested systematically tested and in fact there's a lot

1381
01:30:40,000 --> 01:30:41,910
of support for the

1382
01:30:41,930 --> 01:30:44,690
we actually had a rich theoretical framework

1383
01:30:44,740 --> 01:30:49,520
pulling together all sorts of observations in different ways wrote many many books and articles

1384
01:30:49,520 --> 01:30:52,010
and articulators theory very richly

1385
01:30:52,090 --> 01:30:55,020
and most of all i think he has some really striking findings

1386
01:30:55,090 --> 01:30:58,790
before we actually nobody noticed these conservation findings

1387
01:30:58,810 --> 01:31:05,000
before we actually nobody noticed the babies had this problem tracking in understanding objects

1388
01:31:05,010 --> 01:31:07,110
at the same time however

1389
01:31:07,160 --> 01:31:12,880
there are limitations in is theory some of these limitations are theoretical

1390
01:31:13,310 --> 01:31:18,490
it's an interesting question as to whether he really explains how the trial goes from

1391
01:31:18,520 --> 01:31:21,230
concrete thinker to an abstract thing

1392
01:31:21,240 --> 01:31:26,910
or how it goes from not having object permanence one derstanding object permanence

1393
01:31:26,990 --> 01:31:31,640
his methodological limitations he actually was really begin to question and answer

1394
01:31:31,680 --> 01:31:35,530
one problem with this is that children are very good language

1395
01:31:35,550 --> 01:31:39,650
and this may be due to underestimate how much they know

1396
01:31:39,700 --> 01:31:42,980
and this is particularly a problem the younger you get

1397
01:31:43,360 --> 01:31:46,590
methodology is going to whom heavy

1398
01:31:46,600 --> 01:31:51,740
in discussion of any science and that includes psychology often

1399
01:31:51,750 --> 01:31:55,690
ninety percent of the game is discovering clever method

1400
01:31:55,700 --> 01:31:57,220
through which to test

1401
01:31:57,240 --> 01:31:59,240
you're hypotheses

1402
01:31:59,250 --> 01:32:02,510
i want to talk a little bit about that regarding babies i'll give you another

1403
01:32:02,510 --> 01:32:05,480
example of a very different domain

1404
01:32:05,490 --> 01:32:09,100
there is a set of scientists interested in studying typically

1405
01:32:09,110 --> 01:32:13,880
so when you tickle somebody under what circumstances will a lack where you have to

1406
01:32:13,880 --> 01:32:17,660
take all look can you take yourself doesn't have to be a surprise and so

1407
01:32:18,390 --> 01:32:21,380
it turns out very difficult to study this in the lab

1408
01:32:21,390 --> 01:32:27,420
you know the experimental credit you come to let OK undergraduate students and

1409
01:32:27,430 --> 01:32:32,110
and in fact an example of a mythological attempt was done by henry like university

1410
01:32:32,110 --> 01:32:34,710
of pennsylvania who built the typical machine

1411
01:32:34,730 --> 01:32:39,200
which was this box with these two giant hands the one and

1412
01:32:39,220 --> 01:32:43,180
and then it was a failure because people could not go near the typical machine

1413
01:32:43,180 --> 01:32:45,540
vote involving laughter

1414
01:32:45,560 --> 01:32:53,960
but we will discuss when we have electron laughter a bit of the technical sciences

1415
01:32:54,010 --> 01:32:55,520
and finally factual

1416
01:32:55,540 --> 01:32:57,790
what do infants and children really now

1417
01:32:57,800 --> 01:33:05,190
it's possible that due to the methodological limitations of phd he systematically underestimated

1418
01:33:05,270 --> 01:33:06,600
what children

1419
01:33:06,650 --> 01:33:10,060
and babies no and in fact i will present some evidence suggesting that this is

1420
01:33:10,060 --> 01:33:12,440
the fact that this is the case

1421
01:33:14,400 --> 01:33:19,120
i want to introduce you to the modern science of infant cognition

1422
01:33:19,130 --> 01:33:22,880
in the cognition has been something studied for a very long time

1423
01:33:22,900 --> 01:33:25,260
and there's a certain view

1424
01:33:25,270 --> 01:33:27,020
this had behind it

1425
01:33:27,110 --> 01:33:34,890
a tremendous philosophical and psychological consensus and is summarized in this onion headline here

1426
01:33:34,900 --> 01:33:36,380
and ideas

1427
01:33:36,410 --> 01:33:38,720
that that babies are stupid

1428
01:33:38,770 --> 01:33:42,570
the babies really don't know much about the world

1429
01:33:43,490 --> 01:33:48,070
the work that this new headliners satirizing is the recent studies which i want to

1430
01:33:48,070 --> 01:33:51,160
talk about suggest the contrary

1431
01:33:51,210 --> 01:33:53,620
babies might be smarter than you think

1432
01:33:54,790 --> 01:34:00,660
and to discover the intelligence of babies we have ourselves be pretty smart in developing

1433
01:34:00,660 --> 01:34:06,350
different techniques to study what baby knows you can ask you questions babies can talk

1434
01:34:06,660 --> 01:34:11,330
you could look at what it does with babies are not very coordinated or skills

1435
01:34:11,400 --> 01:34:13,620
CD is clever methods

1436
01:34:13,670 --> 01:34:15,210
one clever method is

1437
01:34:15,220 --> 01:34:17,600
to look at their brain waves

1438
01:34:19,740 --> 01:34:25,170
this trial underwrite died during testing is tragic crushed by the weights of electrodes he's

1439
01:34:26,080 --> 01:34:29,210
he studied brainwaves

1440
01:34:29,230 --> 01:34:33,280
one of the few things babies can do is they get stuck in the past

1441
01:34:33,290 --> 01:34:36,880
you might think well you know how we learn anything from that well for instance

1442
01:34:36,920 --> 01:34:41,650
you could build machines that would be bizarre kind of passive fire they hear music

1443
01:34:41,690 --> 01:34:43,120
or they hear language

1444
01:34:43,140 --> 01:34:47,300
and then you could look at how much they such passive fire to determine what

1445
01:34:47,300 --> 01:34:48,880
they like

1446
01:34:50,420 --> 01:34:55,820
undeniably we know most of our not we got most of our knowledge about babies

1447
01:34:55,820 --> 01:34:58,350
from studies of the looking times

1448
01:34:58,680 --> 01:35:02,790
that's one thing babies can do they can work

1449
01:35:02,810 --> 01:35:05,480
and i have a picture of

1450
01:35:05,490 --> 01:35:06,880
elizabeth spelke e

1451
01:35:07,010 --> 01:35:12,580
the developmental psychologist who developed the most research on looking at babies looking times and

1452
01:35:12,580 --> 01:35:14,220
what you can learn from them

1453
01:35:14,240 --> 01:35:17,590
and i had no two ways you can learn from looking

1454
01:35:17,610 --> 01:35:19,500
one is preference

1455
01:35:19,520 --> 01:35:21,490
so for instance suppose you want to know

1456
01:35:21,510 --> 01:35:25,950
for whatever reason the babies like the looks of dogs or cats

1457
01:35:26,710 --> 01:35:28,530
you could put a baby now

1458
01:35:28,540 --> 01:35:32,130
i have a picture of the dog here a picture of a cat here

1459
01:35:32,150 --> 01:35:34,290
and see which one the baby looks at

1460
01:35:34,300 --> 01:35:36,240
babies can move their eyes

1461
01:35:36,250 --> 01:35:40,080
and that tell you something two babies distinguish pretty faces

1462
01:35:40,130 --> 01:35:41,460
from ugly faces

1463
01:35:41,480 --> 01:35:45,560
well put a pretty face here an ugly face here see if the baby first

1464
01:35:45,560 --> 01:35:50,290
looking at alternative dataset exactly being a frequentist from mythological point of view now that

1465
01:35:50,290 --> 01:35:54,210
so much analysis point of view there's analysis to show you that procedure has been

1466
01:35:54,210 --> 01:35:55,830
frequentist properties itself

1467
01:35:55,830 --> 01:35:58,580
that's very interesting broadcaster techniques

1468
01:35:58,630 --> 01:36:00,630
OK so

1469
01:36:00,690 --> 01:36:05,730
i think one more slide on an introduction to move on to more concrete stuff

1470
01:36:05,770 --> 01:36:08,540
so what do you do with the frequencies what kind of activities you do well

1471
01:36:08,540 --> 01:36:12,130
you also write down models to develop procedures that the more you know the kind

1472
01:36:12,130 --> 01:36:16,690
of the analysis side of the story is a kind of the hierarchy of mathematical

1473
01:36:16,690 --> 01:36:20,310
things you do first of all you may try to prove consistency that if there

1474
01:36:20,310 --> 01:36:25,880
is the correct answer your conversion that no matter what the correct answer was that

1475
01:36:25,900 --> 01:36:30,710
often kind of fairly straightforward and not that informative more important thing to do is

1476
01:36:30,710 --> 01:36:36,080
to get rates of convergence towards the rate of convergence o two procedures both good

1477
01:36:36,150 --> 01:36:39,710
both consistent but maybe one of them has a faster convergence rate in terms of

1478
01:36:39,710 --> 01:36:41,040
number of data points

1479
01:36:41,650 --> 01:36:45,850
i would prefer that procedure what a lot of work and that is all that

1480
01:36:45,850 --> 01:36:50,380
and more hard but also very important is to try to get sampling distribution that

1481
01:36:50,790 --> 01:36:54,900
as the number of data points slow the perhaps i converted some nice distribution like

1482
01:36:54,900 --> 01:36:58,230
oh or almost like that i could use that distribution

1483
01:36:58,230 --> 01:37:00,480
to it to give me error bars

1484
01:37:00,480 --> 01:37:03,900
i can get error bars by finding out the sampling distribution

1485
01:37:03,920 --> 01:37:08,040
OK so there is certainly work on consistency in an animal military someone rates there's

1486
01:37:08,040 --> 01:37:14,520
very little companies nations so classical frequentist statistics focus on parametric statistics you know in

1487
01:37:14,520 --> 01:37:19,270
the forties and fifties but since then mainly nonparametric really there's a lot of nonparametric

1488
01:37:19,270 --> 01:37:23,210
testing and there's tons of other kind of person function estimation in all these large

1489
01:37:23,210 --> 01:37:26,600
p small n problems where is going to infinity you know the

1490
01:37:26,960 --> 01:37:31,290
number parameters going to infinity itself the number of data points in of the old

1491
01:37:31,290 --> 01:37:35,830
city also classical statistics with parametric and so on but that's not the tools were

1492
01:37:35,830 --> 01:37:40,870
developed to be general nonparametric perform part of the story one of the most general

1493
01:37:40,870 --> 01:37:46,100
tools is empirical process theory empirical process theory talks about convergence of

1494
01:37:46,480 --> 01:37:53,960
objects uniformly as defined consistency reasons which uniform various basis function spaces parameter spaces measure

1495
01:37:53,960 --> 01:37:57,870
spaces and so on and so forth to statistical learning theory is really a part

1496
01:37:57,870 --> 01:38:02,520
of that it's it's it's a particular area of empirical process theory that focuses on

1497
01:38:02,520 --> 01:38:04,000
zero one loss

1498
01:38:04,040 --> 01:38:09,270
but it's the tools rademacher and all that were developed in empirical process theory all

1499
01:38:09,270 --> 01:38:14,130
books and this appears to vary this tools available used prove things about the bootstrap

1500
01:38:14,250 --> 01:38:18,600
two things improved m estimators and so on so a lot of three thousand uses

1501
01:38:18,600 --> 01:38:22,650
this big heavy tool and there are lots of other tools that are not simpler

1502
01:38:22,740 --> 01:38:25,770
but that was all that was available

1503
01:38:25,810 --> 01:38:30,250
OK i'm going take a little positive only for so many questions and discover stretching

1504
01:38:31,130 --> 01:38:34,750
and the rest of my presentation today in the next time you are going to

1505
01:38:34,750 --> 01:38:38,870
be some little vignettes on the research that i've been involved in this all frequentist

1506
01:38:38,870 --> 01:38:42,520
and try to give you better flavour what requires actually really is like what the

1507
01:38:42,520 --> 01:38:46,410
kind of problems are set them up and see that there are some challenges there

1508
01:38:46,410 --> 01:38:47,670
and see how overcome them

1509
01:38:47,710 --> 01:38:51,900
the kind of machine learning methods but then analyse frequent point of view chapter house

1510
01:38:52,000 --> 01:38:53,540
all the way through the and

1511
01:38:53,560 --> 01:38:56,670
so i think i probably in the rest of this talk a talk about experimental

1512
01:38:56,670 --> 01:39:01,370
design and then these things three things will be for the next presentation so questions

1513
01:39:01,370 --> 01:39:05,080
answered philosophical stuff first

1514
01:39:13,670 --> 01:39:19,980
yeah i mean one of my current favourite books just kind of statistics in general

1515
01:39:19,980 --> 01:39:23,210
is and vanderplaats asymptotic statistics

1516
01:39:23,210 --> 01:39:28,850
and take some of the catholic view has got bayesian and frequentist arguments throughout it

1517
01:39:28,850 --> 01:39:31,130
by more frequent is overall but it's

1518
01:39:31,150 --> 01:39:33,880
it's it's got bayesian there as well

1519
01:39:34,380 --> 01:39:38,790
jim berger if you don't have you not minister james berger yet you should be

1520
01:39:38,810 --> 01:39:43,380
he's got a great book on statistical decision theory the first edition of it was

1521
01:39:43,380 --> 01:39:47,940
frequencies the second issues bayesian it's kind of good read both of them

1522
01:39:47,960 --> 01:39:53,810
and he anyway the second is as well as a lot of merging frequentist and

1523
01:39:53,810 --> 01:39:55,600
bayesian ideas and

1524
01:39:55,630 --> 01:39:59,210
i i i just think reading his book and his papers in journals is a

1525
01:39:59,210 --> 01:40:02,710
very good educational experience

1526
01:40:03,710 --> 01:40:12,920
is objective to be also subjected well you know sure in the sense that

1527
01:40:12,980 --> 01:40:17,250
i have written to be complicated model and some of the parameters i'm going to

1528
01:40:17,250 --> 01:40:20,650
possibly able to elicit subjectively and there's a whole bunch of others that are often

1529
01:40:20,650 --> 01:40:25,500
called nuisance parameters or whatever that don't want to or can't was should actively try

1530
01:40:25,560 --> 01:40:27,520
is subjective bayesian methods for those

1531
01:40:27,540 --> 01:40:32,250
so you know most bayesians do this actually in real life they will sit down

1532
01:40:32,250 --> 01:40:35,290
and say something this problem sort of believers in this range for this for that

1533
01:40:35,290 --> 01:40:38,130
reason and there's a scale factor i have no idea what should be able to

1534
01:40:38,130 --> 01:40:39,290
put the jeffreys prior

1535
01:40:39,370 --> 01:40:42,690
now that's coming out hoping to do a lot about how but that's kind of

1536
01:40:42,690 --> 01:40:46,100
your life fact blending objective basis

1537
01:40:46,110 --> 01:40:48,790
so i think objective bayes kind of big

1538
01:40:48,790 --> 01:40:55,920
ten to incorporate your subjective as well so that idea

1539
01:40:55,940 --> 01:41:04,420
five yes

1540
01:41:04,560 --> 01:41:09,850
OK now doubt present function the the data and reference priors this is your you're

1541
01:41:09,850 --> 01:41:13,230
sitting there you haven't seen any data yet all you're thinking about prior should i

1542
01:41:14,440 --> 01:41:18,520
OK you're free to envision dataset you can get

1543
01:41:18,540 --> 01:41:22,960
the city of your friends and imagine possible dataset you can get

1544
01:41:23,610 --> 01:41:30,110
and the divergence functions in the in the reference prior is an expectation over possible

1545
01:41:30,110 --> 01:41:32,250
things that you could get

1546
01:41:32,290 --> 01:41:35,900
so in fact it has a little frequencies kind of mathematical character but it's perfectly

1547
01:41:36,710 --> 01:41:41,100
baby isn't afraid of what you know to dream about data sets

1548
01:41:41,310 --> 01:41:45,350
and i just wanted to say

1549
01:41:45,380 --> 01:41:49,540
i have been possible

1550
01:41:52,420 --> 01:41:56,400
o one of the

1551
01:41:56,420 --> 01:42:04,020
she was in patagonia posterior prior to posterior or you prefer

1552
01:42:07,080 --> 01:42:11,900
in this season

1553
01:42:11,940 --> 01:42:15,100
what is known

1554
01:42:15,100 --> 01:42:17,380
you don't know

1555
01:42:17,420 --> 01:42:22,860
no not that i i wrote probability model the beginning everybody agrees you sort of

1556
01:42:22,860 --> 01:42:24,560
have to start there

1557
01:42:24,580 --> 01:42:26,480
OK and

1558
01:42:27,420 --> 01:42:31,790
i can imagine datasets the probability model

1559
01:42:31,790 --> 01:42:35,020
that defines a probability measure i can go from there now

1560
01:42:35,020 --> 01:42:39,380
take averages with respect that that's what the reference that to use the the the

1561
01:42:40,730 --> 01:42:45,330
given likelihood you do this hope this this this divergence maximisation gives you prior and

1562
01:42:45,330 --> 01:42:47,980
now you see a real dataset you you put your prior to give your real

1563
01:42:47,980 --> 01:42:52,080
like you like on that dataset you observe into posterior perfectly bayesian

1564
01:42:58,020 --> 01:43:03,250
yeah i it it depends on it sort of the experimental design flavor you know

1565
01:43:03,250 --> 01:43:06,100
and you know that which is which is arguably a good thing

1566
01:43:06,180 --> 01:43:08,600
one thing about how your data will be gathered

1567
01:43:08,650 --> 01:43:12,380
and there's a very good jose bernardo was the first person to talk about this

1568
01:43:12,380 --> 01:43:16,940
in great detail he's got a lot of papers talking about why experiment design should

1569
01:43:16,940 --> 01:43:21,130
so now you have to substitute a and b which is not very good the

1570
01:43:21,130 --> 01:43:26,880
critical it could change them but either your label baby

1571
01:43:28,930 --> 01:43:32,480
you you're labelled by a and you have a child

1572
01:43:32,490 --> 01:43:34,250
who is

1573
01:43:34,270 --> 01:43:38,870
well in the previous iteration which means we either little baby always the child who

1574
01:43:38,870 --> 01:43:42,820
is the or you're has labeled by a as a child was labelled a b

1575
01:43:42,820 --> 01:43:43,800
and so on

1576
01:43:43,820 --> 01:43:47,640
so this is the way you write until operators so just to show you that

1577
01:43:47,640 --> 01:43:49,760
with this fixed point

1578
01:43:49,770 --> 01:43:52,770
you can capture and although

1579
01:43:52,810 --> 01:43:58,030
formulas that would tell you about some eventually something good will happen

1580
01:43:58,080 --> 01:44:00,930
it is very much related to

1581
01:44:00,930 --> 01:44:02,530
liveness properties

1582
01:44:02,630 --> 01:44:05,810
but along you can enforce some

1583
01:44:07,950 --> 01:44:10,000
properties along the

1584
01:44:10,110 --> 01:44:14,340
the the ongoing past you using to reach this eventually

1585
01:44:14,360 --> 01:44:15,080
all right

1586
01:44:15,190 --> 01:44:18,880
so now we quickly so this is what i said here

1587
01:44:19,030 --> 01:44:24,750
and i had an existential quantification here for good for all

1588
01:44:24,760 --> 01:44:33,600
question put m you're gonna which here instead of requiring that there exists one success

1589
01:44:33,690 --> 01:44:36,100
there exists one child

1590
01:44:36,120 --> 01:44:39,440
like here

1591
01:44:39,440 --> 01:44:41,110
if i want

1592
01:44:41,140 --> 01:44:42,910
the property of

1593
01:44:42,950 --> 01:44:44,330
this until

1594
01:44:44,360 --> 01:44:47,250
to hold for all purposes

1595
01:44:47,260 --> 01:44:49,390
i have to require this property for all

1596
01:44:49,410 --> 01:44:50,880
this and

1597
01:44:50,960 --> 01:44:53,270
so this box

1598
01:44:53,300 --> 01:44:55,430
simply means that

1599
01:44:55,470 --> 01:45:03,780
i there are satisfied b or i satisfy a and all my successes in tree

1600
01:45:03,810 --> 01:45:07,550
belong to this fixed point so it means that the left one may be labelled

1601
01:45:07,550 --> 01:45:08,930
by b

1602
01:45:08,930 --> 01:45:13,420
and the right one BBC labelled by a but has a child with

1603
01:45:13,430 --> 01:45:17,830
it is such that all child or children will also have the property

1604
01:45:17,860 --> 01:45:20,920
so by just enforcing the

1605
01:45:20,920 --> 01:45:22,270
the way

1606
01:45:22,330 --> 01:45:25,230
new iterate

1607
01:45:25,240 --> 01:45:28,520
how the other sets propagates down the tree

1608
01:45:28,540 --> 01:45:33,270
if you put the diamond it means that just one successor has to be in

1609
01:45:34,910 --> 01:45:36,230
and if you put box

1610
01:45:36,290 --> 01:45:40,060
simply says that all successes should be in this set

1611
01:45:41,270 --> 01:45:44,090
the property propagates along all passes

1612
01:45:44,110 --> 01:45:47,030
so for quantification of

1613
01:45:50,630 --> 01:45:56,490
so now what does this mean

1614
01:45:57,710 --> 01:46:00,680
you can play the these you can complete rating

1615
01:46:02,820 --> 01:46:07,740
what they claim is that if i take new y

1616
01:46:07,770 --> 01:46:10,820
p a

1617
01:46:13,120 --> 01:46:15,030
the output for you

1618
01:46:15,070 --> 01:46:16,830
o why

1619
01:46:17,520 --> 01:46:18,770
but is this formula

1620
01:46:18,800 --> 01:46:23,290
it's the greatest fixpoint so if i want to compute it a computed by treating

1621
01:46:23,290 --> 01:46:26,330
from the full set of nodes

1622
01:46:26,340 --> 01:46:27,860
and iterate

1623
01:46:28,460 --> 01:46:30,170
my function f

1624
01:46:30,880 --> 01:46:32,570
what is the function

1625
01:46:32,580 --> 01:46:34,970
associated to this formula in the

1626
01:46:34,970 --> 01:46:36,740
ladies of set of nodes

1627
01:46:41,750 --> 01:46:42,780
so at

1628
01:46:43,420 --> 01:46:44,760
of this

1629
01:46:44,800 --> 01:46:47,510
and this

1630
01:46:47,520 --> 01:46:51,740
so if i take a set of nodes while it's

1631
01:46:51,880 --> 01:46:54,340
must be labelled by a

1632
01:46:59,360 --> 01:47:02,360
and it must be

1633
01:47:02,380 --> 01:47:04,620
such that

1634
01:47:04,640 --> 01:47:06,140
if i

1635
01:47:06,160 --> 01:47:08,830
i want to the

1636
01:47:13,070 --> 01:47:15,260
the nodes so that both

1637
01:47:15,270 --> 01:47:17,020
both children

1638
01:47:33,350 --> 01:47:37,120
and i one of the greatest greatest fixpoint of this

1639
01:47:37,160 --> 01:47:39,530
function f

1640
01:47:39,530 --> 01:47:42,470
so what you should understand for the

1641
01:47:42,480 --> 01:47:44,750
greatest is fixed by different start

1642
01:47:44,790 --> 01:47:47,960
looking at the computer would start with the full

1643
01:47:47,990 --> 01:47:50,020
set of nodes

1644
01:47:50,960 --> 01:47:53,210
we iterate

1645
01:47:53,220 --> 01:47:55,790
descending chain

1646
01:47:55,870 --> 01:48:04,200
until instabilities

1647
01:48:04,210 --> 01:48:06,390
so let's try to understand

1648
01:48:06,410 --> 01:48:09,940
just with one or two iterations which ones we capture

1649
01:48:10,060 --> 01:48:14,680
well we try and not which ones were captured which ones

1650
01:48:18,160 --> 01:48:22,830
and what what is not what cannot be because

1651
01:48:22,840 --> 01:48:24,190
in order to be in the

1652
01:48:24,200 --> 01:48:27,750
greatest fixpoint you have to be known as so

1653
01:48:27,760 --> 01:48:30,250
who do we get rid of

1654
01:48:30,260 --> 01:48:33,440
along this compute iterative computation

1655
01:48:33,920 --> 01:48:35,710
so she

1656
01:48:37,120 --> 01:48:38,260
who is not

1657
01:48:38,260 --> 01:48:41,810
in here was in here so first of all

1658
01:48:41,840 --> 01:48:45,180
i would get rid of for those that are not labeled by a

1659
01:48:45,750 --> 01:48:46,830
no way

1660
01:48:46,850 --> 01:48:49,200
to be in this greatest fixpoint

1661
01:48:49,200 --> 01:48:50,710
if you're not an old

1662
01:48:50,730 --> 01:48:52,620
which is labeled by a

1663
01:48:52,670 --> 01:48:53,960
now what's next

1664
01:48:56,090 --> 01:48:57,130
you cannot be

1665
01:48:57,220 --> 01:49:02,660
i mean even some iteration you cannot be in the next iteration if what

1666
01:49:02,700 --> 01:49:05,770
if you have a child

1667
01:49:05,780 --> 01:49:07,530
so the first iteration

1668
01:49:07,540 --> 01:49:14,030
you have the shape was not labeled by a

1669
01:49:14,040 --> 01:49:16,080
right because

1670
01:49:16,090 --> 01:49:17,980
the first iteration

1671
01:49:17,980 --> 01:49:19,670
which tells you that

1672
01:49:19,690 --> 01:49:24,690
well we may try to look at the first iteration has here OK and labeled

1673
01:49:24,690 --> 01:49:26,430
by a

1674
01:49:34,920 --> 01:49:37,500
who has

1675
01:49:37,520 --> 01:49:39,170
who was children

1676
01:49:39,200 --> 01:49:43,030
belong to the set and all the children belong to the set

1677
01:49:43,060 --> 01:49:44,920
so this is

1678
01:49:46,440 --> 01:49:48,720
the set of all nodes

1679
01:49:48,740 --> 01:49:51,160
so after first iteration of the said

1680
01:49:51,170 --> 01:49:56,000
i i capture well i rule out so i exclude doing all the nodes that

1681
01:49:56,000 --> 01:49:57,300
are not labeled by a

1682
01:49:57,430 --> 01:50:01,060
six next iteration what's going on the next iteration

1683
01:50:01,060 --> 01:50:05,470
want numbers that we report for the error bar and so what people do is

1684
01:50:05,470 --> 01:50:10,810
they estimate this expectation value using simply the second derivative

1685
01:50:10,830 --> 01:50:11,800
a logo

1686
01:50:11,820 --> 01:50:16,580
at the maximum of the likelihood function so that's the formula you can actually use

1687
01:50:16,600 --> 01:50:20,670
to get the numerical value of an air so notice here i don't have a

1688
01:50:20,670 --> 01:50:21,990
hat over the

1689
01:50:22,010 --> 01:50:25,640
the i that gives you number

1690
01:50:29,580 --> 01:50:35,300
i think in a number of you have probably used this program route which contains

1691
01:50:35,300 --> 01:50:41,800
also the programming we to do some parameter fitting and sometimes when you fit these

1692
01:50:41,800 --> 01:50:45,180
parameters it also gives you the areas you should be aware

1693
01:50:45,310 --> 01:50:49,600
what it's doing is it's getting in the error bars based on that type of

1694
01:50:49,930 --> 01:50:51,670
formula it's basically

1695
01:50:51,720 --> 01:50:58,100
numerically determining this is the second derivative of your log likelihood function and on the

1696
01:50:58,100 --> 01:51:03,910
basis of that approximation reporting the errors

1697
01:51:03,960 --> 01:51:07,220
OK now this this message

1698
01:51:07,240 --> 01:51:10,030
also from databases of of

1699
01:51:10,040 --> 01:51:14,610
closely related to the child called the graphical methods

1700
01:51:14,630 --> 01:51:17,300
and the idea of this method is the following

1701
01:51:17,300 --> 01:51:21,390
if you expand the log likelihood function about its

1702
01:51:22,550 --> 01:51:27,580
what you get is a lot of data is then approximately equal to

1703
01:51:27,600 --> 01:51:34,280
log scale of data had right that by construction the maximum like of log likelihoods

1704
01:51:34,300 --> 01:51:35,380
plus there

1705
01:51:35,550 --> 01:51:41,580
first order terms with the derivative with respect to the parameter of valuated theta had

1706
01:51:41,600 --> 01:51:45,320
time state of mind they have and the second order term and so forth but

1707
01:51:45,320 --> 01:51:46,250
let me just consider

1708
01:51:46,630 --> 01:51:50,370
that many terms k through second order

1709
01:51:50,390 --> 01:51:55,110
OK so the first term is basically log l max

1710
01:51:55,130 --> 01:51:57,090
the second term

1711
01:51:57,100 --> 01:51:58,190
is zero

1712
01:51:58,210 --> 01:52:02,310
right this second term is zero because the derivatives of a function at its maximum

1713
01:52:02,310 --> 01:52:03,850
has to be zero

1714
01:52:04,610 --> 01:52:06,450
and then in the third term

1715
01:52:06,460 --> 01:52:10,280
i have here is the second derivative of the log likelihood function

1716
01:52:10,290 --> 01:52:14,640
evaluated at its maximum and i can relate that using the previous over and i

1717
01:52:14,640 --> 01:52:16,550
can relate that to my estimate

1718
01:52:16,560 --> 01:52:18,560
for the standard deviation

1719
01:52:18,930 --> 01:52:23,870
so that means that i can write

1720
01:52:23,890 --> 01:52:28,010
this expression log l as lomax max minus

1721
01:52:28,100 --> 01:52:30,270
a data miner's they had square

1722
01:52:30,290 --> 01:52:34,200
and then that if you go back to the previous overhead i basically approximate that

1723
01:52:34,200 --> 01:52:38,770
by two times the estimate of the variance squared

1724
01:52:38,830 --> 01:52:42,530
so basically what it says is the following is that if you if you evaluate

1725
01:52:42,530 --> 01:52:46,110
this function at data plus or minus one

1726
01:52:46,140 --> 01:52:50,020
the standard deviation of they had a plus or minus one arab or if they

1727
01:52:50,880 --> 01:52:52,290
that means that this

1728
01:52:52,570 --> 01:52:57,710
term here is going to cancel the variance in the denominator and it means that

1729
01:52:57,710 --> 01:53:03,070
by moving the parameter plus or minus one standard deviation away from the best estimate

1730
01:53:04,020 --> 01:53:09,870
it causes the log likelihood function to decrease by by one half

1731
01:53:09,910 --> 01:53:11,840
right so that's an important role

1732
01:53:11,850 --> 01:53:17,930
to get you change the parameter away from them from the maximum likelihood values until

1733
01:53:17,930 --> 01:53:22,250
the log likelihood function decreases by one have

1734
01:53:22,360 --> 01:53:27,460
so here's an example of that with the same example that i should previously with

1735
01:53:27,460 --> 01:53:32,330
this exponential distribution i should show this picture really really this is what the log

1736
01:53:32,330 --> 01:53:37,190
likelihood function would look like in that case there's the log likelihood function is a

1737
01:53:37,190 --> 01:53:38,390
function of tau

1738
01:53:38,420 --> 01:53:42,300
it has a maximum at some point and that's how hat

1739
01:53:42,300 --> 01:53:46,800
and so my role here is the following i have a certain maximum value i

1740
01:53:46,800 --> 01:53:48,580
go down by one half

1741
01:53:48,600 --> 01:53:52,950
that gives me that line there then dropped these lines down here and the distance

1742
01:53:52,950 --> 01:53:55,600
there or the distance there

1743
01:53:55,610 --> 01:53:57,900
in principle they should be similar

1744
01:53:57,910 --> 01:54:00,030
the thing should be almost symmetric

1745
01:54:00,030 --> 01:54:04,020
and in this particular example the not quite symmetric here i call that distance there

1746
01:54:04,050 --> 01:54:07,620
delta tau minus and that distance there

1747
01:54:07,640 --> 01:54:08,950
delta tau plus

1748
01:54:08,970 --> 01:54:13,000
so well not quite important

1749
01:54:13,000 --> 01:54:15,690
in this circumstance like this you could do one of two things you can say

1750
01:54:15,690 --> 01:54:19,890
well how accurately you really need to estimate the error bars and in most cases

1751
01:54:19,890 --> 01:54:22,870
you would say well they're pretty close to o point one five

1752
01:54:22,890 --> 01:54:25,110
and that's the number that i would write down

1753
01:54:25,120 --> 01:54:26,850
for my error bar

1754
01:54:26,850 --> 01:54:28,990
you will recall that's essentially the same

1755
01:54:29,030 --> 01:54:32,050
the values that we got from the monte carlo method i think it was o

1756
01:54:32,050 --> 01:54:35,470
point one five one right so the getting essentially the same answer

1757
01:54:35,490 --> 01:54:40,500
in other cases where the likelihood function comes out very asymmetric

1758
01:54:40,520 --> 01:54:42,800
it might be appropriate to actually give

1759
01:54:42,810 --> 01:54:48,560
the plus value and the minus value separately so sometimes you see measurements quoted with

1760
01:54:48,630 --> 01:54:51,320
asymmetric error bars

1761
01:54:51,340 --> 01:54:53,780
so we do that too sometimes as well

1762
01:54:53,790 --> 01:54:58,160
it turns out in this case that if the sample size gets much larger

1763
01:54:58,180 --> 01:54:58,960
then this

1764
01:54:59,010 --> 01:55:03,150
likelihood function becomes more symmetric if the sample size is very small if you only

1765
01:55:03,150 --> 01:55:09,940
had five measurements instead of fifty measurements then it would be noticeably asymmetric

1766
01:55:10,100 --> 01:55:12,160
OK so we have the monte carlo method

1767
01:55:12,170 --> 01:55:17,510
the information inequality methods closely related graphical methods

1768
01:55:17,510 --> 01:55:21,040
OK this the only thing methods for now there are probably more but those those

1769
01:55:21,040 --> 01:55:24,050
three are pretty good pretty good start

1770
01:55:24,060 --> 01:55:27,700
OK let's let's go on now too closely related

1771
01:55:27,760 --> 01:55:32,920
method of parameter estimation and that is called the method of least squares

1772
01:55:32,920 --> 01:55:40,180
shape isomers when we are in lighter system and when we never fission

1773
01:55:40,230 --> 01:55:41,990
this is a very

1774
01:55:42,170 --> 01:55:46,500
a famous example of spin isomer that have been observed and the

1775
01:55:46,510 --> 01:55:49,030
storytelling is very quickly studied

1776
01:55:49,050 --> 01:55:54,230
it was at one hundred seventy eight so you have here the ground state

1777
01:55:54,250 --> 01:55:57,950
theory and experiment so ground state professional band

1778
01:55:57,960 --> 01:56:01,650
you have here first isomeric eight minus state

1779
01:56:01,690 --> 01:56:03,310
the most

1780
01:56:03,320 --> 01:56:07,090
important one these ones the sixteen flows isomeric state

1781
01:56:07,100 --> 01:56:08,980
thirty one years

1782
01:56:09,000 --> 01:56:12,890
so you see that the sixteen is located here around

1783
01:56:12,920 --> 01:56:17,380
the twelfth blue states are an event template states so mean that it

1784
01:56:17,390 --> 01:56:21,740
if you want to then you will have a chance of the municipality

1785
01:56:21,770 --> 01:56:25,700
which is bigger than six so it is not very favourable to have this kind

1786
01:56:25,700 --> 01:56:26,850
of position

1787
01:56:26,850 --> 01:56:28,510
so that's why you have

1788
01:56:28,530 --> 01:56:31,030
this long lifetime

1789
01:56:31,080 --> 01:56:34,500
so you can imagine many things with the size of the image and you can

1790
01:56:34,500 --> 01:56:37,160
produce it when you want and then you can

1791
01:56:37,170 --> 01:56:41,110
depopulated when you want you won't like you would have

1792
01:56:41,120 --> 01:56:43,550
like a laser so you can store

1793
01:56:43,600 --> 01:56:46,300
the storage of the energy here

1794
01:56:46,310 --> 01:56:51,830
this is far from being made completely because it's very difficult to be populated when

1795
01:56:51,830 --> 01:56:52,720
you want

1796
01:56:52,740 --> 01:56:54,810
so to populate its

1797
01:56:54,810 --> 01:56:56,550
you can do a reaction but for

1798
01:56:56,560 --> 01:57:00,170
for the population you need to we to increase the energy to another level and

1799
01:57:00,170 --> 01:57:01,390
then go back

1800
01:57:01,400 --> 01:57:04,640
but this is under study because here you imagine that the

1801
01:57:04,660 --> 01:57:05,890
the story of

1802
01:57:05,890 --> 01:57:08,420
you have a few immediately so

1803
01:57:08,440 --> 01:57:11,990
energy so this is something that

1804
01:57:12,000 --> 01:57:16,910
is under study

1805
01:57:16,920 --> 01:57:22,230
why is it also important to look at these spin isomers

1806
01:57:22,260 --> 01:57:23,340
it's because

1807
01:57:23,950 --> 01:57:30,150
as i told you that correspond to improve your input states so individual excitations so

1808
01:57:30,150 --> 01:57:30,800
thank you

1809
01:57:30,820 --> 01:57:35,160
if you there are pure states so completely different from the other but pure states

1810
01:57:35,930 --> 01:57:41,350
then we can pin down what are their structure what are the deformation spin parity

1811
01:57:41,370 --> 01:57:45,490
g factory you don't have all this makes in collective motion and things

1812
01:57:45,500 --> 01:57:47,130
there are pure states

1813
01:57:47,140 --> 01:57:50,680
so then they give important information for theory

1814
01:57:50,710 --> 01:57:54,970
this is very important to know the single particle states

1815
01:57:54,990 --> 01:57:59,340
because some calculations have been formed to study the spins of the ground state of

1816
01:58:00,650 --> 01:58:01,420
and it was

1817
01:58:01,450 --> 01:58:03,300
not so good

1818
01:58:03,300 --> 01:58:07,740
in even even nuclei is you have all these

1819
01:58:07,760 --> 01:58:12,130
all these pairs first been that's been down and because of symmetry to ground state

1820
01:58:12,130 --> 01:58:13,450
is zero plus

1821
01:58:13,460 --> 01:58:14,800
in on i

1822
01:58:14,820 --> 01:58:17,970
you have another family and so the ground state

1823
01:58:18,010 --> 01:58:22,420
is out of numbers so one has two three and five

1824
01:58:22,430 --> 01:58:25,130
what do people negative state depending on which

1825
01:58:25,190 --> 01:58:28,310
is a single nucleons so it's very difficult to know

1826
01:58:28,320 --> 01:58:31,430
even the ground state spin and parity of the ground state

1827
01:58:31,450 --> 01:58:33,460
of the odd nuclei

1828
01:58:33,470 --> 01:58:35,870
and when you perform some calculations

1829
01:58:36,340 --> 01:58:38,950
you see that sometimes

1830
01:58:38,980 --> 01:58:42,230
very quite often you don't find the good speed

1831
01:58:43,140 --> 01:58:47,660
and if you know exactly has been because of the ground state of the only

1832
01:58:48,630 --> 01:58:51,780
you better know the single particle levels

1833
01:58:51,810 --> 01:58:53,190
so you can

1834
01:58:53,660 --> 01:58:54,920
sheikh saad

1835
01:58:54,940 --> 01:58:59,400
by doing this study of spin isomers

1836
01:58:59,400 --> 01:59:02,570
it's quite difficult to obtain the ground state for odd

1837
01:59:02,590 --> 01:59:08,640
but system experimentally it's better to study the spin isomers because they have given lifetime

1838
01:59:08,640 --> 01:59:11,950
so you have time to study that so that's why people

1839
01:59:11,980 --> 01:59:15,950
really like to perform experiments concerning spin isomers

1840
01:59:15,970 --> 01:59:20,970
what are they doing so what they consider the magnetic moment is proportional to the

1841
01:59:20,970 --> 01:59:25,020
orbital moment that they introduced yesterday and speed

1842
01:59:25,030 --> 01:59:29,850
so depending on the orbital you have a given value of new

1843
01:59:29,860 --> 01:59:32,420
so then if you know you are if you know she

1844
01:59:32,440 --> 01:59:34,160
g sorry to same

1845
01:59:34,210 --> 01:59:35,210
then you know

1846
01:59:35,220 --> 01:59:40,860
what the orbital of the occupied level and if you have configuration mixing on

1847
01:59:40,860 --> 01:59:42,870
so experimentally what they do

1848
01:59:42,890 --> 01:59:47,590
they do fragmentation reactions so they bombarded target then you have fragmentation

1849
01:59:47,600 --> 01:59:49,670
and then they can produce nuclei

1850
01:59:49,690 --> 01:59:53,000
in their isomeric state so

1851
01:59:53,330 --> 01:59:57,920
was also what is important is that they have aligned spins

1852
01:59:57,920 --> 02:00:00,610
what does it mean to mean that the when they win

1853
02:00:00,660 --> 02:00:03,240
he came from the eyes to the ground state

1854
02:00:03,250 --> 02:00:04,660
there we any gamma

1855
02:00:04,670 --> 02:00:08,460
that will be needed all in the same direction

1856
02:00:08,470 --> 02:00:11,420
then they pulled by the merits in my need feel

1857
02:00:11,430 --> 02:00:16,130
so when you do that you have rotation which corresponds to allow more rotation so

1858
02:00:16,130 --> 02:00:19,070
you know the frequency

1859
02:00:19,080 --> 02:00:23,940
what sorry what they put are germanium detectors to detect the gamma

1860
02:00:23,970 --> 02:00:26,870
so then the tags and amazon both detectors

1861
02:00:26,880 --> 02:00:31,270
and when you played the difference between what is depicted in one detectors miners what

1862
02:00:31,390 --> 02:00:33,450
to take in the other one

1863
02:00:33,510 --> 02:00:37,670
so then you have this policy oscillation here

1864
02:00:37,690 --> 02:00:41,350
you can calculate the larmor frequency and then you can deduce

1865
02:00:41,350 --> 02:00:43,690
this g factor

1866
02:00:43,740 --> 02:00:45,880
so from this experiment

1867
02:00:45,890 --> 02:00:52,370
you can see what he we told that is filled by the nucleons

1868
02:00:52,550 --> 02:00:57,680
this is a very recent example this is so for forty three so close to

1869
02:00:57,730 --> 02:00:59,400
the end it was twenty eight

1870
02:01:00,030 --> 02:01:04,210
also to look at this shape twenty eight what is the ordering between these two

1871
02:01:04,210 --> 02:01:07,860
levels how it is in this specific meetings

1872
02:01:07,860 --> 02:01:10,710
what is the Commission was set up on

1873
02:01:11,990 --> 02:01:18,450
only free when only the result of states that is symmetric and positive definite we

1874
02:01:18,450 --> 02:01:19,750
had a descent direction

1875
02:01:19,970 --> 02:01:21,650
that's all but now we're interested

1876
02:01:22,790 --> 02:01:28,370
not not convergence theorem just the theory about that we get there a descent direction

1877
02:01:28,890 --> 02:01:31,690
that's this theorem is a respectively

1878
02:01:33,350 --> 02:01:35,870
yeah OK

1879
02:01:37,190 --> 02:01:42,590
so worker iteration order and q because that's the work per iteration in practice for

1880
02:01:42,590 --> 02:01:45,710
solving a system of equations were not do

1881
02:01:45,890 --> 02:01:50,670
and if you know that which how fast matrix multiplication

1882
02:01:50,730 --> 02:01:55,060
OK so we're ignore all of that stuff again because

1883
02:01:55,520 --> 02:02:02,270
that's what people do it of and this might come about so-called causing Newton methods

1884
02:02:02,350 --> 02:02:08,750
OK so prove the result so let me try and fashion approved and if I

1885
02:02:08,750 --> 02:02:12,040
don't finish the proof in time and the track finish the proof in time an

1886
02:02:12,060 --> 02:02:15,670
on in time you can read it in the notes because

1887
02:02:16,650 --> 02:02:21,450
I feel on superficial level the proof is just make the the proof is not

1888
02:02:23,500 --> 02:02:27,420
unless you spend a lot of time studying these methods then you can see what's

1889
02:02:27,420 --> 02:02:33,230
going on in the proof but I think for the purposes of this course so

1890
02:02:33,230 --> 02:02:37,630
I wanted to prove so that you can see that in fact it's true but

1891
02:02:37,630 --> 02:02:40,770
I'm not sure how much insight on the fashioning

1892
02:02:40,770 --> 02:02:44,600
In this proof but under need the following result

1893
02:02:44,710 --> 02:02:47,380
OK which is

1894
02:02:47,400 --> 02:02:51,980
that says the following 2 statements are equivalent which you're going to prove which is

1895
02:02:51,980 --> 02:02:53,770
exercise seven

1896
02:02:53,770 --> 02:02:54,960
of the homework

1897
02:02:55,050 --> 02:02:56,390
that is

1898
02:02:58,080 --> 02:03:04,400
of the norm of then inverse is less than or equal to 1 over H

1899
02:03:04,400 --> 02:03:12,130
is completely equivalent to the statement that the norm of em is at least h

1900
02:03:12,170 --> 02:03:16,330
times the normal the for anything

1901
02:03:17,330 --> 02:03:23,490
that is exercise seven the new nodes and for those of you that came later

1902
02:03:23,580 --> 02:03:25,710
which is essentially all of you

1903
02:03:26,070 --> 02:03:27,750
but I'm not

1904
02:03:30,130 --> 02:03:34,270
so if some of you downloaded the lecture notes in after about 10 AM this

1905
02:03:35,750 --> 02:03:41,070
you only have Proposition 2 . 1 has only 2 parts

1906
02:03:41,250 --> 02:03:46,000
if you look downloaded before 10 AM this morning it has 3 parts

1907
02:03:46,230 --> 02:03:49,900
you only have to prove the 1st 2 part and I just tell you this

1908
02:03:49,900 --> 02:03:53,810
is an easy proof there's you can do

1909
02:03:54,750 --> 02:04:00,730
a freshman at MIT could do this person OK so that it it's

1910
02:04:01,730 --> 02:04:05,150
you don't need any heavy machinery to do this proof you just need the definition

1911
02:04:05,150 --> 02:04:06,010
of the norm

1912
02:04:06,190 --> 02:04:06,850
that's all

1913
02:04:07,530 --> 02:04:09,590
maybe not a freshman in itself

1914
02:04:10,610 --> 02:04:11,850
the OK

1915
02:04:12,790 --> 02:04:15,050
we need 1 other

1916
02:04:15,060 --> 02:04:17,290
piece of information

1917
02:04:17,440 --> 02:04:21,170
to do the proof

1918
02:04:21,350 --> 02:04:23,690
and that is Proposition 2 . 2

1919
02:04:24,250 --> 02:04:30,570
for Proposition 2 . 2 states where put

1920
02:04:32,590 --> 02:04:38,090
yeah but to have you

1921
02:04:39,370 --> 02:04:45,370
what Thank you write in

1922
02:04:53,480 --> 02:04:58,790
Proposition 2 . 2

1923
02:04:59,100 --> 02:05:06,270
is the simple fact that relies on the fundamental theorem of calculus

1924
02:05:06,430 --> 02:05:17,010
which just says it is the gradient of f the minus the gradient vector X

