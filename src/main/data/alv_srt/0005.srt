1
00:00:00,000 --> 00:00:03,080
let's imagine

2
00:00:03,120 --> 00:00:07,630
can you see this over there

3
00:00:07,650 --> 00:00:09,020
then i will right over there

4
00:00:09,040 --> 00:00:12,830
i was right

5
00:00:12,850 --> 00:00:16,550
alright let's see if this works

6
00:00:26,600 --> 00:00:27,450
o point

7
00:00:27,470 --> 00:00:30,850
here on the twenty first century we still have worked this out OK

8
00:00:31,860 --> 00:00:35,740
that's really you could everybody can see this right here right

9
00:00:35,750 --> 00:00:40,120
OK so look let's imagine we reaction that looks like this

10
00:00:40,170 --> 00:00:45,380
the reaction profile that looks like this were these two images are

11
00:00:45,460 --> 00:00:48,990
actually equivalent

12
00:00:49,070 --> 00:00:50,830
OK i tried to run one

13
00:00:51,700 --> 00:00:55,720
well not exactly but pretty much on exactly the same level

14
00:00:55,740 --> 00:01:00,350
and let's say we start out with a large number of molecules right over here

15
00:01:00,410 --> 00:01:06,540
now if there were an enzyme around the enzyme might lower the activation energy and

16
00:01:06,540 --> 00:01:07,790
in so doing

17
00:01:08,640 --> 00:01:10,000
make it possible

18
00:01:10,080 --> 00:01:12,910
four molecules to to tunnel through this hill

19
00:01:12,920 --> 00:01:14,790
and move already here

20
00:01:14,800 --> 00:01:19,710
the fact that we want to get over here it has the same free energy

21
00:01:19,710 --> 00:01:21,000
is over there

22
00:01:21,040 --> 00:01:27,400
it means that the the catalyst may principle also facilitate a back reaction what i

23
00:01:27,400 --> 00:01:32,310
mean by back reaction i mean going in exactly the opposite direction and so once

24
00:01:32,310 --> 00:01:34,730
molecules over here for

25
00:01:34,740 --> 00:01:39,250
the lower the energy lowering effects of the enzyme they allow them to move in

26
00:01:39,250 --> 00:01:40,550
both directions

27
00:01:40,630 --> 00:01:46,830
and and therefore what we will have is ultimately the establishment of an equilibrium

28
00:01:46,870 --> 00:01:51,240
if these two energy states are equivalent then i will tell you

29
00:01:51,260 --> 00:01:56,390
fifty percent of molecules and up here and fifty percent of molecules and up here

30
00:01:56,400 --> 00:02:02,400
and here we're beginning now to wrestle between with two different independent concepts the rate

31
00:02:02,400 --> 00:02:03,820
of the reaction

32
00:02:03,870 --> 00:02:06,660
and the equilibrium state of the reaction

33
00:02:06,680 --> 00:02:12,110
note that the enzyme has no effect whatsoever on the equilibrium state e these two

34
00:02:12,110 --> 00:02:18,550
article free energies equilibrium state where the energy barrier is this high

35
00:02:18,560 --> 00:02:20,440
or whether it's this high

36
00:02:20,520 --> 00:02:21,840
is irrelevant

37
00:02:21,850 --> 00:02:27,480
the fact is if the energy if the enzyme makes possible this motion back and

38
00:02:27,480 --> 00:02:32,460
forth the ultimate equilibrium state will be fifty percent of molecules here and fifty percent

39
00:02:32,460 --> 00:02:36,400
of the molecules there and therefore the enzyme

40
00:02:36,410 --> 00:02:40,650
really only affects the rate at which the reaction takes place

41
00:02:40,660 --> 00:02:45,230
will it happen in the microsecond or will happen in a day or will happen

42
00:02:45,230 --> 00:02:46,680
in a million years

43
00:02:46,700 --> 00:02:51,510
the enzyme has no effect whatsoever on the ultimate end product which in this case

44
00:02:51,510 --> 00:02:53,680
is the equilibrium

45
00:02:53,830 --> 00:03:00,500
of course there is already there is a simple mathematical formalism which relates the difference

46
00:03:00,500 --> 00:03:03,190
in free energy is with the

47
00:03:03,580 --> 00:03:06,100
the the equilibrium

48
00:03:06,610 --> 00:03:09,070
here we we might have a situation where

49
00:03:09,080 --> 00:03:12,550
eighty percent of molecules and up at equilibrium over here

50
00:03:12,600 --> 00:03:14,440
twenty percent in that period

51
00:03:14,500 --> 00:03:17,110
or we might end up with a state where

52
00:03:17,120 --> 00:03:20,270
ninety nine point nine percent of the molecules and up here

53
00:03:20,290 --> 00:03:24,580
and the point of point one percent of molecules and up here

54
00:03:24,580 --> 00:03:30,340
but that ultimate equilibrium state is in no way influenced by the enzyme they just

55
00:03:30,340 --> 00:03:32,520
make it happen in real time

56
00:03:32,550 --> 00:03:36,250
and therefore the repeated call point i made last time

57
00:03:36,270 --> 00:03:40,330
if most biochemical reactions are to occur in real time

58
00:03:40,330 --> 00:03:42,820
i e on the order of seconds or minutes

59
00:03:42,830 --> 00:03:46,930
an enzyme has to be around to make sure they happen in the absence of

60
00:03:46,930 --> 00:03:49,370
such an enzyme of intermediation

61
00:03:49,390 --> 00:03:54,480
it just won't happen in real time even though in principle it's energetically

62
00:03:55,600 --> 00:03:59,390
so let's just keep that very much in mind in the in the course of

63
00:03:59,390 --> 00:04:02,730
discussions that happen and let's just begin now

64
00:04:02,780 --> 00:04:03,970
to look at

65
00:04:03,980 --> 00:04:10,220
in important energy generating reactions in the cell which is called glycolysis

66
00:04:10,740 --> 00:04:15,610
we already know these the prefix glycol like refers to sugar

67
00:04:15,620 --> 00:04:19,220
and licenses l SAS refers to the breakdown

68
00:04:19,280 --> 00:04:21,410
of a certain compound

69
00:04:21,410 --> 00:04:24,550
i'm not going to ask you nor anyone else in this room and ask you

70
00:04:24,550 --> 00:04:26,450
to memorize this sequence

71
00:04:27,900 --> 00:04:30,800
but i'd like you to look at it and see what taken lessons we can

72
00:04:30,800 --> 00:04:35,180
distill out of that what wisdom we can learn from looking at such a complex

73
00:04:35,180 --> 00:04:36,950
series of reactions

74
00:04:37,000 --> 00:04:41,680
perhaps the first thing we can learn is that when we think about biochemical reactions

75
00:04:41,680 --> 00:04:44,700
we don't think of them as happening in isolation

76
00:04:44,710 --> 00:04:49,090
here i'm talking about for example in this case i could be talking about a

77
00:04:49,090 --> 00:04:50,260
plus b

78
00:04:50,510 --> 00:04:53,010
going to see plus b

79
00:04:53,020 --> 00:04:55,600
and there might be a back reaction

80
00:04:55,630 --> 00:04:57,880
two weeks to reach equilibria

81
00:04:57,930 --> 00:05:02,870
and we're just isolating that simple reaction from all others around but in the real

82
00:05:02,870 --> 00:05:08,010
world in living cells most reactions are parts of very long

83
00:05:08,080 --> 00:05:13,020
pathways for each of these steps here indicates one of the others what a step

84
00:05:13,030 --> 00:05:14,850
in the pathway

85
00:05:15,070 --> 00:05:20,320
what we're interested in here is how glucose which i advertised two lectures being an

86
00:05:20,320 --> 00:05:25,250
important energy source is actually broken down how to sell harvest the energy which is

87
00:05:25,250 --> 00:05:26,700
inherent in glucose

88
00:05:26,720 --> 00:05:30,430
in order to generate among other things a ATP

89
00:05:30,440 --> 00:05:33,280
which we have said repeatedly is the energy currency

90
00:05:33,290 --> 00:05:39,670
ATP is used by hundreds of different biochemical reactions in order to make them happen

91
00:05:39,710 --> 00:05:42,030
these other biochemical reactions are

92
00:05:42,040 --> 00:05:45,130
and you're gonna they require the investment of energy

93
00:05:45,130 --> 00:05:50,620
and almost invariably but not invariably almost invariably the cell will grab hold of an

94
00:05:50,620 --> 00:05:52,340
ATP molecule

95
00:05:52,350 --> 00:05:56,190
break it down usually to ANP or a

96
00:05:56,250 --> 00:05:59,390
and then utilize the energy which derives

97
00:05:59,400 --> 00:06:01,510
from breaking down ATP

98
00:06:01,570 --> 00:06:05,330
it will invest energy in an organic reaction

99
00:06:05,350 --> 00:06:07,790
which is otherwise wouldn't happen

100
00:06:07,820 --> 00:06:10,550
so here we reach the idea

101
00:06:10,570 --> 00:06:17,040
the perhaps by investing energy in a reaction it the equilibrium is shifted because by

102
00:06:17,040 --> 00:06:18,290
investing energy

103
00:06:18,290 --> 00:06:19,770
the terminal speed

104
00:06:19,780 --> 00:06:22,770
is proportional to the square root of the radius

105
00:06:22,780 --> 00:06:24,340
for a given

106
00:06:24,390 --> 00:06:26,880
density of the the object

107
00:06:26,890 --> 00:06:30,240
if you take a battle with a radius of about one centimeter

108
00:06:30,270 --> 00:06:32,170
you throw it off a high building

109
00:06:32,200 --> 00:06:37,160
it will reduce speed which will not exceed seventy five miles per hour because of

110
00:06:37,160 --> 00:06:38,470
the and drag

111
00:06:38,480 --> 00:06:40,560
if you jump out of the plane

112
00:06:40,580 --> 00:06:43,160
and you have no parachute

113
00:06:43,200 --> 00:06:45,730
and i make the assumption

114
00:06:45,740 --> 00:06:47,480
that your mass

115
00:06:47,530 --> 00:06:50,130
is about seventy kilograms

116
00:06:50,170 --> 00:06:55,540
i wonder rough numbers if i can approximate you biosphere was a radius of about

117
00:06:55,540 --> 00:06:59,340
forty centimeters that's also approximation you're not really like is for you but i want

118
00:06:59,340 --> 00:07:01,380
to get some some rough numbers

119
00:07:01,390 --> 00:07:03,420
then the terminal velocity

120
00:07:03,430 --> 00:07:05,320
it is a hundred and fifty

121
00:07:05,350 --> 00:07:08,020
miles per hour

122
00:07:08,110 --> 00:07:11,710
so if you do job out of the plane

123
00:07:11,710 --> 00:07:13,500
i have no parachute

124
00:07:13,550 --> 00:07:18,820
you will not go much faster than hundred fifty miles per hour

125
00:07:18,830 --> 00:07:22,580
i just read an article yesterday about skydivers will jump out of planes and they

126
00:07:22,580 --> 00:07:26,960
want to open the parachute at the very last possible and they reach terminal velocity

127
00:07:26,960 --> 00:07:30,560
is of hundred twenty miles per hour it doesn't surprise me it's very close to

128
00:07:30,560 --> 00:07:32,630
disrupt number that calculated

129
00:07:32,630 --> 00:07:35,560
was then they open the parachute and air drag

130
00:07:35,590 --> 00:07:37,480
increases enormously

131
00:07:37,500 --> 00:07:39,280
and then they slow down

132
00:07:39,300 --> 00:07:43,810
even further

133
00:07:43,820 --> 00:07:46,110
i told you raindrop

134
00:07:46,160 --> 00:07:48,130
almost all raindrops

135
00:07:48,170 --> 00:07:49,730
operate in regime two

136
00:07:49,770 --> 00:07:52,410
when they fall to the terminal velocities

137
00:07:52,420 --> 00:07:55,420
dictated by the descriptor

138
00:07:55,430 --> 00:07:58,660
however if you make the baltics small

139
00:07:58,670 --> 00:07:59,850
now comes the time

140
00:08:00,810 --> 00:08:02,130
you really and to

141
00:08:02,140 --> 00:08:03,770
regime one

142
00:08:03,790 --> 00:08:06,740
and all assignment number four i've asked you

143
00:08:06,740 --> 00:08:07,860
to calculate

144
00:08:07,880 --> 00:08:08,880
where that

145
00:08:08,890 --> 00:08:13,110
happens and i can do it for water because the radius of that water will

146
00:08:13,110 --> 00:08:14,340
be so small

147
00:08:14,360 --> 00:08:18,810
it would evaporate immediately so i chose or for that so i'm asking you know

148
00:08:18,810 --> 00:08:23,190
simon for they can or drop make it smaller and smaller and smaller and smaller

149
00:08:23,190 --> 00:08:24,570
and there comes the time

150
00:08:24,580 --> 00:08:27,030
the two begin to enter regime one

151
00:08:27,040 --> 00:08:30,250
and i want you to calculate where that crossovers

152
00:08:30,300 --> 00:08:31,610
between these two

153
00:08:34,950 --> 00:08:36,670
i have here

154
00:08:36,720 --> 00:08:39,140
a ball

155
00:08:39,190 --> 00:08:41,600
you may call it the balloon but i call it the ball because there's no

156
00:08:41,600 --> 00:08:43,150
really mean

157
00:08:43,160 --> 00:08:44,320
and this

158
00:08:44,380 --> 00:08:46,900
ball ways

159
00:08:46,980 --> 00:08:52,850
approximately thirty four grams

160
00:08:53,640 --> 00:08:54,680
let me

161
00:08:54,760 --> 00:08:57,560
raise some here because i want

162
00:08:58,670 --> 00:09:02,740
so we know them mass

163
00:09:02,760 --> 00:09:05,240
and we know the radius

164
00:09:05,240 --> 00:09:06,820
the mass

165
00:09:06,830 --> 00:09:09,720
is about thirty four grand

166
00:09:09,720 --> 00:09:10,900
and the radius

167
00:09:10,910 --> 00:09:13,030
is about thirty five centimeters

168
00:09:13,050 --> 00:09:16,190
about seventy centimeters across

169
00:09:16,190 --> 00:09:18,240
i can calculate what the terminals

170
00:09:20,170 --> 00:09:22,450
then they will be terminal speed

171
00:09:22,470 --> 00:09:25,960
i know i'm definitely going to be in this regime

172
00:09:26,010 --> 00:09:27,530
so i know the mass

173
00:09:27,530 --> 00:09:30,150
i know c two which in areas o point eight five

174
00:09:30,170 --> 00:09:31,960
i know to rate is an ontology

175
00:09:31,980 --> 00:09:35,900
so i find that i find about one point eight

176
00:09:37,380 --> 00:09:38,820
the second

177
00:09:38,840 --> 00:09:40,070
so if i drop it

178
00:09:40,090 --> 00:09:42,900
from the a height of three metres which i'm going to do

179
00:09:43,000 --> 00:09:45,880
you would think that the time it takes to hit the floor

180
00:09:45,940 --> 00:09:48,010
would be about my three meters

181
00:09:48,030 --> 00:09:49,840
divided by one point eight

182
00:09:49,840 --> 00:09:51,130
meters per second

183
00:09:51,150 --> 00:09:53,170
which is about one point seven

184
00:09:54,340 --> 00:09:55,940
that's not bad

185
00:09:55,980 --> 00:09:58,010
that's not a bad approximation

186
00:09:58,050 --> 00:10:00,670
however it will of course take longer

187
00:10:00,690 --> 00:10:02,940
and the reason why it will take longer is

188
00:10:03,710 --> 00:10:09,320
the terminal velocity the terminals speed is not achieved instantaneously

189
00:10:09,360 --> 00:10:12,130
with the ball bearings it was within nine seconds

190
00:10:12,150 --> 00:10:16,170
i can assure you that it will take a lot longer

191
00:10:16,170 --> 00:10:18,000
now if you want to

192
00:10:18,010 --> 00:10:20,510
calculate the time it takes

193
00:10:20,570 --> 00:10:22,210
to get close to the terminal

194
00:10:23,130 --> 00:10:25,150
that is not an easy task

195
00:10:26,480 --> 00:10:30,360
you're going to end up with a nasty differential equation

196
00:10:30,360 --> 00:10:32,300
you're going to get MG

197
00:10:32,320 --> 00:10:35,360
you going to get the acceleration which is the result of

198
00:10:35,360 --> 00:10:39,290
set the stage for electrochemical processes

199
00:10:39,330 --> 00:10:41,460
like chemical processing

200
00:10:41,490 --> 00:10:47,260
so what he reasoned is that when you inject charged species into water

201
00:10:47,310 --> 00:10:50,440
you do the same as we do when we

202
00:10:50,440 --> 00:10:54,830
doped semiconductors and we raise the conductivity

203
00:10:54,880 --> 00:10:57,750
and we solid evidence for that last day

204
00:10:58,550 --> 00:11:03,040
let's look at two examples this is hcl as a gas

205
00:11:03,050 --> 00:11:07,830
hydrogen chloride as the gas this is of a polar molecule you've done

206
00:11:07,930 --> 00:11:14,500
analysis of this many times if we bubble this through water it will dissolve and

207
00:11:14,500 --> 00:11:17,520
according to arrhenius dissociate to give us the

208
00:11:17,550 --> 00:11:24,620
proton which is now dissolved in water denoted a queue for aqueous and chloride ions

209
00:11:24,630 --> 00:11:29,650
so we can see injection of charges pluses and minuses and sodium hydroxide is the

210
00:11:29,650 --> 00:11:36,770
prototypical example of a base it's solid room temperature dissolved in water it gives us

211
00:11:36,770 --> 00:11:38,400
a sodium ion

212
00:11:38,520 --> 00:11:42,150
plus hydroxyl

213
00:11:43,610 --> 00:11:48,400
furthermore rainier said that there is an overarching reaction

214
00:11:48,500 --> 00:11:54,110
it involves a reconstitution of the solvent water in this case is acting as the

215
00:11:55,880 --> 00:12:00,070
water is acting as the solvent so we can run the reaction vertically now we

216
00:12:00,070 --> 00:12:01,840
can take

217
00:12:01,880 --> 00:12:07,110
hcl aqueous placenta o h aqueous to give us

218
00:12:07,120 --> 00:12:08,550
in ACL

219
00:12:11,770 --> 00:12:13,040
h two l

220
00:12:13,090 --> 00:12:18,840
so what we're doing is we're reacting the acid with the base to get

221
00:12:18,880 --> 00:12:24,930
the solvent back water plus salt and this is the neutralisation reaction

222
00:12:24,990 --> 00:12:31,010
neutralisation reaction that is to take it away from either acidic or

223
00:12:31,030 --> 00:12:36,460
the basic solution neutralisation involves recombination

224
00:12:38,610 --> 00:12:39,930
the solved

225
00:12:39,950 --> 00:12:43,130
i really want to talk about today is in reference to

226
00:12:43,170 --> 00:12:47,500
aqueous solution chemistry acids and bases but these definitions

227
00:12:47,540 --> 00:12:53,000
of proton donor and so on applied to nonaqueous solvents but for three online when

228
00:12:53,000 --> 00:12:53,930
we're going to say

229
00:12:54,000 --> 00:12:55,910
we're going to stay in

230
00:12:55,960 --> 00:12:57,990
the aqueous phase

231
00:12:58,000 --> 00:13:02,500
now one of the shortcomings of arrhenius theory it was good for the eighteen eighties

232
00:13:02,530 --> 00:13:08,920
but shortcomings always manifest themselves through data and people had known for quite some time

233
00:13:08,980 --> 00:13:11,260
so that they could dissolved ammonia

234
00:13:11,270 --> 00:13:14,950
another molecule that we've seen three online one

235
00:13:14,960 --> 00:13:16,800
they could dissolve ammonia

236
00:13:18,760 --> 00:13:20,200
and when they dissolved in

237
00:13:20,210 --> 00:13:21,620
ammonia and water

238
00:13:21,620 --> 00:13:24,400
they found that they could neutralize

239
00:13:24,410 --> 00:13:26,400
they got an aqueous solution

240
00:13:26,420 --> 00:13:29,570
that neutralizes acid

241
00:13:29,620 --> 00:13:33,590
they found that they got an aqueous solution and neutralizes acid so then they concluded

242
00:13:33,590 --> 00:13:38,500
that ammonia dissolved in water is acting as though it's the base but there's no

243
00:13:38,540 --> 00:13:42,620
hydroxyl here there's no oxygen so you can make o h minus here

244
00:13:42,660 --> 00:13:47,030
so something was incomplete how to explain

245
00:13:47,050 --> 00:13:52,050
how to explain in the absence of

246
00:13:52,070 --> 00:13:54,180
in the absence of of

247
00:13:54,210 --> 00:13:55,920
o eight minus

248
00:13:55,930 --> 00:14:01,670
well i had to wait until the twentieth century and the explanation came from two

249
00:14:01,670 --> 00:14:04,280
places simultaneously

250
00:14:05,860 --> 00:14:07,820
nineteen twenty three

251
00:14:10,530 --> 00:14:16,250
he's in denmark i suppose he's driving a volvo two and lowery in the UK

252
00:14:16,260 --> 00:14:21,550
ronstadt and lower both independently in the same year enunciated

253
00:14:24,600 --> 00:14:29,090
and enhanced definition of broader definition of acid and base

254
00:14:29,100 --> 00:14:32,200
which is as follows that asset

255
00:14:32,220 --> 00:14:38,330
as it remains the same as arrhenius a arrhenius it's substance that's a proton

256
00:14:38,350 --> 00:14:39,720
proton donors

257
00:14:39,760 --> 00:14:42,850
so that seems arrhenius

258
00:14:47,650 --> 00:14:50,270
but then to capture the notion that

259
00:14:50,360 --> 00:14:56,130
an aqueous solution of ammonia can act as as a base they define the base

260
00:14:56,150 --> 00:15:00,620
in terms of something that is chemistry free and they said this is something that

261
00:15:00,620 --> 00:15:02,510
has a proton

262
00:15:04,250 --> 00:15:08,860
a proton acceptor now that's different no mention of

263
00:15:08,880 --> 00:15:11,000
no mention of hydroxyl

264
00:15:11,010 --> 00:15:16,470
anything that accepts protons would fall under the definition of branstad lowery

265
00:15:17,610 --> 00:15:21,000
so here's the prototypical reaction

266
00:15:21,010 --> 00:15:25,330
any substance that has a proton some right protons

267
00:15:25,380 --> 00:15:30,880
plus everything else so a represents the rest of the chemistry of the compound where

268
00:15:30,880 --> 00:15:33,430
h is a hydrogen that can be

269
00:15:33,460 --> 00:15:38,780
dissolved in water and liberated so this is a proton donor

270
00:15:38,800 --> 00:15:43,180
and for the rest the lecture instead of writing aged in the right little p

271
00:15:43,200 --> 00:15:49,700
plus to indicate protons so a change is the proton donor and reacts with some

272
00:15:49,840 --> 00:15:53,710
branstad lowry base which is a proton

273
00:15:55,160 --> 00:15:57,330
it's a proton acceptor

274
00:15:57,340 --> 00:15:59,080
when it reacts

275
00:15:59,340 --> 00:16:04,160
b accepts the proton and becomes the eight plus

276
00:16:06,100 --> 00:16:08,950
has lost a proton and it

277
00:16:09,010 --> 00:16:11,540
is now simply a minus

278
00:16:11,570 --> 00:16:15,540
well if you walk in the room or refute i know this never happens but

279
00:16:15,540 --> 00:16:17,730
if if you had momentarily

280
00:16:17,740 --> 00:16:22,380
i dozov and hadn't caught the last thing that i said you might look at

281
00:16:22,380 --> 00:16:26,720
the right hand side of the equation say well BH class

282
00:16:26,740 --> 00:16:29,140
has the proton that can give away

283
00:16:29,150 --> 00:16:30,230
and become

284
00:16:30,240 --> 00:16:31,580
they could be so

285
00:16:31,590 --> 00:16:33,370
this BH plus

286
00:16:33,380 --> 00:16:36,490
also is a proton donor

287
00:16:36,530 --> 00:16:40,500
and a minus we know from looking at the left side of equation is perfectly

288
00:16:40,500 --> 00:16:46,250
capable of binding to this a minus must be also a protocol

289
00:16:47,780 --> 00:16:53,270
so now what do we see we see that the equation comes in

290
00:16:55,700 --> 00:16:59,620
b and b h plants are donor acceptor in there

291
00:16:59,630 --> 00:17:02,920
associated with one another so i'm going to denote

292
00:17:02,930 --> 00:17:07,820
the BNP age plus associated with one another and h a

293
00:17:07,830 --> 00:17:12,880
and a minus are associated with one another and we call such pairs

294
00:17:12,890 --> 00:17:16,700
we call such pairs conjugate pair

295
00:17:16,710 --> 00:17:20,340
these are conjugate pairs con is the latin for with

296
00:17:21,350 --> 00:17:25,840
yukari is is the latin for to marry and also the the same word for

297
00:17:25,840 --> 00:17:26,700
your book

298
00:17:26,750 --> 00:17:30,450
i think if you pronounce the j is why you can see how you get

299
00:17:30,460 --> 00:17:35,950
you in modern english and in fact i will designate these with your to show

300
00:17:35,950 --> 00:17:43,240
that they are conjugate acid base pairs just to be clear about definitions let's look

301
00:17:43,280 --> 00:17:49,360
h a this is a proton donor so this is both a bronze lowery acid

302
00:17:49,390 --> 00:17:54,160
and it's also an arrhenius acid it conforms to the old definition of arrhenius

303
00:17:56,660 --> 00:18:02,320
this is a proton acceptor b so therefore it's a bronze that lowery base

304
00:18:02,360 --> 00:18:06,450
but it's not an arrhenius base because it has no hydroxyl so you can see

305
00:18:06,450 --> 00:18:10,700
this is the case of in some cases there is a an overlap and others

306
00:18:10,700 --> 00:18:15,370
of circumstances ok if you have a quadratic error function and

307
00:18:15,650 --> 00:18:19,300
then you find the optimum in one step ok using this

308
00:18:21,530 --> 00:18:25,160
so this is the multivariate setting here with where now instead

309
00:18:25,160 --> 00:18:28,120
of having a second order derivative we have the multivariate

310
00:18:28,120 --> 00:18:31,430
equivalent which is the hessian matrix the hessian matrix

311
00:18:32,140 --> 00:18:35,000
has the second-order derivatives according to pairs of

312
00:18:35,000 --> 00:18:39,290
parameters and so the iteration just most in the direction of the

313
00:18:39,300 --> 00:18:43,590
gradient of the object and the learning rate is the inverse of the

314
00:18:43,600 --> 00:18:46,830
hessian ok you do have to estimate the hessian

315
00:18:47,040 --> 00:18:50,270
ok and so typically these methods in order to be used require

316
00:18:50,270 --> 00:18:53,530
more data than methods that are just based on

317
00:18:54,770 --> 00:18:58,790
step-size so this is called newton-raphson method

318
00:18:59,050 --> 00:19:03,320
sometimes called fisher scoring what's bad

319
00:19:03,550 --> 00:19:09,650
well that depends ok typically this kind of second-order method

320
00:19:09,660 --> 00:19:14,070
requires a few error iterations in order to do the computation because

321
00:19:14,080 --> 00:19:18,750
you sort of take optimal steps at the same time you need a batch

322
00:19:18,760 --> 00:19:20,890
of data so it's not an online algorithm

323
00:19:22,020 --> 00:19:25,820
and also there's this inversion of the hessian with which is

324
00:19:26,150 --> 00:19:29,000
expensive there are actually tricks

325
00:19:29,160 --> 00:19:32,700
h for avoiding this kind of explicit computation an inversion

326
00:19:33,290 --> 00:19:34,970
which may hear about later on

327
00:19:37,020 --> 00:19:41,520
but you can actually do this for logistic regression and it results

328
00:19:41,550 --> 00:19:44,610
an algorithm called iterative re we cursive least-squares

329
00:19:45,080 --> 00:19:50,260
and the hessian has a nice form in this case where you have the features

330
00:19:50,270 --> 00:19:53,800
and and this kind of diagonal matrix of h times one minus h

331
00:19:53,980 --> 00:19:57,260
and so the weight updates also become quite nice and it's it's

332
00:19:57,260 --> 00:20:00,260
it's a good algorithm if you are going to do logistic regression

333
00:20:00,260 --> 00:20:02,270
and you can work with these large sees

334
00:20:03,660 --> 00:20:06,990
how do we do regular decision for logistic regression all we do

335
00:20:06,990 --> 00:20:09,320
just what way we do it for linear regression

336
00:20:09,470 --> 00:20:12,890
ok we're going to have to put a prior over the parameter

337
00:20:13,550 --> 00:20:19,150
ok so for example we can just do l to regular izations we can just

338
00:20:19,160 --> 00:20:23,250
say on the weight of a that the norm of the weight vector to be small

339
00:20:23,940 --> 00:20:27,330
this means that again we have a sort of quadratic element in the

340
00:20:27,330 --> 00:20:29,700
error and the optimization is easy

341
00:20:30,540 --> 00:20:34,410
it's a little bit harder now to make sense of what's going on in

342
00:20:34,420 --> 00:20:37,760
the sense that this is suggesting a gaussian prior

343
00:20:38,210 --> 00:20:41,250
then the data comes in and we don't have a conjugate set

344
00:20:41,280 --> 00:20:44,400
is not a conjugates we don't really get a conjugate pasteur

345
00:20:44,770 --> 00:20:47,370
there are other things that one can do here error

346
00:20:47,370 --> 00:20:50,590
a different kinds of regular izations that perhaps would be better

347
00:20:50,590 --> 00:20:55,220
matched to binary data case for example you can consider what is

348
00:20:55,220 --> 00:20:58,280
the error what does the error look like for by binary data will

349
00:20:58,280 --> 00:21:01,640
typically the error is not a gaussian type of error

350
00:21:02,390 --> 00:21:06,140
but it's actually flipping the label and so perhaps working with

351
00:21:06,150 --> 00:21:09,780
gaussian here is not ideal however from a practical point of

352
00:21:09,780 --> 00:21:13,060
view this is very nice ok and so that's one of the reasons why

353
00:21:13,060 --> 00:21:14,120
like people use this

354
00:21:17,070 --> 00:21:20,350
now what's probabilistic view of logistic regression just like

355
00:21:20,350 --> 00:21:22,490
we thought about linear regression we thought about this

356
00:21:22,490 --> 00:21:25,870
kind of noise model we can treat logistic regression in the

357
00:21:25,870 --> 00:21:29,060
same point okay so we can consider the output as being produced

358
00:21:29,060 --> 00:21:33,190
by hypothesis for us some noise ok

359
00:21:35,550 --> 00:21:38,530
but this is actually kind of strange

360
00:21:39,450 --> 00:21:42,840
ok again because the output is binary and what does this kind of

361
00:21:42,840 --> 00:21:43,870
model really me

362
00:21:46,490 --> 00:21:51,140
so instead what we will do is we will consider a continuous variable

363
00:21:51,420 --> 00:21:55,750
ok why hot that's produced by the hypothesis class of noise

364
00:21:56,220 --> 00:21:59,900
and then going to consider the output being generated by thresholding

365
00:21:59,910 --> 00:22:03,980
this variable ok at zero so we're going to generate one if this

366
00:22:03,990 --> 00:22:08,570
variables positive and zero otherwise and so in this case we

367
00:22:08,580 --> 00:22:11,750
actually obtain a nice probabilistic model

368
00:22:12,050 --> 00:22:15,430
for logistic regression now have the

369
00:22:15,610 --> 00:22:19,230
the graphical models of this more complicated we have the inputs

370
00:22:20,560 --> 00:22:23,480
we have epsilon these are used to go

371
00:22:23,830 --> 00:22:27,530
into this sort of y i have a variable

372
00:22:27,740 --> 00:22:33,870
which is then used to generate want ok

373
00:22:35,920 --> 00:22:39,620
so again there's this sort of notion of of a latent variable somewhere

374
00:22:39,620 --> 00:22:42,260
in there that we don't actually get to observe

375
00:22:42,470 --> 00:22:46,420
but that controls the output gets now the other interesting thing

376
00:22:46,420 --> 00:22:48,700
about this kind of interpretation is that it directly

377
00:22:48,700 --> 00:22:52,870
shows you that logistic regression is not a generative model

378
00:22:53,140 --> 00:22:55,750
in other words if i just consider the inputs

379
00:22:56,080 --> 00:22:58,530
and the outputs is not a good weight for

380
00:22:58,680 --> 00:23:01,580
for me to generate what this y hacked might have been

381
00:23:02,620 --> 00:23:06,050
ok and so there are other models other probabilistic models

382
00:23:06,050 --> 00:23:09,490
that are fully probabilistic that allow you to put to relate these

383
00:23:09,490 --> 00:23:11,480
latent variables in in nice ways

384
00:23:15,530 --> 00:23:18,820
ok so we're going to do recap now little bit

385
00:23:19,280 --> 00:23:22,350
we talked about machine learning algorithms hopefully this

386
00:23:22,350 --> 00:23:25,040
was more of a refresher if you've seen this now for the first

387
00:23:25,040 --> 00:23:27,050
time sure it's like completely with by

388
00:23:28,430 --> 00:23:31,600
so so whenever you have a machine learning algorithm have to

389
00:23:31,600 --> 00:23:34,480
think of making a choice of a hypothesis

390
00:23:34,620 --> 00:23:38,270
a choice of an error function and choice of an optimization procedure

391
00:23:39,380 --> 00:23:42,980
very often we're going to make gradient descent kind of optimization

392
00:23:42,980 --> 00:23:45,780
procedure just because they're convenient and they work

393
00:23:45,780 --> 00:23:51,390
sort of in many many cases in some cases this optimization is easy

394
00:23:51,390 --> 00:23:54,020
and in closed form in almost all the case you're going to see

395
00:23:54,020 --> 00:23:56,680
for the rest of the week this is not the case in going to have

396
00:23:56,680 --> 00:23:59,040
to work harder to to get that

397
00:24:01,050 --> 00:24:04,890
and all the algorithms are affected by this kind of bias-variance

398
00:24:04,900 --> 00:24:09,300
trade-off so the overfitting fun common-ion is always a concern

399
00:24:09,300 --> 00:24:12,450
you always have to do cross validation you always have to make

400
00:24:12,450 --> 00:24:15,580
sure that you're not building a model that's too large for the amount

401
00:24:15,580 --> 00:24:19,290
of data that you have a you could do regular sation to control

402
00:24:19,300 --> 00:24:21,620
that you can put priors on hypothesis space

403
00:24:21,620 --> 00:24:25,000
and i should be a little bit of this patient interpretation i'm not

404
00:24:25,000 --> 00:24:27,610
sure how much you'll see of this for the rest of the week but

405
00:24:27,610 --> 00:24:30,730
it's a you're body graph that can scale

406
00:24:30,770 --> 00:24:33,470
linear away was t

407
00:24:33,520 --> 00:24:34,470
so you can

408
00:24:34,480 --> 00:24:38,000
make a mistake almost every time

409
00:24:38,010 --> 00:24:40,030
and here is

410
00:24:40,090 --> 00:24:43,430
a simple example showing why this is so

411
00:24:43,550 --> 00:24:47,480
so examples are drawn independently from

412
00:24:47,520 --> 00:24:49,440
this distribution d

413
00:24:49,460 --> 00:24:55,060
which has supported only examples with equal probability

414
00:24:55,110 --> 00:24:58,950
and you have you want to compete policies

415
00:24:58,970 --> 00:25:04,880
that predict constant actions so you have buy one predict and some action a one

416
00:25:05,020 --> 00:25:09,430
and by to predict and some action e two

417
00:25:09,560 --> 00:25:11,610
have just examples

418
00:25:11,630 --> 00:25:16,860
and this is the these are the words and the words are fixed

419
00:25:18,390 --> 00:25:24,450
if you draw x one which is the probability one half than action one has

420
00:25:24,460 --> 00:25:29,720
both actions happening reworded some small reward see zero point one

421
00:25:29,730 --> 00:25:31,960
and if you don't act still

422
00:25:31,970 --> 00:25:34,490
the action

423
00:25:34,510 --> 00:25:36,960
two has a large reward

424
00:25:36,960 --> 00:25:40,960
of one and action one has no really work

425
00:25:40,970 --> 00:25:43,860
so with probability one half

426
00:25:43,910 --> 00:25:47,010
x one is drawn

427
00:25:47,020 --> 00:25:48,380
and then

428
00:25:48,430 --> 00:25:49,450
the learning

429
00:25:49,480 --> 00:25:54,620
he goes either pi one or PI two problem randomizes because there's is no basis

430
00:25:54,620 --> 00:25:59,280
for choosing otherwise so seeds it chooses pi one

431
00:25:59,340 --> 00:26:02,270
and observes the words zero point one

432
00:26:02,290 --> 00:26:06,970
and then from this point on there will always choose

433
00:26:06,980 --> 00:26:08,800
by one because

434
00:26:08,830 --> 00:26:10,960
the sum of the board

435
00:26:10,970 --> 00:26:15,060
zero point one and it will never explore

436
00:26:15,090 --> 00:26:17,650
action two

437
00:26:17,690 --> 00:26:21,450
because this empirical some of words will always be higher

438
00:26:21,470 --> 00:26:28,710
for a one

439
00:26:28,720 --> 00:26:31,120
right that will be comparing

440
00:26:31,140 --> 00:26:33,320
a one a two

441
00:26:33,380 --> 00:26:37,930
the dome of the sum of rewards will be zero point one four one

442
00:26:37,940 --> 00:26:41,280
because it has previously been chosen

443
00:26:41,310 --> 00:26:46,930
and since it too was never chosen in the past to have

444
00:26:46,970 --> 00:26:49,410
zero as it some of the word

445
00:26:49,620 --> 00:26:52,800
and i don't have to be chosen

446
00:26:53,490 --> 00:26:56,240
the learner will never discovered this one

447
00:26:56,240 --> 00:27:00,640
and it will not compete it will have

448
00:27:00,820 --> 00:27:06,870
you add that scales linearly with the number of steps

449
00:27:06,910 --> 00:27:08,750
so this is about

450
00:27:08,770 --> 00:27:13,530
let's modify the simple longer from

451
00:27:16,170 --> 00:27:19,530
chosen randomly by exploring in the first of all

452
00:27:20,570 --> 00:27:25,930
but just choose random actions for the first tau rounds and observe there

453
00:27:27,560 --> 00:27:29,640
and then find

454
00:27:29,640 --> 00:27:32,080
the policy that is

455
00:27:32,100 --> 00:27:36,810
the leader on the first on the exploration data on the first tau rounds and

456
00:27:36,810 --> 00:27:39,650
then just go with that policy for the remaining

457
00:27:50,950 --> 00:27:57,750
it's i suppose that again examples are drawn independently from a fixed distribution d over

458
00:27:57,770 --> 00:28:02,900
acts grant zero one to the to reward doctors so access and the words are

459
00:28:02,900 --> 00:28:08,550
generated from some distribution and then we can show that for this modified

460
00:28:08,560 --> 00:28:13,050
algorithm regret skills sublinearly

461
00:28:13,400 --> 00:28:17,230
two million two thirds

462
00:28:17,420 --> 00:28:21,260
and here's the dependence on the number of actions

463
00:28:21,270 --> 00:28:23,710
and the

464
00:28:24,270 --> 00:28:28,420
number of policies that are competing with

465
00:28:28,420 --> 00:28:30,820
and you can use standard

466
00:28:30,880 --> 00:28:36,560
arguments to substitute that by the BBC dimension of the set

467
00:28:36,620 --> 00:28:42,380
your computer network

468
00:28:42,390 --> 00:28:47,370
so you can do better than you can have a nontrivial statements saying that you

469
00:28:48,290 --> 00:28:52,640
there is some learning going on because you regret this sublinear

470
00:28:52,650 --> 00:28:57,020
in c

471
00:28:57,080 --> 00:28:58,370
in the preface

472
00:28:58,390 --> 00:29:00,390
very simple

473
00:29:00,400 --> 00:29:04,770
so let's look looking at this quantity

474
00:29:04,780 --> 00:29:06,730
was some and over

475
00:29:06,730 --> 00:29:09,020
the examples

476
00:29:09,050 --> 00:29:10,440
so acts

477
00:29:10,480 --> 00:29:13,440
action taken the word of that action

478
00:29:14,050 --> 00:29:16,800
and this is the indicator function

479
00:29:16,820 --> 00:29:18,230
it tells

480
00:29:18,270 --> 00:29:22,890
whether this one one the policy agrees with the action taken so it's similar to

481
00:29:22,890 --> 00:29:24,910
the qualitative look at

482
00:29:24,910 --> 00:29:28,530
before except that there is this key

483
00:29:28,550 --> 00:29:31,510
in this case here because you weighted by

484
00:29:31,520 --> 00:29:37,480
the probability of taking that action four and the exploration was uniform

485
00:29:37,520 --> 00:29:41,320
so you're waiting instead of summing the words

486
00:29:41,350 --> 00:29:44,970
over the rounds where the policy agrees with the action

487
00:29:44,970 --> 00:29:50,590
you're some in importance weighted estimates of the word so this is your

488
00:29:52,020 --> 00:29:56,250
then you divide by the probability of taking that action

489
00:29:56,260 --> 00:29:58,460
so you divide by one of our key

490
00:29:58,460 --> 00:30:00,930
which means multiplying by

491
00:30:00,980 --> 00:30:05,880
and we'll see how this is important

492
00:30:05,880 --> 00:30:08,730
so instead of some in the words

493
00:30:08,740 --> 00:30:14,430
over the rounds were the indicator function is true you're some in

494
00:30:14,470 --> 00:30:19,030
the you some way that rewards weighted by the probability

495
00:30:19,060 --> 00:30:20,830
of taking the action

496
00:30:20,850 --> 00:30:22,240
and so

497
00:30:22,250 --> 00:30:25,530
if you divide that by time this is just

498
00:30:25,650 --> 00:30:31,740
an empirical estimate of the reward weighted estimate of the war and you can use

499
00:30:31,760 --> 00:30:35,920
standard deviation barnsley half-timbered two

500
00:30:35,970 --> 00:30:44,430
by and the deviation from the expectation with respect to this underlying distribution

501
00:30:44,480 --> 00:30:49,990
so this is the this is is perhaps by how much it can deviate from

502
00:30:49,990 --> 00:30:57,730
the true expectation simultaneously for all policies pi with high probability with probability one minus

503
00:31:01,620 --> 00:31:05,380
right so that bounds the regret let's see you

504
00:31:05,390 --> 00:31:10,690
we can bound the regret on their exploration around by now

505
00:31:10,710 --> 00:31:15,010
by saying that we've got this regret can be at most one in every round

506
00:31:15,010 --> 00:31:16,410
so let's say it

507
00:31:16,460 --> 00:31:19,310
for the bound and then

508
00:31:20,040 --> 00:31:24,890
c minor style which is certainly upper bounded by c

509
00:31:24,890 --> 00:31:28,870
the remaining rounds and this is by how much you can deviate

510
00:31:30,140 --> 00:31:34,180
the true expectation

511
00:31:34,190 --> 00:31:36,410
so this is by how much

512
00:31:36,430 --> 00:31:38,350
how much you p four

513
00:31:38,370 --> 00:31:39,620
going with this

514
00:31:39,630 --> 00:31:43,680
an empirical leader versus the true leader

515
00:31:43,800 --> 00:31:47,120
you've selected on the basis of the exploration around

516
00:31:47,140 --> 00:31:49,470
and then you just optimise

517
00:31:49,490 --> 00:31:51,000
tell you find

518
00:31:51,010 --> 00:31:54,790
it out to minimize this upper bound

519
00:31:54,790 --> 00:31:56,150
the information content

520
00:31:59,240 --> 00:32:01,160
this suggests that really improbable

521
00:32:02,770 --> 00:32:07,990
and smallest things that was written about the fact that something is definitely don't happen if you had probability one

522
00:32:08,480 --> 00:32:09,470
that's got zero

523
00:32:10,140 --> 00:32:11,040
information content

524
00:32:11,780 --> 00:32:12,730
the at probability

525
00:32:14,640 --> 00:32:15,340
then it's got

526
00:32:16,890 --> 00:32:18,640
and information content of one bit

527
00:32:22,080 --> 00:32:24,170
if something happened about the probability

528
00:32:25,040 --> 00:32:25,640
no why

529
00:32:29,210 --> 00:32:29,770
for example

530
00:32:30,570 --> 00:32:32,760
getting tale from colonies

531
00:32:34,210 --> 00:32:34,870
that's got

532
00:32:40,600 --> 00:32:45,250
that's the information content of not to one hundred twenty nine

533
00:32:46,280 --> 00:32:49,980
which is not hard quantisation test when you get

534
00:32:50,790 --> 00:32:51,670
going to gain

535
00:32:52,460 --> 00:32:54,420
fifteen hundreds a bit

536
00:32:55,260 --> 00:32:58,300
well many exciting things happen the rare events

537
00:33:05,370 --> 00:33:07,350
the catalog-based to all over the place

538
00:33:09,620 --> 00:33:11,740
this which is three point three

539
00:33:18,030 --> 00:33:18,610
he says

540
00:33:19,090 --> 00:33:21,480
every had everyone in this file on the screen

541
00:33:22,720 --> 00:33:25,960
is going through the bit bits the information content

542
00:33:28,190 --> 00:33:32,710
so it claiming this is the right way to measure information content i don't know

543
00:33:33,200 --> 00:33:38,170
it's the compressed file length that we should aspire to someone when we invent compression

544
00:33:42,120 --> 00:33:42,830
all x

545
00:33:50,640 --> 00:33:51,150
while like

546
00:33:53,140 --> 00:33:54,680
to which we should ask why

547
00:34:10,740 --> 00:34:12,250
okay so shannon's play

548
00:34:12,870 --> 00:34:14,240
and kind of cooling this

549
00:34:14,820 --> 00:34:16,420
the shannon information content

550
00:34:17,360 --> 00:34:18,490
until convinced that at

551
00:34:18,910 --> 00:34:22,320
and then i'll stop using the name of the school in the information content because

552
00:34:22,320 --> 00:34:24,300
we will have established that really is

553
00:34:24,980 --> 00:34:25,500
the right way

554
00:34:25,960 --> 00:34:26,320
to measure

555
00:34:32,380 --> 00:34:34,270
let's not do so

556
00:34:37,500 --> 00:34:38,010
which we do

557
00:34:41,770 --> 00:34:44,140
let's just make some observations about these

558
00:34:45,330 --> 00:34:46,910
the shannon information content

559
00:34:54,470 --> 00:34:55,090
five one

560
00:34:56,040 --> 00:34:58,500
this side information content is additive

561
00:35:03,620 --> 00:35:04,490
random variables

562
00:35:15,470 --> 00:35:16,640
what i mean by this

563
00:35:17,520 --> 00:35:20,160
okay this is really shorthand for an outcome so

564
00:35:20,740 --> 00:35:21,720
i put outcome in there

565
00:35:23,780 --> 00:35:25,360
so actions at age equal

566
00:35:29,720 --> 00:35:30,810
i'll try and be

567
00:35:32,190 --> 00:35:34,050
the notation but if you get to be

568
00:35:34,470 --> 00:35:36,470
notation that's not fun anymore so

569
00:35:37,130 --> 00:35:38,540
i'm gonna hope that

570
00:35:39,110 --> 00:35:41,200
we clear enough without being too obsessive

571
00:35:47,980 --> 00:35:52,630
the information content the shannon information content is additive for independent random variables

572
00:35:53,400 --> 00:35:54,290
for example

573
00:35:55,520 --> 00:35:56,630
in fact documents on the

574
00:35:57,200 --> 00:35:57,650
that's why

575
00:35:58,580 --> 00:35:59,170
we will be

576
00:36:00,300 --> 00:36:02,020
from a single rule is the hat

577
00:36:03,020 --> 00:36:04,650
the random variables x

578
00:36:05,830 --> 00:36:08,040
with probability distribution that's why

579
00:36:08,680 --> 00:36:09,240
this about

580
00:36:10,240 --> 00:36:11,570
times fewer i

581
00:36:15,530 --> 00:36:17,760
right that's the definition is independent

582
00:36:32,420 --> 00:36:32,900
if we do

583
00:36:33,180 --> 00:36:34,320
and so the land

584
00:36:35,760 --> 00:36:39,640
the shannon information content of the random variables taking on particular

585
00:36:40,160 --> 00:36:44,610
values and i use the shorthand age action might not means the outcome x being

586
00:36:44,610 --> 00:36:45,290
one of the

587
00:36:52,600 --> 00:36:55,420
i mean to

588
00:36:57,110 --> 00:37:01,310
find that

589
00:37:01,350 --> 00:37:03,580
rows and columns of the matrix

590
00:37:06,270 --> 00:37:09,150
so it's about

591
00:37:09,180 --> 00:37:11,520
no matrix factorisation

592
00:37:53,930 --> 00:37:58,550
the next step

593
00:37:58,610 --> 00:38:01,710
is so

594
00:38:29,800 --> 00:38:33,600
all these

595
00:38:33,600 --> 00:38:38,610
sure there will be more in the context of knowledge about

596
00:38:47,140 --> 00:38:50,120
so that

597
00:38:55,900 --> 00:38:56,890
no more

598
00:38:59,400 --> 00:39:03,870
this was or

599
00:39:03,960 --> 00:39:09,550
what's more

600
00:39:10,180 --> 00:39:13,770
in this before

601
00:39:13,870 --> 00:39:16,330
the last

602
00:39:27,610 --> 00:39:29,800
time was

603
00:39:50,640 --> 00:39:56,230
one is

604
00:40:01,180 --> 00:40:02,900
so we get

605
00:40:04,250 --> 00:40:06,250
four was

606
00:40:06,270 --> 00:40:08,920
and he he

607
00:40:57,010 --> 00:40:59,820
in the after the war

608
00:41:05,660 --> 00:41:09,140
that's when

609
00:41:14,640 --> 00:41:20,240
one twenty

610
00:41:43,300 --> 00:41:47,120
i'm not just because you don't

611
00:41:47,430 --> 00:41:51,100
the whole space

612
00:41:51,110 --> 00:41:53,060
twenty five

613
00:41:53,130 --> 00:41:56,790
this problem

614
00:41:56,800 --> 00:42:00,800
this is

615
00:42:03,280 --> 00:42:10,970
one is

616
00:42:10,980 --> 00:42:15,570
so will

617
00:42:23,860 --> 00:42:25,310
this should

618
00:42:36,540 --> 00:42:42,300
so one

619
00:42:54,540 --> 00:43:04,370
the next one

620
00:43:04,420 --> 00:43:06,540
so what

621
00:43:14,640 --> 00:43:17,860
so what

622
00:43:20,940 --> 00:43:23,370
and then

623
00:43:34,570 --> 00:43:37,700
right it's

624
00:43:40,490 --> 00:43:43,550
i want to see

625
00:43:48,000 --> 00:43:49,350
what's more

626
00:43:51,380 --> 00:43:56,870
was seen from the point

627
00:43:56,870 --> 00:44:00,130
and then solve if primal fix equals zero

628
00:44:01,030 --> 00:44:05,620
the solve the linear system of equations you thereby eliminating its but since then no

629
00:44:05,620 --> 00:44:08,770
constraints on the lagrange multipliers everything goes away

630
00:44:08,870 --> 00:44:14,790
so the unconstrained problem the lagrange function will be just the unconstrained objective anyway

631
00:44:14,840 --> 00:44:21,900
this is just a way of incorporating the constraints explicitly

632
00:44:24,360 --> 00:44:26,380
if you do this

633
00:44:26,390 --> 00:44:27,910
you will get

634
00:44:27,920 --> 00:44:29,730
this optimisation problem that you

635
00:44:29,810 --> 00:44:34,370
i will explain it in a much simpler case later on their way novelty detection

636
00:44:34,420 --> 00:44:37,090
so at the moment

637
00:44:37,140 --> 00:44:40,960
don't worry that much about it but basically what it is

638
00:44:41,000 --> 00:44:43,170
this optimisation problem

639
00:44:43,360 --> 00:44:48,420
let me explain the various bits and pieces out five j of lagrange multipliers

640
00:44:48,470 --> 00:44:51,230
y i y j x i x j

641
00:44:51,240 --> 00:44:53,290
basically the kernel matrix

642
00:44:53,340 --> 00:44:56,340
and here are just the lagrange multipliers game

643
00:44:56,390 --> 00:44:57,960
now what happened is

644
00:44:57,960 --> 00:44:59,350
i have considerably more

645
00:44:59,370 --> 00:45:02,760
complicated objective function

646
00:45:02,790 --> 00:45:04,420
lots of lagrange multipliers

647
00:45:04,440 --> 00:45:06,640
but i've got very simple constraints

648
00:45:06,690 --> 00:45:10,010
you just have to constraint that our y y equals zero

649
00:45:10,010 --> 00:45:11,530
which arises from

650
00:45:11,550 --> 00:45:13,580
the constant offset b

651
00:45:13,640 --> 00:45:17,660
but the outline being great people in here that just happens because they are lagrange

652
00:45:21,150 --> 00:45:22,550
as we will see

653
00:45:22,550 --> 00:45:24,370
that is a linear combination

654
00:45:24,390 --> 00:45:26,330
of the excise

655
00:45:26,340 --> 00:45:27,830
and so the african right there

656
00:45:28,500 --> 00:45:30,580
as a linear combination of y y

657
00:45:30,630 --> 00:45:34,310
excited x the constant offset

658
00:45:34,320 --> 00:45:37,780
and finally we have the so-called contact conditions which means

659
00:45:37,890 --> 00:45:39,410
the constraint

660
00:45:39,460 --> 00:45:41,980
conflict and multiply have to avenge

661
00:45:43,850 --> 00:45:48,360
there's a simple example that i can demonstrate will assume this box into besides one

662
00:45:48,360 --> 00:45:50,640
pack of biscuits

663
00:45:50,650 --> 00:45:53,520
it's been a few more but anyway similar empty

664
00:45:54,480 --> 00:45:57,940
the books imposes constraints on my random variable

665
00:45:57,960 --> 00:45:59,470
the biscuits

666
00:45:59,520 --> 00:46:02,650
now that's even assuming this box box were closed

667
00:46:02,670 --> 00:46:07,040
so i've got constraints on all sides it's nice convexity it

668
00:46:07,070 --> 00:46:11,900
the only place where the constraints will exert a force on the biscuit is where

669
00:46:11,920 --> 00:46:13,320
the biscuit actually

670
00:46:14,110 --> 00:46:15,590
the box

671
00:46:15,600 --> 00:46:18,620
so these constraints might as well not be there

672
00:46:18,670 --> 00:46:23,350
so for instance if i have to biscuits and there i can impose additional constraints

673
00:46:23,390 --> 00:46:24,580
using the box

674
00:46:24,590 --> 00:46:28,520
and it's not going to change the location of the biscuit

675
00:46:28,620 --> 00:46:31,310
if i were to change the optimisation problem

676
00:46:31,310 --> 00:46:34,360
and remove this constraint i get a big mess

677
00:46:35,850 --> 00:46:37,240
this is

678
00:46:37,250 --> 00:46:40,660
basically what contact conditions two

679
00:46:42,010 --> 00:46:43,530
ponce force

680
00:46:43,570 --> 00:46:44,760
so those of you have

681
00:46:44,770 --> 00:46:47,270
the answer to the mechanics vol

682
00:46:47,350 --> 00:46:49,500
just classical mechanics at some point

683
00:46:49,540 --> 00:46:52,510
you would have heard of the virtual forces that again

684
00:46:52,520 --> 00:46:56,600
from you know constraint movements of particles

685
00:46:56,680 --> 00:47:02,110
and those virtual forces are exactly those out

686
00:47:02,190 --> 00:47:05,900
those who have not done physics don't worry

687
00:47:08,070 --> 00:47:13,290
who wants to see the prose

688
00:47:13,580 --> 00:47:17,330
who does not want to see the truth

689
00:47:20,410 --> 00:47:24,960
and the reason cao cao the the proof in that case

690
00:47:25,820 --> 00:47:27,040
OK anyway so

691
00:47:27,050 --> 00:47:30,790
his lagrange function it's going to be very easy don't worry

692
00:47:30,800 --> 00:47:33,920
and all you need to do is you need to take derivatives with respect to

693
00:47:33,920 --> 00:47:36,660
w and b

694
00:47:36,680 --> 00:47:41,320
so it is with respect to w one one half the square w is just

695
00:47:41,320 --> 00:47:42,130
w you

696
00:47:42,220 --> 00:47:43,650
it's easy

697
00:47:43,660 --> 00:47:45,260
and here

698
00:47:46,320 --> 00:47:50,300
this expression here to accuse me alpha i y i x i

699
00:47:50,350 --> 00:47:54,330
everything else disappears so that this appears to disappear

700
00:47:54,440 --> 00:47:59,530
so w gives me is somewhere else y y axis

701
00:47:59,530 --> 00:48:01,750
this is exactly what we have here

702
00:48:06,190 --> 00:48:07,800
db of failed

703
00:48:07,850 --> 00:48:09,360
just gives me

704
00:48:09,980 --> 00:48:13,800
now he is the type of lesion across that are in your hand

705
00:48:13,850 --> 00:48:16,140
some of alpha y i

706
00:48:16,150 --> 00:48:19,640
what happens it just cut-and-paste from the max

707
00:48:19,690 --> 00:48:24,800
so what you get out of this this summation constraint some of alpha y equals

708
00:48:26,580 --> 00:48:29,760
so what you can see is actually the free variables

709
00:48:29,820 --> 00:48:32,680
it will eat equality constraints

710
00:48:32,740 --> 00:48:35,110
by the same token

711
00:48:35,130 --> 00:48:40,330
constrained variables will only lead to constraints to inequality constraints

712
00:48:40,330 --> 00:48:43,460
equality constraints will lead to free variables in the field

713
00:48:43,510 --> 00:48:47,600
so that's the so-called theory

714
00:48:49,210 --> 00:48:53,230
now once you can plug of those terms back in here

715
00:48:53,280 --> 00:48:54,460
you will get

716
00:48:54,470 --> 00:48:58,470
the objective function that i showed you

717
00:48:58,480 --> 00:49:03,810
so we would go and maximizes beast but of course rather maximizing the negative square

718
00:49:03,860 --> 00:49:06,510
we can also minimizes way

719
00:49:06,580 --> 00:49:10,330
that's the only change that made

720
00:49:10,360 --> 00:49:14,110
the the proof wasn't that hard was

721
00:49:14,280 --> 00:49:18,230
now why is this really cool

722
00:49:19,350 --> 00:49:22,980
this expression here is really independent of dimensionality physics

723
00:49:23,020 --> 00:49:28,850
the same idea is what we saw in the perceptron here we can see parisian

724
00:49:28,910 --> 00:49:31,300
and the other thing that we see is that there will be only depends on

725
00:49:31,300 --> 00:49:33,260
basically is the pile of

726
00:49:33,280 --> 00:49:36,790
other points which all of them correspond to the

727
00:49:36,840 --> 00:49:41,390
brown point here correspond to all of the points on the brown clustering brown ring

728
00:49:41,770 --> 00:49:44,120
the green one correspond to all the points

729
00:49:44,140 --> 00:49:45,390
and the green ring

730
00:49:45,400 --> 00:49:50,300
and the same for the blue one for clustering this dataset

731
00:49:50,310 --> 00:49:51,610
the rules of

732
00:49:51,620 --> 00:49:58,960
of view of the sorry with k means is trivial simply phi three points in

733
00:49:58,980 --> 00:50:02,970
in in these locations and it became is its immediate this like a means and

734
00:50:02,980 --> 00:50:07,440
the role of this of this idea mean pictures now doing the same thing with

735
00:50:07,440 --> 00:50:12,430
images what wanted it is this is one way of defining an affinity an affinity

736
00:50:12,430 --> 00:50:16,860
matrix or graph in an image is what it is they

737
00:50:16,860 --> 00:50:18,090
took this image

738
00:50:18,110 --> 00:50:23,700
and for this specific experiment they used an edge to edge detector to find an

739
00:50:23,700 --> 00:50:26,970
edges and they use different features of each pixel

740
00:50:26,990 --> 00:50:31,320
for example the use the information they got from asian vector and the used RGB

741
00:50:31,340 --> 00:50:36,200
or the grayscale information they got from the image itself and they built the graph

742
00:50:36,240 --> 00:50:39,930
fast cry for the connected neighbouring pixels

743
00:50:39,950 --> 00:50:44,360
and the value of the edge depends both on the value of the pixels

744
00:50:44,390 --> 00:50:47,550
and from the information they got from the edge detectors

745
00:50:47,680 --> 00:50:53,160
the kind of of segmentation they get these four think six and seven segments

746
00:50:53,180 --> 00:50:56,820
this is the kind of results so they get when the news segment images with

747
00:50:56,820 --> 00:51:00,300
their technique with this with this technique

748
00:51:00,930 --> 00:51:06,860
in questions

749
00:51:14,570 --> 00:51:19,640
it here

750
00:51:25,700 --> 00:51:33,010
what is the k means

751
00:51:33,050 --> 00:51:35,430
o this is something that

752
00:51:35,450 --> 00:51:40,320
they just suggested hand

753
00:51:40,410 --> 00:51:45,840
i think it was in in jordan was in this paper the first suggested use

754
00:51:45,840 --> 00:51:51,300
the k means and then they simply kept on doing it in other papers and

755
00:51:51,300 --> 00:51:52,970
if you look at the way

756
00:51:52,990 --> 00:51:54,470
that investors are

757
00:51:54,490 --> 00:51:58,300
in the in the i can because basically you could call it if you look

758
00:51:58,300 --> 00:52:01,360
if you look at that conductors then k means is not really a reasonable thing

759
00:52:01,390 --> 00:52:05,780
that you could do something more interesting than that but this is simple

760
00:52:05,820 --> 00:52:09,610
and you don't want to complicate algorithm more than that and i'm not sure that

761
00:52:09,610 --> 00:52:15,880
it's it after doing all these all this computation much of it's justifiable to do

762
00:52:15,890 --> 00:52:20,340
other the other other algorithms in the in the i can vector space you could

763
00:52:20,340 --> 00:52:21,720
for example do

764
00:52:21,740 --> 00:52:26,330
like i mean why not going to look right or something that simply itself if

765
00:52:26,330 --> 00:52:29,800
you if you like if you know the k clusters and i can space you

766
00:52:29,800 --> 00:52:32,860
could do against the clustering on that in the space

767
00:52:33,110 --> 00:52:37,390
but we know what the founder what you could put to good experiment enough is

768
00:52:37,390 --> 00:52:42,110
the k means often works good enough even if it something else in order to

769
00:52:42,110 --> 00:52:46,320
get better results in the don't really have any good explanation to why we need

770
00:52:46,320 --> 00:52:48,640
to do

771
00:52:53,640 --> 00:52:57,410
they can better themselves

772
00:52:57,510 --> 00:53:03,180
so this is what is what seen them in the that conductors themselves are the

773
00:53:03,180 --> 00:53:06,090
ones that you need to look at but then came in a similar way to

774
00:53:06,090 --> 00:53:09,220
looking for clusters in if you stack all of that this is one of the

775
00:53:09,220 --> 00:53:12,470
other you could lead to things but you could also could also look at each

776
00:53:12,470 --> 00:53:17,280
of the conductors myself i'm going to talk about it in the second and the

777
00:53:17,280 --> 00:53:20,140
third one in the last one of them going here is is

778
00:53:20,140 --> 00:53:24,010
the one this is the first one actually was published in two thousand here the

779
00:53:24,010 --> 00:53:25,640
metrics to use is

780
00:53:25,640 --> 00:53:27,910
the mine w

781
00:53:28,930 --> 00:53:33,490
and they don't normalise it here but i can probably solve is this one year

782
00:53:33,590 --> 00:53:35,890
if this is and the platinum

783
00:53:35,910 --> 00:53:39,640
the miners w and i can make it so that the problem is solved this

784
00:53:40,530 --> 00:53:44,450
and if we multiply by eighteen months one we get basically the minus one lv

785
00:53:44,470 --> 00:53:46,780
equals lambda v

786
00:53:46,800 --> 00:53:51,160
so even though it is presented like this and then present in the same year

787
00:53:51,160 --> 00:53:56,800
it is the normalized laplacian problems that we're looking for looking at so they define

788
00:53:56,840 --> 00:54:01,280
this matrix demons w they computer i can vectors and they do pretty much the

789
00:54:01,280 --> 00:54:05,470
same thing as male and she did in the in the in the in the

790
00:54:05,470 --> 00:54:10,180
paper i just mentioned before which means that they called k i vectors and they

791
00:54:11,410 --> 00:54:14,550
k means on the rows of this matrix

792
00:54:14,570 --> 00:54:18,490
and the kind of examples they show here is that if they take

793
00:54:18,490 --> 00:54:22,530
this image which is the noisy step on it and the

794
00:54:22,550 --> 00:54:24,220
the graph here

795
00:54:24,240 --> 00:54:27,490
the graph here again each pixel is the name of the graph to compute an

796
00:54:27,490 --> 00:54:31,910
idea to you basically get convicted tells you value it gives you a value to

797
00:54:31,910 --> 00:54:33,120
each of the nodes

798
00:54:33,140 --> 00:54:35,890
so you can plot bag vector

799
00:54:35,930 --> 00:54:40,950
it's an image as well if this is the original images the second largest

800
00:54:40,970 --> 00:54:46,860
the second smallest sorry good idea like correspond to the second smallest eigenvalue and if

801
00:54:46,860 --> 00:54:50,410
you look a means and this kind of aiken victory

802
00:54:50,410 --> 00:54:52,240
this is the competition and you get

803
00:54:52,240 --> 00:54:57,550
the same thing with three segments in this image gives you one two and three

804
00:54:57,570 --> 00:55:01,860
segments if you look at the first three eigen vectors of the k means

805
00:55:01,930 --> 00:55:03,930
on them

806
00:55:03,950 --> 00:55:10,390
and said it looks say pretty much the same thing this normalisation or normalisation so

807
00:55:10,410 --> 00:55:13,890
this is what this light is about they are very closely related

808
00:55:13,910 --> 00:55:18,890
if you look at the first one that they introduced into the wise we look

809
00:55:18,890 --> 00:55:20,340
we see this kind of

810
00:55:20,360 --> 00:55:25,140
matrix the matrix that the problem that in a second problem is always is that

811
00:55:25,780 --> 00:55:29,930
four male and she was sold this one unfortunately the third one

812
00:55:30,140 --> 00:55:33,110
now let's see what we can do here all can we move from different that

813
00:55:33,110 --> 00:55:38,340
one if you look at the page and the njw the first one

814
00:55:38,360 --> 00:55:42,010
i can problem and we multiply both sides by dt the manifold of we get

815
00:55:42,010 --> 00:55:43,360
this expression here

816
00:55:43,950 --> 00:55:46,380
and this is pretty much the same

817
00:55:46,410 --> 00:55:51,340
is that expression if we define the to be

818
00:55:51,360 --> 00:55:54,910
the minds of you so if we define beta beta minus one

819
00:55:54,930 --> 00:55:59,990
but it myself you we get this fixed this problem and that problem to be

820
00:56:01,320 --> 00:56:04,180
and if we look at it

821
00:56:04,200 --> 00:56:06,070
additionally problem which is

822
00:56:06,090 --> 00:56:11,720
this is most processing problems and we something similar we get that lambda which is

823
00:56:11,720 --> 00:56:13,950
that the value of male and she

824
00:56:14,050 --> 00:56:18,860
equal to one minus like my one minus mu were used again by the austrian

825
00:56:18,860 --> 00:56:25,180
malik so simply on a mirror image of all of that values each item should

826
00:56:25,180 --> 00:56:27,880
say each i value your

827
00:56:28,070 --> 00:56:30,510
between minus one in one

828
00:56:30,530 --> 00:56:33,160
so taking one minus

829
00:56:33,200 --> 00:56:35,840
simply gives us a mirror image

830
00:56:35,840 --> 00:56:40,140
one of the of the order of about this is why taking the largest because

831
00:56:40,140 --> 00:56:43,140
i vectors corresponding to the largest values here

832
00:56:43,180 --> 00:56:47,410
we're taking the one corresponds to the smallest here gives you the same i can

833
00:56:47,410 --> 00:56:49,820
which at the end

834
00:56:54,780 --> 00:56:58,390
so they are very closely related and you can move from each if you know

835
00:56:58,390 --> 00:57:01,340
the solution of one of them you can very easily move the solution of the

836
00:57:01,340 --> 00:57:04,530
other one the only thing that we need to keep in mind is that if

837
00:57:04,530 --> 00:57:09,080
you want to use enjoy NYC to do this extra normalisation of roles in diagonal

838
00:57:09,080 --> 00:57:11,700
vectors which you don't need to do

839
00:57:11,820 --> 00:57:15,030
and the reason that you don't need to do that is exactly this which implication

840
00:57:16,860 --> 00:57:20,280
if we is the normalisation of rows of u

841
00:57:20,280 --> 00:57:24,590
we're trying to predict are not just is not just a single real value or

842
00:57:24,750 --> 00:57:31,240
simple binary number but is something that's more complex in structure and in some sense

843
00:57:31,430 --> 00:57:35,900
so we could think about you know sequences strings trees and graphs and what have

844
00:57:35,900 --> 00:57:39,290
you that we now see on the output side cases and i'm not talking about

845
00:57:39,290 --> 00:57:43,350
the input side you know where we can define give course kernels over strings and

846
00:57:43,350 --> 00:57:46,650
trees and graphs and the like but i talk about the output side we want

847
00:57:46,650 --> 00:57:50,060
to predict the tree we want to predict a sequence

848
00:57:50,060 --> 00:57:55,500
OK and the interesting thing is that you know typically we so we have multiple

849
00:57:55,500 --> 00:58:00,500
response variables and his response variables are not it would be very suboptimal in many

850
00:58:00,500 --> 00:58:05,960
cases to treat them independently right we don't want to make independent predictions but rather

851
00:58:06,760 --> 00:58:11,640
interdependency between the outputs into account so we would really want to look at the

852
00:58:11,640 --> 00:58:13,660
whole prediction we make

853
00:58:13,670 --> 00:58:19,080
right and not just to predict bit by bit in the way independently

854
00:58:19,090 --> 00:58:20,870
so the question is how can we

855
00:58:20,880 --> 00:58:26,720
how can we use state-of-the-art machine learning problems to generalize to to that family of

856
00:58:26,720 --> 00:58:32,560
problems and the idea here is that we actually combine support vector machines with markov

857
00:58:32,560 --> 00:58:37,010
random field ultimately but i will not really talk much about markov random fields here

858
00:58:37,350 --> 00:58:41,430
but still try to get the main idea across

859
00:58:41,450 --> 00:58:45,980
so whether these problems show up just to you know provide some motivation and background

860
00:58:45,980 --> 00:58:52,750
someone is think about problems like handwritten character recognition right very often people treated the

861
00:58:52,750 --> 00:58:56,210
way that they say well i have a window of a particular character and then

862
00:58:56,210 --> 00:58:59,830
i try to predict what the letter is but that's really not what you typically

863
00:58:59,830 --> 00:59:03,650
have in practice because in practice you often have a sequence of letters and you

864
00:59:03,650 --> 00:59:08,840
want you know and handwritten words and you want to predict the sequence of letters

865
00:59:09,110 --> 00:59:10,440
and there are some

866
00:59:10,550 --> 00:59:15,180
regularities in a language like english for instance you know which letters are likely to

867
00:59:15,300 --> 00:59:18,600
appear together right which has to do with genetics and so on and so forth

868
00:59:19,160 --> 00:59:24,010
so it's not completely independent for instance here you know this letter he could also

869
00:59:24,010 --> 00:59:25,070
be in the e

870
00:59:25,080 --> 00:59:27,030
right or it could be a c

871
00:59:27,040 --> 00:59:30,060
but it makes more sense to treat it as i see because then you know

872
00:59:30,060 --> 00:59:31,420
this word would

873
00:59:31,450 --> 00:59:35,840
because not yours is something like it so that the context provides a lot of

874
00:59:37,140 --> 00:59:41,330
there are other things like protein folding

875
00:59:41,340 --> 00:59:44,990
where what we might want to do right this is of course i'm not saying

876
00:59:44,990 --> 00:59:48,480
that with these methods you can actually solve the protein folding problem but just to

877
00:59:48,480 --> 00:59:52,250
give you some idea right at some point we would like to take a sequence

878
00:59:52,250 --> 00:59:54,150
of amino acids like this

879
00:59:54,170 --> 00:59:57,390
and predict certain things so for instance you know something

880
00:59:57,400 --> 01:00:02,450
part of the secondary structure or perhaps even the serious struck structure of the protein

881
01:00:02,460 --> 01:00:06,910
so the the input is a sequence and the output is also perhaps the sequence

882
01:00:07,290 --> 01:00:11,560
or we could try to predict what is known as the contact matrix and things

883
01:00:11,560 --> 01:00:12,590
like that

884
01:00:14,170 --> 01:00:21,310
or from the closer to our application domain here and is something like information extraction

885
01:00:21,310 --> 01:00:26,620
where interested we have documents like that and we are interested in in finding that

886
01:00:26,700 --> 01:00:33,050
a named entities like you know like names countries organizations locations and so on and

887
01:00:33,050 --> 01:00:35,620
so forth and somehow

888
01:00:36,130 --> 01:00:38,390
mark them

889
01:00:38,420 --> 01:00:43,420
kind of producing markup like this where we actually see we put tags in there

890
01:00:43,440 --> 01:00:44,600
that they would

891
01:00:44,750 --> 01:00:49,200
identify these entities and we can think of it actually has as of the problem

892
01:00:49,200 --> 01:00:54,140
of producing a label sequence where with with each word we

893
01:00:54,150 --> 01:00:58,170
we classify and this is another named entity not named entity not an invented in

894
01:00:58,170 --> 01:01:03,010
the fourth and this is now an organisation and this is not not not and

895
01:01:03,010 --> 01:01:06,730
so on and so forth and here we have country and country right so you

896
01:01:06,730 --> 01:01:09,060
can think of this

897
01:01:09,150 --> 01:01:13,300
i can be encoded information in the text can be encoded for instance in terms

898
01:01:13,300 --> 01:01:17,940
of the label sequence and then there are certain dependencies right among these labels and

899
01:01:17,940 --> 01:01:21,140
we would like to capture those

900
01:01:21,150 --> 01:01:24,330
and then you know this is also a special case of what i'm talking about

901
01:01:24,330 --> 01:01:29,590
here is another example is natural language that's natural language parsing where you know the

902
01:01:29,590 --> 01:01:33,640
input is a sequence that in english what we want to produce to predict the

903
01:01:33,640 --> 01:01:39,460
past tree like that right label parse tree and we will assume that the rules

904
01:01:39,460 --> 01:01:44,160
of the grammar given OK so we know what type of rules are allowed but

905
01:01:44,170 --> 01:01:45,150
we don't know

906
01:01:45,170 --> 01:01:51,170
sort of you know how likely a particular posture is given a particular sentence right

907
01:01:51,180 --> 01:01:55,360
of course typically that there are many parts trees that are possible for sentence but

908
01:01:55,360 --> 01:01:59,500
we want to predict the most the most plausible the most likely one

909
01:01:59,530 --> 01:02:02,980
and so we can also think of that as the problem given a string

910
01:02:03,000 --> 01:02:07,690
o sentence in english language we want to predict tree

911
01:02:07,710 --> 01:02:11,040
right so how do we do that

912
01:02:11,040 --> 01:02:16,720
and it just so

913
01:02:16,740 --> 01:02:17,780
the way we

914
01:02:17,790 --> 01:02:21,660
we set up the problem is in a two step process first we define a

915
01:02:21,660 --> 01:02:23,140
discriminant function or

916
01:02:23,150 --> 01:02:28,090
you could also called the compatibility function and then we use that actually to produce

917
01:02:28,090 --> 01:02:34,470
the output so the other compatibility function or the discriminative discriminant functions

918
01:02:34,490 --> 01:02:38,650
we define in the following way at the function i call capital and here it

919
01:02:38,650 --> 01:02:41,590
takes an input and an output OK

920
01:02:41,590 --> 01:02:47,600
and is parametrized by some vector w and produces a real number how does it

921
01:02:47,600 --> 01:02:52,300
produce the real number while it computes an inner product between this weight vector and

922
01:02:52,300 --> 01:02:56,820
what i call a joint feature map that takes inputs and outputs

923
01:02:56,850 --> 01:03:01,620
the a pair of input and output and maps to some m dimensional feature representation

924
01:03:01,620 --> 01:03:05,580
available on the next slides i will give you some examples of how the people

925
01:03:05,610 --> 01:03:10,060
that map actually looks like right so not as it if you think about binary

926
01:03:10,060 --> 01:03:12,570
classification right then

927
01:03:12,620 --> 01:03:13,280
this is

928
01:03:14,160 --> 01:03:17,840
kind of similar only there you only take the input and you compute an inner

929
01:03:17,840 --> 01:03:22,860
product of the inputs which actually within the wires is binary label would actually show

930
01:03:22,860 --> 01:03:27,430
up in front of this inner product as a sign basically

931
01:03:27,440 --> 01:03:31,740
and so we define a function what we want is we want to just the

932
01:03:32,910 --> 01:03:38,400
in a way that the correct input output pairs get higher f values as compared

933
01:03:39,030 --> 01:03:43,230
some x y prime with the incorrect output OK we just one basically for a

934
01:03:43,230 --> 01:03:49,070
given x the correct output to give us high values and incorrect ones to produce

935
01:03:49,090 --> 01:03:53,500
law values so that if we if we have to make a decision we can

936
01:03:53,500 --> 01:03:57,410
take for a given x k x is an important is an english sentence and

937
01:03:57,410 --> 01:04:02,390
we want to predict the past we take the argmax over all possible parse trees

938
01:04:02,390 --> 01:04:06,970
let's say if we could do that effectively and look at this compatibility function and

939
01:04:06,970 --> 01:04:10,150
choose the one that is most compatible with the input

940
01:04:11,250 --> 01:04:15,370
and now we can adjust these weights in a way that we change this compatibility

941
01:04:15,370 --> 01:04:19,640
function and we want to shape it in a way that you know four forces

942
01:04:19,640 --> 01:04:27,170
in training patterns training pairs input output pairs we make the correct predictions

943
01:04:27,530 --> 01:04:32,290
OK so the binary classification case a special can be can be thought of as

944
01:04:32,290 --> 01:04:37,100
a special case of that in the following way that we define this joint feature

945
01:04:37,100 --> 01:04:43,290
map psi effects come why just as y why not being one minus one variable

946
01:04:43,320 --> 01:04:45,800
time some pizza representation

947
01:04:45,820 --> 01:04:47,030
five x just

948
01:04:47,060 --> 01:04:53,410
a typical feature map that's used in kernel and the incoming based method so

949
01:04:53,430 --> 01:04:57,260
so then we can see that if we take the argmax over all possible values

950
01:04:57,260 --> 01:05:01,670
for y of this function as we defined it

951
01:05:01,690 --> 01:05:05,880
you know we plug is in here and then kind of you know this

952
01:05:05,930 --> 01:05:09,340
can be seen to be equivalent to this just taking the sign of the inner

953
01:05:09,340 --> 01:05:15,570
product which is exactly the way we define linear classifiers in the binary problems right

954
01:05:16,070 --> 01:05:18,120
so you can see this is

955
01:05:18,120 --> 01:05:23,180
and this is a slightly different setting from the usual generous actually in that sense

956
01:05:23,180 --> 01:05:28,800
you actually have to take this is the set up that's how we we can

957
01:05:28,800 --> 01:05:30,070
manage to

958
01:05:30,120 --> 01:05:34,150
get that they morphine because this is where we have for lunch as well so

959
01:05:35,420 --> 01:05:39,440
we have it set up this way hopefully you can utilize the table for taking

960
01:05:39,440 --> 01:05:44,810
notes and what can again and we have an exciting they

961
01:05:44,850 --> 01:05:52,060
of five sessions presentations and activities ahead of us and i hope you had a

962
01:05:52,060 --> 01:05:56,160
good less last night was that get going for today

963
01:05:56,170 --> 01:05:59,680
and maybe one more minute people still trickling in

964
01:06:01,490 --> 01:06:07,920
hello everybody gives me great pleasure

965
01:06:07,960 --> 01:06:10,110
to be introducing st

966
01:06:10,130 --> 01:06:12,770
to be giving this talk today

967
01:06:12,780 --> 01:06:17,110
on foster equalisation parts y coordinate descent

968
01:06:17,170 --> 01:06:19,670
two what is well known in this community

969
01:06:19,680 --> 01:06:24,800
his yellow book on elements of statistical learning for many of us

970
01:06:24,940 --> 01:06:31,130
who have gotten into data mining through the nonstatistical part this has been

971
01:06:31,240 --> 01:06:35,250
a very valuable book it has given us the much needed

972
01:06:35,500 --> 01:06:41,910
foundations for understanding many of the data mining operators that we just take for granted

973
01:06:42,200 --> 01:06:48,500
and hence the idea of this talk think that it's going to be really valuable

974
01:06:48,500 --> 01:06:54,140
for us to understand the data mining from one of the strongholds in the field

975
01:06:54,140 --> 01:06:55,740
of statistics

976
01:06:55,890 --> 01:07:01,770
he has another book this one is far statisticians on generalized additive models which he

977
01:07:01,770 --> 01:07:03,780
has co-authored with should

978
01:07:04,030 --> 01:07:10,160
he has been working in statistics for thirty years and of course he has made

979
01:07:10,610 --> 01:07:15,520
a diverse range of contributions in many different areas as a five to just use

980
01:07:15,520 --> 01:07:22,080
two words to summarize the most of what he has done its nonparametric regression and

981
01:07:24,750 --> 01:07:28,830
he's lying on these tools

982
01:07:29,200 --> 01:07:33,140
for solving these problems in the bio

983
01:07:33,150 --> 01:07:37,550
and may be seen in genomics kind of industry

984
01:07:37,570 --> 01:07:43,050
on the computing side in fact many of the

985
01:07:43,080 --> 01:07:50,320
libraries which are there in the famous statistical software packages like s plus and i

986
01:07:50,520 --> 01:07:54,990
have been due to his influence and contribution

987
01:07:55,110 --> 01:08:01,840
but he confesses that his favorite languages in fact fortran not the statistical languages

988
01:08:01,860 --> 01:08:09,800
he has been with the department of statistics stanford for almost nineteen years first five

989
01:08:10,580 --> 01:08:12,460
as a phd student

990
01:08:12,460 --> 01:08:18,430
and then twenty twelve years as a free-floating faculty member and for the past two

991
01:08:18,430 --> 01:08:22,400
years he has been on a sentence apparently he is the chair of the department

992
01:08:22,820 --> 01:08:26,580
and he's really waiting for that turn to another one

993
01:08:26,800 --> 01:08:34,760
and like many great statisticians he has spent late nineteen very productive and funnier as

994
01:08:35,050 --> 01:08:37,290
in the labs in its he

995
01:08:37,400 --> 01:08:42,540
so he's very fond of those that period too he grew up in south africa

996
01:08:42,540 --> 01:08:48,320
in fact and he enjoys surfing and and he continues to live with that passion

997
01:08:48,320 --> 01:08:55,490
although sometimes the t two goes into the important so this is very well known

998
01:08:55,490 --> 01:08:59,390
in the field and he has given many talks in fact this is his twenty

999
01:08:59,390 --> 01:09:07,070
first keynote talk in over the last eight years is a fellow of the american

1000
01:09:07,070 --> 01:09:12,430
statistical foundation and institute of mathematical statistics and i thought it was a big deal

1001
01:09:12,550 --> 01:09:16,770
but then he tells me in the morning today that he was a fellow of

1002
01:09:16,770 --> 01:09:22,320
the royal statistical society even before he got his phd so without further delay it

1003
01:09:22,320 --> 01:09:25,620
may moving come to vote to give us stock

1004
01:09:25,670 --> 01:09:36,530
my life

1005
01:09:36,580 --> 01:09:39,620
and everyone you meet

1006
01:09:39,630 --> 01:09:45,280
well thank you very much for small healing for organizing the conference and so need

1007
01:09:45,390 --> 01:09:48,210
for very nice introduction thank you very much

1008
01:09:48,220 --> 01:09:49,510
she asked me for

1009
01:09:49,520 --> 01:09:54,530
some background it and i gave a quite about shoes speed some of the worst

1010
01:09:54,540 --> 01:09:55,350
thank you

1011
01:09:56,880 --> 01:09:58,810
it's very nice to be

1012
01:09:58,820 --> 01:10:00,880
i know lot of you

1013
01:10:00,890 --> 01:10:03,980
the computer science field and

1014
01:10:04,010 --> 01:10:07,730
was really you talk and coordinate descent because

1015
01:10:07,820 --> 01:10:13,310
i think coordinator since being dismissed as a technique for optimisation city among my engineering

1016
01:10:15,430 --> 01:10:19,900
undaunted agree with plans to head and i hope to convince you today

1017
01:10:19,930 --> 01:10:23,430
at least in this application

1018
01:10:23,480 --> 01:10:25,470
so this is joint work with my

1019
01:10:25,480 --> 01:10:32,630
i two co-authors jerry friedman and rob tibshirani from stanford university

1020
01:10:34,270 --> 01:10:37,160
one of the favourite pastime

1021
01:10:37,210 --> 01:10:42,560
jerry's the class of carbon ions and actually to be fair to rob that's my

1022
01:10:42,560 --> 01:10:46,190
students young pop graduating and i wasn't able to be there so he

1023
01:10:46,760 --> 01:10:48,470
he went in

1024
01:10:48,480 --> 01:10:54,250
and congratulated for me

1025
01:10:56,140 --> 01:10:59,310
this is the story about linear models

1026
01:10:59,710 --> 01:11:02,910
and using linear models in data mining

1027
01:11:02,930 --> 01:11:07,060
probably ten years ago we would get such to talk because the new models were

1028
01:11:07,060 --> 01:11:11,020
somewhat out of phase fashion we were doing much more aggressive

1029
01:11:12,280 --> 01:11:17,100
but some things changed datasets have grown wide

1030
01:11:17,390 --> 01:11:22,260
which means we have many more features than samples in a lot of applications

1031
01:11:22,270 --> 01:11:27,570
and so the linear model has regained favor in data miners toolbox

1032
01:11:27,640 --> 01:11:32,370
there have so many variables from the linear model in some applications is all we

1033
01:11:33,230 --> 01:11:34,760
we can manage to do

1034
01:11:35,200 --> 01:11:39,890
and before we start overfitting so you some examples

1035
01:11:39,910 --> 01:11:44,020
so document classification is is a prime example

1036
01:11:44,070 --> 01:11:46,120
we often use the bag of words

1037
01:11:47,850 --> 01:11:51,560
we we take the collection of say words in english language and mark them for

1038
01:11:51,620 --> 01:11:54,350
his presence they were absent in the document

1039
01:11:54,370 --> 01:11:55,960
and that can easily lead to

1040
01:11:55,960 --> 01:12:02,160
twenty thousand features for each document and we might have something like five thousand documents

1041
01:12:02,330 --> 01:12:06,330
so in statistics job we'd say p is bigger than we always use p for

1042
01:12:06,330 --> 01:12:10,010
the number of variables and in the number of samples

1043
01:12:10,040 --> 01:12:14,330
so for example the spam classification example is is

1044
01:12:14,890 --> 01:12:19,030
is one email classification is spam or not

1045
01:12:19,080 --> 01:12:20,710
it's the document

1046
01:12:20,730 --> 01:12:22,820
we look at the the set of words

1047
01:12:22,870 --> 01:12:29,400
the some of the early spam filters would do that and classify document is spam

1048
01:12:29,460 --> 01:12:30,760
in fact

1049
01:12:30,770 --> 01:12:32,210
little embarrassingly

1050
01:12:32,220 --> 01:12:36,260
when i was first invited by sunita to be the keynote speaker in this in

1051
01:12:36,260 --> 01:12:37,830
this conference

1052
01:12:38,810 --> 01:12:41,500
message went straight into my spam back

1053
01:12:41,540 --> 01:12:43,020
so i didn't see

1054
01:12:43,080 --> 01:12:45,850
and then she said reminder two weeks later

1055
01:12:45,870 --> 01:12:49,230
they don't want to actually give this talk about and i have to tell you

1056
01:12:49,310 --> 01:12:54,140
that i haven't seen the misadventure enough i found in the in the spam black

1057
01:12:54,200 --> 01:12:58,280
so my guess my spam filter still work in progress

1058
01:12:58,320 --> 01:13:01,500
image deblurring classification images

1059
01:13:01,540 --> 01:13:05,260
notoriously have a large number of features if you use the pixels in

1060
01:13:05,270 --> 01:13:07,030
as features

1061
01:13:07,080 --> 01:13:10,620
and the number of samples can be relatively small

1062
01:13:10,700 --> 01:13:14,060
the area where we i worked a lot is in genomics

1063
01:13:14,520 --> 01:13:17,470
so for example in microarray studies

1064
01:13:17,480 --> 01:13:19,750
you can easily forty thousand features

1065
01:13:19,760 --> 01:13:22,830
which may be represent versions of genes

1066
01:13:23,780 --> 01:13:25,220
you might only have a

1067
01:13:25,250 --> 01:13:28,330
if you is a hundred samples

1068
01:13:28,400 --> 01:13:32,140
and more recently in genome wide association studies

1069
01:13:32,190 --> 01:13:34,220
where we measure snaps

1070
01:13:34,270 --> 01:13:36,980
single nucleotide polymorphisms

1071
01:13:37,000 --> 01:13:38,340
and they can easily be

1072
01:13:38,350 --> 01:13:42,770
five hundred thousand ninety seven million synapses mission along the genome

1073
01:13:43,540 --> 01:13:45,100
often they do these in

1074
01:13:45,120 --> 01:13:48,140
for specific diseases and trying to discover

1075
01:13:48,150 --> 01:13:53,660
cause us to specific diseases and you might say two thousand case control samples

1076
01:13:53,790 --> 01:13:55,560
so in all these cases

1077
01:13:55,600 --> 01:13:58,700
we've got many many more features than we have observations

1078
01:13:58,700 --> 01:14:01,800
and the linear model is very useful

1079
01:14:01,810 --> 01:14:04,750
and we use them such as linear regression

1080
01:14:04,760 --> 01:14:06,290
majestic regression

1081
01:14:06,290 --> 01:14:08,360
if we go just quickly go back to this picture

1082
01:14:08,830 --> 01:14:12,690
this is an infinite dimensional space right and if we use say girls in

1083
01:14:14,230 --> 01:14:15,760
the family of gaussians as a model

1084
01:14:17,340 --> 01:14:23,210
guassian has is characterized by a finite dimensional parameter vector so it has a finite

1085
01:14:23,210 --> 01:14:27,430
number of degrees of freedom right a parametric model is a finite number of degrees

1086
01:14:27,430 --> 01:14:27,860
of freedom

1087
01:14:28,320 --> 01:14:28,910
so it is

1088
01:14:29,540 --> 01:14:33,210
it is finite dimensional subset of this space here

1089
01:14:34,030 --> 01:14:37,450
and a finite dimension a subset of an infinite dimensional space is small

1090
01:14:38,930 --> 01:14:41,480
so we don't have it

1091
01:14:41,660 --> 01:14:44,820
it will be difficult if are in general it is if we don't know what

1092
01:14:44,820 --> 01:14:48,120
the true distribution is that it would be difficult to get close to the true

1093
01:14:49,270 --> 01:14:49,890
and the ideas

1094
01:14:50,340 --> 01:14:51,000
if you can make

1095
01:14:51,470 --> 01:14:54,030
if you if you want to make a more and more flexible that means if

1096
01:14:54,030 --> 01:14:57,340
you want this model to spread out more widely over space

1097
01:14:58,320 --> 01:14:59,980
and be closer to any possible

1098
01:15:00,990 --> 01:15:04,770
distribution of the of the data and then you have to use a nonparametric model

1099
01:15:05,830 --> 01:15:06,910
you can't do that with a finite

1100
01:15:08,250 --> 01:15:09,090
fundamental parameters

1101
01:15:23,470 --> 01:15:27,950
if u if you do parametric statistics a parametric bayesian statistics and you have to choose the prior

1102
01:15:28,830 --> 01:15:30,250
and you don't know anything about what

1103
01:15:30,900 --> 01:15:33,370
what the parameters should look like then of course

1104
01:15:33,840 --> 01:15:37,300
but what you usually do is choose a uniform prior right a

1105
01:15:38,150 --> 01:15:39,110
maximum uncertainty

1106
01:15:39,780 --> 01:15:42,550
now if we don't know anything about what our model is in this case

1107
01:15:43,070 --> 01:15:47,880
then would also like to choose something like a uniform prior something that spreads out evenly over the whole space

1108
01:15:48,490 --> 01:15:49,860
but as i mentioned earlier already

1109
01:15:51,110 --> 01:15:56,650
the space is infinite-dimensional in infinite dimensional this uniform distribution just don't exist and

1110
01:15:59,380 --> 01:16:01,260
we need we need something else can

1111
01:16:03,990 --> 01:16:05,970
we just at this point say a few words about what

1112
01:16:06,800 --> 01:16:08,170
about the interpretation of

1113
01:16:09,590 --> 01:16:10,700
of prior assumptions

1114
01:16:11,150 --> 01:16:13,530
in bayesian nonparametrics because they are a bit different

1115
01:16:15,280 --> 01:16:18,170
the prior assumptions that we make a parametric statistics these these

1116
01:16:18,740 --> 01:16:22,880
you know technically it's kind of the same thing but the interpretation comes out a little bit differently

1117
01:16:23,380 --> 01:16:23,760
so in

1118
01:16:24,400 --> 01:16:27,110
in parametric statistics if you say put a prior on calcium

1119
01:16:27,740 --> 01:16:29,030
then the prior assumption is

1120
01:16:30,650 --> 01:16:33,630
the mean of my data something something that says the mean of my data is

1121
01:16:33,630 --> 01:16:34,650
probably going to be over here

1122
01:16:35,150 --> 01:16:38,050
and that's what about putting a prior on august expresses

1123
01:16:38,840 --> 01:16:39,570
whereas in

1124
01:16:40,720 --> 01:16:42,420
in these nonparametric models

1125
01:16:44,320 --> 01:16:49,010
what expressed by the prior assumption is typically more and more structural properties so for

1126
01:16:49,010 --> 01:16:50,700
example in the jeep regression model

1127
01:16:51,090 --> 01:16:51,610
he so that

1128
01:16:53,470 --> 01:16:54,490
and if if if

1129
01:16:56,490 --> 01:16:58,030
if the unknown parameters the bandwidth

1130
01:16:58,630 --> 01:17:00,220
of course the girls in process prior

1131
01:17:00,610 --> 01:17:02,420
we don't know the benefits of the process prior

1132
01:17:02,820 --> 01:17:05,320
well if we if we choose the benefits of the gas process prior

1133
01:17:07,150 --> 01:17:09,590
we can send any continuous function from that model

1134
01:17:10,260 --> 01:17:15,300
and the prior assumption says it will probably oscillate at a certain rate and was

1135
01:17:15,430 --> 01:17:17,860
probably have frequency roughly in the range

1136
01:17:18,420 --> 01:17:19,510
that's our prior assumption

1137
01:17:20,470 --> 01:17:21,110
end in

1138
01:17:22,880 --> 01:17:25,030
remember what we talked about yesterday in clustering

1139
01:17:26,950 --> 01:17:32,700
if we choose for example if you choose is a chinese restaurant process prior dirichlet process prior then hour assumption

1140
01:17:33,220 --> 01:17:35,200
that the prior assumption that encodes is

1141
01:17:35,820 --> 01:17:40,200
how does the number of the does they distribution of cluster sizes behave

1142
01:17:40,700 --> 01:17:42,510
the dirty process could see assumption

1143
01:17:42,950 --> 01:17:48,610
it the number of of clusters distributed logarithmically in the number of data points right

1144
01:17:48,610 --> 01:17:50,360
it's an assumption on the decay

1145
01:17:50,920 --> 01:17:54,130
of this cluster weights and if we believe that for example

1146
01:17:54,670 --> 01:17:55,220
that we have

1147
01:17:55,800 --> 01:17:58,570
so i believe we have a small number of clusters and that's may be a good

1148
01:17:59,240 --> 01:17:59,950
choice yeah

1149
01:18:00,760 --> 01:18:04,780
and if we believe that the clusters follows for the distribution of class follows a power law

1150
01:18:05,280 --> 01:18:07,670
then we would instead use the pitman yor process

1151
01:18:08,720 --> 01:18:09,510
for instance you

1152
01:18:14,760 --> 01:18:15,990
just see how much time i have

1153
01:18:19,150 --> 01:18:21,090
all right so these these kind of results that

1154
01:18:26,130 --> 01:18:26,510
the people

1155
01:18:27,260 --> 01:18:30,260
like order that the mathematics statisticians are interested in

1156
01:18:31,090 --> 01:18:31,780
to quantify

1157
01:18:34,590 --> 01:18:37,130
how well behaved these models are there are basically two

1158
01:18:37,530 --> 01:18:39,740
two types of properties that people are interested in

1159
01:18:40,380 --> 01:18:42,220
one is a property called consistency

1160
01:18:42,950 --> 01:18:45,220
and roughly speaking what consistency means is

1161
01:18:45,860 --> 01:18:46,920
my going to recover

1162
01:18:47,630 --> 01:18:49,170
the right value of the parameters

1163
01:18:49,900 --> 01:18:52,510
supposing that have infinitely an infinite amount of data

1164
01:18:54,430 --> 01:18:54,920
so in

1165
01:18:55,920 --> 01:18:59,650
in parametric statistics are parametric non bayesian statistics

1166
01:19:00,260 --> 01:19:02,300
i'm consistency simply means that these

1167
01:19:04,590 --> 01:19:07,380
the estimator is going to converge to the true parameter values

1168
01:19:08,900 --> 01:19:10,240
now in bayesian statistics

1169
01:19:10,700 --> 01:19:15,280
we're not actually computing an estimator point estimator about what we want compute is a posterior

1170
01:19:16,200 --> 01:19:21,800
so we have to formulate what it means forum four posterior to converge and basically what that means is

1171
01:19:22,280 --> 01:19:25,280
the easiest way to think about is simply that the posterior as we see more

1172
01:19:25,280 --> 01:19:26,820
and more data becomes more and more people

1173
01:19:28,610 --> 01:19:33,240
and that in the in the limit of infinitely many data points that they become a delta peak

1174
01:19:33,990 --> 01:19:35,610
at singapore and that should be

1175
01:19:36,030 --> 01:19:38,110
if it's consistent that should be the true parameter about

1176
01:19:46,990 --> 01:19:47,800
now there's there's

1177
01:19:48,300 --> 01:19:52,380
a simple definition of of consistency bayesian models in which is following

1178
01:19:52,880 --> 01:19:56,340
there's a problem tell you in advance there's a problem with this definition this definition

1179
01:19:56,340 --> 01:19:57,550
has been used for a long time

1180
01:19:58,190 --> 01:19:59,220
and it's the following if we

1181
01:19:59,920 --> 01:20:00,240
if we

1182
01:20:01,430 --> 01:20:03,300
assume that we sample the data from

1183
01:20:04,070 --> 01:20:06,900
specific distribution which has parameter values theta not

1184
01:20:08,200 --> 01:20:12,010
so you don't what is the true parameter value that is actually generated the data

1185
01:20:14,740 --> 01:20:16,090
if the posterior converges

1186
01:20:17,490 --> 01:20:19,800
to this this true parameter values

1187
01:20:21,300 --> 01:20:23,590
with probability one under the prior

1188
01:20:25,130 --> 01:20:28,260
then we say that the model is consistent why do we have this extra with

1189
01:20:28,260 --> 01:20:30,470
probability one and the prior if you sample

1190
01:20:31,130 --> 01:20:36,650
if you sample from a bayesian model then remember we do stage sampling we first sample a parameter value

1191
01:20:37,110 --> 01:20:37,650
from the prime

1192
01:20:38,630 --> 01:20:41,510
then we plug that into the likelihood and we sample idea from

1193
01:20:42,280 --> 01:20:44,590
from the likelihood and that's the way we generate idea

1194
01:20:48,990 --> 01:20:52,510
now what we have to ask is if we say the posterior converges okay we

1195
01:20:52,510 --> 01:20:56,050
can take this data that we have sampled we can compute the posterior from there

1196
01:20:57,420 --> 01:21:02,010
but what it will what it will converge to depends on which parameter value we

1197
01:21:02,010 --> 01:21:06,050
have drawn from the prior right and hopefully it will converge to the actual value

1198
01:21:09,320 --> 01:21:10,570
this definition here says

1199
01:21:11,010 --> 01:21:12,880
it it always has to do with it

1200
01:21:13,740 --> 01:21:14,610
except fore

1201
01:21:15,720 --> 01:21:16,970
some parameter values

1202
01:21:17,800 --> 01:21:19,950
as long as there has probability zero

1203
01:21:21,510 --> 01:21:22,650
that seems perfectly safe

1204
01:21:22,650 --> 01:21:23,980
assigned them

1205
01:21:24,030 --> 01:21:26,310
assign each one of them a countable set

1206
01:21:26,320 --> 01:21:28,980
and show that i can use it to all of the artists

1207
01:21:28,980 --> 01:21:30,290
so the first

1208
01:21:31,230 --> 01:21:36,170
the unit circle in the x y plane

1209
01:21:36,190 --> 01:21:38,190
the second set is

1210
01:21:38,200 --> 01:21:39,590
on the axes

1211
01:21:39,610 --> 01:21:43,970
the sets with coordinates where with the in square loss for in

1212
01:21:44,000 --> 01:21:47,640
the distance from each BN

1213
01:21:49,170 --> 01:21:53,440
o point on this article is to a

1214
01:21:54,270 --> 01:21:57,330
well for this purpose for the last one

1215
01:21:58,410 --> 01:22:04,020
two in response to the assignment of scholars

1216
01:22:04,070 --> 01:22:06,520
to be assigned to call the

1217
01:22:06,540 --> 01:22:09,930
countable set and was one and so on

1218
01:22:09,930 --> 01:22:14,570
what about to be

1219
01:22:14,580 --> 01:22:18,430
well i think there a a countable subsets of the

1220
01:22:21,840 --> 01:22:25,420
its power is the same power is this set

1221
01:22:25,450 --> 01:22:27,860
there is a one-to-one correspondence

1222
01:22:27,880 --> 01:22:30,060
now if your site

1223
01:22:30,080 --> 01:22:35,420
colors to the b and you cannot assign a finite set of colors

1224
01:22:35,460 --> 01:22:37,000
it has to be in c

1225
01:22:37,080 --> 01:22:39,960
if it is infinite that one of these point p

1226
01:22:40,000 --> 01:22:41,980
will be assigned subset of the

1227
01:22:41,990 --> 01:22:45,270
and you cannot choose the kind of funny because all of them are taken by

1228
01:22:57,070 --> 01:22:59,070
OK we fool is

1229
01:22:59,080 --> 01:23:02,550
the this it

1230
01:23:02,570 --> 01:23:06,730
we just mentioned the all this and what the plane is not a useful

1231
01:23:06,760 --> 01:23:11,080
is this thing to see text us the point with integer coordinates

1232
01:23:11,100 --> 01:23:12,970
on the x axis

1233
01:23:12,980 --> 01:23:15,990
they will perform in infinite complete

1234
01:23:16,030 --> 01:23:17,500
bipartite graph

1235
01:23:17,500 --> 01:23:21,840
all the points will be connected by an actual the

1236
01:23:25,300 --> 01:23:29,440
we prove that the are two of but his car which is will

1237
01:23:29,460 --> 01:23:31,980
i will present this will it takes more than once

1238
01:23:32,010 --> 01:23:34,130
one slide so i don't believe

1239
01:23:34,130 --> 01:23:37,510
conference will take more than one slide

1240
01:23:37,530 --> 01:23:42,940
as we saw the full

1241
01:23:44,480 --> 01:23:47,300
space is not

1242
01:23:47,300 --> 01:23:49,710
how to choose the

1243
01:23:49,760 --> 01:23:53,740
as of last thursday

1244
01:23:53,760 --> 01:23:56,840
we call that the this one is

1245
01:23:56,860 --> 01:23:59,300
and if one chooses all

1246
01:23:59,300 --> 01:24:04,380
the surprising fact that these regardless of the whether you believe in the

1247
01:24:07,900 --> 01:24:11,010
so in any model of set theory

1248
01:24:11,780 --> 01:24:13,300
this is

1249
01:24:13,360 --> 01:24:15,090
and if one chooses

1250
01:24:20,320 --> 01:24:21,570
so this

1251
01:24:21,630 --> 01:24:26,560
however one problem the quality of this in the query and i hope we are

1252
01:24:27,860 --> 01:24:30,900
move to build a simple puzzle he was one

1253
01:24:33,570 --> 01:24:36,170
much that is suitable for high-school students

1254
01:24:36,170 --> 01:24:39,440
forget about the notation might be to compensate for them

1255
01:24:39,440 --> 01:24:42,530
it's not that FNB the minimal

1256
01:24:42,550 --> 01:24:47,940
the maximum perimeter of a rectangle among all possible partition the unit square into an

1257
01:24:47,980 --> 01:24:51,610
even number for us

1258
01:24:51,760 --> 01:25:01,320
so this is a trivial problem when n is equal to case where just politicians

1259
01:25:01,320 --> 01:25:02,260
this way

1260
01:25:02,280 --> 01:25:03,800
in two

1261
01:25:03,840 --> 01:25:06,550
k sports where each one of them all

1262
01:25:06,630 --> 01:25:09,900
size one one with k one

1263
01:25:09,900 --> 01:25:21,090
one of is partition into rectangles in case n is taking place plus one show

1264
01:25:21,090 --> 01:25:23,570
the picture and the second is just

1265
01:25:23,630 --> 01:25:26,260
partition the square into rectangles

1266
01:25:26,280 --> 01:25:29,610
of size one hundred eight by one of the first ones

1267
01:25:29,670 --> 01:25:31,070
OK i think

1268
01:25:31,110 --> 01:25:36,170
the proof that this is optimal is far from easy it was done by in

1269
01:25:36,230 --> 01:25:39,940
then you look like

1270
01:25:43,510 --> 01:25:47,070
so here is an example of what said he was a politician of the square

1271
01:25:47,820 --> 01:25:50,280
two four six forty nine

1272
01:25:51,510 --> 01:25:54,820
which minimizes the largest perimeter

1273
01:25:54,840 --> 01:25:57,360
it was the a partition of the square

1274
01:25:57,380 --> 01:25:58,280
in two

1275
01:25:58,300 --> 01:26:01,880
forty two six times seven rectangle

1276
01:26:01,940 --> 01:26:06,030
it's not obvious that this is the minimum

1277
01:26:06,130 --> 01:26:07,090
the limit

1278
01:26:07,110 --> 01:26:13,300
among all possible partitioning the square into four triangles but it will

1279
01:26:13,300 --> 01:26:16,840
for and one of the major reasons is that

1280
01:26:16,860 --> 01:26:21,400
having we didn't actually OK

1281
01:26:21,410 --> 01:26:25,140
let's start from those from another perspective

1282
01:26:25,150 --> 01:26:28,880
technically this would be no problem to make this

1283
01:26:28,920 --> 01:26:34,920
because you could just like changes system and allowed yet another parameter to the subject

1284
01:26:34,930 --> 01:26:36,700
in whatever it doesn't matter the day

1285
01:26:37,120 --> 01:26:41,980
we had a number of different ideas how to realize to define the syntax

1286
01:26:41,980 --> 01:26:45,130
the system can do easily to solve the problem

1287
01:26:45,190 --> 01:26:48,900
the actual problem comes fifty users

1288
01:26:48,910 --> 01:26:50,120
in the week wiki

1289
01:26:50,150 --> 01:26:51,830
you're expecting

1290
01:26:51,920 --> 01:26:57,930
or do users expecting to be able to change everything quickly

1291
01:26:57,950 --> 01:27:00,110
now if you have very

1292
01:27:00,130 --> 01:27:02,010
and you see some resolved

1293
01:27:02,240 --> 01:27:04,910
you want to be able to to change his her UG and you want to

1294
01:27:04,910 --> 01:27:08,720
be able to change did way use quickly you have to be able to figure

1295
01:27:10,030 --> 01:27:14,330
does this data come from

1296
01:27:14,330 --> 01:27:17,080
now if it would be possible to add

1297
01:27:17,120 --> 01:27:21,830
arbitrary triples to the wiki on every side

1298
01:27:21,850 --> 01:27:25,510
it would be very hard to use it to figure out to which

1299
01:27:25,510 --> 01:27:27,310
page has actually to go

1300
01:27:27,330 --> 01:27:30,990
to change that we're value

1301
01:27:31,000 --> 01:27:34,240
so we didn't know we were not able to solve that

1302
01:27:34,330 --> 01:27:40,850
problem from the user perspective that's why we don't have the ability to add arbitrary

1303
01:27:40,850 --> 01:27:48,230
the triples there are other semantic wikis that allow exactly that for example travel wiki

1304
01:27:48,230 --> 01:27:54,700
engine allows to add arbitrary RDF and i don't know about wikipedia does it to

1305
01:27:54,800 --> 01:27:55,750
give you

1306
01:28:06,740 --> 01:28:08,550
they also

1307
01:28:08,850 --> 01:28:11,750
because this is

1308
01:28:12,410 --> 01:28:15,770
the model

1309
01:28:19,400 --> 01:28:22,240
the neuron

1310
01:28:22,330 --> 01:28:24,730
the market

1311
01:28:24,790 --> 01:28:28,880
if you think want use

1312
01:28:30,310 --> 01:28:34,970
what they do is

1313
01:28:35,050 --> 01:28:39,920
so is the one

1314
01:28:58,880 --> 01:29:07,000
OK it will be pretty hard to solve that problem technically i think this is

1315
01:29:07,000 --> 01:29:08,250
the problem that has

1316
01:29:08,270 --> 01:29:12,200
be solved by the community that's using the wiki and if you look at wikipedia

1317
01:29:12,360 --> 01:29:17,760
it's definitely been issued their another example that i like to use because much closer

1318
01:29:17,760 --> 01:29:20,900
than to attend to taiwan is nicola test

1319
01:29:21,030 --> 01:29:25,740
and there's a lot of discussion going on in nicholas haslam website if he's croatian

1320
01:29:25,770 --> 01:29:29,740
if he's serbian or whatever other thing he could be

1321
01:29:29,740 --> 01:29:34,850
we during serban others here me up involves

1322
01:29:36,470 --> 01:29:40,040
to level hierarchy where there's one level of recurrent net

1323
01:29:40,050 --> 01:29:44,980
that is reading c words in a sentence but you also imagine a characters

1324
01:29:44,980 --> 01:29:48,170
in a word and another level which is working at the level here

1325
01:29:48,170 --> 01:29:52,320
sentences and and you can have more levels if you want and

1326
01:29:52,890 --> 01:29:56,280
but you can imagine many ways that you can implement these ideas

1327
01:29:56,280 --> 01:29:57,260
of hierarchy

1328
01:30:00,190 --> 01:30:04,600
now if we goes to something are very recent with which i think brings

1329
01:30:05,020 --> 01:30:08,600
fresh ideas to this question of long term dependencies

1330
01:30:08,950 --> 01:30:11,940
memory networks which you will hear more about

1331
01:30:13,990 --> 01:30:18,600
a you can think of the networks as some kind of

1332
01:30:18,970 --> 01:30:23,610
recurrent network where the state part of the state is now stored

1333
01:30:23,620 --> 01:30:28,060
in in a big memory big outside memory i'm going to show you next

1334
01:30:28,060 --> 01:30:29,940
picture because it's going to easier

1335
01:30:29,940 --> 01:30:34,750
to understand so so so you have a set of memory cells

1336
01:30:35,210 --> 01:30:38,630
and at each time step what happens is that those memory cells

1337
01:30:38,640 --> 01:30:42,220
either get copied from the previous time step or somebody writes

1338
01:30:42,230 --> 01:30:44,960
something into a cell and somebody is is

1339
01:30:45,220 --> 01:30:49,220
net right recurrent net and when you write something into the

1340
01:30:49,220 --> 01:30:52,470
cell of course you're you're going to forgetting all values but

1341
01:30:52,470 --> 01:30:54,640
most of the time if you have a big memory

1342
01:30:55,270 --> 01:30:58,550
and you're only writing one place at time basically

1343
01:30:59,250 --> 01:31:01,750
you're just copying and so information gets

1344
01:31:01,750 --> 01:31:04,030
propagated for a very long time other words

1345
01:31:04,190 --> 01:31:06,650
if you have like a memory it can memory of

1346
01:31:07,310 --> 01:31:10,360
during machine when you store things in there

1347
01:31:10,770 --> 01:31:15,640
it robustly stays for long time right so that kind of sidesteps the

1348
01:31:15,640 --> 01:31:18,190
the problem long-term dependencies or lease it reduces

1349
01:31:22,390 --> 01:31:28,180
so this is a nice recent paper from new

1350
01:31:28,570 --> 01:31:32,580
and also involving toronto people where

1351
01:31:33,100 --> 01:31:38,320
we try to think of how to characterize these recurrent net

1352
01:31:38,330 --> 01:31:42,790
architectures and we came up with three

1353
01:31:43,700 --> 01:31:47,880
interesting measures to help us understand an architecture

1354
01:31:48,310 --> 01:31:53,880
so one of them is the the a feedforward depth that's like the

1355
01:31:53,890 --> 01:31:57,240
yellow paso memory we stack recurrent net so how long does it

1356
01:31:57,250 --> 01:32:00,780
take to go from input to output through

1357
01:32:01,140 --> 01:32:05,870
different layers that's basically like going in the feedforward

1358
01:32:05,880 --> 01:32:10,350
direction then you have the recurrent depth which

1359
01:32:10,640 --> 01:32:16,410
tells us what's the longest path from a particular timestep to

1360
01:32:16,420 --> 01:32:20,320
later in the future potentially going through all the complications

1361
01:32:20,320 --> 01:32:23,920
of the architecture and in this particular architecture you see that

1362
01:32:24,070 --> 01:32:27,260
you can you can pack tool nonlinearities

1363
01:32:27,540 --> 01:32:31,190
for each timestep because you can go through this layer than then

1364
01:32:31,200 --> 01:32:34,520
the one above and then at the same time step and then go to the

1365
01:32:34,520 --> 01:32:37,540
next timestep at the same level and so this red path here

1366
01:32:37,880 --> 01:32:46,220
actually pacs twice as much depth per timestep than a standard

1367
01:32:47,370 --> 01:32:49,690
architecture and then the third quantity

1368
01:32:49,930 --> 01:32:52,860
that interesting to consider is the skip coefficient

1369
01:32:53,890 --> 01:32:58,160
which tells us about blue arrows that tells us not about the

1370
01:32:58,170 --> 01:33:02,370
longest path from beginning to the end but the shortest path

1371
01:33:02,820 --> 01:33:05,570
that's also useful when you want to have these paths

1372
01:33:06,420 --> 01:33:08,800
in order to learn long-term dependencies

1373
01:33:10,480 --> 01:33:13,870
so their experiments that i don't have to tell you about showing

1374
01:33:13,870 --> 01:33:16,920
how all these things through all of these things matter

1375
01:33:18,110 --> 01:33:21,330
briefly there's been a a bunch of work exploring

1376
01:33:21,660 --> 01:33:26,220
how we can playing with that the weight nature x

1377
01:33:26,600 --> 01:33:31,210
in order to make it easier to learn term dependencies so for example

1378
01:33:31,210 --> 01:33:33,110
you could initialize it to be father

1379
01:33:34,410 --> 01:33:38,060
or you can actually constrain it to be unitary matrix

1380
01:33:38,480 --> 01:33:40,810
which has all the eigenvalues equal to one

1381
01:33:42,950 --> 01:33:46,860
or you can play a game similar to what you have been

1382
01:33:47,970 --> 01:33:53,930
nets where sometimes the jacoby is just the identity because you

1383
01:33:53,930 --> 01:33:56,940
just copy the few state next state yeah and you can random we

1384
01:33:56,940 --> 01:33:59,820
do that and this is called zone now recent paper

1385
01:34:00,090 --> 01:34:01,700
also from yeah

1386
01:34:04,840 --> 01:34:13,250
ok i'm now earlier i said that in a standard recurrent net

1387
01:34:13,740 --> 01:34:16,930
we can interpret the computation and the the the model

1388
01:34:16,940 --> 01:34:21,740
as a directed graph model without any latent variables

1389
01:34:22,680 --> 01:34:26,950
and what that means is that all the randomness that is injected when

1390
01:34:26,960 --> 01:34:31,170
we generate something is happening at the level of observations

1391
01:34:31,170 --> 01:34:33,790
have the visible variables at the data level right

1392
01:34:33,790 --> 01:34:36,820
so if we're for working on pixels it's like the way that we

1393
01:34:36,820 --> 01:34:39,900
generated images that we randomly pick the next pixel and

1394
01:34:39,900 --> 01:34:42,280
then the next pixel given the previous pixels

1395
01:34:43,130 --> 01:34:46,120
but it would make a lot of sense if instead

1396
01:34:47,460 --> 01:34:51,440
that the randomness was about taking decisions regarding not low

1397
01:34:51,450 --> 01:34:54,630
level not the pixels but the high-level hierarchy like

1398
01:34:54,700 --> 01:34:57,980
what is this picture going to be about what is this text going

1399
01:34:57,980 --> 01:35:00,190
to be about what what is this music the about

1400
01:35:00,190 --> 01:35:04,780
you'd like to take these high-level decisions and then condition

1401
01:35:04,780 --> 01:35:07,600
on these high-level decisions you might want to put in the details

1402
01:35:07,600 --> 01:35:11,650
at low level so for this you need to introduce late actually can

1403
01:35:11,650 --> 01:35:14,320
variables not just the deterministic ones we have a regular

1404
01:35:14,320 --> 01:35:18,090
recurrent net we need to introduce the

1405
01:35:18,680 --> 01:35:21,420
variables so sort of a high-level of the current key

1406
01:35:21,510 --> 01:35:24,790
corresponding to high-level abstractions so like you decide on

1407
01:35:24,790 --> 01:35:27,330
the high-level structure and then given that the

1408
01:35:27,460 --> 01:35:34,070
decide low levels structure so we the last nips we had a paper about

1409
01:35:34,890 --> 01:35:38,900
combining ideas from what's called the original one orders which

1410
01:35:39,250 --> 01:35:41,880
hopefully will be also covered later this week

1411
01:35:41,880 --> 01:35:45,000
in recurrent nets you haven't seen these things yet i'm not going

1412
01:35:45,000 --> 01:35:48,250
to go much detail and how much time but basically you can combine

1413
01:35:48,250 --> 01:35:52,260
the idea is in some of these modern latent variable models

1414
01:35:52,590 --> 01:35:55,020
in recurrent nets can stick in high-level

1415
01:35:55,740 --> 01:35:59,410
random decisions in in the computational graph

1416
01:36:00,110 --> 01:36:01,080
in the model

1417
01:36:03,620 --> 01:36:06,240
so that the model showed you earlier which has

1418
01:36:07,020 --> 01:36:10,410
two levels of recurrent nets like one working on words the other

1419
01:36:10,410 --> 01:36:14,190
working on sentences you can also have random variables at each

1420
01:36:14,200 --> 01:36:18,240
timestep which control sort of high-level decisions about you know

1421
01:36:18,240 --> 01:36:21,640
maybe what topic are we going to be talking about the next sentence

1422
01:36:21,640 --> 01:36:24,220
given everything that we've said before

1423
01:36:24,560 --> 01:36:27,950
and then you know given that we can generate the next sentence

1424
01:36:30,680 --> 01:36:34,370
and that's that's also been work of julian serban

1425
01:36:36,790 --> 01:36:41,440
so there are other fully observed neural nets

1426
01:36:42,170 --> 01:36:45,010
there are directed graphical models that are

1427
01:36:45,510 --> 01:36:49,270
not really recurrent net but or you know maybe different from recurrent

1428
01:36:49,270 --> 01:36:51,120
nets and is worth talking about now

1429
01:36:53,610 --> 01:36:55,970
and they belong to a larger family which

1430
01:36:55,970 --> 01:36:59,140
uses the same decomposition i told you the beginning remember i

1431
01:36:59,140 --> 01:37:01,580
told you that we can take a joint distribution

1432
01:37:01,580 --> 01:37:04,890
p of x one two axiom we can write it as a product of conditionals

1433
01:37:04,890 --> 01:37:08,420
that's when one of the basic know things you should know about

1434
01:37:08,840 --> 01:37:12,850
probability distributions and so in general we get

1435
01:37:12,850 --> 01:37:15,840
three of the sort here

1436
01:37:16,040 --> 01:37:20,930
we need to know that all randomized algorithms require make model comparisons now we know

1437
01:37:20,930 --> 01:37:23,470
that so all is well

1438
01:37:23,520 --> 01:37:26,920
that's that's the comparison model any questions

1439
01:37:26,930 --> 01:37:31,680
before we go on

1440
01:37:33,400 --> 01:37:36,580
so the next topic is to burst out

1441
01:37:37,540 --> 01:37:39,090
of the comparison model

1442
01:37:39,100 --> 01:37:41,730
and try to sort in linear time

1443
01:37:41,740 --> 01:37:52,150
it's pretty clear as long as you don't have some kind of parallel algorithm or

1444
01:37:52,150 --> 01:37:57,080
something really fancy you can sort any better than linear time exhibit leask scott look

1445
01:37:57,080 --> 01:37:58,000
at the data

1446
01:37:58,030 --> 01:38:01,990
matter what you're doing the daily gotta look at otherwise you not sorting

1447
01:38:03,210 --> 01:38:06,770
OK so linear times the best we can hope for and logan it's pretty close

1448
01:38:06,860 --> 01:38:09,250
how could we saw in linear time

1449
01:38:09,320 --> 01:38:13,530
we're going to need some more powerful assumption and this is the counterexample

1450
01:38:13,560 --> 01:38:18,960
we're going to have to move outside the comparison model do something else with elements

1451
01:38:18,970 --> 01:38:21,630
and what we're going to do is assumed

1452
01:38:21,680 --> 01:38:23,300
that there are integers

1453
01:38:23,320 --> 01:38:25,300
in a particular range

1454
01:38:25,310 --> 01:38:28,530
and we'll use that to sort in linear time

1455
01:38:28,540 --> 01:38:31,290
so we're see two algorithms for sorting

1456
01:38:31,330 --> 01:38:33,790
faster than n log n

1457
01:38:33,800 --> 01:38:35,730
the first one is pretty simple

1458
01:38:35,860 --> 01:38:40,200
and we'll use it in the second half it's called counting sort

1459
01:38:40,330 --> 01:38:46,280
so the input to counting sort is an array is usual

1460
01:38:46,290 --> 01:38:51,450
but we're going to assume what those array elements look like

1461
01:38:51,460 --> 01:38:54,180
so each AI

1462
01:38:54,230 --> 01:38:55,990
is an integer

1463
01:38:56,050 --> 01:38:57,610
in the range from one

1464
01:38:57,630 --> 01:38:59,490
to catch

1465
01:38:59,490 --> 01:39:01,700
so this is a pretty strong assumption

1466
01:39:01,710 --> 01:39:05,290
the running time is actually going to depend on case of k smallest going to

1467
01:39:05,290 --> 01:39:09,670
be good algorithm case big it's going to be really bad algorithm

1468
01:39:09,720 --> 01:39:11,150
the worse than an organ

1469
01:39:11,170 --> 01:39:15,340
care goal

1470
01:39:15,350 --> 01:39:19,380
is to output some sort of version of this rank

1471
01:39:19,390 --> 01:39:23,920
let's call this

1472
01:39:23,970 --> 01:39:26,190
sorting a

1473
01:39:26,210 --> 01:39:30,800
so it's going to be easier to write down the output directly instead of writing

1474
01:39:30,800 --> 01:39:32,120
down the permutation

1475
01:39:32,210 --> 01:39:34,470
for this algorithm

1476
01:39:34,520 --> 01:39:37,760
and then we have some things iliary storage

1477
01:39:37,770 --> 01:39:41,140
i'm about to write down the pseudocode which is why

1478
01:39:41,190 --> 01:39:44,430
and for declaring all my variables here

1479
01:39:44,450 --> 01:39:49,270
and the ciliary storage will have length k

1480
01:39:49,290 --> 01:39:50,530
which is the range

1481
01:39:50,550 --> 01:39:52,380
on my input values

1482
01:39:52,400 --> 01:39:54,560
so let's

1483
01:39:54,580 --> 01:39:57,520
see the algorithm

1484
01:40:10,500 --> 01:40:15,090
this is counting sort

1485
01:40:15,110 --> 01:40:24,750
and it takes a little while to write down but it's

1486
01:40:24,800 --> 01:40:27,110
it is straightforward

1487
01:40:27,110 --> 01:40:36,130
the first is initialisation

1488
01:40:39,030 --> 01:40:40,800
then we do some counting

1489
01:40:52,130 --> 01:41:12,270
we do some something

1490
01:41:55,790 --> 01:42:00,050
actually write the output

1491
01:42:01,840 --> 01:42:35,110
so does that perfectly clear to everyone

1492
01:42:35,110 --> 01:42:36,690
no one could

1493
01:42:36,710 --> 01:42:40,340
this should illustrate how obscure so code can be

1494
01:42:40,360 --> 01:42:43,020
and when you're solving your problems that you should keep in mind that it's really

1495
01:42:43,020 --> 01:42:44,690
hard to understand

1496
01:42:44,710 --> 01:42:46,590
just given pseudocode like this

1497
01:42:46,610 --> 01:42:51,460
we need some kind of english description of what's going on because well you could

1498
01:42:51,460 --> 01:42:54,860
walk through to figure out what this means it could take half an hour to

1499
01:42:54,860 --> 01:42:56,290
an hour

1500
01:42:56,300 --> 01:42:59,090
and that's not a good way of expressing yourself

1501
01:42:59,110 --> 01:43:02,690
so what i will give you now is the english description will

1502
01:43:02,690 --> 01:43:04,380
refer back to this

1503
01:43:04,400 --> 01:43:06,670
to understand

1504
01:43:06,670 --> 01:43:10,940
this is sort of our bible what the other is supposed to do

1505
01:43:10,940 --> 01:43:13,760
sometimes and not even aware that that's what they're doing

1506
01:43:13,830 --> 01:43:16,930
and i want you to understand that on those occasions

1507
01:43:16,940 --> 01:43:19,730
our most likely respond by

1508
01:43:19,740 --> 01:43:21,480
inviting you to consider

1509
01:43:21,490 --> 01:43:22,880
the article of faith

1510
01:43:22,890 --> 01:43:26,890
that lies behind the question is creating a particular problem for you

1511
01:43:26,990 --> 01:43:31,450
i'm not going to be drawn into a philosophical or theological debate over the merits

1512
01:43:31,570 --> 01:43:36,200
of that belief but also point out how or why that belief might be making

1513
01:43:36,200 --> 01:43:40,340
it difficult for you to read or except what the text is actually and not

1514
01:43:40,340 --> 01:43:44,670
ideally saying and leave you to think about that and i see is wonderful learning

1515
01:43:44,670 --> 01:43:46,320
opportunities for the class

1516
01:43:46,340 --> 01:43:48,960
those are no way problem for me

1517
01:43:48,970 --> 01:43:51,130
all right so that's

1518
01:43:51,200 --> 01:43:55,910
if you sort of necessary facts and figures now about the bible and then i

1519
01:43:55,910 --> 01:43:58,850
need to talk a little bit about the organisation of the course so those are

1520
01:43:58,850 --> 01:44:00,690
the last two things we really need to do

1521
01:44:00,790 --> 01:44:04,140
i'm an overview of the structure of the bible so you have a couple handouts

1522
01:44:04,140 --> 01:44:05,570
that should help you here

1523
01:44:05,600 --> 01:44:09,830
so the bible is this assemblage of books and writings

1524
01:44:09,850 --> 01:44:13,790
i'm dating from approximately one thousand BCE we're gonna

1525
01:44:13,800 --> 01:44:18,860
here very diverse opinions about how far back this stuff dates down to the second

1526
01:44:18,860 --> 01:44:22,950
century last but within the hebrew bible was written in the one sixties

1527
01:44:22,960 --> 01:44:29,050
BCE some of these stories some of these books which we don't think roughly from

1528
01:44:29,050 --> 01:44:33,710
a certain date they will contain narrative snippets or legal materials are traditions oral traditions

1529
01:44:33,710 --> 01:44:37,920
that may even date back to stretch back further in time they were perhaps transmitted

1530
01:44:37,920 --> 01:44:42,560
orally and then ended up in these these written forms the bible is written largely

1531
01:44:42,560 --> 01:44:45,220
in hebrew hence the name hebrew bible

1532
01:44:45,260 --> 01:44:47,770
there are a few passages in aramaic

1533
01:44:47,820 --> 01:44:51,990
so you have to hand out that breaks down the three major components that someone

1534
01:44:53,700 --> 01:44:56,880
written in two columns per page OK

1535
01:44:57,080 --> 01:45:02,550
for the we're going to talk in minute about those three sections so you want

1536
01:45:02,550 --> 01:45:06,300
to have handy

1537
01:45:06,320 --> 01:45:11,790
these writings have had a profound and lasting impact on three world religions judaism christianity

1538
01:45:11,790 --> 01:45:13,270
and islam

1539
01:45:13,290 --> 01:45:18,420
for the jewish communities to first compile these writings in the pre-christian era

1540
01:45:18,420 --> 01:45:19,750
the bible was

1541
01:45:19,800 --> 01:45:25,000
perhaps first and foremost a record of god's eternal covenant with the jewish people

1542
01:45:25,040 --> 01:45:28,040
so the jews refer to the bible as the ten i

1543
01:45:28,060 --> 01:45:31,360
the term you see up here should be

1544
01:45:31,390 --> 01:45:34,200
also on that she turned which is really

1545
01:45:34,210 --> 01:45:35,630
the letters ten

1546
01:45:38,110 --> 01:45:41,830
and they put the leaving they're making easy to pronounce because it's hard to pronounce

1547
01:45:41,840 --> 01:45:43,280
the ten OK

1548
01:45:43,300 --> 01:45:46,520
and this is an acronym

1549
01:45:46,540 --> 01:45:48,060
and the the t

1550
01:45:48,070 --> 01:45:49,380
dance for torah

1551
01:45:49,380 --> 01:45:53,510
which is the word that means instruction or teaching often translated law i think that's

1552
01:45:53,510 --> 01:45:55,090
a very poor translation

1553
01:45:55,150 --> 01:45:57,880
since instruction way teaching

1554
01:45:57,880 --> 01:46:01,390
and that refers to the first five books that you see listed here genesis through

1555
01:46:02,400 --> 01:46:04,950
the second division of the bible is

1556
01:46:04,990 --> 01:46:08,650
referred to as navy even which is the hebrew word for profit

1557
01:46:09,220 --> 01:46:12,860
actually the prophet is divided really into two parts because there are two types of

1558
01:46:12,860 --> 01:46:15,800
writing in the prophetic section of the bible

1559
01:46:15,930 --> 01:46:20,430
the first or former profits continues the kind of narrative prose account of the history

1560
01:46:20,430 --> 01:46:23,950
of israel focusing on the activities of israel's profit

1561
01:46:23,950 --> 01:46:28,810
right so the for the former prophets are narrative text

1562
01:46:28,860 --> 01:46:35,700
the latter prophets are poetic and oracular writings that bear the name of the prophet

1563
01:46:35,700 --> 01:46:41,670
to home the writings are ascribed it the three major prophets isaiah jeremiah anything feel

1564
01:46:41,710 --> 01:46:45,920
the twelve minor prophets which in the hebrew bible get counted together as one book

1565
01:46:46,080 --> 01:46:48,260
was a lot of those twelve or very small

1566
01:46:49,260 --> 01:46:53,330
the final section of the bible is referred to as too vehement hebrew which simply

1567
01:46:53,330 --> 01:46:54,880
means writing from that

1568
01:46:54,890 --> 01:46:57,440
that's probably about fifty percent of the hebrew going to get the whole course of

1569
01:46:57,440 --> 01:47:00,510
please don't be scared by two or three other terms are useful along the way

1570
01:47:00,510 --> 01:47:03,930
but this there's really no need to know hebrew i just want you to understand

1571
01:47:03,930 --> 01:47:08,180
why to not is is the word that's used to refer to the bible

1572
01:47:08,190 --> 01:47:12,310
so the qu two v with ratings are really miscellaneous they contain works of various

1573
01:47:13,310 --> 01:47:20,030
and the three parts corresponding very roughly to the process of canonization or authoritativeness for

1574
01:47:20,030 --> 01:47:26,250
the community the torah the torah probably reached effects and authoritative status first

1575
01:47:26,260 --> 01:47:30,320
and the books of the prophets and finally the writings and probably by the end

1576
01:47:30,320 --> 01:47:34,400
of the first century all of this was organised in some way

1577
01:47:34,400 --> 01:47:37,400
if you look at the other hand out you'll see however

1578
01:47:37,450 --> 01:47:40,670
that any course on the bible is going to run immediately into the problem of

1579
01:47:40,670 --> 01:47:42,770
defining the object of study

1580
01:47:42,920 --> 01:47:48,850
because different bible served different communities over the centuries one of the earliest translations of

1581
01:47:48,850 --> 01:47:51,990
the hebrew bible was the translation into greek

1582
01:47:52,000 --> 01:47:53,700
known as the step two again

1583
01:47:53,700 --> 01:47:57,650
it was written for the benefit was translated for the benefit of jews who lived

1584
01:47:57,650 --> 01:48:03,060
in alexandria greek speaking jews who lived in alexandria egypt in the hellenistic period before

1585
01:48:03,150 --> 01:48:06,580
somewhere around three the third or second century

1586
01:48:07,750 --> 01:48:09,680
the translation

1587
01:48:09,710 --> 01:48:14,990
has some divergences with the traditional hebrew text of the bible as we now have

1588
01:48:14,990 --> 01:48:18,130
it including the order of the books and some of these things are charted for

1589
01:48:18,130 --> 01:48:20,290
you on the chart that i've i've handed out

1590
01:48:20,340 --> 01:48:21,810
the step two again

1591
01:48:21,820 --> 01:48:24,860
the tokens rationale for in the book the temporal

1592
01:48:24,900 --> 01:48:26,630
they've clustered books

1593
01:48:27,860 --> 01:48:33,280
through esther which tell of things past the books of jobs through the song of

1594
01:48:33,280 --> 01:48:34,980
songs the the song simon

1595
01:48:35,030 --> 01:48:41,400
contain wisdom that applies to the president and then the prophetic books isaiah malachi contain

1596
01:48:41,420 --> 01:48:43,510
hotel of things future

1597
01:48:43,570 --> 01:48:48,380
some copies of the set two again contains books not included in the hebrew canon

1598
01:48:48,380 --> 01:48:51,360
but accepted in the early christian can

1599
01:48:51,380 --> 01:48:55,760
this step two we get the greek translation became by and large the bible of

1600
01:48:56,880 --> 01:49:00,980
or more precisely it became the old testament of the hebrew bible

1601
01:49:01,030 --> 01:49:07,030
turn the church adopted the hebrew bible is the precursor to it's largely hellenistic gospels

1602
01:49:07,050 --> 01:49:11,920
was an important association for it with an old and respected tradition

1603
01:49:11,980 --> 01:49:17,760
our primary concern is the bible of the ancient israelite and jewish community

1604
01:49:17,780 --> 01:49:22,650
the twenty four books grouped in the torah prophets in writing on that other she

1605
01:49:22,650 --> 01:49:30,840
way of making decisions and the trick classify fire is making decisions according to the

1606
01:49:30,840 --> 01:49:37,730
lines that separate vertically or horizontally because they're just setting thresholds on the features

1607
01:49:37,820 --> 01:49:44,460
and now there is a so-called fit versus robustness trade-off

1608
01:49:44,820 --> 01:49:49,480
i told you that you know there is this problem of eventually of affecting what

1609
01:49:49,710 --> 01:49:54,190
should we sure you know a linear decision boundary for example or decision boundary that

1610
01:49:54,190 --> 01:49:58,130
that fits very well the training examples

1611
01:49:58,130 --> 01:50:01,210
well fist in new data

1612
01:50:01,230 --> 01:50:07,480
that are you not that we're not your training examples and i'm representing here is

1613
01:50:07,480 --> 01:50:15,840
filled and filled circles if you're lucky they're all going to fall on the right

1614
01:50:15,840 --> 01:50:19,840
side of the decision boundary that you've inferred from your training example

1615
01:50:19,890 --> 01:50:25,290
if you're unlucky you're going to make mistakes and are going to be more likely

1616
01:50:25,290 --> 01:50:30,090
to make mistakes if you have a simple decision boundary like that or if you

1617
01:50:30,090 --> 01:50:34,990
have a more complex one so here i'm making one mistake of one blue example

1618
01:50:34,990 --> 01:50:37,990
which is classified on the right side

1619
01:50:38,020 --> 01:50:42,920
and here i'm actually making many more errors even though you know i was fitting

1620
01:50:42,920 --> 01:50:45,230
very well the training examples

1621
01:50:45,230 --> 01:50:51,730
with my new test examples and making tons of errors so

1622
01:50:52,440 --> 01:50:56,820
obviously you know this is a made-up example i could also have drawn to test

1623
01:50:56,820 --> 01:51:02,600
examples of the deform the right side of the decision boundary in the non-linear case

1624
01:51:02,690 --> 01:51:10,730
but as it turns out there are theoretical arguments that tell you that in general

1625
01:51:10,790 --> 01:51:15,420
it's a better idea to use the simpler model and will go over that you

1626
01:51:15,480 --> 01:51:24,320
in future lectures there is another dimension that you can play with which is the

1627
01:51:24,730 --> 01:51:26,000
bias value

1628
01:51:26,450 --> 01:51:32,210
and as you can see here you can put more weight on the errors of

1629
01:51:32,250 --> 01:51:37,690
on one class as you put weight on the of the other class so if

1630
01:51:37,690 --> 01:51:44,130
you think that making error of classifying blue example into the red class is worse

1631
01:51:44,210 --> 01:51:48,570
than classifying in red example into the bill class then you might want to shift

1632
01:51:48,570 --> 01:51:52,880
your decision boundary a little bit upwards so that you have some safety margin and

1633
01:51:52,900 --> 01:51:58,290
you're not going to be making errors of classifying blue into the red class otherwise

1634
01:51:58,290 --> 01:52:01,090
if it's the other way around you might want to shift your decision boundary in

1635
01:52:01,090 --> 01:52:02,630
the other direction

1636
01:52:02,630 --> 01:52:11,490
in general you can vary that bias value and money to the trade-off between the

1637
01:52:11,490 --> 01:52:15,790
error you're making on the positive class in the area making the negative class and

1638
01:52:15,790 --> 01:52:21,070
this is what's is the so-called roc curve for our curve but the ROC curve

1639
01:52:21,130 --> 01:52:24,050
key is is you know by varying the bias

1640
01:52:24,090 --> 01:52:25,370
it plots

1641
01:52:25,390 --> 01:52:30,270
the success rate of the positive class so-called hit rate

1642
01:52:30,390 --> 01:52:32,590
versus one minus the

1643
01:52:33,360 --> 01:52:39,870
the negative class success rate of also called the false alarm rate

1644
01:52:39,890 --> 01:52:47,810
and the larger the area under that curve is the better from the point of

1645
01:52:47,810 --> 01:52:51,630
view of the classification accuracy

1646
01:52:51,690 --> 01:52:54,960
and this is one way people

1647
01:52:54,980 --> 01:53:03,380
measure classification accuracy was to be independent on the particular choice of the bias that

1648
01:53:03,380 --> 01:53:07,420
that you make the idea ROC curve is this one it has an area of

1649
01:53:07,420 --> 01:53:12,270
one and it's you know making no

1650
01:53:13,610 --> 01:53:19,070
on the negative class and then when you move device of making no error on

1651
01:53:19,070 --> 01:53:20,190
the positive class

1652
01:53:20,730 --> 01:53:26,190
the around the case when you making you know you're flipping a kind of making

1653
01:53:26,190 --> 01:53:32,820
decisions at random for the positive and negative class gives you the diagonal and so

1654
01:53:32,820 --> 01:53:36,940
are you get in that case an AUC of zero point five and this is

1655
01:53:36,940 --> 01:53:43,110
you know one actual roc care people measure things in different ways in different domains

1656
01:53:43,110 --> 01:53:48,400
so sometimes people measure lift curve ROC curves and in which case they plot you

1657
01:53:48,400 --> 01:53:52,790
know the fraction of good customers receive the fractions of customers selected don't want to

1658
01:53:52,790 --> 01:53:59,230
go into detail that but you shouldn't confuse both right it's just to plucking a

1659
01:53:59,230 --> 01:54:04,570
little bit something different you looking the hit rate versus the total fraction of of

1660
01:54:04,570 --> 01:54:09,900
people selected more general issue here is the the real picture

1661
01:54:09,920 --> 01:54:14,590
more generally if you have two classes and making predictions

1662
01:54:14,590 --> 01:54:18,370
then you will have several types of errors the false positive which are when you're

1663
01:54:18,370 --> 01:54:24,790
classifying positively an example but it really was of the negative class the false negative

1664
01:54:24,790 --> 01:54:29,490
when you're classifying an example as negative but it really was of the positive class

1665
01:54:30,110 --> 01:54:35,410
and then you have the correct classifications between negative and if true positive

1666
01:54:35,420 --> 01:54:41,090
if you add up to two negative plus false positive is the total number of

1667
01:54:41,090 --> 01:54:42,650
negative examples

1668
01:54:43,310 --> 01:54:47,190
false negative intra-party the total number of positive examples

1669
01:54:47,210 --> 01:54:52,320
and then what i call rasher rejected examples of the total number of examples that

1670
01:54:52,320 --> 01:54:56,900
you have classified as negative so when you made your decision so some of that

1671
01:54:57,920 --> 01:55:03,270
you have the selected examples the ones that you have split classified as positive

1672
01:55:03,270 --> 01:55:06,000
according to your decision function

1673
01:55:06,020 --> 01:55:07,820
and this is the overall total

1674
01:55:07,930 --> 01:55:15,990
so using these quantities you can compute virtually all the measurements that people use to

1675
01:55:17,290 --> 01:55:19,630
the performance of classic fires

1676
01:55:19,770 --> 01:55:25,270
so you talk about false alarm of false alarm rate as the fraction of false

1677
01:55:25,270 --> 01:55:32,910
positive over the total number of negative examples negative meaning belonging to the negative class

1678
01:55:32,920 --> 01:55:39,340
and actually traces the true positive over the total number of positive examples the fraction

1679
01:55:39,340 --> 01:55:45,420
of selected is the number of selected over the total number of examples

1680
01:55:45,550 --> 01:55:50,630
OK you have also precision so using this or you get a picture of of

1681
01:55:50,630 --> 01:55:54,670
you will see x-rays when hotspot here you will not see x-rays

1682
01:55:54,670 --> 01:55:56,850
so we observe from these systems

1683
01:55:56,850 --> 01:55:59,750
x-ray positions

1684
01:56:00,580 --> 01:56:02,130
think of the following

1685
01:56:02,170 --> 01:56:06,850
x-ray positions our clock the clock of the rotating neutron star

1686
01:56:06,900 --> 01:56:10,420
if the neutron star in a binary system because they always and the binary system

1687
01:56:10,420 --> 01:56:12,920
the x-ray binaries if it's coming to you

1688
01:56:12,920 --> 01:56:17,830
using doppler shift the text of the clock commanded closer together if the neutron star

1689
01:56:17,830 --> 01:56:21,770
moves away from you the text of the clocks a little bit further apart that's

1690
01:56:21,770 --> 01:56:23,980
exactly the doppler shift is all about

1691
01:56:24,060 --> 01:56:29,080
so by timing the policies of the x-rays you can get a handle on the

1692
01:56:29,080 --> 01:56:30,400
doppler shift of the

1693
01:56:30,400 --> 01:56:34,020
neutron star that means you can get the speed of the neutron star you can

1694
01:56:34,020 --> 01:56:37,520
get the radius of the orbit you can get the period just like we discussed

1695
01:56:38,810 --> 01:56:42,060
but now you taken off the x-ray observations but we have to be made from

1696
01:56:42,060 --> 01:56:43,600
outside to the atmosphere

1697
01:56:43,650 --> 01:56:46,080
because x-rays are absorbed by the earth's atmosphere

1698
01:56:46,130 --> 01:56:49,270
now you think an optical telescope and you look from the ground

1699
01:56:49,330 --> 01:56:50,500
and now you see

1700
01:56:50,520 --> 01:56:53,000
the optical spectrum of the donor

1701
01:56:53,020 --> 01:56:55,940
and what you see in the don't you see these absorption lines

1702
01:56:55,960 --> 01:57:00,250
and as the donor moves around the centre of mass this absorption lines move back

1703
01:57:00,250 --> 01:57:01,270
and forth

1704
01:57:01,290 --> 01:57:04,900
the doppler shift of the donor so you know the velocity of the donor you

1705
01:57:04,900 --> 01:57:08,380
know the radius of the donor not the radius of the domain of radius of

1706
01:57:08,380 --> 01:57:10,520
the orbit you know the period

1707
01:57:10,580 --> 01:57:12,310
so now we have a situation

1708
01:57:12,350 --> 01:57:14,420
but i just described earlier

1709
01:57:14,440 --> 01:57:17,440
that you have the doppler shift of both

1710
01:57:18,770 --> 01:57:21,460
and remember i told you that you also get

1711
01:57:21,460 --> 01:57:25,370
massive you get the mass of the donor and the mass of the

1712
01:57:25,370 --> 01:57:28,420
the creator

1713
01:57:28,440 --> 01:57:31,770
before i go ahead let me show you some

1714
01:57:31,810 --> 01:57:35,100
slides so we have to lower this again

1715
01:57:35,150 --> 01:57:36,110
it's possible

1716
01:57:42,600 --> 01:57:45,100
i want to show you an artist

1717
01:57:45,100 --> 01:57:47,480
conception of such a

1718
01:57:47,520 --> 01:57:57,370
binary system

1719
01:57:57,480 --> 01:58:01,630
so this is what it may look like

1720
01:58:01,670 --> 01:58:04,060
you see the don't know there

1721
01:58:04,110 --> 01:58:07,250
and you see the neutron star

1722
01:58:07,270 --> 01:58:09,960
right here so small of course invisible

1723
01:58:09,980 --> 01:58:12,150
and this is the accretion disk

1724
01:58:12,170 --> 01:58:13,710
swirls in the matter

1725
01:58:13,750 --> 01:58:16,420
hands up on the neutron star and is another view

1726
01:58:19,830 --> 01:58:21,020
give you an idea

1727
01:58:21,020 --> 01:58:22,460
one of the donor

1728
01:58:22,460 --> 01:58:24,040
and then this world of matter

1729
01:58:24,080 --> 01:58:29,130
and then this world and ends up here on the north pole

1730
01:58:29,150 --> 01:58:30,540
and here

1731
01:58:30,600 --> 01:58:31,460
you see

1732
01:58:31,480 --> 01:58:34,380
they that were obtained in nineteen seventy one

1733
01:58:34,380 --> 01:58:38,900
it's clear evidence for the existence of these rotating neutron stars with the x three

1734
01:58:40,330 --> 01:58:44,790
you see here the observed x-ray intensity as a function of time

1735
01:58:44,870 --> 01:58:48,350
and the actual data it is very thin lines

1736
01:58:48,400 --> 01:58:52,250
and this bold line was drawn over by authors to convince you

1737
01:58:52,270 --> 01:58:53,310
that you see

1738
01:58:53,330 --> 01:58:55,790
the signal which is highly periodic

1739
01:58:55,810 --> 01:58:59,060
time from here to here is one point two four seconds this object was called

1740
01:58:59,060 --> 01:59:00,540
hercules x one

1741
01:59:00,540 --> 01:59:04,000
so this is one of the magnetic poles and this is due to magnetic

1742
01:59:04,060 --> 01:59:06,500
one magnetic pole and other magnetic

1743
01:59:06,520 --> 01:59:11,210
so you see unmistakably the rotation of the neutron star the x-ray position

1744
01:59:11,290 --> 01:59:13,900
you see data from the same object

1745
01:59:13,960 --> 01:59:17,870
but now the time scale is very different from here to here is one day

1746
01:59:17,920 --> 01:59:19,650
this is two days

1747
01:59:19,650 --> 01:59:20,920
and when you look at this

1748
01:59:20,920 --> 01:59:23,210
they don't forget this for now

1749
01:59:23,230 --> 01:59:25,400
notice that you see

1750
01:59:25,420 --> 01:59:29,330
the source is active in x-rays the one point four two second oscillations you cannot

1751
01:59:29,330 --> 01:59:32,250
see of course anymore because the timescale is different

1752
01:59:32,330 --> 01:59:35,150
but notice here no x-rays at all

1753
01:59:35,150 --> 01:59:37,900
one point seven days later no x-rays at all

1754
01:59:37,920 --> 01:59:40,630
one point seven days later no x-rays at all

1755
01:59:40,650 --> 01:59:45,060
so what you're looking at here what we call x-ray eclipse when the neutron star

1756
01:59:45,060 --> 01:59:46,630
moves behind

1757
01:59:46,650 --> 01:59:50,670
the donor star all the x-rays are absorbed by the donor star you get extra

1758
01:59:50,670 --> 01:59:56,060
clips is another words you get independently from the doppler shift you also get

1759
01:59:56,080 --> 01:59:58,290
the period of the orbit

1760
01:59:58,330 --> 02:00:00,080
by the x-ray

1761
02:00:02,060 --> 02:00:06,850
and this really changed our concept of

1762
02:00:08,380 --> 02:00:09,520
of astronomy

1763
02:00:09,540 --> 02:00:11,380
the existence of these

1764
02:00:11,440 --> 02:00:13,980
neutron star binaries

1765
02:00:16,060 --> 02:00:17,880
now comes the part

1766
02:00:17,880 --> 02:00:22,190
well i the masses of these objects i already alluded due to the idea of

1767
02:00:22,190 --> 02:00:25,770
the possibility that there may be black holes

1768
02:00:26,520 --> 02:00:30,500
all the mass measurements that have been done to date of these neutron stars where

1769
02:00:30,520 --> 02:00:31,520
you UCD

1770
02:00:31,540 --> 02:00:32,750
both nations

1771
02:00:32,770 --> 02:00:34,210
all of them

1772
02:00:34,230 --> 02:00:35,560
very close

1773
02:00:35,560 --> 02:00:38,350
two one point four solar mass

1774
02:00:38,400 --> 02:00:41,210
and there's a good reason for that that's not an accident

1775
02:00:41,270 --> 02:00:43,310
in nineteen thirty

1776
02:00:43,330 --> 02:00:45,520
the physicist john cigar

1777
02:00:47,040 --> 02:00:48,600
that white dwarfs

1778
02:00:48,630 --> 02:00:52,270
could not exist if the mass is larger than one point four solar mass was

1779
02:00:52,270 --> 02:00:57,170
quantum mechanical calculations for which he received in nineteen eighty three nobel prize

1780
02:00:57,210 --> 02:01:00,480
remember we discussed wide or early white dwarf

1781
02:01:00,500 --> 02:01:04,730
it's about a radius of ten thousand kilometres about the same as the earth

1782
02:01:04,790 --> 02:01:07,310
and imagine that you have a white dwarf

1783
02:01:07,350 --> 02:01:11,440
you add that to the white dwarf you pass the one point four solar mass

1784
02:01:11,440 --> 02:01:15,500
more than the white dwarf will collapse and becomes a neutron star

1785
02:01:15,810 --> 02:01:19,480
and so when we measure the masses of neutron stars it turns out

1786
02:01:19,500 --> 02:01:24,730
maybe someone by surprise that they are all very close to one point four

1787
02:01:25,830 --> 02:01:30,330
you could add more matter to the neutron star by creating more and more matter

1788
02:01:30,400 --> 02:01:32,040
and you reach the point

1789
02:01:32,060 --> 02:01:36,730
that's the neutron star becomes as massive as three times the mass of the sun

1790
02:01:36,790 --> 02:01:41,230
we believe that the neutron star can no longer support itself and becomes

1791
02:01:41,270 --> 02:01:43,110
a black hole

1792
02:01:43,130 --> 02:01:46,830
so now comes the question what is a black hole

1793
02:01:46,850 --> 02:01:47,960
the black hole

1794
02:01:47,980 --> 02:01:51,190
is the most bizarre objects that you can imagine

1795
02:01:51,230 --> 02:01:55,060
and it is something that you want to stay away from two

1796
02:01:55,060 --> 02:01:56,110
a black hole

1797
02:01:56,130 --> 02:01:59,130
has no size

1798
02:01:59,130 --> 02:02:01,080
unlike the neutron star

1799
02:02:01,110 --> 02:02:02,480
it has no size

1800
02:02:03,230 --> 02:02:05,880
it doesn't have mass

1801
02:02:05,900 --> 02:02:07,540
and it has a lot of mass

1802
02:02:07,600 --> 02:02:10,670
three times the mass of the sun ten times the mass of the sun a

1803
02:02:10,710 --> 02:02:13,210
hundred times the mass of the sun

1804
02:02:13,250 --> 02:02:14,730
so it has mass

1805
02:02:14,790 --> 02:02:16,810
but it has no side

1806
02:02:16,830 --> 02:02:19,480
we identify

1807
02:02:19,560 --> 02:02:21,600
around the black hole

1808
02:02:21,610 --> 02:02:23,730
the sphere with radius are

1809
02:02:23,770 --> 02:02:25,350
which we call

1810
02:02:25,350 --> 02:02:27,540
the event horizon

1811
02:02:27,540 --> 02:02:32,230
imagine you're are at the event horizon

1812
02:02:32,250 --> 02:02:33,440
you want to

1813
02:02:33,440 --> 02:02:35,370
get away from the black hole

1814
02:02:35,380 --> 02:02:37,690
what kind of speed do you need

1815
02:02:37,750 --> 02:02:39,020
you should be able

1816
02:02:39,040 --> 02:02:42,150
to give me you answer immediately escape velocity

1817
02:02:42,150 --> 02:02:42,980
must be

1818
02:02:43,000 --> 02:02:45,080
two and

1819
02:02:45,130 --> 02:02:46,330
divided by

1820
02:02:46,330 --> 02:02:48,710
the radius of the event horizon

1821
02:02:48,770 --> 02:02:52,420
in other words the radius of the event horizon itself

1822
02:02:52,440 --> 02:02:54,380
equals two mg

1823
02:02:54,420 --> 02:02:57,110
divided by c squared

1824
02:02:57,150 --> 02:02:59,900
if you tell me what and this i'll will tell you what the radius of

1825
02:02:59,900 --> 02:03:02,290
the event horizon is

1826
02:03:02,350 --> 02:03:05,900
i when the little fast here i skipped an important step

1827
02:03:05,920 --> 02:03:10,790
v is the escape velocity from the event horizon which is at a distance capital

1828
02:03:10,810 --> 02:03:12,610
are from the mass and

1829
02:03:12,650 --> 02:03:14,130
so we see that here

1830
02:03:14,170 --> 02:03:17,810
now the escape velocity can never be larger than the speed of light

1831
02:03:17,830 --> 02:03:20,110
so the maximum value possible

1832
02:03:21,670 --> 02:03:24,650
and now you look at this part of the equation in you take

1833
02:03:24,670 --> 02:03:26,560
the radius on one side

1834
02:03:26,560 --> 02:03:29,730
you get the radius of the event horizon

1835
02:03:29,770 --> 02:03:31,130
equals two and

1836
02:03:33,290 --> 02:03:34,400
divided by

1837
02:03:35,020 --> 02:03:36,370
c square

1838
02:03:36,440 --> 02:03:41,380
that's how i found that equation sorry that i went a little too fast

1839
02:03:41,520 --> 02:03:43,630
if m is the mass of the earth

1840
02:03:43,670 --> 02:03:46,810
the radius of the event horizon is one centimeter

1841
02:03:46,810 --> 02:03:50,310
now we want to look at

1842
02:03:52,930 --> 02:03:54,820
so we have seen

1843
02:03:54,870 --> 02:03:56,770
an analysis of

1844
02:03:56,950 --> 02:04:01,140
the problem

1845
02:04:01,730 --> 02:04:03,920
we can also write

1846
02:04:04,030 --> 02:04:06,680
the following me

1847
02:04:06,690 --> 02:04:13,530
one minus two

1848
02:04:23,040 --> 02:04:41,250
so this is another way of writing the constrained optimisation problem that give rise to

1849
02:04:41,250 --> 02:04:43,440
the update

1850
02:04:43,450 --> 02:04:46,200
for the green we so before

1851
02:04:46,250 --> 02:04:51,230
this this algorithm and the solution of an iterative update

1852
02:04:51,240 --> 02:04:52,540
it's cool

1853
02:04:53,940 --> 02:05:01,890
passive aggressive

1854
02:05:01,970 --> 02:05:07,120
passive aggressive air

1855
02:05:08,810 --> 02:05:12,190
because of this

1856
02:05:12,210 --> 02:05:18,390
and the article passive-aggressive because it doesn't have more than which is not an aggressive

1857
02:05:18,390 --> 02:05:20,710
model which is carried out

1858
02:05:20,760 --> 02:05:23,110
aggressively to fix the date

1859
02:05:23,240 --> 02:05:26,920
and this is your

1860
02:05:27,210 --> 02:05:31,390
comedy crime out

1861
02:05:31,400 --> 02:05:33,800
yoram singer shy

1862
02:05:33,810 --> 02:05:36,350
shallow versus well

1863
02:05:36,360 --> 02:05:38,660
we also have to show that

1864
02:05:38,680 --> 02:05:42,530
a bunch of

1865
02:05:42,570 --> 02:05:44,370
the at all

1866
02:05:44,380 --> 02:05:47,030
the visit

1867
02:05:47,040 --> 02:05:53,340
paper in the journal of machine learning research but if you look for the great

1868
02:05:54,700 --> 02:05:59,340
OK now of course we can wonder

1869
02:05:59,380 --> 02:06:02,770
other variants of these are growing in which

1870
02:06:02,820 --> 02:06:05,360
you are looking at

1871
02:06:05,370 --> 02:06:10,650
the other version of the SVM the one we it gives rise to the so-called

1872
02:06:10,650 --> 02:06:12,260
boxcar track

1873
02:06:12,270 --> 02:06:18,090
in which the slack variables doesn't appear clear about here

1874
02:06:20,900 --> 02:06:22,370
when i'm looking at

1875
02:06:22,380 --> 02:06:27,790
problem like

1876
02:06:27,810 --> 02:06:30,970
one way of looking at the problem

1877
02:06:31,040 --> 02:06:32,380
in w

1878
02:06:32,500 --> 02:06:37,610
god one minus

1879
02:06:40,530 --> 02:06:43,770
class c

1880
02:06:47,550 --> 02:06:48,950
such that

1881
02:06:49,120 --> 02:06:56,600
your y w x being good at one time

1882
02:07:01,510 --> 02:07:06,110
but this is

1883
02:07:06,670 --> 02:07:08,360
this is called the

1884
02:07:08,370 --> 02:07:09,700
passive aggressive

1885
02:07:17,750 --> 02:07:20,450
there is another way

1886
02:07:20,460 --> 02:07:22,400
so both both ways

1887
02:07:22,410 --> 02:07:26,460
both of these are ways of coping

1888
02:07:28,410 --> 02:07:31,090
teams that are potentially non separable

1889
02:07:31,330 --> 02:07:33,860
you relax the aggressive

1890
02:07:33,880 --> 02:07:36,420
condition that is in fact

1891
02:07:39,130 --> 02:07:42,140
the analysis the reduction i was showing you

1892
02:07:42,290 --> 02:07:44,390
before that and work

1893
02:07:44,440 --> 02:07:50,150
for this second version here thought about properties of this algorithm will have to work

1894
02:07:50,390 --> 02:07:52,080
in different ways

1895
02:07:53,310 --> 02:07:54,280
by the way

1896
02:07:54,290 --> 02:07:56,720
in this paper

1897
02:07:56,770 --> 02:08:00,400
that proves of proofs of of

1898
02:08:00,440 --> 02:08:02,710
of the about the words showing you

1899
02:08:02,720 --> 02:08:06,230
giving using different techniques

1900
02:08:06,250 --> 02:08:10,070
so i am

1901
02:08:11,310 --> 02:08:15,320
by using some some of the things that the conventional to prove

1902
02:08:15,370 --> 02:08:16,860
about support for these

1903
02:08:16,870 --> 02:08:19,130
but the techniques that i think are

1904
02:08:19,170 --> 02:08:24,280
most suggestive for relating it and explaining new things

1905
02:08:24,290 --> 02:08:27,580
and what i did back any more usable

1906
02:08:27,590 --> 02:08:30,400
in the paper you will find another another

1907
02:08:30,420 --> 02:08:33,040
there is a different edition

1908
02:08:35,180 --> 02:08:39,150
first of all let me start by

1909
02:08:40,900 --> 02:08:45,820
let me start by the solution of it but want to solve this using the

1910
02:08:45,960 --> 02:08:47,560
founder of the

1911
02:08:48,450 --> 02:08:51,580
one of the musicians

1912
02:08:51,590 --> 02:08:53,850
to derive the up

