1
00:00:00,000 --> 00:00:03,710
you may get something unexpected if you have a small the sample size to get

2
00:00:04,770 --> 00:00:07,380
unexpected somehow have to find

3
00:00:07,440 --> 00:00:08,710
the middle

4
00:00:08,740 --> 00:00:13,600
they have optimal equivalence emphasizing questions now what properties of data

5
00:00:13,610 --> 00:00:18,650
actually determine the value of the equivalent sample size

6
00:00:18,700 --> 00:00:21,330
so we treated now as an additional parameter

7
00:00:21,340 --> 00:00:24,560
that we want to learn and among all the different

8
00:00:24,850 --> 00:00:30,860
criteria that we could optimize we just follow the objective function of time still under

9
00:00:31,140 --> 00:00:34,100
and co-workers from last year that we want to

10
00:00:38,010 --> 00:00:41,640
marginal likelihood again and find the alpha and

11
00:00:41,650 --> 00:00:44,680
the graph maximizing

12
00:00:44,720 --> 00:00:48,890
and that can be done easily in accordance percent

13
00:00:48,950 --> 00:00:50,710
approach just iterate

14
00:00:50,840 --> 00:00:53,320
two steps in the first step

15
00:00:53,330 --> 00:00:55,230
you optimize graph

16
00:00:55,240 --> 00:00:57,110
for a fixed

17
00:00:58,260 --> 00:01:01,990
sample size value that can be done with any standard

18
00:01:02,000 --> 00:01:04,100
structure learning algorithm

19
00:01:04,150 --> 00:01:08,350
and the second step is now optimise the equivalent

20
00:01:08,350 --> 00:01:10,350
sample size value

21
00:01:10,390 --> 00:01:11,950
for a fixed graph

22
00:01:11,970 --> 00:01:13,630
and that's not interesting

23
00:01:13,640 --> 00:01:16,440
part how can we do that

24
00:01:16,470 --> 00:01:20,380
and what i do is now do a lot of very crude approximations

25
00:01:20,400 --> 00:01:25,790
because i just want to find out what are the main effects of main properties

26
00:01:25,790 --> 00:01:27,830
of the data that affect

27
00:01:27,900 --> 00:01:30,860
the equivalent sample size value

28
00:01:30,870 --> 00:01:34,440
so let's start out again with the marginal likelihood

29
00:01:34,480 --> 00:01:35,380
and this

30
00:01:35,630 --> 00:01:39,420
peter grew out mentioned yesterday you can understand it

31
00:01:39,470 --> 00:01:43,590
a measure of predictive accuracy in the prequential

32
00:01:43,640 --> 00:01:47,020
that means that you assume that theta arrive sequentially

33
00:01:47,070 --> 00:01:48,790
and then every

34
00:01:48,800 --> 00:01:51,300
that you predict that point

35
00:01:51,310 --> 00:01:54,790
given all the previous data points

36
00:01:54,800 --> 00:01:57,500
and now i approximate that by

37
00:01:57,550 --> 00:01:59,970
the frequentist test our

38
00:02:00,020 --> 00:02:02,800
which is very crude i admitted

39
00:02:02,810 --> 00:02:06,590
so this is not the test error that you

40
00:02:06,600 --> 00:02:09,870
typically get when you cross validation

41
00:02:11,040 --> 00:02:14,820
now as the next step i can approximate this one

42
00:02:14,830 --> 00:02:16,470
on the one hand the

43
00:02:16,510 --> 00:02:18,590
a great information criteria

44
00:02:18,600 --> 00:02:20,370
so i

45
00:02:20,420 --> 00:02:23,690
and just computed from the training error and the penalty

46
00:02:23,710 --> 00:02:27,080
and doc ic ac information criterion independent

47
00:02:30,380 --> 00:02:31,840
so that's the one thing

48
00:02:31,850 --> 00:02:35,440
and then i can do is second approximation

49
00:02:35,450 --> 00:02:37,710
there i assume no

50
00:02:37,720 --> 00:02:40,720
that in this test relates my

51
00:02:41,880 --> 00:02:43,350
described by the model

52
00:02:45,040 --> 00:02:48,110
test distribution that now this

53
00:02:48,170 --> 00:02:52,570
true distribution takes the form as implied by the patient approach

54
00:02:53,200 --> 00:02:55,690
i don't know the optimal

55
00:02:56,660 --> 00:03:00,000
alpha star

56
00:03:00,000 --> 00:03:02,630
and now i can just create both of those

57
00:03:04,500 --> 00:03:07,420
and get a leading order approximation my

58
00:03:07,480 --> 00:03:10,740
excellent performance optimally equivalent sample size

59
00:03:10,750 --> 00:03:14,790
and as you can see it's a ratio of the effective number

60
00:03:14,800 --> 00:03:17,630
of parameters innovation network

61
00:03:20,920 --> 00:03:22,860
basically the negative

62
00:03:22,870 --> 00:03:27,110
entropy of the empirical distribution or the maximum likelihood

63
00:03:27,150 --> 00:03:28,750
divided by n

64
00:03:28,750 --> 00:03:33,180
and the other term is basically the cross the likelihood of the

65
00:03:34,170 --> 00:03:37,610
prior q and the empirical distribution

66
00:03:37,660 --> 00:03:40,290
divided by an again

67
00:03:40,360 --> 00:03:44,800
a sanity check we can now see that the nominator denominator is indeed

68
00:03:44,860 --> 00:03:46,560
a positive number

69
00:03:46,600 --> 00:03:50,990
if q is uniform because it can be rewritten in terms of entropy is and

70
00:03:50,990 --> 00:03:52,860
KL divergence

71
00:03:52,910 --> 00:03:57,290
and so we actually get that the

72
00:03:57,330 --> 00:04:00,110
star is in the positive test p

73
00:04:00,130 --> 00:04:02,740
now we can also derive a few properties

74
00:04:02,740 --> 00:04:08,810
i would like to thank your organisation of the wide wonderful workshop i really enjoy

75
00:04:08,840 --> 00:04:10,390
being here

76
00:04:10,450 --> 00:04:15,290
and i might be talking about stability selection and i think it's a good idea

77
00:04:16,360 --> 00:04:17,880
an example

78
00:04:17,940 --> 00:04:21,150
what i mean by stability

79
00:04:21,190 --> 00:04:26,310
and so i think this is indeed one example stability

80
00:04:26,920 --> 00:04:29,720
i say what

81
00:04:29,720 --> 00:04:32,240
not sleeping beings

82
00:04:32,290 --> 00:04:34,540
i will not so easily change his mind

83
00:04:34,690 --> 00:04:37,760
i think it just means that we can

84
00:04:37,770 --> 00:04:39,370
what makes

85
00:04:39,400 --> 00:04:41,440
airplane going to tell me

86
00:04:41,500 --> 00:04:42,290
and so

87
00:04:42,320 --> 00:04:45,590
this is the what she called

88
00:04:45,620 --> 00:04:47,870
there's actually

89
00:04:55,960 --> 00:04:59,440
congratulations to the humboldt prize

90
00:04:59,510 --> 00:05:05,620
i think that the right place to say that replaced here you've got it

91
00:05:05,650 --> 00:05:08,160
well let's good jokes young side

92
00:05:08,220 --> 00:05:09,490
they talk about

93
00:05:09,510 --> 00:05:10,820
my talk

94
00:05:11,620 --> 00:05:12,570
what i wanted

95
00:05:12,600 --> 00:05:15,210
explain you can show you that

96
00:05:15,220 --> 00:05:18,960
you want to do some kind of better detection bars

97
00:05:19,020 --> 00:05:21,960
recovery happen all variables

98
00:05:21,960 --> 00:05:23,800
of components or

99
00:05:23,830 --> 00:05:25,570
factors in the

100
00:05:25,660 --> 00:05:29,160
and the top of the race kind of the

101
00:05:29,270 --> 00:05:31,350
burning question how many people

102
00:05:31,380 --> 00:05:33,020
actually you know what

103
00:05:33,070 --> 00:05:36,410
how many components meaning

104
00:05:36,460 --> 00:05:39,350
so roughly speaking what today

105
00:05:39,360 --> 00:05:40,510
talk about

106
00:05:40,620 --> 00:05:43,820
regularized estimation discrete structures

107
00:05:43,830 --> 00:05:46,660
in high dimensional saving and i think

108
00:05:46,710 --> 00:05:49,930
this is very real and in many practical applications

109
00:05:49,990 --> 00:05:53,160
often also substantially more

110
00:05:53,380 --> 00:06:02,550
here is another example of his example this biology example would be willing to fix

111
00:06:02,770 --> 00:06:05,430
just go with little example

112
00:06:05,440 --> 00:06:06,690
it's an example being the

113
00:06:06,710 --> 00:06:13,090
actually quite a lot of success by applying this last type one station

114
00:06:13,150 --> 00:06:15,030
so what about

115
00:06:15,050 --> 00:06:19,920
finding it for transcription factor binding sites in DNA sequences

116
00:06:19,960 --> 00:06:24,420
roughly speaking this just decide the protein can bind to DNA

117
00:06:24,460 --> 00:06:26,090
i mean mean it

118
00:06:26,110 --> 00:06:29,530
have to tell you the information about the problem

119
00:06:29,550 --> 00:06:32,490
univariate response variable y

120
00:06:32,620 --> 00:06:37,180
a major some kind of binding intensity on of course

121
00:06:37,240 --> 00:06:40,370
we have a very high dimensional exists

122
00:06:40,420 --> 00:06:47,300
and the j th component j covariance matrices score of candidate binding site so you

123
00:06:47,300 --> 00:06:49,270
put together a lot of candidates

124
00:06:49,330 --> 00:06:52,200
and want to link

125
00:06:52,390 --> 00:06:56,120
so what you do is or what we do is a random variable selection in

126
00:06:56,120 --> 00:06:56,830
additive model

127
00:06:57,300 --> 00:06:59,150
so we went

128
00:06:59,210 --> 00:07:00,400
well why

129
00:07:00,930 --> 00:07:04,900
in the function of x and y

130
00:07:04,990 --> 00:07:10,300
sample sizes to eighty seven he is not super high dimensional but in many real

131
00:07:10,900 --> 00:07:14,430
they have problems in high dimensions in biology

132
00:07:14,490 --> 00:07:15,530
if p

133
00:07:15,550 --> 00:07:16,780
o one million

134
00:07:19,310 --> 00:07:21,370
really very high dimensional

135
00:07:21,460 --> 00:07:26,980
so the random variable selection is that model because the same for the next four

136
00:07:28,000 --> 00:07:31,250
you want to reason why algorithm and

137
00:07:31,270 --> 00:07:32,680
twenty six going

138
00:07:34,830 --> 00:07:39,680
today then you would say OK these twenty six variables these are interesting candidates interesting

139
00:07:39,680 --> 00:07:42,400
binding sites in

140
00:07:42,420 --> 00:07:46,180
you report these findings to the biologists

141
00:07:48,150 --> 00:07:50,400
we this

142
00:07:50,420 --> 00:07:53,490
i don't trust this selection algorithms

143
00:07:54,330 --> 00:07:58,370
the burning question how stable are these

144
00:07:58,390 --> 00:08:01,900
and to be honest i mean never quite many biology problem

145
00:08:01,920 --> 00:08:04,780
i would never support when

146
00:08:04,800 --> 00:08:08,980
we really want to make sure that your findings are kind of more stable more

147
00:08:08,980 --> 00:08:18,800
robust and only then you reported to the biology because it's quite laborious biological station

148
00:08:18,810 --> 00:08:19,820
OK so

149
00:08:19,850 --> 00:08:22,130
how can we address this question

150
00:08:22,180 --> 00:08:25,570
we trust our in-house they findings

151
00:08:25,620 --> 00:08:26,980
and i mean

152
00:08:27,010 --> 00:08:29,620
i don't have so many great idea

153
00:08:29,670 --> 00:08:31,290
i think the simple idea

154
00:08:31,370 --> 00:08:35,750
let's use subsampling or bootstrapping

155
00:08:35,800 --> 00:08:38,870
and applied to biology problems

156
00:08:39,050 --> 00:08:41,910
the run sparse modeling

157
00:08:41,950 --> 00:08:46,880
it includes the variable selection two thousand seven he almost

158
00:08:46,990 --> 00:08:51,800
the first subsample dataset maybe select variable ten

159
00:08:51,900 --> 00:08:54,990
some of the variables are selected over and over again

160
00:08:55,040 --> 00:08:59,420
and of course you end up with a keep the variable be declared to be

161
00:08:59,420 --> 00:09:01,600
stable if have occurred

162
00:09:01,620 --> 00:09:09,570
more than eighty percent seven or ninety percent or in general high threshold to cross

163
00:09:11,800 --> 00:09:13,750
he was reported variables

164
00:09:13,790 --> 00:09:16,300
and in this particular data

165
00:09:16,360 --> 00:09:19,610
only two stable findings me

166
00:09:19,660 --> 00:09:21,810
initially twenty six

167
00:09:21,860 --> 00:09:27,130
i this subsampling stability procedure only two stable remains

168
00:09:27,180 --> 00:09:32,590
and actually is too stable findings they are highly interesting was blocking the protein

169
00:09:32,600 --> 00:09:37,190
one of the covariance its corresponding to the function

170
00:09:37,240 --> 00:09:42,550
like nonlinear corresponds to a true known binding site

171
00:09:45,280 --> 00:09:47,000
stable covariance

172
00:09:47,030 --> 00:09:50,620
response to the people which is almost linear

173
00:09:50,680 --> 00:09:56,840
and this is an interesting can be used for elements of the particle

174
00:09:56,880 --> 00:09:57,990
and do

175
00:09:58,050 --> 00:10:01,110
a lot of citations because i told

176
00:10:01,120 --> 00:10:03,430
this is stable

177
00:10:03,490 --> 00:10:07,920
OK so biology aside this kind of my introductory example

178
00:10:07,970 --> 00:10:12,870
and this is what you want to ask questions how she

179
00:10:14,540 --> 00:10:16,690
and in election he

180
00:10:16,790 --> 00:10:22,850
it was really more than eighty percent of the time ninety

181
00:10:22,860 --> 00:10:23,850
OK so

182
00:10:23,870 --> 00:10:28,570
this is game with an outline the show you

183
00:10:28,620 --> 00:10:32,560
in our experience and i actually spent quite a while

184
00:10:32,810 --> 00:10:37,310
subsampling bootstrapping in the context of selection you

185
00:10:38,050 --> 00:10:43,170
the course is nothing else than the procedure on selection are solely or primarily you've

186
00:10:43,170 --> 00:10:44,630
heard about the

187
00:10:44,680 --> 00:10:47,860
it and the predictions are just doing this

188
00:10:48,360 --> 00:10:50,680
and it really works

189
00:10:51,610 --> 00:10:58,410
he were in our many empirical example what's new is that we can justify

190
00:10:58,480 --> 00:11:03,760
this procedure a very simple procedure and very interesting properties

191
00:11:03,800 --> 00:11:06,680
what will be able to achieve and i'll show you

192
00:11:06,740 --> 00:11:07,610
you can show

193
00:11:07,620 --> 00:11:10,940
finite sample control of reform

194
00:11:11,010 --> 00:11:13,620
easily able to do so

195
00:11:13,810 --> 00:11:18,440
so in a sense what we're doing is really a marriage of high dimensional selection

196
00:11:18,450 --> 00:11:22,810
i would like to see that the graph modeling

197
00:11:26,570 --> 00:11:30,050
OK so here is what it is

198
00:11:30,100 --> 00:11:31,240
let's just

199
00:11:31,250 --> 00:11:34,200
we simply consider linear model

200
00:11:34,200 --> 00:11:35,700
so these are the

201
00:11:35,710 --> 00:11:39,120
three main classes of approaches to

202
00:11:39,210 --> 00:11:41,230
two graph comparison

203
00:11:41,240 --> 00:11:46,520
the isomorphism based approaches to graph edit distance based approaches and the approaches

204
00:11:46,570 --> 00:11:52,750
based on top logical descriptors

205
00:11:52,810 --> 00:11:54,050
as i have said

206
00:11:54,060 --> 00:11:56,170
they either suffer from the fact

207
00:11:56,180 --> 00:11:59,310
they have worst-case exponential runtime

208
00:11:59,320 --> 00:12:06,420
or that they neglect important topological information that is encoded in the graph structure

209
00:12:06,430 --> 00:12:07,990
that's why

210
00:12:08,050 --> 00:12:13,430
and this is somehow what inspired to work on graph cards people are interested in

211
00:12:13,430 --> 00:12:17,220
a polynomial runtime alternative

212
00:12:17,970 --> 00:12:23,100
graph edit distances and to isomorphism based graph comparison

213
00:12:23,180 --> 00:12:24,520
that captures

214
00:12:24,540 --> 00:12:30,540
as much of the topological information encoded in the graphs as possible

215
00:12:30,560 --> 00:12:32,350
so graph kernels

216
00:12:32,360 --> 00:12:34,720
similarity measures on graphs

217
00:12:34,840 --> 00:12:37,430
let's compare substructures of graphs

218
00:12:37,470 --> 00:12:41,810
which are computable in polynomial time

219
00:12:41,840 --> 00:12:44,400
and ideally such

220
00:12:44,420 --> 00:12:50,220
a graph kernels such a similarity measure on graphs is computable in polynomial time

221
00:12:50,290 --> 00:12:51,800
would be expressive

222
00:12:54,100 --> 00:12:57,100
although most of the topology that is encoded

223
00:12:57,110 --> 00:12:58,970
in the graph structure

224
00:12:59,100 --> 00:13:02,400
it will be efficient to compute

225
00:13:02,420 --> 00:13:05,650
it will be positive definite and that's a requirement

226
00:13:07,750 --> 00:13:13,920
machine learning such as this color can be used in connection with convex optimization tools

227
00:13:14,820 --> 00:13:16,620
get more into this later on

228
00:13:16,640 --> 00:13:17,770
in the tutorial

229
00:13:17,790 --> 00:13:20,130
and a fourth criterion is that

230
00:13:20,180 --> 00:13:24,950
your graph kernel - your similarity measure on graphs held be applicable to a wide range of

231
00:13:26,800 --> 00:13:29,220
of course you you might be able to

232
00:13:29,230 --> 00:13:31,150
define efficient draft

233
00:13:31,200 --> 00:13:33,800
kernels for subgraphs of graphs strings

234
00:13:33,810 --> 00:13:35,750
or trees

235
00:13:37,540 --> 00:13:39,880
graphs is cross in general

236
00:13:39,890 --> 00:13:44,100
is interesting and not all graphs that are being produced in application domains of trees

237
00:13:44,100 --> 00:13:52,890
all strings we really need similarity measures to do that work on graphs in general

238
00:13:52,980 --> 00:13:55,300
now we have an idea of how

239
00:13:55,320 --> 00:13:58,230
graph kernels related to the other approaches

240
00:13:58,240 --> 00:14:01,050
two graph comparison

241
00:14:01,060 --> 00:14:04,080
but we don't know yet what a kernel really is

242
00:14:04,100 --> 00:14:06,250
so this is the next question

243
00:14:06,310 --> 00:14:08,000
try to answer what is

244
00:14:08,010 --> 00:14:12,990
graph kernels and starting with the first what is a kernel

245
00:14:13,050 --> 00:14:15,590
the easiest way to explain what the problem

246
00:14:15,600 --> 00:14:17,720
in feature space

247
00:14:17,760 --> 00:14:18,880
and you do so

248
00:14:19,380 --> 00:14:24,890
measuring similarity between objects in your feature space or by computing distances between objects in

249
00:14:24,890 --> 00:14:27,340
your in your feature space

250
00:14:27,390 --> 00:14:33,320
and one way of measuring similarity between objects in your future space is to compute

251
00:14:33,320 --> 00:14:35,330
an inner product

252
00:14:35,370 --> 00:14:40,000
between the objects in your feature space

253
00:14:40,050 --> 00:14:41,390
so what you do is

254
00:14:41,400 --> 00:14:46,510
you map your data points where mapping phi from input space to feature space and

255
00:14:46,510 --> 00:14:48,370
then you take an inner product

256
00:14:48,440 --> 00:14:52,220
between these objects in your feature space

257
00:14:52,220 --> 00:14:57,880
and the kernel function and now that's that's it was crucial in the message here

258
00:14:57,900 --> 00:14:59,270
is a function

259
00:14:59,290 --> 00:15:00,750
on two

260
00:15:00,810 --> 00:15:02,960
objects from the input space

261
00:15:02,970 --> 00:15:04,110
which is

262
00:15:05,290 --> 00:15:12,010
that inner product between these two objects in the feature space

263
00:15:12,070 --> 00:15:15,520
this means that we can compute

264
00:15:15,580 --> 00:15:20,200
the similarity between two objects x and x prime

265
00:15:20,240 --> 00:15:24,080
in feature space by evaluating a function

266
00:15:24,120 --> 00:15:25,540
in input space

267
00:15:25,550 --> 00:15:29,900
and that's the famous commentary

268
00:15:29,960 --> 00:15:33,620
there are several important consequences of this

269
00:15:33,630 --> 00:15:34,890
one is that

270
00:15:34,900 --> 00:15:36,290
by choosing

271
00:15:36,300 --> 00:15:40,020
another different kernel function k

272
00:15:40,080 --> 00:15:41,540
you implicitly

273
00:15:41,550 --> 00:15:43,890
move your learning problem

274
00:15:43,900 --> 00:15:48,420
two are different to another feature space

275
00:15:48,430 --> 00:15:51,750
another implication is

276
00:15:51,760 --> 00:15:56,300
as long as you've defined this function K

277
00:15:56,310 --> 00:16:00,370
and as long as you can show that the corresponding feature space exists you do

278
00:16:00,370 --> 00:16:03,230
not even have to know the mapping phi

279
00:16:03,270 --> 00:16:06,170
to that feature space

280
00:16:06,210 --> 00:16:11,360
because you can compute all similarity scores between objects new feature space by evaluating

281
00:16:11,420 --> 00:16:12,940
this kind of function

282
00:16:12,950 --> 00:16:16,860
and the third and for our purposes most important consequence

283
00:16:17,970 --> 00:16:21,290
that x and x-prime need not be vectorial data

284
00:16:21,340 --> 00:16:27,820
they can be any type of structured data strings time series and graphs

285
00:16:27,830 --> 00:16:31,680
as long as you can measure the similarity in terms

286
00:16:31,700 --> 00:16:33,600
of a scalar

287
00:16:33,630 --> 00:16:37,320
and as long as you can show that the corresponding feature space exists

288
00:16:37,320 --> 00:16:38,220
in which

289
00:16:38,240 --> 00:16:42,150
this scalar corresponds to the inner product between

290
00:16:42,190 --> 00:16:47,440
these two graphs

291
00:16:47,540 --> 00:16:49,200
know this

292
00:16:49,210 --> 00:16:55,330
previous slides I will answer the question of what what a graph kernel is. a graph kernel is a kernel

293
00:16:56,230 --> 00:17:00,060
operates on two graphs to measure the similarity

294
00:17:00,070 --> 00:17:05,120
in terms of an inner product in some feature space

295
00:17:05,230 --> 00:17:07,140
in this graph problems

296
00:17:08,600 --> 00:17:11,740
to the larger class of our convolution codes

297
00:17:11,780 --> 00:17:14,790
which were defined by haussler  in  nineteen ninety nine

298
00:17:14,800 --> 00:17:16,880
the idea here

299
00:17:17,750 --> 00:17:21,260
it's simple but it has inspired a lot of important work

300
00:17:21,840 --> 00:17:25,600
on constant structured data

301
00:17:25,650 --> 00:17:28,270
a convolution kernel works by

302
00:17:30,310 --> 00:17:35,510
two given a structured objects x and x prime into days parts

303
00:17:35,520 --> 00:17:39,370
to give a very simple example take two graphs decompose them

304
00:17:39,440 --> 00:17:43,330
into this set of nodes

305
00:17:43,360 --> 00:17:47,070
and then this convolution kernel compares all

306
00:17:47,080 --> 00:17:48,790
pairs of substructures

307
00:17:48,800 --> 00:17:51,090
from the first

308
00:17:51,100 --> 00:17:54,860
graph from the first structured object and from the second structured object

309
00:17:54,940 --> 00:17:59,160
so all pairs of sub-structures are compared to each other in my example of all

310
00:17:59,160 --> 00:18:00,400
pairs of nodes

311
00:18:00,440 --> 00:18:02,640
from g and g prime

312
00:18:02,670 --> 00:18:05,630
would be compared to each other by a second

313
00:18:05,680 --> 00:18:09,600
kernel function the kind of function k parts here which operates on the

314
00:18:09,640 --> 00:18:15,540
substructures of the two structured objects that we want to compare that's to simple underlying

315
00:18:16,280 --> 00:18:18,340
and graph kernels as I have said before

316
00:18:18,350 --> 00:18:21,150
are such convolution kernels

317
00:18:21,200 --> 00:18:22,910
where x

318
00:18:22,960 --> 00:18:26,020
is a graph and x-prime is a graph as well

319
00:18:26,020 --> 00:18:28,480
if you check the literature on graph problems

320
00:18:28,490 --> 00:18:31,560
don't be confused because it the second

321
00:18:31,570 --> 00:18:34,050
way of using the term graph kernels

322
00:18:34,100 --> 00:18:35,810
and in fact my

323
00:18:35,810 --> 00:18:38,700
my professor in cambridge

324
00:18:38,740 --> 00:18:43,760
uses other terminology in his paper so that's really confusing

325
00:18:43,800 --> 00:18:45,240
sometimes people

326
00:18:45,250 --> 00:18:46,160
will refer

327
00:18:46,180 --> 00:18:50,750
to a graph kernel when they compare two nodes in one graph

328
00:18:50,750 --> 00:18:52,670
by account so they can be

329
00:18:52,730 --> 00:18:57,690
quite confusing when i refer graph here in this tutorial i always mean that measuring

330
00:18:57,690 --> 00:18:59,750
similarity between two

331
00:19:03,840 --> 00:19:07,330
obviously if we decompose

332
00:19:07,380 --> 00:19:12,250
two structured objects to graphs in a different way for instance if i decompose two

333
00:19:12,250 --> 00:19:14,090
graphs into a set of walks

334
00:19:14,100 --> 00:19:16,240
rather than the set of nodes

335
00:19:16,310 --> 00:19:20,870
then this results in a new type of graph coloring so by changing the decomposition

336
00:19:20,870 --> 00:19:22,590
i changed graph problem

337
00:19:22,670 --> 00:19:25,320
as well

338
00:19:25,320 --> 00:19:30,010
and why is it useful to have such a graph kernel because

339
00:19:30,010 --> 00:19:34,100
nation senses

340
00:19:34,330 --> 00:19:38,560
it is i

341
00:19:40,480 --> 00:19:46,680
i was so this is what i want to use

342
00:19:46,720 --> 00:19:48,770
it's all in

343
00:19:50,700 --> 00:19:52,320
this is mess

344
00:19:53,470 --> 00:19:55,610
this is

345
00:19:55,770 --> 00:20:02,110
citizens who were

346
00:20:02,110 --> 00:20:05,940
what to do

347
00:20:06,040 --> 00:20:10,730
that was i schools

348
00:20:12,660 --> 00:20:16,450
does not as some management right

349
00:20:16,480 --> 00:20:21,940
the answer is yes

350
00:20:21,950 --> 00:20:26,120
you can see how this

351
00:20:26,170 --> 00:20:29,920
well done is a town

352
00:20:31,710 --> 00:20:34,110
so take it

353
00:20:36,460 --> 00:20:38,100
so i

354
00:20:39,530 --> 00:20:41,880
the property that every edge

355
00:20:42,110 --> 00:20:45,230
the number of five so know

356
00:20:45,260 --> 00:20:46,670
every edge is

357
00:20:46,680 --> 00:20:49,100
connected to try

358
00:20:49,100 --> 00:20:50,670
well maybe not time

359
00:20:50,740 --> 00:20:53,260
you can see it that all

360
00:20:53,280 --> 00:20:55,670
that's fine

361
00:20:56,910 --> 00:21:01,640
so has been on the exactly two forces zero

362
00:21:01,660 --> 00:21:03,890
so the time

363
00:21:03,910 --> 00:21:06,180
it's example that

364
00:21:06,230 --> 00:21:09,720
which i been don't to exactly one

365
00:21:09,760 --> 00:21:14,010
this it that

366
00:21:19,440 --> 00:21:21,960
under those circumstances very

367
00:21:22,170 --> 00:21:24,660
policy that

368
00:21:24,660 --> 00:21:28,190
this is about science and scientists

369
00:21:39,130 --> 00:21:42,010
this is a

370
00:21:42,100 --> 00:21:50,380
did not

371
00:21:50,430 --> 00:21:54,880
next step set of centres lost

372
00:21:54,930 --> 00:21:59,070
the back question of

373
00:22:00,100 --> 00:22:04,350
statistical used

374
00:22:06,650 --> 00:22:08,470
here it is

375
00:22:08,770 --> 00:22:15,370
to this end the listening

376
00:22:15,520 --> 00:22:17,710
will be learning about this

377
00:22:17,740 --> 00:22:20,590
now are about

378
00:22:20,600 --> 00:22:26,010
so what is it like this one

379
00:22:26,150 --> 00:22:29,040
this is

380
00:22:29,060 --> 00:22:31,510
there some statistics

381
00:22:31,680 --> 00:22:39,270
suppose that the statistical set

382
00:22:39,330 --> 00:22:41,870
originally came from some

383
00:22:41,890 --> 00:22:45,360
scientific rules one nation

384
00:22:48,040 --> 00:22:52,210
the main reason

385
00:22:53,820 --> 00:22:54,920
so they

386
00:22:54,970 --> 00:22:56,560
so some

387
00:22:56,660 --> 00:23:02,060
new zealand music the

388
00:23:05,540 --> 00:23:09,520
o is the same as well

389
00:23:12,640 --> 00:23:14,420
to measure the

390
00:23:14,540 --> 00:23:16,760
one one one

391
00:23:18,680 --> 00:23:20,810
and what we will be

392
00:23:20,940 --> 00:23:23,940
is that you are

393
00:23:23,960 --> 00:23:26,410
are actually test

394
00:23:27,270 --> 00:23:28,850
it's not

395
00:23:31,770 --> 00:23:33,750
so that's it

396
00:23:33,790 --> 00:23:35,890
what want to see

397
00:23:36,600 --> 00:23:40,270
good song one

398
00:23:40,410 --> 00:23:43,500
it's time to find song

399
00:23:43,580 --> 00:23:45,620
for this

400
00:23:45,640 --> 00:23:49,040
one that human races

401
00:23:52,020 --> 00:23:56,290
this is a new this by show

402
00:23:58,480 --> 00:24:02,660
however there's problem

403
00:24:02,690 --> 00:24:04,430
which is

404
00:24:04,440 --> 00:24:09,000
finding these

405
00:24:09,100 --> 00:24:10,620
i a

406
00:24:10,870 --> 00:24:13,080
question to ask is

407
00:24:13,100 --> 00:24:15,560
how do make sense

408
00:24:15,580 --> 00:24:16,910
this is from the

409
00:24:16,940 --> 00:24:20,390
that you

410
00:24:20,410 --> 00:24:22,190
you want

411
00:24:25,200 --> 00:24:28,170
someone has a child

412
00:24:29,730 --> 00:24:31,870
is actually one

413
00:24:31,930 --> 00:24:35,100
and what one

414
00:24:35,140 --> 00:24:38,190
there is a problem we have

415
00:24:38,210 --> 00:24:40,310
one of the variance

416
00:24:40,330 --> 00:24:42,540
but very very unstable

417
00:24:43,500 --> 00:24:45,100
the quality

418
00:24:45,140 --> 00:24:48,830
presented to losing changes

419
00:24:48,830 --> 00:24:52,740
pretty famous guys i'm harry shum who was believe

420
00:24:52,800 --> 00:24:55,000
head of microsoft research asia

421
00:24:55,020 --> 00:25:00,730
the computer vision place and with the risk records that LSP who's over microsoft as

422
00:25:00,730 --> 00:25:01,960
well in seattle

423
00:25:02,070 --> 00:25:06,710
and they basically proposed in the non is that

424
00:25:06,720 --> 00:25:08,500
you could instead of

425
00:25:08,550 --> 00:25:11,800
instead of trying to have this he pluses delta p

426
00:25:11,810 --> 00:25:14,250
if you waltz

427
00:25:14,260 --> 00:25:15,940
allow this compositional

428
00:25:16,620 --> 00:25:22,270
and a final does so if if if anything whatever you just think of

429
00:25:22,280 --> 00:25:38,140
philip's army of what was on the board

430
00:25:38,170 --> 00:25:42,050
so there is the final piece goods is that if i have a whole heap

431
00:25:42,630 --> 00:25:45,980
take one x x one

432
00:25:50,550 --> 00:25:52,110
but i got some and

433
00:25:52,140 --> 00:25:55,210
which is my final

434
00:25:55,210 --> 00:25:58,710
the pixels which

435
00:25:58,990 --> 00:26:07,520
where x one is of where x one all these axes the two vectors

436
00:26:07,660 --> 00:26:13,400
the nice thing about this is that this is in fact

437
00:26:13,530 --> 00:26:16,680
the multiplication so i'm not adding anything here

438
00:26:16,740 --> 00:26:21,960
and what i can do is to say i had let

439
00:26:21,970 --> 00:26:24,980
prior that guys from this

440
00:26:24,990 --> 00:26:28,890
this trying to sort of kind of like this small trying to start off with

441
00:26:28,990 --> 00:26:31,110
i composed this EM

442
00:26:31,140 --> 00:26:34,040
and it makes me bigger

443
00:26:34,050 --> 00:26:36,400
again so it doubles doubles what's on

444
00:26:36,420 --> 00:26:41,500
now five floodplain if i could composite what i can apply this exact same end

445
00:26:41,500 --> 00:26:44,090
to the strike

446
00:26:44,170 --> 00:26:47,680
and it will get twice as large

447
00:26:48,060 --> 00:26:51,350
you get twice as large again

448
00:26:51,400 --> 00:26:55,730
however if i do and added walk

449
00:26:55,740 --> 00:27:00,420
so if i don't represent if i don't represent these what is composite bodies adding

450
00:27:00,420 --> 00:27:03,800
on the update

451
00:27:03,810 --> 00:27:07,600
i want to get this effect he was taken to scale were taken to rotation

452
00:27:07,760 --> 00:27:13,550
it'll just basically the what if i'm adding the what the world be specific or

453
00:27:13,550 --> 00:27:16,060
where exactly i am mean the image and so

454
00:27:16,070 --> 00:27:19,850
what this is not about is that i don't i can actually get some very

455
00:27:19,850 --> 00:27:24,380
big computational savings by taking advantage of this compositional idea

456
00:27:25,680 --> 00:27:26,350
and so

457
00:27:26,380 --> 00:27:28,300
what's going on here

458
00:27:28,310 --> 00:27:31,760
is that instead of thinking that people are still to be

459
00:27:31,770 --> 00:27:34,840
i've got a very actually walk wx p

460
00:27:34,860 --> 00:27:36,820
composed with

461
00:27:36,820 --> 00:27:38,830
delta p

462
00:27:38,860 --> 00:27:40,850
and here and so on

463
00:27:40,870 --> 00:27:46,960
this is a slide from from so many and actually and they basically demonstrate here

464
00:27:46,960 --> 00:27:49,290
that you can actually break this down steps

465
00:27:49,980 --> 00:27:51,320
i've got my imply

466
00:27:51,340 --> 00:27:54,120
and i can what this template first by

467
00:27:54,610 --> 00:27:58,830
just delta pay so instead of doing the entire world which report me into the

468
00:27:58,830 --> 00:28:03,520
source image i can do it delta p that's slightly distorts things then i can

469
00:28:03,530 --> 00:28:07,600
match to the image with my final p such i finally get there so i

470
00:28:07,600 --> 00:28:11,150
can break it into steps rather than having to do here

471
00:28:11,230 --> 00:28:12,980
i have to do everything

472
00:28:13,000 --> 00:28:14,330
in terms of

473
00:28:14,340 --> 00:28:18,850
this single what function to go the source image not compose things

474
00:28:18,900 --> 00:28:20,910
so like i i will

475
00:28:20,960 --> 00:28:26,750
still don't quite follow that simon so why is it useful well-reasoned useful is that

476
00:28:26,750 --> 00:28:30,340
i can actually inverse composed as well

477
00:28:30,390 --> 00:28:35,380
and i was talking about i was talking about this before so if instead of

478
00:28:35,380 --> 00:28:37,830
me trying to work out

479
00:28:37,840 --> 00:28:42,020
how the source image should move with respect to the template

480
00:28:42,040 --> 00:28:46,310
i could actually work out how might imply should move so i could get the

481
00:28:46,310 --> 00:28:47,460
gradients and here

482
00:28:47,560 --> 00:28:52,730
i could work out how this template should what should change this delta p

483
00:28:52,750 --> 00:28:54,040
with respect

484
00:28:54,040 --> 00:28:59,070
two source image but instead of them having to what template

485
00:28:59,130 --> 00:29:00,770
i in the war

486
00:29:00,790 --> 00:29:02,980
and i want to the source image

487
00:29:03,000 --> 00:29:04,500
and the next thing that happens here

488
00:29:04,520 --> 00:29:09,330
is the gradient and the actual template stay the same in every iteration

489
00:29:09,340 --> 00:29:10,730
so there is no

490
00:29:10,750 --> 00:29:14,290
jacobian doesn't have to change every iteration

491
00:29:14,310 --> 00:29:15,840
and will

492
00:29:15,840 --> 00:29:17,480
i've got here

493
00:29:19,960 --> 00:29:23,690
the intrinsic value of the called of the yellow line

494
00:29:23,730 --> 00:29:27,570
and this is an intrinsic value of output we've got them both the same exercise

495
00:29:28,800 --> 00:29:30,270
all right

496
00:29:30,280 --> 00:29:33,460
and then i've shown here the stock price when the blue line

497
00:29:33,460 --> 00:29:38,170
stock price again stock prices just a forty five degree line line with a slope

498
00:29:38,170 --> 00:29:39,550
of one

499
00:29:39,570 --> 00:29:44,460
well you notice that if i were to buy a call

500
00:29:44,520 --> 00:29:48,250
and write a put that senior shorting inputs

501
00:29:48,250 --> 00:29:49,690
i would have

502
00:29:49,710 --> 00:29:52,340
the combined portfolio with just those two

503
00:29:52,380 --> 00:29:54,070
i would have

504
00:29:54,130 --> 00:29:59,110
the yellow line here and i have minus the pink line here

505
00:29:59,110 --> 00:30:03,440
i would have a parallel straight line looks just like the stock price just shifted

506
00:30:04,820 --> 00:30:07,090
thirty five by a call

507
00:30:07,110 --> 00:30:09,750
and short a put a right put

508
00:30:09,770 --> 00:30:14,090
it's the same thing as owning the stock minus the exercise price

509
00:30:14,130 --> 00:30:17,480
so that's that's what we have here the

510
00:30:17,500 --> 00:30:19,230
put call shared relations

511
00:30:19,280 --> 00:30:22,250
so we're taking account of dividends

512
00:30:22,250 --> 00:30:26,610
that diagram didn't show the fact that stocks were paid my pay dividends between now

513
00:30:26,610 --> 00:30:28,550
and the exercise

514
00:30:28,570 --> 00:30:31,210
but you can see when i was just saying

515
00:30:31,230 --> 00:30:36,000
got i said colonies what this is what i call

516
00:30:36,020 --> 00:30:41,190
minus the price of i have a minus sign in front of everything

517
00:30:41,210 --> 00:30:45,750
but it is just the with the diagram shows so it put called charity means

518
00:30:45,800 --> 00:30:47,400
is that we only need

519
00:30:47,460 --> 00:30:48,710
the theory of

520
00:30:48,730 --> 00:30:51,630
either call prices or put prices

521
00:30:51,670 --> 00:30:55,230
and then the other one falls right output called parity

522
00:30:56,190 --> 00:30:59,630
all i need is the theory of call prices so we we just forget about

523
00:31:00,380 --> 00:31:02,400
and we just talk about calls

524
00:31:02,420 --> 00:31:04,690
from now on

525
00:31:04,710 --> 00:31:07,610
and then if if i give you a problem to ask you

526
00:31:07,630 --> 00:31:11,250
what is the input price what is the price of the put you would go

527
00:31:11,250 --> 00:31:14,020
in and calculate the price of call

528
00:31:14,050 --> 00:31:16,500
and then calculate

529
00:31:16,750 --> 00:31:19,520
we we the put option price will put this on the right-hand side of the

530
00:31:20,360 --> 00:31:21,520
the put price

531
00:31:21,520 --> 00:31:23,630
would equal the call price

532
00:31:23,650 --> 00:31:25,800
plus the present value of the straight plays

533
00:31:25,820 --> 00:31:29,070
thus the present value of dividends minus the price of the stock

534
00:31:29,070 --> 00:31:32,690
so that and so that makes it very easy or we have to do is

535
00:31:32,690 --> 00:31:35,130
worry about calls

536
00:31:37,150 --> 00:31:39,040
and so

537
00:31:39,090 --> 00:31:40,170
we're my

538
00:31:42,380 --> 00:31:45,670
the question for financial theory is

539
00:31:45,690 --> 00:31:51,230
what determines the pink line we you agree that it should be above intrinsic value

540
00:31:51,250 --> 00:31:55,130
as as the option gets closer to exploration

541
00:31:55,210 --> 00:31:59,920
as time moves on and the exercise days is getting closer and closer in time

542
00:31:59,940 --> 00:32:03,800
this pink lines and go down down down on the last day

543
00:32:03,820 --> 00:32:06,270
he hits the intrinsic value

544
00:32:06,280 --> 00:32:11,710
so but what what is it before they exercise so

545
00:32:11,770 --> 00:32:14,300
i want to start with

546
00:32:14,400 --> 00:32:20,000
theory which illustrates how we calculate these thing

547
00:32:20,020 --> 00:32:21,880
but this is a theory that

548
00:32:22,940 --> 00:32:24,550
applies to us

549
00:32:24,570 --> 00:32:26,750
so so we can understand it easily

550
00:32:26,840 --> 00:32:30,860
it applies to a stripped-down situation

551
00:32:30,920 --> 00:32:34,170
i want to derive the price of an option

552
00:32:34,550 --> 00:32:36,360
under the assumption

553
00:32:37,320 --> 00:32:41,900
it's very simple there is only one period now an exercise

554
00:32:41,920 --> 00:32:44,710
it's in it's a european option we're gonna

555
00:32:44,730 --> 00:32:46,880
exercise in one we have

556
00:32:47,000 --> 00:32:49,670
exercise data one period

557
00:32:49,960 --> 00:32:55,590
and i also under the restrictive assumptions and this is for pedagogical purposes just to

558
00:32:55,610 --> 00:32:57,900
simplify up

559
00:32:57,920 --> 00:33:01,750
that the stock price s is the stock price today

560
00:33:03,540 --> 00:33:07,020
and the stock this stock is very special because

561
00:33:07,040 --> 00:33:09,340
next period it to have only

562
00:33:09,360 --> 00:33:11,050
two values

563
00:33:11,070 --> 00:33:13,550
it's as times u

564
00:33:13,590 --> 00:33:16,360
if the stock goes up u stands for up

565
00:33:16,380 --> 00:33:19,040
and it's less times d

566
00:33:19,050 --> 00:33:21,230
if the stock price goes down

567
00:33:21,360 --> 00:33:25,770
and what i'm saying with the what's arbitrary here is i'm saying that there's only

568
00:33:25,770 --> 00:33:29,610
two possible prices for the stock experience as you

569
00:33:29,690 --> 00:33:32,150
or SD

570
00:33:33,800 --> 00:33:35,880
that's not real world because as you know

571
00:33:35,900 --> 00:33:39,540
there's all kinds of infinite number of possible prices next year

572
00:33:39,550 --> 00:33:41,630
but again this is just

573
00:33:41,690 --> 00:33:44,110
i think that we should be able to figure out what the price of a

574
00:33:44,110 --> 00:33:45,340
call option

575
00:33:45,420 --> 00:33:48,090
on this stock should be worth

576
00:33:48,110 --> 00:33:49,940
it's very simple

577
00:33:50,000 --> 00:33:52,000
but it's

578
00:33:52,020 --> 00:33:56,920
this is very simple but the people who invented this one the nobel prize so

579
00:33:56,980 --> 00:34:01,960
how hard i don't want to make it this wasn't so simple in the history

580
00:34:01,960 --> 00:34:04,170
of financial thinking

581
00:34:04,190 --> 00:34:08,540
but anyway so you understand the situation that we're proposing and it's just like this

582
00:34:08,540 --> 00:34:12,840
is very funny stock that we know for some reason we know

583
00:34:12,860 --> 00:34:14,820
s is the price today

584
00:34:14,840 --> 00:34:18,400
and next period when the option exercise dated

585
00:34:18,400 --> 00:34:21,800
it's pretty either going to be as you are going to be as as times

586
00:34:21,960 --> 00:34:24,150
are going to be as times the

587
00:34:24,170 --> 00:34:27,400
OK and then there is an interest rate then we can both by our land

588
00:34:27,420 --> 00:34:31,900
at this risk this interest rates so what should the option be worth

589
00:34:31,940 --> 00:34:34,250
OK so in this case

590
00:34:34,250 --> 00:34:35,960
i'm going to call c

591
00:34:35,980 --> 00:34:38,920
the current price of the car

592
00:34:39,050 --> 00:34:42,500
today this is before and this is before

593
00:34:42,550 --> 00:34:44,420
the exercise dates so

594
00:34:44,460 --> 00:34:49,400
the price of the car was going to be worth more than the intrinsic value

595
00:34:49,420 --> 00:34:51,980
i'm going to call c sub you

596
00:34:51,980 --> 00:34:56,150
the value of the call next period if the price is up

597
00:34:56,190 --> 00:35:00,320
and see some of the the value of the call next period if the price

598
00:35:00,320 --> 00:35:01,960
is down

599
00:35:03,340 --> 00:35:07,230
that we that's the thing we read off of those

600
00:35:07,230 --> 00:35:09,860
like broken straight lines

601
00:35:09,920 --> 00:35:14,380
so a CU would be you know the stock price mind if it's the stock

602
00:35:14,380 --> 00:35:17,230
minus the exercise price if it's in the money

603
00:35:17,250 --> 00:35:19,090
we know that in advance

604
00:35:19,090 --> 00:35:24,210
because we already know what the two possible prices are next period so we already

605
00:35:24,860 --> 00:35:30,550
what the two possible options values are next here this is the intrinsic value

606
00:35:30,610 --> 00:35:34,770
if it's up and this is the intrinsic value of its down

607
00:35:35,840 --> 00:35:40,980
will call either strike price of the option exercise price of the option

608
00:35:41,000 --> 00:35:43,280
is everything clear here

609
00:35:43,280 --> 00:35:48,970
when alpha is constant if alpha in particular is

610
00:35:48,980 --> 00:35:53,230
if it doesn't have to be constant lesson com

611
00:35:53,240 --> 00:35:54,700
o one right

612
00:35:55,580 --> 00:35:58,410
or equivalently which is what you said

613
00:36:03,210 --> 00:36:04,880
it is all

614
00:36:06,230 --> 00:36:13,300
which is to say if the number of elements in the table

615
00:36:13,360 --> 00:36:16,100
these were

616
00:36:16,110 --> 00:36:19,200
upper bounded by a constant times and

617
00:36:19,280 --> 00:36:23,550
and the search costs cost a lot of people hashtable

618
00:36:23,560 --> 00:36:25,240
runs in constant

619
00:36:25,320 --> 00:36:26,970
search time

620
00:36:26,970 --> 00:36:28,550
that's actually wrong

621
00:36:28,580 --> 00:36:31,000
depends upon the load factor of the

622
00:36:32,940 --> 00:36:39,070
OK and people have made programming errors based on that misunderstanding of hash tables

623
00:36:39,120 --> 00:36:42,010
because they have hash table is too small

624
00:36:42,030 --> 00:36:44,710
the number of elements putting in there

625
00:36:44,720 --> 00:36:46,610
doesn't help

626
00:36:46,940 --> 00:36:50,430
number way know in fact will grow

627
00:36:50,480 --> 00:36:51,620
with the

628
00:36:51,920 --> 00:36:57,160
this is one plus an over and actually grows with n

629
00:36:57,240 --> 00:37:00,960
so unless you make sure that keeps up with n

630
00:37:03,210 --> 00:37:06,130
this doesn't stay constant

631
00:37:06,130 --> 00:37:12,600
it turns out for successful searches also one out

632
00:37:12,620 --> 00:37:16,250
for that you need to do a little bit more mathematics

633
00:37:16,310 --> 00:37:19,360
because you have to conditional on

634
00:37:19,360 --> 00:37:22,630
searching for the items in the table but it turns out is also one plus

635
00:37:23,510 --> 00:37:26,670
and that's that you can read about in the book

636
00:37:26,710 --> 00:37:31,330
and also there's a more rigorous proof i sort of have glossed over

637
00:37:33,210 --> 00:37:34,450
stop here

638
00:37:34,460 --> 00:37:39,270
OK doing for more intuitive proof so of those things you should look for the

639
00:37:45,230 --> 00:37:50,010
so this is one reason why caching is such a popular method is basically let

640
00:37:50,020 --> 00:37:52,710
you represent a dynamic set

641
00:37:52,750 --> 00:37:54,710
we were one

642
00:37:54,760 --> 00:37:59,660
cost per operation constant cost per operation inserting deleting

643
00:37:59,660 --> 00:38:03,160
so as long as the table you're keeping

644
00:38:03,210 --> 00:38:04,470
it is

645
00:38:04,470 --> 00:38:09,320
not much smaller than the number of items that you're putting in

646
00:38:10,160 --> 00:38:14,510
and then all the operations end up being in constant time

647
00:38:14,670 --> 00:38:21,180
but it depends upon this strongly upon this assumption of simple uniform hashing

648
00:38:21,180 --> 00:38:22,810
and so

649
00:38:22,830 --> 00:38:26,750
no matter what hash function you pick

650
00:38:26,770 --> 00:38:29,270
i can always find a set of

651
00:38:30,910 --> 00:38:32,690
elements that are going to hash

652
00:38:32,750 --> 00:38:35,830
that hash functions crashed badly

653
00:38:35,900 --> 00:38:37,160
i just

654
00:38:37,210 --> 00:38:40,540
generate a whole bunch of them and to see where the hash function takes them

655
00:38:40,540 --> 00:38:42,210
in the end

656
00:38:42,300 --> 00:38:44,930
recall once hash to the same place

657
00:38:44,970 --> 00:38:49,510
actually way of countering that but in practice

658
00:38:51,120 --> 00:38:54,180
i understand that

659
00:38:54,230 --> 00:38:58,990
most programs that need to use things are really reverse engineering hash function

660
00:38:59,010 --> 00:39:01,440
so there's some very simple hash functions

661
00:39:01,440 --> 00:39:03,780
but this game will be fine

662
00:39:03,790 --> 00:39:10,110
a large margin classifier that perfectly classifies all the training examples and with large margin

663
00:39:10,140 --> 00:39:16,480
and we show that adaboost is finding this maxmin strategy by applying general game playing

664
00:39:18,220 --> 00:39:20,160
through repeated play

665
00:39:20,170 --> 00:39:23,930
so what are the consequences of this so one of them was

666
00:39:23,950 --> 00:39:27,540
i suggested in the question that was just ask so the first thing is that

667
00:39:27,540 --> 00:39:31,040
it's showing that the weights on the weak classifiers

668
00:39:31,060 --> 00:39:35,930
are actually converging to an approximate maximum strategy for this game so this is telling

669
00:39:35,930 --> 00:39:41,220
us what the weights are converging to in adaboost

670
00:39:41,420 --> 00:39:46,030
likewise it's telling us something about the convergence of these distributions it saying that the

671
00:39:46,030 --> 00:39:52,990
average of the distributions computed by adaboost is converging to an approximate minmax strategy for

672
00:39:52,990 --> 00:39:54,430
this game

673
00:39:54,460 --> 00:39:59,810
it's also showing that this connection between margins and edges that i mentioned before before

674
00:39:59,810 --> 00:40:04,430
imagine that we can show that larger edges imply larger margins

675
00:40:04,480 --> 00:40:09,260
we now see that the reason for that is because of this game theoretic view

676
00:40:09,260 --> 00:40:12,430
of the algorithm that margins are edges

677
00:40:12,440 --> 00:40:15,750
are connected by the minimax theorem they're both

678
00:40:15,770 --> 00:40:20,230
exactly related to the value of this game

679
00:40:20,250 --> 00:40:25,610
and this also explains why adaboost maximizes margins

680
00:40:25,620 --> 00:40:31,040
looking up

681
00:40:31,210 --> 00:40:43,670
because this general algorithm it's got parameter parameter where you can

682
00:40:43,700 --> 00:40:48,870
two and how how how good an approximation you want and adaboost is setting that

683
00:40:48,880 --> 00:40:50,190
parameter in one

684
00:40:50,230 --> 00:40:51,220
particular way

685
00:40:51,260 --> 00:40:59,220
i'm not explaining the details but so it's it's only finding an approximation of it

686
00:40:59,230 --> 00:41:00,420
OK so

687
00:41:00,420 --> 00:41:03,680
that's what i want to say about the game theory actually wants a one other

688
00:41:03,680 --> 00:41:10,330
thing about it so it different instantiation of the same game playing algorithm also gives

689
00:41:10,330 --> 00:41:14,610
online algorithms for those of you have seen online algorithms metric there's a tutorial on

690
00:41:14,610 --> 00:41:19,730
that but for instance the weighted majority algorithm so this provides a connection to other

691
00:41:19,760 --> 00:41:22,540
learning models as well

692
00:41:22,820 --> 00:41:28,320
that's what i want to say about adaboost in game theory

693
00:41:28,330 --> 00:41:31,350
so what i want to do now is to go on to the second way

694
00:41:31,350 --> 00:41:36,910
of thinking about adaboost which is thinking about it as an optimisation algorithm

695
00:41:38,190 --> 00:41:40,170
if i'm sure you've seen in this

696
00:41:40,190 --> 00:41:41,410
summer school

697
00:41:41,410 --> 00:41:47,020
that many or maybe even most learning and statistical methods can be viewed as minimising

698
00:41:47,020 --> 00:41:50,560
some kind of gloucester cost objective function

699
00:41:50,610 --> 00:41:54,420
which in one way or another is measuring the fit to the data so classic

700
00:41:54,420 --> 00:41:56,680
example is least squares regression

701
00:41:56,700 --> 00:42:00,560
we're trying to minimize the sum of the squared areas of your model

702
00:42:00,580 --> 00:42:02,100
compared to the correct

703
00:42:04,360 --> 00:42:06,400
so adaboost also

704
00:42:06,420 --> 00:42:09,350
can be viewed as an algorithm for minimizing

705
00:42:09,720 --> 00:42:11,740
what a loss function

706
00:42:12,040 --> 00:42:17,560
and why does this matter will it's helpful to understand this

707
00:42:17,570 --> 00:42:22,660
because the first fully kind clear flies with the goal of the algorithm is

708
00:42:22,700 --> 00:42:27,310
and it's also useful in proving convergence properties about the algorithm

709
00:42:28,780 --> 00:42:33,870
if we can beat decouple the algorithm from what it is that it's minimizing then

710
00:42:33,870 --> 00:42:39,740
possibly that would make it possible to derive faster algorithms for the same objective function

711
00:42:40,160 --> 00:42:45,200
or conversely we take the same algorithm generalize it so that it can be used

712
00:42:45,530 --> 00:42:48,490
for other loss functions and their by other

713
00:42:48,510 --> 00:42:52,410
learning problems of various kinds

714
00:42:53,570 --> 00:42:56,130
what is it that adaboost is minimizing

715
00:42:56,140 --> 00:42:59,840
so if you go back to the training year proof

716
00:42:59,850 --> 00:43:04,930
we show that the training error the final classifier is at most product of zee

717
00:43:05,210 --> 00:43:11,460
is these normalisation constants and we showed that this normalisation factor

718
00:43:11,510 --> 00:43:16,360
can be written in general as this function of epsilon t and alpha t

719
00:43:16,410 --> 00:43:21,020
excellent is the weighted their the teeth we classifier and alpha t is the parameter

720
00:43:21,240 --> 00:43:25,650
and then we selected alpha t to minimize this expression

721
00:43:25,660 --> 00:43:28,840
giving this expression

722
00:43:28,860 --> 00:43:31,430
OK so if you look at this more closely well

723
00:43:31,450 --> 00:43:35,390
first of all as i just said i alpha t has been chosen to minimize

724
00:43:36,740 --> 00:43:40,310
in this expression and also ht

725
00:43:40,310 --> 00:43:46,750
the weak classifiers being chosen to minimize the weak classifiers or epsilon t

726
00:43:46,750 --> 00:43:50,980
two things change

727
00:43:51,000 --> 00:43:55,330
and different techniques of being exploited in computer vision

728
00:43:55,340 --> 00:43:59,830
together with about visual word representation

729
00:43:59,840 --> 00:44:02,000
for classification proposed

730
00:44:02,000 --> 00:44:07,040
and both generative and discriminative meters have been used

731
00:44:07,040 --> 00:44:13,170
and among the disconnected needed support vector machine is one of the most popular

732
00:44:14,500 --> 00:44:20,310
that there are some special least addressed

733
00:44:21,670 --> 00:44:25,440
for image categorisation

734
00:44:25,460 --> 00:44:26,290
by the way

735
00:44:26,310 --> 00:44:28,650
generative approach

736
00:44:28,690 --> 00:44:36,440
obtain competitive results this means explaining this discriminatively has been discussed

737
00:44:36,460 --> 00:44:37,520
this school

738
00:44:37,520 --> 00:44:38,750
one of the

739
00:44:38,770 --> 00:44:40,380
the first

740
00:44:40,380 --> 00:44:42,500
more than that have been used

741
00:44:42,500 --> 00:44:44,830
with bag of visual words

742
00:44:44,880 --> 00:44:46,900
in two thousand three

743
00:44:48,130 --> 00:44:53,310
and also discuss justified probabilistic latent semantic analysis

744
00:44:53,330 --> 00:44:56,560
it is one of the state of the art approach to

745
00:44:56,560 --> 00:44:57,690
there will

746
00:44:58,300 --> 00:45:01,790
the classification adequately station in

747
00:45:06,380 --> 00:45:08,460
for some patients

748
00:45:08,480 --> 00:45:10,630
you are probably and

749
00:45:10,650 --> 00:45:13,580
in this case the visual world

750
00:45:13,580 --> 00:45:17,670
and if we have a problem with a hundred visual world

751
00:45:21,000 --> 00:45:23,230
is an indicator vector well

752
00:45:23,230 --> 00:45:27,310
there's one position and zero in that position

753
00:45:28,330 --> 00:45:29,940
dublin old

754
00:45:29,940 --> 00:45:32,100
indicator collection

755
00:45:32,110 --> 00:45:33,110
region one

756
00:45:33,130 --> 00:45:38,480
the image here the degree and that topic

757
00:45:38,520 --> 00:45:39,690
for instance

758
00:45:40,670 --> 00:45:42,750
within that scene

759
00:45:42,830 --> 00:45:44,790
and the simplest generative model

760
00:45:44,830 --> 00:45:46,150
is there naive

761
00:45:46,440 --> 00:45:47,580
by small

762
00:45:47,730 --> 00:45:50,520
we have just you know

763
00:45:50,540 --> 00:45:53,730
and the set of nodes in this is the set of

764
00:45:55,830 --> 00:45:57,170
within an image

765
00:45:57,230 --> 00:46:00,150
one of the class of the image

766
00:46:01,750 --> 00:46:03,310
this graph decomposes

767
00:46:03,330 --> 00:46:04,540
in this too

768
00:46:04,560 --> 00:46:05,420
this too

769
00:46:05,880 --> 00:46:08,110
probability prior probability

770
00:46:08,130 --> 00:46:09,330
over the last

771
00:46:09,360 --> 00:46:14,790
and the likelihood of observing a set of

772
00:46:14,810 --> 00:46:16,330
vision were even

773
00:46:20,110 --> 00:46:22,810
independence making visual words

774
00:46:22,830 --> 00:46:25,190
we can get this kind of probability

775
00:46:25,860 --> 00:46:29,460
what we should do is to estimate

776
00:46:29,460 --> 00:46:34,480
the probability of serving a specific region were given the class

777
00:46:34,500 --> 00:46:38,580
and taking into account the

778
00:46:38,580 --> 00:46:42,580
we can approximate this posterior probability

779
00:46:42,730 --> 00:46:46,480
five is the composition of that

780
00:46:47,840 --> 00:46:51,860
we can use that marks to infer the class and this was the first the

781
00:46:51,860 --> 00:46:53,040
most simple

782
00:46:54,190 --> 00:46:57,480
use with about the visual world model in two thousand three

783
00:46:58,880 --> 00:47:00,290
other the problem

784
00:47:01,330 --> 00:47:03,790
image categorisation

785
00:47:03,790 --> 00:47:06,290
object categorisation specifically

786
00:47:06,440 --> 00:47:09,730
and the set of objects was like this

787
00:47:09,730 --> 00:47:11,580
there are seven

788
00:47:13,790 --> 00:47:17,330
is also obtained by sort

789
00:47:17,380 --> 00:47:19,520
although these that set

790
00:47:19,750 --> 00:47:22,520
where we have some object class

791
00:47:22,540 --> 00:47:25,670
i've been using SIFT descriptors

792
00:47:25,690 --> 00:47:28,210
and a simple thing means

793
00:47:28,230 --> 00:47:30,750
to build the vocabulary

794
00:47:30,770 --> 00:47:33,060
and the naive bayes classifier

795
00:47:33,080 --> 00:47:34,560
and i

796
00:47:34,580 --> 00:47:38,110
very nice results but not too bad

797
00:47:38,130 --> 00:47:40,520
because this was the first war

798
00:47:40,520 --> 00:47:44,000
with this number of classes the diffusion

799
00:47:44,020 --> 00:47:46,270
and by the way if you

800
00:47:46,310 --> 00:47:48,380
say some

801
00:47:48,420 --> 00:47:50,960
powerful medium for classification like

802
00:47:52,210 --> 00:47:54,360
the linear

803
00:47:54,380 --> 00:47:59,020
you can obtain better results first

804
00:47:59,040 --> 00:48:02,480
and moving on more sophisticated models

805
00:48:02,600 --> 00:48:05,670
the probabilistic semantic more than that

806
00:48:05,690 --> 00:48:09,960
it is mother from text community as well

807
00:48:09,960 --> 00:48:12,600
and in this case we have three nodes

808
00:48:12,600 --> 00:48:15,100
no the for the documents

809
00:48:16,360 --> 00:48:18,440
and topics

810
00:48:18,440 --> 00:48:22,790
and then of the visual of the words within a document

811
00:48:22,810 --> 00:48:28,980
and with this definition we have the process of the generative process of selecting an

812
00:48:28,980 --> 00:48:32,400
image with probability with some some probability

813
00:48:32,400 --> 00:48:33,600
then given

814
00:48:33,650 --> 00:48:39,400
the common because some topics and even at all this we can select

815
00:48:39,400 --> 00:48:43,880
is no conditioning and then we infuse CRF and we wait an hour and see

816
00:48:43,880 --> 00:48:48,790
what happens in the sham animals don't have any lesions have about dublin of startle

817
00:48:48,800 --> 00:48:53,300
that effect is totally blocked animals with lesions to the bed nucleus but not all

818
00:48:53,330 --> 00:48:56,480
animals that have lesions of the central nucleus of the amygdala

819
00:48:56,500 --> 00:49:01,210
so just like the difference between light hand startle and fear potentiated startle

820
00:49:01,220 --> 00:49:04,480
CRH enhanced our looked more like lightning and startled

821
00:49:04,500 --> 00:49:09,310
so again we get this double dissociation these be the amygdala and the bed nucleus

822
00:49:09,580 --> 00:49:13,960
fear potentiated startle being dependent on the make the bed nucleus when i say they

823
00:49:13,960 --> 00:49:18,650
may on talking about the central nucleus and vice versa for light enhanced darwin's serie

824
00:49:20,690 --> 00:49:24,670
so in the schema them CRF seems to

825
00:49:24,680 --> 00:49:28,360
act on receptors in the bed nucleus and we have much better evidence for that

826
00:49:28,370 --> 00:49:33,650
now if we give an antagonist into the bed nucleus locally it blocks the effects

827
00:49:33,650 --> 00:49:38,650
of zero

828
00:49:38,660 --> 00:49:41,040
so it turns out we've also

829
00:49:41,050 --> 00:49:47,120
got a hold of a very selective CRF antagonists and peptide peptide great big molecules

830
00:49:47,430 --> 00:49:51,280
they don't get into the brain when you given systemically so large drug companies are

831
00:49:51,280 --> 00:49:56,260
trying to develop seraphim tag because they think they're going to be very interesting compounds

832
00:49:56,260 --> 00:50:00,870
for blocking stress and depression and anxiety and so forth and it turns out that

833
00:50:00,870 --> 00:50:05,430
when we get this compound it blocks the slightly enhanced that's not very pretty dose

834
00:50:05,430 --> 00:50:11,440
response curve but lower doses fallen here but interestingly even over wide range of doses

835
00:50:11,740 --> 00:50:17,600
the CRF antagonists doesn't touch fear potentiated startle so once again we get the difference

836
00:50:17,610 --> 00:50:23,110
between the role of CRF in fear potentiated startle versus all linehan startled

837
00:50:23,130 --> 00:50:31,050
or which is right hand stalinists case and more recently we've been finding that hormonal

838
00:50:31,650 --> 00:50:38,290
regulation seems to occur with behaviors depending on the bed nucleus and not potentiated startle

839
00:50:38,290 --> 00:50:44,880
so testosterone in male rats reduces fear anxiety attacks on the bed nucleus and that

840
00:50:44,880 --> 00:50:50,290
doesn't do anything to fear potentiated startle and progesterone in female rats and so

841
00:50:50,300 --> 00:50:55,170
tentatively what was suggested as you really have these two systems that act in parallel

842
00:50:55,170 --> 00:50:59,160
in the brain you have a fear system that accident very phasic way

843
00:50:59,220 --> 00:51:04,090
it recovers quickly so you can get onto another to you know be ready for

844
00:51:04,090 --> 00:51:08,730
something else whereas you have another system that acts in a more sustainable way

845
00:51:08,730 --> 00:51:13,460
and in this kind of sense that's roughly the difference between fear and anxiety and

846
00:51:13,460 --> 00:51:18,810
zion is less specific last longer seems that to be activated by a more diffuse

847
00:51:21,790 --> 00:51:24,030
the question though

848
00:51:24,070 --> 00:51:29,110
is you know what is the fundamental differences are just words fear and anxiety can

849
00:51:29,110 --> 00:51:33,580
we come up with the more operational definition to try to understand in the more

850
00:51:33,580 --> 00:51:39,730
detailed way what's going on here so the two differences between fear potentiated startle

851
00:51:39,770 --> 00:51:46,510
and light enhanced startle CHI and startled is fear potentiated startle depends on conditioning whereas

852
00:51:46,510 --> 00:51:51,570
lighter sear enhanced oil doesn't require conditioning and the second is fear potentiated startle is

853
00:51:51,570 --> 00:51:54,930
very short term these three seconds ten second life

854
00:51:54,960 --> 00:52:02,620
whereas seriation and starlight hence total is much more long-lasting so to disambiguate those two

855
00:52:02,620 --> 00:52:06,590
what we did is purposely took a long queue

856
00:52:06,620 --> 00:52:09,760
so specific you but it's long in duration

857
00:52:09,760 --> 00:52:11,940
and now we condition

858
00:52:11,990 --> 00:52:12,660
and so

859
00:52:12,690 --> 00:52:17,570
if in fact that's depend on the bed nucleus would say duration is dependent on

860
00:52:17,570 --> 00:52:20,140
the central nucleus it would say was conditioning

861
00:52:20,160 --> 00:52:24,500
in this work by david walker so what we do is take an animal given

862
00:52:24,580 --> 00:52:28,730
clicker because that's quite salient so the clock comes on click click click click click

863
00:52:28,730 --> 00:52:33,320
for eight minutes and then at random times during that we give the animal some

864
00:52:33,320 --> 00:52:37,400
sharks we do that day one we do that and day two so basically what

865
00:52:37,400 --> 00:52:41,610
the animal learns is that when the clicker comes on the risk but they don't

866
00:52:41,610 --> 00:52:43,110
know when they're going to get

867
00:52:43,160 --> 00:52:48,400
so it's kind of the state of heightened continual apprehension and now we assess startle

868
00:52:48,400 --> 00:52:50,490
all i mean we assess

869
00:52:50,530 --> 00:52:55,730
fear or anxiety in the presence of this eight minute tone with the startle stimulus

870
00:52:55,730 --> 00:53:00,590
either with the qu yuan or with no click around

871
00:53:00,680 --> 00:53:05,380
and what we find is that when clickers on there's no change from the previous

872
00:53:05,380 --> 00:53:08,390
post is actually a lot of context condition here

873
00:53:09,110 --> 00:53:12,840
for the afficionados and the

874
00:53:13,920 --> 00:53:19,010
but when we turn the click around we get a big healthy increasing startled

875
00:53:21,090 --> 00:53:23,740
the question is is

876
00:53:23,820 --> 00:53:29,300
is the central nucleus or the bed nucleus mediating this long-term activation

877
00:53:29,350 --> 00:53:33,760
now this is a complicated slide but if you get it you understand right away

878
00:53:33,830 --> 00:53:37,010
and if you don't get it i try to explain it again

879
00:53:38,490 --> 00:53:42,500
we're looking at startle now when this tone is on for eight minutes and we

880
00:53:42,500 --> 00:53:45,890
just broken up to the first half the town in the second half the time

881
00:53:45,980 --> 00:53:49,930
we have animals that have can tell us so we can activate either the bed

882
00:53:49,930 --> 00:53:52,890
nucleus the central nucleus of the amygdala

883
00:53:52,910 --> 00:53:59,310
and will either vehicle or this compound in activates glutamate transmission so the vehicle animals

884
00:53:59,310 --> 00:54:02,740
do which you think they're afraid beginning at the time of the clicker in the

885
00:54:02,740 --> 00:54:06,900
end of the clicker karttunen here's different experiments stoner clicker

886
00:54:06,980 --> 00:54:11,320
so they basically sort of level across this time that's not significant increase

887
00:54:11,440 --> 00:54:16,160
the really cool thing is that the animals that have the bed nucleus and activation

888
00:54:16,170 --> 00:54:20,510
we have lots of fear at the beginning of the of the stimulus but not

889
00:54:20,510 --> 00:54:22,340
all of the and very little

890
00:54:22,360 --> 00:54:25,130
and it's kind of vice versa with the central nucleus

891
00:54:25,150 --> 00:54:29,250
at the beginning they have some fear but it's certainly less than the bed nucleus

892
00:54:29,250 --> 00:54:33,790
animals and at the end they have perfectly normal fear seems the vehicles

893
00:54:35,020 --> 00:54:37,540
so as i said if you get it you get it right away and if

894
00:54:37,540 --> 00:54:41,690
you don't get it it's the idea is that when the stimulus comes on the

895
00:54:41,690 --> 00:54:45,560
first thing does is activated the central nucleus because ripping down to the brain stem

896
00:54:45,560 --> 00:54:50,420
to produce the phasic fear response but then something else activates are sort of hands

897
00:54:50,420 --> 00:54:54,900
after the bed nucleus and the bed nucleus can respond in the same way so

898
00:54:54,900 --> 00:54:59,560
that's why we activate the bed nucleus earlier nothing happens but it blocks later on

899
00:54:59,560 --> 00:55:03,090
and sort of vice versa for the central nucleus and the reason i'm sort of

900
00:55:03,090 --> 00:55:07,630
waffling here is that we don't know how long the all we know is the

901
00:55:07,630 --> 00:55:12,170
central nucleus were totally what if this is a three-second tone we don't know within

902
00:55:12,170 --> 00:55:16,460
three seconds to four minutes and we're working on that right now

903
00:55:16,610 --> 00:55:21,650
and when you activate the bays the lateral which gives the sensory information to both

904
00:55:21,670 --> 00:55:25,540
bed nucleus and the central blocks both phenomena

905
00:55:26,420 --> 00:55:29,480
the cool thing is if you use just the right dose

906
00:55:29,480 --> 00:55:34,750
now if you give the CRF antagonists so here's the control condition for four minutes

907
00:55:34,750 --> 00:55:39,100
second four minutes so that's kind of level if you give the CRF antagonists this

908
00:55:39,100 --> 00:55:42,750
fear to the first four minutes but it's got the last four minutes

909
00:55:42,790 --> 00:55:46,560
so that some evidence that what's happening in the sustain fear that we call it

910
00:55:46,770 --> 00:55:50,960
is there's release of CRF and actually that release comes from the central nucleus the

911
00:55:50,980 --> 00:55:55,500
made by now higher dose i have to be honest does blocked both that's again

912
00:55:55,500 --> 00:56:00,630
because we're we're we're collapsing of these big chunks of time

913
00:56:00,630 --> 00:56:04,720
these are the exploratory data analysis phase

914
00:56:04,740 --> 00:56:09,050
then we use descriptive data mining

915
00:56:09,100 --> 00:56:11,180
to deter mined unusual it is

916
00:56:11,190 --> 00:56:14,730
to narrow down the search list for

917
00:56:14,760 --> 00:56:15,670
and for that

918
00:56:15,670 --> 00:56:18,600
inspection of suspicious list

919
00:56:18,610 --> 00:56:21,630
and then in the second phase of the make

920
00:56:21,650 --> 00:56:24,470
we will make inferential data analysis

921
00:56:24,500 --> 00:56:31,820
here we will use predictive data mining to the indicated that needs to be expected

922
00:56:31,870 --> 00:56:35,160
and as a tool ospreys

923
00:56:35,210 --> 00:56:37,690
we plan to develop an expert system

924
00:56:37,960 --> 00:56:41,950
to help the

925
00:56:41,970 --> 00:56:43,210
time consuming

926
00:56:43,260 --> 00:56:46,250
inspection process

927
00:56:46,270 --> 00:56:49,110
now i would like to call it is

928
00:56:50,010 --> 00:56:52,210
for this

929
00:56:54,790 --> 00:56:59,910
the first phase the exploratory data analysis phase is used and is used in the

930
00:57:02,070 --> 00:57:07,510
here what we do is the customer that can't actually be

931
00:57:07,520 --> 00:57:09,690
the consolidated comes

932
00:57:09,690 --> 00:57:13,560
only because of the cost and then we look at the work the customer

933
00:57:13,620 --> 00:57:19,430
so the cost cluster its introspective some behaving variables

934
00:57:20,740 --> 00:57:23,200
and the customers close to the

935
00:57:23,210 --> 00:57:27,000
the centre of the clusters will be regarded as

936
00:57:27,020 --> 00:57:29,050
and not least because

937
00:57:29,070 --> 00:57:32,800
and the one five way from the cluster centers

938
00:57:32,810 --> 00:57:36,530
could be a very good this potential

939
00:57:37,210 --> 00:57:40,210
first suspicious

940
00:57:54,680 --> 00:57:56,730
i don't think so

941
00:58:00,150 --> 00:58:03,730
if you believe that all bad guys behave similarly

942
00:58:03,750 --> 00:58:05,490
then they can be

943
00:58:05,500 --> 00:58:08,070
together in cluster one

944
00:58:08,080 --> 00:58:09,210
i believe

945
00:58:13,880 --> 00:58:16,060
at the end we will see

946
00:58:23,040 --> 00:58:28,330
on the issue of how we can say that the customer showed unusual behaviour

947
00:58:28,350 --> 00:58:32,000
for example is the customer who made fifty

948
00:58:32,280 --> 00:58:34,430
money transfer last month

949
00:58:34,490 --> 00:58:37,560
is unusual

950
00:58:37,790 --> 00:58:41,480
actually it could be or it may be or may not be

951
00:58:42,410 --> 00:58:47,640
for example if it's if is an individual we can suspect

952
00:58:48,000 --> 00:58:52,580
but if you are the only the small business then we may be regarded as

953
00:58:58,100 --> 00:59:00,270
if he's doing this always

954
00:59:01,160 --> 00:59:03,810
it is more probable that it is not

955
00:59:05,410 --> 00:59:08,660
if if you don't do it if it doesn't belong

956
00:59:08,660 --> 00:59:12,040
one to three month and lost most of them

957
00:59:12,080 --> 00:59:14,040
i mean if the transference

958
00:59:14,060 --> 00:59:17,200
then we should also suspect from

959
00:59:18,040 --> 00:59:22,310
the question is if it is normal now depends on the

960
00:59:23,330 --> 00:59:26,580
demographic behavioral segment of discussed image

961
00:59:26,950 --> 00:59:32,290
for example if user blocked and unblocked may have some

962
00:59:32,580 --> 00:59:34,700
common behaviors

963
00:59:38,000 --> 00:59:42,250
so we have to know its history pieces what you have

964
00:59:43,750 --> 00:59:45,870
by looking at this time

965
00:59:45,930 --> 00:59:48,480
the aspects

966
00:59:48,850 --> 00:59:50,980
only after that we can

967
00:59:51,140 --> 00:59:55,060
we can judge if it is a model and not

968
00:59:55,100 --> 01:00:00,580
the first aspect the finding the period of of the clusters is handled automatically by

969
01:00:00,580 --> 01:00:03,430
most clustering of all your sins

970
01:00:03,450 --> 01:00:07,850
but for the second aspect that is to cope with the voting behavior of the

971
01:00:07,850 --> 01:00:10,180
end of the customers

972
01:00:13,790 --> 01:00:16,160
we can create new variables

973
01:00:16,200 --> 01:00:17,980
like we did here

974
01:00:18,000 --> 01:00:21,540
the we calculate the deviation

975
01:00:21,890 --> 01:00:25,080
and then use them in the cluster

976
01:00:25,100 --> 01:00:26,600
by division i mean

977
01:00:27,310 --> 01:00:31,640
the value for the customer value in the last month

978
01:00:31,640 --> 01:00:33,250
minus the evidence

979
01:00:33,250 --> 01:00:35,520
well in in the last six months

980
01:00:35,600 --> 01:00:38,270
so it's different from its past

981
01:00:38,810 --> 01:00:44,060
divided by the standard deviation of the behaviour of the last six months

982
01:00:45,930 --> 01:00:48,310
that if the deviation values

983
01:00:48,330 --> 01:00:51,060
these large either negative or positive

984
01:00:51,080 --> 01:00:54,770
then that means the customer is showing

985
01:00:56,250 --> 01:00:57,540
a different

986
01:00:57,540 --> 01:00:59,450
behaviours from

987
01:00:59,500 --> 01:01:03,560
his routine

988
01:01:04,180 --> 01:01:06,480
for example

989
01:01:06,500 --> 01:01:08,330
what we have used for this page

990
01:01:08,410 --> 01:01:10,430
some very well

991
01:01:10,430 --> 01:01:11,430
for example

992
01:01:11,430 --> 01:01:12,500
we use the

993
01:01:14,200 --> 01:01:16,970
current number of crude transactions

994
01:01:17,270 --> 01:01:19,290
the variable itself

995
01:01:20,810 --> 01:01:22,430
next to

996
01:01:22,450 --> 01:01:24,160
we develop

997
01:01:24,200 --> 01:01:27,980
here come of good contamination deviation

998
01:01:29,620 --> 01:01:33,620
in this case the variable itself and the divisions

999
01:01:33,910 --> 01:01:35,830
for this very well

1000
01:01:35,830 --> 01:01:39,060
on both

1001
01:01:39,520 --> 01:01:45,560
taught as important and well taken place in the cluster cluster

1002
01:01:45,870 --> 01:01:51,560
likewise we have some other means

1003
01:01:51,660 --> 01:01:54,680
then the

1004
01:01:56,160 --> 01:01:58,750
that we're using k variables

1005
01:01:58,790 --> 01:02:01,970
the number of being able to be used in the closing is k

1006
01:02:02,020 --> 01:02:04,270
and assume not decided

1007
01:02:04,290 --> 01:02:06,600
to have and clusters

1008
01:02:06,680 --> 01:02:10,600
then are making plastic materials

1009
01:02:10,600 --> 01:02:13,840
the first part of the here

1010
01:02:13,850 --> 01:02:16,560
something very simple

1011
01:02:19,940 --> 01:02:22,810
we have a well

1012
01:02:23,110 --> 01:02:27,030
just after big after all

1013
01:02:27,070 --> 01:02:31,860
back to using the the number of the big fj is the high level feature

1014
01:02:34,210 --> 01:02:36,160
this is

1015
01:02:36,210 --> 01:02:39,190
fj of x y

1016
01:02:39,200 --> 01:02:41,810
mine is divided wj

1017
01:02:41,810 --> 01:02:44,530
of this

1018
01:02:44,580 --> 01:02:46,500
z function

1019
01:02:56,010 --> 01:02:58,450
part here

1020
01:03:00,090 --> 01:03:01,560
going to be

1021
01:03:01,580 --> 01:03:05,030
let's go is going to work out this is going to work out to be

1022
01:03:06,040 --> 01:03:07,460
very elegant

1023
01:03:07,560 --> 01:03:12,530
but it takes a few lines to to work out what it is

1024
01:03:22,270 --> 01:03:23,920
good question so

1025
01:03:23,940 --> 01:03:28,700
this is a partial derivative with respect to parameter number j

1026
01:03:28,790 --> 01:03:30,700
and this is the sum

1027
01:03:31,700 --> 01:03:33,370
he said

1028
01:03:33,410 --> 01:03:35,200
all the other one disappear

1029
01:03:35,210 --> 01:03:39,530
it's a linear function of w is all the other ones disappear

1030
01:03:49,100 --> 01:03:53,350
to get some room

1031
01:03:56,410 --> 01:04:00,210
divided wj log p

1032
01:04:07,750 --> 01:04:09,710
x y

1033
01:04:14,690 --> 01:04:16,340
i i d w j

1034
01:04:19,950 --> 01:04:24,090
divide the wj others e

1035
01:04:28,390 --> 01:04:30,770
one minus divided wj

1036
01:04:32,500 --> 01:04:38,130
log c

1037
01:04:38,190 --> 01:04:40,930
so this is one of those e

1038
01:04:40,980 --> 01:04:43,330
day by day wj

1039
01:04:58,920 --> 01:05:00,220
when we

1040
01:05:00,270 --> 01:05:12,530
OK i'm just looking at my own notes and trying to figure out

1041
01:05:12,550 --> 01:05:17,400
well that went along

1042
01:05:17,420 --> 01:05:22,870
OK so easy is the summation

1043
01:05:24,000 --> 01:05:25,670
this is

1044
01:05:25,730 --> 01:05:28,680
one of us e

1045
01:05:28,690 --> 01:05:31,390
some of

1046
01:05:31,440 --> 01:05:35,310
why prime

1047
01:05:36,210 --> 01:05:40,440
this is actually a minus here

1048
01:05:40,480 --> 01:05:42,220
is that

1049
01:05:42,280 --> 01:05:43,180
OK now

1050
01:05:44,170 --> 01:05:47,270
the mine is here not here

1051
01:05:47,820 --> 01:05:49,780
one of fuzzy

1052
01:05:50,200 --> 01:05:53,100
then xe is the summation

1053
01:05:53,110 --> 01:05:56,640
and so i'm going to bring the derivative inside the summation

1054
01:05:57,690 --> 01:06:00,360
the sum of the y prime

1055
01:06:00,370 --> 01:06:02,170
of my

1056
01:06:02,190 --> 01:06:03,600
day by

1057
01:06:03,600 --> 01:06:06,320
day wj

1058
01:06:07,850 --> 01:06:10,350
and then

1059
01:06:10,420 --> 01:06:11,620
inside here

1060
01:06:11,620 --> 01:06:15,330
we have an exponential

1061
01:06:15,350 --> 01:06:17,820
the model

1062
01:06:23,510 --> 01:06:27,520
some of the j prime

1063
01:06:27,580 --> 01:06:29,700
wj prime

1064
01:06:30,880 --> 01:06:36,610
two primary accent my prime

1065
01:06:36,620 --> 01:06:38,230
of my

1066
01:06:38,260 --> 01:06:41,890
so really the only thing down here to write out what see is

1067
01:06:41,910 --> 01:06:46,100
which is the sum of the numerator so here's the numerator

1068
01:06:50,620 --> 01:06:53,210
the derivative have an exponential is

1069
01:06:53,230 --> 01:06:54,560
the exponential

1070
01:06:54,560 --> 01:06:57,410
times iterative what's inside the exponential

1071
01:06:57,460 --> 01:06:59,950
so we have

1072
01:06:59,960 --> 01:07:02,650
one of us e

1073
01:07:02,660 --> 01:07:05,220
some of the the y prime

1074
01:07:05,230 --> 01:07:08,110
and then

1075
01:07:14,250 --> 01:07:19,340
which is the sum of the j prime wj prime

1076
01:07:19,340 --> 01:07:19,740
okay so

1077
01:07:20,260 --> 01:07:26,200
for those who know what this means is far from hamill tony and just like an instantaneous time unitary

1078
01:07:26,610 --> 01:07:31,750
transformation that has some known easily prepared ground state then you slowly transition it to

1079
01:07:31,750 --> 01:07:36,860
a hamiltonian whose ground state encodes the solution to guarantee complete problem and there's a

1080
01:07:36,860 --> 01:07:41,030
theorem that says that as long as you vary the hamiltonian slowly enough thee

1081
01:07:41,730 --> 01:07:46,380
grounds that be the state of your quantum computer must just get tracked you know

1082
01:07:46,380 --> 01:07:49,470
along with the ground state and so then at the end you'll be able to

1083
01:07:49,470 --> 01:07:54,060
measure and get the solution to and be complete problem going to be key the

1084
01:07:54,060 --> 01:07:57,470
million dollar question here is how slowly slowly enough

1085
01:07:58,060 --> 01:08:01,440
i and here the problem is that you know the running time you need to

1086
01:08:01,450 --> 01:08:06,300
run this algorithm fore is determined by what's called the inverse igon value gap of

1087
01:08:06,310 --> 01:08:07,200
the hamiltonian

1088
01:08:07,580 --> 01:08:11,490
okay and what you find when you try running this on you know hard like

1089
01:08:11,490 --> 01:08:17,010
three three-set instances for example and people have tried it both numerically ants you've analytically

1090
01:08:17,450 --> 01:08:21,470
a very know that the last decade okay find that of any other igon value

1091
01:08:21,470 --> 01:08:26,430
gap just one little point it becomes exponentially small so it looks like the two

1092
01:08:26,430 --> 01:08:30,660
i can values are crossing each other you know they not quite okay but you

1093
01:08:30,660 --> 01:08:34,620
know because you know because about one little place where these i can values almost

1094
01:08:34,620 --> 01:08:38,060
kiss that's why you have to run the algorithm for exponential time

1095
01:08:38,690 --> 01:08:42,190
okay so far he told me the story that he once asked an expert in

1096
01:08:42,190 --> 01:08:47,190
condensed matter physics but based on your experience you know over decades those sort of

1097
01:08:47,190 --> 01:08:51,530
similar physical systems do you think that this i can value gap is going to

1098
01:08:51,530 --> 01:08:57,910
decrease polynomially are exponentially has the size of the system increases and the experts said

1099
01:08:57,910 --> 01:09:02,090
well i think it will decrease exponentially and far he said well why what makes

1100
01:09:02,090 --> 01:09:05,730
you say that any experts said well because otherwise the algorithm would work

1101
01:09:07,870 --> 01:09:10,580
so you know if you believe strongly enough people

1102
01:09:10,730 --> 01:09:12,990
the problems are hard you can reason backwards from there

1103
01:09:13,310 --> 01:09:13,780
but right

1104
01:09:15,320 --> 01:09:20,310
so what we know today is that on some fitness landscapes these diabetic algorithm can

1105
01:09:20,310 --> 01:09:25,880
reach a global minimum exponentially faster than classical simulated annealing but on other types of

1106
01:09:25,880 --> 01:09:30,090
fitness landscapes it us about the same or even worse to know what sort of

1107
01:09:30,090 --> 01:09:34,730
behavior predominates in practice would help a lot have a quantum computer test it our

1108
01:09:35,480 --> 01:09:39,060
but now some of you might be saying that is under this company that's you

1109
01:09:39,060 --> 01:09:43,220
know already you know claims that already built quantum computers that can you know ronnie

1110
01:09:43,570 --> 01:09:48,320
quantum adiabatic algorithm one day there's you know it's called d-wade systems you know and

1111
01:09:48,320 --> 01:09:51,660
they they have these devices right and you know here's the situation

1112
01:09:52,140 --> 01:09:57,020
we know that there are devices assertive can solve optimization problems on up to about

1113
01:09:57,020 --> 01:10:01,710
one hundred bits you in fact sell them reasonably well you know fairly quickly we

1114
01:10:01,710 --> 01:10:05,800
also know that we use the one qubit level there is some kind of quantum

1115
01:10:06,400 --> 01:10:12,290
coherence intuitive devices what we don't know at this point is whether the quantum coherence

1116
01:10:12,310 --> 01:10:17,130
is playing any kind of causal role in speeding up the computation in other words

1117
01:10:17,150 --> 01:10:19,300
it remains consistent with what we know today

1118
01:10:19,790 --> 01:10:24,970
that's what do wave has done is basically to build like a very fast special-purpose

1119
01:10:25,240 --> 01:10:30,200
classical computer for simulated annealing okay now there's a group of your essay which is

1120
01:10:30,200 --> 01:10:34,230
currently running tests with this machine and hopefully will know a lot more

1121
01:10:34,680 --> 01:10:36,830
in the near future about you know about

1122
01:10:37,590 --> 01:10:42,380
it's characterization that but for now i'd say it remains you know so to speak a black box

1123
01:10:44,230 --> 01:10:46,180
sorry sorry as an aside

1124
01:10:47,510 --> 01:10:49,390
you know you may wander can on

1125
01:10:49,800 --> 01:10:53,230
you know is actually true that you know as all the popular articles

1126
01:10:53,790 --> 01:10:59,280
the in qubits can encode to do the empower classical bits well you know that's

1127
01:10:59,280 --> 01:11:03,710
the number of bits that you would need to describe the state event qubits but

1128
01:11:03,770 --> 01:11:07,470
you know the number of bits that you can actually read out by measuring the

1129
01:11:07,470 --> 01:11:11,590
qubits may be much much smaller than okay in in fact you know i was

1130
01:11:11,590 --> 01:11:14,960
able to use the machine learning type of ideas to show

1131
01:11:15,490 --> 01:11:16,490
that in certain you know

1132
01:11:17,170 --> 01:11:23,240
operational sense is a quantum state event qubits behaves effectively like it has only a

1133
01:11:23,240 --> 01:11:26,390
polynomial number that's rather than an exponential number

1134
01:11:26,780 --> 01:11:28,300
so here's one theorem that

1135
01:11:28,890 --> 01:11:29,680
that approved

1136
01:11:30,220 --> 01:11:35,270
the given any and qubit states i suppose that you only care about size behavior

1137
01:11:35,510 --> 01:11:41,470
on two outcome measurements in some finite set cave then there exists a subset of

1138
01:11:41,470 --> 01:11:48,830
those measurements caught a very small size only analog measurements such that is a few

1139
01:11:49,190 --> 01:11:55,540
training process research started with the maximally ignorant guesses about what's i was a new

1140
01:11:55,540 --> 01:12:00,380
posts selected on your state giving you the right answers on all the measurements in

1141
01:12:00,380 --> 01:12:04,490
the city you want all analog and have them then you would end up with

1142
01:12:04,490 --> 01:12:10,770
the state there approximately simulates the behavior of fly your desired state on all

1143
01:12:11,160 --> 01:12:14,210
the measurements in or in the entire set

1144
01:12:14,960 --> 01:12:19,470
okay so what you know andy the idea of the proof is the use darwinian

1145
01:12:19,470 --> 01:12:25,460
sort of training process similar to boosting she repeatedly find measurements were carried guess is

1146
01:12:25,460 --> 01:12:30,040
still badly wrong even conditioned on being right on all the previous guess is okay

1147
01:12:30,370 --> 01:12:33,830
and then you know if you condition on being right on the new example then

1148
01:12:33,930 --> 01:12:38,290
review much you must learn something right and if there's no you know example we're

1149
01:12:38,390 --> 01:12:42,120
badly wrong then you're done right and then you and then you prove a bound

1150
01:12:42,260 --> 01:12:46,890
by using the linearity of quantum mechanics on the number of you know training steps

1151
01:12:46,890 --> 01:12:50,120
you have to go through before you know you must have a state that you

1152
01:12:50,120 --> 01:12:51,810
know that works for everything

1153
01:12:52,250 --> 01:12:57,060
okay i know well what this means is that we can describe the behavior of

1154
01:12:57,060 --> 01:13:01,910
an and qubit states i you know on well enough to survive reproduce you know

1155
01:13:02,150 --> 01:13:06,030
that the probability that you're gonna get a yes you know under to the end

1156
01:13:06,030 --> 01:13:10,230
different measurements and we can do that you know using just a summary with only

1157
01:13:10,360 --> 01:13:15,610
and squared log in classical bits it's a polynomial number of classical bits case only

1158
01:13:15,610 --> 01:13:19,620
at this sort of question you know quantum states if you to shrink down the

1159
01:13:19,620 --> 01:13:20,590
size in some way

1160
01:13:21,090 --> 01:13:25,570
on you know no later approved another theorem which he knows even more directly from

1161
01:13:25,570 --> 01:13:30,500
the machine learning flavor it is given in cubits states i suppose you only care

1162
01:13:30,500 --> 01:13:35,810
about its behavior on two outcome measurements that are drawn from some probability distribution did

1163
01:13:36,160 --> 01:13:37,110
and then you can do

1164
01:13:37,730 --> 01:13:40,410
draw some sample measurements and one-upped

1165
01:13:41,310 --> 01:13:46,380
you know independently from day and the number of measurements only has the bi-linear not

1166
01:13:46,380 --> 01:13:54,120
exponentially with the number of qubits over sample measurements then find any hypothesis states that

1167
01:13:54,120 --> 01:14:00,380
approximately agrees with site your target state on all of those sample measurements can then

1168
01:14:00,380 --> 01:14:05,840
one can prove that with high probability over the choice of sample measurements their feet

1169
01:14:06,030 --> 01:14:12,610
must also approximately simulate the behavior of site almost measurements drawn from the entire distribution

1170
01:14:13,070 --> 01:14:17,630
okay and the proof idea is to use the notion of fat shattering dimension from

1171
01:14:17,630 --> 01:14:22,810
there is some common caching strategies talk about that

1172
01:14:25,300 --> 01:14:29,160
you would want to do and adaptive algorithm why because

1173
01:14:29,170 --> 01:14:30,910
the applications are

1174
01:14:33,880 --> 01:14:35,010
the the the

1175
01:14:35,020 --> 01:14:39,990
the optimal caching strategies application as well as time dependent

1176
01:14:40,000 --> 01:14:41,900
choosing one is suboptimal

1177
01:14:41,960 --> 01:14:48,350
is it the screen where may be straight or window of three hundred

1178
01:14:48,400 --> 01:14:52,720
OK the best strategy varies with time

1179
01:14:52,730 --> 01:14:58,230
twenty one of these good as the best but also shift somehow

1180
01:14:58,270 --> 01:15:00,830
and here needed data

1181
01:15:00,940 --> 01:15:02,740
mister rates got worse

1182
01:15:02,760 --> 01:15:08,140
so what's it good compare everybody would say oh my arguments online that

1183
01:15:08,160 --> 01:15:12,210
so one of the main thing in this research and practice situation is to come

1184
01:15:12,210 --> 01:15:14,780
up with the stringent compared

1185
01:15:14,790 --> 01:15:18,050
very simple compared to is best fixed

1186
01:15:18,060 --> 01:15:20,510
OK here twelve policies

1187
01:15:20,530 --> 01:15:25,180
in hindsight you look at the whole data compute the mystery of the

1188
01:15:26,120 --> 01:15:28,580
apologies to all policies and pick the best one

1189
01:15:28,590 --> 01:15:31,320
dublin of core course should be as good as the best

1190
01:15:31,370 --> 01:15:34,630
fixed chosen in hindsight

1191
01:15:34,710 --> 01:15:40,640
this is trivial to achieve actually in the state because it is highly variable

1192
01:15:40,690 --> 01:15:42,890
he is best three things

1193
01:15:42,940 --> 01:15:45,710
is the minimum number of missus

1194
01:15:45,750 --> 01:15:48,230
with the most are factors

1195
01:15:49,770 --> 01:15:52,090
what is the reason for

1196
01:15:52,110 --> 01:15:56,840
some users who are not you decide to get back

1197
01:15:56,860 --> 01:15:59,500
so we fetch

1198
01:15:59,530 --> 01:16:01,150
OK and

1199
01:16:01,200 --> 01:16:04,530
is the minimum number of missus with most are factors

1200
01:16:04,540 --> 01:16:10,090
when you have a sequence of switching policies no metadata switching over time

1201
01:16:10,110 --> 01:16:15,030
so you might be in the situation where this policy is best then this policy

1202
01:16:15,030 --> 01:16:17,110
is best then from trials

1203
01:16:17,150 --> 01:16:21,690
fifty to seventy and size is best from seventy two one twenty

1204
01:16:21,710 --> 01:16:25,900
GDS is best and then is best for the segment

1205
01:16:25,900 --> 01:16:29,080
you want to be as good as the best switching policy but

1206
01:16:29,090 --> 01:16:30,350
what happens here

1207
01:16:30,370 --> 01:16:32,870
well when you want to switch

1208
01:16:32,870 --> 01:16:36,180
from this capture this cash you need to be

1209
01:16:36,190 --> 01:16:40,970
why because this guy has different things kept different things and this guy did

1210
01:16:41,800 --> 01:16:45,680
there's a reef edge cost how many files so how much how much you have

1211
01:16:45,710 --> 01:16:48,460
to refresh

1212
01:16:54,190 --> 01:17:01,500
the cost the mister miss rate of these combined policies miss rate some or all

1213
01:17:01,500 --> 01:17:04,180
of these segments plus the total

1214
01:17:04,190 --> 01:17:08,340
among the three factors so we this becomes the point in now blocked

1215
01:17:08,340 --> 01:17:09,620
tony revenge

1216
01:17:09,650 --> 01:17:10,750
total mess

1217
01:17:13,020 --> 01:17:17,470
so interest each of these composite strategy gives alpha point

1218
01:17:17,470 --> 01:17:22,340
and we interested in the lower envelope of this

1219
01:17:22,400 --> 01:17:25,850
which means for given me we will look for the

1220
01:17:25,870 --> 01:17:32,280
for given number three is we look for the best paul pascal combined

1221
01:17:36,970 --> 01:17:40,120
that means in terms of miss rate

1222
01:17:40,130 --> 01:17:44,180
so i'm going to go this goal of this game

1223
01:17:44,220 --> 01:17:50,430
i'm not interested in comparing just against the best fixed policy and the interesting

1224
01:17:50,430 --> 01:17:53,600
compare against the best competition policy

1225
01:17:53,620 --> 01:17:55,430
we should use

1226
01:17:55,430 --> 01:17:57,990
politician over time this time

1227
01:17:58,880 --> 01:18:04,800
each time i shift from one policy to the next time you refresh

1228
01:18:06,590 --> 01:18:11,840
this guy has to discard it so it's five one three five

1229
01:18:11,900 --> 01:18:16,090
and this is the highest firewall this one had find five one but not the

1230
01:18:16,090 --> 01:18:20,030
five minutes ago and you have to get three five

1231
01:18:20,210 --> 01:18:22,030
to remove edges

1232
01:18:22,630 --> 01:18:28,170
have after some of the factors that drove point for the total this is my

1233
01:18:28,230 --> 01:18:31,510
sort of like cost the number of features

1234
01:18:32,520 --> 01:18:35,300
total number of missus

1235
01:18:36,540 --> 01:18:42,740
for a given set b is i want to get the point is the lowest

1236
01:18:42,760 --> 01:18:44,650
because they want to minimize missus

1237
01:18:44,650 --> 01:18:46,600
so i'm interested in

1238
01:18:46,610 --> 01:18:49,530
the lower in the question

1239
01:18:49,530 --> 01:18:51,830
you have to understand this

1240
01:18:51,840 --> 01:18:56,840
so a lot work on this was designing online comparison

1241
01:18:56,840 --> 01:19:03,400
and in the definition of the input about of serving a process so just serving

1242
01:19:03,400 --> 01:19:08,760
the let's say the y variable the output variable

1243
01:19:08,760 --> 01:19:13,920
so in that case i even in a situation which is even more so

1244
01:19:13,960 --> 01:19:19,610
with respect to the situation of the input of modelling which is still in that

1245
01:19:19,610 --> 01:19:25,110
case i have an input and they can model some colours i two relationship in

1246
01:19:25,110 --> 01:19:28,150
some way in this potential functions

1247
01:19:29,110 --> 01:19:31,650
so is the

1248
01:19:31,670 --> 01:19:36,150
it is very important to take into account the in that cases

1249
01:19:36,690 --> 01:19:44,980
what is it because i've shown that we want to derive a statistical model

1250
01:19:44,990 --> 01:19:47,090
of the of the data

1251
01:19:47,170 --> 01:19:50,650
which the the data

1252
01:19:51,730 --> 01:19:54,940
structure of this place as a function of

1253
01:19:54,940 --> 01:19:57,880
they are past venues

1254
01:19:59,130 --> 01:20:09,190
in the classical theory which is the case with this problem usually refers to

1255
01:20:09,820 --> 01:20:14,960
what are known as ottoman more is the first oo

1256
01:20:16,650 --> 01:20:22,130
we will see briefly what these modelling are because they have to arrive at the

1257
01:20:22,130 --> 01:20:23,860
end of the lesson with the

1258
01:20:23,940 --> 01:20:29,210
delete according to the embedding algorithm that is reported in the book

1259
01:20:33,070 --> 01:20:38,210
one thing which is that you can assume that when we just observe one thing

1260
01:20:38,230 --> 01:20:40,460
series all of which

1261
01:20:40,480 --> 01:20:43,840
i have also just the output variable

1262
01:20:43,920 --> 01:20:48,840
is that the the it is shown that the time series

1263
01:20:48,900 --> 01:20:55,110
is stationary which is the following that the statistical property of the process which generates

1264
01:20:55,130 --> 01:21:01,170
the data do not change over time

1265
01:21:01,190 --> 01:21:07,690
so if you start time series the nose

1266
01:21:07,840 --> 01:21:15,930
statistical book time series you start defining two different kind of the mothers of the

1267
01:21:15,930 --> 01:21:23,590
time series before the first one is the so-called out aggressive more the which is

1268
01:21:23,590 --> 01:21:26,690
also used in the engineering tradition

1269
01:21:26,750 --> 01:21:36,340
because is something which seems somewhat similar to the a difference equations someone here is

1270
01:21:36,340 --> 01:21:43,880
just reported the simplest out aggressive models in which we have some data which are

1271
01:21:43,880 --> 01:21:45,630
sampled over time

1272
01:21:45,650 --> 01:21:54,760
these data are all fully evenly spaced and this more than just says that the

1273
01:21:54,760 --> 01:21:58,650
the next value of the time series is a linear function

1274
01:21:58,650 --> 01:22:04,880
all of the current value of the time series asserting noise

1275
01:22:07,210 --> 01:22:14,880
what is interesting if you want to make it parallelism with the dynamic modelling that

1276
01:22:14,900 --> 01:22:17,760
we have used until now

1277
01:22:17,780 --> 01:22:24,230
is that if you if you think of the noise which is easier it is

1278
01:22:24,230 --> 01:22:27,070
to use which affects the next value

1279
01:22:27,090 --> 01:22:29,010
as an input

1280
01:22:29,010 --> 01:22:31,050
you can see the here

1281
01:22:31,050 --> 01:22:33,230
we have the dynamical system

1282
01:22:33,300 --> 01:22:36,650
because we have the this relation is

1283
01:22:36,690 --> 01:22:38,380
difference equations so

1284
01:22:38,380 --> 01:22:43,710
this the relation of the next value is a function of the current one which

1285
01:22:43,710 --> 01:22:46,360
is driven by the noise

1286
01:22:46,360 --> 01:22:47,840
so the noise

1287
01:22:47,860 --> 01:22:50,610
is this sort of unknown input

1288
01:22:52,030 --> 01:22:58,380
it makes the dynamics the system change over time so is

1289
01:22:58,380 --> 01:23:00,210
very interesting to

1290
01:23:00,260 --> 01:23:04,610
see not only the noise is the some

1291
01:23:04,650 --> 01:23:07,460
sort of disturbance but let's say

1292
01:23:07,550 --> 01:23:14,150
but this is the sort of inputs which makes it the time series behaving that

1293
01:23:14,150 --> 01:23:19,800
way in some way so it can understand another way to see this problem is

1294
01:23:19,800 --> 01:23:25,070
of course to estimate the a one but it can also be used to estimate

1295
01:23:25,070 --> 01:23:29,030
what is the what are the inputs which are

1296
01:23:29,070 --> 01:23:36,280
the drivers of the dynamics so what your goal can be effectively to estimated is

1297
01:23:37,210 --> 01:23:40,510
there these noise that k by k

1298
01:23:40,530 --> 01:23:42,820
that is what exactly you are doing it

1299
01:23:45,780 --> 01:23:48,710
in other more than which is

1300
01:23:49,170 --> 01:23:54,730
related it's possible to show using popular during

1301
01:23:54,750 --> 01:24:01,900
theorem which is related with a data model is the so-called moving average model

1302
01:24:01,920 --> 01:24:08,300
in this case the effect of this driver which is the noise these since some

1303
01:24:08,300 --> 01:24:10,690
way integrated over time

1304
01:24:10,690 --> 01:24:13,610
so the next value of the r

1305
01:24:13,650 --> 01:24:17,150
barely all the time series is the function

1306
01:24:17,150 --> 01:24:23,030
all of the past noise values not only of the current web

1307
01:24:23,050 --> 01:24:28,940
if you think of this notion of the noise is the driver of the dynamic

1308
01:24:29,420 --> 01:24:33,010
you understand that this kind of model because you see

1309
01:24:33,030 --> 01:24:38,360
you say you are saying here that your comment output that is the function of

1310
01:24:38,360 --> 01:24:41,550
the past story of the inputs of the system

1311
01:24:41,570 --> 01:24:48,090
seeing the noise is an is an unknown output of the dynamic systems

1312
01:24:48,090 --> 01:24:49,820
that we have

1313
01:24:49,960 --> 01:24:52,960
again if you take a look at the literature

1314
01:24:52,960 --> 01:24:56,650
you find the high number

1315
01:24:56,690 --> 01:25:00,750
of combinations of the

1316
01:25:00,750 --> 01:25:03,480
being are made in in image

1317
01:25:03,510 --> 01:25:05,650
mothers in which two

1318
01:25:06,070 --> 01:25:10,110
at the end of the story you arrive these are my modelling which is the

1319
01:25:10,110 --> 01:25:12,840
following way

1320
01:25:12,860 --> 01:25:16,350
a new flow together

1321
01:25:16,350 --> 01:25:17,340
we did

1322
01:25:17,340 --> 01:25:25,250
the output we have also some deterministic input so something which is known to serve

1323
01:25:25,250 --> 01:25:28,800
as the input of the system we can write to them

1324
01:25:28,810 --> 01:25:33,550
even more general model of the input output system

1325
01:25:33,560 --> 01:25:38,590
of the time series which is just the composition

1326
01:25:38,600 --> 01:25:48,010
of to aggressive there it remains of related to the moving average part and that

1327
01:25:48,010 --> 01:25:54,210
the which is on the contrary related to the extent that puts which are even

1328
01:25:54,210 --> 01:25:59,100
which have no so these these are known and these are known say

1329
01:25:59,100 --> 01:26:00,600
mister k

1330
01:26:00,630 --> 01:26:08,470
looking them in the dynamics is the function

1331
01:26:08,720 --> 01:26:11,760
this is just the two

1332
01:26:11,760 --> 01:26:14,080
so what about reinforcement learning

1333
01:26:14,660 --> 01:26:20,580
is a little bit of back material available here if you shouldn't ignore get

1334
01:26:21,290 --> 01:26:27,280
OK so i guess the first question is what is reinforced

1335
01:26:27,330 --> 01:26:32,140
and reinforcement learning is probably the most general

1336
01:26:32,150 --> 01:26:35,630
former learning problem and we try to solve

1337
01:26:35,640 --> 01:26:38,550
so yesterday talked about

1338
01:26:38,560 --> 01:26:42,660
classification and in various forms supervised learning

1339
01:26:42,690 --> 01:26:47,210
there's also something on a semi supervised learning work people have in addition

1340
01:26:47,220 --> 01:26:50,640
a bunch of unlabelled data that try to learn from

1341
01:26:50,650 --> 01:26:51,870
and then

1342
01:26:54,040 --> 01:26:57,790
this is the simplest form of reinforcement learning which

1343
01:26:58,000 --> 01:27:01,900
to reinforcement learning in markov decision process

1344
01:27:01,920 --> 01:27:07,110
and then there's the general form of reinforcement learning which contains everything

1345
01:27:10,880 --> 01:27:11,690
it is

1346
01:27:11,700 --> 01:27:16,120
the question is or why is it important to study reinforcement learning

1347
01:27:21,250 --> 01:27:24,410
if you facing learning problem in the real world

1348
01:27:24,420 --> 01:27:28,120
you can ask questions is this reinforcement learning problem

1349
01:27:28,130 --> 01:27:31,080
and it turns out the answer is always yes

1350
01:27:31,230 --> 01:27:38,510
so in essence what does it mean that interesting but the point here is that

1351
01:27:38,510 --> 01:27:43,800
if we have any kind of theory for reinforcement learning then it can apply very

1352
01:27:43,800 --> 01:27:45,950
generally all kinds of problems

1353
01:27:47,160 --> 01:27:51,330
now it's not the kind of things we can prove for reinforcement learning

1354
01:27:51,340 --> 01:27:54,210
are not as strong as you might hope because

1355
01:27:54,220 --> 01:27:56,680
in reinforcement learning is incredibly general

1356
01:27:57,760 --> 01:27:59,940
nevertheless you can prove some things

1357
01:28:02,430 --> 01:28:04,150
these things can

1358
01:28:04,210 --> 01:28:05,180
help you

1359
01:28:05,810 --> 01:28:09,570
understand how to get solutions to individual problems

1360
01:28:10,150 --> 01:28:13,800
when face some problem in the real world

1361
01:28:13,820 --> 01:28:17,350
you start out as stubs reinforce morning you know the answer is yes

1362
01:28:17,360 --> 01:28:21,360
need to ask yourself is this something which is simpler than the full reinforcement learning

1363
01:28:22,470 --> 01:28:24,350
so the goal typically

1364
01:28:24,360 --> 01:28:27,780
interested airing down what problem you have is

1365
01:28:27,830 --> 01:28:31,020
you start out with general reinforcement learning in

1366
01:28:31,040 --> 01:28:34,160
then you discover that you know maybe

1367
01:28:34,210 --> 01:28:37,280
this is some sort of simple problem

1368
01:28:37,930 --> 01:28:40,820
there are other problems as well which are here

1369
01:28:40,870 --> 01:28:44,490
and then what you discover exactly how simple you can make the problem

1370
01:28:44,540 --> 01:28:47,610
then you can try to attack it

1371
01:28:49,990 --> 01:28:54,490
by the way

1372
01:28:54,510 --> 01:28:58,000
questions are good so should feel free to ask questions today

1373
01:28:58,050 --> 01:29:02,710
i s

1374
01:29:06,850 --> 01:29:09,000
yeah it is actually pretty interesting

1375
01:29:09,890 --> 01:29:12,620
versions of learning so in bandits

1376
01:29:12,640 --> 01:29:13,710
what happens is

1377
01:29:13,720 --> 01:29:17,860
you have it's like you go to the casino

1378
01:29:17,870 --> 01:29:21,510
and you have a bunch of machines you can put money into any component or

1379
01:29:22,480 --> 01:29:26,200
in some of the arms payoff more than other times

1380
01:29:26,210 --> 01:29:29,640
but you don't know which pays that the most intense

1381
01:29:29,690 --> 01:29:34,030
so the goal in and then learning is to try to

1382
01:29:34,630 --> 01:29:39,800
do as well as the best individual are by exploring and some according to some

1383
01:29:42,170 --> 01:29:43,250
active learning

1384
01:29:43,270 --> 01:29:46,470
is a different setting

1385
01:29:46,490 --> 01:29:49,410
so in general reinforcement learning what happens is

1386
01:29:49,430 --> 01:29:51,510
you can interact with the environment

1387
01:29:51,520 --> 01:29:54,670
and the process of interaction with environment changes

1388
01:29:54,720 --> 01:29:57,260
what's going to in the future

1389
01:29:57,280 --> 01:29:59,750
in active learning that's not true

1390
01:29:59,760 --> 01:30:05,380
but you are actively interacting with the environment so that he is you want classification

1391
01:30:05,390 --> 01:30:07,270
so you start requesting

1392
01:30:07,700 --> 01:30:10,380
labels for individual examples

1393
01:30:10,430 --> 01:30:12,300
and then

1394
01:30:12,320 --> 01:30:17,430
at some point you get some labels introduce classifier

1395
01:30:17,470 --> 01:30:21,320
so active learning is sort of the interactive version

1396
01:30:22,130 --> 01:30:27,320
just classification learning or just supervised learning

1397
01:30:31,070 --> 01:30:34,560
this is one of the very basic things it seems that

1398
01:30:34,580 --> 01:30:37,650
interaction the ability to interact with the world

1399
01:30:37,730 --> 01:30:39,710
is radically more powerful than

1400
01:30:39,760 --> 01:30:41,460
just the ability to do

1401
01:30:41,590 --> 01:30:44,990
batch learning as we talked about yesterday

1402
01:30:46,060 --> 01:30:49,060
so what i mean there have been more probable what i mean is the number

1403
01:30:49,060 --> 01:30:51,140
of labelled you may need to request

1404
01:30:51,150 --> 01:30:53,930
can the logarithmic in the number that are required to learn

1405
01:30:53,970 --> 01:30:56,070
if you just

1406
01:30:56,120 --> 01:30:57,120
using batch

1407
01:30:57,140 --> 01:31:01,650
learning we talked about yesterday

1408
01:31:01,660 --> 01:31:06,410
so always logarithmic example where might be logarithmic is

1409
01:31:06,820 --> 01:31:07,960
suppose you

1410
01:31:08,010 --> 01:31:09,490
you just trying to

1411
01:31:09,500 --> 01:31:10,820
learn to predict

1412
01:31:10,980 --> 01:31:13,030
a threshold online right

1413
01:31:13,050 --> 01:31:17,200
so the and of there's no no is going be very easy to

1414
01:31:17,290 --> 01:31:20,310
the first query in the middle to see if

1415
01:31:20,360 --> 01:31:23,110
it's a threshold above and below

1416
01:31:23,120 --> 01:31:24,630
the query point

1417
01:31:24,640 --> 01:31:28,870
and that partitions the set and then you can just because of the query

1418
01:31:28,880 --> 01:31:33,720
just narrowing in very quickly on where the threshold this

1419
01:31:33,720 --> 01:31:36,140
we have some kind of a

1420
01:31:36,190 --> 01:31:39,630
the loop feedback loop here going on

1421
01:31:39,680 --> 01:31:43,180
and this is why i wanted to be a little bit of time explaining how

1422
01:31:43,180 --> 01:31:48,470
we can use the challenges as as a teaching tool so this reaches a little

1423
01:31:48,470 --> 01:31:51,170
bit also the theme of the second speaker because

1424
01:31:51,180 --> 01:31:55,030
in the class i taught last year

1425
01:31:55,040 --> 01:31:58,300
on feature selection i used the

1426
01:31:58,320 --> 01:32:00,080
feature selection challenge

1427
01:32:00,100 --> 01:32:04,260
and as the motivation for the students to r

1428
01:32:04,340 --> 01:32:08,390
to do some projects and task was to

1429
01:32:08,400 --> 01:32:16,820
match outperform the results of the best challengers in the NIPS two-thousand three challenge

1430
01:32:16,850 --> 01:32:21,320
so again you know this is the

1431
01:32:21,930 --> 01:32:26,220
for the book which there is that the data sets are and also a lot

1432
01:32:26,220 --> 01:32:31,400
of teaching materials so this is i guess you know my attempt to create curriculum

1433
01:32:31,590 --> 01:32:36,980
this is the curriculum consists of a

1434
01:32:37,010 --> 01:32:44,460
the city was the head of the challenge chapters contributed by by people

1435
01:32:44,470 --> 01:32:51,390
ten chapters that are tutorials and which show followed essentially in in my class and

1436
01:32:51,390 --> 01:32:56,680
then there are chapters that research papers written about the results of the challenge on

1437
01:32:56,680 --> 01:32:57,710
the data

1438
01:32:57,720 --> 01:33:01,190
it same class people had to

1439
01:33:01,210 --> 01:33:08,490
these different aspects the two illustrated so they they had to produce results on all

1440
01:33:08,490 --> 01:33:11,040
five data sets of the challenge

1441
01:33:11,050 --> 01:33:17,320
and the data sets to include just one by one in the in the curriculum

1442
01:33:17,320 --> 01:33:18,660
in the class

1443
01:33:18,690 --> 01:33:24,860
associated with a particular difficulty in feature selection

1444
01:33:24,860 --> 01:33:29,550
and there were classes that were

1445
01:33:29,560 --> 01:33:30,600
link to dr

1446
01:33:30,610 --> 01:33:34,530
the tutorial chapters so they were then

1447
01:33:34,780 --> 01:33:40,540
a simple courses that i would give their classes which were linked to readings so

1448
01:33:40,540 --> 01:33:44,590
this is a more like some the students had to read the chapter then it

1449
01:33:44,590 --> 01:33:48,550
was all researched up upturn in that case and each student to complete the class

1450
01:33:48,550 --> 01:33:53,410
had to choose one of the research chapters and presented

1451
01:33:53,440 --> 01:33:57,830
so this was one of the requirements for the class the second was to produce

1452
01:33:57,830 --> 01:34:00,250
results on all five datasets

1453
01:34:00,280 --> 01:34:06,540
and the sole requirement was that they would make a poster and then we had

1454
01:34:06,540 --> 01:34:07,730
the poster session

1455
01:34:07,750 --> 01:34:12,790
in end they would present the poster about the results that they had on the

1456
01:34:12,800 --> 01:34:14,510
five datasets

1457
01:34:17,550 --> 01:34:23,980
in addition to that because i had no assistant this class was a class i

1458
01:34:23,980 --> 01:34:29,490
taught in switzerland tth and i was just guest professor to give me your system

1459
01:34:29,630 --> 01:34:34,790
so for the class sometimes discussed how to

1460
01:34:35,440 --> 01:34:41,700
produce results on the dataset but sometimes they use datasets close to familiarize the students

1461
01:34:41,700 --> 01:34:49,690
with the hull you become a consultant in machine learning because this is my job

1462
01:34:49,690 --> 01:34:50,580
and i thought

1463
01:34:50,670 --> 01:34:55,450
this would be a little bit different view for the students will always start by

1464
01:34:55,450 --> 01:34:56,930
academic people

1465
01:34:56,950 --> 01:35:02,240
i came there and can from the practical world and then delivering to them in

1466
01:35:02,240 --> 01:35:06,380
a class on machine learning i told them right away well by the end of

1467
01:35:06,380 --> 01:35:09,750
the this class i hope that you know you can do consultant like me and

1468
01:35:09,750 --> 01:35:12,640
go unsolved problems in the outside world

1469
01:35:12,660 --> 01:35:22,240
so i told them also about how to write the proposal to get get contract

1470
01:35:23,010 --> 01:35:26,000
how to file for patents

1471
01:35:26,050 --> 01:35:28,970
to protect your intellectual property

1472
01:35:32,440 --> 01:35:39,440
other you know practical aspects of the of the work that they didn't

1473
01:35:39,490 --> 01:35:45,360
so this completes the biggest the curriculum and and i wanted to give you a

1474
01:35:45,360 --> 01:35:49,050
little glimpse of the dataset

1475
01:35:49,060 --> 01:35:52,660
that that that we used

1476
01:35:52,660 --> 01:35:58,160
there is a story about the massacre which is the computationally efficient way

1477
01:35:59,350 --> 01:36:02,540
represent functions in very high dimensional spaces

1478
01:36:02,580 --> 01:36:04,560
the to some extent

1479
01:36:04,640 --> 01:36:10,490
in the your minds just for you

1480
01:36:11,200 --> 01:36:13,540
up to this point

1481
01:36:13,550 --> 01:36:15,330
we have discussed

1482
01:36:15,350 --> 01:36:17,790
the classic or did that he

1483
01:36:17,830 --> 01:36:20,930
and the deep inference

1484
01:36:20,970 --> 01:36:23,520
and the classical pattern recognition problem

1485
01:36:23,530 --> 01:36:26,580
but all the questions we can ask we have asked

1486
01:36:26,630 --> 01:36:28,080
find the function

1487
01:36:28,280 --> 01:36:30,550
going to be the classification

1488
01:36:30,680 --> 01:36:33,760
if you in in real life you don't care so much about the function because

1489
01:36:33,760 --> 01:36:36,050
not going to classify everything

1490
01:36:36,060 --> 01:36:41,090
but there are many cases in which you can ask questions from all points

1491
01:36:41,100 --> 01:36:42,930
and the

1492
01:36:42,930 --> 01:36:46,530
one of these cases with called transduction

1493
01:36:49,470 --> 01:36:52,290
traditionally at least in the western world

1494
01:36:52,300 --> 01:36:53,550
when you make

1495
01:36:55,050 --> 01:36:57,910
you have two steps you start from that

1496
01:36:57,930 --> 01:37:01,300
and you have an induction step what you find the general rule

1497
01:37:01,390 --> 01:37:03,900
and when you have the general the theory

1498
01:37:03,910 --> 01:37:05,830
you apply that

1499
01:37:05,900 --> 01:37:07,450
applied to that

1500
01:37:07,450 --> 01:37:11,050
and then you make your decision to take an action

1501
01:37:11,090 --> 01:37:13,780
but why should we go through that

1502
01:37:13,780 --> 01:37:15,280
intermediate function

1503
01:37:15,290 --> 01:37:17,060
which is something that is

1504
01:37:18,100 --> 01:37:22,200
difficult to of ten one maybe sometimes you can go straight from the

1505
01:37:25,630 --> 01:37:27,930
let's set the poem formally

1506
01:37:27,940 --> 01:37:29,660
so you have training data

1507
01:37:29,720 --> 01:37:31,120
but then classes

1508
01:37:31,140 --> 01:37:34,830
the pattern recognition the class of plus and minus one

1509
01:37:34,880 --> 01:37:36,790
and you've given in advance

1510
01:37:36,790 --> 01:37:37,630
the test

1511
01:37:37,650 --> 01:37:39,840
so what happens

1512
01:37:39,890 --> 01:37:41,960
so you know

1513
01:37:41,960 --> 01:37:45,880
what questions you're going to ask what answer you want

1514
01:37:47,340 --> 01:37:51,090
so now among the admissible set the classification vehicles

1515
01:37:51,110 --> 01:37:56,000
so basically possibilities of answers your going to give you all the best this but

1516
01:37:56,000 --> 01:37:57,540
then this but

1517
01:37:57,550 --> 01:38:02,030
if you would like to find the best one

1518
01:38:02,080 --> 01:38:05,810
if you if you think about it that to think about important here

1519
01:38:05,860 --> 01:38:09,630
the first one is that you assume that when you're given the training data

1520
01:38:09,680 --> 01:38:12,930
you have a very precise idea about what questions to be asked

1521
01:38:12,940 --> 01:38:15,860
so that was the case but in many significant case

1522
01:38:15,910 --> 01:38:17,480
this is true

1523
01:38:17,520 --> 01:38:19,280
and the second one is that

1524
01:38:19,330 --> 01:38:21,760
well this is all the discrete

1525
01:38:21,760 --> 01:38:25,950
now we have is the longer one in three dimensional space or a continuous space

1526
01:38:25,950 --> 01:38:29,700
all in and this is the problem that purely combinatorial

1527
01:38:29,780 --> 01:38:31,290
you have

1528
01:38:31,360 --> 01:38:35,000
the set lasted for possible answers you could give

1529
01:38:35,030 --> 01:38:39,510
and because you have a limited number of there's the the set of possible and

1530
01:38:39,750 --> 01:38:41,730
is limited

1531
01:38:41,780 --> 01:38:44,030
and try to find the best one

1532
01:38:44,050 --> 01:38:47,840
so now becoming combinatorial

1533
01:38:47,890 --> 01:38:50,330
and in fact if you look at the prose

1534
01:38:50,350 --> 01:38:54,000
of the VC theory and this is the first part the called the proof is

1535
01:38:54,000 --> 01:38:59,110
purely combinatorial discussing properties here that are just as

1536
01:38:59,130 --> 01:39:04,050
as essential for something like the law of large numbers

1537
01:39:04,050 --> 01:39:09,180
so the first and the important concepts in the context so the first important concept

1538
01:39:09,180 --> 01:39:11,490
is the concept of equivalence classes

1539
01:39:11,500 --> 01:39:13,180
so here the red points

1540
01:39:13,200 --> 01:39:16,050
the red circle and the black lexical

1541
01:39:16,070 --> 01:39:18,790
there are two classes examples of the class

1542
01:39:18,870 --> 01:39:20,490
and the triangle

1543
01:39:20,570 --> 01:39:24,600
of those that have the ones we would like to classify

1544
01:39:24,620 --> 01:39:27,070
now if you consider all hyperplanes

1545
01:39:27,160 --> 01:39:28,560
you don't say that

1546
01:39:28,640 --> 01:39:32,180
some plants like all these here

1547
01:39:32,230 --> 01:39:33,880
i going to classify

1548
01:39:33,900 --> 01:39:36,800
all these formed the same way

1549
01:39:36,850 --> 01:39:39,130
so for whatever

1550
01:39:39,180 --> 01:39:43,390
for the purpose of this financial problems the equivalent

1551
01:39:43,440 --> 01:39:47,150
so for this hyperplanes here between these are going to

1552
01:39:47,310 --> 01:39:51,180
with this two one thread all these red and or this one black

1553
01:39:51,220 --> 01:39:56,410
and we're gonna see whether you think this one or this one or anyone there

1554
01:39:56,460 --> 01:39:59,580
they're going to produce the same output

1555
01:39:59,640 --> 01:40:02,950
so somehow the size of this equivalence classes

1556
01:40:02,990 --> 01:40:07,080
which is you see in particular is related the margin of the more margin there

1557
01:40:07,080 --> 01:40:11,230
are between the point the more you will be able to fit hyperplanes

1558
01:40:11,240 --> 01:40:13,890
the size of the equivalent class

1559
01:40:14,000 --> 01:40:15,880
is a critical factor

1560
01:40:15,900 --> 01:40:21,710
in the kind of generalization ability we going to happen

1561
01:40:22,780 --> 01:40:26,910
so let me often gives this example which is from the KDD cup and the

1562
01:40:26,910 --> 01:40:28,350
problem was to

1563
01:40:29,800 --> 01:40:31,010
you had the

1564
01:40:31,910 --> 01:40:36,860
part and binary patterns dimension one hundred forty thousand

1565
01:40:36,910 --> 01:40:41,020
with the training set of only two two thousand example

1566
01:40:41,020 --> 01:40:45,470
but this set has only one of few examples

1567
01:40:45,490 --> 01:40:47,730
during that competition

1568
01:40:47,780 --> 01:40:52,100
there was the distribution of of was about these and the we was about

1569
01:40:52,230 --> 01:40:55,870
but think sixty nine something like this

1570
01:40:56,960 --> 01:40:57,780
if few

1571
01:40:57,790 --> 01:41:00,850
maybe one or two years later jason weston

1572
01:41:00,870 --> 01:41:04,040
right so as we end with four smart encoding so

1573
01:41:04,080 --> 01:41:07,750
it's not is of the pure result at this point and gotten much nicer result

1574
01:41:07,800 --> 01:41:11,220
was seventy five percent correct

1575
01:41:11,280 --> 01:41:13,380
by using exactly the same thing

1576
01:41:13,400 --> 01:41:15,700
but being in front the the model that is

1577
01:41:15,700 --> 01:41:17,710
making your system

1578
01:41:17,760 --> 01:41:19,290
that would not only

1579
01:41:19,320 --> 01:41:24,590
use the training but in the training level but also the test but down but

1580
01:41:24,590 --> 01:41:26,510
of course not this label

1581
01:41:26,550 --> 01:41:33,290
he was able to go to eighty two percent and that is very significant

1582
01:41:35,530 --> 01:41:36,820
so that was the funds of the

1583
01:41:36,840 --> 01:41:38,770
fans of the four

1584
01:41:38,770 --> 01:41:40,550
to follow that object

1585
01:41:40,570 --> 01:41:43,520
i should say by the way the people who do computer vision have a variety

1586
01:41:43,520 --> 01:41:48,720
of motivations for this there are those who see it as as

1587
01:41:48,780 --> 01:41:51,820
looking at the mystery of how is it the human c i then we have

1588
01:41:51,820 --> 01:41:57,650
this wonderful living existence proof of humans walking around the world making a great deal

1589
01:41:57,650 --> 01:42:00,200
of this is your data and so on

1590
01:42:00,360 --> 01:42:03,740
we can be completely confident that is possible vision is possible we're all the living

1591
01:42:03,740 --> 01:42:07,780
proof of this but how can it be done and some people want to answer

1592
01:42:07,780 --> 01:42:11,780
this in a very biological kind of way and look for mechanisms in the brain

1593
01:42:11,780 --> 01:42:13,650
that could map on two

1594
01:42:13,730 --> 01:42:19,110
is your processes and then there are psychologists call themselves like physicists to take the

1595
01:42:19,110 --> 01:42:23,690
brain is a black box and use psychological experiments to see

1596
01:42:23,700 --> 01:42:25,510
how you respond to particular

1597
01:42:25,520 --> 01:42:28,120
patterns may be the most famous

1598
01:42:28,140 --> 01:42:34,780
those are better utilisation his random dot stereograms which are the precursor of those auto

1599
01:42:34,780 --> 01:42:39,030
stereo grand christmas cards were very popular ten years ago when you have this sort

1600
01:42:39,030 --> 01:42:43,890
of with the cardinals something emerges and that you cross your eyes a distinctly uncomfortable

1601
01:42:46,370 --> 01:42:51,410
and then the other end of the spectrum there are people who want to make

1602
01:42:51,410 --> 01:42:53,970
things make inventions that

1603
01:42:54,020 --> 01:42:59,630
use the kind of imitating in in a loose way human vision but you know

1604
01:42:59,630 --> 01:43:03,790
would be wonderful if we could make certain machines that react to visual input in

1605
01:43:03,790 --> 01:43:08,590
the same way and as i think you were saying cameras at two dollars are

1606
01:43:08,600 --> 01:43:12,140
thrown out so i it's used to be the case that you have to apologize

1607
01:43:12,140 --> 01:43:16,250
for making machines which had cameras in people would say well you know why don't

1608
01:43:16,280 --> 01:43:21,170
use an accelerometer what user touch sensor you know other senses the perceived as being

1609
01:43:21,170 --> 01:43:24,290
cheap and that's no longer the case the camera is now pretty much as cheap

1610
01:43:24,290 --> 01:43:27,660
as any other senses so we anything we can do with vision

1611
01:43:30,340 --> 01:43:35,820
i think the problem of yes the telling when people falling asleep in their cars

1612
01:43:35,980 --> 01:43:38,740
was mentioned well that's no longer

1613
01:43:38,750 --> 01:43:43,430
something that you can only think of as exotic that i'm interested in a number

1614
01:43:43,430 --> 01:43:47,600
of different kinds of inventions and applications

1615
01:43:47,610 --> 01:43:53,880
here's one idea which a colleague of mine in microsoft in redmond was looking at

1616
01:43:53,930 --> 01:43:55,210
the idea that

1617
01:43:55,260 --> 01:43:59,280
using the camera on your mobile phone actually have to be pointing the other way

1618
01:43:59,410 --> 01:44:04,830
this camera for this particular idea to work usually point backwards needed to point forward

1619
01:44:05,130 --> 01:44:09,130
could you have a program in the mobile phone that looked at you speaking and

1620
01:44:09,130 --> 01:44:13,160
was able to pick up enough of the variation of your expressions and so on

1621
01:44:13,250 --> 01:44:15,140
to transmit two

1622
01:44:16,480 --> 01:44:18,210
the other end of the conversation

1623
01:44:18,230 --> 01:44:21,070
very dense code about the

1624
01:44:21,080 --> 01:44:26,940
the relate your expression so that at the far end you could animate some

1625
01:44:26,950 --> 01:44:30,300
cartoon or it could be a replica of yourself

1626
01:44:31,320 --> 01:44:34,190
this is the sort of thing i mean the lady on the left is real

1627
01:44:34,190 --> 01:44:38,180
by the way and

1628
01:44:38,910 --> 01:44:42,170
her emotions are being picked up using computer vision

1629
01:44:42,190 --> 01:44:47,600
and so on ad hoc methods in computer vision that have specific feature detectors from

1630
01:44:47,600 --> 01:44:51,010
mouth and eyebrows and so on because we'd like to be able to do these

1631
01:44:51,010 --> 01:44:54,140
things in in a systematic way

1632
01:44:54,160 --> 01:44:57,840
it's quite cute isn't talking to the cartoon on the right is actually in many

1633
01:44:57,840 --> 01:45:01,450
ways far more acceptable to do this than to talk to

1634
01:45:01,500 --> 01:45:07,720
a realistic replicas so there's been a lot of progress actually on

1635
01:45:07,770 --> 01:45:12,240
face painting by numbers in about two hundred number seems to be what what you

1636
01:45:12,240 --> 01:45:18,040
need to code up a realistic face so you could actually take

1637
01:45:18,050 --> 01:45:19,130
take the numbers

1638
01:45:19,140 --> 01:45:23,360
calculated from the moving picture on the left and send two hundred and numbers per

1639
01:45:23,360 --> 01:45:28,250
video frame down the wire use them to reanimate sort of photo realistic

1640
01:45:28,280 --> 01:45:33,710
face but we're very sensitive to very to slight artifacts in reconstructions and even though

1641
01:45:33,710 --> 01:45:37,400
they are very good now i have the feeling that possibly people might find it

1642
01:45:37,400 --> 01:45:41,750
more acceptable to be looking at something like this where there is no pretense that

1643
01:45:42,030 --> 01:45:47,310
favour realism and yet there's a lot of emotional content

1644
01:45:47,330 --> 01:45:51,080
another idea that's been around for awhile popularized by xerox

1645
01:45:51,090 --> 01:45:53,740
is the idea that you could have

1646
01:45:53,740 --> 01:45:59,220
result is always the same statement that is if the restricted isometry property holds which

1647
01:45:59,220 --> 01:46:04,880
i even need in the noiseless case what if it holds then surprisingly

1648
01:46:06,800 --> 01:46:08,610
this is the solution to this

1649
01:46:08,610 --> 01:46:10,400
the linear programs

1650
01:46:10,410 --> 01:46:13,010
it is easy to solve on the computer

1651
01:46:13,040 --> 01:46:18,150
it is very accurate and how accurate this is what the cheese

1652
01:46:18,160 --> 01:46:22,970
the are called bound is it achieves its adaptive this errors it's mean squared error

1653
01:46:22,970 --> 01:46:27,330
is proportional to the number of coefficients there is times the knowledge level

1654
01:46:28,670 --> 01:46:31,710
a factor which is proportional to the

1655
01:46:31,740 --> 01:46:36,680
logarithmic of the total number of dimensions you have to pay

1656
01:46:36,690 --> 01:46:39,430
and so when you solve this kind of

1657
01:46:39,670 --> 01:46:42,810
this kind of data fitting problem by this

1658
01:46:42,830 --> 01:46:47,450
the dynamics selection you have i mean squared error the index indicates the stochastic noise

1659
01:46:47,450 --> 01:46:49,400
exactly proportional

1660
01:46:49,410 --> 01:46:54,740
so is it true unknown number of parameters achieve what we call statistics that activity

1661
01:46:55,160 --> 01:46:59,210
is when the object gets simpers i mean squared error decreases

1662
01:46:59,240 --> 01:47:03,430
and then in the constant you pay log factors that you have to pay

1663
01:47:03,440 --> 01:47:04,730
no matter what

1664
01:47:04,760 --> 01:47:07,650
and that's the price you need to pay for not knowing

1665
01:47:07,650 --> 01:47:10,740
that's the price you pay compared to zero articles at the price you pay for

1666
01:47:10,740 --> 01:47:15,510
not knowing ahead of time or causes significant cult i have to search for them

1667
01:47:15,520 --> 01:47:17,850
and you have to pay a price for it

1668
01:47:17,860 --> 01:47:19,290
and that's log p

1669
01:47:19,480 --> 01:47:23,450
this is to ask you can show that no estimator whatsoever

1670
01:47:23,470 --> 01:47:27,850
can actually do better than p times in six months so you really at the

1671
01:47:27,850 --> 01:47:31,240
limit of what you can do

1672
01:47:31,290 --> 01:47:37,590
OK alright so in fact you can you get really small error is as good

1673
01:47:37,590 --> 01:47:38,300
as if

1674
01:47:38,410 --> 01:47:43,280
you know ahead of time which components are big and which components are small

1675
01:47:43,310 --> 01:47:46,420
and that means that the

1676
01:47:46,540 --> 01:47:48,960
you can really around these things in practice

1677
01:47:48,990 --> 01:47:52,950
now there are all kinds of articles that you are more sophisticated than one i've

1678
01:47:54,450 --> 01:48:00,420
presented that goes back to the whole literature model selection statistics where you can actually

1679
01:48:01,210 --> 01:48:03,760
much more precise or calls

1680
01:48:03,780 --> 01:48:06,880
but since i'm running out of time i'll skip this

1681
01:48:06,890 --> 01:48:10,110
but you can only get what you would get by meaning by

1682
01:48:12,850 --> 01:48:17,790
which coordinators you're which are not but you could actually by having even more so

1683
01:48:17,790 --> 01:48:21,720
powerful are called to tell you in the case according to is not deal

1684
01:48:21,740 --> 01:48:23,940
it was a towards estimating unknown

1685
01:48:23,950 --> 01:48:25,970
and you could mimic there

1686
01:48:26,220 --> 01:48:31,760
performances are calls as well you can get very sharp bounds which are

1687
01:48:35,110 --> 01:48:37,300
OK so there is no

1688
01:48:37,330 --> 01:48:42,470
a lot of theory and statistics about an one minimisation these days so for example

1689
01:48:42,470 --> 01:48:47,220
i've shown you results for the lasso but these results inspired statistician to actually pull

1690
01:48:47,230 --> 01:48:52,930
similar results i should be reserved for the dantzig selector but is inspired statisticians to

1691
01:48:52,930 --> 01:48:54,860
prove similar result for the lasso

1692
01:48:54,870 --> 01:48:59,710
and that this was achieved by peter be calling his colleagues by tom gen

1693
01:49:00,080 --> 01:49:01,900
shortly after

1694
01:49:01,910 --> 01:49:04,990
and then in the statistical world it seemed that

1695
01:49:05,020 --> 01:49:08,850
you know l one minimisation in connection with model selection

1696
01:49:08,850 --> 01:49:14,150
it is getting a lot of attention it's very active community of britain researchers managed

1697
01:49:14,150 --> 01:49:18,580
to get new and exciting results on almost on a weekly basis at the moment

1698
01:49:18,720 --> 01:49:24,580
OK so there's a lot of and perhaps could shinseki talked about some of these

1699
01:49:26,130 --> 01:49:28,130
last week

1700
01:49:29,840 --> 01:49:33,540
OK so it's connected with model selection and so on and so on so in

1701
01:49:33,540 --> 01:49:36,020
practice the dantzig selector works

1702
01:49:37,160 --> 01:49:38,660
really well so

1703
01:49:38,670 --> 01:49:40,430
if i have

1704
01:49:41,630 --> 01:49:47,630
of size speed close to fifty six seventy two measurements gas in matrix noise level

1705
01:49:47,630 --> 01:49:52,860
sigma which is point eleven i choose the threshold to be about three point five

1706
01:49:52,860 --> 01:49:54,270
so that

1707
01:49:54,300 --> 01:49:58,690
so a transpose or is lesson points thirty nine

1708
01:49:58,690 --> 01:50:01,000
when i look at the

1709
01:50:01,030 --> 01:50:06,400
the selection offered by the dantzig selector in blue you have the original factory red

1710
01:50:06,400 --> 01:50:07,990
you have the reconstruction

1711
01:50:08,000 --> 01:50:12,020
and you see two things first of all each time the signal is not always

1712
01:50:12,020 --> 01:50:16,230
the dantzig selector is actually correctly identifying the not zero

1713
01:50:16,250 --> 01:50:21,080
and whenever he was dantzig selector correctly identifies its

1714
01:50:21,080 --> 01:50:24,480
but what you can see also is that there is a little bit of

1715
01:50:24,500 --> 01:50:27,670
of the bias that is dantzig selector since two

1716
01:50:27,690 --> 01:50:32,600
bring observations back towards the there's shrinkage effect which is due to his l one

1717
01:50:32,600 --> 01:50:36,950
norm so it's a bit like a soft thresholding type estimates

1718
01:50:36,970 --> 01:50:42,330
in fact if a is orthogonal was then it is exactly soft thresholding

1719
01:50:42,410 --> 01:50:45,260
and so there's something so there's a little bias

1720
01:50:45,280 --> 01:50:49,120
that it is introduced by you want to shrink down the amplitude somehow

1721
01:50:49,130 --> 01:50:52,210
and so there's a little bias that you can correct by

1722
01:50:52,230 --> 01:50:57,720
having this two stage procedure where you rendered the danzig selection to find out those

1723
01:50:57,720 --> 01:51:01,640
components in the genome say that have

1724
01:51:01,870 --> 01:51:07,250
the predicted response predicted value which is not the and then you get your sparse

1725
01:51:07,250 --> 01:51:10,510
subset the new performance queries on

1726
01:51:10,600 --> 01:51:12,660
so when you do this

1727
01:51:12,670 --> 01:51:17,580
you see that you receptors your observation you can eliminate this bias now

1728
01:51:17,590 --> 01:51:21,700
so you have a two stage procedure where you find the location that are not

1729
01:51:21,700 --> 01:51:24,870
zero by znd selection and then you performance was

1730
01:51:24,880 --> 01:51:28,540
and that would remove the

1731
01:51:28,560 --> 01:51:33,280
OK so in practice it works extremely well in fact if we were to compare

