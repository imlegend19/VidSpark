1
00:00:00,000 --> 00:00:05,660
it was

2
00:00:14,000 --> 00:00:17,410
you can

3
00:00:29,540 --> 00:00:32,350
it's not for

4
00:00:50,500 --> 00:00:53,570
he or she

5
00:00:53,600 --> 00:00:56,620
in the early nineties you

6
00:01:03,390 --> 00:01:06,230
i only

7
00:01:06,250 --> 00:01:08,570
i q

8
00:01:08,600 --> 00:01:13,300
there you where one

9
00:01:26,590 --> 00:01:33,850
now i will

10
00:02:16,620 --> 00:02:18,910
all right

11
00:02:29,970 --> 00:02:31,910
yes your

12
00:02:32,240 --> 00:02:36,180
you call it

13
00:03:00,760 --> 00:03:03,000
you always

14
00:03:16,290 --> 00:03:19,790
per year

15
00:03:22,450 --> 00:03:24,620
the two

16
00:03:27,350 --> 00:03:28,810
all right

17
00:03:36,290 --> 00:03:37,970
i mean

18
00:03:44,200 --> 00:03:48,430
he variety

19
00:04:03,200 --> 00:04:05,430
you all

20
00:04:25,520 --> 00:04:32,740
it seems that i

21
00:04:41,290 --> 00:04:43,160
in the world

22
00:04:45,810 --> 00:04:50,370
you i

23
00:04:50,390 --> 00:04:52,240
the area

24
00:04:52,270 --> 00:04:55,700
you have to be very old

25
00:04:55,700 --> 00:04:59,450
were already

26
00:05:05,450 --> 00:05:06,640
three he

27
00:05:07,020 --> 00:05:10,430
he who are

28
00:05:39,910 --> 00:05:42,790
and then query

29
00:05:45,330 --> 00:05:48,700
all you know

30
00:05:50,890 --> 00:06:02,490
we want

31
00:06:05,910 --> 00:06:08,770
well for

32
00:06:08,790 --> 00:06:11,410
or or

33
00:06:11,430 --> 00:06:13,740
a of

34
00:06:13,760 --> 00:06:16,080
so are

35
00:06:31,950 --> 00:06:38,160
the last of

36
00:06:43,670 --> 00:06:46,310
you all

37
00:06:46,310 --> 00:06:49,160
in own

38
00:07:00,140 --> 00:07:10,200
and in

39
00:07:10,200 --> 00:07:15,950
if i have here a traveling wave

40
00:07:16,010 --> 00:07:18,900
which is moving in this direction

41
00:07:18,950 --> 00:07:20,720
as tension t

42
00:07:20,720 --> 00:07:23,880
mu is the mass per unit length

43
00:07:23,880 --> 00:07:25,720
that i can write down

44
00:07:26,970 --> 00:07:28,570
as a function

45
00:07:28,760 --> 00:07:30,530
x and t

46
00:07:30,530 --> 00:07:34,130
i can give it a certain amplitude a

47
00:07:34,280 --> 00:07:36,820
by signed

48
00:07:36,840 --> 00:07:41,010
i can write is now in many different ways but let's write it down

49
00:07:41,010 --> 00:07:42,110
this form

50
00:07:43,530 --> 00:07:45,010
i x

51
00:07:47,070 --> 00:07:51,130
you know this

52
00:07:51,150 --> 00:07:52,010
that the

53
00:07:53,070 --> 00:07:54,740
it's call it squared

54
00:07:55,610 --> 00:07:58,320
provided by new

55
00:07:58,340 --> 00:08:00,970
so this is the traveling wave goes into the plus

56
00:08:01,010 --> 00:08:02,720
x direction

57
00:08:02,820 --> 00:08:06,880
is anti-matter moving in the plus x direction no

58
00:08:06,900 --> 00:08:09,450
but is anything moving yes

59
00:08:09,470 --> 00:08:11,630
it is moving in the y direction

60
00:08:11,680 --> 00:08:14,260
so since these particles

61
00:08:14,280 --> 00:08:15,380
i mass

62
00:08:15,380 --> 00:08:17,180
and they moving in this direction

63
00:08:17,200 --> 00:08:19,380
there is kinetic energy

64
00:08:19,400 --> 00:08:22,780
due to the motion in this direction not to

65
00:08:23,470 --> 00:08:27,260
mass that moves along no mass moves in that direction

66
00:08:27,300 --> 00:08:30,030
of here

67
00:08:30,070 --> 00:08:32,570
suppose i carve out here

68
00:08:35,840 --> 00:08:37,630
this is the direction of x

69
00:08:37,650 --> 00:08:40,470
take a small section dx

70
00:08:40,490 --> 00:08:41,800
then the

71
00:08:41,820 --> 00:08:43,570
kinetic energy

72
00:08:45,760 --> 00:08:49,280
the economic

73
00:08:49,320 --> 00:08:52,590
find a little bit of kinetic energy

74
00:08:52,650 --> 00:08:54,470
is one half

75
00:08:54,490 --> 00:08:57,150
times the mass of the atom

76
00:08:57,150 --> 00:09:01,530
times the velocity in the y direction square there is a one right kinetic energy

77
00:09:01,530 --> 00:09:03,380
is one half squared

78
00:09:03,430 --> 00:09:05,380
never confuse this v

79
00:09:05,410 --> 00:09:06,910
with that we

80
00:09:06,950 --> 00:09:08,490
that is the

81
00:09:08,550 --> 00:09:10,490
velocity of propagation

82
00:09:10,510 --> 00:09:15,860
this is the velocity of the string in this direction

83
00:09:15,900 --> 00:09:18,530
so i can write this down

84
00:09:18,590 --> 00:09:22,990
b and is obviously new times dx if i have length dx

85
00:09:23,050 --> 00:09:24,490
and i haven't you

86
00:09:24,530 --> 00:09:25,930
kilograms per meter

87
00:09:25,950 --> 00:09:28,550
so i get here one half

88
00:09:29,630 --> 00:09:31,010
i'm px

89
00:09:31,010 --> 00:09:31,990
and for this

90
00:09:31,990 --> 00:09:33,110
i write down

91
00:09:33,150 --> 00:09:35,450
why did he quit

92
00:09:35,450 --> 00:09:36,650
the velocity

93
00:09:36,760 --> 00:09:41,820
why reactions and i use partial derivatives because i do it at a given location

94
00:09:41,840 --> 00:09:44,380
four x

95
00:09:44,470 --> 00:09:47,550
so what is the t

96
00:09:50,490 --> 00:09:52,400
that's easy that's a piece of cake

97
00:09:52,470 --> 00:09:54,070
there's my functions

98
00:09:54,090 --> 00:09:56,800
so i get in a

99
00:09:56,860 --> 00:10:00,180
then i get k n minus the

100
00:10:00,220 --> 00:10:01,740
so i get the minus

101
00:10:03,170 --> 00:10:04,380
and if he

102
00:10:04,430 --> 00:10:07,110
and then the sign becomes co-signed

103
00:10:07,150 --> 00:10:08,910
so i get a

104
00:10:08,930 --> 00:10:10,260
and x

105
00:10:10,320 --> 00:10:15,470
lines fifty

106
00:10:15,530 --> 00:10:19,110
should put brackets around two that self explain

107
00:10:19,110 --> 00:10:23,360
and now i want to know what the creator of this is

108
00:10:23,780 --> 00:10:27,590
his credits

109
00:10:27,610 --> 00:10:33,320
now i can calculate the total energy is kinetic energy in one wavelength

110
00:10:33,380 --> 00:10:35,700
i'll have to do is integrate now

111
00:10:35,740 --> 00:10:40,470
from zero to allow them to get the total energy in one way

112
00:10:40,490 --> 00:10:43,240
so i'm going to do that right down here now

113
00:10:45,590 --> 00:10:48,930
following closely i have half

114
00:10:52,780 --> 00:10:53,990
then i get the

115
00:10:54,010 --> 00:10:57,150
BYD t square again a square

116
00:10:57,320 --> 00:11:00,930
i got case great

117
00:11:01,070 --> 00:11:03,260
i get these grants

118
00:11:03,320 --> 00:11:07,720
then i got into rule from zero whether of this function

119
00:11:11,610 --> 00:11:13,050
why we teach

120
00:11:13,110 --> 00:11:15,930
and then i have no idea x which is this the axon

121
00:11:15,930 --> 00:11:20,740
that's what the integral is the direct effect

122
00:11:20,740 --> 00:11:24,550
thank you very much

123
00:11:24,880 --> 00:11:27,430
course credit

124
00:11:27,450 --> 00:11:29,570
what is this integral

125
00:11:29,800 --> 00:11:33,860
maybe you don't remember but i've done is often enough that i don't know

126
00:11:33,910 --> 00:11:36,650
in the goal of cosine square of that function

127
00:11:36,650 --> 00:11:37,780
is land

128
00:11:37,780 --> 00:11:41,110
divided by two and i'll leave you with that it's very easy

129
00:11:42,990 --> 00:11:44,410
i can also

130
00:11:44,430 --> 00:11:49,590
take this out and right for that t overview

131
00:11:49,610 --> 00:11:51,200
so you can add it now

132
00:11:51,260 --> 00:11:57,340
and this is in one way flight is my shorthand notation all i'm calculating for

133
00:11:57,340 --> 00:12:01,130
you now is how much is in one wavelength

134
00:12:01,150 --> 00:12:02,820
so you're going to get now

135
00:12:02,880 --> 00:12:04,200
one have mu

136
00:12:04,320 --> 00:12:08,740
a script

137
00:12:08,740 --> 00:12:11,150
the case grew i can write down

138
00:12:11,200 --> 00:12:12,740
four by

139
00:12:12,780 --> 00:12:15,450
divided by let that square

140
00:12:15,490 --> 00:12:17,800
for the square i can write down

141
00:12:18,680 --> 00:12:20,400
divided by new

142
00:12:20,410 --> 00:12:22,280
and then i have my lab

143
00:12:22,300 --> 00:12:24,930
of the two

144
00:12:24,970 --> 00:12:26,110
well this new

145
00:12:26,170 --> 00:12:27,720
girls this new

146
00:12:27,760 --> 00:12:28,510
this too

147
00:12:28,530 --> 00:12:30,550
this for this too

148
00:12:30,590 --> 00:12:33,130
in one that kills one year

149
00:12:33,260 --> 00:12:35,260
now we have the final result

150
00:12:35,280 --> 00:12:37,240
that he

151
00:12:38,990 --> 00:12:42,200
in one wavelength

152
00:12:44,260 --> 00:12:46,260
a script

153
00:12:46,630 --> 00:12:48,070
by script

154
00:12:48,110 --> 00:12:49,320
time t

155
00:12:49,360 --> 00:12:50,590
five by land

156
00:12:52,880 --> 00:12:55,990
if you look at this

157
00:12:56,030 --> 00:12:58,220
you ask me is that obvious

158
00:12:58,220 --> 00:12:58,990
i would say

159
00:12:59,030 --> 00:13:01,570
what do we

160
00:13:01,630 --> 00:13:03,880
there's team that that

161
00:13:03,880 --> 00:13:05,590
my goodness and i can also

162
00:13:05,610 --> 00:13:09,990
cocktail the whole thing i can get the out and get feedback in again

163
00:13:10,050 --> 00:13:11,450
i will admit

164
00:13:11,680 --> 00:13:13,700
i don't have very good feeling

165
00:13:13,700 --> 00:13:16,860
for this function except for one

166
00:13:16,900 --> 00:13:18,450
i don't know

167
00:13:18,490 --> 00:13:22,470
always the energy in the wave is always proportional

168
00:13:22,510 --> 00:13:25,930
so the amplitude square you going to see that when we do

169
00:13:26,030 --> 00:13:30,610
electromagnetic waves in eight o three is always proportional to a square so that's the

170
00:13:30,610 --> 00:13:35,780
only one for which i may have a feeling i know don't be there

171
00:13:35,840 --> 00:13:36,900
and the rest

172
00:13:36,970 --> 00:13:41,510
i leave you with that to see whether perhaps you can talk yourself into understanding

173
00:13:41,570 --> 00:13:43,180
why you see the symbols

174
00:13:43,200 --> 00:13:47,570
where they are

175
00:13:47,610 --> 00:13:49,070
now clearly

176
00:13:49,110 --> 00:13:51,360
there is also potential energy

177
00:13:51,360 --> 00:13:52,950
because it takes energy

178
00:13:53,010 --> 00:13:55,260
was to make that

179
00:13:55,340 --> 00:13:59,050
straight line into curve

180
00:13:59,070 --> 00:14:02,630
and that means work that you have to do

181
00:14:02,740 --> 00:14:05,220
you have to squeeze

182
00:14:05,240 --> 00:14:08,720
two stretch its there's attention you have to stretch the

183
00:14:08,720 --> 00:14:10,180
so we have to do

184
00:14:10,260 --> 00:14:12,760
work to just get the shape

185
00:14:12,800 --> 00:14:14,780
and that is potential energy

186
00:14:14,800 --> 00:14:16,550
and the potential energy

187
00:14:16,590 --> 00:14:18,400
four waif-like

188
00:14:19,220 --> 00:14:22,880
i want you to do on their own it's worked out nicely in french

189
00:14:22,900 --> 00:14:24,650
i will not do it today

190
00:14:24,720 --> 00:14:25,950
happens to me

191
00:14:25,970 --> 00:14:27,720
exactly the same

192
00:14:27,740 --> 00:14:31,570
as the kinetic energy which is by no means obvious

193
00:14:31,570 --> 00:14:34,590
how that's how that is possible

194
00:14:34,600 --> 00:14:37,870
imagine that i have two subspaces

195
00:14:37,890 --> 00:14:42,450
here i'm planning to lies in three but what it really means to subspaces of

196
00:14:42,470 --> 00:14:47,600
dimension in know an in space that could be a very very high damage

197
00:14:48,800 --> 00:14:50,400
well you can take

198
00:14:50,420 --> 00:14:54,640
those two lines projected into a plane

199
00:14:54,650 --> 00:14:59,320
in the number of groups is preserved

200
00:14:59,370 --> 00:15:02,250
and the dimensions of the substances

201
00:15:02,270 --> 00:15:03,870
are preserved

202
00:15:03,880 --> 00:15:08,600
and therefore you don't need to solve the clustering problem in the original space

203
00:15:08,620 --> 00:15:12,110
you can solve it on the projected space

204
00:15:12,970 --> 00:15:17,560
the dimension of the subspace in which you need to protect on two is whatever

205
00:15:17,570 --> 00:15:22,990
the maximum dimensional subspaces plus one

206
00:15:23,050 --> 00:15:28,770
and a few feet this robust you can also of this is this affair

207
00:15:28,840 --> 00:15:32,050
essentially you can pick prediction around the

208
00:15:32,070 --> 00:15:33,610
and it will work

209
00:15:33,630 --> 00:15:36,070
of course there are some that failed

210
00:15:36,300 --> 00:15:39,920
for instance in this case of two lines if you pick a plane that is

211
00:15:39,920 --> 00:15:43,120
perpendicular to one of the lines the lines becomes the origin

212
00:15:43,140 --> 00:15:44,920
and so they can disappear

213
00:15:44,940 --> 00:15:50,150
but the center of projection but failed is enormous that in it so i random

214
00:15:50,150 --> 00:15:52,750
projection do but of course you know

215
00:15:52,790 --> 00:15:58,720
different projections will give affect the quality of the clustering and an open-plan room here

216
00:15:58,720 --> 00:16:03,770
really is how to choose a good prediction and that that's something where

217
00:16:04,820 --> 00:16:08,720
but for most of the applications we just using PCA

218
00:16:08,740 --> 00:16:13,140
and it seems to be working in various application but i'm not saying that is

219
00:16:13,140 --> 00:16:16,190
necessarily the right thing to do

220
00:16:17,540 --> 00:16:21,770
so this is a summary of the algorithm

221
00:16:21,780 --> 00:16:25,950
this is what it is you take your other points

222
00:16:25,970 --> 00:16:27,440
you embed it

223
00:16:27,470 --> 00:16:30,200
with polynomial embedding

224
00:16:30,220 --> 00:16:33,110
to form what i call the embedded data

225
00:16:34,410 --> 00:16:39,030
useful for the null space of this matrix to get polynomials

226
00:16:39,050 --> 00:16:43,040
and here is i'm striking here all the set of problems that i could potentially

227
00:16:43,040 --> 00:16:44,460
get from the null space

228
00:16:44,470 --> 00:16:48,270
once i

229
00:16:48,290 --> 00:16:52,700
have computer polynomials i take derivatives and that gives me

230
00:16:52,750 --> 00:16:55,920
the basis for the normal to subspace

231
00:16:55,960 --> 00:16:59,200
and then the dimensions of subspace is just

232
00:16:59,250 --> 00:17:02,670
they and the mention of them in space minus the rank of all the possible

233
00:17:04,820 --> 00:17:08,850
and then there is an optimal way of choosing the points where to avoid this

234
00:17:08,850 --> 00:17:15,070
derivatives by a certain distance function this is the distance function before it was the

235
00:17:15,070 --> 00:17:19,540
ratio of the polynomial by the derivative here's some more complicated formula because here i'm

236
00:17:19,540 --> 00:17:22,990
using multiple opponents at the same time but i i think you get the intrusion

237
00:17:22,990 --> 00:17:32,430
there is roughly speaking the ratio of the value of the polynomial by the truth

238
00:17:32,900 --> 00:17:36,120
connection with spectral clustering

239
00:17:37,550 --> 00:17:42,950
how many of you are not familiar with spectral clustering

240
00:17:44,330 --> 00:17:46,050
about thirty percent

241
00:17:46,150 --> 00:17:48,590
so spectral clustering

242
00:17:48,950 --> 00:17:52,050
this another technique for doing clustering

243
00:17:52,060 --> 00:17:55,560
which is based on the following idea

244
00:17:55,580 --> 00:17:56,800
imagine that

245
00:17:56,810 --> 00:17:58,070
for every

246
00:17:58,080 --> 00:17:59,760
a pair of points

247
00:17:59,780 --> 00:18:05,860
i could build some sort of measure of similarity that tells me how close

248
00:18:05,880 --> 00:18:09,680
four house so how similar these two points

249
00:18:09,690 --> 00:18:14,840
and the idea is that if two points belong to the same

250
00:18:14,860 --> 00:18:17,170
the similarity should be one

251
00:18:17,180 --> 00:18:22,120
any if two points belong to different groups dissimilarity should be easy

252
00:18:22,140 --> 00:18:26,880
so in the standard clustering problems for instance where i have data that moves around

253
00:18:26,880 --> 00:18:28,330
the cluster centers

254
00:18:28,340 --> 00:18:32,640
i could take the distance to the euclidean distance between points

255
00:18:32,660 --> 00:18:36,380
right they being handled pointer

256
00:18:36,400 --> 00:18:37,150
close by

257
00:18:37,200 --> 00:18:38,590
have small distance

258
00:18:38,610 --> 00:18:42,570
so they belong to the same points that are far apart how large this so

259
00:18:42,570 --> 00:18:47,840
you just been into the minds of the distance that will give you some measure

260
00:18:48,060 --> 00:18:51,300
so has been the question of how could apply

261
00:18:52,180 --> 00:18:53,930
technique to

262
00:18:53,950 --> 00:18:55,760
clustering subspaces

263
00:18:55,880 --> 00:18:58,510
and the key

264
00:18:58,530 --> 00:19:01,850
problem had been that it was very difficult to define

265
00:19:01,870 --> 00:19:04,790
a similarity measure for instance

266
00:19:04,800 --> 00:19:09,810
these points right here or

267
00:19:09,810 --> 00:19:14,230
who thinks is going to be between seventy and eighty percent

268
00:19:14,310 --> 00:19:17,580
OK about three four people who you think that is going to be more than

269
00:19:17,580 --> 00:19:21,580
eighty percent

270
00:19:21,600 --> 00:19:25,340
nobody that's good you're going the right direction

271
00:19:25,350 --> 00:19:27,060
OK so i see you all

272
00:19:27,070 --> 00:19:30,290
not all but i see some of you holding pens and sort of itching to

273
00:19:30,290 --> 00:19:34,480
start writing some some equations on so let's it together

274
00:19:34,490 --> 00:19:36,580
OK as

275
00:19:36,600 --> 00:19:40,410
steve gunn is very illustrious bayesian would say

276
00:19:40,420 --> 00:19:43,430
you have to always write down the probabilities of everything

277
00:19:43,450 --> 00:19:48,290
OK so so let's let's give let's give names to things

278
00:19:48,310 --> 00:19:49,500
let's call c

279
00:19:49,500 --> 00:19:53,280
so actually you see nom already doing this thing of skipping the

280
00:19:53,290 --> 00:19:57,010
the proper procedure if i were to do the proper first procedure i would define

281
00:19:57,010 --> 00:20:01,390
the random variable called small see for example OK and then you can take the

282
00:20:01,390 --> 00:20:05,650
value either big c or c bar OK that's the alphabet but in some sloppy

283
00:20:05,650 --> 00:20:10,170
i just skip that step and go directly to the alphabet right now we call

284
00:20:10,890 --> 00:20:15,040
the presence of cancer and c bar no cancer all right and we do the

285
00:20:15,040 --> 00:20:20,510
same for a positive mammography and and a negative mammography and bar really i wouldn't

286
00:20:20,510 --> 00:20:23,980
have to define other random variable small and here that

287
00:20:24,020 --> 00:20:28,010
it takes values from an alphabet of size two or already

288
00:20:28,060 --> 00:20:31,430
OK so now let's let's formalize things

289
00:20:31,460 --> 00:20:32,710
the the three

290
00:20:32,740 --> 00:20:34,480
things that we said earlier

291
00:20:34,500 --> 00:20:36,350
were that

292
00:20:36,430 --> 00:20:42,600
p of c or p of small equals big already is equal to one percent

293
00:20:42,600 --> 00:20:44,140
OK that's the first fact

294
00:20:45,650 --> 00:20:48,290
what is it about

295
00:20:50,530 --> 00:20:53,070
you see that

296
00:20:53,080 --> 00:20:58,680
none of

297
00:20:58,710 --> 00:21:00,730
so it's again what is the

298
00:21:04,530 --> 00:21:05,570
correct yes

299
00:21:05,620 --> 00:21:10,930
o one percent of its own and no no no no no no what i

300
00:21:10,930 --> 00:21:15,670
said earlier this is one percent of scanned women have breast cancer breast cancer so

301
00:21:15,680 --> 00:21:18,280
you can always condition on being scanned here

302
00:21:18,280 --> 00:21:21,360
OK everybody is kind

303
00:21:21,470 --> 00:21:26,050
so then we can just skip OK so p of c is going to be

304
00:21:26,050 --> 00:21:28,740
one percent than what we said is that

305
00:21:28,790 --> 00:21:33,100
is that the probability of detection so if if there are if there is cancer

306
00:21:33,160 --> 00:21:37,530
the probability that the mammography will pick it up is eighty percent

307
00:21:37,530 --> 00:21:38,730
all right

308
00:21:38,740 --> 00:21:39,350
and now

309
00:21:39,380 --> 00:21:41,750
now if there is no cancer

310
00:21:41,790 --> 00:21:47,340
the probability of false alarm is nine point six percent OK now we formalise things

311
00:21:47,340 --> 00:21:48,480
a little bit

312
00:21:48,490 --> 00:21:50,880
so and what was the question again that we ask

313
00:21:50,900 --> 00:21:53,070
the question was

314
00:21:55,840 --> 00:21:59,780
one has a positive mammography what is the probability that she has breast cancer

315
00:21:59,780 --> 00:22:05,640
we can also write that formally as what is what is p of c given

316
00:22:07,260 --> 00:22:09,860
and again i apologize for sloppy notation

317
00:22:09,880 --> 00:22:10,960
this should really be

318
00:22:10,960 --> 00:22:13,140
p of small sequences given

319
00:22:13,210 --> 00:22:17,020
small r equals began

320
00:22:17,030 --> 00:22:19,570
we can sort of see this very messy to hold sequence and all over the

321
00:22:22,990 --> 00:22:25,150
so now if i give you a bit of time i fear many of you

322
00:22:25,150 --> 00:22:28,070
will have solved is already so let's let's

323
00:22:28,120 --> 00:22:29,340
let's move on

324
00:22:29,390 --> 00:22:32,930
so an easy way to an easy way we have two random variables and they

325
00:22:32,930 --> 00:22:36,370
each take two values so it's almost like we can really

326
00:22:36,390 --> 00:22:37,400
they can example

327
00:22:37,410 --> 00:22:41,830
and raw table right and things are a lot easier i mean counting

328
00:22:41,840 --> 00:22:44,970
it's actually easier than than anything right

329
00:22:46,080 --> 00:22:51,220
let's just this issue very concrete treatment let's take this mistake ten thousand subjects

330
00:22:52,710 --> 00:22:53,970
and so now

331
00:22:53,980 --> 00:22:57,200
in which categories with a fall given information that we have

332
00:22:57,250 --> 00:22:59,090
OK let's take first

333
00:22:59,110 --> 00:23:04,450
let's assume there is concern OK so let's take one percent of them so one

334
00:23:04,450 --> 00:23:08,330
percent of ten thousand is one hundred OK we take that and now we divide

335
00:23:08,330 --> 00:23:12,510
them into groups we can do that now because we know that

336
00:23:12,520 --> 00:23:17,120
the probability of the mammography being positive for this group is eighty percent so eighty

337
00:23:17,120 --> 00:23:21,490
of those hundred people are going to get a positive mammography and now we also

338
00:23:21,490 --> 00:23:23,870
can't obstruct right so we know that twenty

339
00:23:23,890 --> 00:23:25,210
percent actually

340
00:23:25,210 --> 00:23:29,480
do not get a positive mammography OK so this would be a case of of

341
00:23:29,480 --> 00:23:33,520
non detection OK when the system fails to detect the presence of cancer

342
00:23:34,540 --> 00:23:40,000
ten thousand minus one hundred is nine thousand nine hundred so now let's let's look

343
00:23:40,000 --> 00:23:42,020
at what happens to

344
00:23:43,280 --> 00:23:45,040
OK fair enough

345
00:23:45,090 --> 00:23:48,820
so p of c bar would be ninety nine percent OK so this would be the the

346
00:23:48,830 --> 00:23:52,750
the subjects that do not have cancer now how can we divide them up well

347
00:23:52,750 --> 00:23:57,310
we've been told that for the people that do not have cancer the probability of

348
00:23:57,310 --> 00:24:00,680
still getting a positive mammography is nine point six percent so we just do the

349
00:24:00,680 --> 00:24:04,600
math and we get the nine hundred and fifty percent of those subjects actually get

350
00:24:04,600 --> 00:24:08,070
a positive mammography although they didn't have cancer

351
00:24:08,080 --> 00:24:10,070
eight thousand nine hundred fifty

352
00:24:10,090 --> 00:24:11,400
that's basically

353
00:24:11,410 --> 00:24:16,150
this number minus number get a negative mammography and they do not have cancer so

354
00:24:16,150 --> 00:24:18,810
now we can go in and do this little tables

355
00:24:18,820 --> 00:24:20,450
and we can put all the numbers in

356
00:24:21,470 --> 00:24:25,290
in our almost done

357
00:24:25,300 --> 00:24:30,170
so what we're going to do now is we're going to obtain bayes rule in

358
00:24:30,170 --> 00:24:33,800
the very very intuitive way

359
00:24:33,840 --> 00:24:37,970
let's remember what the question was

360
00:24:37,980 --> 00:24:40,960
the what the question was what is the probability of c

361
00:24:40,970 --> 00:24:46,600
conditioned on the fact that it's OK sorry that and the mammography was positive

362
00:24:48,590 --> 00:24:52,110
so we know that the mammography was positive so we can almost ignore this column

363
00:24:52,680 --> 00:24:55,880
so what we need to do is we need to go into this column and

364
00:24:55,880 --> 00:24:58,920
divide this number by the sum of these two numbers

365
00:24:58,970 --> 00:25:01,210
and that's what's going to give us the answer and you can already see that

366
00:25:01,220 --> 00:25:03,260
is not such a big number

367
00:25:03,270 --> 00:25:07,450
OK let's do it so the first thing we need is is the denominator right

368
00:25:07,450 --> 00:25:11,360
you can also put it in words so of all subjects that have a positive

369
00:25:11,360 --> 00:25:16,600
mammography how many of these actually have cancer that's the question that is

370
00:25:16,620 --> 00:25:18,810
what is p of the given OK

371
00:25:18,810 --> 00:25:22,270
so the the first thing we need is always p them but we've seen before

372
00:25:22,270 --> 00:25:27,140
that there is a rule of marginalization OK so if i have the joint distribution

373
00:25:27,200 --> 00:25:30,900
i just need to add up to sum up over and over the thing that

374
00:25:30,900 --> 00:25:32,620
doesn't interest me

375
00:25:32,670 --> 00:25:36,930
so i just add up over c and c bar OK again the notation here

376
00:25:36,930 --> 00:25:39,930
actually hurts to a little bit because here you would really want to say p

377
00:25:39,930 --> 00:25:42,560
of small n equals big n

378
00:25:42,600 --> 00:25:44,300
OK and then i sort of

379
00:25:44,300 --> 00:25:47,950
marginalize over the values that small c can take so small so you can take

380
00:25:47,950 --> 00:25:51,210
the value big c or c bar OK

381
00:25:51,260 --> 00:25:54,210
but i think if you look at the table it's actually much easier than the

382
00:25:54,210 --> 00:25:56,710
this image is supposed to here

383
00:25:56,990 --> 00:25:59,480
like this one

384
00:25:59,710 --> 00:26:03,090
in which you have to do with this

385
00:26:03,110 --> 00:26:05,000
this help the

386
00:26:07,340 --> 00:26:09,000
what happened the and

387
00:26:09,040 --> 00:26:11,630
what was the model

388
00:26:11,680 --> 00:26:12,950
you can consider

389
00:26:12,970 --> 00:26:16,360
so we might call

390
00:26:22,260 --> 00:26:23,820
because that would the

391
00:26:23,870 --> 00:26:25,130
so on

392
00:26:31,160 --> 00:26:34,630
i could change

393
00:26:35,680 --> 00:26:38,870
if you don't

394
00:26:39,750 --> 00:26:42,600
with the middle point

395
00:26:43,590 --> 00:26:46,650
so it's been that way

396
00:26:46,660 --> 00:26:47,800
how many

397
00:26:47,890 --> 00:26:50,900
and this is the second dimension to those

398
00:26:55,900 --> 00:26:59,990
and because i should say that this kind of mess

399
00:27:00,300 --> 00:27:03,870
it depends on the point challenge because

400
00:27:06,020 --> 00:27:09,120
you have to be made

401
00:27:10,210 --> 00:27:12,260
which are like that learning

402
00:27:12,460 --> 00:27:14,090
that you o

403
00:27:14,100 --> 00:27:19,000
because the in central

404
00:27:20,290 --> 00:27:21,100
in that

405
00:27:21,760 --> 00:27:24,030
and that is going to play

406
00:27:26,360 --> 00:27:27,590
what about

407
00:27:27,710 --> 00:27:29,410
but the main

408
00:27:29,460 --> 00:27:31,500
that we

409
00:27:31,550 --> 00:27:33,640
so far

410
00:27:33,650 --> 00:27:35,920
all right

411
00:27:37,820 --> 00:27:39,370
that's because

412
00:27:39,370 --> 00:27:40,950
wait wait

413
00:27:42,960 --> 00:27:44,410
the main

414
00:27:44,430 --> 00:27:45,990
just to

415
00:27:47,170 --> 00:27:50,910
o point that you

416
00:27:52,880 --> 00:27:56,360
so this is an example of this

417
00:27:59,300 --> 00:28:01,390
o point

418
00:28:01,450 --> 00:28:02,620
how can

419
00:28:09,450 --> 00:28:11,970
some but this one

420
00:28:19,750 --> 00:28:22,040
between the

421
00:28:23,870 --> 00:28:25,120
neighboring pixels

422
00:28:27,000 --> 00:28:27,830
the that

423
00:28:27,840 --> 00:28:29,890
four one

424
00:28:30,850 --> 00:28:31,910
in the same way

425
00:28:31,930 --> 00:28:34,460
my the that

426
00:28:34,470 --> 00:28:36,670
one of the

427
00:28:36,730 --> 00:28:38,320
this debate

428
00:28:39,700 --> 00:28:40,890
this one

429
00:28:42,370 --> 00:28:45,520
so what

430
00:28:46,660 --> 00:28:48,890
o this that

431
00:28:48,910 --> 00:28:52,420
while she won why

432
00:28:53,550 --> 00:28:56,490
i talked about before

433
00:28:56,500 --> 00:29:00,360
so how can detect one more

434
00:29:00,360 --> 00:29:04,490
we apply my all pixels of the image

435
00:29:04,510 --> 00:29:06,180
and the two

436
00:29:06,210 --> 00:29:08,840
response to the model

437
00:29:10,800 --> 00:29:14,820
we know which one is the stronger

438
00:29:17,010 --> 00:29:22,460
we can understand which like we have this part of the image

439
00:29:22,560 --> 00:29:24,390
it is also possible

440
00:29:24,410 --> 00:29:26,580
one particular direction

441
00:29:26,730 --> 00:29:30,160
they should not one particular one

442
00:29:31,790 --> 00:29:33,140
give you the strongest

443
00:29:45,040 --> 00:29:47,110
line which i want to

444
00:29:47,180 --> 00:29:48,770
the great

445
00:29:50,120 --> 00:29:52,340
it is

446
00:29:54,690 --> 00:29:56,400
like this

447
00:29:59,970 --> 00:30:01,670
by the way

448
00:30:01,680 --> 00:30:05,930
and this one one because they

449
00:30:05,950 --> 00:30:07,540
one night

450
00:30:07,540 --> 00:30:10,400
you also the right

451
00:30:10,430 --> 00:30:11,560
but this is

452
00:30:11,570 --> 00:30:13,300
a little bit like this

453
00:30:15,360 --> 00:30:17,090
the fact one

454
00:30:21,070 --> 00:30:23,420
why the image

455
00:30:23,420 --> 00:30:25,770
i just want to return to something

456
00:30:25,820 --> 00:30:29,570
which you pointed out yesterday search again i don't know

457
00:30:29,620 --> 00:30:33,380
you will proving one half of the correspondence lemma

458
00:30:33,500 --> 00:30:36,550
if w forces all instances of t

459
00:30:36,560 --> 00:30:38,280
in our is reflexive

460
00:30:38,420 --> 00:30:40,040
we went through the process

461
00:30:40,060 --> 00:30:43,810
and what i said is OK suppose one is true and not too

462
00:30:43,830 --> 00:30:47,810
in other words suppose wlr forces all instances of t

463
00:30:47,860 --> 00:30:50,660
but our is not reflexive

464
00:30:50,670 --> 00:30:53,660
hence there is some irreflexive w in w

465
00:30:53,710 --> 00:30:55,150
in big w so

466
00:30:55,170 --> 00:30:55,980
i said that

467
00:30:56,030 --> 00:30:58,110
OK that's little w

468
00:30:58,160 --> 00:31:00,790
and it's a reflexive phenomena

469
00:31:00,840 --> 00:31:04,910
indicate that by saying that there isn't an arrow

470
00:31:05,160 --> 00:31:10,510
from from w two itself

471
00:31:10,520 --> 00:31:14,210
then we built to model and we found that there was a contradiction

472
00:31:14,350 --> 00:31:16,200
what you suggest is

473
00:31:16,220 --> 00:31:18,800
we could just forget these

474
00:31:18,860 --> 00:31:22,490
right we just delete these guys and make w dating

475
00:31:22,500 --> 00:31:24,180
because then it would make box

476
00:31:24,260 --> 00:31:27,070
not true OK we can do that

477
00:31:27,120 --> 00:31:30,830
because we given the model were not allowed to fiddle with the model

478
00:31:30,880 --> 00:31:34,320
so we given the brain were not allowed to fiddle with the frame

479
00:31:34,340 --> 00:31:38,300
OK we can add whatever evaluation we want to work but you can't go around

480
00:31:39,800 --> 00:31:44,120
ah OK so that's why i wasn't fast enough to pick that up but i

481
00:31:44,120 --> 00:31:48,260
thought about it last year why not do that you know it's as simple as

482
00:31:49,280 --> 00:31:52,580
OK there is there isn't OK so that's just

483
00:31:53,270 --> 00:31:56,380
one point now

484
00:31:58,150 --> 00:32:02,840
i finished off sort of giving a little cook's tour of various

485
00:32:09,990 --> 00:32:11,440
the frame class

486
00:32:11,450 --> 00:32:14,620
that's the class of kripke frames for which

487
00:32:14,750 --> 00:32:15,920
this logic is

488
00:32:15,940 --> 00:32:17,480
seven complete

489
00:32:17,540 --> 00:32:21,200
and the first order condition and a lot of that right so

490
00:32:21,290 --> 00:32:23,390
why reflexivity remain

491
00:32:23,460 --> 00:32:24,620
that w

492
00:32:24,640 --> 00:32:26,890
reaches itself

493
00:32:27,160 --> 00:32:29,700
and what we saw is that there is general

494
00:32:29,750 --> 00:32:33,130
theorem which says that you take any one of these and have them as shapes

495
00:32:33,130 --> 00:32:34,190
to you all

496
00:32:36,730 --> 00:32:41,630
then you get an appropriate hilbert calculus which is stronger than k

497
00:32:41,760 --> 00:32:43,740
because you can prove all things

498
00:32:43,810 --> 00:32:47,040
and if you take the appropriate

499
00:32:47,050 --> 00:32:50,720
the union of all of these conditions you get a class of kripke frames

500
00:32:50,790 --> 00:32:56,280
which satisfies all the conditions that you've taken the name the correspondence between these two

501
00:32:56,280 --> 00:33:00,540
that's the soundness and completeness theorems still carries through

502
00:33:00,780 --> 00:33:02,490
so what i want to do now

503
00:33:02,520 --> 00:33:05,630
in the first just i want to do it briefly but i want to give

504
00:33:05,630 --> 00:33:07,360
you a tour of

505
00:33:07,440 --> 00:33:12,360
what is known about these three notions

506
00:33:13,060 --> 00:33:16,720
you would calculate the kripke frames and the condition

507
00:33:16,820 --> 00:33:18,890
and the first order definable here

508
00:33:19,870 --> 00:33:26,480
the fact that something is first order definable is called correspondence theory about the shape

509
00:33:26,480 --> 00:33:28,940
corresponds to a first order formula

510
00:33:29,070 --> 00:33:34,000
the fact that there is another condition which captures the shape and vice versa is

511
00:33:34,000 --> 00:33:36,420
known as the incompleteness of the

512
00:33:40,850 --> 00:33:42,500
to some definition

513
00:33:42,630 --> 00:33:43,930
i will say that

514
00:33:43,940 --> 00:33:46,030
a normal modal logic l

515
00:33:46,100 --> 00:33:51,610
is determined by class of kripke and so some frame plasticity and determines the logic

516
00:33:55,050 --> 00:34:00,550
every frame validates five if and only if phi is a theorem

517
00:34:00,600 --> 00:34:05,480
OK every frame validates five if and only if i is deducible from the empty

518
00:34:07,430 --> 00:34:09,190
remember i defined

519
00:34:09,200 --> 00:34:13,310
logics into a nice that you can define logic is all the validity

520
00:34:13,320 --> 00:34:16,370
all the things that a logical consequence of

521
00:34:17,020 --> 00:34:18,140
two doucet

522
00:34:18,150 --> 00:34:22,330
all the theorems all the things which is deducible from the empty set

523
00:34:22,340 --> 00:34:24,600
what i'm saying is that when the whole

524
00:34:24,910 --> 00:34:26,850
for frame class and the

525
00:34:26,860 --> 00:34:31,350
the logic you say that the frame class determines the logic

526
00:34:31,410 --> 00:34:35,430
unfortunately there is another notion of complete which is that

527
00:34:35,500 --> 00:34:37,450
they said that the logic is a

528
00:34:37,510 --> 00:34:39,380
if some frame class

529
00:34:39,400 --> 00:34:40,290
the terms

530
00:34:43,430 --> 00:34:45,130
that logic

531
00:34:45,140 --> 00:34:47,620
and normal modal logic is canonical

532
00:34:47,670 --> 00:34:52,850
if it's determined by its canonical frame so remember we did this canonical model construction

533
00:34:52,850 --> 00:34:55,470
using infinite sets of formulae

534
00:34:55,480 --> 00:35:00,490
and maximally consistent extensions of our assumptions gamma

535
00:35:00,510 --> 00:35:04,460
that gave us the frame and from that we build to model by saying something

536
00:35:04,460 --> 00:35:07,640
is true in the world if it's in the world if it's in the set

537
00:35:07,650 --> 00:35:09,620
associated with the world

538
00:35:09,630 --> 00:35:10,740
so if the

539
00:35:10,760 --> 00:35:12,980
canonical model

540
00:35:13,030 --> 00:35:16,220
if the validities of the canonical frame

541
00:35:16,260 --> 00:35:20,540
are exactly the things that deducible from the empty set and you save the logic

542
00:35:20,540 --> 00:35:21,640
is canonical

543
00:35:21,690 --> 00:35:23,490
as it

544
00:35:23,510 --> 00:35:25,650
very general theorem notice of course

545
00:35:25,660 --> 00:35:28,190
i can't go into the details because

546
00:35:28,230 --> 00:35:31,210
otherwise you know it just takes up the whole lecture

547
00:35:31,250 --> 00:35:36,040
so this is shape there's a particular class of formulae called sulcus formulae

548
00:35:36,140 --> 00:35:42,160
you can find a very good proof in blackburn roc and venema

549
00:35:45,240 --> 00:35:52,560
two can't

550
00:35:52,580 --> 00:35:54,570
really OK

551
00:35:56,260 --> 00:35:58,930
can i go on

552
00:36:01,980 --> 00:36:05,140
and then there are these three theorem

553
00:36:05,190 --> 00:36:08,630
these three theories are more which are going to be an admin

554
00:36:10,840 --> 00:36:13,300
you have a solid with formula five

555
00:36:13,360 --> 00:36:17,250
and you know that corresponds to some first order condition on frames

556
00:36:17,320 --> 00:36:22,930
and that's effectively computable from five look at you you look at five

557
00:36:22,940 --> 00:36:25,920
and there is an algorithm which i'll compute the frame

558
00:36:25,930 --> 00:36:28,210
so for example

559
00:36:29,090 --> 00:36:32,220
you know if i give you five

560
00:36:32,250 --> 00:36:33,630
shape five

561
00:36:33,710 --> 00:36:37,960
and there is an algorithm which you can crank the handle and little bit out

562
00:36:38,310 --> 00:36:40,720
five five corresponds to this

563
00:36:47,020 --> 00:36:50,470
moreover if a is the sole course formula

564
00:36:50,480 --> 00:36:55,720
or if they want to assault formulated and we take the shapes

565
00:36:55,730 --> 00:36:56,590
then the

566
00:36:56,600 --> 00:36:57,960
hilbert logic

567
00:36:57,970 --> 00:37:00,980
OK i one a case canonical

568
00:37:01,030 --> 00:37:03,090
in other words

569
00:37:03,170 --> 00:37:07,790
its canonical frame the validities are the canonical frame are all the things you can

570
00:37:07,790 --> 00:37:10,940
deduce from the empty set

571
00:37:12,150 --> 00:37:14,150
in the other half is

572
00:37:15,650 --> 00:37:17,050
this is

573
00:37:17,150 --> 00:37:21,140
it's determined by exactly the union of all the conditions right if you give me

574
00:37:21,260 --> 00:37:25,950
of course for the say a one i can compute some condition

575
00:37:25,950 --> 00:37:28,150
in general for example if sigma

576
00:37:28,190 --> 00:37:31,880
is the diagonal covariance matrix that says that all of these these are independent of

577
00:37:31,880 --> 00:37:32,890
each other

578
00:37:33,970 --> 00:37:35,550
the way you get

579
00:37:35,560 --> 00:37:37,940
interesting structure in this vector t

580
00:37:37,960 --> 00:37:41,050
it is by assuming that there are

581
00:37:41,080 --> 00:37:45,500
nonzero elements on the off diagonal of sigma

582
00:37:45,510 --> 00:37:48,910
so that's for now just imagine that

583
00:37:48,920 --> 00:37:51,680
the elements sigma i j

584
00:37:51,700 --> 00:37:54,260
of this covariance matrix

585
00:37:54,270 --> 00:37:58,330
is going to be some function of i and j

586
00:37:58,350 --> 00:38:01,680
OK so i think of these points is being order

587
00:38:01,820 --> 00:38:06,830
t one through t and then i'm going to make the covariance of neighboring points

588
00:38:06,850 --> 00:38:11,790
be higher than the covariance of points far away so i and plus one have

589
00:38:11,790 --> 00:38:17,330
higher covariance ironi plus ten are going to have lower covariance and now we can

590
00:38:17,530 --> 00:38:21,440
just draw random samples of this vector

591
00:38:21,810 --> 00:38:23,460
and see what they look like

592
00:38:23,470 --> 00:38:26,550
and we're going to draw these vectors as a function of the index going from

593
00:38:26,550 --> 00:38:28,260
one through n

594
00:38:35,380 --> 00:38:38,320
there are different ways that i can make the

595
00:38:38,330 --> 00:38:41,120
covariance depend on

596
00:38:41,130 --> 00:38:46,150
i and j the covariance between the he i t

597
00:38:46,160 --> 00:38:47,530
can depend on

598
00:38:47,550 --> 00:38:49,700
the difference between i and j

599
00:38:49,720 --> 00:38:54,530
say it could be e to the minus the difference between i and j squared

600
00:38:55,310 --> 00:38:59,520
with different length scales in other words it could care about the distance between i

601
00:38:59,520 --> 00:39:00,460
and j

602
00:39:00,490 --> 00:39:03,730
more or less but you'll see that the examples

603
00:39:06,380 --> 00:39:13,240
let's pick one for this covariance function and draw

604
00:39:13,260 --> 00:39:14,410
one sample

605
00:39:14,420 --> 00:39:15,310
of that

606
00:39:16,140 --> 00:39:20,720
so here it is drawn a hundred dimensional vector

607
00:39:20,740 --> 00:39:23,060
so t one through t hundreds

608
00:39:23,080 --> 00:39:26,040
in order and connecting up the dots

609
00:39:26,050 --> 00:39:28,690
and this is for covariance

610
00:39:29,200 --> 00:39:33,420
function that basically relates i and j with the particular length scale and you can

611
00:39:33,420 --> 00:39:37,990
see intuitively what that length scale is it's sort of the skill with which this

612
00:39:37,990 --> 00:39:39,530
function wiggles

613
00:39:39,550 --> 00:39:43,390
OK now i just switched to talking about this as a function

614
00:39:43,400 --> 00:39:45,190
even though this is the vector from

615
00:39:45,200 --> 00:39:48,070
one two hundred

616
00:39:48,160 --> 00:39:51,310
and in fact it sort of looks like a function we can think of this

617
00:39:51,310 --> 00:39:55,200
space now instead of being the index that goes from one two hundred as being

618
00:39:55,200 --> 00:39:57,160
some input space x

619
00:39:57,180 --> 00:40:00,030
OK so this is the function of x which happens to go from one two

620
00:40:00,040 --> 00:40:04,500
hundred but it could have nominated known non integer values here

621
00:40:04,950 --> 00:40:09,600
and one draw this function looks like this let's try

622
00:40:09,620 --> 00:40:11,940
remember this is again

623
00:40:11,960 --> 00:40:14,070
distribution in hundred dimensions

624
00:40:14,090 --> 00:40:17,750
and i just wanted it i can draw another sample from exactly the same guassian

625
00:40:17,750 --> 00:40:20,660
distributions and put on top of that

626
00:40:20,710 --> 00:40:23,410
that sort of in green right there

627
00:40:23,430 --> 00:40:24,910
i can try another one

628
00:40:24,930 --> 00:40:28,960
and if i draw a few of these samples you get a sense for what

629
00:40:28,960 --> 00:40:31,190
the functions under this

630
00:40:31,400 --> 00:40:38,600
calcium prior over functions look like they're sort of functions that have variants of roughly

631
00:40:38,600 --> 00:40:42,280
one and a wiggle about with someone scale

632
00:40:42,300 --> 00:40:47,660
OK so if i draw even more functions you get to see

633
00:40:47,710 --> 00:40:50,330
just like typical examples of this

634
00:40:51,770 --> 00:41:02,520
essentially an guassian process what you're doing is you're generalizing the multivariate gaussians distribution

635
00:41:02,530 --> 00:41:04,910
over these vectors to be

636
00:41:06,440 --> 00:41:08,790
distribution over functions of

637
00:41:09,020 --> 00:41:13,730
these are just examples of draws from that a distribution over functions

638
00:41:15,490 --> 00:41:21,370
they have a particular properties variances lengthscales and so on which you can control

639
00:41:21,380 --> 00:41:23,500
so i

640
00:41:24,550 --> 00:41:26,860
as an example of

641
00:41:26,880 --> 00:41:33,640
another covariance function

642
00:41:33,650 --> 00:41:40,940
i drawn examples with a particular different length scales

643
00:41:40,960 --> 00:41:43,540
and you can see that these functions

644
00:41:43,560 --> 00:41:46,010
now we go last

645
00:41:46,430 --> 00:41:49,120
as you vary the input dimension

646
00:41:49,200 --> 00:41:52,870
so they have a longer length scale so really i could draw

647
00:41:52,960 --> 00:41:55,400
functions with

648
00:41:55,450 --> 00:42:01,960
shorter length scales or i could draw functions that have difference different forms of the

649
00:42:01,960 --> 00:42:03,690
covariance so instead of

650
00:42:03,710 --> 00:42:06,010
the covariance going down

651
00:42:06,020 --> 00:42:10,000
with e to the minus distance squared it could go down with e to the

652
00:42:10,000 --> 00:42:14,750
minus absolute value of the distance if i do that

653
00:42:15,720 --> 00:42:20,070
i'll get

654
00:42:20,120 --> 00:42:23,530
functions that look like this they're actually quite rough

655
00:42:24,670 --> 00:42:25,780
these are

656
00:42:25,800 --> 00:42:30,530
the special case against processes they are given by things like brownian motion

657
00:42:30,550 --> 00:42:33,980
so here are just some examples of these kinds of functions

658
00:42:40,290 --> 00:42:45,300
and this relates the second way of thinking about downstream processes

659
00:42:45,320 --> 00:42:46,690
i can also

660
00:42:46,700 --> 00:42:51,290
i think of just linear functions as a special case of gas processes

661
00:42:55,040 --> 00:42:56,920
sorry the weekly

662
00:42:56,960 --> 00:43:01,040
and that's the numerical issues should be a straight line to measure the

663
00:43:01,100 --> 00:43:03,460
i played around with this look strange

664
00:43:03,470 --> 00:43:07,770
this is just this this is just a random draw of the straight line

665
00:43:07,790 --> 00:43:12,470
OK to ignore the wiggles that's just bad numerics on my part

666
00:43:12,580 --> 00:43:15,900
here's another random draw the straight line

667
00:43:15,910 --> 00:43:18,070
here's another

668
00:43:18,080 --> 00:43:20,290
you know here five more

669
00:43:20,310 --> 00:43:22,090
there are twenty one

670
00:43:22,110 --> 00:43:24,430
so these are also examples of

671
00:43:24,450 --> 00:43:31,380
draws from guassian process which happens to have a particular covariance structure that gives you

672
00:43:31,380 --> 00:43:32,610
straight lines

673
00:43:33,990 --> 00:43:38,960
these are fairly easy to define once you familiar with these covariance structure

674
00:43:40,150 --> 00:43:43,550
what do you do with calcium processes

675
00:43:44,580 --> 00:43:46,800
what you do is

676
00:43:46,850 --> 00:43:51,040
use them for learning is for learning about functions so essentially what you want to

677
00:43:51,040 --> 00:43:53,760
do is you want to get examples of function

678
00:43:53,770 --> 00:43:56,190
are examples of data points from the function

679
00:43:56,200 --> 00:44:01,040
and you have a prior over functions and using those examples of inputs and outputs

680
00:44:01,310 --> 00:44:06,780
we can infer posterior over functions which actually also happens to be a guassian process

681
00:44:06,790 --> 00:44:09,040
and then you can make predictions

682
00:44:09,060 --> 00:44:12,770
so let's do that let's imagine that we had

683
00:44:12,780 --> 00:44:15,810
these input output data points

684
00:44:19,360 --> 00:44:21,820
now i'm going to

685
00:44:21,840 --> 00:44:28,920
fit guassian process for do inference under guassian process with these inputs and outputs OK

686
00:44:28,970 --> 00:44:31,140
and what this shows is

687
00:44:31,240 --> 00:44:32,930
the boolean function

688
00:44:32,950 --> 00:44:38,430
and the standard deviation of the function or maybe two standard deviations in this case

689
00:44:38,450 --> 00:44:43,830
so it shows you the sort of posterior prior functions had all those different kind

690
00:44:43,840 --> 00:44:47,510
of weekly things the posterior functions is going to look like it goes through the

691
00:44:47,510 --> 00:44:48,930
data because

692
00:44:48,950 --> 00:44:52,270
you know the power like that gives the posterior

693
00:44:52,280 --> 00:44:54,640
you know somehow manages the data

694
00:44:54,650 --> 00:44:58,310
and have some uncertainty in the uncertainty is going

695
00:44:59,060 --> 00:45:02,230
as you go far away from the data like you would expect me that's what

696
00:45:07,240 --> 00:45:12,480
let's go back to the talk

697
00:45:12,550 --> 00:45:18,330
so that's the perspective of gas processes just starting from gas in distribution we go

698
00:45:18,330 --> 00:45:21,060
into more than math but i think the important thing is to get the pictures

699
00:45:21,060 --> 00:45:25,280
observing the entire sample will be the probability of observing

700
00:45:25,290 --> 00:45:26,520
each individual

701
00:45:26,830 --> 00:45:29,150
for example if i have

702
00:45:29,200 --> 00:45:31,580
one show a collection of images

703
00:45:31,580 --> 00:45:32,790
in my

704
00:45:32,810 --> 00:45:36,960
database the collection of images the probability of that

705
00:45:36,990 --> 00:45:38,700
observing the entire collection

706
00:45:38,710 --> 00:45:40,940
is the problem of the probe into absurd

707
00:45:40,950 --> 00:45:45,580
each one thing which is independent

708
00:45:47,250 --> 00:45:49,380
by doing that i will think these

709
00:45:49,400 --> 00:45:54,350
expression for the probability of observing an entire some sample

710
00:45:54,370 --> 00:45:59,320
in this case the statistical definition for some series of observations

711
00:45:59,350 --> 00:46:00,390
if i have

712
00:46:00,450 --> 00:46:04,770
at hand to make statistical inference

713
00:46:04,780 --> 00:46:06,250
when we see

714
00:46:06,260 --> 00:46:14,040
these object as a function of the the these hasname is called the likelihood function

715
00:46:14,090 --> 00:46:17,330
the likelihood function is a function of the data

716
00:46:17,410 --> 00:46:19,460
it tells us

717
00:46:19,470 --> 00:46:23,620
for a given sample packs

718
00:46:23,650 --> 00:46:28,130
which feed those are more likely

719
00:46:28,220 --> 00:46:32,330
to improve the at that

720
00:46:32,450 --> 00:46:34,760
let's give an example

721
00:46:39,830 --> 00:46:43,290
any questions

722
00:46:43,310 --> 00:46:48,500
please feel free to drop

723
00:46:48,580 --> 00:46:49,850
let's assume

724
00:46:50,760 --> 00:46:53,090
i know that my observations follow

725
00:46:53,120 --> 00:46:55,740
part of the gulf distribution

726
00:46:55,770 --> 00:46:56,960
in these

727
00:46:56,970 --> 00:47:01,160
what about x here

728
00:47:01,180 --> 00:47:03,240
and now it's time to observe one

729
00:47:03,260 --> 00:47:06,770
what i observe one observation here another here's another here

730
00:47:06,790 --> 00:47:08,540
other here rather here

731
00:47:10,080 --> 00:47:10,840
the here

732
00:47:10,850 --> 00:47:12,080
another here

733
00:47:12,090 --> 00:47:13,810
on the other hand

734
00:47:13,820 --> 00:47:17,760
that's all

735
00:47:17,780 --> 00:47:20,490
i ask you to

736
00:47:20,570 --> 00:47:25,740
tell me what you think when you think that every of gaussian distribution lines

737
00:47:27,400 --> 00:47:29,210
this is just observation

738
00:47:29,280 --> 00:47:31,500
concentrated in this region

739
00:47:31,540 --> 00:47:33,990
some of them are far away

740
00:47:34,010 --> 00:47:36,160
but most concentrated

741
00:47:36,260 --> 00:47:38,190
for tell OK give me

742
00:47:38,200 --> 00:47:43,420
what you think is the gaussian distribution that's generating process you possibly tell me well

743
00:47:43,480 --> 00:47:48,380
maybe something like this

744
00:47:48,390 --> 00:47:54,160
there are many observation region so probably the mean of the girlfriends over here

745
00:47:54,230 --> 00:47:56,870
so this is what people want

746
00:47:56,880 --> 00:47:58,150
which has given

747
00:47:58,160 --> 00:47:59,690
i mean

748
00:47:59,750 --> 00:48:00,890
in the given

749
00:48:00,890 --> 00:48:03,190
standard deviation

750
00:48:04,110 --> 00:48:04,880
and this

751
00:48:04,890 --> 00:48:07,210
now for the same sample

752
00:48:07,250 --> 00:48:10,030
for the same sample x x

753
00:48:10,040 --> 00:48:13,100
this particular sample here

754
00:48:13,160 --> 00:48:16,780
for the same sample i proposed this model

755
00:48:16,790 --> 00:48:24,500
these blue model

756
00:48:24,520 --> 00:48:31,760
which of these two models think it's more likely to have generated the samples

757
00:48:31,770 --> 00:48:33,980
the red one

758
00:48:34,030 --> 00:48:35,320
so there's something

759
00:48:35,370 --> 00:48:39,300
about the red mullets bad about the problem of what is

760
00:48:39,310 --> 00:48:41,040
the likelihood function

761
00:48:41,060 --> 00:48:43,910
because you mooted if you multiply

762
00:48:43,920 --> 00:48:48,630
these values of this function on these observations

763
00:48:48,670 --> 00:48:52,520
you will get a larger number than if you multiply the values of these of

764
00:48:52,570 --> 00:48:57,310
observations according to this distribution

765
00:48:58,350 --> 00:49:00,070
so actually what we are

766
00:49:00,130 --> 00:49:04,140
doing is we're fixing these sample x

767
00:49:04,150 --> 00:49:08,640
and you ask you which of the infinitely many possible

768
00:49:08,680 --> 00:49:10,680
gulshan function

769
00:49:11,480 --> 00:49:14,890
the gaussian function that maximize

770
00:49:16,020 --> 00:49:25,670
a function which is the product that product

771
00:49:25,680 --> 00:49:28,860
now the only thing we know that it is that we have

772
00:49:28,900 --> 00:49:33,610
got distribution we don't know the parameters the question is what are the parameters

773
00:49:33,630 --> 00:49:37,630
but the mean and lots the same of the gaussians such that these what is

774
00:49:41,000 --> 00:49:43,650
you may think that this the reliable criteria or not

775
00:49:44,780 --> 00:49:50,530
this has the name is called the maximum likelihood estimate is functional

776
00:49:50,540 --> 00:49:52,090
that produces

777
00:49:52,220 --> 00:49:57,230
feet according to to describe what is called the maximum like this

778
00:49:58,040 --> 00:50:02,100
it's probably one of the most common ways of helping us to

779
00:50:02,150 --> 00:50:04,280
mean this marks like

780
00:50:04,390 --> 00:50:08,060
no particular focus on this

781
00:50:08,070 --> 00:50:10,430
this is its most popular

782
00:50:10,870 --> 00:50:14,960
so the goal is we're going to observe observe some data

783
00:50:15,900 --> 00:50:20,250
and we want to maximize expression with respect to people because we want to find

784
00:50:20,250 --> 00:50:23,060
which to maximize like

785
00:50:23,140 --> 00:50:27,850
the same way as these functions the best describe is that

786
00:50:27,860 --> 00:50:32,420
we want to find which to to maximize just like any questions he

787
00:50:32,480 --> 00:50:36,610
emma going too slowly for you please you can calibrate

788
00:50:36,640 --> 00:50:38,800
who who who think i'm going to be

789
00:50:38,810 --> 00:50:41,230
you have different

790
00:50:41,470 --> 00:50:45,110
OK i was thinking way too fast

791
00:50:45,150 --> 00:50:46,150
well in the data

792
00:50:46,170 --> 00:50:51,460
feature or you are very good liars

793
00:50:51,470 --> 00:50:54,650
but anyway let's go

794
00:50:54,670 --> 00:50:57,520
so the negative

795
00:50:57,850 --> 00:51:01,850
OK so this is like the

796
00:51:01,960 --> 00:51:07,240
we're going particularly to be interested in an object which is

797
00:51:07,250 --> 00:51:11,220
like function to maximize function or minimize the negative

798
00:51:11,240 --> 00:51:12,410
of these

799
00:51:12,470 --> 00:51:14,190
like function

800
00:51:14,250 --> 00:51:19,120
it's complicated to maximize or minimize product so we just take the logo

801
00:51:19,140 --> 00:51:20,630
of these products

802
00:51:20,640 --> 00:51:22,120
producer some

803
00:51:22,140 --> 00:51:25,260
because if you have any function

804
00:51:25,280 --> 00:51:27,850
that you want to maximize or minimize

805
00:51:27,850 --> 00:51:31,060
the expectation of y is just a

806
00:51:31,100 --> 00:51:34,820
so we generate we generate all the samples side

807
00:51:34,900 --> 00:51:40,010
we for example we defined new random variable y i the expectation y is just

808
00:51:40,010 --> 00:51:41,000
the entropy

809
00:51:41,060 --> 00:51:44,070
now by the law of large numbers we know that we generate a lot of

810
00:51:46,620 --> 00:51:51,380
then this average must converge to its expected value

811
00:51:51,450 --> 00:51:54,230
so when you generate a lot of samples

812
00:51:54,280 --> 00:51:58,150
the sum of the y i was going to look very close to it

813
00:51:58,210 --> 00:51:59,530
what does that mean

814
00:51:59,560 --> 00:52:04,110
well it is the sum of the y arises within epsilon of a

815
00:52:05,630 --> 00:52:11,480
OK this is so so this event is going to happen with probability one as

816
00:52:11,480 --> 00:52:13,360
you increase the number of points

817
00:52:15,060 --> 00:52:18,500
well the sum of the y is has a very nice form the sum of

818
00:52:18,500 --> 00:52:20,310
the y axis is the log

819
00:52:20,330 --> 00:52:24,510
the log of the probability of x one plus log probability of x two plus

820
00:52:24,510 --> 00:52:26,360
log probability of x three

821
00:52:26,470 --> 00:52:30,800
therefore it's just the log one of the probability the entire sequence

822
00:52:30,820 --> 00:52:33,720
the point so-called independently

823
00:52:33,720 --> 00:52:35,530
and therefore the stem cells

824
00:52:35,540 --> 00:52:39,300
is that the probability of this particular sequence

825
00:52:39,310 --> 00:52:40,960
it's going to lie within

826
00:52:40,970 --> 00:52:44,590
within epsilon of what people call calling the typical probability

827
00:52:44,610 --> 00:52:47,670
it's going to live within two to the minus end

828
00:52:47,680 --> 00:52:52,250
h minus epsilon into the minus an explosives

829
00:52:52,310 --> 00:52:55,620
OK so this is that this is another way in which this

830
00:52:55,680 --> 00:52:59,230
in which and it turns out to be very natural

831
00:52:59,230 --> 00:53:02,840
which turns out to be a natural notion of randomness

832
00:53:02,900 --> 00:53:06,350
OK so let's return to our main question and c

833
00:53:06,400 --> 00:53:10,000
now that we have the notion of randomness we can actually try to formalize this

834
00:53:10,000 --> 00:53:11,370
question a little bit

835
00:53:11,480 --> 00:53:15,610
OK so we had a large set which in this toy example was just english

836
00:53:16,560 --> 00:53:18,890
we measure the bunch of features for each word

837
00:53:18,970 --> 00:53:21,730
because of its where we define security features

838
00:53:21,740 --> 00:53:26,450
the first feature was just that the word has made more than five

839
00:53:26,460 --> 00:53:30,280
now we wanted to find the a distribution over and over s

840
00:53:30,290 --> 00:53:34,050
i wanted to find an assignment of probabilities to all the work

841
00:53:34,080 --> 00:53:39,100
that satisfy the constraints we'd observed OK we observed that you know thirty percent of

842
00:53:39,100 --> 00:53:44,420
the words having more than five so that satisfies these particular constraints but otherwise is

843
00:53:44,420 --> 00:53:46,520
is as random as possible

844
00:53:46,560 --> 00:53:49,890
and now we can formalise that we can see that what we want is something

845
00:53:49,890 --> 00:53:53,870
that has maximum entropy subject to these constraints

846
00:53:53,960 --> 00:53:57,110
OK this is called the maximum entropy principle

847
00:53:57,170 --> 00:54:01,100
so now we actually know notion of entropy we can actually formalise this is an

848
00:54:01,100 --> 00:54:02,650
optimisation problem

849
00:54:02,740 --> 00:54:05,080
OK let's see what the problem looks like

850
00:54:05,100 --> 00:54:07,990
so we're looking for a probability distribution p

851
00:54:08,000 --> 00:54:11,180
it's over a set s which is english words OK

852
00:54:11,320 --> 00:54:15,870
we can think of this probability distribution is just a really long vector so

853
00:54:15,990 --> 00:54:18,820
the fifty thousand words it's just to vector with

854
00:54:18,870 --> 00:54:23,060
with fifty thousand entries which all probabilities and so they're all you know greater than

855
00:54:23,070 --> 00:54:25,510
or equal to zero and add up to one

856
00:54:25,570 --> 00:54:28,130
OK so we have this we have this vector

857
00:54:28,150 --> 00:54:31,040
its entries are these effects that the probability of x

858
00:54:31,060 --> 00:54:33,400
we want them to add up to one and we want them all to be

859
00:54:33,400 --> 00:54:37,460
positive is of sort of a reasonable probability distribution

860
00:54:37,510 --> 00:54:40,400
we want to find the thing that has maximum entropy

861
00:54:40,520 --> 00:54:42,990
subject to the constraints we observed

862
00:54:43,000 --> 00:54:47,140
and the constraints with the expected values of these features had certain

863
00:54:47,320 --> 00:54:52,890
certain we put something OK so for instance feature one that i equals one

864
00:54:52,950 --> 00:54:57,750
i was simply one if the word having more than five and zero otherwise

865
00:54:57,750 --> 00:54:59,560
and we wanted it to be the case

866
00:54:59,580 --> 00:55:01,890
the expected value of that feature

867
00:55:01,910 --> 00:55:07,410
the average value of that feature according to this distribution was point three

868
00:55:07,490 --> 00:55:12,360
so i you about this this optimisation problem

869
00:55:12,470 --> 00:55:16,300
you know we have the notion of entropy we have another reversion random we can

870
00:55:16,300 --> 00:55:18,000
write it down precisely

871
00:55:18,040 --> 00:55:22,200
so the nice thing about this is that the constraints turned out to be linear

872
00:55:22,240 --> 00:55:24,660
the thing that we're solving forest

873
00:55:24,680 --> 00:55:26,510
OK with solving for this vector

874
00:55:26,560 --> 00:55:30,860
these are linear constraints this is this is just a linear equation over here

875
00:55:30,880 --> 00:55:33,370
so we have a bunch of linear constraints

876
00:55:33,420 --> 00:55:36,010
these define some sort of policy group

877
00:55:36,060 --> 00:55:40,020
and the function we optimizing is a concave function

878
00:55:40,040 --> 00:55:44,250
so this entire thing becomes convex optimisation problem

879
00:55:45,550 --> 00:55:48,890
so a convex optimisation problem is just the generic

880
00:55:48,890 --> 00:55:50,060
and so

881
00:55:50,080 --> 00:55:53,970
this now is a plot of phi one against fighting and we can now just

882
00:55:53,970 --> 00:55:56,680
divide the point

883
00:55:56,750 --> 00:55:59,910
again we add a third feature for the bias that means that plane doesn't have

884
00:55:59,910 --> 00:56:01,970
to be through the origin

885
00:56:02,660 --> 00:56:04,250
so in theory

886
00:56:04,270 --> 00:56:05,830
given any dataset

887
00:56:05,850 --> 00:56:09,120
if we come up with the right set of basis functions we can construct a

888
00:56:09,120 --> 00:56:10,370
new dataset

889
00:56:10,370 --> 00:56:13,270
which is linearly separable

890
00:56:13,290 --> 00:56:17,250
so maybe it's going to be easier to do that rather than going back and

891
00:56:17,250 --> 00:56:18,790
writing new code and doing

892
00:56:18,810 --> 00:56:23,560
more complicated decision rules don't deal with planes we just about the data and total

893
00:56:23,560 --> 00:56:27,560
the simple temporal what's the planes will work

894
00:56:27,600 --> 00:56:30,830
not always the right thing to do but it's useful trick one you'll see come

895
00:56:30,830 --> 00:56:36,370
up a lot

896
00:56:43,060 --> 00:56:48,660
there are different areas machine learning and one larger the people often identified school supervised

897
00:56:48,660 --> 00:56:52,720
learning or more particularly sort of batch supervised learning which is the setting where you're

898
00:56:52,720 --> 00:56:57,040
given a training set to a data set of x and y you get to

899
00:56:57,040 --> 00:57:00,180
look at it for long time you build your prediction machine and then you can

900
00:57:00,180 --> 00:57:04,970
deploy that to make predictions on the future and i talked about one particular version

901
00:57:04,970 --> 00:57:08,040
of that which is this binary classification

902
00:57:08,080 --> 00:57:10,480
within supervised learning is also

903
00:57:10,500 --> 00:57:12,020
a whole bunch of

904
00:57:12,950 --> 00:57:16,830
tasks that you could imagine that match the same story if you're

905
00:57:16,910 --> 00:57:20,180
given the batch a whole bunch of examples that you get to look at build

906
00:57:20,180 --> 00:57:24,620
the prediction machines the predictions are necessarily plus one minus one

907
00:57:24,640 --> 00:57:25,560
they could be

908
00:57:25,560 --> 00:57:27,560
other sorts of prediction so

909
00:57:28,200 --> 00:57:34,330
we've already seen multiclass classification the land usage there could be multiple different labels nearest

910
00:57:34,330 --> 00:57:37,910
neighbour it was obvious how to implement the decision boundary

911
00:57:37,910 --> 00:57:39,520
let's say

912
00:57:39,580 --> 00:57:41,640
we might also have to predict

913
00:57:41,640 --> 00:57:46,390
a real number say if we wanted to predict properties of concrete or well

914
00:57:46,430 --> 00:57:52,160
well sourced material than a strength or slow ratio or some property of the material

915
00:57:52,160 --> 00:57:55,430
involved in the real numbers is not going to be plus minus one

916
00:57:55,520 --> 00:58:02,240
netflix movie ratings are stars i give this may be one to five stars and

917
00:58:02,240 --> 00:58:07,450
that's what's called ordinal regression so these stars one two three four five not really

918
00:58:07,450 --> 00:58:11,330
five separate classes because we know they have an ordering two them and we should

919
00:58:11,330 --> 00:58:12,540
take that into account say

920
00:58:12,600 --> 00:58:17,910
it's special prominence have a special name and your continued unmolested sort of different types

921
00:58:18,580 --> 00:58:24,450
outputs particular applications have particular names for the sort of thing we have to protect

922
00:58:24,500 --> 00:58:26,430
now i don't have time

923
00:58:26,450 --> 00:58:30,600
sadly to go through sort of how you would solve all of these different types

924
00:58:30,600 --> 00:58:31,870
of supervised learning

925
00:58:31,890 --> 00:58:35,750
problem but my promise was that a lot of the ideas that i already showed

926
00:58:35,750 --> 00:58:40,660
you binary classification to extend these other cases so i'm going to look at one

927
00:58:40,660 --> 00:58:43,100
of these cases real valued regression

928
00:58:43,120 --> 00:58:45,640
to show you how these different ideas apply

929
00:58:45,660 --> 00:58:47,660
to this problem and then hopefully

930
00:58:47,720 --> 00:58:50,830
you'll be in a better position to see how you would solve the other problems

931
00:58:50,830 --> 00:58:52,600
as well

932
00:58:53,220 --> 00:58:55,160
the real valued regression

933
00:58:55,220 --> 00:59:00,120
we now have inputs so we've got to inputs again but the output now

934
00:59:00,200 --> 00:59:02,200
is the real numbers so i can't just

935
00:59:02,250 --> 00:59:05,470
draw two different colours blue dots and red dots a

936
00:59:05,480 --> 00:59:09,430
the three d plot now where the red points of the data set have different

937
00:59:09,430 --> 00:59:12,100
real valued type so this could be

938
00:59:12,100 --> 00:59:17,390
amount of sand in my concrete and the percentages some aggregate i'm using and many

939
00:59:19,200 --> 00:59:21,370
OK and

940
00:59:21,390 --> 00:59:24,160
just as with classification

941
00:59:24,200 --> 00:59:27,370
when we want to sort of fit what's going on we could make a parametric

942
00:59:27,370 --> 00:59:29,100
assumption we could say

943
00:59:30,160 --> 00:59:32,830
maybe we could put a plane through this data set

944
00:59:32,870 --> 00:59:36,790
and for that plane and maybe that will be useful thing to do

945
00:59:39,250 --> 00:59:41,270
you need some algorithm for

946
00:59:41,290 --> 00:59:44,040
discovering the plane because we don't have to draw by hand

947
00:59:44,060 --> 00:59:46,560
and there are many different ways you could do that

948
00:59:46,620 --> 00:59:49,680
but really standard way it's called least squares

949
00:59:50,640 --> 00:59:54,330
you see all of these black lines joining the data plane

950
00:59:54,370 --> 00:59:58,970
those differences of the residuals of the errors the sort of mismatch between model and

951
00:59:58,970 --> 01:00:00,680
the observed data

952
01:00:00,680 --> 01:00:01,930
if we

953
01:00:01,950 --> 01:00:05,620
square all of those are terms and add them up then we get square error

954
01:00:05,620 --> 01:00:09,980
and we could find the plane for which is that error term the smallest possible

955
01:00:10,040 --> 01:00:11,770
that's called least squares

956
01:00:11,830 --> 01:00:15,660
it's very very important and the way you can tell it's important is that this

957
01:00:15,660 --> 01:00:17,850
is the matlab code to implement it

958
01:00:17,930 --> 01:00:20,350
so if it wasn't important they went to

959
01:00:20,410 --> 01:00:24,560
got one symbol that would do this for you in one line like that

960
01:00:24,580 --> 01:00:31,060
it's very fast you can do this for a very large datasets almost immediately

961
01:00:31,140 --> 01:00:34,100
and that just gives you the answer

962
01:00:34,140 --> 01:00:38,330
you might not be always what you want to do but it's important to know

963
01:00:39,520 --> 01:00:43,720
that's one of the things you could do

964
01:00:43,770 --> 01:00:46,540
and now we could go through the same progression

965
01:00:47,640 --> 01:00:52,040
questions we asked about classification so one thing one question is

966
01:00:52,060 --> 01:00:55,660
my data set isn't going to lie on the plane i think the nonlinearity in

967
01:00:55,660 --> 01:00:59,830
my data so all real datasets have nonlinearities

968
01:01:00,480 --> 01:01:04,200
the strength of my concrete for this amount of aggregate is here and the strength

969
01:01:04,220 --> 01:01:07,310
of my concrete this amount of aggregate is here is not going to be the

970
01:01:07,310 --> 01:01:11,120
case that if i add more and more aggregate strength goes on forever i'm not

971
01:01:11,120 --> 01:01:14,410
going to be able to build concrete with infinite strength at some point it will

972
01:01:14,410 --> 01:01:17,540
saturate and for almost any sort of physical

973
01:01:17,560 --> 01:01:21,120
real system things don't go off to infinity so

974
01:01:21,220 --> 01:01:22,660
make assuming that

975
01:01:22,680 --> 01:01:28,850
linear trendlines will extend forever is always about assumptions the nonlinearities in problems

976
01:01:28,970 --> 01:01:31,930
here's the fake data that the linear

977
01:01:32,680 --> 01:01:35,160
and the code i used to fit it

978
01:01:36,560 --> 01:01:38,200
how did this happen

979
01:01:38,220 --> 01:01:42,100
i invented new fake features so

980
01:01:42,160 --> 01:01:47,270
i had my data set of inputs and i had one input in this case

981
01:01:47,310 --> 01:01:49,470
and the bias term

982
01:01:49,500 --> 01:01:51,180
and then

983
01:01:51,220 --> 01:01:54,290
to the ones on the input features i also added the square of the input

984
01:01:54,290 --> 01:01:56,040
from the keyboard the input

985
01:01:56,060 --> 01:01:57,520
all the way up to

986
01:01:57,580 --> 01:02:01,720
the sixth order polynomial of so now and pretending that my data set has seven

987
01:02:01,720 --> 01:02:05,870
columns and even though it's really only one dimensional datasets

988
01:02:07,220 --> 01:02:08,250
and then

989
01:02:08,250 --> 01:02:10,990
completely determined by electrons

990
01:02:11,040 --> 01:02:14,520
because they are the so-called majority carriers

991
01:02:14,570 --> 01:02:16,240
so i hope that now you

992
01:02:16,250 --> 01:02:20,750
i understand and i have a feeling why we call it majority carriers because there

993
01:02:21,420 --> 01:02:28,290
fourteen orders of magnitude the concentration is fourteen orders of magnitude larger

994
01:02:28,360 --> 01:02:31,330
the of electrons then of evolves

995
01:02:31,340 --> 01:02:33,220
of course this is also

996
01:02:33,240 --> 01:02:39,430
illustrated in the so-called band diagram by shifting the fermi level

997
01:02:39,460 --> 01:02:46,100
towards the conduction band and intrinsic material thing to say

998
01:02:46,100 --> 01:02:50,160
the fermi level occupies the position more or less

999
01:02:50,220 --> 01:02:51,430
in the middle

1000
01:02:51,450 --> 01:02:54,790
of the band gap

1001
01:02:56,150 --> 01:02:58,440
making the semiconductor

1002
01:02:58,450 --> 01:03:01,320
mark period and type the fermi level

1003
01:03:02,620 --> 01:03:03,930
two words

1004
01:03:03,940 --> 01:03:08,330
the conduction band and why

1005
01:03:08,330 --> 01:03:11,280
do you know what this

1006
01:03:11,330 --> 01:03:12,600
fermi level

1007
01:03:12,620 --> 01:03:14,100
or fermi energy

1008
01:03:14,110 --> 01:03:20,370
represents the semiconductor material

1009
01:03:20,420 --> 01:03:23,100
it actually tells you all the

1010
01:03:25,800 --> 01:03:28,890
the energy of electron

1011
01:03:28,910 --> 01:03:30,500
in the material is

1012
01:03:30,500 --> 01:03:36,080
of the valence electron and so you can understand when we have more electrons

1013
01:03:36,090 --> 01:03:38,420
the higher

1014
01:03:38,440 --> 01:03:41,420
positions when we put them

1015
01:03:41,430 --> 01:03:48,110
in the conduction band was high energies and also the average energy of these electrons

1016
01:03:48,110 --> 01:03:49,670
will go up

1017
01:03:49,690 --> 01:03:51,410
so the

1018
01:03:51,450 --> 01:03:56,780
this is represented by fermi level so the fermilab has to go up

1019
01:03:57,000 --> 01:03:59,580
of course when we do it

1020
01:03:59,600 --> 01:04:02,740
the other way around italy

1021
01:04:02,750 --> 01:04:05,060
have these holes

1022
01:04:05,070 --> 01:04:06,650
so actually what we do

1023
01:04:06,670 --> 01:04:08,950
we have

1024
01:04:14,330 --> 01:04:18,500
so let's now take a ten to twenty so

1025
01:04:18,580 --> 01:04:21,460
one thousand times more boron atoms

1026
01:04:21,570 --> 01:04:24,520
in this example phosphorus atoms

1027
01:04:24,570 --> 01:04:25,360
then we know

1028
01:04:25,380 --> 01:04:29,040
what the concentration of always will be ten to twenty

1029
01:04:29,040 --> 01:04:32,080
from this equation we can easily calculate

1030
01:04:32,090 --> 01:04:35,970
how many electrons we have in this material you see around two

1031
01:04:36,040 --> 01:04:39,400
it's almost no free electrons in such material

1032
01:04:41,150 --> 01:04:42,210
so it means

1033
01:04:42,220 --> 01:04:44,100
most of the electrons now

1034
01:04:44,120 --> 01:04:46,800
see in the valence band

1035
01:04:46,800 --> 01:04:49,550
so the average

1036
01:04:49,600 --> 01:04:53,550
the energy of electrons will go down and we will see it by moving the

1037
01:04:53,550 --> 01:04:57,720
fermi level towards the valence band is this clear

1038
01:05:00,660 --> 01:05:03,520
which one

1039
01:05:05,900 --> 01:05:08,530
no these are not random random

1040
01:05:10,830 --> 01:05:16,580
OK you see this is the distance of the firm level from the

1041
01:05:16,600 --> 01:05:19,260
the maximum energy of the valence band

1042
01:05:19,300 --> 01:05:26,330
and it depends on the concentration so once you change the concentration

1043
01:05:26,350 --> 01:05:28,470
then also this distance

1044
01:05:28,520 --> 01:05:32,100
will change

1045
01:05:32,110 --> 01:05:38,300
so now using again this band diagram is a nice model to illustrate some of

1046
01:05:38,300 --> 01:05:39,630
the processes

1047
01:05:39,680 --> 01:05:42,870
we know that the band gap of

1048
01:05:42,920 --> 01:05:47,410
for example crystalline silicon is one point one electron world so when we have a

1049
01:05:47,410 --> 01:05:52,520
photon with a certain energy

1050
01:05:53,240 --> 01:05:56,860
depending on the energy of the photon

1051
01:05:56,900 --> 01:05:59,900
electron hole pair will be generated or not

1052
01:05:59,910 --> 01:06:01,170
so what is the

1053
01:06:01,200 --> 01:06:02,950
limit for the energy

1054
01:06:02,960 --> 01:06:05,150
it is the energy of the band gap

1055
01:06:05,200 --> 01:06:06,300
now again

1056
01:06:07,010 --> 01:06:12,360
in reality is a little bit more complicated because you probably remember we have direct

1057
01:06:12,360 --> 01:06:18,280
and indirect semiconductors indirect semiconductors photon energies enough

1058
01:06:18,910 --> 01:06:23,840
generate electron hole pair in indirect semiconductors where the

1059
01:06:23,860 --> 01:06:26,890
maximum of the energy level

1060
01:06:26,960 --> 01:06:34,310
is above the minimum of the conduction band in indirect semiconductors this is not true

1061
01:06:34,310 --> 01:06:40,250
so we need also the phonon interaction it means the interaction we

1062
01:06:40,260 --> 01:06:43,100
semiconductor lattice is to get this

1063
01:06:45,670 --> 01:06:52,300
electron hole generation but schematically we need the photon that has at least

1064
01:06:52,340 --> 01:06:58,610
the energy of the band gap of the semiconductor to generate these electron hole pair

1065
01:06:58,630 --> 01:07:03,060
when we have a photon that has for example free electron world

1066
01:07:04,260 --> 01:07:05,890
two electrons was

1067
01:07:05,950 --> 01:07:09,010
electron was more than the band gap of silicon

1068
01:07:09,080 --> 01:07:10,570
it will generate

1069
01:07:10,580 --> 01:07:11,750
electron hole pair

1070
01:07:11,770 --> 01:07:14,860
which at the beginning has

1071
01:07:14,910 --> 01:07:17,900
the difference in energy free electron world

1072
01:07:17,940 --> 01:07:23,770
but in interaction with the lettuce electrons want to go to the energy which is

1073
01:07:25,510 --> 01:07:29,890
and the holds also the minimum energy for holes so actually they will come to

1074
01:07:29,890 --> 01:07:31,810
see in the energy

1075
01:07:33,130 --> 01:07:34,680
that are the

1076
01:07:34,700 --> 01:07:38,270
the bottom of the valley the conduction band and the top of the valence but

1077
01:07:38,460 --> 01:07:42,250
so finally these electron hole pair will help

1078
01:07:42,250 --> 01:07:46,710
the energy only one point one electron what so we are losing almost two electron

1079
01:07:46,710 --> 01:07:47,860
world by

1080
01:07:47,860 --> 01:07:49,550
with respect to another one

1081
01:07:49,570 --> 01:07:51,440
but that's just not very helpful

1082
01:07:57,460 --> 01:08:01,110
o use english

1083
01:08:01,460 --> 01:08:04,920
these services which have

1084
01:08:06,980 --> 01:08:10,760
how does this is

1085
01:08:10,780 --> 01:08:13,780
so in space you can define measures on hilbert space

1086
01:08:13,820 --> 01:08:16,090
but what you cannot define is a measure that

1087
01:08:16,110 --> 01:08:22,210
basically does does not change that is that is flat that does not change no

1088
01:08:22,210 --> 01:08:23,880
matter how it shifted around

1089
01:08:23,900 --> 01:08:27,420
can years

1090
01:08:27,530 --> 01:08:32,320
what we're

1091
01:08:40,400 --> 01:08:42,650
so basically you need to finite dimension

1092
01:08:42,670 --> 01:08:47,400
and kind of as an intuition for why it doesn't work in

1093
01:08:47,440 --> 01:08:52,440
inherit space doesn't immediately say anything about translation invariance but the problem was volume in

1094
01:08:52,440 --> 01:08:56,650
infinite dimensional spaces and say you have

1095
01:08:56,690 --> 01:08:58,260
so you're looking at

1096
01:08:58,320 --> 01:08:59,380
and q

1097
01:08:59,400 --> 01:09:04,880
it was actually angst say one-half so in one dimension that's just an an interval

1098
01:09:04,920 --> 01:09:06,570
the volume of that is

1099
01:09:06,590 --> 01:09:07,590
one half

1100
01:09:07,610 --> 01:09:12,590
in two dimensions it directly or it's a square

1101
01:09:12,610 --> 01:09:15,460
and the volume of that is

1102
01:09:15,480 --> 01:09:16,670
one four

1103
01:09:17,400 --> 01:09:20,090
three dimension the volume is one and so on

1104
01:09:20,150 --> 01:09:23,340
so if you take let if you keep the edge length fixed

1105
01:09:23,360 --> 01:09:26,230
and let the dimension go to infinity

1106
01:09:26,300 --> 01:09:28,050
that converges to zero

1107
01:09:28,090 --> 01:09:32,130
or anything that smaller than one any the edge length smaller than one it converges

1108
01:09:32,130 --> 01:09:33,550
to zero

1109
01:09:33,570 --> 01:09:37,130
for one it always stays one for anything larger than one

1110
01:09:37,150 --> 01:09:38,710
it converges to infinity

1111
01:09:38,820 --> 01:09:41,570
and if you take any finite dimensional

1112
01:09:41,570 --> 01:09:45,980
you always get a well defined notion of volume but basically if you take this

1113
01:09:45,980 --> 01:09:48,110
if you take this this limit to infinity

1114
01:09:48,130 --> 01:09:52,070
then you have three kinds of cubes in three dimensions right those is what the

1115
01:09:52,070 --> 01:09:54,900
results when one of those things that one

1116
01:09:54,920 --> 01:09:59,800
and the olympic measure basically what it does is it's defined in a somewhat similar

1117
01:09:59,800 --> 01:10:01,050
manner two

1118
01:10:01,240 --> 01:10:05,340
to the way that the riemann integral that you all know is defined in terms

1119
01:10:05,340 --> 01:10:09,120
of step functions of limits of the functions and basically what it does is conning

1120
01:10:10,070 --> 01:10:12,730
and but in a limited sense so if you have a smooth volume and you

1121
01:10:12,730 --> 01:10:14,150
want to take the measure of that

1122
01:10:14,240 --> 01:10:18,440
the new approximate that william by bye cubes to make smaller and smaller

1123
01:10:18,610 --> 01:10:21,670
but this whole notion of the volume of a cube is is a big problem

1124
01:10:21,670 --> 01:10:27,440
in an image

1125
01:10:27,460 --> 01:10:31,550
more questions

1126
01:10:36,210 --> 01:10:42,320
please don't feel intimidated by the name of the theorem OK it's it's very it's

1127
01:10:42,340 --> 01:10:44,530
very simple and very useful here

1128
01:10:44,530 --> 01:10:50,690
it's not necessarily simple to prove but it's it's easy enough to understand so that

1129
01:10:50,690 --> 01:10:52,570
the question is

1130
01:10:52,590 --> 01:10:56,400
when you when you look at this the question that arises immediately is

1131
01:10:56,400 --> 01:10:58,820
if we have these two measures given

1132
01:10:58,880 --> 01:11:01,740
is density we

1133
01:11:01,760 --> 01:11:06,230
repositories matters so if their density that transforms one into the other

1134
01:11:07,900 --> 01:11:10,800
and the theorem here tells us when that is the case

1135
01:11:10,820 --> 01:11:13,240
and basically what it says it is

1136
01:11:13,260 --> 01:11:14,400
if you

1137
01:11:14,420 --> 01:11:15,840
you have to make one

1138
01:11:15,860 --> 01:11:19,860
kind of basic sanity cheque and if that works out there is the density

1139
01:11:21,190 --> 01:11:23,480
and that works as follows so if you

1140
01:11:23,480 --> 01:11:25,650
if you think of of this

1141
01:11:26,590 --> 01:11:31,730
of this reweighting idea as you you generate one measured by reweighting another one

1142
01:11:31,740 --> 01:11:33,280
and that means that

1143
01:11:33,300 --> 01:11:36,800
if if this measure here is zero somewhere on the set

1144
01:11:37,530 --> 01:11:39,500
then this measure you must also be there

1145
01:11:39,510 --> 01:11:45,440
because you can never transform something that is zero by the that's that is zero

1146
01:11:45,480 --> 01:11:51,210
into something nonzero by reweighting with something finite that won't work

1147
01:11:52,090 --> 01:11:56,070
so that obviously can't work if this measure somewhere

1148
01:11:56,160 --> 01:11:59,210
where this measure is not then they can't be a density

1149
01:11:59,210 --> 01:12:00,150
that's a very

1150
01:12:00,170 --> 01:12:01,460
basic observation

1151
01:12:02,530 --> 01:12:06,650
and now what is identical in theorem tells us simply if unless this is the

1152
01:12:06,650 --> 01:12:09,360
case there's always the density

1153
01:12:09,380 --> 01:12:14,860
so that's very satisfactory that's the kind of result you like to see

1154
01:12:14,880 --> 01:12:17,690
OK so what it says is if

1155
01:12:18,000 --> 01:12:23,570
this this kind of property years is formalized with

1156
01:12:23,820 --> 01:12:29,420
was fancy symbol and it has name is called absolute continuity

1157
01:12:29,420 --> 01:12:31,840
and i i wouldn't bother you with that

1158
01:12:32,340 --> 01:12:36,360
if it wasn't so so only present in the literature so if you're looking to

1159
01:12:36,360 --> 01:12:40,940
the literature and you see absolute continuity all it means is simply simply this property

1160
01:12:41,880 --> 01:12:46,280
this measure is that this measure mu two is said to be absolutely continuous with

1161
01:12:46,280 --> 01:12:48,090
respect to the measure mu one

1162
01:12:49,510 --> 01:12:50,960
if this doesn't happen

1163
01:12:51,940 --> 01:12:55,030
whenever this measure zero this one is there as well

1164
01:13:05,030 --> 01:13:06,050
yes there is

1165
01:13:06,050 --> 01:13:11,050
there is a connection to the usual notion of continuity that's right and there's also

1166
01:13:11,050 --> 01:13:11,980
in this

1167
01:13:11,980 --> 01:13:17,860
in this notation here there is some connection to the usual notion of differentiability

1168
01:13:19,170 --> 01:13:20,570
i would rather

1169
01:13:20,590 --> 01:13:21,800
take that offline

1170
01:13:34,420 --> 01:13:40,940
the the reason why

1171
01:13:40,960 --> 01:13:44,320
why you never see a bayes equation

1172
01:13:44,360 --> 01:13:46,280
for the dirichlet process

1173
01:13:46,300 --> 01:13:48,400
it's because between the prior

1174
01:13:49,570 --> 01:13:52,030
the posterior of the dirichlet process

1175
01:13:52,050 --> 01:13:58,840
this relationship breaks down i would say more on that later

1176
01:14:02,440 --> 01:14:07,820
now after about ten minutes left to tell you

1177
01:14:07,820 --> 01:14:12,960
how probability is formalized this framework we construct stochastic processes in infinite dimensional spaces and

1178
01:14:12,980 --> 01:14:15,000
how to construct a gaussian process

1179
01:14:20,800 --> 01:14:23,400
the idea how that works is the following

1180
01:14:23,420 --> 01:14:24,690
we have some

1181
01:14:24,710 --> 01:14:27,880
we have some space over here

1182
01:14:30,420 --> 01:14:32,500
but you think of my station my hair

1183
01:14:32,550 --> 01:14:37,900
as per unit it may only illustrate that

1184
01:14:37,920 --> 01:14:44,960
if you if you want to convince people that you seriously injures theory then then

1185
01:14:44,960 --> 01:14:47,300
you do your illustrations with x fig

1186
01:14:47,300 --> 01:14:53,110
the following content is provided under creative commons license your support will help MIT opencourseware

1187
01:14:53,110 --> 01:14:57,200
continue to offer high quality educational resources for free

1188
01:14:57,210 --> 01:15:02,040
to make a donation or to view additional materials from hundreds of MIT courses

1189
01:15:02,070 --> 01:15:12,460
his visit MIT opencourseware OCW that MIT that EDU

1190
01:15:13,390 --> 01:15:15,510
let's get started

1191
01:15:15,530 --> 01:15:18,640
one announcement

1192
01:15:18,690 --> 01:15:21,580
one announced test three

1193
01:15:21,630 --> 01:15:24,060
the celebration of learning

1194
01:15:24,100 --> 01:15:26,280
it's going to be on wednesday

1195
01:15:26,310 --> 01:15:28,250
which means no lecture

1196
01:15:28,250 --> 01:15:31,390
the lecture one stansted the celebration

1197
01:15:31,630 --> 01:15:37,140
please go to your rooms as assigned here you're writing eleven o'clock

1198
01:15:37,180 --> 01:15:39,070
probably in this room right now

1199
01:15:39,070 --> 01:15:41,530
and those in the room assignments

1200
01:15:41,570 --> 01:15:43,650
they are the same as for test two

1201
01:15:43,670 --> 01:15:46,110
not the same as for test one we move

1202
01:15:46,220 --> 01:15:50,290
more people out of ten to fifteen give a little more room here

1203
01:15:50,400 --> 01:15:52,270
and there's plenty room of these others

1204
01:15:52,330 --> 01:15:56,580
those the right at one o'clock not go to the normal lecture room six one

1205
01:15:56,580 --> 01:15:59,300
twenty were writing and twenty six one hundred

1206
01:16:00,050 --> 01:16:01,720
and the coverage is right here

1207
01:16:04,960 --> 01:16:07,780
upon the the website so

1208
01:16:07,860 --> 01:16:09,390
you should know

1209
01:16:09,440 --> 01:16:10,680
what we're

1210
01:16:10,690 --> 01:16:12,050
going to be

1211
01:16:12,050 --> 01:16:15,770
examining you one and the same comments made before

1212
01:16:15,800 --> 01:16:17,210
you know i want to

1213
01:16:17,220 --> 01:16:20,610
give you feedback let you know how you doing with your study

1214
01:16:20,640 --> 01:16:24,860
methods are effective or not it's an attempt to

1215
01:16:24,940 --> 01:16:29,300
re-test admission to MIT or anything like that if you've done your work should do

1216
01:16:29,300 --> 01:16:30,160
very well

1217
01:16:30,270 --> 01:16:31,720
if you haven't done your work

1218
01:16:31,770 --> 01:16:35,270
you shouldn't do very well and will be able to tell you so

1219
01:16:35,280 --> 01:16:39,630
so just remember what we've said in the past please take some time read the

1220
01:16:39,630 --> 01:16:41,250
whole exam

1221
01:16:41,300 --> 01:16:43,630
do these questions for you

1222
01:16:43,640 --> 01:16:46,530
the ones that you find easiest to those first

1223
01:16:46,550 --> 01:16:50,850
but let's let's be honest with you know this is high-school anymore

1224
01:16:50,890 --> 01:16:54,720
so i mean some people tell me after the second test your head have a

1225
01:16:54,720 --> 01:16:57,770
cheaper hardware used will think about it you think i want to give you an

1226
01:16:58,580 --> 01:17:00,110
and give you a question

1227
01:17:00,110 --> 01:17:05,050
that requires you to take something off the age she transferred to the answer page

1228
01:17:05,060 --> 01:17:07,050
it's not pattern recognition

1229
01:17:07,090 --> 01:17:10,510
this is the medical school for quite a while you know you've to think here

1230
01:17:10,510 --> 01:17:11,990
you've got to think

1231
01:17:14,010 --> 01:17:16,740
you know in what is now the third test

1232
01:17:16,760 --> 01:17:20,080
it's going to be more and more thinking and less and less

1233
01:17:20,110 --> 01:17:24,860
role although will have some confidence builders and they i don't want to knock people

1234
01:17:26,040 --> 01:17:27,920
after a balance

1235
01:17:28,550 --> 01:17:32,170
so work work parametrically

1236
01:17:32,210 --> 01:17:35,860
try not to immediately start punching in numbers

1237
01:17:35,860 --> 01:17:40,260
and if you don't do question in great detail

1238
01:17:40,270 --> 01:17:43,960
right you know tell me what you do quite some energy is

1239
01:17:44,020 --> 01:17:46,460
minimize something to maximize something

1240
01:17:46,520 --> 01:17:48,890
give us a sense that you have

1241
01:17:48,960 --> 01:17:50,650
graphs with the material

1242
01:17:53,450 --> 01:17:55,180
keep your eyes on your paper

1243
01:17:56,240 --> 01:18:00,480
our effort is going to be to get those graded on wednesday in bacteria recitation

1244
01:18:01,140 --> 01:18:02,300
on thursday

1245
01:18:04,580 --> 01:18:09,960
i think that's an remember please bring bring your five items we're we're having more

1246
01:18:09,960 --> 01:18:12,020
and more people showing up minus

1247
01:18:12,060 --> 01:18:13,430
periodic table

1248
01:18:13,460 --> 01:18:17,680
table of constants of something to write with that people ask me for pounds for

1249
01:18:18,870 --> 01:18:22,230
no one has asked me if renergie sullivan

1250
01:18:22,310 --> 01:18:24,640
and had to help anybody on that one

1251
01:18:25,710 --> 01:18:29,050
good well today what i wanna do is

1252
01:18:29,080 --> 01:18:34,560
started new unit i want to talk today about organic chemistry now this is this

1253
01:18:34,580 --> 01:18:39,680
going to be the the most minimal introduction to organic chemistry with reason we talk

1254
01:18:39,680 --> 01:18:45,140
about organic chemistry all is in order to prepare ourselves for more solid state chemistry

1255
01:18:45,140 --> 01:18:50,710
specifically talk about powers and ultimately we talk about biochemistry

1256
01:18:50,730 --> 01:18:54,550
because even though this is solid state chemistry we as

1257
01:18:55,830 --> 01:18:58,480
beings are solid state devices

1258
01:18:58,490 --> 01:19:00,170
were made of soft matter

1259
01:19:00,240 --> 01:19:01,680
this is the polymer

1260
01:19:01,770 --> 01:19:03,750
this is a

1261
01:19:03,780 --> 01:19:08,610
semiconductor band gap of two to three electron volts but it's not made out of

1262
01:19:08,690 --> 01:19:10,410
three five semiconductor

1263
01:19:10,450 --> 01:19:14,840
nature's figured out a way of doing this without

1264
01:19:14,880 --> 01:19:16,630
in organic

1265
01:19:17,690 --> 01:19:20,970
so we need to know a little bit but if you really wanted organic chemistry

1266
01:19:20,970 --> 01:19:25,110
we have to take five twelve so i can't teach one fifteen minutes with people

1267
01:19:25,870 --> 01:19:29,610
in one semester so let's get a few definitions under our belts

1268
01:19:29,860 --> 01:19:36,130
organic chemistry is the chemistry of compounds containing both carbon and hydrogen

1269
01:19:36,180 --> 01:19:39,180
not just carbon carbon and hydrogen so diamond

1270
01:19:39,200 --> 01:19:41,490
graphite or not organic

1271
01:19:41,540 --> 01:19:47,070
because they contain carbon hydrogen and what makes these two elements special that

1272
01:19:47,160 --> 01:19:51,320
that they figure so prominently in living

1273
01:19:51,340 --> 01:19:55,250
organisms first of all hydrogen hydrogen is the

1274
01:19:55,260 --> 01:19:58,170
element with the lowest atomic number

1275
01:19:58,240 --> 01:20:02,480
lowest atomic number and when it loses its electron it

1276
01:20:02,490 --> 01:20:05,980
changes character dramatically we've talked about this in recent

1277
01:20:06,030 --> 01:20:10,290
lectures were we have simply a proton

1278
01:20:10,340 --> 01:20:15,240
if is capable of forming covalent bonds but when it finds itself in a compound

1279
01:20:15,240 --> 01:20:21,420
forming a covalent bond something that strongly electronegative it's so denuded of electrons even in

1280
01:20:21,420 --> 01:20:23,680
often don't do much better

1281
01:20:23,700 --> 01:20:26,270
maybe a little bit better than a lot better

1282
01:20:26,280 --> 01:20:27,690
and the new methods

1283
01:20:27,980 --> 01:20:32,630
and the linear methods are immensely much faster because instead of having to compare

1284
01:20:33,770 --> 01:20:38,240
of image each window to maybe a thousand or two thousand kernels

1285
01:20:38,550 --> 01:20:43,390
we simply dot product that the descriptors to extract from that

1286
01:20:43,420 --> 01:20:45,910
with some simple

1287
01:20:46,890 --> 01:20:48,920
that causes the detection school

1288
01:20:48,980 --> 01:20:52,160
so the nearest theme tend to dominate

1289
01:20:52,180 --> 01:20:55,910
these kinds of detection tasks you're evaluating things lot

1290
01:20:56,200 --> 01:20:58,290
many many windows in each image

1291
01:20:58,310 --> 01:21:03,240
a lot faster and they work could roughly as well as the new ones

1292
01:21:03,290 --> 01:21:06,680
OK so this is the detector when you're running it you do the same thing

1293
01:21:06,680 --> 01:21:11,440
scanning at multiple scales for the support vector machine tickets detections out

1294
01:21:11,790 --> 01:21:14,270
so that's

1295
01:21:14,280 --> 01:21:15,560
pretty straightforward

1296
01:21:16,030 --> 01:21:21,540
now if you look at a few run linear SVM you can look back and

1297
01:21:21,540 --> 01:21:23,550
see with coefficients for large

1298
01:21:23,560 --> 01:21:28,140
in figure out from that which the wavelets were important

1299
01:21:28,160 --> 01:21:34,360
and work out exactly what it is to detect actually chewing on tobacco detections and

1300
01:21:34,380 --> 01:21:38,480
you find that fact what it's doing is simply doing kind of

1301
01:21:38,490 --> 01:21:40,500
approximate ten plate

1302
01:21:40,520 --> 01:21:45,170
matching to the rough out of the silhouette of the person

1303
01:21:45,180 --> 01:21:46,730
so in fact

1304
01:21:46,740 --> 01:21:51,000
even though the support vector machine and use different descriptive we're not doing something is

1305
01:21:51,010 --> 01:21:53,570
very different from the previous methods

1306
01:21:53,820 --> 01:21:59,990
but that's OK we've got you know maybe slightly better learning method

1307
01:22:00,000 --> 01:22:04,580
probably the performance is slightly better but it's not it's not very different

1308
01:22:05,270 --> 01:22:09,530
that happens quite often just moved to actions of

1309
01:22:14,670 --> 01:22:17,250
another kind of message

1310
01:22:18,070 --> 01:22:19,330
so this is

1311
01:22:19,340 --> 01:22:23,340
face detector was made about a decade ago now

1312
01:22:24,630 --> 01:22:25,830
what it does

1313
01:22:25,840 --> 01:22:30,450
is in each detection window calculates the set of wavelet coefficients

1314
01:22:30,460 --> 01:22:33,400
a large number of positions and scales

1315
01:22:33,710 --> 01:22:35,780
and that's it

1316
01:22:35,790 --> 01:22:41,490
it's the probabilistic method rather than a machine learning method builds distributional model

1317
01:22:41,590 --> 01:22:46,390
the distribution of responses you expect on the positive class the negative class

1318
01:22:46,410 --> 01:22:48,310
tries to use that to do

1319
01:22:49,810 --> 01:22:53,160
the problem with that is that if you have many coefficients many ways it's in

1320
01:22:53,210 --> 01:22:57,210
calculating very high dimensional distribution you need to work on that

1321
01:22:57,340 --> 01:22:58,670
you can't

1322
01:22:58,680 --> 01:23:05,130
you physically counterexample build histogram that distribution it just has too many regions if typically

1323
01:23:05,130 --> 01:23:10,230
got the only features the right here but there certainly hundreds if not thousands of

1324
01:23:11,160 --> 01:23:15,960
each of which is the real numbers you can't physically stored table with thousands of

1325
01:23:17,420 --> 01:23:18,510
OK so

1326
01:23:18,530 --> 01:23:23,310
this detect so so so the classic machine learning way of dealing with best so

1327
01:23:23,310 --> 01:23:25,760
can use my i naive bayes

1328
01:23:25,790 --> 01:23:28,520
so i do something with each of the individual

1329
01:23:29,710 --> 01:23:35,190
maybe all histogram some probabilistic model the it's responses on the positive class is this

1330
01:23:35,190 --> 01:23:39,670
is the negative class and that will give me my first initial model which we

1331
01:23:39,670 --> 01:23:41,790
you can do that it doesn't work very well

1332
01:23:42,060 --> 01:23:47,020
it's it's too simplistic car the wavelet to to correlated with one another

1333
01:23:47,030 --> 01:23:51,560
so what this detector does is it makes a kind of compromise solution

1334
01:23:51,620 --> 01:23:53,680
this is OK

1335
01:23:53,700 --> 01:23:57,870
it's a that i want to build histogram based distribution

1336
01:23:57,880 --> 01:24:02,320
so be sampling everything of building tables over empirical

1337
01:24:02,340 --> 01:24:07,960
positive and negative and i want to build tables which are kind of

1338
01:24:07,980 --> 01:24:11,190
a size it's convenient to fit into memory but is going to be big enough

1339
01:24:11,190 --> 01:24:13,030
to have a lot of discrimination and that

1340
01:24:13,400 --> 01:24:16,630
so the size the talk was about one hundred thousand cells

1341
01:24:18,840 --> 01:24:23,260
what can you do with that well if i quantise my descriptors very very coarsely

1342
01:24:23,290 --> 01:24:25,390
maybe into just two or three values

1343
01:24:25,710 --> 01:24:30,860
then i can have some power of two or three to whatever power gets ten

1344
01:24:30,860 --> 01:24:36,180
thousand cells it turns out you do the sums you're five to eight

1345
01:24:38,790 --> 01:24:42,110
well let's see you can include the single histogram

1346
01:24:42,130 --> 01:24:45,430
and still build a probabilistic model that will fit into memory it will fit into

1347
01:24:46,260 --> 01:24:49,320
we can evaluate it quickly without too much too many problems

1348
01:24:49,340 --> 01:24:55,200
but it will it will capture the joint statistics of those five to eight life

1349
01:24:56,660 --> 01:24:58,800
so this is what the thing does

1350
01:24:58,810 --> 01:25:00,740
it takes its of wavelets

1351
01:25:00,760 --> 01:25:02,260
in fact it looks

1352
01:25:02,260 --> 01:25:03,370
OK so

1353
01:25:03,560 --> 01:25:06,370
but first pieces

1354
01:25:06,370 --> 01:25:10,550
i want to look at the OCR collection of science magazine from nineteen ninety to

1355
01:25:10,550 --> 01:25:13,020
two thousand so

1356
01:25:13,080 --> 01:25:14,560
just to give you an idea

1357
01:25:14,590 --> 01:25:16,710
of what the status

1358
01:25:16,760 --> 01:25:19,460
is the basically

1359
01:25:19,620 --> 01:25:22,470
have you all seen j store

1360
01:25:22,470 --> 01:25:24,140
that work

1361
01:25:24,170 --> 01:25:30,340
OK so j store is a an online archive of scientific articles and what they

1362
01:25:30,340 --> 01:25:33,190
do is an amazing project is they

1363
01:25:33,220 --> 01:25:38,330
take the original bound articles and they scan them and they run them through optical

1364
01:25:38,330 --> 01:25:44,690
character recognition software and then the index the original scans the the output of the

1365
01:25:46,220 --> 01:25:48,500
so the

1366
01:25:48,540 --> 01:25:53,130
it's cool because so you know it'll screw stuff up like this is two columns

1367
01:25:53,130 --> 01:25:57,140
and maybe they're OCR software can handle two columns and it may be it doesn't

1368
01:25:57,140 --> 01:26:00,470
get all the words right but it's pretty accurate and maybe the punctuation is totally

1369
01:26:00,470 --> 01:26:03,430
bogus but that's OK but for

1370
01:26:03,460 --> 01:26:06,340
doing search when you want to search for word like

1371
01:26:06,370 --> 01:26:11,870
you know phenotype then as long as they got phenotype a bunch of times in

1372
01:26:11,870 --> 01:26:15,040
the article you're going to get the right skin back and since you never interact

1373
01:26:15,050 --> 01:26:19,430
specifically with the noisy OCR very effective system for

1374
01:26:19,440 --> 01:26:25,730
searching online for searching articles online that are archived and they have signs all the

1375
01:26:25,730 --> 01:26:28,920
way back to eighteen eighty one taxi that later on

1376
01:26:28,920 --> 01:26:30,810
OK but of course

1377
01:26:30,840 --> 01:26:34,270
j store has problem which is that they have millions and millions of articles but

1378
01:26:34,270 --> 01:26:37,310
they're only organised by journal and by day

1379
01:26:37,340 --> 01:26:41,930
and what they want is a system where people can can go through and browse

1380
01:26:41,930 --> 01:26:47,260
and examine these articles topic oriented way but simply have skin so many articles etc

1381
01:26:47,260 --> 01:26:51,800
etc out of the question to to annotate the scans of keywords moreover and they

1382
01:26:51,800 --> 01:26:55,290
want to be automatic they they don't want to have to read every article and

1383
01:26:55,290 --> 01:26:57,830
assigned keywords the whole point is that you don't have to do that you can

1384
01:26:57,830 --> 01:27:02,960
just put them through machine and then led scholars use them effectively so there are

1385
01:27:02,960 --> 01:27:06,930
interested in looking at the kind of topic the composition of their archives so here

1386
01:27:06,930 --> 01:27:11,520
we're taking their collection of science magazine for ten years looking at seventeen thousand documents

1387
01:27:11,830 --> 01:27:18,430
this is eleven million words and we used a vocabulary of twenty thousand terms basically

1388
01:27:18,430 --> 01:27:25,090
a practical point you typically remove stopwords words like that of but or and and

1389
01:27:25,090 --> 01:27:29,540
very rare words which don't affect inference model

1390
01:27:29,550 --> 01:27:34,930
or somebody recently told me might affect the inference that we talk about that later

1391
01:27:34,940 --> 01:27:39,470
but the so this is the doctrine for analyzing we theta one hundred topic LDA

1392
01:27:39,470 --> 01:27:43,470
model using variational inference but could have been

1393
01:27:43,470 --> 01:27:46,500
OK so here's that same article that i showed you

1394
01:27:46,510 --> 01:27:49,640
i should say that this article was actually in the test set so this article

1395
01:27:49,640 --> 01:27:53,350
we didn't

1396
01:27:53,380 --> 01:27:54,630
we didn't fit with

1397
01:27:54,730 --> 01:27:59,340
this is the article how many genes an organism to survive in an evolutionary sense

1398
01:27:59,790 --> 01:28:06,630
and using posterior inference so here the wt and for this article right

1399
01:28:06,680 --> 01:28:09,930
i draw my here the WTEM's

1400
01:28:09,940 --> 01:28:16,430
and with posterior inference this is the expected value of the data given those w

1401
01:28:16,440 --> 01:28:17,420
OK so

1402
01:28:17,470 --> 01:28:22,190
termites the which is just expectation

1403
01:28:23,430 --> 01:28:25,460
given w

1404
01:28:25,470 --> 01:28:28,410
one and topic

1405
01:28:28,420 --> 01:28:30,750
that we

1406
01:28:30,760 --> 01:28:31,920
o i think

1407
01:28:31,940 --> 01:28:33,840
that's it this is an approximation of

1408
01:28:33,930 --> 01:28:35,800
so this is like the

1409
01:28:35,800 --> 01:28:39,300
you need to sort of make assumptions on what the correlations are across

1410
01:28:39,830 --> 01:28:43,790
across samples across data points

1411
01:28:43,810 --> 01:28:46,810
so you could then you could then sort of right the whole thing

1412
01:28:46,870 --> 01:28:50,300
is a very very long vector where you would sort of

1413
01:28:50,320 --> 01:28:53,360
you know if dataset was sort of an n times the matrix

1414
01:28:53,400 --> 01:28:55,220
you could then sort of

1415
01:28:55,240 --> 01:28:57,970
you could then sort of stuck it all is very long vector

1416
01:28:57,970 --> 01:29:01,080
and if if there wasn't if there wasn't any correlations

1417
01:29:01,100 --> 01:29:04,620
across data points across the the and so on and so the point i mean

1418
01:29:04,980 --> 01:29:07,620
i mean the vector y or x y and y j

1419
01:29:07,640 --> 01:29:12,560
if there wasn't and all the labels the dimensional OK now if there wasn't correlations

1420
01:29:12,560 --> 01:29:14,170
between them

1421
01:29:14,220 --> 01:29:17,860
the covariance matrix of the very very long vector that's and times the

1422
01:29:17,860 --> 01:29:21,900
it would be a block diagonal alright and you will have this little blocks

1423
01:29:21,950 --> 01:29:25,830
we then dimensions for a specific data point and then it would be easier across

1424
01:29:25,830 --> 01:29:29,190
the points so that would be a simple way of of dealing with non i

1425
01:29:29,190 --> 01:29:29,970
i d

1426
01:29:30,060 --> 01:29:32,930
case there would sort of make a strong assumption

1427
01:29:32,960 --> 01:29:42,700
i mean he's

1428
01:29:45,030 --> 01:29:48,250
there is a correlation shows

1429
01:29:52,180 --> 01:30:00,360
anyone wants making it comes from the

1430
01:30:00,400 --> 01:30:02,500
so far

1431
01:30:04,970 --> 01:30:10,440
so you think you are right

1432
01:30:16,750 --> 01:30:20,960
x y

1433
01:30:20,970 --> 01:30:24,770
there are

1434
01:30:53,900 --> 01:30:55,480
i we define it yesterday

1435
01:30:59,940 --> 01:31:02,150
you can think of correlation as

1436
01:31:02,160 --> 01:31:04,280
given that you've observed

1437
01:31:04,380 --> 01:31:05,450
feature one

1438
01:31:05,480 --> 01:31:08,920
is that informative about what the output what the value of future two is going

1439
01:31:08,920 --> 01:31:12,180
to be doesn't tell you anything so if you were to would you be able

1440
01:31:12,180 --> 01:31:14,020
to for example one of them

1441
01:31:14,910 --> 01:31:16,190
condition on that

1442
01:31:16,210 --> 01:31:19,640
and then sample independently the other one or or would you actually have to take

1443
01:31:19,640 --> 01:31:23,550
into account what the value of of of that of the first one was or

1444
01:31:24,240 --> 01:31:29,120
now for example whole vectors here you're sampling here you sampling

1445
01:31:29,120 --> 01:31:32,200
vectors set out of size two times times

1446
01:31:32,260 --> 01:31:34,910
one right so this is

1447
01:31:34,940 --> 01:31:37,710
vectors with too little numbers in in right now

1448
01:31:37,740 --> 01:31:41,130
and sampling one of those vectors OK and i know that the values

1449
01:31:41,140 --> 01:31:44,850
in the vector are correlated so what if a simple one of them if i

1450
01:31:45,730 --> 01:31:48,930
this one here and it turns out to be here i have a strong idea

1451
01:31:48,930 --> 01:31:52,690
of what were the other can be right and actually i just wanted to cut

1452
01:31:52,690 --> 01:31:55,810
a slice of discussing here and you will see that the other one the conditional

1453
01:31:55,810 --> 01:31:58,380
distribution actually depends on the value

1454
01:31:58,380 --> 01:31:59,700
that this one took

1455
01:31:59,720 --> 01:32:04,000
if there are correlated otherwise it doesn't depend but now when i when i'm done

1456
01:32:04,000 --> 01:32:06,620
something that one what i'm not going to sample

1457
01:32:06,620 --> 01:32:11,250
a second point in this plane if i said that i'm sampling them independently i

1458
01:32:11,250 --> 01:32:15,550
can actually forget what i wanted to sample it doesn't depend all

1459
01:32:16,330 --> 01:32:18,200
they wouldn't be independent if

1460
01:32:18,210 --> 01:32:22,220
if you were in the was saying if my sampling scheme actually

1461
01:32:22,230 --> 01:32:25,350
sort of took into account what i just sampled OK

1462
01:32:25,390 --> 01:32:29,930
so i'm saying here is the independent

1463
01:32:29,940 --> 01:32:37,200
everyone happy

1464
01:32:38,100 --> 01:32:42,280
so now if we wanted to if we had seen the data and we wanted

1465
01:32:42,280 --> 01:32:46,510
to estimate the mean and the covariance matrix of the gaussians we could do the

1466
01:32:46,510 --> 01:32:48,680
usual thing right so we maximize the

1467
01:32:49,120 --> 01:32:53,680
likelihood and this is the thing that we've got since it's done most comfortably if

1468
01:32:53,680 --> 01:32:55,560
you take logarithms first right

1469
01:32:59,870 --> 01:33:01,890
if you just the math

1470
01:33:01,960 --> 01:33:02,960
i suppose

1471
01:33:02,970 --> 01:33:06,250
it doesn't come as a surprise to anyone that

1472
01:33:06,310 --> 01:33:09,640
the estimate of the mean is the sample mean and the estimate of the covariance

1473
01:33:09,640 --> 01:33:12,210
is just the sample covariance

1474
01:33:15,260 --> 01:33:16,730
so if you remember

1475
01:33:16,740 --> 01:33:18,450
if you remember

1476
01:33:18,450 --> 01:33:21,450
what we said yesterday in the lecture any

1477
01:33:21,500 --> 01:33:25,800
think now any element of this matrix here OK what is it going to be

1478
01:33:27,720 --> 01:33:31,510
if i if i just concentrate on a single element

1479
01:33:31,560 --> 01:33:35,730
if it wasn't diagonal elements is quite quite here is just i go

1480
01:33:35,740 --> 01:33:40,680
i take these dimension of y is instructed the component of the mean and that

1481
01:33:40,700 --> 01:33:42,700
square that i some right

1482
01:33:42,740 --> 01:33:45,970
and if it's an of diagonal elements just to that for forty two

1483
01:33:45,990 --> 01:33:51,260
sort of dimensions involved OK

1484
01:33:51,270 --> 01:33:57,360
right so i remember yesterday when we were talking about tossing a coin and we

1485
01:33:57,360 --> 01:33:59,760
wanted to estimate what the probability of heads

1486
01:33:59,830 --> 01:34:01,240
i was going to be right

1487
01:34:01,540 --> 01:34:04,360
and were saying that the maximum likelihood estimator

1488
01:34:04,390 --> 01:34:08,980
i simply said number of heads divided by number of trials or right

1489
01:34:09,010 --> 01:34:12,180
i we sort of discussed now what happens

1490
01:34:12,270 --> 01:34:14,780
if i toss the coin once

1491
01:34:14,870 --> 01:34:18,110
is the maximum likelihood estimator a good idea

1492
01:34:18,110 --> 01:34:24,270
of the vector w and the separating hyperplane should intersect this connecting line half

1493
01:34:24,320 --> 01:34:27,690
in the middle

1494
01:34:27,710 --> 01:34:30,820
now one thing that i haven't told you yet if you

1495
01:34:30,840 --> 01:34:36,940
look again at this definition of separating hyperplane the hyperplane satisfy this equation and that

1496
01:34:36,940 --> 01:34:41,780
the right this function is negative and positive but of course i could multiply these

1497
01:34:41,790 --> 01:34:43,270
hyperplane with any

1498
01:34:43,320 --> 01:34:50,590
non-zero number and geometric location of the set of points satisfying this equation wouldn't change

1499
01:34:50,760 --> 01:34:58,790
so there's this scaling freedom in the separating hyperplane if i have some non-zero constant

1500
01:34:58,790 --> 01:35:01,060
then the set of points equal to this one

1501
01:35:01,180 --> 01:35:04,020
so this thing here

1502
01:35:04,140 --> 01:35:07,510
parameterisation describes the same hyperplane this one

1503
01:35:07,910 --> 01:35:12,940
and to remove this scaling degree of freedom will say the hyperplane is in canonical

1504
01:35:12,940 --> 01:35:18,700
form with respect to some set of data that top points if the minimum value

1505
01:35:18,700 --> 01:35:20,150
of this function here

1506
01:35:20,210 --> 01:35:20,990
is one

1507
01:35:21,000 --> 01:35:22,640
so will take the modulus

1508
01:35:22,660 --> 01:35:28,430
but real valued quantity here we asked this modulus b one on the closest datapoints

1509
01:35:28,470 --> 01:35:31,430
this means on on these points here

1510
01:35:31,560 --> 01:35:36,310
the modulus of this function should be one

1511
01:35:36,350 --> 01:35:40,480
so let's call it the canonical optimal hyperplane

1512
01:35:40,530 --> 01:35:45,460
the nice thing about canonical representation is that we can all relate to the margin

1513
01:35:45,460 --> 01:35:49,390
of separation to the size of this weight vector w

1514
01:35:49,430 --> 01:35:53,280
so you can see this as follows so here we have the line with this

1515
01:35:53,280 --> 01:35:58,740
function takes the value minus one hand we have line plus one and let's consider

1516
01:35:58,740 --> 01:36:02,900
some points that lie exactly on these two lines x one x two

1517
01:36:02,950 --> 01:36:08,290
so we know that x one satisfies this equation this equation x two satisfies this

1518
01:36:09,620 --> 01:36:13,740
so we can take the difference of these two equations and again use the linearity

1519
01:36:13,740 --> 01:36:18,160
of the product of x one minus x two here

1520
01:36:18,750 --> 01:36:23,240
and then we can divide both sides by the length of w

1521
01:36:23,290 --> 01:36:25,710
and what we have on the left side

1522
01:36:25,820 --> 01:36:30,630
this is just a unit vector in the direction of w so taking the dot

1523
01:36:30,630 --> 01:36:35,770
product between this vector is a unit vector means simply measuring how long is the

1524
01:36:36,790 --> 01:36:39,440
along the direction of w

1525
01:36:39,450 --> 01:36:45,000
this is the one connecting x two to x one we take this vector projected

1526
01:36:45,000 --> 01:36:49,790
onto this direction of w and measure how long in other words we exactly measure

1527
01:36:49,790 --> 01:36:53,730
the size of this margin orthogonal to the hyperplane

1528
01:36:53,810 --> 01:36:55,920
along the direction of w

1529
01:36:56,000 --> 01:36:58,030
so this

1530
01:36:58,070 --> 01:37:01,060
this is the size of the margin and on the right-hand side we have two

1531
01:37:01,060 --> 01:37:03,200
of the length of the so we can see

1532
01:37:03,450 --> 01:37:09,020
the the size of the margin is inversely proportional to the length of w if

1533
01:37:09,020 --> 01:37:14,280
we want to maximize the margin of separation we would have to minimize the length

1534
01:37:14,280 --> 01:37:15,480
of w

1535
01:37:15,520 --> 01:37:21,540
while ensuring that separates the data and then it's in canonical form

1536
01:37:21,560 --> 01:37:23,440
so we're going to do this and the

1537
01:37:23,540 --> 01:37:29,100
the next slide after this i guess maybe two after this before i get to

1538
01:37:29,910 --> 01:37:31,700
i just briefly want to

1539
01:37:31,710 --> 01:37:34,690
give a little bit of motivation why we should actually

1540
01:37:34,700 --> 01:37:36,230
maximise the marginal

1541
01:37:36,280 --> 01:37:40,720
of course you have much more details about this in the children talked a lot

1542
01:37:40,720 --> 01:37:42,280
about statistical learning theory

1543
01:37:42,480 --> 01:37:50,440
so i've deliberately skipped all statistical arguments for this kind of try to give you

1544
01:37:50,440 --> 01:37:52,910
some intuitive argument

1545
01:37:53,080 --> 01:38:00,200
one is this motivation in terms of noise on the patterns you can imagine if

1546
01:38:00,200 --> 01:38:06,990
you had a separation of two classes which has a certain margin indicated barrows sometimes

1547
01:38:06,990 --> 01:38:11,320
people measure the margin from here to here but usually is actually just from the

1548
01:38:11,320 --> 01:38:13,500
from the hyperplane to one side

1549
01:38:13,510 --> 01:38:15,410
but it does not just a factor of two

1550
01:38:15,610 --> 01:38:20,980
so if you have such a separating hyperplane you could imagine that even if you

1551
01:38:20,980 --> 01:38:23,350
put some noise on your patterns

1552
01:38:23,370 --> 01:38:28,220
if the patterns well separated with margin if the noise is smaller than this model

1553
01:38:28,220 --> 01:38:32,470
you would still classified incorrectly so you can think of it as some kind of

1554
01:38:32,470 --> 01:38:35,160
robustness with respect to noise if you want

1555
01:38:35,210 --> 01:38:37,630
another way to think of is the following

1556
01:38:37,990 --> 01:38:42,310
suppose you have some events in data points here of one class here's the other

1557
01:38:42,310 --> 01:38:48,870
class and i suppose you moral have computed the smallest sphere containing data points

1558
01:38:48,890 --> 01:38:50,650
it's called israel are

1559
01:38:50,660 --> 01:38:53,030
for simplicity we assume that

1560
01:38:53,050 --> 01:38:58,010
separation goes through the origin if you then want to

1561
01:38:58,030 --> 01:39:02,840
encode the direction of the hyperplane in this simple two-dimensional example

1562
01:39:03,630 --> 01:39:06,090
what you would have to encode is this angle

1563
01:39:07,430 --> 01:39:10,470
however if you know that

1564
01:39:10,490 --> 01:39:15,040
margin between the two classes at least rho and you can also work out with

1565
01:39:15,040 --> 01:39:17,450
which accuracy you would have to code come out

1566
01:39:17,470 --> 01:39:22,480
it turns out the largest margin is the smaller the accuracy will come up that

1567
01:39:22,480 --> 01:39:27,720
you need to have the largest delta gamma this lack you can allowing the coding

1568
01:39:27,720 --> 01:39:28,120
of gamma

1569
01:39:28,510 --> 01:39:31,410
so this can actually be related to some

1570
01:39:31,420 --> 01:39:36,270
compression ideas the larger the margin separation the fewer bits you need to code the

1571
01:39:36,270 --> 01:39:41,670
direction of the hyperplane in again this can also be related to ideas of learning

1572
01:39:42,500 --> 01:39:46,150
but i'm not going to go into detail and that's just a little points on

1573
01:39:46,150 --> 01:39:47,850
the side

1574
01:39:47,860 --> 01:39:49,700
so we going back to the

1575
01:39:49,840 --> 01:39:54,320
formulation of hard to do the optimal margin hyperplanes

1576
01:39:54,480 --> 01:39:55,520
remember here

1577
01:39:55,530 --> 01:40:00,360
so we have to max minimize the length of w subject to the constraint that

1578
01:40:00,360 --> 01:40:03,270
we separate the data correctly and we have a canonical

1579
01:40:05,460 --> 01:40:07,250
the right this is false

1580
01:40:07,300 --> 01:40:13,290
let me just to that once more

1581
01:40:13,330 --> 01:40:16,390
remember lot of positive points

1582
01:40:17,890 --> 01:40:23,710
function here has to be plus one allowed for the negative points this function has

1583
01:40:23,710 --> 01:40:27,570
to be minus one of smaller cities of extreme ones it was plus one minus

1584
01:40:27,570 --> 01:40:30,770
one but the ones further back here it has to be smaller than minus one

1585
01:40:30,770 --> 01:40:32,240
in the ones

1586
01:40:32,260 --> 01:40:35,330
a lot over here it has to be larger than plus one

1587
01:40:35,510 --> 01:40:38,930
so we can actually write these two constraints compactly

1588
01:40:38,950 --> 01:40:44,070
if we multiply them with the class labels so here the class labels plus one

1589
01:40:44,070 --> 01:40:47,730
so multiplying this with the class labels doesn't change anything

1590
01:40:48,230 --> 01:40:51,840
but if we multiply this one with the class label actually turns out to get

1591
01:40:51,840 --> 01:40:55,350
exactly the performance on the other side so we have a compact way of writing

1592
01:40:55,350 --> 01:40:57,100
these constraints

1593
01:40:57,110 --> 01:40:59,680
and then both constraints can be written

1594
01:41:00,180 --> 01:41:02,450
this is

1595
01:41:02,640 --> 01:41:04,610
so what we want to do is we want to

1596
01:41:04,610 --> 01:41:06,110
the output

1597
01:41:06,130 --> 01:41:08,660
is a

1598
01:41:09,170 --> 01:41:14,900
the output

1599
01:41:14,920 --> 01:41:17,710
is a spanning tree

1600
01:41:22,060 --> 01:41:24,690
by spanning tree we mean x

1601
01:41:24,740 --> 01:41:29,090
all the vertices

1602
01:41:32,290 --> 01:41:35,950
k is going to have a minimum wage

1603
01:41:38,730 --> 01:41:44,040
OK so we can write

1604
01:41:44,050 --> 01:41:45,700
the weight of the tree

1605
01:41:45,730 --> 01:41:47,630
is going to be

1606
01:41:47,800 --> 01:41:49,400
by that we mean the sun

1607
01:41:49,770 --> 01:41:52,210
overall images that are in the tree

1608
01:41:52,230 --> 01:41:56,870
of the way the individual edges

1609
01:42:05,740 --> 01:42:08,930
here i've got a little bit of abuse of notation

1610
01:42:09,420 --> 01:42:13,870
OK which is that instead of writing when i should be writing his w of

1611
01:42:13,870 --> 01:42:15,490
the edge u v

1612
01:42:15,510 --> 01:42:17,950
because this is a mapping from edges

1613
01:42:17,970 --> 01:42:19,420
which would give me

1614
01:42:19,430 --> 01:42:21,390
a double parentheses

1615
01:42:21,430 --> 01:42:25,280
and you know as you know i love to abuse notation

1616
01:42:25,370 --> 01:42:29,600
some examples that extra parentheses because we understand that it's really the weight of the

1617
01:42:30,680 --> 01:42:33,870
OK not the way of the ordered pair

1618
01:42:35,640 --> 01:42:38,470
so that's just just a little

1619
01:42:38,970 --> 01:42:42,490
notational convenience

1620
01:42:42,510 --> 01:42:45,530
OK so one of the things when you start when we do that take on

1621
01:42:45,550 --> 01:42:51,510
examiner notational convenience can make the difference between having a horrible time writing upper problem

1622
01:42:51,530 --> 01:42:53,090
and in time

1623
01:42:53,110 --> 01:42:57,900
we're thinking about what kind of notation you used in writing up

1624
01:42:57,960 --> 01:43:01,450
and writing up solutions to problems and so forth

1625
01:43:01,470 --> 01:43:04,130
OK and just in general and technical communication

1626
01:43:04,170 --> 01:43:08,200
we good notation people understand you you have to poor notation

1627
01:43:09,470 --> 01:43:11,990
nobody pays attention to what you're doing because

1628
01:43:12,010 --> 01:43:15,100
they don't understand what you're saying OK

1629
01:43:15,190 --> 01:43:20,130
so let's do an example

1630
01:43:23,380 --> 01:43:27,870
OK there's

1631
01:43:27,900 --> 01:43:29,620
here's a graph

1632
01:43:30,730 --> 01:43:48,470
think for this somebody asked once if i was inspired by biochemistry or something

1633
01:43:48,480 --> 01:43:52,420
OK well i wasn't just writing these things down

1634
01:43:53,190 --> 01:43:56,590
so here's a graph must give us some edge weights

1635
01:44:12,720 --> 01:44:14,590
there's an edge weights

1636
01:44:14,600 --> 01:44:19,140
and now we want is we want to find a tree

1637
01:44:19,160 --> 01:44:23,330
so we're connected basically square

1638
01:44:24,560 --> 01:44:28,800
such that every vertex is part of the tree

1639
01:44:28,820 --> 01:44:32,220
it's kind the minimum weight possible

1640
01:44:34,680 --> 01:44:39,580
so can somebody suggested me some edges that have to be in this minimum spanning

1641
01:44:40,400 --> 01:44:45,970
yes and i'm good

1642
01:44:45,980 --> 01:44:49,200
nine has to be america's y

1643
01:44:49,220 --> 01:44:52,060
is the only one connecting it to this vertex

1644
01:44:52,110 --> 01:44:56,130
and likewise fifteen has to be in so those both have to be and what

1645
01:44:56,130 --> 01:44:59,000
most likely

1646
01:44:59,020 --> 01:45:01,560
but this means that the needs to know

1647
01:45:01,580 --> 01:45:02,360
to get drink

1648
01:45:17,290 --> 01:45:21,960
so this thing is a little bit too complicated right in so and so

1649
01:45:21,980 --> 01:45:23,380
he many times

1650
01:45:23,380 --> 01:45:25,790
again listen something this

1651
01:45:25,810 --> 01:45:31,020
OK you don't worry you know you're right on complicated things you know you taken

1652
01:45:31,330 --> 01:45:33,840
and sometimes things simply

1653
01:45:36,380 --> 01:45:40,340
in this case in general they didn't as far as i know i'm i struggling

1654
01:45:40,400 --> 01:45:41,110
to a

1655
01:45:42,040 --> 01:45:45,270
what you can do is going true

1656
01:45:45,270 --> 01:45:49,500
your was the current right is the last straw

1657
01:45:49,500 --> 01:45:53,000
we call this lasted minimax

1658
01:45:53,100 --> 01:45:58,730
so now the situation is

1659
01:45:58,830 --> 01:46:03,770
i so the current right the last row so only one in so left

1660
01:46:03,790 --> 01:46:07,100
and the kernel parameter that i'm going to pick

1661
01:46:07,110 --> 01:46:09,210
if the are in so

1662
01:46:10,100 --> 01:46:11,480
what is this

1663
01:46:11,520 --> 01:46:13,210
will also are

1664
01:46:13,230 --> 01:46:15,880
miners in

1665
01:46:15,900 --> 01:46:21,920
and the previous ones goldman simplifies to this so i'm left with this thing a

1666
01:46:21,920 --> 01:46:23,880
very simple formula for this in

1667
01:46:23,900 --> 01:46:26,020
so it turns out this thing

1668
01:46:26,060 --> 01:46:29,270
can be analyzed in many contexts

1669
01:46:29,420 --> 01:46:34,860
and this very simple motivation without divergences are already gets you

1670
01:46:34,860 --> 01:46:38,170
some of the fanciest algorithms that i

1671
01:46:38,170 --> 01:46:40,860
using divergences

1672
01:46:40,860 --> 01:46:44,020
got into four algorithms for

1673
01:46:44,040 --> 01:46:47,960
gulshan in linear regression

1674
01:46:47,960 --> 01:46:54,210
so after seeing this i have high respect for this game theoretic stuff

1675
01:46:54,270 --> 01:47:00,420
OK so it did the right thing for governance linear regression

1676
01:47:00,440 --> 01:47:06,960
let's talk about the elected

1677
01:47:08,460 --> 01:47:11,130
these kind of problems have been studying

1678
01:47:11,900 --> 01:47:14,290
you know fifty sixty years

1679
01:47:14,310 --> 01:47:15,250
it turns out

1680
01:47:15,270 --> 01:47:19,900
if you look at them in the worst case into this very simple things

1681
01:47:20,690 --> 01:47:23,600
you get better

1682
01:47:23,610 --> 01:47:28,730
so i just want to give you this kind of little examples that you might

1683
01:47:28,750 --> 01:47:30,540
i have seen before because

1684
01:47:30,560 --> 01:47:35,270
they show the subtlety of this analysis

1685
01:47:36,310 --> 01:47:40,460
the forward algorithm is kind of just adjusting the counts and one of them is

1686
01:47:41,440 --> 01:47:44,210
coming up

1687
01:47:44,330 --> 01:47:46,540
there two versions

1688
01:47:46,580 --> 01:47:51,190
you put in the second last would be the one adding the one count here

1689
01:47:51,210 --> 01:47:54,900
s is the total number of this we end only case so we have a

1690
01:47:54,920 --> 01:47:57,340
conflict so this is the total number of

1691
01:48:00,630 --> 01:48:01,790
one is heads

1692
01:48:01,790 --> 01:48:05,790
total number ones and then you have half

1693
01:48:05,900 --> 01:48:07,520
he might buy

1694
01:48:07,610 --> 01:48:10,190
number of letters plus one

1695
01:48:10,210 --> 01:48:14,810
this is sort of the best algorithm in this context it turns out

1696
01:48:16,250 --> 01:48:20,190
they is that if you go to the last minimax it has this form

1697
01:48:20,210 --> 01:48:23,840
which is seemingly completely unrelated

1698
01:48:23,860 --> 01:48:25,880
or at least not easily

1699
01:48:25,880 --> 01:48:31,330
related to anything of this type it so not related to maximum likelihood kind of

1700
01:48:31,330 --> 01:48:33,770
estimates in the exponential family

1701
01:48:33,790 --> 01:48:36,100
not obviously at least

1702
01:48:36,150 --> 01:48:41,790
the worst-case regret bounds also law people one placebo now the constant is slightly improved

1703
01:48:42,520 --> 01:48:46,980
whenever you have an application

1704
01:48:47,420 --> 01:48:51,460
that uses these counts well what if you try this out

1705
01:48:51,480 --> 01:48:55,900
and see whether it's slightly better

1706
01:48:55,900 --> 01:48:57,610
so this was

1707
01:48:59,520 --> 01:49:02,360
the fancy of this last step minimax

1708
01:49:02,420 --> 01:49:05,860
and in some cases

1709
01:49:05,860 --> 01:49:08,790
you can even analyse this

1710
01:49:08,790 --> 01:49:10,660
is that we're going to the following test time

1711
01:49:11,140 --> 01:49:13,190
we're gonna take the hidden units we've learned

1712
01:49:13,810 --> 01:49:16,270
and the training time we dropped them out with a probability a half

1713
01:49:16,740 --> 01:49:18,940
the test time we just half their outgoing weights

1714
01:49:19,980 --> 01:49:23,630
so the expected value coming out one vision that stays the same training and test time

1715
01:49:24,530 --> 01:49:28,450
hand if we do that at and we have a softmax the output we are

1716
01:49:28,450 --> 01:49:31,670
exactly taking the geometric mean of all to to the edge models

1717
01:49:32,310 --> 01:49:33,110
just dropped out the math

1718
01:49:35,050 --> 01:49:35,910
but means that

1719
01:49:36,620 --> 01:49:39,030
making the model is just a factor of two big test time

1720
01:49:39,590 --> 01:49:41,760
we can get the exact geometric mean all these models

1721
01:49:42,470 --> 01:49:46,210
and that's that's basically what we couldn't do with neural nets before we couldn't train

1722
01:49:46,220 --> 01:49:48,470
also different models and then has an efficient and test time

1723
01:49:51,060 --> 01:49:52,170
if we have more hidden layers

1724
01:49:52,600 --> 01:49:55,980
we just you drop out and a point five in every level works better on the whole

1725
01:49:56,810 --> 01:50:02,170
ant with more hidden layers if youhave you going way to reach in unit test time

1726
01:50:02,710 --> 01:50:06,040
you're no longer taking the exact geometric mean what you've got

1727
01:50:06,560 --> 01:50:09,700
but it's sort of like mean field you getting approximately that's actually works just fine

1728
01:50:12,850 --> 01:50:14,580
this shows my deep respect for mathematics

1729
01:50:18,640 --> 01:50:19,690
if you have an input there

1730
01:50:21,580 --> 01:50:25,090
you can do the same in the input layer typically it's best not to drop

1731
01:50:25,090 --> 01:50:27,010
by half a drop less than half of them

1732
01:50:27,480 --> 01:50:29,180
and that's already been discovered was

1733
01:50:30,290 --> 01:50:31,860
done in yorkshire benches lab by

1734
01:50:32,380 --> 01:50:34,640
yes sure and she got pascal vincent

1735
01:50:35,840 --> 01:50:37,220
got a french pascoe vessel

1736
01:50:39,060 --> 01:50:40,500
ants i think some other people

1737
01:50:41,970 --> 01:50:45,710
they discover four autoencoders it's a very good idea to drop somebody inputs

1738
01:50:46,990 --> 01:50:49,020
and we just doing the same thing but for the hidden layers as well

1739
01:50:50,460 --> 01:50:52,370
now all if you're familiar with dropout

1740
01:50:52,890 --> 01:50:55,860
is very well known that it works in one special case

1741
01:50:57,750 --> 01:50:58,740
suppose i'm doing

1742
01:50:59,140 --> 01:51:00,030
logistic regression

1743
01:51:00,780 --> 01:51:03,490
and i have a large number of inputs suppose i have say a billion

1744
01:51:04,130 --> 01:51:05,260
i wanted to google over the summer

1745
01:51:05,990 --> 01:51:06,930
have a billion inputs

1746
01:51:08,270 --> 01:51:11,050
and i only have say a billion training examples

1747
01:51:13,340 --> 01:51:16,400
what we could do is we could drop out all but one input

1748
01:51:17,270 --> 01:51:18,870
and then fit a logistic regression model

1749
01:51:21,330 --> 01:51:22,340
and then test time

1750
01:51:23,330 --> 01:51:26,510
this isn't quite the same structure a similar a test time what i could do

1751
01:51:27,080 --> 01:51:31,500
is just divide outputs all these units by end if end a number of units

1752
01:51:34,260 --> 01:51:37,680
if i forget to do the division by that's called naive bayes if i do

1753
01:51:37,680 --> 01:51:42,170
the division by and get the same maximum here two possible answers

1754
01:51:42,690 --> 01:51:46,520
so naive bayes is just a version dropout was very similar to drop out

1755
01:51:46,980 --> 01:51:50,290
and it's well known that it works if you've got if you're liable to overfit

1756
01:51:50,510 --> 01:51:51,960
it works better than logistic regression

1757
01:51:52,490 --> 01:51:55,400
so averaging all these models together which is what many basis during

1758
01:51:55,830 --> 01:51:56,320
is a good idea

1759
01:51:59,920 --> 01:52:03,050
so the question is how well does this drug work if you try some big neural nets

1760
01:52:05,150 --> 01:52:07,320
obviously for a neural net is not overfitting

1761
01:52:07,700 --> 01:52:09,160
you don't need a regularize like this

1762
01:52:12,400 --> 01:52:16,000
but if you and your nets not overfitting that means you're not using a big enough neural net

1763
01:52:17,830 --> 01:52:21,590
this people who say you know you also data so now we don't need to worry about overfitting

1764
01:52:22,310 --> 01:52:24,140
they just don't understand how the brain works

1765
01:52:26,770 --> 01:52:30,590
in the brain you have many many more parameters and have training cases

1766
01:52:31,090 --> 01:52:32,860
you have about ten the nine training cases

1767
01:52:33,290 --> 01:52:35,270
fortunately for me it's about two times ten to the nine

1768
01:52:36,920 --> 01:52:38,820
you've have about ten to the fourteenth parameters

1769
01:52:39,880 --> 01:52:42,870
and that's because synapses are a whole lot cheaper than experiences

1770
01:52:43,690 --> 01:52:47,640
and so you want have lots of synapses per experience and i'm gonna get overfitting problem

1771
01:52:51,070 --> 01:52:53,900
let's look at one experiment done by matisse stressed over

1772
01:52:54,640 --> 01:52:57,140
and he did lots of experiments and drop but this is one of them

1773
01:52:58,850 --> 01:52:59,770
he took timit

1774
01:53:00,700 --> 01:53:02,830
he took a chaldean implementation done by

1775
01:53:03,540 --> 01:53:04,850
and with them over software

1776
01:53:06,420 --> 01:53:08,520
anti trained a standard acoustic model

1777
01:53:09,250 --> 01:53:13,890
i'm sort of deep nets acoustic model and it gets twenty two point seven percent error

1778
01:53:14,410 --> 01:53:16,340
on this particular version of timid

1779
01:53:19,390 --> 01:53:22,710
when you find in the normal way which is just by using a small learning

1780
01:53:22,710 --> 01:53:25,030
rate and backprop if you fine-tune it using

1781
01:53:25,500 --> 01:53:26,460
um dropout

1782
01:53:27,260 --> 01:53:32,300
then you get started with again of small learning rate but use dropout stand nineteen

1783
01:53:32,310 --> 01:53:34,640
we said so that's a ten percent improvement

1784
01:53:35,530 --> 01:53:39,810
or something like that's a small subset okay to really get like ten percent improvement this

1785
01:53:41,150 --> 01:53:41,940
at the time

1786
01:53:42,340 --> 01:53:44,650
but with the record for sort a speaker-independent emit

1787
01:53:45,180 --> 01:53:47,620
people have since cut down to eighteen point seven percent

1788
01:53:48,200 --> 01:53:50,970
and actually someone in my lab got down to seventeen point seven percent

1789
01:53:54,880 --> 01:53:57,270
if you look at what happens during the fine tuning

1790
01:53:57,760 --> 01:54:00,160
this is the standard thing you do with neural nets before

1791
01:54:00,970 --> 01:54:01,490
you would

1792
01:54:02,970 --> 01:54:06,770
do some pre-training then you start fine tuning and i showing you hear these frame

1793
01:54:06,770 --> 01:54:09,620
classification right not the owner which what's different numbers

1794
01:54:10,280 --> 01:54:12,100
and you see the classic early stopping curve

1795
01:54:12,700 --> 01:54:14,130
comes down and then it fits

1796
01:54:14,720 --> 01:54:16,380
this is just various different size models

1797
01:54:19,960 --> 01:54:21,620
if you don't use regularizer

1798
01:54:22,160 --> 01:54:22,900
u overfit

1799
01:54:23,670 --> 01:54:25,810
anne so you should do early stopping and get this

1800
01:54:26,860 --> 01:54:27,500
a minimum here

1801
01:54:28,160 --> 01:54:30,750
but the point is if you drop it just keeps going down

1802
01:54:31,250 --> 01:54:34,230
and actually for the same length of time it gets to about the same place

1803
01:54:35,310 --> 01:54:37,360
so it's not actually slower than early stopping

1804
01:54:39,800 --> 01:54:43,340
it's a but it just keeps coming back and it doesn't go up eventually slightly

1805
01:54:43,340 --> 01:54:46,650
in the end is a bit like boosting which for a long time people didn't

1806
01:54:46,650 --> 01:54:48,680
get what it does get worse in the end but much

1807
01:54:49,180 --> 01:54:50,350
the very gently compared with this

1808
01:54:51,460 --> 01:54:52,940
so there was one very nice example

1809
01:54:53,330 --> 01:54:57,710
there's another example you've heard already but i can't resist mentioning it when you got

1810
01:54:57,710 --> 01:54:59,700
a good result you just have to keep saying it

1811
01:55:01,220 --> 01:55:03,740
there with this competition on recognizing objects in images

1812
01:55:04,930 --> 01:55:06,750
whether the classification task is to

