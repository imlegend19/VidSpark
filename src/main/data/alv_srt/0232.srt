1
00:00:00,000 --> 00:00:02,210
OK good morning welcome to the

2
00:00:02,220 --> 00:00:08,300
the second picture of the day my name is thomas hofmann i would currently at

3
00:00:09,210 --> 00:00:10,470
in switzerland

4
00:00:10,490 --> 00:00:17,430
i think in the program it says google research but that's not quite true i'm

5
00:00:17,430 --> 00:00:22,770
actually director in engineering centre and while i think we have interesting research problems and

6
00:00:23,520 --> 00:00:26,500
some i want to mention here

7
00:00:26,520 --> 00:00:33,560
my work these days is really more about building systems and working with engineering teams

8
00:00:33,860 --> 00:00:36,100
actually putting things in production

9
00:00:36,140 --> 00:00:41,080
OK this is what i've been doing the last three years so some of the

10
00:00:41,080 --> 00:00:45,500
work i will present here sort of pre-dates my google time work going on structured

11
00:00:45,500 --> 00:00:47,320
prediction and then

12
00:00:47,320 --> 00:00:52,940
cover that and some recent work with other people have done in this area as

13
00:00:52,940 --> 00:00:54,350
you will see hopefully

14
00:00:54,360 --> 00:00:58,320
i want to talk about towards the end when i discuss so the first part

15
00:00:58,320 --> 00:01:01,070
of the lecture this morning will mainly be

16
00:01:01,080 --> 00:01:03,110
about the methodology

17
00:01:03,120 --> 00:01:03,950
and then

18
00:01:04,010 --> 00:01:08,810
formal setting and then in the afternoon i will talk more about applications that that

19
00:01:09,790 --> 00:01:13,570
most of the applications will be in information retrieval so you have an

20
00:01:13,590 --> 00:01:17,970
you might imagine sort of you know some of the connections between working at google

21
00:01:17,970 --> 00:01:20,000
and working on this specific

22
00:01:20,010 --> 00:01:25,730
problem we might have in the end enough time to either you know finnish literally

23
00:01:25,730 --> 00:01:29,840
or if you have questions about google things in general use of machine learning in

24
00:01:29,840 --> 00:01:33,280
an industrial setting i'm also happy to answer this

25
00:01:34,230 --> 00:01:36,670
so let me begin to feel free to

26
00:01:36,680 --> 00:01:40,180
o interrupt me and any time so

27
00:01:40,200 --> 00:01:42,310
of course we know you know just two

28
00:01:42,320 --> 00:01:47,560
wake up after the break that supervised machine learning

29
00:01:47,570 --> 00:01:51,710
you know is is a wonderful thing to have where we can learn functional dependencies

30
00:01:51,710 --> 00:01:57,170
between inputs and outputs and we can do this based on training data

31
00:01:57,170 --> 00:02:02,900
by using inductive inference and its most basic form of course we are facing classification

32
00:02:02,900 --> 00:02:07,590
problems and you know you've you've seen probably many examples here you know many examples

33
00:02:07,590 --> 00:02:12,780
in your work classification problems such as optical character recognition where we give you know

34
00:02:12,780 --> 00:02:17,120
given an image of the character map to you know one of the possible digits

35
00:02:17,120 --> 00:02:22,830
all characters and things so for instance in the context of information retrieval probably the

36
00:02:22,830 --> 00:02:29,560
most important application things like document correct the document categorisation we're given

37
00:02:29,580 --> 00:02:37,620
a particular document represented just by the text or some additional features or semi structured

38
00:02:37,620 --> 00:02:41,800
data metadata that we might have to map it to

39
00:02:41,810 --> 00:02:45,680
certain categories so here's an example from my toes where we might not be enough

40
00:02:45,690 --> 00:02:49,180
to things like location code and to certain

41
00:02:49,990 --> 00:02:54,530
provide some topical annotations for instance that this document is money markets foreign exchange markets

42
00:02:54,530 --> 00:02:56,720
and so on

43
00:02:56,740 --> 00:03:00,840
of course in document character is a categorisation

44
00:03:01,340 --> 00:03:07,650
you know just to contrast it with other things you might other approaches that you

45
00:03:07,650 --> 00:03:12,210
might see the advantage of machine learning is that

46
00:03:12,270 --> 00:03:14,150
we can basically harness

47
00:03:14,150 --> 00:03:20,020
the knowledge that humans have like an expert like this one here on

48
00:03:22,870 --> 00:03:28,620
given a particular classification system or particular taxonomy that people might be interested in and

49
00:03:29,660 --> 00:03:34,430
use these training data to feed into mechanism learning machine of some sort

50
00:03:34,460 --> 00:03:40,970
two then actually output using an inference mechanism complicated are now that in practice has

51
00:03:40,970 --> 00:03:42,430
turned out to be more

52
00:03:42,460 --> 00:03:46,810
efficient than say the traditional approaches that you've seen in this area

53
00:03:46,830 --> 00:03:51,180
for instance that have been based on what's called knowledge engineering where people have tried

54
00:03:51,180 --> 00:03:57,330
to design like handcrafted rules of how to classify things right so

55
00:03:57,340 --> 00:04:02,720
so it's interesting to note that right it's not clear priority that for particular problem

56
00:04:02,720 --> 00:04:08,400
machine learning is always the best technique to use but in text categorisation actually has

57
00:04:08,400 --> 00:04:14,750
been quite successful and much more successful than other methods based on knowledge engineering the

58
00:04:14,750 --> 00:04:20,100
advantage of machine learning is pretty clear whereas knowledge engineering right the knowledge that resides

59
00:04:20,100 --> 00:04:21,390
within an expert

60
00:04:21,400 --> 00:04:25,480
to get that out and to get it formalized right is very

61
00:04:25,510 --> 00:04:30,190
difficult task is very hard to achieve good coverage very very hard to achieve good

62
00:04:30,190 --> 00:04:39,340
precision the indirect approach through actually creating training examples has the advantage that really you

63
00:04:39,340 --> 00:04:43,350
know we can just let the expert do what the expertise good namely

64
00:04:43,400 --> 00:04:47,070
you know categorizing things using whatever knowledge he has and we don't have to worry

65
00:04:47,070 --> 00:04:51,340
about the formalisation but rather than we can use an inference mechanism to come back

66
00:04:51,400 --> 00:04:55,630
to get really something that generalizes to new data

67
00:04:57,140 --> 00:05:01,730
and i should also say sorry i should also say that

68
00:05:01,780 --> 00:05:07,040
text categorisation i'm not going to talk much about the plain text categorisation example but

69
00:05:07,040 --> 00:05:13,340
has probably been one of the most successful applications of machine learning in practice and

70
00:05:13,720 --> 00:05:18,670
and if you look at today's systems they actually do reach human level accuracy

71
00:05:18,690 --> 00:05:25,080
in the categorisation performance that you see over realistic taxonomies

72
00:05:25,090 --> 00:05:31,110
often what you notice that the precision of human annotation might be higher in the

73
00:05:31,110 --> 00:05:35,920
sense that if humans has belongs to that class is more reliable typically than ever

74
00:05:35,920 --> 00:05:39,600
machine makes a prediction but what machines are much better

75
00:05:39,630 --> 00:05:44,480
at is the recall aspect of it by taking document water all the annotations that

76
00:05:44,480 --> 00:05:50,320
the document should get if you think about huge taxonomies with many many classes humans

77
00:05:50,320 --> 00:05:51,530
are sometimes likely to

78
00:05:51,920 --> 00:05:56,320
basically omit certain categories because they don't think about it right which is very natural

79
00:05:56,320 --> 00:06:00,210
if you have a huge categories systems machines are usually better at that

80
00:06:00,270 --> 00:06:03,700
part which you can think of as i recall part

81
00:06:03,710 --> 00:06:07,660
OK but this is not what i would like to talk about here mainly i

82
00:06:07,660 --> 00:06:10,290
would like to talk about

83
00:06:10,320 --> 00:06:13,900
problem called structured classification of structured prediction

84
00:06:13,910 --> 00:06:19,590
so by that what i what i mean is really a setting where

85
00:06:19,600 --> 00:06:24,280
what we're trying to predict is not the class variable binary variables or

86
00:06:24,280 --> 00:06:26,470
not even a multiclass problem

87
00:06:26,480 --> 00:06:31,530
but it something more interesting so to give you an idea of what we will

88
00:06:31,530 --> 00:06:35,670
be talking about here and many more examples will follow you can think about all

89
00:06:35,670 --> 00:06:40,290
kinds of structured objects such as sequences strings trees and so on and so forth

90
00:06:40,290 --> 00:06:45,280
and what if you have a machine learning problem the prediction problem where this is

91
00:06:45,280 --> 00:06:48,700
you're doing is effectively asking everybody in the audience too

92
00:06:48,720 --> 00:06:52,770
sample from their prior on what random sequences look like and what you get if

93
00:06:52,770 --> 00:06:54,490
you do that is data like this

94
00:06:54,540 --> 00:06:58,940
so this is data from an experiment i did with kids at the canada USA

95
00:06:59,980 --> 00:07:01,550
which is

96
00:07:01,560 --> 00:07:05,650
run by the mere appearance in my life of their mathematicians whose

97
00:07:05,740 --> 00:07:08,070
attending here

98
00:07:09,100 --> 00:07:13,200
we played this game and this is just a histogram of binary for five binary

99
00:07:13,200 --> 00:07:17,390
sequences that kids produced in this game we can call to get a sense of

100
00:07:17,390 --> 00:07:21,030
justice sequential dependence we can collapse over parity so just you know one one one

101
00:07:21,030 --> 00:07:25,410
one one zero zero zero zero zero same we get data set like this and

102
00:07:25,410 --> 00:07:28,870
we can compare with the original versions experiment which was done not such as an

103
00:07:28,870 --> 00:07:33,850
experiment but is an actual test of psychic powers by zenith radio network in the

104
00:07:33,850 --> 00:07:37,300
golden age of radio they play this game over the airwaves and how people write

105
00:07:37,300 --> 00:07:42,050
down on on postcards the five bit sequences that were being transmitted centers in here's

106
00:07:42,050 --> 00:07:45,950
the data from the zenith radio experiments we can line up with the math camp

107
00:07:45,950 --> 00:07:50,890
data and you can see what is shocking correspondence i think i'm pretty shocked by

108
00:07:51,030 --> 00:07:55,700
this you know my main strength as a scientist is on the theoretical side of

109
00:07:55,700 --> 00:07:59,400
this is this is just one experiment i did has replicator classic result but so

110
00:07:59,400 --> 00:08:02,890
i think this is some of the coolest data that i have collected at least

111
00:08:03,270 --> 00:08:08,240
which maybe isn't seen very much but i think it's amazing how close you know

112
00:08:08,240 --> 00:08:14,000
it's smart map the high school students in the early two thousands come to the

113
00:08:14,050 --> 00:08:18,410
randomness intuitions of typical people in the nineteen thirties

114
00:08:18,430 --> 00:08:20,670
the only difference is the sequence here

115
00:08:20,770 --> 00:08:26,230
the math kids for some reason put down way too many zero zero zero zero

116
00:08:26,230 --> 00:08:30,050
zero maybe they were just being cute or

117
00:08:30,160 --> 00:08:34,220
pains and that any so except for that

118
00:08:34,250 --> 00:08:38,440
a bit of you can imagine the meta cognition they might lead a clever math

119
00:08:38,570 --> 00:08:43,480
kid put that sequence down when you're supposed to be writing down random sequence basically

120
00:08:43,480 --> 00:08:47,540
the same really suggested something deep about how the mind sees patterns here and we

121
00:08:47,540 --> 00:08:51,140
want to try to explain that the way the way i would suggest we explain

122
00:08:51,140 --> 00:08:54,890
this is is very much in line with same what nick was putting out there

123
00:08:54,890 --> 00:08:58,780
in terms of the introspection works or what i think many of the example i

124
00:08:58,780 --> 00:09:03,420
was i was talking before the visual perception example roger shepherd tables show which is

125
00:09:03,420 --> 00:09:05,370
that when we introspect on cognition

126
00:09:05,410 --> 00:09:08,730
and that often means just judgments were asked to make you know in in real

127
00:09:08,730 --> 00:09:13,070
world context and experiment what we're introspective on of the output of the computation not

128
00:09:13,080 --> 00:09:16,450
the intermediate processes are representations so in this case

129
00:09:16,460 --> 00:09:20,960
the out the computations our minds are designed to do i would argue and many

130
00:09:20,960 --> 00:09:25,170
others have argued is some kind of causal inference we were interested in the causal

131
00:09:25,180 --> 00:09:27,960
processes that give rise to data we see

132
00:09:27,970 --> 00:09:32,350
and it's it's a slightly different computation then the question might be

133
00:09:32,390 --> 00:09:35,940
asked if i say which is more likely to have come from original is more

134
00:09:35,940 --> 00:09:39,400
likely to come from a fair coin is more like the question which of these

135
00:09:39,420 --> 00:09:42,390
is more likely to to have been produced by frequent which of these is the

136
00:09:42,400 --> 00:09:46,150
fair coin the better call saul description of so in a sense it's not what

137
00:09:46,150 --> 00:09:49,210
i'm asking about the likelihood of the data given that it's a fair coin were

138
00:09:49,210 --> 00:09:54,530
saying given to hypothesis a fair or unfair how which which of these provide better

139
00:09:54,530 --> 00:09:57,980
evidence for a fair coin when you're going in reverse and doing the causal inference

140
00:09:58,280 --> 00:10:00,800
so this is a nice problem because we can capture this in a very simple

141
00:10:00,800 --> 00:10:04,710
form of bayesian inference which if we just have to hypotheses than bayes rule has

142
00:10:04,710 --> 00:10:09,460
this nice odds for me you just divide you write down the posterior probability for

143
00:10:09,460 --> 00:10:13,360
both hypotheses and take the ratio of this is called the posterior odds and it's

144
00:10:13,360 --> 00:10:17,330
just the product of the prior odds and what's called the likelihood ratio the ratio

145
00:10:17,330 --> 00:10:20,800
of how well the data are predicted by the two hypotheses so we can see

146
00:10:20,800 --> 00:10:23,160
how these analysis would work here

147
00:10:23,190 --> 00:10:26,890
and also how we can use this the basic idea of base to work backwards

148
00:10:27,120 --> 00:10:30,850
to figure out something about the priors in people's heads just to say are why

149
00:10:30,870 --> 00:10:35,710
second recording did in those other studies in perceptions sensorimotor ones

150
00:10:35,720 --> 00:10:39,540
looking at behavior in sort of trying to fill in the black box assuming bayesian

151
00:10:39,540 --> 00:10:42,970
logic so here let's say we have these two pieces of data heads heads tails

152
00:10:42,980 --> 00:10:45,860
tails heads heads heads heads they consider just two

153
00:10:45,920 --> 00:10:51,830
this is just you know it's not very textbook toy example two alternative hypotheses are

154
00:10:51,840 --> 00:10:55,470
fair coin always heads coins and then we can do this analysis with a much

155
00:10:55,470 --> 00:10:59,980
bigger space ultimate hypothesis but then for those two hypotheses what is bayes rule in

156
00:10:59,980 --> 00:11:02,390
this art form look like well

157
00:11:02,420 --> 00:11:06,660
for the sequence heads heads tails heads tails the likelihood is again one of thirty

158
00:11:08,470 --> 00:11:12,360
four fair coin and zero four always had one right because there's no way and

159
00:11:12,360 --> 00:11:16,330
always heads coin is going to produce any tails little in that sequence what about

160
00:11:16,330 --> 00:11:20,190
the priors well those are less accessible on our point here is actually try to

161
00:11:20,190 --> 00:11:24,410
say can we can we can we look at behavior the introspective outputs of this

162
00:11:24,410 --> 00:11:28,950
computation and make some inference about what the priors were well if we plug let's

163
00:11:28,950 --> 00:11:33,030
let's call it along the prior of the fair coin i guess i should have

164
00:11:33,060 --> 00:11:35,830
that's one seems like something small so maybe i should have put the other way

165
00:11:35,830 --> 00:11:38,890
around whatever epsilon to some number between zero and one

166
00:11:38,920 --> 00:11:43,780
then we plug that in here and then we can say well what

167
00:11:43,780 --> 00:11:47,710
what about that's one would produce the judgement that when you look at heads heads

168
00:11:47,710 --> 00:11:50,480
tails heads tails it looks like a fair point it seems to provide very strong

169
00:11:50,480 --> 00:11:55,920
evidence for fair coin to high posterior odds here on quite well this doesn't really

170
00:11:55,920 --> 00:12:00,910
tell us anything because since the data are incapable of coming from the always heads

171
00:12:00,910 --> 00:12:04,390
coin that zero here then this is going to the posterior odds going to be

172
00:12:04,390 --> 00:12:08,450
infinity is going to provide an infinite reason to believe in a fair coin under

173
00:12:08,450 --> 00:12:12,440
this very simple hypothesis space no matter what that's like this but becomes more interesting

174
00:12:12,440 --> 00:12:15,060
if we look at the sequence of all heads because now

175
00:12:15,080 --> 00:12:18,610
depending on how you set one you get a different posterior odds

176
00:12:18,630 --> 00:12:21,630
so if we had more time we could play this game and try to you

177
00:12:21,630 --> 00:12:24,770
know i guess what are absolute values are we should be stopping

178
00:12:24,810 --> 00:12:29,480
right so we will place but six minutes no problem we will not play that

179
00:12:29,500 --> 00:12:33,870
game but instead of just just observe that from for many people when we look

180
00:12:33,870 --> 00:12:37,870
at the sequence of all heads it's slight coincidence but it's not enough to make

181
00:12:37,870 --> 00:12:41,290
us think from o in most cases that is really some trequenu element requires are

182
00:12:41,290 --> 00:12:45,330
pretty rare so if you plug in the value of let's say nine hundred ninety

183
00:12:45,330 --> 00:12:49,920
this is not upsetting that we have to have zero zero one one costs for

184
00:12:49,920 --> 00:12:53,480
the binary potential what controls a zero

185
00:12:53,520 --> 00:12:56,140
OK i think that's just about everything about this

186
00:12:56,190 --> 00:13:01,950
graph is a little fragment of the graph separated up here but everything is just

187
00:13:01,960 --> 00:13:06,190
the same the caicos where gk costs

188
00:13:08,210 --> 00:13:10,440
the canonical form in the way he said

189
00:13:10,500 --> 00:13:11,740
OK so now

190
00:13:12,150 --> 00:13:14,270
we have we associate

191
00:13:16,170 --> 00:13:18,750
and assignment of nodes and the

192
00:13:18,760 --> 00:13:23,420
cost of that assignment with cut the cost of that kind

193
00:13:23,460 --> 00:13:26,520
or equivalently

194
00:13:26,570 --> 00:13:28,300
now we invoke the

195
00:13:28,330 --> 00:13:30,900
i think forty or so years old

196
00:13:30,920 --> 00:13:35,500
ford fulkerson algorithm mincut maxflow which says that

197
00:13:36,560 --> 00:13:40,510
that was not consider all possible cuts through this graph

198
00:13:40,560 --> 00:13:44,250
the the rules are only the cut has to pass between the source and the

199
00:13:44,250 --> 00:13:47,040
sink but otherwise you can rotate you

200
00:13:47,100 --> 00:13:51,820
and then i asked well we have always possible cuts is the minimum cut that

201
00:13:51,820 --> 00:13:54,260
is the one with the minimum cost

202
00:13:54,270 --> 00:13:57,870
and the ford fulkerson algorithm says that

203
00:13:58,840 --> 00:14:00,240
the cost of that car

204
00:14:00,360 --> 00:14:01,470
is equal

205
00:14:01,500 --> 00:14:03,290
so the total

206
00:14:03,320 --> 00:14:05,180
hello that you can force

207
00:14:06,100 --> 00:14:09,130
diagonal that was time hunting ln

208
00:14:09,130 --> 00:14:10,840
you know from

209
00:14:10,870 --> 00:14:16,770
siberia here and you know it distributed across some pipeline to you know germany into

210
00:14:16,770 --> 00:14:19,730
poland and coming out in somewhere near london

211
00:14:19,770 --> 00:14:23,140
here i want to know just you know forcing is much flow through the network

212
00:14:23,140 --> 00:14:26,710
as i can you know that the oil take whatever it did it once i

213
00:14:26,710 --> 00:14:31,710
was going to increase the pressure to make sure that all the pipelines are fully

214
00:14:31,710 --> 00:14:37,410
utilized as far as possible because there might be some transverse pipelines you know like

215
00:14:37,410 --> 00:14:40,900
this one here that use the tall or don't get forty years

216
00:14:40,910 --> 00:14:42,890
but nonetheless i'm going to push and push

217
00:14:42,910 --> 00:14:46,030
and get as much flow through this network as i can

218
00:14:46,050 --> 00:14:50,860
well then the ford fulkerson theorem says the total amount of flow you get by

219
00:14:50,860 --> 00:14:54,470
by pushing as hard as you can see is actually equal to

220
00:14:54,480 --> 00:14:55,820
the minimum cut

221
00:14:55,870 --> 00:15:00,870
but you get by considering all possible cut the network does this make sense

222
00:15:00,910 --> 00:15:05,020
it sort of does because the minimum cut is actually if you like searching for

223
00:15:05,020 --> 00:15:10,460
bottlenecks in some distributed fashion saying you know where you know tell me on aggregate

224
00:15:10,460 --> 00:15:14,920
what is the bottleneck the aggregate bottleneck in this

225
00:15:14,980 --> 00:15:19,980
in this diagram and that bottleneck determines what's the maximum amount of flow that you'll

226
00:15:19,980 --> 00:15:22,960
be able to get through the diagrams of this kind of duality

227
00:15:23,020 --> 00:15:26,170
between the minimum cut the the maximum flow

228
00:15:26,340 --> 00:15:30,360
that's going to be very useful because now we could we have the option of

229
00:15:30,360 --> 00:15:35,260
solving the minimum cut the the energy minimum cost simply by pushing oil through

230
00:15:35,930 --> 00:15:38,320
network and seeing how much we can get through

231
00:15:38,330 --> 00:15:42,300
and more than that we can tell not only what is the energy of the

232
00:15:42,300 --> 00:15:44,830
minimum cut by measuring simply the total

233
00:15:44,840 --> 00:15:49,490
maximum flow we can also get the minimal configurations

234
00:15:49,500 --> 00:15:51,680
simply by looking for all the

235
00:15:51,740 --> 00:15:54,850
links which are saturated

236
00:15:56,870 --> 00:15:58,510
whatever link is

237
00:16:00,010 --> 00:16:03,450
that means that the cut passes through that link

238
00:16:03,470 --> 00:16:04,850
and so the the

239
00:16:04,870 --> 00:16:10,670
note will be assigned to the opposite of the sourcing

240
00:16:10,700 --> 00:16:15,140
i i simply you know i've marked in dark blue here in some putative flow

241
00:16:15,140 --> 00:16:17,120
all the

242
00:16:17,160 --> 00:16:20,220
links which are saturated it could include also

243
00:16:21,120 --> 00:16:25,110
i guess it should have included should but separated these links as well they should

244
00:16:25,110 --> 00:16:26,430
be in dark blue

245
00:16:26,490 --> 00:16:30,260
all of the things which are saturated coming in dark blue

246
00:16:31,080 --> 00:16:34,020
then the couple passes through this link

247
00:16:34,100 --> 00:16:37,630
so you can actually read out from the maximal flow from the quantity of the

248
00:16:37,630 --> 00:16:43,110
flow and also where the flow is saturated you can read out this the solution

249
00:16:43,110 --> 00:16:46,510
we want is actually not the energy that we're interested in the configuration we will

250
00:16:46,660 --> 00:16:51,480
know solving segmentation you want actually which nodes in the foreground and which ones are

251
00:16:52,600 --> 00:16:57,100
OK so that's just you know play with this little bit simple tutorial examples

252
00:16:57,200 --> 00:16:59,280
so i've dreamed up

253
00:16:59,370 --> 00:17:00,400
a problem

254
00:17:00,410 --> 00:17:02,080
in canonical form

255
00:17:02,120 --> 00:17:02,910
so that

256
00:17:02,920 --> 00:17:04,320
the cheese

257
00:17:04,330 --> 00:17:09,490
as i promised have the here and one one values zero and the off diagonal

258
00:17:09,490 --> 00:17:12,930
value set equal

259
00:17:12,950 --> 00:17:15,050
and for the efforts

260
00:17:15,100 --> 00:17:18,350
the you re potentials the only rules are that the

261
00:17:18,390 --> 00:17:21,720
weights have to be positive all these when positive

262
00:17:21,750 --> 00:17:23,490
so i'm solving this problem

263
00:17:23,500 --> 00:17:27,280
one the simple problem i can think of the form former we're interested in

264
00:17:27,340 --> 00:17:29,320
but i'm doing it in the new way

265
00:17:29,370 --> 00:17:32,920
now let's see if i've done everything right

266
00:17:32,930 --> 00:17:34,770
this should be

267
00:17:34,780 --> 00:17:36,930
f one of zero

268
00:17:36,960 --> 00:17:42,830
or maybe roman

269
00:17:42,840 --> 00:17:45,500
i said i could swap inside

270
00:17:47,160 --> 00:17:50,470
cutting this arrow here should be the cost of assigning idea

271
00:17:50,490 --> 00:17:54,420
i think you forgot to swap over the cost of the thing that you do

272
00:17:54,420 --> 00:17:58,540
that actually should be seven that one of the attributes or alternatively the problem of

273
00:17:58,550 --> 00:18:01,250
solved this f one one equals

274
00:18:01,250 --> 00:18:03,320
and give you an interpretation of what's left

275
00:18:04,030 --> 00:18:04,990
so what is left is

276
00:18:05,530 --> 00:18:07,450
so let's assume a special case that

277
00:18:07,890 --> 00:18:08,820
we have a kernel

278
00:18:09,450 --> 00:18:13,270
which is goes in the normalized goes in that has integral one

279
00:18:14,040 --> 00:18:15,330
so it's a valid density model

280
00:18:16,390 --> 00:18:17,790
if this is the case then

281
00:18:19,480 --> 00:18:25,300
this thing is still normalized because this sums over the number of cosets which implies we divide by and plus

282
00:18:26,450 --> 00:18:29,630
so this is a density model and actually says it's as simple as a well

283
00:18:29,630 --> 00:18:33,930
known density model is the so-called parzen windows density estimate of the positive class

284
00:18:35,590 --> 00:18:38,490
this is a parzen windows density estimate of the negative class

285
00:18:39,260 --> 00:18:42,020
and the overall this gives you a classification rule which is

286
00:18:42,490 --> 00:18:48,480
one of the simplest possible classification classification rule to basically you just estimated density so the two classes

287
00:18:49,170 --> 00:18:50,790
and to just check whether all the

288
00:18:51,270 --> 00:18:52,000
density is high

289
00:18:52,780 --> 00:18:53,810
four your test point

290
00:18:54,800 --> 00:18:57,530
under the one model or in the other model so that's

291
00:18:57,970 --> 00:19:00,030
one possible way of classifying data

292
00:19:00,980 --> 00:19:01,330
in the

293
00:19:01,980 --> 00:19:02,790
so that's nothing

294
00:19:04,290 --> 00:19:09,100
no deep methods to classify data but this was when first noticed this was somewhat surprising that

295
00:19:09,790 --> 00:19:12,900
this turns out to be such a nice simple geometric

296
00:19:14,080 --> 00:19:14,720
so it's just

297
00:19:15,670 --> 00:19:17,270
and a nearest mean

298
00:19:17,690 --> 00:19:18,820
classifier if you want

299
00:19:19,570 --> 00:19:20,850
and in the feature space

300
00:19:23,540 --> 00:19:24,750
and also want to say of course

301
00:19:25,280 --> 00:19:29,930
and this construction also works for other types of kernels they don't have to be normalized

302
00:19:30,690 --> 00:19:34,100
they don't even have to be positive so it goes in a causes

303
00:19:34,590 --> 00:19:36,880
not just a positive definite function but it's also

304
00:19:37,470 --> 00:19:41,000
positive everywhere the values that you get out was positive but if you take a

305
00:19:41,000 --> 00:19:43,030
polynomial kernel it can also get negative values

306
00:19:43,720 --> 00:19:48,940
so a positive definite function doesn't have to be positive and we we prove that is positive on the diagonal

307
00:19:49,400 --> 00:19:52,590
but of course of diagonal elements for polynomial kernel can be negative

308
00:19:53,180 --> 00:19:56,380
for the girls in the company but anyway so this is something that's

309
00:19:56,990 --> 00:20:00,810
from that point-of-view modelling past winners but then from another point a is less general because

310
00:20:01,430 --> 00:20:05,420
parzen windows you can also use window functions that are not positive definite kernels that

311
00:20:07,260 --> 00:20:07,690
as long as they

312
00:20:08,210 --> 00:20:10,990
positive and normalized are nonnegative normalized

313
00:20:18,480 --> 00:20:20,050
so it seems i was actually

314
00:20:21,250 --> 00:20:22,560
thinking of not going into

315
00:20:23,430 --> 00:20:25,580
he sees for integrating that's

316
00:20:27,920 --> 00:20:31,440
so let me first just give you the brief idea thespians

317
00:20:32,020 --> 00:20:34,600
and then maybe tomorrow i'll tell you more about experience

318
00:20:42,720 --> 00:20:43,290
this is the map

319
00:20:43,890 --> 00:20:44,790
so i

320
00:20:47,740 --> 00:20:51,670
i'm not sure if i remember correctly but this is the so this is the only volcano

321
00:20:53,480 --> 00:20:55,050
that sorry this is a new one this is the

322
00:20:56,330 --> 00:20:56,870
as the old one

323
00:20:57,560 --> 00:21:00,860
and i think this is the so-called win-win but this is a completely happy

324
00:21:01,480 --> 00:21:04,040
that's that's the old one but that's the one is still active in it

325
00:21:06,220 --> 00:21:06,790
well maybe

326
00:21:07,790 --> 00:21:09,590
here's the inheritance behind this the

327
00:21:11,870 --> 00:21:13,110
and here says new okay

328
00:21:13,670 --> 00:21:14,100
thank you

329
00:21:18,290 --> 00:21:18,790
we always

330
00:21:20,690 --> 00:21:23,330
whatever yes and which is the part that they would talking about film

331
00:21:28,440 --> 00:21:30,650
yeah so that mean maybe some well

332
00:21:33,130 --> 00:21:37,370
so i think they were talking about that they were talking about the company behind you

333
00:21:37,960 --> 00:21:38,760
this said it differently

334
00:21:39,490 --> 00:21:43,290
but so i think that talk about this will come here and this is the one that's still active

335
00:21:43,990 --> 00:21:45,230
and there have been some eruptions

336
00:21:45,750 --> 00:21:49,290
i think the most recent one was down here and the one in the fifties was probably somewhere here

337
00:21:50,180 --> 00:21:52,780
then those nodding his wife is from the canary islands

338
00:21:54,540 --> 00:21:56,520
and i think it is this land is relatively new

339
00:21:57,810 --> 00:22:01,890
and my recollection is that they they worried that half of this thing here will

340
00:22:02,290 --> 00:22:07,110
slide into this ocean and then the big wave will start from here and move in this direction

341
00:22:08,450 --> 00:22:12,990
it might also move into other directions but discovery i

342
00:22:13,650 --> 00:22:16,340
discovery channel it's important that moves in this direction

343
00:22:18,950 --> 00:22:22,560
president is much close of course but then nobody worries about people in brazil there

344
00:22:23,740 --> 00:22:25,190
if they are in this direction

345
00:22:29,000 --> 00:22:30,290
one should be doing about this

346
00:22:31,100 --> 00:22:31,810
and so

347
00:22:35,360 --> 00:22:36,390
so this is a

348
00:22:37,580 --> 00:22:38,630
this is a very short

349
00:22:39,730 --> 00:22:44,130
very short summary of what it means to i'm not sure maybe we'll talk will talk about multi-task tomorrow

350
00:22:45,240 --> 00:22:48,600
but now using this these mean classifier and we can

351
00:22:49,620 --> 00:22:52,330
we can do something slightly more clever so we can stick with

352
00:22:52,780 --> 00:22:53,880
using a hyperplane

353
00:22:55,530 --> 00:22:58,790
the reproducing kernel hilbert space which is what the main classifier was doing

354
00:22:58,790 --> 00:23:00,330
typically by hand

355
00:23:00,420 --> 00:23:02,820
but if you want to use one of the tools

356
00:23:02,830 --> 00:23:07,430
go ahead but then you take what came out of it so you may be

357
00:23:07,480 --> 00:23:09,170
you make the b

358
00:23:09,180 --> 00:23:12,500
rules of the alignment of this is the same as this and this is the

359
00:23:12,500 --> 00:23:17,770
inverse of this business into their own you know into adopt into part of the

360
00:23:17,770 --> 00:23:21,710
documents so in your RDF graphs you have your data

361
00:23:21,750 --> 00:23:25,350
plus the mapping so i'd say about ninety percent of the ones out there right

362
00:23:25,350 --> 00:23:28,070
now we're just OWL same

363
00:23:28,090 --> 00:23:32,100
and in fact they are not even use necessarily

364
00:23:32,120 --> 00:23:34,620
it it really should be called

365
00:23:34,630 --> 00:23:36,890
OWL kind of the same as

366
00:23:36,900 --> 00:23:39,830
but i'll just give you a kind of the same as it only gives you

367
00:23:39,830 --> 00:23:44,730
a formal simplicity ignore the model theory and those are the same as if you

368
00:23:44,740 --> 00:23:48,710
you guys in the i community one live with it they don't really care

369
00:23:48,730 --> 00:23:50,940
because they're using it

370
00:23:50,960 --> 00:23:53,570
behind the web applications works

371
00:23:53,580 --> 00:23:56,750
because they don't need a hundred percent

372
00:23:56,820 --> 00:24:00,780
OK so in in the linked data web so for example DB pedia has been

373
00:24:00,810 --> 00:24:03,830
very popular thing in dbp

374
00:24:03,890 --> 00:24:05,780
is wikipaedia

375
00:24:05,800 --> 00:24:08,300
whenever you have one of these boxes

376
00:24:08,310 --> 00:24:11,290
this is you know this person here's where they were born in this in this

377
00:24:11,290 --> 00:24:16,420
and this is very easy to turn that into a semantic web representation so you

378
00:24:16,420 --> 00:24:21,560
say that a person named albert einstein and it says here he is a scientist

379
00:24:21,690 --> 00:24:26,310
use a subclassof scientists and then says he was born in such and such a

380
00:24:26,310 --> 00:24:28,740
town in germany and so you say

381
00:24:28,870 --> 00:24:31,050
the city was this and that the

382
00:24:31,060 --> 00:24:35,490
the country was with germany and he won the nobel prize and nobel prize that

383
00:24:35,490 --> 00:24:39,600
is awarded nobel prizes nobel prizes and awards

384
00:24:39,610 --> 00:24:41,700
things like that so

385
00:24:41,710 --> 00:24:46,650
and there's a lot of other stuff which essentially the links in the pedia labels

386
00:24:46,820 --> 00:24:49,740
have in which wikipedia has

387
00:24:49,810 --> 00:24:54,370
a fairly large dataset used to be about a million and a half triples and

388
00:24:54,370 --> 00:24:58,610
it's now much larger than the triple against the RDF graph you see a lot

389
00:24:58,610 --> 00:25:00,720
of people's lives

390
00:25:00,730 --> 00:25:05,570
o thing here and then link with the label on it and that three your

391
00:25:05,570 --> 00:25:09,830
eyes or whatever you want to think about but all of these ontology things even

392
00:25:09,840 --> 00:25:12,430
seeing can be realized in that model

393
00:25:12,440 --> 00:25:15,310
so they can build built on top of this so

394
00:25:15,320 --> 00:25:18,890
as an example musicbrainz music ontology

395
00:25:18,900 --> 00:25:20,760
a lot of information about

396
00:25:20,860 --> 00:25:24,530
performers and things like that so maybe you're looking at the performer there

397
00:25:24,610 --> 00:25:29,720
and you say in this guise interesting from me is wikipedia personality asserted these links

398
00:25:29,730 --> 00:25:33,310
to go get the DBT pt information and then you say

399
00:25:33,330 --> 00:25:36,760
all of that says he was born you know you're where i live

400
00:25:36,770 --> 00:25:37,930
what other

401
00:25:37,950 --> 00:25:41,720
people were born near where i live where you can use germany

402
00:25:41,770 --> 00:25:46,130
for that you go over to geonames which turns a lot of countries and cities

403
00:25:46,130 --> 00:25:51,620
until at long's and then you use the lat long reason so conceptually you can

404
00:25:51,620 --> 00:25:56,840
take something like this and imagine a lot of really interesting queries that could be

405
00:25:56,840 --> 00:26:00,820
answered if you had some kind of

406
00:26:00,830 --> 00:26:04,010
front end to this and a lot of people are interested in the front end

407
00:26:04,010 --> 00:26:08,910
to this many people are building browsers so you pointed something and then you follow

408
00:26:08,940 --> 00:26:13,130
but the problem is this thing is growing so big so fast that browsing is

409
00:26:13,130 --> 00:26:14,940
clearly problematic

410
00:26:14,990 --> 00:26:18,710
without some way of or just like the web we can write to once upon

411
00:26:18,710 --> 00:26:20,660
a time when i first use the web

412
00:26:20,720 --> 00:26:21,630
you went to

413
00:26:21,640 --> 00:26:25,760
the web page and then it linked it was just the page with about one

414
00:26:25,760 --> 00:26:30,110
hundred links on it but sent out to the hundred other things on the web

415
00:26:30,240 --> 00:26:33,650
right in their local is that thing got bigger and bigger people came up with

416
00:26:33,660 --> 00:26:37,130
new ways of organizing that information

417
00:26:37,750 --> 00:26:43,300
a browser like tabulator which berners-lee coined by ora lassila there's about a dozen more

418
00:26:43,380 --> 00:26:46,890
that you see a lot of demos on tuesday night at the

419
00:26:46,900 --> 00:26:48,920
poster demo session

420
00:26:48,940 --> 00:26:52,410
those things are starting to get overwhelmed and now they are starting to look at

421
00:26:52,410 --> 00:26:56,870
well how do we do reverse indexing and pagerank so so if it once upon

422
00:26:56,870 --> 00:26:59,050
a time you came to my photo file

423
00:26:59,140 --> 00:27:02,780
and found four friends so it didn't really matter what are you show the

424
00:27:03,720 --> 00:27:08,110
my facebook can be done to to my livejournal can be done to have lots

425
00:27:08,110 --> 00:27:09,830
of other things certainly

426
00:27:09,850 --> 00:27:14,430
i have thousands of people and in fact linked to me and you want them

427
00:27:14,430 --> 00:27:17,910
altered you don't have to go through saying is this is this is this is

428
00:27:17,910 --> 00:27:22,660
this is you like sort who some kind of ordering so

429
00:27:22,680 --> 00:27:26,760
so a lot of the question this cloud has been how do we use it

430
00:27:26,770 --> 00:27:30,450
but the interesting thing from an application point of view

431
00:27:30,470 --> 00:27:34,730
so this is open this is available you can start playing with it tomorrow

432
00:27:37,110 --> 00:27:41,350
if you're sitting here you can't because the conductivity is in good faith that you

433
00:27:41,350 --> 00:27:43,460
can play

434
00:27:43,480 --> 00:27:47,110
this the website but what's interesting about this thing is

435
00:27:47,130 --> 00:27:51,960
it was a couple of hundred thousand triples a year and a half ago

436
00:27:51,980 --> 00:27:56,430
a couple billion triples at the beginning of this year and twenty three billion last

437
00:27:56,430 --> 00:28:00,950
time i heard the number which was i think beginning in the summer

438
00:28:01,000 --> 00:28:05,180
so it's growing by orders of magnitude as people dumb stuff now i don't know

439
00:28:05,180 --> 00:28:07,120
if it's still growing at rates

440
00:28:07,280 --> 00:28:10,150
what mean you can get your hands

441
00:28:10,160 --> 00:28:11,880
and more data

442
00:28:11,900 --> 00:28:15,370
then you can put into any of the tools you're likely to get your hands

443
00:28:15,370 --> 00:28:17,740
on right now very easily

444
00:28:17,790 --> 00:28:21,210
so a lot of stuff so a lot of people who are worried about

445
00:28:21,260 --> 00:28:24,150
the chicken and then what will cause them so there's a bit of a mess

446
00:28:24,150 --> 00:28:27,990
out there you hear a lot of you good to database conference say oh haha

447
00:28:27,990 --> 00:28:31,740
there's RDF store still have trouble with more than a million triples

448
00:28:31,760 --> 00:28:36,030
in fact that was true in two thousand six

449
00:28:36,040 --> 00:28:41,190
right now there is also upstairs on tuesday night you also see the billion triple

450
00:28:41,190 --> 00:28:45,930
challenge where we made one point one billion that's ten to nine

451
00:28:45,980 --> 00:28:50,930
available one of the UK companies said they were very disappointing

452
00:28:50,980 --> 00:28:55,870
because england a billion is a million million ten to the twelfth and they thought

453
00:28:55,870 --> 00:28:58,410
that would be a challenge but they

454
00:28:58,440 --> 00:29:04,740
routinely host for other companies tend to the nineteenth onto the triples now again there's

455
00:29:04,740 --> 00:29:08,710
not a lot inferencing there's not a lot of ontology in space we're still talking

456
00:29:08,710 --> 00:29:10,240
about data level

457
00:29:12,060 --> 00:29:13,690
the other thing going on

458
00:29:13,720 --> 00:29:17,860
is this always coevolution and technology

459
00:29:17,870 --> 00:29:21,500
so at the same time you've seen this you know we we go up the

460
00:29:21,500 --> 00:29:26,560
ontology and logic theory that was an interesting idea

461
00:29:26,650 --> 00:29:31,430
and conceptual right but if you come out to the real world nowadays there's a

462
00:29:31,430 --> 00:29:33,340
lot of other stuff happening

463
00:29:33,350 --> 00:29:38,280
so OWL is one of several different ways of doing sort of knowledge is stuff

464
00:29:38,300 --> 00:29:41,400
but also when you have RDF you need a way to query so there's the

465
00:29:41,400 --> 00:29:43,120
standard called particles

466
00:29:43,130 --> 00:29:46,210
some other things i show you main i just thought this was when i was

467
00:29:46,220 --> 00:29:49,020
for another talk i was giving

468
00:29:49,040 --> 00:29:52,300
this morning i was pulling through small slides but no one had ever seen this

469
00:29:52,300 --> 00:29:55,500
before this is the first layer cake diagram

470
00:29:55,500 --> 00:30:01,030
whereas in normal distributions with rather flat but if powers norovirus so you know that

471
00:30:01,070 --> 00:30:04,380
everything just pick to the mean when right

472
00:30:04,390 --> 00:30:07,010
so you have a big issue up here

473
00:30:07,020 --> 00:30:09,310
that means that you're pretty much certain

474
00:30:09,330 --> 00:30:11,520
about what you guys OK the prediction

475
00:30:11,530 --> 00:30:14,890
it's rather reliable

476
00:30:16,400 --> 00:30:19,900
you have this kind of

477
00:30:19,900 --> 00:30:21,450
spread parameter set

478
00:30:21,510 --> 00:30:22,890
you can do a lot of sense

479
00:30:22,900 --> 00:30:23,780
for example

480
00:30:23,790 --> 00:30:27,390
we can all modify this correlation and regression trees

481
00:30:27,400 --> 00:30:30,630
to solving this label ranking problem

482
00:30:32,310 --> 00:30:35,290
two major modifications as well

483
00:30:35,310 --> 00:30:39,470
it's about the speedy criteria right how speak the tree

484
00:30:39,490 --> 00:30:45,390
so in the regression tree this is based on this standard duration

485
00:30:45,400 --> 00:30:46,670
and here

486
00:30:46,680 --> 00:30:49,190
we can just using special permit instead

487
00:30:49,760 --> 00:30:51,850
we can define goodness of fit

488
00:30:51,860 --> 00:30:54,380
in terms of the set of parameters

489
00:30:54,470 --> 00:30:57,490
that you just speak the truth

490
00:30:57,500 --> 00:31:02,150
using this goodness of fit

491
00:31:02,170 --> 00:31:04,030
a second what application is

492
00:31:04,070 --> 00:31:07,880
above this stopping criterion for partition the tree OK

493
00:31:07,890 --> 00:31:08,800
we now

494
00:31:08,820 --> 00:31:11,080
so far currently with stop two

495
00:31:11,100 --> 00:31:12,480
partition the tree

496
00:31:12,530 --> 00:31:15,350
even first the tree is pill

497
00:31:15,370 --> 00:31:16,570
OK all

498
00:31:16,580 --> 00:31:18,860
the number of labels node is too small

499
00:31:18,880 --> 00:31:23,310
OK because he the case is made those says two parts for any

500
00:31:23,370 --> 00:31:25,980
OK that was studied in the

501
00:31:27,000 --> 00:31:29,610
if you do the whole training that to

502
00:31:29,850 --> 00:31:31,470
labour looks

503
00:31:31,560 --> 00:31:34,580
approximate look like this OK this

504
00:31:34,650 --> 00:31:40,260
quite similar as a regression tree the differences in the in the leaves you rankings

505
00:31:40,270 --> 00:31:43,360
instead of revenue numbers

506
00:31:45,950 --> 00:31:51,320
now i will show you some experimental results the here is that the site or

507
00:31:51,330 --> 00:31:53,070
the resulting one big table

508
00:31:53,080 --> 00:31:57,880
we compare our local politics namely this

509
00:31:57,900 --> 00:31:59,990
no coupled with label approach

510
00:32:00,010 --> 00:32:03,050
this label ranking trees compare them with this

511
00:32:03,070 --> 00:32:05,480
constant complication

512
00:32:07,530 --> 00:32:09,490
there are too many messages here

513
00:32:10,570 --> 00:32:12,280
we found that

514
00:32:12,300 --> 00:32:14,780
if they decide is rather good

515
00:32:15,400 --> 00:32:17,580
if the ranking is a complete

516
00:32:18,120 --> 00:32:20,510
the meta labels and also high

517
00:32:20,530 --> 00:32:22,070
we found out of this

518
00:32:22,360 --> 00:32:24,230
it's based label ranking

519
00:32:24,260 --> 00:32:27,290
is significantly better

520
00:32:27,310 --> 00:32:29,580
the second observation is

521
00:32:29,610 --> 00:32:31,820
we found love with all of that

522
00:32:31,830 --> 00:32:34,060
so the size of the neighbouring country

523
00:32:34,070 --> 00:32:35,440
isn't that big

524
00:32:35,610 --> 00:32:37,250
this is the columns

525
00:32:37,430 --> 00:32:40,480
after this they were tree

526
00:32:40,490 --> 00:32:44,800
it is the relative size of the neighbouring country compared

527
00:32:44,820 --> 00:32:49,000
so convolutional decision tree which is train only on the top label

528
00:32:49,650 --> 00:32:52,280
from the size is not

529
00:32:52,300 --> 00:32:54,250
this is a rather go essay

530
00:32:54,260 --> 00:32:56,930
so here

531
00:32:56,940 --> 00:32:58,330
it is

532
00:32:58,360 --> 00:33:00,000
but to the learning curve

533
00:33:00,020 --> 00:33:04,650
of the local polish compare with this constant classification

534
00:33:05,770 --> 00:33:07,010
we found that

535
00:33:07,980 --> 00:33:10,400
one local poachers of local metals

536
00:33:10,490 --> 00:33:11,890
a more flexible

537
00:33:11,900 --> 00:33:16,060
so it can exploit more information this information

538
00:33:16,110 --> 00:33:18,570
compared with a model based approach

539
00:33:18,600 --> 00:33:20,770
while this model based approach

540
00:33:20,780 --> 00:33:22,130
has some home

541
00:33:22,150 --> 00:33:24,950
the stronger bias this see this

542
00:33:24,980 --> 00:33:29,720
so it's lack of flexibility

543
00:33:31,530 --> 00:33:36,520
and i pretty much done here here's some among conclusion

544
00:33:36,530 --> 00:33:38,270
we have introduced

545
00:33:38,290 --> 00:33:39,820
and it's this approach

546
00:33:39,830 --> 00:33:41,150
for label ranking

547
00:33:41,170 --> 00:33:45,610
well we're using published model namely the models models

548
00:33:45,740 --> 00:33:52,010
and once you give this label ranking problem of four probabilistic treatment you can gather

549
00:33:52,010 --> 00:33:54,610
all the news useful information for example

550
00:33:54,620 --> 00:33:56,920
this set of parameters

551
00:33:58,080 --> 00:33:59,670
so the set of parameters

552
00:33:59,680 --> 00:34:00,930
i can tell you

553
00:34:00,940 --> 00:34:03,280
how reliable your prediction is

554
00:34:03,290 --> 00:34:09,330
OK and our approach can deal with complete and that's where this incoming rec well

555
00:34:09,330 --> 00:34:11,810
OK we can deal with pose case

556
00:34:11,820 --> 00:34:13,160
there are of course

557
00:34:13,180 --> 00:34:15,450
some future work we want to

558
00:34:16,600 --> 00:34:18,430
namely it is

559
00:34:20,490 --> 00:34:22,540
approximation for this

560
00:34:22,560 --> 00:34:27,450
in country ranking case we want to make this inference more efficient is is rather

561
00:34:27,450 --> 00:34:28,680
complex problems

562
00:34:28,700 --> 00:34:31,150
they're always space to improve

563
00:34:31,170 --> 00:34:32,770
the second is

564
00:34:32,780 --> 00:34:35,690
the approach i propose here

565
00:34:35,720 --> 00:34:36,940
it's not only

566
00:34:36,970 --> 00:34:39,020
useful for label ranking problem

567
00:34:39,040 --> 00:34:41,800
but it can some other type of problems as well

568
00:34:42,530 --> 00:34:44,870
so we will we also like to see

569
00:34:44,930 --> 00:34:48,930
what we can do in the future for the type of problems

570
00:34:48,950 --> 00:34:50,630
OK so basically

571
00:34:50,630 --> 00:34:52,630
this is all from my side

572
00:34:52,650 --> 00:34:56,490
and if you have any problems i like to answer

573
00:35:06,580 --> 00:35:13,140
it just makes you speak any of questions always try that before

574
00:35:13,270 --> 00:35:17,230
they are really remains

575
00:35:17,300 --> 00:35:19,220
one is my

576
00:35:21,590 --> 00:35:24,420
we more of

577
00:35:24,440 --> 00:35:28,610
o with a

578
00:35:28,630 --> 00:35:31,190
you were

579
00:35:34,300 --> 00:35:36,070
very easily

580
00:35:36,080 --> 00:35:42,440
the fact that we're beginning because we don't have very explicit pruning strategy for the

581
00:35:44,070 --> 00:35:46,520
we somehow expect that

582
00:35:46,620 --> 00:35:48,130
the size of the tree

583
00:35:48,200 --> 00:35:49,970
this the big

584
00:35:50,830 --> 00:35:54,480
and i think is this question right well that was one of

585
00:35:56,520 --> 00:36:00,220
but in fact with it turns out that the size of the tree

586
00:36:00,320 --> 00:36:02,780
not as because we imagine

587
00:36:02,880 --> 00:36:05,990
right you can see here this the relative size of the tree

588
00:36:06,030 --> 00:36:09,840
it is rather save comparable with what is true

589
00:36:09,870 --> 00:36:11,030
OK so

590
00:36:11,040 --> 00:36:13,460
overfitting is not

591
00:36:13,470 --> 00:36:16,560
all this is not a little bit of experiment

592
00:36:16,590 --> 00:36:20,800
as well as for the news label poachers

593
00:36:20,810 --> 00:36:25,990
because many labour voters you just have to control how big your neighborhood is

594
00:36:26,000 --> 00:36:29,980
OK if you have a good control of the neighborhood then is also not the

595
00:36:29,990 --> 00:36:32,280
likely you of video data

596
00:36:32,300 --> 00:36:36,030
but it does that of question

597
00:36:47,710 --> 00:36:49,490
OK the question is why

598
00:36:49,890 --> 00:36:52,060
it uses models model

599
00:36:52,890 --> 00:36:54,780
first for that

600
00:36:54,800 --> 00:36:59,060
this model is more is kind of the standard also golden standard statistics to dealing

601
00:36:59,060 --> 00:37:00,630
with the ranking data

602
00:37:00,680 --> 00:37:03,170
and there already had been done

603
00:37:03,180 --> 00:37:04,450
on this model is more

604
00:37:04,470 --> 00:37:07,640
this is a somewhat simple reason

605
00:37:07,660 --> 00:37:12,770
but of course you could also use some other type of models for dealing ranking

606
00:37:13,690 --> 00:37:15,450
this is not a problem

607
00:37:15,470 --> 00:37:20,050
you could also try some other model of course as well

608
00:37:20,080 --> 00:37:21,290
OK so

609
00:37:21,310 --> 00:37:24,000
so to called

610
00:37:24,000 --> 00:37:28,690
sample so time essentially the same particle in this weighted empirical measure

611
00:37:28,990 --> 00:37:31,470
so that's the idea

612
00:37:34,050 --> 00:37:38,430
one thing which is nice but that is that if you were to use stallman

613
00:37:39,840 --> 00:37:46,910
sequential importance sampling on whether sample before that you add your son paul where

614
00:37:46,930 --> 00:37:50,180
distributed according to the policy distribution qn

615
00:37:50,940 --> 00:37:56,150
on the weights with correcting for the discrepancy between pi and you know if your

616
00:37:56,150 --> 00:37:58,120
son paul OK

617
00:37:58,120 --> 00:38:01,330
you obtain this new approximation

618
00:38:01,380 --> 00:38:07,750
of the time solution that the new sample you ever up for immediately essentially distributed

619
00:38:07,750 --> 00:38:10,890
according to the target distribution pi

620
00:38:10,910 --> 00:38:15,450
there are statistically dependent was going make the analysis of such algorithm much more complicated

621
00:38:15,580 --> 00:38:16,940
but basically

622
00:38:17,000 --> 00:38:20,660
at least one done by doing this for sampling is that you managed to eliminate

623
00:38:20,660 --> 00:38:27,200
particles with low weights multiply particle evaluates on obtain approximately distributed according to the target

624
00:38:27,420 --> 00:38:30,160
so one way to arrive at the beach

625
00:38:30,170 --> 00:38:32,220
prod small

626
00:38:32,290 --> 00:38:33,440
to clear away

627
00:38:33,440 --> 00:38:38,410
it is to set essentially what you've been doing is that if you sampled

628
00:38:38,420 --> 00:38:41,760
capital in times from these discrete distribution

629
00:38:43,150 --> 00:38:47,010
build this associated on empirical measure

630
00:38:47,030 --> 00:38:49,550
then this corresponds simply

631
00:38:49,560 --> 00:38:53,030
to approximate the original target distribution

632
00:38:53,040 --> 00:38:55,830
by the following day OK so what does it mean here

633
00:38:55,870 --> 00:39:01,250
so remember that before resampling i was editing as as i was associating to each

634
00:39:02,410 --> 00:39:09,410
i await which was essentially wn OK now what you've done essentially when or so-called

635
00:39:09,660 --> 00:39:12,210
capital timeform described empirical measures

636
00:39:12,230 --> 00:39:14,830
essentially corresponds to

637
00:39:17,350 --> 00:39:19,800
the particle exercise basically

638
00:39:20,610 --> 00:39:25,770
number of time which is given by NI OK i have been given to each

639
00:39:25,770 --> 00:39:26,840
of these could p

640
00:39:27,270 --> 00:39:29,610
o weight equal to one of

641
00:39:30,240 --> 00:39:35,160
basically this is one way process to be clear to for two two

642
00:39:35,160 --> 00:39:39,630
to think about the reason why we've have been doing is that essentially at time

643
00:39:39,630 --> 00:39:43,580
and we've attributed to particle one

644
00:39:43,590 --> 00:39:48,190
two on and we've attributed number of copy of spring if you want which is

645
00:39:48,200 --> 00:39:52,690
are in one and two and capital and on obviously because it's still there have

646
00:39:52,690 --> 00:39:57,800
been sampled capital and they've been sampled from the discrete distribution deal distributed according to

647
00:39:57,800 --> 00:40:04,230
multinomial distribution of parliament attempted to land on portability given by the normalized OK so

648
00:40:04,230 --> 00:40:06,690
that's essentially what we've been doing

649
00:40:06,700 --> 00:40:08,290
only in the sense

650
00:40:08,310 --> 00:40:10,250
these kind of

651
00:40:10,270 --> 00:40:16,030
approximation or something positive some good properties in particular

652
00:40:17,190 --> 00:40:21,110
it provides you you into a bit of multicellular when you do this something step

653
00:40:21,480 --> 00:40:26,180
but because of the property of the multinomial you can see that essentially the new

654
00:40:26,180 --> 00:40:32,700
multicolored approximation you get is an i is essentially estimate of the origin or is

655
00:40:32,700 --> 00:40:38,510
unbiased and sense the expectation of these respect to distribution of the mean value the

656
00:40:38,580 --> 00:40:43,300
divided between them is basically the original version so we do

657
00:40:43,370 --> 00:40:46,690
we do basically this kind of

658
00:40:46,760 --> 00:40:48,450
chilling particles

659
00:40:48,500 --> 00:40:54,750
well multiply particles but we do you know kind of clear away such that essentially

660
00:40:54,750 --> 00:40:59,060
this property satisfied OK we see that a kind of there is an unbiased operation

661
00:40:59,060 --> 00:41:00,770
with some

662
00:41:00,770 --> 00:41:03,230
so it seems when you look at it

663
00:41:03,250 --> 00:41:05,810
for to decide or is it looks a bit silly

664
00:41:06,930 --> 00:41:08,520
you have an original

665
00:41:08,540 --> 00:41:11,660
o point two measure which is a weighted sum of the time as

666
00:41:11,680 --> 00:41:17,980
on you approximated one small bar something capital and and time from so

667
00:41:19,440 --> 00:41:23,540
basically you introduce some kind of a world in time because you just add an

668
00:41:23,540 --> 00:41:27,200
additional approximation was basically

669
00:41:27,210 --> 00:41:32,060
the benefits of of particle images there are some things that is that for most

670
00:41:32,060 --> 00:41:36,270
point of entrance like the baseball in particular is going to fall beneficial for future

671
00:41:36,270 --> 00:41:41,370
time steps so given time index samples of particle ten times OK

672
00:41:41,390 --> 00:41:45,250
but the important thing is that the following time index this ten particles are going

673
00:41:45,250 --> 00:41:49,580
to evolve differently so that's why basically it it's actually

674
00:41:49,600 --> 00:41:54,140
kind of useful from the particle on that rhetorical point of view as potential

675
00:41:55,910 --> 00:42:00,660
so they are better sampling scheme actually which have been described in the literature or

676
00:42:00,700 --> 00:42:05,840
being essentially by a single in the US but having like actually la vie and

677
00:42:05,840 --> 00:42:10,270
so you can basically follows the description of them in the book by happy mood

678
00:42:10,810 --> 00:42:11,680
for example

679
00:42:11,710 --> 00:42:14,230
which was published in two thousand five

680
00:42:15,540 --> 00:42:17,560
now let's basically

681
00:42:17,560 --> 00:42:22,330
not every honest onto this is something operation that's basically looked at

682
00:42:22,350 --> 00:42:27,210
the the resulting so it is going to be very

683
00:42:27,230 --> 00:42:29,160
small modification

684
00:42:29,180 --> 00:42:31,020
what i've been doing before

685
00:42:31,040 --> 00:42:35,500
so i'm interested to solve for a sequence of target distribution pi and compute the

686
00:42:35,500 --> 00:42:39,660
normalizing constant on i'm going to use the importance sampling

687
00:42:39,680 --> 00:42:44,480
on essentially at each time step i'm going to use this was something operation to

688
00:42:44,480 --> 00:42:49,390
get rid of particles with low weights on multiply particles which so get at time

689
00:42:50,140 --> 00:42:53,910
i saw old capital and particle according to chew on

690
00:42:53,930 --> 00:42:57,980
on our way to get to exactly the same as before

691
00:42:58,000 --> 00:42:59,520
now what to do

692
00:42:59,540 --> 00:43:02,000
OK when your at time one

693
00:43:02,020 --> 00:43:04,930
so you have your approximation

694
00:43:04,960 --> 00:43:07,500
the target distribution

695
00:43:07,520 --> 00:43:08,620
pi one

696
00:43:08,640 --> 00:43:13,140
the next one which is a weighted sum of the as

697
00:43:15,680 --> 00:43:19,200
what to do

698
00:43:19,210 --> 00:43:22,640
you're so called capital and particles

699
00:43:22,660 --> 00:43:24,160
for this

700
00:43:24,180 --> 00:43:26,060
weighted empirical measure

701
00:43:26,080 --> 00:43:31,810
to obtain capital new particles which for the sake of simplicity i also do not

702
00:43:31,810 --> 00:43:36,660
x one OK because i don't want to overload notation so i call i denote

703
00:43:36,660 --> 00:43:42,330
the resampled particle as the result articles so don't forget that basically in the said

704
00:43:42,330 --> 00:43:46,960
this is the set of so-called father called is a subset of the original set

705
00:43:46,960 --> 00:43:50,660
of particle because of the particle have died on some of them have been copied

706
00:43:50,660 --> 00:43:55,200
multiple times so you have something approaching

707
00:43:55,210 --> 00:43:56,830
when you have time to

708
00:43:57,540 --> 00:43:59,750
what you do

709
00:43:59,750 --> 00:44:05,460
use sorry when you have time and what to do sample important component x

710
00:44:06,180 --> 00:44:08,520
according to the UN

711
00:44:08,600 --> 00:44:10,000
the conditional

712
00:44:10,020 --> 00:44:13,870
basically importance sampling solution

713
00:44:13,890 --> 00:44:16,350
and then you weights

714
00:44:16,370 --> 00:44:17,680
the particles

715
00:44:17,700 --> 00:44:21,480
on the where you're a damn is a little bit different from what we've done

716
00:44:22,480 --> 00:44:26,810
that is your away them according to what you're write them according to

717
00:44:28,290 --> 00:44:33,460
gamma and the the ratio of successive target normalise target distribution

718
00:44:33,460 --> 00:44:35,140
time to OK

719
00:44:35,160 --> 00:44:36,200
so this is the

720
00:44:36,210 --> 00:44:39,500
problems which considering at time

721
00:44:39,500 --> 00:44:45,010
one is the case or remember what we have think we for sampling when we

722
00:44:45,010 --> 00:44:48,040
will not resampling i had the importance weight

723
00:44:50,390 --> 00:44:52,500
what the in the

724
00:44:52,500 --> 00:44:55,000
so i should clarify

725
00:44:55,000 --> 00:44:57,290
great looking for

726
00:44:57,450 --> 00:44:59,890
is not a solved problem which you agree

727
00:45:01,810 --> 00:45:05,970
OK so another another problem that people have have been trying to work on and

728
00:45:05,970 --> 00:45:10,910
in natural language processing but haven't really structured prediction for yet is learning about linguistic

729
00:45:10,910 --> 00:45:15,330
types so we often talk about tokens words in text

730
00:45:15,350 --> 00:45:19,180
for example in the sentence barack obama likes to talk about barack obama there are

731
00:45:19,180 --> 00:45:23,430
two instances of the token obama both of which instantiate the type

732
00:45:23,450 --> 00:45:27,930
getting a little bit metaphysical here but the question is are the things to say

733
00:45:27,930 --> 00:45:32,330
about the the word obama on its own absolutely refers to a person name its

734
00:45:32,330 --> 00:45:37,810
canyon there are lots of things we can say about words at the type level

735
00:45:38,200 --> 00:45:43,200
and so we often call these things in these collections of knowledge lexicons are ontologies

736
00:45:43,200 --> 00:45:45,770
and people would like to build this from data as well and you can do

737
00:45:45,770 --> 00:45:48,750
that this type of structure prediction but nobody really has

738
00:45:49,060 --> 00:45:52,930
maybe maybe started but i i would say that we haven't really figured out how

739
00:45:52,930 --> 00:45:57,680
to think about that discourse structure dialogue structure learning world knowledge from text these are

740
00:45:57,680 --> 00:46:02,700
all problems people are thinking about but structured prediction isn't really that older using right

741
00:46:02,700 --> 00:46:05,870
now for that but it might be

742
00:46:05,930 --> 00:46:08,580
gonna skip that

743
00:46:08,620 --> 00:46:10,270
take a quick fires

744
00:46:10,290 --> 00:46:16,000
any questions at this point

745
00:46:16,020 --> 00:46:22,330
this is the quietest rule of thirty people i've ever spoken to

746
00:46:22,930 --> 00:46:27,640
so i'm going to go and talk about the coding

747
00:46:28,910 --> 00:46:31,310
here's my notation i'm going to expand it a little bit

748
00:46:31,370 --> 00:46:35,040
scrimtex is going to be a set of possible inputs often it sigma star like

749
00:46:35,700 --> 00:46:40,140
and we use big bold text to mean random variable of the inputs and i'm

750
00:46:40,140 --> 00:46:43,750
going to use script y again to be the set of possible outputs and these

751
00:46:43,750 --> 00:46:48,040
big bold y to be the random variable of outputs and h is going to

752
00:46:48,040 --> 00:46:52,720
be a prediction function that maps access to what

753
00:46:52,830 --> 00:46:56,580
a decoder is simply an implementation of x sorry image

754
00:46:56,600 --> 00:47:00,660
the texan accent and returns your one and the reason it's called decoding this may

755
00:47:00,680 --> 00:47:05,140
be puzzling so i'm going to give us a very quick history lesson so nowadays

756
00:47:05,140 --> 00:47:08,720
in NLP we like to talk about structured prediction it's you know the heart metaphor

757
00:47:08,720 --> 00:47:12,100
that we love to use to describe everything we do at least i am in

758
00:47:12,100 --> 00:47:16,250
this talk but before that maybe ten years ago the thing everybody used was what

759
00:47:16,270 --> 00:47:17,680
source channel models

760
00:47:17,770 --> 00:47:20,290
everything was seen as the source channel model

761
00:47:20,310 --> 00:47:21,830
it's not a bad metaphor

762
00:47:21,850 --> 00:47:27,450
the idea is that you you want to model jointly this these two random variables

763
00:47:27,450 --> 00:47:28,500
x and y

764
00:47:28,580 --> 00:47:31,910
x happens to be your input y happens we output and so the way you

765
00:47:31,950 --> 00:47:35,890
going to do that is going to model first the distribution over y and then

766
00:47:35,890 --> 00:47:39,870
given y you're going to generate x to you know little generative model we haven't

767
00:47:39,870 --> 00:47:43,870
really done anything fancy here it's just the just the chain rule so there's source

768
00:47:43,870 --> 00:47:46,500
model generates wide channel model generates acts

769
00:47:46,520 --> 00:47:47,430
given y

770
00:47:47,450 --> 00:47:49,890
and our job is to decode

771
00:47:51,660 --> 00:47:55,430
we want to find the accent marks there is most likely given sorry we want

772
00:47:55,430 --> 00:47:59,180
to find the one that is most likely given access to all the major backs

773
00:47:59,330 --> 00:48:00,660
and so we use bayes rule

774
00:48:01,140 --> 00:48:05,640
we ignore the denominator because the marginal is constant and this is decoder

775
00:48:05,770 --> 00:48:10,310
that's the decoding usually refers to this sort of balancing of the source model the

776
00:48:10,310 --> 00:48:11,410
the channel model

777
00:48:11,410 --> 00:48:15,730
and this this metaphor you can get you can get so much out of it

778
00:48:16,140 --> 00:48:18,140
the classic example

779
00:48:19,770 --> 00:48:23,370
had kicked off its use in NLP i think part from a part of speech

780
00:48:23,370 --> 00:48:29,470
recognition awards it's widespread was this great story told about sixty years ago by warren

781
00:48:29,470 --> 00:48:33,520
weaver about translations of this was back when the goal of the goal of all

782
00:48:33,580 --> 00:48:37,750
natural language processing in the united states of course was to

783
00:48:37,790 --> 00:48:40,140
spine the russians

784
00:48:40,140 --> 00:48:44,200
and so the story was that they're speaking russian that's what you hear that the

785
00:48:44,220 --> 00:48:46,290
observable that's the axe

786
00:48:46,310 --> 00:48:51,330
but in fact those pesky russians there actually thinking in english course because that's what

787
00:48:51,330 --> 00:48:52,620
everybody thinks

788
00:48:52,680 --> 00:48:58,450
of course so so what happened is that they thought this thought in russia devious

789
00:48:58,450 --> 00:49:04,020
sort of course in russian and then it passed through their broken mechanism i'm sorry

790
00:49:04,040 --> 00:49:07,810
getting myself confused they had this devious thought in english because that's the language of

791
00:49:07,810 --> 00:49:11,640
thought and that pass through this filtering came out of structure and so our job

792
00:49:11,640 --> 00:49:15,560
is to decode it recover with the original message was as it as it existed

793
00:49:15,560 --> 00:49:20,520
before i got garbled so why of course is the is the english the true

794
00:49:20,520 --> 00:49:23,080
the true language underneath

795
00:49:23,100 --> 00:49:26,700
OK so i'm going to keep the term decoder because people and hope you like

796
00:49:26,700 --> 00:49:28,990
it and it's sort of

797
00:49:29,000 --> 00:49:32,990
abstract away the problem of decoding from what is often called inference but that's a

798
00:49:32,990 --> 00:49:35,810
very loaded term in machine learning so i'm not going to call it in fact

799
00:49:35,810 --> 00:49:37,430
just going to call

800
00:49:37,450 --> 00:49:39,910
so generally what what you want to do

801
00:49:39,930 --> 00:49:44,390
you know this this equation at the top sums up a lot of different cases

802
00:49:44,390 --> 00:49:48,890
of decoding but generally it involves finding the y

803
00:49:48,910 --> 00:49:53,140
that minimizes sum expected risk some expected loss

804
00:49:53,160 --> 00:49:58,000
given some distribution over the true value of y so we use l y x

805
00:49:58,000 --> 00:50:00,770
y to be the last

806
00:50:00,770 --> 00:50:05,220
when you choose the cost that you pay when you choose label lowercase y and

807
00:50:05,250 --> 00:50:08,810
the true value is the third argument given input x

808
00:50:08,810 --> 00:50:11,890
so we're going to say that that the true value is distributed according to some

809
00:50:11,890 --> 00:50:14,020
distribution which we may or may not know

810
00:50:14,040 --> 00:50:17,330
and we want to find the y that is the least offensive under the distribution

811
00:50:17,430 --> 00:50:19,330
and if we take the most common case

812
00:50:19,750 --> 00:50:22,430
with the loss is zero one meaning

813
00:50:22,430 --> 00:50:25,600
you lose one point if you get it wrong and you don't lose any points

814
00:50:25,600 --> 00:50:26,850
if you get it right

815
00:50:26,910 --> 00:50:31,390
zero one loss then what this gives you back is just find the most likely

816
00:50:31,390 --> 00:50:32,870
white x

817
00:50:33,520 --> 00:50:36,870
that seems that we have a probability model over y given access which we don't

818
00:50:38,680 --> 00:50:41,600
there's more there's a more general framework that

819
00:50:41,620 --> 00:50:45,290
it encompasses almost everything that people do

820
00:50:45,290 --> 00:50:49,580
in NLP which is to use linear models

821
00:50:49,600 --> 00:50:51,680
so i'm going i'm going to use g

822
00:50:51,930 --> 00:50:54,270
know a feature vector function

823
00:50:54,370 --> 00:50:59,230
it takes x y maps them into some euclidean space

824
00:50:59,350 --> 00:51:01,160
this this is probably familiar

825
00:51:01,180 --> 00:51:05,650
you have to remember here that this is a mapping not just the inputs and

826
00:51:05,650 --> 00:51:10,120
mapping both everything works better if you read this way to get input and output

827
00:51:10,140 --> 00:51:14,450
you mapped into this this euclidean space and then you take an inner product with

828
00:51:14,450 --> 00:51:18,560
and thank you very much for that introduction i think you'll city here again the

829
00:51:18,560 --> 00:51:23,860
other organizers this meeting which is obviously about this meeting

830
00:51:23,890 --> 00:51:24,900
as i was

831
00:51:24,910 --> 00:51:26,860
walking back from the

832
00:51:26,880 --> 00:51:30,360
equally fabulous dinner last night

833
00:51:30,390 --> 00:51:32,160
my mind was

834
00:51:33,410 --> 00:51:35,430
of complexity and i

835
00:51:35,430 --> 00:51:40,450
felt as though withdraws being pushed into my head

836
00:51:40,840 --> 00:51:46,060
as i walk through the old city of dresden back to my hotel

837
00:51:46,830 --> 00:51:52,290
and i almost felt like singing beethoven's ninth because it is amazing

838
00:51:52,300 --> 00:51:53,930
particularly of course

839
00:51:53,940 --> 00:51:55,070
for somebody

840
00:51:55,080 --> 00:51:56,640
from england to see

841
00:51:57,260 --> 00:51:58,720
extraordinarily well

842
00:51:58,730 --> 00:52:03,330
the city interesting has been reconstructed it makes them feel extremely good

843
00:52:03,350 --> 00:52:07,470
so thank you very much for the invitation to come to

844
00:52:08,650 --> 00:52:10,470
systems biology

845
00:52:10,750 --> 00:52:18,360
can we get this to work

846
00:52:23,810 --> 00:52:31,260
well you fix the problems there are stop talking

847
00:52:31,450 --> 00:52:35,480
i can use to compute but does it work

848
00:52:35,500 --> 00:52:42,090
it's not going to go forward right OK thank you thanks a lot

849
00:52:43,480 --> 00:52:45,430
almost every university

850
00:52:46,260 --> 00:52:46,920
has a

851
00:52:47,030 --> 00:52:54,330
caught on this password systems biology even harvard university now has department of systems biology

852
00:52:54,450 --> 00:52:58,310
the only problem is the person if asked to head it doesn't know what it

853
00:52:59,540 --> 00:53:03,310
and i think very few know what it is

854
00:53:03,360 --> 00:53:09,030
because it's a buzzword got going over the last six seven years the buzzword of

855
00:53:09,030 --> 00:53:13,450
course but ten to fifteen years ago was sequencing the genome and then it became

856
00:53:13,450 --> 00:53:18,530
bioinformatics there is an urgent need for to try to define what on earth it

857
00:53:19,620 --> 00:53:24,130
because i tell you i sit on many of the grant committees allocate money in

858
00:53:24,130 --> 00:53:25,890
this area internationally

859
00:53:25,960 --> 00:53:31,700
and ninety percent of the work that is happening is simply for genomics for bioinformatics

860
00:53:32,270 --> 00:53:35,410
we are urgently needed for some kind of definition

861
00:53:35,420 --> 00:53:37,690
of what the subject is and that's what i'm going to

862
00:53:37,730 --> 00:53:39,050
trying to do here

863
00:53:39,070 --> 00:53:44,580
and my first point is that it's not a new subject

864
00:53:44,610 --> 00:53:46,330
for systems biologist

865
00:53:46,360 --> 00:53:48,760
to my mind was closed

866
00:53:48,800 --> 00:53:52,260
who in eighteen sixty five published this remarkable

867
00:53:52,330 --> 00:53:55,510
critics limits in experiment

868
00:53:55,520 --> 00:53:57,460
in which he

869
00:53:57,480 --> 00:54:02,540
enunciated in one of the first examples of assistance biological approach

870
00:54:02,570 --> 00:54:08,110
where he wrote that the living organism doesn't really exist in the media extensions but

871
00:54:08,110 --> 00:54:10,450
in the liquid media and

872
00:54:10,460 --> 00:54:15,050
a complex organism should be looked upon as an assemblage of simple organisms that live

873
00:54:15,050 --> 00:54:21,700
in liquid media anterior he had this concept but the internal environment in which all

874
00:54:21,700 --> 00:54:27,910
of ourselves are drenched is kept under control to be constantly referred to as the

875
00:54:27,910 --> 00:54:33,440
constancy of the internal environment in new therefore the control mechanisms operating at level of

876
00:54:33,440 --> 00:54:35,200
the organism as a whole

877
00:54:35,200 --> 00:54:38,290
and i think this is one of the reasons why one should regard him as

878
00:54:38,290 --> 00:54:45,360
perhaps the first systems biologist and he also made comment is very relevant to

879
00:54:45,360 --> 00:54:50,730
a meeting on complexity and the many mathematicians are here

880
00:54:50,790 --> 00:54:56,210
which i'm afraid is misled everybody because he wrote in his book the most useful

881
00:54:56,210 --> 00:55:00,490
part of physiology and medicine to follow now seek to discover new facts instead of

882
00:55:00,490 --> 00:55:04,960
trying to reduce the equations the facts which science which possesses

883
00:55:04,980 --> 00:55:11,610
the emphasis on that statement which incidentally has being used in british and american physiology

884
00:55:11,610 --> 00:55:16,570
has a kind of anti theoretical stance the journal of physiology one time even used

885
00:55:16,570 --> 00:55:20,690
to reject theoretical papers i think it was hodgkin and huxley the only ones who

886
00:55:20,690 --> 00:55:25,450
managed to break through the barrier before i managed to do so i just followed

887
00:55:25,450 --> 00:55:29,570
in the past that statement is the basis of

888
00:55:29,990 --> 00:55:34,700
this and theoretical stance within within physiology which i think is damaged it

889
00:55:35,810 --> 00:55:38,890
but it's a misconception of what they wrote

890
00:55:38,890 --> 00:55:42,950
because the emphasis on that statement is on the word now

891
00:55:42,960 --> 00:55:46,930
he meant in nineteen sixty five we don't have the data yet he went on

892
00:55:46,930 --> 00:55:48,850
to say

893
00:55:48,880 --> 00:55:53,760
the application of mathematics to natural phenomena is the aim of all science because the

894
00:55:53,760 --> 00:55:59,960
expression levels phenomena should always be mathematical so simply a temporary point was saying at

895
00:55:59,960 --> 00:56:04,520
that time nineteen sixty five we didn't know enough indeed we didn't have to do

896
00:56:04,520 --> 00:56:11,280
very much useful mathematical analysis but the time clearly because when have

897
00:56:11,500 --> 00:56:16,450
i think the time has come for very good reasons which is that

898
00:56:16,460 --> 00:56:19,510
in some ways the problem we face today

899
00:56:19,520 --> 00:56:26,390
in analyzing complexity in biological systems resembles that thought that because the reason why he

900
00:56:26,390 --> 00:56:32,220
insisted on this systems level concept of the consistency the media tell you

901
00:56:32,270 --> 00:56:37,350
was that he was facing the challenge from the organic chemists who managed to synthesize

902
00:56:37,350 --> 00:56:41,440
organic molecules the molecules people thought only existed

903
00:56:41,490 --> 00:56:46,580
inside living systems where there was some special forces vital force that enable them to

904
00:56:46,580 --> 00:56:51,550
do what they were doing and of course the organic chemistry is completely wrong

905
00:56:52,450 --> 00:56:54,450
ben response those

906
00:56:54,460 --> 00:56:59,830
to the reductionism the organic chemists who as it were claiming that one day reduce

907
00:56:59,850 --> 00:57:04,690
life just to the number of organic molecules which we can make a response to

908
00:57:04,690 --> 00:57:07,900
all right thanks for coming everybody

909
00:57:09,220 --> 00:57:14,650
really this is going to be about the very basics of monte carlo planning as

910
00:57:14,650 --> 00:57:20,160
i see them and we'll talk about some of the recent progress is for example

911
00:57:20,160 --> 00:57:21,430
the UCT

912
00:57:21,430 --> 00:57:26,540
algorithm l committee and so if you really waiting for that you can take a

913
00:57:26,540 --> 00:57:32,180
break and wake up when i mention UCT again so so we

914
00:57:32,200 --> 00:57:37,280
the left just giving a few preliminary is about markov decision processes

915
00:57:37,310 --> 00:57:39,440
mainly for the notation

916
00:57:39,450 --> 00:57:44,980
and then we're going to try to motivate monte carlo planning why is it useful

917
00:57:44,980 --> 00:57:47,020
what is it

918
00:57:47,030 --> 00:57:51,810
and then basically i'm going to talk about two classes of monte carlo planning techniques

919
00:57:51,810 --> 00:57:56,990
as i see it's one of call uniform monte carlo is not going to be

920
00:57:56,990 --> 00:57:59,060
very intelligent about how

921
00:57:59,060 --> 00:58:01,160
it samples the environment

922
00:58:01,200 --> 00:58:08,340
and then we'll talk about adaptive monte carlo techniques at the end culminating in the

923
00:58:08,340 --> 00:58:14,190
UCT algorithm which you might be familiar with because it's been so successful

924
00:58:14,220 --> 00:58:14,890
in go

925
00:58:14,910 --> 00:58:17,630
more recently other problems

926
00:58:17,640 --> 00:58:19,920
so i

927
00:58:19,940 --> 00:58:25,040
and the peace we're going to model the world is a markov decision process will

928
00:58:25,040 --> 00:58:29,730
will talk about that in a minute but the basic model is we have this

929
00:58:29,750 --> 00:58:31,230
robot or

930
00:58:31,250 --> 00:58:32,910
and and

931
00:58:32,920 --> 00:58:38,140
he can send actions to the world possibly their stochastic actions in the world gives

932
00:58:38,140 --> 00:58:44,250
us states and rewards back or if you like observations and rewards back and we're

933
00:58:44,250 --> 00:58:49,470
interested in what's going on in this guy's head he wants to maximize his reward

934
00:58:49,690 --> 00:58:54,110
so we're gonna stick in monte carlo planning techniques

935
00:58:54,130 --> 00:58:59,530
but an MDP is basically composed of four things the first thing is going to

936
00:58:59,530 --> 00:59:01,170
be a set of states

937
00:59:01,170 --> 00:59:07,690
these are the states of the world that sort of completely describe how the world

938
00:59:07,690 --> 00:59:11,880
is going to function if we take an action we have a finite set of

939
00:59:11,880 --> 00:59:17,180
actions and the thing that makes a markov decision process has been a markov and

940
00:59:17,180 --> 00:59:24,110
it is because the transition distribution p of t is the first order markov process

941
00:59:24,120 --> 00:59:29,570
what this means is this is a conditional probability distribution that tells you if you

942
00:59:29,570 --> 00:59:32,070
take the a in state s

943
00:59:32,080 --> 00:59:35,800
what's the probability of ending up in as prime

944
00:59:35,860 --> 00:59:40,920
so that's what we mean by transition distribution and its first order because it only

945
00:59:40,920 --> 00:59:45,140
depends on s a right now

946
00:59:45,140 --> 00:59:50,890
the reward distribution is also going to be first order markov chain for any state

947
00:59:50,900 --> 00:59:55,480
it tells us if we take action a what is the distribution over real valued

948
00:59:55,480 --> 01:00:00,620
rewards and for the sake of this talk we're going to be assuming that that

949
01:00:00,620 --> 01:00:02,700
this has finite

950
01:00:02,730 --> 01:00:07,730
the rewards are bounded within some finite range

951
01:00:08,050 --> 01:00:11,860
so those are mdps

952
01:00:11,890 --> 01:00:14,920
pretty basic stuff that's just for notation

953
01:00:14,930 --> 01:00:18,580
so if we have an MDP what is the solution

954
01:00:18,600 --> 01:00:25,140
well it's not good enough generally to output a straight line planned action one action

955
01:00:25,140 --> 01:00:29,270
two the action three because once you take action one

956
01:00:29,270 --> 01:00:35,350
the transition to some state that maybe action two wasn't so well suited for so

957
01:00:35,350 --> 01:00:39,820
you really need to form a plan that can handle or whatever state you end

958
01:00:39,820 --> 01:00:43,800
up in in any moment we call center plan policy

959
01:00:43,820 --> 01:00:47,920
and right now we're not gonna compare we're not going to care about how we

960
01:00:47,920 --> 01:00:54,050
get that policy can be computed online as you're planning or offline before but policy

961
01:00:54,050 --> 01:01:00,800
is simply a mapping could be stochastic from one is pi for policy from states

962
01:01:00,800 --> 01:01:02,110
to actions

963
01:01:03,110 --> 01:01:06,040
basically it tells us what to do or in state s

964
01:01:06,100 --> 01:01:12,480
this defines a continuous reactive controller for this agent in every moment is in state

965
01:01:12,490 --> 01:01:16,390
takes an action ends up in a new state looks this policy and figures out

966
01:01:16,390 --> 01:01:21,490
what to do next so that's sort of policy is and the main question is

967
01:01:21,490 --> 01:01:26,610
how do we determine whether policy is good or bad and intuitively policy is good

968
01:01:26,610 --> 01:01:33,010
if on and the chief the high reward when we executed in the environment

969
01:01:33,040 --> 01:01:39,440
so specifically in this talk we're going to talk about finite horizon discounted reward it's

970
01:01:39,440 --> 01:01:45,990
amount for but we're going to denote this function by v pi s h h

971
01:01:45,990 --> 01:01:49,410
is our horizon that's how long we're going to think ahead that's how far into

972
01:01:49,410 --> 01:01:53,730
the future going to think ahead and what that function tells us is what is

973
01:01:53,730 --> 01:01:58,710
the expected value we're going to get following this policy starting in as if we

974
01:01:58,710 --> 01:02:04,600
follow for a h steps possibly in county and so let's think about what this

975
01:02:05,490 --> 01:02:09,180
so what if you run the policy for each step starting in s

976
01:02:09,190 --> 01:02:13,670
you're going to get a sequence of rewards for a sequence of eight rewards and

977
01:02:13,670 --> 01:02:19,050
this is really a random sequences of random variables because the dynamics of random the

978
01:02:19,050 --> 01:02:20,730
reward function is random

979
01:02:20,740 --> 01:02:26,220
was the random sequence and what we're going to be interested in is the

980
01:02:27,420 --> 01:02:33,110
the of the sequences so the expected value of the discounted sum and we discount

981
01:02:33,110 --> 01:02:37,280
later rewards more than earlier rewards so beta t

982
01:02:37,300 --> 01:02:38,850
is the discount factor

983
01:02:38,860 --> 01:02:42,510
if you set it to one that all these rewards are equal to our sort

984
01:02:42,510 --> 01:02:46,370
of treated equally and so it allows us to trade off how much we care

985
01:02:46,370 --> 01:02:53,480
about the future usually is just there for mathematical convenience sometimes you can motivated by

986
01:02:53,480 --> 01:02:58,350
economic principles of the probability of dying and whatnot

987
01:02:58,360 --> 01:03:02,840
but in the end what we're interested in is the optimal policy

988
01:03:02,880 --> 01:03:08,260
so an optimal policy is one that dominates all other policies across all states in

989
01:03:08,260 --> 01:03:09,610
terms of its value

990
01:03:12,860 --> 01:03:15,750
i'm not going to cover the slide in detail of the say

991
01:03:17,460 --> 01:03:19,800
if you really like infinite horizon

992
01:03:19,800 --> 01:03:25,000
problems so so in many cases you might have a continuous controller and you plan

993
01:03:25,000 --> 01:03:28,050
for the infinite horizon

994
01:03:28,110 --> 01:03:32,760
and they have to introduce the discount factor to make things well defined a really

995
01:03:32,760 --> 01:03:38,740
finite horizons results are going to be good enough and the main reason is that

996
01:03:38,810 --> 01:03:44,650
we can approximate the infinite horizon value function with a finite horizon

997
01:03:44,690 --> 01:03:50,220
and if you choose your horizon long enough the approximation error goes to zero quickly

998
01:03:51,910 --> 01:03:57,340
so basically everything that i say here will apply to infinite horizon problems you can

999
01:03:57,340 --> 01:03:58,840
refer to this slide

1000
01:03:58,850 --> 01:04:05,100
again if you care to care to see that so so the basic results in

1001
01:04:05,110 --> 01:04:09,300
MDP theory is a pretty old results

1002
01:04:09,310 --> 01:04:15,730
optimal policies are guaranteed to exist so there are optimal policies is there is not

1003
01:04:15,730 --> 01:04:19,560
a partial order on policies and

1004
01:04:19,860 --> 01:04:22,430
we have algorithms for computing that

1005
01:04:22,430 --> 01:04:24,800
all IP addresses

1006
01:04:24,810 --> 01:04:28,170
and then we have to compute the size of the story

1007
01:04:28,260 --> 01:04:32,100
if you have many many many IP addresses

1008
01:04:32,110 --> 01:04:34,550
it's a problem to maintain this

1009
01:04:35,820 --> 01:04:40,710
is the same another example and it was the first proposed in this paper

1010
01:04:40,710 --> 01:04:41,960
this algorithm

1011
01:04:41,980 --> 01:04:47,050
if you want to have an idea of the number of different distinct work

1012
01:04:47,470 --> 01:04:51,100
in the text or in the

1013
01:04:51,310 --> 01:04:58,100
the idea is the following

1014
01:04:58,100 --> 01:04:59,250
there is

1015
01:05:01,290 --> 01:05:02,820
of and

1016
01:05:04,090 --> 01:05:08,540
which is going to be maintained incrementally

1017
01:05:08,540 --> 01:05:10,260
this structure here

1018
01:05:10,270 --> 01:05:12,600
and is equal to weight

1019
01:05:12,610 --> 01:05:18,720
is initialized by zero

1020
01:05:18,720 --> 01:05:22,470
then you have the a hashing function has functions

1021
01:05:22,470 --> 01:05:24,010
we which

1022
01:05:25,590 --> 01:05:27,230
each element

1023
01:05:27,290 --> 01:05:29,640
of your stream of the stream

1024
01:05:29,690 --> 01:05:31,550
in two

1025
01:05:31,940 --> 01:05:33,110
and b

1026
01:05:35,480 --> 01:05:38,180
for instance this IP address

1027
01:05:40,250 --> 01:05:42,640
in two is b

1028
01:05:45,650 --> 01:05:48,350
since it is a hash function

1029
01:05:48,350 --> 01:05:52,770
this has function distributes uniformly the elements of the stream

1030
01:05:52,800 --> 01:05:54,010
on the

1031
01:05:54,220 --> 01:05:59,040
two power as a possibility

1032
01:05:59,050 --> 01:06:03,080
the method is very simple

1033
01:06:03,090 --> 01:06:07,590
it's to maintain and update

1034
01:06:09,110 --> 01:06:10,210
this a bit

1035
01:06:11,550 --> 01:06:15,320
for each new element arriving in the stream

1036
01:06:15,340 --> 01:06:17,900
it is ash

1037
01:06:17,960 --> 01:06:23,210
so for instance this element is ash into this

1038
01:06:23,260 --> 01:06:26,520
the position of the leftmost one

1039
01:06:26,540 --> 01:06:27,820
in h

1040
01:06:35,000 --> 01:06:36,590
a bit strange

1041
01:06:36,600 --> 01:06:37,460
as carrier

1042
01:06:37,470 --> 01:06:39,860
it is updated and

1043
01:06:41,080 --> 01:06:47,750
there is zero here if it wasn't there are here it forced to one

1044
01:06:47,760 --> 01:06:49,170
very very

1045
01:06:53,470 --> 01:06:55,230
the reason

1046
01:06:55,260 --> 01:06:58,150
is that once a sufficient number

1047
01:06:58,180 --> 01:07:01,510
of elements have been seen

1048
01:07:01,550 --> 01:07:06,850
it's possible to have an estimation of the number of different

1049
01:07:08,320 --> 01:07:10,440
already seen in the stream

1050
01:07:10,460 --> 01:07:12,130
it seems

1051
01:07:15,680 --> 01:07:20,650
selecting the leftmost zero in this

1052
01:07:24,900 --> 01:07:27,180
and the world

1053
01:07:27,180 --> 01:07:30,210
and with this position are

1054
01:07:30,210 --> 01:07:32,520
you can compute the number

1055
01:07:32,610 --> 01:07:36,010
seven estimation of the the number of different

1056
01:07:36,430 --> 01:07:40,170
that use appearing in this period

1057
01:07:40,210 --> 01:07:42,420
the idea is that

1058
01:07:42,470 --> 01:07:43,970
for an element

1059
01:07:44,020 --> 01:07:46,850
already seen we expect

1060
01:07:48,560 --> 01:07:49,680
on the

1061
01:07:49,710 --> 01:07:51,800
left here

1062
01:07:51,810 --> 01:07:52,790
there will be

1063
01:07:52,800 --> 01:07:55,090
half one then hop

1064
01:07:55,180 --> 01:07:57,380
during those

1065
01:07:57,430 --> 01:08:00,510
and here they will be

1066
01:08:01,630 --> 01:08:03,340
divided by four

1067
01:08:06,900 --> 01:08:08,040
and so on

1068
01:08:08,040 --> 01:08:13,820
so the probability a very small number of and the probability of

1069
01:08:13,840 --> 01:08:19,650
having all one here is very small so this gives an estimation of a number

1070
01:08:19,650 --> 01:08:20,820
of different

1071
01:08:24,100 --> 01:08:26,300
you have

1072
01:08:26,350 --> 01:08:29,670
very nice results

1073
01:08:29,680 --> 01:08:32,930
so this this is the first

1074
01:08:34,590 --> 01:08:36,980
so the idea here it is

1075
01:08:38,850 --> 01:08:41,890
you have especially specialized tasks

1076
01:08:41,970 --> 01:08:44,190
you would need a very large

1077
01:08:45,520 --> 01:08:48,000
to maintain some statistics

1078
01:08:48,010 --> 01:08:51,460
and you have this marks structure which

1079
01:08:51,470 --> 01:08:54,760
maintains incrementally exactly

1080
01:08:54,800 --> 01:08:59,000
the information you want which is summarized information

1081
01:08:59,010 --> 01:09:00,720
without storing or

1082
01:09:01,680 --> 01:09:03,520
the elements

1083
01:09:03,670 --> 01:09:08,890
another example is the count sketch allegories

1084
01:09:08,920 --> 01:09:12,720
the goal of this sketch

1085
01:09:13,970 --> 01:09:15,720
to find quickly

1086
01:09:15,730 --> 01:09:18,710
the k most frequent elements industry

1087
01:09:19,250 --> 01:09:24,190
a large number n of distinct values of course if you have a small number

1088
01:09:24,250 --> 01:09:25,560
of possible

1089
01:09:25,580 --> 01:09:27,650
IP addresses

1090
01:09:28,520 --> 01:09:31,640
there's no problem you have your around ten

1091
01:09:31,650 --> 01:09:34,650
a you addresses and you you can find the

1092
01:09:34,720 --> 01:09:36,100
the most quick

1093
01:09:37,360 --> 01:09:39,640
so this is one

1094
01:09:39,640 --> 01:09:45,090
nice solution for the one hundred most frequent IP addresses going through a router

1095
01:09:45,140 --> 01:09:46,460
so you have here

1096
01:09:46,470 --> 01:09:47,930
an input stream

1097
01:09:47,960 --> 01:09:49,310
with different

1098
01:09:49,360 --> 01:09:51,140
elements arriving

1099
01:09:51,170 --> 01:09:52,430
what you want to do

1100
01:09:52,480 --> 01:09:55,440
is to be that the distribution

1101
01:09:55,460 --> 01:10:03,100
of the elements the frequency of all the elements and then extract on the k

1102
01:10:03,100 --> 01:10:05,210
most africans

1103
01:10:05,260 --> 01:10:09,800
k most frequent elements industry

1104
01:10:09,800 --> 01:10:11,040
standard solution

1105
01:10:11,050 --> 01:10:12,920
maintaining this array

1106
01:10:12,970 --> 01:10:18,300
of all the IP addresses incrementing counter

1107
01:10:18,340 --> 01:10:21,600
and providing for when necessary

1108
01:10:21,610 --> 01:10:25,310
of course these arrays very large

1109
01:10:25,320 --> 01:10:27,180
the idea is

1110
01:10:27,180 --> 01:10:29,580
i'm going to introduce them

1111
01:10:29,600 --> 01:10:31,310
concentration inequalities

1112
01:10:31,320 --> 01:10:32,800
the tell us how

1113
01:10:32,810 --> 01:10:35,130
sample averages

1114
01:10:35,130 --> 01:10:36,810
converge to

1115
01:10:36,840 --> 01:10:38,740
two expectations

1116
01:10:38,750 --> 01:10:42,680
and how concentrated these things i guess with more generally

1117
01:10:42,680 --> 01:10:46,200
then just sample averages we're looking at a concentration of other

1118
01:10:48,450 --> 01:10:51,250
more general random variables about the expectation

1119
01:10:51,450 --> 01:10:52,640
and then

1120
01:10:52,680 --> 01:10:55,160
we'll be using these these ideas

1121
01:10:55,320 --> 01:11:01,550
improving this bounds using a measure of complexity call rademacher averages

1122
01:11:04,540 --> 01:11:07,520
will get onto those in a moment

1123
01:11:07,530 --> 01:11:11,740
OK but just for stability

1124
01:11:11,790 --> 01:11:13,890
i believe intuition

1125
01:11:15,460 --> 01:11:16,970
if we think of this

1126
01:11:16,980 --> 01:11:19,500
the framework is i've outlined we have

1127
01:11:19,510 --> 01:11:23,000
a class of functions capitalist

1128
01:11:23,000 --> 01:11:26,850
and we'd like to choose some function from that class so as to minimize the

1129
01:11:28,010 --> 01:11:30,590
of the loss of that minimize the risk

1130
01:11:32,850 --> 01:11:36,010
but we have these these competing issues if we were to choose

1131
01:11:36,020 --> 01:11:39,670
a function from a more complex class then

1132
01:11:39,710 --> 01:11:43,610
we're going to have to do better in terms of the approximation property right that

1133
01:11:43,610 --> 01:11:48,190
the infimum over the class that the minimum over the class of the risk

1134
01:11:48,240 --> 01:11:53,590
for classes is something that will be smaller but will suffer on the estimation side

1135
01:11:53,600 --> 01:11:59,080
the more complex class and we only have information about the class presented to us

1136
01:11:59,080 --> 01:12:00,260
in the form of these

1137
01:12:00,270 --> 01:12:02,700
this finite set of labelled examples

1138
01:12:05,950 --> 01:12:09,690
the key issue here is trading of these two properties the approximation properties in the

1139
01:12:09,690 --> 01:12:13,000
estimation problem

1140
01:12:13,010 --> 01:12:16,520
so coming up with estimates of the the risk

1141
01:12:16,940 --> 01:12:19,970
in terms of for instance the sample size in some measure of the complexity of

1142
01:12:19,970 --> 01:12:25,400
that tells us how to control the complexity that for instance using some method of

1143
01:12:25,400 --> 01:12:30,340
sieves some approach where we say for a given sample size we can work with

1144
01:12:30,340 --> 01:12:34,380
the class that's this complex and is the sample size grows we can choose some

1145
01:12:34,380 --> 01:12:36,410
function for bigger and bigger class

1146
01:12:40,480 --> 01:12:42,160
an approach that's very

1147
01:12:42,180 --> 01:12:45,640
very closely related is the regularisation approach for instance

1148
01:12:45,650 --> 01:12:48,500
you know control introducing an extra

1149
01:12:48,530 --> 01:12:53,190
parameter into an empirical criterion

1150
01:12:53,190 --> 01:12:54,290
such as the

1151
01:12:57,340 --> 01:12:59,460
i regularisation parameter in the SVM

1152
01:12:59,460 --> 01:13:02,740
OK so

1153
01:13:02,760 --> 01:13:07,160
these are two general approaches of controlling controlling complexity

1154
01:13:07,200 --> 01:13:11,470
so let's think now about suppose we have a case where we've got very rich

1155
01:13:15,500 --> 01:13:17,560
and we split up into some hierarchy

1156
01:13:17,570 --> 01:13:18,830
right so we're thinking

1157
01:13:19,030 --> 01:13:21,920
choosing functions from this class but it's way too rich to

1158
01:13:26,740 --> 01:13:31,450
to estimate some function from that class near optimally using a finite sample so we

1159
01:13:31,450 --> 01:13:36,140
split up we come up with some hierarchy we represent functions we represent this class

1160
01:13:36,140 --> 01:13:39,740
as a union of of simpler classes

1161
01:13:39,980 --> 01:13:44,030
f one f two and so on where for each k

1162
01:13:44,070 --> 01:13:47,420
what we're going to do is choose the best efforts of k in the class

1163
01:13:47,420 --> 01:13:49,380
capital k

1164
01:13:49,380 --> 01:13:55,740
OK and for instance we might do that by minimizing empirical risk

1165
01:13:55,760 --> 01:13:57,890
and then will choose

1166
01:13:57,890 --> 01:13:59,130
and if k

1167
01:13:59,140 --> 01:14:01,230
right one of these optimal guys

1168
01:14:01,240 --> 01:14:05,050
so as to minimize the complexity panellist empirical error

1169
01:14:05,300 --> 01:14:08,080
OK so this is a generic approach that we might take you can think of

1170
01:14:08,460 --> 01:14:10,220
you can think of this

1171
01:14:10,550 --> 01:14:15,820
hierarchical representation for instance for things like decision trees came might might be index k

1172
01:14:15,830 --> 01:14:20,010
might be the restriction on the number of decision nodes in the tree

1173
01:14:20,030 --> 01:14:23,230
well for the

1174
01:14:23,270 --> 01:14:26,590
a reproducing kernel hilbert space the index k

1175
01:14:26,600 --> 01:14:27,480
might allow

1176
01:14:27,480 --> 01:14:29,540
and increase increasing radius

1177
01:14:29,560 --> 01:14:32,860
football in RKHS right if some k would be

1178
01:14:33,240 --> 01:14:38,330
a subset a ball in the RKHS of radius r k where k is increasing

1179
01:14:38,330 --> 01:14:40,990
with k

1180
01:14:41,060 --> 01:14:43,970
and then with thinking of so is is one

1181
01:14:43,970 --> 01:14:58,830
so i think of

1182
01:14:58,980 --> 01:15:00,510
and that

1183
01:15:00,530 --> 01:15:03,260
you might want to see

1184
01:15:04,920 --> 01:15:06,560
prior work

1185
01:15:07,690 --> 01:15:12,240
he served as a certain member of the committee for the defence of

1186
01:15:12,320 --> 01:15:13,990
these are

1187
01:15:13,990 --> 01:15:15,210
doctor the

1188
01:15:15,350 --> 01:15:17,610
where i see

1189
01:15:20,210 --> 01:15:23,070
looks at the station was seminar

1190
01:15:24,630 --> 01:15:26,600
he thought it

1191
01:15:30,850 --> 01:15:32,710
like here

1192
01:15:32,810 --> 01:15:39,820
we start to collaboration with his or you know

1193
01:15:39,860 --> 01:15:43,820
with four years ago within european

1194
01:15:43,820 --> 01:15:45,990
training and e

1195
01:15:47,180 --> 01:15:51,130
and he proved to be very fruitful collaboration

1196
01:15:51,170 --> 01:15:58,170
in all the participants are operations so once it's very much like work

1197
01:15:58,220 --> 01:15:59,410
this ensures

1198
01:15:59,550 --> 01:16:04,160
question thank you very much of course it's also my pleasure to to come to

1199
01:16:06,440 --> 01:16:10,600
my second visit here and unfortunately just for one day

1200
01:16:10,610 --> 01:16:13,530
but hopefully it will not be the last one

1201
01:16:13,550 --> 01:16:19,610
and today i will give you a brief introduction about the

1202
01:16:19,660 --> 01:16:24,380
activities that we have in my group and high work during the last

1203
01:16:24,390 --> 01:16:28,060
now more than twenty years on

1204
01:16:28,140 --> 01:16:29,940
liquid crystalline

1205
01:16:30,080 --> 01:16:34,800
parliament networks and this was all and is the title of my lecture

1206
01:16:34,860 --> 01:16:37,220
let me briefly introduce

1207
01:16:37,270 --> 01:16:38,520
this lecture

1208
01:16:38,610 --> 01:16:40,380
with some

1209
01:16:40,390 --> 01:16:42,630
i suppose lights

1210
01:16:43,270 --> 01:16:46,640
i think

1211
01:16:46,690 --> 01:16:48,460
o quite well we

1212
01:16:52,760 --> 01:16:53,690
you all know

1213
01:16:53,690 --> 01:16:55,900
i think you all know robust

1214
01:16:55,940 --> 01:16:58,560
they everywhere they are used everywhere

1215
01:16:58,620 --> 01:17:01,600
but i think not all of you

1216
01:17:01,620 --> 01:17:05,000
really know these exceptional properties

1217
01:17:05,000 --> 01:17:07,120
of the conventional rubber

1218
01:17:07,160 --> 01:17:10,780
and how to describe such material

1219
01:17:10,910 --> 01:17:16,030
this decision for example might be such conventional rather

1220
01:17:16,040 --> 01:17:18,160
that you can get everywhere

1221
01:17:18,180 --> 01:17:23,280
and if you look at the chemistry of such a system it's prepared from

1222
01:17:23,410 --> 01:17:26,600
a matter of linear problem

1223
01:17:26,600 --> 01:17:29,950
and the amount of linear polymath is

1224
01:17:29,970 --> 01:17:35,930
cross linked to three dimensional pull in the network by a chemical reaction and then

1225
01:17:35,930 --> 01:17:38,130
you end up with the rubber

1226
01:17:38,160 --> 01:17:44,480
and this crosslinking reactions for example can be take place in the norman isotropic many

1227
01:17:44,480 --> 01:17:46,850
of the problem

1228
01:17:46,910 --> 01:17:48,940
and when you

1229
01:17:49,600 --> 01:17:53,120
at the change of the properties going from

1230
01:17:53,130 --> 01:17:56,000
the following are made to the polymer network

1231
01:17:56,130 --> 01:17:58,950
then at least the typical liquid

1232
01:18:00,090 --> 01:18:06,190
is not change if you look for the mobility of the single chain segments and

1233
01:18:06,190 --> 01:18:07,530
such a robot

1234
01:18:07,590 --> 01:18:12,410
it's the dynamics still remains essentially unchanged

1235
01:18:12,410 --> 01:18:15,030
but due to the three-dimensional

1236
01:18:15,040 --> 01:18:20,470
network of course football brownian diffusion is prevented

1237
01:18:20,510 --> 01:18:26,370
and you obtain material which resemble the properties of the solid

1238
01:18:26,410 --> 01:18:31,880
in other words you already have two interesting properties on the one hand side behaves

1239
01:18:31,880 --> 01:18:33,130
like a solid

1240
01:18:33,180 --> 01:18:36,260
on the other hand it behaves like a liquid

1241
01:18:36,280 --> 01:18:41,030
interestingly when you take such rather than when you make a deformation of the rubber

1242
01:18:41,030 --> 01:18:43,500
band you retracted

1243
01:18:43,530 --> 01:18:47,780
and this rate vector force can be simply described with

1244
01:18:47,820 --> 01:18:54,320
so thermodynamics hand for an idea one can say that this we expect the force

1245
01:18:54,320 --> 01:18:55,630
is simply given

1246
01:18:55,690 --> 01:18:57,980
by the change of the

1247
01:19:00,540 --> 01:19:02,270
with the deformation

1248
01:19:02,290 --> 01:19:06,730
in this way the description is very similar to that of a gas

1249
01:19:06,790 --> 01:19:12,480
well when you change the entropy of the system for example by by changing the

1250
01:19:12,480 --> 01:19:14,070
because here

1251
01:19:14,120 --> 01:19:15,920
on our task now is

1252
01:19:15,930 --> 01:19:16,990
to solve

1253
01:19:17,630 --> 01:19:20,120
differential equation

1254
01:19:20,150 --> 01:19:21,530
and that of course

1255
01:19:21,570 --> 01:19:22,530
you can do

1256
01:19:22,590 --> 01:19:23,680
three seconds

1257
01:19:23,710 --> 01:19:26,130
you recognise this differential equation

1258
01:19:26,140 --> 01:19:28,220
it is axed global got

1259
01:19:28,270 --> 01:19:31,010
plus something

1260
01:19:31,110 --> 01:19:33,460
times x

1261
01:19:33,530 --> 01:19:35,090
so the

1262
01:19:35,100 --> 01:19:39,410
the new frequency which i will call all got plus

1263
01:19:39,470 --> 01:19:41,980
new frequency omega plus

1264
01:19:42,020 --> 01:19:44,270
is the square of

1265
01:19:44,270 --> 01:19:46,300
of two

1266
01:19:46,310 --> 01:19:47,920
omega as square

1267
01:19:47,980 --> 01:19:50,350
was omega three

1268
01:19:50,420 --> 01:19:55,110
and what you see is something that i really anticipated this was consistent with intuition

1269
01:19:55,140 --> 01:19:57,730
it is larger than all because zero

1270
01:19:57,810 --> 01:20:02,840
because this is the effect of the spring

1271
01:20:02,850 --> 01:20:05,280
so omega zero is the one

1272
01:20:06,360 --> 01:20:08,390
and omega plus now

1273
01:20:09,010 --> 01:20:10,720
this grammar

1274
01:20:11,680 --> 01:20:12,910
omega as

1275
01:20:12,960 --> 01:20:15,420
squared loss omega zero

1276
01:20:15,490 --> 01:20:18,180
craig omega as

1277
01:20:18,190 --> 01:20:19,990
square is defined

1278
01:20:20,000 --> 01:20:27,880
this way

1279
01:20:27,890 --> 01:20:29,900
so if i now turn my to my

1280
01:20:29,910 --> 01:20:33,010
the general solution for now you accept the fact that

1281
01:20:33,100 --> 01:20:34,750
that is the general

1282
01:20:34,820 --> 01:20:36,760
solution the superposition

1283
01:20:38,090 --> 01:20:42,070
the normal modes

1284
01:20:43,990 --> 01:20:45,660
the key point is

1285
01:20:45,680 --> 01:20:49,100
that independent of your initial conditions

1286
01:20:49,110 --> 01:20:52,210
omega minus that was discriminative deal fell

1287
01:20:52,220 --> 01:20:54,490
i never reported any initial condition

1288
01:20:54,590 --> 01:20:59,110
independent of the initial condition omega plus that is what i never put any initial

1289
01:21:01,930 --> 01:21:04,720
it is independent of the initial conditions

1290
01:21:04,770 --> 01:21:07,530
is this ratio

1291
01:21:07,590 --> 01:21:11,420
which is plus one

1292
01:21:11,480 --> 01:21:14,040
independent of the initial conditions

1293
01:21:14,050 --> 01:21:15,440
this ratio

1294
01:21:15,500 --> 01:21:18,540
it is minus one

1295
01:21:18,630 --> 01:21:23,180
in other words if i didn't tell you the initial conditions which i haven't

1296
01:21:23,190 --> 01:21:25,270
i can predict

1297
01:21:25,510 --> 01:21:29,100
this ratio is plus one

1298
01:21:29,140 --> 01:21:32,300
and i can predict that this ratio is minus one

1299
01:21:32,350 --> 01:21:33,680
if i give this one

1300
01:21:33,740 --> 01:21:37,120
if i make these three and this is my three if i make this five

1301
01:21:37,120 --> 01:21:38,740
and this minus five

1302
01:21:38,770 --> 01:21:40,630
ratio is minus one

1303
01:21:40,710 --> 01:21:43,940
so ratios are independent of initial conditions

1304
01:21:44,030 --> 01:21:49,550
and the frequencies independent of initial conditions if i tell you the initial conditions then

1305
01:21:49,550 --> 01:21:51,010
of course you can also

1306
01:21:51,980 --> 01:21:53,420
the individual

1307
01:21:57,520 --> 01:21:59,250
suppose no

1308
01:21:59,250 --> 01:22:03,010
i start this system of

1309
01:22:05,140 --> 01:22:09,360
some way at time t equals zero which i can choose

1310
01:22:09,400 --> 01:22:11,730
and i choose the following

1311
01:22:11,780 --> 01:22:13,300
t equals zero

1312
01:22:13,320 --> 01:22:16,460
i make x one

1313
01:22:18,240 --> 01:22:20,020
just some number c

1314
01:22:20,170 --> 01:22:24,110
we can choose three centimetres whatever you want to choose

1315
01:22:24,120 --> 01:22:25,910
but i may be one zero

1316
01:22:26,010 --> 01:22:29,130
so as i release it

1317
01:22:31,760 --> 01:22:34,490
and suppose i make excellent

1318
01:22:34,510 --> 01:22:37,260
and i make the two zero

1319
01:22:39,920 --> 01:22:42,730
if you don't know what that means of course i'm going to demonstrate it it

1320
01:22:42,730 --> 01:22:45,280
means that this one

1321
01:22:45,750 --> 01:22:50,060
o one i offset this all i do and i let this one go

1322
01:22:50,110 --> 01:22:51,720
my hands off

1323
01:22:51,750 --> 01:22:53,740
and then i want to know what's going to happen

1324
01:22:53,790 --> 01:22:57,010
right because i release this one with zero speed

1325
01:22:57,050 --> 01:23:00,740
that's what you see there that is one of an infinite number of

1326
01:23:00,740 --> 01:23:02,090
initial conditions

1327
01:23:02,120 --> 01:23:02,900
that you

1328
01:23:02,910 --> 01:23:06,010
they choose

1329
01:23:06,070 --> 01:23:08,850
so this model has to be

1330
01:23:08,870 --> 01:23:12,440
substituted into my

1331
01:23:12,500 --> 01:23:14,500
general solution

1332
01:23:14,510 --> 01:23:17,070
so you have to take the derivative

1333
01:23:18,130 --> 01:23:19,860
x one so you have to

1334
01:23:19,910 --> 01:23:21,740
qx one got

1335
01:23:21,760 --> 01:23:23,970
put equal to zero

1336
01:23:24,020 --> 01:23:26,790
i leave that you're competent hands

1337
01:23:26,840 --> 01:23:30,010
then you have to take x two you have to take

1338
01:23:30,040 --> 01:23:31,170
x two

1339
01:23:31,280 --> 01:23:34,620
but you have to make that equal to zero and early that interview your competent

1340
01:23:34,620 --> 01:23:36,090
hands it's easy

1341
01:23:36,110 --> 01:23:37,930
and you will find them

1342
01:23:37,930 --> 01:23:41,280
in this particular case for this particular example

1343
01:23:41,310 --> 01:23:43,310
if i minus e equals zero

1344
01:23:43,360 --> 01:23:44,370
and phi two

1345
01:23:44,380 --> 01:23:48,410
and five plus

1346
01:23:48,440 --> 01:23:51,370
it will take you more than a few minutes

1347
01:23:51,420 --> 01:23:52,870
to find out

1348
01:23:52,930 --> 01:23:55,790
i didn't want to waste your time on taking the derivatives of such a simple

1349
01:23:58,510 --> 01:24:00,490
so if we take this

1350
01:24:00,500 --> 01:24:03,930
right now then i will substitute

1351
01:24:04,010 --> 01:24:07,510
these results in the equation so you did the hard work

1352
01:24:07,530 --> 01:24:11,410
you did the velocities i will do the position so i'm going to substitute in

1353
01:24:11,410 --> 01:24:15,710
that equation t equals zero i know already that the phi sign all there

1354
01:24:15,710 --> 01:24:18,940
and so then i get that c

1355
01:24:18,990 --> 01:24:20,750
time equals zero

1356
01:24:20,800 --> 01:24:24,600
it's going to be x zero minus

1357
01:24:24,610 --> 01:24:26,190
x zero minus

1358
01:24:26,240 --> 01:24:29,930
he is zero so this is why he is zero so this is why

1359
01:24:29,980 --> 01:24:34,450
plus axial plus

1360
01:24:35,660 --> 01:24:37,180
this initial condition

1361
01:24:37,210 --> 01:24:41,750
then i got to the second initial conditions zero

1362
01:24:41,800 --> 01:24:44,100
it's actually minus

1363
01:24:47,650 --> 01:24:50,230
x zero plus

1364
01:24:50,280 --> 01:24:52,670
minus sign

1365
01:24:52,670 --> 01:24:56,080
but what this high dimensional data started saying that in high dimensions

1366
01:24:57,570 --> 01:24:58,390
the prototype

1367
01:24:58,870 --> 01:25:00,230
is never going to be observed

1368
01:25:00,820 --> 01:25:02,450
because you always be corrupted

1369
01:25:02,970 --> 01:25:03,450
in the shell

1370
01:25:03,860 --> 01:25:05,720
some distance away from the prototype

1371
01:25:06,160 --> 01:25:07,370
which is kind of an odd thing

1372
01:25:09,930 --> 01:25:14,440
how does not go on so i suppose used you hear this nice thing okay fine

1373
01:25:15,570 --> 01:25:18,370
and people would prove things about it but i never had intuition so

1374
01:25:18,920 --> 01:25:24,540
this is an exercise in trying to understand the intuition and is based around the calcium echolocators another exercise

1375
01:25:25,360 --> 01:25:26,750
these are curse-of-dimensionality

1376
01:25:27,730 --> 01:25:29,800
exercises in the old bishop's book

1377
01:25:30,670 --> 01:25:34,740
and what i want you to show you see in this a one-dimensional egg and i'm showing

1378
01:25:35,360 --> 01:25:38,420
identity across this sick yellow in the middle is the yolk

1379
01:25:39,270 --> 01:25:42,560
and then the white on the outside is this development and then

1380
01:25:42,990 --> 01:25:44,110
i put a green that

1381
01:25:46,330 --> 01:25:48,080
right at the interface between the young

1382
01:25:48,860 --> 01:25:50,220
and now and the whiter egg

1383
01:25:50,690 --> 01:25:54,550
so green there is overboard during like a

1384
01:25:55,010 --> 01:25:55,710
some places do

1385
01:25:56,550 --> 01:25:57,820
you see on the outside it

1386
01:25:58,410 --> 01:26:03,670
the green covering its very iron sulfide and it's my eggs smell bad because they've

1387
01:26:03,670 --> 01:26:06,910
got sulfur them so the green there is no more than

1388
01:26:08,110 --> 01:26:11,670
it's got a relatively thin layer that makes a case bad actually so

1389
01:26:12,080 --> 01:26:13,630
you shouldn't over water x

1390
01:26:14,210 --> 01:26:14,670
on the

1391
01:26:15,340 --> 01:26:17,740
it's been boiled and way we set it up is that

1392
01:26:19,060 --> 01:26:26,100
the iron sulfide is between point nine five and one point o five standard deviations from the mean so between

1393
01:26:26,510 --> 01:26:30,390
ninety five percent and one hundred five percent and the standard deviation from the mean

1394
01:26:31,170 --> 01:26:34,780
so he looks up to that point nine five the standard deviation and the white

1395
01:26:34,910 --> 01:26:38,250
goes from one point o five to infinity this is a big

1396
01:26:39,090 --> 01:26:44,470
i mean what really would be a square density right so here with fitting a gas intensity across it

1397
01:26:45,730 --> 01:26:49,680
so that's one dimensional leg so if we look at the one-dimensional and we find

1398
01:26:49,750 --> 01:26:53,450
it's really nice could the yoke is my favorite the egg that's nice and running

1399
01:26:53,450 --> 01:26:58,170
in tasty and sixty five point eight percent your egg is the tasty bit okay

1400
01:26:58,170 --> 01:27:02,640
the why it is a more healthy it's got protein in twenty nine percent over

1401
01:27:03,190 --> 01:27:04,020
your egg is

1402
01:27:04,460 --> 01:27:09,160
so near close to thirty percent if your leg is protein and five percent vegas's

1403
01:27:09,160 --> 01:27:11,250
nasty iron sulfide that you don't like

1404
01:27:13,240 --> 01:27:15,940
okay so what happens in two dimensions you can compute these

1405
01:27:16,450 --> 01:27:22,590
volumes again the volume that's obviously a probability distribution so that the total volumes hundred

1406
01:27:22,590 --> 01:27:27,440
percent and in two dimensions you get sixty percent fewer actually you've lost five percent

1407
01:27:29,300 --> 01:27:35,110
i am seven point four percent zurich is now the iron sulfide and thirty three

1408
01:27:35,110 --> 01:27:37,550
percent thirty acres is now in the white

1409
01:27:38,430 --> 01:27:42,440
okay the reason right obviously because this high density here low-density here so even tho

1410
01:27:42,440 --> 01:27:43,730
you're sort of saying this is now

1411
01:27:44,230 --> 01:27:48,700
the viewer probably egg and this is a guassian x services higher peak is a

1412
01:27:48,710 --> 01:27:52,460
mountain bike there so there's more density in the middle and there's only outside

1413
01:27:54,890 --> 01:27:59,040
okay but now got three dimensions the density outside goes up a bit ten percent

1414
01:27:59,120 --> 01:28:02,710
the density is now in the science of i fifty six percent so we've lost

1415
01:28:02,710 --> 01:28:03,020
some of the

1416
01:28:03,540 --> 01:28:07,020
how does behave as we go higher and higher dimension

1417
01:28:07,990 --> 01:28:11,190
well the nice thing so we've seen this sort order but that's seems to be

1418
01:28:11,190 --> 01:28:13,050
going up white goes up yeah

1419
01:28:13,650 --> 01:28:18,240
you goes down and iron sulfide goes up so does not continue to be the case

1420
01:28:18,720 --> 01:28:21,790
well what it turns out is we can actually compute

1421
01:28:22,290 --> 01:28:24,380
the density where the probability masses

1422
01:28:26,780 --> 01:28:31,170
if as if we're thinking about this and in high dimensions all i'm doing is

1423
01:28:31,400 --> 01:28:37,560
i'm thinking of gas in distribution well i'm sampling independently across axes with a fixed

1424
01:28:38,030 --> 01:28:41,030
variance so that the variance the gaussians is sigma squared

1425
01:28:41,630 --> 01:28:46,800
what it turns out is the distance from the mean which is obviously from pythagoras

1426
01:28:46,800 --> 01:28:51,890
theorem the square root why i want plus why i to squared so his sample

1427
01:28:51,890 --> 01:28:53,170
one his sample to

1428
01:28:53,650 --> 01:28:58,240
that's where we are not that's distance from the mean yet so the distance is given by the square root

1429
01:28:58,730 --> 01:29:01,780
now i can't do much with the square root what you can but it's actually

1430
01:29:01,780 --> 01:29:05,360
easier to work with the square of the distance and that's gonna be an ongoing

1431
01:29:06,420 --> 01:29:07,580
in everything i do

1432
01:29:08,170 --> 01:29:10,230
so it turns out that the square

1433
01:29:10,860 --> 01:29:15,530
all this distance is kai's square distributed yes that's a statistical known

1434
01:29:16,040 --> 01:29:16,300
o thing

1435
01:29:16,940 --> 01:29:17,810
used quite a lot

1436
01:29:18,230 --> 01:29:22,180
so each of these samples their length is kai square distributed

1437
01:29:22,630 --> 01:29:24,140
so if you want the square

1438
01:29:24,730 --> 01:29:28,020
the distance of the whole thing is just the square error but

1439
01:29:28,460 --> 01:29:31,340
then is the sum of some kind i squared distributed

1440
01:29:33,010 --> 01:29:34,050
so that's rather nice

1441
01:29:34,610 --> 01:29:36,160
because it's analytics so

1442
01:29:36,160 --> 01:29:39,260
a year after being is

1443
01:29:39,300 --> 01:29:42,580
you see

1444
01:29:42,580 --> 01:29:46,180
it's just a sequence of integers edges

1445
01:29:46,240 --> 01:29:49,050
will define what the block and that is

1446
01:29:50,140 --> 01:29:55,470
you have pay attention because is sometimes keep

1447
01:29:55,560 --> 01:29:57,660
a blocked path

1448
01:29:57,660 --> 01:30:00,280
is about which contains

1449
01:30:00,300 --> 01:30:01,600
the following

1450
01:30:01,640 --> 01:30:02,800
and the surface

1451
01:30:02,820 --> 01:30:07,200
tail to tail or head tail

1452
01:30:07,220 --> 01:30:11,600
one of these

1453
01:30:13,220 --> 01:30:16,780
i mean

1454
01:30:16,800 --> 01:30:25,240
is that contains

1455
01:30:31,410 --> 01:30:34,100
are these

1456
01:30:36,450 --> 01:30:41,550
these these are this or

1457
01:30:41,600 --> 01:30:43,260
i had to

1458
01:30:43,300 --> 01:30:45,550
the tale tales

1459
01:30:45,550 --> 01:30:47,010
i had failed

1460
01:30:55,240 --> 01:30:57,220
head to head

1461
01:31:01,740 --> 01:31:02,680
or had

1462
01:31:02,740 --> 01:31:06,050
no which is not observed

1463
01:31:06,100 --> 01:31:08,560
not observe not conditions

1464
01:31:08,580 --> 01:31:11,390
not any of its this descendants is observed

1465
01:31:11,390 --> 01:31:15,700
so again let's let's go to the image

1466
01:31:15,760 --> 01:31:16,870
i have that

1467
01:31:16,890 --> 01:31:19,160
for example this path here

1468
01:31:19,220 --> 01:31:22,010
a e f b

1469
01:31:22,050 --> 01:31:23,970
OK the

1470
01:31:24,030 --> 01:31:28,830
let's cheque whether this is blocked or not

1471
01:31:28,850 --> 01:31:31,490
for this they have to be blocked

1472
01:31:31,510 --> 01:31:34,180
either of these two conditions

1473
01:31:34,220 --> 01:31:35,200
it has to hold

1474
01:31:35,220 --> 01:31:36,620
one or the other

1475
01:31:37,850 --> 01:31:39,680
i checked the first edition

1476
01:31:39,720 --> 01:31:41,890
these that has

1477
01:31:41,910 --> 01:31:43,450
an observer

1478
01:31:43,450 --> 01:31:44,320
it you know

1479
01:31:44,330 --> 01:31:47,780
they have an absurdity know

1480
01:31:49,490 --> 01:31:52,160
because it has an observer html

1481
01:31:52,200 --> 01:31:55,430
not by the way doesn't have any observed

1482
01:31:55,490 --> 01:31:57,890
is that it doesn't have any observable

1483
01:31:57,950 --> 01:32:01,510
so the first condition is not met

1484
01:32:03,200 --> 01:32:07,200
now let's check whether the second condition is met

1485
01:32:08,660 --> 01:32:09,990
must contain

1486
01:32:09,990 --> 01:32:14,990
h h and also had to have no

1487
01:32:15,030 --> 01:32:20,100
well the only has two has molding this that these

1488
01:32:20,140 --> 01:32:24,010
must contain has which is not also

1489
01:32:24,030 --> 01:32:26,030
this one is not observed

1490
01:32:26,030 --> 01:32:31,580
nor any of its descendants are observed or is same

1491
01:32:31,740 --> 01:32:38,410
doesn't satisfy the second condition idea

1492
01:32:38,490 --> 01:32:49,240
say again

1493
01:32:52,370 --> 01:33:00,390
g if if she was here

1494
01:33:00,390 --> 01:33:03,010
and then there was an arrow from j to be

1495
01:33:07,450 --> 01:33:11,200
because the bad ending b

1496
01:33:11,220 --> 01:33:13,120
not all if you consider the that

1497
01:33:13,180 --> 01:33:16,830
eight the change

1498
01:33:16,830 --> 01:33:18,430
then it's back

1499
01:33:18,450 --> 01:33:20,330
i will be blocked

1500
01:33:21,350 --> 01:33:22,970
you have a child

1501
01:33:23,010 --> 01:33:25,120
which is not observed

1502
01:33:25,160 --> 01:33:29,510
and and that suffices

1503
01:33:30,760 --> 01:33:35,890
and it's not observe big if there is no children this condition does not apply

1504
01:33:35,890 --> 01:33:38,910
if i had just one notion of this

1505
01:33:45,050 --> 01:33:45,990
there is one

1506
01:33:45,990 --> 01:33:50,840
the idea is that there's always something goes wrong in so that the issue 1st

1507
01:33:50,840 --> 01:33:54,500
start up work for for project is

1508
01:33:54,540 --> 01:33:58,120
if like does come where you're going to do to be ready in Howard you're

1509
01:33:58,120 --> 01:34:03,440
going to be ready now another another website that I used kind illustrate how to

1510
01:34:03,440 --> 01:34:08,760
solve the problem that another full life-cycle someone in small among is run by a

1511
01:34:08,760 --> 01:34:14,420
guy named Don Mccaskell who happens to museums on history or storage service and he

1512
01:34:14,420 --> 01:34:17,580
had a scaling problems like 2 years ago

1513
01:34:17,620 --> 01:34:19,300
In his problem was

1514
01:34:19,320 --> 01:34:22,940
that he had saturated in storage area network

1515
01:34:23,330 --> 01:34:27,360
now we was lucky and we launched Amazon history in the week that he had

1516
01:34:27,360 --> 01:34:31,540
the problem so he was able to deploy solution but if you think about it

1517
01:34:31,540 --> 01:34:35,240
you know some things you can't just throw money at us all into can't just

1518
01:34:35,240 --> 01:34:39,360
go down at the computer store and buy a new storage area storage area network

1519
01:34:39,360 --> 01:34:42,420
and plug it in and of course go problems not that simple

1520
01:34:42,720 --> 01:34:44,680
so you really have to to think about that

1521
01:34:44,820 --> 01:34:46,660
here's another

1522
01:34:46,700 --> 01:34:51,720
this was interesting in several ways 1st of all those 2 lines at the bottom

1523
01:34:51,820 --> 01:34:54,080
of the same 2 lines we just looked at

1524
01:34:54,240 --> 01:34:57,240
In the green line in this case is Facebook

1525
01:34:57,560 --> 01:35:02,440
I people in the room here have a Facebook account but case so that most

1526
01:35:02,440 --> 01:35:08,350
of you so I thought I think it's interesting on several fronts 1st of all

1527
01:35:08,400 --> 01:35:10,640
you can see that the portion here

1528
01:35:10,820 --> 01:35:15,620
it is however its servers fast enough to keep up with demand so this is

1529
01:35:15,620 --> 01:35:19,820
a different kinds of great kind of problem but it's still so where you're going

1530
01:35:19,820 --> 01:35:23,820
to come up with servers fast enough you know I we were going to order

1531
01:35:23,820 --> 01:35:27,620
the following is going to take to get here or you can argue with when

1532
01:35:27,620 --> 01:35:29,920
they don't show up on time or the truck

1533
01:35:29,940 --> 01:35:31,260
you don't get a bump in

1534
01:35:31,320 --> 01:35:35,560
they crashed over and decide on that broken on yet to bolt the man all

1535
01:35:35,560 --> 01:35:37,540
those problems you have to deal with

1536
01:35:37,700 --> 01:35:41,880
but it also shows something else that I I find fascinating above those of you

1537
01:35:41,880 --> 01:35:46,500
who have faced bitter Facebook users any of European like since the 1st stopped using

1538
01:35:47,380 --> 01:35:54,100
a case of some of the amendments but because I find this fascinating to which

1539
01:35:54,110 --> 01:35:57,680
is you know they had a big drop in traffic now interestingly it looks like

1540
01:35:57,680 --> 01:35:59,360
the story for all that

1541
01:35:59,420 --> 01:36:04,540
all the less than a grafted hadn't started but that brings up another issue and

1542
01:36:04,540 --> 01:36:07,530
I'm not saying that this is what happened Facebook it might be a glitch in

1543
01:36:07,590 --> 01:36:09,740
graft but

1544
01:36:09,780 --> 01:36:11,560
sometimes brevity sets in

1545
01:36:11,600 --> 01:36:14,460
and then we're going to do let's say for example you can see that's not

1546
01:36:14,460 --> 01:36:17,700
the case but let's say for example that the traffic was down here for the

1547
01:36:17,700 --> 01:36:22,060
war in state you for what you have to do with all that

1548
01:36:22,660 --> 01:36:26,100
your immediately big investors so that other thing to think about

1549
01:36:26,840 --> 01:36:31,040
and then finally I just shot I just this 1 the 2nd 1

1550
01:36:31,100 --> 01:36:32,720
now you can plan for the worst

1551
01:36:33,220 --> 01:36:37,550
now this 1 is interesting actually could you plan for it this is Mozilla that

1552
01:36:37,550 --> 01:36:43,340
comes the download site for Firefox and I would say that they're doing quite well

1553
01:36:43,340 --> 01:36:44,760
the black eyes

1554
01:36:45,040 --> 01:36:48,410
would you know you don't need all the server capacity here I really don't know

1555
01:36:49,380 --> 01:36:52,340
what this demand is going to be here so

1556
01:36:52,420 --> 01:36:56,120
you know if you only have integrated needs or in a case like this all

1557
01:36:56,120 --> 01:37:00,060
the doing his downloading maybe you really don't need a Web servers such a tall

1558
01:37:00,060 --> 01:37:03,160
but these were all the kinds of things that you need to think about

1559
01:37:03,200 --> 01:37:05,380
it when you're I've got plenty of

1560
01:37:05,440 --> 01:37:07,640
the project known

1561
01:37:07,680 --> 01:37:09,740
we didn't surveys him

1562
01:37:09,780 --> 01:37:15,690
inside and we talked to our project teams and we discovered that pretty much across

1563
01:37:15,690 --> 01:37:18,640
all of our product teams and so that about

1564
01:37:18,680 --> 01:37:22,380
70 per cent of their time was spent on these infrastructure issues

1565
01:37:22,600 --> 01:37:27,400
that I took that much time to negotiate data center space to line up the

1566
01:37:27,410 --> 01:37:31,880
server fleet to make certain the server showed up on the nature of the service

1567
01:37:31,880 --> 01:37:35,920
work when they showed up on bolt the Minnesota etc

1568
01:37:36,640 --> 01:37:39,760
this is we call this the monarch

1569
01:37:39,900 --> 01:37:42,780
In what we're saying here is that you shouldn't have to do this you know

1570
01:37:42,780 --> 01:37:47,560
this is a lot like job were like that when job net came along

1571
01:37:47,600 --> 01:37:50,580
you were able to do away with some of level

1572
01:37:50,620 --> 01:37:54,180
work that you step into writing code in C plus plus so now you don't

1573
01:37:54,180 --> 01:37:58,520
have to for example manage memory the idea of a little control but maybe that's

1574
01:37:58,520 --> 01:37:59,300
not a bad thing

1575
01:37:59,590 --> 01:38:02,560
but you're able to be much more productive and so the same it's basically the

1576
01:38:02,560 --> 01:38:08,830
same idea again this time infrastructure pockets so

1577
01:38:08,900 --> 01:38:12,930
I want to do is just surf through a few and certainly not all of

1578
01:38:12,930 --> 01:38:17,400
these Web services if I did we'd be here all nite but when going to

1579
01:38:18,440 --> 01:38:22,880
His talk a little bit about the infrastructures service appear in the left corner but

1580
01:38:22,880 --> 01:38:23,540
we have

1581
01:38:23,620 --> 01:38:26,600
a number of other services that I'm not going to touch on tonight unless somebody

1582
01:38:26,600 --> 01:38:28,360
really wants hear

1583
01:38:28,820 --> 01:38:32,960
any questions or at other ways and just an indictment

1584
01:38:34,160 --> 01:38:41,000
but they so the 1st like talked about in Amazon simple storage service otherwise known

1585
01:38:41,000 --> 01:38:47,440
as Arizona 3 not like all these Web services it's the 1st and foremost is

1586
01:38:47,440 --> 01:38:49,140
the weather service meaning that

1587
01:38:49,200 --> 01:38:54,940
there is an XML will request response protocol going across the wire so the idea

1588
01:38:54,940 --> 01:38:58,660
here is that when you push data into the club

1589
01:38:58,700 --> 01:39:02,260
the store DataClub the club really lands and an Sunday

1590
01:39:02,680 --> 01:39:07,360
and we knew pushed it into the cloud your making an authenticated our Web service

1591
01:39:07,360 --> 01:39:10,880
called using a public key private based on the show of hands at the most

1592
01:39:10,880 --> 01:39:15,300
of you already know that I'll ever get to create a signature Though the consists

1593
01:39:15,300 --> 01:39:20,200
of a pretty complex Hashim the name of a bucket in an object in the

1594
01:39:20,200 --> 01:39:27,700
API method you're calling time time live on the production but

1595
01:39:27,740 --> 01:39:33,950
the point is that people win this data lands in the Amazon data center it

1596
01:39:33,950 --> 01:39:38,140
works a lot like a rate that is that there's mobile no rules for the

1597
01:39:38,140 --> 01:39:42,520
Dayton strike across insider data center in the idea there

1598
01:39:42,560 --> 01:39:47,740
Is that if something happens inside our data center for example racked goes down or

1599
01:39:47,740 --> 01:39:48,940
something like that

1600
01:39:48,980 --> 01:39:50,340
you never see it

1601
01:39:50,360 --> 01:39:54,790
you never see a because just like reading the other parts were still the others

1602
01:39:54,800 --> 01:40:00,920
images are still available and you and I I we just make you know you

1603
01:40:00,920 --> 01:40:04,180
just get served the data and you're not even aware that we had a problem

1604
01:40:04,180 --> 01:40:07,940
behind the scenes and of course behind the scenes we do things like take the

1605
01:40:07,940 --> 01:40:13,260
defective Rakoff bring in a blank cracked revealed the status of its always available

1606
01:40:13,520 --> 01:40:15,560
all we take it 1 step further

1607
01:40:15,620 --> 01:40:20,920
we actually replicate the data center in real time so that the mobile

1608
01:40:21,040 --> 01:40:25,040
real-time data centers all synchronized with each other and that way

1609
01:40:25,080 --> 01:40:29,560
if there's a problem with the interrogators and so for example of somebody cut the

1610
01:40:29,590 --> 01:40:32,920
fiber in the parking lot were you know the switch to the data center goes

1611
01:40:32,920 --> 01:40:34,400
down something like that

1612
01:40:34,440 --> 01:40:36,120
you data is still available

1613
01:40:36,160 --> 01:40:41,000
and in separated into what we call availability not talking about availability is always a

1614
01:40:41,000 --> 01:40:46,280
bit more interested in something I don't want to do well on pricing because I'm

1615
01:40:46,280 --> 01:40:49,900
not here to sell you something here to tell you about the technology but I

1616
01:40:49,900 --> 01:40:54,750
really think I can't ignore it simply because this is like a 50 per cent

1617
01:40:54,760 --> 01:40:57,340
of what seems to be that people want

1618
01:40:57,380 --> 01:40:58,880
In excited about

1619
01:40:58,960 --> 01:41:04,020
all these Web services so the 1st pockets got people excited is the fact that

1620
01:41:04,020 --> 01:41:06,210
my god

1621
01:41:06,230 --> 01:41:12,020
it's too early to hear myself

1622
01:41:12,070 --> 01:41:20,250
which is not good for

1623
01:41:22,420 --> 01:41:25,330
so have figured it out yet

1624
01:41:28,560 --> 01:41:31,690
you could say that the good

1625
01:41:31,710 --> 01:41:35,190
all right well it turns out the

1626
01:41:35,200 --> 01:41:39,980
nice friendly people google provided the tool for us to be able to answer questions

1627
01:41:39,980 --> 01:41:41,120
like this

1628
01:41:41,760 --> 01:41:44,890
you know like all questions we go to google

1629
01:41:45,970 --> 01:41:48,800
the tool they provide it isn't just the plain google

1630
01:41:48,810 --> 01:41:53,950
there's there's something called google that which is in the google labs website is an

1631
01:41:53,950 --> 01:41:56,640
experimental tool not really meant for any

1632
01:41:56,650 --> 01:41:58,310
commercial purposes

1633
01:41:58,330 --> 01:42:01,780
and it's really fun cool so that

1634
01:42:01,830 --> 01:42:05,710
and you few items from the that of things

1635
01:42:05,720 --> 01:42:08,980
and then next press one of these buttons down here

1636
01:42:09,030 --> 01:42:13,960
and we'll try to predict other items in the that's OK

1637
01:42:13,970 --> 01:42:16,190
so we entergy

1638
01:42:16,240 --> 01:42:17,830
and darwin

1639
01:42:17,840 --> 01:42:21,210
we press one of these buttons

1640
01:42:21,260 --> 01:42:24,090
and here's the list that google comes up with

1641
01:42:24,310 --> 01:42:26,970
darwin g the

1642
01:42:28,950 --> 01:42:30,370
god will

1643
01:42:30,380 --> 01:42:34,660
downing curtain with the cavendish william the crime

1644
01:42:34,710 --> 01:42:37,480
king and corpus christi

1645
01:42:39,170 --> 01:42:40,730
most of you that says

1646
01:42:42,320 --> 01:42:45,740
but these are actually it turns out these are

1647
01:42:45,750 --> 01:42:48,680
the name of the college of cambridge university

1648
01:42:48,990 --> 01:42:54,380
cambridge happens to have college of darwin another college of jesus

1649
01:42:54,430 --> 01:42:59,290
and are my best guess is that there are a bunch of lists on the

1650
01:42:59,290 --> 01:43:00,330
web of

1651
01:43:00,400 --> 01:43:02,490
the names of colleges in cambridge

1652
01:43:02,500 --> 01:43:04,250
and what google that

1653
01:43:04,300 --> 01:43:08,510
it seems to be doing is finding those lists saying

1654
01:43:08,520 --> 01:43:10,900
there are two elements of this list

1655
01:43:11,420 --> 01:43:15,140
i can predict the other elements of the work

1656
01:43:15,150 --> 01:43:20,160
anyway i invite you all to play with google that it's a great tool it

1657
01:43:20,160 --> 01:43:24,750
works sometimes extremely well and sometimes the really don't work at all

1658
01:43:25,040 --> 01:43:29,600
but what it did was it made us think of the really i think interesting

1659
01:43:29,600 --> 01:43:32,830
machine learning and information retrieval problem

1660
01:43:32,880 --> 01:43:34,270
and this is

1661
01:43:34,310 --> 01:43:41,380
the problem that i can formally in the following way listing more abstractly

1662
01:43:42,750 --> 01:43:45,120
and imagine that

1663
01:43:45,210 --> 01:43:48,620
we have a universe of objects the

1664
01:43:48,670 --> 01:43:54,400
now i've shown you pictures of things like the yellow train car london by the

1665
01:43:54,660 --> 01:43:56,410
side centre

1666
01:43:56,460 --> 01:44:00,740
but i don't think of those pictures think of any kind of object you might

1667
01:44:00,740 --> 01:44:02,380
want to search for

1668
01:44:02,980 --> 01:44:06,180
the universe of items could be documents that could be

1669
01:44:07,580 --> 01:44:08,810
mp three

1670
01:44:08,830 --> 01:44:13,180
whatever you want

1671
01:44:13,190 --> 01:44:16,490
it could be a mixture of objects of different kinds

1672
01:44:16,500 --> 01:44:21,490
as long as there all represented in some common

1673
01:44:21,500 --> 01:44:26,860
four which is tricky if you want to represent documents and pictures the form

1674
01:44:26,870 --> 01:44:29,510
but you know you could imagine saying

1675
01:44:29,520 --> 01:44:34,070
maybe having a representation where there's the like this that the picture and then you

1676
01:44:34,070 --> 01:44:38,420
have got the picture features that doesn't have any document features or something like that

1677
01:44:38,460 --> 01:44:43,180
but that is the basic requirement is that all the objects should be represented in

1678
01:44:43,180 --> 01:44:45,150
some common space

1679
01:44:47,390 --> 01:44:50,750
we can think of this problem in several different ways one of the ways we

1680
01:44:50,750 --> 01:44:55,190
could think of it as clustering on the band so one thing you could do

1681
01:44:55,420 --> 01:44:59,780
the universe of of objects is run your favourite clustering algorithm right

1682
01:44:59,830 --> 01:45:02,710
but that's not really what we're interested in here

1683
01:45:02,730 --> 01:45:06,140
well we're interested in is the following question

1684
01:45:06,190 --> 01:45:07,780
given a query

1685
01:45:07,830 --> 01:45:09,700
which is

1686
01:45:09,750 --> 01:45:12,610
that our universe of items

1687
01:45:12,620 --> 01:45:13,960
like for example

1688
01:45:13,970 --> 01:45:16,200
the red car and the blue car

1689
01:45:16,210 --> 01:45:21,180
we want retrieval algorithm that will return a larger set

1690
01:45:21,190 --> 01:45:23,240
of our universe divided

1691
01:45:24,530 --> 01:45:27,160
it is a good generalisation of

1692
01:45:27,450 --> 01:45:31,510
this marks that that we gave the given red-carpet car we would hope that could

1693
01:45:31,510 --> 01:45:32,500
give us back

1694
01:45:32,550 --> 01:45:33,980
the cars

1695
01:45:35,140 --> 01:45:36,480
well given

1696
01:45:36,500 --> 01:45:42,410
redcar tomato we would hope that it could give back maybe the red object

1697
01:45:43,970 --> 01:45:47,760
and the basic idea the reason we could call the clustering on demand is that

1698
01:45:48,030 --> 01:45:50,720
we're trying to find the cluster right

1699
01:45:50,770 --> 01:45:54,550
but instead of being completely unsupervised what we do is

1700
01:45:54,740 --> 01:45:58,560
as a query we give it a few points that we

1701
01:45:58,670 --> 01:46:03,900
search belong to some cluster concept that we're interested in and we want the algorithm

1702
01:46:03,940 --> 01:46:11,900
to infer what other objects belong to that work on

1703
01:46:11,910 --> 01:46:16,150
and we can think of this is the problem of generalisation from a small set

1704
01:46:16,150 --> 01:46:17,240
of ideas

1705
01:46:17,290 --> 01:46:19,710
so the query is the set of items

1706
01:46:19,730 --> 01:46:24,890
the the the the meaning like cost cluster concept

1707
01:46:24,900 --> 01:46:29,230
and the information through method should rank other items x

1708
01:46:29,280 --> 01:46:32,450
by how well x fifteen with the query set

1709
01:46:32,460 --> 01:46:37,670
so imagine we now represented are objects in terms of some features based on measured

1710
01:46:37,670 --> 01:46:39,690
features of the object

1711
01:46:39,740 --> 01:46:42,150
and imagine that our query is

1712
01:46:42,230 --> 01:46:47,150
all of the blue object here the web two three four five six seven eight

1713
01:46:49,340 --> 01:46:50,640
and the

1714
01:46:50,650 --> 01:46:53,530
i don't we're trying to score if it here

1715
01:46:53,550 --> 01:46:58,220
we might think that it has that it doesn't seem to belong in the query

1716
01:46:58,230 --> 01:47:00,540
set that would here it might belong

1717
01:47:00,550 --> 01:47:05,110
in the group that so you prefer to read

1718
01:47:05,150 --> 01:47:07,890
this idea higher than the item

1719
01:47:07,890 --> 01:47:10,990
you want to know about them individually the case

1720
01:47:10,990 --> 01:47:13,330
so you like some that continually

1721
01:47:13,330 --> 01:47:19,910
the third dimension the process experience

1722
01:47:19,910 --> 01:47:21,950
look is often a small cross sectional

1723
01:47:21,970 --> 01:47:24,560
you might be more the end of longitudinal

1724
01:47:25,220 --> 01:47:29,510
that captures we think of a lot of studies that you can you can do

1725
01:47:30,680 --> 01:47:34,260
this is something that is still was still working on right now and this kind

1726
01:47:34,970 --> 01:47:37,330
not to talk about a very long something is going to be in the handbook

1727
01:47:37,330 --> 01:47:39,120
of mixed methods research second edition

1728
01:47:39,140 --> 01:47:41,310
so we still invite as we speak OK

1729
01:47:41,370 --> 01:47:44,490
it's just that we have to come up to point to answer to come up

1730
01:47:44,490 --> 01:47:49,070
with a inclusive model that includes all everything has been written about expected data analysis

1731
01:47:49,070 --> 01:47:50,700
across all the fields

1732
01:47:50,700 --> 01:47:52,740
i mean identified thirteen

1733
01:47:52,810 --> 01:47:54,200
kind of decisions

1734
01:47:54,200 --> 01:47:58,200
so this this diagram represents for decisions that you make

1735
01:47:58,220 --> 01:48:01,870
and when you do it makes analysis

1736
01:48:01,870 --> 01:48:03,180
so the top there

1737
01:48:03,200 --> 01:48:08,640
once you you know you've decided research goal objective research questions rational and so forth

1738
01:48:08,660 --> 01:48:10,370
it leads you to the

1739
01:48:10,370 --> 01:48:11,040
you know

1740
01:48:11,080 --> 01:48:13,240
how times that gonna collect

1741
01:48:13,290 --> 01:48:16,010
i gonna collect qualitative data

1742
01:48:16,060 --> 01:48:18,100
and qualitative data

1743
01:48:18,120 --> 01:48:21,290
or this one because you can actually do make a fist a funicular one time

1744
01:48:21,450 --> 01:48:23,430
but transport transformation

1745
01:48:25,660 --> 01:48:28,540
western michigan take place where you make those decisions

1746
01:48:28,560 --> 01:48:31,270
before the study begins to make all your decisions

1747
01:48:31,290 --> 01:48:32,410
during the study

1748
01:48:32,430 --> 01:48:33,530
or you can make some

1749
01:48:33,540 --> 01:48:35,970
the combination of that

1750
01:48:36,010 --> 01:48:40,660
what's your rationale and purpose using can actually agree with the person who opposed evaluators

1751
01:48:40,680 --> 01:48:42,580
is it to triangulate

1752
01:48:42,660 --> 01:48:46,950
triangulation means that you want to see if he is consistent

1753
01:48:46,990 --> 01:48:50,510
so you want to find out your political affiliation

1754
01:48:50,560 --> 01:48:54,680
i could give you some some scale that kind of tax that

1755
01:48:55,580 --> 01:48:58,140
based on political

1756
01:48:58,180 --> 01:49:01,470
philosophy i could also observe u

1757
01:49:01,470 --> 01:49:04,970
and so forth so many different ways and if the quality and the core data

1758
01:49:05,010 --> 01:49:06,910
give you the same conclusions

1759
01:49:06,950 --> 01:49:10,290
you can see the triangulated your data and that's one powerful way to mixed methods

1760
01:49:10,290 --> 01:49:13,950
triangulation number with this complementary to complement

1761
01:49:13,950 --> 01:49:15,890
OK one to cover the

1762
01:49:15,910 --> 01:49:19,680
number times initiation

1763
01:49:19,700 --> 01:49:22,350
that's look for contradictions and tensions so

1764
01:49:22,370 --> 01:49:24,770
and then after you feel get

1765
01:49:24,850 --> 01:49:27,540
according to maybe contradicted the quality

1766
01:49:27,560 --> 01:49:31,450
that's not bad news is often good news when you think some of the best

1767
01:49:31,470 --> 01:49:33,350
over time so my best

1768
01:49:33,370 --> 01:49:35,330
innovations of com

1769
01:49:35,350 --> 01:49:37,540
when there are contradictions

1770
01:49:38,160 --> 01:49:41,330
the first is i realized that the earth is not flat

1771
01:49:41,330 --> 01:49:43,410
that was really important decision

1772
01:49:43,470 --> 01:49:45,700
observations so

1773
01:49:46,310 --> 01:49:48,990
what's your mixed analysis philosophy

1774
01:49:51,080 --> 01:49:53,290
so four

1775
01:49:53,290 --> 01:49:55,540
if you were going down this

1776
01:49:55,560 --> 01:49:59,180
this map and of course it's much more obvious when you read the article that

1777
01:49:59,180 --> 01:50:01,120
space is those in detail

1778
01:50:01,930 --> 01:50:03,430
it should help you make

1779
01:50:03,430 --> 01:50:05,620
decisions that you need to do mixed analysis

1780
01:50:06,370 --> 01:50:08,430
again this is still work-in-progress it's

1781
01:50:08,430 --> 01:50:11,350
imagine it close to being the final form

1782
01:50:12,700 --> 01:50:14,180
as we speak french

1783
01:50:14,240 --> 01:50:17,560
as of the other chapters is about thirty seven chapters in the handbook on different

1784
01:50:18,450 --> 01:50:20,680
so it's very exciting

1785
01:50:20,700 --> 01:50:22,740
o come out next year

1786
01:50:22,740 --> 01:50:25,680
so we moved to sets ten and eleven

1787
01:50:25,700 --> 01:50:30,850
and that's legitimation thing that's really important that we always whatever study we do

1788
01:50:30,930 --> 01:50:33,160
we looked to legitimate findings

1789
01:50:34,390 --> 01:50:38,680
and we have come up with some different models to help you do that so

1790
01:50:38,680 --> 01:50:40,660
the quantitative way

1791
01:50:40,660 --> 01:50:42,120
you have

1792
01:50:42,140 --> 01:50:44,310
this builds on campbell and stanley

1793
01:50:44,330 --> 01:50:50,490
similar work with a identified intelligibility raises external validity

1794
01:50:50,540 --> 01:50:52,220
but one of the things we notice

1795
01:50:53,600 --> 01:50:57,430
in their work was that it was very much for psychology

1796
01:50:57,430 --> 01:51:00,740
so there are lots of things for example in education and other fields

1797
01:51:00,760 --> 01:51:05,430
that's when the california in the model havoc and it was really good luck criticism

1798
01:51:05,430 --> 01:51:07,220
it's just an observation

1799
01:51:07,220 --> 01:51:09,560
and i also get the impression that only

1800
01:51:09,600 --> 01:51:13,220
they tend to talk about in terms of experiments and you know experimentally so there's

1801
01:51:13,220 --> 01:51:15,370
only one type of quantitative design

1802
01:51:15,370 --> 01:51:17,870
so we try to to come up with something that is applicable to all these

1803
01:51:17,870 --> 01:51:21,120
are very descriptive of course compounded for

1804
01:51:21,120 --> 01:51:22,180
what have you

1805
01:51:23,470 --> 01:51:26,200
the other thing that we tried to do was to recognise

1806
01:51:26,220 --> 01:51:27,290
that validity

1807
01:51:28,470 --> 01:51:32,620
can look at different phases of a qualitative study

1808
01:51:33,580 --> 01:51:36,040
when during the data collection stage

1809
01:51:37,180 --> 01:51:39,240
the data analysis stage

1810
01:51:39,290 --> 01:51:41,200
and during their

1811
01:51:41,260 --> 01:51:43,060
interpretation stage

1812
01:51:43,100 --> 01:51:46,560
so is there with for example issue of mortality which asian people jumping out of

1813
01:51:46,560 --> 01:51:48,120
the study

1814
01:51:48,160 --> 01:51:50,430
that can happen to data collection stage agreed

1815
01:51:50,470 --> 01:51:52,220
people drop out for different reasons

1816
01:51:52,240 --> 01:51:56,560
OK belong to the study they die literally when you get tired only do anymore

1817
01:51:56,560 --> 01:51:57,760
they move

1818
01:51:57,770 --> 01:51:59,180
what have you

1819
01:51:59,180 --> 01:52:01,720
that's the more traditional sensory hair mortality

1820
01:52:01,740 --> 01:52:05,600
but another way more tightly could come to the fore as the data analysis

1821
01:52:05,680 --> 01:52:08,120
you may have the information

1822
01:52:08,180 --> 01:52:10,060
but for example you don't trust it

1823
01:52:10,060 --> 01:52:11,990
you think it might be an outlier

1824
01:52:12,060 --> 01:52:13,790
to use the researcher

1825
01:52:13,810 --> 01:52:15,910
decided to move it

1826
01:52:15,950 --> 01:52:18,260
OK so that decisions

1827
01:52:18,330 --> 01:52:20,720
could affect intelligibility

1828
01:52:20,770 --> 01:52:23,220
because if you if they

1829
01:52:23,220 --> 01:52:26,640
it should be removed is an outlier that's one thing but you might have an

1830
01:52:26,640 --> 01:52:28,260
extreme observation

1831
01:52:28,310 --> 01:52:30,700
when i worked for pfizer

1832
01:52:30,720 --> 01:52:34,580
we could never ever for our lives and that's obvious

1833
01:52:35,450 --> 01:52:37,220
would have to check and down

1834
01:52:37,240 --> 01:52:40,010
it doesn't matter what part of the country the where would have to to have

1835
01:52:40,010 --> 01:52:42,970
to go back to investigate and get them to find out whether the observation is

1836
01:52:42,990 --> 01:52:43,990
about it

1837
01:52:44,040 --> 01:52:46,290
because so in something out

1838
01:52:46,330 --> 01:52:49,390
actually turns out to be true observation could affect

1839
01:52:49,450 --> 01:52:51,720
information about that particular medication

1840
01:52:52,740 --> 01:52:56,200
in most of the food get sloppy so you just throw out you know

1841
01:52:58,290 --> 01:53:00,370
as a mixed methods research will go further

1842
01:53:00,370 --> 01:53:04,180
we would say that we should try out you should do qualitative

1843
01:53:04,220 --> 01:53:07,700
get quite information from all those we think liars

1844
01:53:07,720 --> 01:53:12,080
because i could speak sometimes be much more important find about people who don't fit

1845
01:53:12,080 --> 01:53:12,950
your model

1846
01:53:12,950 --> 01:53:15,740
the people for

1847
01:53:15,760 --> 01:53:17,850
military modifying

1848
01:53:17,870 --> 01:53:21,660
the column of you have in a in a good way

1849
01:53:21,680 --> 01:53:25,620
because that's how that works just has about fifty different sources consider

1850
01:53:25,640 --> 01:53:29,530
one source the camera standing consider which is really big and lava fields

1851
01:53:29,540 --> 01:53:33,240
it's something i call the nineteen seventy matlab implementation bias

1852
01:53:33,290 --> 01:53:35,720
the extent to which the intervention

1853
01:53:35,760 --> 01:53:39,370
is implemented in a good way in the way that is meant to be implemented

1854
01:53:39,370 --> 01:53:41,910
that's really really important consideration

1855
01:53:41,970 --> 01:53:43,040
to look at

1856
01:53:43,060 --> 01:53:44,260
if it's not

1857
01:53:44,310 --> 01:53:47,240
that can affect the internal validity

1858
01:53:47,330 --> 01:53:51,040
you have several example it in the top the top end there

1859
01:53:51,080 --> 01:53:52,220
to consider

1860
01:53:53,390 --> 01:53:57,530
ecological validity is a huge issue in political sciences you know

1861
01:53:57,530 --> 01:54:00,350
if you look at voting behavior depends for example

1862
01:54:00,410 --> 01:54:04,330
if it matters what part of the country region city look

1863
01:54:04,370 --> 01:54:06,660
because people often voting clusters

1864
01:54:06,790 --> 01:54:10,580
so there are lots of little things to look at

1865
01:54:10,600 --> 01:54:11,760
on both sides

1866
01:54:11,760 --> 01:54:15,950
you have measurements of those you know for the quality instruments

1867
01:54:15,970 --> 01:54:18,680
newsom essex

1868
01:54:20,240 --> 01:54:23,600
of konstanz construct and criterion

1869
01:54:23,600 --> 01:54:25,600
the book it down a bit further

1870
01:54:25,970 --> 01:54:27,660
into the different types

1871
01:54:27,740 --> 01:54:31,200
OK all subtypes

1872
01:54:31,220 --> 01:54:35,010
then qualitative legitimation to the same quality we went to the literature

1873
01:54:35,040 --> 01:54:39,390
and found what's out there in terms of ways to legitimate your findings

1874
01:54:40,410 --> 01:54:43,240
again we will give access to these articles

1875
01:54:43,310 --> 01:54:45,370
so population

1876
01:54:45,930 --> 01:54:50,600
yes i have the same CC population doesn't that some the quantitative

1877
01:54:50,640 --> 01:54:52,640
if your goal is to generalize

1878
01:54:53,370 --> 01:54:56,390
the population of the ties in qualitative that's not the go

1879
01:54:56,450 --> 01:55:00,290
if it is that then you look at that as a potential threat

1880
01:55:01,720 --> 01:55:03,850
the show you this some of that there

1881
01:55:03,870 --> 01:55:04,720
but now

1882
01:55:04,740 --> 01:55:09,080
this really should differentiate mixed methods for quantitative and qualitative

1883
01:55:09,120 --> 01:55:13,100
is that you not only have to consider the individual legitimation issues for the quantitative

1884
01:55:13,100 --> 01:55:15,680
phases and the qualitative phases

1885
01:55:15,680 --> 01:55:18,660
but first i want to continue the story

1886
01:55:18,680 --> 01:55:24,990
so the first thing is that we try to artificially generated is news data and

1887
01:55:25,200 --> 01:55:28,290
the idea is to assume that a given user

1888
01:55:28,310 --> 01:55:33,030
is this is one of the tricks people playing information retrieval because always have good

1889
01:55:33,030 --> 01:55:37,830
data to the experiment you to do so we assume that one user is interested

1890
01:55:37,830 --> 01:55:40,010
in one class only

1891
01:55:40,060 --> 01:55:46,450
what is unknown but these the label for the user so we assume that a

1892
01:55:46,450 --> 01:55:49,620
user is only interested in one of the classes

1893
01:55:52,080 --> 01:55:54,450
so we have ten thousand articles

1894
01:55:54,470 --> 01:56:00,340
as a lot of them belong to more than one class we generate four hundred

1895
01:56:00,340 --> 01:56:06,030
sixty artificial users by choosing at random set of the positive and negative examples

1896
01:56:06,040 --> 01:56:07,760
for every user

1897
01:56:07,760 --> 01:56:12,720
and the goal is to predict the probability that a user like a new article

1898
01:56:12,720 --> 01:56:14,310
which is

1899
01:56:14,330 --> 01:56:17,930
and the plot shows we rain

1900
01:56:18,330 --> 01:56:21,260
all the unseen articles in the dataset

1901
01:56:21,270 --> 01:56:23,100
and select the

1902
01:56:23,100 --> 01:56:28,450
and highest rate articles it's like search engine you get the highest ranked on the

1903
01:56:28,450 --> 01:56:29,760
first page

1904
01:56:29,770 --> 01:56:34,430
and that just shows how many of the top and read articles

1905
01:56:34,450 --> 01:56:36,430
actually positive

1906
01:56:36,450 --> 01:56:40,790
but truly belong to the class which this user of belong

1907
01:56:40,790 --> 01:56:44,660
here some results for the experiment where

1908
01:56:44,700 --> 01:56:46,180
the active user

1909
01:56:46,200 --> 01:56:50,430
has seen five ratings has done five ratings people

1910
01:56:50,450 --> 01:56:51,350
so now

1911
01:56:51,620 --> 01:56:56,290
we present the sixth grade for him we want to because of the way

1912
01:56:56,310 --> 01:57:01,830
and here's the top and so typically what is that only ten of five

1913
01:57:01,850 --> 01:57:09,080
article to him so he left and so this is a content based approach every

1914
01:57:09,100 --> 01:57:13,930
individual has its own model performance well

1915
01:57:13,950 --> 01:57:18,370
i was this is the this is the one that one this is an i

1916
01:57:18,390 --> 01:57:22,290
was based collaborative filtering and here the

1917
01:57:22,310 --> 01:57:28,220
the features which essentially what what counts documents a quite strong social content based approach

1918
01:57:28,220 --> 01:57:32,640
is better than a collaborative approach but this

1919
01:57:32,660 --> 01:57:34,290
well i mean this

1920
01:57:36,120 --> 01:57:42,930
usually process mixture model but both so this would mean that if i present

1921
01:57:42,950 --> 01:57:49,680
five items ninety percent of them would be in the right

1922
01:57:57,200 --> 01:58:01,060
well such as use assumed like

1923
01:58:01,080 --> 01:58:05,140
news articles out of his own class

1924
01:58:05,140 --> 01:58:08,350
and there a couple of them from the same class

1925
01:58:08,370 --> 01:58:12,830
but the first is unknown to us that this has one

1926
01:58:13,790 --> 01:58:16,080
he was

1927
01:58:16,700 --> 01:58:21,060
from there we assume that each user has one reference in terms of the class

1928
01:58:22,240 --> 01:58:23,970
so in this sense

1929
01:58:27,200 --> 01:58:31,340
so that is why it is a little bit you know it's one thing to

1930
01:58:31,430 --> 01:58:37,180
the chilean andes property of the experiments that led the real world

1931
01:58:37,200 --> 01:58:39,450
it was very difficult to get is that

1932
01:58:39,470 --> 01:58:45,990
in the second experiment we have

1933
01:58:46,010 --> 01:58:50,060
this is a great painting

1934
01:58:50,080 --> 01:58:56,370
you set this up to sixty four two images

1935
01:58:56,410 --> 01:58:58,890
and nineteen

1936
01:58:58,910 --> 01:59:01,510
it's not just the average

1937
01:59:01,740 --> 01:59:06,720
the features are low level features describe the images

1938
01:59:09,430 --> 01:59:12,370
o four four

1939
01:59:12,600 --> 01:59:17,560
getting in the high dimensional feature space in this

1940
01:59:17,560 --> 01:59:23,760
but they are obviously not very strong indicators of the fact like it's not so

1941
01:59:23,760 --> 01:59:26,930
in the first year the features and by the

1942
01:59:26,950 --> 01:59:34,100
again we want to highest-ranked pictures and other members of a lot because i'm great

1943
01:59:34,100 --> 01:59:37,660
pictures system to be in the negative class

1944
01:59:37,870 --> 01:59:41,890
and put get something that only

1945
01:59:41,910 --> 01:59:43,970
now these two

1946
01:59:43,990 --> 01:59:45,810
life change order

1947
01:59:45,810 --> 01:59:49,290
so this is what level of

1948
01:59:49,330 --> 01:59:50,640
the political

1949
01:59:50,660 --> 01:59:55,620
is about the size and invested

1950
01:59:55,620 --> 01:59:59,510
but in our article which could still

1951
01:59:59,540 --> 02:00:01,080
exploit this

1952
02:00:01,120 --> 02:00:03,540
the fact was

1953
02:00:04,160 --> 02:00:06,370
is also a plus

1954
02:00:06,370 --> 02:00:09,970
as well as the fact that they i don't want to get the message here

1955
02:00:10,040 --> 02:00:19,290
what but it's actually shows that this satisfies the fact by the fact that

1956
02:00:19,290 --> 02:00:20,350
this is true

1957
02:00:26,100 --> 02:00:28,810
he said

1958
02:00:29,060 --> 02:00:32,290
because i made a mistake

1959
02:00:32,290 --> 02:00:37,260
i mean what i believe that this is a list of

1960
02:00:37,370 --> 02:00:40,740
a lot of one

1961
02:00:42,510 --> 02:00:46,260
so that they can

1962
02:00:47,950 --> 02:00:51,140
the only thing later

1963
02:01:09,180 --> 02:01:13,740
this is part of

1964
02:01:15,580 --> 02:01:16,910
so that of

1965
02:01:16,930 --> 02:01:19,200
this the

1966
02:01:19,310 --> 02:01:25,260
he did the setting

1967
02:01:25,260 --> 02:01:29,120
this was the start of the

1968
02:01:29,140 --> 02:01:31,330
and this

1969
02:01:31,350 --> 02:01:34,260
you have to do

1970
02:01:34,330 --> 02:01:36,870
some of the the law

1971
02:01:36,870 --> 02:01:40,910
sorry for the

