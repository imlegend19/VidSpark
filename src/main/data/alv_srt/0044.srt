1
00:00:00,000 --> 00:00:04,150
he insisted that the rule of very nice but they have

2
00:00:04,310 --> 00:00:10,210
the information and knowledge is the rule effective predictive i needed to separate to extract

3
00:00:10,210 --> 00:00:13,810
only a portion of all of that time is tool for having training using the

4
00:00:13,810 --> 00:00:19,630
remaining for testing just to have some information and accuracy and we ought to have

5
00:00:19,630 --> 00:00:25,690
in the future in the future somewhat more marked more different market just perform

6
00:00:25,710 --> 00:00:31,330
experimental different morphology and to complete rejected

7
00:00:31,650 --> 00:00:37,420
goodness me to do in this case what is important is that the

8
00:00:37,420 --> 00:00:42,940
the idea is working well on articles working not one and the idea is that

9
00:00:42,940 --> 00:00:46,080
the query language that is independent the specific i'm going to

10
00:00:46,650 --> 00:00:49,060
using we can

11
00:00:49,080 --> 00:00:56,230
OK take a we try and use the classification system and about the stacks of

12
00:00:56,230 --> 00:00:58,400
the language is expected

13
00:00:58,420 --> 00:01:03,900
the same

14
00:01:14,770 --> 00:01:22,320
the assumption is that we work on a two d objects because i suppose that's

15
00:01:22,320 --> 00:01:26,730
when you have three d points so we have to define some discrete also for

16
00:01:26,890 --> 00:01:27,670
percent that

17
00:01:28,500 --> 00:01:35,580
OK it's possible to simply and because in the special databases we the support to

18
00:01:35,710 --> 00:01:43,130
represent the geometry at the level and to the dimension so now we use the

19
00:01:43,150 --> 00:01:46,690
two d ed dimension but we can use the three d four d and so

20
00:01:46,690 --> 00:01:54,060
on and then we have just change defined the some descriptive that are interesting for

21
00:01:54,080 --> 00:02:00,920
a specific task because we can use the distance between points and one thousand times

22
00:02:01,100 --> 00:02:04,770
are some other descriptive interesting for some

23
00:02:09,440 --> 00:02:14,940
proximity but it's very close to distance concept because if you say the density density

24
00:02:14,960 --> 00:02:19,020
so OK i'm supposing i'm

25
00:02:19,060 --> 00:02:25,440
describing the term and the reason with drastically two points and the density of responsibility

26
00:02:25,440 --> 00:02:28,190
in these regions i could not

27
00:02:39,270 --> 00:02:43,250
yes OK but in this case we're are thinking the two classes

28
00:02:43,270 --> 00:02:48,770
it's possible to extend in this actually didn't work with these because it works only

29
00:02:48,810 --> 00:02:50,650
with the classification decision

30
00:02:50,790 --> 00:02:54,040
and the typically works with the two d

31
00:02:54,060 --> 00:02:56,020
in two dimensional data

32
00:02:56,020 --> 00:03:01,520
but the framework is done in a way that is simple to be expanded to

33
00:03:01,580 --> 00:03:03,960
all these opportunities

34
00:03:04,060 --> 00:03:07,940
this is my on

35
00:03:13,750 --> 00:03:15,230
on five

36
00:03:43,770 --> 00:03:48,790
what kind of

37
00:03:53,480 --> 00:04:00,130
does the task that briefly percent of the map interpretation by to you have presented

38
00:04:00,130 --> 00:04:02,310
themselves with different landscape

39
00:04:02,650 --> 00:04:12,020
and expert have define the four different landscape his system of find are catastrophic and

40
00:04:12,600 --> 00:04:14,150
system of cliques

41
00:04:14,230 --> 00:04:22,600
so it is used just to characterize these different regions or to extract the classification

42
00:04:22,620 --> 00:04:28,130
to discriminate these different concepts and because of this is discovery of selected on the

43
00:04:28,130 --> 00:04:33,980
side of that belong to the from the landscape and use association rules discovery frequent

44
00:04:34,060 --> 00:04:41,170
that characterize the landscape so some are core core and solve all of the properties

45
00:04:41,170 --> 00:04:47,460
of relations that characterize specific morphology for example as i said before we discriminate from

46
00:04:47,460 --> 00:04:52,900
incendiary that call core the landscape in the ninety percent of

47
00:04:52,940 --> 00:04:55,250
class and the

48
00:04:55,270 --> 00:04:59,600
OK and other applications that we've done is not present you just because it's not

49
00:04:59,620 --> 00:05:06,620
implement integrated but we have dismantled the for clustering score so that works on structured

50
00:05:06,620 --> 00:05:07,870
data from

51
00:05:07,890 --> 00:05:14,250
the thomas represented the graph so you have an neighbouring the relation and you can

52
00:05:14,270 --> 00:05:20,900
cluster to be only objects that are connected in that directly connected in the graph

53
00:05:21,560 --> 00:05:23,730
in the case of the genes

54
00:05:23,750 --> 00:05:28,330
so the idea is to combine clustering of with graph partitioning just to have a

55
00:05:28,330 --> 00:05:34,460
group of object genes that correspond to the graph of the structure and in that

56
00:05:34,460 --> 00:05:36,230
case we have used the the

57
00:05:36,250 --> 00:05:37,980
four class

58
00:05:38,020 --> 00:05:45,870
recognise some class of we can use the same data in this application and we

59
00:05:45,870 --> 00:05:53,170
discover what is it to become original form of cultivation of the olive tree and

60
00:05:53,170 --> 00:06:01,580
and monterrey and on measure for the one and with these data then with these

61
00:06:01,580 --> 00:06:08,500
are two are able to extract the jonson that are characterized by only three and

62
00:06:08,500 --> 00:06:14,000
the most forward see that there is characterising the physical situation so the presence of

63
00:06:14,020 --> 00:06:17,730
font of the close of

64
00:06:17,730 --> 00:06:20,150
two to pass to science

65
00:06:20,270 --> 00:06:25,330
the colliery that opened we walked in song and so we have using these other

66
00:06:25,330 --> 00:06:28,290
applications but to no no

67
00:06:28,310 --> 00:06:29,730
the other application

68
00:06:35,170 --> 00:06:36,620
because of the classification

69
00:06:36,620 --> 00:06:39,460
three of

70
00:06:39,480 --> 00:06:44,400
he also very so much

71
00:06:44,400 --> 00:06:50,990
is consistent with being due to a cosmoillogical constant in other words is W equal

72
00:06:50,990 --> 00:06:58,680
to minus one and constant with time are we just measuring just measuring Einstein's cosmological

73
00:06:58,680 --> 00:07:06,520
constant if the acceleration is not due to a cosmological constant then we can probe

74
00:07:06,520 --> 00:07:11,920
the underlying dynamics by  measuring as well as possible the time evolution of the dark energy

75
00:07:11,920 --> 00:07:19,180
in other words determine W of Z now aware of the possibility that the

76
00:07:19,180 --> 00:07:23,500
answer may be on the left hand side of the Einstein equations we have to

77
00:07:23,500 --> 00:07:29,520
search for a possible failure of general relativity through the comparison of the effect of

78
00:07:29,520 --> 00:07:34,860
dark energy on cosmic expansion with the effect of dark energy on the growth of

79
00:07:34,860 --> 00:07:41,800
cosmological structures like galaxies and galaxy clusters this is hard to quantify but I'll talk

80
00:07:41,800 --> 00:07:49,720
a little bit about it so although our only handle at present on dark energy

81
00:07:49,720 --> 00:07:54,760
is how it affects the cosmological the expansion rate of the universe there is

82
00:07:54,760 --> 00:08:02,220
a rich program that one can imagine for observational astronomy so if you want to

83
00:08:02,220 --> 00:08:08,720
know how dark energy affects the expansion rate as a function of redshift then you

84
00:08:08,720 --> 00:08:14,960
have observations of cosmological parameters that's sensitive to the expansion rate as a function of

85
00:08:14,960 --> 00:08:21,180
redshift and these parameters include the luminosity distance the angular diameter distance and how the

86
00:08:21,180 --> 00:08:27,380
volume of the universe changes with redshift and in turn the luminosity distance as a

87
00:08:27,380 --> 00:08:34,520
function of redshift can be determined by looking at supernovae or clusters of galaxies the

88
00:08:34,520 --> 00:08:40,640
angular diameter distance as a function of redshift can be measured by baryon acoustic

89
00:08:40,640 --> 00:08:46,080
oscillations I'll talk about this  strong lensing I won't talk about because I think

90
00:08:46,080 --> 00:08:51,840
at present it's a not as promising as weak lensing which I will talk about and

91
00:08:51,840 --> 00:08:56,660
the volume expansion as a function of redshift can be determined by clusters and strong

92
00:08:56,660 --> 00:09:03,060
lensing the growth of structure in the universe is an important measuremeant to

93
00:09:03,060 --> 00:09:11,820
make in linear perturbation theory there is an equation for how density perturbations grow

94
00:09:11,820 --> 00:09:17,860
with time this is the Fourier component K of Delta rho over rho and

95
00:09:17,860 --> 00:09:24,020
this is the equation in the Friedmann model and you see the expansion rate enters

96
00:09:24,020 --> 00:09:29,300
so if we can determine the growth rate of structure in the universe it gives

97
00:09:29,300 --> 00:09:35,420
us information on the expansion rate of the universe also this is an important

98
00:09:35,420 --> 00:09:41,780
measurement because if  somehow gravity is modified then there might be a source for the

99
00:09:41,780 --> 00:09:47,300
growth of structure from this new theory of gravity that's not present in Einstein's theory of

100
00:09:47,300 --> 00:09:53,460
gravity and the growth of structure in the universe can be determined by looking at

101
00:09:53,460 --> 00:10:00,660
clusters of galaxies weak lensing looking at the power spectrum how that evolves with redshift

102
00:10:00,660 --> 00:10:07,120
finally again this is harder to quantify testing gravity and looking in the solar

103
00:10:07,120 --> 00:10:14,400
system millimeter scale everyone knows next year you'll be producing Kaluza-Klein gravitons here

104
00:10:14,400 --> 00:10:19,940
at CERN  you know that would tell us something about gravity so maybe at accelerators

105
00:10:19,940 --> 00:10:25,340
or again looking at the power spectrum so there are many types of observations you can

106
00:10:25,340 --> 00:10:31,180
make to determine as well as possible the expansion rate of the universe as a

107
00:10:31,180 --> 00:10:37,290
function of redshift and to see whether it's consistent with a cosmological constant or there's some dynamics

108
00:10:37,290 --> 00:10:42,300
in probe as well as possible the dynamics and also look to see if

109
00:10:42,300 --> 00:10:48,980
somehow gravity is our theory of gravity is incomplete so let me talk about a

110
00:10:48,980 --> 00:10:56,800
couple of these I'd like to talk about supernovae baryon ac acoustic oscillations clusters and

111
00:10:56,800 --> 00:11:05,540
lensing so look using type one A supernovae it's standard candles is a well developed

112
00:11:05,540 --> 00:11:13,240
and practiced technique one measures the redshift of the supernovae or the host galaxy of the

113
00:11:13,240 --> 00:11:18,580
supernovae and the intensity of the  light as a function of time the light

114
00:11:18,580 --> 00:11:24,640
curve  because you ha to make it a standard candle you have to adjust

115
00:11:24,640 --> 00:11:32,060
for the width of the light curve you also have to take a spectrum of a

116
00:11:32,060 --> 00:11:37,300
galaxy to see whether it is a type one A supernovae or a type two B

117
00:11:37,300 --> 00:11:44,040
type two seven you know these different types of supernovae there are

118
00:11:44,040 --> 00:11:52,580
as I mentioned systematics possibility of dust evolution intrinsic luminosity dispersion magnetic fields et cetera the

119
00:11:52,580 --> 00:11:57,260
explosion may not be spherical if you're ever in an astrophysics talk and you wanna ask a

120
00:11:57,260 --> 00:12:03,740
question say what about magnetic fields what about  nonspherical possibilities and what about

121
00:12:03,740 --> 00:12:11,740
dust  what about evolution those are good questions no matter what the seminar is about

122
00:12:11,740 --> 00:12:17,580
the people who do supernovae  make a pretty convincing argument that these systematic uncertainties are

123
00:12:17,790 --> 00:12:22,500
under control and again as you get more and more and more data your system

124
00:12:22,500 --> 00:12:28,610
you get better handles on your systematic uncertainty there is a lot of  information

125
00:12:28,630 --> 00:12:35,680
per supernovae I've lost count of how many high  redshift supernovae we've observed dozens maybe

126
00:12:35,680 --> 00:12:38,800
close to a hundred each one of them gives us a little piece of

127
00:12:38,800 --> 00:12:47,560
information the present procedure is to discover a supernovae by a wide area survey of

128
00:12:47,560 --> 00:12:52,380
the sky looking for changes this is usually done today the state of the art

129
00:12:52,380 --> 00:12:59,380
is with four meter class telescopes then the hard part of this is to arrange time

130
00:12:59,380 --> 00:13:07,320
on eight meter class telescopes to do the follow-up spectroscopy this is very time-consuming to

131
00:13:07,320 --> 00:13:11,520
discover the supernovae you can look at a wide area of the sky and see things

132
00:13:11,520 --> 00:13:18,740
exploding but to do the spectroscopy you focus in one at a time so one

133
00:13:18,740 --> 00:13:25,230
of the questions that people  are asking is do yous really need the spectroscopy

134
00:13:25,230 --> 00:13:32,240
to determine the redshift of the supernovae well enough in a couple of months

135
00:13:32,240 --> 00:13:40,760
they'll be casting the mirror for a telescope known as LSST the large synoptic survey telescope

136
00:13:40,760 --> 00:13:46,300
which will be an eight meter class telescope with a very large field of view

137
00:13:46,320 --> 00:13:51,520
that will basically take am a motion  picture a movie of the sky every night

138
00:13:51,620 --> 00:13:58,540
this telescope will discover in ten years  millions of supernovae however they won't

139
00:13:58,540 --> 00:14:02,730
be able to do the spectrum they won't be able to tell whether it's type

140
00:14:02,730 --> 00:14:10,960
one A or not unless they use photometric redshifts the traditional way to determine the redshift

141
00:14:11,080 --> 00:14:17,300
of an object is to take a spectrum and this is a sort of just a simple graph an

142
00:14:17,310 --> 00:14:23,660
illustration from I don't know where I stole it from probably a textbook that as the

143
00:14:23,770 --> 00:14:29,660
object is farther and farther away the spectrum is shifted to the red and you see where

144
00:14:29,720 --> 00:14:37,880
particular ignition or  absorption lines are in the spectra and then you determine the redshift however today

145
00:14:37,890 --> 00:14:44,760
there are images of the sky taking with taken with multicolor photometry  so there are images

146
00:14:44,760 --> 00:14:50,170
the probability that the next point will fall in this particular being located at x

147
00:14:50,530 --> 00:14:54,450
that's really an enormous this as a function of x

148
00:14:55,480 --> 00:15:00,530
this is the probability that the next point will fall in a bin size at plus minus

149
00:15:02,490 --> 00:15:06,190
a bin of with x located at x approximately right

150
00:15:07,250 --> 00:15:10,750
so if i multiply them by the total number of data points we got

151
00:15:11,400 --> 00:15:12,400
which is an

152
00:15:15,850 --> 00:15:17,610
that's really expected

153
00:15:20,870 --> 00:15:21,240
and in

154
00:15:23,190 --> 00:15:24,490
and that's is equal to

155
00:15:26,080 --> 00:15:26,730
on z

156
00:15:27,470 --> 00:15:28,810
it's a mindset lambda

157
00:15:29,860 --> 00:15:31,490
we want some way of turning this

158
00:15:32,110 --> 00:15:37,590
expected number which will relate to the actual measured number into some sort of straight line away

159
00:15:38,150 --> 00:15:40,390
the thing that we are measuring his x that's

160
00:15:41,080 --> 00:15:41,790
variable that

161
00:15:42,220 --> 00:15:43,770
changes us moving around

162
00:15:44,400 --> 00:15:45,810
and there's a divided by lambda

163
00:15:46,430 --> 00:15:49,860
and it's got an exponential that's horrible but with this we know how to get

164
00:15:49,900 --> 00:15:51,340
an exponential to take the log

165
00:16:00,900 --> 00:16:02,330
so we can have the log

166
00:16:03,330 --> 00:16:04,280
the expected

167
00:16:12,240 --> 00:16:13,530
with with delta x

168
00:16:14,740 --> 00:16:16,670
log expected count is

169
00:16:18,150 --> 00:16:21,160
all hand on z which is some sort of constant

170
00:16:30,630 --> 00:16:32,250
that's now a straight line

171
00:16:32,760 --> 00:16:34,160
the incentive something rather

172
00:16:34,550 --> 00:16:35,340
and the slope of

173
00:16:36,120 --> 00:16:37,050
minus one lambda

174
00:16:38,610 --> 00:16:38,940
all right

175
00:16:41,220 --> 00:16:43,730
if you've been subjected to the physics

176
00:16:44,330 --> 00:16:45,320
training that says

177
00:16:45,730 --> 00:16:49,190
and this is a high school thing to do is and take the data monday

178
00:16:49,230 --> 00:16:52,390
around get a straight line because then everyone knows how to fit straight lines there

179
00:16:55,850 --> 00:16:57,670
i do number one was

180
00:17:00,580 --> 00:17:02,800
put the data into bins and get a histogram

181
00:17:03,190 --> 00:17:06,540
it doesn't solve the problem we now need to do something with the histogram things

182
00:17:06,540 --> 00:17:11,010
we could do with a histogram notice i've done twice the fact beans one sided

183
00:17:11,010 --> 00:17:15,350
with indians the second time to just emphasize the answer we end up with may

184
00:17:15,350 --> 00:17:16,610
depend on the size of events

185
00:17:20,330 --> 00:17:21,150
is e

186
00:17:21,700 --> 00:17:25,240
physics education i take the logo histogram then fit a straight line

187
00:17:26,080 --> 00:17:27,800
okay so the suggestion is

188
00:17:28,280 --> 00:17:29,490
take the actual data

189
00:17:30,560 --> 00:17:36,080
the thing that is true is that the log the expected number is some constant minus x lambda

190
00:17:36,790 --> 00:17:38,940
what we're i'm going to take the actual data

191
00:17:39,730 --> 00:17:40,230
and be

192
00:17:41,320 --> 00:17:42,410
we take the logo at

193
00:17:44,480 --> 00:17:44,860
and plot

194
00:17:45,460 --> 00:17:46,100
against x

195
00:17:47,060 --> 00:17:48,110
what what what what

196
00:17:50,100 --> 00:17:51,880
and with them going fit a straight line

197
00:17:52,460 --> 00:17:56,810
and when as a lot of extra questions getting raised by this approach when we say

198
00:17:58,150 --> 00:18:01,440
straight line how do we fit a straight line to just use the straight line

199
00:18:01,440 --> 00:18:05,140
fitting thing that comes on the computer that's probably what most people would leap to

200
00:18:06,090 --> 00:18:08,530
but something that could very easily be the case

201
00:18:08,990 --> 00:18:10,520
is when you define your bins

202
00:18:10,940 --> 00:18:14,030
if you define them before you actually get the data which is sort of correct

203
00:18:14,030 --> 00:18:15,830
related to classical statistics he shouldn't

204
00:18:16,230 --> 00:18:18,650
change or estimator after looking at the data

205
00:18:19,800 --> 00:18:24,020
it could well be the case that one might have zero count and it

206
00:18:25,010 --> 00:18:26,600
now when you take the log zero

207
00:18:27,590 --> 00:18:28,810
you can have not been

208
00:18:30,170 --> 00:18:32,080
log zero you have infinitely

209
00:18:32,720 --> 00:18:33,650
negative value

210
00:18:34,060 --> 00:18:35,880
to put into your straight line fitting methods

211
00:18:39,410 --> 00:18:43,230
it all sounds pretty good and it looks fairly good if you get lucky

212
00:18:44,370 --> 00:18:45,230
up on the screen here

213
00:18:45,860 --> 00:18:50,000
with nice fat beans and lots of data you can see a straight line is definitely going

214
00:18:50,520 --> 00:18:52,080
it's going to give you the right sort of answer

215
00:18:52,670 --> 00:18:55,280
but i have a universal methods for answering problems

216
00:18:57,900 --> 00:18:58,900
so just used to be

217
00:19:00,730 --> 00:19:01,860
so here's another suggestion

218
00:19:02,560 --> 00:19:03,580
take the data

219
00:19:04,410 --> 00:19:06,010
put it in only two bins

220
00:19:07,440 --> 00:19:13,530
andover gonna fix the beans beforehand or will we sort adaptively move the boundary between the bins

221
00:19:14,360 --> 00:19:17,360
what do you want to do is make sure we get some points in each bin you

222
00:19:17,870 --> 00:19:19,810
and then do something that's a nice idea

223
00:19:20,260 --> 00:19:22,840
so i did this is this slide in number three

224
00:19:23,620 --> 00:19:25,170
if it straight line

225
00:19:28,350 --> 00:19:32,970
i don't know before is just used to be be smart about where you put the been boundary

226
00:19:33,770 --> 00:19:35,230
case if you've got three points

227
00:19:36,740 --> 00:19:38,840
what are two beams can be we could put

228
00:19:40,080 --> 00:19:43,440
well here or here or here or here there is actually quite a lot of

229
00:19:43,440 --> 00:19:48,530
latitude wavelet been boundary and really they have to in one then one another one

230
00:19:48,540 --> 00:19:50,060
one and two in the other so

231
00:19:51,060 --> 00:19:53,020
it's a nice idea probably

232
00:19:53,620 --> 00:19:54,890
you there's probably something in it

233
00:19:55,430 --> 00:20:00,690
but it's still got pitfalls namely if you actually end up with only three data points you kinda struggle

234
00:20:01,650 --> 00:20:04,680
because there's a lot of bitterness now by the way put your been boundaries

235
00:20:08,650 --> 00:20:10,370
are there any other suggestions we've sort

236
00:20:10,850 --> 00:20:13,470
perhaps exhausted a whole bunch of different ways

237
00:20:14,190 --> 00:20:16,280
making been putting the data into bins

238
00:20:17,500 --> 00:20:19,490
and then doing something with those histograms

239
00:20:20,300 --> 00:20:21,000
another ideas

240
00:20:26,860 --> 00:20:27,580
okay so

241
00:20:27,980 --> 00:20:30,130
and other suggestion sufficient number five

242
00:20:31,500 --> 00:20:32,560
could we look at

243
00:20:33,240 --> 00:20:34,420
the cumulative distribution

244
00:20:38,440 --> 00:20:39,480
so uh

245
00:20:40,310 --> 00:20:41,220
residing five

246
00:20:42,200 --> 00:20:43,210
if i give you some data

247
00:20:45,550 --> 00:20:45,960
like that

248
00:20:46,280 --> 00:20:49,380
you can deduce from the cumulative distribution which is

249
00:20:50,870 --> 00:20:51,320
one the

250
00:20:57,660 --> 00:20:59,020
so that's the cumulative

251
00:20:59,850 --> 00:21:01,550
distribution function from the data

252
00:21:02,000 --> 00:21:04,130
and for any particular hypothesis

253
00:21:04,570 --> 00:21:07,870
it says what lambda is which defines the probability density

254
00:21:08,860 --> 00:21:09,650
there is also a senior

255
00:21:12,560 --> 00:21:13,570
four about five lambda

256
00:21:14,090 --> 00:21:14,580
which will be

257
00:21:16,140 --> 00:21:16,700
something like that

258
00:21:18,780 --> 00:21:23,460
and now we've got a thing has little steps in it where the data points have become alkali smooth

259
00:21:24,270 --> 00:21:26,240
and we could repeat now

260
00:21:27,150 --> 00:21:29,720
the conversation we had a moment ago where we say all let's have a measure

261
00:21:29,720 --> 00:21:31,340
of how close this is this

262
00:21:31,340 --> 00:21:33,380
numbers with uncertainty

263
00:21:33,400 --> 00:21:37,360
then and you do so in sort of logic logically consistent way you discover that

264
00:21:37,380 --> 00:21:41,070
the rules which the those numbers perhaps satisfy

265
00:21:41,090 --> 00:21:43,280
i just the sum and product rule of probability

266
00:21:43,300 --> 00:21:46,220
so you might as well call the probabilities you can make them add up to

267
00:21:46,280 --> 00:21:49,320
ten instead of adding up to one or something but that would be a trivial

268
00:21:49,320 --> 00:21:51,880
differences convention so

269
00:21:51,940 --> 00:21:55,440
so the first thing to say is that if you're going to use continuous continuous

270
00:21:55,440 --> 00:22:01,170
numbers represent uncertainty using to continue in in the consistent most consistent way then you

271
00:22:01,170 --> 00:22:03,420
lead automatically to probability theory

272
00:22:03,470 --> 00:22:07,400
which is think is really we using problem the traditional view of probabilities is frequencies

273
00:22:07,400 --> 00:22:13,110
of a random recurrent events frequencies of a coin toss but in a bayesian framework

274
00:22:13,110 --> 00:22:17,170
we using probabilities much more general way to quantify uncertainty even for events which are

275
00:22:17,170 --> 00:22:21,450
not repeatable you what's the probability that the the moon once orbited the sun instead

276
00:22:21,450 --> 00:22:24,800
of the full something you can't do that experiment a million times making the fraction

277
00:22:24,800 --> 00:22:29,150
of means that all that is perfectly OK to talk about probabilities

278
00:22:29,200 --> 00:22:33,410
i think the question actually asked was but slightly different which is i've got a

279
00:22:33,410 --> 00:22:39,300
machine that produces these continuous course how do i turn them into probabilities one

280
00:22:39,320 --> 00:22:45,950
one possible answer john platt some years ago took support vector machines which was trained

281
00:22:45,950 --> 00:22:51,900
which produces a continuous output before the threshold and he treated that is the input

282
00:22:51,900 --> 00:22:57,970
to a two parameter logistic sigmoid so i think we had yesterday so

283
00:22:57,990 --> 00:23:00,150
you've got x you're

284
00:23:00,240 --> 00:23:04,400
have put your support vector machine which was not designed to produce probabilities and he

285
00:23:04,400 --> 00:23:08,740
took get two parameter models and then put that through a logistic sigmoid that's the

286
00:23:08,740 --> 00:23:10,820
function which

287
00:23:10,840 --> 00:23:22,090
so that that maps the real axis onto zero why

288
00:23:24,280 --> 00:23:28,880
took a data set of values of x and labels and worked out the values

289
00:23:28,880 --> 00:23:33,800
of a and b by minimizing a sort of cross entropy measure the label

290
00:23:33,800 --> 00:23:38,360
so effectively what he's doing is retrofitting a logistic sigmoid to this

291
00:23:38,380 --> 00:23:40,760
school function which is something that

292
00:23:40,780 --> 00:23:43,920
you know bigger if you're more likely to be in one class small if you

293
00:23:43,920 --> 00:23:46,300
want to be the class but

294
00:23:46,360 --> 00:23:50,260
but it sort of probability so that that's one way of doing it

295
00:23:50,280 --> 00:23:54,940
but people than other things since then i

296
00:23:54,980 --> 00:23:56,700
i guess this

297
00:23:57,680 --> 00:24:01,050
OK so

298
00:24:01,070 --> 00:24:04,240
yes any other questions questions

299
00:24:10,170 --> 00:24:13,030
he was

300
00:24:15,550 --> 00:24:19,130
OK so the question so what if one of these factors is much more important

301
00:24:19,130 --> 00:24:22,280
than the other so the question is what do you mean by more important so

302
00:24:22,280 --> 00:24:25,490
i guess what you mean by more important is let's say the image data is

303
00:24:25,490 --> 00:24:29,340
very very reliable and really you know most of the time we get to run

304
00:24:29,360 --> 00:24:32,110
the blood data is of pretty useless it's sort of it it sort of sums

305
00:24:32,130 --> 00:24:35,300
is a little bit correlates cancer is and i think that's what you mean by

306
00:24:35,300 --> 00:24:40,550
more important right in that case the in the model should do the right thing

307
00:24:40,550 --> 00:24:45,400
because the because the blood data is very uncertain it she gives posterior probabilities which

308
00:24:45,400 --> 00:24:49,130
is sort of you know sometimes the point six sometimes the point four is always

309
00:24:49,130 --> 00:24:52,610
quite uncertain whereas the image data is really coming out you know point nine nine

310
00:24:52,610 --> 00:24:54,970
nine point and and

311
00:24:55,030 --> 00:24:58,510
the sum product rule probability would do the right thing for you in that case

312
00:24:58,510 --> 00:25:00,470
under that assumption of independence

313
00:25:00,530 --> 00:25:04,320
assume that independence used in the middle once you assume that the graph

314
00:25:04,380 --> 00:25:08,570
and everything else is just the sum and product of the probabilities doing the right

315
00:25:09,400 --> 00:25:11,950
a small questions over

316
00:25:11,970 --> 00:25:13,530
here i think whether

317
00:25:18,860 --> 00:25:22,200
what going to do now is is change topic slightly and talk a little bit

318
00:25:22,200 --> 00:25:27,010
about some standard distributions i'm sure all very familiar to you but look at them

319
00:25:27,010 --> 00:25:31,860
in terms of things like exponential families conjugate priors and so on and these are

320
00:25:31,860 --> 00:25:36,090
important topics that are gonna play an important role throughout the summer school and if

321
00:25:36,090 --> 00:25:39,630
you're not familiar with the the it's useful to get familiar early on because i'm

322
00:25:39,630 --> 00:25:42,300
sure you'll encounter than lots of other lectures

323
00:25:42,340 --> 00:25:44,780
so let's just start with a very simple

324
00:25:45,780 --> 00:25:49,560
four according to have a coin which reflected its heads we get a one if

325
00:25:49,560 --> 00:25:52,800
its tails zero and

326
00:25:52,800 --> 00:25:55,670
we can assign a probability that the

327
00:25:55,670 --> 00:26:00,630
coin lands heads the probability of x equals one is given by parameter mu so

328
00:26:00,650 --> 00:26:03,840
probably vesicles there is just one minus mu

329
00:26:03,840 --> 00:26:10,320
x y and z the and and some of the valuable state of the labels

330
00:26:10,320 --> 00:26:20,020
million you and the men that means that f becomes a function of human genes

331
00:26:20,300 --> 00:26:23,440
and then we can ask ourselves how sensitive is

332
00:26:24,900 --> 00:26:33,980
to the value of EUR and whether we can so that's sort of the chain

333
00:26:33,980 --> 00:26:39,460
of all is something like this and let me explain to you again where this

334
00:26:39,460 --> 00:26:48,540
comes from so

335
00:26:50,940 --> 00:26:56,320
what this quantity means is if we change you the constant what happens to the

336
00:26:56,320 --> 00:27:00,820
value of well why would the value of f change in the 1st place when

337
00:27:00,820 --> 00:27:04,050
F is just a function of x y z and the ability of you whether

338
00:27:04,070 --> 00:27:09,060
it changes because x y and z depend on your success we have to figure

339
00:27:09,060 --> 00:27:12,080
out how quickly x y and z change from the change

340
00:27:12,940 --> 00:27:18,140
well how quickly to do that is precisely from experts you of white offered you

341
00:27:18,140 --> 00:27:22,340
thought the these are the rate of change of x y z when we change

342
00:27:22,880 --> 00:27:23,960
and now

343
00:27:23,970 --> 00:27:25,860
when we change x y and z

344
00:27:25,900 --> 00:27:31,620
that causes f to change how much has exchange well social and next tells us

345
00:27:31,620 --> 00:27:35,730
how quickly as changes in rate of change of X so

346
00:27:37,010 --> 00:27:42,780
that's the change in goes just by the fact that the exchanges when you change

347
00:27:43,640 --> 00:27:45,680
but when and why also changes

348
00:27:45,690 --> 00:27:50,840
why changes this state and that causes to change at that and z changes as

349
00:27:50,840 --> 00:27:55,100
well and that causes have to change advances and the effects of together

350
00:27:55,870 --> 00:27:57,100
does that make sense

351
00:28:06,410 --> 00:28:11,970
and so in particular we can use the chain will to do changes of valuables

352
00:28:11,970 --> 00:28:16,910
if we have a say a function defined in terms of poor coordinates on the

353
00:28:17,120 --> 00:28:21,210
and would like to switch it to rectangular coordinates x and y then we can

354
00:28:21,210 --> 00:28:23,870
use the chain of almost all the little buffer derivatives

355
00:28:28,380 --> 00:28:36,520
and finally the last but not least we've seen how to deal with non independent

356
00:28:36,520 --> 00:28:46,880
valuables so when all valuable x y z related by some equations

357
00:28:47,640 --> 00:28:51,480
so 1 way we can deal with this is to sorry for 1 of the

358
00:28:51,480 --> 00:28:56,740
variables and get go back to independent variables but we can't always do that of

359
00:28:56,740 --> 00:29:00,300
course on the exam you can be sure that I will make sure that you

360
00:29:00,300 --> 00:29:03,920
cannot solve for I would want to win because that would be too easy

361
00:29:04,730 --> 00:29:09,420
um so that when we have to keep all of them but handle you know

362
00:29:09,440 --> 00:29:13,480
take take into account relations with seems to

363
00:29:14,260 --> 00:29:22,080
useful methods 1 of them is to find the minimum of a maximum of a

364
00:29:22,080 --> 00:29:28,880
function when ribosomal them and that's the method of Lagrange multipliers

365
00:29:29,240 --> 00:29:33,500
but because the problem

366
00:29:36,480 --> 00:29:39,360
the the

367
00:29:40,840 --> 00:29:51,440
so we're going to find the minimum or maximum of function that subject to the

368
00:29:51,440 --> 00:29:58,200
constraint g equals

369
00:29:58,280 --> 00:30:06,620
constant what we find that equations that saying that the variance of that is actually

370
00:30:06,620 --> 00:30:08,930
proportional to the gradient of G

371
00:30:10,280 --> 00:30:15,480
so there's a new variable here lambda the multiplier

372
00:30:19,700 --> 00:30:23,500
you know for example if an injury was here I had functions of the variables

373
00:30:23,500 --> 00:30:30,200
so this becomes equations that subjects than that of X X and Y equals land

374
00:30:30,200 --> 00:30:36,180
that is why we have so this is equal and adjacent if and when we

375
00:30:36,180 --> 00:30:40,520
plug in the formulas for F and G well we're left with the equations involving

376
00:30:40,520 --> 00:30:47,100
before bibles x y z and under what song where we don't have actually fallen

377
00:30:47,100 --> 00:30:49,900
dependent variables we also have these relations

378
00:30:50,010 --> 00:30:58,300
genes equals with another constraint was waiting x y and z together

379
00:30:58,720 --> 00:31:02,740
so that we can find and development situations

380
00:31:02,800 --> 00:31:06,560
something easy and sometimes it's very hot or even impossible

381
00:31:07,130 --> 00:31:13,860
so but here on the test I haven't decided yet but it could well be

382
00:31:13,860 --> 00:31:16,960
that the problem of the ones with the players will just ask you to avoid

383
00:31:16,960 --> 00:31:26,690
the equations not the sort of case so thank you cannot missing anything but just

384
00:31:26,700 --> 00:31:33,720
you know before you start solving check whatever problem to to sort moment if it

385
00:31:33,720 --> 00:31:48,400
doesn't then I probably wouldn't come

386
00:31:50,840 --> 00:31:53,580
yeah yeah

387
00:31:53,620 --> 00:32:00,790
of the system

388
00:32:00,920 --> 00:32:22,100
yeah so and so and other topic that we so just yesterday is constrained partial

389
00:32:22,100 --> 00:32:26,880
and I guess I have to be explained a little bit becomes

390
00:32:26,880 --> 00:32:30,560
limitation it's also limitation of ideas and so forth

391
00:32:32,230 --> 00:32:36,190
so so this i think gives you some fluid of what the scope of the

392
00:32:36,190 --> 00:32:38,090
problem is it's been studied for

393
00:32:38,130 --> 00:32:42,460
quite some number of years in the human visual perception and in computer vision

394
00:32:42,480 --> 00:32:45,530
which dates back this forty years

395
00:32:45,630 --> 00:32:50,490
so i'll take you through some examples of how we think about this and some

396
00:32:50,490 --> 00:32:51,740
concrete results

397
00:32:51,830 --> 00:32:55,210
so here is one way of thinking about some image

398
00:32:55,270 --> 00:32:57,040
i don't know what it is

399
00:32:57,050 --> 00:32:58,390
but i have my

400
00:32:58,410 --> 00:33:00,370
i have a set of examples

401
00:33:00,380 --> 00:33:03,950
and the the label elephant would fly except exeter

402
00:33:04,000 --> 00:33:06,180
so what i do is i tried to do

403
00:33:06,850 --> 00:33:11,780
correspondence and matching between query image and the exemplars

404
00:33:11,790 --> 00:33:16,960
and a value to similarity score and let's see the similarity score turns out to

405
00:33:16,960 --> 00:33:17,890
be this

406
00:33:18,060 --> 00:33:22,420
highest of the difference to be the smallest between these two

407
00:33:22,440 --> 00:33:24,680
and then business that's my

408
00:33:24,690 --> 00:33:28,130
if you want to think of what to put it in machine learning framework

409
00:33:28,250 --> 00:33:32,940
this is the nearest neighbour classifier with the set in distance function

410
00:33:32,960 --> 00:33:37,950
and there is something to be said about nearest neighbour classifier asymptotically

411
00:33:37,970 --> 00:33:39,870
given enough examples

412
00:33:39,940 --> 00:33:45,230
i nearest neighbour classifier gets to within a factor of two of optimal bayesian classifier

413
00:33:46,170 --> 00:33:53,500
all the offences scheme that tried to remember the humble nearest neighbour classifier

414
00:33:53,520 --> 00:33:54,790
OK so

415
00:33:54,800 --> 00:33:59,050
if you notice i should you these views but you might ask the objects actually

416
00:33:59,050 --> 00:34:00,250
three dimensional

417
00:34:00,390 --> 00:34:03,430
and current thinking on this is

418
00:34:03,630 --> 00:34:07,730
mean by the problem we we know how to sort two d two to the

419
00:34:07,730 --> 00:34:13,130
matching problems so we deal with three d objects by distorting multiple two d views

420
00:34:13,180 --> 00:34:14,440
so you have

421
00:34:14,450 --> 00:34:18,070
so here is this is anything box and what we do is we want to

422
00:34:18,070 --> 00:34:22,090
store using a three d model but we can store multiple two d views

423
00:34:22,130 --> 00:34:24,270
and here are the multiple two d views

424
00:34:24,280 --> 00:34:26,240
and the matching machinery

425
00:34:26,290 --> 00:34:27,750
has to be robust

426
00:34:28,430 --> 00:34:33,140
working even though you have a small number of two d views

427
00:34:33,150 --> 00:34:36,310
and the answer that you can actually do that there is an experiment based on

428
00:34:36,310 --> 00:34:38,650
some work we have done a few years ago

429
00:34:38,690 --> 00:34:43,120
that we needed to sort this on the y axis is the eighteen x is

430
00:34:43,180 --> 00:34:45,610
the number of examples you stored

431
00:34:47,060 --> 00:34:52,350
so these are different similarity functions and this is obviously the fanciest one that we

432
00:34:52,920 --> 00:34:57,920
proposed and even with four prototypes but object we could get an error rate of

433
00:34:57,930 --> 00:34:59,390
two three percent

434
00:34:59,400 --> 00:35:00,520
so pretty smart

435
00:35:01,430 --> 00:35:05,710
this general idea if you will selective use works

436
00:35:05,720 --> 00:35:09,660
that you can see president three d objects multiple two d views

437
00:35:09,710 --> 00:35:14,550
and this makes life much easier because natural objects we don't necessarily have the models

438
00:35:15,390 --> 00:35:17,430
four industrial parks we do

439
00:35:17,450 --> 00:35:21,340
but industrial parts of a small fraction of what we are interested in the you

440
00:35:21,500 --> 00:35:24,420
just work with images life is better

441
00:35:24,460 --> 00:35:28,380
that's got infected

442
00:35:28,390 --> 00:35:32,410
OK so how do we solve the correspondence problem how do we match

443
00:35:32,430 --> 00:35:35,270
in different parts of an object to each other

444
00:35:35,270 --> 00:35:40,140
so i'm going to go through some of the ideas talk about deformable template matching

445
00:35:40,140 --> 00:35:44,070
and how we use machine learning for finding discriminative features

446
00:35:44,160 --> 00:35:45,090
OK so

447
00:35:45,090 --> 00:35:47,330
so the correspondence problem

448
00:35:47,360 --> 00:35:48,980
can be described this way

449
00:35:49,000 --> 00:35:50,710
so here had two

450
00:35:50,720 --> 00:35:52,960
two versions of the letter a

451
00:35:54,140 --> 00:35:59,900
what we are somehow able to do humans intuitively is to say that these two

452
00:35:59,920 --> 00:36:04,450
some of corresponding points and is the different point on the ship

453
00:36:04,460 --> 00:36:06,360
and these are not identical shapes

454
00:36:07,940 --> 00:36:11,410
but so we still have the notion of correspondence and this in other settings where

455
00:36:11,470 --> 00:36:15,090
mean that from one image to the people the noise can be matched to the

456
00:36:15,090 --> 00:36:18,130
people than those in the image and so on

457
00:36:19,130 --> 00:36:20,440
how do we do this

458
00:36:22,150 --> 00:36:26,030
this the secret is that we try to come up with some descriptive if i

459
00:36:26,030 --> 00:36:27,620
just look at that point

460
00:36:27,620 --> 00:36:28,900
the point is the same

461
00:36:30,040 --> 00:36:32,380
but if i tried to characterise

462
00:36:32,400 --> 00:36:34,750
some more extended neighbourhood

463
00:36:34,770 --> 00:36:35,680
i can

464
00:36:35,740 --> 00:36:38,670
distinguish between the different points in the ship

465
00:36:38,690 --> 00:36:42,420
so here's one idea at this point and newport

466
00:36:42,680 --> 00:36:46,830
i see is this kind of dark wood like that

467
00:36:46,870 --> 00:36:51,050
around that point and you count the number of

468
00:36:51,100 --> 00:36:57,130
of sort of these these image as points in each of these beings

469
00:36:57,140 --> 00:37:01,290
so we can't hear you get four point b points and you get

470
00:37:02,220 --> 00:37:03,660
and so on and so forth

471
00:37:03,670 --> 00:37:08,980
so that's what it is doing is around point it is characterising the distribution of

472
00:37:09,030 --> 00:37:10,130
the points

473
00:37:10,130 --> 00:37:14,350
and because this the shape context

474
00:37:14,400 --> 00:37:18,380
so here is the point and then so we've got

475
00:37:18,430 --> 00:37:23,210
these different bins so that they have been added in space so they have different

476
00:37:23,210 --> 00:37:26,210
angular orientation and the different distances

477
00:37:26,280 --> 00:37:30,310
this is the distance at the angular distance so you get you get so we

478
00:37:30,320 --> 00:37:32,870
had this element of sixteen bins then

479
00:37:32,950 --> 00:37:37,410
and now we've got columns here so that means more columns

480
00:37:37,430 --> 00:37:40,790
so this is what the pattern around this point looks like so most of the

481
00:37:40,790 --> 00:37:43,900
action is below and to the right which is captured here

482
00:37:43,910 --> 00:37:45,400
consider this point

483
00:37:45,410 --> 00:37:46,630
what happens here

484
00:37:46,640 --> 00:37:51,890
i know that the points distributed equally all around you so therefore the distribution with

485
00:37:51,890 --> 00:37:54,840
respect to theta is much more uniform

486
00:37:54,840 --> 00:37:56,580
so you get this distribution

487
00:37:56,630 --> 00:37:59,290
now what is the case with these guys

488
00:37:59,310 --> 00:38:01,190
unlike the original shape

489
00:38:01,300 --> 00:38:04,380
these are now fixed length vectors

490
00:38:04,390 --> 00:38:05,460
because we have

491
00:38:05,520 --> 00:38:07,890
we have sixty bins if i go back

492
00:38:08,300 --> 00:38:10,630
and a certain number of

493
00:38:10,630 --> 00:38:14,470
so if i count one two three four five and that these i get sixty

494
00:38:15,230 --> 00:38:17,380
so now i've got sixteen

495
00:38:17,380 --> 00:38:23,160
i preferred is because it is guaranteed to find the smallest number of sparse signals

496
00:38:23,170 --> 00:38:29,080
but NOL one is this is pretty good in

497
00:38:29,120 --> 00:38:33,980
there are lots of algorithms on this one you know the idea of

498
00:38:34,090 --> 00:38:39,100
this compressive sensing and so on maybe five ten years old because you see publications

499
00:38:39,160 --> 00:38:45,190
really critical ones appearing but their underlying algorithms that contribute to recovery have been there

500
00:38:45,190 --> 00:38:49,890
for a long time for example the matching pursuit algorithm is something that can be

501
00:38:49,890 --> 00:38:56,780
tried for this has been around since the early nineties malakand zhang and variation of

502
00:38:56,780 --> 00:39:01,310
the the matching pursuit algorithm came in the context of the wavelet

503
00:39:01,330 --> 00:39:10,090
decomposition ideas and if you introduce intermediate step but orthogonalise in the matching pursuit algorithm

504
00:39:10,430 --> 00:39:12,530
then you have the things again

505
00:39:12,570 --> 00:39:17,850
from the early nineties but the thing that i'm going to talk about today is

506
00:39:17,850 --> 00:39:20,020
very briefly is is the

507
00:39:20,040 --> 00:39:26,160
the basis pursuit algorithm one which returned ashore in the previous slide and if you

508
00:39:26,260 --> 00:39:32,430
assume a nice free signals then you know you just saw this as a linear

509
00:39:32,430 --> 00:39:38,150
programming problem you minimize the nom l one norm of x that you're trying to

510
00:39:38,150 --> 00:39:43,810
recover is subject to the condition that you have compressive measurements y which is a

511
00:39:43,810 --> 00:39:48,860
fee transforming x but in general we we have all kinds of errors that contaminate

512
00:39:48,860 --> 00:39:51,000
the compressive measurements because there

513
00:39:51,140 --> 00:39:54,650
collected by a real sense are so you can assume

514
00:39:54,720 --> 00:40:00,140
no the explorers and and then you really do as as what is known as

515
00:40:00,140 --> 00:40:02,970
the basis pursuit denoising algorithm

516
00:40:02,990 --> 00:40:07,630
where you have to somehow figure out what the epsilon is to bound the noise

517
00:40:08,140 --> 00:40:13,510
could be really and so forth so you you again minimise the the l one

518
00:40:13,510 --> 00:40:15,410
norm of the signal you are trying to

519
00:40:16,110 --> 00:40:21,160
given the condition that they had to arm area is less than some so there

520
00:40:21,710 --> 00:40:24,840
there's so much return on this and i will

521
00:40:24,900 --> 00:40:26,370
you know i encourage you to

522
00:40:26,380 --> 00:40:32,270
read several papers by professor candies by professor brian wagner and the papers are just

523
00:40:32,270 --> 00:40:35,540
coming like crazy right this is so well

524
00:40:35,850 --> 00:40:38,580
to kind of read all of them to get a good view of what the

525
00:40:38,580 --> 00:40:41,010
field is about

526
00:40:41,020 --> 00:40:44,330
that's a computer vision researcher machine vision researcher

527
00:40:44,350 --> 00:40:47,630
OK so what does it mean what this compressive sensing

528
00:40:47,680 --> 00:40:49,410
going to do for us

529
00:40:49,420 --> 00:40:51,880
because we are always looking for

530
00:40:51,890 --> 00:40:56,570
good ideas from other fields computer vision and you know we

531
00:40:57,130 --> 00:40:58,250
we like

532
00:40:58,250 --> 00:41:03,960
optimisation approaches we like manifolds like particle filters and so forth so

533
00:41:04,010 --> 00:41:08,860
most of these things can come from from left to right center field and always

534
00:41:08,860 --> 00:41:14,550
eagerly looking for new approaches for solving imposed a condition problems

535
00:41:14,630 --> 00:41:18,590
OK and we can all make a

536
00:41:18,600 --> 00:41:23,690
we cannot you know very easily to an optimisation because if you go back to

537
00:41:23,690 --> 00:41:30,790
the mid eighties regularisation can approach for solving will post early vision problems computing optical

538
00:41:30,790 --> 00:41:35,180
flow shape from shading stereo and all of them nicely made as any other two

539
00:41:35,180 --> 00:41:40,460
problem and we have the smoothness criterion and even to the ground equations and came

540
00:41:40,460 --> 00:41:43,540
up with iterative equations for solving them are

541
00:41:43,550 --> 00:41:49,580
is on solvers and so forth so we have learned this in our classes

542
00:41:49,620 --> 00:41:53,320
OK so i have to do is replace the l two and l one

543
00:41:53,370 --> 00:41:56,810
and then see what you know so we can at least the entry for us

544
00:41:57,010 --> 00:41:58,510
it's not totally

545
00:41:58,510 --> 00:42:04,460
alien concept right replacing l two with l one and having the appropriate constraints and

546
00:42:04,460 --> 00:42:07,860
see what it buys so computer vision researchers

547
00:42:07,890 --> 00:42:10,350
you are a uniquely

548
00:42:10,360 --> 00:42:16,340
in opposition to look into this but that's again what i'll argue as argued areas

549
00:42:16,360 --> 00:42:17,580
it's too tempting

550
00:42:17,580 --> 00:42:21,460
right to do that and should be and what we gain by doing that this

551
00:42:21,460 --> 00:42:26,460
is what would like to express now the folks at rice university built something of

552
00:42:26,490 --> 00:42:30,860
a single pixel camera and i didn't put those views here because again the copyright

553
00:42:30,860 --> 00:42:35,000
issue so i was very careful as what slides i'm going to use from others

554
00:42:35,000 --> 00:42:41,850
and so on so you can surely papers on that it's very interesting device that

555
00:42:41,860 --> 00:42:44,270
gets to the compressive measurements directly

556
00:42:44,280 --> 00:42:46,210
OK by introducing

557
00:42:46,230 --> 00:42:52,890
randomisation in the process of acquiring the electron i'll show is similar setup that is

558
00:42:53,150 --> 00:42:58,300
going my stories are using for compressive sensing of reflectance fields that are useful and

559
00:42:58,310 --> 00:43:01,910
graphics my first entry into this field was through

560
00:43:01,990 --> 00:43:07,490
doctor jo r and we kind of look back ground subtraction problem you know when

561
00:43:07,490 --> 00:43:13,930
you have a stationary camera moving camera and objects are moving OK

562
00:43:13,940 --> 00:43:17,690
background subtraction is the first thing you do especially for a stationary camera case to

563
00:43:17,690 --> 00:43:19,350
look at

564
00:43:19,370 --> 00:43:24,680
things that are moving to separate them from non-living backgrounds but the first thought that

565
00:43:24,680 --> 00:43:26,780
occurred to us is that the

566
00:43:26,840 --> 00:43:28,300
background is really

567
00:43:28,320 --> 00:43:32,470
a lot and the foreground are very small and sparks in terms of spatial extent

568
00:43:32,680 --> 00:43:38,630
so when compressive sensing ideas be useful for coming up with the background background subtraction

569
00:43:38,710 --> 00:43:43,550
and we are forced to ECCV two thousand eight and then we came

570
00:43:43,580 --> 00:43:49,320
the next another interesting development is the face recognition paper from berkeley university of illinois

571
00:43:50,050 --> 00:43:54,480
where they used to sparsity as as a way of coming up with the face

572
00:43:54,480 --> 00:44:00,910
recognition algorithms that we know that has the best results now for the bayesian set

573
00:44:01,310 --> 00:44:04,670
and we looked at it and we said OK maybe we can do it for

574
00:44:05,620 --> 00:44:07,040
now i'll tell you why

575
00:44:07,040 --> 00:44:12,500
we motivated for doing out is not faces and then again we came across the

576
00:44:12,500 --> 00:44:17,880
very beautiful paper by appears to this is from press around with his group at

577
00:44:19,070 --> 00:44:22,990
and i think this is a great example of compressive sensing because

578
00:44:23,060 --> 00:44:27,530
acquiring reflectance fields is part of the sense right and

579
00:44:27,540 --> 00:44:28,390
if you

580
00:44:28,410 --> 00:44:34,360
look at the demands on how much data you to collect completely characterize reflectance fields

581
00:44:34,480 --> 00:44:38,650
and we know that maybe the reflectance fields themselves out sparse then it looks like

582
00:44:38,650 --> 00:44:44,240
a very nice out the problem to study in the context of compressive sensing and

583
00:44:44,250 --> 00:44:44,890
so on

584
00:44:44,890 --> 00:44:48,670
object with mass and two so twice the mass

585
00:44:48,720 --> 00:44:50,170
so that should be

586
00:44:50,230 --> 00:44:52,150
the square root

587
00:44:52,210 --> 00:44:54,910
of and two divided by n one

588
00:44:54,970 --> 00:44:56,220
times ten times

589
00:44:56,230 --> 00:44:57,370
the period

590
00:44:57,480 --> 00:44:59,530
one of n one

591
00:44:59,620 --> 00:45:00,860
so i can make

592
00:45:01,020 --> 00:45:03,550
a prediction because this is the square root of two

593
00:45:03,590 --> 00:45:05,470
and i know what this is

594
00:45:05,510 --> 00:45:08,400
so i'll take my calculator

595
00:45:08,420 --> 00:45:11,230
and i'll take the square root of two

596
00:45:11,270 --> 00:45:13,450
and multiply that by

597
00:45:13,500 --> 00:45:14,700
so let's take

598
00:45:14,780 --> 00:45:18,450
fifteen point one five

599
00:45:18,490 --> 00:45:23,420
so that comes out to be twenty one point four two

600
00:45:23,480 --> 00:45:26,930
twenty one point four two is not clear that this too is meaningful

601
00:45:26,990 --> 00:45:33,300
and i'll come to sixty four dollar question what is the uncertainty is the prediction

602
00:45:36,060 --> 00:45:38,450
and this now becomes a little tricky

603
00:45:38,560 --> 00:45:41,420
so what i'm telling you now make confusion a bit it's not meant to be

604
00:45:41,730 --> 00:45:44,420
but i really wanna hold you responsible for it

605
00:45:44,510 --> 00:45:46,040
you may not think

606
00:45:46,080 --> 00:45:48,640
that the uncertainty in this measurement

607
00:45:48,650 --> 00:45:51,920
it follows from the uncertainty in this which is through which is about o point

608
00:45:51,920 --> 00:45:53,100
six percent

609
00:45:53,150 --> 00:45:55,260
and from the uncertainty in this

610
00:45:55,270 --> 00:45:57,080
this has about uncertainty

611
00:45:57,110 --> 00:45:58,810
o point six percent

612
00:45:58,820 --> 00:46:02,630
i got a low because i measure ten oscillations you see is only one out

613
00:46:02,630 --> 00:46:05,410
of four hundred fifty which is low

614
00:46:05,470 --> 00:46:08,590
what you may think that the uncertainty in there

615
00:46:08,650 --> 00:46:10,090
he calls this character

616
00:46:10,100 --> 00:46:12,960
of three hundred seventy two plus and minus one

617
00:46:12,960 --> 00:46:16,120
divided by hundred eighty six thousand ninety one

618
00:46:16,180 --> 00:46:20,480
and now you may argue and it's completely reasonable that you would argue that way

619
00:46:20,490 --> 00:46:22,570
you would say well this is roughly

620
00:46:22,620 --> 00:46:25,350
a quarter of a percent guarantee under the square root

621
00:46:25,370 --> 00:46:26,580
and this is roughly

622
00:46:26,590 --> 00:46:30,560
half percent error one out of two hundred is about half

623
00:46:30,570 --> 00:46:34,220
so you would add up to two hours quarter plus half that's about o point

624
00:46:35,060 --> 00:46:37,070
and because of the square root

625
00:46:37,120 --> 00:46:40,340
that becomes o point three five percent and that's wrong

626
00:46:40,380 --> 00:46:42,660
and the reason why that's completely wrong

627
00:46:42,670 --> 00:46:44,240
it has to do with the fact

628
00:46:44,960 --> 00:46:47,790
these two errors coupled to each other

629
00:46:48,790 --> 00:46:52,370
the hundred eighty six including the three seventy two

630
00:46:52,420 --> 00:46:56,100
the best way i can show you this suppose i measured and one divided by

631
00:46:56,110 --> 00:46:57,120
n one

632
00:46:57,130 --> 00:47:01,160
which would be hundred and eighty six plus or minus one divided by a hundred

633
00:47:01,180 --> 00:47:03,520
and eighty six plus minus one

634
00:47:03,580 --> 00:47:05,990
that number is one with a hundred heroes

635
00:47:06,040 --> 00:47:10,170
this number is one you have to master one object divided by the same object

636
00:47:10,220 --> 00:47:14,000
rather if you would say otherwise however percent error and this is the half percent

637
00:47:14,000 --> 00:47:17,080
there you would say to ratio has an error of one percent and that's not

638
00:47:17,080 --> 00:47:18,210
the case

639
00:47:18,220 --> 00:47:21,100
so i will not bother you was that i will not hold you responsible for

640
00:47:21,100 --> 00:47:23,680
that but it turns out that if you

641
00:47:23,720 --> 00:47:28,000
do it correctly you take the error of this into account of about o point

642
00:47:28,000 --> 00:47:29,110
six percent

643
00:47:29,160 --> 00:47:30,950
that the error in this

644
00:47:30,960 --> 00:47:34,960
ratio is really much less than point two percent you can almost forget about

645
00:47:35,000 --> 00:47:39,540
i will allow generously for one percent error in the final answer

646
00:47:39,580 --> 00:47:40,310
and so

647
00:47:40,320 --> 00:47:42,060
i stick to my prediction

648
00:47:42,110 --> 00:47:44,030
that the ten

649
00:47:45,320 --> 00:47:46,680
double the mass

650
00:47:46,700 --> 00:47:47,760
it's going to be

651
00:47:47,780 --> 00:47:51,110
like this

652
00:47:51,130 --> 00:47:54,920
and now we're going to get the observation

653
00:47:57,360 --> 00:47:58,330
and two

654
00:47:58,340 --> 00:48:00,210
has double the mass

655
00:48:00,210 --> 00:48:02,450
and that of course always has my

656
00:48:02,460 --> 00:48:06,090
uncertainty of my reaction time nothing i can do about that

657
00:48:06,110 --> 00:48:08,290
we will compare these two numbers

658
00:48:08,320 --> 00:48:12,040
so i'll put it in the autumn as on top of it

659
00:48:12,120 --> 00:48:15,320
in here

660
00:48:15,360 --> 00:48:19,520
taken together so that they won't fall off

661
00:48:19,520 --> 00:48:23,230
there we go

662
00:48:24,930 --> 00:48:27,460
o nobody that correctly crowd of two

663
00:48:27,500 --> 00:48:30,210
times fifteen point one five

664
00:48:30,250 --> 00:48:31,450
well given

665
00:48:31,500 --> 00:48:34,920
amplitude something like thirty maybe thirty five centimetres

666
00:48:34,950 --> 00:48:37,070
there we go

667
00:48:41,430 --> 00:48:43,330
much slower you see that

668
00:48:43,330 --> 00:48:45,480
natural perception action

669
00:48:45,490 --> 00:48:51,580
OK so first now let me give you some very basic intuition about unlabelled souls

670
00:48:51,620 --> 00:48:55,390
suppose they just give you to label points red and blue

671
00:48:55,410 --> 00:48:58,230
i think most people would agree

672
00:48:58,280 --> 00:48:59,480
and that's

673
00:48:59,490 --> 00:49:03,470
i i can ask you what the most natural classifier so you have some unlabeled

674
00:49:03,470 --> 00:49:07,540
point how would you could even the new point how would you classify it you

675
00:49:07,630 --> 00:49:11,390
most people would agree that something like this seems very natural evolution b here

676
00:49:11,840 --> 00:49:14,660
red should be given this still point

677
00:49:14,670 --> 00:49:16,270
that of course

678
00:49:16,280 --> 00:49:19,730
you know it can be something quite different but this is the most simple thing

679
00:49:19,730 --> 00:49:21,130
there is a lot of

680
00:49:21,150 --> 00:49:22,910
what can machine learning

681
00:49:23,870 --> 00:49:27,720
you know why this is in some sense optimal classifier

682
00:49:30,130 --> 00:49:34,000
i tell you something that i showed you the thing to point

683
00:49:34,010 --> 00:49:37,770
but i tell you well actually all possible points

684
00:49:37,810 --> 00:49:41,410
lie on this two circles i don't tell you anything about the clusters it's still

685
00:49:41,410 --> 00:49:46,040
a two class problem but i don't any possible points rely on one of the

686
00:49:47,870 --> 00:49:51,740
now i hope you agree that cutting it like this

687
00:49:51,820 --> 00:49:53,560
perhaps is less

688
00:49:55,960 --> 00:49:57,250
then in this case

689
00:49:59,300 --> 00:50:02,630
something like that is more reasonable

690
00:50:03,210 --> 00:50:04,990
knowing something about

691
00:50:05,040 --> 00:50:09,660
somehow what the data looks like about the geometry of the data

692
00:50:09,720 --> 00:50:11,660
changes our perception

693
00:50:11,670 --> 00:50:16,070
of which point i similarly somehow things that points in each of the circle you

694
00:50:16,070 --> 00:50:19,550
are more similar to each other than to points

695
00:50:20,660 --> 00:50:24,500
the circles

696
00:50:24,510 --> 00:50:29,120
so that one intuition and i'm going to repeat this a few times

697
00:50:29,140 --> 00:50:31,580
OK so

698
00:50:31,640 --> 00:50:33,210
similar situation

699
00:50:33,220 --> 00:50:35,310
two red points to blue points

700
00:50:35,330 --> 00:50:41,270
the black point i think most people would agree to build classified as blue

701
00:50:41,520 --> 00:50:48,680
so suppose now i tell you that actually all data licences can

702
00:50:48,780 --> 00:50:52,380
now as you can see well

703
00:50:53,550 --> 00:50:58,250
i think it's pretty self pretty evident that this should be read

704
00:50:58,260 --> 00:51:01,490
so what does it mean that means that the shape of of the curve tells

705
00:51:01,500 --> 00:51:02,580
us something about where

706
00:51:03,120 --> 00:51:05,480
the data is likely to be

707
00:51:05,530 --> 00:51:07,090
so actually

708
00:51:07,100 --> 00:51:11,270
in some sense you can say that even though the two points

709
00:51:13,680 --> 00:51:15,010
very close

710
00:51:15,020 --> 00:51:17,110
this the black and blue points

711
00:51:17,130 --> 00:51:22,590
there somehow not close in the right sense of being close they're not closed

712
00:51:22,600 --> 00:51:27,070
along the tongue and somehow along the curve is what counts

713
00:51:27,770 --> 00:51:30,720
john it is important and this is

714
00:51:30,740 --> 00:51:36,220
what one may call in manifold assumption thing that what counts how you should really

715
00:51:36,220 --> 00:51:37,440
measure distances

716
00:51:37,530 --> 00:51:39,710
you should really measured distances

717
00:51:41,370 --> 00:51:44,240
the space of your data

718
00:51:44,290 --> 00:51:52,210
OK so now there is another assumption which can be made

719
00:51:52,280 --> 00:51:57,330
which may be called cluster assumption and this is the following so again suppose i

720
00:51:57,330 --> 00:52:00,370
give you this points and i give you a black point i asked to classify

721
00:52:00,370 --> 00:52:04,470
the black point you might say the bike point is probably blue

722
00:52:04,560 --> 00:52:07,120
that's looks reasonable

723
00:52:07,330 --> 00:52:10,790
now i tell you case all data lies on this

724
00:52:10,860 --> 00:52:15,140
this is a so a uniform distribution on this it doesn't have to be

725
00:52:15,210 --> 00:52:19,860
uniform you can imagine that somehow the density is high here high and low in

726
00:52:19,860 --> 00:52:22,420
between but for

727
00:52:22,470 --> 00:52:25,320
it's hard to make a picture of that this

728
00:52:25,400 --> 00:52:31,680
so now you i hope you would agree that somehow there is this big clusters

729
00:52:31,690 --> 00:52:36,020
this sort of connected but not very strongly and this

730
00:52:36,160 --> 00:52:38,980
that belong to the same cluster

731
00:52:39,030 --> 00:52:42,490
so now you've probably seen

732
00:52:42,510 --> 00:52:45,360
it looks a little more likely that ran

733
00:52:45,370 --> 00:52:46,280
it should be

734
00:52:46,520 --> 00:52:51,370
the black should be classified as red

735
00:52:51,410 --> 00:52:54,700
so the somehow the important intuitions

736
00:52:54,720 --> 00:53:01,160
which need to be formalized in which will be to some degree formalized in stuff

737
00:53:01,210 --> 00:53:02,650
OK so

738
00:53:04,370 --> 00:53:08,050
not just that i actually didn't say anything about the data i just said i

739
00:53:08,050 --> 00:53:11,920
mean about the actual data i just said something about the space of all possible

740
00:53:13,040 --> 00:53:17,020
but in reality of course we don't know what the things would have the space

741
00:53:17,020 --> 00:53:17,720
of all

742
00:53:18,180 --> 00:53:21,930
digits just for example all handwritten digits we have no idea what the shape of

743
00:53:21,930 --> 00:53:23,020
the spaces

744
00:53:23,130 --> 00:53:28,310
i mean it's some sort of complicated space and whatever eight hundred dimensional space i

745
00:53:28,310 --> 00:53:29,790
think is like

746
00:53:29,840 --> 00:53:33,700
you know resolution maybe like twenty five i twenty five whatever

747
00:53:34,540 --> 00:53:40,310
we actually don't know what it we just have some examples sampled from that space

748
00:53:40,810 --> 00:53:43,680
so really what we want to somehow

749
00:53:43,730 --> 00:53:49,620
to learn something about the structure of the manifold as part was saying in the

750
00:53:49,620 --> 00:53:53,350
previous that we want to learn it from unlabelled data

751
00:53:55,470 --> 00:53:59,790
as you can see if you have one labelled data you can see that

752
00:53:59,910 --> 00:54:04,220
this is of course it example but it gives some idea that we can learn

753
00:54:04,220 --> 00:54:05,790
some geometric features

754
00:54:05,800 --> 00:54:13,720
and we can hopefully you this to improve our inference procedures

755
00:54:13,730 --> 00:54:16,270
so this is really this

756
00:54:16,330 --> 00:54:21,940
the flight perhaps provide some intuition for

757
00:54:21,950 --> 00:54:25,000
OK so now i will be a little bit

758
00:54:25,010 --> 00:54:29,350
more formal so what i would like to say here is that what is a

759
00:54:29,350 --> 00:54:31,290
manifold assumption

760
00:54:31,310 --> 00:54:36,980
it's somehow the functions of interest so we would like to look at functions of

761
00:54:37,030 --> 00:54:41,570
you know function is something like the probability that a given point is blue

762
00:54:41,590 --> 00:54:46,570
versus probability that this point is right so this is somehow function of interest

763
00:54:46,570 --> 00:54:48,450
and it's measure often

764
00:54:48,520 --> 00:54:50,770
represented by sigma squared

765
00:54:50,780 --> 00:54:52,720
that's the greek letters sigma

766
00:54:52,740 --> 00:54:54,840
lower case words

767
00:54:54,850 --> 00:54:57,510
or especially when talking about

768
00:55:01,050 --> 00:55:05,800
estimates of the variance we we sometimes they squared

769
00:55:06,260 --> 00:55:11,970
or we say standard deviation

770
00:55:13,780 --> 00:55:14,720
the variance

771
00:55:14,820 --> 00:55:18,030
the standard deviation is the square root of the variance

772
00:55:21,380 --> 00:55:23,900
population variance

773
00:55:24,840 --> 00:55:29,700
variance of some random variable x

774
00:55:29,770 --> 00:55:31,630
it is defined

775
00:55:33,550 --> 00:55:36,550
the summation i equals

776
00:55:36,610 --> 00:55:38,740
one two infinity

777
00:55:38,780 --> 00:55:41,240
the probability

778
00:55:41,260 --> 00:55:42,550
that x

779
00:55:42,570 --> 00:55:45,280
equals x by

780
00:55:45,300 --> 00:55:47,360
times that survive

781
00:55:54,570 --> 00:55:55,610
all right so

782
00:55:55,820 --> 00:55:58,030
news of x

783
00:55:58,030 --> 00:56:00,190
that news of x is the

784
00:56:00,240 --> 00:56:03,300
i mean we just defined x

785
00:56:03,340 --> 00:56:06,570
this the expectation of vectors are also erected

786
00:56:06,650 --> 00:56:13,320
so it's the it's the probability weighted average of the square deviations from the mean

787
00:56:13,360 --> 00:56:14,860
if it moves like

788
00:56:14,900 --> 00:56:16,670
either way from the mean

789
00:56:16,670 --> 00:56:19,570
this number squared is a big number more

790
00:56:19,590 --> 00:56:21,090
x moves

791
00:56:21,130 --> 00:56:23,920
the bigger the variances

792
00:56:24,970 --> 00:56:28,320
there's also another variance measure

793
00:56:28,360 --> 00:56:29,380
which we use

794
00:56:29,400 --> 00:56:32,280
in the sample

795
00:56:32,300 --> 00:56:35,010
also there sometimes

796
00:56:35,030 --> 00:56:39,530
so this is sigma squared

797
00:56:39,570 --> 00:56:42,090
there's also another variance measure

798
00:56:42,110 --> 00:56:44,570
which is for the sample

799
00:56:44,690 --> 00:56:47,510
and we have n observations

800
00:56:47,570 --> 00:56:49,050
it the

801
00:56:49,110 --> 00:56:50,470
the summation

802
00:56:50,470 --> 00:56:52,260
i equals one

803
00:56:56,070 --> 00:56:57,400
x bar

804
00:56:58,760 --> 00:57:00,930
all over

805
00:57:01,070 --> 00:57:03,860
there should be squared

806
00:57:04,490 --> 00:57:09,610
OK so that's the

807
00:57:09,650 --> 00:57:12,240
sample variance

808
00:57:12,260 --> 00:57:15,240
some people will divide by n minus one

809
00:57:17,670 --> 00:57:20,720
i suppose i would accept either answer

810
00:57:20,740 --> 00:57:23,260
i'm just keeping it simple here

811
00:57:26,590 --> 00:57:31,260
they divided by n minus one to make it an unbiased estimator of the

812
00:57:31,280 --> 00:57:33,490
population variance but

813
00:57:33,510 --> 00:57:35,990
i'm just going to show it in a simple way here so you see what

814
00:57:35,990 --> 00:57:36,950
it is

815
00:57:37,110 --> 00:57:41,030
it's a measure of how much text deviates from the mean but it is it

816
00:57:41,050 --> 00:57:44,260
squared it waits big deviations law

817
00:57:44,280 --> 00:57:47,670
because the square of big numbers really big so

818
00:57:48,570 --> 00:57:51,970
that's the difference

819
00:57:52,920 --> 00:57:54,720
all right so that complete

820
00:57:54,760 --> 00:57:56,430
central tendency

821
00:57:56,450 --> 00:57:57,920
and dispersion

822
00:57:57,920 --> 00:58:02,970
we're going to be talking about these in finance as regards returns because

823
00:58:03,150 --> 00:58:07,700
generally the idea here is that we want high returns so we want to high

824
00:58:07,700 --> 00:58:10,090
expected value returns

825
00:58:10,110 --> 00:58:14,240
but we don't like variance so expected value is good

826
00:58:14,260 --> 00:58:16,720
variance is bad because

827
00:58:16,760 --> 00:58:19,470
that's risk that's uncertainty west but this whole

828
00:58:19,470 --> 00:58:23,550
theories about how to avoid how to get a lot of expected return

829
00:58:24,220 --> 00:58:26,990
without getting a lot of risk

830
00:58:27,030 --> 00:58:29,760
another concept that's very basic here

831
00:58:29,780 --> 00:58:31,490
the covariance

832
00:58:31,600 --> 00:58:32,950
and covariance

833
00:58:32,990 --> 00:58:36,400
is a measure of how much two variables move

834
00:58:40,110 --> 00:58:41,740
the covariance

835
00:58:42,060 --> 00:58:43,360
it is

836
00:58:49,430 --> 00:58:50,820
we call it

837
00:58:50,920 --> 00:58:54,300
now we have two random variables of the covariance

838
00:58:54,340 --> 00:58:57,300
of x and y

839
00:58:57,340 --> 00:59:00,530
i just talked about it in the sample term

840
00:59:00,570 --> 00:59:02,590
it's the summation

841
00:59:02,610 --> 00:59:04,320
i equals one

842
00:59:05,740 --> 00:59:07,010
of x

843
00:59:07,030 --> 00:59:10,360
minus bar

844
00:59:13,720 --> 00:59:16,200
minus white bar

845
00:59:16,200 --> 00:59:21,170
to twenty million dollars probably something like that but the probability of getting this monies very

846
00:59:21,190 --> 00:59:23,740
low let's say that we are working data science

847
00:59:23,890 --> 00:59:30,140
and it is a very decent probability of making this mining i'd say ten percent as the probability actually

848
00:59:30,400 --> 00:59:35,400
for a company that doesn't exist right so that they when we optimise this function and

849
00:59:35,420 --> 00:59:37,620
upward like you know something might to

850
00:59:37,840 --> 00:59:39,620
two million dollars something like that

851
00:59:39,780 --> 00:59:45,370
and look on our site other side all base that this function as basically

852
00:59:45,660 --> 00:59:48,490
optimized for the probability of success

853
00:59:48,710 --> 00:59:53,370
and probably getting lower reward that's what idea that when the joined plays number right

854
00:59:53,390 --> 00:59:54,580
four hundred something

855
00:59:55,690 --> 00:59:58,830
the company was very successful at that point so the the

856
00:59:59,090 --> 01:00:01,310
the probability of success was pretty high

857
01:00:01,680 --> 01:00:08,880
the op leaf was relatively low right let's say that well a person can make like a million dollars

858
01:00:08,900 --> 01:00:12,280
if a person joins a very successful company like that

859
01:00:12,450 --> 01:00:13,090
and probably

860
01:00:13,450 --> 01:00:17,720
successes pretty much one hundred percent by the way that the real stories that

861
01:00:17,970 --> 01:00:19,140
i talked to

862
01:00:19,470 --> 01:00:24,030
to executives at length dna i had a chance to talk with them both damn

863
01:00:24,270 --> 01:00:29,280
left google award for when the and i asked pretty much the same question to both of

864
01:00:29,850 --> 01:00:35,160
guys how could we are you leave very strong successful company and

865
01:00:35,430 --> 01:00:38,900
and join a start-up like when the it's probably were risky

866
01:00:39,240 --> 01:00:43,890
and they gave me exactly the same ansari was very surprised you know what run

867
01:00:44,220 --> 01:00:47,180
there's just no worries it'll

868
01:00:47,620 --> 01:00:53,230
so if we look here and here and up with like knows founding a company our

869
01:00:53,250 --> 01:00:55,490
objective function gives like about to me and

870
01:00:55,890 --> 01:01:01,860
add the areas like you know a data scientist on a very successful companies let the

871
01:01:01,880 --> 01:01:03,380
objective function gives us

872
01:01:03,800 --> 01:01:08,160
one million something like that what i'm saying is the kind of goes down

873
01:01:08,510 --> 01:01:10,430
if you are same number why i'm

874
01:01:11,730 --> 01:01:17,300
this you know early-stage share a start-up

875
01:01:17,920 --> 01:01:21,430
the risk the probability of like getting your money

876
01:01:21,760 --> 01:01:26,250
is the same ten percent as if you are a founder right but your reward is

877
01:01:26,260 --> 01:01:30,380
much lower like substantially lower probably tenax low or something like that

878
01:01:30,600 --> 01:01:32,390
if you join

879
01:01:32,630 --> 01:01:40,580
relatively me established company if you're number fifty something like that dan your probability of success

880
01:01:40,600 --> 01:01:47,610
is not one hundred percent right but your reward steel like on number fifty number two hundred is is

881
01:01:47,630 --> 01:01:48,410
kind of see

882
01:01:48,660 --> 01:01:51,090
so solemn you think or

883
01:01:51,400 --> 01:01:57,040
the estimate about two million boxes the expected value as data science

884
01:01:57,350 --> 01:02:02,070
founder for data science start yeah yeah not ok you know sort of the probability of

885
01:02:05,470 --> 01:02:09,820
the value given success if you want to say that the data science guide three

886
01:02:09,910 --> 01:02:12,790
or a person three person team

887
01:02:13,460 --> 01:02:16,940
ok so i think run may have done more carefully

888
01:02:18,240 --> 01:02:20,300
i have here here's a here's my off

889
01:02:20,620 --> 01:02:22,540
high-level react that analysis

890
01:02:23,010 --> 01:02:24,100
if that were true

891
01:02:24,560 --> 01:02:29,340
given salaries are familiar with for a data scientist these days

892
01:02:29,840 --> 01:02:32,220
in that equation would say you should never leave your job

893
01:02:33,160 --> 01:02:34,460
my guess is it's

894
01:02:36,560 --> 01:02:39,790
this is a new area that we know is in high demand

895
01:02:40,460 --> 01:02:43,730
that we know is is going to be crucial for quite

896
01:02:43,930 --> 01:02:44,900
and and

897
01:02:45,110 --> 01:02:47,760
upcoming times so this is not like your average

898
01:02:48,100 --> 01:02:49,540
start-up addressing

899
01:02:49,780 --> 01:02:53,270
an average you know consumer play in a very crowded space

900
01:02:53,720 --> 01:02:54,480
this is a

901
01:02:56,680 --> 01:03:01,750
great space here right so your chances of getting competition there's so many barriers that have

902
01:03:01,770 --> 01:03:03,960
to do with knowledge have to do with intuition

903
01:03:04,470 --> 01:03:07,340
all of that that's working for you and mices my suspicion

904
01:03:07,690 --> 01:03:10,400
a different way to condition his numbers

905
01:03:11,690 --> 01:03:13,310
data science and big data

906
01:03:13,660 --> 01:03:20,390
i would imagine that it turns to be higher might might have guessed actually totally yeah

907
01:03:21,230 --> 01:03:22,640
but i don't i guess is

908
01:03:23,360 --> 01:03:24,950
you could easily stand to make

909
01:03:26,570 --> 01:03:28,440
ten million dollars in five years

910
01:03:29,050 --> 01:03:34,830
so that would be like yeah i think that it's a definite the ballpark were

911
01:03:34,850 --> 01:03:41,930
not too far away right yeah i wanted to yeah yeah yeah yeah what i wanted

912
01:03:41,950 --> 01:03:46,290
to say i was talking about appli right it's a together with the seller after also

913
01:03:46,300 --> 01:03:47,220
it's like a little bit of

914
01:03:49,260 --> 01:03:53,420
i guess thing here right then forget it i'm not going to check eat thousand foot

915
01:03:53,430 --> 01:03:57,190
going to look for a startup yeah i

916
01:03:57,490 --> 01:03:58,380
take it back

917
01:03:58,920 --> 01:03:59,840
the health insurance

918
01:04:01,380 --> 01:04:02,130
if you like

919
01:04:02,610 --> 01:04:04,210
expectation that stupid move

920
01:04:06,770 --> 01:04:10,820
well because ensures making money so expectations you know that you will spend and

921
01:04:11,000 --> 01:04:12,560
less on your health care

922
01:04:12,760 --> 01:04:15,310
and if you paid insurance otherwise it would be out of business

923
01:04:16,990 --> 01:04:17,790
my point is

924
01:04:18,280 --> 01:04:20,850
very person perspective this male the true

925
01:04:21,050 --> 01:04:23,300
maybe it's kind of the female but

926
01:04:23,690 --> 01:04:27,370
so i'm running the single mom get with nine your own

927
01:04:27,550 --> 01:04:30,200
and his great expectations could make

928
01:04:30,400 --> 01:04:31,310
ten million dollars

929
01:04:31,540 --> 01:04:35,400
i still need to feed and mom and i can afford looking

930
01:04:35,820 --> 01:04:38,390
fourteen now as de side that may fail

931
01:04:39,060 --> 01:04:40,600
i think you're right that

932
01:04:40,870 --> 01:04:42,850
the downside potential actually isn't as

933
01:04:43,230 --> 01:04:44,730
baker it's just last time

934
01:04:44,880 --> 01:04:46,840
the good news is if it was yeah well

935
01:04:47,030 --> 01:04:48,710
yes men other things you can do

936
01:04:48,900 --> 01:04:52,610
it's not like step without a job for the rest of your life right so

937
01:04:52,790 --> 01:04:53,400
the risk

938
01:04:53,930 --> 01:04:57,480
is not bad that ches the point when i entered the game

939
01:04:58,010 --> 01:05:02,290
different expectations on life that came with sometimes for myself some

940
01:05:02,840 --> 01:05:08,980
khushi salary that i knew i would be easily making gladdens i negotiated might terms

941
01:05:09,170 --> 01:05:09,960
joining a

942
01:05:10,090 --> 01:05:11,350
twenty people started saying

943
01:05:11,970 --> 01:05:12,820
equities great

944
01:05:13,130 --> 01:05:13,790
could you

945
01:05:14,160 --> 01:05:17,320
i n for somewhat more security and

946
01:05:17,700 --> 01:05:22,060
that's actually an argument you can have it's not like you have to look for free and look for the big

947
01:05:22,110 --> 01:05:23,980
bang waste in between

948
01:05:25,640 --> 01:05:27,940
ok so i think we

949
01:05:28,330 --> 01:05:32,420
we want our we've moved to one of the we are there is this a

950
01:05:32,430 --> 01:05:35,300
couple questions that seem to have gotten a lot of up votes

951
01:05:35,590 --> 01:05:37,370
a couple down books to

952
01:05:37,650 --> 01:05:41,070
one to take a couple questions and yes yes yes i think it's

953
01:05:41,090 --> 01:05:41,830
time to

954
01:05:41,960 --> 01:05:47,190
go to first when not so from china and the souls

955
01:05:47,790 --> 01:05:50,370
or the ground for close to the first

956
01:05:51,170 --> 01:05:59,870
so this question for everyone much actually put it to someone in our

957
01:05:59,890 --> 01:06:01,160
so you

958
01:06:01,430 --> 01:06:04,650
had to know when an idea start-up or really

959
01:06:12,080 --> 01:06:13,050
so i can

960
01:06:14,800 --> 01:06:16,220
you know if it's if it's personal

961
01:06:16,650 --> 01:06:17,830
you're doing it

962
01:06:18,020 --> 01:06:21,070
it's a completely different objective function and if

963
01:06:21,280 --> 01:06:22,110
you're an investor

964
01:06:22,380 --> 01:06:25,710
so use these days i do this a lot i evaluate

965
01:06:25,870 --> 01:06:27,970
probably hundreds of years to try to the site

966
01:06:28,400 --> 01:06:30,270
how many tens are we going to invest in

967
01:06:31,260 --> 01:06:35,880
my yeah and that are there are many many many many factors right

968
01:06:36,430 --> 01:06:37,570
it's not

969
01:06:37,710 --> 01:06:40,590
my first answer to that question is it's not the idea

970
01:06:41,550 --> 01:06:42,470
it's the team

971
01:06:43,370 --> 01:06:49,830
so where it's it's team team team because that idea most likely is going to change and change multiple times

972
01:06:50,290 --> 01:06:52,610
so and by the way i c team

973
01:06:52,610 --> 01:06:57,020
the interesting thing is in the beginning you graph could be rather pick the white

974
01:06:57,060 --> 01:06:59,110
grow for example you say all

975
01:06:59,120 --> 01:07:04,260
this must be cobbled doesn't cover what it is useless or not frequent you can

976
01:07:04,260 --> 01:07:09,410
drop it was dropped those car belongs the genes or something you find the remaining

977
01:07:09,410 --> 01:07:12,120
part actually may not set by this

978
01:07:12,170 --> 01:07:16,500
that's apart or night start being becoming effective in cutting

979
01:07:16,530 --> 01:07:18,730
that's why in the middle can cut

980
01:07:18,750 --> 01:07:22,770
so this is the way you know you can recognise you can cut the URL

981
01:07:24,360 --> 01:07:30,750
but sometimes it's very shaky week or this pattern inseparable they give you a conference

982
01:07:30,750 --> 01:07:32,460
like this

983
01:07:32,480 --> 01:07:35,720
the vertex connectivity of the pattern is greater than ten

984
01:07:35,730 --> 01:07:38,080
OK but it's conductivity means

985
01:07:38,090 --> 01:07:39,060
the verdict

986
01:07:39,070 --> 01:07:41,420
how many things they connect

987
01:07:41,480 --> 01:07:45,790
but why are you doing the graph pattern mining find graph

988
01:07:45,790 --> 01:07:49,570
OK what is the kind of lifting you have to go back to your data

989
01:07:49,570 --> 01:07:53,720
set to see how they connect with the remaining parts so you can not separate

990
01:07:53,720 --> 01:07:54,870
your pattern

991
01:07:54,890 --> 01:07:57,060
from your grasp percent

992
01:07:57,650 --> 01:07:59,690
so in that sense

993
01:07:59,710 --> 01:08:01,930
if you want to evaluate this pattern

994
01:08:01,950 --> 01:08:06,030
suppose this is spending money that you had to put this pattern back to

995
01:08:06,070 --> 01:08:07,300
back to black

996
01:08:07,310 --> 01:08:13,020
that simply says you can while you're thinking you're doing pattern mining grow growing this

997
01:08:13,020 --> 01:08:18,360
pattern you're never forget this pattern from witchcraft and how they connect with the remaining

998
01:08:19,290 --> 01:08:22,560
that part called pattern inseparable OK

999
01:08:22,570 --> 01:08:27,730
and why you mining find this pattern inseparable means you go back to original graph

1000
01:08:27,730 --> 01:08:30,460
you find the constraints can still

1001
01:08:30,480 --> 01:08:32,720
can you not be satisfiable

1002
01:08:32,780 --> 01:08:33,700
then you can

1003
01:08:33,730 --> 01:08:34,860
cut the grass

1004
01:08:34,880 --> 01:08:38,870
so that's the local pattern inseparable and with this

1005
01:08:38,920 --> 01:08:40,210
actually we

1006
01:08:40,230 --> 01:08:41,370
work out

1007
01:08:41,380 --> 01:08:43,730
the most commonly used

1008
01:08:43,780 --> 01:08:46,820
these you know that different constraints

1009
01:08:46,830 --> 01:08:51,250
OK then we check up check about whether it's astronomy

1010
01:08:51,300 --> 01:08:56,970
pattern on and a weak pattern antimonotone always pattern inseparable

1011
01:08:57,000 --> 01:09:02,510
and pattern separable the good thing good news you look at how many things are

1012
01:09:02,510 --> 01:09:05,790
yes because you go yes you and i think of

1013
01:09:05,980 --> 01:09:09,460
OK for patterns in several the anti monotone

1014
01:09:09,480 --> 01:09:14,000
everybody say yes OK simply to put back you if you know is not a

1015
01:09:14,140 --> 01:09:17,250
useful can cut OK some you can see

1016
01:09:17,290 --> 01:09:19,910
some part of the yes some part of this now

1017
01:09:20,030 --> 01:09:24,950
OK that that means the nice is not always usable but least you can use

1018
01:09:24,950 --> 01:09:29,860
the last one two your code can do a lot of interesting things OK

1019
01:09:29,860 --> 01:09:33,860
so that's a lot of course we have experiment results our don't have time to

1020
01:09:33,860 --> 01:09:35,120
discuss it

1021
01:09:35,140 --> 01:09:38,650
i'll go through the index part OK

1022
01:09:39,200 --> 01:09:41,700
so actually when we did this

1023
01:09:41,730 --> 01:09:43,130
graph pattern mining

1024
01:09:43,140 --> 01:09:44,690
we talked to some

1025
01:09:44,710 --> 01:09:49,610
chemists and biochemists OK lots of people in our campus

1026
01:09:49,690 --> 01:09:55,780
and then the chemists not only want not just like like appear you know like

1027
01:09:55,780 --> 01:09:58,800
you coming to work together with the chemists

1028
01:09:58,880 --> 01:10:00,630
they said

1029
01:10:00,650 --> 01:10:02,860
you do graph mining five u

1030
01:10:02,860 --> 01:10:05,790
we would like to do mining we reached that stage yet

1031
01:10:05,800 --> 01:10:07,810
what we want his search

1032
01:10:07,820 --> 01:10:09,650
OK we get a huge

1033
01:10:09,680 --> 01:10:13,730
graph database because the most chemical compound babies

1034
01:10:13,780 --> 01:10:16,040
for example i work with one

1035
01:10:16,360 --> 01:10:20,720
chemists here is working on those biochemist for the antibiotics

1036
01:10:20,750 --> 01:10:27,790
anybody see they know the antibiotics by exposure to human some people abuse it finally

1037
01:10:27,790 --> 01:10:33,820
the those germs the they develop some resistance you have the developing new antibiotics

1038
01:10:33,830 --> 01:10:35,060
but it's

1039
01:10:35,070 --> 01:10:39,760
outside in nature there are lots of chemical compounds but you want to search

1040
01:10:39,760 --> 01:10:44,590
your pages those in that in those queries can actually come matching your site

1041
01:10:44,650 --> 01:10:48,840
giving a deeper experience the user and more traffic to the publisher

1042
01:10:48,960 --> 01:10:52,920
we've seen a lot of work by publishers doing whatever they can to get themselves

1043
01:10:52,920 --> 01:10:56,860
more traffic so when you have these kind of natural resources and driving traffic

1044
01:10:56,960 --> 01:11:00,920
publishers will be very interested in doing whatever they can

1045
01:11:00,940 --> 01:11:05,150
another interesting twist here is that the systems could give publishers

1046
01:11:05,210 --> 01:11:08,760
feedback on improving their content so you could say

1047
01:11:08,780 --> 01:11:12,570
instead of having to optimize your pages for packing in a bunch of keywords to

1048
01:11:12,570 --> 01:11:15,670
gain the search engine that publishers could be told

1049
01:11:15,970 --> 01:11:20,650
actually this this is this concept on your page is not clearly expressed by to

1050
01:11:20,670 --> 01:11:24,860
express more clearly and then do a better job of matching users

1051
01:11:24,880 --> 01:11:28,190
and as the the ecosystem of course is users

1052
01:11:28,190 --> 01:11:32,610
there's a fair amount of work on getting users to play games and to create

1053
01:11:32,610 --> 01:11:37,820
and improve resources several papers here in this conference on that topic

1054
01:11:37,840 --> 01:11:42,320
in addition to these things as they become useful tools for the users the users

1055
01:11:42,320 --> 01:11:48,360
can provide feedback to get better search powerset is already experimenting with ideas where users

1056
01:11:48,360 --> 01:11:50,820
can give results thumbs-up or thumbs-down

1057
01:11:50,840 --> 01:11:55,090
and and then if it's a thumbs-down dive in and actually express which of the

1058
01:11:55,090 --> 01:11:59,170
ontological resources you know may or may not have been the right ones or other

1059
01:11:59,170 --> 01:12:00,460
kinds of things like that

1060
01:12:00,530 --> 01:12:08,490
in addition as some users might want to create ontologies lightweight little simple ontologies to

1061
01:12:09,270 --> 01:12:11,780
their own search experience better

1062
01:12:11,780 --> 01:12:13,970
they could potentially refer to

1063
01:12:13,990 --> 01:12:19,150
their their family members by the rules and then use use that kind of all

1064
01:12:19,240 --> 01:12:21,900
the world for the role to matching their family members

1065
01:12:21,990 --> 01:12:26,380
these kinds of things similar a group of people to do is to do the

1066
01:12:26,380 --> 01:12:28,110
same kind of thing

1067
01:12:28,170 --> 01:12:32,550
the third element of this community would be developers there's been a lot of work

1068
01:12:32,550 --> 01:12:40,070
lately in vertical search which is people making very specialised search engines by putting a

1069
01:12:40,070 --> 01:12:44,420
lot of their knowledge into the system i think it's possible that you can move

1070
01:12:44,420 --> 01:12:49,010
this kind of level of abstraction so that the vertical search specialists can really just

1071
01:12:49,010 --> 01:12:52,240
packages are knowledge by means of overthrowing to the ontology

1072
01:12:52,280 --> 01:12:58,190
and make that ontology available for use in the context of the general search engines

1073
01:12:58,190 --> 01:12:59,260
so the

1074
01:12:59,260 --> 01:13:02,440
so i guess is the fourth part of the community that talk about which are

1075
01:13:02,440 --> 01:13:04,110
advertisers so

1076
01:13:04,130 --> 01:13:10,880
advertisers could create and upload ontologies to express all the kinds of things should match

1077
01:13:10,880 --> 01:13:12,240
their offering

1078
01:13:12,260 --> 01:13:16,960
and that can help make a much better job matching so the interesting thing about

1079
01:13:16,960 --> 01:13:20,760
all this is that once you start with a broad platform when that actually gets

1080
01:13:20,760 --> 01:13:22,230
deployed a larger scale

1081
01:13:22,230 --> 01:13:23,730
then there are

1082
01:13:23,760 --> 01:13:29,470
self-interest in all different aspects of the ecosystem that now becomes worse than adding more

1083
01:13:29,470 --> 01:13:32,240
knowledge system because it will make their lives better

1084
01:13:32,300 --> 01:13:33,110
and so

1085
01:13:33,110 --> 01:13:36,050
i think once we get these kind of systems out there

1086
01:13:36,070 --> 01:13:41,470
then it's possible that semantic web can take off faster than anyone realizes and we

1087
01:13:41,470 --> 01:13:47,030
can go from broadly applicable systems too deeply specialized vertically oriented

1088
01:13:47,190 --> 01:13:52,260
experts you know in pretty fast order

1089
01:13:52,360 --> 01:13:55,920
so to conclude power sets very active

1090
01:13:56,150 --> 01:14:02,380
is and has been plainly very active with the research communities were looking to develop

1091
01:14:02,380 --> 01:14:06,530
an open platform where people could use our API eyes and also the people access

1092
01:14:06,530 --> 01:14:12,550
technology to build mashups and the kinds of applications world's contributing datasets annotations and open

1093
01:14:12,550 --> 01:14:13,840
source software back

1094
01:14:13,860 --> 01:14:19,380
we're excited and interested in partnership collaborations and you can now go

1095
01:14:20,030 --> 01:14:23,190
try power laps which is our early community

1096
01:14:23,210 --> 01:14:24,570
if you go to

1097
01:14:24,650 --> 01:14:29,440
www powerset dot com you can sign up now letting in people pretty quickly into

1098
01:14:29,440 --> 01:14:30,970
the service

1099
01:14:30,990 --> 01:14:42,260
from so that i like to thank you all for your attention

1100
01:14:42,340 --> 01:14:48,300
OK to i think to summarise the question i did have trouble hearing yourself i

1101
01:14:48,300 --> 01:14:51,400
miss it then you can have some kind of echo going on here in the

1102
01:14:53,070 --> 01:14:57,880
so my question the results look very exciting and the kind of things people been

1103
01:14:57,880 --> 01:15:03,440
talking about doing for a very long time it looks like our cities using technology

1104
01:15:03,440 --> 01:15:07,710
seems to have been around for a long time namely the LFG technology from xerox

1105
01:15:07,710 --> 01:15:11,630
parc so why is it suddenly now this stuff is actually possible to stack up

1106
01:15:11,650 --> 01:15:13,470
the question correctly

1107
01:15:14,440 --> 01:15:19,230
so the the first thing is that are

1108
01:15:19,230 --> 01:15:22,050
it's not that the workers just was done

1109
01:15:22,070 --> 01:15:26,260
twenty years ago and has been sitting around and so now

1110
01:15:26,280 --> 01:15:30,970
some things change sort of suddenly make make it workable in fact the party group

1111
01:15:30,990 --> 01:15:32,940
was founded in nineteen seventy two

1112
01:15:33,030 --> 01:15:37,840
and the interesting story here is that every five years the group go address more

1113
01:15:37,860 --> 01:15:41,530
key problems and then come back to the management and say well we really need

1114
01:15:41,530 --> 01:15:46,130
some major accomplishments but the technology is not ready to be applied

1115
01:15:46,150 --> 01:15:48,630
and this actually continue for thirty years

1116
01:15:48,630 --> 01:15:52,820
so you know people might remember back in the eighties there was a period where

1117
01:15:53,090 --> 01:15:56,030
towards the end of the eighties there was the semantic web you know winter

1118
01:15:56,050 --> 01:16:01,260
i people just stopped working on semantics largely in the community

1119
01:16:01,280 --> 01:16:06,460
and there was lot in computational linguistics so things like excitement about you know empirical

1120
01:16:06,460 --> 01:16:11,230
methods in machine learning of natural language resources but it was all syntactic on the

1121
01:16:11,230 --> 01:16:14,300
grammars and there was no and and semantic systems is kind of almost a decade

1122
01:16:14,320 --> 01:16:19,010
or were hardly doing anything but actually the the folks apart

1123
01:16:19,130 --> 01:16:21,420
and their in our colleagues in the community

1124
01:16:21,440 --> 01:16:24,400
didn't stop working on these they they actually kept going so was on the only

1125
01:16:24,400 --> 01:16:29,050
place the kept working on the entire stack natural language they just cut tackling one

1126
01:16:29,050 --> 01:16:35,130
problem after another so they made some major improvements with respect to ambiguity management

1127
01:16:35,150 --> 01:16:38,190
which was really it took a long time to work through

1128
01:16:38,210 --> 01:16:43,360
the problem of ambiguity is you know plagues natural language systems and normally what happens

1129
01:16:43,610 --> 01:16:48,150
is people make the best systems have long pipeline and they make the best guess

1130
01:16:48,150 --> 01:16:50,110
at each step along the way

1131
01:16:50,130 --> 01:16:51,780
well if you make a wrong guess

1132
01:16:51,780 --> 01:16:55,010
early on then you're guaranteed the interpretation of everybody

1133
01:16:55,010 --> 01:16:58,420
so these algorithms are pretty standard and

1134
01:16:58,550 --> 01:17:03,510
given a model is relatively straightforward to it to derive an algorithm for doing inference

1135
01:17:03,530 --> 01:17:05,930
or learning in your model okay

1136
01:17:06,680 --> 01:17:08,370
so one thing which I

1137
01:17:08,420 --> 01:17:14,240
which is I always find a bit weird about probabilistic models is that

1138
01:17:14,430 --> 01:17:19,010
given our observed data x here we have two things that we don't know

1139
01:17:19,010 --> 01:17:24,120
about we have latent variables y and our parameter's theta right

1140
01:17:24,160 --> 01:17:26,550
and if you

1141
01:17:26,570 --> 01:17:27,600
look above this

1142
01:17:28,970 --> 01:17:34,190
the way in which we estimate the unobserved latent variables is quite different from the

1143
01:17:34,190 --> 01:17:39,160
way we estimate the parameters right so one is just doing co co one is just computing the

1144
01:17:39,160 --> 01:17:43,620
conditional distribution while the other one is trying to do maximum likelihood and always I

1145
01:17:43,620 --> 01:17:47,980
always find that a bit weir strange because they should basically be the same process

1146
01:17:47,980 --> 01:17:52,310
is both of them are about the process of inferring well we did not

1147
01:17:52,310 --> 01:17:54,720
observe from what we did observe okay

1148
01:17:54,840 --> 01:18:00,860
and basically the Bayesian modeling approach

1149
01:18:01,080 --> 01:18:06,060
does that does both of these both inference and learning in exactly the

1150
01:18:06,060 --> 01:18:11,580
same way and to do that of course you you will need a prior distribution

1151
01:18:11,580 --> 01:18:19,110
over your parameters so this is our prior distribution over parameters so that we can estimate both

1152
01:18:19,110 --> 01:18:22,530
the unobserved latent variables and the unobserved parameters

1153
01:18:22,630 --> 01:18:28,630
in the same process which is simply to compute the posterior distribution over

1154
01:18:28,630 --> 01:18:33,460
the latent variables and the parameters together given the observed data okay

1155
01:18:33,490 --> 01:18:37,480
so the posterior distribution here is gonna be again given by base rule

1156
01:18:37,940 --> 01:18:40,080
which is now gonna be

1157
01:18:40,150 --> 01:18:44,480
a ratio between the joint's distribution over the all the

1158
01:18:44,490 --> 01:18:51,370
random variables whether the observed or unobserved and also with the parameters okay

1159
01:18:51,790 --> 01:18:54,720
and then of course this

1160
01:18:54,790 --> 01:19:00,340
in the denominator here we'll have the marginal distribution of simply the observed variables where

1161
01:19:00,340 --> 01:19:04,910
we have marginalized out both the parameters and the latent variables

1162
01:19:04,960 --> 01:19:11,560
again we can do things like prediction and classification and so forth so prediction would correspond

1163
01:19:11,560 --> 01:19:16,060
to computing the conditional distribution of a new test item

1164
01:19:16,080 --> 01:19:22,220
given your training sets x one to x n and this would be of course

1165
01:19:22,220 --> 01:19:23,130
just be

1166
01:19:23,170 --> 01:19:30,370
basically integral of the conditional probability of the test item given the parameters where we've integrated other

1167
01:19:30,370 --> 01:19:36,410
parameters with respect to its posterior distribution okay so this is just

1168
01:19:37,390 --> 01:19:41,790
follows from the from probability theory

1169
01:19:42,180 --> 01:19:47,060
and classification would be half similar so we'd like to

1170
01:19:47,100 --> 01:19:49,240
we can compute the

1171
01:19:49,270 --> 01:19:56,720
conditional probability of the test items given the data set corresponding to the ceeth class where we

1172
01:19:56,720 --> 01:19:58,460
have again marginalized out

1173
01:19:58,490 --> 01:20:01,080
the parameters corresponding to the ceeth class

1174
01:20:02,480 --> 01:20:09,670
so one thing to note is that this prior distribustion is is

1175
01:20:09,670 --> 01:20:15,860
is very useful because it allows us to regularize our learning so I guess

1176
01:20:15,890 --> 01:20:20,740
you guess have all know that when you do maximum likelihood

1177
01:20:20,770 --> 01:20:25,860
learning then you have to be careful not to overfit to your data right and one way

1178
01:20:25,860 --> 01:20:30,970
which you could address all the fitting is by regularizing your parameters and one way

1179
01:20:31,130 --> 01:20:35,720
which could think about this prior distribution here is basically as a way of regu

1180
01:20:35,720 --> 01:20:38,120
of regularizing your parameters okay

1181
01:20:39,200 --> 01:20:44,340
so basically if you look at the posterior distribution of the parameters given

1182
01:20:44,600 --> 01:20:46,030
our observed

1183
01:20:46,080 --> 01:20:52,250
data is gonna be a ratio between the joints between the observed data and the parameters

1184
01:20:52,250 --> 01:20:56,840
divided by the marginal and if you look at this joint's

1185
01:20:56,870 --> 01:20:59,240
distribution here it will be

1186
01:20:59,270 --> 01:21:04,580
high if the particle asset on the parameters

1187
01:21:05,390 --> 01:21:10,810
gives high probability of to the observed data and has high probability under the prior okay

1188
01:21:11,080 --> 01:21:16,650
and basically we can think of this product here as a tradeoff in finding a

1189
01:21:16,650 --> 01:21:20,220
good parameter settings that fits both the prior well

1190
01:21:20,250 --> 01:21:22,060
as well as the data well

1191
01:21:25,410 --> 01:21:31,750
something else which I like to say is that in the Bayesian way

1192
01:21:31,750 --> 01:21:37,790
of looking at things there's really no difference between inference and learning okay so both of

1193
01:21:37,790 --> 01:21:39,160
I have the first one

1194
01:21:44,200 --> 01:21:51,850
microphones please

1195
01:21:51,890 --> 01:21:53,830
you need the microphone

1196
01:21:55,870 --> 01:21:56,830
rudi rizman

1197
01:22:01,310 --> 01:22:08,250
take your microphone please closer you again proved that power of

1198
01:22:08,250 --> 01:22:16,910
interdisciplinary and transdisciplinary analysis and interpretation of the chosen topic so I

1199
01:22:16,910 --> 01:22:24,930
take the privilege to ask you whether you would encourage the aesthetical interpretation of

1200
01:22:24,930 --> 01:22:36,700
ugliness and beautiful on other spheres of social I'm here referring having in mind especially to use

1201
01:22:37,620 --> 01:22:44,770
kind of analytical differentiation to the fields of the history of ideas can we speak

1202
01:22:45,010 --> 01:22:55,660
also about ugly and beautiful ideas ugly and beautiful ideologies ugly and beautiful politics

1203
01:22:55,660 --> 01:23:06,970
if there is anything like beautiful politics at all if this question makes any sense I would be grateful for your brief comment thank you

1204
01:23:06,970 --> 01:23:13,010
I hate metaphors

1205
01:23:13,040 --> 01:23:20,270
except except in poetry except in poetry so it's usual normal we

1206
01:23:20,270 --> 01:23:22,120
speak of

1207
01:23:22,140 --> 01:23:27,660
beautiful moral behaviour beautiful idea but it is a metaphor is a metaphor

1208
01:23:27,660 --> 01:23:35,770
except we speak of the beauty and of the elegance of the mathematical demonstration

1209
01:23:35,770 --> 01:23:41,830
in that case we are a really aesthetically speaking of a

1210
01:23:41,830 --> 01:23:47,060
of a balance of a proportion but to say you have a beautiful idea

1211
01:23:47,060 --> 01:23:52,450
is like to say we made a beautiful love yesterday and I want to

1212
01:23:52,450 --> 01:24:03,020
have then a beautiful scotch to finish the experience prosm

1213
01:24:03,020 --> 01:24:15,950
Ivan ti ja tukaj prosm mogoče bi mel pa kej več mikrofonov pa bi šlo lažje

1214
01:24:17,610 --> 01:24:24,950
my name is Ivan Svetlik and I must say doctor Eco I've enjoyed your talk

1215
01:24:24,990 --> 01:24:31,410
very much but it seems to me that from the beginning where it was funny it turned to a

1216
01:24:31,490 --> 01:24:34,040
very serious conclusion at the end

1217
01:24:37,330 --> 01:24:42,290
my friend Rudi took it too far from your mouth and you're taking it

1218
01:24:42,830 --> 01:24:45,140
too close to your mouth

1219
01:24:45,160 --> 01:24:52,220
make it a beautiful level sorry what is

1220
01:24:52,470 --> 01:25:04,490
it okay sorry I wanted to say that I enjoyed you talk

1221
01:25:04,490 --> 01:25:10,720
very much but it seemed to me that it turned from a rather funny beginning to a very serious

1222
01:25:12,350 --> 01:25:16,400
by which I mean that this disappearing

1223
01:25:16,560 --> 01:25:24,990
distinction between beauty and ugliness makes us thinking about what we were

1224
01:25:24,990 --> 01:25:30,220
going to and I would like to ask you could you link or interpret

1225
01:25:30,220 --> 01:25:39,180
this disappearing difference from the perspective of raising individualism in the society because individualism

1226
01:25:39,180 --> 01:25:41,540
has become so

1227
01:25:41,600 --> 01:25:46,430
high value that everybody has the right to be different and

1228
01:25:46,450 --> 01:25:48,850
and therefore perhaps

1229
01:25:48,850 --> 01:25:53,310
there is a natural conclusion that ugliness has become very much acceptable

1230
01:25:56,970 --> 01:26:02,930
I quoted this idea that the the distinction

1231
01:26:03,510 --> 01:26:08,240
beauty ugliness is disappearing as an idea which has been

1232
01:26:08,290 --> 01:26:14,930
put forth there are many essays on the subject but I think it concerns only

1233
01:26:14,970 --> 01:26:21,740
contemporary art because in the rest of our life think of designs

1234
01:26:21,790 --> 01:26:23,970
design the form of a car

1235
01:26:24,010 --> 01:26:25,720
of a TV

1236
01:26:25,750 --> 01:26:32,490
set of dishes we are still following notions of of beauty

1237
01:26:32,520 --> 01:26:38,080
which are the classical ones we want a beautiful car very well designed with harmonious

1238
01:26:38,080 --> 01:26:45,080
shapes so so we have still an idea pretty a pretty traditional

1239
01:26:45,080 --> 01:26:52,270
idea of of beauty in the form of a tie jacket and so easy and

1240
01:26:52,270 --> 01:27:00,430
contemporary art the distinction has been cancelled because I repeat art is no more interested in beauty

1241
01:27:00,430 --> 01:27:06,700
but in something else that these can express a form of individualism and the

1242
01:27:07,330 --> 01:27:14,770
decision to be original at any cost that's true that's true and that's probably the reason

1243
01:27:14,770 --> 01:27:19,910
why many aspects of contemporary art are divorsed from

1244
01:27:19,990 --> 01:27:26,510
the society in which they live that they are still alive only in the circuit of

1245
01:27:26,510 --> 01:27:30,850
art galleries and few collectors that's and they don't

1246
01:27:31,040 --> 01:27:37,060
do not realize that on the contrary people is watching cinema television

1247
01:27:37,080 --> 01:27:41,620
where you can have the triumph as another form of ugliness

1248
01:27:42,370 --> 01:27:48,220
to each chapter of my book is I have not which is kitch

1249
01:27:48,220 --> 01:27:54,310
the fake beauty but that's another story as kipling would have said

1250
01:27:58,790 --> 01:27:59,750
thank you very much

1251
01:28:00,870 --> 01:28:04,410
še kakšno vprašanje ja prosm

1252
01:28:24,180 --> 01:28:33,700
as an Italian and expert on beauty and ugliness what do you think about darfur which is very close

1253
01:28:33,700 --> 01:28:42,020
to us now just accross the mediterranean sea and a desert but I've spoken about darfur

1254
01:28:42,100 --> 01:28:48,990
when I said that we are continually continuosly confronted by newspaper or TV

1255
01:28:48,990 --> 01:28:52,780
with that OK that that connects to

1256
01:28:53,340 --> 01:28:55,790
later parts of the course and a beautiful way

1257
01:28:55,820 --> 01:29:01,670
shows our formula shows this formula from from later on to be activities which will

1258
01:29:01,670 --> 01:29:07,450
come in March uh to be actually very useful for this puzzle that we never

1259
01:29:07,450 --> 01:29:08,450
really result

1260
01:29:11,080 --> 01:29:12,220
the nonlinear turned

1261
01:29:13,310 --> 01:29:17,450
a direct attack on this inverse

1262
01:29:20,200 --> 01:29:21,490
we can get it directly

1263
01:29:22,190 --> 01:29:28,400
and getting it directly will be cut will connect to our differential equations so I'm

1264
01:29:28,400 --> 01:29:31,630
I'm proposing to a racist

1265
01:29:31,640 --> 01:29:39,510
if nobody objects I'll erase this this rank 1 and this wonderful formula because because

1266
01:29:39,560 --> 01:29:45,320
you will see that was see it again and I'll just mention it can to

1267
01:29:45,320 --> 01:29:52,230
erase it totally out of the way that didn't 1 1 application of what 1

1268
01:29:52,300 --> 01:29:57,120
because I spoke about the squares there yet so let me on complete that thought

1269
01:29:57,120 --> 01:29:58,190
before I give up

1270
01:30:02,210 --> 01:30:05,640
was I'm trying to find the position of the

1271
01:30:07,600 --> 01:30:13,770
and people weren't given that show I suppose I'm trying to solve this least-squares problem

1272
01:30:13,890 --> 01:30:15,580
you equals

1273
01:30:15,600 --> 01:30:17,580
some measurements

1274
01:30:17,710 --> 01:30:25,020
OK what's my I a is going to be false discovery rectangular and it's going

1275
01:30:25,040 --> 01:30:27,010
to be a matrix with ones

1276
01:30:27,790 --> 01:30:30,750
minus 1 1

1277
01:30:30,860 --> 01:30:38,860
minus 1 1 n minus 1 1 so it also so far

1278
01:30:38,900 --> 01:30:41,230
as a final guys

1279
01:30:41,230 --> 01:30:41,900
minus 1

1280
01:30:43,070 --> 01:30:52,280
that I'm looking for the positions all of this

1281
01:30:52,450 --> 01:30:54,070
but say

1282
01:30:54,210 --> 01:31:04,490
satellites and I measure the want the 2 p 3 before and the 5 OK

1283
01:31:04,490 --> 01:31:08,490
what the what's up here

1284
01:31:08,870 --> 01:31:10,580
it's not

1285
01:31:10,600 --> 01:31:12,950
that it shouldn't have

1286
01:31:13,990 --> 01:31:15,880
for hope for

1287
01:31:15,900 --> 01:31:22,550
1 2 3 4 yeah good effect of the credit so that it no need

1288
01:31:22,550 --> 01:31:28,190
to say problem in words and trying to this this this

1289
01:31:31,300 --> 01:31:34,380
the a it's moving along

1290
01:31:34,770 --> 01:31:37,710
so I 1st measure its 1st positions

1291
01:31:37,710 --> 01:31:42,560
make a measurement you want will be 1 is my 1st equation

1292
01:31:45,980 --> 01:31:50,810
OK so know approximately where it is

1293
01:31:50,820 --> 01:31:57,250
now the 2nd equation is due to the minus you 1 is some B 2

1294
01:31:57,890 --> 01:32:03,810
so 1 of my I 1 of my measure there

1295
01:32:03,880 --> 01:32:08,630
I measured the difference how much it moved right so I didn't know exactly where

1296
01:32:10,190 --> 01:32:13,080
but the 2nd measurements

1297
01:32:13,100 --> 01:32:17,530
told me something about where it is now

1298
01:32:18,620 --> 01:32:23,620
by measuring the relative error which of course everybody can think of applications where that

1299
01:32:23,620 --> 01:32:27,840
would be the case and then the next the 3rd equation would be a measurement

1300
01:32:27,840 --> 01:32:36,860
of the next 1 and plus and the next and now pretty well no

1301
01:32:37,770 --> 01:32:41,170
I know something about you for

1302
01:32:41,210 --> 01:32:44,950
because it's approximate so what is you for is approximately equal

1303
01:32:45,880 --> 01:32:49,950
I guess if I just had those for measurements what would be my best guess

1304
01:32:49,950 --> 01:32:52,270
for you for

1305
01:32:53,780 --> 01:32:55,470
the right

1306
01:32:55,470 --> 01:32:59,160
if I wanted to know you 4 out of the 4 measurements if I add

1307
01:32:59,160 --> 01:33:04,140
those equations that would be give me you for equal one plus 2 + 3

1308
01:33:04,230 --> 01:33:08,490
4 plus errors and that's like the best I could do that would be the

1309
01:33:08,490 --> 01:33:16,970
best least-squares now right now only used for OK and that's exactly so that exactly

1310
01:33:16,970 --> 01:33:19,530
corresponds at this matrix T

1311
01:33:19,930 --> 01:33:25,690
if the albeit somewhat had member what to do with least squares

1312
01:33:25,710 --> 01:33:29,770
but had a lower what's the equations so if I have a rectangular system as

1313
01:33:29,770 --> 01:33:33,710
I do here there's not going to be a solution so I'm looking for you

1314
01:33:33,720 --> 01:33:37,830
approximately is close to be as I can get it and what to do what

1315
01:33:37,830 --> 01:33:39,580
equation to ourselves

1316
01:33:39,600 --> 01:33:41,810
that A transpose the right

1317
01:33:42,450 --> 01:33:47,490
so so so that leads me to believe a transfer of points that for the

1318
01:33:47,490 --> 01:33:48,650
best you

1319
01:33:48,750 --> 01:33:55,140
is a transpose believed that the equation actually solve now the 1 I had only

1320
01:33:55,140 --> 01:33:56,950
when it was 4 by 4

1321
01:33:59,190 --> 01:34:04,430
that I had 4 equations for announced actually matrix was completely invertible I could solve

1322
01:34:04,430 --> 01:34:08,340
it directly you you gave me the best answer you for it is the sum

1323
01:34:08,340 --> 01:34:09,300
of the

1324
01:34:10,130 --> 01:34:10,730
but now

1325
01:34:11,270 --> 01:34:13,270
I get a 5th measurement

1326
01:34:14,250 --> 01:34:16,810
and NFS measurement is

1327
01:34:16,880 --> 01:34:22,270
minus you for equals some someplace B 5

1328
01:34:23,750 --> 01:34:29,190
but basically it's a it's a measurement of plus there is a measurement of where

1329
01:34:29,190 --> 01:34:32,230
the final guy ideas but it's got errors

1330
01:34:33,290 --> 01:34:36,370
now what's the best estimate for you for

1331
01:34:36,380 --> 01:34:37,080
let me ask you

1332
01:34:38,510 --> 01:34:43,810
what's the best measurement for the final position the best estimate of the final position

1333
01:34:43,810 --> 01:34:48,550
you for based on 5 data

1334
01:34:49,310 --> 01:34:53,450
how you might say OK the last measurement

1335
01:34:53,470 --> 01:34:56,210
was measuring you for exactly

1336
01:34:56,230 --> 01:34:59,540
could do better than that but you can't

1337
01:34:59,560 --> 01:35:04,320
because you had information previous and you're not gonna throw them away

1338
01:35:04,400 --> 01:35:07,170
right there there's error in this measurement

1339
01:35:07,170 --> 01:35:08,320
so it's not exact

1340
01:35:08,810 --> 01:35:16,630
and there and measurements possibly greater because it was like like compounded but still information

1341
01:35:16,630 --> 01:35:17,710
is in there

1342
01:35:17,710 --> 01:35:24,320
goes and then we inserted in their place and that's why it's called insertion sort

1343
01:35:24,360 --> 01:35:28,180
so we just sort of move the things copy the things up and we find

1344
01:35:28,180 --> 01:35:29,720
where it goes

1345
01:35:29,720 --> 01:35:32,490
and then we put it into place

1346
01:35:32,550 --> 01:35:33,970
and then we

1347
01:35:33,970 --> 01:35:35,570
now we have that

1348
01:35:35,590 --> 01:35:38,680
from a from one j is sorted and now we

1349
01:35:38,760 --> 01:35:41,700
tend work on j plus one

1350
01:35:42,720 --> 01:35:46,470
so let's give an example of that

1351
01:35:47,320 --> 01:35:51,010
imagine we're doing eight two four

1352
01:35:51,030 --> 01:35:53,410
nine three six

1353
01:35:53,410 --> 01:35:55,260
so we started out

1354
01:35:55,280 --> 01:35:56,110
with two

1355
01:35:56,110 --> 01:35:57,820
j jake two

1356
01:35:58,880 --> 01:36:03,380
we figure out that we want to insert their so now we have two eight

1357
01:36:03,380 --> 01:36:07,200
four nine three six

1358
01:36:07,530 --> 01:36:10,180
then we look at the four

1359
01:36:10,200 --> 01:36:12,800
and we say all that goes over here

1360
01:36:12,970 --> 01:36:14,880
two four eight

1361
01:36:14,930 --> 01:36:17,490
nine three six

1362
01:36:17,510 --> 01:36:21,550
after the second iteration of the outer loop

1363
01:36:21,570 --> 01:36:24,130
then we look at nine

1364
01:36:24,150 --> 01:36:26,800
we discover immediately it just goes right there

1365
01:36:26,820 --> 01:36:29,430
very little work to do on that step

1366
01:36:30,380 --> 01:36:34,220
so we have exactly the same thing output

1367
01:36:34,220 --> 01:36:35,930
after that iteration

1368
01:36:35,950 --> 01:36:37,680
when we look at the three

1369
01:36:37,700 --> 01:36:40,490
that's going to be inserted over there

1370
01:36:40,510 --> 01:36:41,820
two three

1371
01:36:41,880 --> 01:36:43,300
four eight

1372
01:36:43,300 --> 01:36:44,800
nine six

1373
01:36:44,800 --> 01:36:46,550
finally six

1374
01:36:46,610 --> 01:36:48,840
that goes on there

1375
01:36:48,840 --> 01:36:52,660
two three four five six eight nine

1376
01:36:52,660 --> 01:36:55,800
at that point we're done

1377
01:37:10,700 --> 01:37:13,380
there is this year starts one yes

1378
01:37:14,090 --> 01:37:16,030
a one day

1379
01:37:21,360 --> 01:37:23,780
so this is the insertion sort algorithm

1380
01:37:23,800 --> 01:37:27,320
and it's the first album algorithm that we're going to analyse

1381
01:37:27,340 --> 01:37:31,700
OK we pick out plots and tools that we have from our math background to

1382
01:37:31,700 --> 01:37:35,570
help to analyse so first of all it took take a look at the issue

1383
01:37:36,380 --> 01:37:37,890
running time

1384
01:37:41,220 --> 01:37:47,720
so the running time depends of this algorithm depends on a lot of things

1385
01:37:49,260 --> 01:37:53,130
so for one thing it depends on is the input itself

1386
01:37:59,360 --> 01:38:03,390
OK so for example if the input is already sorted

1387
01:38:08,970 --> 01:38:13,530
insertion sort has very little work to do

1388
01:38:13,530 --> 01:38:17,110
OK because every time it's going to be like this case

1389
01:38:17,130 --> 01:38:22,180
and have to shuffle too many guys over because there are already in place

1390
01:38:24,070 --> 01:38:28,240
in some sense what's the worst case for insertion sort

1391
01:38:28,340 --> 01:38:31,160
yeah its reverse sorted

1392
01:38:31,180 --> 01:38:33,450
that is going to do a lot of work

1393
01:38:33,530 --> 01:38:36,930
it's not the shuffle everything over on each step

1394
01:38:36,990 --> 01:38:39,880
OK of the outer loop

1395
01:38:39,910 --> 01:38:41,240
in addition

1396
01:38:41,260 --> 01:38:45,950
to the actual input it depends of course on the input size

1397
01:38:45,970 --> 01:38:53,630
so here for example we did six elements

1398
01:38:53,650 --> 01:38:55,240
it's going to take longer

1399
01:38:55,260 --> 01:39:00,220
if we for example do six times ten to the ninth elements

1400
01:39:05,280 --> 01:39:07,860
so we're sorting a lot more stuff

1401
01:39:07,880 --> 01:39:10,550
can take us a lot longer

1402
01:39:10,610 --> 01:39:16,030
so typically the way we handle that is we're going to parameterize

1403
01:39:19,090 --> 01:39:20,970
things in the input size

1404
01:39:21,010 --> 01:39:24,150
so there

1405
01:39:24,160 --> 01:39:28,430
talk about time as a function of the size of things that we are

1406
01:39:29,970 --> 01:39:31,760
so we can look at what

1407
01:39:31,760 --> 01:39:34,200
is the behaviour of that

1408
01:39:34,740 --> 01:39:39,800
and the last thing i want to say about running time is generally we want

1409
01:39:39,950 --> 01:39:43,510
upper bounds on the running time

1410
01:39:44,660 --> 01:39:45,800
we want to know

1411
01:39:45,820 --> 01:39:49,430
the time is no more than a certain amount

1412
01:39:49,450 --> 01:39:54,630
and the reason is because that represents a guarantee to the user

1413
01:39:55,490 --> 01:39:58,340
so if i say it's not going to run for example if i tell you

1414
01:39:58,340 --> 01:40:02,430
here's the program in run run more than three seconds

1415
01:40:02,450 --> 01:40:06,410
OK that gives you really information about how you can use it for example in

1416
01:40:06,410 --> 01:40:09,840
real time setting

1417
01:40:10,530 --> 01:40:12,630
was by said here's the program

1418
01:40:15,090 --> 01:40:16,070
you know

1419
01:40:16,090 --> 01:40:19,110
goes these three seconds you don't know if it's going to go

1420
01:40:19,110 --> 01:40:19,880
you know

1421
01:40:19,910 --> 01:40:21,470
for three years

1422
01:40:21,510 --> 01:40:23,680
doesn't give you that much guarantee

1423
01:40:23,680 --> 01:40:29,010
if you're a user so gently we want upper bounds because it represents the guarantee

1424
01:40:29,030 --> 01:40:34,280
the user

1425
01:40:38,680 --> 01:40:46,760
there are different kinds of analyses that people do

1426
01:40:46,860 --> 01:41:01,550
the one are mostly going to focus on is what's called worst case

1427
01:41:04,450 --> 01:41:06,490
OK this is what we do

1428
01:41:08,030 --> 01:41:12,990
where we on youtube

1429
01:41:13,010 --> 01:41:15,510
to be the maximum time

1430
01:41:17,470 --> 01:41:19,280
on any

1431
01:41:21,390 --> 01:41:27,180
of size n

1432
01:41:28,160 --> 01:41:33,180
this the maximum input the maximum could possibly cost us on an input of size

1433
01:41:34,650 --> 01:41:38,930
OK so what that does is if you look at the fact that sometimes the

1434
01:41:38,930 --> 01:41:44,070
inputs are better and sometimes there were we're looking at the worst case of those

1435
01:41:44,070 --> 01:41:47,610
because that's the way we're going to be able to make a guarantee it always

1436
01:41:47,610 --> 01:41:48,820
does something

1437
01:41:48,970 --> 01:41:51,320
rather than just sometimes there's something

1438
01:41:52,340 --> 01:41:56,990
look at the maximum notice if i didn't have maximum t event in some sense

1439
01:41:57,010 --> 01:42:01,110
it is a relation not to function

1440
01:42:01,110 --> 01:42:04,430
because the time an input of size n

1441
01:42:04,450 --> 01:42:07,530
but depends upon which input of size n

1442
01:42:07,550 --> 01:42:09,860
i could have many different times

1443
01:42:09,860 --> 01:42:14,780
but by putting the maximum added it turns that relation into a function because there's

1444
01:42:14,780 --> 01:42:16,820
only one maximum time

1445
01:42:16,840 --> 01:42:18,490
it will take

1446
01:42:20,660 --> 01:42:24,280
sometimes we'll talk about average case

1447
01:42:27,050 --> 01:42:30,050
sometimes we do this

1448
01:42:30,160 --> 01:42:37,320
here two and is then the expected time

1449
01:42:37,390 --> 01:42:44,320
overall class

1450
01:42:44,340 --> 01:42:49,160
size n

1451
01:42:49,160 --> 01:42:58,880
do we have a criterion good criterion what to learn they can we can specify

1452
01:42:58,880 --> 01:43:01,230
convex cost function

1453
01:43:01,250 --> 01:43:07,810
and they decided to tool use as it is convex function they

1454
01:43:09,830 --> 01:43:11,220
the other e

1455
01:43:11,220 --> 01:43:16,430
for that if they have just believe that they ask them they want to find

1456
01:43:16,790 --> 01:43:20,170
an embedding of the the data that i

1457
01:43:21,190 --> 01:43:23,220
maximal margin

1458
01:43:23,230 --> 01:43:24,770
the block there

1459
01:43:24,790 --> 01:43:25,940
did it

1460
01:43:25,940 --> 01:43:29,390
they sort of with a kinematic because

1461
01:43:29,420 --> 01:43:31,970
and they have these optimisation problem

1462
01:43:31,980 --> 01:43:34,280
that is the min max problem so

1463
01:43:34,330 --> 01:43:38,910
i mean you should minimize with respect to the gamma function and maximize with respect

1464
01:43:38,930 --> 01:43:46,380
to this problem interesting from the solution is the only one that can be computed

1465
01:43:46,420 --> 01:43:49,410
by rearranging the problem opportunity

1466
01:43:50,250 --> 01:43:52,120
semidefinite programming

1467
01:43:52,160 --> 01:43:56,030
OK you can find the passage on paper

1468
01:43:56,050 --> 01:43:57,790
i just to give you

1469
01:43:57,800 --> 01:44:01,140
the idea

1470
01:44:01,200 --> 01:44:07,980
they have right of the problem and they want to see the labelling data

1471
01:44:08,000 --> 01:44:11,770
so i find troubling me the one on the right side

1472
01:44:12,370 --> 01:44:18,140
what do they do is if you if you if you look at the matter

1473
01:44:18,160 --> 01:44:22,250
is k i t that means that

1474
01:44:22,250 --> 01:44:26,560
they maximize the margin only on training data

1475
01:44:26,560 --> 01:44:29,600
and but use to the unlabelled data

1476
01:44:29,620 --> 01:44:32,240
and let's do the text blocks o

1477
01:44:32,250 --> 01:44:36,200
they're not the math explored and the that the rest

1478
01:44:36,200 --> 01:44:37,890
and at the same time

1479
01:44:38,790 --> 01:44:41,120
was that cannot function

1480
01:44:43,000 --> 01:44:47,950
two twenty ton capacity control as shown before what i average

1481
01:44:48,000 --> 01:44:49,600
is that this

1482
01:44:49,600 --> 01:44:52,770
o effect is coming from

1483
01:44:52,810 --> 01:44:54,270
combination of four

1484
01:44:54,270 --> 01:44:58,390
some other mixes is

1485
01:44:58,930 --> 01:45:02,390
they work by land has been

1486
01:45:02,390 --> 01:45:04,910
told pioneering work very nice

1487
01:45:04,930 --> 01:45:11,700
the problem is that doesn't scale much soul from

1488
01:45:11,700 --> 01:45:13,450
that time a lot of

1489
01:45:13,470 --> 01:45:14,890
other people

1490
01:45:14,910 --> 01:45:19,180
try to develop new approaches for can combination

1491
01:45:19,200 --> 01:45:24,950
the scalar bit more and for example you might like the work by about two

1492
01:45:24,950 --> 01:45:26,000
thousand four

1493
01:45:26,020 --> 01:45:30,600
would like to remind you think it's quite nice work on the field

1494
01:45:30,620 --> 01:45:34,140
they restrict their attention to

1495
01:45:34,180 --> 01:45:40,100
chemical munitions combination kind of that for the web

1496
01:45:40,100 --> 01:45:41,410
some of them all

1497
01:45:41,450 --> 01:45:48,220
you see what why and more than zero so this is the kind of

1498
01:45:48,270 --> 01:45:52,640
they wanted or convex combination of camel of the time

1499
01:45:52,640 --> 01:45:53,790
eighty five

1500
01:45:53,810 --> 01:45:58,270
you can even they're more likely migration

1501
01:45:58,270 --> 01:46:00,830
but of course you have to many combination

1502
01:46:00,830 --> 01:46:02,540
so two

1503
01:46:02,580 --> 01:46:08,930
the design and cost function that you can optimize to find the same time the

1504
01:46:08,930 --> 01:46:11,790
solution of all support vector machines and they

1505
01:46:12,350 --> 01:46:16,790
i mean that that was you the best can combination

1506
01:46:16,790 --> 01:46:21,980
this is quite nice formulation of the problem in the case

1507
01:46:22,000 --> 01:46:23,180
these numbers

1508
01:46:23,370 --> 01:46:26,970
the problem is jointly convex in in w more

1509
01:46:26,980 --> 01:46:32,240
and so that there is a unique global minimum

1510
01:46:32,240 --> 01:46:38,480
find nice to be able to apply his approach and discussing in general both can

1511
01:46:38,500 --> 01:46:41,850
communication on CV

1512
01:46:41,870 --> 01:46:49,660
and they the problem of classifying flowers and flowers can be represented by many features

1513
01:46:49,660 --> 01:46:51,600
like color texture

1514
01:46:51,660 --> 01:46:56,160
and the tried several features from computer vision system that one

1515
01:46:56,160 --> 01:47:01,310
and you see that the right side you see that get quite good results for

1516
01:47:01,310 --> 01:47:03,430
example using SIFT

1517
01:47:03,450 --> 01:47:04,810
and then

1518
01:47:04,830 --> 01:47:09,970
now we can see what do we get if you do also simple cannot combination

1519
01:47:09,970 --> 01:47:13,640
as productive can for example or other

1520
01:47:13,660 --> 01:47:16,370
some of can averaging if you want

1521
01:47:16,390 --> 01:47:17,470
you get

1522
01:47:17,520 --> 01:47:22,740
fifteen percent more than using a single feature

1523
01:47:22,750 --> 01:47:24,390
then the

1524
01:47:24,410 --> 01:47:26,970
apply it can lead to the

1525
01:47:26,980 --> 01:47:30,370
shortly before in the collection

1526
01:47:30,390 --> 01:47:36,140
and this what they get the same performance more or less but the difference is

1527
01:47:36,140 --> 01:47:39,890
that the time to compute the it's much

1528
01:47:39,950 --> 01:47:40,720
to compute

1529
01:47:40,890 --> 01:47:45,850
additional problem is one and fifty two seconds

1530
01:47:45,850 --> 01:47:51,500
that is that if we use simply product of the can is to say to

1531
01:47:52,370 --> 01:47:53,890
so i

1532
01:47:56,500 --> 01:48:01,450
i want that you can combination is not useful

1533
01:48:01,480 --> 01:48:06,310
not need what happened in the following they do some clever stuff and it also

1534
01:48:06,310 --> 01:48:11,140
proposed approach to the to the problem of chemical combination

1535
01:48:12,450 --> 01:48:17,850
right to the conclusion that if all the future the two you get good

1536
01:48:17,870 --> 01:48:20,890
you don't need to be to to select the uganda

1537
01:48:20,910 --> 01:48:24,000
put them together like simply summing the can

1538
01:48:24,000 --> 01:48:29,200
making the product but if some features are not going to be able to detect

1539
01:48:29,200 --> 01:48:31,720
these ultimately automatically then you need

1540
01:48:31,720 --> 01:48:34,600
the use of multiple kernel learning

1541
01:48:34,600 --> 01:48:36,910
and this is some of what is shown

1542
01:48:36,930 --> 01:48:38,970
in the plot the

1543
01:48:38,980 --> 01:48:40,370
from the paper

1544
01:48:40,480 --> 01:48:43,250
from the very it's like well

1545
01:48:43,270 --> 01:48:47,080
when you have some nice to the features of the features

1546
01:48:47,100 --> 01:48:49,890
that was some discriminative by

1547
01:48:49,910 --> 01:48:51,060
this marks

1548
01:48:51,060 --> 01:48:55,850
more robust to use can learning that to automatically

1549
01:48:55,870 --> 01:49:03,350
so the two combined can simply by doctor or by averaging

1550
01:49:03,350 --> 01:49:05,520
i think is what was acquired

1551
01:49:05,580 --> 01:49:07,830
interesting what from

1552
01:49:07,850 --> 01:49:11,310
practical and theoretical point of view

1553
01:49:11,310 --> 01:49:13,720
i'll summarize bit

1554
01:49:14,430 --> 01:49:17,540
so support vector machine system

1555
01:49:17,540 --> 01:49:20,470
OK so one of the so i talked about the fact that

1556
01:49:20,480 --> 01:49:25,620
noise hurts adaboost but we can actually use that fact one advantage

1557
01:49:25,640 --> 01:49:30,040
and use adaboost as a way of finding outliers so it turns out that the

1558
01:49:30,040 --> 01:49:32,490
examples that have the most weight

1559
01:49:32,500 --> 01:49:36,040
at the end of running adaboost are likely to be

1560
01:49:36,060 --> 01:49:39,630
outliers in the sense that they are likely to be they're mislabeled

1561
01:49:39,650 --> 01:49:40,800
or just

1562
01:49:40,810 --> 01:49:43,360
inherently ambiguous

1563
01:49:44,210 --> 01:49:48,120
on this particular dataset these are some of the examples that have the highest weight

1564
01:49:48,120 --> 01:49:49,570
on the last round

1565
01:49:49,600 --> 01:49:53,110
so the first one says i'm trying to make a credit card called that to

1566
01:49:53,130 --> 01:49:54,790
be calling card

1567
01:49:54,840 --> 01:50:00,050
but it's actually labelled in the datasets clocked which is just clearly mistake

1568
01:50:00,060 --> 01:50:01,930
this one person said hello

1569
01:50:01,930 --> 01:50:04,250
that was the entire thing the person said

1570
01:50:04,260 --> 01:50:05,490
that was labelled

1571
01:50:05,510 --> 01:50:08,120
as a request for rate information

1572
01:50:08,560 --> 01:50:13,260
this one yes i'd like to make a long distance collect call please clearly should

1573
01:50:13,760 --> 01:50:18,220
labelled the collect call instead it was labelled calling card so this can be used

1574
01:50:18,220 --> 01:50:23,260
as a tool for instance to clean up your data sets

1575
01:50:23,360 --> 01:50:29,300
OK here's another application also to a similar kind of spoken dialogue task

1576
01:50:31,670 --> 01:50:34,230
this is also work i did when i was at eighteen t

1577
01:50:34,240 --> 01:50:40,020
so at eighteen two labs they try to start a business called the natural voices

1578
01:50:40,020 --> 01:50:41,840
business which would sell

1579
01:50:41,870 --> 01:50:44,500
very good text to speech

1580
01:50:44,510 --> 01:50:53,780
engine so take text and were converted to very realistic sounding synthesized speech and this

1581
01:50:53,780 --> 01:50:57,850
actually has nothing to do with that in particular but but they didn't want to

1582
01:50:57,850 --> 01:51:01,840
do is to set up a kind of automatic storefront your help desk so people

1583
01:51:01,840 --> 01:51:06,600
could call up and ask questions about this product you know they could ask for

1584
01:51:06,600 --> 01:51:11,760
demo they transfer pricing information technical support and so on

1585
01:51:11,810 --> 01:51:16,290
and again they didn't one person to have to go through a menu pushing one

1586
01:51:16,290 --> 01:51:17,850
pushing two and so on

1587
01:51:17,870 --> 01:51:22,630
i just wanted the person to be able to have interactive dialogue with the system

1588
01:51:24,010 --> 01:51:29,280
i can give you a demo what that sounds like so you can hear two

1589
01:51:30,330 --> 01:51:31,930
the man's voice

1590
01:51:31,940 --> 01:51:34,080
is a person calling

1591
01:51:34,100 --> 01:51:41,020
to ask about this the woman's voice is the synthesized voice

1592
01:51:41,060 --> 01:51:42,520
the name was crystal

1593
01:51:42,520 --> 01:51:49,050
this came amazingly her name they bore hole personality this person this made person anyway

1594
01:51:49,750 --> 01:51:51,740
man is talking to crystal

1595
01:51:52,780 --> 01:51:56,400
the thing i'm going to focus on is that they're having this interactive dialogue in

1596
01:51:56,420 --> 01:51:59,270
the computer is

1597
01:51:59,290 --> 01:52:01,430
carrying on the conversation in this

1598
01:52:01,430 --> 01:52:03,560
fairly intelligent life

1599
01:52:03,590 --> 01:52:05,790
now how can i help you

1600
01:52:05,830 --> 01:52:09,330
we like demonstration

1601
01:52:12,590 --> 01:52:15,370
sure i can help you with that

1602
01:52:15,400 --> 01:52:19,960
i condemn voices in english spanish korean war germany

1603
01:52:19,970 --> 01:52:24,180
or i can give you an idea of what custom going down like

1604
01:52:24,420 --> 01:52:26,120
which would you prefer

1605
01:52:26,180 --> 01:52:28,380
that's the voice

1606
01:52:28,440 --> 01:52:32,870
OK then constantly for you

1607
01:52:32,890 --> 01:52:35,080
one is likely to find

1608
01:52:35,090 --> 01:52:39,070
cartridge because the power to move his voice town

1609
01:52:39,090 --> 01:52:40,810
here it is

1610
01:52:40,840 --> 01:52:47,010
american finished prepared with the the with the prior to the people third when

1611
01:52:47,610 --> 01:52:50,120
i'd like to the customer voice again

1612
01:52:50,130 --> 01:52:55,070
can be no question answering

1613
01:52:55,130 --> 01:52:57,500
twenty five going on

1614
01:52:57,500 --> 01:53:01,000
called which has the power to move one town

1615
01:53:01,010 --> 01:53:09,270
the other thing is prepared to reason with an impressive sixty third when

1616
01:53:09,340 --> 01:53:14,060
could we have vanished when

1617
01:53:14,150 --> 01:53:17,850
OK band for you

1618
01:53:17,860 --> 01:53:21,490
it's not on the market right now but it will be the next language we

1619
01:53:21,500 --> 01:53:22,310
get out

1620
01:53:22,360 --> 01:53:24,940
female voice

1621
01:53:24,940 --> 01:53:25,770
it found

1622
01:53:26,060 --> 01:53:30,820
seventy percent of going that you don't have to go to the people

1623
01:53:31,630 --> 01:53:35,690
we custom again

1624
01:53:38,310 --> 01:53:39,820
so you get the

1625
01:53:39,840 --> 01:53:46,060
idea OK so how does something like this work

1626
01:53:46,080 --> 01:53:47,230
so here's

1627
01:53:47,230 --> 01:53:48,790
human up here

1628
01:53:48,820 --> 01:53:53,550
and the telephone caller telephone callers says something in

1629
01:53:53,560 --> 01:53:59,720
sound signal that raw utterance gets passed through an automatic speech recognition is the recognizer

1630
01:53:59,720 --> 01:54:05,650
the speech recogniser converts the sound signal the text three noisy text because speech recognizers

1631
01:54:06,510 --> 01:54:13,100
far from perfect that text gets passed to the natural language understanding unit which is

1632
01:54:13,100 --> 01:54:15,020
trying to extract

1633
01:54:15,040 --> 01:54:17,950
the meaning of what was said to the extent

1634
01:54:18,020 --> 01:54:23,480
but that meaning can be placed in one of this case twenty four categories

1635
01:54:23,530 --> 01:54:27,790
the categories are things like once you're demo wants to speak to sales reps and

1636
01:54:27,790 --> 01:54:29,130
so on

1637
01:54:29,150 --> 01:54:32,870
that predicted category gets past the dialogue manager

1638
01:54:32,890 --> 01:54:37,430
the dialogue manager is in charge of keeping track of the actual state of the

1639
01:54:38,530 --> 01:54:43,050
so given the current state given the predicted category

1640
01:54:43,110 --> 01:54:45,930
the dialogue manager updates the state

1641
01:54:45,940 --> 01:54:48,670
and formulates text response

1642
01:54:48,690 --> 01:54:51,560
which gets passed to the text-to-speech engine

1643
01:54:51,590 --> 01:54:54,350
this is crystal that's the thing that we're trying

1644
01:54:54,360 --> 01:54:56,730
we're trying to sell

1645
01:54:56,740 --> 01:54:59,640
and that gets converted to

1646
01:55:00,930 --> 01:55:05,520
which gets played for the human and it all has to happen in real time

1647
01:55:05,530 --> 01:55:10,220
so the part that i was working on is this natural language understanding you know

1648
01:55:10,230 --> 01:55:15,800
which is really just a classification task we're just trying to take text and classified

1649
01:55:15,800 --> 01:55:18,390
into one of these twenty four categories

1650
01:55:18,420 --> 01:55:22,620
so for the weak classifiers we did the same thing is what i described before

1651
01:55:22,620 --> 01:55:24,680
we use these very simple

1652
01:55:24,690 --> 01:55:28,510
rules that are like decision stumps that just test for the presence

1653
01:55:28,520 --> 01:55:31,680
of the word or phrase

1654
01:55:31,690 --> 01:55:33,930
so a few challenges came up and

1655
01:55:33,940 --> 01:55:35,960
in working on

1656
01:55:36,050 --> 01:55:41,260
so one of the things that came up was an initial shortage of data

1657
01:55:42,080 --> 01:55:47,090
in order to deploy system like this very quickly and cheaply

1658
01:55:47,500 --> 01:55:51,170
you can't wait around until you have a lot of data

1659
01:55:51,180 --> 01:55:54,560
but in order for the system to be accurate in its prediction

1660
01:55:54,610 --> 01:55:57,150
you need to train it with a lot of data

1661
01:55:57,170 --> 01:56:01,630
she so we kind of have the bootstrapping problem so in order to deploy the

1662
01:56:01,640 --> 01:56:04,810
system you need to have a lot of labeled data so that can

1663
01:56:04,830 --> 01:56:09,960
so that you can train your system and actually deployed on the other hand to

1664
01:56:09,960 --> 01:56:13,140
get the labelled data the best way to get the labelled data

1665
01:56:13,180 --> 01:56:17,860
it is for the system to already be deployed so they collecting data as people

1666
01:56:17,860 --> 01:56:20,670
carry on conversations with the system

1667
01:56:20,690 --> 01:56:24,040
so to get around this kind of chicken and egg problem

1668
01:56:24,450 --> 01:56:30,800
we used human knowledge to try to compensate for insufficient amount of data early on

1669
01:56:30,800 --> 01:56:33,060
in the training of these systems

1670
01:56:34,960 --> 01:56:39,210
i talked about using exponential loss which tries to

1671
01:56:39,220 --> 01:56:44,080
minimizing exponential loss were trying to fit the training data

1672
01:56:44,100 --> 01:56:48,040
but instead what we did was we devised a way where humans could come up

1673
01:56:48,770 --> 01:56:53,960
a model of very rough model but pretty good model called the prior model

1674
01:56:53,970 --> 01:56:56,540
and then we devised the loss function

1675
01:56:56,570 --> 01:56:59,550
which was a variant on the exponential loss

1676
01:56:59,560 --> 01:57:03,360
where on the one hand we're trying to fit the training data that we do

1677
01:57:03,360 --> 01:57:07,900
have but we're also trying to fit this prior model which was built by the

1678
01:57:07,900 --> 01:57:12,880
human so we've got these two things we're trying to fit both the

1679
01:57:12,890 --> 01:57:18,080
and that worked pretty well so for instance on one dataset called the ATP titles

1680
01:57:19,370 --> 01:57:23,520
this is showing the air right this green curve is the air that we get

1681
01:57:23,520 --> 01:57:27,400
using the just the prior model built by the humans

1682
01:57:27,440 --> 01:57:30,030
it's pretty bad in this case but there are

1683
01:57:30,130 --> 01:57:33,390
there are a lot of categories are remember how many so even

1684
01:57:33,390 --> 01:57:37,730
the very

1685
01:58:17,250 --> 01:58:23,120
you see it

1686
01:58:45,350 --> 01:58:46,370
i mean that

1687
01:58:51,850 --> 01:58:59,000
you know

1688
01:59:38,350 --> 01:59:48,460
when you find

1689
02:00:10,020 --> 02:00:12,770
you are

1690
02:00:14,040 --> 02:00:16,310
people were

1691
02:00:16,330 --> 02:00:18,560
it is

1692
02:00:38,120 --> 02:00:42,980
these are

1693
02:00:53,290 --> 02:01:02,580
it is

1694
02:01:07,680 --> 02:01:10,960
many of the

1695
02:01:10,960 --> 02:01:13,660
and we

1696
02:01:13,680 --> 02:01:16,230
we looked

1697
02:01:16,230 --> 02:01:20,680
that is really

1698
02:01:37,770 --> 02:01:42,850
thank you

1699
02:01:48,620 --> 02:01:52,910
in that direction

1700
02:02:05,460 --> 02:02:06,870
you are

1701
02:02:23,410 --> 02:02:28,810
by saying

1702
02:02:37,270 --> 02:02:39,020
many of them

1703
02:02:56,250 --> 02:03:00,100
i assume that your

1704
02:03:00,120 --> 02:03:06,310
thing to get

1705
02:03:06,960 --> 02:03:07,950
go away

1706
02:03:23,140 --> 02:03:24,330
they are

