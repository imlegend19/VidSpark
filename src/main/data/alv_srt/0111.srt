1
00:00:00,000 --> 00:00:02,170
we need

2
00:00:02,380 --> 00:00:05,500
it will be

3
00:00:22,780 --> 00:00:38,520
but we don't know

4
00:00:39,650 --> 00:00:40,400
we get

5
00:00:50,300 --> 00:00:57,030
they all the

6
00:00:57,120 --> 00:01:00,260
four or

7
00:01:00,310 --> 00:01:03,120
and they gave me

8
00:01:03,140 --> 00:01:09,540
one is a

9
00:01:09,770 --> 00:01:14,640
you can do it

10
00:01:35,720 --> 00:01:40,390
i mean

11
00:01:40,410 --> 00:01:50,270
i don't know what

12
00:02:01,090 --> 00:02:04,840
it's one

13
00:02:08,380 --> 00:02:12,510
you want do it

14
00:02:25,370 --> 00:02:32,530
if you want to know about

15
00:02:32,540 --> 00:02:41,830
you one

16
00:02:41,850 --> 00:02:50,950
and you do get

17
00:03:42,600 --> 00:03:44,940
now you have

18
00:03:45,070 --> 00:03:47,360
one of them

19
00:03:54,150 --> 00:03:55,350
you know this

20
00:03:55,440 --> 00:03:59,490
that is

21
00:04:07,150 --> 00:04:08,330
i mean

22
00:04:09,570 --> 00:04:11,690
you want

23
00:04:36,570 --> 00:04:43,650
you got me you know it

24
00:04:46,740 --> 00:04:50,130
where you

25
00:04:50,130 --> 00:04:51,630
in your application

26
00:04:53,040 --> 00:04:57,260
and created the semantic web client object specify sparql query

27
00:04:58,340 --> 00:05:00,690
execute the query as you usually do

28
00:05:00,730 --> 00:05:03,230
using the query engines

29
00:05:03,360 --> 00:05:05,590
the only difference here is to create model

30
00:05:05,670 --> 00:05:07,260
you must provide

31
00:05:07,380 --> 00:05:11,210
in the modern view of the semantic web client object

32
00:05:11,300 --> 00:05:13,360
that's basically all the take

33
00:05:14,280 --> 00:05:21,940
to create the web of data in your applications as more extensive additions or changes

34
00:05:21,940 --> 00:05:24,360
to applications needed

35
00:05:24,670 --> 00:05:27,050
however if you want more control

36
00:05:27,150 --> 00:05:32,960
you can use these methods you can reload you can reissue URI lookups in order

37
00:05:32,980 --> 00:05:36,340
to reload more graphs can clear the location

38
00:05:36,630 --> 00:05:38,050
furthermore you can

39
00:05:41,030 --> 00:05:43,190
but i look ups explicitly

40
00:05:43,360 --> 00:05:44,320
and again

41
00:05:46,110 --> 00:05:50,980
customers site listening to the requests in order to

42
00:05:51,150 --> 00:05:53,650
execute application specific code

43
00:05:53,840 --> 00:05:55,920
four retrieved graphs

44
00:05:55,960 --> 00:05:59,880
can set configuration options

45
00:06:00,880 --> 00:06:04,250
what interesting for applications that

46
00:06:05,860 --> 00:06:09,360
consider the qualities of retrieved data

47
00:06:10,150 --> 00:06:12,690
the pull provenance information

48
00:06:12,820 --> 00:06:18,110
the library contains several sources of provenance information first of all we have this the

49
00:06:18,530 --> 00:06:19,920
class which is

50
00:06:20,150 --> 00:06:22,460
it represents is in

51
00:06:22,590 --> 00:06:24,190
the retreat graphs

52
00:06:24,360 --> 00:06:25,920
and or and

53
00:06:25,960 --> 00:06:29,630
in this class officer method get force

54
00:06:29,800 --> 00:06:31,110
from which you can

55
00:06:33,380 --> 00:06:38,280
it returns the URI of the of the the graph which which contains this

56
00:06:41,130 --> 00:06:47,030
second we have been looking graphs always contains the provenance graph which is especially graph

57
00:06:47,210 --> 00:06:50,250
this provenance information about

58
00:06:50,860 --> 00:06:56,690
the retrieved graphs provenance information such as the source you have the retrieval time

59
00:06:57,030 --> 00:06:59,730
and the library provides lists of

60
00:06:59,900 --> 00:07:04,650
we bias that has been retrieved successfully or not

61
00:07:04,750 --> 00:07:05,800
as well as

62
00:07:05,860 --> 00:07:09,420
information about redirected eyes

63
00:07:09,630 --> 00:07:10,940
so to sum up

64
00:07:10,980 --> 00:07:14,900
the semantic web client client library enables you to consumers data

65
00:07:15,090 --> 00:07:17,300
from the web

66
00:07:17,420 --> 00:07:21,800
you can issue sparked the careers of trying steal curious

67
00:07:21,940 --> 00:07:24,000
and the library

68
00:07:24,110 --> 00:07:27,480
executes directed browsing algorithm to dynamically

69
00:07:27,550 --> 00:07:30,230
retrieve RDF graphs from

70
00:07:30,400 --> 00:07:33,230
during query execution

71
00:07:33,320 --> 00:07:37,050
further information about the lives as well as the library itself is available from this

72
00:07:38,800 --> 00:07:42,820
and for future versions of the library i'm currently planning

73
00:07:43,050 --> 00:07:50,260
the development of the genes for smart caching replacement of local copies of root crops

74
00:07:50,590 --> 00:07:55,510
since this library is free software project everyone is invited to not only use the

75
00:07:55,510 --> 00:07:57,820
library revert to contribute

76
00:07:58,090 --> 00:07:59,710
so now i hand over to talk

77
00:08:01,110 --> 00:08:06,090
any questions in the state

78
00:08:12,190 --> 00:08:15,170
they are there other libraries and other

79
00:08:20,780 --> 00:08:22,190
and i don't know

80
00:08:22,480 --> 00:08:28,190
i don't know about this but there are people

81
00:08:28,320 --> 00:08:29,800
this one is for java

82
00:08:29,940 --> 00:08:36,900
i think so but i don't know

83
00:08:37,050 --> 00:08:38,940
in the

84
00:08:46,110 --> 00:08:55,610
quite useful to things like

85
00:08:55,690 --> 00:09:00,800
like the rest of the thing

86
00:09:09,860 --> 00:09:15,110
is there anything

87
00:09:15,710 --> 00:09:16,840
the what

88
00:09:16,960 --> 00:09:19,940
we just in general

89
00:09:39,190 --> 00:09:44,880
but you don't know to use line

90
00:10:24,820 --> 00:10:33,380
not that i know

91
00:10:33,400 --> 00:10:35,130
the library

92
00:10:35,230 --> 00:10:37,020
javascript language

93
00:10:37,050 --> 00:10:38,480
and it is here too

94
00:10:40,860 --> 00:10:41,570
the have

95
00:10:41,590 --> 00:10:43,520
XML documents

96
00:10:43,670 --> 00:10:44,610
then you have

97
00:10:46,900 --> 00:10:48,820
in the java sea

98
00:10:48,960 --> 00:10:51,480
nothing like this

99
00:10:53,400 --> 00:10:56,460
imagine that

100
00:11:26,570 --> 00:11:32,260
but i

101
00:11:32,360 --> 00:11:46,630
OK so long and just talk briefly and trying to wrap up the session

102
00:11:46,730 --> 00:11:50,340
and we'll get on some questions afterwards so in summary

103
00:11:50,550 --> 00:11:53,880
the data centric approach for publishing structured data on the web

104
00:11:54,150 --> 00:11:59,820
rather than using proprietary web API is built on on web standards

105
00:12:00,300 --> 00:12:02,030
it reduces the

106
00:12:02,030 --> 00:12:05,740
four hundred and letting it has do with parameter of

107
00:12:08,680 --> 00:12:12,510
you can afford it for the french

108
00:12:12,510 --> 00:12:14,890
and if you have material that

109
00:12:14,890 --> 00:12:21,350
four other by for example have to a lot for all sure that everybody in

110
00:12:21,350 --> 00:12:23,740
this room is a that

111
00:12:23,760 --> 00:12:25,740
i'm sure that you have

112
00:12:25,850 --> 00:12:29,030
when people

113
00:12:29,120 --> 00:12:31,830
i call this whole things

114
00:12:32,030 --> 00:12:35,850
and the lower level of the tree of

115
00:12:36,120 --> 00:12:39,140
o one hundred

116
00:12:44,180 --> 00:12:48,580
because it compared to the fact that the high

117
00:12:48,720 --> 00:12:52,200
and have dealt with the electric

118
00:12:52,240 --> 00:12:59,700
what i'm saying is that society can do different things in the atmosphere of the

119
00:13:00,830 --> 00:13:04,310
five people say it's important to have

120
00:13:04,330 --> 00:13:06,120
why have ours

121
00:13:06,220 --> 00:13:10,450
why all have seen or

122
00:13:10,550 --> 00:13:12,970
that one more than

123
00:13:15,140 --> 00:13:21,410
because it's it's a a driver of innovation the drywall while also fighting to look

124
00:13:21,510 --> 00:13:22,950
for the difference

125
00:13:23,100 --> 00:13:25,930
by the way benefit and that the

126
00:13:25,950 --> 00:13:32,200
why was this kind of information could have another but like that what people call

127
00:13:32,350 --> 00:13:37,930
for of my which happens

128
00:13:38,090 --> 00:13:48,030
everybody liked him but they were to cross the path you

129
00:13:48,050 --> 00:13:50,780
thank could be

130
00:13:50,800 --> 00:13:54,330
so whatever the cost all over the world

131
00:13:55,410 --> 00:13:59,780
what want

132
00:13:59,910 --> 00:14:02,470
what we have to

133
00:14:02,530 --> 00:14:04,550
the product

134
00:14:08,300 --> 00:14:10,350
one of two

135
00:14:14,310 --> 00:14:16,010
what do

136
00:14:16,010 --> 00:14:23,570
we have to cope with what we see

137
00:14:23,760 --> 00:14:26,950
good for

138
00:14:43,430 --> 00:14:45,660
couple of

139
00:14:45,720 --> 00:14:51,530
so that was not always directed towards

140
00:14:51,760 --> 00:14:52,740
like this

141
00:14:54,370 --> 00:14:57,030
we need to know what you have

142
00:14:57,050 --> 00:14:59,930
for example very simple

143
00:14:59,990 --> 00:15:01,910
you know what i'm about

144
00:15:01,930 --> 00:15:05,700
so that's the one that i so we have

145
00:15:07,010 --> 00:15:11,570
well developed field for the proposed

146
00:15:14,700 --> 00:15:15,390
for example

147
00:15:15,780 --> 00:15:20,200
university things to to search for example that

148
00:15:20,240 --> 00:15:24,030
we also the find some to kind the

149
00:15:24,160 --> 00:15:30,950
two different cultures have additional holes five hundred two bands or the

150
00:15:36,470 --> 00:15:38,300
four months

151
00:15:38,310 --> 00:15:41,970
and this is the world war

152
00:15:43,410 --> 00:15:46,010
product conditions tools

153
00:15:49,030 --> 00:15:50,850
there was also a long

154
00:15:50,870 --> 00:15:52,620
the production or both

155
00:15:55,330 --> 00:15:59,510
we have a process in place for example where we tried to two

156
00:15:59,600 --> 00:16:02,620
the above with the body

157
00:16:03,760 --> 00:16:06,120
two countries with

158
00:16:07,450 --> 00:16:10,550
and this is the last

159
00:16:16,370 --> 00:16:19,950
of the

160
00:16:19,950 --> 00:16:21,910
is about one point

161
00:16:21,910 --> 00:16:23,530
just a brief out

162
00:16:25,620 --> 00:16:29,050
next on this friday you are writing tutorial

163
00:16:29,140 --> 00:16:32,500
it's two PM there's only one tutorial this week at two PM it's going to

164
00:16:32,500 --> 00:16:36,510
be in this room so i'm actually come to talk about sort of

165
00:16:36,530 --> 00:16:38,820
the issues involved with

166
00:16:38,880 --> 00:16:42,430
comparing presenting the two and really important that you

167
00:16:42,470 --> 00:16:44,520
some attention to

168
00:16:44,530 --> 00:16:48,220
so today we continue our discussion of

169
00:16:48,280 --> 00:16:52,210
networking and network layering if you remember the last time

170
00:16:53,390 --> 00:16:55,500
talked about

171
00:16:55,630 --> 00:16:59,250
three layers that are in any typical network stack

172
00:16:59,280 --> 00:17:03,530
and these three layers he said would be and then later

173
00:17:03,540 --> 00:17:06,930
the network layer

174
00:17:07,960 --> 00:17:11,860
and the link layer

175
00:17:11,880 --> 00:17:14,350
and we went through the example of how

176
00:17:14,360 --> 00:17:19,210
these three layers interact with each other as a message is sent to the network

177
00:17:20,270 --> 00:17:21,630
on a typical

178
00:17:24,960 --> 00:17:28,250
he said that these three layers

179
00:17:29,220 --> 00:17:39,630
and there might also be a receiver node

180
00:17:39,640 --> 00:17:42,160
and then there could be several each

181
00:17:42,170 --> 00:17:44,360
time messages

182
00:17:44,410 --> 00:17:45,880
centre the network

183
00:17:45,880 --> 00:17:48,030
my pass through any number of

184
00:17:48,220 --> 00:17:50,600
intermediate gateway nodes

185
00:17:50,610 --> 00:17:53,440
or in the intermediate switches so

186
00:17:53,460 --> 00:17:55,330
what impact did send in

187
00:17:55,330 --> 00:17:57,850
it gets sent through the anywhere

188
00:17:57,860 --> 00:18:00,800
in the network layer down in the link layer

189
00:18:00,850 --> 00:18:04,050
the link layer chooses the next link to send the packet out over

190
00:18:04,080 --> 00:18:06,220
since it's one of these which is

191
00:18:06,270 --> 00:18:07,410
the switch

192
00:18:07,440 --> 00:18:11,690
looks at the packets ended up to its its own network layer which is in

193
00:18:11,690 --> 00:18:14,050
charge of determining the next

194
00:18:14,050 --> 00:18:15,830
link the message will take

195
00:18:15,830 --> 00:18:19,550
and on the next play and the next stop the message goes after to the

196
00:18:19,680 --> 00:18:21,870
link layer the link layer determines

197
00:18:22,770 --> 00:18:24,860
yet another way for the message to take

198
00:18:24,900 --> 00:18:28,860
the network layer determines yet another like message for link for the masses to take

199
00:18:28,860 --> 00:18:29,870
and finally

200
00:18:29,890 --> 00:18:35,490
the message reaches the receiver the message propagates through the link layer into the network

201
00:18:35,490 --> 00:18:39,060
layer and then finally to the end and layout using

202
00:18:40,020 --> 00:18:44,700
we talked a little bit last time about various things that happen in this architecture

203
00:18:44,740 --> 00:18:51,020
we said that the there's this process of encapsulation

204
00:18:51,020 --> 00:18:54,050
it happens at each step along the way

205
00:18:55,040 --> 00:18:58,480
you can layer may attach headers on the packet

206
00:18:58,490 --> 00:19:03,360
the header trailer on the packet network layer may tetrahedra trailer and only player may

207
00:19:03,360 --> 00:19:06,960
attach header trailer but at no point does anyway look at the data that higher

208
00:19:06,960 --> 00:19:09,240
level higher layers set

209
00:19:09,270 --> 00:19:12,050
you also notice that in this architecture

210
00:19:13,020 --> 00:19:16,210
we we'll we've shown here is that only the link layer in the network layer

211
00:19:16,260 --> 00:19:19,920
of this which is the reporting packets are actually looking at that are actually

212
00:19:19,930 --> 00:19:21,770
processing the packet so there's no

213
00:19:21,840 --> 00:19:25,510
the into and layer by definition is not involved in the forwarding the packet the

214
00:19:25,510 --> 00:19:31,090
and and layer is only involved when one of the endpoints of communication is involved

215
00:19:31,110 --> 00:19:34,460
so i want to talk about today we're finished very briefly our discussion at the

216
00:19:34,460 --> 00:19:37,550
link layer and then we're going to turn focus mostly

217
00:19:37,670 --> 00:19:41,520
on the networking layer

218
00:19:44,210 --> 00:19:47,640
the remember the last time we got as far as saying that the link layer

219
00:19:47,640 --> 00:19:51,830
is in charge of the number of sort of important issues with the transmission of

220
00:19:51,830 --> 00:19:52,950
data across

221
00:19:52,960 --> 00:19:55,270
one link of the network we talked

222
00:19:55,400 --> 00:19:58,010
for a while it in the class last time about this

223
00:19:59,430 --> 00:20:00,950
to digital

224
00:20:03,330 --> 00:20:04,760
backwards digital

225
00:20:04,770 --> 00:20:07,310
the analog to digital conversion

226
00:20:07,330 --> 00:20:10,480
we're going to talk about the other issues that we need to to talk about

227
00:20:10,480 --> 00:20:13,400
in the context of the network layer the link layer

228
00:20:13,460 --> 00:20:14,340
it is

229
00:20:15,050 --> 00:20:19,020
the issue of framing so idea with framing is when you're sending a message out

230
00:20:19,020 --> 00:20:20,210
over a network link

231
00:20:20,990 --> 00:20:24,150
receiver on the other end is to have some way of knowing that this is

232
00:20:24,380 --> 00:20:29,240
a packet starting work packages and or as we call these packets when a link

233
00:20:29,240 --> 00:20:31,220
layer frames so

234
00:20:31,240 --> 00:20:35,570
she reframing is to identify the sort of beginning and end of every

235
00:20:35,590 --> 00:20:39,400
one of these frames as it transmits over the network and there's a fairly this

236
00:20:39,400 --> 00:20:43,560
sort of fairly obvious way to do this is well attach some special symbol put

237
00:20:43,560 --> 00:20:48,810
some special symbol at the beginning and end of every packet

238
00:20:48,860 --> 00:20:55,180
so for example through an ethernet the payload of an ethernet packet that might contain

239
00:20:55,180 --> 00:20:57,860
the destination address the source address

240
00:20:57,870 --> 00:20:59,000
the type so

241
00:20:59,010 --> 00:21:02,000
talk more about what the title means in a minute

242
00:21:02,010 --> 00:21:06,820
the data and some text some information can be used to detect errors

243
00:21:06,850 --> 00:21:10,010
and the preamble

244
00:21:10,030 --> 00:21:13,050
it is a special code that is attached to the beginning of every one of

245
00:21:13,050 --> 00:21:18,490
these messages and this is used to make manchester encoding remember we talked about last

246
00:21:18,490 --> 00:21:22,650
class which is used as this this is you start to make a lot of

247
00:21:22,650 --> 00:21:26,980
things like loop to lock into the message which we talked about the last class

248
00:21:26,990 --> 00:21:28,050
and it might be

249
00:21:28,060 --> 00:21:31,820
now in the case of the genetic sequence a well-defined sequence one zero one zero

250
00:21:31,820 --> 00:21:36,810
one zero one zero but remember that with manchester encoding that the data is actually

251
00:21:36,810 --> 00:21:39,800
transmitted over the network looks a little bit different

252
00:21:39,820 --> 00:21:43,170
and then following the preamble there's the start of frame symbol

253
00:21:43,220 --> 00:21:47,260
and at the end of the message there's this and simple

254
00:21:47,280 --> 00:21:50,610
so one thing we might be concerned about is a for example

255
00:21:50,630 --> 00:21:51,660
what is the

256
00:21:51,670 --> 00:21:56,010
network layer tries to send the message that contains the and symbol in it

257
00:21:56,120 --> 00:22:00,040
i would be a problem right because then the and layer would have you know

258
00:22:00,050 --> 00:22:03,860
in it's the BN layer would have inadvertantly

259
00:22:03,890 --> 00:22:08,240
terminated the message even though the really and the message this was just something happened

260
00:22:08,250 --> 00:22:10,880
to be the same code is whatever the

261
00:22:10,910 --> 00:22:13,610
link layer it shows inference and code symbol

262
00:22:14,610 --> 00:22:18,900
and the reason that this is concerned we don't want the network you have to

263
00:22:18,900 --> 00:22:21,530
understand lots of details about how the link layer

264
00:22:21,870 --> 00:22:25,030
operates underneath by the link layer of the network layer shouldn't have to make any

265
00:22:25,030 --> 00:22:31,030
assumptions about what are valid symbols for transmitting what invalid symbols for transmit

266
00:22:31,060 --> 00:22:34,300
so the way that we're going to solve this is through one of two techniques

267
00:22:36,130 --> 00:22:40,160
the first so the first technique that will talk about for a moment is this

268
00:22:40,160 --> 00:22:43,590
idea of bit stuffing

269
00:22:45,270 --> 00:22:47,950
it's about in a minute another simple thing we could do

270
00:22:47,990 --> 00:22:53,200
would be to simply use a code that can possibly be generated by see for

271
00:22:53,200 --> 00:22:57,570
example the the manchester encoding scheme so only if the network layer sends a message

272
00:22:57,570 --> 00:22:59,620
like one one one one

273
00:22:59,670 --> 00:23:03,860
the link layer is going to convert that into some sequence of ones and zeros

274
00:23:04,110 --> 00:23:08,800
once it applies manchester coding so if the the link layer on the receiver sees

275
00:23:08,800 --> 00:23:12,550
the message like one one one one one it the that can possibly be a

276
00:23:12,550 --> 00:23:15,220
valid code for the

277
00:23:15,370 --> 00:23:19,470
there can possibly be invalid messages that could have been generated by the by the

278
00:23:19,470 --> 00:23:22,820
network layer only the link layer can actually send a sequence of bits out so

279
00:23:22,820 --> 00:23:25,720
we can tell that would be terminated symbol

280
00:23:25,760 --> 00:23:29,990
another simple way though to send one of these encodes is using this technique called

281
00:23:30,000 --> 00:23:30,970
bit stuffing

282
00:23:31,040 --> 00:23:35,490
it's pretty simple and kind of any technique can be used in general when you

283
00:23:35,490 --> 00:23:39,780
your possibilities even though some nodes

284
00:23:39,800 --> 00:23:45,430
my question was in this process i believe this is something that you know

285
00:23:45,490 --> 00:23:49,270
to select some actions in the future what would you do

286
00:23:50,320 --> 00:23:57,520
so the question is the question was supposed to have prior knowledge of the posterior

287
00:23:57,520 --> 00:23:58,830
distribution somehow

288
00:23:58,840 --> 00:24:00,440
after the same

289
00:24:00,450 --> 00:24:02,160
you will probably

290
00:24:02,180 --> 00:24:04,600
what actually hard

291
00:24:04,610 --> 00:24:08,930
OK so this is a misunderstanding make sure

292
00:24:11,030 --> 00:24:12,840
because what is

293
00:24:12,850 --> 00:24:14,130
this method is

294
00:24:14,150 --> 00:24:18,410
this estimation

295
00:24:18,490 --> 00:24:20,460
it's not really

296
00:24:20,480 --> 00:24:22,170
what action should

297
00:24:22,190 --> 00:24:23,800
it's estimating

298
00:24:23,810 --> 00:24:26,140
five the water

299
00:24:26,150 --> 00:24:30,330
hard choose actions we choose actions by computing the policy

300
00:24:30,340 --> 00:24:33,560
the optimal with respect

301
00:24:33,570 --> 00:24:36,300
so the centre half

302
00:24:36,320 --> 00:24:38,460
so it's a

303
00:24:38,480 --> 00:24:40,550
the planet was

304
00:24:40,570 --> 00:24:43,980
but that's all

305
00:24:43,990 --> 00:24:45,140
OK so

306
00:24:45,730 --> 00:24:50,400
so we face the exploration exploitation the strategy

307
00:24:50,420 --> 00:24:52,850
exploitation will be

308
00:24:52,880 --> 00:24:56,900
choose a policy that all the respective more

309
00:24:58,600 --> 00:24:59,960
she actually

310
00:24:59,980 --> 00:25:02,170
so as to obtain information

311
00:25:02,190 --> 00:25:04,180
so as to improve my model

312
00:25:04,220 --> 00:25:06,010
so that later in time

313
00:25:06,020 --> 00:25:09,620
maybe i can get a better one

314
00:25:09,640 --> 00:25:13,040
and that's the exploration exploitation dilemma

315
00:25:13,110 --> 00:25:19,320
again not faced faced in supervised and unsupervised learning but the key key question in

316
00:25:19,330 --> 00:25:25,290
reinforcement learning and michael littman is going to present an algorithm called the cube algorithm

317
00:25:26,340 --> 00:25:31,880
michael collins and i developed some years ago that will actually address this specific thing

318
00:25:32,090 --> 00:25:35,670
you asked me not to talk about that so you can talk about

319
00:25:46,540 --> 00:25:56,040
so there are many different formulations of the problem of exploration exploitation one version

320
00:25:56,050 --> 00:25:59,530
you know when it is a little house steps

321
00:25:59,570 --> 00:26:02,000
and we want to maximize the world

322
00:26:02,900 --> 00:26:04,310
you can call that way

323
00:26:04,330 --> 00:26:08,430
or weighted EQ is that we

324
00:26:08,450 --> 00:26:14,460
what about how is going to live but instead simply as that it is always

325
00:26:14,460 --> 00:26:18,970
doing about where could about how know how was going

326
00:26:18,990 --> 00:26:20,470
something like that so

327
00:26:20,480 --> 00:26:25,400
they are very different ways of formulating the problem so

328
00:26:26,130 --> 00:26:31,970
i think we have a good question which is we rarely in reinforcement learning assume

329
00:26:32,070 --> 00:26:35,650
training phase in the face and the phase testing phase

330
00:26:35,670 --> 00:26:36,550
there is

331
00:26:36,570 --> 00:26:38,090
rarely that thing

332
00:26:38,140 --> 00:26:43,220
right because in some sense life is not like that it is not

333
00:26:43,230 --> 00:26:48,080
like the test training phase testing you can argue that education a bit like

334
00:26:48,100 --> 00:26:52,480
the has to be that it's the but not part like just continuously which is

335
00:26:52,480 --> 00:26:56,870
going to live we're always trying to choose actions so as to

336
00:26:56,890 --> 00:27:02,630
improve what happens in the future so there is no so the separation between training

337
00:27:02,630 --> 00:27:04,460
and test sets something that's what

338
00:27:04,470 --> 00:27:07,250
is it really that we will be forced down

339
00:27:07,300 --> 00:27:13,030
you can certainly ask many questions from the formulation and people

340
00:27:25,640 --> 00:27:28,320
yes yes certainly

341
00:27:28,340 --> 00:27:29,830
certainly you can

342
00:27:29,850 --> 00:27:34,590
you can make you can make the exploration exploitation problem precise in different ways and

343
00:27:34,590 --> 00:27:36,380
the choice is

344
00:27:36,550 --> 00:27:38,610
not addressing this point

345
00:27:38,620 --> 00:27:40,080
what the different choices are

346
00:27:40,100 --> 00:27:46,200
how the how to formulate exploration exploitation again michael littman we'll talk about a specific

347
00:27:46,240 --> 00:27:47,720
i think l

348
00:27:48,620 --> 00:27:50,370
do it

349
00:27:50,390 --> 00:27:54,940
i say that because a lot of people are not aware of solving the exploration

350
00:27:54,940 --> 00:27:57,560
exploitation let michael talk

351
00:27:57,570 --> 00:28:02,420
OK so let me just say something very simple and obvious

352
00:28:02,430 --> 00:28:04,490
you can imagine that

353
00:28:04,510 --> 00:28:07,670
as long as we use efficient exploration that is we

354
00:28:07,720 --> 00:28:10,120
in the living try every action

355
00:28:10,140 --> 00:28:12,000
in every state in the office

356
00:28:12,020 --> 00:28:16,920
the following simple the results obtained obvious that

357
00:28:16,930 --> 00:28:22,560
if i visited every state action pair the limit my model the converge

358
00:28:22,580 --> 00:28:26,860
you want to live there for my so called policy

359
00:28:26,870 --> 00:28:29,080
will converge to the optimal policy

360
00:28:30,170 --> 00:28:31,930
now is sort

361
00:28:31,950 --> 00:28:34,230
an interesting results but

362
00:28:34,250 --> 00:28:39,040
wait for the discrete discrete finite

363
00:28:39,050 --> 00:28:40,030
and the said

364
00:28:54,310 --> 00:28:55,940
and like

365
00:28:55,990 --> 00:28:59,930
so so the

366
00:29:00,820 --> 00:29:06,880
yes i am i'm not quite ready to to make the connection between supervised unsupervised

367
00:29:06,890 --> 00:29:13,150
reinforcement learning but actually so so modern buildings like supervised learning problem in the sense

368
00:29:13,180 --> 00:29:18,150
that given these mappings state action actually given examples state action next state

369
00:29:18,150 --> 00:29:19,650
this whole process

370
00:29:20,260 --> 00:29:21,610
later it was

371
00:29:21,630 --> 00:29:24,970
this is the graphical

372
00:29:24,990 --> 00:29:27,320
directed graphical models

373
00:29:27,340 --> 00:29:28,320
so we have

374
00:29:28,340 --> 00:29:29,780
he sees

375
00:29:29,860 --> 00:29:36,780
so we can be presented as late as patients some document

376
00:29:36,900 --> 00:29:39,090
the documents

377
00:29:39,510 --> 00:29:41,940
research topics

378
00:29:41,940 --> 00:29:45,010
this is a small change

379
00:29:45,070 --> 00:29:52,280
and then for each of these samples from the two so you can see

380
00:29:54,800 --> 00:29:57,530
you might in the text have two topics

381
00:29:57,550 --> 00:29:59,760
one topic money

382
00:29:59,780 --> 00:30:01,420
more important than the others

383
00:30:02,240 --> 00:30:06,240
and there are certain words like you is that for bit

384
00:30:08,450 --> 00:30:13,940
they fit the first topics one of the things that fit to

385
00:30:13,970 --> 00:30:17,490
this was just three years

386
00:30:18,630 --> 00:30:19,780
it could be a

387
00:30:20,740 --> 00:30:28,530
holds topics represents

388
00:30:28,550 --> 00:30:32,690
so now we can describe our document

389
00:30:32,710 --> 00:30:39,550
so the joint probability model of the document and its work we can describe probability

390
00:30:41,300 --> 00:30:46,170
document and then this is the actual document generates work

391
00:30:46,190 --> 00:30:49,590
but now we can split this into two

392
00:30:52,900 --> 00:30:56,220
top and this is of course

393
00:30:56,240 --> 00:30:59,050
this is made this page

394
00:30:59,090 --> 00:31:05,090
that you have to know in advance a certain number of of topics otherwise computation

395
00:31:07,090 --> 00:31:13,820
for this but this was the case with latent semantic analysis that i mentioned direction

396
00:31:13,820 --> 00:31:15,650
it's not for the us

397
00:31:15,720 --> 00:31:17,010
i feel

398
00:31:20,490 --> 00:31:22,510
given these topics

399
00:31:22,670 --> 00:31:25,590
we can model document

400
00:31:25,610 --> 00:31:30,630
as the distribution of topics each topic has distribution

401
00:31:32,490 --> 00:31:35,940
we to delete so if we have a large corpus

402
00:31:39,360 --> 00:31:46,630
we train the score was by maximizing the likelihood function loss of

403
00:31:46,630 --> 00:31:53,840
the joint probability we by the frequency of words the change

404
00:31:53,840 --> 00:31:55,650
and we we

405
00:31:55,690 --> 00:31:58,740
get to collect these values

406
00:31:58,780 --> 00:32:03,280
these are can of only words and

407
00:32:04,130 --> 00:32:08,280
and you can change with the

408
00:32:09,780 --> 00:32:14,340
which is why are not actually which ones belong to topic

409
00:32:14,360 --> 00:32:18,780
and what is the probability of the word belonging to topic and also for each

410
00:32:18,780 --> 00:32:21,570
document you know it is

411
00:32:21,590 --> 00:32:23,760
the topic distribution

412
00:32:27,880 --> 00:32:29,760
why not

413
00:32:31,010 --> 00:32:32,420
so we have this

414
00:32:33,720 --> 00:32:36,450
we have this document

415
00:32:36,470 --> 00:32:39,010
and then we want to know

416
00:32:39,030 --> 00:32:45,320
how much in what is the distribution of topic one topic one topic two in

417
00:32:46,590 --> 00:32:48,420
this the same

418
00:32:48,440 --> 00:32:55,860
three c is very useful

419
00:32:55,900 --> 00:32:59,630
that it has limitations that

420
00:32:59,650 --> 00:33:01,510
you have your training set

421
00:33:01,510 --> 00:33:02,710
you today

422
00:33:02,720 --> 00:33:05,900
and if you new document

423
00:33:05,920 --> 00:33:10,050
there are two ways fold a few documents into your model

424
00:33:10,110 --> 00:33:14,070
but if you have many new documents you have to be true

425
00:33:14,090 --> 00:33:16,150
and also you have to estimate

426
00:33:16,170 --> 00:33:19,970
each time

427
00:33:19,990 --> 00:33:21,130
you have

428
00:33:21,150 --> 00:33:22,920
if your document collection

429
00:33:24,150 --> 00:33:27,400
you also the number of

430
00:33:28,780 --> 00:33:36,490
but you have to estimate is spending see grows linearly which documents so

431
00:33:36,490 --> 00:33:42,920
it was

432
00:33:44,260 --> 00:33:47,030
we have that actually that

433
00:33:47,030 --> 00:33:49,070
so it's late should keep

434
00:33:49,150 --> 00:33:52,030
which was published in two thousand two

435
00:33:52,840 --> 00:33:58,470
july two thousand genes in terms of machine learning research

436
00:33:58,530 --> 00:34:00,400
and in this model

437
00:34:00,420 --> 00:34:01,360
you can

438
00:34:04,260 --> 00:34:08,280
we need treat a topic topic mixture which is the key

439
00:34:08,280 --> 00:34:11,170
he did not know

440
00:34:11,190 --> 00:34:13,470
ten times here

441
00:34:13,530 --> 00:34:17,110
it's it's quite

442
00:34:17,130 --> 00:34:19,590
so now we can cage

443
00:34:19,590 --> 00:34:23,360
also in that

444
00:34:24,190 --> 00:34:26,090
which we have to estimate

445
00:34:26,090 --> 00:34:32,990
and this allows us also document to infer that the construction

446
00:34:33,010 --> 00:34:35,110
now this models

447
00:34:35,130 --> 00:34:37,940
similar models

448
00:34:37,950 --> 00:34:39,880
due work

449
00:34:39,880 --> 00:34:41,190
this set

450
00:34:41,220 --> 00:34:44,190
it is composed of

451
00:34:44,230 --> 00:34:44,880
and you

452
00:34:45,400 --> 00:34:48,840
so they to which

453
00:34:48,860 --> 00:34:51,010
condition now these not

454
00:34:51,630 --> 00:34:53,490
i possible to

455
00:34:53,490 --> 00:34:55,650
between the value

456
00:34:56,820 --> 00:35:00,570
all these bands so long as lead to model

457
00:35:01,300 --> 00:35:03,440
more simpler model the

458
00:35:04,170 --> 00:35:06,380
the term is also come

459
00:35:08,010 --> 00:35:13,590
with the first topic distribution was

460
00:35:13,610 --> 00:35:18,260
but i go well i

461
00:35:18,280 --> 00:35:20,210
well actually what is

462
00:35:20,570 --> 00:35:24,780
you can look at his publications

463
00:35:24,800 --> 00:35:31,110
it's not going to that is the location is very expensive it gives you

464
00:35:31,110 --> 00:35:33,300
in the appendix all

465
00:35:33,320 --> 00:35:34,720
the necessary

466
00:35:34,740 --> 00:35:37,280
computations that you need

467
00:35:38,940 --> 00:35:40,030
this may

468
00:35:41,000 --> 00:35:43,380
the source fuel

469
00:35:43,400 --> 00:35:47,300
you can change just sort called

470
00:35:47,320 --> 00:35:50,650
four of her soul

471
00:35:50,710 --> 00:35:51,900
and so

472
00:35:51,920 --> 00:35:54,950
our experience is that the light and so on

473
00:35:54,970 --> 00:36:00,740
now if you you have questions regarding certain implementations will change

474
00:36:04,190 --> 00:36:06,690
so actually

475
00:36:06,720 --> 00:36:08,510
what i'm trying to do

476
00:36:09,920 --> 00:36:17,050
in its explain this publication you learn simple splits models

477
00:36:17,050 --> 00:36:20,010
and how can you predict that those words

478
00:36:20,030 --> 00:36:22,830
so this is what happens in machine learning

479
00:36:22,990 --> 00:36:28,060
the simple thing which we did in the out of this competition was first predicted

480
00:36:29,380 --> 00:36:34,520
like the single gene structure and then try to extend the scene gene structure by

481
00:36:34,870 --> 00:36:36,850
including what

482
00:36:36,890 --> 00:36:40,360
based on against sequence

483
00:36:40,360 --> 00:36:42,400
OK but this is not

484
00:36:42,830 --> 00:36:48,160
i mean this ignores quite a few facts how are the mighty ducks

485
00:36:48,490 --> 00:36:51,030
and the points that you cannot

486
00:36:51,050 --> 00:36:52,580
you cannot

487
00:36:52,600 --> 00:36:58,000
in predicting the effects of that it's hard to find out the sex so i

488
00:36:58,000 --> 00:37:03,300
mean it's better to find excellent together with idea this it's

489
00:37:03,350 --> 00:37:08,520
and this is essentially what you would like to do would like to predict these

490
00:37:08,520 --> 00:37:13,450
kind of splice products which are shown you on the previous slide based on the

491
00:37:13,450 --> 00:37:18,950
DNA sequence so this is more difficult inference task on input sequence of inputs like

492
00:37:18,950 --> 00:37:22,480
you just have a sequence on the output side problem

493
00:37:23,710 --> 00:37:28,630
these graphs have to have some structures about it but it's not easy to come

494
00:37:28,630 --> 00:37:32,130
up with the dynamic programming algorithm can actually be called this

495
00:37:32,150 --> 00:37:36,340
but if you can do this then you can just use structured output learning and

496
00:37:36,380 --> 00:37:37,990
all the time

497
00:37:38,010 --> 00:37:40,100
but it's it's

498
00:37:40,940 --> 00:37:46,070
so and you think i mean we started expanding this to a certain kind of

499
00:37:46,760 --> 00:37:50,620
i mean this subset

500
00:37:50,660 --> 00:37:57,990
OK so i showed you some results i mean this result i think it so

501
00:37:57,990 --> 00:38:05,550
d structure prediction still be challenge that continue to the i've shown you that the

502
00:38:05,560 --> 00:38:12,210
accuracy for quantitatively genes the is already quite a bit accuracy of ninety five

503
00:38:12,220 --> 00:38:18,800
so the first version of finding system seems very promising

504
00:38:18,900 --> 00:38:26,220
also compared to create was actually in this comparison that's another discriminative gene finding system

505
00:38:26,230 --> 00:38:27,820
it's right here

506
00:38:27,850 --> 00:38:29,050
right now

507
00:38:29,740 --> 00:38:35,600
so we have believe that this was comparison compares favourably to

508
00:38:35,620 --> 00:38:42,740
and it's computational challenge how to build a twenty five in this picture

509
00:38:42,760 --> 00:38:48,080
and of course not we can use this finding system to rehabilitate the norms and

510
00:38:48,150 --> 00:38:52,070
also for the newly sequenced genomes

511
00:38:52,090 --> 00:38:57,380
but you challenge for instance how can you want you know to apply what we

512
00:38:57,380 --> 00:39:00,540
learned to another jail maybe parameters

513
00:39:00,670 --> 00:39:03,620
to change had to be changed only slightly

514
00:39:03,620 --> 00:39:06,390
so we need some way of

515
00:39:06,490 --> 00:39:10,540
getting information from one organism to another which is related

516
00:39:12,270 --> 00:39:13,990
i think this is

517
00:39:20,230 --> 00:39:22,480
it so

518
00:39:22,520 --> 00:39:27,820
they actually quite a few contributors dimension i would like to mention the ensemble

519
00:39:27,840 --> 00:39:33,120
that i've worked together with a lot of people and supplies but also the other

520
00:39:33,120 --> 00:39:36,760
guys including those with the of

521
00:39:36,780 --> 00:39:42,220
and y in g w h five e of the alexander seems to have a

522
00:39:42,220 --> 00:39:45,280
lot of work on this

523
00:39:45,290 --> 00:39:52,430
OK so find topic which i think is on

524
00:39:54,350 --> 00:39:58,910
resequencing hiding every for the detection of forty

525
00:39:58,920 --> 00:40:01,990
one part of this is again

526
00:40:02,040 --> 00:40:04,370
applications of structure of the so

527
00:40:04,400 --> 00:40:08,960
it is the technique should be understood by everybody

528
00:40:11,020 --> 00:40:14,800
the idea is what the question is a the ballot question behind that is to

529
00:40:14,800 --> 00:40:21,890
understand the genetic basis of variation in plants what so what you see here is

530
00:40:21,890 --> 00:40:28,580
a plant growing in different places one in which one getting and

531
00:40:28,600 --> 00:40:32,160
i mean these are common seems to be the same species you across the i

532
00:40:32,160 --> 00:40:33,020
mean there

533
00:40:33,030 --> 00:40:35,030
almost has the same you know

534
00:40:35,230 --> 00:40:39,580
and what you would like to understand the differences between these two you know and

535
00:40:39,580 --> 00:40:43,620
to understand why this one looks like this and the other one looks back so

536
00:40:43,620 --> 00:40:47,970
this is a very important question about it

537
00:40:47,970 --> 00:40:50,660
and one

538
00:40:50,710 --> 00:40:55,130
i would like to understand what kind of sequence changes fewer in short time frames

539
00:40:55,680 --> 00:40:59,400
which kind of polymorphisms and gene underlying action

540
00:40:59,420 --> 00:41:04,930
unlike adoption so that means maybe cambridge but the different than getting it has to

541
00:41:04,930 --> 00:41:13,070
adapt to the best there and what are the consequences of these polymorphisms fourteen

542
00:41:13,340 --> 00:41:18,590
so and with this analysis arabidopsis tion chosen to you know with a one hundred

543
00:41:18,590 --> 00:41:19,950
twenty pages long

544
00:41:19,990 --> 00:41:22,470
and the

545
00:41:22,490 --> 00:41:29,520
i mean there's a lot of resources on the on this terrible to see

546
00:41:29,530 --> 00:41:34,790
OK so and in this study we consider resequencing of twenty

547
00:41:34,810 --> 00:41:38,480
ecotypes that means

548
00:41:38,520 --> 00:41:42,270
strains which come from different geographic location

549
00:41:42,290 --> 00:41:45,050
all over the world so it

550
00:41:45,070 --> 00:41:45,820
i mean

551
00:41:45,870 --> 00:41:52,060
you see you there in two thousand different strains collected and this was spread all

552
00:41:52,060 --> 00:41:53,540
over the world

553
00:41:53,570 --> 00:41:56,280
i mean everywhere but

554
00:41:56,350 --> 00:42:00,720
OK so what is the we can think harry

555
00:42:00,740 --> 00:42:02,330
we expect that

556
00:42:02,460 --> 00:42:04,880
so let's say i mean

557
00:42:04,880 --> 00:42:11,220
we have the sequence of the reference genome sequenced you know the the genome center

558
00:42:11,230 --> 00:42:12,670
also in

559
00:42:12,690 --> 00:42:19,230
the idea is that you would like to determine small sequence variation among the different

560
00:42:19,340 --> 00:42:21,590
strengths with you could

561
00:42:21,680 --> 00:42:24,420
so what we have

562
00:42:25,980 --> 00:42:27,910
which i fixed to some error

563
00:42:27,920 --> 00:42:30,280
and those nucleotide complementary

564
00:42:30,310 --> 00:42:32,230
two with the reference sequence

565
00:42:32,490 --> 00:42:36,750
and that's why every position in the sequence there for

566
00:42:36,760 --> 00:42:41,490
four probes in the position of the problem is very

567
00:42:42,390 --> 00:42:46,180
and whenever this snippets position and you will see

568
00:42:46,200 --> 00:42:53,730
e there i mean that one of those reference products will bind to to the

569
00:42:53,800 --> 00:42:55,560
this piece of DNA

570
00:42:55,580 --> 00:42:57,830
and by

571
00:42:57,840 --> 00:43:01,700
by observing which one had actually bound you can find out

572
00:43:01,710 --> 00:43:03,960
which is the position that

573
00:43:03,980 --> 00:43:07,230
i mean which is the nuclear

574
00:43:08,950 --> 00:43:17,410
so do this now we have two sequences those complementary many of the edits the

575
00:43:17,500 --> 00:43:24,220
position of the eight active here and then you see he on the web position

576
00:43:24,220 --> 00:43:25,960
you like one

577
00:43:25,980 --> 00:43:26,820
but you know

578
00:43:26,840 --> 00:43:31,690
and you do this also in the first round three x eight measurements position that

579
00:43:31,690 --> 00:43:36,060
you know this means that like a hundred twenty times

580
00:43:36,100 --> 00:43:43,040
eight million measurements which is like a billion measurements is quite big

581
00:43:43,080 --> 00:43:47,520
so like ninety nine percent of all faces very simple on and you know and

582
00:43:48,100 --> 00:43:53,580
it is very important because friend billion oligos and twenty

583
00:43:53,600 --> 00:43:56,030
twenty billion membership

584
00:43:56,030 --> 00:44:01,270
last time

585
00:44:01,280 --> 00:44:02,740
we discussed

586
00:44:04,560 --> 00:44:07,900
acceleration is caused by pollution

587
00:44:07,910 --> 00:44:09,790
well by paul

588
00:44:09,800 --> 00:44:13,080
today we will express this more qualitatively

589
00:44:13,800 --> 00:44:17,350
three laws which are called newton's laws

590
00:44:17,350 --> 00:44:18,620
the first law

591
00:44:18,640 --> 00:44:20,080
really goes back to the

592
00:44:20,830 --> 00:44:23,390
the first part of the seventeenth century

593
00:44:23,440 --> 00:44:27,760
it was galileo who expressed what he called the law of

594
00:44:27,780 --> 00:44:29,940
inertia and i'll read you

595
00:44:29,950 --> 00:44:33,110
is law

596
00:44:33,150 --> 00:44:34,130
a body

597
00:44:37,800 --> 00:44:39,330
anybody in motion

598
00:44:39,330 --> 00:44:45,080
continues to move at a constant velocity along the straight line

599
00:44:45,110 --> 00:44:49,440
unless acted upon by an external force

600
00:44:49,510 --> 00:44:51,840
and now i read to you know tens

601
00:44:51,930 --> 00:44:54,410
on words in his famous book

602
00:44:58,410 --> 00:45:02,360
perseveres in its state of rest

603
00:45:02,410 --> 00:45:04,590
or of uniform motion

604
00:45:04,640 --> 00:45:06,590
in a right line

605
00:45:06,590 --> 00:45:07,890
on less

606
00:45:07,910 --> 00:45:11,330
it is compelled to change that state

607
00:45:11,340 --> 00:45:13,510
by forces

608
00:45:14,940 --> 00:45:16,780
upon it

609
00:45:16,780 --> 00:45:18,140
and newton's first law

610
00:45:18,150 --> 00:45:21,000
is clearly against our daily experiences

611
00:45:21,010 --> 00:45:24,080
things that move don't move along the straight line

612
00:45:24,090 --> 00:45:27,200
and i will continue to move and the reason is because gravity

613
00:45:27,250 --> 00:45:31,120
and there's a lot of these even if you remove gravity

614
00:45:31,170 --> 00:45:33,140
then the friction

615
00:45:33,140 --> 00:45:34,390
as an

616
00:45:34,420 --> 00:45:36,730
so things will always come to hold

617
00:45:36,780 --> 00:45:38,410
we believe those

618
00:45:38,480 --> 00:45:39,720
but in the absence

619
00:45:39,720 --> 00:45:42,760
of any force is indeed an object

620
00:45:42,780 --> 00:45:44,470
if it had a certain velocity

621
00:45:44,480 --> 00:45:46,940
would continue along a straight line

622
00:45:47,910 --> 00:45:49,140
and never

623
00:45:49,190 --> 00:45:51,260
and never

624
00:45:51,360 --> 00:45:52,760
now this law

625
00:45:52,780 --> 00:45:54,840
it's very fundamental law

626
00:45:54,890 --> 00:45:58,110
does not hold in all reference frames

627
00:45:58,140 --> 00:46:00,360
for instance it doesn't hold

628
00:46:00,570 --> 00:46:04,910
in reference frame which itself is being accelerated

629
00:46:05,970 --> 00:46:08,940
but i accelerate myself right here

630
00:46:08,970 --> 00:46:12,310
you john my so i take my bicycle

631
00:46:12,380 --> 00:46:13,320
o my

632
00:46:13,370 --> 00:46:18,190
motorcycle my car and you see me being accelerated in this direction

633
00:46:18,240 --> 00:46:19,620
you sit there

634
00:46:19,630 --> 00:46:20,540
and you say

635
00:46:21,030 --> 00:46:23,520
its velocity is changing

636
00:46:23,560 --> 00:46:27,970
therefore according to the first law there must be a force on him and you

637
00:46:28,500 --> 00:46:35,090
you feel that force actually i do feel that i feel someone pushing me

638
00:46:35,090 --> 00:46:36,850
consistent with the first law

639
00:46:36,910 --> 00:46:39,250
perfect first slow works for you

640
00:46:39,270 --> 00:46:40,810
nine years

641
00:46:40,850 --> 00:46:43,150
i'm being accelerated in this direction

642
00:46:43,270 --> 00:46:48,280
you will come towards me being accelerated in this direction i say first of all

643
00:46:48,280 --> 00:46:49,620
should work

644
00:46:50,400 --> 00:46:52,740
these people should feel a push

645
00:46:52,780 --> 00:46:56,740
i said hey there you feel push you say if you must

646
00:46:56,780 --> 00:47:01,070
there is no pushed is not there for the first law doesn't work from my

647
00:47:01,070 --> 00:47:04,060
frame of reference if i'm being accelerated

648
00:47:05,810 --> 00:47:08,150
so now comes the question

649
00:47:08,150 --> 00:47:09,410
when does

650
00:47:09,470 --> 00:47:10,690
the first floor

651
00:47:10,710 --> 00:47:11,970
work well

652
00:47:12,060 --> 00:47:14,090
the first law works

653
00:47:14,150 --> 00:47:18,880
when the frame of reference is what we call an inertial frame of reference

654
00:47:18,900 --> 00:47:21,790
an inertial frame of reference would then be afraid

655
00:47:21,810 --> 00:47:26,280
in which there are no accelerations of any kind

656
00:47:26,290 --> 00:47:29,910
is that possible is twenty six one hundred is this lecture hall

657
00:47:29,910 --> 00:47:30,840
in inertial

658
00:47:30,840 --> 00:47:32,630
reference frame

659
00:47:32,650 --> 00:47:34,850
four one

660
00:47:34,910 --> 00:47:38,190
the earth rotates about its own axis in twenty six one hundred goes with that

661
00:47:38,190 --> 00:47:40,590
give you centripetal acceleration

662
00:47:40,650 --> 00:47:41,720
number two

663
00:47:41,720 --> 00:47:44,430
it goes around the sun

664
00:47:44,440 --> 00:47:50,100
that gives you the the centripetal acceleration including including new including twenty six one hundred

665
00:47:50,250 --> 00:47:51,720
the sun goes around

666
00:47:51,740 --> 00:47:52,970
the milky way

667
00:47:53,060 --> 00:47:55,810
you can go on and on so clearly

668
00:47:55,810 --> 00:47:57,240
twenty six one hundred

669
00:47:57,250 --> 00:47:58,440
it's not

670
00:47:58,520 --> 00:47:59,500
an initial

671
00:47:59,520 --> 00:48:02,290
reference frame

672
00:48:02,310 --> 00:48:05,190
we can try to make an estimate

673
00:48:05,240 --> 00:48:08,340
on how large these accelerations i

674
00:48:08,430 --> 00:48:10,000
that we experience

675
00:48:10,820 --> 00:48:13,970
in twenty six one hundred and that's part was the one

676
00:48:14,050 --> 00:48:16,360
that is due to the earth rotation

677
00:48:16,410 --> 00:48:19,340
he is the earth

678
00:48:19,340 --> 00:48:22,910
rotating with angular velocity omega

679
00:48:24,510 --> 00:48:26,220
here is the creator

680
00:48:26,230 --> 00:48:30,150
and have had a certain radius

681
00:48:31,140 --> 00:48:34,020
of the this is the symbol for

682
00:48:34,050 --> 00:48:37,380
i don't have twenty six one hundred is here but let's just take the worst

683
00:48:38,400 --> 00:48:39,840
that you're on the equator

684
00:48:42,070 --> 00:48:46,240
you go around like this in order to do that you need the centripetal acceleration

685
00:48:46,270 --> 00:48:47,650
a c

686
00:48:47,650 --> 00:48:49,720
which as we have seen last time

687
00:48:49,760 --> 00:48:52,530
it was only because we are

688
00:48:52,550 --> 00:48:54,170
how large is that one

689
00:48:54,890 --> 00:48:57,340
the period of rotation for the earth

690
00:48:58,150 --> 00:49:01,020
twenty four hours

691
00:49:01,880 --> 00:49:04,130
thirty six hundred seconds

692
00:49:04,170 --> 00:49:06,090
so omega

693
00:49:06,090 --> 00:49:07,590
because two pi

694
00:49:07,590 --> 00:49:09,850
divided by twenty four

695
00:49:09,910 --> 00:49:14,040
nine thirty six hundred and that we can be in radiance

696
00:49:14,050 --> 00:49:15,330
per second

697
00:49:15,340 --> 00:49:20,300
and so you can calculate now what omega square are surface if you know

698
00:49:20,350 --> 00:49:22,410
at the radius of the earth

699
00:49:22,470 --> 00:49:24,280
is about six thousand

700
00:49:24,330 --> 00:49:26,090
four hundred kilometres

701
00:49:26,140 --> 00:49:27,220
make sure

702
00:49:27,290 --> 00:49:30,360
you convert this to me is of course

703
00:49:30,380 --> 00:49:31,780
you will find them

704
00:49:31,790 --> 00:49:36,720
that the centripetal acceleration at the equator which is the worst case is less here

705
00:49:36,840 --> 00:49:39,670
o point o three four

706
00:49:40,760 --> 00:49:41,980
the second square

707
00:49:42,090 --> 00:49:44,730
and this is way way less

708
00:49:44,780 --> 00:49:47,490
this is three hundred times smaller

709
00:49:47,540 --> 00:49:50,910
then the gravitational acceleration that your experience here

710
00:49:51,790 --> 00:49:54,840
and if we take the motion of the earth around the sun

711
00:49:54,900 --> 00:49:58,230
then there is an additional factor of five times slower

712
00:49:58,390 --> 00:50:02,840
in other words these accelerations even though they real and they can be measured easily

713
00:50:03,450 --> 00:50:04,800
today's high-tech

714
00:50:04,830 --> 00:50:06,100
in some entation

715
00:50:06,110 --> 00:50:09,530
they are much much slower than what we used to

716
00:50:09,540 --> 00:50:14,270
which is the gravitational acceleration and therefore in spite of these accelerations

717
00:50:14,280 --> 00:50:16,890
we will it's except this whole

718
00:50:16,900 --> 00:50:19,220
as a reasonably good

719
00:50:20,110 --> 00:50:23,280
frame of reference in which the first law

720
00:50:23,330 --> 00:50:26,150
then it should hold

721
00:50:26,320 --> 00:50:27,540
can newton's law

722
00:50:27,570 --> 00:50:29,030
we proved

723
00:50:29,040 --> 00:50:30,150
the answer is

724
00:50:31,110 --> 00:50:32,910
because it's impossible

725
00:50:33,780 --> 00:50:35,070
be sure

726
00:50:35,080 --> 00:50:36,660
your reference frame

727
00:50:36,670 --> 00:50:38,400
it without any acceleration

728
00:50:38,410 --> 00:50:39,720
i do we believe in is

729
00:50:39,720 --> 00:50:41,130
yes we do

730
00:50:41,170 --> 00:50:45,860
we believe it since it is consistent within the uncertainty of the measurement was all

731
00:50:47,970 --> 00:50:51,220
i have been done

732
00:50:51,220 --> 00:50:52,380
now we come

733
00:50:52,420 --> 00:50:53,450
to the

734
00:50:53,490 --> 00:50:54,840
the second floor

735
00:50:54,890 --> 00:50:58,640
newton's second law

736
00:50:58,650 --> 00:50:59,360
i have

737
00:50:59,380 --> 00:51:02,340
the spring

738
00:51:04,790 --> 00:51:07,450
get gravity for now you can do it somewhere in outer space

739
00:51:07,460 --> 00:51:09,730
this is the relaxed length of the spring

740
00:51:09,730 --> 00:51:15,190
images the images are taken independently you know and you want to

741
00:51:15,190 --> 00:51:20,050
two to build the probability probability model for these images

742
00:51:20,070 --> 00:51:28,570
this is what people in optimisation called the sample average approach however the real situation

743
00:51:28,610 --> 00:51:34,650
many combinatorial optimization problems is that the definition of your problem instances noisy

744
00:51:34,690 --> 00:51:42,070
you most get one or two cases so maybe ten

745
00:51:42,090 --> 00:51:47,480
but there is much more to be learned about local smoothness and segment statistics from

746
00:51:47,480 --> 00:51:50,800
a very small number of of of

747
00:51:50,820 --> 00:51:57,840
of of let's say images so that you are forced to use a you know

748
00:51:57,840 --> 00:52:05,440
statistical relations between neighbouring feature descriptors in order to build up your model

749
00:52:05,460 --> 00:52:09,130
and i think that that is one of the challenges at least i perceived so

750
00:52:09,130 --> 00:52:14,190
far if you have a setting where objects are independent then the whole bag of

751
00:52:14,190 --> 00:52:21,500
statistical learning theory volvo like you know you have you have large deviation recites you

752
00:52:21,500 --> 00:52:26,780
have IID contributions of your empirical risks to build up your costs and then you

753
00:52:26,780 --> 00:52:33,800
can easily define what you mean by expected costs and so on and then couple

754
00:52:34,030 --> 00:52:36,860
it's no longer so clear

755
00:52:36,880 --> 00:52:40,780
it's probably one of can discuss later

756
00:52:42,460 --> 00:52:46,900
OK so so maybe this example also makes it clear

757
00:52:46,920 --> 00:52:52,050
what at least one possible road is i'm not saying this is the only one

758
00:52:52,050 --> 00:52:59,110
but what one possible wrote two to learning the more complicated setting here you have

759
00:52:59,130 --> 00:53:06,920
you have protein data they are pairwise compared you get these these these sort of

760
00:53:06,920 --> 00:53:14,130
these little bit structure but otherwise randomly looking matrix here is symmetric so some of

761
00:53:14,130 --> 00:53:19,150
the structure is coming from symmetry here if you permute you're you're you're columns and

762
00:53:19,150 --> 00:53:22,190
you're rose then you clearly see that you have

763
00:53:22,250 --> 00:53:29,210
big groups in here three and you want to find these these groups for example

764
00:53:29,210 --> 00:53:36,840
data are abundant here is an example from biology molecular biology we have first use

765
00:53:36,840 --> 00:53:40,130
that falling up the the

766
00:53:40,150 --> 00:53:45,440
research by gaming gaming coffin you dont we have used it for these landsat imagery

767
00:53:45,550 --> 00:53:50,300
basically what you do is you have a and a local image patch here you

768
00:53:50,320 --> 00:53:57,000
have look image patch here you get some empirical distribution of your features in that

769
00:53:57,000 --> 00:54:02,340
area and that area then you apply a statistical test whether these two area as

770
00:54:02,340 --> 00:54:09,980
below come from the same source are empirical realizations of these cells and if if

771
00:54:10,000 --> 00:54:15,000
if a high square statistics for example tells you these are very similar then you

772
00:54:15,000 --> 00:54:18,610
should have a small the value otherwise you get the large one and then you

773
00:54:18,610 --> 00:54:24,360
have a pairwise clustering problem and this is the the ultimate labeling of three classes

774
00:54:25,280 --> 00:54:30,840
now it's not so clear how you will generalize from this solution we basically have

775
00:54:30,840 --> 00:54:37,110
a bunch of of simon variables of all my different local image patches with the

776
00:54:37,130 --> 00:54:42,090
local statistics extracted there how i would i would use how i would use the

777
00:54:42,090 --> 00:54:45,500
solution to my training image and then sort of

778
00:54:45,500 --> 00:54:51,050
just copy it to the test image without actually optimizing on the test image from

779
00:54:55,250 --> 00:54:56,960
and i should also say

780
00:54:56,960 --> 00:55:02,520
in that setting you know you compare all possible image pairs here so you want

781
00:55:02,520 --> 00:55:06,440
to take a one up here and compare it to that one down here so

782
00:55:06,500 --> 00:55:11,860
all these all these these sites that come between each other

783
00:55:12,090 --> 00:55:17,900
now the cost function is sort of rebuild according to k means for that for

784
00:55:17,900 --> 00:55:23,380
that new data type basically what you do is you some up the dissimilarities between

785
00:55:23,380 --> 00:55:28,170
object i and j whenever both objects belong to the same cluster so that's the

786
00:55:28,170 --> 00:55:32,570
compactness criterion which also have been in k means when they belong to the same

787
00:55:32,570 --> 00:55:34,270
european commissioner for

788
00:55:34,750 --> 00:55:39,530
two hundred forty million network called messenger communication over one month

789
00:55:39,540 --> 00:55:42,520
microsoft messenger right scale structure

790
00:55:43,560 --> 00:55:48,620
global population but of course this not just that it's just

791
00:55:48,640 --> 00:55:50,440
making this

792
00:55:50,470 --> 00:55:51,660
thank you

793
00:55:51,680 --> 00:55:55,860
there's really much more so the question is how does massive network data compare small-scale

794
00:55:56,790 --> 00:55:59,560
and the newly appointed intrinsic

795
00:55:59,570 --> 00:56:01,940
july KDD efforts

796
00:56:01,940 --> 00:56:06,620
inference for example you see

797
00:56:06,650 --> 00:56:08,540
this is necessary you

798
00:56:08,700 --> 00:56:10,850
more in line

799
00:56:10,870 --> 00:56:12,530
right so more

800
00:56:12,540 --> 00:56:14,180
it can be observed

801
00:56:14,190 --> 00:56:15,650
really close the

802
00:56:15,810 --> 00:56:20,830
that are truly there are literally invisible at at smaller scales rates and

803
00:56:20,840 --> 00:56:21,960
the point of view

804
00:56:21,980 --> 00:56:24,200
talk something where

805
00:56:24,220 --> 00:56:26,620
you can see what you have a hundred million people

806
00:56:26,640 --> 00:56:27,370
thank you

807
00:56:27,390 --> 00:56:30,070
you can see thirty four

808
00:56:30,100 --> 00:56:31,950
ferrari two hundred people

809
00:56:33,700 --> 00:56:35,560
at the same time less

810
00:56:35,570 --> 00:56:38,060
namely the security

811
00:56:38,070 --> 00:56:40,970
according to this he knew exactly who these people were

812
00:56:40,980 --> 00:56:47,210
what is social ties is able to interpret the data and get some information processes

813
00:56:47,230 --> 00:56:49,650
the rivalry is decision

814
00:56:49,820 --> 00:56:51,150
the groups and so itself

815
00:56:51,940 --> 00:56:53,340
well we got a much larger scale

816
00:56:53,350 --> 00:56:55,940
i don't see any one of these

817
00:56:55,960 --> 00:56:58,640
because of the the human inspection and so on

818
00:56:58,650 --> 00:57:00,780
a lot to

819
00:57:00,790 --> 00:57:05,650
it's much harder to pose nuanced questions and so i think one of the big

820
00:57:05,650 --> 00:57:06,570
challenge area

821
00:57:06,610 --> 00:57:10,490
it's really try to find the point where these two things beta or

822
00:57:10,500 --> 00:57:12,730
OK we have to

823
00:57:12,740 --> 00:57:15,330
tracks social research one

824
00:57:15,350 --> 00:57:21,290
very complex questions about small groups focused on processes and outcomes and there really is

825
00:57:21,320 --> 00:57:25,310
measuring massive data mining is not significant

826
00:57:25,320 --> 00:57:27,940
but when trying to get back to the question

827
00:57:27,960 --> 00:57:30,450
in the inference process model

828
00:57:31,620 --> 00:57:33,370
and so that he

829
00:57:33,400 --> 00:57:34,690
an issue

830
00:57:34,750 --> 00:57:35,820
the tag

831
00:57:36,810 --> 00:57:37,720
in addressing

832
00:57:40,100 --> 00:57:41,450
the methodology

833
00:57:41,500 --> 00:57:43,490
come come into play the

834
00:57:43,500 --> 00:57:46,220
this is also kind of talk

835
00:57:49,000 --> 00:57:50,850
now data is really much

836
00:57:50,930 --> 00:57:55,440
don't try to draw pictures of nodes connected to

837
00:57:55,450 --> 00:57:57,060
connected to

838
00:57:57,520 --> 00:57:59,490
much much more

839
00:57:59,500 --> 00:58:01,390
in this context and

840
00:58:01,410 --> 00:58:03,710
and what can be can be free

841
00:58:03,720 --> 00:58:11,470
second the first half the talk is going to be the most models and conditional

842
00:58:11,470 --> 00:58:14,210
basic vocabulary for expressing

843
00:58:14,230 --> 00:58:16,320
complex of expressions and complex

844
00:58:16,360 --> 00:58:18,600
it's a wonderful life data

845
00:58:24,400 --> 00:58:25,640
this created

846
00:58:25,650 --> 00:58:30,600
i think in some of the questions of really interesting social networks and large

847
00:58:30,620 --> 00:58:32,610
there train something that

848
00:58:32,620 --> 00:58:37,330
into the system that we can do and think about all the issues that come

849
00:58:37,330 --> 00:58:39,310
into play when you deal with large

850
00:58:39,320 --> 00:58:41,270
ten years

851
00:58:41,280 --> 00:58:43,080
is found in this certain

852
00:58:43,100 --> 00:58:49,220
data which are really in some cases particularly extreme context right question whom

853
00:58:49,240 --> 00:58:50,650
the fantasy

854
00:58:51,290 --> 00:58:52,800
data going to

855
00:58:52,810 --> 00:58:58,030
and that's then roughly following talks for this happen is more expression

856
00:58:58,050 --> 00:59:02,730
OK and starring

857
00:59:02,770 --> 00:59:05,390
three one example

858
00:59:05,400 --> 00:59:07,430
you have to talk about

859
00:59:07,460 --> 00:59:12,920
for small social networks for an analysis this

860
00:59:12,950 --> 00:59:14,140
or between

861
00:59:14,580 --> 00:59:17,250
the basic social questions moving into

862
00:59:17,310 --> 00:59:21,810
interesting mathematical models and as because some valid

863
00:59:22,830 --> 00:59:24,660
it relates more closely

864
00:59:24,670 --> 00:59:30,990
the story is possible and also raise more conceptually difficult questions

865
00:59:31,010 --> 00:59:33,480
possible sources

866
00:59:36,330 --> 00:59:37,860
three slides but this

867
00:59:37,870 --> 00:59:40,150
so this is my work

868
00:59:40,200 --> 00:59:43,750
so social psychologist in the the nineteen sixties experiment the

869
00:59:43,850 --> 00:59:46,970
six degrees of separation experiments many maybe

870
00:59:46,990 --> 00:59:49,240
for you may not

871
00:59:49,260 --> 00:59:51,510
not understand with all

872
00:59:51,520 --> 00:59:57,200
six inches apart in societal small energy difference friend of a friend of a friend

873
00:59:57,200 --> 00:59:57,970
of yours be honest

874
00:59:58,140 --> 01:00:01,160
in all he developed experimental

875
01:00:01,170 --> 01:00:03,560
now in which she picked up

876
01:00:03,730 --> 01:00:08,850
for reasons of cost sharing messages etc it's and trying to make sure that

877
01:00:08,880 --> 01:00:12,390
effect of the people

878
01:00:12,400 --> 01:00:14,340
at random

879
01:00:14,350 --> 01:00:15,740
citizen kane son

880
01:00:15,770 --> 01:00:17,720
no trespassing

881
01:00:18,920 --> 01:00:21,340
all right that seems to form

882
01:00:21,440 --> 01:00:24,930
just this and this the same trying this

883
01:00:24,930 --> 01:00:25,900
the story

884
01:00:25,910 --> 01:00:29,990
we sent this letter to a friend someone you know is this

885
01:00:30,010 --> 01:00:34,790
with the goal of regions named individuals on the target perspective

886
01:00:34,870 --> 01:00:40,800
the name address patients personal data has some reference for finding but you can be

887
01:00:40,800 --> 01:00:42,830
able to read militants

888
01:00:42,850 --> 01:00:48,400
for with the goal of this process can now

889
01:00:48,410 --> 01:00:54,180
so so far this a different from something we study which again

890
01:00:54,200 --> 01:00:57,180
typical hundreds of studies of social networks in this

891
01:00:57,640 --> 01:01:02,560
this small contains just because is you want to study the global financial release the

892
01:01:02,560 --> 01:01:04,190
french in their entirety

893
01:01:04,200 --> 01:01:08,190
united states can help to measure and so in a sense

894
01:01:08,200 --> 01:01:15,920
stroboscopic method methodology directly after this industry is led to the top of the tree

895
01:01:15,990 --> 01:01:17,260
and the

896
01:01:17,270 --> 01:01:22,440
definitions of medium was far me was

897
01:01:23,560 --> 01:01:24,530
six chain

898
01:01:24,550 --> 01:01:32,330
natural plant sixty four changed completing the number sixteen separation but interesting

899
01:01:32,340 --> 01:01:33,580
of course

900
01:01:33,580 --> 01:01:38,970
if you want to formalise your prior knowledge rate is five

901
01:01:39,010 --> 01:01:46,240
ah now centred at the end with the edge weights change to what they plus

902
01:01:46,240 --> 01:01:50,640
the number of times you cross that it just the same as the irish like

903
01:01:51,330 --> 01:01:54,310
and if you wanted to read about resources

904
01:01:54,430 --> 01:01:59,540
the was in the paper and it was so clear that any and all sins

905
01:01:59,770 --> 01:02:02,790
a couple of years ago bayesian analysis for

906
01:02:02,790 --> 01:02:04,930
markov chains so

907
01:02:04,990 --> 01:02:08,150
the next transparency

908
01:02:11,820 --> 01:02:12,790
the store

909
01:02:14,130 --> 01:02:15,150
go through

910
01:02:15,160 --> 01:02:22,030
mixtures of conjugate priors are dense it's easy to simulate from prior posterior by using

911
01:02:22,070 --> 01:02:27,120
random walk with reinforcement that is very easy to run this random walk with reinforcement

912
01:02:27,180 --> 01:02:33,530
just start off new london and that the proportion of times that you cover image

913
01:02:36,400 --> 01:02:38,430
to the to the prior

914
01:02:38,480 --> 01:02:42,540
the probability of an edge i just i just wrote down it's very convenient the

915
01:02:42,540 --> 01:02:49,600
connection between random walk with reinforcement and and this conjugate prior analysis is convenient computationally

916
01:02:49,610 --> 01:02:57,390
computationally this w johnson style of characterization so and haven't told you about it and

917
01:02:57,390 --> 01:02:58,220
this is the wrong

918
01:02:58,340 --> 01:03:03,550
first the talk so we want is what is this five and there's a reason

919
01:03:03,550 --> 01:03:07,980
for my showing it to you to come from x transparency

920
01:03:07,990 --> 01:03:13,100
OK now is

921
01:03:13,130 --> 01:03:18,410
if x is a vector indexed by the edges of the graph is the edge

922
01:03:19,610 --> 01:03:24,890
the original weights of the graph

923
01:03:25,000 --> 01:03:29,460
everything looks kind nice was the product of city that

924
01:03:29,470 --> 01:03:35,080
the rate of descent understanding the notation

925
01:03:35,100 --> 01:03:36,230
six v

926
01:03:36,230 --> 01:03:38,030
it is the sum of x

927
01:03:38,050 --> 01:03:40,570
so in this way leads out of the

928
01:03:40,600 --> 01:03:46,570
so those exhibitions in the are in fact sums of the edge went to

929
01:03:47,260 --> 01:03:51,500
strange sort of sort of the way it should be whatever that means that strange

930
01:03:51,630 --> 01:03:56,830
is that the term and this a conjugate prior what's the square root of the

931
01:03:56,830 --> 01:03:58,470
determinant going on

932
01:03:58,480 --> 01:04:04,330
going on there about ten years so it's the determinant of matrix indexed by the

933
01:04:04,690 --> 01:04:09,470
edges and a and so

934
01:04:09,530 --> 01:04:17,610
well no it's indexed by the homology of the graph so i the dyad

935
01:04:17,630 --> 01:04:22,140
sons around cycles of x being

936
01:04:22,180 --> 01:04:26,120
and the i assigned around cycles with signs on them

937
01:04:26,550 --> 01:04:30,610
OK let me try to explain that got this graph

938
01:04:35,780 --> 01:04:41,880
have to choose the basis for the cycle space and is the standard

939
01:04:41,900 --> 01:04:47,970
the minimum spanning tree of graph that the tree and its that's not tree if

940
01:04:47,970 --> 01:04:52,550
you have a cycle if you take all the edges that are tree

941
01:04:53,810 --> 01:04:57,540
and take them out one at a time that is the basis for the set

942
01:04:57,540 --> 01:04:59,870
of cycles in the graph and

943
01:05:00,020 --> 01:05:03,300
the size and

944
01:05:03,310 --> 01:05:06,800
you've got to orient the cycles it doesn't matter how we you have to into

945
01:05:06,920 --> 01:05:10,570
the standard problem that you have to do and

946
01:05:10,780 --> 01:05:15,420
just as a plus sign if the orientation of edges the same in the two

947
01:05:15,420 --> 01:05:18,910
cycles and the minus sign otherwise

948
01:05:18,920 --> 01:05:23,490
the only thing that was so the normalizing constant of you know as you know

949
01:05:23,490 --> 01:05:29,090
if you try to compare two different markov models no normalizing constant is an enormous

950
01:05:29,090 --> 01:05:31,830
benefit in computational and of

951
01:05:31,850 --> 01:05:36,610
bayesian statistics so we can resolve this

952
01:05:36,620 --> 01:05:39,990
but i think you need to know

953
01:05:40,000 --> 01:05:43,290
direct apology comes in the probability

954
01:05:43,330 --> 01:05:49,840
and is now trying to come into statistics are are the

955
01:05:49,870 --> 01:05:54,910
first the first multi group of the graph is just an integral part of this

956
01:05:55,010 --> 01:05:58,860
because i know that when i was a graduate student i can recognise when i

957
01:05:58,860 --> 01:05:59,540
met it

958
01:05:59,540 --> 01:06:01,750
in this problem and so

959
01:06:01,760 --> 01:06:07,430
that means that

960
01:06:07,460 --> 01:06:11,320
what next transparency OK

961
01:06:11,340 --> 01:06:15,720
next to step

962
01:06:19,730 --> 01:06:26,210
this is a test for processes markov chain OK if you do this in tests

963
01:06:26,210 --> 01:06:31,530
you put a prior on the markov chain you can compute posterior is and i

964
01:06:31,530 --> 01:06:36,680
would not have test to markov chain versus independent because i could put a prior

965
01:06:36,680 --> 01:06:44,560
on independent reasons space factors it's natural to test the markov chain something

966
01:06:44,600 --> 01:06:50,990
with a complicated dependence for example second order markov markov something like that

967
01:06:51,150 --> 01:06:55,160
so the problem becomes hard to put a prior on

968
01:06:55,470 --> 01:06:59,270
for markov chains are we talk about article to turn

969
01:06:59,600 --> 01:07:00,810
now i see

970
01:07:00,810 --> 01:07:04,700
of the maximum likelihood estimators not some sort of derivation

971
01:07:04,720 --> 01:07:08,630
that they are the correct ones

972
01:07:08,670 --> 01:07:11,590
OK so here is an example of how this works

973
01:07:11,650 --> 01:07:14,630
suppose we consider an exponential

974
01:07:15,690 --> 01:07:18,470
so i've written in here with a random variable t

975
01:07:18,490 --> 01:07:20,220
so that might represent some

976
01:07:20,230 --> 01:07:21,490
decay time

977
01:07:21,500 --> 01:07:23,060
and the parameter tau

978
01:07:23,070 --> 01:07:26,040
represents the mean lifetime

979
01:07:26,060 --> 01:07:28,830
and suppose you can measure and

980
01:07:28,990 --> 01:07:32,160
life times and values of t

981
01:07:32,180 --> 01:07:36,140
right so the likelihood function is the a function of this parameter tau

982
01:07:36,160 --> 01:07:39,660
and that is simply given by the product over all the elements

983
01:07:39,670 --> 01:07:40,960
of the data sample

984
01:07:41,110 --> 01:07:44,920
and then what i have is the PDF each time evaluated with the key piece

985
01:07:44,920 --> 01:07:48,570
of i and so i simply have to find the value of tau

986
01:07:48,590 --> 01:07:51,120
which maximizes that function

987
01:07:51,130 --> 01:07:52,560
and that's easy to do

988
01:07:52,610 --> 01:07:57,680
as i mentioned it's much easier to work with the log of the likelihood function

989
01:07:57,730 --> 01:08:01,190
and you can see why that is because when you take the logarithm of that

990
01:08:01,190 --> 01:08:05,080
product to get gets converted into a sum of logs

991
01:08:05,140 --> 01:08:10,340
right so it is it's numerically easier to deal with some of the products and

992
01:08:10,340 --> 01:08:15,300
furthermore if the PDF contains an exponential terms than when you take the logarithm that

993
01:08:15,300 --> 01:08:16,810
exponential term goes away

994
01:08:16,890 --> 01:08:20,510
so that is the expression for the log likelihood function

995
01:08:20,510 --> 01:08:22,830
so i simply have to differentiate

996
01:08:22,850 --> 01:08:24,820
that with respect to how

997
01:08:24,830 --> 01:08:26,750
and set it equal to zero

998
01:08:26,760 --> 01:08:28,030
and so for tau

999
01:08:28,040 --> 01:08:31,570
and when i find the solution i write it with the perhaps because they had

1000
01:08:31,630 --> 01:08:36,340
represents the estimator and in this particular case the function happens to turn out to

1001
01:08:37,160 --> 01:08:41,500
the arithmetic average of the individual t values so that's

1002
01:08:41,500 --> 01:08:48,010
very special simple case i mean this is not always so simple

1003
01:08:48,020 --> 01:08:52,250
OK so to illustrate how that works i again wrote a little monte carlo programme

1004
01:08:52,250 --> 01:08:53,650
which generated

1005
01:08:53,670 --> 01:08:54,970
fifty values

1006
01:08:54,970 --> 01:08:59,980
according to an exponential distribution given the technology that we learned yesterday are two days

1007
01:08:59,980 --> 01:09:03,720
ago you could easily do that so those tick marks represent the

1008
01:09:04,800 --> 01:09:06,060
p values

1009
01:09:06,080 --> 01:09:10,980
and i use those in my maximum likelihood estimator of theta generate i used the

1010
01:09:10,980 --> 01:09:13,660
true value of tau equals one

1011
01:09:13,680 --> 01:09:19,770
and my maximum likelihood estimator lo and behold comes out very close to one

1012
01:09:21,100 --> 01:09:23,080
so that's a very simple example

1013
01:09:24,580 --> 01:09:25,940
the estimated value

1014
01:09:25,970 --> 01:09:29,310
and the true value of course are never exactly equal

1015
01:09:29,320 --> 01:09:33,380
because the data are subject to random fluctuations

1016
01:09:33,400 --> 01:09:34,320
and so on

1017
01:09:34,390 --> 01:09:36,250
i need to somehow

1018
01:09:38,050 --> 01:09:43,400
the statistical uncertainty in my estimate the uncertainty that arises from the fact that i

1019
01:09:43,400 --> 01:09:45,280
have a finite data sample

1020
01:09:47,000 --> 01:09:48,690
that leads to general

1021
01:09:48,710 --> 01:09:50,860
topic of the variance of

1022
01:09:51,820 --> 01:09:56,000
and there are a number of important ways of

1023
01:09:56,010 --> 01:10:00,740
determining the standard deviation variance of these estimators

1024
01:10:00,760 --> 01:10:02,530
so we'll discuss

1025
01:10:02,550 --> 01:10:05,010
three or four waves

1026
01:10:05,030 --> 01:10:08,160
the most straight forward way conceptually

1027
01:10:08,210 --> 01:10:14,180
would be following what is the standard deviation of an estimator represents how much

1028
01:10:14,230 --> 01:10:19,890
estimators value would fluctuate upon repetition of the measurement

1029
01:10:20,000 --> 01:10:24,650
so find let's write a monte carlo programme to simulate the entire measurement and then

1030
01:10:24,650 --> 01:10:27,070
repeat that a large number of times

1031
01:10:28,390 --> 01:10:31,160
that's what i did i

1032
01:10:31,180 --> 01:10:32,610
generated somehow

1033
01:10:32,620 --> 01:10:37,260
and a large number of times the entire sample of fifty

1034
01:10:37,270 --> 01:10:40,820
measured values coarsest different sample each time

1035
01:10:40,830 --> 01:10:45,220
for each repeated experiment you get a value of tau had you put that in

1036
01:10:45,220 --> 01:10:47,110
a in a histogram

1037
01:10:47,130 --> 01:10:50,920
and after some large number of experiments you get this histogram here

1038
01:10:50,930 --> 01:10:56,000
so then if you just compute the standard deviation of those towers values you get

1039
01:10:57,020 --> 01:10:58,560
the statistical error

1040
01:10:58,580 --> 01:11:00,030
of of tau

1041
01:11:00,040 --> 01:11:04,290
and i like this this example because it illustrates

1042
01:11:04,320 --> 01:11:09,240
most directly exactly what you mean by this standard deviation of an estimator it's the

1043
01:11:09,240 --> 01:11:14,020
level of how much the value would be distributed how widely the value would be

1044
01:11:14,020 --> 01:11:17,890
distributed upon repetition of the entire experiment

1045
01:11:17,920 --> 01:11:20,890
you get some value sigma had of tau

1046
01:11:20,980 --> 01:11:22,740
is o point

1047
01:11:22,810 --> 01:11:23,980
one five

1048
01:11:25,890 --> 01:11:30,710
you can notice this distribution here this is roughly gaussians shape it sort of has

1049
01:11:30,710 --> 01:11:31,790
the bell shape

1050
01:11:33,180 --> 01:11:39,280
that's kind of interesting because the original distribution of p values is exponential

1051
01:11:39,320 --> 01:11:42,720
so can we understand roughly wide that gaussians

1052
01:11:42,720 --> 01:11:47,310
well that's actually pretty easy because the estimator itself was the average so it is

1053
01:11:48,360 --> 01:11:49,660
to the sum

1054
01:11:49,680 --> 01:11:54,250
of those random variables and so recall from the central limit theorem the central limit

1055
01:11:54,250 --> 01:11:56,480
theorem said that if you take the sum

1056
01:11:56,500 --> 01:12:01,280
of a sum of random variables as long as there are sufficiently large number of

1057
01:12:01,280 --> 01:12:02,770
terms in that some

1058
01:12:02,780 --> 01:12:06,060
then it's distribution the distribution of the sum

1059
01:12:06,080 --> 01:12:08,510
becomes gaussians

1060
01:12:08,510 --> 01:12:10,630
and that's exactly what we're seeing here

1061
01:12:10,660 --> 01:12:16,460
that's that's almost always true for maximum likelihood estimators as long as you have a

1062
01:12:16,460 --> 01:12:18,310
sufficiently large number of

1063
01:12:19,100 --> 01:12:22,920
measurements in your data sample

1064
01:12:25,580 --> 01:12:29,360
the monte carlo method is a little bit difficult in the sense that you actually

1065
01:12:29,360 --> 01:12:33,250
have to sit down and write a computer program and usually you make lots of

1066
01:12:33,250 --> 01:12:37,890
measurements and actually have to write monte carlo programme just to analyse the error for

1067
01:12:37,890 --> 01:12:41,730
each that would be a lot of extra effort

1068
01:12:41,730 --> 01:12:45,880
so fortunately there are some approximate methods that

1069
01:12:45,900 --> 01:12:47,270
are much faster

1070
01:12:47,310 --> 01:12:53,160
and one important one is based on an inequality called the information inequality is sometimes

1071
01:12:53,160 --> 01:12:54,330
called the

1072
01:12:54,350 --> 01:13:00,280
the whole come out fresh or cf inequality but anyway never mind the name

1073
01:13:00,280 --> 01:13:06,840
the inequality sets a lower bound on the variance of any estimator so this is

1074
01:13:06,840 --> 01:13:11,630
love this inequality applies not only to maximum likelihood estimators but estimators

1075
01:13:11,770 --> 01:13:12,790
but you derive

1076
01:13:12,800 --> 01:13:14,940
in any way

1077
01:13:14,960 --> 01:13:19,080
and what it says is that the variance of any estimator theta hamas must be

1078
01:13:19,080 --> 01:13:22,120
greater than or equal to this expression here

1079
01:13:22,150 --> 01:13:23,650
b is the bias

1080
01:13:23,670 --> 01:13:28,420
and here i have the expectation value of the set negative second derivative

1081
01:13:28,440 --> 01:13:31,060
of the log likelihood function

1082
01:13:31,860 --> 01:13:36,680
in many cases the biases either identically zero or is very small

1083
01:13:36,690 --> 01:13:39,540
so very often that there can be neglected

1084
01:13:39,550 --> 01:13:42,070
so that means that furthermore

1085
01:13:42,080 --> 01:13:44,010
even though it is an inequality

1086
01:13:44,040 --> 01:13:45,440
in many

1087
01:13:45,460 --> 01:13:47,510
problems of practical interest

1088
01:13:47,590 --> 01:13:49,580
quality almost holds

1089
01:13:50,790 --> 01:13:55,610
so in in many cases we can simply approximate the variance of data had by

1090
01:13:55,610 --> 01:13:57,810
this expression here minus one

1091
01:13:57,820 --> 01:14:03,660
over the expectation value of the second derivative of the log likelihood function

1092
01:14:03,710 --> 01:14:07,820
now find what we really want is not the variance of the

1093
01:14:07,850 --> 01:14:10,390
estimator of a parameter we want

1094
01:14:11,300 --> 01:14:14,780
of the variance of the estimator of a parameter that is to say we really

1095
01:14:14,780 --> 01:14:16,220
and it takes

1096
01:14:16,240 --> 01:14:19,220
the set of stocks simulate

1097
01:14:19,280 --> 01:14:23,430
a big figure number this is play but for a bunch of graphs

1098
01:14:23,470 --> 01:14:25,860
i want to say what graph it is

1099
01:14:25,930 --> 01:14:30,410
and whether or not i believe in big mo

1100
01:14:30,430 --> 01:14:34,010
it sets the mean to zero

1101
01:14:34,030 --> 01:14:37,430
and then for as in the stocks it moves it

1102
01:14:37,470 --> 01:14:40,430
giving it the bias the momentum

1103
01:14:40,450 --> 01:14:43,700
and then it shows the history

1104
01:14:43,720 --> 01:14:45,470
and then computes the mean

1105
01:14:46,860 --> 01:14:49,800
getting me the mean of all the stocks and we've seen this sort of thing

1106
01:14:49,800 --> 01:14:52,050
many times before

1107
01:14:52,090 --> 01:14:54,240
and then got some constants

1108
01:14:54,260 --> 01:14:57,800
by the way i want to emphasise that i've name these constants to make it

1109
01:14:57,800 --> 01:14:59,320
easier to change

1110
01:14:59,320 --> 01:15:01,780
starting with twenty stocks

1111
01:15:01,820 --> 01:15:04,660
a hundred days

1112
01:15:05,090 --> 01:15:07,820
and then what i do is i stocks someone

1113
01:15:07,860 --> 01:15:09,890
stocks one

1114
01:15:09,930 --> 01:15:13,200
will be empty list stocks two is the empty list

1115
01:15:13,240 --> 01:15:21,530
why do you think i'm starting with bias is zero

1116
01:15:23,180 --> 01:15:25,760
what do you think the mean should be

1117
01:15:25,780 --> 01:15:30,510
if i simulate various things that the bias is zero

1118
01:15:30,530 --> 01:15:33,910
i started one hundred dollars is the average price of the stock

1119
01:15:33,950 --> 01:15:36,640
what the average value of the property

1120
01:15:36,680 --> 01:15:40,840
if my code is correct which is the average price p after say a hundred

1121
01:15:42,010 --> 01:15:44,740
if there is no bias

1122
01:15:46,030 --> 01:15:48,240
one hundred exactly

1123
01:15:48,260 --> 01:15:51,070
since there is no upward downward bias

1124
01:15:51,070 --> 01:15:54,590
they may fluctuate widely but if i look at enough stocks

1125
01:15:54,590 --> 01:15:58,090
the average should be right around one hundred

1126
01:15:58,140 --> 01:16:01,430
i don't know what the average would be if i chose different bias

1127
01:16:01,490 --> 01:16:03,680
a little bit complicated

1128
01:16:03,700 --> 01:16:06,430
so i chose the simplest bias

1129
01:16:06,530 --> 01:16:11,160
important lesson so that there will be some predictability in the results

1130
01:16:11,180 --> 01:16:14,860
and i would have some if you will small test for knowing whether or not

1131
01:16:14,860 --> 01:16:16,800
i was getting

1132
01:16:16,840 --> 01:16:22,050
my code seem to be working

1133
01:16:22,110 --> 01:16:23,200
all right

1134
01:16:23,260 --> 01:16:25,820
and initially well maybe initially

1135
01:16:25,860 --> 01:16:33,890
just to be simple on the start of momentum equal to false

1136
01:16:33,890 --> 01:16:35,680
because again it seems simpler

1137
01:16:35,760 --> 01:16:38,340
have a model where there is no momentum

1138
01:16:38,360 --> 01:16:43,610
i'm looking for the simplest model possible for the first time i run it

1139
01:16:43,660 --> 01:16:48,430
and then we looked at this little loop before frying range numbers stocks

1140
01:16:48,530 --> 01:16:51,530
i'm going to create two different list stocks

1141
01:16:51,550 --> 01:16:53,160
one where the moves

1142
01:16:53,160 --> 01:16:57,680
are the distributions are chosen from a uniform and the other where there

1143
01:16:59,050 --> 01:17:01,140
some sort of curious as to

1144
01:17:01,200 --> 01:17:03,890
again which is the right way to think about this

1145
01:17:03,910 --> 01:17:08,590
all right

1146
01:17:08,610 --> 01:17:11,760
and then

1147
01:17:11,780 --> 01:17:16,510
and then just call it will see what we get

1148
01:17:16,530 --> 01:17:20,240
so let's do it let's hope that all the changes i made nine introduced syntax

1149
01:17:24,450 --> 01:17:34,160
alright aleister did something to what it did

1150
01:17:35,110 --> 01:17:37,530
the test on the left you'll remember

1151
01:17:37,570 --> 01:17:39,110
i was the one with

1152
01:17:39,110 --> 01:17:41,110
test one i believe was the

1153
01:17:41,120 --> 01:17:45,610
the uniform distribution on test two is the gaussians

1154
01:17:45,620 --> 01:17:49,610
so but let's what should we do first

1155
01:17:52,030 --> 01:17:57,220
do the smoke test number one is the mean more or less what we expected

1156
01:17:57,320 --> 01:18:01,550
well it looks like it's dead on one hundred which was our initial price

1157
01:18:01,640 --> 01:18:03,660
in test two

1158
01:18:03,660 --> 01:18:05,520
know how to solve it

1159
01:18:05,540 --> 01:18:07,030
and furthermore

1160
01:18:07,050 --> 01:18:11,880
every algorithm that follows the divide and conquer paradigm will have occurred to pretty much

1161
01:18:11,880 --> 01:18:13,240
the same form

1162
01:18:13,260 --> 01:18:17,520
very much like our good friend the master method

1163
01:18:18,940 --> 01:18:21,750
let's do it for so we sort of already know the answer

1164
01:18:21,940 --> 01:18:24,320
get a bit practice

1165
01:18:30,410 --> 01:18:34,030
this is the commercial recurrence you should know i love this recurrence because it comes

1166
01:18:34,030 --> 01:18:35,560
up all over the place

1167
01:18:35,640 --> 01:18:38,740
it comes from this

1168
01:18:38,760 --> 01:18:40,760
this general approach by

1169
01:18:40,770 --> 01:18:44,410
just seeing what are the sizes of the subproblems you're solving

1170
01:18:44,460 --> 01:18:47,540
and how many there are and how much extra work you're doing

1171
01:18:47,580 --> 01:18:49,300
so you have here

1172
01:18:49,490 --> 01:18:52,040
the size of the problems

1173
01:18:52,110 --> 01:18:57,540
it happens here the pose problems have the same size roughly

1174
01:18:58,310 --> 01:19:00,190
there's the sloppiness

1175
01:19:00,340 --> 01:19:03,940
we have which really should be t o four manova two post you've ceiling event

1176
01:19:03,940 --> 01:19:08,720
at two and when you go to recitation on friday you'll see why that's OK

1177
01:19:08,780 --> 01:19:11,760
cause ceilings dark matter this if you can prove that

1178
01:19:11,770 --> 01:19:16,570
that's happy you can assume that n is a power of two

1179
01:19:16,670 --> 01:19:20,280
OK but we're just gonna sing that for now so we just have to problems

1180
01:19:20,280 --> 01:19:22,160
of size and over the

1181
01:19:22,170 --> 01:19:23,210
two is the

1182
01:19:23,230 --> 01:19:25,210
number of problems

1183
01:19:30,630 --> 01:19:34,780
and this water and is all the extra work we're doing now

1184
01:19:34,830 --> 01:19:40,830
what's the actual work potentially conquering is always just recursions sort of no work there

1185
01:19:40,830 --> 01:19:42,980
except this we are

1186
01:19:43,030 --> 01:19:47,070
the dividing in this case is trivial but in general might involve some work the

1187
01:19:47,070 --> 01:19:49,530
combining here involves linear work

1188
01:19:49,630 --> 01:19:54,350
so this is the divide and conquer

1189
01:19:54,360 --> 01:19:56,390
running time

1190
01:19:56,510 --> 01:20:02,540
so this is the non recursive work

1191
01:20:02,550 --> 01:20:07,130
and that's generally how you convert divide-and-conquer argument recurrences really easy

1192
01:20:07,150 --> 01:20:10,450
and usually get to apply the master method here

1193
01:20:10,460 --> 01:20:14,860
we are in case

1194
01:20:16,720 --> 01:20:18,380
this is case two

1195
01:20:18,420 --> 01:20:24,220
and k is zero here so

1196
01:20:24,240 --> 01:20:28,190
in the recursion tree all of the costs are roughly the same they're all and

1197
01:20:28,190 --> 01:20:30,080
log base p of a year

1198
01:20:30,100 --> 01:20:31,860
and so the log base

1199
01:20:31,870 --> 01:20:36,380
two of two is just and so these are equal so we get an extra

1200
01:20:36,380 --> 01:20:40,490
large factor because of the number of levels and recursion tree

1201
01:20:40,580 --> 01:20:42,650
remember the intuition

1202
01:20:42,660 --> 01:20:47,060
behind the master so this is an login and that's good merge sort is a

1203
01:20:47,060 --> 01:20:50,620
fast learning algorithm and morgan social circles and squares

1204
01:20:50,720 --> 01:20:54,690
in some sense in logan is the best you can do will cover that

1205
01:20:54,700 --> 01:20:57,280
in two lectures from now but

1206
01:20:58,410 --> 01:21:03,890
today we're going to do different divide and conquer sorting is one problem is all

1207
01:21:03,890 --> 01:21:05,940
sorts of problems we might want to solve

1208
01:21:05,960 --> 01:21:08,610
so how it turns out a lot of them you can apply

1209
01:21:08,620 --> 01:21:10,270
divide and conquer to

1210
01:21:10,280 --> 01:21:13,260
not every problem

1211
01:21:13,300 --> 01:21:16,220
like how to wake up in the morning it's not so easy to solve the

1212
01:21:16,220 --> 01:21:17,960
divine comparable

1213
01:21:17,970 --> 01:21:18,900
maybe you could

1214
01:21:18,920 --> 01:21:24,590
maybe it's a good problem set problem

1215
01:21:26,380 --> 01:21:33,630
the next divide and conquer algorithm we're going to look at is even simpler

1216
01:21:34,900 --> 01:21:39,040
sorting even simpler than merge sort but it drives the whole point

1217
01:21:39,060 --> 01:21:42,350
drives home the point when you have only one some from how many people seen

1218
01:21:42,350 --> 01:21:45,140
binary search before

1219
01:21:45,150 --> 01:21:46,360
anyone hasn't

1220
01:21:49,150 --> 01:21:52,570
i will go very quickly and so you have some element x

1221
01:21:52,590 --> 01:21:56,560
you want to find x in a sorted array

1222
01:21:56,580 --> 01:22:03,290
how many people had not seen it before the sun recitation

1223
01:22:03,300 --> 01:22:05,690
no OK go visit another class

1224
01:22:05,700 --> 01:22:11,160
probably six double winners get detector the prerequisites OK

1225
01:22:11,180 --> 01:22:15,000
so i just want phrase it as the divide-and-conquer because you don't normally see it

1226
01:22:15,000 --> 01:22:15,860
that way

1227
01:22:15,880 --> 01:22:23,100
so you compare the device that is to compare acts with the middle element

1228
01:22:23,170 --> 01:22:24,690
in your right

1229
01:22:24,710 --> 01:22:28,820
then the contra step so here's your

1230
01:22:28,830 --> 01:22:31,840
here's the middle elements you compare acts

1231
01:22:31,850 --> 01:22:35,790
with this thing is that say x is smaller than the middle element and you're

1232
01:22:35,800 --> 01:22:37,060
a you know

1233
01:22:37,070 --> 01:22:40,860
the axes in the left half sort of

1234
01:22:40,870 --> 01:22:46,680
nice looking very very whatever we're just think about recursively i'm going to solve the

1235
01:22:46,680 --> 01:22:48,220
problem of finding x in this

1236
01:22:49,920 --> 01:22:55,810
so in paris in one sub

1237
01:22:55,820 --> 01:22:59,770
mike mercer we had two recursions

1238
01:22:59,790 --> 01:23:01,560
and then

1239
01:23:01,570 --> 01:23:03,860
the combined step

1240
01:23:03,870 --> 01:23:06,190
we don't do anything

1241
01:23:06,200 --> 01:23:13,700
i mean if you find that if you find x in here then you found

1242
01:23:13,700 --> 01:23:14,970
x and the whole room

1243
01:23:14,990 --> 01:23:15,940
there's nothing to

1244
01:23:15,960 --> 01:23:18,970
bring it back up really

1245
01:23:19,060 --> 01:23:23,870
so this is just raising binary search as

1246
01:23:23,890 --> 01:23:28,100
in divide-and-conquer paradigm this kind of a trivial example but there are lots of circumstances

1247
01:23:28,100 --> 01:23:30,680
really need to recurse on one side

1248
01:23:30,700 --> 01:23:34,820
and it's important to see how much of a difference

1249
01:23:34,830 --> 01:23:39,340
making one recurring versus making one two recursions can be

1250
01:23:39,350 --> 01:23:42,160
so here we have

1251
01:23:42,760 --> 01:23:45,350
this is the recurrence for binary search

1252
01:23:45,370 --> 01:23:48,260
we start with the problem size and we reduce to one

1253
01:23:48,270 --> 01:23:54,320
there is an implicit one factor here one problem subproblem size and over two

1254
01:23:54,320 --> 01:23:55,590
OK i guess we

1255
01:24:09,290 --> 01:24:12,650
OK so it's probably worth pointing out that we

1256
01:24:12,700 --> 01:24:18,220
when we compute the rademacher complexity this linear function as and also called the

1257
01:24:18,340 --> 01:24:20,070
boosting case

1258
01:24:20,080 --> 01:24:21,180
we compute its

1259
01:24:22,720 --> 01:24:27,730
the thresholding your remember i mean we didn't use the actual

1260
01:24:27,740 --> 01:24:29,960
classification of who

1261
01:24:29,970 --> 01:24:32,870
rademacher complexity of the classification functions

1262
01:24:32,880 --> 01:24:35,450
as we did with the VC class

1263
01:24:35,470 --> 01:24:37,030
we computed of this

1264
01:24:39,240 --> 01:24:41,280
past that underlies

1265
01:24:41,970 --> 01:24:44,500
the thresholding

1266
01:24:45,770 --> 01:24:48,270
and you know this would correspond in the way

1267
01:24:49,090 --> 01:24:51,360
the move from the BBC

1268
01:24:51,380 --> 01:24:53,630
sounds to the margin bounds

1269
01:24:53,640 --> 01:24:57,180
if we went to the thresholding we know that BBC

1270
01:25:09,300 --> 01:25:11,790
so we went to the

1271
01:25:11,800 --> 01:25:16,710
to these large high dimensional feature spaces we know that the

1272
01:25:16,910 --> 01:25:20,090
we see dimension it's very very high

1273
01:25:20,100 --> 01:25:21,420
and so

1274
01:25:21,470 --> 01:25:24,950
we will be lost so what we're attempting to do is to

1275
01:25:24,970 --> 01:25:30,130
this and she would be seen again and uses marginal

1276
01:25:30,140 --> 01:25:32,760
aspects to get away

1277
01:25:33,050 --> 01:25:35,950
and so we can we compute the

1278
01:25:36,510 --> 01:25:42,980
the rademacher complexity the underlying function class and we've seen that that's actually the nine

1279
01:25:42,980 --> 01:25:44,390
that's not a problem

1280
01:25:44,410 --> 01:25:47,320
what we have to do now is to get from that

1281
01:25:47,340 --> 01:25:51,580
underlying function class to the actual classification we're interested in

1282
01:25:51,600 --> 01:25:54,620
and in a way this is starting to give us a hint of how we

1283
01:25:54,620 --> 01:25:55,960
might do it

1284
01:25:56,530 --> 01:25:59,680
what we're going to do is going to take this heavy sight loss

1285
01:25:59,690 --> 01:26:01,870
and we going to approximate it with this

1286
01:26:04,330 --> 01:26:07,860
a loss hinge loss

1287
01:26:07,880 --> 01:26:10,950
and then we're going to apply the rademacher complexity to that

1288
01:26:16,550 --> 01:26:20,060
treatment on top of the underlying function class

1289
01:26:20,080 --> 01:26:22,030
so this is the sort of

1290
01:26:23,410 --> 01:26:26,110
like loss function that we apply

1291
01:26:26,260 --> 01:26:31,970
so it's a piecewise linear it starts at zero and then it goes up to

1292
01:26:33,990 --> 01:26:37,170
between minus gamma and

1293
01:26:37,190 --> 01:26:40,000
zero sum between minus cameron zero

1294
01:26:40,030 --> 01:26:44,380
it goes in a straight line up from zero to one

1295
01:26:44,390 --> 01:26:46,180
so it has slope

1296
01:26:46,190 --> 01:26:50,460
no one over gamma in that in that region

1297
01:26:51,390 --> 01:26:52,680
in that way

1298
01:26:52,690 --> 01:26:54,120
we can

1299
01:26:54,140 --> 01:26:58,180
upper bound this is the thing that was sort of interested in well without that

1300
01:26:58,180 --> 01:27:03,020
minus one that's what we're interested in that was what was on the previous slide

1301
01:27:03,030 --> 01:27:03,970
which was the

1302
01:27:03,990 --> 01:27:09,800
although the misclassification was just this expected value the heaviside function thing

1303
01:27:09,820 --> 01:27:11,580
so we can sort of

1304
01:27:11,590 --> 01:27:15,710
and all that i have added minus one will see why nominator worry about that

1305
01:27:15,710 --> 01:27:17,470
but it will go away

1306
01:27:17,860 --> 01:27:22,580
here's what i've done is shown that this age is upper bounded by the way

1307
01:27:23,450 --> 01:27:27,940
when you know the the thing makes it straight up from zero to one before

1308
01:27:27,940 --> 01:27:32,470
we get to zero the heaviside jumps up to up to one zero

1309
01:27:32,490 --> 01:27:33,350
and so

1310
01:27:33,370 --> 01:27:37,670
this is actually a larger value than this one so we can say less than

1311
01:27:37,670 --> 01:27:39,900
or equal to this but this is now

1312
01:27:39,920 --> 01:27:42,240
it is actually a smooth function

1313
01:27:43,350 --> 01:27:45,630
in fact we can therefore apply

1314
01:27:45,640 --> 01:27:48,820
the rademacher complexity to this function

1315
01:27:48,840 --> 01:27:54,710
bound so the true expectation of this function is the empirical value of its

1316
01:27:56,680 --> 01:27:58,040
estimation of its

1317
01:27:58,990 --> 01:28:02,880
plus the rademacher complexity of the underlying function class

1318
01:28:02,900 --> 01:28:04,580
plus this normal

1319
01:28:04,590 --> 01:28:08,460
so we have with the rademacher so this is applying the basic rademacher bound this

1320
01:28:10,430 --> 01:28:14,290
what is the function class of surely there should be at some b here i

1321
01:28:15,210 --> 01:28:21,120
or sorry if someone because we had a restriction on the function on the norms

1322
01:28:21,120 --> 01:28:23,300
of the weight vectors to be one

1323
01:28:23,310 --> 01:28:27,770
that's why we did that make

1324
01:28:29,750 --> 01:28:31,710
so that if someone here

1325
01:28:31,750 --> 01:28:38,670
and this is the function a minus one which is the function that we're effectively

1326
01:28:38,670 --> 01:28:42,750
applying what i mean by that is apply a two

1327
01:28:42,770 --> 01:28:44,650
and then subtract one from it

1328
01:28:44,660 --> 01:28:46,710
and f is

1329
01:28:46,880 --> 01:28:52,070
sort of this function here minus one here that OK

1330
01:28:52,090 --> 01:28:54,850
so that's basically the way

1331
01:28:54,870 --> 01:28:56,370
we're reviewing it

1332
01:28:56,390 --> 01:29:01,650
this is the composition of every function from here with this single function here

1333
01:29:01,660 --> 01:29:03,370
that's what i mean by this

1334
01:29:06,140 --> 01:29:07,310
so that's

1335
01:29:07,320 --> 01:29:09,160
so we went about to this

1336
01:29:09,170 --> 01:29:13,590
by moving to this sort of soft versions

1337
01:29:17,090 --> 01:29:20,550
the second inequality is basically just an application of the

1338
01:29:20,560 --> 01:29:22,720
general rademacher bound

1339
01:29:22,740 --> 01:29:24,040
to this

1340
01:29:26,240 --> 01:29:28,790
so i to this function here

1341
01:29:28,800 --> 01:29:31,470
so the true expectation of this function

1342
01:29:31,480 --> 01:29:36,700
is the empirical value plus the rademacher complexity of the function classes drawn from

1343
01:29:36,720 --> 01:29:39,070
plus you know that's the standard directly

1344
01:29:39,090 --> 01:29:42,780
maybe i should just go back to the general

1345
01:29:42,800 --> 01:29:46,540
OK so every want to say

1346
01:29:48,550 --> 01:29:53,740
OK so that's that's what that is the only difficulty is that

1347
01:29:53,750 --> 01:29:57,520
it's not immediately obvious that this function is drawn from this plan that's what i

1348
01:29:57,520 --> 01:29:58,980
was trying to explain

1349
01:29:59,890 --> 01:30:03,160
OK this from what i mean by this is the functions that you get by

1350
01:30:03,160 --> 01:30:06,250
taking a linear function of norm one

1351
01:30:06,620 --> 01:30:11,370
multiplying by minus the classification output of the values

1352
01:30:11,390 --> 01:30:14,010
applying this function a

1353
01:30:14,030 --> 01:30:17,910
and then subtracting one so that converts every function

1354
01:30:17,930 --> 01:30:20,170
every linear function into a function

1355
01:30:20,200 --> 01:30:23,190
and that's the function from this class sort of what i mean it's a bit

1356
01:30:23,190 --> 01:30:24,900
of arbitration but

1357
01:30:24,900 --> 01:30:29,220
and in terms of representations with graphical models allow us to do is they allow

1358
01:30:29,220 --> 01:30:34,670
us to represent conditional independence relationships between variables

1359
01:30:34,930 --> 01:30:40,290
without bothering with the details of their parametric forms of the probability distribution

1360
01:30:40,380 --> 01:30:45,570
so we're just representing the relationship between these variables that helps answer questions like

1361
01:30:45,580 --> 01:30:48,390
is a dependent on b

1362
01:30:48,400 --> 01:30:51,850
given that we know the value of c

1363
01:30:53,270 --> 01:30:56,870
we can cannot just by looking at the structure of the graph

1364
01:30:56,890 --> 01:30:59,520
without having to know you know what it

1365
01:30:59,560 --> 01:31:01,710
probability distributions are

1366
01:31:01,720 --> 01:31:08,840
and then computationally graphical models are incredibly useful because

1367
01:31:08,890 --> 01:31:10,760
they allow us to define

1368
01:31:10,780 --> 01:31:17,710
general message passing algorithms to implement probabilistic inference efficiently

1369
01:31:17,730 --> 01:31:21,360
so we can answer questions like

1370
01:31:21,370 --> 01:31:26,520
what is the probability distribution over some variable given that we know that the variables

1371
01:31:26,520 --> 01:31:28,980
be takes on the value see

1372
01:31:28,990 --> 01:31:32,790
without enumerating all the variables

1373
01:31:32,800 --> 01:31:34,170
in our model

1374
01:31:34,180 --> 01:31:38,090
just by passing messages in our graph

1375
01:31:38,900 --> 01:31:47,540
graphical models really borrow ideas from different fields there fundamentally statistical there are representations of

1376
01:31:47,540 --> 01:31:55,450
statistical dependency you obviously make use of some of the elementary graph theory

1377
01:31:55,490 --> 01:31:57,730
and in terms of

1378
01:31:57,780 --> 01:32:03,710
computer science when you think of actually implementing algorithms for doing inference in probabilistic models

1379
01:32:03,760 --> 01:32:07,830
you make use of

1380
01:32:07,850 --> 01:32:09,940
basic ideas in computer science

1381
01:32:09,950 --> 01:32:13,700
develop efficient algorithms object oriented representation of the earth

1382
01:32:18,210 --> 01:32:23,740
the main thing graphical models do is represent conditional independence so let me introduce some

1383
01:32:23,740 --> 01:32:27,720
notation and let's talk about the cost of the conditional independence

1384
01:32:27,730 --> 01:32:32,010
so the notation is the following we say that

1385
01:32:33,000 --> 01:32:35,800
is conditionally independent from y

1386
01:32:35,820 --> 01:32:37,470
it can be

1387
01:32:37,480 --> 01:32:39,750
that's the notation that we use

1388
01:32:40,210 --> 01:32:47,080
and this means that the probability of x given y and b

1389
01:32:48,800 --> 01:32:53,190
it's not dependent on y another the probability of x given y and b is

1390
01:32:53,190 --> 01:32:57,530
just equal to the probability of x given the

1391
01:32:58,080 --> 01:33:00,620
in this condition

1392
01:33:00,670 --> 01:33:03,220
it's just that the

1393
01:33:03,250 --> 01:33:09,580
playing on the conditioning by have been possibility shouldn't really conditional something that's impossible

1394
01:33:09,730 --> 01:33:16,070
so whenever you y and p is strictly greater than zero

1395
01:33:16,080 --> 01:33:19,940
another way to see conditional independence is as follows

1396
01:33:20,000 --> 01:33:21,870
the probability distribution

1397
01:33:22,450 --> 01:33:25,110
for x and y

1398
01:33:26,160 --> 01:33:30,230
x and y are conditionally independent given v if the joint probability between x and

1399
01:33:30,240 --> 01:33:31,680
y given the

1400
01:33:31,690 --> 01:33:34,460
factors into the probability of x given the

1401
01:33:34,470 --> 01:33:37,360
and the probability of y given

1402
01:33:38,170 --> 01:33:42,520
so is the usual notion of independence or marginal independence

1403
01:33:42,550 --> 01:33:48,750
which i have down here except that we condition on having observed values of variables

1404
01:33:48,770 --> 01:33:50,700
the variable p

1405
01:33:50,790 --> 01:33:56,040
and in general we can think of conditional independence relationships between sets of variables

1406
01:33:56,080 --> 01:33:57,920
so we write down

1407
01:33:58,950 --> 01:34:04,040
this that act is conditionally independent from the set y given the set b

1408
01:34:04,060 --> 01:34:07,940
what we mean by that is that for all

1409
01:34:07,950 --> 01:34:13,920
the variable x in the that acts and all variables y and that really why

1410
01:34:13,930 --> 01:34:18,010
these are independent given the entire set b

1411
01:34:18,070 --> 01:34:21,750
having observed all the variables in t

1412
01:34:21,760 --> 01:34:26,690
now that the traditional independence that we all learn about in elementary probability

1413
01:34:26,770 --> 01:34:29,170
we're going to call marginal independence

1414
01:34:29,190 --> 01:34:34,180
and that's just conditional independence condition on on nothing condition on

1415
01:34:34,230 --> 01:34:39,380
not observing anything represent that the empty that here that is the probability that and

1416
01:34:40,030 --> 01:34:45,020
back the probability that the probability of y

1417
01:34:47,360 --> 01:34:52,520
so here are some examples of things we might think about as being

1418
01:34:52,570 --> 01:34:55,730
you know satisfy conditional independence really

1419
01:34:57,030 --> 01:35:01,100
if you get a beating find the amount of your speeding fine

1420
01:35:01,120 --> 01:35:05,300
it is independent of the type of of car that you're driving

1421
01:35:05,310 --> 01:35:08,780
given the speed that you're driving right that

1422
01:35:08,790 --> 01:35:12,880
we should really penalize you are driving a porsche or something like that

1423
01:35:12,920 --> 01:35:17,290
obviously these things are not

1424
01:35:17,340 --> 01:35:24,210
you know they're not marginally independent because presumably some some types of cars drive faster

1425
01:35:24,210 --> 01:35:25,990
than other types of parts

1426
01:35:26,000 --> 01:35:27,160
and so on

1427
01:35:27,170 --> 01:35:31,530
you know you just measured types of cars and amount of speeding fine you would

1428
01:35:31,540 --> 01:35:36,940
find some dependency but it should be conditionally independent given c

1429
01:35:37,000 --> 01:35:41,710
now we know that both smoking causes both

1430
01:35:41,760 --> 01:35:43,690
you know your keen to get yellow

1431
01:35:43,700 --> 01:35:45,780
and one cancer

1432
01:35:47,460 --> 01:35:51,170
given that you smoke presumably you know in

1433
01:35:51,270 --> 01:35:54,530
fairly simplistic model of the world where you don't do other things that are in

1434
01:35:54,530 --> 01:36:00,600
your the yellow lung cancer is independent of you smoking

1435
01:36:00,650 --> 01:36:04,540
if you are modelling an object this could be

1436
01:36:04,580 --> 01:36:10,290
for example modelling the movement of objects in video or modelling some robotic joint or

1437
01:36:10,290 --> 01:36:11,720
something like that

1438
01:36:11,800 --> 01:36:14,110
if you measure the police

1439
01:36:14,130 --> 01:36:19,310
position and velocity of that object at time t plus one and following some sort

1440
01:36:19,310 --> 01:36:24,830
of newtonian dynamics any measure the position and velocity and he minus one

1441
01:36:24,850 --> 01:36:30,870
then these two things we presume are conditionally independent given the position and velocity at

1442
01:36:30,870 --> 01:36:35,710
that was what she was

1443
01:36:35,730 --> 01:36:37,850
that so one the princes

1444
01:36:37,850 --> 01:36:40,290
this is only small

1445
01:36:43,440 --> 01:36:47,480
that is to all of us

1446
01:36:48,460 --> 01:36:50,710
is the loss way

1447
01:36:50,730 --> 01:36:53,230
right across the UK

1448
01:36:54,650 --> 01:36:55,810
so all

1449
01:36:55,830 --> 01:37:00,580
and reproduced in the right hand

1450
01:37:01,060 --> 01:37:03,380
when i said well written

1451
01:37:03,400 --> 01:37:04,870
the virus

1452
01:37:05,000 --> 01:37:07,940
thirty five is a relation

1453
01:37:13,830 --> 01:37:17,330
that's the that's what chains

1454
01:37:17,350 --> 01:37:22,770
because all sorts of hundred years

1455
01:37:22,790 --> 01:37:24,600
many different varieties

1456
01:37:24,620 --> 01:37:28,730
she are of you know that

1457
01:37:28,750 --> 01:37:33,810
that is why also we as long

1458
01:37:35,650 --> 01:37:40,270
right by the the way working with that's reason

1459
01:37:40,290 --> 01:37:42,960
well they

1460
01:37:44,310 --> 01:37:47,250
the best solution for

1461
01:37:47,270 --> 01:37:49,080
to sing

1462
01:37:49,100 --> 01:37:52,690
you can also use the rest

1463
01:37:52,710 --> 01:37:56,560
we want to see the

1464
01:37:56,580 --> 01:37:59,560
that's what it

1465
01:38:01,310 --> 01:38:06,270
you know there's a lot of these cells

1466
01:38:08,980 --> 01:38:11,900
one city

1467
01:38:11,920 --> 01:38:13,460
this is shown

1468
01:38:13,460 --> 01:38:17,210
i agree that really is this is the

1469
01:38:17,230 --> 01:38:23,520
that is here is the outline that

1470
01:38:23,520 --> 01:38:25,210
and the y axis of

1471
01:38:25,270 --> 01:38:26,520
this is the

1472
01:38:26,540 --> 01:38:28,730
and all the rest of of us

1473
01:38:28,750 --> 01:38:33,900
so we're not part of europe

1474
01:38:34,080 --> 01:38:36,120
a zero cost

1475
01:38:36,480 --> 01:38:39,580
but we must also handle

1476
01:38:39,670 --> 01:38:43,380
this is is that the sun

1477
01:38:43,400 --> 01:38:48,210
the green line here is

1478
01:38:48,310 --> 01:38:51,690
of these losses athens

1479
01:38:51,710 --> 01:38:54,210
one way

1480
01:38:58,540 --> 01:39:00,770
this is this one

1481
01:39:03,560 --> 01:39:06,420
no just

1482
01:39:08,190 --> 01:39:11,690
right now you can see all these things they

1483
01:39:11,710 --> 01:39:14,900
it is the as that of this

1484
01:39:17,000 --> 01:39:17,960
this one

1485
01:39:17,980 --> 01:39:29,900
you just the way it did not fall no

1486
01:39:32,750 --> 01:39:38,150
so this is more so can

1487
01:39:38,150 --> 01:39:40,330
that's what don't

1488
01:39:40,350 --> 01:39:43,420
to those years all these things

1489
01:39:43,440 --> 01:39:47,880
he has

1490
01:39:47,900 --> 01:39:54,250
he the

1491
01:39:54,250 --> 01:39:58,350
this one is

1492
01:39:59,020 --> 01:40:04,270
this is one

1493
01:40:04,290 --> 01:40:06,690
have been lost which

1494
01:40:06,770 --> 01:40:12,210
in the reason i thought

1495
01:40:12,420 --> 01:40:15,020
in this the machine

1496
01:40:15,040 --> 01:40:21,170
at the extended to solve

1497
01:40:23,120 --> 01:40:27,230
where is will be

1498
01:40:27,250 --> 01:40:31,310
so the context of the

1499
01:40:31,330 --> 01:40:36,750
it is located in the problem is this

1500
01:40:41,580 --> 01:40:44,420
three strange

1501
01:40:44,480 --> 01:40:46,020
to use

1502
01:40:46,020 --> 01:40:49,870
to use the one which is the

1503
01:40:52,480 --> 01:40:55,210
approximation to what you want

1504
01:41:03,420 --> 01:41:05,420
to work with

1505
01:41:05,540 --> 01:41:13,770
you want to present you a little not too

1506
01:41:13,790 --> 01:41:16,100
there is

1507
01:41:16,100 --> 01:41:19,150
be able to restore all these properties

1508
01:41:19,160 --> 01:41:20,200
that will be

1509
01:41:20,220 --> 01:41:21,620
the hardest part

1510
01:41:21,670 --> 01:41:24,770
the first thing we do is prove that these properties imply

1511
01:41:24,790 --> 01:41:29,160
the tree has to have higher order log and therefore all searches and queries on

1512
01:41:29,160 --> 01:41:33,090
that data structure will run fast and the hard part will be to make sure

1513
01:41:33,090 --> 01:41:34,740
these properties stay true

1514
01:41:34,770 --> 01:41:36,950
if they initially held true

1515
01:41:36,970 --> 01:41:39,840
when we make changes to the tree

1516
01:41:47,990 --> 01:41:50,390
let's look at the height

1517
01:41:50,420 --> 01:41:53,090
a red black tree

1518
01:41:53,110 --> 01:42:14,210
and from this will start to see why this why were these properties come from

1519
01:42:14,230 --> 01:42:16,260
why which was these properties

1520
01:42:35,640 --> 01:42:45,910
so the claim is the height of a red black tree with n keys not

1521
01:42:45,910 --> 01:42:49,780
saying those here because i really only want to count the internal nodes not these

1522
01:42:49,780 --> 01:42:51,670
extra leaves that we've got

1523
01:42:51,690 --> 01:42:56,100
as i did last two times larger than plus one so order log n

1524
01:42:56,140 --> 01:43:00,350
we have pretty precise found

1525
01:43:00,400 --> 01:43:02,410
factor of two

1526
01:43:02,430 --> 01:43:05,470
there's proof of this is the textbook

1527
01:43:05,520 --> 01:43:08,050
by induction agreed that

1528
01:43:08,140 --> 01:43:11,910
what i'm going to give us more proof sketch

1529
01:43:11,930 --> 01:43:18,630
the proof by induction because

1530
01:43:18,690 --> 01:43:23,080
all the practice you can get to find action is good the proof sketch on

1531
01:43:23,080 --> 01:43:26,380
the other hand is a lot more intuition what's going on from the red black

1532
01:43:27,220 --> 01:43:30,780
and connects up with recitation on friday so that means

1533
01:43:30,780 --> 01:43:33,520
i tell you that instead

1534
01:43:33,820 --> 01:43:35,940
i believe

1535
01:43:36,980 --> 01:43:38,490
like or here

1536
01:44:06,680 --> 01:44:11,190
the first thing i'm going to do this and i'm going to manipulate history until

1537
01:44:11,190 --> 01:44:13,230
it looks like something that i know

1538
01:44:13,260 --> 01:44:16,560
the first thing the the main change going to make

1539
01:44:16,710 --> 01:44:21,270
is to merge each red node into its parent

1540
01:44:21,280 --> 01:44:24,690
we know that parent average node must be black

1541
01:44:24,720 --> 01:44:37,620
emerging trend into its black power

1542
01:44:39,470 --> 01:44:41,370
let's look at that here

1543
01:44:41,390 --> 01:44:46,710
take this road merges into its current state for now merge into its spam and

1544
01:44:46,710 --> 01:44:52,580
so on this one which i can reach but i'm going to redraw this picture

1545
01:44:52,640 --> 01:44:55,270
so seven

1546
01:44:55,280 --> 01:44:56,860
so the top node

1547
01:44:56,890 --> 01:44:59,470
now becomes in some sense of the

1548
01:44:59,470 --> 01:45:01,560
and it's a

1549
01:45:01,570 --> 01:45:04,810
they got merged together but no one else joined

1550
01:45:04,830 --> 01:45:08,270
then on the left we have three

1551
01:45:08,270 --> 01:45:12,470
nothing join that there some leaves structural

1552
01:45:13,660 --> 01:45:16,610
if you look at me i'm going to have to draw

1553
01:45:18,480 --> 01:45:23,290
so four

1554
01:45:23,830 --> 01:45:25,170
so some merging

1555
01:45:25,230 --> 01:45:27,840
these notes together

1556
01:45:27,850 --> 01:45:31,080
an emerging all of these nodes together

1557
01:45:31,100 --> 01:45:34,470
because each of these red red-nosed merges into that black

1558
01:45:34,480 --> 01:45:39,230
an emerging these tuners together in this remote into that blacked so now you can

1559
01:45:39,230 --> 01:45:44,080
see from the root which is now seven slash eighteen there are three children hanging

1560
01:45:45,900 --> 01:45:47,410
so in that picture

1561
01:45:47,430 --> 01:45:49,010
like two

1562
01:45:49,050 --> 01:45:51,030
draw that fact

1563
01:45:51,860 --> 01:45:53,790
so something like can get this support

1564
01:45:56,850 --> 01:45:59,770
this is a

1565
01:45:59,810 --> 01:46:02,320
between seven and eighteen

1566
01:46:02,370 --> 01:46:03,670
i have this

1567
01:46:03,680 --> 01:46:05,710
conglomerate node

1568
01:46:10,390 --> 01:46:12,320
and there are four

1569
01:46:12,320 --> 01:46:15,060
leaves hanging off of that now

1570
01:46:15,080 --> 01:46:22,260
and after the right after eighteen

1571
01:46:22,260 --> 01:46:27,180
i have a conglomerate twenty two slashed twenty six

1572
01:46:27,210 --> 01:46:29,910
and there are three please hang up there

1573
01:46:34,320 --> 01:46:37,760
kind of weird tree because we deal mainly with binary trees so far but this

1574
01:46:37,760 --> 01:46:40,760
is a foreshadowing what will come on friday

1575
01:46:40,780 --> 01:46:45,330
this is something called two three four tree

1576
01:46:45,470 --> 01:46:53,470
i guess is why it's called the two three four tree

1577
01:46:53,480 --> 01:46:58,330
every node can have two three or four kids here

1578
01:46:58,390 --> 01:47:00,820
except the leaves they have zero

1579
01:47:00,830 --> 01:47:04,820
OK is another nice property of two three four trees

1580
01:47:04,820 --> 01:47:08,840
maybe hinted that so there's really no control over whether you have

1581
01:47:08,850 --> 01:47:13,000
two children are three children or four children

1582
01:47:13,010 --> 01:47:17,260
but there's another nice property

1583
01:47:23,550 --> 01:47:24,500
all of that

1584
01:47:24,520 --> 01:47:28,390
all the leaves have the same depth exactly all of these guys

1585
01:47:28,420 --> 01:47:30,440
i have the same depth of the tree

1586
01:47:30,490 --> 01:47:34,170
why is that because the property for

1587
01:47:34,490 --> 01:47:39,930
on friday you'll see just how to maintain their property but this transformation we get

1588
01:47:40,180 --> 01:47:43,210
all the leaves have the same depth because

1589
01:47:43,230 --> 01:47:45,430
their deaths now

1590
01:47:45,460 --> 01:47:47,300
or they say are hiding the tree

1591
01:47:47,340 --> 01:47:49,320
is there black kite

1592
01:47:49,540 --> 01:47:55,150
the death of these leaves will be the black eyed of the room

1593
01:47:55,160 --> 01:47:58,850
because we so we're erasing all the right notes and we said if we look

1594
01:47:58,850 --> 01:48:02,320
at the past and we ignore all the red nodes than the number of black

1595
01:48:02,320 --> 01:48:05,770
nodes along the path is the same now are basically just leaving all the black

1596
01:48:05,770 --> 01:48:09,440
and it can be sparse because in this part of the dictionary

1597
01:48:09,490 --> 01:48:10,410
so that

1598
01:48:10,430 --> 01:48:14,740
is it true that way of motivating that we should be learning the

1599
01:48:14,790 --> 01:48:17,020
the idea is that we learned

1600
01:48:18,720 --> 01:48:22,270
well only for a number of things here by one of the top on the

1601
01:48:22,270 --> 01:48:25,670
web so you can complete the fill in the blanks so you're forced to do

1602
01:48:25,670 --> 01:48:26,020
that now

1603
01:48:26,400 --> 01:48:28,170
but we're going to learn the

1604
01:48:28,210 --> 01:48:29,950
to sparsify signals

1605
01:48:29,960 --> 01:48:34,180
we're going to learn the for the task so we're going to have different dictionaries

1606
01:48:34,180 --> 01:48:35,950
if you wanna the noise

1607
01:48:35,970 --> 01:48:37,450
or if you want to classify

1608
01:48:37,460 --> 01:48:39,140
we also learn the

1609
01:48:39,160 --> 01:48:41,890
four sensing different sensing devices

1610
01:48:41,900 --> 01:48:44,670
OK so that's the whole that i have freedom

1611
01:48:44,720 --> 01:48:47,970
of learning the of course i have to show you how we do that but

1612
01:48:47,970 --> 01:48:48,990
that's going to be there

1613
01:48:49,040 --> 01:48:51,090
main concept here

1614
01:48:51,990 --> 01:48:54,550
dictionary learning is the main message here

1615
01:48:55,300 --> 01:48:56,950
and there are a number of ways

1616
01:48:56,970 --> 01:48:58,250
of doing that

1617
01:48:58,310 --> 01:49:01,710
but the basic idea is the following

1618
01:49:01,720 --> 01:49:04,130
we're going to have how are we going to learn b

1619
01:49:04,140 --> 01:49:06,340
and have a lot of image

1620
01:49:07,400 --> 01:49:10,630
so i put all my images as columns here

1621
01:49:10,750 --> 01:49:14,760
OK so this is and that was my dimension but this is a very large

1622
01:49:15,890 --> 01:49:17,060
ten thousand

1623
01:49:17,080 --> 01:49:21,570
you can have a lot of images images are not problems today became the problem

1624
01:49:21,570 --> 01:49:23,910
that we have too many images is the problem

1625
01:49:23,920 --> 01:49:25,200
OK so

1626
01:49:25,210 --> 01:49:30,170
if you go to flickr there is a six thousand images loaded i mean

1627
01:49:30,270 --> 01:49:31,130
more or less

1628
01:49:31,150 --> 01:49:34,710
so so that's not a big problem today

1629
01:49:34,720 --> 01:49:39,790
we learned dictionary in such a way that every single one of these images

1630
01:49:39,800 --> 01:49:42,570
it's sparsely represented with distinction

1631
01:49:42,590 --> 01:49:43,950
that's the whole got

1632
01:49:43,970 --> 01:49:49,380
our optimisation problem looks like this and the sum over all the images

1633
01:49:49,410 --> 01:49:51,440
i want to a good approximation

1634
01:49:51,450 --> 01:49:53,270
of every single image

1635
01:49:53,280 --> 01:49:57,760
by a fixed dictionary and alpha j and i want on my alpha j is

1636
01:49:57,770 --> 01:50:00,340
for all the images for the p images

1637
01:50:00,370 --> 01:50:03,530
i want them to be spies and you're welcome to put here in the one

1638
01:50:03,530 --> 01:50:06,040
norm and make this alasso problems

1639
01:50:06,120 --> 01:50:08,920
is to find and el zero problem

1640
01:50:10,000 --> 01:50:14,340
but the optimisation is in contrast to what it was before

1641
01:50:14,360 --> 01:50:16,790
that he was only around five

1642
01:50:16,790 --> 01:50:18,860
the optimisation silver balls

1643
01:50:18,870 --> 01:50:22,890
the dictionary under representation both at the same time

1644
01:50:23,880 --> 01:50:27,170
even if i put a one

1645
01:50:27,170 --> 01:50:29,920
here this is a non convex problem

1646
01:50:29,930 --> 01:50:33,990
OK base convex if i fix one and optimize forty out if i put on

1647
01:50:34,000 --> 01:50:35,580
a one here

1648
01:50:35,890 --> 01:50:41,520
so the basic idea of that is to do i the optimisation animation that next

1649
01:50:41,530 --> 01:50:45,740
so once again it example is a linear combination of atoms from the

1650
01:50:45,790 --> 01:50:48,470
and each one has to be sparse

1651
01:50:49,560 --> 01:50:53,890
is a partial list of a lot of work that has been done in this

1652
01:50:54,910 --> 01:50:57,030
OK just a partial list

1653
01:50:58,180 --> 01:51:01,930
so how do we learn addiction and tell you just one way there are other

1654
01:51:01,930 --> 01:51:04,940
ways of learning the dictionary but i just want to give you

1655
01:51:04,960 --> 01:51:08,580
from the pedagogic point of view one way of doing that is not the there

1656
01:51:08,830 --> 01:51:12,960
the only way and sometimes with don't we used to use this a lot we

1657
01:51:12,960 --> 01:51:14,070
use it to be less today

1658
01:51:14,490 --> 01:51:19,030
in our group based very good way of learning the dictionary an unfortunate there's a

1659
01:51:19,030 --> 01:51:21,250
lot of red but we can still work here

1660
01:51:21,360 --> 01:51:27,190
so the idea is that we have been initialized dictionary let's say without wisconsin plus

1661
01:51:27,250 --> 01:51:29,790
forty year plus higher function

1662
01:51:29,840 --> 01:51:32,840
remember normally this is overcomplete

1663
01:51:32,860 --> 01:51:35,620
so we're going to initialize the dictionary

1664
01:51:35,630 --> 01:51:36,800
and we have done that

1665
01:51:39,780 --> 01:51:41,790
matching pursuit or a one

1666
01:51:41,800 --> 01:51:43,790
so with this dictionary

1667
01:51:43,810 --> 01:51:45,440
so let's look at this

1668
01:51:45,460 --> 01:51:50,030
this is like was prepared by by making it's a very nice light

1669
01:51:50,040 --> 01:51:53,330
so the basic idea is that they go every

1670
01:51:53,340 --> 01:51:59,080
every single one of my images and i represent leaves sparsely with the given action

1671
01:51:59,110 --> 01:52:00,930
OK so i have aspires school

1672
01:52:01,060 --> 01:52:05,000
i mentioned if used in one this is a convex problem

1673
01:52:05,040 --> 01:52:10,050
if you use the zero then you do matching pursuits orthogonal matching pursuit in chennai

1674
01:52:11,720 --> 01:52:15,160
i have for everything that i have is sparse representation

1675
01:52:15,200 --> 01:52:17,540
and i'm going to have the dictionary

1676
01:52:17,560 --> 01:52:19,800
in the particular case of the case

1677
01:52:19,810 --> 01:52:22,190
the basic idea is that you fix

1678
01:52:22,200 --> 01:52:24,020
what's called the active set

1679
01:52:24,040 --> 01:52:28,280
and that's that basically by almost all the algorithms that are out there

1680
01:52:28,290 --> 01:52:30,840
so i'm not gonna i'm going change the

1681
01:52:30,870 --> 01:52:35,670
atoms of my dictionary but i'm not gonna i'm not going to change this iteration

1682
01:52:35,810 --> 01:52:38,650
that means that are being used by the city

1683
01:52:38,750 --> 01:52:40,320
and the idea is very simple

1684
01:52:40,330 --> 01:52:45,720
so you go to the first item of the dictionary use alcohol use me

1685
01:52:45,740 --> 01:52:47,490
and you see that this

1686
01:52:47,510 --> 01:52:48,940
images you

1687
01:52:49,000 --> 01:52:50,770
and these images

1688
01:52:50,790 --> 01:52:55,380
and then you say OK and change myself so you that you like me you're

1689
01:52:55,380 --> 01:52:57,200
going to like me even more

1690
01:52:57,250 --> 01:53:00,260
OK and that just honestly the problem

1691
01:53:01,700 --> 01:53:04,660
then you go to the next atom as a whole use me

1692
01:53:04,670 --> 01:53:09,240
i might find seventy images use me i'm going to say i find myself i

1693
01:53:09,240 --> 01:53:12,830
the change myself they use seven likely more is like a weighted average of all

1694
01:53:12,830 --> 01:53:15,220
you guys which is an

1695
01:53:15,930 --> 01:53:17,700
and then you keep going

1696
01:53:17,720 --> 01:53:20,040
so you've got a new dictionary

1697
01:53:20,050 --> 01:53:22,050
one column at the time

1698
01:53:22,070 --> 01:53:24,050
and then you iterate this cuts

1699
01:53:24,060 --> 01:53:28,660
what you have in your dictionary the new sparse coding the dictionary and got

1700
01:53:29,440 --> 01:53:33,960
the basic idea in every dictionary learning out there is that you fix the active

1701
01:53:34,990 --> 01:53:40,630
fixing the active set makes the problem of dictionary update a convex problem

1702
01:53:41,790 --> 01:53:42,870
so so

1703
01:53:42,920 --> 01:53:46,370
you fix the active set by doing and then one or or or in p

1704
01:53:46,370 --> 01:53:47,970
and then you optimize to

1705
01:53:48,030 --> 01:53:49,800
and then you're iterate

1706
01:53:49,820 --> 01:53:52,000
OK so that's the basic idea

1707
01:53:52,260 --> 01:54:00,870
yeah all images or or we have now online learning that we are learning as

1708
01:54:00,870 --> 01:54:05,410
images but this is the simplest form the set of all the images but that's

1709
01:54:05,410 --> 01:54:09,200
a good question just to give you an idea we train

1710
01:54:10,070 --> 01:54:14,900
three thousand images for by just a few thousand patches it by a so we

1711
01:54:14,940 --> 01:54:18,890
talking sixty four with a thousand or five thousand ten thousand

1712
01:54:18,910 --> 01:54:21,950
but you can do that online this as they can

1713
01:54:22,270 --> 01:54:26,920
thanks by the way i am happy to entertain questions during the the talk if

1714
01:54:26,920 --> 01:54:30,380
need not too many but just a few

1715
01:54:30,390 --> 01:54:43,790
and we were all

1716
01:54:45,060 --> 01:54:50,650
he will be

1717
01:54:50,660 --> 01:54:54,290
you our

1718
01:54:54,290 --> 01:55:00,850
we talked about hierarchical Dirichlet processes and the idea of hierarchical Dirichlet process is that you

1719
01:55:00,850 --> 01:55:06,510
have to processes by using a shared base measure which is itself a

1720
01:55:06,510 --> 01:55:14,910
Dirichlet process distributed and that allows us to share topics across multiple documents

1721
01:55:14,910 --> 01:55:26,550
and that works pretty well let me see okay yeah so I think this idea of

1722
01:55:26,550 --> 01:55:32,610
hierarchy and building more complex models from simpler ones is a a very important idea

1723
01:55:32,650 --> 01:55:38,210
in in bayesian modelling and also in Bayesian nonparametrics because it allows us to

1724
01:55:38,210 --> 01:55:44,050
build well more complex models and we see that hierarchical models are kind of

1725
01:55:44,050 --> 01:55:52,500
quite natural technique for combining this building blocks and we've seen lots of applications in

1726
01:55:52,500 --> 01:55:59,210
computational linguistics time series sequential models and so forth and vision and genetics that's a

1727
01:55:59,210 --> 01:56:05,430
different way of constructing this of more complex models from simpler ones which kind

1728
01:56:05,430 --> 01:56:10,190
of kind of comes under the name of dependent random measures and this I used

1729
01:56:10,190 --> 01:56:17,830
mostly for things like spatial and temporal type of data where basically every data

1730
01:56:17,830 --> 01:56:23,050
points so that you either with a points in space or points in in time

1731
01:56:23,090 --> 01:56:30,350
and you want to build dependence across either in a spatial domain or a temporal domain

1732
01:56:30,390 --> 01:56:39,270
that`s also then an idea of nested of nesting multiple processes together to build kind of

1733
01:56:39,270 --> 01:56:45,990
a more complex  ones as well so I'll just briefly tell about dependent random

1734
01:56:45,990 --> 01:56:54,290
measures so this is basically a measured valued stochastic process so G indexed by phi

1735
01:56:54,290 --> 01:57:01,010
where phi is some value in the covariate space okay so if think of phi

1736
01:57:01,480 --> 01:57:07,090
as a point somewhere in the space so that basically at every phi we might have a different

1737
01:57:07,090 --> 01:57:14,930
stochastic process G G of phi and we can construct this on dependent random

1738
01:57:14,930 --> 01:57:20,510
measures such that each G of phi is gonna be marginally Dirichlet process so

1739
01:57:20,510 --> 01:57:27,230
that that's called a dependent Dirichlet process so basically we have a bunch

1740
01:57:27,230 --> 01:57:33,550
of Dirichlet processes and they have some some sort of dependence that is related

1741
01:57:33,560 --> 01:57:38,970
spatially so if two points in space are closer together then they then they`re corresponding

1742
01:57:38,970 --> 01:57:44,290
Dirichlet processes will be somehow more similar to each other okay and this is this can

1743
01:57:44,290 --> 01:57:48,930
be used in in various spatial modelling type of applications as well as in things

1744
01:57:48,930 --> 01:57:53,890
like density regression so density regression is a problem

1745
01:57:53,890 --> 01:57:59,850
that's kind of more general than just simply regression or classification in regression or

1746
01:57:59,870 --> 01:58:05,670
classification you have an input and you want to predict the mean output at that

1747
01:58:05,670 --> 01:58:10,690
point in space and in regression you may assume say a Gaussian noise around the mean

1748
01:58:10,690 --> 01:58:14,490
or in classification you're simply trying to predict the class and it's only a

1749
01:58:14,490 --> 01:58:20,350
finite number of classes in density regression given a point you given an input

1750
01:58:20,350 --> 01:58:25,070
points you'd like to actually predict a whole distribution over outputs and that distribution can be

1751
01:58:25,070 --> 01:58:28,980
very complex and you'd like to learn about not just the mean of this the

1752
01:58:28,980 --> 01:58:34,310
the distribution but also the shape of the distribution and you'd like to learn about

1753
01:58:34,310 --> 01:58:39,370
how things things like not just the mean might change with the with the

1754
01:58:39,370 --> 01:58:45,130
input space but things like the variance may change with the input space as well okay

1755
01:58:45,130 --> 01:58:50,890
so this is a kind of a much richer type of problem and that's I guess

1756
01:58:50,890 --> 01:58:56,350
people have explored this in things like biostatistics where you do want to to model

1757
01:58:56,350 --> 01:59:06,930
the the complex distribution on your output space given your input space that`s it

1758
01:59:06,930 --> 01:59:12,690
okay so in the next two sections I`ll tell about two

1759
01:59:12,690 --> 01:59:18,400
extensions of this idea of a hierarchical Dirichlet process the first one is gonna be

1760
01:59:18,400 --> 01:59:24,730
a degeneralization from a Dirichlet process to a Pitman Yor process

1761
01:59:24,770 --> 01:59:30,490
and we see that`s very important for doing for modelling languages and the second one

1762
01:59:30,490 --> 01:59:38,110
will be to build infinite hidden Markov models from this idea of hierarchical Dirichlet

1763
01:59:38,110 --> 01:59:44,710
process so let's start off with the language modelling so in language modelling the most popular

1764
01:59:44,710 --> 01:59:51,410
approach is basically N gram language models so we'll start with that so what is the

1765
01:59:51,410 --> 01:59:57,010
problem here the problem is that we have sequences of words or characters they're typically discrete

1766
01:59:57,010 --> 02:00:02,130
objects so things like this okay and basically this sequence of words just think of it

1767
02:00:02,130 --> 02:00:06,990
as a sentence coming from some language so English or Japanese or Chinese and so forth

1768
02:00:06,990 --> 02:00:13,250
and what we'd like to do is to build a model probabilistic model for

1769
02:00:13,260 --> 02:00:18,890
these sequences such that it assigns high probability to the sequences that you typically observe

1770
02:00:18,970 --> 02:00:22,870
and low probability to sequences that you don't typically observe and the idea is that

1771
02:00:22,870 --> 02:00:27,030
you'd like to model languages so that you can you can have a model that

1772
02:00:27,030 --> 02:00:35,010
can tell a system whether the sentence is a typical sentence or not okay and this is

1773
02:00:35,060 --> 02:00:41,430
used in things like speech recognition and machine translation and so forth and the simplest

1774
02:00:41,430 --> 02:00:46,030
way of building this sort of language models is to basically take this as sequences

1775
02:00:46,030 --> 02:00:53,770
of words and build high order Markov models of this sort of sequences so a N order model

1776
02:00:53,770 --> 02:00:58,950
is this the N minus first order model basically assumes that the

1777
02:00:58,950 --> 02:01:03,730
probability of a sentence is gonna be a product over each word in the

1778
02:01:03,730 --> 02:01:09,930
sentence of the probability of that would given the previous N minus one word okay so

1779
02:01:09,930 --> 02:01:18,080
you care us assume that basically given the previous N N minus one words the

1780
02:01:18,090 --> 02:01:23,270
word that you're trying to predict next word I is independent of words further back

