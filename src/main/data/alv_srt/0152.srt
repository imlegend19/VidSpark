1
00:00:00,000 --> 00:00:04,340
so it's sort of a real life thing as close as you can get

2
00:00:04,340 --> 00:00:07,320
OK so this is

3
00:00:07,370 --> 00:00:10,930
with the long term memory

4
00:00:10,970 --> 00:00:14,790
right we cycled and you came back to the same experts again you pulled out

5
00:00:14,790 --> 00:00:15,550
from the

6
00:00:15,640 --> 00:00:20,330
from the intermediate league intermediate waste back up to the

7
00:00:22,630 --> 00:00:23,390
if you

8
00:00:23,410 --> 00:00:24,440
she about

9
00:00:25,180 --> 00:00:26,680
the start of the two

10
00:00:26,710 --> 00:00:29,830
then all the segments look the same

11
00:00:29,840 --> 00:00:32,080
you don't have a long term memory

12
00:00:32,180 --> 00:00:35,900
there you can prove bounds the time the loss of the algorithm is as good

13
00:00:35,900 --> 00:00:38,740
as the loss of the best partition

14
00:00:38,760 --> 00:00:41,000
x you know it's like the composite

15
00:00:41,020 --> 00:00:44,590
expert plus the number of bits to write down the number the

16
00:00:44,590 --> 00:00:47,930
the partition

17
00:00:47,950 --> 00:00:52,990
OK this details the constant factors are not perfect and this action on the complete

18
00:00:52,990 --> 00:00:58,400
problem is the constant that this would be tied to use on a complete graph

19
00:00:59,400 --> 00:01:02,130
let me show you again

20
00:01:02,140 --> 00:01:08,610
sharing to the decaying past shared past protesters following behaviour this is just one segment

21
00:01:08,660 --> 00:01:10,950
it pulls out a good way

22
00:01:11,010 --> 00:01:11,940
and then

23
00:01:11,950 --> 00:01:15,540
the second which comes up in the first which goes down to some kind of

24
00:01:15,540 --> 00:01:17,980
intermediate layer

25
00:01:18,030 --> 00:01:22,370
we're kind of sort of long-term memory a second stage from that one can be

26
00:01:22,370 --> 00:01:26,660
pulled up again if it's needed again

27
00:01:26,660 --> 00:01:31,540
if you share the start of a vector you don't have this behavior

28
00:01:31,570 --> 00:01:33,370
now this is interesting

29
00:01:33,440 --> 00:01:38,080
if x going on in these updates and i'm completely fascinated by the imagine from

30
00:01:38,080 --> 00:01:42,950
moment the first expertise good for y but is a short spur with second expertise

31
00:01:44,010 --> 00:01:46,880
and again first good short for second

32
00:01:46,890 --> 00:01:49,430
so what and what happens is

33
00:01:49,450 --> 00:01:52,030
the good expert has most of the weight

34
00:01:52,150 --> 00:01:54,400
but the second

35
00:01:54,440 --> 00:01:58,050
but the second expert is ratcheted up

36
00:01:58,140 --> 00:02:00,140
it's like it gets a little bit

37
00:02:00,190 --> 00:02:01,390
the weights

38
00:02:01,390 --> 00:02:03,630
get wrote a little bit

39
00:02:03,680 --> 00:02:07,540
and the k of course one of the the first experts good

40
00:02:08,260 --> 00:02:12,670
they can't and pull so your rationale

41
00:02:12,720 --> 00:02:14,780
the second best expert

42
00:02:15,050 --> 00:02:16,670
so quite curious

43
00:02:16,710 --> 00:02:19,020
here the case where

44
00:02:19,650 --> 00:02:20,670
i did

45
00:02:21,820 --> 00:02:25,850
one two three four five six one

46
00:02:25,860 --> 00:02:26,990
and beta two

47
00:02:27,020 --> 00:02:29,840
and then a lot of one and beta three a lot of one and the

48
00:02:29,840 --> 00:02:32,090
better for a lot of one and a bit of

49
00:02:34,690 --> 00:02:37,390
the good expert the first expert this

50
00:02:37,400 --> 00:02:40,230
is always on the top of the second

51
00:02:40,260 --> 00:02:41,550
the experts

52
00:02:41,570 --> 00:02:48,590
are actually as well so there you can overlay the two phenomena

53
00:02:49,540 --> 00:02:53,350
back to sharp is crucial

54
00:02:53,390 --> 00:02:58,460
fixed uniform past mix and look the past average is the easiest

55
00:02:59,340 --> 00:03:02,940
stable algorithms beta because one of the ian of

56
00:03:02,970 --> 00:03:05,180
shifting great

57
00:03:05,660 --> 00:03:10,340
half percent works way well it's very stable

58
00:03:13,870 --> 00:03:15,740
he is the overall thing

59
00:03:15,790 --> 00:03:19,170
this was the theory we can prove things but now caching

60
00:03:19,210 --> 00:03:21,460
the theory doesn't apply

61
00:03:22,460 --> 00:03:26,120
we can still use these algorithms as heuristics in terms of the world

62
00:03:28,120 --> 00:03:33,090
so we process request on virtual caches

63
00:03:33,110 --> 00:03:35,760
we apply lost in the sheer charities

64
00:03:35,820 --> 00:03:42,260
and then process the request on the real based on the combined weightings of all

65
00:03:42,320 --> 00:03:43,340
and we

66
00:03:43,350 --> 00:03:47,030
we object if necessary so that sort of neural

67
00:03:52,830 --> 00:03:57,300
virtual cash gives me a ranking of all the objects

68
00:03:57,300 --> 00:03:59,800
world that it is

69
00:03:59,810 --> 00:04:02,010
not improving any more and so

70
00:04:02,020 --> 00:04:05,770
where there used to be some intrinsic reward there is no intrinsic you want no

71
00:04:05,770 --> 00:04:08,050
more sometimes so

72
00:04:08,070 --> 00:04:09,530
the reinforcement learner

73
00:04:09,570 --> 00:04:11,830
have some some serious

74
00:04:11,850 --> 00:04:13,830
task to do

75
00:04:13,850 --> 00:04:16,580
and the first

76
00:04:16,590 --> 00:04:21,850
methods that we use for that very very simple and and all of you probably

77
00:04:22,380 --> 00:04:26,040
have some experience with those in nineteen ninety one twenty years ago

78
00:04:26,080 --> 00:04:30,140
as of prediction machine be just use

79
00:04:30,160 --> 00:04:34,200
neural networks as you as you

80
00:04:34,210 --> 00:04:40,540
as you often use how many people in this audience use neural networks occasionally

81
00:04:40,560 --> 00:04:42,200
hands up

82
00:04:42,230 --> 00:04:45,510
so maybe twenty percent was actually more like

83
00:04:45,550 --> 00:04:49,720
fifty five that there's another so it's about twenty five percent

84
00:04:49,740 --> 00:04:55,800
twenty years ago the we have this this little

85
00:04:55,820 --> 00:05:00,570
prediction machine which as you know is compressor every neural networks that predicts the future

86
00:05:00,990 --> 00:05:03,900
it is also compressed because what you don't

87
00:05:03,950 --> 00:05:08,490
predict well you have to store extra so it's not good in compression in whatever

88
00:05:08,490 --> 00:05:13,260
you you predict you don't have to store extra so you're compressing the data and

89
00:05:13,520 --> 00:05:16,520
what you cannot predict well that you need access

90
00:05:16,520 --> 00:05:18,780
storage space

91
00:05:18,790 --> 00:05:20,840
just store and then see

92
00:05:20,860 --> 00:05:23,550
the act

93
00:05:23,560 --> 00:05:29,340
it was just the standard q learning algorithm plus neural network functionapproximator and there was

94
00:05:29,340 --> 00:05:34,620
an environment you know twenty years ago people always had these little maze environments and

95
00:05:34,620 --> 00:05:37,700
that the agents could run around

96
00:05:37,770 --> 00:05:42,810
and get sometimes it's only war but then some parts of this environment where more

97
00:05:42,810 --> 00:05:45,520
easily predictable than others and so on

98
00:05:46,960 --> 00:05:52,490
prediction machine like to improve more in certain areas and less in others which means

99
00:05:52,490 --> 00:05:57,080
that there was more intrinsic reward in certain areas where the but there was more

100
00:05:57,080 --> 00:06:01,550
compression programs and more learning progress and so the system like to go there and

101
00:06:01,550 --> 00:06:06,850
then we had examples where with sometimes actually helps to speed up the because you

102
00:06:06,850 --> 00:06:10,980
are you know more about the world and sometimes this helps to speed up exactly

103
00:06:10,980 --> 00:06:12,310
what is wrong

104
00:06:12,320 --> 00:06:15,850
twenty years ago we did that and

105
00:06:15,870 --> 00:06:16,930
i'm mean one

106
00:06:16,970 --> 00:06:20,880
kind of happy that today but only recently this house

107
00:06:20,900 --> 00:06:24,310
picked up steam and lots of people are starting to get interested in this sand

108
00:06:24,620 --> 00:06:29,370
and build their own little versions of of clear systems like this

109
00:06:29,370 --> 00:06:33,880
now neural networks can be replaced the pass with support vector machines and all kinds

110
00:06:33,880 --> 00:06:36,740
of things but they are now

111
00:06:36,800 --> 00:06:39,810
and estimated and they are really good at what happened just a couple of months

112
00:06:39,810 --> 00:06:46,160
ago is not be used plain standard plane or backup networks to break the minister

113
00:06:46,260 --> 00:06:49,770
of one to the handwritten digit recognition

114
00:06:49,790 --> 00:06:54,570
one rocket it is anybody in the audience who knows this on this dataset

115
00:06:54,580 --> 00:06:58,730
which can be quite a few of which has a long history of broken back

116
00:06:58,730 --> 00:07:02,880
record since nineteen ninety eight the county find and

117
00:07:02,910 --> 00:07:05,290
and you look at these

118
00:07:05,330 --> 00:07:07,890
these wreckers

119
00:07:07,950 --> 00:07:12,680
and in the very beginnings of multi-layered perceptrons but then it was all support vector

120
00:07:12,680 --> 00:07:18,580
machines certain special cases are computer convolutional networks or

121
00:07:18,590 --> 00:07:24,330
so combinations of classifiers and support vector machines combined with something and more recently deep

122
00:07:24,330 --> 00:07:27,820
networks for example by canton and bengio

123
00:07:27,830 --> 00:07:29,880
and others who

124
00:07:29,950 --> 00:07:36,450
pre training unsupervised fashion these deep networks with have which have many layers and then

125
00:07:38,190 --> 00:07:44,240
texas system and and the train the supervised fashion to to mark one of the

126
00:07:46,520 --> 00:07:48,860
and all of us

127
00:07:48,910 --> 00:07:51,190
it's not really necessary

128
00:07:51,200 --> 00:07:56,930
it turns out he other guys who really did works instantiated sound and within my

129
00:07:56,940 --> 00:08:01,070
and down there is is look government and my my office neighbour who is mostly

130
00:08:01,070 --> 00:08:06,320
working on swarm intelligence and artificial and and who has papers with thousands and thousands

131
00:08:06,320 --> 00:08:10,180
so obviously the answer is to take t equal to the largest of those lower

132
00:08:11,300 --> 00:08:13,260
that's the maximum

133
00:08:13,280 --> 00:08:17,510
so for sixty the optimal value this expression

134
00:08:17,530 --> 00:08:20,880
and if you allowed to optimize over x and t jointly

135
00:08:20,890 --> 00:08:22,150
then you

136
00:08:22,160 --> 00:08:26,720
choose the value of t and four x which is the optimal this problem

137
00:08:26,740 --> 00:08:37,410
so there are some applications in machine learning probably all know about that are very

138
00:08:37,410 --> 00:08:42,510
well known so basically if we want to separate so the basic one basic problem

139
00:08:42,510 --> 00:08:46,630
is if you want to linearly separate two sets of points in rn

140
00:08:46,680 --> 00:08:47,800
the dark

141
00:08:47,820 --> 00:08:51,180
circles and the open circles

142
00:08:51,200 --> 00:08:55,700
we define a hyperplane are separating hyperplane like this so we look for

143
00:08:55,740 --> 00:08:58,570
the coefficient vector and constant b

144
00:08:58,610 --> 00:08:59,680
that takes

145
00:08:59,700 --> 00:09:03,130
therefore it is i find function takes positive values on the axes and negative values

146
00:09:04,180 --> 00:09:07,530
and that's one strict set of

147
00:09:07,590 --> 00:09:10,990
a set of strict linear inequalities

148
00:09:11,010 --> 00:09:13,470
it was a and b

149
00:09:13,530 --> 00:09:16,720
and the exercise and why is unknown so this is the set of linear inequalities

150
00:09:16,720 --> 00:09:18,390
in a b

151
00:09:18,450 --> 00:09:23,570
it's not strict space can easily it starts its districts inequality but you can easily

152
00:09:23,570 --> 00:09:26,220
make it into non strict inequality

153
00:09:26,240 --> 00:09:30,550
by noting that this is homogeneous or if a and b satisfy these conditions

154
00:09:30,570 --> 00:09:32,840
any positive multiple of a and b

155
00:09:32,840 --> 00:09:34,510
set aside

156
00:09:34,530 --> 00:09:39,220
so you can also make this right hand side equal to one or negative one

157
00:09:39,240 --> 00:09:44,150
and then just replace it with non-stick inequality

158
00:09:44,240 --> 00:09:47,760
and then you have a set of linear inequalities in a and b any solution

159
00:09:47,820 --> 00:09:52,470
if it solvable defined separating hyperplane

160
00:09:53,720 --> 00:09:58,280
as an extension you can look at problems where the two sets are not separable

161
00:09:58,300 --> 00:10:02,130
and then try to define an approximate separating hyperplane

162
00:10:02,180 --> 00:10:06,820
that separates most of the sets of points x and y

163
00:10:06,820 --> 00:10:10,970
so one of our formulation would be the following so a piecewise linear optimisation problem

164
00:10:10,970 --> 00:10:13,650
in the same variables a and b

165
00:10:13,660 --> 00:10:17,700
so the first term you look at the max of zero and then for each

166
00:10:17,700 --> 00:10:22,610
point x i dislike one minus i suppose x i might be

167
00:10:22,630 --> 00:10:27,070
so fx i lies on the correct side of the hyperplane

168
00:10:27,490 --> 00:10:29,110
that's zero

169
00:10:29,150 --> 00:10:33,010
and this max is zero and there is no contribution to the sum

170
00:10:33,030 --> 00:10:36,130
if x is on the negative on the wrong side of the hyperplane if it's

171
00:10:36,130 --> 00:10:41,860
misclassified by that hyperplane then the penalty equal to the slack

172
00:10:41,860 --> 00:10:44,030
in the court

173
00:10:44,090 --> 00:10:48,030
and we do the same the other set of points and uses the penalty on

174
00:10:48,030 --> 00:10:50,200
the separating hyperplane that he

175
00:10:50,220 --> 00:10:54,010
to can minimize to find an approximate separating hyperplane

176
00:10:55,550 --> 00:10:58,340
obviously piecewise linear because it's max of

177
00:10:58,360 --> 00:10:59,930
linear functions of a b

178
00:10:59,930 --> 00:11:02,570
you can easily write to

179
00:11:02,590 --> 00:11:05,200
using the same trick as before

180
00:11:11,430 --> 00:11:18,090
so the one this because i used one in the right hand side here

181
00:11:18,090 --> 00:11:22,130
and one doesn't really any positive number will be

182
00:11:23,180 --> 00:11:24,680
good here

183
00:11:27,430 --> 00:11:33,070
so if i scale one then the a and b would scale homogeneous

184
00:11:33,110 --> 00:11:39,510
sorry but there's only there's all the term objective so if

185
00:11:39,570 --> 00:11:42,110
i scale one then a and b if

186
00:11:42,130 --> 00:11:45,070
it would be optimal then it can be scale

187
00:11:45,180 --> 00:11:51,570
that's because you're defined

188
00:11:51,590 --> 00:11:54,380
separation strict inequality

189
00:11:54,410 --> 00:11:59,430
so if a and b satisfy this that i can always kill and b two

190
00:11:59,430 --> 00:12:02,550
make greater than any positive number on the right

191
00:12:02,760 --> 00:12:08,760
and if it separates with right hand side one negative one then obviously satisfies this

192
00:12:08,860 --> 00:12:13,810
would be different if i define separation by non strict inequality here

193
00:12:13,860 --> 00:12:17,860
and i just don't strict inequality and then

194
00:12:17,860 --> 00:12:29,880
also excluded non this UAB has the solution then this step could be equivalent

195
00:12:30,700 --> 00:12:35,950
so what the common problems that are result in LP's are infinity and one optimisation

196
00:12:35,970 --> 00:12:38,800
so i think the one norm of x minus b

197
00:12:38,820 --> 00:12:41,990
the one norm is defined as the sum of the absolute values

198
00:12:41,990 --> 00:12:47,510
then i can be written as an LP again by introducing extra variables and y

199
00:12:47,590 --> 00:12:48,820
vector y

200
00:12:48,840 --> 00:12:50,720
and then linear inequalities

201
00:12:50,740 --> 00:12:54,280
and again it's easy to see that coolant because if you fix x in this

202
00:12:54,280 --> 00:12:56,090
problem in this LP

203
00:12:56,150 --> 00:13:00,010
then you get an easy problem and why because

204
00:13:00,090 --> 00:13:00,990
these are

205
00:13:01,010 --> 00:13:02,970
componentwise inequalities

206
00:13:02,990 --> 00:13:08,660
so each component why i was to bounce lower bounds

207
00:13:08,680 --> 00:13:11,970
and why i an an upper bound on minus one

208
00:13:11,990 --> 00:13:16,610
so combine these inequalities say that's why i created than the absolute value of the

209
00:13:16,610 --> 00:13:18,950
i th component of x minus p

210
00:13:19,220 --> 00:13:22,200
i components between one minus y

211
00:13:22,260 --> 00:13:24,130
so that's an upper bound on the

212
00:13:24,150 --> 00:13:25,800
absolute value

213
00:13:25,820 --> 00:13:30,880
this objective function is separable so can minimize of each why i separately

214
00:13:30,890 --> 00:13:33,010
because the constraints are also uncoupled

215
00:13:33,030 --> 00:13:36,280
couple the difference and entries in my

216
00:13:36,300 --> 00:13:40,820
so the answer for this problem fix x will be to choose y equal to

217
00:13:40,820 --> 00:13:44,110
the absolute value of the i th component of a money to be

218
00:13:44,130 --> 00:13:46,130
if x is a fixed

219
00:13:48,160 --> 00:13:49,200
and then

220
00:13:49,220 --> 00:13:51,820
so that's the known as the one norm

221
00:13:51,880 --> 00:13:56,430
and if you're allowed to minimize of x and y then you choose this made

222
00:13:56,430 --> 00:13:57,860
the same choice for y

223
00:13:57,910 --> 00:13:59,240
for x u

224
00:13:59,240 --> 00:14:01,550
the point with the minimum one

225
00:14:03,050 --> 00:14:07,300
right hand side

226
00:14:10,010 --> 00:14:13,880
so here we minimize of x that's in nonlinear but convex

227
00:14:13,920 --> 00:14:15,720
optimisation problem

228
00:14:15,740 --> 00:14:19,800
here i introduce new vector variable y and actually that should be

229
00:14:19,820 --> 00:14:22,340
if a and this used

230
00:14:22,360 --> 00:14:24,280
rule dimension of a

231
00:14:24,280 --> 00:14:26,360
and i minimize jointly over x and y

232
00:14:26,380 --> 00:14:30,800
so that makes this an and into NLP because the objective is linear in all

233
00:14:30,800 --> 00:14:32,090
those labels

234
00:14:32,110 --> 00:14:36,300
and the cost also linear just linear inequalities in x y

235
00:14:38,360 --> 00:14:43,160
there's a synaptic activity here for the chebyshev norm infinity norm if i'm interested in

236
00:14:43,160 --> 00:14:44,590
minimizing the maximum

237
00:14:44,610 --> 00:14:46,970
after the value of x minus b

238
00:14:47,030 --> 00:14:50,180
then this would be the equivalent of an equivalent b

239
00:14:50,180 --> 00:14:53,510
so here the extra variable that introduces scalar

240
00:14:53,550 --> 00:14:55,300
set of effective

241
00:14:55,320 --> 00:14:56,630
and then the

242
00:14:56,650 --> 00:14:59,720
the constraints are that's

243
00:14:59,740 --> 00:15:03,240
so what do one here the boldface one is the vector of ones

244
00:15:03,260 --> 00:15:07,450
so each constraint users that the components of x minus b must be less than

245
00:15:08,220 --> 00:15:09,950
each component of x minus p

246
00:15:09,970 --> 00:15:12,450
and created and minus one

247
00:15:12,470 --> 00:15:14,260
for which

248
00:15:14,280 --> 00:15:18,010
so that means that the absolute value of the components of x minus b

249
00:15:18,030 --> 00:15:19,590
are bounded by y

250
00:15:20,410 --> 00:15:25,590
again to see the equivalence we can fix x here and then we minimize over

251
00:15:25,590 --> 00:15:27,090
the scale of the y

252
00:15:27,130 --> 00:15:29,300
that's very easy optimisation problem

253
00:15:29,360 --> 00:15:33,200
because you just have a bunch of lower bounds and y

254
00:15:33,240 --> 00:15:36,530
and quite equal to the largest lower bound

255
00:15:36,530 --> 00:15:41,990
the first this talk about coordinate ascent which is just another set-back box optimisation out

256
00:15:41,990 --> 00:15:45,910
of this

257
00:15:53,610 --> 00:15:56,390
and to describe coordinates and so on

258
00:15:57,090 --> 00:16:00,870
i just consider the problem just one should consider the problem of we want to

259
00:16:00,870 --> 00:16:09,510
maximize some function w which is a function of alpha one three alpha and

260
00:16:09,530 --> 00:16:11,720
with no constraints

261
00:16:12,370 --> 00:16:19,010
so for now forget about the constraint the alpha rise must be between zero and

262
00:16:19,010 --> 00:16:22,140
see forget about the constraint that you know some of what i alpha i must

263
00:16:22,140 --> 00:16:23,490
be zero

264
00:16:23,510 --> 00:16:27,390
on the distance the coordinates

265
00:16:30,010 --> 00:16:34,490
out of one on one repeat until convergence

266
00:16:34,550 --> 00:16:41,660
what are you want to on and the inner loop of cord in the sense

267
00:16:42,640 --> 00:16:45,240
hosts all the parameters

268
00:16:45,260 --> 00:16:47,930
except for i think

269
00:16:47,990 --> 00:16:51,970
and then it just maximizes the function with respect to just one of the parameters

270
00:16:52,010 --> 00:16:54,090
expected just one alpha i

271
00:16:54,110 --> 00:16:58,050
so let's write as alpha i guess data as

272
00:16:58,490 --> 00:17:03,990
argmax over writers i had

273
00:17:04,010 --> 00:17:13,990
of w alpha one alpha

274
00:17:16,820 --> 00:17:20,590
and this is this is really the a fancy way of saying you know hold

275
00:17:23,280 --> 00:17:35,280
it's alpha i and just optimise wrt lies by optimisation objective with respect to only

276
00:17:35,320 --> 00:17:36,700
alpha i

277
00:17:36,720 --> 00:17:39,660
this is just a fancy way writing it

278
00:17:39,680 --> 00:17:43,820
on and this is called an ascent

279
00:17:46,200 --> 00:17:48,430
one picture

280
00:17:50,240 --> 00:17:53,160
this can be useful for coordinates and on

281
00:17:53,200 --> 00:17:58,050
if you imagine you're trying to optimise a quadratic function

282
00:17:58,080 --> 00:18:02,450
on the look like that

283
00:18:02,470 --> 00:18:07,490
so is that these are the contours of a quadratic function and the minimum here

284
00:18:07,530 --> 00:18:12,660
this work or the center look like this my

285
00:18:12,700 --> 00:18:17,180
you can call itself into the columns of one mile for the other two axes

286
00:18:17,180 --> 00:18:19,570
and so let's say i start down here

287
00:18:19,590 --> 00:18:25,930
all that begin by minimizing disrespect alpha one so go there

288
00:18:26,890 --> 00:18:32,180
and and then when you point minimize respect alpha two simon that

289
00:18:32,200 --> 00:18:34,950
and then minimize respect one

290
00:18:34,970 --> 00:18:36,160
respect alpha two

291
00:18:36,180 --> 00:18:39,760
two and so on

292
00:18:40,450 --> 00:18:43,870
he always taking these axis aligned steps to

293
00:18:43,890 --> 00:18:46,300
it's the to try to get to the

294
00:18:47,720 --> 00:18:52,340
it turns out that design modifications to the sum the terms of the variations of

295
00:18:52,340 --> 00:18:53,430
this as well

296
00:18:54,120 --> 00:18:56,450
the way to describe the algorithm will always

297
00:18:56,470 --> 00:19:00,300
you know doing this an alternating or that we always optimise respect alpha one and

298
00:19:00,300 --> 00:19:04,450
alpha two and alpha one and alpha two are

299
00:19:04,470 --> 00:19:07,680
one about to say applies only in high dimensions but it turns out that we

300
00:19:07,680 --> 00:19:13,030
have a lot of parameters alpha one alpha and you may not choose always visit

301
00:19:13,030 --> 00:19:17,740
them in order you may choose to you have a heuristic to choose which alpha

302
00:19:17,740 --> 00:19:22,220
update next depending on what you think well i make no progress and have only

303
00:19:22,260 --> 00:19:26,110
two parameters and it makes sense to use alternate between them doesn't but if you

304
00:19:27,030 --> 00:19:28,340
high dimensional

305
00:19:28,430 --> 00:19:32,800
if you have high dimensional parameters that are sometimes you may use of update them

306
00:19:32,820 --> 00:19:37,490
in a different order doing so they make faster progress towards what's the minimum to

307
00:19:40,870 --> 00:19:44,120
and it turns out that common sense is on

308
00:19:44,120 --> 00:19:49,490
compared to some hours he saw previously compared to newton's method called the the centre

309
00:19:49,490 --> 00:19:51,550
usually take a lot more steps

310
00:19:51,550 --> 00:19:58,590
but on the chief advantage of corners and when it works well is that sometimes

311
00:19:58,610 --> 00:20:00,840
the optimisation objective w

312
00:20:01,840 --> 00:20:09,890
sometimes it's very inexpensive to optimize w respect any one of the parameters

313
00:20:09,950 --> 00:20:15,070
and so called centers take a lot to you many more iterations than newton's method

314
00:20:15,070 --> 00:20:16,590
in order to converge

315
00:20:17,530 --> 00:20:22,530
there it turns out that there are many optimisation problems for which is particularly easy

316
00:20:22,550 --> 00:20:23,590
to fix

317
00:20:23,610 --> 00:20:27,890
you know all but one of the parameters and optimize respect just that one parameter

318
00:20:27,910 --> 00:20:32,300
and if that's true then the inner loop of cornerstones sense of optimizing to alpha

319
00:20:32,300 --> 00:20:34,090
rai very quickly

320
00:20:34,090 --> 00:20:36,890
and called semi be very low

321
00:20:36,910 --> 00:20:41,640
and it turns out that this will be true when we modify this be to

322
00:20:41,640 --> 00:20:45,410
solve SVM optimisation problems

323
00:20:47,660 --> 00:20:54,870
questions about this

324
00:21:23,430 --> 00:21:25,550
so this been applied is

325
00:21:25,570 --> 00:21:29,070
two our support vector machine two optimisation problem

326
00:21:30,200 --> 00:21:32,680
so it turns out that on

327
00:21:32,700 --> 00:21:38,110
according to census in its basic form does not work for the following reason on

328
00:21:38,110 --> 00:21:43,260
reasons with constraints on the alpha rise we call from

329
00:21:43,280 --> 00:21:49,660
you know what i previously we have a constraint that summer i y alpha i

330
00:21:49,660 --> 00:21:51,240
must zero

331
00:21:51,240 --> 00:21:55,660
and so if you fix all the others except for one

332
00:21:55,700 --> 00:22:00,800
you know then you can't change one alpha without violating the constraint because if if

333
00:22:00,800 --> 00:22:05,450
i just had to change alpha one year while alpha one is actually exactly determines

334
00:22:05,450 --> 00:22:08,740
the function of the other alphas because because this was an interview

335
00:22:08,760 --> 00:22:11,430
and so there is a lot of

336
00:22:11,450 --> 00:22:17,120
which is the centre of gravity john has a colleague at microsoft on december out

337
00:22:17,120 --> 00:22:18,260
of them

338
00:22:18,280 --> 00:22:21,760
on therefore is trying to change one hour for the time

339
00:22:21,870 --> 00:22:24,140
we were trying to change

340
00:22:24,800 --> 00:22:25,720
o two

341
00:22:25,740 --> 00:22:33,470
so on what this is a simple algorithm

342
00:22:33,470 --> 00:22:40,260
sensor sequential minimal optimization and terminal refers to the fact that was choosing the smallest

343
00:22:40,260 --> 00:22:44,140
number of alpha rise to change over time which in this case changes to the

344
00:22:44,140 --> 00:22:48,020
actually quantify this behavior as you can see here this is based on the latent

345
00:22:48,240 --> 00:22:50,680
amount these are normal males

346
00:22:50,700 --> 00:22:54,080
and this is the female treated with the prostate gland in this is the male

347
00:22:54,080 --> 00:22:59,680
treated with inhibitor and this is normal females so we've completely sex reversed

348
00:22:59,690 --> 00:23:02,110
their behaviour

349
00:23:02,130 --> 00:23:08,960
so that's type one sex dimorphism that we have correlated very strongly brain behavioral change

350
00:23:08,960 --> 00:23:13,940
with the brain phenotype change let's go quickly to a second type of this sex

351
00:23:13,940 --> 00:23:19,350
dimorphism which is sexually dimorphic nucleus that i mentioned the SDE and which is some

352
00:23:19,350 --> 00:23:24,830
five times larger in males versus females so it's dark collection of spots each one

353
00:23:24,830 --> 00:23:29,900
of those black dots a single neuron if we treat the female near-daily with testosterone

354
00:23:29,900 --> 00:23:36,240
and look at the st adulthood she has now email sized STM there's an to

355
00:23:36,240 --> 00:23:40,450
this nucleus in the human brain to generate a lot of excitement when it was

356
00:23:40,450 --> 00:23:46,020
reported in the nineteen eighties it's the sex differences not nearly as dramatic as it

357
00:23:46,030 --> 00:23:48,510
is in in the rats but nonetheless is there

358
00:23:48,770 --> 00:23:53,210
so one could ask the question how does this sex difference come about

359
00:23:53,220 --> 00:23:58,150
there could be multiple ways right you could say well because males start with more

360
00:23:58,150 --> 00:24:03,050
neurons in one on surviving males males make more neurons something about sort but in

361
00:24:03,050 --> 00:24:07,380
fact the work of art on old and several other investigators found over the years

362
00:24:07,380 --> 00:24:12,130
that males and females start with the same number of neurons but since the male

363
00:24:12,130 --> 00:24:16,890
gets the as dial from is just as strong his neurons survive but in the

364
00:24:16,890 --> 00:24:18,680
female they die

365
00:24:18,700 --> 00:24:24,600
and they died during a specific sensitive period and as a result the males sdn

366
00:24:24,610 --> 00:24:26,990
is much much larger than the females

367
00:24:27,200 --> 00:24:31,160
now we know that this is a general principle for a way to make a

368
00:24:31,160 --> 00:24:35,670
brain region larger and one sex versus the other and we know this is true

369
00:24:35,670 --> 00:24:37,830
from multiple other nuclei

370
00:24:37,840 --> 00:24:43,700
all of which are directly related to reproduction so this was considered the only way

371
00:24:43,710 --> 00:24:48,170
to get more neurons in one sex versus the

372
00:24:48,180 --> 00:24:51,550
but as i said there are other types of sex differences right many that are

373
00:24:51,550 --> 00:24:53,380
not true dimorphism

374
00:24:53,460 --> 00:24:57,410
the other just variances and averages along a continuum

375
00:24:57,640 --> 00:25:03,200
so we turn our attention to the hippocampus this is the brain region not directly

376
00:25:03,200 --> 00:25:10,910
involved in reproduction instead major brain region controls stress response and spatial learning it's very

377
00:25:10,910 --> 00:25:15,400
very heavily studied for its role in learning and memory both of these two things

378
00:25:15,400 --> 00:25:18,650
should differences between males and females

379
00:25:18,670 --> 00:25:24,050
so we first asked is there sex difference in cell death in the hippocampus

380
00:25:24,080 --> 00:25:28,160
after exhaustive studies we could find no difference in the number of neurons that die

381
00:25:28,160 --> 00:25:33,920
during development of the male versus female hippocampus so then we asked the opposite is

382
00:25:33,920 --> 00:25:38,610
there difference in cell genesis between male and female hippocampus

383
00:25:38,620 --> 00:25:43,240
to study cell genesis you take a different approach in which you give a drug

384
00:25:43,240 --> 00:25:44,640
called BRD you

385
00:25:44,660 --> 00:25:50,090
deoxy uridine which will recognise as an analogue of DNA

386
00:25:50,100 --> 00:25:56,420
and it becomes incorporated into sl when it divides because the DNA is replicated we

387
00:25:56,420 --> 00:26:00,390
can then use antibodies to visualize the BRD you

388
00:26:00,460 --> 00:26:01,690
and we can thereby right

389
00:26:01,700 --> 00:26:04,140
date the birth of a new cells

390
00:26:04,330 --> 00:26:10,330
so what we did was injected animals females with male hormones we can build compared

391
00:26:10,360 --> 00:26:16,020
males and females in the presence of BRD you to ask are you making more

392
00:26:16,020 --> 00:26:19,010
neurons collected the brain's some days later

393
00:26:19,020 --> 00:26:22,860
what you can see is that the first just look at the female versus the

394
00:26:22,860 --> 00:26:28,150
male is that there's a dramatic sex difference the males making many more neurons than

395
00:26:28,150 --> 00:26:31,980
the female during this period of time this is in a couple of days after

396
00:26:31,980 --> 00:26:36,380
birth and the image that you see over here this is the hippocampus all those

397
00:26:36,390 --> 00:26:39,150
little black dots newly born cells

398
00:26:39,150 --> 00:26:44,880
and this is the male or female making fewer of these newly born cells

399
00:26:44,890 --> 00:26:50,690
so we can conclude testosterone stimulates cell genesis in the developing male brain but at

400
00:26:50,690 --> 00:26:54,400
that point we can only conclude that they are cells not that they are neurons

401
00:26:54,400 --> 00:26:56,630
i misspoke when i said that

402
00:26:56,640 --> 00:27:00,530
we have to ask what kind of cells will they become because stem cells can

403
00:27:00,530 --> 00:27:05,260
become multiple things they can become neural progenitor cells which will lead to

404
00:27:05,810 --> 00:27:07,220
principal neurons

405
00:27:07,240 --> 00:27:11,610
such as the program on the answer the campus or interneurons or they can become

406
00:27:12,350 --> 00:27:17,150
that will become the den sites that make them myelin for the astrocytes that i

407
00:27:17,150 --> 00:27:21,540
spoke about earlier to find out what kind of cells are going to become we

408
00:27:21,540 --> 00:27:30,100
so we should

409
00:28:59,090 --> 00:29:06,230
one the more

410
00:29:07,120 --> 00:29:11,700
say in you to be able

411
00:29:11,760 --> 00:29:13,680
always all

412
00:29:35,710 --> 00:29:37,840
is all

413
00:30:15,860 --> 00:30:37,400
one this is the first

414
00:30:37,410 --> 00:30:51,990
call letters were this follows

415
00:30:57,080 --> 00:30:58,460
o thing

416
00:31:02,410 --> 00:31:09,130
o one of

417
00:31:42,290 --> 00:31:44,190
one of the

418
00:31:44,290 --> 00:31:47,630
the problem was

419
00:31:54,370 --> 00:31:58,250
so the situation

420
00:32:19,860 --> 00:32:24,830
o four

421
00:32:33,920 --> 00:32:43,080
there's of

422
00:32:54,430 --> 00:32:58,330
what want

423
00:33:06,810 --> 00:33:08,860
it is

424
00:33:54,750 --> 00:33:55,550
it is

425
00:35:32,160 --> 00:35:40,260
so this is

426
00:36:27,650 --> 00:36:28,990
so the

427
00:36:33,950 --> 00:36:36,020
this is

428
00:36:36,040 --> 00:36:36,960
it is

429
00:36:36,960 --> 00:36:39,080
OK so

430
00:36:39,080 --> 00:36:42,710
i was in the middle of some vignettes yesterday but i got a couple questions

431
00:36:42,710 --> 00:36:46,360
i've that it's from the board and now how do i do this

432
00:36:46,370 --> 00:36:50,520
and we still have vegas tried like this i don't believe come on really think

433
00:36:50,520 --> 00:36:52,520
that's going work

434
00:36:52,680 --> 00:36:54,230
it does

435
00:36:57,610 --> 00:37:00,130
so i did a little bit of decision theory yesterday

436
00:37:00,140 --> 00:37:06,540
and the couple questions afterwards provoked we want to respond so

437
00:37:06,540 --> 00:37:09,840
i have a couple of different kinds of expectations

438
00:37:09,920 --> 00:37:14,660
of the loss function so there was an expectation

439
00:37:14,830 --> 00:37:19,210
the unconditional expectation the frequentist looked like this

440
00:37:19,260 --> 00:37:23,370
so no conditioning

441
00:37:23,380 --> 00:37:25,830
and there's is being held fixed

442
00:37:25,840 --> 00:37:29,840
and the expectations the threat the axis the provision of x given data if you

443
00:37:29,840 --> 00:37:33,300
want but indexed by status by the way frequencies one saying

444
00:37:33,300 --> 00:37:35,660
the unconditional expectation

445
00:37:35,670 --> 00:37:40,300
and it's important to note that this state is the same as the state

446
00:37:40,350 --> 00:37:44,230
that's really critical so just the function of of one theta not of two different

447
00:37:44,230 --> 00:37:45,350
kinds of data

448
00:37:46,510 --> 00:37:50,570
in the bayesian expectation was the conditional expectations

449
00:37:50,590 --> 00:37:54,770
so you might want to put a little exercise because that's being held fixed rate

450
00:37:54,770 --> 00:38:00,480
is now the random variable and this conditional expectation conditional x

451
00:38:00,740 --> 00:38:04,130
but they get in their way is a random quantity

452
00:38:05,900 --> 00:38:11,350
OK so those are two different things and so this object here

453
00:38:11,370 --> 00:38:14,870
is frequent expectation the kind of the definition i was given yesterday what it means

454
00:38:14,870 --> 00:38:17,740
to be frequentist the least and if you look at the paper he says are

455
00:38:17,740 --> 00:38:21,020
frequent sounds are going on here was that there was an expectation of the sample

456
00:38:22,070 --> 00:38:23,730
that's the kind of a giveaway

457
00:38:24,510 --> 00:38:28,480
and in particular to two examples of the loss function squared loss is the one

458
00:38:28,480 --> 00:38:33,630
that you know pattern that talk yesterday and today but in the papers it does

459
00:38:33,630 --> 00:38:37,400
the causes of this loss function is just three

460
00:38:37,400 --> 00:38:42,680
the los angeles now called delta have access to share statistical procedure let's call it

461
00:38:42,680 --> 00:38:44,500
they'd have to simplicity

462
00:38:44,520 --> 00:38:46,370
so my notations some

463
00:38:46,370 --> 00:38:51,470
and so this might be just data how does this square with a particular example

464
00:38:51,470 --> 00:38:55,580
of a loss function already familiar with the case that the loss

465
00:38:56,080 --> 00:38:59,210
this is a random quantity for free point of view and that's around the query

466
00:38:59,210 --> 00:39:01,390
from a bayesian point of view of

467
00:39:02,280 --> 00:39:08,270
and now for the frequentist a risk we take expectation of this thing

468
00:39:08,280 --> 00:39:11,340
or this user and query that to is fixed

469
00:39:11,370 --> 00:39:16,350
OK so if you do that you would write risk of data the expectation that

470
00:39:16,350 --> 00:39:18,520
they have stated square

471
00:39:18,750 --> 00:39:20,530
just manchester mean square

472
00:39:20,560 --> 00:39:24,020
right in the traditional thing to do at this point is two

473
00:39:24,030 --> 00:39:29,740
added subtracted quantity

474
00:39:29,750 --> 00:39:39,300
that quantities is now again in expectation of really care or all those expectations the

475
00:39:39,300 --> 00:39:42,740
frequentist expectation over the sample space

476
00:39:42,770 --> 00:39:44,680
OK so that's not just a number

477
00:39:44,690 --> 00:39:46,710
and this is still random

478
00:39:46,720 --> 00:39:47,970
right now you can

479
00:39:47,990 --> 00:39:52,120
paul all part the square and you get two pieces which are square get across

480
00:39:52,120 --> 00:39:58,150
product across vanishes because the cost always fashion statistics is called the pythagorean theorem

481
00:39:58,150 --> 00:40:02,060
so you get this one here

482
00:40:02,060 --> 00:40:08,310
that's called the variance

483
00:40:08,330 --> 00:40:12,430
and then you get this quantity here this is the constant

484
00:40:12,440 --> 00:40:17,890
with respect to that expectation as is this so we just simply get data minus

485
00:40:17,990 --> 00:40:21,620
caterham school

486
00:40:21,640 --> 00:40:24,300
that's called the square by

487
00:40:24,300 --> 00:40:28,340
OK so that's the bias variance decomposition that's all you've heard about that i just

488
00:40:28,340 --> 00:40:32,280
want to make clear that in doing that you're writing down these expectations with respect

489
00:40:32,280 --> 00:40:36,340
to data that's the frequentist expeditions over the sample space

490
00:40:36,360 --> 00:40:39,720
because when you are the words by very european frequentist

491
00:40:39,740 --> 00:40:42,520
but it's not the only thing that three-quarters do they don't just write down by

492
00:40:42,520 --> 00:40:46,670
the variance decompositions which just one of the most simple things you can do is

493
00:40:46,680 --> 00:40:47,720
very useful

494
00:40:47,740 --> 00:40:51,580
if you would square at least two separate things out of these two pieces and

495
00:40:51,580 --> 00:40:55,180
after you can argue separately on these two pieces the bias for example might go

496
00:40:55,180 --> 00:41:00,760
away asymptotically that's popular public desire and you also like the variance perhaps to go

497
00:41:00,760 --> 00:41:04,410
to zero asymptotically if you can show those two things together

498
00:41:04,430 --> 00:41:10,070
you've shown consistency either they'd had goes to theta in this case in mean square

499
00:41:10,130 --> 00:41:14,060
and that's that's what we mean by consistency in this particular setting i would talk

500
00:41:14,240 --> 00:41:18,870
much about consistency today in general consistency just means the number of data points larger

501
00:41:18,870 --> 00:41:20,890
converge the right answer

502
00:41:20,910 --> 00:41:24,370
OK that's what we mean by consistency and so one way in the simplest kind

503
00:41:24,370 --> 00:41:27,490
of parametric setting to do that is that if you would square loss to look

504
00:41:27,490 --> 00:41:31,880
at the by seymour et cetera okay so any more questions on that you will

505
00:41:31,880 --> 00:41:37,490
use the data means that the frequentist expectation other sample points and when you've got

506
00:41:37,490 --> 00:41:40,360
to bayesian thing to bayesian response to focus on

507
00:41:40,370 --> 00:41:44,030
the fact that they got

508
00:41:46,260 --> 00:41:49,860
was the only other thing that i want to say that i didn't say yesterday

509
00:41:49,890 --> 00:41:52,160
so this was just now

510
00:41:52,170 --> 00:41:57,500
cut through some of the slides by myself aware i was so we're talking about

511
00:41:57,550 --> 00:42:01,990
f divergences in experimental design surrogate losses

512
00:42:02,760 --> 00:42:08,260
and this was the first if i think for little vignettes in frequentist analysis so

513
00:42:08,260 --> 00:42:13,190
the goal here is to show you here is a kind of data analysis problem

514
00:42:13,200 --> 00:42:17,870
here's a fancy machine learning style algorithm of some kind of you not try to

515
00:42:17,870 --> 00:42:22,290
much datasets and that's great works fine all that the whole goal here is to

516
00:42:22,290 --> 00:42:26,030
say what it means to do some analysis of this list let's you frequentist analysis

517
00:42:26,030 --> 00:42:29,790
was convince yourself it's actually good method in some sense that it works well on

518
00:42:29,790 --> 00:42:31,510
dataset for the clever

519
00:42:31,560 --> 00:42:34,240
OK so i'm going to show you have that kind of thing is carried out

520
00:42:34,240 --> 00:42:37,740
in a bunch of little little vignettes because the first one had to do with

521
00:42:37,740 --> 00:42:43,330
the experimental design problem where we sitting in developing the quantizer as well as a

522
00:42:43,370 --> 00:42:48,550
discriminant function jointly so we can have that kind of setup where we had xis

523
00:42:48,550 --> 00:42:53,500
which are in know available and environment principle you can measure them but you don't

524
00:42:53,500 --> 00:42:59,060
use them centrally was distributed classification problem we first quantizing the easy access by an

525
00:42:59,060 --> 00:43:06,720
experimental design or a quantizer q and the disease are small numbers of bits and

526
00:43:06,720 --> 00:43:11,410
transmit them to a central site we can do the quantization for the discrimination which

527
00:43:11,410 --> 00:43:12,800
is the function gamma

528
00:43:12,850 --> 00:43:17,440
so this is kind of an example of lot providing an emerging field of distributed

529
00:43:17,440 --> 00:43:21,500
show that that's equal to the variance of the original variable x

530
00:43:21,500 --> 00:43:25,820
divided by the number of observations of x so that the well known result that

531
00:43:25,820 --> 00:43:29,830
you've almost certainly see this is that the standard deviation of new had is equal

532
00:43:29,830 --> 00:43:31,390
to the standard deviation

533
00:43:31,400 --> 00:43:33,490
of the original variable x

534
00:43:33,500 --> 00:43:34,980
divided by the square root

535
00:43:35,070 --> 00:43:38,960
of the number of observations so that i just guessed the function i just guessed

536
00:43:38,960 --> 00:43:41,070
that that would be inappropriate estimator

537
00:43:41,120 --> 00:43:46,500
and that claim that if you worked out those properties that that's what you get

538
00:43:46,670 --> 00:43:51,380
OK suppose i consider the the variance of x as my parameter i want to

539
00:43:53,590 --> 00:43:58,000
an estimator for it so sigma squared had what would that be

540
00:43:58,960 --> 00:44:02,440
again if you look back at what the definition of the variances it was something

541
00:44:02,440 --> 00:44:06,580
like the expectation value of the quantity x minus mu

542
00:44:07,750 --> 00:44:11,590
so if you think again about these expectation values is being averages

543
00:44:11,640 --> 00:44:14,500
then you might be led to to write down some kind of a

544
00:44:14,510 --> 00:44:19,180
i guess for the estimator like this formula here is sort of the average of

545
00:44:19,180 --> 00:44:21,760
the square of x minus its

546
00:44:21,820 --> 00:44:23,430
arithmetic average

547
00:44:23,440 --> 00:44:29,340
that's also is sufficiently important estimated that has a special name and notation

548
00:44:29,390 --> 00:44:32,310
you've probably seen this before you've probably seen that has this

549
00:44:32,320 --> 00:44:35,490
very often people use this one over n minus one

550
00:44:35,500 --> 00:44:38,750
and you think well hang on why not use one over and it's supposed to

551
00:44:38,760 --> 00:44:42,780
somehow represent an average why why use one over n minus one

552
00:44:42,830 --> 00:44:46,110
and the reason is the following the reason is that if you define it this

553
00:44:46,110 --> 00:44:48,380
way with the one over n minus one

554
00:44:48,440 --> 00:44:52,630
and if you work out the bias of this estimator then you find exactly

555
00:44:53,880 --> 00:44:58,000
if you had one over and then you would have a small bias

556
00:44:58,010 --> 00:45:01,760
it actually we have a slightly smaller variance but but it would have nonzero bias

557
00:45:01,760 --> 00:45:02,380
and so

558
00:45:02,380 --> 00:45:06,960
people somehow would prefer to work with estimators that have zero bias so that's the

559
00:45:06,960 --> 00:45:09,020
entire motivation free for this

560
00:45:09,030 --> 00:45:11,130
for this formula

561
00:45:11,240 --> 00:45:14,800
so now things get complicated now if you want to work at the variance of

562
00:45:14,800 --> 00:45:17,110
the estimator of the variance

563
00:45:17,120 --> 00:45:21,700
then fine you can work that out as well and here's what the

564
00:45:21,710 --> 00:45:25,210
you get you get some formulas that depends on what are called the the central

565
00:45:26,030 --> 00:45:28,320
of the distribution u four and u two

566
00:45:28,320 --> 00:45:31,960
which can be related to the PDF in this way

567
00:45:32,010 --> 00:45:33,430
i think we don't need

568
00:45:33,440 --> 00:45:35,670
the labour that point but

569
00:45:35,680 --> 00:45:39,990
so the point is that i simply wrote down some estimators for mu and sigma

570
00:45:41,130 --> 00:45:42,540
by guessing

571
00:45:42,550 --> 00:45:45,680
by looking at the original definition of the parameter and saying well if i had

572
00:45:45,680 --> 00:45:49,310
a finite data sample what would be a good way to

573
00:45:49,330 --> 00:45:51,140
to estimate the parameters

574
00:45:51,160 --> 00:45:52,450
now in general

575
00:45:52,470 --> 00:45:55,090
there's only so much mileage that you can get out of that type of approach

576
00:45:55,370 --> 00:45:57,490
what we need now is more general

577
00:45:57,510 --> 00:46:02,950
method that will allow us to construct functions that can be used to estimate parameters

578
00:46:02,990 --> 00:46:03,680
and so

579
00:46:03,700 --> 00:46:07,350
i want to talk about the by far the most important method is the method

580
00:46:07,350 --> 00:46:12,260
of maximum likelihood so to introduce that i mean the first say what the likelihood

581
00:46:12,260 --> 00:46:13,760
function is

582
00:46:13,830 --> 00:46:18,630
so the idea is the following suppose the outcome of some experiments is this set

583
00:46:18,630 --> 00:46:19,920
of numbers

584
00:46:19,960 --> 00:46:23,760
suppose i model that is coming from some joint pdf that

585
00:46:23,800 --> 00:46:29,030
it is characterized by some parameter or vector of parameters theta

586
00:46:29,100 --> 00:46:34,330
now the idea is that you can evaluate this PDF with the data you got

587
00:46:34,340 --> 00:46:38,610
and regarded as a function of the parameter or parameters

588
00:46:38,630 --> 00:46:40,860
that's called the likelihood function

589
00:46:40,870 --> 00:46:42,870
so the likelihood function

590
00:46:42,960 --> 00:46:44,280
is the same

591
00:46:44,290 --> 00:46:46,990
as this joint pdf for the data

592
00:46:47,000 --> 00:46:50,140
but i'm regarding it not as a function of the data i regard the data

593
00:46:50,140 --> 00:46:55,660
as constant look at that as a function of the parameter or parameters

594
00:46:55,680 --> 00:46:59,000
so that's the general

595
00:46:59,040 --> 00:47:03,080
a way to write down the likelihood function now a very common situation

596
00:47:03,090 --> 00:47:07,550
is it i i have a random variable x that follows some pdf f of

597
00:47:08,760 --> 00:47:10,500
and i observe an

598
00:47:12,630 --> 00:47:16,980
values of the variable all coming from the same period

599
00:47:17,000 --> 00:47:21,060
well i insist the observations are independent remember we just we define what we mean

600
00:47:21,060 --> 00:47:24,550
by independence that meant that the probability for all of them

601
00:47:24,580 --> 00:47:28,200
was given by the product of the individual probabilities

602
00:47:28,220 --> 00:47:34,160
so if i insisted i have an independent observations all following the same PDF

603
00:47:34,170 --> 00:47:37,220
then the joint pdf is simply given by the product

604
00:47:37,290 --> 00:47:39,140
of the individual pdfs

605
00:47:39,150 --> 00:47:42,340
and that's very often the situation we find ourselves in

606
00:47:42,380 --> 00:47:46,790
so that's very very commonly the formula that you'll see written down as the likelihood

607
00:47:47,770 --> 00:47:52,600
but that's that's really only pertains to this special case more generally the likelihood function

608
00:47:52,600 --> 00:47:55,080
is simply the probability for the data

609
00:47:55,090 --> 00:47:56,450
given the parameter

610
00:47:56,460 --> 00:47:58,870
evaluated with the data that you got

611
00:47:58,910 --> 00:48:04,770
and then treated as a function of the parameters

612
00:48:07,240 --> 00:48:08,650
here's some examples of

613
00:48:08,670 --> 00:48:10,710
of likelihood functions

614
00:48:10,720 --> 00:48:12,540
here what i've done

615
00:48:13,630 --> 00:48:16,170
it's not likely functions which is to illustrate the

616
00:48:16,760 --> 00:48:17,930
the idea

617
00:48:17,980 --> 00:48:24,220
what i've done here is i've taken a guassian distributions and using the monte carlo

618
00:48:24,220 --> 00:48:31,250
method i generated a sample generated some values that came from that guassian distribution

619
00:48:32,280 --> 00:48:36,530
the idea is that if the hypothesized value of the parameter is very close to

620
00:48:36,530 --> 00:48:37,660
the true value

621
00:48:37,700 --> 00:48:41,200
then what you expect is a very high probability

622
00:48:41,250 --> 00:48:42,760
to get data

623
00:48:42,780 --> 00:48:45,450
at least similar to the data that you've got

624
00:48:45,450 --> 00:48:49,940
engendering be manifest knowledge so this is what knowledge representation

625
00:48:50,000 --> 00:48:53,410
and a few readers like me i really don't understand what he's saying

626
00:48:54,030 --> 00:48:55,790
it's actually he's trying to make two

627
00:48:55,800 --> 00:49:01,250
two points and i understood that because in nineteen labels on a new label the

628
00:49:01,290 --> 00:49:05,800
but first the a one says that knowledge can be interpreted proposition another which you

629
00:49:05,800 --> 00:49:08,200
can have an attitude which

630
00:49:08,670 --> 00:49:10,410
is either true or false

631
00:49:10,580 --> 00:49:17,140
so the knowledge is just an attitude and that it can be interpreted as propositional

632
00:49:17,210 --> 00:49:20,440
both some straw polls

633
00:49:20,500 --> 00:49:25,140
and the second part is that determines exactly how the system behaves so

634
00:49:25,790 --> 00:49:30,670
the reasoning that you do the manipulation of the symbols you perform on explicit knowledge

635
00:49:30,670 --> 00:49:32,830
is going to eventually

636
00:49:32,860 --> 00:49:36,520
determine the behavior have been works

637
00:49:36,530 --> 00:49:41,040
so knowledge based system is one that's designed in this way the stuff that explicitly

638
00:49:41,040 --> 00:49:43,790
represented can be interpreted propositional true false

639
00:49:43,870 --> 00:49:46,110
you've done plenty of true false stops

640
00:49:46,120 --> 00:49:49,220
you should be fairly comfortable there and

641
00:49:49,280 --> 00:49:53,530
furthermore the as you crank handling you manipulate symbols and so on what gets better

642
00:49:53,530 --> 00:49:58,810
at the end you can use that to figure out how the thing works

643
00:49:58,820 --> 00:50:03,360
OK i'm not sure how many of you can make a little bit of problem

644
00:50:03,410 --> 00:50:07,710
so let me try and explain this programming language now the beauty of prolog is

645
00:50:07,710 --> 00:50:10,360
that when it comes straight from first order logic

646
00:50:10,530 --> 00:50:14,040
and i see this idea of taking first order logic and turning it into a

647
00:50:14,040 --> 00:50:15,420
programming language

648
00:50:16,290 --> 00:50:19,510
let's look at this program each

649
00:50:19,790 --> 00:50:23,160
line can you can turn accentuate take to be

650
00:50:24,410 --> 00:50:25,860
an implication

651
00:50:27,260 --> 00:50:29,220
i'm going to write the error

652
00:50:29,430 --> 00:50:30,500
that one

653
00:50:30,510 --> 00:50:36,540
if you think of implication is our narratives that one

654
00:50:36,630 --> 00:50:41,830
sorry i'm looking at my and your screen should be that way

655
00:50:41,990 --> 00:50:44,540
OK so basically it says that

656
00:50:45,570 --> 00:50:48,890
this thing on the right-hand and left-hand side sorry this goal

657
00:50:48,900 --> 00:50:50,410
it is true

658
00:50:50,460 --> 00:50:52,760
provided the things on the right-hand side true

659
00:50:54,330 --> 00:50:59,360
let's look at the bottom of which we use the terms in the first half

660
00:50:59,360 --> 00:51:01,360
is just a fairly

661
00:51:01,430 --> 00:51:04,620
can sort procedure approach

662
00:51:04,690 --> 00:51:10,310
let's have a look at this one

663
00:51:10,410 --> 00:51:15,470
so this rule basically says that if the the colour of some object z is

664
00:51:17,130 --> 00:51:19,140
and x is made of z

665
00:51:19,150 --> 00:51:22,530
then the colour x y

666
00:51:22,540 --> 00:51:26,170
and then we've got some other information here might say that

667
00:51:26,190 --> 00:51:31,800
grass is native vegetation and the color vegetation is great

668
00:51:31,820 --> 00:51:35,140
so the the things done the bottom just single

669
00:51:35,160 --> 00:51:40,920
sort of propositions and the things above with this call dashes just an implication is

670
00:51:41,010 --> 00:51:43,790
location one

671
00:51:45,090 --> 00:51:48,110
if we try and execute this program essentially the way it works is we we

672
00:51:48,110 --> 00:51:53,020
ask queries for instance if we are sort of fairly simple query

673
00:51:53,040 --> 00:51:56,310
we twitch natural first program if as query

674
00:51:56,370 --> 00:51:59,370
pretty colors no truth

675
00:51:59,410 --> 00:52:02,260
well it says well in order to figure that out

676
00:52:02,280 --> 00:52:05,580
i've got to see if i can prove things on the right-hand side because the

677
00:52:05,580 --> 00:52:07,170
implication is that way

678
00:52:07,280 --> 00:52:10,180
so this is the things on this sort of implication the true

679
00:52:10,290 --> 00:52:13,190
in the things on this this sort of implication true

680
00:52:13,310 --> 00:52:15,670
so it works backwards you say

681
00:52:15,690 --> 00:52:18,680
what's the it's pretty colours notrel falls

682
00:52:18,770 --> 00:52:21,940
he goes well i can figure that out if i could figure out where the

683
00:52:21,970 --> 00:52:23,130
things on the

684
00:52:23,140 --> 00:52:25,370
right hand side of the implication true

685
00:52:25,410 --> 00:52:27,700
and in this case all going to do is going to write something to the

686
00:52:27,700 --> 00:52:29,280
screen and say yes

687
00:52:29,360 --> 00:52:31,580
similarly for photographs and so on

688
00:52:31,630 --> 00:52:35,910
so the the first program is what we would call a fairly sort of procedural

689
00:52:37,150 --> 00:52:41,070
it doesn't really contain any knowledge is no sense in which there is any

690
00:52:41,130 --> 00:52:46,030
you know there's any sort of inference going on in trying to work out whether

691
00:52:46,030 --> 00:52:50,310
you know pretty colors not

692
00:52:50,370 --> 00:52:54,360
in the second one in the second programme however we've tried to

693
00:52:54,400 --> 00:52:58,080
it is out of it we try to write down some facts about the domain

694
00:52:58,080 --> 00:53:00,910
in the logic which when the program

695
00:53:00,950 --> 00:53:04,400
execute so when you ask questions this program it will need to

696
00:53:04,410 --> 00:53:07,310
so to figure out there will need to do some inference in order to work

697
00:53:07,310 --> 00:53:08,620
out whether

698
00:53:08,630 --> 00:53:10,210
the perceptron

699
00:53:10,220 --> 00:53:12,190
the question is true

700
00:53:12,200 --> 00:53:16,920
so i've down some facts and some of the facts well colors snow is white

701
00:53:16,920 --> 00:53:20,970
the color the sky yellow the colour vegetation is green

702
00:53:21,000 --> 00:53:23,520
grass is native vegetation

703
00:53:23,560 --> 00:53:27,900
if i know the colour of something some objects that's why

704
00:53:27,910 --> 00:53:30,870
and x is made of z

705
00:53:30,880 --> 00:53:33,170
and i know the color must be one

706
00:53:33,280 --> 00:53:36,250
so we could some simple sort of knowledge

707
00:53:36,290 --> 00:53:39,210
and these things we to take to be true by

708
00:53:39,220 --> 00:53:42,000
and then you for us to carry well we can just go through our inference

709
00:53:42,000 --> 00:53:44,880
process and what is

710
00:53:46,060 --> 00:53:49,840
in prolog even though you can sort of the work is itself by the sages

711
00:53:49,860 --> 00:53:51,040
going backwards

712
00:53:51,040 --> 00:53:55,590
solving some calls it turns out that the essential inference procedure has being used just

713
00:53:56,890 --> 00:54:01,790
just the resolution proof to work out whether some question is true or false

714
00:54:01,890 --> 00:54:05,770
OK let's not going get too worried about how prolog execute and so on but

715
00:54:05,770 --> 00:54:08,920
the main point i want to make about this example is in the lower half

716
00:54:08,930 --> 00:54:12,890
we've tried to explicitly write down some knowledge is the knowledge that we have is

717
00:54:12,910 --> 00:54:16,110
the colour of certain things in the sky vegetation

718
00:54:16,160 --> 00:54:18,780
what things are made of so we know what crosses made of

719
00:54:18,790 --> 00:54:20,330
and some rules

720
00:54:20,360 --> 00:54:23,680
like how we determine the color from

721
00:54:24,790 --> 00:54:27,570
nine how something is made

722
00:54:27,630 --> 00:54:31,150
and the suggestion is that the bottom half is this is a better way of

723
00:54:31,150 --> 00:54:33,340
doing things you have written down what you know

724
00:54:33,490 --> 00:54:35,390
you derive new knowledge

725
00:54:35,400 --> 00:54:37,550
so the question would be why

726
00:54:37,570 --> 00:54:39,760
why is the bottom half

727
00:54:39,760 --> 00:54:42,040
look more attractive than top

728
00:54:42,090 --> 00:54:44,770
or if you think the top looks more attractive than you can

729
00:54:44,790 --> 00:54:46,470
you can let me tell me

730
00:54:46,580 --> 00:54:48,590
that's the question to you

731
00:54:48,600 --> 00:54:53,680
the even agree with me that the so the bottom half somewhat more intuitive leap

732
00:54:53,940 --> 00:54:56,740
the top

733
00:54:58,440 --> 00:55:03,370
it does definitely does something it does the same as the talk about the equivalent

734
00:55:03,370 --> 00:55:07,620
it turns but but notice well we're not quite there yet

735
00:55:08,120 --> 00:55:11,940
it turns out that you can't get rid of this is actually a lower bound

736
00:55:12,020 --> 00:55:13,020
matches it

737
00:55:13,020 --> 00:55:16,100
so so now that we've almost got an algorithm for

738
00:55:16,120 --> 00:55:19,120
approximately optimally solving

739
00:55:19,140 --> 00:55:21,620
an MDP

740
00:55:23,230 --> 00:55:24,710
if we can express

741
00:55:24,710 --> 00:55:26,520
w needs to be

742
00:55:28,230 --> 00:55:33,460
to guarantee if w can be independent of the number of states we've got result

743
00:55:33,480 --> 00:55:36,600
now we can pick w the way that we just we did earlier for the

744
00:55:36,600 --> 00:55:40,170
uniformbandit because notice that

745
00:55:40,170 --> 00:55:42,420
you know these values here

746
00:55:42,420 --> 00:55:48,390
are only approximate values of the star because there's that propagates up the tree

747
00:55:48,420 --> 00:55:50,940
and so the real sort of contribution

748
00:55:50,960 --> 00:55:56,140
theoretically that paper is to show that the doing a careful analysis of this recursion

749
00:55:56,190 --> 00:55:59,540
and showing that these errors still propagate too badly

750
00:55:59,580 --> 00:56:04,000
and in fact you can pick you get to pick larger w then what we

751
00:56:04,000 --> 00:56:07,690
did before but can pick w in a way that does not depend on the

752
00:56:07,690 --> 00:56:09,040
number of states

753
00:56:10,980 --> 00:56:16,980
that that is the basic theoretical result in the in the current paper this is

754
00:56:16,980 --> 00:56:20,850
the journal version really occurred in nineteen ninety nine

755
00:56:20,890 --> 00:56:23,440
which is surprising

756
00:56:23,500 --> 00:56:28,020
it's a major that problem so the good news is that we can achieve near

757
00:56:28,020 --> 00:56:33,310
optimality for value of w that's independent of the state space size

758
00:56:33,330 --> 00:56:35,540
and this is the first sort of near optimal

759
00:56:35,560 --> 00:56:37,270
mdp algorithm

760
00:56:37,310 --> 00:56:40,580
that was independent of the state space size so i was

761
00:56:40,580 --> 00:56:45,250
one of the landmark result the bad news is the theoretical values are typically quite

762
00:56:45,250 --> 00:56:50,480
large always the case and were exponential in h

763
00:56:50,500 --> 00:56:54,000
they show a lower bound in that paper that shows you can

764
00:56:54,020 --> 00:56:57,920
is the title about you can't get rid of h from the exponent

765
00:56:57,940 --> 00:57:00,440
right so

766
00:57:00,500 --> 00:57:04,580
so we sort of trade-off depends on the number of states to dependence to the

767
00:57:04,580 --> 00:57:06,350
horizon more less

768
00:57:06,370 --> 00:57:09,190
so that's what you end up with

769
00:57:09,230 --> 00:57:11,310
in practice

770
00:57:11,330 --> 00:57:14,830
you know if you have a heuristic function is a small value of h

771
00:57:14,850 --> 00:57:18,770
you'll get some improvements

772
00:57:18,790 --> 00:57:23,980
it's an MDP

773
00:57:24,100 --> 00:57:26,580
no it just assumes it's an MDP

774
00:57:26,600 --> 00:57:31,170
so the markov property is all really assumes it can be cyclic it does it

775
00:57:31,170 --> 00:57:35,500
doesn't matter

776
00:57:35,520 --> 00:57:40,480
so i think they also some bounded rewards you could probably get rid of that

777
00:57:40,500 --> 00:57:42,350
if you want

778
00:57:44,040 --> 00:57:48,310
so that's sort of the theoretical result that that sort of

779
00:57:48,330 --> 00:57:53,710
there are for historical interest it's not the most practical algorithm obviously have to build

780
00:57:53,710 --> 00:57:56,290
the big tree

781
00:57:56,330 --> 00:58:02,670
and it doesn't actually seem so good from an intuitive standpoint

782
00:58:02,710 --> 00:58:07,870
if we think about what it's doing is generating a huge amount of work

783
00:58:07,890 --> 00:58:12,500
for this action here and what if we have two actions that

784
00:58:12,520 --> 00:58:15,770
sort of one is a lot better than the other one

785
00:58:15,810 --> 00:58:21,170
it feels like a waste to spend the same effort generating history is history it

786
00:58:21,170 --> 00:58:21,870
would be nice

787
00:58:21,890 --> 00:58:24,730
how we could adaptively

788
00:58:24,790 --> 00:58:29,350
control the amount of work we put into figuring out the values for these actions

789
00:58:29,350 --> 00:58:33,940
so actions that are clearly suboptimal early on we don't worry about we put our

790
00:58:33,940 --> 00:58:36,870
resources into disambiguating

791
00:58:36,890 --> 00:58:39,080
closer to optimal actions

792
00:58:39,440 --> 00:58:44,330
so it's for that case for that reason i'm referring to sparse sampling is sort

793
00:58:44,330 --> 00:58:48,980
of a uniform monte carlo approach is not very smart about where it spends its

794
00:58:49,810 --> 00:58:54,600
it's just let's just do this thing and return the action

795
00:58:58,290 --> 00:59:02,440
now we're going to get into the adaptive monte carlo and we're going to start

796
00:59:02,440 --> 00:59:06,580
out by going to the single state case again

797
00:59:06,600 --> 00:59:11,560
so we end up with a monte carlo with a multi armed bandit problem and

798
00:59:11,560 --> 00:59:13,790
the change the objective and this is actually the

799
00:59:14,240 --> 00:59:20,440
the original objective of the multi armed bandit problem from the nineteen sixties

800
00:59:20,440 --> 00:59:26,940
this problem in experimental design that sort of where it came out originated

801
00:59:27,980 --> 00:59:30,670
we're going to call this the regret minimisation

802
00:59:30,690 --> 00:59:33,650
objective for bandits

803
00:59:33,710 --> 00:59:37,850
or loss minimisation has been called that as well so we want to do is

804
00:59:37,850 --> 00:59:43,650
find the arm pulling strategy such that the expected total reward at time n is

805
00:59:43,650 --> 00:59:45,750
close to the best possible

806
00:59:45,770 --> 00:59:49,980
so you know if we went to vegas we have the slot machines and some

807
00:59:49,980 --> 00:59:56,790
of them were biased positively and some biased negatively if we knew that best are

808
00:59:56,810 --> 00:59:59,690
we go there and pull it and that's the best we could do

809
01:00:00,020 --> 01:00:02,600
on average in the limit

810
01:00:02,620 --> 01:00:07,330
now we we we don't know the biases of these arms and so the question

811
01:00:07,330 --> 01:00:13,420
is what we want to pull arms because we want to make money but after

812
01:00:13,420 --> 01:00:15,390
and poles we'd like to somehow

813
01:00:15,420 --> 01:00:18,270
ensure that we're close to the optimal

814
01:00:18,290 --> 01:00:19,940
if we had no

815
01:00:20,000 --> 01:00:23,710
the best arm was so if you don't know what the best arm is how

816
01:00:24,560 --> 01:00:28,620
can we do after a particular number of samples to if we had known the

817
01:00:28,620 --> 01:00:29,850
optimal arm

818
01:00:29,870 --> 01:00:33,710
this this sort of problem that exploration and exploitation

819
01:00:33,730 --> 01:00:37,120
because you need to explore arms at some rate

820
01:00:37,140 --> 01:00:43,370
in order to find ones that are promising but you also need to exploit arms

821
01:00:43,370 --> 01:00:47,730
that already looked promising in order to make sure your profits don't go to

822
01:00:47,750 --> 01:00:52,420
the one thing it's pretty clear is that the uniformbandit is a really bad choice

823
01:00:52,420 --> 01:00:56,500
four if you're in vegas city what have a slot a bunch of slot machines

824
01:00:56,500 --> 01:01:00,870
in front of you uniformbandit will start at the first one paul w times second

825
01:01:00,870 --> 01:01:05,020
one w times and so on and so forth but really would probably sort of

826
01:01:05,020 --> 01:01:09,270
walk around and paul different arms and keep statistics of these arms and pull the

827
01:01:09,270 --> 01:01:14,100
one that looks most promising the question here is other algorithms that can get some

828
01:01:14,100 --> 01:01:16,080
theoretical bounds on how close

829
01:01:16,100 --> 01:01:19,000
two of more you're going to be without knowing

830
01:01:19,000 --> 01:01:22,350
just begin the this the

831
01:01:22,390 --> 01:01:24,130
there's an extraordinary

832
01:01:24,530 --> 01:01:34,090
a sense of verbal chaos kind of word hoard that modern poetry and modernism generally

833
01:01:34,100 --> 01:01:42,610
kind of linguistic environment of great complexity from which modern poetry and modernism emerged

834
01:01:42,630 --> 01:01:49,380
this is an image called rotterdam by the artist edward ones

835
01:01:49,400 --> 01:01:53,870
it's a woodcut image from blast i like it because it's the kind of image

836
01:01:53,870 --> 01:01:58,340
of the modern city that makes the modern city look like language look like letters

837
01:01:58,340 --> 01:02:03,650
look like a kind of scout scattered alphabet a kind of babble

838
01:02:03,660 --> 01:02:09,910
i read it it's picture for me of linguistic environment if you will of modern

839
01:02:11,180 --> 01:02:17,850
behind this environment are the great social processes of migration and modernization

840
01:02:17,860 --> 01:02:21,050
that produced that new urban form

841
01:02:21,100 --> 01:02:23,070
the metropolis

842
01:02:23,090 --> 01:02:30,390
all the poetry even that new england hayseed robert frost begin their careers in metropolitan

843
01:02:31,390 --> 01:02:34,800
primarily in london and new york

844
01:02:34,830 --> 01:02:38,450
all that is solid melts in the air

845
01:02:38,470 --> 01:02:39,940
karl marx said

846
01:02:39,990 --> 01:02:45,760
evoking the accelerating transformation of modern economic and social life

847
01:02:45,770 --> 01:02:51,010
the metropolis is the centre of this unsettled world that marks describes

848
01:02:51,030 --> 01:02:52,840
coming to the metropolis

849
01:02:52,840 --> 01:03:00,360
a hundred or ninety years ago now entailed for the writers that will be reading

850
01:03:00,370 --> 01:03:02,180
as much as for anyone else

851
01:03:02,260 --> 01:03:05,640
i can break with the world that they had known to break either with the

852
01:03:05,650 --> 01:03:07,880
native language

853
01:03:07,890 --> 01:03:08,920
and this is what the

854
01:03:08,940 --> 01:03:12,470
immigrant or the expatriate experiences

855
01:03:12,480 --> 01:03:19,610
or perhaps with native ways of speaking in knowing familiar spheres of reference

856
01:03:19,610 --> 01:03:22,600
life in the modern metropolis was

857
01:03:22,650 --> 01:03:29,820
the familiar rising it d naturalized language where there are many languages in use

858
01:03:29,870 --> 01:03:37,080
well language comes to seem arbitrary rather than natural as the product of convention

859
01:03:37,120 --> 01:03:42,950
not something you simply born into but something that you learn something that is made

860
01:03:42,960 --> 01:03:45,180
and that can be remade

861
01:03:45,210 --> 01:03:51,760
this is a presumption of all the poets we'll be reading modern american writers and

862
01:03:51,760 --> 01:03:54,590
artists immigrated famously

863
01:03:54,640 --> 01:03:56,230
two london

864
01:03:56,260 --> 01:03:57,910
to paris

865
01:03:58,160 --> 01:04:02,920
another key event in the making of modernism as the great migration of african americans

866
01:04:02,920 --> 01:04:04,420
from the rural south

867
01:04:04,440 --> 01:04:05,860
to the urban north

868
01:04:05,870 --> 01:04:13,360
langston hughes's poetry comes out of this experience in the community of black intellectuals and

869
01:04:13,360 --> 01:04:16,000
artists created specifically

870
01:04:16,050 --> 01:04:17,630
in harlem

871
01:04:17,640 --> 01:04:22,180
and you'll see on your hand out two quotations from poems by

872
01:04:22,240 --> 01:04:31,950
hughes the first of one twenty fifth street giving as well here images of black

873
01:04:31,950 --> 01:04:35,210
life in the rural south transpose two

874
01:04:36,970 --> 01:04:42,910
there is in those images i think kind of utopian promise that the familiar ordinary

875
01:04:42,910 --> 01:04:48,630
pleasures rural life can be recaptured in the society of plenty

876
01:04:48,680 --> 01:04:55,800
but also something hallucinatory and and troubling about those images and vaguely disturbing

877
01:04:55,820 --> 01:05:02,030
that's brought out i think in the related famous poem harlem on the

878
01:05:02,090 --> 01:05:08,550
the next side of the page where well if we had faces is food in

879
01:05:08,630 --> 01:05:19,840
the first text something possibly reassuring those spaces begin to look like dangerous objectification is

880
01:05:19,840 --> 01:05:26,070
in the second one where that reason the sun threatens to

881
01:05:27,880 --> 01:05:34,110
metropolis is in modern poetry set against a backdrop of

882
01:05:34,150 --> 01:05:38,300
war and violence and conflict and

883
01:05:38,470 --> 01:05:45,320
modern poetry as it absorbs the world of the metropolis absorbs that

884
01:05:45,380 --> 01:05:50,280
that violence and energy as well

885
01:05:50,550 --> 01:05:56,400
the metropolis well is it's it's a place of ambivalence a place promise

886
01:05:56,610 --> 01:05:58,010
and the threat

887
01:05:58,240 --> 01:06:02,030
of exultation and also of dread

888
01:06:03,050 --> 01:06:09,380
ambivalence that i'm describing is is that the the centre modern literature generally

889
01:06:09,400 --> 01:06:10,550
the metropolis

890
01:06:10,570 --> 01:06:12,490
is crowded with language

891
01:06:12,490 --> 01:06:17,780
crowded with faces but there's also a pervasive sense of

892
01:06:18,900 --> 01:06:24,130
in loneliness and of loss capture also again

893
01:06:24,180 --> 01:06:31,010
paradigmatic lee in the wasteland in i've i included there more lines from that poem

894
01:06:31,010 --> 01:06:34,300
the nymphs are departed ellie says

895
01:06:34,320 --> 01:06:39,950
is speaking of spiritual and imaginative state

896
01:06:39,970 --> 01:06:44,720
modern poetry arises in only its case

897
01:06:44,740 --> 01:06:46,440
with the death of god

898
01:06:46,450 --> 01:06:53,070
with the loss of the theological justification for life with a sense of disenchantment

899
01:06:55,030 --> 01:06:56,700
sensor depletion

900
01:06:56,700 --> 01:07:00,240
depletion of meaning and value

901
01:07:00,260 --> 01:07:02,840
the metropolis which brutes people

902
01:07:02,860 --> 01:07:07,340
takes them away it takes them out of traditional cultures

903
01:07:07,360 --> 01:07:12,420
also approves traditional religious beliefs and practices

904
01:07:12,420 --> 01:07:17,760
well it's poetry the poetry created out of this experience is the poetry of spiritual

905
01:07:19,740 --> 01:07:26,800
modernity is in his work a condition of social and psychological

906
01:07:29,180 --> 01:07:35,220
which is both but the private personal dilemma and in public one as he

907
01:07:35,220 --> 01:07:37,360
understands that

908
01:07:37,360 --> 01:07:40,450
compare two elements

909
01:07:41,300 --> 01:07:44,050
well it's sense of the city

910
01:07:44,050 --> 01:07:47,170
this one this is a photograph by alfred stiglitz

911
01:07:47,200 --> 01:07:49,700
city of ambition

912
01:07:49,720 --> 01:07:52,360
this is the modern cities

913
01:07:52,400 --> 01:07:53,740
not as the

914
01:07:53,740 --> 01:07:59,800
cena fragmentation or despair but rather place of ascent

915
01:07:59,880 --> 01:08:02,610
and aspirations

916
01:08:03,070 --> 01:08:06,110
it's also

917
01:08:06,180 --> 01:08:08,070
a scene of

918
01:08:10,990 --> 01:08:17,240
past and future this is a photo by another american photographer walker evans a photo

919
01:08:17,240 --> 01:08:20,720
of the brooklyn bridge you recognise it

920
01:08:20,720 --> 01:08:23,630
and here is

921
01:08:25,840 --> 01:08:26,840
another image

922
01:08:26,840 --> 01:08:31,340
and finally this is the matrix t of decomposition

923
01:08:31,360 --> 01:08:36,730
it is an n by n square matrix where n is the number of documents

924
01:08:37,730 --> 01:08:39,210
for everything

925
01:08:39,210 --> 01:08:43,440
that follows the process six because we don't need it be only one

926
01:08:43,460 --> 01:08:44,270
so what we do

927
01:08:44,280 --> 01:08:45,280
i'm going to

928
01:08:45,280 --> 01:08:50,130
actually what does this mean and LSI dimensions but for completeness i'm putting here

929
01:08:50,670 --> 01:08:53,090
this is again an orthonormal matrix

930
01:08:53,110 --> 01:08:56,820
the column vectors have unit length

931
01:08:56,820 --> 01:08:59,420
each document has unit length

932
01:08:59,480 --> 01:09:03,030
and in two distinct column vectors orthogonal to each other so

933
01:09:03,050 --> 01:09:05,530
the document representations you all

934
01:09:05,530 --> 01:09:07,610
orthogonal to each other so it's

935
01:09:07,610 --> 01:09:10,440
it's the basis and that's

936
01:09:10,440 --> 01:09:15,250
these are again the semantic dimensions for matrices u and sigma that capture distinct topics

937
01:09:15,250 --> 01:09:20,400
like politics sports economics so too is again the borderlands dimension

938
01:09:20,420 --> 01:09:22,500
you can see that we have three

939
01:09:23,460 --> 01:09:25,280
was not important

940
01:09:25,300 --> 01:09:29,770
no it's not so these three are one documents

941
01:09:29,780 --> 01:09:34,070
and these three alan documents because these are positive valence and piece of the negative

942
01:09:38,070 --> 01:09:41,460
and so this to capture

943
01:09:41,590 --> 01:09:46,900
the values of documents on the semantic dimensions

944
01:09:46,940 --> 01:09:50,280
and here's the decomposition in its full glory

945
01:09:50,340 --> 01:09:54,530
this is used sigma dt so if you multiply this matrix with this matrix with

946
01:09:54,530 --> 01:09:58,780
this matrix you get the original matrix

947
01:09:58,820 --> 01:10:00,730
so that size of the composition of

948
01:10:00,770 --> 01:10:02,730
the term document matrix

949
01:10:02,750 --> 01:10:03,550
in two

950
01:10:03,550 --> 01:10:06,150
a representation of the terms

951
01:10:06,210 --> 01:10:08,730
a representation of the documents

952
01:10:08,730 --> 01:10:15,320
and the representation of the importance of the dimension of the semantic dimensions

953
01:10:17,900 --> 01:10:22,030
we've decomposed the term document matrix c into a product of three matrices u sigma

954
01:10:23,480 --> 01:10:28,280
the matrix u consists of one row vector for each card

955
01:10:28,320 --> 01:10:32,590
document activity consists of one column vector for each document

956
01:10:32,610 --> 01:10:34,030
the singular value matrix

957
01:10:34,420 --> 01:10:39,730
it is the diagonal matrix with you about ways reflecting the importance of each dimension

958
01:10:39,750 --> 01:10:42,880
and so when we do this

959
01:10:42,880 --> 01:10:46,300
because i wanted to dimensionality reduction

960
01:10:46,320 --> 01:10:51,170
the key problem is interesting not only tells us how important its dimensions

961
01:10:51,170 --> 01:10:54,980
by setting less important dimensions to zero we keep the important information but get rid

962
01:10:54,980 --> 01:10:57,070
of the details

963
01:10:57,090 --> 01:10:59,110
these details may be noise

964
01:10:59,110 --> 01:11:00,170
in that case

965
01:11:00,190 --> 01:11:03,960
the reduced size of better representation because it is less noise

966
01:11:03,980 --> 01:11:05,590
this size

967
01:11:05,610 --> 01:11:09,630
all these details may make things dissimilar that should be similar

968
01:11:10,400 --> 01:11:12,860
the reduced LSI representation

969
01:11:12,880 --> 01:11:18,650
representations better representation because it represents in low similarity

970
01:11:18,940 --> 01:11:23,130
and his analogy that may help you why fewer details can be better

971
01:11:23,150 --> 01:11:27,170
consider an image of a blue flower image of a yellow flower

972
01:11:27,190 --> 01:11:29,000
if you mean color

973
01:11:29,030 --> 01:11:34,270
then we can see the similarity that's more easily so sometimes it's good to omit

974
01:11:34,280 --> 01:11:39,170
details because then you see similarities between things but

975
01:11:39,320 --> 01:11:46,110
OK and how does it work this limiting of details in the dimensionality reduction is

976
01:11:46,130 --> 01:11:50,300
very simple we simply set all dimensions to zero

977
01:11:50,320 --> 01:11:54,090
up to a given point i mean you want to keep so the dimensions always

978
01:11:54,090 --> 01:11:58,340
ordered according to importance one is always more important then two then three then four

979
01:11:58,340 --> 01:12:01,110
then five and so what we do is we

980
01:12:02,420 --> 01:12:06,340
decide we want to keep two dimensions for example and then he said all other

981
01:12:06,340 --> 01:12:07,730
dimensions to zero

982
01:12:07,770 --> 01:12:10,650
and that's what what i have done here so

983
01:12:10,670 --> 01:12:11,590
three to four

984
01:12:11,610 --> 01:12:13,730
three to five cities every year

985
01:12:13,730 --> 01:12:15,670
three to five cities in europe

986
01:12:15,690 --> 01:12:18,980
three to five percent to zero here

987
01:12:19,000 --> 01:12:25,190
actually really zero singular values sigma this has the effect of setting the corresponding dimensions

988
01:12:25,230 --> 01:12:31,150
in UM mvt two zero when computing the product sequence u sigma v t

989
01:12:31,170 --> 01:12:37,920
and that's just your basic matrix algebra

990
01:12:41,300 --> 01:12:45,300
so these are the original matrices from the decomposition

991
01:12:45,320 --> 01:12:48,500
and the only change i would to make is that i i'm going to set

992
01:12:48,500 --> 01:12:52,480
these three numbers to zero

993
01:12:52,500 --> 01:12:57,290
and then i have multiplied three matrices with each other and this is not reduced

994
01:12:57,290 --> 01:12:59,570
representation of the term

995
01:12:59,590 --> 01:13:01,920
document matrix

996
01:13:01,980 --> 01:13:08,090
and this is this matrix has rank two so it's effective dimensionality is two

997
01:13:08,110 --> 01:13:12,770
so i reduce the space to two-dimensional space

998
01:13:12,820 --> 01:13:14,480
so this is the original

999
01:13:14,500 --> 01:13:16,570
with these three values because of

1000
01:13:16,570 --> 01:13:17,360
and that

1001
01:13:17,380 --> 01:13:19,280
product gives me the original matrix

1002
01:13:19,320 --> 01:13:22,380
and then the first set these three to zero then i get this new and

1003
01:13:22,380 --> 01:13:25,980
this is our reduced representation but i'm going to work with now

1004
01:13:26,110 --> 01:13:29,860
that's that's the way to compare the two matrices on one slide

1005
01:13:30,150 --> 01:13:31,270
this one here

1006
01:13:31,280 --> 01:13:32,880
and this one

1007
01:13:32,900 --> 01:13:36,460
and if you do that then you see some of these have changed much for

1008
01:13:36,460 --> 01:13:39,650
example here we have one of two point five

1009
01:13:39,670 --> 01:13:41,710
but some have changed for the better

1010
01:13:41,730 --> 01:13:43,800
for example from zero to minus two

1011
01:13:43,820 --> 01:13:45,250
o point three nine

1012
01:13:45,250 --> 01:13:47,570
now here's an example why

1013
01:13:47,570 --> 01:13:48,980
perhaps this

1014
01:13:50,110 --> 01:13:52,340
it's better than this representation

1015
01:13:52,340 --> 01:13:55,670
let's look at document d two and d three

1016
01:13:55,690 --> 01:13:56,610
if e

1017
01:13:56,630 --> 01:13:58,500
compute the dot product of the

1018
01:13:58,510 --> 01:14:00,030
the similarity

1019
01:14:00,130 --> 01:14:02,900
it is zero right because

1020
01:14:02,960 --> 01:14:06,110
she competed are likely to be to constantly

1021
01:14:06,130 --> 01:14:07,710
then you get zero

1022
01:14:07,730 --> 01:14:10,860
if you compute the dot product of two and three

1023
01:14:10,880 --> 01:14:14,400
in the new matrix

1024
01:14:14,440 --> 01:14:18,010
then you get zero point five two

1025
01:14:18,050 --> 01:14:19,230
so the two

1026
01:14:19,250 --> 01:14:21,440
documents are called to each other

1027
01:14:21,480 --> 01:14:23,710
completely dissimilar

1028
01:14:23,770 --> 01:14:28,090
in the old matrix and they are quite similar to the new metrics

1029
01:14:28,150 --> 01:14:30,750
and that's a good thing why is a good thing because

1030
01:14:30,770 --> 01:14:34,420
if you look at the terms in the document d two contains both

1031
01:14:34,440 --> 01:14:35,630
and ocean

1032
01:14:35,630 --> 01:14:38,380
then the computational complexity is linear

1033
01:14:38,400 --> 01:14:44,380
there are data structures where you actually get rid of the the

1034
01:14:47,210 --> 01:14:52,320
it turns out that once you have this the representation of the data that the

1035
01:14:52,320 --> 01:14:54,230
way to compute similarity measures

1036
01:14:54,300 --> 01:14:56,650
you can

1037
01:14:57,740 --> 01:14:59,320
the writer of

1038
01:14:59,400 --> 01:15:01,920
anomaly detection algorithms

1039
01:15:03,150 --> 01:15:07,150
that use some geometric notion of an out

1040
01:15:07,260 --> 01:15:09,170
here are three examples

1041
01:15:09,210 --> 01:15:12,070
so one of the most number of centre of mass

1042
01:15:12,630 --> 01:15:14,440
where you started

1043
01:15:14,480 --> 01:15:18,110
think of the data is concentrated in some

1044
01:15:18,130 --> 01:15:21,670
in some compact part of the space

1045
01:15:21,690 --> 01:15:26,840
and you've got an analogy of points as distance from the point

1046
01:15:27,170 --> 01:15:28,530
from the centre of mass

1047
01:15:28,550 --> 01:15:30,030
the whole day

1048
01:15:30,070 --> 01:15:34,510
so it's a very simple measure but it turns out to be quite effective for

1049
01:15:34,590 --> 01:15:36,360
certain kinds of applications

1050
01:15:36,380 --> 01:15:41,090
if you want to do feature space you have to be somewhat tricky there is

1051
01:15:41,130 --> 01:15:42,940
well known for the

1052
01:15:42,940 --> 01:15:50,460
central feature space that you cannot do this online so that's fixed

1053
01:15:51,530 --> 01:15:53,260
speed up the station

1054
01:15:53,280 --> 01:15:57,050
another way to do anomaly detection is to used clustering

1055
01:15:58,130 --> 01:16:03,530
measure and of point but it has as the inverse of the carbon out of

1056
01:16:03,530 --> 01:16:04,670
the cluster

1057
01:16:06,590 --> 01:16:08,190
o the point get fired

1058
01:16:08,260 --> 01:16:15,670
you can also do some local density measures like the one he uses

1059
01:16:15,690 --> 01:16:21,510
notions the cancellations being appointed clique in nature

1060
01:16:22,340 --> 01:16:24,460
point in order to

1061
01:16:25,840 --> 01:16:29,170
and now here for example rationality

1062
01:16:30,070 --> 01:16:31,420
the point is far away

1063
01:16:31,420 --> 01:16:33,070
from the point in clique

1064
01:16:33,130 --> 01:16:35,960
that this counts towards the knowledge

1065
01:16:36,730 --> 01:16:40,730
if all points in the clique a well separated

1066
01:16:40,780 --> 01:16:43,710
well separated and this is the fact that

1067
01:16:43,710 --> 01:16:46,780
sort of indicates and sparsely populated

1068
01:16:46,860 --> 01:16:47,820
the body

1069
01:16:47,820 --> 01:16:48,880
but the balance of the

1070
01:16:48,880 --> 01:16:51,210
in fact we come up with the score

1071
01:16:51,240 --> 01:16:56,650
that is relevant to an analysis point

1072
01:16:56,670 --> 01:16:59,690
so there are some technical details of how to run this algorithm

1073
01:16:59,730 --> 01:17:02,010
incrementally perhaps

1074
01:17:03,260 --> 01:17:07,380
if the slide show some experiment

1075
01:17:08,780 --> 01:17:09,900
as i said

1076
01:17:09,900 --> 01:17:11,170
our goal was

1077
01:17:12,360 --> 01:17:17,360
evaluate the ability of algorithms to detect unknown attacks

1078
01:17:17,380 --> 01:17:19,280
and of course i wanted to to

1079
01:17:19,340 --> 01:17:21,610
run experiments on sunday

1080
01:17:21,650 --> 01:17:24,030
realistic data so

1081
01:17:24,070 --> 01:17:25,710
we have

1082
01:17:25,740 --> 01:17:28,570
creating our own dataset

1083
01:17:28,900 --> 01:17:34,980
our institute where we have invited the penetration testing experts

1084
01:17:35,980 --> 01:17:39,880
it had model system that set up an ambush by

1085
01:17:39,940 --> 01:17:42,260
so penetration testing is

1086
01:17:42,280 --> 01:17:46,920
like white hat white hat hacker in know the companies that

1087
01:17:46,920 --> 01:17:50,880
run this business basically testing

1088
01:17:52,480 --> 01:17:56,230
mechanisms of companies and happen to know one

1089
01:17:56,460 --> 01:18:02,030
which people and we have divided into this work for us to say

1090
01:18:02,090 --> 01:18:03,360
data extraction

1091
01:18:03,380 --> 01:18:06,340
so we have

1092
01:18:06,820 --> 01:18:12,460
tax in this way and also collected data normal environment by

1093
01:18:12,480 --> 01:18:15,460
inviting people to use this

1094
01:18:15,820 --> 01:18:19,780
the second dataset considered the intrusion detection evaluation

1095
01:18:19,800 --> 01:18:22,510
carried out by dublin nineteen ninety nine

1096
01:18:22,510 --> 01:18:26,690
so this is the only publicly available dataset for evaluation

1097
01:18:26,710 --> 01:18:32,320
it has affected much larger scale datasets collect several weeks

1098
01:18:33,440 --> 01:18:38,900
the main from the data set is that it was created in a simulated environment

1099
01:18:40,010 --> 01:18:41,920
they basically models

1100
01:18:43,570 --> 01:18:44,780
large scale

1101
01:18:44,840 --> 01:18:46,440
have a system

1102
01:18:46,460 --> 01:18:51,170
and sense of tax in the system

1103
01:18:52,090 --> 01:18:57,570
the system is similar to the artifacts have been heavily criticized in the security community

1104
01:18:57,590 --> 01:18:59,090
that's the only

1105
01:18:59,110 --> 01:19:02,820
publicly available datasets we have also run hours of their order

1106
01:19:02,980 --> 01:19:03,960
c he

1107
01:19:04,780 --> 01:19:08,530
to make sure the results consistent with the world

1108
01:19:10,920 --> 01:19:12,610
we have

1109
01:19:12,610 --> 01:19:15,630
evaluate the impact of various different algorithms

1110
01:19:16,400 --> 01:19:17,690
this kind of data

1111
01:19:17,710 --> 01:19:21,440
so as i said you know there are many ways one can

1112
01:19:21,460 --> 01:19:27,400
define the notion of analyticity so we basically run algorithms on the set of features

1113
01:19:27,400 --> 01:19:30,030
that characterize

1114
01:19:30,050 --> 01:19:31,440
so the question

1115
01:19:33,540 --> 01:19:37,960
i come back to the estimation error it was later found to be actually what

1116
01:19:37,990 --> 01:19:41,850
the show this model is actually multivariate autoregressive model

1117
01:19:41,900 --> 01:19:48,960
and what we were doing actually is we're we're estimating this multivariate autoregressive parameters from

1118
01:19:48,960 --> 01:19:50,990
real world EEG data

1119
01:19:51,040 --> 01:19:56,850
and then from this estimated delta aggressive parameters we computed all these coupling measures

1120
01:19:56,910 --> 01:20:01,320
so what i'm going to to talk and the next minute

1121
01:20:02,780 --> 01:20:05,620
we can estimate term here

1122
01:20:05,630 --> 01:20:07,340
that's true

1123
01:20:08,230 --> 01:20:12,950
the thing is that in each analysis where we are very used to look in

1124
01:20:13,000 --> 01:20:14,450
specific frequencies

1125
01:20:14,460 --> 01:20:21,070
and so the parameters itself doesn't give a nice representation on that and the parameters

1126
01:20:21,070 --> 01:20:23,660
itself depend also on the sampling rate

1127
01:20:24,690 --> 01:20:26,770
converting this into

1128
01:20:28,230 --> 01:20:34,880
into the frequency domain it's easy to compare the data with different sampling rates because

1129
01:20:34,880 --> 01:20:39,390
we have to spectacle presentation so that's one of the of the advantages just two

1130
01:20:39,660 --> 01:20:41,630
percent data

1131
01:20:41,710 --> 01:20:48,030
i agree that it's not for it's not really

1132
01:20:48,040 --> 01:20:52,810
necessary and so sense so but if you want to look at specific frequencies then

1133
01:20:52,810 --> 01:20:57,860
we need to convert convert this into into the frequency domain

1134
01:20:58,440 --> 01:21:04,010
OK the data will be applied to this method this because the public available the

1135
01:21:04,010 --> 01:21:08,020
data set to be also provided to the BCI competition two thousand three

1136
01:21:08,420 --> 01:21:13,360
and this was basically a motor imagery task with four different gases

1137
01:21:13,400 --> 01:21:18,930
in the following i will present just one class because it's just too much data

1138
01:21:18,930 --> 01:21:24,660
and then just for to demonstrate the purpose it's just percent on classes the left

1139
01:21:24,800 --> 01:21:28,350
movement concerning the

1140
01:21:28,370 --> 01:21:32,410
timing the q was presented that second three

1141
01:21:32,430 --> 01:21:35,230
so the area before second three

1142
01:21:35,250 --> 01:21:40,440
consider this the resting into by the difference in the world and the actual activity

1143
01:21:40,580 --> 01:21:42,170
done after

1144
01:21:43,600 --> 01:21:44,730
after the q

1145
01:21:44,740 --> 01:21:48,640
there was in this case there was no feedback given so this was kind of

1146
01:21:48,640 --> 01:21:54,070
training sample training session which could be used to train i guess if i and

1147
01:21:54,070 --> 01:21:56,880
subsequent BCI experiments

1148
01:21:56,970 --> 01:22:00,530
we recorded sixty channels

1149
01:22:00,550 --> 01:22:09,090
but in order to provide somehow to keep it simple i just selected five channels

1150
01:22:09,130 --> 01:22:16,470
and these are marked by just five starts to keep things simple and

1151
01:22:18,100 --> 01:22:25,680
what we get is from from this is what we do

1152
01:22:26,060 --> 01:22:32,740
time frequency analysis as it has been already proposed it several blocks really so here

1153
01:22:32,740 --> 01:22:39,350
we have the time axis from second o to nine and the frequency from silhouettes

1154
01:22:39,370 --> 01:22:46,120
to sixty minutes unfortunately here it is in the top ten sixty below kilos

1155
01:22:46,160 --> 01:22:51,730
just the other way around and again be that they're going on is the time

1156
01:22:51,730 --> 01:22:58,180
frequency maps of the all back and we did also significance testing whether the coupling

1157
01:22:58,180 --> 01:23:00,510
is significant different than zero

1158
01:23:00,570 --> 01:23:05,830
in this case and on the right-hand side the significance test is whether there is

1159
01:23:05,830 --> 01:23:11,650
a significant change from the thing in devoted to the to the actual actual activation

1160
01:23:12,770 --> 01:23:15,970
that's the kind of event related analysis

1161
01:23:15,970 --> 01:23:22,960
this shows the body here in the all-time cost spectrum in the absolute values

1162
01:23:22,970 --> 01:23:25,190
it's very difficult to see

1163
01:23:25,200 --> 01:23:26,450
anything here

1164
01:23:26,470 --> 01:23:31,060
if we look at the event related analysis then we see that there are significant

1165
01:23:31,060 --> 01:23:37,480
changes in the i find peter in chapter thirty in this specific dataset

1166
01:23:37,490 --> 01:23:40,320
which is clearly task related

1167
01:23:40,340 --> 01:23:43,050
in this case again but lieutenant

1168
01:23:43,740 --> 01:23:45,730
left hand finger movement

1169
01:23:45,750 --> 01:23:48,250
the left hand movement imaginary movements

1170
01:23:48,300 --> 01:23:56,740
if you look at the coherence but we get this coherence is also significant all

1171
01:23:56,770 --> 01:24:03,870
over the place so that's what the and here we see that event related father

1172
01:24:03,870 --> 01:24:05,070
to coherence

1173
01:24:05,080 --> 01:24:12,490
this session is especially prominent in the general received four is included so he before

1174
01:24:12,490 --> 01:24:16,530
you have set c four to c three and so on

1175
01:24:16,550 --> 01:24:22,320
also here is a little bit but c four is the most prominent activity

1176
01:24:22,330 --> 01:24:28,950
you see more please much more prominence than in dbs related to c three so

1177
01:24:28,990 --> 01:24:34,220
it should somehow explain but that that's the subject of on the right and left

1178
01:24:34,220 --> 01:24:40,520
hand movement and so the corresponding area is on the seafront of the electrodes four

1179
01:24:40,530 --> 01:24:43,200
if a computer face

1180
01:24:43,210 --> 01:24:48,730
face is not there were significant only where we have large power

1181
01:24:48,730 --> 01:24:52,500
so that we have a large amplitude

1182
01:24:52,550 --> 01:24:57,660
and the event related but we see here again c four c four is involved

1183
01:25:00,250 --> 01:25:06,140
if you look at the imaginary part of coherency we get some somehow similar picture

1184
01:25:06,280 --> 01:25:10,450
eventrelated part of imaginary coherence here so so

1185
01:25:10,460 --> 01:25:14,660
the c four is is very much involved in that part

1186
01:25:14,730 --> 01:25:19,660
maybe just for curiosity and interesting aspect is that here

1187
01:25:19,690 --> 01:25:22,190
we get

1188
01:25:22,240 --> 01:25:24,230
b here

1189
01:25:24,240 --> 01:25:26,490
we get an increase

1190
01:25:26,490 --> 01:25:30,510
as well as here but if you look at the absolute value of the

1191
01:25:30,560 --> 01:25:35,890
imaginary part of the here then we see here that before the before the q

1192
01:25:35,930 --> 01:25:38,220
the image gradient is zero

1193
01:25:38,240 --> 01:25:40,230
and then becomes positive

1194
01:25:40,230 --> 01:25:45,530
and in this should be payoff channel its first it's first negative and then it

1195
01:25:45,530 --> 01:25:50,370
becomes a hero in those cases this results in the positive in an increase in

1196
01:25:50,370 --> 01:25:53,120
the imaginary part of the

1197
01:25:53,130 --> 01:25:57,320
so let's just somehow interesting which says that

1198
01:25:57,380 --> 01:26:01,770
it's not sufficient to look only at the event related analysis but it's always good

1199
01:26:01,770 --> 01:26:06,910
to look into the at the absolute values

1200
01:26:06,990 --> 01:26:11,200
if we look at the partial coherence

1201
01:26:11,270 --> 01:26:15,990
the picture looks very very different what we see here that the coupling between

1202
01:26:16,010 --> 01:26:17,240
c four

1203
01:26:17,260 --> 01:26:22,140
in this role that it's very much change here in this case have said this

1204
01:26:22,140 --> 01:26:24,440
is very much involved in in

1205
01:26:24,900 --> 01:26:32,230
in partial coherence so that should already thing about that something is not

1206
01:26:32,280 --> 01:26:34,900
as trivial as this one that

1207
01:26:34,930 --> 01:26:40,520
OK let's let's continue with the directed transfer function that potential function

1208
01:26:40,600 --> 01:26:46,620
does not become much significant but only in the best of c four

1209
01:26:46,630 --> 01:26:50,120
then we need to see some significant changes

1210
01:26:50,120 --> 01:26:56,540
so i gave you

1211
01:26:56,550 --> 01:26:58,360
a very broad brush

1212
01:26:58,380 --> 01:27:00,480
big picture of

1213
01:27:01,080 --> 01:27:04,030
it all works in energy and distance

1214
01:27:05,050 --> 01:27:07,590
and i want to look at the particles

1215
01:27:07,590 --> 01:27:08,660
more carefully

1216
01:27:08,680 --> 01:27:12,500
and then in the third lecture the forces more carefully and in the fourth lecture

1217
01:27:12,540 --> 01:27:15,660
bring it all together so this list is going to be about particles

1218
01:27:15,680 --> 01:27:17,790
starting with

1219
01:27:17,800 --> 01:27:19,250
if you like a bit of history

1220
01:27:19,250 --> 01:27:20,640
that's the

1221
01:27:20,660 --> 01:27:23,790
the structure of matter

1222
01:27:23,820 --> 01:27:25,070
there have been two

1223
01:27:25,080 --> 01:27:27,690
independent uncomplimentary ways

1224
01:27:27,710 --> 01:27:32,970
that we've discovered or deduced the nature of the structure of matter long

1225
01:27:32,970 --> 01:27:35,960
is the discovery of spectral

1226
01:27:35,970 --> 01:27:40,250
as in the case of the atoms for example the discovery of atomic spectra in

1227
01:27:40,250 --> 01:27:44,890
the nineteenth century was what gave the clue that something must be going on

1228
01:27:44,900 --> 01:27:46,870
and led to the

1229
01:27:46,890 --> 01:27:48,060
discovery of the

1230
01:27:48,100 --> 01:27:49,420
eventually quantum

1231
01:27:49,460 --> 01:27:53,500
theory is results in the pattern of spectral lines the series of

1232
01:27:53,510 --> 01:27:54,540
energy levels

1233
01:27:54,560 --> 01:27:57,650
and then scattering from the heart center

1234
01:27:57,670 --> 01:28:02,560
for example rather and his team discovered atomic nucleus by scattering so these are all

1235
01:28:02,560 --> 01:28:06,650
very familiar in the world of atomic physics and atoms one makes in high school

1236
01:28:06,670 --> 01:28:09,070
and first university and

1237
01:28:09,090 --> 01:28:14,340
qualitatively the same story has been replicated throughout the as we will see

1238
01:28:14,340 --> 01:28:16,230
quantitatively it is different

1239
01:28:16,250 --> 01:28:18,870
and the reason it's qualitatively different is

1240
01:28:18,890 --> 01:28:23,650
as we saw in the last lecture shorter and shorter distances require higher and higher

1241
01:28:23,650 --> 01:28:29,030
energies resolve them but as we will see surprisingly the pattern of the physics qualitatively

1242
01:28:29,030 --> 01:28:33,170
stays the same so let's just look at this for example if i showed you

1243
01:28:35,400 --> 01:28:39,170
at some stage in history you think you've found elementary objects in which everything is

1244
01:28:39,170 --> 01:28:44,510
made and then later you discover that there actually structured systems made smaller pieces

1245
01:28:44,590 --> 01:28:47,620
the critical thing is that quantum mechanics

1246
01:28:47,620 --> 01:28:51,480
constrains the behaviour of those smaller pieces

1247
01:28:51,480 --> 01:28:55,780
so they can't go every which way they have to go in to particular quantum

1248
01:28:55,780 --> 01:29:00,460
states and is that structured system jumps from one quantum state to another

1249
01:29:00,500 --> 01:29:01,840
the energy

1250
01:29:01,880 --> 01:29:03,530
is omitted

1251
01:29:03,550 --> 01:29:08,060
in the quantized form and it is the detection of the quantized energy b

1252
01:29:08,080 --> 01:29:09,340
photons all

1253
01:29:09,370 --> 01:29:16,150
articles that tells you what the structure of this system that originated it is

1254
01:29:16,150 --> 01:29:20,780
so suppose i showed you see energy levels that you deduced i've listed here

1255
01:29:20,810 --> 01:29:26,500
on the left molecules calcium atoms so the nucleus and even the proton and the

1256
01:29:26,500 --> 01:29:30,710
reason for these energy levels of the molecules are collections of atoms which can vibrate

1257
01:29:30,710 --> 01:29:35,970
and rotate around each other and individual atom as electrons in different levels around the

1258
01:29:35,970 --> 01:29:38,960
nucleus and electrons can jump from one level to another

1259
01:29:39,000 --> 01:29:43,910
the nucleus itself with neutrons and protons they can vibrate and rotate

1260
01:29:43,930 --> 01:29:48,500
giving a nuclear excitation levels and the protons

1261
01:29:48,520 --> 01:29:51,120
themselves we now know are made of quarks

1262
01:29:51,130 --> 01:29:56,350
if the quarks in the lowest energy state the quantum mechanics allows

1263
01:29:56,370 --> 01:30:00,740
the energy adds up to what we call a proton those quarks can be excited

1264
01:30:00,780 --> 01:30:05,240
to higher energy levels and you get what are called resonance excite ations of the

1265
01:30:05,240 --> 01:30:09,850
proton and the neutron so forth if i showed you one of these patterns you

1266
01:30:09,850 --> 01:30:11,210
couldn't immediately

1267
01:30:11,220 --> 01:30:15,720
which he was but the moment i put an energy scale on it then you

1268
01:30:15,720 --> 01:30:17,870
see immediately how different they are

1269
01:30:17,910 --> 01:30:22,850
but the excitation scales of molecules are fractions of an EV

1270
01:30:22,870 --> 01:30:26,600
expectations of electrons in atoms on the EV scale

1271
01:30:26,620 --> 01:30:32,600
the nucleons the generic word for protons and neutrons the nucleons in the nucleus

1272
01:30:32,620 --> 01:30:37,150
are excited on the MTV scale and the quarks inside the proton

1273
01:30:37,160 --> 01:30:40,350
we are excited on the hundreds of any scale

1274
01:30:40,370 --> 01:30:45,940
so clearly to the same story quantitatively smaller smaller distances

1275
01:30:45,960 --> 01:30:51,400
higher and higher energy scales of matches

1276
01:30:51,410 --> 01:30:52,830
so that is the

1277
01:30:52,840 --> 01:30:55,210
route of spectrum

1278
01:30:55,220 --> 01:31:01,600
showing you the structure then you have the direct experimental evidence for it by scattering

1279
01:31:01,620 --> 01:31:04,310
beams of your object

1280
01:31:04,330 --> 01:31:08,830
and discovering that has got constituents inside

1281
01:31:08,840 --> 01:31:13,510
the classical example of that's was back in nineteen seventeen when ready

1282
01:31:13,530 --> 01:31:15,460
originally manchester

1283
01:31:15,470 --> 01:31:17,780
later on the cambridge

1284
01:31:18,080 --> 01:31:20,800
in his original work it was done in manchester

1285
01:31:20,820 --> 01:31:23,790
in fact he didn't do it kind remarks and in rather got the credit that's

1286
01:31:23,790 --> 01:31:28,790
another thing you learn about the priorities in science the your bosses will get credit

1287
01:31:28,790 --> 01:31:29,360
but they

1288
01:31:30,900 --> 01:31:36,710
they scattered alpha particles produced by naturally occurring radioactivity and alpha particles have quite low

1289
01:31:36,710 --> 01:31:41,540
energies enough energy to resolve there is an atomic nucleus in the center of the

1290
01:31:41,540 --> 01:31:44,420
atom but not enough energy to resolve anything

1291
01:31:44,470 --> 01:31:48,840
inside the nucleus so for the for the nucleus was the point charge

1292
01:31:48,840 --> 01:31:55,340
fifty years later when got beams of electrons

1293
01:31:55,460 --> 01:32:00,520
scattered of nuclear electron beam to some hundreds of MTV able to resolve the protons

1294
01:32:00,520 --> 01:32:03,460
and neutrons in nuclei and

1295
01:32:03,470 --> 01:32:05,050
by the nineteen seventies

1296
01:32:05,080 --> 01:32:06,340
electron beams

1297
01:32:06,350 --> 01:32:08,710
of tens of g

1298
01:32:08,720 --> 01:32:12,340
well able to resolve the quarks inside the protons and the experiments were done at

1299
01:32:12,340 --> 01:32:14,280
stanford in california

1300
01:32:14,280 --> 01:32:18,700
by collaboration involving people from stanford and people from MIT

1301
01:32:18,710 --> 01:32:23,340
and those of you who know these guys stakes at MIT in cambridge massachusetts

1302
01:32:23,350 --> 01:32:28,090
it's sort of inversion from manchester and cambridge and the also was invented in nineteen

1303
01:32:28,090 --> 01:32:33,210
seventy one hundred seventeen bit of artistic license OK so

1304
01:32:33,230 --> 01:32:35,650
let me show you some things about relevance

1305
01:32:35,670 --> 01:32:40,460
discovery of the nuclear as this is something you all know but there may be

1306
01:32:40,460 --> 01:32:45,570
some features of this which you might find novel that has been

1307
01:32:45,590 --> 01:32:48,020
remember being targeted detector was the rules

1308
01:32:48,070 --> 01:32:52,640
he's been alpha particles from natural radioactivity the target with a thin piece of gold

1309
01:32:52,640 --> 01:32:56,830
leaf the detector was a small scintillation screen screen

1310
01:32:56,910 --> 01:33:00,450
and this i thought you might find interesting you can't read it here but you'll

1311
01:33:00,450 --> 01:33:04,210
be able to properly look at it on the web and zoom in and see

1312
01:33:04,210 --> 01:33:08,800
that's the simple way of getting in works when all we're dealing with is

1313
01:33:08,860 --> 01:33:13,110
once all you in one solve but to make

1314
01:33:15,040 --> 01:33:17,550
richard so that we can treat

1315
01:33:17,610 --> 01:33:20,350
the conditions where we have more than one solitude

1316
01:33:20,400 --> 01:33:23,210
the other approach is to use a

1317
01:33:23,270 --> 01:33:26,780
turn on as solubility product

1318
01:33:26,800 --> 01:33:28,690
the solubility product

1319
01:33:28,750 --> 01:33:32,230
in the solubility product is best shown by

1320
01:33:32,250 --> 01:33:34,850
example let's look at

1321
01:33:34,920 --> 01:33:39,020
salt dissolved in water but very sparingly silver chloride

1322
01:33:39,140 --> 01:33:41,330
silver chloride

1323
01:33:41,370 --> 01:33:44,600
it has a lattice energy that's all said

1324
01:33:44,640 --> 01:33:48,560
we can get some to dissolve in water but only sparingly so

1325
01:33:48,620 --> 01:33:51,580
so i'm going to put equal sign if we move from

1326
01:33:51,630 --> 01:33:55,130
left to right were dissolving so let's call the reaction

1327
01:33:56,840 --> 01:33:58,950
and we move from

1328
01:33:59,010 --> 01:34:03,980
right to left that's called precipitation

1329
01:34:04,000 --> 01:34:09,990
precipitation and so silver chloride dissociates it dissociates to form

1330
01:34:10,060 --> 01:34:12,010
silver cations

1331
01:34:12,060 --> 01:34:15,960
and to show that there dissolved in water on the right a q

1332
01:34:15,970 --> 01:34:18,100
i mean that's in aqueous solution

1333
01:34:18,140 --> 01:34:20,120
and the chloride

1334
01:34:20,140 --> 01:34:22,630
and lines again a q

1335
01:34:22,680 --> 01:34:26,020
and this notion that when an ionic solid

1336
01:34:26,040 --> 01:34:28,180
it dissolves in water

1337
01:34:28,180 --> 01:34:34,790
it dissolves not as a molecule of silver chloride but rather as cation and anion

1338
01:34:34,790 --> 01:34:40,660
was in fact what one arrhenius his nobel prize he got his nobel prize not

1339
01:34:41,750 --> 01:34:45,710
i think is really good work on activation energies and so on the paved the

1340
01:34:45,710 --> 01:34:49,960
way for our understanding of catalysis and so on he got his nobel prize for

1341
01:34:51,820 --> 01:34:58,250
which was called the theory of electrolytic dissociation meaning that when an ionic solid dissolves

1342
01:34:58,270 --> 01:35:00,960
it forms an electrolyte that is to say

1343
01:35:01,000 --> 01:35:03,680
an ionic conductors so that was his

1344
01:35:03,700 --> 01:35:09,600
nobel prize and what we do is we can express the solubility of silver chloride

1345
01:35:09,600 --> 01:35:14,210
by the solubility product denoted KSP

1346
01:35:14,230 --> 01:35:19,200
as the product of the instant concentration of each of the ions and art first

1347
01:35:19,200 --> 01:35:22,060
blush you're going to say well that's just the same as

1348
01:35:22,100 --> 01:35:27,250
talking about the concentration of the of the salt dissolved and that that is the

1349
01:35:27,250 --> 01:35:29,210
case in this trivial example

1350
01:35:29,270 --> 01:35:31,440
we can move on to something a little more

1351
01:35:31,460 --> 01:35:35,060
sophisticated in a moment so i'm going to take the concentration

1352
01:35:35,080 --> 01:35:39,910
using square brackets to denote concentration so it's the product of the concentration of the

1353
01:35:39,910 --> 01:35:41,850
silver catalan

1354
01:35:41,890 --> 01:35:44,620
and the concentration of chloride and

1355
01:35:45,540 --> 01:35:51,180
concentrations here are in units of models per litre salt plug in the value of

1356
01:35:51,180 --> 01:35:57,160
the silver ion concentration and the chloride ion concentration and that gives me the

1357
01:35:57,180 --> 01:35:59,460
the solubility product

1358
01:35:59,480 --> 01:36:01,230
and then this goes up to some

1359
01:36:01,250 --> 01:36:04,180
master value and that's the

1360
01:36:04,230 --> 01:36:07,790
saturation so now i want to show you that that we have some evidence for

1361
01:36:08,430 --> 01:36:13,290
so here's some evidence what i'm showing is the plot of conductivity the conductivity of

1362
01:36:13,290 --> 01:36:18,680
the solution as a function of the number of moles of silver chloride that has

1363
01:36:18,680 --> 01:36:22,000
been added and what do you see first of all

1364
01:36:22,060 --> 01:36:24,850
here we have the conductivity of pure water

1365
01:36:24,870 --> 01:36:29,100
pure water is a very very poor conduct we should really properly call it an

1366
01:36:29,100 --> 01:36:34,930
insulator and in units of this reciprocal which is the same in units of seaman's

1367
01:36:34,930 --> 01:36:40,560
per centimetre the conductivity of pure water is less than ten to the minus seven

1368
01:36:40,600 --> 01:36:44,870
ten to the minus seven it's some number times ten to the minus eight seems

1369
01:36:44,870 --> 01:36:50,230
per centimetre so what's happening when we dissolve silver chloride we're adding this is the

1370
01:36:50,230 --> 01:36:51,620
same as doping

1371
01:36:51,660 --> 01:36:58,000
we're adding charge carriers only instead of in adding holes to the valence band or

1372
01:36:58,000 --> 01:37:04,350
electrons to the conduction band that's going to help us because water is water is

1373
01:37:04,350 --> 01:37:10,230
capable of dissolving iron so we're going to make an ionic conductor by injecting carriers

1374
01:37:10,310 --> 01:37:14,960
the more silver chloride be dissolved the greater the number of carriers the greater the

1375
01:37:14,960 --> 01:37:19,250
number of carriers the greater the conductivity so by measuring conductivity

1376
01:37:19,290 --> 01:37:24,180
as a function of concentration we can determine if we are

1377
01:37:24,200 --> 01:37:26,020
dissolving the silver

1378
01:37:27,040 --> 01:37:30,620
as opposed to we keep adding it just seems to the bottom so let's look

1379
01:37:31,640 --> 01:37:36,870
what we find is as we add silver chloride the conductivity of the solution rises

1380
01:37:36,870 --> 01:37:42,100
up to a maximum value at around ten to the minus five models of silver

1381
01:37:42,100 --> 01:37:47,020
chloride per litre of solution so that would be a concentration of ten to the

1382
01:37:47,020 --> 01:37:49,250
minus five more

1383
01:37:49,250 --> 01:37:52,850
and then what happens if we keep adding silver chloride but

1384
01:37:52,890 --> 01:37:56,290
the conductivity doesn't change so what do you think is happening if you could stare

1385
01:37:56,290 --> 01:38:00,270
at the beaker is the the silver chloride just fall into the bottom so the

1386
01:38:00,270 --> 01:38:04,750
knee in the curve indicates that we've reached saturation solubility

1387
01:38:04,810 --> 01:38:05,810
that's good

1388
01:38:05,830 --> 01:38:09,210
so we have evidence to to support

1389
01:38:10,620 --> 01:38:11,330
we can do

1390
01:38:11,350 --> 01:38:17,580
then compare the solubility with the solubility product in the following we can say that

1391
01:38:17,600 --> 01:38:19,660
the relationship between

1392
01:38:19,680 --> 01:38:22,230
the saturation solubility

1393
01:38:22,370 --> 01:38:27,730
really equal to the concentration of silver chloride dissolved

1394
01:38:27,750 --> 01:38:33,160
which arrhenius is told this is really the concentration of silver ion which is equal

1395
01:38:33,160 --> 01:38:37,770
to the concentration of chloride ions all of these are the same

1396
01:38:37,830 --> 01:38:41,620
and so a little bit of algebra we can show them that

1397
01:38:42,250 --> 01:38:44,040
relationship between

1398
01:38:44,080 --> 01:38:47,080
this and KSP is

1399
01:38:47,080 --> 01:38:54,140
since the concentration of silver equals the concentration of chloride then we can take sp

1400
01:38:54,160 --> 01:38:59,480
the one half will then pull the sea start saturation solubility

1401
01:38:59,540 --> 01:39:02,600
and for silver chloride it turns out that

1402
01:39:02,680 --> 01:39:05,310
the product the solubility product is

1403
01:39:05,310 --> 01:39:06,710
one point eight

1404
01:39:06,730 --> 01:39:09,410
times ten to the minus ten

1405
01:39:09,410 --> 01:39:16,750
which then means that the saturation solubility for is the square root of

1406
01:39:16,830 --> 01:39:20,370
one point three times ten to the minus five and sure enough that's what you

1407
01:39:20,370 --> 01:39:23,980
see on this plot you see that some just a little bit higher than ten

1408
01:39:24,020 --> 01:39:25,600
minus five more

1409
01:39:25,620 --> 01:39:28,210
the plot has flat now but now

1410
01:39:28,250 --> 01:39:30,850
let's look at the more interesting case

1411
01:39:30,910 --> 01:39:35,930
and put the solubility product to work and ask ourselves what happens if we add

1412
01:39:35,930 --> 01:39:39,100
silver chloride not to pure water

1413
01:39:39,120 --> 01:39:44,040
but to a solution that contains already some other dissolved salt is that going to

1414
01:39:44,040 --> 01:39:48,080
have an impact on the amount of silver chloride we can dissolve

1415
01:39:48,080 --> 01:39:50,230
the null space of A

1416
01:39:50,380 --> 01:39:54,100
has got what it

1417
01:39:54,220 --> 01:39:56,640
only the zero vector

1418
01:39:56,640 --> 01:40:00,880
there are no free variables to give other values

1419
01:40:00,890 --> 01:40:05,730
so that the null space is only the zero vector

1420
01:40:05,750 --> 01:40:07,840
but but but but

1421
01:40:10,470 --> 01:40:16,140
and what about our solution away x equal the solutions

1422
01:40:16,210 --> 01:40:22,740
back to a x equal being what what's the story on that 1 so now

1423
01:40:22,750 --> 01:40:26,670
that's coming from today's lecture

1424
01:40:26,860 --> 01:40:31,780
the solution x it

1425
01:40:31,780 --> 01:40:36,580
what's the complete solution

1426
01:40:36,600 --> 01:40:39,010
it's just x

1427
01:40:39,040 --> 01:40:44,340
particular right if if if there resonance if there is a solution it's actually equal

1428
01:40:44,340 --> 01:40:45,750
x particular

1429
01:40:45,800 --> 01:40:48,840
there's nothing but the other is just 1 solution

1430
01:40:49,410 --> 01:40:51,600
if there is 1 at all

1431
01:40:51,690 --> 01:40:54,080
so it's unique solutions

1432
01:40:54,080 --> 01:40:56,990
unique means only 1

1433
01:40:57,860 --> 01:41:02,970
unique solution if it exists it

1434
01:41:03,060 --> 01:41:05,970
it exists

1435
01:41:06,040 --> 01:41:10,010
so in other words I would say that we put it in a different way

1436
01:41:10,530 --> 01:41:16,730
there either 0 or 1 solution

1437
01:41:19,580 --> 01:41:29,280
this is all in this case are equal and find all the because many many

1438
01:41:29,340 --> 01:41:33,150
applications in reality the columns will be

1439
01:41:33,660 --> 01:41:39,210
well you will be what out later call independence

1440
01:41:39,380 --> 01:41:42,380
and we'll have a

1441
01:41:42,380 --> 01:41:45,150
nothing to look for in the null space

1442
01:41:45,170 --> 01:41:50,580
and will only have particular solution OK

1443
01:41:50,580 --> 01:41:56,450
everybody see that part possibility it but I need an example right

1444
01:41:56,580 --> 01:41:59,210
so let me create an example

1445
01:41:59,230 --> 01:42:01,280
what sort of a matrix

1446
01:42:01,280 --> 01:42:06,010
what's the shape of a matrix that has full column rank

1447
01:42:06,100 --> 01:42:09,430
so can I squeeze in an example here

1448
01:42:13,120 --> 01:42:18,880
it exists let me put an example and that it's just the right place to

1449
01:42:18,880 --> 01:42:19,880
put it

1450
01:42:19,890 --> 01:42:21,800
and example

1451
01:42:21,800 --> 01:42:25,380
because the example will be like tall and thin

1452
01:42:25,700 --> 01:42:27,820
it will have

1453
01:42:29,400 --> 01:42:37,150
I mean here's an example 1 2 6 5 3 1 1 1 brilliant exam

1454
01:42:38,650 --> 01:42:41,860
so there is a matrix A

1455
01:42:41,920 --> 01:42:46,670
and what's its rank

1456
01:42:46,690 --> 01:42:50,640
what's the rank of the matrix

1457
01:42:50,670 --> 01:42:55,430
how many pivots will I find if I do elimination

1458
01:42:55,470 --> 01:42:57,620
to write to

1459
01:42:57,670 --> 01:43:01,190
I see a payment there

1460
01:43:01,220 --> 01:43:07,430
act I've certainly those 2 columns a headed off in different directions

1461
01:43:07,470 --> 01:43:12,640
when I do elimination of certain get another pivot here find and I can use

1462
01:43:12,640 --> 01:43:16,880
those to clean out a below and above so

1463
01:43:17,950 --> 01:43:19,900
actually tell me what it

1464
01:43:19,970 --> 01:43:22,640
role reduced

1465
01:43:22,780 --> 01:43:25,990
reduced row echelon form would be

1466
01:43:26,010 --> 01:43:29,120
can you carry that

1467
01:43:29,120 --> 01:43:33,010
that elimination process to the bitter end

1468
01:43:33,060 --> 01:43:34,340
so what it was

1469
01:43:34,900 --> 01:43:41,540
I subtract a multiple of this role from these rows cleanup all zeros that

1470
01:43:41,740 --> 01:43:46,430
I've got something image here what do I do with that I go subtracted below

1471
01:43:46,430 --> 01:43:47,990
and above

1472
01:43:48,060 --> 01:43:52,910
and then I divide through and what's the what's R 4 that example maybe I

1473
01:43:52,910 --> 01:43:57,190
can you'll allow me to put that just here in the next

1474
01:43:57,190 --> 01:43:59,210
you also have issues

1475
01:43:59,210 --> 01:44:01,210
just by men makes it

1476
01:44:01,280 --> 01:44:04,460
and just identified nine

1477
01:44:04,480 --> 01:44:06,210
issues to consider

1478
01:44:06,230 --> 01:44:07,900
we need to make for study

1479
01:44:08,840 --> 01:44:11,670
first one we talked about sample integration

1480
01:44:11,730 --> 01:44:12,750
that is where

1481
01:44:12,750 --> 01:44:16,230
you might have different sizes between the qualitative and quantitative and you want to be

1482
01:44:16,230 --> 01:44:16,880
able to

1483
01:44:18,000 --> 01:44:19,800
your conclusions

1484
01:44:19,820 --> 01:44:23,110
OK the question is to what extent you justify to do that

1485
01:44:23,130 --> 01:44:25,880
when you have such different sample sizes

1486
01:44:25,920 --> 01:44:28,300
could then you have inside outside

1487
01:44:28,300 --> 01:44:32,380
so you have a situation where in the quite a few more outside looking in

1488
01:44:32,380 --> 01:44:36,670
and coordinates that's you know activities and make sense h one one infinity

1489
01:44:36,690 --> 01:44:40,050
and so when you have any study may be used in both those so what

1490
01:44:40,050 --> 01:44:41,230
extent to those

1491
01:44:41,320 --> 01:44:43,840
work together in a study

1492
01:44:43,860 --> 01:44:47,340
because realizations you might do in makes reference study

1493
01:44:47,360 --> 01:44:50,710
to overcome the which is the one in some weaknesses one side you get my

1494
01:44:50,730 --> 01:44:53,500
generalization you have the quantitative component

1495
01:44:53,610 --> 01:44:55,030
to what extent

1496
01:44:55,050 --> 01:44:57,980
is that we these risation met

1497
01:44:57,980 --> 01:45:00,550
sequential this is a really important one because a lot of

1498
01:45:00,550 --> 01:45:03,800
people to like to the sequential studies

1499
01:45:03,820 --> 01:45:07,820
sometimes people want to combine focus group research into media interviews

1500
01:45:07,880 --> 01:45:11,190
people have found which when you do first can make a difference

1501
01:45:11,210 --> 01:45:15,230
if individually in if we got a couple of presentations here about lee interviews and

1502
01:45:15,230 --> 01:45:17,460
and expert interviewing going on

1503
01:45:17,610 --> 01:45:18,960
you do that first

1504
01:45:18,960 --> 01:45:21,610
then you folk fully outfitted with focus groups

1505
01:45:21,800 --> 01:45:26,090
you might get different information if you focus first

1506
01:45:26,750 --> 01:45:31,000
do you really care about what your goals are an individual data collection that so

1507
01:45:31,020 --> 01:45:35,360
that's within you might yourself that sequential we've been

1508
01:45:35,360 --> 01:45:38,570
we've been paradox so in qualitative you have

1509
01:45:38,610 --> 01:45:41,780
focus group that is in the museum of those observations

1510
01:45:41,800 --> 01:45:44,940
that and makes behave across

1511
01:45:45,050 --> 01:45:46,630
sequential issues

1512
01:45:47,900 --> 01:45:51,710
when you convert quantitative qualitative data and vice versa

1513
01:45:51,730 --> 01:45:53,610
that might be an issue

1514
01:45:53,630 --> 01:45:56,840
there are times when you count things

1515
01:45:56,900 --> 01:45:58,480
is misleading

1516
01:45:58,530 --> 01:46:02,550
what does not always mean important is you know so something may say something

1517
01:46:03,090 --> 01:46:04,530
many times

1518
01:46:04,590 --> 01:46:06,210
say one thing once

1519
01:46:06,230 --> 01:46:10,170
and that one thing may be the most important experience they had she had

1520
01:46:10,230 --> 01:46:13,980
so there are times recounting you have to be careful about you know how to

1521
01:46:14,130 --> 01:46:15,000
not always

1522
01:46:15,050 --> 01:46:16,380
you what you need

1523
01:46:16,420 --> 01:46:19,280
but there are times when it is very powerful

1524
01:46:21,500 --> 01:46:24,340
then you have had the most amazing when you mix

1525
01:46:24,800 --> 01:46:27,340
the paradigm

1526
01:46:27,460 --> 01:46:30,940
this knowledge intelligence so forth within your study

1527
01:46:34,750 --> 01:46:38,650
the idea of making guest starts which is going back and forth

1528
01:46:38,670 --> 01:46:39,940
multiple validities

1529
01:46:39,960 --> 01:46:43,360
you have the ability for the different phases that you have to examine

1530
01:46:43,420 --> 01:46:44,880
and the political

1531
01:46:44,900 --> 01:46:46,780
a lot of political issues going on

1532
01:46:46,800 --> 01:46:48,820
for writing your article

1533
01:46:48,820 --> 01:46:51,420
when you write makes reference state is one of the big challenges because you know

1534
01:46:51,460 --> 01:46:53,570
lot of times people read your work

1535
01:46:53,590 --> 01:46:55,860
by the strong one not on the other so

1536
01:46:55,860 --> 01:46:59,920
what kind of language to use to make sure that they try the whole article

1537
01:46:59,920 --> 01:47:02,590
not to skip the courtside skip the side

1538
01:47:02,710 --> 01:47:04,480
that's always the challenge also

1539
01:47:04,500 --> 01:47:07,300
for those decision makers policymakers

1540
01:47:07,340 --> 01:47:08,570
you know a a lot of time

1541
01:47:08,590 --> 01:47:13,630
for example in the US is a huge bias towards quality about the government level

1542
01:47:13,670 --> 01:47:15,090
and so they are often

1543
01:47:16,110 --> 01:47:17,940
they want take is not seriously

1544
01:47:17,940 --> 01:47:19,550
the qualitative findings

1545
01:47:19,590 --> 01:47:21,900
so that's number child we have to try and make

1546
01:47:21,960 --> 01:47:23,570
make all our findings

1547
01:47:23,710 --> 01:47:25,480
pam porter

1548
01:47:25,500 --> 01:47:26,320
so these

1549
01:47:26,320 --> 01:47:28,190
nine issues

1550
01:47:28,230 --> 01:47:29,670
to consider

1551
01:47:29,710 --> 01:47:32,420
when you do quality we do make efforts study

1552
01:47:32,440 --> 01:47:37,570
sexual when you write that report

1553
01:47:37,590 --> 01:47:40,570
these issues i said you need to consider

1554
01:47:40,960 --> 01:47:42,380
different styles of writing

1555
01:47:42,420 --> 01:47:45,400
i am i most as together additionally

1556
01:47:45,440 --> 01:47:49,900
OK audiences i said you have different audiences different backgrounds

1557
01:47:49,900 --> 01:47:52,690
reports also tend to be longer this is a big issue for us

1558
01:47:52,780 --> 01:47:54,280
so long time we tend to

1559
01:47:54,320 --> 01:47:57,500
be limited to how many journals because mean something to

1560
01:47:57,530 --> 01:48:00,920
because there's some journals have strict page that means you can't call it a mixed

1561
01:48:00,920 --> 01:48:01,800
methods study

1562
01:48:01,840 --> 01:48:05,000
and it my number three quality studies well

1563
01:48:05,050 --> 01:48:09,170
and still for some people so the conversion emerging field

1564
01:48:09,230 --> 01:48:11,860
we have a long long way to go to those it just makes method can

1565
01:48:12,050 --> 01:48:13,400
play really big well

1566
01:48:13,400 --> 01:48:15,110
in this movement

1567
01:48:15,170 --> 01:48:18,170
the newly formed the question so when you have

1568
01:48:18,170 --> 01:48:22,190
that the study should be able to that we we don't believe that one study

1569
01:48:22,190 --> 01:48:25,780
is definitive one study and say OK we have the answer

1570
01:48:25,840 --> 01:48:29,900
we believe the second part of the world you have to have the case study

1571
01:48:29,920 --> 01:48:32,280
because you may be something like sample

1572
01:48:32,380 --> 01:48:34,340
effect the findings

1573
01:48:34,400 --> 01:48:36,710
and after time you may

1574
01:48:36,730 --> 01:48:40,130
if i may need to be formulated the questions you change the question

1575
01:48:40,190 --> 01:48:42,690
study or a lot

1576
01:48:42,750 --> 01:48:46,980
and we do you study all possible people to do it

1577
01:48:47,000 --> 01:48:48,480
so part i was

1578
01:48:49,960 --> 01:48:53,340
the city has been seen somehow that was going on

1579
01:48:53,340 --> 01:48:56,480
but also outside may not be explicit

1580
01:48:56,520 --> 01:48:57,650
but it may be

1581
01:48:57,670 --> 01:49:03,230
a situation where it's just like infected in textbooks and articles or what journals publish

1582
01:49:03,280 --> 01:49:06,920
you might see a bias towards one of the other so these different ways paradigm

1583
01:49:06,920 --> 01:49:08,610
was manifest themselves

1584
01:49:08,710 --> 01:49:14,090
education of united nations but this might feel differently now

1585
01:49:14,110 --> 01:49:16,920
that's a pretty big

1586
01:49:16,940 --> 01:49:20,020
especially literacy research looking at reading

1587
01:49:21,480 --> 01:49:24,860
a lot of great studies in reading

1588
01:49:24,900 --> 01:49:28,940
but a lot of them tend to be simplistic an opinion makes everything because what

1589
01:49:29,170 --> 01:49:30,480
want to do is which

1590
01:49:30,480 --> 01:49:33,530
answer questions like which method is best

1591
01:49:35,610 --> 01:49:37,820
but if you do the other pole here

1592
01:49:37,840 --> 01:49:39,110
and i asked you

1593
01:49:39,110 --> 01:49:40,360
the way you know

1594
01:49:40,360 --> 01:49:42,690
meet when you're in school

1595
01:49:42,750 --> 01:49:45,860
i bet you can we can find an array of different ways that you know

1596
01:49:45,860 --> 01:49:47,980
how to it wasn't all one way

1597
01:49:47,980 --> 01:49:49,330
very very few

1598
01:49:49,340 --> 01:49:50,290
of the

1599
01:49:50,310 --> 01:49:55,400
of these ages active most of them will be zero

1600
01:49:57,830 --> 01:50:02,960
typically the sum of the ages with thinking of them being positive values i mean

1601
01:50:02,960 --> 01:50:05,170
that can be guaranteed if age

1602
01:50:05,190 --> 01:50:07,130
always contains its complement

1603
01:50:07,150 --> 01:50:11,610
so every h and h has a complementary function otherwise you take the absolute value

1604
01:50:11,610 --> 01:50:13,110
here doesn't minimum

1605
01:50:13,150 --> 01:50:18,730
but with thinking of this is sort of measuring a measure of complexity

1606
01:50:18,770 --> 01:50:19,880
of the function

1607
01:50:19,900 --> 01:50:23,040
all that we we create and so

1608
01:50:23,090 --> 01:50:25,000
we're going to bound that by some

1609
01:50:25,020 --> 01:50:26,610
constant b

1610
01:50:26,630 --> 01:50:30,880
and we can think of this as a convex combination convex hull

1611
01:50:30,880 --> 01:50:33,380
convex combination up to scale b

1612
01:50:33,400 --> 01:50:36,210
of the functions in eight

1613
01:50:36,850 --> 01:50:40,960
and what typically you do in boosting

1614
01:50:40,960 --> 01:50:46,170
this is not always completely explicit but you can be seen is you minimize some

1615
01:50:46,170 --> 01:50:48,150
function of the margin distribution

1616
01:50:48,170 --> 01:50:52,190
so adaboost is an exponential function of the margin you optimize

1617
01:50:52,210 --> 01:50:53,250
two your

1618
01:50:53,770 --> 01:50:55,460
you're you're

1619
01:50:57,190 --> 01:51:01,630
and you end up with a reasonably sparse set of these ages

1620
01:51:02,230 --> 01:51:04,810
and so

1621
01:51:05,290 --> 01:51:09,920
we'll see how to include the margin analysis later but what i'm gonna concentrate on

1622
01:51:09,920 --> 01:51:14,500
now is looking at the rademacher complexity of this class of functions

1623
01:51:14,560 --> 01:51:18,170
in terms of the rademacher complexity of this set of weak learners

1624
01:51:18,190 --> 01:51:21,130
so you would expect because you've got you know

1625
01:51:21,130 --> 01:51:23,380
weak learners sound weak right so

1626
01:51:23,400 --> 01:51:28,440
you expect low rademacher complexity these like decision stumps or something at this title

1627
01:51:28,550 --> 01:51:34,060
and you would therefore expect this to be much more complex

1628
01:51:34,920 --> 01:51:38,190
and therefore you pay a price for doing boosting

1629
01:51:38,210 --> 01:51:41,170
however if you do the analysis this is the analysis

1630
01:51:42,590 --> 01:51:47,750
here is the rademacher empirical rademacher complexity of convex hull of h

1631
01:51:47,790 --> 01:51:49,250
at scale b

1632
01:51:49,270 --> 01:51:51,090
so that's the supernova

1633
01:51:51,110 --> 01:51:54,710
choosing an h and h the whole set of ages and

1634
01:51:54,730 --> 01:51:57,610
a set of parameters that are bounded by b

1635
01:51:57,630 --> 01:51:58,750
of this

1636
01:51:58,770 --> 01:52:04,570
quantity the correlation with with this random noise the expected value of that

1637
01:52:04,590 --> 01:52:07,730
OK so what do you do you swap these two sons

1638
01:52:07,750 --> 01:52:10,590
and you're going to some of a bring that out

1639
01:52:10,610 --> 01:52:12,980
slightly increases things potentially

1640
01:52:13,000 --> 01:52:16,610
but now you can just replace that by most b

1641
01:52:16,630 --> 01:52:18,070
take the b out

1642
01:52:18,090 --> 01:52:22,130
and you just end up with this some super over h j

1643
01:52:22,960 --> 01:52:24,460
h of

1644
01:52:24,480 --> 01:52:26,090
this correlation

1645
01:52:26,090 --> 01:52:28,290
which is just the rademacher complexity

1646
01:52:28,290 --> 01:52:31,400
of the the weak learners themselves

1647
01:52:31,480 --> 01:52:33,210
so actually

1648
01:52:33,210 --> 01:52:35,270
you don't add anything

1649
01:52:35,290 --> 01:52:38,480
by taking convex combinations to the complexity

1650
01:52:38,540 --> 01:52:42,330
except in the sense that you scale up

1651
01:52:43,130 --> 01:52:48,340
the multiplying factor of these so if we set b equal to one for instance

1652
01:52:48,340 --> 01:52:51,020
we just take a strictly convex hull

1653
01:52:51,040 --> 01:52:52,570
of the set of functions

1654
01:52:52,590 --> 01:52:56,190
we don't increase the rademacher complexity at all

1655
01:52:56,210 --> 01:52:57,310
so this is

1656
01:52:57,310 --> 01:53:01,540
you know a little bit counterintuitive we've increase the set of functions massively

1657
01:53:01,630 --> 01:53:04,480
and yet we haven't increased its complexity

1658
01:53:04,560 --> 01:53:08,250
the reason is intuitively is that the

1659
01:53:08,830 --> 01:53:11,000
when you take the convex hull

1660
01:53:11,730 --> 01:53:13,980
points of the

1661
01:53:13,980 --> 01:53:18,420
polytope defined that are the functions in the weak learner class

1662
01:53:18,440 --> 01:53:22,040
o or some subset of those functions and we plan to cast

1663
01:53:22,060 --> 01:53:26,340
and the rademacher complexity is always extreme at some

1664
01:53:26,360 --> 01:53:28,020
o point in the polytope

1665
01:53:28,040 --> 01:53:29,400
so you know enough

1666
01:53:29,440 --> 01:53:33,400
at a point that was already in you know when you're looking for that suit

1667
01:53:33,440 --> 01:53:35,190
in the rademacher complexity here

1668
01:53:35,250 --> 01:53:38,340
you're always going to end up with the point was already knew we learn actually

1669
01:53:38,340 --> 01:53:41,170
not going to get anything worse than you had before

1670
01:53:41,630 --> 01:53:43,830
so this is for me

1671
01:53:43,830 --> 01:53:46,900
that's the key is to y boosting works

1672
01:53:46,920 --> 01:53:48,650
in some sense this

1673
01:53:48,690 --> 01:53:52,950
in some sense you're increasing the complexity but you're not paying any price was like

1674
01:53:52,950 --> 01:53:54,790
too good to be true you know

1675
01:53:54,810 --> 01:53:56,880
and you know boosts

1676
01:54:04,330 --> 01:54:12,440
maybe i'm not sure possible using the similar

1677
01:54:12,480 --> 01:54:18,250
strategy of taking com which means convex combinations you take an average over the

1678
01:54:18,270 --> 01:54:20,570
function class

1679
01:54:20,690 --> 01:54:23,900
yeah it's not really a convex combination of with

1680
01:54:23,920 --> 01:54:27,840
but maybe maybe there's something could be done

1681
01:54:31,230 --> 01:54:33,330
so this means that

1682
01:54:33,380 --> 01:54:38,400
essentially moving the convex hull incurs no complexity penalty as far as rhetoric complexity is

1683
01:54:39,880 --> 01:54:41,960
in if we take the course one

1684
01:54:42,000 --> 01:54:44,290
so i'll show you the end

1685
01:54:44,360 --> 01:54:47,290
how this can be factored into bound for

1686
01:54:47,290 --> 01:54:51,380
boosting but before i do that i'll go onto looking at

1687
01:54:51,380 --> 01:54:53,840
the rademacher complexity for

1688
01:54:55,290 --> 01:54:58,790
so how do we we approach that's well

1689
01:54:58,790 --> 01:55:03,060
svm some these you know slightly weird functions that are defined in terms of the

1690
01:55:03,880 --> 01:55:07,420
but of course we really think of them in terms of the

1691
01:55:07,690 --> 01:55:09,250
the linear

1692
01:55:09,270 --> 01:55:13,060
functions in the kernel defined feature space so this is the projection into the kernel

1693
01:55:13,060 --> 01:55:14,980
defined feature space and this is just

1694
01:55:14,980 --> 01:55:18,150
a linear function there with bounded norm

1695
01:55:18,190 --> 01:55:21,630
which i'm going to gain bound by a constant b

1696
01:55:21,650 --> 01:55:26,230
so really were interested in the rademacher complexity of this class of functions

1697
01:55:26,230 --> 01:55:28,820
the two all approaches that

1698
01:55:28,830 --> 01:55:31,850
kind of our based on bayes

1699
01:55:31,860 --> 01:55:36,690
i'm gonna first talk about rule based directed models

1700
01:55:36,690 --> 01:55:42,720
and this is one of the ones that have probably the longest history

1701
01:55:42,760 --> 01:55:47,510
and this is an exhaustive but

1702
01:55:47,530 --> 01:55:51,350
there was

1703
01:55:51,380 --> 01:55:53,470
a lot of the original work

1704
01:55:54,150 --> 01:55:55,850
by the pool

1705
01:55:55,860 --> 01:55:57,730
darwin co

1706
01:55:57,740 --> 01:55:59,280
and then some of the

1707
01:55:59,310 --> 01:56:01,020
more recent work

1708
01:56:01,060 --> 01:56:08,630
is included here but there's lots of others as well and let me kind of

1709
01:56:08,680 --> 01:56:12,270
build up giving caricatures

1710
01:56:12,310 --> 01:56:14,950
each other approaches

1711
01:56:16,530 --> 01:56:18,050
at an intuitive level

1712
01:56:18,060 --> 01:56:21,250
we start off we have logic programs

1713
01:56:21,270 --> 01:56:24,300
and in the logic programme we're going

1714
01:56:24,310 --> 01:56:29,030
can change your example of bit from before i will say OK

1715
01:56:29,700 --> 01:56:35,260
the author of the paper is famous then there will be a

1716
01:56:40,230 --> 01:56:42,120
probably true lilac time

1717
01:56:43,040 --> 01:56:46,810
not all the time so we want to make that a little bit

1718
01:56:46,820 --> 01:56:48,790
less categorical

1719
01:56:51,370 --> 01:56:56,070
first way to do this is with very technical term called the fudge factor so

1720
01:56:56,920 --> 01:56:59,060
added something that says

1721
01:57:00,120 --> 01:57:03,730
now what's the probability of being accepted

1722
01:57:04,950 --> 01:57:06,070
i will say

1723
01:57:06,090 --> 01:57:11,040
that again we have the same rule

1724
01:57:11,120 --> 01:57:15,480
and now has a probability point six

1725
01:57:17,510 --> 01:57:23,680
intuitive meaning here is

1726
01:57:23,730 --> 01:57:27,070
for all p in a if a is the author of p and a is

1727
01:57:27,070 --> 01:57:30,530
famous then p is accepted with probability point six

1728
01:57:30,540 --> 01:57:32,740
so that seems

1729
01:57:33,800 --> 01:57:35,870
kind of the right idea

1730
01:57:35,890 --> 01:57:38,610
but then we have to think about

1731
01:57:41,120 --> 01:57:42,250
how does this

1732
01:57:42,270 --> 01:57:45,340
really work in the case where

1733
01:57:47,770 --> 01:57:48,680
for example

1734
01:57:48,690 --> 01:57:50,180
other possible

1735
01:57:50,210 --> 01:57:53,780
ways in which paper could be accept so

1736
01:57:53,810 --> 01:57:58,470
ideally there is also the case well if high quality paper

1737
01:57:58,550 --> 01:58:02,960
maybe it will be accepted two and maybe it would be nice if that probability

1738
01:58:02,960 --> 01:58:05,530
were little bit higher than the probability that

1739
01:58:05,590 --> 01:58:09,830
accepted if it's by a famous

1740
01:58:10,900 --> 01:58:18,580
one way of dealing with that is we can change maker

1741
01:58:18,620 --> 01:58:26,030
interpretation is probably a little bit more precise and so we can say that

1742
01:58:26,050 --> 01:58:28,270
this point six min

1743
01:58:30,430 --> 01:58:35,090
if there are no other possible causes for the paper being accepted

1744
01:58:35,620 --> 01:58:38,190
this probability is point six

1745
01:58:38,240 --> 01:58:39,090
and then

1746
01:58:39,140 --> 01:58:42,660
if more than one possible cause holds

1747
01:58:42,890 --> 01:58:45,060
then we'll have to introduce

1748
01:58:45,110 --> 01:58:48,410
some sort of probabilistic combining rule

1749
01:58:48,430 --> 01:58:51,900
to take that into account

1750
01:58:53,840 --> 01:58:55,640
we can

1751
01:58:55,690 --> 01:58:57,590
kind of appeal again

1752
01:58:58,770 --> 01:59:00,560
are intuitive notions

1753
01:59:00,580 --> 01:59:04,140
one logic programme what would you do

1754
01:59:04,180 --> 01:59:07,430
while logic programming as the disjunction

1755
01:59:07,490 --> 01:59:09,830
and so what the

1756
01:59:10,990 --> 01:59:15,240
two disjunction the probabilistic setting

1757
01:59:15,340 --> 01:59:22,470
we can do probabilistic disjunction which does

1758
01:59:26,840 --> 01:59:30,430
we are kind of doing the or

1759
01:59:31,340 --> 01:59:32,400
these different

1760
01:59:32,410 --> 01:59:34,960
because the

1761
01:59:35,030 --> 01:59:37,530
and to make this tractable

1762
01:59:41,690 --> 01:59:44,850
kind of causal independence there

1763
01:59:44,860 --> 01:59:47,710
each of the potential causes

1764
01:59:50,410 --> 01:59:54,330
is independent p myself but

1765
01:59:54,380 --> 01:59:57,460
obviously this is

1766
01:59:57,570 --> 02:00:00,540
often times and approximation so

1767
02:00:00,570 --> 02:00:03,510
in this case in particular

1768
02:00:03,520 --> 02:00:08,110
the quality of the paper and the famous author may well not be

1769
02:00:08,160 --> 02:00:11,460
independent but still it's a useful

1770
02:00:11,470 --> 02:00:15,110
kind of approximation that we can make

1771
02:00:23,110 --> 02:00:25,900
this fits into

1772
02:00:25,920 --> 02:00:28,530
computing the probabilities so

1773
02:00:28,580 --> 02:00:33,100
the probability of the paper being accepted

1774
02:00:33,150 --> 02:00:35,400
and let's say

1775
02:00:35,410 --> 02:00:38,820
alice is an author and alice is famous

1776
02:00:40,230 --> 02:00:42,890
the paper

1777
02:00:42,900 --> 02:00:49,020
is high quality

1778
02:00:49,030 --> 02:00:51,110
we can

1779
02:00:51,130 --> 02:00:52,750
compute this

1780
02:00:55,000 --> 02:00:58,530
the probability that at least one of the

1781
02:00:59,400 --> 02:01:01,460
cause it succeeds

1782
02:01:01,460 --> 02:01:02,780
is simply

1783
02:01:02,790 --> 02:01:06,070
one minus the probability that all of the true

1784
02:01:06,080 --> 02:01:08,410
possible causes fail

1785
02:01:09,920 --> 02:01:13,210
it's one minus

1786
02:01:17,210 --> 02:01:21,040
this is first rule

