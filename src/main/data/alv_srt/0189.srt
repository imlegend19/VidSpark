1
00:00:00,000 --> 00:00:01,530
what does gene

2
00:00:01,640 --> 00:00:03,080
look at this

3
00:00:04,780 --> 00:00:06,880
i look i see you're

4
00:00:06,880 --> 00:00:08,980
i don't see you p wave

5
00:00:09,000 --> 00:00:14,830
but you have an amazing t way i know you have an amazing are way

6
00:00:14,850 --> 00:00:17,310
it looks like you are waves in the wrong direction

7
00:00:17,370 --> 00:00:23,030
you feel OK alright

8
00:00:23,060 --> 00:00:28,960
yeah i can see if the have well maybe some people survive without the way

9
00:00:28,960 --> 00:00:30,850
it's certainly an unusual

10
00:00:30,860 --> 00:00:32,470
unusual hard

11
00:00:32,490 --> 00:00:36,600
but if you tell me that you're happy then i'll take your word for that

12
00:00:36,630 --> 00:00:39,670
i think it would be nice if you show class now we just stand up

13
00:00:39,670 --> 00:00:42,220
for a while the two other muscles begin to act

14
00:00:42,270 --> 00:00:44,390
and they will see then just under enough

15
00:00:44,430 --> 00:00:47,920
you will see move little bit your arms

16
00:00:47,930 --> 00:00:51,890
well he arms you see now you get the electric dipole field from the other

17
00:00:51,900 --> 00:00:53,440
muscles in the body

18
00:00:53,490 --> 00:00:55,710
which contract

19
00:00:55,850 --> 00:01:00,960
this is even more interesting to your hartmann

20
00:01:00,970 --> 00:01:04,960
all right down again and that's the kind of

21
00:01:04,970 --> 00:01:07,120
and then you can clean up

22
00:01:07,180 --> 00:01:10,970
looks good

23
00:01:13,100 --> 00:01:16,330
part carranza not so easy to interpret

24
00:01:16,400 --> 00:01:19,730
but i think you're looking fine

25
00:01:19,760 --> 00:01:21,170
you feel like

26
00:01:21,170 --> 00:01:29,780
that's important thank you very much for volunteering very courageous

27
00:01:29,810 --> 00:01:34,470
and it's to clean that stuff of is water soluble so it's not so bad

28
00:01:34,500 --> 00:01:36,170
what's your name by the way

29
00:01:37,270 --> 00:01:41,670
you great

30
00:01:53,540 --> 00:01:54,780
and i want to talk

31
00:01:56,160 --> 00:02:02,350
aurora borealis

32
00:02:02,400 --> 00:02:04,190
if we have a magnetic field

33
00:02:06,720 --> 00:02:09,360
and we have a charged particle let's take

34
00:02:09,400 --> 00:02:11,420
make it plus

35
00:02:11,500 --> 00:02:14,990
and the velocity of the charge particles

36
00:02:15,060 --> 00:02:19,140
in this direction

37
00:02:19,180 --> 00:02:23,580
then the force on that chart particle the foreign force

38
00:02:23,640 --> 00:02:25,580
because q

39
00:02:25,600 --> 00:02:27,490
i'm the crosby

40
00:02:27,620 --> 00:02:32,800
i'm going to decompose this philosophy now

41
00:02:32,810 --> 00:02:35,540
in two one component

42
00:02:35,550 --> 00:02:37,980
parallel to the magnetic fields

43
00:02:37,990 --> 00:02:40,170
and in two components

44
00:02:40,200 --> 00:02:44,620
perpendicular to the magnetic field to the vectorial some of these two is be

45
00:02:44,670 --> 00:02:46,740
so i can rewrite this

46
00:02:46,770 --> 00:02:49,220
as q

47
00:02:50,100 --> 00:02:51,760
the parallel

48
00:02:51,860 --> 00:02:54,380
please be perpendicular

49
00:02:54,380 --> 00:02:55,950
across the b

50
00:02:55,990 --> 00:03:01,150
but the parallel across was zero

51
00:03:01,200 --> 00:03:04,550
because the angle is easier zero agrees on the eighty degrees of the sine of

52
00:03:04,550 --> 00:03:06,580
the angle is zero

53
00:03:06,600 --> 00:03:08,260
so to force

54
00:03:08,280 --> 00:03:11,950
is exclusively determined

55
00:03:11,950 --> 00:03:12,920
by this time

56
00:03:12,930 --> 00:03:17,540
it's about perpendicular component

57
00:03:17,560 --> 00:03:19,180
so i was going to happen this

58
00:03:19,180 --> 00:03:20,440
charged particles

59
00:03:20,490 --> 00:03:23,140
is going to circle around

60
00:03:23,180 --> 00:03:28,330
but then it continues to go in this direction with the velocity v

61
00:03:28,350 --> 00:03:32,000
and so you're going to see a

62
00:03:32,070 --> 00:03:34,970
as like this

63
00:03:34,970 --> 00:03:37,100
by the israelis are

64
00:03:37,100 --> 00:03:40,460
that circle

65
00:03:40,500 --> 00:03:44,380
that radius i still remember it from the lecture when we discuss that that was

66
00:03:44,380 --> 00:03:46,670
envy divided by q

67
00:03:46,680 --> 00:03:50,180
but we now is of course the perpendicular component

68
00:03:50,220 --> 00:03:51,990
divided by b

69
00:03:52,000 --> 00:03:54,430
and then in this direction

70
00:03:54,490 --> 00:03:56,970
it continues unaltered the

71
00:03:58,210 --> 00:04:01,820
which is the parallel component

72
00:04:01,880 --> 00:04:03,520
magnetic field of the earth

73
00:04:03,550 --> 00:04:05,420
not straight line

74
00:04:05,420 --> 00:04:07,100
it is curved

75
00:04:07,150 --> 00:04:09,440
so charged particles

76
00:04:09,450 --> 00:04:12,040
inspire around the magnetic field

77
00:04:12,160 --> 00:04:14,240
for the magnetic field lines

78
00:04:14,300 --> 00:04:16,360
and they come down on earth where

79
00:04:16,400 --> 00:04:18,660
the magnetic field lines enter which is near

80
00:04:19,550 --> 00:04:20,950
magnetic poles

81
00:04:29,060 --> 00:04:30,240
the sun emits

82
00:04:30,250 --> 00:04:31,710
the plasma

83
00:04:33,470 --> 00:04:35,610
highly ionized

84
00:04:35,720 --> 00:04:39,400
electrons and protons

85
00:04:39,520 --> 00:04:41,410
call that the solar wind

86
00:04:41,410 --> 00:04:46,380
two groups so that the circle or ellipse is simply shows that these two clusters

87
00:04:47,110 --> 00:04:49,150
hello there

88
00:04:49,160 --> 00:04:51,880
you know subtrees together

89
00:04:51,900 --> 00:04:54,950
next step would be

90
00:04:56,220 --> 00:05:01,650
split this note further on this knowledge and

91
00:05:01,670 --> 00:05:03,690
and so on so that

92
00:05:03,710 --> 00:05:05,680
comes to the

93
00:05:05,700 --> 00:05:09,120
in this case the sixty groups sixty clusters

94
00:05:09,170 --> 00:05:12,570
so again this somehow show the structure approximately

95
00:05:16,080 --> 00:05:17,550
this picture basically

96
00:05:17,560 --> 00:05:19,680
does of for first

97
00:05:19,700 --> 00:05:20,680
again the

98
00:05:20,730 --> 00:05:24,670
projects are the clusters which are kind of close are on the picture

99
00:05:24,900 --> 00:05:27,810
are also more similar by the content

100
00:05:27,830 --> 00:05:30,210
and for each cluster but basically

101
00:05:30,230 --> 00:05:34,640
corresponds to the size of the size of the clutch cluster response to the number

102
00:05:34,640 --> 00:05:35,880
of documents which are

103
00:05:35,930 --> 00:05:36,640
in the

104
00:05:37,450 --> 00:05:43,380
additionally it has a couple of nice properties so we can zoom in zoom out

105
00:05:43,410 --> 00:05:44,650
so it

106
00:05:44,700 --> 00:05:49,750
it's pretty kind of you to do with this kind of structure

107
00:05:49,760 --> 00:05:53,410
basically this is the picture of the taxonomy of the

108
00:05:53,710 --> 00:05:55,570
of the topics in some way

109
00:05:58,210 --> 00:06:02,750
OK maybe i can show you create them on this

110
00:06:09,340 --> 00:06:20,780
in one

111
00:06:22,110 --> 00:06:26,160
for this graph based visualisation

112
00:06:26,170 --> 00:06:28,930
so we load in this case

113
00:06:28,950 --> 00:06:35,070
a corpus of eleven thousand news articles from ACM

114
00:06:35,090 --> 00:06:41,880
and then we simply

115
00:06:41,900 --> 00:06:43,360
press visualize

116
00:06:43,370 --> 00:06:45,870
in this case we will split them into

117
00:06:45,950 --> 00:06:47,170
then groups

118
00:06:47,190 --> 00:06:49,440
so if you please

119
00:06:49,460 --> 00:06:55,360
so the lower defined the limits

120
00:06:55,710 --> 00:06:59,360
see the structure basically now if you go in

121
00:06:59,390 --> 00:07:01,060
we performed so

122
00:07:01,070 --> 00:07:02,500
the technology news

123
00:07:02,560 --> 00:07:07,600
basically so that's why there's are so many computers of computer related words so this

124
00:07:07,600 --> 00:07:09,630
is obviously looks cluster

125
00:07:09,650 --> 00:07:12,440
so if you click on the cluster we get

126
00:07:12,610 --> 00:07:16,490
the most relevant keywords for this particular cluster

127
00:07:16,530 --> 00:07:18,890
it's open source windows five percent

128
00:07:18,910 --> 00:07:19,850
and so on

129
00:07:24,170 --> 00:07:29,750
most relevant documents are documents which are the closest to the centre of the

130
00:07:29,750 --> 00:07:32,180
clusters of this one particular

131
00:07:32,530 --> 00:07:36,580
so here we have this HTML representation of this

132
00:07:36,630 --> 00:07:38,580
of these texts

133
00:07:38,590 --> 00:07:40,100
the news so

134
00:07:40,140 --> 00:07:44,230
two thousand four the growing fuelled by something

135
00:07:44,250 --> 00:07:45,870
the reason she

136
00:07:45,880 --> 00:07:47,480
a lot of that

137
00:07:50,190 --> 00:07:51,170
so basically a

138
00:07:51,220 --> 00:07:54,500
here we have corpus of eleven thousand news

139
00:07:55,230 --> 00:07:59,640
a couple of seconds we can see that the structure of this

140
00:07:59,890 --> 00:08:02,650
news source in this case

141
00:08:02,660 --> 00:08:07,630
OK i mean will be able to play a little bit more with this but

142
00:08:07,680 --> 00:08:10,060
i don't have much time

143
00:08:11,720 --> 00:08:16,050
another them which is this tiling based again the same collection

144
00:08:16,060 --> 00:08:18,340
of the

145
00:08:18,760 --> 00:08:22,190
one of the new

146
00:08:22,210 --> 00:08:25,370
and so a here

147
00:08:25,370 --> 00:08:26,930
so we visualize

148
00:08:28,420 --> 00:08:32,610
and stopping criteria would be within it

149
00:08:32,630 --> 00:08:36,790
clusters with less than one hundred documents one please

150
00:08:36,790 --> 00:08:38,970
any more so

151
00:08:39,000 --> 00:08:39,890
here we see

152
00:08:39,910 --> 00:08:42,670
the structure of the press

153
00:08:42,710 --> 00:08:50,570
certain notes then we see again keywords documents text and so on we can

154
00:08:50,580 --> 00:08:53,420
then on this

155
00:08:53,440 --> 00:08:55,910
the has a couple of

156
00:09:03,820 --> 00:09:10,710
part of the left left part the needs of all known named entities extracted from

157
00:09:10,710 --> 00:09:12,080
the corpus

158
00:09:12,090 --> 00:09:13,840
the new bar shows

159
00:09:15,130 --> 00:09:17,370
the relationship between selected

160
00:09:17,380 --> 00:09:23,140
one of the selected name entities and other most most related to them

161
00:09:23,300 --> 00:09:28,740
and here need to see the context so we are the what what the typical

162
00:09:28,740 --> 00:09:30,430
conflict between

163
00:09:30,440 --> 00:09:33,830
in this case microsoft research and MIT so the

164
00:09:33,830 --> 00:09:36,280
one highly weighted work

165
00:09:36,340 --> 00:09:41,850
and microsoft research and private agency

166
00:09:42,630 --> 00:09:44,100
not probably

167
00:09:44,100 --> 00:09:49,780
the reasons are many different different ways

168
00:09:49,790 --> 00:09:54,100
this is the way how we can explore and here on the right we see

169
00:09:54,110 --> 00:09:56,250
the whole profile of the

170
00:09:56,270 --> 00:09:58,870
particular name entities so i can show you

171
00:09:59,810 --> 00:10:02,330
short example

172
00:10:10,630 --> 00:10:13,420
OK so here if i

173
00:10:13,430 --> 00:10:14,790
but in very

174
00:10:14,800 --> 00:10:16,820
then this is the most

175
00:10:18,120 --> 00:10:22,500
and to the google news news google com

176
00:10:22,530 --> 00:10:24,330
this was the same as

177
00:10:24,340 --> 00:10:26,800
i been keen

178
00:10:26,820 --> 00:10:31,110
here's the mean you

179
00:10:31,130 --> 00:10:34,340
and then google would turn

180
00:10:34,360 --> 00:10:39,930
set of

181
00:10:40,220 --> 00:10:42,320
OK set of

182
00:10:43,870 --> 00:10:45,460
o from slovenia

183
00:10:45,520 --> 00:10:50,420
and we collect them in this chapter

184
00:10:50,470 --> 00:10:55,680
so i want to repeat again because you take some time

185
00:10:55,680 --> 00:10:57,410
just another it's you have the

186
00:10:57,430 --> 00:10:59,310
news z

187
00:11:04,290 --> 00:11:09,850
you may wonder whether you may have also sorry in this example here you have

188
00:11:09,930 --> 00:11:13,910
this and he has mu z

189
00:11:14,100 --> 00:11:16,040
and here

190
00:11:16,040 --> 00:11:18,490
you had this for operator here

191
00:11:18,500 --> 00:11:20,680
i just tried this tactic tree

192
00:11:20,770 --> 00:11:25,950
of the formula and here's the question is whether we do have an article in

193
00:11:25,950 --> 00:11:27,620
some way or not

194
00:11:27,680 --> 00:11:30,270
if i have an occurrence of y here

195
00:11:30,330 --> 00:11:32,930
the nations that

196
00:11:32,990 --> 00:11:35,830
will be incremented by one

197
00:11:37,910 --> 00:11:40,910
what is also a bit many

198
00:11:40,930 --> 00:11:44,870
if you have not worked on fixed point you you can't really

199
00:11:44,950 --> 00:11:47,410
get it immediately but if you

200
00:11:47,430 --> 00:11:50,160
but if you bet together on the

201
00:11:50,180 --> 00:11:55,600
greatest fixpoint only six point seven things of the same kind you don't

202
00:11:55,620 --> 00:11:57,830
you don't get alternation like in

203
00:11:57,950 --> 00:12:00,350
two men first the logic

204
00:12:00,370 --> 00:12:06,660
several existential quantification to do not codifications they they do not at

205
00:12:06,680 --> 00:12:09,240
complexity in

206
00:12:09,270 --> 00:12:12,490
solving or in interpreting this formula

207
00:12:12,520 --> 00:12:17,660
what's what courses and nation between exist for exist for its for and this is

208
00:12:17,660 --> 00:12:22,620
exactly the same thing here what what is costly the alternation between

209
00:12:22,620 --> 00:12:24,560
least and greatest fixpoints

210
00:12:24,640 --> 00:12:25,930
because you may

211
00:12:26,020 --> 00:12:29,390
compute the least fix points that are

212
00:12:29,430 --> 00:12:30,620
put together

213
00:12:30,640 --> 00:12:31,950
in one shot

214
00:12:31,950 --> 00:12:34,520
using some pictorial

215
00:12:34,520 --> 00:12:37,770
computation like domain something

216
00:12:37,810 --> 00:12:41,410
so alternation just measures

217
00:12:41,430 --> 00:12:44,080
how complex the formula is in terms of

218
00:12:44,120 --> 00:12:46,700
how many iterations you would need

219
00:12:46,720 --> 00:12:48,540
to compute

220
00:12:48,540 --> 00:12:53,290
so this will play a very important role so here you will have an example

221
00:12:53,310 --> 00:12:54,600
here of

222
00:12:54,680 --> 00:12:58,560
formula as i said which is of alternation depth one and here it will be

223
00:12:58,560 --> 00:13:02,580
of alternation depth two

224
00:13:02,680 --> 00:13:04,490
OK so just to make

225
00:13:04,500 --> 00:13:07,220
so what is important is that

226
00:13:07,330 --> 00:13:09,390
things you can express

227
00:13:09,410 --> 00:13:10,180
so the the

228
00:13:10,200 --> 00:13:13,620
thickness of the hierarchy here

229
00:13:13,640 --> 00:13:16,490
is that if you don't know for more than

230
00:13:16,490 --> 00:13:17,760
the nation that

231
00:13:18,990 --> 00:13:21,970
there are things you will not be able to express that you would be able

232
00:13:21,970 --> 00:13:25,120
to express which takes this one nation

233
00:13:25,140 --> 00:13:28,490
so just remember that it's like infinitely often

234
00:13:28,540 --> 00:13:31,700
i cannot express it

235
00:13:31,720 --> 00:13:34,370
without i mean

236
00:13:34,370 --> 00:13:36,060
hello allowing me to

237
00:13:37,490 --> 00:13:39,330
to kind of

238
00:13:39,350 --> 00:13:41,600
but together into some

239
00:13:41,620 --> 00:13:43,450
two viable is one

240
00:13:43,470 --> 00:13:45,310
which which is

241
00:13:45,390 --> 00:13:50,770
greatest fixpoint and one which is least fixpoint

242
00:13:54,060 --> 00:13:58,490
so there's also something interesting about how many bibles

243
00:13:58,540 --> 00:14:02,560
you can use to write formulas but this is another story if you know like

244
00:14:02,560 --> 00:14:04,640
looking for solution

245
00:14:04,660 --> 00:14:07,520
precision logic with two levels of

246
00:14:07,540 --> 00:14:10,600
these lists

247
00:14:10,620 --> 00:14:13,240
express expressiveness than first of the logic

248
00:14:13,260 --> 00:14:15,950
and in in particular becomes disabled

249
00:14:15,970 --> 00:14:19,060
which is not the case of fuzzy logic in general

250
00:14:22,540 --> 00:14:27,410
for the next hour the show we see

251
00:14:27,450 --> 00:14:29,200
we see the following

252
00:14:29,890 --> 00:14:31,240
remember that's OK

253
00:14:31,290 --> 00:14:35,450
writing mu calculus formula super the hectic

254
00:14:35,970 --> 00:14:37,850
it's synthetically particularly

255
00:14:37,890 --> 00:14:39,990
complicated subjects

256
00:14:40,020 --> 00:14:42,310
let's see you can kind of

257
00:14:42,330 --> 00:14:45,930
organize the formulas of the mu calculus

258
00:14:45,950 --> 00:14:48,410
in terms of the complexity of

259
00:14:50,500 --> 00:14:54,270
the reason great greatest fixpoint in your formula

260
00:14:54,410 --> 00:14:56,080
in tracks

261
00:14:56,100 --> 00:14:57,870
in the way

262
00:14:58,040 --> 00:15:01,970
so we're not see complicated formulas anymore that was the

263
00:15:01,990 --> 00:15:06,540
i mean really worst time you would have gotten a calculus i believe so now

264
00:15:06,540 --> 00:15:08,180
will simply

265
00:15:08,200 --> 00:15:10,990
try to understand

266
00:15:11,020 --> 00:15:12,180
another tool

267
00:15:12,180 --> 00:15:15,990
which is connected to it which are tree automata

268
00:15:19,120 --> 00:15:24,890
there are very standard problems that you already know logic one is

269
00:15:24,990 --> 00:15:28,790
we call it was chicken problem was just given the model

270
00:15:28,810 --> 00:15:30,850
and given the formula

271
00:15:30,870 --> 00:15:33,870
is it true that the model satisfies the formula

272
00:15:33,870 --> 00:15:36,580
that is called

273
00:15:36,760 --> 00:15:38,580
more than taking

274
00:15:40,410 --> 00:15:44,470
satisfiability is given the formula can you find the model

275
00:15:48,000 --> 00:15:50,910
also there's something really interesting his when you have

276
00:15:50,950 --> 00:15:52,890
infinite objects

277
00:15:52,910 --> 00:15:56,470
people want to know whether the mother you give me

278
00:15:56,470 --> 00:15:58,500
so to do that you just take half

279
00:15:58,510 --> 00:16:00,320
and you subtract

280
00:16:01,300 --> 00:16:04,370
constant alpha times the gradient

281
00:16:04,390 --> 00:16:08,050
of that functional

282
00:16:08,070 --> 00:16:12,540
actually do this you'll get terrible overfitting so instead what we can do is we

283
00:16:12,540 --> 00:16:14,980
can restrict the form of the update

284
00:16:14,980 --> 00:16:16,680
to belong to

285
00:16:16,690 --> 00:16:22,450
class of functions class of weak classifiers in other words we can restrict the update

286
00:16:22,480 --> 00:16:24,200
to have this form

287
00:16:24,210 --> 00:16:26,720
where ht has to belong to

288
00:16:26,730 --> 00:16:28,870
class of weak classifiers

289
00:16:28,890 --> 00:16:31,710
so we would like to do an update like this

290
00:16:31,710 --> 00:16:35,020
restricting ourselves to do an update like this

291
00:16:35,770 --> 00:16:39,130
so the natural thing to do is to choose ht which is as close as

292
00:16:40,240 --> 00:16:43,290
the gradient the negative gradient

293
00:16:43,350 --> 00:16:45,750
and that also

294
00:16:45,750 --> 00:16:50,290
is equivalent to adaboost so this is another way of viewing adaboost so services is

295
00:16:50,290 --> 00:16:53,520
called functional gradient descent because you've got this functional

296
00:16:53,540 --> 00:16:58,850
and you're doing gradient descent or you're trying to do gradient descent

297
00:16:58,860 --> 00:17:02,640
the advantages of these views of boosting is the

298
00:17:02,660 --> 00:17:07,850
lead to all kinds of generalizations to other loss functions

299
00:17:07,880 --> 00:17:14,460
another advantage is that this provides a principled way of estimating conditional probabilities

300
00:17:15,210 --> 00:17:20,790
until now i've assume that our goal is to just predict whether an example is

301
00:17:20,790 --> 00:17:25,260
positive or negative and to be correct as often as possible

302
00:17:25,330 --> 00:17:31,110
in practice what we often want to do instead is forgiven example predict the probability

303
00:17:31,180 --> 00:17:36,270
that example being positive and negative we want to predict its conditional probability

304
00:17:37,710 --> 00:17:42,750
viewing adaboost as an algorithm for minimizing exponential loss is helpful for coming up with

305
00:17:42,750 --> 00:17:44,600
a principled way of doing this

306
00:17:44,620 --> 00:17:49,830
so again here's our exponential loss in work and thinking about the idea of the

307
00:17:49,830 --> 00:17:51,840
true exponential loss

308
00:17:51,850 --> 00:17:55,030
which is the expected exponential loss one

309
00:17:55,070 --> 00:17:59,410
x and y are chosen from the true distribution that's generating the data

310
00:17:59,410 --> 00:18:03,690
on adaboost is minimizing an empirical version of this

311
00:18:03,700 --> 00:18:05,200
but if we could somehow

312
00:18:05,210 --> 00:18:08,230
octane that true expectation

313
00:18:08,260 --> 00:18:11,210
we will be able to write it in this form we can write it as

314
00:18:11,210 --> 00:18:14,890
the expected value over the axes

315
00:18:14,910 --> 00:18:18,410
and then we can look at separately at the case where y is plus one

316
00:18:18,410 --> 00:18:20,500
to minus one

317
00:18:20,520 --> 00:18:25,850
OK so then we get probability wise positive for x positive times the loss plus

318
00:18:25,850 --> 00:18:30,460
the probability that x is negative times the loss

319
00:18:30,480 --> 00:18:31,950
and then

320
00:18:31,970 --> 00:18:38,110
then we can imagine choosing the f which minimizes this over all possible functions over

321
00:18:38,110 --> 00:18:39,740
all functions f

322
00:18:39,750 --> 00:18:44,190
and you can do that just by differentiating this setting it equal to zero

323
00:18:44,200 --> 00:18:47,830
if you do that they get that f of x is equal to this expression

324
00:18:47,860 --> 00:18:50,200
it's kind of logodds expression

325
00:18:50,210 --> 00:18:52,050
which is equivalent to

326
00:18:52,070 --> 00:18:56,680
the probability of x being positive being equal to this as a function of f

327
00:18:56,680 --> 00:18:57,990
of x

328
00:18:59,110 --> 00:19:02,320
to take the oath which is output by adaboost

329
00:19:02,360 --> 00:19:05,910
and converted into a probability estimate we can just take

330
00:19:05,910 --> 00:19:10,950
and plug it into this expression as an estimate of this conditional probability there are

331
00:19:10,950 --> 00:19:14,560
lot of assumptions that are being made here like that the empirical

332
00:19:14,600 --> 00:19:18,000
exponential loss is close to the true exponential loss

333
00:19:18,010 --> 00:19:21,300
and now we can optimize over all of us

334
00:19:22,390 --> 00:19:24,960
assumptions don't hold in general but

335
00:19:25,000 --> 00:19:29,640
this can work pretty well so like here's a case on an actual data sets

336
00:19:31,890 --> 00:19:33,440
the dots

337
00:19:33,450 --> 00:19:38,590
the next corner of each one of these dots represent the estimated probability

338
00:19:38,640 --> 00:19:42,700
using this method and the y value represents the actual

339
00:19:42,700 --> 00:19:47,190
fraction of positive examples for some set of examples so

340
00:19:47,200 --> 00:19:50,880
if this method really work well then you would get predictions very close to the

341
00:19:50,880 --> 00:19:55,210
line likewise acts as you see in this case but this is what ten thousand

342
00:19:55,210 --> 00:19:56,720
training examples

343
00:19:56,760 --> 00:20:02,030
if user smaller training sets this doesn't work as well

344
00:20:02,080 --> 00:20:04,910
OK so this loss minimisation view

345
00:20:04,910 --> 00:20:13,570
OK so thanks everyone for coming and

346
00:20:13,630 --> 00:20:15,610
staying in this room i

347
00:20:15,620 --> 00:20:19,310
so let's talk about today is about

348
00:20:19,320 --> 00:20:22,310
networks in the diffusion in networks and

349
00:20:22,320 --> 00:20:30,360
we can so basically that there are multiple kinds of

350
00:20:30,580 --> 00:20:31,950
so i should

351
00:20:32,000 --> 00:20:36,270
OK it's OK you must

352
00:20:38,560 --> 00:20:39,270
it is

353
00:20:41,270 --> 00:20:42,310
OK so

354
00:20:42,460 --> 00:20:47,340
i want to first make a short introduction in why why why why do we

355
00:20:47,340 --> 00:20:50,790
care about networks why do we analyse them and so on right so

356
00:20:51,080 --> 00:20:55,150
social network analysis later became like a nice area where there there is a lot

357
00:20:55,150 --> 00:21:01,010
of interaction between sociologists computer scientists physicists and so on and there are many different

358
00:21:01,010 --> 00:21:04,150
domains for social network analysis right now

359
00:21:04,200 --> 00:21:06,940
large scale network data the was

360
00:21:06,990 --> 00:21:12,920
became available in this traditional domains are usually analyzed the social social networks people like

361
00:21:13,180 --> 00:21:14,430
frank friendship and

362
00:21:14,620 --> 00:21:19,750
informal contacts among people collaboration influencing companies organizations

363
00:21:19,800 --> 00:21:26,060
professions professional communities and so on but then we i emergence of women computing applications

364
00:21:26,060 --> 00:21:27,030
that there are a lot of

365
00:21:28,320 --> 00:21:33,740
venues for social network analysis of for example like online communities blogging social social networks

366
00:21:33,740 --> 00:21:37,730
and social media electronic markets and and so on right and also there are a

367
00:21:37,730 --> 00:21:42,210
lot of people seeking information in these in these domains for example

368
00:21:42,230 --> 00:21:47,740
services like myspace delicious flickr linkedin yahoo answers facebook and so on

369
00:21:47,760 --> 00:21:53,150
and just introduce if a few examples of networks or networks can come from various

370
00:21:53,150 --> 00:21:57,190
domains like here i have a few examples so we can study the internet like

371
00:21:57,190 --> 00:22:02,490
the level of autonomous systems we can have like citation networks which would be scientific

372
00:22:02,490 --> 00:22:06,940
papers referring to each other you can have one where where nodes are web pages

373
00:22:06,940 --> 00:22:10,380
and the hyperlinks between them you can have more

374
00:22:10,390 --> 00:22:16,970
exotic type of types of networks like sexual networks of sexual contact contact friendship networks

375
00:22:16,970 --> 00:22:21,630
and so on and there are many more of them and for example this

376
00:22:22,120 --> 00:22:23,600
from there

377
00:22:23,620 --> 00:22:27,550
what i showed what sort of information networks you can also have like social networks

378
00:22:27,550 --> 00:22:33,990
of people and some kind of interactions and some very traditional networks are or well

379
00:22:33,990 --> 00:22:37,860
known networks are of the florence families where there is a link between two families

380
00:22:37,860 --> 00:22:43,870
in florence if a member of one family married the other one does actually karate

381
00:22:43,870 --> 00:22:46,910
club network is also very famous and so on

382
00:22:46,950 --> 00:22:52,480
there are also networks coming from not so social sciences for example we can have

383
00:22:52,480 --> 00:22:58,630
like protein interaction networks can have networks built from text like semantic networks were there

384
00:22:58,630 --> 00:23:05,440
are relationships between concepts or what they call language networks where there are links between

385
00:23:05,440 --> 00:23:07,910
words and also like you can use your

386
00:23:07,930 --> 00:23:09,240
they favourite open source

387
00:23:09,250 --> 00:23:12,310
so after project create a network of

388
00:23:12,390 --> 00:23:18,820
so mining social network data has has a long history and one of the more

389
00:23:18,820 --> 00:23:24,710
well-known examples is this one from seventies by by the way actually where he observed

390
00:23:24,710 --> 00:23:31,280
in network of social ties between in karate club at the university and during his

391
00:23:31,280 --> 00:23:37,220
observation period that that was a few years longer what what what happened is that

392
00:23:37,270 --> 00:23:43,280
due to due to some conflicts the karate club split into the two clubs right

393
00:23:43,280 --> 00:23:48,040
and what what can what you can do is if you find like the minimum

394
00:23:48,050 --> 00:23:52,230
cut these in this so the best way to split the network is to split

395
00:23:52,230 --> 00:23:57,440
it is disputed here and what i also got color-coded here is that all the

396
00:23:57,450 --> 00:23:58,690
people that went

397
00:23:59,190 --> 00:24:04,300
the joint either one of these two kind of this newly newly developed karate clubs

398
00:24:04,340 --> 00:24:08,660
and for example what is interesting is that the minimum cut can sort of explaining

399
00:24:08,660 --> 00:24:13,520
how people went and each of the newly established karate clubs joined

400
00:24:13,960 --> 00:24:16,210
the other

401
00:24:16,230 --> 00:24:21,470
o point that i want to do it the four motivation is that go complex

402
00:24:21,470 --> 00:24:25,610
networks as phenomena and just and not just some kind of design artifacts or design

403
00:24:25,630 --> 00:24:31,370
systems right and what i'm interested in then is what are the common patterns that

404
00:24:31,370 --> 00:24:37,200
emerge through these networks and even more right there is this citation i have here

405
00:24:37,210 --> 00:24:40,170
that we want to keep laws of motion for the web right we want to

406
00:24:40,170 --> 00:24:45,200
find statistical tools and methods to quantify large networks and

407
00:24:45,220 --> 00:24:49,360
why why why do we hope to get from from these there are basically like

408
00:24:49,460 --> 00:24:53,070
three parts to the problem i would like to know what to measure so what

409
00:24:53,110 --> 00:24:57,950
better statistical properties of this network data was me once we know what to measure

410
00:24:57,950 --> 00:25:03,220
the second question like pens what the design principles and models that can describe

411
00:25:03,230 --> 00:25:06,370
this type of data and once we have good models that we want to do

412
00:25:06,370 --> 00:25:10,590
the next step which is like understand understanding why networks are the way they are

413
00:25:10,600 --> 00:25:15,410
we want to do predictions on the of the network evolution and things like that

414
00:25:15,410 --> 00:25:17,710
so there are three parts to the problem

415
00:25:17,730 --> 00:25:21,990
networks are also very rich data right traditionally

416
00:25:22,630 --> 00:25:26,850
or let's put it is that you want to read things from the networks you

417
00:25:26,850 --> 00:25:31,310
want them to be large so you want large-scale really speaking completely right don't want

418
00:25:31,310 --> 00:25:36,370
friendships or links to be missing from the traditional you could sort of get two

419
00:25:36,370 --> 00:25:37,570
out of three

420
00:25:37,580 --> 00:25:39,910
of these things but now

421
00:25:39,930 --> 00:25:44,810
through the online systems where everything is locked because we can get all three of

422
00:25:44,820 --> 00:25:47,880
the the right you can get online communities where there is a complete waste of

423
00:25:47,880 --> 00:25:52,030
user activities when things were created and so on

424
00:25:52,050 --> 00:25:58,400
six same is true for email blogging electronic markets instant messaging and so on

425
00:25:59,830 --> 00:26:03,710
we've seen this slide i think two days ago but still the scale of the

426
00:26:03,710 --> 00:26:07,750
network data has increased tremendously over the last few years so

427
00:26:07,790 --> 00:26:12,890
so you're just have like the the networks trying to sort of chronologically sorted from

428
00:26:12,890 --> 00:26:18,120
like small ones to something decent of like four million to the largest social network

429
00:26:18,120 --> 00:26:20,600
analyzed to date is that like all the people

430
00:26:20,600 --> 00:26:22,350
some historical data

431
00:26:22,360 --> 00:26:26,350
then that same statistic is being used to make a claim about the reliability of

432
00:26:26,350 --> 00:26:28,160
the model which

433
00:26:28,170 --> 00:26:29,590
and that's not

434
00:26:29,610 --> 00:26:31,820
it's not a consistent argument

435
00:26:31,840 --> 00:26:39,240
what's the simplest thing we can do to test so

436
00:26:40,690 --> 00:26:45,100
number one simplest vanilla method of testing something is

437
00:26:46,400 --> 00:26:48,220
training on some training data

438
00:26:48,400 --> 00:26:52,820
have some new data which is used to test the status fresh you've not seen

439
00:26:53,800 --> 00:26:55,850
maybe you had the last three years of financial

440
00:26:55,860 --> 00:26:58,220
some financial index train your

441
00:26:58,230 --> 00:27:00,840
you know whatever it is astrological

442
00:27:00,850 --> 00:27:01,880
neural network

443
00:27:01,920 --> 00:27:04,940
that's really what they call it by the way

444
00:27:05,470 --> 00:27:10,760
and you

445
00:27:10,810 --> 00:27:13,870
then evaluate that on some new data which is not

446
00:27:14,220 --> 00:27:17,630
not seem like the next six months of data if

447
00:27:17,730 --> 00:27:20,600
that's the thing that you test you look at how accurate that was on the

448
00:27:20,600 --> 00:27:21,810
test data

449
00:27:22,060 --> 00:27:24,350
so in the case of classification

450
00:27:24,360 --> 00:27:26,180
we might be interested in things like

451
00:27:26,190 --> 00:27:30,850
true positives true negative false positives and false negatives so

452
00:27:30,860 --> 00:27:34,860
assuming we've got some positive class the negative class you might find it easy to

453
00:27:34,860 --> 00:27:40,980
think about disease diagnosis in this case they were with screening test for cancer perhaps

454
00:27:40,980 --> 00:27:45,060
or malaria which we're looking for from visual data

455
00:27:45,090 --> 00:27:47,860
a true positive is when someone

456
00:27:47,860 --> 00:27:49,730
as malaria and our system

457
00:27:49,740 --> 00:27:53,620
correctly says that you have not a true negative is when you're healthy in the

458
00:27:53,620 --> 00:27:55,110
system says you

459
00:27:55,120 --> 00:27:56,730
correctly says you healthy

460
00:27:56,740 --> 00:27:59,060
has two types of mistakes you can make

461
00:27:59,100 --> 00:28:00,340
one of them is

462
00:28:00,640 --> 00:28:03,140
false positive where you are

463
00:28:03,150 --> 00:28:07,200
actually healthy but the system says sixty take tablets

464
00:28:07,210 --> 00:28:09,480
and most seriously here

465
00:28:09,480 --> 00:28:13,620
a false negative where you are sick and the test comes back and says you're

466
00:28:13,770 --> 00:28:15,030
you're actually healthy

467
00:28:15,040 --> 00:28:20,890
OK so there's various things you might want to look at and you know the

468
00:28:20,890 --> 00:28:23,680
most basic thing is how many times to get it right

469
00:28:23,700 --> 00:28:29,160
divided by the total number of cases that the accuracy narrators one minus that its

470
00:28:30,810 --> 00:28:33,160
this is

471
00:28:37,480 --> 00:28:43,970
right right absolutely we talk at the start about

472
00:28:44,520 --> 00:28:48,610
the decision theoretic approach that's why you might have a loss function

473
00:28:51,560 --> 00:28:55,000
this might describe how how serious is it

474
00:28:55,020 --> 00:28:56,610
if you get it wrong

475
00:28:56,620 --> 00:29:02,450
and you this could be perhaps a matrix of all the possible combinations and you

476
00:29:02,450 --> 00:29:03,180
would say

477
00:29:03,200 --> 00:29:07,940
if i false negative how bad is that the false positive how about islam and

478
00:29:07,940 --> 00:29:10,250
then when you're choosing your class you might

479
00:29:10,260 --> 00:29:13,080
try to minimize this loss

480
00:29:13,090 --> 00:29:16,610
and so obviously in the case of nigeria false negative is a much bigger deal

481
00:29:16,610 --> 00:29:17,860
than false positive

482
00:29:17,880 --> 00:29:22,740
you know false positive means you've got to buy snacks drugs false negative potentially is

483
00:29:22,740 --> 00:29:24,260
because untreated died

484
00:29:25,240 --> 00:29:29,160
the loss on false negative is much higher and if you've weights that with the

485
00:29:29,160 --> 00:29:31,610
probability of the thing being positive and negative

486
00:29:31,610 --> 00:29:33,480
you know if you're not sure

487
00:29:33,500 --> 00:29:37,760
it's the posterior probability from your classifiers says fifty fifty

488
00:29:39,000 --> 00:29:41,190
this loss matrix says

489
00:29:41,210 --> 00:29:43,680
we get a huge loss if we give false

490
00:29:43,860 --> 00:29:47,500
negative and only a little less if we get false positive

491
00:29:47,560 --> 00:29:49,630
and minimizing the loss overall

492
00:29:49,680 --> 00:29:52,190
will tend to say positive

493
00:29:54,860 --> 00:29:56,080
OK so this

494
00:29:56,090 --> 00:29:59,740
the things we can do it like this that kind of decision theoretic approach looking

495
00:29:59,740 --> 00:30:01,780
at the consequences of your

496
00:30:02,690 --> 00:30:06,090
the things you can look at like the

497
00:30:06,110 --> 00:30:08,340
the precision and recall

498
00:30:08,350 --> 00:30:12,080
this very different measures and we don't really have time to go through all of

499
00:30:12,080 --> 00:30:16,430
them in detail but again this depends on what kind of application you're looking at

500
00:30:17,400 --> 00:30:19,590
what you what you're interested in

501
00:30:22,280 --> 00:30:23,360
you know form

502
00:30:23,370 --> 00:30:28,060
from medical data you're quite often interested in sensitivity and specificity

503
00:30:28,820 --> 00:30:32,120
sensitivity is how many of the true cases

504
00:30:32,160 --> 00:30:32,900
did you

505
00:30:32,950 --> 00:30:34,380
did you manage to find

506
00:30:34,440 --> 00:30:37,380
so that's the number of true positives divided by

507
00:30:37,400 --> 00:30:39,680
true positives plus false negatives

508
00:30:39,760 --> 00:30:44,000
OK the number of positives you've got the actual number of malaria cases you got

509
00:30:44,060 --> 00:30:48,720
divided by the total number of people with malaria came into use your system

510
00:30:48,740 --> 00:30:51,190
so that's the proportion of

511
00:30:51,210 --> 00:30:53,650
actually sick people that you've you've

512
00:30:53,660 --> 00:30:59,850
gone with the system so in in these medical systems are also interesting specificity

513
00:30:59,860 --> 00:31:01,250
and that is

514
00:31:01,270 --> 00:31:06,350
basically saying if you say someone's negative if you say the healthy

515
00:31:06,360 --> 00:31:08,520
what proportion of negatives to you

516
00:31:08,530 --> 00:31:09,250
do you get

517
00:31:09,260 --> 00:31:12,560
and if you've got a hundred percent sensitivity and hundred percent specificity

518
00:31:12,610 --> 00:31:13,610
then you're doing

519
00:31:14,980 --> 00:31:17,230
if there are less than a hundred and you can see

520
00:31:17,270 --> 00:31:20,200
how bad is that you can see how meaningful is for the system to say

521
00:31:20,240 --> 00:31:22,110
positive or negative

522
00:31:22,120 --> 00:31:23,860
OK so it could be different

523
00:31:23,870 --> 00:31:30,510
things you might want to do with negative and positive depending on your application

524
00:31:30,520 --> 00:31:32,900
training and testing is

525
00:31:33,030 --> 00:31:34,590
great if you have

526
00:31:34,590 --> 00:31:37,280
welcome to the

527
00:31:37,290 --> 00:31:39,340
the last morning

528
00:31:39,470 --> 00:31:42,790
c so

529
00:31:43,010 --> 00:31:46,510
well i thought i would do is start it kind of give

530
00:31:46,620 --> 00:31:50,070
quick recap we talked about yesterday morning

531
00:31:50,120 --> 00:31:53,570
and so we we spent

532
00:31:53,620 --> 00:31:58,500
half the time talking about an online learning algorithm and half the time talking about

533
00:31:58,550 --> 00:32:02,430
game theory we talked a little bit about some connections for instance we

534
00:32:02,450 --> 00:32:03,400
we saw how

535
00:32:03,420 --> 00:32:04,640
we can use

536
00:32:04,750 --> 00:32:08,160
the analysis of these online learning algorithms to

537
00:32:09,120 --> 00:32:10,560
the kind of nice

538
00:32:10,570 --> 00:32:14,210
effective proof of minimax theorem

539
00:32:14,260 --> 00:32:20,620
and then we we talked a nash equilibrium of things today we going to do

540
00:32:20,620 --> 00:32:24,800
is talk a little bit more about some some more connections between these two areas

541
00:32:24,850 --> 00:32:25,900
and then the

542
00:32:25,960 --> 00:32:30,480
in the second half today i'm going to talk about single the different about

543
00:32:30,530 --> 00:32:32,560
some work we've been doing on

544
00:32:32,580 --> 00:32:38,600
somewhat different way of thinking about kernel functions and more general similarity functions and how

545
00:32:38,600 --> 00:32:40,700
and why use them

546
00:32:40,750 --> 00:32:42,230
OK so

547
00:32:42,290 --> 00:32:44,510
mean this

548
00:32:44,520 --> 00:32:50,430
so just to do a quick recap so kind we're we're motivated by settings like

549
00:32:50,470 --> 00:32:55,880
this one where you reviewed set of decisions so for me time

550
00:32:55,990 --> 00:32:57,930
the suburbs in the

551
00:32:58,020 --> 00:33:01,300
no longer right here in my work i have to drive to work every day

552
00:33:01,640 --> 00:33:05,910
it's repeated decisions every day and make a decision and

553
00:33:05,960 --> 00:33:08,720
and you don't

554
00:33:10,550 --> 00:33:11,250
but c

555
00:33:11,260 --> 00:33:12,520
and we would like to do

556
00:33:12,550 --> 00:33:13,400
it is

557
00:33:13,420 --> 00:33:17,710
i have some algorithm for making decisions even even though you really don't know what's

558
00:33:17,710 --> 00:33:19,320
going to happen in the future

559
00:33:19,370 --> 00:33:24,060
and we talked about algorithms with a nice property where you can every day and

560
00:33:24,060 --> 00:33:27,840
make it tries to get some feedback to make it tries to find out this

561
00:33:27,840 --> 00:33:30,640
information and

562
00:33:30,670 --> 00:33:35,740
well you'd like to do is have some strategy so that in the long run

563
00:33:35,760 --> 00:33:39,870
you're doing nearly as well or better than the best fixed decision you could have

564
00:33:39,870 --> 00:33:41,740
made in hindsight

565
00:33:41,750 --> 00:33:44,210
so i guess i should should make the point that

566
00:33:44,330 --> 00:33:47,790
so most of machine learning we look at the back we have

567
00:33:47,800 --> 00:33:50,130
imagine some big

568
00:33:50,200 --> 00:33:55,510
collection of data probability distribution of data we get a random sample so if if

569
00:33:55,510 --> 00:33:58,920
we were in that kind of world if we assume that every day with IID

570
00:33:58,920 --> 00:34:00,220
random draw

571
00:34:00,230 --> 00:34:01,680
then you could imagine

572
00:34:01,690 --> 00:34:06,350
a simpler algorithm which would be the way it for a while just take howe's

573
00:34:06,390 --> 00:34:10,510
each thing this record on average how well start after some period of time just

574
00:34:10,510 --> 00:34:13,280
pick the thing is done that's in the past and use that

575
00:34:13,290 --> 00:34:16,230
but we'd like to be able to do that kind of thing without having to

576
00:34:16,230 --> 00:34:20,590
make those assumptions and also to be able to kind of immediately be making decisions

577
00:34:20,590 --> 00:34:24,500
as we go without having to wait until we take data

578
00:34:24,520 --> 00:34:26,950
if you get good statistics

579
00:34:26,990 --> 00:34:29,800
so either because you don't want to wait or because you just don't believe the

580
00:34:29,800 --> 00:34:33,370
statistics are there in the world maybe change

581
00:34:33,420 --> 00:34:37,400
OK so we we talked about album for this

582
00:34:39,600 --> 00:34:42,470
because we had this matrix view of the world

583
00:34:42,490 --> 00:34:47,570
see OK so just to remind some notation can be

584
00:34:47,640 --> 00:34:51,110
some some terminology will be using it so we have this this view that every

585
00:34:55,240 --> 00:34:57,620
you go back

586
00:34:57,640 --> 00:34:58,690
here so

587
00:34:58,710 --> 00:35:02,500
we're doing this is the matrix the weights are algorithm against the world every day

588
00:35:02,500 --> 00:35:07,790
we figure out the work which represents states impact my take every day

589
00:35:07,820 --> 00:35:13,960
life picks column which represents some sort of traffic patterns indicates our cost how much

590
00:35:13,970 --> 00:35:18,280
we would have paid had chosen heathrow and then we had a this entry here

591
00:35:18,320 --> 00:35:22,710
our out on that day and maybe we get the feedback the algorithms we talked

592
00:35:22,710 --> 00:35:26,090
about mostly one slide on bandit model but

593
00:35:26,180 --> 00:35:29,840
most the model we get full feedback to find out how well you would have

594
00:35:29,840 --> 00:35:32,670
done had he made your choices

595
00:35:33,700 --> 00:35:38,640
that that's actually natural learning setting where these might correspond to different prediction rules and

596
00:35:38,640 --> 00:35:41,830
then when the world is you is the the right answer then you can score

597
00:35:41,830 --> 00:35:44,510
for yourself how are you would have that

598
00:35:44,560 --> 00:35:49,020
OK so we're not that kind of you and then we

599
00:35:49,130 --> 00:35:52,050
just our probability distribution over

600
00:35:52,080 --> 00:35:54,580
what rose we plan to select next

601
00:35:54,640 --> 00:35:57,230
we we keep like this

602
00:35:57,240 --> 00:36:01,010
and then we define this notion of regret which was we look at how much

603
00:36:01,020 --> 00:36:03,110
we paid on average per day

604
00:36:03,110 --> 00:36:05,910
we have a set of new wave function

605
00:36:05,930 --> 00:36:11,570
and we read that until convergence of convergence of every every everything for example mean

606
00:36:11,570 --> 00:36:15,610
energy are different metrics elements

607
00:36:15,620 --> 00:36:18,670
when you have something which is will converge

608
00:36:18,680 --> 00:36:23,260
you can calculate all the properties of the nucleus in its ground state so you

609
00:36:23,260 --> 00:36:24,400
have the energy

610
00:36:24,420 --> 00:36:29,170
the state of the nuclear which is of minimal energy so you can calculate all

611
00:36:29,170 --> 00:36:34,030
the mean values values of energy mean but also of the formation

612
00:36:34,040 --> 00:36:37,740
for example you have different people operators

613
00:36:37,760 --> 00:36:39,090
this is the

614
00:36:39,260 --> 00:36:42,160
the scalar quadruple there is also a donation

615
00:36:42,180 --> 00:36:45,820
where is this going on means that the proton and neutron we still

616
00:36:45,820 --> 00:36:50,140
this is said before which are interface

617
00:36:50,170 --> 00:36:55,340
so this is the final that and you can calculate the mean

618
00:36:57,130 --> 00:37:01,850
that's the way we define the shape of the nuclei so here

619
00:37:02,250 --> 00:37:09,230
two remarks first we are dealing with quantum objects so why do we consider shape

620
00:37:09,230 --> 00:37:13,840
i want to answer that because of these mean function so we have

621
00:37:14,510 --> 00:37:17,200
given the time we have

622
00:37:18,340 --> 00:37:24,560
of having the article the nucleons in a given orbital all the articles have different

623
00:37:24,620 --> 00:37:28,260
extensions special extensions so you have to

624
00:37:28,340 --> 00:37:32,100
provided if present of different

625
00:37:32,180 --> 00:37:34,630
nucleons and different orbitals

626
00:37:34,650 --> 00:37:38,670
and this is because of these mean value that we we can say that we

627
00:37:39,900 --> 00:37:43,050
we know the shape of the nuclei of the

628
00:37:43,060 --> 00:37:47,010
i wanted to be calculated in the interesting frame

629
00:37:47,030 --> 00:37:50,020
so of course if you this in the laboratory

630
00:37:50,040 --> 00:37:53,340
you will have a mix of all these different then you have something which is

631
00:37:54,410 --> 00:37:58,130
but the intrinsic friends of friends associated with the need to

632
00:37:58,150 --> 00:37:59,100
it has

633
00:37:59,110 --> 00:38:00,580
and information

634
00:38:00,580 --> 00:38:03,390
if we do that for all the nucleons

635
00:38:03,410 --> 00:38:07,510
you'll find that most of the nuclei are deformed in their ground state

636
00:38:07,530 --> 00:38:13,050
only magic nuclei or nuclear which are close to major city are very cool

637
00:38:13,070 --> 00:38:17,470
so this is plotted here so this is from infinite so hartree fock

638
00:38:17,580 --> 00:38:20,410
speculation proton and neutron

639
00:38:20,430 --> 00:38:24,660
so but i was in fact something related to these q two zero

640
00:38:25,850 --> 00:38:27,490
and you see that

641
00:38:27,510 --> 00:38:31,010
sorry there's a great quote

642
00:38:31,030 --> 00:38:33,950
you've spherical euclidean spherical nuclei

643
00:38:33,970 --> 00:38:38,970
for magic nuclei the other ones are well informed

644
00:38:39,070 --> 00:38:45,700
for example here you see that the opinions are also well different

645
00:38:46,390 --> 00:38:48,950
it is when you are interested in the state

646
00:38:48,990 --> 00:38:54,070
but with this minimisation principle you can also impose some constraints so you can put

647
00:38:54,070 --> 00:38:57,850
external field look at the response of the nuclei

648
00:38:57,870 --> 00:39:01,340
so this is done with a well-known mathematical approaches

649
00:39:01,350 --> 00:39:03,390
is the use of lagrange parameters

650
00:39:03,410 --> 00:39:06,260
so then you have minimization principle

651
00:39:06,280 --> 00:39:07,780
so you don't only

652
00:39:07,870 --> 00:39:11,970
h here but again impose the number of neutrons protons and you can impose some

653
00:39:11,970 --> 00:39:13,780
deformation here

654
00:39:13,800 --> 00:39:15,070
so you start

655
00:39:15,470 --> 00:39:20,820
that these conditions that you have the mean value is something that keep the given

656
00:39:20,820 --> 00:39:22,390
value small q

657
00:39:24,640 --> 00:39:25,990
when you do that

658
00:39:26,010 --> 00:39:33,240
you can obtain what we we call potential care so very famous example insufficient value

659
00:39:33,240 --> 00:39:38,340
so you have here the energy in any loss function of the political information

660
00:39:38,350 --> 00:39:43,760
so you can impose here city and then increase the information here

661
00:39:43,870 --> 00:39:47,200
two here you can even break into two fragments

662
00:39:47,220 --> 00:39:48,720
so you here

663
00:39:48,720 --> 00:39:52,030
the lowest energy state so this is the ground state

664
00:39:52,050 --> 00:39:57,140
so if you do not impose any constraint you we only obtain this

665
00:39:57,160 --> 00:40:00,600
if the constraint you can be from different nations

666
00:40:00,620 --> 00:40:05,680
you see here that you have different values first and second barriers and here

667
00:40:05,700 --> 00:40:08,580
what we call is very well

668
00:40:08,620 --> 00:40:13,510
so it so well where you can have some state inside so it means that

669
00:40:13,510 --> 00:40:17,100
you can have metastable states

670
00:40:17,340 --> 00:40:21,240
in a different way that stable states here

671
00:40:21,260 --> 00:40:25,760
and you will see that they are important for the fission process

672
00:40:28,780 --> 00:40:33,050
now do you know what are the most commonly used constraints because with this approach

673
00:40:33,070 --> 00:40:34,850
is constrained whatever you want

674
00:40:34,870 --> 00:40:37,010
so these are some shapes

675
00:40:37,400 --> 00:40:40,580
this is for example symmetry so this is

676
00:40:40,600 --> 00:40:44,240
not very visible but this is a small fragmented make one

677
00:40:44,260 --> 00:40:48,850
so constrained which is used for example to describe fish as i told you you

678
00:40:48,850 --> 00:40:51,030
have a small fragment the big one

679
00:40:51,030 --> 00:40:53,100
so you need repair ship

680
00:40:53,120 --> 00:40:57,760
in order to to have access to cease shape to these fragmentation was a small

681
00:40:57,760 --> 00:41:00,320
and weak one

682
00:41:00,340 --> 00:41:04,760
so what is for example now the problems with this information

683
00:41:05,820 --> 00:41:09,720
to my mind there are two problems the first one is that it's complicated and

684
00:41:09,720 --> 00:41:10,870
in nature

685
00:41:10,890 --> 00:41:15,950
it's not so complicated so it means that it should be principle associated two

686
00:41:15,950 --> 00:41:17,160
hi interaction

687
00:41:17,160 --> 00:41:19,450
so it will not be so pertinent

688
00:41:19,470 --> 00:41:21,490
so the constraints the

689
00:41:21,490 --> 00:41:27,950
and the deformations which are pertinent to the lowest PCD so the most simplest ones

690
00:41:27,950 --> 00:41:32,430
the other problem with this one that integrates a lot of symmetries

691
00:41:32,430 --> 00:41:33,490
and with this

692
00:41:33,510 --> 00:41:34,520
o point you'll here

693
00:41:34,540 --> 00:41:36,260
setting up UV

694
00:41:36,280 --> 00:41:38,070
it's actually use it

695
00:41:38,090 --> 00:41:42,630
engineer will come in with all the goggles and everything else but once it's actually

696
00:41:42,630 --> 00:41:45,680
an operated you can change from the UV

697
00:41:45,700 --> 00:41:47,450
two near infrared

698
00:41:47,460 --> 00:41:51,390
but really because about

699
00:41:51,410 --> 00:41:54,790
change to neighbours so just go through

700
00:41:54,830 --> 00:41:58,680
this is a class one legion closure but it can be doesn't have to be

701
00:41:58,680 --> 00:42:05,720
a safety interlock automatic alignment with optimisation and to get this wavelength optimisation what we've

702
00:42:06,460 --> 00:42:08,580
in the spectrometer box

703
00:42:08,590 --> 00:42:11,000
his remains the pass on

704
00:42:11,020 --> 00:42:17,200
the original roman institute a single delivery path and that was always compromising lasers because

705
00:42:17,200 --> 00:42:18,970
if you want to work in the u v

706
00:42:18,980 --> 00:42:20,750
you would need one man said

707
00:42:20,760 --> 00:42:24,850
and if you wanted to work in the infrared union other

708
00:42:24,860 --> 00:42:29,260
so what we've done here is we optimized parkland and one for u v

709
00:42:29,270 --> 00:42:34,390
one for visible and one for the infrared that is in all the instruments standard

710
00:42:34,400 --> 00:42:38,880
now that means that your optimizing the delivery path

711
00:42:39,670 --> 00:42:45,360
with this option you then optimize the roman going back through is always the same

712
00:42:45,360 --> 00:42:49,030
so you only a single path going back to this

713
00:42:49,040 --> 00:42:54,790
we have an option here to put out the fault with a fibre-optic probe just

714
00:42:54,790 --> 00:42:57,740
plug in the india and you can then other remote sensor

715
00:42:58,130 --> 00:43:01,750
that you see has advantages when it comes to attaching

716
00:43:01,770 --> 00:43:03,890
to other pieces of equipment

717
00:43:03,900 --> 00:43:09,910
high precision creating stage or to focus or to convert galaxy or to reveal insights

718
00:43:10,160 --> 00:43:11,880
a selection

719
00:43:12,030 --> 00:43:17,410
and basically everything in here is also validated and you can actually just check it

720
00:43:17,420 --> 00:43:19,800
out so if someone misaligned

721
00:43:19,820 --> 00:43:23,030
you just go back into the software back to default position

722
00:43:23,050 --> 00:43:26,780
you buy it was running it may not be running perfectly but at least you

723
00:43:26,780 --> 00:43:29,890
are run

724
00:43:29,930 --> 00:43:37,580
to enable will go here o back

725
00:43:37,600 --> 00:43:39,420
and technology

726
00:43:39,720 --> 00:43:42,920
so this is the five problem

727
00:43:43,190 --> 00:43:50,400
you put objective and here the set fire and optical fibre here the reason accessories

728
00:43:50,420 --> 00:43:51,460
since sky

729
00:43:51,480 --> 00:43:54,400
scanning electron microscope combined

730
00:43:54,410 --> 00:43:55,770
for FTIR

731
00:43:55,790 --> 00:44:01,290
combined effect and some of these are standard probes which you can use remote sensing

732
00:44:01,370 --> 00:44:07,410
you put these into beamlines all sorts of things about high-pressure reactor vessel plasmas plasma

733
00:44:07,410 --> 00:44:08,900
jewish you can take

734
00:44:08,920 --> 00:44:11,730
the fiber to that under the what the

735
00:44:11,750 --> 00:44:18,560
gives huge flexibility mika sampling again you put a an achromatic lens on the is

736
00:44:18,630 --> 00:44:24,090
big bold if you wish to temperature control sales of coal sales because peacock also

737
00:44:24,130 --> 00:44:28,560
liquid nitrogen and liquid helium temperature experiments you can do

738
00:44:28,600 --> 00:44:30,290
using this material

739
00:44:30,310 --> 00:44:36,950
we have some databases polarization you can actually set up before i change the polarization

740
00:44:37,030 --> 00:44:43,370
laser depolarized always polarized raman depolarized around so if you look in crystallography

741
00:44:43,390 --> 00:44:47,450
you can do all of these things and study crystal get what some information

742
00:44:47,460 --> 00:44:51,600
and chemistry in high-pressure sales again

743
00:44:51,620 --> 00:44:52,840
all available

744
00:44:52,980 --> 00:44:57,330
and i think the important thing here is a flat flexible object path

745
00:44:57,400 --> 00:45:00,450
the whole system is modular

746
00:45:00,460 --> 00:45:02,090
you can buy the original

747
00:45:02,100 --> 00:45:03,350
basic instrument

748
00:45:03,370 --> 00:45:04,800
and all the time

749
00:45:04,820 --> 00:45:05,920
i build the tower

750
00:45:05,930 --> 00:45:10,500
to do all the other application and this is the same with the

751
00:45:10,550 --> 00:45:14,540
if the same sky the family and some by the

752
00:45:15,170 --> 00:45:15,980
and then

753
00:45:16,000 --> 00:45:19,680
by urine samples or if you've got lots of money can buy it all together

754
00:45:19,690 --> 00:45:21,300
it's target

755
00:45:21,490 --> 00:45:23,240
this is done

756
00:45:23,260 --> 00:45:24,960
because we know

757
00:45:24,970 --> 00:45:26,700
using our own software

758
00:45:26,710 --> 00:45:31,060
the software runs the control runs all electronics

759
00:45:31,090 --> 00:45:37,870
does all the mapping has all the standard programming algorithms that the processing algorithms if

760
00:45:37,870 --> 00:45:40,620
you want to do something special with your data

761
00:45:41,150 --> 00:45:44,950
you can export it out and run in the program so that is all built

762
00:45:44,950 --> 00:45:47,210
in an always part of the

763
00:45:48,730 --> 00:45:54,760
this is the this twenty one CFR part eleven is purely for the pharmaceutical people

764
00:45:55,920 --> 00:46:03,110
i'm going to talk about the nanoscale raman developments and i thank my colleague matthew

765
00:46:04,740 --> 00:46:07,470
all this what i am very fast i don't need to do the work i

766
00:46:07,470 --> 00:46:12,420
just presented which means i don't always understand everything that's going on but i freely

767
00:46:12,420 --> 00:46:13,800
admit that but i'm

768
00:46:16,590 --> 00:46:22,050
so that that this what what is a highly specialized why would you use EFM

769
00:46:23,790 --> 00:46:26,870
well the main reason is if you're looking for things like

770
00:46:27,030 --> 00:46:29,860
quantum dots

771
00:46:29,880 --> 00:46:31,320
trying to find the

772
00:46:31,330 --> 00:46:32,900
is the most difficult part

773
00:46:32,930 --> 00:46:38,190
if you look in nanomaterials finding the material you want analyse is the hard part

774
00:46:38,230 --> 00:46:43,100
so if you do this you have to use a another technique for you just

775
00:46:43,100 --> 00:46:47,570
take a sample scan over until you get around p i hope like hell you've

776
00:46:47,570 --> 00:46:50,990
the response to have to be

777
00:46:51,010 --> 00:46:56,000
normal service so we had we had that been

778
00:46:56,020 --> 00:46:58,450
plus an excellent

779
00:46:58,460 --> 00:47:00,720
so the here

780
00:47:00,740 --> 00:47:02,330
we had

781
00:47:03,580 --> 00:47:05,780
which is well

782
00:47:05,780 --> 00:47:07,680
forces the

783
00:47:07,710 --> 00:47:12,010
well and generalized linear models we don't have to have an epsilon is normally distributed

784
00:47:12,030 --> 00:47:16,250
we can have all the distributions not all things like the binomial in the past

785
00:47:16,250 --> 00:47:17,480
on that were in

786
00:47:22,760 --> 00:47:25,220
the nice thing about this is that

787
00:47:25,290 --> 00:47:29,970
though is interesting solve you do you if i do this with a piece of

788
00:47:29,970 --> 00:47:32,380
paper you don't need a computer this

789
00:47:32,400 --> 00:47:37,530
or held for patients

790
00:47:37,550 --> 00:47:43,890
in general nice reliable algorithm for fitting this model for estimating the basis so that's

791
00:47:43,890 --> 00:47:45,370
what we like

792
00:47:45,570 --> 00:47:50,810
and it's also expands the range of what we can do quite a long way

793
00:47:50,810 --> 00:47:52,640
so small

794
00:47:52,660 --> 00:47:59,570
and the most recent sort of expansion through this idea is was called generalized additive

795
00:48:00,640 --> 00:48:04,230
which is exactly the same thing as before except

796
00:48:04,230 --> 00:48:06,170
this is having

797
00:48:06,200 --> 00:48:09,100
despite his being beaten times

798
00:48:09,120 --> 00:48:12,650
this is a function of x

799
00:48:12,670 --> 00:48:14,320
and the function of x

800
00:48:14,340 --> 00:48:18,320
usually the what are called lines

801
00:48:18,740 --> 00:48:21,100
and that allows to get

802
00:48:21,120 --> 00:48:25,200
things that have local properties to model so here's an example of one of them

803
00:48:25,240 --> 00:48:30,780
is the trend in this is affected by having

804
00:48:32,230 --> 00:48:37,490
continuous so looks used what in fact it's only smooth

805
00:48:37,510 --> 00:48:38,410
so it's smooth

806
00:48:38,430 --> 00:48:40,350
the second derivative

807
00:48:40,370 --> 00:48:43,960
continuous points the derivatives is a set of

808
00:48:43,970 --> 00:48:47,300
piecewise cubic foot

809
00:48:47,310 --> 00:48:50,620
so that's

810
00:48:50,640 --> 00:48:55,540
allows it things like follow the the result plot here

811
00:48:55,540 --> 00:48:56,570
rather than

812
00:48:56,590 --> 00:48:59,040
have to keep going because it has a particular

813
00:49:02,130 --> 00:49:03,530
so those

814
00:49:03,530 --> 00:49:06,490
the main areas in ireland

815
00:49:06,510 --> 00:49:09,500
this part of the world i suppose statistics

816
00:49:09,550 --> 00:49:10,950
we can say

817
00:49:10,970 --> 00:49:14,090
o model is generalized additive model fairly easily if future

818
00:49:14,150 --> 00:49:19,410
use of the car do it trivially for your problem at the moment is we

819
00:49:19,410 --> 00:49:21,060
don't really know

820
00:49:21,070 --> 00:49:22,600
again what the statistical

821
00:49:24,150 --> 00:49:30,140
all of these models so that's the current research area statistics

822
00:49:31,530 --> 00:49:35,820
so we've got all those things i will come back then too

823
00:49:35,840 --> 00:49:40,300
the linear model and how do we choose which pages are there and how to

824
00:49:40,300 --> 00:49:42,080
estimate which beta

825
00:49:43,310 --> 00:49:49,340
so what is it well that's about do estimation is we have a method to

826
00:49:49,660 --> 00:49:52,660
much like

827
00:49:53,900 --> 00:49:57,530
fifty years it's until fifty years ago fifty years ago

828
00:49:58,070 --> 00:50:01,410
i went out into the field

829
00:50:01,430 --> 00:50:04,790
other terms with lots set of

830
00:50:08,060 --> 00:50:12,720
from agriculture applied statistics and we harvest it's

831
00:50:12,740 --> 00:50:17,530
those was the statistics class are harvested

832
00:50:17,650 --> 00:50:18,570
what the

833
00:50:18,590 --> 00:50:19,710
the idea is

834
00:50:19,720 --> 00:50:26,470
but when i got short story obviously twenty four kilogrammes it and it's not the

835
00:50:26,640 --> 00:50:29,340
sole here is an experiment

836
00:50:29,370 --> 00:50:32,490
and what we try to find out more

837
00:50:32,500 --> 00:50:34,950
we these are output

838
00:50:35,000 --> 00:50:36,500
so so i

839
00:50:36,520 --> 00:50:41,490
this is the output we have sixty four

840
00:50:41,500 --> 00:50:45,200
observations of the output gap is the beyond that of

841
00:50:45,210 --> 00:50:48,820
a plot of land three meters by two meters

842
00:50:48,840 --> 00:50:54,130
so the experiment the design fairly carefully

843
00:50:55,830 --> 00:50:57,060
what we have here

844
00:50:57,080 --> 00:50:59,570
it is

845
00:51:01,130 --> 00:51:02,890
observations that

846
00:51:02,900 --> 00:51:08,300
that corresponds to the type of users come back we

847
00:51:08,320 --> 00:51:10,760
so the sea

848
00:51:10,770 --> 00:51:15,790
on the twenty eighth of august so the twenty first of august nineteen ninety eight

849
00:51:15,810 --> 00:51:16,840
we apply

850
00:51:16,890 --> 00:51:22,700
this is a writer one kilogram per hectare and for that with these four

851
00:51:22,700 --> 00:51:24,070
lots of land

852
00:51:24,090 --> 00:51:29,780
so those are put through by two main bottles and those deals out and kill

853
00:51:29,780 --> 00:51:31,330
killing in kilograms

854
00:51:31,380 --> 00:51:33,410
and that's what we are

855
00:51:33,430 --> 00:51:35,080
the columns signify

856
00:51:35,090 --> 00:51:42,530
is that this one piece of land this these plots were almost killed in contiguous

857
00:51:42,530 --> 00:51:47,410
block one and this somewhere else and these somewhere else somewhere else

858
00:51:47,430 --> 00:51:52,470
and the point is that there are different positions relative to the root

859
00:51:52,470 --> 00:51:56,550
and the ricci flow to the field so what we're expecting is that these

860
00:51:56,570 --> 00:52:00,320
four places will have different

861
00:52:00,500 --> 00:52:03,080
so some the characteristics basically

862
00:52:06,230 --> 00:52:07,530
what we're interested in

863
00:52:07,550 --> 00:52:12,130
if the effects of these these call frequencies the things that we can control this

864
00:52:12,130 --> 00:52:13,560
is just

865
00:52:13,570 --> 00:52:16,570
and after the feeling that we the cropping

866
00:52:18,550 --> 00:52:20,650
scientific purposes

867
00:52:20,680 --> 00:52:25,430
what know what the effect of this on the to turn it really bothered about

868
00:52:26,510 --> 00:52:29,240
because nobody else has this field

869
00:52:29,390 --> 00:52:34,210
this is a picture of the things that we throughout the experiment and to generalize

870
00:52:34,870 --> 00:52:38,280
other people going to somewhere else

871
00:52:38,330 --> 00:52:42,320
this doesn't help is very much what we would like to say about the operation

872
00:52:42,320 --> 00:52:45,090
right OK so

873
00:52:45,110 --> 00:52:53,260
so that we have

874
00:52:53,450 --> 00:52:56,750
the axes the inputs will correspond to

875
00:52:56,750 --> 00:53:01,850
the difference at the turn it back into market the difference only based on the

876
00:53:02,970 --> 00:53:05,230
so in densities

877
00:53:06,340 --> 00:53:13,500
we have labels for sixteen different combinations and the remember my is OK

878
00:53:16,270 --> 00:53:19,080
his model we we might use to describe that

879
00:53:19,100 --> 00:53:25,540
we are yield as a weight

880
00:53:25,550 --> 00:53:28,700
as equal to some constant

881
00:53:28,810 --> 00:53:31,240
beta times

882
00:53:31,260 --> 00:53:35,810
that's be general so what is the

883
00:53:35,830 --> 00:53:38,080
if j

884
00:53:38,090 --> 00:53:41,570
if you want to receive treatment b

885
00:53:41,720 --> 00:53:43,570
so that is

886
00:53:43,570 --> 00:53:45,980
if what

887
00:53:45,990 --> 00:53:47,300
j was the

888
00:53:47,300 --> 00:53:51,560
so girls and boys would try to meet their

889
00:53:51,610 --> 00:54:00,850
future husband-and-wife through internet and the idea was can we propose better beatings between based

890
00:54:00,850 --> 00:54:03,930
on profiles of of the people

891
00:54:04,050 --> 00:54:06,980
and that was quite an interesting application

892
00:54:07,000 --> 00:54:13,780
there was an application for construction projects and there was very interesting application for the

893
00:54:13,780 --> 00:54:15,120
european commission

894
00:54:15,130 --> 00:54:20,770
in which we have analysed all the IST projects which were funded by the european

895
00:54:20,770 --> 00:54:27,360
commission in the previous framework programme and the idea was to try to better understand

896
00:54:27,550 --> 00:54:29,130
where the large amount of

897
00:54:29,300 --> 00:54:35,670
the money went to do so we were using text mining and web mining

898
00:54:36,140 --> 00:54:42,240
in order to analyse this data and i will illustrate this application as well

899
00:54:42,260 --> 00:54:45,690
so starting with the mean media application

900
00:54:45,710 --> 00:54:53,330
especially for the people from statistics you are very well aware of questioners doing analysis

901
00:54:53,330 --> 00:54:58,510
of questions with statistical methods here we started with the questioners

902
00:54:58,530 --> 00:55:00,010
about people

903
00:55:00,020 --> 00:55:07,720
reading habits and habits of watching TV programs listening to the radio programs and this

904
00:55:08,300 --> 00:55:15,990
is for a number of years medium has been collecting such information and there was

905
00:55:16,880 --> 00:55:22,400
extensive questionnaire so it doesn't hundred questions obviously not all the questions were given to

906
00:55:22,400 --> 00:55:27,740
people because they died on something so many questions but by answering the question then

907
00:55:27,740 --> 00:55:33,980
you have automatically as a couple of others is still it's quite laborious process students

908
00:55:33,980 --> 00:55:34,840
go to

909
00:55:34,900 --> 00:55:36,270
two home

910
00:55:36,280 --> 00:55:38,930
ask people about their habits

911
00:55:38,940 --> 00:55:42,830
and did the results of these analyses are

912
00:55:42,850 --> 00:55:51,470
then published in your publication it's basically the basic statistics about frequencies of reading listening

913
00:55:51,480 --> 00:55:59,670
watching different TV programs and then distributions with respect to sex age education buying habits

914
00:55:59,670 --> 00:56:04,470
and so on and so on so we tried to

915
00:56:05,590 --> 00:56:12,190
an alternative analysis the for this we got eight thousand questionnaires

916
00:56:14,020 --> 00:56:19,550
this was data for the year nineteen ninety eight and that was about lifestyle space

917
00:56:19,560 --> 00:56:24,040
time activities personal viewpoints reading listening watching of media

918
00:56:24,060 --> 00:56:30,020
and so on so it was very good quality data obviously the professionals have collected

919
00:56:30,390 --> 00:56:36,280
and also it was cleaned before we got it so the medium itself cleans the

920
00:56:36,280 --> 00:56:40,890
data so it was extremely good quality data so we didn't have to bother about

921
00:56:40,890 --> 00:56:46,330
noise elimination missing data and things like that so basically we got

922
00:56:47,450 --> 00:56:49,620
table of on top of

923
00:56:49,890 --> 00:56:56,260
which in each row corresponds to one person who was answering his questions one individual

924
00:56:56,550 --> 00:57:06,160
and the columns were about the attributes of about lifestyle the spare time activities and

925
00:57:06,160 --> 00:57:06,920
so on

926
00:57:07,620 --> 00:57:11,140
for classification purposes we needed to

927
00:57:11,410 --> 00:57:16,690
select one attributes which was the target attribute for classification

928
00:57:16,820 --> 00:57:25,150
and then after having done this data preprocessing then we did data mining and here

929
00:57:25,150 --> 00:57:29,040
we are free to choose what data mining task

930
00:57:29,090 --> 00:57:34,070
perform because the client omidyar didn't give us any

931
00:57:34,080 --> 00:57:40,850
specific requests in advance so we decided to answer some questions like which journals

932
00:57:40,860 --> 00:57:44,210
all magazines are read by the readers of another journal

933
00:57:44,230 --> 00:57:49,890
what are the properties of individuals that are consumers of particular media offer and which

934
00:57:49,890 --> 00:57:53,260
properties of distinctive for users of different channels

935
00:57:53,260 --> 00:57:57,120
it was this figure explains two things number one you can predict the entries that

936
00:57:57,120 --> 00:58:00,590
you have not seen very well but on top of it of course you removed

937
00:58:00,590 --> 00:58:06,030
all of the nodes of the entries that you've seen because initially y minus and

938
00:58:06,030 --> 00:58:10,360
divided by m squared on the visible set will be wanting to the noise power

939
00:58:10,640 --> 00:58:12,570
so you reduce the noise level

940
00:58:12,590 --> 00:58:16,160
by a factor twenty in the entries that you've seen

941
00:58:16,180 --> 00:58:21,180
and you correct you print correctly predict all the answers

942
00:58:21,300 --> 00:58:24,510
we've seen this was done experiments which is

943
00:58:24,570 --> 00:58:27,870
was real datasets so we took a data set

944
00:58:28,180 --> 00:58:30,110
that have temperatures

945
00:58:30,120 --> 00:58:33,430
fourteen seventy two locations around the world

946
00:58:33,430 --> 00:58:35,930
in two thousand eight so it's

947
00:58:35,970 --> 00:58:39,800
what we have forty two for about fifteen hundred locations about three hundred and sixty

948
00:58:39,800 --> 00:58:42,820
six days because o easily peer

949
00:58:42,910 --> 00:58:47,280
and what we did is we we held seventy percent of the entries

950
00:58:47,300 --> 00:58:48,120
and then

951
00:58:48,140 --> 00:58:52,850
run this algorithm on only thirty percent to try to predict the entries that we

952
00:58:52,850 --> 00:58:54,160
have not seen

953
00:58:54,180 --> 00:58:55,720
and when we do is this

954
00:58:55,760 --> 00:58:58,740
we see that the errors very very little

955
00:58:58,740 --> 00:59:00,140
that is the

956
00:59:00,160 --> 00:59:03,530
the relative error between the truth and the recovery

957
00:59:03,530 --> 00:59:07,510
is roughly three percent accurate

958
00:59:07,550 --> 00:59:11,950
OK and it's the reason it works very well it's because this matrix of temperature

959
00:59:11,950 --> 00:59:16,370
data has very low rank since if i look at the best rank two approximation

960
00:59:16,370 --> 00:59:19,350
that i can compute by looking at all of the entries

961
00:59:19,370 --> 00:59:24,160
i would say that you know the relative error is about one point five percent

962
00:59:24,180 --> 00:59:27,890
because this is is a low rank matrix and not surprisingly

963
00:59:27,930 --> 00:59:30,070
we can actually

964
00:59:30,090 --> 00:59:32,660
completed very very well

965
00:59:34,990 --> 00:59:37,930
what i'd like to talk about now in the last

966
00:59:37,990 --> 00:59:41,030
ten minutes some computational aspects it is

967
00:59:41,050 --> 00:59:44,760
OK so now we have to deal with very large matrices we have to solve

968
00:59:44,760 --> 00:59:49,240
this optimisation problem and how it can this

969
00:59:49,260 --> 00:59:54,090
so i claim that it's a semidefinite programme therefore we all right and when i

970
00:59:54,090 --> 00:59:56,050
say this is of course i'm lying

971
00:59:56,070 --> 01:00:02,300
because it is not true that off the shelf semidefinite programming solvers as sdpt three

972
01:00:02,300 --> 01:00:03,410
or c do me

973
01:00:03,490 --> 01:00:07,640
can solve problems are very large size like this in fact if you use off

974
01:00:07,640 --> 01:00:09,620
the shelf algorithms and you

975
01:00:09,640 --> 01:00:13,200
see them with matrix completion problem they will choke

976
01:00:13,220 --> 01:00:15,350
when n is about one hundred

977
01:00:15,370 --> 01:00:21,180
OK then you can try to customize interior point methods but they won't do much

978
01:00:22,220 --> 01:00:27,010
so it's extremely problematic because it means that if i want to use customized

979
01:00:27,010 --> 01:00:31,660
optimisation packages that can complete matrices

980
01:00:31,680 --> 01:00:36,680
one hundred by one hundred we very far from one thousand by one thousand thirty

981
01:00:36,680 --> 01:00:40,570
thousand by thirty thousand or even the netflix size

982
01:00:42,050 --> 01:00:45,740
so what i'd like to introduce is a very simple algorithm

983
01:00:45,760 --> 01:00:46,820
that will

984
01:00:47,030 --> 01:00:49,890
so nuclear norm minimisation problem

985
01:00:50,120 --> 01:00:53,820
subject to convex constraints which is very

986
01:00:53,840 --> 01:00:56,370
simple algorithm that happens to be

987
01:00:56,370 --> 01:01:00,260
extremely efficient when the optimal solution has low rank

988
01:01:00,300 --> 01:01:03,510
when the optimal solution does not have the the right may not be a very

989
01:01:03,510 --> 01:01:05,680
good outcome

990
01:01:05,700 --> 01:01:10,010
so i would like to explain these are gruesome in the limited setting of the

991
01:01:10,010 --> 01:01:12,700
matrix completion problem which

992
01:01:12,720 --> 01:01:14,510
i will recast as it is

993
01:01:14,530 --> 01:01:18,720
so we want to minimize the nuclear norm subjective data constraints

994
01:01:18,720 --> 01:01:23,200
so now i'm going to look at it explains the data comes cassidy data constraint

995
01:01:23,280 --> 01:01:28,570
simply the projection of x onto the feasible set has to matter the projection of

996
01:01:28,570 --> 01:01:31,950
a onto the feasible sets and what is p omega

997
01:01:32,070 --> 01:01:37,090
well omega of x is essentially leaving the entries of x intact if they've been

998
01:01:37,090 --> 01:01:40,620
observed and otherwise set centers

999
01:01:40,700 --> 01:01:45,700
so it's a projection on to the locations that i have seen

1000
01:01:45,720 --> 01:01:47,780
OK so we want to solve this problem

1001
01:01:48,720 --> 01:01:49,780
OK so

1002
01:01:49,780 --> 01:01:53,260
this algorithm is very simple and what it does is it uses as the key

1003
01:01:54,430 --> 01:01:57,300
what we call singular value thresholding

1004
01:01:57,320 --> 01:02:02,390
so it's a nonlinear operation about the matrix to nonlinear transformation of the matrix that

1005
01:02:02,390 --> 01:02:03,550
goes like this

1006
01:02:03,570 --> 01:02:09,510
given a matrix x and i can compute its SVD just like this

1007
01:02:09,570 --> 01:02:12,410
and i'm going to introduce the frank

1008
01:02:12,430 --> 01:02:15,640
of x the depends on the parameter tau

1009
01:02:15,800 --> 01:02:17,610
it's a very simple thing

1010
01:02:17,620 --> 01:02:21,070
what i do is give me the matrix are right out this a singular value

1011
01:02:22,410 --> 01:02:25,660
and i push its singular values towards zero

1012
01:02:25,680 --> 01:02:27,610
by an amount which is

1013
01:02:27,660 --> 01:02:30,220
essentially the shrinkage parameter top

1014
01:02:30,240 --> 01:02:33,780
so what i do is i just compute the SVD and i reduce the singular

1015
01:02:33,780 --> 01:02:35,240
values by tau

1016
01:02:35,260 --> 01:02:39,220
of course making sure that they don't become negative

1017
01:02:39,240 --> 01:02:40,240
OK so

1018
01:02:40,260 --> 01:02:44,340
so i compute singular values and a decrease their values by tau

1019
01:02:44,350 --> 01:02:49,370
and if this becomes negative i decided to zero

1020
01:02:50,490 --> 01:02:54,990
more prosaically what i'm doing is if you're familiar with soft thresholding what i'm doing

1021
01:02:54,990 --> 01:02:58,510
is applied soft thresholding to the singular value

1022
01:02:59,450 --> 01:03:02,640
and of course this will reduce the rank of the matrix it is he will

1023
01:03:02,640 --> 01:03:04,090
not increase it

1024
01:03:04,110 --> 01:03:06,640
because of course

1025
01:03:06,660 --> 01:03:10,410
it will it can only reduce the rank in fact it will reduce the rank

1026
01:03:10,410 --> 01:03:15,660
tremendously if in a lot of singular values happen to be below the threshold tau

1027
01:03:15,660 --> 01:03:19,110
because then will just add them to c and a lot of singular values will

1028
01:03:22,930 --> 01:03:23,740
and now

1029
01:03:23,760 --> 01:03:25,570
the algorithm

1030
01:03:25,620 --> 01:03:27,700
is very very simple

1031
01:03:28,700 --> 01:03:30,590
you fixed parameter tau

1032
01:03:30,590 --> 01:03:34,050
and the sequence of positive steps times delta k

1033
01:03:34,050 --> 01:03:38,160
you start with an auxiliary matrix y zero which is equal to zero and you

1034
01:03:38,160 --> 01:03:40,720
go through this very simple iterations

1035
01:03:40,740 --> 01:03:44,140
we're going to take like a minus one and shrink its singular values at the

1036
01:03:44,140 --> 01:03:45,410
level tau

1037
01:03:45,430 --> 01:03:47,530
and then you can update y

1038
01:03:47,550 --> 01:03:51,800
so update y as y k equals the previous value of y

1039
01:03:51,820 --> 01:03:57,180
plus the step size times the projection of minus exc

1040
01:03:57,430 --> 01:03:59,570
and that's the algorithm

1041
01:03:59,590 --> 01:04:02,760
so this is simply the arteries image can be coded

1042
01:04:03,470 --> 01:04:07,820
five matlab lines and it's just right there

1043
01:04:07,870 --> 01:04:13,070
and these are in was inspired by recent work in l one minimisation in the

1044
01:04:13,070 --> 01:04:14,990
area of compressed sensing

1045
01:04:15,010 --> 01:04:20,140
and essentially by the work of stan sure and his colleagues at UCLA but

1046
01:04:20,140 --> 01:04:21,430
this is

1047
01:04:21,470 --> 01:04:26,090
algorithm so basically all the computational cost is here

1048
01:04:26,120 --> 01:04:30,010
the computational cost is to apply the shrinkage operator to y

1049
01:04:30,030 --> 01:04:31,320
several times

1050
01:04:31,340 --> 01:04:33,840
because this update is absolutely trivial

1051
01:04:33,840 --> 01:04:37,390
and that made s three VM which is this line

1052
01:04:37,460 --> 01:04:41,900
for the good

1053
01:04:41,980 --> 01:04:45,100
but finally let's look at the graph based method

1054
01:04:45,110 --> 01:04:48,540
and in the graph based method you have all these

1055
01:04:48,590 --> 01:04:51,590
data points one dimensions

1056
01:04:51,600 --> 01:04:53,790
and to the graph you will create

1057
01:04:53,850 --> 01:04:57,450
simply going to be a graph that connects all those points

1058
01:04:57,520 --> 01:04:58,980
one day

1059
01:04:59,230 --> 01:05:03,620
again we have some mismatch here

1060
01:05:03,640 --> 01:05:06,090
because there is no

1061
01:05:06,500 --> 01:05:10,080
well separated components

1062
01:05:10,090 --> 01:05:11,120
so when you do

1063
01:05:11,140 --> 01:05:15,800
graph based method it's going to propagate labels

1064
01:05:17,650 --> 01:05:19,100
as you can see the

1065
01:05:19,110 --> 01:05:21,330
this line the same

1066
01:05:21,340 --> 01:05:22,710
black line

1067
01:05:24,660 --> 01:05:26,550
actually always

1068
01:05:26,610 --> 01:05:29,160
farther away from the

1069
01:05:29,180 --> 01:05:34,330
supervised learning decision boundary so in the end it has a larger

1070
01:05:34,350 --> 01:05:38,000
error than supervised learning

1071
01:05:38,060 --> 01:05:44,590
so this is this example simply says again that it's very important

1072
01:05:44,640 --> 01:05:49,810
to get the correct assumption for some supervised learning and when the assumption is wrong

1073
01:05:49,810 --> 01:05:51,160
when it doesn't fit

1074
01:05:51,220 --> 01:05:55,450
your data set than the performance can be really bad

1075
01:05:55,460 --> 01:05:59,660
so this leads to the natural question how can we quantify this notion that

1076
01:06:00,060 --> 01:06:03,040
the assumption is correct or wrong

1077
01:06:03,050 --> 01:06:06,790
are there any theory behind this

1078
01:06:06,800 --> 01:06:08,610
so we will discuss

1079
01:06:08,640 --> 01:06:11,100
very simple version of

1080
01:06:11,150 --> 01:06:15,650
pac learning theory originally by balcan and blum

1081
01:06:16,740 --> 01:06:20,660
attempts to quantify the contribution of unlabelled data

1082
01:06:20,720 --> 01:06:27,660
so we will give a really quick review of a learning theory

1083
01:06:27,670 --> 01:06:31,300
OK so let's first go back to supervised

1084
01:06:31,720 --> 01:06:33,850
now in supervised learning you have this

1085
01:06:33,870 --> 01:06:36,980
labelled data let's call it the better training set

1086
01:06:36,980 --> 01:06:41,870
you have our labelled data points there are sampled i i d from some unknown

1087
01:06:41,870 --> 01:06:43,850
joint distribution p

1088
01:06:44,000 --> 01:06:50,350
you also have a family of functions that you want to pick things from for

1089
01:06:50,350 --> 01:06:56,360
example the family big as can be all linear functions

1090
01:06:58,050 --> 01:07:01,280
the way we're going to

1091
01:07:01,290 --> 01:07:05,670
two training in an abstract abstract sense is to say that we're going to find

1092
01:07:05,670 --> 01:07:10,120
the function small as within this big from function family

1093
01:07:10,140 --> 01:07:13,890
such that small has zero training

1094
01:07:14,030 --> 01:07:17,580
so this is yehat

1095
01:07:17,620 --> 01:07:22,660
of a function as is simply the training error so this is a boolean expressions

1096
01:07:22,660 --> 01:07:25,410
are one good to that's the training

1097
01:07:29,750 --> 01:07:35,220
OK so we assume that the training goes as follows you give me

1098
01:07:35,230 --> 01:07:39,180
the training set the samples from the unknown p distribution

1099
01:07:39,200 --> 01:07:43,990
i search through big ass and find a small as

1100
01:07:44,040 --> 01:07:46,540
that has zero training

1101
01:07:47,650 --> 01:07:52,000
and i want to see how well this small as forms

1102
01:07:53,950 --> 01:07:56,790
the quantity and really interested being is

1103
01:07:56,800 --> 01:08:00,980
the generalisation performance or the performance of future test data

1104
01:08:01,000 --> 01:08:05,870
of my function as an in here i give it the subscript the because my

1105
01:08:05,870 --> 01:08:06,980
f is picked

1106
01:08:07,150 --> 01:08:09,720
using the training set

1107
01:08:09,740 --> 01:08:12,220
so this he has no hat

1108
01:08:12,280 --> 01:08:13,970
that's the true error

1109
01:08:13,980 --> 01:08:19,500
it's defined as the expectation where you sample your test points from the on p

1110
01:08:21,170 --> 01:08:24,530
and this is simply how many of the error rate

1111
01:08:28,180 --> 01:08:29,850
can we say anything about

1112
01:08:29,900 --> 01:08:32,160
it's future test error

1113
01:08:32,580 --> 01:08:36,020
the big problem here is i do not know p

1114
01:08:36,030 --> 01:08:36,860
right so

1115
01:08:36,890 --> 01:08:38,360
we cannot directly

1116
01:08:38,370 --> 01:08:40,450
compute this quantity

1117
01:08:40,460 --> 01:08:42,890
but it turns out

1118
01:08:42,900 --> 01:08:47,910
even though i cannot give it a precise number i can bond

1119
01:08:47,970 --> 01:08:50,250
the value of this e

1120
01:08:50,300 --> 01:08:52,680
without knowing pl

1121
01:08:52,690 --> 01:08:55,110
here's how it goes

1122
01:08:55,810 --> 01:08:58,550
so first notice that

1123
01:09:00,500 --> 01:09:04,270
function that i picked as subscript d

1124
01:09:04,350 --> 01:09:06,060
is a random variable

1125
01:09:06,280 --> 01:09:08,970
is the venerable in the sense that

1126
01:09:08,980 --> 01:09:13,260
my training set these random so if you give me around the i have a

1127
01:09:14,480 --> 01:09:15,500
after a

1128
01:09:15,560 --> 01:09:17,510
so it's a random variable

1129
01:09:17,530 --> 01:09:21,600
with respect to random draws of my training set

1130
01:09:23,000 --> 01:09:26,570
this is a random variable therefore

1131
01:09:29,420 --> 01:09:33,550
there is a p although i do not know it's OK so

1132
01:09:33,600 --> 01:09:35,590
for any function

1133
01:09:35,600 --> 01:09:38,480
it's true error he is actually fixed

1134
01:09:38,490 --> 01:09:43,110
and in this case so i just don't know it but it's a fixed quantity

1135
01:09:43,840 --> 01:09:46,270
so i can look at the event

1136
01:09:48,470 --> 01:09:50,100
classifier as the

1137
01:09:50,100 --> 01:09:53,060
i train right selected

1138
01:09:54,100 --> 01:09:57,440
larger true error then absolutely

1139
01:09:57,450 --> 01:10:01,620
that's going to be around the event that depends on my random draw of

1140
01:10:01,640 --> 01:10:03,150
the training set

1141
01:10:04,050 --> 01:10:06,450
so this is a random boolean event

1142
01:10:06,460 --> 01:10:09,800
either happens or it doesn't have

1143
01:10:09,820 --> 01:10:12,590
so absent some fixed threshold

1144
01:10:12,590 --> 01:10:15,300
just this next part now take this too

1145
01:10:15,320 --> 01:10:17,760
really to the results

1146
01:10:17,770 --> 01:10:19,800
based on the goal line

1147
01:10:20,870 --> 01:10:22,550
in terms of slide

1148
01:10:25,050 --> 01:10:32,270
slide numbers aligned

1149
01:10:32,270 --> 01:10:36,040
in my numbering i did so i been adding some stuff and changing it and

1150
01:10:37,410 --> 01:10:43,200
i'm really grateful to you all for not hang on didnt say exactly that but

1151
01:10:43,200 --> 01:10:46,620
maybe i haven't had time to sort of harbor across

1152
01:10:48,140 --> 01:10:53,820
i also saw that the printed six lots the pages across to achieve

1153
01:10:53,870 --> 01:10:55,640
is that someone shares

1154
01:10:57,100 --> 01:11:03,140
well thank you sure look like not lies like me because i couldn't

1155
01:11:03,150 --> 01:11:10,430
but sorry for forgetting to here about sixty just just around sixty slides inside fifty

1156
01:11:10,430 --> 01:11:11,250
four so

1157
01:11:11,290 --> 01:11:16,060
we be done in the y just keep talking and mining up will just

1158
01:11:16,070 --> 01:11:20,230
go through its twenty past three will just go through the it after four down

1159
01:11:20,260 --> 01:11:22,730
one stop that's OK with everyone

1160
01:11:23,030 --> 01:11:26,400
this course it's OK

1161
01:11:26,480 --> 01:11:29,280
but nevertheless this is sort of the the

1162
01:11:29,930 --> 01:11:32,090
the point of the whole set of lectures

1163
01:11:32,230 --> 01:11:36,260
so if you could pay attention to this part the grateful officially

1164
01:11:36,280 --> 01:11:37,620
i think are all

1165
01:11:37,670 --> 01:11:41,310
and i didn't want to be talking about this at two o'clock is at least

1166
01:11:41,320 --> 01:11:42,900
data after lunch

1167
01:11:45,120 --> 01:11:50,120
he gets as part of this thing would have to go find it's not like

1168
01:11:50,170 --> 01:11:53,260
but all the

1169
01:11:53,280 --> 01:11:54,510
all right

1170
01:11:54,570 --> 01:11:55,620
well so

1171
01:11:55,640 --> 01:11:59,560
what happens with kernel memory where do we go from here that something on this

1172
01:11:59,560 --> 01:12:01,790
and work with the with the concept

1173
01:12:01,840 --> 01:12:02,820
we saw two

1174
01:12:02,840 --> 01:12:05,760
not going really YSL's about whether

1175
01:12:08,780 --> 01:12:11,560
well was that successor apostrophe who was

1176
01:12:11,570 --> 01:12:16,060
talk about that on this page

1177
01:12:16,310 --> 01:12:21,560
o yeah right structure the successor and

1178
01:12:21,620 --> 01:12:23,340
yeah that's right so

1179
01:12:23,400 --> 01:12:26,850
yes i think you right the intention there would be not to have the eleventh

1180
01:12:26,850 --> 01:12:28,680
variable would have

1181
01:12:28,750 --> 01:12:31,990
would be indicated by x with eleven

1182
01:12:32,080 --> 01:12:33,430
posture is after

1183
01:12:33,470 --> 01:12:35,660
thank you that's that's what makes it

1184
01:12:35,710 --> 01:12:39,460
and the actual successor function

1185
01:12:41,900 --> 01:12:44,030
it's the it's

1186
01:12:44,090 --> 01:12:45,650
all right

1187
01:12:45,660 --> 01:12:49,000
this is the next piece of work that you have to do which we don't

1188
01:12:49,900 --> 01:12:51,710
keep saying the popular

1189
01:12:53,440 --> 01:12:55,800
we have to set up some stuff

1190
01:12:55,840 --> 01:12:59,830
as a rita to being out for something more things and show that we dance

1191
01:12:59,830 --> 01:13:05,300
we've actually simulate the operations of the formal system inside of arithmetic or something like

1192
01:13:06,400 --> 01:13:08,400
so we agreed now the

1193
01:13:08,400 --> 01:13:12,100
that phi has neural kernel number

1194
01:13:12,120 --> 01:13:16,060
OK which are now use that terminology left and right corners

1195
01:13:17,000 --> 01:13:21,250
it has a numeral corresponding to its what the actual number whenever i choose to

1196
01:13:21,250 --> 01:13:22,580
go abroad

1197
01:13:22,590 --> 01:13:25,750
i think that's the rule

1198
01:13:27,850 --> 01:13:29,400
a turing machine

1199
01:13:29,470 --> 01:13:34,870
can calculate whether some particular numbers the kernel not expression

1200
01:13:34,870 --> 01:13:38,550
i suppose that was kind of like my hands

1201
01:13:38,590 --> 01:13:40,050
in the previous lecture

1202
01:13:41,030 --> 01:13:45,560
because it's unique and depending on the coding scheme we've chosen whether it's the original

1203
01:13:46,550 --> 01:13:49,100
colonel one in terms of prime decomposition

1204
01:13:49,240 --> 01:13:52,910
whether it's the clear the last one in terms of

1205
01:13:52,970 --> 01:13:56,250
strange sequences of digits

1206
01:13:56,250 --> 01:13:59,150
we could easily we can effectively figure out

1207
01:13:59,160 --> 01:14:03,840
whether something or other is the kernel number and expression

1208
01:14:04,910 --> 01:14:06,500
it also turns out

1209
01:14:06,500 --> 01:14:09,740
on display solution to you

1210
01:14:09,780 --> 01:14:13,150
that recursive functions can now be defined formally

1211
01:14:13,190 --> 01:14:14,400
in these terms

1212
01:14:14,410 --> 01:14:16,130
because of that equivalence thing

1213
01:14:16,150 --> 01:14:18,150
we took just went through

1214
01:14:18,160 --> 01:14:19,750
cherishing can

1215
01:14:19,750 --> 01:14:22,190
we can find a recursive function

1216
01:14:22,210 --> 01:14:26,010
what is that recursive function when talking about to know what it is we just

1217
01:14:26,010 --> 01:14:28,690
started day

1218
01:14:28,730 --> 01:14:30,780
and we're also going to need to be either

1219
01:14:32,400 --> 01:14:33,540
the idea

1220
01:14:33,570 --> 01:14:35,350
of substituting

1221
01:14:35,360 --> 01:14:37,650
attorney four

1222
01:14:37,770 --> 01:14:40,770
four four formula

1223
01:14:40,920 --> 01:14:43,030
my on my have free very

1224
01:14:43,040 --> 01:14:46,090
so applying value to function

1225
01:14:46,900 --> 01:14:50,960
that function is to be defined and it turns out that we can because that's

1226
01:14:50,960 --> 01:14:52,530
it decidable thing

1227
01:14:52,580 --> 01:14:54,780
inside formal languages

1228
01:14:54,780 --> 01:15:00,700
and we normally teachers who teaching formal logic some kind of pressure

1229
01:15:00,740 --> 01:15:01,670
just say

1230
01:15:01,690 --> 01:15:04,510
o like k which is going to take this formula

1231
01:15:05,910 --> 01:15:10,260
got constant i so we're just going to substitute achieve i

1232
01:15:10,270 --> 01:15:14,960
and of course there are some complications in this is exhibited right inside something

1233
01:15:16,160 --> 01:15:18,970
but of course to be official we need to

1234
01:15:18,980 --> 01:15:23,590
actually formalize the notion of substitution which we do as well

1235
01:15:23,650 --> 01:15:25,070
and here

1236
01:15:25,080 --> 01:15:28,710
what we're saying is that we can also define recursive function

1237
01:15:28,780 --> 01:15:30,570
to do that substitution

1238
01:15:30,650 --> 01:15:33,580
for the functions

1239
01:15:33,630 --> 01:15:36,100
then now getting to

1240
01:15:36,110 --> 01:15:38,320
we can define a predicate in

1241
01:15:38,330 --> 01:15:40,970
now formal system

1242
01:15:41,030 --> 01:15:43,210
not exhibiting that definition

1243
01:15:43,270 --> 01:15:44,840
we can define a predicate

1244
01:15:46,530 --> 01:15:48,210
and probability

1245
01:15:48,280 --> 01:15:49,740
he says that

1246
01:15:52,820 --> 01:15:53,710
let's see

1247
01:15:54,090 --> 01:15:58,170
the formula the intention this

1248
01:16:03,770 --> 01:16:12,970
so it's not

1249
01:16:13,040 --> 01:16:15,720
well on the next page so i posted to do

1250
01:16:16,610 --> 01:16:18,410
the provability predicate there

1251
01:16:18,440 --> 01:16:19,710
so the

1252
01:16:19,820 --> 01:16:20,990
it's the formula

1253
01:16:22,880 --> 01:16:24,150
is provable

1254
01:16:24,150 --> 01:16:26,050
in some language or other

1255
01:16:27,110 --> 01:16:28,640
in that same language

1256
01:16:28,650 --> 01:16:31,450
looked at all to prove

1257
01:16:31,460 --> 01:16:33,900
sure i get this right

1258
01:16:38,780 --> 01:16:40,710
for some close to

1259
01:16:40,820 --> 01:16:45,760
in fact he will turn out to be the goal number of this generation

1260
01:16:45,800 --> 01:16:47,700
OK so provability

1261
01:16:47,740 --> 01:16:50,220
is a predicate which says

1262
01:16:51,150 --> 01:16:55,650
it's got two numbers as arguments to actually two numerals and formal systems got two

1263
01:16:55,650 --> 01:16:57,640
numbers arguments

1264
01:16:57,670 --> 01:16:59,190
and it's true

1265
01:16:59,200 --> 01:17:00,760
in the full system

1266
01:17:00,800 --> 01:17:02,470
just in case

1267
01:17:02,520 --> 01:17:05,230
the formula five was proved by

1268
01:17:05,240 --> 01:17:10,170
the derivation that have ended up being cut is t

1269
01:17:10,190 --> 01:17:12,770
so we just simulating all this activity

1270
01:17:12,780 --> 01:17:16,790
that we normally do externally with learning logic courses in

1271
01:17:16,840 --> 01:17:18,570
great to see

1272
01:17:23,740 --> 01:17:29,200
now it's not not fact it's crucial it isn't yet

1273
01:17:29,210 --> 01:17:33,040
now i'm just saying

1274
01:17:33,050 --> 01:17:34,690
yes that's right

1275
01:17:34,730 --> 01:17:37,410
that's right i haven't heard anything about

1276
01:17:37,640 --> 01:17:43,150
not that provable i said is that there is an encoding

1277
01:17:43,920 --> 01:17:47,900
and the pos and the definition of predicate possible

1278
01:17:47,910 --> 01:17:51,990
OK because it's the function can be done is recursive functions

1279
01:17:52,040 --> 01:17:54,730
it can be embedded in first order logic

1280
01:17:54,800 --> 01:17:57,090
you will have just this effect

1281
01:17:57,110 --> 01:17:58,050
it's the

1282
01:17:58,080 --> 01:18:00,850
that's the special effect of this function

1283
01:18:00,900 --> 01:18:03,100
so defined inside this rich

1284
01:18:03,110 --> 01:18:05,650
formal language

1285
01:18:05,650 --> 01:18:10,790
has the five times higher probability to be scattered then the red light

1286
01:18:10,810 --> 01:18:12,020
and this

1287
01:18:12,040 --> 01:18:14,380
hold as long as the

1288
01:18:14,380 --> 01:18:17,960
particles of which the radiation scanners

1289
01:18:17,980 --> 01:18:22,770
are smaller than the wavelength of light typically a few tens of micro

1290
01:18:22,770 --> 01:18:26,690
so then we have really scared and then we have the dependence on

1291
01:18:26,730 --> 01:18:28,850
wavelengths to the depart four

1292
01:18:28,860 --> 01:18:33,980
if the particles grow in size this dependence weakness weakens

1293
01:18:34,020 --> 01:18:37,460
and then the difference between the red and the blue becomes less

1294
01:18:37,480 --> 01:18:40,250
and by the time you read five microns

1295
01:18:40,270 --> 01:18:46,000
particles of heaviside size of five microns and ten microns then the probability is the

1296
01:18:46,000 --> 01:18:49,330
same for all callers

1297
01:18:49,350 --> 01:18:51,130
and that's what i

1298
01:18:51,130 --> 01:18:53,330
i would like to demonstrate to you

1299
01:18:53,360 --> 01:18:55,960
very shortly

1300
01:18:56,000 --> 01:18:58,650
there is an extra bonus

1301
01:18:58,710 --> 01:19:00,710
any extra bonuses

1302
01:19:00,750 --> 01:19:04,250
it's not so intuitive but i will convince you of that

1303
01:19:04,270 --> 01:19:09,330
that's radiation that was carried over ninety degrees becomes

1304
01:19:09,380 --> 01:19:15,600
linearly polarized hundred percent on polarized radiation comes in

1305
01:19:15,650 --> 01:19:17,100
you are scatterers

1306
01:19:17,100 --> 01:19:20,850
light goes off and all their actions but goes off at ninety degrees

1307
01:19:20,860 --> 01:19:22,230
is hundred percent

1308
01:19:22,230 --> 01:19:26,790
linearly polarized i'll first convince you of that and then i will do demonstration

1309
01:19:28,580 --> 01:19:32,560
that shows that indeed it is linearly polarized that's why i wanted you to bring

1310
01:19:32,560 --> 01:19:36,400
your policies

1311
01:19:36,500 --> 01:19:39,830
see radiation coming from this site

1312
01:19:39,850 --> 01:19:42,440
and he uses electron that starts to shake

1313
01:19:42,460 --> 01:19:44,560
and if you have very small particles

1314
01:19:44,560 --> 01:19:50,440
one turns into tens of icons the electrons in those particles will begin to shape

1315
01:19:50,460 --> 01:19:54,420
and so this electron start to shake up and down and the electric field must

1316
01:19:55,310 --> 01:19:57,830
in the plane of are an

1317
01:19:57,830 --> 01:19:59,460
look very carefully

1318
01:19:59,520 --> 01:20:02,710
this electric field is indeed in the plane of are

1319
01:20:02,730 --> 01:20:07,620
and a this electric field is in the plane of are in a this electric

1320
01:20:07,620 --> 01:20:11,020
field is in the plane of our in a

1321
01:20:11,210 --> 01:20:12,790
so this is the direction

1322
01:20:13,580 --> 01:20:15,230
oscillation of if you

1323
01:20:16,080 --> 01:20:20,380
the direction of data zero there is no radiation going because the acceleration is like

1324
01:20:20,380 --> 01:20:24,120
this and if you go off at a different angle than the electric vector also

1325
01:20:24,120 --> 01:20:28,580
laid perpendicular to its direction of propagation

1326
01:20:28,670 --> 01:20:30,360
why now

1327
01:20:30,400 --> 01:20:33,900
is radiation hundred percent polarized

1328
01:20:33,960 --> 01:20:35,810
if it's carries over

1329
01:20:35,830 --> 01:20:39,310
ninety degrees

1330
01:20:43,230 --> 01:20:47,460
this have been of unpolarized radiation that comes to you

1331
01:20:47,500 --> 01:20:49,860
linearly polarized light comes to you

1332
01:20:49,880 --> 01:20:53,170
she she she she there is

1333
01:20:53,170 --> 01:20:54,560
the next wave

1334
01:20:54,580 --> 01:20:57,940
and the next word in the next race in the next wave and the next

1335
01:20:57,940 --> 01:21:00,440
word recall the unpolarized light

1336
01:21:00,460 --> 01:21:03,630
one comes in like this the next one comes in like this one comes in

1337
01:21:03,630 --> 01:21:08,000
like this one called the unpolarized light

1338
01:21:08,000 --> 01:21:12,420
you are looking here in the blackboard

1339
01:21:12,420 --> 01:21:15,420
at this radiation and their scanners

1340
01:21:15,670 --> 01:21:18,000
dust very fine dust

1341
01:21:18,020 --> 01:21:22,540
and so some life comes through

1342
01:21:22,690 --> 01:21:24,980
angle is ninety degrees right

1343
01:21:25,040 --> 01:21:27,040
because it came like this

1344
01:21:27,040 --> 01:21:32,850
now does this the whole board ninety degrees someone who is here

1345
01:21:32,900 --> 01:21:36,690
also ninety degrees

1346
01:21:36,770 --> 01:21:39,630
what is the direction of factor

1347
01:21:39,650 --> 01:21:42,830
when this life which is why i

1348
01:21:42,850 --> 01:21:45,810
is in the plane of a or

1349
01:21:45,830 --> 01:21:47,960
that means it must be in the record

1350
01:21:49,210 --> 01:21:53,540
the vector is in the plane of acceleration this accelerat issues like this that's the

1351
01:21:53,540 --> 01:21:57,400
plane of the blackboard this acceleration is like this and are is also in the

1352
01:21:57,400 --> 01:22:01,620
black or so sony vector must be in the plane of the record

1353
01:22:01,630 --> 01:22:05,790
but it must also be perpendicular to are so it must also like this and

1354
01:22:05,940 --> 01:22:10,020
also like like this and if you look here

1355
01:22:10,040 --> 01:22:12,670
it was also made like this

1356
01:22:12,690 --> 01:22:17,190
so when you look at scatterplots on the ninety degrees if it is

1357
01:22:17,210 --> 01:22:18,480
rayleigh scattering

1358
01:22:18,480 --> 01:22:22,420
you will see that the radiation is hundred percent linearly or

1359
01:22:22,420 --> 01:22:24,100
an amazing thing

1360
01:22:24,100 --> 01:22:26,100
and that's my goal

1361
01:22:26,150 --> 01:22:28,630
to demonstrate this to you now

1362
01:22:28,630 --> 01:22:31,020
he was a very dramatic experiment

1363
01:22:31,130 --> 01:22:35,960
which i put my house on the line

1364
01:22:35,960 --> 01:22:39,040
i'm going to to smoke a cigarette

1365
01:22:39,130 --> 01:22:42,460
and i advise you not to

1366
01:22:42,460 --> 01:22:47,130
and and this is work in a similar they have the signatures fall

1367
01:22:47,150 --> 01:22:49,110
computer takes the role so far

1368
01:22:49,200 --> 01:22:50,460
but obvious

1369
01:22:50,650 --> 01:22:55,220
this advantage of this system is every time a new have new attack

1370
01:22:55,220 --> 01:22:58,470
there is a delay in detecting these attacks have to go to the website and

1371
01:22:58,470 --> 01:23:02,780
all this the signature to take this computer intrusion and then you can take so

1372
01:23:02,780 --> 01:23:07,080
basically this systems are incapable of taking a new and emerging

1373
01:23:07,100 --> 01:23:08,260
hope attracts

1374
01:23:08,280 --> 01:23:14,990
and anomaly detection can be extremely important here because they can do this

1375
01:23:15,000 --> 01:23:19,460
OK i didn't mention briefly about for detection

1376
01:23:22,610 --> 01:23:26,600
so you made a deal with different kinds of fruits as i mentioned credit card

1377
01:23:26,600 --> 01:23:30,690
fraud some people trying to steal credit card and using music to buy something on

1378
01:23:31,540 --> 01:23:33,260
this typical and

1379
01:23:33,720 --> 01:23:36,850
it insurance claim you may claim two

1380
01:23:36,850 --> 01:23:41,490
lots of the people still something real but that's not actually the case or mobile

1381
01:23:41,490 --> 01:23:44,890
cell phone fraud fraud if you lose the cell phone in someone outside you use

1382
01:23:44,890 --> 01:23:46,250
that instead of

1383
01:23:46,310 --> 01:23:51,500
and in this case most of the time you can temporal data and you will

1384
01:23:51,510 --> 01:23:57,220
have to deal with fast and accurate real time detection so basically here data streams

1385
01:23:57,250 --> 01:23:58,810
not occurring at

1386
01:23:58,900 --> 01:24:03,540
regular uniform intervals so they occur every time a new kind transactions or something that

1387
01:24:03,540 --> 01:24:08,240
happened for especially in credit card transactions and then have to react fast because you

1388
01:24:08,280 --> 01:24:09,380
have to stop

1389
01:24:11,220 --> 01:24:12,620
fraudulent transaction

1390
01:24:12,630 --> 01:24:15,000
and to misclassification cost is very high

1391
01:24:15,020 --> 01:24:19,220
if so if you don't say something is intrusion you may actually the for example

1392
01:24:19,290 --> 01:24:22,740
company may lose a lot of money

1393
01:24:22,960 --> 01:24:27,270
in the healthcare informatics you may deal with

1394
01:24:27,350 --> 01:24:31,750
different problems for example if analyzing certain disease

1395
01:24:31,760 --> 01:24:37,280
anomalies may represent disease outbreaks for example that certain

1396
01:24:37,330 --> 01:24:39,540
the disease can happen sort of area

1397
01:24:39,630 --> 01:24:45,020
if you're dealing with for example people who have certain types of cancer

1398
01:24:45,040 --> 01:24:48,170
using anomaly detection you may

1399
01:24:48,170 --> 01:24:52,120
the take this cancer in advance or at least eight so and then you can

1400
01:24:52,120 --> 01:24:53,580
save lives

1401
01:24:53,610 --> 01:24:57,960
and usually in this case only normal data is available

1402
01:24:57,970 --> 01:24:59,990
very few instances of

1403
01:25:00,270 --> 01:25:05,810
class all of this can be available and in this case misclassification cost is very

1404
01:25:05,810 --> 01:25:10,690
high for example if you miss the fact that the cancer you lose the personal

1405
01:25:10,690 --> 01:25:12,120
life that's that's

1406
01:25:12,120 --> 01:25:13,730
really expensive and

1407
01:25:13,750 --> 01:25:17,860
the data can be quite complex depending on the problem for example in the case

1408
01:25:17,860 --> 01:25:19,290
of disease outbreak

1409
01:25:19,310 --> 01:25:22,430
they may be both spatial and temporal

1410
01:25:23,810 --> 01:25:30,710
i mentioned earlier that are extremely important problems in industrial damage detection in decision involved

1411
01:25:31,010 --> 01:25:33,510
engineering health applications

1412
01:25:33,520 --> 01:25:38,790
so many companies are starting to apply different data mining techniques

1413
01:25:38,810 --> 01:25:41,100
to help there

1414
01:25:41,140 --> 01:25:45,470
physical bayes model techniques to detect certain failures in that way

1415
01:25:45,530 --> 01:25:49,600
so they figure out that physical based models are not sufficient

1416
01:25:49,630 --> 01:25:53,310
to explain and describe the complex

1417
01:25:53,330 --> 01:25:57,940
engineering systems and then the data driven techniques may help them

1418
01:25:57,990 --> 01:26:00,730
so typical example is active

1419
01:26:00,980 --> 01:26:03,700
the safety and reserve

1420
01:26:03,740 --> 01:26:08,560
recent efforts from the mess they basically they try to use different data driven techniques

1421
01:26:08,560 --> 01:26:12,480
to detect anomalies from the flight record data so

1422
01:26:12,870 --> 01:26:17,370
the idea is to analyse the data from the flights during the flight

1423
01:26:17,380 --> 01:26:22,380
and then if anything unusual and happen you may give the signal to the pilot

1424
01:26:22,490 --> 01:26:26,060
the centre and say there is probably

1425
01:26:26,120 --> 01:26:28,690
robert full some so

1426
01:26:28,730 --> 01:26:31,630
this is also very important for aviation safety

1427
01:26:31,640 --> 01:26:34,550
and it can also save many many lives

1428
01:26:34,570 --> 01:26:39,100
however there are many challenges in his gate data since you're getting the data from

1429
01:26:39,100 --> 01:26:40,460
different types of sensors

1430
01:26:40,510 --> 01:26:44,210
they may may be extremely huge

1431
01:26:44,220 --> 01:26:45,570
can be extremely noisy

1432
01:26:45,680 --> 01:26:49,140
because you're mixing you know all different types of sensors some of some of the

1433
01:26:49,140 --> 01:26:50,870
more reliable so not

1434
01:26:50,920 --> 01:26:55,200
and most of the time the data is not available at all series you don't

1435
01:26:55,200 --> 01:26:58,160
know what is normally aut one

1436
01:26:58,240 --> 01:27:03,290
most of the applications because you're collecting this online here temporal became also have to

1437
01:27:03,290 --> 01:27:07,450
consider different sequential temporal anomaly detection techniques

1438
01:27:07,480 --> 01:27:12,760
and detecting anomalous events require usually immediate intervention

1439
01:27:12,800 --> 01:27:16,600
and again the course here is extremely high because mister take this

1440
01:27:16,640 --> 01:27:19,280
so certainly in this silly correspond to failure

1441
01:27:19,300 --> 01:27:23,630
in this case of patient safety males used hundreds of life

1442
01:27:24,940 --> 01:27:30,540
i mentioned that people tried to use recently different anomaly detection techniques in image processing

1443
01:27:30,540 --> 01:27:31,780
video surveillance

1444
01:27:32,830 --> 01:27:37,540
as you know we have more and more cameras everywhere so they called us if

1445
01:27:37,540 --> 01:27:40,360
you go to their political that's difficult to store called us

1446
01:27:40,740 --> 01:27:45,230
so people used to analyse this videos and images to detect some suspicious events and

1447
01:27:45,240 --> 01:27:48,780
to the thank you for that for example if an airport

1448
01:27:48,870 --> 01:27:53,520
people try to take this kind of some eleven package that's also very important for

1449
01:27:53,560 --> 01:27:59,240
the train stations and so on so using anomaly detection techniques can be

1450
01:27:59,290 --> 01:28:01,610
here very very important

1451
01:28:01,670 --> 01:28:04,600
and again there some key challenges

1452
01:28:04,610 --> 01:28:08,620
you have to take this collective anomalies from the previous case because you have to

1453
01:28:08,620 --> 01:28:12,920
put in the context of different pieces what can be normal what can be done

1454
01:28:12,930 --> 01:28:14,880
only in specific situations

1455
01:28:14,890 --> 01:28:20,190
and data sets can be extremely huge and you have to convert to the raw

1456
01:28:20,240 --> 01:28:23,910
image and video data into the data format is appropriate

1457
01:28:23,920 --> 01:28:28,480
to use the anomaly detection

1458
01:28:28,540 --> 01:28:31,610
OK so this is a taxonomy

1459
01:28:32,370 --> 01:28:36,750
not technically that is also published in this way our group

1460
01:28:36,770 --> 01:28:38,190
so we try to

1461
01:28:38,190 --> 01:28:44,170
and i mean by divided by the sum of all the viruses

1462
01:28:44,220 --> 01:28:49,130
if we put it should be what to not do point nine we mean that

1463
01:28:49,160 --> 01:28:53,410
we explain the ninety percent of the variance of the data by keeping all the

1464
01:28:53,410 --> 01:28:57,520
first k eigenvectors

1465
01:28:57,520 --> 01:29:03,300
PCA being used a lot for dimensionality reduction

1466
01:29:03,310 --> 01:29:09,370
but for many problems maybe not a good idea for example one two

1467
01:29:09,390 --> 01:29:15,690
two to reduce the dimensionality of our data for classification by

1468
01:29:15,720 --> 01:29:18,840
you can see from the features that if you want

1469
01:29:18,880 --> 01:29:22,770
project i would out in the in the direction

1470
01:29:22,810 --> 01:29:28,800
one of the principal component but they are not overlapped

1471
01:29:28,850 --> 01:29:30,910
if we find all

1472
01:29:30,940 --> 01:29:34,190
way to protect our for example

1473
01:29:34,210 --> 01:29:39,860
although using a linear discriminant analysis i wanted to was so the

1474
01:29:39,900 --> 01:29:40,860
so on

1475
01:29:40,870 --> 01:29:45,370
also featured not only discriminant analysis can be used

1476
01:29:45,420 --> 01:29:46,350
two o

1477
01:29:46,350 --> 01:29:51,300
other dimensionality reduction technique with this time

1478
01:29:51,320 --> 01:29:52,890
the max v v

1479
01:29:54,450 --> 01:29:55,570
vector of

1480
01:29:55,570 --> 01:30:01,510
the product of the government that i introduced before

1481
01:30:01,530 --> 01:30:03,840
it is quite

1482
01:30:03,860 --> 01:30:09,860
all this time discussion in computer vision a lot of people and we use the

1483
01:30:09,860 --> 01:30:21,320
PCA for dimensionality reduction as a preprocessing step of the things that is similar

1484
01:30:21,330 --> 01:30:22,720
it contains two

1485
01:30:22,820 --> 01:30:25,270
it's not the it's better to keep in mind

1486
01:30:25,300 --> 01:30:26,680
practical issues

1487
01:30:26,710 --> 01:30:31,600
i mean when i say that the more he becomes disconnected pieces

1488
01:30:31,650 --> 01:30:36,310
you know that alyson not well characterized so it's better to use

1489
01:30:36,320 --> 01:30:40,830
he said when the number of samples now we can get good

1490
01:30:40,840 --> 01:30:42,580
bishop of

1491
01:30:42,590 --> 01:30:49,290
i they that did not representative for the class and we can use linear discriminant

1492
01:30:49,290 --> 01:30:55,090
analysis to perform dimensionality reduction

1493
01:30:57,790 --> 01:31:00,280
what if we have two daughters

1494
01:31:00,290 --> 01:31:03,790
we want to find related direction on

1495
01:31:03,800 --> 01:31:05,630
to be that

1496
01:31:05,660 --> 01:31:10,220
and there are many applications where we need to do that like for example where

1497
01:31:10,220 --> 01:31:17,710
we want to do because language retrieval of documents and we have english and french

1498
01:31:17,850 --> 01:31:26,510
want to discover the relationship between them but we have images and captions as short

1499
01:31:26,630 --> 01:31:28,570
in the introduction

1500
01:31:28,610 --> 01:31:33,210
or for example when want to the speaker recognition and we have some i do

1501
01:31:33,220 --> 01:31:39,380
that is that the reason that for example movement

1502
01:31:40,110 --> 01:31:41,270
what we can do

1503
01:31:41,280 --> 01:31:47,180
we can use a few years of like inviting problems for example we can use

1504
01:31:47,180 --> 01:31:53,030
the principal component analysis on the joint that dataset the joint that is created

1505
01:31:53,040 --> 01:31:56,090
by the two people that is why

1506
01:31:56,100 --> 01:32:00,090
we can maximize by by fashion

1507
01:32:00,100 --> 01:32:07,350
o we can maximize the correlation between the two datasets by canonical correlation analysis

1508
01:32:08,080 --> 01:32:16,670
the same problem of principal component analysis that we saw before but i plug

1509
01:32:17,160 --> 01:32:18,210
or they

1510
01:32:18,320 --> 01:32:24,030
together or x and y is for datasets

1511
01:32:24,030 --> 01:32:24,990
and so

1512
01:32:25,000 --> 01:32:31,910
we have exactly the same problem as before the metric the objective function is made

1513
01:32:31,920 --> 01:32:36,670
by covariance matrices and the cross covariance matrices

1514
01:32:36,690 --> 01:32:39,760
all of the other

1515
01:32:40,060 --> 01:32:45,460
we want to compute the optimal projection wx and WY in order to maximize the

1516
01:32:45,460 --> 01:32:49,070
value of the joint the set

1517
01:32:49,350 --> 01:32:54,420
we can do the same thing you computing the ground but in the end you

1518
01:32:54,420 --> 01:32:59,290
get that again via problem exactly as before

1519
01:32:59,310 --> 01:33:01,850
and so it is given by the problem

1520
01:33:01,860 --> 01:33:08,780
the good news is that this article is essentially independent of it is been applied

1521
01:33:08,790 --> 01:33:14,370
the technique could be to maximize the covariance between projections

1522
01:33:14,390 --> 01:33:16,980
in this case we can solve the issue

1523
01:33:18,660 --> 01:33:19,900
if we think

1524
01:33:19,920 --> 01:33:24,660
this can can also see that this optimisation problem

1525
01:33:24,680 --> 01:33:26,860
up and down

1526
01:33:26,880 --> 01:33:32,780
and even they we've learned we mean that the solution to the same up to

1527
01:33:32,780 --> 01:33:35,610
to constant factors

1528
01:33:35,660 --> 01:33:38,760
it's also we went to the following problem and maybe

1529
01:33:38,800 --> 01:33:40,080
more he easy to

1530
01:33:40,100 --> 01:33:46,050
so to see all to sort of computer programs and again we

1531
01:33:46,050 --> 01:33:51,320
so what is a good idea and the have wx and WY equal to zero

1532
01:33:51,340 --> 01:33:57,680
and what we get is again and again by the problem

1533
01:33:57,700 --> 01:34:01,760
the last thing about the problem is canonical correlation analysis

1534
01:34:01,770 --> 01:34:08,510
we are maximising the correlation assuming the variance is irrelevant to cfd

1535
01:34:08,550 --> 01:34:11,250
what does it mean that time

1536
01:34:11,250 --> 01:34:11,810
an answer

1537
01:34:12,450 --> 01:34:17,350
give you an answer in the limit of so the complexity here is well technically

1538
01:34:17,350 --> 01:34:19,310
speaking after run this are going on forever

1539
01:34:20,940 --> 01:34:26,570
but you know in practice we do everything the finite precision arithmetic anyway so even

1540
01:34:26,600 --> 01:34:31,110
the most beautiful algorithms that converge in a finite number of iterations are subject a

1541
01:34:31,110 --> 01:34:36,540
small errors themselves and so to be honest when when you get within some small

1542
01:34:36,540 --> 01:34:40,360
abseil on the right answer he should be allowed to shut this down and say

1543
01:34:40,650 --> 01:34:42,960
i know there are and there would be just as good

1544
01:34:44,070 --> 01:34:45,210
but it's hard to know

1545
01:34:45,770 --> 01:34:47,990
how many iterations this you need to do

1546
01:34:48,620 --> 01:34:53,750
guarantee that you within some given a priority estimate epsilon

1547
01:34:54,800 --> 01:34:57,450
so that's the upside and the downside it's easy

1548
01:34:58,230 --> 01:35:02,190
each step is a regression problem you're just do this is called

1549
01:35:02,760 --> 01:35:09,720
iteratively reweighted least squares because each of these calculations is solving a least square weighted

1550
01:35:09,720 --> 01:35:13,500
least squares problem where the weights depend on your current estimate

1551
01:35:14,890 --> 01:35:16,890
and they just keep reweighting as you go

1552
01:35:17,580 --> 01:35:20,320
and you can prove that that this is convergent

1553
01:35:20,970 --> 01:35:21,910
fairly generally

1554
01:35:22,560 --> 01:35:25,730
but it's not linear programming and so i won't dwell on this algorithm

1555
01:35:26,170 --> 01:35:28,210
i'd rather go to the linear programming over them

1556
01:35:30,150 --> 01:35:30,430
all right

1557
01:35:31,350 --> 01:35:33,750
how do we model this as a linear programming problem

1558
01:35:34,220 --> 01:35:41,110
well just like the one before with are with structure and the hang weights wi absolute values here

1559
01:35:41,810 --> 01:35:45,400
end i has do a slightly different track but the

1560
01:35:45,860 --> 01:35:50,300
the takeaway messages whenever absolute values you should say yourself

1561
01:35:50,500 --> 01:35:52,940
i can probably make this a linear programming problem

1562
01:35:54,750 --> 01:35:58,200
here's the trick here is similar but different in the details

1563
01:36:00,170 --> 01:36:01,230
if you look at this thing

1564
01:36:01,900 --> 01:36:03,440
and i started calculus

1565
01:36:04,190 --> 01:36:05,040
end i would

1566
01:36:05,590 --> 01:36:06,800
teach methods and integration

1567
01:36:07,650 --> 01:36:10,670
and one amendments methods integration over time my students

1568
01:36:13,820 --> 01:36:15,370
use the greedy substitution

1569
01:36:15,790 --> 01:36:17,150
which means by which i meant

1570
01:36:18,060 --> 01:36:22,600
take the only is that you can find in the problem and give it a new variable name

1571
01:36:23,650 --> 01:36:26,910
and substituted out so and that's exactly what we do here

1572
01:36:27,810 --> 01:36:30,060
the analyst thing for us is this

1573
01:36:30,570 --> 01:36:32,210
the absolute values of this thing

1574
01:36:32,760 --> 01:36:33,710
so give it a name

1575
01:36:34,410 --> 01:36:38,010
we have a bunch these absolute values there indexed by and so a new variable

1576
01:36:38,010 --> 01:36:39,960
tiai i for everyone is absolute values

1577
01:36:41,250 --> 01:36:44,310
now i would like to say that he i is equal to

1578
01:36:44,770 --> 01:36:47,050
be absolute value of this difference

1579
01:36:49,170 --> 01:36:49,960
that's the truth

1580
01:36:50,910 --> 01:36:53,510
ti i should be equal to the set but now i'm going

1581
01:36:55,310 --> 01:36:57,870
be a little bit clever like we were a little bit clever

1582
01:36:58,410 --> 01:37:00,040
before with the absolute values

1583
01:37:00,650 --> 01:37:01,830
end say okay

1584
01:37:02,240 --> 01:37:04,850
that's would be good to write equals absolute value because they

1585
01:37:05,460 --> 01:37:12,620
haven't made things better but how about if i say look if ti i is greater than the absolute value

1586
01:37:13,410 --> 01:37:16,690
if ti i is bigger than or equal to the absolute value than ti i

1587
01:37:16,690 --> 01:37:18,190
would be bigger than this expression

1588
01:37:19,730 --> 01:37:22,770
negativity i would be less than this expression because

1589
01:37:23,830 --> 01:37:25,830
we want squeeze on both sides

1590
01:37:26,310 --> 01:37:29,590
and there will be true if t e i is bigger than the absolute value

1591
01:37:34,540 --> 01:37:36,320
the relaxation is okay

1592
01:37:36,880 --> 01:37:39,260
because i'm trying to minimize the sum that he is

1593
01:37:41,440 --> 01:37:42,800
and so i'll never leave

1594
01:37:44,300 --> 01:37:47,830
slack again it's the same idea never leaves slack

1595
01:37:48,210 --> 01:37:49,730
in both of these inequalities

1596
01:37:50,210 --> 01:37:54,510
if i didn't have slackened bottles inequalities that ti i wouldn't be the absolute value

1597
01:37:54,510 --> 01:37:58,590
would be bigger than the absolute value of poland and tight in every case

1598
01:37:59,110 --> 01:38:02,750
in cases where this is positive i wanna pull it tight on the right in

1599
01:38:02,750 --> 01:38:07,710
cases where this is negative and important tight on the left and leave no slack

1600
01:38:07,840 --> 01:38:09,380
on one of the two sides

1601
01:38:10,150 --> 01:38:15,410
and because i minimizing i will never leave slack on both sides and therefore

1602
01:38:15,910 --> 01:38:19,040
after mality eyes will be the absolute values

1603
01:38:20,620 --> 01:38:22,970
so it's essentially the same trick i showed you before r

1604
01:38:24,610 --> 01:38:26,090
and that's a linear programming problem

1605
01:38:26,570 --> 01:38:27,620
we can solve it

1606
01:38:29,340 --> 01:38:30,510
the simplex methods one

1607
01:38:32,970 --> 01:38:36,240
here's now here's another example model why not just spend a few minutes on it

1608
01:38:36,720 --> 01:38:38,360
this is the informalformal you

1609
01:38:39,240 --> 01:38:42,000
we least absolute deviations regression

1610
01:38:43,490 --> 01:38:48,390
so we've got to parameters and mendoza the site does give us the size of the matrix so

1611
01:38:48,850 --> 01:38:52,230
i've got a set of i have indices by indices

1612
01:38:52,800 --> 01:38:53,820
i don't have to write it this way

1613
01:38:55,100 --> 01:38:55,670
but anyway

1614
01:38:56,130 --> 01:38:58,620
i goes from one them jay goes from one then

1615
01:38:59,390 --> 01:38:59,810
and then

1616
01:39:00,270 --> 01:39:06,000
then they matrix is that is a matrix whose two-dimensional so indexed by isin jay-z i come jay

1617
01:39:07,410 --> 01:39:08,040
data be

1618
01:39:08,040 --> 01:39:10,700
the pages of canine hurt

1619
01:39:11,020 --> 01:39:16,140
which were registered together to compensate for the loss of the difference in the different

1620
01:39:17,190 --> 01:39:20,970
and the goal here was to try to investigate what is the viability

1621
01:39:21,020 --> 01:39:23,270
of the fiber orientation

1622
01:39:23,310 --> 01:39:27,770
so the main the first eigenvalue of the covariance matrix of the times

1623
01:39:27,790 --> 01:39:30,380
here's the orientation and the second order

1624
01:39:30,390 --> 01:39:35,570
the second eigenvector is related to the sheet in the structure

1625
01:39:35,600 --> 01:39:40,010
and this is this is the result of the statistical analysis and basically

1626
01:39:40,050 --> 01:39:41,400
so using

1627
01:39:41,530 --> 01:39:46,400
the fine environment and what we can show that basically this is a

1628
01:39:46,460 --> 01:39:51,560
the variance is very low almost everywhere which means that the the structure of the

1629
01:39:51,560 --> 01:39:52,810
poem is

1630
01:39:54,640 --> 01:39:56,510
in this case

1631
01:39:56,510 --> 01:39:57,750
so this means that

1632
01:39:57,860 --> 01:40:01,350
we can use such among the mean

1633
01:40:01,800 --> 01:40:07,210
and so as a model to simulate you have

1634
01:40:07,210 --> 01:40:08,280
a second

1635
01:40:08,530 --> 01:40:12,350
example of statistics on on an atomic objects

1636
01:40:12,470 --> 01:40:17,030
is given with the work of that on on

1637
01:40:18,260 --> 01:40:20,250
spines and it's basically

1638
01:40:20,290 --> 01:40:24,690
the idea is to study the formation of scoring it explains

1639
01:40:24,710 --> 01:40:28,650
so what we did is modeled the spine is an articulated object and so the

1640
01:40:28,650 --> 01:40:33,350
degrees of freedom of the pose related portals of successive that address

1641
01:40:33,370 --> 01:40:34,560
and if we are doing

1642
01:40:34,570 --> 01:40:39,990
statistical analyses computing the mean and the currents metrics on the whole

1643
01:40:40,220 --> 01:40:42,800
articulated model which is a

1644
01:40:42,810 --> 01:40:48,630
kind of factor of seventeen rigid body transformations we can extract the for for small

1645
01:40:48,630 --> 01:40:52,900
the variation and we can see here the full for small from

1646
01:40:52,920 --> 01:40:55,260
in front of you and that's you

1647
01:40:55,260 --> 01:40:57,440
and what is interesting is that the full

1648
01:40:57,450 --> 01:41:00,810
we were able to show that the full first small are related with the clinical

1649
01:41:02,310 --> 01:41:03,710
so it's a case where

1650
01:41:03,710 --> 01:41:06,140
using the right model

1651
01:41:06,160 --> 01:41:12,500
two tomorrow to which is which is geometric which is intrinsically in romanian manifold is

1652
01:41:12,500 --> 01:41:14,410
giving the right way to model

1653
01:41:14,420 --> 01:41:17,040
the and that

1654
01:41:17,050 --> 01:41:21,600
OK so let's go to the second part of the top which is now how

1655
01:41:21,620 --> 01:41:22,950
to work with

1656
01:41:22,990 --> 01:41:25,100
manifold valued images

1657
01:41:25,120 --> 01:41:29,420
but ground with with example so images

1658
01:41:29,440 --> 01:41:30,970
so we have seen

1659
01:41:30,980 --> 01:41:36,050
the geodesic interpolation so the interpolation between two values

1660
01:41:36,060 --> 01:41:40,530
now what we need to work on images is by no it right now

1661
01:41:40,530 --> 01:41:42,490
sinc interpolation

1662
01:41:42,510 --> 01:41:44,850
so how can we do that

1663
01:41:44,910 --> 01:41:47,350
well what we can do is

1664
01:41:47,360 --> 01:41:49,730
rephrased the

1665
01:41:49,740 --> 01:41:54,550
interpolation in terms of weighted means and this is usually what is done i actually

1666
01:41:54,560 --> 01:41:56,250
by no interpretation

1667
01:41:56,990 --> 01:42:00,660
the way to the well the the mean of the full

1668
01:42:00,690 --> 01:42:06,490
the value of the different colors weighted by the training they all all by weights

1669
01:42:06,530 --> 01:42:11,230
so if we are doing that the value of each at each point here is

1670
01:42:11,230 --> 01:42:14,800
obtained by the minimisation of this function

1671
01:42:14,810 --> 01:42:17,050
one thing that i did said

1672
01:42:17,450 --> 01:42:19,040
and this is too uncertain

1673
01:42:19,050 --> 01:42:24,250
thirty two but was concerned about the exact closed form

1674
01:42:24,300 --> 01:42:25,900
is that we have a very

1675
01:42:25,920 --> 01:42:30,840
efficient algorithm to compute the mean in any type of money manifold which is a

1676
01:42:31,830 --> 01:42:33,460
goes into

1677
01:42:34,560 --> 01:42:38,920
and which is usually converging in five to ten iteration to the numerical accuracy of

1678
01:42:38,920 --> 01:42:43,560
the machine so this is not something which is

1679
01:42:43,570 --> 01:42:46,280
i mean it's taking more time than the closed form

1680
01:42:46,310 --> 01:42:47,790
but not so much

1681
01:42:47,810 --> 01:42:50,410
so we can compute weighted means

1682
01:42:50,880 --> 01:42:55,650
the cost of ten times let's say ten iterations at each point and this is

1683
01:42:55,650 --> 01:42:56,850
the example

1684
01:42:56,940 --> 01:43:02,630
interoperation between these four concerts is euclidean interpolation between the coefficients

1685
01:43:02,650 --> 01:43:07,600
and this is the using the fine environment

1686
01:43:07,610 --> 01:43:09,090
so how do know

1687
01:43:09,100 --> 01:43:10,960
that we have a weighted mean

1688
01:43:10,960 --> 01:43:16,060
well we can try to use that tool to generalise to other types of algorithms

1689
01:43:16,120 --> 01:43:19,070
like point of course motion forcing

1690
01:43:19,130 --> 01:43:20,900
which is nothing that

1691
01:43:21,070 --> 01:43:22,310
a weighted mean

1692
01:43:22,360 --> 01:43:24,020
with caution weight

1693
01:43:24,020 --> 01:43:25,990
so this is the way we can do

1694
01:43:27,040 --> 01:43:30,550
gulshan missing this is this type of thing

1695
01:43:30,630 --> 01:43:34,730
we can go also for harmonic regularisation so in that case there is something that

1696
01:43:34,730 --> 01:43:36,040
happens at that

1697
01:43:36,050 --> 01:43:37,970
since we are working on the manifold

1698
01:43:39,750 --> 01:43:41,970
one derivative of this

1699
01:43:41,990 --> 01:43:48,020
the tree is the manifold laplacian which takes into account the curvature of the manifold

1700
01:43:48,030 --> 01:43:53,410
but there is one very interesting things in the exponential map remember i said this

1701
01:43:53,410 --> 01:43:56,360
expansion that is the basic tool

1702
01:43:56,440 --> 01:43:58,670
it is actually well

1703
01:43:59,340 --> 01:44:01,040
curvature geodesics

1704
01:44:01,080 --> 01:44:04,090
along the geodesics in that direction is new

1705
01:44:04,140 --> 01:44:05,260
so this is the

1706
01:44:05,860 --> 01:44:07,490
thanks to this property

1707
01:44:07,490 --> 01:44:10,540
this numerical scheme which is basically

1708
01:44:10,550 --> 01:44:12,290
how to compute the

1709
01:44:12,310 --> 01:44:14,410
one of the laplacian in the

1710
01:44:14,450 --> 01:44:19,900
in the exponential map is intrinsically taking into account the curvature of the money

1711
01:44:19,960 --> 01:44:23,460
which means that we have a numerical scheme which is very simple

1712
01:44:23,500 --> 01:44:24,950
to compute

1713
01:44:25,100 --> 01:44:27,810
the derivative of the child and we can

1714
01:44:27,810 --> 01:44:29,270
try to make it more

1715
01:44:29,270 --> 01:44:31,270
with the gradient descent

1716
01:44:31,320 --> 01:44:33,160
and this can be

1717
01:44:33,170 --> 01:44:36,010
then we generalised to anisotropic filtering

1718
01:44:36,050 --> 01:44:37,540
so here's an example

1719
01:44:37,690 --> 01:44:42,240
this is the raw image so this is around the particles so here you can

1720
01:44:42,240 --> 01:44:43,370
see that there are

1721
01:44:43,650 --> 01:44:45,650
a set of tensors which are

1722
01:44:47,940 --> 01:44:52,410
very well actually there is attract going around

1723
01:44:52,460 --> 01:44:56,700
so but there is also some some big errors some point so we want to

1724
01:44:56,700 --> 01:44:58,230
regularize that

1725
01:44:58,280 --> 01:45:04,790
if we take gulshan schmoozing then there are is there is some

1726
01:45:05,940 --> 01:45:07,830
fx which is not very one

1727
01:45:07,860 --> 01:45:11,100
what we won the big ten cells are taking over the small ones

1728
01:45:11,110 --> 01:45:15,940
before using the cautions was singing in the romanian setting then it's already better and

1729
01:45:15,940 --> 01:45:18,960
if we're taking the anisotropic filtering

1730
01:45:18,980 --> 01:45:20,550
then we can consider

1731
01:45:20,560 --> 01:45:23,350
the boundaries between different types of

1732
01:45:23,560 --> 01:45:28,550
regions a particular we can conserve some very fine details here

1733
01:45:28,550 --> 01:45:30,670
in the

1734
01:45:30,690 --> 01:45:32,440
the image

1735
01:45:32,450 --> 01:45:34,310
so this is

1736
01:45:34,360 --> 01:45:38,030
to show that we can utilize many different algorithms

1737
01:45:38,040 --> 01:45:41,210
on manifold well the the ideas

1738
01:45:41,260 --> 01:45:45,810
we also need to estimate the time since we just i mean the estimation of

1739
01:45:45,810 --> 01:45:50,040
what will happen to your covariance matrix

1740
01:45:52,220 --> 01:45:53,760
if the data

1741
01:45:53,830 --> 01:45:55,980
lives in the manifold

1742
01:45:56,020 --> 01:45:59,960
inside your space so you have the ten thousand dimensional space OK

1743
01:46:01,190 --> 01:46:02,120
but actually

1744
01:46:02,150 --> 01:46:05,090
the data for has the shape of some sort of

1745
01:46:05,110 --> 01:46:10,280
flying so span whatever it is it really only spans

1746
01:46:10,290 --> 01:46:13,040
a submanifold of dimension

1747
01:46:13,050 --> 01:46:16,920
twenty it's a what happens to your covariance matrix when you try to feed goes

1748
01:46:16,920 --> 01:46:19,000
into that

1749
01:46:19,010 --> 01:46:23,380
so the covariance matrix of size one thousand times a thousand right

1750
01:46:23,440 --> 01:46:27,790
now what happens to it

1751
01:46:27,820 --> 01:46:29,610
is offering

1752
01:46:29,660 --> 01:46:31,280
so what is it going to have

1753
01:46:32,100 --> 01:46:33,050
ranked twenty

1754
01:46:33,060 --> 01:46:35,680
now can you invert that things

1755
01:46:35,690 --> 01:46:38,170
cameron invertebrates

1756
01:46:38,190 --> 01:46:44,070
OK so what factor analysis does is it finds a way around of parliament rising

1757
01:46:44,130 --> 01:46:46,750
covariance matrices in a clever way

1758
01:46:46,760 --> 01:46:50,170
OK and then you can still throw girls and it

1759
01:46:50,210 --> 01:46:53,060
so how do we do it

1760
01:46:53,210 --> 01:46:54,520
this is how it sounds

1761
01:46:55,300 --> 01:46:58,430
the data we observe so now we need to be a little bit careful about

1762
01:46:58,430 --> 01:47:03,000
the x and y is not really matters so it's otherwise this is data space

1763
01:47:04,340 --> 01:47:07,260
this is your to capture the now is going to be

1764
01:47:07,310 --> 01:47:08,570
a thousand OK

1765
01:47:08,690 --> 01:47:10,520
and and twenty

1766
01:47:10,550 --> 01:47:12,060
k is going to be twenty

1767
01:47:12,070 --> 01:47:16,580
so captain k is really the size of the dimension of the space that is

1768
01:47:17,960 --> 01:47:22,130
the that sort of contains the data OK is your little pancake living in this

1769
01:47:22,130 --> 01:47:23,250
big space

1770
01:47:23,260 --> 01:47:24,850
OK so now

1771
01:47:24,860 --> 01:47:26,330
what you do is

1772
01:47:26,340 --> 01:47:30,350
you pose before only the following generative model you sort of say

1773
01:47:31,610 --> 01:47:32,440
the way

1774
01:47:32,440 --> 01:47:34,750
the data was generated actually

1775
01:47:34,760 --> 01:47:38,520
it is i had a thousand linear models

1776
01:47:38,560 --> 01:47:39,860
each of them

1777
01:47:41,300 --> 01:47:46,760
the inputs that live in a much smaller space of dimension only twenty

1778
01:47:47,680 --> 01:47:48,670
and as a

1779
01:47:48,690 --> 01:47:49,670
and actually

1780
01:47:49,680 --> 01:47:55,010
i'm also going to impose that this sources or or factors if you will

1781
01:47:55,030 --> 01:47:56,300
hence the name

1782
01:47:56,350 --> 01:48:00,230
this twenty factors they are independent

1783
01:48:00,580 --> 01:48:07,300
factors OK is zero mean and variance one

1784
01:48:07,320 --> 01:48:08,530
and now

1785
01:48:08,540 --> 01:48:09,980
what i do is

1786
01:48:09,980 --> 01:48:11,920
i take

1787
01:48:11,930 --> 01:48:13,010
let let d

1788
01:48:15,070 --> 01:48:18,400
just pick one of the thousand dimensions let's speak dimension

1789
01:48:18,440 --> 01:48:19,330
forty three

1790
01:48:19,590 --> 01:48:23,750
you can sort of see that i have a vector that is of size

1791
01:48:23,760 --> 01:48:28,740
twenty is the weight vector that that multiplies all of the sources and spits out

1792
01:48:28,740 --> 01:48:34,250
the value of the of dimension forty three my high dimensional space OK so have

1793
01:48:34,290 --> 01:48:38,000
this thousand this battery of of linear models

1794
01:48:38,740 --> 01:48:41,740
i remember is about how the graph where

1795
01:48:41,790 --> 01:48:45,780
weather data would appear here again who was like an autoencoder with with the bottleneck

1796
01:48:45,780 --> 01:48:49,310
and so there's been a beautiful stuff done in neural nets

1797
01:48:49,360 --> 01:48:54,170
many years ago in this in this sense here here we just take

1798
01:48:54,200 --> 01:48:57,350
the bit that that sort of generous the data OK

1799
01:48:57,370 --> 01:49:00,090
so you can see where the compression is going to come from

1800
01:49:00,100 --> 01:49:02,010
the compression comes from the fact

1801
01:49:02,040 --> 01:49:05,500
but i don't think there are so many degrees of freedom out their generating my

1802
01:49:06,310 --> 01:49:11,010
my data might look as if it's a thousand dimensional but actually there was only

1803
01:49:12,010 --> 01:49:14,590
random sources generating the data

1804
01:49:23,200 --> 01:49:26,610
one very cool thing about factorizes is that

1805
01:49:26,700 --> 01:49:29,700
it also says that

1806
01:49:30,110 --> 01:49:35,130
it also allows for modeling independent noise OK this one is really independent all the

1807
01:49:38,020 --> 01:49:41,450
so i have a little little random number here

1808
01:49:41,460 --> 01:49:42,860
and that's going to be

1809
01:49:44,030 --> 01:49:47,970
random noise so this this matrix

1810
01:49:50,340 --> 01:49:52,560
has size

1811
01:49:52,580 --> 01:49:56,760
a thousand times a thousand and its diagonal OK because all these noise terms are

1812
01:49:58,000 --> 01:50:02,130
all right so basically what i'm saying is

1813
01:50:02,170 --> 01:50:04,480
but actually

1814
01:50:04,500 --> 01:50:07,690
module on noise

1815
01:50:07,750 --> 01:50:13,190
my data lives only in a twenty dimensional space but actually also allowing for noise

1816
01:50:13,210 --> 01:50:14,050
all right

1817
01:50:14,050 --> 01:50:18,380
so it might seem might seem as it might even seem as as if the

1818
01:50:18,380 --> 01:50:21,210
data was actually feeling the space

1819
01:50:21,250 --> 01:50:25,150
but actually there might only be twenty sources of correlation

1820
01:50:25,170 --> 01:50:27,300
and the correlations that i'm interested in

1821
01:50:27,340 --> 01:50:29,480
i'm not so interested in noise

1822
01:50:29,520 --> 01:50:30,900
and this afternoon

1823
01:50:30,960 --> 01:50:33,610
will do an example with the

1824
01:50:33,630 --> 01:50:36,690
with the mnist digits

1825
01:50:36,820 --> 01:50:42,050
where will take we compute principal components and then we'll start to add noise and

1826
01:50:42,050 --> 01:50:46,170
see what happens to PCA if you start adding a lot of noise in particular

1827
01:50:46,210 --> 01:50:49,090
you can think about it over lunch what happens

1828
01:50:49,090 --> 01:50:53,480
PCA if i just take one of the pixels of the images and i had

1829
01:50:53,500 --> 01:50:54,980
tons of noise to it

1830
01:50:55,020 --> 01:50:57,900
OK just a single pixel and then to compute PCA

1831
01:50:57,920 --> 01:51:02,000
and for some reason i only want to keep the two principal components so one

1832
01:51:02,000 --> 01:51:04,210
of them is going to be a bit screwed right

1833
01:51:04,210 --> 01:51:07,070
and and factor analysis is gonna is going to be

1834
01:51:07,090 --> 01:51:10,480
perfectly resistant to that

1835
01:51:10,500 --> 01:51:14,340
so how how does my model look like now OK

1836
01:51:14,360 --> 01:51:26,130
i think everything is equations in next slide so we can safely move on to

1837
01:51:27,030 --> 01:51:29,920
let's let's try to try to understand how

1838
01:51:29,940 --> 01:51:34,630
this factory capital a little bit

1839
01:51:34,780 --> 01:51:37,360
we have this linear model so now

1840
01:51:37,360 --> 01:51:39,420
now i just used

1841
01:51:39,440 --> 01:51:41,250
matrices and vectors

1842
01:51:41,260 --> 01:51:45,480
because it's more convenient OK

1843
01:51:45,500 --> 01:51:49,250
we have all this collection of vectors that live in the big

1844
01:51:49,300 --> 01:51:51,590
captain d dimensional space we have

1845
01:51:52,460 --> 01:51:54,590
factors are sources that leaving the

1846
01:51:54,610 --> 01:51:57,610
capital k dimensional space which is smaller than

1847
01:51:57,630 --> 01:51:59,840
and the

1848
01:51:59,860 --> 01:52:00,690
i take

1849
01:52:00,710 --> 01:52:05,690
and zero one

1850
01:52:05,710 --> 01:52:08,630
priors of the axis so now

1851
01:52:08,670 --> 01:52:09,920
i guess

1852
01:52:09,940 --> 01:52:13,520
actually it's a pity that i'm showing this equation i would like to ask you

1853
01:52:13,520 --> 01:52:15,760
to compute the covariance of y

1854
01:52:15,780 --> 01:52:17,530
but since we did earlier

1855
01:52:17,550 --> 01:52:20,670
now we all know how to do it OK

1856
01:52:20,690 --> 01:52:24,500
you can sort of see that if i the the quantity that i'm really interested

1857
01:52:24,500 --> 01:52:29,690
in is what what distribution have i now defined over the data right

1858
01:52:29,750 --> 01:52:33,320
and you can see that you could you could compute it straight from here right

1859
01:52:34,400 --> 01:52:37,610
we know we have prior x and we have a prior over

1860
01:52:37,650 --> 01:52:40,960
the noise term and they both have zero mean so the mean is going to

1861
01:52:40,960 --> 01:52:45,360
be zero for that reason and i only have to compute the covariance but now

1862
01:52:45,360 --> 01:52:50,190
you all know how to compute covariances i take expectation of y times its transpose

1863
01:52:50,840 --> 01:52:52,940
and that may be and that gives me

1864
01:52:54,020 --> 01:52:56,940
the covariance matrix so you can now see

1865
01:52:57,820 --> 01:53:01,340
we're still going to use the multivariate gaussian to model our data but it just

1866
01:53:01,340 --> 01:53:09,780
has more interesting covariance matrix

1867
01:53:09,780 --> 01:53:15,210
if you have any questions comments or anything

1868
01:53:17,190 --> 01:53:20,250
one more

1869
01:53:20,380 --> 01:53:27,250
it's it's an interesting covariance matrix because it's gonna concentrate on modeling a number of

