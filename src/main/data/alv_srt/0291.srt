1
00:00:00,000 --> 00:00:03,730
you know

2
00:00:10,880 --> 00:00:14,070
what we want

3
00:00:36,840 --> 00:00:40,530
so four

4
00:00:44,120 --> 00:00:45,230
i would say

5
00:00:45,250 --> 00:00:47,270
there was

6
00:00:51,310 --> 00:00:56,150
these three stories

7
00:01:08,250 --> 00:01:13,400
so that

8
00:01:13,500 --> 00:01:22,070
that is in a

9
00:01:24,690 --> 00:01:31,020
one of

10
00:01:44,020 --> 00:01:47,820
next the same

11
00:01:47,830 --> 00:01:54,520
i will

12
00:02:20,370 --> 00:02:28,270
so said

13
00:02:40,240 --> 00:02:53,660
and the idea is to give you

14
00:02:53,660 --> 00:02:58,600
then there's the question you're you might believe that among the sources there there are

15
00:02:58,600 --> 00:03:02,180
sources you know because there are people around me this music or something like that

16
00:03:02,510 --> 00:03:05,510
you know that there's a couple of sources and one of the sources might be

17
00:03:05,510 --> 00:03:10,140
interesting others might be less interesting to you but at least you want to demix

18
00:03:10,140 --> 00:03:11,180
the signal

19
00:03:11,510 --> 00:03:16,970
and the thing that you know is well the idea that you have is independent

20
00:03:17,010 --> 00:03:22,780
so you think that actually has the sources are statistically independent from each other and

21
00:03:22,780 --> 00:03:26,060
that's what you're looking for you looking for independence

22
00:03:26,080 --> 00:03:29,470
and well you need a little bit more information you have to have an idea

23
00:03:29,470 --> 00:03:30,410
that them

24
00:03:30,410 --> 00:03:36,140
roughly the idea what the statistics of that sources might be references for speech signals

25
00:03:36,530 --> 00:03:37,760
you would think

26
00:03:37,810 --> 00:03:42,180
well maybe the source distribution is a bit heavy tailed

27
00:03:42,450 --> 00:03:44,660
and so you could put this into the model

28
00:03:44,850 --> 00:03:46,910
so let's look at the model

29
00:03:47,010 --> 00:03:48,490
what you would do

30
00:03:48,490 --> 00:03:52,580
the generative model for that is used in ICA

31
00:03:52,620 --> 00:03:56,390
is the following you would say x

32
00:03:56,410 --> 00:03:58,510
they have different times t

33
00:03:58,530 --> 00:04:00,830
and you have the vector x

34
00:04:00,850 --> 00:04:06,580
in this axis is has is d dimensional vector of d is the number of

35
00:04:06,580 --> 00:04:09,560
what we might call it sensors

36
00:04:09,830 --> 00:04:12,780
this is what you observe

37
00:04:12,810 --> 00:04:14,390
that's all

38
00:04:14,410 --> 00:04:16,470
they have to model something

39
00:04:16,470 --> 00:04:18,510
you believe that

40
00:04:18,550 --> 00:04:22,740
these observed signals are

41
00:04:22,760 --> 00:04:26,280
a mixture well linear mixture

42
00:04:26,970 --> 00:04:29,830
so called source signals

43
00:04:29,850 --> 00:04:32,450
and we don't know what a is

44
00:04:32,450 --> 00:04:35,030
and you don't know what the s are and maybe

45
00:04:35,050 --> 00:04:38,640
so the there's always something that doesn't fit into your model you want to put

46
00:04:38,640 --> 00:04:39,930
into noise

47
00:04:40,030 --> 00:04:42,060
that would be nice

48
00:04:43,180 --> 00:04:47,720
we try to estimate a try to estimate this because that's what you're interested in

49
00:04:47,990 --> 00:04:50,410
at the end and

50
00:04:50,430 --> 00:04:53,390
well let's say if noise would be

51
00:04:53,390 --> 00:04:55,370
would be gone

52
00:04:55,390 --> 00:04:57,720
which is not in practice probably

53
00:04:57,740 --> 00:04:59,910
then you could sort of try

54
00:04:59,930 --> 00:05:04,740
and also if the numbers of sensors is equal to the number of sources they

55
00:05:04,740 --> 00:05:06,490
could say well if i know

56
00:05:06,560 --> 00:05:10,580
a that i can actually invert a

57
00:05:10,580 --> 00:05:13,560
and put to the other side and then estimate s

58
00:05:13,580 --> 00:05:17,060
so this would be sort of the easiest case no noise

59
00:05:17,310 --> 00:05:18,780
there's a little bit of

60
00:05:18,790 --> 00:05:24,560
ambiguity is of course the permutation of sources

61
00:05:24,660 --> 00:05:28,310
because if you have some some a i mean if you have sources one and

62
00:05:28,310 --> 00:05:29,870
you're just sort of

63
00:05:30,140 --> 00:05:32,330
bermuda was source to it doesn't

64
00:05:32,370 --> 00:05:38,810
and have also permute the corresponding elements in the matrix a it doesn't change

65
00:05:39,240 --> 00:05:43,640
right so this is sort of the generative model we can

66
00:05:43,640 --> 00:05:49,200
well because the basic ingredient is that all the components of s are assumed to

67
00:05:49,200 --> 00:05:56,280
be statistically independent there's a couple of different schools and approaches some people wouldn't

68
00:05:56,330 --> 00:06:00,010
so to do it the way i explain it here there would also in some

69
00:06:00,010 --> 00:06:01,330
cases you have to do other things

70
00:06:01,720 --> 00:06:06,640
you would look at the temporal structure of the signal

71
00:06:08,490 --> 00:06:11,620
the very basic model completely ignores hold

72
00:06:11,640 --> 00:06:16,470
the temporal structure of the model so they don't look at the way you know

73
00:06:16,490 --> 00:06:22,310
how speech or whatever signals are correlated they just look only at the

74
00:06:22,410 --> 00:06:28,990
statistics the overall marginal statistics of the source so actually is model is not true

75
00:06:28,990 --> 00:06:31,830
but probably sufficient for doing something

76
00:06:32,930 --> 00:06:34,760
a bit more into

77
00:06:34,810 --> 00:06:37,780
the details were this a couple of

78
00:06:37,930 --> 00:06:44,030
o thing you could do for it with maybe i'll just say a few words

79
00:06:44,180 --> 00:06:46,950
sort of a list of ways of

80
00:06:47,720 --> 00:06:49,180
it is doing

81
00:06:49,220 --> 00:06:51,200
i see a

82
00:06:51,220 --> 00:06:53,600
different interpretations actually

83
00:06:53,620 --> 00:06:55,280
this could be

84
00:06:55,330 --> 00:06:59,470
sources and sensors

85
00:06:59,510 --> 00:07:01,120
but you could also in

86
00:07:03,830 --> 00:07:07,600
for images

87
00:07:07,600 --> 00:07:09,760
one way of

88
00:07:09,780 --> 00:07:11,220
doing that

89
00:07:11,260 --> 00:07:12,780
would be to say

90
00:07:16,310 --> 00:07:18,600
if i write this in

91
00:07:19,520 --> 00:07:22,910
components i have some over j

92
00:07:22,950 --> 00:07:24,970
eight i j

93
00:07:24,990 --> 00:07:26,260
s j

94
00:07:27,310 --> 00:07:30,530
in let's forget about the noise four seconds

95
00:07:31,370 --> 00:07:33,180
so we say for each t

96
00:07:33,200 --> 00:07:34,870
this relation holds

97
00:07:36,330 --> 00:07:39,050
well i mean this these are just names

98
00:07:39,160 --> 00:07:46,890
we could also look at the following just right it's slightly different a

99
00:07:46,910 --> 00:07:50,580
t in jail to see immediately what i mean

100
00:07:51,510 --> 00:07:52,660
this j

101
00:07:52,660 --> 00:07:55,100
if i take be the following

102
00:07:55,160 --> 00:07:57,200
i observe

103
00:07:58,180 --> 00:08:02,240
a spatial signal picture image

104
00:08:03,930 --> 00:08:06,530
in this case so i wouldn't mean

105
00:08:06,580 --> 00:08:08,620
the pixel that i observed

106
00:08:08,950 --> 00:08:13,290
and this would be the actual that would be something that picture image the image

107
00:08:13,290 --> 00:08:16,260
changes with time so that would be time than here

108
00:08:16,260 --> 00:08:20,030
to solve this problem is that we only to consider

109
00:08:20,040 --> 00:08:22,180
exactly you know

110
00:08:22,190 --> 00:08:25,000
not normally and he was

111
00:08:25,010 --> 00:08:29,250
not interested in america solutions at the beginning at all

112
00:08:29,340 --> 00:08:34,530
and and later he realized that there is simply no way out

113
00:08:34,570 --> 00:08:39,610
having exact solution so once you've got the exact solutions for many many

114
00:08:39,690 --> 00:08:43,550
specifications is lies that there is no generic way of doing that

115
00:08:43,610 --> 00:08:47,780
then he turned into this turned to the medical issues

116
00:08:47,820 --> 00:08:53,630
and then he discovered that stuff is found are very nice problems there

117
00:08:53,640 --> 00:08:57,060
so anyway so he said his is a great guy

118
00:08:58,480 --> 00:09:04,450
what we start this is that we are going to simplify things little bit so

119
00:09:04,450 --> 00:09:08,060
previous that i said that the policy could depend on the past on the full

120
00:09:09,690 --> 00:09:12,320
but we will see that for our purposes

121
00:09:12,460 --> 00:09:14,500
the so called stationary

122
00:09:14,500 --> 00:09:19,160
the term is that policies such as starfish so what that is objects

123
00:09:19,240 --> 00:09:22,910
so this is just a mapping from states to actions so you have

124
00:09:23,840 --> 00:09:28,220
current state and given the current state to decidable about what to do next so

125
00:09:28,220 --> 00:09:32,410
in this example the robots could no it's card and it's in the two d

126
00:09:33,350 --> 00:09:38,150
and that's it state and depending on that no matter what the very came from

127
00:09:38,150 --> 00:09:43,310
it just this side support vertical

128
00:09:43,390 --> 00:09:48,890
so we only this set of functions so the acts is

129
00:09:49,010 --> 00:09:51,870
the set of all functions

130
00:09:51,890 --> 00:09:54,630
from the state space the real numbers

131
00:09:54,710 --> 00:09:57,250
so i put this thing here

132
00:09:57,310 --> 00:10:01,760
that the vietnam norm the simply on or off these finite so

133
00:10:02,690 --> 00:10:07,270
supremum norm

134
00:10:07,290 --> 00:10:11,010
is defined simply as

135
00:10:11,010 --> 00:10:15,730
you know the size sort of the size of the function

136
00:10:15,780 --> 00:10:17,950
so you take the absolute value

137
00:10:17,950 --> 00:10:22,260
of the function and you take the supreme with respect to the state space

138
00:10:22,310 --> 00:10:24,520
so let's call it the

139
00:10:24,520 --> 00:10:28,580
OK sorry

140
00:10:28,640 --> 00:10:30,790
so you given this function and

141
00:10:30,890 --> 00:10:34,410
so basically but this is the function should be banned

142
00:10:34,450 --> 00:10:39,190
so that's why you have this notation bfx p stands for bob for this the

143
00:10:39,190 --> 00:10:41,050
space of bounded functions

144
00:10:41,110 --> 00:10:43,070
so should be very about this

145
00:10:43,160 --> 00:10:47,310
but if you have a finite state space

146
00:10:48,430 --> 00:10:52,000
then there is no need to so

147
00:10:52,320 --> 00:10:56,980
if for every of state you assign any number

148
00:10:56,980 --> 00:11:00,560
you take the supreme which in this case should be just the max

149
00:11:00,590 --> 00:11:05,060
this is always a finite number then what this value functions

150
00:11:05,070 --> 00:11:10,090
these are just fact strike just the indexing is a bit strange since it after

151
00:11:10,090 --> 00:11:12,740
seeing the of i

152
00:11:12,780 --> 00:11:16,360
well i could go from one to n

153
00:11:16,460 --> 00:11:19,070
and then this looks like a vector

154
00:11:19,090 --> 00:11:20,260
we just say

155
00:11:20,260 --> 00:11:22,440
we have an axe

156
00:11:22,520 --> 00:11:26,920
where goes from i don't know one to then maybe

157
00:11:26,930 --> 00:11:29,490
so we don't care about the names of the

158
00:11:29,500 --> 00:11:30,710
the states

159
00:11:33,180 --> 00:11:36,990
so we see that the the if the state space is finite we can call

160
00:11:36,990 --> 00:11:39,750
them by the natural numbers

161
00:11:39,760 --> 00:11:42,860
and in the states record like that

162
00:11:42,910 --> 00:11:44,280
this is clear

163
00:11:44,360 --> 00:11:47,930
so function alive at vectors but

164
00:11:47,950 --> 00:11:52,540
should be treated more like functions if the state space is infinite

165
00:11:52,680 --> 00:11:56,620
but in general functions like vectors and

166
00:11:58,670 --> 00:12:01,790
and so these are the bounded functions and then

167
00:12:01,810 --> 00:12:04,490
we define this operator t o five

168
00:12:04,530 --> 00:12:06,580
for the PFI operator is

169
00:12:06,610 --> 00:12:09,380
this is defined in the following way so

170
00:12:09,510 --> 00:12:13,920
this takes effect i i call them back to extract no photo

171
00:12:14,010 --> 00:12:19,450
think of simplicity is you take that and this operator in terms of actor

172
00:12:20,750 --> 00:12:22,610
so what it does is that

173
00:12:22,630 --> 00:12:25,350
it's an averaging or the next states

174
00:12:25,370 --> 00:12:28,850
we suspect that the transition probabilities

175
00:12:28,940 --> 00:12:30,930
and so here is the

176
00:12:31,000 --> 00:12:32,080
very important

177
00:12:32,140 --> 00:12:33,270
fifty five

178
00:12:33,320 --> 00:12:34,180
is we

179
00:12:34,180 --> 00:12:35,690
is that

180
00:12:36,380 --> 00:12:38,570
so the next thing is the game

181
00:12:38,580 --> 00:12:40,320
is is

182
00:12:42,780 --> 00:12:45,280
but otherwise it should be clear so

183
00:12:45,290 --> 00:12:49,160
this is an average we suspect next states

184
00:12:49,160 --> 00:12:54,430
so why is the next prime so this is the conditional probability or act

185
00:12:54,450 --> 00:12:56,080
you're executing

186
00:12:56,100 --> 00:12:58,270
some actions to what action

187
00:12:58,380 --> 00:13:00,350
he said that we care about it

188
00:13:01,590 --> 00:13:06,440
stationary policy so the station police illiteracy and action

189
00:13:07,270 --> 00:13:12,090
so given that and given the action returned by the stationary policy

190
00:13:12,110 --> 00:13:15,380
you have the probability distribution over the next state

191
00:13:15,430 --> 00:13:18,330
and you take the immediate reward received

192
00:13:18,390 --> 00:13:21,190
you have these two government times

193
00:13:22,820 --> 00:13:25,210
the value and the next day

194
00:13:25,260 --> 00:13:29,120
and you just average our use some over the next few

195
00:13:29,140 --> 00:13:32,190
so that's what the operator does

196
00:13:32,310 --> 00:13:33,700
and then

197
00:13:33,740 --> 00:13:37,920
that is this literature and that's a step

198
00:13:37,970 --> 00:13:39,310
the value function

199
00:13:39,330 --> 00:13:45,260
the policy is the fixed point of PFI

200
00:13:45,300 --> 00:13:48,060
so why is the street

201
00:13:48,110 --> 00:13:50,170
so maybe this is

202
00:13:50,200 --> 00:13:54,240
one prove that i should do

203
00:13:54,250 --> 00:13:57,180
because this is so crucial

204
00:13:57,250 --> 00:14:00,570
today understanding of anything what's coming

205
00:14:00,630 --> 00:14:03,180
so we should spend some time on it

206
00:14:04,850 --> 00:14:11,070
what's the point here so was the definition of the value functions of see finite

207
00:14:11,090 --> 00:14:13,300
this size or

208
00:14:14,770 --> 00:14:18,330
maybe maybe a little bit larger

209
00:14:18,380 --> 00:14:23,050
i i don't know i don't see from the map

210
00:14:23,110 --> 00:14:28,250
so what's the value function so this was the expectations

211
00:14:28,250 --> 00:14:30,000
of the

212
00:14:31,520 --> 00:14:36,260
this called the top that turned

213
00:14:37,570 --> 00:14:38,870
that policy pi

214
00:14:38,890 --> 00:14:42,490
is executed and the first is just axe

215
00:14:42,530 --> 00:14:45,030
so that's the definition of the value function

216
00:14:45,040 --> 00:14:47,690
oh by the way so what do we mean by

217
00:14:47,690 --> 00:14:49,350
but these activation

218
00:14:49,350 --> 00:14:53,570
in the visible range is cheap because they can use in economy

219
00:14:53,590 --> 00:14:55,420
to do this he must images

220
00:14:55,440 --> 00:14:58,040
but if i try to build an infrared camera

221
00:14:58,080 --> 00:15:02,050
then this kind of magical chemistry does not occur with silicon anyone have to use

222
00:15:02,350 --> 00:15:03,710
the type of detector

223
00:15:03,760 --> 00:15:07,260
i would have to use in gaza germanium as if i want to make now

224
00:15:07,260 --> 00:15:08,810
CCD array

225
00:15:08,830 --> 00:15:13,510
full of in gaza germanium detectors that costs a lot of money

226
00:15:13,530 --> 00:15:18,180
so sensors can be extremely expensive to manufacture and for this reason i could have

227
00:15:18,180 --> 00:15:21,720
far fewer sensors than the number of desired pixels

228
00:15:21,810 --> 00:15:26,230
the measurements can be also very expensive and that happens in a lot of applications

229
00:15:26,970 --> 00:15:30,550
a lot of people are interested in fuel cells these days it is very important

230
00:15:30,580 --> 00:15:34,600
to know what happens inside the fuel cell while the only imaging techniques that i'm

231
00:15:34,600 --> 00:15:39,810
aware of to actually see what's going on inside of yourself is essentially neutron imaging

232
00:15:39,910 --> 00:15:43,970
which neutron scattering which is you bombards yourselves with neutrons and you look at the

233
00:15:43,970 --> 00:15:48,870
scattering patterns and in fear what's happening inside this yourself now when you start bombarding

234
00:15:48,870 --> 00:15:52,360
things with neutrons it costs a lot of money

235
00:15:52,390 --> 00:15:53,400
and before

236
00:15:53,410 --> 00:15:56,970
you want to make the next set of measurement which will cost tens of thousands

237
00:15:56,970 --> 00:16:00,200
of dollars you really want to make sure that you need to do so

238
00:16:00,220 --> 00:16:05,420
and on all of these applications because measurements are very expensive i can only recorded

239
00:16:05,420 --> 00:16:09,020
a few linear functionals about the image of interest

240
00:16:09,920 --> 00:16:13,460
it could be that it takes a long time to make up an image

241
00:16:13,470 --> 00:16:17,360
and so because it takes a long time i can only make a few measurements

242
00:16:17,370 --> 00:16:21,410
and so for all these reasons we could be in a situation where the number

243
00:16:21,430 --> 00:16:26,270
of equations that i have about the image of interest is much smaller than the

244
00:16:26,270 --> 00:16:29,370
total number of pixels it and i'm asking you

245
00:16:29,390 --> 00:16:31,510
is it possible to recover

246
00:16:31,760 --> 00:16:34,330
and pixel image from

247
00:16:34,340 --> 00:16:38,340
so the data of from linear measurements about this image

248
00:16:38,350 --> 00:16:43,950
and of course you know going back to the underdetermined system of equations looks suspicious

249
00:16:43,980 --> 00:16:48,080
but yet now you've got to be kind of well educated

250
00:16:48,270 --> 00:16:51,490
maybe it's not completely out of reach

251
00:16:51,520 --> 00:16:55,840
and so which measurements should we take how should we go about reconstructing these images

252
00:16:55,840 --> 00:16:57,730
from a few measurements

253
00:16:57,740 --> 00:17:02,970
the compressed sensing was born at very practical experiment which was suggested to me

254
00:17:03,110 --> 00:17:08,040
by radiology and experiment goes something like this in radiology

255
00:17:08,100 --> 00:17:13,100
it's a very complicated process it's a wonderful imaging modality that relies on some quantum

256
00:17:13,100 --> 00:17:17,840
effects essentially spin relaxation and so i'm not going to describe the physics which is

257
00:17:17,840 --> 00:17:23,830
quite complicated and it just should tell you that in radiology MRI is essentially acquiring

258
00:17:23,910 --> 00:17:28,650
pictures of living tissues by sampling the fourier domain essential

259
00:17:28,660 --> 00:17:32,970
so in magnetic resonance imaging so we have an MRI scan on the right we

260
00:17:32,970 --> 00:17:38,350
have an image which is the people i by p pixel image we have square

261
00:17:38,370 --> 00:17:40,810
so and pixels

262
00:17:40,840 --> 00:17:45,580
so essentially when you and whenever you to one of his choral zone

263
00:17:45,850 --> 00:17:50,790
mr machine is essentially measuring your fourier coefficients so there's a slice of your body

264
00:17:50,790 --> 00:17:52,660
which are going to call f

265
00:17:52,670 --> 00:17:55,720
right so we're going try to images slice of your body

266
00:17:55,720 --> 00:17:58,310
and so what the MRI machine is giving you

267
00:17:58,350 --> 00:18:02,800
is your ramp up the magnetic field and ramping up the magnetic fields the gradient

268
00:18:02,800 --> 00:18:08,810
of the magnetic fields corresponding to frequency point in k space frequency omega one omega

269
00:18:08,810 --> 00:18:14,260
two and what the call is doing is essentially measuring the fourier coefficients of the

270
00:18:14,260 --> 00:18:16,990
density of interest

271
00:18:17,020 --> 00:18:22,640
now it's common in denmark to have a number of measurements and which is much

272
00:18:22,640 --> 00:18:27,280
smaller than the desire desire resolution which is much smaller than the number of pixels

273
00:18:27,660 --> 00:18:31,040
is if you go to MR conferences they'll tell you

274
00:18:31,050 --> 00:18:37,140
that well if you take fewer measurement pixels the reconstruction is impossible sometimes it that

275
00:18:37,140 --> 00:18:38,980
the decorate things with

276
00:18:39,010 --> 00:18:43,530
invoking the name shannon nyquist and if you want to sample shine and then you

277
00:18:43,530 --> 00:18:46,990
cannot reconstruct the image and so on and so forth i would like to state

278
00:18:47,150 --> 00:18:48,900
more precisely that

279
00:18:49,180 --> 00:18:53,560
everybody knows that if we have fewer equations than unknowns and the reconstruction is not

280
00:18:55,390 --> 00:18:59,990
OK so the number of measurements must match the number of unknowns

281
00:19:01,060 --> 00:19:06,140
can you see this picture because now it's becoming important to see pictures

282
00:19:06,160 --> 00:19:07,670
and you see

283
00:19:07,700 --> 00:19:10,770
it's not good or maybe there's a way to be in the limelight exactly sure

284
00:19:10,770 --> 00:19:12,040
what i need to do

285
00:19:12,060 --> 00:19:17,370
OK so on the left you see typically an image of living tissues that in

286
00:19:17,370 --> 00:19:19,340
the spatial domain that cut

287
00:19:19,360 --> 00:19:21,790
so is the body and so you see a picture

288
00:19:21,800 --> 00:19:23,150
and so

289
00:19:23,160 --> 00:19:27,060
the problem with MRI it's a wonderful imaging modality

290
00:19:27,070 --> 00:19:31,130
it does it allows you to see things that also imaging without do not allow

291
00:19:31,130 --> 00:19:34,580
you to see for example if you take a CT scan

292
00:19:34,610 --> 00:19:36,730
of the body

293
00:19:36,780 --> 00:19:38,990
the city will not tell you

294
00:19:39,000 --> 00:19:42,290
whether it's a life or death

295
00:19:42,300 --> 00:19:44,400
because just measuring absorption

296
00:19:44,410 --> 00:19:49,030
of things and so the city scandal not make the difference between tissues that are

297
00:19:49,040 --> 00:19:50,590
like tissues that are there

298
00:19:50,620 --> 00:19:51,910
mark this

299
00:19:51,930 --> 00:19:56,550
it's a wonderful imaging modality but the problem is we don't use it very much

300
00:19:56,550 --> 00:19:58,570
because it's extremely slow

301
00:19:58,580 --> 00:20:02,900
so the problem is that to make to collect one of these for coefficient takes

302
00:20:02,910 --> 00:20:07,440
quite a long time because it uses penalized stations and they have a certain time

303
00:20:07,440 --> 00:20:11,160
scale and there's no way i can speedy so it's physical process i have to

304
00:20:11,840 --> 00:20:15,830
before i can make a measurement because they have to wait until the spin relaxes

305
00:20:18,660 --> 00:20:19,530
and so

306
00:20:20,630 --> 00:20:23,290
because it takes awhile to acquire imaging

307
00:20:23,300 --> 00:20:26,710
this means a lot of things that we cannot do high-resolution imaging because we cannot

308
00:20:26,710 --> 00:20:28,030
acquire a lot of data

309
00:20:28,080 --> 00:20:30,490
and also we can add the video

310
00:20:30,500 --> 00:20:31,210
and so

311
00:20:31,230 --> 00:20:34,430
more doctors are extremely interesting now

312
00:20:34,440 --> 00:20:39,310
in speeding up market which is very important to wide applicability also nobody likes to

313
00:20:40,630 --> 00:20:44,790
powers and this can affect has problems to spend too much time in this camp

314
00:20:45,060 --> 00:20:49,140
and you start moving and not exactly sure what you measuring anymore

315
00:20:49,150 --> 00:20:53,130
OK and of course if we could make faster MRI images and you could have

316
00:20:53,130 --> 00:20:59,030
this throughput of patients who can which would greatly increase which is an enormous

317
00:20:59,040 --> 00:21:03,710
of enormous importance because it scans cost so much money

318
00:21:03,840 --> 00:21:09,170
so everybody is interested for various reasons is to increase resolution to enable video

319
00:21:09,190 --> 00:21:11,870
or two kind of increases throughput through

320
00:21:11,890 --> 00:21:13,620
this can to kind of

321
00:21:13,640 --> 00:21:14,690
be able to

322
00:21:16,240 --> 00:21:19,760
but because of the physical thing accelerating MR means only one thing is that you

323
00:21:19,760 --> 00:21:21,840
have to sample s

324
00:21:21,870 --> 00:21:23,210
all right and that's

325
00:21:23,240 --> 00:21:27,240
kind of the data acquisition strategy that people were thinking of

326
00:21:27,480 --> 00:21:29,230
applying so

327
00:21:29,250 --> 00:21:33,110
we wish to acquire as many fourier coefficients are pixels but it's impossible so we

328
00:21:33,110 --> 00:21:37,740
can downsample the fourier transform and what we're going to do is we're going select

329
00:21:37,740 --> 00:21:40,230
radio lines going through the origin

330
00:21:40,250 --> 00:21:45,160
and then we get samples the fourier transform not only along those lines

331
00:21:45,170 --> 00:21:50,110
right so we geologists of the university of wisconsin by the name of czech misread

332
00:21:50,130 --> 00:21:55,530
approach me with the data set where he had about twenty two radial lines and

333
00:21:55,530 --> 00:22:00,020
it's a megapixel image so we have twenty about twenty radial lines and along each

334
00:22:00,840 --> 00:22:04,520
the measured about one thousand four coefficients

335
00:22:05,140 --> 00:22:08,450
so we have a megapixel images one million unknowns

336
00:22:08,480 --> 00:22:13,330
we have twenty radial lines with one thousand measurements per line and so we just

337
00:22:13,330 --> 00:22:17,630
do rapid counting and what do we discover we discover that ninety eight percent of

338
00:22:17,630 --> 00:22:21,370
the questions are missing the number of equations i have is two percent of what

339
00:22:21,370 --> 00:22:24,320
i really need

340
00:22:24,320 --> 00:22:26,130
we face

341
00:22:26,160 --> 00:22:33,250
OK actually is one question because i would consider mathematical answer as one domain specific

342
00:22:33,250 --> 00:22:35,660
markup that are close community

343
00:22:35,670 --> 00:22:36,970
everybody really has

344
00:22:37,010 --> 00:22:39,420
pre-arranged so is

345
00:22:39,420 --> 00:22:44,230
i don't know exactly how anathema for example was created in the community process last

346
00:22:44,500 --> 00:22:48,040
but not understanding is that this is a percent

347
00:22:48,050 --> 00:22:53,320
pre-defined text that you can use to annotate your resources and this is actually what

348
00:22:53,330 --> 00:22:57,210
also dublin core gives you four documents in a very generic way that we have

349
00:22:57,210 --> 00:23:02,700
a large agreement so that people know that amount of people agreed on dublin core

350
00:23:02,720 --> 00:23:08,290
as the standard for imitating documents with metadata so it's actually an ontology

351
00:23:08,920 --> 00:23:13,470
there exists some translations of the dublin core into

352
00:23:13,500 --> 00:23:18,660
so you have exactly the same vocabulary in RDF available and one of the features

353
00:23:18,660 --> 00:23:24,200
for example that dublin included is extendable so you can extend the dublin core standard

354
00:23:24,200 --> 00:23:26,380
with you

355
00:23:26,390 --> 00:23:29,200
that we can vocabulary

356
00:23:29,200 --> 00:23:34,000
fits perfectly well to the model so you can extend also RDF

357
00:23:34,010 --> 00:23:37,160
dublin core ontology with whatever you need so

358
00:23:37,160 --> 00:23:42,710
this this is exactly replication which already existed which is now standardised

359
00:23:42,740 --> 00:23:47,920
you know even larger community the dublin core community that's very big but now the

360
00:23:47,920 --> 00:23:52,810
RDF community is growing even bigger and dublin core will be one part of this

361
00:23:52,880 --> 00:23:57,050
actually so it's a very experimental ontology but now that

362
00:23:57,580 --> 00:24:02,700
it into another representation language to make it usable with the available technologies

363
00:24:03,710 --> 00:24:07,390
i know which is feasible for dublin core from other there are not so familiar

364
00:24:07,390 --> 00:24:12,050
with but i guess it would be feasible

365
00:24:12,260 --> 00:24:16,450
but the question

366
00:24:16,580 --> 00:24:18,580
which is not very

367
00:24:18,820 --> 00:24:31,240
i want

368
00:24:31,250 --> 00:24:36,820
OK this that as far as i know this that x seven the next one

369
00:24:36,820 --> 00:24:41,880
will be about twenty one or strange what

370
00:24:41,880 --> 00:24:49,830
and within and seven there the possibility to embed some kind of metadata and the

371
00:24:49,830 --> 00:24:55,430
first but when i said in in our vision that are quite technologies will be

372
00:24:55,430 --> 00:25:00,780
combined for example with multimedia and the state of of course how to annotate video

373
00:25:00,780 --> 00:25:04,960
streams how to annotate pictures how to enrich these

374
00:25:05,500 --> 00:25:10,040
multimedia documents with no we're not going to make them more findable

375
00:25:11,170 --> 00:25:13,740
as we saw yesterday for example from

376
00:25:13,740 --> 00:25:19,130
mean that's presentation that the problem is how to find pictures for example describe that

377
00:25:19,130 --> 00:25:25,050
texts and then search for key words to try to to to give an example

378
00:25:25,050 --> 00:25:30,080
and then so please find something that could be partially solved if you have a

379
00:25:30,080 --> 00:25:34,820
well-defined meaning in behind which describes what is in the picture so you have an

380
00:25:34,820 --> 00:25:39,380
ontology about the domain of the picture and then you can embed these metadata within

381
00:25:39,630 --> 00:25:44,590
video stream or within picture for example they ever exists first approaches and as far

382
00:25:44,590 --> 00:25:49,180
as i know this will be also part of one of the very promising i

383
00:25:49,180 --> 00:25:54,410
piece of integrated projects which hopefully will be funded by the european commission it's called

384
00:25:54,420 --> 00:25:59,920
a i think and as the target as one of the targets to handle multimedia

385
00:25:59,920 --> 00:26:05,330
documents and that's small part of that

386
00:26:05,360 --> 00:26:12,320
product and i see that within from years these big pieces some to collaborate so

387
00:26:12,320 --> 00:26:16,000
this is what we mean by we drive our technology into other application areas for

388
00:26:17,740 --> 00:26:21,930
so i think in one of the for upcoming em peg standards there will be

389
00:26:22,090 --> 00:26:25,240
even more facilities for annotation

390
00:26:25,280 --> 00:26:39,420
he is the man just three which that i know from

391
00:26:39,420 --> 00:26:45,090
it's just the france televisions to television

392
00:26:45,120 --> 00:26:49,780
i don't know national television and

393
00:26:49,790 --> 00:26:55,280
i mean the guy two years ago and the real value attempting i think it

394
00:26:55,280 --> 00:26:56,740
was soccer games

395
00:26:56,890 --> 00:27:02,080
they had like fifteen seventy people sitting there and attempting those soccer games with key

396
00:27:02,960 --> 00:27:07,460
and the highly interested when we said that we can use ontology is to describe

397
00:27:08,140 --> 00:27:12,670
the domain of seconds and then you have some and trees that to i think

398
00:27:12,670 --> 00:27:15,420
especially those you wanting to

399
00:27:15,460 --> 00:27:19,790
companies with less interested and one of our case studies will also be in the

400
00:27:19,790 --> 00:27:22,120
media sector but i don't know

401
00:27:22,840 --> 00:27:29,820
the results will reach so that the and we think about how to include it

402
00:27:29,860 --> 00:27:33,890
but also this these which

403
00:27:33,930 --> 00:27:37,670
so what our sense is also due

404
00:27:37,780 --> 00:27:43,660
so they be exactly what my problem that first

405
00:27:47,660 --> 00:27:50,670
much more

406
00:27:51,160 --> 00:27:56,580
further questions

407
00:27:56,590 --> 00:28:04,000
so you don't have like

408
00:28:04,010 --> 00:28:12,310
still others want and very interesting to you are operating with some of national library

409
00:28:12,710 --> 00:28:17,820
germany something national lottery transformed to london do

410
00:28:18,060 --> 00:28:22,280
a campaign mind maybe to charge them

411
00:28:22,680 --> 00:28:27,380
two working together to just kind of

412
00:28:27,830 --> 00:28:34,210
and next how to do you so we have one ongoing project which has one

413
00:28:34,210 --> 00:28:36,500
partner the BOP

414
00:28:36,540 --> 00:28:41,580
what you might be familiar with which is beyond and service where you can ask

415
00:28:41,580 --> 00:28:48,780
for citation details so that we also try to enhance that michael lie i think

416
00:28:48,780 --> 00:28:49,400
it's a bit faster

417
00:28:50,560 --> 00:28:51,820
and they optimize distribution

418
00:28:52,670 --> 00:28:53,470
forced sigma

419
00:28:54,920 --> 00:28:58,500
doesn't have its maximum sigma and it's actually got at one minus one

420
00:29:00,240 --> 00:29:01,330
so this is the case where we

421
00:29:02,520 --> 00:29:04,630
constrain the distribution to be wrong

422
00:29:05,080 --> 00:29:08,390
in a fairly simple way making it separable is a fairly standard thing to do

423
00:29:09,380 --> 00:29:15,050
and then you end up with the optimized distribution having disorder properties that we kind of want

424
00:29:16,420 --> 00:29:19,030
it's not the case if you marginalize out

425
00:29:19,480 --> 00:29:23,160
new that you get the correct posterior the true posterior over mu

426
00:29:23,550 --> 00:29:27,070
it actually a student distribution is a mixture of all those cast and so

427
00:29:27,780 --> 00:29:29,530
that we have magically sort

428
00:29:30,020 --> 00:29:36,370
got round about the fundamental problem the true posterior when marginalize it is a slightly nasty mixture of gaussians

429
00:29:39,250 --> 00:29:41,630
it's got the right sort of properties

430
00:29:42,130 --> 00:29:46,990
so if you ask what's what's its mean most characteristic with it's got the right characteristic with

431
00:29:49,190 --> 00:29:51,820
okay so those example number one any questions about

432
00:29:52,470 --> 00:29:52,890
but let

433
00:29:59,460 --> 00:30:03,210
example number two is these spin system which we've discussed

434
00:30:03,810 --> 00:30:04,280
on and off

435
00:30:04,720 --> 00:30:05,520
in the past

436
00:30:07,890 --> 00:30:09,610
has been system has couplings

437
00:30:12,230 --> 00:30:14,390
between the spins which are binary variables

438
00:30:17,320 --> 00:30:19,950
spin systems can display all sorts of exciting

439
00:30:30,590 --> 00:30:32,160
so the nasty red distribution

440
00:30:38,310 --> 00:30:39,620
it into minus

441
00:30:40,410 --> 00:30:41,310
peter times and

442
00:30:44,430 --> 00:30:44,920
one z

443
00:30:46,270 --> 00:30:47,350
notice that e

444
00:30:47,410 --> 00:30:48,940
addition because we're doing physics

445
00:30:49,560 --> 00:30:52,700
all the temperature parameter or it coldness parameter beta

446
00:30:53,540 --> 00:30:55,230
which multiplies the energy upstairs

447
00:30:56,900 --> 00:31:00,110
hand it makes the z beta dependent

448
00:31:00,850 --> 00:31:04,600
the way the free energy response that's is we show in

449
00:31:06,770 --> 00:31:07,250
a beta

450
00:31:09,150 --> 00:31:11,280
and then the free energy depends both on theta

451
00:31:12,390 --> 00:31:13,200
hand beta

452
00:31:15,020 --> 00:31:17,660
mccain and then the free energy is always a bound on

453
00:31:18,430 --> 00:31:25,220
z and i can pick any beta andy the variational free energy minimization fall that's a particular problem

454
00:31:27,680 --> 00:31:29,360
and the spring system is

455
00:31:32,020 --> 00:31:33,020
of x is

456
00:31:33,450 --> 00:31:34,290
minus a half

457
00:31:36,640 --> 00:31:37,800
overall pairs spins

458
00:31:38,520 --> 00:31:39,030
x them

459
00:31:39,770 --> 00:31:40,400
x and

460
00:31:42,190 --> 00:31:42,750
weighted by

461
00:31:44,160 --> 00:31:44,710
jay and

462
00:31:45,700 --> 00:31:48,840
and then there may be local field supplying which spins

463
00:31:49,700 --> 00:31:52,230
like this and be x is lived in

464
00:31:54,410 --> 00:31:55,790
minus one plus one

465
00:31:57,030 --> 00:31:59,640
power and so the x glyphs on the corners over

466
00:32:00,330 --> 00:32:00,720
i think you

467
00:32:01,560 --> 00:32:03,380
and the reason is a very nasty

468
00:32:03,840 --> 00:32:05,400
problem is because this

469
00:32:05,890 --> 00:32:06,940
term here the fact that

470
00:32:07,460 --> 00:32:11,490
there are couplings between pairs of spins is what makes it a nasty red

471
00:32:11,890 --> 00:32:12,050
o thing

472
00:32:13,290 --> 00:32:17,240
if only it were the case that we didn't have these coupling terms then it

473
00:32:17,240 --> 00:32:21,120
would be a simple distribution because this is a nasty this just says energy is

474
00:32:21,120 --> 00:32:22,880
a sum of terms one three spin

475
00:32:23,370 --> 00:32:27,710
and not means when you experience at that these spins are independent from each other

476
00:32:27,720 --> 00:32:28,850
if we only had this term

477
00:32:29,340 --> 00:32:32,500
so wouldn't it be nice if the distribution looks like this

478
00:32:33,100 --> 00:32:37,100
and made by saying well let's go ahead and declare are approximating distribution

479
00:32:37,610 --> 00:32:38,120
the be simple

480
00:32:38,810 --> 00:32:40,160
that's planet have the form

481
00:32:41,570 --> 00:32:43,280
q of x is

482
00:32:44,300 --> 00:32:47,680
a normalized version of he could be some over and

483
00:32:48,150 --> 00:32:48,960
i and

484
00:32:50,160 --> 00:32:51,020
x at

485
00:32:53,770 --> 00:32:57,770
so now in this and approximating distribution each other spins has just got its own

486
00:32:57,770 --> 00:33:01,340
personal applied field that favours being up or down depending on the

487
00:33:02,150 --> 00:33:02,970
in and

488
00:33:03,370 --> 00:33:06,390
so this is just a several with distribution and very easy to work with

489
00:33:07,730 --> 00:33:12,690
emphasize that several i can write this as product from one end of an individual

490
00:33:13,900 --> 00:33:15,840
distribution for the end spin alone

491
00:33:20,730 --> 00:33:24,800
just like a moment ago we said let's pretend the distribution over mu and sigma squared

492
00:33:25,220 --> 00:33:26,690
gets approximated by separable

493
00:33:27,550 --> 00:33:32,250
product distributions so the parameters in this case are these eigenstates to

494
00:33:33,170 --> 00:33:36,360
the thing i generally calling theta is the set of all these

495
00:33:36,950 --> 00:33:37,540
they ends

496
00:33:43,460 --> 00:33:44,620
just like this one other way

497
00:33:49,020 --> 00:33:49,920
this is italy

498
00:33:54,350 --> 00:33:58,150
so because it's a separable distribution it's easy to evaluate its entropy

499
00:33:59,430 --> 00:34:00,440
hand it's not

500
00:34:01,160 --> 00:34:02,570
trivially obvious but we can

501
00:34:03,130 --> 00:34:07,740
easily evaluate the expected value of energy even though the energy couplings in it

502
00:34:09,220 --> 00:34:11,730
let's go to the screen after talk through this

503
00:34:12,310 --> 00:34:16,840
the screen shows the approximating distribution in green on the second equation

504
00:34:19,180 --> 00:34:24,160
that equation right beta after all and colorful green and red so we can keep track of what's what

505
00:34:24,940 --> 00:34:25,390
and you see

506
00:34:25,840 --> 00:34:31,220
we've got a simple entropy is just the entropy over several distributions so the right hand green term is easy

507
00:34:31,700 --> 00:34:37,840
they only nastiness comes in these from the red terms where we could be average value over the couplings

508
00:34:39,460 --> 00:34:40,630
under the distribution q

509
00:34:41,400 --> 00:34:42,440
so we now go down

510
00:34:43,000 --> 00:34:46,660
the next equation in parentheses we've got the expected value of the energy

511
00:34:50,860 --> 00:34:51,630
together coupling

512
00:34:52,520 --> 00:34:57,710
terms contribution we just need to know the some overall then all jam many times

513
00:34:58,150 --> 00:34:59,700
the average value of x bahram

514
00:35:00,330 --> 00:35:01,860
the average value x but then

515
00:35:02,280 --> 00:35:03,740
because q is separable

516
00:35:05,690 --> 00:35:06,500
simple set

517
00:35:06,970 --> 00:35:07,390
it's true

518
00:35:08,120 --> 00:35:11,430
so now all you need to know to evaluate the energy is well what's the

519
00:35:11,430 --> 00:35:13,060
mean value of each other spins

520
00:35:13,880 --> 00:35:14,390
and the q

521
00:35:16,400 --> 00:35:16,840
and then you can

522
00:35:17,300 --> 00:35:21,900
our tot up all the fuel and squared contributions for each of these pairs

523
00:35:22,590 --> 00:35:23,310
possible couplings

524
00:35:25,100 --> 00:35:27,590
so we can evaluate the variational free energy

525
00:35:28,280 --> 00:35:29,720
hand when you look at this and you say

526
00:35:30,150 --> 00:35:32,350
how do i just it with respect to theta

527
00:35:32,740 --> 00:35:34,580
remember theta hair is the set of

528
00:35:35,000 --> 00:35:37,790
biases with individual spins

529
00:35:38,780 --> 00:35:45,890
hand what effect does have well they change what the probability was being up is which changes be

530
00:35:46,330 --> 00:35:47,970
i mean value of the spin so

531
00:35:47,970 --> 00:35:52,190
or coming up with new ideas that cover cover four things were

532
00:35:52,190 --> 00:35:53,800
you're going to yes

533
00:35:56,380 --> 00:35:58,650
how making assumptions

534
00:35:58,700 --> 00:36:00,880
well in other places you can

535
00:36:00,880 --> 00:36:02,690
so without but not here

536
00:36:02,780 --> 00:36:05,570
you will be assuming good inductive biases

537
00:36:07,530 --> 00:36:12,720
no data analysis process will work with an inductive bias because otherwise assuming that it's

538
00:36:12,740 --> 00:36:15,440
quite nice and anything can happen

539
00:36:15,440 --> 00:36:18,960
but the main point is to be aware of which ones

540
00:36:19,010 --> 00:36:24,530
which inductive biases and work

541
00:36:24,590 --> 00:36:26,170
because sometimes

542
00:36:26,170 --> 00:36:30,720
you actually mean something and the we're actually

543
00:36:30,740 --> 00:36:32,970
and bob

544
00:36:32,990 --> 00:36:36,260
we then men including

545
00:36:36,970 --> 00:36:39,960
the necessity arises

546
00:36:40,050 --> 00:36:42,190
which does not mean

547
00:36:42,240 --> 00:36:47,220
that somebody comes in and says his learning of finite automata

548
00:36:47,240 --> 00:36:51,590
and then you say but what if your data are not cannot be bothered by

549
00:36:51,590 --> 00:36:53,550
a finite automata

550
00:36:53,570 --> 00:36:56,400
this is not this sort of challenge

551
00:36:56,420 --> 00:36:57,820
it's the other is saying

552
00:36:59,110 --> 00:37:01,300
bias is the automaton mine waste

553
00:37:01,320 --> 00:37:05,200
rather she work

554
00:37:05,570 --> 00:37:09,050
four different one to the other if necessary at some point

555
00:37:10,420 --> 00:37:12,700
somebody else is biased studios

556
00:37:12,760 --> 00:37:17,260
but we really to have them at some point if necessary

557
00:37:18,650 --> 00:37:19,940
it's a

558
00:37:19,990 --> 00:37:21,590
five maybe

559
00:37:21,590 --> 00:37:22,960
very useful for

560
00:37:22,970 --> 00:37:24,440
every this life

561
00:37:24,460 --> 00:37:29,280
but i don't think it's that would produce a

562
00:37:30,400 --> 00:37:34,130
thank you very much for your attention during the morning we see each other in

563
00:37:34,130 --> 00:37:34,920
the lab

564
00:37:34,970 --> 00:37:37,690
you know if you one our

565
00:37:37,740 --> 00:37:41,560
and we will go with through some examples of what i have seen here and

566
00:37:41,560 --> 00:37:47,400
we will for some examples of a workflow data mining tools that are out there

567
00:37:48,010 --> 00:37:52,590
i have one ready for you to copy in your and i find something we

568
00:37:52,590 --> 00:37:55,440
will work with the time with them all with some

569
00:37:55,970 --> 00:37:59,090
applets that are out there on the internet

570
00:37:59,460 --> 00:38:02,440
i think we need

571
00:38:04,690 --> 00:38:12,510
but eleven thirty statistical

572
00:38:12,570 --> 00:38:14,780
o eight hundred

573
00:38:14,800 --> 00:38:18,860
during surgery surgery i i misunderstood this

574
00:38:19,780 --> 00:38:22,190
maybe not they want most

575
00:38:22,200 --> 00:38:27,170
or we stop here only one out from here we go to the left

576
00:38:27,170 --> 00:38:31,720
i i said oh my my my what still has spain time

577
00:38:31,720 --> 00:38:34,760
and spain then is one hour more

578
00:38:34,760 --> 00:38:37,860
and i didn't realise that we were going to

579
00:38:41,740 --> 00:38:54,190
we also then

580
00:38:54,240 --> 00:38:58,490
it's just very very a

581
00:38:58,570 --> 00:39:00,700
a very high level description

582
00:39:00,840 --> 00:39:04,030
a not too much into the

583
00:39:04,050 --> 00:39:10,380
now this is one of the two himself and some methods the other being bagging

584
00:39:10,400 --> 00:39:15,690
in other words we our inductive bias so to speak

585
00:39:17,900 --> 00:39:19,690
it has three components

586
00:39:19,740 --> 00:39:25,900
one of them is to keep distribution of probability on a number of predictors so

587
00:39:25,900 --> 00:39:30,380
we're going to use one predictor we're going to combine several of them

588
00:39:30,400 --> 00:39:33,320
and we will a choose weights

589
00:39:35,110 --> 00:39:42,260
but are we keep waiting the observations themselves so or a because we

590
00:39:42,280 --> 00:39:43,630
be offering

591
00:39:43,690 --> 00:39:48,420
samples from all of generations according to weights

592
00:39:48,460 --> 00:39:51,590
and that's weights we evolve over time

593
00:39:51,650 --> 00:39:53,490
now the thirty

594
00:39:53,550 --> 00:39:58,990
there he is

595
00:40:00,470 --> 00:40:03,820
it's going to be one thing to predict

596
00:40:03,820 --> 00:40:06,010
and there was this

597
00:40:06,050 --> 00:40:07,470
no because howard

598
00:40:07,470 --> 00:40:08,470
but please

599
00:40:08,490 --> 00:40:11,440
the one that is very easy to train

600
00:40:11,490 --> 00:40:13,030
very easy to construct

601
00:40:13,090 --> 00:40:16,140
because you will be doing it over and over and over and over again it's

602
00:40:16,140 --> 00:40:19,090
a very expensive to construct nothing like in SVM

603
00:40:20,010 --> 00:40:22,440
the whole thing is going to last forever

604
00:40:22,460 --> 00:40:26,860
so they can very sympathetic to

605
00:40:28,760 --> 00:40:31,670
no way to each

606
00:40:31,740 --> 00:40:35,470
check for it would this is making mistakes it

607
00:40:35,590 --> 00:40:37,510
in doing well

608
00:40:38,220 --> 00:40:40,630
where the predators is making mistakes

609
00:40:40,630 --> 00:40:44,720
you can weight

610
00:40:44,780 --> 00:40:45,820
to be

611
00:40:46,490 --> 00:40:49,760
so as long as there is some sort of important

612
00:40:49,780 --> 00:40:51,820
what would you do i mean by the

613
00:40:51,860 --> 00:40:53,780
the predictor may be very

614
00:40:53,800 --> 00:40:55,610
weak very simpler

615
00:40:55,630 --> 00:40:58,880
the winning the one thing we need that it does

616
00:41:00,530 --> 00:41:03,070
if the they data that the nothing nothing

617
00:41:03,130 --> 00:41:05,300
then the whole thing is not going to work

618
00:41:05,380 --> 00:41:09,590
so it seems to be very weak may be doing very little

619
00:41:09,610 --> 00:41:11,940
but it has to be doing some

620
00:41:11,990 --> 00:41:15,420
what do i mean by this it has to be

621
00:41:15,440 --> 00:41:18,150
at least seen on better

622
00:41:18,200 --> 00:41:21,920
than flipping a coin

623
00:41:21,920 --> 00:41:24,070
and this absolutely

624
00:41:24,110 --> 00:41:27,880
must exist and be are real numbers

625
00:41:27,920 --> 00:41:31,970
a specific number of maybe ten to the minus five

626
00:41:32,010 --> 00:41:36,880
but ten to the minus better than random guessing

627
00:41:36,920 --> 00:41:41,860
it doesn't work if you're predictor as things start going difficult

628
00:41:41,860 --> 00:41:42,640
uniquely decodable

629
00:41:44,750 --> 00:41:48,150
right so wannabe ideas were floated now is

630
00:41:48,560 --> 00:41:49,460
easy to code

631
00:41:51,310 --> 00:41:54,050
could mean for example that we'd like a prefix code please

632
00:41:55,480 --> 00:41:57,490
but we really do want small expected length

633
00:42:01,670 --> 00:42:02,230
can you

634
00:42:10,900 --> 00:42:14,720
let me suggest an idea now about how we get small expected length

635
00:42:20,650 --> 00:42:22,000
we've found the code here

636
00:42:22,580 --> 00:42:25,070
which were speculating could be the best you can do

637
00:42:25,570 --> 00:42:27,500
for this ensemble with a simple code

638
00:42:27,910 --> 00:42:31,910
because it happens every day entropy and we've already heard some theorems about not being

639
00:42:31,910 --> 00:42:33,020
able to do better than entropy

640
00:42:35,370 --> 00:42:36,350
and it has the property

641
00:42:37,510 --> 00:42:41,920
is the key intuition about symbol codes and is over all the data compression has

642
00:42:41,920 --> 00:42:47,900
a key property you give short code words to the probable outcomes and longer codewords

643
00:42:48,680 --> 00:42:51,190
the less probable problems going that's what it's all about

644
00:42:52,970 --> 00:42:54,210
and that's a possible

645
00:42:54,900 --> 00:42:56,080
i guess about how

646
00:42:56,680 --> 00:42:58,640
we might get the smallest possible

647
00:42:59,220 --> 00:42:59,960
expected length

648
00:43:00,360 --> 00:43:03,380
we could say you give me a list probabilities of sort them

649
00:43:03,900 --> 00:43:04,410
and i'll give

650
00:43:04,950 --> 00:43:07,860
these shortest possible codewords tonight from one

651
00:43:08,310 --> 00:43:11,650
and the end steadily longer codewords as we go down the list

652
00:43:12,430 --> 00:43:14,860
machete neighbour and see what you think of a bad idea

653
00:43:18,860 --> 00:43:23,190
okay so suggested an idea here it's got the right spirit is giving short

654
00:43:27,020 --> 00:43:27,990
the problem becomes

655
00:43:29,150 --> 00:43:30,890
so long the less probable ones

656
00:43:31,850 --> 00:43:33,100
second work anyone

657
00:43:33,670 --> 00:43:35,100
criticize that's class

658
00:43:39,680 --> 00:43:42,290
if you've got a huge number symbols then watch

659
00:43:43,660 --> 00:43:44,580
it might not be good

660
00:43:45,170 --> 00:43:46,040
it's definitely not good

661
00:43:48,340 --> 00:43:49,000
probably not good

662
00:43:50,590 --> 00:43:51,990
i might agree with you

663
00:43:54,540 --> 00:43:55,140
let me give you

664
00:43:56,130 --> 00:43:57,370
some backup all

665
00:43:57,890 --> 00:43:58,540
intuition there

666
00:43:59,970 --> 00:44:01,440
we found a case where this

667
00:44:02,240 --> 00:44:02,990
proposed algorithm

668
00:44:03,520 --> 00:44:06,940
is getting us to entropy so we can't be too critical that but maybe we

669
00:44:06,940 --> 00:44:08,720
can find a counter-example where it doesn't

670
00:44:09,210 --> 00:44:09,810
it doesn't work

671
00:44:10,360 --> 00:44:11,770
andrew it doesn't need to be

672
00:44:14,510 --> 00:44:16,440
we can give them all a probability a quarter

673
00:44:17,750 --> 00:44:20,760
it's just a make life interesting will add that's ceylon

674
00:44:22,390 --> 00:44:26,510
one of them another one over two to one subtract that's long overdue from this one

675
00:44:26,950 --> 00:44:28,080
in fact that's a long

676
00:44:28,520 --> 00:44:30,870
not one that's on is a billion okay

677
00:44:31,550 --> 00:44:34,020
so there all essentially they all have a probability of a quarter

678
00:44:35,090 --> 00:44:35,400
all right

679
00:44:36,210 --> 00:44:36,480
and now

680
00:44:37,040 --> 00:44:37,650
we apply this

681
00:44:38,200 --> 00:44:39,160
suggested algorithm

682
00:44:39,890 --> 00:44:41,010
and what can do end up with

683
00:44:42,370 --> 00:44:43,950
and that with his six

684
00:44:44,810 --> 00:44:45,110
all right

685
00:44:46,380 --> 00:44:48,380
so this is the new probability distribution

686
00:44:49,140 --> 00:44:52,680
when you use code six which is proposed by this algorithm hat

687
00:44:53,150 --> 00:44:54,700
the expected length

688
00:44:55,160 --> 00:44:55,920
that's is

689
00:44:56,640 --> 00:44:58,460
all the links are one

690
00:44:59,670 --> 00:45:02,260
three andy three is the last one in the list

691
00:45:02,880 --> 00:45:05,570
and the average those things are all equiprobable

692
00:45:06,020 --> 00:45:06,820
he will take long

693
00:45:07,390 --> 00:45:08,080
it is now

694
00:45:09,330 --> 00:45:09,760
you know who

695
00:45:10,050 --> 00:45:11,570
o nine o four

696
00:45:12,480 --> 00:45:13,140
link is to

697
00:45:14,500 --> 00:45:15,230
and the quarter

698
00:45:17,020 --> 00:45:18,440
is the optimal symbol code

699
00:45:20,600 --> 00:45:21,640
how we do better how

700
00:45:24,460 --> 00:45:24,640
you know

701
00:45:27,480 --> 00:45:28,250
you see five

702
00:45:28,670 --> 00:45:29,750
and the expected length will be to

703
00:45:41,480 --> 00:45:43,750
but he length of five

704
00:45:49,250 --> 00:45:52,770
so we can't just use that's simple rule that not the right spirit

705
00:45:53,400 --> 00:45:56,270
we need to identify some sort of trade-off

706
00:45:57,420 --> 00:45:58,600
when do you give

707
00:46:00,460 --> 00:46:02,670
codewords to the most probable guy

708
00:46:02,670 --> 00:46:06,920
so what and the integrals there

709
00:46:09,440 --> 00:46:12,210
so now what galerkin siberia going to be

710
00:46:12,230 --> 00:46:13,730
i i've got

711
00:46:13,750 --> 00:46:17,540
i've got this you is now a combination of

712
00:46:17,620 --> 00:46:19,250
from one to four

713
00:46:19,250 --> 00:46:20,690
of the

714
00:46:20,710 --> 00:46:22,900
five fifi

715
00:46:22,920 --> 00:46:27,180
and i have made a smart choice of these fees and smart means that i

716
00:46:27,180 --> 00:46:33,560
can do the calculations quickly

717
00:46:34,230 --> 00:46:37,020
hold on a minute

718
00:46:37,040 --> 00:46:41,710
well let

719
00:46:41,730 --> 00:46:45,960
if i put these piecewise linear functions into this form

720
00:46:46,000 --> 00:46:52,460
what is going to happen

721
00:46:52,460 --> 00:46:55,440
problem we're going to have arrived

722
00:46:55,480 --> 00:46:58,520
because the derivative of piecewise linear is

723
00:46:58,600 --> 00:47:00,770
as piecewise

724
00:47:02,170 --> 00:47:04,080
and here i've got

725
00:47:04,140 --> 00:47:08,080
another derivative

726
00:47:08,100 --> 00:47:09,750
yes that's right

727
00:47:09,750 --> 00:47:14,830
so what i have done i just screwed up but but in one

728
00:47:14,900 --> 00:47:18,150
line by changing our world

729
00:47:18,210 --> 00:47:23,040
i can make myself correct

730
00:47:23,060 --> 00:47:27,000
we get this term was the

731
00:47:27,020 --> 00:47:31,690
so i'm going to put equal zero here OK

732
00:47:31,830 --> 00:47:32,810
see what

733
00:47:32,870 --> 00:47:37,400
i want this is the a weak form

734
00:47:37,420 --> 00:47:39,370
that's the weak form

735
00:47:39,380 --> 00:47:42,920
with before the integration by parts

736
00:47:42,960 --> 00:47:44,140
and now

737
00:47:44,150 --> 00:47:46,440
i could take you to be

738
00:47:46,500 --> 00:47:48,960
my piecewise linear

739
00:47:49,000 --> 00:47:52,830
and the derivative would be piecewise constant that's fine

740
00:47:52,880 --> 00:47:57,770
and what about the v which i take for these

741
00:47:57,810 --> 00:47:59,270
well i can i

742
00:47:59,290 --> 00:48:02,690
i could take the same thing for the visa right to take some other choice

743
00:48:02,690 --> 00:48:03,810
for vs

744
00:48:03,810 --> 00:48:07,460
but but this is so i'm sorry that that's in yellow and this is in

745
00:48:07,460 --> 00:48:10,460
why because this is the one

746
00:48:10,500 --> 00:48:11,770
and i should have been

747
00:48:11,790 --> 00:48:15,620
pointing to

748
00:48:15,710 --> 00:48:19,960
that's the form before i integrate by parts and

749
00:48:22,100 --> 00:48:24,120
and impose this

750
00:48:24,920 --> 00:48:30,790
additional derivative on you this was much better

751
00:48:31,690 --> 00:48:34,870
well i think are probably the simplest idea

752
00:48:34,870 --> 00:48:38,150
let this has to be true

753
00:48:38,190 --> 00:48:44,960
so you is a combination of these again wonder for these the CIP i

754
00:48:46,230 --> 00:48:48,370
and what should i take for re

755
00:48:48,420 --> 00:48:50,920
i'm looking for four equations

756
00:48:52,170 --> 00:48:53,810
i'm just going to take for v

757
00:48:54,440 --> 00:48:57,330
four test functions

758
00:49:00,980 --> 00:49:02,850
the be region right

759
00:49:02,870 --> 00:49:04,670
it's true for every v

760
00:49:04,710 --> 00:49:06,770
so i pick any four these

761
00:49:06,770 --> 00:49:10,230
and one natural choice in this sort of symmetric problem is

762
00:49:10,250 --> 00:49:13,080
let the test functions b the trial function

763
00:49:13,100 --> 00:49:18,500
but i don't have to make that choice so can i summarise this galerkin idea

764
00:49:18,520 --> 00:49:19,770
he's got an

765
00:49:19,790 --> 00:49:22,650
he chooses

766
00:49:23,980 --> 00:49:29,060
trial function

767
00:49:31,520 --> 00:49:34,120
and he chooses and

768
00:49:34,120 --> 00:49:35,690
test functions

769
00:49:38,960 --> 00:49:41,310
well if i don't say that word

770
00:49:43,560 --> 00:49:45,830
and those could be the same

771
00:49:45,920 --> 00:49:47,920
very often they are the same

772
00:49:48,040 --> 00:49:50,100
but they need to be the same

773
00:49:50,140 --> 00:49:51,770
and then

774
00:49:51,830 --> 00:49:53,650
his equations are

775
00:49:53,710 --> 00:49:56,290
this week he takes the weak form

776
00:49:56,330 --> 00:49:57,690
where he

777
00:49:57,710 --> 00:49:59,730
training be

778
00:49:59,770 --> 00:50:05,520
in turn it one of these sides

779
00:50:05,540 --> 00:50:07,040
so this is for j

780
00:50:07,040 --> 00:50:08,640
o one and two

781
00:50:08,640 --> 00:50:14,310
after n

782
00:50:14,330 --> 00:50:19,060
we've got to n equations in n unknowns that was our job

783
00:50:19,100 --> 00:50:21,730
we've got to n equations in n unknowns

784
00:50:21,770 --> 00:50:24,560
what are the unknowns there these weights

785
00:50:24,600 --> 00:50:26,290
one of the equations

786
00:50:26,290 --> 00:50:28,120
there the weak form

787
00:50:28,120 --> 00:50:30,210
for each

788
00:50:30,580 --> 00:50:32,440
test functions

789
00:50:32,460 --> 00:50:36,640
so we're testing it in time

790
00:50:41,060 --> 00:50:43,290
this is going to be

791
00:50:43,330 --> 00:50:45,810
that's four equations in four unknowns

792
00:50:45,810 --> 00:50:50,350
i was going to say the most interesting case is when you actually have contradictions

793
00:50:50,350 --> 00:50:52,980
in new knowledge base meaning it's not satisfiable

794
00:50:52,990 --> 00:50:55,370
this is the first order logic breaks down

795
00:50:55,380 --> 00:50:58,180
if we have a contradiction in the first on knowledge base you can prove anything

796
00:50:58,180 --> 00:50:59,470
from it

797
00:50:59,520 --> 00:51:01,650
this is basically what makes it so brittle

798
00:51:01,660 --> 00:51:04,930
and that makes it impossible to build large knowledge bases because you have to ensure

799
00:51:04,930 --> 00:51:06,360
that they consistent

800
00:51:06,360 --> 00:51:10,400
and makes it impossible to merge this from multiple sources without killing them all up

801
00:51:10,400 --> 00:51:11,950
and making inconsistent

802
00:51:12,240 --> 00:51:16,590
markov logic however has no problem with inconsistency

803
00:51:16,600 --> 00:51:21,940
but markov logic does is it weighs the evidence on either side of the probability

804
00:51:22,210 --> 00:51:26,880
so this is what we want by going you know to this more general language

805
00:51:26,910 --> 00:51:30,240
OK so now let's look at how we do inference

806
00:51:30,260 --> 00:51:32,770
in my collection

807
00:51:32,800 --> 00:51:36,880
so how do we do MEP MPE inference as a reminder the goal is to

808
00:51:36,880 --> 00:51:40,160
find the most likely state of world given evidence so

809
00:51:40,160 --> 00:51:44,720
well defined y that maximizes p of y given x where y is the query

810
00:51:44,720 --> 00:51:46,690
index is the evidence

811
00:51:46,810 --> 00:51:51,580
well let's replace p of y given x by the expression you know represented by

812
00:51:51,580 --> 00:51:55,940
an MLN so here's what we're trying to maximize is the expression that we have

813
00:51:55,940 --> 00:51:57,360
a couple of slides ago

814
00:51:57,360 --> 00:52:01,880
now this is a constant this is a monotonically increasing function so maximizing this whole

815
00:52:01,880 --> 00:52:04,890
thing is just the same thing as maximizing this

816
00:52:05,220 --> 00:52:08,690
no one is maximizing this

817
00:52:08,690 --> 00:52:12,330
what i'm trying to maximize here is the sum of the weights of the ground

818
00:52:12,330 --> 00:52:14,050
formulas that are satisfied

819
00:52:14,780 --> 00:52:17,520
well this is the a well known problem in computer science this is just the

820
00:52:17,520 --> 00:52:19,800
weighted maxsat problem

821
00:52:19,800 --> 00:52:24,590
so sad is finding as an assignment that satisfies the formulas max is finding a

822
00:52:24,610 --> 00:52:28,800
that an assignment that says as many forms as possible the weighted version of that

823
00:52:28,800 --> 00:52:30,830
is one every formula has away

824
00:52:31,140 --> 00:52:33,330
and so the nice thing is that

825
00:52:33,340 --> 00:52:35,610
in order to do MEP inference in

826
00:52:35,640 --> 00:52:40,270
markov logic we don't need to invent anything new we can just use a weighted

827
00:52:40,270 --> 00:52:42,690
SAT solver

828
00:52:42,720 --> 00:52:50,180
in particular we can use the weighted version of the walksat called maxwalksat

829
00:52:51,350 --> 00:52:54,300
the walksat is very scalable with

830
00:52:54,360 --> 00:52:58,070
it works can solve problems with millions of variables in minutes and i'm talking about

831
00:52:58,070 --> 00:53:01,640
the heart of these ones in arkansas of practically instantaneously

832
00:53:01,710 --> 00:53:06,350
so this is very fast but even better than that and surprisingly is that this

833
00:53:06,350 --> 00:53:11,400
can actually doing the MEP inference in markov logic can actually be faster

834
00:53:11,420 --> 00:53:13,390
the doing logical inference

835
00:53:13,430 --> 00:53:15,550
and the reason can be fast is the following

836
00:53:15,580 --> 00:53:19,400
the maxwalksat and looks almost the same with that

837
00:53:19,460 --> 00:53:22,460
if i take some of the constraints that were hard

838
00:53:22,520 --> 00:53:27,610
but should have been soft and make them soft now the problem becomes easier

839
00:53:27,670 --> 00:53:30,180
and i could potentially solve it in less time

840
00:53:30,270 --> 00:53:34,180
this is surprising because you think it for combining logic and probability the results should

841
00:53:34,180 --> 00:53:38,930
actually be less efficient than actually in this case you can actually be more efficient

842
00:53:38,950 --> 00:53:43,640
the maxwalksat is just walks set with a very simple change which is instead of

843
00:53:43,640 --> 00:53:47,420
trying to maximize the number of satisfied clauses at each step we're just trying to

844
00:53:47,420 --> 00:53:50,040
maximize the sum of the weights of the satisfied clauses

845
00:53:50,140 --> 00:53:53,960
and then we can also have a slightly different termination criterion but that's and that's

846
00:53:54,650 --> 00:53:57,450
so you can basically just take this out of the box and use it for

847
00:53:57,450 --> 00:54:03,210
the first not scalable inference in markov logic except there

848
00:54:03,230 --> 00:54:06,960
in this slag is this problem that already alluded to earlier of the memory explosion

849
00:54:06,960 --> 00:54:10,730
that you get when you try to run that you know

850
00:54:10,740 --> 00:54:14,920
if you take an MLN and try to propositionalized to then running you know maxwalksat

851
00:54:15,140 --> 00:54:18,420
there's a good chance that unless you the man is very small one even fit

852
00:54:18,420 --> 00:54:20,730
in memory and you know you and

853
00:54:20,740 --> 00:54:22,230
things will stop right there

854
00:54:22,240 --> 00:54:23,620
so what can you do

855
00:54:23,640 --> 00:54:29,390
what can you do is you can exploit the fact that relational domains

856
00:54:29,400 --> 00:54:31,050
tend to be very sparse

857
00:54:31,070 --> 00:54:33,110
what i mean by that

858
00:54:33,140 --> 00:54:35,580
think of the friends x y relation

859
00:54:35,580 --> 00:54:39,330
there are six billion people on earth which means there are thirty six billion billion

860
00:54:39,330 --> 00:54:41,120
possible groundings

861
00:54:41,150 --> 00:54:42,150
huge number

862
00:54:43,120 --> 00:54:46,870
most people have only in a few dozen maybe a couple hundred friends

863
00:54:46,890 --> 00:54:51,580
which means that the vast majority of the groundings of friends x y or false

864
00:54:51,600 --> 00:54:55,640
because of that the vast majority of the groundings of most clauses are going to

865
00:54:55,640 --> 00:54:57,280
be true

866
00:54:57,360 --> 00:55:03,210
because as long as a precondition in the classes false the clause is is satisfied

867
00:55:03,470 --> 00:55:07,860
so the vast majority of atoms are false the vast majority of classes are true

868
00:55:08,210 --> 00:55:13,240
what we can do is we can represent explicitly only the true atoms and the

869
00:55:13,240 --> 00:55:14,870
false clauses

870
00:55:14,880 --> 00:55:17,170
and that's a much much smaller things

871
00:55:17,180 --> 00:55:22,260
and what we can do when we do inference is we can ground clauses lazily

872
00:55:22,280 --> 00:55:24,360
we started with no clauses

873
00:55:24,360 --> 00:55:28,850
meaning that we assume that they are all satisfied and only is cause become satisfy

874
00:55:29,100 --> 00:55:33,040
potentially in satisfy during the inference doing ground

875
00:55:33,160 --> 00:55:37,310
this increases the order of magnitude in the size of the things they can do

876
00:55:37,310 --> 00:55:40,220
inference on by orders and orders of magnitude

877
00:55:40,230 --> 00:55:45,360
and you know the argument that this is called lazysat and even though it was

878
00:55:45,360 --> 00:55:51,490
motivated by markov logic it's actually a good thing for classical satisfiability problems like planning

879
00:55:51,500 --> 00:55:55,350
and again they you get very big payoffs by doing this

880
00:55:55,360 --> 00:55:57,240
so the other big task

881
00:55:57,290 --> 00:55:59,990
inference is computing probabilities

882
00:56:00,010 --> 00:56:03,660
in the most general type of query that you might want answers what's the problem

883
00:56:03,750 --> 00:56:08,620
what's the marginal probability of some formula given some mln or what's the conditional probability

884
00:56:08,620 --> 00:56:11,780
of some form of given another in

885
00:56:12,660 --> 00:56:13,940
now in principle

886
00:56:13,970 --> 00:56:18,100
a question like this is easy to answer because the probability of the formula is

887
00:56:18,100 --> 00:56:22,530
just the sum of the probabilities of the world where the formula is true

888
00:56:22,550 --> 00:56:25,730
the problem of course is that in doing that in the brute force way is

889
00:56:25,730 --> 00:56:29,990
just not feasible what you can do however is you something like markov chain monte

890
00:56:29,990 --> 00:56:32,110
carlo to sample worlds

891
00:56:32,190 --> 00:56:35,840
and then i check ritual the formula holds or not

892
00:56:35,850 --> 00:56:40,410
and the probability of the formula is just the fraction of world where we're told

893
00:56:40,670 --> 00:56:45,230
and if conditioning on another formula what i can do is i i i first

894
00:56:45,230 --> 00:56:48,860
checked with another formula holden if it doesn't i ignore that sample

895
00:56:48,870 --> 00:56:52,650
and then i'm only counting when the samples of the second formula

896
00:56:52,670 --> 00:56:56,800
now if you evidence to formula two

897
00:56:56,860 --> 00:57:00,400
is a conjunction of ground atoms is typically the case for a bunch of facts

898
00:57:00,400 --> 00:57:03,290
that you know that it's the symptoms of the patient or the words on the

899
00:57:03,290 --> 00:57:07,210
page and the links and whatnot you can do something even better which is i

900
00:57:07,210 --> 00:57:11,490
can construct only the minimum subset of the network that i need to answer the

901
00:57:13,460 --> 00:57:16,900
but i mean i know suppose i want question about you only need to worry

902
00:57:16,900 --> 00:57:20,490
about you and your friends not the other you know five hundred nine hundred and

903
00:57:20,490 --> 00:57:22,430
something to people on

904
00:57:22,460 --> 00:57:25,460
so this is another way in which you know we can we can really scale

905
00:57:25,480 --> 00:57:28,800
to larger domains is shown the whole world is very large but most of the

906
00:57:28,800 --> 00:57:31,960
time we only need to worry about a small fraction of

907
00:57:32,000 --> 00:57:36,750
in fact this is a generalisation of the the knowledge based model construction idea

908
00:57:36,780 --> 00:57:40,070
and then once i had that small network i can apply MCMC or some other

909
00:57:40,070 --> 00:57:42,420
difference of OK

910
00:57:42,490 --> 00:57:46,560
there is another type of approach that we we can follow that people have only

911
00:57:46,560 --> 00:57:50,060
begun to look at which is lifted inference we we reason at the level of

912
00:57:50,060 --> 00:57:53,800
whole classes of objects in resolution in first order logic

913
00:57:53,820 --> 00:57:57,350
i want to talk about that here but you know it's interesting direction

914
00:57:57,360 --> 00:58:00,880
so how do we create the minimum ground network that we need to answer a

915
00:58:01,900 --> 00:58:03,170
very simple

916
00:58:03,180 --> 00:58:07,740
i start with an empty network and with the queen nodes in a queue

917
00:58:07,750 --> 00:58:11,530
and then i the following are taken from the front of the queue and added

918
00:58:11,530 --> 00:58:14,610
to the network and check whether it's in the evidence

919
00:58:14,660 --> 00:58:17,360
if it's in the evidence i know its value and i can stop

920
00:58:17,400 --> 00:58:20,130
if it's not the evidence i need to take its neighbors

921
00:58:20,160 --> 00:58:21,850
and put them in the queue

922
00:58:21,850 --> 00:58:24,460
i'm back to talk about

923
00:58:24,480 --> 00:58:29,300
while gaston distributions

924
00:58:29,310 --> 00:58:30,840
so either

925
00:58:30,870 --> 00:58:33,030
it's an interesting giving half the talk and then

926
00:58:33,040 --> 00:58:36,820
three days passing because you bump into a lot of people and has some interesting

927
00:58:36,820 --> 00:58:38,420
interactions about what's going on so some people

928
00:58:38,920 --> 00:58:41,780
some people said to me you know

929
00:58:41,800 --> 00:58:46,190
i i get it i understand what's what's going on right i i realize that

930
00:58:46,190 --> 00:58:49,690
it's not so easy to understand and some you might still be struggling and thinking

931
00:58:49,690 --> 00:58:54,900
this is this is this is mysterious that's all i apologize you for not

932
00:58:54,930 --> 00:58:58,220
really knowing

933
00:58:58,230 --> 00:59:00,370
how to tell you

934
00:59:00,520 --> 00:59:03,730
what's going on in a bit better way but if you're feeling a little uneasy

935
00:59:03,730 --> 00:59:05,620
about what's going on and don't despair

936
00:59:05,650 --> 00:59:11,080
you can i think this is this is a very natural when we talk about

937
00:59:11,080 --> 00:59:15,000
this kind of stuff that today actually i won't i won't talk about much more

938
00:59:15,000 --> 00:59:18,830
stuff but i'll try and talk about the same stuff and a few other ways

939
00:59:18,830 --> 00:59:23,410
and maybe that's gonna make that's going to help you this so actually gave i

940
00:59:23,410 --> 00:59:25,370
give a course on on machine learning once

941
00:59:25,840 --> 00:59:30,960
and i was supposed to prepare ten talks but actually i only got around to

942
00:59:30,960 --> 00:59:31,970
preparing nine

943
00:59:31,990 --> 00:59:36,660
so that can happen and then at the end i was sort of had given

944
00:59:36,660 --> 00:59:41,050
the last lecture i don't i i didn't have another lecture so i know i

945
00:59:41,050 --> 00:59:44,270
didn't really know what to do so i just decided just to give the last

946
00:59:44,270 --> 00:59:48,770
lecture again because i don't know what to do right so that was pretty embarrassing

947
00:59:48,770 --> 00:59:52,800
but it but people really liked it i just happened to be the gaussianprocess stuff

948
00:59:52,800 --> 00:59:56,080
right and i just said the same thing the same stuff where i can explain

949
00:59:56,190 --> 00:59:57,230
any way

950
00:59:57,370 --> 01:00:00,680
so i'm going to try and go to try and explain if i could give

951
01:00:00,680 --> 01:00:02,520
the same talk again but actually

952
01:00:02,540 --> 01:00:03,990
i i've got it

953
01:00:04,010 --> 01:00:08,130
i'm going to use some slightly different slides this time

954
01:00:08,130 --> 01:00:11,430
OK so there are a lot of good a good questions from people that have

955
01:00:11,430 --> 01:00:14,960
been talking to you should use interrupt me as i go along and even if

956
01:00:14,960 --> 01:00:18,290
you have questions that are not directly related to on talking about that's fine as

957
01:00:18,290 --> 01:00:20,020
long as the related stochastic processes

958
01:00:20,040 --> 01:00:25,680
so one of the questions was you know it's a bit fishy that i i

959
01:00:25,680 --> 01:00:30,730
talk about these i had these infinite dimensional gaus processes are infinite dimensional distributions and

960
01:00:30,730 --> 01:00:34,540
then in the end i want to talk about would like proper gauss in distribution

961
01:00:34,540 --> 01:00:39,540
the multivariate constitution distribution demanding we can i just couldn't just stick with the usual

962
01:00:39,540 --> 01:00:43,070
stuff and there are two reasons why you can do that so one of the

963
01:00:43,070 --> 01:00:43,970
reasons is

964
01:00:44,000 --> 01:00:47,720
beforehand you don't know where you're going to make predictions right so you don't know

965
01:00:47,730 --> 01:00:50,760
what the identity of those random variables are right so it so it's better to

966
01:00:50,760 --> 01:00:53,970
be the case that you can figure out what's going on with respect to those

967
01:00:53,970 --> 01:00:55,880
random variables as well

968
01:00:55,890 --> 01:00:59,950
and that's why use only to the the process there's no there's no good reason

969
01:00:59,950 --> 01:01:04,690
which i'm going to talk about why why infinite dimensional models are generally

970
01:01:05,090 --> 01:01:10,160
preferable to to finite dimensional ones so this statistical reason and i'll try and in

971
01:01:10,160 --> 01:01:11,820
and come back to that

972
01:01:11,860 --> 01:01:16,510
so also you'll notice that i have an incredibly sloppy attitude to

973
01:01:16,540 --> 01:01:20,730
two mathematics right so this morning you why did a very good job of actually

974
01:01:20,730 --> 01:01:25,160
telling us you know why it is that this that this construction talking about the

975
01:01:25,540 --> 01:01:30,010
process that we're white actually exists in wide so why it's a good thing to

976
01:01:30,070 --> 01:01:33,790
to talk about and i haven't i haven't haven't told anything about whether these things

977
01:01:33,790 --> 01:01:39,310
exist actually peter orbanz in our lab he told me a few months ago that

978
01:01:39,310 --> 01:01:41,130
gaussianprocess actually exist

979
01:01:42,640 --> 01:01:44,750
and i realized that i had never

980
01:01:44,760 --> 01:01:45,970
worried about that

981
01:01:45,980 --> 01:01:49,380
so i thought it was it was great that they exist but actually to be

982
01:01:49,380 --> 01:01:52,690
honest with you if you told me that they don't exist that would also be

983
01:01:52,690 --> 01:01:57,010
fine with me because i know that it works

984
01:02:03,090 --> 01:02:07,570
right so let's let's i can take some questions now we can we can try

985
01:02:07,570 --> 01:02:12,200
to get the ball rolling again so that the this space with the place where

986
01:02:12,200 --> 01:02:15,510
we left off was that i was a change this expression of the to look

987
01:02:15,510 --> 01:02:19,170
a little bit more like the one that was supposed to look like on the

988
01:02:19,190 --> 01:02:23,810
slide so i was trying to illustrate this cartoon example that what we actually doing

989
01:02:23,810 --> 01:02:28,140
in the casting process is just fitting which is fitting actually with maximum likelihood to

990
01:02:28,140 --> 01:02:32,190
this so this is called the marginal likelihood with if you if you think about

991
01:02:33,030 --> 01:02:38,370
marginalizing over the over the over the parameters over the functions but if you just

992
01:02:38,370 --> 01:02:41,790
think about in this space in this space then it's just the likelihood of the

993
01:02:41,790 --> 01:02:47,460
data so so so one person's marginal likelihood might be the another person's likelihood that

994
01:02:47,460 --> 01:02:51,910
you can introduce latent variables and then marginalize over them and so that might be

995
01:02:52,300 --> 01:02:55,980
so so so you should fight about what what this is the marginal likelihood are

996
01:02:55,980 --> 01:03:00,040
likely be right this way it just looks like a likelihood but it pops out

997
01:03:00,160 --> 01:03:04,360
when you marginalise out the functions but you don't have to refer back to the

998
01:03:04,360 --> 01:03:09,160
functions again right nobody forces you to actually have that interpretation

999
01:03:09,180 --> 01:03:12,450
and i was and i was and we we're looking at this little example i

1000
01:03:12,450 --> 01:03:15,600
was saying well if you if you tell me that the best estimate for the

1001
01:03:15,600 --> 01:03:20,410
variance here is actually the empirical variance something which is called the empirical variance then

1002
01:03:20,410 --> 01:03:24,540
we're all happy about that that kind of statement and actually what's going on in

1003
01:03:24,540 --> 01:03:27,550
the gaps process is just that your somehow

1004
01:03:27,560 --> 01:03:31,700
you somehow using this big galson distribution that lives in this very high dimensional space

1005
01:03:32,100 --> 01:03:36,530
using you're trying to model to fit the parameters of the house process such that

1006
01:03:36,530 --> 01:03:41,550
that multivariate gaus in cloud fictional cloud of data

1007
01:03:41,640 --> 01:03:45,390
in this case i can draw because it's in one dimension like normally i can

1008
01:03:45,390 --> 01:03:46,390
draw it for you

1009
01:03:46,920 --> 01:03:50,210
so i think that i really liked about david livestock is that he talked a

1010
01:03:50,210 --> 01:03:53,850
lot about you know what is the posterior look like i thought i would try

1011
01:03:53,850 --> 01:03:57,490
to to right so i talked quite a lot about what does the posterior look

1012
01:03:58,390 --> 01:04:02,390
and and i think one of the reasons that

1013
01:04:02,420 --> 01:04:06,360
i was saying that is hard to understand what's going on the model and what

1014
01:04:06,390 --> 01:04:07,410
is really

1015
01:04:07,470 --> 01:04:12,540
understanding what's going on the model i think just and i'm talking he was talking

1016
01:04:12,540 --> 01:04:18,450
about this intuitive statisticians like as a really nice way of thinking about what understanding

1017
01:04:18,450 --> 01:04:24,220
means so you understand something when you're intuitive statisticians agrees with what's going on right

1018
01:04:24,220 --> 01:04:27,490
if you can predict what's what's going to

1019
01:04:28,100 --> 01:04:31,230
happen and then you're then you're happy about what's going on right if it doesn't

1020
01:04:31,490 --> 01:04:35,930
it is not able to predict what's happening right then then during the mysterious domain

1021
01:04:35,930 --> 01:04:40,680
right then you're still you know in give incorporating data into the model

1022
01:04:40,990 --> 01:04:45,350
OK so now i'm going to talk about a different way of thinking about thousand

1023
01:04:45,350 --> 01:04:50,710
process where we actually constructing distributions over functions by very explicitly doing that and in

1024
01:04:50,710 --> 01:04:55,450
the in the function domain and then i'll show that those kind of constructions also

1025
01:04:55,450 --> 01:04:57,270
correspond to gas process

1026
01:04:57,290 --> 01:04:59,410
so here a list of the simple one

1027
01:04:59,430 --> 01:05:03,720
OK so i consider classify actions here and this is just a linear functions right

1028
01:05:03,720 --> 01:05:07,980
so just the function i'm looking at is a express b but now a and

1029
01:05:07,990 --> 01:05:13,030
b are random parameters OK so what if i can be around and then i

1030
01:05:13,030 --> 01:05:15,460
get a distribution over functions right

1031
01:05:15,470 --> 01:05:18,480
each time are beginning to be again to function

1032
01:05:18,490 --> 01:05:22,680
so we can look at so an output output galson priors on and gauss implies

1033
01:05:22,700 --> 01:05:25,970
and b so i have zero mean

1034
01:05:26,710 --> 01:05:29,340
prior here with

1035
01:05:29,910 --> 01:05:33,540
variance of alpha for a another variance of beta for b

1036
01:05:33,590 --> 01:05:37,590
and now i can compute i can compute things like i can compute the mean

1037
01:05:37,590 --> 01:05:41,840
function OK so the mean function of column mu x here the mean function function

1038
01:05:41,840 --> 01:05:43,230
is just the expectation

1039
01:05:43,250 --> 01:05:44,640
of the function

1040
01:05:44,640 --> 01:05:45,870
friends who have

1041
01:05:48,690 --> 01:05:54,370
another example that given in the same paper is the problem the publishing in conferences

1042
01:05:54,430 --> 01:05:58,620
like if you have if you wanted have a number of coauthors published in ICML

1043
01:05:58,620 --> 01:05:59,980
the probability that you

1044
01:06:00,000 --> 01:06:04,120
publishing ICML that that's going to be an increasing function of the number of quarters

1045
01:06:04,120 --> 01:06:06,580
in order to have enough conference

1046
01:06:06,610 --> 01:06:08,270
again this is

1047
01:06:10,230 --> 01:06:12,100
mister scruff this

1048
01:06:12,140 --> 01:06:16,990
again there is that there is a similar graphs showing

1049
01:06:17,020 --> 01:06:23,510
there's the paper for example mentioning this in particular because the experimental results

1050
01:06:23,580 --> 01:06:28,740
present are actually related to the current tagging data so the tagging what cavalier

1051
01:06:28,750 --> 01:06:32,850
a lot of web two o two point o systems nowadays are based on the

1052
01:06:32,850 --> 01:06:38,180
notion of tag like that you can put something there and you and other people

1053
01:06:38,180 --> 01:06:42,880
can tag is that helps people to find this information online

1054
01:06:42,940 --> 01:06:48,130
and it's observed that tagging which cabinet people use there is a large correlation if

1055
01:06:48,130 --> 01:06:49,730
they are friends with each other they

1056
01:06:49,850 --> 01:06:52,390
and use the same wars describe objects

1057
01:06:52,420 --> 01:06:56,250
this is the paper by mahler and others

1058
01:06:56,310 --> 01:07:00,350
and so this this is actually the graph from the paper

1059
01:07:00,380 --> 01:07:02,570
this is the red line

1060
01:07:02,710 --> 01:07:07,940
represents the overlap of the tagging vocabulary of two users

1061
01:07:07,990 --> 01:07:09,600
that our friends to each other

1062
01:07:09,610 --> 01:07:15,360
and the other line the bottom line overlapping the tagging vocabulary of two random users

1063
01:07:15,370 --> 01:07:19,760
and as you see the the red curve is essentially shifted to the right which

1064
01:07:19,940 --> 01:07:23,440
the writing which i believe overlap quite a bit

1065
01:07:23,450 --> 01:07:30,910
and finally another example is the adoption of paid voice over IP services and the

1066
01:07:30,910 --> 01:07:32,310
centre for example

1067
01:07:32,520 --> 01:07:37,420
i'm a young messenger you can pay and sign up for the premium service that

1068
01:07:37,420 --> 01:07:38,430
allows you to

1069
01:07:39,640 --> 01:07:41,370
using voice over IP

1070
01:07:41,370 --> 01:07:45,690
and we looked at the data and you can actually see the all that

1071
01:07:45,700 --> 01:07:49,160
if you have a large number of friends that are already using this voice over

1072
01:07:49,160 --> 01:07:52,680
IP service are much more likely to be used

1073
01:07:52,700 --> 01:07:57,950
because of this is this is not really a surprise see that every river and

1074
01:07:58,050 --> 01:08:03,180
document in many instances but one thing that is not entirely clear is what the

1075
01:08:03,180 --> 01:08:05,210
source of the correlation

1076
01:08:05,870 --> 01:08:11,810
i'm mentioning important categories here for the source of correlation that we observed with your

1077
01:08:11,810 --> 01:08:13,750
contacts social

1078
01:08:13,750 --> 01:08:18,020
one is the probably not the most obvious one is the social influence fact that

1079
01:08:18,040 --> 01:08:23,270
one person performing an action can induce his or her friends to do the same

1080
01:08:24,060 --> 01:08:25,270
like the same way

1081
01:08:25,290 --> 01:08:29,370
and this can be done by providing information like buying in new product your friends

1082
01:08:29,370 --> 01:08:32,380
mind i know about it but you tell them about it and they might go

1083
01:08:32,380 --> 01:08:34,180
and buy the product

1084
01:08:34,200 --> 01:08:37,300
or it could be way increasing the value of the action

1085
01:08:37,330 --> 01:08:42,310
this is called an economic jargon this is called the adoption externalities the fact that

1086
01:08:42,750 --> 01:08:46,810
for example if i knew using yahoo messenger AOL instant messenger

1087
01:08:46,820 --> 01:08:48,920
you will derive more utility

1088
01:08:49,010 --> 01:08:54,960
by using the same instant messaging software as opposed to a different set of or

1089
01:08:54,960 --> 01:08:58,130
for example if you have a lot of friends that have cell phones you're more

1090
01:08:58,130 --> 01:09:00,590
likely to benefit of having

1091
01:09:00,670 --> 01:09:02,850
doing business

1092
01:09:02,940 --> 01:09:07,700
so this is one of the categories this is essentially the category that probably the

1093
01:09:07,700 --> 01:09:12,640
most important one the one that there is an element of causation here you

1094
01:09:12,680 --> 01:09:17,780
using something you adopting some technology we were looking for a new norms culture thing

1095
01:09:18,290 --> 01:09:20,490
can induce your friends

1096
01:09:22,150 --> 01:09:25,640
another source of correlation is the number of

1097
01:09:25,680 --> 01:09:31,510
monthly by the similar individuals are more likely to become friends well for example if

1098
01:09:31,510 --> 01:09:32,570
you study

1099
01:09:32,580 --> 01:09:37,340
the social network among scientists probably only to discover that mathematicians are likely to be

1100
01:09:37,340 --> 01:09:39,960
friends but that doesn't necessarily mean that

1101
01:09:40,060 --> 01:09:46,940
if i am a mathematician and if i if i want to become a mathematician

1102
01:09:46,940 --> 01:09:53,540
five become friends and additions that increases might like my life without becoming it's probably

1103
01:09:53,540 --> 01:09:58,480
mostly because mathematicians are just more like a more likely to come from

1104
01:10:00,210 --> 01:10:03,200
finally the third reason that we we can observe

1105
01:10:03,360 --> 01:10:06,700
correlation in the process

1106
01:10:06,720 --> 01:10:11,940
factors that we don't observe essentially external elements in a mormon can influence both individuals

1107
01:10:11,940 --> 01:10:14,840
so it is in this last part of the

1108
01:10:14,860 --> 01:10:19,410
the lecture this morning we talk about optimisation

1109
01:10:19,420 --> 01:10:21,360
OK i have i've drawn a few

1110
01:10:21,430 --> 01:10:23,210
functions here

1111
01:10:23,270 --> 01:10:24,960
from two are

1112
01:10:25,020 --> 01:10:27,640
and so

1113
01:10:27,660 --> 01:10:30,410
there are a few things that i wanted to show you

1114
01:10:30,500 --> 01:10:32,230
so let's think about the

1115
01:10:32,230 --> 01:10:33,680
and that are here

1116
01:10:37,710 --> 01:10:39,770
so i'm going to do is to

1117
01:10:39,800 --> 01:10:42,270
find the global minimum of the function

1118
01:10:42,320 --> 01:10:45,340
what this local minima of the function

1119
01:10:45,340 --> 01:10:46,720
that you have here

1120
01:10:46,730 --> 01:10:51,120
and i have drawn some function therefore which should

1121
01:10:51,120 --> 01:10:52,810
may not be

1122
01:10:52,830 --> 01:10:54,980
that easy to find them

1123
01:10:55,810 --> 01:11:00,800
there there are several ways were

1124
01:11:01,040 --> 01:11:03,800
you could face for

1125
01:11:03,810 --> 01:11:07,800
finding the minimum of the function of the first one being that the minimum may

1126
01:11:07,800 --> 01:11:09,660
not exist

1127
01:11:09,670 --> 01:11:12,120
if you think that the function that is that

1128
01:11:12,160 --> 01:11:16,820
gary's and then use it and you won't find the global minimum right

1129
01:11:16,900 --> 01:11:23,730
goes to minus infinity as x goes to zero and then

1130
01:11:23,740 --> 01:11:26,930
so it's useless to try to

1131
01:11:27,340 --> 01:11:34,200
to try to find if you just afraid to minimize

1132
01:11:34,210 --> 01:11:37,520
OK if you have

1133
01:11:37,570 --> 01:11:39,320
this function here in red

1134
01:11:39,340 --> 01:11:42,540
there's definitely a local minimum here

1135
01:11:42,570 --> 01:11:49,650
but it's not global because the the function goes to minus infinity here

1136
01:11:49,680 --> 01:11:53,520
you have to be aware that when you look on minimum then the they might

1137
01:11:54,410 --> 01:11:58,090
local or global say that if the

1138
01:11:58,100 --> 01:12:01,760
the point is that the global minimum is

1139
01:12:01,790 --> 01:12:02,550
if o

1140
01:12:03,840 --> 01:12:07,990
why difference than than x then f of

1141
01:12:08,050 --> 01:12:10,260
so why is bigger than f of x

1142
01:12:11,400 --> 01:12:14,790
obviously this one here in the blue colour is a local minimum

1143
01:12:14,800 --> 01:12:16,770
is a global minimum

1144
01:12:20,730 --> 01:12:23,750
OK so

1145
01:12:23,810 --> 01:12:27,030
let's assume that

1146
01:12:27,080 --> 01:12:27,880
you know

1147
01:12:27,920 --> 01:12:31,280
there's going to be at least some local minima in your functions

1148
01:12:31,290 --> 01:12:34,820
well how do you how are you going to find it

1149
01:12:34,850 --> 01:12:39,010
you have new ways to find it

1150
01:12:39,010 --> 01:12:43,670
it was the first thing you do is you want to find the minimum

1151
01:12:43,940 --> 01:12:45,110
i differentiate

1152
01:12:45,160 --> 01:12:48,410
so if your function is nice and differentiable

1153
01:12:48,450 --> 01:12:55,730
then you're going to differentiate your function and what you're going to find it is

1154
01:12:55,880 --> 01:13:05,230
all the points where the different and different the derivative is zero otherwise all minimum

1155
01:13:06,130 --> 01:13:06,940
they are not

1156
01:13:06,950 --> 01:13:08,640
right OK they're not

1157
01:13:08,780 --> 01:13:10,530
obviously they not ready

1158
01:13:10,540 --> 01:13:16,060
because local maxima are points where the different everything derivatives you as well

1159
01:13:16,100 --> 01:13:16,970
and then

1160
01:13:16,980 --> 01:13:19,220
there is another points

1161
01:13:19,230 --> 01:13:23,880
which of course at the points where the derivative may be easier

1162
01:13:23,920 --> 01:13:25,260
OK so

1163
01:13:25,320 --> 01:13:31,140
if you have a function you have no other information on the function and you

1164
01:13:31,140 --> 01:13:34,440
just take the derivative then you're not done too

1165
01:13:34,470 --> 01:13:36,610
to make sure that you have the minimum

1166
01:13:37,600 --> 01:13:41,160
the minimum if you

1167
01:13:41,230 --> 01:13:46,200
have the derivative and the of your function is twice differentiable like what can you

1168
01:13:46,200 --> 01:13:48,160
do to make sure it's a minimum

1169
01:13:48,480 --> 01:13:54,130
it's the minimum of the sign of the second derivative is

1170
01:13:54,170 --> 01:13:56,070
but was you

1171
01:13:56,130 --> 01:13:58,060
what to me

1172
01:13:59,130 --> 01:14:01,100
so it's a minimum if

1173
01:14:01,100 --> 01:14:07,540
you and your derivative is

1174
01:14:07,600 --> 01:14:09,940
getting bigger and bigger and bigger

1175
01:14:09,940 --> 01:14:13,420
which means that the second derivative is positive

1176
01:14:17,980 --> 01:14:22,570
so an

1177
01:14:22,570 --> 01:14:26,450
OK so you have a function you need to compute the derivative

1178
01:14:26,470 --> 01:14:31,240
you compute the second derivative and you find that the security and derivative is strictly

1179
01:14:31,240 --> 01:14:32,490
positive then

1180
01:14:32,590 --> 01:14:34,270
sure that you have a minute

1181
01:14:35,920 --> 01:14:42,640
and i know way that you know that this minimum is local or global

1182
01:14:42,670 --> 01:14:47,030
one there's actually i don't think there's a way and so then if you know

1183
01:14:47,700 --> 01:14:53,180
but i don't think there's a way without further assumptions to know if the minimum

1184
01:14:53,180 --> 01:14:56,710
your you found is global or local

1185
01:14:56,740 --> 01:14:59,900
right other than you find several

1186
01:15:00,110 --> 01:15:04,720
a local minima and you look at which one is the is the

1187
01:15:04,770 --> 01:15:05,920
the most meaning

1188
01:15:08,390 --> 01:15:15,780
even when everything's alright then you could differentiate and you know that the second derivative

1189
01:15:15,780 --> 01:15:17,750
is strictly positive then

1190
01:15:18,940 --> 01:15:23,700
you're not exactly sure with which i mean i you found OK

1191
01:15:27,110 --> 01:15:29,320
the next thing that

1192
01:15:34,570 --> 01:15:35,380
OK so

1193
01:15:35,390 --> 01:15:39,060
you could wonder what are the minimum conditions

1194
01:15:39,130 --> 01:15:43,200
that you should have on as one your function f so that you know that

1195
01:15:43,200 --> 01:15:44,280
there is

1196
01:15:44,330 --> 01:15:45,820
only one local

1197
01:15:45,820 --> 01:15:47,580
one global minimum

1198
01:15:47,670 --> 01:15:49,900
and what would that be

1199
01:15:49,900 --> 01:15:54,360
and there's no condition on how it depends on the second variable Y

1200
01:15:54,400 --> 01:15:58,770
and in fact Y can be an integer or constraint to be integer or can be a vector or

1201
01:15:59,010 --> 01:16:01,530
matrix it doesn't matter

1202
01:16:01,710 --> 01:16:03,660
and then we take the maximum

1203
01:16:03,660 --> 01:16:07,910
Of this function to evaluate this new function G of X you take the maximum of

1204
01:16:07,920 --> 01:16:11,180
F of X and Y over all possible Y in some sent A

1205
01:16:11,190 --> 01:16:16,730
and again there's no restriction on what A is it doesn't have to be convex or connected or

1206
01:16:16,750 --> 01:16:18,360
there's no

1207
01:16:18,400 --> 01:16:19,940
condition on A

1208
01:16:19,950 --> 01:16:22,690
well that's always convex

1209
01:16:22,730 --> 01:16:23,730
in X there is

1210
01:16:23,790 --> 01:16:24,950
the resulting function

1211
01:16:24,990 --> 01:16:30,420
so the previous example is where Y is just the index

1212
01:16:30,420 --> 01:16:33,860
or Y is an integer and the set A is a set of

1213
01:16:34,010 --> 01:16:36,860
From integers from one to M

1214
01:16:36,920 --> 01:16:42,970
and I use here in this notes we use the supremum if we use a maximum

1215
01:16:43,010 --> 01:16:47,600
for the Maximum over a finite set and the supremum over a set that could be

1216
01:16:48,550 --> 01:16:50,010
so that's the definition of

1217
01:16:50,120 --> 01:16:53,680
and if you like it you can just replaced by Max just keep in mind that it can be

1218
01:16:54,320 --> 01:16:57,660
A doesn't have to be a finite set

1219
01:16:57,970 --> 01:17:04,050
so a very good example of this is the maxim eigen value of a symmetric matrix which

1220
01:17:04,050 --> 01:17:06,950
is a convex function of X

1221
01:17:06,970 --> 01:17:10,750
and that's easy to see so very short proof is this so you know from linear

1222
01:17:10,790 --> 01:17:15,400
algebra that the maximum eigen value can be expressed like this it's the maximum of Y

1223
01:17:15,400 --> 01:17:17,160
transpose X Y

1224
01:17:17,190 --> 01:17:21,900
if you take the maximum over all vectors Y with unit norm

1225
01:17:21,900 --> 01:17:25,680
so in this case this is the set A

1226
01:17:25,690 --> 01:17:27,470
the units sphere

1227
01:17:28,240 --> 01:17:31,510
and this is the function F of X and Y

1228
01:17:31,550 --> 01:17:35,380
so for fixed Y it's clear that that's a linear function of X

1229
01:17:35,400 --> 01:17:40,080
if you expand these products you get this linear function of the entries and X

1230
01:17:40,080 --> 01:17:44,940
so for fixed Y there's a linear or and therefore also convex function of X

1231
01:17:44,950 --> 01:17:49,010
so then we know from this property that if you take the maximum of these functions

1232
01:17:49,010 --> 01:17:53,270
over any set Y you get automatically a convex function of X

1233
01:17:53,290 --> 01:17:59,960
and you can easily think of some other equivalent definitions of the maxim eigen value for

1234
01:17:59,960 --> 01:18:03,680
which it's not immediately obvious that it's convex for example if you think of it as

1235
01:18:03,680 --> 01:18:08,820
the largest root of the characteristic polynomial then it's not clear at all that if you

1236
01:18:08,820 --> 01:18:11,060
change the entries in the matrix X

1237
01:18:11,100 --> 01:18:17,230
that that largest root is a convex function of the entries it's actually a very complicated

1238
01:18:19,230 --> 01:18:26,510
so that's a maximization rule so you have a function of two variables X and Y we

1239
01:18:26,510 --> 01:18:31,100
maximize over the second and the condition was that the function had to be convex

1240
01:18:31,100 --> 01:18:34,420
in the first variable X for a given Y

1241
01:18:34,450 --> 01:18:38,600
there's something that looks very similar and refers to minimization

1242
01:18:39,140 --> 01:18:41,010
or the infimum

1243
01:18:41,030 --> 01:18:45,710
so here again we take a function of two variables X and Y and under some conditions

1244
01:18:45,710 --> 01:18:50,660
we can conclude that the minimum over Y gives you convex function of X

1245
01:18:50,690 --> 01:18:54,730
so it's very similar to this maximization rule on the previous page

1246
01:18:55,320 --> 01:18:59,660
but the conditions are much more restrictive so here the conditions are at F has to be

1247
01:18:59,660 --> 01:19:01,660
jointly convex in X and Y

1248
01:19:01,660 --> 01:19:05,990
not just convex in X for a given Y but it has to be jointly convex

1249
01:19:06,010 --> 01:19:07,900
in X and Y as a variable

1250
01:19:08,340 --> 01:19:14,920
and also if there are constraints in the optimization has to be constraints over a convex set C

1251
01:19:15,190 --> 01:19:19,860
but if that's the case then you can conclude that

1252
01:19:19,860 --> 01:19:22,640
sorry H is convex

1253
01:19:23,120 --> 01:19:25,120
so again there are some

1254
01:19:25,170 --> 01:19:30,730
simple useful examples suppose you take the distance to a convex set C

1255
01:19:30,970 --> 01:19:34,440
in any norm that's always a convex function of X

1256
01:19:35,040 --> 01:19:39,710
because this is can be written like this it's the minimum distance

1257
01:19:39,730 --> 01:19:42,710
from X to any point in C

1258
01:19:42,950 --> 01:19:48,100
we've seen that norms are always convex so the norm of X minus Y is jointly convex

1259
01:19:48,120 --> 01:19:50,600
in X and Y

1260
01:19:50,620 --> 01:19:54,400
C is a convex set so if you take the minimum distance you get

1261
01:19:54,400 --> 01:19:57,270
a convex function of the remaining variable X

1262
01:19:57,400 --> 01:20:03,360
and that's true for any norm but only for convex sets

1263
01:20:03,920 --> 01:20:09,680
another one that's very useful in optimization is as follows so suppose we take a

1264
01:20:09,680 --> 01:20:12,990
linear programming problem in Y S variable

1265
01:20:13,210 --> 01:20:18,620
so I take minimize C transpose Y with constraints that A Y is less that X so X is

1266
01:20:18,620 --> 01:20:20,860
just the right hand side in inequalities

1267
01:20:20,860 --> 01:20:25,920
and I define as the function H the

1268
01:20:25,970 --> 01:20:31,010
or assign I assign to H the optimal value of this linear programming problem

1269
01:20:31,030 --> 01:20:34,100
as a function of the righthand side X

1270
01:20:34,100 --> 01:20:36,730
so to evaluate this function in general

1271
01:20:36,750 --> 01:20:39,670
you would have to add a certain X

1272
01:20:40,030 --> 01:20:43,860
you'd have to pick X and solve this LP and Y and then the optimal

1273
01:20:43,860 --> 01:20:46,840
value is the function value at X

1274
01:20:46,920 --> 01:20:52,660
so there's no closed-form expression of this H of X but you can compute it by solving

1275
01:20:52,670 --> 01:20:53,490
an LP

1276
01:20:53,680 --> 01:20:55,400
X as righthand side

1277
01:20:56,360 --> 01:21:02,880
but one useful property that's important in optimization is that if you define

1278
01:21:02,880 --> 01:21:04,380
a function like this

1279
01:21:04,740 --> 01:21:08,290
then this is a convex function of X

1280
01:21:08,340 --> 01:21:12,420
so the optimal value of a linear programming problem is a convex function of it's

1281
01:21:12,790 --> 01:21:15,950
the righthand sides in the constraints

1282
01:21:16,030 --> 01:21:20,320
and that follows from the same rule so in this case we define F of X and Y

1283
01:21:20,320 --> 01:21:26,490
as a function that's has this as its domain so it's only defied defined

1284
01:21:26,490 --> 01:21:32,440
for pairs X and Y that satisfy these that satisfy these inequalities and outside the domain we can

1285
01:21:32,440 --> 01:21:34,250
give it a value plus infinity

1286
01:21:34,750 --> 01:21:39,940
and then on its domain when X and Y satisfy these inequalities we

1287
01:21:39,970 --> 01:21:41,950
define it as C transpose Y

1288
01:21:42,150 --> 01:21:47,420
so if you do that you get a function that's a jointly convex in X and Y because

1289
01:21:47,420 --> 01:21:48,640
its domain

1290
01:21:48,680 --> 01:21:54,450
is convex in X and Y it's just a set of linear inequalities and on its domain it obviously satisfies

1291
01:21:54,450 --> 01:21:56,180
Jensen's inequality

1292
01:21:56,190 --> 01:22:01,990
so that's a convex function of jointly in X and Y and therefore and its minimum

1293
01:22:01,990 --> 01:22:07,640
over Y is equal to H of X so therefore H is convex

1294
01:22:07,640 --> 01:22:11,030
so this is true for LPs but also of course for many other

1295
01:22:11,640 --> 01:22:13,820
convex problems

1296
01:22:13,870 --> 01:22:20,320
the optimal value as a function the right-hand side is convex in general

1297
01:22:20,360 --> 01:22:27,380
H is a function of X so I define a function of X

1298
01:22:27,380 --> 01:22:31,730
and the value at X is the optimal value of this LP

1299
01:22:31,750 --> 01:22:33,820
so I fix X

1300
01:22:34,010 --> 01:22:37,510
and then solve this linear program that Y is variable

1301
01:22:37,530 --> 01:22:40,950
and then the optimal value is the function value

1302
01:22:40,990 --> 01:22:42,660
of H at X

1303
01:22:43,420 --> 01:22:48,790
so the it's interesting because it's a function is how the optimal value changes it

1304
01:22:48,790 --> 01:22:52,730
changes in the righthand side to it rises in sensitivity analysis and and

1305
01:22:52,730 --> 01:22:53,840
so on

1306
01:22:59,770 --> 01:23:01,420
then we have a few more

1307
01:23:01,550 --> 01:23:07,080
so an obvious question is what about composition if I have a composition of H and G of X under

1308
01:23:07,080 --> 01:23:09,490
what condition is that convex

1309
01:23:09,690 --> 01:23:16,180
well there are some useful rules for example if X is convex and the

1310
01:23:17,270 --> 01:23:19,940
or non decreasing and G is convex

1311
01:23:19,970 --> 01:23:22,440
then you can conclude that F is convex

1312
01:23:22,440 --> 01:23:25,980
it looks like minor but is going to be big later on

1313
01:23:26,020 --> 01:23:29,810
we have a point p going in three-dimensional space

1314
01:23:29,930 --> 01:23:31,730
and here

1315
01:23:31,780 --> 01:23:32,720
we have

1316
01:23:32,780 --> 01:23:35,180
the entire behaviour

1317
01:23:35,220 --> 01:23:36,470
of the object

1318
01:23:36,480 --> 01:23:37,560
as it

1319
01:23:38,470 --> 01:23:41,420
projection along the x axis

1320
01:23:41,430 --> 01:23:43,250
this is the position

1321
01:23:43,270 --> 01:23:45,040
this is its velocity

1322
01:23:45,060 --> 01:23:47,130
and this is acceleration

1323
01:23:47,140 --> 01:23:48,450
and here

1324
01:23:48,530 --> 01:23:51,160
CD entire behaviour

1325
01:23:51,200 --> 01:23:52,750
on the z axis

1326
01:23:52,770 --> 01:23:56,490
this is the position on the axis is this is the velocity component in this

1327
01:23:57,380 --> 01:24:01,150
and this is the acceleration in the x and you you have the y

1328
01:24:01,170 --> 01:24:02,610
and that words

1329
01:24:02,670 --> 01:24:03,730
we have no

1330
01:24:03,750 --> 01:24:05,780
the three-dimensional motion

1331
01:24:05,840 --> 01:24:07,150
we have cut

1332
01:24:07,150 --> 01:24:08,030
in two

1333
01:24:08,940 --> 01:24:10,780
one dimensional motions

1334
01:24:10,840 --> 01:24:13,480
this is a one-dimensional motion

1335
01:24:13,480 --> 01:24:17,550
there's behavior only along the x axis and this is the behaviour only along the

1336
01:24:17,550 --> 01:24:22,530
y axis and this is the behaviour only along the axis and the three together

1337
01:24:22,610 --> 01:24:24,750
make up

1338
01:24:24,750 --> 01:24:28,820
the actual motion of the particles

1339
01:24:28,940 --> 01:24:32,760
what have we gained it looks like this looks like a mathematical zoo

1340
01:24:32,760 --> 01:24:34,090
you would say well

1341
01:24:34,170 --> 01:24:37,440
this is what is going to be like it's going to be held

1342
01:24:39,730 --> 01:24:43,110
not quite in fact is going to help you

1343
01:24:43,110 --> 01:24:45,490
a great deal

1344
01:24:45,550 --> 01:24:50,880
first of all if i throw of the tennis ball in class like this

1345
01:24:50,940 --> 01:24:52,900
then the whole trajectory

1346
01:24:53,960 --> 01:24:57,650
the whole trajectories in one plane in the vertical plane

1347
01:24:57,710 --> 01:25:01,730
so even though it is in three dimensions we can always represented

1348
01:25:01,750 --> 01:25:05,780
by two axes by two-dimensional it y axis and x axis

1349
01:25:05,800 --> 01:25:06,840
it's already

1350
01:25:06,840 --> 01:25:12,110
the three-dimensional problem often becomes the two dimensional problem

1351
01:25:12,150 --> 01:25:14,030
we always great success

1352
01:25:15,130 --> 01:25:16,940
these trajectories

1353
01:25:16,940 --> 01:25:18,550
by decomposing

1354
01:25:18,550 --> 01:25:23,320
it's very complicated motion imagine what an incredibly complicated arc that is

1355
01:25:23,380 --> 01:25:28,360
and yet we're going to decompose it into emotion in the x direction

1356
01:25:28,400 --> 01:25:32,260
which leads a life of its own independent of the motion in the y direction

1357
01:25:32,260 --> 01:25:34,150
which live a life of its own

1358
01:25:34,210 --> 01:25:36,840
and of course you always have to combine the two to know

1359
01:25:36,900 --> 01:25:38,210
what the particle

1360
01:25:43,230 --> 01:25:46,570
we know the equation so well

1361
01:25:46,610 --> 01:25:48,280
from our last lecture

1362
01:25:51,170 --> 01:25:54,820
with constant acceleration

1363
01:25:54,840 --> 01:25:59,320
the first line tells you what the position as a function of time

1364
01:25:59,360 --> 01:26:01,380
the index t

1365
01:26:01,420 --> 01:26:03,380
well suited to changing with time

1366
01:26:03,380 --> 01:26:05,650
it is the position it equal zero

1367
01:26:05,690 --> 01:26:08,780
what's the velocity at equal zero times t

1368
01:26:08,840 --> 01:26:13,490
was one half a x squared if there is an acceleration in exploration

1369
01:26:13,510 --> 01:26:17,300
the velocity immediately comes from taking the derivative of this function

1370
01:26:17,360 --> 01:26:21,840
any acceleration comes from taking the derivative of this function

1371
01:26:21,900 --> 01:26:23,820
now if we have a

1372
01:26:24,730 --> 01:26:26,570
which is more complicated

1373
01:26:26,590 --> 01:26:29,110
which reaches out to two or three dimensions

1374
01:26:29,130 --> 01:26:31,510
we can decompose the motion three

1375
01:26:31,570 --> 01:26:33,030
but particular axes

1376
01:26:33,050 --> 01:26:36,610
and you can replace every axiom by y

1377
01:26:36,670 --> 01:26:38,280
which gives you the entire

1378
01:26:38,280 --> 01:26:41,420
behaviour in the y direction and if you want to know to behaviour in this

1379
01:26:41,420 --> 01:26:44,550
direction you replace every xy advising

1380
01:26:44,550 --> 01:26:45,380
and then

1381
01:26:46,570 --> 01:26:48,820
i have decomposed the motion

1382
01:26:48,820 --> 01:26:50,670
in three

1383
01:26:50,710 --> 01:26:53,460
directions each of them i

1384
01:26:55,840 --> 01:26:58,090
and that's what i wanna do now

1385
01:26:58,150 --> 01:27:01,760
i'm going to

1386
01:27:03,090 --> 01:27:05,010
an object

1387
01:27:06,340 --> 01:27:09,670
when apple

1388
01:27:09,710 --> 01:27:13,150
in twenty six one hundred

1389
01:27:13,190 --> 01:27:16,420
and we know that it's in the vertical plane so we have

1390
01:27:16,420 --> 01:27:19,340
we only deal with full dimensional problems

1391
01:27:19,420 --> 01:27:21,260
this being

1392
01:27:21,280 --> 01:27:22,320
i call this my

1393
01:27:22,340 --> 01:27:23,570
x axis

1394
01:27:23,660 --> 01:27:25,820
and i'm going to call this

1395
01:27:25,900 --> 01:27:27,590
y axis

1396
01:27:27,590 --> 01:27:29,990
i call it increasing value of x

1397
01:27:30,010 --> 01:27:33,260
and i call this increasing value of y

1398
01:27:33,320 --> 01:27:37,510
i could have called this increasing value of y

1399
01:27:37,570 --> 01:27:41,760
today i decided to call this increasing value y i'm free

1400
01:27:41,800 --> 01:27:44,150
in that choice

1401
01:27:44,170 --> 01:27:46,320
i throw of an object

1402
01:27:46,340 --> 01:27:48,070
a certain angle

1403
01:27:48,110 --> 01:27:49,750
and i see

1404
01:27:49,800 --> 01:27:52,610
motion like this going it comes back

1405
01:27:52,650 --> 01:27:56,050
to the ground

1406
01:27:56,090 --> 01:27:58,280
my initial

1407
01:27:58,650 --> 01:28:00,170
we need

1408
01:28:00,230 --> 01:28:02,190
but i through it

1409
01:28:02,210 --> 01:28:03,820
it was the zero

1410
01:28:03,820 --> 01:28:05,210
and the angle here

1411
01:28:05,260 --> 01:28:08,490
it's all fine

1412
01:28:10,030 --> 01:28:11,780
x component

1413
01:28:11,840 --> 01:28:14,050
of that initial velocity

1414
01:28:14,090 --> 01:28:15,730
it is zero

1415
01:28:15,760 --> 01:28:17,590
the cosine of

1416
01:28:17,590 --> 01:28:21,180
illustrating what I meant course and thinking about Terminal 5 at Heathrow many of you

1417
01:28:21,180 --> 01:28:24,250
know about what happened over there in last week or so

1418
01:28:24,760 --> 01:28:28,170
which was shirk entrusted

1419
01:28:28,760 --> 01:28:33,070
but kicked so I'd like to show this this is actually a lot of real

1420
01:28:34,260 --> 01:28:37,940
and 1 show how they are using the kind of all these 3 services together

1421
01:28:37,980 --> 01:28:40,750
but what this is websites

1422
01:28:41,440 --> 01:28:46,460
the websites actually right here and then got back Amazon Web Services where they're doing

1423
01:28:46,460 --> 01:28:50,510
all the work and the idea is that you can upload of video filed habit

1424
01:28:50,510 --> 01:28:53,250
Trans crowed like from an EDI

1425
01:28:53,250 --> 01:28:54,480
to an impatient

1426
01:28:54,650 --> 01:28:57,510
and work something like this

1427
01:28:58,150 --> 01:29:00,590
there's so

1428
01:29:00,590 --> 01:29:03,320
you've got were using as prehistoric

1429
01:29:03,340 --> 01:29:04,190
when they are

1430
01:29:04,190 --> 01:29:11,460
using as historic asked U.S. Azalia workflow engine many seeking reaction to the transfer so

1431
01:29:11,460 --> 01:29:13,710
what happens if you go to the other

1432
01:29:13,750 --> 01:29:18,110
websites and post file uploaded in the Amazon street

1433
01:29:18,610 --> 01:29:21,510
put entry into SQL

1434
01:29:21,550 --> 01:29:22,800
and then

1435
01:29:22,820 --> 01:29:24,570
easy to that there's a job

1436
01:29:25,480 --> 01:29:29,150
In starts it up and start transfer of something like this

1437
01:29:29,210 --> 01:29:32,530
so now that seemed to work in the 1st

1438
01:29:32,570 --> 01:29:35,800
the 2nd one's waiting and so on and so forth and just keeps going

1439
01:29:35,820 --> 01:29:41,800
With the first one is puts the got got back up industries

1440
01:29:41,840 --> 01:29:45,210
they started 2nd jobs and so on and so forth

1441
01:29:45,650 --> 01:29:47,530
they are

1442
01:29:47,530 --> 01:29:50,340
thing is that all that you can get into a situation like this where you

1443
01:29:50,340 --> 01:29:56,110
have to much inbound work you can't keep up with what's going on well if

1444
01:29:56,110 --> 01:30:02,170
that happens what they're doing is monitoring the queue length and if the huge it's

1445
01:30:02,170 --> 01:30:03,170
too long

1446
01:30:03,210 --> 01:30:07,710
this is simply stand up more easy to instances and ketchup because you know of

1447
01:30:07,710 --> 01:30:10,070
transporting is easy to do and Carol

1448
01:30:10,820 --> 01:30:15,300
and then of course eventually you catch up and put it away

1449
01:30:16,460 --> 01:30:20,340
1 last 1 I lot of talk about before moving to the devil

1450
01:30:20,530 --> 01:30:23,980
In this is an Amazon simple B B which is what it sounds like a

1451
01:30:23,980 --> 01:30:29,480
database as the weather service so the idea here is that rather than actually running

1452
01:30:30,530 --> 01:30:35,670
which you do is you authenticate and make a Web service call into the virtual

1453
01:30:35,670 --> 01:30:41,190
database doesn't operate like sequel I'm really wanna make that clear that it's not sequel

1454
01:30:41,230 --> 01:30:46,050
but that because it's healthy mostly you can start out with a prototype and without

1455
01:30:46,050 --> 01:30:51,170
changing anything you can move it up into a candy by database that scales very

1456
01:30:51,170 --> 01:30:54,090
well now there's no scheme

1457
01:30:54,090 --> 01:30:58,480
so that means you literally just created what we call the Maine

1458
01:30:58,530 --> 01:31:01,820
and I think that's a table for just a moment section doesn't have to be

1459
01:31:01,820 --> 01:31:07,650
used like you created in just start putting data on and we take care of

1460
01:31:08,610 --> 01:31:11,460
there's no data types everything a string

1461
01:31:11,480 --> 01:31:13,070
and I

1462
01:31:13,070 --> 01:31:19,780
but it works a lot like Amazon has 3 and in that there's mobile nodes

1463
01:31:19,780 --> 01:31:23,710
so that when you put something in we replicate across mobile nodes mobile data centers

1464
01:31:23,710 --> 01:31:27,400
just like Amazon history so it's a very resilient data store

1465
01:31:27,510 --> 01:31:29,690
Inter High

1466
01:31:30,260 --> 01:31:32,190
but many time this

1467
01:31:32,420 --> 01:31:39,420
the state's of during very inexpensive costly to run an ongoing most cost data-transfer not

1468
01:31:39,530 --> 01:31:42,250
much to do with the

1469
01:31:42,300 --> 01:31:46,780
that actual were so inexpensive that there's lots of lots of zeros in front of

1470
01:31:46,780 --> 01:31:48,920
the each transaction costs

1471
01:31:49,260 --> 01:31:51,170
look something like this

1472
01:31:51,670 --> 01:31:56,780
so we've had told the straight out of our documentation but so would you do

1473
01:31:56,780 --> 01:32:00,650
if you start creating remain here they created 1 gold my story

1474
01:32:00,690 --> 01:32:05,460
and then they put in their home near Somersworth 1st think of it as the

1475
01:32:05,460 --> 01:32:06,570
primary key

1476
01:32:06,630 --> 01:32:10,440
but the the name of the record and evaluates as 1 2 3 could just

1477
01:32:10,440 --> 01:32:12,440
as easily have been ABC

1478
01:32:12,900 --> 01:32:19,070
and then you start getting at so for example an afternoon descriptions of sweaters

1479
01:32:19,070 --> 01:32:23,130
another 1 with colors blue notice that actually have a 2nd color

1480
01:32:23,400 --> 01:32:28,170
I didn't to do during the normalization Erskine anything it just put in multiple Latrobe

1481
01:32:28,170 --> 01:32:29,750
somewhere where

1482
01:32:29,780 --> 01:32:32,750
and you can do the same thing in the 2nd row just like the 1st

1483
01:32:32,750 --> 01:32:37,780
broke and then along the 3rd row or another name and you tributes without doing

1484
01:32:37,780 --> 01:32:40,670
anything just entered the United Way

1485
01:32:40,980 --> 01:32:42,920
so it's very very flexible

1486
01:32:42,940 --> 01:32:48,300
OK so if you want and more think most of it sounds like most of

1487
01:32:48,300 --> 01:32:50,860
the already know what that ends and

1488
01:32:51,960 --> 01:32:55,420
there's a work like this and that

1489
01:32:55,460 --> 01:33:02,070
there's my e-mail address if you'd like to to use that an that would like

1490
01:33:02,420 --> 01:33:05,550
it is by the way the very best way to find out information is on

1491
01:33:05,550 --> 01:33:10,070
our forms because I can really small but you know so with that would like

1492
01:33:10,090 --> 01:33:12,860
to do is jump over

1493
01:33:12,860 --> 01:33:16,900
case but you could also be in this position where the center of the ellipsoid

1494
01:33:16,900 --> 01:33:22,600
is outside the constraints and if you make it smaller you can see even more outside the constraints

1495
01:33:22,880 --> 01:33:27,520
so we're talking about this is our posterior it's a little bit of that ellipsoid

1496
01:33:27,520 --> 01:33:33,620
that satisfies the constraints renormalized so we've got a precise geometrical view of what happens

1497
01:33:33,620 --> 01:33:37,940
in the limit and we can see the way that it depends on the variants of

1498
01:33:37,940 --> 01:33:42,340
the posterior and in particular we know how that depends on the variants of the

1499
01:33:42,340 --> 01:33:50,580
prior and the likelihood in this not full ranked situation what what what you get

1500
01:33:50,580 --> 01:34:00,140
is that this ellipsoid flattens and get smaller at different rates so this is

1501
01:34:00,140 --> 01:34:08,020
the sort of theorem that we have expressed in some but not

1502
01:34:08,020 --> 01:34:15,680
complete detail so the posterior distribution in this case with ill-posed a convergence is

1503
01:34:15,680 --> 01:34:21,220
to a point mass we can characterize that promise exactly this happens as the exposure time

1504
01:34:21,220 --> 01:34:26,200
goes to infinity and is quite curious situation and this is a property of inverse problems here

1505
01:34:26,620 --> 01:34:32,600
because even with the infinite matter data the solution isn't unique this means that to

1506
01:34:32,600 --> 01:34:39,800
get convergence the prior has get more informative as well that's something a little bit uncomfortable

1507
01:34:39,880 --> 01:34:45,360
eventhough you're getting more and more information because you're running the experiment for longer and longer

1508
01:34:45,360 --> 01:34:50,160
if you want convergence to a point then the the prior has to shrink as well

1509
01:34:50,170 --> 01:34:55,000
what we've found out is that that prior can shrink at a much lower rate than the

1510
01:34:55,000 --> 01:35:03,540
than the data data variance and we got rate and time results and in the

1511
01:35:03,540 --> 01:35:08,920
limit we've gotta quite nice characterization because of the ellipsoid getting smaller differently in

1512
01:35:08,920 --> 01:35:14,400
different dimensions ain't getting truncated they actually the limit distribution is for the most part gaussian

1513
01:35:14,400 --> 01:35:21,480
but the presence of the boundary means that in certain dimensions it's exponential no gaussian

1514
01:35:21,480 --> 01:35:25,380
that's all I'm gonna say about that I and it's the only thing I said about theory

1515
01:35:25,380 --> 01:35:29,140
in the all all of this talks but it's just an insert sort of way so you can

1516
01:35:29,140 --> 01:35:39,020
get some sort of handle if you've gotten extremely talented co collaborator

1517
01:35:39,180 --> 01:35:43,580
you can do the analysis it's an instance how you can get a handle on

1518
01:35:43,580 --> 01:35:51,280
the on the solution okay that's that's the second seminar and I'm gonna give you at least a bit

1519
01:35:51,280 --> 01:35:58,350
of the third one any questions at that point before we do something again completely

1520
01:35:58,370 --> 01:36:07,380
different okay so this is about latent factor analysis and that's unknown

1521
01:36:07,380 --> 01:36:13,760
is a class of models that is much used in machine learning typically generalized that mixtures of factors

1522
01:36:13,770 --> 01:36:21,900
and so on but this is a quite a vanilla version of it the the situation

1523
01:36:21,900 --> 01:36:27,680
we're thinking about here is problems arising in genomics but where there are several

1524
01:36:27,680 --> 01:36:35,010
essentially several parallel assays and this is the case really of two so we're looking

1525
01:36:35,020 --> 01:36:39,860
at data from this paper is few years ago now but it was essentially relating

1526
01:36:39,860 --> 01:36:48,900
RNA transcripts with some metabolites okay and in this case there were just small

1527
01:36:48,900 --> 01:36:56,360
number of conditions and two hundred nineteen variables representing levels of gene expression and levels of of

1528
01:36:56,360 --> 01:37:02,920
these metabolites there's there's some structure structure to the data which I won't be talking

1529
01:37:02,920 --> 01:37:09,440
about modeling they're actually short time series but I'll leave out that particular component and the data

1530
01:37:09,440 --> 01:37:17,420
look something like this so these are the two hundred nineteen variables plotted against the

1531
01:37:17,420 --> 01:37:24,400
eighteen the eighteen different conditions and these are the I think these are the metabolites and those

1532
01:37:24,600 --> 01:37:33,880
there well I don't know it doesn't matter I am convinced now these are the transcription there's the metabolites so all I've done

1533
01:37:33,880 --> 01:37:38,340
they're on the original scale and that's more helpful where you can see that they've been standardized separately

1534
01:37:38,340 --> 01:37:42,960
to get some idea of the degree of covariation there is between the two sets

1535
01:37:42,960 --> 01:37:52,200
multiple sets of time series so the particular question we're exploring was can we find sparse

1536
01:37:52,200 --> 01:37:56,600
models for the dependence well the correlation between these two

1537
01:37:56,660 --> 01:38:14,580
yes I I'm not sure probably everything is so

1538
01:38:14,580 --> 01:38:21,160
we wanted to use factor factor models cause what we are thinking of here is you know the factors being

1539
01:38:21,160 --> 01:38:27,760
somehow biologically interpretable as as as as background factors in the natural sense but it's only

1540
01:38:27,760 --> 01:38:31,140
the covariation that we're interested in modeling here so we hope

1541
01:38:31,140 --> 01:38:37,680
we're gonna find a sparse representation of that and that's so that's the objective so the

1542
01:38:38,150 --> 01:38:43,340
the model looks something like this this is our notation for the standard linear

1543
01:38:43,340 --> 01:38:49,080
factor model so X J I the data J is the variable index A is the observation index

1544
01:38:49,420 --> 01:38:55,960
and we assume this linear model and apart from an intercept we

1545
01:38:55,960 --> 01:39:04,440
have essentially A times lambda so A are the factor loadings and lambda are the factors and then there's

1546
01:39:04,440 --> 01:39:12,100
some noise and it's grossly model cause none of these things

1547
01:39:12,100 --> 01:39:17,500
on the right-hand side are observed so it's like a linear model with unknown co covariance

1548
01:39:17,500 --> 01:39:27,200
that's what factor analysis is I'm gonna use the indices consistently so J is an observation sorry it's a variable number  I is

1549
01:39:27,200 --> 01:39:34,220
the observation number and L running from one to K is a factor so what we're looking

1550
01:39:34,220 --> 01:39:38,560
for is low rank representation so we hope that L will be much smaller than

1551
01:39:38,560 --> 01:39:39,760
P or N

1552
01:39:39,960 --> 01:39:45,480
we're interested in dimension reduction I said that we're interested in sparsity so that

1553
01:39:45,480 --> 01:39:51,760
a loss of these loadings will be zero and we're probably working on the case

1554
01:39:51,760 --> 01:39:57,020
where P is much bigger than N and N is much bigger than K so I think we'll have some sort

1555
01:39:57,020 --> 01:40:02,800
of apriori belief we think nature's being kind to us that that

1556
01:40:02,800 --> 01:40:10,880
it would be commom patterns of loadings for multiple variables to do with variable

1557
01:40:10,890 --> 01:40:16,760
thing on the same pathways and we hope that nature would have helped

1558
01:40:16,760 --> 01:40:22,080
us in this regard as well that's what we're that's what we trying to identify

1559
01:40:22,080 --> 01:40:26,920
this is a cartoon on what the model looks like on a small scale so these

1560
01:40:26,920 --> 01:40:31,700
are basically images representing the different matrices here so we have in this case it

1561
01:40:31,700 --> 01:40:40,040
looks like it looks like we have ten observations we have five variables

1562
01:40:40,040 --> 01:40:46,260
and we have sorry we have three variables and we have five factors so that's the

1563
01:40:46,260 --> 01:40:55,080
that's the factor matrix and that's the loading matrix so factor analysis goes

1564
01:40:55,080 --> 01:41:00,890
so all rules that are

1565
01:41:01,880 --> 01:41:03,960
the problem

1566
01:41:18,030 --> 01:41:20,670
the above

1567
01:41:26,080 --> 01:41:28,550
this is talk far

1568
01:41:32,660 --> 01:41:35,440
one of

1569
01:41:37,140 --> 01:41:40,050
after the

1570
01:42:08,210 --> 01:42:11,530
you are also

1571
01:42:31,040 --> 01:42:41,250
one of the first and

1572
01:43:16,280 --> 01:43:21,330
so can

1573
01:43:23,460 --> 01:43:32,230
well that's

1574
01:43:40,550 --> 01:43:49,800
what i mean by that

1575
01:43:51,760 --> 01:43:55,090
one of

1576
01:43:55,110 --> 01:43:58,550
as possible

1577
01:44:09,880 --> 01:44:12,720
o point

1578
01:44:13,000 --> 01:44:14,350
that's right

1579
01:44:29,470 --> 01:44:32,520
of course

1580
01:44:35,860 --> 01:44:39,130
that are

1581
01:44:42,330 --> 01:44:45,060
that's all

1582
01:44:45,070 --> 01:44:49,550
so i hope

1583
01:44:58,690 --> 01:45:01,590
the curvature

1584
01:45:02,050 --> 01:45:05,170
the performance

1585
01:45:05,190 --> 01:45:08,210
hundred one

1586
01:45:11,250 --> 01:45:14,940
it's the

1587
01:45:24,770 --> 01:45:30,250
but also

1588
01:45:40,330 --> 01:45:44,270
last three

1589
01:45:44,270 --> 01:45:49,330
i write this out as a x and a delta x

1590
01:45:49,350 --> 01:45:52,880
next was a delta x

1591
01:45:52,900 --> 01:45:56,250
now what to do next

1592
01:45:56,270 --> 01:45:57,480
here's my

1593
01:45:57,480 --> 01:45:59,480
true equation and here's my

1594
01:46:03,080 --> 01:46:08,810
computational equation so i got those two i just subtract that from that

1595
01:46:10,190 --> 01:46:14,420
so i learned that a delta x

1596
01:46:14,460 --> 01:46:16,560
delta p

1597
01:46:16,710 --> 01:46:22,580
so i have an idea now of how being

1598
01:46:22,670 --> 01:46:24,420
delta b is

1599
01:46:24,440 --> 01:46:26,810
still to be

1600
01:46:26,810 --> 01:46:29,500
OK how they could delta b

1601
01:46:34,350 --> 01:46:37,650
from from this equation

1602
01:46:37,710 --> 01:46:41,750
so delta b is is what you get by multiplying delta x by

1603
01:46:41,790 --> 01:46:47,190
so all our our work on the norms of the matrix says

1604
01:46:49,960 --> 01:46:59,120
this thing can blow up by more than the norm of a

1605
01:46:59,170 --> 01:47:00,500
that's so this

1606
01:47:00,500 --> 01:47:06,460
these facts just come from these simple equations by the idea of what the normal

1607
01:47:11,850 --> 01:47:13,440
now my any better

1608
01:47:13,540 --> 01:47:19,630
well i have a feeling that if i just put these two pieces together

1609
01:47:19,670 --> 01:47:20,670
i'll get

1610
01:47:20,750 --> 01:47:25,020
i guess for i'm gonna do i'm going to divide one by the other

1611
01:47:25,020 --> 01:47:28,020
to get something like that

1612
01:47:28,060 --> 01:47:30,230
who was that good

1613
01:47:30,250 --> 01:47:34,270
i don't know if it's so great i don't think i've done it right

1614
01:47:34,330 --> 01:47:38,850
why do i not saying right because i want inequality where delta x is less

1615
01:47:38,850 --> 01:47:46,770
than something i've brilliantly come up with equality where it's larger than some

1616
01:47:48,520 --> 01:47:50,480
embarrassment here

1617
01:47:52,170 --> 01:47:56,540
maybe what i wanted to do was have inverse in this one

1618
01:47:56,560 --> 01:47:59,460
that would have been like

1619
01:47:59,480 --> 01:48:03,400
version of this one

1620
01:48:03,420 --> 01:48:08,130
and this one i shouldn't have taken the position should state where i would be

1621
01:48:08,170 --> 01:48:10,650
a x equals b would have been

1622
01:48:10,670 --> 01:48:13,270
all right can can i

1623
01:48:13,290 --> 01:48:15,360
yes on the videotape

1624
01:48:15,420 --> 01:48:18,880
we cannot we can

1625
01:48:21,880 --> 01:48:24,100
future viewers well

1626
01:48:24,100 --> 01:48:30,500
in this so this is the length of b is less than the normal

1627
01:48:30,500 --> 01:48:32,980
i'm not

1628
01:48:33,000 --> 01:48:37,960
line and this one says that the size of the output

1629
01:48:38,020 --> 01:48:42,580
let's recall the size of the universe

1630
01:48:42,650 --> 01:48:46,330
times the size of delta

1631
01:48:47,750 --> 01:48:56,440
now we're getting somewhere

1632
01:48:56,480 --> 01:48:59,960
now i have dealt explicitly call something

1633
01:49:00,000 --> 01:49:05,440
but i want relative error divided by x can i do that

1634
01:49:05,440 --> 01:49:07,600
what happens here

1635
01:49:07,690 --> 01:49:10,130
i started with this one

1636
01:49:10,130 --> 01:49:14,400
and i divided by delta phi x

1637
01:49:14,420 --> 01:49:23,630
o k eight

1638
01:49:25,270 --> 01:49:28,900
x is bigger than

1639
01:49:28,900 --> 01:49:32,710
b over the length of a sofie divided by

1640
01:49:32,710 --> 01:49:34,750
something smaller

1641
01:49:35,710 --> 01:49:39,150
this axis bigger than be the over

1642
01:49:39,170 --> 01:49:43,250
so if i divide instead by b already have grown more

1643
01:49:43,250 --> 01:49:47,330
so let me divided by b over a instead instead of dividing by

1644
01:49:47,350 --> 01:49:50,190
there's a divided by b

1645
01:49:52,620 --> 01:49:57,650
except here

1646
01:49:58,480 --> 01:50:00,400
is right

1647
01:50:00,420 --> 01:50:04,080
and i've seen i now see what's the condition number right now see what is

1648
01:50:04,080 --> 01:50:08,560
this condition and you see other by by

1649
01:50:10,440 --> 01:50:14,710
spoiled the blackboard so it should be here it's

1650
01:50:14,730 --> 01:50:18,600
you see delta x over x here is delta b over b and here you

1651
01:50:20,250 --> 01:50:23,250
the condition

1652
01:50:23,270 --> 01:50:26,750
the condition number is the normal way

1653
01:50:28,500 --> 01:50:29,940
times the norm of

1654
01:50:29,980 --> 01:50:33,250
isn't that he

1655
01:50:33,270 --> 01:50:37,460
condition number is the norm of the first times the normal

1656
01:50:37,480 --> 01:50:45,560
that's the right quantity

1657
01:50:45,560 --> 01:50:51,810
but this is taken more time and but i still having got this far i

1658
01:50:51,810 --> 01:50:53,440
would like to give it another

1659
01:50:53,460 --> 01:50:55,330
a little bit because i'm

1660
01:50:55,350 --> 01:50:58,630
i would like to understand what this

1661
01:50:58,630 --> 01:51:04,440
suppose a is an orthogonal matrix

1662
01:51:04,460 --> 01:51:07,000
what's its conditional

1663
01:51:07,920 --> 01:51:14,150
because if if it's an orthogonal matrix norm is one and its inverse is also

1664
01:51:14,150 --> 01:51:16,330
an orthogonal matrix that's one

1665
01:51:16,350 --> 01:51:18,330
so the condition number is one

1666
01:51:18,380 --> 01:51:21,400
what's the condition number of the sky

1667
01:51:21,460 --> 01:51:23,750
a symmetric matrix

1668
01:51:23,830 --> 01:51:27,880
well tell me it's norman tell me it's tell me its inverse so for well

1669
01:51:27,900 --> 01:51:30,000
we can do is inverse easily

1670
01:51:30,020 --> 01:51:32,150
what's the inverse of this matrix

1671
01:51:32,170 --> 01:51:35,980
minus thirty zero zero one

1672
01:51:36,750 --> 01:51:41,250
so tell me the norms of those two guys

1673
01:51:41,290 --> 01:51:43,130
three and

1674
01:51:43,190 --> 01:51:44,600
one right

1675
01:51:44,630 --> 01:51:47,380
so the condition number is

1676
01:51:49,790 --> 01:51:50,960
exactly so

1677
01:51:50,980 --> 01:51:55,600
those those are easy because they were diagonal

1678
01:51:56,350 --> 01:51:59,060
symmetric matrices are just as easy

1679
01:51:59,100 --> 01:52:01,360
for a symmetric matrix

1680
01:52:01,380 --> 01:52:06,630
so what's so so this is for general matrix if the matrix is symmetric one

1681
01:52:06,630 --> 01:52:08,880
i keep this this

1682
01:52:10,330 --> 01:52:11,850
going so this will

1683
01:52:12,940 --> 01:52:15,290
conclude the

1684
01:52:15,290 --> 01:52:17,080
our table

1685
01:52:18,100 --> 01:52:19,650
norms and now

1686
01:52:21,270 --> 01:52:24,190
condition numbers condition

1687
01:52:24,190 --> 01:52:27,350
OK so what's the what are the condition numbers

1688
01:52:27,400 --> 01:52:31,150
if it's orthogonal we said condition number was one

1689
01:52:31,190 --> 01:52:34,230
if now here's is the case i'm interested in

1690
01:52:34,270 --> 01:52:35,460
if they

1691
01:52:35,460 --> 01:52:37,730
if it is symmetric matrix

1692
01:52:37,730 --> 01:52:40,190
because we do get a lot of those

1693
01:52:41,500 --> 01:52:43,230
what is the

1694
01:52:43,250 --> 01:52:44,880
condition number

1695
01:52:44,900 --> 01:52:47,540
remembering remembering our formula

1696
01:52:47,540 --> 01:52:52,210
you have to multiply the norm of a by inverse and we did a test

1697
01:52:53,210 --> 01:52:55,750
where we got three and one

1698
01:52:56,650 --> 01:53:01,350
one i take a shot on this

1699
01:53:01,420 --> 01:53:07,940
the bill at ratio great raid perfect lambda max

1700
01:53:10,060 --> 01:53:12,170
lambda men

1701
01:53:12,210 --> 01:53:14,020
that tells you how are the

1702
01:53:15,560 --> 01:53:18,040
and you see the beauty of that number

1703
01:53:18,060 --> 01:53:20,630
if i multiply the matrix by a hundred

1704
01:53:20,630 --> 01:53:23,600
then i multiply both pieces by a hundred

1705
01:53:23,650 --> 01:53:27,980
same conditional so so you can't cheat by just

1706
01:53:30,310 --> 01:53:34,750
OK and the answer here will be sigma max

1707
01:53:34,750 --> 01:53:39,000
in here in in fact there is the hubble parameter so this is the effect

1708
01:53:39,230 --> 01:53:46,980
of the background expansion of the universe the basically tend to slow down these exponential

1709
01:53:46,980 --> 01:53:51,390
growth of perturbations that we had in the previous slide the solution of the previous

1710
01:53:51,390 --> 01:53:55,290
equation since it is at this time was something that was growing exponentially but now

1711
01:53:55,290 --> 01:53:57,430
you have this extra term here

1712
01:53:57,450 --> 01:54:01,790
and he says that enemy sort of dumping

1713
01:54:01,810 --> 01:54:06,830
OK so now we can study the solutions of this equation and this will tell

1714
01:54:08,390 --> 01:54:13,310
but to asian in the universe evolved

1715
01:54:13,350 --> 01:54:18,540
so let's write from here and let's see what happened in the radiation dominated area

1716
01:54:18,560 --> 01:54:24,270
and the radiation dominated omega if omega government is less than unity

1717
01:54:24,290 --> 01:54:27,390
h is one of two t

1718
01:54:27,410 --> 01:54:29,120
and if you plug it in here

1719
01:54:29,140 --> 01:54:31,750
delta grows logarithmically

1720
01:54:31,750 --> 01:54:34,930
so in the radiation dominated there are basically

1721
01:54:34,980 --> 01:54:37,310
again division don't grow the grandmother

1722
01:54:37,350 --> 01:54:45,430
in the dominated hand if omega on there is much bigger than omega matter and

1723
01:54:45,450 --> 01:54:49,810
in that sense so less than unity then they harbor

1724
01:54:49,810 --> 01:54:54,230
what i meant that is basically the have given by lambda remember

1725
01:54:56,520 --> 01:55:02,180
and then the university remember exponders financially the universe expand this actually look what happened

1726
01:55:02,180 --> 01:55:05,350
to put it in here and look what happened to that that the solution is

1727
01:55:05,350 --> 01:55:10,520
the constant loss of constant e to the minus two blah blah blah

1728
01:55:10,560 --> 01:55:12,750
so again petitioned don't

1729
01:55:12,770 --> 01:55:16,270
this thing doesn't grow

1730
01:55:16,270 --> 01:55:19,180
it's a that perturbation of rules

1731
01:55:19,230 --> 01:55:25,160
only one matter dominates perturbation can grow because when matter dominates you plug it in

1732
01:55:25,160 --> 01:55:29,810
there and you get something like the perturbation goes like a constant t to two

1733
01:55:29,810 --> 01:55:35,690
third plus some became more that we can do

1734
01:55:35,710 --> 01:55:39,710
how does this perturbation grow where they grow in the matter dominated they're elected to

1735
01:55:39,710 --> 01:55:44,140
the two two-thirds and remember to the third is directly proportional to the scale factor

1736
01:55:44,140 --> 01:55:48,460
in this case factor goes like one divided one c

1737
01:55:48,480 --> 01:55:52,310
this is very useful to know

1738
01:55:52,350 --> 01:55:56,210
because when perturbation can grow in the life of the universe

1739
01:55:56,230 --> 01:56:00,060
they can grow at the rate that you can write down in less than line

1740
01:56:00,210 --> 01:56:03,660
goes like the scale factor in this case if i could easily one of one

1741
01:56:03,660 --> 01:56:08,000
class c and c is the set of all quantities

1742
01:56:08,020 --> 01:56:10,160
very very useful

1743
01:56:10,290 --> 01:56:15,810
now the cabinet here had not always done i've done linear perturbations in gene perturbation

1744
01:56:15,810 --> 01:56:18,730
always work is that is less than unity

1745
01:56:18,750 --> 01:56:23,010
and we know that today that there's not less the unity remember for as it

1746
01:56:23,010 --> 01:56:26,180
was this huge number ten to fifty whatever

1747
01:56:26,230 --> 01:56:30,140
and so you can already see that this applies some very large scale and if

1748
01:56:30,140 --> 01:56:34,580
you want to apply something like this and smaller and smaller scale with traditional ladder

1749
01:56:34,580 --> 01:56:37,950
ladders you have to do things like revision deirdre

1750
01:56:37,960 --> 01:56:41,950
or more complicated stuff or at the end of the day plug everything computer and

1751
01:56:41,950 --> 01:56:46,120
let the computer involved is highly nonlinear equation

1752
01:56:48,540 --> 01:56:52,000
so what's the schematic growth of perturbations

1753
01:56:54,980 --> 01:56:57,460
in the

1754
01:56:57,480 --> 01:57:04,580
radiation dominated and remember you have perturbation in the radiation to behave like sound waves

1755
01:57:04,580 --> 01:57:08,600
unless you are you know outside eyes and and things like that and so the

1756
01:57:09,710 --> 01:57:15,100
and as long as the universe is ionized and the body and so couple to

1757
01:57:15,100 --> 01:57:21,310
the photons because it's mostly made of hydrogen and idea genius ionize and it's ionize

1758
01:57:21,370 --> 01:57:27,060
because the falcons keep heating these items then the body and to follow the same

1759
01:57:27,230 --> 01:57:30,690
distribution as the radiation so the body so also sources

1760
01:57:30,710 --> 01:57:35,140
and then at some point when the universe cools enough that they cannot keep these

1761
01:57:35,140 --> 01:57:38,960
hydrogen ionized then the body goes the wrong way

1762
01:57:38,980 --> 01:57:43,660
they behave like much but the radiation keep doing whatever it was before

1763
01:57:43,660 --> 01:57:44,580
and so

1764
01:57:44,600 --> 01:57:51,140
the body perturbation without dark matter with the then started growing like this

1765
01:57:51,210 --> 01:57:53,950
but what happens if you add that matter to the mix

1766
01:57:53,960 --> 01:57:56,730
if you have that much of the mix that method is not

