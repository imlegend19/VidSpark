1
00:00:00,000 --> 00:00:03,430
and then adding in a decision noted that leaf

2
00:00:03,440 --> 00:00:07,170
so as to minimize some sort of an empirical criterion some sort of error criterion

3
00:00:07,170 --> 00:00:10,690
based on the on the training data

4
00:00:12,200 --> 00:00:13,090
and there

5
00:00:13,100 --> 00:00:17,420
there are various heuristics i mean it's common to grow to grow tree completely until

6
00:00:17,420 --> 00:00:19,920
the tree correctly classifies all the data

7
00:00:19,930 --> 00:00:22,820
and then and then prune the tree

8
00:00:22,860 --> 00:00:25,460
the print back according to some

9
00:00:26,780 --> 00:00:30,080
it's an empirical criterion again

10
00:00:30,090 --> 00:00:36,530
OK i will be considering these as examples of basis classes for instance for

11
00:00:38,180 --> 00:00:41,220
methods that combine classifiers

12
00:00:42,640 --> 00:00:44,110
OK so let's think about

13
00:00:44,170 --> 00:00:48,730
that those voting method

14
00:00:48,740 --> 00:00:50,340
so here

15
00:00:50,350 --> 00:00:53,110
class of functions

16
00:00:53,320 --> 00:00:54,960
these these functions that

17
00:00:54,970 --> 00:00:56,180
again there

18
00:00:56,190 --> 00:00:59,940
they're thresholded linear combinations but now not of the

19
00:01:01,460 --> 00:01:06,140
components of the vector x but of functions defined on x functions from some

20
00:01:06,180 --> 00:01:07,980
basis class g

21
00:01:07,990 --> 00:01:08,990
OK so

22
00:01:09,730 --> 00:01:12,310
here we're taking you can think of this is

23
00:01:12,840 --> 00:01:15,810
each if t is a member of that committee

24
00:01:15,820 --> 00:01:17,490
right after some

25
00:01:17,550 --> 00:01:20,610
basis function maybe it's a decision tree

26
00:01:20,630 --> 00:01:24,470
and when combining those FT's with some real valued weights

27
00:01:24,490 --> 00:01:27,540
and then we're taking the weighted majority decision

28
00:01:31,300 --> 00:01:34,350
OK so and an example

29
00:01:34,440 --> 00:01:42,770
there were considered to later again this is these cheese very small decision trees decision

30
00:01:42,770 --> 00:01:46,990
trees of depth one these are these are also called decision stumps

31
00:01:47,000 --> 00:01:48,840
OK so the class here

32
00:01:49,300 --> 00:01:51,810
these are just functions that map two

33
00:01:51,830 --> 00:01:53,940
the two values depending on some

34
00:01:53,950 --> 00:01:56,230
linear threshold function on a single

35
00:01:56,300 --> 00:02:00,020
single component of the input vector

36
00:02:00,040 --> 00:02:10,260
one class of algorithms for

37
00:02:10,280 --> 00:02:13,130
working with this class of functions

38
00:02:13,310 --> 00:02:17,270
other boosting algorithms that maintain

39
00:02:17,280 --> 00:02:21,410
some sort of a probability distribution over the data

40
00:02:21,460 --> 00:02:23,620
and choose the basis functions

41
00:02:23,810 --> 00:02:25,040
in order

42
00:02:25,800 --> 00:02:28,570
in some sequential order at each step

43
00:02:31,350 --> 00:02:34,930
some sort of an empirical criterion

44
00:02:34,940 --> 00:02:37,940
and typically this can be expressed just in terms of the

45
00:02:37,960 --> 00:02:41,710
this weighting function is weighted empirical error

46
00:02:41,720 --> 00:02:42,900
and the intuition

47
00:02:42,910 --> 00:02:45,380
here is that they were adjusting the

48
00:02:45,430 --> 00:02:48,940
the distribution of the data so as to emphasise the mistakes that

49
00:02:51,470 --> 00:02:58,090
committee our previous combination has made on the door

50
00:03:03,810 --> 00:03:06,030
because of the need to spell out

51
00:03:06,640 --> 00:03:10,670
the particular approach

52
00:03:10,710 --> 00:03:14,960
taken with with that of ship

53
00:03:17,480 --> 00:03:21,030
well let me just let let me just say that

54
00:03:21,270 --> 00:03:29,470
adaboost is is one approach you with the weighting is chosen such so as to

55
00:03:29,470 --> 00:03:33,070
the choice of the weighting corresponds to choosing a function

56
00:03:33,130 --> 00:03:37,720
from the space the linear span of our basis class

57
00:03:37,760 --> 00:03:39,520
so as to minimize

58
00:03:39,870 --> 00:03:43,550
the sample average of some exponential

59
00:03:43,560 --> 00:03:47,460
exponentially decreasing function of y times f of x

60
00:03:47,500 --> 00:03:48,370
OK so

61
00:03:49,500 --> 00:03:53,280
intuitively you know if f of x is this real valued

62
00:03:54,250 --> 00:03:56,870
real valued function

63
00:03:57,590 --> 00:04:00,370
why is it takes values plus and minus one

64
00:04:01,630 --> 00:04:04,310
you know it seems reasonable that we would be trying to

65
00:04:04,330 --> 00:04:06,730
to find a small value of this

66
00:04:06,740 --> 00:04:08,590
criterion which corresponds to

67
00:04:08,610 --> 00:04:13,210
y times f of x being large and positive

68
00:04:13,220 --> 00:04:20,230
right so that means that has the correct sign

69
00:04:20,250 --> 00:04:24,310
OK and

70
00:04:24,330 --> 00:04:30,530
i think that's also fine now about that boosting algorithms kernel methods

71
00:04:30,570 --> 00:04:31,750
and you've seen

72
00:04:31,770 --> 00:04:34,650
we've seen this here we

73
00:04:34,680 --> 00:04:37,320
we've got a rather general

74
00:04:37,380 --> 00:04:42,960
the main input space x on which we we define an inner product

75
00:04:42,960 --> 00:04:47,010
and come up with a suitable inner product

76
00:04:47,060 --> 00:04:49,390
i should say and come up with a hilbert space

77
00:04:49,460 --> 00:04:52,500
is the reproducing kernel hilbert space

78
00:04:52,510 --> 00:04:55,190
and the class of functions that we're considering

79
00:04:55,230 --> 00:04:59,600
we can view SVM ends is working with

80
00:04:59,620 --> 00:05:02,630
a bounded subset of linear functions

81
00:05:02,640 --> 00:05:03,540
right so

82
00:05:03,550 --> 00:05:05,750
we look at

83
00:05:05,750 --> 00:05:07,520
functions that that

84
00:05:07,940 --> 00:05:10,330
compute just inner products

85
00:05:10,340 --> 00:05:11,670
right but there

86
00:05:11,670 --> 00:05:16,750
the the the functions of bounded the RKHS norm of functions is bounded

87
00:05:16,790 --> 00:05:20,020
equally we can parameterize by an element of the public space

88
00:05:20,030 --> 00:05:21,270
and think of

89
00:05:21,280 --> 00:05:22,510
the parameter

90
00:05:22,530 --> 00:05:25,920
w is having found all

91
00:05:25,930 --> 00:05:28,320
and again just as in the perceptron case

92
00:05:28,340 --> 00:05:30,390
you know the the neat thing about

93
00:05:30,790 --> 00:05:34,440
this kind of approach is that even with the general inner product on on complicated

94
00:05:34,440 --> 00:05:36,800
space we can always represent the

95
00:05:37,260 --> 00:05:39,570
the function implicitly

96
00:05:39,650 --> 00:05:43,040
using this dual representation

97
00:05:43,050 --> 00:05:45,050
uh by

98
00:05:45,090 --> 00:05:50,310
recording just the weights associated with each just the weight associated with each

99
00:05:50,520 --> 00:05:55,800
each of the excise in training data

100
00:05:55,820 --> 00:05:59,780
right in the approach that's used in the typical SVM is to

101
00:06:00,900 --> 00:06:05,600
consider that cluster functions and we minimize some empirical criterion

102
00:06:05,610 --> 00:06:09,560
all right this is the sample average of a cost function again involving y times

103
00:06:09,560 --> 00:06:13,240
f of x plus a penalty term so this is the regularisation

104
00:06:13,280 --> 00:06:17,710
kind of approach we have the regularized empirical

105
00:06:17,720 --> 00:06:20,060
criterion here

106
00:06:20,150 --> 00:06:23,210
and the cost function that we're concerned with

107
00:06:23,250 --> 00:06:26,770
right and in this case is is a piecewise linear function it's the the hinge

108
00:06:28,180 --> 00:06:35,540
all right so

109
00:06:35,560 --> 00:06:39,830
this was intended to give you an overview of the

110
00:06:39,850 --> 00:06:43,790
kinds of approaches that i want to consider

111
00:06:43,810 --> 00:06:47,470
function classes and algorithms for pattern classification

112
00:06:47,470 --> 00:06:51,090
now let's get on two

113
00:06:52,720 --> 00:06:56,070
performance guarantees so getting risks pounds four

114
00:06:56,640 --> 00:07:01,180
algorithms of this kind

115
00:07:01,200 --> 00:07:05,350
so the general approach that i want to use here is the is is rademacher

116
00:07:07,420 --> 00:07:10,460
start by telling you a bit about you know the the very idea of wristbands

117
00:07:10,460 --> 00:07:13,960
i guess that we want to understand how we should control the complexity of the

118
00:07:13,960 --> 00:07:18,050
functions that we have so it's the best trade off the

119
00:07:18,770 --> 00:07:25,930
the competing issues right the that we face these approximation a particular approximation estimation

120
00:07:25,930 --> 00:07:26,870
let's recap where

121
00:07:27,420 --> 00:07:32,200
well up to in the course and what relevant readings in the textbook is

122
00:07:33,260 --> 00:07:40,660
we done a information theory data compression and coding when now inference and data modeling of course

123
00:07:42,200 --> 00:07:48,160
relevant chapters are shown here that you know nothing about what colour methods

124
00:07:48,740 --> 00:07:49,420
nine that

125
00:07:50,710 --> 00:07:53,630
variational methods coming up in the next lecture at the right

126
00:07:54,050 --> 00:07:55,930
like the highlight some additional reading

127
00:07:56,520 --> 00:08:01,010
which covers material we were doing lectures but it's a good thing to do at the cost

128
00:08:01,300 --> 00:08:05,320
i encourage you to read a very short chapter called what is meant that twenty seven

129
00:08:05,900 --> 00:08:09,130
hand in order to understand this lecture and the next one

130
00:08:10,410 --> 00:08:14,060
you may want to read about ising models in chapter that was not already

131
00:08:14,440 --> 00:08:15,150
familiar with them

132
00:08:16,310 --> 00:08:18,800
there's a whole bunch recommended exercises on

133
00:08:19,290 --> 00:08:20,120
the website and

134
00:08:21,030 --> 00:08:22,010
the only screen

135
00:08:23,440 --> 00:08:24,030
you may enjoy

136
00:08:24,580 --> 00:08:25,680
doing the same thing

137
00:08:25,960 --> 00:08:27,870
you want fully understand the material then

138
00:08:28,280 --> 00:08:29,550
doing exercises there

139
00:08:30,090 --> 00:08:31,000
keep this course

140
00:08:31,550 --> 00:08:32,960
the course website in there

141
00:08:33,530 --> 00:08:34,800
the book's website is there

142
00:08:37,350 --> 00:08:39,690
we're in the middle talking about what elements

143
00:08:41,300 --> 00:08:43,160
in the last lecture we introduce them

144
00:08:43,800 --> 00:08:46,640
basic standard want our methods

145
00:08:48,000 --> 00:08:51,290
and the rules of the game are that we are interested in

146
00:08:52,230 --> 00:08:56,520
i read this region called papers and it's red is a bit nasty

147
00:08:58,640 --> 00:08:59,070
it's not

148
00:09:00,240 --> 00:09:02,040
utterly that's we can compute it

149
00:09:02,640 --> 00:09:05,160
so we can write it in minus

150
00:09:07,180 --> 00:09:07,800
on z

151
00:09:08,190 --> 00:09:10,460
and we have got a computer program that can

152
00:09:11,460 --> 00:09:12,960
evaluate this thing before any

153
00:09:13,710 --> 00:09:16,620
the value of x x being a high-dimensional thing

154
00:09:17,280 --> 00:09:19,580
these properties were interested in the distribution

155
00:09:19,990 --> 00:09:21,320
so the rules of the game are

156
00:09:21,920 --> 00:09:23,580
so you can evaluate

157
00:09:29,070 --> 00:09:29,720
but not

158
00:09:30,350 --> 00:09:31,650
z we don't know z

159
00:09:32,300 --> 00:09:34,170
z would be the first we'd like to know there

160
00:09:35,290 --> 00:09:38,450
and we often give up on their because it's so hard know it

161
00:09:40,340 --> 00:09:41,170
and the problems with

162
00:09:41,720 --> 00:09:43,600
i've seen on the one hand

163
00:09:45,860 --> 00:09:46,450
problem one

164
00:09:47,570 --> 00:09:48,510
did make an example

165
00:09:50,320 --> 00:09:51,180
there are

166
00:09:51,960 --> 00:09:52,660
the company

167
00:09:54,040 --> 00:09:54,990
problem that has

168
00:09:56,500 --> 00:09:59,590
you know by solving a problem on or in some other way please tell me

169
00:09:59,590 --> 00:10:02,470
the expected value of some functions and then

170
00:10:03,720 --> 00:10:04,700
and that's red

171
00:10:06,660 --> 00:10:07,840
the average value on

172
00:10:08,870 --> 00:10:10,370
that is some function i'm interested in

173
00:10:14,410 --> 00:10:16,490
that's the average under the he read

174
00:10:17,140 --> 00:10:20,640
distribution of the body several times and i

175
00:10:20,990 --> 00:10:21,380
we want

176
00:10:22,630 --> 00:10:23,690
the average value of

177
00:10:24,570 --> 00:10:25,820
we can use

178
00:10:26,830 --> 00:10:30,480
the solution to the problem one at the problem to because we can

179
00:10:31,230 --> 00:10:33,600
say well here's an estimator all

180
00:10:35,510 --> 00:10:36,690
the expected value

181
00:10:40,860 --> 00:10:42,540
by hand

182
00:10:42,900 --> 00:10:43,330
coming up

183
00:10:44,780 --> 00:10:47,550
at the random sample that you got from the distribution

184
00:10:48,830 --> 00:10:50,580
looking at the empirical average

185
00:10:53,350 --> 00:10:54,600
possible way of using

186
00:10:55,380 --> 00:10:56,600
along to

187
00:10:58,640 --> 00:11:01,960
anne sometimes today than he read distribution

188
00:11:02,730 --> 00:11:03,100
will be

189
00:11:03,530 --> 00:11:09,490
that he always been systems the one example i mentioned in the last lecture lectures maybe we are interested in

190
00:11:12,230 --> 00:11:15,490
then we have spin system with an energy

191
00:11:17,570 --> 00:11:19,820
that depends on the coupling yet

192
00:11:21,220 --> 00:11:22,040
ant d

193
00:11:22,040 --> 00:11:23,840
you need to robots

194
00:11:23,840 --> 00:11:24,860
on the

195
00:11:24,950 --> 00:11:30,650
so this sort of thing seems to be good for

196
00:11:31,930 --> 00:11:33,930
think of reason

197
00:11:33,930 --> 00:11:35,000
so far

198
00:11:35,030 --> 00:11:43,170
should we also actually what i said was sent to what what's new sense

199
00:11:43,950 --> 00:11:48,440
the very fast

200
00:11:48,450 --> 00:11:51,610
notion of how this

201
00:11:51,610 --> 00:11:59,030
there are all kinds of cells in the sense of the centralized computation is that

202
00:11:59,030 --> 00:12:02,510
we are all centralized computation

203
00:12:05,000 --> 00:12:07,060
so that that

204
00:12:07,090 --> 00:12:09,440
i got the assumption

205
00:12:10,530 --> 00:12:11,940
and the point is

206
00:12:11,960 --> 00:12:15,000
well want to know is

207
00:12:15,020 --> 00:12:18,830
the coverage region

208
00:12:18,840 --> 00:12:20,510
because the things

209
00:12:20,570 --> 00:12:21,980
doesn't have other

210
00:12:22,860 --> 00:12:25,710
and is doesn't have gaps

211
00:12:25,740 --> 00:12:27,870
it's curious on what wish

212
00:12:27,880 --> 00:12:30,870
that and see what happens

213
00:12:30,890 --> 00:12:32,880
that allowed

214
00:12:32,910 --> 00:12:35,170
it wasn't the best coverage

215
00:12:35,180 --> 00:12:38,250
with on the ship and crew

216
00:12:40,700 --> 00:12:44,160
so this is this one

217
00:12:44,170 --> 00:12:46,270
this is that we want

218
00:12:51,600 --> 00:12:54,880
it's kind

219
00:12:55,710 --> 00:12:56,740
can't you

220
00:12:56,820 --> 00:12:58,660
what is it really is

221
00:12:59,060 --> 00:13:01,220
so i think

222
00:13:01,410 --> 00:13:04,650
for what i did i

223
00:13:10,160 --> 00:13:12,530
strong reduced

224
00:13:16,620 --> 00:13:17,860
three years

225
00:13:18,330 --> 00:13:20,880
we're all the time

226
00:13:21,690 --> 00:13:23,500
see that

227
00:13:23,650 --> 00:13:27,360
very close to generate lot

228
00:13:27,400 --> 00:13:31,580
the next thing is that this

229
00:13:31,680 --> 00:13:34,750
you can

230
00:13:37,400 --> 00:13:40,740
this was used these users

231
00:13:40,760 --> 00:13:44,350
what is it

232
00:13:44,680 --> 00:13:47,160
then the song has a humid

233
00:13:47,340 --> 00:13:50,860
it means that is really satisfied

234
00:13:50,990 --> 00:13:53,420
we aim

235
00:13:53,430 --> 00:13:54,790
it's not too

236
00:13:58,790 --> 00:14:01,210
one of those circumstances

237
00:14:01,260 --> 00:14:04,430
there exists a surprise

238
00:14:05,780 --> 00:14:09,830
the image

239
00:14:09,870 --> 00:14:13,700
and can do it

240
00:14:17,360 --> 00:14:18,630
he was

241
00:14:18,640 --> 00:14:21,480
how is this

242
00:14:21,540 --> 00:14:23,080
this week

243
00:14:23,080 --> 00:14:27,410
crossing the vision and mission

244
00:14:27,420 --> 00:14:32,320
you do need to be all kinds of things

245
00:14:32,340 --> 00:14:35,840
and you end up with a one sided test set c

246
00:14:35,860 --> 00:14:37,060
published in

247
00:14:37,070 --> 00:14:40,430
your second self yes

248
00:14:41,270 --> 00:14:42,680
multi s

249
00:14:42,700 --> 00:14:45,090
they said yes

250
00:14:45,100 --> 00:14:47,410
the tennessee college

251
00:14:47,480 --> 00:14:54,320
so sitting the car you get to be a call

252
00:14:54,340 --> 00:14:56,400
he says most

253
00:14:56,420 --> 00:14:58,290
what not just yet

254
00:14:58,310 --> 00:15:01,470
why not just

255
00:15:02,850 --> 00:15:04,360
but not just

256
00:15:04,370 --> 00:15:06,290
test has come

257
00:15:06,310 --> 00:15:07,720
c so

258
00:15:07,770 --> 00:15:13,580
you could get false negative you can say not yet you can't guarantee in my

259
00:15:13,620 --> 00:15:15,460
case was

260
00:15:15,470 --> 00:15:23,870
but but on other things that don't know what positives that was

261
00:15:23,930 --> 00:15:26,380
surely there is should

262
00:15:26,430 --> 00:15:28,460
maybe there's whole sentence

263
00:15:28,480 --> 00:15:30,660
but we think not

264
00:15:31,700 --> 00:15:35,190
so we use we use something

265
00:15:35,240 --> 00:15:38,180
the go on precise genetic information

266
00:15:38,190 --> 00:15:41,590
allmusic has become one sided tests

267
00:15:42,780 --> 00:15:44,680
this type one and type two errors

268
00:15:45,840 --> 00:15:48,030
and time

269
00:15:48,090 --> 00:15:53,940
this is what you need to be

270
00:15:53,970 --> 00:15:57,670
and this

271
00:15:57,670 --> 00:16:00,740
it's true one

272
00:16:01,320 --> 00:16:03,540
i just really don't match

273
00:16:07,150 --> 00:16:11,290
the rules that

274
00:16:18,500 --> 00:16:24,500
using the internet

275
00:16:24,570 --> 00:16:26,320
the problem

276
00:16:26,420 --> 00:16:31,530
but this kind of question

277
00:16:33,010 --> 00:16:34,610
it is

278
00:16:34,620 --> 00:16:35,610
the rules

279
00:16:37,250 --> 00:16:41,350
this talk about this in terms of course

280
00:16:42,410 --> 00:16:45,980
this is a generalized

281
00:16:46,000 --> 00:16:49,180
so is

282
00:16:49,180 --> 00:16:50,730
after rain personally

283
00:16:50,770 --> 00:16:55,180
OK this will tell you how likely i to injury encounter that paid or random

284
00:16:56,120 --> 00:16:57,910
given the current model of the web

285
00:16:57,930 --> 00:16:59,290
and i should and

286
00:16:59,310 --> 00:17:02,660
more often on popular pages then

287
00:17:02,680 --> 00:17:04,230
so my webpage

288
00:17:05,700 --> 00:17:11,330
OK so we want to compute the stationary distribution of a lot of markov chain

289
00:17:11,330 --> 00:17:14,410
OK under sir robert certain certain

290
00:17:14,410 --> 00:17:19,540
probability is guaranteed by the way it's set up this there a unique stationary distribution

291
00:17:19,540 --> 00:17:21,390
exists here

292
00:17:21,520 --> 00:17:27,540
so how do i find out the probability of being in each

293
00:17:40,710 --> 00:17:44,520
OK well there's this property so let me set up a

294
00:17:44,580 --> 00:17:51,020
is anyone familiar with the transition matrix

295
00:17:51,060 --> 00:17:57,560
after it will

296
00:17:59,640 --> 00:18:04,020
so a a transition matrix is gonna represent the distribution over the next fifty plus

297
00:18:04,020 --> 00:18:07,830
one given the current state e

298
00:18:08,580 --> 00:18:11,460
so here on the on the columns and guides

299
00:18:11,460 --> 00:18:14,410
current state on the road to the next stage

300
00:18:14,520 --> 00:18:17,680
in every entry in the matrix will be the same probability

301
00:18:17,730 --> 00:18:22,020
of getting to that rose state given that i'm in the called state

302
00:18:25,520 --> 00:18:29,160
however as prime given as

303
00:18:29,200 --> 00:18:32,940
as some

304
00:18:32,960 --> 00:18:36,410
however this

305
00:18:36,580 --> 00:18:39,580
using transmission ixti

306
00:18:42,120 --> 00:18:46,500
so do do the rows columns or someone one

307
00:18:46,520 --> 00:18:49,520
so given that many current state

308
00:18:49,580 --> 00:18:53,330
might just be several possible next has some to one

309
00:18:53,350 --> 00:18:55,180
next a are in the

310
00:19:00,210 --> 00:19:02,600
the rosettes into the one

311
00:19:02,660 --> 00:19:08,310
so they want wanted see the set of

312
00:19:08,370 --> 00:19:10,330
i sorry the columns just

313
00:19:10,410 --> 00:19:11,870
very good

314
00:19:11,920 --> 00:19:15,160
because this situation should should someone

315
00:19:15,350 --> 00:19:19,310
in the course the one i get proper probability distribution

316
00:19:19,410 --> 00:19:21,460
and then if i know

317
00:19:21,460 --> 00:19:25,230
that some state

318
00:19:25,250 --> 00:19:30,140
at some time t

319
00:19:30,230 --> 00:19:31,000
so now

320
00:19:31,020 --> 00:19:32,730
now we may

321
00:19:34,640 --> 00:19:37,980
a vector

322
00:19:38,000 --> 00:19:41,000
from s one down two as

323
00:19:42,750 --> 00:19:45,160
and to use

324
00:19:45,210 --> 00:19:46,250
in by

325
00:19:46,310 --> 00:19:48,960
and encourages in

326
00:19:49,020 --> 00:19:52,250
please just a distribution over

327
00:19:52,310 --> 00:19:53,500
current state

328
00:19:53,500 --> 00:19:56,600
o point one point o one point three

329
00:19:56,680 --> 00:20:01,330
o point five where so that it sums to one

330
00:20:01,560 --> 00:20:04,560
so far i knew that at time t

331
00:20:04,580 --> 00:20:07,520
i had distribution pt

332
00:20:07,520 --> 00:20:11,830
i it is wrong or that i know that my distribution p m t plus

333
00:20:12,960 --> 00:20:19,710
i think is the transition matrix times p so it's either this or the reverse

334
00:20:19,770 --> 00:20:25,600
but you know if it out but the point is that i can't distribution multiplied

335
00:20:25,600 --> 00:20:28,830
by the transition matrix i distribution

336
00:20:28,910 --> 00:20:33,560
if i compute p infinity

337
00:20:33,600 --> 00:20:36,250
given random starting state

338
00:20:36,270 --> 00:20:40,350
that's my stationary distribution

339
00:20:40,370 --> 00:20:43,830
and again you have to certain properties to guarantee that unique

340
00:20:43,870 --> 00:20:46,540
stationary distribution exists

341
00:20:48,080 --> 00:20:52,830
simply because i said that random chance of changing to any page on the web

342
00:20:52,910 --> 00:20:54,250
it's not some problems

343
00:20:54,250 --> 00:20:58,560
the probability that to guarantee that this is the stationary distribution exists

344
00:20:58,600 --> 00:21:01,290
form a markov model

345
00:21:01,330 --> 00:21:06,770
OK so google pagerank simply said the popularity web pages the stationary distribution of that

346
00:21:06,790 --> 00:21:07,830
web page

347
00:21:07,910 --> 00:21:11,370
how often what's the probability in the web page and random surfing

348
00:21:11,440 --> 00:21:14,230
so computing

349
00:21:14,270 --> 00:21:17,210
OK now what away

350
00:21:17,250 --> 00:21:20,000
to solve this

351
00:21:25,620 --> 00:21:26,290
and this

352
00:21:26,310 --> 00:21:28,870
slide here

353
00:21:30,440 --> 00:21:32,910
so what if i said to stationary distribution

354
00:21:33,040 --> 00:21:35,430
OK so so does called high

355
00:21:35,440 --> 00:21:36,750
this is not a policy

356
00:21:36,830 --> 00:21:41,270
this is a stationary distribution

357
00:21:41,270 --> 00:21:44,520
so it is are different sites like

358
00:21:44,980 --> 00:21:47,810
so i'm just

359
00:21:47,830 --> 00:21:50,430
looking at

360
00:21:50,440 --> 00:21:54,850
you have this but you you you you you have this property that p infinity

361
00:21:54,870 --> 00:21:56,180
it was t

362
00:21:56,230 --> 00:21:57,850
p infinity

363
00:21:57,960 --> 00:22:02,230
has a single stationary distribution it back to the stationary distribution

364
00:22:03,770 --> 00:22:05,100
so what you can do

365
00:22:05,120 --> 00:22:08,410
with this

366
00:22:08,410 --> 00:22:14,940
is you can solve a linear system

367
00:22:15,000 --> 00:22:17,140
right so

368
00:22:17,160 --> 00:22:18,770
i can say

369
00:22:18,940 --> 00:22:22,140
tp infinity

370
00:22:22,140 --> 00:22:24,060
minus p infinity

371
00:22:24,250 --> 00:22:26,830
here this pipe there's people here

372
00:22:26,980 --> 00:22:28,890
eagle zero

373
00:22:31,140 --> 00:22:32,600
now in fact around

374
00:22:32,890 --> 00:22:33,960
p infinity

375
00:22:33,980 --> 00:22:36,080
i did t

376
00:22:38,000 --> 00:22:40,390
minus the identity matrix

377
00:22:40,620 --> 00:22:45,060
because there

378
00:22:46,750 --> 00:22:50,520
and now as any constraints on p infinity

379
00:22:50,580 --> 00:22:54,120
like like like like constrained p infinity do any device of this as as as

380
00:22:54,120 --> 00:22:59,000
as a linear system

381
00:22:59,290 --> 00:23:01,040
yes i

382
00:23:01,060 --> 00:23:03,750
seven by transpose

383
00:23:19,040 --> 00:23:21,680
if i additional constraints which are

384
00:23:21,710 --> 00:23:24,680
that the probabilities in the columns sum to one

385
00:23:24,730 --> 00:23:28,430
that's all this solve this this this this linear system

386
00:23:28,980 --> 00:23:30,620
this is

387
00:23:30,680 --> 00:23:34,410
just a linear system right it's linear algebra set of equations

388
00:23:34,430 --> 00:23:37,020
additional constraints saying summer

389
00:23:37,040 --> 00:23:41,460
i have p infinity i one

390
00:23:43,160 --> 00:23:47,680
linear algebra with matrix inversion i can solve this problem is

391
00:23:47,730 --> 00:23:53,330
so we try to convert a trillion dimensional matrix

392
00:23:53,390 --> 00:23:54,850
i don't mind

393
00:23:54,850 --> 00:23:59,650
using this information we are going to constrain the location the possible location of the

394
00:23:59,650 --> 00:24:06,450
person that you can go in this particular seems to want we use it in

395
00:24:06,450 --> 00:24:11,450
terms of floor fields one OK and is motivated by identification and expand the make

396
00:24:11,450 --> 00:24:16,310
this big buildings they have a claimed that there is a fire then how people

397
00:24:16,310 --> 00:24:21,240
are going to make these buildings and they actually fluffy idea

398
00:24:21,270 --> 00:24:23,520
which you can use your so

399
00:24:25,400 --> 00:24:29,420
we have to do is given the location of the person in this frame we

400
00:24:29,420 --> 00:24:33,430
want to find out where it is this person to be

401
00:24:33,450 --> 00:24:38,230
in the next and these are possible places and for each of these players find

402
00:24:38,230 --> 00:24:45,880
the probability of finding confidence and the confidence will depend on the multiple sources of

403
00:24:45,880 --> 00:24:51,750
information and of course appearing but more importantly that seems to show which would be

404
00:24:51,750 --> 00:24:53,860
in terms of flow fields

405
00:24:53,870 --> 00:24:58,980
so fluffy like a very simple we have three kinds of flora field what's gonna

406
00:24:58,980 --> 00:25:05,040
stand floor feed and more to the regions in the scene of which are attractive

407
00:25:05,040 --> 00:25:08,300
in nature like the exact location

408
00:25:08,390 --> 00:25:16,950
his car in the cortex location and the dynamics lockheed martin the local interactions that

409
00:25:16,950 --> 00:25:22,890
that person is going to go based on the neighbouring people and then one of

410
00:25:22,910 --> 00:25:28,770
the flower field is going to look at the which are positive nature and i

411
00:25:28,790 --> 00:25:32,620
was not going to get people i'm going to go there and we want to

412
00:25:32,620 --> 00:25:37,900
model so so in this strange thing we have these three main assumptions one is

413
00:25:37,900 --> 00:25:42,270
we have to say that person is called months ago we actually want to go

414
00:25:43,360 --> 00:25:46,250
and the there is no obstacle

415
00:25:46,280 --> 00:25:50,800
that persons directly going to go there and that will be modest flow

416
00:25:50,970 --> 00:25:54,880
that we have seen the large and i was of quite common structures like three

417
00:25:54,950 --> 00:26:02,500
games was about to various and opposing flows and that's what the flow field in

418
00:26:02,500 --> 00:26:03,540
the final

419
00:26:03,610 --> 00:26:06,190
the person anymore

420
00:26:06,200 --> 00:26:10,970
towards the goal is the flow of the crowd around him or her and that

421
00:26:11,000 --> 00:26:12,820
more the first

422
00:26:13,410 --> 00:26:18,220
so the algorithm is given a sequence like that will our learning period for scene

423
00:26:18,590 --> 00:26:26,050
and you learn static flow fields and the wall computer optical flow and the locations

424
00:26:26,060 --> 00:26:31,540
and then finally sinks which are the places where the motion cease to exist unless

425
00:26:31,900 --> 00:26:38,460
basically why distance for each pixel from that exit location the steady flow field

426
00:26:38,490 --> 00:26:39,270
in the body

427
00:26:39,280 --> 00:26:47,660
he was obviously the segmentation the particle flow may and after defeating and then

428
00:26:47,670 --> 00:26:53,450
find the edges of the boundary fluffy the segmentation and look like a distance transfer

429
00:26:53,520 --> 00:26:55,010
of that i mean

430
00:26:55,030 --> 00:27:01,630
next steps are needed to get the place so given the parts and here we

431
00:27:01,630 --> 00:27:06,220
will look at the appearance and then on my computer

432
00:27:07,060 --> 00:27:15,070
because changing periodically and so given these three philosophies this standing by the flow field

433
00:27:15,150 --> 00:27:18,970
we come of these properties can also be simple idea

434
00:27:18,980 --> 00:27:21,260
but to explain so

435
00:27:21,300 --> 00:27:26,820
the tracking data using these factors that influence the steady flow of

436
00:27:26,880 --> 00:27:28,400
and about the flow field

437
00:27:28,410 --> 00:27:33,270
of course appearance so the main point here is that if you use that appearance

438
00:27:33,270 --> 00:27:37,990
which is traditionally used in the tracking and then you get a flat surface because

439
00:27:37,990 --> 00:27:43,780
the crowd of people who are very similar and that you cannot distinguish them so

440
00:27:43,780 --> 00:27:48,570
you don't have any unique it but if you want to study philosophy dynamic flow

441
00:27:48,570 --> 00:27:53,500
field and so on you get a nice surface and that gives us a unique

442
00:27:53,750 --> 00:27:56,200
place than expected by some to be

443
00:27:56,880 --> 00:28:04,500
so that's the experiments so this sequence so many people here and we want and

444
00:28:04,790 --> 00:28:07,340
the same actually i might

445
00:28:07,420 --> 00:28:11,400
so if you look at these ships could pass

446
00:28:11,670 --> 00:28:17,680
it's pretty small it's about fourteen miles twenty two and had about five hundred frames

447
00:28:17,760 --> 00:28:19,450
was selected by the one

448
00:28:19,470 --> 00:28:22,400
and we would have attracted one hundred forty three of those

449
00:28:22,410 --> 00:28:29,780
so these are kind of typical results showing these tracks by different colours are and

450
00:28:29,790 --> 00:28:34,420
there are lots of five humans it's very difficult to to

451
00:28:34,570 --> 00:28:40,200
keep track of these people show you each independently

452
00:28:40,200 --> 00:28:42,600
a different place one at time

453
00:28:42,680 --> 00:28:45,510
and these are the tracks

454
00:28:45,560 --> 00:28:49,010
and we actually cost and student two

455
00:28:49,340 --> 00:28:54,730
the main trade these people for the country and then

456
00:28:54,750 --> 00:28:57,450
you know we don't want to our

457
00:28:57,490 --> 00:29:03,660
what this work and so these are the competitors tracks the ground truth with our

458
00:29:03,660 --> 00:29:08,690
computed as you see there pretty similar which is good but of course there are

459
00:29:08,690 --> 00:29:11,290
some areas in these areas

460
00:29:11,300 --> 00:29:12,840
and then

461
00:29:12,940 --> 00:29:18,490
so the second sequence even more complex lots of people

462
00:29:20,430 --> 00:29:27,390
these are the and the typical size of what the best sixteen we have about

463
00:29:27,390 --> 00:29:31,080
this is somehow more fragile and gated by context

464
00:29:32,210 --> 00:29:34,480
when you think about it

465
00:29:34,500 --> 00:29:40,540
much of psychotherapy really depends on this extension process if you're afraid of snakes you

466
00:29:40,540 --> 00:29:45,250
don't look at pictures of snakes you touch snakes god forbid let's make no crap

467
00:29:45,390 --> 00:29:46,710
around your neck

468
00:29:46,730 --> 00:29:50,150
but if i can get you look at pictures of snakes make you realize that

469
00:29:50,150 --> 00:29:54,980
can hurt you and that actually touches naked make you realize that they poisonous and

470
00:29:54,980 --> 00:29:58,460
they get support when around your neck you get over the fear of snakes very

471
00:29:58,460 --> 00:30:03,230
well it's very well used the so-called treatment of simple phobias but you actually take

472
00:30:03,230 --> 00:30:07,250
people back to where they have a panic attack they had a panic attack driving

473
00:30:07,250 --> 00:30:09,730
over a bridge you take them back and say you know you not can have

474
00:30:09,730 --> 00:30:13,410
a panic attack again because of petrified of going over the bridge because they're afraid

475
00:30:13,410 --> 00:30:17,350
to have a panic attack if you can talk to get the guy to talk

476
00:30:17,350 --> 00:30:19,980
about the head and head the bamboo platter

477
00:30:20,060 --> 00:30:24,710
reminder that happened forty years ago it's not going to happen to him again for

478
00:30:24,710 --> 00:30:29,710
better for worse that's the best treatment for post-traumatic stress syndrome and people that are

479
00:30:30,540 --> 00:30:35,830
have contamination phobias have to wash their hands continually if you can get in touch

480
00:30:35,830 --> 00:30:40,040
the bottom of the toilet seat and don't let them touch to wash their hands

481
00:30:40,310 --> 00:30:44,190
it's very painful but eventually they realise they don't die if they don't wash their

482
00:30:44,190 --> 00:30:49,890
hands and is effective in treating these people and also cues associated with drug intake

483
00:30:49,980 --> 00:30:53,620
are very powerful listeners of craving

484
00:30:53,640 --> 00:30:58,620
the reason it's so hard to quit cocaine and cigarettes is all these conditions cues

485
00:30:58,620 --> 00:31:03,310
that say what i really want to have a cigarette you know the old joke

486
00:31:03,370 --> 00:31:04,960
is smoke after sex

487
00:31:05,210 --> 00:31:08,230
never looked but the point is you have all these

488
00:31:10,350 --> 00:31:19,640
jane got and she laughed at that point you have all these cues that are

489
00:31:19,640 --> 00:31:23,520
associated with drug intake and they are very powerful listeners

490
00:31:24,420 --> 00:31:28,410
so the idea is that if

491
00:31:28,420 --> 00:31:32,520
we could

492
00:31:32,520 --> 00:31:36,180
we actually say we have some data now

493
00:31:36,230 --> 00:31:38,250
i threw myself off with job

494
00:31:38,290 --> 00:31:43,310
so we actually have some data now

495
00:31:45,480 --> 00:31:53,120
extinction is dependent on gaba and maybe coming from the medial prefrontal cortex great greg

496
00:31:53,120 --> 00:31:54,390
quirk and

497
00:31:58,140 --> 00:32:01,460
together that's my name for me

498
00:32:01,480 --> 00:32:06,390
holland noord done some beautiful work showing that the prefrontal cortex is critically involved in

499
00:32:06,390 --> 00:32:10,980
extinction we don't know what this area is we think it's exerts inhibition by gamma

500
00:32:11,330 --> 00:32:13,060
and so the idea is

501
00:32:14,790 --> 00:32:20,660
sorry i get something wrong with my slides rate if you could combine these cycles

502
00:32:20,660 --> 00:32:27,600
with exposure based psychotherapy could you improve the effectiveness of the therapy so if psychotherapy

503
00:32:27,600 --> 00:32:33,540
depends on this extinction process we know that we can facilitate extinctions with instruction with

504
00:32:33,560 --> 00:32:39,560
animals with the cycle surrounding can you facilitate psychotherapy with the cyclists

505
00:32:39,600 --> 00:32:43,310
so the preload for that is to show

506
00:32:43,350 --> 00:32:46,980
that the NMDA receptor is critical for extinction

507
00:32:47,000 --> 00:32:52,290
and let me just show you a little bit about the NMDA receptor in cartoon

508
00:32:52,290 --> 00:32:57,230
form b is going to talk about two binding sites one glutamate binding site and

509
00:32:57,230 --> 00:32:59,620
another called the glycine binding site

510
00:32:59,640 --> 00:33:05,750
so it turns out that of glutamate binds to the NMDA receptor nothing happens

511
00:33:05,750 --> 00:33:12,620
it requires the concomitant binding of glycine so so if guy senior which are divergence

512
00:33:12,620 --> 00:33:19,870
amino acids bind also nothing happens but now when glutamate binds you get an activation

513
00:33:19,870 --> 00:33:25,370
of the NMDA channel calcium rushes in and things happen in the cell

514
00:33:25,410 --> 00:33:30,770
and so the idea was that if we could block the NMDA receptor could we

515
00:33:30,770 --> 00:33:32,390
block extension

516
00:33:32,410 --> 00:33:33,390
and if so

517
00:33:33,420 --> 00:33:37,870
that would be evidence then that the NMDA receptor was involved in extinction

518
00:33:37,870 --> 00:33:42,080
and so we have two groups of animals we train them to be afraid we

519
00:33:42,100 --> 00:33:47,580
give them a pre-test and that both groups get sixty lights without sharks

520
00:33:47,600 --> 00:33:52,810
after infusion of vehicle or an MBA in tag called a five directly into the

521
00:33:52,810 --> 00:33:56,040
amygdala and then we test them a couple of days later

522
00:33:56,060 --> 00:34:01,060
and sure enough what we find is good extinction the control animals but remarkably a

523
00:34:01,060 --> 00:34:06,330
total blockade of extension in animals and had this NMDA receptor blocker

524
00:34:10,160 --> 00:34:13,810
this is where we get into one of the idea of the cycle serious so

525
00:34:13,810 --> 00:34:19,750
again when you have glycine binding and glutamate binding you get activation of this channel

526
00:34:19,830 --> 00:34:22,140
but under certain circumstances

527
00:34:22,140 --> 00:34:25,940
and so this is what we have here

528
00:34:25,960 --> 00:34:30,450
the these are three different features two on the left are

529
00:34:30,460 --> 00:34:31,790
all relevant features

530
00:34:31,800 --> 00:34:34,580
and the third one is irrelevant feature

531
00:34:34,600 --> 00:34:35,940
the top lots

532
00:34:36,070 --> 00:34:38,190
basically given was unit waiting

533
00:34:38,290 --> 00:34:39,120
this is the

534
00:34:39,910 --> 00:34:42,180
the distribution of information games

535
00:34:42,230 --> 00:34:44,330
and here we have

536
00:34:44,370 --> 00:34:46,950
when we waited using the complexity measure

537
00:34:46,960 --> 00:34:50,980
so the spikes going on on the right these plots

538
00:34:50,990 --> 00:34:55,000
they were actually caused by the smaller nodes to being split

539
00:34:55,010 --> 00:34:57,270
is purely by chance applied to the features

540
00:34:57,320 --> 00:35:00,760
and we find that we actually suppress these when we

541
00:35:00,810 --> 00:35:03,120
i mean on the complexity

542
00:35:03,170 --> 00:35:06,520
that i can see if you take the average of these distributions classifier with the

543
00:35:08,700 --> 00:35:14,640
a much more accurate measure of the feature relevance

544
00:35:14,720 --> 00:35:17,540
OK so what can we do these measures feature relevance well

545
00:35:17,550 --> 00:35:19,760
you could do some sort of feature selection schemes

546
00:35:21,020 --> 00:35:24,600
rank them in order and select the k best output

547
00:35:24,750 --> 00:35:26,550
some sort of threshold

548
00:35:26,780 --> 00:35:29,200
or we could do some sort of feature weighting

549
00:35:29,220 --> 00:35:33,140
well actually rely on the features to a different extent according to how relevant thing

550
00:35:34,080 --> 00:35:38,930
and with random forest we can do this by altering the feature sampling distribution

551
00:35:38,940 --> 00:35:44,750
a member of said that if these features are chosen randomly split with equal probability

552
00:35:44,770 --> 00:35:48,080
now we can do is we can actually change the probabilities so the more relevant

553
00:35:48,080 --> 00:35:49,520
features can be chosen

554
00:35:49,530 --> 00:35:51,600
with greater all this

555
00:35:51,610 --> 00:35:55,620
this can be done in two ways we could do a parallel skiing

556
00:35:55,670 --> 00:35:56,850
we're actually

557
00:35:56,860 --> 00:35:58,180
delta force and after

558
00:35:58,230 --> 00:36:02,540
peachtree we could get some measure of how relevant features are

559
00:36:02,590 --> 00:36:05,930
and use that to update features that distribution

560
00:36:05,940 --> 00:36:08,130
the problem with this is that

561
00:36:08,140 --> 00:36:10,210
in the initial stage of the algorithm

562
00:36:10,290 --> 00:36:13,420
you don't have very much information about features

563
00:36:14,560 --> 00:36:16,320
you have to be careful not to

564
00:36:16,370 --> 00:36:18,170
overweight some features

565
00:36:18,180 --> 00:36:20,380
because the album may not be able to recover from

566
00:36:20,430 --> 00:36:22,190
as persons

567
00:36:22,230 --> 00:36:25,570
the other way and is by using a two stage approach

568
00:36:25,580 --> 00:36:28,440
well we analyse the feature elements

569
00:36:28,490 --> 00:36:29,740
in the first stage

570
00:36:29,750 --> 00:36:31,600
and then we use that to fix

571
00:36:31,610 --> 00:36:39,170
the thing to some distribution prior to the forced construction

572
00:36:39,180 --> 00:36:41,840
OK so a parallel scheme

573
00:36:41,890 --> 00:36:47,120
i want to do is try to control this update right established mission we chose

574
00:36:47,120 --> 00:36:49,250
confidence intervals in this case

575
00:36:50,290 --> 00:36:53,740
we can use this statistic which is the basis of the average information gain here

576
00:36:54,780 --> 00:36:56,790
a theoretical to me

577
00:36:57,250 --> 00:37:00,660
sample size and sample standard deviation

578
00:37:00,680 --> 00:37:01,540
has the

579
00:37:01,550 --> 00:37:04,640
students two distribution minimise one degrees of freedom

580
00:37:04,660 --> 00:37:06,710
so it can form

581
00:37:07,200 --> 00:37:09,880
one conference limits

582
00:37:09,890 --> 00:37:11,440
on the statistic

583
00:37:11,820 --> 00:37:14,110
he went two

584
00:37:14,160 --> 00:37:18,120
which gives us some level of confidence which we can set so we would use

585
00:37:18,120 --> 00:37:19,870
ninety five percent the case

586
00:37:19,880 --> 00:37:23,950
and organism rearranged so we get these confidence bounds

587
00:37:23,990 --> 00:37:27,390
on our project management which

588
00:37:27,410 --> 00:37:30,780
now the idea is that once before these

589
00:37:30,830 --> 00:37:35,440
confidence intervals around it to measure feature relevance with within use that to update the

590
00:37:35,440 --> 00:37:39,650
petersen distribution by maintaining the uniform distribution

591
00:37:39,690 --> 00:37:43,430
but it still lies within each of these confidence intervals

592
00:37:43,450 --> 00:37:47,150
so that was the algorithm proceeds these companies will become smaller

593
00:37:47,220 --> 00:37:50,070
starts also the features and distribution to a greater extent

594
00:37:50,080 --> 00:37:52,250
but in the initial stages

595
00:37:52,270 --> 00:37:53,470
it should

596
00:37:53,510 --> 00:37:58,500
ere on the side of caution

597
00:37:58,560 --> 00:38:01,270
OK so i'm going to get the convergence rates

598
00:38:01,320 --> 00:38:02,340
these plots

599
00:38:02,360 --> 00:38:04,030
these plots showing

600
00:38:04,050 --> 00:38:06,520
the average confidence interval size

601
00:38:06,540 --> 00:38:09,270
averaged over all of features for the datasets

602
00:38:09,880 --> 00:38:11,970
these convergence rates

603
00:38:12,020 --> 00:38:16,150
basically reflects how fast we converted into of

604
00:38:16,160 --> 00:38:17,610
to some distribution

605
00:38:20,800 --> 00:38:24,670
they depend on the level of conferences also depend on the number of features in

606
00:38:24,670 --> 00:38:29,410
the data and on the average tree size sixty percent of its

607
00:38:29,420 --> 00:38:32,460
it's got slower convergence because has got the highest number of features

608
00:38:32,470 --> 00:38:36,420
and this built six large trees so not generating

609
00:38:36,430 --> 00:38:38,190
but many samples

610
00:38:38,190 --> 00:38:41,970
the all the visible clustering functions do satisfy

611
00:38:41,990 --> 00:38:43,650
this relaxed

612
00:38:43,670 --> 00:38:47,280
consistency requirements

613
00:38:47,280 --> 00:38:52,740
you can you can play with expansion for every pair but i want do it

614
00:38:52,740 --> 00:38:54,070
when you allow

615
00:38:55,780 --> 00:39:01,690
constance for every pair of clusters you can create a counterexample like this because

616
00:39:01,710 --> 00:39:03,720
is if i ever had

617
00:39:03,740 --> 00:39:06,400
human history clusters

618
00:39:06,420 --> 00:39:11,150
now expanded such as this cluster is here this clusters is here and third class

619
00:39:11,150 --> 00:39:13,110
that is here

620
00:39:13,170 --> 00:39:20,420
and now what you actually do is say this is one cluster and the cluster

621
00:39:20,490 --> 00:39:24,320
because with these proportions they look to get

622
00:39:24,340 --> 00:39:29,090
so i want to fix constants for the between cluster in the

623
00:39:30,190 --> 00:39:32,170
so we have such

624
00:39:32,220 --> 00:39:37,030
relaxation of those axioms in such a way that we have pretty happy every natural

625
00:39:37,030 --> 00:39:40,340
clustering function satisfies all of them

626
00:39:40,400 --> 00:39:45,260
of course to show that every now and clustering function fails one of them

627
00:39:45,280 --> 00:39:49,240
we have to come up with an exhaustive list of not clustering functions and

628
00:39:49,380 --> 00:39:52,570
that's not something which is readily

629
00:39:52,570 --> 00:39:54,630
do it but i don't know how to formulate

630
00:39:56,360 --> 00:39:58,720
as a summary

631
00:39:58,760 --> 00:40:01,210
come on

632
00:40:01,220 --> 00:40:05,950
let me give you some examples of clusterings which do not fall into the climate

633
00:40:06,090 --> 00:40:11,740
agreement so the climate is that your input is just a pair set of points

634
00:40:11,740 --> 00:40:13,130
and pairwise distances

635
00:40:13,150 --> 00:40:15,490
but sometimes when you do clustering

636
00:40:15,490 --> 00:40:21,760
the relationship you look not just pairwise relationship that so for example in edge detection

637
00:40:21,820 --> 00:40:26,190
if i just gave you pairs of distances you will not know which points on

638
00:40:26,220 --> 00:40:28,050
the same page but

639
00:40:28,050 --> 00:40:30,720
a relationship like this if i

640
00:40:30,740 --> 00:40:34,820
if you have points like this and say

641
00:40:34,990 --> 00:40:39,820
you want to cluster this picture and we naturally cluster into here is one class

642
00:40:40,010 --> 00:40:41,690
here's another cluster

643
00:40:41,690 --> 00:40:42,490
but not

644
00:40:42,510 --> 00:40:47,530
looking at the pairwise distances but we're looking at the linear relationship which involves triples

645
00:40:47,530 --> 00:40:51,300
of points rather than pairs of points on may be more than triples of points

646
00:40:51,570 --> 00:40:56,740
so it doesn't fall into the framework the clustering is just the function of pairs

647
00:40:56,780 --> 00:40:59,760
here the the function could be

648
00:40:59,780 --> 00:41:02,010
the clustering could be a function of

649
00:41:02,030 --> 00:41:04,240
triple wise

650
00:41:04,240 --> 00:41:06,970
relationship which collinearity

651
00:41:06,990 --> 00:41:09,800
so edge detection is a good example of clustering

652
00:41:09,820 --> 00:41:14,440
which is that in practice but does not fall into this framework which is based

653
00:41:14,440 --> 00:41:17,440
on just the with this is that we're

654
00:41:27,860 --> 00:41:34,800
now i'm not fully contributors

655
00:41:34,800 --> 00:41:44,190
no i think i if i apply to our intuitive semantics of what to do

656
00:41:44,190 --> 00:41:45,740
when you do edge detection

657
00:41:45,760 --> 00:41:50,420
we do have class and detection is the task is to use clustering for when

658
00:41:50,420 --> 00:41:55,560
you have a big show you want to detect objects you do education is all

659
00:41:55,560 --> 00:41:59,340
those points lying on the same page

660
00:42:02,900 --> 00:42:13,820
if you just you just to thank you all

661
00:42:15,240 --> 00:42:19,110
i'm talking about you you assume that your picture is now called it is just

662
00:42:19,110 --> 00:42:20,570
binary prefixes

663
00:42:20,590 --> 00:42:25,280
and now you want to detect just on by over binary mixtures saying it on

664
00:42:25,650 --> 00:42:29,420
all binary pixels when you want to say those i mean an edge is more

665
00:42:29,420 --> 00:42:34,170
than just the pairwise distances it is some notion of linearity of continuity of the

666
00:42:34,170 --> 00:42:38,300
past or simplicity of the past the connector

667
00:42:47,150 --> 00:42:51,550
you can easily do it in the petitioning i mean i'm

668
00:42:51,570 --> 00:42:56,030
and that better drawing but it could be the case that your point

669
00:42:56,050 --> 00:42:57,550
look like this

670
00:42:57,550 --> 00:43:03,760
right now i ask you to clustering and you say here is one class and

671
00:43:03,760 --> 00:43:10,760
here's another cluster and there's no point belong simultaneously to both clusters

672
00:43:15,840 --> 00:43:23,840
with what is it that's what

673
00:43:27,170 --> 00:43:32,950
OK so my data representation here is just a set of elements and pairwise distances

674
00:43:32,990 --> 00:43:37,090
and what i'm saying is that some things which cannot be captured by this very

675
00:43:37,090 --> 00:43:39,840
simple representation

676
00:43:39,860 --> 00:43:44,510
if i have here on top of the distances between the points i also have

677
00:43:44,590 --> 00:43:49,470
some geometrical structure i e really embedded in two

678
00:43:49,510 --> 00:43:53,150
and i have some relationship which are

679
00:43:53,190 --> 00:43:57,530
more complex adjust the pairwise distances

680
00:44:00,800 --> 00:44:02,440
in this analysis

681
00:44:02,510 --> 00:44:05,470
the input is only a set of pairwise this

682
00:44:05,510 --> 00:44:08,570
why not

683
00:44:08,880 --> 00:44:17,170
OK but also way so vector quantization he's

684
00:44:17,590 --> 00:44:22,610
and if you center is allowed to be UK you can also formulas k means

685
00:44:22,610 --> 00:44:23,530
in this way

686
00:44:23,530 --> 00:44:27,820
because k means although all the information it looks at is the pairwise distances you

687
00:44:27,820 --> 00:44:28,920
want to minimize

688
00:44:29,280 --> 00:44:33,110
you worried about choosing the centre outside your inputs it

689
00:44:33,130 --> 00:44:39,110
right but you can say this can be overcome by some technicolor addition but if

690
00:44:39,110 --> 00:44:44,550
not just consider vector quantization in which you are forced to pick

691
00:44:44,570 --> 00:44:52,240
but i don't want to get into this game of examples and counterexamples i enjoyed

692
00:44:52,260 --> 00:44:58,840
very much but i don't think it's so instructed the use of of the time

693
00:44:59,720 --> 00:45:03,150
here is another example of clustering which is not

694
00:45:03,170 --> 00:45:07,210
based on just what this is and this is a texture clustering if i give

695
00:45:07,210 --> 00:45:09,530
you such a picture editor class right

696
00:45:09,550 --> 00:45:10,650
then you say

697
00:45:10,670 --> 00:45:15,470
i mean and you can have more interesting result it just means limitation enjoying

698
00:45:15,510 --> 00:45:19,630
of text they want to cluster things by the texture it's more than just the

699
00:45:19,630 --> 00:45:26,760
pairwise distances it's something about more points together relationship which involve more points together

700
00:45:27,760 --> 00:45:32,130
and now that nice example that i know that i really like

701
00:45:32,150 --> 00:45:38,820
is the professors example because all is perfect example of clustering one of clustering task

702
00:45:38,840 --> 00:45:44,420
people at NIPS especially like to test themselves on is you take the web pages

703
00:45:44,420 --> 00:45:47,210
of your university that's what you have

704
00:45:47,210 --> 00:45:48,750
there is an underlying function why

705
00:45:49,330 --> 00:45:52,580
the generated the data by flipping bent coins with bias so why

706
00:45:53,520 --> 00:45:55,450
so that's the interpolation of this so

707
00:46:00,160 --> 00:46:01,810
why using a bent coin

708
00:46:03,330 --> 00:46:04,450
of virus

709
00:46:06,700 --> 00:46:07,200
what this means

710
00:46:09,930 --> 00:46:14,700
the regularizer that's being used a penalized enormous weights because we decided we didn't like enormous waste

711
00:46:15,310 --> 00:46:19,580
is interpreted as the prior belief about the weights that says if you w is

712
00:46:21,520 --> 00:46:25,370
of w squared for example is saying that the prior probability of w

713
00:46:25,810 --> 00:46:26,930
is a normal distribution

714
00:46:28,450 --> 00:46:29,180
the main zero

715
00:46:30,390 --> 00:46:32,290
hand with standard deviation

716
00:46:32,700 --> 00:46:33,500
sigma squared

717
00:46:36,790 --> 00:46:40,430
is equal to one overall for so this parameter alpha which a moment ago

718
00:46:40,890 --> 00:46:46,410
a moment ago is a pain in the neck arbitrary regularisation constant is now interpreted as being the variance

719
00:46:46,950 --> 00:46:47,980
have a prior distribution

720
00:46:52,180 --> 00:46:53,480
this interpretation can be

721
00:46:54,500 --> 00:46:56,680
ignored or it can be exploited

722
00:46:58,850 --> 00:47:03,540
the exportation says okay let's take this literally and then we can do a whole new things

723
00:47:03,950 --> 00:47:05,620
for example we can actually ask

724
00:47:06,950 --> 00:47:09,600
what's the right way to turn inference into prediction

725
00:47:11,200 --> 00:47:13,390
if someone asks for prediction what should do

726
00:47:14,310 --> 00:47:16,020
well if you believe this model is correct

727
00:47:16,750 --> 00:47:21,750
and you're asked to predict what's the probability that the next thing that we just

728
00:47:21,750 --> 00:47:25,620
observed at location x plus x and plus one

729
00:47:26,580 --> 00:47:32,370
is in class one what's the probability that this new point on the screen at say they'll be or wherever

730
00:47:32,790 --> 00:47:33,770
what's the probability

731
00:47:34,180 --> 00:47:35,390
that same potato

732
00:47:35,890 --> 00:47:36,410
or cat

733
00:47:36,810 --> 00:47:37,500
or raising

734
00:47:40,580 --> 00:47:40,950
the data

735
00:47:41,520 --> 00:47:43,830
we've seen so far and all the assumptions

736
00:47:45,450 --> 00:47:50,210
will be answered that question isn't found by saying please work at the w that

737
00:47:50,210 --> 00:47:53,580
minimizes this and then use to make predictions you answer is done

738
00:47:54,020 --> 00:47:59,140
by marginalizing so the correct answer if you believe all these assumptions is you should marginalise

739
00:47:59,580 --> 00:48:00,230
over w

740
00:48:00,770 --> 00:48:02,730
posterior probability p w given data

741
00:48:04,080 --> 00:48:06,020
multiplied by the prediction which is why

742
00:48:07,000 --> 00:48:07,970
evaluated at

743
00:48:08,600 --> 00:48:09,390
example of one

744
00:48:11,310 --> 00:48:11,830
the parameters

745
00:48:13,290 --> 00:48:17,310
so that's what you actually ought to do if you believe the assumptions that you've implicitly

746
00:48:17,750 --> 00:48:18,230
in making

747
00:48:20,140 --> 00:48:21,500
that's an example of

748
00:48:22,230 --> 00:48:25,250
an average value of a simple function under nasty

749
00:48:28,560 --> 00:48:29,930
hand we know freeways

750
00:48:31,020 --> 00:48:31,950
the cat sat elephant

751
00:48:32,660 --> 00:48:33,870
we can use monte carlo methods

752
00:48:34,620 --> 00:48:36,060
we can use variational methods

753
00:48:39,580 --> 00:48:40,390
we can use

754
00:48:42,200 --> 00:48:43,000
the pluses methods

755
00:48:45,830 --> 00:48:46,560
so let's do that

756
00:48:48,950 --> 00:48:54,970
just to make clear what is read the nasty red distribution is thing posterior probability the weights given the data

757
00:48:55,640 --> 00:48:56,080
and that's

758
00:48:56,730 --> 00:48:57,500
is this thing here

759
00:48:59,080 --> 00:49:00,120
it's a nasty red thing

760
00:49:00,660 --> 00:49:03,520
but we can compute it has the form the minus and w

761
00:49:03,910 --> 00:49:05,480
and we have got a formula four

762
00:49:07,390 --> 00:49:09,200
so we can attack this in a variety of ways

763
00:49:10,330 --> 00:49:14,060
and the first one way i will use is called multicolored

764
00:49:14,980 --> 00:49:17,790
so we can take our handbook a monte carlo methods

765
00:49:19,680 --> 00:49:20,230
we can say

766
00:49:21,480 --> 00:49:21,830
goodness me

767
00:49:23,200 --> 00:49:23,770
we can say

768
00:49:24,410 --> 00:49:27,290
forward five sites and

769
00:49:31,410 --> 00:49:31,890
we can say

770
00:49:32,700 --> 00:49:34,560
let's give ourselves and one colour methods

771
00:49:37,830 --> 00:49:38,700
use carlo

772
00:49:40,810 --> 00:49:42,370
evaluate this quantity here

773
00:49:43,710 --> 00:49:45,810
how different is that's well if we use

774
00:49:46,470 --> 00:49:49,980
hamiltonian monte carlo during the hamiltonian encounter a few lectures ago

775
00:49:50,410 --> 00:49:56,160
don't involve computing the gradient and then going downhill with momentum and randomizing the momentum from time to time

776
00:49:57,000 --> 00:50:01,270
a very simple version a very hamiltonian monte carlo methods called along provide methods

777
00:50:01,710 --> 00:50:02,500
which involves

778
00:50:04,180 --> 00:50:06,350
going downhill a little bit and adding some noise

779
00:50:08,830 --> 00:50:11,100
how does different differ from steepest descent

780
00:50:12,180 --> 00:50:12,700
it differs by

781
00:50:13,180 --> 00:50:14,000
having a better noise

782
00:50:14,560 --> 00:50:16,080
so steepest descent just goes downhill

783
00:50:16,500 --> 00:50:19,470
what we're gonna do now is to see sense with a bit of noise

784
00:50:20,930 --> 00:50:23,660
under the and accept reject decision in there as well

785
00:50:24,860 --> 00:50:28,740
so we can use the long bound methods hand if we go back to

786
00:50:29,310 --> 00:50:29,740
this thing

787
00:50:33,020 --> 00:50:33,890
what we will now see

788
00:50:38,850 --> 00:50:40,710
the larger methods starting from

789
00:50:41,240 --> 00:50:42,600
this place at the origin

790
00:50:43,110 --> 00:50:46,390
where we use the just go downhill and here's where we ended up having gone

791
00:50:46,390 --> 00:50:49,610
downhill when just the same amount of computer time all we have to do

792
00:50:50,060 --> 00:50:53,570
is going downhill and add some noise is actually not a huge cost to do this

793
00:50:54,020 --> 00:50:57,940
fancy bayesian inference methods you just add some noise at the right amount of noise

794
00:50:58,490 --> 00:51:01,370
and it put in accepts and rejects to make sure it's all

795
00:51:04,250 --> 00:51:08,530
here's what happens in a zoom in on the first elbow be optimization

796
00:51:09,210 --> 00:51:10,230
so this is the first album

797
00:51:10,710 --> 00:51:14,070
and during the same amount of computer time is where we get to

798
00:51:15,440 --> 00:51:20,690
with a large methods are goes on around the goes are generally downhill that adding noise in an interesting way

799
00:51:23,710 --> 00:51:26,530
what you can think of happening is we used go

800
00:51:27,370 --> 00:51:29,360
in w space w one w two

801
00:51:29,790 --> 00:51:34,240
it's actually three dimensional for this toy problem we use go downhill to an optimum

802
00:51:34,690 --> 00:51:37,230
now we are being there as the minimum of the red

803
00:51:38,000 --> 00:51:38,850
function which

804
00:51:39,650 --> 00:51:40,100
has some

805
00:51:41,160 --> 00:51:42,230
strange shaped like this

806
00:51:43,650 --> 00:51:49,080
hand we're going downhill and adding noise in such a way as the sample from time red distribution

807
00:51:50,990 --> 00:51:56,230
okay here's what happens after forty thousand iterations we commonly believe only and we've sampled

808
00:51:56,700 --> 00:51:58,020
from the entire red

809
00:52:00,530 --> 00:52:03,320
and we can take those sample points

810
00:52:03,980 --> 00:52:04,820
hand add up

811
00:52:06,600 --> 00:52:07,530
the predictions why

812
00:52:08,040 --> 00:52:09,900
for each of those so we can approxlmate

813
00:52:12,270 --> 00:52:14,440
this using the standard monte carlo idea

814
00:52:16,710 --> 00:52:18,560
this is approximately one over are

815
00:52:20,990 --> 00:52:22,160
are is what are

816
00:52:25,150 --> 00:52:26,120
the second plus one

817
00:52:28,040 --> 00:52:28,690
using weight

818
00:52:29,490 --> 00:52:30,690
are where these weights

819
00:52:31,190 --> 00:52:31,710
w are

820
00:52:32,400 --> 00:52:32,940
come from

821
00:52:37,200 --> 00:52:37,970
the miracle monte

822
00:52:39,250 --> 00:52:39,860
hamiltonian in

823
00:52:42,580 --> 00:52:45,080
all right that's the plan so suggested just some a are often

824
00:52:45,860 --> 00:52:46,860
and i'll be there right now

825
00:52:48,650 --> 00:52:51,180
so manganese forty thousand just pick

826
00:52:53,410 --> 00:52:54,600
hundred also samples

827
00:52:55,120 --> 00:52:59,600
here's what the weights were doing with time wandering up and down autocorrelated weight

828
00:52:59,600 --> 00:53:02,200
built with what

829
00:53:02,200 --> 00:53:04,620
and then you see some improving coming down

830
00:53:04,630 --> 00:53:07,050
both sides see that year

831
00:53:08,790 --> 00:53:11,000
nicely insulated plastic

832
00:53:11,080 --> 00:53:15,160
norris regular TED for nothing special

833
00:53:15,170 --> 00:53:19,680
and then he was a coffee can and here is a coffee can

834
00:53:19,700 --> 00:53:21,540
open he open here

835
00:53:21,550 --> 00:53:23,620
open he opened must be met

836
00:53:23,630 --> 00:53:24,920
everything that you see

837
00:53:24,930 --> 00:53:27,130
you can read must be metal

838
00:53:27,200 --> 00:53:28,680
it's essential for

839
00:53:28,760 --> 00:53:30,830
this to work

840
00:53:30,840 --> 00:53:31,540
and no

841
00:53:31,540 --> 00:53:34,340
you left the water running

842
00:53:34,350 --> 00:53:38,750
out here in know groundwater on their so you see water running here

843
00:53:38,790 --> 00:53:42,040
you can sort of just

844
00:53:42,050 --> 00:53:44,080
and collected here

845
00:53:44,090 --> 00:53:45,200
in two

846
00:53:45,240 --> 00:53:47,370
metal rockets use one of them

847
00:53:47,380 --> 00:53:49,880
and here is the

848
00:53:49,890 --> 00:53:51,920
what is important now

849
00:53:52,030 --> 00:53:54,290
that was some copper or some other

850
00:53:55,180 --> 00:53:56,670
you can connect this

851
00:53:57,880 --> 00:53:58,950
with this one

852
00:53:58,960 --> 00:54:03,140
this can with this one this means that these two should not touch to other

853
00:54:03,140 --> 00:54:06,200
means you have to jump over see you see them here

854
00:54:06,210 --> 00:54:09,250
they are not touching

855
00:54:09,300 --> 00:54:11,090
and now you bought

856
00:54:11,090 --> 00:54:13,620
all of these cancers

857
00:54:13,660 --> 00:54:17,060
a metal rods and then at the end of the rod

858
00:54:17,080 --> 00:54:18,500
you can easily

859
00:54:18,560 --> 00:54:20,630
demonstrated you get sparks

860
00:54:20,630 --> 00:54:24,550
you can get sixty thousand volts and today as i said maybe

861
00:54:24,550 --> 00:54:25,710
only twenty

862
00:54:25,720 --> 00:54:28,210
thirty thousand falls and this is called

863
00:54:29,450 --> 00:54:30,990
one drop

864
00:54:31,000 --> 00:54:32,580
it was

865
00:54:32,630 --> 00:54:34,170
he invented by

866
00:54:34,220 --> 00:54:37,370
englishman william ponsonby lord kelvin

867
00:54:37,410 --> 00:54:40,740
in the nineteenth century

868
00:54:40,850 --> 00:54:43,090
so i'll started running now

869
00:54:43,100 --> 00:54:45,990
and then i will change the light situation so that

870
00:54:45,990 --> 00:54:48,490
you know the better

871
00:54:48,540 --> 00:54:54,770
look at it

872
00:54:54,840 --> 00:55:00,450
i think it's running

873
00:55:00,500 --> 00:55:02,880
i we have bubbles that's not so good

874
00:55:02,880 --> 00:55:09,000
is it to TV setting three

875
00:55:10,670 --> 00:55:13,390
so let me change the light settings a little

876
00:55:13,410 --> 00:55:15,170
all three

877
00:55:15,180 --> 00:55:17,250
and one and four

878
00:55:17,290 --> 00:55:18,790
then see

879
00:55:20,680 --> 00:55:23,770
the opening year and if you wait and you look you you will see there

880
00:55:23,770 --> 00:55:27,050
is this one

881
00:55:27,050 --> 00:55:35,790
look at the you can see it but also look there

882
00:55:35,950 --> 00:55:37,670
to be a little patient

883
00:55:37,680 --> 00:55:40,200
there's another one

884
00:55:40,210 --> 00:55:43,310
and so the gap opening is like a spark plug

885
00:55:43,380 --> 00:55:48,340
is about half a centimeter

886
00:55:48,350 --> 00:55:50,260
so i would say

887
00:55:50,310 --> 00:55:52,120
fifteen twenty thousand holes

888
00:55:57,240 --> 00:56:01,960
and this can be built on i'll tell you what this key if you want

889
00:56:01,960 --> 00:56:03,410
to build it at all

890
00:56:03,410 --> 00:56:04,640
it's not difficult

891
00:56:04,710 --> 00:56:06,160
what is key is

892
00:56:06,170 --> 00:56:07,710
that this can

893
00:56:07,720 --> 00:56:11,410
and this can

894
00:56:12,140 --> 00:56:16,950
supported by a very high insulating material you can see that here

895
00:56:17,010 --> 00:56:19,510
is a huge collateral or plastic

896
00:56:19,550 --> 00:56:21,550
and this is also plus

897
00:56:21,590 --> 00:56:23,590
equally important is

898
00:56:23,630 --> 00:56:27,120
that these buckets at the bottom which are made of metal

899
00:56:27,140 --> 00:56:30,130
i completely insulated from the table

900
00:56:30,130 --> 00:56:34,960
so you can put them for instance on a stack of a glass

901
00:56:35,010 --> 00:56:37,720
plates or porcelain plates if you have

902
00:56:37,770 --> 00:56:42,120
but if you put him straight on the table you will never work

903
00:56:42,160 --> 00:56:44,620
and that's the mistake that most students make

904
00:56:44,630 --> 00:56:47,380
that they don't have the proper

905
00:56:47,380 --> 00:56:50,460
electric installation where it's needed

906
00:56:51,170 --> 00:56:52,740
this has to be supported

907
00:56:52,740 --> 00:56:54,000
with scintillators

908
00:56:54,040 --> 00:56:57,120
and also with and then you can do it

909
00:56:57,130 --> 00:57:04,890
it's not that difficult

910
00:57:05,000 --> 00:57:06,380
the ultimate

911
00:57:06,380 --> 00:57:11,630
as the features may be to discuss is one along the lines of what he

912
00:57:11,630 --> 00:57:13,130
said was starting to say

913
00:57:13,150 --> 00:57:18,760
how to move around in machine learning what kind of directions you

914
00:57:18,820 --> 00:57:23,510
wonderful when you start doing research and then the other one as this

915
00:57:23,990 --> 00:57:30,370
what is there and beyond phd what kind of professional perspective

916
00:57:30,380 --> 00:57:35,070
don't want to have have the luck of having people who are doing right now

917
00:57:35,070 --> 00:57:42,160
research in quite different context from green continuous so teaching and research to people who

918
00:57:42,160 --> 00:57:47,990
are have started their own company working for other companies and i think will benefit

919
00:57:47,990 --> 00:57:49,390
from seeing this

920
00:57:49,410 --> 00:57:52,270
a different perspectives

921
00:57:52,290 --> 00:57:57,130
of course this is just two questions have gotten to use so actually

922
00:57:57,130 --> 00:57:58,230
very much

923
00:57:58,250 --> 00:58:03,560
even more like if you have anything else to say that after all these experiences

924
00:58:03,560 --> 00:58:07,280
in research and teaching and applications of machine learning

925
00:58:07,370 --> 00:58:13,050
all experiences our personal experiences so restrictive as well as possible

926
00:58:15,040 --> 00:58:16,030
one school

927
00:58:16,060 --> 00:58:21,880
start maybe discussing along the lines of what is it that has said

928
00:58:21,920 --> 00:58:24,190
i don't want anything else

929
00:58:26,190 --> 00:58:29,000
link is not the same

930
00:58:29,030 --> 00:58:35,580
that during the puberty when you begin to use you really want to thank you

931
00:58:35,620 --> 00:58:39,310
have the ability of the musing so you go

932
00:58:39,330 --> 00:58:43,130
directly to the court because that's what he nutrition

933
00:58:44,220 --> 00:58:47,290
in part of machine tools

934
00:58:47,320 --> 00:58:48,530
but it's true that

935
00:58:50,010 --> 00:58:53,680
finishing on that you have to register

936
00:58:53,690 --> 00:58:54,760
go back

937
00:58:54,780 --> 00:58:56,490
and use you really

938
00:58:56,510 --> 00:59:02,320
and you see that the really important part is the motivation really blending what is

939
00:59:02,320 --> 00:59:04,700
the proper way to

940
00:59:04,710 --> 00:59:06,890
they want to use the internet

941
00:59:06,900 --> 00:59:09,420
graphical instead

942
00:59:10,520 --> 00:59:14,150
the same is more important to be considered

943
00:59:18,550 --> 00:59:27,760
calling the are telling me that some people from among us probably are more method

944
00:59:27,760 --> 00:59:30,960
oriented so we see all to use this method

945
00:59:30,970 --> 00:59:33,930
because that's what we know find applications for these

946
00:59:34,000 --> 00:59:37,400
you may be trying to return to tell me

947
00:59:37,420 --> 00:59:38,170
but l

948
00:59:38,190 --> 00:59:41,380
everybody wants to ensure that if

949
00:59:41,390 --> 00:59:41,980
about the

950
00:59:41,990 --> 00:59:46,490
the UM and this sort of thing OK so i'm not going to take the

951
00:59:46,490 --> 00:59:47,830
the two questions the two

952
00:59:48,590 --> 00:59:50,440
could just trying to say

953
00:59:51,570 --> 00:59:52,670
so i mean

954
00:59:52,680 --> 00:59:54,720
right and i'm just looking at the field

955
00:59:54,790 --> 01:00:00,460
mainly in france seeing how out there are they are not perspective with machine learning

956
01:00:01,140 --> 01:00:04,430
and they are highly dispersive b

957
01:00:04,430 --> 01:00:05,990
this last term

958
01:00:06,010 --> 01:00:14,880
france what she for postdocs doctorates and for the positions seem to be hot

959
01:00:14,890 --> 01:00:19,980
and i don't have a lot to some degree not censoring courage

960
01:00:20,030 --> 01:00:25,400
nevertheless what you get when you look into this was people were really looking for

961
01:00:25,400 --> 01:00:32,300
people where just capable of developing new theories machine i they want to use it

962
01:00:32,310 --> 01:00:36,760
in whatever they were working with they wanted to work with machine that meant they

963
01:00:37,260 --> 01:00:42,230
the road good machine probably open minded to do think

964
01:00:42,270 --> 01:00:43,400
the same time

965
01:00:43,490 --> 01:00:45,080
that's when got to

966
01:00:45,100 --> 01:00:46,780
got during system

967
01:00:46,800 --> 01:00:51,280
you can doesn't mean spread out all sorts of things which to everywhere but doesn't

968
01:00:51,280 --> 01:00:56,440
mean you keep your eyes open and say it's not just truth she but there

969
01:00:56,440 --> 01:01:00,710
are going editors as we were talking earlier about biology

970
01:01:00,770 --> 01:01:05,020
so that you talk about about surely no more that was going to give an

971
01:01:05,020 --> 01:01:07,920
example in this he

972
01:01:07,920 --> 01:01:09,930
called user model

973
01:01:10,070 --> 01:01:15,250
modeling is why the name indicates which is one the the model of used and

974
01:01:15,250 --> 01:01:17,890
typically then use that to be able to

975
01:01:17,890 --> 01:01:20,680
they will fall into different made sometimes

976
01:01:20,730 --> 01:01:23,700
but the frequency with which he land into

977
01:01:23,710 --> 01:01:28,280
one night compared to another may doesn't necessarily have anything to do with the probability

978
01:01:28,280 --> 01:01:30,030
mass inside this means

979
01:01:30,080 --> 01:01:33,480
so it doesn't guarantee that you will get the right answer might be that ninety

980
01:01:33,480 --> 01:01:36,550
percent of your chains will fall into this matrix of the big basin of attraction

981
01:01:36,550 --> 01:01:37,740
and algorithm

982
01:01:37,790 --> 01:01:40,590
but in fact most of the matter over here

983
01:01:42,570 --> 01:01:46,180
and the painful short answer is that there is no magic bullet to this algorithms

984
01:01:46,180 --> 01:01:48,010
are described very simple and they have very

985
01:01:48,400 --> 01:01:53,340
like technical requirements but it also means they can't guarantee against really nasty distributions i

986
01:01:53,340 --> 01:01:54,760
mean you construct

987
01:01:54,840 --> 01:01:59,380
a probability distribution with a really massive but type peak hidden somewhere

988
01:01:59,390 --> 01:02:00,820
viscerally then

989
01:02:00,830 --> 01:02:04,020
there's no way any reasonable algorithm will find it unless you put in knowledge about

990
01:02:04,020 --> 01:02:06,920
the topic and the

991
01:02:06,930 --> 01:02:12,350
you've got to sort of just always be slightly on your

992
01:02:12,360 --> 01:02:16,240
so i'm just posting here the argument about them that you can that's just for

993
01:02:16,240 --> 01:02:21,560
the record if you look into the slide slides already on the website

994
01:02:21,620 --> 01:02:25,770
here's a couple of figures from a paper by cosmic which is an example of

995
01:02:25,770 --> 01:02:30,560
like a typical thing people will do when they're running the chain so he's running

996
01:02:30,560 --> 01:02:35,770
a markov chain on high dimensional parameter space and here are some of the parameters

997
01:02:35,780 --> 01:02:41,550
and his chain and he's plotting various things about these parameters as that he he's

998
01:02:41,560 --> 01:02:44,310
visited his his change so

999
01:02:44,380 --> 01:02:48,710
this plot here is something called the autocovariance and the saying

1000
01:02:48,720 --> 01:02:49,650
what's the

1001
01:02:49,670 --> 01:02:55,970
covariance between a parameter in the chain and say that the same parameter six hundred

1002
01:02:55,970 --> 01:02:57,510
steps later so

1003
01:02:57,560 --> 01:03:02,310
is six hundred that long enough to forget where you come from is the autocovariance

1004
01:03:02,320 --> 01:03:03,270
very small

1005
01:03:03,280 --> 01:03:08,620
and he plotted this for a lot of different quantities because they're not experienced classes

1006
01:03:08,620 --> 01:03:12,780
certain plot about a scalar quantity so you could imagine the saying well i'm going

1007
01:03:12,780 --> 01:03:14,560
to do your current plot

1008
01:03:14,600 --> 01:03:18,810
for the probability you can just sort of plot what happened to the probability of

1009
01:03:18,810 --> 01:03:21,160
your training you would get one of these cats

1010
01:03:21,170 --> 01:03:25,730
the color very sensibly drew lots of these curves one for each parameter in this

1011
01:03:25,730 --> 01:03:29,800
model because it looked at a lot of this has very re-paint it looks as

1012
01:03:29,800 --> 01:03:34,270
though the chains mixing almost immediately because the autocovariance drops to close to zero almost

1013
01:03:34,270 --> 01:03:39,280
immediately for most of his parameters these identified one parameter where in fact

1014
01:03:40,250 --> 01:03:43,880
the last time is very long looks to take a very long time for the

1015
01:03:43,960 --> 01:03:45,950
chain to forget about

1016
01:03:45,960 --> 01:03:49,000
where it came from in terms of the parameter k and model

1017
01:03:51,350 --> 01:03:53,520
if you're going to do some sort of diagnostic

1018
01:03:53,530 --> 01:03:57,630
so you gnostics might tell you make some very quickly and everything is fine and

1019
01:03:57,650 --> 01:04:00,730
you're going to have to go away and do several diagnostics and there's no sort

1020
01:04:00,730 --> 01:04:03,660
of magic bullet for the different things you could try

1021
01:04:03,680 --> 01:04:10,170
what it identified this parameter the problem he's drawn plot of where that parameter actually

1022
01:04:11,470 --> 01:04:12,990
over time so

1023
01:04:13,000 --> 01:04:14,900
he initialize the somewhere

1024
01:04:14,910 --> 01:04:19,570
and after several thousand iterations it looks as though the statistics of what that parameter

1025
01:04:19,580 --> 01:04:23,970
doing change from this initial run this the classic sort of science then inherited you

1026
01:04:23,970 --> 01:04:24,900
might want to

1027
01:04:24,920 --> 01:04:30,050
toss out the results from these initial few thousand iterations

1028
01:04:30,100 --> 01:04:36,360
i've given here a couple of references and the reason i'm taking this these plots

1029
01:04:36,360 --> 01:04:40,830
from someone else's papers because i'm not an expert on this sort of error comparatively

1030
01:04:40,830 --> 01:04:44,950
diagnosing attend and i think it's safe to really

1031
01:04:44,960 --> 01:04:47,840
unless you're doing research in this area to just

1032
01:04:47,890 --> 01:04:54,070
following someone else's precise instructions because then you can blame them say rather than i

1033
01:04:54,070 --> 01:04:56,150
did this and i did this and i did this which would be very sensible

1034
01:04:56,210 --> 01:04:59,380
trial of things you like when you're writing the paper you cite someone and say

1035
01:04:59,410 --> 01:05:02,020
i did exactly the thing they to do that level

1036
01:05:02,040 --> 01:05:07,170
so a software package called code which provides the whole in diagnostic routines and you

1037
01:05:07,170 --> 01:05:09,900
have to use our which i don't but i just want my data out to

1038
01:05:09,920 --> 01:05:14,780
file what the minimal staffing catering read back in

1039
01:05:14,790 --> 01:05:19,560
and this paper here has been very sensible intelligent discussion about all of these issues

1040
01:05:19,560 --> 01:05:25,720
stemming running multiple runs and then so i highly recommend sort of looking at that

1041
01:05:25,720 --> 01:05:31,120
much more lengthy discussion if you're interested in doing things in practice

1042
01:05:31,130 --> 01:05:33,360
questions that were

1043
01:05:41,600 --> 01:05:46,060
the question is why am i calling this mixing and

1044
01:05:46,110 --> 01:05:50,310
one answer is i'm probably being incredibly useful the term because some people mean something

1045
01:05:50,310 --> 01:05:51,900
very specific so down

1046
01:05:51,910 --> 01:05:56,480
close attention to my precise vocabulary but

1047
01:05:56,500 --> 01:06:01,180
i mean loosely the idea is that you're just trying to

1048
01:06:01,190 --> 01:06:06,420
one around the whole space so are you stuck in one little river region all

1049
01:06:06,420 --> 01:06:09,200
has your chains to mixed with interesting regions

1050
01:06:09,200 --> 01:06:12,140
this is what error analysis is

1051
01:06:12,180 --> 01:06:18,560
you have a long complicated pipeline to combining many machine learning components so many of

1052
01:06:18,560 --> 01:06:20,470
these values and learning algorithms

1053
01:06:20,560 --> 01:06:22,330
and so

1054
01:06:22,870 --> 01:06:26,140
is often very useful to figure out how much of the error

1055
01:06:26,160 --> 01:06:30,580
can be attributed to each of these components

1056
01:06:33,240 --> 01:06:36,520
well one two in the air analysis procedure

1057
01:06:36,540 --> 01:06:40,600
is will repeatedly poking the ground truth for each component and see how we have

1058
01:06:40,600 --> 01:06:42,140
seen this

1059
01:06:42,160 --> 01:06:44,600
so what i mean by that is on

1060
01:06:44,620 --> 01:06:47,950
the figure on the bottom left to bottom right that city of accuracy of the

1061
01:06:47,950 --> 01:06:49,830
system is a five percent

1062
01:06:49,850 --> 01:06:51,560
the like to know

1063
01:06:51,580 --> 01:06:54,370
where my fifteen percent error comes from

1064
01:06:54,430 --> 01:06:59,740
and so what i do is of course my test set and actually code in

1065
01:06:59,740 --> 01:07:03,520
our service that actually implements my correct

1066
01:07:03,560 --> 01:07:08,370
on background vocals actually go going give it give my album what is the correct

1067
01:07:08,370 --> 01:07:10,470
background versus foreground

1068
01:07:10,520 --> 01:07:15,240
the idea that discovered that due to now that i'm giving that ground truth data

1069
01:07:15,240 --> 01:07:16,580
in the test set

1070
01:07:16,640 --> 01:07:19,970
that's accuracy increases the five point one percent

1071
01:07:19,980 --> 01:07:25,740
and now i'll go in and you get my out the ground truth face detection

1072
01:07:26,060 --> 01:07:30,750
what's going actually on my test set i just tell the our where the faces

1073
01:07:30,810 --> 01:07:32,080
of i do that

1074
01:07:32,130 --> 01:07:36,640
that same albums accuracy increases in ninety one percent and and so on and go

1075
01:07:36,640 --> 01:07:39,580
for each of these components

1076
01:07:39,580 --> 01:07:41,930
and just give it

1077
01:07:41,950 --> 01:07:47,810
on the ground truth label for each other components because to that no segmentation i

1078
01:07:47,810 --> 01:07:51,950
was trying to find figure one knows is going and tell us when knows is

1079
01:07:51,950 --> 01:07:56,180
that it doesn't feel and as i do this one component through the other end

1080
01:07:56,180 --> 01:08:00,600
up giving at the correct output label and i one hundred percent accuracy

1081
01:08:00,620 --> 01:08:04,520
and now you can look at this table serves color from the bottom to says

1082
01:08:04,520 --> 01:08:07,640
logistic regression i'm not going to the stable

1083
01:08:07,640 --> 01:08:10,120
and on c

1084
01:08:10,140 --> 01:08:13,890
you know how much getting the ground truth labels for each of these components could

1085
01:08:13,890 --> 01:08:15,890
help us to find the performance

1086
01:08:15,910 --> 01:08:22,240
in particular the stable you notice that when i added the face detection ground truth

1087
01:08:22,240 --> 01:08:25,330
performance jump from eighty five point one percent accuracy

1088
01:08:25,350 --> 01:08:27,180
o to ninety one percent accuracy

1089
01:08:27,200 --> 01:08:31,500
so this tells me that the only i can get better face detection

1090
01:08:31,500 --> 01:08:34,890
maybe i improves the accuracy by six percent

1091
01:08:34,910 --> 01:08:41,120
on the west in contrast when i say a in better

1092
01:08:41,120 --> 01:08:45,750
background removal my accuracy improves from eighty five to eighty five point one percent

1093
01:08:45,870 --> 01:08:50,770
so the sort of diagnostic also tells you that if goes to improve the system

1094
01:08:50,790 --> 01:08:54,600
it's a waste of your time to to try to improve the background subtracted because

1095
01:08:54,600 --> 01:08:59,080
even if you got ground truth this gives you most point one percent accuracy

1096
01:08:59,080 --> 01:09:03,350
well as you can do better face detection maybe much larger potential for things

1097
01:09:03,430 --> 01:09:05,620
so this sort of diagnostic gain

1098
01:09:07,010 --> 01:09:11,120
it is very useful because it goes into the system there are so many different

1099
01:09:11,120 --> 01:09:15,710
pieces can easily choose to spend the next few months choosing the right piece

1100
01:09:16,420 --> 01:09:22,750
hispanic or useful diagnostic tells you what the piece may actually work hard work

1101
01:09:25,680 --> 01:09:28,910
this sort of another type of analysis is sort of the opposite of what just

1102
01:09:28,910 --> 01:09:30,350
talked about

1103
01:09:30,390 --> 01:09:32,510
the analysis of twelve

1104
01:09:32,510 --> 01:09:36,710
try to explain the difference between the current performance in perfect performance

1105
01:09:36,720 --> 01:09:42,060
whereas this a plate of analysis tries to explain the difference between some baselines some

1106
01:09:42,060 --> 01:09:46,130
really bad performance and current performance

1107
01:09:46,130 --> 01:09:50,390
so for example let's suppose you build a very good anti-spam cos five but having

1108
01:09:50,390 --> 01:09:55,770
lots of people clever features images expression c and it features the spelling correction for

1109
01:09:55,770 --> 01:09:58,550
years in the host features that you had the features

1110
01:09:58,560 --> 01:10:01,800
in text parser features javascript parser features

1111
01:10:01,800 --> 01:10:03,850
he just that images and so on

1112
01:10:03,870 --> 01:10:08,170
so now let's europe eastern system you want to figure out how well how much

1113
01:10:08,170 --> 01:10:10,710
they need these components actually

1114
01:10:10,760 --> 01:10:14,870
on the mirror writing research papers including this was piece to make a big difference

1115
01:10:14,870 --> 01:10:17,960
can you have the document that can be justified

1116
01:10:17,980 --> 01:10:20,410
so in the data analysis

1117
01:10:20,410 --> 01:10:21,630
it's all we do

1118
01:10:21,660 --> 01:10:27,200
so in this example let's say that simple logistic regression without any clever improvements get

1119
01:10:27,200 --> 01:10:29,160
ninety four percent performance

1120
01:10:29,170 --> 01:10:32,470
if you want to figure out what accounts for your improvement from ninety four to

1121
01:10:32,470 --> 01:10:35,430
nineteen ninety nine percent performance

1122
01:10:35,460 --> 01:10:40,160
so in the data analysis on a surface having components one of the time was

1123
01:10:40,160 --> 01:10:43,810
remove components one of time to see how it operates

1124
01:10:43,840 --> 01:10:48,550
so start with the overall systems ninety nine percent accuracy

1125
01:10:48,550 --> 01:10:52,500
and they were removed spelling correction and see how much performance drops

1126
01:10:52,540 --> 01:10:57,080
the route to send host features and see how much performance drops and so on

1127
01:10:57,310 --> 01:11:03,420
so in this contrived example

1128
01:11:05,240 --> 01:11:10,420
you see that i guess the biggest drop occurs when you remove the text parser

1129
01:11:10,420 --> 01:11:15,670
features and so you can then make a credible case that the text parser features

1130
01:11:15,720 --> 01:11:18,180
what really makes the biggest difference

1131
01:11:18,200 --> 01:11:19,660
and also tell

1132
01:11:19,670 --> 01:11:21,810
for instance that all

1133
01:11:21,840 --> 01:11:26,840
removing the centre hosted shows on this line right before dropping ninety nine point nine

1134
01:11:26,840 --> 01:11:30,790
eight point two nine point nine percent so there's also means that in case you

1135
01:11:30,790 --> 01:11:34,240
want to get rid of the center host features to speed up computational something that

1136
01:11:34,240 --> 01:11:36,790
would be a good candidate for elimination

1137
01:11:40,680 --> 01:11:42,430
which is the same

1138
01:11:42,850 --> 01:11:46,540
i just want to show you what the was removed things the answer is no

1139
01:11:46,540 --> 01:11:53,230
there's no guarantee against the results of soap actors sometimes is fairly natural ordering of

1140
01:11:53,230 --> 01:11:58,710
the both ties analysis easier houses data analysis sentences a very natural ordering which happens

1141
01:11:59,010 --> 01:12:04,420
things on sometimes there's and quite often you to choose one ordering just go through

1142
01:12:04,420 --> 01:12:05,160
it all

1143
01:12:05,170 --> 01:12:09,470
on and they think of these analyses as of all those costs still few free

1144
01:12:09,700 --> 01:12:12,020
then on this one

1145
01:12:12,090 --> 01:12:13,590
one thing this

1146
01:12:13,590 --> 01:12:18,120
so there's this memory function you want talk about it saves the model

1147
01:12:18,210 --> 01:12:22,010
and change the case where and built a new model now these two cases where

1148
01:12:22,010 --> 01:12:25,060
it's and

1149
01:12:25,090 --> 01:12:28,660
holds on to the original model so

1150
01:12:28,680 --> 01:12:31,340
as the case has been

1151
01:12:31,350 --> 01:12:36,570
misclassified over and over its relative weight starts to grow and the ones that gets

1152
01:12:36,570 --> 01:12:39,640
right start to fade now

1153
01:12:39,660 --> 01:12:44,880
some people like this to to marriage you know the fact that you bring all

1154
01:12:45,840 --> 01:12:51,040
but that is taken for granted and the socks that are still on the floor

1155
01:12:51,060 --> 01:12:52,180
you know

1156
01:12:52,220 --> 01:12:55,090
one the month starts mid importance

1157
01:12:55,130 --> 01:13:00,750
you can imagine how hysterical some of these models again after whatsoever

1158
01:13:00,800 --> 01:13:05,000
the reason i read about this thing i thought using the latest model the one

1159
01:13:05,000 --> 01:13:05,960
that has the

1160
01:13:05,960 --> 01:13:10,890
you know that can actually paid the least way to the most hysterical model

1161
01:13:10,910 --> 01:13:14,920
and the most weight to the original model and you have a weighted sum

1162
01:13:14,970 --> 01:13:21,330
of these of these models the goaoc so it's very interesting clever procedure and it

1163
01:13:21,330 --> 01:13:26,200
took awhile to generalise to bring into the world of the models but eventually

1164
01:13:26,210 --> 01:13:27,800
as you add more

1165
01:13:28,770 --> 01:13:32,540
points that are in dispute are all along the boundaries

1166
01:13:33,840 --> 01:13:38,390
you end up with the decision boundary is much closer than your original base learner

1167
01:13:38,390 --> 01:13:41,060
which had square corners

1168
01:13:41,080 --> 01:13:42,800
would have allowed to do

1169
01:13:42,810 --> 01:13:45,920
yes thank you so i want to give the bit

1170
01:13:45,960 --> 01:13:50,770
solution because it was really helpful for me in understanding so

1171
01:13:50,830 --> 01:13:55,120
case which are one of the sources of variety

1172
01:13:55,600 --> 01:14:00,470
all right so now look if we focus on the tree world

1173
01:14:00,540 --> 01:14:06,710
a little bit again the generalized ensembles work with any kind of learners with again

1174
01:14:06,720 --> 01:14:09,460
trees are all relatively

1175
01:14:09,470 --> 01:14:11,840
easy to understand so we focus on

1176
01:14:11,840 --> 01:14:13,510
and we build

1177
01:14:13,510 --> 01:14:14,880
rule ensembles

1178
01:14:15,660 --> 01:14:18,790
we could we could say our model is a linear function

1179
01:14:18,810 --> 01:14:20,120
and our

1180
01:14:20,160 --> 01:14:23,300
basis functions are these served base learners

1181
01:14:23,330 --> 01:14:26,380
which can capture nonlinear so

1182
01:14:26,390 --> 01:14:31,200
we generate the models are the basis functions and then post fit to the data

1183
01:14:31,770 --> 01:14:38,730
with regularized regression using favorite thing now as we are a representation we lose we

1184
01:14:38,730 --> 01:14:39,730
lose our

1185
01:14:39,800 --> 01:14:44,180
interpretability but again you can take any method no matter how black box and you

1186
01:14:44,180 --> 01:14:47,850
can do post processing and iterative

1187
01:14:47,870 --> 01:14:52,700
tweaking the see which variables are getting the most important so

1188
01:14:52,710 --> 01:14:55,160
it's not interpretable

1189
01:14:55,170 --> 01:14:58,010
if you show a picture of but is interpretable

1190
01:14:58,010 --> 01:15:03,130
if you work with it and see which which variables are most influential smile

1191
01:15:03,140 --> 01:15:07,630
so i think of the tree has cut a tree is a collection of rules

1192
01:15:07,720 --> 01:15:12,200
the indicator functions and these constants better

1193
01:15:12,200 --> 01:15:14,220
and so you have these different regions

1194
01:15:14,250 --> 01:15:16,810
that are themselves the

1195
01:15:16,840 --> 01:15:19,580
the product to the indicator function so here

1196
01:15:19,590 --> 01:15:23,880
if x one is greater than twenty two years x two scored twenty seven inside

1197
01:15:23,880 --> 01:15:28,790
this boundary region region one and you have all these different regions are defined by

1198
01:15:28,790 --> 01:15:31,720
the sort of building block rules

1199
01:15:31,750 --> 01:15:36,770
so you could decompose the tree

1200
01:15:36,800 --> 01:15:37,890
into these

1201
01:15:37,910 --> 01:15:41,380
these rules and have a larger set of base learners we've been talking about based

1202
01:15:41,380 --> 01:15:45,260
on being a tree but you can break the tree up into its components and

1203
01:15:45,260 --> 01:15:48,430
let those be part of your alphabet for

1204
01:15:48,450 --> 01:15:49,520
two together

1205
01:15:49,550 --> 01:15:52,300
the if you use those

1206
01:15:52,310 --> 01:15:57,370
words in your model that could be more interpretable

1207
01:15:58,130 --> 01:16:01,630
still ways people still piecewise constant model

1208
01:16:01,640 --> 01:16:07,640
not linear targets can be a problem again with trees with piecewise constant but there's

1209
01:16:07,640 --> 01:16:12,550
nothing to hold back from throwing in more base learners that have those properties the

1210
01:16:12,550 --> 01:16:17,710
linear trend properties so you can think of combined

1211
01:16:17,710 --> 01:16:19,730
o thing where inputs are linear

1212
01:16:19,750 --> 01:16:26,540
or our region and you have different sets weight if it is

1213
01:16:26,560 --> 01:16:31,300
so use the same

1214
01:16:31,300 --> 01:16:33,540
use approximate optimisation

1215
01:16:33,550 --> 01:16:35,300
approach to software the

1216
01:16:35,310 --> 01:16:37,540
split definitions for rule

1217
01:16:39,420 --> 01:16:42,140
is one rule for each terminal node

1218
01:16:44,420 --> 01:16:47,000
if you if you have a shallow tree

1219
01:16:47,020 --> 01:16:52,430
you could also use some of the intermediate nodes regions e to to increase the

1220
01:16:52,430 --> 01:16:55,680
variety of things you have to work with

1221
01:17:00,350 --> 01:17:03,620
so now you have two sets of parameters year

1222
01:17:03,620 --> 01:17:06,580
in linear grammars in your tree parameters

1223
01:17:06,630 --> 01:17:11,450
and you might penalize both again as well as the question point earlier we could

1224
01:17:12,990 --> 01:17:16,600
pay attention to the regularizer normalising the data

1225
01:17:16,620 --> 01:17:21,230
such away so the absolute value of the parameter has some meaning

1226
01:17:21,260 --> 01:17:25,370
because japan was according to the end of a little trivial

1227
01:17:25,370 --> 01:17:26,620
enjoy it

1228
01:17:26,640 --> 01:17:28,880
enjoy it

1229
01:17:28,890 --> 01:17:32,020
so now what i want you to do is to look at

1230
01:17:33,100 --> 01:17:34,390
at the table

1231
01:17:34,400 --> 01:17:37,870
this table is going to show that this is not such as smart

1232
01:17:37,920 --> 01:17:42,280
tagline to work with again i will not expect you to commit the stable to

1233
01:17:42,280 --> 01:17:46,670
memory i would never ask you to tell me what the solubility of something is

1234
01:17:46,670 --> 01:17:49,410
by hard i would instead turn around and say

1235
01:17:49,860 --> 01:17:51,100
for example

1236
01:17:53,790 --> 01:17:56,060
copper bromide

1237
01:17:57,300 --> 01:18:01,790
it's aqueous solutions and then let's take it from there i tell you what the

1238
01:18:01,820 --> 01:18:03,700
the fact is but what i want you

1239
01:18:03,750 --> 01:18:07,290
notice first of all is that the table is broken into two

1240
01:18:07,300 --> 01:18:13,090
regimes soluble ionic compounds and insoluble ionic compounds

1241
01:18:13,150 --> 01:18:18,230
soluble iron compounds insoluble ionic compounds so it's not so simple

1242
01:18:18,270 --> 01:18:20,460
it's not so simple there's something else

1243
01:18:20,470 --> 01:18:22,000
going on here

1244
01:18:22,020 --> 01:18:28,360
and let's take a look there's one they for example the the chlorides chlorides are

1245
01:18:28,360 --> 01:18:34,890
generally soluble and the oxides are generally not soluble so what do we have to

1246
01:18:34,890 --> 01:18:41,030
think about well it's the competition really it's a competition we look at sodium chloride

1247
01:18:41,030 --> 01:18:43,870
it exists as a solid

1248
01:18:45,040 --> 01:18:47,860
and now we have to compare

1249
01:18:47,880 --> 01:18:52,090
whether it wants to go into aqueous solution

1250
01:18:52,100 --> 01:18:55,740
whether it wants to go into aqueous solution

1251
01:18:55,740 --> 01:18:59,860
and what we see is that the answer here is yes

1252
01:18:59,920 --> 01:19:02,250
whereas the answer for

1253
01:19:02,250 --> 01:19:05,050
magnesium oxide which is also

1254
01:19:05,060 --> 01:19:07,260
an ionic crystal

1255
01:19:08,250 --> 01:19:12,560
it does not form

1256
01:19:12,640 --> 01:19:16,310
aqueous solutions so what's going on here well

1257
01:19:16,320 --> 01:19:22,800
what's the major difference here sodium chloride exists sodium cations and chloride anions

1258
01:19:22,810 --> 01:19:26,810
whereas magnesium is diving so we magnesium

1259
01:19:26,860 --> 01:19:33,890
cations oxide anions and in the crystal lattice we have a much much higher crystal

1260
01:19:33,890 --> 01:19:38,350
lattice energy for magnesium oxide if you just go through the madelon

1261
01:19:38,360 --> 01:19:40,650
derivation you'll have doubled

1262
01:19:40,660 --> 01:19:43,620
double so it's in essence four times

1263
01:19:43,630 --> 01:19:47,280
four times in chernoff the melting point of sodium chloride is about

1264
01:19:47,380 --> 01:19:51,800
eight hundred degrees melting point whereas the melting point of magnesium oxide is about twenty

1265
01:19:51,810 --> 01:19:52,990
eight hundred degrees

1266
01:19:53,050 --> 01:19:58,370
what happens when we try to make this dissolve in a solution is something that

1267
01:19:58,370 --> 01:20:02,010
looks like this as a cartoon showing water

1268
01:20:02,010 --> 01:20:06,940
dissolving sodium chloride sodium chloride is to get oriented the green

1269
01:20:07,360 --> 01:20:12,980
spheres represent the chloride ions and the blue spheres represents sodium ions and just

1270
01:20:13,020 --> 01:20:15,140
he things could you can see up here

1271
01:20:15,200 --> 01:20:19,360
the one two three four five the face of an FCC

1272
01:20:19,410 --> 01:20:24,150
gravity lattice but we've got two atoms per lattice points are basis et cetera et

1273
01:20:24,150 --> 01:20:29,500
cetera and here's the water molecule which looks sort of like mickey mouse here with

1274
01:20:29,540 --> 01:20:31,300
the the red showing the

1275
01:20:31,350 --> 01:20:35,700
oxygen and the white showing the hydrogen so the oxygen

1276
01:20:35,720 --> 01:20:39,710
and is the negative and of the dipole with the cartoon showing is that the

1277
01:20:40,010 --> 01:20:46,100
negative and of the dipole is presenting itself to the cations

1278
01:20:46,120 --> 01:20:47,600
and if the

1279
01:20:47,620 --> 01:20:49,440
attractive force

1280
01:20:49,440 --> 01:20:53,160
between the soluble in the solvent exceeds

1281
01:20:53,220 --> 01:20:57,630
the binding energy of the crystal binding energy will have solution

1282
01:20:57,660 --> 01:20:59,810
and that's borne out here

1283
01:20:59,810 --> 01:21:01,690
well you see that we have

1284
01:21:01,740 --> 01:21:08,560
the difference in coulombic force of attraction that makes it accessible for sodium chloride

1285
01:21:08,590 --> 01:21:11,940
four water to dissolve it but inaccessible for

1286
01:21:11,960 --> 01:21:16,020
magnesium oxide because the forces are too strong and the see the complement here that

1287
01:21:16,020 --> 01:21:18,400
the positive and of the

1288
01:21:18,420 --> 01:21:22,810
dipole is presenting itself to the chloride and i

1289
01:21:22,880 --> 01:21:25,050
i'm always amused when people draw

1290
01:21:25,050 --> 01:21:31,600
ions colored because i know they try to color coding but i think about it

1291
01:21:31,610 --> 01:21:38,760
if you could isolate the chloride ion what color do you think it would be

1292
01:21:38,790 --> 01:21:42,220
there's an answer to that question

1293
01:21:42,230 --> 01:21:48,610
what's its electronic structure he always was back what what's its electronic structure it's isoelectronic

1294
01:21:48,610 --> 01:21:50,860
with the

1295
01:21:52,610 --> 01:21:54,850
and what do you think the color of oregon is what do you think the

1296
01:21:54,850 --> 01:21:59,730
color very noble gases so actually these things should all be clear colorless i just

1297
01:21:59,730 --> 01:22:03,360
wanted to point that out i know the color-coded for your amusement but it is

1298
01:22:04,050 --> 01:22:09,060
i was staring at this does green i and asking myself

1299
01:22:09,120 --> 01:22:11,940
should i be green no no they should

1300
01:22:11,980 --> 01:22:15,220
OK so now let's talk about measures of solubility

1301
01:22:15,270 --> 01:22:19,550
measures of solubility it gets put some numbers on this

1302
01:22:20,550 --> 01:22:22,840
we can express solubility

1303
01:22:22,890 --> 01:22:26,980
in the following units

1304
01:22:27,020 --> 01:22:30,050
there are a variety of units but the only ones that we really

1305
01:22:30,100 --> 01:22:31,540
we need to worry about

1306
01:22:31,540 --> 01:22:36,720
o is the molar quantity which represents models of solitude

1307
01:22:36,730 --> 01:22:39,540
models of solitude

1308
01:22:39,590 --> 01:22:40,630
divided by

1309
01:22:42,590 --> 01:22:44,160
of solution

1310
01:22:44,170 --> 01:22:50,340
leaders of solution and member the solution is the sum of all solurions

1311
01:22:50,350 --> 01:22:53,310
also you plus the solver

1312
01:22:53,310 --> 01:22:56,470
also used the solvent constitute the solution

1313
01:22:56,470 --> 01:22:59,370
and so moles per litre when we say

1314
01:22:59,400 --> 01:23:05,030
sodium chloride dissolves in so many moles per litre we can abbreviate that by

1315
01:23:05,030 --> 01:23:10,230
upper case and upper case and which we term mole

1316
01:23:10,240 --> 01:23:13,510
more so if i have a one molar solution

1317
01:23:13,530 --> 01:23:17,130
of sodium chloride in water one molar

1318
01:23:17,190 --> 01:23:19,770
is equivalent to say one mole

1319
01:23:19,810 --> 01:23:22,030
of NAACL

1320
01:23:22,050 --> 01:23:24,150
in one litre

1321
01:23:25,420 --> 01:23:28,040
of the solution to us

1322
01:23:28,060 --> 01:23:31,820
was about to write water it's not this one leader of

1323
01:23:31,880 --> 01:23:33,150
the solution

1324
01:23:36,700 --> 01:23:38,500
here i think you can see that

1325
01:23:38,550 --> 01:23:41,740
there it's not an all-or-nothing situation

1326
01:23:41,790 --> 01:23:43,200
in other words

1327
01:23:43,230 --> 01:23:46,840
certain substances are insoluble for reasons we've

1328
01:23:46,860 --> 01:23:50,810
scene but other substances are soluble up to a point

1329
01:23:50,840 --> 01:23:55,570
you know from experience you can add sugar to water and it dissolves very rapidly

1330
01:23:55,570 --> 01:24:01,090
up to a certain value beyond which you can add the sugar storage shake it

1331
01:24:01,100 --> 01:24:06,900
do whatever you want the sugar will not dissolve beyond certain limits which we call

1332
01:24:06,900 --> 01:24:09,820
the solubility limit

1333
01:24:09,840 --> 01:24:13,420
the solubility limit

1334
01:24:13,500 --> 01:24:18,590
or in aqueous systems they simply call it the solubility

1335
01:24:18,650 --> 01:24:20,440
the solubility

1336
01:24:21,290 --> 01:24:23,330
at this limit we've reached

1337
01:24:23,400 --> 01:24:27,500
condition term saturation

1338
01:24:27,500 --> 01:24:32,610
we say the solution is saturated with respect to the

1339
01:24:34,270 --> 01:24:36,920
to the solitude and can take no more

1340
01:24:36,950 --> 01:24:41,940
and there are two measures FOR saturation solubility one is

1341
01:24:41,960 --> 01:24:47,710
just denoted c star or some people use c seven as which means the

1342
01:24:50,080 --> 01:24:53,250
concentration of solitude

1343
01:24:53,380 --> 01:24:56,200
at saturation

1344
01:24:56,200 --> 01:24:59,730
you as for me to leave eric was yours

1345
01:24:59,740 --> 01:25:01,560
lecture course

1346
01:25:01,590 --> 01:25:04,810
with the cancellation that we can see it on

1347
01:25:04,820 --> 01:25:08,190
the tape

1348
01:25:08,200 --> 01:25:13,090
two two interesting well several things very interesting for me one is i knew david

1349
01:25:13,090 --> 01:25:15,090
has to this in graduate school

1350
01:25:15,110 --> 01:25:19,060
and no idea that his article

1351
01:25:19,450 --> 01:25:21,020
lead to

1352
01:25:21,060 --> 01:25:23,130
eric was or is interested in

1353
01:25:23,180 --> 01:25:24,540
in education

1354
01:25:25,570 --> 01:25:29,560
because we were unable to stay for the punchline the remedy

1355
01:25:29,560 --> 01:25:32,120
what do what's

1356
01:25:32,780 --> 01:25:35,660
what's the right thing to be doing

1357
01:25:35,690 --> 01:25:38,380
so i'll watch the tape to find that out

1358
01:25:38,450 --> 01:25:40,510
the second thing i wanted to say

1359
01:25:40,520 --> 01:25:43,590
there was one point there which i didn't agree with

1360
01:25:43,590 --> 01:25:46,950
and i don't know what you feel it he got to the top of the

1361
01:25:46,950 --> 01:25:50,100
pyramid the one in two hundred sixty

1362
01:25:51,770 --> 01:25:53,060
of the

1363
01:25:53,150 --> 01:25:57,520
starting students who went on to do a phd in the jewish star around them

1364
01:25:57,520 --> 01:25:58,780
and said i'm not

1365
01:25:58,810 --> 01:26:00,950
worried about them

1366
01:26:00,980 --> 01:26:04,150
i worry about them about that part too

1367
01:26:04,210 --> 01:26:08,560
i don't think i think from the first of all

1368
01:26:09,840 --> 01:26:14,510
the ones who are the next generation who are carrying the torch

1369
01:26:14,520 --> 01:26:16,200
so there's something

1370
01:26:16,210 --> 01:26:19,270
pretty important about them even if numerically

1371
01:26:19,290 --> 01:26:21,150
it's a small number

1372
01:26:21,160 --> 01:26:23,460
and i don't think

1373
01:26:23,490 --> 01:26:28,680
i mean i used to think that maybe people became mathematicians or physicists

1374
01:26:28,740 --> 01:26:30,850
so have you couldn't stop on

1375
01:26:30,900 --> 01:26:34,740
right i mean they they destroyed born in that direction they had

1376
01:26:34,740 --> 01:26:37,240
they had to matter gene or something

1377
01:26:37,260 --> 01:26:42,570
and you didn't have to take care of them what happened but i don't believe

1378
01:26:42,570 --> 01:26:44,880
that any more i think that

1379
01:26:44,930 --> 01:26:48,200
you know as write me right now we're

1380
01:26:49,910 --> 01:26:56,570
feel about the same if we measure success by any standard measurements then we should

1381
01:26:56,570 --> 01:26:58,020
probably all be

1382
01:26:58,040 --> 01:27:01,240
working for morgan stanley or something

1383
01:27:01,800 --> 01:27:04,150
now i don't think

1384
01:27:04,180 --> 01:27:06,240
any of us is going there tomorrow

1385
01:27:07,870 --> 01:27:14,210
and that's sort of a joke that about the money part of the rewards but

1386
01:27:14,240 --> 01:27:16,580
the the reward of

1387
01:27:16,650 --> 01:27:20,210
there are so many intangible but highly important rewards

1388
01:27:20,240 --> 01:27:22,680
that if the system doesn't offer them

1389
01:27:22,680 --> 01:27:24,400
people make a different choice

1390
01:27:25,590 --> 01:27:28,770
i mean and and some of those rewards are internal

1391
01:27:28,780 --> 01:27:32,330
i mean it's a great feeling to actually you know can use your mind and

1392
01:27:32,330 --> 01:27:34,520
understand them but

1393
01:27:34,530 --> 01:27:37,960
you need encouragement to so i i wouldn't

1394
01:27:37,990 --> 01:27:42,940
assume that the that the special ones

1395
01:27:42,960 --> 01:27:43,960
could be

1396
01:27:45,000 --> 01:27:50,020
and i'm sure i mean he's obviously a fantastic features of the special ones are

1397
01:27:50,020 --> 01:27:55,680
learning like mad OK now what do i do with the kalman filter

1398
01:27:59,560 --> 01:28:04,060
again tried to establish what's the framework

1399
01:28:04,060 --> 01:28:07,220
one of the inputs for the outputs

1400
01:28:07,270 --> 01:28:11,150
so what will the formulas be formulas for

1401
01:28:11,190 --> 01:28:17,810
and then maybe a little discussion of those matrix formulas

1402
01:28:17,830 --> 01:28:21,210
but let me get the framework straight for

1403
01:28:24,490 --> 01:28:30,430
the the point is as i was in the list of topics

1404
01:28:30,470 --> 01:28:32,460
the point is that

1405
01:28:32,500 --> 01:28:36,390
i'm going to end up with a very big system can i write it script

1406
01:28:36,420 --> 01:28:38,860
a script you

1407
01:28:38,940 --> 01:28:43,620
equals script b or something

1408
01:28:43,670 --> 01:28:44,590
and that

1409
01:28:44,650 --> 01:28:49,270
is a collection of equations

1410
01:28:49,310 --> 01:28:51,520
coming at different times

1411
01:28:51,560 --> 01:28:54,240
four different for for changing u

1412
01:28:54,250 --> 01:28:57,150
and with measurements b so that me

1413
01:28:57,270 --> 01:29:00,060
look this is discrete time of course

1414
01:29:00,080 --> 01:29:05,060
i could i could be writing differential equations things are clearer in the the discrete

1415
01:29:05,900 --> 01:29:06,990
so let me

1416
01:29:07,030 --> 01:29:09,060
so i have

1417
01:29:09,060 --> 01:29:11,490
say i measure some

1418
01:29:14,200 --> 01:29:16,380
now that's that's a vector

1419
01:29:16,390 --> 01:29:22,670
so as i started with some standard equation in a zero you zero equals c

1420
01:29:23,550 --> 01:29:26,180
static at time zero

1421
01:29:26,190 --> 01:29:30,490
i have a standard measurement equation five OK

1422
01:29:32,610 --> 01:29:34,750
goes on

1423
01:29:34,800 --> 01:29:36,980
the system changes

1424
01:29:36,990 --> 01:29:41,740
but it changes by no longer out of this is of course is garnering much

1425
01:29:45,250 --> 01:29:48,140
and that there were going to assume has some

1426
01:29:53,120 --> 01:29:55,130
very covariance matrix

1427
01:29:55,200 --> 01:29:56,850
thank you

1428
01:29:56,860 --> 01:30:01,430
o thing is zero maybe i need is a signature of e

1429
01:30:01,430 --> 01:30:02,660
OK fine

1430
01:30:02,740 --> 01:30:05,200
now that we know how to deal with

1431
01:30:05,220 --> 01:30:10,330
now the next step is time moves forward sean x and that this was the

1432
01:30:10,330 --> 01:30:12,040
measurement equation

1433
01:30:12,070 --> 01:30:16,300
the next equation will be a state equation this you know what is the state

1434
01:30:16,300 --> 01:30:17,670
at time zero

1435
01:30:17,680 --> 01:30:21,690
and the state at time one is connected to the state at time zero

1436
01:30:21,700 --> 01:30:23,780
by some linear

1437
01:30:23,790 --> 01:30:25,580
ah equation

1438
01:30:25,600 --> 01:30:29,370
so you want to call some after writing

1439
01:30:29,470 --> 01:30:31,620
sorry x to be you

1440
01:30:31,680 --> 01:30:32,750
and i

1441
01:30:32,810 --> 01:30:36,890
i'm going to change that well referenced second son like look up to see what

1442
01:30:37,640 --> 01:30:39,430
letter and

1443
01:30:39,430 --> 01:30:40,440
so this

1444
01:30:42,110 --> 01:30:44,260
the change of state the dynamics

1445
01:30:44,280 --> 01:30:46,870
between times are on time one

1446
01:30:48,050 --> 01:30:50,820
of course it's not perfect either

1447
01:30:50,830 --> 01:30:54,140
so there's error in the use epsilon

1448
01:30:54,170 --> 01:30:56,180
for the state here and now

1449
01:30:56,190 --> 01:31:01,080
use the index one where or another person might use zero it is not critical

1450
01:31:01,100 --> 01:31:03,370
OK then we're in state one

1451
01:31:03,430 --> 01:31:05,980
and i take a measurement state one

1452
01:31:05,990 --> 01:31:07,610
so a one

1453
01:31:09,420 --> 01:31:10,380
in some

1454
01:31:10,430 --> 01:31:16,310
measuring b one minus some error e one f long for the errors in the

1455
01:31:16,320 --> 01:31:19,170
state equation he's for the error in the

1456
01:31:19,180 --> 01:31:20,780
and then sure enough

1457
01:31:20,800 --> 01:31:21,720
u two

1458
01:31:21,730 --> 01:31:22,540
it is

1459
01:31:22,560 --> 01:31:24,350
you want

1460
01:31:25,620 --> 01:31:27,870
and of course and after one

1461
01:31:28,870 --> 01:31:31,120
and then let's say we do and measurement

1462
01:31:31,130 --> 01:31:33,430
time to start stop there

1463
01:31:33,440 --> 01:31:35,850
eight two u two with b two

1464
01:31:35,860 --> 01:31:38,110
i minus e

1465
01:31:39,870 --> 01:31:41,670
and one

1466
01:31:41,690 --> 01:31:43,010
further touch

1467
01:31:43,030 --> 01:31:45,990
well you see i didn't insist on the same

1468
01:31:46,030 --> 01:31:49,940
a matrix that every step of course i don't really have to insist on the

1469
01:31:49,940 --> 01:31:51,130
same as

1470
01:31:51,140 --> 01:31:53,640
matrix at everything

1471
01:31:53,680 --> 01:31:55,560
so this is the

1472
01:31:55,610 --> 01:31:58,920
this is the frame and this is the least squares problem this is the big

1473
01:31:58,920 --> 01:32:02,640
problem you will be

1474
01:32:02,690 --> 01:32:05,250
i can if i bring you all the

1475
01:32:05,280 --> 01:32:07,350
the use until the

1476
01:32:07,360 --> 01:32:10,850
one side of the equation and the bees and the errors on the other side

1477
01:32:10,850 --> 01:32:15,010
of the equation then i have the big system that's to be so let's do

1478
01:32:15,780 --> 01:32:17,680
so what is swept a

1479
01:32:17,690 --> 01:32:20,230
let me just copy down the

1480
01:32:20,240 --> 01:32:24,170
what have i got in this matrix model and what's my unknown you would see

1481
01:32:24,170 --> 01:32:25,560
if i can get it right

1482
01:32:25,640 --> 01:32:29,130
i i i ended up with five

1483
01:32:29,180 --> 01:32:31,560
walker equations everybody knows that

1484
01:32:31,580 --> 01:32:33,570
everybody is clear that these

1485
01:32:33,620 --> 01:32:35,370
use typically

1486
01:32:35,380 --> 01:32:39,310
the use will be the same size but probably we don't even have to require

1487
01:32:40,110 --> 01:32:43,180
but there will be vectors not scalars

1488
01:32:43,180 --> 01:32:46,180
known under this name

1489
01:32:49,660 --> 01:32:54,940
any questions regarding this simulated annealing procedure

1490
01:33:21,650 --> 01:33:28,420
so this is the question of randomness when actually went stochasticity comes into play in

1491
01:33:28,420 --> 01:33:31,950
this and with so it's actually two points

1492
01:33:32,470 --> 01:33:33,880
one thing is

1493
01:33:34,930 --> 01:33:36,210
you see

1494
01:33:36,220 --> 01:33:38,320
generating new solution

1495
01:33:38,330 --> 01:33:44,120
i didn't discuss this in detail you really generate

1496
01:33:44,180 --> 01:33:48,680
a solution that is somewhere in the neighborhood of the current solution but of course

1497
01:33:48,680 --> 01:33:54,660
there's many options to do that so we usually involves here this is one point

1498
01:33:54,730 --> 01:34:01,420
and the second point is comes just this probability of accepting the worst solution so

1499
01:34:01,420 --> 01:34:07,680
this is used as the probability were solution is sometimes accepted sometimes not

1500
01:34:13,120 --> 01:34:15,930
i would like to

1501
01:34:20,680 --> 01:34:27,040
but if you look at it from a global perspective the idea is to minimize

1502
01:34:27,040 --> 01:34:29,900
it but sometimes you need to

1503
01:34:38,950 --> 01:34:40,940
so o

1504
01:34:45,370 --> 01:34:48,110
yes is this is a question of

1505
01:34:48,130 --> 01:34:49,580
for example

1506
01:34:49,630 --> 01:34:53,450
how efficient is random search was as techniques like these

1507
01:34:53,470 --> 01:34:59,330
well empirically it can be shown that usually it's it's

1508
01:34:59,360 --> 01:35:04,580
random search is much worse but in those i don't know if

1509
01:35:05,420 --> 01:35:06,390
part of the

1510
01:35:06,410 --> 01:35:09,100
so called no free lunch theorem

1511
01:35:10,070 --> 01:35:14,740
just so you know very much into details but it was a question of this

1512
01:35:14,750 --> 01:35:16,110
this regards

1513
01:35:16,160 --> 01:35:20,430
the efficiency of the search and optimisation techniques

1514
01:35:20,450 --> 01:35:22,540
this says that on average

1515
01:35:22,560 --> 01:35:25,870
over all problems this is again

1516
01:35:25,880 --> 01:35:27,400
there's no

1517
01:35:27,410 --> 01:35:30,850
the the best the most successful technique so

1518
01:35:30,860 --> 01:35:38,360
this is in theory but in practice just not to be too much disappointing disappointed

1519
01:35:38,420 --> 01:35:43,230
you're always in certain particular problem and then it matters

1520
01:35:43,250 --> 01:35:45,840
check the methodologies which one is

1521
01:35:45,850 --> 01:35:50,130
but if we this is just the average based on average you can

1522
01:35:50,140 --> 01:35:53,410
you can send

1523
01:35:57,100 --> 01:35:58,560
now about the

1524
01:35:58,610 --> 01:36:03,770
second topic which is evolutionary algorithms

1525
01:36:03,820 --> 01:36:05,560
evolutionary algorithms

1526
01:36:05,570 --> 01:36:10,260
were inspired by biological evolution

1527
01:36:10,320 --> 01:36:11,480
this means

1528
01:36:13,410 --> 01:36:19,180
evolved in nature you know there's there's the darwinian theory that is the principle of

1529
01:36:19,600 --> 01:36:22,640
heredity and so on so the organisms

1530
01:36:22,700 --> 01:36:30,890
compete in a certain environment that is survival of the fittest principle that is known

1531
01:36:30,890 --> 01:36:33,410
from nature so

1532
01:36:33,410 --> 01:36:42,170
the fittest individuals survive they reproduce so their properties are then inherited by their offspring

1533
01:36:42,170 --> 01:36:46,650
and propagated through populations in next generations

1534
01:36:46,670 --> 01:36:51,670
so this is exactly the idea that is used to be the evolutionary algorithms

1535
01:36:51,690 --> 01:37:00,970
now we can characterize these algorithms as simplified models of these natural biological evolution

1536
01:37:00,990 --> 01:37:03,920
that implement these principles

1537
01:37:05,330 --> 01:37:09,240
it many of these techniques were actually first used

1538
01:37:09,260 --> 01:37:17,460
to simulate or to assist biologists in their experimental studies but very soon

1539
01:37:17,480 --> 01:37:21,910
the computer scientists or programmers this coward

1540
01:37:21,920 --> 01:37:24,920
that this methodology is ready

1541
01:37:24,940 --> 01:37:27,170
can be used in a very pragmatic way

1542
01:37:27,180 --> 01:37:29,430
as an efficient and effective search

1543
01:37:29,480 --> 01:37:31,790
and optimisation technique

1544
01:37:31,810 --> 01:37:38,110
so if we now this talk about evolutionary algorithms we must know that these are

1545
01:37:38,130 --> 01:37:43,020
stochastic search and optimization algorithms

1546
01:37:43,040 --> 01:37:46,860
so the key idea with this evidence is that

1547
01:37:47,020 --> 01:37:48,610
we explore

1548
01:37:48,670 --> 01:37:51,130
the search space

1549
01:37:51,460 --> 01:37:56,610
in order to find a good solution to computer simulated evolution

1550
01:37:56,620 --> 01:37:58,450
or in other words

1551
01:37:58,480 --> 01:38:03,260
problem solving is implemented in this way

1552
01:38:05,810 --> 01:38:06,790
it is

1553
01:38:06,810 --> 01:38:13,920
nice to have this and all analogy in mind when we discuss evolutionary patterns so

1554
01:38:13,920 --> 01:38:16,650
number further you probably just your query

1555
01:38:16,670 --> 01:38:19,320
number the independent so

1556
01:38:19,560 --> 01:38:21,500
we can drop this here

1557
01:38:21,710 --> 01:38:27,740
in the denominator d and then we have only two independent on the query

1558
01:38:27,760 --> 01:38:29,930
which of ranking

1559
01:38:29,960 --> 01:38:32,600
we can also try

1560
01:38:32,660 --> 01:38:37,070
now in the literature you will see that all of this latter to

1561
01:38:37,120 --> 01:38:39,010
can be dropped

1562
01:38:39,010 --> 01:38:41,760
assuming also that you are

1563
01:38:41,810 --> 01:38:44,200
are independent so

1564
01:38:44,210 --> 01:38:48,350
the joint probability of documents relevant to that

1565
01:38:48,820 --> 01:38:53,680
two random variables and also independent and we come up with

1566
01:38:54,840 --> 01:38:57,020
model generative model

1567
01:38:57,020 --> 01:38:58,550
that your document

1568
01:38:58,560 --> 01:39:01,050
generate your career

1569
01:39:01,050 --> 01:39:03,010
so this is how how

1570
01:39:03,020 --> 01:39:05,360
in the literature and how you come to

1571
01:39:05,390 --> 01:39:10,450
such a language models but now let's start with the case

1572
01:39:10,460 --> 01:39:11,810
this model

1573
01:39:11,820 --> 01:39:13,960
we still have to estimate

1574
01:39:15,180 --> 01:39:19,640
the probability that the document generates the query

1575
01:39:19,650 --> 01:39:21,310
and you very often

1576
01:39:21,310 --> 01:39:24,020
it is composed of that attitude

1577
01:39:25,140 --> 01:39:30,400
can the grand canyon unique and can be things work

1578
01:39:30,820 --> 01:39:33,610
so we assume

1579
01:39:33,630 --> 01:39:35,790
again independence

1580
01:39:39,560 --> 01:39:41,820
attributes of query terms

1581
01:39:41,820 --> 01:39:43,540
given the document

1582
01:39:46,330 --> 01:39:48,530
so we can actually

1583
01:39:48,530 --> 01:39:51,340
consider only the individuals

1584
01:39:51,370 --> 01:39:55,410
credit so i suppose you have credit

1585
01:39:55,420 --> 01:39:58,180
you could consider only individual twenty

1586
01:39:58,270 --> 01:40:00,150
and your

1587
01:40:00,170 --> 01:40:03,380
in the independent so you can estimate

1588
01:40:04,280 --> 01:40:07,590
quite easily from document central

1589
01:40:07,790 --> 01:40:10,120
with the independence assumption to make

1590
01:40:10,130 --> 01:40:12,000
the product

1591
01:40:12,000 --> 01:40:13,750
and again can

1592
01:40:17,550 --> 01:40:19,550
the sum of log p

1593
01:40:19,560 --> 01:40:24,990
when we consider all these great

1594
01:40:25,520 --> 01:40:27,470
and this is actually

1595
01:40:27,550 --> 01:40:28,880
the creation of

1596
01:40:28,910 --> 01:40:31,950
which stars which i start

1597
01:40:32,650 --> 01:40:34,450
look at the very

1598
01:40:36,310 --> 01:40:40,020
well i prefer to discuss this language model

1599
01:40:40,130 --> 01:40:45,060
so we have a committee composed of two

1600
01:40:45,080 --> 01:40:50,080
any computer probability that the document that generates

1601
01:40:50,100 --> 01:40:51,980
the quite

1602
01:40:51,980 --> 01:40:54,370
so we have this

1603
01:40:56,890 --> 01:40:58,120
of two

1604
01:40:58,480 --> 01:41:00,670
generated by document

1605
01:41:00,680 --> 01:41:01,800
which we can

1606
01:41:02,920 --> 01:41:04,710
by just looking at

1607
01:41:04,750 --> 01:41:08,800
the term frequency for instance in the document

1608
01:41:08,850 --> 01:41:10,390
to see how frequent

1609
01:41:10,450 --> 01:41:16,430
that is in proportion to all of the other terms of the document

1610
01:41:16,510 --> 01:41:19,460
of course the very very likely

1611
01:41:19,510 --> 01:41:21,030
will not occur

1612
01:41:21,040 --> 01:41:22,490
in document

1613
01:41:23,620 --> 01:41:25,030
we have a very for

1614
01:41:25,040 --> 01:41:27,630
representation of the document

1615
01:41:27,680 --> 01:41:29,320
so we need

1616
01:41:29,330 --> 01:41:32,490
some form of

1617
01:41:32,560 --> 01:41:34,260
muti action

1618
01:41:34,550 --> 01:41:36,490
to find

1619
01:41:37,410 --> 01:41:41,270
two not talked in the world

1620
01:41:41,280 --> 01:41:43,870
in this equation

1621
01:41:43,900 --> 01:41:47,030
and also yesterday well

1622
01:41:47,040 --> 01:41:49,360
different forms of marketing

1623
01:41:53,520 --> 01:41:58,140
we often moved in this model with the collection

1624
01:41:58,200 --> 01:42:03,090
so the two of you look at the turn of your question

1625
01:42:03,090 --> 01:42:05,440
OK so

1626
01:42:05,660 --> 01:42:09,680
we know that

1627
01:42:09,890 --> 01:42:11,390
she is

1628
01:42:11,400 --> 01:42:15,510
and those will be space induced by kernel

1629
01:42:15,530 --> 01:42:21,260
and the idea of the kernel is the to preprocess the data

1630
01:42:21,800 --> 01:42:24,530
first of all the idea of the feature space and then i'll tell you about

1631
01:42:24,530 --> 01:42:27,500
the data is preprocessed your data

1632
01:42:27,510 --> 01:42:31,080
by mapping the data into some dot product space h

1633
01:42:31,100 --> 01:42:36,170
so each x is preprocessed into biophysics and then rather than learning the mapping from

1634
01:42:36,170 --> 01:42:39,930
x to y for instance x is some pattern then y is the label plus

1635
01:42:39,930 --> 01:42:41,770
minus one

1636
01:42:41,780 --> 01:42:44,150
we now that the mapping from phi x y

1637
01:42:44,210 --> 01:42:48,930
so typically the dimensionality of the space is very large and much larger than the

1638
01:42:48,930 --> 01:42:52,640
dimensionality of the input space if the input space is a linear space at all

1639
01:42:53,380 --> 01:42:58,040
so this thing is a high dimensional things you know turned your data into high

1640
01:42:58,040 --> 01:43:03,300
dimensional objects and you want to learn with these data and classical statistics would tell

1641
01:43:03,300 --> 01:43:06,410
you that a bad idea because there's something called because dimensionality

1642
01:43:06,800 --> 01:43:11,280
if you have high dimensional data how usually need a lot more data than if

1643
01:43:11,280 --> 01:43:13,220
he had low dimensional data

1644
01:43:13,250 --> 01:43:18,660
however equipped with all our knowledge of statistical learning theory

1645
01:43:18,910 --> 01:43:23,960
we cannot say we're not worried about dimensionality which is worried about capacity so we

1646
01:43:23,960 --> 01:43:27,580
can come up with function classes to learn on this data

1647
01:43:27,940 --> 01:43:31,050
that have small capacity then we're not worried

1648
01:43:31,050 --> 01:43:34,630
moreover we would say that

1649
01:43:34,660 --> 01:43:40,190
people are afraid of dimensionality because in some cases dimensionality is the same as capacity

1650
01:43:40,190 --> 01:43:44,860
the interesting cases where it's not so for instance we've seen one case where dimensionality

1651
01:43:46,000 --> 01:43:47,690
are the same

1652
01:43:47,690 --> 01:43:52,050
and this was this case you reference to the same if we're separating hyperplanes in

1653
01:43:52,050 --> 01:43:55,030
RN VC dimension will be in plus one

1654
01:43:55,050 --> 01:43:58,280
so here the curse of dimensionality if you want

1655
01:43:58,330 --> 01:44:03,350
however if we consider hyperplanes with a margin we don't have this because any more

1656
01:44:03,350 --> 01:44:08,920
so the interesting thing we would then say is only capacity and not dimensionality

1657
01:44:11,860 --> 01:44:12,730
OK so

1658
01:44:14,760 --> 01:44:16,400
let's look at one example

1659
01:44:16,420 --> 01:44:19,620
suppose we have two classes of data

1660
01:44:19,660 --> 01:44:21,470
like this

1661
01:44:21,560 --> 01:44:23,610
we assume that there is some

1662
01:44:23,660 --> 01:44:28,320
and the decision boundary which is an ellipse in this case that separates the crosses

1663
01:44:28,320 --> 01:44:29,840
from the red circles

1664
01:44:29,850 --> 01:44:34,970
and if we want to solve this classification problem with survive to estimate is ellipse

1665
01:44:35,010 --> 01:44:39,080
but instead of estimating the ellipse we could also just take a detour into a

1666
01:44:39,130 --> 01:44:45,380
high dimensional space in this case three-dimensional computing all monomials the product or the two

1667
01:44:45,390 --> 01:44:49,050
one of monomials of degree two in the input variables

1668
01:44:49,070 --> 01:44:54,610
there are three possible such monomials don't worry about this scaling factor and once we

1669
01:44:54,610 --> 01:44:58,940
are in this space you think about it for a second you will see the

1670
01:45:00,410 --> 01:45:03,070
boundary of the ellipse

1671
01:45:03,090 --> 01:45:08,500
actually maps into a separating hyperplane in that space since ones written in x one

1672
01:45:08,500 --> 01:45:14,520
square next to square this ellipse equation becomes an find equation

1673
01:45:14,530 --> 01:45:19,860
in this case mapping into the high dimensional space is suddenly given us the possibility

1674
01:45:19,860 --> 01:45:23,030
to solve the problem using a separating hyperplane

1675
01:45:23,040 --> 01:45:26,720
more generally we will be interested in

1676
01:45:26,740 --> 01:45:31,260
patterns not of dimension two but of dimension n

1677
01:45:31,270 --> 01:45:37,660
so for instance we might have images mentioned sixteen times sixteen two hundred fifty six

1678
01:45:37,970 --> 01:45:42,010
moreover we might be interested not in productive all two but in products of all

1679
01:45:42,010 --> 01:45:46,790
of the where could be any natural number in this case the number of possible

1680
01:45:46,790 --> 01:45:51,700
products of the dimensionality of all features based on the right-hand side will grow like

1681
01:45:51,700 --> 01:45:53,180
into the power of the

1682
01:45:53,200 --> 01:45:59,260
even in this relatively low dimensional case we considered product of the five so all

1683
01:45:59,270 --> 01:46:05,600
feature space consists of all possible product of five different pixels in these images we

1684
01:46:05,600 --> 01:46:08,860
would certainly be working in a space of dimension ten to the power of ten

1685
01:46:08,880 --> 01:46:11,070
which is something that you want to avoid

1686
01:46:11,180 --> 01:46:16,260
it turns out we can actually avoided using kernel functions which turn not you can

1687
01:46:16,260 --> 01:46:20,110
compute dot product in that space without having to into that space

1688
01:46:20,180 --> 01:46:23,240
to see this take two points x and x prime

1689
01:46:23,260 --> 01:46:26,910
not into that space so this is the a mapping from the last slide

1690
01:46:26,920 --> 01:46:32,890
in take the dot product between them denoted the productivity this transpose so these two

1691
01:46:34,040 --> 01:46:36,970
if you do that

1692
01:46:36,990 --> 01:46:39,220
and you can just work out in your in your head if you like you

1693
01:46:39,220 --> 01:46:40,860
get three terms

1694
01:46:40,900 --> 01:46:46,920
and these three terms will actually be complete binomial formula that you can rewrite the

1695
01:46:46,920 --> 01:46:50,760
square of the the dot product taken in input space

1696
01:46:50,780 --> 01:46:54,230
if we don't see this now and may be surprising see the first time i

1697
01:46:55,140 --> 01:47:02,420
you just try this out yourself afterwards because that's one of the surprising things about

1698
01:47:02,440 --> 01:47:06,510
so rather than nothing these two points into that space and taking the product we

1699
01:47:06,510 --> 01:47:10,000
could as well have taken the dot product in the input space and then raised

1700
01:47:10,000 --> 01:47:11,680
to the power of two

1701
01:47:11,690 --> 01:47:16,120
we would have got the same result would have saved the mapping into this three-dimensional

1702
01:47:16,120 --> 01:47:19,320
space which would make a big difference in this case because we only talking about

1703
01:47:19,330 --> 01:47:23,140
two dimensions was three will turn out the same thing also works

1704
01:47:23,160 --> 01:47:25,210
in the general case

1705
01:47:25,220 --> 01:47:28,930
so this is an example of a kernel function is a function

1706
01:47:28,940 --> 01:47:32,340
corresponds to adopt product in another space

1707
01:47:32,360 --> 01:47:35,550
so here's the more general case

1708
01:47:35,570 --> 01:47:40,740
let's consider n dimensional patterns or inputs and product quality

1709
01:47:40,760 --> 01:47:42,270
and actually i

1710
01:47:42,290 --> 01:47:44,290
go back was i will start

1711
01:47:44,310 --> 01:47:46,190
with the generalized form

1712
01:47:46,200 --> 01:47:47,720
this come here

1713
01:47:48,650 --> 01:47:51,530
generalize this thing and see what happens

1714
01:47:51,540 --> 01:47:56,400
so this is the product between two n dimensional vectors

1715
01:47:56,420 --> 01:48:00,160
but can write like this canonical problem raised to the power of the this gives

1716
01:48:00,160 --> 01:48:03,010
me a the full from here

1717
01:48:03,020 --> 01:48:05,660
these indices j one through j d

1718
01:48:05,670 --> 01:48:09,950
and that here i then these terms have grouped them

1719
01:48:09,970 --> 01:48:11,940
everything that depends on x

1720
01:48:11,980 --> 01:48:16,410
on the left and everything that depend on ex-prime i have put on the right

1721
01:48:16,500 --> 01:48:21,190
so just multiplied out this big thing here

1722
01:48:21,210 --> 01:48:24,160
and if you look at this you notice

1723
01:48:24,210 --> 01:48:28,330
first of all this is a non linear function of x this is the same

1724
01:48:28,330 --> 01:48:31,740
linear function evaluated ex-prime

1725
01:48:31,760 --> 01:48:34,330
this is just a lot of some

1726
01:48:34,380 --> 01:48:38,930
so if you want this is just a dot product in

1727
01:48:38,940 --> 01:48:41,590
in a slightly transformed space

1728
01:48:41,600 --> 01:48:46,630
and the axis of the space are spanned by these products of features of the

1729
01:48:46,630 --> 01:48:52,270
inputs so these are all the possible products of the features of the input vectors

1730
01:48:52,330 --> 01:48:56,070
and here is the same for the second so this is just some non linear

1731
01:48:56,080 --> 01:48:58,020
transformation of x

1732
01:48:58,040 --> 01:49:01,660
here we have the same for prime and then we take the product in this

1733
01:49:01,660 --> 01:49:06,770
very high dimensional space so the dimensionality of the space is into the paul d

1734
01:49:09,150 --> 01:49:12,970
bingo so then we have a way of computing the product in this very high

1735
01:49:12,970 --> 01:49:14,150
dimensional space

1736
01:49:14,160 --> 01:49:19,270
simply by computing the partition the input space in the plane this simple and linearity

1737
01:49:19,280 --> 01:49:21,190
raising to the party

1738
01:49:21,240 --> 01:49:23,940
and the same thing works

1739
01:49:23,950 --> 01:49:28,070
more generally and actually i will not use this way but i will

1740
01:49:28,080 --> 01:49:31,930
do it differently the same thing will work more generally

1741
01:49:33,800 --> 01:49:39,250
a larger class of kernels called positive definite kernels and i think i will try

1742
01:49:39,250 --> 01:49:41,250
at age is

1743
01:49:41,270 --> 01:49:42,610
i considered

1744
01:49:42,610 --> 01:49:47,040
that said this information about parts of speech we also

1745
01:49:47,460 --> 01:49:54,130
going ignore all past which has a not so why is that

1746
01:49:54,150 --> 01:49:56,150
well because if you get just

1747
01:49:56,210 --> 01:50:00,400
and now which is very common to all sentences that say

1748
01:50:02,560 --> 01:50:08,940
president of the united states barack obama said already six months after years of sentences

1749
01:50:08,940 --> 01:50:13,900
but it doesn't tell you anything about what happens so to minimise chances to get

1750
01:50:13,900 --> 01:50:14,960
something like that

1751
01:50:14,960 --> 01:50:17,560
what we we do is we at

1752
01:50:17,580 --> 01:50:22,380
perhaps which casts has a so this gives us

1753
01:50:24,440 --> 01:50:29,060
it makes it more likely that we get a and is sentence tells what really

1754
01:50:29,860 --> 01:50:31,540
but they

1755
01:50:31,560 --> 01:50:36,460
and then finally the total path length is normalized so that you know

1756
01:50:36,480 --> 01:50:41,250
that has the lowest average in edge weights

1757
01:50:41,250 --> 01:50:42,880
so can be very

1758
01:50:42,900 --> 01:50:45,960
shaw also in terms of interest and has the

1759
01:50:45,960 --> 01:50:52,170
the most total weeks but there might be among to which which was not always

1760
01:50:52,210 --> 01:50:56,310
is higher whose average rate is still lots

1761
01:50:56,320 --> 01:50:59,040
rather longer but with

1762
01:50:59,040 --> 01:51:05,210
which maximizes work which minimize the average regret

1763
01:51:06,310 --> 01:51:10,340
nobody data and this was just without

1764
01:51:10,440 --> 01:51:15,040
so this is just an example know how to to relate to

1765
01:51:15,060 --> 01:51:17,670
i don't you know the page is called google news

1766
01:51:17,690 --> 01:51:22,380
pegasus the mail change little bit so what's

1767
01:51:22,400 --> 01:51:24,750
good news about

1768
01:51:24,770 --> 01:51:26,630
it aggregates

1769
01:51:26,650 --> 01:51:30,900
news from different sources does not have the right to use it does provide news

1770
01:51:31,110 --> 01:51:33,560
to itself but it just takes

1771
01:51:34,340 --> 01:51:38,820
o news available on the the web science

1772
01:51:39,110 --> 01:51:40,520
with respect to the

1773
01:51:40,540 --> 01:51:44,090
languages that aggregates news about

1774
01:51:44,130 --> 01:51:47,920
which are likely to be about the same thing together and also categories

1775
01:51:48,290 --> 01:51:53,270
so for example this is a web page correspond to the english version

1776
01:51:53,310 --> 01:51:56,750
think that just dotcom used good outcomes

1777
01:51:56,900 --> 01:52:02,580
you have some categories like sports entertainment business

1778
01:52:03,960 --> 01:52:07,040
top stories with right

1779
01:52:07,040 --> 01:52:10,900
so without categories then put to the top of the web page

1780
01:52:10,920 --> 01:52:11,750
so that

1781
01:52:11,770 --> 01:52:17,360
what you see there normally is that you see a snippet from a news article

1782
01:52:17,420 --> 01:52:22,340
tell you where it comes from just a few of the TED this news article

1783
01:52:22,340 --> 01:52:26,130
you can click on it and then you will arrive

1784
01:52:27,630 --> 01:52:31,190
page three times in this case it also

1785
01:52:31,210 --> 01:52:32,790
gives you

1786
01:52:32,810 --> 01:52:40,670
link is final you can really should can gives you what should was in this

1787
01:52:41,380 --> 01:52:44,440
almost seven hundred articles which are all

1788
01:52:44,460 --> 01:52:47,880
unlike many of which are likely to be about this very same event

1789
01:52:47,920 --> 01:52:51,790
so it's no longer just a topic is not only about the

1790
01:52:55,500 --> 01:52:58,810
so the problem is that

1791
01:52:58,860 --> 01:53:03,040
so it's not only about sports about seventy

1792
01:53:04,730 --> 01:53:07,980
this is what happens if you click on their

1793
01:53:08,000 --> 01:53:09,540
really for which

1794
01:53:09,560 --> 01:53:12,400
for each this related articles

1795
01:53:12,840 --> 01:53:16,340
so you see it's snippets those articles and you can see that what you get

1796
01:53:20,190 --> 01:53:25,290
so sentences that it's almost the same thing

1797
01:53:25,290 --> 01:53:28,610
this is what is often used for

1798
01:53:29,360 --> 01:53:30,090
after the concert

1799
01:53:31,730 --> 01:53:36,250
this is of related to use this

1800
01:53:37,420 --> 01:53:42,590
it says that it is not known or use cluster

1801
01:53:42,610 --> 01:53:48,190
the collection of news on this topic is introduced us to become from different sources

1802
01:53:48,210 --> 01:53:51,880
what you can expect from them is that they were published the same time

1803
01:53:51,940 --> 01:53:56,400
but like to be able to see that but of course it's all done automatically

1804
01:53:56,400 --> 01:54:01,090
so there is no human intervention what it means is that there are definitely duplicates

1805
01:54:01,190 --> 01:54:06,440
are and there's also voiced so nice means that you get something that doesn't belong

1806
01:54:06,460 --> 01:54:07,400
to the class

1807
01:54:07,420 --> 01:54:11,230
so for example you can have the last sentence of eighteen

1808
01:54:11,250 --> 01:54:14,130
two teams and

1809
01:54:14,130 --> 01:54:17,440
it can happen in the same cluster you suddenly get

1810
01:54:17,520 --> 01:54:20,920
and article about a very different game

1811
01:54:21,750 --> 01:54:25,400
this and this is what i mean with the noise

1812
01:54:25,420 --> 01:54:28,750
and so those newscasts now

1813
01:54:28,900 --> 01:54:31,940
to test their just presented

1814
01:54:31,960 --> 01:54:33,790
i need to get

1815
01:54:33,790 --> 01:54:35,500
so somewhere

1816
01:54:35,710 --> 01:54:41,040
so sentence passed again clusters of sentences which is very similar to not only

1817
01:54:41,060 --> 01:54:43,290
that there is sufficient to be monitored

1818
01:54:43,320 --> 01:54:44,920
the same position

1819
01:54:44,920 --> 01:54:49,080
and to obtain knows what did

1820
01:54:49,090 --> 01:54:54,940
i just took the first sentence is expected news articles here the idea is very

1821
01:54:54,940 --> 01:55:00,150
simple so if you know about summarisation petition so

1822
01:55:00,210 --> 01:55:06,040
for example i used to be called document distant conference dr or text analysis conference

1823
01:55:06,040 --> 01:55:11,900
tech what happens there you get the data gets used to work with local news

1824
01:55:11,920 --> 01:55:16,770
with and then you're trying to your time you have to provide a the food

1825
01:55:16,790 --> 01:55:24,960
system which automatically summary and then you get tested for safety or usually fifty newscasters

1826
01:55:24,960 --> 01:55:30,000
you process with this systems you get results for every use must be sent back

1827
01:55:30,000 --> 01:55:33,590
and then a couple of months later there is

1828
01:55:33,610 --> 01:55:35,270
the results are published so

1829
01:55:35,290 --> 01:55:37,840
your system is

1830
01:55:38,270 --> 01:55:40,480
by humans also automatically

1831
01:55:40,500 --> 01:55:44,340
that's a great respect different parameters how

1832
01:55:44,360 --> 01:55:46,270
readability assign rewards OWL

1833
01:55:46,340 --> 01:55:49,940
so which more and how many mistakes

1834
01:55:49,960 --> 01:55:51,980
grammatical mistakes him so

1835
01:55:52,000 --> 01:55:53,540
that's very helpful

1836
01:55:53,560 --> 01:55:55,150
and then the british

1837
01:55:55,150 --> 01:55:56,380
let's call

1838
01:55:58,610 --> 01:56:02,940
what happens there is they also provide space so we don't know what this these

1839
01:56:03,040 --> 01:56:04,980
is is

1840
01:56:05,020 --> 01:56:10,270
it's a system which is very simplistic

1841
01:56:10,290 --> 01:56:15,190
you can think of it but it should be reasonable and if your system which

1842
01:56:15,190 --> 01:56:20,810
is expected to be more complex is performs worse than this is system that things

1843
01:56:20,810 --> 01:56:22,690
we really went wrong places

1844
01:56:22,690 --> 01:56:27,150
or if you get the same results of this is that in the absence of

1845
01:56:27,250 --> 01:56:28,250
being of years

1846
01:56:28,270 --> 01:56:29,460
you didn't really

1847
01:56:29,690 --> 01:56:32,420
do you need

1848
01:56:32,440 --> 01:56:34,110
good job

1849
01:56:34,130 --> 01:56:35,040
so four

1850
01:56:35,080 --> 01:56:37,690
this is a petition what they used to do this

1851
01:56:37,710 --> 01:56:42,090
they there's an old person's baseline

1852
01:56:42,670 --> 01:56:50,500
during the summary of documents for documents selected randomly in the sentences are put together

1853
01:56:50,630 --> 01:56:56,250
and surprisingly is a very competitive so did some more

1854
01:56:56,250 --> 01:57:00,710
complex systems may perform worse than there are different reasons for that so one of

1855
01:57:00,710 --> 01:57:05,230
them is that if you try to modify it so you can get a matter

1856
01:57:05,320 --> 01:57:09,310
or if you get sentences from different parts of the document is likely you put

1857
01:57:09,310 --> 01:57:14,800
some pronouns which people geometry we cannot be interpreted out of context and the first

1858
01:57:14,800 --> 01:57:19,150
sentences that may have been pronounced this i good and then

1859
01:57:19,190 --> 01:57:21,500
usually one also for

1860
01:57:21,520 --> 01:57:29,130
property of just vision of news articles is that the very often be summarise the

1861
01:57:29,130 --> 01:57:30,460
content of the news

1862
01:57:30,460 --> 01:57:35,020
so it's not your fault magazine articles for example based on the very first sentence

1863
01:57:35,420 --> 01:57:39,270
sometimes just introduction but with uses three which was here

1864
01:57:39,290 --> 01:57:41,940
if we go back for example to this station

