1
00:00:00,000 --> 00:00:01,900
in general

2
00:00:01,920 --> 00:00:04,590
here the idea is

3
00:00:04,650 --> 00:00:08,250
so i just think how much risk is our economic well being when i make

4
00:00:08,250 --> 00:00:10,670
a decision i'm taking some risk

5
00:00:10,710 --> 00:00:15,860
i want to take a decision about by my my

6
00:00:15,880 --> 00:00:19,280
job career taking the really risky decision

7
00:00:19,300 --> 00:00:24,400
if i want to be an academic economist and started do a phd and not

8
00:00:24,690 --> 00:00:30,590
able to know now if you i will be able to secede the finish my

9
00:00:30,610 --> 00:00:31,880
phd thesis

10
00:00:31,900 --> 00:00:36,400
hi i'm not able to know if i'm going to sit in finding a good

11
00:00:37,300 --> 00:00:41,320
in a good university and so i'm taking lot of risks

12
00:00:41,820 --> 00:00:45,050
is it possible to diversify its risks

13
00:00:45,070 --> 00:00:47,020
maybe yes maybe not

14
00:00:48,050 --> 00:00:51,690
is it possible that the market for the classification of this

15
00:00:53,230 --> 00:00:54,730
well it depends

16
00:00:54,750 --> 00:00:57,570
on the distribution of the risks

17
00:00:57,570 --> 00:01:02,630
if this risks our lady stable distributed with an awful lot one the market will

18
00:01:02,710 --> 00:01:04,480
not work

19
00:01:04,500 --> 00:01:09,550
if there the shots of distributed with an offer higher one while the market could

20
00:01:10,340 --> 00:01:18,040
so it's a it's a tremendous deal tremendously important to two to understand this factor

21
00:01:18,070 --> 00:01:23,480
to apply this theory to to this to this type of of literature well the

22
00:01:23,480 --> 00:01:28,080
main idea is the following one maybe it's a it's something which

23
00:01:28,130 --> 00:01:34,270
it's not when we have something do thinking about it very very

24
00:01:34,320 --> 00:01:38,150
frequently our wealth

25
00:01:38,170 --> 00:01:40,420
our present our wealth now

26
00:01:40,440 --> 00:01:45,500
can be defined by the some of the discounted present discounted

27
00:01:45,520 --> 00:01:51,190
the value of r wealth coming from financial markets so dividends

28
00:01:51,230 --> 00:01:56,150
interest rates discounted to now

29
00:01:56,170 --> 00:01:58,800
from here to the end of my life

30
00:02:00,270 --> 00:02:07,110
the present discounted value of like human capital what i will able to

31
00:02:07,130 --> 00:02:12,110
to gain working and this depends on my choices

32
00:02:12,170 --> 00:02:15,020
depends on what choices in terms of career

33
00:02:15,110 --> 00:02:19,440
and also on illiquid assets income

34
00:02:19,460 --> 00:02:21,860
for example the value of the house

35
00:02:21,860 --> 00:02:23,570
then i'm going to buy

36
00:02:23,590 --> 00:02:24,820
in twenty years

37
00:02:24,840 --> 00:02:28,630
all the value of the house that in which i live

38
00:02:28,670 --> 00:02:29,920
in twenty years

39
00:02:29,920 --> 00:02:34,150
and this is liquid it's not easy to sell the house

40
00:02:34,900 --> 00:02:38,860
the problem is that the first one

41
00:02:38,880 --> 00:02:42,250
can be diversified financial markets

42
00:02:42,270 --> 00:02:43,150
this one

43
00:02:43,210 --> 00:02:46,480
is not of this find function one

44
00:02:46,480 --> 00:02:49,090
we are learning all these three

45
00:02:50,710 --> 00:02:54,740
the idea is is it possible to diversify summer of love

46
00:02:54,940 --> 00:02:57,090
this risk

47
00:02:57,090 --> 00:03:00,150
using financial markets to make

48
00:03:00,170 --> 00:03:03,230
well for policies for example

49
00:03:03,250 --> 00:03:09,000
this is idea of showing one the main point is that in terms of wealth

50
00:03:09,020 --> 00:03:14,730
this council ten percent discount format so there is a huge scope for diversification in

51
00:03:14,730 --> 00:03:16,320
this case

52
00:03:21,040 --> 00:03:24,860
well what about these shocks

53
00:03:26,610 --> 00:03:28,920
i have some

54
00:03:28,940 --> 00:03:36,590
summers lies about some possible implementation and i'm going out of time so i'm going

55
00:03:36,590 --> 00:03:40,400
to just to present some results i think i was skip theoretical results but i

56
00:03:40,460 --> 00:03:44,300
don't have time enough so what about it

57
00:03:44,320 --> 00:03:50,170
once again for the united states there is a huge amount of food available as

58
00:03:50,170 --> 00:03:55,840
regard for the average highly wage

59
00:03:55,860 --> 00:03:59,960
four occupations of the four-digit level

60
00:03:59,980 --> 00:04:04,880
and we have data from two thousand one to two thousand three to five thousand

61
00:04:04,880 --> 00:04:11,300
six the the number of data points is quite high for economics

62
00:04:11,360 --> 00:04:16,820
so we can try to reason with this data

63
00:04:16,840 --> 00:04:24,770
this is an idea of how much you gaining if you work in some position

64
00:04:24,770 --> 00:04:33,070
position means that we have data for a supply manager working in the automotive sector

65
00:04:33,070 --> 00:04:37,340
and supply manager working in the food sector

66
00:04:37,980 --> 00:04:45,020
so we have data for different positions in different sectors it doesn't matter

67
00:04:47,250 --> 00:04:53,090
since we have data for different years and we can try to

68
00:04:59,040 --> 00:05:00,460
of wages

69
00:05:00,480 --> 00:05:03,110
over time some

70
00:05:03,130 --> 00:05:07,040
i know that i sample is very small but these are the

71
00:05:07,050 --> 00:05:12,820
we have about about this position so we are forced to use them

72
00:05:12,840 --> 00:05:19,880
so if you calculate the cumulative growth rates

73
00:05:21,090 --> 00:05:24,190
cumulative growth rates which means that you

74
00:05:24,210 --> 00:05:28,800
abstract from a mere increases in prices

75
00:05:30,050 --> 00:05:32,670
so if you are clear

76
00:05:32,690 --> 00:05:35,750
and you consider just the real wage

77
00:05:37,440 --> 00:05:38,980
the number of

78
00:05:39,000 --> 00:05:43,320
fan which is that you can eat in five years for example

79
00:05:43,380 --> 00:05:45,170
independently of the

80
00:05:45,190 --> 00:05:47,750
price of sandwiches in for years

81
00:05:47,770 --> 00:05:51,130
you have a distribution

82
00:05:51,150 --> 00:05:54,710
which is basically an abortion

83
00:05:55,040 --> 00:05:59,650
this is the cumulative growth rates

84
00:05:59,690 --> 00:06:01,270
if five years

85
00:06:01,300 --> 00:06:05,380
for all these industries

86
00:06:08,050 --> 00:06:14,230
if you take occupational majors

87
00:06:15,770 --> 00:06:19,210
you have these special majors

88
00:06:19,230 --> 00:06:20,840
and if you take

89
00:06:20,860 --> 00:06:24,610
the community growth rates of

90
00:06:24,610 --> 00:06:26,840
are basically

91
00:06:26,860 --> 00:06:33,070
upper bound this by a sum of probabilities now I have one probability for each of these terms

92
00:06:33,070 --> 00:06:40,960
and is a sum of a calligraphic N such probabilities and then I can use a general thought

93
00:06:41,000 --> 00:06:42,530
for each term

94
00:06:42,650 --> 00:06:50,260
so this is this second form of the Chernoff bound that was referring

95
00:06:51,250 --> 00:06:55,000
to empirical quantities how far they deviate from each other

96
00:06:55,110 --> 00:06:59,900
if I plug this in and then what I get is

97
00:07:00,050 --> 00:07:02,690
I have this shattering coefficient

98
00:07:02,690 --> 00:07:07,050
I also have a sum that goes over this number of terms and for each term

99
00:07:07,050 --> 00:07:09,630
I get something like this and

100
00:07:09,880 --> 00:07:14,780
if I put this together I get this kind of bound what does this tell us

101
00:07:14,860 --> 00:07:18,900
this tells us that if the shattering coefficient

102
00:07:18,940 --> 00:07:25,670
if it's somehow is well-behaved in particular if it doesn't grow exponentially in M then this second

103
00:07:25,670 --> 00:07:27,090
term will always win

104
00:07:27,150 --> 00:07:30,710
because this the case is exponentially fast

105
00:07:30,750 --> 00:07:38,320
so in that case the right inside will go to zero whether I'm speaking differently now

106
00:07:38,440 --> 00:07:44,820
maybe I'm getting too emotional that the bound is close this is the so-called Vapnik Chervonenkis

107
00:07:44,820 --> 00:07:47,880
time unequality so this is a a

108
00:07:48,190 --> 00:07:57,320
people call this a tail bound in statistics and maybe just to discuss it little bit so

109
00:07:57,320 --> 00:08:00,880
there are two types of randomness in here one is that

110
00:08:00,920 --> 00:08:06,840
this P over here refers to drawing training examples we are drawing training examples from our unknown

111
00:08:06,840 --> 00:08:12,780
underlined probability distribution so this is this tells us how likely is it that we

112
00:08:12,800 --> 00:08:17,590
get a training set for which this is the case

113
00:08:17,590 --> 00:08:21,730
and the second randomness is that here on the

114
00:08:21,750 --> 00:08:22,690
these risks

115
00:08:22,920 --> 00:08:30,590
and these are an expectation over the same probability distribution well okay this one is a product measure so

116
00:08:30,610 --> 00:08:37,400
products of this distributions this is the distribution itself so here we also have

117
00:08:37,420 --> 00:08:43,400
we are using this probability distribution when we compute this expectation and here of course this is a risk so its

118
00:08:43,610 --> 00:08:47,420
expectation over the whole distribution in other words we think of this as a test

119
00:08:49,250 --> 00:08:56,640
and we can also rewrite this bound if we want so now here in this form the bound

120
00:08:56,640 --> 00:09:03,270
tells us that is very unlikely for training error and test error to be very

121
00:09:03,270 --> 00:09:12,050
different upsilon different it's very unlikely this term will be small for large training set sizes we can rewrite this

122
00:09:12,300 --> 00:09:18,250
by specifying the probability with which we want the risks to be close to the

123
00:09:18,250 --> 00:09:23,320
training error and then solving for upsilon maybe this is a nice exercise

124
00:09:23,320 --> 00:09:27,650
if you want to play around with with it and we can then rewrite this

125
00:09:27,650 --> 00:09:31,110
bound in two lines

126
00:09:32,650 --> 00:09:38,020
in that case the bound will tell us that with the probability of at least

127
00:09:38,020 --> 00:09:39,690
one minus delta

128
00:09:39,980 --> 00:09:47,260
this test error is upper bounded by the training error plus some quantity that

129
00:09:47,260 --> 00:09:51,460
depends on this shattering coefficient

130
00:09:51,920 --> 00:09:56,050
here you can also see this thing here will go to zero as M goes to

131
00:09:56,050 --> 00:09:57,500
infinity and if this

132
00:09:57,880 --> 00:10:04,630
shattering coefficient does not grow exponentially in M then this thing here will always win against

133
00:10:04,630 --> 00:10:07,300
this because here we have a a logarithm

134
00:10:07,360 --> 00:10:13,690
and this bound is this is now a typical learning theory bound

135
00:10:13,710 --> 00:10:19,210
it's kind of surprising that something like this is true it tells independent of the

136
00:10:19,210 --> 00:10:25,420
functions so this holds uniform over the whole function class in particular it holds for the function minimizing

137
00:10:25,420 --> 00:10:27,550
the risk so if we choose

138
00:10:27,590 --> 00:10:32,900
all function by minimizing some kind of training error then this bound still holds because it

139
00:10:32,900 --> 00:10:46,150
holds for all functions and maybe let's see maybe I should discuss this a little bit actually but actually let's

140
00:10:46,150 --> 00:10:50,610
first talk a bit more about capacity concepts and I think in the end I will have I will state

141
00:10:50,630 --> 00:10:55,400
something like this bound again so this is not the best possible bound you can give

142
00:10:55,400 --> 00:11:03,630
people have looked at constants and so on in detail and more sophisticated ways of doing things

143
00:11:03,630 --> 00:11:08,400
that's one comment the other comment we can minimize the bound over the function so that's

144
00:11:08,400 --> 00:11:12,280
something that maybe we would like to do cause in the end we want a function that generalizes

145
00:11:12,280 --> 00:11:15,710
well so we might say well okay you might say

146
00:11:16,320 --> 00:11:20,440
okay you've convinced me that I shouldn't be minimizing just a training error

147
00:11:20,480 --> 00:11:25,070
so I gave you an example where we used the class of all possible functions

148
00:11:25,190 --> 00:11:30,090
the training is basically meaningless the test error can be arbitrarily different eventhough the training

149
00:11:30,090 --> 00:11:35,650
errors are the same so maybe you convinced the training error is not the whole story

150
00:11:35,730 --> 00:11:39,880
but you might say well in that case let's just minimize this right inside

151
00:11:41,150 --> 00:11:44,820
of course we would like to do something like that we cannot do it directly because

152
00:11:44,820 --> 00:11:48,280
okay this is a property of the function here but this over here is

153
00:11:48,280 --> 00:11:52,980
not a property of the function it's a property of the function class that we are

154
00:11:52,980 --> 00:11:57,360
choosing from so it doesn't matter what is the solution that you came up with

155
00:11:57,360 --> 00:11:59,770
at the end but it matters

156
00:11:59,780 --> 00:12:04,130
what set of solutions you were chosing it from so if you give someone a training set

157
00:12:04,170 --> 00:12:08,590
some machinery guy he comes back and says hey I have a solution for you

158
00:12:08,630 --> 00:12:11,980
then you cannot assess whether this is a good solution or not you should you

159
00:12:11,980 --> 00:12:17,270
should just asking well which solutions did you choose from and if he said I choose from our

160
00:12:17,270 --> 00:12:22,650
a huge class of solutions then can say well that's probably rubbish what you're telling me If he

161
00:12:22,650 --> 00:12:52,540
so that is

162
00:12:52,540 --> 00:12:54,290
i will tell

163
00:12:54,290 --> 00:12:57,750
i will be your lecture this term

164
00:12:57,770 --> 00:13:01,570
make sure you have and out and make sure you read it

165
00:13:01,630 --> 00:13:05,880
it tells you everything you want to know about the course

166
00:13:05,890 --> 00:13:07,340
this course

167
00:13:07,410 --> 00:13:10,070
is about waves and vibrations

168
00:13:10,150 --> 00:13:11,990
but also solutions

169
00:13:12,060 --> 00:13:15,430
periodic not so periodic events

170
00:13:15,490 --> 00:13:17,700
when you look around in the world

171
00:13:17,740 --> 00:13:20,170
you see them everywhere

172
00:13:20,210 --> 00:13:22,280
one thing

173
00:13:22,340 --> 00:13:24,370
your heartbeat

174
00:13:24,420 --> 00:13:27,090
that's the periodic oscillation

175
00:13:27,100 --> 00:13:29,420
least i hope that for most of you

176
00:13:29,430 --> 00:13:31,340
this periodic

177
00:13:32,620 --> 00:13:34,650
some kind of aperiodic

178
00:13:35,870 --> 00:13:38,310
the blinking of your eyes

179
00:13:38,400 --> 00:13:40,680
the daily routines and you have it's

180
00:13:44,400 --> 00:13:45,960
taking a shower

181
00:13:45,980 --> 00:13:47,450
two classes

182
00:13:47,460 --> 00:13:51,380
and occasionally doing some work all those are

183
00:13:51,430 --> 00:13:55,450
periodic actions

184
00:13:55,510 --> 00:13:57,150
when you drink

185
00:13:57,210 --> 00:13:59,100
i think some orange juice

186
00:13:59,150 --> 00:14:00,980
the notices i try to

187
00:14:00,990 --> 00:14:03,590
move the liquid down into my stomach

188
00:14:03,630 --> 00:14:06,540
that's not a steady stream

189
00:14:06,540 --> 00:14:08,260
but it's a periodic motion

190
00:14:08,270 --> 00:14:15,990
look at my throat

191
00:14:16,010 --> 00:14:16,980
in fact

192
00:14:17,040 --> 00:14:20,230
even if i don't want to swallow the lake

193
00:14:20,270 --> 00:14:23,340
but simply have the ball was liquid and i turn it over

194
00:14:23,380 --> 00:14:26,840
that we all know that the water doesn't come out like a steady stream

195
00:14:26,850 --> 00:14:29,900
but it goes clock clock clock clock

196
00:14:29,960 --> 00:14:33,650
that's some kind of the periodic motion

197
00:14:33,680 --> 00:14:34,940
i have you

198
00:14:34,990 --> 00:14:39,400
the toy which i used to entertain my dinner guests particularly

199
00:14:39,460 --> 00:14:42,060
physicists is interesting

200
00:14:42,120 --> 00:14:46,330
the liquid and the idea is to get the liquid there

201
00:14:46,340 --> 00:14:49,960
and then the problem is how can you do to them the fastest possible way

202
00:14:50,020 --> 00:14:54,770
well if you turn it over you see that phenomenon that i just mentioned

203
00:14:54,770 --> 00:14:55,680
which is that

204
00:14:57,370 --> 00:15:00,180
well it's not a steady stream it's almost pathetic

205
00:15:00,190 --> 00:15:03,310
the way that runs from one side to the

206
00:15:03,310 --> 00:15:05,000
i will take minutes

207
00:15:05,060 --> 00:15:07,990
before is there

208
00:15:08,030 --> 00:15:11,830
but it can be done in seventeen seconds

209
00:15:11,870 --> 00:15:15,470
during the five minute intermission that we have

210
00:15:15,520 --> 00:15:17,060
you may give it a try

211
00:15:17,080 --> 00:15:18,810
and i hope you won't break it

212
00:15:18,810 --> 00:15:21,360
and see whether any of you can think of the way

213
00:15:21,400 --> 00:15:23,270
you can transfer liquid

214
00:15:23,330 --> 00:15:28,500
in seventeen seconds

215
00:15:28,560 --> 00:15:30,650
you have breakfast in the morning

216
00:15:30,680 --> 00:15:32,160
and you can surely

217
00:15:32,180 --> 00:15:34,280
put your breakfast plate on the table

218
00:15:34,330 --> 00:15:38,560
what do you

219
00:15:38,560 --> 00:15:44,140
some kind of the periodic motion

220
00:15:44,180 --> 00:15:47,870
and two things can happen to displayed

221
00:15:47,890 --> 00:15:53,250
it can move as in only i called is as muesli because i'm straumur

222
00:15:53,370 --> 00:15:55,710
but it can also well

223
00:15:55,750 --> 00:15:59,650
in fact something can wobble without moving at

224
00:15:59,650 --> 00:16:01,120
that something can

225
00:16:01,180 --> 00:16:03,620
move as smoothly without warning

226
00:16:03,640 --> 00:16:05,280
in this case does both

227
00:16:05,310 --> 00:16:13,650
and the a example of that

228
00:16:13,680 --> 00:16:15,190
is what's called

229
00:16:15,240 --> 00:16:17,800
the is this

230
00:16:17,840 --> 00:16:19,930
which is

231
00:16:20,000 --> 00:16:21,360
a metal disk

232
00:16:21,370 --> 00:16:23,410
you'll see it shortly there

233
00:16:23,460 --> 00:16:25,940
and these metal disc

234
00:16:26,030 --> 00:16:31,160
we're going to wobble in this in a way that i will place

235
00:16:31,180 --> 00:16:33,280
and then we'll follow its motion

236
00:16:33,300 --> 00:16:34,810
as in new zealand

237
00:16:34,840 --> 00:16:36,930
and the wobbling frequency

238
00:16:36,990 --> 00:16:39,050
what is interesting as you will see

239
00:16:39,090 --> 00:16:42,400
that the as new so motion which has a certain period

240
00:16:42,480 --> 00:16:45,350
that period gets longer in time

241
00:16:45,400 --> 00:16:47,230
but the wall motion

242
00:16:47,260 --> 00:16:49,270
the frequency goes up

243
00:16:49,280 --> 00:16:52,510
so i started here

244
00:16:52,520 --> 00:16:54,460
and then i'll show it to you

245
00:16:54,540 --> 00:16:57,290
in a way that is more

246
00:16:58,860 --> 00:17:01,170
you can follow that

247
00:17:01,220 --> 00:17:02,900
it's an amazing toy

248
00:17:02,960 --> 00:17:04,780
to work out to physics

249
00:17:04,790 --> 00:17:06,850
very very difficult

250
00:17:06,860 --> 00:17:11,400
i was told that professor wilczek at MIT once gave a one-hour lecture

251
00:17:11,410 --> 00:17:13,690
exclusively on the explanation

252
00:17:13,730 --> 00:17:16,380
of these orders this

253
00:17:16,470 --> 00:17:18,010
try to see the

254
00:17:18,020 --> 00:17:24,050
as useful motion it will become clear as it slows down further

255
00:17:24,070 --> 00:17:27,840
you may be able to use the wall motion i will hold my microphone close

256
00:17:30,050 --> 00:17:37,650
here it is

257
00:17:37,710 --> 00:17:47,100
very high frequency already that you hear it

258
00:17:47,160 --> 00:17:51,730
quite amazing isn't it when you look at this

259
00:17:51,900 --> 00:17:53,650
while building

260
00:17:53,660 --> 00:17:58,270
frequency increased quite rapidly

261
00:17:58,340 --> 00:18:02,150
recall the as you so motion slows down

262
00:18:02,160 --> 00:18:03,800
how the frequency of

263
00:18:03,820 --> 00:18:06,010
the wobble goes up

264
00:18:06,020 --> 00:18:15,250
that an outcome of

265
00:18:16,640 --> 00:18:18,400
very difficult

266
00:18:18,460 --> 00:18:21,100
piece of physics right there

267
00:18:21,260 --> 00:18:38,170
if you take a tennis ball

268
00:18:38,270 --> 00:18:39,770
this is the super bowl

269
00:18:39,820 --> 00:18:41,200
and you want it

270
00:18:46,590 --> 00:18:50,170
the school student involvement thank you

271
00:18:50,220 --> 00:18:53,800
then you also get some kind of the periodic motion whereby by

272
00:18:53,840 --> 00:18:56,600
again the frequency increases just like

273
00:18:56,610 --> 00:18:58,840
in the case of the order this guy the

274
00:18:58,890 --> 00:19:01,630
breakfast plate

275
00:19:01,630 --> 00:19:02,770
zero z

276
00:19:03,000 --> 00:19:05,790
right but if you sort of right i said that's the thing that makes the

277
00:19:05,790 --> 00:19:07,690
distribution normalize

278
00:19:07,730 --> 00:19:12,250
and so if you had a discrete set of variables would be some if you

279
00:19:12,250 --> 00:19:14,880
had continuous ones it would be an integral

280
00:19:14,880 --> 00:19:19,650
it's just a constant that you get by summing or integrating over all configurations this

281
00:19:19,650 --> 00:19:22,980
product of all the terms on your critiques

282
00:19:22,980 --> 00:19:26,060
so it seems kind of boring but

283
00:19:26,270 --> 00:19:31,040
in any learning problem that thing will be something like the log the likelihood of

284
00:19:31,040 --> 00:19:32,360
your data

285
00:19:32,380 --> 00:19:36,940
take the logo it would be the log likelihood so things like maximum likelihood or

286
00:19:36,940 --> 00:19:43,610
expectation maximisation all of these things care very much the only efficient if you are

287
00:19:43,610 --> 00:19:46,130
able to compute this quantity quickly

288
00:19:46,170 --> 00:19:53,310
so it looks kind of innocuous but it's actually very important

289
00:19:53,340 --> 00:19:57,710
right if we think about the complexity even in the discrete case you can see

290
00:19:57,710 --> 00:19:59,560
it's exponential

291
00:19:59,580 --> 00:20:00,960
right because

292
00:20:00,980 --> 00:20:04,810
even if you just have to states a binary variables every node you have to

293
00:20:04,810 --> 00:20:06,540
the information

294
00:20:06,590 --> 00:20:11,710
so that thing if you think about naive is very hard

295
00:20:11,770 --> 00:20:15,520
we're going to see is that if you are clever about it if you exploit

296
00:20:15,520 --> 00:20:19,420
the graph structure when you've trees then you can do this exactly

297
00:20:19,520 --> 00:20:21,480
you can do it by essentially

298
00:20:21,480 --> 00:20:26,810
it's essentially form of dynamic programming it's known as the max product and the sum

299
00:20:26,810 --> 00:20:29,190
product algorithm that's that's what we'll see

300
00:20:29,190 --> 00:20:34,190
when you have cycles it's actually quite hard to many complexity theoretic results in the

301
00:20:34,190 --> 00:20:35,900
literature that tell you that

302
00:20:35,900 --> 00:20:39,920
it's very unlikely that people are going to be able to compute these sums in

303
00:20:39,920 --> 00:20:43,300
general exactly for general problems

304
00:20:44,010 --> 00:20:47,460
OK so the other problem that you might be interested in is also a summation

305
00:20:47,460 --> 00:20:52,180
or integration problem let me just go back to the examples that we had

306
00:20:52,230 --> 00:21:00,020
right so when we talking about speech recognition or we talking about predicting typewriter sequences

307
00:21:00,020 --> 00:21:04,010
on your on your laptop or your ipad what you'd like to do is you

308
00:21:04,010 --> 00:21:09,260
may be observed all the keystrokes up to some time t and you'd like to

309
00:21:09,260 --> 00:21:12,230
predict the next time t plus one

310
00:21:12,250 --> 00:21:16,180
that's what you'd like to do is marginalize efficiently like to summer integrate over all

311
00:21:16,180 --> 00:21:21,670
these variables to try and predict what the marginal distribution is at that node

312
00:21:21,680 --> 00:21:26,880
it's again a kind of summation problem into summation and integration problems

313
00:21:30,170 --> 00:21:33,210
what's another problem that we might want to solve in this case

314
00:21:42,310 --> 00:21:45,320
how many people know about sequence alignment problems in

315
00:21:45,340 --> 00:21:47,190
computational biology

316
00:21:47,220 --> 00:21:50,980
OK so what kind of problem i want to solve in that area

317
00:22:01,720 --> 00:22:05,440
you could do that

318
00:22:05,460 --> 00:22:08,720
but maybe let's let's be more concrete you might like to do the sequence alignment

319
00:22:08,720 --> 00:22:13,800
problem you might like to search officially overall alignments and match alignments you might like

320
00:22:13,800 --> 00:22:20,430
to find the alignment that maximizes the probability of being matched under some probability model

321
00:22:20,440 --> 00:22:24,420
right so problems like sequence alignment many of them you can formulate them as

322
00:22:24,430 --> 00:22:27,250
you're essentially building a graphical model

323
00:22:27,260 --> 00:22:29,150
over sequences

324
00:22:29,170 --> 00:22:34,590
and you're penalizing certain transitions are seen some transitions are more likely than others

325
00:22:34,600 --> 00:22:38,100
and what you'd like to do it's kind of like a batch computation it's not

326
00:22:38,100 --> 00:22:40,580
an online computation you'd like to search

327
00:22:40,630 --> 00:22:44,860
over all sequences over the entire length of the sequence you'd like to search all

328
00:22:44,860 --> 00:22:46,800
sequences and maximize

329
00:22:46,850 --> 00:22:50,710
i find this sequence that has the highest probability of much of

330
00:22:50,770 --> 00:22:53,420
matching the observed data

331
00:22:54,020 --> 00:22:55,970
right so that's

332
00:22:56,020 --> 00:23:01,440
like this problem i defined its instead of the summation that's a this problem down

333
00:23:02,400 --> 00:23:06,420
in this problem we want to sum over all the variables in this problem we

334
00:23:06,420 --> 00:23:09,170
want to maximize over the the variables

335
00:23:09,210 --> 00:23:14,130
so there's sort of two different types of problems those that involve summations and those

336
00:23:14,130 --> 00:23:16,720
that involve maximizations

337
00:23:16,770 --> 00:23:20,800
and it sort of depends on your application sometimes you might want to compute the

338
00:23:20,800 --> 00:23:22,100
likelihood of data

339
00:23:22,220 --> 00:23:27,050
or you might want to compute marginal distribution to make predictions in other times you

340
00:23:27,050 --> 00:23:31,390
might want to search the whole space to find the best alignment or for computer

341
00:23:31,390 --> 00:23:36,490
the following content is provided under a Creative Commons license your support will help MIT

342
00:23:36,490 --> 00:23:40,790
OpenCourseWare continue to offer high-quality educational resources for free

343
00:23:41,280 --> 00:23:45,600
to make a donation or to view additional materials from hundreds of MIT courses

344
00:23:46,120 --> 00:23:50,820
visit MIT OpenCourseWare at ocw . MIT . EDU

345
00:23:51,340 --> 00:23:55,030
OK let's get started

346
00:23:55,120 --> 00:23:58,890
settle down a couple of announcements to make

347
00:23:59,190 --> 00:24:03,690
and it's with great sadness that we announce there will be no lecture on Monday

348
00:24:05,190 --> 00:24:07,150
no 5

349
00:24:08,360 --> 00:24:12,340
with all the kleenex consumed the Kimberly-Clark went up 2 points on the New York

350
00:24:12,340 --> 00:24:20,600
Stock Exchange religious freedom interview the timing of of where we are in the lectures

351
00:24:20,600 --> 00:24:26,880
versus the whole of homework and there's no opportunity to interact with the recitation instructors

352
00:24:26,880 --> 00:24:31,140
on Monday we're doing just this 1 time when the slide that the weekly quiz

353
00:24:31,150 --> 00:24:40,220
over the Thursday that gives her time for people to contact their recitation structure and

354
00:24:40,430 --> 00:24:45,760
what's going on with the lessons about I'd perhaps some of you haven't figured out

355
00:24:45,760 --> 00:24:50,640
that the lecture has started and I still here talking and I invite people to

356
00:24:50,640 --> 00:24:53,780
leave if they insist on

357
00:24:57,940 --> 00:25:01,240
we up

358
00:25:05,620 --> 00:25:11,080
so last day we talked about secondary bonding and we looked at the various forms

359
00:25:11,080 --> 00:25:17,380
of secondary bonding here's the cartoons dipole-dipole interactions dipole induced dipole and solutions a little

360
00:25:17,380 --> 00:25:23,220
bit of a divergent induced dipole induced dipole which was the London dispersion forces our

361
00:25:23,220 --> 00:25:29,500
vendor evolves bonds and also hydrogen bonds and all of these helped us answer the

362
00:25:29,500 --> 00:25:33,520
question what's the state of aggregation reason why ROC of aggregation is that this is

363
00:25:33,520 --> 00:25:38,800
solid state chemistry we wanna when something is a solid so this helps us get

364
00:25:38,810 --> 00:25:43,100
to that conclusion and then we came to the point where realize that three-quarters the

365
00:25:43,100 --> 00:25:50,180
periodic table wasn't covered by the either are ionic bonding covalent bonding or

366
00:25:50,200 --> 00:25:53,610
vendor balls bonding is primary form a bonding so we had to come up with

367
00:25:53,610 --> 00:25:58,950
something else something else was metallic bonding we saw appalled through of early attempt in

368
00:25:58,950 --> 00:26:05,830
1900 to model metals as a gas of free electrons moving about course where the

369
00:26:05,830 --> 00:26:10,770
iron core in fact is a mix of the nucleus plus all the inner shell

370
00:26:11,990 --> 00:26:17,300
and recognize that didn't take us very far so we wanted to continue the discussion

371
00:26:17,300 --> 00:26:22,330
today we're going to look at this and the next installment which took place both

372
00:26:22,350 --> 00:26:27,650
30 years later as part of the quantum revolution and is really to benchmarks here

373
00:26:28,040 --> 00:26:30,800
the first one is

374
00:26:30,880 --> 00:26:38,140
Felix Bloch Felix Bloch and display modern 1928

375
00:26:38,400 --> 00:26:40,960
this was his PhD thesis

376
00:26:40,980 --> 00:26:45,600
this is his PhD thesis we for his PhD thesis which was done under the

377
00:26:45,600 --> 00:26:52,890
supervision of heisenberg that Leipzig took on an interesting problem so far using quantum theory

378
00:26:52,890 --> 00:26:54,920
applied to gasses right

379
00:26:54,930 --> 00:27:00,420
the lighter gasses bought talked talked about atomic hydrogen atomic hydrogen

380
00:27:00,760 --> 00:27:03,500
you know the complexities of condensed matter

381
00:27:03,820 --> 00:27:07,120
blocks PhD thesis on quantum theory of solid

382
00:27:07,740 --> 00:27:14,760
let's take quantum theory applied to solids not bad for an open and quantum theory

383
00:27:14,760 --> 00:27:26,560
of solids with his PhD at year's what he did you can follow the derivations

384
00:27:26,560 --> 00:27:32,060
but it's quite simple firstly recognizes the atoms in a metal are arranged in a

385
00:27:32,180 --> 00:27:41,560
periodic position there's an array not just thrown together in some jumbled so Adams Adams

386
00:27:41,560 --> 00:27:48,060
and solid are on sat in regular

387
00:27:48,280 --> 00:27:57,480
patterns rule learning next week arranged in such solids were the atoms were found in

388
00:27:57,480 --> 00:28:04,300
regular patterns are called crystals crystals so he took that as 1 the benchmark the

389
00:28:04,300 --> 00:28:09,180
2nd thing he says that the Member through says that the valence electrons free to

390
00:28:09,180 --> 00:28:14,390
move around this is what if if these valence electrons were free to move around

391
00:28:14,580 --> 00:28:19,780
what would they be subjected to they would be subjected to a potential this is

392
00:28:19,780 --> 00:28:24,100
how we derive the energy levels and even a single hydrogen atom that the positive

393
00:28:24,100 --> 00:28:29,380
charge the court that the negative charge electrodes and gonna electron and sees the net

394
00:28:29,380 --> 00:28:33,820
positive charge of the atomic for but it's sees an array

395
00:28:33,830 --> 00:28:35,030
this season

396
00:28:35,080 --> 00:28:39,120
periodic variation potential so that's the 2nd thing he says

397
00:28:39,220 --> 00:28:52,590
the periodic periodic potential periodic potential impacts on the valence electrons

398
00:28:54,640 --> 00:29:03,770
and then invokes wave mechanics involves wave mechanics remember Schrodinger's equation was only enunciated a

399
00:29:03,790 --> 00:29:05,580
couple years earlier 1926

400
00:29:06,540 --> 00:29:12,540
is a PhD student years about the Schrodinger equation as applied to engage Adams and

401
00:29:12,540 --> 00:29:13,720
he's applying it to

402
00:29:13,890 --> 00:29:15,450
condensed matter

403
00:29:15,470 --> 00:29:28,680
solves the Schrödinger equation with these constraints but and what is again get he gets

404
00:29:28,680 --> 00:29:36,790
a set of solutions he gets a family of solutions but the family of solutions

405
00:29:37,180 --> 00:29:39,830
and these give rise to a set of

406
00:29:39,890 --> 00:29:47,210
wavelength of the electron that could move quickly through the crystal they could propagate in

407
00:29:47,210 --> 00:29:52,140
other words by invoking the wave like properties of the electron he can rationalize how

408
00:29:52,140 --> 00:29:58,330
these valence electrons can move through barriers that classical physics would impose

409
00:29:58,510 --> 00:30:08,620
and so pretty soon of false electronic conductivity electronic conductivity so this is pretty good

410
00:30:08,620 --> 00:30:11,470
for a further opening

411
00:30:12,140 --> 00:30:19,940
volley and therefore PhD and he eventually to constant faculty positions and then in the

412
00:30:19,940 --> 00:30:25,240
1930's when the nazis came to power block as did many of the scientists from

413
00:30:25,240 --> 00:30:30,150
europe left and came to the United States and settled at Stanford and eventually he

414
00:30:30,150 --> 00:30:34,140
won a Nobel Prize but not for this work this this was OK we got

415
00:30:34,140 --> 00:30:41,530
Nobel Prize in 1952 for work started in 1945 and had to do with the

416
00:30:42,120 --> 00:30:45,030
magnetic moments in the nucleus

417
00:30:45,290 --> 00:30:48,240
so you have more than 1 good idea so that's the 1st thing we need

418
00:30:48,270 --> 00:30:53,700
to think about how I in the band theory the 2nd was pair gentleman walter

419
00:30:53,700 --> 00:31:02,710
height Walter height and in the same manner from last lecture Fritz London this was

420
00:31:02,710 --> 00:31:08,260
before he came up with the dispersion idea the dispersion bonding was 1930 but he

421
00:31:08,890 --> 00:31:16,470
and height were decided to go in 1927 desserts and spend a year post talking

422
00:31:16,470 --> 00:31:21,950
with training will did they know the trading your had negotiated a position of Berlin

423
00:31:22,200 --> 00:31:27,760
and names on they know new no soon arrived then the shredding left but anyways

424
00:31:27,760 --> 00:31:32,950
that's what brought them to right and they decided let's take this idea of linear

425
00:31:32,950 --> 00:31:40,010
combination of atomic orbitals and instead asking LinkedIn questions like is lithium 2 stable is

426
00:31:40,010 --> 00:31:43,840
so this is advantageous for property standpoint but

427
00:31:43,860 --> 00:31:47,220
it's very disadvantageous from the standpoint of

428
00:31:47,240 --> 00:31:51,780
recycling how could one possibly recycled this material

429
00:31:51,820 --> 00:31:52,940
let's compare

430
00:31:52,960 --> 00:31:54,070
these two

431
00:31:54,090 --> 00:31:55,980
let's compare the case of

432
00:31:57,690 --> 00:32:00,480
so polyethylene consists of

433
00:32:00,550 --> 00:32:02,960
many chains and they will

434
00:32:03,010 --> 00:32:05,440
into penetrates and entangled

435
00:32:05,440 --> 00:32:07,820
what are the bonds between these chains

436
00:32:07,840 --> 00:32:09,990
the bonds between these chains are

437
00:32:10,010 --> 00:32:11,300
van der waals

438
00:32:11,300 --> 00:32:13,220
this is the van der waals

439
00:32:13,280 --> 00:32:17,090
these are van der waals bonds and we know that animals bonds are operative at

440
00:32:17,090 --> 00:32:22,240
low temperatures but not operative high temperatures so if we take polyethylene and heated

441
00:32:22,240 --> 00:32:24,920
these bonds where we can and this will turn liquid

442
00:32:24,940 --> 00:32:26,720
and we can be reprocess this

443
00:32:26,740 --> 00:32:28,900
so by

444
00:32:28,960 --> 00:32:31,400
going to high temperature

445
00:32:31,440 --> 00:32:34,280
you can reprocess

446
00:32:34,300 --> 00:32:35,990
because we're only breaking these

447
00:32:36,010 --> 00:32:42,190
weak van der waals bonds now if we take this crosslinked

448
00:32:45,030 --> 00:32:46,860
polly i supreme

449
00:32:46,900 --> 00:32:49,380
crosslink collie i supreme

450
00:32:49,400 --> 00:32:51,630
show it in this manner

451
00:32:51,650 --> 00:32:55,280
how are we going to reprocess this if we if we begin to heat this

452
00:32:55,530 --> 00:32:58,460
we're going to have to go to an extremely high temperature because we have to

453
00:32:58,460 --> 00:33:00,510
break covalent bonds

454
00:33:00,530 --> 00:33:03,440
well if we go to a temperature high enough to break these bonds

455
00:33:03,490 --> 00:33:07,150
we got a temperature high enough to break the backbone we go to thermal destruction

456
00:33:07,150 --> 00:33:08,630
of the entire material

457
00:33:08,650 --> 00:33:14,880
so such materials cannot be reprocessed by heating so these are called thermo set

458
00:33:14,940 --> 00:33:16,170
for most set

459
00:33:16,320 --> 00:33:18,880
for most of

460
00:33:19,030 --> 00:33:22,610
thermal said are very difficult to process

461
00:33:22,610 --> 00:33:24,130
difficult to

462
00:33:25,380 --> 00:33:30,670
you i say well you don't have to just do it by by thermally reprocessing

463
00:33:30,670 --> 00:33:32,940
why don't you use some kind of chemistry

464
00:33:32,960 --> 00:33:37,760
going there with chemistry that will attack these bonds these are strong covalent bonds what

465
00:33:37,760 --> 00:33:38,880
kind of

466
00:33:38,900 --> 00:33:42,920
solving solvent is going to be strong enough to kill a snip those bonds is

467
00:33:42,920 --> 00:33:46,210
going to be very aggressive solvent and

468
00:33:46,220 --> 00:33:49,150
one of the health effects of the use of that's all you want about the

469
00:33:49,150 --> 00:33:53,710
company and the use of that and find out later that you've got class-action lawsuits

470
00:33:53,710 --> 00:33:58,320
using such chemicals so this is the issue when you drive down the highway

471
00:33:58,320 --> 00:34:05,090
and you see these mounds these mountains of spent automobile tires how to recycle

472
00:34:05,130 --> 00:34:06,380
this is the issue

473
00:34:06,510 --> 00:34:10,340
whereas over here you've made this up and you can redo it so such materials

474
00:34:10,340 --> 00:34:13,170
are called thermal plastic

475
00:34:13,220 --> 00:34:14,900
thermal plastic

476
00:34:14,900 --> 00:34:19,550
they can be reprocessed or rendering plastic by

477
00:34:19,550 --> 00:34:26,920
the temperature rise so they're environmentally much much easier to process

478
00:34:26,980 --> 00:34:29,490
a few more things here with the polyethylene

479
00:34:29,530 --> 00:34:32,220
this shows what i was doing on the

480
00:34:32,240 --> 00:34:33,650
on the projector

481
00:34:34,380 --> 00:34:39,190
the document camera rather this is the partially crystallized polyethylene and this is what it

482
00:34:39,190 --> 00:34:41,820
looks like in the zones you don't have

483
00:34:41,840 --> 00:34:43,150
all of the

484
00:34:43,170 --> 00:34:47,400
change lining up but if you look at the atoms on and you will see

485
00:34:47,400 --> 00:34:51,650
there's this c two h four units in the c two h four units are

486
00:34:51,650 --> 00:34:52,780
lined up

487
00:34:53,550 --> 00:34:57,530
and sure enough what is the basis unit here

488
00:34:57,530 --> 00:35:01,050
it looks author on the as you would expect the molecule that has the aspect

489
00:35:01,050 --> 00:35:04,150
ratio of two to one you expect is going to come out in one of

490
00:35:04,150 --> 00:35:07,480
the brevity lattices that also has

491
00:35:07,490 --> 00:35:10,400
some and i start to be associated with it

492
00:35:10,440 --> 00:35:12,840
and here's an x-ray diffraction data

493
00:35:12,840 --> 00:35:13,880
this is the

494
00:35:13,900 --> 00:35:18,610
fraction of a fully crystalline polymers of we took a short length

495
00:35:19,960 --> 00:35:23,440
and called it very slowly or took it out of the solvent we would end

496
00:35:23,440 --> 00:35:25,940
up with something that's totally crystallin we see

497
00:35:25,960 --> 00:35:27,900
we have a discrete peaks

498
00:35:27,940 --> 00:35:33,190
here is the same material amorphous and this fits all the painful to look at

499
00:35:33,190 --> 00:35:38,090
this because it's reminiscent of a question on the recent test this is what the

500
00:35:38,090 --> 00:35:40,010
this is what the data shows

501
00:35:40,050 --> 00:35:43,510
so by looking at this

502
00:35:43,740 --> 00:35:49,110
mixed spectrum you can get a measure of the extent of crystallisation by looking at

503
00:35:49,110 --> 00:35:55,510
the relative intensity of the peaks versus this broad single peak which is associated with

504
00:35:55,570 --> 00:35:57,170
the first nearest neighbours

505
00:35:57,170 --> 00:36:01,650
because an ethylene we know the carbon has hydrogens nearest neighbors

506
00:36:01,690 --> 00:36:02,780
and so on

507
00:36:02,780 --> 00:36:08,300
so this works just as well as the the free volume as a measure of

508
00:36:08,300 --> 00:36:09,760
the degree of

509
00:36:12,220 --> 00:36:13,650
so we see this

510
00:36:13,780 --> 00:36:19,030
the last thing i want to touch upon is the polymer synthesis just to put

511
00:36:19,030 --> 00:36:20,090
that into

512
00:36:23,570 --> 00:36:27,780
setting i've already shown you addition polymerization

513
00:36:27,800 --> 00:36:32,590
in addition polymerization

514
00:36:34,190 --> 00:36:37,760
in addition polymerization involves the use of

515
00:36:40,090 --> 00:36:41,980
radical or

516
00:36:41,980 --> 00:36:47,170
time searching around the neighborhoods or outline

517
00:36:47,260 --> 00:36:53,270
the motorbike which solves the segmentation cont problem completely for the the learner makes life

518
00:36:53,270 --> 00:36:59,430
easier or if we know what describes motorbikes well we came and put them

519
00:36:59,480 --> 00:37:03,830
the location of the most important features so how much are you willing to say

520
00:37:03,830 --> 00:37:05,340
to the algorithm

521
00:37:05,350 --> 00:37:07,540
it also depends how flexible

522
00:37:07,550 --> 00:37:12,280
your model is how you know what is the the cost of human labor and

523
00:37:12,290 --> 00:37:13,640
so on

524
00:37:13,660 --> 00:37:18,880
of course there is the issue of batch one versus incremental learning it's all good

525
00:37:18,880 --> 00:37:24,170
we have all the data available but in many cases like surveillance and so on

526
00:37:24,440 --> 00:37:29,800
that we don't have that luxury or when we simply have too many images we

527
00:37:29,800 --> 00:37:37,120
want to have efficient way of learning including the incremental methods we also have been

528
00:37:37,120 --> 00:37:42,430
in training images in from a statistical point of view we have issues of over

529
00:37:42,430 --> 00:37:47,210
fitting be very careful of that because what you see and the learning is the

530
00:37:47,210 --> 00:37:53,410
noise of the training rather than the general nature of the object category and then

531
00:37:53,410 --> 00:38:00,470
of course negative example is not a trivial issue at all what is the negative

532
00:38:00,930 --> 00:38:03,380
of a motor bike so

533
00:38:03,380 --> 00:38:08,920
and last we have the issue of prior prior will talk more about it but

534
00:38:08,920 --> 00:38:14,010
it's a very interesting it's a very interesting concept in terms of

535
00:38:14,150 --> 00:38:16,510
knowledge and learning

536
00:38:16,520 --> 00:38:18,920
so k recognition

537
00:38:18,940 --> 00:38:20,120
is were

538
00:38:20,140 --> 00:38:26,180
no you have learned model you design the model now that the anomaly image comes

539
00:38:26,850 --> 00:38:32,290
you're looking for the object and so in recognition there their speed is the main

540
00:38:32,290 --> 00:38:36,110
issue because if you want to put this into robots are real

541
00:38:36,140 --> 00:38:39,350
world systems you don't want to the

542
00:38:39,360 --> 00:38:40,900
the machinery to be

543
00:38:40,920 --> 00:38:45,920
to be cranking for ten hours in in order to recognise one thing so

544
00:38:45,940 --> 00:38:49,180
do you want to search all over the space

545
00:38:49,190 --> 00:38:56,240
for that object or do you want to have more efficient search over scale location

546
00:38:56,240 --> 00:39:01,370
to find objects so these are issues one should keep in mind OK so

547
00:39:02,610 --> 00:39:08,330
so i've spent my first half of the lectures setting up all the other the

548
00:39:08,380 --> 00:39:13,990
problems and give you a taste of of what object recognition is is involved in

549
00:39:13,990 --> 00:39:17,830
fact if you go home with this idea of

550
00:39:17,850 --> 00:39:19,510
these three major

551
00:39:19,520 --> 00:39:27,480
questions in object recognition and categorisation representation learning and testing be very happy so from

552
00:39:27,480 --> 00:39:35,700
now on we're gonna go into specially toulmin generative models that have been used in

553
00:39:35,700 --> 00:39:37,330
in the domain of

554
00:39:37,350 --> 00:39:39,330
visual recognition

555
00:39:39,340 --> 00:39:44,530
so the first one i term that i termed it as bag of words models

556
00:39:46,630 --> 00:39:52,120
most of the early bag of words models are related to texture and

557
00:39:52,130 --> 00:39:56,020
here are just some references for you so what does

558
00:39:56,030 --> 00:40:02,050
that's of words model meaning or in a very very intuitive way what we're doing

559
00:40:02,050 --> 00:40:08,570
is we take objects and we represent it in some kind of collection

560
00:40:09,490 --> 00:40:11,440
the building blocks

561
00:40:11,460 --> 00:40:18,160
of these objects so i specifically ask when will these building blocks to emphasise the

562
00:40:18,160 --> 00:40:26,110
fact that most of the bag of words models do not have coherent geometric information

563
00:40:27,180 --> 00:40:33,170
why do we even think was is possible to describe objects we actually got inspiration

564
00:40:33,170 --> 00:40:41,090
from the text document community where if i show you document like this kind of

565
00:40:41,130 --> 00:40:45,820
a lot to read but if i tell you there are some words like sensory

566
00:40:45,820 --> 00:40:52,310
brain visual cortex and so on despite knowing these keywords are building blocks you get

567
00:40:52,320 --> 00:40:57,030
a feeling of which class of document it comes from must be some kind of

568
00:40:57,030 --> 00:41:01,780
scientific document about probably the visual system

569
00:41:01,790 --> 00:41:05,830
the same thing goes for a second document where i tell you it includes things

570
00:41:05,830 --> 00:41:12,310
like import-export trading you know it's probably some kind of business analysis or business news

571
00:41:12,710 --> 00:41:17,650
so that means building blocks even without

572
00:41:17,670 --> 00:41:26,570
the coherant geometric or in this case ordering information syntactic information gives you a lot

573
00:41:26,570 --> 00:41:27,560
of meaning

574
00:41:27,560 --> 00:41:34,350
two hundred documents and we believe the same is so for visual visual objects so

575
00:41:34,350 --> 00:41:41,770
here the basic representation would be you take objects different classes of objects you somehow

576
00:41:41,770 --> 00:41:45,850
find a dictionary of

577
00:41:45,860 --> 00:41:54,190
these building blocks you represent each class of these objects by this dictionary so

578
00:41:54,210 --> 00:42:01,340
i'm gonna go through a more detailed algorithm for for a couple of case studies

579
00:42:01,620 --> 00:42:08,120
of different models from this bag of words object categories so we'll start with some

580
00:42:08,120 --> 00:42:11,040
kind of learning with different classes of objects

581
00:42:11,490 --> 00:42:13,380
and now we want to do

582
00:42:13,390 --> 00:42:20,080
this is where the representation goes remember our our three issues so we will show

583
00:42:20,080 --> 00:42:25,420
you how we represent images as bag of words and i will look at learning

584
00:42:25,420 --> 00:42:30,820
really what the model is and how we learn that that and we want to

585
00:42:30,820 --> 00:42:37,370
give taken although image and represented in the same way as all the training images

586
00:42:37,370 --> 00:42:43,170
and make a decision based on the classifiers we have learned so that the overall

587
00:42:45,110 --> 00:42:51,350
and will look at the representation issues first so there are three steps in representation

588
00:42:51,430 --> 00:42:56,530
how do we get these building blocks how do we express them

589
00:42:59,010 --> 00:43:00,980
i in

590
00:43:00,980 --> 00:43:03,470
that's quite a according the same question

591
00:43:03,490 --> 00:43:04,490
and so it

592
00:43:04,500 --> 00:43:07,730
that itself to use in mixed method techniques

593
00:43:09,450 --> 00:43:12,840
most compatible

594
00:43:12,890 --> 00:43:14,870
most aligned

595
00:43:15,640 --> 00:43:16,810
is when you

596
00:43:16,850 --> 00:43:20,550
both questions both the quality separate them out

597
00:43:20,550 --> 00:43:22,940
both open ended and non directional

598
00:43:22,950 --> 00:43:24,810
that that's really

599
00:43:24,900 --> 00:43:28,040
combination the works really well

600
00:43:28,340 --> 00:43:31,740
so you might have a descriptive research design

601
00:43:31,780 --> 00:43:35,860
and almost any of the qualitative research designs combined together

602
00:43:35,960 --> 00:43:38,030
so example here would be

603
00:43:38,070 --> 00:43:40,320
what's implications of the global recession

604
00:43:40,330 --> 00:43:42,500
on the ability of countries and states

605
00:43:42,500 --> 00:43:44,310
to address global warming

606
00:43:45,190 --> 00:43:48,660
so in that situation there you've got the all the all the facing in the

607
00:43:48,660 --> 00:43:52,670
open so whatever quantity that you cannot be more and quality there will be more

608
00:43:52,670 --> 00:43:58,290
open ended and you put together you get an open-ended situation OK

609
00:43:59,820 --> 00:44:01,940
what design could be collective case study

610
00:44:01,980 --> 00:44:03,390
and that

611
00:44:03,440 --> 00:44:06,570
subsumes mix of design is another thing you can do

612
00:44:06,650 --> 00:44:09,250
and that's called a better design

613
00:44:09,710 --> 00:44:12,550
and in tuchikoi because

614
00:44:12,660 --> 00:44:14,040
fact a

615
00:44:14,050 --> 00:44:15,770
and it all

616
00:44:15,780 --> 00:44:19,320
we identified its ties to me because you can have

617
00:44:19,370 --> 00:44:21,650
so the first one first type

618
00:44:21,660 --> 00:44:25,130
you have set foot quantitative and qualitative questions

619
00:44:26,260 --> 00:44:28,640
and typically you might

620
00:44:28,700 --> 00:44:31,600
the question might be about how you integrate it

621
00:44:32,720 --> 00:44:37,890
for example the quantitative results in quantifiers there might be a question of interest

622
00:44:37,940 --> 00:44:40,690
second you might have a hybrid question

623
00:44:40,730 --> 00:44:42,270
but what you said earlier

624
00:44:42,290 --> 00:44:46,950
which might lead to a breakdown in the quantitative and qualitative questions

625
00:44:52,590 --> 00:44:57,520
we do with this give example of the second question two extended interpretations much defaming

626
00:44:57,520 --> 00:45:00,960
found at the two thousand four u s presidential election

627
00:45:00,970 --> 00:45:05,380
OK how companies can be framed to better engage college students

628
00:45:05,400 --> 00:45:08,510
so that might be a question that fits into the second time

629
00:45:09,600 --> 00:45:12,750
the question is evolving different phases of the study

630
00:45:12,750 --> 00:45:16,850
so as you do the study they just ignore the questions involved

631
00:45:17,810 --> 00:45:21,910
so the so called to touch going and teddlie

632
00:45:21,960 --> 00:45:25,790
they raise questions about whether only quite a quantification should be written

633
00:45:25,800 --> 00:45:29,130
or whether you should have a single mixed methods question that you saw earlier

634
00:45:29,200 --> 00:45:33,760
or single mix question transcends the quality and quantity some questions

635
00:45:34,460 --> 00:45:37,290
so they have a few recommendations first

636
00:45:37,380 --> 00:45:41,820
mixed methods studies need atleast one explicit formulated mixed methods question objective about the nature

637
00:45:43,780 --> 00:45:45,460
and that question

638
00:45:46,810 --> 00:45:49,830
quite a tighter questions that led to the specific needs

639
00:45:50,010 --> 00:45:52,910
in answer to questions

640
00:45:52,960 --> 00:45:54,090
OK second

641
00:45:54,120 --> 00:45:56,810
so to benefit from these ones

642
00:45:56,850 --> 00:45:59,320
a large and makes specific question

643
00:46:00,610 --> 00:46:04,470
and it gives you the possibility of softball questions

644
00:46:05,690 --> 00:46:10,850
the natural form of the question might be quite different in been sequential

645
00:46:12,310 --> 00:46:15,550
so that's kind of this the quick tour through research questions

646
00:46:15,570 --> 00:46:17,450
six is often design

647
00:46:17,520 --> 00:46:21,090
and this is quite complex if you want to have a design that you know

648
00:46:21,090 --> 00:46:22,420
you are able to

649
00:46:22,480 --> 00:46:24,610
to make sense of

650
00:46:24,660 --> 00:46:29,940
so we often have some force that typology so we say that a random sample

651
00:46:29,950 --> 00:46:31,810
in its qualitative

652
00:46:31,970 --> 00:46:34,190
none sampling in qualitative

653
00:46:34,330 --> 00:46:36,790
but that's not strictly true

654
00:46:36,870 --> 00:46:40,520
they're tighter generalizations external statistical

655
00:46:40,520 --> 00:46:42,100
intelligence statistical

656
00:46:42,120 --> 00:46:44,780
a if you have hours to external

657
00:46:44,820 --> 00:46:48,140
that's when you go from the sample to the population so people say about generalizing

658
00:46:48,140 --> 00:46:52,370
that's the big what they mean you go for from the sample to the population

659
00:46:52,510 --> 00:46:55,220
another type that we don't often talk about this much

660
00:46:55,230 --> 00:46:59,810
his intelligence is categorisation so you have a mate may be doing qualitative study

661
00:46:59,870 --> 00:47:01,110
and you have

662
00:47:01,160 --> 00:47:02,560
a sample

663
00:47:02,660 --> 00:47:04,860
you have what's called key informants

664
00:47:04,900 --> 00:47:07,090
and from there you go from the key informants

665
00:47:07,100 --> 00:47:09,320
to the population

666
00:47:09,370 --> 00:47:10,750
then you have

667
00:47:10,750 --> 00:47:12,620
and it's good jazz asian

668
00:47:13,290 --> 00:47:15,910
and that's where is very common in qualitative

669
00:47:15,970 --> 00:47:17,820
well you see how it fits

670
00:47:18,160 --> 00:47:20,970
so when you say and the themes where

671
00:47:21,010 --> 00:47:24,470
you making in some kind of liquid risation k

672
00:47:24,520 --> 00:47:27,050
OK so case stands you might be the case study

673
00:47:27,070 --> 00:47:28,710
you go one by one

674
00:47:28,710 --> 00:47:30,090
in the case

675
00:47:30,110 --> 00:47:32,780
two major innovations case-by-case

676
00:47:32,780 --> 00:47:35,090
the naturalistic state

677
00:47:35,150 --> 00:47:36,360
points out

678
00:47:36,380 --> 00:47:38,390
it's where you the reader

679
00:47:38,410 --> 00:47:41,990
makes generalizations once you read the article from the findings to those of phi beta

680
00:47:42,040 --> 00:47:44,320
site so we we've identified

681
00:47:44,370 --> 00:47:46,480
so when you have a mixed methods study

682
00:47:46,480 --> 00:47:48,110
you essentially have

683
00:47:48,130 --> 00:47:50,580
at least two phases have a quantitative phase

684
00:47:50,590 --> 00:47:52,420
and the qualitative phase

685
00:47:53,750 --> 00:47:56,860
those faces i have some combination of random sampling could be

686
00:47:56,880 --> 00:47:58,510
random on random

687
00:47:59,530 --> 00:48:03,720
and the most common the most common type you have combinations

688
00:48:03,770 --> 00:48:07,090
is that type four which were both in non random

689
00:48:07,160 --> 00:48:09,310
because researchers have found that

690
00:48:09,320 --> 00:48:12,550
in the qualitative what about ninety five percent studies

691
00:48:12,560 --> 00:48:14,350
the social with sizes

692
00:48:14,400 --> 00:48:19,410
use non random sampling and five ten years opposite that's the goal

693
00:48:19,420 --> 00:48:22,630
most quantitative research about majorisation

694
00:48:22,670 --> 00:48:26,190
and you look almost every information technique has that an assumption

695
00:48:26,190 --> 00:48:27,740
it's typically hard to do

696
00:48:30,590 --> 00:48:34,480
the political world is probably probably have more instances that because you're dealing with

697
00:48:34,540 --> 00:48:37,690
a lot i you may may involve polling

698
00:48:37,740 --> 00:48:40,600
we able to do that but in many other fields is very difficult to defend

699
00:48:41,630 --> 00:48:43,580
true sense

700
00:48:43,580 --> 00:48:49,680
at where is the sense of communication and interaction start in terms of not only

701
00:48:49,700 --> 00:48:55,690
according to to the social emotional side and then you could robot potentially bridges with

702
00:48:55,690 --> 00:48:57,680
people to help stop its own

703
00:48:57,700 --> 00:49:00,150
cognitive development that was really the inspiration

704
00:49:00,780 --> 00:49:04,250
because is still an idea that was driving lot of the work and social robots

705
00:49:04,330 --> 00:49:08,310
or developmental approach something to show you a video of this robot in action with

706
00:49:08,310 --> 00:49:10,610
the person

707
00:49:11,690 --> 00:49:15,360
can work

708
00:49:16,170 --> 00:49:23,770
OK and with this work list for this

709
00:49:31,300 --> 00:49:42,750
and then show you

710
00:49:42,770 --> 00:49:44,050
we're going to get there

711
00:49:57,080 --> 00:49:58,200
believe that

712
00:49:58,210 --> 00:50:03,730
OK thank you for

713
00:50:03,740 --> 00:50:05,660
very much

714
00:50:05,670 --> 00:50:11,020
he all along

715
00:50:11,070 --> 00:50:14,340
so long some

716
00:50:14,440 --> 00:50:15,440
kind of

717
00:50:34,370 --> 00:50:36,830
one one

718
00:50:41,840 --> 00:50:44,070
i don't want to

719
00:50:50,230 --> 00:50:55,730
one two

720
00:50:57,270 --> 00:51:06,500
so which is particularly good video i think

721
00:51:06,520 --> 00:51:09,820
but what you want to see if this was really kind of our first attempt

722
00:51:09,820 --> 00:51:11,200
at this and so

723
00:51:11,240 --> 00:51:14,070
one of the things that we learn about this was that it was really

724
00:51:14,120 --> 00:51:17,560
provocative and raises questions in terms of

725
00:51:17,580 --> 00:51:20,880
you know what kind of creature we so to speak that we are so willing

726
00:51:20,880 --> 00:51:24,040
and able to almost effortless effortlessly going to this kind of

727
00:51:24,050 --> 00:51:27,070
very interpersonal interaction with something that obviously

728
00:51:27,080 --> 00:51:30,450
just to machine i mean pulling sort of social cues like i think i of

729
00:51:30,560 --> 00:51:34,020
but it is not trying to hide the fact that this machine

730
00:51:34,270 --> 00:51:38,850
so i decided that's what kind of machine we need to build to really be

731
00:51:38,850 --> 00:51:42,730
able to participate in an engaging sort of interactions with humans and of course the

732
00:51:42,730 --> 00:51:45,110
scientific question what could be learned

733
00:51:45,340 --> 00:51:49,620
about ourselves to the positive actually trying to create these sorts machines and of course

734
00:51:49,620 --> 00:51:51,100
the final thrust would be

735
00:51:51,120 --> 00:51:56,400
given abilities in machine learning disabilities what kinds of new applications like that enable

736
00:51:56,450 --> 00:51:59,570
to help improve quality of life for people all over the world so this is

737
00:51:59,570 --> 00:52:03,360
the question that motivates my work in the personal robotics

738
00:52:03,410 --> 00:52:06,010
to go back to the project

739
00:52:06,070 --> 00:52:10,160
so i think one of the things that's very intriguing about robotics is when you

740
00:52:10,160 --> 00:52:11,700
think about humans

741
00:52:11,710 --> 00:52:15,680
we're starting to appreciate in science that we are really you know our behavior it's

742
00:52:15,690 --> 00:52:20,230
very complex is first gets at the intersection multiple facets or which are shown here

743
00:52:20,230 --> 00:52:23,970
which i think also share strong energy provided to the fact that we used to

744
00:52:23,970 --> 00:52:29,250
think that emotion cognition with very distinctive was cool rationality and there was hot motion

745
00:52:29,260 --> 00:52:34,340
but now we're finding that actually motion honest deeply intertwined then that can be intelligent

746
00:52:34,490 --> 00:52:38,020
certain motion systems are functioning properly we should think of as being the might be

747
00:52:38,020 --> 00:52:39,340
distinct from the body

748
00:52:39,350 --> 00:52:42,500
but now to development and so forth the finding the course manager told that we

749
00:52:42,500 --> 00:52:46,150
have is very intimately connected to the embodiment that we have and of course were

750
00:52:46,150 --> 00:52:50,190
profound social species so i think one thing that's exciting about robotics is as the

751
00:52:50,190 --> 00:52:53,160
technology perhaps better than any other technology today

752
00:52:53,180 --> 00:52:56,990
and really engage us and all of these different facets all these differences and i

753
00:52:56,990 --> 00:52:58,780
think that really opened up new

754
00:52:59,020 --> 00:53:02,250
opportunities in your questions to explore

755
00:53:03,030 --> 00:53:07,380
the big i think better for change with a kind of paradigm change that a

756
00:53:07,380 --> 00:53:11,280
lot of social robotics which are pushing that is in the past really robots more

757
00:53:11,280 --> 00:53:14,100
so toward tool that we can use to do things for us that we can

758
00:53:14,100 --> 00:53:15,690
do task far from us

759
00:53:15,710 --> 00:53:19,010
and of course new sort of work is posing this question

760
00:53:19,030 --> 00:53:24,170
what if robots can be partners who can do things with collaborators as and i

761
00:53:24,890 --> 00:53:28,080
this is the opening of all these questions in terms of how do you need

762
00:53:28,140 --> 00:53:32,830
human human bodies robot bodies and robert mines

763
00:53:32,840 --> 00:53:36,170
so that the fundamental question what i want to touch impact that with you the

764
00:53:36,170 --> 00:53:37,240
rest of this talk

765
00:53:37,370 --> 00:53:42,260
so morning my my subordinating bodies we know that from psychology

766
00:53:42,280 --> 00:53:44,600
we know from things such as the room by the back

767
00:53:44,760 --> 00:53:49,800
four people were project mental states to understand and explain this behavior even though it's

768
00:53:49,800 --> 00:53:50,770
clearly not

769
00:53:50,820 --> 00:53:54,540
having either any sort and features and it turns out that even for humans is

770
00:53:54,540 --> 00:53:57,620
is very low level features that when they move in a proper way this is

771
00:53:57,630 --> 00:54:01,890
the famous that i have a little showing that even just moving shapes on the

772
00:54:01,890 --> 00:54:05,630
screen they move an appropriate way that suggests chasing and fleeing

773
00:54:05,690 --> 00:54:10,480
people don't describe the shapes in terms of physics in terms of spatial relations they

774
00:54:10,480 --> 00:54:14,080
describe happening streams in terms of emotions and mental states

775
00:54:14,090 --> 00:54:17,650
there's something about the way that our brains are wired with things moving behave in

776
00:54:17,650 --> 00:54:18,390
certain ways

777
00:54:18,410 --> 00:54:22,450
we use the only different way to think about them using terms this theorem under

778
00:54:22,450 --> 00:54:26,500
the mental capacity to understand things in terms of the underlying mental states and of

779
00:54:26,500 --> 00:54:31,190
course robotics is that you don't have to have an exact humanoid form in order

780
00:54:31,190 --> 00:54:34,210
to do this is a very intriguing designs

781
00:54:34,290 --> 00:54:37,940
so i want to show you video that maybe many of you have seen this

782
00:54:37,940 --> 00:54:39,470
in two thousand four

783
00:54:39,490 --> 00:54:41,660
when apple computers

784
00:54:41,670 --> 00:54:49,380
i was trying to introduce new idea

785
00:55:10,960 --> 00:55:18,200
so we are able to engage many kinds of morphology in this way of interacting

786
00:55:18,200 --> 00:55:22,260
so one question that we had recently with what we actually built

787
00:55:22,280 --> 00:55:25,980
a robot computer what if instead of having sort of animation and what we actually

788
00:55:25,980 --> 00:55:29,740
built an everyday artifacts and infuse robotic

789
00:55:29,760 --> 00:55:33,500
qualities to it so this is called broker and rocco has a number of degrees

790
00:55:33,500 --> 00:55:34,370
of freedom

791
00:55:34,390 --> 00:55:38,970
and you can imagine putting cameras and sensors and can areas behaviours here postures and

792
00:55:38,970 --> 00:55:44,640
so forth so why is the interesting and this is the collaboration with the cartoon

793
00:55:44,660 --> 00:55:48,340
the computer immediately to mary pressures

794
00:55:48,350 --> 00:55:50,020
why is this interesting

795
00:55:51,560 --> 00:55:56,270
we were inspired one of the questions that are going to computer was inspired by

796
00:55:56,270 --> 00:55:58,160
a study done in nineteen eighty four by

797
00:55:58,170 --> 00:55:59,470
john risk and

798
00:55:59,480 --> 00:56:03,430
which is really exploring in humans again this interface between emotion

799
00:56:04,570 --> 00:56:06,170
an embodiment and

800
00:56:06,180 --> 00:56:07,790
and the role of the body

801
00:56:07,800 --> 00:56:14,170
and it's very intriguing study where he took set of human subjects and he had

802
00:56:14,180 --> 00:56:16,790
to do with that trace fossils so identified

803
00:56:16,800 --> 00:56:18,150
o point here

804
00:56:18,220 --> 00:56:21,370
so this is an interesting puzzle task were

805
00:56:21,500 --> 00:56:22,870
is working

806
00:56:39,020 --> 00:56:42,490
it also

807
00:56:42,510 --> 00:56:46,690
so have the option to feel quite quite a number of times so basically what

808
00:56:46,690 --> 00:56:51,160
he did is under the biometrics experiment invented the people in these two conditions into

809
00:56:51,160 --> 00:56:54,410
another room and had to actually sit in the past year so this in very

810
00:56:55,340 --> 00:56:57,480
might be analogous to being proud

811
00:56:57,490 --> 00:57:00,200
successful and the idea perhaps it's too

812
00:57:00,210 --> 00:57:03,500
as if they fail and how the whole that three minutes

813
00:57:03,550 --> 00:57:08,540
and then brought them back and do more distressing for all of the unsolvable

814
00:57:08,590 --> 00:57:11,510
what he found was that OK

815
00:57:11,520 --> 00:57:17,430
what he found was in the case where people are body posture and mood congruence

816
00:57:17,470 --> 00:57:21,720
people had more persistence on the task even in the case where the institute

817
00:57:22,110 --> 00:57:24,890
so we called this that stoop to conquer

818
00:57:24,910 --> 00:57:28,720
so this is an intriguing is intuitive right is coded but

819
00:57:28,740 --> 00:57:34,430
again is deeply intertwined systems of course it's intriguing to think about what the robot

820
00:57:34,480 --> 00:57:38,930
can robots interface with us in our bodies and our emotions and conditions in which

821
00:57:38,930 --> 00:57:40,400
that can help us perform better

822
00:57:41,300 --> 00:57:45,260
one question would be can you actually replicating this to conquer fact

823
00:57:45,260 --> 00:57:48,200
as the problem

824
00:57:48,230 --> 00:57:53,190
so but this picture was that a little bit vague

825
00:57:54,370 --> 00:57:56,070
it was vague because

826
00:57:56,120 --> 00:58:03,440
depending so you can you can put the ball in very different places so you

827
00:58:03,440 --> 00:58:06,640
can put the ball the of this robot or agent

828
00:58:06,650 --> 00:58:12,970
at the other place that there is a physical interaction with the environment like you

829
00:58:12,970 --> 00:58:18,080
have a robot and it has person and we assign acting on the environment and

830
00:58:18,080 --> 00:58:23,570
the environment is is putting energy into the sensors of the robot you can put

831
00:58:23,570 --> 00:58:24,900
the body that

832
00:58:24,920 --> 00:58:30,120
but on the other hand if you don't want to do this the physical interactions

833
00:58:30,140 --> 00:58:35,390
but you may only are interested in this business of the brain pong

834
00:58:35,430 --> 00:58:38,410
then probably you want to get some deeper

835
00:58:38,430 --> 00:58:41,900
and this is what we are going to do so this is the viewpoint that

836
00:58:41,900 --> 00:58:46,100
we're going to take you don't want to care about the physical processes

837
00:58:46,110 --> 00:58:47,890
they say that after

838
00:58:47,900 --> 00:58:50,930
this interaction happens is the environment

839
00:58:50,940 --> 00:58:52,650
that need some

840
00:58:52,660 --> 00:58:58,110
mechanism that that transforms advertising into cities of bits or whatever

841
00:58:58,120 --> 00:59:02,890
so i just wanted to care about the software not the brain pod

842
00:59:03,680 --> 00:59:08,690
this figure shows that shows how this is happening so there are these acts sensations

843
00:59:08,690 --> 00:59:09,850
coming in

844
00:59:09,890 --> 00:59:15,120
and the actual agent that we do care about is somebody inside and many things

845
00:59:15,120 --> 00:59:19,700
can happen just this in the boundary of the robot you know the physical boundary

846
00:59:19,700 --> 00:59:20,880
of the robot

847
00:59:20,900 --> 00:59:22,790
so we

848
00:59:22,810 --> 00:59:27,220
do not necessarily care is the the pure sensor readings right so we don't want

849
00:59:27,230 --> 00:59:29,750
to care about the what voltages or whatever

850
00:59:29,770 --> 00:59:33,150
so that would be some nice and clever

851
00:59:33,150 --> 00:59:37,790
mechanism that do some field carrying the rejection of noise or whatever

852
00:59:37,810 --> 00:59:42,530
but what's important for us is that after this is done

853
00:59:42,580 --> 00:59:47,120
we can really just care about the brain of the robot

854
00:59:47,160 --> 00:59:51,270
so in particular

855
00:59:51,280 --> 00:59:57,050
it it it is good to keep in mind that you could have sensations coming

856
00:59:57,050 --> 01:00:02,440
from axon and other than the internet part of the robot put it to stay

857
01:00:03,190 --> 01:00:05,780
what do i mean by that so if you have here

858
01:00:05,860 --> 01:00:07,470
if you have a body

859
01:00:07,520 --> 01:00:10,510
then maybe you have

860
01:00:10,550 --> 01:00:13,560
times are something like that if you're a box

861
01:00:13,580 --> 01:00:18,540
and arms could have various chance and then you can measure the angles of the

862
01:00:18,540 --> 01:00:24,320
chance of the angular velocity of the challenge those would be internal sensations and

863
01:00:24,350 --> 01:00:29,470
and those sensations i really part of a of the sensations

864
01:00:30,780 --> 01:00:32,190
in particular

865
01:00:32,200 --> 01:00:37,400
there is one more thing that will hopefully become more clear

866
01:00:37,470 --> 01:00:42,510
so we have we see here an arrow that states state

867
01:00:42,550 --> 01:00:49,400
so particle of what happens is that there should be a mechanism the robot that

868
01:00:49,400 --> 01:00:55,400
does some processing that constructs so something that's called the states it was the state

869
01:00:55,440 --> 01:00:59,870
so the state should be something like citation statistics for the future

870
01:00:59,910 --> 01:01:05,670
and this will be your main assumption that that in this series of talks

871
01:01:05,720 --> 01:01:11,120
that that is some mechanism that's able to construct the state if

872
01:01:11,130 --> 01:01:16,140
we see the consequences if there is no such mechanism

873
01:01:16,600 --> 01:01:21,800
or how you can approximate such a mechanism but but this will be are working

874
01:01:21,800 --> 01:01:23,760
how do this

875
01:01:23,770 --> 01:01:24,670
and then

876
01:01:24,680 --> 01:01:27,890
you know this spring practices sizeable the actions

877
01:01:27,900 --> 01:01:32,410
but actions could be high level actions they do not necessarily have to be real

878
01:01:32,780 --> 01:01:36,020
actions like turning the wheel by is

879
01:01:36,060 --> 01:01:39,650
just one degree or something like that

880
01:01:39,660 --> 01:01:44,000
so anyway so we're are going to have sector is some details

881
01:01:44,230 --> 01:01:46,780
by proposing mathematical model

882
01:01:46,810 --> 01:01:49,680
so this is a generic mathematica model

883
01:01:49,730 --> 01:01:53,900
so you have planned control the

884
01:01:53,990 --> 01:01:58,260
the state of the plant is developing according to this is activation so this is

885
01:01:58,270 --> 01:02:00,290
a very generic information so

886
01:02:00,350 --> 01:02:05,660
it has no structural whatsoever i'm just saying that the next state is dependent on

887
01:02:05,670 --> 01:02:11,060
the previous state and the current actions and there is some noise injected into the

888
01:02:11,060 --> 01:02:15,150
system so this state i was in in a noisy

889
01:02:15,190 --> 01:02:16,260
and then

890
01:02:16,280 --> 01:02:20,770
what the robot can also have this is not actually the state but just something

891
01:02:20,770 --> 01:02:30,270
what is about

892
01:02:30,280 --> 01:02:34,080
and you find

893
01:03:28,030 --> 01:03:31,610
thank you

894
01:03:46,340 --> 01:03:48,490
are not

895
01:04:12,230 --> 01:04:13,940
we have help

896
01:04:41,660 --> 01:04:44,890
from i think

897
01:04:48,090 --> 01:04:50,670
right here is

898
01:05:06,140 --> 01:05:09,670
they area

899
01:05:55,700 --> 01:06:00,420
we must

900
01:06:23,460 --> 01:06:29,040
in the

901
01:06:29,350 --> 01:06:31,410
i mean it

902
01:06:35,570 --> 01:06:38,730
so he

903
01:06:48,340 --> 01:06:50,460
we want to

904
01:07:21,380 --> 01:07:28,860
o o

905
01:07:28,860 --> 01:07:30,270
so when

906
01:07:31,780 --> 01:07:33,560
your you're notion how

907
01:07:33,570 --> 01:07:36,670
close you are to the best error rate

908
01:07:36,690 --> 01:07:38,480
the best

909
01:07:38,500 --> 01:07:40,330
is larger than new

910
01:07:40,390 --> 01:07:46,700
will be able to get an exponential convergence

911
01:07:46,720 --> 01:07:48,970
and then

912
01:07:52,120 --> 01:07:53,340
that someone

913
01:07:53,360 --> 01:07:56,450
you know just how close you are to the best boxes

914
01:07:56,500 --> 01:07:59,520
is smaller than

915
01:08:00,090 --> 01:08:03,650
the convergence is is no longer

916
01:08:03,650 --> 01:08:05,940
it should be a new there

917
01:08:05,960 --> 01:08:08,320
then get convergence is no longer

918
01:08:08,340 --> 01:08:12,460
exponential it's going to be as if we're in supervised learning land

919
01:08:13,950 --> 01:08:17,420
this is going to be new squared here

920
01:08:17,440 --> 01:08:20,750
so we all get a little bit of benefit from doing active learning

921
01:08:20,820 --> 01:08:22,940
some sort of constant factor improvement

922
01:08:22,980 --> 01:08:25,230
the number of labelled samples that are required

923
01:08:30,530 --> 01:08:35,060
so these two are obviously straightforward to show you a little bit how having these

924
01:08:35,060 --> 01:08:41,120
proofs work because it's it's informative

925
01:08:41,120 --> 01:08:45,420
OK so does seem to think about is sort of the noiseless case

926
01:08:45,430 --> 01:08:47,640
one year when you're running through

927
01:08:47,660 --> 01:08:51,810
this to think about the noiseless case for thresholds when year

928
01:08:51,870 --> 01:08:53,140
first thinking about the fear

929
01:08:53,220 --> 01:08:55,590
so in the noiseless case thresholds

930
01:08:55,670 --> 01:08:58,740
the area is a function of the threshold parameter is going to be

931
01:08:58,760 --> 01:09:00,900
it's kind of like like this

932
01:09:00,940 --> 01:09:02,090
some sort of

933
01:09:02,120 --> 01:09:03,700
absolute value

934
01:09:03,720 --> 01:09:09,610
and then

935
01:09:09,610 --> 01:09:11,910
the claim is that

936
01:09:11,960 --> 01:09:12,970
if you have

937
01:09:12,980 --> 01:09:15,640
a small no

938
01:09:15,680 --> 01:09:19,960
that doesn't really all to this picture very much it's going to increase the to

939
01:09:19,960 --> 01:09:24,860
buy new and then these lines here can generally shift but i don't know

940
01:09:24,870 --> 01:09:26,990
or maybe a bit less

941
01:09:27,010 --> 01:09:30,950
so this this picture is going to look very similar when you have a small

942
01:09:30,950 --> 01:09:34,310
middle married

943
01:09:34,910 --> 01:09:37,560
so what that means is that

944
01:09:37,580 --> 01:09:39,740
is it a constant fraction

945
01:09:39,750 --> 01:09:43,810
the classifier to to area point you've i tried to this is about point two

946
01:09:43,810 --> 01:09:45,270
five here

947
01:09:45,290 --> 01:09:48,120
in a constant fraction about half of them

948
01:09:48,130 --> 01:09:51,530
have an area greater than point two five

949
01:09:52,940 --> 01:09:53,710
OK so

950
01:09:53,720 --> 01:09:56,420
what that means is that if we have

951
01:09:56,430 --> 01:10:00,040
about blog whatever delta samples to

952
01:10:00,060 --> 01:10:02,390
the we label just at random

953
01:10:02,400 --> 01:10:05,690
we're going to be able to get upper bounds and lower bounds

954
01:10:05,740 --> 01:10:06,950
which are

955
01:10:06,980 --> 01:10:16,060
type two within maybe one eight

956
01:10:16,090 --> 01:10:17,790
it's still in the general case

957
01:10:17,810 --> 01:10:23,560
the same logic applies except now the number of labelled samples requires is maybe a

958
01:10:23,560 --> 01:10:24,720
little bit larger

959
01:10:24,730 --> 01:10:28,210
it is the this this data collection company

960
01:10:28,220 --> 01:10:32,020
this disagreement coefficient

961
01:10:34,070 --> 01:10:36,390
OK so now

962
01:10:37,730 --> 01:10:39,460
since i have the fractions

963
01:10:39,470 --> 01:10:41,720
and have fraction of classifiers

964
01:10:41,730 --> 01:10:44,810
air which is very the point two five

965
01:10:44,830 --> 01:10:47,440
any bounded tied to within

966
01:10:47,480 --> 01:10:49,700
o point one two five

967
01:10:49,710 --> 01:10:51,830
you can be you can guarantee

968
01:10:51,860 --> 01:10:53,980
at the minimum of around

969
01:10:53,990 --> 01:10:55,620
it will

970
01:10:55,700 --> 01:10:56,440
will be

971
01:10:56,460 --> 01:10:58,640
o point one two five less

972
01:10:58,650 --> 01:10:59,440
and the

973
01:10:59,460 --> 01:11:02,560
the least half the cost of the hypotheses have

974
01:11:04,410 --> 01:11:05,620
have narrative

975
01:11:05,880 --> 01:11:08,690
the lower bound of twenty five or more

976
01:11:08,740 --> 01:11:10,120
so you can eliminate

977
01:11:10,130 --> 01:11:12,720
have a disagreement region

978
01:11:13,020 --> 01:11:19,390
that's the that's the fundamental logic which is going on

979
01:11:19,410 --> 01:11:22,790
and then you need if you do about this but you know how to do

980
01:11:27,510 --> 01:11:28,360
OK so

981
01:11:28,360 --> 01:11:29,540
so now

982
01:11:29,570 --> 01:11:34,540
that that's the case when you get exponential improvement in very happy we still sort

983
01:11:34,540 --> 01:11:36,370
of like to understand what happens when

984
01:11:36,410 --> 01:11:38,810
when you have a substantial

985
01:11:38,830 --> 01:11:41,320
no married

986
01:11:44,610 --> 01:11:45,390
we get

987
01:11:45,980 --> 01:11:49,790
the basic recursion process is going to

988
01:11:49,820 --> 01:11:52,740
keep as it were keeping we cursing

989
01:11:53,630 --> 01:11:58,870
the size of the region is something like lightning

990
01:11:58,890 --> 01:12:06,540
and that's just using the prefix there

991
01:12:06,570 --> 01:12:08,020
and now it's going to happen

992
01:12:08,040 --> 01:12:11,420
it is we have this termination condition which is done

993
01:12:11,450 --> 01:12:13,250
is less than epsilon

994
01:12:14,940 --> 01:12:15,520
this is

995
01:12:15,550 --> 01:12:20,170
this is one part of done is disagree with reason to be like news so

996
01:12:20,170 --> 01:12:24,430
we have this new multiplayer here

997
01:12:24,440 --> 01:12:29,790
so what happened is we recursively times and now the noise rises to height restriction

998
01:12:31,750 --> 01:12:33,250
so now we have two

999
01:12:33,260 --> 01:12:35,800
compute upper and lower bounds

1000
01:12:35,820 --> 01:12:39,940
and then we we get a little the boost from this new because new value

1001
01:12:40,310 --> 01:12:43,040
is is less than or equal to one

1002
01:12:43,710 --> 01:12:44,750
so if we want

1003
01:12:44,770 --> 01:12:47,650
this quantity to be less than that someone

1004
01:12:47,660 --> 01:12:49,680
we just need to get the bounds

1005
01:12:49,700 --> 01:12:52,430
we we need this to the men minus

1006
01:12:52,440 --> 01:12:54,360
the upper bound must lower bound

1007
01:12:54,380 --> 01:12:55,460
to be

1008
01:12:55,530 --> 01:13:00,530
spent two epsilon over new since new is less than one

1009
01:13:00,550 --> 01:13:06,030
it means that we need your samples to get tight enough found

1010
01:13:06,040 --> 01:13:09,150
and then and then done will be

1011
01:13:09,160 --> 01:13:10,270
about that song

1012
01:13:10,300 --> 01:13:11,410
so that's why

1013
01:13:11,420 --> 01:13:14,970
you get this kind of behaviour that

1014
01:13:14,990 --> 01:13:17,240
and then i guess

1015
01:13:17,240 --> 01:13:21,830
that people look at one two three or four abstract but not really more than

1016
01:13:23,200 --> 01:13:25,060
like his whole often the click

1017
01:13:25,070 --> 01:13:26,280
one link

1018
01:13:26,290 --> 01:13:28,790
and you can see that even more skewed for the

1019
01:13:28,810 --> 01:13:31,780
in this case almost three times as many people

1020
01:13:31,800 --> 01:13:32,830
click on

1021
01:13:32,830 --> 01:13:34,660
the result ranked number one

1022
01:13:34,680 --> 01:13:37,120
then on the result rank number two

1023
01:13:37,150 --> 01:13:40,680
so writing is very important in the sense that people click on the top ranked

1024
01:13:41,870 --> 01:13:43,750
and you can actually for people

1025
01:13:43,780 --> 01:13:49,610
here we have a result for regular search rankings for the first result is relevant

1026
01:13:49,610 --> 01:13:53,780
and the second is not relevant and that case it's not surprising that most people

1027
01:13:53,780 --> 01:13:56,550
take on the first result but then if you

1028
01:13:56,560 --> 01:13:57,980
search around the two

1029
01:13:58,000 --> 01:14:00,750
so now the first one is not one of them but

1030
01:14:00,810 --> 01:14:04,130
the second the second is the relevant one people still think on the first one

1031
01:14:04,130 --> 01:14:05,870
even though it is not relevant

1032
01:14:05,900 --> 01:14:07,310
so you can see there

1033
01:14:07,330 --> 01:14:10,880
some people automatically go the first one because they

1034
01:14:10,900 --> 01:14:12,400
i have developed some

1035
01:14:12,420 --> 01:14:18,120
trust in the search engines and the search engines will actually present most relevant

1036
01:14:18,160 --> 01:14:20,500
the result first

1037
01:14:20,580 --> 01:14:24,510
so in summary important frank

1038
01:14:24,560 --> 01:14:28,580
viewing abstracts uses a lot more likely to read the abstracts of the top ranked

1039
01:14:29,500 --> 01:14:32,080
then the extract of the lower ranked pages

1040
01:14:32,090 --> 01:14:36,330
taking the distribution is even more skewed for clicking

1041
01:14:36,400 --> 01:14:40,040
there's a very strong bias to become the top ranked page

1042
01:14:40,060 --> 01:14:43,070
even if the top ranked page is not relevant thirty percent of users will click

1043
01:14:44,620 --> 01:14:48,470
so getting the ranking is very important in getting the top ranked page right is

1044
01:14:48,470 --> 01:14:53,700
most important and so that's that's supposed to serve as motivation for

1045
01:14:53,840 --> 01:15:00,030
four why ranking information retrieval is an important problem

1046
01:15:01,120 --> 01:15:05,700
OK so that was the only way model the index processing boolean queries why do

1047
01:15:05,700 --> 01:15:07,340
we need ranked retrieval

1048
01:15:07,870 --> 01:15:12,850
and maybe can ask all i put some resources on

1049
01:15:12,860 --> 01:15:15,630
our books side information the article

1050
01:15:15,810 --> 01:15:20,690
so you've have for this that to you find this stuff useful information resources

1051
01:15:20,710 --> 01:15:23,150
shakespeare search engine that school in

1052
01:15:23,180 --> 01:15:27,480
and then russell's home page where you can look up the original slides that of

1053
01:15:27,480 --> 01:15:30,940
the of the user study the national

1054
01:15:30,960 --> 01:15:34,490
maybe i can ask the question

1055
01:15:34,540 --> 01:15:38,780
many of you i think uses maxent so i assume you spotlight and i'm not

1056
01:15:38,780 --> 01:15:44,160
sure that on on on this is this a similar search system is spotlighted billions

1057
01:15:44,160 --> 01:15:51,390
of system

1058
01:15:51,390 --> 01:15:54,370
no why not

1059
01:16:01,760 --> 01:16:04,430
right so it's important to distinguish between

1060
01:16:04,440 --> 01:16:06,350
a pupil in system where

1061
01:16:06,370 --> 01:16:12,060
things are where everything is presented that's the that satisfies the william period but then

1062
01:16:12,070 --> 01:16:14,610
some of things coastal

1063
01:16:14,630 --> 01:16:16,480
i mean have

1064
01:16:16,510 --> 01:16:21,880
some of the order of is imposed on this set were not so spotlight

1065
01:16:21,890 --> 01:16:24,640
imposes an ordering like you suggested that

1066
01:16:25,220 --> 01:16:27,430
i think it's actually the most recent opening

1067
01:16:27,640 --> 01:16:30,930
or a combination of the most recent opening up the number of openings

1068
01:16:30,940 --> 01:16:36,660
and in many other search engines use use use time for example if you search

1069
01:16:36,660 --> 01:16:40,920
for citations often you get the most recent citations on the line for example the

1070
01:16:40,920 --> 01:16:45,990
most recent citations first but they are million engines in the sense that

1071
01:16:46,050 --> 01:16:50,310
a document that does not contain all the keywords is not included in the results

1072
01:16:51,150 --> 01:16:54,000
and google also was initially a search engine like that

1073
01:16:54,030 --> 01:16:59,260
it was william in the sense that only documents that contain all the search terms

1074
01:16:59,260 --> 01:17:00,540
were included in the

1075
01:17:00,960 --> 01:17:04,980
results but then not was supposed to

1076
01:17:05,070 --> 01:17:07,700
any questions about this

1077
01:17:07,700 --> 01:17:10,020
o point or anything else in this

1078
01:17:10,030 --> 01:17:14,730
part of the lecture

1079
01:17:14,740 --> 01:17:28,390
OK and that's continue with the vector space model

1080
01:17:28,900 --> 01:17:41,710
OK a quick review of tf idf weighting

1081
01:17:41,730 --> 01:17:46,450
then the vector space model which represents theories and documents and i dimensional space

1082
01:17:46,490 --> 01:17:52,560
and then pivot normalisation that's an alternative to cosine normalisation that removes a bias inherent

1083
01:17:52,560 --> 01:17:54,290
in standard length normalisation

1084
01:17:54,300 --> 01:17:58,750
the reason for this is that again it's very nice methodological point

1085
01:18:00,240 --> 01:18:04,710
your system is not what you wanted to do how can you analyse it and

1086
01:18:05,220 --> 01:18:10,540
how can you prove that this is a nice example of

1087
01:18:10,560 --> 01:18:12,760
so we start with this line

1088
01:18:13,670 --> 01:18:14,980
these are the terms

1089
01:18:15,030 --> 01:18:19,200
all words by the way of using trying work interchangeably years

1090
01:18:19,200 --> 01:18:21,120
and these documents

1091
01:18:21,130 --> 01:18:23,030
and now moving from by

1092
01:18:23,460 --> 01:18:25,400
term incidence matrix

1093
01:18:25,480 --> 01:18:31,060
two a column matrix where each document is represented as a vector of counts so

1094
01:18:31,060 --> 01:18:35,880
now i want to use the card information the fact that sometimes occur several times

1095
01:18:35,880 --> 01:18:40,220
in the document because that is important information for ranking

1096
01:18:40,240 --> 01:18:43,920
we use the term frequency for that and that is defined as

1097
01:18:43,930 --> 01:18:50,440
the number of times that you because that is the term frequency tf td

1098
01:18:50,570 --> 01:18:54,220
we want to rank documents according to query document matching scores

1099
01:18:54,240 --> 01:18:58,930
and use the city as a component in these matching scores

1100
01:18:58,940 --> 01:19:00,670
but how

1101
01:19:00,760 --> 01:19:05,520
we could use water frequency simply the can't but that's not what we want because

1102
01:19:05,620 --> 01:19:09,590
the document ten occurrences of the term is more relevant than a document with one

1103
01:19:09,590 --> 01:19:10,770
occurrence speed

1104
01:19:10,800 --> 01:19:12,140
could surmise

1105
01:19:12,220 --> 01:19:16,140
but not type ten times more

1106
01:19:16,140 --> 01:19:19,460
relevance does not increase proportionally with term frequency

1107
01:19:19,480 --> 01:19:23,350
so we're trying to find some criteria for ranking but we don't want to do

1108
01:19:23,350 --> 01:19:25,470
it proportionally because

1109
01:19:25,490 --> 01:19:27,140
we need to have some of

1110
01:19:27,150 --> 01:19:33,650
we need to be very careful about how we include this information directly

1111
01:19:33,670 --> 01:19:38,210
so instead of raw frequency going to use long sequences

1112
01:19:38,210 --> 01:19:41,430
suppose there the the function minimizing

1113
01:19:41,440 --> 01:19:43,380
the objective is separable

1114
01:19:43,390 --> 01:19:49,510
so it's it's the sum of functions of blocks of the original vector right so

1115
01:19:49,510 --> 01:19:52,650
it separates out like this this could be all the way down to the components

1116
01:19:52,650 --> 01:19:54,530
but this could be vectors as well so it

1117
01:19:54,550 --> 01:19:56,080
this is separable

1118
01:19:56,280 --> 01:19:58,740
well in that case the lagrangian is separable

1119
01:19:58,760 --> 01:20:03,260
and because you're adding now find function but all i find functions are separable then

1120
01:20:03,260 --> 01:20:04,350
the component so

1121
01:20:04,380 --> 01:20:09,060
this is separable and what that says is that this minimisation

1122
01:20:09,970 --> 01:20:15,020
over the lagrangian over x actually splits and can be done in parallel because

1123
01:20:15,050 --> 01:20:18,020
it's it's because humanizing something that is of some

1124
01:20:18,030 --> 01:20:20,340
of functions of independent variables

1125
01:20:20,350 --> 01:20:22,140
so you just do it in parallel

1126
01:20:23,060 --> 01:20:24,930
that leads to

1127
01:20:24,940 --> 01:20:29,030
dual decomposition and that looks like this and again i know that many people know

1128
01:20:29,030 --> 01:20:34,830
this so but this is just sort of give you historical background and review so

1129
01:20:34,830 --> 01:20:37,200
with our this each of these things

1130
01:20:37,310 --> 01:20:44,910
minimizes the lagrangian give you a price vector each minimizes their the lagrangian terms separately

1131
01:20:45,330 --> 01:20:50,870
you collect the contribution to the equality constraints and you evaluate the residual of zero

1132
01:20:50,870 --> 01:20:51,780
you quit

1133
01:20:51,800 --> 01:20:56,910
otherwise you update the price vector and you can see that what this requires is

1134
01:20:57,120 --> 01:20:58,310
something like

1135
01:20:58,330 --> 01:20:59,970
the gathering the scattered

1136
01:21:00,020 --> 01:21:01,750
right so you have to gather

1137
01:21:01,770 --> 01:21:04,110
these components to evaluate the residual

1138
01:21:04,120 --> 01:21:06,320
and you scatter the price effect

1139
01:21:06,330 --> 01:21:11,150
right so that but now you have died doing distributed optimisation

1140
01:21:14,070 --> 01:21:15,210
all right

1141
01:21:15,220 --> 01:21:21,310
next topic is that's maybe from around nineteen sixty something like the next method of

1142
01:21:22,590 --> 01:21:29,350
and what method of multipliers well sort its method that's meant to robustify dual asset

1143
01:21:29,400 --> 01:21:30,090
right so

1144
01:21:30,100 --> 01:21:33,200
to actually make it so that it would work on things like linear programming and

1145
01:21:33,200 --> 01:21:34,880
stuff like that which this

1146
01:21:34,900 --> 01:21:38,310
absolutely will not and the idea here

1147
01:21:39,410 --> 01:21:42,400
goes back at least the sixties but in fact it can be can be argued

1148
01:21:42,400 --> 01:21:45,490
to go earlier to operator splitting methods but that case

1149
01:21:45,550 --> 01:21:50,920
it is to the normal this is this is here the usual lagrangian terms and

1150
01:21:50,920 --> 01:21:53,670
you can think of this is sort of like pure free market term this is

1151
01:21:53,670 --> 01:21:57,960
going to violate the constraints but you pay for it these prices

1152
01:21:58,020 --> 01:22:01,500
well you pay for it or you're subsidized for depends on which end of that

1153
01:22:01,500 --> 01:22:03,800
year on

1154
01:22:03,820 --> 01:22:07,800
and then you have another term which is always nonnegative quadratic penalty

1155
01:22:07,820 --> 01:22:10,190
right and this this this one is

1156
01:22:10,200 --> 01:22:11,580
that's always

1157
01:22:11,620 --> 01:22:16,580
sort of cost is never subsidy right so this is this is simply cost for

1158
01:22:16,580 --> 01:22:21,060
not for the market not clearing for example in economics right so it's an augmented

1159
01:22:21,060 --> 01:22:23,940
lagrangian and the method of multipliers

1160
01:22:23,950 --> 01:22:24,970
it looks like this

1161
01:22:24,990 --> 01:22:28,560
you minimize the augmented lagrangian

1162
01:22:28,570 --> 01:22:29,540
and then

1163
01:22:29,550 --> 01:22:33,080
you do i dual update and it's exactly the same

1164
01:22:34,040 --> 01:22:35,700
it's identical to

1165
01:22:37,380 --> 01:22:40,930
do less and except for one thing the

1166
01:22:40,950 --> 01:22:42,870
the the gain

1167
01:22:42,910 --> 01:22:47,430
in the gradient update has a very specific choices to play this role it's the

1168
01:22:47,870 --> 01:22:49,410
it's the parameters here

1169
01:22:49,420 --> 01:22:52,900
right and the reason for that and it's easy to

1170
01:22:52,920 --> 01:22:54,120
i understand that

1171
01:22:54,200 --> 01:22:57,750
the original problem i mean when it differentiable

1172
01:22:57,770 --> 01:23:00,490
the original problem the optimality conditions are primal

1173
01:23:00,520 --> 01:23:04,100
what about feasibility so you have to x equals b and then you have to

1174
01:23:04,100 --> 01:23:08,690
have dual feasibility which is that the gradient policy transpose why stars zero

1175
01:23:08,720 --> 01:23:13,570
and if xk plus one minimizes this thing where you work out the guns and

1176
01:23:13,570 --> 01:23:17,460
differentiable case but it's the same and non differentiable case you work out what

1177
01:23:17,470 --> 01:23:21,550
what it means to minimize that will it just means that the gradient of the

1178
01:23:21,550 --> 01:23:25,670
augmented lagrangian is zero when you work out what that is and you recognise

1179
01:23:25,690 --> 01:23:28,060
this term here and you realize

1180
01:23:28,070 --> 01:23:32,880
if i were to if this were to be y plus one then it says

1181
01:23:32,880 --> 01:23:36,960
the following it says that at the end of method of multipliers each step

1182
01:23:37,130 --> 01:23:38,950
dual feasibility always

1183
01:23:38,960 --> 01:23:44,130
right so so in fact what it says is that with that choice of

1184
01:23:44,160 --> 01:23:46,050
with that choice of update

1185
01:23:46,180 --> 01:23:51,220
dual update you could dual feasibility sort of that happens for free then what you're

1186
01:23:51,220 --> 01:23:52,470
what it means is

1187
01:23:52,490 --> 01:23:57,460
as the algorithm progresses which are hoping is that the is that the residuals the

1188
01:23:57,460 --> 01:23:59,920
primal residuals will go to zero

1189
01:23:59,940 --> 01:24:02,350
OK so that's the that's the idea

1190
01:24:04,100 --> 01:24:06,980
now that's good news and bad news

1191
01:24:07,000 --> 01:24:09,410
so the good news is that

1192
01:24:09,420 --> 01:24:14,770
adding the quadratic augmenting term on i should add you know you can add

1193
01:24:14,780 --> 01:24:18,630
other types of augmenting terms that's become very popular in the last couple of years

1194
01:24:18,630 --> 01:24:23,670
we bregman divergences are things like that always keep i'll stick with quadratic because it's

1195
01:24:23,670 --> 01:24:27,750
simple and if you understand that you can surely understand what happens if you had

1196
01:24:27,750 --> 01:24:30,660
like in entropy or something like that there are some OK

1197
01:24:31,120 --> 01:24:33,210
the good news is adding that

1198
01:24:33,230 --> 01:24:36,950
adding that regularisation makes this goal makes

1199
01:24:37,050 --> 01:24:41,600
dual as and go from a very fragile algorithm

1200
01:24:41,610 --> 01:24:45,310
that's the kind of the coaxed to work and in most cases you would think

1201
01:24:45,310 --> 01:24:47,560
that doesn't work at all to something

1202
01:24:47,560 --> 01:24:51,750
OK so in this case we said the costa going to catch point one millisecond

1203
01:24:51,750 --> 01:24:53,620
with point one seconds

1204
01:24:53,670 --> 01:24:57,620
costa going to the desk is ten million seconds

1205
01:24:57,620 --> 01:25:01,020
and now what we're left is to figure out what the probability of this

1206
01:25:05,270 --> 01:25:06,600
if you think about

1207
01:25:06,630 --> 01:25:10,060
a system for a little bit you might say well OK so what's the what's

1208
01:25:10,060 --> 01:25:13,860
the system look like what's the probability that the idea get that they we're actually

1209
01:25:13,860 --> 01:25:15,710
going to get hit this

1210
01:25:17,040 --> 01:25:21,550
and if you have with applications are doing is simply generating random

1211
01:25:21,600 --> 01:25:26,260
requests for random pages which might be one model the application behaves there doesn't seem

1212
01:25:26,260 --> 01:25:28,960
like the cat is going to buy is very much right because the cat is

1213
01:25:28,960 --> 01:25:31,400
going to be a small fraction of the total size of the disk it might

1214
01:25:31,400 --> 01:25:35,460
be in RAM so we might have a hundred megabytes we might ten jacobi to

1215
01:25:36,150 --> 01:25:37,240
right so

1216
01:25:37,250 --> 01:25:40,770
that means there's a factor one hundred difference between these two things of the discrete

1217
01:25:40,810 --> 01:25:44,430
page requests random probability that something is going to be in the captions can be

1218
01:25:44,430 --> 01:25:47,570
very low it's going to be only say one percent

1219
01:25:47,600 --> 01:25:52,050
but it turns out that almost all applications have an interesting property which is commonly

1220
01:25:52,050 --> 01:25:53,630
referred to as

1221
01:25:53,770 --> 01:26:01,000
locality of reference

1222
01:26:01,060 --> 01:26:05,480
what this means is that if you look at which pages a program is likely

1223
01:26:05,480 --> 01:26:10,770
to access over time be these pages of data or parts of a program typically

1224
01:26:10,770 --> 01:26:16,770
programs page has been accessed recently is very likely to be accessed again so simple

1225
01:26:16,770 --> 01:26:19,430
example might be that if you look at the files that are being used in

1226
01:26:19,430 --> 01:26:24,310
a computer system usually there's a small set of files the users working with any

1227
01:26:24,310 --> 01:26:29,200
given point in time running certain programs editing certain documents and there's a huge array

1228
01:26:29,200 --> 01:26:33,690
of other programs and files and documents there on the system that you are accessing

1229
01:26:33,690 --> 01:26:37,830
so and when those files are doing this active files are being accessed is much

1230
01:26:38,090 --> 01:26:42,230
there's a much higher probability of getting a hit of looking at those active files

1231
01:26:42,250 --> 01:26:45,880
there is going to any one of the other files so even when the difference

1232
01:26:45,880 --> 01:26:48,980
between these things is a factor of one hundred in the case of the web

1233
01:26:49,790 --> 01:26:54,170
maybe be very likely that we would have no ninety percent maybe of the pages

1234
01:26:54,170 --> 01:26:55,400
that have been accessed

1235
01:26:55,500 --> 01:26:57,880
are already in the cache stake

1236
01:26:57,880 --> 01:27:03,190
so in that case sorry this probability should be point nine ninety percent

1237
01:27:03,200 --> 01:27:06,750
OK so i suppose that the probability of

1238
01:27:06,810 --> 01:27:10,940
excuse me this should be this is the probability of hitting ninety percent probability of

1239
01:27:10,950 --> 01:27:12,560
missus then ten percent

1240
01:27:12,620 --> 01:27:16,750
OK so the if you if you think if you look at the now with

1241
01:27:16,750 --> 01:27:21,000
this sort of formula evaluates to receive point one plus

1242
01:27:21,060 --> 01:27:26,290
ten times point one so it's one point one million seconds

1243
01:27:26,300 --> 01:27:30,270
on average assuming that we get ninety percent hit rate cash

1244
01:27:31,300 --> 01:27:35,000
so if you if you know come come back to this diagram that means that

1245
01:27:35,000 --> 01:27:37,500
we can the throughput of this boxes

1246
01:27:37,560 --> 01:27:40,320
you know is one over

1247
01:27:40,330 --> 01:27:44,260
one point one member because throughput is just one over latency when we have just

1248
01:27:44,380 --> 01:27:50,290
sort of this one module which means that are now we can process something like

1249
01:27:50,310 --> 01:27:53,900
this this is something like

1250
01:27:53,990 --> 01:27:56,040
say this should be

1251
01:27:56,200 --> 01:28:00,580
one of related is that this is in millisecond so this point one one

1252
01:28:02,500 --> 01:28:06,090
the number of seconds this is point zero zero one one right so one of

1253
01:28:06,090 --> 01:28:07,890
our point zero zero one one is

1254
01:28:07,950 --> 01:28:11,790
about nine hundred cases we can get about nine hundred requests per second which can

1255
01:28:11,790 --> 01:28:14,020
process instead of just one hundred

1256
01:28:14,070 --> 01:28:18,120
this is approximately equal

1257
01:28:18,250 --> 01:28:21,220
OK so what we did manage to do is to increase the throughput of the

1258
01:28:21,250 --> 01:28:23,660
stage by about a factor of nine

1259
01:28:23,720 --> 01:28:26,260
i mean you can see that sort of all three of these stages are close

1260
01:28:26,260 --> 01:28:30,880
to about one thousand across the second increase the performance of the system pretty dramatically

1261
01:28:30,880 --> 01:28:33,050
by introducing cash

1262
01:28:33,120 --> 01:28:35,630
and you have to remember that this cat is only going to be a sort

1263
01:28:35,630 --> 01:28:37,250
of it's only going to be a good idea

1264
01:28:37,290 --> 01:28:40,880
when we're sure that we have this locality of reference property so if the web

1265
01:28:40,880 --> 01:28:42,250
server is

1266
01:28:42,330 --> 01:28:46,030
going to a random page then we're not going to be if the web server

1267
01:28:46,030 --> 01:28:49,450
sort of disaster for random page and every request the cache is not going to

1268
01:28:49,450 --> 01:28:54,080
be a good choice but most of the time web servers at this locality property

1269
01:28:54,100 --> 01:28:56,790
so the last little detail that we need to talk about when we talk about

1270
01:29:01,250 --> 01:29:03,100
is simply

1271
01:29:03,110 --> 01:29:04,380
the is the

1272
01:29:04,400 --> 01:29:07,260
sort of question about how we do

1273
01:29:07,280 --> 01:29:12,120
how we deal with page eviction so in this diagram here

1274
01:29:13,140 --> 01:29:15,980
when we call added to the cache

1275
01:29:16,000 --> 01:29:20,210
if the cache is already full of results we have to pick something to replace

1276
01:29:21,150 --> 01:29:26,470
so we need what's called a page removal or page

1277
01:29:26,540 --> 01:29:32,210
replacement property policies

1278
01:29:33,890 --> 01:29:37,990
you guys presumably have seen different page remove or plays replacement properties and six double

1279
01:29:38,070 --> 01:29:39,150
four before

1280
01:29:39,220 --> 01:29:42,000
i'll just talk about two very quickly

1281
01:29:42,050 --> 01:29:45,950
fifo policy and l are you policy

1282
01:29:45,960 --> 01:29:50,300
so if i o means is first in first out so it says the the

1283
01:29:50,410 --> 01:29:53,390
that the thing to throw out is the first thing that we loaded into the

1284
01:29:54,370 --> 01:29:55,870
OK so y access

1285
01:29:55,880 --> 01:29:57,350
a set of pages

1286
01:29:57,370 --> 01:30:02,950
no one is supposed to have a three element cache accesses pages one two three

1287
01:30:03,380 --> 01:30:05,120
two one four

1288
01:30:05,170 --> 01:30:09,420
what i'm going to do is low this with what i mean

1289
01:30:09,470 --> 01:30:13,410
you can access these three pages this is going to be paid one will be

1290
01:30:13,410 --> 01:30:17,720
the first one in my cache so when i try and load page four

1291
01:30:17,740 --> 01:30:19,390
i'm going to give it h one

1292
01:30:19,450 --> 01:30:22,330
i think that's what's going to happen in five five system because one was the

1293
01:30:22,330 --> 01:30:24,000
first one is loaded

1294
01:30:24,090 --> 01:30:28,510
but the least recently used approach says is instead of evicting the first thing that

1295
01:30:28,510 --> 01:30:32,150
was putting the cash we want to evict the last thing that was read

1296
01:30:32,210 --> 01:30:33,220
from the cache

1297
01:30:33,230 --> 01:30:36,740
so in this case if i do one two three two one and then again

1298
01:30:36,740 --> 01:30:39,820
for the last thing it was red with three and that's what i want from

1299
01:30:41,210 --> 01:30:44,450
so if you think about the intuition behind these two policies

1300
01:30:44,550 --> 01:30:49,570
and you said you realize that this fifo approach has a problem which is that

1301
01:30:49,950 --> 01:30:53,360
it's sort of contrary to this of the notion of locality of reference right because

1302
01:30:53,360 --> 01:30:58,430
it says it doesn't capture anything about health sort of frequently data i data is

1303
01:30:58,430 --> 01:31:01,040
accessed or when it was last accessed it simply

1304
01:31:01,100 --> 01:31:06,320
you just throw something in vitro something that has the it grows the sort of

1305
01:31:06,320 --> 01:31:10,960
everybody knows g packet type of fourier analysis on images

1306
01:31:12,460 --> 01:31:16,070
it assumes a fixed to the topology so we really can't use it in three

1307
01:31:18,540 --> 01:31:23,000
but given the widespread importance of three d there's really and the fact that three

1308
01:31:23,000 --> 01:31:24,060
d images

1309
01:31:24,070 --> 01:31:26,410
and three d objects are very large

1310
01:31:26,530 --> 01:31:28,550
you want some sort of compression methods

1311
01:31:30,330 --> 01:31:34,390
there's a lot of interest in this in the computer graphics community

1312
01:31:34,750 --> 01:31:38,500
so what i'm going to show you is one of the solutions that was open

1313
01:31:38,510 --> 01:31:41,770
computer graphics and then when compared with another approach

1314
01:31:41,820 --> 01:31:44,140
that i've been working on

1315
01:31:44,160 --> 01:31:48,380
so let's first familiarize themselves with what we mean by three objects so it really

1316
01:31:48,380 --> 01:31:51,540
object essentially is specified by

1317
01:31:51,560 --> 01:31:53,500
jamie two components

1318
01:31:53,510 --> 01:31:56,580
so you have an object here is a pretty small objects

1319
01:31:56,600 --> 01:32:02,120
you can model is a graph and this is the adjacency matrix of the graph

1320
01:32:02,150 --> 01:32:04,370
the topology of the object

1321
01:32:04,420 --> 01:32:07,960
and it's geometry by its geometry means

1322
01:32:08,520 --> 01:32:14,030
you can think of the XYZ coordinates functions and the graphs essentially implanting here each

1323
01:32:14,030 --> 01:32:16,180
of these coordinate functions on the graph

1324
01:32:16,200 --> 01:32:20,350
so the problem of three compression is essentially to find

1325
01:32:20,400 --> 01:32:24,110
a new set of coordinate representations

1326
01:32:24,120 --> 01:32:28,140
that you can use to projective geometry on the take up far less space than

1327
01:32:28,140 --> 01:32:29,910
the usual representation

1328
01:32:29,920 --> 01:32:32,150
now there are two approaches to solve this problem

1329
01:32:32,170 --> 01:32:36,490
one approach is to essentially ignore topology and just focus on geometry

1330
01:32:36,510 --> 01:32:41,210
so you can just take the score functions and then use your favourite compression method

1331
01:32:41,230 --> 01:32:45,350
but there's been growing movement in computer graphics to see whether you can actually do

1332
01:32:45,350 --> 01:32:52,760
something with topology and derive adaptive compression methods that bill that are specific to each

1333
01:32:54,630 --> 01:32:57,750
so as you go to large objects you can notice the size of the particular

1334
01:32:57,750 --> 01:32:59,620
on growing and so

1335
01:33:02,060 --> 01:33:05,570
even this is not a particularly large objects so you know this is this is

1336
01:33:05,570 --> 01:33:08,470
a very challenging problem because you objects in millions

1337
01:33:08,520 --> 01:33:13,370
of vertices and so you want to fast compression methods

1338
01:33:13,390 --> 01:33:18,240
so the method i want to start talking about first is one that was introduced

1339
01:33:18,240 --> 01:33:21,830
by khanin gottesman from israel nine siggraph two thousand

1340
01:33:21,840 --> 01:33:23,640
the idea is very simple

1341
01:33:23,650 --> 01:33:26,600
you take the topology of the object it's a graph

1342
01:33:26,610 --> 01:33:29,720
you generate the eigen vectors of the graph plots in

1343
01:33:29,740 --> 01:33:32,920
and then you project the coordinate functions on

1344
01:33:32,930 --> 01:33:36,490
the first few i vectors of the graph class and this is essentially

1345
01:33:36,930 --> 01:33:39,020
fourier analysis

1346
01:33:39,110 --> 01:33:41,490
extended to graphs so

1347
01:33:41,500 --> 01:33:46,420
let's first review this method and then let's see what the limitations of this method

1348
01:33:46,430 --> 01:33:49,640
are before introducing a different method

1349
01:33:49,660 --> 01:33:52,570
so we've seen that the graph the class c and has been used in many

1350
01:33:52,570 --> 01:33:56,920
contexts in machine learning actually was for studying on the mid-seventies by fiddler was a

1351
01:33:56,920 --> 01:34:01,440
czech mathematician to be used for spectral clustering lots of work and semi supervised learning

1352
01:34:01,440 --> 01:34:02,670
in this conference

1353
01:34:03,960 --> 01:34:09,010
and someone that you may not be familiar with we've been exploring the use of

1354
01:34:09,010 --> 01:34:15,760
the class c for approximating functions called value functions in markov decision process

1355
01:34:15,780 --> 01:34:20,000
so is about the class and let's think of about differential representation of according to

1356
01:34:20,000 --> 01:34:25,360
functions rather represent each function exactly what we're going to do is going to look

1357
01:34:25,360 --> 01:34:29,480
at the difference between the value of the function at some coordinator at some vertex

1358
01:34:29,480 --> 01:34:34,750
x minus the average of the values that you would get by taking the neighbors

1359
01:34:34,790 --> 01:34:38,980
and computing sort of the the the average to the extent the degree of vertex

1360
01:34:39,870 --> 01:34:44,290
so you can represent this effect in matrix form using this and this is one

1361
01:34:44,290 --> 01:34:46,820
of the class in school the discrete blocks in

1362
01:34:51,620 --> 01:34:55,320
there's a strong connection between the discrete laplacian and random walks

1363
01:34:55,370 --> 01:34:57,030
so if i take any

1364
01:34:57,040 --> 01:34:58,350
a weighted graph

1365
01:34:58,360 --> 01:35:01,510
undirected graph and random walk is just

1366
01:35:01,560 --> 01:35:03,860
the probability of visiting any neighbour

1367
01:35:03,870 --> 01:35:07,790
with probability proportional to one by degree of the vertex

1368
01:35:07,850 --> 01:35:12,710
the two kinds of plants intimately familiar with other combinatorial applies in those mentioned in

1369
01:35:12,720 --> 01:35:13,770
the first talk

1370
01:35:13,780 --> 01:35:18,820
and the normalized laplacian and you can easily show the normalized policy n

1371
01:35:18,840 --> 01:35:24,680
has its spectral structure very closely related to the random walk matrix and that's a

1372
01:35:24,680 --> 01:35:27,590
fairly easy identity show

1373
01:35:29,360 --> 01:35:32,280
this is classical fourier analysis on graphs

1374
01:35:32,300 --> 01:35:37,140
and let's see how it works on this problem before we look at what limitations

1375
01:35:37,140 --> 01:35:38,800
it has

1376
01:35:38,820 --> 01:35:43,230
so first let's look like in plus in on three d objects here some lori

1377
01:35:43,230 --> 01:35:45,010
connectors on the skull had

1378
01:35:45,020 --> 01:35:49,560
here's the higher order i lectures the two things are not one is generally the

1379
01:35:49,560 --> 01:35:54,190
supported by conductors the whole object so these basis functions are global demand not local

1380
01:35:54,280 --> 01:35:55,960
this is actually going to be a problem

1381
01:35:56,180 --> 01:36:01,560
and second you see the higher order argument is generally start eliminating high frequency regions

1382
01:36:01,560 --> 01:36:04,890
and so as you look at the argument personal or high order

1383
01:36:04,930 --> 01:36:10,300
you will see that they essentially have more variation and

1384
01:36:10,320 --> 01:36:13,080
they get less small

1385
01:36:13,090 --> 01:36:16,380
so the process is very simple you take a target function

1386
01:36:16,390 --> 01:36:20,600
you projected on the first few i vectors of the class and if you do

1387
01:36:20,600 --> 01:36:25,500
in linear least squares fitting non-linear least squares you can select the documents are order

1388
01:36:25,520 --> 01:36:28,600
and this is standard fourier

1389
01:36:28,930 --> 01:36:33,360
this discourse projection so here's an example of the pig and his original reconstruction of

1390
01:36:33,360 --> 01:36:35,040
the hundred basis functions

1391
01:36:35,060 --> 01:36:38,750
and his with a hundred basis functions

1392
01:36:38,800 --> 01:36:43,350
there's been a lot of follow-on work about with this approach in computer graphics where

1393
01:36:43,350 --> 01:36:48,060
people have to apply this to do things like spectral watermarking and various other things

1394
01:36:48,410 --> 01:36:51,460
there some variants of this approach that i will talk about so what are the

1395
01:36:51,460 --> 01:36:53,420
limitations of this approach

1396
01:36:53,430 --> 01:36:57,990
the limitations of this approach primarily have to do with the limitations of fourier analysis

1397
01:36:58,740 --> 01:37:03,330
one of the problems with doing i can vector approximations of these core functions is

1398
01:37:03,330 --> 01:37:08,240
that when you have functions that have very varying degree of smoothness it's very hard

1399
01:37:08,240 --> 01:37:11,710
to get them right so here you have the horns of the cow

1400
01:37:11,720 --> 01:37:14,470
which are very sharp

1401
01:37:14,520 --> 01:37:18,710
and you also have very smooth regions and you're trying to get large

1402
01:37:18,720 --> 01:37:24,610
i can vectors to correspond to approximate these sort of a shop regions while retaining

1403
01:37:24,610 --> 01:37:26,920
fidelity in the very small regions

1404
01:37:26,920 --> 01:37:29,070
especially for long

1405
01:37:29,070 --> 01:37:33,400
they will have very small number of users point or the point of view would

1406
01:37:33,400 --> 01:37:35,030
be useless and

1407
01:37:35,050 --> 01:37:39,510
remember that point separated b so it will be extraction

1408
01:37:39,530 --> 01:37:43,490
what you want this naive implementation is feasible to put into

1409
01:37:43,510 --> 01:37:46,070
random access memory and to use

1410
01:37:46,090 --> 01:37:48,590
so what we can do

1411
01:37:48,690 --> 01:37:50,650
we can

1412
01:37:50,650 --> 01:37:52,440
you can compress

1413
01:37:52,490 --> 01:37:54,440
we can compress the this is

1414
01:37:54,460 --> 01:37:56,760
the simplest approach what we can do

1415
01:37:56,820 --> 01:37:58,570
you can say OK

1416
01:37:58,570 --> 01:38:01,550
most of the work especially the end

1417
01:38:01,610 --> 01:38:04,090
they have the same

1418
01:38:04,090 --> 01:38:05,590
we have already

1419
01:38:05,610 --> 01:38:08,820
when we have only one point to the next right

1420
01:38:08,840 --> 01:38:14,010
again remove find special additional type of node history

1421
01:38:14,030 --> 01:38:18,110
and if you looking for kat the same example

1422
01:38:18,130 --> 01:38:21,880
in this area we have terminal also meaning that this is what

1423
01:38:21,880 --> 01:38:25,030
but if you looking for constantly

1424
01:38:25,070 --> 01:38:28,470
really it from this arabian coming to

1425
01:38:28,490 --> 01:38:33,110
this training and although there is no any other branches everything but a up

1426
01:38:34,510 --> 01:38:36,360
this approach

1427
01:38:36,400 --> 01:38:40,440
we have the same the same complexity we need to check exactly the same number

1428
01:38:40,460 --> 01:38:42,550
of characters

1429
01:38:42,570 --> 01:38:45,340
while size of memory

1430
01:38:45,400 --> 01:38:49,380
reduces dramatically and this talk it's important we now

1431
01:38:49,400 --> 01:38:52,110
can feet everything into

1432
01:38:52,130 --> 01:38:54,970
into our random access memory

1433
01:38:55,030 --> 01:38:56,360
also have

1434
01:38:56,360 --> 01:38:58,490
dry structures they

1435
01:38:58,530 --> 01:39:04,210
i have a lot of what the advantages because you can read them again incinerate

1436
01:39:04,210 --> 01:39:06,510
alphabetically sorted

1437
01:39:09,940 --> 01:39:12,510
but all of them the only one so

1438
01:39:12,550 --> 01:39:16,820
the biggest disadvantage is that

1439
01:39:16,840 --> 01:39:20,300
a lot of allocation of memories so there are some

1440
01:39:20,320 --> 01:39:25,070
additional memory but it's actually an avoidable should are doing if you are going to

1441
01:39:25,070 --> 01:39:27,840
do research in the dictionary

1442
01:39:29,550 --> 01:39:32,490
basically we always have some

1443
01:39:32,490 --> 01:39:37,690
not uniform but memory access but it's again in many senses and would you will

1444
01:39:37,690 --> 01:39:38,970
always have some

1445
01:39:39,050 --> 01:39:44,050
not a local memory access pattern in dictionary

1446
01:39:44,070 --> 01:39:47,030
OK so another approach what we can do

1447
01:39:47,030 --> 01:39:50,090
we can use that is completely another approach

1448
01:39:50,110 --> 01:39:55,210
in all previous slides we did some checking with some results we went to

1449
01:39:55,230 --> 01:39:57,050
to the right to the left

1450
01:39:57,070 --> 01:40:00,760
but what we can do can we can completely avoid

1451
01:40:00,780 --> 01:40:01,760
they can

1452
01:40:01,780 --> 01:40:03,920
take this work at

1453
01:40:03,970 --> 01:40:08,510
then calculate the hash function that has all

1454
01:40:08,530 --> 01:40:10,840
all of this war

1455
01:40:10,840 --> 01:40:14,530
and great some small numbers of small index

1456
01:40:14,550 --> 01:40:19,440
let's see this has a hash function that exist in the world simply too

1457
01:40:19,440 --> 01:40:22,510
some together owing according to all

1458
01:40:22,510 --> 01:40:23,920
characters of this work

1459
01:40:23,920 --> 01:40:28,050
it's not important but because it it doesn't

1460
01:40:28,050 --> 01:40:32,460
and around the place on the order of of characters in the world so you

1461
01:40:32,460 --> 01:40:35,260
can use something more complex but if you are you

1462
01:40:35,280 --> 01:40:37,240
if you use

1463
01:40:37,260 --> 01:40:39,860
somewhere in a prose you can do you can do it

1464
01:40:39,880 --> 01:40:41,530
and then

1465
01:40:41,550 --> 01:40:43,240
simply jump to

1466
01:40:43,300 --> 01:40:45,010
every element

1467
01:40:45,190 --> 01:40:50,050
this indicates that you got from this from this calculation

1468
01:40:50,110 --> 01:40:54,820
so sociology very fast only calculating fishpond

1469
01:40:54,840 --> 01:40:55,940
in so far

1470
01:40:55,940 --> 01:41:01,650
it's extremely fast too because simply doing this calculation and go to this element in

1471
01:41:01,650 --> 01:41:04,840
the input data

1472
01:41:07,510 --> 01:41:09,960
actually have

1473
01:41:10,010 --> 01:41:11,550
have to

1474
01:41:11,570 --> 01:41:13,510
the problem with this

1475
01:41:13,540 --> 01:41:16,440
so we need to select good point because

1476
01:41:16,460 --> 01:41:21,210
if you have and like it it's possible that huge number of towards getting the

1477
01:41:21,210 --> 01:41:23,880
same partition disarray

1478
01:41:23,880 --> 01:41:25,460
and it's not cool

1479
01:41:25,530 --> 01:41:30,150
you to call to avoid this but it's unavoidable because we're trying to

1480
01:41:30,170 --> 01:41:34,630
lose information using a lot of the compression

1481
01:41:34,650 --> 01:41:37,440
this is the first problem the second problem

1482
01:41:37,470 --> 01:41:41,960
in the two problems are connected to the size of the story

1483
01:41:42,010 --> 01:41:48,170
many scripting languages are using fashion default implementation of

1484
01:41:48,210 --> 01:41:52,170
map of structures that maps key to win

1485
01:41:53,090 --> 01:41:56,550
and name some of them is problem quite

1486
01:41:56,550 --> 01:42:00,920
why they're doing this extremely exactly because it fast

1487
01:42:02,380 --> 01:42:06,260
and in all scripting languages you're doing a lot of research and so so that

1488
01:42:06,260 --> 01:42:07,380
if you're

1489
01:42:07,420 --> 01:42:12,110
programmer only programmers here

1490
01:42:13,030 --> 01:42:20,050
if you are a programmer and you want to implement information retrieval system using

1491
01:42:20,070 --> 01:42:23,510
i there's always using something like this adding for

1492
01:42:23,510 --> 01:42:25,550
or if you using

1493
01:42:25,570 --> 01:42:28,510
get the same the same situation

1494
01:42:28,530 --> 01:42:33,860
the problem is what all doing if they have a college

1495
01:42:33,860 --> 01:42:36,530
if suddenly optics

1496
01:42:36,550 --> 01:42:38,280
for example dogs

1497
01:42:38,280 --> 01:42:43,740
comes with booklet full function and we see that in the same

1498
01:42:43,760 --> 01:42:47,510
cell of the array in the same element

1499
01:42:47,510 --> 01:42:49,760
we already have

1500
01:42:49,760 --> 01:42:51,570
don't care

1501
01:42:51,610 --> 01:42:53,590
so what are going to do

1502
01:42:53,610 --> 01:42:55,840
well it's pretty simple thing

1503
01:42:55,860 --> 01:42:57,530
in this case is simply

1504
01:42:57,550 --> 01:43:00,880
incremental size of this series twice

1505
01:43:00,900 --> 01:43:04,240
and reallocate elements new position

1506
01:43:04,240 --> 01:43:07,340
it's OK for scripting language

1507
01:43:07,360 --> 01:43:11,570
but if you are implemented by playing out a search engine

1508
01:43:11,570 --> 01:43:14,510
very soon when in this in this

1509
01:43:16,440 --> 01:43:20,340
you see that not only is system became extremely slow

1510
01:43:20,360 --> 01:43:21,400
not only

1511
01:43:21,420 --> 01:43:28,260
you're all tiny bit application that is very interesting but automation system is going down

1512
01:43:28,300 --> 01:43:30,740
because the size of the array

1513
01:43:30,760 --> 01:43:32,470
became so so huge

1514
01:43:32,510 --> 01:43:36,030
but it doesn't fit into memory in the process trying to

1515
01:43:36,050 --> 01:43:38,170
so this problem doing a lot

1516
01:43:38,230 --> 01:43:40,150
is that a lot of walking

1517
01:43:40,190 --> 01:43:45,150
removing weight from memory to the skin trying to relocate

1518
01:43:45,170 --> 01:43:50,860
and i have a lot of people who all around into this problem the programming

1519
01:43:51,050 --> 01:43:52,960
problem the try to

1520
01:43:52,960 --> 01:43:56,410
decision tree well that's the kind of thing is right

1521
01:43:56,460 --> 01:43:59,700
here we're trying to do is say OK what's the analogue for

1522
01:43:59,740 --> 01:44:05,390
for clustering and analog here is the property if the relation between the target and

1523
01:44:05,390 --> 01:44:10,240
the similarity function or science what relations give us enough information then we can hope

1524
01:44:10,240 --> 01:44:14,820
to cluster well and by what kind of

1525
01:44:14,910 --> 01:44:19,650
so i'm not coming the properties we haven't necessarily you know

1526
01:44:19,710 --> 01:44:23,490
the only ones one continues but so we're trying to figure out

1527
01:44:23,500 --> 01:44:27,910
what this theory looks like you know what kind of properties are sufficient for clustering

1528
01:44:27,970 --> 01:44:32,250
what properties are not and right now we've been looking at history property where you

1529
01:44:32,250 --> 01:44:36,960
can search the street conditions for your goals to produce tree you can look at

1530
01:44:36,960 --> 01:44:39,710
other kind of goals

1531
01:44:39,770 --> 01:44:45,540
you can relate the two implicit assumptions made by approximation algorithms percent objectives like k

1532
01:44:45,540 --> 01:44:50,430
median k means so if you're using an algorithm with approximation of one of these

1533
01:44:50,760 --> 01:44:56,680
implicitly you're hoping any approximate solution to this objective must actually be close to the

1534
01:44:56,680 --> 01:45:00,310
target function is that's the reason trying to optimize the objective

1535
01:45:00,360 --> 01:45:03,510
and if you make it explicit if you say look i'm just scanner

1536
01:45:03,550 --> 01:45:08,760
i'm using this argument because i'm hoping that any approximate solution of the objective function

1537
01:45:08,760 --> 01:45:10,040
is close to the target

1538
01:45:10,060 --> 01:45:13,350
and that you can prove that implies your data

1539
01:45:13,360 --> 01:45:17,770
most data points have to actually be strictly more similar to points of their labelled

1540
01:45:17,780 --> 01:45:22,300
another label the most have to have that condition can kinda connect these two the

1541
01:45:22,300 --> 01:45:26,000
simplest things you're something which is

1542
01:45:26,150 --> 01:45:30,590
this is just and so in this part we looking at what properties similarity function

1543
01:45:30,610 --> 01:45:33,190
are sufficient to be useful for clustering

1544
01:45:33,230 --> 01:45:37,660
reviewing the unlabelled data multiclass learning problem

1545
01:45:37,720 --> 01:45:42,740
the an interesting theory we had to relax we mean by useful useful means you're

1546
01:45:42,950 --> 01:45:47,310
producing just the right answer you really need strong conditions particularly organ a little slack

1547
01:45:47,330 --> 01:45:50,720
you say look it can produce the tree like the user figure out how specifically

1548
01:45:50,720 --> 01:45:55,410
meant to be then you can start saying all these conditions are sufficient

1549
01:45:55,430 --> 01:45:58,120
it this a kind of PAC model for clustering

1550
01:45:58,220 --> 01:46:03,450
these live interesting directions to explore here something we've just been looking at relatively recently

1551
01:46:03,510 --> 01:46:05,430
and i just want to mention some

1552
01:46:05,490 --> 01:46:07,350
nine research question

1553
01:46:08,080 --> 01:46:13,930
for instance very natural a very powerful class clustering algorithms are local spectral methods that

1554
01:46:14,030 --> 01:46:18,200
use i conductors and we haven't been able to come up with really natural properties

1555
01:46:19,470 --> 01:46:23,540
you know you say look if you're someone has this property then that spectral algorithm

1556
01:46:23,540 --> 01:46:26,220
will be well you can of course to find the property b

1557
01:46:26,600 --> 01:46:28,490
the property of the algorithm does

1558
01:46:28,540 --> 01:46:29,660
so you can say

1559
01:46:29,660 --> 01:46:34,010
i algorithm and the problem on the algorithm work to find them

1560
01:46:34,180 --> 01:46:37,120
a simple theorem is that has the property out

1561
01:46:37,220 --> 01:46:41,930
we are going natural properties something to look at the natural thing if you satisfy

1562
01:46:41,950 --> 01:46:45,660
that motivates the structure so we don't really know

1563
01:46:45,660 --> 01:46:50,930
this large surface could we don't really have very efficient algorithm especially when the topic

1564
01:46:51,930 --> 01:46:55,030
be nice to be able to do that

1565
01:46:55,030 --> 01:47:00,330
also this notion of usefulness so one thing i think it's quite interesting is

1566
01:47:00,330 --> 01:47:02,890
you could relax the notion of the tree

1567
01:47:02,910 --> 01:47:05,430
saints that's enough to produce small

1568
01:47:05,450 --> 01:47:07,510
directed acyclic graphs

1569
01:47:09,510 --> 01:47:12,430
for example if you go to our website for

1570
01:47:12,450 --> 01:47:14,600
the company like

1571
01:47:14,660 --> 01:47:16,200
the cell stuff like

1572
01:47:16,200 --> 01:47:18,510
that's buyer whatever company so

1573
01:47:18,510 --> 01:47:24,400
electronic that have little navigation bar where you can you navigate through the through the

1574
01:47:24,400 --> 01:47:25,450
items they sell

1575
01:47:25,470 --> 01:47:29,620
so have to be like a tree and so you can look at camera or

1576
01:47:29,620 --> 01:47:31,490
TV's are

1577
01:47:31,530 --> 01:47:35,970
refrigerators are selling but you can also search by price you can go there two

1578
01:47:36,010 --> 01:47:40,280
a hundred dollars a hundred and fifty whatever and and then you can you can

1579
01:47:40,280 --> 01:47:44,140
get to the cameras between one hundred two fifty either by cameras first immensely my

1580
01:47:44,160 --> 01:47:48,970
price price first then by can so the kind of organizing this stuff in that

1581
01:47:48,970 --> 01:47:52,800
structure and with and so it's a little bit you know way to think of

1582
01:47:52,910 --> 01:47:57,300
what's really happening is that having a single clustering you have a bunch of users

1583
01:47:57,350 --> 01:48:00,810
and each user has a cluster they're interested in like i might be interested in

1584
01:48:00,830 --> 01:48:03,260
cameras between two hundred fifty dollars

1585
01:48:03,310 --> 01:48:07,510
what i want is that the natural clusters are somehow living in the structure so

1586
01:48:07,510 --> 01:48:09,580
you be interesting to try to develop

1587
01:48:09,600 --> 01:48:11,160
the theory that

1588
01:48:11,180 --> 01:48:13,470
or other kinds of feedback so

1589
01:48:13,490 --> 01:48:17,100
the kind of feedback we're talking about now is the user really feedback to the

1590
01:48:17,930 --> 01:48:21,640
can then decide how far they want to click that it's a certain kind of

1591
01:48:24,160 --> 01:48:28,640
here's deciding how specifically meant to be you can imagine other kinds of feedback control

1592
01:48:30,490 --> 01:48:32,950
and so that's this is just

1593
01:48:33,030 --> 01:48:35,530
going back to the first part of some

1594
01:48:35,530 --> 01:48:39,390
the references related

1595
01:48:39,410 --> 01:48:41,390
all right so

1596
01:48:43,040 --> 01:48:46,870
so if we can just have questions or

1597
01:48:47,370 --> 01:48:50,760
but the things that

1598
01:49:04,330 --> 01:49:18,720
right so if you also had some knowledge about what the person one is like

1599
01:49:18,740 --> 01:49:21,350
there are really only interested in cluster bigger than a certain

1600
01:49:21,370 --> 01:49:23,410
five you can throw out

1601
01:49:23,410 --> 01:49:24,850
the parts the tree

1602
01:49:24,890 --> 01:49:26,720
so we were

1603
01:49:26,760 --> 01:49:30,890
the kind of property looking at were only making the assumption that for instance

1604
01:49:31,040 --> 01:49:33,180
everybody in the same cluster is

1605
01:49:33,180 --> 01:49:33,850
you know

1606
01:49:33,870 --> 01:49:37,530
the axes more similar to everybody else the same cluster anybody else we didn't assume

1607
01:49:37,530 --> 01:49:41,660
anything about how big question maybe x its own cluster by

1608
01:49:43,490 --> 01:49:47,930
given only the information we really want to produce poultry we make of no assumption

1609
01:49:47,930 --> 01:49:51,030
about what user one certainly if you had some other

1610
01:49:51,080 --> 01:49:54,700
no information about what the user wants like they said

1611
01:49:54,760 --> 01:49:57,060
furthermore every cluster size

1612
01:49:57,080 --> 01:50:01,800
a certain amount and you could print out things i think that's interesting question you

1613
01:50:01,800 --> 01:50:03,280
know whether this other

1614
01:50:03,330 --> 01:50:07,530
prior or other kinds of information they tell you look i got the similarity function

1615
01:50:07,560 --> 01:50:13,030
i think it's good and furthermore my clusters are big furthermore something else something else

1616
01:50:13,180 --> 01:50:15,970
you'd like to be able to use all the information

1617
01:50:15,990 --> 01:50:20,620
so right now we basically only using the similarity function one could imagine

1618
01:50:20,680 --> 01:50:21,760
you know

1619
01:50:21,780 --> 01:50:22,560
trying to

1620
01:50:22,580 --> 01:50:28,450
together kind information to count two

1621
01:50:32,010 --> 01:50:34,970
more so

1622
01:50:41,060 --> 01:50:45,540
on the other side from which starts with the

1623
01:50:45,560 --> 01:50:46,930
what is

1624
01:50:47,080 --> 01:50:49,890
so just

1625
01:50:51,600 --> 01:50:52,530
is not

1626
01:50:52,580 --> 01:50:59,680
all songs which is essentially the same function the

1627
01:51:02,300 --> 01:51:03,830
seems to the

1628
01:51:05,830 --> 01:51:06,870
this is just

1629
01:51:07,930 --> 01:51:12,100
OK so so so so

1630
01:51:13,450 --> 01:51:17,700
which was some of the sources seem to be saying is

1631
01:51:17,760 --> 01:51:20,640
long beach

1632
01:51:20,720 --> 01:51:29,600
similar work was just to start with one to one

1633
01:51:29,600 --> 01:51:34,260
on the relation of

1634
01:51:40,360 --> 01:51:42,200
a major

1635
01:51:46,360 --> 01:51:53,000
i think of

1636
01:52:57,760 --> 01:53:01,380
i mean are

1637
01:53:51,130 --> 01:53:53,800
well known

1638
01:54:19,980 --> 01:54:22,640
on the nineteenth

1639
01:54:22,640 --> 01:54:26,750
because of web two point o

1640
01:54:27,510 --> 01:54:30,380
a lot of online communities

1641
01:54:30,390 --> 01:54:31,730
pop up

1642
01:54:31,730 --> 01:54:35,800
so on the relevant to topic is social networks

1643
01:54:35,840 --> 01:54:41,630
we briefly discuss these two things so you have the big picture about the environment

1644
01:54:41,630 --> 01:54:43,280
where the block sphere

1645
01:54:43,300 --> 01:54:44,390
it's OK

1646
01:54:45,630 --> 01:54:47,010
the wheel

1647
01:54:48,960 --> 01:54:54,210
the blogsphere so by looking at this this is also from the

1648
01:54:54,220 --> 01:55:01,470
well wikipedia and what do you think you can be used to that very different

1649
01:55:01,570 --> 01:55:05,690
this is like web versus web two point o when we had where we don't

1650
01:55:05,690 --> 01:55:10,690
have web one point o and then we have web two point o but nowadays

1651
01:55:10,690 --> 01:55:16,920
probably you've heard about web three point o o four point o x point that

1652
01:55:16,930 --> 01:55:21,490
it's really not as significant as this

1653
01:55:23,190 --> 01:55:26,770
but we just talk about alec away at one point two one can you tell

1654
01:55:26,800 --> 01:55:28,880
the difference

1655
01:55:28,890 --> 01:55:31,070
what is the key difference here

1656
01:55:32,770 --> 01:55:36,560
the key difference here is

1657
01:55:36,590 --> 01:55:39,400
we have a lot of consumers here

1658
01:55:39,490 --> 01:55:41,070
but over there we had a lot of

1659
01:55:43,680 --> 01:55:45,010
these producers

1660
01:55:45,020 --> 01:55:47,050
they also very willing

1661
01:55:47,070 --> 01:55:48,390
to share

1662
01:55:48,420 --> 01:55:50,280
the contents the predicts

1663
01:55:50,300 --> 01:55:56,070
that's the difference because of that difference so you see lal of

1664
01:55:56,130 --> 01:55:58,320
this social media

1665
01:55:58,330 --> 01:56:03,880
application pop up

1666
01:56:03,880 --> 01:56:07,050
so characteristics of web two point o

1667
01:56:07,060 --> 01:56:08,900
we have like a rich

1668
01:56:08,920 --> 01:56:14,260
internet applications user generated the contents

1669
01:56:14,270 --> 01:56:17,590
o you probably have this sorry i should

1670
01:56:18,510 --> 01:56:20,210
say this earlier

1671
01:56:20,220 --> 01:56:21,920
we have a copy of that

1672
01:56:22,890 --> 01:56:24,010
but i think

1673
01:56:24,020 --> 01:56:26,420
we summit notes we

1674
01:56:26,730 --> 01:56:29,870
i changed alot OK so

1675
01:56:29,950 --> 01:56:34,620
this is bad news and the good news both right so we change the lot

1676
01:56:34,620 --> 01:56:35,500
for better

1677
01:56:35,510 --> 01:56:37,560
for better for worse or better

1678
01:56:37,780 --> 01:56:43,500
and this character now we come back to talk about to this characteristics the web

1679
01:56:43,530 --> 01:56:50,950
two point of o rich internet applications lot of applications new applications user-generated content

1680
01:56:50,960 --> 01:56:53,570
using rich contents

1681
01:56:53,580 --> 01:56:54,750
i will

1682
01:56:54,760 --> 01:56:56,860
later i will show you some examples

1683
01:56:56,870 --> 01:56:59,860
and the user developed the widgets

1684
01:56:59,900 --> 01:57:03,550
then collaborative environment

1685
01:57:03,570 --> 01:57:06,260
participatory web and the citizen

1686
01:57:06,270 --> 01:57:10,470
journalism have we heard about the solution to the rest of you have heard about

1687
01:57:10,470 --> 01:57:14,070
these things reason a lot less

1688
01:57:14,130 --> 01:57:16,380
eleven richest power of the long tail

1689
01:57:16,390 --> 01:57:20,210
i was the user generated data that's really

1690
01:57:20,240 --> 01:57:23,070
where the power is OK this is

1691
01:57:24,570 --> 01:57:27,020
web two point o is more

1692
01:57:27,570 --> 01:57:30,450
paradigm shift then that technology shift

1693
01:57:30,450 --> 01:57:33,330
of course it stimulated by the

1694
01:57:34,920 --> 01:57:37,590
improvements right innovation

1695
01:57:37,610 --> 01:57:39,320
now these are some examples

1696
01:57:39,340 --> 01:57:42,570
these are some examples web two point of service

1697
01:57:42,630 --> 01:57:45,940
blogs blog posts wordpress

1698
01:57:45,990 --> 01:57:47,780
and wiki keep

1699
01:57:49,440 --> 01:57:51,520
we give us city

1700
01:57:51,550 --> 01:57:58,060
social networking sites how many of you have account to facebook myspace

1701
01:57:58,070 --> 01:57:59,960
all of you

1702
01:58:00,010 --> 01:58:01,330
ronnie to you have one

1703
01:58:01,450 --> 01:58:05,590
so which one could or facebook

1704
01:58:05,610 --> 01:58:11,990
facebook so you usually we have all sorts some accounts their social networking sites

1705
01:58:12,080 --> 01:58:16,630
and digital media sharing websites youtube flickr

1706
01:58:16,640 --> 01:58:18,470
social tagging

1707
01:58:18,520 --> 01:58:20,860
these are all social media

1708
01:58:20,880 --> 01:58:26,030
because of this of participatory for users voluntarily

1709
01:58:26,060 --> 01:58:30,090
two not of things amazingly many OK

1710
01:58:30,130 --> 01:58:31,630
you were not imagine

1711
01:58:31,640 --> 01:58:33,510
in the early days

1712
01:58:33,520 --> 01:58:38,840
and in other cities of the other things so many many applications

1713
01:58:38,870 --> 01:58:42,360
the top twenty most visited websites

1714
01:58:42,430 --> 01:58:44,740
hopefully you recognise some of them

1715
01:58:44,800 --> 01:58:47,780
but it's not the given by us by alex

1716
01:58:47,800 --> 01:58:52,800
and forty percent of the top twenty websites are

1717
01:58:52,830 --> 01:58:57,210
web two point o sec what does that mean that means that allows these sites

1718
01:58:57,210 --> 01:59:03,370
allow people to participate to contribute OK

1719
01:59:03,400 --> 01:59:05,200
this is the one

1720
01:59:05,230 --> 01:59:07,430
then we have

1721
01:59:07,440 --> 01:59:10,950
and more and more people flock to the web

1722
01:59:10,960 --> 01:59:16,330
using various applications i showed the social for some social purposes

1723
01:59:16,350 --> 01:59:18,990
i don't know for me i have like

1724
01:59:19,000 --> 01:59:21,300
however a teenager can

1725
01:59:21,310 --> 01:59:24,450
i have three kids but the genetic changes that

1726
01:59:24,610 --> 01:59:26,000
he prefers

1727
01:59:26,030 --> 01:59:27,690
to talk to his friends

1728
01:59:27,740 --> 01:59:29,150
three internet

1729
01:59:29,200 --> 01:59:33,720
he does not want to invite them to hold OK it's very probably you have

1730
01:59:33,720 --> 01:59:35,100
the same experience

1731
01:59:35,470 --> 01:59:37,230
so that that's the trend

1732
01:59:37,240 --> 01:59:38,800
that's the tremendous

1733
01:59:38,810 --> 01:59:41,790
people they still socialize

1734
01:59:41,820 --> 01:59:44,080
but they socialize in a different way

1735
01:59:44,090 --> 01:59:48,350
when i was young i was running out out of the street and played with

