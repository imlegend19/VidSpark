1
00:00:00,000 --> 00:00:02,400
so it's it's quite strong

2
00:00:02,420 --> 00:00:06,900
so that's the goal of maximum likelihood basically

3
00:00:07,860 --> 00:00:09,780
the idea behind the enemy's

4
00:00:09,800 --> 00:00:15,440
sometimes you have a set of functions or a set of distribution here that are

5
00:00:16,880 --> 00:00:19,590
that for which we we don't know how to

6
00:00:19,610 --> 00:00:22,280
to solve that task directly

7
00:00:22,300 --> 00:00:24,360
but it would help if we

8
00:00:24,360 --> 00:00:28,360
we were able to define to introduce a new it inviable viable for which we

9
00:00:28,360 --> 00:00:29,480
don't value

10
00:00:29,500 --> 00:00:32,130
but if we knew it

11
00:00:32,170 --> 00:00:34,340
it would help in solving the problem

12
00:00:34,340 --> 00:00:39,340
first and then the way to estimated according to the current solution that we have

13
00:00:39,340 --> 00:00:43,420
so we are going to have an iterative solution where we first decide that the

14
00:00:43,440 --> 00:00:47,440
solution is going to be given gushing mixture model with

15
00:00:47,460 --> 00:00:51,510
a given set of parameters according to the set of parameters i'm going to try

16
00:00:51,510 --> 00:00:56,420
to estimate this hidden viable which i'm going to tell you what it could be

17
00:00:56,440 --> 00:01:00,460
and that could according to that new estimating going to be able to find a

18
00:01:00,460 --> 00:01:06,510
new values for the parameters and i'm going to go on and on and on

19
00:01:06,530 --> 00:01:12,010
so e BEM is separated into two steps what people call the e step and

20
00:01:12,150 --> 00:01:18,610
m step expectation state is there to estimate

21
00:01:18,630 --> 00:01:22,250
currently what would be the best value of these hidden viable

22
00:01:22,270 --> 00:01:27,070
and the sense that is assuming that the best value is the current value what

23
00:01:27,070 --> 00:01:30,400
is now the best value of the parameters and i'm going to

24
00:01:30,460 --> 00:01:32,460
do that i can actively

25
00:01:32,480 --> 00:01:35,440
and what's interesting

26
00:01:35,460 --> 00:01:37,940
is that you can show

27
00:01:37,960 --> 00:01:39,400
it converges to

28
00:01:39,400 --> 00:01:45,480
to keep the local optima what does that mean exactly so

29
00:01:45,480 --> 00:01:50,340
for gaussian mixture models we're going to decide that the heat viable

30
00:01:50,360 --> 00:01:52,780
is the information that

31
00:01:52,840 --> 00:01:57,710
someone is going to tell you that i suppose you have all these points

32
00:01:57,730 --> 00:02:05,050
and you are searching for the best mixture of two gaussians that would have generated

33
00:02:06,430 --> 00:02:10,420
the so the around here are going to present my two goshen and of course

34
00:02:10,420 --> 00:02:11,880
at the beginning i don't know

35
00:02:11,920 --> 00:02:14,050
what we should they be so

36
00:02:14,070 --> 00:02:17,960
i just put them in a way that there are better ways to do that

37
00:02:17,980 --> 00:02:20,920
but let's make this i put them somewhere

38
00:02:20,940 --> 00:02:22,630
and now

39
00:02:22,650 --> 00:02:27,420
i'm going to to do the following two steps first i'm going to assume that

40
00:02:27,420 --> 00:02:31,710
each of the point has indeed been generated by one of the gaussians but i

41
00:02:31,710 --> 00:02:32,710
just don't know

42
00:02:32,730 --> 00:02:34,860
which one

43
00:02:35,530 --> 00:02:39,530
and i'm going to call that my hidden variables because i don't know that

44
00:02:40,530 --> 00:02:45,000
you you will agree with me that if you knew the information so if i

45
00:02:45,000 --> 00:02:49,860
tell you that this point was generated by this one while this one was generated

46
00:02:49,860 --> 00:02:55,320
by this way you could have two separate problem you would take all the points

47
00:02:55,340 --> 00:02:57,630
generated by gulshan one

48
00:02:57,650 --> 00:03:00,210
eight and compute the mean

49
00:03:00,230 --> 00:03:06,440
and commercial matrix of that goshen separately and you do that for each goshen in

50
00:03:06,440 --> 00:03:10,600
fact afterwards the weight would be very easy to compute it would be simply the

51
00:03:11,280 --> 00:03:16,010
ratio between the number of points of caution so it's in need that to solve

52
00:03:16,010 --> 00:03:19,500
the the gaussian mixture model problem

53
00:03:19,530 --> 00:03:23,110
if you knew which point was generated by which

54
00:03:23,710 --> 00:03:25,960
but you don't know of course

55
00:03:27,730 --> 00:03:31,070
the first thing that you're going to do is the e step

56
00:03:31,090 --> 00:03:35,880
and the e step is basically going to try to estimate that he arrival so

57
00:03:35,880 --> 00:03:39,070
i'm going to estimate what is the probability

58
00:03:39,070 --> 00:03:45,320
that that point was generated by this discussion or that goshen so i'm going to

59
00:03:45,320 --> 00:03:50,550
compute for that pointer pretty of goshen and for that point to pretty of goshen

60
00:03:52,570 --> 00:03:56,010
and i'm going to do that for all the points

61
00:03:56,030 --> 00:03:58,730
and and that's the e step

62
00:03:59,400 --> 00:04:03,110
and and then

63
00:04:03,300 --> 00:04:07,610
m step is that now that i know

64
00:04:07,630 --> 00:04:08,940
somehow which

65
00:04:08,960 --> 00:04:11,860
caution generated which point

66
00:04:11,880 --> 00:04:17,400
i'm going to modify the goshen so i'm going to move my migration toward the

67
00:04:17,400 --> 00:04:23,650
points that were generated by the that goshen so too as to maximize that pretty

68
00:04:23,650 --> 00:04:27,880
to have been generated the point that the step

69
00:04:27,900 --> 00:04:29,550
now this is

70
00:04:29,570 --> 00:04:31,340
very nice graphically

71
00:04:31,360 --> 00:04:35,590
but there is also a more formal way to show it

72
00:04:35,590 --> 00:04:36,800
i'm going to

73
00:04:37,190 --> 00:04:41,210
to show it very quickly

74
00:04:41,230 --> 00:04:46,300
more formally in fact what we do is so we have this invaluable

75
00:04:46,300 --> 00:04:48,480
which we call q here

76
00:04:48,500 --> 00:04:49,250
and we

77
00:04:49,270 --> 00:04:54,190
we have to define it and that that you are in the EMI grid nobody's

78
00:04:54,190 --> 00:04:58,940
going to tell you how to define this inviable but this is the only difficult

79
00:04:58,940 --> 00:05:02,840
part define it as an opportunity finally there's no longer than the flows

80
00:05:02,860 --> 00:05:04,920
and if you solution

81
00:05:04,920 --> 00:05:08,210
so let's suppose that someone gave you this

82
00:05:08,230 --> 00:05:09,730
information about the

83
00:05:09,750 --> 00:05:16,530
what is going to be the hidden variables you create artificially function that you call

84
00:05:16,550 --> 00:05:22,400
the auxiliary function of two sets of parameters and i'm going to explain what they

85
00:05:22,400 --> 00:05:27,070
are and these auxiliary function is is going to be expected value over the hidden

86
00:05:28,280 --> 00:05:32,800
over all the possible values of the variable of the log likelihood of the joint

87
00:05:32,800 --> 00:05:38,070
of your data and the hidden variable given one set of data

88
00:05:38,070 --> 00:05:39,030
and the whole

89
00:05:39,030 --> 00:05:45,420
the whole thing is given the data and the other set of parameters this looks

90
00:05:45,420 --> 00:05:49,480
very complex constraints but

91
00:05:49,500 --> 00:05:54,880
we can we can work with that but what's very interesting is that

92
00:05:54,880 --> 00:05:59,640
like dialect of form of predicate logic and first-order logic and by that all i

93
00:05:59,640 --> 00:06:04,380
mean is that in the situation calculus certain predicates means so special

94
00:06:04,390 --> 00:06:07,270
things but otherwise it's just first order logic

95
00:06:07,280 --> 00:06:09,900
so we could try to this in dynamic logic we could try to do this

96
00:06:09,900 --> 00:06:11,840
in many other logics and that's

97
00:06:13,080 --> 00:06:16,380
and it's the way of describing these dynamic systems in

98
00:06:16,430 --> 00:06:19,050
situation calculus in a sort of fairly

99
00:06:19,060 --> 00:06:21,590
the simple form was first proposed

100
00:06:21,680 --> 00:06:23,500
by john mccarthy in

101
00:06:23,510 --> 00:06:26,840
late fifties situation calculus in this sort of film has been around for a long

102
00:06:27,610 --> 00:06:30,840
it wasn't until later in the eighties and so that it was actually developed if

103
00:06:30,840 --> 00:06:34,580
a bit more to the point where can

104
00:06:34,630 --> 00:06:36,320
OK now

105
00:06:36,330 --> 00:06:40,450
the terms in our logics of the terms in in the situation calculus there are

106
00:06:40,450 --> 00:06:42,270
two important ties terms

107
00:06:42,320 --> 00:06:45,590
once in terms of actions that we can perform

108
00:06:45,650 --> 00:06:49,640
and the other ones are these things called situations which i look at the moment

109
00:06:49,640 --> 00:06:54,180
situations are essentially states that a little bit more complicated than that

110
00:06:54,190 --> 00:06:57,670
i can as state stress situations are going to be composed of these things called

111
00:06:57,670 --> 00:07:01,310
fly swatter florence florence adjust properties of the world

112
00:07:01,340 --> 00:07:03,890
that describe what's true

113
00:07:04,030 --> 00:07:07,170
particular situation in a particular state

114
00:07:07,190 --> 00:07:10,100
so for instance now you know my hand is empty

115
00:07:10,110 --> 00:07:11,390
pick up the cup

116
00:07:11,430 --> 00:07:13,010
you know my hand is

117
00:07:13,030 --> 00:07:14,500
no longer

118
00:07:14,510 --> 00:07:15,790
OK so

119
00:07:15,850 --> 00:07:19,680
the status of my hand is like floor because it can change value from one

120
00:07:19,680 --> 00:07:25,250
situation to the next after performing action it may change the truth value of this

121
00:07:25,260 --> 00:07:30,500
of of this pretty good so just predicates essentially that can change value y can

122
00:07:30,500 --> 00:07:33,610
cannot change truth value because when you perform an action

123
00:07:33,640 --> 00:07:37,900
the action change the truth value that's all sorts of things change

124
00:07:41,160 --> 00:07:41,920
OK now

125
00:07:41,940 --> 00:07:45,490
i just put this method one the two on the screen to show two different

126
00:07:45,490 --> 00:07:48,630
ways of describing particular situations

127
00:07:48,960 --> 00:07:54,180
we're going to adopt the first method the only reason i put the second method

128
00:07:54,180 --> 00:07:57,110
and is because he is commonly cited in the literature

129
00:07:57,180 --> 00:07:58,600
there's really no

130
00:08:00,500 --> 00:08:02,980
so the first thing that we're going to do is we're going to make the

131
00:08:03,000 --> 00:08:07,970
states of the things that we call situations first class citizens analogy so we're making

132
00:08:09,130 --> 00:08:12,120
so we can talk about remember i said we don't want to talk about what's

133
00:08:12,120 --> 00:08:13,460
true now

134
00:08:13,480 --> 00:08:16,660
but we also want to talk about what is true in the past and what

135
00:08:16,660 --> 00:08:20,610
might be true in the future if i take this particular sequence of actions of

136
00:08:20,610 --> 00:08:22,310
this course

137
00:08:22,370 --> 00:08:23,970
OK so

138
00:08:24,010 --> 00:08:27,830
a particular form like on so on is the floor and he just says look

139
00:08:27,830 --> 00:08:29,710
this object the the cause c

140
00:08:29,720 --> 00:08:32,950
it's on this subject i call a in s one

141
00:08:33,010 --> 00:08:35,800
a situation as well

142
00:08:35,810 --> 00:08:40,690
see these are just blocks is

143
00:08:40,720 --> 00:08:47,330
also A's on this thing which i'm going to call the table

144
00:08:47,340 --> 00:08:51,970
based on the table

145
00:08:52,290 --> 00:08:56,210
basically there's nothing out of the sea it's clear there's nothing

146
00:08:56,220 --> 00:08:57,080
OK so

147
00:08:57,090 --> 00:08:58,420
this is a particularly

148
00:08:58,430 --> 00:09:01,230
stable situation

149
00:09:01,270 --> 00:09:04,990
and i've described using this platform is about

150
00:09:05,040 --> 00:09:08,890
so this is not the sort of made this situation is sort of more abstract

151
00:09:10,260 --> 00:09:16,310
an object here has about this one so it's constant

152
00:09:16,360 --> 00:09:18,720
another way is to use this holds predicate

153
00:09:18,760 --> 00:09:22,440
we make on instead of making on the predictor we make it function

154
00:09:23,630 --> 00:09:27,860
we say that property on same being on a hold in this one

155
00:09:27,900 --> 00:09:29,100
so the others

156
00:09:29,110 --> 00:09:33,510
but as i just put that there is an example of an otherwise see literature

157
00:09:33,520 --> 00:09:36,510
i'm going to stick with this one

158
00:09:36,520 --> 00:09:40,330
o k actions in the situation calculus name so we can have actions and arguments

159
00:09:40,330 --> 00:09:44,220
like put x y so put the object on top of the object one

160
00:09:44,240 --> 00:09:47,940
move x from lot white blood cells

161
00:09:47,960 --> 00:09:54,710
clear so clear thing on top of their their actions before

162
00:09:54,720 --> 00:09:58,760
OK now situation is a snapshot of the world particular point in time

163
00:09:58,800 --> 00:10:03,170
we're not winning k one in time it is so we're not really interested in

164
00:10:03,230 --> 00:10:07,160
in time so much in this formalism so for instance we don't talk about how

165
00:10:07,160 --> 00:10:09,480
long we might stay in a particular state

166
00:10:09,500 --> 00:10:12,250
all we're interested in is that we're in a particular state

167
00:10:12,300 --> 00:10:15,030
we do an action we get some other state

168
00:10:15,090 --> 00:10:19,230
how long that transition takes how long we stay in any these particular states we

169
00:10:19,230 --> 00:10:20,190
don't care about

170
00:10:20,250 --> 00:10:26,120
it's just the ordering that were interested

171
00:10:26,130 --> 00:10:31,100
you will know that i get to that point six

172
00:10:33,350 --> 00:10:35,270
well let's let's get to announce so

173
00:10:35,330 --> 00:10:36,960
put it this way the

174
00:10:36,970 --> 00:10:42,330
by state i'm going to mean the collection of points that are true

175
00:10:42,380 --> 00:10:44,330
at a particular point

176
00:10:44,370 --> 00:10:49,180
now situation differs from state in the situation is just

177
00:10:51,300 --> 00:10:54,710
you know the things that are true

178
00:10:54,720 --> 00:10:57,890
but it's also a history of how you got

179
00:10:57,910 --> 00:11:02,110
so the difference between the situation in the state is defined

180
00:11:02,110 --> 00:11:05,020
different bird types so instead of having

181
00:11:06,600 --> 00:11:08,780
one to the other we take a run

182
00:11:08,800 --> 00:11:14,390
of one type of murder and then run of another type of mercer we cluster

183
00:11:14,450 --> 00:11:15,290
this is

184
00:11:15,300 --> 00:11:22,700
clustered arrangement of types into what we call these clusters are called blocks

185
00:11:22,730 --> 00:11:24,870
blocks of the same type of of more

186
00:11:24,880 --> 00:11:29,340
and such material is called a block copolymer

187
00:11:29,420 --> 00:11:32,010
a block copolymer that showing up here

188
00:11:32,020 --> 00:11:33,160
where you see

189
00:11:33,180 --> 00:11:38,910
there's what four units of a and then four units of two units of a

190
00:11:38,910 --> 00:11:42,780
four units of be this is a little bit unrealistic typically commercial

191
00:11:42,900 --> 00:11:47,510
block copolymer so usually die blocks are try blocks will usually have if it were

192
00:11:47,510 --> 00:11:48,650
dyed block of

193
00:11:48,660 --> 00:11:53,960
polyethylene within one run of polyethylene and one run of polyvinyl chloride and that's it

194
00:11:53,960 --> 00:11:54,860
you don't

195
00:11:54,870 --> 00:11:56,950
you don't do this sort of thing

196
00:11:56,960 --> 00:12:00,630
the artist got carried away and somebody didn't proofread so

197
00:12:00,640 --> 00:12:03,420
so there it is and the last thing you can do is the one on

198
00:12:03,420 --> 00:12:04,390
the bottom

199
00:12:04,470 --> 00:12:08,350
which is called the graph the graph this sort of a type

200
00:12:08,350 --> 00:12:10,960
it's the type of the block copolymer

201
00:12:11,850 --> 00:12:15,470
what you have is you have a grasp of

202
00:12:15,530 --> 00:12:18,450
one more time

203
00:12:18,510 --> 00:12:23,290
in the form of the side chains

204
00:12:23,330 --> 00:12:25,870
on two

205
00:12:26,160 --> 00:12:27,700
another type

206
00:12:27,710 --> 00:12:30,370
a backbone

207
00:12:30,380 --> 00:12:31,860
another type

208
00:12:31,860 --> 00:12:33,500
for the backbone

209
00:12:33,510 --> 00:12:36,540
so you cluster number two clusters the very

210
00:12:36,550 --> 00:12:39,550
deliberately in this manner that's shown in

211
00:12:39,560 --> 00:12:43,370
and the bottom one so the backbone is all ages and the side chain is

212
00:12:44,730 --> 00:12:47,880
and here's one and some of you are going to be doing some traveling side

213
00:12:47,930 --> 00:12:52,710
i've put up a very common one this is a

214
00:12:52,720 --> 00:13:00,360
acrylonitrile dying stiring so you've seen i'm so the last diary what what this is

215
00:13:00,410 --> 00:13:02,860
the backbone is all

216
00:13:02,870 --> 00:13:04,400
due to die

217
00:13:04,410 --> 00:13:07,270
and the side chains are alternately

218
00:13:07,320 --> 00:13:09,160
acrylamide trials

219
00:13:09,230 --> 00:13:13,670
and stiring and this is what used for hard sided luggage and also this all

220
00:13:13,670 --> 00:13:14,840
these old

221
00:13:15,890 --> 00:13:18,200
are made of this graph

222
00:13:19,660 --> 00:13:23,440
and by playing with the the nature of the side chains we can obviously make

223
00:13:23,440 --> 00:13:27,820
this so that it's somewhat flexible or we can make it so that's rather rich

224
00:13:27,860 --> 00:13:30,060
again examples of

225
00:13:30,060 --> 00:13:31,670
of architecture

226
00:13:31,730 --> 00:13:34,010
at at the local level

227
00:13:34,010 --> 00:13:37,430
dictating properties

228
00:13:39,840 --> 00:13:41,090
we have these

229
00:13:41,250 --> 00:13:44,200
examples of how we can tailor

230
00:13:44,250 --> 00:13:46,180
the property is the second thing is

231
00:13:46,790 --> 00:13:49,710
configurations of the side groups

232
00:13:49,810 --> 00:13:53,650
side group configuration

233
00:13:53,690 --> 00:13:56,130
these are all options that we have

234
00:13:56,140 --> 00:14:01,160
and we exercise them in the synthesis by playing with the synthesis conditions and also

235
00:14:01,160 --> 00:14:03,120
the catalysts that are present

236
00:14:03,160 --> 00:14:07,000
and that's shown here is basically three ways you can put the side of science

237
00:14:07,000 --> 00:14:08,970
so the the upper one

238
00:14:09,040 --> 00:14:10,580
the other one shows

239
00:14:11,220 --> 00:14:12,260
methyl group

240
00:14:12,420 --> 00:14:17,600
randomly going sometimes it's above sometimes it's blown there seems to be no pattern this

241
00:14:18,270 --> 00:14:24,080
a notion of side configuration is referred to as taxes

242
00:14:24,090 --> 00:14:25,600
texas city it means

243
00:14:25,610 --> 00:14:29,330
the placement of the side groups so you said i mean there's only three choices

244
00:14:29,330 --> 00:14:32,730
here you put them all on the same side of the chain that's the bottom

245
00:14:32,730 --> 00:14:37,520
one ISO tactic or you put them on alternate once it's on one side of

246
00:14:37,520 --> 00:14:41,100
the chain then it's on the other side of the change alternate back and forth

247
00:14:41,100 --> 00:14:46,530
with regularity that's india tactic in the third cases come what may there's no

248
00:14:46,540 --> 00:14:52,120
no seeming plan and that's the tactics of those of the those of the three

249
00:14:52,120 --> 00:14:53,400
types that you

250
00:14:53,450 --> 00:14:55,120
you need to be

251
00:14:55,160 --> 00:14:56,710
aware of

252
00:14:56,720 --> 00:14:58,800
i tactic

253
00:14:58,800 --> 00:15:01,210
cindy tactic

254
00:15:01,290 --> 00:15:04,210
and a tactic

255
00:15:04,260 --> 00:15:08,070
in figure those out i so obviously means they're all on the same side of

256
00:15:10,060 --> 00:15:15,100
a tactic as as a moral political so this is random

257
00:15:15,110 --> 00:15:20,300
and then by elimination this must be alternating

258
00:15:20,380 --> 00:15:22,690
alternating current

259
00:15:22,830 --> 00:15:27,470
in the next slide little doodling for so here's just to bring home what's going

260
00:15:27,470 --> 00:15:31,660
on the first one of the top is a tactic polypropylene so you see

261
00:15:31,680 --> 00:15:34,530
there's the there's properly

262
00:15:34,570 --> 00:15:38,710
and what what's happened is this double bond has been broken and now can propagate

263
00:15:38,780 --> 00:15:42,850
in this method group in some cases above in some cases below the chain here's

264
00:15:43,500 --> 00:15:48,370
polystyrene this vinyl benzene or phenol

265
00:15:48,670 --> 00:15:50,070
at the

266
00:15:50,110 --> 00:15:54,830
four styrene and the double bond is broken you get these feel groups hanging off

267
00:15:54,830 --> 00:15:58,040
the side and this is just vinyl chloride which we saw

268
00:15:58,090 --> 00:15:59,780
over here so

269
00:15:59,870 --> 00:16:02,610
again seeing how the the surfaces

270
00:16:02,650 --> 00:16:06,640
OK and the last thing we can look at is is the backbone

271
00:16:06,770 --> 00:16:08,830
his backbone architecture

272
00:16:08,840 --> 00:16:12,430
four chain architecture

273
00:16:12,450 --> 00:16:15,860
chain here referring to the main chain

274
00:16:18,030 --> 00:16:21,550
she architecture

275
00:16:21,560 --> 00:16:28,110
and this falls under the general terms of conformal mality

276
00:16:28,160 --> 00:16:33,930
con formality and we're going to revisit this we talk about proteins very important biological

277
00:16:33,930 --> 00:16:38,130
systems and it it it's the direct result of this which i showed you last

278
00:16:38,130 --> 00:16:41,050
day that this bond is free to rotate

279
00:16:41,060 --> 00:16:44,340
and if it rotates in certain ways

280
00:16:44,360 --> 00:16:45,800
so that all of the

281
00:16:45,970 --> 00:16:48,790
side groups line up perfectly

282
00:16:48,810 --> 00:16:52,810
you end up with this so-called eclipsed version so if you look at the chain

283
00:16:52,810 --> 00:16:58,090
on the very and you'd see one dot represents the backbone or actually zigzagging so

284
00:16:58,090 --> 00:16:59,020
you see the

285
00:16:59,150 --> 00:17:02,750
the edge of the zigzag and then all of the pendant groups would be lined

286
00:17:02,750 --> 00:17:06,460
up in front of one another you'd see nothing except the very first one that's

287
00:17:06,460 --> 00:17:11,650
the eclipse and that's the most regular straight line and this is staggered were that

288
00:17:11,650 --> 00:17:15,030
it rotates were wishes and i showed you this one where this is this is

289
00:17:15,030 --> 00:17:19,460
hardly something with the polarisation index of thirty six hundred but it starts giving you

290
00:17:19,460 --> 00:17:23,600
the sense of some more nearly straight and some are

291
00:17:23,620 --> 00:17:25,670
very heavily coil

292
00:17:25,690 --> 00:17:28,420
both of these are still called straight chains

293
00:17:28,430 --> 00:17:30,140
of these are still called

294
00:17:30,160 --> 00:17:31,880
straight change but

295
00:17:31,890 --> 00:17:35,550
there are some things that we can look at so first we have the linear

296
00:17:36,340 --> 00:17:37,300
the linear

297
00:17:37,380 --> 00:17:45,760
the linear chain molecule and

298
00:17:45,840 --> 00:17:46,980
so that's one

299
00:17:47,620 --> 00:17:52,290
and furthermore if you look here there some fine structure you may even see that

300
00:17:52,290 --> 00:17:56,670
at some point along here there's even some attempt at

301
00:17:56,680 --> 00:17:59,060
crystallisation but

302
00:17:59,060 --> 00:18:08,660
i don't think this is one

303
00:18:10,390 --> 00:18:12,130
this is a

304
00:18:13,160 --> 00:18:15,440
let's get

305
00:18:18,250 --> 00:18:21,680
said well i want to

306
00:18:25,880 --> 00:18:32,850
and so it proved

307
00:18:33,410 --> 00:18:37,540
all the

308
00:18:39,470 --> 00:18:43,270
this really what something is is just like

309
00:18:43,360 --> 00:18:46,300
what will choose

310
00:18:48,820 --> 00:18:49,990
so what

311
00:18:50,000 --> 00:18:52,000
time should

312
00:18:52,010 --> 00:18:53,980
can process

313
00:18:55,370 --> 00:18:58,650
so just

314
00:18:58,710 --> 00:19:00,520
john shawe

315
00:19:02,230 --> 00:19:04,560
so and

316
00:19:04,610 --> 00:19:05,860
two is

317
00:19:14,140 --> 00:19:16,220
last week

318
00:19:21,900 --> 00:19:26,340
what he also

319
00:19:26,400 --> 00:19:31,130
with a

320
00:19:35,800 --> 00:19:37,870
we had

321
00:19:39,540 --> 00:19:45,150
this is the best of

322
00:19:52,500 --> 00:19:56,260
was one of those

323
00:19:56,390 --> 00:19:59,790
so the

324
00:20:04,110 --> 00:20:12,590
there must be a lot but there is

325
00:20:12,650 --> 00:20:18,640
so for instance we should

326
00:20:18,650 --> 00:20:23,890
the rest of the world

327
00:20:28,020 --> 00:20:30,640
he was

328
00:20:31,340 --> 00:20:36,090
what is also

329
00:20:36,140 --> 00:20:40,560
when he

330
00:20:56,360 --> 00:20:59,340
his real name

331
00:21:04,660 --> 00:21:08,440
just check

332
00:21:08,480 --> 00:21:11,760
say you want to do it

333
00:21:14,140 --> 00:21:17,300
there is

334
00:21:19,010 --> 00:21:24,180
you want to

335
00:22:02,110 --> 00:22:04,370
we're patient

336
00:22:04,520 --> 00:22:11,260
so what i did wrong

337
00:22:11,270 --> 00:22:14,540
all of these

338
00:22:20,070 --> 00:22:24,780
that we

339
00:22:26,110 --> 00:22:27,550
was just

340
00:22:41,560 --> 00:22:44,400
i don't know what he said

341
00:22:44,420 --> 00:22:48,100
OK so

342
00:22:51,240 --> 00:22:53,420
regret say

343
00:22:53,440 --> 00:22:59,020
six months

344
00:23:06,540 --> 00:23:10,690
so it is just

345
00:23:10,860 --> 00:23:13,390
you new

346
00:23:19,270 --> 00:23:21,760
but range

347
00:23:21,760 --> 00:23:25,590
what i'm going to talk about today is learning graph matching

348
00:23:27,380 --> 00:23:30,300
so i have to add i'm not really

349
00:23:30,340 --> 00:23:34,260
not expert on the graph matching or on graphs and so on

350
00:23:34,520 --> 00:23:37,770
we came into doing this by

351
00:23:37,780 --> 00:23:43,070
well first solving a web page ranking from then we realize that by modifying things

352
00:23:43,070 --> 00:23:44,050
a little bit

353
00:23:44,100 --> 00:23:47,220
we could actually get something that works quite well this graph matching

354
00:23:48,730 --> 00:23:52,300
i is the bottom line of this talk what you should remember

355
00:23:52,350 --> 00:23:56,270
besides well i'll be talking a little bit about various graph matching algorithms and you

356
00:23:56,270 --> 00:23:59,570
can use them and i can make them better is

357
00:23:59,580 --> 00:24:00,970
well generally

358
00:24:01,030 --> 00:24:03,100
o thing in science so

359
00:24:03,150 --> 00:24:08,160
quite often people work very hard at finding answers to questions that somebody gave them

360
00:24:08,170 --> 00:24:12,610
or questions that they spend very little time thinking about and they were very hard

361
00:24:12,620 --> 00:24:14,030
on the answers

362
00:24:14,080 --> 00:24:16,230
this is an example of world

363
00:24:16,280 --> 00:24:20,470
and algorithmic example of well if you spend quite a bit more time on asking

364
00:24:20,470 --> 00:24:21,740
the right questions

365
00:24:21,750 --> 00:24:25,550
the answers will be much better and actually easier to obtain

366
00:24:28,120 --> 00:24:31,830
this is what we are able to do is for graph matching which is known

367
00:24:31,830 --> 00:24:33,600
to be hard

368
00:24:33,620 --> 00:24:35,980
that if you restrict yourself to

369
00:24:35,990 --> 00:24:38,810
the set of graphs that you really interested in solving

370
00:24:38,870 --> 00:24:40,020
in matching

371
00:24:40,030 --> 00:24:44,910
and if you actually tried to use statistics to learn

372
00:24:44,930 --> 00:24:46,390
the right question

373
00:24:46,470 --> 00:24:49,540
you will be able to find the answer by well

374
00:24:49,550 --> 00:24:53,870
what are we talking about the hungarian marriage method so linear assignment

375
00:24:53,920 --> 00:24:57,980
and we already talked to look a little bit about linear assignment and so on

376
00:24:58,270 --> 00:25:01,940
in the previous two days but i'll briefly explain anyway

377
00:25:05,270 --> 00:25:09,410
master that actually

378
00:25:10,640 --> 00:25:13,110
the outline of this talk is of the first

379
00:25:13,160 --> 00:25:17,100
actually explain in a little bit like graph matching is a good idea and by

380
00:25:17,100 --> 00:25:21,460
all means don't think that this is a fairly complete list for two reasons first

381
00:25:21,460 --> 00:25:22,280
the fall

382
00:25:22,290 --> 00:25:27,500
i'm not very much of an expert secondly it's more about the others namely how

383
00:25:27,500 --> 00:25:29,480
to learn this graph matching

384
00:25:29,520 --> 00:25:30,570
the know

385
00:25:31,850 --> 00:25:34,770
about some existing algorithms for graph matching

386
00:25:34,870 --> 00:25:38,740
and the one be talking about specifically are linear quadratic assignment

387
00:25:38,840 --> 00:25:39,750
there are there

388
00:25:39,800 --> 00:25:43,260
methods out there which were instances junction trees and so on but

389
00:25:43,300 --> 00:25:47,190
these actually some of the more popular ones

390
00:25:47,210 --> 00:25:48,130
and dean

391
00:25:48,140 --> 00:25:49,520
this is

392
00:25:49,570 --> 00:25:53,680
this section is actually just going to be one slide is too

393
00:25:53,730 --> 00:25:56,540
but stop and realize well maybe we should have

394
00:25:56,560 --> 00:26:00,760
a better question rather than working really hard in finding an answer

395
00:26:00,780 --> 00:26:03,710
and once we realize that actually

396
00:26:03,730 --> 00:26:06,170
finding that better question is very easy

397
00:26:06,220 --> 00:26:10,240
so then three justice it down and cracked the handling and stuff

398
00:26:10,290 --> 00:26:13,560
and i say that it actually does work reasonably well

399
00:26:13,610 --> 00:26:15,680
this is where i need your help

400
00:26:15,690 --> 00:26:18,950
in the context of extensions because there are a couple of

401
00:26:18,970 --> 00:26:21,610
straightforward extensions where it's very easy to see

402
00:26:21,620 --> 00:26:23,350
i will design the algorithm

403
00:26:23,370 --> 00:26:25,640
and the issues of the we

404
00:26:25,650 --> 00:26:27,600
in some cases not quite sure

405
00:26:27,650 --> 00:26:32,560
what the problem for that will be so it's like hammer look for

406
00:26:35,740 --> 00:26:40,010
well graphmatching well why do you care about as well

407
00:26:40,030 --> 00:26:43,400
biology and chemistry many might have molecules stored in the database you might want to

408
00:26:43,400 --> 00:26:44,430
match them up

409
00:26:44,510 --> 00:26:47,830
you might have regulatory networks and you want to match them

410
00:26:47,850 --> 00:26:50,740
you might want to estimate functions of proteins

411
00:26:50,780 --> 00:26:53,680
another application that's

412
00:26:53,720 --> 00:26:56,060
maybe a little bit more easy to visualize is

413
00:26:56,080 --> 00:26:58,360
object matching in computer vision

414
00:26:58,380 --> 00:27:02,370
so for instance wide baseline matching would extract features of two images

415
00:27:02,380 --> 00:27:06,550
and you need to actually match up the feature points news that three d camera

416
00:27:08,160 --> 00:27:13,230
camera calibration sounds a lot of geometric computer vision algorithms

417
00:27:13,290 --> 00:27:14,500
need such

418
00:27:15,390 --> 00:27:17,230
matching beforehand

419
00:27:17,240 --> 00:27:20,190
another one will be for instance let's say you've got

420
00:27:20,930 --> 00:27:22,150
a road map of

421
00:27:22,160 --> 00:27:23,320
say a

422
00:27:23,450 --> 00:27:27,130
princes italy and you have updated satellite images

423
00:27:27,170 --> 00:27:30,600
you want to add all the roads that were built illegally

424
00:27:30,610 --> 00:27:34,650
and you could do that by simply matching up with what you already have so

425
00:27:34,650 --> 00:27:38,090
far and then you just identify neurons that to

426
00:27:38,100 --> 00:27:41,230
and you want to do that recently automatically

427
00:27:43,450 --> 00:27:46,880
two just to show you that this is not such a trivial problem

428
00:27:46,900 --> 00:27:48,870
i mean here two identical graphs

429
00:27:48,880 --> 00:27:53,580
and if you use a little bit of imagination you can see well they basically

430
00:27:53,580 --> 00:27:54,700
reflect those

431
00:27:54,750 --> 00:27:56,090
to age down

432
00:27:56,100 --> 00:28:00,380
paul notes down and will see that those graphs are actually identical

433
00:28:00,400 --> 00:28:04,080
this is a fairly easy case because actually this is a plane graph so matching

434
00:28:04,080 --> 00:28:06,010
planar graphs is not at the heart

435
00:28:06,030 --> 00:28:09,290
but what this feature is that

436
00:28:10,220 --> 00:28:13,150
very simple objects are so easy to match

437
00:28:13,200 --> 00:28:14,700
at least three months

438
00:28:14,720 --> 00:28:18,890
and so what would expect the computers have a very easy time

439
00:28:20,050 --> 00:28:22,810
you also have all sorts of ambiguities

440
00:28:22,820 --> 00:28:24,110
so let's say

441
00:28:24,170 --> 00:28:25,180
this is

442
00:28:25,190 --> 00:28:25,980
the graph

443
00:28:26,020 --> 00:28:27,450
the same graph here

444
00:28:27,550 --> 00:28:32,240
and well how match one in this case i've actually got

445
00:28:33,140 --> 00:28:36,340
different rotations and i can flip things as well right

446
00:28:36,490 --> 00:28:38,800
and it's always going to be the same graph

447
00:28:38,810 --> 00:28:42,950
but these actually things that will make you live very hard afterwards when you want

448
00:28:42,950 --> 00:28:47,290
to design an automatic graph matching

449
00:28:49,480 --> 00:28:51,450
and it should actually be showing those

450
00:28:51,460 --> 00:28:54,340
pictures now and i don't know why

451
00:28:54,490 --> 00:28:56,590
OK so what you

452
00:28:56,660 --> 00:29:02,030
you should have seen an output of the slice after those pictures afterwards is

453
00:29:02,070 --> 00:29:03,740
they are

454
00:29:03,750 --> 00:29:04,710
if you

455
00:29:04,720 --> 00:29:06,590
if if you have

456
00:29:06,600 --> 00:29:10,730
images as in computer vision and extract those features and you want to match up

457
00:29:10,770 --> 00:29:14,140
so for instance if you see a person at two different positions

458
00:29:14,220 --> 00:29:16,120
the in

459
00:29:16,150 --> 00:29:19,170
you want to match up that he has moved in a certain way in the

460
00:29:19,170 --> 00:29:22,530
hands and all that you can then use that for the processing

461
00:29:22,880 --> 00:29:27,810
i don't know why it's not showing up to class

462
00:29:30,870 --> 00:29:35,720
the problem is that there is no currently known polynomial time algorithm for matching

463
00:29:36,860 --> 00:29:40,200
but checking is actually fairly easy if i give you match between two graphs and

464
00:29:40,200 --> 00:29:43,480
let's say they unlabelled it's a fairly straightforward thing

465
00:29:43,490 --> 00:29:47,220
it's not a straightforward if you have an issue with the two graphs are not

466
00:29:47,220 --> 00:29:50,180
identical and you just want to do an approximate match

467
00:29:50,200 --> 00:29:51,310
three hard

468
00:29:51,330 --> 00:29:56,150
so in that case we might just want to find the best match that's actually

469
00:29:56,160 --> 00:29:57,760
much more common problem

470
00:29:58,710 --> 00:29:59,870
for instance

471
00:29:59,880 --> 00:30:03,930
as in computer vision i mean you get to

472
00:30:03,980 --> 00:30:07,650
pictures of the same person you want to match features between them i mean what

473
00:30:07,650 --> 00:30:09,500
is the same person i mean the

474
00:30:09,520 --> 00:30:13,360
picture might look slightly different if it isn't it's kind of trivial problem

475
00:30:13,400 --> 00:30:19,280
you might have for instance several surveillance cameras in you tracking the same person

476
00:30:19,290 --> 00:30:24,110
and those cameras were installed by different manufacturers at different resolutions may be somewhere column

477
00:30:24,110 --> 00:30:25,830
and retrieval models

478
00:30:26,230 --> 00:30:27,700
it defined

479
00:30:27,760 --> 00:30:31,870
by the representation of the information need

480
00:30:31,870 --> 00:30:33,040
and your

481
00:30:33,050 --> 00:30:37,770
document or object that you want to be true

482
00:30:37,870 --> 00:30:39,900
it is also defined by

483
00:30:39,960 --> 00:30:42,840
the function of ranking

484
00:30:42,870 --> 00:30:46,240
six which computes the development of

485
00:30:46,250 --> 00:30:49,570
object to question

486
00:30:49,580 --> 00:30:52,380
so we will look at that

487
00:30:54,300 --> 00:30:59,630
yesterday which model curtsy quite popular

488
00:30:59,630 --> 00:31:01,660
i mean field so

489
00:31:01,680 --> 00:31:04,460
also the

490
00:31:04,480 --> 00:31:06,790
i included

491
00:31:07,070 --> 00:31:10,110
very recently whole

492
00:31:10,130 --> 00:31:12,760
with regard to antecedent

493
00:31:13,110 --> 00:31:15,510
which was on the way

494
00:31:18,320 --> 00:31:19,780
i would think about

495
00:31:19,830 --> 00:31:21,770
that's fine

496
00:31:22,570 --> 00:31:24,470
i speak about questions

497
00:31:24,510 --> 00:31:28,900
so is a much more focused

498
00:31:28,960 --> 00:31:31,030
in this this

499
00:31:31,330 --> 00:31:33,630
then we will very shortly

500
00:31:33,650 --> 00:31:37,320
x one x two x three which

501
00:31:37,330 --> 00:31:41,580
i think also the application that out yesterday

502
00:31:41,880 --> 00:31:44,630
it's very much in this topic

503
00:31:44,680 --> 00:31:48,290
and finally i fly anywhere

504
00:31:48,320 --> 00:31:51,910
many of the search problem that it could be

505
00:31:51,930 --> 00:31:58,900
the second thing is here

506
00:31:59,100 --> 00:32:00,100
i have

507
00:32:00,230 --> 00:32:04,270
the problem i think my mind that was copy

508
00:32:07,620 --> 00:32:11,220
the large computers

509
00:32:11,260 --> 00:32:27,110
i'm sorry

510
00:32:27,290 --> 00:32:50,130
well maybe maybe not

511
00:32:50,590 --> 00:32:53,650
i think it's part of a large

512
00:32:53,670 --> 00:32:55,570
to be sure that you can copy

513
00:32:55,630 --> 00:32:58,660
mean computer

514
00:32:59,970 --> 00:33:44,070
OK i've not seen it it might suitcase

515
00:33:44,080 --> 00:33:46,090
in fact it is

516
00:33:53,080 --> 00:33:56,840
i think it was because was the better

517
00:33:58,140 --> 00:34:00,860
but it's not

518
00:34:01,120 --> 00:34:22,930
i don't have

519
00:34:23,050 --> 00:34:25,640
so by

520
00:34:32,150 --> 00:34:37,130
but they come to my talk

521
00:34:37,130 --> 00:34:39,410
to complete

522
00:34:42,680 --> 00:34:46,430
i think

523
00:34:46,460 --> 00:36:13,100
on your part which

524
00:36:21,690 --> 00:36:50,460
i can think

525
00:36:54,420 --> 00:36:55,450
i'm sorry

526
00:36:56,630 --> 00:36:58,670
it's a very old computer in my

527
00:36:58,690 --> 00:36:59,910
my better to you

528
00:36:59,920 --> 00:37:03,540
it's not

529
00:37:06,340 --> 00:37:11,940
so we were have able to probability my

530
00:37:11,960 --> 00:37:14,620
and the probabilistic models

531
00:37:14,630 --> 00:37:18,930
actually the problem of estimating the probability

532
00:37:20,240 --> 00:37:28,080
given the surface you can have different kinds of can be query

533
00:37:29,030 --> 00:37:34,790
given a document collection you be estimated the search

534
00:37:35,720 --> 00:37:41,360
and then you rank the documents according to a prob

535
00:37:41,360 --> 00:37:48,850
that means that the time had come to

536
00:37:48,890 --> 00:37:51,190
so there are many different models

537
00:37:52,830 --> 00:37:54,120
which can

538
00:37:54,130 --> 00:37:57,850
and i think that language and

539
00:37:58,900 --> 00:38:03,100
so the support

540
00:38:03,110 --> 00:38:05,450
random variables

541
00:38:05,470 --> 00:38:10,100
the document the document q query

542
00:38:10,140 --> 00:38:12,320
and around on

543
00:38:12,320 --> 00:38:17,120
i which can have a value binary

544
00:38:20,860 --> 00:38:23,070
so i think the question

545
00:38:23,090 --> 00:38:25,800
we have to estimate the probability

546
00:38:30,010 --> 00:38:32,560
given the document and query

547
00:38:32,570 --> 00:38:33,440
which can be

548
00:38:35,070 --> 00:38:37,630
collins is by value

549
00:38:37,660 --> 00:38:40,790
it can be translated one minus the probability

550
00:38:40,800 --> 00:38:42,270
that to you

551
00:38:42,290 --> 00:38:43,700
a number of

552
00:38:43,730 --> 00:38:50,190
given the document and query

553
00:39:10,710 --> 00:39:18,780
think i right

554
00:39:33,860 --> 00:39:36,630
generative relevance models

555
00:39:36,650 --> 00:39:41,810
so we estimate the probability of relevance given your documents and you query

556
00:39:41,880 --> 00:39:44,280
we use the

557
00:39:44,290 --> 00:39:47,140
for the estimation

558
00:39:48,210 --> 00:39:50,290
if we use the two

559
00:39:51,820 --> 00:39:52,700
we obtain

560
00:39:53,010 --> 00:39:57,640
we have to estimate actually the probability that

561
00:39:57,650 --> 00:40:00,520
the probability that you think well

562
00:40:00,530 --> 00:40:02,720
given that are relevant

563
00:40:02,740 --> 00:40:05,260
now we can also use the line

564
00:40:05,640 --> 00:40:08,200
which need to be

565
00:40:08,230 --> 00:40:10,440
computer ratio

566
00:40:10,450 --> 00:40:12,200
of dallas

567
00:40:12,220 --> 00:40:18,960
given document and query from the of the number of a given document and query

568
00:40:19,700 --> 00:40:22,960
by using these rules

569
00:40:23,490 --> 00:40:26,810
we can actually draw the denominator

570
00:40:26,820 --> 00:40:27,780
i know her

571
00:40:28,820 --> 00:40:31,880
so i want to have a little bit overweight have

572
00:40:31,950 --> 00:40:33,510
we get actually is

573
00:40:33,510 --> 00:40:35,880
the language model that we of

574
00:40:39,210 --> 00:40:44,610
the probabilistic models and

575
00:40:44,610 --> 00:40:46,490
the only actually

576
00:40:48,180 --> 00:40:53,210
published in nineteen sixty seventy six by

577
00:40:53,260 --> 00:40:57,620
robert and karen sparck jones

578
00:40:57,630 --> 00:41:01,870
actually we knew

579
00:41:01,870 --> 00:41:03,010
in where

580
00:41:03,020 --> 00:41:04,850
this a

581
00:41:04,860 --> 00:41:06,340
in fact

582
00:41:06,380 --> 00:41:09,530
so in the language models

583
00:41:09,530 --> 00:41:11,400
that we are looking

584
00:41:11,420 --> 00:41:19,380
this practice into the probability of document given that and the probability that your document

585
00:41:19,390 --> 00:41:21,120
and relevant to generate

586
00:41:21,880 --> 00:41:24,960
so actually are looking at the generative model

587
00:41:25,020 --> 00:41:27,280
when you document generally

588
00:41:28,630 --> 00:41:29,410
and the other

589
00:41:29,780 --> 00:41:32,240
classical models of human

590
00:41:32,270 --> 00:41:34,610
that's the problem

591
00:41:34,610 --> 00:41:38,570
we have different structural factor here

592
00:41:38,580 --> 00:41:40,280
we have to be the

593
00:41:42,370 --> 00:41:47,770
generate a certain documents and to compute this problem

594
00:41:47,810 --> 00:41:50,580
now starting from a lot of

595
00:41:50,590 --> 00:41:54,330
we can again use the

596
00:41:54,680 --> 00:41:57,590
well we can so we can

597
00:41:58,960 --> 00:42:01,630
in fact two

598
00:42:03,960 --> 00:42:05,430
into this

599
00:42:05,440 --> 00:42:06,390
and then

600
00:42:06,410 --> 00:42:10,330
by applying based on the middle here

601
00:42:10,370 --> 00:42:12,490
and also in the denominator

602
00:42:12,510 --> 00:42:15,900
we can some of the terms in the equation

603
00:42:15,920 --> 00:42:20,980
and actually also move to the purpose of ranking

604
00:42:21,030 --> 00:42:24,120
which is great

605
00:42:24,410 --> 00:42:28,460
a lot of rather split into two

606
00:42:28,470 --> 00:42:31,780
sounds like

607
00:42:34,920 --> 00:42:35,770
we did

608
00:42:35,770 --> 00:42:40,260
so actually led to it could be any more

609
00:42:40,490 --> 00:42:42,920
for the purpose

610
00:42:46,560 --> 00:42:48,340
it could be considered for

611
00:42:48,360 --> 00:42:50,690
can conspiracy how

612
00:42:50,710 --> 00:42:51,800
it can be done

613
00:42:52,260 --> 00:42:53,630
now we want to do

614
00:42:53,640 --> 00:42:55,660
well we want to

615
00:42:55,710 --> 00:42:57,660
fifteen simple model

616
00:42:57,670 --> 00:42:59,420
and we assume that

617
00:42:59,430 --> 00:43:03,370
the conditioned on different set of number relevancy

618
00:43:03,370 --> 00:43:06,320
the document is independent of the query

619
00:43:06,370 --> 00:43:12,670
so which means that we have conceded dependency the probability of document given

620
00:43:12,670 --> 00:43:16,440
just each have their own interests in mind

621
00:43:16,460 --> 00:43:20,080
OK and so in computer science and a lot of interest in this frightens routing

622
00:43:20,080 --> 00:43:25,210
on the internet got multiple people routing their past and the fact that someone's downloading

623
00:43:25,210 --> 00:43:26,480
some big file

624
00:43:26,490 --> 00:43:28,070
is affecting your

625
00:43:28,690 --> 00:43:32,710
performance of your machine all this kind of stuff so that each person's behavior of

626
00:43:32,710 --> 00:43:36,910
facts has an impact on everyone else but i they're out to get users there

627
00:43:36,910 --> 00:43:40,820
are to do something and you want to figure out what's going to happen if

628
00:43:40,820 --> 00:43:43,720
everybody act in their own self-interest

629
00:43:43,730 --> 00:43:45,670
online auctions so

630
00:43:45,670 --> 00:43:49,180
google makes its money by selling the lands on the side of

631
00:43:49,200 --> 00:43:54,040
search pages and they have these building in a lot of different companies bidding for

632
00:43:54,040 --> 00:43:59,750
the search for keywords and so each acting in their own self-interest and not come

633
00:43:59,770 --> 00:44:00,640
you know

634
00:44:00,640 --> 00:44:02,600
active necessarily adversarially

635
00:44:02,620 --> 00:44:07,160
the company a stub is not directly to company b of this is justice to

636
00:44:07,160 --> 00:44:09,430
make money for themselves so

637
00:44:09,600 --> 00:44:14,530
again the case for many participants their behavior affect each other they each have their

638
00:44:14,530 --> 00:44:17,140
own interests in mind

639
00:44:20,540 --> 00:44:24,670
the theory of general some games kind of economics because economists were interested in studying

640
00:44:24,670 --> 00:44:26,400
economics and trying to study

641
00:44:26,430 --> 00:44:30,630
you know how prices change when people are buying and selling in their own interests

642
00:44:30,630 --> 00:44:32,760
and doing various things

643
00:44:32,890 --> 00:44:36,410
what like us on

644
00:44:36,480 --> 00:44:40,690
right so here a very simple general sum games in general sum games have win-win

645
00:44:40,690 --> 00:44:43,930
situations you can of lose-lose situation

646
00:44:43,930 --> 00:44:46,740
so here's maybe not exciting game

647
00:44:46,790 --> 00:44:48,260
so the first

648
00:44:48,280 --> 00:44:51,630
the call games but the really more interaction

649
00:44:51,630 --> 00:44:57,410
like the focus of again raymond someone when someone loses these are really more interactions

650
00:44:57,670 --> 00:45:01,630
i think it would be initiative called interaction theory

651
00:45:01,650 --> 00:45:03,790
i get again

652
00:45:03,870 --> 00:45:08,670
the interaction to use the simply which side of the sidewalk you walk

653
00:45:09,720 --> 00:45:13,920
you can walk on the left side of the right side of the site and

654
00:45:13,930 --> 00:45:17,040
the person coming towards you can walk on the left side of the right side

655
00:45:20,430 --> 00:45:24,560
i think i've done this for each person's perspective so that if you both walk

656
00:45:24,560 --> 00:45:25,630
on the right

657
00:45:25,630 --> 00:45:27,140
it's good for both of you

658
00:45:27,150 --> 00:45:30,590
or if you both walk on the left it's good for both of you but

659
00:45:30,590 --> 00:45:35,900
if you walk on the left and they are working on their right

660
00:45:35,910 --> 00:45:40,170
then you know you've got to do this sort of thing to figure out why

661
00:45:40,200 --> 00:45:44,660
in the same the other way around OK so these are you both when hear

662
00:45:44,700 --> 00:45:48,930
you both work

663
00:45:49,040 --> 00:45:51,700
there is no right answer in this game is now

664
00:45:51,750 --> 00:45:53,120
optimal thing to do

665
00:45:53,130 --> 00:45:57,060
everybody walking on the left is the nice thing everybody walking on the right is

666
00:45:57,060 --> 00:45:58,320
the nice thing

667
00:45:58,510 --> 00:46:03,950
there's no thing about optimality

668
00:46:04,720 --> 00:46:10,000
you can look alike which movie should you go to see a friend you try

669
00:46:10,010 --> 00:46:12,360
to figure out what you want to go see

670
00:46:12,370 --> 00:46:17,150
what are the the three hundred meters apart something anyway for some you know each

671
00:46:17,150 --> 00:46:20,180
have a different movie like the scene you'd like to also go with your friends

672
00:46:20,230 --> 00:46:24,540
you may prefer movie number one movie number two your friend may prefer moving them

673
00:46:24,860 --> 00:46:25,960
movie number one

674
00:46:25,980 --> 00:46:28,740
but you kind of like to go together so

675
00:46:28,790 --> 00:46:31,360
here you're payoffs are know

676
00:46:31,400 --> 00:46:35,610
this and this are both nice places to be at least going to friends movie

677
00:46:35,660 --> 00:46:38,910
but you would prefer this movie and friends

678
00:46:38,930 --> 00:46:41,960
there's no longer any value to the game

679
00:46:42,100 --> 00:46:47,270
no longer notion of the optimal we're going to look at what kind of reasonable

680
00:46:47,280 --> 00:46:53,120
so the notion here of nash equilibria

681
00:46:53,540 --> 00:46:57,150
so nash equilibrium

682
00:46:57,150 --> 00:47:01,860
is a stable pair strategies could be randomized like we had before

683
00:47:01,870 --> 00:47:04,990
it is stable pair of traditional mean by state

684
00:47:05,020 --> 00:47:09,390
stable means

685
00:47:09,390 --> 00:47:11,220
neither player

686
00:47:11,270 --> 00:47:14,120
have an incentive to deviate

687
00:47:14,120 --> 00:47:18,360
on their own

688
00:47:18,370 --> 00:47:21,970
so let's look at this what side of sidewalk to walk again

689
00:47:22,030 --> 00:47:26,420
so here we have

690
00:47:26,420 --> 00:47:31,610
right so think of the nash equilibria

691
00:47:31,620 --> 00:47:34,960
in this game

692
00:47:34,980 --> 00:47:36,490
OK so

693
00:47:36,500 --> 00:47:40,600
and as go over to the pair strategies could be randomized to that given the

694
00:47:40,600 --> 00:47:45,090
way the other players playing you have no particular reason to do anything different than

695
00:47:45,090 --> 00:47:49,030
what you're doing and given the way of applying the other player has no particular

696
00:47:49,030 --> 00:47:52,630
reason to anything different from what they're doing

697
00:47:53,170 --> 00:47:56,920
can anyone think of some nash equilibria in this game

698
00:48:04,700 --> 00:48:08,670
so in name name one

699
00:48:10,200 --> 00:48:14,790
left left everybody walking on the left is a perfectly fine actually going if everyone

700
00:48:14,790 --> 00:48:18,730
else working the left usually walking left to if you're walking left data work left

701
00:48:18,760 --> 00:48:19,930
the stable

702
00:48:19,940 --> 00:48:24,480
there's no reason for any lady everyone can after finding and

703
00:48:24,490 --> 00:48:27,580
is that the way people work and sidewalks here people drive on the left side

704
00:48:27,580 --> 00:48:31,410
imagine in my work

705
00:48:31,410 --> 00:48:34,660
and lastly we have the same which wraps around

706
00:48:34,680 --> 00:48:39,370
so x one minus xk

707
00:48:45,550 --> 00:48:47,800
science right

708
00:48:48,580 --> 00:48:51,640
so here's a bunch of constraints what do you suggest i do with them

709
00:48:51,660 --> 00:49:02,580
anything interesting about these constraints on the left-hand side

710
00:49:11,930 --> 00:49:14,680
sounds like the right word

711
00:49:14,780 --> 00:49:16,830
what was it

712
00:49:16,850 --> 00:49:18,870
telescopes yes

713
00:49:18,950 --> 00:49:22,910
everything cancels if i added these up is next to minimize x two there's an

714
00:49:22,910 --> 00:49:27,260
x minus x one and x one and x minus xk klxk xk everything here

715
00:49:27,280 --> 00:49:31,410
councils by the black ten slots so what happens if i have the right hand

716
00:49:31,410 --> 00:49:33,600
side over overhearing zero

717
00:49:33,600 --> 00:49:34,780
my favorite answer

718
00:49:35,010 --> 00:49:36,560
and every year

719
00:49:38,950 --> 00:49:42,760
the all the weights of all the edges in the negative weight cycle which is

720
00:49:42,760 --> 00:49:44,720
the weight of the cycle

721
00:49:44,740 --> 00:49:45,850
which is negative

722
00:49:45,870 --> 00:49:51,030
so zero is strictly less than zero contradiction

723
00:49:51,050 --> 00:49:55,100
contradiction women within the same anything that was false

724
00:49:55,780 --> 00:50:00,120
so really contradiction in in the mathematical sense we didn't contradict the world we just

725
00:50:00,120 --> 00:50:04,990
said that these constraints are contradictory in other words if you pick n values of

726
00:50:04,990 --> 00:50:08,030
the excise there's no way that these can all be true

727
00:50:08,050 --> 00:50:10,140
because then you would get a contradiction

728
00:50:10,160 --> 00:50:14,970
it so was impossible for these things to be satisfied by some real exercise

729
00:50:14,990 --> 00:50:17,240
so these must be

730
00:50:20,160 --> 00:50:22,080
so there's is no

731
00:50:22,180 --> 00:50:24,180
satisfying assignment

732
00:50:24,180 --> 00:50:27,030
more precise

733
00:50:27,050 --> 00:50:31,390
x one to xn

734
00:50:31,410 --> 00:50:35,680
no weights can we satisfy those constraints because they add up to

735
00:50:35,700 --> 00:50:38,830
zero left side negative on the right-hand side

736
00:50:38,850 --> 00:50:40,240
OK so that's pretty

737
00:50:40,260 --> 00:50:41,720
that's easy

738
00:50:41,740 --> 00:50:46,470
for the the reverse direction will be only slightly harder

739
00:50:46,490 --> 00:50:53,200
OK so called we have the connection

740
00:50:53,220 --> 00:50:57,430
motivation suppose you want to solve these difference constraints and we'll see one such

741
00:50:57,450 --> 00:50:58,970
application is actually

742
00:50:58,990 --> 00:51:02,100
you go around four

743
00:51:02,160 --> 00:51:06,450
four different constraints of a number of papers they care about distance constraints

744
00:51:06,510 --> 00:51:09,970
and they all use shortest paths to solve

745
00:51:09,990 --> 00:51:13,220
so if we can prove a connection between shortest has which we know how to

746
00:51:13,220 --> 00:51:16,300
compute an difference constraints then we'll have something cool

747
00:51:16,330 --> 00:51:21,640
and next class was even more applications of different constraints turns out there really

748
00:51:21,660 --> 00:51:23,030
really useful

749
00:51:23,050 --> 00:51:25,850
for all pairs shortest paths

750
00:51:28,950 --> 00:51:32,780
that's just prove equivalence initially

751
00:51:33,200 --> 00:51:39,740
so the reverse direction is if there's no negative weight cycle

752
00:51:39,820 --> 00:51:44,930
in the constraint graph

753
00:51:48,970 --> 00:51:51,760
and the system be satisfiable

754
00:51:51,780 --> 00:51:58,220
so claims that the negative weight cycles are the only barriers to finding a solution

755
00:51:58,280 --> 00:52:06,050
two design difference constraints

756
00:52:06,200 --> 00:52:12,410
this feeling

757
00:52:12,430 --> 00:52:19,180
yeah i detect string graph good

758
00:52:41,470 --> 00:52:45,910
so here we going to see techniques this is very useful for thinking about short

759
00:52:48,640 --> 00:52:52,390
it's a bit hard to guess especially if you see this is useful in problem

760
00:52:52,390 --> 00:52:57,450
sets having quizzes and finals and everything so keep this in mind

761
00:52:57,470 --> 00:53:04,330
i mean i'm using it to this rather simple theorem by the idea of changing

762
00:53:04,330 --> 00:53:07,370
the graph constraint graph g

763
00:53:07,370 --> 00:53:09,240
changing the graph is a very

764
00:53:09,260 --> 00:53:10,640
powerful idea

765
00:53:10,660 --> 00:53:13,280
so we're going and

766
00:53:13,350 --> 00:53:15,030
a new vertex

767
00:53:25,350 --> 00:53:26,760
user sourced look

768
00:53:26,800 --> 00:53:34,640
and we're going to have a bunch of edges from because being a source about

769
00:53:34,640 --> 00:53:40,100
to be connected to some things one and a zero weight edge

770
00:53:40,180 --> 00:53:43,930
rates are an inch from everywhere

771
00:53:47,220 --> 00:53:54,780
so to every other vertex in the constraint graph those those vertices are followed by

772
00:53:54,870 --> 00:53:57,530
one up to be and so

773
00:53:57,550 --> 00:54:00,760
have my constraint graph let me copy was one

774
00:54:00,780 --> 00:54:09,490
the second change was going to back up your work before you make changes

775
00:54:11,700 --> 00:54:16,370
so now i want to add a new vertex s over here my source take

776
00:54:16,370 --> 00:54:19,330
my constraint graph whatever looks like at the end

777
00:54:19,330 --> 00:54:21,050
we have zero edges

778
00:54:21,060 --> 00:54:24,140
to all the other vertices

779
00:54:24,260 --> 00:54:29,050
so what are now

780
00:54:29,080 --> 00:54:31,030
what i do

781
00:54:31,050 --> 00:54:37,780
but what did you do quite have candidate source now which can reach all the

782
00:54:39,550 --> 00:54:41,220
shortest paths from s

783
00:54:42,100 --> 00:54:47,820
well paths from s exist anger from st everywhere in weighted most zero

784
00:54:47,830 --> 00:54:50,680
OK maybe less

785
00:54:50,700 --> 00:54:53,800
it could have been as well

786
00:54:53,820 --> 00:54:58,030
you know like the two i can get to by zero minus two so lessons

787
00:54:58,030 --> 00:55:01,220
are going to be a little careful what is there is a negative weight cycle

788
00:55:02,140 --> 00:55:06,470
then there wouldn't be any shortest path fortunately we assume there is no negative weight

789
00:55:06,470 --> 00:55:10,120
cycle in the original graph and if you think about it there's no negative weight

790
00:55:10,120 --> 00:55:13,220
cycle in the original graph we add as

791
00:55:13,280 --> 00:55:17,550
to an edge from s to everywhere else we're not making any new negative weight

792
00:55:17,550 --> 00:55:21,410
cycles because you can start as an go somewhere the cost of zero which doesn't

793
00:55:21,410 --> 00:55:25,330
affect any weights and then you are forced to stay in the old graph

794
00:55:25,350 --> 00:55:27,970
so there can be any new negative weight cycles

795
00:55:27,990 --> 00:55:29,470
so this graph

796
00:55:29,490 --> 00:55:33,220
so the modified graph has no negative weight cycles

797
00:55:33,240 --> 00:55:34,890
that's good

798
00:55:34,910 --> 00:55:40,600
because it also has passed from as and therefore it has shortest path from s

799
00:55:40,620 --> 00:55:45,580
the modified graph has no

800
00:55:53,050 --> 00:55:54,330
because didn't before

801
00:55:54,330 --> 00:55:56,850
and it has

802
00:55:57,350 --> 00:55:58,950
have from st

803
00:56:00,080 --> 00:56:04,240
there is a path from s to every vertex

804
00:56:04,260 --> 00:56:08,370
may not have been before before i couldn't get from p two to three for

805
00:56:09,300 --> 00:56:14,080
that's what that's still true but from as i can get everywhere

806
00:56:14,080 --> 00:56:19,800
so that means that in this graph the modified graph has shortest path trees has

807
00:56:20,740 --> 00:56:22,820
from s

808
00:56:22,870 --> 00:56:29,640
there was old fighter called the shortest path weights like i ran bellman ford from

809
00:56:32,010 --> 00:56:35,720
i will get a bunch of finite numbers dv for every

810
00:56:37,470 --> 00:56:39,140
for every vertex

811
00:56:39,160 --> 00:56:46,410
it seems like a good idea list

812
00:56:47,180 --> 00:56:50,240
she has exist that's just assigned

813
00:56:50,390 --> 00:56:53,010
x y

814
00:56:53,030 --> 00:56:57,530
to be the shortest path away from s to v on

815
00:56:57,550 --> 00:56:58,720
so it's

816
00:56:58,720 --> 00:57:03,240
in one that that's a good choice for number

817
00:57:03,240 --> 00:57:06,840
the shortest path from s to be i this is finite because it's less than

818
00:57:07,700 --> 00:57:11,220
and it's great to minus infinity

819
00:57:11,240 --> 00:57:13,560
so some finite number that's what we need to

820
00:57:13,560 --> 00:57:18,230
lots of mutation going on right to edit distance is longer while here instances is

821
00:57:18,250 --> 00:57:22,040
very short right just need to be d and i get a c

822
00:57:22,080 --> 00:57:25,540
right so the way to do this is every node will be will be the

823
00:57:25,540 --> 00:57:32,270
quote edges are some kind of approximate inclusions inclusion relationships and edges also how weights

824
00:57:32,290 --> 00:57:39,280
which means that it means let's see how likely how likely it a particular quote

825
00:57:39,280 --> 00:57:42,850
has evolved from from a set of parents and what they would like to do

826
00:57:42,850 --> 00:57:47,140
this in this this and the the way to construct the graph is to ensure

827
00:57:47,140 --> 00:57:52,890
that the graph will be a directed acyclic graph of approximate quote inclusion is we

828
00:57:52,890 --> 00:57:57,330
want you want to delete the minimum total edge weights such that each component has

829
00:57:57,330 --> 00:58:00,640
because single sink node right so let me show you what i mean by that

830
00:58:00,730 --> 00:58:04,810
then i'll explain why this makes sense so in this particular example i would delete

831
00:58:04,810 --> 00:58:08,830
this particular edges and that would be my solution might i would have disconnected components

832
00:58:09,080 --> 00:58:12,930
that all all end up in a single sink node right so i could say

833
00:58:12,930 --> 00:58:17,430
that this is sort of the mother called the mother phrase from from which everything

834
00:58:17,430 --> 00:58:18,790
else has evolved

835
00:58:18,830 --> 00:58:23,330
right and this way i can now identify different mutational variants of a single phrase

836
00:58:23,350 --> 00:58:26,330
right so what i was showing you on the previous slide was the graph like

837
00:58:27,060 --> 00:58:31,410
right i'm showing you different variants and how including one another for depending around the

838
00:58:31,410 --> 00:58:35,500
better for right of course the question is how do i do this kind of

839
00:58:35,500 --> 00:58:40,790
partitioning as as well as pretty much or as many kinds of partitioning by this

840
00:58:40,790 --> 00:58:43,080
deck partitioning is also NP hard

841
00:58:43,160 --> 00:58:48,500
but there are some very effective heuristics the observation that led to design the statistics

842
00:58:48,500 --> 00:58:51,180
is that basically the idea is that it's enough to know

843
00:58:51,580 --> 00:58:57,890
notes parent to reconstruct the optimal solution by terrified so if at every node i

844
00:58:57,890 --> 00:59:03,250
that the true parent from where the nodes evolved i can reconstruct the optimal solutions

845
00:59:03,250 --> 00:59:06,770
of the way the way to our heuristic works is we start at the top

846
00:59:07,080 --> 00:59:12,870
and the and we then proceed sort of topological order of these back and every

847
00:59:12,930 --> 00:59:17,500
every step we ask nodes who's who when you think you you you have most

848
00:59:17,500 --> 00:59:21,830
likely evolved from and that we also include the size of the of the cluster

849
00:59:21,830 --> 00:59:26,600
and the parent of the node belongs to this way we sort of can extract

850
00:59:26,600 --> 00:59:32,430
this kind of thing is that the approximate this inclusion relationships and give different mutations

851
00:59:32,430 --> 00:59:36,140
variants of the same piece of content what is interesting here is that we don't

852
00:59:36,160 --> 00:59:41,520
assume human supervision be identified short pieces of content identified how how these pieces of

853
00:59:41,520 --> 00:59:45,160
content through there and we can start looking good at them and so this is

854
00:59:45,160 --> 00:59:49,230
this is the plot again before the two thousand eight u s presidential election of

855
00:59:49,250 --> 00:59:52,950
where xx is time and y axis is the number of mentions but i was

856
00:59:52,950 --> 00:59:55,750
right and what is nice is that you get now you get to see a

857
00:59:55,750 --> 01:00:00,750
very nice description of what was media talking about in this

858
01:00:00,790 --> 01:00:04,960
three months before the presidential election right you have sort of this first spike is

859
01:00:04,960 --> 01:00:10,640
that the republican convention of such democratic convention this was republican convention this was the

860
01:00:10,930 --> 01:00:14,460
famous lipstick on a pig

861
01:00:14,480 --> 01:00:20,390
quote there is that was then sort of thing the the statement from the fundamentals

862
01:00:20,390 --> 01:00:24,730
of us or our economy are strong and then a week later they realized that

863
01:00:24,730 --> 01:00:28,830
it's the opposite so the entire economies danger and so on and so forth right

864
01:00:28,910 --> 01:00:32,330
and this way you you can basically just but just by doing this you can

865
01:00:32,330 --> 01:00:36,580
get very nice insights into what works what was the online media and the blogosphere

866
01:00:36,580 --> 01:00:41,390
talking about another thing that you can do is actually not looking like rank sites

867
01:00:41,580 --> 01:00:47,000
by how how quickly they tend to mention information and in particular what i'm showing

868
01:00:47,000 --> 01:00:51,710
you here i'm having the rank leg which means negative length leg in our system

869
01:00:51,710 --> 01:00:57,270
is that this method website dance tends to mention use before they reached popularity so

870
01:00:57,270 --> 01:01:02,080
zero speak popularity and the negative means you have developed cities before the

871
01:01:02,100 --> 01:01:06,730
and what you start seeing here for example is the professional blogs tend to mention

872
01:01:06,730 --> 01:01:12,450
information much much sooner before it becomes the most popular and then these are the

873
01:01:12,450 --> 01:01:16,980
mainstream media sites there are still some that but they still creek but they appear

874
01:01:17,040 --> 01:01:17,540
after the

875
01:01:17,950 --> 01:01:22,680
professional blog site and of course now we can go and select sub parts of

876
01:01:22,680 --> 01:01:28,080
this quote unquote and a particular topic to do find finer resolution analysis so here

877
01:01:28,100 --> 01:01:32,710
is here is an example example of how we can define the a finer resolution

878
01:01:32,710 --> 01:01:34,250
analysis with this kind of

879
01:01:34,480 --> 01:01:39,270
this kind of approach so this is something that we did together with the new

880
01:01:39,270 --> 01:01:44,910
formations project on x excellence in journalism right and they really wanted understand the

881
01:01:44,910 --> 01:01:50,000
media coverage of the current economic crisis right and then they try to see what

882
01:01:50,000 --> 01:01:53,620
are the main statement of the main phrases that that that got the most attention

883
01:01:53,620 --> 01:01:58,160
on the web who who mentioned those phrases and so on right here is here

884
01:01:58,160 --> 01:02:03,480
is use an example from the from the report that you have to create right

885
01:02:03,480 --> 01:02:07,710
so this was for only two thousand nine these are different from the phrase is

886
01:02:07,960 --> 01:02:12,910
all about the economic crisis this is who was the person that said when they

887
01:02:12,910 --> 01:02:17,930
said i mean this was the number of mentions in our in our dataset and

888
01:02:17,930 --> 01:02:22,620
what is interesting to see here is that basically it was the first to get

889
01:02:22,620 --> 01:02:26,250
the most attention on this particular issue was actually barack obama right and then you

890
01:02:26,250 --> 01:02:29,330
know the

891
01:02:29,410 --> 01:02:33,520
but the key to the chief of the federal reserve bank he was only here

892
01:02:34,000 --> 01:02:39,540
towards the bottom the top republican voice was ranked only number forty nine on this

893
01:02:39,540 --> 01:02:40,750
particular issue

894
01:02:40,750 --> 01:02:41,850
and the

895
01:02:41,870 --> 01:02:46,460
i think that was ranked number five was this cartoon from the new york post

896
01:02:46,460 --> 01:02:50,040
web sort of the policemen should the monkey and then they say

897
01:02:50,080 --> 01:02:52,810
we need to find someone else to write the next stimulus bill so this sort

898
01:02:52,810 --> 01:02:56,410
of a controversial they had to pull it off but again he got like super

899
01:02:56,410 --> 01:02:58,010
under this probabilistic model

900
01:02:58,040 --> 01:02:59,950
and this is the function

901
01:02:59,950 --> 01:03:04,510
which is differentiable convex we can solve in

902
01:03:04,530 --> 01:03:07,220
and the number of different ways very very quickly

903
01:03:07,780 --> 01:03:13,390
so has that solved the problem of saying how we came to pick which algorithm

904
01:03:13,390 --> 01:03:14,760
again to use

905
01:03:14,810 --> 01:03:16,720
it solved the problem of

906
01:03:16,790 --> 01:03:21,300
it doesn't matter which optimize use because all of them again to give the same

907
01:03:21,310 --> 01:03:22,130
the school

908
01:03:23,570 --> 01:03:27,260
this thing is convex it turns out that you always get the same answer no

909
01:03:27,260 --> 01:03:30,250
matter which optimisation kg

910
01:03:30,340 --> 01:03:33,300
but there's another problem

911
01:03:33,320 --> 01:03:34,680
which is

912
01:03:34,750 --> 01:03:36,270
it's always possible

913
01:03:36,280 --> 01:03:38,860
to get an empirical loss zero

914
01:03:40,850 --> 01:03:44,150
what i haven't said is

915
01:03:44,160 --> 01:03:48,300
how we should should choose how to extend the dataset

916
01:03:48,360 --> 01:03:52,200
the reason i'm talking about linear rules remember the fact that i can to talk

917
01:03:52,200 --> 01:03:57,150
about just decision planes is that i'm allowing myself the freedom to add extra features

918
01:03:57,160 --> 01:04:00,680
to my data set so that any problem can be solved with the linear decision

919
01:04:01,730 --> 01:04:02,950
so now

920
01:04:02,960 --> 01:04:05,760
if i'm optimizing squared loss

921
01:04:05,810 --> 01:04:08,200
i have some black points which is the data set

922
01:04:08,210 --> 01:04:09,710
i could fit

923
01:04:09,760 --> 01:04:11,820
a plane which is the blue line

924
01:04:11,870 --> 01:04:14,890
or you could extend the dataset to have seven columns

925
01:04:14,900 --> 01:04:16,530
and i could magenta line

926
01:04:16,580 --> 01:04:20,140
magenta line has very square square there

927
01:04:20,910 --> 01:04:25,700
under that objective the magenta line is the best curve but maybe we don't really

928
01:04:25,700 --> 01:04:29,060
believe that if we see an input here is likely to have a response down

929
01:04:29,060 --> 01:04:31,450
here given the all the data that lie

930
01:04:31,460 --> 01:04:33,250
lake close to the slide

931
01:04:33,270 --> 01:04:33,960
it's a

932
01:04:34,020 --> 01:04:36,590
there's something wrong with that objective

933
01:04:40,610 --> 01:04:44,600
in the classification setting it's very easy to get an empirical loss zero is so

934
01:04:44,620 --> 01:04:46,710
the nearest neighbour would do that

935
01:04:46,840 --> 01:04:52,310
what's the nearest neighbour to this one point which is itself it predicts itself and

936
01:04:53,270 --> 01:04:56,390
remembering the whole training set in just spitting it out again

937
01:04:56,390 --> 01:05:00,910
so that no interesting learning has gone on if you just remember the whole dataset

938
01:05:01,200 --> 01:05:02,350
and you say

939
01:05:02,360 --> 01:05:06,110
well now i can predict that dataset because i happen to know all the answers

940
01:05:06,150 --> 01:05:08,100
like cheating on an exam

941
01:05:09,710 --> 01:05:15,950
we're going to need to do something else

942
01:05:19,560 --> 01:05:26,210
this is the tough issues there isn't one right way to proceed because

943
01:05:26,250 --> 01:05:29,180
it depends on what a problems like

944
01:05:30,670 --> 01:05:34,940
i said that this magenta line was on an unreasonable fit to the data

945
01:05:34,950 --> 01:05:38,410
who am i to say that so maybe you know

946
01:05:38,420 --> 01:05:45,370
the data definitely lie on a sixth order polynomial and for some physical reason and

947
01:05:45,420 --> 01:05:49,460
given these observations you're sure that it's going to happen

948
01:05:49,500 --> 01:05:50,620
and so

949
01:05:50,660 --> 01:05:52,860
you need to introduce

950
01:05:52,910 --> 01:05:55,260
some idea of what it means to

951
01:05:55,320 --> 01:05:59,790
generalize what you need to know about the problem what learning means in order to

952
01:05:59,790 --> 01:06:03,210
formally say why we don't like the magenta curve

953
01:06:03,290 --> 01:06:06,790
and there's multiple different ways of going about doing that

954
01:06:06,860 --> 01:06:09,070
and i'm going to mention two of them

955
01:06:09,130 --> 01:06:10,470
later on in the lecture

956
01:06:11,920 --> 01:06:16,710
but there are fortunately some very simple practical things that we can do right away

957
01:06:17,030 --> 01:06:18,920
you could go and write code to do

958
01:06:18,930 --> 01:06:22,500
this afternoon if you wanted to

959
01:06:23,250 --> 01:06:25,220
one of the practical things we can do

960
01:06:25,240 --> 01:06:28,260
this whole that was called the validation set so

961
01:06:28,270 --> 01:06:30,460
instead of fitting my whole

962
01:06:31,710 --> 01:06:34,010
i'm going to draw a line from point and say

963
01:06:34,020 --> 01:06:37,950
OK i've got this big dataset but i'm only going to use

964
01:06:37,950 --> 01:06:39,210
this is that

965
01:06:39,920 --> 01:06:41,190
i'm going to

966
01:06:41,200 --> 01:06:45,530
it might be squares i'm going to fit my logistic regression model whatever to this

967
01:06:45,530 --> 01:06:48,950
dataset and hold out some of the

968
01:06:48,990 --> 01:06:51,860
because it might be useful later and so the rest of it i'm going to

969
01:06:51,860 --> 01:06:54,020
call validations

970
01:06:54,670 --> 01:06:55,370
it's a

971
01:06:55,380 --> 01:06:59,570
the blue crosses here are my training set the data that i'm actually giving to

972
01:07:00,190 --> 01:07:02,800
the training algorithm and i fitted

973
01:07:02,820 --> 01:07:09,260
and back again to curve the green curve which use different temporal representations of expanded

974
01:07:09,260 --> 01:07:11,840
them out using different orders polynomial

975
01:07:12,550 --> 01:07:16,180
these cats are generated using different number the columns of x

976
01:07:18,420 --> 01:07:23,020
the red dots are now this validation set that i held out so the kids

977
01:07:23,020 --> 01:07:26,820
didn't see the red dots when they are being produced nothing about the computer codes

978
01:07:26,820 --> 01:07:28,630
or the red dots

979
01:07:29,300 --> 01:07:34,380
these red dots now witness to the fact that the magenta curve silly

980
01:07:34,400 --> 01:07:38,970
the gender can actually shoots off the board and extrapolate really wildly

981
01:07:39,780 --> 01:07:43,200
very poorly predicts these held point

982
01:07:45,380 --> 01:07:49,630
this green curve which is under fitting is not capturing this is actually a non-linear

983
01:07:50,670 --> 01:07:51,820
doesn't predict

984
01:07:51,840 --> 01:07:55,460
this held that data nearly as well as the black curve so this gives us

985
01:07:56,200 --> 01:08:01,910
a principled reason for rejecting the curse which underfitting not overfitting so now we've got

986
01:08:01,910 --> 01:08:03,620
a score for picking

987
01:08:03,630 --> 01:08:06,360
what model we use how many columns today include in

988
01:08:06,370 --> 01:08:07,450
my data set

989
01:08:08,350 --> 01:08:12,200
it's the choice that gives me the best validation performance because

990
01:08:12,250 --> 01:08:13,390
that's the

991
01:08:13,390 --> 01:08:20,760
procedure which is actually generalize to future data haven't been senior

992
01:08:20,790 --> 01:08:24,150
so you don't want to necessarily have to visualize what you're doing

993
01:08:24,430 --> 01:08:29,300
no matter how complicated your problem is you can always draw this plot

994
01:08:29,360 --> 01:08:30,160
you could

995
01:08:30,170 --> 01:08:33,880
but the different models along the x axis so this is a different number of

996
01:08:33,880 --> 01:08:36,720
columns of text and my input space into

997
01:08:36,740 --> 01:08:39,040
and then on the y axis are plotted

998
01:08:39,860 --> 01:08:44,400
error that i got from the validation set in this case the mean squared error

999
01:08:44,460 --> 01:08:47,510
so now the blue curve shows progression progression of training

1000
01:08:47,540 --> 01:08:50,960
so this is something that's always a good idea to plot sanity cheque

1001
01:08:50,980 --> 01:08:55,590
he's your algorithm actually improving the training costs monotonically because

1002
01:08:55,600 --> 01:08:59,090
if not then it's probably a bug in your code so that's was a good

1003
01:08:59,090 --> 01:09:00,430
character plot

1004
01:09:01,460 --> 01:09:05,350
and the red line shows the validation performance so

1005
01:09:05,390 --> 01:09:07,110
here says well

1006
01:09:08,010 --> 01:09:10,790
on different data it's reasonable that you're going to do

1007
01:09:10,800 --> 01:09:13,650
a little bit worse because you've actually fitted to

1008
01:09:13,650 --> 01:09:16,630
your observations so you're never going to be quite as well

1009
01:09:16,650 --> 01:09:20,890
on different cases and exactly the case using fuzzy that's why the red curve is

1010
01:09:20,890 --> 01:09:24,550
high but follow the same shape it is true that

1011
01:09:24,570 --> 01:09:26,670
you won't fitting the flat line so

1012
01:09:28,040 --> 01:09:29,030
these terms

1013
01:09:29,050 --> 01:09:31,200
give you non-linear effects is a good idea

1014
01:09:31,260 --> 01:09:32,840
up to a point

1015
01:09:32,960 --> 01:09:33,600
it's a

1016
01:09:33,610 --> 01:09:38,150
the good case you've seen where i think by the political polynomials and this red

1017
01:09:38,150 --> 01:09:42,240
curve shows that yes those are the ones that do best and then rapidly

1018
01:09:42,260 --> 01:09:47,380
as you overfit as you put in spurious curves on really well justified by the

1019
01:09:48,310 --> 01:09:51,540
the validation set picks you up on that and you did very badly

1020
01:09:52,470 --> 01:10:01,510
this kind of says don't actually fit these really complicated models

1021
01:10:05,440 --> 01:10:08,430
holding out validation set is something that

1022
01:10:08,440 --> 01:10:12,690
you should always do routinely i think because it's very easy to kid yourself

1023
01:10:12,700 --> 01:10:14,960
that you've come up with a great idea

1024
01:10:15,030 --> 01:10:18,590
because you're going to try out a whole bunch of different things in the data

1025
01:10:19,420 --> 01:10:22,850
and one of the more then seem to be doing really well

1026
01:10:22,860 --> 01:10:27,480
and that could just be by chance unless you've held out some dataset you didn't

1027
01:10:27,480 --> 01:10:28,310
look at

1028
01:10:28,450 --> 01:10:33,240
you're never going to be really sure whether you're just fooling yourself or whether you

1029
01:10:33,240 --> 01:10:35,720
actually made real progress on your problem

1030
01:10:35,720 --> 01:10:43,620
once we make some assumptions about probability distributions of right of the centre

1031
01:10:43,640 --> 01:10:48,770
we can actually compute the probability of an item act

1032
01:10:48,850 --> 01:10:51,630
given the query

1033
01:10:51,700 --> 01:10:54,320
how probable is that you don't

1034
01:10:54,330 --> 01:10:59,800
given the query that we might want to score items by their probability given the

1035
01:11:03,070 --> 01:11:06,870
well see how to compute the date but let's think about this

1036
01:11:06,920 --> 01:11:08,620
it turns out that

1037
01:11:08,630 --> 01:11:10,630
it's score by

1038
01:11:10,660 --> 01:11:13,040
this you get terrible results

1039
01:11:13,750 --> 01:11:16,620
afterwards you think about it you realize why

1040
01:11:16,630 --> 01:11:21,230
the problem is that the probability of an item given the query set

1041
01:11:21,280 --> 01:11:26,840
is sensible ranking criterion because it depends on the properties of the item

1042
01:11:26,890 --> 01:11:29,670
that have nothing to do with query

1043
01:11:32,100 --> 01:11:36,750
some items may just be more probable than others regardless of the query sets of

1044
01:11:36,750 --> 01:11:39,760
items are just more typical

1045
01:11:39,810 --> 01:11:42,550
the kind of you can think of it as they live more in the middle

1046
01:11:42,550 --> 01:11:47,440
of whatever feature space you're measuring the another idea in those will always be ranked

1047
01:11:47,440 --> 01:11:52,750
higher by this criterion regardless of what the query that is

1048
01:11:52,850 --> 01:11:57,940
and also if your items are things like documents let's say if you were to

1049
01:11:57,940 --> 01:12:00,750
actually model something like documents

1050
01:12:00,760 --> 01:12:03,020
or any other kind of strange

1051
01:12:03,030 --> 01:12:05,330
the probability of any string

1052
01:12:05,350 --> 01:12:08,540
decreases with the number of characters that string

1053
01:12:09,100 --> 01:12:14,330
this sort of measure would have a bias towards short documents are short strings which

1054
01:12:14,330 --> 01:12:15,980
seems also really silly

1055
01:12:16,030 --> 01:12:18,870
we don't we don't really want that

1056
01:12:18,920 --> 01:12:20,310
arranged by

1057
01:12:21,400 --> 01:12:25,280
criteria that are not exactly relevant to our query

1058
01:12:25,300 --> 01:12:27,330
so what we really want to know

1059
01:12:27,360 --> 01:12:32,710
is the probability of the idea given the query that

1060
01:12:34,060 --> 01:12:38,320
two here that the probability of the data before observing the query

1061
01:12:38,360 --> 01:12:43,010
so we're interested in is how much of the query that increase or decrease the

1062
01:12:43,010 --> 01:12:45,510
probability of that item

1063
01:12:45,520 --> 01:12:50,670
and that's what we're going to use as our ranking criterion

1064
01:12:50,720 --> 01:12:53,170
so here is the basic idea

1065
01:12:54,060 --> 01:12:56,880
after getting the query that

1066
01:12:57,670 --> 01:12:59,180
what we do in

1067
01:12:59,190 --> 01:13:01,550
we rank all the items by

1068
01:13:01,570 --> 01:13:03,170
it by score

1069
01:13:03,180 --> 01:13:07,680
which is the ratio of the probability of the item given the query set

1070
01:13:08,680 --> 01:13:13,000
the probability of the item before observing the query set

1071
01:13:13,050 --> 01:13:15,870
now how do we actually compute this ratio

1072
01:13:15,880 --> 01:13:19,150
well do this we need to make some assumptions

1073
01:13:19,170 --> 01:13:22,270
well we're going to do is we're going to have some simple

1074
01:13:22,280 --> 01:13:24,020
parameterized model

1075
01:13:25,350 --> 01:13:28,870
objects in our universe so the object represented by

1076
01:13:29,270 --> 01:13:31,510
that there's the features

1077
01:13:31,520 --> 01:13:36,020
and for any probability distribution that we define on x

1078
01:13:36,620 --> 01:13:40,310
parameterized by some set of parameters

1079
01:13:40,390 --> 01:13:45,010
and then if we define the prior distribution on what the parameters of that concept

1080
01:13:46,460 --> 01:13:48,160
it can be

1081
01:13:48,210 --> 01:13:51,420
we can actually compute all the quantities of interest

1082
01:13:51,530 --> 01:13:55,470
the let's go through that and try to understand how that works

1083
01:13:57,040 --> 01:13:59,570
the nominator here here back

1084
01:13:59,650 --> 01:14:01,610
is simply

1085
01:14:01,640 --> 01:14:07,960
the probability of x given theta pi the probability of the data integrated over all

1086
01:14:10,560 --> 01:14:12,080
under are bottle

1087
01:14:12,160 --> 01:14:16,280
this is the prior probability of observing item act

1088
01:14:19,100 --> 01:14:20,430
the numerator

1089
01:14:20,440 --> 01:14:23,560
the probability of i observing item x

1090
01:14:23,560 --> 01:14:25,540
given the query that

1091
01:14:27,170 --> 01:14:30,420
expressed by exactly the the same equation

1092
01:14:30,460 --> 01:14:34,560
the probability of x given the parameters but now instead of averaging with respect to

1093
01:14:34,560 --> 01:14:36,630
the prior over the parameters

1094
01:14:36,680 --> 01:14:40,490
we have the average with respect to the posterior over the parameters

1095
01:14:40,530 --> 01:14:41,920
given the query that

1096
01:14:43,240 --> 01:14:45,880
essentially when we observe the query that

1097
01:14:45,960 --> 01:14:48,480
the person enters into the system

1098
01:14:48,820 --> 01:14:50,890
we go from the prior

1099
01:14:50,910 --> 01:14:53,760
over what the parameters of that concept could be

1100
01:14:53,770 --> 01:14:56,850
the posterior over what the parameters of that concept

1101
01:14:56,910 --> 01:14:58,040
could be

1102
01:14:58,040 --> 01:15:01,740
and so that given by bayes rule here

1103
01:15:01,790 --> 01:15:05,080
the posterior over the parameters given the query set

1104
01:15:05,980 --> 01:15:07,660
the likelihood of the

1105
01:15:07,900 --> 01:15:09,730
query set

1106
01:15:09,780 --> 01:15:12,740
for the setting of the parameters

1107
01:15:12,980 --> 01:15:15,120
the prior normalized

1108
01:15:15,180 --> 01:15:17,530
so very basically

1109
01:15:17,560 --> 01:15:21,050
uses of marginalisation and they will

1110
01:15:21,990 --> 01:15:24,840
any questions about this

1111
01:15:25,450 --> 01:15:31,360
well i'll get get to that

1112
01:15:31,390 --> 01:15:35,800
in a minute but the way we should think about this is

1113
01:15:39,910 --> 01:15:42,910
it's going to be a representation of what we think

1114
01:15:42,920 --> 01:15:47,060
the items in our universe are are

1115
01:15:47,070 --> 01:15:51,300
how we think a cluster will be represented or concert will be represented in our

1116
01:15:51,300 --> 01:15:55,700
universe so for example if the items are documents

1117
01:15:55,730 --> 01:15:58,430
then we have a huge history of

1118
01:15:58,430 --> 01:16:01,210
been there

1119
01:17:14,240 --> 01:17:23,510
the thing

1120
01:17:44,680 --> 01:17:46,520
so far the

1121
01:18:31,030 --> 01:18:42,740
so here we are

1122
01:18:43,400 --> 01:18:45,850
that he

1123
01:18:47,540 --> 01:18:52,990
a lot

1124
01:19:05,940 --> 01:19:10,610
two years later

1125
01:19:17,200 --> 01:19:19,250
we are

1126
01:19:45,160 --> 01:19:48,570
one year

1127
01:20:07,660 --> 01:20:11,650
so they

1128
01:20:43,740 --> 01:20:49,100
you are a

1129
01:21:08,120 --> 01:21:19,790
the last thing they need

1130
01:21:23,880 --> 01:21:27,420
or few more

1131
01:21:29,580 --> 01:21:31,450
the median

1132
01:22:27,480 --> 01:22:31,970
o two

1133
01:23:20,600 --> 01:23:22,630
the same thing

1134
01:23:22,650 --> 01:23:23,960
he might be

1135
01:23:35,820 --> 01:23:39,570
what are the

1136
01:24:12,270 --> 01:24:16,230
the next

1137
01:24:38,020 --> 01:24:41,060
i know

1138
01:24:50,890 --> 01:24:55,860
if you lucky

1139
01:25:00,480 --> 01:25:03,470
he also

1140
01:25:03,470 --> 01:25:08,520
and if you are sure that point cannot be

1141
01:25:08,540 --> 01:25:09,890
an outlier

1142
01:25:10,520 --> 01:25:12,180
you just can

1143
01:25:12,200 --> 01:25:16,520
discarded from further pruning it from further consideration

1144
01:25:16,540 --> 01:25:21,720
a second second sorry it's taken over there is a nested loop based over

1145
01:25:23,660 --> 01:25:24,470
the other

1146
01:25:24,490 --> 01:25:29,830
learning memory buffer is divided in two parts and the second part is just used

1147
01:25:30,390 --> 01:25:31,580
this can

1148
01:25:31,600 --> 01:25:36,890
and compare all points with the points from the first so you can just load

1149
01:25:36,910 --> 01:25:38,740
one part of the the data

1150
01:25:38,770 --> 01:25:41,310
based in the first part of the back buffer

1151
01:25:41,330 --> 01:25:45,330
and then the second part of the buffer is used to go through the

1152
01:25:45,330 --> 01:25:46,540
the whole dataset

1153
01:25:46,540 --> 01:25:50,240
and compare the case of classical

1154
01:25:52,180 --> 01:25:59,490
and the third kind of government grid based approach so here the idea is to

1155
01:25:59,490 --> 01:26:04,040
below the grid such that any two points from the same grid cells

1156
01:26:04,060 --> 01:26:06,560
i have distance of at most epsilon

1157
01:26:06,560 --> 01:26:09,930
to each other and then it's clear if you think about it a second time

1158
01:26:09,930 --> 01:26:12,410
it's clear that points it only

1159
01:26:12,470 --> 01:26:14,740
any pointed only compare

1160
01:26:14,790 --> 01:26:18,830
with points from neighbouring cells because

1161
01:26:18,830 --> 01:26:20,310
other cells

1162
01:26:20,330 --> 01:26:22,640
can only contain points that have

1163
01:26:22,830 --> 01:26:26,270
a distance of more than epsilon to the point

1164
01:26:26,270 --> 01:26:27,270
OK so

1165
01:26:27,350 --> 01:26:32,520
this is just reduces the number of comparisons between points

1166
01:26:32,560 --> 01:26:36,080
in order to compute the sets neighborhoods

1167
01:26:36,290 --> 01:26:42,990
then there is an interesting extension to the original work which is not really an

1168
01:26:43,010 --> 01:26:47,850
outlier model but i think it's worth mentioning because it's one of the core points

1169
01:26:47,850 --> 01:26:55,160
of the data mining the two authors here came up with the

1170
01:26:55,200 --> 01:26:56,490
the solution to one

1171
01:26:56,510 --> 01:27:00,470
derive knowledge about the outliers to be clear

1172
01:27:00,490 --> 01:27:02,870
what describe the outliers that

1173
01:27:02,890 --> 01:27:04,310
i found

1174
01:27:04,330 --> 01:27:10,140
and so the idea is based on this distance based on family just

1175
01:27:10,160 --> 01:27:11,950
discussed here

1176
01:27:14,290 --> 01:27:20,450
what what they did is they try to find minimal subsets of attributes that explains

1177
01:27:20,450 --> 01:27:22,410
the outlierness of a point

1178
01:27:24,140 --> 01:27:26,220
minimum sets of

1179
01:27:27,200 --> 01:27:29,950
in which the point is still an outlier

1180
01:27:29,970 --> 01:27:30,870
so maybe

1181
01:27:30,890 --> 01:27:32,470
i just tried to

1182
01:27:32,490 --> 01:27:34,540
make it clear by this example here

1183
01:27:34,560 --> 01:27:37,540
so for example the following

1184
01:27:37,680 --> 01:27:38,580
players in

1185
01:27:38,600 --> 01:27:40,620
this is just the sports statistics

1186
01:27:40,620 --> 01:27:41,890
dataset for

1187
01:27:41,910 --> 01:27:47,680
some hockey players you have some measurements like power-play goals short-handed goals whatever

1188
01:27:47,680 --> 01:27:50,040
not to be in pocket expert

1189
01:27:50,890 --> 01:27:53,830
i understand that so the point is

1190
01:27:54,290 --> 01:27:56,510
well these four guys

1191
01:27:58,540 --> 01:28:05,370
detected as outliers with this model here and the intensional knowledge derived from this result

1192
01:28:05,910 --> 01:28:12,830
the following summary linear is an outlier all already in the one dimensional space policy

1193
01:28:12,850 --> 01:28:16,640
they also just considering only this attribute means

1194
01:28:16,740 --> 01:28:19,350
or would already find

1195
01:28:19,370 --> 01:28:21,250
this player is an outlier

1196
01:28:22,160 --> 01:28:27,410
also the two dimensional space of shorthanded goals and game-winning goals so just considering to

1197
01:28:27,410 --> 01:28:28,540
these two

1198
01:28:29,180 --> 01:28:31,310
attributes here but also

1199
01:28:31,370 --> 01:28:33,160
be enough to identify

1200
01:28:34,990 --> 01:28:36,910
as is an outlier

1201
01:28:36,930 --> 01:28:38,310
OK so

1202
01:28:38,930 --> 01:28:42,700
hope some of the year so the method tries to

1203
01:28:42,750 --> 01:28:45,330
find the minimum subset of attributes

1204
01:28:47,020 --> 01:28:49,850
it is already sufficient to

1205
01:28:49,870 --> 01:28:52,660
make this point and of life

1206
01:28:54,370 --> 01:28:55,620
and the same for the other

1207
01:28:55,660 --> 01:28:57,220
the other guys here

1208
01:28:57,220 --> 01:29:03,350
and this can cause very interesting because that gives the domain expert more

1209
01:29:03,370 --> 01:29:05,370
insight into the data dataset

1210
01:29:05,450 --> 01:29:10,970
just set telling hockey free camera lemieux an outlier

1211
01:29:11,140 --> 01:29:14,330
the frequency of course is an outstanding player

1212
01:29:14,350 --> 01:29:19,850
but on the other side you can say OK he's out outstanding because as

1213
01:29:20,470 --> 01:29:24,930
how goals seems to have a lot of power play goals and

1214
01:29:24,950 --> 01:29:27,060
a lot of shorthanded goals

1215
01:29:27,080 --> 01:29:29,200
and whatever

1216
01:29:29,200 --> 01:29:33,510
so given a little bit more than just the outcast but also an explanation

1217
01:29:33,520 --> 01:29:35,600
what constitutes the

1218
01:29:35,660 --> 01:29:42,540
other alkali other distance based approaches are usually based on

1219
01:29:42,560 --> 01:29:46,200
the distance to the k nearest neighbors of objects

1220
01:29:46,200 --> 01:29:47,810
so they are

1221
01:29:47,830 --> 01:29:54,220
basically two two variants here the first model just takes the k nearest neighbour distance

1222
01:29:54,220 --> 01:29:56,450
is the distance to the k nearest neighbour

1223
01:29:56,450 --> 01:29:57,700
o point

1224
01:29:57,740 --> 01:30:01,740
the k nearest neighbour distance of a point is an outlier score

1225
01:30:01,790 --> 01:30:02,770
the higher the

1226
01:30:02,810 --> 01:30:04,740
the distance

1227
01:30:04,740 --> 01:30:06,510
the higher the outline of

1228
01:30:06,520 --> 01:30:08,040
the smaller the distance

1229
01:30:08,040 --> 01:30:09,680
the more the point is

1230
01:30:09,700 --> 01:30:10,660
and normal

1231
01:30:12,410 --> 01:30:16,080
and the second variant here is

1232
01:30:16,120 --> 01:30:19,370
that not only the k nearest neighbour distance is considered but

1233
01:30:19,370 --> 01:30:22,540
and commissioned at the end of the

1234
01:30:22,550 --> 01:30:27,580
talking to you about modern financial time series using grammatical evolution

1235
01:30:27,630 --> 01:30:29,110
this is basically

1236
01:30:29,120 --> 01:30:34,930
based on work by myself and steve phelps

1237
01:30:35,330 --> 01:30:40,130
grammatical evolution is just basically genetic programming approach

1238
01:30:40,700 --> 01:30:46,080
but unlike in genetic programming where you work directly under solutions in grammatical evolution you

1239
01:30:47,010 --> 01:30:49,440
on integers strings and

1240
01:30:49,490 --> 01:30:52,420
natural selection happens

1241
01:30:55,220 --> 01:30:59,590
programs basically

1242
01:30:59,640 --> 01:31:08,200
generally when in modeling financial time series when

1243
01:31:08,210 --> 01:31:10,440
you find finer function

1244
01:31:10,480 --> 01:31:14,560
effects like an econometric model modelling

1245
01:31:14,900 --> 01:31:16,790
the basic you know

1246
01:31:17,100 --> 01:31:18,810
such that

1247
01:31:18,850 --> 01:31:23,070
the function f x explains your data

1248
01:31:23,090 --> 01:31:26,280
are which here is is really just the log returns

1249
01:31:26,290 --> 01:31:27,830
given the constraints

1250
01:31:27,850 --> 01:31:31,670
and when you have so many parameters to deal with

1251
01:31:31,720 --> 01:31:33,310
the search space

1252
01:31:33,320 --> 01:31:34,340
it's quite big

1253
01:31:35,180 --> 01:31:37,160
you need

1254
01:31:37,180 --> 01:31:39,030
some sort of

1255
01:31:39,040 --> 01:31:40,680
heuristic in trying to

1256
01:31:40,690 --> 01:31:45,150
look for promising areas in that search space

1257
01:31:45,200 --> 01:31:50,700
and our approach basically we use and grammatical evolution to try and find a mathematical

1258
01:31:51,900 --> 01:31:54,620
for the return at time t

1259
01:31:54,670 --> 01:31:59,320
and in assessing we want that model to be profitable basically

1260
01:31:59,340 --> 01:32:03,890
and how we assess the profitability of the models is basically using the sharpe ratio

1261
01:32:04,860 --> 01:32:06,890
we have omitted the risk free rate

1262
01:32:08,560 --> 01:32:13,090
well we trade in high frequency and bandwidth assume that in high frequency

1263
01:32:13,100 --> 01:32:14,840
the risk free rate

1264
01:32:14,850 --> 01:32:17,510
it's in significant

1265
01:32:18,220 --> 01:32:22,740
along the return the predicted return is greater than zero

1266
01:32:22,750 --> 01:32:24,160
or short the project

1267
01:32:25,060 --> 01:32:26,470
it's less than zero

1268
01:32:26,530 --> 01:32:28,110
in doing things this way

1269
01:32:28,830 --> 01:32:32,720
a model that basically learns to predict

1270
01:32:32,740 --> 01:32:35,430
to get to get writes most of the time

1271
01:32:35,440 --> 01:32:39,430
would make a high a higher sharpe ratio

1272
01:32:39,440 --> 01:32:42,470
while the model that kind of

1273
01:32:42,520 --> 01:32:50,970
get it wrong most of the time is penalized

1274
01:32:52,130 --> 01:32:54,660
what we do in this

1275
01:32:54,710 --> 01:32:58,980
in the words of marrying you

1276
01:32:59,200 --> 01:33:01,380
heuristic search is like

1277
01:33:01,450 --> 01:33:03,370
having the gumball machine

1278
01:33:03,830 --> 01:33:05,870
you stick acquired in

1279
01:33:05,880 --> 01:33:08,100
and you get the gumball out

1280
01:33:08,140 --> 01:33:10,960
so you you make an initial guess is

1281
01:33:11,010 --> 01:33:15,830
and you hope and that you know evolutionary operators of mutation in

1282
01:33:15,840 --> 01:33:18,460
natural selection will natural selection mainly

1283
01:33:18,470 --> 01:33:20,030
i would move the search

1284
01:33:20,040 --> 01:33:22,380
tools like a promising area

1285
01:33:22,430 --> 01:33:26,900
but i mean because you have no knowledge regarding the search space knowledge and knowledge

1286
01:33:26,900 --> 01:33:30,270
of the state space you could end up in area that's

1287
01:33:32,240 --> 01:33:33,690
so you kind of need to

1288
01:33:33,760 --> 01:33:38,060
sample the search space several times

1289
01:33:38,110 --> 01:33:39,800
which is what we have done so

1290
01:33:39,860 --> 01:33:45,510
for thirty experiments were done thirty experiments and for each of those thirty experiments we

1291
01:33:47,200 --> 01:33:49,610
two hundred solutions

1292
01:33:49,620 --> 01:33:55,370
we run the algorithm and we take you know we observe basically the

1293
01:33:55,380 --> 01:33:57,070
the distribution of fitness

1294
01:33:57,080 --> 01:34:04,250
along the way and the distribution of fitness in the final solution

1295
01:34:04,270 --> 01:34:09,680
basically we use and

1296
01:34:09,730 --> 01:34:11,130
we're testing this

1297
01:34:11,140 --> 01:34:11,930
use in

1298
01:34:11,970 --> 01:34:14,760
high frequency data

1299
01:34:14,780 --> 01:34:17,600
four three forty stocks

1300
01:34:17,610 --> 01:34:20,430
these are basically it's basically clean data

1301
01:34:20,470 --> 01:34:25,900
take take data samples at five minute intervals or cleaned and everything

1302
01:34:25,940 --> 01:34:29,130
we test for autocorrelation user lose

1303
01:34:29,390 --> 01:34:35,510
i'm not going to try pronounce that and basically the test is negative for the

1304
01:34:35,510 --> 01:34:38,150
hypothesis that

1305
01:34:38,210 --> 01:34:43,690
there's also correlation

1306
01:34:43,710 --> 01:34:47,840
and of course what we expect this is what we've done is we've broken the

1307
01:34:49,130 --> 01:34:50,730
and to four samples

1308
01:34:50,780 --> 01:34:54,960
so and what for each run

1309
01:34:54,970 --> 01:34:57,200
we train the algorithm

1310
01:34:57,250 --> 01:34:58,870
in sample

1311
01:34:58,920 --> 01:35:00,160
and then we used

1312
01:35:00,270 --> 01:35:05,510
for the saturdays all three of sets of data in evaluating the algorithm for on

1313
01:35:06,140 --> 01:35:10,020
use the second set is basically a fold cross validation

1314
01:35:12,870 --> 01:35:15,020
as expected in sample

1315
01:35:15,030 --> 01:35:16,460
what happens is

1316
01:35:16,500 --> 01:35:18,570
natural selection

1317
01:35:18,580 --> 01:35:21,940
so to move the distribution of

1318
01:35:23,590 --> 01:35:24,990
towards the

1319
01:35:25,050 --> 01:35:27,050
positive tales

1320
01:35:27,090 --> 01:35:32,110
which is what we want so basically that's the initial distribution

1321
01:35:32,170 --> 01:35:34,700
of solutions

1322
01:35:34,710 --> 01:35:37,820
and and that's the final distribution of solutions

1323
01:35:37,830 --> 01:35:39,520
natural selection kind of

1324
01:35:41,180 --> 01:35:43,300
final distribution of solution

1325
01:35:43,350 --> 01:35:46,320
towards problems in areas which is what we get

1326
01:35:50,940 --> 01:35:52,220
this is

1327
01:35:52,230 --> 01:35:56,390
pretty much saying the same thing what we're seeing here is the average mean fitness

1328
01:35:56,410 --> 01:35:58,660
four thirty experiments

1329
01:35:58,680 --> 01:36:01,170
as we go through generations

1330
01:36:01,180 --> 01:36:06,620
and as you can see here for the three stocks we have improvement standard deviation

1331
01:36:07,370 --> 01:36:08,390
in fitness

1332
01:36:08,420 --> 01:36:10,040
is reduced reducing in the

1333
01:36:10,050 --> 01:36:18,860
the entire population is sort of convergence to one area of the solution space

1334
01:36:22,080 --> 01:36:23,020
how do we know

1335
01:36:24,940 --> 01:36:30,600
the prediction wall how we know what this is basically out of sample performance thirty

1336
01:36:30,600 --> 01:36:32,480
best solutions

1337
01:36:32,910 --> 01:36:34,200
produced by GE

1338
01:36:34,210 --> 01:36:35,590
the three stocks

1339
01:36:35,610 --> 01:36:36,830
compared to

1340
01:36:36,950 --> 01:36:38,150
fair quite

1341
01:36:38,160 --> 01:36:39,780
so what we have done this

1342
01:36:39,790 --> 01:36:42,790
at each time interval

1343
01:36:42,980 --> 01:36:48,350
mind you it but we're basically making decision time away in our holding period and

1344
01:36:48,350 --> 01:36:51,620
frequency of training is the same thing

1345
01:36:52,430 --> 01:36:55,150
at each time interval with flipping a coin

1346
01:36:55,200 --> 01:37:01,090
well it's going long ago short and that's what basically for the FCS is quite

1347
01:37:01,170 --> 01:37:04,920
and we're making a decision whether to go along the schoolgirl short with the generated

1348
01:37:04,950 --> 01:37:06,970
basic a thirty by

1349
01:37:06,970 --> 01:37:13,480
it's often the final stage of long-term relationships that have gone back

1350
01:37:13,490 --> 01:37:16,800
we don't share information with each other anymore

1351
01:37:16,810 --> 01:37:18,510
so there's no intimacy

1352
01:37:18,600 --> 01:37:22,770
we don't feel physically attracted to each other anymore

1353
01:37:22,820 --> 01:37:25,800
there's no passion

1354
01:37:25,820 --> 01:37:29,180
but we better stay together for the kids

1355
01:37:29,240 --> 01:37:32,550
always better stay together for appearances sake

1356
01:37:32,560 --> 01:37:36,670
or better together because financially it would be a disaster if we don't

1357
01:37:36,730 --> 01:37:39,850
all the reasons other than intimacy and passion

1358
01:37:39,870 --> 01:37:45,140
that people might commit to each other that's what star calls and you know what's

1359
01:37:46,530 --> 01:37:50,400
is in societies where marriages are arranged

1360
01:37:50,410 --> 01:37:52,930
this is often the first stage

1361
01:37:53,720 --> 01:37:55,870
of a love relationship

1362
01:37:55,880 --> 01:38:00,760
these two people who have maybe never seen each other before who have never shared

1363
01:38:00,760 --> 01:38:02,810
secrets there's no intimacy

1364
01:38:02,870 --> 01:38:07,690
who have that don't know if they are physically attracted to each other or

1365
01:38:07,700 --> 01:38:14,390
on their wedding day revealed to each other and committed legally and sometimes religiously

1366
01:38:14,400 --> 01:38:16,000
to each other

1367
01:38:16,020 --> 01:38:20,730
right the common is there but at that moment nothing else might be there

1368
01:38:20,800 --> 01:38:25,700
what's interesting of course is that such relationships don't seem to have any

1369
01:38:25,730 --> 01:38:27,310
greater chance

1370
01:38:27,570 --> 01:38:29,910
ending in divorce

1371
01:38:30,750 --> 01:38:33,070
people who marry for love

1372
01:38:33,080 --> 01:38:36,000
but there's a big con found big problem

1373
01:38:36,030 --> 01:38:41,170
in studies of those kind of relationships what might have be

1374
01:38:42,030 --> 01:38:45,700
what might be the problem in this statement i just made these kind of relationships

1375
01:38:45,970 --> 01:38:49,770
are just as likely to survive people who marry for love

1376
01:38:53,120 --> 01:38:54,890
so they they the data

1377
01:38:54,930 --> 01:38:59,030
the cards are more likely to occur in societies

1378
01:38:59,050 --> 01:39:00,810
that frown on divorce

1379
01:39:00,820 --> 01:39:04,420
make it very costly socially costly

1380
01:39:04,440 --> 01:39:08,790
to divorce so they they stay together for all kinds of reasons not always such

1381
01:39:08,810 --> 01:39:10,150
good ones i now

1382
01:39:10,640 --> 01:39:14,170
who was who was it who sang the song

1383
01:39:14,190 --> 01:39:16,610
two out of three ain't bad

1384
01:39:16,780 --> 01:39:19,060
seven meat loaf

1385
01:39:21,920 --> 01:39:27,240
it was not presently professor as it was meatloaf it was life

1386
01:39:27,300 --> 01:39:31,260
are all saying there was a so called because

1387
01:39:31,300 --> 01:39:35,740
we love saying on two out of three above let's see if two out of

1388
01:39:36,420 --> 01:39:38,970
a band what you have

1389
01:39:39,020 --> 01:39:44,280
intimacy we share secrets passion we feel physically

1390
01:39:44,290 --> 01:39:48,470
attracted to each other but we're not making any commitments here

1391
01:39:48,490 --> 01:39:55,240
star recalls that romantic love this is a physical attraction with close binding but no

1392
01:39:55,240 --> 01:39:59,490
commitment romario and julie when they first met

1393
01:39:59,530 --> 01:40:03,650
this is the way relationships start

1394
01:40:03,650 --> 01:40:07,400
we like each other physically attracted to each other due to you

1395
01:40:07,480 --> 01:40:11,780
i enjoy spending time with you but i'm not making any long-term commitments are not

1396
01:40:11,780 --> 01:40:17,270
even willing to use the l word in in describing what it is we have

1397
01:40:17,310 --> 01:40:22,160
right many of you might have been in relationships of the sort that that's wrong

1398
01:40:22,170 --> 01:40:26,890
that's romantic love now what we have intimacy we share

1399
01:40:27,160 --> 01:40:29,050
secrets with each other

1400
01:40:29,060 --> 01:40:31,770
but there is no particular physical attraction

1401
01:40:31,770 --> 01:40:34,740
but we are really committed to this

1402
01:40:35,930 --> 01:40:40,920
this is what the system recalls companion in love this is your best friend

1403
01:40:40,930 --> 01:40:47,120
we are committed to sharing intimacy to be friends forever but physical attraction is not

1404
01:40:47,120 --> 01:40:48,540
part of the equation

1405
01:40:49,370 --> 01:40:51,470
this is sort of the

1406
01:40:51,480 --> 01:40:53,160
maybe the greek ideal

1407
01:40:55,530 --> 01:40:56,800
of some kind

1408
01:40:56,810 --> 01:40:59,040
well what if we have partial

1409
01:40:59,050 --> 01:41:00,790
and sexually attracted to you

1410
01:41:00,800 --> 01:41:04,860
but no intimacy i don't really know that much about you i don't really show

1411
01:41:04,860 --> 01:41:05,810
anything i

1412
01:41:07,160 --> 01:41:11,370
but i am committed to maintaining this physical attraction

1413
01:41:12,530 --> 01:41:13,880
well that's

1414
01:41:13,910 --> 01:41:19,410
what system recalls that she was love it's a whirlwind courtship it's a hollywood romance

1415
01:41:19,660 --> 01:41:22,050
and might lead to a shotgun wedding

1416
01:41:22,070 --> 01:41:26,070
maybe you find yourself in las vegas and you get married for a day and

1417
01:41:26,070 --> 01:41:27,390
a half and

1418
01:41:27,400 --> 01:41:31,000
and then i realized that this wasn't such a good idea

1419
01:41:31,040 --> 01:41:35,100
maybe it was britney in years ago well you get

1420
01:41:35,120 --> 01:41:36,510
you get the idea

1421
01:41:37,860 --> 01:41:40,060
that's that's fatuous love

1422
01:41:40,070 --> 01:41:44,050
we are we are basically committed to each other

1423
01:41:44,070 --> 01:41:45,610
for sex

1424
01:41:45,620 --> 01:41:48,830
but it it's very hard to make those relationships last a long time because we

1425
01:41:48,830 --> 01:41:52,630
might not have anything in common we might share anything with each other we might

1426
01:41:52,630 --> 01:41:55,920
not trust each other we're not particularly bonded

1427
01:41:56,020 --> 01:42:00,370
to each other on the other hand if you have all three intimacy passion commitment

1428
01:42:00,500 --> 01:42:02,330
this is constant love

1429
01:42:02,380 --> 01:42:05,470
according to sternberg complete love this

1430
01:42:05,520 --> 01:42:06,870
is how

1431
01:42:06,880 --> 01:42:09,020
he defines one

1432
01:42:09,060 --> 01:42:12,040
OK so now we have the definition of love and you can now

1433
01:42:12,240 --> 01:42:16,120
as a homework assignment sit down tonight

1434
01:42:16,140 --> 01:42:19,270
and make a list of every person you know

1435
01:42:19,290 --> 01:42:23,630
by the three elements of love

1436
01:42:23,660 --> 01:42:27,140
and just start putting the check marks in the boxes

1437
01:42:27,170 --> 01:42:29,900
and telling up your personal love

1438
01:42:29,910 --> 01:42:31,070
box scores

1439
01:42:31,080 --> 01:42:36,150
and we don't want to collect those we don't even wanna see those but

1440
01:42:36,150 --> 01:42:38,540
you can have fun with that then you can ask the other people to do

1441
01:42:38,540 --> 01:42:40,870
two and you can compare with each other

1442
01:42:40,890 --> 01:42:44,040
and you know if you will survive this exercise you will be

1443
01:42:44,110 --> 01:42:49,030
better for you know what doesn't kill you makes you stronger that's the idea behind

1444
01:42:49,030 --> 01:42:51,570
that exercise

1445
01:42:51,630 --> 01:42:54,200
now the social psychology

1446
01:42:54,220 --> 01:42:58,540
of love really has been a social psychology of attraction

1447
01:42:58,610 --> 01:43:00,580
what makes people

1448
01:43:00,610 --> 01:43:06,150
find each other attractive what makes them want to be intimate what makes them

1449
01:43:06,160 --> 01:43:11,400
physically desirable to each other what might lead to a commitment

1450
01:43:11,420 --> 01:43:14,390
the decision to make a commitment to making

1451
01:43:16,990 --> 01:43:20,650
this is just so that giving this lecture level two if you're holding hands here

1452
01:43:20,650 --> 01:43:23,250
and in the front really

1453
01:43:23,450 --> 01:43:36,100
all three elements present in university passionately

1454
01:43:36,290 --> 01:43:39,810
just just checking just check

1455
01:43:39,810 --> 01:43:42,850
right now when we look at how that appears in the theorem

1456
01:43:42,870 --> 01:43:46,040
it is the theory tells us that if we got a class

1457
01:43:46,250 --> 01:43:48,370
a class of

1458
01:43:48,470 --> 01:43:52,180
functions that take values in some bounded interval

1459
01:43:53,890 --> 01:43:57,680
then expectations are uniformly close to sample averages

1460
01:43:57,680 --> 01:43:59,910
where provided that

1461
01:43:59,950 --> 01:44:02,660
this notion of complexity is small

1462
01:44:02,700 --> 01:44:05,810
right so if we are in the case where you know we're not doing much

1463
01:44:05,810 --> 01:44:06,990
better than

1464
01:44:06,990 --> 01:44:10,020
if we try this supreme away and we have just single function this thing would

1465
01:44:10,020 --> 01:44:14,200
be zero right if we have a much richer class than that in the supreme

1466
01:44:14,240 --> 01:44:15,560
lexus lineup up really well

1467
01:44:15,930 --> 01:44:19,560
this is a large value then the theorem becomes weaker

1468
01:44:19,600 --> 01:44:21,140
right we get a much bigger

1469
01:44:22,120 --> 01:44:25,910
here on the right-hand side the we can't guarantee that the the

1470
01:44:26,790 --> 01:44:29,330
expectation is not much bigger than that

1471
01:44:29,390 --> 01:44:33,700
the sample average that's i've written inequality in one direction here i mean i could

1472
01:44:33,700 --> 01:44:37,350
have put absolute values around the difference between

1473
01:44:37,700 --> 01:44:41,080
the expectation the sample average it

1474
01:44:43,020 --> 01:44:46,890
you know the interval being zero one is not really crucial it could be any

1475
01:44:46,890 --> 01:44:49,640
bounded interval so i could consider negative functions

1476
01:44:51,200 --> 01:44:52,620
the the set of

1477
01:44:52,620 --> 01:44:55,790
of negative g where g comes from g and get the same thing so you

1478
01:44:55,790 --> 01:44:56,700
know i

1479
01:44:56,750 --> 01:44:59,580
i mean i i can get exactly the same inequality but two-sided i guess i'm

1480
01:45:01,100 --> 01:45:06,790
and i should say in everything that that i'm doing really relying on the fact

1481
01:45:06,790 --> 01:45:08,470
that we have bounded

1482
01:45:08,520 --> 01:45:12,870
random variables will see later that this is this is crucial for the concentration inequalities

1483
01:45:12,870 --> 01:45:13,740
that we

1484
01:45:13,770 --> 01:45:14,890
what we need

1485
01:45:14,910 --> 01:45:18,220
i think it it could be relaxed if you know something about the tails of

1486
01:45:18,270 --> 01:45:22,580
of you know certain distributions will appear but i that

1487
01:45:22,640 --> 01:45:27,120
and in that case for binary valued functions nine in fact for everything that we

1488
01:45:27,120 --> 01:45:31,620
do know these tools are powerful enough we can we can always work with bounded

1489
01:45:31,700 --> 01:45:33,720
loss functions

1490
01:45:33,740 --> 01:45:36,350
OK so

1491
01:45:36,410 --> 01:45:37,580
so let's

1492
01:45:37,580 --> 01:45:41,470
take a look at

1493
01:45:41,490 --> 01:45:44,240
how this theorem is proved

1494
01:45:44,250 --> 01:45:46,850
what we're looking at here i guess one crucial thing that

1495
01:45:47,290 --> 01:45:51,520
they should emphasise again is that we're talking about a result that is uniform over

1496
01:45:51,520 --> 01:45:52,600
the class

1497
01:45:52,600 --> 01:45:53,430
right so

1498
01:45:53,430 --> 01:45:55,640
so with high probability

1499
01:45:56,430 --> 01:45:58,470
every function in our class

1500
01:45:58,490 --> 01:46:01,370
has expectations sample average close

1501
01:46:01,430 --> 01:46:04,540
OK and that means if we choose some function to minimize

1502
01:46:04,540 --> 01:46:08,910
the empirical risk minimiser sample average of the loss

1503
01:46:08,910 --> 01:46:12,710
then we know that the loss for that function that minimizes is is going to

1504
01:46:12,710 --> 01:46:14,750
be not too big

1505
01:46:14,810 --> 01:46:15,970
OK so

1506
01:46:17,330 --> 01:46:21,490
you know it's a it's a uniform convergence results

1507
01:46:21,600 --> 01:46:23,830
it tells us about

1508
01:46:24,120 --> 01:46:26,910
the uniform deviations between

1509
01:46:27,740 --> 01:46:30,390
expectations and then sample averages

1510
01:46:30,430 --> 01:46:34,450
OK so what we really concerned with here

1511
01:46:34,540 --> 01:46:37,680
saying you know that the probability of the event that they

1512
01:46:37,700 --> 01:46:41,180
the maximum over this class of the difference between these two

1513
01:46:41,250 --> 01:46:44,200
big that probability should be small

1514
01:46:44,200 --> 01:46:46,220
OK so

1515
01:46:46,220 --> 01:46:48,410
so we're interested in this quantity

1516
01:46:48,470 --> 01:46:50,950
OK the maximum over hough class

1517
01:46:51,600 --> 01:46:57,220
of the difference between expectations and sample averages

1518
01:46:57,240 --> 01:47:00,200
OK and the concentration result that we use for that

1519
01:47:00,220 --> 01:47:03,370
particular theorem is is

1520
01:47:03,870 --> 01:47:06,060
a general one

1521
01:47:07,220 --> 01:47:10,270
called the demons inequalities also known as

1522
01:47:10,390 --> 01:47:11,680
thing is among

1523
01:47:13,520 --> 01:47:18,540
there there

1524
01:47:18,560 --> 01:47:20,020
there's another name that

1525
01:47:20,060 --> 01:47:22,830
skating for a moment but you know this is this is the result i guess

1526
01:47:22,830 --> 01:47:24,270
appeared in a paper of

1527
01:47:24,330 --> 01:47:27,470
o something equivalent in a paper of lifting

1528
01:47:27,470 --> 01:47:29,430
you know a very long time ago

1529
01:47:31,810 --> 01:47:35,490
so this result is the concentration result for

1530
01:47:35,520 --> 01:47:38,870
founded random variables

1531
01:47:38,890 --> 01:47:41,290
it tells us that

1532
01:47:41,350 --> 01:47:43,410
that if

1533
01:47:43,410 --> 01:47:47,270
it in particular you know we have a random variable such as this one

1534
01:47:47,290 --> 01:47:51,410
that depends on x one through x n right so this thing is just the

1535
01:47:51,410 --> 01:47:56,700
maximum over a class of expectation one sample averages OK that's random because it's it's

1536
01:47:56,700 --> 01:48:00,850
a function of the random variables x one through x and through the sample average

1537
01:48:00,890 --> 01:48:06,140
OK now make demons inequality tells us that if we have a function of i

1538
01:48:06,140 --> 01:48:09,970
i d random variables as we do here

1539
01:48:13,470 --> 01:48:18,220
then we get concentration of the

1540
01:48:18,250 --> 01:48:24,250
one of the random variable about its expectations

1541
01:48:24,270 --> 01:48:31,410
o provided that no one of those independent random variables has a big influence

1542
01:48:33,290 --> 01:48:34,540
how how do we

1543
01:48:34,540 --> 01:48:37,930
formalise that so the random variable with thinking of is this one

1544
01:48:37,990 --> 01:48:43,140
in our case but let's think now about well OK so so this thing is

1545
01:48:43,140 --> 01:48:45,850
a function of x one through xn

1546
01:48:45,870 --> 01:48:47,240
and the crucial

1547
01:48:47,250 --> 01:48:54,140
condition that's supply-and-demand inequality is that when one of these independent random variables changes the

1548
01:48:54,180 --> 01:48:58,180
the function that we have to changed by much so how do we measure that

1549
01:48:58,180 --> 01:49:00,390
will the sample average of g

1550
01:49:00,430 --> 01:49:03,810
g is a function that takes values in the interval zero one

1551
01:49:04,620 --> 01:49:06,870
so the sample average

1552
01:49:06,910 --> 01:49:09,410
can't change by any more than one over n

1553
01:49:09,450 --> 01:49:13,700
OK might go from one extreme like zero and in particular x i

1554
01:49:13,700 --> 01:49:18,060
gnx i might change from zero to to one if we change that excite

1555
01:49:18,100 --> 01:49:19,520
OK so

1556
01:49:19,770 --> 01:49:22,270
the same thing must be true for the supreme if we change one of the

1557
01:49:22,270 --> 01:49:26,060
excise this thing can change by more than one over in

1558
01:49:26,430 --> 01:49:31,540
and that means that this random variable is concentrated around its expectation so

1559
01:49:31,540 --> 01:49:36,490
precisely you know the the former dimon's inequality is that you know this random variable

1560
01:49:36,490 --> 01:49:40,160
is not much bigger than its expectation plus a little bit this little bit depends

1561
01:49:40,160 --> 01:49:41,020
on the

1562
01:49:41,060 --> 01:49:46,100
the the fluctuations that we see when we change one of those random variables

1563
01:49:46,160 --> 01:49:49,740
OK so this is a very general and

1564
01:49:49,770 --> 01:49:53,850
nice easy to apply inequality we just we're talking about a random variable that can

1565
01:49:53,850 --> 01:49:58,490
be expressed as a function of a bunch of independent random variables we change any

1566
01:49:58,490 --> 01:50:00,220
one of those

1567
01:50:00,950 --> 01:50:05,270
does the function change but not too much and if that's the case then the

1568
01:50:05,790 --> 01:50:08,060
random variable is is

1569
01:50:08,100 --> 01:50:14,140
concentrated around its expectation at this kind of a at a level it depends on

1570
01:50:14,410 --> 01:50:15,450
the size of the

1571
01:50:15,490 --> 01:50:17,580
the fluctuations

1572
01:50:17,600 --> 01:50:20,040
OK so that's at the heart

1573
01:50:20,060 --> 01:50:22,620
of of the profile right so

1574
01:50:22,700 --> 01:50:25,930
let me state this concentration inequality

1575
01:50:30,740 --> 01:50:32,990
so we have

1576
01:50:32,990 --> 01:50:34,570
so an elegant program

1577
01:50:34,740 --> 01:50:36,660
it doesn't matter

1578
01:50:36,660 --> 01:50:40,240
if it if it's very very slow as long as it's the smallest program that

1579
01:50:40,240 --> 01:50:42,010
calculates what it calculates

1580
01:50:42,030 --> 01:50:44,490
and it calculates an infinite amount of time

1581
01:50:44,550 --> 01:50:47,070
OK so that's important

1582
01:50:49,550 --> 01:50:53,160
because if you put a bound on the time you you you you can find

1583
01:50:53,390 --> 01:50:55,380
an elegant program for something

1584
01:50:55,390 --> 01:50:58,430
it's only when there is no bound on the run time

1585
01:50:58,470 --> 01:51:03,340
so anyway so let me repeat crucial result is you can never prove that program

1586
01:51:03,340 --> 01:51:04,470
is elegant

1587
01:51:04,550 --> 01:51:07,070
except with financing exceptions

1588
01:51:07,910 --> 01:51:24,300
so let me write that down

1589
01:51:24,470 --> 01:51:28,240
now there may actually be a finite number of exceptions but the infinite number of

1590
01:51:28,240 --> 01:51:30,030
elegant programs so

1591
01:51:30,120 --> 01:51:34,760
let let me give you a more and more precise statement of this this is

1592
01:51:34,760 --> 01:51:38,640
an incomplete list result by the way like that of nineteen thirty one

1593
01:51:38,660 --> 01:51:40,430
it could be result

1594
01:51:40,450 --> 01:51:46,050
saying that there are limits to what reasoning mathematical proof can achieve let me let

1595
01:51:46,050 --> 01:51:49,450
me give you more precise statements

1596
01:51:49,530 --> 01:51:50,890
you need

1597
01:51:50,910 --> 01:51:52,510
you need

1598
01:51:52,550 --> 01:51:56,390
on n bit theory

1599
01:51:56,450 --> 01:52:07,680
to prove that an MBA program development

1600
01:52:07,820 --> 01:52:10,030
so let me say it another way

1601
01:52:10,050 --> 01:52:16,640
two two to prove that you can prove some things

1602
01:52:17,550 --> 01:52:19,320
i have to tell me what methods

1603
01:52:19,380 --> 01:52:23,530
what methods you're use

1604
01:52:25,620 --> 01:52:31,860
the normal way you you prove something can be proved metamathematics is the idea is

1605
01:52:31,860 --> 01:52:37,450
you don't discuss what can be proved informally because it's not clear so usually the

1606
01:52:37,530 --> 01:52:41,530
chen for a result of on probability is you talk about what's called the formal

1607
01:52:41,530 --> 01:52:46,320
axiomatic theory or formal axiomatic system that consists of a set of the finite set

1608
01:52:46,320 --> 01:52:52,030
of axioms and you using symbolic logic that normally would be first-order logic but but

1609
01:52:52,030 --> 01:52:53,110
i don't care

1610
01:52:53,140 --> 01:52:56,180
and the idea is you start with some axioms and then you get all the

1611
01:52:56,180 --> 01:52:59,860
consequences of the axioms and the consequences of the consequences of the axioms and so

1612
01:52:59,860 --> 01:53:03,800
on and so forth and those are the theorems in your formal axiomatic system

1613
01:53:07,320 --> 01:53:10,890
the ancient greeks in in alexandria

1614
01:53:10,930 --> 01:53:13,320
and that in simple cases but in alexandria

1615
01:53:13,340 --> 01:53:17,110
the euclidean put where there was a library the famous library

1616
01:53:17,140 --> 01:53:21,380
put together this wonderful book the elements

1617
01:53:22,530 --> 01:53:26,760
and and there you have it the sample of the mathematical theory where he starts

1618
01:53:26,780 --> 01:53:30,070
off with axioms or postulates and uses consequences

1619
01:53:30,070 --> 01:53:31,450
starting from there

1620
01:53:31,510 --> 01:53:34,720
and the notion of a formal axiomatic theory

1621
01:53:34,970 --> 01:53:41,390
is very similar but instead of using ancient greek or or or english

1622
01:53:41,430 --> 01:53:45,300
use of an artificial language use symbolic logic

1623
01:53:45,320 --> 01:53:47,320
and it has precise grammar

1624
01:53:48,490 --> 01:53:50,640
the key notion

1625
01:53:50,720 --> 01:53:56,030
in my opinion one of the key fact about formal axiomatic theory of formal mathematical

1626
01:53:57,200 --> 01:54:02,300
is that there is a mechanical procedure for deducing all the consequences of the axioms

1627
01:54:02,300 --> 01:54:04,890
for producing all the theorems

1628
01:54:05,010 --> 01:54:07,470
in principle you don't need mathematicians

1629
01:54:07,490 --> 01:54:08,820
in theory

1630
01:54:09,010 --> 01:54:13,450
because what you can do is just systematically started using using symbolic logic to use

1631
01:54:13,450 --> 01:54:16,660
all the consequent the the axioms so you go through all the proofs are one

1632
01:54:16,660 --> 01:54:20,260
step along the positive two steps on all the proof there are three steps long

1633
01:54:20,410 --> 01:54:22,610
there only a finite number of such proofs

1634
01:54:22,700 --> 01:54:24,200
and in this way

1635
01:54:24,220 --> 01:54:27,930
you could in theory when you have the form automatic theory

1636
01:54:27,950 --> 01:54:32,220
you have a mechanical procedure which will deduce all the consequences of the axioms which

1637
01:54:32,220 --> 01:54:36,720
all the theorems in your in your mathematical theory now this will be slow

1638
01:54:36,760 --> 01:54:40,530
and in fact is useless and to do this in practice

1639
01:54:40,570 --> 01:54:47,140
because to get interesting mathematical results you in such a long prove that you know

1640
01:54:47,140 --> 01:54:50,050
the number of possible proves that is

1641
01:54:50,070 --> 01:54:52,930
and steps long grows exponentially on n

1642
01:54:53,970 --> 01:54:58,490
so it's there are only a finite number of proofs of any given size you

1643
01:54:58,490 --> 01:54:59,130
know uniform one

1644
01:54:59,550 --> 01:55:01,320
axiomatic theory

1645
01:55:01,340 --> 01:55:06,510
but in fact you cannot run through all possible proof systematically you can go through

1646
01:55:06,510 --> 01:55:07,860
the tree of all possible

1647
01:55:07,860 --> 01:55:11,950
deductions all possible process in a practical way

1648
01:55:12,010 --> 01:55:16,720
but you can do it in theory because it's it's a large but finite set

1649
01:55:16,720 --> 01:55:20,560
the number of proofs of any given size and in the english literature on this

1650
01:55:20,560 --> 01:55:24,910
the the shape the curve will go a different direction

1651
01:55:25,890 --> 01:55:28,570
so here that on the ROC curve

1652
01:55:28,890 --> 01:55:33,160
random would be one here and the performance of the this

1653
01:55:33,220 --> 01:55:36,560
whereas in our case it goes down this small

1654
01:55:36,570 --> 01:55:38,470
look at the better

1655
01:55:38,490 --> 01:55:44,880
so those little thing to make it makes it quite clear that the different forms

1656
01:55:49,140 --> 01:55:53,920
now we're going to be looking one complication with this is that

1657
01:55:53,990 --> 01:55:55,600
once you've found in the county

1658
01:55:55,630 --> 01:55:59,320
if a set of particular threshold and say that this account is now four we

1659
01:55:59,320 --> 01:56:02,990
should really be keeping in in out of the peer groups from that point on

1660
01:56:03,000 --> 01:56:05,910
so the set of of complications of how to

1661
01:56:05,920 --> 01:56:09,440
following analysis of a very simple way out of this is just look at this

1662
01:56:09,440 --> 01:56:11,100
on a daily basis

1663
01:56:12,390 --> 01:56:18,050
look at one day's worth of transactions build building

1664
01:56:18,090 --> 01:56:21,570
look it up here but we don't is that they these days with of information

1665
01:56:21,570 --> 01:56:23,650
about what we believe is being defrauded

1666
01:56:23,710 --> 01:56:27,430
well the use of pictures to determine outliers

1667
01:56:27,470 --> 01:56:32,520
and then just make a performance because each day in the test data so this

1668
01:56:32,530 --> 01:56:36,110
then ask the question how do we combine the

1669
01:56:36,150 --> 01:56:37,420
performance curve

1670
01:56:37,430 --> 01:56:44,830
so the most straightforward method here is that we we average over a horizontal line

1671
01:56:44,830 --> 01:56:47,340
which basically means that

1672
01:56:47,380 --> 01:56:50,890
assuming that the number of

1673
01:56:51,070 --> 01:56:52,790
accounts activity today

1674
01:56:52,830 --> 01:56:54,500
reporting the same event

1675
01:56:54,520 --> 01:56:56,110
what we're saying is that

1676
01:56:56,130 --> 01:57:01,480
our for given number of for investigations of the company might assume that the same

1677
01:57:01,480 --> 01:57:02,330
each day

1678
01:57:02,330 --> 01:57:10,770
that's kind of what we should be everything online

1679
01:57:10,790 --> 01:57:13,040
so for the experiments and

1680
01:57:13,050 --> 01:57:17,780
look at a smaller subset of data the ones which had i've ordered transactions eighty

1681
01:57:17,780 --> 01:57:21,190
transaction the first three months before the forty three

1682
01:57:21,220 --> 01:57:26,430
that left me actually just four thousand counts six percent of those were defrauded in

1683
01:57:26,430 --> 01:57:28,530
the final fourth month

1684
01:57:29,840 --> 01:57:33,620
we use the first three months to build the peer groups and

1685
01:57:33,670 --> 01:57:38,650
the final each day than to to check out forms

1686
01:57:38,660 --> 01:57:42,700
the the number of parameters involved so close the building of the ips so many

1687
01:57:42,710 --> 01:57:48,690
segments subdivided the data into is initially just as it defaults for seven to eight

1688
01:57:48,700 --> 01:57:52,040
some assuming window that we use to find out whether it's an outline of the

1689
01:57:52,040 --> 01:57:53,750
set seven days

1690
01:57:53,770 --> 01:57:56,280
the size of about one hundred

1691
01:57:56,300 --> 01:58:00,560
and we got a robust method but what is an issue

1692
01:58:01,870 --> 01:58:03,750
the first thing to look at is

1693
01:58:03,770 --> 01:58:08,370
contamination helpful will contaminate picture so here

1694
01:58:08,380 --> 01:58:11,400
well looking at basically the area under the curve so

1695
01:58:11,460 --> 01:58:15,060
the smaller the value the better the performance is between zero and ten percent the

1696
01:58:15,060 --> 01:58:17,980
size of the

1697
01:58:18,000 --> 01:58:21,140
of the data set

1698
01:58:21,750 --> 01:58:26,460
but since we know where we have a lot of old well fort actually occurred

1699
01:58:26,460 --> 01:58:30,660
what i can do is i can just artificially remove them from peer groups once

1700
01:58:31,100 --> 01:58:33,700
defrauded the fact that you can see this is

1701
01:58:34,120 --> 01:58:35,500
these dots underneath

1702
01:58:35,530 --> 01:58:40,590
it makes a big difference with a small group of people it's because get so

1703
01:58:40,600 --> 01:58:46,130
attenuated because the the because of the amount of distance

1704
01:58:46,170 --> 01:58:49,160
can absorb these to better

1705
01:58:49,210 --> 01:58:50,590
but is initially

1706
01:58:50,600 --> 01:58:54,820
and to the can

1707
01:58:54,870 --> 01:58:56,990
changing the

1708
01:58:57,020 --> 01:58:59,890
the degree of granularity of the

1709
01:58:59,950 --> 01:59:03,290
building the pictures are just silly some entire

1710
01:59:04,480 --> 01:59:07,050
time series

1711
01:59:07,050 --> 01:59:09,120
which is one three-dimensional point

1712
01:59:09,410 --> 01:59:12,120
that's how

1713
01:59:12,120 --> 01:59:14,630
we know how good the form to be

1714
01:59:14,650 --> 01:59:18,270
this segment of the the market more detail about

1715
01:59:20,000 --> 01:59:23,270
the times is performance gets more modestly better

1716
01:59:23,290 --> 01:59:25,060
after about

1717
01:59:25,060 --> 01:59:31,470
sixty so we get a modest improvement with more details about the times but again

1718
01:59:31,470 --> 01:59:34,120
with segment data already by

1719
01:59:34,230 --> 01:59:40,310
using the high-volume accounts so they are based on the right

1720
01:59:40,370 --> 01:59:45,750
the summary statistic window this is what we use to one point for draws occur

1721
01:59:45,790 --> 01:59:48,830
you just use one day's worth of data

1722
01:59:48,850 --> 01:59:52,770
we don't have to do the performance as the data

1723
01:59:52,790 --> 01:59:55,250
in the data needs to summarise

1724
01:59:55,290 --> 01:59:59,370
this is a larger force could look that up to seven days and it's kind

1725
01:59:59,850 --> 02:00:02,480
a which is used for instance

1726
02:00:02,520 --> 02:00:04,850
major is in

1727
02:00:04,900 --> 02:00:06,000
critical for

1728
02:00:06,020 --> 02:00:12,830
the banking engine

1729
02:00:12,850 --> 02:00:16,170
so this the beginning that one

1730
02:00:16,210 --> 02:00:22,350
issue about useful thing about using pictures the outliers results last the entire population six

1731
02:00:22,350 --> 02:00:24,600
hundred fifty investigate this

1732
02:00:24,620 --> 02:00:26,560
and the support is just

1733
02:00:26,600 --> 02:00:32,870
based on wikipedia decides on top population minus the target account you're looking at

1734
02:00:35,600 --> 02:00:38,560
well in the next room is the some

1735
02:00:40,650 --> 02:00:46,210
percy we don't robustify using some of the parameters set by the beginning we get

1736
02:00:46,210 --> 02:00:49,060
this kind of performance

1737
02:00:49,080 --> 02:00:49,900
if every

1738
02:00:49,920 --> 02:00:52,810
removed four fourteen ten

1739
02:00:52,810 --> 02:00:58,480
like an organ within the contamination before it's likely that form the song down here

1740
02:01:01,310 --> 02:01:07,230
robustifying will still get a modest performance again

1741
02:01:07,270 --> 02:01:09,900
but compared to

1742
02:01:09,940 --> 02:01:15,350
the global methods this is quite correct the impact four

1743
02:01:15,350 --> 02:01:17,690
the labels

1744
02:01:19,350 --> 02:01:22,000
here we

1745
02:01:44,170 --> 02:01:53,860
if you are

1746
02:01:56,130 --> 02:02:03,510
in the winter

1747
02:02:03,570 --> 02:02:05,790
are a part

1748
02:02:09,730 --> 02:02:11,230
is a

1749
02:02:42,360 --> 02:02:44,990
you never

1750
02:02:57,900 --> 02:03:04,560
o thing that

1751
02:03:16,230 --> 02:03:19,750
are you

1752
02:03:28,270 --> 02:03:32,540
the other thing around here

1753
02:03:39,430 --> 02:03:43,180
so it's which

1754
02:03:43,190 --> 02:03:48,650
it's probably not

1755
02:03:48,670 --> 02:03:54,900
and the international community wrong

1756
02:03:54,910 --> 02:03:57,590
can't see all

1757
02:04:06,590 --> 02:04:09,120
so that

1758
02:04:14,930 --> 02:04:23,500
are the

1759
02:04:39,240 --> 02:04:43,680
for example

1760
02:05:02,290 --> 02:05:05,210
we also see

1761
02:05:05,230 --> 02:05:08,650
he a

1762
02:05:11,200 --> 02:05:16,740
the best

1763
02:05:36,460 --> 02:05:41,850
a lot of work in

1764
02:05:46,930 --> 02:05:56,810
in each

1765
02:06:00,130 --> 02:06:02,480
we use

1766
02:06:02,500 --> 02:06:08,190
so here here here

1767
02:06:08,190 --> 02:06:10,560
and my very first lecture

1768
02:06:10,560 --> 02:06:13,130
and the one that we used yesterday

1769
02:06:13,180 --> 02:06:15,280
in in the laboratory

1770
02:06:15,290 --> 02:06:17,120
it was the second function

1771
02:06:17,160 --> 02:06:17,700
so that they

1772
02:06:17,950 --> 02:06:20,520
this our true function

1773
02:06:20,560 --> 02:06:23,370
and we make IID observations

1774
02:06:23,860 --> 02:06:25,630
at each x

1775
02:06:26,710 --> 02:06:27,950
target value

1776
02:06:28,010 --> 02:06:32,570
of course is going to be the deterministic component so our true function

1777
02:06:32,580 --> 02:06:35,340
contaminated with some

1778
02:06:35,390 --> 02:06:40,350
some noise

1779
02:06:40,360 --> 02:06:43,220
so let's not take functional view

1780
02:06:43,280 --> 02:06:44,740
of modelling

1781
02:06:44,760 --> 02:06:46,600
rather than

1782
02:06:46,690 --> 02:06:48,510
the parametric view that we took

1783
02:06:48,510 --> 02:06:52,190
on the first two two sets of lectures

1784
02:06:52,210 --> 02:06:56,520
so things simplify no because of what we see as well

1785
02:06:56,540 --> 02:06:58,480
we're going to have a set of functions

1786
02:07:00,020 --> 02:07:01,710
and they going to be conditioned on

1787
02:07:01,730 --> 02:07:04,160
the set of attributes that we have

1788
02:07:04,170 --> 02:07:07,620
and and then in the parameters that define the covariance function

1789
02:07:07,660 --> 02:07:09,260
associated with my

1790
02:07:09,260 --> 02:07:10,990
my prior

1791
02:07:11,020 --> 02:07:13,120
so i'll just define

1792
02:07:13,170 --> 02:07:16,560
the prior my functions as this multivariate gaussian

1793
02:07:16,560 --> 02:07:17,940
it was a zero

1794
02:07:18,050 --> 02:07:19,670
train function

1795
02:07:19,690 --> 02:07:21,550
and the covariance function

1796
02:07:22,980 --> 02:07:27,780
well the talk about larger shortly

1797
02:07:27,790 --> 02:07:30,720
the likelihood of our data

1798
02:07:32,300 --> 02:07:34,800
i really specific realization

1799
02:07:34,820 --> 02:07:35,710
of our

1800
02:07:39,730 --> 02:07:44,010
the noise variance we know of course there's no just a product

1801
02:07:44,100 --> 02:07:46,950
of gaussians

1802
02:07:46,980 --> 02:07:51,030
which is now centered on the value of the functional value

1803
02:07:52,990 --> 02:07:57,350
which has been drawn from a gaussian process prior

1804
02:07:57,390 --> 02:08:00,580
and this of course is not just are

1805
02:08:00,610 --> 02:08:02,090
a spherical goes in

1806
02:08:02,100 --> 02:08:04,550
in n dimensional space seeing this

1807
02:08:06,990 --> 02:08:09,600
of course we are all good bayesians for the next hour

1808
02:08:10,900 --> 02:08:13,590
we really want the posterior

1809
02:08:13,630 --> 02:08:14,870
of function

1810
02:08:14,900 --> 02:08:19,270
let's remember we started we started off with a prior

1811
02:08:19,280 --> 02:08:22,700
a gaussian process prior are functions

1812
02:08:22,790 --> 02:08:26,240
we've observe some data in the form of the likelihood

1813
02:08:26,340 --> 02:08:30,820
i mean i want to update our prior belief

1814
02:08:30,830 --> 02:08:35,940
the the the likelihood to the posterior

1815
02:08:38,840 --> 02:08:41,030
this should look familiar now

1816
02:08:41,160 --> 02:08:44,690
we have a gaussian process prior on our functions

1817
02:08:44,700 --> 02:08:46,080
our likelihood

1818
02:08:46,090 --> 02:08:47,760
is gaussian

1819
02:08:47,840 --> 02:08:50,590
and it doesn't look as if too many of you have lost any sleep and

1820
02:08:50,600 --> 02:08:51,710
actually how we

1821
02:08:51,740 --> 02:08:53,720
the product of two gaussians

1822
02:08:53,850 --> 02:08:55,180
will lead

1823
02:08:55,240 --> 02:08:56,370
to another

1824
02:08:56,370 --> 02:09:00,730
in and the fact that the ratio of these garrisons in the marginalized form

1825
02:09:00,760 --> 02:09:03,300
all turns out to be gaussian

1826
02:09:03,310 --> 02:09:04,580
but it does

1827
02:09:04,590 --> 02:09:10,710
which means that our posterior over functions

1828
02:09:10,730 --> 02:09:12,570
no has a mean

1829
02:09:13,670 --> 02:09:15,370
and the covariance function

1830
02:09:15,380 --> 02:09:17,590
which is basically defined

1831
02:09:18,490 --> 02:09:22,150
the prior covariance matrix

1832
02:09:22,170 --> 02:09:23,940
the covariance

1833
02:09:24,160 --> 02:09:26,570
over noise

1834
02:09:26,570 --> 02:09:28,530
and of course the

1835
02:09:28,540 --> 02:09:32,960
the actual target values

1836
02:09:33,020 --> 02:09:35,260
when we want to make predictions

1837
02:09:38,200 --> 02:09:40,130
the predictive distributions

1838
02:09:40,140 --> 02:09:43,660
are also goes in as well and i one

1839
02:09:43,680 --> 02:09:46,210
boy was the details

1840
02:09:46,260 --> 02:09:51,170
so it's a very simple example based on the function that we're trying to model

1841
02:09:52,070 --> 02:09:54,930
yesterday afternoon

1842
02:09:57,770 --> 02:09:59,600
so we have the the true

1843
02:09:59,610 --> 02:10:02,510
the same function in red

1844
02:10:02,560 --> 02:10:05,210
and we have some data samples

1845
02:10:05,230 --> 02:10:08,630
right which

1846
02:10:08,730 --> 02:10:11,350
the the the values of the function

1847
02:10:11,350 --> 02:10:14,520
and have been contaminated with some noise

1848
02:10:14,540 --> 02:10:16,370
and we have

1849
02:10:16,380 --> 02:10:21,570
i decided to just for the purposes of this exercise to not sampled uniformly along

1850
02:10:21,570 --> 02:10:23,810
here so we've we've sampled

1851
02:10:23,820 --> 02:10:26,700
the data in little clumps

1852
02:10:26,730 --> 02:10:29,700
so the posterior predictive

1853
02:10:30,020 --> 02:10:33,490
values of our function the posterior mean

1854
02:10:33,940 --> 02:10:38,410
it turns out to be this little black line here

1855
02:10:38,570 --> 02:10:40,380
and the predictive variance

1856
02:10:40,400 --> 02:10:42,130
again all comes over

1857
02:10:42,130 --> 02:10:44,680
very naturally

1858
02:10:44,700 --> 02:10:47,710
and here the values here

1859
02:10:47,840 --> 02:10:52,720
and as you would expect in areas where we have the top

1860
02:10:52,780 --> 02:10:55,300
then of certainty

1861
02:10:55,310 --> 02:10:58,340
four prior posterior uncertainty

1862
02:11:01,860 --> 02:11:05,080
and in areas where we don't have a lot of data

1863
02:11:05,130 --> 02:11:07,610
then posterior uncertainty

1864
02:11:07,630 --> 02:11:09,770
remains fairly high

1865
02:11:09,780 --> 02:11:12,430
and if you

1866
02:11:12,460 --> 02:11:15,290
for those of you who are a layout

1867
02:11:15,290 --> 02:11:20,070
because we're using ago simply because there is no radial basis functions

1868
02:11:20,120 --> 02:11:22,630
to define the covariance function here

1869
02:11:22,690 --> 02:11:25,130
then the prior variance

1870
02:11:25,140 --> 02:11:28,380
it's going to be one

1871
02:11:28,390 --> 02:11:30,810
plus and minus one

1872
02:11:30,880 --> 02:11:32,790
so what you see

1873
02:11:32,800 --> 02:11:34,270
in areas

