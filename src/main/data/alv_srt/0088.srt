1
00:00:00,000 --> 00:00:05,890
right so that's where

2
00:00:06,040 --> 00:00:08,810
this is the possibility score

3
00:00:08,830 --> 00:00:09,890
this the the

4
00:00:11,100 --> 00:00:14,000
confidence of facts for centuries

5
00:00:14,750 --> 00:00:17,540
thomas for the first for the

6
00:00:18,520 --> 00:00:20,910
as each other in a different way

7
00:00:20,920 --> 00:00:23,830
it's more like this out

8
00:00:23,850 --> 00:00:26,710
so we we actually go back to the

9
00:00:26,730 --> 00:00:28,870
he was born

10
00:00:28,890 --> 00:00:31,080
the reason why i think it's

11
00:00:31,140 --> 00:00:34,310
just because possible was ranked number one more

12
00:00:34,310 --> 00:00:39,750
OK if you can be part of the merger okay

13
00:00:42,420 --> 00:00:45,960
one hundred random books

14
00:00:45,980 --> 00:00:49,230
from these solitons of also the so

15
00:00:49,250 --> 00:00:52,060
OK in the bookstore he also

16
00:00:52,230 --> 00:00:54,960
one chance of course

17
00:00:54,960 --> 00:00:59,390
this time it was later revealed religion whether it's right or wrong

18
00:00:59,540 --> 00:01:02,410
go get the

19
00:01:02,600 --> 00:01:06,850
one of the best of both

20
00:01:06,870 --> 00:01:10,270
the book's authors or not so popular

21
00:01:10,270 --> 00:01:15,120
OK so what find if i see this about to fire

22
00:01:15,140 --> 00:01:17,000
response no

23
00:01:17,060 --> 00:01:19,640
and i don't know

24
00:01:19,670 --> 00:01:21,580
five hundred sixty four

25
00:01:21,580 --> 00:01:23,020
missing data

26
00:01:23,060 --> 00:01:27,670
for every day and she will be part of

27
00:01:27,710 --> 00:01:34,370
the reason is that we we use by the part of sentence well with all

28
00:01:34,370 --> 00:01:39,440
the stories of things we come up with some power

29
00:01:39,460 --> 00:01:43,710
in the interesting way network reason why

30
00:01:44,710 --> 00:01:47,290
can be box

31
00:01:47,290 --> 00:01:49,060
so we have not only

32
00:01:49,060 --> 00:01:51,330
boston with many bookstores

33
00:01:51,730 --> 00:01:55,670
we have this one first

34
00:01:56,230 --> 00:01:58,310
to our surprise

35
00:01:58,330 --> 00:01:58,980
that's people

36
00:01:59,000 --> 00:02:01,370
so we know that

37
00:02:01,440 --> 00:02:03,920
there are there are you can see

38
00:02:03,980 --> 00:02:06,480
they almost in every

39
00:02:06,500 --> 00:02:09,290
but by the act

40
00:02:09,390 --> 00:02:11,870
a six five the

41
00:02:11,890 --> 00:02:13,290
well known

42
00:02:15,140 --> 00:02:17,620
you want to demonstrate

43
00:02:17,620 --> 00:02:20,500
but the point six five

44
00:02:20,500 --> 00:02:21,870
because the

45
00:02:21,890 --> 00:02:24,390
it's more on their books

46
00:02:25,440 --> 00:02:28,170
so of course is problem

47
00:02:28,190 --> 00:02:30,410
the world is

48
00:02:30,540 --> 00:02:33,040
the figure in nineteen

49
00:02:33,060 --> 00:02:35,040
two thousand seven

50
00:02:35,060 --> 00:02:39,190
so this is a

51
00:02:40,650 --> 00:02:44,250
this model was

52
00:02:44,270 --> 00:02:49,350
on his website there are ten i

53
00:02:49,370 --> 00:02:52,460
by the time and

54
00:02:52,480 --> 00:03:00,100
yes this is very far actually in this paper is said we discussed later on

55
00:03:00,100 --> 00:03:02,520
the topic at that

56
00:03:02,870 --> 00:03:08,600
this is copied the samples are they going to do what they want to work

57
00:03:08,600 --> 00:03:13,910
on more than that you can that's that's quite possible

58
00:03:15,190 --> 00:03:16,580
in some way

59
00:03:16,600 --> 00:03:19,120
so now we can use

60
00:03:19,140 --> 00:03:22,310
google news has more complex than that

61
00:03:22,330 --> 00:03:24,700
of course news agencies

62
00:03:26,000 --> 00:03:28,850
you can send their children to go to the page

63
00:03:28,850 --> 00:03:35,290
he so they need some copy saying but some of the carbon in one way

64
00:03:35,390 --> 00:03:36,980
something like

65
00:03:37,920 --> 00:03:39,870
so in the summer

66
00:03:39,890 --> 00:03:42,710
also for this kind of environment

67
00:03:42,730 --> 00:03:46,310
all the provinces or is it just using

68
00:03:46,330 --> 00:03:50,500
i think to change is of course that's so there are a lot of the

69
00:03:50,620 --> 00:03:57,350
carbon that that we studied some it's just what you call gets

70
00:03:57,370 --> 00:04:01,410
actually after we have very nicely

71
00:04:02,980 --> 00:04:05,230
in general

72
00:04:05,940 --> 00:04:12,640
lost to solve the problem that they were lost during the really big companies

73
00:04:15,730 --> 00:04:18,650
in the research she published two papers

74
00:04:20,170 --> 00:04:23,040
in the year the conference

75
00:04:23,040 --> 00:04:27,290
for she finer ones actually that

76
00:04:27,310 --> 00:04:28,580
this is the car

77
00:04:28,810 --> 00:04:30,150
this is a problem

78
00:04:30,170 --> 00:04:33,480
people like you said is important

79
00:04:33,500 --> 00:04:36,390
how to get rid of

80
00:04:36,410 --> 00:04:38,310
the interesting thing is that

81
00:04:38,330 --> 00:04:39,850
that show

82
00:04:42,100 --> 00:04:48,080
only at the world cup in the united if everybody scaffold

83
00:04:48,620 --> 00:04:52,540
you can discuss

84
00:04:54,980 --> 00:04:56,810
then in

85
00:04:58,330 --> 00:05:01,580
the interesting thing is similarity search

86
00:05:01,600 --> 00:05:03,580
which i did this

87
00:05:03,600 --> 00:05:06,580
in the network you want to search

88
00:05:06,600 --> 00:05:08,230
this is different from us

89
00:05:08,270 --> 00:05:09,730
search for example

90
00:05:09,790 --> 00:05:11,120
you've got little

91
00:05:11,140 --> 00:05:12,580
you have something

92
00:05:12,620 --> 00:05:14,500
you want to find

93
00:05:14,520 --> 00:05:18,730
sometimes you want for example companies really one

94
00:05:18,750 --> 00:05:22,460
some of these problems and i want you

95
00:05:22,520 --> 00:05:24,750
scenes of surgeons

96
00:05:28,310 --> 00:05:29,870
you can think

97
00:05:29,870 --> 00:05:34,140
four kinds of search

98
00:05:34,140 --> 00:05:35,920
one search

99
00:05:36,000 --> 00:05:38,600
this structure so

100
00:05:38,620 --> 00:05:45,190
you isomorphic to this one this one this one company some some

101
00:05:46,020 --> 00:05:48,440
four always comes out there

102
00:05:48,460 --> 00:05:49,980
one this one this

103
00:05:50,000 --> 00:05:51,370
search you

104
00:05:51,390 --> 00:05:53,040
look across the US

105
00:05:54,580 --> 00:06:00,810
this semantic similarity search for example this is what is most american christmas policies

106
00:06:01,000 --> 00:06:08,350
this is all crystal palace's two brothers one sister

107
00:06:08,370 --> 00:06:14,540
once you business process was not as well as follows

108
00:06:14,560 --> 00:06:16,600
it's really

109
00:06:16,620 --> 00:06:19,420
produce one papers published papers

110
00:06:20,790 --> 00:06:22,580
on the party

111
00:06:23,390 --> 00:06:25,960
some of those

112
00:06:25,960 --> 00:06:27,140
this one

113
00:06:28,440 --> 00:06:30,710
for some some kind

114
00:06:30,730 --> 00:06:31,960
OK so

115
00:06:32,150 --> 00:06:37,290
we also have a similar because they are similar to what is working on problems

116
00:06:37,500 --> 00:06:39,480
using this

117
00:06:39,500 --> 00:06:41,480
i was working on something

118
00:06:41,520 --> 00:06:43,540
some profits in user

119
00:06:43,560 --> 00:06:47,370
for his father was active in our data mining

120
00:06:48,190 --> 00:06:49,940
there is

121
00:06:51,600 --> 00:06:52,410
how can we

122
00:06:55,210 --> 00:06:57,020
the similarity is based on for

123
00:06:57,120 --> 00:06:59,230
this is

124
00:06:59,290 --> 00:07:01,960
the first few

125
00:07:04,170 --> 00:07:05,790
what substructures

126
00:07:05,790 --> 00:07:07,810
the reason that

127
00:07:07,870 --> 00:07:08,850
you want to construct

128
00:07:09,960 --> 00:07:12,370
you gave graph those

129
00:07:12,540 --> 00:07:17,940
this is really going there are twenty one structure this

130
00:07:19,940 --> 00:07:22,230
his website is

131
00:07:23,250 --> 00:07:24,960
with this kind stuff

132
00:07:25,010 --> 00:07:28,060
i don't want to have fun

133
00:07:28,080 --> 00:07:29,890
not only he was

134
00:07:30,830 --> 00:07:33,420
michigan wisconsin or

135
00:07:33,440 --> 00:07:36,040
CMU structure

136
00:07:36,040 --> 00:07:40,440
that's reinforced structures which list is

137
00:07:41,460 --> 00:07:49,270
this one is is a paper was published in february two thousand and six

138
00:07:49,290 --> 00:07:51,920
one of the structure search

139
00:07:51,920 --> 00:07:53,380
eight can

140
00:07:53,430 --> 00:07:55,100
school day one

141
00:07:55,130 --> 00:07:57,590
you know so was formula a two

142
00:07:57,600 --> 00:08:00,090
there's conditions group data

143
00:08:00,100 --> 00:08:01,270
and so on

144
00:08:02,220 --> 00:08:05,580
everything maintain soundness and completeness that's

145
00:08:06,590 --> 00:08:08,180
this theorem

146
00:08:08,200 --> 00:08:09,730
is the one that gives us

147
00:08:09,840 --> 00:08:12,200
that diagram on the previous page

148
00:08:12,210 --> 00:08:16,260
and then of course given the collection itself is one the logic is complete

149
00:08:16,270 --> 00:08:19,270
that's just follows from the other two

150
00:08:20,410 --> 00:08:22,180
so for a long time

151
00:08:22,190 --> 00:08:23,790
people thought great

152
00:08:23,840 --> 00:08:28,250
we've got this notion of sulcus formula and look you can have a calculus you

153
00:08:28,250 --> 00:08:29,630
can have cookie

154
00:08:29,640 --> 00:08:33,330
frame with an eye condition and its first order definable

155
00:08:34,120 --> 00:08:37,350
so now i what i want to do is show the limits

156
00:08:37,400 --> 00:08:40,140
of this

157
00:08:40,150 --> 00:08:42,410
i'm sorry

158
00:08:43,770 --> 00:08:45,300
the conditional

159
00:08:45,320 --> 00:08:46,700
these conditions

160
00:08:46,750 --> 00:08:52,770
right i've just said condition here i'm a condition on the reachability relation so

161
00:08:52,820 --> 00:08:57,080
you know the shape box find classifier corresponds to

162
00:08:57,150 --> 00:09:00,560
that for all w wrw

163
00:09:00,580 --> 00:09:06,300
sorry shape of by implied by corresponds to a for all w

164
00:09:07,580 --> 00:09:09,840
and the are condition is the

165
00:09:11,090 --> 00:09:13,300
that every points is it right

166
00:09:15,360 --> 00:09:20,800
exactly all of these axioms so quest on that

167
00:09:20,810 --> 00:09:21,540
on that

168
00:09:21,570 --> 00:09:23,630
page which is gone

169
00:09:24,700 --> 00:09:27,180
given an arbitrary formula five

170
00:09:27,190 --> 00:09:31,270
it doesn't work that's why self-interest theorem is so important

171
00:09:31,370 --> 00:09:32,460
OK so

172
00:09:32,470 --> 00:09:37,020
it's undecidable you give me an arbitrary shape

173
00:09:37,110 --> 00:09:38,650
and ask me what's

174
00:09:39,270 --> 00:09:41,410
first order correspondent

175
00:09:41,460 --> 00:09:44,990
it's an undecidable problem

176
00:09:45,040 --> 00:09:46,070
OK so

177
00:09:46,080 --> 00:09:47,500
we know that

178
00:09:47,520 --> 00:09:51,030
southwest outside cells which were on shaky ground

179
00:09:51,040 --> 00:09:53,950
right it may make you may be able to find one

180
00:09:53,970 --> 00:09:57,030
but you won't be able to give the general algorithm which says

181
00:09:57,070 --> 00:10:01,140
given the fight it out the first of course

182
00:10:01,150 --> 00:10:04,670
what about conditions on r which are not

183
00:10:04,730 --> 00:10:10,330
captured by any shape we saw that there were conditions on r which corresponded exactly

184
00:10:10,330 --> 00:10:11,520
to shape

185
00:10:11,570 --> 00:10:17,660
these three conditions are examples of one that can't be changed

186
00:10:17,760 --> 00:10:20,920
i reflexivity said every point sees itself

187
00:10:20,970 --> 00:10:22,740
you reflexivity says

188
00:10:22,750 --> 00:10:26,040
no points is itself

189
00:10:26,950 --> 00:10:28,410
the symmetry set

190
00:10:29,420 --> 00:10:31,110
if you have

191
00:10:31,120 --> 00:10:34,340
i just want to use the same variables u and v

192
00:10:34,360 --> 00:10:36,770
so if you

193
00:10:36,810 --> 00:10:39,580
see the

194
00:10:39,630 --> 00:10:42,360
and this is you

195
00:10:42,470 --> 00:10:45,210
and actually they have to be the same

196
00:10:45,230 --> 00:10:49,370
so you can't have two distinct points to see each other

197
00:10:49,500 --> 00:10:51,780
and the third one is a symmetry

198
00:10:51,790 --> 00:10:53,370
which says that

199
00:10:53,380 --> 00:10:55,960
if you can go from here to here

200
00:10:56,000 --> 00:10:58,560
then you can't come back

201
00:10:58,570 --> 00:11:00,420
OK it might seem as if

202
00:11:00,470 --> 00:11:04,230
the second and third of this is the same but they're not cages

203
00:11:04,240 --> 00:11:06,670
work your way through that

204
00:11:06,690 --> 00:11:10,380
how do you prove that you can look at that in the book by blackburn

205
00:11:10,450 --> 00:11:14,830
can very much you define transformation frames

206
00:11:14,870 --> 00:11:16,590
by which you can

207
00:11:16,600 --> 00:11:19,310
joined two friends together to get bigger frame

208
00:11:19,320 --> 00:11:23,420
or cedar point and just look at all the children but you can reach which

209
00:11:23,420 --> 00:11:25,230
is called the generated of frame

210
00:11:25,280 --> 00:11:28,220
and another notion which is called the p m offic image i don't want to

211
00:11:28,220 --> 00:11:31,690
go into the details and what you can do is you can show that

212
00:11:31,770 --> 00:11:37,210
first order properties are preserved by taking these transformations and so you can cook up

213
00:11:37,760 --> 00:11:39,250
frame so that

214
00:11:39,260 --> 00:11:44,330
you prove that provide contradiction that now friend class can

215
00:11:44,400 --> 00:11:47,100
capture reflexivity say

216
00:11:48,350 --> 00:11:51,990
what about

217
00:11:52,000 --> 00:11:54,650
another question is it the case

218
00:11:55,760 --> 00:11:58,440
all shapes

219
00:11:58,480 --> 00:12:02,420
if they correspond correspond to first order

220
00:12:02,430 --> 00:12:05,320
conditions so we know that some shapes

221
00:12:05,340 --> 00:12:07,190
i don't have any correspondence

222
00:12:07,230 --> 00:12:10,240
but if they do they always correspond to first order

223
00:12:10,250 --> 00:12:12,070
well the answer is no

224
00:12:12,090 --> 00:12:14,030
so here are two logics

225
00:12:14,680 --> 00:12:16,450
and the condition

226
00:12:16,450 --> 00:12:18,910
making your statement not

227
00:12:18,960 --> 00:12:22,830
the and dependent on the given future that you choose

228
00:12:22,870 --> 00:12:25,040
so what does that mean

229
00:12:25,110 --> 00:12:27,490
remember had structures where you had

230
00:12:28,380 --> 00:12:30,830
and transitions of going to state

231
00:12:31,950 --> 00:12:33,970
so here is about

232
00:12:34,010 --> 00:12:36,700
choosing one execution in this structure

233
00:12:36,720 --> 00:12:40,080
so it means you first choose an infinite

234
00:12:41,320 --> 00:12:42,380
and then

235
00:12:43,370 --> 00:12:46,840
properties about

236
00:12:46,860 --> 00:12:48,980
so state

237
00:12:49,000 --> 00:12:50,100
branching time

238
00:12:54,790 --> 00:12:59,740
the view of branching time logic is to see the future as branching of g

239
00:12:59,790 --> 00:13:04,760
so it doesn't mean it means that instead of choosing a particular

240
00:13:07,010 --> 00:13:11,730
you just interested in in which point you the structure and you can tell about

241
00:13:11,780 --> 00:13:14,810
all possible passes

242
00:13:14,900 --> 00:13:17,130
you can tell about the set of

243
00:13:17,170 --> 00:13:19,330
that's start

244
00:13:19,340 --> 00:13:21,540
from this state

245
00:13:21,560 --> 00:13:23,220
so what you do

246
00:13:23,240 --> 00:13:24,510
is that

247
00:13:24,570 --> 00:13:28,240
you allow yourself to use quantification

248
00:13:28,270 --> 00:13:30,590
over the set of possible passes

249
00:13:30,600 --> 00:13:33,850
that starts from the current state in

250
00:13:35,240 --> 00:13:40,900
so she can start simply says

251
00:13:41,700 --> 00:13:46,160
you would think here

252
00:13:46,160 --> 00:13:51,640
something where you have simply and with the negation you would have the universal quantification

253
00:13:51,640 --> 00:13:52,670
as well

254
00:13:52,780 --> 00:13:54,770
you simply say here

255
00:13:54,810 --> 00:13:59,130
so this is going to be interpreted

256
00:13:59,150 --> 00:14:01,560
game along the path

257
00:14:03,420 --> 00:14:06,930
when i tell you that

258
00:14:06,950 --> 00:14:11,360
along the given sequence

259
00:14:11,360 --> 00:14:15,470
i currently have the existence

260
00:14:15,530 --> 00:14:18,610
o passed satisfying some formula

261
00:14:18,610 --> 00:14:21,940
so this is your standard models are very familiar with

262
00:14:21,970 --> 00:14:24,870
so that's my purse i thought

263
00:14:24,970 --> 00:14:29,410
OK i mean starting position i but this is not a big issue

264
00:14:29,490 --> 00:14:31,090
think of just

265
00:14:31,170 --> 00:14:33,860
and if in in this that i tell you that

266
00:14:33,890 --> 00:14:36,990
there exist the best

267
00:14:37,020 --> 00:14:39,820
which satisfies

268
00:14:40,730 --> 00:14:44,080
the site now is formula in this mountain logic

269
00:14:44,080 --> 00:14:45,430
but think of site

270
00:14:45,450 --> 00:14:47,200
just that formula

271
00:14:47,440 --> 00:14:50,570
it tells me only that from here

272
00:14:50,590 --> 00:14:54,210
there exists a path which may not be this one

273
00:14:54,260 --> 00:14:57,020
it may be another one

274
00:14:57,030 --> 00:14:59,420
in the structure

275
00:14:59,420 --> 00:15:04,320
so it does you that from here i mean where i am currently in this

276
00:15:04,370 --> 00:15:08,530
i'm currently on the execution but in this execution here

277
00:15:08,590 --> 00:15:11,850
there may exist another future

278
00:15:11,860 --> 00:15:14,360
and this branching

279
00:15:14,380 --> 00:15:17,820
it is very important when you think of programs where

280
00:15:17,830 --> 00:15:20,230
the programs in is an open system

281
00:15:20,290 --> 00:15:23,840
i have some use which is going to enter value

282
00:15:23,890 --> 00:15:25,140
like the menu

283
00:15:25,150 --> 00:15:26,790
what you want to do you want to play

284
00:15:26,820 --> 00:15:29,230
the first game second game for game

285
00:15:29,280 --> 00:15:30,610
please type

286
00:15:30,690 --> 00:15:34,780
and you try and depending on what you type the program execution would be very

287
00:15:36,180 --> 00:15:40,120
from many another way would have to OK

288
00:15:40,160 --> 00:15:45,160
so given the program when you're in a given state considering

289
00:15:45,250 --> 00:15:47,580
possible futures so the sum

290
00:15:50,080 --> 00:15:52,960
of these equations is is worse than

291
00:15:52,990 --> 00:15:54,600
it makes sense

292
00:15:54,610 --> 00:15:56,430
so in this logic

293
00:15:56,470 --> 00:15:58,700
you can say there exist away

294
00:15:58,770 --> 00:16:02,110
there exists a possible execution which may be

295
00:16:02,180 --> 00:16:04,170
o one which which forks

296
00:16:04,220 --> 00:16:06,430
from the one you currently looking at

297
00:16:06,480 --> 00:16:11,460
so that blah blah blah so the blah blah blah is LTL formulas

298
00:16:11,470 --> 00:16:13,940
but now that you have this ability to

299
00:16:14,030 --> 00:16:15,590
to use the

300
00:16:15,630 --> 00:16:18,620
quantification wherever you want

301
00:16:18,660 --> 00:16:20,430
well you may of course

302
00:16:20,450 --> 00:16:27,170
here say that there exist to pass well at some point

303
00:16:27,170 --> 00:16:32,380
satisfying site and this site if you if you go down side at some point

304
00:16:32,380 --> 00:16:36,570
you're missing site contains also for example a proposition p is true

305
00:16:38,240 --> 00:16:39,680
vision q

306
00:16:40,680 --> 00:16:42,290
for that

307
00:16:42,310 --> 00:16:44,140
something it's

308
00:16:44,160 --> 00:16:48,510
so here we have p all along the and the new HQ

309
00:16:48,560 --> 00:16:49,870
and from now on

310
00:16:49,920 --> 00:16:51,310
wherever you go

311
00:16:52,090 --> 00:16:55,630
say trying hold

312
00:16:55,630 --> 00:16:57,050
in their own right

313
00:17:00,760 --> 00:17:04,900
here in order to understand how to use evolutionary simulations it would be nice to

314
00:17:04,900 --> 00:17:07,220
know about how their component parts worked

315
00:17:07,310 --> 00:17:12,990
unfortunate that poorly understood

316
00:17:13,070 --> 00:17:16,630
when this work was done there was little explosion of interest in neutrality in search

317
00:17:17,690 --> 00:17:23,760
so that's search spaces where points that are adjacent with respect to genetic operators share

318
00:17:23,800 --> 00:17:29,150
the same fitness so those genetic operators don't bring about any change in fitness

319
00:17:29,170 --> 00:17:34,470
populations can drift across neutral networks in in search spaces

320
00:17:34,480 --> 00:17:36,050
so here's the picture

321
00:17:36,050 --> 00:17:38,810
these plateau here are neutral

322
00:17:38,850 --> 00:17:40,660
with respect to evolution

323
00:17:40,750 --> 00:17:44,050
and this white line that you can't really see shows the progress of the population

324
00:17:44,050 --> 00:17:49,310
as it climbs sort of jumps up onto neutral networks eventually manages to make it

325
00:17:49,310 --> 00:17:50,750
all the way to the top

326
00:17:50,790 --> 00:17:55,250
or maybe it's error minimisation this kind of what i don't know

327
00:17:55,290 --> 00:17:57,850
so what we were interested in

328
00:17:57,870 --> 00:17:58,870
was whether

329
00:17:58,890 --> 00:18:04,960
drifting across neutral networks was going to retire an evolutionary process for certain classes of

330
00:18:05,010 --> 00:18:09,900
of problem or whether as some people have claimed neutral networks were beneficial because they

331
00:18:09,900 --> 00:18:10,900
open up

332
00:18:10,930 --> 00:18:17,330
transitions to a wide wide variety of locations in search spaces and prevent you from

333
00:18:17,330 --> 00:18:18,570
getting stuck

334
00:18:18,670 --> 00:18:20,080
and local optima

335
00:18:20,110 --> 00:18:25,660
and for the for the particular class of landscape that we were looking at which

336
00:18:25,660 --> 00:18:28,240
were this sort of idealised RNA

337
00:18:28,440 --> 00:18:34,480
secondary structure RNA folding landscape we could demonstrate although there was neutrality there

338
00:18:34,660 --> 00:18:41,030
it was structured such populations didn't spend a lot of time

339
00:18:41,050 --> 00:18:45,550
neutral refuges refuse you like called

340
00:18:45,590 --> 00:18:48,770
so some there was some sense in which

341
00:18:48,950 --> 00:18:51,690
if you're

342
00:18:51,790 --> 00:18:59,110
imagine that the this is error minimisation this landscape right so so you start out

343
00:18:59,160 --> 00:19:01,200
so i would say error minimisation

344
00:19:01,330 --> 00:19:03,870
imagine it's hill climbing

345
00:19:03,910 --> 00:19:09,520
imagine that the population has reached this plateau here

346
00:19:09,650 --> 00:19:15,750
it could be the majority of transitions off the plateau are negative right

347
00:19:16,060 --> 00:19:20,010
and what you find is that regular GA

348
00:19:21,530 --> 00:19:26,890
if there are points which are close to apply then parents which find themselves on

349
00:19:26,890 --> 00:19:31,450
the edge of that cliff will have offspring and say half of their offspring will

350
00:19:31,450 --> 00:19:35,330
be over the cliff with low fitness and that means that you don't expect those

351
00:19:36,350 --> 00:19:40,980
you expect the population to persist very long with parents like that parents who are

352
00:19:40,980 --> 00:19:46,830
deep inside this neutral landscape who consistently generate offspring with the same fitness is there

353
00:19:46,840 --> 00:19:51,270
are going to outcompete lineages that involve those parents are going to outcompete lineages that

354
00:19:51,290 --> 00:19:54,200
parents of these these dangerous cliff edges

355
00:19:54,260 --> 00:19:58,330
but it's the edges of the neutral network where the transitions are so if you

356
00:19:58,440 --> 00:20:03,350
if your population is being forced into the middle of neutral networks then maybe it's

357
00:20:03,350 --> 00:20:09,920
gonna that's gonna retard the rate at which is discovers evolutionary innovations but

358
00:20:09,960 --> 00:20:14,070
the structure of the neutrality within the space was such that that didn't happen so

359
00:20:14,330 --> 00:20:17,310
the point for this lecture is not to explain

360
00:20:17,310 --> 00:20:22,550
much more about neutrality but show that if we want to understand

361
00:20:23,290 --> 00:20:29,470
how we want to understand the implications of our models one include components that

362
00:20:29,480 --> 00:20:33,020
we have yet to have mature theories on then maybe we should be spending part

363
00:20:33,020 --> 00:20:34,130
of our time

364
00:20:34,330 --> 00:20:38,940
exploring how how those systems work and not just consign that some some kind of

365
00:20:38,940 --> 00:20:41,400
engineering activity

366
00:20:41,490 --> 00:20:46,470
OK so is another reason model that i'm going to try to explain in a

367
00:20:46,470 --> 00:20:49,690
bit more detail than the neutrality one

368
00:20:49,890 --> 00:20:54,680
so i've already mentioned termite construction i've already shown you fascinating DVD of the man

369
00:20:54,680 --> 00:20:57,830
pumping a termite mound full of plaster of paris

370
00:20:57,850 --> 00:21:00,440
so the

371
00:21:03,220 --> 00:21:08,070
the the things the termite builds are impressive

372
00:21:08,090 --> 00:21:13,700
OK you end up with fungus farms and climate control and tunnels and air-conditioning and

373
00:21:13,700 --> 00:21:19,290
what have you but it's largely unexplained how they achieve all of this complexity but

374
00:21:19,290 --> 00:21:24,290
we know how it starts stops with pillars that have joined together to form low

375
00:21:24,290 --> 00:21:30,900
walls and those walls grow and enclose spaces to form domes and in particular the

376
00:21:30,900 --> 00:21:36,090
first thing is built is called the queen chamber which don't like structure built around

377
00:21:36,210 --> 00:21:36,610
a huge

378
00:21:38,330 --> 00:21:44,020
termite we i think of them on the radio today the queen termite and around

379
00:21:44,020 --> 00:21:47,180
so you can see these little depositions

380
00:21:47,220 --> 00:21:48,190
which are

381
00:21:49,240 --> 00:21:50,680
increased space

382
00:21:50,690 --> 00:21:53,910
pillars which are being

383
00:21:53,930 --> 00:21:59,490
extended vertically and eventually they will start to merge together to form wall

384
00:21:59,610 --> 00:22:03,760
and then that wall grows and encircles the queen

385
00:22:03,890 --> 00:22:06,210
the previous models of this

386
00:22:06,530 --> 00:22:12,940
had been constructed with partial differential equations that were one of those equations described the

387
00:22:12,940 --> 00:22:17,700
distribution of termites across some space one of the one of them described the distribution

388
00:22:17,700 --> 00:22:22,550
of building material that have been deposited across that space and one of them described

389
00:22:22,550 --> 00:22:25,940
the distribution of the pheromone that the termites give off

390
00:22:26,010 --> 00:22:29,390
one facts i think maybe even two pheromones

391
00:22:29,510 --> 00:22:38,130
so very the graph that the mathematics models produced looked like this

392
00:22:38,190 --> 00:22:41,870
so this is a graph this axis time

393
00:22:42,010 --> 00:22:43,950
on this axis is space

394
00:22:43,960 --> 00:22:46,660
what you should be saying is that at the beginning of time you've got a

395
00:22:46,660 --> 00:22:47,790
very rough

396
00:22:47,790 --> 00:22:52,310
plus the norm of the transpose x three if you look at the supremum

397
00:22:52,360 --> 00:22:54,860
o like that

398
00:22:54,870 --> 00:22:58,780
and then we see that this is a second order cone

399
00:22:58,790 --> 00:23:06,340
a linear function of x and then the euclidean norm of some things that make

400
00:23:06,350 --> 00:23:09,060
and the problem makes a lot of sense because what this means is that for

401
00:23:09,060 --> 00:23:12,250
example by zero means no uncertainty

402
00:23:12,260 --> 00:23:18,600
just a i just a single point and it reduces the LP which just mean

403
00:23:18,650 --> 00:23:20,350
of a i

404
00:23:20,370 --> 00:23:24,290
then if the by is large large matrix and at some slack in the constraints

405
00:23:24,300 --> 00:23:28,160
to make sure that x is feasible for all possible values of a i

406
00:23:29,060 --> 00:23:31,980
how you could have exact this

407
00:23:32,000 --> 00:23:35,210
in this problem is

408
00:23:35,230 --> 00:23:39,710
clever because you add up all the slack

409
00:23:39,760 --> 00:23:43,490
and then the amount of slack have always also depends on the on how the

410
00:23:43,760 --> 00:23:45,590
product of the next

411
00:23:45,640 --> 00:23:50,800
could be that like here apart from the actions and or another direction

412
00:23:50,840 --> 00:23:52,560
depending on

413
00:23:52,570 --> 00:23:56,080
singular values of the i

414
00:23:56,120 --> 00:24:01,600
that's an example of robust convex optimisation problems

415
00:24:01,630 --> 00:24:02,950
to take an LP

416
00:24:03,030 --> 00:24:07,520
with uncertainty that turns into an association SOCP

417
00:24:07,560 --> 00:24:13,730
another example is the squares suppose take least squares problem x minus p minimize two

418
00:24:14,750 --> 00:24:18,000
but now is uncertain and i assume that a

419
00:24:18,050 --> 00:24:18,990
it's actually

420
00:24:19,100 --> 00:24:23,790
so in this model like this is then

421
00:24:23,800 --> 00:24:25,650
normal choice it's zero

422
00:24:25,750 --> 00:24:29,290
press firms that depend on the unknown parameter u

423
00:24:29,300 --> 00:24:31,270
and this unknown parameter you is

424
00:24:31,280 --> 00:24:35,800
bounded norm peterson

425
00:24:35,840 --> 00:24:36,730
and then

426
00:24:36,730 --> 00:24:39,780
so now that makes our matrix a certain

427
00:24:39,790 --> 00:24:44,380
and in this robust worst case approach to minimize the worst-case values that minimize the

428
00:24:45,400 --> 00:24:47,250
of this nuclear norm

429
00:24:47,300 --> 00:24:51,870
over all possible choices you ask the first is that this

430
00:24:51,990 --> 00:24:57,070
and this turns out to again writers and it is the set of an SDP

431
00:24:57,070 --> 00:24:59,700
that's exactly this problem

432
00:24:59,730 --> 00:25:02,320
we have access variable and you two

433
00:25:02,380 --> 00:25:04,650
except for one

434
00:25:04,670 --> 00:25:08,360
and by solving this is the problem

435
00:25:08,380 --> 00:25:12,740
and so this gives you an idea of what happened was i think in this

436
00:25:12,740 --> 00:25:14,610
case two

437
00:25:14,630 --> 00:25:17,630
very well known parameters

438
00:25:17,670 --> 00:25:19,300
for some a and b

439
00:25:19,340 --> 00:25:23,060
and here i come look at the distribution of the residuals in the least squares

440
00:25:23,060 --> 00:25:27,150
problems for three possible choices of x

441
00:25:27,230 --> 00:25:32,400
if i assume that the uncertainty happen in your view that defined like this to

442
00:25:33,780 --> 00:25:36,860
i assume that u is an

443
00:25:36,920 --> 00:25:43,050
uniformly distributed over the this two and there's you know there's one suppose that because

444
00:25:43,050 --> 00:25:44,560
certain x

445
00:25:44,590 --> 00:25:48,840
and then take for you and the choices in the universe

446
00:25:48,890 --> 00:25:53,960
and then for different choices of actually get the different distribution of the residuals

447
00:25:54,000 --> 00:25:59,250
so the first one the problem here is that east coast solution because i just

448
00:25:59,250 --> 00:26:01,330
an audience

449
00:26:01,340 --> 00:26:04,370
this problem i just added additional thirty

450
00:26:04,380 --> 00:26:08,600
and optimized for a zero nominal case

451
00:26:08,640 --> 00:26:11,550
then if i use that solution x

452
00:26:11,580 --> 00:26:17,640
and in the actual model that uncertainty than i this distribution model

453
00:26:17,700 --> 00:26:18,820
we see that

454
00:26:18,820 --> 00:26:22,530
the best solution is quite a bit better than the other solutions but it's also

455
00:26:23,770 --> 00:26:24,790
the actual

456
00:26:24,810 --> 00:26:26,300
these girls objective

457
00:26:26,360 --> 00:26:30,680
could actually be much worse than what you ask

458
00:26:30,700 --> 00:26:33,700
that's not robust non robust solution

459
00:26:33,750 --> 00:26:37,900
residual depends very much on

460
00:26:37,960 --> 00:26:40,600
the second choice here x

461
00:26:41,010 --> 00:26:43,000
OK this is the kind of solution

462
00:26:43,000 --> 00:26:48,650
so that's another the classical way of making x more about respect the choices a

463
00:26:48,650 --> 00:26:51,530
is added the organization

464
00:26:51,730 --> 00:26:55,830
the penalty term or a constant multiple of of the norm of x

465
00:26:55,840 --> 00:26:57,600
that's the kind of solution

466
00:26:57,680 --> 00:27:01,280
it's used to make it more robust that actually makes it does make it more

467
00:27:01,280 --> 00:27:03,430
about the distribution of residuals

468
00:27:03,450 --> 00:27:08,350
three is more narrow and these

469
00:27:08,360 --> 00:27:13,020
this solution is more robust against errors a

470
00:27:13,100 --> 00:27:15,890
the first one

471
00:27:15,950 --> 00:27:19,800
you pay a price for the best case solution also

472
00:27:19,840 --> 00:27:21,590
the script

473
00:27:21,600 --> 00:27:22,890
and this

474
00:27:23,980 --> 00:27:28,820
distribution is the solution of this robust spacecraft solution

475
00:27:28,830 --> 00:27:33,840
he really minimizing the worst case solution actually actually minimizing the

476
00:27:34,270 --> 00:27:38,290
but at the end of the distribution

477
00:27:38,400 --> 00:27:41,100
minimize the worst case solution pushing this

478
00:27:41,150 --> 00:27:43,140
the left as far as it can

479
00:27:43,160 --> 00:27:47,850
and that results in a more narrow distribution is more sensitive

480
00:27:47,860 --> 00:27:53,060
and the price to pay is called the best solution is

481
00:27:55,020 --> 00:27:56,350
as high as

482
00:27:57,920 --> 00:28:01,100
that's sort of the goal of robust optimization tried to make

483
00:28:01,130 --> 00:28:03,100
the ocean more of

484
00:28:03,140 --> 00:28:05,840
respect to the uncertainty

485
00:28:05,840 --> 00:28:09,290
without actually decorating the normal solution

486
00:28:09,290 --> 00:28:13,160
the solution for the normal system too much

487
00:28:13,280 --> 00:28:18,750
so that's robust optimization second very active area is

488
00:28:19,560 --> 00:28:21,210
seven programming

489
00:28:21,270 --> 00:28:24,250
using polynomial optimisation

490
00:28:24,420 --> 00:28:27,840
and the idea is the following

491
00:28:28,490 --> 00:28:36,860
suppose you have polynomial extensible

492
00:28:36,920 --> 00:28:40,410
two x is vector of coefficients

493
00:28:40,420 --> 00:28:42,810
and is the vector of basis functions

494
00:28:43,090 --> 00:28:45,880
for example when

495
00:28:46,050 --> 00:28:50,240
he is an independent variable could be just to scare could be multidimensional

496
00:28:50,240 --> 00:28:54,560
so i will come to the whole

497
00:28:54,570 --> 00:28:55,990
thanks for

498
00:28:55,990 --> 00:28:59,710
think we speak to you in front of you today

499
00:28:59,900 --> 00:29:04,140
that was quite an important he made bit at the last minute

500
00:29:04,190 --> 00:29:07,550
because initially as in cinemas this summer and

501
00:29:07,600 --> 00:29:10,920
one anyway i'm here so

502
00:29:10,940 --> 00:29:18,080
that's a work which was presented during idea and then i made an extension of

503
00:29:18,080 --> 00:29:18,990
the two

504
00:29:19,050 --> 00:29:23,940
that's why and and to a bit more about the topic to the talk even

505
00:29:23,940 --> 00:29:27,260
last week two idea

506
00:29:27,270 --> 00:29:34,270
first my quote also for the history of the popes addresses from port again

507
00:29:34,290 --> 00:29:40,120
with whom i initiated this is this work about two years ago professor cook

508
00:29:40,180 --> 00:29:44,910
from ztx leiden university he's my supervisor in holland

509
00:29:44,930 --> 00:29:51,010
but but three can from the ICT group of TU delft the technical university of

510
00:29:51,010 --> 00:29:52,480
delft holland

511
00:29:52,530 --> 00:29:55,170
and myself so above

512
00:29:55,190 --> 00:29:57,370
the second is by the

513
00:29:57,390 --> 00:30:02,230
the set out of his the guy will remain most the kids with whom i

514
00:30:02,230 --> 00:30:09,010
have collaborated most is easier to make this paper in this contribution

515
00:30:09,030 --> 00:30:17,060
i started working text classification two years ago when i was

516
00:30:17,060 --> 00:30:21,310
in bottou go my my sisters this possible brazil supervised this is this

517
00:30:21,330 --> 00:30:23,510
so it was six months is

518
00:30:23,530 --> 00:30:27,260
and then the data in french was propositional command assume

519
00:30:27,280 --> 00:30:29,140
classification to king

520
00:30:29,330 --> 00:30:34,780
i can text classification study so the goal was to give suggestion to study practically

521
00:30:34,840 --> 00:30:37,250
pragmatically if we want to use

522
00:30:37,250 --> 00:30:39,590
classification agreed

523
00:30:39,610 --> 00:30:42,250
OK well what's happening

524
00:30:42,260 --> 00:30:47,200
so i kept this work when i got involved in business studies in holland i

525
00:30:47,200 --> 00:30:54,120
kept working this party but time so now i am in ztx leiden institute of

526
00:30:54,120 --> 00:30:59,770
advanced computer science in in leiden university and the distribution of both so-called que and

527
00:30:59,770 --> 00:31:04,400
the funding comes from and big the netherlands bioinformatics centre

528
00:31:04,440 --> 00:31:07,380
two main projects i will

529
00:31:07,440 --> 00:31:12,470
present in one or two slide my project in

530
00:31:12,490 --> 00:31:14,120
in bioinformatics

531
00:31:14,160 --> 00:31:19,240
but then we spend most of the time we are discussing about text classification study

532
00:31:19,250 --> 00:31:26,040
so one would about and they can just well that's a big initiative in india

533
00:31:26,060 --> 00:31:32,630
in holland it's a foundation that coordinates research education and support activities

534
00:31:32,680 --> 00:31:35,310
in bioinformatic and citizens

535
00:31:35,340 --> 00:31:41,960
that's quite a big project with twenty one million euro and and then about semantic

536
00:31:41,960 --> 00:31:47,600
position new position created just forces bioinformatic initiative in in the sentence

537
00:31:47,660 --> 00:31:48,740
so many

538
00:31:48,750 --> 00:31:55,900
many different entities collaborate to universities companies and then also hospitals

539
00:31:55,910 --> 00:32:01,440
university leiden university

540
00:32:01,440 --> 00:32:07,160
that's is oldest universities and is and fifteen seventy five it was created as a

541
00:32:07,160 --> 00:32:11,320
gift from william of orange to this from my supervisor because

542
00:32:11,340 --> 00:32:13,910
obviously i don't know anything about my university yet

543
00:32:13,970 --> 00:32:16,470
so here it is

544
00:32:16,520 --> 00:32:19,650
a gift from william of orange to the citizens of flight in

545
00:32:19,660 --> 00:32:24,100
we had was to the long siege the spaniards

546
00:32:24,160 --> 00:32:28,500
so is the oldest university in holland

547
00:32:29,190 --> 00:32:34,030
now those are about ninety co-workers in the lab and then number the floor we

548
00:32:34,120 --> 00:32:35,790
the mathematicians

549
00:32:35,840 --> 00:32:42,250
and so on brain and in his lab and there few surviving clusters focused ago

550
00:32:42,250 --> 00:32:44,340
in computer systems

551
00:32:44,350 --> 00:32:47,470
the foundation of software technology and imaging

552
00:32:47,490 --> 00:32:52,130
and the bioinformatic cluster is spread over a cliff and images

553
00:32:53,590 --> 00:32:58,310
well i don't university is really a nice space to be because obviously that she

554
00:32:58,310 --> 00:33:06,490
will visit us next winter for a couple of months so that's even more interesting

555
00:33:06,510 --> 00:33:10,920
so one of two states about just bioinformatics to know i'm not just

556
00:33:10,970 --> 00:33:13,450
text classification guy but also

557
00:33:13,460 --> 00:33:20,900
i do we set with bioinformatics we analyse data about osteoarthritis so that's when you

558
00:33:20,900 --> 00:33:22,050
join skate

559
00:33:25,640 --> 00:33:26,550
that's it

560
00:33:26,600 --> 00:33:33,610
complex we analyse the complex phenotype of course outright is so it's a disease which

561
00:33:33,610 --> 00:33:35,720
is spreading over your body

562
00:33:35,780 --> 00:33:37,340
the joint location

563
00:33:37,350 --> 00:33:43,270
and we have to do with sibling pairs suppose and sisters recorded an integrated to

564
00:33:45,480 --> 00:33:48,980
and then we recorded at multiple join size which means

565
00:33:49,040 --> 00:33:50,460
under hands

566
00:33:50,760 --> 00:33:56,700
under spines civically and alone by the hips and knees we recorded with roger graph

567
00:33:56,710 --> 00:34:01,210
is a disease severity of

568
00:34:01,220 --> 00:34:05,220
it's always severity under different during location

569
00:34:05,290 --> 00:34:09,780
and this involves many techniques

570
00:34:09,780 --> 00:34:15,380
the mining visualisation statistical testing something techniques as well as by computing so quite

571
00:34:15,470 --> 00:34:19,770
a large amount of techniques to study it is

572
00:34:19,800 --> 00:34:24,600
and then the workflow is the following morning so you might be used to

573
00:34:24,680 --> 00:34:27,800
this kind of focal for the workflow

574
00:34:27,820 --> 00:34:30,780
data transformation model based clustering

575
00:34:30,780 --> 00:34:32,320
because we want

576
00:34:32,330 --> 00:34:37,630
to see down groups of individuals sharing the same phenotype so we try to clusters

577
00:34:37,630 --> 00:34:40,910
in by by phenotype

578
00:34:40,970 --> 00:34:45,200
so this is people have what they keep stuff like this is one so we

579
00:34:45,200 --> 00:34:47,760
can start discussing

580
00:34:47,770 --> 00:34:49,590
visualisation which is

581
00:34:49,600 --> 00:34:54,660
very important in when we work in collaboration with biologist is the key point

582
00:34:54,720 --> 00:34:58,730
and then of course when you we do we say clusters we have to assess

583
00:34:58,740 --> 00:35:00,100
the validity

584
00:35:00,130 --> 00:35:04,880
and in addition to computers and specific datasets

585
00:35:04,900 --> 00:35:05,740
we need

586
00:35:05,760 --> 00:35:06,970
to assess

587
00:35:08,180 --> 00:35:10,630
two to see them more sisters

588
00:35:10,640 --> 00:35:13,340
in one studies and what we expect

589
00:35:13,350 --> 00:35:17,800
so that's

590
00:35:18,110 --> 00:35:21,720
what what we have to do when work was biologist

591
00:35:21,770 --> 00:35:27,530
two visualisation among the three that i was using

592
00:35:27,540 --> 00:35:33,380
this one we not use it yet at the moment we use it for y

593
00:35:33,400 --> 00:35:34,100
but then

594
00:35:34,830 --> 00:35:39,920
when forces visualization so one is bayes and pull out coordinates and one is part

595
00:35:39,920 --> 00:35:41,660
of the coordinates

596
00:35:42,030 --> 00:35:44,730
we also use heat maps quite heavily

597
00:35:44,740 --> 00:35:48,400
and in that case we can

598
00:35:48,410 --> 00:35:51,930
reflect as good as possible

599
00:35:52,030 --> 00:35:59,030
the phenotype of the individuals who favour here so because he is a robust source

600
00:36:00,420 --> 00:36:02,050
and then we have

601
00:36:02,100 --> 00:36:03,800
different clusters

602
00:36:04,760 --> 00:36:07,780
and then we can observe increasing severity here

603
00:36:07,790 --> 00:36:09,530
of of sand

604
00:36:09,540 --> 00:36:12,160
the disease for this clusters

605
00:36:12,170 --> 00:36:17,540
so i cannot show more results because data confidentiality by biology so we much

606
00:36:19,170 --> 00:36:21,990
protecting the datasets

607
00:36:22,010 --> 00:36:24,180
so let's go on we

608
00:36:24,220 --> 00:36:30,340
with text classification in the behavior off is him in text classification

609
00:36:30,400 --> 00:36:33,520
so i'm not really anisian gave more

610
00:36:33,990 --> 00:36:35,330
i try to

611
00:36:35,330 --> 00:36:38,740
so let's suppose we have a very simple belief net. so this is a binary belief

612
00:36:38,750 --> 00:36:42,430
net. these are the binary things; i've given them names

613
00:36:42,490 --> 00:36:45,990
this happened to me in california. i was lying in bed in the house jumped

614
00:36:46,000 --> 00:36:50,600
and i just assumed a truck had hit the house because i'm english. actually it was an earthquake

615
00:36:50,650 --> 00:36:54,270
but let's suppose that it could have been either

616
00:36:54,290 --> 00:36:56,170
houses don't just jump

617
00:36:56,190 --> 00:37:00,300
so by putting a bias of minus twenty there, you say the probability the house just jumps is

618
00:37:00,300 --> 00:37:02,700
about e to the minus twenty

619
00:37:02,710 --> 00:37:06,310
the probability of a truck hitting the house is much more likely than the house just jumping. let's make

620
00:37:06,310 --> 00:37:08,300
that e to the minus ten about

621
00:37:08,320 --> 00:37:10,770
the earthquake about the same

622
00:37:10,780 --> 00:37:13,410
let's suppose these are independent causes

623
00:37:13,420 --> 00:37:16,860
that is, we ignore the fact that trucks steer funny in earthquakes

624
00:37:16,870 --> 00:37:19,100
so let's make these independent causes

625
00:37:20,460 --> 00:37:21,820
the most likely

626
00:37:21,830 --> 00:37:26,670
state here in your sort of prior distribution is that neither of these will happen

627
00:37:26,970 --> 00:37:30,100
it's very unlikely that both of them will happen

628
00:37:30,110 --> 00:37:33,840
and there's a small chance of e to the minus ten of one happening and

629
00:37:33,840 --> 00:37:35,570
the other not happening.

630
00:37:35,590 --> 00:37:39,530
but because of these big weights here if a truck did hit the house,

631
00:37:39,550 --> 00:37:43,340
then there's an even chance the house will jump because now the total input is 0

632
00:37:43,390 --> 00:37:46,800
similarly in an earthquake, there's an even chance the house will jump

633
00:37:47,010 --> 00:37:50,470
but you'd be very silly to explain the house jumping by saying there's an earthquake *and* a truck hit the

634
00:37:52,150 --> 00:37:56,510
you're appealing to two very unlikely things to try and explain this extremely unlikely thing.

635
00:37:56,510 --> 00:38:00,770
the product of these unlikely things just as unlikely as this extremely unlikely thing, so you don't win

636
00:38:00,770 --> 00:38:02,470
by doing that

637
00:38:02,480 --> 00:38:07,200
so the posterior distribution over these hidden causes look something like this

638
00:38:07,250 --> 00:38:09,780
possibly with more nines

639
00:38:09,830 --> 00:38:12,690
the probability they both happened is very unlikely

640
00:38:12,700 --> 00:38:15,100
this is given that you see the house jumps

641
00:38:15,120 --> 00:38:20,370
the probability that neither happened is very unlikely because the house after all did jump

642
00:38:20,390 --> 00:38:22,310
this might have happened

643
00:38:22,330 --> 00:38:24,360
or this might have happened

644
00:38:24,440 --> 00:38:27,130
and those each have probability of about point five

645
00:38:29,120 --> 00:38:33,930
all the probability is here so these two things are highly anti correlated.

646
00:38:33,950 --> 00:38:37,560
they were independent in the prior--they were assumed to be independent events--

647
00:38:37,580 --> 00:38:40,460
but once you've seen that the house jumped

648
00:38:40,470 --> 00:38:44,990
then if you believe this you almost certainly don't believe that, and vice versa

649
00:38:45,000 --> 00:38:46,800
that's explaining away

650
00:38:46,880 --> 00:38:49,200
pearl was the first person to make a big deal of this

651
00:38:49,210 --> 00:38:50,900
and it's what makes inference

652
00:38:50,910 --> 00:38:52,670
difficult in these networks

653
00:38:52,710 --> 00:38:53,870
and we're gonna

654
00:38:53,920 --> 00:38:58,590
get rid of it

655
00:38:58,640 --> 00:39:00,390
there's two ways to get rid of it

656
00:39:00,400 --> 00:39:03,220
you just pretend it's not there -- that's called variational methods

657
00:39:03,240 --> 00:39:05,660
or there's a more honest way to do it, which is what we're gonna do

658
00:39:13,670 --> 00:39:17,190
the reason it's hard to learn some deep belief net that has these layers of

659
00:39:17,190 --> 00:39:18,990
hidden variables

660
00:39:19,000 --> 00:39:23,900
driving the layers below until you eventually get to observe the data

661
00:39:23,920 --> 00:39:27,840
let's think about learning these weights here, just these weights here. you go from the bottom layer of

662
00:39:27,910 --> 00:39:30,920
hidden variables to the observed data

663
00:39:30,930 --> 00:39:33,190
in order to learn these,

664
00:39:33,200 --> 00:39:36,710
we need to sample from the posterior here

665
00:39:36,800 --> 00:39:38,620
the posterior here

666
00:39:38,630 --> 00:39:40,340
depends on two things

667
00:39:40,350 --> 00:39:42,450
the posterior distribution of these

668
00:39:42,500 --> 00:39:43,710
vectors here

669
00:39:43,710 --> 00:39:45,070
it depends on

670
00:39:45,080 --> 00:39:46,770
the data

671
00:39:46,800 --> 00:39:48,840
i'll call that dependency the likelihood term

672
00:39:48,930 --> 00:39:51,330
and it also depends on the prior

673
00:39:51,370 --> 00:39:54,530
OK if the stuff up here said this unit never turns on,

674
00:39:54,550 --> 00:39:58,940
then that's that. it can't turn on even if the data would like it to

675
00:39:58,960 --> 00:40:02,330
and you have to take the product of two distributions then you normalize. a distribution comes

676
00:40:02,350 --> 00:40:05,520
from here which is the prior distribution over the possible vectors there

677
00:40:05,540 --> 00:40:07,310
and a distribution comes from here

678
00:40:07,320 --> 00:40:09,200
which i'll call the likelihood distribution

679
00:40:09,220 --> 00:40:14,020
and you multiply them together then normalize and that gives you the posterior here

680
00:40:14,060 --> 00:40:17,430
so it looks like to learn these weights

681
00:40:17,450 --> 00:40:18,900
you need the posterior

682
00:40:18,910 --> 00:40:22,400
and to know the posterior, you need to know all these weights here

683
00:40:22,450 --> 00:40:25,420
therefore, you're gonna have to learn all the weights at the same time

684
00:40:25,420 --> 00:40:29,190
trying to find out where the two distributions are the same

685
00:40:29,230 --> 00:40:30,490
based on

686
00:40:30,490 --> 00:40:34,900
the doctor that we've drawn from those distributions

687
00:40:35,550 --> 00:40:38,700
we've done one two and three

688
00:40:38,780 --> 00:40:42,130
and now what i'm going to convince you that actually

689
00:40:42,160 --> 00:40:45,450
this mean in feature space

690
00:40:45,500 --> 00:40:49,460
and we've been playing with that now for the past three lectures

691
00:40:49,460 --> 00:40:51,710
these are very very powerful quantity

692
00:40:51,760 --> 00:40:52,710
in fact

693
00:40:52,710 --> 00:40:53,720
if you

694
00:40:53,730 --> 00:40:56,320
pick feature map in the right way

695
00:40:56,370 --> 00:41:00,470
this uniquely identify the distribution

696
00:41:00,560 --> 00:41:02,990
this is great because it means that rather than

697
00:41:02,990 --> 00:41:04,530
playing with distribution

698
00:41:04,580 --> 00:41:08,280
you can play with the point in hilbert space

699
00:41:08,290 --> 00:41:12,120
so at least i find is sometimes a bit daunting computing kullback leibler divergences in

700
00:41:12,120 --> 00:41:13,300
similar things

701
00:41:13,490 --> 00:41:15,550
but playing with means

702
00:41:15,790 --> 00:41:19,550
in the feature space they can compute distances although this is really easy

703
00:41:19,570 --> 00:41:21,990
that's what i really used to doing

704
00:41:22,040 --> 00:41:25,250
so maybe i just find it easier because i've been doing it for a long

705
00:41:26,180 --> 00:41:30,830
but it turns out to get very very simple algorithms

706
00:41:30,850 --> 00:41:34,050
and you can use it for quite a few things

707
00:41:34,490 --> 00:41:35,690
and after the break

708
00:41:35,700 --> 00:41:38,920
will cover this

709
00:41:40,750 --> 00:41:43,450
the first thing you need to discuss in this context

710
00:41:43,500 --> 00:41:46,000
is the two sample problem

711
00:41:46,040 --> 00:41:47,580
the two sample problem is

712
00:41:48,310 --> 00:41:51,990
and given two sets of observations from two different distributions

713
00:41:52,020 --> 00:41:53,040
or at least four

714
00:41:53,060 --> 00:41:55,640
two different to this mission i don't know

715
00:41:55,660 --> 00:42:00,220
and i want to and so one very simple question given those two sets of

716
00:42:01,580 --> 00:42:05,790
other drawn from the same distribution or not

717
00:42:05,810 --> 00:42:07,870
well it will be very hard to give

718
00:42:07,890 --> 00:42:10,200
such an answer with certainty

719
00:42:10,250 --> 00:42:14,020
i can give such an answer with high probability

720
00:42:14,040 --> 00:42:17,290
sounds very dry very abstract and a bit boring

721
00:42:17,370 --> 00:42:21,370
as the efficiency love answering such questions

722
00:42:21,390 --> 00:42:24,600
it turns out you can do a lot of useful things with it

723
00:42:24,700 --> 00:42:27,410
and the applications are really fun

724
00:42:27,470 --> 00:42:28,750
so namely

725
00:42:28,770 --> 00:42:30,560
what you can do is

726
00:42:30,600 --> 00:42:34,100
you can do things like start integration

727
00:42:34,100 --> 00:42:36,520
three into attribute matching

728
00:42:36,680 --> 00:42:41,160
or you can actually do things like sample bias correction so in other words they've

729
00:42:41,160 --> 00:42:44,200
got the training set drawn from one distribution

730
00:42:44,220 --> 00:42:47,200
the test set drawn from another

731
00:42:47,950 --> 00:42:50,040
you just given the training and the test set

732
00:42:50,060 --> 00:42:52,100
obviously tested unlabelled

733
00:42:52,180 --> 00:42:56,140
you told you based on that test it

734
00:42:56,200 --> 00:42:59,600
and it turns out you can actually prove that and

735
00:42:59,600 --> 00:43:01,370
fairly reasonable assumptions

736
00:43:01,390 --> 00:43:03,580
you can do

737
00:43:03,600 --> 00:43:05,500
basic as well as if you knew

738
00:43:05,520 --> 00:43:11,600
how the test distribution differs from the training institution

739
00:43:13,540 --> 00:43:15,410
the first part now is

740
00:43:15,510 --> 00:43:17,810
fairly heavy-duty statistics

741
00:43:18,060 --> 00:43:21,730
and then will do lots of applications

742
00:43:21,810 --> 00:43:23,060
so it will be a bit

743
00:43:24,790 --> 00:43:25,970
OK so

744
00:43:25,970 --> 00:43:27,700
let's look at the city

745
00:43:27,870 --> 00:43:30,180
given some data x

746
00:43:30,290 --> 00:43:31,770
and why so

747
00:43:31,830 --> 00:43:35,040
not to be confused in this case the water not labels

748
00:43:35,060 --> 00:43:36,230
i'm just give some

749
00:43:36,250 --> 00:43:38,290
diversity and another one there

750
00:43:38,450 --> 00:43:40,040
i want to test whether

751
00:43:40,040 --> 00:43:41,100
people's q

752
00:43:42,270 --> 00:43:45,080
distribution p distribution q

753
00:43:45,080 --> 00:43:47,390
well what can i use this word

754
00:43:47,500 --> 00:43:49,390
let's say i'm working

755
00:43:49,410 --> 00:43:51,140
with various micro

756
00:43:51,160 --> 00:43:52,850
array flaps

757
00:43:52,870 --> 00:43:56,970
and while they all do some measurements for some big cancer study that i would

758
00:43:56,970 --> 00:43:58,890
like to do something with it

759
00:43:58,950 --> 00:44:01,870
and you know they all have their own protocols and all that

760
00:44:01,910 --> 00:44:03,270
i would like to know

761
00:44:03,270 --> 00:44:07,810
whether there really measuring the same thing

762
00:44:07,810 --> 00:44:11,930
so you could look at each feature separately and look at the means and well

763
00:44:12,310 --> 00:44:15,500
might give me something but what i want to do is just you know i

764
00:44:15,500 --> 00:44:20,310
should be some dots and you tell me other drawn from the same distribution

765
00:44:20,350 --> 00:44:22,250
or let's say

766
00:44:22,290 --> 00:44:24,750
after companies which merged

767
00:44:24,810 --> 00:44:26,950
that customer databases

768
00:44:27,000 --> 00:44:31,040
and obviously because they were all outsourcing and everything that followed the person who's written

769
00:44:31,040 --> 00:44:33,330
the database schema

770
00:44:33,370 --> 00:44:35,220
you want to be able to

771
00:44:35,220 --> 00:44:37,100
rediscover fairly easily

772
00:44:37,140 --> 00:44:39,250
what that schema looked like

773
00:44:39,310 --> 00:44:42,120
for instance you might want to know

774
00:44:42,120 --> 00:44:43,580
what are names or

775
00:44:44,470 --> 00:44:45,930
and the other

776
00:44:45,990 --> 00:44:49,220
terms such as you can really merge database

777
00:44:51,410 --> 00:44:54,100
you get the sample boss correction problem

778
00:44:54,160 --> 00:44:56,750
well the training and test sets different

779
00:44:56,770 --> 00:44:58,890
well that can occur fairly frequently

780
00:44:58,930 --> 00:45:00,310
for instance

781
00:45:00,310 --> 00:45:05,110
how you might represent images you might just decide on an arbitrary ordering trying use

782
00:45:05,110 --> 00:45:06,650
this very big stick here

783
00:45:06,660 --> 00:45:11,120
an arbitrary ordering of the pixel so you might scan out the pixels in a

784
00:45:11,120 --> 00:45:12,230
row by row

785
00:45:12,290 --> 00:45:16,150
and then you might miss the value of each pixel is the number here say

786
00:45:16,170 --> 00:45:19,740
between zero and two fifty five so if this is a hundred by one hundred

787
00:45:19,750 --> 00:45:25,400
image hundred pixels by hundred pixels and you have all the pixel values as numbers

788
00:45:25,400 --> 00:45:31,680
between zero and two fifty five then what you have is a ten thousand long

789
00:45:32,800 --> 00:45:37,890
OK so your representation of the images the ten thousand long vectors and furthermore

790
00:45:37,920 --> 00:45:39,120
how many

791
00:45:39,130 --> 00:45:40,840
possible images are there

792
00:45:45,810 --> 00:45:50,170
right two hundred fifty five raised to the ten thousand which suffice it to say

793
00:45:50,170 --> 00:45:54,030
is an unimaginably large number and so

794
00:45:54,050 --> 00:45:56,380
the following strategy

795
00:45:56,410 --> 00:45:58,150
is not going to work

796
00:45:58,160 --> 00:46:01,400
every time you see an image that's labeled as stanmore

797
00:46:01,410 --> 00:46:04,050
the she or alex or someone that you know

798
00:46:04,100 --> 00:46:08,150
put it on the side and little scratchpad these are people that i know every

799
00:46:08,150 --> 00:46:11,030
time you get a new image compared to see if it's the same as one

800
00:46:11,030 --> 00:46:12,950
of the people that you know if so

801
00:46:12,980 --> 00:46:15,490
give me the name and if not so i don't know

802
00:46:15,510 --> 00:46:19,450
OK that's not going to work because there are so many images of as the

803
00:46:19,510 --> 00:46:23,300
images you will never see the same image again even if you put a camera

804
00:46:23,300 --> 00:46:25,750
in front of me right now take a picture of me and take another picture

805
00:46:25,800 --> 00:46:28,180
second later it's not going to be the same

806
00:46:28,180 --> 00:46:31,990
not in the actual details of the images that were taken so

807
00:46:32,050 --> 00:46:36,420
we definitely have to be careful about how we manipulate this representation

808
00:46:37,140 --> 00:46:39,010
what we're going to do is going to take away

809
00:46:39,030 --> 00:46:44,330
numerically representing everything but then we have to live rely on probability and statistics in

810
00:46:44,330 --> 00:46:45,330
order to do

811
00:46:45,330 --> 00:46:50,420
whatever calculation need to do matching images to people or detecting whether something is spam

812
00:46:50,420 --> 00:46:52,440
or not spam and so on

813
00:46:52,730 --> 00:46:56,400
and so in in particular we're going to do is we're going to

814
00:46:56,760 --> 00:47:02,440
use something called the random variable a random variable is like the variable in a

815
00:47:02,440 --> 00:47:03,640
computer program

816
00:47:03,640 --> 00:47:06,000
but represents a quantity

817
00:47:06,080 --> 00:47:11,250
whose value changes depending on exactly which data the programs looking so you might say

818
00:47:11,250 --> 00:47:12,330
for example

819
00:47:12,340 --> 00:47:14,440
the pixel here in this position

820
00:47:14,450 --> 00:47:18,430
that's a random variable which i might call x three hundred eighty four this is

821
00:47:18,450 --> 00:47:20,460
three hundred eighty four pixel k

822
00:47:20,480 --> 00:47:25,550
and that is the variable which changes depending on which image we're looking at and

823
00:47:26,870 --> 00:47:31,700
if we had some probability distribution over images that would induce a probability distribution over

824
00:47:31,700 --> 00:47:36,050
that pixel and that's why it's considered a sort of random variable changes depending on

825
00:47:36,050 --> 00:47:38,020
what the image represents

826
00:47:38,050 --> 00:47:41,950
and the value of these variables is often sort of uncertainty and that's why we're

827
00:47:41,950 --> 00:47:45,860
going to employ the theory of probability

828
00:47:45,900 --> 00:47:47,150
OK so

829
00:47:47,190 --> 00:47:51,850
random variables can either be continuous random variables in which case they just take on

830
00:47:51,870 --> 00:47:56,470
real values usually between negative infinity infinity or maybe you know that there are positive

831
00:47:56,470 --> 00:48:00,330
or something or can be what's called discrete are categorical

832
00:48:00,350 --> 00:48:05,800
so these discrete categorical their values take on just say zero one that's a binary

833
00:48:05,800 --> 00:48:09,990
variable or a set of fixed set of values like in if you're doing so

834
00:48:10,040 --> 00:48:17,520
simple weather prediction and generally what we do is we have repeated measurements of various

835
00:48:17,520 --> 00:48:21,720
quantities and so this is the convention here i'm going to use the indices i

836
00:48:21,720 --> 00:48:22,590
and j

837
00:48:22,600 --> 00:48:26,300
to denote the components of the different

838
00:48:26,840 --> 00:48:33,650
values in our representation and can indices like an and to represent the data cases

839
00:48:33,650 --> 00:48:36,450
are records multiple measurements so x

840
00:48:37,030 --> 00:48:43,880
and is the i th value i th input or the i th representation

841
00:48:43,890 --> 00:48:48,190
of the end the case here and usually use extra inputs and y for outputs

842
00:48:48,190 --> 00:48:50,450
but that's not going to be hard rule

843
00:48:50,460 --> 00:48:54,210
and again we can just think of this whole thing is a vector the end

844
00:48:54,210 --> 00:48:59,700
data cases of vector so chinese boldface which may be shows better on printed notes

845
00:48:59,700 --> 00:49:00,710
on the screen

846
00:49:02,260 --> 00:49:04,950
so i

847
00:49:04,950 --> 00:49:10,440
i i just want to cover some of these these basic topics that i listed

848
00:49:10,440 --> 00:49:13,950
here on the left to try and set the stage for

849
00:49:14,050 --> 00:49:18,040
not only my lectures but for some things that other people be talking about

850
00:49:18,050 --> 00:49:21,580
so we

851
00:49:21,620 --> 00:49:25,670
given the inputs and outputs that we are represented we often want to compute something

852
00:49:25,670 --> 00:49:29,680
about them so for example given an input image represented now is the vector i

853
00:49:29,680 --> 00:49:33,190
give you that vector and i want to ask is this picture of my face

854
00:49:33,190 --> 00:49:34,240
for example

855
00:49:34,260 --> 00:49:39,210
so our computer program you can think of now is a mathematical function x which

856
00:49:39,210 --> 00:49:43,750
probably should be bold here is the representation of the input

857
00:49:43,760 --> 00:49:48,140
and f is the function which are computer program is going to actually implement and

858
00:49:48,140 --> 00:49:50,390
z is the answer

859
00:49:50,400 --> 00:49:54,190
so maybe we think of that is a binary function which just says is the

860
00:49:54,190 --> 00:49:58,890
picture of salmon be very simple machine learning task

861
00:50:01,300 --> 00:50:03,700
when you start thinking about this function

862
00:50:03,710 --> 00:50:06,830
you start to realize that

863
00:50:06,840 --> 00:50:09,380
there's a lot of these functions here right

864
00:50:09,400 --> 00:50:13,520
if there's two hundred fifty five to the ten thousand possible inputs and there's two

865
00:50:13,520 --> 00:50:19,340
possible outputs then this is sort of zero one cross two hundred fifty five to

866
00:50:19,340 --> 00:50:24,020
ten thousand that's the space of possible functions again that space functions as a huge

867
00:50:24,020 --> 00:50:25,450
number members and

868
00:50:26,860 --> 00:50:27,690
the key

869
00:50:27,780 --> 00:50:32,800
two in designing a machine learning algorithm is to think about the task that you

870
00:50:32,800 --> 00:50:34,480
want to do in the real world

871
00:50:34,510 --> 00:50:41,470
represent that task as a function approximation problem and then passed of all possible functions

872
00:50:41,470 --> 00:50:46,300
that i could consider what subset do i want to restrict myself to considering

873
00:50:46,310 --> 00:50:50,280
and the reason you want to restrict yourself to considering a subset is because if

874
00:50:50,280 --> 00:50:52,740
i show you a finite amount of data

875
00:50:52,750 --> 00:50:56,590
that's a finite amount of data can only help you choose between a certain number

876
00:50:56,590 --> 00:51:00,490
of functions as those of you would like to think in information theory terms imagine

877
00:51:00,490 --> 00:51:03,330
that i show you some data and that data has a certain number of bits

878
00:51:03,330 --> 00:51:04,860
of information in it

879
00:51:04,900 --> 00:51:11,120
the most you could ever hope to do is choose between two to the

880
00:51:11,140 --> 00:51:15,470
number of bits in your training data possible functions so if i show you a

881
00:51:15,490 --> 00:51:19,410
hundred images and you think those hundred images have you know a few thousand bits

882
00:51:19,410 --> 00:51:24,330
and then the most you could possibly hope to use the function space has cardinality

883
00:51:24,330 --> 00:51:28,400
to to a few thousand just a big number but not nearly as big as

884
00:51:28,400 --> 00:51:30,130
all possible functions right

885
00:51:30,580 --> 00:51:36,700
so the set of functions that you're willing to consider is called your hypothesis space

886
00:51:36,760 --> 00:51:41,410
so we don't just make up functions randomly choose them from a very carefully

887
00:51:41,440 --> 00:51:48,460
restricted set and that set is called our hypothesis space and that hypothesis spaces generally

888
00:51:48,460 --> 00:51:53,280
indexed by a set of parameters which generically call status and you can think of

889
00:51:53,280 --> 00:51:56,300
those as no we can turn to

890
00:51:56,330 --> 00:52:01,900
choose different functions in the sense so the hypothesis space h is a sort of

891
00:52:01,900 --> 00:52:07,270
map from these not stated into functions from the input to the output

892
00:52:07,290 --> 00:52:12,840
and the hardest part of doing machine learning especially the probabilistic approach is deciding how

893
00:52:12,840 --> 00:52:17,670
to represent the inputs and the outputs and how to select the hypothesis space for

894
00:52:17,670 --> 00:52:19,960
those of you who does little bit of work in this field you may be

895
00:52:19,960 --> 00:52:21,140
we don't know

896
00:52:21,430 --> 00:52:25,960
there's a pretty good indirect evidence that i mention later that is indeed the case

897
00:52:25,960 --> 00:52:30,280
but experiment to discover one more tomorrow we have to revise the textbooks but that

898
00:52:30,280 --> 00:52:31,980
is where it is

899
00:52:38,280 --> 00:52:38,890
now i

900
00:52:38,900 --> 00:52:40,890
i need to be careful

901
00:52:40,900 --> 00:52:42,970
because i said that

902
00:52:43,000 --> 00:52:45,010
in beta decay

903
00:52:45,030 --> 00:52:46,740
if you omit an electron here

904
00:52:46,750 --> 00:52:49,940
and the neutrino goes long way it hits something downstream it turns back into an

905
00:52:51,100 --> 00:52:57,120
that is true in all experiments done in a laboratory

906
00:52:57,140 --> 00:52:58,430
you know in a small

907
00:52:59,530 --> 00:53:03,660
but if you know those neutrinos travel far enough

908
00:53:03,680 --> 00:53:07,740
it turns out that there's a very small chance that the first they go

909
00:53:07,880 --> 00:53:10,480
they will somehow sort have lose the memory

910
00:53:11,210 --> 00:53:15,440
how they were born and change from one variety into another so

911
00:53:15,620 --> 00:53:21,090
if you've got a process which produced electron type neutrino for example

912
00:53:21,130 --> 00:53:23,330
in the heart of the some

913
00:53:23,350 --> 00:53:29,460
where hydrogen is turning into helium and emitting positrons and electrons type neutrinos

914
00:53:29,880 --> 00:53:34,890
by the time the electron type neutrinos have traveled one hundred fifty million kilometres

915
00:53:34,920 --> 00:53:39,890
it turns out that is long enough for a strange thing to happen called neutrino

916
00:53:39,890 --> 00:53:41,720
oscillations that the first go

917
00:53:42,130 --> 00:53:46,990
the greater the chance that they have lost the election time memory and start beginning

918
00:53:46,990 --> 00:53:50,690
to think that they become sort of a bipolar in fact can try

919
00:53:50,790 --> 00:53:54,980
but they think on the me one time neutrino all maybe i tau tau neutrino

920
00:53:55,000 --> 00:53:58,780
and by the time they get there pretty much

921
00:53:58,840 --> 00:54:01,350
one third chance each

922
00:54:01,360 --> 00:54:07,760
the detectors here on earth forty years deep underground detecting the solar neutrinos were only

923
00:54:07,760 --> 00:54:11,980
able to detect neutrinos of the electron time

924
00:54:12,010 --> 00:54:16,100
very sensible because that's all they thought they were looking for

925
00:54:16,180 --> 00:54:21,630
and they found about one-third as many arriving at the theory says should be arriving

926
00:54:21,640 --> 00:54:24,600
and it took forty years to sort that out

927
00:54:24,650 --> 00:54:27,010
before the discovery

928
00:54:27,040 --> 00:54:30,820
that a there are three varieties of neutrino and b

929
00:54:30,820 --> 00:54:32,620
but if neutrinos have

930
00:54:32,630 --> 00:54:34,550
a little bit of mass

931
00:54:34,570 --> 00:54:39,850
this phenomenon of changing the variety can happen if they absolutely massless as people had

932
00:54:39,850 --> 00:54:42,430
for many years that could not happen

933
00:54:42,480 --> 00:54:44,590
but if they have a little bit of mass

934
00:54:44,590 --> 00:54:49,710
the wonders of quantum mechanics enable the neutrinos to change from one form to another

935
00:54:49,930 --> 00:54:52,670
and that we now know is the reason why

936
00:54:52,670 --> 00:54:58,340
after one hundred fifty million kilometres only one-third were remaining as the electron type

937
00:54:58,360 --> 00:55:03,960
that was why the only detecting one-third as many theorists suggested that li ray davis

938
00:55:03,960 --> 00:55:05,930
who started program

939
00:55:05,960 --> 00:55:09,650
back in the late nineteen fifties to sixty

940
00:55:09,660 --> 00:55:13,860
so long it is notable prizes nobel prize i think it eighty eight

941
00:55:14,000 --> 00:55:17,760
so it's a long time in the neutrino business before things get sorted out this

942
00:55:17,760 --> 00:55:19,870
phenomenon of neutrino oscillations

943
00:55:19,880 --> 00:55:21,090
it is now

944
00:55:24,780 --> 00:55:28,200
what now the state of the art is trying to

945
00:55:28,220 --> 00:55:30,170
qualitatively measure

946
00:55:30,720 --> 00:55:32,690
see precisely

947
00:55:32,700 --> 00:55:33,910
how it happens

948
00:55:33,920 --> 00:55:38,930
and from its hopefully be able to determine what the different masses of these neutrino

949
00:55:38,930 --> 00:55:40,660
varieties are

950
00:55:42,430 --> 00:55:45,860
state of the art of the moment is that we know that

951
00:55:45,960 --> 00:55:50,320
not the three neutrino varieties can massless otherwise this phenomenon would happen

952
00:55:50,390 --> 00:55:54,320
but then masses i said are exceedingly small

953
00:55:54,340 --> 00:55:55,880
very near to each other

954
00:55:55,930 --> 00:55:59,400
and precisely what they are and why they are what we are is one of

955
00:55:59,400 --> 00:56:02,360
the big questions is being chased the moment and you'll hear much more about that

956
00:56:02,400 --> 00:56:07,290
in following lectures but the set of experiments that have been done it started off

957
00:56:07,370 --> 00:56:08,570
a bit like

958
00:56:08,590 --> 00:56:14,350
particle physics using natural things produced in the cosmos namely neutrinos from the sun

959
00:56:14,430 --> 00:56:20,240
neutrinos are also produced in cosmic ray collisions in the upper atmosphere

960
00:56:20,270 --> 00:56:24,450
they tend to be more of the muon neutrino variety for technical reasons

961
00:56:24,820 --> 00:56:27,570
and it was about ten to fifteen years ago

962
00:56:27,570 --> 00:56:33,490
when they started discovering that when they detected those what we call cosmic neutrinos underground

963
00:56:33,490 --> 00:56:34,780
the they also

964
00:56:34,840 --> 00:56:37,860
seem to be changing from one form to another and it was at that moment

965
00:56:38,050 --> 00:56:42,990
for half is to many problems in the solar neutrinos problems with the cosmic neutrinos

966
00:56:43,030 --> 00:56:45,260
may be the common story here

967
00:56:45,320 --> 00:56:49,400
and now experiments have been done in laboratories by using

968
00:56:49,420 --> 00:56:51,030
neutrino beams

969
00:56:51,030 --> 00:56:56,190
there are produced in labs in particular here it's for example you can produce beams

970
00:56:57,130 --> 00:57:02,760
so you want to neutrinos and five them through the earth towards rome there's the

971
00:57:02,760 --> 00:57:06,470
laboratory under the grounds sasso wrote the detector

972
00:57:06,490 --> 00:57:07,700
you have to fight through the

973
00:57:07,820 --> 00:57:11,260
the curvature of the earth of course means that rome is not she said been

974
00:57:11,420 --> 00:57:16,010
here when in space time around the beamline is pointing downward direction

975
00:57:16,030 --> 00:57:18,340
road you can then measure

976
00:57:19,260 --> 00:57:20,550
number of

977
00:57:20,570 --> 00:57:25,400
neutrinos of this variety arrived in rome compared with the number set out from here

978
00:57:25,420 --> 00:57:30,450
and from that see how many have changed disappears and the similar experiment happening in

979
00:57:30,450 --> 00:57:36,900
japan and so it's amazing which a major research programs trying to understand neutrinos and

980
00:57:36,900 --> 00:57:39,820
that i think is one of the major things that will hear about

981
00:57:39,840 --> 00:57:44,090
and what's exciting things in the future that come out of all of this

982
00:57:44,110 --> 00:57:45,280
is the ability

983
00:57:45,340 --> 00:57:49,690
to detect neutrinos i haven't told you how this conundrum was solved

984
00:57:50,840 --> 00:57:53,970
it is made into a whole new science the idea

985
00:57:54,150 --> 00:57:57,650
by detecting neutrinos from outer space

986
00:57:57,720 --> 00:57:59,200
we can

987
00:57:59,200 --> 00:58:01,240
i have new means of

988
00:58:01,260 --> 00:58:04,630
doing astronomy without the use of the electromagnetic spectrum

989
00:58:04,650 --> 00:58:08,490
the project we've detected neutrinos in the heart of the sun for example on one

990
00:58:08,490 --> 00:58:11,280
occasion detected from supernova nineteen eighty seven

991
00:58:11,320 --> 00:58:14,220
and people hopefully waiting for the super to happen

992
00:58:14,240 --> 00:58:17,110
but a remarkable things have been discovered neutrinos is already

993
00:58:17,130 --> 00:58:21,530
so i'll just make a couple of people here is a nice picture to

994
00:58:21,550 --> 00:58:26,610
show half of the conundrum it is true that an individual neutrino can travel

995
00:58:26,630 --> 00:58:32,170
statistically through light years have led to have been detected but it's a bit like

996
00:58:32,170 --> 00:58:37,360
the national lottery anybody here ever won the major prize the national lottery

997
00:58:37,450 --> 00:58:42,360
nobody able to handle this point is into the case but somebody always does never

998
00:58:42,360 --> 00:58:42,840
any of us

999
00:58:43,510 --> 00:58:48,420
if enough people into somebody wins if enough neutrinos set out

1000
00:58:48,430 --> 00:58:51,400
by chance one of them will bump into something

1001
00:58:51,550 --> 00:58:55,800
so detecting neutrinos requires

1002
00:58:55,820 --> 00:58:59,070
two things and as i said i mentioned in the first

1003
00:58:59,090 --> 00:59:01,900
the of them set out

1004
00:59:02,840 --> 00:59:04,860
a very big detector

1005
00:59:04,880 --> 00:59:07,840
to try to capture those combined

1006
00:59:08,970 --> 00:59:11,590
and the third thing is actually it turns out that

1007
00:59:11,610 --> 00:59:17,490
the high energy neutrino has the bigger the chance to bump into something so neutrinos

1008
00:59:17,490 --> 00:59:23,270
clustering algorithms usually the way they show the algorithm is better than all the previous

1009
00:59:23,270 --> 00:59:25,770
algorithms is by showing the pictures

1010
00:59:25,810 --> 00:59:30,630
look at the picture on two constraints that my clustering is the right one but

1011
00:59:30,630 --> 00:59:31,430
this is

1012
00:59:31,670 --> 00:59:37,580
limited scientific value and it can only work in dimension two but we are clustering

1013
00:59:37,580 --> 00:59:38,590
that in in

1014
00:59:38,600 --> 00:59:39,870
in hundreds of dimensions

1015
00:59:39,890 --> 00:59:43,580
so we really want to answer the question what is a good clustering

1016
00:59:43,630 --> 00:59:46,840
and we really want to be able to say

1017
00:59:46,880 --> 00:59:51,980
can carry out clustering efficiently how much running time doing it you don't know what

1018
00:59:52,020 --> 00:59:56,270
a good clustering you can't answer the question how much money time you need and

1019
00:59:56,290 --> 00:59:57,570
how much space

1020
00:59:57,580 --> 01:00:02,430
can we distinguish clusterable from structures that if i give you some data and

1021
01:00:02,450 --> 01:00:03,940
and i tell you

1022
01:00:03,960 --> 01:00:08,070
here is my my patients for medical there

1023
01:00:08,130 --> 01:00:14,230
repository and they have their i want us to do those patients naturally cluster into

1024
01:00:14,230 --> 01:00:17,380
groups so we will treat each group separately

1025
01:00:17,390 --> 01:00:19,950
although all just one big

1026
01:00:21,610 --> 01:00:24,760
can we define the notion of class that

1027
01:00:24,780 --> 01:00:30,420
these are very important questions and we don't have answers to these kind of questions

1028
01:00:31,990 --> 01:00:34,340
i will start by

1029
01:00:36,180 --> 01:00:39,890
axiomatic approach was initiated by jon kleinberg and then

1030
01:00:39,910 --> 01:00:45,160
we'll see what can we take from them so i don't have what i think

1031
01:00:45,230 --> 01:00:49,240
was in two thousand two he tries to give some

1032
01:00:50,640 --> 01:00:55,030
to allow the definition of the clustering in general way so it restricts the set

1033
01:00:55,030 --> 01:00:59,530
of clusterings of looking at but it's still pretty general so the idea is that

1034
01:00:59,530 --> 01:01:03,750
our our domain is going to be a finite set of points and on top

1035
01:01:03,750 --> 01:01:04,510
of it

1036
01:01:04,520 --> 01:01:08,860
we can cluster points and we don't have the notion of which points as most

1037
01:01:08,860 --> 01:01:14,200
similar to each other and which bones are not similar so input is the pen

1038
01:01:14,210 --> 01:01:15,100
which is

1039
01:01:15,110 --> 01:01:21,790
a set of the main points and a distance function which i call dissimilarity function

1040
01:01:21,800 --> 01:01:24,000
on pairs of points so

1041
01:01:24,010 --> 01:01:27,770
we have this notion of our input is just to find a set of points

1042
01:01:27,770 --> 01:01:31,210
and the notion of how similar they are to each other and so we require

1043
01:01:31,250 --> 01:01:33,390
this dissimilarity function

1044
01:01:33,440 --> 01:01:38,490
is a symmetric the distance between x and y is the same as y

1045
01:01:38,500 --> 01:01:44,100
x and the distance zero only two of the point itself

1046
01:01:44,210 --> 01:01:49,270
and then we define a clustering function is a function that takes as input which

1047
01:01:49,270 --> 01:01:50,390
is the pattern

1048
01:01:50,400 --> 01:01:52,960
of domain and distance

1049
01:01:52,970 --> 01:01:56,810
so a clustering function takes such an input and output

1050
01:01:56,860 --> 01:01:58,880
partition of the domain

1051
01:01:58,920 --> 01:02:04,010
that's basically what clustering functions do

1052
01:02:04,020 --> 01:02:08,500
and what we wish to be able to define is to find some properties that

1053
01:02:08,500 --> 01:02:12,250
distinguish clustering functions from functions which

1054
01:02:12,340 --> 01:02:16,630
not clustering functions not every function to take that

1055
01:02:16,640 --> 01:02:20,440
as input and outputs the partition of the that would be considered a clustering function

1056
01:02:20,440 --> 01:02:24,850
right if i if you have a function that takes the real numbers as input

1057
01:02:24,850 --> 01:02:29,030
and outputs the partition into rational numbers and irrational numbers

1058
01:02:29,050 --> 01:02:32,450
you are not likely to call it a clustering

1059
01:02:33,160 --> 01:02:36,850
how do we distinguish between functions that we call clustering in front of the court

1060
01:02:38,870 --> 01:02:40,640
a plan offered

1061
01:02:40,650 --> 01:02:42,400
three axioms

1062
01:02:42,410 --> 01:02:47,970
so don't sound pretty natural so the first one is scale invariant

1063
01:02:47,990 --> 01:02:53,130
so what do we mean by scale invariance if i take my dissimilarity distance matrix

1064
01:02:53,380 --> 01:02:56,180
and i have multiplied by a constant

1065
01:02:56,190 --> 01:02:59,520
so i didn't change the relative distances just

1066
01:02:59,530 --> 01:03:01,370
two call the distances and say

1067
01:03:01,390 --> 01:03:04,000
if the similarity between

1068
01:03:04,020 --> 01:03:05,880
i mean john was too

1069
01:03:05,890 --> 01:03:10,850
before now i have multiplied by ten to will be twenty news similarity between your

1070
01:03:10,890 --> 01:03:14,400
seventy but not seventy magical everything right

1071
01:03:14,410 --> 01:03:20,760
what we expect is a good clustering function will not be sensitive to this scale

1072
01:03:20,760 --> 01:03:24,960
scaling if we scale the distances we should get the same clustering so that the

1073
01:03:24,960 --> 01:03:29,170
scale invariance requirement f is the clustering function

1074
01:03:29,230 --> 01:03:30,980
that's the in there

1075
01:03:31,960 --> 01:03:33,590
a distance metric

1076
01:03:33,610 --> 01:03:39,000
and if we inputs lambda times the distance the we should get the same in

1077
01:03:39,000 --> 01:03:42,080
the same partition as was input to the

1078
01:03:42,080 --> 01:03:44,360
because you know how to write these down

1079
01:03:47,780 --> 01:03:49,730
is independent of b

1080
01:03:49,740 --> 01:03:51,860
given seed means what he

1081
01:03:51,870 --> 01:03:54,160
of a given c

1082
01:03:54,240 --> 01:03:56,740
is equal to p of a given c

1083
01:03:56,800 --> 01:03:58,240
he of b

1084
01:03:58,300 --> 01:04:01,510
so what we need to show is that these

1085
01:04:01,570 --> 01:04:08,650
when would what to check is whether these problems is equal to this problem

1086
01:04:09,690 --> 01:04:14,690
well not not with physical what what these from implies is from

1087
01:04:19,940 --> 01:04:22,470
so i just assume that these well

1088
01:04:22,490 --> 01:04:23,950
then what we have

1089
01:04:23,960 --> 01:04:26,050
well p of a b

1090
01:04:26,090 --> 01:04:27,830
given scene

1091
01:04:27,900 --> 01:04:33,120
is equal to p of ABC divided what you seem

1092
01:04:33,130 --> 01:04:35,060
by our assumption

1093
01:04:35,080 --> 01:04:38,550
p of ABC can be factorized in this way

1094
01:04:40,990 --> 01:04:45,470
but p of a p of the given a is equal to p of c

1095
01:04:45,530 --> 01:04:48,760
p of a given scene by bayes rule

1096
01:04:48,850 --> 01:04:55,160
so this first problem here is equal to the force approach

1097
01:04:55,200 --> 01:04:57,820
so immediately you can

1098
01:05:01,480 --> 01:05:04,060
fact here

1099
01:05:04,100 --> 01:05:06,120
then you get p of a given c

1100
01:05:06,150 --> 01:05:12,170
p of the given c

1101
01:05:12,200 --> 01:05:13,970
it's exactly what you

1102
01:05:16,110 --> 01:05:20,040
we need to understand exactly what is the relation

1103
01:05:20,110 --> 01:05:22,260
i mean what is an efficient means

1104
01:05:22,320 --> 01:05:25,820
is that a factorisation

1105
01:05:25,860 --> 01:05:29,410
of the bayesian networks

1106
01:05:32,010 --> 01:05:35,300
the conditional independence statements

1107
01:05:35,310 --> 01:05:37,250
given by the notion of

1108
01:05:38,320 --> 01:05:39,880
separation between

1109
01:05:39,900 --> 01:05:42,780
this to y

1110
01:05:43,320 --> 01:05:44,980
in other words

1111
01:05:45,080 --> 01:05:48,580
it's a very simple notion of separation we we're just

1112
01:05:48,620 --> 01:05:51,680
noting that these two vials are separated by this guy

1113
01:05:51,690 --> 01:05:54,470
so these conditional independence statements

1114
01:05:54,480 --> 01:05:59,860
precisely when the whole thing is

1115
01:05:59,930 --> 01:06:05,670
we are here trying to figure out which conditional independence statements hold for every graph

1116
01:06:05,760 --> 01:06:07,690
these one holds for

1117
01:06:07,730 --> 01:06:09,600
now we keep on going

1118
01:06:09,710 --> 01:06:11,950
we need to check the other way around

1119
01:06:12,040 --> 01:06:16,730
will condition dependence imply factorizations

1120
01:06:16,780 --> 01:06:21,220
to see if these two things are equal for this graph

1121
01:06:23,450 --> 01:06:28,090
so now we are going to assume these facts here

1122
01:06:28,700 --> 01:06:33,660
we went to check whether the factorisation of these graphs will emerge

1123
01:06:33,700 --> 01:06:35,750
from these assumption

1124
01:06:35,830 --> 01:06:37,880
that's exactly what happened

1125
01:06:37,950 --> 01:06:40,470
you just need to work through

1126
01:06:41,280 --> 01:06:43,090
just assume that this is true

1127
01:06:43,100 --> 01:06:45,180
in other words it is happening

1128
01:06:45,190 --> 01:06:50,200
now you open the joint distribution according to this way

1129
01:06:50,200 --> 01:06:52,490
and now

1130
01:06:53,410 --> 01:06:56,410
a b given c you can factorize

1131
01:06:56,460 --> 01:07:01,190
because you have these expressions here for factorizations conditional independence

1132
01:07:01,200 --> 01:07:03,420
and you obtain this by bayes rule

1133
01:07:03,430 --> 01:07:06,210
you know these property

1134
01:07:08,330 --> 01:07:09,960
is equal to or

1135
01:07:13,010 --> 01:07:14,100
the of a

1136
01:07:14,110 --> 01:07:17,710
the of c given a is equal to p of c

1137
01:07:17,760 --> 01:07:19,460
the given c

1138
01:07:19,470 --> 01:07:22,420
it's the first and third

1139
01:07:22,430 --> 01:07:26,080
i call to this effect is here

1140
01:07:26,080 --> 01:07:29,360
and they will you get the factorisation of this thing

1141
01:07:30,130 --> 01:07:31,620
so i mean you should

1142
01:07:31,680 --> 01:07:35,200
you should pay a lot of attention what's going on here

1143
01:07:35,260 --> 01:07:40,460
there are two parallel fields one of the few of graph theory that's been telling

1144
01:07:40,470 --> 01:07:44,210
this is the conditional independence statements that

1145
01:07:44,260 --> 01:07:48,480
would make sense to observe in the graph

1146
01:07:50,310 --> 01:07:52,940
it's just

1147
01:07:55,230 --> 01:07:58,860
the conditional if conditional independence statements

1148
01:07:58,930 --> 01:08:00,560
is given by

1149
01:08:00,670 --> 01:08:03,130
the notion of separation

1150
01:08:03,180 --> 01:08:05,570
and you make it precise

1151
01:08:05,580 --> 01:08:07,970
one of the supernova separation

1152
01:08:08,030 --> 01:08:13,990
that's exactly reflects the conditions the factorisation

1153
01:08:15,080 --> 01:08:21,400
factorisation conditional independence are synonyms for these graphs

1154
01:08:21,410 --> 01:08:23,330
one implies there

1155
01:08:23,380 --> 01:08:27,550
building by the one

1156
01:08:27,580 --> 01:08:30,840
now let's look at these graphs

1157
01:08:30,900 --> 01:08:37,240
who works for all graphs of three nodes

1158
01:08:37,330 --> 01:08:41,070
what is too much with detailed knowledge

1159
01:08:42,120 --> 01:08:45,730
want to relax but

1160
01:08:45,740 --> 01:08:48,220
remember that we have three no

1161
01:08:48,280 --> 01:08:49,470
how many

1162
01:08:49,480 --> 01:08:52,580
how many bayesian networks we have three no

1163
01:08:53,610 --> 01:08:58,990
technically we have five right because we have the line here which is be

1164
01:08:59,000 --> 01:09:01,230
we have these one year

1165
01:09:01,290 --> 01:09:05,570
which is the fully connected bayesian network

1166
01:09:07,850 --> 01:09:11,250
we have these one here

1167
01:09:11,260 --> 01:09:13,810
and it's just right away

1168
01:09:13,820 --> 01:09:14,950
we just

1169
01:09:14,990 --> 01:09:17,730
so let's not that is too because

1170
01:09:17,790 --> 01:09:22,180
this is the three d o is the entire set of conditional independence statements

1171
01:09:22,190 --> 01:09:23,290
this is

1172
01:09:23,300 --> 01:09:28,610
there are no conditional independence they try of these two graphs dangerous

1173
01:09:28,670 --> 01:09:32,080
so we studied these graph here

1174
01:09:32,150 --> 01:09:34,690
OK just

1175
01:09:34,700 --> 01:09:40,590
now we are going to study the graph

1176
01:09:40,630 --> 01:09:45,810
so we have one child and one parent and one grandparent

1177
01:09:45,820 --> 01:09:48,100
we have got two children and one

1178
01:09:49,550 --> 01:09:54,310
what the other options

1179
01:09:54,340 --> 01:09:56,800
we have two parents and one child

1180
01:09:58,160 --> 01:10:00,330
let's look at each one of them

1181
01:10:00,340 --> 01:10:02,120
we saw that for these

1182
01:10:04,660 --> 01:10:08,410
we saw that for these particular

1183
01:10:12,090 --> 01:10:14,530
when we observe

1184
01:10:14,650 --> 01:10:15,820
this thing here

1185
01:10:15,900 --> 01:10:16,930
this becomes

1186
01:10:16,940 --> 01:10:20,570
independent so in these graphs

1187
01:10:22,360 --> 01:10:25,070
a b

1188
01:10:25,100 --> 01:10:26,530
and c

1189
01:10:26,540 --> 01:10:30,530
these graph represents a condition that the

1190
01:10:30,540 --> 01:10:32,030
you can see

1191
01:10:32,080 --> 01:10:32,960
and the

1192
01:10:32,970 --> 01:10:36,080
the factorisation for the graph implies

1193
01:10:36,090 --> 01:10:40,290
these conditional independence and vice versa

1194
01:10:41,580 --> 01:10:43,830
let's look for these guys

1195
01:10:43,940 --> 01:10:48,510
first let's let's ask them all simple question is a and b

1196
01:10:48,510 --> 01:10:52,270
with these models because the product based model not mixture based model

1197
01:10:53,860 --> 01:10:57,190
again and again if you look at you know trying to do recognition or trying

1198
01:10:57,190 --> 01:10:58,920
to do in this case information retrieval

1199
01:10:59,320 --> 01:11:02,880
you can do much better than the traditional topic mixture based models

1200
01:11:04,500 --> 01:11:07,420
if you're looking at speech the same story holds

1201
01:11:09,300 --> 01:11:09,940
you know you can

1202
01:11:11,530 --> 01:11:13,420
this was done by group at stanford wear

1203
01:11:13,840 --> 01:11:16,460
and if you look at the first basis or discovering

1204
01:11:16,880 --> 01:11:17,170
you know

1205
01:11:17,630 --> 01:11:18,570
that's kind structure

1206
01:11:20,280 --> 01:11:24,150
and the remarkable thing is that what what turns out is that if you're if

1207
01:11:24,230 --> 01:11:28,550
few apply this model then it essentially discovers phonemes at the first level

1208
01:11:29,340 --> 01:11:31,230
right so that you know

1209
01:11:32,300 --> 01:11:35,610
this is sort of has a corresponds to a particular phoneme or even

1210
01:11:36,210 --> 01:11:40,500
this is has a correspondence between particular know so here what i'm showing you this

1211
01:11:40,500 --> 01:11:42,550
is what the the actual phonemes

1212
01:11:42,960 --> 01:11:44,480
and this is what the model is learning

1213
01:11:45,210 --> 01:11:47,400
so by basically applying

1214
01:11:49,460 --> 01:11:50,480
pretty much the same model

1215
01:11:51,070 --> 01:11:56,550
the model can basically discover phonemes which we know is a useful thing for the speech recognition right

1216
01:11:57,000 --> 01:11:58,460
and it sort of doesn't on its own

1217
01:11:58,940 --> 01:12:03,360
you can see that there is a correspondence that was discovered and i think that in

1218
01:12:03,800 --> 01:12:04,090
you know

1219
01:12:04,510 --> 01:12:05,280
this is not a speech

1220
01:12:06,230 --> 01:12:07,820
community but if you go to the speech

1221
01:12:08,230 --> 01:12:09,340
conferences use

1222
01:12:09,750 --> 01:12:14,070
you know you'll see a lot of these models declining kind models basically making a huge impact

1223
01:12:14,590 --> 01:12:18,300
just because you basically learning what are the right representations

1224
01:12:18,800 --> 01:12:20,590
to extract from noisy signals

1225
01:12:21,480 --> 01:12:22,530
and noisy speech signals

1226
01:12:24,880 --> 01:12:30,030
and maybe i can just point ah again the difference between the two models is

1227
01:12:30,030 --> 01:12:35,460
that's you know you've probably seen nearest neighbors clustering or sort of local density estimators

1228
01:12:36,210 --> 01:12:39,670
and the idea behind these models that use essentially partitioning the space

1229
01:12:40,170 --> 01:12:42,110
clustering based models and mixture based models

1230
01:12:42,570 --> 01:12:45,480
you partition the space to finding these local regions

1231
01:12:46,070 --> 01:12:48,300
a new finding parameters for each local region

1232
01:12:49,980 --> 01:12:52,460
so number regions is linear with the number of parameters

1233
01:12:53,750 --> 01:12:54,770
but if you look at sort of

1234
01:12:55,230 --> 01:13:01,300
distributed type of models like restricted boltzmann machines factor model species coding models

1235
01:13:01,730 --> 01:13:02,440
they all sort of do

1236
01:13:03,690 --> 01:13:04,820
something slightly different

1237
01:13:05,530 --> 01:13:08,980
and that is the following right imagine i have a two dimensional data

1238
01:13:09,630 --> 01:13:10,380
the first hidden

1239
01:13:11,320 --> 01:13:13,030
able can partition the data into two

1240
01:13:13,420 --> 01:13:16,030
so the two points right see zero see one

1241
01:13:16,650 --> 01:13:18,380
if i introduced another hidden variable

1242
01:13:18,940 --> 01:13:21,250
i can partition the place again in two

1243
01:13:23,480 --> 01:13:23,670
you know

1244
01:13:25,000 --> 01:13:27,280
see to once it easier right notice

1245
01:13:27,960 --> 01:13:29,710
two things i have four partitions

1246
01:13:30,130 --> 01:13:31,750
if i introduce another hidden variable

1247
01:13:32,190 --> 01:13:33,590
i can partition the plane as well

1248
01:13:34,840 --> 01:13:37,420
right so the interesting thing about these models

1249
01:13:38,000 --> 01:13:39,420
is that each parameter

1250
01:13:40,070 --> 01:13:43,730
each hidden variable effects many regions not just local regions

1251
01:13:44,190 --> 01:13:48,360
and the number of regions grows roughly speaking exponential in number of parameters

1252
01:13:49,650 --> 01:13:51,670
so so so this is where you know

1253
01:13:53,530 --> 01:13:58,150
these model shine because they to some extent able to do a little bit better with the curse of dimensionality

1254
01:13:59,000 --> 01:14:02,210
something that local models cannot deal so the difference between

1255
01:14:03,000 --> 01:14:03,820
between the two models

1256
01:14:04,480 --> 01:14:08,360
so if you look at you know a lot of different domains like natural images

1257
01:14:08,360 --> 01:14:12,860
text collaborative filtering you know we do applications motion capture and i'm gonna show you

1258
01:14:12,860 --> 01:14:14,530
some of the more speech perception

1259
01:14:15,380 --> 01:14:19,030
the beauty of these models and what makes it exciting is about yet the same

1260
01:14:19,030 --> 01:14:21,610
learning algorithm we have multiple domains

1261
01:14:22,170 --> 01:14:23,230
multiple input domains

1262
01:14:23,750 --> 01:14:27,090
right it that's fine interesting structure in every single domain

1263
01:14:29,230 --> 01:14:31,170
but obviously there are limitations right

1264
01:14:31,750 --> 01:14:35,670
and there are limitations to the types of structures that can be extracted by just

1265
01:14:35,670 --> 01:14:37,670
basically single layer of these our

1266
01:14:38,320 --> 01:14:39,230
nonlinear features

1267
01:14:39,940 --> 01:14:43,820
right and then as we've seen you sort of able to extract things like low wages

1268
01:14:44,420 --> 01:14:48,690
right or if you're looking at words you can extract correlations between words

1269
01:14:49,090 --> 01:14:52,780
always speech you can extract all four little parts of speech signal

1270
01:14:53,500 --> 01:14:57,590
right and obviously there is a need to go beyond right so people knew about this

1271
01:14:58,710 --> 01:14:59,710
you before but

1272
01:15:00,510 --> 01:15:04,650
up until five years ago we didn't really have good learning algorithms for for being

1273
01:15:04,650 --> 01:15:09,730
able to learn multiple levels of representation so focus has been focusing on just building

1274
01:15:09,730 --> 01:15:11,610
these a single layer models

1275
01:15:13,900 --> 01:15:14,610
so now let me

1276
01:15:15,770 --> 01:15:17,840
introduce you to the deep belief networks

1277
01:15:18,380 --> 01:15:19,210
and then we're gonna take it

1278
01:15:19,570 --> 01:15:20,340
i have an outbreak

1279
01:15:24,170 --> 01:15:24,500
i'm just

1280
01:15:24,920 --> 01:15:25,840
to use weather

1281
01:15:28,800 --> 01:15:31,510
whether we're gonna run out of coffee and not eh

1282
01:15:34,250 --> 01:15:37,250
anyways let me let me just speak for another fifteen

1283
01:15:37,780 --> 01:15:40,150
twenty minutes and then and then we're gonna take a break

1284
01:15:41,510 --> 01:15:44,070
okay so this is the exciting part deep belief networks

1285
01:15:45,130 --> 01:15:47,840
right so these probabilistic generative models

1286
01:15:49,500 --> 01:15:52,800
u have multiple layers of linear representations

1287
01:15:53,380 --> 01:15:57,800
and the beauty of these models is that there's a fast greedy layer-wise pre-training out

1288
01:15:58,750 --> 01:16:03,170
and pre-training algorithm was discovered back in two thousand six and basically opened up

1289
01:16:04,630 --> 01:16:06,590
the space of of of deep learning

1290
01:16:08,280 --> 01:16:10,280
the interesting thing about this algorithm is that's

1291
01:16:10,920 --> 01:16:13,670
inferring the distribution of the latent variables for

1292
01:16:14,900 --> 01:16:16,110
is easy in these models

1293
01:16:16,530 --> 01:16:17,530
and there was a breakthrough

1294
01:16:18,050 --> 01:16:21,150
right because all of a sudden show union image i can quickly tell you what

1295
01:16:21,150 --> 01:16:24,840
a high level representations that make up this image and you can do very quickly

1296
01:16:24,840 --> 01:16:26,750
be today what's died

1297
01:16:26,800 --> 01:16:30,210
first cutting DNA how do you have DNA

1298
01:16:34,340 --> 01:16:36,720
restriction enzymes

1299
01:16:45,150 --> 01:16:50,250
it turns out that the way you could cut DNA at particular places is as

1300
01:16:50,250 --> 01:16:52,170
follows from take piece of DNA

1301
01:16:52,180 --> 01:16:54,570
here's the double stranded piece of DNA

1302
01:16:54,630 --> 01:16:58,280
will go a g c t a g

1303
01:16:58,340 --> 01:17:03,520
a c g t c t t a c c

1304
01:17:03,550 --> 01:17:09,330
hydroxyl there three prime and let's go back to the other strand what we have

1305
01:17:09,380 --> 01:17:11,130
GG t

1306
01:17:13,440 --> 01:17:16,130
a c t

1307
01:17:16,140 --> 01:17:17,750
c t

1308
01:17:17,770 --> 01:17:20,290
HEC c

1309
01:17:20,300 --> 01:17:22,100
i dropped so there

1310
01:17:22,110 --> 01:17:24,500
prior my double stranded DNA

1311
01:17:24,520 --> 01:17:29,500
it turns out that there exists an enzyme

1312
01:17:29,550 --> 01:17:36,030
that recognizes that exact sequence

1313
01:17:36,040 --> 01:17:39,200
g a c

1314
01:17:39,210 --> 01:17:40,550
the enzymes

1315
01:17:40,630 --> 01:17:42,930
goes by the name

1316
01:17:46,670 --> 01:17:51,050
this prompted this enzyme looks scanned along the DNA

1317
01:17:51,060 --> 01:17:54,370
and it finds the sequence GATTACA

1318
01:17:54,380 --> 01:17:59,860
actually it's understand what about on the other strings it's a

1319
01:18:01,150 --> 01:18:03,830
what's reverse happens symmetric

1320
01:18:03,840 --> 01:18:09,800
that's very good and that in turn out most restriction enzymes do that OK so

1321
01:18:09,810 --> 01:18:12,000
what it does when it finds that

1322
01:18:12,020 --> 01:18:15,850
with the benefit of colored chalk is just showing up here is

1323
01:18:16,040 --> 01:18:20,240
it cleaves the DNA fragment

1324
01:18:20,350 --> 01:18:22,310
like that

1325
01:18:22,350 --> 01:18:24,280
and what it gives you the

1326
01:18:25,770 --> 01:18:28,890
a broken

1327
01:18:28,910 --> 01:18:30,900
double strand

1328
01:18:30,910 --> 01:18:32,820
with an over half

1329
01:18:32,880 --> 01:18:35,170
as you can see

1330
01:18:37,050 --> 01:18:40,590
five prime and three prime three prime

1331
01:18:40,650 --> 01:18:42,760
five prime

1332
01:18:42,770 --> 01:18:47,560
this has a hydroxyl here this has the phosphate there and this other fragments here's

1333
01:18:47,560 --> 01:18:50,320
a TTC

1334
01:18:50,340 --> 01:18:53,610
c c c c

1335
01:18:57,680 --> 01:19:01,360
but stops there

1336
01:19:01,370 --> 01:19:04,070
so what happens is

1337
01:19:04,080 --> 01:19:07,280
and this has a chance five prime

1338
01:19:07,290 --> 01:19:09,060
three prime

1339
01:19:09,070 --> 01:19:11,520
three prime five times

1340
01:19:11,540 --> 01:19:18,320
i get to

1341
01:19:18,340 --> 01:19:20,140
fragments of DNA

1342
01:19:20,150 --> 01:19:23,230
that have been broken there and heaven over

1343
01:19:23,280 --> 01:19:25,320
the overhang is complementary to two

1344
01:19:25,330 --> 01:19:31,500
those two sequences match match each other there's what's called the five prime overhang

1345
01:19:31,590 --> 01:19:37,420
and they are complementary so we have complementary

1346
01:19:37,470 --> 01:19:39,450
that is matching

1347
01:19:39,510 --> 01:19:41,690
five primary

1348
01:19:42,050 --> 01:19:45,590
this is called

1349
01:19:45,600 --> 01:19:47,410
he go or one

1350
01:19:47,490 --> 01:19:50,520
because it's purified this particular enzyme

1351
01:19:50,570 --> 01:19:53,330
from e i

1352
01:19:53,340 --> 01:19:56,570
strain are

1353
01:19:57,550 --> 01:20:01,830
and it's the number one such enzyme that was purified from

1354
01:20:01,840 --> 01:20:03,570
that is very simple and country

1355
01:20:03,590 --> 01:20:05,540
now here's a question

1356
01:20:06,450 --> 01:20:08,350
why bacteria

1357
01:20:08,360 --> 01:20:11,070
in enzyme like this

1358
01:20:11,080 --> 01:20:17,420
there are some people feel that the reason is that this enzyme is here precisely

1359
01:20:17,430 --> 01:20:23,550
to allow molecular biologists to cut and paste in this represents oppression on the part

1360
01:20:23,550 --> 01:20:24,870
of the solution

1361
01:20:24,920 --> 01:20:27,640
there are others who think that's less likely

1362
01:20:30,440 --> 01:20:31,750
find this stuff

1363
01:20:32,630 --> 01:20:36,660
shaggy dog story i have to tell you the following shaggy dog story so this

1364
01:20:36,660 --> 01:20:41,470
is a fun shaggy dog story and it's an MIT shaggy dog story because it

1365
01:20:41,470 --> 01:20:46,540
comes from work of salvatore loria who was very famous biologist who worked here at

1366
01:20:49,790 --> 01:20:56,170
so the gloria was studying bacteriophage lambda bacteriophage of the viruses

1367
01:20:56,180 --> 01:20:58,740
that infect bacteria

1368
01:20:58,750 --> 01:21:06,370
so he was studying bacteriophage and he took his bacteriophage and used to infect

1369
01:21:06,390 --> 01:21:10,230
a strain of bacteria strain

1370
01:21:10,360 --> 01:21:16,320
and he also used the to infect a strain of bacteria

1371
01:21:16,340 --> 01:21:17,750
strain b

1372
01:21:19,290 --> 01:21:20,940
so what that

1373
01:21:20,950 --> 01:21:27,520
what you do is you played long of bacterial cells

1374
01:21:27,570 --> 01:21:32,150
you kind of have us motion bacterial cells that you played here with virus mixed

1375
01:21:32,150 --> 01:21:34,940
in and where the virus

1376
01:21:34,990 --> 01:21:36,310
the virus

1377
01:21:36,320 --> 01:21:42,410
growth virus grows replicates and either kills or slows down the growth the cells so

1378
01:21:42,410 --> 01:21:48,100
the bacterial cells grow everywhere else but we're viral particle landed there is an absence

1379
01:21:48,100 --> 01:21:50,690
of bacterial cells and that whole

1380
01:21:50,700 --> 01:21:54,260
in the long run this whole thing is called a along of bacteria and the

1381
01:21:54,260 --> 01:21:56,660
holes in the one are called plaques

1382
01:21:56,670 --> 01:21:59,660
so we did this

1383
01:21:59,700 --> 01:22:02,710
he found that we needed to australia he got a bunch of plax

1384
01:22:02,780 --> 01:22:04,680
and when he did it on strain b

1385
01:22:04,800 --> 01:22:06,590
he didn't

1386
01:22:06,610 --> 01:22:08,310
no blocks

1387
01:22:08,360 --> 01:22:18,280
so what you suppose they measures

1388
01:22:18,330 --> 01:22:22,440
string these different styles resistant to the virus on the virus has come in into

1389
01:22:22,440 --> 01:22:25,990
much more efficient and perhaps more effective and so

1390
01:22:25,990 --> 01:22:28,400
this is one example clearly

1391
01:22:28,440 --> 01:22:31,280
young right people can think of many others

1392
01:22:33,810 --> 01:22:35,940
but today

1393
01:22:40,000 --> 01:22:44,580
i'm i'm i'm racing i was going to give another example of this kind of

1394
01:22:44,580 --> 01:22:48,720
thing but i don't really have time

1395
01:22:49,860 --> 01:22:52,250
light it up a little bit

1396
01:22:52,310 --> 01:22:56,020
so this is the so the last part and it better be only eleven minutes

1397
01:22:56,020 --> 01:22:57,390
because that's all we've got

1398
01:22:57,410 --> 01:23:01,390
you have plains to catch and places to be cars to drive

1399
01:23:01,750 --> 01:23:06,810
i thought i might sort of give very light sort of discussion of the some

1400
01:23:06,810 --> 01:23:08,810
of the tools that people in the

1401
01:23:08,860 --> 01:23:11,910
in articulatory phonetics use

1402
01:23:12,430 --> 01:23:15,360
to study some of these principles if i have time

1403
01:23:15,390 --> 01:23:21,070
talk about some speech corpora exist in other resources that in terms of workshops and

1404
01:23:21,070 --> 01:23:22,000
things like that

1405
01:23:24,110 --> 01:23:32,400
one of the most common measuring apparatus you you here referred to is the

1406
01:23:35,320 --> 01:23:37,750
and used in

1407
01:23:38,110 --> 01:23:41,530
acoustic like acoustic articulography

1408
01:23:41,530 --> 01:23:46,690
and actually that technology has recently gone through something of a revolution

1409
01:23:46,690 --> 01:23:50,900
is that now we see this core character can you see here is that too

1410
01:23:51,940 --> 01:23:54,850
this is organised

1411
01:23:54,860 --> 01:23:55,650
can you see

1412
01:23:55,660 --> 01:23:58,530
we i probably in the interview that you can see

1413
01:23:58,580 --> 01:24:02,150
but you can see he's got this is why are sort of

1414
01:24:02,150 --> 01:24:03,220
in his mouth

1415
01:24:03,270 --> 01:24:08,930
they been attached to a long standing in and lower lip in various places and

1416
01:24:08,960 --> 01:24:12,600
he's got a smile on his face now but wait half an hour

1417
01:24:12,610 --> 01:24:18,560
of uttering nonsense syllables and see how it is flying but the in this this

1418
01:24:18,560 --> 01:24:27,500
new technology actually will obtain a sort of three dimensional trajectories of of articulatory articulatory

1419
01:24:27,500 --> 01:24:31,560
motion and it's actually much easier to use the old system you had like a

1420
01:24:31,560 --> 01:24:35,040
helmet and would just in you had you had a very

1421
01:24:35,060 --> 01:24:37,740
careful placement of these coils

1422
01:24:37,780 --> 01:24:41,620
and the article and the article is for you to get the the measurements such

1423
01:24:41,620 --> 01:24:46,980
is that you thought you're getting over on the right is a young

1424
01:24:46,990 --> 01:24:49,660
we have one of these and we have a guy

1425
01:24:49,720 --> 01:24:55,390
vince clarke was came from the haskins at mcgill who who

1426
01:24:55,400 --> 01:24:58,240
does these kinds of experiments the

1427
01:24:59,240 --> 01:25:05,040
and the right here is the plots based two-dimensional plot exposition

1428
01:25:05,780 --> 01:25:12,580
position here for any x and y axis and each one of these colors corresponds

1429
01:25:12,580 --> 01:25:17,490
to the trajectory of the different articulators and you can see here

1430
01:25:17,490 --> 01:25:23,540
probably on your paper not here but there's a ton tip the image position back

1431
01:25:23,560 --> 01:25:29,220
position that corresponds to different placements of the of the of the coils

1432
01:25:29,640 --> 01:25:30,960
and so

1433
01:25:31,120 --> 01:25:36,250
what people have actually done with these is actually tried to do articulatory to acoustic

1434
01:25:36,250 --> 01:25:40,270
mapping because general you will have a parallel corpus of the

1435
01:25:40,320 --> 01:25:42,740
article these articulatory

1436
01:25:42,750 --> 01:25:45,990
measurements and then also the acoustics

1437
01:25:47,160 --> 01:25:48,620
is an electro

1438
01:25:49,900 --> 01:25:52,860
which is something you will like come off guard in

1439
01:25:52,870 --> 01:25:54,690
and you can european

1440
01:25:54,690 --> 01:25:57,200
football they wear mouth guards

1441
01:25:57,220 --> 01:25:58,140
now working

1442
01:25:58,190 --> 01:26:00,280
american football with that

1443
01:26:00,320 --> 01:26:05,280
more violent that this is not about this is this is an array of sensors

1444
01:26:05,280 --> 01:26:09,930
so expensive device shouldn't spit it out when you don't know but it does not

1445
01:26:09,950 --> 01:26:13,860
interfere with speech a little bit and so the speaker has to be trained as

1446
01:26:13,860 --> 01:26:17,070
to get used to it but what are the sensors to is a sort of

1447
01:26:17,070 --> 01:26:24,810
measure the percentage of contact between ten and the palate and it's it's it's sort

1448
01:26:24,810 --> 01:26:28,440
of and you can see here there is the the red

1449
01:26:28,490 --> 01:26:30,570
the dots here

1450
01:26:30,650 --> 01:26:32,520
correspond to the

1451
01:26:32,520 --> 01:26:34,390
the points at which

1452
01:26:34,400 --> 01:26:40,360
you know the tongue actually contacts the the palate and you see basically the point

1453
01:26:40,360 --> 01:26:45,710
here was to show that it's different depending on different different sort of acoustic prosodic

1454
01:26:47,080 --> 01:26:55,250
so there's also the electric got electrocorticographic which measures airflow through the glottis alex mention

1455
01:26:55,310 --> 01:27:00,030
the EMG that he's actually were suggesting is going to use it for real time

1456
01:27:00,070 --> 01:27:01,990
language translation actually

1457
01:27:02,040 --> 01:27:03,190
excites here

1458
01:27:03,200 --> 01:27:07,680
it's quite exciting that you would speak in same would force you to say the

1459
01:27:09,600 --> 01:27:13,650
now he's he admitted that was one of his more speculative ideas which is that

1460
01:27:13,650 --> 01:27:15,490
so many ideas

1461
01:27:15,530 --> 01:27:21,150
the the the other interesting measurement things are are used for you part partial direct

1462
01:27:21,150 --> 01:27:23,490
measurements and mentioned before

1463
01:27:29,070 --> 01:27:32,720
audio visual ASR are everywhere

1464
01:27:32,740 --> 01:27:40,570
basically you will in general you'll you'll go through a process of sort of synchronizing

1465
01:27:40,570 --> 01:27:42,560
your your video with the the

1466
01:27:42,720 --> 01:27:49,290
with the face and then sort of locating the mouth and so on doing feature

1467
01:27:50,280 --> 01:27:57,680
these days my understanding is that the features are not highly parameterised but in

1468
01:27:58,440 --> 01:28:03,160
back in the day they would use to form deformable templates and very highly parametric

1469
01:28:03,160 --> 01:28:05,570
representation there's lots of that now

1470
01:28:05,570 --> 01:28:06,750
and you

1471
01:28:06,810 --> 01:28:16,980
well you

1472
01:28:18,160 --> 01:28:23,650
one eight

1473
01:28:23,710 --> 01:28:26,640
so that nu

1474
01:28:31,830 --> 01:28:41,770
one more

1475
01:28:41,860 --> 01:28:43,600
so far

1476
01:29:41,120 --> 01:29:46,330
i seen

1477
01:30:06,800 --> 01:30:09,980
you need to be

1478
01:30:28,920 --> 01:30:34,860
this line will be

1479
01:30:38,250 --> 01:30:45,360
very good work the

1480
01:30:46,540 --> 01:30:49,620
bring he

1481
01:30:53,950 --> 01:30:55,970
based on these

1482
01:31:13,370 --> 01:31:21,500
what you're saying

1483
01:31:53,640 --> 01:31:57,860
one of the problems where we are

1484
01:32:07,640 --> 01:32:08,650
if are

1485
01:32:37,670 --> 01:32:50,590
right off the

1486
01:32:50,620 --> 01:32:54,610
and what

1487
01:32:56,710 --> 01:32:58,860
our goal

1488
01:32:58,920 --> 01:33:11,270
it is often very

1489
01:33:24,960 --> 01:33:35,460
as you can

1490
01:33:43,880 --> 01:33:49,920
or one

1491
01:34:00,550 --> 01:34:04,270
these are you

1492
01:34:04,270 --> 01:34:08,540
and t that means you can not find any smaller sequence

1493
01:34:08,590 --> 01:34:10,840
that satisfies one two

1494
01:34:10,840 --> 01:34:13,880
the support constraint what i didn't talk about with these

1495
01:34:13,880 --> 01:34:16,960
you can also see what is the maximum gap

1496
01:34:16,980 --> 01:34:21,670
that means if the elements appear too far away probably they don't make any sense

1497
01:34:21,690 --> 01:34:26,340
but sometimes in in in the DNA and the proteins

1498
01:34:26,420 --> 01:34:30,770
the physical position in the sequence is not as important as their

1499
01:34:30,790 --> 01:34:32,590
geometry composition because

1500
01:34:32,610 --> 01:34:35,080
you know they have this holding in

1501
01:34:35,880 --> 01:34:39,650
and the sequence there might look very far away once the therefore they might come

1502
01:34:39,650 --> 01:34:42,590
very close so sometimes you might look at their

1503
01:34:42,630 --> 01:34:45,020
the physical attraction rather than

1504
01:34:45,020 --> 01:34:50,590
the sequence position in a sequence

1505
01:34:52,020 --> 01:34:57,560
so there are various algorithms again if anybody interested the courts can be downloaded

1506
01:34:57,940 --> 01:35:01,540
so this is a way of showing how we can mind these things

1507
01:35:01,980 --> 01:35:06,810
so it's it's it's a set a credit to kielder them i don't want to

1508
01:35:06,810 --> 01:35:09,650
go through so there's some performance results

1509
01:35:10,150 --> 01:35:13,480
so i'll go to the next moral

1510
01:35:13,810 --> 01:35:16,750
graph model model mining

1511
01:35:16,770 --> 01:35:21,900
as i said these are details that you know genuinely interested in you really

1512
01:35:21,960 --> 01:35:24,670
spend time to become the concept

1513
01:35:24,690 --> 01:35:28,440
but i give you a general feel for what sequence mining is

1514
01:35:28,500 --> 01:35:32,940
so when you're looking in graphs what you're trying to do that you want to

1515
01:35:32,940 --> 01:35:34,440
find the

1516
01:35:34,440 --> 01:35:36,860
the subgraphs president once

1517
01:35:36,880 --> 01:35:38,690
one group and not

1518
01:35:38,730 --> 01:35:41,360
representing the other group so you can see in this case

1519
01:35:41,400 --> 01:35:44,170
this is a kind of annotated graphs

1520
01:35:44,380 --> 01:35:49,520
so what you're trying to show is that this structure is appears here

1521
01:35:49,560 --> 01:35:51,170
but not in this

1522
01:35:51,190 --> 01:35:53,480
and similarly this structure

1523
01:35:53,900 --> 01:35:55,420
it appears

1524
01:35:55,480 --> 01:35:58,770
in this but not in the and the reason is

1525
01:35:59,730 --> 01:36:00,730
you have

1526
01:36:01,170 --> 01:36:05,340
and you know if you look at this thing that right

1527
01:36:06,920 --> 01:36:10,340
there are necessary so this is set

1528
01:36:10,360 --> 01:36:11,650
so if you look at here

1529
01:36:11,670 --> 01:36:16,670
the zero is present there and edges all match and you don't have that much

1530
01:36:17,630 --> 01:36:19,230
so in the next one

1531
01:36:20,630 --> 01:36:21,730
so even though

1532
01:36:21,960 --> 01:36:24,170
this this edges common

1533
01:36:24,920 --> 01:36:29,460
and have done is i've removed is that means once you post this vertices

1534
01:36:29,480 --> 01:36:31,380
this is still a congress party

1535
01:36:31,440 --> 01:36:34,920
but if you say connected component then of course this is not

1536
01:36:34,960 --> 01:36:39,810
and three seats itself the congress because you don't have any that we see

1537
01:36:39,810 --> 01:36:45,210
so typically could be a chemical compound and then you're you're interested in these things

1538
01:36:45,250 --> 01:36:50,360
so in chemistry people were saying that you know you sometimes

1539
01:36:50,360 --> 01:36:52,520
somebody finds the molecules

1540
01:36:52,520 --> 01:36:56,340
and they wanted to know is that any molecule that already known that has a

1541
01:36:56,340 --> 01:36:57,750
lot of structure

1542
01:36:57,770 --> 01:37:00,590
chemical structure same as the one you want to know

1543
01:37:00,590 --> 01:37:04,460
so it's very very difficult so they not some gifted individuals

1544
01:37:04,460 --> 01:37:07,960
you seem to know it is there are you this chemical looks like similar to

1545
01:37:08,220 --> 01:37:09,900
support chemical

1546
01:37:10,580 --> 01:37:15,360
they really wanted to that can help and these guys have done this stuff

1547
01:37:15,420 --> 01:37:17,040
and they were able to

1548
01:37:17,060 --> 01:37:21,150
build a structure and i'll show you second

1549
01:37:22,560 --> 01:37:23,920
so what have done is

1550
01:37:23,940 --> 01:37:27,340
these are the list let's assume these are chemical structures

1551
01:37:27,360 --> 01:37:32,090
there are the chemical compounds and what they done is they've built some features using

1552
01:37:32,090 --> 01:37:34,360
these kinds of structures so for example

1553
01:37:34,380 --> 01:37:38,320
later they have chosen these features and how you choose these things again it's a

1554
01:37:38,320 --> 01:37:40,540
kind of contrast mining you can do

1555
01:37:40,920 --> 01:37:43,150
so what happens is that

1556
01:37:43,230 --> 01:37:46,730
you can map each of these graphs into in terms of these features

1557
01:37:46,820 --> 01:37:50,320
so the next thing is when equity comes what you do is you create is

1558
01:37:50,320 --> 01:37:52,810
mapped into one of these structures

1559
01:37:52,840 --> 01:37:56,920
what are the present in your query and and straightaway you look into this thing

1560
01:37:56,960 --> 01:37:59,080
and this made the

1561
01:37:59,080 --> 01:38:00,340
OK this like

1562
01:38:00,460 --> 01:38:02,440
phenomenally is

1563
01:38:02,440 --> 01:38:05,840
you can deal with matching

1564
01:38:06,900 --> 01:38:07,770
that are

1565
01:38:07,790 --> 01:38:12,000
somehow structurally similar and then you can find them much more

1566
01:38:12,070 --> 01:38:21,980
so this is a big impact in terms of chemistry and of the other

1567
01:38:22,020 --> 01:38:25,590
so that that's one one application of the contrast mining

1568
01:38:25,650 --> 01:38:29,090
and you can also do edit distances and all kinds of this

1569
01:38:29,110 --> 01:38:31,960
you know what to come to these measurements you know there a limited as i

1570
01:38:31,960 --> 01:38:39,360
said before i find so you can come up all kinds of measuring techniques

1571
01:38:39,500 --> 01:38:41,310
OK so

1572
01:38:41,360 --> 01:38:45,270
so when you are given two models general models and you want to compare them

1573
01:38:45,270 --> 01:38:47,670
how they differ or not

1574
01:38:47,750 --> 01:38:52,270
OK this measurements out all scoring functions can be given

1575
01:38:52,310 --> 01:38:53,690
they could be

1576
01:38:53,690 --> 01:38:59,270
euclidean distance or jakarta distance lots of things you can do

1577
01:39:01,400 --> 01:39:03,190
i'm sorry and rushing through

1578
01:39:03,710 --> 01:39:05,150
but i give you

1579
01:39:05,190 --> 01:39:07,290
good examples here and there

1580
01:39:07,310 --> 01:39:09,230
so that leaves it makes sense to you

1581
01:39:09,290 --> 01:39:14,880
so there are various measures between given for support it says somebody is given a

1582
01:39:14,880 --> 01:39:15,860
data set

1583
01:39:15,860 --> 01:39:20,290
i do one model clustering and you do have kind of clustering and and somebody

1584
01:39:20,290 --> 01:39:21,980
might ask how

1585
01:39:22,060 --> 01:39:27,340
different these clusterings of the big difference between them so you can do using

1586
01:39:27,380 --> 01:39:31,190
either rand index or card index and i'll how we could do

1587
01:39:31,190 --> 01:39:33,690
or we can do some kind of mutual information

1588
01:39:33,690 --> 01:39:35,230
or clustering error

1589
01:39:35,250 --> 01:39:36,110
OK so

1590
01:39:36,110 --> 01:39:41,070
equally but we would like it to be moved along our minds

1591
01:39:41,950 --> 01:39:45,460
we can more about what happens along the manifold

1592
01:39:45,480 --> 01:39:46,760
and what happens

1593
01:39:46,770 --> 01:39:48,660
in this somehow

1594
01:39:48,670 --> 01:39:51,510
outer space

1595
01:39:51,530 --> 01:39:58,420
and for that for for that purpose where adding this extra term this intrinsic movement

1596
01:39:58,420 --> 01:40:01,710
and this is going to be basically i want

1597
01:40:01,760 --> 01:40:03,370
the gradient

1598
01:40:03,370 --> 01:40:05,150
of this function

1599
01:40:05,190 --> 01:40:10,280
along the manifold itself along the probability distribution more generally so really what you want

1600
01:40:10,290 --> 01:40:13,720
you want to consider manifold with probability distribution on it

1601
01:40:13,740 --> 01:40:16,550
and i want that quantity

1602
01:40:16,570 --> 01:40:20,570
tools somehow baseball i want to control i want to have control over

1603
01:40:20,580 --> 01:40:23,640
how much it changes along my

1604
01:40:29,460 --> 01:40:33,470
you can shows theorem about this

1605
01:40:33,480 --> 01:40:39,670
basically there is some version of the representer theorem for the continuous case

1606
01:40:40,750 --> 01:40:44,860
so if you most so you can just imagine this being granted

1607
01:40:45,430 --> 01:40:46,920
there is a lot less

1608
01:40:48,640 --> 01:40:51,700
and then basically there some version of represent first one

1609
01:40:51,710 --> 01:40:55,930
let's keep this point

1610
01:40:59,520 --> 01:41:01,720
manifold for

1611
01:41:02,360 --> 01:41:06,140
so now what what is the norm the norm is really this is this gradient

1612
01:41:06,340 --> 01:41:08,430
and this is what we want to control

1613
01:41:08,450 --> 01:41:11,740
and this of course is controlled by the what lies and we said before however

1614
01:41:11,750 --> 01:41:15,890
there are various other things we can do we can for example take the differential

1615
01:41:15,890 --> 01:41:22,880
operators on the manifold for example one can try taking passionate maybe this would be

1616
01:41:22,880 --> 01:41:27,320
related cool fashion i can maps by donoho grimes

1617
01:41:27,700 --> 01:41:29,190
one can

1618
01:41:29,230 --> 01:41:31,980
i tried to do

1619
01:41:31,980 --> 01:41:35,430
one can do or exponential of the placenta

1620
01:41:36,840 --> 01:41:39,450
this is related to the work which actually

1621
01:41:39,490 --> 01:41:42,660
will be presented i think today

1622
01:41:46,970 --> 01:41:48,930
and there are

1623
01:41:49,050 --> 01:41:52,210
other things one can also the wall

1624
01:41:58,140 --> 01:42:02,270
this is to say that if we actually knew the manifold cover perfect solution would

1625
01:42:03,330 --> 01:42:06,990
we know exactly what optimise and we can do very well but the manifold is

1626
01:42:06,990 --> 01:42:08,030
not known

1627
01:42:08,080 --> 01:42:11,500
and we already if we already know how what to do with it because we

1628
01:42:11,500 --> 01:42:15,650
can estimate the laplace operator from the data

1629
01:42:15,690 --> 01:42:19,480
so instead of trying to actually

1630
01:42:19,530 --> 01:42:24,150
do this differential operator integrates gradient what we do we estimate the plus in from

1631
01:42:24,150 --> 01:42:28,640
the data and use this laplacian as the penalty for most and this love plus

1632
01:42:28,640 --> 01:42:30,830
and of course is just

1633
01:42:30,880 --> 01:42:35,230
it's just the matrix associated to the point

1634
01:42:35,280 --> 01:42:40,160
so it's an by n matrix where ice to help plus you so l is

1635
01:42:40,160 --> 01:42:43,820
the number of labelled points you is the number of unlabeled points so the the

1636
01:42:43,820 --> 01:42:45,400
slope plus and is the

1637
01:42:45,440 --> 01:42:47,420
helpless human

1638
01:42:47,430 --> 01:42:52,430
so on each data point with labelled and unlabelled you have

1639
01:42:52,450 --> 01:42:54,220
you have some sort of

1640
01:42:57,890 --> 01:42:59,930
when we put this here

1641
01:42:59,990 --> 01:43:04,040
you get the best now just becomes to evaluate this function

1642
01:43:04,080 --> 01:43:06,980
and few labeled and unlabeled points

1643
01:43:07,020 --> 01:43:08,480
and you

1644
01:43:08,480 --> 01:43:11,080
take f principles l

1645
01:43:11,160 --> 01:43:14,880
so now when you write a function that you labeled and unlabeled point you get

1646
01:43:14,880 --> 01:43:16,740
help us well

1647
01:43:16,800 --> 01:43:19,410
you multi you take from schools

1648
01:43:19,450 --> 01:43:20,530
times l

1649
01:43:20,550 --> 01:43:21,910
times f

1650
01:43:21,940 --> 01:43:23,540
so that's matrix

1651
01:43:23,560 --> 01:43:28,710
roll times the matrix times column so you get the number of this number you

1652
01:43:28,840 --> 01:43:32,110
the intrinsic penalty associated to you function

1653
01:43:32,230 --> 01:43:34,130
and you want to minimize

1654
01:43:34,130 --> 01:43:38,940
in the upper rhine across from modulation which is the dominant crop which they developed

1655
01:43:38,940 --> 01:43:44,320
in the same direction o in all across which is most sensical to integrate them

1656
01:43:45,670 --> 01:43:47,800
what comes out of that if you do that is

1657
01:43:47,820 --> 01:43:53,010
and he component of many EEG components that has looked like in one has the

1658
01:43:53,010 --> 01:43:56,510
difference between standard target cities differently if you look like

1659
01:43:56,530 --> 01:44:02,690
processing negativity into BP three and that's saying it's mismatch just saying it's processing negativity

1660
01:44:02,690 --> 01:44:05,190
that has mismatch in it to all the

1661
01:44:05,190 --> 01:44:06,240
here p

1662
01:44:06,690 --> 01:44:12,400
the first people here it the central topography which makes sense with with this type

1663
01:44:12,400 --> 01:44:14,880
of component and then correlates with

1664
01:44:14,920 --> 01:44:19,190
this map with the across trial modulation of this map which is bilaterally in the

1665
01:44:19,190 --> 01:44:22,650
temporal lobes but here in the frontal lobe and inverse

1666
01:44:22,650 --> 01:44:24,090
blob here the

1667
01:44:24,110 --> 01:44:26,470
the reason of the critical information

1668
01:44:26,630 --> 01:44:29,440
altogether if you look at after i literature EEG literature

1669
01:44:30,450 --> 01:44:34,170
makes sense with respect to the locus of generation of n one

1670
01:44:34,220 --> 01:44:38,170
of the auditory onset response will be looking at is some kind of joint representation

1671
01:44:39,010 --> 01:44:43,760
the auditory onset response and its modulation across trials the interesting part with this response

1672
01:44:43,860 --> 01:44:47,000
doesn't show the learning curve that we want to that we were so what did

1673
01:44:47,000 --> 01:44:50,320
we would think it should show but rather a transient

1674
01:44:50,380 --> 01:44:54,470
two the onset of random or the occurrence of random target after caesar regular once

1675
01:44:54,470 --> 01:44:59,990
was just responding to the incremental surprise it doesn't by itself could modulate with the

1676
01:44:59,990 --> 01:45:02,420
decrement of surprise

1677
01:45:02,440 --> 01:45:03,440
so that's that

1678
01:45:03,570 --> 01:45:07,360
i didn't

1679
01:45:07,380 --> 01:45:10,740
i have a good feeling of how robust this analysis would be

1680
01:45:10,760 --> 01:45:13,130
i mean you do a lot of things in their lot of

1681
01:45:14,720 --> 01:45:17,260
steps in this analysis which look

1682
01:45:17,280 --> 01:45:19,260
city or look tacky

1683
01:45:19,280 --> 01:45:23,670
so i took a look at the different set from a colleague of our stuff

1684
01:45:23,670 --> 01:45:27,170
and they've been published simultaneously with us

1685
01:45:27,210 --> 01:45:29,340
basically engineering science

1686
01:45:29,340 --> 01:45:33,440
they looked at the year end which is narrowly negativity which occurs only when subjects

1687
01:45:33,440 --> 01:45:36,170
making error in some kind of task

1688
01:45:36,240 --> 01:45:42,050
can be complex or simple test test with time constraints accuracy constraints they do that

1689
01:45:42,090 --> 01:45:47,300
they get this post response negativity at about fifty to eighty million seconds after after

1690
01:45:47,300 --> 01:45:49,190
response on so

1691
01:45:49,220 --> 01:45:51,740
it's a pretty specific component two errors

1692
01:45:51,760 --> 01:45:56,780
has this topography and correlates on a single trial basis with this region in the

1693
01:45:56,780 --> 01:45:58,440
anterior cingulate

1694
01:45:58,470 --> 01:46:01,010
which makes sense with respect to

1695
01:46:01,010 --> 01:46:05,400
these and data for example or to source estimates

1696
01:46:05,420 --> 01:46:08,970
which is nice so what we ask if this is what happens when people make

1697
01:46:08,970 --> 01:46:10,380
an error

1698
01:46:10,400 --> 01:46:11,880
how would we

1699
01:46:11,900 --> 01:46:15,570
go about and extract the brain signal that would predict that people are going to

1700
01:46:15,570 --> 01:46:16,880
make an error

1701
01:46:16,900 --> 01:46:20,610
is there anything that we can see in the from my data in the data

1702
01:46:20,720 --> 01:46:23,900
that will allow us to make a prediction that on the next trial you've so

1703
01:46:23,970 --> 01:46:27,650
so many chapters so present of it tends to make an error

1704
01:46:27,670 --> 01:46:31,470
we do the same thing

1705
01:46:31,550 --> 01:46:34,130
group ICA model

1706
01:46:34,400 --> 01:46:37,530
which just consists of this taking part

1707
01:46:37,550 --> 01:46:43,010
reduction part concatenation estimation of the components that protection of the components that we have

1708
01:46:43,010 --> 01:46:45,690
the step where we may component selection

1709
01:46:45,690 --> 01:46:47,510
which is

1710
01:46:48,840 --> 01:46:50,820
difficult to solve

1711
01:46:50,860 --> 01:46:54,580
we have some some map based criteria so we do the ICA like like suggested

1712
01:46:54,580 --> 01:47:00,010
the previous talk multiple times on non identical subsets of the data as to subsample

1713
01:47:00,440 --> 01:47:01,820
big data set into

1714
01:47:01,820 --> 01:47:03,130
neighboring voxels

1715
01:47:03,150 --> 01:47:05,780
it should be calling it should result in the same

1716
01:47:05,820 --> 01:47:12,070
components if its physiological importance because of the spatial correlation and whatever is not seen

1717
01:47:12,820 --> 01:47:17,030
across these red repetitions of the analysis is not taken into

1718
01:47:17,050 --> 01:47:22,320
consideration further then we look at these components in sec whether their physiological not

1719
01:47:22,360 --> 01:47:26,690
with respect to cortical generation so there's many components that we need to see is

1720
01:47:27,840 --> 01:47:30,260
components that relate to

1721
01:47:30,280 --> 01:47:31,470
blood flow

1722
01:47:31,490 --> 01:47:36,590
which tend to be many components because there's flow signal this dynamic signal into india

1723
01:47:36,590 --> 01:47:41,590
from my and then we have an additional step we check the population relevance of

1724
01:47:41,590 --> 01:47:46,260
these components is there on the average or on the FDR statistic

1725
01:47:46,400 --> 01:47:48,440
and activation of this component

1726
01:47:48,510 --> 01:47:52,220
gift of the group in

1727
01:47:52,240 --> 01:47:53,860
group ICA is

1728
01:47:53,950 --> 01:48:00,380
detect components in the population when they consistent about ten percent in the subjects so

1729
01:48:01,010 --> 01:48:05,880
you do see components that some subjects have some components of the subjects have

1730
01:48:05,880 --> 01:48:08,710
there are not significant

1731
01:48:08,710 --> 01:48:12,190
it's not that all subjects have to have all components

1732
01:48:12,220 --> 01:48:13,260
to start with

1733
01:48:13,280 --> 01:48:15,820
because that makes the text difficult

1734
01:48:17,220 --> 01:48:20,340
and then when when we when we when we check those

1735
01:48:20,740 --> 01:48:24,590
the components that have an additional event response that

1736
01:48:25,440 --> 01:48:28,110
has some some some role in the

1737
01:48:28,130 --> 01:48:30,670
in the stimulus paradigm we take further

1738
01:48:30,720 --> 01:48:32,590
and do some deconvolution here

1739
01:48:35,880 --> 01:48:38,710
when we when we do that with a stimulus timing

1740
01:48:38,800 --> 01:48:39,590
and then

1741
01:48:39,610 --> 01:48:43,420
multiply the pseudo inverse of convolution metric of the system's timing with the with the

1742
01:48:43,860 --> 01:48:47,550
time course to get an estimated rest

1743
01:48:47,590 --> 01:48:50,880
and with the graph we can get back to the parametric modulation

1744
01:48:50,900 --> 01:48:53,190
so we take out the human dynamic lag

1745
01:48:53,210 --> 01:48:54,590
of the data

1746
01:48:54,610 --> 01:48:59,030
and when we do that

1747
01:48:59,050 --> 01:49:00,150
we get

1748
01:49:00,170 --> 01:49:01,300
this kind of map

1749
01:49:01,340 --> 01:49:03,090
in the inferior frontal gyrus

1750
01:49:03,110 --> 01:49:05,630
for example many of superior frontal gyrus

1751
01:49:05,630 --> 01:49:10,030
with this editor have the starts being the individual observations

1752
01:49:10,050 --> 01:49:11,990
and this type of modulation

1753
01:49:11,990 --> 01:49:14,220
this trial here being the error trial

1754
01:49:14,220 --> 01:49:16,090
and before the error occurs this

1755
01:49:17,050 --> 01:49:20,570
reduces activation over thirty seconds

1756
01:49:21,820 --> 01:49:24,380
so we thought that five

1757
01:49:24,470 --> 01:49:27,220
we got another component which is in a precarious

1758
01:49:27,240 --> 01:49:29,070
an event related the activation

1759
01:49:29,130 --> 01:49:31,010
that gradually

1760
01:49:31,050 --> 01:49:33,070
increases its activation

1761
01:49:33,090 --> 01:49:35,450
before never occurs

1762
01:49:35,470 --> 01:49:38,550
these two systems here interact

1763
01:49:38,570 --> 01:49:41,440
in an antagonistic fashion if you will and

1764
01:49:41,450 --> 01:49:43,510
if it looks like

1765
01:49:43,510 --> 01:49:46,900
if tries look like this here so this would be

1766
01:49:46,920 --> 01:49:51,280
bigger than zero and this would be smaller than zero probability of making error on

1767
01:49:51,280 --> 01:49:53,970
the next trial increases by fifty percent

1768
01:49:53,990 --> 01:49:57,860
if it's the other way around it decreased by forty percent

1769
01:49:59,260 --> 01:50:03,800
the point is we wouldn't be able to see this with the standard model

1770
01:50:06,720 --> 01:50:12,090
currently we did the group ICA of the data and got this type of component

1771
01:50:12,090 --> 01:50:14,400
which is the stimulus like you're p with this

1772
01:50:14,780 --> 01:50:16,570
negative deflection poster early

1773
01:50:16,590 --> 01:50:18,630
makes it into visual and two

1774
01:50:18,630 --> 01:50:23,360
we also have gradient before error shows a slightly different modulation after or during the

1775
01:50:23,360 --> 01:50:30,800
error but then again there's great here here here which makes this a coherent system

1776
01:50:30,880 --> 01:50:32,940
conclusion of this

1777
01:50:33,940 --> 01:50:38,340
that a makes EEG fmri together not just for artifact rejection

1778
01:50:38,380 --> 01:50:41,860
improve the detection of correspondences in concurrent data

1779
01:50:41,860 --> 01:50:44,900
we did that in this case with the one i meant

1780
01:50:44,920 --> 01:50:49,610
we we cover something that is known that we don't mess with the previous

1781
01:50:49,610 --> 01:50:51,650
this is a

1782
01:50:51,730 --> 01:50:56,150
and they have some parameters which could be for example in the case of multivariate

1783
01:50:56,150 --> 01:51:00,100
gaussian distribution the mean vector and covariance matrix

1784
01:51:00,180 --> 01:51:02,040
after this

1785
01:51:06,810 --> 01:51:10,410
see i'm sorry see means conditional independence

1786
01:51:11,650 --> 01:51:18,200
motivated distributions extracted according to conditional independence this is the subject of the second hour

1787
01:51:18,200 --> 01:51:20,190
of of course

1788
01:51:21,740 --> 01:51:24,870
essentially the structure of the these

1789
01:51:24,870 --> 01:51:25,950
of these

1790
01:51:25,960 --> 01:51:29,710
mortified when the distributional study

1791
01:51:29,790 --> 01:51:31,210
is essentially

1792
01:51:31,260 --> 01:51:32,550
given by

1793
01:51:32,560 --> 01:51:36,190
pattern that call pattern conditional independence

1794
01:51:36,230 --> 01:51:37,860
and so for me

1795
01:51:37,860 --> 01:51:40,770
maybe some of these

1796
01:51:40,910 --> 01:51:43,930
but if you sure take over the world some

1797
01:51:45,070 --> 01:51:47,360
so their bodies

1798
01:51:51,490 --> 01:51:55,870
you may want to answer this questions for example if you have this probabilistic model

1799
01:51:56,190 --> 01:52:02,420
you may be interested in computing what's the probability of a particular

1800
01:52:02,470 --> 01:52:05,470
realisation of these right

1801
01:52:05,480 --> 01:52:06,760
love value

1802
01:52:06,820 --> 01:52:11,030
think of these spirals from x one to xn for example

1803
01:52:12,340 --> 01:52:14,650
pixels in an image OK

1804
01:52:14,700 --> 01:52:18,710
if you think of them as pixels in the image

1805
01:52:18,760 --> 01:52:20,140
we can

1806
01:52:20,140 --> 01:52:23,410
assume you have a model for

1807
01:52:23,460 --> 01:52:26,510
the noise images

1808
01:52:26,520 --> 01:52:28,480
and then the question is

1809
01:52:28,540 --> 01:52:32,710
what's the probability of a given the just some so

1810
01:52:33,780 --> 01:52:38,420
we want to compute the probability of particle element of all the image space

1811
01:52:40,710 --> 01:52:42,770
to be very difficult

1812
01:52:42,840 --> 01:52:44,710
we'll see

1813
01:52:44,760 --> 01:52:49,750
because you have many variables and they interact with each other it's not like the

1814
01:52:49,800 --> 01:52:53,060
the interact exactly precisely according to these

1815
01:52:53,070 --> 01:52:55,430
i conditional independence structures

1816
01:52:55,570 --> 01:52:59,280
for him

1817
01:52:59,290 --> 01:53:04,740
and the other question is they are approximation for what's the most likely image

1818
01:53:04,760 --> 01:53:18,750
most likely corrected the image from the

1819
01:53:18,760 --> 01:53:20,340
appropriate model for

1820
01:53:20,360 --> 01:53:28,070
for the lies

1821
01:53:29,320 --> 01:53:30,940
i think that

1822
01:53:32,040 --> 01:53:35,300
it's possible to say

1823
01:53:35,350 --> 01:53:37,460
it doesn't mean however that

1824
01:53:37,470 --> 01:53:42,130
a particular case of a noisy image processing it doesn't mean that you can have

1825
01:53:42,130 --> 01:53:45,620
an estimator for the more so

1826
01:53:45,640 --> 01:53:48,980
you can estimate the noise if you know how

1827
01:53:49,000 --> 01:53:50,460
images behave

1828
01:53:50,500 --> 01:53:52,380
natural images

1829
01:53:52,420 --> 01:53:53,410
you can

1830
01:53:53,980 --> 01:53:56,160
two compute how that

1831
01:53:56,350 --> 01:53:58,540
the images from

1832
01:53:58,560 --> 01:54:00,500
what it should be made

1833
01:54:01,960 --> 01:54:07,510
maybe you should have across the image created for performing inference

1834
01:54:07,530 --> 01:54:09,980
we need to have knowledge

1835
01:54:10,030 --> 01:54:13,660
i want you

1836
01:54:13,740 --> 01:54:15,960
so again the next four years

1837
01:54:15,960 --> 01:54:19,530
OK let's assume you want to try to

1838
01:54:19,570 --> 01:54:21,110
and so

1839
01:54:21,120 --> 01:54:23,130
the following question

1840
01:54:23,180 --> 01:54:24,560
we want to compute

1841
01:54:24,610 --> 01:54:27,160
the probability

1842
01:54:27,170 --> 01:54:28,190
we have

1843
01:54:28,200 --> 01:54:32,420
what if i distribution with anybody

1844
01:54:34,800 --> 01:54:39,470
and let's assume that every one of these viruses the discrete fire

1845
01:54:39,510 --> 01:54:40,470
for example

1846
01:54:40,490 --> 01:54:42,780
this described by with that takes only two

1847
01:54:42,840 --> 01:54:44,490
possible then

1848
01:54:44,590 --> 01:54:46,340
good example could be

1849
01:54:46,340 --> 01:54:47,440
i don't know

1850
01:54:47,610 --> 01:54:52,130
the weather the weather can be made ward of

1851
01:54:53,400 --> 01:54:57,130
it also required to be in the hands of tail

1852
01:54:57,130 --> 01:55:02,470
for each one of these viruses from x one to xn can attain two values

1853
01:55:02,490 --> 01:55:05,590
just making a simple example

1854
01:55:05,610 --> 01:55:10,300
we can ask the question if i have this model is multivariate model that combines

1855
01:55:10,300 --> 01:55:12,010
all the five

1856
01:55:12,070 --> 01:55:17,740
i mean interested in the outcome of one particular driver

1857
01:55:17,780 --> 01:55:20,670
for example if you have a model of of the word in australia may be

1858
01:55:20,670 --> 01:55:25,130
interested in the question what's the probability it's going to rain today and dark

1859
01:55:27,110 --> 01:55:31,300
i don't have a model for diary only i have a model for the entire

1860
01:55:33,150 --> 01:55:35,440
in australia

1861
01:55:35,470 --> 01:55:38,050
so the question is not interesting whether it's

1862
01:55:38,070 --> 01:55:43,320
training camp not training camp really silly when operating in sydney

