1
00:00:00,000 --> 00:00:08,100
we can do the same thing with an omission evoked potential which is just exchange

2
00:00:08,110 --> 00:00:10,600
the target for break so

3
00:00:10,620 --> 00:00:13,190
one two three four nothing

4
00:00:13,210 --> 00:00:16,590
and people adapt to break as well as they do two target is just to

5
00:00:16,940 --> 00:00:21,660
the trick that europe is also to be stimulus induced but you can also make

6
00:00:21,660 --> 00:00:23,630
a silent gap stimulus

7
00:00:23,640 --> 00:00:28,670
so that you don't have stimulus related potentials but just everything that is top down

8
00:00:28,710 --> 00:00:30,720
if you will

9
00:00:30,740 --> 00:00:33,680
well you don't have much of an an two or p three you see something

10
00:00:33,680 --> 00:00:37,910
you what i think it doesn't hold the statistic but you do see is in

11
00:00:38,630 --> 00:00:44,080
so people adapt to break in sequence as well as they do two two different

12
00:00:48,100 --> 00:00:49,930
and across

13
00:00:49,940 --> 00:00:55,140
repetitions of these cycles so what i this this trend of having earlier learning curves

14
00:00:55,140 --> 00:00:59,520
are introduced to before this also happens on the on the scene he pretty clearly

15
00:00:59,790 --> 00:01:03,720
it doesn't is not so clear with p three in two but it's still significant

16
00:01:03,720 --> 00:01:07,010
this one we don't know why the way that's there but

17
00:01:07,090 --> 00:01:08,160
you still have

18
00:01:08,210 --> 00:01:12,060
the trend that the learning curve has the transient that is really an early across

19
00:01:13,150 --> 00:01:16,710
right so people learn they learn faster the longer you do the same thing with

20
00:01:20,090 --> 00:01:24,280
now what we want to know is OK we know that

21
00:01:24,300 --> 00:01:27,510
at number of electrodes at different latencies

22
00:01:27,530 --> 00:01:29,040
we have these effects

23
00:01:29,050 --> 00:01:30,900
where do they come from

24
00:01:31,830 --> 00:01:35,580
it's a bit tricky to make to make a spatial inference from easy so that's

25
00:01:35,580 --> 00:01:36,550
what we wanted to do

26
00:01:36,560 --> 00:01:37,840
after right

27
00:01:37,860 --> 00:01:41,340
but then we already have the functional modulation we

28
00:01:41,360 --> 00:01:44,340
we we had the signal that we wanted to look at in the easy so

29
00:01:44,340 --> 00:01:48,030
what we wanted to individuals not

30
00:01:48,110 --> 00:01:52,260
and my study with some other e they wanted to do the same thing at

31
00:01:52,260 --> 00:01:55,600
the same time right so i wanted to do easy inside the scanner and get

32
00:01:55,600 --> 00:01:57,130
the same effect in the EEG

33
00:01:57,140 --> 00:01:58,590
and predict the fmri

34
00:01:58,610 --> 00:02:00,220
with that signal

35
00:02:01,300 --> 00:02:04,400
so that you have this learning effect on the same subjects in both

36
00:02:06,890 --> 00:02:10,960
so what we did is that these amplifiers which you can put close to the

37
00:02:10,960 --> 00:02:14,210
scanner keep the subject inside the scanner

38
00:02:14,720 --> 00:02:18,540
there's not much that you do technically that's difficult you can just by these amplifiers

39
00:02:19,500 --> 00:02:25,710
you keep the something happy new this paradigm while record from my and easy

40
00:02:25,760 --> 00:02:31,720
simultaneously and afterwards he begin processing the whole thing starting with the artifact removal

41
00:02:31,760 --> 00:02:36,360
the gradient artefacts impacting on the signal quality in an EEG you'd have quite interesting

42
00:02:36,380 --> 00:02:38,160
artifacts all kinds of evil things

43
00:02:38,170 --> 00:02:40,500
which you can control for

44
00:02:40,520 --> 00:02:44,810
right to have the ECG so we can control for about this recording an artifact

45
00:02:44,880 --> 00:02:49,130
and the gradient artefacts in the goods cannot tends to be very text so you

46
00:02:49,130 --> 00:02:53,130
can subtract it with template based correction

47
00:02:53,650 --> 00:02:57,450
after that we tend to unmix EEG with ICA

48
00:02:57,500 --> 00:03:02,890
in our application we just did that for getting rid of artifacts

49
00:03:02,960 --> 00:03:04,880
i movement

50
00:03:05,930 --> 00:03:07,960
residual current things

51
00:03:07,970 --> 00:03:11,750
are picked up pretty nicely by c and then we just back protected the residual

52
00:03:11,820 --> 00:03:16,280
and use that forcing for estimation in the different

53
00:03:16,310 --> 00:03:19,970
version that came out by by seventy even though they picked out one component that

54
00:03:19,970 --> 00:03:24,120
they could reliably identify and related to one component

55
00:03:24,130 --> 00:03:27,040
what you're p components

56
00:03:27,090 --> 00:03:29,820
and then we did something like quantification where we

57
00:03:29,830 --> 00:03:31,260
i took a signal

58
00:03:31,580 --> 00:03:37,720
this and wavelet denoising so this country constrained time frequency content to whatever we thought

59
00:03:37,730 --> 00:03:41,710
was relevant and then convolve that within the canonical ref

60
00:03:41,730 --> 00:03:44,780
and predicted the atom activation

61
00:03:44,790 --> 00:03:46,720
in this way

62
00:03:46,740 --> 00:03:47,790
you basically do

63
00:03:47,800 --> 00:03:49,720
you can do

64
00:03:49,740 --> 00:03:52,490
timepoint by timepoint correlations between e

65
00:03:52,520 --> 00:03:53,660
and the from

66
00:03:53,830 --> 00:04:00,230
you get to a resolution of the my response as long as signal

67
00:04:00,240 --> 00:04:03,840
represents different time points in different voxels

68
00:04:03,890 --> 00:04:05,820
that's the only

69
00:04:05,840 --> 00:04:09,320
that's how it works here

70
00:04:09,340 --> 00:04:16,610
this will be just to standard from my analysis so define the onset times of

71
00:04:16,610 --> 00:04:17,960
our targets

72
00:04:17,970 --> 00:04:22,470
and correlated with the human activation you get

73
00:04:22,490 --> 00:04:24,690
the usual suspects of of processing

74
00:04:24,840 --> 00:04:27,170
here temporal lobes parietal lobes

75
00:04:27,190 --> 00:04:29,440
you from the data

76
00:04:29,460 --> 00:04:33,670
these medial the midline areas here some a precarious

77
00:04:33,690 --> 00:04:37,370
and so forth and to force is just basically there's nice paper by can kill

78
00:04:37,710 --> 00:04:42,140
neuroimage image from two years ago when you have enough subjects

79
00:04:42,150 --> 00:04:44,060
you basically light up the whole range

80
00:04:44,100 --> 00:04:47,120
because the ball is so simple that basically the whole

81
00:04:47,710 --> 00:04:51,880
the whole conglomerate of brain areas that don't have anything to do with the task

82
00:04:52,090 --> 00:04:54,010
would activate after a while

83
00:04:54,050 --> 00:04:57,570
that's that's pretty nice idea

84
00:04:57,590 --> 00:05:02,230
then we had basically what happened to slides before

85
00:05:02,240 --> 00:05:06,180
we have these amplitude modulations this learning function for example

86
00:05:06,270 --> 00:05:08,230
which is not expressed

87
00:05:08,240 --> 00:05:12,450
in the same way across timepoints right then it will be a single are

88
00:05:12,470 --> 00:05:18,410
representation this this learning function occurs at different time points and to change its shape

89
00:05:18,420 --> 00:05:20,680
it changes

90
00:05:20,690 --> 00:05:25,250
basically how it represented in the areas that my generated so you'd have a sinusoidal

91
00:05:25,270 --> 00:05:29,550
or sigmoid they might be pretty correlated but still the shape is different related to

92
00:05:29,550 --> 00:05:31,220
different types of functions

93
00:05:31,270 --> 00:05:36,860
and reducing the fires can localize localize these guys

94
00:05:36,870 --> 00:05:39,990
at different time points basically is just in this

95
00:05:40,000 --> 00:05:44,750
this figure here

96
00:05:45,790 --> 00:05:49,320
you get a bit tricky in the in the in the techniques but bear with

97
00:05:49,320 --> 00:05:53,730
me so so what we do is a single trial denoising where we started off

98
00:05:53,730 --> 00:05:59,040
after artefact correction with independent component analysis we think that while this component look like

99
00:05:59,040 --> 00:06:02,950
in europe even even for data this component doesn't look like in european we take

100
00:06:02,950 --> 00:06:04,370
it out of of the data

101
00:06:04,390 --> 00:06:08,100
this is as much as we use ICA here right

102
00:06:08,110 --> 00:06:08,980
and then

103
00:06:10,670 --> 00:06:14,040
take for just two

104
00:06:14,040 --> 00:06:17,960
you can do it very much better so here's our mark zero

105
00:06:17,960 --> 00:06:19,060
we call this

106
00:06:19,080 --> 00:06:22,060
originally the concept we can we call the two camps

107
00:06:22,110 --> 00:06:26,560
and this one however is made of two large cameras glued together so we call

108
00:06:26,560 --> 00:06:27,940
this one the glue can

109
00:06:27,940 --> 00:06:34,630
and we use this one for quite a while but it's not very satisfactory because

110
00:06:35,710 --> 00:06:42,170
USB cameras and USB is not temporary synchronized you cannot get USB do temporal synchronization

111
00:06:42,230 --> 00:06:47,580
clearly the stereo the whole point is that the images a synchronized so actually if

112
00:06:47,580 --> 00:06:51,850
you use firewire cameras firewire happens does happen in the

113
00:06:51,900 --> 00:06:58,400
the middle where synchronization you can do that but that's sort of more expensive route

114
00:06:58,420 --> 00:07:01,060
various reasons so we've now built

115
00:07:01,060 --> 00:07:06,810
had a local company called cambridge signal processing build as a problem

116
00:07:06,830 --> 00:07:08,940
camera which they call the hydra can

117
00:07:09,000 --> 00:07:11,290
in which is a USB camera

118
00:07:12,650 --> 00:07:17,420
but proper secret synchronized because it synchronized the level of hardware

119
00:07:17,440 --> 00:07:19,650
and so having a lot of fun

120
00:07:19,670 --> 00:07:22,290
playing with this work because things can you do with it well

121
00:07:22,290 --> 00:07:23,880
he is my colleague

122
00:07:23,880 --> 00:07:25,900
holly williams

123
00:07:25,920 --> 00:07:29,610
chatting away on the on the camera

124
00:07:29,630 --> 00:07:33,690
and you know it is rather distant we set the camera rather distant and you

125
00:07:33,690 --> 00:07:35,610
know i'd like to see him a little bit more

126
00:07:35,650 --> 00:07:37,040
a little bit more close-up

127
00:07:37,040 --> 00:07:38,830
so we get more emotional

128
00:07:38,850 --> 00:07:42,580
the collection and one of the things we are very interested in microsoft is trying

129
00:07:42,580 --> 00:07:44,770
to see how teleconferencing is going to work

130
00:07:44,790 --> 00:07:49,540
because you know we you just blew your carbon budget line here for this conference

131
00:07:49,540 --> 00:07:53,230
so the next time you want to collaborate with someone really trying to home

132
00:07:53,250 --> 00:07:55,860
and you know be sitting in front of your workstation

133
00:07:55,900 --> 00:07:59,700
and this even of their workstation you're working on a paper or something that would

134
00:07:59,700 --> 00:08:00,710
be civilized

135
00:08:01,580 --> 00:08:03,130
anyway in order to see

136
00:08:03,130 --> 00:08:07,330
you're the bigger your collaborating with i recommend all these very

137
00:08:07,330 --> 00:08:11,920
you know but here it is move that the brain that's a bit messy

138
00:08:13,540 --> 00:08:15,860
so you know that's why we have to zoom out the camera in the first

139
00:08:15,860 --> 00:08:19,560
place because although would have liked more emotional contact

140
00:08:19,580 --> 00:08:23,190
you dangerous to get it so

141
00:08:23,310 --> 00:08:29,710
is something which is using stereo cameras automatically to estimate where the

142
00:08:29,730 --> 00:08:34,060
the object of interest is and the sort of log onto that target

143
00:08:34,080 --> 00:08:36,920
and here is a little bit later in the conversation

144
00:08:37,480 --> 00:08:41,480
is you know moving out of the frame

145
00:08:41,500 --> 00:08:45,190
temporarily but the camera soon catches up with that and frames him out

146
00:08:45,250 --> 00:08:51,110
sort of imitating the rules of reasonably good can work to given the picture of

147
00:08:51,360 --> 00:08:55,560
you can do this with the monocular camera probably you know you'd say well you

148
00:08:55,560 --> 00:08:59,350
should be using an SVM in the face detector but you know it doesn't always

149
00:08:59,350 --> 00:09:01,230
work i mean

150
00:09:02,750 --> 00:09:04,540
you know sometimes it doesn't work at all

151
00:09:04,560 --> 00:09:07,790
i mean and you know what i want to

152
00:09:07,860 --> 00:09:08,790
show you the

153
00:09:09,480 --> 00:09:13,610
which you i both the shop like we could well if you may not just

154
00:09:13,610 --> 00:09:14,650
the two faces

155
00:09:16,230 --> 00:09:21,770
you can try to do this monocular using motion and color and so on but

156
00:09:21,810 --> 00:09:25,920
and because that works to some extent but stereo is incredibly reliable and robust really

157
00:09:25,920 --> 00:09:29,670
is the method of choice as i say there is no real reason not to

158
00:09:29,670 --> 00:09:30,850
use it

159
00:09:33,130 --> 00:09:35,610
actually what we could do it sold before

160
00:09:35,650 --> 00:09:37,000
stereo problem

161
00:09:37,020 --> 00:09:38,250
and i'm just going to

162
00:09:38,480 --> 00:09:41,810
keep going for a few minutes then you get your natural break

163
00:09:42,460 --> 00:09:46,860
so you know why not go directly to the jugular and ask the right question

164
00:09:46,860 --> 00:09:50,850
you this is a good principle in information processing is sorry you know make this

165
00:09:50,850 --> 00:09:57,110
principle in many forms yourself that you know the curse of modularity i called my

166
00:09:57,110 --> 00:10:01,400
friend ollie williams called it that you know why define intermediate representations if you don't

167
00:10:01,400 --> 00:10:06,020
need to say exactly what it was when computing computer computing variables whose values he

168
00:10:06,020 --> 00:10:10,210
didn't want know so we ask the question OK tell me what is in the

169
00:10:10,210 --> 00:10:12,230
foreground and what is in the background so

170
00:10:12,940 --> 00:10:14,920
to do this we take the

171
00:10:14,940 --> 00:10:20,170
here's the stereo problems we last saw what the one in shape the dynamic programming

172
00:10:20,210 --> 00:10:23,250
and we repeat the likelihood term out of that

173
00:10:23,290 --> 00:10:24,980
is the likelihood term

174
00:10:26,650 --> 00:10:29,000
actually the purpose of convenience

175
00:10:29,040 --> 00:10:33,360
if you remember i said that xk in this formulation the problem was the coordinate

176
00:10:34,540 --> 00:10:36,650
two world let's say

177
00:10:36,650 --> 00:10:38,690
equivalently i could make it

178
00:10:38,710 --> 00:10:41,060
according to to repair the corner to

179
00:10:41,060 --> 00:10:43,040
coupled with the disparity

180
00:10:44,190 --> 00:10:47,940
so i'm going to propose that representations of thinking and then as i'm going to

181
00:10:47,940 --> 00:10:53,770
take an and the disparity of its equivalent pretty much

182
00:10:53,790 --> 00:10:54,690
so now

183
00:10:54,710 --> 00:11:00,730
this likelihood here can be re expressed as the likelihood for a particular pixel in

184
00:11:00,730 --> 00:11:02,540
the right image

185
00:11:02,540 --> 00:11:08,060
is a disease involving the destruction of motor control and loss of motor control difficulty

186
00:11:08,070 --> 00:11:13,250
moving in one factor in parkinson's is too little of the neurotransmitter

187
00:11:13,260 --> 00:11:14,930
known as dopamine

188
00:11:14,980 --> 00:11:16,520
the drug delta

189
00:11:16,560 --> 00:11:18,610
increases the supply of dopamine

190
00:11:18,630 --> 00:11:24,430
and so does something to alleviate at least temporarily the symptoms of parkinson's

191
00:11:24,480 --> 00:11:28,040
so you have neurons and clustered together

192
00:11:28,050 --> 00:11:30,410
and if i only communicate to one another

193
00:11:30,430 --> 00:11:32,820
so how does this all works

194
00:11:32,820 --> 00:11:37,490
to give rise to creatures who could do interesting things like talk think

195
00:11:37,540 --> 00:11:41,180
well again it used to be believed that the brain is wired up like a

196
00:11:42,580 --> 00:11:45,540
like the PC or mac or something like that

197
00:11:45,590 --> 00:11:47,830
but we know this can be true

198
00:11:47,840 --> 00:11:49,250
it can be true

199
00:11:49,260 --> 00:11:53,700
because there's two ways in which the brain is better than here

200
00:11:53,720 --> 00:11:57,150
for one thing

201
00:11:57,200 --> 00:12:00,240
the brain is highly resistant to damage

202
00:12:00,250 --> 00:12:02,040
if you have a laptop

203
00:12:02,060 --> 00:12:04,560
and i persuaded open that

204
00:12:04,620 --> 00:12:05,820
for me

205
00:12:05,820 --> 00:12:07,480
and i take the players

206
00:12:07,530 --> 00:12:10,220
and just about anywhere

207
00:12:10,240 --> 00:12:12,320
your laptop will be destroyed

208
00:12:12,340 --> 00:12:14,860
but the brain is actually more resilient

209
00:12:14,870 --> 00:12:17,120
you can take a lot of brain damage

210
00:12:17,130 --> 00:12:18,100
and still

211
00:12:18,120 --> 00:12:24,470
preserve some mental functioning to some interesting sensor some sort of damage resistance built into

212
00:12:24,470 --> 00:12:25,650
the brain

213
00:12:25,660 --> 00:12:30,380
that allows different parts of the brain to take over if some parts are damaged

214
00:12:30,460 --> 00:12:32,360
a second consideration is

215
00:12:32,370 --> 00:12:34,820
the brain is extremely fast

216
00:12:34,820 --> 00:12:36,500
your computer works on

217
00:12:36,500 --> 00:12:38,300
wires electricity

218
00:12:38,350 --> 00:12:40,580
but your brain

219
00:12:40,630 --> 00:12:42,110
uses tissues

220
00:12:42,120 --> 00:12:44,830
and tissue is extremely slow

221
00:12:44,880 --> 00:12:48,870
the paradox then is how do you create such a fast computer with such low

222
00:12:50,300 --> 00:12:54,360
you can what if if if the brain was wired about the personal computer it

223
00:12:54,360 --> 00:12:57,840
would take you four hours to recognise face

224
00:12:57,860 --> 00:13:00,910
but in fact we could do things extremely quickly

225
00:13:00,930 --> 00:13:04,910
so the question then is how is the brain wired

226
00:13:04,930 --> 00:13:06,660
and the answer is

227
00:13:06,700 --> 00:13:08,740
unlike many component like

228
00:13:08,790 --> 00:13:13,040
commercially generating computers the brain works parallel

229
00:13:13,060 --> 00:13:18,070
processing massively parallel distributed processing

230
00:13:18,120 --> 00:13:21,810
there's a whole lot of research and this is research some of which takes place

231
00:13:21,930 --> 00:13:26,650
outside psychology departments in engineering department of computer science department

232
00:13:26,700 --> 00:13:29,140
trying to figure out

233
00:13:30,750 --> 00:13:34,270
a computer can do the same things brains can

234
00:13:34,290 --> 00:13:38,710
and one way people do this is they take a hint from nature

235
00:13:38,760 --> 00:13:44,140
and they try to construct a massively distributed networks to do aspects of reasoning

236
00:13:44,160 --> 00:13:45,710
is a very simple

237
00:13:45,720 --> 00:13:47,710
computational network

238
00:13:47,720 --> 00:13:52,290
that is interesting because it kind of looks to some extent like the way neurons

239
00:13:53,040 --> 00:13:56,110
and this is often known as neural networks

240
00:13:56,140 --> 00:14:01,500
and people who study this often claim to be studying neural network models

241
00:14:01,510 --> 00:14:06,660
to try to build smart machines by modeling them after the brains and in the

242
00:14:06,660 --> 00:14:08,780
last twenty years or so

243
00:14:08,790 --> 00:14:11,560
this has been used in vibrant area of study

244
00:14:11,570 --> 00:14:16,730
where people are trying to wire machines they can do brain like things from components

245
00:14:16,730 --> 00:14:20,820
that look a lot like neurons and are wired together as neurons are

246
00:14:23,770 --> 00:14:25,610
consideration all of this

247
00:14:25,630 --> 00:14:29,070
is it this is very young field and nobody knows how to do it

248
00:14:29,120 --> 00:14:31,030
is no machine yet

249
00:14:31,040 --> 00:14:33,300
that can recognise faces

250
00:14:34,180 --> 00:14:35,880
understand sentences

251
00:14:35,890 --> 00:14:38,820
at the level of the two-year-old human

252
00:14:38,820 --> 00:14:41,910
there's no machine and they can do just about anything people can do

253
00:14:41,920 --> 00:14:44,670
in an interesting way and this is in part

254
00:14:44,720 --> 00:14:49,220
because the human brain is wired up in extraordinarily more complicated way

255
00:14:49,270 --> 00:14:51,600
then any sort of simple neural network

256
00:14:51,630 --> 00:14:57,560
this is where the schematic diagram you're not responsible for this

257
00:14:57,570 --> 00:15:00,060
parts of the visual cortex

258
00:15:00,070 --> 00:15:04,720
and the thing to realize about this is is extraordinarily simplified

259
00:15:04,760 --> 00:15:08,060
so the brain is a complicated system

260
00:15:09,040 --> 00:15:10,870
so we've talked a little bit

261
00:15:10,890 --> 00:15:11,770
about it

262
00:15:11,770 --> 00:15:15,210
the basic building blocks of the brain neurons

263
00:15:15,220 --> 00:15:20,470
we then talk about how neurons can communicate to one another then turned to how

264
00:15:20,470 --> 00:15:23,840
neurons are are wired up together

265
00:15:23,860 --> 00:15:27,060
now let's talk a little bit about different parts of the brain

266
00:15:27,880 --> 00:15:31,910
there's some things you don't actually need your brain to do

267
00:15:31,960 --> 00:15:36,470
the study what you don't need your brain to do has often drawn upon this

268
00:15:36,470 --> 00:15:42,640
weird methodology where this was actually done in france alive we would decapitate people

269
00:15:42,650 --> 00:15:46,890
and when they decapitated people psychologists would rush to the body

270
00:15:47,000 --> 00:15:50,320
the headless person inserted chess tests reflexes

271
00:15:50,340 --> 00:15:53,040
and stuff like that is kind of gruesome but we know

272
00:15:53,080 --> 00:15:55,490
there are some things you don't need to reign

273
00:15:55,490 --> 00:15:58,820
you know new brain for newborns active

274
00:15:58,880 --> 00:16:03,270
lim flex asian and withdraw from paint

275
00:16:03,310 --> 00:16:06,210
you're little pull back even if you had is gone

276
00:16:06,230 --> 00:16:09,730
erection of the penis can be done about the brain

277
00:16:09,730 --> 00:16:14,410
vomiting also the photograph i and you volunteer

278
00:16:15,530 --> 00:16:19,520
very simple there's a lot of of all any of excellent any of the above

279
00:16:19,530 --> 00:16:21,560
can you stand up just

280
00:16:22,400 --> 00:16:25,970
this is an insurance i want to stay away from now is is if you

281
00:16:25,970 --> 00:16:27,560
hold your hand

282
00:16:27,570 --> 00:16:29,990
and one can flash

283
00:16:29,990 --> 00:16:31,470
sceptic long time

284
00:16:31,490 --> 00:16:35,500
but is is probably the heart that is the question so

285
00:16:35,520 --> 00:16:38,630
this is what i get if i actually take

286
00:16:40,820 --> 00:16:43,960
what are you looking for the posterior x

287
00:16:44,050 --> 00:16:46,710
seven the civil is noisy data as to have it is good have it is

288
00:16:46,710 --> 00:16:51,210
not you want to estimate the posterior distribution x

289
00:16:51,950 --> 00:16:53,320
that the question

290
00:16:56,690 --> 00:17:00,090
OK so what i was writing there was

291
00:17:00,930 --> 00:17:07,030
how how complex the the posterior distribution tracks get complex is this function you get

292
00:17:07,050 --> 00:17:10,980
if i have two data points

293
00:17:12,470 --> 00:17:14,820
based on that that's the case here

294
00:17:14,830 --> 00:17:19,420
and basically phi phi just substitute in those form the top into that

295
00:17:19,430 --> 00:17:22,820
what i find is that some weighted sum of four terms

296
00:17:23,680 --> 00:17:26,020
each of which is constant

297
00:17:26,250 --> 00:17:29,850
so that's that's how the complexity grows

298
00:17:29,870 --> 00:17:32,820
but here's the thing if you actually take a data set of one hundred points

299
00:17:32,940 --> 00:17:35,480
actually compute this posterior part of this is what you get

300
00:17:35,490 --> 00:17:37,580
suppose for

301
00:17:37,590 --> 00:17:42,310
so yes that is it is a mixture of two to one hundred gaussians

302
00:17:42,330 --> 00:17:45,000
what it ends up having a very simple

303
00:17:45,030 --> 00:17:49,700
shape effectively even though the algebraic form is very complex the actual shape is quite

304
00:17:49,700 --> 00:17:53,330
simple and this happens a lot in bayesian inference problems mainly because you have a

305
00:17:53,340 --> 00:17:57,380
sort of central limit theorem facts more data more data you have affecting certain parameter

306
00:17:57,440 --> 00:18:01,370
essentially makes the distribution of question the gas even though each individual likely could be

307
00:18:01,370 --> 00:18:03,610
very nongaussian

308
00:18:03,630 --> 00:18:08,450
so this gives us some hope that probably we could do some sort of approximation

309
00:18:08,460 --> 00:18:14,520
and get a good representation of the posterior very simple compact representation the this posterior

310
00:18:14,580 --> 00:18:19,410
and compute with that thing going to do the one hundred mixture of gaussians

311
00:18:22,080 --> 00:18:25,110
so i'm going to talk about is how can we be computed represent the posterior

312
00:18:25,110 --> 00:18:28,700
distribution compactly OK so it this point is sort of a fork in the road

313
00:18:28,720 --> 00:18:32,950
what type of approximate inference technique you want to use now you've just seen a

314
00:18:32,950 --> 00:18:37,030
whole bunch of lectures on sampling and something is certainly one way you could approach

315
00:18:37,030 --> 00:18:37,970
this problem

316
00:18:37,990 --> 00:18:42,990
i would say that something is particularly good for cases where the posterior distribution has

317
00:18:42,990 --> 00:18:47,950
a very complicated shape that's not something you could easily summarize with some sort of

318
00:18:47,950 --> 00:18:49,450
analytic form

319
00:18:49,460 --> 00:18:52,790
so for example if you have a shape on the left which has lots of

320
00:18:52,790 --> 00:18:53,880
modes in it

321
00:18:54,700 --> 00:18:58,440
using for example a particle filter a bunch of samples to represent the distribution would

322
00:18:58,440 --> 00:18:59,400
be a good idea

323
00:18:59,450 --> 00:19:02,700
i know and if you have a nice smooth posterior distribution like the one i

324
00:19:02,710 --> 00:19:06,420
showed you you can actually do something much cheaper and much simpler which is just

325
00:19:06,420 --> 00:19:10,400
to approximate it with some parametric family such as calcium

326
00:19:13,620 --> 00:19:17,160
you have the trade-off here so the nice thing about sampling is that

327
00:19:17,180 --> 00:19:21,540
is that it allows you to wait

328
00:19:22,070 --> 00:19:26,070
OK is that use enough samples you going to get a good representation of the

329
00:19:26,070 --> 00:19:29,230
posterior even if you don't know what it looks like it's long enough samples to

330
00:19:29,230 --> 00:19:35,210
get something that captures the most the posterior is on and sample that letters

331
00:19:35,220 --> 00:19:39,480
the thing about deterministic approximations is that you don't really know in advance how accurate

332
00:19:39,480 --> 00:19:43,430
is going to be honest you actually examine their posterior distribution see that doesn't match

333
00:19:43,450 --> 00:19:47,220
the former fitting to see how does it is adjusted if you don't do that

334
00:19:47,220 --> 00:19:51,170
you will really know how accurate results are so that's so sort of this unpredictability

335
00:19:52,070 --> 00:19:55,660
the other hand it is very fast

336
00:19:56,940 --> 00:20:01,190
within deterministic approximations there's still a whole bunch of things you could do so one

337
00:20:01,190 --> 00:20:04,520
of the original methods people use was called method

338
00:20:04,720 --> 00:20:09,970
and what happens there is you essentially find the mode of the posterior distribution finding

339
00:20:10,000 --> 00:20:13,650
the derivatives that mode and you choose the gas which is the same mode in

340
00:20:13,650 --> 00:20:16,070
the same there's your posterior

341
00:20:16,080 --> 00:20:21,100
so essentially you're features the girls in which matches are posterior one point very well

342
00:20:21,160 --> 00:20:24,940
and this has been used for various problems sometimes it's very effective so applied for

343
00:20:24,940 --> 00:20:28,740
example to do bayesian PCA where it works very well for the problem

344
00:20:28,750 --> 00:20:31,250
another method you can use

345
00:20:31,310 --> 00:20:33,250
it's called variational bounds

346
00:20:33,300 --> 00:20:35,660
so in this case we'll be finding

347
00:20:35,670 --> 00:20:37,560
i can see in that

348
00:20:37,580 --> 00:20:41,450
doesn't just imagine one point but it is the sort of the lower bound to

349
00:20:41,450 --> 00:20:43,240
my poster distribution

350
00:20:43,270 --> 00:20:48,310
and one of the reasons that people like using lower bounds is you get certain

351
00:20:48,310 --> 00:20:53,670
guarantees about approximation you get a nice convergence properties of the island and so on

352
00:20:53,680 --> 00:20:57,820
but today i'm not for either of those and the third approach

353
00:20:57,830 --> 00:21:00,170
which is called moment matching

354
00:21:00,180 --> 00:21:03,200
and i'll talk about racial bounds tomorrow

355
00:21:03,210 --> 00:21:05,770
today i'm going to focus on this one and the reason i like this one

356
00:21:05,770 --> 00:21:10,580
is because for certain problems where you're posterior distribution does not share assumptions very well

357
00:21:10,740 --> 00:21:12,710
and get very good accuracy with this method

358
00:21:12,720 --> 00:21:18,360
and i'm going to be talking about the medical expectation propagation which is basically a

359
00:21:18,360 --> 00:21:22,400
method for finding a gaussian approximation to posterior

360
00:21:22,420 --> 00:21:27,740
which was assert with essentially a combination rose the combination of two previous albums one

361
00:21:27,740 --> 00:21:31,400
is called assumed density filtering which came out of the common filtering literature

362
00:21:31,450 --> 00:21:35,650
and the belief propagation which came out of the machine learning literature essentially it's it

363
00:21:35,660 --> 00:21:37,470
subsumes both of them

364
00:21:38,650 --> 00:21:43,700
so since the both special cases of the elements two but a OK so today

365
00:21:43,700 --> 00:21:47,660
what we focus on more matching tomorrow talk about variational

366
00:21:48,490 --> 00:21:49,770
OK so

367
00:21:49,780 --> 00:21:53,220
first of all let's come back to our problem is there any hope of finding

368
00:21:53,220 --> 00:21:57,210
a gas and the posterior well what i'm i'm showing you i'm showing again that

369
00:21:57,210 --> 00:22:02,520
same exact posterior distribution x and i'm showing the best gas fact and by gas

370
00:22:02,520 --> 00:22:05,230
and i just mean the one that has the same moment the same mean and

371
00:22:06,020 --> 00:22:09,240
as the posterior distribution and we see that the reason because there was not a

372
00:22:09,240 --> 00:22:13,840
perfect fit and that's because our posterior wasn't exactly as it was it has heavier

373
00:22:13,840 --> 00:22:15,710
tails and calcium

374
00:22:15,730 --> 00:22:19,630
as a result i think perfect but it does a reasonable job OK so so

375
00:22:19,630 --> 00:22:24,440
essentially our target is going to be define that purple curve as fast as possible

376
00:22:24,470 --> 00:22:27,900
and the and the area between the purple curve and the green curve will never

377
00:22:27,900 --> 00:22:31,090
be able to recover using gas approximation

378
00:22:33,550 --> 00:22:35,250
what is the basic strategy

379
00:22:35,260 --> 00:22:38,300
so recall that we had this factor graph

380
00:22:38,320 --> 00:22:39,720
four x

381
00:22:39,770 --> 00:22:44,160
which had among defectors get some wise

382
00:22:44,180 --> 00:22:50,140
and the reason that the posterior distribution tracks

383
00:22:50,170 --> 00:22:51,980
wasn't was because

384
00:22:51,980 --> 00:22:55,080
some of the factors in this moment because i think there were some of the

385
00:22:55,080 --> 00:22:59,520
gaussians well what if we were so lucky as all the factors in our model

386
00:22:59,540 --> 00:23:03,580
augustine more than we would end up with the gas in fairfax that we want

387
00:23:03,630 --> 00:23:05,490
so one way of getting that

388
00:23:05,560 --> 00:23:09,580
girls in approximate gaussian posterior on x is to take each of the factors in

389
00:23:09,580 --> 00:23:13,010
our model and approximate each one of them with the gaussians

390
00:23:13,030 --> 00:23:16,020
and if we approximate each one of these factors one by one of the gaussians

391
00:23:16,370 --> 00:23:22,160
will ensure that are approximate posterior because which is multiplying together we get approximate posterior

392
00:23:22,220 --> 00:23:24,020
that essentially what i'm going to do

393
00:23:24,070 --> 00:23:29,620
so works in a very different way than than for example the past the plasma

394
00:23:29,740 --> 00:23:33,500
deform the whole posterior defines mode and so on going to do it in very

395
00:23:33,500 --> 00:23:37,130
distributed way i'm going to take each factor one of the time approximated and eventually

396
00:23:37,130 --> 00:23:39,290
but not together

397
00:23:39,340 --> 00:23:40,620
OK so

398
00:23:40,630 --> 00:23:43,430
mathematically i'm going to take that some weighted sum of gaussians and i'm going to

399
00:23:44,890 --> 00:23:48,970
and that guess in some free proud going to call my in and those the

400
00:23:48,970 --> 00:23:51,350
numbers that and you find my out

401
00:23:54,820 --> 00:23:58,320
at first this might sound a bit fishy so what i'm going to do take

402
00:23:58,320 --> 00:24:02,010
something which is the sum of the gaussians roughly one thousand how can i possibly

403
00:24:02,010 --> 00:24:07,180
is exactly what our eyes are doing we're trying to study face

404
00:24:07,200 --> 00:24:08,300
how many people

405
00:24:08,320 --> 00:24:11,110
i have seen this study before

406
00:24:11,120 --> 00:24:14,570
feel OK so here's the goal is fixed as the a video let's see if

407
00:24:14,570 --> 00:24:15,410
i can get to work so

408
00:24:15,880 --> 00:24:20,520
what i want to do with the two teams shares there's team with white shirts

409
00:24:20,530 --> 00:24:22,590
team with black shirts

410
00:24:22,610 --> 00:24:26,660
and the team both teams are going be passing a ball back and forth to

411
00:24:26,660 --> 00:24:30,490
each other and the goal here is to you're supposed to pay attention to the

412
00:24:30,490 --> 00:24:35,750
people in white the white team and count how many times the balls passed through

413
00:24:35,750 --> 00:24:37,220
this video sequence

414
00:24:37,310 --> 00:24:40,190
so that's what you're supposed to do to try to do that we'll see how

415
00:24:40,190 --> 00:24:41,570
this goes

416
00:24:41,630 --> 00:24:46,900
it's the thing

417
00:25:00,630 --> 00:25:14,260
OK so how many times

418
00:25:14,300 --> 00:25:18,300
four teams OK and how many people saw the dancing girl

419
00:25:18,320 --> 00:25:22,160
a lot of people i heard laughing but the maybe it's just a matter of

420
00:25:22,160 --> 00:25:26,620
how far you are from the screen and so forth but what the study was

421
00:25:26,620 --> 00:25:31,160
as they had subjects washes video and told them to count the number of times

422
00:25:31,230 --> 00:25:38,460
the white team passes the ball and a fairly significant portion of the subjects miss

423
00:25:38,460 --> 00:25:43,220
the dancing girl altogether and this is an example of how

424
00:25:43,240 --> 00:25:45,050
active learning is

425
00:25:45,060 --> 00:25:47,400
a blessing and a curse in the way

426
00:25:47,420 --> 00:25:50,720
you can if you're trying to figure out how many times suppose passed you might

427
00:25:50,720 --> 00:25:56,030
miss something that is arguably very salient and an important about this video sequence namely

428
00:25:56,030 --> 00:25:59,820
the mountain gorilla and so that's a great example of how

429
00:25:59,830 --> 00:26:04,070
once you start decide to focus once you try to be active in very selective

430
00:26:04,070 --> 00:26:07,800
and how you collect information you can overlook things and this is the greedy or

431
00:26:07,800 --> 00:26:11,160
myopic nature of active learning in action

432
00:26:13,070 --> 00:26:15,360
what's another reason that

433
00:26:15,380 --> 00:26:20,060
i'm interested in active learning understanding complex systems

434
00:26:21,100 --> 00:26:22,490
in problems

435
00:26:22,500 --> 00:26:27,610
for example studying the internet or other social networks and things like that often it's

436
00:26:27,610 --> 00:26:30,160
very expensive and difficult to get

437
00:26:30,170 --> 00:26:34,990
lots and lots of labeled ground truth data and so

438
00:26:35,000 --> 00:26:37,840
we apply to for example in the internet mapping

439
00:26:37,860 --> 00:26:43,370
context be able to selectively decide where to probe the internet which routes to make

440
00:26:43,370 --> 00:26:48,120
in order to help resolve our understanding of the structure topology of the internet

441
00:26:48,260 --> 00:26:54,520
another great example is this neon network which is the national ecological of observation network

442
00:26:54,770 --> 00:27:00,680
this consists of you who knows how many different sensing systems wireless sensor networks aerial

443
00:27:00,680 --> 00:27:06,340
sensing satellite sensors on and so forth and you could say well it's just put

444
00:27:06,340 --> 00:27:08,380
all this information

445
00:27:08,400 --> 00:27:11,650
and store it somewhere and then analyse somewhere and so on and so forth and

446
00:27:11,650 --> 00:27:16,630
that really becomes an impossible task what would like to do something a bit more

447
00:27:16,630 --> 00:27:21,900
selective go out and collect information in some selective fashion

448
00:27:21,920 --> 00:27:28,290
in order to learn something about the environment and ecology and a more efficient manner

449
00:27:28,300 --> 00:27:33,850
so this is kind of like learning by queries wireless sensor networks where we're which

450
00:27:33,850 --> 00:27:37,350
sensors should you query so this is an oil spill maybe would like to try

451
00:27:37,350 --> 00:27:44,430
to determine its extent seaward selectively sensors in order to figure that out you can

452
00:27:44,520 --> 00:27:48,310
as i mentioned network monitoring and also

453
00:27:48,330 --> 00:27:53,070
and trying to discover it apologies clusters and so forth and social networks and other

454
00:27:53,070 --> 00:27:58,150
networks active sensing an active data collection can be very effective

455
00:27:58,200 --> 00:28:02,360
the basic idea is when where and how to collect information

456
00:28:02,380 --> 00:28:03,740
so and then

457
00:28:03,760 --> 00:28:04,910
it may be that the

458
00:28:04,970 --> 00:28:09,790
grandest through the motivation of all is to automate science and this is something that

459
00:28:09,790 --> 00:28:13,820
people are seriously interested in and we can just walk through it so the ideas

460
00:28:13,840 --> 00:28:18,110
how to science work today we we have a scientist has background knowledge he makes

461
00:28:18,110 --> 00:28:22,800
some hypothesis he designs an experiment he collects information

462
00:28:22,810 --> 00:28:27,640
data from the experiment he analyses it and it goes back to the scientist and

463
00:28:27,650 --> 00:28:33,820
the scientists either finds that the data supports hypothesis or they there's something wrong with

464
00:28:33,820 --> 00:28:38,950
this hypothesis of the scientists up to him to go back to find that

465
00:28:39,070 --> 00:28:42,970
hypothesis come up with a new experiment and so on and so forth and so

466
00:28:42,970 --> 00:28:47,510
the the the the point here is that there's a person in the world which

467
00:28:47,510 --> 00:28:51,770
is a good thing but it's also potentially bottle and we know that there are

468
00:28:51,770 --> 00:28:57,750
lots of problems in learning in statistical inference where human intuition is quite limited especially

469
00:28:57,820 --> 00:29:00,290
given to high dimensional situations and so

470
00:29:00,330 --> 00:29:04,340
the idea of automating science is to try to

471
00:29:04,350 --> 00:29:09,260
i reduce the burden of the human in the loop

472
00:29:09,270 --> 00:29:11,320
take advantage of the fact that

473
00:29:11,340 --> 00:29:13,500
computers and machines may actually

474
00:29:13,500 --> 00:29:17,610
be better suited to finding patterns in high dimensional spaces

475
00:29:17,640 --> 00:29:22,740
and so this is this idea of autonomous experimentation automating science and there's an need

476
00:29:22,750 --> 00:29:25,400
sort of

477
00:29:26,830 --> 00:29:29,820
article reviews that was put together by

478
00:29:29,840 --> 00:29:34,840
a panel of scientists microsoft the cambridge put this together a few years was called

479
00:29:34,840 --> 00:29:37,080
toward towards twenty twenty

480
00:29:37,230 --> 00:29:42,290
science and the idea there was this whole automation process so here's how

481
00:29:42,300 --> 00:29:44,140
how why we might want to do it

482
00:29:44,170 --> 00:29:46,890
in traditional set suppose we're trying to

483
00:29:47,960 --> 00:29:53,100
OK computer based classifier to classify microarrays to do something like that what you might

484
00:29:53,100 --> 00:29:59,210
have a lot of data collected unlabelled you give it to an expert scientists that

485
00:29:59,210 --> 00:30:05,550
expert labels statuses this is cancer this is in cancer something like that and then

486
00:30:06,120 --> 00:30:10,880
passive learner produces some sort of classifiers the machine

487
00:30:11,060 --> 00:30:15,320
in active learning it will work a little bit differently what would happen if the

488
00:30:15,320 --> 00:30:16,320
data we go

489
00:30:16,340 --> 00:30:21,590
the unlabelled they will go straight to the machine the machine would start asking for

490
00:30:22,560 --> 00:30:25,000
to try to get a sense of what's going on to maybe ask for the

491
00:30:25,000 --> 00:30:29,070
but they correspond to symbols of level one level

492
00:30:30,820 --> 00:30:33,850
so what is this give you well

493
00:30:33,860 --> 00:30:37,910
so the green line here but the black dots are

494
00:30:38,110 --> 00:30:43,800
increasing rounds of just kind of flat training training with two categories four categories a

495
00:30:43,800 --> 00:30:47,690
category and so on the green around the green is kind of doing this hierarchical

496
00:30:47,690 --> 00:30:51,970
training and the first thing to note is were a lot higher than the baseline

497
00:30:52,020 --> 00:30:55,370
so we actually start in this game lower than the baseline because we started with

498
00:30:55,370 --> 00:30:56,890
an extremely simple grammars

499
00:30:56,940 --> 00:31:01,300
and we end up with a basically reasonable results it's now up to about eighty

500
00:31:01,300 --> 00:31:05,670
eight percent and just to give you a sense of the range in which people

501
00:31:05,680 --> 00:31:10,480
report can state-of-the-art parsing numbers is kind of in the high eighties to kind of

502
00:31:10,490 --> 00:31:13,270
very very low nineties range

503
00:31:13,290 --> 00:31:16,280
OK and you can see that the the kind of the

504
00:31:16,290 --> 00:31:21,060
the further you split the better it is to train things hierarchical

505
00:31:21,200 --> 00:31:24,450
so it turns out this is the problem

506
00:31:24,960 --> 00:31:28,650
because this graph doesn't keep going it doesn't look like it's asymptote right you think

507
00:31:28,650 --> 00:31:32,230
we can keep winning in one are activities all hit like three hundred percent right

508
00:31:32,450 --> 00:31:37,450
but it doesn't work as it does it doesn't work because you run out of

509
00:31:37,450 --> 00:31:43,160
kind of computational resources this gets this ramsgate big the number of of rules grows

510
00:31:43,540 --> 00:31:46,160
and grows cubicle in the number of states

511
00:31:46,180 --> 00:31:50,250
and it turns out that were really wasting alot splitting everything everywhere and maybe this

512
00:31:50,250 --> 00:31:54,290
should be obvious but it took us a while to realize this and in particular

513
00:31:54,290 --> 00:31:57,550
those a part of speech called com

514
00:31:57,820 --> 00:32:00,740
the words in the part of speech comma are

515
00:32:00,800 --> 00:32:02,080
the common

516
00:32:02,190 --> 00:32:06,450
so we split this into many many clusters each of which contains the comma

517
00:32:06,500 --> 00:32:09,450
so this is clearly a waste so we don't want to be doing this

518
00:32:09,450 --> 00:32:13,210
but you know even beyond that what if you're just putting determiners again and here

519
00:32:13,210 --> 00:32:16,430
is kind of further down the terminus poetry and if you look over there on

520
00:32:16,430 --> 00:32:21,060
the left you can see that we've got two categories that really answer different

521
00:32:21,070 --> 00:32:22,740
so are these two similar

522
00:32:22,750 --> 00:32:25,430
to keep the split or do we want to kind of stop splitting at this

523
00:32:25,430 --> 00:32:27,780
point i don't know this is why i kind of

524
00:32:27,950 --> 00:32:31,000
the computer sort this out and so i have to have some kind of test

525
00:32:31,080 --> 00:32:33,950
the test we use as we look at the likelihood with and without the split

526
00:32:33,950 --> 00:32:37,340
and if the gain in likelihood isn't enough then we reverse the split and we

527
00:32:37,340 --> 00:32:41,500
get these kind of ragged trees that only split where where it is

528
00:32:42,580 --> 00:32:46,710
so now essentially we're splitting of the categories but then we take half of half

529
00:32:46,710 --> 00:32:49,800
of the splits their least useful and we merge them back together

530
00:32:49,810 --> 00:32:50,860
what does this do

531
00:32:50,890 --> 00:32:55,730
it kind of shrinks the rate at which our grammars grow substantially and suddenly all

532
00:32:55,730 --> 00:32:59,300
of these points that we are achieving with very big grammars they'll shift over to

533
00:32:59,300 --> 00:33:04,040
the left we get similar accuracy is much smaller grammars the actresses actually got to

534
00:33:04,080 --> 00:33:09,020
and we let us do more rounds of this without blowing up the grammar size

535
00:33:09,020 --> 00:33:11,340
and now we can get a lot further so now if we do some kind

536
00:33:11,340 --> 00:33:15,190
of merging as well so that we're not allocating primers everywhere but only where they're

537
00:33:15,190 --> 00:33:19,180
needed then we gain not only in size but also in accuracy and her up

538
00:33:19,180 --> 00:33:21,320
to eighty nine point five percent

539
00:33:22,560 --> 00:33:26,950
and this is already starting to be state

540
00:33:26,970 --> 00:33:31,030
so i so we're allocating complexity where it needed

541
00:33:31,070 --> 00:33:34,580
return to this later but for now we might kind of look and see what

542
00:33:34,580 --> 00:33:38,750
kinds of what kinds of things we make complex and it turns out noun phrase

543
00:33:38,770 --> 00:33:43,070
and verb phrases prepositional phrases and other things that you've heard of are in fact

544
00:33:43,070 --> 00:33:47,360
very common and they are also very complex and we captured that by kind of

545
00:33:47,360 --> 00:33:49,410
allocating a lot of parameters to them

546
00:33:49,470 --> 00:33:52,990
but then things that you have heard about any trust me really don't know about

547
00:33:53,190 --> 00:33:57,690
like the not a category and the next symbol with the service remained very simple

548
00:33:57,690 --> 00:34:02,980
both because they are very frequent and because they're not very complex

549
00:34:03,040 --> 00:34:07,050
same thing if you look at parts of speech so nouns and adjectives these kinds

550
00:34:07,050 --> 00:34:08,640
of things are very complicated

551
00:34:08,720 --> 00:34:11,820
they get split a lot whereas are from the common

552
00:34:11,870 --> 00:34:15,400
and the part of speech to which also only contains one word don't get split

553
00:34:15,400 --> 00:34:18,420
off because there's never any like again from

554
00:34:18,420 --> 00:34:23,560
OK so we can allocate income parameters in an incremental way

555
00:34:23,610 --> 00:34:27,240
so that we learn reasonable things and we also stop when there's one this can

556
00:34:27,240 --> 00:34:29,590
no more support for the split

557
00:34:29,740 --> 00:34:32,120
all kinds of things we learned

558
00:34:32,210 --> 00:34:33,420
there are some candy

559
00:34:34,000 --> 00:34:37,510
there are some of the proper nouns so it's it remember in the treebank there's

560
00:34:37,510 --> 00:34:40,200
just one kind of proper noun but it turns out months are very different from

561
00:34:40,200 --> 00:34:42,430
people in their syntactic distribution

562
00:34:42,450 --> 00:34:44,790
the system is learned that appropriately so

563
00:34:44,810 --> 00:34:49,540
months in particular abbreviated months because they happen to be used in the stylistically particular

564
00:34:50,110 --> 00:34:54,620
get one category first name middle names last names or get their own category you

565
00:34:54,620 --> 00:34:57,970
can see the first word of a place in the second world of place are

566
00:34:57,970 --> 00:35:01,520
syntactically different distributions they get their own categories

567
00:35:01,540 --> 00:35:06,090
in terms of pronouns there's only one pronoun but it turns out that according to

568
00:35:06,090 --> 00:35:10,090
the statistics there are there are three kinds of case

569
00:35:10,160 --> 00:35:14,680
in english there's nominative case there's accusative case and there's upper case

570
00:35:14,690 --> 00:35:17,800
and the system is when this generalisation which never would have come

571
00:35:17,830 --> 00:35:20,460
from a top down kind of linguistic

572
00:35:20,480 --> 00:35:23,550
the notion where uppercase is not the case

573
00:35:24,040 --> 00:35:25,340
the joke too

574
00:35:25,600 --> 00:35:32,750
here's another split so you can see adverbs have been split i personally would have

575
00:35:32,750 --> 00:35:35,960
had trouble figuring out how to put the adverbs but it turns out that there's

576
00:35:35,960 --> 00:35:39,340
various kinds of degree adverbs in a pattern differently syntactically

577
00:35:39,380 --> 00:35:44,190
numbers there's a very big difference between numbers that mean years and numbers that means

578
00:35:44,190 --> 00:35:45,650
stock prices

579
00:35:45,700 --> 00:35:49,770
so they get different categories because they they kind of occur in different places in

580
00:35:49,770 --> 00:35:50,600
the tree

581
00:35:50,690 --> 00:35:53,450
and so on

582
00:35:53,490 --> 00:35:56,910
OK so this is a case where what we've learned

583
00:35:57,330 --> 00:35:59,910
we kind of this is an incremental learning regime

584
00:35:59,950 --> 00:36:03,990
we're starting with only the coarsest observation of the course is kind of syntax and

585
00:36:03,990 --> 00:36:08,010
then using generalized policy iteration this was the optimal policy that there is this much

586
00:36:08,010 --> 00:36:10,300
because this multicolored controller

587
00:36:10,310 --> 00:36:12,590
the club which in the end

588
00:36:12,610 --> 00:36:14,520
so i see it says

589
00:36:14,520 --> 00:36:17,600
more or less around eighteen or nineteen

590
00:36:17,630 --> 00:36:19,370
or above stick

591
00:36:19,420 --> 00:36:23,880
and below him if you use ways and then you more complex only if you

592
00:36:25,260 --> 00:36:26,610
no boys

593
00:36:26,630 --> 00:36:33,470
at the bottom

594
00:36:38,330 --> 00:36:40,190
and then i

595
00:36:40,240 --> 00:36:41,300
right OK

596
00:36:41,310 --> 00:36:44,660
so let me lately discovered something of exploration well

597
00:36:45,280 --> 00:36:47,140
briefly the more

598
00:36:48,010 --> 00:36:52,420
so what you can start in every possible state action OK you actually had like

599
00:36:52,420 --> 00:36:53,570
that table

600
00:36:54,560 --> 00:36:58,410
so then what you should do is

601
00:36:58,430 --> 00:37:02,780
you the policy

602
00:37:08,440 --> 00:37:12,290
chooses OK now you actually need to use a stochastic policy

603
00:37:12,340 --> 00:37:16,670
OK because you want to follow what you think is the best action but occasionally

604
00:37:16,670 --> 00:37:18,040
have to explore

605
00:37:18,040 --> 00:37:22,020
OK so in this case you choose the greedy best action

606
00:37:22,110 --> 00:37:26,710
with one minus epsilon classifier number of action that you have not state

607
00:37:26,860 --> 00:37:28,270
without probability

608
00:37:28,310 --> 00:37:31,060
OK for every other nine reaction

609
00:37:31,090 --> 00:37:35,610
you choose it with absolute over number action OK you can see that this would

610
00:37:35,610 --> 00:37:37,900
sum to one

611
00:37:39,230 --> 00:37:42,770
the probability distribution OK so

612
00:37:42,810 --> 00:37:46,940
most time to the reaction with some probability choose a

613
00:37:46,980 --> 00:37:52,230
no not all actions OK and now blackjack table then you can start exploring randomly

614
00:37:52,440 --> 00:37:56,810
as long as every state can reach every other state under some action they're guaranteed

615
00:37:56,940 --> 00:37:59,230
for all states and actions

616
00:37:59,520 --> 00:38:03,860
OK so the summary

617
00:38:03,920 --> 00:38:09,170
OK so this is a very that their images of monte carlo methods over

618
00:38:09,360 --> 00:38:12,000
dynamic programming solutions that we saw earlier

619
00:38:12,040 --> 00:38:13,770
our policy iteration

620
00:38:13,790 --> 00:38:17,520
you are direct interaction with environment you don't need to know

621
00:38:20,820 --> 00:38:27,730
OK this makes your problem to specify model is supported only by model

622
00:38:27,790 --> 00:38:29,480
OK now

623
00:38:29,520 --> 00:38:33,610
what happens if you model your problem has a markovian process

624
00:38:33,610 --> 00:38:37,440
but you didn't really productive fishing years and your state actually make work markovian

625
00:38:37,460 --> 00:38:39,360
it turns out the monte carlo

626
00:38:39,420 --> 00:38:40,820
control methods are actually

627
00:38:42,230 --> 00:38:44,340
two markovian violations

628
00:38:44,380 --> 00:38:48,460
which actually means that if your model is not a good model

629
00:38:48,520 --> 00:38:52,080
and then may be better off actually settling in the world

630
00:38:58,290 --> 00:39:01,020
the other two we can more clear as i as i move on to the

631
00:39:01,020 --> 00:39:02,820
next lecture

632
00:39:02,840 --> 00:39:06,440
OK let me start into that

633
00:39:06,480 --> 00:39:07,710
so what

634
00:39:07,710 --> 00:39:10,130
we discovered monte carlo methods

635
00:39:10,170 --> 00:39:13,980
this is one extreme there we were recovered a

636
00:39:14,040 --> 00:39:19,770
OK effectively

637
00:39:19,770 --> 00:39:24,080
monte carlo methods sample

638
00:39:25,130 --> 00:39:27,540
the entire trajectory

639
00:39:27,540 --> 00:39:29,520
so can anyone think of

640
00:39:29,540 --> 00:39:33,980
so advantages as some some imagine that you could use

641
00:39:34,060 --> 00:39:36,110
then occurring methods

642
00:39:38,860 --> 00:39:41,500
monte carlo methods

643
00:39:42,270 --> 00:39:48,000
can you think of one disadvantage of the monte carlo sampling

644
00:39:48,040 --> 00:39:58,500
approach or in general what the problem is simply

645
00:39:58,540 --> 00:40:00,610
thank you

646
00:40:01,480 --> 00:40:04,650
o memory and time course right

647
00:40:05,810 --> 00:40:10,400
so i said if the state space sorry if if

648
00:40:12,900 --> 00:40:17,670
it can often take a large number of samples to get a good

649
00:40:17,710 --> 00:40:18,880
value estimate

650
00:40:20,690 --> 00:40:24,980
unfortunately the monte carlo method you have to wait

651
00:40:24,980 --> 00:40:29,220
so you see here some kind of knowledge idea this one

652
00:40:29,230 --> 00:40:34,520
just that there is no one's again instead of being again it's a equivalent to

653
00:40:34,520 --> 00:40:36,050
one be more

654
00:40:36,070 --> 00:40:42,190
you see some some changes in the same so in this particular case

655
00:40:42,220 --> 00:40:48,580
the only thing nineteen will be given by this newfound which is the maximum between

656
00:40:50,850 --> 00:40:54,440
put on what you get

657
00:40:54,460 --> 00:41:00,040
nine thousand hundred ninety so it's again in light of the two states that the

658
00:41:00,040 --> 00:41:02,830
least significant bits of the smallest ones

659
00:41:02,850 --> 00:41:04,400
value should be

660
00:41:04,410 --> 00:41:08,250
constant of the full range but in fact it's not and you will see things

661
00:41:08,250 --> 00:41:11,020
like that so find it is this

662
00:41:11,020 --> 00:41:14,770
these beaches is a bit longer than what it should be it's

663
00:41:14,790 --> 00:41:19,590
half moon and you will get at another place you get another one which is

664
00:41:19,590 --> 00:41:23,550
smallest so you can get a very good for instance in the idea with this

665
00:41:23,550 --> 00:41:27,530
missing but then if you look at the data it you will see some

666
00:41:27,540 --> 00:41:28,860
some films

667
00:41:28,880 --> 00:41:31,820
and again the world to see this effect

668
00:41:31,830 --> 00:41:35,020
to to see the effect it in the new world

669
00:41:35,030 --> 00:41:40,140
move very slowly you are going to this in looking which is the output are

670
00:41:40,140 --> 00:41:43,210
you put some fully on the that

671
00:41:43,220 --> 00:41:48,330
like for the kids and you should get some simple blood flow completely flat and

672
00:41:48,340 --> 00:41:49,960
if it's not

673
00:41:50,130 --> 00:41:55,460
it will tell you what are the nonlinearity

674
00:41:55,470 --> 00:42:02,540
in terms of also quite often some missing code so as to give them time

675
00:42:02,630 --> 00:42:09,250
to really get code which is easy to navigate uses could because something inside

676
00:42:09,260 --> 00:42:14,410
or you can get all sorts of movement something which is not monotone which is

677
00:42:14,500 --> 00:42:18,750
you go you from the to the on the background to sense

678
00:42:18,770 --> 00:42:20,350
can you

679
00:42:20,380 --> 00:42:24,330
a very important things to keep in mind is the effective number of bits because

680
00:42:24,330 --> 00:42:28,860
when you when you get to that is you know it's an n bit ADC

681
00:42:28,920 --> 00:42:31,090
and indeed see we introduce you

682
00:42:31,110 --> 00:42:37,300
a quantization of which is assigned to be divided by twelve to to twenty

683
00:42:37,310 --> 00:42:39,260
just because

684
00:42:39,340 --> 00:42:43,310
you it is in between plus and minus

685
00:42:43,330 --> 00:42:47,680
if it is been should continue the i find

686
00:42:47,710 --> 00:42:49,880
square would of twelve

687
00:42:49,890 --> 00:42:54,060
so now if you could if you are to see that which is pure sinusoids

688
00:42:54,070 --> 00:43:00,590
with this opportunity with a being the full of a full-scale

689
00:43:00,710 --> 00:43:04,760
then you will get to know i end

690
00:43:04,770 --> 00:43:06,560
which is given by

691
00:43:06,570 --> 00:43:07,860
but like this

692
00:43:07,890 --> 00:43:09,400
formula which is

693
00:43:10,010 --> 00:43:13,330
the islands and of wonderful

694
00:43:15,400 --> 00:43:20,340
and if you do find similar to the i shall always be with that you

695
00:43:20,360 --> 00:43:24,360
in indeed as then load this

696
00:43:24,430 --> 00:43:32,150
the value of this thing and all of this is village

697
00:43:32,170 --> 00:43:37,250
the signal and the only way you will find something like that which is nicely

698
00:43:37,250 --> 00:43:41,770
in the ability to do so and then BBC will give you a signal to

699
00:43:41,770 --> 00:43:47,320
noise ratio which is six times the number of bits plus one point eight db

700
00:43:47,340 --> 00:43:50,070
they were and then you want

701
00:43:50,080 --> 00:43:53,060
and you find that doesn't fit and so what

702
00:43:53,070 --> 00:43:58,060
what we call the effective number of bits is what the end behind that we

703
00:43:58,060 --> 00:44:00,800
have to put in this formula so that find

704
00:44:00,960 --> 00:44:04,160
quite signal to noise ratio so i take an example

705
00:44:04,170 --> 00:44:09,590
that's the BBC from another device which is given full twenty seven learning from twenty

706
00:44:09,590 --> 00:44:16,330
to sixty five million and five if we just put inside this nice new so

707
00:44:16,340 --> 00:44:21,660
it and then you make a fast fourier transform of the output you will get

708
00:44:21,660 --> 00:44:24,980
a signal you your sincerity which is there

709
00:44:26,190 --> 00:44:30,220
no then you look at where is the flow of the knowledge and you find

710
00:44:30,220 --> 00:44:33,400
that in this case you get seventy db

711
00:44:33,420 --> 00:44:38,880
between the nineteenth of your and and to get seventy b

712
00:44:38,890 --> 00:44:43,730
here in this formula it will give you an equal to eleven point four so

713
00:44:43,740 --> 00:44:48,600
that's not the twenty BBC this at this particular frequency would the point four and

714
00:44:48,600 --> 00:44:50,660
that's a good one could be that you

715
00:44:50,680 --> 00:44:54,940
and it is used for the song forty three twenty two

716
00:44:54,960 --> 00:44:56,410
eight to fifteen

717
00:44:56,440 --> 00:45:00,340
number of bits and then that's what calls for you for your measurement this is

718
00:45:00,340 --> 00:45:04,800
something you should see in general is well documented

719
00:45:04,830 --> 00:45:06,260
that i should

720
00:45:06,310 --> 00:45:08,810
but it's always good to measure i

721
00:45:08,830 --> 00:45:14,850
so we have this was a different type of ABC's

722
00:45:14,880 --> 00:45:18,660
so this is the simplest one is is the flat simplex one instruction because it

723
00:45:18,670 --> 00:45:21,370
is just that if you want to have

724
00:45:21,580 --> 00:45:23,350
then did

725
00:45:23,370 --> 00:45:25,770
you just put two

726
00:45:25,770 --> 00:45:28,940
to the poor and minus one combined to

727
00:45:28,940 --> 00:45:35,270
but we will

728
00:45:35,280 --> 00:45:39,170
what you need to be

729
00:45:39,960 --> 00:45:44,860
you you

730
00:45:48,260 --> 00:45:50,300
would be

731
00:45:57,520 --> 00:45:58,790
this is

732
00:45:59,810 --> 00:46:06,710
general motors even use

733
00:46:14,730 --> 00:46:16,810
and then he

734
00:46:16,850 --> 00:46:22,100
he was that

735
00:46:27,290 --> 00:46:32,850
this is it's

736
00:46:37,480 --> 00:46:49,680
so just write your need

737
00:46:52,950 --> 00:46:58,040
this is this

738
00:47:04,770 --> 00:47:08,620
you know

739
00:47:12,150 --> 00:47:14,230
you know

740
00:47:21,790 --> 00:47:24,980
so there's is no

741
00:47:30,890 --> 00:47:35,230
it is used to do

742
00:47:35,230 --> 00:47:39,730
brown and this

743
00:47:41,830 --> 00:47:48,660
in vision

744
00:47:50,370 --> 00:47:51,690
you you

745
00:47:53,680 --> 00:47:55,950
it is

746
00:47:55,960 --> 00:47:59,730
he was born

747
00:48:00,660 --> 00:48:04,040
this be used

748
00:48:05,660 --> 00:48:13,480
you seem to be

749
00:48:14,890 --> 00:48:18,120
the reason why

750
00:48:19,180 --> 00:48:23,850
you know you can do it

751
00:48:39,960 --> 00:48:42,810
he was

752
00:48:48,560 --> 00:48:51,980
so what

753
00:48:52,630 --> 00:48:56,100
from the

754
00:48:57,690 --> 00:48:59,540
six that

755
00:48:59,560 --> 00:49:01,690
if you can

756
00:49:12,370 --> 00:49:16,750
two rules

757
00:49:16,960 --> 00:49:20,430
i mean

758
00:49:21,830 --> 00:49:25,040
the to

759
00:49:25,060 --> 00:49:29,180
i want to give

760
00:49:37,370 --> 00:49:43,120
well you

761
00:49:50,390 --> 00:49:52,660
you rules

762
00:49:52,690 --> 00:49:55,180
if you want to give

763
00:49:55,210 --> 00:49:57,690
you need to

764
00:50:01,680 --> 00:50:06,850
the next two years

765
00:50:08,560 --> 00:50:14,210
this is a

766
00:50:14,250 --> 00:50:16,770
roberts c

767
00:50:16,930 --> 00:50:23,710
you can do it

768
00:50:23,870 --> 00:50:26,160
in this one

769
00:50:26,460 --> 00:50:29,020
may also

770
00:50:36,480 --> 00:50:37,830
this is of

771
00:50:37,850 --> 00:50:38,640
to get

772
00:50:44,560 --> 00:50:49,230
you are do

773
00:50:57,930 --> 00:51:00,390
the ricci flow

774
00:51:00,410 --> 00:51:02,160
so far

775
00:51:07,520 --> 00:51:10,460
one of the main who is

776
00:51:10,480 --> 00:51:13,660
this year one

777
00:51:26,190 --> 00:51:28,960
so i

778
00:51:28,960 --> 00:51:33,560
then you can start the question really going for

779
00:51:34,410 --> 00:51:36,860
the all noise

780
00:51:37,100 --> 00:51:39,200
when we can

781
00:51:42,820 --> 00:51:47,570
we had market

782
00:51:48,130 --> 00:51:53,880
and one thing we can use these feature

783
00:51:53,920 --> 00:51:57,420
well at least for in the classification

784
00:51:57,510 --> 00:52:00,300
so from the party

785
00:52:00,310 --> 00:52:03,480
we might extract more features

786
00:52:03,500 --> 00:52:06,170
which means that

787
00:52:06,180 --> 00:52:09,390
we could extract for instance the park

788
00:52:09,420 --> 00:52:11,170
between two work

789
00:52:11,200 --> 00:52:12,610
in the past

790
00:52:12,620 --> 00:52:15,440
so expect to complete on

791
00:52:15,470 --> 00:52:23,380
and i mean going one which was in the water going down the far

792
00:52:23,390 --> 00:52:25,650
so you can write here

793
00:52:27,690 --> 00:52:30,920
taking off his car

794
00:52:30,930 --> 00:52:34,520
this of course is that there information

795
00:52:34,530 --> 00:52:37,840
if you use the features of the text

796
00:52:37,850 --> 00:52:40,410
it might be

797
00:52:40,420 --> 00:52:47,150
now all we only have a few of the kind of girl you come be

798
00:52:49,360 --> 00:52:52,280
usually we don't you complete parts

799
00:52:52,300 --> 00:52:58,280
but we could pull it

800
00:52:58,300 --> 00:53:01,050
this could be split out silly because

801
00:53:01,070 --> 00:53:05,380
living in a small

802
00:53:05,430 --> 00:53:08,530
in the small

803
00:53:08,570 --> 00:53:10,380
and he

804
00:53:10,400 --> 00:53:15,200
the feature in classification

805
00:53:17,180 --> 00:53:18,450
we we can use

806
00:53:19,720 --> 00:53:22,970
like semantic

807
00:53:22,990 --> 00:53:28,880
if we had like the some people use more than

808
00:53:30,360 --> 00:53:33,090
you can find me anymore

809
00:53:33,110 --> 00:53:36,110
maybe also hyperbole

810
00:53:36,130 --> 00:53:41,160
more generally a certain for which we could also in

811
00:53:42,950 --> 00:53:46,720
so for like four

812
00:53:46,780 --> 00:53:49,110
we could use HTML

813
00:53:49,110 --> 00:53:50,470
in the

814
00:53:50,490 --> 00:53:52,470
on the web

815
00:53:52,490 --> 00:53:55,550
these features

816
00:53:57,840 --> 00:54:01,050
to describe information which want to classify

817
00:54:01,070 --> 00:54:06,950
or to describe the context in which you

818
00:54:08,090 --> 00:54:08,990
two o

819
00:54:08,990 --> 00:54:13,150
a second topic it's all in that

820
00:54:14,590 --> 00:54:16,530
the concept

821
00:54:16,550 --> 00:54:18,760
in the rest of the chapter

822
00:54:18,760 --> 00:54:20,630
so what

823
00:54:21,160 --> 00:54:27,450
in information extraction well the only one in four

824
00:54:27,470 --> 00:54:33,070
because we have be a very well already quite a lot out

825
00:54:33,090 --> 00:54:38,160
pattern recognition in which all used for

826
00:54:38,220 --> 00:54:41,380
information extraction

827
00:54:41,400 --> 00:54:45,180
so what actually what we want to do is i think system

828
00:54:46,200 --> 00:54:47,450
and classified

829
00:54:53,220 --> 00:54:54,930
you you might

830
00:54:54,950 --> 00:54:56,360
i don't know

831
00:54:56,380 --> 00:54:58,840
very much about your background

832
00:54:58,860 --> 00:55:06,110
some of you might know that i'm very quickly all about that in the a

833
00:55:06,220 --> 00:55:12,820
support of supervised learning unsupervised learning and weakly supervised learning

834
00:55:12,880 --> 00:55:14,300
so what

835
00:55:14,300 --> 00:55:17,340
in the

836
00:55:17,400 --> 00:55:18,780
which may

837
00:55:18,780 --> 00:55:23,340
if we had a

838
00:55:23,360 --> 00:55:25,970
for example they are

839
00:55:25,990 --> 00:55:28,630
we can learn from

840
00:55:28,650 --> 00:55:30,800
with within the y

841
00:55:37,610 --> 00:55:41,900
and it all the information we have

842
00:55:42,110 --> 00:55:44,160
the newspaper texts

843
00:55:44,220 --> 00:55:50,490
and what would be able first me in the next four location that

844
00:55:50,510 --> 00:55:57,010
we believe the person to take be a look it then you can

845
00:55:57,070 --> 00:55:59,530
so the system act

846
00:55:59,550 --> 00:56:04,450
the classification impact so the correlation of of patterns

847
00:56:05,200 --> 00:56:05,650
after ten

848
00:56:05,660 --> 00:56:07,430
we're going to maybe

849
00:56:07,720 --> 00:56:12,650
part of of speech other information and the class y

850
00:56:12,720 --> 00:56:17,780
we can learn from the training set

851
00:56:20,400 --> 00:56:26,030
the classification function for prediction or classification

852
00:56:26,030 --> 00:56:30,920
and then you can apply to move on and that's that

853
00:56:30,930 --> 00:56:35,010
you encounter a new sentence putting new that

854
00:56:35,070 --> 00:56:38,630
in which you will recognise the

855
00:56:38,660 --> 00:56:40,610
he has

856
00:56:40,630 --> 00:56:42,880
so that name

857
00:56:42,900 --> 00:56:44,400
i might not have been

858
00:56:44,400 --> 00:56:49,180
in your training that it might be a completely new names because the names of

859
00:56:49,180 --> 00:56:51,550
the first names of companies

860
00:56:51,550 --> 00:56:56,900
which is why we can can become and can be there are many new

861
00:56:56,930 --> 00:57:01,950
but the context if you take into account the context of the feature

862
00:57:01,970 --> 00:57:03,070
so we

863
00:57:03,090 --> 00:57:06,840
well the other words in the name for

864
00:57:06,860 --> 00:57:08,950
then you might

865
00:57:08,970 --> 00:57:12,720
these for a new example apply

866
00:57:14,800 --> 00:57:17,050
this is the class

867
00:57:17,090 --> 00:57:18,950
o point five

868
00:57:18,970 --> 00:57:21,820
learning so you have unsupervised

869
00:57:21,840 --> 00:57:26,930
who had already labelled training

870
00:57:27,780 --> 00:57:32,280
information extraction and text classification in general

871
00:57:32,590 --> 00:57:36,650
we can the problem two class learning problem

872
00:57:36,780 --> 00:57:38,220
so you

873
00:57:38,240 --> 00:57:45,260
you classified information objects certain words for instance

874
00:57:45,300 --> 00:57:49,240
belongs to a particular class or not

875
00:57:49,260 --> 00:57:51,530
o or one

876
00:57:51,550 --> 00:57:53,780
and the problem

877
00:57:56,530 --> 00:57:58,780
we can learn from

878
00:57:58,840 --> 00:58:01,840
but you might then

879
00:58:01,860 --> 00:58:07,050
and the world belongs to class

880
00:58:07,110 --> 00:58:09,680
you know we have

881
00:58:09,700 --> 00:58:11,610
we compute the probability

882
00:58:11,630 --> 00:58:14,090
of course not to class

883
00:58:16,150 --> 00:58:17,630
we will

884
00:58:17,630 --> 00:58:25,100
it was being closed in its own context because in greek mythology people have not

885
00:58:25,100 --> 00:58:28,420
just one i are they have some blue hat and so on and this is

886
00:58:28,420 --> 00:58:34,630
completely valid in greek mythology context while in the rest of this normal common sense

887
00:58:34,630 --> 00:58:37,290
it's not so that's why you need this

888
00:58:37,310 --> 00:58:38,410
concept of

889
00:58:38,420 --> 00:58:45,110
context is one simple example which actually but you need the smallest on every step

890
00:58:49,910 --> 00:58:52,740
on top you have the concept think

891
00:58:52,800 --> 00:58:57,930
and every everything else is in turn inherited from this think let's say here we

892
00:58:57,930 --> 00:59:02,980
have human beings is all mean that we can encode it

893
00:59:02,990 --> 00:59:06,810
different aspects of human life

894
00:59:07,440 --> 00:59:11,790
what's the structure of this ontology and so on the upper part we have this

895
00:59:11,790 --> 00:59:19,180
upper ontology which are very abstract concepts about the life of me to make the

896
00:59:19,180 --> 00:59:27,230
concepts and relationships between them below this we have courtier is which already know about

897
00:59:27,230 --> 00:59:32,360
the space time and causality and so on then here we have

898
00:59:32,370 --> 00:59:36,790
specific domains and on the bottom you effects which actually have

899
00:59:36,850 --> 00:59:39,840
now what would be an example

900
00:59:40,920 --> 00:59:45,670
of upper ontology would be that i think individual temporal think you've and so these

901
00:59:45,670 --> 00:59:49,060
are really abstract things which

902
00:59:49,230 --> 00:59:55,320
need to be represented courtier would be something like this for all events a and

903
00:59:55,320 --> 00:59:58,240
b if a causes b

904
00:59:58,250 --> 01:00:02,300
this implies a precedes b we all know this but

905
01:00:02,300 --> 01:00:07,480
despite being written somewhere and this is what is written in this first order logic

906
01:00:07,480 --> 01:00:11,060
in on this level domain specific theory would be

907
01:00:11,090 --> 01:00:14,860
some like this for any mammal m and any

908
01:00:14,920 --> 01:00:17,040
anthrax bacteria a

909
01:00:17,050 --> 01:00:21,750
if m is being exposed to a then this causes m to be infected by

910
01:00:21,750 --> 01:00:26,110
a a and is that you can also apply this rule then we know already

911
01:00:26,110 --> 01:00:27,160
that bit

912
01:00:27,190 --> 01:00:29,310
more that's

913
01:00:29,320 --> 01:00:35,360
this exposure must have happened before because of the rule of law

914
01:00:36,170 --> 01:00:37,100
and effects

915
01:00:37,170 --> 01:00:43,610
this is an example affected by the following john is a person infected by anthrax

916
01:00:43,650 --> 01:00:46,940
now since

917
01:00:47,040 --> 01:00:50,740
you know in the knowledge base the john is a person and first person we

918
01:00:51,320 --> 01:00:54,040
a lot of this kind of animal and human like the

919
01:00:54,740 --> 01:00:59,780
type of animal and so on you know a lot about what the person is

920
01:00:59,780 --> 01:01:02,610
you know what is the fact that we know what is and and we can

921
01:01:02,610 --> 01:01:05,740
infer out of this a lot of new facts this is

922
01:01:05,750 --> 01:01:08,170
the point of this

923
01:01:09,050 --> 01:01:12,700
a short already disliked how certain claims

924
01:01:12,740 --> 01:01:18,620
sentences that from news articles or somewhere else are translated into first order logic so

925
01:01:18,620 --> 01:01:21,330
all the examples are from this there is the main because

926
01:01:22,790 --> 01:01:24,730
the company had

927
01:01:24,750 --> 01:01:27,060
projects from this area

928
01:01:27,080 --> 01:01:32,650
so these are the kind of rules and these are the actual effects which

929
01:01:32,660 --> 01:01:38,270
you can extract by let's say this text mining techniques which i was showing before

930
01:01:38,660 --> 01:01:39,550
from the

931
01:01:39,560 --> 01:01:41,700
text documents and

932
01:01:41,720 --> 01:01:45,180
so this is located is based in afghanistan so this is encoded in this simple

933
01:01:45,180 --> 01:01:49,890
fact which blocked into this such knowledge

934
01:01:49,910 --> 01:01:51,670
this is an example how

935
01:01:51,680 --> 01:01:53,150
looks like

936
01:01:53,160 --> 01:01:56,650
concept cycle psychoanalyst

937
01:01:56,670 --> 01:02:03,510
so just to show you how the knowledge can be structured so psychoanalysts

938
01:02:03,550 --> 01:02:06,120
a couple of lexical representations

939
01:02:06,140 --> 01:02:13,590
and his position of medical care professionals position of health professionals professional does professional this

940
01:02:13,650 --> 01:02:14,940
one line of

941
01:02:15,000 --> 01:02:22,980
sports innovations the psychologist scientist researcher person medical patient person homo sapiens logical spaces so

942
01:02:23,010 --> 01:02:29,160
we can come from this biological space down to psychoanalyst and back necessary and reasoning

943
01:02:29,160 --> 01:02:32,160
engines is using this so

944
01:02:32,180 --> 01:02:33,540
we know for a person

945
01:02:33,550 --> 01:02:38,950
a lot of things and since psychoanalyst just personalisation of the person

946
01:02:38,970 --> 01:02:43,820
then you know that this holds also for cycle that but this is just taxonomic

947
01:02:43,820 --> 01:02:51,230
relationship of what we have fifteen thousand relationships more in knowledge base

948
01:02:51,260 --> 01:02:54,950
here is just one nice example how

949
01:02:54,960 --> 01:03:01,430
reaches vocabulary in this cyc knowledge base see here this is vocabulary of the relationship

950
01:03:03,080 --> 01:03:05,120
if something is in its

951
01:03:05,130 --> 01:03:06,450
inside something else

952
01:03:06,460 --> 01:03:08,830
so this is relationship for in

953
01:03:08,850 --> 01:03:15,930
among so this is inside well in such a way that is in contain completely

954
01:03:15,930 --> 01:03:21,850
this if this object is contained completely in something else or is in contain partially

955
01:03:22,530 --> 01:03:27,400
because it's not contain completely version in contain closed

956
01:03:28,530 --> 01:03:31,040
it's close all

957
01:03:31,080 --> 01:03:35,830
in the spring and go in content open so it can fall out then they

958
01:03:35,830 --> 01:03:38,580
would use this so this is just

959
01:03:38,630 --> 01:03:41,250
this simple relationship in

960
01:03:41,260 --> 01:03:42,330
would have such

961
01:03:42,340 --> 01:03:44,660
so many specializations

962
01:03:44,680 --> 01:03:51,420
but a further on connectedtoinside would have another one sticksinto screwedin and so on so

963
01:03:51,420 --> 01:03:52,300
if they

964
01:03:52,300 --> 01:03:59,860
try to describe describe one situation in mechanical physical world and they would use this

965
01:03:59,860 --> 01:04:03,800
kind of vocabulary to describe it in proper way

966
01:04:03,800 --> 01:04:08,710
and so this is just one last example

967
01:04:08,780 --> 01:04:15,220
the whole cycle work so this is six from tents called cyc analytic environment here

968
01:04:15,450 --> 01:04:19,260
i don't know if you can see so here's the question which you can type

969
01:04:19,260 --> 01:04:24,790
being in a natural language question is the following never maybe two years ago it

970
01:04:24,800 --> 01:04:29,100
was this that because the prime minister of lebannon got killed

971
01:04:29,120 --> 01:04:30,630
and the question was

972
01:04:30,640 --> 01:04:34,250
who has a motive for the dissemination of in africa

973
01:04:34,270 --> 01:04:36,130
two non-trivial question

974
01:04:38,350 --> 01:04:40,520
with that being the question

975
01:04:40,530 --> 01:04:45,420
the cyc translate this into first order theory then you can correct this very little

976
01:04:45,420 --> 01:04:53,720
bit more or edit for but most likely to get translated into proper form automatically

977
01:04:56,920 --> 01:04:57,960
and the press

978
01:04:59,890 --> 01:05:06,200
but in here and then we get the list of answers

979
01:05:07,510 --> 01:05:12,540
let's start to the UK the united states israel so this would be candidates

980
01:05:12,550 --> 01:05:16,230
and then we show here is you

981
01:05:16,250 --> 01:05:17,430
the name

982
01:05:17,460 --> 01:05:19,710
speculation speculation level so

983
01:05:19,730 --> 01:05:20,950
how much

984
01:05:23,840 --> 01:05:31,880
how difficult are what is the type of reasoning which site performs resnick engine for

985
01:05:31,900 --> 01:05:34,510
performance on the other side it and

986
01:05:34,530 --> 01:05:38,420
can be speculation well my mind mildly

987
01:05:38,430 --> 01:05:42,330
mainly of moderates and these are sources

988
01:05:42,350 --> 01:05:47,480
from the news stories which are encoded in the cyc knowledge base which support this

989
01:05:47,480 --> 01:05:49,780
particular place now if e

990
01:05:52,850 --> 01:05:58,470
but in further on then we would get justification for such a claim

991
01:05:58,500 --> 01:06:04,300
it's a question here would be the following so why would have a motive to

992
01:06:04,300 --> 01:06:05,460
they are perfect

993
01:06:07,310 --> 01:06:10,010
but then we sort of in between e to close

994
01:06:10,020 --> 01:06:12,170
plus four new two the plus one

995
01:06:12,250 --> 01:06:14,990
they had this very funny shape

996
01:06:15,070 --> 01:06:17,050
reminds we

997
01:06:17,060 --> 01:06:20,250
the shape of this blue a little bit

998
01:06:20,270 --> 01:06:22,070
sort of like this

999
01:06:22,180 --> 01:06:24,070
o think

1000
01:06:24,230 --> 01:06:26,220
and there's one surface

1001
01:06:26,230 --> 01:06:28,310
which is most unusual

1002
01:06:28,390 --> 01:06:30,260
people potential surface

1003
01:06:30,270 --> 01:06:33,680
which here

1004
01:06:33,740 --> 01:06:37,590
has the point where the electric field is zero sort of like

1005
01:06:37,890 --> 01:06:42,370
twisting the neck of course you get something like this

1006
01:06:42,380 --> 01:06:44,780
so you have here surface

1007
01:06:44,780 --> 01:06:48,000
which has a point here is exactly at that point

1008
01:06:48,050 --> 01:06:50,750
the electric field is zero

1009
01:06:50,800 --> 01:06:52,120
that does not mean

1010
01:06:52,160 --> 01:06:54,730
that the potential is zero of course not

1011
01:06:54,770 --> 01:06:56,610
potential is positive here

1012
01:06:56,630 --> 01:06:57,800
if you come

1013
01:06:57,800 --> 01:07:01,170
with the positive charge from the lobby seventy you have to march up to that

1014
01:07:01,170 --> 01:07:06,030
point you have to do positive work you have to overcome both the repelling force

1015
01:07:06,030 --> 01:07:09,490
from the plus four and the repelling force from the plus one

1016
01:07:09,580 --> 01:07:13,470
but finally when you reach that point you can rest

1017
01:07:13,470 --> 01:07:17,030
because is no force on you at that point that's what it means that the

1018
01:07:17,030 --> 01:07:21,300
electric field is zero it does not mean that you haven't done any work

1019
01:07:21,410 --> 01:07:23,580
never confuse electric fields

1020
01:07:26,890 --> 01:07:29,310
i want to draw your attention to the fact

1021
01:07:29,310 --> 01:07:34,810
that the green lines the field lines are everywhere perpendicular to the people tentials i'll

1022
01:07:34,810 --> 01:07:38,260
get back to that during my next lectures that is not an accident that is

1023
01:07:39,310 --> 01:07:41,270
the case

1024
01:07:42,720 --> 01:07:43,650
next well

1025
01:07:43,730 --> 01:07:47,420
shows you something that is a little bit more complicated

1026
01:07:49,240 --> 01:07:51,010
calculated for us

1027
01:07:51,050 --> 01:07:55,700
the equipotential surface into the red ones and surfaces again you have to rotate them

1028
01:07:55,700 --> 01:07:56,900
about the vertical

1029
01:07:56,920 --> 01:07:58,860
to make it three-dimensional

1030
01:07:58,870 --> 01:08:01,470
now we have a minus one charge

1031
01:08:01,470 --> 01:08:04,090
on the plus four

1032
01:08:04,110 --> 01:08:08,550
so whenever it is read the surface the potential is positive

1033
01:08:08,590 --> 01:08:10,730
whenever i have drawn in blue

1034
01:08:10,770 --> 01:08:12,020
the potential

1035
01:08:15,330 --> 01:08:18,990
first if we were very far away from both the plus four and the minus

1036
01:08:20,110 --> 01:08:21,140
you expect

1037
01:08:21,150 --> 01:08:24,560
to be looking at the charge which is effectively plus three

1038
01:08:24,560 --> 01:08:29,370
so if you go very far away for sure the potential is everywhere positive and

1039
01:08:29,370 --> 01:08:31,910
you expect it to be spherical

1040
01:08:31,930 --> 01:08:33,370
if you look here

1041
01:08:33,420 --> 01:08:37,820
very far away from the plus four the minus one indeed this has already

1042
01:08:37,930 --> 01:08:38,920
the shape

1043
01:08:38,970 --> 01:08:40,020
of a

1044
01:08:42,240 --> 01:08:46,030
so that's clear that the plus for the minus one far away behave like plus

1045
01:08:48,570 --> 01:08:50,990
if you very close to the plus four

1046
01:08:50,990 --> 01:08:55,060
you get nice spheres around the plus four

1047
01:08:56,200 --> 01:08:59,070
potential if very close to the minus one

1048
01:08:59,110 --> 01:09:03,020
notice that the blue surfaces so almost my spheres

1049
01:09:03,020 --> 01:09:03,970
but now

1050
01:09:03,990 --> 01:09:05,300
all negative

1051
01:09:05,350 --> 01:09:07,530
very close to the minus one

1052
01:09:07,570 --> 01:09:11,290
negative potential

1053
01:09:11,390 --> 01:09:12,300
there is

1054
01:09:12,300 --> 01:09:14,590
he won surface

1055
01:09:14,680 --> 01:09:16,540
we now has zero potential

1056
01:09:16,600 --> 01:09:17,620
has to be

1057
01:09:17,660 --> 01:09:18,620
because if you

1058
01:09:18,630 --> 01:09:21,260
the negative potential closer to minus one

1059
01:09:21,260 --> 01:09:23,860
and you have positive potential very far out

1060
01:09:23,910 --> 01:09:26,290
you got to go through service words here

1061
01:09:26,290 --> 01:09:29,460
so does he was surface i still have put it in blue

1062
01:09:29,510 --> 01:09:31,720
which is actually everywhere on the surface

1063
01:09:31,730 --> 01:09:32,910
the potential

1064
01:09:32,970 --> 01:09:34,310
is zero

1065
01:09:34,310 --> 01:09:36,400
is the electric field zero order

1066
01:09:36,410 --> 01:09:37,950
absolutely not

1067
01:09:37,950 --> 01:09:40,310
electric field should not be confused

1068
01:09:43,820 --> 01:09:47,410
what it means is that if you take a test charge in your pocket you

1069
01:09:47,410 --> 01:09:49,090
come from infinity

1070
01:09:49,100 --> 01:09:51,540
and you walk through that surface

1071
01:09:51,700 --> 01:09:56,250
by the time average that surface you've done zero work that's what it means

1072
01:09:57,890 --> 01:10:01,310
is zero

1073
01:10:01,350 --> 01:10:02,910
there is view one point

1074
01:10:02,930 --> 01:10:04,710
which we discussed earlier

1075
01:10:04,710 --> 01:10:08,570
in my lectures where the electric field is zero

1076
01:10:08,630 --> 01:10:11,000
the potential is not zero there

1077
01:10:11,000 --> 01:10:14,100
potential is definitely positive here

1078
01:10:14,150 --> 01:10:15,960
because he was zero

1079
01:10:16,000 --> 01:10:20,870
surface using already positive surface and this is the positive surface sort of potential is

1080
01:10:23,630 --> 01:10:25,140
if you reach that point

1081
01:10:25,180 --> 01:10:27,340
there's no force on the chart

1082
01:10:28,110 --> 01:10:30,010
that means electric fields

1083
01:10:30,050 --> 01:10:32,770
zero not so easy of course to calculate

1084
01:10:32,790 --> 01:10:37,160
these surfaces maxwell was capable of doing that kind of ten years ago

1085
01:10:37,210 --> 01:10:39,650
and nowadays we can do that very easily

1086
01:10:45,800 --> 01:10:49,320
the key potential surfaces which have different values

1087
01:10:49,370 --> 01:10:51,620
can never intersect

1088
01:10:51,660 --> 01:10:57,570
plus five vols surface can never intersect with the plus three or minus one you

1089
01:10:57,570 --> 01:11:01,240
think about why that is why is it will be total violation

1090
01:11:01,400 --> 01:11:03,410
conservation of energy

1091
01:11:03,430 --> 01:11:05,540
so equipotential surfaces

1092
01:11:05,540 --> 01:11:07,390
different values can never

1093
01:11:10,640 --> 01:11:13,400
all right

1094
01:11:13,450 --> 01:11:16,790
so you've seen that for the various charge configurations

1095
01:11:16,840 --> 01:11:19,240
the equipotential surfaces

1096
01:11:19,280 --> 01:11:21,710
a very complicated shapes

1097
01:11:22,950 --> 01:11:30,730
can always be calculated it's very easy way

1098
01:11:30,740 --> 01:11:32,150
now comes the question

1099
01:11:32,220 --> 01:11:35,820
why do we introduce electric potentials leads them

1100
01:11:35,840 --> 01:11:37,960
and leeds

1101
01:11:38,080 --> 01:11:40,270
people tential surfaces

1102
01:11:40,320 --> 01:11:43,810
isn't it true that if we know the electric field vector is everywhere in space

1103
01:11:44,490 --> 01:11:46,510
that determines uniquely

1104
01:11:46,520 --> 01:11:48,380
how charges will move

1105
01:11:48,410 --> 01:11:50,160
what acceleration with they will

1106
01:11:51,020 --> 01:11:54,240
that means how to kinetic energy will change the answer is yes if you know

1107
01:11:54,240 --> 01:11:56,630
electric field everywhere in space sure

1108
01:11:57,630 --> 01:11:59,660
you can predict everything that happens

1109
01:11:59,670 --> 01:12:02,900
it was a charge in that field but there are examples

1110
01:12:02,920 --> 01:12:07,130
twenty electric fields as are so incredibly complicated

1111
01:12:07,200 --> 01:12:09,270
that it is easier to work

1112
01:12:09,270 --> 01:12:11,000
with the equipotentials

1113
01:12:11,010 --> 01:12:14,180
because the change in kinetic energy is

1114
01:12:14,240 --> 01:12:15,720
discussed now

1115
01:12:15,750 --> 01:12:18,250
really depends only on the change

1116
01:12:18,260 --> 01:12:19,450
in the potential

1117
01:12:19,500 --> 01:12:21,760
you go from one point to another

1118
01:12:21,800 --> 01:12:26,320
so you will see very shortly that sometimes if you're only interested in change of

1119
01:12:26,320 --> 01:12:27,530
kinetic energy

1120
01:12:27,570 --> 01:12:31,830
not necessarily interested in the details of the trajectory

1121
01:12:32,880 --> 01:12:37,360
the key potentials come in very handy

1122
01:12:37,460 --> 01:12:40,310
never confuse

1123
01:12:41,650 --> 01:12:42,550
which is

1124
01:12:42,610 --> 01:12:46,050
electrostatic potential energy

1125
01:12:46,060 --> 01:12:47,380
with v

1126
01:12:47,480 --> 01:12:49,070
which is

1127
01:12:53,640 --> 01:12:54,810
has unit

1128
01:12:56,480 --> 01:13:00,060
and this has to reduce book call vol

1129
01:13:00,060 --> 01:13:02,950
so for example i mean we can generalize search

1130
01:13:02,980 --> 01:13:05,200
just check this log

1131
01:13:05,250 --> 01:13:09,250
so the search procedure is you start at the top left let's say we're looking

1132
01:13:09,250 --> 01:13:12,180
for seventy two

1133
01:13:12,180 --> 01:13:16,140
you start the top left fourteen is smaller than seventy two so i try to

1134
01:13:16,140 --> 01:13:18,770
go right seventy nine it's too big

1135
01:13:18,810 --> 01:13:21,270
so i i followed this area

1136
01:13:21,310 --> 01:13:24,910
but ourselves that's too much so instead i go down

1137
01:13:24,930 --> 01:13:28,710
fourteen still i go to the right of fifty that that's still small and seventy

1138
01:13:28,710 --> 01:13:29,850
two OK

1139
01:13:30,230 --> 01:13:33,350
try to go right again all seventy nine that's too big

1140
01:13:33,350 --> 01:13:36,910
it's good so i down so i get fifty

1141
01:13:36,930 --> 01:13:40,950
ah good try the same thing over and over try to go the right sixty

1142
01:13:40,950 --> 01:13:44,680
six it's OK to go right or something and it's too big so i go

1143
01:13:44,680 --> 01:13:48,540
down and i got the right of seventy two

1144
01:13:48,560 --> 01:13:51,310
otherwise go too far in trying to go down ourselves

1145
01:13:51,370 --> 01:13:53,350
element must not be there

1146
01:13:53,370 --> 01:13:55,100
very simple search on

1147
01:13:55,120 --> 01:13:56,370
the same is here

1148
01:13:56,370 --> 01:13:58,600
except just remove l one and l two

1149
01:13:58,620 --> 01:14:01,350
go right until that would go too far then go down think around it so

1150
01:14:01,350 --> 01:14:04,730
that we could finding good and might have this login times

1151
01:14:04,770 --> 01:14:08,310
in each level including only walking a couple of steps

1152
01:14:08,330 --> 01:14:12,480
because the ratio between these two size is only two

1153
01:14:12,540 --> 01:14:15,060
so this will cost to log

1154
01:14:16,270 --> 01:14:20,040
so i mean that was to check because we're using intuition over here

1155
01:14:20,060 --> 01:14:23,350
a little bit shaky

1156
01:14:23,450 --> 01:14:26,600
so this is an ideal skip list

1157
01:14:26,620 --> 01:14:29,770
we have to support assertions and deletions as soon as we do need to delete

1158
01:14:29,940 --> 01:14:32,850
so now we're going to maintain the structure of the two

1159
01:14:33,370 --> 01:14:37,140
two special there's only one of these

1160
01:14:37,200 --> 01:14:41,580
where you were the you know everything is perfectly spaced out and everything is

1161
01:14:41,600 --> 01:14:42,500
you full

1162
01:14:42,520 --> 01:14:43,980
so we can do that

1163
01:14:43,980 --> 01:14:47,080
we're going to maintain roughly this structure

1164
01:14:47,180 --> 01:14:49,640
as best we can

1165
01:14:51,120 --> 01:14:52,950
if any one of you knows

1166
01:14:53,000 --> 01:14:54,500
someone in

1167
01:14:54,600 --> 01:14:58,540
new york city some planning can tell us

1168
01:14:58,560 --> 01:15:00,480
so asking

1169
01:15:00,700 --> 01:15:13,060
so this is the real this is basically a data structure could use this is

1170
01:15:13,060 --> 01:15:18,230
the starting point but then start using skip lists

1171
01:15:18,250 --> 01:15:20,310
and we need to somehow

1172
01:15:22,200 --> 01:15:25,770
insertions and deletions

1173
01:15:25,790 --> 01:15:29,330
and maintain roughly the structure well enough

1174
01:15:30,430 --> 01:15:31,370
the search

1175
01:15:31,390 --> 01:15:37,730
still cost or log n time

1176
01:15:37,810 --> 01:15:41,020
so let's focus on insertions

1177
01:15:41,230 --> 01:15:44,180
we do insertions has that deletions are

1178
01:15:44,200 --> 01:15:48,680
really trivial

1179
01:15:49,290 --> 01:16:08,140
and again this is all from first principles are not allowed to say anything

1180
01:16:08,180 --> 01:16:12,040
but it would be nice we could talk

1181
01:16:13,520 --> 01:16:19,120
so suppose you want to insert an element x we is set out to search

1182
01:16:19,120 --> 01:16:22,370
for element so how do we inserted

1183
01:16:22,390 --> 01:16:25,250
well the first thing we should do is figure out where to go so research

1184
01:16:26,730 --> 01:16:33,370
because search access to find somewhere

1185
01:16:34,970 --> 01:16:37,600
in the bottomless just anywhere

1186
01:16:37,700 --> 01:16:41,890
pretty easy to find a replacement topless that takes constant time

1187
01:16:42,040 --> 01:16:43,470
we want to know

1188
01:16:43,500 --> 01:16:46,290
because of has constant length

1189
01:16:46,310 --> 01:16:49,910
we want to know where x goes in the bottomless so we're search let's say

1190
01:16:49,910 --> 01:16:51,850
we want to insert eighty

1191
01:16:51,870 --> 01:16:56,450
we search for eighty we get a bit of a social seventy five

1192
01:16:56,540 --> 01:17:00,870
will find seventy five it's three here between seventy to seventy nine using the same

1193
01:17:03,180 --> 01:17:07,500
OK if it's there already we complain because i'm going to assume all keys are

1194
01:17:07,500 --> 01:17:08,890
distinct are now

1195
01:17:08,890 --> 01:17:12,600
west you start with a convex function you subtract off

1196
01:17:12,620 --> 01:17:14,850
the linear approximation

1197
01:17:14,870 --> 01:17:16,810
and then

1198
01:17:16,850 --> 01:17:17,720
the gap

1199
01:17:17,770 --> 01:17:20,350
in the approximation is the divergence

1200
01:17:20,540 --> 01:17:23,270
really understand this

1201
01:17:23,350 --> 01:17:25,750
it always has this formula

1202
01:17:25,810 --> 01:17:29,680
convex function minus the linear approximation

1203
01:17:30,520 --> 01:17:35,020
it has various properties defenses property

1204
01:17:42,770 --> 01:17:46,680
the saccharine theorem

1205
01:17:46,680 --> 01:17:47,740
which says

1206
01:17:47,740 --> 01:17:51,370
that the if you project onto convex set

1207
01:17:51,430 --> 01:17:53,830
this inequality always holds

1208
01:17:53,890 --> 01:18:01,080
no matter what the bregman divergences

1209
01:18:01,120 --> 01:18:08,720
this is opposite from the triangle inequality so that's why we use the term divergence

1210
01:18:09,790 --> 01:18:14,560
i gave a bunch of examples

1211
01:18:14,560 --> 01:18:17,470
right euclidean distance squared

1212
01:18:19,080 --> 01:18:30,930
and then he algorithms that interpolates between the two families

1213
01:18:32,490 --> 01:18:33,740
auc entropy

1214
01:18:33,750 --> 01:18:36,790
then i gave the matrix versions of them

1215
01:18:36,930 --> 01:18:41,060
and now i talked about how i motivate these updates in general

1216
01:18:41,060 --> 01:18:43,240
trade between divergence

1217
01:18:43,240 --> 01:18:44,970
and loss

1218
01:18:44,990 --> 01:18:49,430
this holds you back this makes you lost small

1219
01:18:49,430 --> 01:18:54,540
OK this is the general form of the update

1220
01:18:54,600 --> 01:19:01,220
general form of the update

1221
01:19:01,270 --> 01:19:06,910
you always apply to link function to the weight vector

1222
01:19:06,950 --> 01:19:11,080
you subtract the gradient and then you go back

1223
01:19:16,520 --> 01:19:22,470
these days are various

1224
01:19:22,470 --> 01:19:25,870
analysis that we have done in the simple analysis to the

1225
01:19:25,890 --> 01:19:30,930
the learning rate is constant in some of the fancy analysis we have learning rates

1226
01:19:30,930 --> 01:19:32,100
one or

1227
01:19:33,310 --> 01:19:37,970
this very deep problem in this context the problem is

1228
01:19:37,990 --> 01:19:40,930
i would like to have the trade-off

1229
01:19:40,930 --> 01:19:43,450
parameter vector which is also

1230
01:19:43,470 --> 01:19:47,120
over here as the parameters not just as the weight vector but also in

1231
01:19:47,120 --> 01:19:48,200
and eta

1232
01:19:48,320 --> 01:19:52,950
and then i would need a more complicated expression in here which also would have

1233
01:19:53,010 --> 01:19:55,270
involved the divergence

1234
01:19:55,290 --> 01:19:58,950
of the current learning rate to deal with learning rate

1235
01:19:59,040 --> 01:20:00,770
plus something additional

1236
01:20:00,810 --> 01:20:05,740
and we haven't quite piece this together they some kind of phd thesis looking out

1237
01:20:05,810 --> 01:20:10,470
that brings design very carefully the also derives an update

1238
01:20:10,490 --> 01:20:13,350
for the learning rate

1239
01:20:13,450 --> 01:20:15,450
we seen inklings of it

1240
01:20:15,470 --> 01:20:17,120
but it's not perfect yet

1241
01:20:17,160 --> 01:20:19,600
it's kind of frustrating

1242
01:20:19,620 --> 01:20:21,270
so beautiful

1243
01:20:24,430 --> 01:20:28,330
OK it's kind of going later on we going to motivate bregman divergences and exponential

1244
01:20:28,330 --> 01:20:30,270
families is going from

1245
01:20:30,290 --> 01:20:32,160
sort of the

1246
01:20:32,200 --> 01:20:34,290
this generalisation of exponentials

1247
01:20:34,310 --> 01:20:38,160
families that have the temperature in exponent street in that case

1248
01:20:38,160 --> 01:20:44,520
OK here i showed you pictorially what the trade of does

1249
01:20:44,520 --> 01:20:46,810
some going over this because it's important

1250
01:20:46,850 --> 01:20:49,890
this is the last

1251
01:20:49,890 --> 01:20:51,080
here's an example

1252
01:20:51,100 --> 01:20:54,810
divergence terms and here's the trade-off

1253
01:20:54,810 --> 01:20:56,470
between the two

1254
01:20:56,490 --> 01:21:05,430
tension between staying where you are verses updating and greedily going to minimizing loss

1255
01:21:05,430 --> 01:21:08,890
OK then i'll give you an example

1256
01:21:08,890 --> 01:21:12,240
i of sort of a general outline for how to improve bounds

1257
01:21:12,290 --> 01:21:17,180
use these convexity of the loss and use properties of

1258
01:21:17,220 --> 01:21:20,510
the divergence you get some kind of telescoping

1259
01:21:20,560 --> 01:21:23,930
i just want to get the flavour of how it's done

1260
01:21:23,990 --> 01:21:27,100
at the end you arrive at the bound of this type

1261
01:21:27,160 --> 01:21:29,290
total loss of the output

1262
01:21:29,310 --> 01:21:32,250
so moses biggest loss of the compared to

1263
01:21:32,290 --> 01:21:36,250
plus some divergence the beginning

1264
01:21:36,270 --> 01:21:45,890
notice bounds hold for arbitrary sequences of examples OK

1265
01:21:45,910 --> 01:21:50,140
here's some examples found for linear regression

1266
01:21:50,750 --> 01:21:53,080
these bounds are done for the worst case

1267
01:21:53,080 --> 01:21:58,290
for arbitrary sequence of examples sometimes called the worst-case relative loss bounds

1268
01:21:58,330 --> 01:22:00,310
worst-case regret bounds

1269
01:22:00,330 --> 01:22:05,250
it turns out if you pick any of these problems like a and and analyse

1270
01:22:05,270 --> 01:22:09,580
the out and out to analysis you do the worst case analysis union converted to

1271
01:22:09,580 --> 01:22:14,540
an average case analysis you be to beat the best bounds

1272
01:22:14,600 --> 01:22:18,160
the around for the average case for the case where the instances are generated by

1273
01:22:18,160 --> 01:22:20,010
some distribution

1274
01:22:20,060 --> 01:22:22,250
this is kind of

1275
01:22:22,250 --> 01:22:26,770
new level of power the documents similarly when you take into account titles and five

1276
01:22:26,870 --> 01:22:30,240
those kinds of information all really open up new possibilities for

1277
01:22:30,320 --> 01:22:31,970
richard better search

1278
01:22:31,990 --> 01:22:36,230
so what unexploited aspect the document is next

1279
01:22:37,110 --> 01:22:41,810
you know it's actually the linguistic structure document irony here is the most is actually

1280
01:22:41,810 --> 01:22:43,640
made of linguistic structure

1281
01:22:43,660 --> 01:22:47,060
and currently is mostly discarded and ignored

1282
01:22:47,070 --> 01:22:53,040
this is you know largely unaddressed the document side due to historically to computational cost

1283
01:22:53,040 --> 01:22:58,360
and complexity and most previous work in the natural language community trying to apply natural

1284
01:22:58,360 --> 01:23:03,330
language search has really focused on the query only and their user enters a query

1285
01:23:03,330 --> 01:23:09,060
and the system tries to understand the query and translate that into a keyword advanced

1286
01:23:09,120 --> 01:23:11,920
or you know you were critically through the search engines

1287
01:23:11,930 --> 01:23:13,930
that can go some of the way

1288
01:23:13,940 --> 01:23:15,240
but ultimately

1289
01:23:15,450 --> 01:23:18,260
it's just that there are things you would like to try to do the impossible

1290
01:23:18,260 --> 01:23:25,510
it's fundamentally limited because then your system while translated queries it still faced with an

1291
01:23:25,510 --> 01:23:30,470
old keyword is style interface into the documents themselves

1292
01:23:30,510 --> 01:23:37,250
so this structurally has immense value and is where the document's intent is action coding

1293
01:23:37,250 --> 01:23:43,310
so i powerset semantic index are indexer cracks linguistic structure these documents to extract meaning

1294
01:23:43,450 --> 01:23:47,990
the applies deep natural in which the entire corpus to build the right representation

1295
01:23:48,040 --> 01:23:53,540
this actually becomes a new kind of index creates a platform for innovation

1296
01:23:53,600 --> 01:23:57,560
that allows for greatly expanding capabilities we think we're really just at the start of

1297
01:23:57,560 --> 01:24:01,740
this and that together as a community world all going to be discovering the possibilities

1298
01:24:01,740 --> 01:24:04,580
that this enables

1299
01:24:04,810 --> 01:24:10,610
now natural language for consumer search especially the processing of the content on the index

1300
01:24:10,610 --> 01:24:13,770
inside was really not possible

1301
01:24:13,800 --> 01:24:17,490
that this kind of the scale that we're looking even five years ago

1302
01:24:17,560 --> 01:24:19,430
several things happened

1303
01:24:19,750 --> 01:24:25,170
in terms of economics the cost came down more has just

1304
01:24:25,270 --> 01:24:27,930
driven to make you know making many

1305
01:24:27,950 --> 01:24:31,720
orders of magnitude more computing available then when people were really trying to do and

1306
01:24:31,720 --> 01:24:34,070
natural language processing

1307
01:24:34,080 --> 01:24:39,200
even now benefiting from four corps and core processes among other things the cost is

1308
01:24:39,200 --> 01:24:44,980
going down really fast in addition to the technology is natural language technologies have sped

1309
01:24:44,990 --> 01:24:46,450
up in recent times

1310
01:24:46,500 --> 01:24:50,940
on the other side economics revenue came up so

1311
01:24:51,140 --> 01:24:54,560
all quite awhile ago people were looking at search there was almost no money to

1312
01:24:54,560 --> 01:24:58,100
be made it was something we might get some users but no and no one

1313
01:24:58,100 --> 01:25:02,250
knew how to make money that's all changed now the

1314
01:25:02,310 --> 01:25:08,100
advertising revenue from search is you massive and you can actually afford with this kind

1315
01:25:08,100 --> 01:25:12,720
of economics to do a lot of computing on the back end

1316
01:25:12,730 --> 01:25:17,490
it's also the case that the user experience can be transformed even with the current

1317
01:25:17,490 --> 01:25:22,540
NL capabilities in natural language has come along way but it's not perfect but the

1318
01:25:22,540 --> 01:25:27,950
good news about search is that keyword search is not perfect either so perfection is

1319
01:25:27,950 --> 01:25:29,190
not required here

1320
01:25:29,210 --> 01:25:34,850
but you know and we now the existence of robust broad coverage integrated systems

1321
01:25:34,860 --> 01:25:41,090
in addition there is a challenge to work across multiple languages and several of these

1322
01:25:41,090 --> 01:25:45,780
systems including the one power cities using are based on multilingual core platform

1323
01:25:45,790 --> 01:25:49,830
and you can then plug ins essentially plug in new languages to make the system

1324
01:25:50,500 --> 01:25:56,300
you know there there's an issue here about changing user behavior because users used entering

1325
01:25:56,300 --> 01:26:00,980
keywords i think in this research community we all look towards the future in any

1326
01:26:00,980 --> 01:26:04,850
case and this is the kind of change that should be a change to something

1327
01:26:04,850 --> 01:26:09,570
easier because you don't need to its were used to keyword search down but it

1328
01:26:09,590 --> 01:26:13,270
still is something artificial there's it it's the gap between what we really think of

1329
01:26:13,340 --> 01:26:15,410
our and we have to do so

1330
01:26:15,440 --> 01:26:19,970
kind of like when you're shifting from going from driving a car with a manual

1331
01:26:19,970 --> 01:26:22,200
transmission to an automatic transmission

1332
01:26:22,210 --> 01:26:25,920
you have to change the behavior but it's the change is something that's easier at

1333
01:26:25,920 --> 01:26:27,190
least for some of us

1334
01:26:27,200 --> 01:26:33,390
these kind of things are actually already happening we're nothing more voice interfaces in mobile

1335
01:26:33,390 --> 01:26:38,570
contexts we want to use language and get more precise results and already with careers

1336
01:26:38,730 --> 01:26:45,180
naver and yahoo answers and an increasing trend towards giving users shortcuts an instant answers

1337
01:26:45,180 --> 01:26:50,010
directly on on web search there's more and more opportunities to be using language to

1338
01:26:50,010 --> 01:26:52,190
get a better result in search

1339
01:26:52,200 --> 01:26:57,650
and even just using keywords these kinds of system by having more information coming out

1340
01:26:57,650 --> 01:27:03,830
of the documents can actually give better results without any changes we have

1341
01:27:03,840 --> 01:27:05,780
so the language technology

1342
01:27:06,010 --> 01:27:10,700
has really evolved a lot there are now on large scale grammars that can practically

1343
01:27:10,700 --> 01:27:18,740
employed there's also an increasing amount of lexical ontological knowledge resources things like wordnet verbnet

1344
01:27:19,780 --> 01:27:20,880
so know

1345
01:27:20,890 --> 01:27:23,200
the things many people here work on

1346
01:27:23,220 --> 01:27:29,270
in addition there's been a lot of improvement in data driven acquisition methods to acquire

1347
01:27:29,270 --> 01:27:34,620
new ontological natural resources and to bridge different kinds of ontologies so the work of

1348
01:27:34,620 --> 01:27:40,860
this community is really directly feeding into these you know enabling these kinds of applications

1349
01:27:40,870 --> 01:27:43,290
or discuss more

1350
01:27:43,300 --> 01:27:46,570
another big trend is open source software

1351
01:27:46,880 --> 01:27:53,830
these are big areas for most smaller efforts including start-ups and research organisations to create

1352
01:27:53,830 --> 01:27:58,840
things in large scale but the open source community is actually now working to

1353
01:27:58,840 --> 01:28:01,970
let me know the demonstration relatively small circuit

1354
01:28:02,030 --> 01:28:04,290
and measures of voltages for you

1355
01:28:04,320 --> 01:28:08,480
and show you that

1356
01:28:08,510 --> 01:28:11,200
the voltage is indeed add up to zero

1357
01:28:11,210 --> 01:28:19,910
so you my those circuits

1358
01:28:19,990 --> 01:28:58,280
so i'm going to show you a simple circuit that looks like this and let's

1359
01:28:58,280 --> 01:28:59,860
go ahead and

1360
01:28:59,870 --> 01:29:02,520
measure some voltages and currents

1361
01:29:02,530 --> 01:29:04,470
in terminology remember

1362
01:29:04,470 --> 01:29:08,450
this is called a loop

1363
01:29:08,470 --> 01:29:11,340
so if i take place far from the points c

1364
01:29:11,370 --> 01:29:14,160
and travel through the water source

1365
01:29:14,160 --> 01:29:18,470
come to denote a dancefloor one and all the way down throughout the back seat

1366
01:29:18,470 --> 01:29:20,280
that's loop

1367
01:29:20,330 --> 01:29:23,970
so the only this point eight is the node where

1368
01:29:23,980 --> 01:29:25,360
resistor r one

1369
01:29:25,370 --> 01:29:28,960
what source we zero and are four connected

1370
01:29:29,080 --> 01:29:32,100
just make sure your terminology is correct

1371
01:29:32,220 --> 01:29:36,910
it's all of those let me make some quick measurements for you and show you

1372
01:29:36,910 --> 01:29:42,180
that are these KVL and KCL are indeed in the truth

1373
01:29:44,200 --> 01:29:47,210
so it's up there could have a

1374
01:29:47,220 --> 01:29:49,760
grammar volunteer

1375
01:29:49,840 --> 01:29:51,010
and you want

1376
01:29:51,090 --> 01:29:54,720
well you have to do the right things the work one

1377
01:29:56,860 --> 01:29:59,180
OK so

1378
01:29:59,230 --> 01:30:04,020
let me take some measurements and wanted to write down what i measure on the

1379
01:30:05,270 --> 01:30:09,080
what i do is one of his talk here

1380
01:30:09,110 --> 01:30:13,080
but although in

1381
01:30:13,120 --> 01:30:14,150
focus on

1382
01:30:14,170 --> 01:30:15,450
this peer

1383
01:30:15,460 --> 01:30:17,150
and focus on

1384
01:30:17,190 --> 01:30:29,340
this node and makes some measurements

1385
01:30:29,390 --> 01:30:30,330
all right

1386
01:30:31,680 --> 01:30:38,520
you see the circuit up there

1387
01:30:38,540 --> 01:30:41,620
OK so i get few more

1388
01:30:41,680 --> 01:30:43,960
for the voltage from CQA

1389
01:30:43,970 --> 01:30:54,610
so in two i don't labels

1390
01:30:54,760 --> 01:31:03,610
OK so the next one is

1391
01:31:03,620 --> 01:31:06,010
minus one point six

1392
01:31:06,210 --> 01:31:18,870
and so that the and doing up a b b a b

1393
01:31:18,880 --> 01:31:22,270
OK and then let me do

1394
01:31:22,330 --> 01:31:25,750
the last one and

1395
01:31:25,760 --> 01:31:29,470
it is minus one point three seven

1396
01:31:29,520 --> 01:31:41,340
i could be the measurements ideas made have been this way so it's

1397
01:31:41,390 --> 01:31:45,310
it was written is busy but that's it's OK for about it so

1398
01:31:45,320 --> 01:31:47,170
so then number thank you

1399
01:31:47,420 --> 01:31:51,180
i appreciate your help here

1400
01:31:53,190 --> 01:31:55,360
OK so

1401
01:31:57,770 --> 01:32:00,120
within bonds of experimental error

1402
01:32:00,180 --> 01:32:04,550
notice that if i am out of this we will produce they the nicely sum

1403
01:32:04,550 --> 01:32:06,800
up to zero

1404
01:32:06,810 --> 01:32:10,050
OK next let me focus on this note here

1405
01:32:10,100 --> 01:32:16,140
and add this node let me go ahead and measure some currents

1406
01:32:16,200 --> 01:32:18,670
what i do know is changed to

1407
01:32:18,750 --> 01:32:20,970
james to an AC

1408
01:32:20,990 --> 01:32:25,300
AC voltage so that i can go ahead and measure the current without breaking my

1409
01:32:26,540 --> 01:32:46,320
again this time around you'll get to see the measurements that i'm taking as well

1410
01:32:46,400 --> 01:32:47,700
so we have here

1411
01:32:51,050 --> 01:32:54,680
i guess you can see this for what i have here is three wires

1412
01:32:54,720 --> 01:32:57,020
that pulled out from the

1413
01:32:57,040 --> 01:32:58,460
and this is the node

1414
01:32:58,470 --> 01:33:00,470
OK so three wires

1415
01:33:00,480 --> 01:33:03,570
coming into the node is to make it a little easier for me to measure

1416
01:33:04,750 --> 01:33:07,460
OK so i everybody keep your fingers crossed

1417
01:33:07,470 --> 01:33:09,340
so i don't look like a fool

1418
01:33:09,600 --> 01:33:11,560
i hope this works out OK

1419
01:33:13,270 --> 01:33:15,210
so you don't forget

1420
01:33:15,210 --> 01:33:16,890
this is the same so i it will give you

1421
01:33:17,260 --> 01:33:23,750
however the different rows or columns related to one another but i'm not going to

1422
01:33:23,770 --> 01:33:29,120
going over all the same masses of dry dung the last sort of half thing

1423
01:33:29,130 --> 01:33:32,820
but basically identical and the cells are still all in the book and how is

1424
01:33:32,820 --> 01:33:38,000
identical is all the book and the final picture is that

1425
01:33:38,020 --> 01:33:41,960
more dimensional scaling is

1426
01:33:41,970 --> 01:33:44,060
this is certainly where

1427
01:33:44,070 --> 01:33:45,870
instead of having

1428
01:33:45,880 --> 01:33:50,730
a set of experts labels that tell you

1429
01:33:50,750 --> 01:33:57,250
if you think of the axes given coordinates in space and in the course world

1430
01:33:57,270 --> 01:34:01,700
suppose that we have got is

1431
01:34:01,720 --> 01:34:03,120
the distance

1432
01:34:03,120 --> 01:34:05,350
between each pair

1433
01:34:05,360 --> 01:34:09,270
of objects in space so we might have

1434
01:34:09,330 --> 01:34:11,430
so what i mean that situation

1435
01:34:12,850 --> 01:34:18,410
what i think of two citizen travelling between them and how much does it cost

1436
01:34:18,420 --> 01:34:19,510
you might have

1437
01:34:19,520 --> 01:34:21,940
for example we can look at minute which is

1438
01:34:21,950 --> 01:34:23,820
places in the

1439
01:34:23,930 --> 01:34:28,350
britain and you got distances you want to draw a map that

1440
01:34:28,350 --> 01:34:34,930
and that's the objective is to draw a map that represents those distances that you

1441
01:34:35,740 --> 01:34:41,100
so much where the distance between the sexes is telling you about the price of

1442
01:34:41,100 --> 01:34:42,920
the tickets

1443
01:34:43,010 --> 01:34:46,910
so it's not the geographical distance they actually

1444
01:34:46,990 --> 01:34:49,080
and in this case

1445
01:34:49,090 --> 01:34:52,440
not the geographical distance but how far you actually have to go

1446
01:34:53,070 --> 01:34:55,730
in terms of driving rather than

1447
01:34:55,770 --> 01:35:00,750
flying in a helicopter all crow flies if you go by road

1448
01:35:00,800 --> 01:35:07,600
and the really classical example was man

1449
01:35:07,620 --> 01:35:11,160
i was told this egyptologist copy petri

1450
01:35:11,210 --> 01:35:13,560
what is the

1451
01:35:13,570 --> 01:35:19,650
it was each of these two things incidents of different types of classes

1452
01:35:19,670 --> 01:35:25,150
i would list which tunes the vases appeared in all which types of those work

1453
01:35:25,380 --> 01:35:31,460
coefficient measures how similar or different things from ones who were compared to another so

1454
01:35:31,890 --> 01:35:37,460
so this is how these sorts of things arise you a classic common standard method

1455
01:35:37,460 --> 01:35:39,650
of doing this is

1456
01:35:39,680 --> 01:35:45,710
one of largest arises things like taste testing when you ask people to stop eight

1457
01:35:47,140 --> 01:35:48,720
or does

1458
01:35:49,440 --> 01:35:52,930
so it's a small similar to chocolate and chocolate see those and things like that

1459
01:35:52,940 --> 01:35:54,560
so you rankings

1460
01:35:54,560 --> 01:35:55,650
but not

1461
01:35:57,000 --> 01:35:59,980
so you have a question about differences but not

1462
01:36:01,770 --> 01:36:05,840
the actual cases you know this is just a question how similar things are not

1463
01:36:05,880 --> 01:36:09,810
about their absolute properties

1464
01:36:11,170 --> 01:36:13,980
so we try do is we have the pairwise

1465
01:36:13,990 --> 01:36:17,530
coefficients measure how similar one justice to another or

1466
01:36:17,550 --> 01:36:18,710
o different

1467
01:36:19,900 --> 01:36:23,430
with local distances i have some examples as to

1468
01:36:23,470 --> 01:36:27,510
we're trying to construct is

1469
01:36:27,580 --> 01:36:32,450
the corresponding inputs the axes that would have given that space or map

1470
01:36:32,470 --> 01:36:36,150
o corresponds to distances

1471
01:36:36,160 --> 01:36:43,310
and various types one is classical scaling classical scaling because it was the first one

1472
01:36:43,320 --> 01:36:44,460
people came up with

1473
01:36:44,480 --> 01:36:53,670
so psychometric scaling because mathematically there's an assumption that the distances between places are actually

1474
01:36:53,690 --> 01:36:57,930
metric which if you don't know what i'm talking about that don't worry me normal

1475
01:37:02,920 --> 01:37:07,660
president for this article principal components analysis because it has a relationship to principal components

1476
01:37:07,660 --> 01:37:13,470
analysis paul not interrupts right OK

1477
01:37:13,490 --> 01:37:15,990
and it's been a long time

1478
01:37:17,980 --> 01:37:19,260
to show you itself

1479
01:37:19,270 --> 01:37:21,050
the is

1480
01:37:21,060 --> 01:37:27,700
taken in britain we have something called the automobile association they publish a little table

1481
01:37:27,780 --> 01:37:31,140
of distances between cities

1482
01:37:31,150 --> 01:37:32,410
by road

1483
01:37:34,150 --> 01:37:36,400
they're not the shortest distance is that

1484
01:37:36,550 --> 01:37:38,550
the fastest distances

1485
01:37:38,570 --> 01:37:43,720
the short journey time distance into the time not in terms of

1486
01:37:43,940 --> 01:37:45,910
miles it is about

1487
01:37:45,910 --> 01:37:48,740
right so taking

1488
01:37:48,770 --> 01:37:52,680
and applied classical principal component

1489
01:37:52,680 --> 01:37:56,520
now we are experts so we are starting to

1490
01:37:56,530 --> 01:38:04,780
the standard to combine things coming from different perspectives so we learn to learn something

1491
01:38:04,780 --> 01:38:07,320
about dynamic systems theory

1492
01:38:07,320 --> 01:38:09,230
and we can easily see

1493
01:38:09,260 --> 01:38:11,520
that if we we are modelling

1494
01:38:11,950 --> 01:38:16,980
we did using bayesian networks a situation like this one

1495
01:38:17,040 --> 01:38:19,280
in which we have

1496
01:38:19,280 --> 01:38:27,060
output measurements only of one of the state variables so which reflects the state variable

1497
01:38:27,100 --> 01:38:31,720
we may have a situation of partial observability

1498
01:38:31,860 --> 01:38:38,320
so we have the dynamical system which is is punishable partially observable states

1499
01:38:38,490 --> 01:38:40,990
and we can use it also

1500
01:38:41,020 --> 01:38:47,990
bayesian networks and so this kind of modelling to represent the systems which are dynamic

1501
01:38:48,070 --> 01:38:56,190
which instead of being a deterministic stochastic systems in which the state is not fully

1502
01:38:56,230 --> 01:38:58,180
observable so

1503
01:38:58,230 --> 01:38:59,160
i need

1504
01:38:59,180 --> 01:39:05,940
to put some knowledge around here or to use a particular optimisation goal is to

1505
01:39:05,940 --> 01:39:07,100
try to derive

1506
01:39:07,650 --> 01:39:10,390
the knowledge about the

1507
01:39:10,410 --> 01:39:15,200
transition of the state x x one

1508
01:39:15,290 --> 01:39:19,890
OK so you survive the for

1509
01:39:19,930 --> 01:39:29,720
around one hour and a half cylinder last the meaning of the of the

1510
01:39:29,730 --> 01:39:31,580
talk i have to

1511
01:39:31,640 --> 01:39:36,530
tell you something about one of the main topics of

1512
01:39:36,570 --> 01:39:42,500
the chapter of the book which was written by elizabeth bradley

1513
01:39:44,860 --> 01:39:49,260
one of the topic itself is called delay coordinate embedding

1514
01:39:49,280 --> 01:39:53,230
why have left this topic at the end of the story because i think that

1515
01:39:53,890 --> 01:39:59,760
we do have understood the let's say the basic components of the problem so we

1516
01:39:59,760 --> 01:40:07,940
have dynamical system this dynamical system can be typically described using the state space

1517
01:40:07,990 --> 01:40:13,030
which is a fundamental concept that tells you what's going on really inside of the

1518
01:40:13,030 --> 01:40:18,390
system when we are modelling we can if we are able make can model the

1519
01:40:19,360 --> 01:40:25,110
using the state variable sometimes we can't because we don't have enough knowledge to do

1520
01:40:25,190 --> 01:40:33,530
we are using input output systems input output systems can be deterministic or stochastic they

1521
01:40:33,540 --> 01:40:37,640
can be represented in different ways so now

1522
01:40:37,650 --> 01:40:46,030
the interesting question is if i just have all the variables m i

1523
01:40:46,660 --> 01:40:49,610
the arrival the presentation

1524
01:40:49,620 --> 01:40:53,140
of the output variable in the state space

1525
01:40:53,310 --> 01:40:58,850
so it's something which is missing in our discussion of the

1526
01:40:58,980 --> 01:41:04,770
and this idea of the coordinate embedding is exactly trying

1527
01:41:04,780 --> 01:41:08,240
to answer these questions

1528
01:41:08,260 --> 01:41:09,740
so if i'm able

1529
01:41:09,760 --> 01:41:17,740
to reconstruct the space the state space representation from a uni dimensional time series and

1530
01:41:17,740 --> 01:41:21,180
being available set of sample data

1531
01:41:21,190 --> 01:41:28,450
so it has been shown that it is possible to have the representation with there's

1532
01:41:28,450 --> 01:41:37,650
some properties some good properties in the main good properties that the qualitative behavior of

1533
01:41:37,650 --> 01:41:44,350
the face portrayed the which able to derive using this technique of the coordinate embedding

1534
01:41:44,440 --> 01:41:48,040
is the same of the real world

1535
01:41:48,070 --> 01:41:52,140
what does it mean this means that what they are doing using this technique is

1536
01:41:52,140 --> 01:41:57,060
not exactly this the state space in terms of

1537
01:41:57,100 --> 01:42:06,500
the same quantities so the scale for example is not preserved but what is certain

1538
01:42:06,500 --> 01:42:12,100
is that if i found if a able to find it equilibrium point in the

1539
01:42:12,100 --> 01:42:19,220
states space right using this technique then this equilibrium point will exist also in the

1540
01:42:19,220 --> 01:42:21,360
real world and in the true

1541
01:42:21,370 --> 01:42:28,450
OK so this so-called invari ants preserved the moving it is representing the evolution of

1542
01:42:28,450 --> 01:42:37,200
the state space in this using this technique with respect to the the real ones

1543
01:42:37,220 --> 01:42:41,530
so what is the idea well the idea is extremely simple when i started this

1544
01:42:41,530 --> 01:42:43,760
technique i far as well

1545
01:42:43,810 --> 01:42:52,950
it's so simple and by forces you're insured is through data is to add a

1546
01:42:53,070 --> 01:42:59,190
certain number of state variables using the same values of y which is time which

1547
01:42:59,520 --> 01:43:10,020
observing over time delay the all certain things to so using their also an example

1548
01:43:10,460 --> 01:43:13,410
of before example is better than

1549
01:43:13,440 --> 01:43:18,600
hundreds of the theorem so let's start with the an example

1550
01:43:18,600 --> 01:43:24,370
so let's study is example which is performed on a linear system

1551
01:43:24,370 --> 01:43:27,060
he has a very popular

1552
01:43:27,110 --> 01:43:30,300
probabilistic model for solving collaborative filtering

1553
01:43:30,320 --> 01:43:34,630
the idea is the following so we're gonna assume that every user is gonna

1554
01:43:34,630 --> 01:43:35,760
have some

1555
01:43:35,820 --> 01:43:40,170
some features which are unobserved

1556
01:43:40,170 --> 01:43:41,740
and this is a

1557
01:43:41,760 --> 01:43:47,960
a vector in the Euclidean space which basically describes what sort of movies does the

1558
01:43:47,960 --> 01:43:49,240
user like

1559
01:43:49,280 --> 01:43:56,870
corresponding to every movie we're also gonna assume that there's a probability vector which describes

1560
01:43:56,910 --> 01:43:59,690
what type of movie that movie is

1561
01:43:59,710 --> 01:44:02,650
and we

1562
01:44:02,690 --> 01:44:06,970
postulate that the rating that user i gives to movie j

1563
01:44:07,340 --> 01:44:11,930
is gonna be Gaussian distributed with a mean given by the dot product between

1564
01:44:11,930 --> 01:44:14,730
the user feature and the movie feature

1565
01:44:14,730 --> 01:44:15,760
so that

1566
01:44:15,780 --> 01:44:21,040
if the two feature vectors count lineup together so what that says is that the sort of

1567
01:44:21,040 --> 01:44:25,410
movies that the user likes is very similar to the sort of movie that that

1568
01:44:25,430 --> 01:44:26,540
movie is

1569
01:44:27,890 --> 01:44:32,650
and of course we'r gonna assume that there's some variants around this mean okay

1570
01:44:32,650 --> 01:44:36,320
so that we can handle

1571
01:44:36,340 --> 01:44:42,170
all the randomness in the ratings of the of the of the users okay

1572
01:44:43,950 --> 01:44:48,260
yeah r i j is gonna be Gaussian distribution over the mean given by this dog products

1573
01:44:48,260 --> 01:44:51,580
and the variants given by sigma square and

1574
01:44:51,630 --> 01:44:53,080
you can now

1575
01:44:53,130 --> 01:44:56,150
I forgot to to descr

1576
01:44:56,190 --> 01:45:01,020
to tell you what is the prior of these features and the prior of sigma square but

1577
01:45:01,020 --> 01:45:04,970
you can imagine that you know these features might be given priors which are

1578
01:45:04,970 --> 01:45:08,410
Gaussian and this feature's also Gaussian while

1579
01:45:08,450 --> 01:45:13,470
for sigma square you might use a gamma or it or inverse gamma a prior

1580
01:45:13,780 --> 01:45:21,020
and given our ratings we can now again compute the com the posterior distribution over user

1581
01:45:21,030 --> 01:45:25,370
features and movie features and that basically

1582
01:45:25,370 --> 01:45:27,430
kind of solves the problem for us okay

1583
01:45:27,450 --> 01:45:33,630
because with the user features and product features we can we can estimate

1584
01:45:33,650 --> 01:45:37,760
how much does each user likes each movie

1585
01:45:38,340 --> 01:45:42,560
according to this model of course one thing to note about this is

1586
01:45:45,240 --> 01:45:46,970
we are not just

1587
01:45:46,970 --> 01:45:48,820
kind of a

1588
01:45:48,870 --> 01:45:53,340
the dimensional vector right so we're all kind of complex beings we like certain movies for

1589
01:45:53,340 --> 01:45:57,000
some reason we don't like certain movies for some reason and really this is

1590
01:45:57,000 --> 01:46:02,560
simply a very simp very this is simply a model which makes very strong simplifying

1591
01:46:02,560 --> 01:46:09,150
assumptions about the generative process that jet that gave us the movie ratings okay

1592
01:46:09,480 --> 01:46:13,340
but still is a very useful model

1593
01:46:13,390 --> 01:46:17,190
and you can kind of test this model on by

1594
01:46:18,790 --> 01:46:25,540
entering this into the Netflix competition and so forth right so you could kind of test it by looking how well

1595
01:46:25,540 --> 01:46:29,690
it predicts ratings which which it it doesn't which it hasn't

1596
01:46:29,690 --> 01:46:31,730
observed and this does really well okay

1597
01:46:31,740 --> 01:46:39,020
so yeah so I'd like to kind of emphasize that of course all models are wrong

1598
01:46:39,040 --> 01:46:43,520
but some models are useful okay so this is a phrase by

1599
01:46:43,970 --> 01:46:51,110
a famous statistician that I forgot any more right so so that's

1600
01:46:51,110 --> 01:46:55,780
Bayesian machine learning of course today and tomorrow I'd like to talk

1601
01:46:55,780 --> 01:46:57,260
about a particular

1602
01:46:57,300 --> 01:46:58,930
class of

1603
01:46:58,950 --> 01:47:05,410
Bayesian models called Bayesian nonoarametric models and if you're interested in this there's a book

1604
01:47:05,420 --> 01:47:10,060
on Bayesian nonparametrics edited by Hjort et al that just came out last

1605
01:47:12,730 --> 01:47:19,820
so let's start off by kind of telling by saying what is a nonparametric model okay so

1606
01:47:19,820 --> 01:47:24,690
basically I can tell you what it is not it's basically not a parametric model okay

1607
01:47:24,730 --> 01:47:30,260
so what is a parametric model a parametric model is basically a model with a

1608
01:47:30,260 --> 01:47:36,300
certain fixed number of parameters so for example a Gaussian is a parametric model because it has

1609
01:47:36,300 --> 01:47:40,410
a mean and a variance it has two parameters and that's it okay

1610
01:47:40,450 --> 01:47:46,230
so basically a nonparametric model is not a parametric model

1611
01:47:46,240 --> 01:47:50,080
not in the sense that it doesn't have any parameters but rather in the sense

1612
01:47:50,080 --> 01:47:56,020
that it has either an infinite number of parameters of a number of parameters

1613
01:47:56,930 --> 01:48:03,150
changes with the amount of data that is observed okay and is in that sense

1614
01:48:03,150 --> 01:48:08,560
that is not a parametric model a somewhat different way of doing what

1615
01:48:08,560 --> 01:48:13,970
is a nonparametric model is that is a family of distributions that's dense in some large

1616
01:48:13,970 --> 01:48:19,890
space okay I'll come back to this a bit later so the idea yeah I'll come back to this a bit later

1617
01:48:20,410 --> 01:48:25,730
so why are we interested in doing Bayesian nonparametrics

1618
01:48:25,740 --> 01:48:27,910
so let's imagine the following so

1619
01:48:27,910 --> 01:48:31,110
imagine that you're trying to do a classification okay

1620
01:48:31,130 --> 01:48:36,670
so in classification you have some inputs and you have some outputs

1621
01:48:36,790 --> 01:48:42,130
and you may have a space of hy very large space of hypothesis which basically corres

1622
01:48:42,160 --> 01:48:48,580
corresponds to the space of all smooth functions from input space to output space okay

1623
01:48:48,650 --> 01:48:54,690
so your output could be if it's binary then it just consists of either zero or one but your

1624
01:48:54,730 --> 01:48:57,580
input could say an Euclidean

1625
01:48:57,950 --> 01:49:03,760
space so this is the space of all possible smooth functions from your input space

1626
01:49:03,760 --> 01:49:08,430
to your output space and there might be some true underlying function which

1627
01:49:08,470 --> 01:49:12,430
you like to learn about so this function is a function that actually

1628
01:49:12,430 --> 01:49:16,210
tells you for every input what should be the true output okay

1629
01:49:16,230 --> 01:49:23,130
if you do something like in your own networks or linear regression or something then

1630
01:49:23,130 --> 01:49:25,910
basically you're assuming a parametric model

1631
01:49:25,910 --> 01:49:28,240
of this space okay

1632
01:49:28,490 --> 01:49:31,480
and what is a model a model's basically a very

1633
01:49:31,520 --> 01:49:35,740
tiny subset of this space that can be parameterized by your model

1634
01:49:38,710 --> 01:49:43,480
and if you're bayes Bayesian about this you might put a prior on

1635
01:49:43,500 --> 01:49:46,410
on this very small space

1636
01:49:46,670 --> 01:49:53,610
and it's very unlikely that the true function will actually be in the space it's very

1637
01:49:53,610 --> 01:49:58,970
unlikely that your true function is say a linear function because you know

1638
01:49:58,970 --> 01:50:02,410
in many exa in many revolt

1639
01:50:02,410 --> 01:50:05,300
CSSP projection so you you

1640
01:50:05,300 --> 01:50:09,720
so what may be just to explain this more probably so

1641
01:50:09,740 --> 01:50:11,430
you see of

1642
01:50:11,450 --> 01:50:13,320
this is a way to

1643
01:50:13,370 --> 01:50:14,740
get features

1644
01:50:14,780 --> 01:50:18,260
right so for every c p

1645
01:50:18,300 --> 01:50:20,030
projector you get

1646
01:50:20,070 --> 01:50:23,800
quantity it every time and so this is a time series

1647
01:50:23,850 --> 01:50:26,160
and you can classify

1648
01:50:26,180 --> 01:50:31,660
so see you have six of the CSP filters so this means that you can

1649
01:50:31,680 --> 01:50:35,820
involve one hundred twenty eight dimensional time signal

1650
01:50:35,850 --> 01:50:38,850
with six filters so this means that you

1651
01:50:38,890 --> 01:50:43,720
you know you project down this one hundred twenty eight times serious to six times

1652
01:50:43,720 --> 01:50:46,510
here and then you can you know

1653
01:50:46,570 --> 01:50:50,910
classify every actor and this time series and

1654
01:50:50,950 --> 01:50:53,320
to the left or right OK

1655
01:50:53,370 --> 01:50:59,570
and you can do the same for these cssps of course you the classifier and

1656
01:50:59,570 --> 01:51:05,550
and see what comes out of it and you do some model selection

1657
01:51:05,600 --> 01:51:07,050
now you see that

1658
01:51:07,070 --> 01:51:12,430
in many cases CSSP can actually give a large improvement so

1659
01:51:12,470 --> 01:51:17,390
so where's here for this data point for example CSP gives you

1660
01:51:17,450 --> 01:51:23,780
fifty percent fifteen percent error is less than five percent here

1661
01:51:23,820 --> 01:51:28,180
there are some cases words about the same there is no significant difference

1662
01:51:28,870 --> 01:51:30,090
and this some

1663
01:51:30,100 --> 01:51:32,280
points which doesn't work

1664
01:51:32,300 --> 01:51:33,120
this is

1665
01:51:33,140 --> 01:51:37,350
it's all actually a very interesting to look at these cases doesn't work and see

1666
01:51:37,490 --> 01:51:41,220
what is the physiology behind so

1667
01:51:41,320 --> 01:51:46,050
but maybe the bottom line of this idea is that we should not only focus

1668
01:51:46,050 --> 01:51:49,550
on the spatial distribution of the

1669
01:51:49,550 --> 01:51:54,280
the brain signals but we should also look at that time temporal structure at the

1670
01:51:54,280 --> 01:51:55,200
same time

1671
01:51:55,220 --> 01:51:59,300
now there's more there is a parameter to select which is the tower which is

1672
01:51:59,300 --> 01:52:03,260
the delay that i take into account the time scale

1673
01:52:03,280 --> 01:52:05,510
and this needs to be

1674
01:52:05,530 --> 01:52:09,740
chosen appropriately

1675
01:52:12,660 --> 01:52:18,600
there is even another and for those of you at NIPS we had in NIPS

1676
01:52:18,600 --> 01:52:22,700
paper on on this this is just adding some more s is here

1677
01:52:22,720 --> 01:52:28,330
and it's two to give don he can in basically the bottom line is that

1678
01:52:28,410 --> 01:52:31,760
you don't want to specify pre specified this this

1679
01:52:31,820 --> 01:52:36,070
delay of the filter but you what actually want to learn it

1680
01:52:38,390 --> 01:52:42,600
from the data because in the way this is always something if you if you

1681
01:52:42,600 --> 01:52:45,910
do an experiment then you have to look at the spectrum of the individual and

1682
01:52:45,910 --> 01:52:50,780
then you know where to put the band filter and so on so forth so

1683
01:52:50,800 --> 01:52:56,200
it it takes some some knowledge or some some feeling for the physiology

1684
01:52:56,220 --> 01:53:01,910
and it would be nice to have this more automatic OK

1685
01:53:03,410 --> 01:53:04,050
if you

1686
01:53:04,070 --> 01:53:06,220
look at this these are

1687
01:53:06,220 --> 01:53:10,100
this is just the motivation for CSSP and there were not for the talk about

1688
01:53:10,100 --> 01:53:15,450
it because you can just download the paper from the NIPS pages it's also quite

1689
01:53:15,450 --> 01:53:18,140
technical but just to get the idea

1690
01:53:23,050 --> 01:53:24,370
the the

1691
01:53:24,430 --> 01:53:29,450
here the here the idea of this plot was to show that

1692
01:53:31,510 --> 01:53:34,680
of the spectrum that you see here

1693
01:53:34,680 --> 01:53:35,680
a quite

1694
01:53:36,220 --> 01:53:40,050
discriminative so you see these these little

1695
01:53:40,070 --> 01:53:46,660
the gray boxes down here they're very good for discriminating between between two classes

1696
01:53:46,700 --> 01:53:49,600
and you see that there are two bombs in the spectrum so you could in

1697
01:53:49,600 --> 01:53:55,300
principle use both of these pumps so there's a lot of fun higher

1698
01:53:55,390 --> 01:53:59,450
peak but which one should use

1699
01:53:59,450 --> 01:54:02,970
and it could be that that for example

1700
01:54:02,990 --> 01:54:04,680
you know

1701
01:54:04,740 --> 01:54:10,180
that one peak is very good for the discrimination and the other one is not

1702
01:54:12,280 --> 01:54:19,740
but the point in common spatial pattern as this CSS SP is that it automatically

1703
01:54:19,740 --> 01:54:26,100
finds the most discriminative so you don't need is a prespecified this

1704
01:54:26,100 --> 01:54:28,090
on that band

1705
01:54:28,140 --> 01:54:31,780
but you just get it automatically to the point

1706
01:54:33,530 --> 01:54:35,720
actually very convenient

1707
01:54:37,660 --> 01:54:41,680
i will slightly change gears

1708
01:54:45,550 --> 01:54:50,330
let's look at this motorhome and colours

1709
01:54:54,050 --> 01:54:56,510
if we look at the motor cortex than

1710
01:54:56,530 --> 01:54:59,120
this is the place where the mouse is represented

1711
01:54:59,160 --> 01:55:02,490
the hand these individual digits

1712
01:55:02,550 --> 01:55:04,320
the arm

1713
01:55:06,470 --> 01:55:07,490
and so on

1714
01:55:07,510 --> 01:55:10,720
so so the question now is so we've

1715
01:55:10,740 --> 01:55:13,590
address the issue of multiclass

1716
01:55:14,820 --> 01:55:20,550
the the the question for us was was how far can we actually get

1717
01:55:20,680 --> 01:55:24,160
can we discriminate between fingers c

1718
01:55:24,180 --> 01:55:27,030
because if we could be

1719
01:55:27,050 --> 01:55:32,450
discriminate between fingers and joints and things like that then in principle we could is

1720
01:55:32,760 --> 01:55:33,800
we could

1721
01:55:33,800 --> 01:55:36,180
o controller robot i'm saying

1722
01:55:36,220 --> 01:55:37,700
because we have all these

1723
01:55:37,740 --> 01:55:39,280
so to say primitive

1724
01:55:43,660 --> 01:55:49,800
on the other side EEG has limited resolution so so it's actually quite open whether

1725
01:55:49,800 --> 01:55:51,370
we can do it or not

1726
01:55:51,390 --> 01:55:53,070
in real time

1727
01:55:53,070 --> 01:55:57,950
because we don't want to make an averaging over many trials with averaging etc it's

1728
01:55:58,240 --> 01:56:03,100
much easier to distinguish say between gauges

1729
01:56:04,050 --> 01:56:05,370
we just

1730
01:56:05,410 --> 01:56:07,030
show you

1731
01:56:07,050 --> 01:56:10,010
i i show i showed these plots

1732
01:56:10,070 --> 01:56:12,140
before this this was

1733
01:56:13,350 --> 01:56:17,090
o point in time before

1734
01:56:17,100 --> 01:56:19,120
the movement takes place

