1
00:00:00,000 --> 00:00:03,100
divergence or distance

2
00:00:03,110 --> 00:00:06,530
happened in this paper in 1951

3
00:00:06,540 --> 00:00:09,540
and Kullback and Leibler

4
00:00:09,570 --> 00:00:12,110
called it the mean information for

5
00:00:15,010 --> 00:00:21,440
The definition already immediately recognizes that what we are defining there is

6
00:00:21,440 --> 00:00:25,440
actually the expectation of a certain random variable

7
00:00:25,700 --> 00:00:29,200
and you already see the word 'information' there

8
00:00:29,210 --> 00:00:34,840
and in fact in the paper (title) on information and sufficiency

9
00:00:34,870 --> 00:00:42,690
so what was the motivation of Kullback and Leibler to introduce this measure?

10
00:00:42,700 --> 00:00:48,870
OK, so here's the first paragraph from the introduction

11
00:00:48,870 --> 00:00:54,150
generalizes to the abstract case shannon's definition of information

12
00:00:54,160 --> 00:01:00,060
we information is essentially the same as shadow although the motivation was different

13
00:01:00,080 --> 00:01:01,410
in china

14
00:01:01,470 --> 00:01:07,610
apparently has investigated the concept more completely so the last sentence

15
00:01:09,530 --> 00:01:11,970
a big understatement

16
00:01:13,860 --> 00:01:18,490
in fact i will come back to this later in and out

17
00:01:18,530 --> 00:01:22,500
i'll tell you more in detail what was what was it that we never had

18
00:01:22,500 --> 00:01:24,110
to do with this

19
00:01:24,110 --> 00:01:28,230
but you not show

20
00:01:28,240 --> 00:01:32,610
shannon was interested in in what we say the operational

21
00:01:33,160 --> 00:01:39,440
the role of these quantities setting up an engineering problem and then just showing their

22
00:01:39,510 --> 00:01:43,990
this quantity as well as entropy as well as mutual information and the answers to

23
00:01:43,990 --> 00:01:51,510
those problems worse whereas winner did not they he only used one of these quantities

24
00:01:51,510 --> 00:01:54,950
and essentially said that he thought it would be

25
00:01:55,070 --> 00:02:02,860
one interesting exercise to actually optimise this quantity when the variance of the random variable

26
00:02:02,860 --> 00:02:05,390
is fixed

27
00:02:05,400 --> 00:02:09,280
OK so the most

28
00:02:09,310 --> 00:02:13,610
state the most important special case of

29
00:02:13,630 --> 00:02:15,650
relative entropy

30
00:02:15,660 --> 00:02:20,810
is the mutual information so as as we're going to come in later

31
00:02:20,830 --> 00:02:28,660
the relative entropy is a great way to measure the similarity between two distributions and

32
00:02:28,660 --> 00:02:31,790
asymmetric way off

33
00:02:31,820 --> 00:02:36,890
of doing that and i

34
00:02:36,910 --> 00:02:40,610
we can apply that definition two

35
00:02:40,820 --> 00:02:43,620
the special case where the first

36
00:02:43,630 --> 00:02:51,020
distribution is the joint distribution and the second one is the product of marginals

37
00:02:51,990 --> 00:02:54,150
he for

38
00:02:54,170 --> 00:02:57,850
if this random variables x and y

39
00:02:59,150 --> 00:03:03,370
independent then of course we're going to get zero otherwise we're going to get a

40
00:03:06,090 --> 00:03:08,130
and the negative numbers

41
00:03:08,140 --> 00:03:17,570
OK so this is extremely extremely general way of defining of gauging and the degree

42
00:03:17,570 --> 00:03:18,960
of dependence

43
00:03:18,980 --> 00:03:20,500
of course we all

44
00:03:20,540 --> 00:03:25,390
have grown up with correlation coefficient and so on but

45
00:03:25,410 --> 00:03:26,910
for example that one

46
00:03:26,940 --> 00:03:29,720
only applies to

47
00:03:29,730 --> 00:03:32,560
real valued random variables

48
00:03:32,570 --> 00:03:37,670
here x and y can be whatever you want x can be discrete y can

49
00:03:40,120 --> 00:03:44,000
anything you want

50
00:03:44,020 --> 00:03:44,940
you can

51
00:03:44,980 --> 00:03:49,490
you can throw it inside these mutual information

52
00:03:49,610 --> 00:03:53,880
of course you could also define it the other way you could define the

53
00:03:53,900 --> 00:03:59,740
the the relative entropy of the the product of the marginals

54
00:03:59,750 --> 00:04:05,930
relative to the joint distribution and that's what we call a loud tomb information was

55
00:04:05,930 --> 00:04:13,690
mutual this spelled backwards and the it's not as famous as this one as mutual

56
00:04:13,690 --> 00:04:15,520
information but you can still

57
00:04:15,530 --> 00:04:19,290
at least right one paper about it

58
00:04:23,670 --> 00:04:28,010
really relative entropy is the mother of all information measures

59
00:04:28,230 --> 00:04:34,040
mutual information is a special case and to business for this special case of mutual

60
00:04:37,190 --> 00:04:38,060
if i

61
00:04:38,110 --> 00:04:44,940
you have a finite alphabet then you can see in the bottom equation here

62
00:04:44,950 --> 00:04:48,310
there the relative entropy of

63
00:04:49,120 --> 00:04:51,110
of a distribution

64
00:04:51,220 --> 00:04:54,110
with respect to the

65
00:04:54,160 --> 00:04:56,770
equally likely distribution

66
00:04:56,860 --> 00:05:02,210
uniformly distributed on that alphabet that's as the logarithm of

67
00:05:02,220 --> 00:05:03,620
the alphabet size

68
00:05:03,640 --> 00:05:08,250
minus the entropy

69
00:05:08,270 --> 00:05:10,860
OK so you can you can define

70
00:05:10,870 --> 00:05:15,670
and to be in that special case through relative entropy in the more general case

71
00:05:15,780 --> 00:05:19,730
just do it like this you say the entropy of a random variable is as

72
00:05:19,740 --> 00:05:21,740
the mutual information

73
00:05:21,770 --> 00:05:24,720
of that on the with its

74
00:05:24,740 --> 00:05:29,110
if you think about it then it's going to be just the relative entropy of

75
00:05:29,240 --> 00:05:32,730
the joint distribution with the product of the two marginals

76
00:05:32,830 --> 00:05:36,620
the joint distribution when x is equal to x

77
00:05:39,630 --> 00:05:41,170
OK so

78
00:05:41,190 --> 00:05:43,400
but very important

79
00:05:43,410 --> 00:05:44,590
it is

80
00:05:44,600 --> 00:05:53,060
the conditional relative entropy just like regular shannon entropy it's very important to also have

81
00:05:53,090 --> 00:05:55,830
the the conditional entropy

82
00:05:55,850 --> 00:06:00,710
here we can do the same thing and here's the definition and in fact

83
00:06:00,730 --> 00:06:03,650
conditional relative entropy is more general

84
00:06:03,760 --> 00:06:07,270
the relative entropy by

85
00:06:07,330 --> 00:06:09,660
is also the other way around because

86
00:06:09,690 --> 00:06:14,160
this is just a special case of the other definition of the definition of conditional

87
00:06:14,160 --> 00:06:15,340
relative entropy

88
00:06:15,370 --> 00:06:19,190
so let me explain this a little bit what you have here

89
00:06:19,210 --> 00:06:24,650
is three objects so the conditional relative entropy is defined for three objects rather than

90
00:06:24,650 --> 00:06:27,150
the relative entropy is defined for two

91
00:06:27,160 --> 00:06:35,670
here we have the conditional distribution of y given x another one z given x

92
00:06:35,670 --> 00:06:39,860
and px so think of this

93
00:06:39,870 --> 00:06:42,170
all these conditional distributions

94
00:06:42,200 --> 00:06:48,240
sometimes they are called transition kernels or markov kernels things like that you can think

95
00:06:48,240 --> 00:06:54,900
of them even in the absence of having defined joint distributions between

96
00:06:54,910 --> 00:06:55,940
x and z

97
00:06:55,950 --> 00:06:57,200
in end

98
00:06:57,210 --> 00:07:02,220
now y and x but in any case if you find those

99
00:07:02,220 --> 00:07:06,250
we could get the same results by going through pages of mathematics using the sum

100
00:07:06,250 --> 00:07:09,600
product rule is just much easier to look at the picture read the results of

101
00:07:11,430 --> 00:07:15,750
i get a little bit like my background in physics and physics are the single

102
00:07:15,750 --> 00:07:22,040
fireman diagrams doing perturbation expansions in field theory and essentially could do everything without the

103
00:07:22,040 --> 00:07:26,560
diagrams with the diagrams help you set out the calculations very orderly way and therefore

104
00:07:26,560 --> 00:07:31,730
avoid repeating what's the same calculations over and over again and so makes the calculation

105
00:07:31,760 --> 00:07:33,750
very easy and efficient

106
00:07:35,080 --> 00:07:36,530
on some issues efficiency

107
00:07:36,540 --> 00:07:42,430
the diagrams also lead to efficient software implementation so we have algorithms for doing inference

108
00:07:42,430 --> 00:07:46,110
and learning on graphs and you'll see these are expressed in terms of message passing

109
00:07:46,110 --> 00:07:50,620
algorithms express them in terms of sending little messages around on the graph to lots

110
00:07:50,620 --> 00:07:55,690
of local calculations we can build software that reflects that structure that software will be

111
00:07:55,690 --> 00:07:58,010
very efficient

112
00:07:58,050 --> 00:08:02,080
so what mentioned two kinds of graph the things even may mention others

113
00:08:02,150 --> 00:08:06,550
and the talk about use directed graph in order to design the model

114
00:08:06,610 --> 00:08:08,550
and having designed the model

115
00:08:08,560 --> 00:08:12,030
we can turn it into a different kind of graph called the factor graph which

116
00:08:12,030 --> 00:08:13,440
is convenient for

117
00:08:13,440 --> 00:08:17,500
inference and learning using that model

118
00:08:18,330 --> 00:08:20,570
so this graph all about well let's

119
00:08:20,590 --> 00:08:26,140
it starts with an arbitrary distribution over three variables x y and z

120
00:08:26,200 --> 00:08:30,940
and if we apply the product rule twice

121
00:08:30,960 --> 00:08:32,970
we can write this

122
00:08:32,980 --> 00:08:35,950
symmetrical distribution rather nonsymmetric way

123
00:08:35,960 --> 00:08:39,560
as the product of the distribution of each of the three variables x y and

124
00:08:39,560 --> 00:08:44,360
z conditioned on other variables so to do this we chose the particular ordering of

125
00:08:44,360 --> 00:08:48,010
the variables we can represent that by graph

126
00:08:48,020 --> 00:08:53,170
so each of these variables is represented by circle in the graph

127
00:08:53,190 --> 00:08:59,730
and the relationships between the variables are described by links between the between circles between

128
00:08:59,730 --> 00:09:04,000
the nodes in these links carry parasites like for the directed graph

129
00:09:06,140 --> 00:09:10,580
the interpretation of the graph is that it specifies the way in which the joint

130
00:09:10,580 --> 00:09:16,880
distribution decomposes into this product of conditionals in particular there one factor for each variable

131
00:09:16,890 --> 00:09:20,660
and the conditioning variables are the parents in the graph

132
00:09:20,670 --> 00:09:22,950
so there's an narrow from x to y then x is said to be the

133
00:09:22,950 --> 00:09:24,440
parents of y

134
00:09:24,440 --> 00:09:29,600
so this graph has the joint distribution of x y and z decomposes into p

135
00:09:29,600 --> 00:09:33,440
of x given its parents well it doesn't have any parents

136
00:09:33,450 --> 00:09:37,390
p of y given its parents has one parent is x

137
00:09:37,420 --> 00:09:40,660
pz given its parents which x y

138
00:09:40,670 --> 00:09:44,270
so that graph is equivalent to that decomposition

139
00:09:45,450 --> 00:09:48,610
we have a rule about these graphs in order for this to to work properly

140
00:09:48,910 --> 00:09:52,870
there must be any directed cycles it is if you follow any loops in the

141
00:09:53,690 --> 00:09:56,780
it was never be the case that you can go around the loop following the

142
00:09:56,780 --> 00:09:58,480
direction of the arrows

143
00:09:58,530 --> 00:10:02,000
just related to this ordering of the variables

144
00:10:02,040 --> 00:10:05,870
so directed graph

145
00:10:06,060 --> 00:10:11,440
more generally has the property that are missing links and it's the absence of links

146
00:10:11,440 --> 00:10:16,160
in the graph really conveys information so in this example this is any distribution of

147
00:10:16,230 --> 00:10:19,960
three variables is described by this graph this graph is fully connected every variable is

148
00:10:19,960 --> 00:10:21,480
connected to every other

149
00:10:21,520 --> 00:10:26,050
once we start emitting links here's a graph of the seven variables were not every

150
00:10:26,050 --> 00:10:29,770
potential link is is present makes missing is missing links

151
00:10:30,250 --> 00:10:32,560
tell us about independence properties

152
00:10:34,540 --> 00:10:40,600
this graph with missing links therefore describes more restrictive family of distributions therefore encode some

153
00:10:40,600 --> 00:10:46,470
specific knowledge is still looking all possible distributions looking the narrow family distributions

154
00:10:46,490 --> 00:10:47,870
so we can write down

155
00:10:48,800 --> 00:10:54,310
factorizations specified by the graph the same way as forces seven variables

156
00:10:54,320 --> 00:10:56,790
with seven nodes for each variable

157
00:10:56,820 --> 00:10:59,820
and so the joint distribution is the product of the conditional

158
00:10:59,840 --> 00:11:01,570
one for each variable

159
00:11:01,580 --> 00:11:04,440
and these conditionals conditional on its parents

160
00:11:05,070 --> 00:11:07,060
p of x one to x seven

161
00:11:07,100 --> 00:11:10,540
is written as the product of p x one given its parents there aren't any

162
00:11:10,540 --> 00:11:12,210
solution x two and x three

163
00:11:12,250 --> 00:11:17,120
if x four given its parents and parents of x four x one x two

164
00:11:17,120 --> 00:11:21,480
x three px one given x one x two x three and so on for

165
00:11:21,480 --> 00:11:23,620
the remaining factors

166
00:11:23,650 --> 00:11:28,150
and that's i haven't said whether these are continuous variables of discrete variables or the

167
00:11:28,150 --> 00:11:31,890
gas distribution gamma distributed or whatever this is

168
00:11:31,940 --> 00:11:34,250
really describing the whole family

169
00:11:34,250 --> 00:11:41,590
of distributions the family of distributions that have a certain independence properties

170
00:11:41,620 --> 00:11:44,190
so we can use directed graphs two

171
00:11:44,190 --> 00:11:47,450
to specify models and capture prior knowledge

172
00:11:47,460 --> 00:11:50,850
and the other thing that i won't dwell on too long but the direction of

173
00:11:50,850 --> 00:11:55,810
the arrows can be thought of as representing causality so if we think about little

174
00:11:55,810 --> 00:12:00,890
boxes of fruit example became the first of all we in the the the physical

175
00:12:00,890 --> 00:12:04,900
process in the physical world is we first of all choose box and having chosen

176
00:12:04,910 --> 00:12:09,170
box regions side which choose a piece of fruit that's the sort of ordering relationship

177
00:12:09,170 --> 00:12:10,830
we have in the physical process

178
00:12:10,910 --> 00:12:14,550
the machine learning we want to go in the opposite direction we observed the piece

179
00:12:14,560 --> 00:12:17,770
of the identity of the piece of fruit we want to infer which box it

180
00:12:17,770 --> 00:12:23,410
came from and so that's a typically each of these graphs is examples later the

181
00:12:23,410 --> 00:12:28,810
arrow specify the directions of causal physical process in the world and they we typically

182
00:12:28,810 --> 00:12:30,770
observe notes down here

183
00:12:30,800 --> 00:12:35,420
we want to infer posterior distributions of nodes higher the graph yes question

184
00:12:35,520 --> 00:12:43,050
yes we'll see examples of temporal

185
00:12:43,070 --> 00:12:46,660
what you show some examples later actually we have time information creates a lot of

186
00:12:46,660 --> 00:12:51,150
questions or questions

187
00:12:59,790 --> 00:13:06,650
yes indeed there are there are other kinds of graphs there is that the undirected

188
00:13:06,650 --> 00:13:09,840
graph for example that don't specify

189
00:13:09,890 --> 00:13:14,380
sort of soft relationships between variables without giving them the status of one being the

190
00:13:14,380 --> 00:13:19,880
cause of another and also we might introduce variables that don't necessarily have an interpretation

191
00:13:19,880 --> 00:13:23,490
in the physical world the unobserved variables but there is part of the modelling process

192
00:13:23,550 --> 00:13:24,960
we don't necessarily

193
00:13:24,970 --> 00:13:28,350
we may or may not wish to interpret those is being

194
00:13:28,380 --> 00:13:31,380
some actual piece of physics in the in the real world

195
00:13:31,380 --> 00:13:37,610
you explained how many variables and then you at some point discourse flattens out and

196
00:13:37,610 --> 00:13:39,400
then you will say well

197
00:13:39,650 --> 00:13:40,970
i'm i'm

198
00:13:41,190 --> 00:13:46,420
explain almost all the variants but one hundred sixty two hundred twenty six

199
00:13:46,510 --> 00:13:48,880
only could use under twenty six dimensions

200
00:13:48,900 --> 00:13:53,840
if you look at the course in LSI either much much flat so you can

201
00:13:53,840 --> 00:13:58,340
use that as a heuristic for choosing the number of dimensions so

202
00:13:58,380 --> 00:14:00,780
as far as i know it's always done

203
00:14:01,840 --> 00:14:06,070
by tuning people try different values and choose the best

204
00:14:06,190 --> 00:14:17,340
values like a hundred two hundred and three hundred very frequently used

205
00:14:17,340 --> 00:14:21,340
reduction to one hundred dimensions to other dimensions three hundred much which is quite amazing

206
00:14:21,340 --> 00:14:22,690
because you

207
00:14:22,720 --> 00:14:25,340
if you think of a space that has

208
00:14:25,360 --> 00:14:27,780
tens of thousands of

209
00:14:27,820 --> 00:14:32,670
dimensions are reduced to two hundred that's that's a lot of reduction but

210
00:14:32,690 --> 00:14:34,970
that seems to be good

211
00:14:51,630 --> 00:14:52,900
and no

212
00:14:52,920 --> 00:15:00,740
no i think there really so different that as far as i know nobody has

213
00:15:00,860 --> 00:15:02,950
tried to reconcile the

214
00:15:02,970 --> 00:15:11,170
well maybe i should answer your question so this is so that i mean this

215
00:15:11,170 --> 00:15:15,010
type of analysis there are many more ways of doing it and one of them

216
00:15:16,760 --> 00:15:22,420
other models you talk about the semantic

217
00:15:22,440 --> 00:15:24,840
there's a allocation models

218
00:15:27,470 --> 00:15:29,240
they are

219
00:15:29,240 --> 00:15:32,490
so this kind of progression from latent semantic indexing

220
00:15:32,550 --> 00:15:38,030
which i presented here probabilistic latent semantic indexing with which which has a better probabilistic

221
00:15:38,030 --> 00:15:41,550
foundation to these ideas models

222
00:15:41,550 --> 00:15:43,530
that that are even

223
00:15:44,970 --> 00:15:49,400
i mean have a even better modeling of the

224
00:15:49,420 --> 00:15:54,260
of the domain because probabilistic latent semantic indexing also some problems in that it does

225
00:15:54,260 --> 00:15:55,450
not really

226
00:15:55,900 --> 00:15:59,320
b model of the document generation process

227
00:15:59,940 --> 00:16:01,990
i'm not sure

228
00:16:03,780 --> 00:16:09,030
i so that the relationship between these models as far as i can see

229
00:16:14,360 --> 00:16:17,280
one advantage of this is that it's much more

230
00:16:17,280 --> 00:16:22,720
efficient than display allocation these layers colour allocation i'm not sure whether anybody as far

231
00:16:22,720 --> 00:16:27,450
as it can be pretty hairy to get to work for large collections and get

232
00:16:27,450 --> 00:16:30,950
good results and this is this is a unique solution

233
00:16:31,010 --> 00:16:38,050
it's very well understood mathematics so that's the advantages

234
00:16:45,820 --> 00:16:50,400
yeah that's a good question because if you actually did the decomposition

235
00:16:50,530 --> 00:16:53,860
as i claimed that you should do

236
00:16:53,880 --> 00:16:57,360
this decomposition i mean you can actually do that for

237
00:16:57,510 --> 00:17:01,130
very large matrices so what you need to do instead is you need to do

238
00:17:01,130 --> 00:17:06,070
it incrementally so that compute one singular value and one single dimension after the other

239
00:17:06,070 --> 00:17:07,010
and then

240
00:17:07,570 --> 00:17:12,420
you tell the system i want to compute a hundred and will only computer hundred

241
00:17:12,440 --> 00:17:16,240
and there are iterative algorithms for doing that

242
00:17:16,470 --> 00:17:21,650
and it's called the land trust me

243
00:17:23,940 --> 00:17:28,840
it's basically iterative method for computing the singular value decomposition up to a certain

244
00:17:34,740 --> 00:17:37,320
she was

245
00:17:37,320 --> 00:17:38,970
so i

246
00:17:38,990 --> 00:17:42,690
i've been in

247
00:17:50,970 --> 00:17:55,800
what would

248
00:18:11,440 --> 00:18:18,220
what's the comparison again LSI on the one hand and the people who

249
00:18:18,260 --> 00:18:29,570
it's not for everyone

250
00:18:31,050 --> 00:18:32,720
and then

251
00:18:32,740 --> 00:18:33,880
it was yes

252
00:18:35,220 --> 00:18:38,010
it may be that the

253
00:18:38,030 --> 00:18:45,420
but i'm not sure with a

254
00:18:45,470 --> 00:18:50,550
i recommend semantic indexing for that because it's a

255
00:18:50,550 --> 00:18:53,530
i mean it's really

256
00:18:53,550 --> 00:18:59,170
he had orthogonal dimensions and the topic is really define a combination of thousands of

257
00:18:59,170 --> 00:19:05,190
different dimensions and it's not clear to some additional work work to actually find the

258
00:19:05,190 --> 00:19:08,240
topics to extract topics out of

259
00:19:08,240 --> 00:19:09,650
science so

260
00:19:09,670 --> 00:19:13,300
so i could only be the first step in that problem which something else

261
00:19:16,110 --> 00:19:18,320
the claim is certainly that

262
00:19:18,380 --> 00:19:24,090
the dirichlet methods can do that directly personal skeptical that they

263
00:19:24,130 --> 00:19:26,010
work out of the box and

264
00:19:26,030 --> 00:19:31,970
once you're done with hand holding several aspects of them i'm not sure whether you're

265
00:19:31,970 --> 00:19:35,400
better off than some other methods i mean if you if that's not the focus

266
00:19:35,400 --> 00:19:39,190
of europe and you just want to fight extract the topics i would just use

267
00:19:39,210 --> 00:19:40,610
personally clustering

268
00:19:40,630 --> 00:19:44,280
simple clustering of

269
00:19:44,380 --> 00:19:47,380
question i guess the whole issue of

270
00:19:48,970 --> 00:19:51,940
well this is a

271
00:19:51,950 --> 00:19:53,670
so it seems that

272
00:19:53,690 --> 00:19:54,710
this is

273
00:19:54,760 --> 00:19:57,720
try to capture something

274
00:19:58,670 --> 00:20:01,970
causal relationships between to the

275
00:20:04,280 --> 00:20:06,550
the next generation

276
00:20:07,720 --> 00:20:09,220
relations between two

277
00:20:09,690 --> 00:20:12,170
these approaches

278
00:20:12,320 --> 00:20:14,940
four how

279
00:20:14,940 --> 00:20:20,400
compress space doesn't necessarily mean it's going to be easy to

280
00:20:20,420 --> 00:20:22,070
what is actually

281
00:20:22,070 --> 00:20:26,300
maybe i would like to find an automatic way of saying well i want to

282
00:20:26,740 --> 00:20:29,500
obtain the ten percent first

283
00:20:34,480 --> 00:20:38,390
basic would like to specify such a fraction beforehand

284
00:20:38,410 --> 00:20:41,780
and we want to make everything automatic after we've got the computer to do it

285
00:20:41,780 --> 00:20:45,260
for us and it's too late the computer the hard work and we just at

286
00:20:45,270 --> 00:20:48,100
the parameters

287
00:20:48,810 --> 00:20:50,610
the idea is actually quite simple

288
00:20:50,620 --> 00:20:55,960
just take an adaptive hyperplane rather than the to the x equals one we say

289
00:20:55,970 --> 00:20:57,990
stated that sequel wrote

290
00:20:58,030 --> 00:21:02,240
and then this threshold rho adaptive

291
00:21:02,280 --> 00:21:04,920
and depending on how this is picked

292
00:21:04,960 --> 00:21:08,070
i will get an automatic solutions

293
00:21:11,410 --> 00:21:13,680
same problem as before

294
00:21:13,690 --> 00:21:18,210
the only difference is that we have now is we have minus in times new

295
00:21:18,210 --> 00:21:19,890
times wrote

296
00:21:19,920 --> 00:21:20,890
it turns out

297
00:21:20,910 --> 00:21:22,780
this is just the right way

298
00:21:24,020 --> 00:21:31,330
penalize sister they get a fraction of new points essentially which are classified wrong

299
00:21:31,350 --> 00:21:33,050
this is essentially because well

300
00:21:33,070 --> 00:21:37,880
the one thing that you can get wrong about i other points exactly on the

301
00:21:39,530 --> 00:21:40,800
if you try

302
00:21:40,840 --> 00:21:45,050
and insist on those points you get the rather nasty optimisation problem if you say

303
00:21:45,050 --> 00:21:48,630
well you don't really care about how many points it exactly on the margin yet

304
00:21:48,630 --> 00:21:50,860
and my second problem

305
00:21:50,940 --> 00:21:54,810
so a small change became change from dramatically

306
00:21:56,440 --> 00:21:58,480
the difference from a to before is

307
00:21:58,520 --> 00:22:02,660
well essentially objective function is the same in the dual problem but you get one

308
00:22:02,660 --> 00:22:09,340
additional constraint namely some of the alpha i honest because new terms in

309
00:22:09,550 --> 00:22:13,810
and then the linear term in the objective function vanishes because well we know what

310
00:22:13,810 --> 00:22:15,780
this has to be

311
00:22:15,830 --> 00:22:16,910
so if you

312
00:22:16,950 --> 00:22:19,740
look back that was the previous problem

313
00:22:19,760 --> 00:22:22,470
with the linear term in the objective function

314
00:22:22,480 --> 00:22:25,730
and no constraint

315
00:22:25,750 --> 00:22:28,510
and now

316
00:22:28,520 --> 00:22:29,670
we have

317
00:22:29,750 --> 00:22:35,460
non linear term in the objective but we have a constraint

318
00:22:42,350 --> 00:22:43,700
this theorem is

319
00:22:44,800 --> 00:22:47,810
at most a fraction of new points

320
00:22:47,830 --> 00:22:50,480
well lie on the wrong side of the margin

321
00:22:50,520 --> 00:22:53,460
in other words the most that many points

322
00:22:54,770 --> 00:22:57,320
will satisfy

323
00:22:57,340 --> 00:22:59,620
OK so that's the top which would say

324
00:22:59,710 --> 00:23:01,420
f x i

325
00:23:01,430 --> 00:23:04,230
lacey could list and wrote

326
00:23:04,280 --> 00:23:08,740
and that should be f of fixi greater than one

327
00:23:09,430 --> 00:23:12,810
it's the same as it stands here

328
00:23:12,830 --> 00:23:14,110
it is written for classes

329
00:23:15,600 --> 00:23:19,020
you may need to create the new handouts

330
00:23:19,110 --> 00:23:21,950
so how do we prove this

331
00:23:22,820 --> 00:23:24,390
also in the limit

332
00:23:24,430 --> 00:23:26,810
the number of points

333
00:23:26,860 --> 00:23:32,710
lying in the fraction of points lie except on the margin because to zero

334
00:23:32,750 --> 00:23:35,170
so proof idea works as follows

335
00:23:35,210 --> 00:23:38,410
at the optimal solution

336
00:23:38,430 --> 00:23:39,940
we've got

337
00:23:40,000 --> 00:23:42,810
some points here

338
00:23:43,800 --> 00:23:45,790
some points there

339
00:23:49,620 --> 00:23:53,710
now that the optimal solution to make a small parallel shift

340
00:23:53,720 --> 00:23:54,980
of the margin

341
00:23:54,990 --> 00:23:57,240
all the way

342
00:23:57,260 --> 00:23:59,970
now if i move it

343
00:23:59,980 --> 00:24:01,440
closer to the origin

344
00:24:01,490 --> 00:24:05,020
the origin is here

345
00:24:06,770 --> 00:24:12,250
those points which lie on the margin or on the

346
00:24:15,050 --> 00:24:17,570
those points here

347
00:24:17,720 --> 00:24:19,230
for those

348
00:24:19,280 --> 00:24:23,300
my slack variables will be decreased

349
00:24:23,490 --> 00:24:27,300
it will be decreased by

350
00:24:33,110 --> 00:24:35,000
at the same time

351
00:24:36,630 --> 00:24:38,020
delta rho

352
00:24:38,040 --> 00:24:39,820
and it

353
00:24:39,830 --> 00:24:42,160
in one of of those

354
00:24:42,160 --> 00:24:46,450
so my improvement here is a minus times delta rho

355
00:24:46,470 --> 00:24:50,710
but at the same time i have to pay a price of in times new

356
00:24:50,710 --> 00:24:53,640
times filter

357
00:24:54,390 --> 00:25:00,410
of course optimality after seeing that into in modern times filter rho

358
00:25:00,420 --> 00:25:02,070
this less equal

359
00:25:02,870 --> 00:25:05,470
what they have to pay for it

360
00:25:05,480 --> 00:25:08,610
like as if i go in the other direction

361
00:25:08,640 --> 00:25:10,860
those points which sat on the margin

362
00:25:10,870 --> 00:25:13,640
or which are already in margin errors

363
00:25:14,010 --> 00:25:17,020
i will come into play as well

364
00:25:17,060 --> 00:25:18,710
so i think it

365
00:25:18,720 --> 00:25:22,480
one minus and plus so the points which sits on the right-hand side which i

366
00:25:22,480 --> 00:25:24,730
have to pay say the least efficient after the bit

367
00:25:24,780 --> 00:25:28,780
if you work out these optimality conditions you'll get

368
00:25:29,490 --> 00:25:32,190
the fraction of new terms in points

369
00:25:32,220 --> 00:25:33,940
essentially will

370
00:25:33,990 --> 00:25:36,770
end up lying on the wrong side of the margin

371
00:25:36,780 --> 00:25:38,870
and then

372
00:25:38,890 --> 00:25:43,820
wind limit those fractions will become exact well because

373
00:25:44,640 --> 00:25:48,370
a hyperplane is a set of measure zero

374
00:25:48,390 --> 00:25:51,010
so this you get an answer distribution

375
00:25:51,020 --> 00:25:56,500
in which is a set of measure zero can possibly have well non-zero measure

376
00:25:59,750 --> 00:26:02,920
just by the fact that you get convergence

377
00:26:02,970 --> 00:26:05,780
you will get the fact that well

378
00:26:05,830 --> 00:26:10,100
the fraction of those points will go to zero is just like

379
00:26:11,180 --> 00:26:14,460
we seem to what you ask is how what's the probability for point lying on

380
00:26:14,460 --> 00:26:22,220
the on a certain light line as opposed to lie anywhere else

381
00:26:22,480 --> 00:26:30,720
you could

382
00:26:31,470 --> 00:26:36,130
to classification again

383
00:26:36,190 --> 00:26:40,270
all the time that

384
00:26:43,180 --> 00:26:44,550
let's look at

385
00:26:44,560 --> 00:26:48,540
the connection between gas process and support vector machines

386
00:26:51,390 --> 00:26:56,620
classification what we have is that we looked at the log of the y i

387
00:26:56,620 --> 00:26:58,050
given x on theta

388
00:26:58,070 --> 00:27:01,830
and this normalisation here

389
00:27:01,840 --> 00:27:04,680
now what i'm trying to convince areas

390
00:27:04,680 --> 00:27:11,060
that the support vector machines this is actually the margin you're looking at

391
00:27:12,050 --> 00:27:16,280
it looks considerably more complicated than what we're not showed you

392
00:27:16,290 --> 00:27:18,130
so open

393
00:27:18,140 --> 00:27:19,790
that will basically you say

394
00:27:19,810 --> 00:27:21,760
well how far

395
00:27:23,800 --> 00:27:25,800
a point x

396
00:27:25,860 --> 00:27:28,780
ly away from the separating hyperplane

397
00:27:28,780 --> 00:27:30,710
and i'm showing the big east

398
00:27:30,720 --> 00:27:33,390
OK let's match

399
00:27:33,420 --> 00:27:38,280
but this just for the moment this

400
00:27:38,370 --> 00:27:43,010
think about what it takes in order to classify something correctly

401
00:27:43,040 --> 00:27:46,780
well if i'm estimating p of y given x and theta

402
00:27:46,880 --> 00:27:48,690
thing in order to

403
00:27:49,480 --> 00:27:54,380
why i correctly for example i need to ensure that p of why i given

404
00:27:54,380 --> 00:28:00,980
x i is larger than all the other p of y given x size

405
00:28:01,000 --> 00:28:06,810
in other words the observed label is more likely than all of the other guys

406
00:28:06,810 --> 00:28:09,340
to the to the port minus one

407
00:28:09,600 --> 00:28:11,180
which is basically

408
00:28:11,190 --> 00:28:14,780
which relates to the the that the identity

409
00:28:14,790 --> 00:28:19,660
so it means that if we define a dot product the identity then we can

410
00:28:19,680 --> 00:28:23,430
propagated to the the reaction action to any point on the manifold

411
00:28:23,440 --> 00:28:25,490
but there is another condition

412
00:28:25,510 --> 00:28:29,990
the dot product the identity should be invited by the isotropy group

413
00:28:29,990 --> 00:28:32,750
which means that if we apply rotation

414
00:28:32,810 --> 00:28:38,360
then the the point doesn't change the

415
00:28:38,530 --> 00:28:39,650
that should be

416
00:28:39,680 --> 00:28:40,490
the same

417
00:28:40,490 --> 00:28:43,020
and we can show we can try to find all

418
00:28:43,040 --> 00:28:47,740
the rotational invariance kind of product and we end up with this kind of formula

419
00:28:49,010 --> 00:28:54,340
beta the conditions be on the talk here is just to ensure that this is

420
00:28:54,370 --> 00:28:56,000
positive define it

421
00:28:56,010 --> 00:28:57,460
so we can we can

422
00:28:57,480 --> 00:28:58,070
that's where we

423
00:28:58,470 --> 00:29:01,000
we have all the fine

424
00:29:01,010 --> 00:29:04,700
romanian metric on the space of kansas and we can show that the child is

425
00:29:04,700 --> 00:29:08,610
x can be expressed through the matrix exponential

426
00:29:08,620 --> 00:29:10,390
using this from

427
00:29:10,450 --> 00:29:11,630
and the distance

428
00:29:11,640 --> 00:29:16,560
it's is giving in in across from using this formula well this is the logo

429
00:29:16,630 --> 00:29:19,950
of symmetric matrices

430
00:29:19,960 --> 00:29:21,210
OK so i

431
00:29:21,310 --> 00:29:27,960
just just a few references this metric all i should say this family of metric

432
00:29:28,540 --> 00:29:32,900
was some of these metrics were already proposed so this is the

433
00:29:32,910 --> 00:29:40,040
fisher information metric which is well on information geometry but it's and which was rediscovered

434
00:29:40,040 --> 00:29:43,360
for surpassed processing over the last years

435
00:29:43,640 --> 00:29:49,370
actually what is interesting is that very few works proposed metrics was be that different

436
00:29:49,370 --> 00:29:54,640
from there is one work which is obtained by embedding the

437
00:29:54,650 --> 00:29:59,020
comment metrics and the mean into homogeneous coordinates

438
00:29:59,110 --> 00:30:04,310
it's actually we can generalize this construction and obtained the title

439
00:30:04,380 --> 00:30:09,840
well this is an embedding of the of the space of into a high dimensional

440
00:30:11,200 --> 00:30:13,660
which has this metric

441
00:30:13,740 --> 00:30:18,470
and so we can generalize this construction tool in the box and press

442
00:30:18,480 --> 00:30:20,880
any integral there

443
00:30:20,890 --> 00:30:24,900
depending on the dimension of the space we consider

444
00:30:24,970 --> 00:30:28,520
OK so what is the difference with the start metric

445
00:30:28,540 --> 00:30:32,910
so the first thing we can do is to try to patrol the geodesic that

446
00:30:32,910 --> 00:30:35,730
goes from this time so here to to the big ten so there

447
00:30:35,730 --> 00:30:36,850
this is the

448
00:30:36,860 --> 00:30:40,080
interpolation along the geodesic for

449
00:30:40,100 --> 00:30:41,680
the euclidean metric

450
00:30:41,690 --> 00:30:43,160
so the

451
00:30:43,220 --> 00:30:46,840
and operation on the coefficients and this is here the interpolation

452
00:30:46,880 --> 00:30:51,070
with the fine environment in metric and we can show that

453
00:30:51,090 --> 00:30:52,020
it seems to be

454
00:30:52,040 --> 00:30:57,210
malls schools on the on the orientation what is interesting is that the

455
00:30:57,270 --> 00:31:00,640
determinant which is this curve there

456
00:31:00,650 --> 00:31:01,560
it is

457
00:31:01,820 --> 00:31:05,030
interpolated in in one

458
00:31:06,610 --> 00:31:08,040
norway there

459
00:31:08,060 --> 00:31:12,860
well it go through mexico if winter determine to some kind of probability this is

460
00:31:12,860 --> 00:31:14,290
kind of

461
00:31:15,460 --> 00:31:18,160
the euclidean which

462
00:31:18,180 --> 00:31:23,750
OK so now that we have a metric how to the statistics

463
00:31:23,760 --> 00:31:25,650
so from the general point of view

464
00:31:25,680 --> 00:31:29,890
there were many metric is giving us a forum from which

465
00:31:29,950 --> 00:31:35,000
with which we can define the intrinsic probability density functions on riemannian manifold

466
00:31:35,090 --> 00:31:37,630
so this means that we can define interesting

467
00:31:37,710 --> 00:31:39,590
quantities like entropy

468
00:31:39,630 --> 00:31:46,750
we can define the variance which is expected expected expected squared distance this is an

469
00:31:47,680 --> 00:31:52,330
with only two but we can define the mean value with an integral

470
00:31:52,340 --> 00:31:56,490
because we have this would be an interval was result in the manifold manifold is

471
00:31:56,490 --> 00:31:57,410
not linear

472
00:31:57,440 --> 00:31:59,070
so this is not possible

473
00:31:59,110 --> 00:32:03,460
so there are different ways to compute the mean what to generalise to me

474
00:32:03,480 --> 00:32:06,160
and the most

475
00:32:06,230 --> 00:32:11,740
use definitions is the pressure of culture medium which is basically trying to minimise the

476
00:32:12,950 --> 00:32:16,430
in the manifold which is just equally complete

477
00:32:16,570 --> 00:32:19,610
we can show that it is the

478
00:32:19,620 --> 00:32:23,460
ratio comes is characterized as exponential by some

479
00:32:23,540 --> 00:32:26,950
this means that if you take the main point i knew the everything in the

480
00:32:26,950 --> 00:32:27,900
tangent space

481
00:32:28,100 --> 00:32:30,620
and i mean and mean value is new

482
00:32:30,640 --> 00:32:32,910
which means that it corresponds to

483
00:32:32,990 --> 00:32:36,000
the usual it's it's somehow the mostly male

484
00:32:36,060 --> 00:32:40,250
representation of the manifold for this kind of distribution

485
00:32:40,270 --> 00:32:43,870
and what's interesting is that once we have solved for the mean that we can

486
00:32:43,870 --> 00:32:49,020
go to to the tangent space and define all the moments like arrangement mattresses or

487
00:32:49,030 --> 00:32:50,450
higher order

488
00:32:50,460 --> 00:32:53,360
the moment that

489
00:32:53,370 --> 00:32:55,150
so this is

490
00:32:55,230 --> 00:32:56,740
covariance matrices

491
00:32:56,850 --> 00:33:02,690
metrics in on on the riemannian manifold is some kind of object while symmetric matrix

492
00:33:02,690 --> 00:33:04,720
which live in the tangent space

493
00:33:04,760 --> 00:33:06,460
and we can do some PCA

494
00:33:06,740 --> 00:33:09,280
and some simple analysis

495
00:33:09,360 --> 00:33:12,640
we can also define some generalisation of the gaussian

496
00:33:13,160 --> 00:33:17,950
with which we can compute and some other this distance but they will not

497
00:33:19,120 --> 00:33:22,220
in two that in this talk

498
00:33:22,230 --> 00:33:26,680
so just a few examples this is an example

499
00:33:26,680 --> 00:33:29,340
in the language like c it's called mala

500
00:33:29,350 --> 00:33:31,030
or language like

501
00:33:32,930 --> 00:33:34,800
called new

502
00:33:34,810 --> 00:33:36,450
the large table

503
00:33:37,960 --> 00:33:41,360
to create a larger table

504
00:33:41,450 --> 00:33:43,540
remove the items

505
00:33:43,550 --> 00:33:50,260
from the old table

506
00:33:55,580 --> 00:33:56,440
then we

507
00:33:56,460 --> 00:34:00,930
three the OK

508
00:34:01,060 --> 00:34:05,420
let's do an example

509
00:34:10,860 --> 00:34:16,320
let's say i have over here a

510
00:34:16,330 --> 00:34:18,980
table of size one

511
00:34:19,810 --> 00:34:24,790
attempting to begin with and i do an insert

512
00:34:24,870 --> 00:34:27,780
sorry do is

513
00:34:27,800 --> 00:34:30,340
i stick it in the table that's

514
00:34:30,350 --> 00:34:33,110
OK so we're not going to do with hashing is going to do it

515
00:34:33,130 --> 00:34:37,020
as if i just had table is filling up with elements to to abstract

516
00:34:37,190 --> 00:34:41,640
the problem would work with hashing it will work with any kind of excised data

517
00:34:43,440 --> 00:34:46,100
i insert again

518
00:34:46,150 --> 00:34:47,890
groups doesn't fit

519
00:34:49,150 --> 00:34:55,420
OK so what i do is i create

520
00:34:55,470 --> 00:34:56,530
and new

521
00:34:56,960 --> 00:35:01,530
much space

522
00:35:01,630 --> 00:35:04,000
create new

523
00:35:05,210 --> 00:35:07,400
table of of size two

524
00:35:08,320 --> 00:35:10,270
doubling the size

525
00:35:10,320 --> 00:35:12,290
my copy

526
00:35:12,340 --> 00:35:14,140
the old value into new

527
00:35:14,150 --> 00:35:15,340
three this one

528
00:35:15,390 --> 00:35:20,470
and now i can insert it into

529
00:35:20,490 --> 00:35:24,280
why do it again

530
00:35:24,290 --> 00:35:26,320
another overflow

531
00:35:29,290 --> 00:35:31,060
so now i make table

532
00:35:31,080 --> 00:35:33,220
size four

533
00:35:33,390 --> 00:35:36,870
how these guys and

534
00:35:36,890 --> 00:35:40,000
and then i certain number three

535
00:35:40,020 --> 00:35:44,790
sir here

536
00:35:49,870 --> 00:35:52,550
i guess i should be you know marks

537
00:35:52,570 --> 00:35:53,850
so far

538
00:35:53,860 --> 00:35:54,810
OK what

539
00:35:54,900 --> 00:35:56,000
and i do

540
00:35:57,640 --> 00:36:03,230
now make one of size a

541
00:36:06,730 --> 00:36:13,940
companies or

542
00:36:14,150 --> 00:36:18,570
and now i can certify

543
00:36:27,610 --> 00:36:33,850
i certainly understand the basic idea so whenever i overflow and create table twice the

544
00:36:38,200 --> 00:36:41,100
let's do a quick analysis of this

545
00:36:41,120 --> 00:36:45,570
so we have a sequence

546
00:36:48,430 --> 00:36:51,670
insertion operations

547
00:36:54,260 --> 00:36:56,980
what is the worst case cost

548
00:37:00,260 --> 00:37:03,280
and sir

549
00:37:05,730 --> 00:37:08,260
what's the worst case for any

550
00:37:10,470 --> 00:37:11,590
yes water and

551
00:37:11,600 --> 00:37:14,260
order and

552
00:37:14,270 --> 00:37:19,300
whatever the overhead is copying if we counted this one basically an

553
00:37:19,390 --> 00:37:22,820
in plus one copy all those and

554
00:37:23,600 --> 00:37:24,700
four and

555
00:37:27,210 --> 00:37:31,070
so therefore if i have a i have and those are the worst case

556
00:37:32,100 --> 00:37:34,610
of an answer

557
00:37:35,070 --> 00:37:39,380
is equal to

558
00:37:39,460 --> 00:37:41,640
ten times more

559
00:37:41,650 --> 00:37:43,180
which is water

560
00:37:43,200 --> 00:37:44,350
and and square

561
00:37:44,520 --> 00:37:57,080
any questions

562
00:37:57,140 --> 00:38:06,740
it makes sense

563
00:38:23,640 --> 00:38:26,420
not all of them can be worse case good

564
00:38:27,030 --> 00:38:30,030
and that this is totally wrong analysis

565
00:38:30,120 --> 00:38:32,930
just because one can be

566
00:38:32,980 --> 00:38:36,880
worst case or and doesn't mean and necessarily or and

567
00:38:36,890 --> 00:38:40,140
OK so this is totally wrong analysis and search

568
00:38:40,280 --> 00:38:44,340
in fact take

569
00:38:44,440 --> 00:38:48,100
or entirely the worst case

570
00:38:48,100 --> 00:38:52,440
that is parallel to this curve will simply not change anything within the article

571
00:38:53,930 --> 00:39:00,690
this is often portrayed in this two-dimensional where this is possible velocity locally are two

572
00:39:02,270 --> 00:39:04,150
and vx and vy y

573
00:39:04,170 --> 00:39:06,480
and a single linear constraint

574
00:39:06,500 --> 00:39:11,570
which again that's all these emotions have the correct

575
00:39:12,770 --> 00:39:14,740
but in the end

576
00:39:14,870 --> 00:39:18,690
visual system can locally in system can locally figure out the motion

577
00:39:18,700 --> 00:39:23,080
so the emotions and along this line but in general we just have the linear

578
00:39:23,110 --> 00:39:27,940
constraint or a similar constraint in two dimensions

579
00:39:28,000 --> 00:39:34,600
so this oppose this was realized very early on emotion perception and

580
00:39:34,620 --> 00:39:41,290
these researchers most famously until recent her book decided to get rid this person is

581
00:39:41,290 --> 00:39:41,610
about to

582
00:39:43,710 --> 00:39:48,460
so this suggested that the visual system that is to minimize the cost function energy

583
00:39:48,460 --> 00:39:50,570
function has these two terms

584
00:39:52,220 --> 00:39:58,950
which is again linear we have a constraint so it's quadratic constraints are equivalent constraint

585
00:39:59,200 --> 00:40:00,360
at every pixel

586
00:40:00,370 --> 00:40:02,840
possibly noisy

587
00:40:02,850 --> 00:40:09,450
and you have to do with this regularisation term which penalizes deviations from smoothness

588
00:40:10,260 --> 00:40:13,950
just first learned about this i was very skeptical that the brain does anything like

589
00:40:13,950 --> 00:40:19,100
this and this sounds like something that physicists and mathematicians would make up these two

590
00:40:19,100 --> 00:40:22,590
energy functions you add them up with the constant lambda

591
00:40:22,600 --> 00:40:29,170
the square all sorts of things in these cells and not surprisingly this very mathematical

592
00:40:29,170 --> 00:40:33,610
description of perception is surprisingly powerful

593
00:40:33,620 --> 00:40:39,730
so it is a large range of percepts actually are protected by the solutions to

594
00:40:39,730 --> 00:40:41,650
the energy minimisation problem

595
00:40:41,710 --> 00:40:44,290
so here's an example

596
00:40:44,300 --> 00:40:47,370
i'm showing free-standing that i showed you before

597
00:40:47,420 --> 00:40:52,000
and now let's which which you remember appears to rotate

598
00:40:53,590 --> 00:40:57,180
this led to the fact which you will remember appears to be four

599
00:40:57,200 --> 00:40:59,010
non rigidly

600
00:40:59,020 --> 00:41:02,040
and the same value let me just four points

601
00:41:02,070 --> 00:41:06,150
which you'll remember appears to rotate rigidly

602
00:41:06,370 --> 00:41:09,830
so we can take these displays and just feed as input

603
00:41:09,870 --> 00:41:12,750
into this energy minimisation

604
00:41:12,750 --> 00:41:17,490
note that since this is a quadratic energy function and we can

605
00:41:17,500 --> 00:41:22,110
the that is enclosed found by just solving a linear system of equations

606
00:41:22,140 --> 00:41:26,510
and we do that for address and others that does this is what they found

607
00:41:26,530 --> 00:41:28,380
that indeed the

608
00:41:29,520 --> 00:41:33,600
is these arrows pointing in the directions consistent with rotation

609
00:41:33,640 --> 00:41:37,420
the fact that you'll notice that these are pointing out

610
00:41:37,450 --> 00:41:42,500
and he's pointing and so it is predicted to be for non rigidly even of

611
00:41:42,500 --> 00:41:45,530
course is rotating in the same value lips

612
00:41:45,550 --> 00:41:47,120
when we have this for decades

613
00:41:47,130 --> 00:41:51,240
when we minimize energy is predicted to rotate rigidly

614
00:41:51,250 --> 00:41:54,910
and i'm just showing three examples here but if you look in some of these

615
00:41:54,910 --> 00:41:57,350
papers you'll see the wide range

616
00:41:57,840 --> 00:41:59,250
of percepts

617
00:41:59,250 --> 00:42:05,410
which to my surprise are modeled by this energy minimisation framework

618
00:42:05,430 --> 00:42:10,870
but the problem with this with the simple energy functions from the early eighties is

619
00:42:10,870 --> 00:42:14,620
that they failed miserably whenever the display has more than one motion

620
00:42:14,630 --> 00:42:18,160
so remember this display had fifty four that's

621
00:42:18,180 --> 00:42:22,750
and the process was one of separation into two layers of this kind of energy

622
00:42:22,750 --> 00:42:29,910
function is completely incapable of modelling segmentation really tries to find a single velocity field

623
00:42:30,630 --> 00:42:35,020
that's consistent with all this data and so this is what it finds somewhere here

624
00:42:35,070 --> 00:42:42,790
averaging together of the point of rotation and the vertical motion

625
00:42:42,820 --> 00:42:47,420
and this family of these these models led to the number of

626
00:42:47,770 --> 00:42:52,500
researchers to develop protocols models motion analysis

627
00:42:52,890 --> 00:42:57,240
and the model that is if you get the visual input such as the one

628
00:42:57,240 --> 00:42:58,070
shown here

629
00:42:58,210 --> 00:43:01,360
the hand rotating like this over checkerboard

630
00:43:01,400 --> 00:43:06,540
the goal is not to have a single velocity field rather multiple velocity field in

631
00:43:06,540 --> 00:43:11,570
this case the stationary velocity field corresponding to the checkerboard

632
00:43:11,590 --> 00:43:15,760
and the rotating velocity field corresponding to the hand

633
00:43:15,860 --> 00:43:19,590
so these algorithms try to automatically decompose the scene

634
00:43:19,610 --> 00:43:24,340
into multiple articles calculate the velocity of each layer and the assignment of pixels to

635
00:43:24,340 --> 00:43:27,210
layers and the number of layers

636
00:43:27,220 --> 00:43:31,290
and my thesis i developed another one of these models which was also based on

637
00:43:31,300 --> 00:43:33,030
the energy minimisation

638
00:43:33,040 --> 00:43:38,540
but the energy minimisation i was much more complex whereas we just had the signal

639
00:43:38,540 --> 00:43:39,850
velocity field

640
00:43:39,920 --> 00:43:43,600
in this case we have two velocity field v one and v two

641
00:43:43,640 --> 00:43:48,390
and also these binary indicator variables which indicate for every pixel whether it should be

642
00:43:48,390 --> 00:43:50,500
grouped with linear number one

643
00:43:50,510 --> 00:43:52,540
number two

644
00:43:52,550 --> 00:43:56,100
and that each one of the energy function in water

645
00:43:56,120 --> 00:43:57,470
the term again

646
00:43:57,480 --> 00:44:00,110
which just takes into consideration the labels

647
00:44:00,350 --> 00:44:04,510
the excitements every one of the largest wants to be smooth

648
00:44:05,600 --> 00:44:11,900
pixels this these assignment variables and also wanted to be small meaning nearby pixels one

649
00:44:11,900 --> 00:44:12,900
go together

650
00:44:12,900 --> 00:44:16,490
to go from b to e we can go by sea or by the

651
00:44:19,460 --> 00:44:24,360
our life is going to be much easier if we focus on singly connected graph

652
00:44:24,440 --> 00:44:25,810
that's what we'll do

653
00:44:25,820 --> 00:44:30,190
and then we'll start to worry about how to deal with multiple connected graph

654
00:44:31,750 --> 00:44:35,440
our goal is for some node access

655
00:44:35,460 --> 00:44:40,790
we want to compute the probability of x given he which is some evidence

656
00:44:40,800 --> 00:44:44,930
and evidence is observations of other variables in the graph

657
00:44:44,980 --> 00:44:49,080
and that could be a whole bunch of different variables of the node x to

658
00:44:49,080 --> 00:44:52,080
be this one here in our observations could here

659
00:44:52,090 --> 00:44:54,750
here here and there for example

660
00:44:57,690 --> 00:45:03,500
here's the really nice property of singly connected graphs that we're going to exploit

661
00:45:04,540 --> 00:45:08,670
every node x in the graph

662
00:45:08,720 --> 00:45:11,530
the by the evidence into

663
00:45:11,550 --> 00:45:17,420
evidence that upstream from acts and evidence that downstream from x

664
00:45:18,200 --> 00:45:19,950
so if we take

665
00:45:20,220 --> 00:45:24,560
this to be our node act

666
00:45:25,410 --> 00:45:27,130
some of the evidence will

667
00:45:27,140 --> 00:45:31,220
from upstream of the direction of the arrows and some of them will come from

668
00:45:31,230 --> 00:45:33,680
downstream of the direction of the arrows

669
00:45:35,880 --> 00:45:41,870
you might as well what happens is another singly connected graph let's imagine there's no

670
00:45:41,880 --> 00:45:43,860
that is the parent of e

671
00:45:45,030 --> 00:45:48,000
next e with an arrow going this way

672
00:45:48,040 --> 00:45:52,060
is that upstream or downstream of the node c

673
00:45:54,780 --> 00:46:00,960
we're going to consider to be downstream of of the because whatever effect that has

674
00:46:00,960 --> 00:46:01,840
on c

675
00:46:01,950 --> 00:46:03,560
will come to the

676
00:46:05,420 --> 00:46:06,640
so was really like

677
00:46:06,650 --> 00:46:11,330
reverse system and you drop pollution somewhere in you see which way it is that

678
00:46:11,330 --> 00:46:14,910
which was going upstream or downstream

679
00:46:14,930 --> 00:46:16,380
when richard c

680
00:46:16,580 --> 00:46:19,200
now similarly

681
00:46:19,210 --> 00:46:23,200
every edge in in the graph for example the edge between x and y

682
00:46:23,270 --> 00:46:29,940
will divide the evidence into evidence that coming from upstream of the x y and

683
00:46:29,950 --> 00:46:32,690
evidence is coming from downstream of the

684
00:46:32,730 --> 00:46:34,540
x y

685
00:46:37,020 --> 00:46:38,310
and obviously there

686
00:46:38,320 --> 00:46:40,170
it doesn't work for

687
00:46:40,180 --> 00:46:42,550
multiply connected graph

688
00:46:43,080 --> 00:46:45,920
there are multiple ways in which evidence can come in

689
00:46:48,030 --> 00:46:53,340
taking into account all those ways is is going to be computationally very hard

690
00:46:55,470 --> 00:46:56,940
so there

691
00:46:56,960 --> 00:47:01,830
i think of them i think there has been three key ideas behind belief propagation

692
00:47:05,690 --> 00:47:08,360
imagine we have a graph

693
00:47:08,370 --> 00:47:10,780
and we have some variable x

694
00:47:10,830 --> 00:47:15,490
and there's about upstream of x then there's the downstream effects then there's a whole

695
00:47:15,490 --> 00:47:20,920
bunch of other nodes imagine the graph is singly connected so i don't imagine that

696
00:47:20,920 --> 00:47:22,900
there's any way to get from here

697
00:47:22,990 --> 00:47:25,000
here OK

698
00:47:25,020 --> 00:47:30,300
now the first idea is that the probability of the variable x

699
00:47:30,350 --> 00:47:34,910
can be found by combining upstream and downstream evidence

700
00:47:34,920 --> 00:47:40,500
so the probability distribution over x given all the evidence

701
00:47:40,710 --> 00:47:43,040
we can write as the probability of x

702
00:47:43,050 --> 00:47:46,890
and all the evidence provided by the public evidence that is not a function of

703
00:47:51,000 --> 00:47:52,440
evidence we can

704
00:47:52,460 --> 00:47:57,310
split up into upstream of x and downstream of that

705
00:47:57,320 --> 00:47:59,660
and then

706
00:48:00,600 --> 00:48:05,570
pi in the denominator is the constant let's ignore for now

707
00:48:05,620 --> 00:48:07,940
this joint distribution of x

708
00:48:08,020 --> 00:48:10,780
upstream evidence and downstream evidence

709
00:48:10,800 --> 00:48:13,050
we're going to split up into

710
00:48:13,060 --> 00:48:14,300
the product

711
00:48:15,400 --> 00:48:18,160
x given the upstream evidence

712
00:48:19,120 --> 00:48:22,220
the probability of the downstream evidence

713
00:48:23,130 --> 00:48:26,240
x and the upstream evidence

714
00:48:28,700 --> 00:48:31,850
the graph is singly connected

715
00:48:31,870 --> 00:48:36,810
the probability of the downstream evidence given x and the upstream evidence

716
00:48:36,820 --> 00:48:42,050
it's just simply the probability of the downstream evidence given x

717
00:48:42,060 --> 00:48:45,120
that's the term we have here

718
00:48:45,130 --> 00:48:46,170
and that

719
00:48:46,190 --> 00:48:49,480
comes about because for this singly connected graph

720
00:48:49,500 --> 00:48:52,630
these separate

721
00:48:55,650 --> 00:48:57,310
from the upstream variable

722
00:49:04,020 --> 00:49:05,740
we used our key

723
00:49:05,770 --> 00:49:08,500
concept of the separation here

724
00:49:08,550 --> 00:49:12,960
and now what we have is that the probability of x given all the evidence

725
00:49:13,080 --> 00:49:14,670
is the product

726
00:49:14,680 --> 00:49:16,980
you know normalized

727
00:49:16,990 --> 00:49:21,600
product of something that comes from upstream and something that comes from downstream

728
00:49:21,610 --> 00:49:26,860
and i've fallen following the notation in use that pearls

729
00:49:26,910 --> 00:49:27,860
the book

730
00:49:27,910 --> 00:49:31,720
which talks extensively about belief propagation

731
00:49:31,740 --> 00:49:34,990
and he calls the up coming from

732
00:49:35,000 --> 00:49:41,390
upstream high messages and the stuff coming from downstream lambda messages and you might wonder

733
00:49:41,390 --> 00:49:43,260
why well

734
00:49:47,010 --> 00:49:49,000
from upstream corresponds to

735
00:49:49,010 --> 00:49:53,400
this prior so stop coming from upstream we're going to think of as

736
00:49:53,940 --> 00:49:58,720
stuff coming from above this is like the prior term

737
00:49:58,780 --> 00:50:01,960
and that the downstream is like

738
00:50:02,250 --> 00:50:05,310
the likelihood term in normal bayesian decision

739
00:50:05,370 --> 00:50:06,970
so that's why we have high

740
00:50:06,980 --> 00:50:11,430
coming from and lambda is likely to come from the world

741
00:50:15,740 --> 00:50:17,600
although this is you know

742
00:50:17,650 --> 00:50:20,250
i don't take that too literally because the high

743
00:50:20,260 --> 00:50:22,060
fact depends on

744
00:50:22,080 --> 00:50:26,360
data that we but there is not just prior information

745
00:50:26,400 --> 00:50:32,080
it's just depends on data we observed by parents

746
00:50:33,640 --> 00:50:36,930
that's the basic the first basic idea

747
00:50:37,260 --> 00:50:42,310
the second basic idea is the computational idea that that the upstream and downstream evidence

748
00:50:42,310 --> 00:50:47,860
can be computed via a local message passing algorithm between nodes in the graph and

749
00:50:47,860 --> 00:50:50,560
we'll see that in more detail

750
00:50:50,570 --> 00:50:52,870
and the third idea is

751
00:50:52,880 --> 00:50:54,990
really an idea about

752
00:50:55,000 --> 00:50:58,140
the messages that need to be sent around

753
00:50:58,150 --> 00:51:00,410
and the important thing is that

754
00:51:00,700 --> 00:51:05,570
when node sends the message to another node

755
00:51:05,570 --> 00:51:08,720
for every point to define a small neighborhood you look at what happens when you

756
00:51:08,720 --> 00:51:12,410
count how many times you have positive and negative patients

757
00:51:12,440 --> 00:51:18,580
and use that as the prediction like you think the majority or discount the prediction

758
00:51:18,630 --> 00:51:21,950
and the conditions for consistency to occur are

759
00:51:21,970 --> 00:51:25,090
that these neighborhoods should become smaller and smaller

760
00:51:25,140 --> 00:51:27,570
i as assume as you have more data

761
00:51:28,260 --> 00:51:29,910
it seems intuitively

762
00:51:29,930 --> 00:51:33,850
fine you are correct about the smaller neighborhood because you have

763
00:51:33,930 --> 00:51:36,160
more dense coverage of your space

764
00:51:36,180 --> 00:51:37,720
but at the same time

765
00:51:37,730 --> 00:51:39,870
you should average

766
00:51:39,900 --> 00:51:42,080
many points in that

767
00:51:43,250 --> 00:51:44,800
the larger the number of

768
00:51:44,800 --> 00:51:48,560
o point two average more accurate prediction of the problem would be

769
00:51:48,600 --> 00:51:49,800
that's kind of the

770
00:51:50,330 --> 00:51:53,700
very large numbers here

771
00:51:53,810 --> 00:52:00,750
an example is the following

772
00:52:00,840 --> 00:52:04,370
if you think what i call histograms in

773
00:52:04,390 --> 00:52:11,090
so that is ki i two party widely at sorry at least a bounded subset

774
00:52:11,090 --> 00:52:11,990
of five

775
00:52:12,030 --> 00:52:13,840
of art

776
00:52:13,900 --> 00:52:17,330
it's a bit more one the party

777
00:52:18,450 --> 00:52:19,790
and within that set

778
00:52:19,790 --> 00:52:23,470
you define cells that actually make it will just take

779
00:52:23,560 --> 00:52:28,720
it breeds in all directions and you find yourself and what you do that

780
00:52:28,770 --> 00:52:31,590
you take the average of the state over the sale of

781
00:52:32,290 --> 00:52:34,760
council of positive and negative labels

782
00:52:34,810 --> 00:52:36,560
and you are so that

783
00:52:36,590 --> 00:52:42,120
the size of the same age and which is the you know the size

784
00:52:43,120 --> 00:52:48,490
make it so that has any increases the size goes to zero as a simple

785
00:52:49,290 --> 00:52:51,610
does not go to zero too fast

786
00:52:51,690 --> 00:52:53,640
so that you ensure that

787
00:52:53,650 --> 00:52:57,380
eight enter the power in that's the volume of sales

788
00:52:57,400 --> 00:52:58,680
these volumes

789
00:52:58,740 --> 00:53:01,280
multiply by

790
00:53:01,450 --> 00:53:03,760
go to infinity which means that you

791
00:53:03,780 --> 00:53:05,910
i mean the number of

792
00:53:05,970 --> 00:53:09,130
the expected number of points that will be in any given cell

793
00:53:10,130 --> 00:53:11,760
the value

794
00:53:16,440 --> 00:53:20,330
is n times the volume after you have observed and point

795
00:53:20,330 --> 00:53:24,340
the expected number of points that you have in the cell is n times

796
00:53:24,350 --> 00:53:26,270
it and with the i

797
00:53:26,330 --> 00:53:29,720
these expected number of points that are in the cell

798
00:53:29,770 --> 00:53:32,160
should go to infinity so that

799
00:53:32,180 --> 00:53:34,350
you have these

800
00:53:34,370 --> 00:53:38,880
increase the number of points averaged

801
00:53:40,370 --> 00:53:45,280
and to prove that this is the case that these kind of algorithm is consistent

802
00:53:46,650 --> 00:53:49,410
you just use the fact that

803
00:53:49,450 --> 00:53:53,030
continuous functions with bounded support so we have to assume these bounded

804
00:53:53,080 --> 00:53:54,700
what about this

805
00:53:54,720 --> 00:53:56,280
dr dense

806
00:53:57,750 --> 00:53:59,470
lp space

807
00:54:01,200 --> 00:54:05,330
so i mean essentially want to prove some kind of convergence

808
00:54:05,370 --> 00:54:08,380
of two functions i

809
00:54:08,400 --> 00:54:11,290
this convergence will happen whenever

810
00:54:11,330 --> 00:54:16,120
you have convergence in LP which is kind of the

811
00:54:16,130 --> 00:54:20,170
no natural space to define functions when you have a probability measure

812
00:54:20,220 --> 00:54:21,340
on your space

813
00:54:22,220 --> 00:54:26,590
because there are continuous and on the bounded domain domain they are from a continuous

814
00:54:26,590 --> 00:54:31,310
which means that any such function can be approximated arbitrarily well

815
00:54:31,330 --> 00:54:32,330
we ways

816
00:54:32,350 --> 00:54:36,170
it's the ground is the run being like

817
00:54:36,230 --> 00:54:40,310
piecewise constant function something like

818
00:54:40,320 --> 00:54:42,070
so in function

819
00:54:42,090 --> 00:54:48,060
which is continuous on the bounded domain can be approximated arbitrarily well by some function

820
00:54:48,070 --> 00:54:48,810
like this

821
00:54:48,810 --> 00:54:51,120
by just increasing the size

822
00:54:51,180 --> 00:54:52,790
you know making more steps

823
00:54:52,800 --> 00:54:56,670
ultimately that will converge

824
00:54:56,680 --> 00:55:00,830
and this is this convergence is provided by the fact that each and goes to

825
00:55:02,280 --> 00:55:04,110
but then on each cell

826
00:55:04,160 --> 00:55:06,060
you want to convergence of

827
00:55:07,990 --> 00:55:11,530
towards the true probability densities the true probability of

828
00:55:11,760 --> 00:55:15,760
below a given the set of measurements

829
00:55:15,990 --> 00:55:18,330
so get this convergence you just need these

830
00:55:18,390 --> 00:55:22,930
these will guarantee that on the values of these steps will converge to the right

831
00:55:22,980 --> 00:55:25,070
and the other nation with

832
00:55:25,090 --> 00:55:29,200
guarantee that the step size with zero so you have approximation

833
00:55:48,670 --> 00:55:53,560
yes the question is whether this is related to the same things here in

834
00:55:53,600 --> 00:55:55,800
information theory

835
00:55:55,810 --> 00:55:56,830
yes yes

836
00:55:56,840 --> 00:55:58,920
you can think of it this will it's kind of

837
00:56:03,250 --> 00:56:07,260
i mean that's interestingly connected but it's not really i mean does not the same

838
00:56:07,260 --> 00:56:08,480
mathematics but

839
00:56:10,540 --> 00:56:15,100
but get the point was to show that you know if you want a perfect

840
00:56:15,100 --> 00:56:20,000
algorithm perfect in the sense of being able to learn anything

841
00:56:20,880 --> 00:56:25,010
it is in the countable case you're in the non notable

842
00:56:25,010 --> 00:56:27,060
it's not hard

843
00:56:27,110 --> 00:56:28,170
so why bother

844
00:56:36,260 --> 00:56:40,620
the question is how innocent is this assumption that are considering measurable functions

845
00:56:41,270 --> 00:56:43,780
that's the point

846
00:56:47,430 --> 00:56:49,640
it's kind of connected to the fact that you're

847
00:56:49,650 --> 00:56:52,360
introducing the probability distribution for the data

848
00:56:52,370 --> 00:56:55,160
if you introduce the probability distribution then

849
00:56:55,200 --> 00:56:56,600
the best

850
00:56:56,680 --> 00:57:02,200
i mean and this probability solution satisfies the regular actions for probability on the company

851
00:57:02,280 --> 00:57:03,780
continues space

852
00:57:04,970 --> 00:57:10,500
this best function will be measurable so you can restrict measurable functions but this means

853
00:57:10,500 --> 00:57:13,040
equally the concentration of the

854
00:57:13,060 --> 00:57:16,830
the protein ID it's virion and therefore the

855
00:57:17,870 --> 00:57:20,580
it is exactly the value of

856
00:57:20,600 --> 00:57:23,250
pk one

857
00:57:23,330 --> 00:57:25,330
what happens on the other extreme

858
00:57:25,390 --> 00:57:29,040
what happens if all of a sudden there is a rise in ph

859
00:57:29,060 --> 00:57:32,040
if there's is a sudden rise in ph

860
00:57:32,060 --> 00:57:34,670
how does virion response

861
00:57:34,710 --> 00:57:38,330
sudden rise in ph that means

862
00:57:38,370 --> 00:57:42,330
the concentration of protons suddenly fell

863
00:57:43,170 --> 00:57:45,890
we use the list should tell you principle again

864
00:57:45,930 --> 00:57:47,930
we use should tell you principle

865
00:57:47,940 --> 00:57:49,600
go back over here

866
00:57:49,620 --> 00:57:52,980
look virion we say OK what we've got right now

867
00:57:53,000 --> 00:57:57,040
is a sudden dearth of shortage of

868
00:57:57,100 --> 00:58:00,830
proton how consider i respond to that sudden

869
00:58:00,940 --> 00:58:02,390
shortage of protons

870
00:58:02,390 --> 00:58:07,710
all the neutrals virion has proton sitting right here so it's better i can do

871
00:58:07,710 --> 00:58:12,810
in response to the sudden drop in proton concentration is donate

872
00:58:12,830 --> 00:58:15,980
become a proton donor and start

873
00:58:16,040 --> 00:58:18,160
adding protons to the solution

874
00:58:18,160 --> 00:58:24,330
sacrificing its neutrality in order to minimize the impact of that sudden

875
00:58:24,390 --> 00:58:26,730
rise in ph so let's

876
00:58:26,810 --> 00:58:29,100
let's document that reaction

877
00:58:29,120 --> 00:58:31,080
as this one here that's where

878
00:58:31,080 --> 00:58:32,750
as the iron

879
00:58:32,750 --> 00:58:35,640
does the following

880
00:58:35,730 --> 00:58:39,190
will start with the neutral again

881
00:58:39,250 --> 00:58:41,230
that's over year

882
00:58:41,330 --> 00:58:44,210
h three n

883
00:58:46,730 --> 00:58:51,350
so here's the neutrals virion experiences sudden

884
00:58:51,350 --> 00:58:52,910
rise in ph

885
00:58:52,910 --> 00:58:54,710
and responds by

886
00:58:55,660 --> 00:58:58,250
protons into the solution

887
00:58:58,270 --> 00:58:59,460
now the

888
00:58:59,460 --> 00:59:01,560
the left end is the protein eight

889
00:59:01,580 --> 00:59:05,060
leaving neutral

890
00:59:08,140 --> 00:59:13,770
the proton is now donated to the solution and if the solution is in fact

891
00:59:13,830 --> 00:59:17,810
hi in ph then this protons can go and attack

892
00:59:18,640 --> 00:59:20,540
xs hydroxyl

893
00:59:20,560 --> 00:59:23,210
and neutralize it so this is how

894
00:59:23,250 --> 00:59:25,430
xii response

895
00:59:25,440 --> 00:59:28,350
and what we see here this is neutral

896
00:59:28,370 --> 00:59:31,440
this is not neutral but now

897
00:59:31,440 --> 00:59:36,060
the positive and has become neutralized by the loss of the proton so this is

898
00:59:36,060 --> 00:59:38,190
known as negative

899
00:59:38,210 --> 00:59:40,040
this is known as negative

900
00:59:40,040 --> 00:59:41,870
so we can call this

901
00:59:41,930 --> 00:59:44,670
reaction to and it has a k two

902
00:59:44,790 --> 00:59:49,940
and using the notation that i've developed up there the neutral variety

903
00:59:50,000 --> 00:59:55,290
denoting h a so this is where i minus this

904
00:59:55,310 --> 00:59:59,710
proton so this is just the a minus its making sense this is not negative

905
00:59:59,710 --> 01:00:02,080
and if i take a two-way from HAI get

906
01:00:02,940 --> 01:00:06,430
and then this is the age plus

907
01:00:06,440 --> 01:00:09,980
so that's the reaction we're looking at

908
01:00:10,020 --> 01:00:11,560
and we can write the

909
01:00:11,600 --> 01:00:14,020
the case the reaction as

910
01:00:14,040 --> 01:00:18,160
concentration of protons

911
01:00:18,210 --> 01:00:21,810
times concentration of the protein eight is very high

912
01:00:21,830 --> 01:00:24,370
divided by the concentration of

913
01:00:24,410 --> 01:00:26,790
the neutrals better and

914
01:00:26,790 --> 01:00:29,460
take the log base ten of size

915
01:00:29,500 --> 01:00:30,690
pk two

916
01:00:34,160 --> 01:00:35,850
log base ten

917
01:00:35,870 --> 01:00:42,930
on the flip this oversight after promyelocytic here's the BHA divided by

918
01:00:44,620 --> 01:00:45,870
a minus

919
01:00:45,890 --> 01:00:48,100
and again we have fifty percent

920
01:00:48,120 --> 01:00:54,600
neutralisation of and that means that there will be fifty percent of the species consumed

921
01:00:54,600 --> 01:00:56,520
fifty percent of this left

922
01:00:56,580 --> 01:01:01,730
the log of one is zero and that ph we have the

923
01:01:01,750 --> 01:01:04,460
the value of the

924
01:01:05,500 --> 01:01:07,690
acid dissociation constant

925
01:01:07,710 --> 01:01:09,410
i think this

926
01:01:09,410 --> 01:01:13,600
this slide shows what's going on here

927
01:01:13,640 --> 01:01:16,500
but all it does it plots

928
01:01:16,500 --> 01:01:22,440
these two equations if you take the first equation here the logarithmic the sorenson form

929
01:01:22,460 --> 01:01:26,540
the first equation that's the lower part of this graph

930
01:01:26,580 --> 01:01:29,100
the lower part of this graph applied ph

931
01:01:29,120 --> 01:01:35,540
versus the ion dissociation so if you turn this around ph versus the amount of

932
01:01:35,540 --> 01:01:37,940
this that has been consumed

933
01:01:38,000 --> 01:01:42,500
and what you see is that when you have fifty percent this is fifty percent

934
01:01:42,500 --> 01:01:43,080
there are

935
01:01:44,030 --> 01:01:45,350
many questions that

936
01:01:46,290 --> 01:01:47,860
one can ask about language

937
01:01:48,680 --> 01:01:51,940
the most fundamental one truly is a

938
01:01:52,250 --> 01:01:52,830
what it is

939
01:01:53,470 --> 01:01:54,900
so what is language

940
01:01:55,850 --> 01:01:57,260
to the extent that you have some

941
01:01:57,890 --> 01:01:58,940
grasp this

942
01:01:59,990 --> 01:02:01,280
one can proceed to

943
01:02:03,470 --> 01:02:04,750
for other questions

944
01:02:05,500 --> 01:02:07,190
lacking at thing

945
01:02:08,460 --> 01:02:10,300
necessarily curtailed

946
01:02:11,030 --> 01:02:12,910
for example no biologist with

947
01:02:13,970 --> 01:02:15,010
seek discredited

948
01:02:15,690 --> 01:02:18,830
development directly evolution the i

949
01:02:19,410 --> 01:02:22,880
without a fairly clear conception of the

950
01:02:22,920 --> 01:02:23,730
what i

951
01:02:24,410 --> 01:02:26,000
it's essential nature

952
01:02:26,580 --> 01:02:29,670
so it would not be for example to say that

953
01:02:30,390 --> 01:02:31,390
it used to be

954
01:02:32,790 --> 01:02:34,240
order for language that

955
01:02:34,890 --> 01:02:36,820
it's used to communicate among

956
01:02:37,310 --> 01:02:38,420
numerous other users

957
01:02:39,810 --> 01:02:40,830
on the basis that

958
01:02:41,470 --> 01:02:42,610
tentative account

959
01:02:43,080 --> 01:02:44,100
what languages

960
01:02:45,150 --> 01:02:46,470
the more far reaching

961
01:02:48,050 --> 01:02:49,280
we can proceed to

962
01:02:50,640 --> 01:02:51,280
ask how

963
01:02:52,530 --> 01:02:56,080
the concepts principles and results this

964
01:02:57,140 --> 01:02:58,710
characterization of language

965
01:03:00,830 --> 01:03:03,400
other important study acquisition

966
01:03:05,410 --> 01:03:06,900
the neural representation

967
01:03:07,670 --> 01:03:08,940
historical change

968
01:03:13,020 --> 01:03:14,960
reciprocally what's learned about

969
01:03:15,420 --> 01:03:16,760
these topics can

970
01:03:17,390 --> 01:03:17,970
leads to

971
01:03:21,180 --> 01:03:21,820
this here

972
01:03:22,250 --> 01:03:23,340
that was proposed

973
01:03:24,960 --> 01:03:26,950
hierarchy questions not

974
01:03:29,600 --> 01:03:30,190
the first thing

975
01:03:32,250 --> 01:03:36,010
every sensible approach to language must recognise these

976
01:03:36,860 --> 01:03:38,500
is used by individuals

977
01:03:39,820 --> 01:03:40,620
and that

978
01:03:42,180 --> 01:03:43,840
passes uses depends

979
01:03:44,410 --> 01:03:46,150
primarily on the produces

980
01:03:48,270 --> 01:03:48,650
it should be

981
01:03:52,430 --> 01:03:53,970
every approach to language

982
01:03:54,490 --> 01:03:55,740
that's what it is

983
01:03:56,690 --> 01:03:58,900
presupposes tacitly

984
01:03:59,620 --> 01:04:01,630
concept is sometimes called

985
01:04:02,290 --> 01:04:03,230
i language

986
01:04:04,140 --> 01:04:05,270
i you

987
01:04:08,300 --> 01:04:10,210
it some system that

988
01:04:10,700 --> 01:04:12,470
internal to an individual

989
01:04:13,080 --> 01:04:13,790
and that

990
01:04:15,870 --> 01:04:17,210
the problem first

991
01:04:18,750 --> 01:04:19,520
well at

992
01:04:20,590 --> 01:04:21,730
it presupposes

993
01:04:22,160 --> 01:04:25,070
but also conceptions the

994
01:04:29,640 --> 01:04:30,370
that was

995
01:04:33,250 --> 01:04:35,100
i itself presupposes

996
01:04:36,880 --> 01:04:38,010
and senses

997
01:04:38,180 --> 01:04:39,720
the most fundamental issue

998
01:04:41,890 --> 01:04:42,730
line here

999
01:04:43,290 --> 01:04:43,510
i mean

1000
01:04:44,350 --> 01:04:45,380
so in that sense

1001
01:04:46,140 --> 01:04:48,110
something in terms of an individual

1002
01:04:57,230 --> 01:05:01,160
is a system that can all discrete

1003
01:05:05,640 --> 01:05:06,950
six seven

1004
01:05:11,380 --> 01:05:12,220
system that's

1005
01:05:13,220 --> 01:05:14,640
has be determined by

1006
01:05:15,190 --> 01:05:17,920
so fundamental computational procedures

1007
01:05:19,910 --> 01:05:21,310
we can procedure

1008
01:05:22,080 --> 01:05:23,370
which is a special kind

1009
01:05:24,460 --> 01:05:25,230
the first one

1010
01:05:29,850 --> 01:05:30,980
he says

1011
01:05:32,420 --> 01:05:32,850
which is

1012
01:05:34,520 --> 01:05:35,960
according to the same

1013
01:05:37,670 --> 01:05:39,820
first consequence of mister

1014
01:05:42,030 --> 01:05:42,930
almost universal

1015
01:05:45,780 --> 01:05:46,440
and with the

1016
01:05:49,320 --> 01:05:49,860
so what

1017
01:05:52,160 --> 01:05:53,110
this presentation

1018
01:05:54,100 --> 01:05:56,020
it's pretty simple basically

1019
01:05:58,400 --> 01:06:00,160
anything parameter

1020
01:06:05,490 --> 01:06:06,760
it has been added

1021
01:06:08,630 --> 01:06:09,510
see but more

1022
01:06:10,620 --> 01:06:11,100
this is the

1023
01:06:12,150 --> 01:06:12,990
that's the first

1024
01:06:14,430 --> 01:06:16,590
for example the number

1025
01:06:17,680 --> 01:06:18,660
if we understood

1026
01:06:19,720 --> 01:06:21,180
general expressions

1027
01:06:23,010 --> 01:06:24,010
thank you

1028
01:06:24,150 --> 01:06:25,330
and for each other

1029
01:06:26,620 --> 01:06:28,230
the you know just in

1030
01:06:28,900 --> 01:06:29,280
so you

1031
01:06:30,440 --> 01:06:32,560
use on the array

1032
01:06:34,910 --> 01:06:39,690
this expression each which is on the internet

1033
01:06:41,210 --> 01:06:42,010
and he

1034
01:06:47,010 --> 01:06:48,240
one is essentially

1035
01:06:51,590 --> 01:06:52,240
in some form

1036
01:06:52,870 --> 01:06:53,710
and is

1037
01:06:54,480 --> 01:06:55,290
roughly speaking

1038
01:06:57,030 --> 01:06:59,550
francis four

1039
01:07:01,180 --> 01:07:02,060
so that means

1040
01:07:04,350 --> 01:07:05,610
so is it

1041
01:07:06,290 --> 01:07:07,070
sound mean

1042
01:07:09,850 --> 01:07:11,620
in fact was limited

1043
01:07:12,300 --> 01:07:12,640
we now

1044
01:07:13,160 --> 01:07:14,560
the sounds as

1045
01:07:14,940 --> 01:07:16,280
only one o

1046
01:07:16,360 --> 01:07:17,590
the sensorimotor

1047
01:07:25,260 --> 01:07:27,270
well the sound meaning

1048
01:07:29,010 --> 01:07:30,790
it is in principle unbounded

1049
01:07:32,050 --> 01:07:33,390
the brain

1050
01:07:34,940 --> 01:07:35,840
long island

1051
01:07:36,380 --> 01:07:37,160
rich history

1052
01:07:37,820 --> 01:07:38,920
studies language

1053
01:07:39,850 --> 01:07:41,350
we found a few days

1054
01:07:42,160 --> 01:07:42,800
the government

1055
01:07:43,240 --> 01:07:44,180
one year

1056
01:07:45,420 --> 01:07:47,200
many different animals

1057
01:07:47,640 --> 01:07:48,310
so only

1058
01:07:48,780 --> 01:07:51,710
it is almost infinitely large power

1059
01:07:54,270 --> 01:07:55,190
there is a

1060
01:07:55,210 --> 01:07:56,820
sounds years

1061
01:07:57,460 --> 01:07:59,170
that have been we

1062
01:07:59,570 --> 01:08:00,790
the grammar

1063
01:08:02,930 --> 01:08:03,890
the only thing

1064
01:08:04,600 --> 01:08:06,420
is traditional that

1065
01:08:10,410 --> 01:08:11,510
very similar to

1066
01:08:14,460 --> 01:08:15,540
could be made to

1067
01:08:15,810 --> 01:08:16,390
the reason

1068
01:08:17,580 --> 01:08:18,430
the amount

1069
01:08:18,880 --> 01:08:20,410
characterizes she

1070
01:08:21,120 --> 01:08:23,210
is the basis for what can

1071
01:08:25,140 --> 01:08:27,560
according to marvelous invention

1072
01:08:27,560 --> 01:08:29,250
it's time to start now

1073
01:08:29,290 --> 01:08:32,590
it's my pleasure to introduce you to mark ring

1074
01:08:32,720 --> 01:08:35,350
our never mind

1075
01:08:35,370 --> 01:08:40,630
o comes from the university of was then y is a very active person is

1076
01:08:40,670 --> 01:08:44,630
on only from the university of malta

1077
01:08:46,420 --> 01:08:47,760
and he does

1078
01:08:50,350 --> 01:08:55,050
i think is five years now collaboration between the institut

1079
01:08:55,060 --> 01:08:56,650
the university

1080
01:08:56,750 --> 01:09:02,190
very much loss in evolutionary computation on various problems like then

1081
01:09:07,360 --> 01:09:08,930
talk about

1082
01:09:08,940 --> 01:09:13,970
and i say where two areas

1083
01:09:14,530 --> 01:09:15,660
one is

1084
01:09:16,920 --> 01:09:18,480
this kind of is

1085
01:09:18,650 --> 01:09:20,430
usually station

1086
01:09:20,500 --> 01:09:22,870
and applied to the data

1087
01:09:24,120 --> 01:09:26,740
i guess exciting news

1088
01:09:26,960 --> 01:09:30,540
the latest design is a

1089
01:09:30,550 --> 01:09:31,970
these are

1090
01:09:34,680 --> 01:09:41,630
well this presentation talk about the solution as well the additional classrooms

1091
01:09:41,700 --> 01:09:44,930
so for those of you are familiar with additional tracks

1092
01:09:45,000 --> 01:09:48,660
and those so familiar with it

1093
01:09:48,700 --> 01:09:52,200
it's been quite active use in recent years

1094
01:09:52,220 --> 01:09:55,020
and the problem of clustering data

1095
01:09:55,060 --> 01:09:59,580
also as many of you might know is computational problems

1096
01:09:59,590 --> 01:10:02,270
so that for many people who apply

1097
01:10:02,320 --> 01:10:05,030
such heuristics to take this but you can so

1098
01:10:05,090 --> 01:10:06,220
these kind of problems

1099
01:10:06,230 --> 01:10:08,070
general exact

1100
01:10:08,080 --> 01:10:12,800
so that has been quite using selection and the

1101
01:10:12,950 --> 01:10:18,680
it's actually was looking to the literature of define this is all papers published there

1102
01:10:18,740 --> 01:10:21,520
is no genetic systems

1103
01:10:21,850 --> 01:10:25,810
which was quite a surprise to me because it's full already dimension

1104
01:10:25,820 --> 01:10:27,210
and work together

1105
01:10:27,220 --> 01:10:28,970
missionary computation

1106
01:10:28,990 --> 01:10:33,150
also for quite a while this is actually one the best way i can

1107
01:10:33,240 --> 01:10:35,110
so i just

1108
01:10:35,120 --> 01:10:36,570
the value of about

1109
01:10:38,920 --> 01:10:40,080
everybody genetic

1110
01:10:41,720 --> 01:10:45,030
the question to get bench general

1111
01:10:45,070 --> 01:10:48,080
of course made that experience over many years not

1112
01:10:48,180 --> 01:10:51,320
but when it comes to numerical optimisation there also

1113
01:10:51,340 --> 01:10:53,740
definitely needs to consider

1114
01:10:53,760 --> 01:10:56,470
so for instance strategies

1115
01:10:56,510 --> 01:11:00,540
this sort of like to bring to your attention to to

1116
01:11:00,670 --> 01:11:01,840
the this revolution

1117
01:11:01,850 --> 01:11:02,360
the had

1118
01:11:02,370 --> 01:11:03,000
that's what

1119
01:11:03,010 --> 01:11:04,910
they should not very

1120
01:11:04,920 --> 01:11:09,450
also the community they specialized numerical optimisation

1121
01:11:09,490 --> 01:11:11,560
no particular

1122
01:11:11,610 --> 01:11:14,480
what's funny is two examples specialized

1123
01:11:14,570 --> 01:11:19,090
on the same time as the time of optimisation of both introduced the same year

1124
01:11:19,090 --> 01:11:20,410
nineteen ninety five

1125
01:11:20,460 --> 01:11:26,520
but this optimisation is really very popular in lots of the sessions and conferences and

1126
01:11:27,380 --> 01:11:32,300
the station was this differential evolution has very little attention

1127
01:11:32,380 --> 01:11:34,590
it actually is

1128
01:11:34,640 --> 01:11:39,620
i will be able to demonstrate this presentation differential evolution should be

1129
01:11:39,630 --> 01:11:40,490
there many

1130
01:11:40,500 --> 01:11:46,250
also you should consider both when they think about american optimisation three is of the

1131
01:11:46,300 --> 01:11:47,390
o should say

1132
01:11:47,410 --> 01:11:49,090
invented myself

1133
01:11:49,170 --> 01:11:50,510
this is not the case

1134
01:11:50,580 --> 01:11:58,860
but i have to admit my to be something to consider america

1135
01:12:01,970 --> 01:12:06,260
of collaborators is working together

1136
01:12:06,270 --> 01:12:07,870
so we to

1137
01:12:09,110 --> 01:12:15,270
and they were already qualified through the the statistics so

1138
01:12:15,930 --> 01:12:19,410
the interested case of clustering news

1139
01:12:21,400 --> 01:12:24,430
it has to be great variety of applications not only

1140
01:12:24,440 --> 01:12:27,540
this is called a series

1141
01:12:27,900 --> 01:12:32,140
so but also by the maximum

1142
01:12:32,280 --> 01:12:37,410
the fact that he should be sent to the fish were reliable

1143
01:12:37,460 --> 01:12:38,200
and the

1144
01:12:38,670 --> 01:12:44,980
interior but if you look do what's what's the existing techniques to find any

1145
01:12:44,990 --> 01:12:46,730
least find today

1146
01:12:46,820 --> 01:12:50,050
satisfy all these criteria so there no

1147
01:12:53,590 --> 01:12:57,460
it's to creating an additional clustering algorithm hat

1148
01:12:57,710 --> 01:13:00,620
well defined as well so far

1149
01:13:02,140 --> 01:13:03,510
right so

1150
01:13:03,820 --> 01:13:05,680
meadow evolution

1151
01:13:05,690 --> 01:13:10,350
so there is a heuristic search methods same data objects to the

1152
01:13:10,360 --> 01:13:13,240
adequacy according to some statistics

1153
01:13:13,320 --> 01:13:14,730
let's try two

1154
01:13:14,830 --> 01:13:16,090
so for those of you that

1155
01:13:16,100 --> 01:13:19,580
with the mining and statistics when they first met

1156
01:13:19,980 --> 01:13:26,090
this must not be confused with the text data points in your data set what

1157
01:13:26,150 --> 01:13:26,730
they mean

1158
01:13:27,750 --> 01:13:28,700
and the second

1159
01:13:28,910 --> 01:13:31,370
the term centroid is they think about

1160
01:13:31,430 --> 01:13:34,150
any kind of representative for that

1161
01:13:34,200 --> 01:13:37,820
representation of data as

1162
01:13:37,840 --> 01:13:42,300
OK so it's not the the mean if you like the centroid of the cluster

1163
01:13:42,300 --> 01:13:44,350
but it can be any more that would could

1164
01:13:44,360 --> 01:13:47,310
as sensitive data

1165
01:13:48,650 --> 01:13:51,230
so you should ask

1166
01:13:51,460 --> 01:13:54,990
problem can be formulated in this way

1167
01:13:55,070 --> 01:14:02,720
given a set of objects over the properties which is to matrix the

1168
01:14:04,780 --> 01:14:07,540
it to tissue gene

1169
01:14:08,530 --> 01:14:13,870
with the property that each of these classes here is subset all nonempty

1170
01:14:13,980 --> 01:14:16,870
the intersection of these clusters is

1171
01:14:16,940 --> 01:14:20,040
all of clusters can range

1172
01:14:20,060 --> 01:14:22,380
and i would like to give you

1173
01:14:22,420 --> 01:14:24,360
o which is o

1174
01:14:25,410 --> 01:14:26,620
she has not

1175
01:14:26,670 --> 01:14:27,980
compared to other people

1176
01:14:29,200 --> 01:14:33,540
OK county

1177
01:14:33,550 --> 01:14:37,880
by the time the topology such that it is

1178
01:14:37,970 --> 01:14:40,700
this not the minimum expected

1179
01:14:40,820 --> 01:14:45,610
so many ways that

1180
01:14:45,660 --> 01:14:49,210
in this here we investigate

1181
01:14:49,290 --> 01:14:51,230
this is one of the trees with

1182
01:14:51,400 --> 01:14:54,300
so now we are talking about

1183
01:14:55,790 --> 01:14:58,490
we don't this calculated from the

1184
01:14:58,530 --> 01:15:01,470
thanks the company two

1185
01:15:02,590 --> 01:15:04,470
and then

1186
01:15:04,470 --> 01:15:05,750
like this

1187
01:15:05,760 --> 01:15:08,830
area of the capital

1188
01:15:08,840 --> 01:15:13,900
and let's assume that the positive charge so that the electric field lines come out

1189
01:15:13,940 --> 01:15:15,840
the surface like so

1190
01:15:15,900 --> 01:15:19,040
perpendicular to the surface

1191
01:15:19,090 --> 01:15:21,960
always perpendicular to the potential

1192
01:15:22,010 --> 01:15:24,630
so now if i play goes as well

1193
01:15:24,670 --> 01:15:26,840
which tells me that the

1194
01:15:26,840 --> 01:15:28,290
the surface integral

1195
01:15:28,300 --> 01:15:30,540
of the electric flux

1196
01:15:30,590 --> 01:15:32,220
throughout this whole surface

1197
01:15:32,230 --> 01:15:34,650
well there's only flux coming out of this

1198
01:15:34,710 --> 01:15:39,290
surface here i can bring that serves as close to the surface as i wanted

1199
01:15:39,340 --> 01:15:43,470
i could almost make it cuts and everything comes out only through

1200
01:15:45,410 --> 01:15:48,780
so what comes out is the surface area eight

1201
01:15:48,820 --> 01:15:50,750
times the electric field

1202
01:15:53,850 --> 01:15:55,230
in the same direction

1203
01:15:55,320 --> 01:15:58,150
remember is perpendicular to the

1204
01:15:58,200 --> 01:16:00,080
the surface of the potential

1205
01:16:00,090 --> 01:16:02,980
and so this is all the for the surface integral

1206
01:16:03,000 --> 01:16:05,300
and that's all the charge inside

1207
01:16:05,320 --> 01:16:10,830
well the charge inside is of course the surface charge density times the area a

1208
01:16:10,840 --> 01:16:12,230
divided by

1209
01:16:12,250 --> 01:16:14,410
after non-zero uses gauss law

1210
01:16:14,420 --> 01:16:16,140
so you find immediately

1211
01:16:16,150 --> 01:16:17,580
that you like to you

1212
01:16:18,420 --> 01:16:20,460
divided by zero

1213
01:16:20,470 --> 01:16:23,520
so whatever you have conductor if you know the local

1214
01:16:23,530 --> 01:16:25,340
the surface charge density

1215
01:16:25,350 --> 01:16:26,890
you always know to local

1216
01:16:26,900 --> 01:16:28,300
electric field

1217
01:16:28,440 --> 01:16:31,330
and since the surface charge density

1218
01:16:31,340 --> 01:16:32,460
it's going to be

1219
01:16:32,540 --> 01:16:34,210
the high is here

1220
01:16:34,260 --> 01:16:38,450
even though the whole thing is in the potential electric field will also be higher

1221
01:16:38,450 --> 01:16:40,460
here than it will be

1222
01:16:44,470 --> 01:16:47,210
i can demonstrate this to you in a year

1223
01:16:47,290 --> 01:16:48,840
a very simple way

1224
01:16:48,850 --> 01:16:51,470
i have here a cooking pan

1225
01:16:51,520 --> 01:16:53,270
the the cooking pan

1226
01:16:53,320 --> 01:16:56,800
used to more lapses in their large and

1227
01:16:56,820 --> 01:16:59,290
the cooking pan i'm going to charge up

1228
01:16:59,300 --> 01:17:00,710
and the cooking pan

1229
01:17:02,080 --> 01:17:03,230
as the radius

1230
01:17:03,400 --> 01:17:07,270
however this may be twenty centimetres but looking at the handle

1231
01:17:07,280 --> 01:17:11,000
a very small radius is so good for charge on their

1232
01:17:11,010 --> 01:17:12,510
and i'm going to convince you

1233
01:17:12,520 --> 01:17:14,330
that i can scoop of

1234
01:17:14,340 --> 01:17:16,020
more charge here

1235
01:17:17,440 --> 01:17:20,100
the radius is small than i can scope of here

1236
01:17:20,150 --> 01:17:21,590
i have he is small

1237
01:17:21,600 --> 01:17:23,350
flat space

1238
01:17:23,400 --> 01:17:26,270
and i'm going to put this building on the surface here and on the surface

1239
01:17:27,040 --> 01:17:28,170
we're going to see

1240
01:17:28,200 --> 01:17:29,050
from where

1241
01:17:29,080 --> 01:17:30,260
we can

1242
01:17:30,270 --> 01:17:32,670
the scope of the most chart

1243
01:17:32,720 --> 01:17:35,500
still chart from the previous lecture

1244
01:17:35,530 --> 01:17:39,330
so here we see the electroscope that we've seen before

1245
01:17:39,340 --> 01:17:41,840
i'm going to charge this cooking pan

1246
01:17:41,850 --> 01:17:42,920
with my

1247
01:17:42,940 --> 01:17:46,010
favourite technique which is the electrophoresis

1248
01:17:46,020 --> 01:17:48,140
so we have the cat for

1249
01:17:48,190 --> 01:17:50,780
and we have the glass plate

1250
01:17:50,820 --> 01:17:54,500
i'm going to

1251
01:17:54,550 --> 01:17:57,590
rather the first step for

1252
01:17:58,460 --> 01:18:00,960
put my finger on get a little short

1253
01:18:01,020 --> 01:18:02,350
charge depends

1254
01:18:02,360 --> 01:18:03,590
put my finger on

1255
01:18:03,600 --> 01:18:05,150
get another shock

1256
01:18:05,160 --> 01:18:07,090
charge of the band

1257
01:18:07,090 --> 01:18:09,210
and another one

1258
01:18:09,270 --> 01:18:13,150
part the pan make sure they get enough charge on their

1259
01:18:15,220 --> 01:18:16,250
last again

1260
01:18:16,260 --> 01:18:17,910
what is on top

1261
01:18:17,920 --> 01:18:19,540
i put my finger on

1262
01:18:21,390 --> 01:18:23,770
once more

1263
01:18:23,890 --> 01:18:26,410
and once more

1264
01:18:26,460 --> 01:18:29,020
let's assume we have enough charge and there are no

1265
01:18:29,080 --> 01:18:31,320
here is my little school

1266
01:18:31,330 --> 01:18:32,570
i touch it

1267
01:18:32,580 --> 01:18:34,730
outside view of the camp

1268
01:18:34,750 --> 01:18:36,050
of the band

1269
01:18:36,100 --> 01:18:38,460
and go to the electroscope

1270
01:18:38,470 --> 01:18:40,390
you see the charge

1271
01:18:40,440 --> 01:18:41,540
very clear

1272
01:18:41,550 --> 01:18:44,250
what i want to show you now it's very complicated

1273
01:18:44,260 --> 01:18:45,900
is that when i touch here

1274
01:18:45,900 --> 01:18:48,110
the handle

1275
01:18:48,150 --> 01:18:51,400
very small radius that i can take more charge

1276
01:18:51,490 --> 01:18:53,040
there we go

1277
01:18:53,100 --> 01:18:56,560
substantially more that's all i wanted to show you so you see now in front

1278
01:18:56,560 --> 01:18:58,350
of your own eyes for the first time

1279
01:18:58,360 --> 01:19:00,310
even though this is a conductor

1280
01:19:00,320 --> 01:19:02,880
that means that it is an equipotential

1281
01:19:02,890 --> 01:19:06,650
that the surface charge density right here

1282
01:19:07,550 --> 01:19:11,700
higher than the surface charge density here

1283
01:19:11,810 --> 01:19:15,940
only if is the sphere of course versus spherical symmetry reasons will the

1284
01:19:15,970 --> 01:19:19,420
try be uniformly distributed

1285
01:19:19,480 --> 01:19:22,650
if the electric field becomes too high

1286
01:19:22,700 --> 01:19:27,750
we get what we call electrical breakdown we get to discharge into the air

1287
01:19:27,800 --> 01:19:29,490
and the reason for that

1288
01:19:30,130 --> 01:19:31,980
actually quite simple

1289
01:19:31,990 --> 01:19:34,150
you have an electron here

1290
01:19:34,150 --> 01:19:36,620
and this is an electric fields

1291
01:19:36,630 --> 01:19:41,990
electron will start to accelerate the direction

1292
01:19:44,250 --> 01:19:48,690
i will collide with nitrogen and oxygen molecules in the air

1293
01:19:48,700 --> 01:19:52,750
and if the electron has enough kinetic energy to ionize

1294
01:19:52,770 --> 01:19:54,450
that molecule

1295
01:19:55,670 --> 01:19:57,070
one electron

1296
01:19:57,150 --> 01:19:58,760
become two electrons

1297
01:19:58,830 --> 01:20:01,450
originally elected was the electron

1298
01:20:01,470 --> 01:20:03,050
from the i

1299
01:20:03,060 --> 01:20:04,650
and he's now

1300
01:20:04,660 --> 01:20:07,450
start to accelerate in this electric field

1301
01:20:07,470 --> 01:20:08,970
and if they collide

1302
01:20:08,980 --> 01:20:11,520
with the molecules and they made

1303
01:20:13,380 --> 01:20:14,360
each one

1304
01:20:14,370 --> 01:20:16,020
become two electrons

1305
01:20:16,030 --> 01:20:17,890
so we get an evidence

1306
01:20:17,930 --> 01:20:20,620
and there is evidence is an electrical breakdown

1307
01:20:20,680 --> 01:20:22,810
you get this spark

1308
01:20:22,870 --> 01:20:24,900
when the ions that are formed

1309
01:20:24,900 --> 01:20:26,680
become neutral again

1310
01:20:26,690 --> 01:20:28,580
they produce light

1311
01:20:28,600 --> 01:20:29,770
and that's what you see

1312
01:20:29,780 --> 01:20:31,400
that's the light that you see

1313
01:20:32,500 --> 01:20:34,660
this part

1314
01:20:34,700 --> 01:20:37,290
and so is part of occur typically

1315
01:20:37,300 --> 01:20:39,540
at the

1316
01:20:39,560 --> 01:20:41,170
it's sharp points

1317
01:20:41,410 --> 01:20:44,910
areas where the curvature is

1318
01:20:44,930 --> 01:20:46,820
strong whereby the radius

1319
01:20:46,830 --> 01:20:49,790
very small that's why the electric field i

1320
01:20:49,800 --> 01:20:51,950
the highest

1321
01:20:51,970 --> 01:20:55,280
how strong should electric field we

1322
01:20:55,330 --> 01:20:58,500
well we can make an back-of-the-envelope calculations

1323
01:20:58,500 --> 01:21:02,220
good morning everyone feel free to interrupt me whenever because we have a lot of

1324
01:21:02,220 --> 01:21:07,360
time as you know my name is rowland i'm working in the research group on

1325
01:21:07,360 --> 01:21:13,760
complex event processing in counseling and complex event processing as you see is that is

1326
01:21:13,760 --> 01:21:16,890
about real time streaming data

1327
01:21:16,980 --> 01:21:22,570
and we're trying to combine that with what we learned from the semantic web research

1328
01:21:22,660 --> 01:21:24,680
of the last decade

1329
01:21:24,700 --> 01:21:28,410
you see how these two go together

1330
01:21:30,700 --> 01:21:31,740
first of all

1331
01:21:31,770 --> 01:21:36,080
i need to attribute these slides there from

1332
01:21:36,090 --> 01:21:42,150
several tutorials that my boss my supervisor gave on other occasions together with people from

1333
01:21:42,150 --> 01:21:46,780
the event processing community pedro bizarro from coimbra portugal

1334
01:21:46,800 --> 01:21:49,660
o for its from IBM in hi-fi

1335
01:21:49,910 --> 01:21:56,260
my journey from caltech california these are people walking researching on event processing

1336
01:21:56,270 --> 01:21:57,720
now y

1337
01:21:57,730 --> 01:21:59,700
this tutorial

1338
01:21:59,730 --> 01:22:00,880
real time

1339
01:22:00,950 --> 01:22:03,170
is for many applications crucial today

1340
01:22:03,190 --> 01:22:07,580
there is often times on the move we

1341
01:22:07,590 --> 01:22:09,970
so far we we heard a lot about it

1342
01:22:10,060 --> 01:22:11,830
how to model static data

1343
01:22:11,840 --> 01:22:16,520
now we also like to talk about flowing data that we need to process on

1344
01:22:16,520 --> 01:22:17,860
the fly

1345
01:22:17,880 --> 01:22:22,270
so we need to find results immediately on this flowing data without having to store

1346
01:22:22,270 --> 01:22:26,830
the data in the database we want to process this data while it's moving and

1347
01:22:26,830 --> 01:22:30,610
possibly be able to throw the stayed way to deal with large volumes

1348
01:22:30,650 --> 01:22:33,640
very large volumes

1349
01:22:34,580 --> 01:22:40,170
this information is transmitted in the push manner so the information searches for the relevant

1350
01:22:40,170 --> 01:22:44,000
consumers so that

1351
01:22:44,020 --> 01:22:49,190
so that information reaches you more efficiently you don't have to go wrong such as

1352
01:22:49,190 --> 01:22:55,410
this system did something new happened and the next system that something new happened but

1353
01:22:55,670 --> 01:23:00,940
information events about things that happened will be pushed to you once they happen so

1354
01:23:00,940 --> 01:23:06,310
we want this this real-time automatically updating system with flowing data

1355
01:23:06,530 --> 01:23:10,380
this is if you compare google search for example you post the query and you

1356
01:23:10,380 --> 01:23:15,870
get a fixed set of results of the current state of the system google and

1357
01:23:16,350 --> 01:23:22,130
as opposed to twitter where you post subscription to the people you're following and they

1358
01:23:22,130 --> 01:23:27,210
will incrementally update you with what they find all your twitter friends and you'll get

1359
01:23:27,210 --> 01:23:30,720
updates whenever they will find something new so

1360
01:23:30,750 --> 01:23:34,780
we have these if you will standing queries

1361
01:23:34,810 --> 01:23:39,130
which will give you results whenever there is new data as opposed to

1362
01:23:39,150 --> 01:23:46,100
a traditional query which oppose only once and you get the results only once

1363
01:23:46,120 --> 01:23:48,720
OK real time

1364
01:23:49,620 --> 01:23:54,320
is essential for for a lot that we're doing and it's been around for quite

1365
01:23:54,320 --> 01:24:00,090
awhile now only we're trying to formalize this as as a research

1366
01:24:01,190 --> 01:24:04,820
i want to be informed immediately this is really all use cases of my luggage

1367
01:24:04,820 --> 01:24:09,500
is not onboard of my playing when it's about to start i don't want to

1368
01:24:09,500 --> 01:24:13,130
find this out later posting a query but i want to inform and when this

1369
01:24:14,560 --> 01:24:19,260
or when two friends of mine this social media application sitting in the cafe close

1370
01:24:19,280 --> 01:24:23,990
to me and i want to combine different events in the relevant context of sitting

1371
01:24:23,990 --> 01:24:25,590
in the cafe so

1372
01:24:26,660 --> 01:24:31,510
processing events is always having to do combining many sources in different events to get

1373
01:24:31,510 --> 01:24:34,190
an interesting meaningful results

1374
01:24:34,190 --> 01:24:39,510
and inform me immediately after it becomes very likely that there will be a general

1375
01:24:39,530 --> 01:24:44,850
i wrote so even predicting future events is possible if you have learned and train

1376
01:24:44,880 --> 01:24:49,630
just system about his stories histories of events and you will be able to to

1377
01:24:49,630 --> 01:24:54,600
predict with a certain likeliness of what events will follow a certain set of events

1378
01:24:54,600 --> 01:24:56,460
that just happened

1379
01:24:56,500 --> 01:24:58,310
real time in examples

1380
01:24:58,340 --> 01:25:02,690
how much real time data is really out there that needs that this sort of

1381
01:25:02,690 --> 01:25:09,590
on the processing twitter world record was eight thousand eight hundred tweets per second on

1382
01:25:09,590 --> 01:25:16,390
the singer beyonce is pregnancy in was that on august twenty eleven so you can

1383
01:25:16,390 --> 01:25:19,800
see this is is an event frequency of

1384
01:25:19,810 --> 01:25:23,630
two weeks we get and the point is that that we want to process these

1385
01:25:23,630 --> 01:25:25,560
and and learn about

1386
01:25:25,620 --> 01:25:29,880
this event that happened immediately we don't want to put the tweets in the database

1387
01:25:30,110 --> 01:25:32,940
and see at the end of the month what happened but we want to be

1388
01:25:32,940 --> 01:25:38,480
informed that something meaningful just happened previous records were about the fee for women's world

1389
01:25:38,480 --> 01:25:43,870
cup seven thousand per second and this is all just one system on the web

1390
01:25:43,870 --> 01:25:44,980
which has this

1391
01:25:44,990 --> 01:25:47,560
so the frequency of events

1392
01:25:47,570 --> 01:25:51,830
another thing which is more well known as the financial market where you need nano

1393
01:25:51,830 --> 01:25:59,600
seconds time to trade once you get new information about stock price of all electronic

1394
01:25:59,600 --> 01:26:06,850
health you need remote patient monitoring where you have the lines of about five seconds

1395
01:26:06,870 --> 01:26:10,610
smart meters in the smart grid the upcoming

1396
01:26:10,620 --> 01:26:15,460
possibly upcoming energy revolution we have smart meters which help you

1397
01:26:15,490 --> 01:26:20,820
optimize the electric grid has periods about fifteen minutes

1398
01:26:20,830 --> 01:26:24,010
in real time

1399
01:26:24,060 --> 01:26:27,670
when i talk about real time this is the sort of relaxed notion of real

1400
01:26:27,670 --> 01:26:32,850
time though if you've learned in computer science or electrical engineering real time in my

1401
01:26:32,850 --> 01:26:35,120
case does not have hard deadlines

1402
01:26:35,130 --> 01:26:39,210
so i'm not talking about hard constraints but i'm talking about what people say is

1403
01:26:39,210 --> 01:26:43,370
business real time or near real time so i want the best effort processing of

1404
01:26:43,370 --> 01:26:48,850
these messages but not necessarily support hard deadlines so if you hear me talk about

1405
01:26:48,850 --> 01:26:51,260
real time this is the sort of relaxed

1406
01:26:51,290 --> 01:26:55,140
the idea of i want to process data as soon as it is there but

1407
01:26:55,140 --> 01:26:59,040
not necessarily guarantee new millisecond processing time

1408
01:26:59,080 --> 01:27:02,200
just so that terminology is clear

1409
01:27:04,690 --> 01:27:05,510
my goal

1410
01:27:05,520 --> 01:27:06,830
for today is that

1411
01:27:06,830 --> 01:27:12,040
there's a source of information from above and the source of information from below

1412
01:27:15,790 --> 01:27:19,030
deparle who wrote of classic texts on this

1413
01:27:19,040 --> 01:27:24,600
and and develop belief propagation large we call this the

1414
01:27:24,600 --> 01:27:25,990
and the land

1415
01:27:26,470 --> 01:27:34,010
messages hyper prior on lambda for likelihood so the stuff from above

1416
01:27:34,060 --> 01:27:38,390
you can think of is analogous to prior in the stuff from below you can

1417
01:27:38,390 --> 01:27:44,260
think of is analogous to likelihood and you're multiplying this prior likelihood and then renormalizing

1418
01:27:44,260 --> 01:27:48,950
to compute the probability over x given the evidence

1419
01:27:51,060 --> 01:27:54,720
so that's the first idea is is the separation

1420
01:27:54,740 --> 01:27:57,410
between upstream and downstream components

1421
01:27:57,430 --> 01:27:59,600
the second idea is

1422
01:28:00,490 --> 01:28:05,370
this upstream and downstream evidence these messages sorry

1423
01:28:05,410 --> 01:28:07,030
can be computed by

1424
01:28:07,040 --> 01:28:11,970
these probabilities these things that you're trying to multiply it can be computed via a

1425
01:28:11,970 --> 01:28:15,890
local message passing algorithm between the nodes in the graph

1426
01:28:15,930 --> 01:28:18,510
so we'll see that later

1427
01:28:18,530 --> 01:28:21,260
and then the third idea is to make sure

1428
01:28:21,270 --> 01:28:23,330
when you're sending messages

1429
01:28:23,660 --> 01:28:28,990
not to send back to a node any part of the message it sent to

1430
01:28:30,010 --> 01:28:32,430
so you basically want to send

1431
01:28:32,490 --> 01:28:37,160
each node is sending messages to other nodes but it needs to make sure that

1432
01:28:37,160 --> 01:28:44,700
the messages sent to other nodes is only collecting information from all sources

1433
01:28:44,720 --> 01:28:46,740
other than that

1434
01:28:46,950 --> 01:28:48,390
note OK

1435
01:28:48,390 --> 01:28:53,350
and only if you do that will you get the right probabilities coming out

1436
01:28:53,430 --> 01:28:57,220
but all this stuff is not sort of who do it just follows

1437
01:28:57,260 --> 01:29:01,390
you know you can prove it all tediously just by applying bayes rule and the

1438
01:29:01,390 --> 01:29:03,760
factorizations of on the graph

1439
01:29:06,450 --> 01:29:07,220
so i

1440
01:29:07,240 --> 01:29:11,260
and again i'm not going to go into the details of this because it would

1441
01:29:11,260 --> 01:29:13,180
take you know actually

1442
01:29:13,200 --> 01:29:15,640
i talked this before it takes about

1443
01:29:15,700 --> 01:29:19,140
an hour and a half or two hours just to understand the details of belief

1444
01:29:19,140 --> 01:29:23,410
propagation i want to give you a flavor and again you can kind of look

1445
01:29:23,410 --> 01:29:24,580
this stuff up

1446
01:29:24,580 --> 01:29:28,870
or try to code it up on your own at some point

1447
01:29:29,930 --> 01:29:35,180
the belief propagation algorithm involves messages

1448
01:29:35,200 --> 01:29:38,330
that are said top down

1449
01:29:38,350 --> 01:29:39,490
in other words

1450
01:29:40,660 --> 01:29:43,350
a node to its children

1451
01:29:44,390 --> 01:29:47,120
so the message that URI

1452
01:29:47,140 --> 01:29:49,870
sense to node x

1453
01:29:52,260 --> 01:29:54,330
the probability

1454
01:29:54,350 --> 01:29:59,240
distribution over u i given all the evidence that's upstream

1455
01:30:00,080 --> 01:30:02,830
edge u i x

1456
01:30:02,830 --> 01:30:09,850
OK then they are also messages coming bottom up from downstream so the message that

1457
01:30:09,890 --> 01:30:12,350
why jason's acts

1458
01:30:12,370 --> 01:30:14,260
is the probability

1459
01:30:15,760 --> 01:30:18,450
evidence is downstream from

1460
01:30:18,490 --> 01:30:20,490
the edge

1461
01:30:20,490 --> 01:30:22,260
x to y j

1462
01:30:22,270 --> 01:30:25,100
as a function of x

1463
01:30:25,120 --> 01:30:27,350
so these messages are

1464
01:30:27,370 --> 01:30:34,470
are vectors i mean we have we think about how we actually implement these messages

1465
01:30:34,890 --> 01:30:40,600
let's say x is a binary random variable so can take two values zero or

1466
01:30:41,950 --> 01:30:44,680
the this message is

1467
01:30:45,100 --> 01:30:47,870
two valued vector

1468
01:30:48,430 --> 01:30:52,700
with the value corresponding to the setting x equals zero and value correspond to the

1469
01:30:52,700 --> 01:30:54,700
setting x equals one

1470
01:30:54,760 --> 01:30:58,290
and similarly this message would be a

1471
01:30:58,330 --> 01:31:04,390
two element vector so here's your messages are being sent through vectors they get multiplied

1472
01:31:05,390 --> 01:31:08,120
for discrete random variables

1473
01:31:08,160 --> 01:31:12,220
and then the probability that we're interested in the probability of x given all the

1474
01:31:12,220 --> 01:31:13,810
evidence is

1475
01:31:14,740 --> 01:31:16,450
the product of

1476
01:31:16,470 --> 01:31:19,540
all of the downstream messages

1477
01:31:19,640 --> 01:31:22,990
times all the stuff coming from upstream

1478
01:31:23,040 --> 01:31:24,540
we normalize

1479
01:31:24,540 --> 01:31:33,030
and these are just additional equations for how to compute these downstream and upstream messages

1480
01:31:33,040 --> 01:31:35,950
so if you want all the gory details

1481
01:31:36,220 --> 01:31:37,240
you know

1482
01:31:37,270 --> 01:31:42,260
this is all you need to actually implement this algorithm if you wanted to and

1483
01:31:42,260 --> 01:31:47,220
of of course i make my slides available for the summer school

1484
01:31:51,060 --> 01:31:53,160
belief propagation is

1485
01:31:53,200 --> 01:31:59,370
very useful algorithm if you want to do inference in large graphs

1486
01:31:59,370 --> 01:32:02,640
were also executed that's really slow

1487
01:32:02,710 --> 01:32:05,880
we had to change approaches he went and rewrote this entire program in different language

1488
01:32:05,880 --> 01:32:10,620
called CPL which is related to see and the b programming language came before c

1489
01:32:10,680 --> 01:32:13,810
and of course it's very low level it's very fast but didn't have these nice

1490
01:32:13,810 --> 01:32:16,730
high level features and and you got it working but it took a lot of

1491
01:32:17,690 --> 01:32:20,550
and when he was done he decided you know i'm never going to tackle a

1492
01:32:20,550 --> 01:32:24,400
problem like this again until you have a proper tool for the job

1493
01:32:24,410 --> 01:32:27,740
in up eighteen t bell labs which is the same place c programming language came

1494
01:32:27,740 --> 01:32:30,920
out of back you carrying cannon ricci who invented that

1495
01:32:30,940 --> 01:32:33,890
they came with the language called c with classes

1496
01:32:33,900 --> 01:32:36,810
the precursor to c plus plus and people what that

1497
01:32:36,900 --> 01:32:40,490
it combined the best features of stimuli which is that object are to design

1498
01:32:40,520 --> 01:32:43,450
with the best features of c which is the runtime efficiency and said it was

1499
01:32:43,480 --> 01:32:46,450
fast flexible and help to think about problems

1500
01:32:47,270 --> 01:32:49,690
what he would do is work on the implementation

1501
01:32:49,700 --> 01:32:53,190
you had people actually using the language and when they need new features they said

1502
01:32:53,280 --> 01:32:55,740
are doesn't work you go fix it

1503
01:32:55,820 --> 01:32:58,700
and we have this cycle of real programmer solving real problems

1504
01:32:58,760 --> 01:33:01,110
with this developer said OK let's go fix it

1505
01:33:01,140 --> 01:33:03,680
and after a while you end up with c plus plus it's been around for

1506
01:33:03,680 --> 01:33:06,600
think this is the thing the twenty fifth anniversary possible so

1507
01:33:06,620 --> 01:33:08,390
but how exciting

1508
01:33:08,430 --> 01:33:11,890
i want to talk about the flat but what's really driving the development of this

1509
01:33:11,890 --> 01:33:14,510
language was expected to you

1510
01:33:14,550 --> 01:33:17,520
you have some quotes from this book called the design evolution c plus plus good

1511
01:33:17,520 --> 01:33:18,830
but you should read it

1512
01:33:18,840 --> 01:33:19,760
the first one

1513
01:33:19,770 --> 01:33:23,390
c plus plus is evolution should be driven by real problems and you don't want

1514
01:33:23,390 --> 01:33:25,620
to get down in the quest for perfection

1515
01:33:25,640 --> 01:33:30,010
basically this languages are designed to solve real problems in the real programmers like you

1516
01:33:30,010 --> 01:33:31,320
are going to run into

1517
01:33:31,330 --> 01:33:34,890
and not necessarily the best way it could try to do a good job is

1518
01:33:34,890 --> 01:33:38,540
not intended to be perfect there are some sloppy edges but it works

1519
01:33:38,580 --> 01:33:42,440
and because it is driven by real problems in real design issues you can solve

1520
01:33:42,440 --> 01:33:43,980
real problems with it

1521
01:33:43,990 --> 01:33:45,970
if you think that's a good thing in language

1522
01:33:46,020 --> 01:33:49,390
i personally do i think it's good to have a language they can solve problems

1523
01:33:49,400 --> 01:33:52,840
then c plus plus is a good choice

1524
01:33:52,850 --> 01:33:56,010
this is the one that you've seen them most don't try to force people c

1525
01:33:56,010 --> 01:33:59,590
plus plus gives you so many choices that your question should not be well how

1526
01:33:59,590 --> 01:34:02,800
do i do this in the language but more which of these hundreds of options

1527
01:34:02,800 --> 01:34:04,390
is the best choice for me

1528
01:34:04,400 --> 01:34:07,980
it really trust you as the language will give you an incredible amount of flexibility

1529
01:34:08,020 --> 01:34:11,160
even in the world let you make the wrong decision

1530
01:34:11,170 --> 01:34:16,020
c plus plus the language that will let you ride a bicycle without helmet because

1531
01:34:16,020 --> 01:34:19,490
that one family to go under bridges exactly quarter inch over your head you what

1532
01:34:19,490 --> 01:34:21,720
have you not you by

1533
01:34:21,790 --> 01:34:24,180
do you have to worry about a lot of risk but in the end you

1534
01:34:24,180 --> 01:34:28,620
have more flexibility than you will find most of the languages c

1535
01:34:28,700 --> 01:34:30,920
and finally i think the most important one

1536
01:34:30,950 --> 01:34:31,930
is this

1537
01:34:31,940 --> 01:34:36,970
c plus plus makes programming more enjoyable for serious programmers the two things here are

1538
01:34:36,970 --> 01:34:39,030
serious programmers and more enjoyable

1539
01:34:39,050 --> 01:34:43,780
this is a professional language is used in industry everywhere it's very fast it's very

1540
01:34:43,780 --> 01:34:46,900
efficient and has a bit of the learning curve it is kind of tricky to

1541
01:34:46,900 --> 01:34:49,050
get into the language but once you've got it

1542
01:34:49,090 --> 01:34:51,650
the second part the more enjoyable is so true

1543
01:34:51,660 --> 01:34:53,890
this language is find to write things

1544
01:34:53,910 --> 01:34:57,920
once you've got it down you will actually step back inferencing while i just wrote

1545
01:34:57,920 --> 01:35:01,440
something took every single word of the said that i had read has against contains

1546
01:35:01,480 --> 01:35:03,770
specific letter in a single line of code

1547
01:35:03,780 --> 01:35:06,730
or i wanted all the contents of this file into my set one line of

1548
01:35:06,730 --> 01:35:12,450
code or ideas for a template that is a multi-dimensional array any dimension i want

1549
01:35:12,510 --> 01:35:16,940
that's those not trivial accomplishments and you can do with this language it's fine

1550
01:35:16,980 --> 01:35:19,570
it's a great language and if you think that this sort of philosophy is what

1551
01:35:19,570 --> 01:35:23,550
you want we will trust you will really like you make decisions rather than forcing

1552
01:35:23,550 --> 01:35:25,070
into anyone paradigm

1553
01:35:25,080 --> 01:35:26,540
learn c plus plus

1554
01:35:26,610 --> 01:35:28,200
i think a lot

1555
01:35:29,110 --> 01:35:33,080
possibly there's lots of it but you know i'm not going to fly across them

1556
01:35:33,090 --> 01:35:35,960
here to teach about c plus plus let's and some of the details what is

1557
01:35:35,960 --> 01:35:38,010
this language look like

1558
01:35:38,060 --> 01:35:41,030
and the first thing i want to do is talk about what channel about it

1559
01:35:42,430 --> 01:35:45,260
since day one you probably have seen things like this

1560
01:35:45,310 --> 01:35:48,170
how include german very first one right here

1561
01:35:48,210 --> 01:35:52,160
what is in the scandal things everybody actually looked in german before and we call

1562
01:35:52,160 --> 01:35:54,360
that the german while looked inside

1563
01:35:54,370 --> 01:35:55,810
no one

1564
01:35:58,030 --> 01:36:00,440
if you look inside channel

1565
01:36:00,450 --> 01:36:02,460
it looks mostly like this

1566
01:36:02,510 --> 01:36:05,360
there are basically three important ones need to know

1567
01:36:05,370 --> 01:36:09,470
can include strings so we talked about the strange process there was built like and

1568
01:36:09,470 --> 01:36:10,860
or double it's a class

1569
01:36:10,920 --> 01:36:14,450
it's like any other object like the vector map you do need to pound included

1570
01:36:14,870 --> 01:36:18,060
we care for you because since you use it everywhere and it's easy to forget

1571
01:36:18,060 --> 01:36:21,280
and you can get some pretty nasty compilers if you don't include it with you

1572
01:36:21,940 --> 01:36:25,080
well then i will do that for you

1573
01:36:25,090 --> 01:36:29,890
the second thing is the error function using error german just give you from for

1574
01:36:29,900 --> 01:36:30,710
the end

1575
01:36:30,780 --> 01:36:33,830
but this last one right here using namespace std

1576
01:36:33,840 --> 01:36:36,080
using the standard name space

1577
01:36:36,100 --> 01:36:37,790
one hundred is slightly

1578
01:36:37,830 --> 01:36:43,100
it looks almost like you know english sentence you know compiler make my code

1579
01:36:43,120 --> 01:36:44,930
what's going on

1580
01:36:44,980 --> 01:36:48,050
well i'll show you

1581
01:36:48,100 --> 01:36:51,930
suppose i have this sort of program that incivility

1582
01:36:51,990 --> 01:36:54,810
so i just want to know how to do it i was very happy with

1583
01:36:54,810 --> 01:36:57,740
string if you know here i don't have german

1584
01:36:57,770 --> 01:37:00,920
i'm going to make a string and printed

1585
01:37:00,960 --> 01:37:04,080
so the question is when i say string my strength

1586
01:37:04,160 --> 01:37:05,950
one of my referring to

1587
01:37:05,960 --> 01:37:10,280
when this happens the compiler says well go look for something called strain

1588
01:37:10,320 --> 01:37:12,310
and we have included string

1589
01:37:12,310 --> 01:37:16,040
it put inside the big bucket of something called space and

1590
01:37:16,080 --> 01:37:19,320
the idea is that the real name of string is standard during the roman see

1591
01:37:19,320 --> 01:37:22,860
out is standard see out the real name and all the standard and it's just

1592
01:37:22,860 --> 01:37:25,210
that if you make your your own versions of the things it doesn't have the

1593
01:37:25,210 --> 01:37:29,680
same name as the standard conflict in your approach to compile

1594
01:37:29,720 --> 01:37:31,870
but the problem is that if you try to run this

1595
01:37:31,890 --> 01:37:33,240
what what happened is that

1596
01:37:33,260 --> 01:37:37,480
the compilers strange looks up here and this wall

1597
01:37:37,550 --> 01:37:41,040
says oops that's inside the namespace i can go on there

1598
01:37:41,050 --> 01:37:45,510
and again really nasty compiler something like strings not defined yet not find and else

1599
01:37:46,690 --> 01:37:50,810
now time in your program you kind of want things like c out and strange

1600
01:37:50,810 --> 01:37:52,340
actually exist

1601
01:37:52,390 --> 01:37:57,190
so rather than saying you have to standard string standard CEN standard and l

1602
01:37:57,230 --> 01:37:59,780
you can introduce this little phrase right here

1603
01:37:59,790 --> 01:38:01,490
using a standard

1604
01:38:01,520 --> 01:38:02,870
and what this says

1605
01:38:02,900 --> 01:38:06,220
is take the floor and get rid of it it's like mister compiler tear down

1606
01:38:06,220 --> 01:38:08,580
this wall and the result

1607
01:38:08,600 --> 01:38:11,660
is that this kind of barrier to its well that kind of

1608
01:38:11,700 --> 01:38:17,640
ten times it's all still include inside the inside of the centre namespace not accessible

1609
01:38:17,690 --> 01:38:20,090
when you say string goes oh yeah that thing

1610
01:38:20,150 --> 01:38:22,110
and actually compiles

1611
01:38:22,140 --> 01:38:24,560
the most they are not going to be hitting you with c plus plus and

1612
01:38:24,560 --> 01:38:26,860
say you must know this you must know this you must know this is not

1613
01:38:26,860 --> 01:38:28,650
enough time to go over everything

1614
01:38:28,720 --> 01:38:31,720
but this a line might be something worth committing to memory because it's the most

1615
01:38:31,720 --> 01:38:35,220
noticeable change you'll see when you stop using german if you don't put that line

1616
01:38:35,290 --> 01:38:38,690
your code to get lots of areas in just kind of scary

1617
01:38:38,700 --> 01:38:42,340
so if you put this line in nineteen ninety percent of from our problems will

1618
01:38:42,340 --> 01:38:44,570
go away

1619
01:38:44,570 --> 01:38:48,290
that's that one that's stanley and if you want to work

1620
01:38:48,320 --> 01:38:52,890
if you write this code you basically replace all german for ten if i have

1621
01:38:52,920 --> 01:38:56,740
concluded this one STDIV the c standard library

1622
01:38:56,760 --> 01:39:01,680
i'm i'm thinking when they invented the seagram language across them several million dollars per

1623
01:39:01,690 --> 01:39:04,760
hour consonant they put in the header file so

1624
01:39:04,770 --> 01:39:08,050
they put it is small parcels delivery

1625
01:39:08,060 --> 01:39:12,140
and the idea is that you have these two live important ones pattern strings using

1626
01:39:12,140 --> 01:39:13,770
the standard name space

1627
01:39:13,770 --> 01:39:18,510
the circuits essentially multilayer neural net but for booleans

1628
01:39:18,510 --> 01:39:21,980
as as a boolean operations and are cases you can construct way you

1629
01:39:21,980 --> 01:39:26,680
can show if the number of stages of computations in a boolean

1630
01:39:26,690 --> 01:39:29,620
circuit is not sufficient the number of layers

1631
01:39:29,620 --> 01:39:32,120
then you might need to represent certain functions when

1632
01:39:32,120 --> 01:39:35,300
an exponential number of units within

1633
01:39:36,550 --> 01:39:40,580
layers of that that boolean circuit would be to shallow

1634
01:39:40,930 --> 01:39:45,250
the idea is maybe that allows us to compensate for otherwise requiring

1635
01:39:45,260 --> 01:39:50,650
an exponential number of units and you know just in practice i mean

1636
01:39:50,650 --> 01:39:53,670
you're aware of this but in each recognitions been shown to be

1637
01:39:53,670 --> 01:39:57,230
game changing the use of deep neural nets and same thing in computer

1638
01:39:57,230 --> 01:40:03,900
vision so this i guess like this into thousand six there was

1639
01:40:03,910 --> 01:40:08,060
suddenly soms success training deep neural nets and so i'm

1640
01:40:08,270 --> 01:40:12,150
labeling the next few concept and talk about as like deep learning

1641
01:40:12,150 --> 01:40:15,440
concepts that could presumably have been invented before like what

1642
01:40:15,440 --> 01:40:19,220
i talk about so far is mostly things that were known

1643
01:40:19,470 --> 01:40:22,180
you know before the thousand six but

1644
01:40:22,330 --> 01:40:25,830
part of the motivation behind them i guess to some extent was like

1645
01:40:25,980 --> 01:40:29,990
how do we train if effectively very very deep neural nets potentially

1646
01:40:29,990 --> 01:40:32,530
very deep neural nets i'm going over some of the

1647
01:40:32,850 --> 01:40:35,530
considered difficulties a hypothesis for y

1648
01:40:35,530 --> 01:40:39,350
for given deep neural network a given problem you might have difficulties

1649
01:40:39,350 --> 01:40:41,520
fitting to your data get in the book good

1650
01:40:41,700 --> 01:40:44,180
performance generalization performance

1651
01:40:44,710 --> 01:40:48,200
one first hypothesis is that the optimization problem is to hard

1652
01:40:48,210 --> 01:40:52,480
for some reason for architecture so one potential difficulty

1653
01:40:52,480 --> 01:40:54,760
that's well known as the vanishing gradient

1654
01:40:54,890 --> 01:40:58,930
problem if you remember when we covered backprop at every

1655
01:40:59,550 --> 01:41:03,250
every second stage you would multiply the weight matrix the activation

1656
01:41:03,250 --> 01:41:05,850
gradient and would go through the nonlinear already by doing an

1657
01:41:05,850 --> 01:41:10,500
element-wise multiplication with the activation func tions

1658
01:41:10,860 --> 01:41:14,510
their of and if you have saturation you might be in regime

1659
01:41:14,510 --> 01:41:16,630
for some of the units where the gradient

1660
01:41:16,720 --> 01:41:19,810
did there essentially zero to actually be blocking

1661
01:41:20,000 --> 01:41:24,260
the gradients or not passing in the learning information through

1662
01:41:24,270 --> 01:41:27,550
that unit and to lower layers that's one thing that can happen

1663
01:41:27,550 --> 01:41:30,300
actually also multiplying by the weight matrix might

1664
01:41:30,300 --> 01:41:32,890
there might be some problems associated with that

1665
01:41:33,730 --> 01:41:36,020
i think maybe wash over cover has problems

1666
01:41:36,290 --> 01:41:40,720
with it does not are dense so so that might be one problem

1667
01:41:41,550 --> 01:41:45,600
and it is you could also make a statement about how this is nonconvex

1668
01:41:45,600 --> 01:41:48,370
optimization problem there has been some interesting theoretical

1669
01:41:48,370 --> 01:41:51,340
result show doing that if you have a very large neural net with

1670
01:41:51,340 --> 01:41:53,250
a lot of parameters some of the

1671
01:41:53,480 --> 01:41:58,210
issues you suspect in small dimensional problems are non-convex

1672
01:41:59,270 --> 01:42:02,590
are much less harmful them with thing essentially

1673
01:42:02,590 --> 01:42:05,330
in high dimensions when optimizing function that lives in

1674
01:42:05,330 --> 01:42:09,340
very high dimensions the local optima tend to be

1675
01:42:11,380 --> 01:42:15,110
a lot of local optima them to be fairly close to the global optimum

1676
01:42:15,120 --> 01:42:16,940
in terms of absolute performance

1677
01:42:17,110 --> 01:42:20,650
so that's it seems that says come from some insides that people

1678
01:42:20,660 --> 01:42:24,090
and neural nets and those are fairly reassuring results and that

1679
01:42:24,090 --> 01:42:27,470
would explain why in certain cases we don't really see optimization

1680
01:42:27,470 --> 01:42:28,620
problems yes

1681
01:42:36,740 --> 01:42:41,400
so this is on studying actually random neural nets

1682
01:42:41,840 --> 01:42:46,600
which random weights and it it's i what's the reference for for

1683
01:42:46,880 --> 01:42:49,390
for the the fact that most local optima

1684
01:42:50,000 --> 01:42:54,290
bad in neural nets let's dimensions or a

1685
01:43:06,240 --> 01:43:11,550
yeah ok yeah from yeah ok right or

1686
01:43:13,980 --> 01:43:14,840
saddle point

1687
01:43:22,150 --> 01:43:25,700
so another about this might be that when using deep neural nets

1688
01:43:25,710 --> 01:43:29,680
might be in a difficult overfitting situation so if you have

1689
01:43:30,100 --> 01:43:32,800
fairly complex functions that we're fitting to data

1690
01:43:33,690 --> 01:43:36,860
then we might the situation where there's a high variance in

1691
01:43:36,860 --> 01:43:40,020
our estimate of our solution from from our our training set

1692
01:43:40,380 --> 01:43:44,160
and so maybe the also potential probably might face is that

1693
01:43:44,160 --> 01:43:47,250
we essentially overfitting compared to fitting smaller networks

1694
01:43:47,250 --> 01:43:53,410
if you're layers and which you know situation your n will depend

1695
01:43:53,410 --> 01:43:56,210
on the architecture of the problem the amount of data

1696
01:43:57,000 --> 01:44:00,460
so this this discussed use solutions

1697
01:44:01,050 --> 01:44:03,880
and how they relate to either one of of these issues

1698
01:44:04,110 --> 01:44:07,450
so if the problems on the fitting you want to do better at

1699
01:44:07,460 --> 01:44:12,080
optimization one solution effectively that has grown is to

1700
01:44:12,090 --> 01:44:16,170
use unils faster computers faster computer hardware so use cheap

1701
01:44:16,180 --> 01:44:21,060
use if your say on the plateau and this is essentially why optimizations

1702
01:44:21,060 --> 01:44:24,360
taking a long time well if each generation is actually very very

1703
01:44:24,360 --> 01:44:39,490
what about us

1704
01:45:00,700 --> 01:45:06,880
there are

1705
01:45:18,820 --> 01:45:25,550
i i

1706
01:49:44,280 --> 01:49:47,860
OK so

1707
01:51:53,010 --> 01:51:59,130
i can

1708
01:52:49,860 --> 01:52:59,010
it's also

1709
01:52:59,010 --> 01:52:59,800
an approximate

1710
01:53:00,610 --> 01:53:02,130
so here's the diagonal

1711
01:53:02,280 --> 01:53:04,210
these approximate kernel in the

1712
01:53:06,440 --> 01:53:07,880
number of sample points the

1713
01:53:08,550 --> 01:53:09,010
point is

1714
01:53:10,880 --> 01:53:12,030
so it thousand

1715
01:53:12,340 --> 01:53:14,010
thousand one million ten million

1716
01:53:14,650 --> 01:53:16,920
so you can even more than

1717
01:53:17,510 --> 01:53:18,340
the problem was

1718
01:53:18,960 --> 01:53:20,590
approximate kernel games

1719
01:53:21,420 --> 01:53:22,740
if this is feasible

1720
01:53:23,990 --> 01:53:26,190
giving still significantly

1721
01:53:28,090 --> 01:53:30,990
here's the clustering accuracy indian cuisines

1722
01:53:31,800 --> 01:53:35,670
i hope you're all clusters no points should be expressed

1723
01:53:36,510 --> 01:53:39,550
so that means that small number of data points

1724
01:53:41,800 --> 01:53:47,920
san approximation guarantees the accuracy drops a little bit but still significantly higher than

1725
01:53:48,400 --> 01:53:49,110
the kings

1726
01:53:55,110 --> 01:53:57,530
tiny images dataset was what

1727
01:54:00,590 --> 01:54:01,690
it is the right

1728
01:54:02,940 --> 01:54:04,400
the idea is that

1729
01:54:06,940 --> 01:54:08,190
which is the eighty million

1730
01:54:08,670 --> 01:54:09,960
thank you very much

1731
01:54:10,050 --> 01:54:11,070
thumbnail images

1732
01:54:12,170 --> 01:54:13,940
seventy five thousand classes

1733
01:54:15,070 --> 01:54:17,590
and this is represented by two hundred fourteen

1734
01:54:18,170 --> 01:54:20,070
you need information this

1735
01:54:22,610 --> 01:54:24,570
so the from e

1736
01:54:24,670 --> 01:54:25,340
the event

1737
01:54:26,170 --> 01:54:27,530
based on some key words

1738
01:54:28,570 --> 01:54:29,760
this in features

1739
01:54:31,610 --> 01:54:33,840
and a small portion of the data set

1740
01:54:35,030 --> 01:54:36,300
he see far ten

1741
01:54:37,280 --> 01:54:38,550
these ten classes

1742
01:54:39,320 --> 01:54:40,740
sixty thousand images

1743
01:54:41,150 --> 01:54:42,360
manually annotated

1744
01:54:44,340 --> 01:54:45,070
these classes

1745
01:54:46,030 --> 01:54:47,650
so do you really want do this

1746
01:54:48,190 --> 01:54:49,130
small subset

1747
01:54:49,710 --> 01:54:52,260
to determine the accuracy of classification

1748
01:54:54,300 --> 01:54:55,710
on the last year

1749
01:54:59,190 --> 01:55:01,820
so we cluster these tiny images

1750
01:55:04,740 --> 01:55:06,710
the last time approximate

1751
01:55:07,940 --> 01:55:12,420
thousand and once it's only one player as in this example as

1752
01:55:15,210 --> 01:55:18,460
that's the accuracy currently uses about

1753
01:55:20,050 --> 01:55:21,840
thirty percent approximate kernel

1754
01:55:23,130 --> 01:55:26,650
the global spectral clustering is twenty seven percent in

1755
01:55:29,420 --> 01:55:31,280
he was tremendous

1756
01:55:32,070 --> 01:55:33,510
missus three percent three

1757
01:55:33,800 --> 01:55:37,130
centre of large number of images that is very

1758
01:55:37,710 --> 01:55:38,460
laughter is

1759
01:55:39,050 --> 01:55:40,130
but best ones

1760
01:55:40,240 --> 01:55:41,530
classification accuracy

1761
01:55:42,150 --> 01:55:42,510
see by

1762
01:55:43,510 --> 01:55:44,110
people person

1763
01:55:49,960 --> 01:55:53,320
so we can also now this to be approximate kernel

1764
01:55:53,440 --> 01:55:56,780
gain is far better scalability and fast the

1765
01:55:58,880 --> 01:56:04,570
the two main challenges in it doing clustering for large number of points one is

1766
01:56:05,030 --> 01:56:08,260
signing finding the closest cluster centre put all the

1767
01:56:09,610 --> 01:56:10,880
and all processes

1768
01:56:12,420 --> 01:56:13,880
just a few points and find

1769
01:56:16,740 --> 01:56:17,880
so in the end

1770
01:56:18,010 --> 01:56:19,240
for example employees

1771
01:56:19,710 --> 01:56:20,070
and that's

1772
01:56:20,510 --> 01:56:21,440
union miners

1773
01:56:22,150 --> 01:56:22,650
and around

1774
01:56:22,990 --> 01:56:24,340
one to be partitions

1775
01:56:24,710 --> 01:56:25,400
the sidebars

1776
01:56:25,670 --> 01:56:26,690
you must be

1777
01:56:29,900 --> 01:56:31,360
and approximate the game

1778
01:56:31,420 --> 01:56:34,130
he's easy task you find cluster centers

1779
01:56:44,320 --> 01:56:46,110
assign each point in the task is to the

1780
01:56:46,460 --> 01:56:52,240
the center that's an

1781
01:56:52,570 --> 01:56:55,670
combine the labels for each task in the cluster

1782
01:56:55,800 --> 01:56:56,340
so example

1783
01:56:57,740 --> 01:57:00,590
that we now you have multiple clusterings

1784
01:57:00,820 --> 01:57:03,030
they don't and then combine them together

1785
01:57:07,130 --> 01:57:07,820
so you be

1786
01:57:08,690 --> 01:57:10,070
distributed approximate

1787
01:57:10,900 --> 01:57:14,050
keys to the conservative circles

1788
01:57:17,710 --> 01:57:18,840
i dataset

1789
01:57:19,590 --> 01:57:20,650
ten millions even

1790
01:57:21,420 --> 01:57:22,150
it's given you know

1791
01:57:31,320 --> 01:57:31,650
so what

1792
01:57:32,490 --> 01:57:33,840
additionally approximate the

1793
01:57:36,300 --> 01:57:37,490
even the approximate

1794
01:57:38,400 --> 01:57:40,240
you more than ten million points

1795
01:57:40,740 --> 01:57:41,460
it would

1796
01:57:43,210 --> 01:57:45,280
so we need to something for so

1797
01:57:46,380 --> 01:57:47,800
that's the new example

1798
01:57:50,510 --> 01:57:57,210
that's one of the approximate the beginning of the points in the cluster

1799
01:58:01,360 --> 01:58:01,670
so now

1800
01:58:02,210 --> 01:58:04,260
we do this sample and cluster

1801
01:58:06,110 --> 01:58:07,800
one year labelled one

1802
01:58:08,130 --> 01:58:09,710
clustering one billion points

1803
01:58:10,230 --> 01:58:10,710
so today

1804
01:58:11,730 --> 01:58:13,210
seven one million points

1805
01:58:14,110 --> 01:58:16,010
and this is what to be

1806
01:58:16,590 --> 01:58:17,650
sampling quality

1807
01:58:19,240 --> 01:58:21,210
he said that don't have to be

1808
01:58:22,170 --> 01:58:23,900
one million by one million barrels

1809
01:58:25,800 --> 01:58:26,110
so now

1810
01:58:26,990 --> 01:58:27,340
will be

1811
01:58:27,960 --> 01:58:31,320
only time he's this sample cluster

1812
01:58:31,570 --> 01:58:33,150
sample and the

1813
01:58:33,730 --> 01:58:34,420
and it was

1814
01:58:35,440 --> 01:58:37,490
three minutes per game is one point

1815
01:58:38,760 --> 01:58:39,730
sampling cluster

1816
01:58:40,610 --> 01:58:41,760
forty by forty

1817
01:58:46,400 --> 01:58:48,150
and the clustering accuracy so

1818
01:58:48,490 --> 01:58:51,070
these amazing fifty percent random splits

1819
01:58:52,030 --> 01:58:53,090
sixty five percent

1820
01:58:58,490 --> 01:58:59,030
so some

1821
01:59:00,260 --> 01:59:01,010
now is to

1822
01:59:03,550 --> 01:59:05,130
the application the

1823
01:59:05,670 --> 01:59:06,010
this is

1824
01:59:06,940 --> 01:59:09,070
it's not easy to get access to the data

1825
01:59:11,300 --> 01:59:12,030
reason you

1826
01:59:12,190 --> 01:59:12,940
what are you

1827
01:59:13,710 --> 01:59:15,240
is these hyperlink

1828
01:59:17,440 --> 01:59:18,150
it is you

1829
01:59:20,280 --> 01:59:20,670
one of

1830
01:59:26,570 --> 01:59:28,780
you lose something browsing data

1831
01:59:29,210 --> 01:59:34,150
challenges of sparsity so reduce dimensionality and position

1832
01:59:35,710 --> 01:59:38,400
how do you the evaluation is not

1833
01:59:40,030 --> 01:59:40,510
reasons why

1834
01:59:44,440 --> 01:59:46,550
the accuracy of such databases

1835
01:59:46,550 --> 01:59:48,410
repeated his insistence

1836
01:59:48,570 --> 01:59:49,670
and or

1837
01:59:49,690 --> 01:59:52,320
this is just a way of actually understanding

1838
01:59:52,340 --> 01:59:56,750
high dimensional with graphically understanding the behavior of the system in terms of what actually

1839
01:59:58,560 --> 02:00:02,290
the system does what it does you can actually see what this seems to settle

1840
02:00:02,300 --> 02:00:06,270
in the same place that never is actually in the same place to hold equilibrium

1841
02:00:07,520 --> 02:00:09,120
according to

1842
02:00:10,310 --> 02:00:14,520
as you have something there which is more stable

1843
02:00:19,240 --> 02:00:21,740
i'm going to talk about

1844
02:00:21,760 --> 02:00:23,390
conditions of

1845
02:00:23,400 --> 02:00:26,360
the office of complexity what by complexity

1846
02:00:26,370 --> 02:00:29,090
and what we mean by the organisation

1847
02:00:29,720 --> 02:00:34,270
it's no use are really doing ethnographic work which is basically the tools that we

1848
02:00:34,270 --> 02:00:35,680
have use

1849
02:00:35,770 --> 02:00:39,920
available to us what technical systems and there is a whole

1850
02:00:39,980 --> 02:00:46,010
research technology and development digital marketing methods structure during

1851
02:00:46,070 --> 02:00:52,070
and for example monitoring tracking various ways to capture data issues like trust

1852
02:00:52,270 --> 02:00:54,790
and see how they actually work

1853
02:00:54,970 --> 02:00:59,020
types of systems that are both physical rational

1854
02:00:59,030 --> 02:01:00,320
virtual choir

1855
02:01:00,360 --> 02:01:07,400
that's when that's done not far behind by actions in his life in san diego

1856
02:01:07,410 --> 02:01:09,890
UC san diego

1857
02:01:09,970 --> 02:01:11,630
we need to actually trying to model

1858
02:01:11,830 --> 02:01:17,360
former so complex theory focuses complex nonlinear open systems

1859
02:01:18,380 --> 02:01:19,470
so was one

1860
02:01:19,560 --> 02:01:25,710
motivation using this vision from the diamonds on the inside of the kind of like

1861
02:01:25,740 --> 02:01:31,880
pushes new possibilities for new pattern or there's something from the outside is the system

1862
02:01:31,890 --> 02:01:37,210
shown for example in new EU regulations be something to do this is show and

1863
02:01:37,480 --> 02:01:40,490
you have to organize and around the quantitative

1864
02:01:41,770 --> 02:01:47,500
o you can predict from the as any parts have actually use the term is

1865
02:01:47,540 --> 02:01:50,740
one of the companies have with ethnographic research temperature

1866
02:01:50,760 --> 02:01:53,130
focus on the actions of individual

1867
02:01:54,180 --> 02:01:56,330
and i think can put together

1868
02:01:56,940 --> 02:02:02,140
you know we can just extrapolate and unfortunately it doesn't work like because was removed

1869
02:02:02,140 --> 02:02:02,980
from his

1870
02:02:04,190 --> 02:02:07,680
so it's looking organizational systems that you get different

1871
02:02:07,700 --> 02:02:14,130
we need it on a one-to-one basis however under certain conditions you can actually see

1872
02:02:14,140 --> 02:02:15,950
that you can model

1873
02:02:15,970 --> 02:02:18,770
and there's lots of work done by center women

1874
02:02:18,790 --> 02:02:21,270
institute you can actually model

1875
02:02:21,330 --> 02:02:26,070
as an an individual agent and simulations

1876
02:02:26,110 --> 02:02:28,000
and the same

1877
02:02:28,030 --> 02:02:31,930
units should be in the same way under certain conditions

1878
02:02:32,000 --> 02:02:34,310
so my interest in the trust

1879
02:02:37,490 --> 02:02:39,220
you appear distorted around the

1880
02:02:39,280 --> 02:02:47,120
from simple occupied secure simple states in complexity theory large-scale water as complex apparent to

1881
02:02:47,120 --> 02:02:49,120
order the local scale

1882
02:02:49,130 --> 02:02:53,860
so you've got some different ways of approaching this one's got very

1883
02:02:53,870 --> 02:02:57,930
the only stuff something really crazy happens on the other side

1884
02:02:59,920 --> 02:03:01,660
order arises

1885
02:03:02,380 --> 02:03:04,690
as a result of art

1886
02:03:04,740 --> 02:03:07,300
measures of complexity OK

1887
02:03:07,310 --> 02:03:08,470
i two

1888
02:03:08,490 --> 02:03:10,560
measures to write about types

1889
02:03:13,820 --> 02:03:17,310
and also organizing systems are

1890
02:03:17,400 --> 02:03:19,640
well if you on

1891
02:03:19,650 --> 02:03:23,190
complexity theory studies also called position

1892
02:03:23,230 --> 02:03:27,080
what it's all things that organisation emergence cooperation

1893
02:03:27,090 --> 02:03:28,220
so these are

1894
02:03:29,700 --> 02:03:31,370
the same

1895
02:03:31,380 --> 02:03:33,640
the sphere of interest

1896
02:03:33,650 --> 02:03:37,890
around the notion of system and what system systems

1897
02:03:37,900 --> 02:03:43,610
so i deterministic complexity based on information theory algorithmic content strings of bits to find

1898
02:03:43,720 --> 02:03:49,390
the length of the shortest programme the standard was computed and destroy the

1899
02:03:49,430 --> 02:03:54,020
and hold OK so there's an measure complexity

1900
02:03:54,040 --> 02:03:58,980
so in the computational complexity and that this process time

1901
02:03:58,990 --> 02:04:04,080
the complex is equated with around so

1902
02:04:04,150 --> 02:04:08,290
OK need to ask questions to use

1903
02:04:08,320 --> 02:04:09,530
then we have to say

1904
02:04:09,540 --> 02:04:14,960
this part of statistical complexity measure the degree of structural pattern present in a complex

1905
02:04:15,990 --> 02:04:18,820
this is a little bit more about what i'm talking about

1906
02:04:19,050 --> 02:04:26,370
so it was a second problem statistical complexity have randomness equals the maximal complexity served

1907
02:04:29,210 --> 02:04:31,990
keep the key idea here is that

1908
02:04:32,020 --> 02:04:33,480
entropy gen

1909
02:04:33,500 --> 02:04:39,640
the entropy measure is the measure of order in the system for organizations systems

1910
02:04:39,880 --> 02:04:44,200
so it's on the opposite to what should be dark energy entropy and y is

1911
02:04:44,210 --> 02:04:48,040
the measure of the current level of organisation system sort of a contradiction

1912
02:04:48,270 --> 02:04:49,780
the above basically what

1913
02:04:51,130 --> 02:04:53,270
so far as i understand

1914
02:04:53,960 --> 02:05:00,270
that you have the org conditions in stream or disorder and

1915
02:05:00,320 --> 02:05:02,070
these vanish our

