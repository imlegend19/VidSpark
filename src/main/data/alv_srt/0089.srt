1
00:00:00,000 --> 00:00:02,880
hard work

2
00:00:18,400 --> 00:00:20,380
now really

3
00:00:46,840 --> 00:00:49,860
what they

4
00:00:52,380 --> 00:00:55,040
i think

5
00:01:24,110 --> 00:01:25,670
if get

6
00:01:36,440 --> 00:01:37,580
you can use

7
00:03:10,170 --> 00:03:14,650
and it's not

8
00:03:41,840 --> 00:03:43,630
the or

9
00:03:58,590 --> 00:04:02,290
it meant

10
00:04:21,130 --> 00:04:21,730
we have

11
00:04:25,520 --> 00:04:35,840
so what

12
00:04:43,960 --> 00:04:47,250
you know one

13
00:04:53,040 --> 00:04:57,190
many of know

14
00:05:18,400 --> 00:05:19,130
they a

15
00:05:33,110 --> 00:05:37,320
you know about

16
00:05:37,320 --> 00:05:39,290
and that

17
00:05:39,310 --> 00:05:43,070
then of course the end you make predictions and decisions

18
00:05:45,920 --> 00:05:47,970
you know

19
00:05:47,990 --> 00:05:51,550
obviously a lot of what i'm going to talk about is in this

20
00:05:51,570 --> 00:05:56,680
modelling framework here but there are other parts of it the make me uncomfortable

21
00:05:56,740 --> 00:05:59,600
john get to the more depressed part of my talk

22
00:05:59,610 --> 00:06:00,760
two the

23
00:06:00,980 --> 00:06:05,210
sort of a nice this is very nice find modelling a particular problem

24
00:06:05,230 --> 00:06:07,040
i want to really solve it

25
00:06:07,050 --> 00:06:11,550
i tried put some understanding into that etcetera

26
00:06:15,090 --> 00:06:20,180
what about the second part of this question what is an operations let's talk about

27
00:06:20,180 --> 00:06:23,780
parametric versus non parametric model so we're all the same page

28
00:06:23,800 --> 00:06:28,560
parametric models assume some finite set of parameters data

29
00:06:28,570 --> 00:06:30,810
in the parameters

30
00:06:32,270 --> 00:06:36,470
are independent of the observed data in the sense you know that's that's that's captured

31
00:06:36,470 --> 00:06:38,560
by this equation here

32
00:06:38,570 --> 00:06:44,180
you know if you're if you're frequencies this is fine you just open conditioning bar

33
00:06:45,110 --> 00:06:48,920
but essentially the idea is that you you capture

34
00:06:48,930 --> 00:06:51,810
everything there is to know about the data

35
00:06:51,820 --> 00:06:55,360
through your parameters theta

36
00:06:55,370 --> 00:07:00,640
this is what peter orbanz was describing in his first tour was talking about the

37
00:07:00,640 --> 00:07:03,010
pattern and the noise

38
00:07:03,030 --> 00:07:05,960
i think that captures the pattern

39
00:07:05,970 --> 00:07:06,690
but now

40
00:07:06,690 --> 00:07:10,710
we have a parametric model a finite set of parameters is the man

41
00:07:10,730 --> 00:07:16,740
obviously after a good breakfast a like-minded was russia

42
00:07:16,840 --> 00:07:23,490
the complexity of the model is bounded

43
00:07:23,520 --> 00:07:26,490
even if the amount of data is unbounded

44
00:07:26,510 --> 00:07:30,650
so there is if you like information theory and of course i do as well

45
00:07:32,270 --> 00:07:35,400
the parameters are information channel between

46
00:07:35,640 --> 00:07:41,350
your training data in your future predictions on that channel has bottleneck as like traffic

47
00:07:41,350 --> 00:07:45,310
jams through that channel because it is of finite which

48
00:07:47,850 --> 00:07:52,150
non parametric models assume that the data distribution can be defined in terms of such

49
00:07:52,150 --> 00:07:54,410
a finite set of parameters

50
00:07:54,430 --> 00:08:00,010
but they can also be defined by summing some dimensional parameter theta

51
00:08:00,030 --> 00:08:02,760
so you add more lanes to your

52
00:08:02,800 --> 00:08:08,090
information highway so that you don't get traffic jams

53
00:08:09,340 --> 00:08:13,770
and now we can think of our parameter data as a function

54
00:08:13,780 --> 00:08:17,020
the nice thing about that is the amount of information that they can capture about

55
00:08:17,020 --> 00:08:18,670
the data the

56
00:08:18,680 --> 00:08:21,310
it can grow as the amount of data grows

57
00:08:21,310 --> 00:08:22,320
so again

58
00:08:22,340 --> 00:08:24,700
data has been this

59
00:08:24,710 --> 00:08:26,980
efforts to some five precision

60
00:08:27,040 --> 00:08:32,330
and you want capture some bits to make predictions and this makes them more flexible

61
00:08:32,370 --> 00:08:36,440
so that part of the point of doing nonparametric modeling

62
00:08:38,560 --> 00:08:44,570
now why nonparametrics i think actually that transcends the bayesian thing

63
00:08:45,390 --> 00:08:48,550
actually if you look around in machine learning

64
00:08:48,590 --> 00:08:50,960
a lot of successful methods

65
00:08:50,980 --> 00:08:54,100
are essentially nonparametric

66
00:08:54,110 --> 00:08:58,690
i like the whole kernel revolution with ESPN's and things like that and the related

67
00:08:58,690 --> 00:09:03,590
gas processes these are nonparametric models in our view that's one of the reasons why

68
00:09:03,680 --> 00:09:06,360
they were so successful

69
00:09:06,390 --> 00:09:11,240
of course kernel method is replaced the neural networks is a big paradigm machine learning

70
00:09:11,260 --> 00:09:15,460
and people came back afterwards and said i'll make bigger neural networks

71
00:09:15,720 --> 00:09:19,440
or bigger and deeper neural networks

72
00:09:19,450 --> 00:09:24,700
and these are not nonparametric into formal mathematical sense

73
00:09:24,760 --> 00:09:28,870
but there has been a lot of parametric OK

74
00:09:29,350 --> 00:09:35,560
you know one of geoff hinton or young raccoons the networks

75
00:09:35,570 --> 00:09:41,890
well million parameters which is like more than my nonparametric model ever expresses a sense

76
00:09:41,890 --> 00:09:43,760
that i have on my computer so

77
00:09:46,420 --> 00:09:49,370
no no no i say move

78
00:09:49,670 --> 00:09:51,820
the was sixty minutes

79
00:09:51,830 --> 00:09:53,370
we show you what it does

80
00:09:53,420 --> 00:09:56,570
nobody does you know we're six units

81
00:09:56,590 --> 00:10:02,690
so these are also successful because they are very flexible

82
00:10:02,690 --> 00:10:08,200
even methods like to pooh-pooh like k nearest neighbors are pretty damn good

83
00:10:08,210 --> 00:10:10,200
and they are pretty flexible and there

84
00:10:10,210 --> 00:10:13,360
so nonparametric but there are very nonparametric

85
00:10:13,390 --> 00:10:14,040
so what

86
00:10:14,050 --> 00:10:15,210
no here

87
00:10:15,220 --> 00:10:19,540
which is exactly the machinery are essentially nonparametric

88
00:10:19,550 --> 00:10:21,990
and then if you can read down there

89
00:10:22,120 --> 00:10:26,870
or highly scalable OK so there are also you know that you know i don't

90
00:10:26,870 --> 00:10:30,690
know how to measure success in machine learning i just look at you know

91
00:10:30,700 --> 00:10:36,620
two things that people are excited about and are excited about these very fancy

92
00:10:36,640 --> 00:10:43,050
possible evidence or they excited that they can run logistic regression on a hundred billion

93
00:10:43,050 --> 00:10:44,490
data points in

94
00:10:44,540 --> 00:10:51,600
twenty thousand million dimensions and because you know some methods you can make a very

95
00:10:51,600 --> 00:10:58,010
very scalable and you're google or microsoft or yahoo you care about the law

96
00:10:58,020 --> 00:11:01,100
those are successful as well i don't want to put that in fact if i

97
00:11:01,100 --> 00:11:05,100
was working in industry government i would probably be

98
00:11:05,110 --> 00:11:05,810
you know

99
00:11:05,820 --> 00:11:11,060
less interested in the school mathematical flexible things and wanting to figure out how to

100
00:11:11,070 --> 00:11:15,920
do it is the progression of cloud of ten thousand computers

101
00:11:17,950 --> 00:11:21,830
nonparametrics is successful thing isn't restricted to bayesian

102
00:11:22,510 --> 00:11:29,970
but they are already argued bayesian methods are very nice it's the one thing the

103
00:11:29,980 --> 00:11:32,560
two together

104
00:11:32,570 --> 00:11:35,900
so another way to approach this

105
00:11:35,930 --> 00:11:39,600
is to start out with like a wish list

106
00:11:39,620 --> 00:11:44,220
OK i want do machine learning or or statistics

107
00:11:44,230 --> 00:11:48,010
what are the desirable properties of my methods

108
00:11:48,030 --> 00:11:50,200
when we write down analysts this is

109
00:11:50,260 --> 00:11:55,350
my list from late last night so i apologize if i do i any of

110
00:11:55,350 --> 00:11:59,440
your favorable favorite desirable properties

111
00:11:59,530 --> 00:12:00,990
so here you go

112
00:12:01,460 --> 00:12:03,270
good predictive performance

113
00:12:06,360 --> 00:12:09,930
sort of goes hand in hand with good predictive performance

114
00:12:09,950 --> 00:12:12,910
robust to overfitting depends you know the

115
00:12:12,920 --> 00:12:17,920
all this industry of figuring out regularizers and things like that

116
00:12:17,940 --> 00:12:21,960
model based i think is this sort of

117
00:12:22,120 --> 00:12:26,790
you know if you're doing it there are some advantages to this is not absolutely

118
00:12:26,790 --> 00:12:31,990
necessary good predictive performance in your flexible and robust

119
00:12:32,000 --> 00:12:36,060
it doesn't have to be that whatever you're doing corresponds to particular model

120
00:12:36,080 --> 00:12:38,890
but you know if you have a model based

121
00:12:39,000 --> 00:12:43,590
approach you can make it you can make it modular and filled with other things

122
00:12:43,590 --> 00:12:45,950
what i'm about to show you now

123
00:12:45,970 --> 00:12:50,010
is that have come back by the way and revisit this again when i do

124
00:12:50,010 --> 00:12:55,660
which corresponds sampling algorithm and eligibility traces what they should be thinking about to anticipate

125
00:12:56,050 --> 00:13:00,300
legibility traces that marcato is very good

126
00:13:00,320 --> 00:13:05,030
in that is independent state space are also comes with a cost that cost and

127
00:13:05,070 --> 00:13:10,630
a high variance cost so just to anticipate eligibility traces

128
00:13:10,700 --> 00:13:12,510
is the answer

129
00:13:13,030 --> 00:13:17,070
in reinforcement learning to the question of how do we get the advantage of monte

130
00:13:18,410 --> 00:13:21,160
that is independent of ci the state space

131
00:13:21,180 --> 00:13:25,860
and reduce the variance of the same time i'll come to really traces and talk

132
00:13:25,860 --> 00:13:26,700
about that

133
00:13:26,720 --> 00:13:28,800
OK i want to show you just

134
00:13:28,840 --> 00:13:31,890
popular ontologies and applications just two

135
00:13:31,910 --> 00:13:37,030
break the monotony of just talking here is a a video

136
00:13:37,050 --> 00:13:38,990
from peter stone slabs

137
00:13:39,050 --> 00:13:43,470
are using direct reinforcement learning to learn to improve

138
00:13:43,530 --> 00:13:48,470
the gate of walking robots so he has the backing just visit in three weeks

139
00:13:48,470 --> 00:13:51,390
ago and it's really funny to see this is got this

140
00:13:51,450 --> 00:13:52,930
pretty big sized

141
00:13:52,990 --> 00:13:58,840
soccer field because they play soccer these robots what he has is he has you

142
00:13:58,840 --> 00:14:00,880
see because of the OECD's little

143
00:14:00,910 --> 00:14:06,180
poles with these bars of pink and white and so on and what is what

144
00:14:06,180 --> 00:14:09,410
you want to doing is are continuously walking back and forth

145
00:14:09,410 --> 00:14:10,380
back and forth

146
00:14:10,390 --> 00:14:15,070
and they know where they are because they see these bars at the end and

147
00:14:15,070 --> 00:14:18,680
they can make their stopping point where the bar becomes big enough

148
00:14:18,880 --> 00:14:22,550
in the visual field now what they're trying to do is improve off at the

149
00:14:23,220 --> 00:14:26,550
so they're walking gait parameters

150
00:14:26,970 --> 00:14:31,340
you know they have like i think seven rate parameters it to determine how they

151
00:14:32,160 --> 00:14:35,110
and the idea is to make the learned to make

152
00:14:35,150 --> 00:14:40,470
learn to make it walk faster because walking faster would he be an advantage while

153
00:14:40,470 --> 00:14:43,760
playing soccer because you get to the ball

154
00:14:43,780 --> 00:14:46,130
right so just keep walking back and forth

155
00:14:46,180 --> 00:14:47,320
and a learning

156
00:14:47,340 --> 00:14:50,260
using a direct reinforcement learning methods for

157
00:14:50,260 --> 00:14:56,570
tuning these for updating parameters the action choices on the parameter choices the sensors are

158
00:14:59,050 --> 00:15:03,880
for proper reception they know in the past around you know

159
00:15:03,890 --> 00:15:07,430
how far they are from the from the

160
00:15:07,470 --> 00:15:12,510
the the target points on that things like this so i want to show you

161
00:15:12,510 --> 00:15:15,450
so before training this is how to walk before training

162
00:15:15,490 --> 00:15:16,890
looks pretty clumsy to me

163
00:15:16,910 --> 00:15:23,280
but if the walker knees because when the play soccer ball

164
00:15:23,320 --> 00:15:27,200
and the ball needs to be captured between the legs of the working these because

165
00:15:27,200 --> 00:15:32,320
the ball gets captured between the legs and this is after training

166
00:15:32,320 --> 00:15:37,150
you see the work much better much much smaller

167
00:15:37,200 --> 00:15:39,470
so it's an application

168
00:15:39,970 --> 00:15:47,070
it's really faces was walking the walk for days on end to try and improve

169
00:15:50,840 --> 00:15:59,130
here's another one on where somebody using indirect methods and so the helicopter control problem

170
00:15:59,180 --> 00:16:01,530
so so

171
00:16:01,590 --> 00:16:07,300
started can really tell it has done so it's helicopter control problem and a few

172
00:16:07,460 --> 00:16:12,380
hundred causing helicopter but it's about twice the length of the stable that big

173
00:16:12,410 --> 00:16:13,820
that's from

174
00:16:13,840 --> 00:16:15,360
the nose to the end tail

175
00:16:15,450 --> 00:16:18,840
and its body itself is about as big as the stable the the front part

176
00:16:19,110 --> 00:16:21,610
and so let me just give you a little

177
00:16:21,610 --> 00:16:27,720
how house was housework was done we simply moving on

178
00:16:27,840 --> 00:16:34,010
school again

179
00:16:34,030 --> 00:16:35,180
the video

180
00:16:35,260 --> 00:16:37,340
let's see this policy or this one

181
00:16:37,970 --> 00:16:41,780
so this works started because they want to believe helicopter they could take part in

182
00:16:41,780 --> 00:16:47,700
these competitions in this competition where people play with helicopters and the required maneuvers to

183
00:16:47,700 --> 00:16:50,260
them and what i think you're seeing emmanuel where

184
00:16:50,280 --> 00:16:55,530
its nose it's its nose to create a serql so it has to move in

185
00:16:55,530 --> 00:16:56,820
this circle

186
00:16:58,180 --> 00:17:01,630
it's very hard for people who very very few people can do it

187
00:17:01,660 --> 00:17:05,970
big lots lots of time

188
00:17:06,010 --> 00:17:06,780
with these

189
00:17:06,780 --> 00:17:10,220
with these helicopters two black you get these people to get people to do it

190
00:17:10,570 --> 00:17:13,930
and how to do it is that if they had a

191
00:17:13,970 --> 00:17:15,590
the bilinear let model

192
00:17:15,610 --> 00:17:18,430
of state action the next day

193
00:17:18,450 --> 00:17:23,860
so they used about half an hour training with whom you pilot came in and

194
00:17:23,860 --> 00:17:25,590
flew his helicopter

195
00:17:25,610 --> 00:17:28,840
and they collected at state action next mapping

196
00:17:28,890 --> 00:17:30,490
they train neural nets

197
00:17:30,490 --> 00:17:31,660
with that mapping

198
00:17:31,660 --> 00:17:33,090
not a model

199
00:17:33,110 --> 00:17:34,550
and they use that model

200
00:17:34,570 --> 00:17:36,450
two do interact training

201
00:17:36,450 --> 00:17:38,340
so for the optimal control policy

202
00:17:38,380 --> 00:17:40,180
four that helicopter

203
00:17:40,240 --> 00:17:44,860
these are specific interact algorithm that i'm not talked about but you can imagine using

204
00:17:44,860 --> 00:17:51,550
any sort of indirect methods that learn eleven model and all the good videos he

205
00:17:51,550 --> 00:17:56,660
got and doings website policy lots of videos very recently they actually made the helicopter

206
00:17:56,660 --> 00:17:59,010
flying upside down

207
00:17:59,070 --> 00:18:03,130
which is very very hard parity there are only a handful of people in the

208
00:18:03,130 --> 00:18:06,090
country can actually make these very hard these

209
00:18:06,180 --> 00:18:09,360
big size helicopters actually fly upside down

210
00:18:09,390 --> 00:18:12,360
it's very hard to get but they apparently

211
00:18:12,360 --> 00:18:15,800
almost effortlessly manages to make

212
00:18:16,490 --> 00:18:20,570
in direct reinforcement learning work on helicopter is quite amazing

213
00:18:20,630 --> 00:18:22,430
quite amazing to master

214
00:18:24,740 --> 00:18:27,300
you sort of more or less no

215
00:18:27,300 --> 00:18:30,090
having this what i just talked about

216
00:18:30,090 --> 00:18:34,890
how to do this court except you lack the magic touch than doing seems to

217
00:18:34,890 --> 00:18:41,990
have one making very large things work on these matters that so

218
00:18:42,030 --> 00:18:46,860
this to illustrate two different applications of direct and indirect reinforced

219
00:18:46,880 --> 00:18:51,780
question for acme question if you can you're asking more about this application i can

220
00:18:51,780 --> 00:18:53,780
tell you about that

221
00:18:53,820 --> 00:18:55,740
or i can one

222
00:18:55,760 --> 00:19:00,930
you know land right so it

223
00:19:02,530 --> 00:19:07,110
they learn

224
00:19:07,160 --> 00:19:09,150
in those they had been

225
00:19:09,190 --> 00:19:12,300
i the higher because really

226
00:19:12,320 --> 00:19:13,950
you have to be

227
00:19:14,030 --> 00:19:16,510
do this so

228
00:19:17,010 --> 00:19:19,340
it was the system must be able

229
00:19:19,340 --> 00:19:21,530
raise your right

230
00:19:21,530 --> 00:19:25,660
so i can very that you see a helicopter flying

231
00:19:25,720 --> 00:19:27,630
well only tell

232
00:19:27,650 --> 00:19:31,630
it's like this one looks like it might be

233
00:19:31,660 --> 00:19:36,260
normal normal way and then the camera pans back they see three

234
00:19:36,320 --> 00:19:38,260
according the top line

235
00:19:38,380 --> 00:19:39,490
well done

236
00:19:44,660 --> 00:19:47,550
i would like to thank you because you

237
00:19:49,490 --> 00:19:54,800
very briefly one algorithm for sparse sampling out from you before i do that i

238
00:19:54,800 --> 00:19:56,450
mean let me ask you this question

239
00:19:58,360 --> 00:20:00,700
i told you about the algorithm

240
00:20:00,720 --> 00:20:02,160
monte carlo

241
00:20:02,160 --> 00:20:05,760
for solving the policy evaluation from which says if you want to find the value

242
00:20:05,760 --> 00:20:08,820
real valued parameters so it is in one parameter models

243
00:20:08,840 --> 00:20:11,720
but if you look at it you see that for

244
00:20:13,090 --> 00:20:16,990
distribution in the second order in the first order models

245
00:20:17,010 --> 00:20:19,510
the two parameters there is one

246
00:20:19,510 --> 00:20:22,910
and element of the one parameter model which is close to it at least in

247
00:20:22,910 --> 00:20:24,550
euclidean distance

248
00:20:24,930 --> 00:20:28,800
so you would expect that this model

249
00:20:28,820 --> 00:20:31,090
is a lot more like the

250
00:20:31,180 --> 00:20:33,320
one of the two parameters

251
00:20:33,340 --> 00:20:34,410
this model

252
00:20:34,430 --> 00:20:36,320
so both of one parameter

253
00:20:36,340 --> 00:20:39,750
but this one resembles the two parameters a lot more and can fit random data

254
00:20:39,750 --> 00:20:41,110
a lot better

255
00:20:41,130 --> 00:20:46,450
so indeed this normalized maximum likelihood term will be much larger for this model

256
00:20:46,470 --> 00:20:47,930
and that will

257
00:20:47,950 --> 00:20:50,720
translated of this term being larger

258
00:20:50,760 --> 00:20:59,930
so now let's compare this to bayesian model selection

259
00:20:59,950 --> 00:21:03,180
so base so-called bayes factor model selection

260
00:21:03,200 --> 00:21:06,360
if you ignore the priors the models which usually can

261
00:21:06,390 --> 00:21:08,510
you simply

262
00:21:08,530 --> 00:21:10,640
because the model which maximizes

263
00:21:10,660 --> 00:21:13,640
the marginal likelihood of the data so you have

264
00:21:13,660 --> 00:21:17,340
this is this effort you've seen before the integral now over the probability of the

265
00:21:17,340 --> 00:21:18,950
data according to theta

266
00:21:18,970 --> 00:21:21,860
integrated by the prior density of theta

267
00:21:21,910 --> 00:21:24,800
you can look at that for each model and then you pick

268
00:21:24,800 --> 00:21:27,200
the model for which this

269
00:21:27,220 --> 00:21:33,990
integrated probabilities larger so minus log integrate probability smallest that's what bayesian model selection bias

270
00:21:34,010 --> 00:21:40,720
so you can also approximate this by the plus approximation so it's basically taylor expansion

271
00:21:40,740 --> 00:21:43,740
and if you do that it turns out that again on the conditions of the

272
00:21:43,740 --> 00:21:46,340
model this turns out to be equal to this

273
00:21:46,360 --> 00:21:50,390
so you see it's almost the same as this base there's something very similar for

274
00:21:50,410 --> 00:21:51,890
minimum description tools

275
00:21:52,130 --> 00:21:56,410
normalized makes like the minimum description length

276
00:21:56,410 --> 00:21:59,450
in particular you see that these terms

277
00:21:59,470 --> 00:22:02,570
they do not depend on the sample size

278
00:22:02,590 --> 00:22:04,200
so for large

279
00:22:04,220 --> 00:22:06,030
base will do the same

280
00:22:06,050 --> 00:22:07,430
as you

281
00:22:07,450 --> 00:22:09,860
if the number of miles you compare

282
00:22:09,910 --> 00:22:13,280
is a finite

283
00:22:13,280 --> 00:22:16,800
so how does this term relate to this term that well

284
00:22:16,800 --> 00:22:20,200
you see that in base what happens depends on your prior

285
00:22:20,200 --> 00:22:23,320
of course basis depends on the prior

286
00:22:24,450 --> 00:22:28,240
basically what you see here this is the prior

287
00:22:28,260 --> 00:22:31,550
of to evaluated at the maximum likelihood

288
00:22:32,430 --> 00:22:34,360
basically if you're lucky

289
00:22:34,360 --> 00:22:38,020
the maximum likelihood turns out to be the region of parameter space to which gave

290
00:22:38,020 --> 00:22:41,740
a high prior probabilities so your prior assumptions were kind of

291
00:22:42,840 --> 00:22:47,300
then this term will be small because the prior density of the maximum likelihood parameter

292
00:22:47,300 --> 00:22:48,300
will be large

293
00:22:48,320 --> 00:22:51,860
you will give large probability to the data if however you were unlucky and you

294
00:22:51,860 --> 00:22:56,260
get small prior to what turns out to be the probability maximizing theta

295
00:22:56,300 --> 00:22:57,280
then this will be

296
00:22:57,280 --> 00:23:02,300
the probability will be smart smaller mines log probability will be larger so this depends

297
00:23:02,300 --> 00:23:04,990
on your priors

298
00:23:06,240 --> 00:23:11,220
course some bayesian so-called objective bayesians have proposed priors

299
00:23:11,240 --> 00:23:12,360
but you can use

300
00:23:12,390 --> 00:23:16,140
if you don't have any clear prior knowledge

301
00:23:16,160 --> 00:23:19,340
and the most famous of these is the so-called jeffreys prior

302
00:23:19,340 --> 00:23:22,010
which is defined

303
00:23:22,070 --> 00:23:25,860
by the square root of the determinant of the fisher information matrix

304
00:23:25,860 --> 00:23:29,890
usually if you that if you integrate over all theta doesn't become one but

305
00:23:29,910 --> 00:23:33,950
something larger so you have to normalize to make the probability

306
00:23:33,950 --> 00:23:37,970
so this is what sometimes objective bayesians

307
00:23:37,990 --> 00:23:42,340
that's OK to use the model selection and if you plot this in here

308
00:23:42,390 --> 00:23:44,970
you see that you get minus log

309
00:23:44,990 --> 00:23:46,470
of the square root

310
00:23:46,490 --> 00:23:49,890
of the fisher information and they here plus lack of this grid of the fisher

311
00:23:49,890 --> 00:23:55,680
information only council and what remains is plus log of the denominators yet plus lack

312
00:23:55,680 --> 00:23:59,260
of this so then you see that they actually become the same

313
00:23:59,260 --> 00:24:01,200
up to some order of one

314
00:24:01,200 --> 00:24:04,030
so if use jeffreys prior

315
00:24:04,050 --> 00:24:12,470
then asymptotically base and MDL based on the normalized maximum likelihood become actually the same

316
00:24:12,490 --> 00:24:17,550
so interesting is jeffreys prior was proposed in nineteen thirty nine as a kind of

317
00:24:17,550 --> 00:24:21,550
uniform priors on on the space of parameters but on the space of distributions so

318
00:24:21,590 --> 00:24:26,240
the space of parameters depends on your parametrisation is essentially arbitrary

319
00:24:27,200 --> 00:24:31,640
the space of distributions can be be defining the more generic matter

320
00:24:31,660 --> 00:24:35,470
manner and if you put a uniform prior on that a user base

321
00:24:35,490 --> 00:24:36,820
you get

322
00:24:36,840 --> 00:24:40,640
essentially the same as and you at least in the simple setting we're talking about

323
00:24:40,640 --> 00:24:43,240
here in more general settings

324
00:24:43,260 --> 00:24:45,510
it turns out you don't always get the same

325
00:24:45,550 --> 00:24:53,470
because for example if you have nonparametric if used in nonparametric settings then with some

326
00:24:53,470 --> 00:24:55,110
priors you will actually

327
00:24:55,140 --> 00:24:59,530
not compressor data so there are some instances of bayes which can be thought of

328
00:24:59,530 --> 00:25:01,090
as MDL

329
00:25:01,200 --> 00:25:05,610
because they don't lead to compression of the data

330
00:25:08,530 --> 00:25:13,280
given the time i'll skip these less well just talk about these two

331
00:25:14,010 --> 00:25:18,320
there's a certain very important interpretation of what we've been doing which is the so-called

332
00:25:18,320 --> 00:25:22,720
predictive or prequential interpretation

333
00:25:24,930 --> 00:25:26,910
if you codes you can only

334
00:25:26,930 --> 00:25:30,110
you can also look at what happens outcome by outcome

335
00:25:30,140 --> 00:25:34,160
and then you can think of the minus log probability assigned to an outcome as

336
00:25:34,160 --> 00:25:35,840
a loss function

337
00:25:35,860 --> 00:25:39,590
so you can just think of probability says prediction strategies

338
00:25:39,610 --> 00:25:40,840
and and then

339
00:25:40,860 --> 00:25:43,840
one reasonable way to measure how good they are

340
00:25:43,860 --> 00:25:47,160
on the particular outcome but looking at the minus log of the probability assigned to

341
00:25:47,160 --> 00:25:48,050
the outcome

342
00:25:48,070 --> 00:25:52,050
if you assign probability one to the outcome minus log one zero so you loss

343
00:25:52,070 --> 00:25:54,490
is zero if you assign probability zero

344
00:25:54,510 --> 00:25:58,410
you're also be infinite but you can also think of this is

345
00:25:58,430 --> 00:26:00,050
if you use the distribution

346
00:26:00,050 --> 00:26:04,320
as the cold then it's the number of bits you need to encode that particular

347
00:26:05,510 --> 00:26:08,740
so if your goal is compressed the data as much as possible this is logical

348
00:26:08,740 --> 00:26:11,550
loss function

349
00:26:12,640 --> 00:26:14,590
it turns out that it's

350
00:26:14,590 --> 00:26:16,470
if you use

351
00:26:17,860 --> 00:26:21,840
to sequentially predict data

352
00:26:24,930 --> 00:26:26,950
it turns out

353
00:26:26,970 --> 00:26:30,930
then you get you can look at the community of los so that's the some

354
00:26:30,930 --> 00:26:33,180
of these individual or classes

355
00:26:33,200 --> 00:26:36,220
and it turns out that is the same

356
00:26:37,070 --> 00:26:40,860
the code length you need if you could base the whole data so what does

357
00:26:40,860 --> 00:26:41,890
it mean

358
00:26:41,930 --> 00:26:46,220
well this is what we've seen before you get the basic universal code

359
00:26:46,220 --> 00:26:49,700
so is the basic mixture distribution you can turn it into a cold and this

360
00:26:49,700 --> 00:26:52,280
is the code length get for each particular outcome

361
00:26:52,300 --> 00:26:57,070
now you can also use basic when surely to predict outcomes so you have to

362
00:26:57,070 --> 00:27:01,610
predict the first outcome using the bayes predictive distribution for that then you observe the

363
00:27:01,610 --> 00:27:05,760
first outcome you predict the second outcome using the conditional distribution

364
00:27:05,780 --> 00:27:07,320
given the first outcome

365
00:27:07,340 --> 00:27:10,510
and you have a certain level class then you predict the third outcome

366
00:27:10,530 --> 00:27:14,800
using the conditional distribution based on the first two outcomes to predict the fourth outcome

367
00:27:14,800 --> 00:27:18,970
using the conditional distribution based on the first three outcomes et cetera you at all

368
00:27:18,970 --> 00:27:20,630
the losses

369
00:27:20,630 --> 00:27:23,890
so this is what you do you at some of these losses

370
00:27:23,910 --> 00:27:25,780
we always predict using

371
00:27:25,800 --> 00:27:29,570
based condition on the previous outcomes

372
00:27:29,590 --> 00:27:33,820
of course by the definition of the word implies this is equal to this

373
00:27:33,840 --> 00:27:38,030
now because the sum of the logarithms the log of the product

374
00:27:38,050 --> 00:27:39,200
and this

375
00:27:39,200 --> 00:27:41,080
factors in this distribution

376
00:27:41,620 --> 00:27:43,560
you're performing some asians

377
00:27:43,580 --> 00:27:48,510
and that sort of shrinking things down it's making new factors and shrinking them and

378
00:27:48,510 --> 00:27:52,910
there's a very nice graph theoretic correspondence that you're watching this residual graph sort of

379
00:27:53,790 --> 00:27:57,620
and shrink down for instance to the target point here i was interested in x

380
00:27:57,620 --> 00:28:03,890
one so the nice correspondence between the factorisation in the summation and the graph theoretic

381
00:28:03,890 --> 00:28:05,850
properties here

382
00:28:05,870 --> 00:28:11,540
and so what we're going to see is that some kinds of graphs are actually

383
00:28:11,540 --> 00:28:16,560
nice graphs in that they have good elimination orderings and some graphs are not nice

384
00:28:16,560 --> 00:28:21,870
graphs they don't have good elimination orderings and so essentially the elimination algorithm it's useful

385
00:28:21,870 --> 00:28:27,490
because it gives you an algorithmic way to understand why this graph structure affect how

386
00:28:27,490 --> 00:28:31,890
complex it is to for instance some and perform marginalization

387
00:28:31,930 --> 00:28:35,970
it sort of tells you that the way in which the graph is connected controls

388
00:28:35,970 --> 00:28:43,410
how the elimination behaves and that determines in the end how much computational complexity pay

389
00:28:43,430 --> 00:28:47,450
so any questions on that

390
00:28:47,450 --> 00:28:50,680
before i move on i want to move on with that kind of intuition i'd

391
00:28:50,680 --> 00:28:54,540
like to move on to talking about trees which is one of the simplest case

392
00:28:54,540 --> 00:28:57,720
of graphs that have very nice elimination ordering

393
00:28:58,450 --> 00:29:00,970
well i don't know

394
00:29:04,250 --> 00:29:09,020
right so

395
00:29:09,040 --> 00:29:12,660
here i was giving you one example and i had some two cliques on a

396
00:29:12,660 --> 00:29:14,080
three clique

397
00:29:14,720 --> 00:29:21,180
so in general you have to start with whatever model you're given whatever used are

398
00:29:22,270 --> 00:29:27,680
right if you give me a graph that to start with had let's say

399
00:29:27,720 --> 00:29:33,160
clique sizes of alpha and or something it wouldn't be actually very useful model because

400
00:29:33,180 --> 00:29:36,600
you know to store things you'd have to pay complexity to the alpha and for

401
00:29:36,600 --> 00:29:38,330
binary variables

402
00:29:41,510 --> 00:29:44,100
yes so that that's the key point that

403
00:29:44,120 --> 00:29:47,080
elimination can only make things worse for you

404
00:29:47,080 --> 00:29:50,270
and the issue is how much worse doesn't make it for you

405
00:29:50,290 --> 00:29:54,870
this example actually i mean it made new three clique it that there's edge

406
00:29:54,930 --> 00:29:58,080
but i already had three clique so it didn't really make things any worse it

407
00:29:58,080 --> 00:29:59,720
all for me things were sort of

408
00:29:59,850 --> 00:30:02,890
there was no additional sort of jump complexity

409
00:30:04,310 --> 00:30:06,080
what will see is the trees

410
00:30:06,200 --> 00:30:09,990
the simplest case of no jumping complexity but other graphs there is a huge jump

411
00:30:09,990 --> 00:30:20,870
in complexity between this graph and in the residual graph what you get after eliminating

412
00:30:20,990 --> 00:30:24,890
so any other questions about elimination

413
00:30:24,950 --> 00:30:29,450
again i encourage you to display with this example it's it's a simple example but

414
00:30:29,450 --> 00:30:33,850
it actually has all the basic ingredients that you need just to get intuition of

415
00:30:33,910 --> 00:30:38,350
how this works this is not what you implement but it's useful to to play

416
00:30:38,350 --> 00:30:41,930
with because it sort of gives you a feeling for what's going on why why

417
00:30:41,950 --> 00:30:45,330
graph structure actually matters here

418
00:30:45,350 --> 00:30:51,770
OK so

419
00:30:51,790 --> 00:30:54,890
let's move on to the the sum product algorithm

420
00:30:55,450 --> 00:30:59,120
let me first talk about it on trees and then i'm going to talk about

421
00:30:59,240 --> 00:31:03,390
on a more general class of graphs that are called junction trees

422
00:31:03,410 --> 00:31:09,810
and so the first part is talking about ordinary trees recapping what sam did yesterday

423
00:31:09,810 --> 00:31:15,290
but again i'm follow the elimination perspective so it's it's from a somewhat different point

424
00:31:15,290 --> 00:31:16,060
of view

425
00:31:16,140 --> 00:31:19,310
so again remember that tree

426
00:31:19,310 --> 00:31:23,390
is this simply an undirected graph here that has no cycles

427
00:31:24,790 --> 00:31:26,040
you can always

428
00:31:26,060 --> 00:31:29,410
i choose a particular node in the tree to the root

429
00:31:29,430 --> 00:31:33,850
we think about this being the root and then you sort of move

430
00:31:33,870 --> 00:31:36,010
words and you hit the nodes

431
00:31:36,020 --> 00:31:40,740
on the edge and these are called leaves for the obvious reasons

432
00:31:40,940 --> 00:31:46,560
so you want to think about this is again put appointed SMA yesterday's elimination is

433
00:31:47,890 --> 00:31:51,950
but i remember elimination i sort of presupposes that you know i was sitting at

434
00:31:51,950 --> 00:31:54,240
node one and i cared about note one

435
00:31:54,290 --> 00:31:56,810
and that's all i care about in this case

436
00:31:56,850 --> 00:32:00,990
but it could be the case that i care about note one and note seven

437
00:32:01,020 --> 00:32:04,930
or back about all nodes simultaneously that's quite possible

438
00:32:04,970 --> 00:32:08,040
if i give you a noisy image from the telescope

439
00:32:08,180 --> 00:32:12,660
you might care about for instance removing the noise is called de noising you don't

440
00:32:12,660 --> 00:32:15,810
care about just one node you care about all the pixels who'd like to actually

441
00:32:15,810 --> 00:32:20,010
do things simultaneously over all the nodes at once

442
00:32:20,020 --> 00:32:25,330
so if you think about this but if you think about running elimination you see

443
00:32:25,330 --> 00:32:27,580
right away that being very stupid

444
00:32:27,580 --> 00:32:34,580
it's sort of violating the basic principle of of algorithms you're not recycling computations you

445
00:32:34,580 --> 00:32:38,140
doing the same computations again and again and again you only need to do them

446
00:32:39,490 --> 00:32:45,430
so really the sum product algorithm is just a clever way to run elimination simultaneously

447
00:32:45,430 --> 00:32:50,620
for every node of the tree that's all it's doing it's it's cleverly running elimination

448
00:32:50,620 --> 00:32:52,040
for every node

449
00:32:52,080 --> 00:32:56,850
but it's doing in a way that you don't waste computationally only the computation the

450
00:32:56,850 --> 00:32:59,700
minimum number of times needed

451
00:32:59,700 --> 00:33:04,590
now as well by the way notice that if ten is zero

452
00:33:04,670 --> 00:33:07,500
that's going to take care of a constant term two

453
00:33:07,550 --> 00:33:12,420
in other words the reason is the constant term from because that corresponds to the

454
00:33:12,420 --> 00:33:18,130
cosine of zero t which is one

455
00:33:22,500 --> 00:33:26,370
suppose i input instead a and

456
00:33:26,380 --> 00:33:28,600
cosine entity

457
00:33:28,610 --> 00:33:33,060
all you do is multiply the answer by a

458
00:33:33,110 --> 00:33:38,360
the same year multiply the input by b and you multiply the response that's because

459
00:33:38,400 --> 00:33:41,230
equations of linear equations

460
00:33:43,050 --> 00:33:44,360
and now i am i gonna do

461
00:33:44,420 --> 00:33:47,030
i'm going to have the mop

462
00:33:47,090 --> 00:33:50,450
if i add them up for the different ends

463
00:33:50,490 --> 00:33:55,190
and take account also the n equals zero corresponding to the first

464
00:33:55,250 --> 00:33:56,910
constant term

465
00:33:56,980 --> 00:34:01,420
the sum of all these according to my fourier formula is going to be f

466
00:34:03,860 --> 00:34:05,790
what's the summer of

467
00:34:05,800 --> 00:34:07,510
this is the response

468
00:34:07,540 --> 00:34:09,800
the corresponding response as well

469
00:34:09,810 --> 00:34:13,990
it's going to be some asian a and

470
00:34:17,040 --> 00:34:20,360
plus b and y and

471
00:34:20,910 --> 00:34:23,550
the response to the side

472
00:34:23,550 --> 00:34:27,610
that'll be the sum from one to infinity and there will be a constant some

473
00:34:27,610 --> 00:34:34,300
sort of constant term here was just c one

474
00:34:34,300 --> 00:34:38,620
so in other words if this input produces that response

475
00:34:38,630 --> 00:34:43,800
and these are things which we can calculate will lead by this formula for is

476
00:34:45,450 --> 00:34:50,760
the response to things which otherwise we wouldn't have been able to calculate namely any

477
00:34:50,760 --> 00:34:53,520
periodic function of period two pi

478
00:34:53,560 --> 00:34:56,800
will have as is the procedure will be

479
00:34:56,840 --> 00:34:59,550
got periodic function with period two pi

480
00:34:59,580 --> 00:35:03,810
find its fourier series and i'll show you how to do that today

481
00:35:03,840 --> 00:35:06,080
find its fourier series

482
00:35:06,140 --> 00:35:10,580
and then the response to that for a general effort he will be

483
00:35:12,450 --> 00:35:15,250
infinite series of functions

484
00:35:15,290 --> 00:35:20,230
we're the why these things are things you already know how to calculate the responses

485
00:35:20,230 --> 00:35:23,680
to sines and cosines and you just form the some

486
00:35:23,730 --> 00:35:25,680
with those coefficients now

487
00:35:25,760 --> 00:35:30,800
why it is that were it works by the superposition principle

488
00:35:30,850 --> 00:35:32,510
so this is true

489
00:35:32,530 --> 00:35:37,100
the reason i can do the adding and multiplying by a constant i'm using the

490
00:35:37,100 --> 00:35:39,790
superposition principle

491
00:35:42,240 --> 00:35:49,940
this input produces that responds the sum of a bunch of inputs produces the sum

492
00:35:49,940 --> 00:35:55,800
of the corresponding responses and why is that why can i use the superposition principle

493
00:35:55,810 --> 00:35:58,040
because the old and the e

494
00:36:03,410 --> 00:36:04,650
it's OK

495
00:36:04,700 --> 00:36:09,390
since the old is linear

496
00:36:13,890 --> 00:36:19,340
that's what makes all this work

497
00:36:20,230 --> 00:36:25,270
so what we're going to do today is also how to calculate those fourier series

498
00:36:25,270 --> 00:36:30,180
i will not be able to use it to actually solve the differential differential equation

499
00:36:30,180 --> 00:36:31,850
that will take us

500
00:36:31,910 --> 00:36:34,140
pretty much all period two

501
00:36:34,150 --> 00:36:37,160
show how to calculate the fourier series

502
00:36:38,100 --> 00:36:41,900
OK so i solve differential equations on monday wrong

503
00:36:42,400 --> 00:36:44,650
probably won't even get to it then because

504
00:36:44,770 --> 00:36:51,030
the calculation of the fourier series is sufficient amount of work that you really want

505
00:36:51,030 --> 00:36:55,670
to know all the possible tricks and shortcuts there are a forcefully they're not

506
00:36:55,760 --> 00:37:01,140
very clever tricks they just obvious things but it'll take me a period to point

507
00:37:01,140 --> 00:37:02,920
out the obvious things

508
00:37:02,930 --> 00:37:05,300
obviously my sense it's not yours

509
00:37:05,310 --> 00:37:12,440
and finally the third day will be we will solve differential equations are actually carry

510
00:37:12,440 --> 00:37:15,480
out the program but the main thing we're going to get out of it is

511
00:37:15,480 --> 00:37:17,670
another approach to residents

512
00:37:17,680 --> 00:37:20,270
because the things that we're going to be interested in

513
00:37:20,280 --> 00:37:21,880
picking out

514
00:37:21,920 --> 00:37:26,080
which of these terms may possibly produce residents

515
00:37:26,180 --> 00:37:28,770
therefore very crazy

516
00:37:28,770 --> 00:37:31,320
your training data it will get

517
00:37:31,350 --> 00:37:32,860
a score of zero

518
00:37:32,880 --> 00:37:34,280
which means that

519
00:37:34,290 --> 00:37:38,480
the probability that class is going to be zero the probability the complement class will

520
00:37:38,480 --> 00:37:40,900
also be easier to just never seen that word

521
00:37:41,650 --> 00:37:43,150
a very common thing to do

522
00:37:43,150 --> 00:37:47,610
is to put a little bit of smoothing in here so we put on a

523
00:37:47,610 --> 00:37:52,120
waiting an extra value here on the top and bottom

524
00:37:52,230 --> 00:37:57,640
and typical examples and might be one he might be one here what's

525
00:37:57,650 --> 00:38:01,790
so this is the this is the

526
00:38:01,800 --> 00:38:04,890
one you already used for estimating the probability of the word

527
00:38:04,910 --> 00:38:07,270
here in the class

528
00:38:08,350 --> 00:38:11,240
this is called the multinomial distribution

529
00:38:11,250 --> 00:38:14,150
so it's a fancy term for very simple idea

530
00:38:14,160 --> 00:38:18,780
and this particular use of m and p is called the dirichlet prior

531
00:38:18,790 --> 00:38:22,290
again a fancy name for very simple things

532
00:38:23,860 --> 00:38:29,270
if we use these counts of plus one plus point five which means basically the

533
00:38:29,270 --> 00:38:32,940
probabilities go to you know one half never seen

534
00:38:32,970 --> 00:38:34,900
r work before

535
00:38:34,920 --> 00:38:40,410
OK so that basically means that the school for particular class given list of words

536
00:38:40,470 --> 00:38:41,740
is going to be

537
00:38:43,680 --> 00:38:46,760
essentially if one work in log space which is

538
00:38:47,510 --> 00:38:50,640
probabilistic models like this almost always more convenient

539
00:38:50,650 --> 00:38:54,430
because the actual values could very close to zero very quickly

540
00:38:54,480 --> 00:39:00,220
we can and we end up with this by scoring everything by this particular formula

541
00:39:00,240 --> 00:39:03,390
right which you could see is very simple very easy to implement and the only

542
00:39:03,390 --> 00:39:06,470
things you really need to keep track of this the number of times each word

543
00:39:06,470 --> 00:39:09,910
appears in the number of words that are in category

544
00:39:09,930 --> 00:39:11,990
so this is a very very simple algorithm

545
00:39:12,030 --> 00:39:14,760
and it turns out to work quite well

546
00:39:14,850 --> 00:39:17,360
we've got a little

547
00:39:17,380 --> 00:39:19,480
dirichlet prior here OK

548
00:39:19,550 --> 00:39:22,410
now we only need to the nice thing about this is what i was going

549
00:39:22,410 --> 00:39:26,300
to particular document but looking at the score for class

550
00:39:26,310 --> 00:39:27,900
we or non we

551
00:39:27,910 --> 00:39:30,720
i only need to look at the accounts of the words that actually appear in

552
00:39:30,720 --> 00:39:31,770
the document

553
00:39:31,810 --> 00:39:33,180
all right so

554
00:39:33,190 --> 00:39:37,860
i'm accumulating counts for many many different words are right i'm looking at a large

555
00:39:37,860 --> 00:39:44,010
collection say twenty thousand news articles all have counts for thousands of words but each

556
00:39:44,020 --> 00:39:45,370
these counts are going to be

557
00:39:45,390 --> 00:39:46,810
stored in various

558
00:39:46,820 --> 00:39:50,600
pathway right so we only need to worry about counts for the words that actually

559
00:39:50,600 --> 00:39:53,360
occur in the classes they actually occurring

560
00:39:53,400 --> 00:39:57,020
the total number of words for each class so i can actually do this quite

561
00:39:57,020 --> 00:39:58,510
efficiently even if there

562
00:39:58,520 --> 00:40:02,740
a lot of classes there are lot of words

563
00:40:02,750 --> 00:40:05,940
so this is a pretty common technique

564
00:40:05,950 --> 00:40:08,590
it's quite likely that one of you

565
00:40:09,230 --> 00:40:12,640
it's quite likely to each of you have actually use one of these things if

566
00:40:12,640 --> 00:40:13,850
ever used any

567
00:40:13,860 --> 00:40:18,610
learning system that has an adaptive spam filter you know all male reader where you

568
00:40:18,610 --> 00:40:22,040
can sort of classify things as delicate remembers that

569
00:40:22,100 --> 00:40:26,070
most of these work using some variation of nineties is naive bayes

570
00:40:26,090 --> 00:40:29,740
so on the first paper that was published on this was like in the late

571
00:40:29,740 --> 00:40:32,860
nineties nineteen ninety eight and i was at microsoft

572
00:40:32,900 --> 00:40:36,800
and there were some other papers just around the same time the talked about using

573
00:40:36,800 --> 00:40:39,180
naive bayes for spam filtering

574
00:40:39,190 --> 00:40:41,030
and it works fairly well

575
00:40:41,040 --> 00:40:46,020
or at least it it works fairly well until people started getting the idea of

576
00:40:50,160 --> 00:40:52,000
words that were in the message

577
00:40:52,040 --> 00:40:57,270
by using variants look a little bit different so you know instead saying

578
00:40:57,390 --> 00:41:00,510
you know free they might say four

579
00:41:00,520 --> 00:41:03,120
you know three three or something like that

580
00:41:04,890 --> 00:41:09,280
that was in nineteen ninety eight so five years later there was basically a huge

581
00:41:10,920 --> 00:41:14,670
email filters that you could download a lot of these are based on naive bayes

582
00:41:14,670 --> 00:41:17,150
you can sort of tell sometimes from the names

583
00:41:17,280 --> 00:41:20,650
so people were playing with these things

584
00:41:20,660 --> 00:41:21,380
this is

585
00:41:21,390 --> 00:41:25,900
one thing which was in the sourceforge repository was the one of the top downloaded

586
00:41:25,910 --> 00:41:27,390
things for quite awhile

587
00:41:27,480 --> 00:41:31,770
so this is kind of cool so we've got to machine learning program based on

588
00:41:31,770 --> 00:41:38,230
research ideas for a few years ago that is already sorted in wide use

589
00:41:39,480 --> 00:41:43,180
naive bayes has got some really nice things is very fast it's very easy to

590
00:41:44,340 --> 00:41:48,340
it's very well understood formally in the sense that there is this nice probabilistic model

591
00:41:48,340 --> 00:41:50,860
which you used to justify the algorithm

592
00:41:50,870 --> 00:41:54,380
OK and people have been using for a long long time and so

593
00:41:54,390 --> 00:41:56,760
it's well understood experimentally

594
00:41:57,490 --> 00:42:00,990
the bad things about naive bayes is it usually

595
00:42:01,000 --> 00:42:03,000
if you think about are

596
00:42:03,090 --> 00:42:07,670
the problem or if you look at alternative albums you can often find other of

597
00:42:07,700 --> 00:42:10,740
that work will be better so you paying for a little bit a little bit

598
00:42:10,740 --> 00:42:13,300
for their simplicity and ease of implementation

599
00:42:13,520 --> 00:42:18,780
another problem is what would really like these probabilities to be good estimates so if

600
00:42:18,780 --> 00:42:23,410
someone is if they if they so cial probability that's really is point seven

601
00:42:23,500 --> 00:42:26,040
you'd really like to be able to

602
00:42:26,060 --> 00:42:29,600
i believe that probability so you could pass it along to use it in some

603
00:42:29,600 --> 00:42:30,660
other systems

604
00:42:30,780 --> 00:42:36,700
in the later stages of processing but in fact those don't tend to be accurate

605
00:42:36,820 --> 00:42:40,270
just because the assumptions made about the model shows simple

606
00:42:40,280 --> 00:42:43,680
o thing in practice the probability tend to get very very close to zero ones

607
00:42:43,680 --> 00:42:46,950
are not not very useful

608
00:42:48,200 --> 00:42:51,950
that's the first step that's the warm up now we're going to talk about some

609
00:42:53,350 --> 00:42:54,800
learning methods

610
00:42:56,740 --> 00:43:03,030
like naive bayes are relatively fast simple to implement and and and fairly practical for

611
00:43:03,110 --> 00:43:05,880
for text classification methods

612
00:43:05,890 --> 00:43:09,540
so let me go back to this question how we represent text all right

613
00:43:09,560 --> 00:43:14,250
so i talked about a couple things like removing stop words and

614
00:43:14,270 --> 00:43:15,810
seven words

615
00:43:15,820 --> 00:43:18,450
another very common thing you're going to do

616
00:43:19,110 --> 00:43:23,880
to collapse the multiple occurrences of words into a a into something single figure

617
00:43:23,920 --> 00:43:24,560
all right

618
00:43:24,570 --> 00:43:28,540
so this to motivate the recall in the naive bayes algorithm we looked at list

619
00:43:28,540 --> 00:43:29,680
of words

620
00:43:29,700 --> 00:43:32,520
and this what was generated independently

621
00:43:32,540 --> 00:43:35,650
which basically meant the contribution to the score

622
00:43:35,670 --> 00:43:36,890
i was

623
00:43:36,910 --> 00:43:38,890
was insensitive to the position

624
00:43:38,900 --> 00:43:43,030
you get the same contribution to you get the same score you took the words

625
00:43:43,030 --> 00:43:46,110
and reversed were shuffled and randomly

626
00:43:46,150 --> 00:43:49,650
so what does it matter made we don't need to store state the orders rather

627
00:43:49,650 --> 00:43:53,680
than saving is of words in the west we can just say if you know

628
00:43:54,570 --> 00:43:59,000
words of the stems that appear the document and the number of times they appear

629
00:43:59,020 --> 00:44:02,770
so that's basically all the information in this document

630
00:44:02,790 --> 00:44:07,020
r modulo the fact that you know here there's an order of the words here

631
00:44:07,020 --> 00:44:08,070
there's no

632
00:44:09,320 --> 00:44:15,190
right so this is somewhat simpler way of representing the document and

633
00:44:15,220 --> 00:44:18,020
if you look at this particular

634
00:44:18,020 --> 00:44:20,660
picture of the document were essentially we done is i've

635
00:44:20,740 --> 00:44:25,720
i've put in bold made a little bit larger the words that are most frequent

636
00:44:25,720 --> 00:44:28,870
what we're saying here is one function

637
00:44:29,600 --> 00:44:30,950
of a

638
00:44:30,970 --> 00:44:32,540
plus b

639
00:44:32,600 --> 00:44:38,200
which is equal to the same sort of function of a times the same for

640
00:44:38,200 --> 00:44:40,250
a sort of function of b in other words

641
00:44:40,270 --> 00:44:43,640
when we multiply these functions we get

642
00:44:43,720 --> 00:44:46,740
a function of the some of those arguments

643
00:44:47,070 --> 00:44:55,180
what functional form does that

644
00:45:00,540 --> 00:45:02,270
so in other words

645
00:45:02,410 --> 00:45:05,040
the the a class b

646
00:45:05,870 --> 00:45:10,040
equals the to the eight times each day

647
00:45:10,040 --> 00:45:12,160
right so it suggest

648
00:45:12,200 --> 00:45:16,000
that there's an exponential

649
00:45:17,180 --> 00:45:21,100
i suggest

650
00:45:30,870 --> 00:45:35,350
there's also some physical insight that we can apply to the problem

651
00:45:35,350 --> 00:45:37,620
first of all

652
00:45:37,680 --> 00:45:42,580
we expect that states with higher energy in general be less probable than state of

653
00:45:42,580 --> 00:45:44,350
lower energy

654
00:45:45,790 --> 00:45:46,950
you know if it's

655
00:45:46,970 --> 00:45:51,700
it's called not much thermal energy around the how much energy to the particle in

656
00:45:52,040 --> 00:45:54,100
equilibrium collection of particles

657
00:45:54,120 --> 00:45:54,970
i have

658
00:45:54,970 --> 00:45:56,450
at thermal equilibrium

659
00:45:56,540 --> 00:46:00,240
well you know that there's not much the letters around you don't expect it to

660
00:46:00,240 --> 00:46:04,290
be very likely the particle is a huge amount of energy that one individual particle

661
00:46:04,290 --> 00:46:06,080
as it could happen

662
00:46:06,100 --> 00:46:09,060
from some random collisions that just happened

663
00:46:09,080 --> 00:46:12,020
the bulk of the energy that one particle

664
00:46:12,080 --> 00:46:14,700
but on balance is not going to be very probable

665
00:46:14,700 --> 00:46:17,620
lower energy will be more probable

666
00:46:17,680 --> 00:46:22,100
the other thing is you know even thinking about something like that

667
00:46:22,160 --> 00:46:24,350
temperature comes in immediately

668
00:46:25,680 --> 00:46:30,160
if you raised the temperature lot surely we have to expect many more molecules will

669
00:46:30,160 --> 00:46:33,830
start to to become energized more energy

670
00:46:35,160 --> 00:46:38,680
lower probability for higher energy

671
00:46:38,720 --> 00:46:43,100
but the probability violation go up if the temperature goes up

672
00:46:43,160 --> 00:46:47,200
in other words the the ratio of energy the temperature

673
00:46:47,250 --> 00:46:48,790
should be involved here

674
00:46:50,560 --> 00:46:55,020
i should go

675
00:46:57,160 --> 00:47:02,600
as you know i was

676
00:47:02,810 --> 00:47:07,200
should depend

677
00:47:08,390 --> 00:47:12,450
i over t

678
00:47:15,600 --> 00:47:17,410
we can start with this

679
00:47:17,430 --> 00:47:19,140
functional form

680
00:47:20,040 --> 00:47:24,290
you know by the way there's no reason we can't have in general we have

681
00:47:24,290 --> 00:47:29,270
some constant here right we haven't changed anything to do that

682
00:47:29,290 --> 00:47:29,950
and now

683
00:47:29,950 --> 00:47:31,830
let's just use the

684
00:47:31,850 --> 00:47:34,200
a little bit of physical intuition

685
00:47:34,250 --> 00:47:35,750
there were

686
00:47:35,850 --> 00:47:37,290
thinking about here to

687
00:47:37,290 --> 00:47:40,580
refine this just a little bit let's right

688
00:47:40,600 --> 00:47:41,970
we expect the p

689
00:47:43,180 --> 00:47:46,600
the eyes some

690
00:47:46,750 --> 00:47:49,270
exponential to the miners

691
00:47:51,350 --> 00:47:54,720
i over t four c

692
00:47:54,850 --> 00:47:57,410
some constant greater than zero so

693
00:47:57,410 --> 00:47:59,890
this will have the property then the as

694
00:47:59,930 --> 00:48:04,600
energy goes up the probability goes down but it scaled by temperature by raising the

695
00:48:04,600 --> 00:48:07,540
temperature that makes the higher energy states

696
00:48:07,870 --> 00:48:12,040
yet more and more likely

697
00:48:17,160 --> 00:48:20,470
this is

698
00:48:21,640 --> 00:48:24,100
functional form for probability

699
00:48:24,120 --> 00:48:26,620
of the molecule being in state

700
00:48:26,660 --> 00:48:32,410
with energy by i and the only difference between this and what's written conventionally

701
00:48:32,450 --> 00:48:33,720
is the

702
00:48:33,740 --> 00:48:38,560
re the constant is labeled so really what we have is he i

703
00:48:38,650 --> 00:48:41,430
of the i

704
00:48:41,500 --> 00:48:43,200
is proportional to

705
00:48:43,220 --> 00:48:45,640
e to the minus ERI

706
00:48:48,810 --> 00:48:50,520
so you're just kt

707
00:48:50,540 --> 00:48:51,930
we're k

708
00:48:52,430 --> 00:48:58,390
it's called the boltzmann

709
00:48:58,450 --> 00:49:03,580
constant and it's just equal to far

710
00:49:03,600 --> 00:49:06,540
overall the god number the

711
00:49:06,560 --> 00:49:11,480
gas constant per molecule rather than per mole

712
00:49:14,100 --> 00:49:16,890
one way to

713
00:49:16,930 --> 00:49:19,160
try to rationalize this is is

714
00:49:19,160 --> 00:49:23,950
you've probably seen you'll see arrhenius kinetics right

715
00:49:23,970 --> 00:49:26,470
arrhenius rate laws

716
00:49:27,890 --> 00:49:30,160
if you remember what that looks like

717
00:49:32,830 --> 00:49:39,450
you get this rate constants

718
00:49:39,470 --> 00:49:42,580
arrhenius rate constant

719
00:49:42,640 --> 00:49:48,040
some constant they times e to the minus p a all were or

720
00:49:48,080 --> 00:49:50,890
t relax

721
00:49:51,040 --> 00:49:54,080
seen that before

722
00:49:55,390 --> 00:50:00,120
so what's happening here e a is what is an activation

723
00:50:01,770 --> 00:50:03,290
the idea

724
00:50:03,290 --> 00:50:04,470
is a graph

725
00:50:06,600 --> 00:50:11,470
products and there's some barrier you've got to get over the activation energy before you

726
00:50:12,100 --> 00:50:14,450
have reactions to the rate

727
00:50:14,470 --> 00:50:17,790
depends on surrounding that activation barrier

728
00:50:19,770 --> 00:50:22,470
this is really coming from the same

729
00:50:22,500 --> 00:50:25,390
the idea is this which is

730
00:50:25,450 --> 00:50:27,220
the probability

731
00:50:27,240 --> 00:50:31,200
one of the molecules having this much energy

732
00:50:31,270 --> 00:50:33,350
depends on the minus

733
00:50:33,370 --> 00:50:36,700
energy over this is promotional this from all

734
00:50:36,720 --> 00:50:38,540
the exact same relation

735
00:50:39,370 --> 00:50:42,450
so the idea in the context of kinetics

736
00:50:42,450 --> 00:50:45,750
is that the rate depends on

737
00:50:45,790 --> 00:50:47,930
how many molecules can get up here

738
00:50:47,950 --> 00:50:49,660
what the probability is over

739
00:50:49,660 --> 00:50:52,620
getting enough energy to go over the barriers

740
00:50:52,640 --> 00:50:56,870
and that energy that probability is given by these expressions

741
00:50:56,890 --> 00:51:02,970
so in this form you've seen this kind of dependence before and very explicitly

742
00:51:06,520 --> 00:51:15,220
OK let me just

743
00:51:15,270 --> 00:51:16,640
take it one

744
00:51:16,660 --> 00:51:20,100
a small but important step father

745
00:51:20,100 --> 00:51:23,180
which is

746
00:51:23,220 --> 00:51:26,120
that proportionality constant

747
00:51:26,120 --> 00:51:30,520
series untapped one coefficients and plug them into formula

748
00:51:32,900 --> 00:51:34,480
this combination

749
00:51:34,490 --> 00:51:37,480
we take the direct product of two

750
00:51:37,480 --> 00:51:39,450
representations two

751
00:51:39,460 --> 00:51:46,120
fourier transfer matrices conjugate them by the appropriate that for matrices are multiplied by the

752
00:51:46,120 --> 00:51:47,700
appropriate some of

753
00:51:47,700 --> 00:51:51,540
three matrices and the other side of this is the natural generalization of the by

754
00:51:51,540 --> 00:51:55,560
spectrum to noncommutative groups

755
00:51:55,580 --> 00:52:01,490
people are aware of this but it's not widely known especially not widely known in

756
00:52:01,490 --> 00:52:03,910
applied communities

757
00:52:03,920 --> 00:52:08,280
it's also interesting to note that

758
00:52:09,480 --> 00:52:10,570
that this

759
00:52:10,590 --> 00:52:16,500
does become quite large objects re c even in the original classical

760
00:52:16,520 --> 00:52:17,500
the case

761
00:52:17,580 --> 00:52:22,450
what we had was that the original of fourier transform was obviously of the same

762
00:52:22,450 --> 00:52:27,780
size as the original signals but if the original signal was still numbers there ten

763
00:52:27,780 --> 00:52:33,040
minus one is like an n dimensional vector but the by spectrum was and squared

764
00:52:33,040 --> 00:52:34,670
dimensional objects

765
00:52:34,810 --> 00:52:38,210
so it's larger have completeness but the price to pay

766
00:52:38,230 --> 00:52:40,190
by spectrum

767
00:52:40,200 --> 00:52:44,560
there is also larger but the individual components of very large as well because here

768
00:52:44,570 --> 00:52:49,780
taking representation matrices are taking the kronecker product so individual

769
00:52:49,810 --> 00:52:55,560
not only do we need some twice over the set of integral do so representations

770
00:52:56,060 --> 00:52:57,150
we also

771
00:52:57,160 --> 00:53:02,080
four rather large matrices that will contain these invariants

772
00:53:03,570 --> 00:53:05,310
the interesting thing

773
00:53:05,360 --> 00:53:07,960
and by no means trivial

774
00:53:07,990 --> 00:53:13,630
thing is that the completeness property survives in this case for all compact groups and

775
00:53:13,630 --> 00:53:14,810
even some

776
00:53:15,080 --> 00:53:17,070
exceptional noncompact groups

777
00:53:17,270 --> 00:53:22,620
so this was a major result that was approved by caroline his thesis in the

778
00:53:22,620 --> 00:53:23,960
mid nineties

779
00:53:23,980 --> 00:53:25,460
and then

780
00:53:25,480 --> 00:53:28,570
it can i think it kind of sort of remained on the

781
00:53:28,580 --> 00:53:34,400
the fringes of mathematics because applied communities were not necessarily

782
00:53:36,450 --> 00:53:42,310
noncommutative harmonic analysis literature mathematicians did not necessarily see the

783
00:53:42,330 --> 00:53:49,000
how this links up with the math problems in pure mathematics and not very many

784
00:53:49,000 --> 00:53:50,570
people use this thing

785
00:53:50,570 --> 00:53:53,070
but i think this is a really powerful

786
00:53:53,070 --> 00:53:54,710
and deep results

787
00:53:55,280 --> 00:53:59,920
what is the difference the generalisation of the by spectrum to noncommutative and is the

788
00:53:59,940 --> 00:54:04,660
straightforward so as i just tried to explain to you it sort of does follow

789
00:54:04,700 --> 00:54:09,280
the general logic of how you do things when you go from ordinary harmonic analysis

790
00:54:09,280 --> 00:54:12,710
to noncommutative harmonic analysis proving this theory

791
00:54:12,730 --> 00:54:18,270
this theorem is actually not trivial it goes quite deep into representation theory and exploits

792
00:54:18,270 --> 00:54:23,210
the kind of duality between groups and the set of any country usable representations

793
00:54:23,280 --> 00:54:27,860
i also have to say

794
00:54:27,870 --> 00:54:31,120
that there is one condition on the

795
00:54:31,410 --> 00:54:36,570
r in the theorem which at first sight is innocuous and has a direct analogue

796
00:54:36,570 --> 00:54:41,110
in the classical case and that is that the theorem the complete this part of

797
00:54:41,120 --> 00:54:48,580
the theory only works if the original spectrum is nonsingular so each matrix in the

798
00:54:48,580 --> 00:54:52,500
four original fourier transform has to be invertible

799
00:54:52,620 --> 00:54:54,650
so in that sort of

800
00:54:56,710 --> 00:55:01,000
a set of functions of measure zero where one of those

801
00:55:01,020 --> 00:55:04,090
matrices disappears becomes singular

802
00:55:04,120 --> 00:55:05,910
and the

803
00:55:05,910 --> 00:55:11,240
i invariance property still holds because just depends on the on the straightforward and went

804
00:55:12,280 --> 00:55:14,860
the uniqueness thing fails

805
00:55:14,960 --> 00:55:19,200
so it's important to bear in mind when we try and apply this result in

806
00:55:19,200 --> 00:55:20,330
real problems

807
00:55:20,360 --> 00:55:24,520
in particular it's important when we try and apply it to problems

808
00:55:24,530 --> 00:55:28,660
when the original function is not really the group but something that the group is

809
00:55:28,660 --> 00:55:29,850
acting on

810
00:55:29,860 --> 00:55:34,990
so these are called homogeneous spaces this is this is where we put the

811
00:55:35,330 --> 00:55:36,450
stuff too

812
00:55:36,490 --> 00:55:37,790
were right

813
00:55:37,870 --> 00:55:41,080
so what is the kind of context in which

814
00:55:41,190 --> 00:55:43,780
in which

815
00:55:44,110 --> 00:55:50,000
this is noncommutative by spectral calculus results are going to be useful in practical problems

816
00:55:50,530 --> 00:55:52,490
well the sort of

817
00:55:52,490 --> 00:55:58,700
things that i have in mind are things like functions on the sphere and thinking

818
00:55:58,700 --> 00:56:01,900
about what happens to them when we rotate the sphere

819
00:56:07,460 --> 00:56:14,530
so can to consider the two sphere which is just ordinary unit sphere

820
00:56:15,530 --> 00:56:17,000
three three-dimensional space

821
00:56:17,030 --> 00:56:19,940
and consider that we have a function

822
00:56:19,950 --> 00:56:21,860
this complex valued functions

823
00:56:21,940 --> 00:56:23,980
on on the sphere

824
00:56:23,980 --> 00:56:25,810
now if you look at it

825
00:56:25,830 --> 00:56:30,920
so three so the group of three-dimensional rotations

826
00:56:31,870 --> 00:56:36,790
that acts naturally on the sphere rotating the sphere and rotating the function

827
00:56:36,810 --> 00:56:37,570
with it

828
00:56:38,280 --> 00:56:40,400
what's interesting is too

829
00:56:40,400 --> 00:56:43,650
developer sequence of invariance for such functions

830
00:56:43,660 --> 00:56:47,790
which are going to be invariant to this action so the action of

831
00:56:47,920 --> 00:56:50,290
the rotation group on the sphere

832
00:56:50,410 --> 00:56:54,690
now it's clear that the sphere in the rotation group some related but they're not

833
00:56:54,700 --> 00:56:55,870
the same thing

834
00:56:55,870 --> 00:56:59,900
so here the function is not on the rotation group it's on the sphere

835
00:57:01,060 --> 00:57:04,690
and that is an important distinction what's happening here

836
00:57:04,730 --> 00:57:08,830
in mathematical terms is that we have a function of the group but one what

837
00:57:08,850 --> 00:57:14,000
is called a homogeneous space of of group by homogeneous spaces

838
00:57:14,020 --> 00:57:15,880
basically any space

839
00:57:15,900 --> 00:57:18,350
on which group acts

840
00:57:18,370 --> 00:57:19,810
so clearly

841
00:57:19,830 --> 00:57:23,990
if you have a point on the sphere then the rotation group sticks it somewhere

842
00:57:23,990 --> 00:57:25,240
in the sphere

843
00:57:25,250 --> 00:57:26,450
and also

844
00:57:27,950 --> 00:57:32,900
the other requirement is that the entire space has to be swept out by the

845
00:57:32,900 --> 00:57:35,160
action of the group which is again trivially

846
00:57:35,240 --> 00:57:38,570
satisfied in the case of the sphere and this is a three-year-old so you have

847
00:57:38,650 --> 00:57:43,940
the sphere a point on it so you start with the north pole

848
00:57:43,950 --> 00:57:51,170
this is your x o and then you consider have rotation matrices rotation matrices acting

849
00:57:51,170 --> 00:57:54,120
on as point x o then you then

850
00:57:54,230 --> 00:57:57,700
you get all the points on the sphere and this weekend so be about the

851
00:57:57,700 --> 00:58:02,830
entire sphere so you can get anywhere in the sphere just by rotating any

852
00:58:02,860 --> 00:58:08,160
single point you start out with so this is what homogeneous spaces it turns out

853
00:58:08,160 --> 00:58:14,530
that this whole business of fourier transformations naturally generalizes to this round because

854
00:58:14,730 --> 00:58:16,700
when you take the

855
00:58:17,450 --> 00:58:21,450
all you have to do is to choose a single

856
00:58:21,450 --> 00:58:26,380
sort of initial elements such as the north pole in this case and you can

857
00:58:26,380 --> 00:58:29,310
here you have somehow

858
00:58:29,330 --> 00:58:33,920
look like you'd involves all the all the layers interacting in a complicated way you

859
00:58:33,920 --> 00:58:36,780
still have this funny thing that

860
00:58:36,850 --> 00:58:39,220
it's hard to train these things

861
00:58:39,220 --> 00:58:41,820
because of the death

862
00:58:41,850 --> 00:58:42,710
but if you

863
00:58:42,710 --> 00:58:45,760
find a way to initialize this deep structure by

864
00:58:45,780 --> 00:58:48,650
training each level in an unsupervised way

865
00:58:48,670 --> 00:58:52,010
we really get better results

866
00:58:52,010 --> 00:58:54,280
actually he's getting the best

867
00:58:54,290 --> 00:59:00,630
results on mnist which doesn't use convolutional structures

868
00:59:00,660 --> 00:59:02,820
OK so now i'm going to

869
00:59:02,830 --> 00:59:04,860
i spend a bit of time

870
00:59:04,870 --> 00:59:06,340
tell you about

871
00:59:07,450 --> 00:59:09,970
evidence to try to understand why these

872
00:59:09,980 --> 00:59:11,310
these things work

873
00:59:11,360 --> 00:59:12,990
so i've already talked about

874
00:59:13,080 --> 00:59:15,110
why deep architectures such

875
00:59:15,120 --> 00:59:17,540
it might be interesting but why

876
00:59:17,630 --> 00:59:23,080
we get so big difference in performance when we consider training

877
00:59:23,160 --> 00:59:25,960
this unsupervised layer wise

878
00:59:26,960 --> 00:59:32,670
versus training everything together with respect to some single objective

879
00:59:39,810 --> 00:59:42,970
this bill here

880
00:59:43,000 --> 00:59:46,190
right what one first observation is that you can as i said earlier you can

881
00:59:46,190 --> 00:59:48,070
replace rbms by other

882
00:59:50,370 --> 00:59:51,240
the first

883
00:59:51,250 --> 00:59:56,230
things that have been tried or auto encoders and variations of autoencoders particular

884
00:59:56,240 --> 01:00:00,690
in any other constructive been working on sparse autoencoders

885
01:00:00,740 --> 01:00:06,390
but you can also do things like kernel PCA

886
01:00:07,820 --> 01:00:11,530
we've been exploring various autoencoders called denoising autoencoders

887
01:00:11,540 --> 01:00:13,140
i will say a few words about

888
01:00:13,200 --> 01:00:18,400
and then there are some other methods which is very different

889
01:00:22,420 --> 01:00:25,080
the idea is really going to learn

890
01:00:25,120 --> 01:00:27,300
two two two

891
01:00:27,340 --> 01:00:29,620
to use an unsupervised training criterion

892
01:00:29,660 --> 01:00:32,840
at each level or maybe at some intermediate level

893
01:00:33,760 --> 01:00:36,320
tries to learn an embedding of the input

894
01:00:36,380 --> 01:00:39,230
so we already know a lot of learning algorithms for

895
01:00:39,230 --> 01:00:42,980
a supervised learning of embedding otherwise transforming the input into some space in the way

896
01:00:42,980 --> 01:00:47,360
that capture variations well that's exactly what we want in this setting i've been talking

897
01:00:48,980 --> 01:00:49,770
and so

898
01:00:49,780 --> 01:00:52,310
various embedding algorithms have been

899
01:00:52,350 --> 01:00:56,800
blood into this framework and combined with this revised criterion

900
01:00:59,190 --> 01:01:03,380
these these experiments showed you can you can get

901
01:01:03,450 --> 01:01:05,780
really really better results than if you train

902
01:01:05,800 --> 01:01:07,270
everything from scratch

903
01:01:07,270 --> 01:01:09,610
globally with a surprise criteria

904
01:01:09,640 --> 01:01:13,890
one of the interesting variants of this that we're investigating recently

905
01:01:13,950 --> 01:01:16,630
and so there's an ICML paper

906
01:01:18,640 --> 01:01:21,140
what have you know

907
01:01:21,190 --> 01:01:22,810
which is the group but any c

908
01:01:22,840 --> 01:01:26,870
and and my group that the next steps

909
01:01:26,890 --> 01:01:32,830
is about using slow feature so-called slow features for this embedding

910
01:01:34,050 --> 01:01:39,720
one simple idea is that if you have sequential data like video

911
01:01:39,730 --> 01:01:41,560
the features

912
01:01:41,610 --> 01:01:47,770
many features that would be interesting would be abstract features that don't change very quickly

913
01:01:47,770 --> 01:01:51,160
over time so if you consider a sequence of images very likely

914
01:01:51,210 --> 01:01:54,100
the identity of the objects in

915
01:01:54,160 --> 01:01:57,550
the sequence would stay the same from one frame to the next frame

916
01:01:57,620 --> 01:01:59,900
and many other such features i also

917
01:01:59,920 --> 01:02:02,000
the constant or change slowly

918
01:02:02,000 --> 01:02:03,750
so you can take advantage of that

919
01:02:03,760 --> 01:02:05,040
in order to

920
01:02:09,620 --> 01:02:11,370
features to do something

921
01:02:11,380 --> 01:02:12,290
it has to that

922
01:02:12,310 --> 01:02:15,660
property and also you want them to do the same thing so there

923
01:02:15,720 --> 01:02:17,880
details there but

924
01:02:17,910 --> 01:02:22,200
but it's completely unsupervised and can be used to initialize

925
01:02:22,230 --> 01:02:24,630
was the networks or can be combined

926
01:02:24,740 --> 01:02:30,010
so you can have an unsupervised criterion unanimous frustrating and combine them

927
01:02:30,060 --> 01:02:36,060
and train with respect to the some of them

928
01:02:39,620 --> 01:02:42,820
one type of

929
01:02:42,880 --> 01:02:45,550
level local

930
01:02:45,550 --> 01:02:47,720
the model is the autoencoder

931
01:02:47,800 --> 01:02:50,010
let come back

932
01:02:54,450 --> 01:02:55,740
OK so

933
01:02:55,780 --> 01:02:57,560
what is not one quarter

934
01:02:57,760 --> 01:02:58,920
don't clutter

935
01:02:58,940 --> 01:02:59,980
it is

936
01:03:00,010 --> 01:03:03,400
simply something that predicts its input

937
01:03:03,450 --> 01:03:07,350
going through some intermediate code

938
01:03:08,060 --> 01:03:10,380
the simplest autoencoder is just

939
01:03:11,270 --> 01:03:14,710
one linear transformation one fixed nonlinear

940
01:03:14,720 --> 01:03:18,130
transformation like the hyperbolic tangent of sigmoids

941
01:03:18,150 --> 01:03:20,880
and then another linear transformation

942
01:03:20,920 --> 01:03:21,820
and this

943
01:03:21,830 --> 01:03:24,450
tries to predict the input

944
01:03:24,480 --> 01:03:27,550
so you optimize all the parameters to get

945
01:03:27,560 --> 01:03:30,200
the output look like the input using some

946
01:03:31,910 --> 01:03:34,590
so once you've done that training this

947
01:03:34,600 --> 01:03:36,980
which might as well why would be useful

948
01:03:36,980 --> 01:03:38,300
you could stack

949
01:03:38,310 --> 01:03:40,070
these autoencoders sold

950
01:03:40,080 --> 01:03:41,970
the representation you get here

951
01:03:41,970 --> 01:03:45,440
can be used as input for training as second with one

952
01:03:45,490 --> 01:03:47,780
so why would these be useful well actually

953
01:03:48,840 --> 01:03:51,600
if the whole thing was linear you would get

954
01:03:51,610 --> 01:03:52,650
something like

955
01:03:52,650 --> 01:03:57,730
like an unfair comparison here i'm sort of making sort of the treatment of the

956
01:03:57,730 --> 01:04:01,800
maximum likelihood estimator something that little bit of theory

957
01:04:01,820 --> 01:04:02,620
and no

958
01:04:02,630 --> 01:04:05,190
nobody in their right mind would we like to compare

959
01:04:05,200 --> 01:04:11,260
this is the probability with which with other properties and in this is true that

960
01:04:11,270 --> 01:04:12,830
people don't actually

961
01:04:12,840 --> 01:04:16,840
choose the model based on this exactly for that reason why so if you use

962
01:04:16,960 --> 01:04:21,770
dataset two two two to fit your promise to optimize your problem then you know

963
01:04:21,770 --> 01:04:24,900
this not a good estimate of the actual

964
01:04:25,030 --> 01:04:29,840
of the actual predictive abilities of that model but they have to do something else

965
01:04:29,880 --> 01:04:33,350
an unfortunate thing is that doesn't really seem to be an ice and so within

966
01:04:33,350 --> 01:04:34,480
the framework

967
01:04:34,500 --> 01:04:36,730
as to to what you should do instead

968
01:04:36,750 --> 01:04:41,140
so the core problem is that you're reusing the data to do two things once

969
01:04:41,140 --> 01:04:42,820
you try to estimate the problem

970
01:04:42,870 --> 01:04:44,850
and the other thing you want to do is you want to

971
01:04:44,860 --> 01:04:45,750
figure out

972
01:04:45,840 --> 01:04:47,640
how good is the model actually

973
01:04:47,750 --> 01:04:52,710
the one way of resolving the problem is to use cross validation conservation you

974
01:04:52,730 --> 01:04:54,340
yes you a two

975
01:04:54,400 --> 01:04:58,970
sets or into into multiple that and then you train on some subset of the

976
01:04:58,970 --> 01:05:03,710
data and then you try to evaluate on the on the other set and then

977
01:05:03,730 --> 01:05:07,540
that remove this problem

978
01:05:07,550 --> 01:05:10,870
but instead you get different problem you get different problem that you need to fit

979
01:05:10,880 --> 01:05:15,130
multiple models and you need to you to fit these models using only a subset

980
01:05:15,150 --> 01:05:16,580
of the data

981
01:05:17,520 --> 01:05:21,570
say if you split the data into in the new train one half and you

982
01:05:21,570 --> 01:05:26,880
still have to value things then the problem that you're now only using half the

983
01:05:28,060 --> 01:05:33,870
and in real applications this might be a problem i might be much harder to

984
01:05:33,870 --> 01:05:38,170
find the relationship and that you only have happened many fish

985
01:05:38,220 --> 01:05:42,350
they also ways around that so you try to use

986
01:05:42,400 --> 01:05:44,000
you know

987
01:05:44,010 --> 01:05:47,320
anyway cost the nation where you see the thing we only hold out

988
01:05:47,340 --> 01:05:49,510
you know very small part of an effort

989
01:05:49,760 --> 01:05:51,930
trying to model

990
01:05:51,940 --> 01:05:53,410
OK so

991
01:05:53,430 --> 01:05:55,100
the the

992
01:05:55,150 --> 01:05:59,970
initially thought the mysterious thing we had here was that the more the model that

993
01:05:59,970 --> 01:06:01,100
doesn't really

994
01:06:01,110 --> 01:06:02,850
it doesn't seem to officially

995
01:06:02,870 --> 01:06:08,200
the that that to model data well one actually got no votes for this model

996
01:06:08,200 --> 01:06:10,650
from from a hundred people the

997
01:06:10,660 --> 01:06:14,880
prior probability must have been in these smaller than one over a hundred

998
01:06:15,370 --> 01:06:21,280
but this model the base in the form of a treatment with a that the

999
01:06:21,280 --> 01:06:22,770
model as well

1000
01:06:22,890 --> 01:06:28,120
ever so slightly more likely actually of that small equally likely

1001
01:06:28,170 --> 01:06:29,110
it turns out

1002
01:06:29,120 --> 01:06:32,110
six the numbers in the higher

1003
01:06:33,240 --> 01:06:37,100
how can that be y whereas whereas the mechanism coming from that ensuring that this

1004
01:06:37,270 --> 01:06:38,630
is happening

1005
01:06:38,640 --> 01:06:41,000
he is one of my

1006
01:06:41,120 --> 01:06:46,860
there were pictures of try to explain how this comes that you might have seen

1007
01:06:46,860 --> 01:06:49,350
this picture before i think it

1008
01:06:49,470 --> 01:06:53,580
it's a picture which is not so easy to understand will worthwhile

1009
01:06:53,590 --> 01:06:57,690
each trying to think hard about what's going on the

1010
01:06:57,700 --> 01:06:59,840
what i find one i want here

1011
01:06:59,860 --> 01:07:03,350
finding the value of the marginal likelihood

1012
01:07:03,360 --> 01:07:04,800
for notation

1013
01:07:04,870 --> 01:07:09,730
don't want to talk about is the different here this should be

1014
01:07:14,040 --> 01:07:15,580
data given the model

1015
01:07:15,590 --> 01:07:16,600
where the

1016
01:07:16,610 --> 01:07:18,820
probably much like

1017
01:07:19,710 --> 01:07:21,900
following the the the the

1018
01:07:21,920 --> 01:07:24,580
you might like a function

1019
01:07:25,430 --> 01:07:29,530
of a data set out the the the the the the the x axis here

1020
01:07:29,630 --> 01:07:31,870
is is is

1021
01:07:31,880 --> 01:07:34,750
it's an abstract representation of data

1022
01:07:34,800 --> 01:07:38,150
that's the way it's supposed to represent all possible data datasets that they are because

1023
01:07:39,000 --> 01:07:44,080
you can think about these ten point one example was actually only a finite number

1024
01:07:44,080 --> 01:07:49,660
of different outcome if have to think about these things being ordered sequences and there

1025
01:07:49,660 --> 01:07:50,800
are two to the ten

1026
01:07:50,850 --> 01:07:56,560
different possible outcomes for each experiment two possible outcomes we can think about

1027
01:07:56,570 --> 01:08:00,100
you know you know lining of the data

1028
01:08:00,250 --> 01:08:03,910
in some cases would be infinite but that using the best models

1029
01:08:05,570 --> 01:08:08,790
abstract thing

1030
01:08:09,340 --> 01:08:12,420
but the interesting thing is that we

1031
01:08:12,530 --> 01:08:15,350
the marginal likelihood is the probability distribution

1032
01:08:15,400 --> 01:08:18,250
i think the probability of the data given the model

1033
01:08:18,310 --> 01:08:20,570
the probability distribution of the data

1034
01:08:20,670 --> 01:08:22,640
and in probability distributions

1035
01:08:22,710 --> 01:08:23,820
have to normalize

1036
01:08:23,870 --> 01:08:28,080
the other thing that integrate to one of them to one

1037
01:08:28,090 --> 01:08:30,290
and i have drawn a bunch of different models

1038
01:08:31,980 --> 01:08:33,000
i call this one

1039
01:08:33,010 --> 01:08:34,590
the green one here

1040
01:08:34,590 --> 01:08:37,160
OK if you changes flat heads and tails

1041
01:08:37,180 --> 01:08:41,460
randomness should not change and that the property of symmetry

1042
01:08:41,530 --> 01:08:44,190
it's also got this property of additivity

1043
01:08:44,240 --> 01:08:46,110
and this is a very useful thing

1044
01:08:46,170 --> 01:08:50,810
because so have two independent random variables that the entropy of the taken together is

1045
01:08:50,810 --> 01:08:52,880
the sum of the individual his

1046
01:08:52,930 --> 01:08:55,480
and that's why this is the case

1047
01:08:55,500 --> 01:08:56,280
OK so

1048
01:08:56,290 --> 01:08:58,480
i had these two things that are independent

1049
01:08:58,530 --> 01:09:02,200
and i want to look at the joint entropy the some of them taken together

1050
01:09:02,250 --> 01:09:06,290
well that's the sum over all possible outcomes x and y

1051
01:09:06,330 --> 01:09:09,560
just plug into the definition p of x log one of one of the of

1052
01:09:09,560 --> 01:09:10,850
x y

1053
01:09:10,910 --> 01:09:15,030
but in this case because the independent of x y is next time span of

1054
01:09:16,650 --> 01:09:17,740
and so

1055
01:09:17,760 --> 01:09:22,270
and because it's a lot i can split this into its two pieces

1056
01:09:22,350 --> 01:09:25,870
lot one of the p of x plus log one of the of

1057
01:09:26,790 --> 01:09:30,040
so i get these two individuals summations

1058
01:09:30,090 --> 01:09:34,920
and i can now some of the y and in this case i consider x

1059
01:09:34,960 --> 01:09:41,200
OK so i get the the individual entropy effects was individual and the y

1060
01:09:41,210 --> 01:09:46,750
so this is another person of the properties we wanted

1061
01:09:46,800 --> 01:09:50,890
what the variables are not independent

1062
01:09:50,900 --> 01:09:54,340
in that case we wanted to be the case that the entropy of the joint

1063
01:09:54,340 --> 01:09:57,180
distribution is at most

1064
01:09:57,230 --> 01:10:00,510
is that is at most the sum of the individual entropy

1065
01:10:00,560 --> 01:10:02,190
because we want to do so

1066
01:10:02,200 --> 01:10:05,650
and that's called sub additivity because if they are independent

1067
01:10:05,690 --> 01:10:08,890
the the entropy of the joint is just the sum of the individual entropy is

1068
01:10:08,970 --> 01:10:11,690
if the dependent then it's going to be less than that

1069
01:10:11,740 --> 01:10:15,050
and that's another that's another good property that got

1070
01:10:15,120 --> 01:10:20,390
we normalised so that a fair coin has entropy one you could just buy an

1071
01:10:20,390 --> 01:10:21,870
arbitrary constant

1072
01:10:21,890 --> 01:10:26,440
a reasonable normalisation is just to say fair coins entropy one

1073
01:10:26,450 --> 01:10:29,210
and finally we want to be the case

1074
01:10:30,060 --> 01:10:33,770
as as the bias of the queen goes to zero the entropy also goes to

1075
01:10:34,940 --> 01:10:39,610
because as it becomes more deterministic everything goes to zero and it also has that

1076
01:10:41,430 --> 01:10:43,570
so it's got six properties

1077
01:10:43,580 --> 01:10:48,640
that we think you know the intuitively should be true of any measure of randomness

1078
01:10:48,690 --> 01:10:52,280
but now it turns out that entropy is the only measure that satisfies these

1079
01:10:52,330 --> 01:10:53,080
OK so

1080
01:10:53,090 --> 01:10:55,350
proceed an axiomatic way

1081
01:10:55,380 --> 01:10:56,930
if you say look i want to

1082
01:10:56,950 --> 01:10:59,980
i want to come up with a measure of randomness and now let me put

1083
01:10:59,980 --> 01:11:03,340
down a few properties that any measure of randomness should have

1084
01:11:03,380 --> 01:11:05,850
all very reasonable sounding properties

1085
01:11:05,930 --> 01:11:07,750
OK well with extensibility

1086
01:11:07,800 --> 01:11:13,930
that's totally reasonable if you add on fake news some you don't the new outcome

1087
01:11:13,930 --> 01:11:15,130
that never occurs

1088
01:11:15,180 --> 01:11:19,250
sure that should not change the entropy symmetry additivity

1089
01:11:19,270 --> 01:11:21,880
so that activity of normalisation

1090
01:11:21,900 --> 01:11:24,880
these are very you know these are these are things that you know we would

1091
01:11:24,880 --> 01:11:28,310
think of course this should be true of the measure of randomness

1092
01:11:28,420 --> 01:11:32,780
but entropy is the only one that satisfies all of them look into this justifies

1093
01:11:32,830 --> 01:11:37,170
why we fight we use something with that we functional form of the law one

1094
01:11:37,170 --> 01:11:40,620
of the it seems a little crazy at first but this is why we are

1095
01:11:40,620 --> 01:11:41,680
stuck with it

1096
01:11:42,370 --> 01:11:44,940
this is why we have to deal with the thing

1097
01:11:45,640 --> 01:11:48,640
so let's look at some other let's look at some of the properties of this

1098
01:11:50,570 --> 01:11:53,760
first of all it is closely related to the KL divergence

1099
01:11:54,520 --> 01:11:58,260
so some of you might already know this distance measure

1100
01:11:58,280 --> 01:12:02,490
it's a very commonly used distance measure between probability distributions

1101
01:12:02,530 --> 01:12:05,700
suppose i have two probability distributions p and q

1102
01:12:05,740 --> 01:12:07,790
and i want to measure the distance between them

1103
01:12:07,810 --> 01:12:12,190
many ways one could do it one could look one could for instance draw the

1104
01:12:12,190 --> 01:12:16,300
curves to p and q and look at the area between the curves that's very

1105
01:12:16,300 --> 01:12:19,140
intuitive way to do it and that's one distance

1106
01:12:19,890 --> 01:12:22,270
one could look at the o two distance between them

1107
01:12:22,320 --> 01:12:24,680
all one could for some reason

1108
01:12:24,720 --> 01:12:28,100
plug it into this formula that looks a lot more weird than any of those

1109
01:12:28,100 --> 01:12:29,010
other two

1110
01:12:30,290 --> 01:12:35,640
and it turns out that this is something that is very widely used in statistics

1111
01:12:35,930 --> 01:12:39,420
so what are the features of this what are the properties of this

1112
01:12:39,470 --> 01:12:40,720
well first of all

1113
01:12:40,790 --> 01:12:44,160
it has many many unpleasant aspects to it

1114
01:12:44,210 --> 01:12:46,280
OK so first of all it's not symmetric

1115
01:12:46,340 --> 01:12:49,420
the distance from p to q is not the same as the distance from q

1116
01:12:49,420 --> 01:12:51,430
to p

1117
01:12:51,470 --> 01:12:53,990
not at all the desirable thing

1118
01:12:54,040 --> 01:12:56,070
this thing could easily be infinite

1119
01:12:56,080 --> 01:13:00,560
if that's the case he was zero at the point where he is nonzero then

1120
01:13:00,560 --> 01:13:02,680
this whole thing becomes infinite

1121
01:13:02,720 --> 01:13:03,800
OK so

1122
01:13:03,820 --> 01:13:07,600
it's already the the many reasons not to like it but at least the case

1123
01:13:08,830 --> 01:13:10,500
it's always positive

1124
01:13:10,530 --> 01:13:13,840
and zero only if p and q are equal

1125
01:13:13,860 --> 01:13:18,470
so so distance this distribution be

1126
01:13:18,930 --> 01:13:23,160
very useful in many contexts like entropy

1127
01:13:23,280 --> 01:13:26,600
when you're dealing with the product distribution

1128
01:13:26,610 --> 01:13:30,710
these are the product in here

1129
01:13:30,790 --> 01:13:33,600
the log on the product interact very nicely

1130
01:13:33,610 --> 01:13:38,630
so when you're dealing with product distribution system that to be very convenient functional form

1131
01:13:38,720 --> 01:13:43,150
in the same way as we saw the additivity property interest

1132
01:13:43,540 --> 01:13:47,730
and also this is closely related to maximum likelihood

1133
01:13:47,730 --> 01:13:49,300
if you have a bunch of data

1134
01:13:49,360 --> 01:13:51,020
and you have some some

1135
01:13:51,030 --> 01:13:54,500
parameterized model and you want to take the maximum likelihood model

1136
01:13:54,550 --> 01:13:57,620
making the maximum likelihood model is the same as making the model

1137
01:13:57,630 --> 01:14:01,120
it has the smallest scale divergence to the empirical distribution

1138
01:14:01,170 --> 01:14:04,350
that's the distribution where you just take the data points and assign them all the

1139
01:14:04,350 --> 01:14:05,480
same way

1140
01:14:07,110 --> 01:14:11,020
so this is so how is this related to entropy

1141
01:14:11,070 --> 01:14:14,760
to do that we look at the two to see how the here

1142
01:14:15,850 --> 01:14:17,730
is related to the entropy of

1143
01:14:17,760 --> 01:14:19,450
the distribution p

1144
01:14:19,460 --> 01:14:24,210
so that a random variable x has distribution of the KL divergence from p to

1145
01:14:24,210 --> 01:14:26,240
the uniform distribution

1146
01:14:26,260 --> 01:14:29,970
OK so that's just looking into the form of the KL divergence

1147
01:14:29,970 --> 01:14:32,440
that's the log over the uniform distribution

1148
01:14:32,450 --> 01:14:34,910
uniform is just one over everywhere

1149
01:14:34,950 --> 01:14:35,940
so there are

1150
01:14:35,950 --> 01:14:38,300
this possible outcome

1151
01:14:41,100 --> 01:14:46,570
again the log decomposes nicely and we get a lot of people as long as

1152
01:14:46,570 --> 01:14:51,040
and they never that and modelled and modelled and modelled and modelled

1153
01:14:51,040 --> 01:14:55,000
but in the end they figured out it's just not possible to model everything and

1154
01:14:55,000 --> 01:14:59,070
also you have a large blob of ontology which is not very efficient if you

1155
01:14:59,070 --> 01:15:02,960
ask queries and draw inferences and things like that so

1156
01:15:03,020 --> 01:15:06,500
the community shifted a little bit to the point of view that what you need

1157
01:15:06,500 --> 01:15:14,130
is not a whole monolithic large ontology covering everything but rather snow more handy modular

1158
01:15:14,280 --> 01:15:20,820
domain an application ontology is that are specific for certain domains for certain applications and

1159
01:15:20,820 --> 01:15:23,910
it is actually what is going on right now

1160
01:15:24,370 --> 01:15:28,720
but we also try is the standardisation so for example

1161
01:15:28,720 --> 01:15:32,800
hopefully upcoming in the next framework from the european commission there will be a thematic

1162
01:15:32,800 --> 01:15:37,430
network called knowledge that and one idea is to have some kind of standardisation bodies

1163
01:15:37,430 --> 01:15:44,610
also for ontology is that ensures certain quality insurers a certain syntactical correctness and stuff

1164
01:15:44,610 --> 01:15:49,170
like that so the the world is really ongoing

1165
01:15:49,200 --> 01:15:51,630
OK let's

1166
01:15:51,650 --> 01:15:56,460
communication what is it about and is quite nice picture

1167
01:15:56,460 --> 01:16:02,130
from the last century from one and richards which is the so-called meaning triangle maybe

1168
01:16:02,130 --> 01:16:07,090
you've heard about that which is pretty good shows what the problem in communication actually

1169
01:16:07,090 --> 01:16:09,800
so first of all you start

1170
01:16:09,910 --> 01:16:14,800
in the real world let's say with the form and we take jaguar

1171
01:16:14,800 --> 01:16:19,910
so if you read jaguar immediately in your mind you have some pictures of some

1172
01:16:20,000 --> 01:16:26,150
some ideas jaguar is you think about a certain thing so this form evokes the

1173
01:16:26,160 --> 01:16:31,090
concept in your head which refers to a so-called referent which would be a real

1174
01:16:31,090 --> 01:16:33,000
object in the world

1175
01:16:33,000 --> 01:16:37,350
so let's think about what is an jaguar for you but for some people course

1176
01:16:37,360 --> 01:16:41,930
it's the animal jaguar that you're thinking about but for some other people many from

1177
01:16:41,930 --> 01:16:45,330
the car industry thinking about the car jaguar

1178
01:16:45,350 --> 01:16:49,970
and then you see the problem if you if we are communicating i said jaguar

1179
01:16:49,970 --> 01:16:53,980
you think of something but i mean the other one so what they have a

1180
01:16:53,980 --> 01:16:58,350
problem so you have to this ambiguity which is one of the large problems also

1181
01:16:58,350 --> 01:17:00,060
from machine learning

1182
01:17:00,320 --> 01:17:05,330
and this morning it really shows the basic situation that you have to solve also

1183
01:17:05,330 --> 01:17:11,740
for computers so you when your agents communicate with each others you have some pre-defined

1184
01:17:11,740 --> 01:17:16,410
concepts into your ontology and you have to define the meaning for the concept do

1185
01:17:16,410 --> 01:17:21,980
you mean the animal or do you mean the car

1186
01:17:21,980 --> 01:17:24,490
and if you look at the current

1187
01:17:24,560 --> 01:17:29,650
state ontology if you have a lot of different views on ontology is and one

1188
01:17:29,650 --> 01:17:32,410
of the key purposes of my presentation

1189
01:17:32,460 --> 01:17:35,910
in the following will be to clarify a little bit what is actually meant by

1190
01:17:35,910 --> 01:17:41,560
the different views i would distinguish between friend and and back-end solutions but this is

1191
01:17:41,560 --> 01:17:43,370
a rather personal view

1192
01:17:43,480 --> 01:17:47,630
so if you look now for example you have things like topic maps is very

1193
01:17:47,640 --> 01:17:54,300
taxonomy is semantic networks extended ER models and predicate logic also on the one hand

1194
01:17:54,540 --> 01:17:56,410
for example the topic maps

1195
01:17:56,430 --> 01:18:02,700
quite established also an ISO standard quite nice applications this would be a rather front-end

1196
01:18:02,700 --> 01:18:04,240
solution because it can

1197
01:18:04,280 --> 01:18:08,520
you can navigate through and knowledge base you can still click-through

1198
01:18:08,570 --> 01:18:13,410
a topic hierarchies and find some documents on the other hand you have things like

1199
01:18:13,410 --> 01:18:19,480
extended ER models or predicate logic our preferred solution which is now in the back-end

1200
01:18:19,480 --> 01:18:25,590
side because you can draw inferences for example you have really precisely defined semantics which

1201
01:18:25,590 --> 01:18:27,460
helps there

1202
01:18:27,520 --> 01:18:28,630
so what typical

1203
01:18:28,650 --> 01:18:33,980
applications were on the front and of course you would like to provide navigational structures

1204
01:18:34,090 --> 01:18:40,220
you would like to support information retrieval for example query expansion with taxonomy is already

1205
01:18:40,220 --> 01:18:43,570
in the end you want to share their knowledge on the back and you have

1206
01:18:43,570 --> 01:18:52,480
the reason have consistency checking mediation enterprise application integration so these are the typical pass

1207
01:18:52,480 --> 01:18:53,850
if you talk about that

1208
01:18:53,890 --> 01:18:56,060
and would have a little bit into

1209
01:18:56,060 --> 01:19:02,350
the deep of that different topics and show you how relate taxonomy is very topic

1210
01:19:02,350 --> 01:19:05,330
maps and our view of ontology

1211
01:19:05,350 --> 01:19:10,480
let's start with a simple taxonomy well typically what it gives you

1212
01:19:10,590 --> 01:19:16,240
the segmentation of the classification and ordering of elements into the classification system according to

1213
01:19:16,240 --> 01:19:21,090
the relationships between each other so let's take a simple example here we start with

1214
01:19:21,110 --> 01:19:27,020
but generic object for us important use personal public document you know that students and

1215
01:19:27,020 --> 01:19:34,040
researchers isolated the person you know that there exists also doctoral students and phd students

1216
01:19:34,070 --> 01:19:39,300
an interesting topic for us would be semantics and you wouldn't maybe that logic frame

1217
01:19:39,300 --> 01:19:44,060
logic is our preferred representation language for ontology is and you know that ontology somehow

1218
01:19:44,060 --> 01:19:47,810
relates to the semantic so that's basically a taxonomy

1219
01:19:47,870 --> 01:19:50,430
what concepts

1220
01:19:50,560 --> 01:19:53,430
next step would be a thesaurus

1221
01:19:53,480 --> 01:19:58,960
so what you what is actually different is that it's typically for a specific domain

1222
01:19:58,980 --> 01:20:02,410
so define terminology for a certain domain

1223
01:20:02,430 --> 01:20:07,320
if you look a little bit closer the to graph with two primitives and you

1224
01:20:07,320 --> 01:20:11,520
have so-called two fixed relationships on the one hand you could say that a doctoral

1225
01:20:11,520 --> 01:20:16,240
student is synonym to a phd student on the other hand you could for example

1226
01:20:16,240 --> 01:20:22,370
say that athletic similar to ontology so these the two fixed relationships and originally the

1227
01:20:22,890 --> 01:20:25,980
this comes from the bibliography so that was the main

1228
01:20:26,170 --> 01:20:27,800
the original features

1229
01:20:27,870 --> 01:20:32,280
next step a little bit would be the so-called topic maps

1230
01:20:32,280 --> 01:20:36,320
reduce the amount of time to explain these terms

1231
01:20:36,330 --> 01:20:38,840
probably in all of these terms

1232
01:20:38,860 --> 01:20:42,750
last few blogsites blogger's blog posts reversal equal

1233
01:20:42,780 --> 01:20:46,840
chronologically ordered entries blagrove

1234
01:20:46,860 --> 01:20:49,300
number links and trackbacks

1235
01:20:49,320 --> 01:20:50,930
all of these so

1236
01:20:51,650 --> 01:20:56,320
this is the case this is the point everyone can publish

1237
01:20:56,330 --> 01:20:59,050
but not everyone will be heard

1238
01:21:00,840 --> 01:21:02,650
only very few people

1239
01:21:04,300 --> 01:21:05,650
and in these few

1240
01:21:05,660 --> 01:21:07,810
they can make a lot of money

1241
01:21:09,090 --> 01:21:12,980
all they have interests they can exert to their interests

1242
01:21:13,040 --> 01:21:16,640
so then many interesting questions to address

1243
01:21:16,650 --> 01:21:19,260
how to build traffic

1244
01:21:19,310 --> 01:21:21,820
how to find a niche online

1245
01:21:21,830 --> 01:21:23,900
and how to increase influence

1246
01:21:23,930 --> 01:21:25,320
there are many how to use

1247
01:21:25,320 --> 01:21:26,410
we can do

1248
01:21:26,430 --> 01:21:29,060
i was i think anything at all me

1249
01:21:29,070 --> 01:21:31,800
there's even though there's is a site to the world

1250
01:21:31,810 --> 01:21:36,070
the try to based on the blog posts right to tell you it's a female

1251
01:21:37,970 --> 01:21:42,750
female blogger on milk is you just need to find

1252
01:21:42,800 --> 01:21:46,260
this kind of application then you can

1253
01:21:46,330 --> 01:21:47,890
establish a company

1254
01:21:47,950 --> 01:21:49,320
to make money

1255
01:21:49,320 --> 01:21:51,950
you use

1256
01:21:51,970 --> 01:21:56,080
you don't know that's why they do it they don't know based on because it's

1257
01:21:56,080 --> 01:22:00,600
an anonymous right usually but so then how can you tell a blog post is

1258
01:22:00,600 --> 01:22:03,950
written by a male or female

1259
01:22:03,970 --> 01:22:06,320
or computer it's possible

1260
01:22:06,320 --> 01:22:10,870
spam is spam we want to tell us spam blogs

1261
01:22:10,880 --> 01:22:13,450
but basically i hope i have convinced you

1262
01:22:13,500 --> 01:22:15,750
it's a fertile research domain

1263
01:22:15,760 --> 01:22:23,290
because fullprofessor we will or interesting researcher

1264
01:22:23,290 --> 01:22:25,080
OK this is basically

1265
01:22:25,120 --> 01:22:26,190
an example

1266
01:22:26,210 --> 01:22:29,540
this blog site

1267
01:22:29,650 --> 01:22:34,550
we have a blog post this blogger

1268
01:22:35,430 --> 01:22:36,890
types of blocks

1269
01:22:36,900 --> 01:22:38,650
in the early days

1270
01:22:38,650 --> 01:22:43,650
blogs blogs basically we note spire is one person just serve

1271
01:22:45,080 --> 01:22:47,180
his or her

1272
01:22:47,210 --> 01:22:51,780
opinions experiences or things but things change

1273
01:22:51,780 --> 01:22:54,990
after people can comment on

1274
01:22:56,580 --> 01:22:59,450
we have like individual versus community

1275
01:22:59,470 --> 01:23:04,130
nowadays you can see individual blog sites and community blog sites

1276
01:23:04,160 --> 01:23:07,500
these are the different

1277
01:23:07,550 --> 01:23:11,910
so you have like a single of other people can only comment

1278
01:23:11,930 --> 01:23:15,110
when we will see how monte of

1279
01:23:16,390 --> 01:23:20,010
and what you can do is like many people can

1280
01:23:20,880 --> 01:23:24,580
start a new thread and to discuss

1281
01:23:24,640 --> 01:23:29,460
a lot of things to get your comments it it's not necessary only the only

1282
01:23:29,460 --> 01:23:31,600
single off can control

1283
01:23:33,910 --> 01:23:35,850
so the we also

1284
01:23:35,850 --> 01:23:38,630
you can see the regulated versus anonymous

1285
01:23:39,450 --> 01:23:42,890
blog sites

1286
01:23:42,900 --> 01:23:47,310
so how about individual blog sites and the community blog sites there are different

1287
01:23:47,960 --> 01:23:52,920
many ways

1288
01:23:52,980 --> 01:23:55,100
so now blogsphere

1289
01:23:55,190 --> 01:24:01,170
we will talk about like this it's basically it's complex social networks

1290
01:24:01,190 --> 01:24:04,310
OK why because we have these about

1291
01:24:04,310 --> 01:24:08,150
in degree and out degree have links have links

1292
01:24:08,160 --> 01:24:10,230
you can cite other people

1293
01:24:10,240 --> 01:24:12,120
and you can also link

1294
01:24:12,160 --> 01:24:17,140
two you can cite other people's right people can cite to so then having degrees

1295
01:24:17,140 --> 01:24:20,780
and degrees you consider but this is you have to

1296
01:24:21,940 --> 01:24:24,150
so these are

1297
01:24:24,150 --> 01:24:29,790
the rules you have those here so that means the directional

1298
01:24:33,360 --> 01:24:37,290
so we have also the friendship networks versus blogosphere

1299
01:24:37,370 --> 01:24:40,620
friendship networks versus france that

1300
01:24:40,640 --> 01:24:42,450
not exactly the same

1301
01:24:43,350 --> 01:24:44,840
these are the

1302
01:24:45,310 --> 01:24:48,370
i won't go through one by one but they are

1303
01:24:48,420 --> 01:24:50,910
these are the differences between them

1304
01:24:54,690 --> 01:25:00,950
here so you see there's overlap you will give you some examples

1305
01:25:01,000 --> 01:25:05,150
and if you have done the research in social networks and computer science

1306
01:25:05,170 --> 01:25:11,810
it's very likely that you use the citation networks koko offer should networks

1307
01:25:11,850 --> 01:25:15,230
so this is also different from blogsphere

1308
01:25:15,250 --> 01:25:17,970
we listed these like

1309
01:25:17,980 --> 01:25:19,640
citation links

1310
01:25:19,640 --> 01:25:24,310
thank you for DBLP that's one of the dataset you can use to do research

1311
01:25:24,320 --> 01:25:26,690
so you have strict notion of links

1312
01:25:26,760 --> 01:25:29,470
the people cite to what they refer to

1313
01:25:30,740 --> 01:25:33,780
blogs links are

1314
01:25:34,700 --> 01:25:36,170
and often missing

1315
01:25:36,810 --> 01:25:39,330
and then for social networks

1316
01:25:39,330 --> 01:25:51,040
on the other

1317
01:25:55,930 --> 01:25:57,000
you know

1318
01:26:07,630 --> 01:26:15,060
that's not what i would call this a with

1319
01:26:17,830 --> 01:26:23,980
i think we

1320
01:26:23,990 --> 01:26:26,150
if at all

1321
01:26:26,160 --> 01:26:30,250
by two

1322
01:26:34,640 --> 01:26:42,110
all these things that make you of you noise

1323
01:26:42,650 --> 01:26:46,050
it means that there you know

1324
01:26:50,420 --> 01:27:00,100
he my mind

1325
01:27:02,970 --> 01:27:08,060
by that we use

1326
01:27:13,610 --> 01:27:14,280
two weeks

1327
01:27:14,420 --> 01:27:18,230
this is one one

1328
01:27:24,760 --> 01:27:28,020
the like

1329
01:27:33,240 --> 01:27:36,780
he said we have on

1330
01:27:36,790 --> 01:27:40,590
and they use see

1331
01:27:50,010 --> 01:27:53,150
what happened

1332
01:27:58,850 --> 01:28:02,080
one of us

1333
01:28:02,170 --> 01:28:05,130
you get more and more

1334
01:28:05,300 --> 01:28:06,880
what comes

1335
01:28:11,290 --> 01:28:12,100
you know one

1336
01:28:16,250 --> 01:28:18,690
sure sure

1337
01:28:21,190 --> 01:28:25,710
he asked what i

1338
01:28:29,650 --> 01:28:33,130
more less

1339
01:28:33,260 --> 01:28:38,960
by that i mean that we

1340
01:28:38,970 --> 01:28:42,830
it is not

1341
01:28:42,850 --> 01:28:45,080
how do you

1342
01:28:58,910 --> 01:29:09,360
what is doing

1343
01:29:51,600 --> 01:29:54,520
i believe that

1344
01:29:59,530 --> 01:30:03,270
so you know

1345
01:30:03,280 --> 01:30:05,730
you can see

1346
01:30:08,300 --> 01:30:11,060
he was

1347
01:30:11,100 --> 01:30:12,640
would that be

1348
01:30:14,940 --> 01:30:19,750
you can see by

1349
01:30:19,790 --> 01:30:26,270
the other thing is the

1350
01:30:26,280 --> 01:30:28,040
what is all

1351
01:30:31,590 --> 01:30:34,050
this happens

1352
01:30:34,070 --> 01:30:36,390
we need a little

1353
01:30:38,420 --> 01:30:41,590
he e

1354
01:30:45,910 --> 01:30:47,480
can you

1355
01:30:48,460 --> 01:30:50,200
he really

1356
01:31:02,490 --> 01:31:03,710
he she

1357
01:31:03,730 --> 01:31:10,070
what find

1358
01:31:12,730 --> 01:31:16,350
and i

1359
01:31:17,940 --> 01:31:19,930
i think the

1360
01:31:33,080 --> 01:31:37,320
it is so we can see

1361
01:31:37,370 --> 01:31:42,490
what you all

1362
01:31:45,870 --> 01:31:52,250
on line that you know where you have

1363
01:31:52,250 --> 01:31:54,760
why i

1364
01:31:54,800 --> 01:31:57,500
equals one one

1365
01:31:57,520 --> 01:32:05,070
what would this feature function be useful for

1366
01:32:08,350 --> 01:32:11,780
you will learn that this feature function is going to be a negative weight

1367
01:32:11,870 --> 01:32:17,200
and what that means is that you don't have to hide the road

1368
01:32:19,050 --> 01:32:21,020
but much

1369
01:32:21,030 --> 01:32:26,080
you won't be able to learn any patterns about about three hyphens on on hyphens

1370
01:32:26,080 --> 01:32:27,880
in world

1371
01:32:38,620 --> 01:32:42,270
so the part of speech labeling it would be it would it would be quite

1372
01:32:42,270 --> 01:32:45,900
useful to have a low level feature functions like

1373
01:32:45,910 --> 01:32:48,740
proper noun proper nouns proper nouns

1374
01:32:49,610 --> 01:32:52,950
that's unfortunately not allowed by this framework

1375
01:32:55,490 --> 01:33:01,850
it's not alone is not allowed by standards

1376
01:33:01,870 --> 01:33:05,550
it is not allowed by standard linear chain crfs

1377
01:33:05,570 --> 01:33:12,490
and the reason is not allowed is because they don't exist algorithms for that case

1378
01:33:12,490 --> 01:33:15,620
it's it's allowed theoretically it allows mathematically

1379
01:33:17,830 --> 01:33:22,270
but then decided our will break down

1380
01:33:22,330 --> 01:33:33,230
so if if you really wanted to much

1381
01:33:33,240 --> 01:33:36,770
if it was really important to you to have patterns like for like

1382
01:33:36,780 --> 01:33:38,970
proper noun proper nouns proper nouns

1383
01:33:39,000 --> 01:33:40,990
you could

1384
01:33:41,000 --> 01:33:43,840
you could try developing your own algorithms

1385
01:33:45,080 --> 01:33:46,190
you could try

1386
01:33:46,200 --> 01:33:49,140
creating some sort of multiscale model

1387
01:33:49,160 --> 01:33:52,240
so that much

1388
01:33:52,530 --> 01:33:57,510
so that you could group words into groups and then have a CRF that refer

1389
01:33:57,540 --> 01:34:01,250
to just pairs of group as of consecutive groups of words

1390
01:34:01,260 --> 01:34:04,440
or and

1391
01:34:04,450 --> 01:34:09,500
actually both of those have been both of those have been tried their papers in

1392
01:34:09,500 --> 01:34:15,710
the literature about both those approaches you you can relax this assumption of two neighbouring

1393
01:34:15,750 --> 01:34:22,170
one and in fact actually the arms of tomorrow i'm going to be presenting a

1394
01:34:22,170 --> 01:34:23,590
paper at the conference

1395
01:34:23,670 --> 01:34:24,890
which is about

1396
01:34:24,910 --> 01:34:28,990
an extension of conditional random fields

1397
01:34:29,000 --> 01:34:33,270
and we actually have a comparison in the two method

1398
01:34:33,280 --> 01:34:37,340
that was actually published by zoubin ghahramani whose

1399
01:34:37,350 --> 01:34:42,000
professor at cambridge university where they have

1400
01:34:42,650 --> 01:34:48,120
they have an extension of linear chain conditional random field to allow for more triples

1401
01:34:48,160 --> 01:34:54,250
and on this particular application it's the best performing method because it really is a

1402
01:34:54,250 --> 01:34:56,400
lot of information in the triples

1403
01:34:56,410 --> 01:34:57,530
but the

1404
01:34:57,540 --> 01:34:59,970
the algorithms for that case

1405
01:34:59,980 --> 01:35:03,780
much less tractable

1406
01:35:03,940 --> 01:35:07,720
a significant

1407
01:35:07,730 --> 01:35:11,790
and i think that would be an additional conducting going triples the portables i think

1408
01:35:11,790 --> 01:35:13,020
essentially the

1409
01:35:13,030 --> 01:35:16,770
the algorithm is exponential in the number of adjacent

1410
01:35:17,140 --> 01:35:18,740
labels they allow

1411
01:35:18,790 --> 01:35:23,600
so actually linear if you don't pay attention if you don't

1412
01:35:24,960 --> 01:35:30,900
pairs of labels and quadratic the labels cubic if you allow triples of labels

1413
01:35:33,190 --> 01:35:39,610
that's what i was just wondering as i was just saying that i'm not sure

1414
01:35:39,610 --> 01:35:47,660
if if there's a paper in the letter explicitly gives the generalized algorithm for any

1415
01:35:47,660 --> 01:35:50,690
fixed number of adjacent one because the

1416
01:35:50,700 --> 01:35:56,110
this is the this this work that i'm thinking of by zoubin ghahramani

1417
01:35:56,120 --> 01:36:01,860
it may be it's more general than just linear chain crfs

1418
01:36:01,860 --> 01:36:04,870
because it also applies to non-linear crfs

1419
01:36:04,880 --> 01:36:08,990
and so i think maybe a gap in the literature that somebody should write a

1420
01:36:08,990 --> 01:36:10,070
paper that

1421
01:36:10,330 --> 01:36:15,890
that gives the algorithms that are just for linear chain crfs but for any number

1422
01:36:15,910 --> 01:36:18,260
of adjacent labels i think that might be

1423
01:36:18,280 --> 01:36:20,550
what we're doing

1424
01:36:30,720 --> 01:36:32,330
i think

1425
01:36:34,800 --> 01:36:38,410
so the standard algorithms require adjacency

1426
01:36:38,450 --> 01:36:42,420
i think there might be non-standard algorithms that actually have the same time complexity they

1427
01:36:42,420 --> 01:36:44,210
don't require adjacency

1428
01:36:44,250 --> 01:36:48,430
but the standard that would require adjacency

1429
01:36:50,750 --> 01:36:54,650
i guess that's what should

1430
01:36:54,660 --> 01:36:58,640
i get to the next month

1431
01:36:58,650 --> 01:37:03,210
and as you know we have another break coming up doing

1432
01:37:06,150 --> 01:37:08,000
and informant yes so

1433
01:37:08,050 --> 01:37:09,690
and let me

1434
01:37:10,040 --> 01:37:15,500
move on to more talking about what are the two central problems that i'm going

1435
01:37:15,500 --> 01:37:18,520
to provide algorithms for solving

1436
01:37:30,720 --> 01:37:35,240
so the probability of life given x and w

1437
01:37:35,300 --> 01:37:37,170
it is

1438
01:37:38,090 --> 01:37:40,880
xe of expert w

1439
01:37:41,900 --> 01:37:43,740
on the numerator

1440
01:37:43,800 --> 01:37:48,080
exponent exponential of some of the j

1441
01:37:50,560 --> 01:37:52,490
wj j

1442
01:37:52,500 --> 01:37:54,380
x y

1443
01:37:55,190 --> 01:37:56,890
to make a predictions

1444
01:37:56,900 --> 01:38:07,840
why that is the argmax

1445
01:38:07,910 --> 01:38:09,240
over y

1446
01:38:13,000 --> 01:38:16,200
of some of the j

1447
01:38:16,210 --> 01:38:17,940
wj fj

1448
01:38:17,950 --> 01:38:19,330
x y

1449
01:38:21,980 --> 01:38:25,290
if we want to use one of these models to predict y

1450
01:38:25,350 --> 01:38:28,700
then we're going to plug in the x and look for the y has highest

1451
01:38:29,900 --> 01:38:34,440
and the y has highest over years the y has the biggest value for the

1452
01:38:36,470 --> 01:38:40,700
the denominator is constant for all y so when we went away when we want

1453
01:38:40,700 --> 01:38:44,970
to find the argmax the probability we just need to know the

1454
01:38:45,010 --> 01:38:49,750
argmax the numerator and in fact is one more simplification that i can make on

1455
01:38:49,750 --> 01:38:52,800
this argmax

1456
01:38:53,130 --> 01:38:59,530
what can i simplify i don't actually need to find the argmax of this i

1457
01:38:59,530 --> 01:39:02,820
can find a argmaxa something a little bit simpler

1458
01:39:02,840 --> 01:39:07,440
i don't need the exponent because it's a monotonic function so

1459
01:39:07,520 --> 01:39:10,760
i need to make it prediction need to find the argmax over y

1460
01:39:10,760 --> 01:39:11,820
function that the

1461
01:39:12,760 --> 01:39:15,490
this is the prior is too strong

1462
01:39:15,560 --> 01:39:16,810
and if you're interested

1463
01:39:17,200 --> 01:39:18,990
in making probabilistic

1464
01:39:19,030 --> 01:39:23,400
prediction then you're interested in how your error bars behave then you should use models

1465
01:39:23,400 --> 01:39:28,920
like that i should be using models that are and here we are going to

1466
01:39:28,920 --> 01:39:34,000
be nonparametric limit right we're saying well you know that they lots of things could

1467
01:39:34,000 --> 01:39:35,780
could be happening potentially right

1468
01:39:35,800 --> 01:39:37,070
and this sort of a miracle

1469
01:39:37,090 --> 01:39:40,890
that the amount of computation we need to do is is actually the same as

1470
01:39:40,900 --> 01:39:44,350
if only treating these these weekly basis functions

1471
01:39:45,050 --> 01:39:48,870
but as we saw in the slide from yesterday we get this nice error bars

1472
01:39:48,870 --> 01:39:54,330
that increase when we move away from the dataset precisely because this interpretation that with

1473
01:39:54,330 --> 01:39:58,740
these bumps that are living in the feature space and out here then not constrained

1474
01:39:58,740 --> 01:40:00,080
by the data

1475
01:40:00,140 --> 01:40:07,370
the the question about here

1476
01:40:12,170 --> 01:40:16,710
well so the joke so the question is you know doesn't really make a difference

1477
01:40:16,710 --> 01:40:20,980
whether using zero with with with high confidence of their with very low confidence i

1478
01:40:20,980 --> 01:40:25,840
think it makes in many cases makes a huge difference right let's say you're say

1479
01:40:25,870 --> 01:40:29,430
you're using the model to two

1480
01:40:29,450 --> 01:40:33,400
to predict what you should be making some financial from transactions on like that in

1481
01:40:33,400 --> 01:40:36,860
one case it was saying i'm absolutely sure what's going to happen

1482
01:40:36,890 --> 01:40:38,460
so that would give you

1483
01:40:38,500 --> 01:40:40,560
that will give you confidence that

1484
01:40:40,580 --> 01:40:44,120
that's what the model is predicting is actually also going to happen in the other

1485
01:40:44,120 --> 01:40:45,510
case it could say

1486
01:40:45,510 --> 01:40:50,490
well i i'm i'm neutral well actually you know anything could happen right that seems

1487
01:40:50,490 --> 01:40:54,710
to be a very different very different situation

1488
01:40:54,720 --> 01:40:56,240
so typically

1489
01:40:56,280 --> 01:40:59,820
people focus a lot on the mean predictive distribution but i think actually be

1490
01:40:59,870 --> 01:41:01,660
the answer is typically

1491
01:41:01,710 --> 01:41:05,680
more realistically the whole distribution and you should worry about if

1492
01:41:05,740 --> 01:41:07,350
different put things are possible

1493
01:41:07,380 --> 01:41:14,210
then you know this is this is usually an interesting interesting thing to know about

1494
01:41:14,220 --> 01:41:16,060
yes it does mean we

1495
01:41:16,090 --> 01:41:20,140
and user should read

1496
01:41:20,160 --> 01:41:22,470
with binary functions because

1497
01:41:22,490 --> 01:41:24,760
most countries of the

1498
01:41:24,840 --> 01:41:26,960
that's correct

1499
01:41:27,820 --> 01:41:31,990
so always use infinite dimensional models

1500
01:41:31,990 --> 01:41:35,370
OK in this in this case for free flight

1501
01:41:35,390 --> 01:41:36,530
because the same

1502
01:41:36,530 --> 01:41:39,650
the competition to compute with these things as it does

1503
01:41:39,670 --> 01:41:43,330
to compete with the with the bad counterpart

1504
01:41:43,380 --> 01:41:49,890
OK so so so some people actually asked about this site

1505
01:41:52,610 --> 01:41:54,270
well the

1506
01:41:55,560 --> 01:42:02,350
o but here to to solve this problem i only need to store nine numbers

1507
01:42:03,380 --> 01:42:07,640
actually if i recognise that my matrix is symmetric i only need half of nine

1508
01:42:07,660 --> 01:42:11,290
but irony i only need to solve a three by three system here

1509
01:42:12,300 --> 01:42:16,050
there's no need to represent all this stuff

1510
01:42:16,070 --> 01:42:18,990
like the gaussianprocess formulation will be telling me

1511
01:42:19,000 --> 01:42:23,920
anything i could possibly want to know about the distribution

1512
01:42:29,010 --> 01:42:34,040
OK so so there so so

1513
01:42:34,070 --> 01:42:38,680
so so long to models that are currently being used in lots of different fields

1514
01:42:38,700 --> 01:42:41,580
are actually special cases of gas and process

1515
01:42:42,060 --> 01:42:48,090
so so very large neural networks are exactly the reason why i got to think

1516
01:42:48,090 --> 01:42:51,670
about casting process in the first place so radford neal who was in the lower

1517
01:42:51,670 --> 01:42:56,950
where was doing my phd he did his phd on doing inference using monte carlo

1518
01:42:56,950 --> 01:43:00,380
techniques on very large neural networks and had a sentence somewhere in his in his

1519
01:43:01,010 --> 01:43:03,200
in his thesis saying when the

1520
01:43:03,200 --> 01:43:07,820
when the networks become infinitely large this actually tends to process and it might be

1521
01:43:07,820 --> 01:43:11,160
easier to do inference if you think about it as the gaussianprocess process so actually

1522
01:43:11,160 --> 01:43:15,010
chris williams myself look at that sentence and try to figure out what what that

1523
01:43:15,820 --> 01:43:20,540
what i got what i got imposes was we actually came to gaston processes sort

1524
01:43:20,540 --> 01:43:26,060
of backwards it turns out that it's very easy to show how big neural networks

1525
01:43:26,400 --> 01:43:29,600
tend to go process so but a lots of all the models that have been

1526
01:43:30,020 --> 01:43:33,090
used a lot so one in this is spinal also just go through this very

1527
01:43:33,110 --> 01:43:37,250
quickly if you don't know what's finals is then it doesn't matter

1528
01:43:37,250 --> 01:43:41,230
so spineless are our models where you're trying to look at it trying to find

1529
01:43:41,240 --> 01:43:42,600
the the

1530
01:43:42,650 --> 01:43:47,620
they functioned the function to fit data again to find finer function that minimizes this

1531
01:43:47,870 --> 01:43:52,470
loss functions so square difference between what the what the function is is predicting and

1532
01:43:52,470 --> 01:43:57,060
the and the observation and then as the regularisation term here which is some regularisation

1533
01:43:57,060 --> 01:44:02,320
constant lambda times the integral of the second squared second derivative of the function quite

1534
01:44:02,320 --> 01:44:06,290
so it's penalizing the function before curving a lot

1535
01:44:06,490 --> 01:44:12,060
it turns out that the solution to to this optimisation problem let's not get too

1536
01:44:12,060 --> 01:44:16,800
mathematical about about in some some class of functions you can you can solve this

1537
01:44:16,800 --> 01:44:22,910
very variational and the solution is it's called cubic smoothing spine so so there is

1538
01:44:22,910 --> 01:44:23,980
a piecewise

1539
01:44:24,000 --> 01:44:27,100
cubic polynomial and the pieces are

1540
01:44:27,120 --> 01:44:32,220
are from between the data points here in in in one case

1541
01:44:33,590 --> 01:44:38,910
and in one dimensional actually have is not that lie between zero and one and

1542
01:44:38,910 --> 01:44:45,140
you have a piecewise polynomials cubic polynomial inside each of the difference between the two

1543
01:44:45,140 --> 01:44:48,070
points and these models have been used a lot

1544
01:44:48,120 --> 01:44:52,740
it turns out that this is actually also gaussianprocess so i can construct the process

1545
01:44:52,950 --> 01:44:56,360
in this way here so it looks a lot like what we did before so

1546
01:44:56,360 --> 01:45:01,710
the functions are now alpha plus beta x so this is just a linear contribution

1547
01:45:02,410 --> 01:45:04,830
and then we have then we have again

1548
01:45:04,880 --> 01:45:09,620
a bunch of functions looking at the limit as we take more and more these

1549
01:45:09,620 --> 01:45:15,350
functions this function this bracket with the pos means means the positive part so

1550
01:45:15,360 --> 01:45:20,680
you have zero inside the bracket if the if the if the bracket has a

1551
01:45:20,680 --> 01:45:24,000
negative number of you just get the number which is inside the brackets so we

1552
01:45:24,000 --> 01:45:29,130
are adding together the basis functions that are little ramps i zero until some point

1553
01:45:29,130 --> 01:45:31,700
i iowa and and then they start ramping up

1554
01:45:31,700 --> 01:45:36,670
OK and we just give gas in independent components to those and some of the

1555
01:45:36,670 --> 01:45:37,930
whole thing up

1556
01:45:37,950 --> 01:45:44,200
and normalized so that will have a reasonable limit when n goes to infinity

1557
01:45:44,210 --> 01:45:46,620
and then the interesting part

1558
01:45:46,650 --> 01:45:50,200
we already know is to compute the covariance function and you have to do so

1559
01:45:50,430 --> 01:45:53,720
first we get the covariance function of a linear function we already computed that and

1560
01:45:53,720 --> 01:45:56,460
then we get the the

1561
01:45:56,460 --> 01:46:01,230
the there some of these products again we only get the these these these terms

1562
01:46:03,270 --> 01:46:08,040
you don't get across terms again because the the the the gas and random variables

1563
01:46:08,040 --> 01:46:13,600
when independent for the different contributions and you can you can solve this cylindrical and

1564
01:46:13,600 --> 01:46:17,550
you get something which is here gamma is just like a constant here you get

1565
01:46:17,550 --> 01:46:21,220
something which is the which is a cubic polynomial

1566
01:46:21,240 --> 01:46:27,340
OK it's a cubic polynomial but it involves something like the minimum of x and

1567
01:46:27,660 --> 01:46:29,140
ten x ten x prime

1568
01:46:29,170 --> 01:46:30,580
so you can sort of see

1569
01:46:30,590 --> 01:46:34,800
if this is the covariance function and remember the predictions you get from a gaussianprocess

1570
01:46:34,800 --> 01:46:36,060
the mean prediction

1571
01:46:36,080 --> 01:46:39,540
it's just an example

1572
01:46:39,580 --> 01:46:46,370
common x one is the training cases times k nearest times the observations y

1573
01:46:46,380 --> 01:46:48,420
right and that's the mean prediction

1574
01:46:48,520 --> 01:46:54,450
you mean prediction so the mean prediction look like this so the mean prediction you

1575
01:46:54,450 --> 01:46:58,420
so you give a matrix of distances i can recover the matrix of inner products

1576
01:46:58,420 --> 01:47:01,530
if this distance matrix of rules from

1577
01:47:01,630 --> 01:47:04,710
a set of vectors in a euclidean space

1578
01:47:04,790 --> 01:47:10,080
and once make a matrix of inner products how do i find the vectors

1579
01:47:10,180 --> 01:47:12,210
simply by looking at

1580
01:47:12,250 --> 01:47:17,050
dikin vectors of this matrix

1581
01:47:17,080 --> 01:47:21,490
so you do the next step of embedding

1582
01:47:22,460 --> 01:47:26,540
essentially noticing that the matrix of inner product is going to be

1583
01:47:26,550 --> 01:47:30,240
technically going be positive semi definite so to be a little careful you have to

1584
01:47:30,240 --> 01:47:33,330
actually do you could do the spectral factorisation carefully

1585
01:47:33,670 --> 01:47:37,680
keep the orthogonal spaces in those spaces

1586
01:47:37,700 --> 01:47:41,420
cricket this is done over all non-zero igon values as follows

1587
01:47:41,510 --> 01:47:44,080
and i and so this is

1588
01:47:44,080 --> 01:47:45,160
a map

1589
01:47:45,170 --> 01:47:47,130
that will take you from

1590
01:47:47,170 --> 01:47:50,180
for any x from one in one through n

1591
01:47:50,200 --> 01:47:53,290
you get a map CX

1592
01:47:53,340 --> 01:47:56,530
like this

1593
01:47:56,540 --> 01:47:58,300
as written the

1594
01:47:58,320 --> 01:47:59,830
and you can check

1595
01:47:59,890 --> 01:48:01,580
the this map

1596
01:48:01,580 --> 01:48:05,780
well actually satisfy the inner product and satisfy

1597
01:48:05,780 --> 01:48:07,390
the distance matrix

1598
01:48:07,420 --> 01:48:11,460
so the way the multidimensional scaling works is that you start with set of distances

1599
01:48:11,530 --> 01:48:15,600
the distance matrix you go from there that to candidate set of inner products

1600
01:48:16,030 --> 01:48:19,100
and you go from the candidate set of inner products was set of vectors is

1601
01:48:19,200 --> 01:48:25,640
consistent with that inner product matrix

1602
01:48:25,700 --> 01:48:29,470
so what people did they applied this algorithm

1603
01:48:29,490 --> 01:48:30,660
a lot of data

1604
01:48:30,670 --> 01:48:33,280
OK so here are many pictures

1605
01:48:33,380 --> 01:48:35,080
and this is the picture

1606
01:48:36,470 --> 01:48:39,260
i guess the hand

1607
01:48:39,290 --> 01:48:41,660
where the wrist is rotating

1608
01:48:44,210 --> 01:48:48,500
the finger is also extended scientific and see this very well but each of these

1609
01:48:48,500 --> 01:48:53,720
pictures is a picture of the hand and the pictures differ from each other along

1610
01:48:53,720 --> 01:48:54,970
two dimensions

1611
01:48:55,010 --> 01:48:58,670
since some of those pictures the fingers are extended

1612
01:48:58,670 --> 01:49:02,040
and in some of those pictures the wrist is rotated

1613
01:49:02,180 --> 01:49:05,710
so there are two degrees of freedom in the natural degrees of freedom in the

1614
01:49:05,710 --> 01:49:10,970
data pictures corresponding to rotate addressed in pictures corresponding to extended finger

1615
01:49:11,340 --> 01:49:15,290
and basically you collect a lot of these pictures lot of images

1616
01:49:15,380 --> 01:49:17,990
you apply the size of isomap technique

1617
01:49:18,040 --> 01:49:24,160
and you can now embed this data every picture into two-dimensional space

1618
01:49:24,170 --> 01:49:26,630
by asking for

1619
01:49:26,920 --> 01:49:29,210
two dimensional embedding

1620
01:49:29,220 --> 01:49:34,330
once you embedded every such picture in two dimensions space you can look at

1621
01:49:34,380 --> 01:49:38,950
every point in this two-dimensional space in every point in this two-dimensional space corresponds to

1622
01:49:38,950 --> 01:49:40,580
a picture

1623
01:49:40,670 --> 01:49:43,040
and this is what those

1624
01:49:43,080 --> 01:49:46,490
pictures look like you see that you actually recover

1625
01:49:46,500 --> 01:49:54,750
the wrist rotation and the finger extension

1626
01:49:59,160 --> 01:50:06,580
suppose all the data lived actually on manifold in this manifold was flat

1627
01:50:06,600 --> 01:50:09,380
here is the statement that is actually true

1628
01:50:10,580 --> 01:50:12,510
flat so it

1629
01:50:12,750 --> 01:50:19,760
falls flat manifold which is isometric to a convex domain in rn

1630
01:50:19,790 --> 01:50:25,910
there's another algorithm called his united methodist talk about

1631
01:50:26,290 --> 01:50:31,290
call local tangent space alignment didn't talk about and all of them are trying to

1632
01:50:31,290 --> 01:50:33,910
do the same circle of things OK

1633
01:50:34,210 --> 01:50:38,410
what data living on the manifold can you actually on rules

1634
01:50:38,420 --> 01:50:43,820
this manifold can you actually we embed the data into a low dimensional space and

1635
01:50:43,820 --> 01:50:48,580
then you can ask whether this really embedding is correct are isometric

1636
01:50:48,750 --> 01:50:52,660
in some sense and some of these statements may be true

1637
01:50:52,670 --> 01:50:58,430
for some of these other things

1638
01:50:58,460 --> 01:51:03,170
here is another algorithm which people use color locos

1639
01:51:03,210 --> 01:51:05,030
locally linear embedding

1640
01:51:05,040 --> 01:51:07,580
so how does this algorithm looks so

1641
01:51:07,580 --> 01:51:12,030
you could also say that that truck is also class and you could say that

1642
01:51:12,030 --> 01:51:16,820
truck is the of metal you call that's the second example you could say that

1643
01:51:16,820 --> 01:51:18,530
the measure he could

1644
01:51:18,560 --> 01:51:22,730
as the domain is registered to a person which would be the range

1645
01:51:22,730 --> 01:51:27,710
and you could say that for example something is owned by a person or something

1646
01:51:27,710 --> 01:51:32,100
else and this would be the property of registered to these

1647
01:51:32,120 --> 01:51:37,230
i mean the syntactical details sure but just to give you a short glimpse of

1648
01:51:38,770 --> 01:51:42,860
some conclusions

1649
01:51:42,880 --> 01:51:47,840
why is it so important that except that RDF is serialised in XML first of

1650
01:51:47,840 --> 01:51:52,320
all when it gives you the flexibility

1651
01:51:53,060 --> 01:51:58,790
to embed your metadata into XML documents and if you have an RDF aware agent

1652
01:51:59,100 --> 01:52:02,620
he can make a little bit more with the document so that's the principle idea

1653
01:52:02,620 --> 01:52:08,000
behind it but still it's the next step up from so-called plain XML because you

1654
01:52:08,000 --> 01:52:14,290
have from our perspective small ontological commitment to the modelling primitives and i introduced the

1655
01:52:14,290 --> 01:52:19,580
basic ones and it's and it is possible to define further vocabulary that you would

1656
01:52:19,580 --> 01:52:22,790
need for that

1657
01:52:22,790 --> 01:52:28,790
but what is still missing you no precisely described meaning that is still missing in

1658
01:52:28,790 --> 01:52:32,790
RDF and RDF scheming schema and you have no inference model and this is one

1659
01:52:32,790 --> 01:52:36,860
of the key aspects if you have more complex problems which you would like to

1660
01:52:36,860 --> 01:52:40,600
solve then typically you have a large set of actions and this is still not

1661
01:52:40,600 --> 01:52:46,250
included in RDF and RDF schema so the next step from RDF and RDF schema

1662
01:52:46,250 --> 01:52:51,030
is really more ontology vocabulary then you would for example

1663
01:52:51,210 --> 01:52:56,380
i define an inference model and this is also ongoing work actually

1664
01:52:56,620 --> 01:53:01,560
meant to give you an impression what is already existing and where people working on

1665
01:53:01,560 --> 01:53:08,560
so you think you you're you not the request commendation the XML is already a

1666
01:53:08,560 --> 01:53:15,120
standard RDF and RDF schema is also a standard ontology vocabulary is not standard called

1667
01:53:15,120 --> 01:53:19,340
but this is a short tutorial so why not introduce the before standard you but

1668
01:53:19,340 --> 01:53:22,340
if you are interested of course you will find a lot of presentations and stuff

1669
01:53:22,480 --> 01:53:26,430
the web but this is really the next step on top of RDF and RDF

1670
01:53:26,430 --> 01:53:32,560
schema and work in progress the so-called allegedly so there you want to add more

1671
01:53:32,990 --> 01:53:38,540
capabilities so in the ontology vocabulary you only have a simple inference model and allegedly

1672
01:53:38,770 --> 01:53:43,730
would like to add even more inference capabilities and if you're familiar with description logics

1673
01:53:43,730 --> 01:53:47,320
and frame logic then you have a clear picture of what is needed there and

1674
01:53:47,320 --> 01:53:49,880
on top of that but this is still

1675
01:53:49,950 --> 01:53:53,950
a bit more in the future i would say is the so-called proven trust layer

1676
01:53:53,950 --> 01:53:58,530
but especially for business applications of course this will be one of the crucial layers

1677
01:53:58,570 --> 01:54:00,670
but beginning from the bottom

1678
01:54:00,770 --> 01:54:05,730
the semantic layer is now really becoming reality by standards and also by tools that

1679
01:54:05,730 --> 01:54:07,780
support these standards

1680
01:54:07,990 --> 01:54:11,730
and last but not least that further activities mainly driven by the w three c

1681
01:54:13,750 --> 01:54:18,690
the world wide web consortium which is for example the semantic web service committee but

1682
01:54:18,690 --> 01:54:21,400
again this is just a short tutorial but i would like to give you that

1683
01:54:21,400 --> 01:54:27,580
important pointer because the combination of web services and semantic web technologies is seen as

1684
01:54:27,580 --> 01:54:34,230
one of the great potentials and of course something like query languages and so on

1685
01:54:34,250 --> 01:54:40,510
OK so what what what open up until now what is actually an ontology we

1686
01:54:40,510 --> 01:54:46,620
talked a little bit about ontological vocabulary i've shown you some little examples but now

1687
01:54:46,620 --> 01:54:48,190
i would like to give you some

1688
01:54:48,210 --> 01:54:53,690
more detailed views on it when first of all let's start with the purpose and

1689
01:54:53,690 --> 01:54:58,430
that's actually a citation from the knowledge management community davenport is quite well-known writer that

1690
01:54:58,800 --> 01:55:02,540
people can't share knowledge if they do not speak a common language

1691
01:55:02,750 --> 01:55:06,820
and this can be done in the last two agents can conjure knowledge if they

1692
01:55:06,820 --> 01:55:10,710
don't speak a common language so what we want to achieve with ontology is it

1693
01:55:10,810 --> 01:55:17,080
exactly that common language so we want to support the knowledge sharing

1694
01:55:17,230 --> 01:55:22,340
another definition by which was one of the key players in the early nineties and

1695
01:55:22,340 --> 01:55:29,690
started that whole ontology community an ontology is an explicit specification of conceptual innovation

1696
01:55:29,710 --> 01:55:32,340
well that sounds a bit strange what does it give you

1697
01:55:32,360 --> 01:55:37,970
first of all we have the better communication between humans and machines which is given

1698
01:55:37,970 --> 01:55:38,670
to you

1699
01:55:38,830 --> 01:55:44,170
erm and then explicit specification you basically get former construct and this is what agents

1700
01:55:44,170 --> 01:55:50,190
need to communicate to each other and you have you have the explicit specification of

1701
01:55:50,380 --> 01:55:54,800
concept innovation which means that you only cover a certain part of your world which

1702
01:55:54,800 --> 01:55:58,380
you would like to model so maybe you've heard of the site project which was

1703
01:55:58,380 --> 01:56:03,400
one of the very famous projects also in the i think it's in the in

1704
01:56:03,400 --> 01:56:07,320
the eighties there were the first roots and they try to make the whole world

1705
01:56:07,370 --> 01:56:09,530
so it was really quite ambitious

1706
01:56:09,530 --> 01:56:16,290
not only that actually it's not too hard so what happened then it is that

1707
01:56:16,320 --> 01:56:18,550
in two thousand five

1708
01:56:18,640 --> 01:56:23,620
also another spanning tree was proposed called the low stretch spanning trees which get around

1709
01:56:23,620 --> 01:56:28,260
this problem going to the definition here but there's another definition spanning tree which then

1710
01:56:28,260 --> 01:56:29,740
remove this

1711
01:56:29,770 --> 01:56:35,070
this is this problem of having lightweight edges perturbing the spanning tree

1712
01:56:36,890 --> 01:56:41,570
and the advantage with these is that they have a much better condition number

1713
01:56:41,590 --> 01:56:45,230
one problem is that we don't know how to find them in linear time for

1714
01:56:45,240 --> 01:56:47,270
the super linear to find

1715
01:56:47,280 --> 01:56:51,460
another problem is that there still not really very good if you take even though

1716
01:56:52,100 --> 01:56:54,780
simple square mesh

1717
01:56:54,800 --> 01:56:59,470
as the underlying system you want to solve that there is no good spanning tree

1718
01:57:00,170 --> 01:57:05,540
right so when that that's one of the results from two thousand four

1719
01:57:07,780 --> 01:57:11,700
grandmom and iron ninety four proposed that what we really should do is take a

1720
01:57:11,700 --> 01:57:12,930
steiner tree

1721
01:57:12,980 --> 01:57:15,790
so the idea here is is instead of

1722
01:57:15,810 --> 01:57:20,430
taking this graph and requiring ourselves to pick

1723
01:57:20,470 --> 01:57:25,710
a subgraph what we just start a lower self have steiner and

1724
01:57:25,730 --> 01:57:31,070
so in particular what we did here is is these nodes are the original variables

1725
01:57:31,070 --> 01:57:34,340
and we just introduce new variables to form a tree

1726
01:57:38,100 --> 01:57:42,670
so so in fact the square mesh this goes dramatically better condition number and you

1727
01:57:42,670 --> 01:57:44,340
can solve them much faster

1728
01:57:44,350 --> 01:57:50,500
and also experimentally this works quite well substantially better than any of the other spanning

1729
01:57:53,040 --> 01:57:58,790
unfortunately we never really figured out how to find a good way to compute these

1730
01:57:58,790 --> 01:58:01,740
trees in general minutes and i a hard problem

1731
01:58:03,780 --> 01:58:07,870
so we we haven't made any headway there in two thousand seven

1732
01:58:07,930 --> 01:58:12,260
good estimates of proposed using steiner forest

1733
01:58:12,270 --> 01:58:15,120
so the idea now is already going to do is is we're going to take

1734
01:58:15,120 --> 01:58:16,760
this underlying graph

1735
01:58:16,770 --> 01:58:20,530
and we're going to find highly connected pieces in this graph

1736
01:58:20,540 --> 01:58:24,280
i think in this case it probably doesn't actually exist that way but the idea

1737
01:58:24,290 --> 01:58:27,940
now is is what we're going to do now is just a small pieces of

1738
01:58:27,940 --> 01:58:31,640
this graph and we're going to make assigning steiner tree for each one of those

1739
01:58:31,640 --> 01:58:33,670
pieces and

1740
01:58:33,670 --> 01:58:35,430
use that are preconditions

1741
01:58:35,450 --> 01:58:39,790
well actually more precisely

1742
01:58:39,850 --> 01:58:44,530
so the advantage of this is that it's very easy to find these things they

1743
01:58:44,530 --> 01:58:45,540
work well

1744
01:58:45,560 --> 01:58:48,860
with recursive solvers so we haven't got to that so the next thing to do

1745
01:58:48,860 --> 01:58:52,680
is is is shown how to recursively use these ideas

1746
01:58:53,060 --> 01:58:58,770
and also to give good experimental results will show that can unfortunately at this point

1747
01:58:58,770 --> 01:59:02,290
we still have only been able to analyse these for the players system so for

1748
01:59:02,290 --> 01:59:03,550
those we can show

1749
01:59:03,560 --> 01:59:06,430
could run time linear time OK

1750
01:59:07,790 --> 01:59:12,000
so the second idea to invite you had is that what you should do

1751
01:59:12,030 --> 01:59:16,610
but we're now applying this to llo are steiner system so the idea now is

1752
01:59:16,610 --> 01:59:21,760
again these original variables these are steiner variables and these are the edge weights in

1753
01:59:21,760 --> 01:59:23,070
this new graph

1754
01:59:23,120 --> 01:59:28,560
so the idea now that the vieja propose he said well find your preconditions

1755
01:59:28,580 --> 01:59:33,540
and i suppose it's not just a spanning tree but actually has a few more

1756
01:59:33,540 --> 01:59:37,090
edges like i think this has one extra edge in this case right remove this

1757
01:59:37,090 --> 01:59:39,250
says then we get this right a tree

1758
01:59:39,250 --> 01:59:43,290
but suppose in general we may have a lot more other edges let's just remove

1759
01:59:43,290 --> 01:59:47,560
those edges which are easy using direct methods so in this case will remove all

1760
01:59:47,580 --> 01:59:49,380
the original variables

1761
01:59:50,320 --> 01:59:53,010
and then we'll end up with a much smaller system

1762
01:59:53,010 --> 01:59:54,570
and then we'll do now

1763
01:59:54,590 --> 01:59:57,760
is take this system here which is now reduced

1764
01:59:57,760 --> 02:00:02,670
and instead of trying to solve that directly what we we're cursed will just now

1765
02:00:02,670 --> 02:00:07,740
as for recursive solver to solve the simple this newer system and hopefully this nervous

1766
02:00:07,740 --> 02:00:10,490
system is much smaller than the old system so

1767
02:00:10,500 --> 02:00:14,320
it will get a geometric increase decrease in problem size which will give us a

1768
02:00:14,320 --> 02:00:15,400
good solver

1769
02:00:19,970 --> 02:00:23,280
so when we worry about this so

1770
02:00:23,290 --> 02:00:25,400
so maybe i should just

1771
02:00:25,420 --> 02:00:29,700
talk about congestion dilation right so hopefully you will know this stuff i'm not sure

1772
02:00:29,700 --> 02:00:33,570
why this should be here but if you have a simple recurrence relation of this

1773
02:00:34,710 --> 02:00:40,180
right like in the case we started with anyone analyse how fast these things converge

1774
02:00:40,190 --> 02:00:43,210
a standard result says that if you let the air

1775
02:00:43,220 --> 02:00:47,300
b simply your guess minus the actual answer to the problem

1776
02:00:47,330 --> 02:00:50,970
but in fact the gas then the airtime i

1777
02:00:51,010 --> 02:00:56,010
is simply the original air times the genes power of your matrix OK

1778
02:00:56,030 --> 02:01:01,010
so all you need to then shows is that this matrix here goes to zero

1779
02:01:01,020 --> 02:01:03,510
as i goes to infinity right

1780
02:01:05,630 --> 02:01:14,820
so so that gives us and

1781
02:01:14,830 --> 02:01:18,550
so this list let's remember are defined before but if we have a system of

1782
02:01:18,550 --> 02:01:23,390
equations of the form x equals lambda b then first recall lambda and i can

1783
02:01:23,390 --> 02:01:25,970
value and exciting vector

1784
02:01:25,970 --> 02:01:30,440
in our case here all aragon values there are that are

1785
02:01:30,450 --> 02:01:32,450
greater than or equal to zero so

1786
02:01:32,460 --> 02:01:35,530
that's all the posse and actually the zero one and there's a bunch of other

1787
02:01:36,390 --> 02:01:38,390
right so i suppose we sort these

1788
02:01:38,400 --> 02:01:42,840
so the condition number is simply defined to be of this matrix the largest eigen

1789
02:01:42,840 --> 02:01:46,530
value divided by the smallest i can

1790
02:01:46,580 --> 02:01:51,280
so the standard results says that the convergence rate for the basic iterative method is

1791
02:01:51,280 --> 02:01:53,220
one over the condition number

1792
02:01:53,230 --> 02:01:55,510
so this ends up being very poor

1793
02:01:55,520 --> 02:01:59,560
if you then go to conjugate gradient standard result says that you get to take

1794
02:01:59,560 --> 02:02:01,630
the square root of the condition number

1795
02:02:02,150 --> 02:02:07,100
so what is roughly says for simple examples is that the rate of convergence in

1796
02:02:07,100 --> 02:02:11,220
other words if you want one bit of accuracy the number of iterations has to

1797
02:02:11,220 --> 02:02:14,920
be something like the diameter of the graph right

1798
02:02:14,930 --> 02:02:17,510
so if in other words you take the square root of n by spirit of

1799
02:02:17,510 --> 02:02:23,050
anchorage and you can't gradient then you need square event iterations to get one bit

1800
02:02:23,050 --> 02:02:24,020
of accuracy

1801
02:02:25,570 --> 02:02:28,790
so this is the classic analysis

1802
02:02:30,110 --> 02:02:34,230
what we want to do now is bound the condition number of b inverse a

1803
02:02:34,230 --> 02:02:39,010
because that's what we're generating on instead of simply right

1804
02:02:39,600 --> 02:02:43,000
while another way to write that is is simply if we're interested in the i

1805
02:02:43,000 --> 02:02:47,530
can values of this system is that equivalent to the generalized values of the form

1806
02:02:47,530 --> 02:02:51,280
x equals lambda b can

1807
02:02:51,280 --> 02:02:54,550
so the land is called the generalized tagging OK

1808
02:02:54,610 --> 02:02:58,830
so in general than what we're interested in is the condition number again so we're

1809
02:02:58,830 --> 02:03:03,700
interested in the i can values of the system here so the largest divided by

