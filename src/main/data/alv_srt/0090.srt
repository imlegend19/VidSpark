1
00:00:00,000 --> 00:00:05,440
independent random variables are column x one through x n

2
00:00:09,210 --> 00:00:11,750
and we have a function that maps from

3
00:00:11,750 --> 00:00:13,940
from some domain x

4
00:00:13,940 --> 00:00:15,920
for many maps from from

5
00:00:15,940 --> 00:00:18,050
the space that these

6
00:00:20,210 --> 00:00:24,280
the baseline to to the real

7
00:00:27,690 --> 00:00:31,840
this is not true there should be a bounded interval in fact

8
00:00:31,840 --> 00:00:38,570
we know that this is true so that's that's the real and then we have

9
00:00:39,280 --> 00:00:43,360
that whenever we take any value of x one through x n

10
00:00:43,400 --> 00:00:44,670
and and any

11
00:00:44,690 --> 00:00:49,570
and any other x are any all i NEXI prime

12
00:00:49,590 --> 00:00:53,320
OK so maximizing overall the arguments of the function

13
00:00:53,360 --> 00:00:58,070
if we look at how a function g changes when we change x i to

14
00:00:58,070 --> 00:01:00,030
XII prime

15
00:01:01,610 --> 00:01:06,980
the biggest change that we can get is COI i

16
00:01:07,590 --> 00:01:10,610
if we have this condition satisfied this is the

17
00:01:10,630 --> 00:01:13,300
the bounded differences condition

18
00:01:13,320 --> 00:01:17,530
is another name for this inequality the wonderful many of bounded differences inequality

19
00:01:17,550 --> 00:01:19,940
then with high probability

20
00:01:19,960 --> 00:01:24,860
the deviation between a random variable in its expectation is is small

21
00:01:25,110 --> 00:01:28,780
you know on on the order of the two norm of this vector of these

22
00:01:28,780 --> 00:01:31,780
maximal deviations

23
00:01:33,150 --> 00:01:36,230
OK so when we apply that in our case that g takes

24
00:01:36,420 --> 00:01:39,750
has as fluctuations that are of order one over

25
00:01:39,840 --> 00:01:41,610
OK so we get some of

26
00:01:41,690 --> 00:01:43,630
of n terms

27
00:01:43,710 --> 00:01:47,520
of of one hundred square and we get our square that's in

28
00:01:47,610 --> 00:01:51,860
turn down here

29
00:01:52,550 --> 00:01:55,620
all right so that's that's taking it a step in the right direction we've got

30
00:01:55,620 --> 00:01:56,980
away from

31
00:01:56,980 --> 00:01:59,150
you know i remember we're trying to prove

32
00:01:59,170 --> 00:02:01,090
we're trying to show that

33
00:02:01,130 --> 00:02:04,800
that expectations are not much bigger than sample averages

34
00:02:04,820 --> 00:02:09,280
we start uniformly across the club across the class we trying to show that the

35
00:02:09,670 --> 00:02:14,250
that the supremum for over functions of

36
00:02:14,300 --> 00:02:19,210
functions g class differences here is not too big and and

37
00:02:19,230 --> 00:02:22,250
what we first shown is that random variable

38
00:02:22,300 --> 00:02:25,440
is not much bigger than its expectation

39
00:02:33,730 --> 00:02:40,530
there's no conditions saying that g has to be bounded we have

40
00:02:45,520 --> 00:02:50,340
so g is bounded here implicit so so the point is we are always looking

41
00:02:50,340 --> 00:02:52,210
at differences

42
00:02:52,230 --> 00:02:54,840
right and and

43
00:02:54,840 --> 00:02:57,070
for any change of the

44
00:02:57,090 --> 00:02:59,880
of these variables were getting is small

45
00:02:59,900 --> 00:03:01,500
a small change so g

46
00:03:01,500 --> 00:03:03,250
g is necessarily bounded

47
00:03:03,260 --> 00:03:06,960
because we were getting no more than the sum of these changes to

48
00:03:07,000 --> 00:03:11,500
right it is not necessarily that if we subtract some some

49
00:03:11,630 --> 00:03:15,530
the value of g at some fixed value of x one through x xn

50
00:03:15,550 --> 00:03:16,800
right then

51
00:03:16,860 --> 00:03:21,150
then so sent just entering around zero so for some value of x one to

52
00:03:21,210 --> 00:03:24,690
x and this thing is zero then g is bounded by the sum of the

53
00:03:24,690 --> 00:03:26,230
sea ice

54
00:03:26,230 --> 00:03:30,570
right in the way that we use the boundedness of g

55
00:03:32,550 --> 00:03:35,860
i guess what's confusing here is that there are two different genes floating around g

56
00:03:35,880 --> 00:03:38,900
means two different things on this light and on this line

57
00:03:38,900 --> 00:03:40,500
OK so here we are

58
00:03:40,520 --> 00:03:45,110
the class of bounded functions the fact that they are bounded

59
00:03:45,130 --> 00:03:46,590
right means that

60
00:03:46,610 --> 00:03:48,750
the sample average

61
00:03:48,780 --> 00:03:52,250
can't change by much when we change one of the arguments

62
00:03:52,250 --> 00:03:55,840
OK the random variable we're considering

63
00:03:55,840 --> 00:04:00,230
to apply the demons inequality is the supreme over g of the expectation must sample

64
00:04:00,230 --> 00:04:05,050
average so on this slide in stating the dems inequality i have used g

65
00:04:05,840 --> 00:04:08,030
there should be some other and some others

66
00:04:08,090 --> 00:04:12,030
symbol for the random variable right so the random variable we're talking about here

67
00:04:13,170 --> 00:04:15,880
in our case this thing

68
00:04:16,780 --> 00:04:20,110
so sorry for that was an unfortunate choice

69
00:04:20,170 --> 00:04:23,500
of notation

70
00:04:24,230 --> 00:04:28,840
so i want to say a little bit about why

71
00:04:29,050 --> 00:04:31,630
about me damage inequality

72
00:04:31,650 --> 00:04:35,900
and why it's true

73
00:04:35,980 --> 00:04:42,690
and some of the ingredients of the proof he will will pop up later when

74
00:04:42,690 --> 00:04:44,230
we consider the

75
00:04:45,590 --> 00:04:47,760
the VC dimension

76
00:04:47,780 --> 00:04:50,280
OK so

77
00:04:50,320 --> 00:04:54,210
we're looking here at deviations between

78
00:04:54,250 --> 00:04:57,070
the function g of these in random

79
00:04:58,130 --> 00:05:01,020
and the expectation of g

80
00:05:03,570 --> 00:05:07,420
so i want to express that

81
00:05:07,420 --> 00:05:14,530
as a sum of a martingale difference sequence so i mean we not using any

82
00:05:14,530 --> 00:05:17,780
properties of modern nongaussian so you don't have to know what

83
00:05:19,590 --> 00:05:23,730
so we we if we define the initial sequence x one through x i and

84
00:05:23,730 --> 00:05:25,380
worry about how

85
00:05:25,400 --> 00:05:30,420
the conditional expectation expectation changes as we see the i th random variable

86
00:05:30,480 --> 00:05:33,920
right so this is the function g

87
00:05:33,940 --> 00:05:34,940
and we consider

88
00:05:34,940 --> 00:05:37,530
the how the expectation has

89
00:05:37,530 --> 00:05:42,820
increased so the eyes the increasing expectation conditioned on c x one through x i

90
00:05:42,820 --> 00:05:49,170
over what the conditional expectation was then we'd seen x one through x i slide

91
00:05:49,280 --> 00:05:52,130
the condition on just the first one is one of these guys

92
00:05:52,130 --> 00:05:53,250
all right then

93
00:05:53,300 --> 00:05:54,840
the random variable g

94
00:05:54,860 --> 00:05:59,000
minus its expectation is just the sum of these things right is the telescoping some

95
00:05:59,030 --> 00:06:02,110
the first one the expectation of g given nothing

96
00:06:02,150 --> 00:06:03,400
well that's this

97
00:06:03,480 --> 00:06:06,330
in the last one the expectation of g given x one through x and well

98
00:06:06,330 --> 00:06:07,710
that's this

99
00:06:07,760 --> 00:06:10,750
right so we can write it as the some of these things and now it's

100
00:06:10,780 --> 00:06:12,610
some of those guys

101
00:06:12,650 --> 00:06:17,550
that we want to bound the deviations of of this thing

102
00:06:17,570 --> 00:06:20,000
say with high probability the summer's

103
00:06:20,030 --> 00:06:22,030
is small

104
00:06:22,070 --> 00:06:23,440
OK so

105
00:06:23,460 --> 00:06:25,780
you know i want to get the true steps but the

106
00:06:27,480 --> 00:06:32,280
the expectations of these things given the past are all zeros

107
00:06:32,280 --> 00:06:33,170
OK so

108
00:06:33,190 --> 00:06:35,280
you know this allows us to apply

109
00:06:35,880 --> 00:06:37,610
ideas of

110
00:06:37,670 --> 00:06:41,280
of of hosting going back to her

111
00:06:41,300 --> 00:06:44,750
and i'm going to

112
00:06:44,750 --> 00:06:46,800
to go through

113
00:06:46,820 --> 00:06:49,570
the argument here because

114
00:06:49,780 --> 00:06:55,360
it's something that we will see we'll see later on

115
00:06:56,570 --> 00:07:00,380
OK so we can write the deviation between a random variable

116
00:07:00,400 --> 00:07:02,050
and its expectation

117
00:07:02,130 --> 00:07:04,050
as the sum of these

118
00:07:04,090 --> 00:07:07,860
differences between conditional expectations

119
00:07:07,880 --> 00:07:10,980
and now he is attracted pops up again and again

120
00:07:12,320 --> 00:07:15,530
the probability that this thing is bigger than anything

121
00:07:15,530 --> 00:07:20,150
well let's take some monotone transformation of those are the two sides

122
00:07:20,250 --> 00:07:25,170
OK in particular with the monotone transformation take is the exponential function

123
00:07:25,190 --> 00:07:26,090
with some

124
00:07:26,110 --> 00:07:28,730
positive constant s

125
00:07:28,750 --> 00:07:32,960
OK so this is bigger than that precisely when eta this is bigger than e

126
00:07:32,960 --> 00:07:34,550
to the that

127
00:07:34,550 --> 00:07:38,130
well u s times this

128
00:07:39,880 --> 00:07:41,710
this is the this is

129
00:07:42,070 --> 00:07:45,750
something that we can bound using markov inequality

130
00:07:45,780 --> 00:07:46,900
OK so

131
00:07:47,420 --> 00:07:54,210
he does anybody know that that inequality

132
00:07:54,250 --> 00:07:56,360
if you do nobody is admitted

133
00:07:57,170 --> 00:08:02,820
was just saying the the the expectation of some nonnegative random variable

134
00:08:02,860 --> 00:08:06,690
it is at least as big as the probability that is bigger than something divided

135
00:08:06,690 --> 00:08:10,300
by that some time that something

136
00:08:12,440 --> 00:08:13,320
right so

137
00:08:15,380 --> 00:08:19,780
so we can get upper bound on the probability in terms of an expectation of

138
00:08:19,780 --> 00:08:21,250
this thing times

139
00:08:21,260 --> 00:08:22,480
in the months st

140
00:08:22,480 --> 00:08:24,690
OK this is called the chernoff

141
00:08:24,690 --> 00:08:28,570
you get very little revenue and this becomes a very big business problems

142
00:08:28,730 --> 00:08:30,790
you according user experience and the

143
00:08:30,820 --> 00:08:35,060
you have still never let you do this so what we need is some sort

144
00:08:35,060 --> 00:08:35,980
of a

145
00:08:35,990 --> 00:08:38,200
which can converge much more quickly

146
00:08:38,230 --> 00:08:41,990
and the only way to do this is by using some additional structure that might

147
00:08:41,990 --> 00:08:43,400
be available

148
00:08:43,420 --> 00:08:45,390
and as the above mentioned

149
00:08:45,400 --> 00:08:47,980
one possible structure is that of the taxonomy

150
00:08:47,990 --> 00:08:49,140
which is already

151
00:08:49,160 --> 00:08:52,270
in existence this is sort of stuff that

152
00:08:52,490 --> 00:08:55,070
the yahoo states people use

153
00:08:55,960 --> 00:09:00,040
as with the marketing departments of big companies and so on so these are are

154
00:09:00,320 --> 00:09:03,910
these are already existing taxonomies the main thing

155
00:09:03,960 --> 00:09:07,610
and they are actually used for the purposes of selecting

156
00:09:07,650 --> 00:09:11,570
as and my segmenting the at market

157
00:09:11,580 --> 00:09:16,400
so for instance we have classifier that can be in any page at any at

158
00:09:16,400 --> 00:09:19,250
map one particular note in this taxonomy

159
00:09:19,290 --> 00:09:24,460
and and since these taxonomies are being used for marketing purposes it's reasonable to believe

160
00:09:24,460 --> 00:09:28,040
that there is some information in this taxonomy which if

161
00:09:28,070 --> 00:09:31,920
we could use nicely then we could probably come up with better than that

162
00:09:31,990 --> 00:09:34,490
which converges somewhat fast

163
00:09:34,500 --> 00:09:36,480
the question is how we do

164
00:09:36,490 --> 00:09:40,200
so what present next is one possible solution

165
00:09:40,210 --> 00:09:45,940
which is a multi bandit policy which uses this taxonomy and i show briefly that

166
00:09:45,990 --> 00:09:47,530
this does fairly well

167
00:09:47,540 --> 00:09:49,700
but this is not a course the end of the

168
00:09:49,710 --> 00:09:51,890
the question

169
00:09:53,180 --> 00:09:56,530
we the content that makes matrix where we the

170
00:09:56,540 --> 00:10:00,830
if ax and each row corresponds to one big band where we want to figure

171
00:10:00,830 --> 00:10:03,280
out the best ad for this particular

172
00:10:03,290 --> 00:10:06,030
but now we have taxonomies of what's

173
00:10:08,320 --> 00:10:11,230
let's consider just this basic taxonomy

174
00:10:11,240 --> 00:10:16,060
let's say that our world consists of acts which belonged at computers and travel

175
00:10:16,070 --> 00:10:19,700
and these ads have been shown only on the pages that come from

176
00:10:19,710 --> 00:10:22,150
apple computer center

177
00:10:22,160 --> 00:10:24,730
this sort of this segmentation

178
00:10:26,860 --> 00:10:32,620
but except matrix into seven chunks blocks so for instance the central block will correspond

179
00:10:33,570 --> 00:10:38,990
all of all assets belonging to computers being shown on pages belonging to computers again

180
00:10:39,000 --> 00:10:43,450
so since the at all similar pages are ultimately we can sort of expect the

181
00:10:43,450 --> 00:10:45,100
CBS are going to be

182
00:10:45,100 --> 00:10:48,600
broadly similar or homogeneous as compared to

183
00:10:48,650 --> 00:10:51,700
CBS all of the matrix so there is some

184
00:10:51,700 --> 00:10:53,860
structured but this matrix

185
00:10:53,870 --> 00:10:57,140
in the city of matrix that we should be able to use

186
00:10:57,150 --> 00:10:58,780
you know bandit policy

187
00:10:59,570 --> 00:11:04,270
the key idea here is that the same applies to be more homogeneous than on

188
00:11:05,740 --> 00:11:08,120
how can we use this in our banning policy

189
00:11:08,150 --> 00:11:11,700
and then see how we could use it in both the allocation stage

190
00:11:11,710 --> 00:11:14,240
and the estimated chen stadium the one

191
00:11:14,320 --> 00:11:16,190
so in that location

192
00:11:16,200 --> 00:11:17,860
let's say that we have to use

193
00:11:17,900 --> 00:11:19,390
who came to have their say same

194
00:11:19,400 --> 00:11:24,730
came page and this web page was classified into one particular note in the taxonomy

195
00:11:24,810 --> 00:11:25,770
it's a

196
00:11:25,780 --> 00:11:27,980
somewhere inside computers

197
00:11:27,990 --> 00:11:30,270
so this specifies my one

198
00:11:30,540 --> 00:11:32,730
big figure out one at the peak

199
00:11:32,740 --> 00:11:35,900
so from this particular rule

200
00:11:35,920 --> 00:11:37,820
and should be used

201
00:11:37,830 --> 00:11:40,440
now the traditional bandit policy would just not

202
00:11:40,450 --> 00:11:44,350
abandoned on this and that in fact possibly a million pounds and it would be

203
00:11:44,350 --> 00:11:45,850
very slow to converge

204
00:11:45,890 --> 00:11:48,490
what we can do instead

205
00:11:48,530 --> 00:11:52,680
it stood on the bandit on the top level classes so i just under bandit

206
00:11:52,680 --> 00:11:57,230
figaro should show at from happening on different computers or from

207
00:11:57,320 --> 00:12:01,280
so dependencies or lecture natural capital

208
00:12:01,310 --> 00:12:04,940
this reduces the space the fact that i now have to search from

209
00:12:04,950 --> 00:12:06,680
so i go down and say

210
00:12:06,710 --> 00:12:08,650
it on another planet

211
00:12:08,660 --> 00:12:13,400
among children of africa in the taxonomy and they keep doing this one after another

212
00:12:13,440 --> 00:12:18,580
and the late each one the sun and this leaf node corresponds to one had

213
00:12:18,610 --> 00:12:20,020
which i should put the use

214
00:12:20,030 --> 00:12:22,600
so essentially instead of running one bandit

215
00:12:22,650 --> 00:12:26,280
and in the bandit on each level of that exon

216
00:12:26,330 --> 00:12:28,310
OK but why would this

217
00:12:28,320 --> 00:12:30,230
ever work

218
00:12:30,230 --> 00:12:35,940
the reason why is because the higher level bandits some aggregating information from all the

219
00:12:35,940 --> 00:12:38,500
lower levels of the taxonomy so

220
00:12:38,520 --> 00:12:42,070
the high level bandits have very few arms instead of having to deal with millions

221
00:12:42,070 --> 00:12:42,850
of fans

222
00:12:42,890 --> 00:12:46,180
here the level bandit has to deal with only three

223
00:12:46,190 --> 00:12:48,450
so it's much faster convergence

224
00:12:48,480 --> 00:12:50,160
and what this does

225
00:12:50,190 --> 00:12:55,030
is that our full model delivered policy essentially quickly converges on the back

226
00:12:55,080 --> 00:12:56,460
portions of the space

227
00:12:56,480 --> 00:12:59,950
it takes time to figure out what is the best actor at the show

228
00:12:59,960 --> 00:13:03,140
but it figures are very quickly that happened is the good

229
00:13:03,150 --> 00:13:09,250
the best at show is probably somewhere inside so it reduces the space the fact

230
00:13:09,310 --> 00:13:10,570
that we have to search for

231
00:13:12,860 --> 00:13:14,850
that is how

232
00:13:14,860 --> 00:13:18,680
the taxonomy could be used in the allocation stage to figure out which acts to

233
00:13:18,680 --> 00:13:20,640
show next

234
00:13:20,640 --> 00:13:23,000
it can also be used in the estimation

235
00:13:23,020 --> 00:13:26,860
so here's the problem is that i suppose i should add

236
00:13:26,900 --> 00:13:30,490
in response to a certain page and get some feedback maybe the user clicks on

237
00:13:31,280 --> 00:13:36,640
something like that now this feedback can be used to optimize CTR estimates CTR for

238
00:13:36,640 --> 00:13:38,320
this particular

239
00:13:38,350 --> 00:13:42,600
but we also now know that this particular act belongs to a blog

240
00:13:42,610 --> 00:13:46,360
and all the cds in the bloggers fall machines are close to each

241
00:13:46,390 --> 00:13:49,350
so in some sense should be able to use this feedback

242
00:13:49,490 --> 00:13:53,070
four cds for everyone inside the block

243
00:13:53,110 --> 00:13:55,420
how can we model this depends

244
00:13:55,450 --> 00:13:56,480
and this is well

245
00:13:56,600 --> 00:13:59,740
the whole literature and shrinkage models comes

246
00:13:59,770 --> 00:14:03,160
so one very simple changes more go something like this

247
00:14:03,190 --> 00:14:04,990
if i knew the CTR

248
00:14:05,020 --> 00:14:06,890
of particular cell

249
00:14:06,900 --> 00:14:09,230
then i could say that the number of clicks to get in the some of

250
00:14:09,240 --> 00:14:12,530
the number of successes in this it is just a binomial variables

251
00:14:12,540 --> 00:14:14,900
which depends on the number of impressions in the cell

252
00:14:14,940 --> 00:14:16,660
and the city of the

253
00:14:16,680 --> 00:14:20,420
but the city itself i don't know so what i can say is that they

254
00:14:20,450 --> 00:14:22,620
want to see the end of the cell

255
00:14:22,650 --> 00:14:25,770
as something that draws from the block

256
00:14:25,810 --> 00:14:29,250
so in some sense all the cells in a block

257
00:14:29,280 --> 00:14:32,710
i have the CD drawn from the same distribution

258
00:14:32,740 --> 00:14:37,180
and so if this distribution is tight then all the cells will have similar ctrs

259
00:14:37,180 --> 00:14:39,140
as compared to the average

260
00:14:39,150 --> 00:14:43,780
and what's your answer them at all this does is that your estimated CTR becomes

261
00:14:43,780 --> 00:14:45,270
the sum of two terms

262
00:14:45,280 --> 00:14:49,200
so the first one of the densest of services

263
00:14:49,210 --> 00:14:51,560
how often have i seen success

264
00:14:51,570 --> 00:14:53,810
for this particular given

265
00:14:53,830 --> 00:14:55,610
the number of impressions of this

266
00:14:55,660 --> 00:15:00,490
but the second is something like a block quite see what it means for the

267
00:15:00,490 --> 00:15:01,530
entire plot

268
00:15:01,530 --> 00:15:03,640
so what happens is that initially

269
00:15:03,650 --> 00:15:05,230
when you very data

270
00:15:05,240 --> 00:15:09,290
and the block CTR so dominates and this brings you very quickly up to the

271
00:15:09,290 --> 00:15:10,770
mean level

272
00:15:10,820 --> 00:15:14,120
and later on as you collect more and more

273
00:15:14,150 --> 00:15:18,030
the estimated CTR moves more and more was observed the towards

274
00:15:18,040 --> 00:15:21,890
maximum likelihood estimation so what this does is initially

275
00:15:23,490 --> 00:15:26,440
the matrix is too shoot and you have very little

276
00:15:26,450 --> 00:15:29,490
the block CTR sort of brings you up to the average levels

277
00:15:29,490 --> 00:15:33,850
well if you take all the DNA you have the number of a which is

278
00:15:33,850 --> 00:15:39,590
which can be between four hundred thousand million four here for the human genome on

279
00:15:39,670 --> 00:15:43,270
for those of you have seen the answers

280
00:15:43,940 --> 00:15:48,850
so it's not a very small but still going only be on your computer

281
00:15:48,850 --> 00:15:51,190
something that is very low

282
00:15:51,690 --> 00:15:57,040
the examples of the other examples is here if you would like to use to

283
00:15:57,040 --> 00:16:02,130
fly to human you have the impression that the complexity of the organism is increasing

284
00:16:02,130 --> 00:16:04,890
you when you compare human two

285
00:16:04,930 --> 00:16:09,650
the eastern would say well you can do more than and you know that you

286
00:16:09,930 --> 00:16:12,610
size increases say well the goal

287
00:16:12,620 --> 00:16:18,010
number more than but this is not true so there are many

288
00:16:18,270 --> 00:16:22,610
the main exceptions here so in general administration but you really need to show that

289
00:16:22,610 --> 00:16:23,830
if you take plants

290
00:16:23,830 --> 00:16:26,300
there are much bigger than humans

291
00:16:26,320 --> 00:16:30,850
so it's not clear to that is more than but there's just there's no single

292
00:16:30,850 --> 00:16:34,180
there is no single link between number of chromosomes

293
00:16:34,200 --> 00:16:36,420
the size of men

294
00:16:36,450 --> 00:16:39,570
see the complexity of the organism

295
00:16:39,630 --> 00:16:43,730
but just to give you some idea of the of the sizes of these countries

296
00:16:44,000 --> 00:16:46,100
six hundred to describe the

297
00:16:46,110 --> 00:16:48,920
the human genome

298
00:16:48,930 --> 00:16:50,990
so let's that's

299
00:16:51,210 --> 00:16:56,210
how it was so i said that DNA was supposed to contain information to make

300
00:16:56,230 --> 00:17:00,540
human how does it work well what could be more this few because we so

301
00:17:00,550 --> 00:17:01,500
we don't know much

302
00:17:01,550 --> 00:17:04,960
thank about how it works but what we know there's something we know it with

303
00:17:05,080 --> 00:17:10,110
ninety four which is called has nice also called the amount of money by biology

304
00:17:10,190 --> 00:17:16,610
so it's the it's OK just natural science over the last one of the reasons

305
00:17:16,640 --> 00:17:20,520
why it is useful is the american there is the called

306
00:17:20,540 --> 00:17:22,890
to make money because called proteins

307
00:17:22,900 --> 00:17:28,670
and what the following remember that in cell you fields

308
00:17:29,440 --> 00:17:32,440
could you go into a single company

309
00:17:32,590 --> 00:17:38,850
and at some points your yourselves so depending on the we require some some problems

310
00:17:39,100 --> 00:17:46,180
of this approach is looking for the producer of macromolecules which are functions of for

311
00:17:46,180 --> 00:17:51,270
instance head of is protein that using the blood to carry oxygen when you need

312
00:17:51,360 --> 00:17:55,910
live in the there is no specific so that means there is reason somewhere in

313
00:17:55,910 --> 00:18:01,180
your DNA code to make protein so what we need to be in the same

314
00:18:01,180 --> 00:18:05,730
we read in a product that contains a call from king hamad bin and then

315
00:18:05,910 --> 00:18:10,150
it we far into another article called RNA

316
00:18:10,150 --> 00:18:12,050
it is very similar to a

317
00:18:12,470 --> 00:18:16,820
single stranded that are and will lead to the US

318
00:18:16,830 --> 00:18:18,750
and then another missionary

319
00:18:18,770 --> 00:18:22,650
we really are in a region which are just local optima

320
00:18:22,800 --> 00:18:24,180
OK that contains the

321
00:18:24,190 --> 00:18:25,360
the this

322
00:18:25,370 --> 00:18:30,650
specific plans for ordering him to be and then the machinery with real information in

323
00:18:30,650 --> 00:18:33,640
our a to make another molecules

324
00:18:33,680 --> 00:18:35,300
and this would be the protein

325
00:18:35,320 --> 00:18:39,860
because of the central so that you you need to compete with unit two and

326
00:18:40,000 --> 00:18:44,660
a you know this is what you see this is the first when you are

327
00:18:44,660 --> 00:18:49,830
in a little bit of theory when you live in the local

328
00:18:49,920 --> 00:18:54,200
for with computing are in a is called transcription

329
00:18:54,220 --> 00:18:59,240
and the information they will be ready to make the final output is useful

330
00:18:59,290 --> 00:19:04,690
these are here simply means that DNA itself is called the when you need a

331
00:19:04,690 --> 00:19:10,690
source of someone wants to separate into two daughter cells and also when you bristol

332
00:19:10,690 --> 00:19:13,630
when when you want images

333
00:19:13,630 --> 00:19:19,910
this is the control flow of information what's important is constantly reverting there are and

334
00:19:19,910 --> 00:19:24,790
what is going to be many numbers you may have to make many on these

335
00:19:28,140 --> 00:19:36,200
so this specific are in here is is called foreign aid is messenger RNA it

336
00:19:36,200 --> 00:19:41,220
turns out that there are many other functions for and which are not cover so

337
00:19:41,240 --> 00:19:50,150
are they are in republic see because it is the chemical composition of the molecule

338
00:19:50,360 --> 00:19:55,530
one be used for the main use of is to to the world in terms

339
00:19:55,720 --> 00:20:00,010
of mediation between DNA protein but it turns out that the sun

340
00:20:00,020 --> 00:20:05,150
that's what i was looking the same single strand of RNA can have different functions

341
00:20:05,150 --> 00:20:10,140
to do this but also there are transfer RNA there are

342
00:20:10,160 --> 00:20:15,070
a small are in four generations to right so i think the name

343
00:20:15,390 --> 00:20:17,410
he right and RNA a

344
00:20:18,230 --> 00:20:21,830
because they are in not here to write is messenger RNA

345
00:20:21,840 --> 00:20:28,500
there are there are names which have different functions

346
00:20:28,520 --> 00:20:30,440
so i said that the

347
00:20:30,510 --> 00:20:33,100
in this in the central to the image of l

348
00:20:33,100 --> 00:20:36,150
this is called the commission to make proteins

349
00:20:36,160 --> 00:20:42,370
so it could be what proteins are one thing because you are not going to

350
00:20:42,380 --> 00:20:47,670
so putting the chemistry once again is the output variable is an assembly of basic

351
00:20:47,670 --> 00:20:48,740
building blocks

352
00:20:48,770 --> 00:20:54,850
but the attachments are also in your building blocks which are called losses in this

353
00:20:55,820 --> 00:20:59,500
and what you assemble like you think protein

354
00:20:59,530 --> 00:21:06,950
so i wrote this like a sequence of amino acids and find these these things

355
00:21:07,290 --> 00:21:12,170
when you when you put in the solution will take some specific three-dimensional structure like

356
00:21:12,170 --> 00:21:14,650
the one we have here is that this is the

357
00:21:15,330 --> 00:21:21,280
and this is the traditional for protein is is that of relying cities but because

358
00:21:21,280 --> 00:21:23,790
of the composition the weights male

359
00:21:23,820 --> 00:21:28,470
based on specific through social interactions with have function

360
00:21:28,490 --> 00:21:33,000
OK so to begin with take model based on the life and then some configuration

361
00:21:33,000 --> 00:21:35,190
in three d and the final three d

362
00:21:35,320 --> 00:21:37,570
structure and the

363
00:21:37,890 --> 00:21:43,050
chemical properties of this process will allow it to to sequence one

364
00:21:43,080 --> 00:21:48,110
so far as what important that the sequence of events and in fact there are

365
00:21:48,130 --> 00:21:53,070
so i said they are for the intermediate for building blocks in case of proteins

366
00:21:53,080 --> 00:21:55,570
there are twenty amino acids

367
00:21:55,580 --> 00:22:00,240
so for each protein sequence over the alphabet of size twenty

368
00:22:01,960 --> 00:22:04,340
i will not the twenty one

369
00:22:04,340 --> 00:22:05,720
but the data

370
00:22:05,740 --> 00:22:06,690
so the

371
00:22:06,690 --> 00:22:14,640
o thing

372
00:22:24,980 --> 00:22:35,270
thank you for having me here this

373
00:22:36,140 --> 00:22:40,660
to open this conferences workshops

374
00:22:40,770 --> 00:22:45,890
i'm going to talk today about

375
00:22:45,900 --> 00:22:49,430
search technology that's what i primarily work on

376
00:22:49,490 --> 00:22:51,960
two most of here

377
00:22:52,010 --> 00:22:56,270
but with an emphasis on

378
00:22:56,290 --> 00:23:02,700
one hand technically projects that i've worked on it also on the open source

379
00:23:02,780 --> 00:23:05,950
methodology and and sort of how that works

380
00:23:05,960 --> 00:23:09,640
i'm sure it's something

381
00:23:09,660 --> 00:23:12,050
all you're somewhat familiar with

382
00:23:12,500 --> 00:23:15,530
and maybe even participated in some

383
00:23:15,540 --> 00:23:17,550
but there are some

384
00:23:17,630 --> 00:23:21,730
the more important the more i learned about some subtle piece of what really makes

385
00:23:21,730 --> 00:23:24,740
open source work and liked

386
00:23:24,790 --> 00:23:27,680
talk a bit about that in the process

387
00:23:27,920 --> 00:23:32,430
so the first one of my time

388
00:23:32,480 --> 00:23:35,990
not really an academic i suspect that many of you

389
00:23:36,010 --> 00:23:39,180
are academics and researchers

390
00:23:39,230 --> 00:23:41,820
i used to be one of those

391
00:23:41,940 --> 00:23:43,420
some years ago

392
00:23:43,660 --> 00:23:47,430
our to xerox research centre in palo alto

393
00:23:47,480 --> 00:23:48,930
five years and

394
00:23:49,320 --> 00:23:50,490
papers and

395
00:23:50,530 --> 00:23:52,040
that all that good stuff

396
00:23:52,540 --> 00:23:58,990
and i'm not a commercial software developer i haven't worked on

397
00:23:59,040 --> 00:24:01,610
proprietary commercial software

398
00:24:01,660 --> 00:24:07,000
in a few years anyway but i used to do that flows well so that

399
00:24:07,000 --> 00:24:11,050
the the the title on the card is the space is open source guy

400
00:24:11,140 --> 00:24:16,160
and i to the site was on the cards so that's what i call

401
00:24:16,290 --> 00:24:21,900
and so on in that with had i mean i'm a developer software developer and

402
00:24:21,900 --> 00:24:24,540
architect to some degree doing research

403
00:24:24,620 --> 00:24:27,080
so the manager of other people

404
00:24:27,090 --> 00:24:30,860
and to some degree and i'm an evangelist trying to

405
00:24:30,940 --> 00:24:32,990
to each other people about the

406
00:24:33,100 --> 00:24:37,240
how open source works i'm not all i don't think it's the only way to

407
00:24:37,240 --> 00:24:40,130
do things i don't think this

408
00:24:40,300 --> 00:24:43,190
lots of lots of other

409
00:24:43,200 --> 00:24:47,730
you a valid way to develop software so

410
00:24:47,820 --> 00:24:50,720
why then what happens

411
00:24:50,730 --> 00:24:52,970
to distinguish from

412
00:24:53,020 --> 00:24:54,430
the other way

413
00:24:54,440 --> 00:24:57,320
so this

414
00:24:57,330 --> 00:24:58,350
different things like you

415
00:24:58,370 --> 00:24:59,970
might think distinguish

416
00:24:59,980 --> 00:25:06,450
the first one to go visit the licence you you you might think that just

417
00:25:06,450 --> 00:25:09,460
putting software under an OSI approved

418
00:25:09,470 --> 00:25:11,210
license releasing it that way

419
00:25:11,220 --> 00:25:13,330
makes are technically

420
00:25:13,370 --> 00:25:18,140
that's all that's really required i mean that's that's probably the most precise definition of

421
00:25:18,430 --> 00:25:20,090
open source

422
00:25:20,380 --> 00:25:23,900
he to me that there's a little more to it than that

423
00:25:23,950 --> 00:25:25,840
and that is implied

424
00:25:25,890 --> 00:25:28,850
other ways to compare

425
00:25:28,860 --> 00:25:36,360
open source to doing research on and around reading commercial proprietary software easily to caricature

426
00:25:36,390 --> 00:25:37,490
so please don't

427
00:25:37,500 --> 00:25:39,830
take this too literally and these really are

428
00:25:39,850 --> 00:25:42,100
either orthogonal models

429
00:25:42,110 --> 00:25:44,970
there are many ways in which they can be combined the this just

430
00:25:45,050 --> 00:25:47,980
to be clustered i don't want to offend anyone

431
00:25:48,030 --> 00:25:53,530
so you know researcher can you can really things under various licenses in the same

432
00:25:53,560 --> 00:25:55,270
commercial companies

433
00:25:55,290 --> 00:26:01,400
really things under various licenses under the license alone does uniquely identify what i'm talking

434
00:26:01,400 --> 00:26:02,880
about open source

435
00:26:02,990 --> 00:26:04,670
software development

436
00:26:04,990 --> 00:26:11,380
this is the artifacts producing you know in research the primary artifact factors is arguably

437
00:26:11,380 --> 00:26:13,160
the publication

438
00:26:13,290 --> 00:26:15,260
that's something people

439
00:26:15,310 --> 00:26:17,970
are we are really after an

440
00:26:18,010 --> 00:26:22,840
the software is is a means of publication i would argue in most research communities

441
00:26:23,380 --> 00:26:25,560
and and same in

442
00:26:25,630 --> 00:26:31,290
the racial barriers products patterns that those things that you're trying to build and the

443
00:26:31,290 --> 00:26:32,810
source code itself is

444
00:26:32,850 --> 00:26:36,440
it means where is an open source you really focusing on the code so that

445
00:26:36,440 --> 00:26:38,080
and i'm going to discourage this

446
00:26:38,140 --> 00:26:39,470
so instead of just

447
00:26:39,480 --> 00:26:44,230
fitting how well i separate out my original training data how well i fit regression

448
00:26:44,230 --> 00:26:45,790
surface in

449
00:26:45,840 --> 00:26:49,890
my original training data i'm going to have a trade-off between that

450
00:26:49,950 --> 00:26:53,640
and how we can my surfaces

451
00:26:54,540 --> 00:26:56,830
so that i think finally brings the

452
00:26:56,830 --> 00:26:59,310
this final slide the first time

453
00:26:59,320 --> 00:27:05,640
that trade-off we fitted an objective which was something called which was the

454
00:27:05,900 --> 00:27:09,150
the objective that says how well we fit the training data and then i had

455
00:27:09,150 --> 00:27:10,270
something like

456
00:27:10,280 --> 00:27:13,130
slammed the sum of

457
00:27:13,140 --> 00:27:16,460
a bunch of white

458
00:27:17,520 --> 00:27:20,410
this the free parameter here that said

459
00:27:20,460 --> 00:27:21,770
how do you

460
00:27:21,770 --> 00:27:25,950
how do you decide where to set the trade-off between fitting data well and coming

461
00:27:25,950 --> 00:27:28,710
up with sort of simpler explanations

462
00:27:28,720 --> 00:27:34,140
and one way to do that was to just try a whole bunch of different

463
00:27:34,190 --> 00:27:36,440
hold out some data at the beginning

464
00:27:36,450 --> 00:27:41,610
which your fitting procedure doesn't see and then see which is the best lambda which

465
00:27:41,650 --> 00:27:45,500
lambda generalizes to new data that hasn't been seen yet the best and then you

466
00:27:45,500 --> 00:27:48,560
go with that and that something simple you can do

467
00:27:48,690 --> 00:27:51,830
that's not the only thing you can do

468
00:27:51,860 --> 00:27:55,220
so that's what we're going to resume

469
00:27:55,230 --> 00:28:02,110
other any questions

470
00:28:05,240 --> 00:28:08,060
i promise i was going to a broad overview of machine learning

471
00:28:08,100 --> 00:28:11,290
it's not really possible for me to do that because unfortunately i don't have a

472
00:28:11,290 --> 00:28:14,180
deep knowledge of everything and

473
00:28:14,200 --> 00:28:15,480
one of the

474
00:28:15,490 --> 00:28:19,680
large important areas of machine learning that i'm not really an expert in a

475
00:28:19,700 --> 00:28:21,600
statistical learning theory say

476
00:28:21,610 --> 00:28:25,360
there's a lot of statistical learning theory in the major machine learning conferences but it

477
00:28:25,360 --> 00:28:27,710
also has a conference it's a cult

478
00:28:28,700 --> 00:28:31,310
one of the big questions in this area

479
00:28:31,350 --> 00:28:34,830
concerned with is computing bounds on

480
00:28:34,910 --> 00:28:36,420
generalisation error

481
00:28:37,760 --> 00:28:39,500
donal this stuff

482
00:28:39,530 --> 00:28:43,880
and i picked w's and lambda is in some way

483
00:28:44,240 --> 00:28:45,800
and i run some kate

484
00:28:45,810 --> 00:28:47,710
given some training data

485
00:28:47,760 --> 00:28:51,470
but what i really care about is when i play the system

486
00:28:51,480 --> 00:28:53,740
how well is it going to work on future data

487
00:28:55,810 --> 00:28:57,500
it's impossible to now

488
00:28:57,510 --> 00:28:59,280
that because in

489
00:28:59,290 --> 00:29:02,180
you know the world could spit anything after that

490
00:29:02,210 --> 00:29:06,310
but what they can do is give probabilistic guarantees they can say well

491
00:29:06,330 --> 00:29:09,380
with this probability you're going to do no worse than

492
00:29:09,460 --> 00:29:12,670
and this threshold and

493
00:29:12,690 --> 00:29:16,150
that's very nice thing to have it's very nice to be able to give your

494
00:29:16,710 --> 00:29:21,000
the person your consulting for some guarantee that this machine your handing them will do

495
00:29:21,000 --> 00:29:22,880
something useful

496
00:29:24,080 --> 00:29:28,920
one of the simplest ways i sort of understand how to do is to construct

497
00:29:28,920 --> 00:29:32,460
that sort of guarantee you think the validation set so

498
00:29:33,580 --> 00:29:35,470
you held out some data

499
00:29:37,200 --> 00:29:40,210
nothing you did sawa

500
00:29:40,230 --> 00:29:41,230
at all

501
00:29:41,250 --> 00:29:43,800
so i held at some data i put it on

502
00:29:43,810 --> 00:29:47,580
another directory of my hard disk i didn't peak it it even for a moment

503
00:29:47,600 --> 00:29:49,230
in my whole fitting procedure

504
00:29:49,250 --> 00:29:52,790
and then right at the end for my system that i'm about to deploy

505
00:29:52,830 --> 00:29:54,870
i run that on my held out data

506
00:29:54,930 --> 00:29:59,470
and see how well it did it's possible to construct bounds on that performance so

507
00:29:59,540 --> 00:30:00,440
here is

508
00:30:00,450 --> 00:30:03,790
a sample of how well the machine will be one future data that i haven't

509
00:30:03,790 --> 00:30:04,960
seen it all

510
00:30:04,960 --> 00:30:08,100
that gives you an estimate of how well it will perform in confidence bounds on

511
00:30:08,100 --> 00:30:10,300
that fairly easily so that sort of the

512
00:30:10,340 --> 00:30:12,280
easier space of learning theory

513
00:30:12,290 --> 00:30:13,430
it's important

514
00:30:13,440 --> 00:30:15,580
it's in some ways the least interesting

515
00:30:15,630 --> 00:30:21,560
i described k fold cross validation to which is something where you have multiple validation

516
00:30:21,560 --> 00:30:26,200
sets do peak at a bit while you're running much harder to analyse those overlay

517
00:30:26,210 --> 00:30:29,580
there are papers on that area

518
00:30:29,580 --> 00:30:33,210
well learning theory gets excited is

519
00:30:33,250 --> 00:30:34,830
when they

520
00:30:34,830 --> 00:30:40,090
try to compute bounds based not on held out validation set which is philosophically

521
00:30:40,100 --> 00:30:44,360
not very satisfying because your sort of not using all of your data while you're

522
00:30:46,200 --> 00:30:50,980
there are also found you can construct based purely on test on training set performance

523
00:30:53,150 --> 00:30:54,330
i e

524
00:30:54,400 --> 00:30:55,680
optimize some

525
00:30:55,710 --> 00:31:01,240
regularized objectives and because of the regularizer i won't get perfect performance will be some

526
00:31:02,350 --> 00:31:04,910
which are on the wrong side of my decision line

527
00:31:04,960 --> 00:31:05,780
and so

528
00:31:05,790 --> 00:31:07,220
the training error

529
00:31:07,230 --> 00:31:09,040
the wind b

530
00:31:09,600 --> 00:31:13,030
zero and a lot of sense regularizers

531
00:31:13,720 --> 00:31:18,910
summary of learning tree will do without take place training errors and health

532
00:31:19,210 --> 00:31:22,310
up quite this and say well that's related to the error you're going to get

533
00:31:22,310 --> 00:31:26,330
at test time you'll probably do that west test time because some data have two

534
00:31:26,610 --> 00:31:28,280
and they can tell you how much worse

535
00:31:28,370 --> 00:31:32,360
as it bounces along bounds on generalisation error

536
00:31:34,130 --> 00:31:38,530
that's sort of i philosophically pleasing that's a nice thing to do than having to

537
00:31:38,530 --> 00:31:43,170
hold out validation sets which is a bit dirty it's kind of interesting because

538
00:31:43,210 --> 00:31:46,430
it then tells you what sort of possible to learn because you bounds on whether

539
00:31:46,430 --> 00:31:48,880
it's possible to generalize

540
00:31:48,930 --> 00:31:51,680
in the given problem

541
00:31:51,700 --> 00:31:56,900
the difficulty for practical purposes often found very loose sense

542
00:31:56,900 --> 00:32:01,590
and the reason is that they start are

543
00:32:01,610 --> 00:32:04,800
one of the huffman code

544
00:32:04,820 --> 00:32:07,650
yes you within plus one

545
00:32:07,670 --> 00:32:08,570
he has this

546
00:32:08,590 --> 00:32:12,380
less than n plus one and that plus one dominates

547
00:32:12,420 --> 00:32:19,020
so you're miles off from the actual entropy for many distributions such huffman start because

548
00:32:19,020 --> 00:32:22,880
the symbol code and symbol code such in that case

549
00:32:22,880 --> 00:32:25,920
so it's bad for

550
00:32:25,980 --> 00:32:27,550
extreme distributions

551
00:32:27,570 --> 00:32:31,300
and if you're doing a good job of your machine learning and making strong predictions

552
00:32:31,300 --> 00:32:34,710
i as i know the next sentence of jane austen's emma because i figured out

553
00:32:34,710 --> 00:32:36,360
the plot

554
00:32:36,380 --> 00:32:38,650
you get extreme distributions

555
00:32:38,670 --> 00:32:42,650
OK so have been such

556
00:32:44,070 --> 00:32:45,020
in addition

557
00:32:45,020 --> 00:32:48,420
the probability distribution changes

558
00:32:48,420 --> 00:32:51,880
and the changes for two reasons

559
00:32:51,900 --> 00:32:54,960
it changes with the context

560
00:32:54,960 --> 00:33:00,940
just got context dependent predictions and it changes because we are learning as we go

561
00:33:01,000 --> 00:33:04,300
so if i told you i have been calling here and i want you to

562
00:33:04,300 --> 00:33:06,940
compress the outcome from the bent coin but i don't know what the bias is

563
00:33:06,940 --> 00:33:10,150
and i start talking heads tails

564
00:33:10,170 --> 00:33:13,000
you want to learn on the fly what bias the coin is to make an

565
00:33:13,000 --> 00:33:17,030
optimal compressor and you could do something really ugly like listen to the whole file

566
00:33:17,050 --> 00:33:22,710
work out some optimal code some complicated sort then you have the header and things

567
00:33:22,710 --> 00:33:25,030
like that but maybe there's a better way

568
00:33:25,050 --> 00:33:29,570
a final complaint about symbol code is if the alphabet is

569
00:33:29,650 --> 00:33:31,440
zero one

570
00:33:31,460 --> 00:33:36,110
then the optimal symbol code is zero one you don't get any compression so binary

571
00:33:36,110 --> 00:33:39,570
files you've got to do something else anyway

572
00:33:39,590 --> 00:33:43,730
what we want is we want a compression method that solves all of these complaints

573
00:33:43,730 --> 00:33:48,050
we just made here we want to method that worked absolutely fine for extreme distributions

574
00:33:48,370 --> 00:33:53,690
we method that works fine even if the probability distribution changes the context naturally fits

575
00:33:53,690 --> 00:33:57,480
into that way of doing things we want to method is perfectly compatible with learning

576
00:33:57,480 --> 00:34:02,610
because we believe data compression is machine learning and we should be doing probabilistic modeling

577
00:34:02,960 --> 00:34:05,150
for anyone who denies that

578
00:34:05,170 --> 00:34:09,940
if you have a compressor if you've written a compression algorithm you have define probabilities

579
00:34:09,940 --> 00:34:14,250
whether you like it or not because if your compressor encodes

580
00:34:14,730 --> 00:34:16,340
a file

581
00:34:18,670 --> 00:34:23,510
and encode things of x which is a string of bits you have implicitly defined

582
00:34:23,510 --> 00:34:28,170
the probability distribution p of this thing has a length l of x define the

583
00:34:28,170 --> 00:34:33,230
problem is due to minus lx whether you like it or not your compressor is

584
00:34:33,230 --> 00:34:38,980
defining probabilities so you better get used to the idea of data compression is probabilistic

585
00:34:38,980 --> 00:34:40,290
modeling because

586
00:34:40,420 --> 00:34:42,020
that's the interpretation

587
00:34:42,020 --> 00:34:44,500
and it makes the whole thing makes sense

588
00:34:45,710 --> 00:34:50,520
so here is what good is going to assume that we've got an oracle our

589
00:34:51,300 --> 00:34:55,320
let's go back to this game just one moment

590
00:34:55,340 --> 00:34:59,880
here's how arithmetic coding is going to work is going to work on the basis

591
00:34:59,880 --> 00:35:01,300
of this guessing game

592
00:35:01,300 --> 00:35:05,340
plus an extra assumption please assume that through well over there

593
00:35:05,340 --> 00:35:07,790
there's another audience

594
00:35:07,800 --> 00:35:10,090
full of all your identical twins

595
00:35:10,110 --> 00:35:12,360
and my identical twins

596
00:35:12,380 --> 00:35:15,020
an identical twin

597
00:35:15,050 --> 00:35:16,210
comes in

598
00:35:16,380 --> 00:35:18,090
so we're gonna play guessing game

599
00:35:18,110 --> 00:35:21,400
and what actually happened just before all you

600
00:35:21,400 --> 00:35:24,130
identical twins an identical twin

601
00:35:24,150 --> 00:35:25,210
just before

602
00:35:25,230 --> 00:35:29,090
these numbers seventeen one two one three one one one two one one one seven

603
00:35:29,090 --> 00:35:32,480
two two two one two one one nine one two one have been written in

604
00:35:32,480 --> 00:35:36,380
this book and pass through the whole here and i come in the identical thing

605
00:35:36,380 --> 00:35:38,940
comes in and you're identical to say

606
00:35:38,960 --> 00:35:42,820
i have got here before me the headline generating book

607
00:35:42,840 --> 00:35:46,920
please guess the first letter the headline and written in front of me is the

608
00:35:46,920 --> 00:35:48,300
number seventeen

609
00:35:48,320 --> 00:35:53,670
and guess i say no to you know do you know that people saying no

610
00:35:53,670 --> 00:35:59,300
discounting one sixteen seventeen p and all i need to know is the number

611
00:35:59,320 --> 00:36:01,460
and i can put up p

612
00:36:01,480 --> 00:36:02,570
and then

613
00:36:02,590 --> 00:36:08,190
use you read one and someone says i say yes that's right because you're all

614
00:36:08,190 --> 00:36:12,020
perfectly died identical twins and i'm following this rule here

615
00:36:12,030 --> 00:36:15,300
we end up without me knowing anything about what's going on is all thanks to

616
00:36:15,300 --> 00:36:16,230
your expertise

617
00:36:16,690 --> 00:36:19,050
so that we end up with this headline

618
00:36:19,070 --> 00:36:21,500
so identical twins give away

619
00:36:21,500 --> 00:36:23,960
o think compression and compression

620
00:36:23,980 --> 00:36:26,570
now that science fiction

621
00:36:26,590 --> 00:36:31,730
but one thing is that computers are science fiction you can make one computer that

622
00:36:31,730 --> 00:36:35,360
builds the probability model does machine learning it makes predictions and you can have an

623
00:36:35,360 --> 00:36:39,530
identical twin computer runs exactly the same algorithm and make predictions at the other end

624
00:36:39,630 --> 00:36:44,570
and that's the principle after encoding uses we assume that the encoder and the decoder

625
00:36:44,570 --> 00:36:49,790
will both have exactly the same identical twin probabilistic model which you can ask for

626
00:36:49,790 --> 00:36:53,250
any given context what's the probability

627
00:36:54,270 --> 00:36:58,250
the next character being a b c d and you know little probability distribution out

628
00:36:58,250 --> 00:36:59,230
of your expert

629
00:36:59,230 --> 00:37:00,790
so what we do

630
00:37:00,790 --> 00:37:06,130
we ask the expert please tell me the probability the first character being a to

631
00:37:07,150 --> 00:37:11,250
and we get the probabilities and i hope you can see this is a b

632
00:37:11,250 --> 00:37:16,110
c d e f g is the back can you read letters

633
00:37:16,150 --> 00:37:17,190
it right

634
00:37:17,210 --> 00:37:21,650
OK and we take the probability of a the oracle has provided the oracle is

635
00:37:21,650 --> 00:37:26,090
this friendly audience inside the box the reliably you tell us the context so the

636
00:37:26,090 --> 00:37:29,610
context is nothing we haven't got the fact that was the first country give me

637
00:37:29,610 --> 00:37:34,650
the probability of can we take this probabilities and we represent them vertically and order

638
00:37:34,670 --> 00:37:39,860
them like arithmetic interval also the height of this green boxes one and the height

639
00:37:39,860 --> 00:37:44,130
of a little blue box within a written in is the probability of a according

640
00:37:44,130 --> 00:37:45,880
to the article

641
00:37:45,900 --> 00:37:49,940
and we can also take exactly the same info from zero to one we can

642
00:37:49,940 --> 00:37:52,230
subdivide it using binary

643
00:37:52,250 --> 00:37:57,400
with zero top in one second so one neuron so can keep subdividing

644
00:37:57,400 --> 00:38:01,650
and any one of these little strings here say one one

645
00:38:01,710 --> 00:38:04,460
zero one

646
00:38:06,050 --> 00:38:10,440
one which is what this box corresponds to here

647
00:38:10,460 --> 00:38:14,250
defines an interval of the line so you can interpret the binary string as defining

648
00:38:14,250 --> 00:38:16,960
an interval for example the box called one

649
00:38:17,020 --> 00:38:19,020
is the interval from harm

650
00:38:19,050 --> 00:38:23,050
to one if we call the top of the box there in the bottom one

651
00:38:23,050 --> 00:38:26,880
and similarly every letter over here has got an interval associated with it

652
00:38:26,900 --> 00:38:29,530
and we can recursively subdividing can say

653
00:38:29,550 --> 00:38:30,940
let's imagine we're trying to

654
00:38:36,230 --> 00:38:41,550
the drug so we're going to compress

655
00:38:41,650 --> 00:38:45,610
there are the tide has meant

656
00:38:45,610 --> 00:38:48,300
and a group of other people added faulty

657
00:38:48,300 --> 00:38:50,280
so eighteen t has this

658
00:38:50,760 --> 00:38:56,960
or had better but still present has this business called natural voices

659
00:38:57,010 --> 00:39:00,940
which is very realistic sounding text-to-speech engine

660
00:39:00,940 --> 00:39:03,510
so you give the text and outputs

661
00:39:03,590 --> 00:39:06,050
realistic sounding speech

662
00:39:06,070 --> 00:39:08,150
so what eighteen he wanted to do

663
00:39:08,170 --> 00:39:11,990
it is to come up with like storefronts or help desk where people could call

664
00:39:11,990 --> 00:39:14,920
up and have a fairly natural dialogue

665
00:39:15,860 --> 00:39:20,630
this system so the car could call up and ask for demo and pricing information

666
00:39:20,630 --> 00:39:23,440
technical support and so on

667
00:39:23,460 --> 00:39:28,210
and one of the color to be able to have an interactive dialogue with the

668
00:39:28,220 --> 00:39:31,630
system so i can

669
00:39:31,710 --> 00:39:35,340
play one of these dialogues they can see what sounds like

670
00:39:38,610 --> 00:39:43,110
so you can hear it i'm supposed to take my microphones in this

671
00:39:44,090 --> 00:39:48,150
so here's here's what one of those dialogue sounds like so you'll hear a human

672
00:39:48,150 --> 00:39:52,220
voice which is the male voice and the synthesized voice which is the woman's voice

673
00:39:52,220 --> 00:39:53,420
to name is crystal

674
00:39:53,440 --> 00:39:58,860
how can i help you

675
00:39:58,880 --> 00:40:01,610
we like demonstration

676
00:40:01,630 --> 00:40:07,490
sure happy with

677
00:40:07,530 --> 00:40:12,050
i don't know what the english going on or challenge

678
00:40:12,070 --> 00:40:16,480
or i can give you an idea of what can been going down like

679
00:40:16,480 --> 00:40:18,320
which would you prefer

680
00:40:23,400 --> 00:40:24,940
o point for you

681
00:40:25,110 --> 00:40:27,190
one of our going on

682
00:40:27,210 --> 00:40:29,320
called which has the power to

683
00:40:31,190 --> 00:40:32,880
here it is

684
00:40:32,900 --> 00:40:37,130
the mariners finished the first half of the season with the rest of the paper

685
00:40:39,780 --> 00:40:42,280
i'd like to customer voice

686
00:40:42,300 --> 00:40:47,030
the red

687
00:40:47,090 --> 00:40:49,510
which is what we are going on

688
00:40:49,760 --> 00:40:57,380
of which has the power to appoint down here finish the first half of the

689
00:40:57,380 --> 00:41:01,380
people with the rest of the paper

690
00:41:01,400 --> 00:41:06,340
we vanished

691
00:41:06,360 --> 00:41:09,900
OK all the very

692
00:41:09,920 --> 00:41:12,880
it's not on the market right now it will be the

693
00:41:12,880 --> 00:41:14,720
language we get out

694
00:41:14,740 --> 00:41:16,960
female voice

695
00:41:17,380 --> 00:41:24,380
from nineteen ninety seven i got that you don't to go the people

696
00:41:25,460 --> 00:41:27,460
we need custom again

697
00:41:29,340 --> 00:41:31,510
all of the boys

698
00:41:34,690 --> 00:41:37,590
OK so you get the idea

699
00:41:37,610 --> 00:41:41,150
how one of these dialogues go

700
00:41:41,210 --> 00:41:42,570
can hear me

701
00:41:44,070 --> 00:41:56,990
OK so

702
00:41:57,010 --> 00:41:59,070
can you hear me OK

703
00:41:59,070 --> 00:42:01,420
yes stop when you cannot

704
00:42:01,490 --> 00:42:05,320
OK so how does the system like this work

705
00:42:05,380 --> 00:42:07,990
that is a dialogue system like this work

706
00:42:08,030 --> 00:42:12,280
well you've got the human over here and the human says something

707
00:42:12,300 --> 00:42:17,710
and that utterance that speech signal gets passed through an automatic speech recognizer

708
00:42:17,760 --> 00:42:19,710
which converted to text

709
00:42:19,710 --> 00:42:22,300
very noisy text

710
00:42:22,360 --> 00:42:27,780
then that text passed two was called the natural language understanding unit

711
00:42:27,780 --> 00:42:32,110
and the job of the NLU is to try to extract meaning from what was

712
00:42:33,320 --> 00:42:34,800
to the extent

713
00:42:35,880 --> 00:42:40,010
whatever was said can be placed into one of the set of categories and the

714
00:42:40,030 --> 00:42:46,030
categories are things like once the here demo wants because sales representative and so on

715
00:42:46,670 --> 00:42:52,880
predicted category gets as the dialogue manager of the dialogue managers job is to keep

716
00:42:52,880 --> 00:42:58,150
track of the state of the dialog for instance what question was recently asked

717
00:42:58,170 --> 00:43:02,110
and the dialogue manager takes as input that predicted category

718
00:43:02,170 --> 00:43:08,240
and converts it and formulate a text response of some kind

719
00:43:08,280 --> 00:43:12,900
that text response gets converted into computer speech which gives way to the human

720
00:43:12,920 --> 00:43:15,090
and the whole thing has to happen in

721
00:43:15,130 --> 00:43:17,300
real time

722
00:43:18,420 --> 00:43:23,740
the part that we're we're using boosting is for this natural language understanding unit

723
00:43:23,740 --> 00:43:25,760
where we basically are just taking

724
00:43:25,800 --> 00:43:30,860
text which comes out of this speech recogniser and trying to categorise

725
00:43:30,900 --> 00:43:32,900
so it seems on the surface to be

726
00:43:32,920 --> 00:43:36,460
just standard text categorisation problem

727
00:43:36,490 --> 00:43:38,610
and to a large extent that's true

728
00:43:38,650 --> 00:43:41,280
but there are some things that made it a particular

729
00:43:41,320 --> 00:43:43,780
particularly interesting problem

730
00:43:43,800 --> 00:43:45,150
in this case

731
00:43:45,170 --> 00:43:46,490
so in this case

732
00:43:46,490 --> 00:43:50,920
one of the goals of the project was not just to get the system to

733
00:43:51,800 --> 00:43:57,150
but to get a system that works as quickly and cheaply as possible

734
00:43:57,190 --> 00:43:59,460
so in order to do that

735
00:43:59,480 --> 00:44:01,550
it's very difficult

736
00:44:01,570 --> 00:44:03,530
i'm sorry so

737
00:44:03,530 --> 00:44:06,150
in order to train

738
00:44:06,210 --> 00:44:08,050
architects categorizer

739
00:44:08,070 --> 00:44:10,820
we need to have a lot of data

740
00:44:10,840 --> 00:44:14,510
and in order to get a lot of data it takes time

741
00:44:14,550 --> 00:44:18,280
get all of that data so because you need a lot of data

742
00:44:18,340 --> 00:44:19,420
that makes

743
00:44:20,110 --> 00:44:24,510
goal of cheap rapid deployment very difficult

744
00:44:24,530 --> 00:44:28,610
so it's kind of a chicken and egg problem because

745
00:44:29,090 --> 00:44:33,380
we need in order to train our text categorizer

746
00:44:33,400 --> 00:44:36,320
we need to have a lot of labeled data

747
00:44:36,320 --> 00:44:39,300
so that we can train and deploy the system

748
00:44:39,300 --> 00:44:42,570
but on the other hand to get that labelled data we have to have the

749
00:44:42,570 --> 00:44:46,300
system already deployed so that we can be collecting data

750
00:44:46,300 --> 00:44:48,630
so it's kind of a chicken and egg problem

751
00:44:48,650 --> 00:44:50,860
so what we did was to

752
00:44:50,880 --> 00:44:55,760
try to use human knowledge to compensate for insufficient data

753
00:44:55,780 --> 00:45:00,440
so for these kinds of problems a human can pretty well guess a lot of

754
00:45:00,440 --> 00:45:05,420
these prediction rules right a lot of these weak classifiers human can say

755
00:45:05,420 --> 00:45:09,710
well if the person says demo then they probably wanted them out as a sales

756
00:45:09,710 --> 00:45:13,690
representative they probably want to speak to a sales representative

757
00:45:13,690 --> 00:45:17,190
and so on so a human can come up with the whole lot

758
00:45:17,240 --> 00:45:19,590
of these rough rules of

759
00:45:19,610 --> 00:45:23,880
and then the challenge is how to take this prior knowledge which has been built

760
00:45:23,880 --> 00:45:25,300
by humans

761
00:45:25,320 --> 00:45:26,720
and combine it

762
00:45:26,740 --> 00:45:29,590
with the the training data which is available

763
00:45:29,590 --> 00:45:31,050
so as to

764
00:45:31,120 --> 00:45:35,840
take advantage of both sources of data both the training data

765
00:45:35,840 --> 00:45:39,320
and they human crafted rules

766
00:45:39,340 --> 00:45:42,340
so to do that what we did was we took the loss function

767
00:45:42,380 --> 00:45:44,900
which is being minimized by adaboost

768
00:45:44,960 --> 00:45:47,920
and we modified that loss function

769
00:45:47,920 --> 00:45:50,420
so that rather than just trying to fit

770
00:45:50,460 --> 00:45:52,610
the training data that we have

771
00:45:52,630 --> 00:45:53,990
we kind of wanted to

772
00:45:54,030 --> 00:45:59,380
balance the the that training data against fit to the prior model which has built

773
00:45:59,440 --> 00:46:02,050
and built by the human

774
00:46:02,050 --> 00:46:03,280
so this

775
00:46:03,280 --> 00:46:07,150
it seems to work really nicely so for instance this is on

776
00:46:07,170 --> 00:46:12,340
another text categorisation dataset called the EP titled dataset

777
00:46:12,400 --> 00:46:19,150
so this green curve shows how well you the human crafted rules work for this

778
00:46:20,510 --> 00:46:23,570
and the blue curve is selling

779
00:46:23,570 --> 00:46:28,880
how well boosting works when we're using only the training data which is available

780
00:46:28,920 --> 00:46:32,740
as a function of the amount of training data that we have

781
00:46:32,780 --> 00:46:34,570
at our disposal

782
00:46:35,690 --> 00:46:39,960
and the red curve is showing what happens when we combine both of these together

783
00:46:39,980 --> 00:46:42,190
and what you see is that

784
00:46:42,240 --> 00:46:43,650
are fairly small

785
00:46:43,670 --> 00:46:48,340
amount of training examples like a few hundred train up to a few hundred training

786
00:46:49,760 --> 00:46:55,550
you getting big improvement by incorporating this prior knowledge

787
00:46:55,570 --> 00:46:59,340
OK we also applied to this help desk data

788
00:46:59,340 --> 00:47:01,440
the first one is the data term

789
00:47:01,440 --> 00:47:04,050
experts suppose x which is the usual

790
00:47:04,070 --> 00:47:05,690
term which measures

791
00:47:05,710 --> 00:47:09,590
the coherency of the clusters

792
00:47:09,650 --> 00:47:11,960
and then we have regularisation to

793
00:47:11,980 --> 00:47:13,170
this one

794
00:47:13,250 --> 00:47:14,090
this is a

795
00:47:14,110 --> 00:47:15,570
interesting because

796
00:47:15,610 --> 00:47:19,360
this is a concave function over standard simplex

797
00:47:19,380 --> 00:47:22,840
it is maximized the virus and

798
00:47:22,860 --> 00:47:26,030
so just to get into the picture

799
00:47:26,050 --> 00:47:27,440
what happens

800
00:47:27,440 --> 00:47:30,670
when alpha is big enough

801
00:47:30,690 --> 00:47:33,520
large and when alpha is low

802
00:47:33,570 --> 00:47:35,050
then this

803
00:47:35,050 --> 00:47:37,000
will dominate history

804
00:47:37,050 --> 00:47:38,840
so in one sense

805
00:47:38,880 --> 00:47:42,770
we do expect that the objective function becomes concave

806
00:47:42,860 --> 00:47:44,020
this is

807
00:47:44,050 --> 00:47:48,130
picture i think it what happens when a lot of two terms one is very

808
00:47:48,130 --> 00:47:50,860
large with respect to dominate

809
00:47:50,940 --> 00:47:55,130
actually what you doing approximately just maximizing this

810
00:47:55,300 --> 00:47:57,770
this becomes negligible

811
00:47:57,840 --> 00:47:59,630
but then you are maximizing

812
00:47:59,690 --> 00:48:05,360
concave function thing

813
00:48:05,840 --> 00:48:10,230
this is man

814
00:48:18,960 --> 00:48:21,550
i didn't magic so a minus

815
00:48:34,050 --> 00:48:35,190
you are saying

816
00:48:35,210 --> 00:48:38,520
if i understand what you're saying that we are adding the same content of all

817
00:48:38,530 --> 00:48:40,790
the terms the

818
00:48:49,420 --> 00:48:53,170
an offer is only on the right

819
00:48:53,230 --> 00:48:55,000
that's right

820
00:48:58,750 --> 00:49:02,550
actually we can be even more formal than that because then this is related to

821
00:49:02,690 --> 00:49:04,190
given them

822
00:49:04,210 --> 00:49:06,480
one of the most

823
00:49:06,550 --> 00:49:07,730
so i should

824
00:49:07,770 --> 00:49:11,960
probably and the one of the next slide

825
00:49:11,960 --> 00:49:17,340
let's make an observation for what happens actually when i have this of on the

826
00:49:17,340 --> 00:49:18,730
main diagonal

827
00:49:18,730 --> 00:49:21,030
it can be seen this is equivalent

828
00:49:21,520 --> 00:49:26,440
adding of to all the off diagonal elements this is a very simple actually

829
00:49:26,530 --> 00:49:27,980
because OK here

830
00:49:28,020 --> 00:49:29,460
the vector e

831
00:49:29,480 --> 00:49:31,820
it's just that we ones

832
00:49:34,070 --> 00:49:37,050
the prime it's just metrics with old ones

833
00:49:37,070 --> 00:49:39,300
it's a simple trick to denote the metric

834
00:49:39,360 --> 00:49:40,630
don't one

835
00:49:41,270 --> 00:49:46,170
a man of i lost count you is just

836
00:49:46,190 --> 00:49:47,750
the automatic

837
00:49:47,800 --> 00:49:53,210
a month of high lost count so we are adding this constant kappa to all

838
00:49:53,290 --> 00:49:56,480
the elements in the magic

839
00:49:56,530 --> 00:49:58,860
now what happens to the objective function

840
00:49:58,860 --> 00:50:04,900
x transpose the minds of i lost this cap can be written this way

841
00:50:04,940 --> 00:50:06,610
and your ex-prime

842
00:50:06,610 --> 00:50:09,610
a mind of i x lovers

843
00:50:09,610 --> 00:50:10,690
here we show them

844
00:50:10,690 --> 00:50:15,420
count a prime x but x is the vector of the standard simplex so a

845
00:50:15,420 --> 00:50:17,360
prime x is one

846
00:50:17,360 --> 00:50:22,150
and again we have ex-prime e which is the sum of all exactly at one

847
00:50:23,050 --> 00:50:24,690
so this is kind

848
00:50:24,750 --> 00:50:27,690
so for all vectors in the simplex

849
00:50:27,710 --> 00:50:34,880
and in the diagonal on the main this myself is equivalent in terms of finding

850
00:50:34,900 --> 00:50:36,980
solution of the objective function

851
00:50:38,230 --> 00:50:40,460
constant over the over

852
00:50:41,380 --> 00:50:46,340
often over the entire element of

853
00:50:46,340 --> 00:50:50,690
so if kappa is equal to one particular when kappa is equal to all

854
00:50:50,710 --> 00:50:54,270
this means that the diagonal becomes zero

855
00:50:54,270 --> 00:50:55,880
so technically speaking

856
00:50:55,880 --> 00:50:58,880
and in this minds of all the main diagonal

857
00:50:58,920 --> 00:51:02,090
is equivalent in terms of finding the optimizers

858
00:51:02,130 --> 00:51:03,630
two having

859
00:51:03,630 --> 00:51:06,690
for two all the off diagonal elements

860
00:51:06,690 --> 00:51:10,900
in terms of the similarities and whatever what's going on we are saying that we

861
00:51:10,900 --> 00:51:14,460
are adding a constant to all the similarities

862
00:51:14,520 --> 00:51:19,530
so when alpha is very large so dominates the i j

863
00:51:19,550 --> 00:51:25,420
actually what we are doing all the pairwise distances become more or less equal

864
00:51:26,380 --> 00:51:28,690
so i will get a very large cluster

865
00:51:28,750 --> 00:51:32,130
so intuitively what we get is that when alpha is very large

866
00:51:32,130 --> 00:51:34,800
and after the final very large

867
00:51:34,820 --> 00:51:40,230
we get a very large class because all the pairwise distances become more or less

868
00:51:40,230 --> 00:51:41,460
equal to

869
00:51:41,500 --> 00:51:45,000
we can neglect AIG

870
00:51:45,020 --> 00:51:47,190
now we we can see exactly

871
00:51:48,190 --> 00:51:50,420
large means

872
00:51:50,550 --> 00:51:55,190
the first let's see what is the effect of this whole

873
00:51:55,250 --> 00:51:56,020
by the

874
00:51:56,020 --> 00:51:57,150
you know we have

875
00:51:57,210 --> 00:52:00,820
this is the original the we have just clouds on

876
00:52:00,820 --> 00:52:06,130
clearly we have three clusters if of flies large enough in this case for example

877
00:52:06,130 --> 00:52:07,520
the other fifty

878
00:52:07,550 --> 00:52:10,940
we are adding a large number of the entire

879
00:52:10,960 --> 00:52:15,460
semantics of similarity and then we get a unique large clusters

880
00:52:15,550 --> 00:52:18,710
so dominant sets actually is this one

881
00:52:18,770 --> 00:52:21,150
we are changing the scale

882
00:52:21,210 --> 00:52:23,500
twelve ways actually scale

883
00:52:23,500 --> 00:52:25,570
in the world

884
00:52:25,590 --> 00:52:29,910
and the topic of this lecture building index

885
00:52:29,930 --> 00:52:33,910
we remember from the first lectures that

886
00:52:33,940 --> 00:52:36,380
what we are doing creating

887
00:52:36,390 --> 00:52:38,550
toy search engine

888
00:52:38,570 --> 00:52:40,850
and now we

889
00:52:40,890 --> 00:52:41,990
come to

890
00:52:43,260 --> 00:52:48,400
when we need to create index and index is the structure that

891
00:52:50,070 --> 00:52:53,400
our search and so

892
00:52:54,350 --> 00:53:00,780
you don't have this slide in in my presentation in in your book because i

893
00:53:00,780 --> 00:53:05,300
simply copied this slide from my first lecture to remind you

894
00:53:06,280 --> 00:53:10,520
we have decided that it will be inverted index

895
00:53:10,540 --> 00:53:16,870
and it it is a special structures that

896
00:53:16,890 --> 00:53:17,720
i can see

897
00:53:17,740 --> 00:53:19,590
from two plaques

898
00:53:19,610 --> 00:53:23,910
for first are on the left on this slide it's

899
00:53:23,960 --> 00:53:28,600
dictionary where we have all of our because

900
00:53:28,610 --> 00:53:33,700
i assume that during church going from our document

901
00:53:33,710 --> 00:53:38,120
covering a set of rules and getting a set of documents there is

902
00:53:38,270 --> 00:53:42,210
and on the right it was released

903
00:53:42,230 --> 00:53:47,920
it's forcing its ideas of of documents in this work

904
00:53:47,930 --> 00:53:56,990
and also i told that information retrieval is very old and library science

905
00:53:57,000 --> 00:54:02,030
thousands years ago already invented the structure and this is

906
00:54:02,410 --> 00:54:05,950
again the fourth of

907
00:54:06,000 --> 00:54:07,450
of index

908
00:54:08,630 --> 00:54:13,030
mechanical implementation of the same of the same data structure

909
00:54:13,080 --> 00:54:16,200
and because it's seem

910
00:54:16,250 --> 00:54:18,970
building this index is

911
00:54:19,020 --> 00:54:20,980
very the piece of wire

912
00:54:21,750 --> 00:54:24,390
it is very very simple

913
00:54:24,410 --> 00:54:29,800
we create some index then we are going to iterating through the whole collection

914
00:54:29,820 --> 00:54:32,050
get every document

915
00:54:32,100 --> 00:54:36,130
from the document to get more put it into index

916
00:54:36,150 --> 00:54:37,680
and or

917
00:54:37,690 --> 00:54:40,180
if it is already in the

918
00:54:40,190 --> 00:54:42,550
up and till is that this

919
00:54:42,570 --> 00:54:44,070
connected to this work

920
00:54:45,090 --> 00:54:49,470
ideal followed document that processing right now though this is

921
00:54:49,480 --> 00:54:53,190
so what are called in places like length

922
00:54:53,230 --> 00:54:57,780
close to pi semantic that implements the story

923
00:54:57,790 --> 00:55:01,040
and it you it is very simple

924
00:55:01,050 --> 00:55:02,740
four lines of course

925
00:55:03,990 --> 00:55:06,540
we don't have any problem with this

926
00:55:06,550 --> 00:55:11,240
what i really question is going to discuss this lecture

927
00:55:11,260 --> 00:55:19,230
first of all we need to decide how should iterate of documents and extract

928
00:55:19,260 --> 00:55:23,090
and all of the features that we discussed in the first lecture

929
00:55:23,100 --> 00:55:26,170
and i promise you that

930
00:55:26,350 --> 00:55:33,420
i want to talk a lot about it i think that it's pretty simple topic

931
00:55:33,420 --> 00:55:35,180
and the current

932
00:55:35,230 --> 00:55:41,480
second what to do if we are going to index because lecture

933
00:55:41,500 --> 00:55:47,010
what is what should be our approach to election huge

934
00:55:47,060 --> 00:55:53,610
and then again from the second lecture you know that something that usually compress structures

935
00:55:53,630 --> 00:55:57,000
can be not only small but fast

936
00:55:57,020 --> 00:55:59,150
so the question is

937
00:56:00,110 --> 00:56:03,000
compress index and

938
00:56:06,110 --> 00:56:12,700
we're going to discuss situations usually in their lives we don't have a stable complex

939
00:56:12,720 --> 00:56:15,310
we always have some change

940
00:56:15,330 --> 00:56:18,290
if they are indexing internet

941
00:56:18,330 --> 00:56:22,190
we always have news sites and updates on the site

942
00:56:22,200 --> 00:56:24,090
if you are in the same

943
00:56:24,100 --> 00:56:29,950
some documents from the company we always have new documents in new versions of these

944
00:56:29,950 --> 00:56:32,810
documents so we always have some updates

945
00:56:32,830 --> 00:56:39,270
therefore we need to talk about how to update its index what are possible strategies

946
00:56:39,310 --> 00:56:41,730
and the and the and

947
00:56:41,750 --> 00:56:47,300
again from the second lecture i promise you that we always talk about pluralisation and

948
00:56:48,770 --> 00:56:54,790
how we can implement blessing not talking about not not unsupervised classification but class how

949
00:56:54,790 --> 00:56:57,340
to implement this very in

950
00:56:57,390 --> 00:56:59,350
a cluster of computers

951
00:56:59,370 --> 00:57:01,540
in parallel

952
00:57:05,580 --> 00:57:08,580
this is called slide from the first lecture

953
00:57:08,600 --> 00:57:12,660
what to extract features from document

954
00:57:12,680 --> 00:57:15,100
we're doing mineralisation so

955
00:57:17,310 --> 00:57:22,770
great some more general suppose that that created on the first lecture

956
00:57:22,780 --> 00:57:23,690
that can

957
00:57:23,700 --> 00:57:27,080
analyse for much of this document

958
00:57:27,100 --> 00:57:30,040
and can extract all words

959
00:57:30,060 --> 00:57:31,870
from this document

960
00:57:31,890 --> 00:57:34,460
so what you getting the editing

961
00:57:34,480 --> 00:57:38,460
four and we are getting some idea of this document

962
00:57:38,970 --> 00:57:41,950
how we can assign this idea

963
00:57:41,960 --> 00:57:47,330
it can be some external IDE can be for internet that can be or this

964
00:57:47,340 --> 00:57:49,850
document its address

965
00:57:51,650 --> 00:57:54,600
another collection can be the unique he

966
00:57:54,610 --> 00:57:56,170
for the document

967
00:57:56,210 --> 00:58:01,370
so starting from this point for our toy project we assume that we always in

968
00:58:02,710 --> 00:58:06,960
i d of the document is simply not

969
00:58:06,980 --> 00:58:09,280
as intentional

970
00:58:09,290 --> 00:58:14,300
even if this is not an integer let's assume that we can simply create some

971
00:58:14,300 --> 00:58:16,270
table with this he

972
00:58:16,290 --> 00:58:17,930
and man

973
00:58:17,950 --> 00:58:21,460
this is simply to simple intention

974
00:58:21,510 --> 00:58:25,380
and another assumption that we are doing right now the the beginning

975
00:58:25,400 --> 00:58:30,190
that when we get documents to our index

976
00:58:30,210 --> 00:58:32,760
this interchange

977
00:58:33,910 --> 00:58:37,770
always increase so we are adding documents and

978
00:58:37,790 --> 00:58:44,610
this unique identification of these documents is simply increasing intention so fails document is one

979
00:58:44,620 --> 00:58:46,690
second to its

980
00:58:46,710 --> 00:58:49,270
it can be done

981
00:58:49,290 --> 00:58:54,090
it's not like percent rule sometimes needs additional tables and the additional weight

982
00:58:54,110 --> 00:58:56,380
what we have seen that we can do this

983
00:58:56,460 --> 00:59:01,150
it's it's some simplification that

984
00:59:01,180 --> 00:59:05,350
simplify all all our processing of the data

985
00:59:06,950 --> 00:59:10,570
so now we are ready to build index and

986
00:59:10,580 --> 00:59:13,480
as also i talked about it

987
00:59:13,500 --> 00:59:16,280
on my first lecture

988
00:59:16,300 --> 00:59:20,050
i things in the same series of building in texas

989
00:59:21,210 --> 00:59:26,170
definitely implemented in ancient egypt by librarians of this type so of

990
00:59:26,190 --> 00:59:27,180
how this goes

991
00:59:27,190 --> 00:59:28,160
can work

992
00:59:28,180 --> 00:59:30,030
i'm taking a book

993
00:59:30,050 --> 00:59:33,530
i'm taking work from his book for example these bogus

994
00:59:33,540 --> 00:59:38,280
well designed for example these movies book was eighty one so i'm taking first work

995
00:59:38,280 --> 00:59:39,780
from this book

996
00:59:39,790 --> 00:59:41,940
i take this work

997
00:59:41,960 --> 00:59:44,080
i put on some

998
00:59:44,100 --> 00:59:45,880
a piece of paper

999
00:59:45,930 --> 00:59:49,540
number of this document i opened this dryer this war

1000
00:59:49,900 --> 00:59:52,600
on this dryer

1001
00:59:52,620 --> 00:59:53,860
four days

1002
00:59:53,880 --> 01:00:00,190
piece of way for his car into these dry clothes then take next door

1003
01:00:00,210 --> 01:00:02,200
and i do this step by step

1004
01:00:02,210 --> 01:00:05,600
this is this rigorous so from linearisation

1005
01:00:05,610 --> 01:00:08,320
they're getting worse the idea of documents

1006
01:00:09,080 --> 01:00:11,630
using our dictionary

1007
01:00:11,680 --> 01:00:14,010
we find was please for this

1008
01:00:14,020 --> 01:00:14,990
i d

1009
01:00:15,000 --> 01:00:18,310
and the port new to command the and

1010
01:00:18,320 --> 01:00:21,240
this puzzle is very very simple

1011
01:00:21,250 --> 01:00:23,580
and so

1012
01:00:23,630 --> 01:00:25,680
now you

1013
01:00:25,680 --> 01:00:27,860
all right

1014
01:00:27,920 --> 01:00:32,150
long weekend ahead of us one more like to the go

1015
01:00:32,180 --> 01:00:33,570
if i have an object

1016
01:00:33,580 --> 01:00:38,080
mass and gravitational field

1017
01:00:38,090 --> 01:00:41,190
gravitational forces in this direction

1018
01:00:41,300 --> 01:00:45,030
this is my increasing value of y

1019
01:00:45,170 --> 01:00:46,730
this force

1020
01:00:46,830 --> 01:00:49,890
vectorial and written equals minus

1021
01:00:49,890 --> 01:00:51,330
and g

1022
01:00:51,330 --> 01:00:53,280
why rules

1023
01:00:53,330 --> 01:00:55,920
since this is the one dimensional problem

1024
01:00:55,960 --> 01:00:58,460
we will often simply right f

1025
01:00:58,470 --> 01:01:00,390
equals minus

1026
01:01:00,390 --> 01:01:01,110
and g

1027
01:01:01,130 --> 01:01:04,390
this minus sign is important because that's the increasing value

1028
01:01:04,430 --> 01:01:07,750
of y

1029
01:01:07,770 --> 01:01:08,710
if this

1030
01:01:08,740 --> 01:01:09,970
level here

1031
01:01:10,030 --> 01:01:12,740
it is y equals zero

1032
01:01:12,750 --> 01:01:14,280
and i could call this

1033
01:01:14,310 --> 01:01:16,840
gravitational potential energy is zero

1034
01:01:16,880 --> 01:01:19,910
and this is why

1035
01:01:19,950 --> 01:01:23,240
then the gravitational potential energy here

1036
01:01:23,280 --> 01:01:24,700
because plus

1037
01:01:24,740 --> 01:01:26,220
mg y

1038
01:01:26,240 --> 01:01:28,970
this is you

1039
01:01:29,020 --> 01:01:33,220
so if i make a plot

1040
01:01:33,220 --> 01:01:35,920
of the gravitational potential energy

1041
01:01:35,940 --> 01:01:39,160
as a function of y

1042
01:01:40,420 --> 01:01:42,990
i would get a straight line

1043
01:01:43,140 --> 01:01:46,990
this is zero

1044
01:01:47,080 --> 01:01:50,390
so this equals you

1045
01:01:50,440 --> 01:01:52,720
equals and g

1046
01:01:53,830 --> 01:01:57,080
a plus sign

1047
01:01:57,210 --> 01:01:59,600
if i'm here point eight

1048
01:01:59,610 --> 01:02:02,860
and i move that object to point b

1049
01:02:04,000 --> 01:02:06,250
walter lewin move it

1050
01:02:06,250 --> 01:02:08,660
i have to do positive work notice

1051
01:02:08,660 --> 01:02:13,110
that the gravitational potential energy increases if i do positive work to gravity is doing

1052
01:02:13,110 --> 01:02:14,880
negative work

1053
01:02:14,920 --> 01:02:19,660
if i go from a to some other point call it b prime

1054
01:02:20,770 --> 01:02:22,860
i do negative work notice

1055
01:02:22,860 --> 01:02:26,910
the gravitational pull of potential energy goes down if i do negative work

1056
01:02:27,740 --> 01:02:30,610
gravity is doing positive work

1057
01:02:30,630 --> 01:02:34,070
i could have chosen my zero point

1058
01:02:34,090 --> 01:02:36,310
of potential energy

1059
01:02:36,350 --> 01:02:38,470
anyway i please

1060
01:02:38,510 --> 01:02:41,500
i could have chosen it

1061
01:02:41,530 --> 01:02:43,410
right you

1062
01:02:43,410 --> 01:02:44,690
and nothing

1063
01:02:44,730 --> 01:02:47,260
i will change other than that

1064
01:02:47,280 --> 01:02:49,370
i offset

1065
01:02:49,410 --> 01:02:53,200
the zero point of my potential energy but again if i go from

1066
01:02:53,250 --> 01:02:55,000
a to b

1067
01:02:55,010 --> 01:02:59,380
the gravitational potential energy increases by exactly the same amount i have to do exactly

1068
01:02:59,380 --> 01:03:00,380
the same

1069
01:03:00,430 --> 01:03:03,750
works so you are free to choose when you're near earth

1070
01:03:03,790 --> 01:03:05,480
where you choose your

1071
01:03:07,870 --> 01:03:09,970
now we take the situation

1072
01:03:11,130 --> 01:03:13,880
we are not so close to the earth

1073
01:03:13,940 --> 01:03:16,150
here is the earth itself

1074
01:03:16,160 --> 01:03:19,180
which you can also replaced by the sun if you want to

1075
01:03:19,190 --> 01:03:23,560
and this is increasing value of are

1076
01:03:23,600 --> 01:03:26,840
the distance between here and is object

1077
01:03:26,870 --> 01:03:29,450
equals are

1078
01:03:29,470 --> 01:03:31,350
i now know that there is a

1079
01:03:31,410 --> 01:03:33,470
gravitational force

1080
01:03:33,510 --> 01:03:35,790
on this object

1081
01:03:35,810 --> 01:03:37,790
newton's universal

1082
01:03:37,850 --> 01:03:39,310
law of gravity

1083
01:03:39,320 --> 01:03:42,070
and that gravitational force

1084
01:03:49,160 --> 01:03:51,350
divided by these are scrapped

1085
01:03:51,370 --> 01:03:54,410
our rules this is a vectorial notation

1086
01:03:54,450 --> 01:03:56,880
since it is really one-dimensional

1087
01:03:56,940 --> 01:03:59,290
reward just like we did there

1088
01:03:59,350 --> 01:04:02,250
we would delete the

1089
01:04:02,290 --> 01:04:06,380
arrow and we will really unit vector in the positive direction and so we would

1090
01:04:06,380 --> 01:04:07,970
simply write it

1091
01:04:07,970 --> 01:04:09,750
this way

1092
01:04:10,730 --> 01:04:12,690
gravitational potential energy

1093
01:04:12,690 --> 01:04:14,410
we derive last time

1094
01:04:14,440 --> 01:04:16,720
equals minus

1095
01:04:20,880 --> 01:04:26,280
divided by are notice here is are here is are squared

1096
01:04:26,290 --> 01:04:27,410
and if you

1097
01:04:27,410 --> 01:04:29,350
i that

1098
01:04:32,070 --> 01:04:35,790
the plant grows sort of like this

1099
01:04:35,850 --> 01:04:38,720
this is are

1100
01:04:38,900 --> 01:04:40,210
is increasing

1101
01:04:40,260 --> 01:04:44,690
potential energy all these values here negative

1102
01:04:44,740 --> 01:04:48,610
and you get curve which is sort of like this

1103
01:04:48,620 --> 01:04:50,480
this is proportional

1104
01:04:50,490 --> 01:04:52,630
one of four

1105
01:04:52,680 --> 01:04:55,190
now of course if the earth

1106
01:04:55,200 --> 01:04:56,790
at the radius

1107
01:04:56,800 --> 01:04:59,800
which is this big

1108
01:04:59,810 --> 01:05:01,880
then of course this curve

1109
01:05:01,890 --> 01:05:03,370
does not exist

1110
01:05:03,380 --> 01:05:04,500
it stops

1111
01:05:04,510 --> 01:05:06,920
right here

1112
01:05:06,930 --> 01:05:09,650
if i move from point a to point b

1113
01:05:09,680 --> 01:05:12,110
with the mass and in my hand

1114
01:05:13,240 --> 01:05:16,980
that the gravitational potential energy increases

1115
01:05:17,010 --> 01:05:20,070
i have to do positive work is no difference

1116
01:05:20,120 --> 01:05:23,170
if i go from a to another point

1117
01:05:23,180 --> 01:05:25,870
b prime which is closer to the earth

1118
01:05:25,880 --> 01:05:29,170
notice that the gravitational potential energy decreases

1119
01:05:29,180 --> 01:05:31,330
i do negative work

1120
01:05:31,340 --> 01:05:34,820
if i do positive work gravity is doing negative work

1121
01:05:34,830 --> 01:05:37,060
if i do negative work gravity

1122
01:05:37,070 --> 01:05:41,440
is doing positive work

1123
01:05:41,440 --> 01:05:44,380
right here in the earth

1124
01:05:44,400 --> 01:05:46,830
where this

1125
01:05:46,880 --> 01:05:48,400
one of our curve

1126
01:05:48,420 --> 01:05:49,690
it's the earth

1127
01:05:49,700 --> 01:05:52,560
that is of course

1128
01:05:52,640 --> 01:05:55,040
exactly that line

1129
01:05:55,050 --> 01:05:56,360
that dependence

1130
01:05:56,380 --> 01:06:00,260
on y is exactly the same as the dependence of on are and then

1131
01:06:00,310 --> 01:06:06,060
you can simplify matters when you knew earth when the gravitational acceleration doesn't change

1132
01:06:06,070 --> 01:06:09,630
you get a linear relation that

1133
01:06:09,700 --> 01:06:11,760
only in exceptional cases

1134
01:06:11,790 --> 01:06:12,860
when you don't

1135
01:06:13,880 --> 01:06:16,480
very far

1136
01:06:16,510 --> 01:06:18,570
the gravitational force

1137
01:06:18,630 --> 01:06:20,400
it is in the direction

1138
01:06:21,940 --> 01:06:23,480
the increasing

1139
01:06:23,490 --> 01:06:25,200
potential energy

1140
01:06:26,810 --> 01:06:28,050
that when i'm here

1141
01:06:28,070 --> 01:06:29,630
the gravitational force

1142
01:06:29,750 --> 01:06:31,370
is in this direction

1143
01:06:31,430 --> 01:06:34,310
increase in potential energy

1144
01:06:34,320 --> 01:06:35,740
is this way

1145
01:06:35,790 --> 01:06:37,230
the force

1146
01:06:37,270 --> 01:06:38,170
is in this way

1147
01:06:38,180 --> 01:06:40,480
when i'm here

1148
01:06:40,640 --> 01:06:44,000
gravitational potential energy increases in this way

1149
01:06:44,010 --> 01:06:46,210
the gravitational force

1150
01:06:46,240 --> 01:06:48,730
using this direction

1151
01:06:48,880 --> 01:06:51,370
when i'm here

1152
01:06:51,400 --> 01:06:54,860
the gravitational potential energy increases in this direction

1153
01:06:54,870 --> 01:06:56,310
the gravitational force

1154
01:06:56,310 --> 01:07:01,830
is in this direction when here gravitational potential energy increases in this direction

1155
01:07:01,840 --> 01:07:05,340
the gravitational force is in the direction of the force

1156
01:07:05,400 --> 01:07:07,690
is always in the opposite direction

1157
01:07:07,730 --> 01:07:11,250
then the increasing value of the potential energy

1158
01:07:11,330 --> 01:07:14,460
if i release an object zero speed

1159
01:07:14,510 --> 01:07:16,190
it therefore will always

1160
01:07:16,240 --> 01:07:18,340
move towards the lower

1161
01:07:18,360 --> 01:07:19,550
potential energy

1162
01:07:19,560 --> 01:07:20,620
because the force

1163
01:07:20,630 --> 01:07:21,700
i will drive it

1164
01:07:21,710 --> 01:07:23,670
two lower potential

1165
01:07:23,700 --> 01:07:27,040
and that's it

1166
01:07:27,690 --> 01:07:29,630
i changed from gravity

1167
01:07:29,690 --> 01:07:30,940
two spring

1168
01:07:31,010 --> 01:07:34,640
i have a spring which is

1169
01:07:34,690 --> 01:07:36,480
relaxed length l

1170
01:07:36,490 --> 01:07:38,480
i call this actually equals zero

1171
01:07:38,490 --> 01:07:41,320
and i extended

1172
01:07:41,380 --> 01:07:44,460
over the distance x

1173
01:07:44,480 --> 01:07:47,150
there's a mass and the and

1174
01:07:47,170 --> 01:07:49,510
and there will be a spring force

1175
01:07:49,570 --> 01:07:52,210
and that spring force

1176
01:07:53,080 --> 01:07:55,490
equals minus

1177
01:07:55,540 --> 01:08:00,480
kx is one-dimensional situation so i can write it without having

1178
01:08:00,510 --> 01:08:01,710
to worry about the

1179
01:08:01,760 --> 01:08:02,760
the arrows

1180
01:08:02,770 --> 01:08:04,730
there is no friction here

1181
01:08:04,770 --> 01:08:08,090
it's clear that if i hold it in my hand

1182
01:08:08,830 --> 01:08:10,430
the force

1183
01:08:10,430 --> 01:08:17,510
consider all of the possible resulting partitionings i'm covering my space of potential by clustering

1184
01:08:17,510 --> 01:08:18,050
in such a way

1185
01:08:18,430 --> 01:08:20,830
they're guaranteed that

1186
01:08:20,850 --> 01:08:24,490
one of them is epsilon close to the true cost

1187
01:08:25,930 --> 01:08:30,580
it's a different type of use of the statistics here and it works for each

1188
01:08:30,580 --> 01:08:31,680
of them

1189
01:08:31,700 --> 01:08:36,660
of the of the cost functions that i is defined

1190
01:08:36,930 --> 01:08:41,550
so that's all i mean these are very i mean these are results from from

1191
01:08:41,550 --> 01:08:42,830
from this summer

1192
01:08:42,850 --> 01:08:46,580
so we haven't had time yet to try them and we'll that so in this

1193
01:08:46,580 --> 01:08:50,240
sense it's very theoretical we don't know how it works and that what we do

1194
01:08:50,930 --> 01:08:52,010
these are

1195
01:08:52,720 --> 01:08:54,870
as far as i can tell you the first step

1196
01:08:54,990 --> 01:08:57,850
performance guarantees for biclustering algorithms

1197
01:08:57,890 --> 01:09:03,410
tell you that if you follow this way on time algorithm and you get guarantee

1198
01:09:03,410 --> 01:09:04,470
that you are

1199
01:09:04,510 --> 01:09:07,280
within excellently optimum solution

1200
01:09:07,280 --> 01:09:11,970
OK so that's as far as the complexity goes there's only one more issue that

1201
01:09:11,970 --> 01:09:12,600
they wanted to

1202
01:09:14,050 --> 01:09:18,260
and this is the learnability of bi clustering so

1203
01:09:18,260 --> 01:09:20,950
if you consider the netflix problem

1204
01:09:20,970 --> 01:09:21,600
then there

1205
01:09:21,600 --> 01:09:27,280
the problem of writing predicting their right the ranking of movies that it's a slightly

1206
01:09:27,280 --> 01:09:31,470
different problem because the main issue is that the metric is not full have lots

1207
01:09:31,470 --> 01:09:32,990
of missing entries

1208
01:09:33,060 --> 01:09:34,830
so assume that

1209
01:09:34,870 --> 01:09:40,180
i want to predict in which groups assume already the biclustering of my matrix of

1210
01:09:40,560 --> 01:09:41,410
and movies

1211
01:09:41,450 --> 01:09:43,620
and now there's a new movie coming

1212
01:09:43,640 --> 01:09:48,080
i want to predict which group is this new movie going to belong to

1213
01:09:48,100 --> 01:09:49,800
or maybe new users

1214
01:09:49,890 --> 01:09:51,600
is joining my

1215
01:09:51,620 --> 01:09:53,720
pool of their customers

1216
01:09:53,780 --> 01:09:55,640
i want to predict

1217
01:09:55,660 --> 01:09:58,030
which group this user is going to be long

1218
01:09:58,030 --> 01:10:01,430
so that that's a very natural question to ask so

1219
01:10:01,450 --> 01:10:06,320
i call it learnability of bi clustering is a way to predict the cluster membership

1220
01:10:06,330 --> 01:10:08,620
of a previously unseen

1221
01:10:09,580 --> 01:10:10,660
on moving

1222
01:10:10,680 --> 01:10:12,970
so it turns out that it is

1223
01:10:13,720 --> 01:10:18,930
what the case in learning in order do prediction we have to restrict the family

1224
01:10:18,930 --> 01:10:24,010
of potential solutions that we are considering is like a classification if you want to

1225
01:10:24,010 --> 01:10:28,180
be able to to get any guarantees of equality the occasion to restrict yourself to

1226
01:10:28,180 --> 01:10:33,180
some for this class otherwise you have these no free lunch phenomena

1227
01:10:35,550 --> 01:10:36,720
we propose

1228
01:10:36,740 --> 01:10:37,930
so the question is

1229
01:10:37,950 --> 01:10:41,950
how do i restrict the family of all possible partitions of my

1230
01:10:41,970 --> 01:10:46,910
of of all possible clusterings of matrix and i want to do it by taking

1231
01:10:46,910 --> 01:10:51,910
into account side information so assume that now i will allow to take into account

1232
01:10:51,910 --> 01:10:55,870
information they have about users so when someone joins the member

1233
01:10:55,910 --> 01:11:02,530
i'm a asking to give me is date of birth and his address and whatever

1234
01:11:02,800 --> 01:11:06,930
the parameters and of course i have several properties about the movie i know well

1235
01:11:07,180 --> 01:11:08,510
what was the year

1236
01:11:08,550 --> 01:11:10,780
the movie was you know the

1237
01:11:10,990 --> 01:11:14,950
the movie was produced which country it was with the director and so on

1238
01:11:15,010 --> 01:11:17,350
so i want to use the side information

1239
01:11:17,390 --> 01:11:21,350
so i want to use the features of the the rows and the columns to

1240
01:11:21,350 --> 01:11:25,470
restrict the family of potential clusterings

1241
01:11:27,010 --> 01:11:32,290
one natural way of doing it is to restrict our attention to the family of

1242
01:11:32,290 --> 01:11:35,640
all clusterings which are in the feature space is of the road

1243
01:11:35,660 --> 01:11:39,620
and in the feature space is of the columns have only diagram

1244
01:11:39,620 --> 01:11:40,720
so now

1245
01:11:40,720 --> 01:11:45,810
are known from the column architecture also in the early visual cortex for example you

1246
01:11:45,810 --> 01:11:51,480
find patchy long range connections in prefrontal cortex that are typical for example if you

1247
01:11:51,480 --> 01:11:56,600
want to hear connect cells that have the similar orientation preference what you do is

1248
01:11:56,600 --> 01:12:01,760
you need a long range connections to to bridge between all the between all the

1249
01:12:01,760 --> 01:12:08,300
intermediate me the meaning intermingled cells that have a different orientation preferences long-range projections and

1250
01:12:08,300 --> 01:12:14,260
you find similar anatomical substrates also in prefrontal cortex which might suggest that you have

1251
01:12:14,550 --> 01:12:19,510
such an architectural also in the prefrontal cortex and other parts of the brain

1252
01:12:19,540 --> 01:12:25,010
now the problem is that the the voxels here are far too big to resolve

1253
01:12:25,320 --> 01:12:27,260
this cortical columns at

1254
01:12:27,280 --> 01:12:32,240
so the column pattern has as the resolution of say half a millimetre to bit

1255
01:12:32,240 --> 01:12:36,950
smaller depends on the species as well as some species apparently don't have any columns

1256
01:12:36,950 --> 01:12:43,700
for example here dogs apparently don't have columns but never heard that validated anywhere so

1257
01:12:43,700 --> 01:12:46,590
the key is that the problem is you can zoom in with the for my

1258
01:12:46,590 --> 01:12:49,870
resolution to these individual cortical columns

1259
01:12:49,890 --> 01:12:55,620
so you can't get this fine grained information and that's why content selective processing has

1260
01:12:55,620 --> 01:12:58,080
only rarely been studied with for my

1261
01:12:58,090 --> 01:13:04,190
unless use very artificial cases for example where people funny faces and houses because they

1262
01:13:04,190 --> 01:13:10,000
use the face area and the house areas attacked for content selective processing

1263
01:13:12,340 --> 01:13:13,780
a couple of years ago

1264
01:13:13,800 --> 01:13:18,370
we and others have looked at the this in more detail

1265
01:13:18,390 --> 01:13:25,040
and this graph here shows quite nicely that you can possibly expect to still gets

1266
01:13:25,040 --> 01:13:28,040
at some kind of information stored at the column level

1267
01:13:28,940 --> 01:13:31,490
with your pretty large voxels

1268
01:13:31,500 --> 01:13:37,000
and what you can see here on the left is simulated that's kind of pretty

1269
01:13:37,920 --> 01:13:44,960
colin architecture that encodes different orientations and the yellow

1270
01:13:44,970 --> 01:13:47,960
great is the voxel grid views

1271
01:13:47,960 --> 01:13:51,540
and i could say obviously the grid is far too big to to resolve these

1272
01:13:51,920 --> 01:13:56,960
individual regions of orientation selectivity but now you can display a very simple game and

1273
01:13:56,970 --> 01:14:01,960
you can go into one of these voxels and count how many cells there are

1274
01:14:01,960 --> 01:14:05,550
of each different type of orientation preference

1275
01:14:05,560 --> 01:14:08,690
and then you get a self reference count you can see that here in the

1276
01:14:08,690 --> 01:14:10,140
right for each works

1277
01:14:10,200 --> 01:14:14,540
and what you find is that in some voxels you see a surplus of one

1278
01:14:14,540 --> 01:14:18,290
type of cells and some voxels you see a surplus of the different types of

1279
01:14:20,700 --> 01:14:25,800
just due to the random fluctuations in the math you see surpluses sometimes you get

1280
01:14:25,800 --> 01:14:29,250
a few more one cell type sometimes you get a few more of another cell

1281
01:14:30,080 --> 01:14:30,910
and now

1282
01:14:30,930 --> 01:14:36,940
what you predict is that if you stimulate with two different orientations what should happen

1283
01:14:36,940 --> 01:14:43,850
is that you get a patter subtle patterning of the signal is not very strong

1284
01:14:44,200 --> 01:14:49,460
would be expected to be weak because obviously the majority of the response can be

1285
01:14:49,460 --> 01:14:53,850
determined by some kind of common factor that is kind of the same in each

1286
01:14:53,850 --> 01:14:59,860
of these different orientations here but you would expect some subtle patterning of the author

1287
01:14:59,860 --> 01:15:04,710
my response due to these biased sampling effect here

1288
01:15:04,740 --> 01:15:08,820
and in fact the shows for example something from the first study that relate to

1289
01:15:08,830 --> 01:15:14,120
this value can be tiny in front of you see the sampling bias patterns and

1290
01:15:14,640 --> 01:15:20,190
you can also for example measure them i can show you another slightly see more

1291
01:15:20,200 --> 01:15:23,320
so this is the sampling bias pattern

1292
01:15:23,350 --> 01:15:26,060
this is the sampling bias patron in v one

1293
01:15:26,100 --> 01:15:27,960
four different orientations

1294
01:15:27,960 --> 01:15:33,330
this is something buys pageant for color

1295
01:15:33,340 --> 01:15:38,220
in early visual cortex so this is the one here the two is power boarding

1296
01:15:38,220 --> 01:15:42,450
and then v three and you can also see it for the OP for the

1297
01:15:42,450 --> 01:15:46,950
eye of origin even in subcortical structures such as the LGN

1298
01:15:46,950 --> 01:15:50,730
and i could say well this is just some happening who can tell if this

1299
01:15:50,730 --> 01:15:55,850
really is in any way reproducible reliable or whatever

1300
01:15:55,870 --> 01:16:00,260
well the simple way to test that is just measure the pattern again and see

1301
01:16:00,260 --> 01:16:04,900
if it correlates and in fact you get a very high correlation between these patterns

1302
01:16:04,980 --> 01:16:08,270
when you made the measure ones in the new measure them against the very highly

1303
01:16:08,270 --> 01:16:14,600
correlated so that the distribution of preferences in each voxel some voxels have a very

1304
01:16:14,600 --> 01:16:19,700
the following content is provided under a Creative Commons license your support will help MIT

1305
01:16:19,700 --> 01:16:24,000
OpenCourseWare continue to offer high-quality educational resources for free

1306
01:16:24,490 --> 01:16:28,810
to make a donation or to view additional materials from hundreds of MIT courses

1307
01:16:29,330 --> 01:16:34,780
visit MIT OpenCourseWare at ocw . MIT . EDU

1308
01:16:34,810 --> 01:16:40,790
so that we can doesn't begin until at least after the 309 1 lecture

1309
01:16:44,190 --> 01:16:49,540
than play couple of announcements next Tuesday quiz to based on the subject matter of

1310
01:16:49,540 --> 01:16:50,130
homework to

1311
01:16:50,720 --> 01:16:57,860
the periodic table on Thursday and it's gonna be not before too long

1312
01:16:57,910 --> 01:17:01,040
have time for monthly tests the 1st monthly test

1313
01:17:01,910 --> 01:17:08,230
the mnemonics contest deadline is a week from today plots sent to me by e-mail

1314
01:17:08,230 --> 01:17:15,030
if you wanna when they're hottie that hot stars and it's come to my attention

1315
01:17:15,030 --> 01:17:21,130
that people are moving freely about recitations without arranging through my office in some of

1316
01:17:21,130 --> 01:17:23,770
the recitation sections are are

1317
01:17:23,800 --> 01:17:29,990
uncomfortably large which defeats the purpose of the recitations or in part and we want

1318
01:17:29,990 --> 01:17:34,430
you to have access to the instructor and we've got 30 students there instead of

1319
01:17:34,430 --> 01:17:41,220
20 that dilutes your access so are we insist that you

1320
01:17:41,440 --> 01:17:45,540
to the section that assigned to and if you must change because you have a

1321
01:17:46,120 --> 01:17:52,270
you've got some other activities so please do it through my office so that we

1322
01:17:52,270 --> 01:17:53,430
can now

1323
01:17:53,600 --> 01:17:56,520
make the learning experience good for

1324
01:17:56,610 --> 01:18:01,410
the majority and not just for the diffuse and

1325
01:18:01,590 --> 01:18:03,750
so there's something else think

1326
01:18:04,470 --> 01:18:12,390
yes I just just learned yesterday that somewhat disappointment you know my favorite transuranic element

1327
01:18:12,400 --> 01:18:18,410
is 111 because it goes as plutonium but does it has been isolated by the

1328
01:18:18,420 --> 01:18:24,630
team Darmstadt in Germany and they have proposed a name name-brand again and again after

1329
01:18:24,630 --> 01:18:30,270
a villain rank and who was the discoverer of x-rays and that's now before the

1330
01:18:30,270 --> 01:18:36,850
International Union of Pure and Applied Chemistry so maybe before the followers of will have

1331
01:18:36,850 --> 01:18:41,730
to change our each showed you that this is not done steady and so we're

1332
01:18:41,730 --> 01:18:44,570
going to have a whole bunch of would like to invite Miriam so that we

1333
01:18:44,570 --> 01:18:47,710
have a bunch of German names along here

1334
01:18:48,410 --> 01:18:57,970
busy over Berkeley what are they doing they're going on video divorce leaving

1335
01:18:59,690 --> 01:19:03,710
OK so let let's get to the lesson last day

1336
01:19:03,860 --> 01:19:08,860
last day we study the boar model from 1 electron atoms and other models showed

1337
01:19:08,860 --> 01:19:15,010
us that through the quantum condition all of these quantities uh were quantized they were

1338
01:19:15,010 --> 01:19:22,290
functions of the quantum number n and later in the lecture we began to look

1339
01:19:22,290 --> 01:19:28,510
for reconciliation between the boar model and bombers analysis of angstroms

1340
01:19:29,090 --> 01:19:35,310
spectral lines study of atomic hydrogen and so we looked at the prism spectrograph gas

1341
01:19:35,310 --> 01:19:39,140
discharge tube but I showed you the energetics of what goes on in there

1342
01:19:39,790 --> 01:19:44,410
and towards the end of the lecture we started talking about the actual energetics of

1343
01:19:44,410 --> 01:19:49,290
collision I wanna return to that so let's put up that sketch again where we're

1344
01:19:49,290 --> 01:19:50,360
looking at

1345
01:19:50,530 --> 01:19:54,820
the incident electron and this is the electrons that is

1346
01:19:55,630 --> 01:20:02,100
accelerating off the cathode rays electrons all over I wanna make sure we're not confusing

1347
01:20:02,320 --> 01:20:08,100
1 electron for another set of this electron I'm talking about here is this 1

1348
01:20:08,100 --> 01:20:13,390
that is moving off the cathode and towards the end node and on route is

1349
01:20:13,410 --> 01:20:19,640
going to collide with some atomic hydrogen so this is the incident electron and it

1350
01:20:19,640 --> 01:20:28,530
has it has kinetic energy it has kinetic energy of the denoted the incident one

1351
01:20:28,530 --> 01:20:34,500
half mv squared it's not bound is moving along and let's subset stress this is

1352
01:20:34,500 --> 01:20:41,890
well incident velocity incident velocity electron rest mass incident energy and it's gonna collide with

1353
01:20:41,890 --> 01:20:46,530
a hydrogen atom at some point I would draw the hydrogen atom schematically like this

1354
01:20:46,720 --> 01:20:52,350
with protons in the nucleus and alone electrons in some orbit here

1355
01:20:53,990 --> 01:21:02,390
if the energy is the energy of the incident electron is greater than the energy

1356
01:21:03,790 --> 01:21:05,800
for some transitions with

1357
01:21:06,390 --> 01:21:13,290
the hydrogen then it's possible that this electron will capture some of the energy from

1358
01:21:13,290 --> 01:21:18,550
the incident electron and be excited to some other states so let's put that on

1359
01:21:18,550 --> 01:21:24,250
what's called this some initial state and let's call this some final state and that's

1360
01:21:24,250 --> 01:21:33,150
only going occurred when this incident energy is greater than the energy of transition so

1361
01:21:33,200 --> 01:21:38,310
we've got 2 electrons here here's a moving electrons free electron this 1 is bound

1362
01:21:38,590 --> 01:21:40,510
so this is in the target

1363
01:21:40,910 --> 01:21:46,490
or can be specimen which in this case is atomic hydrogen

1364
01:21:48,210 --> 01:21:53,010
let's look at what happens here the excitation occurs

1365
01:21:53,040 --> 01:21:56,600
and we can get the energy required for this

1366
01:21:56,680 --> 01:22:02,100
transition by using the boar model the boar model gives this value gives the value

1367
01:22:02,100 --> 01:22:09,680
and the funeral of the of the postulates that delta E of transition is equals

1368
01:22:09,680 --> 01:22:10,420
minus k

1369
01:22:10,950 --> 01:22:13,980
I'm putting up generally z squared to

1370
01:22:14,210 --> 01:22:20,420
allow for a higher z level of 1 electron atoms and it's 1 over the

1371
01:22:21,970 --> 01:22:27,310
quantum number squared minus 1 over initial quantum number squared so that gives you the

1372
01:22:27,310 --> 01:22:31,670
amount of energy and that energy is going to be taken away from the kinetic

1373
01:22:31,670 --> 01:22:37,750
energy of the incident electron which then will make its merry way off after the

1374
01:22:37,750 --> 01:22:44,130
collision and purposely making that vector shorter to indicate that this electron has been slow

1375
01:22:44,410 --> 01:22:48,050
and we call this electron be scattered electrons

1376
01:22:48,060 --> 01:22:56,450
scattered by the collision with the target hydrogen it has kinetic energy the sub scattered

1377
01:22:56,460 --> 01:23:02,750
as one half m v squared scatter the mass of the electron doesn't change but

1378
01:23:02,750 --> 01:23:07,400
found out well actually we doing the same and if you remember the story behind

1379
01:23:07,400 --> 01:23:12,890
the scenes then that actually adapted for the for the adapted for the US market

1380
01:23:12,890 --> 01:23:17,750
because they had nothing available so they adopted the but due to the fact that

1381
01:23:17,750 --> 01:23:20,510
it was american funding they had to

1382
01:23:20,550 --> 01:23:24,010
so the the same and ask them to please

1383
01:23:24,390 --> 01:23:28,030
and then then they found out that there are still some some things to say

1384
01:23:28,030 --> 01:23:32,600
is are some issues to solve and there has to be some standardisation effort because

1385
01:23:32,600 --> 01:23:36,360
this was product driven so the work was performed within projects then the

1386
01:23:36,890 --> 01:23:43,600
so we said OK we take some the standardisation process the people that worked previously

1387
01:23:43,890 --> 01:23:49,390
in the project and that project then worked jointly together the web and group and

1388
01:23:49,390 --> 01:23:50,790
not find

1389
01:23:50,980 --> 01:23:54,550
standard it hasn't translated by way

1390
01:23:54,630 --> 01:24:04,250
that's the current standards is basically the democrats standard with modifications and extensions but this

1391
01:24:04,250 --> 01:24:10,350
could method that no converter who does automatic translation but also doesn't exist too much

1392
01:24:10,350 --> 01:24:14,310
to develop energy so effort of transforming them

1393
01:24:14,400 --> 01:24:20,180
well maybe not too big and so that's who says that there is that the

1394
01:24:20,350 --> 01:24:26,130
introduce so you have some top you have left on top you have not and

1395
01:24:26,180 --> 01:24:32,810
this well is for example to see that has with the schema

1396
01:24:33,320 --> 01:24:37,350
after that is serialised in XML so it's

1397
01:24:37,570 --> 01:24:42,400
agent comes he's able to process the XML information if i have agent comes he's

1398
01:24:42,590 --> 01:24:48,020
able to process the RDF information and the agent comes he's able to to process

1399
01:24:48,020 --> 01:24:51,750
this information so that we have the

1400
01:24:51,750 --> 01:24:58,100
architecture that some unsolved problems with that layering what happens if the information given in

1401
01:24:58,100 --> 01:25:02,390
the RDF layer contradicts the one which is given the level for example so you

1402
01:25:02,390 --> 01:25:04,010
can make statements in which

1403
01:25:05,680 --> 01:25:10,180
which correspond to the things that are not there for that some problems that have

1404
01:25:10,180 --> 01:25:14,320
to be served that's how can you go into the den cycles in it and

1405
01:25:14,320 --> 01:25:18,890
stuff like that so the process the implementation side a little bit but what i

1406
01:25:18,900 --> 01:25:20,760
see is that we have a standardisation body

1407
01:25:21,260 --> 01:25:26,210
w three c that has a very good reputation that is working on this from

1408
01:25:26,210 --> 01:25:32,550
the blue one standard which is then supported by a lot of tools

1409
01:25:32,550 --> 01:25:33,550
that would be

1410
01:25:33,590 --> 01:25:37,550
so these are the

1411
01:25:37,560 --> 01:25:39,930
then the business people to look

1412
01:25:40,140 --> 01:25:45,940
such too much because know that established standards in some first applications and proof of

1413
01:25:45,940 --> 01:25:46,930
concept this

1414
01:25:47,260 --> 01:25:52,390
but you can do that said that

1415
01:25:52,940 --> 01:25:56,750
we have to distinguish herself the scientific revolution because they're of

1416
01:25:56,760 --> 01:26:04,820
scientific projects would and it has been working commission and some major funding going on

1417
01:26:04,840 --> 01:26:08,970
the web and exists some companies in the

1418
01:26:09,010 --> 01:26:14,550
something like a spin-off enterprises comparable to system of quentin returns and

1419
01:26:14,560 --> 01:26:16,170
that's not true

1420
01:26:16,180 --> 01:26:20,760
to give bed bread and butter for twenty people so there is market that that's

1421
01:26:20,760 --> 01:26:28,010
the difficult market because people not understand couldn't the the benefits of the technology so

1422
01:26:28,010 --> 01:26:33,430
it's very much a consulting job so that that company for example and so we

1423
01:26:33,430 --> 01:26:39,440
can solve some of the problems mobility and this because what's interesting the product but

1424
01:26:39,440 --> 01:26:46,210
they didn't understand what technology so it's not just which use it but specific that

1425
01:26:46,470 --> 01:26:48,760
technology which we have to adapt to the problem

1426
01:26:49,260 --> 01:26:53,020
and then we can do to solve the problem and they have different requirements and

1427
01:26:53,020 --> 01:26:58,130
stuff like that so it's that difficult market which will become easier as soon as

1428
01:26:58,170 --> 01:27:03,260
the standards are well-established as soon as the standard products for ontology modelling

1429
01:27:03,430 --> 01:27:07,440
i mean there are a lot of ontology editors just this simple example that you

1430
01:27:07,440 --> 01:27:14,350
have parents and whatever editor of cement of twins but there no standard product in

1431
01:27:14,350 --> 01:27:18,820
a way to some extent true that but i wouldn't claim that these are the

1432
01:27:18,820 --> 01:27:24,470
standard products for the community and this is an important step toward notes

1433
01:27:24,480 --> 01:27:32,290
the commercial market i would so that potential there already some revenues but it's still

1434
01:27:32,290 --> 01:27:35,510
a large potential

1435
01:27:35,510 --> 01:27:47,940
we're going to use all of its should

1436
01:27:48,050 --> 01:27:54,710
are you going

1437
01:27:54,720 --> 01:28:02,180
and that's the computer quote face-to-face meetings this as a basic ontology language so we

1438
01:28:02,350 --> 01:28:09,400
can define the ships between the concepts and what you do is you know could

1439
01:28:09,440 --> 01:28:13,510
unfortunately representative those folks example that you can define

1440
01:28:13,510 --> 01:28:15,890
relationships as opposed

1441
01:28:15,930 --> 01:28:21,470
so it's not relationship is inverse to another one you could say that one relationship

1442
01:28:21,470 --> 01:28:24,390
is symmetric you could say that a certain

1443
01:28:24,970 --> 01:28:31,440
certain relationship is transitive for example for the are to say that such a big

1444
01:28:31,440 --> 01:28:36,310
of a deal with at the soccerplex see which is the topic of these events

1445
01:28:36,460 --> 01:28:41,520
is also some topic of any of the plants that you have that object identity

1446
01:28:41,520 --> 01:28:43,060
so you can say that two

1447
01:28:43,130 --> 01:28:47,360
entities it not the same which is quite important for certain applications and you have

1448
01:28:47,360 --> 01:28:53,790
some other ontology modelling stuff so RDF and RDF schema is really very basic first

1449
01:28:54,630 --> 01:28:58,850
but not necessary to draw inferences for example like the rules so if you want

1450
01:28:58,850 --> 01:29:04,750
to process of such rules you need more modelling primitives which i introduced in all

1451
01:29:04,750 --> 01:29:05,810
what about

1452
01:29:05,940 --> 01:29:14,590
we are going to to be so

1453
01:29:14,590 --> 01:29:17,520
so there

1454
01:29:18,010 --> 01:29:25,290
it seems to be what

1455
01:29:25,290 --> 01:29:30,550
OK and that is why we can't shake the semantic web that is also very

1456
01:29:30,550 --> 01:29:35,800
large networks that spanned you will large wide area networks was ever developed in each

1457
01:29:35,800 --> 01:29:38,800
of these little square nodes here is the

1458
01:29:38,840 --> 01:29:43,180
is a machine that song so there are four machines only opened in nineteen sixty

1459
01:29:43,180 --> 01:29:44,560
nine and there were these

1460
01:29:46,040 --> 01:29:49,080
routers that were being used

1461
01:29:50,140 --> 01:29:53,040
this is nineteen seventy one so by nineteen seventy one

1462
01:29:53,050 --> 01:29:55,960
there still is a cluster of these machines in california

1463
01:29:55,960 --> 01:30:01,430
but you know the illinois carnegie mellon and boston have suddenly appeared on this map

1464
01:30:02,030 --> 01:30:08,960
MIT's here lincoln labs here harvard here BBN which is another large company that does

1465
01:30:08,960 --> 01:30:11,530
a lot of networking research in this area is here

1466
01:30:12,360 --> 01:30:15,060
the network is started to evolve and in particular you notice that there are now

1467
01:30:15,060 --> 01:30:18,750
these are two clusters these two regions one on the west coast and one on

1468
01:30:18,750 --> 01:30:21,220
the east coast have a number of nodes

1469
01:30:21,220 --> 01:30:24,960
so it just keeps growing and growing and by nineteen eighty is see the network

1470
01:30:24,960 --> 01:30:28,930
has gotten substantially larger and one of the interesting things about this is you starting

1471
01:30:28,940 --> 01:30:35,290
to see a diversity of links so noticed that hawaii is now connected into california

1472
01:30:35,290 --> 01:30:41,780
by way of a satellite connection as his london so we now have not just

1473
01:30:41,780 --> 01:30:45,540
not only just these wires run but in fact we have wireless links were still

1474
01:30:45,540 --> 01:30:48,270
on top of this the at the same time this is happening all the particles

1475
01:30:48,270 --> 01:30:51,300
we've been talking about being developed in one of the whole goals of this would

1476
01:30:51,300 --> 01:30:55,380
be able to support these multiple different kinds of links on top of a standard

1477
01:30:55,390 --> 01:30:57,220
networking protocols

1478
01:30:57,230 --> 01:31:01,900
so by nineteen eighty seven this thing is really become turned into a number of

1479
01:31:01,900 --> 01:31:08,210
decentralised networks as this large network of the urban that there's something smaller called backbone

1480
01:31:08,220 --> 01:31:12,300
and a number of other networks there's military networks and so on these are all

1481
01:31:12,800 --> 01:31:16,600
connected together and all being in this debate by nineteen seven there were all running

1482
01:31:17,270 --> 01:31:22,370
TCP IP protocol that was being used to exchange information between all these things

1483
01:31:23,980 --> 01:31:27,570
around about a few years after this right the internet the this world wide web

1484
01:31:27,570 --> 01:31:31,070
suddenly happened and it became this huge commercial interest in

1485
01:31:31,160 --> 01:31:37,770
the internet and that has really caused part to this explosion of nodes and made

1486
01:31:37,770 --> 01:31:42,280
the network just huge incredibly fast so it's hard to see this but the middle

1487
01:31:42,280 --> 01:31:44,840
of this you notice that there is little orange note here

1488
01:31:44,860 --> 01:31:48,820
this is this bar on the left side showing the out degree of nodes in

1489
01:31:48,820 --> 01:31:52,980
the network so this is this numbers two nine seven seven is that means there's

1490
01:31:52,990 --> 01:31:57,190
no to the centre of the internet has two thousand nine hundred seventy seven links

1491
01:31:57,190 --> 01:31:58,540
to other nodes

1492
01:31:58,590 --> 01:32:02,820
so this is like this is a really incredibly large network and you notice that

1493
01:32:02,820 --> 01:32:09,050
all of these twenty or thirty bright pink red nodes here have hundreds to thousands

1494
01:32:09,050 --> 01:32:12,980
of outgoing connections on each one so there is this core of the internet is

1495
01:32:13,060 --> 01:32:17,570
very highly connected to a very large part of the network now down to out

1496
01:32:17,580 --> 01:32:22,140
around the edges of these much smaller networks that have much lower conductivity

1497
01:32:22,190 --> 01:32:28,440
and these are things like service providers for example that consumers might pay some money

1498
01:32:28,450 --> 01:32:31,330
to get a connection from so this is the sort core of the internet these

1499
01:32:31,330 --> 01:32:34,410
are the people who are not so much the end users on the internet but

1500
01:32:34,410 --> 01:32:38,410
the service providers are providing activity for the end users each one of these little

1501
01:32:38,410 --> 01:32:43,860
nodes represents one of the service providers this was as of two thousand three

1502
01:32:43,890 --> 01:32:45,970
OK so

1503
01:32:46,010 --> 01:32:49,090
what i want to do now is to start as i said we're going to

1504
01:32:49,090 --> 01:32:52,630
start off with a baby version of the network particular going to look at how

1505
01:32:52,690 --> 01:32:57,490
four reading or how routing might work in this very simple network that shown here

1506
01:32:58,510 --> 01:33:01,480
let's see what happens to see what happens to renew the particle that we call

1507
01:33:01,480 --> 01:33:03,300
path vector routing

1508
01:33:03,330 --> 01:33:08,270
the idea behind routing is that we're going to build up the paths from

1509
01:33:08,270 --> 01:33:13,260
that is the sequence of nodes that we should we should forward messages through

1510
01:33:14,350 --> 01:33:17,220
in order to reach a particular destination

1511
01:33:17,260 --> 01:33:20,980
and the way that this work is to reduce stay there have two steps

1512
01:33:24,350 --> 01:33:26,530
here so

1513
01:33:26,540 --> 01:33:31,860
routing is going to consist of two

1514
01:33:31,870 --> 01:33:36,700
two steps

1515
01:33:36,720 --> 01:33:38,490
advertising phase

1516
01:33:38,560 --> 01:33:43,680
advertised steps

1517
01:33:43,680 --> 01:33:48,910
and integrations that

1518
01:33:48,970 --> 01:33:52,970
the idea is that during the advertisment step each node is going to advertise what

1519
01:33:52,970 --> 01:33:55,410
other nodes it knows how to reach

1520
01:33:55,410 --> 01:33:58,870
and then during integration steps each node is going to take

1521
01:33:58,930 --> 01:34:01,990
all of the advertisment her during the previous

1522
01:34:02,010 --> 01:34:06,200
advertising stuff and integrate them into a new set of rules that identifies the new

1523
01:34:06,200 --> 01:34:08,580
set of nodes of this node can reach

1524
01:34:09,310 --> 01:34:12,740
this will be very very clear when i showed example so let's just look at

1525
01:34:12,740 --> 01:34:16,850
the case of all the nodes figuring out how they can reach node c so

1526
01:34:16,850 --> 01:34:19,830
it's going to happen is that first node is going to send out an advertisment

1527
01:34:19,850 --> 01:34:24,350
it says that nobody knows how to reach node right which it obviously does and

1528
01:34:24,350 --> 01:34:28,560
the way reaches node is simply by forwarding a message up to and where

1529
01:34:28,580 --> 01:34:31,680
so it is to reach node e come this way and sends it out over

1530
01:34:31,680 --> 01:34:34,390
the two links that has to node c and d

1531
01:34:34,450 --> 01:34:38,460
so nodes c and d here this advertisment during the integration step what they're going

1532
01:34:38,460 --> 01:34:43,740
to do is to add this information about this connection with to node b

1533
01:34:43,740 --> 01:34:47,970
and they're going to store which link it is based on that message out over

1534
01:34:47,990 --> 01:34:50,780
it was they they should they should send messages out over in order to reach

1535
01:34:50,780 --> 01:34:53,350
i e e

1536
01:35:05,650 --> 01:35:08,280
one of the rule

1537
01:35:12,790 --> 01:35:18,930
is the

1538
01:35:35,310 --> 01:35:38,110
or or

1539
01:35:38,720 --> 01:35:42,620
o machine your

1540
01:36:19,100 --> 01:36:25,520
it is

1541
01:37:15,320 --> 01:37:19,850
so the rest of the questions

1542
01:37:35,400 --> 01:37:40,280
how many people

1543
01:37:55,730 --> 01:38:00,430
so that's she

1544
01:38:00,750 --> 01:38:03,550
in some sense

1545
01:38:03,560 --> 01:38:05,160
this is

1546
01:38:20,430 --> 01:38:23,970
all of of this is that

1547
01:38:53,280 --> 01:38:57,550
falls right

1548
01:39:13,640 --> 01:39:19,530
very very

1549
01:39:21,890 --> 01:39:28,010
we're seeing here

1550
01:39:28,080 --> 01:39:32,540
one four

1551
01:39:45,080 --> 01:39:46,820
no more

1552
01:39:52,260 --> 01:39:55,540
that is just a special case

1553
01:40:00,870 --> 01:40:05,800
you need to use

1554
01:40:25,570 --> 01:40:30,640
but before she has to learn a

1555
01:40:35,170 --> 01:40:41,960
all songs

1556
01:40:43,970 --> 01:40:48,890
rain forests

1557
01:41:11,320 --> 01:41:12,340
we know

1558
01:41:19,370 --> 01:41:22,050
he that

1559
01:41:22,070 --> 01:41:24,590
because there's whole

1560
01:41:26,770 --> 01:41:32,440
this is

1561
01:41:34,640 --> 01:41:36,140
c three

1562
01:41:41,580 --> 01:41:45,030
one hundred per cent

1563
01:41:45,480 --> 01:41:48,110
on is true

1564
01:41:48,150 --> 01:41:49,760
he was

1565
01:41:49,900 --> 01:41:55,400
all right so

1566
01:42:05,030 --> 01:42:09,480
are possible y

1567
01:42:15,610 --> 01:42:17,690
three are

1568
01:42:18,750 --> 01:42:19,460
for example

1569
01:42:19,470 --> 01:42:24,270
the reason we use

1570
01:42:24,400 --> 01:42:29,480
maybe mention is all

1571
01:42:29,480 --> 01:42:32,670
vaccine preventable disease

1572
01:42:32,690 --> 01:42:37,570
we're going to draw the this list in nineteen fifty

1573
01:42:37,620 --> 01:42:41,060
polio been on top

1574
01:42:41,080 --> 01:42:45,080
so some things have really changed

1575
01:42:45,140 --> 01:42:49,840
now what i'd like to do is to briefly explore

1576
01:42:49,870 --> 01:42:52,570
some of the factors that

1577
01:42:52,580 --> 01:42:54,780
i think i have contributed

1578
01:42:57,350 --> 01:42:58,750
has c

1579
01:42:58,890 --> 01:43:03,420
the first is the decline in vaccine preventable diseases

1580
01:43:05,650 --> 01:43:08,220
these of much less diphtheria

1581
01:43:08,230 --> 01:43:09,720
we are not

1582
01:43:11,200 --> 01:43:16,350
of our cultural experience as a society today

1583
01:43:16,400 --> 01:43:19,570
i think also that we're has been brought recognition

1584
01:43:19,580 --> 01:43:23,020
particularly since about the sixties

1585
01:43:23,060 --> 01:43:26,520
about the limits of medicine and technology

1586
01:43:26,570 --> 01:43:29,910
as the nation emerged from the second world war

1587
01:43:29,970 --> 01:43:36,330
we had a view that that modern technology could pretty much anything

1588
01:43:36,380 --> 01:43:37,700
it was the

1589
01:43:37,710 --> 01:43:39,730
the primacy of the engineer

1590
01:43:39,780 --> 01:43:44,290
and i think little by little we have learned that many of the major concerns

1591
01:43:44,290 --> 01:43:45,700
of life

1592
01:43:45,710 --> 01:43:48,000
are not easily solved

1593
01:43:48,710 --> 01:43:50,890
medicine and technology

1594
01:43:50,900 --> 01:43:57,000
and this is i think in part explains some of the resurgence of complementary and

1595
01:43:57,000 --> 01:44:01,810
alternative medicine is people look for other ways to solve

1596
01:44:01,830 --> 01:44:04,340
the problems that they faced

1597
01:44:04,390 --> 01:44:07,980
there's been abroad growth of consumerism which fact

1598
01:44:08,000 --> 01:44:09,330
which have aspects

1599
01:44:09,330 --> 01:44:13,350
our entire society

1600
01:44:13,400 --> 01:44:19,040
and then there are major failures of US health care system and here i was

1601
01:44:19,040 --> 01:44:25,640
pointing specifically to our failure to deal with mental health disorders

1602
01:44:25,650 --> 01:44:28,700
autism being among them

1603
01:44:28,730 --> 01:44:32,760
concomitant with that has been growing

1604
01:44:32,770 --> 01:44:37,200
growth of the vaccine injury compensation fund

1605
01:44:37,230 --> 01:44:39,630
which was of funding created

1606
01:44:39,690 --> 01:44:47,380
roughly in the mid nineteen eighties to compensate people who have been injured by

1607
01:44:47,420 --> 01:44:53,480
was thought to have been injured by a vaccine appropriately manufactured ministers

1608
01:44:53,550 --> 01:44:58,220
and that's funded by a tax on every does vaccine

1609
01:44:58,240 --> 01:45:03,910
well that's fine which cannot be used for anything but compensating for injury is now

1610
01:45:03,910 --> 01:45:10,840
over three billion dollars and it's no surprise that individuals who was family members have

1611
01:45:10,840 --> 01:45:15,760
conditions that are not taken care of by our health care system might look to

1612
01:45:15,770 --> 01:45:21,060
that one as a way of getting some resources

1613
01:45:21,070 --> 01:45:29,380
but there's more because the origins are complex clearly there has been today i distortion

1614
01:45:29,380 --> 01:45:34,910
of scientific process of proposing hypothesis testing

1615
01:45:34,920 --> 01:45:38,980
accepting or rejecting it then refining today

1616
01:45:38,990 --> 01:45:43,470
hypotheses are regularly then first in the media

1617
01:45:43,510 --> 01:45:46,100
and then they are validated by by

1618
01:45:48,370 --> 01:45:53,110
and when you look for the enemy of the enemy is us because i think

1619
01:45:53,110 --> 01:45:55,590
every major medical journal today

1620
01:45:55,600 --> 01:45:58,700
releases its findings first press

1621
01:45:58,720 --> 01:46:03,300
and then those of us who work in clinical medicine often hear about it first

1622
01:46:03,640 --> 01:46:05,510
on the today show or

1623
01:46:05,520 --> 01:46:08,420
for myself on NPR

1624
01:46:08,430 --> 01:46:11,030
he's going in the wrong direction

1625
01:46:11,050 --> 01:46:14,240
then the issue of causality

1626
01:46:15,430 --> 01:46:19,650
the criteria for establishing causality

1627
01:46:19,680 --> 01:46:22,980
differs in the world of science and medicine

1628
01:46:22,990 --> 01:46:27,520
in the legal world and in the world of public opinion and the quickest and

1629
01:46:27,520 --> 01:46:30,840
simplest example of that is breast implants

1630
01:46:30,860 --> 01:46:34,160
do breast implants cause chronic disease

1631
01:46:34,220 --> 01:46:35,380
there is no

1632
01:46:35,400 --> 01:46:38,540
credible scientific evidence that they do

1633
01:46:38,650 --> 01:46:42,820
however the manufacturers were found liable court of law

1634
01:46:42,880 --> 01:46:45,700
and clearly they were found guilty

1635
01:46:45,890 --> 01:46:51,450
you are responsible in the court of public opinion

1636
01:46:51,470 --> 01:46:55,910
risk communication is a very complicated are

1637
01:46:55,950 --> 01:47:01,410
and case reports are enormously powerful

1638
01:47:01,540 --> 01:47:04,180
no matter what the study shows

1639
01:47:04,240 --> 01:47:05,950
when you see a pair parent

1640
01:47:06,010 --> 01:47:08,230
with the child

1641
01:47:08,240 --> 01:47:16,040
who's suffering from a disease or a bad conditions and whose parents believe that that

1642
01:47:16,040 --> 01:47:20,810
was related to the things that is very

1643
01:47:20,820 --> 01:47:27,650
very powerful and often much easier to understand than for example an institute of medicine

1644
01:47:28,570 --> 01:47:29,990
and today

1645
01:47:30,220 --> 01:47:31,570
the media

1646
01:47:31,590 --> 01:47:34,320
he is interested in contacts privacy

1647
01:47:34,380 --> 01:47:36,520
and that's what cells

1648
01:47:36,530 --> 01:47:40,020
the media's concept of balance

1649
01:47:40,060 --> 01:47:43,420
is to present both sides of the story

1650
01:47:43,470 --> 01:47:47,940
i was like this little piece from scientific american

1651
01:47:48,060 --> 01:47:51,550
good journalism values balance above all else

1652
01:47:51,620 --> 01:47:56,790
we are to our readers to present everybody's ideas equally

1653
01:47:56,860 --> 01:48:01,860
if politicians or special interest groups say things that are untrue or misleading our journal

1654
01:48:01,890 --> 01:48:05,990
duties journalists is to quote them without comment or contradiction

1655
01:48:06,000 --> 01:48:09,640
to do otherwise would be elitist and

1656
01:48:09,680 --> 01:48:11,760
you might notice that day

1657
01:48:11,770 --> 01:48:15,040
the that appeared in the scientific american

1658
01:48:15,060 --> 01:48:18,600
april first two thousand ten

1659
01:48:18,610 --> 01:48:21,360
that was that was a joke

1660
01:48:21,380 --> 01:48:26,650
but to make the point the media or do regard

1661
01:48:26,720 --> 01:48:29,560
balance as evidence of

1662
01:48:29,570 --> 01:48:31,300
journalistic integrity

1663
01:48:31,390 --> 01:48:34,700
they tend to equate one expert with another

1664
01:48:34,730 --> 01:48:40,790
and again there focuses mostly on controversy rather than necessarily the search for truth

1665
01:48:40,970 --> 01:48:44,770
now not all these names on this slide may be familiar to you

1666
01:48:44,770 --> 01:48:51,860
but i would submit that those on the right are highly qualified experts or website

1667
01:48:51,900 --> 01:48:57,020
that is a tries to present strictly science-based information

1668
01:48:57,020 --> 01:49:04,270
where some of the photos on the left have particular viewpoints and are very effective

1669
01:49:05,870 --> 01:49:06,860
and today

1670
01:49:06,880 --> 01:49:10,250
communication tends to promote science

1671
01:49:10,290 --> 01:49:15,180
and in part that's because not because in general parents do not have a clear

1672
01:49:15,180 --> 01:49:18,410
idea how to evaluate the credibility

1673
01:49:18,470 --> 01:49:25,630
the source of information about immunizations it's not something that they are knowledgeable about and

1674
01:49:25,650 --> 01:49:27,150
they really don't know

1675
01:49:27,150 --> 01:49:30,130
how to go about figuring out who

1676
01:49:30,140 --> 01:49:33,350
who to believe

1677
01:49:33,420 --> 01:49:37,980
now i'm going to shift in talk now a bit about the history of compulsion

1678
01:49:37,980 --> 01:49:41,020
for immunization

1679
01:49:41,110 --> 01:49:47,740
in both massachusetts and in england laws were passed right at the beginning of the

1680
01:49:47,740 --> 01:49:53,180
the nineteenth century requiring populations to be vaccinated

1681
01:49:53,180 --> 01:49:58,290
so the question was what maybe all the people from the other here but again

1682
01:49:58,460 --> 01:50:02,110
i repeat there is an assumption here

1683
01:50:03,130 --> 01:50:04,510
the class of functions

1684
01:50:04,590 --> 01:50:06,120
i will be a

1685
01:50:06,130 --> 01:50:07,650
you know

1686
01:50:07,660 --> 01:50:11,340
he said the function as to be a bounded set in the matrix

1687
01:50:11,570 --> 01:50:17,050
yes i mean the fact that you can cooperate with by many balls implies that

1688
01:50:19,260 --> 01:50:26,740
within the with respect to all

1689
01:50:26,760 --> 01:50:32,140
yes but you can have the fact that is bounded and enclose the positive impact

1690
01:50:32,150 --> 01:50:36,240
in finite dimensions but you can have an infinite dimensional space but you

1691
01:50:36,260 --> 01:50:39,230
i have this well is it's called

1692
01:50:39,280 --> 01:50:43,240
we come back they get in this case when you can cover with finite many

1693
01:50:44,930 --> 01:50:46,470
yes there is this

1694
01:50:46,490 --> 01:50:50,070
restriction i mean if you want to apply this things

1695
01:50:50,130 --> 01:50:52,140
and you have lot of

1696
01:50:52,160 --> 01:50:52,990
because this

1697
01:50:53,010 --> 01:50:57,600
the notion of being able to cover your space by many

1698
01:50:57,620 --> 01:51:00,270
but which can happen even in infinite dimension

1699
01:51:17,250 --> 01:51:19,570
this is just too

1700
01:51:19,580 --> 01:51:20,850
come back to

1701
01:51:20,860 --> 01:51:24,350
in the IID assumption which

1702
01:51:24,360 --> 01:51:27,880
as i told you is like the only one that i'm allowing myself is to

1703
01:51:27,880 --> 01:51:32,060
assume that the data comes in IIT fashion and i just want to say that

1704
01:51:32,060 --> 01:51:32,770
this is

1705
01:51:32,800 --> 01:51:34,120
so our mind

1706
01:51:34,130 --> 01:51:35,950
assumption in the sense that

1707
01:51:36,000 --> 01:51:37,610
OK we can argue whether

1708
01:51:37,620 --> 01:51:42,000
in our data indeed indeed comes from and i i d

1709
01:51:42,900 --> 01:51:47,280
and of course we never can verify that this is the case

1710
01:51:49,020 --> 01:51:53,580
so maybe we can argue about it but on the other hand it turns out

1711
01:51:54,540 --> 01:51:58,530
this is a convenient setting for studying the problem and you can have all sorts

1712
01:51:58,620 --> 01:52:02,380
things that are kind of more realistic but still give the same result so it's

1713
01:52:02,380 --> 01:52:07,660
fine to work in the ITV framework because you get this the right intuitions

1714
01:52:07,670 --> 01:52:11,890
and if you're not happy with IID because you don't want to to assume this

1715
01:52:11,900 --> 01:52:13,240
and work in these

1716
01:52:13,280 --> 01:52:17,650
two other framework i mean one of them that i mentioned already this online primary

1717
01:52:17,690 --> 01:52:18,920
duty see that's

1718
01:52:18,930 --> 01:52:22,130
in the next lecture that actually

1719
01:52:22,140 --> 01:52:24,980
you can get the exact same type of bounds

1720
01:52:25,230 --> 01:52:29,500
in this framework without making any assumption about how the data is generated but from

1721
01:52:30,520 --> 01:52:34,680
i like the way he writes things is a bit different but in the end

1722
01:52:35,000 --> 01:52:36,310
the same

1723
01:52:36,360 --> 01:52:39,680
you know the same phenomenon occurs

1724
01:52:39,690 --> 01:52:42,130
and the other cases

1725
01:52:42,180 --> 01:52:43,150
i want articles

1726
01:52:43,210 --> 01:52:47,400
if setting where actually you consider that the data is fixed so you have you

1727
01:52:48,520 --> 01:52:50,540
big said that

1728
01:52:50,590 --> 01:52:54,850
extracted from the UCI library for example and you know you have a finite set

1729
01:52:54,860 --> 01:52:58,570
of data and you never we get to see any of the data points you

1730
01:52:58,570 --> 01:53:02,930
just based on what you do typically is used these they think well parts use

1731
01:53:02,930 --> 01:53:05,250
one portraying the other one for testing

1732
01:53:06,720 --> 01:53:09,510
you know and you hope that your algorithm is doing a good job

1733
01:53:09,580 --> 01:53:13,260
and in this case you can you know

1734
01:53:13,270 --> 01:53:17,430
consider that what is random is not the data about the we use these data

1735
01:53:17,430 --> 01:53:21,740
and you can actually make sure that the way these data is uniform in the

1736
01:53:21,740 --> 01:53:23,250
sense that you have you know

1737
01:53:25,330 --> 01:53:29,550
an equal probability of generating any kind of fit of the data that we can

1738
01:53:29,550 --> 01:53:32,210
actually implement that scheme of

1739
01:53:32,260 --> 01:53:34,400
random speaking

1740
01:53:34,450 --> 01:53:36,060
and this is typically what

1741
01:53:36,070 --> 01:53:40,010
we do an experiment and we write when we write papers and we report experimental

1742
01:53:40,010 --> 01:53:43,900
results we have exactly done that and in that same thing you can

1743
01:53:43,920 --> 01:53:46,300
basically derive the same bounds

1744
01:53:46,730 --> 01:53:51,500
for this case where you have a fixed set of data and run the risk

1745
01:53:51,500 --> 01:53:53,140
of these events part

1746
01:53:53,200 --> 01:53:55,580
so this is just to say

1747
01:53:55,590 --> 01:53:57,740
don't be upset that me five

1748
01:53:57,750 --> 01:53:59,910
use the IID assumption

1749
01:54:00,320 --> 01:54:04,310
even though i think i i should not make too many assumptions this is

1750
01:54:04,350 --> 01:54:06,240
perfectly fine in that respect

1751
01:54:06,260 --> 01:54:10,820
at least i it shows you the right information

1752
01:54:11,340 --> 01:54:16,530
OK so now i want to go back to the bounds that i mentioned yesterday

1753
01:54:18,160 --> 01:54:19,700
try to understand

1754
01:54:19,720 --> 01:54:24,340
what do they mean what can i you know infer from that conclude from obtaining

1755
01:54:24,340 --> 01:54:26,020
such bounds

1756
01:54:26,410 --> 01:54:29,410
the first thing i want to mention is that

1757
01:54:29,490 --> 01:54:33,030
when you look at the bounds of this one so when we compare the loss

1758
01:54:33,030 --> 01:54:36,990
of our algorithm with the empirical loss of our algorithm and we get an upper

1759
01:54:36,990 --> 01:54:40,520
bound you know we've been and some confidence that that's what this

1760
01:54:40,570 --> 01:54:43,280
o with high probability

1761
01:54:43,320 --> 01:54:45,640
this is an interesting fact of bounds

1762
01:54:46,610 --> 01:54:49,740
it only tells you OK

1763
01:54:50,810 --> 01:54:55,000
these two quantities are close so which means that if you want to estimate the

1764
01:54:55,000 --> 01:54:59,320
true error rather than from the empirical error this algorithm it's fine you can do

1765
01:55:00,170 --> 01:55:04,700
but it does not guarantee you that your algorithm is performing well does not guarantee

1766
01:55:04,740 --> 01:55:06,550
that this is small

1767
01:55:06,560 --> 01:55:09,820
i mean this may be small only if indeed

1768
01:55:09,830 --> 01:55:12,310
the empirical error is small we can only

1769
01:55:12,350 --> 01:55:16,380
be sure on the given set of data that this is the case but

1770
01:55:16,390 --> 01:55:19,180
you know it's not the statement that will tell you all your have wisdom is

1771
01:55:19,180 --> 01:55:21,980
good in general in

1772
01:55:22,000 --> 01:55:25,540
and in particular if you look at the value of the bonds you know you

1773
01:55:25,560 --> 01:55:29,950
we could say well i ever smaller bound for my algorithm that someone else will

1774
01:55:29,950 --> 01:55:31,960
for ease algorithm so

1775
01:55:32,030 --> 01:55:34,140
my algorithm has to be there were

1776
01:55:34,190 --> 01:55:35,900
actually it's not the case because

1777
01:55:35,940 --> 01:55:38,980
you can have i mean the small band that you can have here is when

1778
01:55:38,980 --> 01:55:42,760
your algorithm is just picking always the same function so

1779
01:55:42,930 --> 01:55:46,900
a trivial algorithm that always in order have the same functional model where the data

1780
01:55:46,900 --> 01:55:52,090
plus the sum of all i said y equals zero

1781
01:55:52,790 --> 01:55:54,390
by david j

1782
01:55:55,460 --> 01:55:58,510
log probability of zero

1783
01:55:59,520 --> 01:56:02,480
same as before

1784
01:56:18,570 --> 01:56:22,350
changing my alma notation a little bit

1785
01:56:22,480 --> 01:56:25,650
somehow i equals one

1786
01:56:25,660 --> 01:56:27,550
so i

1787
01:56:27,590 --> 01:56:32,340
far such that like not changing the summations

1788
01:56:32,420 --> 01:56:35,170
so while was one

1789
01:56:35,190 --> 01:56:38,010
david david j

1790
01:56:38,030 --> 01:56:40,140
log p

1791
01:56:40,150 --> 01:56:42,280
just going to call this

1792
01:56:43,850 --> 01:56:46,390
and this is

1793
01:56:46,400 --> 01:56:48,990
one minus p

1794
01:56:49,020 --> 01:56:53,690
because the probability that y equals zero is one minus the probability that y equals

1795
01:57:03,900 --> 01:57:06,550
the by david j

1796
01:57:07,780 --> 01:57:10,610
log of one minus p

1797
01:57:14,240 --> 01:57:18,700
really see how i got here

1798
01:57:20,340 --> 01:57:22,440
i'm going to

1799
01:57:23,940 --> 01:57:27,210
star work going to evaluate

1800
01:57:27,220 --> 01:57:29,630
by deviated j

1801
01:57:30,950 --> 01:57:32,700
log p

1802
01:57:32,710 --> 01:57:34,800
and this is

1803
01:57:34,840 --> 01:57:37,460
one of the p

1804
01:57:40,300 --> 01:57:41,710
by debated j

1805
01:57:48,380 --> 01:57:52,340
it is a derivative of the log function this is the natural like functions of

1806
01:57:52,340 --> 01:57:53,780
under the log

1807
01:57:53,790 --> 01:57:55,280
is the

1808
01:57:55,330 --> 01:57:56,960
one of the function

1809
01:57:58,660 --> 01:58:03,930
divide j of log of one minus p

1810
01:58:03,950 --> 01:58:07,000
is one of the one minus p

1811
01:58:07,080 --> 01:58:09,290
and then minus one

1812
01:58:11,560 --> 01:58:16,860
i be debated j of p

1813
01:58:16,870 --> 01:58:19,160
so much so

1814
01:58:20,830 --> 01:58:22,360
both cases

1815
01:58:22,420 --> 01:58:23,910
now what i need to do

1816
01:58:36,330 --> 01:58:38,580
so what

1817
01:58:38,590 --> 01:58:45,010
drop the

1818
01:58:45,060 --> 01:58:47,470
open try out

1819
01:58:57,750 --> 01:59:04,510
let me introduce some notation

1820
01:59:06,170 --> 01:59:08,710
so he

1821
01:59:10,540 --> 01:59:13,840
the exponential of my linear function

1822
01:59:13,860 --> 01:59:16,980
minus some from j equals zero the d

1823
01:59:17,700 --> 01:59:19,930
from data j

1824
01:59:24,180 --> 01:59:25,380
and then

1825
01:59:28,800 --> 01:59:31,130
one of one class e

1826
01:59:31,140 --> 01:59:38,880
so this is my arm logistic regression function and one minus p

1827
01:59:38,910 --> 01:59:45,940
it is of one class e and one plus e minus one is he over

1828
01:59:45,940 --> 01:59:49,380
one class

1829
01:59:59,380 --> 02:00:05,860
use that notation

1830
02:00:11,990 --> 02:00:15,540
the by

1831
02:00:15,550 --> 02:00:17,760
the beta j p

1832
02:00:17,820 --> 02:00:20,450
so that's something i need to work out here

1833
02:00:20,460 --> 02:00:24,720
it's and this is the definition of p

1834
02:00:24,760 --> 02:00:28,060
my so it's

1835
02:00:28,110 --> 02:00:29,560
minus one

1836
02:00:30,740 --> 02:00:33,830
one plus e to the minus two

1837
02:00:37,700 --> 02:00:40,990
ninety religion e

1838
02:00:45,860 --> 02:00:49,090
minus one one person e to the minus two

1839
02:00:49,100 --> 02:00:54,790
and then the this is he is an exponential function so the derivative of an

1840
02:00:54,790 --> 02:00:57,020
exponential is the exponential

1841
02:00:59,930 --> 02:01:01,940
thank you

1842
02:01:03,090 --> 02:01:04,600
and then

1843
02:01:04,610 --> 02:01:06,740
debi debated j

1844
02:01:06,750 --> 02:01:08,880
and now i'm taking the derivative

1845
02:01:08,940 --> 02:01:12,420
inside the exponential which is a linear function

1846
02:01:13,320 --> 02:01:18,090
i just really pick out the term here that involves peter j

1847
02:01:19,980 --> 02:01:21,910
and that is

1848
02:01:25,180 --> 02:01:26,840
so i've

1849
02:01:26,860 --> 02:01:28,720
i looked out the window

1850
02:01:32,380 --> 02:01:33,480
now comes the

1851
02:01:33,500 --> 02:01:36,620
the elegant rewriting parts

1852
02:01:36,630 --> 02:01:38,750
so i can

1853
02:01:39,690 --> 02:01:41,620
these minor says

1854
02:01:41,630 --> 02:01:46,580
and then what i get is one of one class e

1855
02:01:46,590 --> 02:01:51,080
and then he over one plus

1856
02:01:51,120 --> 02:01:53,960
because i had one plus to minus two here

1857
02:01:54,000 --> 02:01:56,720
my times

1858
02:01:59,340 --> 02:02:00,630
which is

1859
02:02:00,630 --> 02:02:04,500
OK so let's think about this a little bit more

1860
02:02:04,790 --> 02:02:12,180
how can we use this space to visualize the performance of these trees is the

1861
02:02:12,180 --> 02:02:14,960
way i like to think about it

1862
02:02:14,980 --> 02:02:17,990
if i had treatment for leaves

1863
02:02:18,150 --> 02:02:24,660
think about possible ways that label tree for these two possible classes so after four

1864
02:02:24,670 --> 02:02:31,070
sixteen possible it and all of these can be actually visualized as points in this

1865
02:02:31,310 --> 02:02:34,750
space so for instance the labelling

1866
02:02:34,770 --> 02:02:39,810
plus minus minus minus which corresponds to this point here

1867
02:02:40,900 --> 02:02:44,250
the label minus plus plus

1868
02:02:44,300 --> 02:02:49,510
doesn't matter and all of this is something that needs to be really good morning

1869
02:02:49,510 --> 02:02:51,160
class class

1870
02:02:51,280 --> 02:02:56,330
all correctly that one negative

1871
02:02:56,360 --> 02:02:58,810
and two

1872
02:02:59,930 --> 02:03:02,910
six five days cells

1873
02:03:04,400 --> 02:03:06,470
so mind

1874
02:03:06,470 --> 02:03:09,060
one of class were

1875
02:03:15,130 --> 02:03:17,020
and actually

1876
02:03:17,040 --> 02:03:17,850
one you

1877
02:03:17,880 --> 02:03:24,120
the figure out that they symmetrically opposite so it's point through centre of gravity of

1878
02:03:24,120 --> 02:03:26,140
the space of one from the

1879
02:03:26,150 --> 02:03:29,760
the midpoint in that so you can do

1880
02:03:29,760 --> 02:03:32,840
it is the space but the and the reason why

1881
02:03:32,860 --> 02:03:34,750
it is that some

1882
02:03:34,760 --> 02:03:41,550
all possible represent classifiers structure from this tree has to somehow go

1883
02:03:42,650 --> 02:03:46,210
some of these sixteen points there are no more points

1884
02:03:46,210 --> 02:03:47,300
in in in

1885
02:03:47,310 --> 02:03:50,530
what is this is what discrete ricci

1886
02:03:50,530 --> 02:03:51,730
on this

1887
02:03:51,750 --> 02:03:54,270
on this dataset tracks

1888
02:03:54,590 --> 02:03:57,830
and the actual

1889
02:03:58,210 --> 02:04:05,340
by using here of regular i use some of the points fact the point which

1890
02:04:05,340 --> 02:04:07,880
which is the convex hull of this

1891
02:04:08,030 --> 02:04:11,200
the point is that it's not a coincidence

1892
02:04:12,410 --> 02:04:17,800
now let's use this idea to to think about how history being built so

1893
02:04:17,810 --> 02:04:22,610
decision tree building is nice because the process so what we do we find the

1894
02:04:22,610 --> 02:04:28,200
best the world because the tree by the

1895
02:04:28,330 --> 02:04:35,990
so far we can actually visualize very nicely in ROC space so because just think

1896
02:04:35,990 --> 02:04:40,990
about it if nothing else then basically we have since that year

1897
02:04:41,000 --> 02:04:45,220
and if we use this to rank with the

1898
02:04:45,230 --> 02:04:49,300
go from this point the left the sixth of this form

1899
02:04:49,340 --> 02:04:51,500
about four

1900
02:04:51,500 --> 02:04:54,860
six so i think history is visualized

1901
02:04:55,740 --> 02:04:56,980
by this

1902
02:04:56,990 --> 02:05:00,840
and what we can do about growing tree in the lab

1903
02:05:00,990 --> 02:05:05,050
strong tree and let me know if you need to find

1904
02:05:05,170 --> 02:05:06,290
who here

1905
02:05:06,310 --> 02:05:09,560
so the next get something like this

1906
02:05:09,580 --> 02:05:15,960
and then finally in the that's that we grow right to get something like this

1907
02:05:15,960 --> 02:05:17,810
OK now is

1908
02:05:17,840 --> 02:05:21,780
i can see some of the questions that some new which is

1909
02:05:21,780 --> 02:05:26,020
it doesn't quite look like the raw the show before

1910
02:05:26,040 --> 02:05:29,530
so what answer is about what the show you

1911
02:05:29,730 --> 02:05:33,530
here is not the right thing or it's just the waters

1912
02:05:33,550 --> 02:05:34,990
tree is being built

1913
02:05:35,010 --> 02:05:40,030
we therefore trace the decision trees don't always ordered

1914
02:05:40,040 --> 02:05:41,170
from left to right

1915
02:05:41,270 --> 02:05:44,280
so we have an additional step this is sure

1916
02:05:44,490 --> 02:05:45,790
social thing

1917
02:05:45,800 --> 02:05:48,080
we take a stronger

1918
02:05:48,110 --> 02:05:49,710
and then

1919
02:05:49,720 --> 02:05:54,040
using posterior also realize those leaves

1920
02:05:54,050 --> 02:05:58,370
and we get something like this

1921
02:05:58,390 --> 02:06:03,370
on that by very first animation and

1922
02:06:05,380 --> 02:06:14,280
that means that you

1923
02:06:14,300 --> 02:06:16,390
so this is the key

1924
02:06:16,450 --> 02:06:19,780
decision trees have some kind of syntactic structure

1925
02:06:19,780 --> 02:06:23,810
but that does not modify the ranking order we don't have stay here or to

1926
02:06:23,820 --> 02:06:27,850
believe that we find that the water so we have the those vectors

1927
02:06:27,870 --> 02:06:32,540
but we put them in different order and maybe you can quite easily proven to

1928
02:06:33,250 --> 02:06:34,210
for some time

1929
02:06:34,220 --> 02:06:38,360
on the training set you will always get the context right

1930
02:06:38,380 --> 02:06:42,510
so i think the decision trees you get is the best friend

1931
02:06:43,630 --> 02:06:48,740
as is already such a link between the classification

1932
02:06:48,980 --> 02:06:52,570
OK that's what i was

1933
02:06:55,590 --> 02:06:59,570
it may not be immediately obvious naive bayes is tricky

1934
02:06:59,570 --> 02:07:05,860
certainly if you have this free use it is a a tree like structure one

1935
02:07:06,810 --> 02:07:08,630
now page needs to

1936
02:07:08,640 --> 02:07:14,430
estimate probability for each possible combination of attribute values and that's what we do with

1937
02:07:14,430 --> 02:07:15,840
this country

1938
02:07:15,850 --> 02:07:17,110
is it true that

1939
02:07:17,120 --> 02:07:21,980
of course is always complete decision trees so there's no pruning you need

1940
02:07:22,870 --> 02:07:26,070
all the possible combinations

1941
02:07:26,070 --> 02:07:31,380
also of course there are many different ways in which to draw complete decision trees

1942
02:07:31,710 --> 02:07:36,770
and here is just i just use three short years before so what is happening

1943
02:07:37,700 --> 02:07:41,370
one basic probability estimate

1944
02:07:43,430 --> 02:07:48,310
the joint probability that use marginal probabilities to estimate the joint probability

1945
02:07:49,170 --> 02:07:52,310
you can understand follow the

1946
02:07:52,330 --> 02:07:56,640
prior of here which is uniform so that can do anything

1947
02:07:56,670 --> 02:07:59,370
then i have

1948
02:08:00,070 --> 02:08:02,960
the model for a

1949
02:08:02,970 --> 02:08:04,420
but i will zero

1950
02:08:04,490 --> 02:08:07,530
the market of b are all in

1951
02:08:07,550 --> 02:08:14,470
six of this form that the model for the long before this expected

1952
02:08:14,490 --> 02:08:16,060
one now

