1
00:00:00,000 --> 00:00:03,640
i think an interesting future work is to try to combine these two methods for

2
00:00:04,530 --> 00:00:09,990
using state splitting approaches for the tag based grammar

3
00:00:10,040 --> 00:00:11,580
OK so

4
00:00:11,620 --> 00:00:16,260
now that's an instrument i want to talk a bit about machine translation

5
00:00:16,270 --> 00:00:21,140
as well application of these kind of models and the interesting thing here is that

6
00:00:21,140 --> 00:00:25,110
some of the techniques of showing in parsing not being carried across two

7
00:00:25,200 --> 00:00:27,850
machine learning machine translation

8
00:00:27,910 --> 00:00:33,560
so in statistical approaches to translation again we think this is the structured prediction problem

9
00:00:33,920 --> 00:00:36,850
mapping strings and say german strings in english

10
00:00:36,860 --> 00:00:41,480
and we have a potentially very large set of example translations to we might have

11
00:00:41,480 --> 00:00:45,740
a few hundred thousand examples of translations between the two languages

12
00:00:45,790 --> 00:00:51,390
now you can think of two possible approaches roughly speaking one approach pretraining this mapping

13
00:00:52,340 --> 00:00:55,060
you don't make so much use of latent structure

14
00:00:55,090 --> 00:00:59,370
a second possible approaches where you might use the syntactic structures

15
00:00:59,390 --> 00:01:01,250
maybe this grammatical relations

16
00:01:01,270 --> 00:01:05,140
as the number of intermediate structure in form of this mapping

17
00:01:05,190 --> 00:01:07,390
so let's talk about these two approaches

18
00:01:07,440 --> 00:01:09,910
so the first is free space translation

19
00:01:09,930 --> 00:01:14,470
so this was a major breakthrough in statistical machine translation is really impressive will be

20
00:01:15,350 --> 00:01:16,620
i have worked

21
00:01:16,640 --> 00:01:18,440
in phrase based models

22
00:01:18,460 --> 00:01:19,490
the basic

23
00:01:19,520 --> 00:01:22,670
the components of the basic entries in the model

24
00:01:22,680 --> 00:01:23,980
is a lexico

25
00:01:24,010 --> 00:01:26,840
of the so-called phrase pairs

26
00:01:27,670 --> 00:01:29,590
each entry in this lexicon

27
00:01:29,640 --> 00:01:31,690
looks like a german substring

28
00:01:31,740 --> 00:01:33,250
for example and then again

29
00:01:33,260 --> 00:01:36,120
with an english substring over here

30
00:01:36,190 --> 00:01:41,000
and we might potentially induce very large lexicon of entries in this form for a

31
00:01:41,000 --> 00:01:43,380
set of example translations

32
00:01:43,390 --> 00:01:47,700
now given this lexicon transition proceeds in the following two steps

33
00:01:47,730 --> 00:01:52,480
so firstly we segment the german string

34
00:01:52,530 --> 00:01:54,680
two series of german segments

35
00:01:54,750 --> 00:01:58,300
and for each of those segments we choose an english translation

36
00:01:58,310 --> 00:01:59,990
so essentially the step you

37
00:02:00,000 --> 00:02:02,490
we have a bunch of we segmentation

38
00:02:02,510 --> 00:02:06,240
and now we have a bunch of english fragments down here translations were drawn from

39
00:02:06,240 --> 00:02:08,190
this lexicon

40
00:02:08,230 --> 00:02:09,670
in the second step

41
00:02:09,680 --> 00:02:11,830
we choose some

42
00:02:11,840 --> 00:02:13,710
the resulting english forces

43
00:02:13,730 --> 00:02:16,470
so we might move things around a little bit

44
00:02:16,490 --> 00:02:18,330
so in particular

45
00:02:18,340 --> 00:02:20,270
the subject here elections

46
00:02:20,300 --> 00:02:22,350
this come after the verb take

47
00:02:22,360 --> 00:02:25,870
in german german has quite flexible word order so you often see this kind of

48
00:02:26,470 --> 00:02:28,720
what the subject was asked to appear

49
00:02:28,730 --> 00:02:30,730
for the first in english

50
00:02:30,740 --> 00:02:36,490
so we sort of scrambled these phrases arrange them to produce this new translation

51
00:02:37,630 --> 00:02:41,680
these two steps the choice of the phrases in the two the ordering of course

52
00:02:41,680 --> 00:02:44,080
probabilities or associated costs

53
00:02:44,140 --> 00:02:49,270
which is somehow estimated from from corpus in particular in the real world instead

54
00:02:49,350 --> 00:02:51,620
you make heavy use of the language model

55
00:02:52,680 --> 00:02:56,270
the strings in the language for example the tri grams language model

56
00:02:56,270 --> 00:02:59,230
so these government in transition

57
00:02:59,270 --> 00:03:03,080
but they really don't make any direct use of syntactic information

58
00:03:03,100 --> 00:03:05,390
you might think for example that knowing

59
00:03:05,390 --> 00:03:10,870
the different arguments arguments with the different grammatical relations are on the german side might

60
00:03:10,870 --> 00:03:15,770
be useful in predicting the structure of english tree

61
00:03:15,790 --> 00:03:20,540
so recent work for example i assign the two examples of shown you share

62
00:03:20,540 --> 00:03:27,080
has started to fray the transition process essentially as opposing task

63
00:03:27,080 --> 00:03:32,790
so it would very interesting take i think on translation and shown they were quite

64
00:03:32,790 --> 00:03:34,500
impressive results

65
00:03:34,540 --> 00:03:38,790
particularly in chinese which is the language which has very different from english

66
00:03:39,460 --> 00:03:40,870
in this idea

67
00:03:40,870 --> 00:03:44,620
the phrase is augmented to carry with them

68
00:03:44,810 --> 00:03:47,080
pieces of syntactic structure

69
00:03:47,100 --> 00:03:49,810
so now for his entry in our model

70
00:03:49,830 --> 00:03:51,960
my ten this german strength

71
00:03:52,040 --> 00:03:55,850
with an english crane which has appeared tree fragment of it

72
00:03:55,870 --> 00:03:58,350
so maybe past the english corpus

73
00:03:58,350 --> 00:04:02,270
and annotated or in the strings with these past tree fragment

74
00:04:02,270 --> 00:04:05,140
and say, you figure it out, try and maximize this

75
00:04:05,210 --> 00:04:10,710
and that's a much better way to do abstraction

76
00:04:10,770 --> 00:04:14,120
so if i believe there should be a vertical edge i can say

77
00:04:16,870 --> 00:04:18,030
get rid of all the,

78
00:04:18,030 --> 00:04:22,320
you know, that modulates the correlational structure, but you figure out what the intensity should be

79
00:04:22,320 --> 00:04:25,390
so i don't have to know what the vertical edge means in terms of intensities

80
00:04:25,410 --> 00:04:28,430
i know what it means in terms of correlational structure

81
00:04:28,470 --> 00:04:31,040
and that means i can leave the fine details to the guys at the level

82
00:04:32,570 --> 00:04:35,780
who of course in turn off-load to the guys in the layer below that

83
00:04:35,790 --> 00:04:38,820
that's clearly very good as a management structure

84
00:04:38,860 --> 00:04:41,240
where my new graduate students that come in

85
00:04:41,290 --> 00:04:44,660
i just explain the objective function, which is the number of papers in my

86
00:04:45,430 --> 00:04:51,180
and leave them to just optimise the details

87
00:04:52,150 --> 00:04:56,560
i'm gonna finish by

88
00:04:56,600 --> 00:05:00,020
showing you how to apply all this to time series models

89
00:05:00,030 --> 00:05:04,690
and we have applied this three-way factored stuff, and it's published in the latest ICML

90
00:05:04,690 --> 00:05:06,900
conference two thousand nine

91
00:05:07,570 --> 00:05:09,030
graham taylor and me

92
00:05:09,040 --> 00:05:11,290
but i'm gonna describe stuff slightly before that

93
00:05:11,310 --> 00:05:14,400
it's obvious how to generalize it to three way

94
00:05:14,410 --> 00:05:16,930
normally when people have to deal with time series

95
00:05:19,100 --> 00:05:23,440
they would like to use non linear distributed representations because they're nice and powerful

96
00:05:23,450 --> 00:05:25,390
but they're hard to learn

97
00:05:25,390 --> 00:05:28,620
so they tend to avoid distributed representations and use things like

98
00:05:28,640 --> 00:05:33,860
hidden markov models or avoid nonlinear and use things like linear dynamical systems

99
00:05:33,910 --> 00:05:36,990
we'd like to have our cake and eat it

100
00:05:37,070 --> 00:05:40,660
and if we really do want to use these distributed representations

101
00:05:43,000 --> 00:05:46,940
to make it feasible we want to make inference much simpler in these models

102
00:05:48,290 --> 00:05:51,280
there's a couple of tricks for doing that. the main trick is to say

103
00:05:51,280 --> 00:05:55,440
instead of having a directed model, we'll have an undirected model

104
00:05:55,490 --> 00:05:59,700
in other words, we'll have hidden units and visible units that are a little restricted boltzmann machine

105
00:05:59,780 --> 00:06:01,690
so i have an undirected model

106
00:06:01,690 --> 00:06:04,430
but now we're gonna condition that model

107
00:06:04,440 --> 00:06:07,570
on previous frames of data

108
00:06:07,650 --> 00:06:10,240
and that's gonna give us a temporal model we can learn and then we can

109
00:06:10,240 --> 00:06:13,190
stack up in multiple layers

110
00:06:13,250 --> 00:06:17,860
so let me show you the application domain first, and then i'll show you what the models look like

111
00:06:17,860 --> 00:06:21,900
you put markers all over somebody and lots, have lots of infrared cameras

112
00:06:21,930 --> 00:06:24,110
and then you can see where the joints are in 3-D

113
00:06:24,180 --> 00:06:26,480
and then you can convert that into joint angles

114
00:06:26,490 --> 00:06:29,780
and then you can represent their motion by frames of data in which each frame

115
00:06:29,780 --> 00:06:31,990
has like thirty five joint angles

116
00:06:33,810 --> 00:06:38,270
six more numbers that say the translation and rotation of the base of the spine

117
00:06:38,320 --> 00:06:40,940
actually for the rotation

118
00:06:40,970 --> 00:06:43,360
for rotation this way,

119
00:06:43,400 --> 00:06:44,580
you give the

120
00:06:44,620 --> 00:06:46,240
change in angle

121
00:06:46,250 --> 00:06:48,150
at each time step

122
00:06:48,190 --> 00:06:50,100
but for a rotation this way

123
00:06:50,100 --> 00:06:53,530
and this way you give the absolute angle because physics cares a lot about you being like

124
00:06:53,530 --> 00:06:56,180
that if you're walking, or being like this

125
00:06:56,190 --> 00:07:00,120
so for roll and pitch we give the absolute angle, and for yaw we give the

126
00:07:00,140 --> 00:07:01,530
delta angle

127
00:07:01,580 --> 00:07:04,360
and we do it all in body centered coordinates

128
00:07:04,440 --> 00:07:08,070
that all turned out to be important

129
00:07:08,180 --> 00:07:11,970
so the basic conditional RBM looks like this

130
00:07:12,020 --> 00:07:14,310
here's your RBM

131
00:07:14,360 --> 00:07:18,390
here's your joint angles and your translations and rotations of the base of the spine

132
00:07:18,440 --> 00:07:20,330
here's some binary hidden units

133
00:07:20,390 --> 00:07:22,230
these are gaussian variables

134
00:07:22,360 --> 00:07:24,480
here's previous timeframes

135
00:07:24,620 --> 00:07:27,150
here's some conditioning connections here

136
00:07:27,150 --> 00:07:29,320
some conditioning connections here

137
00:07:29,370 --> 00:07:33,770
so the effect of those conditioning connections is just to change the effective biases

138
00:07:33,810 --> 00:07:35,440
of these units and these units

139
00:07:35,490 --> 00:07:42,640
so as far as the RBM is concerned, is an RBM whose biases keep being changed by the history

140
00:07:42,680 --> 00:07:46,470
if you ignore this stuff, this is an autoregressive model, it's a standard autoregressive model,

141
00:07:46,500 --> 00:07:49,970
right? you have linear stuff that's linearly dependent on the past

142
00:07:49,990 --> 00:07:53,540
now autoregressive models are pretty good at dealing with continuity

143
00:07:53,650 --> 00:07:56,690
but they're not good at sustained generation

144
00:07:56,740 --> 00:07:59,440
their biggest eigenvalue's either bigger than one or smaller than one

145
00:07:59,560 --> 00:08:01,810
so either they'll blow up or they'll collapse

146
00:08:01,860 --> 00:08:06,280
if you make the biggest eigenvalue a little smaller than one and drive them noise, they can keep going, but

147
00:08:06,280 --> 00:08:09,360
they won't produce models nearly as good as the ones i'm going to show you

148
00:08:09,370 --> 00:08:11,320
OK so we know how to learn this

149
00:08:12,660 --> 00:08:14,530
we now know how to learn these connections

150
00:08:14,580 --> 00:08:17,270
you put in data

151
00:08:17,280 --> 00:08:19,780
you then activate hidden units

152
00:08:19,930 --> 00:08:25,060
you then reconstruct the data but taking these conditioning connections into account

153
00:08:25,070 --> 00:08:26,950
and then you reconstruct,

154
00:08:26,970 --> 00:08:28,600
reactivate the hidden units

155
00:08:28,620 --> 00:08:32,570
and you use the difference in the pairwise statistics to learn those

156
00:08:32,570 --> 00:08:37,050
you learn the difference between the activity levels of the hidden units with data and with

157
00:08:37,050 --> 00:08:41,890
reconstructions of data, take that difference, and back propagate it to learn these weights

158
00:08:41,940 --> 00:08:44,570
in other words, that difference times these activities

159
00:08:44,620 --> 00:08:46,270
learns those weights

160
00:08:46,310 --> 00:08:50,330
and similarly the difference between these states with the data from these states in the

161
00:08:53,190 --> 00:08:55,440
this activity here, learns this weight

162
00:08:56,110 --> 00:08:59,160
so we can learn all these weights and these weights and these weights

163
00:08:59,190 --> 00:09:01,940
we can generate from the model

164
00:09:01,990 --> 00:09:03,030
like this

165
00:09:03,030 --> 00:09:05,360
given two previous frames

166
00:09:05,400 --> 00:09:07,940
that determines the biases here

167
00:09:07,990 --> 00:09:09,490
via these connections

168
00:09:09,500 --> 00:09:11,860
and now we can go backwards and forwards for a while

169
00:09:12,030 --> 00:09:13,320
and generate a new frame

170
00:09:13,400 --> 00:09:15,910
and we don't have to go back and forth very long because

171
00:09:15,930 --> 00:09:20,750
with these biases strongly determined by the past there's not much this can do.

172
00:09:20,770 --> 00:09:25,810
it typically has to be very similar to this last frame

173
00:09:25,820 --> 00:09:28,330
most importantly we can stack them up

174
00:09:30,180 --> 00:09:31,890
i can learn this model

175
00:09:33,000 --> 00:09:35,900
with no connections here. these connections aren't here yet

176
00:09:35,950 --> 00:09:38,360
once learn this model

177
00:09:38,360 --> 00:09:40,730
i can now take time frames

178
00:09:40,730 --> 00:09:42,120
of observed data

179
00:09:42,190 --> 00:09:45,810
and convert them into time frames of hidden units

180
00:09:45,850 --> 00:09:47,570
but without these connections here

181
00:09:47,580 --> 00:09:49,600
so now i've got a hidden sequence

182
00:09:49,600 --> 00:09:51,850
and now i can say, well that hidden sequence,

183
00:09:52,180 --> 00:09:54,560
let me go back

184
00:09:54,830 --> 00:09:57,820
these weights

185
00:09:59,790 --> 00:10:02,660
a prior distribution over these hidden units

186
00:10:02,680 --> 00:10:05,430
given these conditioning connections

187
00:10:05,480 --> 00:10:08,850
and they also define the conditional distribution for v given h

188
00:10:08,850 --> 00:10:13,830
or the two PY type of orbitals that were combining so this the same shape

189
00:10:13,830 --> 00:10:17,350
as the shape of the orbit or the shape of the wave function

190
00:10:17,410 --> 00:10:22,450
and we can call this either two p a being compiled with two px b

191
00:10:22,470 --> 00:10:27,270
or we can say since the same shape it's two PY being combined with two

192
00:10:28,950 --> 00:10:33,450
and in either case if we first talk about constructive interference what again we're going

193
00:10:33,450 --> 00:10:38,330
to see is that where these two orbitals come together we're going to see increased

194
00:10:38,330 --> 00:10:42,560
wave function in that area so we saw constructive interference

195
00:10:42,620 --> 00:10:48,430
so again we can name the molecular orbitals and these were going to call also

196
00:10:48,430 --> 00:10:52,890
to point out there is now a bond axis along this nodal plane which is

197
00:10:52,890 --> 00:10:56,540
something we didn't see before were combined as orbital

198
00:10:56,660 --> 00:11:00,390
so we go ahead and name is we're going to call these pi orbitals

199
00:11:00,390 --> 00:11:06,290
well call it either pi two px if we're combined at orbitals four pi two

200
00:11:09,040 --> 00:11:14,160
the reason that i wanted to point out the nodal plane here because this is

201
00:11:14,160 --> 00:11:19,680
why it is called a pi orbitals pi orbitals molecular orbital that have a nodal

202
00:11:19,680 --> 00:11:25,450
plane through the bond axis remember this is our bond axis here you can see

203
00:11:25,450 --> 00:11:29,220
there is this area where the weight function is equal to zero all along that

204
00:11:29,220 --> 00:11:34,270
plane that the normal plain so that's why these are pi orbitals instead of sigma

205
00:11:35,490 --> 00:11:38,270
so again we can think about the probability density

206
00:11:38,290 --> 00:11:42,970
in terms of squaring the function so now what it is that we're squaring is

207
00:11:42,970 --> 00:11:48,600
that we're talking about the px orbital pi two px square

208
00:11:48,640 --> 00:11:50,680
and this is just equal to

209
00:11:53,020 --> 00:11:54,390
that two p

210
00:11:54,410 --> 00:11:59,120
x a plus the two pxb all square or free read write out all of

211
00:11:59,120 --> 00:12:00,100
the terms

212
00:12:00,160 --> 00:12:06,120
we have two px a squared plus two pxb square and then this term here

213
00:12:06,430 --> 00:12:13,100
and again this is our interference term in this case the constructive or destructive interference

214
00:12:13,100 --> 00:12:18,140
constructive interference we're seeing that the wave function adding together and giving us more weight

215
00:12:18,140 --> 00:12:21,640
function in the centre here

216
00:12:21,720 --> 00:12:27,040
all right so we see constructive interference of course we can also see destructive interference

217
00:12:27,060 --> 00:12:31,200
so i changed the colors here to show that these are two p orbitals with

218
00:12:31,200 --> 00:12:32,520
an opposite

219
00:12:32,640 --> 00:12:38,010
or opposite signs so what happens when we add a two p a and we

220
00:12:38,010 --> 00:12:39,620
subtract from it

221
00:12:39,810 --> 00:12:45,510
two pxb were the same with the two PY eight subtracting two PYP that we're

222
00:12:45,510 --> 00:12:50,600
actually going to cancel out the weight function in the centre so we now have

223
00:12:50,620 --> 00:12:54,950
two nodal planes so again this is an insight bonding orbital and what you see

224
00:12:54,950 --> 00:12:56,910
is that there is now less

225
00:12:57,040 --> 00:13:00,220
electron density between the new to the two nuclei

226
00:13:00,220 --> 00:13:04,120
then there was when you had a non-binding

227
00:13:04,140 --> 00:13:07,930
so we're going to call this the sigma two px star

228
00:13:07,930 --> 00:13:12,080
or talking about the two PY orbitals will call this

229
00:13:12,080 --> 00:13:20,950
the excuse me the pi two px star and the pi two PY star

230
00:13:20,970 --> 00:13:26,240
and the pi star orbitals result from any time you have destructive interference from two

231
00:13:26,240 --> 00:13:31,390
p orbitals that are either the p acts or the PY

232
00:13:31,430 --> 00:13:34,310
so now we can move on to an example where we do in fact have

233
00:13:34,310 --> 00:13:38,790
to use some p orbitals so this would be be two

234
00:13:38,830 --> 00:13:42,080
how many electrons are in boron

235
00:13:42,200 --> 00:13:47,700
i the same hand going up there's five electrons so what you'll notice here that

236
00:13:47,700 --> 00:13:53,310
i only in three electrons with what do these electrons represents

237
00:13:53,370 --> 00:13:57,330
valence electrons OK so sometimes you're going to be asked to draw a molecular orbital

238
00:13:57,330 --> 00:14:02,350
diagram where you asked to include all electrons and sometimes will specifically say

239
00:14:02,870 --> 00:14:08,390
only include valence electrons that happens because of space issues that you have to do

240
00:14:08,390 --> 00:14:11,810
that because you can always assume that all of the core

241
00:14:11,810 --> 00:14:15,740
orbitals are already going to be filled so in this case we're just trying to

242
00:14:15,740 --> 00:14:20,850
molecular orbital diagram for the valence electrons so we have three for each and what

243
00:14:20,850 --> 00:14:25,850
we see here is now more combine the p we have our two px two

244
00:14:25,850 --> 00:14:29,080
py y py orbitals that are lower in energy

245
00:14:29,100 --> 00:14:33,640
and then our pi bonding orbitals that are higher in energy

246
00:14:33,750 --> 00:14:37,520
you might be asking where the two pz orbital and then we'll get to that

247
00:14:37,520 --> 00:14:43,160
soon once we need let's just first villainous for the b two k

248
00:14:43,160 --> 00:14:47,390
so we can start at the bottom two electrons in sigma two as

249
00:14:47,430 --> 00:14:50,770
two electrons and two sigma two as star

250
00:14:50,770 --> 00:14:55,740
now we need to jump up to using these pi orbitals and what we're going

251
00:14:55,740 --> 00:14:59,790
to do is put one electron into each of our pi

252
00:14:59,830 --> 00:15:02,740
two px and two py y orbitals

253
00:15:02,750 --> 00:15:07,370
so again you can see as we're filling up molecular orbitals we're using the exact

254
00:15:07,370 --> 00:15:13,290
same principle we used to fill up atomic orbitals

255
00:15:13,310 --> 00:15:18,180
so let's think about what the valence electron configuration is here

256
00:15:18,220 --> 00:15:21,060
so now we're looking at the case of

257
00:15:21,120 --> 00:15:23,140
p two

258
00:15:23,140 --> 00:15:27,970
and what we're looking at is a valence electron molecular orbital diagram so let's just

259
00:15:27,970 --> 00:15:34,220
rather electron configuration for the valence orbitals so that will be thinking about two hours

260
00:15:34,220 --> 00:15:41,780
that is each survey of x is equal to h survey

261
00:15:41,790 --> 00:15:42,850
of y

262
00:15:44,760 --> 00:15:46,390
what it means

263
00:15:46,450 --> 00:15:49,470
for them to clive

264
00:15:49,490 --> 00:15:54,520
so that implies

265
00:15:54,590 --> 00:15:56,830
that the sum

266
00:15:56,840 --> 00:16:00,480
i equals zero to are

267
00:16:00,520 --> 00:16:02,270
a supply

268
00:16:02,280 --> 00:16:03,310
x y

269
00:16:03,330 --> 00:16:05,140
is equal to the sum

270
00:16:05,190 --> 00:16:07,800
of i equals zero to are

271
00:16:07,810 --> 00:16:10,150
of the supply

272
00:16:10,210 --> 00:16:12,540
why so i

273
00:16:14,300 --> 00:16:16,340
actually this is congruent

274
00:16:16,360 --> 00:16:20,700
nine and so congruence those people have seen much number theory

275
00:16:20,730 --> 00:16:22,320
is basically the way

276
00:16:23,290 --> 00:16:26,520
of essentially rather than having to say

277
00:16:26,530 --> 00:16:30,060
mod everywhere in here and mod everywhere in here

278
00:16:30,080 --> 00:16:34,310
we just at the end say OK two and model

279
00:16:34,360 --> 00:16:37,110
you know PN everything is being done model

280
00:16:37,160 --> 00:16:38,340
o module m

281
00:16:39,830 --> 00:16:40,530
and then

282
00:16:40,530 --> 00:16:43,050
typically we use congruent sign

283
00:16:43,060 --> 00:16:45,710
it is more mathematical definition

284
00:16:45,710 --> 00:16:49,840
but this will work for her as engineers

285
00:16:54,940 --> 00:16:59,550
everybody with me so far just applying the definition

286
00:17:00,910 --> 00:17:02,600
that implies

287
00:17:02,700 --> 00:17:08,000
that the sum of i corps zero to are

288
00:17:09,340 --> 00:17:10,250
they are i

289
00:17:10,250 --> 00:17:13,310
x y minus y i

290
00:17:13,360 --> 00:17:16,970
congress zero money

291
00:17:17,110 --> 00:17:23,670
data stored on the other side and

292
00:17:23,780 --> 00:17:28,300
apply the distributive law

293
00:17:28,330 --> 00:17:32,410
now what i'm going to do is pull out

294
00:17:32,540 --> 00:17:36,610
the zero position because that's the one that i care about

295
00:17:36,640 --> 00:17:38,850
this is where it saves me on the map

296
00:17:38,900 --> 00:17:40,460
comparative i didn't

297
00:17:40,470 --> 00:17:42,360
say there was zero

298
00:17:42,380 --> 00:17:46,250
i have to pull out i wouldn't matter but it just would make the map

299
00:17:46,260 --> 00:17:47,450
a little bit

300
00:17:55,480 --> 00:18:09,040
OK so now we just pulled out one term

301
00:18:11,040 --> 00:18:12,340
that implies

302
00:18:16,880 --> 00:18:18,750
a zero

303
00:18:18,770 --> 00:18:21,380
zero minus y zero

304
00:18:21,440 --> 00:18:24,520
congruent to minus

305
00:18:37,000 --> 00:18:41,320
remember that when i minus number mod m

306
00:18:42,590 --> 00:18:48,500
i just map it into whatever into that range from zero to in minus one

307
00:18:48,500 --> 00:18:50,380
so for example

308
00:18:50,380 --> 00:18:56,020
you know minus five mod seven is

309
00:18:57,750 --> 00:18:58,920
OK so

310
00:18:58,960 --> 00:19:01,460
if any of these things are negative

311
00:19:02,130 --> 00:19:03,690
we simply

312
00:19:03,730 --> 00:19:09,090
translate them into by adding multiples of them because adding multiples of them

313
00:19:09,090 --> 00:19:11,230
doesn't affect

314
00:19:20,900 --> 00:19:22,360
and now

315
00:19:22,380 --> 00:19:28,020
the next step we need to use in number theory fact so let's

316
00:19:28,040 --> 00:19:30,840
the latter number theory

317
00:19:45,320 --> 00:19:47,190
take a little digression

318
00:19:47,190 --> 00:19:56,400
OK right so this comes from the theory of finite fields

319
00:19:56,440 --> 00:19:59,270
so for people who were

320
00:20:00,380 --> 00:20:03,170
that's where your plugging your knowledge in

321
00:20:03,170 --> 00:20:06,750
if you're not knowledgable is a great area math learn about

322
00:20:08,270 --> 00:20:10,070
so here's the

323
00:20:10,090 --> 00:20:11,400
the fact

324
00:20:11,440 --> 00:20:13,770
so let m be prime

325
00:20:15,520 --> 00:20:16,750
then for any

326
00:20:16,750 --> 00:20:24,290
xe element of

327
00:20:24,360 --> 00:20:32,250
xe seven an c them is the integers

328
00:20:32,250 --> 00:20:34,270
o mod m

329
00:20:34,320 --> 00:20:37,500
OK so this is essentially numbers from zero

330
00:20:40,270 --> 00:20:42,520
r two n minus one

331
00:20:42,540 --> 00:20:45,790
with all the operations you know times

332
00:20:45,810 --> 00:20:47,730
minus lost its error

333
00:20:47,810 --> 00:20:53,920
OK defined on that such that if you're and the outside the range of zero

334
00:20:53,980 --> 00:20:59,320
ten minus one the renormalized by subtracting or adding multiples of them to get back

335
00:20:59,320 --> 00:21:02,040
within the range from syria minus one

336
00:21:02,170 --> 00:21:06,190
the standard thing and just doing things modulo n

337
00:21:08,070 --> 00:21:10,860
so for any easy such that c

338
00:21:10,900 --> 00:21:13,960
OK is

339
00:21:14,020 --> 00:21:17,170
not congruent to zero

340
00:21:17,250 --> 00:21:19,230
there exists a unique

341
00:21:22,500 --> 00:21:23,980
xe inverse

342
00:21:26,570 --> 00:21:28,520
and since the them

343
00:21:29,480 --> 00:21:32,500
such that if i multiplies the times the

344
00:21:34,750 --> 00:21:39,130
produces something congruent to one mod m

345
00:21:39,150 --> 00:21:46,110
so for any number it says

346
00:21:46,130 --> 00:21:48,460
i find another number

347
00:21:48,480 --> 00:21:50,290
one multiplied by it

348
00:21:50,290 --> 00:21:51,290
it's me

349
00:21:52,520 --> 00:21:54,770
so let's just do an example

350
00:21:54,790 --> 00:21:58,570
for an equal seven

351
00:21:58,650 --> 00:22:04,040
so here we have michael table

352
00:22:04,040 --> 00:22:08,670
so is is not equal to zero so i'd just write down the other numbers

353
00:22:08,770 --> 00:22:15,420
let's figure out what the inverses

354
00:22:15,460 --> 00:22:19,210
OK so what's the inverse of one

355
00:22:19,230 --> 00:22:22,250
but number one multiplied by one gives me one

356
00:22:23,730 --> 00:22:26,440
how about two

357
00:22:26,480 --> 00:22:31,440
number one i multiply by two gives me one

358
00:22:32,460 --> 00:22:33,960
OK because

359
00:22:34,000 --> 00:22:36,270
two times for his age

360
00:22:36,320 --> 00:22:38,860
in eight is congruent to one

361
00:22:38,880 --> 00:22:41,270
mod seven

362
00:22:41,320 --> 00:22:43,210
OK renormalized

363
00:22:43,290 --> 00:22:53,940
what about three

364
00:22:53,960 --> 00:22:57,710
five good five three ten five fifteen

365
00:22:57,770 --> 00:23:02,250
that's congruent to one mod seven because fifteen

366
00:23:02,250 --> 00:23:03,230
divided by

367
00:23:03,250 --> 00:23:06,170
seven is two remainder want

368
00:23:06,230 --> 00:23:10,460
so that's the thing or about four

369
00:23:16,320 --> 00:23:24,320
and six

370
00:23:25,540 --> 00:23:28,320
sixty six

371
00:23:28,420 --> 00:23:31,270
six times six is thirty six

372
00:23:31,320 --> 00:23:32,690
OK mod seven

373
00:23:32,730 --> 00:23:35,750
basically cracked up thirty five

374
00:23:35,810 --> 00:23:36,920
his me one

375
00:23:37,730 --> 00:23:41,170
so you people observe some interesting facts that

376
00:23:41,210 --> 00:23:42,880
the if

377
00:23:42,940 --> 00:23:46,020
if r one numbers and inverse of another

378
00:23:46,090 --> 00:23:49,590
then that other is inverse of the one

379
00:23:50,650 --> 00:23:51,920
that's actually

380
00:23:51,940 --> 00:23:53,960
one of these things improve when you do

381
00:23:53,960 --> 00:23:55,300
like this

382
00:23:55,320 --> 00:23:57,310
there's a book

383
00:23:57,320 --> 00:24:01,650
because i think it's called the world according to wavelets

384
00:24:02,630 --> 00:24:05,840
friend barbara per hour

385
00:24:12,360 --> 00:24:14,270
so the world

386
00:24:14,320 --> 00:24:17,620
a world that according to

387
00:24:21,320 --> 00:24:24,820
she is a hyphenated last name or

388
00:24:24,880 --> 00:24:27,190
one hundred

389
00:24:27,200 --> 00:24:29,000
and published by

390
00:24:29,020 --> 00:24:31,550
a local company

391
00:24:31,560 --> 00:24:36,510
he became few

392
00:24:36,850 --> 00:24:39,610
OK here's side

393
00:24:41,130 --> 00:24:42,710
very nice

394
00:24:42,750 --> 00:24:46,620
people and this book one prices for

395
00:24:46,630 --> 00:24:51,940
so popular exposition so it's quite fun to give the story more than i could

396
00:24:51,940 --> 00:24:52,710
do it

397
00:24:53,300 --> 00:24:54,880
a short time

398
00:24:55,810 --> 00:24:58,520
now we jump to

399
00:24:58,570 --> 00:25:01,500
what we was

400
00:25:01,510 --> 00:25:06,380
four on

401
00:25:07,020 --> 00:25:09,640
scaling its box

402
00:25:12,700 --> 00:25:14,550
well actually

403
00:25:15,380 --> 00:25:18,020
yes it we know this way quite

404
00:25:18,070 --> 00:25:21,630
is the box

405
00:25:21,630 --> 00:25:23,920
now i'm going to take

406
00:25:23,960 --> 00:25:25,610
i mean hard

407
00:25:25,620 --> 00:25:29,670
context set two coefficients

408
00:25:29,690 --> 00:25:30,980
plus and minus

409
00:25:31,060 --> 00:25:34,250
the low and high pass filter right plus and minus

410
00:25:36,700 --> 00:25:39,620
the a combination of boxes and we get this

411
00:25:39,630 --> 00:25:42,870
candidates from zero to one

412
00:25:42,880 --> 00:25:48,080
it's a box after box meinshausen

413
00:25:48,130 --> 00:25:51,650
that are weight

414
00:25:51,670 --> 00:25:52,560
so you see

415
00:26:01,330 --> 00:26:03,270
there you know

416
00:26:05,380 --> 00:26:07,320
where the scaling functions

417
00:26:07,330 --> 00:26:10,580
making up averages

418
00:26:11,450 --> 00:26:14,020
this is key e

419
00:26:15,170 --> 00:26:17,980
for this lecture

420
00:26:18,010 --> 00:26:21,640
the key example

421
00:26:21,690 --> 00:26:23,360
and in

422
00:26:23,370 --> 00:26:26,770
several ways to be jump into

423
00:26:26,930 --> 00:26:28,800
the way is

424
00:26:29,700 --> 00:26:33,190
the spaces w what i call them

425
00:26:33,190 --> 00:26:38,820
the lecture outline called w

426
00:26:38,870 --> 00:26:41,270
one is w

427
00:26:43,330 --> 00:26:48,490
fair enough it's all combinations

428
00:26:51,120 --> 00:26:54,520
so this what remember the day

429
00:26:54,560 --> 00:26:58,560
it was all combinations

430
00:27:06,900 --> 00:27:08,830
the kinds of functions that

431
00:27:08,890 --> 00:27:10,300
at scale

432
00:27:10,320 --> 00:27:11,920
this is the same

433
00:27:14,400 --> 00:27:20,410
that scale

434
00:27:20,470 --> 00:27:22,770
so that we use

435
00:27:22,780 --> 00:27:24,510
and this is

436
00:27:25,720 --> 00:27:27,100
now let me

437
00:27:29,210 --> 00:27:33,560
with this example

438
00:27:33,940 --> 00:27:37,450
put box function in here too

439
00:27:37,520 --> 00:27:39,900
next to it so this is the key

440
00:27:39,910 --> 00:27:42,710
this w

441
00:27:42,720 --> 00:27:46,850
also from zero one

442
00:27:46,860 --> 00:27:50,270
and here's the first one and

443
00:27:50,340 --> 00:27:51,920
the equation

444
00:27:53,720 --> 00:27:55,420
thirty four

445
00:27:55,480 --> 00:27:59,340
you wanted to check from i

446
00:27:59,400 --> 00:28:02,290
all the functions in the zero

447
00:28:02,300 --> 00:28:03,840
at all the functions

448
00:28:03,940 --> 00:28:10,430
two any function w zero one like it

449
00:28:12,890 --> 00:28:15,070
the answer is going to be

450
00:28:15,120 --> 00:28:17,620
p one

451
00:28:18,000 --> 00:28:21,060
functions these are piecewise constant

452
00:28:21,070 --> 00:28:22,790
on intervals

453
00:28:22,830 --> 00:28:25,940
these are all you know the rules

454
00:28:25,980 --> 00:28:27,840
so let's just wrong

455
00:28:27,860 --> 00:28:31,370
so for example

456
00:28:31,820 --> 00:28:34,390
got out

457
00:28:35,930 --> 00:28:38,690
what's what's w zero

458
00:28:40,560 --> 00:28:42,720
state between zero and two

459
00:28:42,740 --> 00:28:46,890
i was in w zero

460
00:28:53,050 --> 00:28:59,890
multiple nominations for this guy on this

461
00:28:59,900 --> 00:29:01,810
so there's a typical

462
00:29:01,820 --> 00:29:03,950
ten i'm just trying one typical

463
00:29:03,970 --> 00:29:06,180
i can be zero

464
00:29:06,300 --> 00:29:09,060
to o thing w zero

465
00:29:09,120 --> 00:29:13,460
the combination of

466
00:29:14,470 --> 00:29:19,030
and it she

467
00:29:19,070 --> 00:29:25,120
with certain rights rights to two-thirds ideas one point six one

468
00:29:25,210 --> 00:29:26,900
combine those

469
00:29:26,910 --> 00:29:30,710
put i was signed yellow here

470
00:29:32,960 --> 00:29:38,850
i wonder what kind of functions like

471
00:29:38,960 --> 00:29:41,550
kind of function that's

472
00:29:57,540 --> 00:30:01,220
because this is piecewise constant union general

473
00:30:01,230 --> 00:30:03,440
automatically on fills

474
00:30:03,450 --> 00:30:06,700
this is certainly piecewise constant also the

475
00:30:06,810 --> 00:30:08,940
results in v one

476
00:30:08,940 --> 00:30:10,990
but now the point is

477
00:30:11,050 --> 00:30:14,290
i think everything you want

478
00:30:14,320 --> 00:30:16,240
from these two

479
00:30:16,300 --> 00:30:19,440
everything one can be split

480
00:30:19,550 --> 00:30:24,600
see here's the decomposition here's the wavelet decomposition

481
00:30:24,610 --> 00:30:27,600
you know what fourier decomposition is useful

482
00:30:29,580 --> 00:30:32,280
wavelet decomposition is used but the thing

483
00:30:32,290 --> 00:30:34,990
multiple scales

484
00:30:38,560 --> 00:30:44,290
believe that i could take any function he won why don't have any function i

485
00:30:44,290 --> 00:30:47,920
don't have all the one right here

486
00:30:50,220 --> 00:30:56,210
the only finds and got some functions in the one here but they're all plus-minus

487
00:30:56,260 --> 00:30:58,910
plus-minus plus-minus

488
00:30:59,830 --> 00:31:02,270
what process

489
00:31:03,850 --> 00:31:05,030
but if i

490
00:31:05,160 --> 00:31:07,860
next the two

491
00:31:07,920 --> 00:31:10,120
all i'm saying is that

492
00:31:10,140 --> 00:31:14,090
one by combining one one

493
00:31:14,100 --> 00:31:16,160
with one minus one

494
00:31:16,200 --> 00:31:18,600
with the right weights

495
00:31:20,060 --> 00:31:22,310
any the a b

496
00:31:23,260 --> 00:31:25,870
that's the on

497
00:31:26,960 --> 00:31:30,580
if i'm sure if something goes up

498
00:31:30,790 --> 00:31:32,390
goes to p

499
00:31:32,530 --> 00:31:34,080
it's a a

500
00:31:38,660 --> 00:31:42,240
not find one might find this guy

501
00:31:42,290 --> 00:31:44,850
but some coefficients of this

502
00:31:45,840 --> 00:31:47,140
what produces

503
00:31:47,140 --> 00:31:48,960
and then you have to learn the

504
00:31:48,970 --> 00:31:53,170
parameters from this training examples that you have available

505
00:31:53,170 --> 00:31:56,310
but again if you don't have the labels that's even

506
00:31:56,370 --> 00:31:58,240
more time to test

507
00:31:58,240 --> 00:32:00,510
and in all parameter techniques

508
00:32:00,580 --> 00:32:04,200
as you know you don't assume any

509
00:32:04,210 --> 00:32:05,780
knowledge of parameters

510
00:32:05,810 --> 00:32:09,860
so use and then use this parameter techniques to estimate the density of the distribution

511
00:32:09,860 --> 00:32:13,030
and some of the well known technically the histograms parzen

512
00:32:13,050 --> 00:32:14,460
in destinations

513
00:32:14,470 --> 00:32:16,500
can be used to estimate

514
00:32:23,030 --> 00:32:25,530
OK i just presented to you

515
00:32:25,590 --> 00:32:28,920
approaches that belong to statistical based techniques

516
00:32:28,920 --> 00:32:31,000
one of them is smartsifter

517
00:32:31,060 --> 00:32:33,510
they can diminish

518
00:32:33,560 --> 00:32:37,570
so this is proposed for the mixture of continuous

519
00:32:37,620 --> 00:32:40,690
and discrete attributes

520
00:32:42,050 --> 00:32:44,450
in this case histograms

521
00:32:44,490 --> 00:32:47,830
i used to represent a probability density for categorical attributes

522
00:32:47,890 --> 00:32:50,190
while find it to make sure

523
00:32:51,140 --> 00:32:55,770
i used to represent probability density for continuous attributes so basically what for both these

524
00:32:55,770 --> 00:32:58,700
types of activities you estimating the that's the first

525
00:32:58,710 --> 00:33:01,870
and then for every test instance this smartsifter

526
00:33:01,900 --> 00:33:03,450
estimating the probability

527
00:33:03,460 --> 00:33:07,230
of the test instance to be generated by the learnt statistical models

528
00:33:07,310 --> 00:33:10,230
this is usually defined by p c minus one

529
00:33:10,230 --> 00:33:13,010
for the first time and minus one

530
00:33:13,040 --> 00:33:16,830
then this this this this is added to the sample

531
00:33:16,840 --> 00:33:20,100
and then the model is re re estimated so right now the probability of the

532
00:33:20,100 --> 00:33:23,060
test instance to be generated from the new model is estimated

533
00:33:23,180 --> 00:33:24,220
as BP

534
00:33:24,260 --> 00:33:27,370
and i trying to measure the difference between this model

535
00:33:27,390 --> 00:33:33,020
and anomaly score for this test basically the difference between this one so basically consort

536
00:33:33,040 --> 00:33:35,480
all the new data records according

537
00:33:35,520 --> 00:33:37,180
this difference

538
00:33:37,220 --> 00:33:39,420
in order take the top as

539
00:33:39,430 --> 00:33:40,510
of course

540
00:33:40,560 --> 00:33:42,970
or anomalies

541
00:33:42,970 --> 00:33:49,250
very similar idea is from a scan from from columbia university

542
00:33:49,270 --> 00:33:55,700
so basically your constructing the new distribution you exactly the data distribution from data because

543
00:33:55,700 --> 00:34:00,430
the data can so we have a m majority distribution in this case this correspond

544
00:34:00,430 --> 00:34:02,190
to model behavior

545
00:34:02,200 --> 00:34:06,320
and a is an almost distributions so constructing the new distribution d

546
00:34:06,350 --> 00:34:12,720
which is one minus lambda times lambda a lambda is some users specify parameters

547
00:34:17,250 --> 00:34:18,700
in the first step

548
00:34:18,700 --> 00:34:23,160
you assign all the data across two majority distribution

549
00:34:23,180 --> 00:34:25,760
and then a is initially empty

550
00:34:25,820 --> 00:34:27,750
then for every data records

551
00:34:27,790 --> 00:34:29,610
from basically

552
00:34:29,720 --> 00:34:35,250
first estimate the parameters for and a of course is empty you you need to

553
00:34:35,250 --> 00:34:37,140
estimate anything at the beginning

554
00:34:37,200 --> 00:34:41,370
and then you compute the log likelihood l of distribution d

555
00:34:41,400 --> 00:34:42,310
right now

556
00:34:42,410 --> 00:34:45,470
you removed data record from initial set

557
00:34:45,480 --> 00:34:47,640
and put it back into a

558
00:34:47,650 --> 00:34:51,390
right now you are trying to re estimate these parameters

559
00:34:51,440 --> 00:34:56,010
and your computing log likelihood l prime of the same distribution

560
00:34:56,030 --> 00:35:01,220
and very similar like in the previous approach computing the difference primus out

561
00:35:01,230 --> 00:35:04,740
and if do this is larger than some kind of threshold

562
00:35:04,770 --> 00:35:07,840
then they to recognise correspond to anomaly otherwise

563
00:35:07,900 --> 00:35:11,740
they record is normal and is moved back to m and then you're repeating this

564
00:35:11,740 --> 00:35:14,030
for every

565
00:35:14,090 --> 00:35:17,620
the data from the dataset and of course

566
00:35:18,180 --> 00:35:20,540
this may be time consuming

567
00:35:20,600 --> 00:35:22,450
this may be

568
00:35:22,460 --> 00:35:25,970
the may be problem because

569
00:35:25,980 --> 00:35:27,250
extremely large

570
00:35:28,970 --> 00:35:35,060
if you move on a track that may not leaving if initial basically is sufficient

571
00:35:35,090 --> 00:35:38,200
the difference in the in log likelihood so

572
00:35:38,200 --> 00:35:41,540
this may not work some the some

573
00:35:41,570 --> 00:35:43,500
real life

574
00:35:43,500 --> 00:35:45,140
kind of metal

575
00:35:47,790 --> 00:35:53,310
there are also some other techniques that combine different

576
00:35:53,320 --> 00:35:57,190
approaches from different communities like information theory based

577
00:35:57,200 --> 00:36:01,870
spectral composition people use visualization techniques

578
00:36:03,600 --> 00:36:09,540
the first talk about the information theory based techniques and

579
00:36:09,550 --> 00:36:11,300
the main idea here is that

580
00:36:11,310 --> 00:36:13,500
anomalies are outliers

581
00:36:13,500 --> 00:36:19,760
typically significantly change the information content from the data

582
00:36:19,790 --> 00:36:24,080
and the general approach here is to detect data records

583
00:36:24,840 --> 00:36:32,290
change this kind of the in information and usually require certain information theoretic

584
00:36:32,300 --> 00:36:33,860
and i think

585
00:36:33,870 --> 00:36:36,730
it measures

586
00:36:36,730 --> 00:36:41,900
obvious advantage is that they can operate in an unsupervised

587
00:36:41,910 --> 00:36:44,270
what they do not require any labels

588
00:36:45,990 --> 00:36:50,260
some of the drawbacks included these techniques require

589
00:36:50,280 --> 00:36:54,720
information theoretic measures the sensitive

590
00:36:54,740 --> 00:36:57,780
to detect irregularity induced by very few anomalies

591
00:36:58,600 --> 00:37:02,560
sometimes this may lead to very poor performance

592
00:37:02,610 --> 00:37:08,100
the simplest in the simplest possible way is to try to use the entropy

593
00:37:08,170 --> 00:37:09,400
try to find

594
00:37:09,410 --> 00:37:11,700
OK size

595
00:37:11,730 --> 00:37:13,660
data subset

596
00:37:13,700 --> 00:37:18,680
and we trying to find such subset whose removal musical leads to to the maximal

597
00:37:18,680 --> 00:37:22,120
decrease in entropy of the data sets so

598
00:37:22,160 --> 00:37:23,820
if you remove certain

599
00:37:23,850 --> 00:37:26,220
data subset and this

600
00:37:26,230 --> 00:37:27,960
delete the maximal

601
00:37:29,270 --> 00:37:35,240
maximal decrease that means this points total because they correspond to the next phase can

602
00:37:35,320 --> 00:37:38,790
apply this incremental fashion one by one data record could

603
00:37:38,800 --> 00:37:40,220
or you can do by

604
00:37:40,220 --> 00:37:44,420
regional or actually subset because of the baby subset

605
00:37:44,440 --> 00:37:48,000
when people use different techniques like linear sort of data

606
00:37:48,010 --> 00:37:50,340
to search for this

607
00:37:52,010 --> 00:37:54,690
substance it does the lexical

608
00:37:54,730 --> 00:37:56,220
in entropy

609
00:37:57,830 --> 00:38:03,120
some researchers use other information theoretic measures like conditional entropy

610
00:38:03,160 --> 00:38:05,840
relative conditional entropy information gain

611
00:38:05,890 --> 00:38:09,440
relative entropy like all book library divergence and so on

612
00:38:09,510 --> 00:38:16,900
there are also some special techniques

613
00:38:16,900 --> 00:38:21,860
and this analysis is based on i can the composition of the data so you

614
00:38:21,860 --> 00:38:25,830
are familiar with the concept of i can converted composition and principal component analysis

615
00:38:25,850 --> 00:38:30,200
so the key idea is to find a combination of activities that captures the bulk

616
00:38:30,200 --> 00:38:34,660
affordability so basically trying to become principal component analysis

617
00:38:34,700 --> 00:38:37,200
and this reduced set of attributes

618
00:38:37,250 --> 00:38:41,170
chemical if this is the set of factors can explain the normal data well

619
00:38:41,280 --> 00:38:46,520
but sometimes they cannot represent anomalies also so

620
00:38:47,450 --> 00:38:50,490
advantage that again they can

621
00:38:50,500 --> 00:38:55,030
perform very well in a surprise more but

622
00:38:55,050 --> 00:39:00,350
they are based on the assumption that anomalies basically

623
00:39:00,370 --> 00:39:01,530
these techniques

624
00:39:01,540 --> 00:39:06,390
try to capture the majority of the data so basically they they try to retained

625
00:39:06,600 --> 00:39:07,970
most of the variance

626
00:39:08,010 --> 00:39:10,300
so the assumption that basically

627
00:39:10,470 --> 00:39:15,810
that in this new space you're assuming that anomalies in data records also very well

628
00:39:15,810 --> 00:39:19,850
distinction which may not be the case so that's why these techniques may not be

629
00:39:19,850 --> 00:39:23,250
able to detect some numbers

630
00:39:25,150 --> 00:39:28,650
this approach exactly this idea so basically

631
00:39:28,650 --> 00:39:32,010
if you have two variables

632
00:39:32,020 --> 00:39:36,140
remember that the ideas we sample x one given x two

633
00:39:36,180 --> 00:39:39,770
and then we moved to x two given x one

634
00:39:39,810 --> 00:39:42,030
and then we moved to x one

635
00:39:42,090 --> 00:39:43,770
given x two

636
00:39:43,780 --> 00:39:49,830
and so on that there was big ips

637
00:39:50,570 --> 00:39:52,060
so in other words

638
00:39:52,070 --> 00:39:59,290
given a particular value of x one

639
00:39:59,300 --> 00:40:04,630
you look at the cut

640
00:40:04,640 --> 00:40:08,510
and you sample x two from it

641
00:40:09,430 --> 00:40:10,870
now next

642
00:40:10,910 --> 00:40:13,480
you look at the next cut

643
00:40:13,530 --> 00:40:16,320
bring some other column

644
00:40:16,330 --> 00:40:19,760
we look at the next cut

645
00:40:19,800 --> 00:40:23,660
well you sample

646
00:40:23,700 --> 00:40:25,060
and new sample

647
00:40:25,070 --> 00:40:27,750
then you

648
00:40:27,790 --> 00:40:30,630
sample the new value of x one

649
00:40:30,670 --> 00:40:32,270
and you keep doing this

650
00:40:32,280 --> 00:40:35,150
the problem with doing this this way

651
00:40:35,160 --> 00:40:38,250
is that if you're doing this cuts and if this thing is very big

652
00:40:38,270 --> 00:40:40,030
we can take e for effort

653
00:40:40,040 --> 00:40:42,700
to get to areas of high profile

654
00:40:42,750 --> 00:40:47,110
now if something is very elongated and you're doing this cat if something is found

655
00:40:47,230 --> 00:40:49,410
you just think this little

656
00:40:49,450 --> 00:40:51,510
increases very little steps

657
00:40:51,520 --> 00:40:55,380
so we want ways of doing this more efficient

658
00:40:55,390 --> 00:40:57,710
the first ideas call blocking

659
00:40:57,730 --> 00:41:00,860
the idea of blocking is you group variables

660
00:41:00,940 --> 00:41:03,700
that are correlated and you sample them together

661
00:41:03,710 --> 00:41:07,860
if you can draw come up with an exact sample of a group of variables

662
00:41:07,990 --> 00:41:10,890
that's very useful because instead of

663
00:41:10,900 --> 00:41:14,740
because if you can sample from directly from the group that instead of sample by

664
00:41:14,740 --> 00:41:16,890
doing stupid things

665
00:41:16,910 --> 00:41:22,210
what you do is you just examples from one thousand today

666
00:41:22,290 --> 00:41:25,580
and that gives you good samples much more efficient

667
00:41:25,640 --> 00:41:29,630
here are some examples i could block

668
00:41:32,750 --> 00:41:35,300
but you have a graphical model

669
00:41:35,440 --> 00:41:38,860
happens to have a square lattice a random field

670
00:41:38,940 --> 00:41:41,240
then you can sort of break

671
00:41:41,260 --> 00:41:44,590
the lattice into two groups

672
00:41:45,730 --> 00:41:50,530
now each group then if this is discrete is just an HMM

673
00:41:50,530 --> 00:41:53,450
if the black guys are observed

674
00:41:53,460 --> 00:41:55,310
and the white guys

675
00:41:55,370 --> 00:41:57,860
inferring the white guys from the black eyes

676
00:41:57,890 --> 00:42:03,400
is an hmm computation if an exact computation there's no approximation there

677
00:42:03,530 --> 00:42:07,150
for example the black ice you computer white guys

678
00:42:09,180 --> 00:42:12,000
now to sample the black guys

679
00:42:12,020 --> 00:42:13,980
because they happen to be in the chain

680
00:42:13,990 --> 00:42:16,780
you can do this forward backward algorithm

681
00:42:16,790 --> 00:42:18,360
we had before

682
00:42:18,370 --> 00:42:23,030
so we draw an exact sample from the distribution the forward backward takes into consideration

683
00:42:23,030 --> 00:42:25,860
the correlations

684
00:42:25,930 --> 00:42:28,430
so we deal with the problems of correlation

685
00:42:28,530 --> 00:42:31,990
in practice what you do is you sample this given this an the example this

686
00:42:32,000 --> 00:42:34,200
given this so you only have two samples

687
00:42:34,230 --> 00:42:38,230
they use a lot of analytical computational and to live with this and that makes

688
00:42:38,230 --> 00:42:40,040
it easy

689
00:42:40,090 --> 00:42:45,640
and then you can do this for other types of graphs factor graph based medicines

690
00:42:45,650 --> 00:42:49,340
when you compute estimators of this

691
00:42:49,360 --> 00:42:50,550
you try

692
00:42:50,570 --> 00:42:51,940
again this is

693
00:42:51,960 --> 00:42:57,240
that actually by is not different from this technique of

694
00:42:57,250 --> 00:43:02,140
because of the theorem of brown black it's called rough localisation it's also known as

695
00:43:03,380 --> 00:43:06,070
and it is when you use an estimator of this

696
00:43:06,120 --> 00:43:12,320
instead of using samples he use these analytical expressions for the expectation is the conditional

697
00:43:18,140 --> 00:43:22,630
yeah we haven't seen the emulator then if anyone's going to describe the

698
00:43:22,670 --> 00:43:27,590
but sometimes you can't exactly and you want to call it to the

699
00:43:27,610 --> 00:43:32,770
there's also a parking and an example of the sort of thing

700
00:43:32,790 --> 00:43:37,530
we have two views of the house he one reconstructed three d model

701
00:43:37,530 --> 00:43:42,740
this other ways of doing this with ransack on that actually work

702
00:43:42,750 --> 00:43:45,420
from one

703
00:43:45,420 --> 00:43:48,890
on balance it's no we're not quite sure in this case case you think this

704
00:43:48,890 --> 00:43:51,870
probabilities ought to be quite useful to see

705
00:43:51,880 --> 00:43:55,810
more precisely how these probabilities can help us

706
00:43:58,620 --> 00:44:01,500
the first thing want to be able to use the probabilities in order to make

707
00:44:01,500 --> 00:44:05,390
decisions in the simplest kind of decision is to minimize

708
00:44:05,400 --> 00:44:10,310
the rate at which we misclassified so we've got to some of the population of

709
00:44:10,310 --> 00:44:14,690
people not to make as few errors as possible in a classification

710
00:44:14,700 --> 00:44:18,960
so to simplify this instead of having a whole image vector this just have a

711
00:44:18,960 --> 00:44:23,080
single variable x is the quantity we measure for each individual

712
00:44:23,100 --> 00:44:26,380
and what's plotted here of the

713
00:44:26,400 --> 00:44:29,040
probability distributions for

714
00:44:29,050 --> 00:44:33,230
the two classes so this is for class one which also is the normal class

715
00:44:33,230 --> 00:44:36,070
and this is class two which is the conscious class

716
00:44:36,080 --> 00:44:39,830
and the problem becomes of course because these distributions overlap

717
00:44:39,880 --> 00:44:43,250
so some of the some of the times so we observe a value x here

718
00:44:43,250 --> 00:44:47,170
this overlap region belongs to class one sometimes belong to class two that's why this

719
00:44:47,170 --> 00:44:49,130
is a hard problem

720
00:44:50,480 --> 00:44:54,400
let's suppose that we introduce what's called the decision boundary so will choose the value

721
00:44:54,400 --> 00:44:58,700
of x hat and everything to the left of that x how will classify as

722
00:44:58,700 --> 00:45:02,000
we call the region one that's the region of normality

723
00:45:02,050 --> 00:45:05,880
and everything to the right of x will be region two will be

724
00:45:05,940 --> 00:45:08,440
classified as cancer

725
00:45:08,500 --> 00:45:14,880
so we want to reduce the rate of mistakes the probability of mistakes and mistake

726
00:45:14,880 --> 00:45:16,900
occurs whenever

727
00:45:17,730 --> 00:45:20,000
x really belongs to class two

728
00:45:20,000 --> 00:45:23,400
but it lands in the region classified as class one so some who really has

729
00:45:23,400 --> 00:45:27,960
cancer we classify them as normal all the way around they really normal we classify

730
00:45:27,980 --> 00:45:29,440
them cancer

731
00:45:29,450 --> 00:45:33,250
what is this probability

732
00:45:33,250 --> 00:45:39,110
well it's just the the interval of the density over that region so for for

733
00:45:39,110 --> 00:45:43,200
vectors which really belong to the cancer class which are classified as normal

734
00:45:43,250 --> 00:45:49,130
the the fraction of images which which have that property is is obtained by integrating

735
00:45:49,130 --> 00:45:53,670
the density of the region ones but it's been to over here and so this

736
00:45:53,670 --> 00:45:57,720
is the density the interesting p of x and c two and so it corresponds

737
00:45:58,900 --> 00:46:01,170
this region here so the sum

738
00:46:01,230 --> 00:46:03,830
of the green and the red regions these are

739
00:46:04,880 --> 00:46:06,370
these are

740
00:46:06,380 --> 00:46:11,500
vectors x which really attentions were classified as normal the red the relations and it's

741
00:46:11,500 --> 00:46:15,270
only images which are normally classified as cancerous

742
00:46:15,330 --> 00:46:21,880
that's the tale of this distribution that's the blue region

743
00:46:22,000 --> 00:46:25,010
now we are free to choose the decision boundary anywhere we like so we can

744
00:46:25,010 --> 00:46:29,100
move x had happened and this this set this axis causing we also have multiple

745
00:46:29,100 --> 00:46:34,390
decision regions but i'm just assuming there's just a single decision boundary the two regions

746
00:46:34,400 --> 00:46:38,790
well should we put well we can see pictorially where to put it because if

747
00:46:38,790 --> 00:46:40,850
we slide this up and down

748
00:46:40,860 --> 00:46:44,890
if we move this let's say this region to the left some of the green

749
00:46:44,890 --> 00:46:47,240
area will become blue

750
00:46:47,250 --> 00:46:51,890
OK so it's still misclassified the red area sort of disappears

751
00:46:54,000 --> 00:46:58,400
this region here was sort of stuck with that sort of irreducible minimum rate of

752
00:46:58,400 --> 00:47:02,350
misclassification can get around that we can get rid of the red region and i

753
00:47:02,350 --> 00:47:05,420
think can see pictorially the place to put it out is what i've called it's

754
00:47:06,350 --> 00:47:10,880
that's the place at which the the total area this college is as small as

755
00:47:10,880 --> 00:47:14,600
possible and if we move even further to the left we'll pick up another red

756
00:47:17,910 --> 00:47:20,250
this is the point at which these curves

757
00:47:20,250 --> 00:47:22,220
cross each other

758
00:47:22,230 --> 00:47:28,540
so the decision criterion can be expressed as follows will classify x as

759
00:47:28,550 --> 00:47:31,700
a region one as normal if if this

760
00:47:31,710 --> 00:47:36,280
joint distribution of x and c one is greater price recalled p of x

761
00:47:36,290 --> 00:47:37,630
and c two

762
00:47:37,650 --> 00:47:41,300
and if i divide both sides by p of x that says we assign x

763
00:47:41,300 --> 00:47:47,410
the region of the high posterior probability which is sort of obvious

764
00:47:47,420 --> 00:47:49,000
very happy with that

765
00:47:49,100 --> 00:47:53,880
so he said it's obvious nobody is considered don't understand i hope that's clear me

766
00:47:53,910 --> 00:47:54,820
all right

767
00:47:54,830 --> 00:47:58,600
so so why bother to do this so we first saw computer probability then we

768
00:47:58,600 --> 00:48:01,590
made the decision why go to all this trouble they give you four

769
00:48:01,610 --> 00:48:04,380
advantage is that it brings

770
00:48:04,390 --> 00:48:09,570
it allows us to do something called minimising risk in interfaces some called loss matrix

771
00:48:09,570 --> 00:48:13,200
this is important because the loss matrix itself could change over time you don't want

772
00:48:13,200 --> 00:48:17,590
to re compute probabilities and explain explain all these four detail it

773
00:48:17,600 --> 00:48:22,270
it allows us to do something called reject option allows us to handle unbalanced class

774
00:48:22,270 --> 00:48:26,540
priors that allows us to combine models together let's go through each of those

775
00:48:26,550 --> 00:48:29,750
in turn just have a look

776
00:48:29,760 --> 00:48:35,290
so loss matrix affects the fact that some kind of misclassifications are more serious than

777
00:48:35,290 --> 00:48:40,990
others so let's suppose that this is the true unknown class was a person standing

778
00:48:40,990 --> 00:48:43,750
in front of us that have cancer or they don't we don't know but this

779
00:48:43,750 --> 00:48:46,960
is the true unknown classes the decision we make

780
00:48:46,970 --> 00:48:51,250
now this loss matrix with assumed that if we make a correct decision if classify

781
00:48:51,250 --> 00:48:56,620
cancerous cancelled classifier normal normal there's no cost associated with that

782
00:48:56,750 --> 00:49:02,220
if we take somebody who's normally misclassifying this cancer that means two things that means

783
00:49:02,220 --> 00:49:05,810
first of all the person gets a bit worried because they've been recalled after test

784
00:49:05,810 --> 00:49:08,840
that we pretty stressful unpleasant experience i'm sure

785
00:49:08,850 --> 00:49:11,120
and also we then have to spend money

786
00:49:11,130 --> 00:49:12,980
giving them more tests

787
00:49:12,980 --> 00:49:16,710
so since this tangent space

788
00:49:16,740 --> 00:49:18,710
this linear space

789
00:49:18,710 --> 00:49:23,880
you can think naturally of tangent vectors

790
00:49:23,900 --> 00:49:29,320
and one point i want to make

791
00:49:29,340 --> 00:49:33,150
is that somehow there is

792
00:49:35,460 --> 00:49:39,880
the connection between tangent vectors and curves

793
00:49:39,900 --> 00:49:43,960
so if you think of as curve so what occurred

794
00:49:43,980 --> 00:49:49,010
fear of t is the curvature map from our two mk

795
00:49:49,030 --> 00:49:52,820
OK so that traces out the curve on the manifold

796
00:49:52,840 --> 00:49:55,130
and should take

797
00:49:55,170 --> 00:49:59,150
the derivative of this with respect

798
00:50:00,590 --> 00:50:02,070
you get a vector

799
00:50:02,090 --> 00:50:03,670
and that's essentially

800
00:50:03,690 --> 00:50:05,010
the tangent vector

801
00:50:05,010 --> 00:50:08,780
so every tangent and vector can be identified with the curve

802
00:50:08,800 --> 00:50:13,610
the tangent vectors in tp where p is the point on the manifold so every

803
00:50:13,610 --> 00:50:18,010
tangent vector can be identified with the curve that passes through

804
00:50:18,800 --> 00:50:20,900
oppose derivative

805
00:50:23,210 --> 00:50:30,980
the vector

806
00:50:30,980 --> 00:50:35,340
tangent vectors can also be thought of as derivative in the following way

807
00:50:35,360 --> 00:50:39,570
if you take a function f from mk two are

808
00:50:39,590 --> 00:50:41,480
and you pick up vector

809
00:50:41,500 --> 00:50:47,030
in the tangent space

810
00:50:47,050 --> 00:50:49,900
since we already discussed how

811
00:50:49,920 --> 00:50:52,380
vectors are identified with cool

812
00:50:52,420 --> 00:50:56,730
so the vector v can be identified with the fear of t

813
00:50:56,820 --> 00:50:59,590
from our two mk

814
00:50:59,610 --> 00:51:02,710
and then this object makes sense by composing the two

815
00:51:02,740 --> 00:51:05,440
f of fear of t

816
00:51:05,460 --> 00:51:08,960
and that's just a regular function from r to r

817
00:51:09,010 --> 00:51:10,800
and so have a function

818
00:51:10,800 --> 00:51:13,570
from m two are

819
00:51:13,590 --> 00:51:15,300
you pick up point p

820
00:51:15,300 --> 00:51:16,740
on the manifold

821
00:51:16,760 --> 00:51:19,670
you consider the tangent space at p is p

822
00:51:19,690 --> 00:51:22,550
you pick up vector v in the standards based

823
00:51:22,650 --> 00:51:26,030
that the vector v can be identified with the color

824
00:51:26,090 --> 00:51:27,400
on the manifold

825
00:51:27,420 --> 00:51:29,380
that passes through p

826
00:51:29,400 --> 00:51:31,280
and if you take

827
00:51:32,380 --> 00:51:33,800
of of

828
00:51:33,960 --> 00:51:36,380
just get a regular function

829
00:51:36,440 --> 00:51:38,050
and if you differentiate it

830
00:51:38,070 --> 00:51:39,570
with respect to t

831
00:51:39,570 --> 00:51:43,070
which we know what to do from general calculus

832
00:51:43,090 --> 00:51:44,500
that's the same thing

833
00:51:44,510 --> 00:51:48,380
as basically taking the directional derivative of f

834
00:51:48,400 --> 00:51:51,980
in the direction v

835
00:51:54,360 --> 00:51:57,240
you can think therefore of tangent vectors

836
00:51:57,260 --> 00:52:01,920
as operators that act on functions and take

837
00:52:02,320 --> 00:52:11,240
directional derivatives in certain directions

838
00:52:11,260 --> 00:52:14,730
the tangent vectors and five it curves on the one hand and the directional derivatives

839
00:52:14,730 --> 00:52:20,030
on the other hand in this way

840
00:52:20,050 --> 00:52:23,630
the thing that gives it its riemannian structure

841
00:52:25,590 --> 00:52:27,230
will be

842
00:52:29,420 --> 00:52:31,980
norms and angles in the tangent space

843
00:52:32,000 --> 00:52:35,570
so that you can talk about inner product between any two vectors

844
00:52:35,590 --> 00:52:37,090
v and w

845
00:52:37,090 --> 00:52:38,550
in the tangent space

846
00:52:38,570 --> 00:52:42,670
since everything here is embedded we don't have to consider it abstractly right now they

847
00:52:42,710 --> 00:52:46,030
can do this abstractly but you see the tangent spaces is just an embedded linear

848
00:52:46,030 --> 00:52:52,820
subspace are just naturally inherits the inner product structure from the space in which it

849
00:52:52,820 --> 00:52:54,070
is embedded

850
00:52:54,090 --> 00:52:57,550
so this is v and w is exactly what you would think it to be

851
00:52:57,550 --> 00:52:59,230
from this picture

852
00:53:01,380 --> 00:53:02,820
but that's what gives

853
00:53:03,480 --> 00:53:09,400
the money in structure

854
00:53:09,420 --> 00:53:11,030
so now that have this

855
00:53:11,050 --> 00:53:14,360
you can talk about length of curves

856
00:53:14,380 --> 00:53:17,210
and you can talk about geodesics

857
00:53:17,230 --> 00:53:20,940
so what is the length of a curve c begin that what has occurred

858
00:53:20,960 --> 00:53:22,800
a curve is a map from

859
00:53:22,820 --> 00:53:25,440
zero one to m say

860
00:53:25,440 --> 00:53:27,320
and if you take its derivative

861
00:53:27,340 --> 00:53:29,300
you actually get

862
00:53:29,320 --> 00:53:33,230
right defeated by dt is actually a vector

863
00:53:33,260 --> 00:53:38,030
and if the norm of the vector that makes sense because all the vectors

864
00:53:38,050 --> 00:53:42,840
living in some space where you have norms and inner products defined

865
00:53:42,840 --> 00:53:44,240
and if you integrate that

866
00:53:44,260 --> 00:53:47,670
along the length of the curve

867
00:53:47,690 --> 00:53:50,380
you get the length of that good

868
00:53:50,440 --> 00:53:53,550
so that's how you would calculate the length of any cut off

869
00:53:53,590 --> 00:53:58,280
and now he between two points you can set up variational problem to ask for

870
00:53:58,880 --> 00:54:00,820
shortest curve

871
00:54:00,820 --> 00:54:03,070
that goes from one point p

872
00:54:03,120 --> 00:54:05,780
the another point q

873
00:54:05,800 --> 00:54:07,440
and that more or less

874
00:54:07,440 --> 00:54:09,810
evidence combining matter

875
00:54:09,850 --> 00:54:13,210
that are being used in the asian network

876
00:54:13,230 --> 00:54:17,500
you can look at some at average weighted sum

877
00:54:17,560 --> 00:54:18,770
you can also

878
00:54:19,040 --> 00:54:21,500
in the publication to grow

879
00:54:21,520 --> 00:54:23,250
you see also that

880
00:54:25,980 --> 00:54:28,400
and or combination

881
00:54:28,420 --> 00:54:29,400
it can be

882
00:54:29,690 --> 00:54:34,520
for instance can be incorporated here in the computation

883
00:54:34,690 --> 00:54:36,750
of these norms

884
00:54:37,120 --> 00:54:40,810
so this model is known to have

885
00:54:40,830 --> 00:54:42,630
quite good results

886
00:54:42,710 --> 00:54:45,940
it is also incorporated

887
00:54:45,940 --> 00:54:50,330
indeed quite insistent which was longtime well

888
00:54:52,080 --> 00:54:53,920
well yes

889
00:54:53,980 --> 00:54:56,750
it's estimated that good retrieval models

890
00:54:56,770 --> 00:55:00,150
developed at the university of

891
00:55:00,210 --> 00:55:04,540
perhaps i forgot to see that for the language modelling approach

892
00:55:04,560 --> 00:55:07,270
you have two kids of leaving

893
00:55:07,270 --> 00:55:09,350
the most toolkits

894
00:55:09,360 --> 00:55:15,420
developed at CMU also in cooperation with the university of

895
00:55:15,420 --> 00:55:17,210
and this is

896
00:55:18,170 --> 00:55:22,500
the software open source of open source software

897
00:55:22,500 --> 00:55:23,920
where you can

898
00:55:25,630 --> 00:55:31,460
change the language models incorporate new language model

899
00:55:31,540 --> 00:55:33,500
four computations

900
00:55:34,880 --> 00:55:36,650
thank you

901
00:55:36,670 --> 00:55:38,270
i think i

902
00:55:38,290 --> 00:55:44,270
he is

903
00:55:44,350 --> 00:55:46,730
most of about

904
00:55:46,750 --> 00:55:48,790
if here network model

905
00:55:49,400 --> 00:55:51,310
you see in this model

906
00:55:51,330 --> 00:55:56,020
you can easily come combine these sources of evidence

907
00:55:56,020 --> 00:56:00,000
you can combine sources of evidence

908
00:56:00,040 --> 00:56:05,380
coming from as as actually also in the language might

909
00:56:07,630 --> 00:56:12,000
which is applied to the simple form of action and that my

910
00:56:12,110 --> 00:56:17,360
you can combine different sources of evidence of development

911
00:56:17,420 --> 00:56:22,440
so we have also study that you outline must instance

912
00:56:22,500 --> 00:56:26,690
where you combine you have

913
00:56:26,690 --> 00:56:27,730
you computer

914
00:56:27,750 --> 00:56:33,670
you compute action video back from BBC

915
00:56:33,670 --> 00:56:36,060
so you can combine evidence

916
00:56:36,100 --> 00:56:38,000
coming from

917
00:56:38,060 --> 00:56:40,980
metadata to be used

918
00:56:41,000 --> 00:56:44,210
the technical metadata metadata

919
00:56:44,210 --> 00:56:47,310
to alter the producer of the video

920
00:56:47,330 --> 00:56:49,630
also information

921
00:56:49,630 --> 00:56:51,310
the image

922
00:56:53,420 --> 00:56:57,810
forty manually annotated manually labeled

923
00:56:57,810 --> 00:57:00,520
but not only the text but also

924
00:57:00,520 --> 00:57:02,830
all the information about

925
00:57:02,830 --> 00:57:08,850
this multimedia sources can be easily integrated in such

926
00:57:08,940 --> 00:57:10,690
the problem is of course

927
00:57:10,730 --> 00:57:12,830
computational complexity

928
00:57:12,850 --> 00:57:17,630
you see that a lot of independence here in the network

929
00:57:17,690 --> 00:57:24,490
let's go back quite well some level you define then you could

930
00:57:24,560 --> 00:57:25,830
find level

931
00:57:25,830 --> 00:57:30,380
so that you can see that you take into account in model

932
00:57:30,400 --> 00:57:32,810
so you have a lot of computation

933
00:57:32,830 --> 00:57:37,270
also lots of numbers to keep track of

934
00:57:37,310 --> 00:57:40,480
of the probabilities of each note

935
00:57:40,630 --> 00:57:44,210
although alternative and croft

936
00:57:44,230 --> 00:57:46,580
give you a

937
00:57:46,600 --> 00:57:53,520
he convenient implementation based on link it at the node that are sourced at each

938
00:57:53,520 --> 00:58:00,000
node still this can be if you have a large document collection considering that

939
00:58:00,020 --> 00:58:02,600
you want to answer to your query

940
00:58:02,620 --> 00:58:10,040
instantly almost instantly within the second you want to know this is this is too

941
00:58:10,060 --> 00:58:12,620
well this is quite a bottleneck

942
00:58:12,630 --> 00:58:14,860
for such an approach

943
00:58:14,920 --> 00:58:16,290
but we have

944
00:58:16,350 --> 00:58:20,650
you believe that the technology failed currently

945
00:58:20,670 --> 00:58:22,460
so i think you know the model

946
00:58:22,830 --> 00:58:23,650
of four

947
00:58:24,810 --> 00:58:26,150
twenty years ago

948
00:58:26,170 --> 00:58:28,600
maybe not so long ago but still

949
00:58:28,650 --> 00:58:35,000
quite some time ago and we have a newer techniques for approximate the thing

950
00:58:35,040 --> 00:58:37,960
and maybe there we

951
00:58:37,980 --> 00:58:43,940
we could proceed again in information retrieval research in research

952
00:58:49,380 --> 00:58:51,520
can also be you

953
00:58:51,520 --> 00:58:54,220
in different ways

954
00:58:54,230 --> 00:58:57,220
so in this case

955
00:58:57,310 --> 00:59:00,540
we have a crystal field splitting energy

956
00:59:00,550 --> 00:59:02,650
that's less than what we call

957
00:59:02,660 --> 00:59:04,400
the pairing

958
00:59:06,220 --> 00:59:11,840
so the pairing energy is how much energy it takes to pair up the electrons

959
00:59:11,900 --> 00:59:14,420
they prefer to have their own orbital

960
00:59:14,500 --> 00:59:18,950
they would prefer not to repair it takes a certain amount of energy to pair

961
00:59:19,900 --> 00:59:21,810
so in this case

962
00:59:23,460 --> 00:59:26,700
the this is small

963
00:59:27,630 --> 00:59:32,810
set of orbitals up here are really not that much higher in energy than these

964
00:59:33,740 --> 00:59:38,290
so it's not that hard to put one up here and it takes more energy

965
00:59:38,290 --> 00:59:41,200
in this case to pair them up

966
00:59:41,210 --> 00:59:44,470
than it does to put one in the higher level

967
00:59:44,480 --> 00:59:46,890
so this one would have

968
00:59:46,930 --> 00:59:49,820
a drawing like this year for electrons you put in

969
00:59:49,820 --> 00:59:52,890
one of them goes over there

970
00:59:52,940 --> 00:59:56,560
in this case

971
00:59:56,560 --> 00:59:59,900
we have a really big difference it would take a lot

972
00:59:59,910 --> 01:00:02,850
of energy to put an electron way up here

973
01:00:02,850 --> 01:00:05,620
so in this case

974
01:00:05,640 --> 01:00:08,400
this wedding energy is greater

975
01:00:08,420 --> 01:00:10,120
then the pairing energy

976
01:00:10,120 --> 01:00:12,680
this is really a lot higher

977
01:00:12,700 --> 01:00:15,500
here is small makes not much difference

978
01:00:15,510 --> 01:00:17,560
here the energy is large

979
01:00:17,560 --> 01:00:20,390
so in this case you would want

980
01:00:20,450 --> 01:00:23,320
to put your for electrons all down

981
01:00:23,360 --> 01:00:26,130
in the lower level

982
01:00:26,180 --> 01:00:28,540
so let's just take a look

983
01:00:28,550 --> 01:00:29,800
some of those

984
01:00:29,820 --> 01:00:32,760
rules and save them again so here

985
01:00:33,730 --> 01:00:35,900
the splitting is small

986
01:00:35,900 --> 01:00:37,830
this is called a week

987
01:00:41,570 --> 01:00:46,700
so the link in here will be weak field ligands and only strong field ligands

988
01:00:46,700 --> 01:00:49,830
and we're going to talk about that next monday

989
01:00:49,850 --> 01:00:54,330
how you learn certain ligands and whether the weak field strong field

990
01:00:54,380 --> 01:00:55,400
but if there are we

991
01:00:55,410 --> 01:01:00,440
why they did not release that energy very far they don't have a lot of

992
01:01:00,440 --> 01:01:04,670
strength they can do a huge splitting they just you know they just do a

993
01:01:04,670 --> 01:01:08,840
little tiny amounts splitting there pretty weak ligands

994
01:01:08,850 --> 01:01:12,670
and in this case you would have a strong

995
01:01:12,730 --> 01:01:15,640
a strong field

996
01:01:15,680 --> 01:01:20,670
are you have large splitting so those ligands come in and why they take those

997
01:01:20,670 --> 01:01:25,290
d orbital energy levels and they put them really far apart so those are strong

998
01:01:25,290 --> 01:01:31,830
field ligands create a strong field and this is large

999
01:01:31,880 --> 01:01:34,550
right so if we look at some of these rules then

1000
01:01:34,600 --> 01:01:37,620
so here are pairing energy is greater

1001
01:01:37,640 --> 01:01:39,660
then that energy difference

1002
01:01:39,710 --> 01:01:41,900
that means you have a weak field

1003
01:01:41,910 --> 01:01:44,720
not much difference is weak field not much

1004
01:01:44,740 --> 01:01:47,670
not much wedding the wedding is small

1005
01:01:47,680 --> 01:01:50,930
and then what you're going to want to do is put all the electrons in

1006
01:01:50,940 --> 01:01:56,690
singly e to the fullest extent possible so you use both t two g and

1007
01:01:57,540 --> 01:02:02,050
you put them all singly until you're done before you can

1008
01:02:02,100 --> 01:02:05,250
on the other hand if you have a strong field that's going to have a

1009
01:02:05,250 --> 01:02:07,070
really large splitting

1010
01:02:07,090 --> 01:02:12,500
the energy difference between the two g and PG sets of orbitals

1011
01:02:12,520 --> 01:02:16,850
then that's where the energy is going to be greater than the pairing energy

1012
01:02:16,870 --> 01:02:20,990
more favorable to prepare them to make that they jump up

1013
01:02:21,050 --> 01:02:24,110
two this higher energy level up here

1014
01:02:24,160 --> 01:02:26,420
so in that case what you want to do

1015
01:02:26,430 --> 01:02:29,190
is you want to hear them all up

1016
01:02:29,200 --> 01:02:30,950
in t two g

1017
01:02:30,960 --> 01:02:36,230
and only after this is completely fall and you can put any more electrons in

1018
01:02:36,960 --> 01:02:40,320
do you want to put an electron way up there

1019
01:02:40,330 --> 01:02:41,910
in e g set

1020
01:02:41,940 --> 01:02:44,040
of orbitals

1021
01:02:44,840 --> 01:02:47,540
what this is what this means then

1022
01:02:47,550 --> 01:02:49,310
it is in this case

1023
01:02:49,360 --> 01:02:50,490
we're going to have

1024
01:02:50,510 --> 01:02:54,640
the maximum amount of unpaired electrons

1025
01:02:54,710 --> 01:02:59,380
because you're going to try to keep the unpaired first long as you possibly can

1026
01:02:59,380 --> 01:03:01,540
and so this is referred to

1027
01:03:01,590 --> 01:03:05,310
as a high spin systems

1028
01:03:05,330 --> 01:03:06,860
in this case

1029
01:03:06,870 --> 01:03:11,220
you're going to have the minimum number of unpaired you're trying to pair them up

1030
01:03:11,220 --> 01:03:14,160
down here before you put one up here

1031
01:03:14,180 --> 01:03:16,300
so this is a low spin

1032
01:03:16,340 --> 01:03:18,340
system minimum number

1033
01:03:18,350 --> 01:03:22,260
of unpaired electrons that are possible

1034
01:03:22,310 --> 01:03:24,820
so these are two two

1035
01:03:24,830 --> 01:03:26,710
diagrams that you'll see

1036
01:03:26,800 --> 01:03:33,680
and one question on on the problem that has you drawing these these diagrams for

1037
01:03:33,680 --> 01:03:36,710
these different conditions

1038
01:03:36,750 --> 01:03:40,660
now this particular one that we're talking about

1039
01:03:40,700 --> 01:03:43,610
this particular complex is high

1040
01:03:43,700 --> 01:03:47,500
so next week we're going to talk about why that's true

1041
01:03:47,520 --> 01:03:51,500
and you'll learn about some of the ligands and be able to predict whether it's

1042
01:03:51,500 --> 01:03:55,730
going to be a high spin or low spin system

1043
01:03:55,770 --> 01:03:58,940
we can also go through now and two

1044
01:03:58,950 --> 01:04:06,040
electron configurations and also the crystal field splitting energy

1045
01:04:06,050 --> 01:04:08,820
so in this case

1046
01:04:08,840 --> 01:04:10,830
are RTN

1047
01:04:12,610 --> 01:04:14,940
it is going to be equal to

1048
01:04:14,990 --> 01:04:17,390
there are three electrons

1049
01:04:17,400 --> 01:04:19,390
in t two g

1050
01:04:19,400 --> 01:04:21,970
and there's one electron

1051
01:04:21,980 --> 01:04:24,400
in e g

1052
01:04:24,410 --> 01:04:27,000
and over here we have

1053
01:04:27,000 --> 01:04:30,310
you cannot close your eyes to move from the back room you don't even need

1054
01:04:30,310 --> 01:04:33,000
to do that you can sort of guess what i could probably tell what that

1055
01:04:33,100 --> 01:04:34,500
documents about

1056
01:04:34,510 --> 01:04:39,060
without looking at all the other word without knowing what the order is right so

1057
01:04:39,060 --> 01:04:41,200
it sort of seems like a reasonable thing to do

1058
01:04:41,210 --> 01:04:43,030
the topical classification

1059
01:04:43,040 --> 01:04:45,840
it sort of seems reasonable you should be able to look at this information and

1060
01:04:45,840 --> 01:04:47,650
make a categorization decisions

1061
01:04:48,730 --> 01:04:50,690
so technically

1062
01:04:51,620 --> 01:04:53,780
it's nice to think about this

1063
01:04:53,790 --> 01:05:00,290
representation so internally what you would represent in the computer is basically some sort of

1064
01:05:00,290 --> 01:05:04,810
hash table which gives you the word frequency which would write the mathematically it's nice

1065
01:05:04,810 --> 01:05:09,500
to think about this as a very long sparse vector OK

1066
01:05:09,510 --> 01:05:13,930
where are the dimensions of the vector of the entire vocabulary not just the words

1067
01:05:13,930 --> 01:05:16,130
that appear in the document to they all

1068
01:05:16,140 --> 01:05:18,910
three hundred thousand words that are used in english

1069
01:05:18,910 --> 01:05:25,410
OK and on each the i th component of the vector is the frequency of

1070
01:05:25,410 --> 01:05:26,740
the i th word

1071
01:05:26,760 --> 01:05:29,900
all right so conceptually you can think of this is a very long vector where

1072
01:05:29,900 --> 01:05:31,830
most of the entries are zero

1073
01:05:31,850 --> 01:05:34,830
so mathematically it's much more convenient so

1074
01:05:34,880 --> 01:05:39,420
as i go through the rest of these that's the that's the mathematical representation to

1075
01:05:39,420 --> 01:05:43,750
use but you know under the hood inside the computer what we're really gonna milk

1076
01:05:43,760 --> 01:05:44,890
that is the sort of

1077
01:05:44,910 --> 01:05:48,400
word frequency paper representation

1078
01:05:48,420 --> 01:05:52,010
OK so a little bit of history now OK

1079
01:05:55,010 --> 01:05:59,050
really serious experimental architects text classification

1080
01:05:59,100 --> 01:06:06,760
was david lewis thesis university of massachusetts back in nineteen ninety two

1081
01:06:07,640 --> 01:06:12,590
so was computer science goes and machine learning goes nineteen ninety two is not that

1082
01:06:12,590 --> 01:06:13,790
long ago

1083
01:06:13,840 --> 01:06:16,850
are so just

1084
01:06:16,860 --> 01:06:18,900
for the purpose of comparison

1085
01:06:19,930 --> 01:06:20,890
the first

1086
01:06:20,900 --> 01:06:25,620
machine learning paper talking about classification of

1087
01:06:27,470 --> 01:06:28,390
you know

1088
01:06:28,430 --> 01:06:32,600
vector data that i know that's widely cited studied under are the ones that go

1089
01:06:32,680 --> 01:06:37,300
to go that earlier that was fisher's nineteen thirty six people talk about linear discriminant

1090
01:06:37,430 --> 01:06:39,390
so was in nineteen thirty six

1091
01:06:40,870 --> 01:06:46,230
david lewis did this text categorisation stuff was fifty years later so why fifty years

1092
01:06:46,230 --> 01:06:50,850
later when people look at text classification for before that given that there are so

1093
01:06:50,850 --> 01:06:52,910
many different applications of it

1094
01:06:52,990 --> 01:06:57,190
well one thing is just we had to get computers that we're ready to handle

1095
01:06:57,190 --> 01:07:00,960
the problem right so the typical text classification problem

1096
01:07:00,970 --> 01:07:06,440
and here's one that you know i had a while back with your singer so

1097
01:07:06,470 --> 01:07:10,300
we have three hundred thousand documents are pretty short but still there were sixty seven

1098
01:07:10,300 --> 01:07:13,740
thousand words that turned out to do we do have a little bit better from

1099
01:07:13,740 --> 01:07:17,680
used for ground so we had like three million four grams so looking at

1100
01:07:17,690 --> 01:07:21,490
thirty three thousand documents and three million features

1101
01:07:21,500 --> 01:07:22,460
OK so

1102
01:07:22,610 --> 01:07:29,370
computationally there's there's there's a cost and storing these things and and are

1103
01:07:29,640 --> 01:07:32,860
and working with them even with efficient methods like days

1104
01:07:32,990 --> 01:07:35,140
but there's also a conceptual problem

1105
01:07:37,340 --> 01:07:39,040
in fishers data

1106
01:07:39,060 --> 01:07:45,290
he was classifying flowers based on four measurements OK so every example and four numbers

1107
01:07:46,700 --> 01:07:49,940
there was the belief that as you increase the number of dimensions the problem we

1108
01:07:49,940 --> 01:07:51,710
are intrinsically harder

1109
01:07:52,420 --> 01:07:57,480
so here we are talking about learning with millions and millions of features

1110
01:07:57,490 --> 01:08:00,410
OK and typically more features and have examples

1111
01:08:00,420 --> 01:08:02,220
so how does network

1112
01:08:02,230 --> 01:08:06,470
so from from the theoretical point of view it seems like it shouldn't work

1113
01:08:07,370 --> 01:08:12,430
and actually even in nineteen ninety two it was unclear why it work

1114
01:08:12,460 --> 01:08:16,200
that sense only sort of come to understand why

1115
01:08:16,220 --> 01:08:24,960
classification can work and does work and basically several different pieces to the puzzle so

1116
01:08:24,970 --> 01:08:27,050
for efficiency

1117
01:08:27,210 --> 01:08:29,400
you don't need to represent

1118
01:08:29,420 --> 01:08:36,000
you know this matrix of three hundred thousand documents and three million features by representing

1119
01:08:36,020 --> 01:08:41,480
on with the matrix it's three hundred nineteen thousand rows and three million columns where

1120
01:08:41,480 --> 01:08:47,000
every values explicitly represented so by using sparse matrices you can represent the stuff inside

1121
01:08:47,000 --> 01:08:49,310
the computer without too much work

1122
01:08:50,210 --> 01:08:54,140
for efficiency you need to use sparse representations

1123
01:08:54,150 --> 01:09:00,200
but the other important observation was if you're using simple classifiers that have wide margins

1124
01:09:00,220 --> 01:09:04,220
and you can theoretically showed that the course of the dimensionality is not going to

1125
01:09:04,220 --> 01:09:05,650
come back and bite you

1126
01:09:05,670 --> 01:09:07,420
so i'm is going to be a a little bit

1127
01:09:07,440 --> 01:09:12,790
the intuition behind these observations so what's the idea here so you got some examples

1128
01:09:12,790 --> 01:09:17,160
in just in two-dimensional space rape get some positive examples and no negative examples down

1129
01:09:18,150 --> 01:09:21,840
so this line right here

1130
01:09:21,840 --> 01:09:25,960
is put in the middle of these things OK of course lots of different ones

1131
01:09:25,960 --> 01:09:27,400
you can draw between these

1132
01:09:27,410 --> 01:09:29,290
i am destroying if you here

1133
01:09:29,290 --> 01:09:30,090
i mean

1134
01:09:30,130 --> 01:09:32,790
of another class

1135
01:09:32,810 --> 01:09:36,630
this is this is already having explained somewhere here

1136
01:09:36,770 --> 01:09:42,110
i don't want better than a bunch this is full of class

1137
01:09:42,130 --> 01:09:45,110
this carries class class k

1138
01:09:45,130 --> 01:09:48,650
just people to see want to try to write that but i need to do

1139
01:09:48,650 --> 01:09:52,610
is i reduce the support of all these will all the world's the perfect i

1140
01:09:52,610 --> 01:09:57,430
reduce the support somebody world had off you

1141
01:09:57,530 --> 01:09:59,350
but that would be the new

1142
01:09:59,430 --> 01:10:02,410
so that's all you're doing introducing very general rules

1143
01:10:02,430 --> 01:10:06,730
and then when you find patterns that contradict that will you're just the will just

1144
01:10:06,730 --> 01:10:10,730
make it a bit more special but it doesn't fall for this contradiction any more

1145
01:10:10,750 --> 01:10:13,810
and then you have these two steps you introduce a new one or when already

1146
01:10:13,810 --> 01:10:16,850
had one it really don't do much

1147
01:10:16,960 --> 01:10:22,470
you can show that this algorithm terminates converges wonderfully it's not problem

1148
01:10:22,510 --> 01:10:28,030
it's actually also computational in another big problem the the coverage is easy you just

1149
01:10:28,030 --> 01:10:31,990
need to check if the problem is that in one of these rectangle to find

1150
01:10:31,990 --> 01:10:35,310
one of the things they do commit it's very easy to use and rules for

1151
01:10:35,320 --> 01:10:41,130
real shrink these space that's summarised coming how you avoid this conflict you can reduce

1152
01:10:41,130 --> 01:10:45,230
it you know you could also use here you have several choices the conflicts in

1153
01:10:45,230 --> 01:10:50,690
our case for users the volume based on our strategy you just try to keep

1154
01:10:50,700 --> 01:10:52,730
the world's largest also

1155
01:10:52,810 --> 01:10:57,890
the output of these this hour and this is the world system

1156
01:10:57,910 --> 01:11:03,210
that has possibly very small number of rules because i mean if you find a

1157
01:11:03,210 --> 01:11:07,500
region here he would model this region up the the the the large rule covering

1158
01:11:07,520 --> 01:11:10,610
all of these belonging to class high and then down here you would get three

1159
01:11:10,610 --> 01:11:14,170
smaller rules that say OK this is a region belongs to class low this region

1160
01:11:14,170 --> 01:11:19,350
as class high in this region in the ones that so only introducing finer grain

1161
01:11:19,350 --> 01:11:22,190
relations in areas where you actually need them

1162
01:11:22,750 --> 01:11:26,410
and not all of these attributes need to actually going to the the rules so

1163
01:11:26,420 --> 01:11:31,410
this will appear for example may not even constrain itself towards the x axis right

1164
01:11:31,410 --> 01:11:35,350
i mean it was introduced infinitely large towards all dimensions so all this one is

1165
01:11:35,350 --> 01:11:39,870
probably going to say is well y needs to be above a certain threshold somewhere

1166
01:11:39,870 --> 01:11:44,430
here then it's plastic x class high in the stand here a little bit more

1167
01:11:44,430 --> 01:11:48,490
more restrictive

1168
01:11:48,610 --> 01:11:50,950
so just to

1169
01:11:51,030 --> 01:11:56,790
i mean that this actually available online on our department's website if you care about

1170
01:11:56,800 --> 01:12:00,970
the sort of the nice thing is so this fuzzy rule learning algorithms free fuzzy

1171
01:12:00,970 --> 01:12:05,790
rule learning our first set of rules the complete is that perfect all-round will classify

1172
01:12:05,790 --> 01:12:12,070
all the training patterns correctly and it actually does generalise reasonably the armed force only

1173
01:12:12,070 --> 01:12:14,530
as long as the training data is conflict three if you have a couple of

1174
01:12:14,530 --> 01:12:19,980
instances device actually the same input patterns that have different output classes there's no great

1175
01:12:19,980 --> 01:12:25,130
that you can build rule system that differentiates those for the ones that do you

1176
01:12:25,130 --> 01:12:30,590
have any background in machine learning mitchell version space that the the interesting thing here

1177
01:12:30,590 --> 01:12:38,530
is that it's generating partial hypotheses each of these rules is describing is representing the

1178
01:12:38,530 --> 01:12:43,090
partial hypothesis for part of your training data sort of responsible to model the part

1179
01:12:43,090 --> 01:12:48,490
of your training data but the core region is the most specific hypothesis the generating

1180
01:12:48,490 --> 01:12:54,050
here this core region is the smallest rectangle that you can possibly find enclosing patterns

1181
01:12:54,050 --> 01:12:55,390
of one class

1182
01:12:55,470 --> 01:12:56,350
and the

1183
01:12:56,390 --> 01:13:01,050
support region is the largest one that you can possibly find just avoiding conflicts right

1184
01:13:01,070 --> 01:13:04,990
the baby shrink this it's a very large rectangle that tries to cover as much

1185
01:13:04,990 --> 01:13:09,210
space but that does not contain only come any conflicts

1186
01:13:09,250 --> 01:13:12,930
the support is also more general than the core doesn't really matter and the nice

1187
01:13:12,930 --> 01:13:15,850
thing here so you can interpret that as the core region is the one with

1188
01:13:15,850 --> 01:13:19,710
the highest degree of confidence we actually have seen patterns that resulted in creation of

1189
01:13:19,710 --> 01:13:23,990
the correct angle if seen patterns of the class and the support region is the

1190
01:13:23,990 --> 01:13:27,870
largest area and we have seen no conflict but we have not seen any couple

1191
01:13:29,090 --> 01:13:34,170
i have a little bit more that in a second

1192
01:13:37,030 --> 01:13:40,730
there are many different i mean if you go in the literature and read journals

1193
01:13:40,730 --> 01:13:46,770
conference articles on on fuzzy rule learning all rooms the silence around you'll find thousands

1194
01:13:48,890 --> 01:13:51,980
if you wanted to classify them you could try to do it like that it's

1195
01:13:51,980 --> 01:13:55,030
at all they have the ones that are constructive like the one i just discussed

1196
01:13:55,030 --> 01:13:59,810
now they try to find fuzzy rules by either growing and shrinking them i mean

1197
01:13:59,810 --> 01:14:04,410
the one i just discussed was one that finds fuzzy rules by shrinking them from

1198
01:14:04,410 --> 01:14:09,170
being very very general to just avoiding conflicts it could also opposite way try to

1199
01:14:09,530 --> 01:14:11,730
grow fuzzy rules so that they

1200
01:14:11,750 --> 01:14:14,870
not grow from one pattern and try to grab more and more patterns of the

1201
01:14:14,870 --> 01:14:17,390
same class to grow them from singletons

1202
01:14:17,490 --> 01:14:22,190
you can have grid based approach one mendel higgins goodman that have this global partitioning

1203
01:14:22,190 --> 01:14:23,110
of space

1204
01:14:23,270 --> 01:14:27,510
you could do more things you could try to adapt those those those boundaries between

1205
01:14:27,510 --> 01:14:28,990
the grid cells

1206
01:14:29,030 --> 01:14:32,010
you could even try to merge grid cells neighbouring grid cells that belong to the

1207
01:14:32,010 --> 01:14:36,990
same class could be merged together rows columns even if they don't cover any points

1208
01:14:36,990 --> 01:14:42,790
of the same class you can have adopted tolerance initial ice rules randomly

1209
01:14:42,810 --> 01:14:46,850
but you use an expert knowledge sometimes have partial expert knowledge that you can use

1210
01:14:46,890 --> 01:14:51,930
and then you optimise rule parameters exactly where the location of the membership functions you

1211
01:14:51,940 --> 01:14:56,890
could even sometimes try to optimise the number of membership functions there are algorithms that

1212
01:14:56,890 --> 01:15:02,070
are based on genetic systems evolutionary algorithms to try to find fuzzy rule systems to

1213
01:15:02,070 --> 01:15:05,970
and our so this looks roughly correspond to this first

1214
01:15:09,400 --> 01:15:10,900
as skip this RDF

1215
01:15:10,930 --> 01:15:16,610
for RDF i sets is just encoding of graph nothing think so

1216
01:15:17,600 --> 01:15:20,640
if you have a description one

1217
01:15:20,710 --> 01:15:22,320
so data

1218
01:15:22,330 --> 01:15:26,680
scrapped in RDF this means that one atomic unit is

1219
01:15:27,020 --> 01:15:30,060
triple object attribute value

1220
01:15:30,100 --> 01:15:35,520
and which is just a note link and another can be constructed out of this

1221
01:15:36,640 --> 01:15:42,460
one bigger structure and an upper-level low levels like RDF schema in our actually at

1222
01:15:42,460 --> 01:15:44,360
semantics for this graph

1223
01:15:44,410 --> 01:15:48,540
i was keeping it so this is what what i

1224
01:15:48,550 --> 01:15:50,820
what i have here basically

1225
01:15:50,830 --> 01:15:52,600
it's written like this

1226
01:15:52,620 --> 01:15:54,920
x and this

1227
01:15:54,980 --> 01:15:57,590
but i was keep this hollow

1228
01:15:57,640 --> 01:16:01,970
this is the next level let's see if go back to this

1229
01:16:02,000 --> 01:16:05,480
so RDF research graph and ontology

1230
01:16:05,500 --> 01:16:09,410
it is smallest one way how to use this graph two

1231
01:16:09,410 --> 01:16:13,100
describe more abstract models now

1232
01:16:13,150 --> 01:16:17,650
what our what you need to know about our not to go too much into

1233
01:16:17,650 --> 01:16:20,720
the data is basically it

1234
01:16:22,350 --> 01:16:23,680
it's logic language

1235
01:16:23,730 --> 01:16:26,380
description logics so this is the common

1236
01:16:26,390 --> 01:16:29,120
with this set of operators

1237
01:16:30,330 --> 01:16:33,150
first order logic which still enable you

1238
01:16:33,470 --> 01:16:35,380
to be

1239
01:16:35,460 --> 01:16:39,720
in tractable so that you can prove

1240
01:16:40,010 --> 01:16:44,870
proof of any claim so all our lives is the

1241
01:16:44,870 --> 01:16:46,180
simple lists

1242
01:16:46,210 --> 01:16:51,230
version of our language very well allowed to use only a couple of operators and

1243
01:16:51,230 --> 01:16:55,650
these are just a couple of simple constraints and basically what you model here so

1244
01:16:55,650 --> 01:17:00,820
all the light is meant just to encode taxonomy three nothing more this

1245
01:17:00,820 --> 01:17:02,620
i DNA

1246
01:17:02,640 --> 01:17:04,940
so this is maximal expressivity

1247
01:17:04,960 --> 01:17:07,450
of logic layer which would still allows

1248
01:17:07,470 --> 01:17:09,900
that every claim

1249
01:17:09,910 --> 01:17:11,590
for every query can be

1250
01:17:11,970 --> 01:17:16,480
proven by to improve small s

1251
01:17:16,540 --> 01:17:18,220
it is but the

1252
01:17:18,540 --> 01:17:21,650
and i would fully small this

1253
01:17:21,650 --> 01:17:25,960
full logic of the first order rich

1254
01:17:25,980 --> 01:17:29,190
can be also so these are three levels of our language

1255
01:17:29,720 --> 01:17:33,310
roughly correspond to set of operators which allowed to use

1256
01:17:33,320 --> 01:17:38,340
which is well user can can can use to describe a

1257
01:17:39,310 --> 01:17:43,400
usually ontology

1258
01:17:43,410 --> 01:17:45,150
OK this

1259
01:17:45,170 --> 01:17:47,980
keep this ontogen also ontogen is one

1260
01:17:48,010 --> 01:17:56,620
system our system for learning ontologies but i was keep it's not so

1261
01:17:56,620 --> 01:17:57,450
and go

1262
01:17:59,060 --> 01:18:02,700
the last part of the semantic web which is a description of the

1263
01:18:02,860 --> 01:18:05,830
cyc system

1264
01:18:06,240 --> 01:18:09,800
so semantic that if

1265
01:18:10,600 --> 01:18:13,940
it's about bringing semantics

1266
01:18:13,970 --> 01:18:16,590
are common understanding into the as

1267
01:18:16,590 --> 01:18:20,600
i said before now there are a couple of systems which

1268
01:18:20,710 --> 01:18:23,610
i are related to

1269
01:18:23,660 --> 01:18:28,850
semantic web or which are quite characteristic like strategy is very popular

1270
01:18:28,860 --> 01:18:35,250
ontology editor about this ontogen is getting also quite popular for learning for building ontologies

1271
01:18:35,250 --> 01:18:38,190
straight from the data and so on a a couple of water

1272
01:18:38,260 --> 01:18:40,660
but one tool which is very

1273
01:18:41,610 --> 01:18:44,060
well known from the past of AI

1274
01:18:44,080 --> 01:18:47,150
is site so probably some

1275
01:18:47,170 --> 01:18:48,990
well i'm sure

1276
01:18:49,360 --> 01:18:51,160
she knows about it but

1277
01:18:51,170 --> 01:18:54,520
well there's no about site OK before

1278
01:18:54,570 --> 01:18:58,740
so psyches

1279
01:18:59,370 --> 01:19:01,150
an example of

1280
01:19:01,160 --> 01:19:02,570
really it is

1281
01:19:02,660 --> 01:19:06,010
semantic that dream but

1282
01:19:06,020 --> 01:19:09,040
still these dreams didn't come true completely

1283
01:19:09,040 --> 01:19:12,420
a little bit of history

1284
01:19:12,440 --> 01:19:15,300
so it it was

1285
01:19:15,310 --> 01:19:16,800
a site was

1286
01:19:16,820 --> 01:19:23,320
and attempts from eighty eight these early eighties to encode common sense knowledge into one

1287
01:19:23,320 --> 01:19:25,980
knowledge base common sense knowledge would mean this

1288
01:19:26,000 --> 01:19:32,500
knowledge which we have on which we operate with on the basis of the project

1289
01:19:32,500 --> 01:19:33,790
started saying

1290
01:19:33,800 --> 01:19:35,100
eighty four

1291
01:19:35,110 --> 01:19:41,790
therefore and as a response to the japanese project few generation computer that's on the

1292
01:19:41,790 --> 01:19:42,620
times of

1293
01:19:42,670 --> 01:19:43,930
although built again

1294
01:19:43,940 --> 01:19:46,690
i guess remember this

1295
01:19:46,730 --> 01:19:49,350
i was in high school at that time

1296
01:19:50,540 --> 01:19:57,410
and so probably this cyc is the only remaining

1297
01:19:57,430 --> 01:20:02,880
i think are the only remaining effort which which is still alive after his fifth

1298
01:20:02,900 --> 01:20:09,010
generation computer which started which was started in japan and in ninety four the company

1299
01:20:09,010 --> 01:20:15,740
cycorp was established in austin texas and later on the company well its own life

1300
01:20:16,410 --> 01:20:21,470
two thousand five the cyc knowledge base got open and available for research so we

1301
01:20:21,470 --> 01:20:22,810
have two version of

1302
01:20:23,360 --> 01:20:26,170
cyc one is open cycle which is

1303
01:20:26,250 --> 01:20:28,390
a common vocabulary

1304
01:20:28,400 --> 01:20:30,590
which can be used for

1305
01:20:30,630 --> 01:20:34,850
four applications or whatever is maximally free

1306
01:20:34,870 --> 01:20:40,600
licence and researchcyc which is what this everything what side these the in this twenty

1307
01:20:40,600 --> 01:20:47,240
years what's interesting let's be open this site in europe for the last year

1308
01:20:47,300 --> 01:20:51,170
so in the down as part of our technological park

1309
01:20:51,240 --> 01:20:56,360
and was maybe also interesting to see to know is that the use

1310
01:20:56,500 --> 01:20:58,730
two thousand six after eighty million

1311
01:20:58,750 --> 01:21:04,200
o dollars was spent into developing in this knowledge base

1312
01:21:04,220 --> 01:21:05,700
and this knowledge base

1313
01:21:05,730 --> 01:21:09,430
aims at least two color more all aspects of

1314
01:21:09,430 --> 01:21:15,080
actually is written in a slightly different coordinate system might find you to be elsewhere

1315
01:21:15,080 --> 01:21:19,720
though there are then you get this very simple form and actually it is very

1316
01:21:19,720 --> 01:21:25,720
simple form is exactly what you get if you have a negative cosmological constant and

1317
01:21:25,720 --> 01:21:30,310
maximal symmetry that's like should the sphere with just

1318
01:21:30,330 --> 01:21:34,350
an appropriate time according to keep them here

1319
01:21:34,350 --> 01:21:38,050
so that's the geometry that one looks at

1320
01:21:38,060 --> 01:21:40,720
and now let's do the following men tal

1321
01:21:41,260 --> 01:21:46,990
exercise suppose we want to think about low-energy excitations in these

1322
01:21:48,160 --> 01:21:53,390
what are all possible very we have very little energy at our disposal one to

1323
01:21:53,390 --> 01:21:58,220
excite the system what can we do what we can definitely have

1324
01:21:58,260 --> 01:22:05,200
gravity tons of extremely low frequency extremely large wavelengths

1325
01:22:05,240 --> 01:22:10,260
stressing that crossed the geometry and those who live out in the bulk of the

1326
01:22:10,260 --> 01:22:11,850
definitely fair

1327
01:22:11,950 --> 01:22:18,760
and those have arbitrarily small energy however because their wavelength is so huge compared to

1328
01:22:18,760 --> 01:22:21,780
the characteristic size of the throat

1329
01:22:21,780 --> 01:22:25,740
they don't really see the for all you can not probe the throat with something

1330
01:22:25,740 --> 01:22:30,720
that has a wavelength that's no millions of times began

1331
01:22:30,740 --> 01:22:35,200
therefore we have gravity in the bulk but which decouples

1332
01:22:35,280 --> 01:22:38,390
on the other hand we have everything

1333
01:22:38,390 --> 01:22:44,180
near the horizon because remember there was this infinite to achieve whatever you put there

1334
01:22:44,200 --> 01:22:48,970
can be an elephant would have actually minus that redshift in two zero so you

1335
01:22:48,970 --> 01:22:55,030
can exciting the horizon anything you wish and it has arbitrarily low energies

1336
01:22:55,080 --> 01:22:59,990
and therefore what we get are all possible excitations in the throat

1337
01:23:00,010 --> 01:23:03,910
due to the large which gives rise to the top of the gravity of the

1338
01:23:09,930 --> 01:23:14,970
now let's think about the same problem now as all as users but thinking about

1339
01:23:14,990 --> 01:23:19,080
it from the string theory point of view namely in terms of the brains

1340
01:23:19,080 --> 01:23:23,810
what do we have that what had this the brains we discussed

1341
01:23:23,870 --> 01:23:28,390
in the last two lectures how there is a gauge theory living on them

1342
01:23:28,430 --> 01:23:33,030
in the case at hand it's actually the most supersymmetric gauge theory i would say

1343
01:23:33,030 --> 01:23:35,430
a word about it which is simply

1344
01:23:35,450 --> 01:23:41,530
in equal for super yang mills in four dimensions show it's a theory that having

1345
01:23:41,530 --> 01:23:49,470
the use non abelian gauge fields plus a big supersymmetric or will describe one

1346
01:23:49,490 --> 01:23:54,510
so this is the massless states of open strings living here and then we have

1347
01:23:54,510 --> 01:23:57,330
supergravity has usually been in the bulk

1348
01:23:57,390 --> 01:24:03,140
therefore again the low-energy excitations of the system are a young music theory on the

1349
01:24:04,240 --> 01:24:09,200
and the copyright line into super gravitons in the bulk big problem because the coupling

1350
01:24:09,200 --> 01:24:15,060
of gravity goes of energy if i go to very low energy gravity couples

1351
01:24:15,180 --> 01:24:16,780
so now you know if you

1352
01:24:16,780 --> 01:24:19,080
look at these two equations

1353
01:24:19,100 --> 01:24:22,580
one plus two is equal to one plus two two

1354
01:24:22,600 --> 01:24:24,740
there is an obvious thing you want to do

1355
01:24:24,790 --> 01:24:28,430
a plus b is a policy implies because to c and therefore you want to

1356
01:24:28,430 --> 01:24:32,680
conjecture that are four let's forget the gravity in the bulk

1357
01:24:32,720 --> 01:24:38,160
any one of four super yang mills before that mentions his maybe exactly equivalent to

1358
01:24:38,180 --> 01:24:43,510
the full string theory or anything you can construct out of the string theory but

1359
01:24:43,510 --> 01:24:49,060
in this very funny for the geometry which is a just five five

1360
01:24:49,120 --> 01:24:56,390
and this is the sharpest statement of what one also holographic correspondence holographic duality was

1361
01:24:56,390 --> 01:25:01,830
most sharply stated aim of the same name ninety seven in this very auditorium actually

1362
01:25:01,830 --> 01:25:06,140
know what works back to

1363
01:25:06,240 --> 01:25:10,240
OK now

1364
01:25:10,280 --> 01:25:14,700
let me say a few more words about this very special theory trying to force

1365
01:25:14,790 --> 01:25:20,060
pretty amazing did not the kind of gauge theory we want to discuss priority if

1366
01:25:20,080 --> 01:25:24,140
we are interested in the standard model because it has indeed

1367
01:25:24,200 --> 01:25:28,580
big age boyzone's of you when rescuing young males

1368
01:25:28,580 --> 01:25:32,720
but it has also for every gauge boson four gauginos

1369
01:25:32,720 --> 01:25:35,660
six scalar partners

1370
01:25:35,720 --> 01:25:40,580
that makes it very very supersymmetric as met because it gets

1371
01:25:40,580 --> 01:25:42,970
but on the other hand this theories of

1372
01:25:43,010 --> 01:25:47,160
great simplicity for the simple reason that is nothing else

1373
01:25:47,200 --> 01:25:54,950
but supersymmetric missing and dimensions i discussed this be the first lecture just dimensionally reduced

1374
01:25:54,970 --> 01:25:56,120
down to

1375
01:25:56,160 --> 01:26:02,100
four dimensions where you decide to call the gauge bosons of the extra six dimensions

1376
01:26:02,100 --> 01:26:04,450
instead of a phi

1377
01:26:04,450 --> 01:26:07,830
so these are the scale as we have the usual

1378
01:26:07,890 --> 01:26:12,080
in their actions of gauge bosons usually community squared

1379
01:26:12,100 --> 01:26:15,810
the whole structure is therefore very simple fixed by these

1380
01:26:15,810 --> 01:26:18,180
and then measure means

1381
01:26:18,240 --> 01:26:23,290
now these much supersymmetry has one advantage in some sense

1382
01:26:23,350 --> 01:26:27,370
namely it gives you vanishing paper functions

1383
01:26:27,370 --> 01:26:31,120
if you compute the data functions for this coupling here

1384
01:26:31,180 --> 01:26:37,390
it's the ability to vary the basic function kappa vanishes this happened doesn't around so

1385
01:26:37,410 --> 01:26:40,830
the theory is conformal scale invariant

1386
01:26:40,850 --> 01:26:45,260
for all values of the young is coupling constant that's very unlike you should be

1387
01:26:45,260 --> 01:26:47,490
aware of the coupling runs

1388
01:26:47,530 --> 01:26:49,390
from very weak

1389
01:26:49,390 --> 01:26:53,120
values are asymptotically free UV region

1390
01:26:53,120 --> 01:26:56,140
two strong coupling in the infrared

1391
01:26:56,140 --> 01:27:01,930
there is also no confinement in this theory is also always called like forces between

1392
01:27:01,930 --> 01:27:06,580
charges because precisely the coupling never gets wrong

1393
01:27:06,580 --> 01:27:10,390
the following content is provided under creative commons license

1394
01:27:10,410 --> 01:27:17,100
your support will help MIT opencourseware continue to offer high quality educational resources for free

1395
01:27:17,110 --> 01:27:21,820
to make a donation or view additional materials from hundreds of MIT courses

1396
01:27:21,850 --> 01:27:25,720
this MIT opencourseware OCW MIT

1397
01:27:25,740 --> 01:27:28,310
that you do you

1398
01:27:28,330 --> 01:27:31,580
welcome to five eleven one

1399
01:27:31,620 --> 01:27:35,440
and today we're going to do is introduce you to the course and the people

1400
01:27:35,440 --> 01:27:37,040
teaching the course

1401
01:27:37,060 --> 01:27:39,230
and we're also going to let you know

1402
01:27:39,310 --> 01:27:46,320
that you're going to be part of the great when exercise that is OCW opencourseware

1403
01:27:46,320 --> 01:27:50,740
so this course is being videotaped this year and this is the

1404
01:27:50,740 --> 01:27:53,920
the announcement that i have to make so

1405
01:27:53,920 --> 01:27:58,160
the videotape is in the back and if you want to

1406
01:27:58,190 --> 01:28:03,550
come up front and participate in the classroom though that you'll be videotaped if you

1407
01:28:03,550 --> 01:28:04,610
want to

1408
01:28:04,630 --> 01:28:08,550
hide your face or whatever you can do that please pay attention to the lectures

1409
01:28:10,130 --> 01:28:12,440
this this course will be available

1410
01:28:12,460 --> 01:28:16,550
on the OCW site in the future not sure exactly what they're doing is going

1411
01:28:16,550 --> 01:28:17,750
to be

1412
01:28:19,520 --> 01:28:23,610
so today we're going to introduce the the chemistry topics which we will cover five

1413
01:28:23,610 --> 01:28:28,890
eleven one and give you general information practical information about course number points you need

1414
01:28:29,140 --> 01:28:33,690
when exams are that kind of thing policies and introduce you to the teachings that

1415
01:28:33,720 --> 01:28:37,630
i am again professor kathy drennan and one of the lectures in this in this

1416
01:28:38,920 --> 01:28:42,690
so because his MIT we're going to start quick

1417
01:28:42,740 --> 01:28:46,240
OK not crisp points or anything don't forget

1418
01:28:46,250 --> 01:28:48,500
but i do want you to tell me

1419
01:28:48,520 --> 01:28:57,360
who these people are

1420
01:28:57,470 --> 01:29:00,690
so what about this person

1421
01:29:00,720 --> 01:29:01,640
that's me

1422
01:29:01,640 --> 01:29:06,000
this is my college yearbook photo

1423
01:29:06,020 --> 01:29:12,910
OK what about this person over here

1424
01:29:12,920 --> 01:29:16,420
it's not me again

1425
01:29:16,430 --> 01:29:18,830
not elizabeth taylor

1426
01:29:18,830 --> 01:29:27,450
if we could roll known as phoebe on friends

1427
01:29:29,990 --> 01:29:34,020
we both went to college at the same time we went to the same college

1428
01:29:34,020 --> 01:29:39,700
does anybody know what college that was

1429
01:29:39,740 --> 01:29:42,020
vassar college very kind

1430
01:29:42,020 --> 01:29:45,540
and graduated the same year now no one has to

1431
01:29:45,550 --> 01:29:48,480
say what year that was even if you know

1432
01:29:48,550 --> 01:29:50,840
what we did graduate the same year

1433
01:29:50,890 --> 01:29:51,670
all right

1434
01:29:53,140 --> 01:29:55,860
given what you know about us

1435
01:29:55,860 --> 01:29:57,520
what do you think

1436
01:29:57,540 --> 01:30:02,650
we thought went to college to study

1437
01:30:02,670 --> 01:30:04,200
computers now

1438
01:30:04,210 --> 01:30:06,230
the inner

1439
01:30:06,240 --> 01:30:08,230
surprisingly no

1440
01:30:08,240 --> 01:30:10,430
nuclear engineering faster

1441
01:30:10,450 --> 01:30:14,610
no for a variety of reasons

1442
01:30:14,640 --> 01:30:16,900
any other gases

1443
01:30:16,990 --> 01:30:18,270
english now

1444
01:30:18,300 --> 01:30:21,740
biology a i heard it

1445
01:30:23,610 --> 01:30:27,520
what do you think i went to college to study

1446
01:30:27,550 --> 01:30:29,590
the other contract

1447
01:30:29,610 --> 01:30:35,140
and or had made up my mind exactly

1448
01:30:35,200 --> 01:30:37,860
bio psychology or drama

1449
01:30:37,870 --> 01:30:42,020
so by psychology was what they called sort of brain and cognitive sciences in those

1450
01:30:43,070 --> 01:30:47,140
so those are two my at the two things i was thinking about

1451
01:30:47,210 --> 01:30:51,920
what do you think we use that ended up majoring in college

1452
01:30:56,510 --> 01:31:00,670
not bias psychology

1453
01:31:03,770 --> 01:31:07,490
what do you think i majored in college has to be a bit easier

1454
01:31:07,510 --> 01:31:09,870
chemistry correct

1455
01:31:09,890 --> 01:31:12,180
and of course are professions

1456
01:31:12,200 --> 01:31:15,670
actress and chemistry professor

1457
01:31:15,670 --> 01:31:18,620
so one may ask

1458
01:31:18,640 --> 01:31:22,460
what happened here

1459
01:31:22,510 --> 01:31:25,000
my understanding about lisa kudrow

1460
01:31:25,010 --> 01:31:28,980
is that she came from hollywood family she went to college and said here's my

1461
01:31:28,980 --> 01:31:33,940
opportunity to study the thing that i find most interesting and that was biology

1462
01:31:34,000 --> 01:31:37,850
and then she went back and participated in the family business which was of course

1463
01:31:37,850 --> 01:31:40,010
the acting profession

1464
01:31:40,060 --> 01:31:40,870
for me

1465
01:31:40,910 --> 01:31:43,400
what happened well

1466
01:31:43,460 --> 01:31:44,960
i have to say

1467
01:31:44,970 --> 01:31:47,390
i did not like chemistry

1468
01:31:47,400 --> 01:31:48,710
in high school

1469
01:31:48,720 --> 01:31:52,430
so i do not think about going to college to study chemistry

1470
01:31:52,460 --> 01:31:56,540
so why did i not like chemistry in high school

1471
01:31:56,590 --> 01:32:00,470
i think it was because of images such as this one

1472
01:32:00,500 --> 01:32:07,580
we spent a lot of time talking about the transition between alchemy and modern chemistry

1473
01:32:07,600 --> 01:32:12,460
was very interested in that kind of thing and there's nothing in these photographs really

1474
01:32:12,460 --> 01:32:14,330
appealed to me personally

1475
01:32:14,390 --> 01:32:17,620
i mean the god rho fund his number

1476
01:32:18,100 --> 01:32:20,630
and he is in fact an interesting

1477
01:32:20,640 --> 01:32:22,830
if not frightening looking man

1478
01:32:24,640 --> 01:32:27,700
this just didn't connect with me

1479
01:32:27,710 --> 01:32:31,510
but then i got to college and they said well after thinking about anything bio

1480
01:32:31,510 --> 01:32:35,760
bio psychology biology you have to take chemistry

1481
01:32:35,770 --> 01:32:38,390
and i said to my advisor no no

1482
01:32:38,400 --> 01:32:42,850
i have taken chemistry in high school and i can assure you the chemistry has

1483
01:32:42,850 --> 01:32:46,470
no relevance whatsoever to the life sciences

1484
01:32:46,480 --> 01:32:50,900
and they said well sorry you feel that way it's incorrect and you have to

1485
01:32:50,900 --> 01:32:52,440
take it anyway

1486
01:32:53,690 --> 01:32:56,070
i like some of you in this room

1487
01:32:56,080 --> 01:32:57,310
are talk

1488
01:32:57,330 --> 01:33:01,440
freshman chemistry because we had to not because we want to do

1489
01:33:01,480 --> 01:33:04,260
and i like hopefully some of you in this room

1490
01:33:04,310 --> 01:33:09,150
discovered the chemistry was actually a lot of fun and that chemistry i got to

1491
01:33:09,250 --> 01:33:14,200
college was pretty much nothing like the chemistry i had seen in high school

1492
01:33:14,260 --> 01:33:17,520
so let me introduce you to some of the topics we're going to be covering

1493
01:33:17,960 --> 01:33:21,080
in chemistry this semester

1494
01:33:22,620 --> 01:33:26,520
there's more detail on your syllabus detail of what will cover every day

1495
01:33:26,520 --> 01:33:30,030
these are the kind of basic things that were covering you need to write this

1496
01:33:30,030 --> 01:33:33,100
down to become familiar with it is this last time

1497
01:33:33,120 --> 01:33:39,580
we start out with some really basic principles so what here atomic theory periodic table

1498
01:33:39,580 --> 01:33:46,290
bonding structures of molecules and there will be a little bit of history in there

1499
01:33:46,290 --> 01:33:52,180
so that's for collaborative filtering has also been applied to things like

1500
01:33:52,200 --> 01:34:00,000
causal this kind is not really causal but graph structure discovery for medical diagnostics protein

1501
01:34:00,000 --> 01:34:04,290
complex discovery an independent components analysis so

1502
01:34:04,310 --> 01:34:05,640
and this is what by

1503
01:34:05,660 --> 01:34:07,310
david also

1504
01:34:07,370 --> 01:34:12,120
and i think quite nice model so it kind of goes as follows so

1505
01:34:12,160 --> 01:34:14,350
each image

1506
01:34:14,370 --> 01:34:16,020
we're going to express

1507
01:34:17,330 --> 01:34:22,660
each image is basically a and and we feel that this is an array of

1508
01:34:22,660 --> 01:34:26,080
pixels with this kind of vectorize that so each image is this can be a

1509
01:34:26,080 --> 01:34:31,220
vector of pixel values and we can express this fact as a linear combination of

1510
01:34:31,220 --> 01:34:36,120
sparse features so this vector here x i is going to be some of the

1511
01:34:36,120 --> 01:34:41,540
features k of some basis vector lambda k times the value of that feature

1512
01:34:41,600 --> 01:34:43,060
why i k

1513
01:34:43,060 --> 01:34:47,100
so while i can think of of it as the activity of feature k

1514
01:34:47,140 --> 01:34:53,220
and in ICA we want a sparse prior on on y

1515
01:34:53,250 --> 01:34:56,430
and the sparse prior which we can consider is

1516
01:34:56,520 --> 01:35:01,000
basically a mixture between gaussians and the point mass at zero

1517
01:35:01,020 --> 01:35:04,930
so why i came to the prior of waikiki account looks like

1518
01:35:04,950 --> 01:35:11,640
something that has point masses are mixed with the girls

1519
01:35:12,730 --> 01:35:15,830
and we can

1520
01:35:15,850 --> 01:35:17,730
describe such point

1521
01:35:17,790 --> 01:35:20,720
such a prior as follows y case in game

1522
01:35:20,720 --> 01:35:23,850
well i k simply going to be a product which is is that i can

1523
01:35:23,850 --> 01:35:27,970
a i k where k is going to be drawn from the girls and i

1524
01:35:27,970 --> 01:35:29,540
can either zero or one

1525
01:35:29,560 --> 01:35:30,930
when is zero

1526
01:35:30,930 --> 01:35:34,160
we just get the point masses zero and one this one is simply going to

1527
01:35:34,160 --> 01:35:35,470
be a k

1528
01:35:35,500 --> 01:35:40,200
and this z i k is going to be this matrix that can be drawn

1529
01:35:40,200 --> 01:35:42,330
from the indian buffet process

1530
01:35:42,430 --> 01:35:46,640
so we're going to have an infinite number of features

1531
01:35:46,640 --> 01:35:47,910
to express our

1532
01:35:47,930 --> 01:35:52,810
to describe image only finally many of which is going to take on a non-zero

1533
01:35:54,430 --> 01:35:57,430
non-zero activity

1534
01:35:57,470 --> 01:36:04,830
so there's quite nice actually gives ICA model with infinite infinite number of features

1535
01:36:21,080 --> 01:36:22,540
he yes so

1536
01:36:22,560 --> 01:36:25,350
so this gives us a case

1537
01:36:25,390 --> 01:36:29,850
and that i cases you could equal to one with probability me OK and you

1538
01:36:29,850 --> 01:36:32,950
go to zero with probability one minus mu k

1539
01:36:35,200 --> 01:36:38,810
the next thing autobots hierarchical dirichlet process so

1540
01:36:42,270 --> 01:36:48,370
one of the motivations for the hierarchical dirichlet process is a topic model which takes

1541
01:36:48,370 --> 01:36:53,000
place going into great detail describing to us and the topic model goes as follows

1542
01:36:53,000 --> 01:36:56,200
right so for every we have a set of documents

1543
01:36:56,220 --> 01:36:57,870
this is given by the outer plate

1544
01:36:57,890 --> 01:37:02,770
and every document we're going to describe the words in the document as a mixture

1545
01:37:02,770 --> 01:37:05,140
model is a mixture of topics

1546
01:37:05,160 --> 01:37:10,180
and so documents so document j is going to have the mixing proportion of pi

1547
01:37:10,180 --> 01:37:13,060
j over topics

1548
01:37:13,640 --> 01:37:18,790
and for every word in the document the generative process is as follows with first

1549
01:37:18,790 --> 01:37:22,310
gonna pick a topic z j i from

1550
01:37:22,350 --> 01:37:23,890
this distribution over

1551
01:37:25,270 --> 01:37:28,120
and once you've picked the topic

1552
01:37:28,230 --> 01:37:31,620
then we pick there were from a distribution prior

1553
01:37:31,620 --> 01:37:32,790
that he

1554
01:37:32,790 --> 01:37:34,790
from a distribution over words

1555
01:37:34,810 --> 01:37:38,850
the specific to that topic so that's given by theta k here

1556
01:37:38,910 --> 01:37:44,690
so zack GI is going to be drawn from multinomial parameters pi g and xj

1557
01:37:44,690 --> 01:37:46,890
i is going to be drawn from a multinomial

1558
01:37:46,930 --> 01:37:52,250
there is given by the distribution over words in topic z j

1559
01:37:52,270 --> 01:37:56,330
and if you are bayesian data can place a dirichlet prior

1560
01:37:56,370 --> 01:38:00,390
if you are simple minded business you can use conjugate priors so we can use

1561
01:38:00,390 --> 01:38:03,180
the dirichlet prior for theta

1562
01:38:03,230 --> 01:38:06,580
we're also going to use the dirichlet prior on pi so

1563
01:38:06,600 --> 01:38:08,330
we have the dirichlet prior pi

1564
01:38:08,370 --> 01:38:12,230
so this is the prime minister and dirichlet prior on

1565
01:38:12,250 --> 01:38:14,270
on the

1566
01:38:14,290 --> 01:38:18,490
on the distribution over words in each topic

1567
01:38:18,500 --> 01:38:22,040
and what we want to do is to say that all we don't actually know

1568
01:38:22,040 --> 01:38:24,790
the number of topics so less

1569
01:38:24,810 --> 01:38:27,770
imagine that in fact there is the potential

1570
01:38:28,180 --> 01:38:32,970
that's potentially an infinite number of topics in the world but

1571
01:38:32,970 --> 01:38:36,100
you know we only see a finite number in the finite corpus

1572
01:38:36,140 --> 01:38:41,370
so somehow we have to take k to infinity in this case the

1573
01:38:41,370 --> 01:38:43,890
number of topics that could potentially

1574
01:38:43,910 --> 01:38:46,330
be out in the world some

1575
01:38:46,330 --> 01:38:49,830
but this this model was right so in both the the

1576
01:38:49,850 --> 01:38:51,870
mixture model and in the

1577
01:38:51,870 --> 01:38:55,490
indian buffet process we see that we can just a allocate to infinity and everything

1578
01:38:55,490 --> 01:38:56,500
works out

1579
01:38:56,540 --> 01:38:59,850
and so in this case it doesn't work

1580
01:38:59,870 --> 01:39:02,540
and we have to fix it somehow so

1581
01:39:02,560 --> 01:39:07,950
i can we can call this sort of problem

1582
01:39:07,970 --> 01:39:13,140
a group clustering problem right so every document in every document we make sure model

1583
01:39:13,160 --> 01:39:18,270
so the clusters the words together into different topics but we have lots of different

1584
01:39:18,270 --> 01:39:20,140
documents in each document we have

1585
01:39:20,350 --> 01:39:21,950
a clustering problem

1586
01:39:22,100 --> 01:39:26,750
and what's interesting about this clustering problem is that so

1587
01:39:26,750 --> 01:39:30,290
so imagine this document one document two dutch documentary

1588
01:39:30,310 --> 01:39:32,660
in document one week we cluster the

1589
01:39:32,720 --> 01:39:35,680
data points in the cluster

1590
01:39:38,120 --> 01:39:40,140
OK so

1591
01:39:40,160 --> 01:39:44,450
in document one we cluster the words in the document to in the different clusters

1592
01:39:44,450 --> 01:39:49,680
similarly for document tool and similarly documentry and what we want to see

1593
01:39:49,700 --> 01:39:51,080
i assume is that

1594
01:39:51,100 --> 01:39:53,120
the clusters

1595
01:39:54,270 --> 01:39:58,830
we could across different documents so this is saying that a topic can occur in

1596
01:39:58,830 --> 01:40:01,040
different documents

1597
01:40:01,490 --> 01:40:04,220
so we want to cluster

1598
01:40:04,250 --> 01:40:05,950
each of these groups

1599
01:40:05,970 --> 01:40:07,750
groups of data points

1600
01:40:07,860 --> 01:40:13,450
together such that the mixing components are shared across the different groups

1601
01:40:13,470 --> 01:40:15,200
this is this kind of

1602
01:40:15,200 --> 01:40:22,090
have domain knowledge language knowledge or robert knowledge or objective knowledge about objects and object

1603
01:40:22,180 --> 01:40:24,750
but in the sense if you look in the end if you look at what

1604
01:40:24,750 --> 01:40:28,460
they publish the inference i of are quite similar

1605
01:40:28,470 --> 01:40:32,570
so the vision we have here is that we can abstract to some extent i

1606
01:40:32,570 --> 01:40:36,050
mean you still need to be an expert in the field to back to some

1607
01:40:36,050 --> 01:40:37,980
extent into something like

1608
01:40:38,000 --> 01:40:42,550
each one is working on what really makes their cost different

1609
01:40:42,570 --> 01:40:44,300
where they are the real action

1610
01:40:44,320 --> 01:40:49,750
we share a common toolbox and inference and learning task here so that we benefit

1611
01:40:49,750 --> 01:40:54,750
cross fertilize each other right whatever the NLP guy is developing

1612
01:40:54,770 --> 01:40:59,980
computer vision can use directly and actually that's happening one of the my opinion very

1613
01:40:59,980 --> 01:41:05,830
interesting cross benefits are topic models so they started something like just to understand what's

1614
01:41:05,840 --> 01:41:09,630
going on in the collection of text documents but then if you look at CVPR

1615
01:41:10,980 --> 01:41:15,570
in recent years to find a lot of topic models of visual words where people

1616
01:41:15,570 --> 01:41:16,870
were inspired by

1617
01:41:16,930 --> 01:41:21,210
these ten words in text trying to identify what that means in image coming up

1618
01:41:21,210 --> 01:41:25,240
with ideas of this is something that catches and then they try to

1619
01:41:25,250 --> 01:41:30,420
learn topic models about the visual word which is then foreign-backed on computer machine learning

1620
01:41:30,420 --> 01:41:34,100
where there's a lot of about dictionary learning which means how can we expect learn

1621
01:41:34,110 --> 01:41:38,920
these words automatically so if we can somehow their combined forces i guess we can

1622
01:41:38,920 --> 01:41:43,380
make much more progress than just working independently of each other

1623
01:41:43,480 --> 01:41:44,800
that's vision

1624
01:41:46,380 --> 01:41:53,710
but in the state the motivating we're interested in combining logic and and probability is

1625
01:41:54,070 --> 01:41:56,380
why this tutorial

1626
01:41:56,410 --> 01:42:01,130
well it's very active there's really a lot going on multidisciplinary

1627
01:42:01,180 --> 01:42:04,860
because as has argued that a lot of subdiscipline of AI

1628
01:42:05,680 --> 01:42:10,470
unfortunately having said that they still speak different languages

1629
01:42:10,470 --> 01:42:15,830
right so we managed now maybe within ten years to almost have agreed on

1630
01:42:15,860 --> 01:42:22,050
few language two languages in the sense within the statistical relational learning community and actually

1631
01:42:22,050 --> 01:42:26,400
this year i guess we managed to also extract from these languages and really identify

1632
01:42:26,470 --> 01:42:28,250
some of the core problems

1633
01:42:28,250 --> 01:42:30,250
then if you talk to other people

1634
01:42:30,270 --> 01:42:33,770
they still have their own language right and we have now two chances we can

1635
01:42:33,770 --> 01:42:37,320
try to learn their language is go solve the problem so that they should be

1636
01:42:37,320 --> 01:42:39,070
interested that's what we will do

1637
01:42:39,090 --> 01:42:40,880
but this tutorial celso

1638
01:42:40,900 --> 01:42:41,850
to show you

1639
01:42:41,860 --> 01:42:46,830
maybe it's a benefit for us just tried to jump on the train

1640
01:42:46,850 --> 01:42:49,150
so why should you be interested

1641
01:42:49,210 --> 01:42:53,320
well of course advertisment here but i guess is really a success story not because

1642
01:42:53,320 --> 01:42:58,690
i'm finding that this success story actually one should take petrol holding success story several

1643
01:42:58,690 --> 01:43:01,390
times but it's also if you look at

1644
01:43:01,680 --> 01:43:06,140
many of these papers these papers could best paper award at

1645
01:43:06,160 --> 01:43:11,860
the conference at the years of the specific subdiscipline right so there have been best

1646
01:43:11,860 --> 01:43:14,360
paper board for example in NLP

1647
01:43:14,370 --> 01:43:18,380
right and so why should they give you the best paper award nothing at all

1648
01:43:18,380 --> 01:43:21,840
interesting so there is really something going on

1649
01:43:21,860 --> 01:43:26,650
in bioinformatics there's a lot going on because these experimental design issues and you can

1650
01:43:26,650 --> 01:43:32,060
ask about relations between different experiments and if you make use of that the were

1651
01:43:32,190 --> 01:43:38,540
and the group by color has shown a lot their within the planning community there's

1652
01:43:38,540 --> 01:43:42,130
a lot going on currently there was follow-up

1653
01:43:42,150 --> 01:43:47,760
champion at the probabilistic competition which was used in planning competition which using a relational

1654
01:43:47,760 --> 01:43:53,610
probabilistic approach so there are a lot of stuff people start to see oh yeah

1655
01:43:53,920 --> 01:43:56,170
really then

1656
01:43:56,220 --> 01:44:00,770
OK so after this motivation i help you got a bit excited

1657
01:44:00,790 --> 01:44:03,460
let's give a short overview of what

1658
01:44:03,760 --> 01:44:08,390
but his relation to the next to these two few applications i've shown you the

1659
01:44:08,390 --> 01:44:13,280
textrunner stuff already in this kind of toy story about submitting papers they of course

1660
01:44:13,300 --> 01:44:15,970
really a lot of applications today

1661
01:44:15,990 --> 01:44:21,520
so there have been NLP information extraction a lot i think one of the main

1662
01:44:21,540 --> 01:44:28,930
how basis safer applications including prediction so within machine learning all the collective classification task

1663
01:44:28,990 --> 01:44:33,790
you can quite often be nicely represented within this relational learning

1664
01:44:33,800 --> 01:44:37,960
framework within social network analysis there's a lot of work

1665
01:44:37,970 --> 01:44:39,160
going on

1666
01:44:39,190 --> 01:44:43,730
think of mixed member so you can view them as related model

1667
01:44:43,740 --> 01:44:48,470
in robot mapping there has been some work by for example

1668
01:44:48,480 --> 01:44:54,520
petra again but also by peter fox we're trying to work on the current activity

1669
01:44:55,740 --> 01:45:01,030
the best in show had once one paper on using markov logic to explain activities

1670
01:45:01,300 --> 01:45:05,330
computational biology there's a lot going on and so on and so on so let's

1671
01:45:05,330 --> 01:45:07,970
have a very brief look at them

1672
01:45:07,990 --> 01:45:12,550
in particular the first example petrol will talk much more about that later where you

1673
01:45:12,550 --> 01:45:15,350
also don't understand how you can model

1674
01:45:15,370 --> 01:45:21,060
this kind of problem using markov logic networks and information extraction for example assume you

1675
01:45:21,060 --> 01:45:23,160
have certain references

1676
01:45:23,180 --> 01:45:24,980
it's a problem than that

1677
01:45:26,320 --> 01:45:28,340
so first task is to

1678
01:45:28,360 --> 01:45:33,510
i understand that these are different papers or or they all paper

1679
01:45:33,520 --> 01:45:37,530
so then next to that you may ask of this paper so then i know

1680
01:45:37,540 --> 01:45:42,170
talked about all authors title and then you like you proposing like petrol

1681
01:45:42,190 --> 01:45:47,370
memory efficient inference relational domains is the title and triple-a iowa six second trying you

1682
01:45:47,370 --> 01:45:49,310
do that for all these

1683
01:45:49,320 --> 01:45:50,880
different papers

1684
01:45:50,910 --> 01:45:51,770
well then

1685
01:45:51,780 --> 01:45:58,350
you start realising zingler p and personal are actually the same person right although from

1686
01:45:58,390 --> 01:46:02,420
from the evidence you've got there looking different but if you do some reasoning for

1687
01:46:02,420 --> 01:46:04,470
example by making use of

1688
01:46:04,480 --> 01:46:09,710
the title here there are two papers with the same title you expect that

1689
01:46:09,760 --> 01:46:13,520
the two of us are identical so you see how to take if you take

1690
01:46:13,520 --> 01:46:16,960
really count we get a better picture of

1691
01:46:17,040 --> 01:46:22,770
and then so on and so on so you see that information extraction and NLP

1692
01:46:22,770 --> 01:46:25,320
tasks definitely we can benefit by

1693
01:46:25,370 --> 01:46:27,340
making use of relations by

1694
01:46:27,400 --> 01:46:29,780
solving tasks jointly by

1695
01:46:29,780 --> 01:46:31,040
making use

1696
01:46:31,060 --> 01:46:35,290
of all the knowledge available or all the information available right

1697
01:46:35,890 --> 01:46:38,080
relations are really hard

1698
01:46:38,090 --> 01:46:40,490
and the reason

1699
01:46:40,490 --> 01:46:42,510
well in the early days

1700
01:46:42,520 --> 01:46:46,880
something on for example gene localisation if you do just collective inference

1701
01:46:46,910 --> 01:46:50,880
where you say the location of one gene also depends on

1702
01:46:50,900 --> 01:46:53,060
genes with similar function

1703
01:46:53,060 --> 01:46:55,290
then with a rather very simple

1704
01:46:55,300 --> 01:46:57,400
kind of graphical relational model

1705
01:46:57,420 --> 01:47:00,580
you can be better than the KDD cup winner

1706
01:47:00,600 --> 01:47:04,420
right so not saying this is the best model nowadays for this kind of task

1707
01:47:04,420 --> 01:47:06,350
all right

1708
01:47:28,650 --> 01:47:36,740
robert here of course

1709
01:48:06,810 --> 01:48:11,880
well i mean

1710
01:48:20,850 --> 01:48:24,890
of course

1711
01:49:09,950 --> 01:49:10,640
all o

1712
01:49:52,780 --> 01:49:55,590
well are

1713
01:52:43,890 --> 01:52:52,390
o he

1714
01:52:52,420 --> 01:52:59,830
i don't know

1715
01:53:21,760 --> 01:53:24,410
however we have

1716
01:53:54,310 --> 01:53:58,130
this is all about

1717
01:54:00,060 --> 01:54:03,130
they want

1718
01:54:13,520 --> 01:54:16,800
i'm sure

1719
01:54:16,800 --> 01:54:21,390
that seems uncomfortable for doing machine learning we want them the machine to really learn

1720
01:54:21,390 --> 01:54:25,040
everything including the preprocessing transformation

1721
01:54:25,050 --> 01:54:27,640
so this is exactly what we do in this work

1722
01:54:27,670 --> 01:54:33,740
so a war guassian process incorporates this preprocessing transformation which is going to be warping

1723
01:54:33,740 --> 01:54:38,790
function of the output space as an integral part of the probabilistic model

1724
01:54:38,790 --> 01:54:43,150
and this allows us at least one way to model not nongaussian processes of those

1725
01:54:43,200 --> 01:54:47,020
there are other ways to do not get the same processes

1726
01:54:47,810 --> 01:54:50,960
the idea of working the observation space

1727
01:54:50,980 --> 01:54:53,600
is that we assume that there

1728
01:54:53,760 --> 01:54:56,570
vector of latent target values

1729
01:54:56,980 --> 01:54:58,590
sets of

1730
01:54:58,600 --> 01:55:01,340
that is modeled by guassian process

1731
01:55:01,360 --> 01:55:03,860
but what we observe are going to be

1732
01:55:06,250 --> 01:55:07,580
piece of and

1733
01:55:07,600 --> 01:55:11,340
and we need to find a mapping between

1734
01:55:11,360 --> 01:55:14,050
z and t and

1735
01:55:14,050 --> 01:55:17,800
so we make a transformation from the true observation space t

1736
01:55:17,810 --> 01:55:19,580
raw data space

1737
01:55:19,630 --> 01:55:21,900
to the latent space the

1738
01:55:21,920 --> 01:55:24,620
by mapping each observation through the same

1739
01:55:24,640 --> 01:55:26,700
nonlinear function f

1740
01:55:26,710 --> 01:55:28,780
and that nonlinear function

1741
01:55:28,790 --> 01:55:32,590
it's going to be parameterized by some parameters side

1742
01:55:32,640 --> 01:55:33,860
the question

1743
01:55:43,400 --> 01:55:46,410
so there are a couple reasons for that so let me just explain one of

1744
01:55:46,410 --> 01:55:47,480
them here

1745
01:55:47,960 --> 01:55:51,380
i think you could do it the other way around as well

1746
01:55:51,420 --> 01:55:52,840
is sitting right there

1747
01:55:52,860 --> 01:55:55,190
but it's a bit simpler to do this

1748
01:55:55,900 --> 01:55:58,770
what we do is we require to be monotonic

1749
01:55:58,780 --> 01:56:01,130
and mapping onto the whole of the real line

1750
01:56:01,150 --> 01:56:07,410
and when we do that basically it means that the probability measure will not

1751
01:56:07,430 --> 01:56:11,820
otherwise probability measure would not be conserved and transformation

1752
01:56:11,830 --> 01:56:16,610
and this will induce about distribution of the target

1753
01:56:16,640 --> 01:56:20,830
but i think we could do it the other way just below more complicated

1754
01:56:23,510 --> 01:56:28,330
when we want to learn the parameters of this casting process in the regular guassian

1755
01:56:29,390 --> 01:56:33,740
we had to minimize the sort of cost function

1756
01:56:33,750 --> 01:56:37,010
as a function of the parameters if we're going to do optimisation of the parameters

1757
01:56:37,010 --> 01:56:41,460
for going to be really bayesian we would do inference over these parameters data so

1758
01:56:41,480 --> 01:56:42,260
we would

1759
01:56:42,270 --> 01:56:44,320
computer distribution over data

1760
01:56:44,340 --> 01:56:46,600
proportional to this

1761
01:56:46,620 --> 01:56:51,160
these are data doesn't appear in here because it appears implicitly as a function of

1762
01:56:51,160 --> 01:56:52,340
the covariance

1763
01:56:52,350 --> 01:56:58,080
matrix OK remember the covariance matrix is a function of data

1764
01:56:58,120 --> 01:56:59,840
so that's where theta appears

1765
01:57:02,630 --> 01:57:07,510
this is an ordinary guassian process in the war guassian process

1766
01:57:07,770 --> 01:57:12,070
we have not only the parameters of our kernel or covariance matrix but we also

1767
01:57:12,070 --> 01:57:16,150
have these parameters of our warping function

1768
01:57:17,680 --> 01:57:21,460
we want to do inference or optimisation over both of these things at the same

1769
01:57:22,390 --> 01:57:24,840
essentially the work as in process

1770
01:57:24,850 --> 01:57:27,680
has exactly the same form

1771
01:57:29,490 --> 01:57:32,440
log likelihood as the regular guassian process

1772
01:57:32,460 --> 01:57:33,810
except that

1773
01:57:33,840 --> 01:57:35,600
instead of

1774
01:57:35,610 --> 01:57:37,290
here and we have

1775
01:57:37,300 --> 01:57:39,230
fifty event because

1776
01:57:39,240 --> 01:57:42,340
each of the target system that through the working transformation

1777
01:57:42,350 --> 01:57:44,280
and the only other term here

1778
01:57:44,300 --> 01:57:46,870
is we need the jacobian

1779
01:57:46,900 --> 01:57:49,330
that walking transformation

1780
01:57:50,340 --> 01:57:55,600
changes the that's the measured the changes the probability from one space

1781
01:57:55,620 --> 01:57:57,060
so this is our new

1782
01:57:57,550 --> 01:58:03,370
log likelihood and we can simply optimise this with respect to both of these parameters

1783
01:58:03,400 --> 01:58:05,080
all we can do

1784
01:58:06,230 --> 01:58:10,320
with respect to both these parameters using this

