1
00:00:00,000 --> 00:00:01,670
cut this

2
00:00:01,720 --> 00:00:03,900
parallel planes

3
00:00:03,920 --> 00:00:07,130
they are to act as here can

4
00:00:07,150 --> 00:00:09,610
then we can that this

5
00:00:09,620 --> 00:00:11,310
OK you can offer

6
00:00:11,320 --> 00:00:12,550
the label

7
00:00:12,570 --> 00:00:13,910
there's a

8
00:00:14,100 --> 00:00:16,340
this could have the same area

9
00:00:16,380 --> 00:00:17,930
really amazing

10
00:00:17,960 --> 00:00:19,700
amazing results

11
00:00:19,720 --> 00:00:22,720
so there are lots of things

12
00:00:22,770 --> 00:00:27,100
in order to understand the importance of the mathematical ideas you have to put it

13
00:00:28,290 --> 00:00:32,550
local context that that's crucial for

14
00:00:39,860 --> 00:00:45,410
because at the end of the book he started saying that the political dimension two

15
00:00:47,120 --> 00:00:54,370
approach to the human and social constructivist conceptual previous that was not clustering

16
00:00:56,830 --> 00:00:57,430
this first

17
00:00:57,450 --> 00:01:00,340
the left wing party because of

18
00:01:00,380 --> 00:01:03,060
and its historic striving for universal

19
00:01:03,060 --> 00:01:07,600
literacy because i believe in universal literacy and that

20
00:01:08,590 --> 00:01:10,310
but he was trying to make up

21
00:01:13,270 --> 00:01:14,910
is on that side of the

22
00:01:16,510 --> 00:01:18,780
well as the

23
00:01:18,830 --> 00:01:22,580
is conservative it to a number of

24
00:01:22,630 --> 00:01:25,030
one of the mathematicians were then

25
00:01:26,540 --> 00:01:28,850
radically conservative

26
00:01:28,870 --> 00:01:31,940
because politically

27
00:01:31,960 --> 00:01:35,590
but i don't see the effects of

28
00:01:35,590 --> 00:01:39,430
in nature mathematics economics what

29
00:01:40,120 --> 00:01:41,550
is claiming that

30
00:01:41,590 --> 00:01:44,040
in this view is right wing

31
00:01:44,090 --> 00:01:48,300
a conservative and is a big surprise well

32
00:01:48,310 --> 00:01:50,100
that's really bad

33
00:01:50,110 --> 00:01:51,580
i given

34
00:01:51,590 --> 00:01:55,180
to try to characterize things that way

35
00:01:55,660 --> 00:01:57,890
it's a completely different book

36
00:02:01,560 --> 00:02:03,900
by paul this

37
00:02:03,920 --> 00:02:06,460
this come from

38
00:02:07,580 --> 00:02:10,050
the educational philosophy

39
00:02:10,060 --> 00:02:14,090
the type book i recommend the book

40
00:02:14,100 --> 00:02:17,870
because the lack of references

41
00:02:17,920 --> 00:02:19,710
so the point of view

42
00:02:19,720 --> 00:02:23,890
and the students if you want to write an essay

43
00:02:23,940 --> 00:02:27,330
you should ask someone to review

44
00:02:27,670 --> 00:02:30,400
and so this is a good book to refute

45
00:02:30,410 --> 00:02:32,450
because of this quotation

46
00:02:32,460 --> 00:02:34,440
but i have here

47
00:02:35,650 --> 00:02:39,820
and so from the

48
00:02:39,890 --> 00:02:41,600
OK OK

49
00:02:41,620 --> 00:02:44,340
but the article about the subject

50
00:02:45,790 --> 00:02:47,810
the fact that these assumptions

51
00:02:47,830 --> 00:02:50,120
have status of belief

52
00:02:50,140 --> 00:02:51,910
well i guess

53
00:02:54,070 --> 00:02:56,330
must remain open to challenge about

54
00:02:56,340 --> 00:02:59,970
and i personally notable

55
00:03:00,000 --> 00:03:03,370
well i don't think that's a valid argument

56
00:03:04,360 --> 00:03:05,780
but it does

57
00:03:05,820 --> 00:03:08,410
raise the spectre of

58
00:03:08,500 --> 00:03:11,510
these beliefs that can be

59
00:03:11,520 --> 00:03:15,750
mathematical assumptions of these beliefs can be very large

60
00:03:15,770 --> 00:03:17,380
challenge time

61
00:03:17,450 --> 00:03:19,570
or is there something more

62
00:03:19,570 --> 00:03:22,130
absolute about

63
00:03:22,220 --> 00:03:28,510
that this is the proper of education and is

64
00:03:28,520 --> 00:03:32,020
added number of books and conferences and so on

65
00:03:32,040 --> 00:03:36,040
and i do at the end of the book as an overview

66
00:03:36,060 --> 00:03:41,390
the point of view about social constructivism and

67
00:03:46,140 --> 00:03:49,580
we've looked at the same time

68
00:03:51,020 --> 00:03:52,450
three points

69
00:03:52,560 --> 00:03:57,050
is the mathematical fact

70
00:03:57,060 --> 00:03:59,380
but is always embarrassing there

71
00:03:59,440 --> 00:04:00,700
and so

72
00:04:01,760 --> 00:04:05,750
so this was pointed out with on that you have have to be careful

73
00:04:05,770 --> 00:04:08,650
when you put in some technical

74
00:04:08,710 --> 00:04:11,860
o thing for example that you get it right

75
00:04:12,200 --> 00:04:13,660
i recommend reading

76
00:04:14,720 --> 00:04:21,600
finally go review because you spend a lot of time studying these books completely

77
00:04:21,600 --> 00:04:24,570
we already

78
00:04:39,320 --> 00:04:43,930
we know

79
00:05:32,950 --> 00:05:34,730
one year

80
00:06:07,070 --> 00:06:10,080
now call

81
00:06:29,650 --> 00:06:33,510
and the

82
00:07:48,510 --> 00:07:49,840
all o

83
00:08:39,400 --> 00:08:46,820
it's not about

84
00:09:21,870 --> 00:09:23,970
i mean

85
00:11:00,560 --> 00:11:04,240
i i

86
00:11:16,240 --> 00:11:20,520
right now

87
00:11:46,960 --> 00:11:48,750
information from

88
00:11:53,340 --> 00:11:59,070
it was red

89
00:11:59,070 --> 00:12:02,270
a lot of logic the last part

90
00:12:05,960 --> 00:12:07,860
this is originally

91
00:12:07,880 --> 00:12:10,780
developed by philosopher

92
00:12:10,830 --> 00:12:13,310
because there are not very happy

93
00:12:13,360 --> 00:12:17,670
with classical logic propositional logic is called

94
00:12:17,680 --> 00:12:21,130
because it doesn't take into account things like

95
00:12:22,780 --> 00:12:24,310
and necessity

96
00:12:24,540 --> 00:12:30,340
originally was just developed to take into account the genome but subsequently

97
00:12:30,390 --> 00:12:31,750
and more and more

98
00:12:31,880 --> 00:12:35,090
this is non classical logic

99
00:12:35,140 --> 00:12:36,650
were developed

100
00:12:36,660 --> 00:12:39,340
so a few of them are listed here

101
00:12:40,110 --> 00:12:43,630
what what is traditionally called ontology

102
00:12:43,680 --> 00:12:47,870
reasons about necessity and possibility

103
00:12:47,920 --> 00:12:51,290
deontic logic reason about obligations

104
00:12:51,350 --> 00:12:55,430
it is permitted that of the country one

105
00:12:55,480 --> 00:12:56,670
temporal logic

106
00:12:56,720 --> 00:12:59,380
it's about time so

107
00:12:59,390 --> 00:13:00,980
it will be

108
00:13:01,030 --> 00:13:05,090
sometimes in the future it always feature

109
00:13:05,160 --> 00:13:07,280
and after attending logic

110
00:13:08,250 --> 00:13:10,040
logic about knowledge

111
00:13:11,250 --> 00:13:13,160
which i believe would be

112
00:13:13,170 --> 00:13:15,140
covered next

113
00:13:15,150 --> 00:13:17,360
next week one of the

114
00:13:17,410 --> 00:13:19,230
OK i lectures

115
00:13:19,320 --> 00:13:23,110
and there is dynamic logic which reasons about

116
00:13:23,120 --> 00:13:26,420
execution of the program so

117
00:13:26,430 --> 00:13:28,240
does this program ever

118
00:13:28,250 --> 00:13:31,780
which state when it comes that law

119
00:13:31,910 --> 00:13:33,710
thus there

120
00:13:33,760 --> 00:13:35,670
from this is from whatever

121
00:13:35,690 --> 00:13:37,820
two provincial adventure

122
00:13:38,980 --> 00:13:40,390
race conditions and the can

123
00:13:40,520 --> 00:13:44,910
dynamic logic will be covered in the article

124
00:13:44,920 --> 00:13:50,420
the second one so you can also choose to attend don't

125
00:13:50,430 --> 00:13:53,550
some machine learning course

126
00:13:55,230 --> 00:13:55,900
well the

127
00:13:55,910 --> 00:13:59,450
however now is just very basic syntax the time

128
00:13:59,500 --> 00:14:01,730
so one give you may

129
00:14:01,870 --> 00:14:04,630
a real world examples you see more

130
00:14:04,640 --> 00:14:06,220
next week

131
00:14:09,510 --> 00:14:12,350
for classical propositional logic

132
00:14:12,360 --> 00:14:17,910
the mathematical model is just as we are doing one

133
00:14:17,930 --> 00:14:20,860
from ontology it's complicated

134
00:14:20,870 --> 00:14:24,050
because we have to reasons about truth in different modes

135
00:14:24,100 --> 00:14:25,550
and one of the

136
00:14:27,830 --> 00:14:28,780
very commonly used

137
00:14:28,810 --> 00:14:30,570
it's not the only one

138
00:14:30,580 --> 00:14:35,420
it is the so-called semantics this was followed by p

139
00:14:35,970 --> 00:14:37,210
one of the

140
00:14:37,220 --> 00:14:40,450
the perspective which is still alive

141
00:14:40,460 --> 00:14:45,760
i think it is when i was twenty one years old

142
00:14:45,810 --> 00:14:48,510
there's a lot of work

143
00:14:48,810 --> 00:14:52,110
it's called the possible worlds semantics

144
00:14:52,130 --> 00:14:53,710
so basically

145
00:14:53,720 --> 00:14:59,630
proposition is true with respect to this can move among these walls

146
00:14:59,640 --> 00:15:01,430
and in truth becomes

147
00:15:04,400 --> 00:15:06,160
so a word

148
00:15:06,220 --> 00:15:07,610
can have

149
00:15:07,620 --> 00:15:13,510
many different interpretations according to which logical type which have these two point four

150
00:15:14,520 --> 00:15:17,980
for example the temporal logic working disappointing kind

151
00:15:17,990 --> 00:15:19,750
past future

152
00:15:19,880 --> 00:15:23,650
for dynamic logic it can you state of mind program

153
00:15:23,670 --> 00:15:25,650
and so on

154
00:15:27,540 --> 00:15:31,650
an autonomous agent state where can be

155
00:15:32,230 --> 00:15:33,700
the knowledge

156
00:15:33,860 --> 00:15:36,180
the set of knowledge that agent

157
00:15:36,200 --> 00:15:39,160
says put it

158
00:15:39,210 --> 00:15:42,080
and more abstract ways

159
00:15:43,790 --> 00:15:46,180
possible causes his relations structure

160
00:15:47,120 --> 00:15:48,470
o point in relation

161
00:15:53,640 --> 00:15:56,370
the syntax of modal logic is first

162
00:15:56,400 --> 00:15:59,420
just take propositional logic two

163
00:15:59,430 --> 00:16:00,810
unary operators

164
00:16:00,820 --> 00:16:02,780
box operator

165
00:16:02,830 --> 00:16:06,280
also used to denote necessity and dying

166
00:16:06,330 --> 00:16:08,240
denotes possibility

167
00:16:09,360 --> 00:16:15,870
and again the reading of the box can be different according to which which logics

168
00:16:15,910 --> 00:16:19,140
that you are interested in example box

169
00:16:19,140 --> 00:16:23,410
so what i say about the semantic web

170
00:16:23,450 --> 00:16:26,000
i started hearing this

171
00:16:26,050 --> 00:16:30,290
idea semantic web i think about ten years ago not sure exactly when it started

172
00:16:30,290 --> 00:16:33,760
but it seemed like when people first started talking about XML

173
00:16:34,060 --> 00:16:39,550
the notion of web based semantics was starting to float around the metric exactly the

174
00:16:39,550 --> 00:16:40,870
prominence here

175
00:16:40,920 --> 00:16:46,110
one ever heard of this thing the semantic web

176
00:16:46,120 --> 00:16:46,690
they are

177
00:16:46,710 --> 00:16:52,050
so i got the pitch i first heard the pitch from tim berners-lee in about

178
00:16:52,060 --> 00:16:55,440
and i think in two thousand one to two thousand two

179
00:16:55,460 --> 00:16:59,120
this was two thousand two hundred first are working for IBM

180
00:16:59,140 --> 00:17:04,870
and to give a presentation and he said something like i'm going back car and

181
00:17:04,870 --> 00:17:10,620
an image of business so so the the the light cruiser on its first try

182
00:17:10,620 --> 00:17:14,440
as will have your eyes work

183
00:17:20,410 --> 00:17:23,120
and i said OK listen

184
00:17:23,330 --> 00:17:28,870
when you cut your ear off the mail it to me

185
00:17:28,950 --> 00:17:33,610
so i did not think much of this idea

186
00:17:33,610 --> 00:17:38,440
i mean doing KR since the mid eighties you know please what he what he

187
00:17:38,470 --> 00:17:41,850
trying to tell what trying to some of you know there's nothing innovative here we've

188
00:17:41,850 --> 00:17:45,550
been doing this stuff for almost twenty years

189
00:17:45,560 --> 00:17:46,870
you always know if there's

190
00:17:46,900 --> 00:17:49,430
and a i person in the audience because

191
00:17:49,500 --> 00:17:51,890
that's the reaction to everything

192
00:17:57,470 --> 00:17:58,650
you know

193
00:18:01,070 --> 00:18:03,750
and the knowledge representation field no

194
00:18:03,770 --> 00:18:05,810
knowledge acquisition does not scale

195
00:18:06,450 --> 00:18:07,780
just doesn't scale

196
00:18:07,790 --> 00:18:09,050
you can't

197
00:18:09,760 --> 00:18:13,330
big knowledge acquisition from people is ridiculous

198
00:18:14,390 --> 00:18:19,470
very complex let me explain to you the computer science complexity hierarchy

199
00:18:25,010 --> 00:18:28,370
if you if you allow the idiots on the web two

200
00:18:28,390 --> 00:18:30,780
build ontologies well look

201
00:18:30,880 --> 00:18:35,150
you put that knowledge in a knowledge based system

202
00:18:35,220 --> 00:18:41,600
it's going to be a bad system

203
00:18:41,600 --> 00:18:45,530
all right so

204
00:18:45,540 --> 00:18:47,280
i kind of missed

205
00:18:47,320 --> 00:18:51,800
so here are some of the things that i was right about

206
00:18:51,850 --> 00:18:57,510
you know actual reasoning you know what description logic OWL DL is exponential

207
00:18:58,330 --> 00:19:00,970
by the way not tractable

208
00:19:01,040 --> 00:19:03,050
case anyone was wondering

209
00:19:04,630 --> 00:19:09,810
track anyway it's not tractable to very complex worst case takes a long time

210
00:19:09,810 --> 00:19:13,940
people accidentally fall into the worst case

211
00:19:14,010 --> 00:19:19,330
you know worst case part of the algorithm sometimes without realizing that they're doing it

212
00:19:19,340 --> 00:19:26,210
you know look bad ontologies do lead to bad systems

213
00:19:27,900 --> 00:19:30,830
first of all and i think most importantly

214
00:19:30,830 --> 00:19:33,040
and i'm going to talk about this again

215
00:19:33,090 --> 00:19:37,730
what the semantic web added and what was innovative about it was not that it's

216
00:19:37,730 --> 00:19:42,530
just adding tags to some KR language

217
00:19:42,710 --> 00:19:45,970
what it what turns out to be the case on the web is that knowledge

218
00:19:45,970 --> 00:19:47,380
acquisition from

219
00:19:47,480 --> 00:19:53,420
everyone on the in the world that seems to be scaling perfectly fine

220
00:19:53,430 --> 00:19:56,250
who who who could have predicted i certainly

221
00:19:56,260 --> 00:19:59,640
never imagine that that could be the case

222
00:19:59,660 --> 00:20:03,020
a lot of people don't actually care about the

223
00:20:03,020 --> 00:20:07,550
the complexity of reasoning because they are not really interested in reason and i'll talk

224
00:20:07,550 --> 00:20:10,590
more about that

225
00:20:10,660 --> 00:20:11,600
and then

226
00:20:11,640 --> 00:20:15,310
the most important lesson i think this community can learn is that

227
00:20:15,310 --> 00:20:20,600
there was an improvement here over the way things were before adding some semantics to

228
00:20:20,600 --> 00:20:24,110
the way people use the information on the web really wasn't improvement even if there

229
00:20:24,110 --> 00:20:26,860
are things wrong with

230
00:20:28,100 --> 00:20:29,290
and by the way

231
00:20:29,300 --> 00:20:33,030
even in this community i find that a lot of people don't know the semantic

232
00:20:33,030 --> 00:20:37,370
web vision so the the original ideas it was explained to me was that

233
00:20:37,420 --> 00:20:38,780
if you look

234
00:20:38,810 --> 00:20:43,440
on the web depending on how you count web pages the vast majority of web

235
00:20:43,440 --> 00:20:48,060
pages are automatically generated from some back in data

236
00:20:48,080 --> 00:20:49,980
and the idea

237
00:20:50,050 --> 00:20:54,960
was to publish the schema for that data and the idea was not that

238
00:20:55,050 --> 00:20:59,300
o and then this is your i think they provide some notion of identity on

239
00:20:59,300 --> 00:21:03,020
the web which i'm going to talk a little bit about so the important critical

240
00:21:03,020 --> 00:21:07,480
averaged over all samples will actually be the true loss

241
00:21:07,500 --> 00:21:11,540
so we can actually replace that he h

242
00:21:12,400 --> 00:21:16,460
this expectation under ghost samples as a tilde

243
00:21:17,120 --> 00:21:23,190
in here and then we'll have the empirical on the true sample and an empirical

244
00:21:23,270 --> 00:21:25,250
ghost sample

245
00:21:25,270 --> 00:21:27,640
but the expectation of that

246
00:21:27,650 --> 00:21:30,810
so that's what we've got here the expectation under s

247
00:21:30,810 --> 00:21:35,060
of this quantity here which is now this is the average

248
00:21:35,080 --> 00:21:37,770
of this this is this expression here

249
00:21:37,830 --> 00:21:41,730
which are now replaces this year h

250
00:21:42,770 --> 00:21:45,210
this is just doing that simple

251
00:21:49,210 --> 00:21:53,370
so now what we're going to do is to move that he as of that

252
00:21:55,380 --> 00:22:01,810
and that's going to correspond to a genuine

253
00:22:01,940 --> 00:22:07,400
increase potentially because y is that case

254
00:22:07,460 --> 00:22:09,040
i mean just go back

255
00:22:09,730 --> 00:22:11,000
the way

256
00:22:12,140 --> 00:22:17,750
here we're having to take a super age which maximizes this expectation

257
00:22:17,770 --> 00:22:19,690
over yes still there

258
00:22:19,710 --> 00:22:21,060
if we move the

259
00:22:21,080 --> 00:22:24,980
still there outside we can choose the age to maximize this

260
00:22:25,000 --> 00:22:27,670
for both as and they're still there

261
00:22:27,730 --> 00:22:30,620
and on average are going to do with query therefore going to be able to

262
00:22:30,620 --> 00:22:36,810
get a higher value because we're taking optimizing h over all choices of SNS tilde

263
00:22:36,810 --> 00:22:40,330
rather than optimizing h forgiven given as

264
00:22:40,360 --> 00:22:42,870
and the average investor

265
00:22:43,150 --> 00:22:44,400
it's clear

266
00:22:44,420 --> 00:22:51,460
so this actually increases by taking it's basically jensen in type inequality we're actually increasing

267
00:22:51,460 --> 00:22:53,640
by bringing that yes tilde out

268
00:22:53,650 --> 00:22:55,310
and we're left with this quantity

269
00:22:56,980 --> 00:22:59,830
and now

270
00:22:59,850 --> 00:23:04,960
we can this is this quantity we can now introduce the fact that the this

271
00:23:04,960 --> 00:23:06,920
is now symmetrisation

272
00:23:06,940 --> 00:23:11,380
because if you go back sorry if we go back here

273
00:23:14,520 --> 00:23:15,500
back here

274
00:23:16,900 --> 00:23:20,370
there's nothing really to distinguish once we move the

275
00:23:20,710 --> 00:23:24,620
he as in the s older out there is nothing to distinguish SNS tilde in

276
00:23:24,620 --> 00:23:25,770
this expression

277
00:23:25,790 --> 00:23:27,350
and so we can just war

278
00:23:27,370 --> 00:23:32,060
elements between the two without changing any of the

279
00:23:32,080 --> 00:23:33,400
of the values the

280
00:23:34,770 --> 00:23:37,600
and so we can again introduced this idea of

281
00:23:37,620 --> 00:23:39,690
actually doing that explicitly

282
00:23:39,690 --> 00:23:41,830
and we do that by adding in

283
00:23:41,830 --> 00:23:44,770
an expectation over a random swapping

284
00:23:44,770 --> 00:23:49,650
of corresponding so sigma i swaps is plus one minus one

285
00:23:49,670 --> 00:23:50,850
and therefore

286
00:23:50,870 --> 00:23:56,210
actually swaps these two it's minus one just effectively changes the roles of that ions

287
00:23:56,210 --> 00:23:57,370
that i told

288
00:23:57,420 --> 00:23:59,270
in that expression

289
00:23:59,290 --> 00:24:04,350
so here the symmetrisation is actually introduced by this expectation over the signal

290
00:24:04,730 --> 00:24:09,980
and sigma xi set is the set of plus one minus one variables

291
00:24:09,980 --> 00:24:12,620
which are chosen with equal

292
00:24:12,620 --> 00:24:14,750
probability random probability

293
00:24:14,770 --> 00:24:18,580
and those are actually what the name rademacher comes from these are known as rademacher

294
00:24:19,620 --> 00:24:20,960
the sigma lies

295
00:24:20,980 --> 00:24:26,710
so just think of it as symmetrisation and and this swapping permutations we had before

296
00:24:26,730 --> 00:24:30,370
OK and now finally you can

297
00:24:30,420 --> 00:24:32,920
upper bound this by simply

298
00:24:32,940 --> 00:24:35,850
splitting the sum into two halves

299
00:24:35,850 --> 00:24:39,900
and taking the super over both halves into independently

300
00:24:39,920 --> 00:24:44,020
and that again is

301
00:24:44,080 --> 00:24:47,690
is only going to increase the value because you can choose the age to optimize

302
00:24:47,690 --> 00:24:49,330
both individually

303
00:24:49,400 --> 00:24:51,600
and so you end up with twice

304
00:24:51,600 --> 00:24:54,670
this quantity where you have the absolute value

305
00:24:54,690 --> 00:24:58,770
now there is the trick you can actually get rid of the absolute value but

306
00:24:58,770 --> 00:25:01,060
that's the detail i what

307
00:25:01,060 --> 00:25:05,040
i don't think it's important the standard definition is with the absolute value

308
00:25:05,170 --> 00:25:07,600
and that now is what you end up with

309
00:25:07,600 --> 00:25:11,810
and we're just going to call at rademacher complexity so it seems a little

310
00:25:11,810 --> 00:25:16,670
strange at this stage we just follow through the sequence of arguments and we've ended

311
00:25:16,670 --> 00:25:19,440
up with this quantity but if we now put that all together

312
00:25:19,500 --> 00:25:21,710
it actually makes a very pretty picture

313
00:25:21,750 --> 00:25:25,770
because if i just get through to the results here

314
00:25:25,790 --> 00:25:28,080
what we have is

315
00:25:28,710 --> 00:25:33,810
expected value of the function is less than its empirical value plus the rademacher complexity

316
00:25:33,810 --> 00:25:39,620
the following content is provided under creative commons license your support will help MIT opencourseware

317
00:25:39,620 --> 00:25:43,720
continue to offer high quality educational resources for free

318
00:25:43,740 --> 00:25:48,520
to make a donation or to view additional materials from hundreds of MIT courses

319
00:25:48,530 --> 00:25:54,060
his MIT opencourseware OCW MIT that EDU

320
00:25:54,080 --> 00:25:58,290
one announcement the weekly quizzes on tuesday based on

321
00:25:58,310 --> 00:26:00,380
homework nine which will cover

322
00:26:00,400 --> 00:26:03,220
glasses and chemical kinetics

323
00:26:03,240 --> 00:26:06,150
speaking of which that's something we started

324
00:26:06,150 --> 00:26:08,090
talking about last day

325
00:26:08,160 --> 00:26:11,330
found this cute called

326
00:26:11,390 --> 00:26:16,220
kinetics is nature's way of making sure that everything doesn't happen all at once

327
00:26:16,470 --> 00:26:18,400
and we looked at the

328
00:26:18,460 --> 00:26:23,290
great loss was generated large shown here

329
00:26:23,290 --> 00:26:26,150
the rate of consumption of a particular

330
00:26:26,180 --> 00:26:31,290
reactant goes is some function of its instant concentration raised to the power which is

331
00:26:31,290 --> 00:26:32,830
the order of reaction

332
00:26:32,860 --> 00:26:35,760
the specific chemical rate constant k

333
00:26:35,760 --> 00:26:37,510
is the proportionality

334
00:26:37,550 --> 00:26:42,140
and k can be influenced by temperature through this arrhenius

335
00:26:42,190 --> 00:26:44,980
type relationship which compares

336
00:26:45,030 --> 00:26:46,750
the exponential of the

337
00:26:46,780 --> 00:26:51,810
activation energy with the available for energy

338
00:26:51,890 --> 00:26:56,360
and we saw that we can integrate the rate equation and put

339
00:26:56,400 --> 00:26:59,530
the data to the test if we have a first order reaction we should get

340
00:26:59,530 --> 00:27:01,730
a semi log

341
00:27:01,790 --> 00:27:04,030
i dependence and i think about that

342
00:27:04,030 --> 00:27:05,280
shown here

343
00:27:05,340 --> 00:27:08,780
there's data showed you last day this is just the normal

344
00:27:08,810 --> 00:27:14,720
the decrease in concentration with time typical attenuation curve if we put it to the

345
00:27:14,720 --> 00:27:18,950
first order test according to this relationship which you get a straight line with slope

346
00:27:18,950 --> 00:27:20,330
mines k

347
00:27:20,390 --> 00:27:22,140
and we saw that in

348
00:27:22,170 --> 00:27:27,450
special case of first order reactions make sense sometimes talk about half life in particular

349
00:27:27,450 --> 00:27:29,250
with radioactive decay

350
00:27:29,360 --> 00:27:34,870
we also look second order reactions which have a slightly different functional dependence shown here

351
00:27:34,890 --> 00:27:39,480
and then we went back and looked at the underlying physical chemistry of the activation

352
00:27:40,390 --> 00:27:42,100
so we came up with this

353
00:27:42,120 --> 00:27:46,020
model which we presented in terms of mechanical analog

354
00:27:46,030 --> 00:27:47,570
and we saw that we could

355
00:27:47,590 --> 00:27:53,210
look at the decrease in chemical potential has not been monotonic but as requiring some

356
00:27:55,350 --> 00:27:57,050
and so we have here the

357
00:27:57,060 --> 00:28:01,520
energy stated the reactance one value the energy state of the products at a lower

358
00:28:01,520 --> 00:28:06,550
value and the difference between those values is the driving force for the reaction delta

359
00:28:06,550 --> 00:28:09,560
if the reaction but in order to get from

360
00:28:09,600 --> 00:28:14,090
the reactance state of the product state we first have to activate and physically there

361
00:28:14,090 --> 00:28:15,960
are a number of things that are going on

362
00:28:15,970 --> 00:28:20,190
and at the end of lecture australian example with reference to an automobile

363
00:28:20,190 --> 00:28:25,720
exhaust catalyst what's happening in terms of forming some kind of an activated complex which

364
00:28:25,720 --> 00:28:28,150
allows the reactance two

365
00:28:28,190 --> 00:28:33,560
we organise themselves so that they can recognise that there's an energy

366
00:28:33,560 --> 00:28:35,590
to be gained by their

367
00:28:35,630 --> 00:28:38,910
reacting with one another and how catalysts

368
00:28:38,930 --> 00:28:40,490
facilitates this

369
00:28:40,500 --> 00:28:43,620
and specifically we see that

370
00:28:43,630 --> 00:28:47,090
there's a tiny tiny fraction of the population

371
00:28:47,090 --> 00:28:52,280
distribution that has enough energy to surmount the activation barrier

372
00:28:53,310 --> 00:29:00,220
the inhibitor and the catalysts respectively either impede the reaction by raising the effective activation

373
00:29:00,220 --> 00:29:05,810
energy thereby lopping off an even greater amount of the distribution or

374
00:29:05,840 --> 00:29:07,960
facilitate the reaction by

375
00:29:07,970 --> 00:29:13,280
lowering the activation energy thereby making a greater fraction of the

376
00:29:13,340 --> 00:29:18,240
thermal distribution available for participation in the reaction

377
00:29:18,250 --> 00:29:19,650
i think that's

378
00:29:19,690 --> 00:29:23,000
that's where we left off last day

379
00:29:24,120 --> 00:29:28,120
so today what i'd like to do is to continue the whole discussion of

380
00:29:28,130 --> 00:29:31,320
what is broadly termed rate phenomenon

381
00:29:32,210 --> 00:29:33,990
chemical kinetics

382
00:29:35,590 --> 00:29:39,890
is part of a unit here three o nine one one call rate processes AR

383
00:29:39,890 --> 00:29:44,070
processes are basically processes that involve

384
00:29:44,130 --> 00:29:48,400
the rate of change with respect to time rate of change with respect to time

385
00:29:48,410 --> 00:29:53,330
and so one of course is chemical kinetics we just

386
00:29:53,350 --> 00:29:55,250
i spent some time looking at

387
00:29:55,300 --> 00:29:56,830
chemical kinetics but

388
00:29:56,930 --> 00:30:03,670
there is another area that we want to touch upon and it's area called transport

389
00:30:04,810 --> 00:30:07,770
transport phenomena

390
00:30:07,870 --> 00:30:13,120
and the reason for talking about this when we think about how we process

391
00:30:13,130 --> 00:30:16,210
materials in order to achieve solids with

392
00:30:16,260 --> 00:30:21,170
desired properties we have to think about the two basics that are required to sustain

393
00:30:21,190 --> 00:30:28,160
reactions and those are matter and energy so let's look at what's involved in each

394
00:30:28,160 --> 00:30:29,820
of these matters

395
00:30:29,940 --> 00:30:33,450
energy we have to furnish these to the reaction

396
00:30:33,460 --> 00:30:38,470
so we're going to think about energy in terms of transport phenomena that gets us

397
00:30:38,470 --> 00:30:40,860
into heat transfer

398
00:30:40,870 --> 00:30:42,980
heat transfer

399
00:30:42,990 --> 00:30:47,160
we're not going to discuss heat transfer in real what i can do everything that

400
00:30:47,160 --> 00:30:48,060
we do

401
00:30:48,060 --> 00:30:50,680
look at something that's

402
00:30:50,730 --> 00:30:53,070
representative of transport phenomena

403
00:30:53,080 --> 00:30:56,760
and if you look at mass transport

404
00:30:58,990 --> 00:31:03,930
and mathematically there are many parallels between heat transfer and mass transport

405
00:31:04,010 --> 00:31:07,750
when we talk about mass transport if we're trying to driver reaction we have to

406
00:31:07,750 --> 00:31:12,010
do two things we have to deliver reactance to the site and we have to

407
00:31:12,010 --> 00:31:14,060
remove products from the site

408
00:31:14,110 --> 00:31:19,140
if we don't do both the reaction could either star or choking good example that

409
00:31:19,140 --> 00:31:21,760
early in the semester we talked about the annenberg

410
00:31:21,810 --> 00:31:27,740
the hindenburg was a good example of a reaction that had a huge driving force

411
00:31:27,750 --> 00:31:33,620
a hydrogen explosion but they never did not explode because we had seven million cubic

412
00:31:33,620 --> 00:31:35,260
feet of

413
00:31:36,410 --> 00:31:40,630
and in order for that to react explosively we would have to deliver seven billion

414
00:31:40,630 --> 00:31:44,940
cubic feet of oxygen instantaneously and we could do that

415
00:31:44,960 --> 00:31:46,170
i couldn't do that so

416
00:31:46,180 --> 00:31:49,620
because the reaction was star thanks to

417
00:31:49,630 --> 00:31:51,510
a transport phenomena

418
00:31:52,760 --> 00:31:55,690
instead of an explosion what we got was the bird

419
00:31:55,750 --> 00:31:58,350
very different and as a result two thirds of the

420
00:31:58,360 --> 00:31:59,690
occupants of the

421
00:31:59,700 --> 00:32:01,630
of the vessel were able to

422
00:32:01,690 --> 00:32:03,750
walk off and

423
00:32:03,810 --> 00:32:09,930
not so many parishes otherwise would so transport phenomena clearly important in the dominant form

424
00:32:09,930 --> 00:32:13,560
of mass transport in solids is diffusion

425
00:32:13,570 --> 00:32:18,700
so we're talking about mass transport in solids we have to talk about diffusion that's

426
00:32:19,350 --> 00:32:21,320
we're going to do it so let's

427
00:32:21,320 --> 00:32:24,720
so that the probabilities are equal to certain things and they make an exact copy

428
00:32:24,720 --> 00:32:28,700
of this new by the maximum to be assumption you get the exact same result

429
00:32:28,720 --> 00:32:34,280
because because you have not changed the constraints on the distribution

430
00:32:42,160 --> 00:32:46,350
remember this ignore the whole first order you know issue i just think of the

431
00:32:46,350 --> 00:32:47,600
propositional case

432
00:32:47,640 --> 00:32:52,120
and that ignore even though the logical let's suppose that you're special just conjunction right

433
00:32:52,200 --> 00:32:54,800
the features that you typically have in markov networks

434
00:32:55,410 --> 00:32:57,030
what i'm saying is like

435
00:32:57,050 --> 00:33:00,780
you tell me a bunch of conjunction their probabilities right i turn the maximum entropy

436
00:33:00,780 --> 00:33:04,220
crank and they and they in a negative input distribution

437
00:33:04,280 --> 00:33:07,660
and you can give me any knowledge base that you want as long as it's

438
00:33:07,660 --> 00:33:14,240
equivalent to the first one the resulting distribution will therefore also be the same

439
00:33:14,260 --> 00:33:18,280
right i could stay the same set of constraints in many logical equivalent ways including

440
00:33:18,280 --> 00:33:22,200
redundant was just by the beginning statements as long as the constraints of the same

441
00:33:22,200 --> 00:33:25,850
right you're saying the same thing about the distribution maximum entropy which have the same

442
00:33:31,350 --> 00:33:35,970
OK so now let's look at how we can learn a markov logic networks

443
00:33:35,990 --> 00:33:41,070
so learning as as a christian mentioned is no longer just the

444
00:33:41,090 --> 00:33:44,530
data for learning is no longer just you know a single table

445
00:33:44,550 --> 00:33:46,780
it's multiple tables

446
00:33:47,530 --> 00:33:51,700
there is an is effectively a relational database

447
00:33:51,700 --> 00:33:55,820
so in statistical relational learning you can learn directly from the database with multiple relations

448
00:33:55,820 --> 00:33:59,010
without having to some russian born into one

449
00:33:59,030 --> 00:34:02,620
and here i'm going to make the closed world assumption which is the assumption that

450
00:34:02,620 --> 00:34:06,220
every atom that is not in that this is false

451
00:34:06,280 --> 00:34:09,410
this is often true it's also often not true if you don't want to make

452
00:34:09,410 --> 00:34:10,890
the assumption that you need the

453
00:34:10,910 --> 00:34:14,530
two versions of the other things that i'm going to describe here and we do

454
00:34:14,530 --> 00:34:18,010
have those impressions but i'm not going to again in the interest of time not

455
00:34:18,010 --> 00:34:19,600
going to go into that

456
00:34:19,620 --> 00:34:23,350
and of course there's two main learning tasks this learning parameters or if you want

457
00:34:23,350 --> 00:34:26,260
to think about that with the weights of the form in the MLN

458
00:34:26,300 --> 00:34:30,100
and then the structure i e discovering the formulas themselves

459
00:34:30,120 --> 00:34:33,890
and learning could be then generative discriminatively and what i'm going to do in this

460
00:34:33,890 --> 00:34:36,800
part is you know briefly go over each of these

461
00:34:36,970 --> 00:34:39,260
so learning

462
00:34:39,260 --> 00:34:42,070
let's start with generative weight learning

463
00:34:42,090 --> 00:34:46,390
the big thing that we're doing in statistical relational learning is removing the assumption of

464
00:34:46,390 --> 00:34:48,220
i data

465
00:34:48,240 --> 00:34:53,140
and this is justified because if you look at any the always the case that

466
00:34:53,200 --> 00:34:55,140
is not i i d

467
00:34:55,200 --> 00:35:00,550
in social networks people influence each other in nineteen medicine you know there's epidemics going

468
00:35:00,550 --> 00:35:04,350
on in on the web pages point to each other and that's something about the

469
00:35:04,350 --> 00:35:08,330
topics in molecular biology this metabolic networks etc etc

470
00:35:08,390 --> 00:35:10,620
so we want to remove the assumption

471
00:35:10,620 --> 00:35:12,120
but but the

472
00:35:12,120 --> 00:35:13,990
the downside of that is like

473
00:35:14,970 --> 00:35:18,370
now everything is depends on everything now life is going to be very hard right

474
00:35:18,370 --> 00:35:19,760
everything's gonna blow up

475
00:35:19,800 --> 00:35:21,930
this was our few going in

476
00:35:21,950 --> 00:35:23,850
as it turns out

477
00:35:23,870 --> 00:35:26,240
the basic learning problem

478
00:35:26,260 --> 00:35:30,450
it doesn't really change that much when you remove the idea assumptions

479
00:35:30,450 --> 00:35:33,590
so we get a lot of power and in fact something that is not be

480
00:35:34,700 --> 00:35:35,760
let's see

481
00:35:35,780 --> 00:35:39,300
why that's the case and and also where you know the additional difficulties are because

482
00:35:39,300 --> 00:35:41,260
of course they have to go somewhere

483
00:35:41,410 --> 00:35:42,800
so what we want to do

484
00:35:42,910 --> 00:35:46,820
we want to maximize the likelihood so i have a set of formulas i don't

485
00:35:46,820 --> 00:35:49,120
know their weights and i have the database

486
00:35:49,140 --> 00:35:52,570
i want to learn the weights for the forms that maximize the likelihood of the

487
00:35:52,570 --> 00:35:53,390
data is

488
00:35:53,740 --> 00:35:57,950
well first of all the good news is that this is a convex optimisation problem

489
00:35:57,970 --> 00:36:02,330
there's only one global optimum right so i don't have local optima problems the bad

490
00:36:02,330 --> 00:36:05,160
news is that there is no closed form so we do and even if we

491
00:36:05,160 --> 00:36:09,330
need to do something like within the the centre lbfgs or you know conjugate gradient

492
00:36:09,330 --> 00:36:10,600
one of those things

493
00:36:10,680 --> 00:36:15,530
another great has very intuitive form which is very similar to the form that it

494
00:36:15,530 --> 00:36:17,800
has four learning markov random fields

495
00:36:17,850 --> 00:36:19,260
and it's as follows

496
00:36:19,280 --> 00:36:24,780
the derivative the partial derivative of the log likelihood with respect to weight

497
00:36:24,800 --> 00:36:26,640
it's just the difference

498
00:36:26,660 --> 00:36:31,320
between the number of true groundings of the corresponding clause in the data that's the

499
00:36:31,320 --> 00:36:32,930
sufficient statistics

500
00:36:32,950 --> 00:36:37,470
and the expected number of true groundings according to to the model

501
00:36:37,470 --> 00:36:41,780
so what happens is that if my model is predicting that this clause is true

502
00:36:41,780 --> 00:36:45,300
less often than it really is its weight needs to go up

503
00:36:45,350 --> 00:36:48,120
if it's pretty and that's more often than it really is the way it needs

504
00:36:48,120 --> 00:36:51,700
to go down and once they all line up with that we've reached the maximum

505
00:36:51,700 --> 00:36:53,010
likelihood solution

506
00:36:54,300 --> 00:36:56,450
and notice that there's a there's this

507
00:36:56,470 --> 00:36:59,200
a small but very large difference between this

508
00:36:59,220 --> 00:37:00,890
and i i d case

509
00:37:00,910 --> 00:37:04,260
in the in the in that i think is what i did was i counted

510
00:37:04,260 --> 00:37:08,370
over my idea over my idea instances

511
00:37:08,390 --> 00:37:13,820
what's happening here that's difference is that i'm counting over groundings of cause

512
00:37:13,850 --> 00:37:18,050
which means that in the ideal case i have only one way of tying parameters

513
00:37:18,050 --> 00:37:18,850
one way

514
00:37:18,850 --> 00:37:21,350
across the instances now

515
00:37:21,370 --> 00:37:25,120
i can type parameters anyway that i want to my heart's content

516
00:37:25,140 --> 00:37:28,970
i wanted to find himself tied parameters i just find the corresponding one

517
00:37:29,070 --> 00:37:32,620
so even though the math is still basically the same

518
00:37:32,640 --> 00:37:36,760
this is much more powerful because now i have complete freedom to type things

519
00:37:36,780 --> 00:37:40,530
you can think of the bias right the learning bias

520
00:37:40,550 --> 00:37:45,010
in learning in markov logic network is that it is is the parameter tying happens

521
00:37:45,010 --> 00:37:48,160
at the runnings of this was in the grounds of that

522
00:37:48,180 --> 00:37:52,200
now of course there's the catch there has to be kept somewhere the catch is

523
00:37:52,200 --> 00:37:54,950
that this is very hard to compute

524
00:37:54,990 --> 00:37:59,100
now this was you know these expectations right requires inference

525
00:37:59,120 --> 00:38:03,390
so those are already intractable in the propositional case but now you know this intractable

526
00:38:03,390 --> 00:38:05,850
but they're going to be over an even bigger problem

527
00:38:05,870 --> 00:38:10,220
so now i'm doing gradient descent with inference in each of the steps this is

528
00:38:10,220 --> 00:38:12,850
probably going to be very slow

529
00:38:12,910 --> 00:38:15,930
so the question is can i do something else

530
00:38:15,990 --> 00:38:19,820
well when we have a problem that's too hard to follow a good strategy is

531
00:38:19,820 --> 00:38:22,240
to change the subject

532
00:38:22,280 --> 00:38:26,660
so one way to change the subject here is to instead of optimizing the likelihood

533
00:38:26,680 --> 00:38:30,950
optimise what's called the pseudo likelihood right which is as the name implies a fake

534
00:38:32,320 --> 00:38:36,200
the so called is something that's designed to behave as much like the likelihood as

535
00:38:36,200 --> 00:38:39,180
possible while being tractable

536
00:38:39,200 --> 00:38:41,070
so this is the likelihood

537
00:38:41,090 --> 00:38:43,070
of state is just the

538
00:38:43,090 --> 00:38:45,320
product over all variables

539
00:38:45,350 --> 00:38:48,760
of the conditional probability of variable given its neighbors

540
00:38:48,760 --> 00:38:50,720
in the state in the data

541
00:38:50,740 --> 00:38:53,780
i e given the state of its markov blanket

542
00:38:53,800 --> 00:38:56,740
so as you see this requires no inference

543
00:38:56,830 --> 00:39:01,490
and if you combine this with you know fast in optimizer like lbfgs the learning

544
00:39:01,490 --> 00:39:05,430
is actually quite efficient you can then you know large complicated melons in a matter

545
00:39:05,430 --> 00:39:08,220
of minutes

546
00:39:08,220 --> 00:39:12,330
of course the likelihood is not a new idea for something it works very well

547
00:39:12,330 --> 00:39:13,030
okay so

548
00:39:13,660 --> 00:39:20,280
the question was how bad way is it to just sample randomly from the parameters end compute the likelihood

549
00:39:21,840 --> 00:39:24,550
it's very good for small datasets

550
00:39:25,860 --> 00:39:29,930
that's a great way of approximating this but what happens is

551
00:39:31,150 --> 00:39:32,410
this likelihood

552
00:39:34,450 --> 00:39:35,800
under i models

553
00:39:36,660 --> 00:39:38,590
it gets exponentially

554
00:39:39,000 --> 00:39:43,200
shorter with more datasets around some parameter values

555
00:39:44,070 --> 00:39:47,450
so sampling from your prior and trying to evaluate the likelihood

556
00:39:50,600 --> 00:39:53,980
is gonna be like looking for needles in the haystack as they say

557
00:39:54,450 --> 00:39:58,630
basically your priors putting its mass all over the place where you're likely it is

558
00:39:58,730 --> 00:40:03,000
only sharply peaked at a few points and so that's sampler will take a very

559
00:40:03,000 --> 00:40:05,100
long time is that it's unbiased

560
00:40:06,110 --> 00:40:09,020
but it'll take a very long time to get the right answer yes

561
00:40:10,420 --> 00:40:11,110
any questions

562
00:40:14,270 --> 00:40:14,550
all right

563
00:40:15,300 --> 00:40:17,840
so let's look in particular at this

564
00:40:19,070 --> 00:40:20,900
this bayesian model comparison

565
00:40:22,750 --> 00:40:24,260
four the polynomials case

566
00:40:26,590 --> 00:40:29,190
i shown you these same eight data points again

567
00:40:29,800 --> 00:40:32,590
well the zero to the seventh order polynomials

568
00:40:33,270 --> 00:40:33,640
the blue

569
00:40:34,310 --> 00:40:36,870
remember is the maximum likelihood polynomial

570
00:40:37,570 --> 00:40:43,840
the green lines are samples from the e posterior polynomials

571
00:40:44,470 --> 00:40:45,410
twenty samples

572
00:40:46,110 --> 00:40:47,130
condition on the data

573
00:40:48,350 --> 00:40:49,570
under certain priors

574
00:40:53,640 --> 00:40:54,830
to be precise

575
00:40:55,490 --> 00:40:57,750
i put calcium priors on thee

576
00:40:59,990 --> 00:41:01,140
o my polynomial

577
00:41:01,560 --> 00:41:02,040
with some

578
00:41:02,450 --> 00:41:03,250
variance which

579
00:41:03,760 --> 00:41:04,680
matters but

580
00:41:05,410 --> 00:41:07,240
you know we can discuss how matters

581
00:41:09,800 --> 00:41:11,910
an inverse gamma prior on the

582
00:41:12,740 --> 00:41:14,790
noise variance sigma squared

583
00:41:18,840 --> 00:41:20,110
now with those priors

584
00:41:20,680 --> 00:41:22,920
i can sample exactly from

585
00:41:23,540 --> 00:41:28,390
the posterior over the polynomials which are drawn in green here and i can also

586
00:41:28,390 --> 00:41:31,310
compute the marginal likelihood exactly and analytically

587
00:41:33,610 --> 00:41:37,330
and here is what we get for these eight data points and these particular priors

588
00:41:40,440 --> 00:41:43,790
my marginal likelihood renormalize this scale here

589
00:41:44,230 --> 00:41:46,350
but my marginal likelihood essentially is putting

590
00:41:47,840 --> 00:41:50,220
all of its mass on

591
00:41:51,350 --> 00:41:53,890
zero zeroth first second third order

592
00:41:54,400 --> 00:41:58,740
and the masses are getting smaller and smaller fourth fifth sixth and seventh

593
00:41:59,240 --> 00:42:00,710
you can even see them on the scale

594
00:42:06,500 --> 00:42:08,960
and if you look at this intuitively makes sense

595
00:42:11,480 --> 00:42:13,390
given the data that we've observed here

596
00:42:15,160 --> 00:42:18,220
the most consistent hypothesis is that it's a quadratic

597
00:42:19,170 --> 00:42:20,430
but it constant

598
00:42:20,900 --> 00:42:24,950
polynomial is also quite reasonable because it could be a lot more noise for example

599
00:42:26,290 --> 00:42:27,370
any questions about this

600
00:42:29,220 --> 00:42:30,850
i can run a demo people want

601
00:42:33,100 --> 00:42:38,910
thank you

602
00:42:40,410 --> 00:42:42,380
okay here is another example

603
00:42:44,700 --> 00:42:45,740
o another

604
00:42:46,240 --> 00:42:47,170
eight data points

605
00:42:48,660 --> 00:42:50,270
anne again thee

606
00:42:51,250 --> 00:42:52,270
the best fits

607
00:42:53,230 --> 00:42:54,440
so it's a fifth-order

608
00:42:55,090 --> 00:42:55,790
i am the

609
00:42:57,260 --> 00:43:00,670
in this case these quadratic comes out overwhelmingly

610
00:43:05,390 --> 00:43:06,170
any questions that

611
00:43:08,350 --> 00:43:10,210
maybe just to twenty data points

612
00:43:14,660 --> 00:43:16,090
what do you think it was generated from

613
00:43:18,270 --> 00:43:19,550
it was actually quadratic

614
00:43:26,010 --> 00:43:26,790
happens if i do

615
00:43:28,820 --> 00:43:30,150
right this case

616
00:43:35,860 --> 00:43:37,460
so this with three data points

617
00:43:38,590 --> 00:43:39,470
yeah question

618
00:43:43,720 --> 00:43:47,560
yes i i have to be careful with my words here

619
00:43:48,290 --> 00:43:49,370
we're not searching

620
00:43:50,590 --> 00:43:53,120
anyone could be searching for the best model because

621
00:43:53,550 --> 00:43:55,460
maybe we are working with the client

622
00:43:55,940 --> 00:44:00,270
and the choir was see what model when they were doing scientific data analysis

623
00:44:00,720 --> 00:44:06,320
and we wanna concluded in the paper that one model is overwhelmingly more probable than any other models

624
00:44:07,140 --> 00:44:09,230
of course if we want to do prediction

625
00:44:09,800 --> 00:44:14,530
what we need to do is average the predictions over the different models weighted by their posterior as

626
00:44:18,000 --> 00:44:22,750
it's you know the better the same bayesian model comparison the bayesian model selection

627
00:44:23,200 --> 00:44:28,800
because model selection basically implies it should apply some cost function forward you know which

628
00:44:29,630 --> 00:44:29,840
you know

629
00:44:30,490 --> 00:44:32,010
what would you prefer to select

630
00:44:33,610 --> 00:44:35,500
how do i sample these green things

631
00:44:40,860 --> 00:44:43,540
this relates to what i'm gonna talk about very soon

632
00:44:45,680 --> 00:44:46,520
in this case

633
00:44:47,230 --> 00:44:48,200
these integrals

634
00:44:50,130 --> 00:44:52,980
because i chose what's called conjugate priors

635
00:44:54,060 --> 00:44:58,880
the posterior over the parameters is also conjugate which means i can actually analytically

636
00:44:59,590 --> 00:45:04,510
represent the posterior r it turns out that the posterior over the coefficients is galcians

637
00:45:04,910 --> 00:45:05,780
so i just have this

638
00:45:06,280 --> 00:45:09,400
draw a sample from a multivariate calcium

639
00:45:10,610 --> 00:45:14,210
and then draw the corresponding polynomial with those coefficients

640
00:45:14,590 --> 00:45:15,120
that's what i do

641
00:45:25,950 --> 00:45:30,850
so what about choosing priors there are different schools farhat choose priors

642
00:45:33,680 --> 00:45:35,540
there's a whole subfield of

643
00:45:38,680 --> 00:45:40,580
is called objective basis

644
00:45:41,260 --> 00:45:43,720
where they tried to come up with objective priors

645
00:45:44,230 --> 00:45:47,410
and these tend to be noninformative priors that attempt to capture

646
00:45:47,920 --> 00:45:50,040
some form of complete ignorance

647
00:45:51,280 --> 00:45:56,000
and have good frequentist properties so for example you mention jeffreys priors that's something that's

648
00:45:56,000 --> 00:45:58,890
a very classical objective priors it's been used

649
00:45:59,480 --> 00:46:00,140
quite frequently

650
00:46:02,630 --> 00:46:04,130
subjective priors

651
00:46:05,990 --> 00:46:07,550
say that's all

652
00:46:09,460 --> 00:46:13,380
we really wanted to they are priors capture your beliefs as well as possible

653
00:46:14,210 --> 00:46:16,880
and there are subjective but not arbitrary so different

654
00:46:17,500 --> 00:46:19,530
people could have different priors coming into

655
00:46:20,120 --> 00:46:22,030
particular modeling domain

656
00:46:23,430 --> 00:46:29,620
hand is not arbitrary because your prior represents your beliefs he beliefs are not arbitrary

657
00:46:33,230 --> 00:46:37,040
and nothing is arbitrary but what you do afterwards because once you specify a prior

658
00:46:37,560 --> 00:46:40,790
and you have your data and everything else follows from from the rules

659
00:46:42,910 --> 00:46:47,830
hierarchical priors on these are not mutually exclusive i mean i can have hierarchical partners

660
00:46:47,830 --> 00:46:49,970
around in the sixties

661
00:46:51,250 --> 00:46:53,840
also pearl i guess in his book in the eighties in

662
00:46:53,890 --> 00:46:54,560
they i

663
00:46:54,580 --> 00:47:01,220
i spoke about the use recent passing of message passing on graphs with cycles

664
00:47:01,250 --> 00:47:02,970
so your question was

665
00:47:02,980 --> 00:47:06,890
if you have cycles how does the ordering of the messages affect things

666
00:47:08,330 --> 00:47:12,320
cycles are where there is no clear answer ordering can affect whether or not it

667
00:47:13,620 --> 00:47:17,630
because there can be multiple fixed points the ordering can affect which fixed point you

668
00:47:17,630 --> 00:47:18,700
go to

669
00:47:18,740 --> 00:47:24,140
there's been a fair bit of work there are some citations in the in the

670
00:47:24,140 --> 00:47:27,280
book that i give you a fair bit of work on trying to make good

671
00:47:27,280 --> 00:47:30,920
schedule means of belief propagation on graphs with cycles

672
00:47:30,970 --> 00:47:35,200
some of my own work studied schedules in which you choose the tree of the

673
00:47:35,200 --> 00:47:38,500
graph and you pass the messages along the tree and then you choose another tree

674
00:47:38,500 --> 00:47:42,130
and you do that that can be better in some settings

675
00:47:42,560 --> 00:47:47,450
some other people have studied what's called adaptive message passing where you you sort of

676
00:47:47,450 --> 00:47:51,280
look at every note how much of the messages fluctuating in that region

677
00:47:51,280 --> 00:47:55,080
they're not changing much then i probably don't need to update because it's you know

678
00:47:55,080 --> 00:47:58,230
it's close to being happy was if they're changing like maybe i should pay more

679
00:47:58,230 --> 00:48:04,000
attention so that's really an ongoing area of research there isn't a clear answer but

680
00:48:04,000 --> 00:48:08,090
i think there's pretty good heuristics at this point for that

681
00:48:08,110 --> 00:48:11,250
any other questions

682
00:48:25,860 --> 00:48:30,120
OK so the question is why

683
00:48:30,140 --> 00:48:34,450
these algorithms are exact for trees while the sunday to become inexact when you have

684
00:48:36,300 --> 00:48:42,120
there's different ways i can explain this

685
00:48:42,160 --> 00:48:44,750
one way is that

686
00:48:44,750 --> 00:48:47,060
when i went to this derivation

687
00:48:47,210 --> 00:48:50,850
right why do you end up taking the product of messages here

688
00:48:50,890 --> 00:48:53,640
the reason the product is the right thing to do if you look at the

689
00:48:53,640 --> 00:48:56,110
algebra is because when you have a graph

690
00:48:56,230 --> 00:48:58,000
the does not have cycles

691
00:48:58,010 --> 00:49:01,060
notes four and five there

692
00:49:01,120 --> 00:49:04,790
you can sort of imagine subtrees growing off of each of them

693
00:49:04,790 --> 00:49:07,730
right there might be part of a bigger graph

694
00:49:07,740 --> 00:49:13,040
and if there's no cycle then i know that nodes four and five are

695
00:49:13,100 --> 00:49:17,250
can be thought of as the roots of subtrees and there is no interaction between

696
00:49:17,250 --> 00:49:20,750
those subtrees rooted at four and five

697
00:49:20,820 --> 00:49:25,520
so it's because of that fact because there are no cycles there is no interaction

698
00:49:25,520 --> 00:49:27,270
between the subterms

699
00:49:27,320 --> 00:49:30,270
it's because of the that fact that the correct thing to do is to take

700
00:49:30,270 --> 00:49:32,130
the product

701
00:49:32,180 --> 00:49:36,900
if you had cycles you see there is an additional term that you're neglecting

702
00:49:36,950 --> 00:49:41,000
that there be some interaction if there is an edge between four and five were

703
00:49:41,000 --> 00:49:44,360
further down in the subtrees and that would lead to errors

704
00:49:44,380 --> 00:49:51,950
another intuition for actually member yesterday we spoke about conditional independence properties

705
00:49:51,990 --> 00:49:56,660
another way of thinking about these actually from the conditional independence point of view is

706
00:49:59,520 --> 00:50:04,560
the interpretation of this message is that node three is summarizing to know too

707
00:50:04,580 --> 00:50:08,670
everything no two needs to know about this part of the graph

708
00:50:08,860 --> 00:50:13,850
now we know from conditional independence properties that node three is the cutset if i

709
00:50:13,850 --> 00:50:17,050
cut node three out it will break two

710
00:50:17,090 --> 00:50:19,460
and one of from three

711
00:50:19,460 --> 00:50:23,180
no break four and five often three of great in this case would break the

712
00:50:23,180 --> 00:50:25,620
graph into three parts

713
00:50:25,640 --> 00:50:29,650
and when you have conditional independence so because you have the cut said it means

714
00:50:29,650 --> 00:50:30,980
that conditioned on three

715
00:50:31,410 --> 00:50:37,920
these two contributions are independent conditionally of each other so conditional independence when you

716
00:50:37,930 --> 00:50:41,100
i have independent sort of means you should multiply things

717
00:50:41,150 --> 00:50:45,780
and that's that's really and another sort of interpretation why the product happens the product

718
00:50:45,780 --> 00:50:55,460
happens because of statistical conditional independence to different way of deriving the algorithm

719
00:50:55,460 --> 00:50:57,910
any other questions

720
00:50:58,930 --> 00:51:01,140
i feel like i'm a tennis match

721
00:51:01,150 --> 00:51:11,970
o and i know there are results for so you asking for message passing on

722
00:51:11,970 --> 00:51:14,110
graphs with cycles

723
00:51:14,160 --> 00:51:17,640
there's only a line of work i think the first person who got some understanding

724
00:51:17,640 --> 00:51:22,180
exactly on the single cycle graphs was you your y in i think nineteen nine

725
00:51:22,520 --> 00:51:26,740
gavin hormone also in his phd thesis at caltech around the same time had some

726
00:51:26,740 --> 00:51:29,920
related results on single cycle graphs

727
00:51:29,970 --> 00:51:36,370
there's a lot of work on graphs now that are what are called locally tree

728
00:51:37,800 --> 00:51:42,980
locally treelike means that if it's a it's graph that is not a tree it

729
00:51:42,980 --> 00:51:44,050
has cycles

730
00:51:44,110 --> 00:51:47,730
that means that if you're in and then you start wondering from one node and

731
00:51:47,730 --> 00:51:52,370
you wonder along edges you'd wonder long time before you come back you find the

732
00:51:52,370 --> 00:51:54,660
cycle and come back to yourself

733
00:51:54,670 --> 00:51:58,810
so if if you build certain classes of random graphs you can show that there

734
00:51:58,810 --> 00:52:00,530
there tree like locally

735
00:52:00,540 --> 00:52:03,830
and they have girth girth is the length of the shortest cycle in a graph

736
00:52:03,830 --> 00:52:06,580
the graph logarithmic in the graph size

737
00:52:06,640 --> 00:52:12,710
for those graphs you can say a lot actually and that example i showed yesterday

738
00:52:12,710 --> 00:52:16,390
with the code it was based on the graph that was locally treelike

739
00:52:16,470 --> 00:52:22,320
and that's part of the reason why message passing work so well is because the

740
00:52:22,320 --> 00:52:26,820
graphs are not trees but there's somehow up to some approximation they look like trees

741
00:52:30,540 --> 00:52:36,900
yes so for instance the canonical examples if you do a grid or lattice which

742
00:52:36,900 --> 00:52:40,720
i'm going to start with the left off

743
00:52:40,730 --> 00:52:47,780
yesterday i hope somebody remembers what we talking about yesterday just in case you don't

744
00:52:47,790 --> 00:52:52,000
we were going to do we basically has to be added

745
00:52:52,020 --> 00:52:58,270
a very brief introduction to planning and then look a variety of our tracks that

746
00:52:58,510 --> 00:52:59,710
the work

747
00:52:59,850 --> 00:53:02,380
learning and planning as they can

748
00:53:02,390 --> 00:53:10,560
and in particular the case of one which is talking about learning search control and

749
00:53:10,580 --> 00:53:16,260
we started looking at the first look at the fact that knowledge based learning

750
00:53:16,280 --> 00:53:19,520
planets with humans like the knowledge

751
00:53:19,540 --> 00:53:25,360
do extremely well are compared to even the current best state of the art domain

752
00:53:25,360 --> 00:53:28,300
independent planets which only have the main physics

753
00:53:28,640 --> 00:53:33,090
so that sort of open source the possibility that are

754
00:53:33,110 --> 00:53:35,590
learning techniques can be used to

755
00:53:35,600 --> 00:53:37,670
improve that

756
00:53:37,690 --> 00:53:39,720
the planning performance

757
00:53:39,760 --> 00:53:43,080
so we look at the kinds of knowledge that

758
00:53:43,130 --> 00:53:48,730
people have been handed down and then we started looking at the whole some of

759
00:53:48,730 --> 00:53:53,130
what the sorts of that knowledge that actually can be automatically learned right now another

760
00:53:53,130 --> 00:53:54,830
mentioned yesterday

761
00:53:54,850 --> 00:54:00,790
not everything that is actually written by humans learn about so there are certain types

762
00:54:00,790 --> 00:54:05,430
of knowledge in fact such as hierarchical task networks there not yet there are no

763
00:54:05,430 --> 00:54:07,170
i think you them right now

764
00:54:07,270 --> 00:54:13,830
effective techniques for learning experience however i can at least tell you the kinds of

765
00:54:13,830 --> 00:54:17,180
things that we already know how to to do it and

766
00:54:17,200 --> 00:54:21,160
and then the rest will be open so you get to do it and then

767
00:54:21,160 --> 00:54:24,330
we will of course after that is all we look at learning the domain knowledge

768
00:54:24,330 --> 00:54:29,250
itself if you don't have the in physics information how you learned and then and

769
00:54:30,870 --> 00:54:35,780
situations where you have all of the fired

770
00:54:35,780 --> 00:54:39,950
basically partial domain knowledge and so you're trying to learn what the domain knowledge

771
00:54:41,200 --> 00:54:43,270
six hundred at the same time

772
00:54:43,290 --> 00:54:46,650
and that's pretty much open right now

773
00:54:47,350 --> 00:54:53,080
so we we were staring at this and in fact i just wanted to this

774
00:54:53,080 --> 00:54:59,290
was the learning from the new using explanation based learning techniques we just started talking

775
00:54:59,290 --> 00:55:04,580
about the last time and i just got this hundred replacement thing on my main

776
00:55:04,870 --> 00:55:06,480
put in the day

777
00:55:06,490 --> 00:55:10,150
do that what it does mean that you go through all of two minutes later

778
00:55:10,150 --> 00:55:15,570
by the six hundred six times in the time and six and five times at

779
00:55:15,570 --> 00:55:20,300
least one of the things went wrong and fix this kind of work so i

780
00:55:20,300 --> 00:55:23,460
don't know whether they actually learn anything from the value but i thought it was

781
00:55:23,550 --> 00:55:24,300
good thing

782
00:55:25,230 --> 00:55:30,800
the idea here is that connects so in this case we were talking about a

783
00:55:33,400 --> 00:55:36,580
search control of a third in

784
00:55:36,580 --> 00:55:42,540
i'm planning problem may it turned out that this particular branch created and what you

785
00:55:42,540 --> 00:55:46,890
want to learn is some sort of the tries rule which tells the learner which

786
00:55:46,890 --> 00:55:50,520
does plan that don't ever get into this branch again

787
00:55:50,540 --> 00:55:52,380
OK and we were talking about

788
00:55:52,390 --> 00:55:57,630
this point that if you can somehow explain why this guy

789
00:55:57,800 --> 00:56:03,480
and if the explanation is smaller than the entire line here which is very important

790
00:56:03,480 --> 00:56:08,210
in fact you've explanation of you feel you is yourself if your explanation of a

791
00:56:08,210 --> 00:56:13,760
plan plan uris on the constantinople and then that he doesn't work you can OK

792
00:56:13,830 --> 00:56:16,050
if there is a subset in this

793
00:56:16,070 --> 00:56:18,860
plan constraints that can actually be

794
00:56:18,920 --> 00:56:23,300
used to explain why the plan failed then you can sort of see them through

795
00:56:23,300 --> 00:56:24,420
the thirty three

796
00:56:24,450 --> 00:56:29,890
looking at so for example in this case i this branch failed because this is

797
00:56:29,890 --> 00:56:36,070
actually a partial order plan scenario this plan failed because you are trying to make

798
00:56:36,390 --> 00:56:40,890
a step zero come before one but step zero except one valid is supposed to

799
00:56:40,890 --> 00:56:42,230
come before zero

800
00:56:42,260 --> 00:56:47,520
and it turns out those constraints while those constraints inconsistent and so that's the smallest

801
00:56:47,520 --> 00:56:53,200
explanation of value for this branch with this this plan has been extended and there

802
00:56:53,270 --> 00:56:57,580
similar failure commission there what you're doing right now

803
00:56:57,590 --> 00:57:01,120
his explanation for failure back up the tree

804
00:57:01,210 --> 00:57:07,450
trying to figure out who is to blame and under what conditions should do not

805
00:57:07,450 --> 00:57:11,370
how gotten into the ground to begin so this is an example

806
00:57:11,380 --> 00:57:17,490
learning search control using this kind of analysis for example as i mentioned yesterday

807
00:57:17,500 --> 00:57:20,350
a rule that you could learn here

808
00:57:20,370 --> 00:57:24,520
he is in fact the sort of like you learn here

809
00:57:24,530 --> 00:57:26,130
i would say that

810
00:57:26,140 --> 00:57:31,390
something like this so that if you are trying to achieve an open admission policy

811
00:57:31,390 --> 00:57:32,660
and when it

812
00:57:32,680 --> 00:57:38,070
goes away from you

813
00:57:40,800 --> 00:57:46,550
rolling objects that's roll something down the hill

814
00:57:46,660 --> 00:57:48,950
classic problem

815
00:57:48,970 --> 00:57:51,180
very often you see that

816
00:57:51,180 --> 00:57:57,280
on exams

817
00:57:57,280 --> 00:57:58,950
whitaker ball

818
00:57:58,970 --> 00:58:00,530
and we roll it down

819
00:58:00,570 --> 00:58:03,570
an inclined

820
00:58:03,620 --> 00:58:05,470
this inclined

821
00:58:05,510 --> 00:58:09,240
the angle is better

822
00:58:09,280 --> 00:58:10,390
he was the ball

823
00:58:10,390 --> 00:58:11,820
the sphere

824
00:58:11,870 --> 00:58:14,120
it's a solid sphere has mass and

825
00:58:14,180 --> 00:58:15,800
israelis are

826
00:58:15,850 --> 00:58:17,260
and in the solid

827
00:58:17,330 --> 00:58:20,240
this is the centre

828
00:58:20,280 --> 00:58:25,530
of the ball i call this morning

829
00:58:25,640 --> 00:58:29,120
i put all the forces on that i can think of

830
00:58:29,240 --> 00:58:33,910
is an engineer

831
00:58:34,050 --> 00:58:35,640
at this point p

832
00:58:35,660 --> 00:58:37,530
normal force

833
00:58:37,530 --> 00:58:39,010
on the surface

834
00:58:39,030 --> 00:58:42,090
and there is friction

835
00:58:42,140 --> 00:58:44,640
this is the frictional force the sum of ff

836
00:58:44,680 --> 00:58:47,720
and the normal force is what we call the contact force

837
00:58:47,820 --> 00:58:48,950
that is the force

838
00:58:48,990 --> 00:58:49,780
which which

839
00:58:49,800 --> 00:58:54,050
inclined pushes back into the ball we normally decompose that

840
00:58:54,100 --> 00:58:58,260
in the direction along the slope of the

841
00:58:58,300 --> 00:59:00,010
these are the only three forces

842
00:59:00,030 --> 00:59:01,390
that is objects

843
00:59:01,470 --> 00:59:02,800
is experiencing

844
00:59:02,800 --> 00:59:06,430
and i can ask you know what is a acceleration

845
00:59:07,530 --> 00:59:08,820
goes down the slope

846
00:59:08,830 --> 00:59:13,070
i can ask you what is the frictional force

847
00:59:13,120 --> 00:59:17,100
i ask you it under the conditions of neural

848
00:59:17,220 --> 00:59:20,990
well this pure only

849
00:59:21,070 --> 00:59:24,090
it means that if this conference here

850
00:59:24,090 --> 00:59:25,780
more one centimetre

851
00:59:25,800 --> 00:59:29,640
at the centre of mass has also moved one step

852
00:59:29,640 --> 00:59:31,620
so it means that the velocity

853
00:59:31,620 --> 00:59:33,800
of the centre

854
00:59:33,890 --> 00:59:36,240
so this velocity

855
00:59:36,390 --> 00:59:37,760
of see

856
00:59:37,760 --> 00:59:39,530
all moments in time

857
00:59:39,570 --> 00:59:41,700
will be omega are

858
00:59:43,510 --> 00:59:45,430
the angular velocity

859
00:59:45,470 --> 00:59:47,120
which is changing with time

860
00:59:47,200 --> 00:59:50,780
that is the condition of pure

861
00:59:50,890 --> 00:59:52,910
at all moments in time

862
00:59:52,950 --> 00:59:54,620
if i have a rotating

863
00:59:54,680 --> 00:59:56,430
sphere or cylinder

864
00:59:57,660 --> 00:59:59,910
will the circle inferential

865
00:59:59,990 --> 01:00:02,010
speed the omega are

866
01:00:02,010 --> 01:00:05,930
it could be standing still and is rotating like this sleeping like hell not moving

867
01:00:07,010 --> 01:00:10,430
then the circum ferentials b is always omega are

868
01:00:11,370 --> 01:00:15,090
if the centre also move was that seems to be only then do we call

869
01:00:15,090 --> 01:00:16,620
it euro

870
01:00:16,680 --> 01:00:18,470
if you take the derivative of this

871
01:00:18,470 --> 01:00:21,990
you get a equals only got times are

872
01:00:22,030 --> 01:00:23,720
the course of our

873
01:00:23,780 --> 01:00:25,620
also being angular

874
01:00:28,300 --> 01:00:30,140
all right we're going to write down

875
01:00:31,510 --> 01:00:34,140
talk relative to that point c

876
01:00:34,180 --> 01:00:35,620
if i take points c

877
01:00:35,680 --> 01:00:37,760
i lose the force and g

878
01:00:37,820 --> 01:00:40,680
lose the force and there's only one force

879
01:00:40,700 --> 01:00:42,200
that i have to do is

880
01:00:43,990 --> 01:00:45,830
relative to point c

881
01:00:45,890 --> 01:00:49,370
the fact that the talk is in the blackboard is of no concern to me

882
01:00:49,430 --> 01:00:51,200
i just want to know the magnitude

883
01:00:51,260 --> 01:00:52,300
it is

884
01:00:52,350 --> 01:00:54,280
times f

885
01:00:54,320 --> 01:00:56,390
are times the friction before

886
01:00:56,470 --> 01:01:02,100
that must be the moment of inertia about point about the points in times of

887
01:01:02,160 --> 01:01:04,200
we've seen is now twice already

888
01:01:04,260 --> 01:01:07,220
and so since all is a defined by our

889
01:01:07,260 --> 01:01:08,720
it is i seen

890
01:01:10,050 --> 01:01:11,970
divided by r

891
01:01:12,010 --> 01:01:13,680
this is one equation

892
01:01:13,800 --> 01:01:17,100
and i have a as unknown

893
01:01:17,100 --> 01:01:19,070
and i have for now

894
01:01:19,070 --> 01:01:22,350
there's not enough anymore

895
01:01:22,430 --> 01:01:25,010
newton's second law always holds

896
01:01:25,010 --> 01:01:26,680
the centre of mass

897
01:01:26,680 --> 01:01:28,370
for the centre of mass

898
01:01:28,370 --> 01:01:31,740
must always holds as equals and made

899
01:01:31,780 --> 01:01:34,910
all i don't know where got the centre of mass and i say

900
01:01:34,990 --> 01:01:39,620
what the forces which are acting down the slope i'm not interested in the ones

901
01:01:39,620 --> 01:01:43,140
perpendicular to the the slope only to ones down the slope

902
01:01:43,160 --> 01:01:47,490
that must be and made a non-negotiable because that is the quality characteristic of the

903
01:01:47,490 --> 01:01:49,200
centre of mass

904
01:01:50,200 --> 01:01:51,720
this one is uphill

905
01:01:51,740 --> 01:01:55,370
and the only one that is down here is the component of g

906
01:01:55,390 --> 01:01:59,070
along the slope which is angie sign beta

907
01:01:59,090 --> 01:02:01,180
i call that the plaster action

908
01:02:01,200 --> 01:02:02,910
so i get ng

909
01:02:02,930 --> 01:02:04,870
sign beta

910
01:02:06,300 --> 01:02:07,740
that frictional force

911
01:02:12,410 --> 01:02:14,280
times age

912
01:02:14,280 --> 01:02:19,290
and in in in terms of goals involves instead of looking at the original models

913
01:02:19,290 --> 01:02:25,450
so examples are various kinds of neural networks actually converge to get across the limit

914
01:02:25,490 --> 01:02:30,640
where the number of hidden units increases basically because you do a similar construction

915
01:02:34,330 --> 01:02:39,660
the relevance vector machine is a special kind of gas process

916
01:02:39,960 --> 01:02:43,900
support vector machines are very closely related to golf courses and there are a bunch

917
01:02:43,900 --> 01:02:44,690
of other

918
01:02:44,710 --> 01:02:52,530
examples of this is just one example

919
01:03:01,310 --> 01:03:09,030
so the question is what about the but i showed you any non stationary

920
01:03:10,030 --> 01:03:15,410
so i guess this one is actually started begin nonstationary because it has

921
01:03:15,590 --> 01:03:18,530
of the behaviour of the and thought

922
01:03:18,770 --> 01:03:22,370
so things like a neural network kernel is typical

923
01:03:22,390 --> 01:03:28,660
example of a non stationary one neural networks will have the tendency that the model

924
01:03:28,660 --> 01:03:33,190
the model to have a lot of action in the central region and then the

925
01:03:33,230 --> 01:03:36,620
the sample function will tend to converge to fixed functions

926
01:03:37,150 --> 01:03:40,240
as you move away from from the origin

927
01:03:40,260 --> 01:03:44,880
these are the squared exponential kernel for template reverts back to zero

928
01:03:44,900 --> 01:03:46,470
when you go far away from

929
01:03:46,480 --> 01:03:50,260
from the data that you can but that's just the consequence of this that you

930
01:03:50,260 --> 01:03:53,670
can have a look at the properties so they can converge to

931
01:03:53,680 --> 01:03:58,770
six values when you move or this is something which is which is useful for

932
01:03:58,770 --> 01:04:05,530
many modelling i think the squared exponential or the newer network rail actually

933
01:04:05,540 --> 01:04:07,430
shown here

934
01:04:07,760 --> 01:04:11,450
this quite interesting model to use

935
01:04:11,470 --> 01:04:15,330
and it's a lot easier to use the counting process with this with the network

936
01:04:15,330 --> 01:04:22,250
on is that training on

937
01:04:22,270 --> 01:04:24,290
OK so here is

938
01:04:24,300 --> 01:04:25,590
here's another example

939
01:04:26,120 --> 01:04:27,790
which is

940
01:04:27,840 --> 01:04:33,320
thought i would be appropriate i also promise somebody to to to talk about this

941
01:04:33,370 --> 01:04:36,920
since i also talked about bayesian inference so

942
01:04:38,170 --> 01:04:41,790
it's going to be an example about anything about

943
01:04:41,800 --> 01:04:45,740
the the likelihood principle like principle says something about something like

944
01:04:45,750 --> 01:04:46,850
that the

945
01:04:46,980 --> 01:04:51,430
the result of the inference bayesian inference is only allowed to depend on the likelihood

946
01:04:51,730 --> 01:04:53,690
of the data that you actually observed

947
01:04:53,710 --> 01:04:58,360
and your observations and it depends on anything else then there's something wrong with your

948
01:04:58,360 --> 01:05:01,130
procedure that that's not allowed

949
01:05:01,150 --> 01:05:05,410
so the question was you know do any procedures we depend on anything else is

950
01:05:05,410 --> 01:05:06,890
not completely safe

951
01:05:07,370 --> 01:05:12,450
and i show an example where you were going to depend on something better than

952
01:05:12,450 --> 01:05:14,570
that taken in a really out and

953
01:05:15,070 --> 01:05:18,440
and i'll show you what bayesian fixes in that case

954
01:05:18,490 --> 01:05:20,630
OK so

955
01:05:20,640 --> 01:05:27,350
a lot of these problems were interested in evaluating intervals of the form we have

956
01:05:27,420 --> 01:05:29,540
an in support some function of x

957
01:05:29,550 --> 01:05:32,590
times the probability distribution over x

958
01:05:32,800 --> 01:05:35,670
so there there are some examples here especially if you are bayesian you have to

959
01:05:35,670 --> 01:05:39,900
do this all the time and for many functions that's nasty what you can do

960
01:05:39,900 --> 01:05:40,930
that closed form

961
01:05:40,940 --> 01:05:43,000
we don't know how to do that

962
01:05:43,020 --> 01:05:44,280
here's an example

963
01:05:44,290 --> 01:05:47,010
one example is you want to make you want to make

964
01:05:47,020 --> 01:05:48,510
predictions from the model

965
01:05:48,520 --> 01:05:50,180
then you have to average

966
01:05:50,950 --> 01:05:54,360
the predictive distribution with respect to the posterior distribution

967
01:05:54,420 --> 01:05:57,990
that's an integral that you have you have to do that has this particular form

968
01:05:57,990 --> 01:06:01,870
of the prior distribution over the variables that you want to integrate out some of

969
01:06:01,870 --> 01:06:03,660
the function of

970
01:06:03,750 --> 01:06:06,540
no example is

971
01:06:06,590 --> 01:06:08,610
you want to work on the marginal likelihood

972
01:06:08,620 --> 01:06:10,490
the marginal likelihood is

973
01:06:10,540 --> 01:06:12,360
the integral of the prior

974
01:06:12,370 --> 01:06:13,450
the likelihood

975
01:06:13,490 --> 01:06:16,740
again probably distribution over all the variables

976
01:06:16,750 --> 01:06:18,230
and some of the function of the

977
01:06:18,250 --> 01:06:23,310
and for many many many cases have many variables these designs in creating an extremely

978
01:06:24,610 --> 01:06:27,630
integral to have to work out

979
01:06:27,880 --> 01:06:29,860
and there are bunch of other examples as well

980
01:06:29,910 --> 01:06:31,180
let's try and think about

981
01:06:31,230 --> 01:06:32,650
how do you

982
01:06:33,090 --> 01:06:35,480
how do you solve this problem

983
01:06:35,530 --> 01:06:38,020
using monte carlo

984
01:06:38,090 --> 01:06:40,880
the basic idea in monte-carlo is that you

985
01:06:42,040 --> 01:06:43,290
things go

986
01:06:43,300 --> 01:06:44,640
with the summer

987
01:06:44,660 --> 01:06:45,970
and i have some

988
01:06:46,740 --> 01:06:50,320
she evaluations of f here

989
01:06:50,370 --> 01:06:55,600
and then i just take the average of those evaluation may take the values of

990
01:06:56,330 --> 01:06:57,670
from the distribution

991
01:06:58,740 --> 01:07:05,580
if i can somehow generate samples random distribution then i can evaluate sum

992
01:07:05,600 --> 01:07:10,670
and none of them very mild conditions this thing converges to the value of the

993
01:07:10,680 --> 01:07:14,600
interval here as t increases

994
01:07:16,520 --> 01:07:19,970
in many cases

995
01:07:19,990 --> 01:07:23,020
well if you can draw samples from this distribution and then you can just justify

996
01:07:23,060 --> 01:07:27,360
that and that will give you an estimate taken of sample then

997
01:07:27,370 --> 01:07:30,650
things will converge

998
01:07:30,660 --> 01:07:35,960
in other cases it might be like not easy to draw samples from p of

999
01:07:35,960 --> 01:07:40,050
x and if that's the case you can do

1000
01:07:40,100 --> 01:07:42,240
a little trick called or something

1001
01:07:42,260 --> 01:07:43,300
so you simply

1002
01:07:43,310 --> 01:07:47,490
something which is very similar to what happened in this morning when you multiply and

1003
01:07:47,490 --> 01:07:50,720
divide by some arbitrary distribution qx

1004
01:07:50,730 --> 01:07:54,660
and then you think of the you have you have you know

1005
01:07:54,900 --> 01:07:57,360
at times p divided by q times q

1006
01:07:57,360 --> 01:08:01,650
so that the slope of the degree distribution doesn't move but since power law this

1007
01:08:01,650 --> 01:08:06,530
funny distribution that can have infinite expectation what happens is that if

1008
01:08:06,590 --> 01:08:10,940
the mode is now here is you get a zero network grows the mold also

1009
01:08:10,940 --> 01:08:14,970
side of the mean of the distribution shifts and this gives you the densification so

1010
01:08:14,970 --> 01:08:18,300
that if you go back and analyse their at least two cases that can happen

1011
01:08:18,300 --> 01:08:22,880
one is the tale distribution gets better and mean increases the other one is that

1012
01:08:22,880 --> 01:08:25,590
you have that you have a specific exponent

1013
01:08:25,590 --> 01:08:30,990
but these last less than two the building and q i know infinite expectation which

1014
01:08:30,990 --> 01:08:32,630
means the more samples you get

1015
01:08:32,650 --> 01:08:35,090
the expectation of the distribution

1016
01:08:58,190 --> 01:09:02,340
there there

1017
01:09:22,800 --> 01:09:29,690
i would say

1018
01:09:35,240 --> 01:09:38,690
so so for example this is the

1019
01:09:38,700 --> 01:09:39,650
but this no

1020
01:09:39,670 --> 01:09:41,190
we have to have

1021
01:09:41,220 --> 01:09:45,110
so in this case write a very simple case seems

1022
01:09:45,130 --> 01:09:48,860
this is known to be connected to to all our other nodes right so we

1023
01:09:48,860 --> 01:09:52,530
could be here and to be always connected everyone knows that is connected to all

1024
01:09:52,530 --> 01:09:54,360
other what's right and then

1025
01:09:54,380 --> 01:09:58,610
and then this these nodes nodes so the nodes in here will be connected to

1026
01:09:58,630 --> 01:10:02,050
smaller portions but that would be more of

1027
01:10:02,070 --> 01:10:03,690
right so it's not

1028
01:10:03,740 --> 01:10:05,280
the degrees it really

1029
01:10:07,050 --> 01:10:10,690
so now instead of having zero one i can have here i j

1030
01:10:10,740 --> 01:10:13,440
the probability of having an edge

1031
01:10:13,460 --> 01:10:18,010
so i have like a probabilistic counterpart so i have no probabilities i do the

1032
01:10:18,010 --> 01:10:21,840
same the same multiplication and this would probabilities here

1033
01:10:21,860 --> 01:10:25,610
and then i can sample the graph from these by just these biased coins so

1034
01:10:25,610 --> 01:10:30,150
for every cell ago exactly

1035
01:10:32,050 --> 01:10:37,510
yeah so probabilities probabilities and now i go three coins to get the samples from

1036
01:10:40,970 --> 01:10:44,990
exactly exactly

1037
01:11:08,190 --> 01:11:10,460
is is

1038
01:11:24,380 --> 01:11:28,150
so you

1039
01:11:46,150 --> 01:11:47,700
i think

1040
01:11:58,490 --> 01:12:04,320
i was thinking about

1041
01:12:04,320 --> 01:12:06,440
it is doing

1042
01:12:06,440 --> 01:12:07,860
on the other hand

1043
01:12:07,880 --> 01:12:12,960
you could think of this process i don't give in to the hierarchy said i

1044
01:12:12,960 --> 01:12:14,190
mean hierarchy

1045
01:12:14,200 --> 01:12:19,010
right distract so you have this i communities right and then you have abstracted them

1046
01:12:19,340 --> 01:12:23,460
and over and they are all similar radionuclides so this is one way but is

1047
01:12:23,470 --> 01:12:31,090
just the input eleven but explicitly going for i wasn't planning to that

1048
01:12:38,010 --> 01:12:41,470
as well as

1049
01:13:24,490 --> 01:13:27,440
on the blog web page

1050
01:13:31,150 --> 01:13:36,320
so we have some preliminary experiments on that just a little situation for the bottom

1051
01:13:36,320 --> 01:13:39,740
line is that it is the technology blog

1052
01:13:39,760 --> 01:13:43,760
well what is going on the computer what it's like for weeks if you like

1053
01:13:43,780 --> 01:13:44,530
and then

1054
01:13:44,550 --> 01:13:47,630
michelle model which is like what political

1055
01:13:47,690 --> 01:13:49,260
and the reason is the

1056
01:13:51,130 --> 01:13:53,590
in in

1057
01:13:53,610 --> 01:14:00,300
in slashdot you're the cascades tend tend to be like che shallow and very

1058
01:14:00,320 --> 01:14:04,880
very wide like they tend to like this so that i know some sporting slashdot

1059
01:14:04,900 --> 01:14:08,510
picks up and then all these people say oh i read this interesting thing was

1060
01:14:08,530 --> 01:14:13,750
there's nothing happened well if you look at the political blogs because gauge that tend

1061
01:14:13,750 --> 01:14:15,650
to be

1062
01:14:15,650 --> 01:14:18,170
deep and not so wide

1063
01:14:18,970 --> 01:14:19,630
this is

1064
01:14:19,650 --> 01:14:25,170
what want some some observations we got so we found that the enslaved type of

1065
01:14:25,170 --> 01:14:27,550
blog cascades just wide

1066
01:14:27,550 --> 01:14:31,840
very shallow y here are deep and narrow some

1067
01:14:31,860 --> 01:14:37,300
but the plan to study these

1068
01:14:55,200 --> 01:14:58,400
so this is all

1069
01:14:58,420 --> 01:15:00,090
paul's school

1070
01:15:02,420 --> 01:15:11,800
so you could even looking at the

1071
01:15:11,840 --> 01:15:13,400
the message

1072
01:15:13,490 --> 01:15:17,190
but if you're looking for the maximum then you have to usually people have also

1073
01:15:17,190 --> 01:15:22,400
assume that the graph is connected otherwise you have these infinity problem right yusuf

1074
01:15:22,400 --> 01:15:26,570
the nodes are not connected and there are distance infinity ward they look at physicist

1075
01:15:26,570 --> 01:15:27,960
look at the

1076
01:15:28,800 --> 01:15:33,420
average so the average over the distribution of the of the shortest path

1077
01:15:33,440 --> 01:15:35,650
in football that there are

1078
01:15:35,670 --> 01:15:37,880
there are claims for that

1079
01:15:37,940 --> 01:15:39,110
one not proof

1080
01:15:39,170 --> 01:15:44,590
the children it slowly increases like logarithmically on log log n so

1081
01:15:44,630 --> 01:15:46,760
and what we chose

1082
01:15:46,760 --> 01:15:48,300
it was to do

1083
01:15:48,320 --> 01:15:52,260
basically we tried many different things and the idea and settle down with this one

1084
01:15:52,260 --> 01:15:55,900
but we also tried just the average we also tried to maximum

1085
01:15:55,900 --> 01:16:00,570
and you didn't make any difference is

1086
01:16:03,630 --> 01:16:05,650
these models

1087
01:16:23,900 --> 01:16:27,220
i don't think so

1088
01:16:28,960 --> 01:16:30,360
and even if this

1089
01:16:30,380 --> 01:16:32,700
one the say

1090
01:16:32,720 --> 01:16:37,670
kind of like you cannot rely

1091
01:16:37,670 --> 01:16:38,780
i don't know

1092
01:16:38,800 --> 01:16:40,170
i would say no

1093
01:16:47,070 --> 01:16:49,570
the work

1094
01:16:54,760 --> 01:16:58,970
what it is

1095
01:17:03,860 --> 01:17:05,700
as a whole

1096
01:17:07,470 --> 01:17:09,220
should actually

1097
01:17:18,990 --> 01:17:21,050
i would like to know

1098
01:17:22,280 --> 01:17:25,030
many most triples here

1099
01:17:25,050 --> 01:17:27,960
so there should be

1100
01:17:28,010 --> 01:17:29,630
there should be

1101
01:17:29,630 --> 01:17:32,490
i been on the main like incremental growth

1102
01:17:32,510 --> 01:17:36,420
and so that they have like independent samples in some sense

1103
01:17:40,150 --> 01:17:41,320
the other thing is

1104
01:17:43,010 --> 01:17:44,150
the future

1105
01:17:44,150 --> 01:17:55,510
that's fine but should have that should be here

1106
01:17:55,530 --> 01:17:57,320
you're the only thing is

1107
01:17:57,320 --> 01:18:01,950
he has this other rather weak assumption which can be described in words

1108
01:18:01,980 --> 01:18:06,350
as happiness selection algorithm needs to be better than random guessing

1109
01:18:06,360 --> 01:18:12,400
that's a pretty weak assumption i mean otherwise don't don't use any machine just throw

1110
01:18:12,400 --> 01:18:15,160
a coin so

1111
01:18:15,250 --> 01:18:20,120
this is a weak assumption this is strong assumption and then

1112
01:18:20,130 --> 01:18:25,320
you get this finite sample by the expected number of false positives

1113
01:18:25,350 --> 01:18:28,290
it is bounded by this very simple formula

1114
01:18:28,300 --> 01:18:33,800
so the formula actually wall he was clear so you have your top fifty variables

1115
01:18:33,800 --> 01:18:37,300
so you have fifty square divided by p

1116
01:18:37,310 --> 01:18:40,480
note if p is large this is a great thing here

1117
01:18:40,490 --> 01:18:43,430
so this is really geared towards high dimensional stuff

1118
01:18:43,430 --> 01:18:45,480
and then you have to think one of the two

1119
01:18:45,490 --> 01:18:47,730
pi treshold minus one

1120
01:18:47,740 --> 01:18:52,400
so now we to point around and you say OK like usually statistics be fixed

1121
01:18:52,400 --> 01:18:56,780
the type one error in significance testing is say five percent on now you say

1122
01:18:56,780 --> 01:18:58,530
i want to tolerate

1123
01:18:58,610 --> 01:19:01,340
expected number of false positives ten

1124
01:19:01,350 --> 01:19:03,120
twenty this is up to you

1125
01:19:03,120 --> 01:19:08,940
you keep in mind how many false positives you tolerate you go into these formulas

1126
01:19:08,940 --> 01:19:13,130
and then use all over europe i so this is the solution how you can

1127
01:19:14,510 --> 01:19:20,860
draw the line what is the threshold line such that eventually control the expected number

1128
01:19:20,870 --> 01:19:22,620
of false positives

1129
01:19:22,630 --> 01:19:25,780
this is a very very general theorem

1130
01:19:27,120 --> 01:19:29,910
the issue is really it works

1131
01:19:29,990 --> 01:19:31,960
i mean the only thing

1132
01:19:32,030 --> 01:19:34,070
and the first ten days

1133
01:19:34,090 --> 01:19:36,460
it just needs to be better than random guessing

1134
01:19:36,480 --> 01:19:40,860
and the second thing is it's not only for regression this work for any

1135
01:19:42,040 --> 01:19:46,750
selection problem so it can be a graph and you can ask me age is

1136
01:19:46,750 --> 01:19:50,990
in or out to can be a cluster can ask whether an observation is in

1137
01:19:50,990 --> 01:19:56,390
the cluster in or out for any in or out problem we can apply theory

1138
01:19:56,410 --> 01:20:01,700
so this goes way beyond variable selection in linear model is can be applied to

1139
01:20:02,420 --> 01:20:05,920
kind of discrete selection problem

1140
01:20:06,690 --> 01:20:10,630
arguably if you have such a general theorem there is no free lunch there's a

1141
01:20:10,630 --> 01:20:11,990
condition here

1142
01:20:12,040 --> 01:20:17,990
and the condition is very strong and it's easy to write down the condition harder

1143
01:20:18,630 --> 01:20:19,660
kind of

1144
01:20:19,740 --> 01:20:26,540
convinced that the condition would hold the condition is exchangeability condition and all what we

1145
01:20:26,540 --> 01:20:27,950
require is

1146
01:20:27,960 --> 01:20:29,920
that the distribution

1147
01:20:29,940 --> 01:20:34,590
that is the j th variable is selected by our algorithm and how you look

1148
01:20:34,590 --> 01:20:38,500
for all j for all the covariance in the linear model which are not to

1149
01:20:38,620 --> 01:20:42,510
be active so these are all the noise covariance

1150
01:20:42,530 --> 01:20:47,630
this is a multivariate distribution have to be exchangeable so you can permute

1151
01:20:47,640 --> 01:20:52,350
the labels for the noise coming in always the same distribution

1152
01:20:52,360 --> 01:20:57,290
it's simple condition to write down but it will be hard to be satisfied

1153
01:20:57,880 --> 01:20:59,000
OK so

1154
01:21:00,070 --> 01:21:03,570
the thing is maybe what i should say here

1155
01:21:05,130 --> 01:21:11,760
OK let's discuss the condition briefly in the linear model said

1156
01:21:12,790 --> 01:21:15,160
the first interesting thing is

1157
01:21:15,190 --> 01:21:19,780
i told you before for variable selection in linear model we had to design assumptions

1158
01:21:19,820 --> 01:21:24,240
and we had this debate i mean assumption which there's the non-zero relational coefficients need

1159
01:21:24,250 --> 01:21:26,040
to be sufficiently large

1160
01:21:26,570 --> 01:21:30,240
so here there's nothing like this involved

1161
01:21:30,260 --> 01:21:31,440
this is great

1162
01:21:31,450 --> 01:21:35,020
but the thing is this is only about

1163
01:21:35,040 --> 01:21:38,890
false positive so this is only one point of the middle if you talk about

1164
01:21:38,890 --> 01:21:41,290
false negative stuff you would miss

1165
01:21:41,360 --> 01:21:45,990
then again something of that sort that the signal is strong enough but even if

1166
01:21:45,990 --> 01:21:47,320
the signal is super week

1167
01:21:47,740 --> 01:21:52,400
in terms of false positive control you don't need to make any assumptions

1168
01:21:53,230 --> 01:21:57,100
and exchangeability condition is really restrictive so

1169
01:21:57,110 --> 01:21:59,750
for example you can argue in linear model

1170
01:21:59,770 --> 01:22:03,000
that if you have a correlation then it holds but if you go to more

1171
01:22:03,000 --> 01:22:06,380
complex coalition structure doesn't

1172
01:22:06,390 --> 01:22:08,450
the thing is if

1173
01:22:08,470 --> 01:22:14,900
this exchangeability condition and the whole theory is as of yet much too rough exchangeability

1174
01:22:14,910 --> 01:22:16,860
condition is certainly not

1175
01:22:16,870 --> 01:22:21,860
necessary and sufficient so we actually don't know where to bound is this this kind

1176
01:22:21,860 --> 01:22:22,990
of approach

1177
01:22:22,990 --> 01:22:28,290
so you should not believe because exchangeability doesn't hold the method isn't good

1178
01:22:28,320 --> 01:22:32,980
we still as through because the conditions necessary condition doesn't all the loss is not

1179
01:22:33,730 --> 01:22:38,110
here we don't know actually what the necessary conditions

1180
01:22:38,140 --> 01:22:42,740
so here is a numerical experiments maybe just before the sake the break i don't

1181
01:22:42,740 --> 01:22:46,770
know whether i that but i tried to do so

1182
01:22:46,780 --> 01:22:51,100
it was simply variable selection in linear models using the last so and we look

1183
01:22:51,100 --> 01:22:55,240
at a range of scenarios so what you see is lot here

1184
01:22:55,260 --> 01:22:58,240
well maybe explain the range of scenarios

1185
01:22:58,890 --> 01:23:03,090
p six hundred sixty is from a real data set design

1186
01:23:03,130 --> 01:23:07,120
matrix x from a real motivation data

1187
01:23:07,120 --> 01:23:10,370
max likelihood you just say the robots there and that's all i know

1188
01:23:11,800 --> 01:23:13,410
in the bayesian approach

1189
01:23:13,430 --> 01:23:15,370
you say that my wife particles

1190
01:23:15,390 --> 01:23:19,950
that's where the likelihood is you see this little bit of mass this overlapping here

1191
01:23:19,970 --> 01:23:23,050
the product of those three normalized

1192
01:23:23,090 --> 01:23:27,410
is that so my data believe is well somewhere quite close like if i had

1193
01:23:27,410 --> 01:23:31,120
more accurate census the width of this likelihood is the accuracy of my sense if

1194
01:23:31,120 --> 01:23:35,460
i would if i increase the accuracy of the centre which goes down of pulled

1195
01:23:35,460 --> 01:23:36,300
more towards

1196
01:23:37,570 --> 01:23:42,220
maximum likelihood solution if i'm more uncertain about where i was initially i'm full more

1197
01:23:42,220 --> 01:23:47,490
towards maximizing solution if i was well where i was initially people less towards it

1198
01:23:47,530 --> 01:23:52,350
so that is really odd situation for gaussians because what actually happens in common filters

1199
01:23:52,350 --> 01:23:54,330
for example are based entirely on this

1200
01:23:54,410 --> 01:23:55,510
that's some

1201
01:23:55,570 --> 01:23:59,950
you the gas in prague as you like in your serious calcium that's very unusual

1202
01:23:59,950 --> 01:24:02,200
normally you don't have a prime

1203
01:24:02,300 --> 01:24:05,490
to likelihood to steer where everything is the same distribution i think as is the

1204
01:24:05,490 --> 01:24:07,800
only example i know of

1205
01:24:07,800 --> 01:24:12,510
sometimes you get the same class distributions for your prior your posterior

1206
01:24:12,510 --> 01:24:17,300
and those are called conjugate models and that's really nice computational for a major criticism

1207
01:24:17,300 --> 01:24:20,990
of bayesian inference is that we don't actually choose these things

1208
01:24:21,050 --> 01:24:25,510
for what we believe we choose for tractability so we choose its conjugate models which

1209
01:24:25,510 --> 01:24:26,890
give us nice tractable

1210
01:24:26,890 --> 01:24:29,720
quality is not and i think that's about

1211
01:24:29,740 --> 01:24:34,090
make things faster makes the algorithms fast

1212
01:24:34,140 --> 01:24:38,240
so they are confronted by counting like thousand posterior

1213
01:24:38,700 --> 01:24:42,390
but if they like and is not as one approach is to approximate the posterior

1214
01:24:42,390 --> 01:24:44,620
with a gas so is it ever likely

1215
01:24:45,800 --> 01:24:48,370
you might start with the priors

1216
01:24:48,390 --> 01:24:52,800
over whether you think i'm going to say anything intelligent or anything down

1217
01:24:53,280 --> 01:24:55,300
in this talk and

1218
01:24:55,350 --> 01:24:56,910
you might be observing

1219
01:24:56,930 --> 01:25:01,200
if i say something intelligent that's an observation of one and if i see something

1220
01:25:01,200 --> 01:25:04,160
dumb observation of minus one now how

1221
01:25:04,180 --> 01:25:07,570
you've got an idea of where i am along this line so if i'm here

1222
01:25:07,570 --> 01:25:09,550
on intelligent and if i'm down here

1223
01:25:09,570 --> 01:25:10,820
i'm and rights

1224
01:25:10,830 --> 01:25:14,390
i'm down here the probability of something done is very high and the probability and

1225
01:25:14,390 --> 01:25:21,090
say something intelligent is one minus that market probability i'm saying something intelligent is high

1226
01:25:21,100 --> 01:25:23,820
the probability of say something dumb islam

1227
01:25:23,820 --> 01:25:27,330
or you could even think of this as well i'll come up for example is

1228
01:25:27,350 --> 01:25:30,970
OK so this is your prior to think i'm

1229
01:25:31,030 --> 01:25:32,300
it made up

1230
01:25:32,300 --> 01:25:36,550
and then what happens i say something intelligent

1231
01:25:36,600 --> 01:25:40,570
so you observe this thing this is not likely to notice not normalized in any

1232
01:25:40,570 --> 01:25:42,160
way or form this is the function

1233
01:25:42,160 --> 01:25:47,160
in fact the distribution of something discrete right so just sums the two are two

1234
01:25:47,160 --> 01:25:49,240
different things some to

1235
01:25:49,300 --> 01:25:53,780
one of the two different probabilities so i say something intelligent and then that the

1236
01:25:55,260 --> 01:25:58,490
so now you move a little bit you think well

1237
01:25:58,490 --> 01:25:59,240
you know

1238
01:25:59,240 --> 01:26:03,760
he he probably in this regime because i you know he's only one intelligent thing

1239
01:26:03,760 --> 01:26:07,550
and you considered by accident so

1240
01:26:07,570 --> 01:26:12,600
i'm going to assume is i have a a little bit more intelligent but not

1241
01:26:12,600 --> 01:26:15,090
that much more so one trick

1242
01:26:15,100 --> 01:26:19,680
i'd like to include a difficult if you do this model is once you've done

1243
01:26:21,300 --> 01:26:26,300
this is a really simple model but the x-box

1244
01:26:26,350 --> 01:26:30,930
ranking system for quality of players is based entirely on this model and what i'm

1245
01:26:30,930 --> 01:26:33,490
going to do next

1246
01:26:33,530 --> 01:26:35,510
which is to replace that

1247
01:26:35,530 --> 01:26:38,890
nongaussian nasty thing we think that they a

1248
01:26:39,180 --> 01:26:44,140
so truncated gaussians convolution this a squashing function of the gaussians with the gaussians and

1249
01:26:44,140 --> 01:26:49,370
the way one can do that is by fitting this halcyon to the pink curve

1250
01:26:49,370 --> 01:26:53,370
that we have before now i think that was a great until i saw some

1251
01:26:53,370 --> 01:27:00,760
of josh's calcium fits to his data sets for the film stuff that's really that's

1252
01:27:00,760 --> 01:27:04,100
about the same as the film fit but it can be really bad because these

1253
01:27:04,100 --> 01:27:07,370
gases can be completely truncated so this can be a bad thing to do and

1254
01:27:07,370 --> 01:27:09,620
i think what you've done so this is the trick

1255
01:27:09,640 --> 01:27:12,780
once you've done this you can then proceed with another observation

1256
01:27:12,850 --> 01:27:14,570
so you can then

1257
01:27:14,570 --> 01:27:18,760
you can then say something dumb and then you can drop down and rest

1258
01:27:18,780 --> 01:27:24,090
one thing to notice is because of the quality of some characteristics of these log

1259
01:27:24,090 --> 01:27:26,740
likelihoods here

1260
01:27:26,740 --> 01:27:28,160
and in fact it's the

1261
01:27:28,180 --> 01:27:30,300
log concavity of the

1262
01:27:32,370 --> 01:27:36,660
big so it's likely not log likelihood divided the logarithm you would see is concave

1263
01:27:36,700 --> 01:27:41,030
it has of nasty property i think for called science point of view i everything

1264
01:27:41,030 --> 01:27:45,350
think it's nasty what will happen if this likelihood is concave and most likely to

1265
01:27:45,370 --> 01:27:49,280
use r is every time you make an observation you become more confident about what

1266
01:27:49,280 --> 01:27:50,550
you see

1267
01:27:50,570 --> 01:27:54,910
so basically i think that's very counterintuitive for maybe it's

1268
01:27:54,930 --> 01:27:57,870
maybe it's because i'm learning all these good words i can see maybe it's because

1269
01:27:57,870 --> 01:28:03,760
of introspection and i think the way i update my beliefs on my myself thinking

1270
01:28:03,760 --> 01:28:07,490
there is not said anything really good yes so i just keep thinking not very

1271
01:28:07,490 --> 01:28:09,780
clever clever but then this is something

1272
01:28:09,780 --> 01:28:12,140
really clever

1273
01:28:12,220 --> 01:28:15,140
then i would increase my uncertainty

1274
01:28:15,160 --> 01:28:18,550
and increased my belief that i thought someone has said that i thought the person

1275
01:28:18,550 --> 01:28:21,620
was clear right that's what i would do i think i was wrong i was

1276
01:28:21,620 --> 01:28:24,120
wrong to be confident about what that person was doing

1277
01:28:24,140 --> 01:28:27,930
that's what these models because the concavity of the likelihood in fact they

1278
01:28:29,490 --> 01:28:33,180
their uncertainty in its all pretty much if you look at in terms of entropy

1279
01:28:33,180 --> 01:28:34,240
and pretty much

1280
01:28:34,240 --> 01:28:35,780
linear type away

1281
01:28:36,780 --> 01:28:42,760
even here i said something really dumb and really intelligent really don't really intelligent really

1282
01:28:43,430 --> 01:28:45,760
you just become really confident

1283
01:28:45,780 --> 01:28:48,700
about whether i was intelligent you would be like oh i'm completely at a loss

1284
01:28:49,120 --> 01:28:52,660
and i think that the intuition is that that's what you should be thinking about

1285
01:28:52,680 --> 01:28:53,370
how i feel

1286
01:28:53,760 --> 01:28:57,720
i have opinions and i'm very opinionated actually

1287
01:28:57,720 --> 01:28:58,780
it's can create

1288
01:29:00,420 --> 01:29:02,280
he's not quite as maybe that's not we thought

1289
01:29:03,860 --> 01:29:05,220
how many people have heard it can create

1290
01:29:05,760 --> 01:29:10,570
and yet so so he's he's not quite as famous as another famous englishman who was born

1291
01:29:11,070 --> 01:29:12,960
exactly a hundred years ago but

1292
01:29:13,420 --> 01:29:18,400
he is well worth reading he was basically reinventor of cognitive science and in psychology in in

1293
01:29:18,860 --> 01:29:19,440
britain and he

1294
01:29:20,090 --> 01:29:24,240
in in a classic work that he wrote just shortly before he died tragically in a bike accident his

1295
01:29:26,260 --> 01:29:30,630
really laid out this idea of a view of what kind of machine the mind

1296
01:29:30,630 --> 01:29:34,650
might be which is basically a simulation engine an engine built these kind of wonderful

1297
01:29:34,650 --> 01:29:37,190
mental models end users them fore

1298
01:29:37,610 --> 01:29:42,110
for all the different tasks of commonsense intelligence reasoning planning explanation and so on

1299
01:29:42,650 --> 01:29:46,200
and it's it's it's a fascinating view which then influence the tradition work on mental

1300
01:29:46,200 --> 01:29:50,820
models other other kinds of any i people who've been influenced by his ideas the

1301
01:29:50,820 --> 01:29:52,400
sort of qualitative reasoning tradition

1302
01:29:52,880 --> 01:29:57,490
which again is is one which has not been traditionally associated with say probabilistic inference

1303
01:29:57,490 --> 01:30:00,320
but another way to understand what we're trying to get at with these probabilistic programs

1304
01:30:00,320 --> 01:30:02,090
is bringing those traditions together

1305
01:30:02,650 --> 01:30:05,340
and ask them OK so let's actually put these interaction

1306
01:30:06,300 --> 01:30:10,760
how might you say capture this learning concepts from a few examples well somehow you

1307
01:30:10,760 --> 01:30:14,800
gotta take all the objects that you've seen here and represents something about the structure

1308
01:30:14,800 --> 01:30:17,760
of the domain the generative processes that gave rise to what you see

1309
01:30:18,360 --> 01:30:22,530
and link the concept is something in the generative structures so something like for example

1310
01:30:22,780 --> 01:30:26,860
but we a biologist might look at objects and think about the evolutionary process processes

1311
01:30:26,860 --> 01:30:27,670
that give rise to them

1312
01:30:28,070 --> 01:30:31,110
you might look at these objective organising some kind of hierarchical

1313
01:30:31,570 --> 01:30:32,440
taxonomic tree

1314
01:30:32,940 --> 01:30:34,550
that reflect some intuitions about

1315
01:30:35,070 --> 01:30:39,050
about which are what they have in common the generative processes that gave rise and

1316
01:30:39,050 --> 01:30:40,590
the parts that grow for example

1317
01:30:41,070 --> 01:30:41,630
and then you see

1318
01:30:42,090 --> 01:30:45,110
a few examples of a new concept like those over there and tree

1319
01:30:45,530 --> 01:30:46,510
and you make some gas

1320
01:30:47,150 --> 01:30:48,280
all the above aware there

1321
01:30:49,050 --> 01:30:50,760
where there were the concept refers to

1322
01:30:51,960 --> 01:30:56,700
the exactly how we capture is a bayesian inference i'm not gonna go into the technical details but basically

1323
01:30:57,690 --> 01:31:01,320
have hypotheses which are the branch point in the tree you assume that the possible

1324
01:31:01,320 --> 01:31:02,780
concepts each branch of the tree

1325
01:31:03,490 --> 01:31:07,140
end they belong to the branch the more distinctive it is for the higher prior

1326
01:31:07,140 --> 01:31:09,110
probability it has been labeled by word

1327
01:31:09,470 --> 01:31:13,300
and then there's a likelihood which is just captures the suspicious coincidence seeing those three

1328
01:31:13,300 --> 01:31:14,990
things all cluster they're not part of the tree

1329
01:31:15,550 --> 01:31:19,130
you can say well these these three examples here they could also be an instance

1330
01:31:19,130 --> 01:31:23,590
of say concept labeling background background background but then it would be a big coincidence

1331
01:31:23,590 --> 01:31:25,590
to see them all the first examples cluster

1332
01:31:26,220 --> 01:31:31,320
and without strong hypothesis space just a couple of examples provides enough statistical leverage the

1333
01:31:31,320 --> 01:31:34,380
figure out that the concept is basically just those things there are not anything else

1334
01:31:35,170 --> 01:31:39,380
and we can just as i showed you for the everyday predictions turn this into a quantitative predictive model

1335
01:31:40,050 --> 01:31:44,800
that we can compare with how both adults and children generalize words but but i'm

1336
01:31:44,800 --> 01:31:47,940
not going to tell you the details this experiment because i wanted to emphasize more

1337
01:31:47,940 --> 01:31:50,440
these deeper questions of where the abstract knowledge comes from

1338
01:31:50,880 --> 01:31:52,740
what leads to these probabilistic programs

1339
01:31:53,240 --> 01:31:57,320
so how in particular did you figure out that should be building tree means in

1340
01:31:57,440 --> 01:32:00,920
you know we but think about objects is organised something like a tree structure

1341
01:32:01,720 --> 01:32:04,490
there are taxonomic relations but there's other domains reasoning there

1342
01:32:05,070 --> 01:32:06,130
have other kinds structures

1343
01:32:06,860 --> 01:32:09,740
so in some very be work charles kemp did when he was

1344
01:32:10,190 --> 01:32:10,610
in a group

1345
01:32:11,110 --> 01:32:14,550
he gave what you could think of as a kind of hierarchical bayesian

1346
01:32:14,970 --> 01:32:19,940
approach to learning abstract structure different domains that is it may be the first bit

1347
01:32:19,940 --> 01:32:23,760
of kind of hierarchical bayesian program induction at least we worked on where we say

1348
01:32:24,510 --> 01:32:28,010
we what we observe is at the at the bottom here is some

1349
01:32:28,590 --> 01:32:32,880
features object system let's say the features animals are plants or something

1350
01:32:33,460 --> 01:32:33,920
and then

1351
01:32:34,440 --> 01:32:37,590
we assume those were giving rise to by some kind of

1352
01:32:38,280 --> 01:32:42,670
process operates some kind of stochastic processes operating over let's say a some kind of

1353
01:32:42,670 --> 01:32:44,630
graph structure let's say here's a tree structure

1354
01:32:45,070 --> 01:32:49,280
what we want to learn is not only the tree that underlies those particular argument

1355
01:32:49,280 --> 01:32:52,300
domain but that the general principle which says it's a tree as opposed to some

1356
01:32:52,300 --> 01:32:53,090
other kind of structure

1357
01:32:53,510 --> 01:32:57,300
and so charles' idea was to represent that's higher level knowledge is something like a

1358
01:32:57,300 --> 01:32:59,190
graph grammar so it's it's a principle

1359
01:32:59,720 --> 01:33:00,240
a little rule

1360
01:33:00,670 --> 01:33:04,470
that generates structures of a particular constraints sort namely trees

1361
01:33:05,070 --> 01:33:09,490
and then by by by by giving different kinds of graph grammars we can grow

1362
01:33:09,490 --> 01:33:14,010
at different kinds of structures like chains or rings are cylinders are grids and so

1363
01:33:15,150 --> 01:33:16,360
and by doing inference at

1364
01:33:16,970 --> 01:33:20,780
multiple levels of this hierarchy these these two you can simultaneously inferred

1365
01:33:21,960 --> 01:33:26,760
grammar that's most likely to have generated the data domain as well as the most

1366
01:33:26,760 --> 01:33:32,090
likely graph if you like the parts of the observed data under the grammar so

1367
01:33:32,090 --> 01:33:35,800
it's directly analogous to a kind of grammar induction and a couple problems with grammar

1368
01:33:35,800 --> 01:33:37,800
induction passing in language

1369
01:33:38,440 --> 01:33:41,110
and this is just a couple of results from this from this model

1370
01:33:41,880 --> 01:33:47,240
given a dataset animals and features this this approach was able to figure out that

1371
01:33:47,240 --> 01:33:50,880
it was organised something like a tree structure to learn kind organising principle

1372
01:33:51,840 --> 01:33:53,030
but say for example given

1373
01:33:53,570 --> 01:33:54,570
data on the on

1374
01:33:55,200 --> 01:33:59,670
voting patterns of supreme court justices in the ust was able to figure out not

1375
01:33:59,740 --> 01:34:04,860
tree structures as most appropriate but actually a linear structure here is you know that

1376
01:34:04,860 --> 01:34:08,360
might be there might be a little small see but here we've got e s

1377
01:34:08,490 --> 01:34:11,940
s supreme court justices who served under the rehnquist court

1378
01:34:12,530 --> 01:34:15,440
and you've got the sort very liberal ones here on the left and the very

1379
01:34:15,740 --> 01:34:19,420
right wing one scalia and thomas and so on over there on the right and

1380
01:34:19,420 --> 01:34:21,260
that this this album is figuring out

1381
01:34:21,760 --> 01:34:25,860
that's out of all the these different possible simple forms of structure for the data

1382
01:34:25,860 --> 01:34:27,700
that consists of how they voted on different cases

1383
01:34:28,110 --> 01:34:31,110
just as a tree structure is the most appropriate to think about animals and their

1384
01:34:31,110 --> 01:34:36,380
properties or one-dimensional chain structures are familiar left-right continuum is the most appropriate structure there

1385
01:34:36,820 --> 01:34:41,800
or for example given distances between cities it figures out that something like a cylinder

1386
01:34:41,860 --> 01:34:43,670
cross between it chain and ring

1387
01:34:44,300 --> 01:34:48,360
roughly corresponding to a constant latitude longitude is the right way to represent the data

1388
01:34:49,740 --> 01:34:53,440
so it beginning to show how hierarchical bayesian approach could learn something that we really

1389
01:34:53,440 --> 01:34:54,940
want call a kind of abstract knowledge

1390
01:34:55,420 --> 01:35:00,590
here's another application of this to a more causal reasoning setting where inaugural familiar with

1391
01:35:00,860 --> 01:35:04,720
the idea of bayes net learning has a kind of probabilistic inference we can say

1392
01:35:04,720 --> 01:35:05,980
OK so

1393
01:35:05,990 --> 01:35:09,370
i'm tom minka i'm going to talk you about approximate inference

1394
01:35:09,370 --> 01:35:14,050
at this point you probably very excited about graphical models and bayesian inference and you

1395
01:35:14,050 --> 01:35:15,810
want to run home and

1396
01:35:15,840 --> 01:35:17,220
implements some models

1397
01:35:17,250 --> 01:35:20,210
the problem is is you quickly find is that

1398
01:35:20,770 --> 01:35:25,710
many times exact and the answers are invisible or if you try to use something

1399
01:35:25,710 --> 01:35:27,160
is going be very slow

1400
01:35:27,160 --> 01:35:30,630
so what you like to do is do something which is both fast and bayesian

1401
01:35:30,630 --> 01:35:32,320
and that's what i'm going to talk about today

1402
01:35:33,680 --> 01:35:35,270
so for example if you want to do

1403
01:35:35,320 --> 01:35:38,760
real time processing of an online data set or if you have huge database you

1404
01:35:38,760 --> 01:35:42,360
want to process that through these methods book welcoming

1405
01:35:42,360 --> 01:35:46,960
so i'm going to be focusing on bayesian inference in this talk so let me

1406
01:35:46,960 --> 01:35:52,070
just give a quick overview i think basing so important and so useful i think

1407
01:35:52,070 --> 01:35:55,070
if you want a formal definition basically what does it mean to be bayesian it

1408
01:35:55,070 --> 01:35:59,230
just means that you consistently use probability theory for reasoning about anything you don't know

1409
01:35:59,230 --> 01:36:03,500
so if you have a latent variable if you missing data priors you'll always represent

1410
01:36:03,500 --> 01:36:06,260
that uncertainty with probability distributions

1411
01:36:06,380 --> 01:36:09,240
and and

1412
01:36:09,250 --> 01:36:12,640
i mean that's useful in a sense because now all the elements are consistent every

1413
01:36:12,640 --> 01:36:16,210
organ can handle these unknowns in exactly the same way you can make very general

1414
01:36:16,210 --> 01:36:20,940
procedures for reasoning about all kinds of different models but is actually another advantage which

1415
01:36:20,940 --> 01:36:26,100
is what chris talked about this morning which is much area so by using bayesian

1416
01:36:26,100 --> 01:36:29,550
inference uses actually get this very nice decoupling of different things so you can decouple

1417
01:36:29,550 --> 01:36:33,580
for example your model from your decision making process

1418
01:36:33,640 --> 01:36:36,570
so what i do is i like to think of bayesian inference is sort of

1419
01:36:36,600 --> 01:36:38,500
flow diagram

1420
01:36:38,520 --> 01:36:41,720
where you have data one and

1421
01:36:42,550 --> 01:36:45,300
and you have your model

1422
01:36:45,320 --> 01:36:48,410
and you put these two together

1423
01:36:48,430 --> 01:36:51,330
and you get a posterior distribution

1424
01:36:51,640 --> 01:37:01,430
right now if you want to make decisions as chris show this morning

1425
01:37:01,470 --> 01:37:04,030
all you need is the posterior distribution you don't need to go back to your

1426
01:37:04,030 --> 01:37:07,640
data so here loss matrix changes you can just go to your posterior and ask

1427
01:37:07,640 --> 01:37:09,470
what my decision b

1428
01:37:09,490 --> 01:37:12,910
or if you are priors in different classes change you can again just make a

1429
01:37:12,910 --> 01:37:16,530
decision based on the posterior already had enough to go back to your data so

1430
01:37:16,530 --> 01:37:19,280
it's always my in that sense it summarize everything you need to know what the

1431
01:37:20,070 --> 01:37:24,400
and if you get more data for example in an online streaming context

1432
01:37:24,410 --> 01:37:27,040
you just take your posterior you incorporated

1433
01:37:27,070 --> 01:37:30,500
use it as the prior to the new data you have to get new posterior

1434
01:37:30,510 --> 01:37:34,940
and again you know the data so it's very very efficient

1435
01:37:34,970 --> 01:37:39,750
we processing data if you think about it now but appealed associated businesses being slow

1436
01:37:39,750 --> 01:37:44,850
but actually i think of it as being very very fast processing because this property

1437
01:37:44,870 --> 01:37:47,970
so he decided basically summarizes what i just said

1438
01:37:49,260 --> 01:37:53,260
the posterior distribution is your friend to summarise everything you have learned from

1439
01:37:53,310 --> 01:37:54,220
the data

1440
01:37:54,230 --> 01:37:58,310
right so instead of thinking of data which i think about distributions

1441
01:37:58,320 --> 01:38:01,340
and essentially describe

1442
01:38:01,350 --> 01:38:04,850
your training data to make predictions you do online learning

1443
01:38:04,910 --> 01:38:06,480
the problem is

1444
01:38:06,500 --> 01:38:11,790
how can we efficiently represent compete with these posterior distributions because often very complicated and

1445
01:38:11,790 --> 01:38:14,350
so that's what i'm going to talk about how we can actually make this into

1446
01:38:14,350 --> 01:38:15,850
practical procedure

1447
01:38:15,950 --> 01:38:19,010
now the tool that we use today

1448
01:38:19,060 --> 01:38:24,570
is a very very important innovation actually which is factor graphs images typographical model

1449
01:38:24,570 --> 01:38:28,160
which is actually i think the best way of thinking about

1450
01:38:28,200 --> 01:38:32,170
approximate inference algorithms in terms of factor graphs so let me just give you a

1451
01:38:32,170 --> 01:38:35,570
quick rse summary of what it was another to give a talk on this but

1452
01:38:35,570 --> 01:38:40,320
let me just give my own take on a factor graph is so factor graphs

1453
01:38:40,370 --> 01:38:43,850
basically can be applied to any function it isn't it isn't something specific to probability

1454
01:38:43,850 --> 01:38:47,470
theory can might any function or does it just shows you way of factoring this

1455
01:38:47,470 --> 01:38:52,260
function into product terms and obviously this is not unique you can take a given

1456
01:38:52,260 --> 01:38:56,520
function factored in many different ways of take a polynomial for example i can factor

1457
01:38:56,520 --> 01:38:57,780
in various ways

1458
01:38:57,840 --> 01:39:01,350
and effective after shows you one particular way affecting the function

1459
01:39:01,410 --> 01:39:05,730
so here i give an example of a function f of x y z is

1460
01:39:05,730 --> 01:39:09,440
not distributions just a function it happens to factor in this way was y y

1461
01:39:09,440 --> 01:39:11,910
fuzzy ecstasy

1462
01:39:11,940 --> 01:39:15,950
and given that factorisation i can now use a factor graph represent factorizations so what

1463
01:39:15,950 --> 01:39:17,940
is the factor graph of this function well

1464
01:39:17,950 --> 01:39:20,360
first of all i have my variables

1465
01:39:20,370 --> 01:39:21,830
x y

1466
01:39:23,360 --> 01:39:27,610
and for each term in my factorisation just make box and i connected to each

1467
01:39:27,610 --> 01:39:28,280
of the

1468
01:39:28,300 --> 01:39:33,800
variables it uses so the first term is the boxes x y the second term

1469
01:39:33,800 --> 01:39:35,600
is the boxes lines e

1470
01:39:35,640 --> 01:39:38,540
and third terms boxes e

1471
01:39:38,540 --> 01:39:42,660
so that's the factor graph for this factoring of this function

1472
01:39:42,660 --> 01:39:46,300
there's yet another kind of covalent modification

1473
01:39:46,350 --> 01:39:50,690
which it which is the process of like oscillation

1474
01:39:50,750 --> 01:39:55,580
in which

1475
01:39:55,590 --> 01:40:02,830
a series of sugar side chains carbohydrate side chains is covalently attached to the polypeptide

1476
01:40:04,040 --> 01:40:08,990
usually on serine or threonine is using the hydroxyl of the side chains earrings of

1477
01:40:08,990 --> 01:40:15,260
readings to attack these polypeptide these are all legal sector right side chains

1478
01:40:15,270 --> 01:40:20,510
we we know from our discussion last time only go right means a an assembly

1479
01:40:20,510 --> 01:40:22,960
of a small number of

1480
01:40:22,970 --> 01:40:28,490
model right in each of these blue hexagons represents a model sector right which are

1481
01:40:28,490 --> 01:40:35,620
covalently linked and also modify the extracellular domain of this protein as it protrudes into

1482
01:40:35,630 --> 01:40:41,590
extracellular space so i'm just opening our eyes to the to the possibility that in

1483
01:40:41,590 --> 01:40:43,770
the future we're going to talk about

1484
01:40:43,780 --> 01:40:48,230
other ways in which proteins are modified to further

1485
01:40:48,250 --> 01:40:52,070
two not their structure to make them more suitable more competent to do the various

1486
01:40:52,070 --> 01:40:54,900
jobs to which they have been assigned

1487
01:40:54,950 --> 01:40:58,790
that's therefore returned to what we talked about the last time

1488
01:40:58,800 --> 01:41:06,050
the fact that the structure of the nucleic acids is based on the simple principle

1489
01:41:06,060 --> 01:41:10,290
here by the way i'm returning to the notion of this

1490
01:41:10,440 --> 01:41:13,960
numbering system we're talking about pantos nucleic acids

1491
01:41:13,970 --> 01:41:17,370
the fact that there are two hydroxyl is here right away tells us that we're

1492
01:41:17,370 --> 01:41:18,760
looking at arrivals

1493
01:41:18,810 --> 01:41:25,500
rather than a deoxyribose which as i said last time lacks this sugar right there

1494
01:41:25,580 --> 01:41:28,130
note as we've said repeatedly

1495
01:41:28,180 --> 01:41:31,630
the hydroxyl side chains of

1496
01:41:31,680 --> 01:41:39,810
of carbohydrates offer numerous opportunities for using dehydration reactions or as they are sometimes called

1497
01:41:39,820 --> 01:41:43,190
condensation reactions where you remove the water

1498
01:41:43,240 --> 01:41:45,650
we take out water dehydration

1499
01:41:45,660 --> 01:41:49,360
or we can call the condensation reactions

1500
01:41:54,330 --> 01:41:58,100
two attach other things and in fact in principle

1501
01:41:58,120 --> 01:42:03,030
there are actually four different hydroxyl so it could be used here to do that

1502
01:42:03,070 --> 01:42:05,480
there's one here is one here

1503
01:42:05,490 --> 01:42:06,720
one here

1504
01:42:06,740 --> 01:42:11,520
and one here the four different drugs cells the the one of the two three

1505
01:42:11,520 --> 01:42:13,290
and five

1506
01:42:13,310 --> 01:42:18,040
i dropped so are in principle opportunities for further modification

1507
01:42:18,050 --> 01:42:19,950
in truth

1508
01:42:19,960 --> 01:42:25,180
the two prime hydroxyl is rarely used as well discussed shortly but the main actors

1509
01:42:25,180 --> 01:42:31,190
are therefore this hydroxyl here in which a condensation reaction is created a glycol city

1510
01:42:32,730 --> 01:42:36,970
that is the bond between a sugar and on sugar entity

1511
01:42:36,980 --> 01:42:40,920
glycol refers obviously the sugars like i kitchen or

1512
01:42:40,930 --> 01:42:46,140
glycosylation we talked about before here upon has been made between the base and we'll

1513
01:42:46,140 --> 01:42:49,780
talk about the different bases shortly and

1514
01:42:49,790 --> 01:42:54,270
and the one prime hydroxyl of the rivals

1515
01:42:54,290 --> 01:42:55,650
over here

1516
01:42:55,670 --> 01:42:58,680
the five provide rocks yet another

1517
01:42:58,690 --> 01:43:00,330
condensation reactions

1518
01:43:00,340 --> 01:43:03,940
sometimes this is called esterification reactions

1519
01:43:03,950 --> 01:43:06,760
and again

1520
01:43:10,100 --> 01:43:11,520
refers to

1521
01:43:11,530 --> 01:43:16,680
these kinds of condensation reactions where an acid and base

1522
01:43:16,690 --> 01:43:22,340
react with one another and once again through a condensation reaction yields the removal of

1523
01:43:22,340 --> 01:43:23,290
the water

1524
01:43:23,300 --> 01:43:27,860
and let's look at what's happening here because not only is one hundred one phosphate

1525
01:43:27,860 --> 01:43:29,430
group attached

1526
01:43:29,440 --> 01:43:30,940
the five prime

1527
01:43:30,960 --> 01:43:34,870
the carbon to the five prime hydroxyl in fact there are three

1528
01:43:34,890 --> 01:43:38,380
and there are located in that each of them has a name

1529
01:43:38,400 --> 01:43:42,460
the in board one is called alpha moving further out as being the furthest out

1530
01:43:42,470 --> 01:43:44,220
is gamma

1531
01:43:44,240 --> 01:43:49,920
and it turns out that this chain of phosphates has very important implications for energy

1532
01:43:49,920 --> 01:43:52,430
metabolism and for biosynthesis

1533
01:43:53,810 --> 01:44:00,480
i'm glad i asked that question because these are all three are highly negatively charged

1534
01:44:00,530 --> 01:44:03,730
this is not going to this is in this is that you know

1535
01:44:03,780 --> 01:44:06,360
negative charges repel one another

1536
01:44:06,370 --> 01:44:14,860
and as a consequence to create triphosphate linkage like this represents pushing together negative charge

1537
01:44:15,000 --> 01:44:18,080
more it is these three phosphate even though

1538
01:44:18,090 --> 01:44:21,800
they don't like to be next to one another and that pushing together the creation

1539
01:44:22,040 --> 01:44:23,880
of the triphosphate change

1540
01:44:23,890 --> 01:44:26,560
represents an investment of energy

1541
01:44:26,580 --> 01:44:31,480
and once the three are pushed together that represents great potential energy much like a

1542
01:44:31,480 --> 01:44:35,060
spring that has been compressed together and we just love to pop part

1543
01:44:35,070 --> 01:44:39,980
these four three phosphate would love to pop apart from one another by virtue of

1544
01:44:39,980 --> 01:44:45,420
the fact that these negative charges are mutually repelling but they can't as long as

1545
01:44:45,420 --> 01:44:49,360
they in this triphosphate configuration

1546
01:44:49,380 --> 01:44:52,160
but once the triphosphate configuration

1547
01:44:52,180 --> 01:44:58,120
is broken then the energy released by they're leaving one another can then be exploited

1548
01:44:58,200 --> 01:45:00,100
for other purposes

1549
01:45:00,150 --> 01:45:03,290
keep in mind just to reinforce what i said second ago

1550
01:45:03,310 --> 01:45:06,480
the difference between rivals deoxyribose

1551
01:45:06,500 --> 01:45:09,450
is the presence of the x happens this oxygen

1552
01:45:09,460 --> 01:45:12,210
and now let's focus in a little more detail

1553
01:45:12,230 --> 01:45:14,040
on the basis

1554
01:45:14,050 --> 01:45:18,840
because the bases are indeed the subject of much of our discussion today

1555
01:45:18,850 --> 01:45:26,060
and we have two basic kinds of of basis they're called nitrogen spaces these spaces

1556
01:45:26,060 --> 01:45:27,550
because they have like

1557
01:45:27,590 --> 01:45:32,310
and if you look at the the the five bases that are depicted here you'll

1558
01:45:32,310 --> 01:45:36,900
see that they are not aromatic rings with just carbons in the like a six-car

1559
01:45:38,010 --> 01:45:41,040
rather all of them have a

1560
01:45:41,050 --> 01:45:44,990
a substantial fraction of nitrogen is actually in the range

1561
01:45:45,040 --> 01:45:47,510
two in the case of these primitives

1562
01:45:47,520 --> 01:45:51,230
and here you see the number actually is is four

1563
01:45:51,240 --> 01:45:53,700
in fact one of these

1564
01:45:53,710 --> 01:45:55,560
nitrogen basis

1565
01:45:55,570 --> 01:46:00,190
indicated here go on has actually if if one appear as the side chains

1566
01:46:00,200 --> 01:46:05,660
this is called i this is outside of the china represents side group and if

1567
01:46:05,660 --> 01:46:09,350
we begin not to make distinctions between the the ring itself

1568
01:46:09,400 --> 01:46:11,340
and the entities that

1569
01:46:11,350 --> 01:46:13,150
protrude out of the ring

1570
01:46:13,160 --> 01:46:17,270
they really represent some of the important distinguishing characteristics

1571
01:46:17,280 --> 01:46:22,010
it's important that we understand primitives have one ring and these are have two rings

1572
01:46:22,010 --> 01:46:25,940
in the the instead of five and a six membered ring

1573
01:46:25,960 --> 01:46:29,730
fused together as you can see the printings have only a six membered ring

1574
01:46:29,740 --> 01:46:35,760
and what's really important in determining their identity is not the basic permitting or appearing

1575
01:46:35,760 --> 01:46:40,800
structure it's once again the side chains to distinguish these one from the other

1576
01:46:40,810 --> 01:46:42,600
here in the case of side c

1577
01:46:42,610 --> 01:46:49,040
we see that there's a carbonyl here oxygen sticking out and the mean over here

1578
01:46:49,050 --> 01:46:52,720
we see you're still which happens to be present in RNA

1579
01:46:52,740 --> 01:46:54,580
but not DNA

1580
01:46:54,590 --> 01:46:57,790
which has two cardinals here and here

1581
01:46:57,800 --> 01:47:02,970
obviously therefore what distinguishes these two for one another it is this oxygen versus this

1582
01:47:02,970 --> 01:47:04,030
i mean

1583
01:47:04,050 --> 01:47:08,180
and here we see the timing which is present in DNA

1584
01:47:08,230 --> 01:47:12,790
but not on a on this will become very familiar to you shortly this looks

1585
01:47:12,790 --> 01:47:15,990
just like you're still except for the fact that there is a metal groups taking

1586
01:47:15,990 --> 01:47:17,910
out here

1587
01:47:17,930 --> 01:47:22,840
now very important for our understanding of what's happening here is the fact

1588
01:47:22,860 --> 01:47:28,300
this method group although it distinguishes finding from your c is itself biologically actually not

1589
01:47:28,300 --> 01:47:29,660
very important

1590
01:47:29,740 --> 01:47:34,330
it's there to be sure and distinguishing mark of t verses you

1591
01:47:34,380 --> 01:47:40,190
but the business end of t verses you in terms of encoding information happens here

1592
01:47:40,190 --> 01:47:45,680
with these two oxygen sticking out there the important accidents here here and therefore from

1593
01:47:45,680 --> 01:47:50,540
the point of view information content as will soon see t and u are essentially

1594
01:47:50,540 --> 01:47:56,610
so adding new rows or new columns to the data matrix is very simple

1595
01:47:56,680 --> 01:48:02,010
and it just requires more work but you once you've updated the upper triangular matrix

1596
01:48:02,010 --> 01:48:07,700
or have the upper triangular matrix updating is very easy matter in terms of adding

1597
01:48:07,700 --> 01:48:11,000
observations or adding columns

1598
01:48:12,880 --> 01:48:18,890
adding and also the leading variables that is relatively easy but this is the hard

1599
01:48:18,890 --> 01:48:23,430
problem and it can be done but it's much more sensitive problem that is the

1600
01:48:23,430 --> 01:48:27,520
problem of down dating if one has outliers and wants to throw out some of

1601
01:48:27,520 --> 01:48:32,740
the observations that you've processed already then that's not a very easy thing to do

1602
01:48:32,750 --> 01:48:36,760
for one thing you can see by throwing up one observation you might change the

1603
01:48:36,760 --> 01:48:38,410
rank of the matrix

1604
01:48:38,430 --> 01:48:45,670
so down dating can can really have a great effect on the matrix computations and

1605
01:48:45,680 --> 01:48:50,260
the thing is no matter what numerical algorithm proposing here i'm sure you're all familiar

1606
01:48:50,260 --> 01:48:51,100
with this

1607
01:48:51,120 --> 01:48:54,870
the problem can be basically ill conditioned so

1608
01:48:55,140 --> 01:48:58,150
let me just say a word about that

1609
01:48:58,200 --> 01:48:59,060
and how we

1610
01:48:59,080 --> 01:49:00,790
do work on that in

1611
01:49:00,810 --> 01:49:02,390
numerical linear algebra

1612
01:49:02,880 --> 01:49:09,160
it depends upon some perturbation theory we will hear more about condition later from from

1613
01:49:09,160 --> 01:49:09,950
our hosts

1614
01:49:10,330 --> 01:49:13,590
so here's the classical theorem

1615
01:49:13,610 --> 01:49:15,150
a kind of fear

1616
01:49:15,170 --> 01:49:17,630
you are given here and here is the vector b

1617
01:49:17,640 --> 01:49:22,370
and then you perturb the matrix a by delta x o delta is the matrix

1618
01:49:22,380 --> 01:49:23,740
and the vector b

1619
01:49:23,750 --> 01:49:25,740
it is perturbed by delta b

1620
01:49:25,760 --> 01:49:27,100
and so

1621
01:49:27,110 --> 01:49:28,680
or the ACT

1622
01:49:28,710 --> 01:49:34,100
is the reason you would have gotten if you have the exact solutions are tailored

1623
01:49:34,130 --> 01:49:40,140
is what you get when you substitute in this approximation vector to the solution so

1624
01:49:40,140 --> 01:49:45,170
here's axtell the here's delta a delta b

1625
01:49:45,180 --> 01:49:49,100
and here's the important results

1626
01:49:49,110 --> 01:49:51,510
the relative error

1627
01:49:51,530 --> 01:49:53,280
is dependent upon

1628
01:49:53,310 --> 01:49:56,820
various things but the importance of the guy that really kills you

1629
01:49:56,900 --> 01:50:02,400
is this a kappa square of a so there can be no matter how accurate

1630
01:50:02,400 --> 01:50:07,320
an algorithm you use you still have kappa square by entering

1631
01:50:07,340 --> 01:50:09,150
providing the

1632
01:50:09,170 --> 01:50:10,640
tension of data

1633
01:50:10,660 --> 01:50:13,880
it is an answer so far

1634
01:50:13,930 --> 01:50:16,490
if you just solving linear equations

1635
01:50:16,510 --> 01:50:22,360
tension square goes out tension goes out but if you're solving least squares problem then

1636
01:50:22,360 --> 01:50:25,440
this enters and the condition number square

1637
01:50:25,450 --> 01:50:26,830
enters so

1638
01:50:26,840 --> 01:50:32,930
something normal equations are solving least squares problems not the normal equations is inherently more

1639
01:50:32,930 --> 01:50:39,360
difficult and solving linear equations you're minimizing some quantities and quadratic form and it may

1640
01:50:39,360 --> 01:50:44,470
be very smooth at the bottom there may be many nearby solutions so

1641
01:50:44,520 --> 01:50:51,520
least squares is inherently more difficult than solving linear equations in n but here's something

1642
01:50:51,520 --> 01:50:57,590
that's been observed before here is that the residual vector the perturbation theory for the

1643
01:50:57,590 --> 01:50:58,480
residual vector

1644
01:50:58,900 --> 01:51:03,660
and that depends just on the condition number and this has been observed before you

1645
01:51:03,660 --> 01:51:07,180
can have a small residual and allows the solution

1646
01:51:07,190 --> 01:51:13,300
so having a small residual does not necessarily imply you have a good approximate solution

1647
01:51:13,300 --> 01:51:18,070
in maybe that only the condition number enters whereas if you look at the solution

1648
01:51:18,840 --> 01:51:21,460
then the condition number square enters so that's

1649
01:51:21,470 --> 01:51:23,660
important pointer

1650
01:51:29,830 --> 01:51:31,970
what about singular systems

1651
01:51:31,980 --> 01:51:39,760
now later i'll have a bit to say about cingular's systems or what's more difficult

1652
01:51:39,760 --> 01:51:44,980
our systems that are nearly singular so it's you know in mathematics you learn to

1653
01:51:44,980 --> 01:51:48,030
rank the rank of the matrix is are

1654
01:51:49,060 --> 01:51:53,080
numerically and computationally this seldom happens

1655
01:51:53,090 --> 01:51:55,370
you don't have the matrix are

1656
01:51:55,380 --> 01:51:57,370
of rank are you have

1657
01:51:57,390 --> 01:52:01,920
so almost our analysis of had before maybe i should

1658
01:52:01,930 --> 01:52:03,790
use that over here

1659
01:52:03,800 --> 01:52:09,070
here's a classical kind of example

1660
01:52:09,820 --> 01:52:17,710
if you look at this matrix

1661
01:52:17,730 --> 01:52:21,310
ones on the diagonal and zeros here

1662
01:52:21,320 --> 01:52:24,350
n minus once everywhere above

1663
01:52:24,370 --> 01:52:25,930
and below

1664
01:52:25,950 --> 01:52:28,980
now this looks like a nice innocent matrix

1665
01:52:29,030 --> 01:52:31,690
its determinant

1666
01:52:31,720 --> 01:52:33,860
equals one

1667
01:52:33,880 --> 01:52:37,020
the eigen values are all equal to one

1668
01:52:37,040 --> 01:52:41,680
so from the map the rank is therefore the rank is and

1669
01:52:41,790 --> 01:52:48,760
however if the so called this a

1670
01:52:48,770 --> 01:52:51,310
you can have a plus

1671
01:52:51,320 --> 01:52:54,770
a matrix of the order of

1672
01:52:55,190 --> 01:52:59,200
two to the minus ten minus one

1673
01:52:59,220 --> 01:53:04,720
times e this rank

1674
01:53:05,490 --> 01:53:08,880
is equal to n minus one OK so what does that say

1675
01:53:08,890 --> 01:53:10,450
so if this is

1676
01:53:10,620 --> 01:53:13,480
twenty one by twenty one matrix

1677
01:53:13,500 --> 01:53:17,700
then this would be to to the minus twenty which is about ten to the

1678
01:53:17,700 --> 01:53:20,620
minus six and others you can

1679
01:53:20,660 --> 01:53:25,970
as a perturbation of order ten to the minus six to this matrix and get

1680
01:53:25,970 --> 01:53:30,340
the matrix of rank minus one so very small perturbations

1681
01:53:30,350 --> 01:53:34,220
added to this matrix will make it rank and minus one and there are lots

1682
01:53:34,220 --> 01:53:35,390
of examples so

1683
01:53:35,430 --> 01:53:39,220
you know you look at this if you took a linear algebra course professor tell

1684
01:53:39,220 --> 01:53:44,830
you know yes this is rank and its determinant is one but it's very near

1685
01:53:44,870 --> 01:53:47,260
a matrix of rank n minus one

1686
01:53:48,140 --> 01:53:50,060
remember if

1687
01:53:50,080 --> 01:53:55,210
catastrophic problems can occur they do when they do computations i mean if you're nearer

1688
01:53:55,280 --> 01:53:59,250
that problem you get when you do the computing so that's something you always have

1689
01:53:59,250 --> 01:54:01,180
to be aware of

1690
01:54:01,180 --> 01:54:03,780
of the nonterminals into terminal

1691
01:54:03,810 --> 01:54:11,130
or some combination of of tunnels terminals the big difference is that in pcfgs

1692
01:54:11,140 --> 01:54:12,930
i assign weights

1693
01:54:12,930 --> 01:54:14,840
to the productions

1694
01:54:14,850 --> 01:54:16,560
sorry assign probabilities

1695
01:54:16,570 --> 01:54:19,440
and what does it mean that

1696
01:54:19,520 --> 01:54:25,170
and again the problem for all productions of the same nonterminals after about one what

1697
01:54:25,170 --> 01:54:27,270
this means is that

1698
01:54:27,270 --> 01:54:30,600
i'm assuming that the nonterminals produced one

1699
01:54:30,630 --> 01:54:33,200
of the other productions and only want

1700
01:54:33,230 --> 01:54:35,260
so this is the old version of those

1701
01:54:35,270 --> 01:54:38,010
so for example in the phrase could be produced is unknown

1702
01:54:38,090 --> 01:54:40,050
why is determined followed by now

1703
01:54:40,230 --> 01:54:43,560
so the first one is probably point seven because it's more common and the second

1704
01:54:43,560 --> 01:54:47,320
one is probably point three and these are usually learned from that

1705
01:54:48,420 --> 01:54:53,300
now you can also use what's called a weighted context free grammars where the production

1706
01:54:53,300 --> 01:54:57,730
was instead of probabilities have weights and the two are equivalent in economics convert from

1707
01:54:57,730 --> 01:55:00,900
one to the other and here if course it's going to be more convenient to

1708
01:55:00,900 --> 01:55:01,960
use weights

1709
01:55:01,970 --> 01:55:05,560
but if you have a PCFG can just converted to to the way form and

1710
01:55:05,560 --> 01:55:06,610
go from there

1711
01:55:06,670 --> 01:55:12,650
statistical parsing algorithms typically assume that the parameters are in chomsky normal form

1712
01:55:12,680 --> 01:55:17,140
which you can reduce any context free grammar to chomsky normal form means that all

1713
01:55:17,140 --> 01:55:19,770
productions are the former nonterminals

1714
01:55:19,780 --> 01:55:23,960
goes to non terminal and non terminal on ontologies to terminal or the channels are

1715
01:55:23,970 --> 01:55:27,650
the actual topics that appear in the input we can assume that you to just

1716
01:55:27,650 --> 01:55:29,090
just for simplicity

1717
01:55:29,100 --> 01:55:34,300
so how do you implement the statistical parts as as an MLN

1718
01:55:35,070 --> 01:55:39,770
we're going to have one evidence predicate which as before is just the evidence predicate

1719
01:55:39,770 --> 01:55:43,720
that describes the text it says that this token appears in this position in the

1720
01:55:43,720 --> 01:55:49,020
input string like for example in the sense that we saw before tokenpizza appears in

1721
01:55:49,020 --> 01:55:50,680
position three

1722
01:55:51,240 --> 01:55:53,680
and now we're going to have query predicates

1723
01:55:53,680 --> 01:55:56,390
the quadratic it's are going to be the constituents

1724
01:55:56,440 --> 01:55:58,600
so for each type

1725
01:55:58,600 --> 01:56:01,850
i'm going to have predicted to form consistent position position

1726
01:56:01,880 --> 01:56:08,350
like for example there's an phrase starting position i anything at position j

1727
01:56:08,360 --> 01:56:12,310
OK so for example in the inputs is this non-free for a starting position to

1728
01:56:12,310 --> 01:56:13,470
position four

1729
01:56:13,480 --> 01:56:19,780
OK and basically by computing the truth thousand discrete predicates we accomplish the pass because

1730
01:56:19,780 --> 01:56:24,400
that tells us exactly what constituents exists over what parts of the of the string

1731
01:56:24,430 --> 01:56:29,070
and then we can evaluate this using the standard precision and recall metrics of constituents

1732
01:56:29,070 --> 01:56:31,010
that people use in statistical parsing

1733
01:56:31,420 --> 01:56:36,640
OK so we convert the grammar into an was very simple and it's actually pretty

1734
01:56:36,640 --> 01:56:40,600
much the same way that in in prolog when you want to do the passing

1735
01:56:40,600 --> 01:56:44,070
of of DCG is so for each rule of the form

1736
01:56:44,100 --> 01:56:47,020
a ghost to BC

1737
01:56:47,050 --> 01:56:49,130
we introduce formal

1738
01:56:49,140 --> 01:56:53,020
d i j and cjk goes to a i

1739
01:56:53,030 --> 01:56:57,390
so this for example if there is an arms inference from i to j

1740
01:56:57,400 --> 01:56:59,350
and the very first from j to k

1741
01:56:59,420 --> 01:57:01,430
and there's a sentence from the article

1742
01:57:02,280 --> 01:57:07,520
so this is one to one translation of grammar rules to animals

1743
01:57:07,530 --> 01:57:11,850
now in addition because the millions are more flexible than the pcfgs we need to

1744
01:57:11,850 --> 01:57:18,510
introduce some some restrictions in particular to two for each nonterminal introduced hard formula that

1745
01:57:18,510 --> 01:57:20,170
says that exactly

1746
01:57:20,180 --> 01:57:23,190
one production of that formula holds of the

1747
01:57:24,610 --> 01:57:29,180
for example if this is the noun phrase it's either amount

1748
01:57:29,200 --> 01:57:31,980
or a or or a a determiner followed by in

1749
01:57:32,130 --> 01:57:36,780
and then of course for each rule of the form nonterminal ghost terminals

1750
01:57:36,810 --> 01:57:39,930
i just in to the samples of the form if there is a token a

1751
01:57:39,930 --> 01:57:43,020
position i there's consistent of you know of

1752
01:57:43,030 --> 01:57:46,590
type capital a from my so the question

1753
01:57:46,600 --> 01:57:51,940
if you want to

1754
01:57:53,200 --> 01:57:56,480
he might be

1755
01:57:56,480 --> 01:57:57,850
it is

1756
01:57:57,860 --> 01:58:01,050
it's a lot

1757
01:58:01,060 --> 01:58:04,060
this is

1758
01:58:15,850 --> 01:58:17,200
it's a matter of taste

1759
01:58:17,220 --> 01:58:21,380
so in addition business exercise to do is to is to see that if you

1760
01:58:21,380 --> 01:58:22,550
introduce these

1761
01:58:22,560 --> 01:58:24,060
additional assumptions

1762
01:58:24,060 --> 01:58:27,740
one becomes according to the other sense that any more than you could represent one

1763
01:58:27,740 --> 01:58:31,890
way you could represent the so you're right i could just say

1764
01:58:31,920 --> 01:58:36,770
introduced was performed in PIJ and gpgtk and as i can

1765
01:58:37,180 --> 01:58:40,200
i just think it's more intuitive to it this way but he could content

1766
01:58:40,300 --> 01:58:41,970
perform just as well

1767
01:58:56,140 --> 01:58:59,400
logic is in the following important facility right

1768
01:58:59,430 --> 01:59:04,030
remember back to when we talk about potential functions and features

1769
01:59:04,060 --> 01:59:06,840
logic is tell me what important features are

1770
01:59:06,850 --> 01:59:10,340
it's a common language for writing the features not just as in logic that is

1771
01:59:10,340 --> 01:59:13,890
that's a certain things are equivalent it's also going to be the case if that

1772
01:59:13,890 --> 01:59:19,530
certain features are equivalent but i'm going to have equivalent is beyond the logical ones

1773
01:59:19,560 --> 01:59:22,740
i'm going to make a set of features are going to be equal to another

1774
01:59:22,810 --> 01:59:26,070
if i can find the set of weights for the first one that implements the

1775
01:59:26,070 --> 01:59:29,690
same thing as the second one with twenty percent of what's it has so there's

1776
01:59:29,700 --> 01:59:34,100
going to be equivalences among those are free to choose it's still the case that

1777
01:59:34,100 --> 01:59:39,350
by writing this in either injective application form i've written something much much more compact

1778
01:59:39,460 --> 01:59:43,730
than if i just after to the probability distribution over the whole plate involving all

1779
01:59:43,730 --> 01:59:45,300
the nonterminals in the production

1780
01:59:45,560 --> 01:59:50,440
that would be building on a lot of useful of useless and painful painful parameters

1781
01:59:56,180 --> 01:59:57,190
you know

1782
01:59:59,010 --> 02:00:00,490
you want to

1783
02:00:04,170 --> 02:00:11,430
we can be absolutely so in some cases the intuitive the optimal are the same

1784
02:00:11,430 --> 02:00:15,140
sometimes they equivalent sometimes they're not the same and are equivalent and you need to

1785
02:00:15,140 --> 02:00:16,220
be careful

1786
02:00:16,230 --> 02:00:17,340
absolutely right

1787
02:00:18,010 --> 02:00:23,070
no miracles right there still a place for knowledge engineers it's easier than before because

1788
02:00:23,070 --> 02:00:27,070
they have more flexibility to be considered things the right way or the wrong way

1789
02:00:27,140 --> 02:00:30,840
and you know we're still discovering what the range of good bad things to do

1790
02:00:31,550 --> 02:00:37,180
you know there's certainly a lot to explore so once you've represented your grammar in

1791
02:00:37,180 --> 02:00:38,240
this way

1792
02:00:38,280 --> 02:00:42,960
and you have the evidence predicates and these query predicates doing MEP inference on this

1793
02:00:42,960 --> 02:00:46,110
network yields the most probable parse

1794
02:00:46,130 --> 02:00:50,640
OK so you condition on the input string and you get the results for the

1795
02:00:50,640 --> 02:00:53,320
consequence that are the most probable parse

1796
02:01:04,490 --> 02:01:15,220
if that is the price is invalid then it will have zero problem

1797
02:01:19,300 --> 02:01:26,300
no but that's implicit in the formulas that i wrote

1798
02:01:26,330 --> 02:01:30,590
this this is is actually one of the things that is accomplished by this formula

1799
02:01:31,800 --> 02:01:34,850
this formulation

1800
02:01:34,850 --> 02:01:39,720
i am saying that if the if the nonterminal holds exactly one production holds no

1801
02:01:39,720 --> 02:01:42,470
more no less

1802
02:01:42,640 --> 02:01:48,890
which basically means that i know missus things can happen because forbidden by this rule

1803
02:01:48,930 --> 02:01:52,670
but i can have like is allowed to things that shouldn't be there

1804
02:01:52,680 --> 02:01:56,350
is a nice exercise to

1805
02:01:56,390 --> 02:02:00,550
so we can do statistical processing you again just by writing in the and again

1806
02:02:00,550 --> 02:02:03,390
you can learn the weights for these formulas from data

1807
02:02:03,490 --> 02:02:08,130
just using the standard discriminative learning algorithms that we have

1808
02:02:08,130 --> 02:02:12,100
so now we know how to see how well we get more missions and try

1809
02:02:12,100 --> 02:02:14,880
to do some semantic processing as well

1810
02:02:14,900 --> 02:02:18,220
so the first thing that we can try to do is combine this leading to

1811
02:02:19,570 --> 02:02:24,890
but so if you pass texts you extract noun phrases from the text that potentially

1812
02:02:24,890 --> 02:02:26,380
represented entities

1813
02:02:26,380 --> 02:02:29,690
like george bush the president and the white house

1814
02:02:30,470 --> 02:02:33,030
and i want to figure out if these are the same one

1815
02:02:33,060 --> 02:02:37,010
this is like the standard problem of coreference resolution natural language what you can do

1816
02:02:37,010 --> 02:02:40,510
that by combining the statistical parts that we just saw

1817
02:02:40,520 --> 02:02:42,730
with the introduction of things that we have

1818
02:02:42,810 --> 02:02:46,920
and the way to do that just rules of the form known phrase i j

1819
02:02:47,340 --> 02:02:53,060
and maybe some other conditions here implies into your type e g

