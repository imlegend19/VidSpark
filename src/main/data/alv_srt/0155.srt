1
00:00:00,000 --> 00:00:01,600
can simply compute

2
00:00:01,610 --> 00:00:03,280
i'm just saying that

3
00:00:03,370 --> 00:00:06,310
if a defining white

4
00:00:07,110 --> 00:00:09,070
at equally

5
00:00:09,090 --> 00:00:13,560
of what the

6
00:00:18,820 --> 00:00:20,530
the minus one

7
00:00:20,590 --> 00:00:22,900
x being

8
00:00:22,960 --> 00:00:26,950
they would have to wait

9
00:00:26,970 --> 00:00:31,300
so i had to wait so i have prediction and they have a way of

10
00:00:33,120 --> 00:00:35,420
the next week

11
00:00:35,420 --> 00:00:36,990
i compute this

12
00:00:37,010 --> 00:00:39,310
it doesn't incremental yet

13
00:00:39,460 --> 00:00:45,900
but let me go on but this is something i can do well

14
00:00:46,010 --> 00:00:47,710
so now

15
00:00:47,720 --> 00:00:51,190
if this is this is your decision perception

16
00:00:51,190 --> 00:00:53,450
yes it is because

17
00:00:53,460 --> 00:00:55,380
if a b

18
00:00:59,050 --> 00:01:00,640
the equal

19
00:01:03,380 --> 00:01:06,150
squared norm of the

20
00:01:06,160 --> 00:01:09,510
then the great

21
00:01:09,530 --> 00:01:10,900
of y

22
00:01:10,900 --> 00:01:13,210
who are

23
00:01:15,280 --> 00:01:18,230
which is equal to to the perceptron weight

24
00:01:22,200 --> 00:01:23,770
OK that's reassuring

25
00:01:25,970 --> 00:01:27,640
i can it's kind of

26
00:01:27,680 --> 00:01:31,210
i mean i can i can do this

27
00:01:31,230 --> 00:01:34,590
i can rewrite from using this potential function

28
00:01:34,600 --> 00:01:37,410
i think it's very comforting shows

29
00:01:39,660 --> 00:01:42,800
this is the function in india is very simple

30
00:01:43,540 --> 00:01:44,840
and then OK

31
00:01:44,840 --> 00:01:49,490
i attached so the perceptron weight is the they outgoing

32
00:01:49,550 --> 00:01:51,280
the use of ball

33
00:01:51,290 --> 00:01:52,920
the potential for

34
00:01:52,930 --> 00:01:58,210
why do i find it in terms of the great i don't want to find

35
00:02:04,800 --> 00:02:07,300
because when you when you

36
00:02:07,910 --> 00:02:13,030
look at the first of these potentials

37
00:02:13,060 --> 00:02:19,400
arbitrary from the pensions then you need to do the analysis you need

38
00:02:19,410 --> 00:02:22,570
i'm not sure from complex analysis

39
00:02:22,620 --> 00:02:26,820
any convex analysis if you read enough of it

40
00:02:26,840 --> 00:02:35,390
probably but you realize that it is more natural look back from a functional

41
00:02:35,550 --> 00:02:41,490
like this and then use and then define the way to the gradient of the

42
00:02:43,300 --> 00:02:47,600
a and this is called also the this

43
00:02:47,620 --> 00:02:51,750
function is called the gradient of the potential is coming function

44
00:02:51,780 --> 00:02:53,440
but i'm not going into

45
00:02:53,450 --> 00:02:53,890
more data

46
00:02:56,190 --> 00:03:01,770
you know what to say

47
00:03:05,540 --> 00:03:07,780
the first now

48
00:03:07,840 --> 00:03:11,660
so we have something that might sound my little silly

49
00:03:11,660 --> 00:03:15,130
i mean i cannot be certain about

50
00:03:15,180 --> 00:03:18,320
many interesting prisoners i might be

51
00:03:18,340 --> 00:03:20,710
now it's tempting to think

52
00:03:20,730 --> 00:03:22,740
of p

53
00:03:23,060 --> 00:03:25,100
are you

54
00:03:25,570 --> 00:03:27,960
four being bigger than two

55
00:03:27,980 --> 00:03:30,200
you need for technical

56
00:03:32,820 --> 00:03:34,550
it gets you now agree

57
00:03:34,990 --> 00:03:36,210
it's getting angry

58
00:03:37,150 --> 00:03:40,710
because i just using and not using

59
00:03:40,810 --> 00:03:44,120
for a couple that you put on the

60
00:03:44,440 --> 00:03:45,990
very common

61
00:03:47,870 --> 00:03:49,780
so can analyse doctor

62
00:03:49,800 --> 00:03:53,900
this is something about but more importantly

63
00:03:53,980 --> 00:03:57,630
can i write this incrementally will be night

64
00:03:57,670 --> 00:04:00,120
there would be no radiation incrementally

65
00:04:00,210 --> 00:04:03,990
and so this is actually possible reason i

66
00:04:09,230 --> 00:04:14,560
is true whenever the potential function is chosen in a proper way so i'm now

67
00:04:14,590 --> 00:04:17,590
imposing more conditions the choice of phi

68
00:04:25,550 --> 00:04:32,350
besides being differentiable i would like to be a convex

69
00:04:37,790 --> 00:04:38,520
so now

70
00:04:38,540 --> 00:04:41,200
is differentiable

71
00:04:41,210 --> 00:04:44,390
strictly convex

72
00:04:44,390 --> 00:04:51,180
but as i have a little work

73
00:04:51,190 --> 00:04:52,420
on the

74
00:04:52,470 --> 00:04:58,440
nine of because the conditions that i'm not going to spend because they're overly technical

75
00:05:00,170 --> 00:05:02,130
but all the fraction of that

76
00:05:02,180 --> 00:05:06,630
with a book about which is the problem i was just about to innovation but

77
00:05:06,630 --> 00:05:07,790
it's not for

78
00:05:09,160 --> 00:05:12,410
but that the potential with this

79
00:05:12,480 --> 00:05:13,450
this condition

80
00:05:13,500 --> 00:05:16,910
OK so now if we had this conditions

81
00:05:20,800 --> 00:05:22,450
the gradient

82
00:05:22,530 --> 00:05:23,920
it is

83
00:05:33,430 --> 00:05:35,340
a few

84
00:05:35,350 --> 00:05:41,640
if you like this that you've got to rockafellar

85
00:05:43,570 --> 00:05:44,630
o thing

86
00:05:44,670 --> 00:05:46,100
convex analysis

87
00:05:46,190 --> 00:05:52,400
and you find a lot of

88
00:05:52,450 --> 00:05:54,720
more than you can

89
00:05:54,720 --> 00:05:56,470
the input

90
00:05:57,380 --> 00:05:59,470
basically the inverse exists

91
00:05:59,510 --> 00:06:04,470
and for these nice polynomial potential is easily computable

92
00:06:04,510 --> 00:06:05,190
so now

93
00:06:05,210 --> 00:06:06,190
if i had this

94
00:06:06,190 --> 00:06:08,280
to do the convolution

95
00:06:08,280 --> 00:06:11,610
this is just element wise so no cost

96
00:06:11,610 --> 00:06:12,250
i mean

97
00:06:12,280 --> 00:06:13,750
cost is

98
00:06:13,770 --> 00:06:15,770
it is

99
00:06:15,780 --> 00:06:17,800
is an

100
00:06:17,820 --> 00:06:19,750
and then what

101
00:06:19,770 --> 00:06:23,840
then we have to go back

102
00:06:23,900 --> 00:06:26,250
one more FFT

103
00:06:26,250 --> 00:06:27,800
during the

104
00:06:27,840 --> 00:06:29,210
answer we want

105
00:06:29,230 --> 00:06:30,880
c starting

106
00:06:30,880 --> 00:06:36,630
c convolved with the DNA and you can use it to t

107
00:06:38,320 --> 00:06:41,230
so there were three ffts

108
00:06:42,480 --> 00:06:47,320
transform the inverse transform for three of these

109
00:06:47,340 --> 00:06:49,130
and log ends

110
00:06:49,150 --> 00:06:51,110
and one

111
00:06:52,400 --> 00:06:53,780
but of course

112
00:06:53,800 --> 00:06:55,500
i mean we saved

113
00:06:55,500 --> 00:07:02,190
enormous amount here if we say if this was a factor two hundred quaker

114
00:07:02,210 --> 00:07:05,230
well we had to do three of

115
00:07:05,250 --> 00:07:08,750
but we're way way way way way OK

116
00:07:08,770 --> 00:07:14,840
so that's convolution convolution so actually this is a good if i had to very

117
00:07:14,840 --> 00:07:16,650
very long numbers

118
00:07:16,670 --> 00:07:21,650
or if the computer had suppose we're somewhere

119
00:07:21,650 --> 00:07:26,650
no rounding offered cutting off its sixteen bits or even double precision suppose we're keeping

120
00:07:26,650 --> 00:07:31,320
full precision for for algebraic coding or something then

121
00:07:31,440 --> 00:07:37,420
then we have to do these locations in full

122
00:07:37,420 --> 00:07:40,800
if i what if i want to multiply a hundred and twenty eight digit number

123
00:07:40,800 --> 00:07:44,090
by another hundred twenty digit number well

124
00:07:44,110 --> 00:07:45,820
it's a convolution

125
00:07:45,840 --> 00:07:49,610
and i can use the convolution rule to do

126
00:07:54,170 --> 00:07:55,380
remaining on my

127
00:07:55,400 --> 00:07:58,980
i what's remaining online

128
00:07:59,010 --> 00:08:02,570
list for this lecture

129
00:08:02,590 --> 00:08:09,300
so circulant matrices i want to express this express the convolution as the matrix

130
00:08:09,320 --> 00:08:13,420
well one matrix is multiplying what vector to give that vector

131
00:08:13,440 --> 00:08:17,250
this is nice is happy that we see the three the three

132
00:08:17,280 --> 00:08:22,530
components of the answer now i just want to see it as a matrix multiplication

133
00:08:25,780 --> 00:08:30,670
some metrics

134
00:08:30,690 --> 00:08:36,440
is multiplying some back to give these three is the vector of values

135
00:08:37,980 --> 00:08:39,300
this is

136
00:08:39,300 --> 00:08:41,190
this is the convolution

137
00:08:41,280 --> 00:08:43,230
three components

138
00:08:43,230 --> 00:08:47,230
you could delete the parentheses around them if you like

139
00:08:47,270 --> 00:08:50,170
really it's just three components

140
00:08:50,250 --> 00:08:55,170
and i claim that there is

141
00:08:55,190 --> 00:08:57,230
some matrix

142
00:08:57,250 --> 00:08:59,670
of seas

143
00:08:59,730 --> 00:09:02,150
multiplying some vector of these

144
00:09:02,170 --> 00:09:05,840
these are all d one d two

145
00:09:05,860 --> 00:09:14,090
what's the what's in the first row of the matrix well it must be seen

146
00:09:18,750 --> 00:09:21,840
was the first roman matrix

147
00:09:21,880 --> 00:09:24,270
these are small by the zero

148
00:09:24,280 --> 00:09:27,150
c one should multiply the two

149
00:09:27,170 --> 00:09:30,840
and c two notion of mobility one so

150
00:09:30,860 --> 00:09:34,820
the role of financial the columns

151
00:09:34,840 --> 00:09:37,980
these are all multiplies what

152
00:09:38,010 --> 00:09:43,050
what's the column what's the zeroth column here that these are all multiplies what these

153
00:09:43,050 --> 00:09:48,840
are all i see zero c one and

154
00:09:49,530 --> 00:09:51,690
c two b

155
00:09:51,780 --> 00:09:55,650
and the second one

156
00:09:55,670 --> 00:09:57,650
d one multiply

157
00:10:00,070 --> 00:10:01,730
is zero

158
00:10:01,730 --> 00:10:03,750
and c one

159
00:10:03,780 --> 00:10:07,210
and the third one day two more

160
00:10:09,690 --> 00:10:12,030
this work to do

161
00:10:12,030 --> 00:10:15,530
are you policy one right

162
00:10:15,550 --> 00:10:20,480
c two and these are all well look

163
00:10:20,500 --> 00:10:24,400
the pattern is

164
00:10:24,420 --> 00:10:29,820
this is my circulant matrix this is my second matrix time my vector

165
00:10:30,730 --> 00:10:33,300
the same result as

166
00:10:33,340 --> 00:10:37,210
c involved with d

167
00:10:37,230 --> 00:10:39,570
that's that's my point

168
00:10:39,630 --> 00:10:42,820
these matrices and matrix of that time

169
00:10:42,840 --> 00:10:44,710
of that type is a

170
00:10:44,750 --> 00:10:48,400
convolution matrix

171
00:10:48,420 --> 00:10:50,570
and what do i mean of that type

172
00:10:50,670 --> 00:10:53,210
i mean that

173
00:10:53,230 --> 00:10:56,920
down the it's human we use the word templates

174
00:10:56,940 --> 00:11:01,210
four constant diagonals but this is constant diagonals

175
00:11:01,270 --> 00:11:04,150
modulo three

176
00:11:04,150 --> 00:11:08,770
the first goal is the same as the minus two diagonal

177
00:11:08,780 --> 00:11:16,630
and the second diagonal is the same as the minus one i

178
00:11:16,650 --> 00:11:24,730
so this is that this is a circulant matrix

179
00:11:30,070 --> 00:11:31,730
it's a convolution

180
00:11:31,750 --> 00:11:34,380
the second

181
00:11:34,440 --> 00:11:38,460
cyclic convolution matrix

182
00:11:38,500 --> 00:11:43,510
convolution with wraparound whatever whatever

183
00:11:43,530 --> 00:11:45,320
adjective we

184
00:11:46,650 --> 00:11:49,150
his most expressive

185
00:11:49,170 --> 00:11:52,980
six click periodic wraparound

186
00:11:53,000 --> 00:11:54,630
those are all telling us

187
00:11:55,480 --> 00:12:00,630
it's not five independent diagonals because we don't want five into we don't have five

188
00:12:00,630 --> 00:12:04,210
independent things it's three independent

189
00:12:04,250 --> 00:12:05,920
numbers there

190
00:12:06,780 --> 00:12:09,090
component zero c one c two

191
00:12:11,940 --> 00:12:17,840
i think it's it's really valuable because filters in the next lecture

192
00:12:17,840 --> 00:12:19,880
this afternoon

193
00:12:19,940 --> 00:12:21,900
are going to be

194
00:12:21,940 --> 00:12:24,800
toeplitz matrices

195
00:12:24,820 --> 00:12:27,480
and you'll see

196
00:12:28,670 --> 00:12:30,440
they could be infinite

197
00:12:30,460 --> 00:12:32,650
so no wrap

198
00:12:32,650 --> 00:12:37,550
so on so it's really polynomially smaller than and to love this period

199
00:12:37,700 --> 00:12:41,260
we can't handle the willow case that's a little bit too strong this thing is

200
00:12:41,260 --> 00:12:44,990
really quite a bit smaller

201
00:12:45,010 --> 00:12:47,990
OK but the answer then is really simple

202
00:12:48,010 --> 00:12:49,130
t event

203
00:12:49,150 --> 00:12:53,240
is the data and the largest event

204
00:12:55,490 --> 00:12:57,420
that's case one

205
00:13:01,610 --> 00:13:04,400
is one out n is pretty much equal

206
00:13:04,420 --> 00:13:08,840
so can someone please be very

207
00:13:08,860 --> 00:13:11,150
and by pretty much equal i mean up to

208
00:13:11,170 --> 00:13:15,010
poly log factors

209
00:13:15,030 --> 00:13:16,510
so this is

210
00:13:16,570 --> 00:13:19,610
log base two and to the power k

211
00:13:19,630 --> 00:13:22,760
should know this station

212
00:13:22,880 --> 00:13:27,320
so for example k can be zero and then they are equal up to constant

213
00:13:27,320 --> 00:13:31,610
factors that for some k greater than equals zero

214
00:13:33,840 --> 00:13:39,440
less than one that works is really important nonnegative should probably be an integer

215
00:13:39,570 --> 00:13:41,970
it doesn't actually matter whether is an integer

216
00:13:42,420 --> 00:13:47,590
but there there's could be and will always be a times blog and just nothing

217
00:13:47,590 --> 00:13:52,360
again the solution is easy here

218
00:13:52,360 --> 00:13:53,490
here then

219
00:13:53,510 --> 00:13:57,010
it is and so log

220
00:13:57,030 --> 00:13:58,070
these b

221
00:13:58,340 --> 00:14:00,570
a a times

222
00:14:00,590 --> 00:14:03,030
was only has to be at least times log k

223
00:14:03,130 --> 00:14:07,130
it turns out it's log to the k plus one

224
00:14:10,880 --> 00:14:15,320
that's case two

225
00:14:15,340 --> 00:14:16,590
one more case

226
00:14:16,610 --> 00:14:21,610
just slightly more complicated

227
00:14:21,630 --> 00:14:23,440
to some slightly more

228
00:14:24,240 --> 00:14:26,630
but history is roughly one

229
00:14:26,650 --> 00:14:28,550
f then grows bigger

230
00:14:28,570 --> 00:14:30,050
and log

231
00:14:30,070 --> 00:14:30,900
it's been there

232
00:14:30,900 --> 00:14:35,840
so should be capital mega there's one place we get mega

233
00:14:35,860 --> 00:14:40,050
log base b and what steps along

234
00:14:40,220 --> 00:14:43,920
some positive and so on

235
00:14:43,970 --> 00:14:51,050
so should grow not just bigger but polynomially bigger there was growing just log factor

236
00:14:51,070 --> 00:14:53,510
figure poly log here it's

237
00:14:53,570 --> 00:14:55,260
a polynomial factor

238
00:14:55,300 --> 00:14:59,990
here in this case we need another assumption about f

239
00:14:59,990 --> 00:15:03,700
because we were a little bit about how quickly f grows

240
00:15:03,700 --> 00:15:08,050
we we we want to make sure that as you go down the recursion f

241
00:15:08,050 --> 00:15:09,340
gets smaller

242
00:15:09,360 --> 00:15:14,530
can i said that get smaller as you go down otherwise you're trying to some

243
00:15:14,530 --> 00:15:16,110
to infinity or whatever

244
00:15:20,240 --> 00:15:25,050
i see this some on prime

245
00:15:25,110 --> 00:15:32,420
for some epsilon crime-ridden zero so what i'd like is that if i just sort

246
00:15:32,420 --> 00:15:35,990
of take the recurrence this here and

247
00:15:36,050 --> 00:15:39,780
and just throw in as instead i get you know

248
00:15:39,800 --> 00:15:41,010
f of and

249
00:15:41,030 --> 00:15:45,800
should be somehow related to eight times for over b what i'd like is it

250
00:15:45,840 --> 00:15:49,990
for them which is at the top of the recursion tree should be bigger

251
00:15:50,880 --> 00:15:53,510
the thing at the next level down the sum

252
00:15:53,530 --> 00:15:57,450
of all the values of the next level down should be bigger by some constant

253
00:15:58,340 --> 00:16:02,910
so here i have the the next level down is the most someone minus epsilon

254
00:16:02,910 --> 00:16:05,340
something strictly less than one

255
00:16:05,360 --> 00:16:07,260
some constant strictly less than one

256
00:16:07,280 --> 00:16:09,450
times the thing at the top

257
00:16:09,470 --> 00:16:15,780
i need that to make sure things are getting smaller as i go down then

258
00:16:15,800 --> 00:16:19,110
human is data from

259
00:16:20,860 --> 00:16:23,280
that's the theory is the master

260
00:16:23,300 --> 00:16:29,030
theorem what everyone calls not named after some dining masters just it's master all methods

261
00:16:29,030 --> 00:16:34,880
because it's very easy to apply

262
00:16:36,360 --> 00:16:38,240
so let's apply a few times

263
00:16:38,260 --> 00:16:41,650
it's a bit to it much to take in all at once

264
00:16:41,650 --> 00:16:45,150
and then we'll give you a sketch of the proof

265
00:16:45,170 --> 00:16:48,420
to see it's really not that surprising that this is true

266
00:16:48,450 --> 00:16:50,650
if you look at the recursion tree

267
00:16:50,740 --> 00:16:53,510
but first this just try using it

268
00:16:53,530 --> 00:16:59,240
so for example we can take him and his four times t about two

269
00:16:59,650 --> 00:17:02,780
class and

270
00:17:02,800 --> 00:17:05,400
OK this is a

271
00:17:05,400 --> 00:17:07,360
this is the

272
00:17:07,380 --> 00:17:09,050
this is after that

273
00:17:09,700 --> 00:17:16,630
so the first thing we should compute is and the log based bank

274
00:17:16,670 --> 00:17:21,240
this i think even i can do log base two of four log base two

275
00:17:21,240 --> 00:17:26,240
like this is an square

276
00:17:26,260 --> 00:17:30,860
OK so is f of n smaller or bigger than n square fenders and that's

277
00:17:30,860 --> 00:17:36,340
where it is clearly bigger by a polynomial factor so we're in case one

278
00:17:37,320 --> 00:17:47,670
so what's the answer

279
00:17:47,700 --> 00:17:50,280
and square

280
00:17:50,280 --> 00:17:58,260
it's data and a lot of experience which is here just squared

281
00:18:03,360 --> 00:18:08,940
there some slight variations on going to keep can be the same and just change

282
00:18:13,240 --> 00:18:17,470
let's say to you is four times here and another two

283
00:18:17,470 --> 00:18:24,170
class and square and is like trail in spelling so m squared is is asymptotically

284
00:18:24,170 --> 00:18:28,410
and that small fortunes diffuse flow they change how the shape

285
00:18:28,430 --> 00:18:32,250
changes and that feedback on how the more

286
00:18:32,270 --> 00:18:35,730
hello really it's this

287
00:18:35,770 --> 00:18:38,150
this complex feedbacks between

288
00:18:38,790 --> 00:18:42,650
the the sort of control signals and the structure is being controlled the brings about

289
00:18:42,730 --> 00:18:46,930
the problem of understanding development

290
00:18:48,190 --> 00:18:49,350
a systemic

291
00:18:50,870 --> 00:18:54,370
complex feedbacks

292
00:18:54,450 --> 00:18:57,730
so what this is what i've been talking about

293
00:18:57,970 --> 00:19:11,430
you can see something done

294
00:19:11,450 --> 00:19:24,210
so i think it's quite easy to be glib about these

295
00:19:29,370 --> 00:19:32,350
you know you can say all those the termite mound

296
00:19:32,370 --> 00:19:33,950
that's very big

297
00:19:33,990 --> 00:19:36,050
you can say who does the brain

298
00:19:36,610 --> 00:19:41,610
it's a very you know that powerful computer and you can think about development well

299
00:19:41,610 --> 00:19:46,450
OK start from a single cell and when you grow up person but really when

300
00:19:46,450 --> 00:19:47,310
you look at

301
00:19:47,330 --> 00:19:50,310
when you look at that process unfolding it is

302
00:19:50,350 --> 00:19:54,050
almost unimaginably difficult to understand how

303
00:19:54,090 --> 00:19:59,710
you can start from that very very simple beginning and end up with a thinking

304
00:19:59,770 --> 00:20:03,050
little foetus

305
00:20:03,110 --> 00:20:08,210
you know with with its heart beating and its regulatory system in its nervous network

306
00:20:08,440 --> 00:20:15,320
you know what when i when i studied

307
00:20:15,430 --> 00:20:19,310
psychology as an undergraduate

308
00:20:19,540 --> 00:20:23,030
this is what they should this is pretty much all they showed us about development

309
00:20:23,110 --> 00:20:24,130
because it was

310
00:20:24,170 --> 00:20:26,850
people just didn't know how to explain it to us

311
00:20:26,910 --> 00:20:28,710
i mean it probably did know stuff

312
00:20:28,750 --> 00:20:32,210
but i don't think the psychologist that told me knew anything about development so he

313
00:20:32,210 --> 00:20:36,410
got this the picture for sort of classic picture of embryo genesis

314
00:20:36,910 --> 00:20:42,350
you've got columns this column here is three stages in the development of

315
00:20:42,370 --> 00:20:46,690
fish that says because of the all these words are in german down here you've

316
00:20:46,690 --> 00:20:50,470
got three stages in the development of mensch so people

317
00:20:50,510 --> 00:20:53,310
like in in between you've got

318
00:20:53,350 --> 00:21:00,310
clinching which are rabbits i think really it's not so can should find is the

319
00:21:01,390 --> 00:21:03,470
who is the chicken

320
00:21:05,120 --> 00:21:06,630
is a salamander

321
00:21:06,650 --> 00:21:10,650
i kinda see what this was but i reckon i must say think i think

322
00:21:10,650 --> 00:21:13,930
there must be a tortoise right because it's got kind of cute shell on its

323
00:21:13,930 --> 00:21:16,450
back but i don't know what that word is

324
00:21:16,490 --> 00:21:21,140
sorry to someone know what the what the german for tortoise is right

325
00:21:22,830 --> 00:21:29,530
the thing to notice here is that even though we're looking at a ridiculously diverse

326
00:21:29,530 --> 00:21:34,290
set of creatures during development

327
00:21:34,370 --> 00:21:38,230
they they don't differentiate really until very late on the share a lot of the

328
00:21:38,230 --> 00:21:41,210
same developmental stages

329
00:21:41,250 --> 00:21:45,990
so it so that the rams home this notion that

330
00:21:46,330 --> 00:21:49,770
you know we also probably share a huge number of genes

331
00:21:49,850 --> 00:21:52,210
with across these creatures

332
00:21:52,230 --> 00:21:54,470
so there's a sense that

333
00:21:54,530 --> 00:21:59,470
the kind of DNA centric view that we've we've been five people have tried to

334
00:21:59,470 --> 00:22:00,470
sell us

335
00:22:01,190 --> 00:22:04,850
it's just not going to answer the questions of interest

336
00:22:04,890 --> 00:22:08,490
the interest is not going to be how many genes there are on the genome

337
00:22:08,620 --> 00:22:13,490
what those genes are or even what proteins those genes code for how those proteins

338
00:22:13,490 --> 00:22:16,490
fold into a certain shape or even how those shapes

339
00:22:17,090 --> 00:22:18,950
clunk together

340
00:22:18,970 --> 00:22:20,170
it's going to be

341
00:22:20,190 --> 00:22:23,610
you know that there's a chain of reasoning up to higher and higher structures and

342
00:22:23,610 --> 00:22:27,600
the feedbacks between that's going to have to be addressed and

343
00:22:27,670 --> 00:22:33,750
the amount of money that was spent developing techniques for bioinformatics i think it's really

344
00:22:33,750 --> 00:22:35,050
even touched on

345
00:22:35,090 --> 00:22:38,930
the methods the new methods that we're going to have to generate

346
00:22:38,970 --> 00:22:42,150
if you wanted to cross the

347
00:22:42,310 --> 00:22:43,630
but that one

348
00:22:45,130 --> 00:22:49,850
so got one more slide on development in some sense the thing that's missing from

349
00:22:49,850 --> 00:22:56,030
the DNA story is the recognition of the physics and chemistry that support the processes

350
00:22:56,030 --> 00:22:58,070
that bring about development

351
00:22:58,290 --> 00:23:04,350
so in some sense physics and chemistry define a world space forms that of possible

352
00:23:04,350 --> 00:23:07,780
which forms are closely related to each other

353
00:23:07,870 --> 00:23:13,770
and development is like this of course trajectory through that space

354
00:23:13,790 --> 00:23:19,510
not only certain trajectories are admissible you can't build you can't grow a machine gun

355
00:23:19,550 --> 00:23:22,630
well don't like to show

356
00:23:22,670 --> 00:23:24,430
so is it

357
00:23:24,530 --> 00:23:31,580
the wraps study of a small example of this kind of ideas so right wrote

358
00:23:32,770 --> 00:23:38,250
with only had three with three parameters that he could vary he could simulation could

359
00:23:38,250 --> 00:23:39,770
generate every

360
00:23:39,790 --> 00:23:43,190
the snail shell that we've ever seen

361
00:23:43,270 --> 00:23:46,270
OK just three parameters that government

362
00:23:46,330 --> 00:23:50,010
the rate at which they turn on the extradition of this and i don't know

363
00:23:50,010 --> 00:23:55,910
what they say so there's three d kind of more space which this cube represents

364
00:23:55,970 --> 00:23:59,370
kind point this cube generates a certain

365
00:23:59,470 --> 00:24:00,750
shell shade

366
00:24:00,770 --> 00:24:05,590
he's shaded in problem you might not be able to see what he's shaded in

367
00:24:05,590 --> 00:24:09,470
the parts of the tube but we have seen in real life

368
00:24:09,590 --> 00:24:15,090
so there's class of shells which is this kind of area the top there's another

369
00:24:15,090 --> 00:24:18,750
class of shells which is down here and there's another class which is this dark

370
00:24:18,750 --> 00:24:23,850
green area which she was quite large areas of this space where we've never seen

371
00:24:23,850 --> 00:24:25,430
a shell like that

372
00:24:25,490 --> 00:24:27,790
there are predicted to be

373
00:24:27,890 --> 00:24:29,070
kind of possible

374
00:24:29,130 --> 00:24:31,210
on the back model but we haven't seen them

375
00:24:31,730 --> 00:24:36,610
and this is the this is the only real example of

376
00:24:36,630 --> 00:24:39,830
people being able to successfully

377
00:24:39,850 --> 00:24:45,930
characterise this more prospects i've come across very very simple you know just looking at

378
00:24:46,010 --> 00:24:49,130
shell structure we're not even looking at the

379
00:24:49,220 --> 00:24:52,700
the creature that lives in the shell you know the shell isn't even in part

380
00:24:52,700 --> 00:24:55,830
the to the biological part of the creature

381
00:24:55,850 --> 00:24:58,570
so here's a bunch of hydra

382
00:24:58,630 --> 00:25:06,610
which are closely related aquatic life forms that brian goodwin has studied

383
00:25:06,760 --> 00:25:10,710
i even though these are very closely related they're all part of the same class

384
00:25:10,710 --> 00:25:18,250
of organism they're ridiculously diverse in their structure and the notion is that

385
00:25:18,310 --> 00:25:22,310
there are it's you can you can't really account for this diversity in terms of

386
00:25:22,310 --> 00:25:27,970
genetic diversity they're very genetically they're genetically very very similar to what accounted for this

387
00:25:27,970 --> 00:25:31,770
is that small changes in genes kind of

388
00:25:31,810 --> 00:25:32,750
open up

389
00:25:32,790 --> 00:25:36,210
different parts of this more respect

390
00:25:36,270 --> 00:25:42,450
so you end up with no coincidence with shallow and little nodules on the end

391
00:25:42,450 --> 00:25:47,630
of stems and what have you done although this shell space is fairly intelligible you

392
00:25:47,630 --> 00:25:53,190
know as we move from here upwards the shells get squatter or

393
00:25:53,210 --> 00:26:00,150
tighter something more space isn't going to be intuitive in that sense even even for

394
00:26:00,150 --> 00:26:04,510
the physical structures of organisms can be far more complicated

395
00:26:04,510 --> 00:26:10,180
either that network flow problems or matching problems things like that

396
00:26:10,240 --> 00:26:14,300
they have a special structure that that you know in the supply known theorems and

397
00:26:15,660 --> 00:26:16,680
this is true

398
00:26:16,700 --> 00:26:19,390
it's modular and training objective

399
00:26:19,410 --> 00:26:21,700
we're going to get into answer

400
00:26:22,990 --> 00:26:24,610
thank you

401
00:26:25,890 --> 00:26:27,410
so rule

402
00:26:27,430 --> 00:26:31,070
so we have

403
00:26:31,220 --> 00:26:38,840
good reference for that i think that but nemhauser

404
00:26:38,840 --> 00:26:43,860
and also

405
00:26:43,860 --> 00:26:45,760
so in the book

406
00:26:45,880 --> 00:26:51,340
the talks about but this kind of stuff and and there's a few other conditions

407
00:26:51,760 --> 00:26:58,200
where you can guarantee integrality but but this is by far the most the most

408
00:26:58,220 --> 00:26:59,800
the surface

409
00:27:01,280 --> 00:27:03,090
OK good so

410
00:27:03,090 --> 00:27:05,010
we have a way to write down

411
00:27:05,070 --> 00:27:09,970
the objective and the constraints in in get a linear programs

412
00:27:09,990 --> 00:27:11,800
q said that

413
00:27:11,880 --> 00:27:13,380
is is less than b

414
00:27:13,390 --> 00:27:16,800
that is an integral and so meaning that answers

415
00:27:16,840 --> 00:27:21,070
has the same score as the the otherwise score so we have lost it

416
00:27:21,110 --> 00:27:23,840
same thing here

417
00:27:23,860 --> 00:27:29,260
not sure how many of you passing so go faster this so here you have

418
00:27:30,110 --> 00:27:34,860
this context rules a goes to c and where it starts

419
00:27:34,890 --> 00:27:37,240
and singh were severed splits

420
00:27:37,990 --> 00:27:40,620
so basically these bands st

421
00:27:40,640 --> 00:27:42,110
and c

422
00:27:42,120 --> 00:27:44,570
spence employees want to eat

423
00:27:45,360 --> 00:27:47,300
the chart representation of this

424
00:27:47,320 --> 00:27:49,360
right where he basically

425
00:27:50,740 --> 00:27:54,070
the span from three to five is

426
00:27:54,070 --> 00:27:55,260
covered by

427
00:27:55,280 --> 00:27:56,320
and he

428
00:27:56,340 --> 00:27:58,140
and then

429
00:27:58,160 --> 00:28:02,640
so we will have variables for each span so that can be inscribed in the

430
00:28:03,660 --> 00:28:04,950
three eight spam

431
00:28:04,970 --> 00:28:10,140
they're going to be variable for each possible nonterminal variables that says is that

432
00:28:10,620 --> 00:28:13,990
spam covered by the nonterminals

433
00:28:14,010 --> 00:28:15,890
same thing that for productions

434
00:28:15,890 --> 00:28:18,010
so for each

435
00:28:18,030 --> 00:28:23,340
triplet actually start middle and so there would be

436
00:28:26,490 --> 00:28:33,590
and actually want to seven so what production was that right so this kind of

437
00:28:33,590 --> 00:28:35,660
thing a b c start

438
00:28:36,700 --> 00:28:39,570
they did so super important the ideas though

439
00:28:39,590 --> 00:28:42,320
you can write down the score again like this

440
00:28:42,360 --> 00:28:45,470
you have two sets of variables

441
00:28:45,490 --> 00:28:47,780
at the route to make sure that

442
00:28:47,840 --> 00:28:52,490
the entire sentence from here to and is covered by some nonterminal so somewhere is

443
00:28:52,490 --> 00:28:53,470
equal to one

444
00:28:53,490 --> 00:28:58,390
this is called school inside constraints we say

445
00:28:59,280 --> 00:29:04,160
if i have a covering has to be there

446
00:29:04,160 --> 00:29:09,200
it is going to produce something so as a sum over all possible midpoints and

447
00:29:09,200 --> 00:29:11,510
all possible productions have been c

448
00:29:11,510 --> 00:29:15,200
this is going to be on is one of them is going to be exactly

449
00:29:15,200 --> 00:29:16,160
one of these

450
00:29:17,510 --> 00:29:21,610
when we think about and the other way around if if

451
00:29:21,620 --> 00:29:26,680
s two s and he's covered by age it must be that

452
00:29:26,700 --> 00:29:29,700
this was generated by somebody else either from

453
00:29:29,780 --> 00:29:30,950
from the

454
00:29:32,570 --> 00:29:39,590
and so some of you know somewhere after some generated a four

455
00:29:39,610 --> 00:29:42,360
somewhere before it was catching

456
00:29:42,380 --> 00:29:47,070
so this this is that is going to produce offspring and this is that

457
00:29:47,090 --> 00:29:49,660
it is not an orphan you know is generated by some

458
00:29:49,680 --> 00:29:50,780
roughly speaking

459
00:29:51,700 --> 00:29:56,200
so it turns out these linear constraints again to ensure that

460
00:29:56,510 --> 00:29:59,610
this linear programme is going to give us

461
00:29:59,620 --> 00:30:01,390
integral solution only part

462
00:30:01,390 --> 00:30:05,570
i'm not suggesting you pass linear programs that would be

463
00:30:05,990 --> 00:30:11,280
city so but the point is that you can write down there

464
00:30:11,430 --> 00:30:13,550
objective and constraint in this way and

465
00:30:13,570 --> 00:30:16,360
guarantee that the solutions to

466
00:30:16,380 --> 00:30:19,200
the other thing to note in all these things that i was talking about the

467
00:30:19,200 --> 00:30:25,780
number of variables and constraints is linear in the number of parts so everything is

468
00:30:26,110 --> 00:30:33,450
small as compared to the number of possibilities that we should consider

469
00:30:33,470 --> 00:30:35,450
during the time

470
00:30:59,220 --> 00:31:04,620
so this is the last thing actually

471
00:31:04,700 --> 00:31:10,010
again here this is this is a very simple one we have the decay

472
00:31:10,030 --> 00:31:14,470
since the mapping to logic gates there's no one to one mapping between licensees

473
00:31:14,490 --> 00:31:20,240
positive and the degree constraint right that at most one of the maps and most

474
00:31:20,240 --> 00:31:21,380
one of the maps

475
00:31:22,320 --> 00:31:25,360
and we again we don't constraint equals to one

476
00:31:25,360 --> 00:31:28,770
as a function of the number of rounds but i've switched to a logarithmic scale

477
00:31:29,990 --> 00:31:34,270
if you just run c four point five by itself it gets the test error

478
00:31:34,270 --> 00:31:39,290
rate of about thirteen percent that's the line with at the top there

479
00:31:39,290 --> 00:31:44,420
as expected the training year goes down very quickly in fact after only five rounds

480
00:31:44,800 --> 00:31:49,880
the training area is zero we perfectly the training set

481
00:31:49,890 --> 00:31:51,400
what about the test error

482
00:31:51,420 --> 00:31:52,860
well the test there

483
00:31:52,860 --> 00:31:58,340
also drops as expected but it keeps on dropping dropping dropping dropping even after thousand

484
00:31:58,340 --> 00:32:01,890
rounds the test there has not started to go up again

485
00:32:01,910 --> 00:32:04,890
and this is kind of amazing because

486
00:32:04,890 --> 00:32:09,650
active run this per thousand rounds you're talking about some more than two million decision

487
00:32:09,650 --> 00:32:14,180
tree nodes so in terms of just the raw number of parameters

488
00:32:14,190 --> 00:32:20,880
this model is absolutely enormous and extremely complicated and yet it's giving very very good

489
00:32:20,880 --> 00:32:23,980
performance as we see over here

490
00:32:24,060 --> 00:32:28,780
what's even more surprising is that protesters continuing to drop even after the training year

491
00:32:29,230 --> 00:32:30,810
is zero

492
00:32:31,580 --> 00:32:35,100
if you look at the training year after five rounds the training year zero and

493
00:32:35,100 --> 00:32:37,270
it stays zero

494
00:32:37,270 --> 00:32:40,210
now if you think about occam's razor predicts

495
00:32:40,230 --> 00:32:42,080
here you have one model

496
00:32:42,080 --> 00:32:45,420
which consists of five decision trees

497
00:32:46,270 --> 00:32:48,080
accuracy on the training set

498
00:32:48,080 --> 00:32:51,250
here's another model two hundred times bigger

499
00:32:51,250 --> 00:32:56,230
it's a thousand decision trees also perfect accuracy on the training set

500
00:32:56,250 --> 00:32:59,080
this one two hundred times more complex

501
00:32:59,100 --> 00:33:03,460
occam's razor says this one should certainly give better test performance

502
00:33:03,460 --> 00:33:07,600
but we see exactly the opposite happening instead the test there's a point four percent

503
00:33:08,270 --> 00:33:11,370
and about three percent here

504
00:33:11,400 --> 00:33:15,270
so i can raise are just seems wrong in this case

505
00:33:15,290 --> 00:33:18,370
good question

506
00:33:18,620 --> 00:33:30,250
i don't know how you measure that it's real data this is actually

507
00:33:30,270 --> 00:33:34,440
well stated that derived from OCR data

508
00:33:36,150 --> 00:33:41,730
eight four cold

509
00:33:45,370 --> 00:33:52,790
well are supposed to be the right answers but usually with any training set there

510
00:33:52,790 --> 00:33:55,920
is almost always a mistake of some kind

511
00:33:55,940 --> 00:34:01,000
so it's so nobody has gone in and deliberately added noise that's what you're asking

512
00:34:01,980 --> 00:34:05,460
because it's real data i would expect that there are probably there might well be

513
00:34:05,460 --> 00:34:10,230
mistakes and some ambiguous examples it but i don't know for sure

514
00:34:23,130 --> 00:34:31,850
right well e so your intuition is right so if you do an experiment was

515
00:34:31,850 --> 00:34:35,500
going to talk about this later but if you do an experiment where you deliberately

516
00:34:35,500 --> 00:34:36,980
add noise

517
00:34:36,980 --> 00:34:38,810
to the data

518
00:34:38,810 --> 00:34:43,000
when you add that is uniformly over your entire space

519
00:34:43,000 --> 00:34:48,330
then then that does cause problems for adaboost because what it does

520
00:34:48,350 --> 00:34:53,350
like you're getting at is that it focuses on the hardest examples so it's really

521
00:34:54,270 --> 00:34:57,620
spinning its wheels on these really hard examples

522
00:34:57,670 --> 00:35:01,080
and so that does happen and there's been various research on how to

523
00:35:01,100 --> 00:35:04,730
try to make it more resistant to that kind of noise for robust to that

524
00:35:04,730 --> 00:35:07,600
kind of noise but on the other hand

525
00:35:07,670 --> 00:35:11,920
you know that's an artificial experiment where you artificially and noise on the other hand

526
00:35:11,920 --> 00:35:17,250
on actual data sets which are certain certainly have some kind of noise and it's

527
00:35:17,250 --> 00:35:22,730
performing well which suggests to me that our model of noise

528
00:35:22,770 --> 00:35:26,980
is not very good the model of noise that says you have people noise everywhere

529
00:35:26,980 --> 00:35:31,190
in your space probably is not realistic prior what's more realistic is that there are

530
00:35:31,210 --> 00:35:35,730
some examples near the true decision boundary whatever that means

531
00:35:35,750 --> 00:35:36,500
which are

532
00:35:36,550 --> 00:35:38,170
more likely to

533
00:35:38,210 --> 00:35:43,080
more susceptible to noise than ones which are far

534
00:35:47,230 --> 00:35:55,900
OK that's one t epsilon t is the area of the weak classifiers

535
00:35:55,920 --> 00:36:00,870
OK if the weak classifiers in the this is the area of the combined classifier

536
00:36:02,100 --> 00:36:06,400
even after the combined classifier has training year zero

537
00:36:08,500 --> 00:36:10,620
weak classifiers

538
00:36:10,670 --> 00:36:15,600
i still have significant air so in this case the weak classifiers happy run this

539
00:36:15,600 --> 00:36:19,900
a few times the weak classifiers will have air somewhere around thirty percent

540
00:36:19,920 --> 00:36:26,170
OK so that stays large so you can continue reweighting the examples and so on

541
00:36:36,250 --> 00:36:37,920
well neural networks

542
00:36:39,060 --> 00:36:46,250
neural networks are quite susceptible to overfitting decision tree algorithms are susceptible to overfitting

543
00:36:47,100 --> 00:36:51,670
it depends here so other

544
00:36:51,830 --> 00:36:57,150
ensemble there are other ensemble method algorithms which do not overfit so for instance there's

545
00:36:57,150 --> 00:36:59,690
bagging and random forests

546
00:36:59,730 --> 00:37:03,830
they will also tend not to over fit you'll tend to get behavior somewhat like

547
00:37:06,540 --> 00:37:08,250
it depends on the algorithm

548
00:37:09,350 --> 00:37:11,830
but this

549
00:37:11,850 --> 00:37:14,750
does that into question

550
00:37:14,750 --> 00:37:19,340
so the inner portion of the solenoid and that's not changing

551
00:37:19,350 --> 00:37:22,620
and so when i changed the shape of this outer loop you will not see

552
00:37:22,620 --> 00:37:23,960
any change

553
00:37:24,020 --> 00:37:25,110
in the

554
00:37:26,660 --> 00:37:27,560
i hope you

555
00:37:27,560 --> 00:37:30,630
it doesn't confuse you i'm going to purposely change the

556
00:37:30,640 --> 00:37:32,820
the size of the

557
00:37:33,200 --> 00:37:35,430
so i'm going to do that now

558
00:37:35,750 --> 00:37:38,340
to see there

559
00:37:38,380 --> 00:37:39,650
very sensitive

560
00:37:39,660 --> 00:37:46,400
and commuter

561
00:37:47,910 --> 00:37:51,490
you're going to see here

562
00:37:51,540 --> 00:37:55,590
this loop

563
00:37:55,640 --> 00:37:57,490
big choir

564
00:37:57,540 --> 00:37:59,600
and i'm going to just put it

565
00:37:59,650 --> 00:38:01,410
over this

566
00:38:03,160 --> 00:38:04,140
the first

567
00:38:04,150 --> 00:38:06,550
make sure to my

568
00:38:06,610 --> 00:38:10,700
beta which is extremely sensitive i can zero it

569
00:38:10,830 --> 00:38:14,300
signed sensitivity the current goes in one direction you will see the needle going one

570
00:38:15,780 --> 00:38:18,830
if the current go in directions you'll see

571
00:38:18,880 --> 00:38:20,620
the change

572
00:38:20,630 --> 00:38:22,640
so now i put this

573
00:38:22,650 --> 00:38:25,880
loop around here

574
00:38:25,930 --> 00:38:28,500
crazy shape this loop

575
00:38:28,610 --> 00:38:30,780
so it's around this

576
00:38:30,790 --> 00:38:33,130
so it once

577
00:38:33,140 --> 00:38:36,710
so the magnetic field is inside the solenoid

578
00:38:36,740 --> 00:38:41,420
so think of the surface which is attached to this crazy loop

579
00:38:41,430 --> 00:38:44,280
and now i'm going to

580
00:38:44,280 --> 00:38:48,680
during the current all and only while the current is changing will be changing magnetic

581
00:38:48,680 --> 00:38:52,000
flux only doing that portion will use the current flow

582
00:38:52,030 --> 00:38:54,850
three two one zero

583
00:38:54,890 --> 00:38:59,230
i will break the current three two one zero

584
00:38:59,280 --> 00:39:04,910
when do the direction

585
00:39:04,970 --> 00:39:07,740
if i change the size of the loop

586
00:39:07,790 --> 00:39:09,810
i'm making no difference

587
00:39:09,840 --> 00:39:13,940
much smaller makes no difference for reasons that i explained to you because the magnetic

588
00:39:14,750 --> 00:39:16,900
is not the tournament in this case

589
00:39:16,950 --> 00:39:20,990
by the size of my loop but is determined by

590
00:39:21,000 --> 00:39:23,090
the solar or so if i do it again now

591
00:39:23,090 --> 00:39:25,590
it was very different shape of the

592
00:39:25,600 --> 00:39:30,110
i mean zero this again

593
00:39:30,120 --> 00:39:32,650
three two one zero

594
00:39:32,670 --> 00:39:34,570
three two one zero

595
00:39:34,610 --> 00:39:39,210
no change almost the same which is so before

596
00:39:39,240 --> 00:39:41,190
now comes something

597
00:39:41,280 --> 00:39:43,810
that may not be so intuitive to you

598
00:39:43,850 --> 00:39:48,930
i'm not going to read this wire three times around

599
00:39:48,980 --> 00:39:50,900
and so this outer loop

600
00:39:50,940 --> 00:39:52,340
these outer

601
00:39:52,380 --> 00:39:53,890
conducting wire

602
00:39:53,900 --> 00:39:55,280
it's not like this

603
00:39:58,440 --> 00:40:01,300
something like that

604
00:40:01,300 --> 00:40:03,460
now i have to attach in my head

605
00:40:03,560 --> 00:40:05,060
a surface

606
00:40:05,060 --> 00:40:06,570
two this

607
00:40:08,520 --> 00:40:10,890
my god what does it look like

608
00:40:10,910 --> 00:40:12,960
what a ridiculous surface

609
00:40:12,970 --> 00:40:16,360
well that's your problem not fire this problems

610
00:40:16,400 --> 00:40:18,050
how can you imagine

611
00:40:18,050 --> 00:40:20,540
that is the surface attached to this loop

612
00:40:20,580 --> 00:40:23,410
well i think the whole thing independent so

613
00:40:23,430 --> 00:40:25,130
take it out and see what you see

614
00:40:25,140 --> 00:40:27,780
the so attach everywhere

615
00:40:27,820 --> 00:40:30,190
on the conducting loop

616
00:40:30,210 --> 00:40:33,930
and if there is room for like this

617
00:40:34,020 --> 00:40:36,690
going up like a spiral staircase

618
00:40:36,730 --> 00:40:39,840
you're going surface it goes up like this

619
00:40:39,920 --> 00:40:44,540
but the magnetic fields go through all three of them

620
00:40:46,430 --> 00:40:48,190
the change in magnetic

621
00:40:49,110 --> 00:40:51,920
we'll go three times taller surface now

622
00:40:52,720 --> 00:40:54,060
finally says

623
00:40:54,060 --> 00:40:57,400
i find that you're going to see three times the EMF that you would see

624
00:40:57,400 --> 00:40:59,130
if there were only one loop

625
00:40:59,170 --> 00:41:01,410
and if you go thousand times around

626
00:41:01,470 --> 00:41:06,140
you get a thousand times the EMF of one will not so intuitive

627
00:41:07,480 --> 00:41:09,910
i'm around now once

628
00:41:09,920 --> 00:41:13,630
i go around twice

629
00:41:13,670 --> 00:41:18,570
and i go around the third time

630
00:41:18,570 --> 00:41:23,140
i three loops around it now

631
00:41:23,150 --> 00:41:25,630
i can see that but that's not so important

632
00:41:25,630 --> 00:41:27,590
three two one zero

633
00:41:27,640 --> 00:41:29,970
so much larger currents

634
00:41:29,990 --> 00:41:34,700
about three times larger with the math is three times larger i break the current

635
00:41:34,750 --> 00:41:37,870
you see three times larger and this is the idea behind

636
00:41:39,360 --> 00:41:41,400
you can get any

637
00:41:41,460 --> 00:41:44,140
imagine that while that you want to

638
00:41:44,180 --> 00:41:45,440
by having

639
00:41:45,490 --> 00:41:46,880
many many

640
00:41:46,880 --> 00:41:50,440
no you can get a two thousands of false

641
00:41:50,450 --> 00:41:55,640
and that's not so intuitive

642
00:41:55,650 --> 00:41:57,020
so far this law

643
00:41:57,020 --> 00:41:59,370
it's very nonintuitive

644
00:41:59,380 --> 00:42:01,420
the usual rule

645
00:42:01,480 --> 00:42:05,730
i was very intuitive piece of said when you go around the circuit

646
00:42:05,740 --> 00:42:09,190
closed loop in the goal of you don't dl is always zero

647
00:42:09,240 --> 00:42:10,150
not through

648
00:42:10,160 --> 00:42:12,240
if you have the changing

649
00:42:12,250 --> 00:42:15,400
magnetic flux

650
00:42:15,440 --> 00:42:18,800
if you ever changing magnetic flux the electric field

651
00:42:18,840 --> 00:42:20,850
inside the conducting wires

652
00:42:20,870 --> 00:42:22,170
now become

653
00:42:22,230 --> 00:42:24,210
non conservative

654
00:42:24,270 --> 00:42:29,940
he is from rule only holds as long as the electric fields are conservative

655
00:42:29,980 --> 00:42:34,940
if an electric field is conservative you go from point one to point two

656
00:42:35,080 --> 00:42:36,920
and i really don't know

657
00:42:37,000 --> 00:42:39,250
is independent of the past

658
00:42:39,270 --> 00:42:45,170
that's the potential difference between two points that uniquely defined that's no longer the case

659
00:42:45,230 --> 00:42:47,620
if you go on the ground once was

660
00:42:47,670 --> 00:42:51,900
this experiment you get opinion you may have you go three times around you get

661
00:42:51,920 --> 00:42:53,670
different about you're

662
00:42:53,690 --> 00:42:55,790
pass is now different

663
00:42:55,790 --> 00:42:57,730
and that's very nonintuitive

664
00:42:57,830 --> 00:42:58,850
because you're dealing

665
00:42:58,870 --> 00:43:04,460
was known conservative field for which we have very little feeling

666
00:43:06,060 --> 00:43:09,400
i'm going to blow your mind

667
00:43:09,440 --> 00:43:14,230
i'm going to make you see something that you won't believe

668
00:43:14,230 --> 00:43:18,540
so try to follow step by step

669
00:43:18,560 --> 00:43:21,250
leading up to this

670
00:43:22,830 --> 00:43:26,830
and very nonintuitive result

671
00:43:26,850 --> 00:43:28,960
i have here a battery

672
00:43:28,960 --> 00:43:31,370
and the battery has an EMF

673
00:43:31,380 --> 00:43:32,400
of one class

674
00:43:32,400 --> 00:43:37,020
here is the resistor

675
00:43:37,080 --> 00:43:38,580
i one

676
00:43:38,580 --> 00:43:40,710
which is one of

677
00:43:40,820 --> 00:43:45,350
user resistor

678
00:43:45,400 --> 00:43:47,640
are two

679
00:43:47,870 --> 00:43:51,770
which is nine hundred ohms

680
00:43:52,060 --> 00:43:54,730
and i'm asking you

681
00:43:54,730 --> 00:43:57,650
what is the current that is flowing around you will laugh at me you'll see

682
00:43:57,650 --> 00:43:59,500
that's almost an insult

683
00:43:59,620 --> 00:44:01,350
we should given that problem

684
00:44:01,370 --> 00:44:03,690
at the first example

685
00:44:05,230 --> 00:44:08,480
he e equals the current that is going to run

686
00:44:08,500 --> 00:44:10,650
divided by our one

687
00:44:10,650 --> 00:44:13,210
plus our

688
00:44:13,270 --> 00:44:15,640
oh my goodness what did i do

689
00:44:15,670 --> 00:44:22,290
i i forgot was law equals i ior remember not i over or

690
00:44:22,310 --> 00:44:24,420
so are we close our two

691
00:44:24,480 --> 00:44:26,370
should go upstairs

692
00:44:26,420 --> 00:44:29,810
and everything that follows is correct so you don't have to worry about that this

693
00:44:29,810 --> 00:44:36,560
was just a big slip of the pen

694
00:44:36,580 --> 00:44:38,940
and so the current i

695
00:44:38,940 --> 00:44:42,690
is ten to the minus three and these

696
00:44:42,710 --> 00:44:44,900
one million

697
00:44:44,960 --> 00:44:46,580
thank you

698
00:44:48,250 --> 00:44:51,350
current is going to flow like this

699
00:44:51,350 --> 00:44:53,710
in linear spaces

700
00:44:53,770 --> 00:44:54,610
OK of

701
00:44:54,650 --> 00:44:57,060
of linear functions in our

702
00:44:57,060 --> 00:45:00,480
it's going to become canonical once use the car

703
00:45:00,500 --> 00:45:05,130
every finite set of data points living in an infinite dimensional space is going to

704
00:45:05,130 --> 00:45:08,340
be separable

705
00:45:09,290 --> 00:45:15,540
there is no real reason to introduce slack variables at all want use the current

706
00:45:16,470 --> 00:45:20,670
we always use slack variables when music

707
00:45:22,020 --> 00:45:25,120
so there the minimum norm solution which exists

708
00:45:25,140 --> 00:45:29,380
in the reproducing kernel hilbert spaces become clearer once we see the reproducing kernel hilbert

709
00:45:29,380 --> 00:45:34,300
space and that is the regularized solution which also exists in this reproducing kernel hilbert

710
00:45:37,750 --> 00:45:42,270
you will always use in practice the regularized solution

711
00:45:42,280 --> 00:45:45,860
so the real reason these things are working is because of regularisation

712
00:45:45,880 --> 00:45:50,550
the minimum norm solution is always telling you give me the minimum norm solution that

713
00:45:50,550 --> 00:45:53,170
is consistent with the data

714
00:45:53,180 --> 00:45:59,460
so you empirical error of the minimum norm solution is always roughly speaking zero

715
00:45:59,520 --> 00:46:04,320
once use the kernel the minimum norm solution exists the maximum margin classifiers so to

716
00:46:04,320 --> 00:46:07,100
speak in this infinite dimensional space exists

717
00:46:07,110 --> 00:46:10,620
but is never actually used in practice

718
00:46:10,640 --> 00:46:17,290
so that point is sort of an important point two

719
00:46:26,950 --> 00:46:30,640
so i give it two algorithms which are motivated by

720
00:46:30,680 --> 00:46:33,930
trying to come up with the linear function that fits the data

721
00:46:33,950 --> 00:46:36,370
are tries to to classify the data

722
00:46:36,370 --> 00:46:41,130
and i showed you that there is no ordinary least squares this ordinary least squares

723
00:46:42,460 --> 00:46:48,070
they may not be unique solution therefore you have two options taken minimum norm solution

724
00:46:48,430 --> 00:46:53,140
or take this regularized least squares solution

725
00:46:53,200 --> 00:46:57,940
and actually also have tried to emphasise the minimum norm solution will always be more

726
00:46:57,940 --> 00:47:00,480
or less zero in particular solution

727
00:47:00,480 --> 00:47:03,880
the same is true for support vector machines you have the minimum norm solution which

728
00:47:03,880 --> 00:47:06,230
is the maximum margin classifiers

729
00:47:06,250 --> 00:47:08,770
which should always zero loss classifier

730
00:47:08,790 --> 00:47:12,100
or regularize classifier

731
00:47:12,210 --> 00:47:15,750
now there are other loss functions you can throw in one common loss function that

732
00:47:15,750 --> 00:47:20,060
you could actually throwing is this so-called logistic loss

733
00:47:21,580 --> 00:47:25,310
depending upon your favourite loss function you can always put that in and you have

734
00:47:25,310 --> 00:47:28,080
an optimisation problem that makes sense

735
00:47:28,100 --> 00:47:33,620
and what we would now like to to do is go beyond linear functions

736
00:47:33,650 --> 00:47:38,100
so we saw that there was the bayes optimal classifier if star

737
00:47:38,120 --> 00:47:40,620
and this is optimal classifier main

738
00:47:40,640 --> 00:47:42,370
not to be linear

739
00:47:42,400 --> 00:47:44,600
maybe some very complicated function

740
00:47:44,620 --> 00:47:46,750
so we would like to actually

741
00:47:46,790 --> 00:47:48,120
you have

742
00:47:48,190 --> 00:47:54,560
a class of functions that have the ability to represent arbitrarily complex

743
00:47:58,290 --> 00:48:03,460
so we would like to reach cluster that functions with which to make predictions

744
00:48:03,480 --> 00:48:07,650
and now there are many classes which are non-linear

745
00:48:07,670 --> 00:48:13,270
that you can use polynomials you can use trigonometric functions continuous functions differentiable functions and

746
00:48:13,270 --> 00:48:17,540
all the functional analysis now set loose on this problem

747
00:48:17,540 --> 00:48:22,190
so you might ask what properties would like from such a class

748
00:48:22,250 --> 00:48:24,850
OK so that we have a well

749
00:48:25,380 --> 00:48:30,940
behaved algorithms

750
00:48:30,960 --> 00:48:34,150
so here are some desirable properties

751
00:48:34,250 --> 00:48:37,750
so first of all it should be a rich class with good approximation part because

752
00:48:37,750 --> 00:48:41,500
that after all is the real reason why we are going to consider

753
00:48:41,790 --> 00:48:46,210
these more complicated function

754
00:48:46,270 --> 00:48:48,830
the second property we would like that

755
00:48:48,920 --> 00:48:52,650
script there should actually have some sort of the linear structure and we can add

756
00:48:52,650 --> 00:48:56,000
subtract functions and we can do nice things the kind of things you can do

757
00:48:56,000 --> 00:49:00,040
with linear functions

758
00:49:00,080 --> 00:49:03,730
so you want don't want to make complicated functions with spherical boundary that you can

759
00:49:03,750 --> 00:49:06,380
then add and so on

760
00:49:06,400 --> 00:49:09,620
and the third property we would like to have this we would like it to

761
00:49:09,620 --> 00:49:11,640
have an inner product structure

762
00:49:11,650 --> 00:49:14,000
so that we can take projections

763
00:49:14,020 --> 00:49:16,170
now if you actually follow

764
00:49:16,170 --> 00:49:21,770
the development of linear classifiers you see that taking projections is a very important step

765
00:49:21,770 --> 00:49:25,080
it's a very important step in another thing we did not discuss

766
00:49:25,080 --> 00:49:29,480
which is the perceptron learning algorithm which maybe some of you are familiar with

767
00:49:29,980 --> 00:49:32,730
it would take too much time to actually describe it to you

768
00:49:32,750 --> 00:49:37,880
but in the perceptron learning algorithm you show that it converges by actually

769
00:49:37,940 --> 00:49:42,690
making use of the inner product structure on the vector space rk

770
00:49:43,650 --> 00:49:48,080
and in many other settings you would actually like this inner product structure so that

771
00:49:48,080 --> 00:49:49,730
we have projections

772
00:49:49,750 --> 00:49:51,080
and so

773
00:49:52,710 --> 00:49:57,730
essentially would like then ultimately use hilbert space

774
00:49:57,730 --> 00:49:58,770
it's linear

775
00:49:58,770 --> 00:50:00,330
complete vector space

776
00:50:00,330 --> 00:50:02,870
with inner product OK

777
00:50:02,870 --> 00:50:07,870
and let me just say a few words about hilbert space just it's intuitively very

778
00:50:07,870 --> 00:50:11,040
clear i think some of you should know it already

779
00:50:11,080 --> 00:50:12,250
for those

780
00:50:12,270 --> 00:50:14,060
who who don't

781
00:50:14,080 --> 00:50:16,810
in our

782
00:50:16,810 --> 00:50:19,400
which is the canonical there are n

783
00:50:19,420 --> 00:50:22,290
given a vector w and given a vector u

784
00:50:22,310 --> 00:50:26,000
you have this inner product and w which is given by

785
00:50:26,000 --> 00:50:26,940
what how

786
00:50:26,960 --> 00:50:32,170
united is the w docu which is some asian WI

787
00:50:36,460 --> 00:50:38,350
and the inner product is

788
00:50:38,770 --> 00:50:41,020
over the years is going to be

789
00:50:42,520 --> 00:50:46,600
two on a space h

790
00:50:46,600 --> 00:50:51,070
there may be connections between the way wave generated the data and so on

791
00:50:51,080 --> 00:50:54,940
but if we take a sort of simple case are trying to classify cancerous tissue

792
00:50:54,940 --> 00:51:00,240
from healthy tissue we can imagine there are two distributions cells that are generated from

793
00:51:00,240 --> 00:51:03,360
the that are actually not cancerous and those that are cancerous

794
00:51:03,750 --> 00:51:10,140
and assuming there's some process by which we select individuals take samples from those individuals

795
00:51:10,140 --> 00:51:12,240
and measure those cells

796
00:51:12,250 --> 00:51:13,690
then we might

797
00:51:13,700 --> 00:51:17,850
i expect those samples are indeed i i d

798
00:51:17,860 --> 00:51:21,150
and we just have a random

799
00:51:21,170 --> 00:51:25,270
sample but you can be sure think of ways in which that wouldn't be the

800
00:51:26,040 --> 00:51:27,030
very quickly

801
00:51:27,060 --> 00:51:34,730
noticeable effects that will come from particular population of visiting hospital et cetera et cetera

802
00:51:34,740 --> 00:51:40,740
so we have in these in this sort of theory of statistical learning theory we're

803
00:51:40,740 --> 00:51:44,130
we're sort of making that distribution

804
00:51:44,150 --> 00:51:48,900
the repository of all that we're interested in everything that we're interested in is is

805
00:51:48,900 --> 00:51:53,650
represented in that distribution is the way in which the processes of the natural artificial

806
00:51:53,650 --> 00:51:56,060
world was studying are

807
00:51:56,070 --> 00:51:58,440
given to us through that distribution

808
00:51:58,450 --> 00:52:02,370
how we access it as i've already said through a training sample

809
00:52:02,410 --> 00:52:06,360
so it's a set of data and i'm already thinking of the classification

810
00:52:06,360 --> 00:52:08,260
or at least the

811
00:52:08,310 --> 00:52:13,750
supervised learning case where we have an input and an output we're interested in predicting

812
00:52:13,780 --> 00:52:18,080
so here's the first example and here is the and for example i tend to

813
00:52:18,080 --> 00:52:20,210
use and the number of training

814
00:52:20,230 --> 00:52:22,720
examples which is

815
00:52:22,770 --> 00:52:24,200
at odds with the way

816
00:52:24,220 --> 00:52:28,780
statisticians were afraid and and some other learning areas but

817
00:52:28,790 --> 00:52:30,890
it may be worth

818
00:52:30,950 --> 00:52:32,240
trying to

819
00:52:32,640 --> 00:52:36,130
more coordinated with other with statisticians perhaps at some point

820
00:52:36,190 --> 00:52:40,490
anyway this is the the training sample which we assume is generated i i d

821
00:52:40,510 --> 00:52:43,280
from the underlying distribution p

822
00:52:43,400 --> 00:52:47,640
so really the task of learning is saying i've got this

823
00:52:47,690 --> 00:52:51,530
finite set of information about which is sort of coloured by p

824
00:52:51,560 --> 00:52:52,760
and what i want to do

825
00:52:52,790 --> 00:52:56,490
is learned something about p that's true in in general

826
00:52:56,490 --> 00:53:00,460
independent of that particular sample

827
00:53:00,470 --> 00:53:02,390
so i'm going to talk now about

828
00:53:02,970 --> 00:53:07,760
the generalisation of a learner because this is a way for classification and probably progression

829
00:53:07,770 --> 00:53:09,700
two is the key concept

830
00:53:09,710 --> 00:53:12,730
the we're we're interested in

831
00:53:12,740 --> 00:53:14,400
statistical learning theory

832
00:53:15,560 --> 00:53:17,750
so i'm going to spend a little bit of time looking at it and i'm

833
00:53:17,750 --> 00:53:20,870
also going to plot some curves to show how it looks

834
00:53:20,880 --> 00:53:23,560
for some learning problems

835
00:53:23,570 --> 00:53:27,240
so i think that will give us hopefully a kind of insight into what learning

836
00:53:27,240 --> 00:53:29,690
theory is actually trying to

837
00:53:29,710 --> 00:53:34,780
to to give us so if we have a learning algorithm a

838
00:53:34,840 --> 00:53:39,490
then what it does is it takes the training set as

839
00:53:39,490 --> 00:53:43,480
and typically selects from some function space f

840
00:53:43,520 --> 00:53:44,600
a function

841
00:53:44,610 --> 00:53:47,860
that it considers the right function for that

842
00:53:47,860 --> 00:53:51,740
training set so it does the best fit it may be an empirical risk it

843
00:53:51,740 --> 00:53:54,310
may be regularized risk or whatever

844
00:53:55,360 --> 00:54:00,470
method that it has has built into it that selects the function

845
00:54:00,500 --> 00:54:04,170
two best fit from that training set

846
00:54:04,200 --> 00:54:08,290
and now what we're interested in is how well that function performs on new data

847
00:54:08,290 --> 00:54:12,590
how really how really good is that function became a fit on the training set

848
00:54:12,730 --> 00:54:14,570
but is it good on on new data

849
00:54:14,580 --> 00:54:17,460
and this is the idea of the generalisation and so

850
00:54:17,600 --> 00:54:22,200
this is this quantity abseil on of as a path

851
00:54:22,220 --> 00:54:27,750
but i i'm writing here which is the expectation under randomly generated new point new

852
00:54:27,750 --> 00:54:30,280
test point of the loss

853
00:54:30,290 --> 00:54:35,640
of that function when applied to x and compared with the correct output y so

854
00:54:35,650 --> 00:54:41,160
x what a while the core a given input output generated according to the distribution

855
00:54:41,170 --> 00:54:45,370
a if s is the function we learned and the loss is telling us how

856
00:54:46,330 --> 00:54:50,940
that function fits so this is the expectation if you like a loss

857
00:54:50,960 --> 00:54:54,070
as we start to use that

858
00:54:54,090 --> 00:54:56,270
that learned function in

859
00:54:56,280 --> 00:54:58,290
in practice

860
00:54:59,560 --> 00:55:02,640
and if we think classification the loss would just be

861
00:55:02,650 --> 00:55:04,010
one of the two

862
00:55:04,020 --> 00:55:05,560
values disagreed

863
00:55:05,600 --> 00:55:07,620
and zero otherwise

864
00:55:07,620 --> 00:55:12,650
well for regression it might be the squared difference between the function applied to x

865
00:55:12,650 --> 00:55:15,290
and the the correct output

866
00:55:17,000 --> 00:55:22,320
so this is a random variable is is we're going to call this the generalisation

867
00:55:22,320 --> 00:55:25,950
of the learner but note that is the random variable because it depends on the

868
00:55:25,950 --> 00:55:27,590
training set as

869
00:55:27,610 --> 00:55:29,210
which is a random quantity

870
00:55:29,250 --> 00:55:32,860
remember it's generated i i d from the same distribution

871
00:55:32,920 --> 00:55:35,000
so so we now

872
00:55:35,020 --> 00:55:38,690
i have a random variable which was thinking of as the

873
00:55:38,690 --> 00:55:40,740
the generalizations so it's

874
00:55:40,750 --> 00:55:43,370
something that will depend on the training set

875
00:55:43,380 --> 00:55:48,430
and what we're really interested in is not being screwed up by training set

876
00:55:48,480 --> 00:55:52,520
so we want to guard against being screwed up training set in

877
00:55:52,900 --> 00:55:55,080
in this in this measure

878
00:55:55,090 --> 00:55:59,450
so what i'm going do is show you now an example of where things go

879
00:55:59,450 --> 00:56:03,550
wrong if we have a rather simplistic learning algorithm

880
00:56:03,560 --> 00:56:04,880
and how

881
00:56:04,910 --> 00:56:05,520
you know

882
00:56:05,520 --> 00:56:08,960
we can get quite bad performance from different training sets

883
00:56:08,980 --> 00:56:16,600
and so this is again a taking this idea the breast cancer dataset from UCI

884
00:56:16,600 --> 00:56:20,120
and we just going to use a very simple parzen window classifier

885
00:56:20,130 --> 00:56:24,540
w class is the average of the positive training examples w minus average of the

886
00:56:24,540 --> 00:56:26,130
negative training examples

887
00:56:26,150 --> 00:56:27,120
and just

888
00:56:27,170 --> 00:56:28,720
creator weight vector

889
00:56:28,770 --> 00:56:30,300
between those two

890
00:56:30,310 --> 00:56:33,570
now we

891
00:56:43,010 --> 00:56:48,780
is the average of the negative since the average to positive

892
00:56:48,790 --> 00:56:49,260
draw drawing

893
00:56:49,620 --> 00:56:51,310
vector between them

894
00:56:51,360 --> 00:56:55,110
take the bisecting hyperplane

895
00:56:56,200 --> 00:57:00,170
that's the classifier that i'm i'm thinking of here is a very simplistic

896
00:57:04,040 --> 00:57:08,850
we can apply it to

897
00:57:08,860 --> 00:57:10,430
this training data

898
00:57:10,450 --> 00:57:14,430
now we wanted to actually measure the expected value of on on a randomly drawn

899
00:57:14,430 --> 00:57:15,620
test points so

900
00:57:15,630 --> 00:57:17,530
i'm just going to use the

901
00:57:17,530 --> 00:57:21,130
you know the second half of the sample we didn't used for training as the

902
00:57:21,130 --> 00:57:25,620
testing half so it's a proxy for the true generalisation error in this case

903
00:57:25,650 --> 00:57:30,010
and that's the way to estimate this quantity

904
00:57:30,020 --> 00:57:31,850
and i'm going to do it

905
00:57:31,870 --> 00:57:34,580
four different training set sizes

906
00:57:34,590 --> 00:57:37,320
this is half of the data

907
00:57:37,330 --> 00:57:38,820
and so on

908
00:57:40,480 --> 00:57:43,150
the idea is i'm going to repeatedly draw

909
00:57:43,160 --> 00:57:47,570
samples of these of say this size here three hundred forty two

910
00:57:47,580 --> 00:57:50,310
use that algorithm

911
00:57:50,360 --> 00:57:52,170
two generated classifiers

912
00:57:52,180 --> 00:57:54,120
and then tested on

913
00:57:55,620 --> 00:57:59,300
the remaining data so every time i do that i get one

914
00:57:59,330 --> 00:58:04,500
generalisation error one sample from that random variable here

915
00:58:06,300 --> 00:58:11,220
i'm assuming m fixed case i'm choosing the sample size is fixed here

916
00:58:11,870 --> 00:58:16,520
but obviously every time i draw samples that size i get different value of this

917
00:58:18,040 --> 00:58:22,480
so i'm not going to repeatedly draw that you know it's a thousand

918
00:58:22,510 --> 00:58:23,820
ten thousand times

919
00:58:23,840 --> 00:58:27,420
and i'm going to keep the histogram of those values

920
00:58:27,470 --> 00:58:31,060
so we can see the kind of errors were getting

921
00:58:31,060 --> 00:58:35,130
and the range of errors we get from different training sets you know we may

922
00:58:35,130 --> 00:58:37,590
be lucky on some training sets to do very well

923
00:58:37,640 --> 00:58:40,050
so we do very badly and so on

924
00:58:40,060 --> 00:58:43,430
and just to sort of give you an idea of how well

925
00:58:44,280 --> 00:58:48,300
this particular algorithm can do i'm gonna first show you

926
00:58:48,310 --> 00:58:53,080
the performance on the complete training sets you know just

927
00:58:53,090 --> 00:58:55,890
test error in this case

928
00:58:55,920 --> 00:58:57,160
so that's this

929
00:58:57,180 --> 00:58:58,830
here the sort of thing

930
00:58:58,860 --> 00:59:02,260
o point one five is approximately the area you get if you just

931
00:59:02,280 --> 00:59:04,000
train on all of the data

932
00:59:04,010 --> 00:59:05,770
know that the

933
00:59:05,770 --> 00:59:10,290
the solution announcement i'm supposed to give a series of lectures standing at four o'clock

934
00:59:10,330 --> 00:59:15,660
and the title is machine learning flavors of random matrices which in retrospect seems to

935
00:59:15,660 --> 00:59:16,930
be quite technical

936
00:59:16,940 --> 00:59:23,070
so just to ensure to let everybody know that actually is going to elementary and

937
00:59:23,080 --> 00:59:27,160
much less than normal exercises so

938
00:59:57,140 --> 00:59:59,840
that's OK

939
01:00:01,240 --> 01:00:02,140
OK so

940
01:00:02,310 --> 01:00:09,170
not trying to be able to compete with that afternoon session

941
01:00:09,390 --> 01:00:13,180
so my first i just wanted to

942
01:00:13,200 --> 01:00:18,350
revisit this slide and maybe a couple of points of people ask me afterwards

943
01:00:18,370 --> 01:00:22,530
throughout the things i have clarified

944
01:00:24,600 --> 01:00:26,030
i think about

945
01:00:26,060 --> 01:00:30,320
i think the double sample trick you know it really is a matter of just

946
01:00:30,320 --> 01:00:31,390
looking at those

947
01:00:31,400 --> 01:00:35,840
the equations and convince yourself that goes through

948
01:00:35,870 --> 01:00:38,120
so the intuition is really

949
01:00:38,740 --> 01:00:41,570
because the second sample somehow independent of the

950
01:00:42,940 --> 01:00:49,190
choice of f the probability that there is a very significant deviation between it's true

951
01:00:49,190 --> 01:00:51,930
error and it's error on the second sample is too small

952
01:00:52,000 --> 01:00:54,250
that's basically what's behind

953
01:00:54,330 --> 01:00:56,840
this one i think maybe

954
01:00:57,300 --> 01:01:02,420
it is more crucial and that's where where you are now using the fact that

955
01:01:02,760 --> 01:01:04,900
really all the information that we're

956
01:01:04,920 --> 01:01:06,440
conditioning on

957
01:01:06,460 --> 01:01:10,320
there were interested in is relating to the performance of the

958
01:01:10,330 --> 01:01:13,420
function on the two samples x and y

959
01:01:13,440 --> 01:01:14,970
here's text here is why

960
01:01:14,990 --> 01:01:21,010
and that enables us to move from a potentially uncountable number functions to a finite

961
01:01:21,010 --> 01:01:27,000
number and perhaps i didn't emphasise that quite enough and i just want to illustrate

962
01:01:27,000 --> 01:01:28,340
it with

963
01:01:28,350 --> 01:01:31,850
maybe some linear functions just to show you that

964
01:01:31,970 --> 01:01:33,770
what is happening

965
01:01:33,780 --> 01:01:36,330
so if you imagine

966
01:01:36,350 --> 01:01:37,850
you know you've got

967
01:01:37,870 --> 01:01:39,510
a two-dimensional

968
01:01:39,550 --> 01:01:42,540
input space with some positive and negative examples

969
01:01:42,560 --> 01:01:49,030
when you do that conditioning onto the i mean this is the double sample thinking

970
01:01:49,030 --> 01:01:53,800
both positive and sorry both the you know for sample and the second sample once

971
01:01:53,800 --> 01:01:59,420
you've project you're only interested in what the functions do on those examples these two

972
01:01:59,420 --> 01:02:02,070
blue functions are effectively the same function

973
01:02:02,090 --> 01:02:03,250
as far as the

974
01:02:03,260 --> 01:02:07,470
conditioning is concerned they behave identically so you can treat them as just a single

975
01:02:08,270 --> 01:02:12,500
although the different weight vectors they behave identically on the sample

976
01:02:12,520 --> 01:02:17,560
and this is where you getting the contraction effectively the function space to just to

977
01:02:17,560 --> 01:02:21,060
finite set of functions similarly these two red ones

978
01:02:21,070 --> 01:02:25,730
you know they behave identically they do the same classification of all these points and

979
01:02:25,730 --> 01:02:28,450
so they look identical as far as the actual

980
01:02:28,620 --> 01:02:31,870
probabilities that you're estimating a concern

981
01:02:31,880 --> 01:02:34,330
OK so that's that's what is meant by that

982
01:02:34,350 --> 01:02:37,070
but step there

983
01:02:37,260 --> 01:02:40,210
where we move from this case here

984
01:02:40,220 --> 01:02:45,730
and we apply the union bound over the number of different

985
01:02:46,140 --> 01:02:50,890
behaviors that we can observe on the double sample and that number is measured by

986
01:02:50,890 --> 01:02:53,320
this growth function if remember two n

987
01:02:53,330 --> 01:02:57,460
that's actually the number of different behaviours on o two m sample

988
01:02:57,480 --> 01:03:03,350
the highest number and again somebody asked his ph of two and a ph and

989
01:03:03,350 --> 01:03:04,580
the maximum yes

990
01:03:04,590 --> 01:03:08,090
the age of and is the upper bound on obviously for a particular sample you

991
01:03:08,090 --> 01:03:10,740
may actually have fewer functions that allow

992
01:03:10,850 --> 01:03:16,310
for a particular set of points bishop and saying they cannot be more than the

993
01:03:16,310 --> 01:03:20,440
number of different functions on the set of points OK

994
01:03:20,620 --> 01:03:22,860
so that's and the other point that

995
01:03:23,320 --> 01:03:26,970
somebody raised is what happens if you consider the case where

996
01:03:26,990 --> 01:03:29,140
you know the of two and is still

997
01:03:29,160 --> 01:03:32,620
the maximal you know to to the eigenvalue

998
01:03:33,140 --> 01:03:36,550
can you get anywhere and we'll see what we get to the end of the

999
01:03:36,550 --> 01:03:40,190
proof that actually no we can't i mean it it ends up totally trivial in

1000
01:03:40,190 --> 01:03:43,680
that case it's only by you know

1001
01:03:43,700 --> 01:03:45,440
something in this sort of

1002
01:03:45,450 --> 01:03:49,640
polynomial bound on the growth function that we actually going to get a win and

1003
01:03:49,640 --> 01:03:52,080
get the statistical result

1004
01:03:52,100 --> 01:03:57,310
OK so we're now in shape to go forward into symmetrisation this is the next

1005
01:03:57,310 --> 01:04:01,480
keywords you need the cocktail party to impress people

1006
01:04:03,170 --> 01:04:08,840
very good

1007
01:04:08,860 --> 01:04:20,630
OK so we had a few already in double sample goes socialisation so

1008
01:04:20,640 --> 01:04:21,960
now this is again

1009
01:04:21,970 --> 01:04:26,990
very nice i think trick i hope you'll agree when i see it is not

1010
01:04:29,870 --> 01:04:32,950
difficult thing to understand once you've seen it but

1011
01:04:33,000 --> 01:04:34,350
to come up with

1012
01:04:34,370 --> 01:04:35,940
amazing OK so

1013
01:04:37,530 --> 01:04:42,180
if we consider to a sample s since the points generated independently were making this

1014
01:04:42,180 --> 01:04:44,950
independence assumption of the way the points are generated

1015
01:04:45,060 --> 01:04:50,340
the probability of generating the same set of points in a different order

1016
01:04:50,370 --> 01:04:56,370
is equal to the probability generating that particular sets say we had points x one

1017
01:04:56,370 --> 01:04:58,050
x two x three

1018
01:04:58,280 --> 01:05:03,270
the chances that we actually generated x two x one x three is identical

1019
01:05:03,290 --> 01:05:06,640
i mean it's it's just happen to get in a different order

1020
01:05:06,650 --> 01:05:11,520
OK and that is true for any reordering of the set of sample points

1021
01:05:12,100 --> 01:05:16,300
and consider now set sigma of permutations

1022
01:05:16,300 --> 01:05:21,220
pushing more flow down the pipeline that is not yet capacity and so the algorithms

1023
01:05:21,220 --> 01:05:25,040
that actually do this have they have the feel of the search algorithm which are

1024
01:05:25,280 --> 01:05:26,810
all the time looking for

1025
01:05:26,830 --> 01:05:29,630
the next bit pipeline could take more flowing

1026
01:05:29,680 --> 01:05:32,130
and this is all going to work

1027
01:05:34,700 --> 01:05:39,460
o point lines all have positive capacity and it's analogy is a bit hard to

1028
01:05:39,460 --> 01:05:43,410
something what the negative class would mean but certainly in the original problem where i

1029
01:05:43,410 --> 01:05:47,380
was just talking about graphs and edges no particular reason why you shouldn't put negative

1030
01:05:47,380 --> 01:05:52,640
costs on the on the edges and

1031
01:05:52,640 --> 01:05:56,580
even though that's a little hard to visualise but what you can see is where

1032
01:05:56,600 --> 01:06:01,620
where the weather are just positive costs in the pipeline analogy is invalid this algorithm

1033
01:06:01,620 --> 01:06:04,160
in a sense can't go wrong if you keep on

1034
01:06:04,160 --> 01:06:10,600
that successive steps of your algorithm increasing the flow somewhere in the network and provide

1035
01:06:10,600 --> 01:06:15,060
you increase it by finite steps you must eventually reach the bound which is the

1036
01:06:15,060 --> 01:06:20,440
total capacity the network to provided the network has a finite capacity and provided you're

1037
01:06:20,440 --> 01:06:25,660
increasing by least one unit capacity at every step of this algorithm which is considered

1038
01:06:25,850 --> 01:06:31,000
continue seeking to increase flow then the algorithm must converge

1039
01:06:31,000 --> 01:06:32,960
so it seems like the positive

1040
01:06:32,980 --> 01:06:37,710
but the the versions the problem with the weights positive must be more tractable

1041
01:06:37,730 --> 01:06:42,870
then the other ones and so this is NP hard result is that applies to

1042
01:06:42,870 --> 01:06:46,890
the problem in all generality where the weights on the edges may be negative but

1043
01:06:46,890 --> 01:06:50,310
if you restrict to the case where the weights on the edges have to be

1044
01:06:50,310 --> 01:06:57,560
positive it turns out that tractable subset of this whole space problems

1045
01:06:57,600 --> 01:07:01,540
so OK you know let's let's do one in our home laboratory here

1046
01:07:02,160 --> 01:07:06,580
just to make it quite clear how all this works is my toy problem on

1047
01:07:06,580 --> 01:07:07,770
two nodes

1048
01:07:07,830 --> 01:07:12,750
and here is the energy we're going to find evidence to nodes with the likelihood

1049
01:07:12,750 --> 01:07:14,750
for each of the nodes and just one

1050
01:07:14,810 --> 01:07:20,060
i seem to cross the nodes and you know it just shows some numbers here

1051
01:07:20,060 --> 01:07:24,890
so the cost of assigning the node to the to the sink x equals zero

1052
01:07:24,930 --> 01:07:28,370
i will say that two so the two goes here remember you but it opposite

1053
01:07:28,390 --> 01:07:31,020
the sink not doesn't go here goes here

1054
01:07:32,750 --> 01:07:37,560
when you cut this that signing that guy to the sink

1055
01:07:37,600 --> 01:07:43,660
and similarly f f one of one that's the cost of assigning the node to

1056
01:07:43,930 --> 01:07:48,250
to the foreground that cost goes here and similarly for the other two that costs

1057
01:07:48,310 --> 01:07:52,660
and then take on gamma records one in the ising model

1058
01:07:52,790 --> 01:07:57,310
so the penalty for making different assignments across these two nodes is well

1059
01:07:57,370 --> 01:08:03,560
so now let's try to cut this network just you know cut chosen at random

1060
01:08:03,640 --> 01:08:08,540
this cut corresponds to a trial solution is assigns x two to zero signs x

1061
01:08:08,540 --> 01:08:10,020
one to one

1062
01:08:10,020 --> 01:08:13,910
and the energy of the solution we can get just by

1063
01:08:13,910 --> 01:08:15,250
adding up the

1064
01:08:15,270 --> 01:08:20,210
the costs and all the edges that we cut so there's seven here one here

1065
01:08:20,210 --> 01:08:21,100
five here

1066
01:08:21,100 --> 01:08:23,750
adds up to thirteen

1067
01:08:23,770 --> 01:08:28,830
and so the cost of this particular trial solution will be thirteen

1068
01:08:28,850 --> 01:08:32,500
OK now that's not necessarily the best one because i just sort of shows around

1069
01:08:32,500 --> 01:08:38,430
them as an illustration but now let's actually solve by using max flow

1070
01:08:39,600 --> 01:08:43,180
and yes one thing i didn't say which i should point out is that when

1071
01:08:43,180 --> 01:08:45,100
you do this max flow

1072
01:08:45,120 --> 01:08:49,000
so you push as much oil as you can see the network you pushed him

1073
01:08:49,000 --> 01:08:52,960
you pushed new person you can't get any harder so you've got the maximum flow

1074
01:08:52,960 --> 01:08:54,500
of oil

1075
01:08:54,500 --> 01:09:00,210
and you know that's the minimum cost e are you happy

1076
01:09:00,230 --> 01:09:03,500
no you're not happy because you actually want to know what the minimum cost was

1077
01:09:03,750 --> 01:09:07,440
you want to know what the configuration that achieves the minimum cost so we need

1078
01:09:07,440 --> 01:09:12,790
a little bit more this out this this situation it turns out that the

1079
01:09:14,580 --> 01:09:15,980
the minimum cut

1080
01:09:16,770 --> 01:09:21,750
the set of edges which is saturated under this maximum flow so you push your

1081
01:09:21,750 --> 01:09:25,430
britain you push you mark when you push or you can mark all the edges

1082
01:09:25,700 --> 01:09:29,560
where the flow is saturated that is the flow through the network equals the capacity

1083
01:09:29,560 --> 01:09:34,710
that little piece of the pipeline and that guy is going to that particular edge

1084
01:09:34,710 --> 01:09:38,730
the saturation one is part of the cuts the cut will pass through all the

1085
01:09:38,730 --> 01:09:43,730
saturated as you see i mark saturated is dark blue here and they coincide with

1086
01:09:43,730 --> 01:09:47,730
all the places where the cut passes

1087
01:09:47,750 --> 01:09:51,660
OK so now let's let's do it let's push flow

1088
01:09:51,680 --> 01:09:53,310
come on

1089
01:09:53,310 --> 01:09:56,290
so to begin with there is no flow the red numbers of the amount of

1090
01:09:56,290 --> 01:10:01,390
flow the blue numbers of the capacities of each of the these pipeline so you

1091
01:10:01,390 --> 01:10:07,160
know your search algorithm might start the source and and begin by looking for

1092
01:10:07,180 --> 01:10:12,540
edges which have some spare capacity where the flow does not leave capacity and choose

1093
01:10:12,540 --> 01:10:17,200
one which one we choose yes this one is on the capacity so let's try

1094
01:10:17,210 --> 01:10:18,680
pushing some flow

1095
01:10:18,710 --> 01:10:22,850
through that edge may be up to its capacity but be well because they might

1096
01:10:22,850 --> 01:10:25,160
not be anywhere for that flow to go

1097
01:10:25,180 --> 01:10:29,600
the output and so this is sort of optimistic amount of flow at this stage

1098
01:10:29,600 --> 01:10:33,020
but then you look at the output from the next node in UCI as this

1099
01:10:33,410 --> 01:10:37,020
an edge with respect and st they choose this one and then you might have

1100
01:10:37,020 --> 01:10:40,560
had to look at some more nodes but actually in this particular case we arrived

1101
01:10:40,560 --> 01:10:44,560
at the sink and so few home and dry we can we found the complete

1102
01:10:44,560 --> 01:10:48,250
path from source to sink which was under capacity and now we can push flow

1103
01:10:48,250 --> 01:10:50,040
down that path

1104
01:10:50,080 --> 01:10:55,750
now you go back to repeat the process now with the residual graph so that's

1105
01:10:55,750 --> 01:11:00,140
like you subtract all of the flows of the past is that you have defined

1106
01:11:00,980 --> 01:11:05,060
graph saint apology but with different capacities because now there all reduced by the amount

1107
01:11:05,060 --> 01:11:07,120
of flow you've established so far

1108
01:11:07,140 --> 01:11:11,290
now we look for something pushing to do when you find you can push we

1109
01:11:11,310 --> 01:11:14,390
could push five units down here but when you get to hear

1110
01:11:14,410 --> 01:11:18,350
you look for they may be the best outflow path and you see that's what

1111
01:11:18,390 --> 01:11:20,080
have is only three units so

1112
01:11:20,120 --> 01:11:24,620
you can't use all of these five units only three and you push the flow

1113
01:11:24,640 --> 01:11:26,250
three through to the sink

1114
01:11:26,270 --> 01:11:30,460
and there is still not finished because in the residual graph

1115
01:11:30,560 --> 01:11:33,350
they'll be two spare units of capacity here

1116
01:11:33,350 --> 01:11:36,540
and so you search again you find out there is an edge with two is

1117
01:11:36,600 --> 01:11:39,770
faster and then you get to hear you try this i don't know

1118
01:11:39,790 --> 01:11:44,430
units capacity in the residual graph here so but looking a bit further you see

1119
01:11:46,370 --> 01:11:49,830
edge with some residual capacity only one unit but hey let's use it and you

1120
01:11:49,830 --> 01:11:54,430
go along there and then is anywhere any outflow possible from here yes the still

1121
01:11:54,430 --> 01:11:57,680
some residual capacity here has go down here and

1122
01:11:59,330 --> 01:12:03,560
you establish another unit to fly

1123
01:12:03,580 --> 01:12:08,140
and now you go back and you do some more searching and you probably visit

1124
01:12:08,140 --> 01:12:12,900
this graph edge again because you see it stories residual capacity of one unit you

1125
01:12:12,900 --> 01:12:15,520
go down there but then we get to hear you see there's no edges left

1126
01:12:15,710 --> 01:12:18,620
residual capacity so you can't push any more flow

1127
01:12:18,640 --> 01:12:21,520
so the algorithm stops

1128
01:12:21,540 --> 01:12:24,830
and you know it was guaranteed that it would stop because every step

1129
01:12:24,890 --> 01:12:27,850
we increase the

1130
01:12:27,850 --> 01:12:30,660
the flow by least one unit and the

1131
01:12:30,660 --> 01:12:35,850
all of the edges have positive flows so the total financial flows the total capacity

1132
01:12:35,850 --> 01:12:38,850
of the graph this is finite so you must reach

1133
01:12:38,890 --> 01:12:41,250
you must reach

1134
01:12:41,910 --> 01:12:44,620
convergence in finite number of steps

1135
01:12:44,640 --> 01:12:47,870
and the only thing left to do now is to mark which are the saturated

1136
01:12:47,870 --> 01:12:51,910
paths because that's going to define the optimal cut so those are all the paths

1137
01:12:51,910 --> 01:12:55,260
o show and now

1138
01:12:55,290 --> 01:12:58,150
this theorem

1139
01:12:58,160 --> 01:13:02,680
which is sometimes not called the theorem sometimes called just dilemma

1140
01:13:04,430 --> 01:13:08,210
it has so many different names that goes y

1141
01:13:08,210 --> 01:13:09,590
both sides lambda

1142
01:13:10,770 --> 01:13:14,140
bernstein's counting theorem

1143
01:13:14,160 --> 01:13:16,520
or order theorem that's not the inside

1144
01:13:16,550 --> 01:13:18,500
because the sciences

1145
01:13:18,520 --> 01:13:20,410
he has

1146
01:13:20,410 --> 01:13:22,410
classical book on group theory

1147
01:13:22,480 --> 01:13:27,030
and people joke normally that all theorems are but inside

1148
01:13:27,070 --> 01:13:30,600
so and because this one was made by

1149
01:13:30,640 --> 01:13:32,920
it seems because she frobenius

1150
01:13:32,930 --> 01:13:36,350
so they call this the theorem that's not the inside

1151
01:13:36,390 --> 01:13:38,390
but the proof

1152
01:13:38,440 --> 01:13:41,960
some of the most you know

1153
01:13:42,060 --> 01:13:44,030
you approve of the

1154
01:13:44,040 --> 01:13:45,530
in book form was

1155
01:13:45,530 --> 01:13:46,460
in the

1156
01:13:46,500 --> 01:13:49,070
virus cycle

1157
01:13:49,100 --> 01:13:51,860
so what this theorem tells us

1158
01:13:51,920 --> 01:13:53,440
is that

1159
01:13:53,500 --> 01:13:58,690
it's very easy to the number of all of the number of orbits

1160
01:13:58,690 --> 01:14:01,020
the action of the group

1161
01:14:04,690 --> 01:14:07,220
of the number of fixed points

1162
01:14:07,240 --> 01:14:09,380
by that action

1163
01:14:09,430 --> 01:14:11,960
so in symbols you write this

1164
01:14:14,910 --> 01:14:16,270
this first us

1165
01:14:16,300 --> 01:14:18,050
the first symbol

1166
01:14:18,070 --> 01:14:20,320
is there not confuse this with the

1167
01:14:20,350 --> 01:14:21,580
notation for

1168
01:14:21,600 --> 01:14:23,440
four of course

1169
01:14:23,490 --> 01:14:26,690
this the bars in other

1170
01:14:26,720 --> 01:14:29,520
this here is the number of orbits

1171
01:14:29,530 --> 01:14:30,860
the left side

1172
01:14:30,970 --> 01:14:32,470
the other side you see

1173
01:14:32,490 --> 01:14:37,570
why whites average because have this division by the order of the group

1174
01:14:37,610 --> 01:14:39,070
what is some

1175
01:14:39,180 --> 01:14:41,770
over all elements of the group

1176
01:14:41,820 --> 01:14:43,790
his son something new something

1177
01:14:43,830 --> 01:14:45,660
the number of

1178
01:14:45,740 --> 01:14:47,100
of something

1179
01:14:47,110 --> 01:14:47,860
which is

1180
01:14:47,880 --> 01:14:49,580
the fix it

1181
01:14:49,600 --> 01:14:53,550
the fix subspace i we define this summer

1182
01:14:53,600 --> 01:14:57,300
but you've got to this this is the mean of all these x

1183
01:14:57,320 --> 01:14:58,630
with g

1184
01:14:58,710 --> 01:15:00,660
in the superscript

1185
01:15:00,670 --> 01:15:03,120
so this is the theorem is called

1186
01:15:03,130 --> 01:15:07,070
clock counting theory

1187
01:15:07,410 --> 01:15:21,670
one is

1188
01:15:26,590 --> 01:15:31,160
so before going to the proof going to give you an example of this

1189
01:15:31,230 --> 01:15:32,350
well you

1190
01:15:32,350 --> 01:15:35,950
visualize this first

1191
01:15:36,030 --> 01:15:40,440
it's nice to talk with the question

1192
01:15:41,450 --> 01:15:44,030
imagine you have you have accused

1193
01:15:44,030 --> 01:15:47,820
with and have three callers

1194
01:15:47,840 --> 01:15:49,070
you want to know

1195
01:15:49,160 --> 01:15:50,530
what the number

1196
01:15:50,570 --> 01:15:53,030
of different cube

1197
01:15:53,040 --> 01:15:54,630
we charge

1198
01:15:54,690 --> 01:15:55,870
rotation only

1199
01:15:57,070 --> 01:15:58,340
of each other

1200
01:15:58,380 --> 01:16:02,940
you just color kids with these three colors the color of them

1201
01:16:02,950 --> 01:16:06,100
and then you want to know how many of them are different

1202
01:16:06,160 --> 01:16:07,790
in the sense that you cannot

1203
01:16:07,790 --> 01:16:09,510
try rotate one

1204
01:16:09,530 --> 01:16:12,940
and get what they want

1205
01:16:13,130 --> 01:16:14,750
all possible

1206
01:16:14,760 --> 01:16:17,010
colorings of the two

1207
01:16:17,070 --> 01:16:21,220
it gives you on seven hundred twenty nine

1208
01:16:21,230 --> 01:16:24,150
color kid

1209
01:16:24,190 --> 01:16:26,160
but that this is not proper

1210
01:16:26,170 --> 01:16:30,820
coloring of his proper coloring and the

1211
01:16:30,870 --> 01:16:34,510
two i just sense faces cannot have the same color here we can put the

1212
01:16:34,510 --> 01:16:37,910
called we want

1213
01:16:37,920 --> 01:16:40,130
so two years the theorem

1214
01:16:40,170 --> 01:16:42,010
we need to count

1215
01:16:42,030 --> 01:16:44,160
five although

1216
01:16:44,200 --> 01:16:48,780
think set elements by the action of the group in this case here we are

1217
01:16:48,780 --> 01:16:50,290
dealing with

1218
01:16:50,350 --> 01:16:55,320
the group here is a continuous group subgroup of rotations

1219
01:16:55,320 --> 01:16:58,590
the group of our patient has infinite many elements

1220
01:16:58,630 --> 01:17:04,010
you can rotate by a very small anglican faith by a real number actually

1221
01:17:04,030 --> 01:17:06,620
so simple

1222
01:17:06,660 --> 01:17:08,600
but still former group has

1223
01:17:08,610 --> 01:17:11,290
product you can add

1224
01:17:11,300 --> 01:17:18,240
a rotation plus another dish composed of another revisionist two rotations

1225
01:17:18,800 --> 01:17:22,810
this here will be

1226
01:17:22,860 --> 01:17:24,850
clear with picture

1227
01:17:24,900 --> 01:17:26,430
i will explain this

1228
01:17:27,810 --> 01:17:30,290
this eight

1229
01:17:30,290 --> 01:17:33,790
route eight hundred and twenty degrees

1230
01:17:33,850 --> 01:17:38,540
that rotation what is back rotation in this picture here

1231
01:17:38,600 --> 01:17:43,580
x of rotation it's an next that cross the the cube

1232
01:17:43,610 --> 01:17:45,860
through the diagonal

1233
01:17:45,860 --> 01:17:50,390
but this is a huge amounts of unstructured data on the taxing on the website

1234
01:17:50,390 --> 01:17:58,210
things like sets up dates or descriptions of photos and links being shared with efficient

1235
01:17:58,260 --> 01:17:59,300
and so this is mostly you

1236
01:17:59,630 --> 01:18:02,420
textual content that people are writing form and

1237
01:18:02,440 --> 01:18:06,540
also of going to the which are two

1238
01:18:06,550 --> 01:18:09,080
there's pixels

1239
01:18:09,500 --> 01:18:14,270
and the centrifugal force to the user

1240
01:18:14,350 --> 01:18:16,160
four years you to go and

1241
01:18:16,190 --> 01:18:18,390
and that's what twenty two

1242
01:18:18,440 --> 01:18:22,400
i get some semantic meaning of it and so

1243
01:18:22,580 --> 01:18:25,860
one of the goals that we've been working on the same thing as much as

1244
01:18:29,210 --> 01:18:30,710
so let me

1245
01:18:30,720 --> 01:18:32,770
just use the

1246
01:18:32,810 --> 01:18:37,570
just a few examples of these things that talk about this structured data category

1247
01:18:37,580 --> 01:18:42,310
so the first one is obviously different so there's always not to people

1248
01:18:43,610 --> 01:18:47,670
we have million users and they all have something like a hundred and thirty pounds

1249
01:18:48,370 --> 01:18:50,120
and you have this

1250
01:18:50,130 --> 01:18:53,920
a large amount something like sixty billion edges

1251
01:18:53,980 --> 01:18:57,530
so that's the foundation everything else

1252
01:18:57,670 --> 01:19:01,950
the next biggest part of the soviet is pages

1253
01:19:02,020 --> 01:19:06,540
so it turns out the pages the page that is almost as big as the

1254
01:19:07,540 --> 01:19:13,520
and so you can find resources but this is a page about gaga but was

1255
01:19:13,520 --> 01:19:18,640
pages three and is about all sorts of activities and i want to do this

1256
01:19:18,640 --> 01:19:24,790
operation in wikipedia where we can download all wikipedia pages for all of the articles

1257
01:19:24,790 --> 01:19:29,250
it's all of these things are for sparse objects within this but you can go

1258
01:19:29,250 --> 01:19:32,720
on like that

1259
01:19:34,000 --> 01:19:37,870
but it was also used it and

1260
01:19:37,880 --> 01:19:42,070
you can think this this bipartite graph for people to photos

1261
01:19:42,770 --> 01:19:43,890
for every

1262
01:19:43,900 --> 01:19:48,570
it's connected to its owner and also connected to other people i

1263
01:19:53,480 --> 01:19:56,010
and the last in the long here is

1264
01:19:56,020 --> 01:19:59,850
places it's sort of a new product is but

1265
01:20:00,350 --> 01:20:06,080
it's a very exciting because it has these new dimensions of the where you were

1266
01:20:06,310 --> 01:20:07,810
when you that

1267
01:20:09,150 --> 01:20:12,490
so when we think of places just as

1268
01:20:12,530 --> 01:20:16,020
with the explosive i wanted to that

1269
01:20:16,030 --> 01:20:20,400
because they have the information and means that whenever you go into second place

1270
01:20:20,450 --> 01:20:24,380
know we now know that you're at the right time and we can use that

1271
01:20:25,640 --> 01:20:29,360
very specific

1272
01:20:31,450 --> 01:20:35,610
it's not the case in the market

1273
01:20:40,750 --> 01:20:45,610
so then finally first of all these are

1274
01:20:46,360 --> 01:20:49,760
but this is the IMDB page four

1275
01:20:49,770 --> 01:20:51,250
this movie which i like

1276
01:20:51,290 --> 01:20:54,400
and you can see here in the right

1277
01:20:54,410 --> 01:20:57,520
this will also fifth with so now

1278
01:20:57,630 --> 01:21:04,550
whenever you are browsing the web you'll see these things on the becoming ubiquitous that

1279
01:21:04,550 --> 01:21:07,730
you can go and like the page and that two

1280
01:21:07,740 --> 01:21:10,030
this random web pages on the web

1281
01:21:10,940 --> 01:21:16,810
so the when used the

1282
01:21:16,830 --> 01:21:23,400
we have all these relationships between the individuals and objects and various other acts

1283
01:21:23,400 --> 01:21:27,570
it gives us the future of structure in this part of the graph

1284
01:21:27,600 --> 01:21:33,750
and the structure let's do interesting it's not just the nodes and edges but

1285
01:21:33,820 --> 01:21:39,480
all these nodes have some sort of information and the edges also have some of

1286
01:21:39,540 --> 01:21:44,810
the edges have direction in in some cases in a different graph alpha timestamp attached

1287
01:21:44,820 --> 01:21:46,690
to them when they created

1288
01:21:46,750 --> 01:21:54,800
and then we can think of the structured and combine into anything you want and

1289
01:21:54,800 --> 01:22:03,450
that's what's this sometimes extract meaning from the context wouldn't otherwise be able to

1290
01:22:03,480 --> 01:22:09,750
so this is nice simple picture where everything is just nodes edges and it's all

1291
01:22:09,780 --> 01:22:11,400
very rosy in

1292
01:22:11,440 --> 01:22:14,020
everything make sense

1293
01:22:14,360 --> 01:22:16,900
but the relative prices of complexity

1294
01:22:16,940 --> 01:22:21,190
there's a number of challenges that would be more than one this one this this

1295
01:22:22,070 --> 01:22:26,520
a unifying different is mean the same thing

1296
01:22:26,560 --> 01:22:29,780
so these are three different pages are all about the same movie

1297
01:22:29,800 --> 01:22:30,730
one then

1298
01:22:30,740 --> 01:22:33,260
it is one of them is from IMDB

1299
01:22:33,270 --> 01:22:38,110
in one of them is it which is the wikipedia space

1300
01:22:39,010 --> 01:22:42,360
of course all these refer to the same is true is

1301
01:22:42,400 --> 01:22:45,440
in the sixties

1302
01:22:45,520 --> 01:22:48,730
and we would like to do is know that you know if you like the

1303
01:22:48,730 --> 01:22:52,270
first one it's sort of the same things like any other

1304
01:22:52,310 --> 01:22:54,570
and the first unfortunately

1305
01:22:54,580 --> 01:22:56,370
this isn't

1306
01:22:56,380 --> 01:22:58,550
the information is isn't

1307
01:22:58,610 --> 01:22:59,850
readily available

1308
01:22:59,860 --> 01:23:05,030
and so we have to go to some of the rotten tomatoes nine over movie

1309
01:23:05,030 --> 01:23:09,450
sites and percentile this is certainly the case

1310
01:23:09,510 --> 01:23:12,520
where you can go and see if you like in one of the same like

1311
01:23:12,520 --> 01:23:16,150
any other

1312
01:23:17,950 --> 01:23:22,600
so there is this the way he happens i checked in my life

1313
01:23:22,610 --> 01:23:28,750
and when i have been there for all all the these other

1314
01:23:28,770 --> 01:23:30,060
and therefore

1315
01:23:30,070 --> 01:23:34,180
and the first there's only one happens for

1316
01:23:34,190 --> 01:23:39,080
but people have got to this place and they take in different ways that in

1317
01:23:39,080 --> 01:23:43,490
athens international airport at the airport increase or decrease version

1318
01:23:43,500 --> 01:23:44,740
all these things

1319
01:23:44,750 --> 01:23:49,770
so one of the challenges we have is that these entities are being created by

1320
01:23:50,930 --> 01:23:56,230
and so how can we go through treat and provide but tools

1321
01:23:56,230 --> 01:23:57,480
times y rules

1322
01:23:57,500 --> 01:24:00,200
the nailsea e

1323
01:24:00,220 --> 01:24:01,870
time zero

1324
01:24:01,880 --> 01:24:05,870
and this is a of x times x is really affected ones from the origin

1325
01:24:05,870 --> 01:24:08,040
to this point

1326
01:24:08,050 --> 01:24:10,680
so we could put in that as the vector if you want to is make

1327
01:24:10,700 --> 01:24:13,320
it the vector

1328
01:24:13,340 --> 01:24:14,920
this is that vector

1329
01:24:15,040 --> 01:24:19,470
a y times always sorry is a of x is this one

1330
01:24:19,570 --> 01:24:22,150
a y times y roof is this one

1331
01:24:22,160 --> 01:24:26,320
and a of the time zero is this one

1332
01:24:26,450 --> 01:24:29,910
so the three green vector was added together

1333
01:24:29,970 --> 01:24:31,700
exactly identical

1334
01:24:31,720 --> 01:24:33,150
to the vector of p

1335
01:24:33,190 --> 01:24:35,310
we've decomposed one vector

1336
01:24:35,360 --> 01:24:36,810
in two three

1337
01:24:38,050 --> 01:24:40,060
we will see that very often

1338
01:24:40,110 --> 01:24:42,970
this is of great use in one

1339
01:24:43,010 --> 01:24:44,650
the magnitude

1340
01:24:44,650 --> 01:24:45,820
of the vector

1341
01:24:45,890 --> 01:24:48,530
the square root

1342
01:24:48,590 --> 01:24:50,350
a x squared

1343
01:24:50,920 --> 01:24:53,630
a white screen

1344
01:24:53,640 --> 01:24:56,760
there is a least squared

1345
01:24:56,790 --> 01:24:58,470
and so we can

1346
01:24:58,570 --> 01:25:00,540
take a simple

1347
01:25:02,030 --> 01:25:04,520
for instance

1348
01:25:04,530 --> 01:25:07,390
i take effect or a

1349
01:25:07,430 --> 01:25:09,540
this is just an example

1350
01:25:09,550 --> 01:25:11,590
see this in action

1351
01:25:11,640 --> 01:25:13,930
and we call a

1352
01:25:13,950 --> 01:25:15,570
three x roof

1353
01:25:15,590 --> 01:25:21,070
so a of xt three

1354
01:25:21,140 --> 01:25:24,000
minus five why rules

1355
01:25:27,120 --> 01:25:28,520
six c

1356
01:25:29,630 --> 01:25:31,120
so what it means

1357
01:25:31,890 --> 01:25:34,650
three units in this direction

1358
01:25:34,650 --> 01:25:35,270
it is

1359
01:25:35,280 --> 01:25:38,620
five units in this direction in the minors y direction

1360
01:25:38,640 --> 01:25:42,280
and six in the policy direction that makes up vector

1361
01:25:42,340 --> 01:25:44,010
and i call that vector

1362
01:25:44,070 --> 01:25:50,520
a what is the magnitude of the vector which are always write down

1363
01:25:50,520 --> 01:25:53,820
with vertical bars if i put two bars on one side

1364
01:25:53,880 --> 01:25:55,370
that's always the magnitude

1365
01:25:55,380 --> 01:25:58,340
or sometimes i simply leave the arrow of

1366
01:25:58,350 --> 01:26:00,640
but to be always on the safe side

1367
01:26:00,670 --> 01:26:03,380
i like this idea that you know it's really the magnitude

1368
01:26:03,440 --> 01:26:06,660
becomes scalar when you do that

1369
01:26:06,720 --> 01:26:09,470
so that would be the square root

1370
01:26:09,480 --> 01:26:11,040
of three squares nine

1371
01:26:11,050 --> 01:26:13,770
five squares twenty five

1372
01:26:13,820 --> 01:26:18,010
six square thirty six the discriminant of seven

1373
01:26:18,120 --> 01:26:19,770
and suppose i asked you

1374
01:26:19,810 --> 01:26:22,090
what is data

1375
01:26:22,100 --> 01:26:26,520
uniquely determined of course is vector is uniquely determined in three-dimensional space

1376
01:26:26,520 --> 01:26:29,050
you should be able to find finer data

1377
01:26:29,880 --> 01:26:31,650
the cosine of fatah

1378
01:26:31,650 --> 01:26:33,230
it is angle here

1379
01:26:33,270 --> 01:26:34,430
ninety degrees

1380
01:26:35,940 --> 01:26:37,730
so the cosine of data

1381
01:26:37,770 --> 01:26:40,920
is a of the divided by itself

1382
01:26:41,010 --> 01:26:43,660
the cosine of data

1383
01:26:43,790 --> 01:26:48,220
quality of the divided by self which in our case

1384
01:26:48,380 --> 01:26:49,530
three six

1385
01:26:49,570 --> 01:26:51,160
divided by the square

1386
01:26:52,370 --> 01:26:54,050
you can do five

1387
01:26:54,110 --> 01:26:55,370
just simply matter

1388
01:26:56,930 --> 01:27:00,070
some numbers

1389
01:27:00,130 --> 01:27:02,980
we not come to a much more difficult part

1390
01:27:02,990 --> 01:27:04,010
o vectors

1391
01:27:04,020 --> 01:27:05,010
and that this

1392
01:27:05,040 --> 01:27:07,590
multiplication of vectors

1393
01:27:07,900 --> 01:27:17,110
we're not going to need to this

1394
01:27:17,150 --> 01:27:19,260
until october

1395
01:27:19,310 --> 01:27:21,980
but i decided we might as well get it over with now

1396
01:27:22,020 --> 01:27:26,930
now to introduce factors can add and subtract you might as well learn about multiplication

1397
01:27:26,990 --> 01:27:29,580
sort of the job is done is like going to the dentist is a little

1398
01:27:29,580 --> 01:27:31,810
painful but it's good for you

1399
01:27:32,110 --> 01:27:33,390
is behind

1400
01:27:33,440 --> 01:27:34,180
the pain

1401
01:27:34,200 --> 01:27:35,660
this appears

1402
01:27:35,670 --> 01:27:37,890
so i'm going to talk about multiplication

1403
01:27:37,900 --> 01:27:40,250
o vector something that will not come back

1404
01:27:40,260 --> 01:27:41,530
until october

1405
01:27:41,540 --> 01:27:43,540
and later in the course

1406
01:27:43,640 --> 01:27:46,370
there are two ways that we multiply vectors

1407
01:27:46,410 --> 01:27:47,980
and one is called

1408
01:27:48,020 --> 01:27:50,260
the dot product

1409
01:27:50,300 --> 01:27:51,890
often also called

1410
01:27:51,900 --> 01:27:53,270
the skater

1411
01:27:55,650 --> 01:27:57,070
doctor b

1412
01:27:57,260 --> 01:27:59,410
in fact not

1413
01:27:59,590 --> 01:28:02,100
and that's the find

1414
01:28:02,140 --> 01:28:03,770
as it is the scalar

1415
01:28:03,790 --> 01:28:06,440
a of x times the of x is the number

1416
01:28:07,020 --> 01:28:08,340
was a of y

1417
01:28:08,390 --> 01:28:09,600
nine b of y

1418
01:28:09,610 --> 01:28:11,140
it's another number

1419
01:28:11,150 --> 01:28:12,520
was a of the

1420
01:28:12,520 --> 01:28:16,220
time to BFC that's an odd number this scalar

1421
01:28:16,390 --> 01:28:18,390
has no longer the direction

1422
01:28:18,430 --> 01:28:21,350
that is

1423
01:28:21,390 --> 01:28:23,120
the dot product

1424
01:28:23,130 --> 01:28:28,500
but as method number one that's completely legitimate you can always use that

1425
01:28:28,510 --> 01:28:30,720
there is another way to find

1426
01:28:30,770 --> 01:28:33,180
the dot product depending upon

1427
01:28:33,250 --> 01:28:37,040
what you being given out to problem is presented to you

1428
01:28:37,050 --> 01:28:38,510
if someone gives you

1429
01:28:38,540 --> 01:28:39,970
the vector a

1430
01:28:43,500 --> 01:28:46,890
you have the vector b

1431
01:28:47,110 --> 01:28:50,500
you happen to know this angle between them this angle theta which has nothing to

1432
01:28:50,510 --> 01:28:52,150
do with that angle theta

1433
01:28:52,390 --> 01:28:55,160
the angle between the two

1434
01:28:56,240 --> 01:28:59,020
the dot product

1435
01:28:59,060 --> 01:29:01,720
is also the following

1436
01:29:01,720 --> 01:29:04,080
so this instrument is not

1437
01:29:04,100 --> 01:29:06,310
was our danger

1438
01:29:06,360 --> 01:29:09,190
but that of course makes it more exciting to work with

1439
01:29:09,190 --> 01:29:12,820
so is this super emeralds

1440
01:29:13,880 --> 01:29:15,020
what i will do first

1441
01:29:15,020 --> 01:29:17,910
now is to put some

1442
01:29:19,590 --> 01:29:21,560
on top

1443
01:29:21,610 --> 01:29:24,280
and then we turn on the then graph

1444
01:29:24,320 --> 01:29:28,120
the confetti may at first go to the

1445
01:29:28,130 --> 01:29:31,150
charge domain is already on top of it and when it picks up some of

1446
01:29:31,150 --> 01:29:32,260
the charge

1447
01:29:32,260 --> 01:29:35,380
it will then spread out because it

1448
01:29:35,400 --> 01:29:37,940
it will repel

1449
01:29:37,940 --> 01:29:41,250
so let's get some

1450
01:29:41,260 --> 01:29:44,420
light on their

1451
01:29:44,480 --> 01:29:49,710
which will make it a little bit better to see

1452
01:29:50,510 --> 01:29:57,290
they put some of this

1453
01:29:57,340 --> 01:30:00,530
on top

1454
01:30:00,560 --> 01:30:03,110
it's just regular confetti

1455
01:30:03,120 --> 01:30:07,050
pieces of paper

1456
01:30:07,280 --> 01:30:17,030
right now all i have to remember is how to

1457
01:30:17,030 --> 01:30:22,110
by the

1458
01:30:22,140 --> 01:30:32,310
most of the action has already occurred

1459
01:30:32,420 --> 01:30:49,320
i will put a little bit more on

1460
01:30:49,420 --> 01:30:50,520
he was part

1461
01:30:50,560 --> 01:30:51,480
don't worry

1462
01:31:00,260 --> 01:31:04,210
the world

1463
01:31:05,260 --> 01:31:08,670
nothing left for the second class

1464
01:31:08,730 --> 01:31:20,730
making perhaps a little darker

1465
01:31:20,860 --> 01:31:29,830
hard it's too dark

1466
01:31:29,870 --> 01:31:33,310
OK i tried once more to the that

1467
01:31:33,350 --> 01:31:35,620
so look at the confetti on top

1468
01:31:35,640 --> 01:31:44,810
i think it's quite convincing some of the fairly will stay there

1469
01:31:44,810 --> 01:31:46,300
well that's

1470
01:31:46,350 --> 01:31:48,780
the reason that is not a good conductor

1471
01:31:48,880 --> 01:31:52,990
so to get a first something and it doesn't get a chance of the telegraph

1472
01:31:52,990 --> 01:31:54,090
then it will not

1473
01:31:54,110 --> 01:31:59,540
spread out

1474
01:31:59,640 --> 01:32:05,130
all right

1475
01:32:09,150 --> 01:32:10,210
that's right

1476
01:32:10,210 --> 01:32:11,580
for the first time to be

1477
01:32:11,600 --> 01:32:16,910
a little bit more quality

1478
01:32:16,920 --> 01:32:18,810
if i take

1479
01:32:18,860 --> 01:32:20,810
two charges

1480
01:32:20,810 --> 01:32:25,470
and we use in general we use are symbol q

1481
01:32:25,540 --> 01:32:28,660
so here we have q one

1482
01:32:28,720 --> 01:32:30,340
and here we have q two

1483
01:32:30,390 --> 01:32:35,440
and let's say they are separated

1484
01:32:37,320 --> 01:32:38,480
distance are

1485
01:32:38,490 --> 01:32:42,550
and the

1486
01:32:42,590 --> 01:32:45,270
unit vector

1487
01:32:45,270 --> 01:32:47,590
in the direction from one to two

1488
01:32:47,640 --> 01:32:49,060
i call that are

1489
01:32:49,910 --> 01:32:50,960
one two

1490
01:32:51,000 --> 01:32:56,130
the roof stands for unit vectors

1491
01:32:56,160 --> 01:33:00,110
if these charges equal both minors of both plus

1492
01:33:00,150 --> 01:33:01,560
then they were repelled

1493
01:33:01,650 --> 01:33:03,020
each other

1494
01:33:03,070 --> 01:33:06,060
so here is the force f

1495
01:33:06,080 --> 01:33:08,820
which i call two

1496
01:33:08,860 --> 01:33:11,590
is the force on two do number one

1497
01:33:11,640 --> 01:33:12,530
and since

1498
01:33:12,530 --> 01:33:14,970
actually equals minus reaction

1499
01:33:15,020 --> 01:33:16,470
voice here

1500
01:33:16,550 --> 01:33:24,370
is to run equal in magnitude but hundred eighty degrees the opposite direction

1501
01:33:24,380 --> 01:33:25,370
cool coulomb

1502
01:33:25,440 --> 01:33:27,900
french physicist

1503
01:33:27,990 --> 01:33:30,790
we did a lot of research on this in the nineteen

1504
01:33:30,800 --> 01:33:33,610
eighteenth century actually

1505
01:33:36,250 --> 01:33:38,890
the following relationship

1506
01:33:39,020 --> 01:33:40,220
that the forces

1507
01:33:40,350 --> 01:33:46,370
is proportional to the product of the charges

1508
01:33:46,390 --> 01:33:51,210
so it's q one times q two

1509
01:33:51,290 --> 01:33:55,310
times a constant which nowadays we call graphs constants k

1510
01:33:55,360 --> 01:33:59,800
divided by the distance between these charges square

1511
01:33:59,810 --> 01:34:03,830
and this is the direction

1512
01:34:03,840 --> 01:34:06,050
of the unit vector

1513
01:34:06,140 --> 01:34:09,260
it goes from one to two this is the force on number two

1514
01:34:09,270 --> 01:34:11,120
o to one

1515
01:34:11,140 --> 01:34:14,870
and notice that this equation is signed sensitive

1516
01:34:14,880 --> 01:34:18,310
because if q one and q two are both negative

1517
01:34:18,320 --> 01:34:20,810
the sources into the forces in this direction

1518
01:34:20,810 --> 01:34:23,530
and if they both positive it's also in this way

1519
01:34:23,580 --> 01:34:25,580
as i have

1520
01:34:25,590 --> 01:34:29,390
however if this if one is positive and one negative

1521
01:34:29,470 --> 01:34:32,560
get mine in this direction so this force flips over

1522
01:34:32,640 --> 01:34:38,490
and and that's one then obviously also flexible so

1523
01:34:38,530 --> 01:34:40,260
and yes i units

1524
01:34:40,280 --> 01:34:41,940
in this course

1525
01:34:41,990 --> 01:34:50,310
we will use for the unit of charge the coulomb named after this great man

1526
01:34:50,360 --> 01:34:54,990
one cool charges how well does amount of charge more than you ever see in

1527
01:34:54,990 --> 01:34:56,310
your lifetime

1528
01:34:56,400 --> 01:35:02,380
we normally work was micro sometimes even less than that

1529
01:35:02,400 --> 01:35:06,470
the charge of one proton

1530
01:35:06,470 --> 01:35:11,720
which is exactly the same as the charge of one electron

1531
01:35:11,740 --> 01:35:14,190
it is approximately one point six

1532
01:35:14,200 --> 01:35:16,310
times ten to the minus

1533
01:35:16,380 --> 01:35:18,860
nineteen cool

1534
01:35:18,870 --> 01:35:22,130
so one could always something like six times ten to the eighteen

1535
01:35:23,610 --> 01:35:28,300
or electrons if the charge is negative

1536
01:35:28,310 --> 01:35:31,920
this constant k

1537
01:35:32,000 --> 01:35:33,750
SR units

1538
01:35:33,810 --> 01:35:35,250
is nine

1539
01:35:35,310 --> 01:35:38,700
times ten to the ninth

1540
01:35:38,730 --> 01:35:41,950
and the unit you can find out because you know that

1541
01:35:42,060 --> 01:35:43,720
this is

1542
01:35:45,170 --> 01:35:48,500
this is cool squares and this square metres

1543
01:35:49,610 --> 01:35:51,260
the unit is noted

1544
01:35:51,300 --> 01:35:53,500
o square meters

1545
01:35:55,000 --> 01:35:56,260
square meter

1546
01:35:56,310 --> 01:36:00,130
divided by square cool that's not so important no one ever things of it that

1547
01:36:02,260 --> 01:36:05,040
for historical reasons

1548
01:36:05,080 --> 01:36:08,090
which may at times be pain in the neck for you

1549
01:36:08,140 --> 01:36:09,670
we write for k

1550
01:36:11,400 --> 01:36:13,280
divided by

1551
01:36:13,290 --> 01:36:15,320
four by

1552
01:36:15,350 --> 01:36:18,720
actually non-zero

1553
01:36:18,720 --> 01:36:20,990
there's nothing magic about that

1554
01:36:21,000 --> 01:36:23,280
it's just a historical reasons

1555
01:36:23,290 --> 01:36:26,730
and so one divided by four pi epsilon zero is nine times ten to the

1556
01:36:26,730 --> 01:36:29,330
ninth that's all that matters

1557
01:36:29,480 --> 01:36:35,090
it's actually non-zero has name is called the permittivity of free space

1558
01:36:35,150 --> 01:36:40,130
but you can forget about that is not important in a

1559
01:36:40,130 --> 01:36:44,480
notice that there is a clear parallel with gravity

1560
01:36:46,000 --> 01:36:47,710
law of gravity

1561
01:36:47,810 --> 01:36:52,020
but the force which in that case is always attracting

1562
01:36:52,070 --> 01:36:54,130
gravity never repelled

1563
01:36:54,170 --> 01:36:57,190
it is the product of two masses

1564
01:36:57,230 --> 01:36:58,540
and then you have your

1565
01:36:58,570 --> 01:37:01,290
the gravitational constant and again you have

1566
01:37:01,310 --> 01:37:03,610
the distance squared

1567
01:37:03,670 --> 01:37:07,540
so there's an enormous parallel between the two is a great beauty

1568
01:37:07,590 --> 01:37:09,750
the electricity

1569
01:37:09,770 --> 01:37:13,340
x in a way that is very parallel to the way that

1570
01:37:13,360 --> 01:37:17,900
gravity works

1571
01:37:17,960 --> 01:37:19,670
if i edit a

1572
01:37:19,670 --> 01:37:21,770
the third charge

1573
01:37:21,770 --> 01:37:24,190
for instance he q three

1574
01:37:24,420 --> 01:37:29,900
and if now i want to know what the forces on

1575
01:37:29,900 --> 01:37:32,010
so i'm going to start at

1576
01:37:32,550 --> 01:37:37,640
where we left off with frame based directed models and

1577
01:37:37,650 --> 01:37:44,390
because we've gone over the directed rule based approaches i'm going to try and go

1578
01:37:44,390 --> 01:37:45,760
a little bit faster here

1579
01:37:50,600 --> 01:37:51,830
two main

1580
01:37:51,850 --> 01:37:58,200
approaches and there are the work of probabilistic relational models and some more recent work

1581
01:37:58,200 --> 01:38:03,570
probabilistic entity relationship models which is by

1582
01:38:03,630 --> 01:38:09,490
recommend at all which is also very nice work on that one actually does do

1583
01:38:09,700 --> 01:38:14,180
a mix of some of the directed and undirected approaches as well

1584
01:38:14,480 --> 01:38:16,270
i mean it

1585
01:38:16,310 --> 01:38:17,280
in this

1586
01:38:17,280 --> 01:38:22,910
section mainly focus on probabilistic relational models and

1587
01:38:22,920 --> 01:38:24,780
i'm mean first go through

1588
01:38:24,880 --> 01:38:30,850
the most vanilla version of probabilistic relational models and then i'm going to

1589
01:38:30,880 --> 01:38:33,340
talk about inference learning

1590
01:38:33,350 --> 01:38:36,810
and then i'm going to talk about kind

1591
01:38:37,090 --> 01:38:42,400
probabilistic relational model that allows a little bit more

1592
01:38:42,420 --> 01:38:46,680
a richer probability distributions over structure

1593
01:38:46,730 --> 01:38:51,090
both for structural uncertainty and for the case we have a class hierarchy

1594
01:38:52,430 --> 01:38:53,840
and then

1595
01:38:53,930 --> 01:38:57,820
after that i'm going to go over and directed approaches

1596
01:38:59,680 --> 01:39:02,870
before the end of the tutorial

1597
01:39:05,510 --> 01:39:09,570
in these systems rather than starting off with the set of rules one starts off

1598
01:39:09,570 --> 01:39:13,310
with some sort of representation of

1599
01:39:13,810 --> 01:39:18,180
objects in the domain and the relations so you can think of this as the

1600
01:39:18,200 --> 01:39:20,420
database schema or

1601
01:39:20,430 --> 01:39:24,210
as a ontology

1602
01:39:26,320 --> 01:39:32,290
entity relationship model but the important thing is i have a description other kinds of

1603
01:39:33,700 --> 01:39:35,680
now you can kind of see

1604
01:39:35,710 --> 01:39:39,340
from the earlier propositional example

1605
01:39:39,340 --> 01:39:40,840
where i had

1606
01:39:42,410 --> 01:39:45,660
i have now broken into others

1607
01:39:46,840 --> 01:39:48,470
and papers

1608
01:39:52,490 --> 01:39:56,470
in addition to the objects of course i have attributes of the objects and i

1609
01:39:56,470 --> 01:39:59,240
have the potential relationships between them

1610
01:40:00,900 --> 01:40:05,240
when i describe directed graphical model

1611
01:40:05,280 --> 01:40:06,440
this works

1612
01:40:06,460 --> 01:40:10,330
very similar to the business from before

1613
01:40:14,930 --> 01:40:18,030
are a few important differences

1614
01:40:18,050 --> 01:40:19,310
for example

1615
01:40:19,330 --> 01:40:20,530
when i say

1616
01:40:20,550 --> 01:40:23,210
that this attribute accepted

1617
01:40:23,280 --> 01:40:25,110
has parents

1618
01:40:25,120 --> 01:40:29,430
the quality of the paper and the mood

1619
01:40:32,560 --> 01:40:33,870
in this case

1620
01:40:33,890 --> 01:40:36,620
i have to specify

1621
01:40:36,650 --> 01:40:39,610
when i'm going outside of the object

1622
01:40:39,650 --> 01:40:44,260
what relationship i followed to get to that attribute and now in this case it's

1623
01:40:44,260 --> 01:40:49,220
trivial because there's only one potential relationship between the paper and the review but you

1624
01:40:49,220 --> 01:40:55,620
can think of something where there's papers and authors and there's multiple kinds of relationships

1625
01:40:55,620 --> 01:41:01,180
between papers and authors and so it would have to specify which of those i

1626
01:41:01,180 --> 01:41:05,370
was actually making use

1627
01:41:05,390 --> 01:41:10,620
beyond that though again just like the base have conditional

1628
01:41:10,660 --> 01:41:12,620
probability table here

1629
01:41:17,620 --> 01:41:19,870
another important difference here

1630
01:41:20,780 --> 01:41:24,400
that this is all specified kind of that

1631
01:41:24,990 --> 01:41:29,340
class level so it's up at a higher level of abstraction

1632
01:41:29,390 --> 01:41:32,220
and to actually

1633
01:41:32,280 --> 01:41:35,690
get the underlying semantics

1634
01:41:38,620 --> 01:41:45,210
probabilistic relational model i have to tell you something about what the random variables are

1635
01:41:45,240 --> 01:41:46,580
so in that case

1636
01:41:47,100 --> 01:41:51,170
the bayesian logic programs the rules

1637
01:41:51,190 --> 01:41:56,860
and the instantiations for the rules told you what the random variables were going to

1638
01:41:56,860 --> 01:41:59,460
be in the probability distribution

1639
01:41:59,460 --> 01:42:01,840
in this case

1640
01:42:01,850 --> 01:42:06,170
relational skeleton

1641
01:42:06,940 --> 01:42:09,070
it's something that tells me

1642
01:42:09,080 --> 01:42:10,380
for each

1643
01:42:12,030 --> 01:42:14,710
what are the

1644
01:42:14,710 --> 01:42:16,320
what's the domain

1645
01:42:16,330 --> 01:42:20,460
what are the space and different authors and this

1646
01:42:20,510 --> 01:42:24,170
can teeny tiny example i see have to others

1647
01:42:25,700 --> 01:42:27,320
what are the papers

1648
01:42:28,280 --> 01:42:30,800
how many reviews to have

1649
01:42:32,520 --> 01:42:34,780
so it tells me for each class harmony

1650
01:42:34,780 --> 01:42:37,290
objects do i have in that class

1651
01:42:38,080 --> 01:42:39,300
in addition

1652
01:42:39,320 --> 01:42:41,710
the relational skeleton

1653
01:42:41,780 --> 01:42:44,540
besides time e

1654
01:42:44,550 --> 01:42:47,340
the numbers of objects in each class

1655
01:42:47,400 --> 01:42:48,840
at least for now

1656
01:42:48,840 --> 01:42:51,340
what i'm doing the simplest semantics for

1657
01:42:52,570 --> 01:42:55,590
i'm going to say that it also tells me

1658
01:42:55,600 --> 01:43:00,460
the relationships that hold between objects so

1659
01:43:00,520 --> 01:43:04,460
i'm saying that i know the authors of the papers

1660
01:43:04,470 --> 01:43:05,460
and i know

1661
01:43:05,460 --> 01:43:10,500
the reviews associated with the papers and sorry i can switch from reviews

1662
01:43:10,540 --> 01:43:12,520
reviewers to reviews

1663
01:43:16,570 --> 01:43:19,970
and if you want to put this in database lingo

1664
01:43:20,020 --> 01:43:24,770
and what i'm saying when i say the risk relational skeleton is given to you

1665
01:43:24,900 --> 01:43:26,350
is i'm saying

1666
01:43:26,400 --> 01:43:30,040
i'm telling you the primary keys of other relations

1667
01:43:30,050 --> 01:43:32,090
and the foreign keys

1668
01:43:32,890 --> 01:43:38,210
all the relations and another way this can be instantiated as background theory have the

1669
01:43:38,210 --> 01:43:41,820
background theory with the background theory tells me what

1670
01:43:41,830 --> 01:43:47,630
are the objects and what are the relationships between objects

1671
01:43:48,550 --> 01:43:51,510
appear and we attribute uncertainty

1672
01:43:52,480 --> 01:43:56,380
it takes the relational skeleton together with

1673
01:43:56,420 --> 01:43:58,160
that template

1674
01:43:58,960 --> 01:44:00,840
the bayes net and together

1675
01:44:00,850 --> 01:44:04,170
that can be seen

1676
01:44:04,210 --> 01:44:09,170
defining the set of random variables that you have a distribution over

1677
01:44:10,100 --> 01:44:17,210
the distribution is over instantiations of attributes of all the objects that were mentioned

1678
01:44:17,270 --> 01:44:23,940
and the relational schema

1679
01:44:25,040 --> 01:44:28,720
to get to the point that we talked about earlier with a more kind of

1680
01:44:28,720 --> 01:44:30,710
pictorial example

1681
01:44:32,160 --> 01:44:37,610
the parameters are used in multiple in places over and over in the bayes here's

1682
01:44:37,610 --> 01:44:40,170
a little fragment

1683
01:44:43,420 --> 01:44:46,040
not so large but

1684
01:44:46,070 --> 01:44:47,050
and rolled

1685
01:44:48,350 --> 01:44:52,150
corresponding to the here and so if i have the

1686
01:44:52,160 --> 01:44:53,380
the case

1687
01:44:53,480 --> 01:44:55,400
paper two

1688
01:44:55,420 --> 01:44:59,280
and whether or not it accepted one can use the

1689
01:44:59,290 --> 01:45:03,110
conditional probability table there was defined in the period

1690
01:45:03,130 --> 01:45:03,980
but now

1691
01:45:04,030 --> 01:45:07,160
i'm going to look at the particular context

1692
01:45:07,210 --> 01:45:08,650
and they looked at

1693
01:45:08,690 --> 01:45:13,610
what the instantiations for the parents are so suppose

1694
01:45:13,640 --> 01:45:16,300
for some reason it turns out that

1695
01:45:16,320 --> 01:45:17,630
the moon

1696
01:45:17,640 --> 01:45:22,210
of the reviewer here was not very good

1697
01:45:22,210 --> 01:45:26,250
and then they put it through something and then they develop it all simultaneously using

1698
01:45:26,250 --> 01:45:30,730
these very complex things so in a couple of hours

1699
01:45:30,750 --> 01:45:33,870
they can get millions of test results

1700
01:45:33,900 --> 01:45:36,940
each of which with human looks at that

1701
01:45:37,930 --> 01:45:41,310
a long time for the human to figure out what's going on

1702
01:45:41,330 --> 01:45:43,370
and is that a

1703
01:45:44,540 --> 01:45:50,630
they can generate an hour something that will keep a trained human busy for years

1704
01:45:50,660 --> 01:45:53,340
and they must be that every hour

1705
01:45:53,360 --> 01:45:55,330
because what a drug company

1706
01:45:55,330 --> 01:46:00,440
succeeds or fails by nowadays is do they find the right ones of these so

1707
01:46:00,440 --> 01:46:03,010
i am i have a hypothesis that

1708
01:46:03,420 --> 01:46:05,820
to cure this particular thing

1709
01:46:05,830 --> 01:46:07,450
i need something that will

1710
01:46:07,450 --> 01:46:10,850
it's called binding in this particular place

1711
01:46:10,870 --> 01:46:15,150
and something that buying in this particular place i know must

1712
01:46:15,170 --> 01:46:16,970
not like water

1713
01:46:16,980 --> 01:46:23,460
much like oxygen and must have three cove and again they these complex expressions so

1714
01:46:23,460 --> 01:46:24,580
what i do

1715
01:46:24,590 --> 01:46:27,460
it is using all an ontology language

1716
01:46:27,480 --> 01:46:31,250
i can build a very complex set of things which says

1717
01:46:31,260 --> 01:46:35,340
anything that has this property cannot possibly have that property

1718
01:46:35,350 --> 01:46:40,150
everything must have one of these four properties nothing can have more than two of

1719
01:46:40,150 --> 01:46:44,750
these properties if you have one of these properties you must also have one of

1720
01:46:44,750 --> 01:46:48,210
those properties a lot of complex statements like that

1721
01:46:48,220 --> 01:46:52,740
so essentially creating a model or theory of the domain

1722
01:46:53,530 --> 01:46:57,310
we have techniques in description logic it's called realisation

1723
01:46:57,330 --> 01:47:00,540
where i can take that data it takes a long time

1724
01:47:00,550 --> 01:47:05,470
in these cases many of these things run on very very large computers for weeks

1725
01:47:05,470 --> 01:47:06,310
or months but they

1726
01:47:06,700 --> 01:47:09,120
graham the whole lot of that data

1727
01:47:09,200 --> 01:47:12,900
and what they come up with is answers so you say

1728
01:47:12,950 --> 01:47:18,330
which of these which in these cases could possibly be revealing

1729
01:47:18,440 --> 01:47:24,000
a hydrophilic air aruba but

1730
01:47:24,260 --> 01:47:28,600
all the things i said before hates water loves oxygen has to bound

1731
01:47:30,280 --> 01:47:33,790
and then what it comes up with twenty or thirty of them were then those

1732
01:47:33,790 --> 01:47:36,190
are the twenty or thirty of them you want to test

1733
01:47:36,200 --> 01:47:40,130
because now instead of testing million you only have to test twenty or thirty

1734
01:47:40,940 --> 01:47:45,380
any extra ones the machine get that shouldn't have been there

1735
01:47:45,400 --> 01:47:48,510
so if you had added one more piece of knowledge that could have made ten

1736
01:47:48,510 --> 01:47:50,080
of those go away

1737
01:47:50,090 --> 01:47:54,290
that saves a lot of money because each of these you look at the expense

1738
01:47:54,300 --> 01:47:57,710
more importantly if you put in something wrong

1739
01:47:57,710 --> 01:48:00,750
the cause is one of the things not to be looked at you could miss

1740
01:48:00,760 --> 01:48:04,990
the drugs that could be the difference between whether your company lives or dies literally

1741
01:48:04,990 --> 01:48:08,620
these the research labs of drug companies have to

1742
01:48:10,370 --> 01:48:13,840
at the level of about one in forty so if one of is one in

1743
01:48:13,840 --> 01:48:15,830
every forty things they try

1744
01:48:15,880 --> 01:48:18,410
works your size

1745
01:48:18,420 --> 01:48:21,950
if one in less than forty of the things you try works

1746
01:48:21,960 --> 01:48:23,650
you're out of business

1747
01:48:23,690 --> 01:48:28,010
right so it really comes out and again i'm simplifying obviously is really a multi-part

1748
01:48:28,010 --> 01:48:28,990
process but

1749
01:48:29,010 --> 01:48:30,450
again the idea is you want

1750
01:48:30,450 --> 01:48:32,680
a lot of data pulled together

1751
01:48:32,690 --> 01:48:37,510
a very complex reasoning and the whole thing to work together well

1752
01:48:37,540 --> 01:48:42,500
so the important thing in this model which is very traditional i model was the

1753
01:48:42,500 --> 01:48:46,240
model that expert systems were built around so in a sense this is think is

1754
01:48:46,250 --> 01:48:48,180
expert systems on the web

1755
01:48:48,200 --> 01:48:52,870
it was built around the fact that they had missed answers are money down the

1756
01:48:54,860 --> 01:48:57,560
now there's another view

1757
01:48:57,570 --> 01:49:01,120
of this ontology stuff which is to happen with all that

1758
01:49:02,080 --> 01:49:06,050
what ontologies are really are was just ways of letting us to find the right

1759
01:49:06,050 --> 01:49:10,320
data at the right time and then we can use these very data intensive tools

1760
01:49:10,320 --> 01:49:11,510
to do things

1761
01:49:11,530 --> 01:49:15,620
again on exact just as i exaggerated the first few i'm exaggerating the second view

1762
01:49:17,100 --> 01:49:21,780
in a sense this he says we're going to build the semantic web

1763
01:49:21,780 --> 01:49:26,690
by simply building a lot of these little ontologies that solve little problems and then

1764
01:49:26,700 --> 01:49:28,490
link them up together

1765
01:49:28,510 --> 01:49:29,560
i happen to be

1766
01:49:29,560 --> 01:49:32,800
you know one of the well-known proponents of that view about you has its own

1767
01:49:34,000 --> 01:49:37,300
right i mean who would ever think that if you just link up a lot

1768
01:49:37,300 --> 01:49:41,670
of different stuff there could be a problem in you know two thousand years ago

1769
01:49:41,710 --> 01:49:42,950
or so

1770
01:49:43,000 --> 01:49:46,630
somebody stated that you know the best way if you have a bunch of people

1771
01:49:46,630 --> 01:49:49,320
who are trying to build the tower the best way to make fall down to

1772
01:49:49,320 --> 01:49:51,590
make them all speak different languages

1773
01:49:51,600 --> 01:49:54,210
that's the biblical explanation

1774
01:49:54,240 --> 01:49:56,320
of why we all speak why

1775
01:49:56,340 --> 01:50:00,540
some you speak german and i speak english and other people speak other languages but

1776
01:50:00,540 --> 01:50:02,400
again the idea was that

1777
01:50:02,420 --> 01:50:05,930
intuitively these guys are trying to build the tower by making it so they couldn't

1778
01:50:05,940 --> 01:50:07,700
communicate correctly

1779
01:50:07,700 --> 01:50:13,400
mask analyse genitalia has is also impacted on their brain and behavior and they have

1780
01:50:13,400 --> 01:50:16,300
been the subject of intense investigation

1781
01:50:16,490 --> 01:50:20,320
one of the things that melissa hines did is that she asked about their toy

1782
01:50:21,610 --> 01:50:25,350
and so they would lay out selection of toys for the children to choose

1783
01:50:25,490 --> 01:50:28,820
this is a little bit hard to see but i'll look into it is this

1784
01:50:28,820 --> 01:50:31,010
is place with girls toys

1785
01:50:31,030 --> 01:50:36,930
these are unaffected bigger these are unaffected girls these girls with CAH and these are

1786
01:50:36,930 --> 01:50:44,000
unaffected boys and boys h here is placed with boys' toys here's unaffected girls girls

1787
01:50:44,100 --> 01:50:49,190
c c h and boys with and without CA age now

1788
01:50:49,190 --> 01:50:52,790
there's two things that you can notice the CAH girls are much much closer to

1789
01:50:52,810 --> 01:50:56,880
boys and they are to unaffected girls the second thing you can notice is that

1790
01:50:56,880 --> 01:50:59,160
there is a huge amount of variability

1791
01:50:59,170 --> 01:51:03,260
right so there's lots and lots of overlap there are some CAH girls to play

1792
01:51:03,260 --> 01:51:08,680
with girls toys and some boys in play with girls toys r and vice versa

1793
01:51:08,690 --> 01:51:11,860
one of the things that you also might be thinking is well we do all

1794
01:51:11,860 --> 01:51:17,710
as parents push our children towards gender typed toys and this is absolutely true and

1795
01:51:17,710 --> 01:51:21,590
melissa hines is very aware of this as well as the fact that test to

1796
01:51:21,600 --> 01:51:27,340
specifically ask about the influence of parents on toy choice and unaffected boys and girls

1797
01:51:27,340 --> 01:51:33,660
there's a huge influence of parental push towards the appropriate toy choices interestingly she founded

1798
01:51:33,660 --> 01:51:39,100
c h girls are particularly resistant to this push from the parents and would persist

1799
01:51:39,100 --> 01:51:43,520
in the choice of the boy-toy regardless of pressure from their parents

1800
01:51:43,540 --> 01:51:49,110
another sex differences we can look at humans it's informative called rough-and-tumble play which is

1801
01:51:49,110 --> 01:51:52,900
basically just the way children play we call it that because what it is that

1802
01:51:52,900 --> 01:51:57,950
we can quantify the frequency at which they physically interact with each other and the

1803
01:51:57,950 --> 01:52:02,160
intensity with which they physically interact with each other this is true in children it's

1804
01:52:02,160 --> 01:52:05,850
true in poppy's is true and what of we say what is true is true

1805
01:52:05,850 --> 01:52:07,550
is the boys play

1806
01:52:08,260 --> 01:52:12,820
and tumble more than girls and the reason it was particularly interested in behavior is

1807
01:52:12,820 --> 01:52:15,380
because this activity occurs before puberty

1808
01:52:15,420 --> 01:52:21,590
before puberty hormones but after developmental forms so it's actually occurring during that window of

1809
01:52:21,590 --> 01:52:26,310
time when there are no more around so it's independent of florence boys show much

1810
01:52:26,310 --> 01:52:33,730
higher levels rough-and-tumble play in humans and primates in puppies in goats in cold in

1811
01:52:33,730 --> 01:52:38,290
rats and mice and voles and just about any species that we can measure playing

1812
01:52:38,300 --> 01:52:43,770
and this is one of the most robust and reliable sex differences across species

1813
01:52:43,800 --> 01:52:49,650
unless the history then sorry in the states at indiana university has also looked at

1814
01:52:49,650 --> 01:52:54,300
this and chk urls these are just two different studies just because whenever you're doing

1815
01:52:54,300 --> 01:52:57,840
studies on humans like this you want to make sure that you completely reveal all

1816
01:52:57,840 --> 01:53:02,840
the data and give every opportunity for accuracy and what you can see is that

1817
01:53:02,840 --> 01:53:08,600
on average the CAH girls play rougher than the control girls but these are the

1818
01:53:08,600 --> 01:53:10,530
individual girls

1819
01:53:10,540 --> 01:53:14,280
and these are put here for a reason for you again to see the enormous

1820
01:53:14,280 --> 01:53:20,880
overlap there are many unaffiliated girls play rougher than many of the CAH girls so

1821
01:53:20,900 --> 01:53:29,170
the hormones are providing prediction of behavior but by no means a determination of behaviour

1822
01:53:29,190 --> 01:53:31,960
and then one just last one example

1823
01:53:31,980 --> 01:53:38,150
this research was done in japan by distinguished investigator named arrived was also interested in

1824
01:53:38,150 --> 01:53:43,170
studying CAH girls because he spent his career studying rats and on his retirement he

1825
01:53:43,170 --> 01:53:47,090
turned to girls and he wanted to ask what's going on in their mind and

1826
01:53:47,090 --> 01:53:51,010
so he asked them to sit down and draw also that these were children between

1827
01:53:51,010 --> 01:53:54,440
three and five years old and is given a blank sheet of paper new box

1828
01:53:54,440 --> 01:53:58,820
of crayons and told to draw whatever came to their mind what this shows appears

1829
01:53:58,820 --> 01:54:03,710
the characteristics of the drawings and again you can see this very well but ninety

1830
01:54:03,710 --> 01:54:08,630
eight percent of the girls to drawing that had a person in it and a

1831
01:54:08,630 --> 01:54:11,050
flower that only

1832
01:54:11,070 --> 01:54:13,500
twenty five percent of the boys too

1833
01:54:13,500 --> 01:54:18,010
drawings had people in it but ninety three percent of the boy's true something had

1834
01:54:18,010 --> 01:54:23,860
a moving object to car train playing it's so in general the girls did was

1835
01:54:23,860 --> 01:54:26,110
is that the actual line house

1836
01:54:26,130 --> 01:54:31,180
two parents and child flowers the sun in the corner boys had no rise and

1837
01:54:31,180 --> 01:54:31,730
they had no

1838
01:54:32,240 --> 01:54:36,320
they often had a top down perspective in the hell out of frenetic energy that

1839
01:54:36,320 --> 01:54:40,340
he also DNA them give back the box of crayons and he weighed the qur'an

1840
01:54:40,340 --> 01:54:43,610
so he could get a sense of the color palette the girls all used up

1841
01:54:43,610 --> 01:54:48,460
the flesh colored crayons and the boys and they tend to use the yellows oranges

1842
01:54:48,460 --> 01:54:52,710
reds the warm colours whereas the boys tend to have a much darker blue greys

1843
01:54:52,710 --> 01:54:54,940
blacks browns powers

1844
01:54:54,960 --> 01:54:58,590
and so these are some of the drawings of just the unaffected children as i

1845
01:54:58,590 --> 01:55:02,460
said you can see will first you can see that was done in japan and

1846
01:55:02,460 --> 01:55:06,910
just to kind of give a little bit of revision of what we did yesterday

1847
01:55:09,740 --> 01:55:14,220
we essentially colour homed in on this idea that there are two big problems in

1848
01:55:14,900 --> 01:55:17,010
computer vision registration and tracking

1849
01:55:17,030 --> 01:55:21,170
the first problem is the learning for life of the ten in the second problem

1850
01:55:21,440 --> 01:55:24,660
is actually the feeling i had when i had one officially fit

1851
01:55:24,660 --> 01:55:27,530
and on the two problems are inherently interrelated

1852
01:55:27,550 --> 01:55:33,960
and lot yesterday we spoke about one way of sort of coupling these to these

1853
01:55:33,960 --> 01:55:35,580
two questions together

1854
01:55:35,630 --> 01:55:41,870
obviously using exhaustive search so we just basically exhaustively search through all the different what

1855
01:55:41,870 --> 01:55:43,650
parameters available to us

1856
01:55:43,700 --> 01:55:47,730
and the second thing then is because with this using exhaustive search

1857
01:55:47,740 --> 01:55:53,760
then if we have a computationally efficient way of doing classification say support vector machine

1858
01:55:53,760 --> 01:55:59,440
adaboost as all along with some interesting ways of computing the features quickly and we

1859
01:55:59,440 --> 01:56:05,110
covered box filters integral images we get the reasonably good job

1860
01:56:05,120 --> 01:56:09,600
but what we also speculated about at the end of yesterday

1861
01:56:09,630 --> 01:56:14,980
was that as soon as i walked space becomes more complicated than perhaps two or

1862
01:56:14,980 --> 01:56:20,040
three dimensions so since who wanted to start doing things more sophisticated than this translation

1863
01:56:20,040 --> 01:56:22,040
scale perhaps rotation

1864
01:56:22,790 --> 01:56:28,850
fall into a bit of a setback because essentially fix the dimensionality of the wall

1865
01:56:28,850 --> 01:56:33,820
space explodes and we can't exhaustively search anymore

1866
01:56:37,540 --> 01:56:40,480
now a major impact on now got way through this

1867
01:56:40,520 --> 01:56:44,010
so we kind of proposed that

1868
01:56:44,050 --> 01:56:47,180
OK good way perhaps do this is to

1869
01:56:47,580 --> 01:56:50,850
perhaps look for some sort of inherited

1870
01:56:50,900 --> 01:56:53,020
relationship between

1871
01:56:53,040 --> 01:56:55,600
a what displacement between the template

1872
01:56:55,650 --> 01:56:59,820
and the source image and actually the appearance despite so it's quite easy for me

1873
01:56:59,820 --> 01:57:05,710
to calculate the appearance displacement is basically the difference between my template my template image

1874
01:57:05,710 --> 01:57:08,440
in my source image at a certain desirable

1875
01:57:08,460 --> 01:57:11,990
but what i really want to find is this what displacement and if you guys

1876
01:57:11,990 --> 01:57:16,150
remember to yesterday slides i kept coming back to that p

1877
01:57:16,220 --> 01:57:19,360
the magical people which is the wall what factors

1878
01:57:19,430 --> 01:57:23,390
that's what work and this is what this important rules about how can i get

1879
01:57:24,210 --> 01:57:25,770
and and so

1880
01:57:26,490 --> 01:57:28,770
a kind of an argument that i was trying to make

1881
01:57:29,080 --> 01:57:32,900
towards the end of this was that perhaps this is a better way to go

1882
01:57:32,900 --> 01:57:37,700
because i it can be continuous rather than discrete b

1883
01:57:37,710 --> 01:57:43,080
because it has lost properties in terms of its relationship with perhaps don't have to

1884
01:57:43,080 --> 01:57:48,490
do exhaustive search anymore perhaps is another way first to search and i guess there

1885
01:57:48,490 --> 01:57:51,970
too big

1886
01:57:53,030 --> 01:57:54,180
and so

1887
01:57:54,180 --> 01:57:57,250
we suspect that the different problems with the noise and things so

1888
01:57:57,270 --> 01:58:02,270
how do how much noise and shift to why do i have to introduce into

1889
01:58:02,400 --> 01:58:03,770
my regressor

1890
01:58:03,770 --> 01:58:07,490
between the image displacement what the space for me to do the job

1891
01:58:07,530 --> 01:58:11,680
and we came back to initial point with the pixel coherence sort of like that

1892
01:58:12,390 --> 01:58:16,270
for small displacements it tends to be a lot of correlation in natural images between

1893
01:58:16,270 --> 01:58:20,840
pixels it only holds up to a certain certain distance

1894
01:58:20,860 --> 01:58:21,450
so i

1895
01:58:21,460 --> 01:58:25,080
that's kind of where we got up to what

1896
01:58:25,090 --> 01:58:26,680
yesterday and we

1897
01:58:26,700 --> 01:58:28,930
kind went through and we show

1898
01:58:29,080 --> 01:58:31,340
i think we've got to distinguish

1899
01:58:31,400 --> 01:58:32,460
so we've got to this

1900
01:58:32,470 --> 01:58:34,970
this gradient this is where

1901
01:58:35,080 --> 01:58:39,750
instead of covering we covered much much earlier that

1902
01:58:39,800 --> 01:58:43,780
if i want to do image interpolation i can use the first order taylor series

1903
01:58:43,780 --> 01:58:49,210
expansion but what i can also use it for is to try and estimated displacement

1904
01:58:49,220 --> 01:58:52,180
so if i got the image intensity here

1905
01:58:52,200 --> 01:58:53,120
o xo

1906
01:58:53,140 --> 01:58:57,740
plus delta x about the image intensity of x o nine o xo

1907
01:58:57,740 --> 01:59:01,520
i can define the gradient i can reasonably good job

1908
01:59:01,580 --> 01:59:03,680
so i can calculate gradient here

1909
01:59:03,680 --> 01:59:07,780
i can do in the local gradient i can do reasonably good job of calculating

1910
01:59:07,780 --> 01:59:12,520
this delta x and essentially it's just to see the hessian of the jacobian

1911
01:59:13,150 --> 01:59:16,110
of the centre of the image gradient

1912
01:59:16,150 --> 01:59:19,770
so pretty pretty standard stuff boscombe mathematics

1913
01:59:19,780 --> 01:59:24,140
and then we'll talk about how the gradients and things and all those kind of

1914
01:59:24,140 --> 01:59:28,970
advocating doing more regression talk approach to the

1915
01:59:30,090 --> 01:59:32,090
we got to

1916
01:59:32,150 --> 01:59:37,080
i believe lucas kanade a that's where we can stop stop

1917
01:59:37,080 --> 01:59:37,840
so this

1918
01:59:38,600 --> 01:59:40,890
this is kind of where where says

1919
01:59:40,900 --> 01:59:43,410
does that sound familiar everyone hopefully

1920
01:59:44,970 --> 01:59:47,310
so there there and stop the revision

1921
01:59:47,350 --> 01:59:49,220
and so

1922
01:59:49,230 --> 01:59:53,290
well starting out with lucas kanade i have now the lucas kanade algorithm is an

1923
01:59:53,290 --> 01:59:58,100
old around concurrency here that actually predates all of the other stuff are showing it's

1924
01:59:58,420 --> 01:59:59,810
actually comes from the idea

1925
01:59:59,830 --> 02:00:06,590
and in nineteen ninety and essentially it was really used for object registration such it

1926
02:00:06,590 --> 02:00:09,790
was much more used for just image matching

1927
02:00:09,810 --> 02:00:14,970
so in like stereo vision problems you've got two images in exactly the same i

1928
02:00:14,970 --> 02:00:15,840
want to

1929
02:00:15,920 --> 02:00:18,520
work at disparity map so basically

1930
02:00:18,520 --> 02:00:22,350
how one image it has changed compared to the other side of the image in

1931
02:00:22,350 --> 02:00:24,220
terms of the pixel movement some things

1932
02:00:24,340 --> 02:00:30,980
and lucas kanade i was very easy and elegant way of computing these disparities in

1933
02:00:30,980 --> 02:00:34,940
if i had the disparity of how one pixel moved between two views i could

1934
02:00:34,940 --> 02:00:36,400
then get an idea that

1935
02:00:36,420 --> 02:00:39,750
and i think get kind of like essentially get like structure

1936
02:00:39,760 --> 02:00:44,770
structure from the image and obviously the movies i have the better i could do

1937
02:00:44,770 --> 02:00:48,010
and essentially the album work what

1938
02:00:48,060 --> 02:00:52,820
so don't know what this really solid because well enough even though it looks kind

1939
02:00:52,820 --> 02:00:54,580
is old

1940
02:00:54,630 --> 02:00:58,350
there's been a lot of literature out there that essentially builds up of the fundamental

1941
02:00:59,540 --> 02:01:03,400
and the real difference between what i was talking about at the pixel level

1942
02:01:03,450 --> 02:01:05,500
lucas kanade a level

1943
02:01:05,510 --> 02:01:07,630
is that what lucas kanade does

1944
02:01:07,640 --> 02:01:10,920
is it essentially instead instead of looking for

1945
02:01:10,940 --> 02:01:14,150
individual pixel movement it looks for

1946
02:01:14,660 --> 02:01:16,340
what displacement

1947
02:01:16,340 --> 02:01:20,590
so when he originally did he did local regions so basically

1948
02:01:20,840 --> 02:01:24,320
say a three by three or four by four five five region

1949
02:01:24,360 --> 02:01:29,290
essentially because individual pixels are too noisy cackle space of so if you got allowed

1950
02:01:29,290 --> 02:01:32,060
to support region you know approximately

1951
02:01:32,060 --> 02:01:36,690
and the social aspects of the web very important they often consortium of course as

1952
02:01:36,690 --> 02:01:41,060
well the whole technology and society domain deals with things in fact

1953
02:01:41,080 --> 02:01:46,290
the organisation of the semantic web is in the technology and society domain

1954
02:01:46,300 --> 02:01:49,360
the MIT will looking at how

1955
02:01:49,460 --> 02:01:54,240
to make to build the policy aware web one of the projects we have is

1956
02:01:54,670 --> 02:01:59,270
to make systems which are aware of the policies associated with data

1957
02:01:59,280 --> 02:02:00,630
those systems which

1958
02:02:00,710 --> 02:02:07,080
in which there's an interlinked set of rules which express the social policies so that

1959
02:02:07,080 --> 02:02:09,990
machines can be built which will work in policy where way

1960
02:02:12,470 --> 02:02:20,330
there is also a we have transparent transparent and accountable data mining initiative tammy project

1961
02:02:20,340 --> 02:02:21,870
is another one which

1962
02:02:21,880 --> 02:02:23,220
looks at how

1963
02:02:23,240 --> 02:02:24,630
you can build machines

1964
02:02:24,640 --> 02:02:27,870
which keep track of where did this come from what has been used what it

1965
02:02:27,870 --> 02:02:30,970
was supposed to be used for and so that when you're just about to make

1966
02:02:30,970 --> 02:02:35,170
a decision on makes forms of actions you could actually check if you're responsible

1967
02:02:35,180 --> 02:02:40,190
whether the data that you think that you've used the machines has has used to

1968
02:02:40,190 --> 02:02:43,670
help you was actually come from appropriate sources

1969
02:02:43,720 --> 02:02:46,650
i think so

1970
02:02:46,700 --> 02:02:50,890
now let's see if this usually with the semantic web since we're on semantic web

1971
02:02:50,920 --> 02:02:52,270
conference so

1972
02:02:53,250 --> 02:02:58,580
a couple of years ago i remember you yourself the florida conference that's what semantic

1973
02:02:58,580 --> 02:03:01,020
web is about integration

1974
02:03:01,030 --> 02:03:04,530
probably still agree with this statement now the question is

1975
02:03:04,940 --> 02:03:15,170
seeing semantic web community is biased to look towards this all day i don't reasoning

1976
02:03:15,170 --> 02:03:19,130
can representation guys while i'm missing here many communities which

1977
02:03:19,550 --> 02:03:23,400
should contribute lot like

1978
02:03:23,420 --> 02:03:28,020
data mining machine learning social network analysis analysis community

1979
02:03:29,550 --> 02:03:34,800
some geologists and so on so i how this it is is this impression correct

1980
02:03:34,800 --> 02:03:35,790
or not

1981
02:03:35,870 --> 02:03:39,600
i think it's changing i think that we have had a period when a certain

1982
02:03:39,600 --> 02:03:43,380
subset of the the semantic web community

1983
02:03:43,550 --> 02:03:48,970
has not buried itself into in description logic and because that's where it was the

1984
02:03:48,970 --> 02:03:52,080
time when we were working on creating the web ontology language and so that was

1985
02:03:52,080 --> 02:03:55,230
what was necessary now i have a phase when people talk a lot in the

1986
02:03:55,230 --> 02:04:00,190
corridors rules because we studying other language all these parts the north components that we

1987
02:04:00,190 --> 02:04:02,390
need for technology which

1988
02:04:02,410 --> 02:04:06,280
take several years each to produce and so it so it's been the work of

1989
02:04:06,280 --> 02:04:07,270
several years

1990
02:04:07,320 --> 02:04:11,090
meanwhile there has been web community so there have been a lot of people who

1991
02:04:11,090 --> 02:04:17,630
are still studying in the sociology or comparative media or many

1992
02:04:17,660 --> 02:04:22,030
different disciplines in which people of mathematics were complexity

1993
02:04:22,050 --> 02:04:26,350
so so there there are people who are have always been studying the where and

1994
02:04:26,530 --> 02:04:30,520
when we talk about websites will be we should include

1995
02:04:30,540 --> 02:04:33,410
so what they've been doing this so it but

1996
02:04:33,420 --> 02:04:37,060
so i think we all we need to bring everybody together and and certainly the

1997
02:04:37,060 --> 02:04:40,830
semantic people is people the semantic web people

1998
02:04:40,850 --> 02:04:46,890
you should not feel should really not be justified and just putting on bankers and

1999
02:04:46,900 --> 02:04:50,090
staying within their own discipline they should be very careful to look at the input

2000
02:04:50,090 --> 02:04:52,550
they get from other from other disciplines

2001
02:04:52,630 --> 02:04:58,630
one hundred percent for for this website is going to be one the conference or

2002
02:04:58,630 --> 02:05:03,020
something separate i think we'll see all kinds of conferences but the conference has been

2003
02:05:03,390 --> 02:05:10,050
i suppose mainstay and has been has an interesting history as it started off as

2004
02:05:10,050 --> 02:05:15,250
being sort of like foo camp sort of style everyone just very exciting new technology

2005
02:05:15,250 --> 02:05:20,120
and then it became a place where people talk about developing the

2006
02:05:20,130 --> 02:05:25,690
the dot coms and then after and those of the sun and sort of when

2007
02:05:25,690 --> 02:05:28,740
the people there was some misunderstanding that

2008
02:05:28,760 --> 02:05:32,970
there was no academic work now that the weapon being down but no but then

2009
02:05:32,970 --> 02:05:36,200
but now it's really it's really picked up because of course of course is a

2010
02:05:36,200 --> 02:05:39,480
huge amount of academic work to be done around the web many different areas and

2011
02:05:39,480 --> 02:05:43,710
if anything the web conference is very broad conference on the things like the semantic

2012
02:05:43,710 --> 02:05:48,370
web conference and other conferences the huge number of conferences just about XML

2013
02:05:48,380 --> 02:05:54,010
which is why he when only one very one facet of web technology

2014
02:05:55,730 --> 02:05:58,030
this is this year's

2015
02:05:58,040 --> 02:06:04,640
well the web conference in edinburgh so the title of the semantic web track was

2016
02:06:04,780 --> 02:06:06,290
web and semantic web

2017
02:06:06,330 --> 02:06:12,660
so partly you answered already but well in my opinion somehow semantic web has tools

2018
02:06:12,680 --> 02:06:18,340
sides of our tools two sides of the spectrum of problems which should solve so

2019
02:06:18,340 --> 02:06:21,260
one would be these all day i like site can maybe

2020
02:06:21,690 --> 02:06:24,460
hello project will be on one side and on the other side would be this

2021
02:06:24,460 --> 02:06:29,520
web scale stuff where one will deal with this ship more like shallow logic shallow

2022
02:06:29,520 --> 02:06:34,100
but michelle stuff but scalable and so on so

2023
02:06:34,270 --> 02:06:40,960
i think the whole area is kind of more like bias towards this

2024
02:06:41,810 --> 02:06:48,990
representation reasoning sites would you agree with this and is changing conditions other scalable stuff

2025
02:06:48,990 --> 02:06:52,820
well it depends who you talk to you find people involved and the number of

2026
02:06:52,870 --> 02:06:55,750
people are to involved in shallow ontologies

2027
02:06:55,910 --> 02:06:59,450
a large scale and others involved in these very deep ontologies in fact we've been

2028
02:06:59,450 --> 02:07:03,730
using the word ontology i think in a way which is in fact a bimodal

2029
02:07:04,100 --> 02:07:09,300
there are projects in which the ontology has a very large number of terms and

2030
02:07:09,300 --> 02:07:14,400
the actual value of the data of the project is in the ontology itself you

2031
02:07:14,410 --> 02:07:18,400
express it in our things like an ontology of all the diseases

2032
02:07:18,410 --> 02:07:22,520
not all the parts of the human body and how they connect in that case

2033
02:07:22,520 --> 02:07:28,110
the ontology holds the that the actually holds information the times the ontology is just

2034
02:07:28,110 --> 02:07:33,900
an ontology of six terms you need to define an RSS feed forward to define

2035
02:07:33,910 --> 02:07:39,120
the common or the couple of dozen terms you need to define a calendar entry

2036
02:07:39,160 --> 02:07:40,390
or something

2037
02:07:40,710 --> 02:07:42,340
and so

2038
02:07:42,360 --> 02:07:48,090
those ontologies are like infrastructure ontologies this small they don't change very often you have

2039
02:07:48,090 --> 02:07:53,240
some agreed with across a very wide community globally and what's interesting is the flow

2040
02:07:53,240 --> 02:07:57,320
of the data which is changing all the time but is expressed it's the only

2041
02:07:57,320 --> 02:08:01,790
instances are changing so these two different uses of the word ontology i think you

2042
02:08:01,790 --> 02:08:06,830
confuse people and people have to know i have looked at the huge ones and

2043
02:08:06,830 --> 02:08:11,060
we tried and tried to take a lot of time and in fact when he

2044
02:08:11,060 --> 02:08:16,780
was was a small one and the understanding that distinction i think this is important

2045
02:08:20,070 --> 02:08:27,030
now the question so i will conference we had one does were invited a couple

2046
02:08:27,030 --> 02:08:32,380
of venture capitalists and people with this kind of skills and some of the answer

2047
02:08:32,380 --> 02:08:35,580
which we got some help from the message we got some of the the still

2048
02:08:35,580 --> 02:08:41,590
very careful about the semantic web so what holds them back actually

2049
02:08:42,970 --> 02:08:45,470
invest a little bit more in the semantic

