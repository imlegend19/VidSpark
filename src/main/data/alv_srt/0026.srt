1
00:00:00,000 --> 00:00:06,320
what do these CSP patterns look like so

2
00:00:08,770 --> 00:00:15,950
these three classes left foot right foot sorry left hand right hand foot

3
00:00:19,250 --> 00:00:20,280
the nose

4
00:00:20,280 --> 00:00:24,330
and you see all these crosses correspond to sensors

5
00:00:24,350 --> 00:00:29,640
so as i said if you move or imagine your left hand then on the

6
00:00:29,650 --> 00:00:30,880
right hemisphere

7
00:00:30,890 --> 00:00:33,060
there's some activity

8
00:00:33,130 --> 00:00:39,180
and in is essentially what what we're always trying to do is to create a

9
00:00:39,180 --> 00:00:41,890
spatial filter that reflects this

10
00:00:43,310 --> 00:00:47,280
we would the most discriminative is so to say to to see it to have

11
00:00:47,280 --> 00:00:53,130
the filter that that takes into into account its physiology so

12
00:00:53,130 --> 00:01:00,370
discriminating left activity and write activity and in the obvious thing for for CSP filters

13
00:01:00,370 --> 00:01:05,540
just to have be very active in this region respectively in this region that

14
00:01:05,550 --> 00:01:14,120
so this is just a few is so that this you convolve this

15
00:01:14,160 --> 00:01:19,730
this thing so this is a positive and this is negative it's like

16
00:01:19,740 --> 00:01:21,670
it's like you

17
00:01:25,280 --> 00:01:28,640
of the motor cortex on the on the right and left side and on the

18
00:01:28,640 --> 00:01:32,970
right side and you convolve this with EEG signal

19
00:01:33,030 --> 00:01:34,130
over time

20
00:01:41,490 --> 00:01:47,140
so typically and so is maybe one more remark maybe you want to ask this

21
00:01:47,140 --> 00:01:51,870
question but maybe not one more remark markers is

22
00:01:52,300 --> 00:01:58,470
so what i haven't said so far is that typically we

23
00:01:58,490 --> 00:02:02,070
two bandpass filtering before doing this year's p

24
00:02:02,280 --> 00:02:03,960
would before computing

25
00:02:04,610 --> 00:02:07,850
it is because so we are

26
00:02:07,860 --> 00:02:11,290
looking at certain frequencies c

27
00:02:11,570 --> 00:02:13,700
twelve months also

28
00:02:13,720 --> 00:02:18,860
and we're looking in this frequency band for some very discriminating filters

29
00:02:19,790 --> 00:02:21,580
this is CSP is

30
00:02:21,630 --> 00:02:22,810
now your question

31
00:02:24,690 --> 00:02:29,300
most obvious these are these

32
00:02:29,320 --> 00:02:34,030
this is just a very simple case so you

33
00:02:36,980 --> 00:02:39,830
compute the very structure

34
00:02:41,940 --> 00:02:43,750
of the single

35
00:02:43,770 --> 00:02:46,860
down to the sea

36
00:02:48,670 --> 00:02:51,220
bandpass filter

37
00:02:51,240 --> 00:02:53,770
and then you have all these very

38
00:02:53,780 --> 00:02:54,860
for the

39
00:02:56,040 --> 00:02:57,130
i j

40
00:02:58,620 --> 00:03:00,660
what about

41
00:03:00,750 --> 00:03:02,580
and then you

42
00:03:02,590 --> 00:03:10,450
so addition whitening solve the problem that you get

43
00:03:17,180 --> 00:03:21,830
like for two classes you get what you get these

44
00:03:21,830 --> 00:03:26,330
protect the p and the protector are and you concatenate them

45
00:03:26,340 --> 00:03:32,890
and that's your project w that you can to the sea

46
00:03:32,900 --> 00:03:38,640
let me just i go

47
00:03:38,670 --> 00:03:42,480
forward a little bit step further

48
00:03:42,520 --> 00:03:45,860
and maybe it will be clearer

49
00:03:55,200 --> 00:03:57,150
this is just this side of

50
00:03:57,190 --> 00:04:04,990
i have been showing my videos information transfer rates and this is just the the

51
00:04:05,000 --> 00:04:07,630
formula how we measure

52
00:04:10,040 --> 00:04:12,000
so if we have

53
00:04:12,020 --> 00:04:19,790
p is the classification performance of or classifier that distinguish between left and right say

54
00:04:19,830 --> 00:04:22,890
and we have and the number of classes then

55
00:04:22,910 --> 00:04:28,890
can we can define the information rate precision as this so this essentially shannon

56
00:04:31,660 --> 00:04:36,740
of course there are smarter and better ways of doing that

57
00:04:36,750 --> 00:04:38,130
and for example

58
00:04:38,130 --> 00:04:41,440
he did on his thesis has some

59
00:04:42,640 --> 00:04:45,470
good insights on that if you want to

60
00:04:45,520 --> 00:04:47,630
look it up

61
00:04:47,630 --> 00:04:56,120
but this is the typical definition of the information transfer rate by the community

62
00:04:56,130 --> 00:05:01,340
of course if you think about it and in particular if you think about my

63
00:05:01,340 --> 00:05:03,040
brain pong game

64
00:05:03,040 --> 00:05:05,800
we i do some continuous control

65
00:05:05,810 --> 00:05:07,480
so you know

66
00:05:07,550 --> 00:05:12,250
this only considers some some discrete classification

67
00:05:12,280 --> 00:05:16,870
so it's it's actually for continuous control this is not the right measure for information

68
00:05:16,870 --> 00:05:18,400
transfer rate

69
00:05:18,420 --> 00:05:26,670
OK images OK so your question comes here just as the previous peer review OK

70
00:05:26,950 --> 00:05:27,960
but then

71
00:05:27,980 --> 00:05:29,730
shine this picture

72
00:05:38,130 --> 00:05:42,390
i showed you before the plot how much we can gain

73
00:05:42,410 --> 00:05:44,320
by combining

74
00:05:44,330 --> 00:05:48,320
and now i showed you the definition of the information transfer rate so you can

75
00:05:48,320 --> 00:05:52,620
actually to saying you can know what is on this axis

76
00:05:52,630 --> 00:05:58,430
that's the the shannon information formula as you sort of the last slide these are

77
00:05:58,430 --> 00:05:59,940
some subjects

78
00:06:01,140 --> 00:06:07,600
and the question of this study is the underlying question is is OK should we

79
00:06:07,600 --> 00:06:10,310
actually just

80
00:06:10,310 --> 00:06:14,230
you know ramp up the number of classes and they get another high information rate

81
00:06:14,230 --> 00:06:16,920
so if you know can we take

82
00:06:16,940 --> 00:06:21,720
hundred classes and then we get in a thousand bits per minute or something like

83
00:06:21,720 --> 00:06:26,240
that and of course this is not possible

84
00:06:26,250 --> 00:06:28,040
why because

85
00:06:28,050 --> 00:06:29,580
if we

86
00:06:29,630 --> 00:06:36,070
distinguish between different classes of all classification performance will go down a little bit because

87
00:06:36,070 --> 00:06:38,980
there's more crosstalk between the class

88
00:06:40,080 --> 00:06:46,210
a difficult problem the end and so if we go from from two classes so

89
00:06:46,630 --> 00:06:52,320
from two classes to three classes between three four classes five plus six plus is

90
00:06:52,530 --> 00:06:54,250
you see that the

91
00:06:56,390 --> 00:06:58,270
of the

92
00:06:58,340 --> 00:07:02,500
this is the information transfer rate

93
00:07:02,500 --> 00:07:05,920
thirty percent from this one i have to have these are called the mixing proportions

94
00:07:05,920 --> 00:07:07,850
and we're going to see them OK

95
00:07:07,860 --> 00:07:10,930
you can think about how we would be if you were to actually sample from

96
00:07:10,930 --> 00:07:14,910
a mixture of gaussians would first have to decide which goes into sample from with

97
00:07:14,910 --> 00:07:18,970
a certain probability and then sample from the right so you would need is mixing

98
00:07:19,890 --> 00:07:22,090
so now if you want

99
00:07:22,100 --> 00:07:25,730
if want to force mixture of gaussians to be the same as k means we

100
00:07:25,730 --> 00:07:30,050
need to take the mixing proportions to be the same for all OK equally likely

101
00:07:30,050 --> 00:07:31,130
that i take

102
00:07:31,170 --> 00:07:33,370
because because k means

103
00:07:33,390 --> 00:07:36,900
it doesn't say that one cluster is going to be bigger than the others it

104
00:07:36,900 --> 00:07:39,510
doesn't say anything about the right

105
00:07:42,970 --> 00:07:44,330
just among

106
00:07:44,340 --> 00:07:47,910
actually becomes a bit later

107
00:07:48,000 --> 00:07:50,970
sorry about that i thought i thought that

108
00:07:51,010 --> 00:07:54,300
the next slide was different one and is OK so before i tell you how

109
00:07:54,300 --> 00:07:55,380
to how to

110
00:07:55,390 --> 00:07:56,420
how to

111
00:07:56,430 --> 00:07:59,550
c k means as a mixture of gaussians

112
00:07:59,560 --> 00:08:01,470
let briefly skim through the

113
00:08:01,510 --> 00:08:04,520
through the limitations of of k means

114
00:08:04,530 --> 00:08:09,170
if you use k means you have noticed that there are initialisation problems that you

115
00:08:09,170 --> 00:08:11,680
get different solutions for different initializations

116
00:08:11,730 --> 00:08:15,930
so now here's a question to you what do you do about that in practice

117
00:08:15,940 --> 00:08:22,630
OK repeat the experiment and and then what you do which one do you keep

118
00:08:22,640 --> 00:08:26,130
OK the best one in which sense

119
00:08:27,970 --> 00:08:33,850
what you exactly so you say you want to minimize the average distortion and and

120
00:08:33,850 --> 00:08:38,010
you just we initialize and you keep the one that's what i do was well

121
00:08:38,120 --> 00:08:43,260
OK the OK another thing is

122
00:08:43,270 --> 00:08:45,780
distances are computed

123
00:08:45,800 --> 00:08:51,680
i in an isotropic way right so if i had my data was this

124
00:08:59,020 --> 00:09:03,900
so what would be the what would be the k means solution here

125
00:09:04,120 --> 00:09:06,920
i can make this longer

126
00:09:07,890 --> 00:09:09,210
and long

127
00:09:09,210 --> 00:09:10,990
so k means here

128
00:09:11,010 --> 00:09:14,140
it's actually going to decide that this is one cluster and this is one class

129
00:09:15,160 --> 00:09:17,000
and the reason is

130
00:09:17,080 --> 00:09:22,090
it doesn't know about learning how to compute distances right so it just

131
00:09:22,150 --> 00:09:23,800
just compute distances

132
00:09:23,820 --> 00:09:26,520
in the same way in in all directions right

133
00:09:26,570 --> 00:09:29,520
so if we could learn a scaling then you could very easily

134
00:09:29,550 --> 00:09:30,800
have actually

135
00:09:30,810 --> 00:09:32,980
done this right if learned

136
00:09:33,000 --> 00:09:37,800
how to compute this is good but it really doesn't

137
00:09:37,830 --> 00:09:41,620
how to find k and they're going to say much about that because when i

138
00:09:41,620 --> 00:09:44,890
talk about mixtures of gaussians i'm not going to tell you either how to find

139
00:09:44,970 --> 00:09:49,630
k so it's it's a tough one actually how to find k

140
00:09:49,700 --> 00:09:53,820
the other thing is so remember this responsibility variables that we talked about these are

141
00:09:53,820 --> 00:09:55,400
i is

142
00:09:55,480 --> 00:09:58,200
for k means there either zero or one

143
00:09:58,210 --> 00:10:02,640
there there far OK we're going to see the mixtures of gaussians they become soft

144
00:10:02,690 --> 00:10:07,530
there's also variants of k means where you can actually make himself

145
00:10:07,540 --> 00:10:10,130
that's a bit of a hack

146
00:10:10,840 --> 00:10:14,230
so this is what i was trying to two two to say before i forgot

147
00:10:14,230 --> 00:10:15,790
that i had a slide in between

148
00:10:15,890 --> 00:10:20,160
so this would be if you had a mixture of gaussians you can sort of

149
00:10:20,160 --> 00:10:23,870
say well each cluster i'm going to think of its cluster as it goes in

150
00:10:23,870 --> 00:10:30,100
distribution really write some good i'm going to imagine that when the data was generated

151
00:10:30,110 --> 00:10:32,890
it was actually generated as i said earlier by

152
00:10:32,910 --> 00:10:34,690
having a mixture of gaussians

153
00:10:34,730 --> 00:10:40,390
which are located at different places and i have certain prior probabilities of drawing from

154
00:10:40,390 --> 00:10:43,940
the OK and they have the mean and covariance function and that's how the data

155
00:10:43,940 --> 00:10:45,410
was generated and now

156
00:10:45,430 --> 00:10:46,950
i get given the data

157
00:10:46,960 --> 00:10:48,620
and i try to sort of guess

158
00:10:48,630 --> 00:10:51,090
where discussions were OK

159
00:10:51,860 --> 00:10:55,420
if we were to take goes instead that they just have no variance and they

160
00:10:55,420 --> 00:10:59,280
have just a unit variance

161
00:10:59,330 --> 00:11:03,020
and i was to take equal mixing proportions

162
00:11:04,070 --> 00:11:07,970
my model would be indeed this sort of a mixture of gaussians here despite k

163
00:11:07,970 --> 00:11:11,770
should be replaced by one over k OK right now

164
00:11:11,790 --> 00:11:16,000
let's conditioned on the fact that we're looking at one specific

165
00:11:16,930 --> 00:11:21,690
so there's little o notation mistake in this line actually

166
00:11:21,850 --> 00:11:25,530
note here

167
00:11:25,530 --> 00:11:27,440
where it says j

168
00:11:27,500 --> 00:11:31,410
it issues a case OK should be equal to j should be little agreement between

169
00:11:31,410 --> 00:11:32,030
the two

170
00:11:32,070 --> 00:11:34,860
so if we if we condition for a moment

171
00:11:34,930 --> 00:11:40,850
on the fact that we're we're computing the likelihood for fixed cluster for fixed gaussian

172
00:11:42,830 --> 00:11:45,870
we have to decide we go through all the points and we have to decide

173
00:11:45,870 --> 00:11:49,490
whether this points belong yes or no to that goes OK

174
00:11:49,520 --> 00:11:51,400
and we can do that by raising

175
00:11:51,420 --> 00:11:52,550
the the

176
00:11:52,560 --> 00:11:56,210
sort of the gods in evaluation of of each point to the power of the

177
00:11:56,240 --> 00:12:00,200
of the indicator variables right because if it's zero is going to be one and

178
00:12:00,200 --> 00:12:03,620
it doesn't contribute to the to the product right

179
00:12:03,640 --> 00:12:08,530
so basically what this is doing it says all only only compute the gaussian likelihood

180
00:12:08,530 --> 00:12:12,310
here for the points that do belong to this class OK and remember j is

181
00:12:12,310 --> 00:12:15,030
k and we're looking at classic

182
00:12:15,030 --> 00:12:18,340
so what happens if we work out the whole thing and if we work out

183
00:12:19,700 --> 00:12:24,300
if we work out the likelihood for the small right given given the responsibilities

184
00:12:24,440 --> 00:12:27,500
well if we were to do that if you look at this equation here what

185
00:12:27,500 --> 00:12:31,660
we would end up with indiana is would end up with having to

186
00:12:33,200 --> 00:12:36,620
a very similar expression to what we had in

187
00:12:36,640 --> 00:12:41,080
k means actually right

188
00:12:41,080 --> 00:12:42,930
so now you can ask yourselves

189
00:12:43,020 --> 00:12:47,190
OK so what was this all about why we need to

190
00:12:47,240 --> 00:12:50,700
look at k means is a mixture of gaussians are trying to sort of make

191
00:12:50,700 --> 00:12:54,180
a smooth introduction of mixtures of gaussians OK

192
00:12:54,980 --> 00:12:59,580
constraining mixtures of gaussians a lot to make them similar to k means and now

193
00:12:59,590 --> 00:13:05,730
what we can do is simply relax assumptions and

194
00:13:05,740 --> 00:13:09,130
OK so

195
00:13:09,170 --> 00:13:14,880
mixtures of gaussians

196
00:13:14,990 --> 00:13:16,740
as we said

197
00:13:16,750 --> 00:13:20,070
we have

198
00:13:20,070 --> 00:13:22,880
number of capital k

199
00:13:22,970 --> 00:13:25,330
garson distributions OK

200
00:13:26,560 --> 00:13:29,950
and we have some indicator variables so s

201
00:13:32,740 --> 00:13:37,070
there's one is i for each data point so if we have n data points

202
00:13:37,070 --> 00:13:40,230
we have an such indicator variables

203
00:13:40,250 --> 00:13:43,000
and this indicator variables what they say

204
00:13:43,000 --> 00:13:48,090
if you know this intersection rule then it's actually immediately obvious that it's convex

205
00:13:48,110 --> 00:13:51,320
because if you look at this definition

206
00:13:51,320 --> 00:13:53,760
for a fixed fxt

207
00:13:53,780 --> 00:13:56,190
and they say that this

208
00:13:56,220 --> 00:13:57,550
for a fixed fee

209
00:13:58,490 --> 00:14:00,570
polynomial has value

210
00:14:00,680 --> 00:14:02,550
absolute value less than one

211
00:14:02,550 --> 00:14:06,080
that basically means it's great and minus one less than one

212
00:14:06,090 --> 00:14:09,220
so get two linear inequalities in the coefficient of x

213
00:14:09,230 --> 00:14:11,920
if you think it is just on

214
00:14:13,060 --> 00:14:18,410
and as a condition of x you get two linear inequality to take the

215
00:14:18,650 --> 00:14:25,400
so for fixed that defines two parallel hyperplanes tool but i ask for example in

216
00:14:26,090 --> 00:14:28,760
you might have for some

217
00:14:29,290 --> 00:14:33,880
this is hyperplane on says that polynomial has a value less than one other is

218
00:14:33,880 --> 00:14:36,390
the value has to be great and minus one

219
00:14:36,540 --> 00:14:38,570
you get two parallel hyperplanes

220
00:14:38,690 --> 00:14:45,130
everything in between is and that that's why the commission and any very t

221
00:14:45,160 --> 00:14:52,350
and as you very peaked at an intersection of many different hyperplanes infinitely many

222
00:14:53,690 --> 00:14:57,250
since all these sets are convex defined

223
00:14:57,420 --> 00:15:01,790
here we do is put these lines correspond to

224
00:15:01,810 --> 00:15:04,840
so i think this polynomial equal to one minus one

225
00:15:04,870 --> 00:15:07,550
for some fixed and then we say the

226
00:15:07,550 --> 00:15:09,790
and your thing that that's not fully political

227
00:15:09,800 --> 00:15:12,340
it's obviously convex

228
00:15:12,490 --> 00:15:19,470
that's this intersection property

229
00:15:19,510 --> 00:15:21,100
that's not very useful

230
00:15:21,130 --> 00:15:26,940
it allows you to immediately see that this is convex without actually

231
00:15:27,100 --> 00:15:30,030
writing an informative

232
00:15:30,040 --> 00:15:36,040
another two more obvious book called property is that if you

233
00:15:36,050 --> 00:15:39,860
take the image of a convex set and they're are not find information

234
00:15:39,920 --> 00:15:44,180
general something defined by this is a linear combination of experts constant

235
00:15:44,200 --> 00:15:47,890
then your opinion you context

236
00:15:47,900 --> 00:15:53,250
for example the projection of a convex set is always convex

237
00:15:53,550 --> 00:15:57,000
and also if you take say anything preimage

238
00:15:57,050 --> 00:15:59,990
and there and find information we look at all

239
00:16:00,010 --> 00:16:04,190
points that are mapped to the set s and it is i find information also

240
00:16:05,530 --> 00:16:13,370
so for example we've seen that the positive semidefinite cone is convex

241
00:16:13,530 --> 00:16:17,280
and from this it immediately follows that this

242
00:16:17,320 --> 00:16:21,300
so is convex the solution set of linear matrix inequality

243
00:16:21,340 --> 00:16:24,990
here we take this is called a linear matrix inequality

244
00:16:25,020 --> 00:16:27,280
we take in symmetry

245
00:16:27,370 --> 00:16:29,670
i mathematics b

246
00:16:29,690 --> 00:16:35,180
and the condition is that this linear combination of those made a i

247
00:16:35,200 --> 00:16:36,990
the coefficients x

248
00:16:36,990 --> 00:16:41,550
is this article to be in the matrix inequality be minus the right left hand

249
00:16:41,550 --> 00:16:44,000
side of the same

250
00:16:44,070 --> 00:16:49,440
and it follows from that just the fact that the of

251
00:16:49,500 --> 00:16:54,930
that the set of semi with convex

252
00:16:54,990 --> 00:16:57,280
and then this property

253
00:16:57,420 --> 00:16:59,670
this is actually the

254
00:16:59,690 --> 00:17:05,050
preimage of the positive semidefinite cone and a linear transformation

255
00:17:05,050 --> 00:17:09,640
b-minus the left hand side

256
00:17:09,670 --> 00:17:11,770
this also is convex

257
00:17:11,780 --> 00:17:15,820
as another example take a positive semidefinite matrix b

258
00:17:15,830 --> 00:17:19,230
and then you say the accessible both the axis based on the square of the

259
00:17:19,230 --> 00:17:21,250
transport act

260
00:17:21,250 --> 00:17:25,730
and also see as both on a

261
00:17:26,020 --> 00:17:31,070
so that's actually an indefinite quadratic inequalities to bring the second term here the left

262
00:17:31,070 --> 00:17:35,590
and you get a quadratic inequality but indefinite positive semidefinite

263
00:17:35,610 --> 00:17:38,290
and then the negative sign on the screen

264
00:17:38,370 --> 00:17:45,030
it's not immediately obvious that context an indefinite quadratic inequality

265
00:17:45,030 --> 00:17:46,820
but there's actually follows from

266
00:17:49,400 --> 00:17:52,050
again this property and the fact that this

267
00:17:52,050 --> 00:17:54,430
second order

268
00:17:54,590 --> 00:17:57,830
because we define the second order cone like this

269
00:17:57,870 --> 00:18:00,800
four are who

270
00:18:00,800 --> 00:18:05,090
and then this is actually kind of USB

271
00:18:05,160 --> 00:18:09,630
again the preimage of the second order cone and the a linear transformation

272
00:18:09,690 --> 00:18:13,250
the tea party becomes is that there equal to t

273
00:18:13,250 --> 00:18:14,890
it but i think that the

274
00:18:14,920 --> 00:18:20,860
if they can ask semantic code of the CP one half that of the ten

275
00:18:20,900 --> 00:18:23,620
the second part of the second order cone

276
00:18:23,630 --> 00:18:27,850
and then this is that does this linear function of x has to be in

277
00:18:28,160 --> 00:18:35,770
second order

278
00:18:35,810 --> 00:18:37,360
and the last one is actually

279
00:18:37,400 --> 00:18:40,360
interesting this is not that important

280
00:18:40,410 --> 00:18:45,870
we've seen that the linear image and i find transformations preserve convexity

281
00:18:45,870 --> 00:18:47,010
i have

282
00:18:48,460 --> 00:18:49,940
query variable

283
00:18:49,950 --> 00:18:52,310
accepted p one

284
00:18:52,350 --> 00:18:54,190
and i looked to see

285
00:18:54,210 --> 00:18:55,710
in my knowledge base

286
00:18:55,720 --> 00:18:58,820
what are the rules that i have not talked about

287
00:18:58,830 --> 00:19:00,860
papers being accepted

288
00:19:02,710 --> 00:19:05,210
it turned out that

289
00:19:05,220 --> 00:19:06,140
i have

290
00:19:07,960 --> 00:19:09,640
this depends on

291
00:19:09,660 --> 00:19:12,590
the quality of the paper

292
00:19:12,600 --> 00:19:16,790
and now the quality

293
00:19:18,220 --> 00:19:21,090
are there except it depends on the quality

294
00:19:21,100 --> 00:19:22,840
i can

295
00:19:25,110 --> 00:19:27,030
can construct

296
00:19:27,810 --> 00:19:33,080
local probability model

297
00:19:33,120 --> 00:19:37,980
given the information that i have

298
00:19:41,260 --> 00:19:44,030
backward chaining on high quality

299
00:19:44,040 --> 00:19:48,570
if you go back and look at the rules

300
00:19:48,590 --> 00:19:51,870
it turns out that there's two different grounding for this

301
00:19:51,930 --> 00:19:53,330
whether there

302
00:19:53,330 --> 00:19:55,030
authors our

303
00:19:55,040 --> 00:19:56,410
or the author

304
00:19:57,640 --> 00:20:01,960
and i actually do put both of these groundings into

305
00:20:01,970 --> 00:20:03,730
the constructed

306
00:20:07,360 --> 00:20:09,480
put in

307
00:20:12,920 --> 00:20:14,810
and i put in

308
00:20:19,710 --> 00:20:23,060
and now i need to create CPT

309
00:20:23,100 --> 00:20:24,320
for this

310
00:20:28,050 --> 00:20:29,130
i'm going to

311
00:20:29,160 --> 00:20:30,730
make it and noisy or

312
00:20:30,750 --> 00:20:31,800
but i have to

313
00:20:31,820 --> 00:20:32,990
can define

314
00:20:33,010 --> 00:20:35,870
how i construct the CPD

315
00:20:36,920 --> 00:20:40,950
you can see that there's is an arbitrary number of potential

316
00:20:40,960 --> 00:20:43,370
what is that these things could be in

317
00:20:43,380 --> 00:20:49,630
injuries i have to specify how this is done

318
00:20:49,640 --> 00:20:51,830
and then if i looked

319
00:20:54,460 --> 00:20:57,210
now it turns out none of them

320
00:20:57,260 --> 00:20:59,330
have any

321
00:20:59,340 --> 00:21:03,310
matching rules in the knowledge base so i can conclude

322
00:21:05,590 --> 00:21:07,170
now i go i

323
00:21:07,190 --> 00:21:09,290
and i do a search

324
00:21:09,300 --> 00:21:11,950
what evidence i have so i

325
00:21:12,480 --> 00:21:14,950
five is far

326
00:21:16,910 --> 00:21:18,640
i go through

327
00:21:18,670 --> 00:21:19,710
and you

328
00:21:20,320 --> 00:21:23,460
variable elimination that i showed earlier

329
00:21:23,460 --> 00:21:29,660
for computing the probability of the paper being accepted everything else is well defined here

330
00:21:29,710 --> 00:21:31,860
and i guess i never did that

331
00:21:31,870 --> 00:21:33,860
actually get

332
00:21:34,300 --> 00:21:39,980
so that's the idea here

333
00:21:58,570 --> 00:22:02,260
right and actually i'll talk a little bit about some of the

334
00:22:02,280 --> 00:22:03,760
kind subtleties

335
00:22:05,330 --> 00:22:07,130
i choose the nice example

336
00:22:07,870 --> 00:22:09,140
it doesn't have

337
00:22:09,160 --> 00:22:11,430
any subtleties it worked out

338
00:22:11,440 --> 00:22:12,820
very well but

339
00:22:12,830 --> 00:22:15,010
six problems come up

340
00:22:24,710 --> 00:22:26,340
well let me

341
00:22:26,350 --> 00:22:31,660
first talk about some of the ways to handle

342
00:22:37,930 --> 00:22:40,340
and then get back to that question

343
00:22:40,750 --> 00:22:42,800
as said at

344
00:22:47,340 --> 00:22:48,310
i think the

345
00:22:48,320 --> 00:22:52,260
the best way to potentially deal with negation will be the context variables and i'm

346
00:22:52,260 --> 00:22:56,130
going to talk about sex

347
00:22:58,040 --> 00:23:03,070
good question

348
00:23:03,390 --> 00:23:07,490
so you guys are anticipating i think and number of the

349
00:23:07,510 --> 00:23:10,030
the things that

350
00:23:10,040 --> 00:23:11,440
first stuff

351
00:23:11,450 --> 00:23:13,500
an even simpler one

352
00:23:15,700 --> 00:23:19,320
i was brought up the issue of

353
00:23:27,980 --> 00:23:31,460
i may well have backward chaining

354
00:23:32,300 --> 00:23:36,230
to the point that

355
00:23:36,240 --> 00:23:39,190
i get things that employ

356
00:23:39,200 --> 00:23:41,470
both the query and evidence

357
00:23:41,480 --> 00:23:43,070
and you need to

358
00:23:43,080 --> 00:23:45,300
be able to reason about the

359
00:23:45,320 --> 00:23:47,690
because it is possible

360
00:23:47,700 --> 00:23:49,430
in business for the

361
00:23:49,440 --> 00:23:53,610
query and evidence have common ancestor and

362
00:23:54,170 --> 00:24:01,320
i need to make sure to include that correctly on the basis that i

363
00:24:02,660 --> 00:24:06,610
and that may get a little bit here because i'm

364
00:24:06,630 --> 00:24:09,980
reasoning about the different possible

365
00:24:10,620 --> 00:24:15,330
instantiations of all the different roles and i have to look at the combinations and

366
00:24:15,330 --> 00:24:17,830
so on

367
00:24:17,870 --> 00:24:21,350
the nice thing is that actually is sufficient because

368
00:24:21,400 --> 00:24:23,450
using kind of the bayes that

369
00:24:26,270 --> 00:24:26,880
i was

370
00:24:28,920 --> 00:24:30,860
you can say that

371
00:24:32,880 --> 00:24:36,760
they descended the things that are separated

372
00:24:38,680 --> 00:24:43,390
are irrelevant i don't need to include them into the business and this is something

373
00:24:43,390 --> 00:24:44,350
that does

374
00:24:44,400 --> 00:24:46,760
so the huge amount

375
00:24:46,760 --> 00:24:47,890
in terms of

376
00:24:47,910 --> 00:24:49,590
the possible construction

377
00:24:49,590 --> 00:24:50,590
is not

378
00:24:59,030 --> 00:25:01,780
it often is the case that

379
00:25:01,790 --> 00:25:06,880
there's some kind of deterministic knowledge that you want to make use of

380
00:25:09,870 --> 00:25:11,550
rather than kind of

381
00:25:11,590 --> 00:25:17,580
taking that into the bay is that we want our approach is to introduce these

382
00:25:17,580 --> 00:25:20,640
the importance weight at time n minus one

383
00:25:20,680 --> 00:25:24,180
time the same quantity x one

384
00:25:24,240 --> 00:25:28,490
over gamma minus one x one n minus one

385
00:25:28,500 --> 00:25:33,290
o basically qn xn

386
00:25:34,140 --> 00:25:35,910
one minus one

387
00:25:35,930 --> 00:25:38,290
OK so before

388
00:25:38,310 --> 00:25:42,540
well i wasn't using the resampling step i was going to impose time in politics

389
00:25:42,540 --> 00:25:44,640
for time and minus one time this

390
00:25:44,640 --> 00:25:49,410
so when you have an for doing this recursion you that essentially doing importance weight

391
00:25:49,600 --> 00:25:53,290
was appalled at basically an increasing number of ten

392
00:25:53,290 --> 00:26:01,370
typically basically it violence that increases exponentially fast with at the time index no basically

393
00:26:01,410 --> 00:26:06,390
when you do resampling time and minus one

394
00:26:07,410 --> 00:26:10,890
at each time step on particle at time n minus one of the new importance

395
00:26:10,890 --> 00:26:14,500
when considering he's basically

396
00:26:14,520 --> 00:26:18,680
by this expression on was the washing or for that when the washington for that

397
00:26:18,700 --> 00:26:19,850
is very simple

398
00:26:19,910 --> 00:26:26,450
once you basically was so important part called according to their own particular solution time

399
00:26:26,450 --> 00:26:32,310
and that's what they're approximately distributed according to pi minus one

400
00:26:34,120 --> 00:26:36,200
after the sampling step

401
00:26:37,240 --> 00:26:41,950
the past x one two x one x two xn minus one xn as the

402
00:26:41,970 --> 00:26:46,240
distribution after the resampling step which is

403
00:26:46,260 --> 00:26:52,040
approximately necessary approximately

404
00:26:52,060 --> 00:26:54,640
the target distribution at time

405
00:26:54,720 --> 00:26:56,290
ten minus one

406
00:26:57,770 --> 00:27:02,640
because you've been resampling particle at time n minus one to the approximated according target

407
00:27:04,080 --> 00:27:04,910
this the

408
00:27:04,930 --> 00:27:11,430
the conditional distribution so this is why once you've basically introduced something that basically the

409
00:27:11,430 --> 00:27:15,720
importance so it is normal basically report from the problems with the time minus one

410
00:27:15,740 --> 00:27:17,520
in one of these guys

411
00:27:17,540 --> 00:27:21,120
basically it simply given by question

412
00:27:22,950 --> 00:27:28,080
once you've got the new parliament something weight at time n OK

413
00:27:28,120 --> 00:27:32,000
what you do yourself or to be the time from the associated with the empirical

414
00:27:32,000 --> 00:27:39,830
measures to obtain w new particles approximately distributed according to certain

415
00:27:43,660 --> 00:27:46,240
no you don't need to you mean for four

416
00:27:46,260 --> 00:27:47,520
in or something

417
00:27:47,540 --> 00:27:51,020
well if you just you you like to like to put more less

418
00:27:51,060 --> 00:27:55,120
you could you could you don't have to have basically and which is constant over

419
00:27:55,120 --> 00:27:58,560
time if you're so you could basically was on call

420
00:27:58,580 --> 00:28:02,410
the number m of them where i could be large large and small shop you

421
00:28:04,240 --> 00:28:07,620
he began people on that but it's possible show

422
00:28:07,640 --> 00:28:10,830
so in particular so you in france instead baseball or

423
00:28:10,850 --> 00:28:14,700
you have to kind of big of light coming out so the weights are really

424
00:28:14,700 --> 00:28:16,970
kind of don't be very well

425
00:28:16,990 --> 00:28:21,950
actually like you i then basically you could include at this stage more particles special

426
00:28:21,970 --> 00:28:23,660
you could do

427
00:28:23,680 --> 00:28:27,520
so these are stories

428
00:28:27,600 --> 00:28:32,390
OK which is like a scene for twenty or modification of the original importance sampling

429
00:28:32,410 --> 00:28:38,470
basically the reason why this is two approximation of the time solution of the approximation

430
00:28:38,540 --> 00:28:43,790
before there are some things that are basically the approximation after sampling all remember here

431
00:28:43,790 --> 00:28:48,740
that have been been abusing notation that essentially part the particle are the result of

432
00:28:48,740 --> 00:28:51,290
all articles on i just susceptibilities get

433
00:28:52,060 --> 00:28:58,160
also what does it give us well remember i was interested in approximating the normalizing

434
00:29:00,260 --> 00:29:01,370
this is in

435
00:29:01,410 --> 00:29:04,470
it's easy to check that if you look at the explosion

436
00:29:04,520 --> 00:29:09,390
of the importance weight at time n which is basically the ratio of successive target

437
00:29:11,180 --> 00:29:15,520
divided by the conditional distribution of xn given x one x two x minus one

438
00:29:15,580 --> 00:29:18,290
if you take the expectation

439
00:29:18,310 --> 00:29:24,370
o basically these normalized importance weights with respect to pi minus one two and then

440
00:29:24,370 --> 00:29:29,200
this thing is simply equal to the ratio of the successive target distribution

441
00:29:31,200 --> 00:29:34,390
OK new approximate destroys two

442
00:29:34,410 --> 00:29:36,680
well after

443
00:29:36,680 --> 00:29:41,450
the resampling step i n minus one my father caught up ultimately decided to go

444
00:29:41,450 --> 00:29:47,870
according to pi minus one they on polar component xn iconic this conditional distribution so

445
00:29:47,870 --> 00:29:51,100
basically you have some balls at time n

446
00:29:51,160 --> 00:29:55,430
before there are some things that which are approximately distributed according to this target

447
00:29:55,450 --> 00:30:03,520
so basically you can use the to basically approximates the ratio of successive normalizing constant

448
00:30:03,520 --> 00:30:09,310
by the symbol on particle average so that very simple errors

449
00:30:10,870 --> 00:30:16,040
if you're interested in the the successive rise to a normalizing constant but basically the

450
00:30:16,040 --> 00:30:21,180
normalizing constant time and you just multiply all of estimate gaza only we go you

451
00:30:21,180 --> 00:30:25,600
have basically the normalizing constant estimate of the target distribution

452
00:30:27,040 --> 00:30:28,700
so i'm a bit funny

453
00:30:28,700 --> 00:30:34,040
this is going to be a bit in poland have time is that is essentially

454
00:30:34,060 --> 00:30:39,720
user sampling scheme which is unbiased that is like essentially all the or something sky

455
00:30:39,720 --> 00:30:41,020
using in pakistan so

456
00:30:41,470 --> 00:30:46,600
basically that is is essentially the expectation of the number of offspring of particle like

457
00:30:46,740 --> 00:30:48,830
is simply proportional to its weight

458
00:30:48,850 --> 00:30:51,310
w and i then you can show

459
00:30:51,330 --> 00:30:57,330
that as in the second country pollen something case the ex chairman of the normalizing

460
00:30:57,330 --> 00:30:59,790
constant astigmatism by action

461
00:30:59,970 --> 00:31:05,140
that's quite a remarkable property despite all this interaction taking place into the algorithm used

462
00:31:05,140 --> 00:31:08,830
to live basically by the system in the normalizing constant

463
00:31:09,080 --> 00:31:15,810
OK so let's basically to kind of better products understand what's going on let's see

464
00:31:15,810 --> 00:31:21,140
what's going on when you apply to newmarket for the time as placement model OK

465
00:31:21,220 --> 00:31:24,950
so in the state space model remember what you're interested in you

466
00:31:25,410 --> 00:31:27,140
the dynamic model

467
00:31:28,100 --> 00:31:32,660
where x one is distributed

468
00:31:32,660 --> 00:31:35,740
according to new then you have a markov process

469
00:31:35,760 --> 00:31:38,060
of transition

470
00:31:40,910 --> 00:31:45,700
on you have acceleration which are approximately distributed

471
00:31:45,720 --> 00:31:51,830
i call which are distributed sort according to g OK so that define the target

472
00:31:52,970 --> 00:31:59,850
was the distribution of the first and later volleyball and the enforce of salvation which

473
00:31:59,850 --> 00:32:02,430
is simply portion the one

474
00:32:02,430 --> 00:32:04,120
that involves the data

475
00:32:04,790 --> 00:32:09,040
the data being the part of the data that we modelling otherwise of course k

476
00:32:09,040 --> 00:32:11,290
itself depends on the axis

477
00:32:11,460 --> 00:32:14,600
i call this the benefit terms

478
00:32:14,650 --> 00:32:16,100
i call this term here

479
00:32:16,150 --> 00:32:17,910
the complexity penalty

480
00:32:17,910 --> 00:32:21,360
to you probably know about the fact that when you fitting models to data you

481
00:32:21,360 --> 00:32:23,650
should worry about where the model

482
00:32:23,670 --> 00:32:24,900
is perhaps

483
00:32:24,910 --> 00:32:29,000
too complex to fit the data and you have to worry about things like overfitting

484
00:32:29,040 --> 00:32:31,150
not in the case against the process

485
00:32:31,150 --> 00:32:35,690
in the case against processes this thing taken care of by itself

486
00:32:35,700 --> 00:32:40,920
and you see that by actually observing that the marginal likelihood exactly has the trade

487
00:32:42,220 --> 00:32:45,450
a data fit term and complexity

488
00:32:45,520 --> 00:32:46,640
penalty term

489
00:32:46,680 --> 00:32:50,120
so also a third term here was very boring

490
00:32:50,930 --> 00:32:51,810
so now

491
00:32:52,320 --> 00:32:55,030
so how would we actually

492
00:32:55,080 --> 00:32:57,870
how do you actually

493
00:32:57,950 --> 00:33:01,270
go about using this thing in practice

494
00:33:02,110 --> 00:33:03,900
what you could do in practice is that

495
00:33:03,990 --> 00:33:06,440
often you have a

496
00:33:06,610 --> 00:33:08,640
covariance functions that involve

497
00:33:09,170 --> 00:33:11,030
the promise of

498
00:33:11,980 --> 00:33:15,640
we would want to try to find out what are good values of those parameters

499
00:33:15,640 --> 00:33:19,650
show you an example of that

500
00:33:19,690 --> 00:33:23,360
here's an example with the same shape of covariance function as before

501
00:33:23,380 --> 00:33:25,540
but have introduced to process here

502
00:33:25,550 --> 00:33:28,530
one parameter is is the squared here

503
00:33:28,890 --> 00:33:31,210
and one parameter is elsewhere here

504
00:33:32,150 --> 00:33:33,230
because the

505
00:33:35,850 --> 00:33:40,160
the the distribution you get over functions depend on exactly

506
00:33:40,210 --> 00:33:41,060
this is the

507
00:33:41,070 --> 00:33:43,560
the form of this this covariance function

508
00:33:43,570 --> 00:33:44,550
for example

509
00:33:44,560 --> 00:33:48,120
notice that i told you that if the axes are far apart

510
00:33:48,150 --> 00:33:48,940
then the

511
00:33:48,960 --> 00:33:52,630
covariance of the corresponding output values will be small

512
00:33:52,640 --> 00:33:56,540
if the is are close together then they have higher commands

513
00:33:56,600 --> 00:33:59,510
but i didn't tell you exactly what is meant by

514
00:33:59,550 --> 00:34:02,380
and what is meant by far apart

515
00:34:02,390 --> 00:34:07,060
this notion can be parameterized right so now down here measures

516
00:34:07,110 --> 00:34:11,620
gives you a scale to measure when is x prime deemed to be close to

517
00:34:11,620 --> 00:34:15,150
be close to each other so if l is large

518
00:34:15,240 --> 00:34:19,070
then even if they are quite far apart though still be deemed as being course

519
00:34:19,080 --> 00:34:20,530
if l is very small

520
00:34:20,540 --> 00:34:24,280
then they have to be really close before the various time

521
00:34:24,280 --> 00:34:28,930
and similarly the square just gives you the variance of the overall process

522
00:34:31,700 --> 00:34:33,110
so now let's try to fit

523
00:34:33,130 --> 00:34:36,370
again using a very simple data so let's try to fit

524
00:34:36,380 --> 00:34:39,920
i guess the process with different values of n not just focus on the values

525
00:34:39,920 --> 00:34:40,780
of the

526
00:34:41,000 --> 00:34:43,950
in general there are the parameters there is also the noise parameter

527
00:34:44,770 --> 00:34:47,910
and the overall skill from let's just focus on a on this case

528
00:34:48,010 --> 00:34:50,540
have a data set with twenty points

529
00:34:50,600 --> 00:34:55,360
and i try to fit in with three different girls and processes had three different

530
00:34:55,360 --> 00:34:57,760
values of the of the length scale parameter

531
00:34:57,770 --> 00:34:58,990
so in red

532
00:34:59,030 --> 00:35:02,120
i've chosen very short lengthscale problem

533
00:35:02,130 --> 00:35:07,370
in green are chosen immediately intermediate length scale l two very long lengthscale

534
00:35:07,420 --> 00:35:11,000
x notice in the the the bluefin here is not a very good fit to

535
00:35:11,000 --> 00:35:12,330
the data

536
00:35:16,940 --> 00:35:21,530
it the model says that even if the axes are quite far apart

537
00:35:21,570 --> 00:35:24,580
they still deemed to have high variance high

538
00:35:24,640 --> 00:35:28,100
the only way you can achieve that is by having a very small function

539
00:35:28,190 --> 00:35:34,440
of course is very small function doesn't do an incredible job of fitting the data

540
00:35:36,530 --> 00:35:39,320
so it's not clear that if the lengthscale too long

541
00:35:39,330 --> 00:35:42,810
then the the mall doesn't really fit the data very well it's it's sort of

542
00:35:42,830 --> 00:35:44,540
under fitting

543
00:35:44,750 --> 00:35:48,250
so what happens if the if the if the data is the length scale very

544
00:35:49,560 --> 00:35:53,570
that's what the the red curve here you see the red curve actually fits the

545
00:35:53,570 --> 00:35:55,420
data points almost exactly

546
00:35:56,060 --> 00:35:58,730
so if the length scale is very short you can also see that the that

547
00:35:58,740 --> 00:36:03,540
the function is able to june two new stuff so in between datapoints right

548
00:36:03,550 --> 00:36:05,160
the characteristic length scale

549
00:36:06,130 --> 00:36:09,190
maybe on the order of the link between two data points here

550
00:36:09,250 --> 00:36:10,810
so they so the

551
00:36:10,830 --> 00:36:14,510
the function can do something significant in between two data points

552
00:36:14,520 --> 00:36:18,650
you get in an almost perfect fit to the data

553
00:36:18,670 --> 00:36:21,510
and in green have the

554
00:36:22,650 --> 00:36:27,480
the value that i get if i just optimise the marginal likelihood

555
00:36:27,490 --> 00:36:30,790
notice that the the green fit is not as good as the right fit

556
00:36:30,860 --> 00:36:33,580
in terms of actually fits the training data

557
00:36:33,630 --> 00:36:37,140
but it does seem to us to be more reasonable fit but it seems that

558
00:36:37,190 --> 00:36:40,420
it's probably more reasonable to expect that a little bit of noise here

559
00:36:40,420 --> 00:36:42,070
some of w

560
00:36:42,070 --> 00:36:44,740
so this thing is is

561
00:36:44,760 --> 00:36:47,990
cohen and otherwise you can just scale up

562
00:36:48,010 --> 00:36:51,150
everything and have the better gamma

563
00:36:51,170 --> 00:36:58,360
so we make this right and it's usual kind of SVM thing we eliminate gamma

564
00:36:58,380 --> 00:37:03,360
and translate that into minimising the norm of w square

565
00:37:03,360 --> 00:37:05,210
president substitution

566
00:37:05,240 --> 00:37:07,050
so it looks very much

567
00:37:07,070 --> 00:37:13,880
like in SVM right now assume without outside variables except these constraints now you have

568
00:37:13,880 --> 00:37:16,210
three example and for each

569
00:37:16,240 --> 00:37:18,740
all ultimate for example

570
00:37:18,800 --> 00:37:23,170
you have some kind of constraint essentially constrains the truth beats out

571
00:37:24,400 --> 00:37:26,210
this alternate this runner-up

572
00:37:27,840 --> 00:37:29,690
order of the margin

573
00:37:29,690 --> 00:37:34,690
in the case where we can do this we need to add slacks the same

574
00:37:34,690 --> 00:37:37,470
way as we did SVM we add slack

575
00:37:37,530 --> 00:37:38,900
for each example

576
00:37:38,920 --> 00:37:40,110
OK so

577
00:37:40,130 --> 00:37:47,230
so i and they're essentially in allow us to violate this constraint that the penalty

578
00:37:47,230 --> 00:37:48,920
if c

579
00:37:52,670 --> 00:37:56,880
so OK so now we have this problem with this we've sort of road down

580
00:37:56,880 --> 00:37:58,740
exactly what we want

581
00:37:59,400 --> 00:38:03,650
and we could senses do brute force enumeration

582
00:38:04,300 --> 00:38:07,240
all these constraints and try to solve it

583
00:38:07,290 --> 00:38:13,670
o ou u this doesn't scale of course and we try we try to do

584
00:38:13,670 --> 00:38:14,740
instead is actually

585
00:38:15,440 --> 00:38:17,420
do this thing

586
00:38:17,440 --> 00:38:22,460
exactly the same problems through what's called so called min max formulation

587
00:38:24,230 --> 00:38:25,990
it's going to be

588
00:38:26,210 --> 00:38:30,380
it turns out was some algebra is going to turn out to be on also

589
00:38:30,440 --> 00:38:31,690
polynomial in

590
00:38:31,710 --> 00:38:34,010
all the things you care about number of wise

591
00:38:34,030 --> 00:38:36,490
number of parts in india

592
00:38:36,490 --> 00:38:37,990
in the input

593
00:38:38,010 --> 00:38:41,110
two years ago from this kind of exponential

594
00:38:41,130 --> 00:38:43,210
enumeration into something that's

595
00:38:43,240 --> 00:38:49,260
that's a much nicer and we do this is by doing the following trick it's

596
00:38:50,400 --> 00:38:53,030
simple trick

597
00:38:53,050 --> 00:38:58,340
essentially what we're seeing here is that the truth beats out everybody else

598
00:38:58,340 --> 00:39:05,260
by this amount everybody else for example so it must beat the max

599
00:39:07,880 --> 00:39:11,880
and then we're going to do is take this max foster now we have something

600
00:39:11,900 --> 00:39:16,760
we agree with the minimisation over continuous space and then embedded in this thing

601
00:39:16,780 --> 00:39:19,610
is the discrete maximisation over y

602
00:39:19,630 --> 00:39:24,340
and then the second trick is to take this

603
00:39:24,400 --> 00:39:26,340
embedded max

604
00:39:26,360 --> 00:39:31,730
and take you know you go from a discrete optimization into continuous one

605
00:39:31,740 --> 00:39:35,360
if we can do that without loss we can essentially just take that

606
00:39:35,400 --> 00:39:40,740
continues decision plug it in and have a joint continues measurable

607
00:39:41,670 --> 00:39:46,300
and that's exactly what we do

608
00:39:50,230 --> 00:39:51,630
the question is whether

609
00:39:55,960 --> 00:40:00,110
so the

610
00:40:00,130 --> 00:40:03,650
this assumption is to make this work are this structure looks like a set having

611
00:40:03,650 --> 00:40:05,340
lost that

612
00:40:05,360 --> 00:40:07,650
it decomposes in the same way

613
00:40:07,650 --> 00:40:13,070
as the model so i summer parts and compare whether i got the part right

614
00:40:13,210 --> 00:40:16,760
so this could be zero one basically but it doesn't one it could be something

615
00:40:16,760 --> 00:40:21,690
weighted as well in certain part making a mistake in a certain part might be

616
00:40:21,690 --> 00:40:23,820
more costly than another part

617
00:40:23,840 --> 00:40:26,420
so it's something decompose into parts

618
00:40:27,010 --> 00:40:30,590
so it's kind of a weighted hamming distance a

619
00:40:30,610 --> 00:40:33,070
so that's t

620
00:40:33,240 --> 00:40:39,090
and then our get decomposes again into this thing into parts this is part of

621
00:40:39,090 --> 00:40:39,820
the score

622
00:40:39,860 --> 00:40:41,300
and this is the part of the

623
00:40:44,490 --> 00:40:49,440
we're going to do then is this mapping from discrete to continuous so what we

624
00:40:49,730 --> 00:40:51,570
do is basically

625
00:40:51,590 --> 00:40:54,920
have some variables e roughly correspond to

626
00:40:55,990 --> 00:41:01,050
OK there are continuous relaxations of y and then

627
00:41:01,150 --> 00:41:02,860
q is going to be

628
00:41:02,880 --> 00:41:06,170
we're all of us are coefficients of the fact that so

629
00:41:06,740 --> 00:41:09,210
i'm going to explain what you is but

630
00:41:09,230 --> 00:41:12,170
this is going write down this this this

631
00:41:12,190 --> 00:41:17,710
descriptions a something which usually would solve the there was a weighted matching organic program

632
00:41:17,740 --> 00:41:22,550
to determine or passing things that said

633
00:41:22,570 --> 00:41:24,970
we write it down as a linear programme

634
00:41:24,990 --> 00:41:27,960
this is not to say this is how we can do prediction right this is

635
00:41:27,960 --> 00:41:32,630
the a tool to to do the learning art and kind of get the reductions

636
00:41:32,650 --> 00:41:33,670
the way you do

637
00:41:33,710 --> 00:41:36,860
your prediction and the run time is still up to you right to this is

638
00:41:36,860 --> 00:41:41,730
just a tool to actually know be able to to map these problems into something

639
00:41:44,440 --> 00:41:46,800
and then you know once we we can kind of

640
00:41:46,800 --> 00:41:48,440
see these guys are

641
00:41:48,490 --> 00:41:53,610
equivalent value in long as they return the same value this in this for now

642
00:41:53,610 --> 00:41:56,510
what dependency when w and l

643
00:41:56,530 --> 00:42:01,340
as long as it is in the same value things like this thing in end

644
00:42:01,360 --> 00:42:03,320
go that

645
00:42:03,340 --> 00:42:08,170
OK so now if this going to sort of take a step back and explain

646
00:42:08,780 --> 00:42:09,740
to do this

647
00:42:09,740 --> 00:42:13,440
argmax using linear programming for a few years problems

648
00:42:18,550 --> 00:42:23,190
and then we can come back and plug it in and see what happens

649
00:42:23,210 --> 00:42:26,530
OK so o

650
00:42:26,530 --> 00:42:30,440
the plan is to take our whatever we're going to do with the dynamic program

651
00:42:30,690 --> 00:42:36,630
and now write it down into more actually declarative way that program you usually is

652
00:42:36,670 --> 00:42:38,030
is thought to

653
00:42:38,050 --> 00:42:42,460
so what do we do that is to take a y fronts in this case

654
00:42:42,460 --> 00:42:45,420
is the thing with sequences right we take the y

655
00:42:45,440 --> 00:42:52,570
and it's still five cience five sequence and we get

656
00:42:52,590 --> 00:42:53,860
encode that

657
00:42:53,860 --> 00:42:56,190
with five variables great

658
00:42:56,190 --> 00:43:00,090
well fire exits kind of vector variables these variables

659
00:43:00,150 --> 00:43:06,570
question so these variables correspond to just going to have what i really want to

660
00:43:06,570 --> 00:43:09,380
be is a binary variable for each of these

661
00:43:09,400 --> 00:43:12,050
for it is it's possibilities again

662
00:43:12,050 --> 00:43:17,070
this corresponds to the first character this correspond to the second character that so

663
00:43:17,070 --> 00:43:21,150
was is less than thirty or years

664
00:43:21,150 --> 00:43:26,820
and those are really the students almost exclusively who have less than forty five

665
00:43:26,860 --> 00:43:29,840
for example two or approximately four

666
00:43:29,930 --> 00:43:32,930
so this is the danger zone doesn't mean

667
00:43:32,970 --> 00:43:35,240
that you will fail

668
00:43:35,340 --> 00:43:41,030
the cause is just a matter of probability says hi

669
00:43:41,070 --> 00:43:44,160
and those of you who are dying to know what are they going to get

670
00:43:46,010 --> 00:43:50,280
all i can say is that if you leave now is more than forty five

671
00:43:50,320 --> 00:43:53,130
so that you've taken all exams

672
00:43:53,150 --> 00:43:56,130
then you will probably end up with an a

673
00:43:56,130 --> 00:43:59,420
really you end up with a i don't know

674
00:43:59,470 --> 00:44:02,990
it's all a matter of probability chances i mean

675
00:44:03,090 --> 00:44:05,760
that's as far as i can go

676
00:44:05,820 --> 00:44:07,050
regarding the

677
00:44:07,050 --> 00:44:08,530
dividing lines

678
00:44:10,030 --> 00:44:12,590
course grade

679
00:44:13,560 --> 00:44:17,430
i will continue to ideas of interference

680
00:44:17,430 --> 00:44:20,300
which has very far reaching consequences

681
00:44:20,340 --> 00:44:23,740
and i will turn to what we call thin films

682
00:44:25,430 --> 00:44:27,550
and i will cover

683
00:44:27,610 --> 00:44:29,950
little incident

684
00:44:31,820 --> 00:44:34,950
doesn't change very much changed two different angles

685
00:44:34,990 --> 00:44:38,300
concept is the same

686
00:44:38,320 --> 00:44:39,510
i mean field

687
00:44:39,550 --> 00:44:41,130
i have here a

688
00:44:41,180 --> 00:44:42,860
thin film of oil

689
00:44:42,880 --> 00:44:46,400
and we will shortly see what i mean by that

690
00:44:46,450 --> 00:44:47,610
this is a air

691
00:44:48,050 --> 00:44:51,180
so we have plane waves of light coming in this way

692
00:44:51,200 --> 00:44:53,780
normal instead

693
00:44:55,180 --> 00:44:59,260
this is an one which is an that's about why

694
00:44:59,260 --> 00:45:01,030
this is and two

695
00:45:01,090 --> 00:45:02,780
it's for oil

696
00:45:02,820 --> 00:45:07,070
it is about one point four five let's make one point five

697
00:45:07,090 --> 00:45:09,050
so we run of

698
00:45:09,070 --> 00:45:10,590
this is an three

699
00:45:10,630 --> 00:45:12,110
which is again air

700
00:45:12,130 --> 00:45:15,340
in this case so that make one doesn't have to be and i will make

701
00:45:17,150 --> 00:45:18,700
and that the separation

702
00:45:18,720 --> 00:45:22,550
order thickness this i should say of or we the

703
00:45:22,590 --> 00:45:24,400
and i call this surface

704
00:45:24,400 --> 00:45:28,010
eight and the service three or air

705
00:45:28,220 --> 00:45:31,150
call that the

706
00:45:31,200 --> 00:45:32,360
so that the lights

707
00:45:32,380 --> 00:45:36,070
in a normal incidence comes in from above

708
00:45:36,090 --> 00:45:37,820
over a large surface

709
00:45:37,930 --> 00:45:39,840
i just put in one area here

710
00:45:39,840 --> 00:45:44,570
and that that intensity BIC

711
00:45:44,610 --> 00:45:47,240
now i'm going to calculate how much

712
00:45:47,280 --> 00:45:49,660
is reflected back into the air

713
00:45:49,740 --> 00:45:52,740
and i put zero here

714
00:45:52,800 --> 00:45:54,220
so offset

715
00:45:54,260 --> 00:45:56,510
just for the purpose of marriage

716
00:45:56,530 --> 00:45:58,530
so some of it is reflected

717
00:45:58,570 --> 00:46:03,220
and some of it goes straight through

718
00:46:03,260 --> 00:46:08,010
all of you are more than capable of calculating how much is reflected

719
00:46:08,070 --> 00:46:10,160
at that point eight

720
00:46:10,360 --> 00:46:13,760
are reflective of effect

721
00:46:13,820 --> 00:46:16,180
is and one-liners and

722
00:46:16,240 --> 00:46:19,050
defined by and one class and two

723
00:46:19,050 --> 00:46:21,150
that mind is o point two

724
00:46:21,150 --> 00:46:25,760
the minus sign tells you that that is the front period effect

725
00:46:25,800 --> 00:46:28,530
five hundred eighty degrees

726
00:46:29,380 --> 00:46:34,240
the intensity of the light that is reflected is then for

727
00:46:34,360 --> 00:46:36,740
this graph not

728
00:46:36,760 --> 00:46:38,860
and he four percent is reflected

729
00:46:38,880 --> 00:46:41,840
and clearly ninety six percent go through

730
00:46:41,880 --> 00:46:44,470
so if this is one hundred

731
00:46:44,510 --> 00:46:46,880
and this is four point zero

732
00:46:46,950 --> 00:46:48,920
and this is ninety six

733
00:46:49,030 --> 00:46:51,900
this slide here

734
00:46:51,920 --> 00:46:53,930
it's the surface b

735
00:46:53,990 --> 00:46:56,430
the boundary between all the air

736
00:46:56,450 --> 00:46:58,490
and some of that light go through

737
00:46:58,510 --> 00:47:00,530
is i'm not interested in

738
00:47:00,530 --> 00:47:03,550
and some of it comes back

739
00:47:03,590 --> 00:47:05,090
so i can calculate

740
00:47:05,110 --> 00:47:07,490
again the are value for that

741
00:47:07,570 --> 00:47:09,990
surface in the

742
00:47:10,030 --> 00:47:11,740
it is now and two

743
00:47:11,760 --> 00:47:16,470
and three divided by and two cross and

744
00:47:20,220 --> 00:47:23,360
this is an three so that plus o point two

745
00:47:23,460 --> 00:47:28,530
so there's no flip e vector as it

746
00:47:28,530 --> 00:47:30,400
and the

747
00:47:30,450 --> 00:47:35,760
the fraction of the light that is reflected is again for percent

748
00:47:35,780 --> 00:47:40,740
however it is four percent of the nineteen six or nineteen six percent and four

749
00:47:40,740 --> 00:47:42,660
percent of ninety six percent

750
00:47:42,720 --> 00:47:45,110
three point eight four

751
00:47:45,130 --> 00:47:51,430
which is this surface again four percent will be reflected in ninety six percent will

752
00:47:51,430 --> 00:47:56,620
but on the final advance next topic let me check with questions

753
00:47:56,760 --> 00:48:00,810
the same thing

754
00:48:00,830 --> 00:48:02,730
choose how

755
00:48:02,730 --> 00:48:04,120
they made

756
00:48:06,710 --> 00:48:10,280
yes absolutely yes the regression

757
00:48:10,340 --> 00:48:14,870
can run into the lookup in which action is not the panacea for the problem

758
00:48:14,870 --> 00:48:20,380
of overfitting on the underfitting on the it can still runs the same problems the

759
00:48:20,380 --> 00:48:24,490
way to regression on which she just said on about

760
00:48:24,500 --> 00:48:27,570
and so some some of these things that he does discover for yourself in a

761
00:48:27,570 --> 00:48:31,190
whole you actually see what you just mentioned

762
00:48:31,210 --> 00:48:35,630
it also like even though we all

763
00:48:43,190 --> 00:48:44,670
o point

764
00:48:44,670 --> 00:48:50,530
so the question is this is what is almost as you know building model because

765
00:48:50,530 --> 00:48:52,490
you need the entire dataset and

766
00:48:53,000 --> 00:48:59,800
the other saying that is that this is a nonparametric so this is an on

767
00:49:00,340 --> 00:49:04,540
i want i want to debate whether you are we really building model on on

768
00:49:04,790 --> 00:49:05,580
its own

769
00:49:05,600 --> 00:49:07,990
this is a perfectly fine

770
00:49:07,990 --> 00:49:09,140
so i think

771
00:49:09,180 --> 00:49:11,230
when you write code implementing

772
00:49:11,240 --> 00:49:14,360
locally weighted linear regression on on

773
00:49:14,380 --> 00:49:20,450
on the day said i think that cold as a whole as building your model

774
00:49:20,500 --> 00:49:22,180
well actually uses

775
00:49:22,190 --> 00:49:28,570
well actually use quite successfully to model the dynamics autonomous helicopter

776
00:49:30,240 --> 00:49:34,020
this was the last of which is the

777
00:49:35,410 --> 00:49:38,240
the what weights of the weights w i

778
00:49:38,250 --> 00:49:39,340
the the

779
00:49:39,990 --> 00:49:41,810
the yes so no

780
00:49:41,840 --> 00:49:45,620
if you can do one thing that is quite common is on how to choose

781
00:49:45,620 --> 00:49:47,680
the bandwith parameter tau

782
00:49:47,720 --> 00:49:51,480
as as using the data which is all about the later we talk what model

783
00:49:55,390 --> 00:50:00,200
a lot of

784
00:50:05,530 --> 00:50:06,510
the see

785
00:50:07,270 --> 00:50:13,190
the weights are not random variables and it's not for the purposes algorithm is not

786
00:50:13,190 --> 00:50:17,870
useful to endow it with probabilistic semantics so you could choose to define things is

787
00:50:17,870 --> 00:50:22,320
calcium but doesn't lead anywhere on in fact

788
00:50:22,350 --> 00:50:27,720
of course it turns out that i happened to choose the bell shape function

789
00:50:28,800 --> 00:50:33,120
four to define my ways is actually find which is a function that doesn't even

790
00:50:33,220 --> 00:50:39,320
greater london address infinity c c weighting function so that sense

791
00:50:39,340 --> 00:50:43,700
you could you could force the definition of the gaussians this or not useful

792
00:50:43,700 --> 00:50:49,070
special use other functions that the integrated and it's done right

793
00:50:49,180 --> 00:50:51,410
last question

794
00:50:57,010 --> 00:51:00,230
one project

795
00:51:01,250 --> 00:51:04,930
so should

796
00:51:05,020 --> 00:51:06,720
so it is very common

797
00:51:09,370 --> 00:51:14,780
because of the way to regression is on was is nonparametric our

798
00:51:14,780 --> 00:51:18,200
every time a prediction you need to fit data to the entire training set to

799
00:51:18,200 --> 00:51:22,280
get on so you have the right but if you have a very large training

800
00:51:22,280 --> 00:51:26,680
set that this is somewhat expensive alpha continues because every time you want to make

801
00:51:26,680 --> 00:51:28,620
predictions you need to fit

802
00:51:28,640 --> 00:51:30,560
you know a straight line two

803
00:51:30,580 --> 00:51:32,520
two huge datasets

804
00:51:32,610 --> 00:51:35,950
turns out there are algorithms that are

805
00:51:36,480 --> 00:51:40,110
so the way to make this much more efficient for large datasets as well as

806
00:51:40,110 --> 00:51:45,480
the little pages it look at the work of andrew more on the trees

807
00:51:45,490 --> 00:51:50,200
the figure out ways to fifty small is much more efficient dispersants

808
00:51:51,510 --> 00:51:55,980
and then mean once the questions later on

809
00:51:57,410 --> 00:52:12,980
so those the way to regression on

810
00:52:12,990 --> 00:52:14,480
remember the

811
00:52:14,490 --> 00:52:17,480
o liner had because at the beginning of this lecture what i want to do

812
00:52:17,480 --> 00:52:18,960
now is on

813
00:52:18,970 --> 00:52:22,560
talk about the probabilistic interpretation of linear regression

814
00:52:22,580 --> 00:52:29,690
and in particular of the this probabilistic interpretation that says move on to on

815
00:52:29,700 --> 00:52:33,710
talk about the just regression which would be a first classification

816
00:52:38,000 --> 00:52:51,470
the other side of the way to regression for how we just talk about ordinary

817
00:52:51,470 --> 00:52:53,770
you know unweighted linear regression

818
00:52:53,770 --> 00:52:59,120
eighteen months

819
00:53:01,190 --> 00:53:07,380
it's the only thing

820
00:53:13,440 --> 00:53:19,260
or something

821
00:53:19,270 --> 00:53:22,990
we do

822
00:53:24,240 --> 00:53:26,370
read the whole

823
00:53:26,410 --> 00:53:29,960
to draw right

824
00:53:32,380 --> 00:53:33,450
we'll see

825
00:53:34,940 --> 00:53:39,120
o general

826
00:53:41,730 --> 00:53:47,610
all square

827
00:54:06,060 --> 00:54:08,890
with with you know

828
00:54:08,920 --> 00:54:13,670
i mean you don't really get

829
00:54:21,610 --> 00:54:22,590
what i

830
00:54:24,540 --> 00:54:27,380
two these

831
00:54:29,520 --> 00:54:33,390
so we see that we can

832
00:54:33,410 --> 00:54:38,450
we growing

833
00:54:45,610 --> 00:54:48,790
then you get

834
00:54:52,380 --> 00:54:55,040
i mean you

835
00:54:55,040 --> 00:54:59,240
it is you

836
00:54:59,290 --> 00:55:03,200
problem is

837
00:55:06,010 --> 00:55:09,770
right from the start

838
00:55:14,300 --> 00:55:16,380
it's a huge

839
00:55:36,160 --> 00:55:40,210
these things

840
00:55:53,410 --> 00:55:56,660
the distribution

841
00:55:56,670 --> 00:56:01,120
so so

842
00:56:03,690 --> 00:56:09,720
in this some way all one

843
00:56:09,780 --> 00:56:12,110
this is

844
00:56:21,990 --> 00:56:24,940
in the UK

845
00:56:25,700 --> 00:56:32,380
you might be able to see what you want

846
00:56:32,510 --> 00:56:34,960
the distribution

847
00:56:36,380 --> 00:56:41,290
what is it

848
00:56:48,390 --> 00:56:50,850
so my

849
00:56:50,860 --> 00:56:55,350
the space

850
00:56:55,810 --> 00:56:58,780
we define the

851
00:57:02,170 --> 00:57:04,060
to see

852
00:57:20,380 --> 00:57:23,860
this was

853
00:57:23,860 --> 00:57:36,560
so you just

854
00:57:40,640 --> 00:57:45,510
you know this is the right

855
00:57:50,510 --> 00:57:52,850
the idea that

856
00:57:52,870 --> 00:57:57,560
this should be addressed these

857
00:57:59,420 --> 00:58:02,880
we use it

858
00:58:03,980 --> 00:58:06,450
he well

859
00:58:07,650 --> 00:58:08,840
we can't

860
00:58:08,850 --> 00:58:11,320
well three

861
00:58:25,290 --> 00:58:27,040
you know

862
00:58:29,110 --> 00:58:33,580
this is one the

863
00:58:33,580 --> 00:58:36,690
to actually implement this this policy

864
00:58:36,750 --> 00:58:42,440
so that's why we're not saying that but obviously eventually would for him in the

865
00:58:42,440 --> 00:58:45,310
improvement is pretty striking i think that's what your

866
00:58:45,620 --> 00:58:51,080
pointing out from this domain so that was a fairly surprising

867
00:58:51,100 --> 00:58:54,810
but it's expensive i've got some work we can try to get rid of this

868
00:58:54,810 --> 00:58:58,920
time by using machine learning or not going to talk about that here

869
00:59:07,730 --> 00:59:11,060
you have to read the policy or

870
00:59:11,080 --> 00:59:15,940
so there's not really learning that were the hours coming from is

871
00:59:15,960 --> 00:59:18,770
we're doing multiple stages of rollout

872
00:59:18,790 --> 00:59:22,810
so if we do two stages the complexity the number of simulator calls goes to

873
00:59:23,440 --> 00:59:28,960
hw square we do five stages this too turned into a five

874
00:59:29,900 --> 00:59:33,870
that's sort of the problem here is so that's why you can't do too many

875
00:59:36,790 --> 00:59:48,520
oh no no you never actually use the theoretical constance never ever so

876
00:59:48,620 --> 00:59:55,370
just pick it take it empirically so

877
00:59:55,460 --> 00:59:57,280
to the

878
01:00:02,150 --> 01:00:05,750
i don't know if they talked about that the papers pretty

879
01:00:05,770 --> 01:00:07,810
i memory

880
01:00:07,830 --> 01:00:09,230
i usually pick ten

881
01:00:09,250 --> 01:00:15,460
well don't know why this is not too much but

882
01:00:16,290 --> 01:00:18,060
but you

883
01:00:25,170 --> 01:00:29,230
so this result really there's only the one parameter

884
01:00:29,250 --> 01:00:34,850
w and basically are time complexity is going to scale quite dramatically with w if

885
01:00:34,850 --> 01:00:40,040
you have a lot of stages so so there's not a lot of tweaking you

886
01:00:40,040 --> 01:00:41,460
you just you know

887
01:00:41,500 --> 01:00:45,460
pick w star with ten

888
01:00:45,480 --> 01:00:48,980
increase it if you don't like that

889
01:00:50,670 --> 01:00:52,500
right so

890
01:00:52,560 --> 01:00:55,620
in these games the horizon is sort of naturally

891
01:00:55,620 --> 01:01:01,400
defined but usually if the game could go on forever you could just have arising

892
01:01:01,400 --> 01:01:03,790
cut off and

893
01:01:03,870 --> 01:01:08,520
so that's going to require domain knowledge so you sort of look at the domain

894
01:01:08,540 --> 01:01:09,940
and you say roughly

895
01:01:09,940 --> 01:01:14,500
i mean you can run your policy for example in c roughly how long does

896
01:01:14,500 --> 01:01:19,580
it typically takes two terms and then you choose the value like that

897
01:01:19,690 --> 01:01:24,620
so in the game of go it's going to be larger than in some other

898
01:01:28,190 --> 01:01:31,750
it simply a

899
01:01:34,520 --> 01:01:38,440
the baseline

900
01:01:53,000 --> 01:01:58,850
the doing search you get you can view this as a search process to some

901
01:01:58,850 --> 01:02:00,810
degree is so weird type of

902
01:02:00,830 --> 01:02:02,960
trying to visualize the

903
01:02:03,000 --> 01:02:08,100
space based space for these multi stage rollout is pretty hard i find but

904
01:02:08,100 --> 01:02:13,670
but i usually just think of it as it's doing multiple iterations of policy iteration

905
01:02:13,790 --> 01:02:18,310
that so i try to understand it be a you're you're adding search just like

906
01:02:18,310 --> 01:02:23,200
if you have a basic heuristic against the baseline performance you could do minimax searcher

907
01:02:23,200 --> 01:02:26,080
or just heuristic search process

908
01:02:42,980 --> 01:02:45,620
we generalize

909
01:02:54,960 --> 01:02:59,310
so so sort of sort of alluding to you can use machine learning to sort

910
01:03:00,290 --> 01:03:02,830
get rid of this cost and try to learn

911
01:03:02,850 --> 01:03:08,850
the original version of the rollout for policy rollout one hundred policy as the technique

912
01:03:08,850 --> 01:03:13,710
called well there are different variants but approximate policy iteration

913
01:03:13,810 --> 01:03:19,850
i've got a paper on that they basically that tries to do this so if

914
01:03:19,850 --> 01:03:23,350
if you search for if you're my web page and look for a paper with

915
01:03:23,350 --> 01:03:26,170
approximate policy iteration title you'll see

916
01:03:27,120 --> 01:03:29,960
a qualification

917
01:03:35,800 --> 01:03:37,940
one of them

918
01:03:39,170 --> 01:03:42,210
and so it is in way the

919
01:03:42,230 --> 01:03:44,480
you don't need to go

920
01:03:44,560 --> 01:03:49,480
that's what we are looking at

921
01:03:53,520 --> 01:03:54,620
i have

922
01:03:54,670 --> 01:03:59,560
so so that the the API approximate policy iteration sort of

923
01:03:59,580 --> 01:04:01,870
i will first learn this in only

924
01:04:01,920 --> 01:04:03,980
the version that generalizes to

925
01:04:04,000 --> 01:04:05,790
generate cheap oil

926
01:04:05,790 --> 01:04:08,830
i mean you could you could

927
01:04:08,900 --> 01:04:13,580
i suppose you can remember whatever you want it so you can define a new

928
01:04:13,580 --> 01:04:19,120
algorithm that tries to remember states you've been to but the basic the basic idea

929
01:04:19,120 --> 01:04:24,270
here is instead of there's no generalization and the basic rule out just

930
01:04:24,420 --> 01:04:28,480
you can instead of doing policy iteration across all states are just doing it for

931
01:04:28,480 --> 01:04:32,350
one state and it takes you time to do that

932
01:04:32,790 --> 01:04:35,080
the set of memory

933
01:04:35,100 --> 01:04:43,120
all right so as here basically trading of time memory for time writes sparse sampling

934
01:04:43,120 --> 01:04:45,160
that's the only way to

935
01:04:45,210 --> 01:04:49,810
see that section before so we went to see core because of so far all

936
01:04:50,070 --> 01:04:51,270
the u

937
01:04:51,280 --> 01:04:54,620
now we are going to see if you are

938
01:04:54,680 --> 01:04:59,820
so these are the use property when we have seen query before you have a

939
01:04:59,820 --> 01:05:04,280
single question to ask you if you want to ask question about the marginal because

940
01:05:04,280 --> 01:05:05,460
people live

941
01:05:05,580 --> 01:05:09,570
and they use this elimination

942
01:05:10,010 --> 01:05:17,070
i mean will be more

943
01:05:17,180 --> 01:05:19,810
i'm just giving example if you have a single query

944
01:05:20,200 --> 01:05:26,950
what the joint distribution px one xn

945
01:05:27,640 --> 01:05:31,610
it's all of them can be seen as

946
01:05:31,650 --> 01:05:40,970
successive elimination and that's why has the name initial will not investigate how this works

947
01:05:41,070 --> 01:05:44,710
so let's start with a graph of the light field because

948
01:05:44,740 --> 01:05:47,240
because what because it doesn't edges

949
01:05:48,000 --> 01:05:50,610
he is already

950
01:05:50,650 --> 01:05:53,940
and our goal is to compute p of x y

951
01:05:53,970 --> 01:05:56,510
with the following elimination on

952
01:05:56,520 --> 01:05:59,520
six five four three

953
01:05:59,570 --> 01:06:01,950
what is the elimination or

954
01:06:02,110 --> 01:06:05,630
order of summation of my body

955
01:06:05,680 --> 01:06:07,960
because here is the expression

956
01:06:08,590 --> 01:06:09,820
p of x y

957
01:06:09,870 --> 01:06:14,860
is the some comic book six of the joint distribution right

958
01:06:14,920 --> 01:06:18,070
so the joint distribution factorizes

959
01:06:18,180 --> 01:06:20,820
system is a markov random field

960
01:06:20,820 --> 01:06:25,390
the joint distribution factorizes in terms of one

961
01:06:25,430 --> 01:06:27,180
in terms of the maximal cliques

962
01:06:27,210 --> 01:06:31,660
so we have factories here corresponding to the maximal cliques of the graph

963
01:06:31,700 --> 01:06:33,080
we have one and two

964
01:06:33,110 --> 01:06:35,320
correspond to this clique

965
01:06:35,370 --> 01:06:38,440
one in three corresponding to this click

966
01:06:38,470 --> 01:06:41,390
three and five for this clearly

967
01:06:41,460 --> 01:06:43,690
two five and six

968
01:06:43,740 --> 01:06:46,690
corresponding to these click

969
01:06:46,760 --> 01:06:49,650
and two and four corresponding to these

970
01:06:49,660 --> 01:06:51,250
is maximal cliques

971
01:06:52,030 --> 01:06:53,690
by him sickly for theorem

972
01:06:53,700 --> 01:06:57,050
remember the expression of the joint distribution

973
01:06:57,060 --> 01:06:58,870
it is the product of

974
01:06:58,890 --> 01:06:59,750
i've tried

975
01:06:59,760 --> 01:07:02,490
one negative function over the maximal cliques

976
01:07:02,500 --> 01:07:06,840
divided by the minimisation of course

977
01:07:06,870 --> 01:07:08,160
it can all different

978
01:07:08,260 --> 01:07:13,050
so yes thanks for pointing that this specific functions can be the same function but

979
01:07:13,050 --> 01:07:14,530
they can be different so

980
01:07:14,570 --> 01:07:17,930
technically there should be some subscription

981
01:07:18,780 --> 01:07:23,700
one two one three each of these c

982
01:07:23,750 --> 01:07:25,030
thanks for that

983
01:07:25,050 --> 01:07:28,510
in general there can be different

984
01:07:28,570 --> 01:07:29,860
and then

985
01:07:29,870 --> 01:07:32,300
we just use the distributive law

986
01:07:32,330 --> 01:07:34,270
right so essentially

987
01:07:34,380 --> 01:07:35,460
you can

988
01:07:35,500 --> 01:07:39,690
think of distributive law is just pushing this summation as far as the

989
01:07:43,180 --> 01:07:45,390
which contains

990
01:07:45,410 --> 01:07:47,140
the index

991
01:07:47,950 --> 01:07:49,220
you can push through

992
01:07:49,250 --> 01:07:54,590
x except here because since we decide on the elimination order six five four three

993
01:07:55,380 --> 01:07:59,080
went to push x six first until it reaches

994
01:07:59,110 --> 01:08:03,950
then this factor and then push x five into which is for that x four

995
01:08:03,950 --> 01:08:05,450
x three two

996
01:08:05,510 --> 01:08:09,760
remember that what we're doing is simply put in evidence

997
01:08:09,810 --> 01:08:13,130
if you have a b was a AC we're seeing what

998
01:08:13,140 --> 01:08:14,600
happens a here

999
01:08:14,610 --> 01:08:16,430
the have a was

1000
01:08:16,440 --> 01:08:17,780
eight people

1001
01:08:17,790 --> 01:08:21,120
that's exactly what we are doing is just that we have more

1002
01:08:21,160 --> 01:08:23,210
we need to use the symbols

1003
01:08:23,220 --> 01:08:24,540
just using

1004
01:08:29,490 --> 01:08:33,540
OK so basically we go through this that here by using distributive law and here

1005
01:08:33,580 --> 01:08:37,220
we actually perform the song

1006
01:08:39,370 --> 01:08:43,100
we performed this song is some of the function of

1007
01:08:43,140 --> 01:08:45,680
over one of those five

1008
01:08:45,750 --> 01:08:52,620
so the results of this competition will be a functionof what

1009
01:08:52,720 --> 01:08:56,120
it will be a function of x two and x y because i'm summing x

1010
01:08:57,020 --> 01:08:59,660
so this is the function of the columns six

1011
01:08:59,720 --> 01:09:01,680
of x two and x one

1012
01:09:01,770 --> 01:09:03,640
i just called in six

1013
01:09:03,650 --> 01:09:07,440
because this function is obtained by marginalizing

1014
01:09:07,460 --> 01:09:08,690
over the body

1015
01:09:08,920 --> 01:09:11,390
six but this is just the x

1016
01:09:11,390 --> 01:09:15,190
is a function of x two and x y

1017
01:09:16,630 --> 01:09:18,890
now i can compute the sum here

1018
01:09:18,980 --> 01:09:23,020
and look that in order to compute the sum is the reason is simple

1019
01:09:27,520 --> 01:09:31,210
for all the values of x two five and six

1020
01:09:31,260 --> 01:09:33,300
so could make computation

1021
01:09:34,420 --> 01:09:37,850
because of table is to damaged

1022
01:09:38,410 --> 01:09:41,480
i have some more x five

1023
01:09:41,480 --> 01:09:46,090
and also more about that but assuming it doesn't learned the identity for some reason

1024
01:09:46,090 --> 01:09:49,930
because we put some constraint on that code then

1025
01:09:49,940 --> 01:09:54,720
actually does something interesting and if you use the same greedy layerwise trick just showed

1026
01:09:54,720 --> 01:09:58,320
to initialize a deep neural network

1027
01:09:58,340 --> 01:10:02,250
it works not as well as the stacks

1028
01:10:02,270 --> 01:10:09,070
RBN's but substantially better than training a regular deep neural network or even shallow neural

1029
01:10:10,640 --> 01:10:12,990
so what you do is you trying one autoencoder

1030
01:10:13,000 --> 01:10:13,990
then you

1031
01:10:14,010 --> 01:10:17,950
forget about the decoder weights here you just use the encoder weights here to find

1032
01:10:18,060 --> 01:10:23,090
our a new representation for the data then you can train another quarter-on-quarter

1033
01:10:23,100 --> 01:10:27,410
and then again once you've trained it you can forget about that that decoder part

1034
01:10:27,410 --> 01:10:34,950
here and you get an unsupervised transformation that's a bit more nonlinear and an abstract

1035
01:10:35,000 --> 01:10:38,040
and you can go with as many layers you want the end you can add

1036
01:10:38,550 --> 01:10:44,850
a layer that's equivalent to logistic regression and finally do this supervised fine tuning in

1037
01:10:44,850 --> 01:10:48,210
the same way that we've done for deep belief networks

1038
01:10:48,220 --> 01:10:50,050
you can even do the same

1039
01:10:50,060 --> 01:10:54,350
o thing in a purely supervised way so instead of training an autoencoder

1040
01:10:54,370 --> 01:10:57,810
you can train i one hidden layer neural network to predict

1041
01:10:57,830 --> 01:11:00,460
the the clusters of interest

1042
01:11:00,460 --> 01:11:02,500
and then throw away those weights

1043
01:11:02,720 --> 01:11:04,090
just use

1044
01:11:04,100 --> 01:11:08,300
the first hidden layer as a new representation and then train another one hidden layer

1045
01:11:08,300 --> 01:11:10,000
neural network

1046
01:11:10,010 --> 01:11:14,130
and so on and so on and in the end you get a deeper network

1047
01:11:14,130 --> 01:11:18,840
and you can fine-tune all the parameters with respect to the target

1048
01:11:18,880 --> 01:11:21,470
classes so

1049
01:11:21,490 --> 01:11:22,620
this actually

1050
01:11:23,540 --> 01:11:31,120
better than training from the beginning of a supervised neural network but it's always worse

1051
01:11:31,120 --> 01:11:35,760
than using unsupervised learning methods that i just mentioned

1052
01:11:35,790 --> 01:11:37,070
it is in

1053
01:11:37,090 --> 01:11:40,260
in experiments we we did

1054
01:11:40,290 --> 01:11:43,330
one thing that we found which

1055
01:11:44,110 --> 01:11:46,630
isn't something that geoff hinton likes but

1056
01:11:46,660 --> 01:11:48,350
is what happens

1057
01:11:50,220 --> 01:11:52,060
and i don't like it either

1058
01:11:52,070 --> 01:11:57,760
it's not enough to do this provides training and get these nice features somehow you

1059
01:11:57,760 --> 01:12:02,340
need to find you do those features for the task of of interest otherwise you

1060
01:12:02,340 --> 01:12:04,070
don't get as good performance

1061
01:12:04,090 --> 01:12:08,110
and this is what this graph shows we train

1062
01:12:08,120 --> 01:12:12,260
one of these stacks

1063
01:12:12,470 --> 01:12:18,750
RBN's or auto autoencoders and this is training time up here we only do unsupervised

1064
01:12:18,750 --> 01:12:21,460
learning and we look at the top

1065
01:12:21,480 --> 01:12:26,290
layer unsupervised layer as features and we use them with a logistic regression to predict

1066
01:12:26,290 --> 01:12:27,640
the class and so

1067
01:12:27,720 --> 01:12:33,340
the features get better and better and now if you continue doing unsupervised learning it

1068
01:12:33,520 --> 01:12:34,360
you know

1069
01:12:34,370 --> 01:12:37,800
it's still some improvement but it doesn't help so much if at this point now

1070
01:12:37,800 --> 01:12:39,690
use you decide to

1071
01:12:39,730 --> 01:12:45,040
fine tune in a purely supervised we all the parameters then the air drops significantly

1072
01:12:45,060 --> 01:12:49,100
the test error dropped significantly and ends up in a better place

1073
01:12:49,110 --> 01:12:52,660
and we try to understand what is going on there

1074
01:12:53,990 --> 01:12:56,460
o to one quarter

1075
01:12:56,460 --> 01:12:58,160
o to associate her

1076
01:12:58,180 --> 01:12:59,100
start with

1077
01:13:03,220 --> 01:13:07,590
because we didn't think about doing these experiments

1078
01:13:09,550 --> 01:13:10,950
as far as i know

1079
01:13:10,960 --> 01:13:13,090
are you

1080
01:13:13,100 --> 01:13:16,000
one that

1081
01:13:16,020 --> 01:13:18,480
not that i know of

1082
01:13:25,380 --> 01:13:27,920
i realize work

1083
01:13:30,760 --> 01:13:36,220
so we experimented with a variant of the autoencoder that is

1084
01:13:36,230 --> 01:13:39,300
working much better than the two input in fact works as well or better than

1085
01:13:39,300 --> 01:13:44,340
rbms for getting these deep networks and it's called the denoising autoencoder and the idea

1086
01:13:44,340 --> 01:13:46,460
is instead of reconstructing

1087
01:13:46,470 --> 01:13:48,060
trying to reconstruct the

1088
01:13:48,080 --> 01:13:52,100
input we can take the input we get corrupted by sending say by setting some

1089
01:13:52,100 --> 01:13:54,930
of these values in the input to zero

1090
01:13:54,940 --> 01:13:56,140
and so

1091
01:13:56,160 --> 01:13:57,860
the network sees

1092
01:13:57,870 --> 01:14:02,460
partial information about the actual input but we can ask it to reconstruct the original

1093
01:14:02,460 --> 01:14:07,720
so that's why it's called the denoising autoencoders trying to and do some stochastic corruption

1094
01:14:07,720 --> 01:14:10,570
that we're going to insert in the input

1095
01:14:10,590 --> 01:14:14,460
it's very similar to pseudolikelihood in the sense that we're trying to predict

1096
01:14:14,490 --> 01:14:18,360
we're trying to reconstruct the whole input so it's like an encoder but we're also

1097
01:14:18,360 --> 01:14:22,160
trying to predict the missing things that had been corrupted

1098
01:14:25,870 --> 01:14:28,630
it turns out that

1099
01:14:28,670 --> 01:14:33,310
it turns out that you can connected to it

1100
01:14:33,330 --> 01:14:38,710
gender model and the training criterion minimizes the variational lower bound on the generative model

1101
01:14:38,710 --> 01:14:40,320
so it

1102
01:14:40,860 --> 01:14:44,500
of the model is similar to an RBM but not quite the same

1103
01:14:44,530 --> 01:14:49,270
and you can also interpret what is going on is learning vector fields

1104
01:14:52,360 --> 01:14:55,790
that sense examples that are unlikely

1105
01:14:57,570 --> 01:14:59,230
two examples are more likely

1106
01:14:59,250 --> 01:15:03,200
so what you do is you you take it so your data lies near manifold

1107
01:15:03,230 --> 01:15:06,080
you take an example you corrupted

1108
01:15:06,490 --> 01:15:10,690
randomly that's the encoder points that i've been called that's just across option and then

1109
01:15:10,690 --> 01:15:13,870
the encoder and decoder together

1110
01:15:13,920 --> 01:15:17,550
try to map the corrupted input input back to

1111
01:15:17,770 --> 01:15:23,370
places where it like to see examples and so you this vector field that pushes

1112
01:15:23,650 --> 01:15:25,060
that points towards high

1113
01:15:25,080 --> 01:15:28,460
density regions

1114
01:15:28,460 --> 01:15:32,350
you can stack these and get pretty good results in here

1115
01:15:32,360 --> 01:15:35,900
we show results on

1116
01:15:35,940 --> 01:15:41,750
what's it's called infinite and this which is an mnist digits with

1117
01:15:41,810 --> 01:15:46,460
and i really large number of transport translations rotations so so we have a huge

1118
01:15:46,510 --> 01:15:50,990
dataset is as big as you want and

1119
01:15:51,030 --> 01:15:55,920
and here we see different experiments with one or three layers using denoising autoencoders or

1120
01:15:55,920 --> 01:15:59,480
IBM's two stack are deep architectures

1121
01:15:59,530 --> 01:16:02,850
and in this particular case the the best results

1122
01:16:02,920 --> 01:16:04,350
is online

1123
01:16:04,360 --> 01:16:06,870
test error

1124
01:16:06,890 --> 01:16:12,280
the best results are obtained with this using it one quarter with three layers

1125
01:16:12,300 --> 01:16:13,170
and the

1126
01:16:13,170 --> 01:16:14,530
rbm here

1127
01:16:14,560 --> 01:16:18,120
and then this is the curve you get with purely supervised training and you can

1128
01:16:18,120 --> 01:16:22,510
see that the slopes are different suggesting that there really going on this on the

1129
01:16:22,510 --> 01:16:23,990
log scale here

1130
01:16:23,990 --> 01:16:29,210
all right so

1131
01:16:29,370 --> 01:16:32,300
welcome to the last hour of monte carlo

1132
01:16:42,560 --> 01:16:46,420
so in the last hour the introduction is now done

1133
01:16:46,470 --> 01:16:50,580
and think the marrow

1134
01:16:50,610 --> 01:16:53,530
what people are doing these days

1135
01:16:53,540 --> 01:16:57,470
OK ninety five to for whatever we

1136
01:16:57,510 --> 01:17:01,590
we got to the twenty first century of monte carlo

1137
01:17:01,690 --> 01:17:05,870
so i'm going to go quickly

1138
01:17:06,230 --> 01:17:10,290
go quickly over here and then you talking to me afterwards

1139
01:17:10,330 --> 01:17:14,150
i need to go quickly because i would rather get you to know what exists

1140
01:17:14,150 --> 01:17:18,010
out there even if you don't understand it so is you know what is out

1141
01:17:18,010 --> 01:17:20,400
there there which can be delicately done

1142
01:17:22,610 --> 01:17:27,270
and for any part where you want the details were you know where to find

1143
01:17:29,470 --> 01:17:32,070
a bunch of people here in all this stuff

1144
01:17:32,100 --> 01:17:33,710
i haven't thought of

1145
01:17:34,790 --> 01:17:36,240
here it is

1146
01:17:36,260 --> 01:17:40,160
we need to answer the question how the clever q

1147
01:17:40,240 --> 01:17:47,490
and i'm going to go on and how they deal with towers question correlations

1148
01:17:48,940 --> 01:17:50,770
here's one way of doing this

1149
01:17:50,790 --> 01:17:53,600
the technical mixtures

1150
01:17:53,660 --> 01:17:58,130
the idea of mixtures this suppose you have to kernel t one and t two

1151
01:17:58,150 --> 01:17:59,900
that are both valid

1152
01:17:59,910 --> 01:18:01,600
and it's very easy to do that

1153
01:18:01,680 --> 01:18:05,570
because you take the metropolis hastings new play different cues

1154
01:18:05,610 --> 01:18:08,430
and because of the construction it's a valid kernel

1155
01:18:08,440 --> 01:18:12,330
so you can actually have an infinite number of different

1156
01:18:12,380 --> 01:18:16,260
kernels that are valid kernels that will have the same invariant distribution pi

1157
01:18:16,270 --> 01:18:21,070
the question is which one is best which one converges faster the solution because all

1158
01:18:21,070 --> 01:18:26,260
these things are valid asymptotically but you don't have an infinite amount of time

1159
01:18:26,300 --> 01:18:27,490
the the european

1160
01:18:27,510 --> 01:18:30,220
want to converge

1161
01:18:31,050 --> 01:18:32,450
here is the good news

1162
01:18:32,460 --> 01:18:35,920
you have won kernel to one another two

1163
01:18:35,930 --> 01:18:38,930
and what i do is multiply the topic question by

1164
01:18:44,300 --> 01:18:46,780
the bottom equation by an

1165
01:18:46,800 --> 01:18:49,030
and if you have the more you get this

1166
01:18:49,050 --> 01:18:54,460
why because of high plus one minus y is just gives you high

1167
01:18:54,480 --> 01:19:00,170
so you get five plus one month of fighting two

1168
01:19:00,860 --> 01:19:02,840
that means that if you choose

1169
01:19:02,870 --> 01:19:04,670
they have common factor

1170
01:19:04,680 --> 01:19:08,390
and if you choose then with probability alpha you choose one

1171
01:19:09,210 --> 01:19:12,210
with probability one minus obvious another algorithm

1172
01:19:12,230 --> 01:19:14,890
you still have to some invariant distribution

1173
01:19:14,900 --> 01:19:17,520
the great news is now you can construct is

1174
01:19:17,560 --> 01:19:22,080
algorithms when you have more than one heuristic and how to solve the problem

1175
01:19:22,080 --> 01:19:27,210
you can actually take a portfolio of them

1176
01:19:27,270 --> 01:19:31,900
and you start randomizing you choose between these

1177
01:19:31,900 --> 01:19:35,170
and then if you want to go one step further you try to control the

1178
01:19:35,170 --> 01:19:36,490
markov chain

1179
01:19:36,490 --> 01:19:38,030
with some

1180
01:19:38,050 --> 01:19:42,490
the maximum likelihood estimate of office you sample

1181
01:19:42,500 --> 01:19:48,550
so you construct adaptive change that allows you to select between these kernels

1182
01:19:48,610 --> 01:19:52,520
and you can do that that's actually a lot of work

1183
01:19:52,740 --> 01:19:56,110
in general is the gulf by

1184
01:19:56,840 --> 01:19:59,840
try them and see what worse than

1185
01:19:59,860 --> 01:20:00,400
that's true

1186
01:20:01,620 --> 01:20:06,020
but the important thing is you can have these mixtures which is very very

1187
01:20:06,060 --> 01:20:08,680
so here's an example of this is actually based on

1188
01:20:08,730 --> 01:20:12,550
a consulting job for the

1189
01:20:12,580 --> 01:20:15,590
for the french atomic energy

1190
01:20:15,610 --> 01:20:19,920
so they have a specific signal by affects the distribution

1191
01:20:20,670 --> 01:20:23,750
who is familiar with the FFT

1192
01:20:23,830 --> 01:20:25,930
what we're trying to get good

1193
01:20:25,990 --> 01:20:29,140
so sometimes you want to compute fifty of the signal

1194
01:20:29,180 --> 01:20:30,860
and the problem the of fifty

1195
01:20:30,870 --> 01:20:33,240
is it is not very good at resolving

1196
01:20:33,300 --> 01:20:35,050
peaks nearby

1197
01:20:35,060 --> 01:20:36,650
OK in the frequency domain

1198
01:20:36,670 --> 01:20:39,240
so they wanted something that would work better

1199
01:20:39,250 --> 01:20:40,790
thank you for t

1200
01:20:41,810 --> 01:20:47,590
we think that signal in the frequency domain we interpret it as the probability distribution

1201
01:20:49,140 --> 01:20:51,260
we come up with two proposal

1202
01:20:51,310 --> 01:20:52,950
one proposal is

1203
01:20:52,980 --> 01:20:54,610
we compute FFT

1204
01:20:54,610 --> 01:20:58,620
and the fifty just picks one peak doesn't pick the two peaks

1205
01:20:58,740 --> 01:21:00,350
we use the FFT

1206
01:21:00,380 --> 01:21:02,240
two more or less tells us

1207
01:21:02,250 --> 01:21:05,430
well the big peaks are

1208
01:21:05,440 --> 01:21:10,560
so it's a proposal distribution we computed fifty and then we put a little cussin

1209
01:21:10,560 --> 01:21:12,740
around the peaks of f t

1210
01:21:12,760 --> 01:21:16,360
we can try to make sure that some of first q

1211
01:21:16,410 --> 01:21:18,760
and this is a good q because this ql

1212
01:21:18,780 --> 01:21:23,090
makers is like an importance sampling makers jump to what is important

1213
01:21:23,130 --> 01:21:24,390
two of

1214
01:21:24,440 --> 01:21:25,530
this big stuff

1215
01:21:25,660 --> 01:21:30,470
the next year that we choose is a random walk

1216
01:21:30,480 --> 01:21:32,310
because the

1217
01:21:32,370 --> 01:21:33,500
you jump here

1218
01:21:33,510 --> 01:21:38,050
and then you to local exploration and then you jump here the local exploration then

1219
01:21:38,050 --> 01:21:41,230
the flow limiting a lot of details

1220
01:21:41,240 --> 01:21:52,470
the service profile is all about advertising and matchmaking services so it's used to characterize

1221
01:21:52,520 --> 01:21:55,240
so about one summary level

1222
01:21:55,260 --> 01:22:02,000
so that if you can think of yellow pages style of description that that's designed

1223
01:22:03,530 --> 01:22:09,520
you a quick look up and and identification of services most useful for a particular

1224
01:22:09,520 --> 01:22:14,280
need so the idea of the discrete of these profiles is that they can be

1225
01:22:14,280 --> 01:22:21,210
used to populate service registry such as UDDI registries which were

1226
01:22:21,220 --> 01:22:23,760
we've seen a lot of tension a few years ago

1227
01:22:26,140 --> 01:22:32,130
it should be noted that the profile can be used both for construction of the

1228
01:22:32,510 --> 01:22:39,260
advertisment for the request that can be used to match against those advertisments should also

1229
01:22:39,260 --> 01:22:44,530
be mentioned service can have many different profiles is not just one right way to

1230
01:22:44,530 --> 01:22:48,090
describe the services might be described in different ways

1231
01:22:48,100 --> 01:22:52,230
three different communities of possible users

1232
01:22:54,080 --> 01:23:01,350
this is an ontology bubble diagrams the simple really really ovals or classes and the

1233
01:23:01,760 --> 01:23:06,170
arrows are properties and this is just

1234
01:23:06,530 --> 01:23:09,800
basically a lot of what's in the profile

1235
01:23:09,810 --> 01:23:14,100
really breaks down into what is known as the functional

1236
01:23:14,110 --> 01:23:21,670
or capabilities descriptions on the left side and non functional descriptions it's on the right

1237
01:23:21,670 --> 01:23:28,200
side which is things like provenance associated with the service policies associated with the service

1238
01:23:28,200 --> 01:23:35,520
quality of service aspects those are probably the errors are used that are expressed most

1239
01:23:35,520 --> 01:23:40,110
commonly in terms of nonfunctional properties this functional properties

1240
01:23:40,500 --> 01:23:48,040
inputs outputs preconditions and effects are actually so recap or summary of functional description which

1241
01:23:48,040 --> 01:23:52,040
comes in the process model so i'll come back to that later in just a

1242
01:23:52,040 --> 01:23:54,590
moment or two and i talk about the process model

1243
01:23:54,610 --> 01:23:57,680
but you can see this is actually quite simple

1244
01:23:57,710 --> 01:24:04,260
so of properties in classes here this is just the basics here the core profile

1245
01:24:04,260 --> 01:24:07,350
but is really critical to know

1246
01:24:07,360 --> 01:24:12,720
if is designed for extensibility in various directions so for example there's nothing here in

1247
01:24:12,720 --> 01:24:14,420
the core profile

1248
01:24:14,440 --> 01:24:16,970
about quality of service

1249
01:24:17,100 --> 01:24:25,080
properties and classes but that has been provided by different groups various research teams in

1250
01:24:25,170 --> 01:24:27,780
various extensions of the profile

1251
01:24:27,800 --> 01:24:35,800
probably the most prevalent using the pro was profiles is to construct a class hierarchy

1252
01:24:35,800 --> 01:24:42,510
of services again for for discovery and matchmaking purposes and this is

1253
01:24:42,520 --> 01:24:47,310
this is the style of his own makes you takes advantage of

1254
01:24:47,330 --> 01:24:54,010
reasoning capabilities in most natural to and description logics OWL DL in particular so classifying

1255
01:24:54,010 --> 01:25:00,780
services depending on their properties and constructing a class hierarchy or you you know additional

1256
01:25:00,780 --> 01:25:02,850
properties as you come down to

1257
01:25:02,960 --> 01:25:06,140
two greater detail greater specialization

1258
01:25:06,140 --> 01:25:09,010
and this is this can be

1259
01:25:09,020 --> 01:25:17,260
tied in with existing standards which predated semantic web searches UNSPSC another such product and

1260
01:25:17,260 --> 01:25:23,980
service standards and some people have actually we express these these kind of standards using

1261
01:25:24,670 --> 01:25:31,070
similar things so i think i've already mentioned that you can have multiple profiles you

1262
01:25:31,070 --> 01:25:32,380
can even multiple

1263
01:25:32,400 --> 01:25:38,490
different kinds of hierarchies in which service profiles for the same service might be situated

1264
01:25:38,490 --> 01:25:40,080
for different purposes

1265
01:25:40,100 --> 01:25:43,230
four different classes of users

1266
01:25:43,240 --> 01:25:46,600
so i think i mentioned it

1267
01:25:46,620 --> 01:25:51,680
profile is typically used to subset of the class hierarchical yellow pages although of course

1268
01:25:51,680 --> 01:25:56,280
there is a great deal more expressiveness then you can go with just the bare-bones

1269
01:25:56,280 --> 01:26:02,760
taxonomy and as we can take advantage of the reasoning capabilities of OWL two

1270
01:26:02,780 --> 01:26:08,030
to do much more precise matching and what you could do with just the taxonomic

1271
01:26:08,710 --> 01:26:13,880
categorizations the profiles also been used for planning purposes

1272
01:26:13,880 --> 01:26:19,230
in which case the focus is on the functional properties inputs outputs preconditions and effects

1273
01:26:20,210 --> 01:26:22,560
as i said focus on a little bit more

1274
01:26:22,580 --> 01:26:24,920
in just a moment

1275
01:26:25,010 --> 01:26:28,190
so we are coming to the process models

1276
01:26:28,220 --> 01:26:33,940
process is really the heart of OWL s where you have all the details about

1277
01:26:33,940 --> 01:26:38,140
how the service works you can spell out the steps

1278
01:26:38,140 --> 01:26:44,930
that the service will follow his wherever workflows style of representation and

1279
01:26:44,960 --> 01:26:52,060
with the degree of detail and description of the preconditions and effects there's lots of

1280
01:26:52,060 --> 01:27:00,540
different kinds of reasoning that can be done to support automated invocation planning composition automated

1281
01:27:00,550 --> 01:27:08,470
enactment interoperation interoperation monitoring service progress and so forth and so on

1282
01:27:08,490 --> 01:27:14,730
this is zero processes of inputs outputs preconditions and effects and also a bit more

1283
01:27:14,730 --> 01:27:18,810
about these categories of processes the next slide

1284
01:27:18,840 --> 01:27:21,250
so the

1285
01:27:21,290 --> 01:27:23,520
this is a very minor detail

1286
01:27:23,530 --> 01:27:26,710
in the process model that just a high level

1287
01:27:26,710 --> 01:27:27,900
based on the

1288
01:27:27,900 --> 01:27:30,570
so i listed are very fast

1289
01:27:30,590 --> 01:27:31,980
and it

1290
01:27:32,030 --> 01:27:33,300
and a broke already

1291
01:27:33,360 --> 01:27:36,980
so this is peter is very important

1292
01:27:37,130 --> 01:27:38,940
and faraday they

1293
01:27:38,980 --> 01:27:41,590
made this discovery

1294
01:27:41,630 --> 01:27:44,440
he was asked by a news reporter

1295
01:27:44,480 --> 01:27:49,300
whether this would ever be of any practical use

1296
01:27:49,360 --> 01:27:51,280
and he answer

1297
01:27:51,280 --> 01:27:54,300
someday you will tax it

1298
01:27:54,360 --> 01:27:57,670
he had that vision

1299
01:27:57,670 --> 01:28:02,090
now indeed what you have seen here looked in the nineteenth century

1300
01:28:02,130 --> 01:28:03,860
as a very

1301
01:28:03,860 --> 01:28:06,650
in this and demonstration

1302
01:28:06,670 --> 01:28:08,460
however this simple

1303
01:28:08,460 --> 01:28:10,340
from normal

1304
01:28:10,360 --> 01:28:13,460
what i'm going to tell you know is no exaggeration

1305
01:28:13,460 --> 01:28:15,260
this simple phenomenon

1306
01:28:15,260 --> 01:28:18,670
runs our entire economy

1307
01:28:18,710 --> 01:28:20,460
it runs or

1308
01:28:21,800 --> 01:28:22,670
the common

1309
01:28:22,690 --> 01:28:31,090
without this phenomenon we would still live more or less like our ancestors in the

1310
01:28:31,090 --> 01:28:33,820
seventeenth and eighteenth century

1311
01:28:33,820 --> 01:28:35,440
we would have candlelight

1312
01:28:35,480 --> 01:28:37,090
for light

1313
01:28:37,190 --> 01:28:41,170
no radio no television no telephone no computer no internet

1314
01:28:41,170 --> 01:28:43,130
no laptop

1315
01:28:46,590 --> 01:28:51,010
imagine how miserable you live food

1316
01:28:51,090 --> 01:28:52,530
without the thing

1317
01:28:52,590 --> 01:28:54,110
without computer games

1318
01:28:54,130 --> 01:28:58,130
without video games without your ipod

1319
01:28:58,130 --> 01:28:59,840
why don't you admit

1320
01:28:59,860 --> 01:29:04,010
you spend a lot of time right before of the two

1321
01:29:04,050 --> 01:29:05,130
like this

1322
01:29:08,070 --> 01:29:10,940
for electricity is produced

1323
01:29:12,110 --> 01:29:13,690
electric generators

1324
01:29:13,730 --> 01:29:16,190
by power stations

1325
01:29:16,260 --> 01:29:19,460
and all they do is they move copper corals

1326
01:29:19,510 --> 01:29:23,780
through magnetic fields

1327
01:29:23,900 --> 01:29:26,130
if you want to move copper coil

1328
01:29:32,110 --> 01:29:34,480
you have to do work

1329
01:29:34,530 --> 01:29:35,610
that means

1330
01:29:35,630 --> 01:29:39,610
you have to put in tennessee

1331
01:29:39,630 --> 01:29:44,670
i put in the energy when i move in and out

1332
01:29:44,730 --> 01:29:47,630
but if you want to do that on a big scale

1333
01:29:47,690 --> 01:29:49,400
you need energy sources

1334
01:29:49,440 --> 01:29:54,090
to do that for you need for instance called which is very common source of

1335
01:29:55,130 --> 01:29:58,030
or nuclear energy they drive termites

1336
01:29:58,070 --> 01:30:00,030
and those turbines rotate

1337
01:30:00,050 --> 01:30:02,570
coral through a magnetic field

1338
01:30:02,610 --> 01:30:04,170
of course you can also

1339
01:30:04,210 --> 01:30:05,920
generating electricity

1340
01:30:05,920 --> 01:30:10,820
these waterfalls like the niagara falls and in some cases was wins

1341
01:30:10,860 --> 01:30:13,960
but it takes energy to produce

1342
01:30:18,710 --> 01:30:23,070
like bolts and all kinds of electric appliances consumer

1343
01:30:25,420 --> 01:30:27,780
and this energy consumption

1344
01:30:27,820 --> 01:30:29,090
is expressed

1345
01:30:29,760 --> 01:30:31,510
in terms of what's

1346
01:30:31,550 --> 01:30:34,280
named after james watt's

1347
01:30:34,340 --> 01:30:37,150
who did a lot of research in this area

1348
01:30:37,190 --> 01:30:40,050
in the eighteenth and nineteenth century

1349
01:30:40,090 --> 01:30:42,050
you have light bulb

1350
01:30:42,110 --> 01:30:43,570
at home

1351
01:30:43,630 --> 01:30:47,820
four mark forty w that means forty watts

1352
01:30:47,860 --> 01:30:49,530
you've got to

1353
01:30:49,610 --> 01:30:52,940
that makes a hundred what they do is more like

1354
01:30:53,000 --> 01:30:56,360
it means that this one if you run it for one hour

1355
01:30:56,440 --> 01:30:59,460
uses two-and-a-half times more energy

1356
01:30:59,460 --> 01:31:01,240
and this one

1357
01:31:01,260 --> 01:31:02,940
this is the energy

1358
01:31:07,880 --> 01:31:10,030
i have here

1359
01:31:10,050 --> 01:31:15,420
of electric generator

1360
01:31:15,460 --> 01:31:18,420
you can rotate

1361
01:31:18,420 --> 01:31:20,980
i want to find the best the point you were sort of the features in

1362
01:31:20,980 --> 01:31:22,460
f one and f two

1363
01:31:22,510 --> 01:31:26,740
and the candidates but points are really be between one and two

1364
01:31:26,810 --> 01:31:31,440
two and three and you would pick the one with the highest game

1365
01:31:31,460 --> 01:31:36,250
and in this case is if one is less than point five

1366
01:31:37,030 --> 01:31:42,590
using this you would split you petitioner datasets on the left to right

1367
01:31:42,640 --> 01:31:47,470
and you repeat the process of the tree and once again the boosting part

1368
01:31:49,410 --> 01:31:52,530
you can have several different gradients in this case it so

1369
01:31:52,550 --> 01:31:53,730
as i mentioned earlier

1370
01:31:54,490 --> 01:32:01,550
primary use least squares regression and for some ranking specific ones they're like be rank

1371
01:32:05,360 --> 01:32:06,740
so an example

1372
01:32:06,750 --> 01:32:11,730
this tree in this case you have you have you can see like this the

1373
01:32:11,730 --> 01:32:13,850
future might actually be split multiple

1374
01:32:13,860 --> 01:32:16,840
time break even within the same tree

1375
01:32:16,850 --> 01:32:24,790
so i would discuss likes the two different types of implementation so we tried

1376
01:32:24,800 --> 01:32:29,210
so an obvious motivations are really in larger datasets and we want to get a

1377
01:32:29,210 --> 01:32:34,010
speedup but one of the additional constraint that we had as well as that our

1378
01:32:34,010 --> 01:32:37,990
datasets that we're working on was still relatively small so

1379
01:32:38,000 --> 01:32:47,260
it was editorial data for ranking and the the good overall goes for maximum speed

1380
01:32:48,640 --> 01:32:54,160
secondly for scalability in terms of data sets size

1381
01:32:54,210 --> 01:32:58,100
but the first thing we did try this is

1382
01:32:58,110 --> 01:33:02,230
pretty early on july two thousand nine we just started in two thousand seven we

1383
01:33:02,400 --> 01:33:07,030
first started playing with the do and GBDT and

1384
01:33:07,050 --> 01:33:08,550
the will we

1385
01:33:08,560 --> 01:33:13,280
figure was how can we get a frame this problem into something that's exactly like

1386
01:33:13,740 --> 01:33:19,140
the canonical map reduce type of problem and it seemed natural that you want to

1387
01:33:19,140 --> 01:33:22,690
partition data set amongst this samples so

1388
01:33:22,700 --> 01:33:26,580
a set of machines get the set of samples taken this

1389
01:33:26,590 --> 01:33:30,220
the set and you can easily see how that would scale with the number of

1390
01:33:31,140 --> 01:33:37,270
so the the implementation we do that in the in the map or what we

1391
01:33:37,270 --> 01:33:39,960
had was that for each

1392
01:33:39,970 --> 01:33:45,560
for each map we would look through all the features their values each

1393
01:33:45,570 --> 01:33:50,460
and we in that a feature value as the key

1394
01:33:50,470 --> 01:33:56,070
and the residual and the weight of the sample as the as the value

1395
01:33:56,080 --> 01:33:58,800
and the reduce to aggregate this so

1396
01:33:58,810 --> 01:34:00,450
for every single

1397
01:34:00,470 --> 01:34:06,690
a unique feature value you would have aggregate of usage losing weight so in our

1398
01:34:06,710 --> 01:34:09,510
in our loss function of least squares loss

1399
01:34:09,560 --> 01:34:15,000
i mean for the squares improving this causes more metric for gain in this case

1400
01:34:15,430 --> 01:34:16,630
we were able to

1401
01:34:16,640 --> 01:34:20,670
just given the sort of lists of the feature value residual

1402
01:34:20,680 --> 01:34:26,430
and the weights to compute with the optimal gain was in a single pass

1403
01:34:27,380 --> 01:34:32,410
what we do with this is we were able to determine the gain in one

1404
01:34:33,240 --> 01:34:36,760
then given this you would partition the data

1405
01:34:36,780 --> 01:34:41,370
and there was another mapreduce jobs so given this key and the data again you

1406
01:34:41,580 --> 01:34:47,630
over data look for the future value for the sample and in this case you

1407
01:34:47,630 --> 01:34:53,450
output to DFS is well if file for left side far from right side

1408
01:34:53,470 --> 01:34:58,700
and you will repeat this process for the next tree or the next node

1409
01:35:00,250 --> 01:35:05,120
you can see is already multiple mapreduce jobs and there is actually another message

1410
01:35:05,170 --> 01:35:07,310
mapreduce job for computing the

1411
01:35:07,320 --> 01:35:11,390
the gradient so it's three mapreduce jobs in total

1412
01:35:12,980 --> 01:35:18,100
first look like this works pretty well so it scales as you add more machines

1413
01:35:20,080 --> 01:35:22,210
but if you look closely

1414
01:35:22,230 --> 01:35:24,290
it is actually pretty

1415
01:35:24,300 --> 01:35:27,620
so for a dataset with

1416
01:35:28,110 --> 01:35:33,240
roughly one point two million samples and five hundred features

1417
01:35:33,260 --> 01:35:36,330
it took about five minutes to train a single node

1418
01:35:36,680 --> 01:35:38,040
this was with

1419
01:35:38,050 --> 01:35:40,220
because hundred machines

1420
01:35:40,230 --> 01:35:46,160
so for a sixty three node tree would take a little bit over two hours

1421
01:35:46,190 --> 01:35:53,660
and they are and what we realized was was the problem of HDFS so

1422
01:35:53,700 --> 01:35:57,900
on the graph over here in the lower right

1423
01:35:57,950 --> 01:36:03,090
this was the example just copy the relatively small for a couple of megabytes amongst

1424
01:36:03,090 --> 01:36:08,610
one hundred machines we just copy this file almost finished with like five seconds

1425
01:36:08,620 --> 01:36:13,210
there's is usually one or two stragglers that took about one to two minutes

1426
01:36:13,220 --> 01:36:14,920
and so

1427
01:36:14,940 --> 01:36:20,390
if you have like three minutes per job i mean per node overhead new training

1428
01:36:20,390 --> 01:36:21,720
lee multiple

1429
01:36:21,730 --> 01:36:28,910
no poetry in this a couple of thousand these now this is very prohibitive process

1430
01:36:28,920 --> 01:36:30,950
so next we looked at how can we

1431
01:36:32,110 --> 01:36:38,880
communication cost so if writing HDFS was the problem this try to minimize so

1432
01:36:38,910 --> 01:36:46,290
another mapreduce implementation we tried was partitioning this time by partitioning the data amongst the

1433
01:36:46,290 --> 01:36:47,480
features so that

1434
01:36:47,500 --> 01:36:52,460
one machine gets set of features and is another that then you can see that

1435
01:36:52,470 --> 01:36:54,750
you can just use single pass through

1436
01:36:54,750 --> 01:36:59,890
characterizes the colours inside colours outside and then iterating on all of those hidden variables

1437
01:37:02,480 --> 01:37:06,030
i have you know you've learned a lot of things this week you might feel

1438
01:37:06,030 --> 01:37:10,180
something you've learned that you could do better than this is important problem and you

1439
01:37:10,280 --> 01:37:11,410
that would be good if you

1440
01:37:11,420 --> 01:37:16,370
learn something that was only are better ways of doing it

1441
01:37:16,380 --> 01:37:18,520
OK and then

1442
01:37:18,520 --> 01:37:21,520
back to that a test database again

1443
01:37:21,520 --> 01:37:23,480
and evaluating over the whole

1444
01:37:23,490 --> 01:37:28,810
database we can see what's the effect of this looser kind of interaction you simply

1445
01:37:29,140 --> 01:37:33,730
drag directed around so much easier than the greasy pan and what you see is

1446
01:37:33,740 --> 01:37:39,200
the error rate goes up a bit from one point four two two point one

1447
01:37:39,260 --> 01:37:45,320
percent of error but still small in these these are all still rather small areas

1448
01:37:47,380 --> 01:37:50,310
it's a very effective way of interacting in you saw

1449
01:37:50,370 --> 01:37:53,270
you so much for

1450
01:37:53,270 --> 01:37:56,930
how often does it work well you the kind of examples that are easy for

1451
01:37:56,930 --> 01:38:01,420
this for graph cut by the way an alternative that sort of intermediate difficulty is

1452
01:38:01,420 --> 01:38:03,280
you can draw

1453
01:38:03,340 --> 01:38:06,680
a curve but there's no longer has to straddle the boundaries so can always could

1454
01:38:06,700 --> 01:38:08,860
very quickly that's a lot easier

1455
01:38:08,900 --> 01:38:12,730
manipulating the greasy pencil but not quite as much work but not as easy i

1456
01:38:12,730 --> 01:38:17,020
mean is dragging rectangle and anyway you see in these

1457
01:38:17,090 --> 01:38:22,290
and these problems all you have to do is these these preliminary interactions and they

1458
01:38:22,290 --> 01:38:24,400
get very beautiful

1459
01:38:24,450 --> 01:38:28,860
answers but then hey the colours are well contrasted these examples here are some other

1460
01:38:28,860 --> 01:38:34,520
examples is the camouflage kind of example i will come out in greek sometimes happens

1461
01:38:34,530 --> 01:38:41,340
hands up the first one given a translation of this

1462
01:38:41,390 --> 01:38:43,490
so you drag a rectangle around this

1463
01:38:43,500 --> 01:38:47,620
this fish and despite the fact that its camouflage come out pretty well i think

1464
01:38:47,620 --> 01:38:51,400
there's a bit of coral here stuck to the fish and

1465
01:38:51,480 --> 01:38:56,120
so you'd like to go in and provide more information to correct that

1466
01:38:58,280 --> 01:39:02,590
you know here's something where it's really not going to work well because the prior

1467
01:39:02,600 --> 01:39:07,260
that prefers short separating boundaries is really not going to work for this

1468
01:39:07,270 --> 01:39:09,120
it's almost fractals

1469
01:39:09,140 --> 01:39:12,390
kind of boundary effects be hard to say with boundary is in fact the notion

1470
01:39:12,390 --> 01:39:16,700
of the boundary is not really very helpful with this segmentation problem so you know

1471
01:39:16,700 --> 01:39:20,490
we can probably expect not going and integrated this this is much more like that

1472
01:39:20,490 --> 01:39:24,520
the problem of segmenting the teddy bear where really you'd like to think of many

1473
01:39:24,520 --> 01:39:30,240
of the pixels as having fractional foreground membership and not the same as the foreground

1474
01:39:30,240 --> 01:39:35,650
probability by the way this is an actual mixing fraction between foreground and background

1475
01:39:36,100 --> 01:39:40,590
the actual linear mixture of light

1476
01:39:40,600 --> 01:39:42,090
and then finally

1477
01:39:42,100 --> 01:39:46,160
you know his just very complex scenes and

1478
01:39:46,540 --> 01:39:50,860
you know no telepathy i've said meaning that how on earth did you expect computers

1479
01:39:50,870 --> 01:39:53,700
no which will get you thinking of maybe one perhaps maybe one of the tracks

1480
01:39:54,140 --> 01:39:57,790
maybe you you could have wanted to many different things and so it's is not

1481
01:39:57,790 --> 01:39:58,670
going to do

1482
01:39:58,770 --> 01:40:02,610
completely magic four you but i don't know if i have it here

1483
01:40:02,860 --> 01:40:06,590
it's all the same problem now the tighter curves and it all works much better

1484
01:40:06,590 --> 01:40:08,520
because you provided a bit more information

1485
01:40:08,520 --> 01:40:12,070
another way of providing information which i guess i haven't got in the slide here

1486
01:40:12,070 --> 01:40:15,810
and i actually didn't get around demo yesterday although i could have done is that

1487
01:40:15,810 --> 01:40:18,110
you can also have some brushes so

1488
01:40:18,190 --> 01:40:20,050
foreground background brushes so you

1489
01:40:20,530 --> 01:40:24,050
do an initial segmentation is almost perfect and then you go into little brushes and

1490
01:40:24,360 --> 01:40:28,090
i want that it as well but you don't have to do with any precision

1491
01:40:28,160 --> 01:40:29,490
you just sort of

1492
01:40:29,500 --> 01:40:33,840
make a mark approximately on that you want really solve the

1493
01:40:33,850 --> 01:40:37,320
maximisation of this posterior probability

1494
01:40:37,350 --> 01:40:40,890
it's all can be done very fast

1495
01:40:40,930 --> 01:40:43,720
the whole thing is very fast anyway you saw that the fact that you've already

1496
01:40:43,720 --> 01:40:46,780
done it once and now you just solving a modification of the problem rather than

1497
01:40:46,780 --> 01:40:51,430
the whole problem benicio in fact the way of even capitalizing on that to make

1498
01:40:51,430 --> 01:40:56,560
the solution fast and i may get to that if i have time

1499
01:40:56,590 --> 01:40:59,700
OK so how are we going to solve these problems that's what i want to

1500
01:40:59,700 --> 01:41:04,520
talk about now the rest of the time

1501
01:41:04,530 --> 01:41:07,610
so generally minimizing

1502
01:41:07,650 --> 01:41:10,010
energy is of this form

1503
01:41:11,600 --> 01:41:18,000
on on the graph is NP hard although there are some interesting special cases but

1504
01:41:18,260 --> 01:41:21,850
you know if you if you were to accept well doesn't seem likely to be

1505
01:41:21,850 --> 01:41:23,570
how you can find it useful

1506
01:41:23,580 --> 01:41:28,260
exact algorithm then you know you start looking at approximate algorithms and has a long

1507
01:41:28,260 --> 01:41:34,730
history of approximate algorithms including MCMC like algorithms the first two which i guess you've

1508
01:41:34,730 --> 01:41:38,730
been talking about earlier in the week and some

1509
01:41:38,750 --> 01:41:44,140
relatively naive algorithms like well let's ignore the fact that this energy function this is

1510
01:41:44,140 --> 01:41:49,010
a complex non convex function and just optimized anyway by local

1511
01:41:49,040 --> 01:41:50,480
local gradient descent

1512
01:41:52,560 --> 01:41:54,010
you know we get away with that

1513
01:41:54,040 --> 01:42:01,450
and there are various other solutions one from under the child involves kind of taking

1514
01:42:01,450 --> 01:42:07,360
the approximating the problem with one that is in defined in the continuous domain that

1515
01:42:07,410 --> 01:42:13,250
continuous space and continuous values so it's like a variational problem rather than a combinatorial

1516
01:42:13,250 --> 01:42:16,990
problem then it turns out there is a need approximation to the variational problem just

1517
01:42:16,990 --> 01:42:21,380
involves solving a differential equation so you then you come back to the discrete domain

1518
01:42:21,380 --> 01:42:27,190
and solve the differential equation is finite elements of finite difference what have you

1519
01:42:27,210 --> 01:42:32,290
and then there's a graph cuts which is the the kind of thing i'm going

1520
01:42:32,290 --> 01:42:36,030
to talk about which actually was it was known about that you could solve certain

1521
01:42:36,030 --> 01:42:42,030
kinds of markov random field optimisations exactly with graph cuts but amazing you know the

1522
01:42:42,200 --> 01:42:45,050
right there is a subclass of problems here which can be solved exactly even though

1523
01:42:45,050 --> 01:42:48,330
they look so so complex and however

1524
01:42:48,450 --> 01:42:52,530
when that was done back in eighty nine it was so slow

1525
01:42:52,540 --> 01:42:56,260
that it was used only as a sort of laboratory benchmark for the exact algorithm

1526
01:42:56,280 --> 01:42:59,280
so that you know was a great insight that nobody thought of it as being

1527
01:42:59,280 --> 01:43:03,950
an algorithm actually using some software that we compute in reasonable time

1528
01:43:06,070 --> 01:43:10,350
then there is no you talked about belief propagation and if you have to talk

1529
01:43:10,350 --> 01:43:12,800
about belief propagation

1530
01:43:12,820 --> 01:43:15,460
so there is the belief propagation and that that's also

1531
01:43:15,910 --> 01:43:17,800
a candidate for solving this

1532
01:43:17,840 --> 01:43:23,230
this kind of problems so that is doing belief propagation as if the if these

1533
01:43:23,230 --> 01:43:24,910
functions are defined over tree

1534
01:43:24,930 --> 01:43:28,060
but you and i know it's not defined over tree and so you can have

1535
01:43:28,410 --> 01:43:31,910
cavalierly ignore the fact that it's not over retrieval do what you would have done

1536
01:43:31,910 --> 01:43:35,700
anyway and see what happens but it's clearly not going to be an exact algorithm

1537
01:43:35,750 --> 01:43:39,330
what does happen and then finally there's the modern

1538
01:43:39,340 --> 01:43:44,290
former graph cut which is what i'm going to talk about mostly which is just

1539
01:43:44,290 --> 01:43:48,130
like before except the algorithms have got so much better there really was sort of

1540
01:43:48,130 --> 01:43:55,310
revolution in computer science in combinatorics on how to solve these graph cut problems efficiently

1541
01:43:57,480 --> 01:44:04,010
computer vision discipline kind of exploited that and adapted it to work particularly well with

1542
01:44:04,010 --> 01:44:07,010
it is

1543
01:44:21,560 --> 01:44:39,950
the first thing

1544
01:44:59,580 --> 01:45:01,580
well my

1545
01:45:11,640 --> 01:45:16,370
we're all in of

1546
01:45:32,740 --> 01:45:36,370
all of a

1547
01:45:36,450 --> 01:45:40,470
the old

1548
01:46:25,970 --> 01:46:31,870
from north

1549
01:46:46,350 --> 01:46:47,810
there are

1550
01:46:53,560 --> 01:46:59,830
well from

1551
01:47:01,930 --> 01:47:04,830
one of

1552
01:48:30,890 --> 01:48:37,580
what this is

1553
01:49:09,100 --> 01:49:20,870
one where

1554
01:49:20,870 --> 01:49:25,810
expand this to bigger number of these

1555
01:49:25,870 --> 01:49:31,040
so this is a good reason because every moment you need to keep in memory

1556
01:49:31,040 --> 01:49:33,060
only one number

1557
01:49:33,070 --> 01:49:37,420
you need to keep only this figure number that you get from enemies and you

1558
01:49:37,420 --> 01:49:40,820
also need to keep pointing to have released

1559
01:49:40,850 --> 01:49:42,550
you can read this

1560
01:49:42,600 --> 01:49:43,980
by far

1561
01:49:44,020 --> 01:49:49,190
because we scanning them from the beginning to the end during this rigorous

1562
01:49:49,210 --> 01:49:52,510
this scanning them in one direction

1563
01:49:52,560 --> 01:49:59,320
and recently come numbers and getting results

1564
01:49:59,600 --> 01:50:02,170
it's usually pretty fast

1565
01:50:02,870 --> 01:50:09,010
the problem is that if you are trying to estimate the complexity of this

1566
01:50:10,010 --> 01:50:14,700
the complexity of this algorithm is simply multiplication of numbers

1567
01:50:14,730 --> 01:50:17,310
of course things in all these lists

1568
01:50:18,390 --> 01:50:22,560
you can understand that usually this is a huge number

1569
01:50:22,570 --> 01:50:27,320
another advantage this error is that if you just keeping in one of these

1570
01:50:27,330 --> 01:50:29,290
these keeping a lot of usually

1571
01:50:29,330 --> 01:50:33,280
because is stuff this is usually this bridge far

1572
01:50:33,320 --> 01:50:39,060
therefore like college is also called so we're looking in one these usually as keeping

1573
01:50:39,060 --> 01:50:41,920
a lot of numbers not a lot of species between

1574
01:50:41,930 --> 01:50:43,720
between the plates

1575
01:50:43,730 --> 01:50:45,140
but still

1576
01:50:45,150 --> 01:50:49,850
it's not usually so fast so this approach you can look into

1577
01:50:49,860 --> 01:50:54,210
many open source search engines and you can see this rigorous

1578
01:50:54,270 --> 01:50:56,410
how we can improve

1579
01:50:56,450 --> 01:50:57,340
the first

1580
01:51:00,560 --> 01:51:03,760
counts two of mind that

1581
01:51:03,780 --> 01:51:05,880
sometimes we can do is keep

1582
01:51:05,900 --> 01:51:08,690
if they have some pointers for

1583
01:51:08,730 --> 01:51:13,450
we can jump over some numbers if you don't know if you are not interested

1584
01:51:13,450 --> 01:51:14,260
in that

1585
01:51:14,280 --> 01:51:15,150
this not

1586
01:51:15,170 --> 01:51:16,410
this numbers

1587
01:51:16,420 --> 01:51:18,810
and and this is the place

1588
01:51:18,820 --> 01:51:20,310
the idea of people this

1589
01:51:20,340 --> 01:51:22,200
pretty simple

1590
01:51:22,260 --> 01:51:24,060
what we're going to do

1591
01:51:24,140 --> 01:51:28,880
we're going to know decompress parts of index

1592
01:51:28,900 --> 01:51:30,290
so it in

1593
01:51:30,330 --> 01:51:34,560
the same example imagine that going five

1594
01:51:34,570 --> 01:51:36,270
we can

1595
01:51:37,280 --> 01:51:39,780
on the second we see that

1596
01:51:39,810 --> 01:51:41,960
there's some point four

1597
01:51:41,980 --> 01:51:47,690
that's how the channel or one two three four to say

1598
01:51:47,710 --> 01:51:50,570
somewhere in this area

1599
01:51:51,350 --> 01:51:53,710
so what is possibly is

1600
01:51:53,720 --> 01:51:56,690
we skip skip list means that

1601
01:51:56,700 --> 01:51:59,310
we are in searching for something

1602
01:51:59,320 --> 01:52:02,200
o point in all

1603
01:52:02,260 --> 01:52:05,710
and using this point as we can judge or

1604
01:52:05,730 --> 01:52:09,100
parts of this list

1605
01:52:09,160 --> 01:52:10,230
what we get

1606
01:52:10,310 --> 01:52:11,810
this structure

1607
01:52:11,820 --> 01:52:17,190
we get better speed and you can do theoretical estimation

1608
01:52:17,440 --> 01:52:18,810
how many

1609
01:52:18,810 --> 01:52:20,720
o point as you need to to

1610
01:52:20,730 --> 01:52:24,330
to keep the speed and size of this game

1611
01:52:25,610 --> 01:52:30,730
you can compress still compress these these pretty well

1612
01:52:30,760 --> 01:52:35,640
not not maybe maybe only a small percent difference in percentage

1613
01:52:36,300 --> 01:52:38,800
compression ratio for this list

1614
01:52:38,810 --> 01:52:42,380
so you see this list of the top imagine that this is a piece of

1615
01:52:42,380 --> 01:52:44,840
these documents

1616
01:52:44,850 --> 01:52:47,190
at the beginning we have some

1617
01:52:48,230 --> 01:52:54,230
this situation you have number one hundred sixty five it's not not just now but

1618
01:52:54,300 --> 01:52:58,260
i will show you what the difference and we have a point

1619
01:52:58,280 --> 01:53:00,980
some way forward in this league

1620
01:53:00,990 --> 01:53:06,150
one point is that even if we have a fixed number of

1621
01:53:06,310 --> 01:53:09,330
keep so we always keep fixed number of

1622
01:53:11,890 --> 01:53:14,800
there is some of these is

1623
01:53:14,930 --> 01:53:17,190
it that we are using compression

1624
01:53:17,210 --> 01:53:20,040
and that means that every number

1625
01:53:20,060 --> 01:53:23,740
can have different number of white feet

1626
01:53:23,770 --> 01:53:28,320
therefore we cannot predict in advance how long you been this part of course list

1627
01:53:28,320 --> 01:53:30,760
and we need to point we need to

1628
01:53:30,780 --> 01:53:34,230
in court somewhere how many of you the

1629
01:53:34,260 --> 01:53:35,190
should ski

1630
01:53:35,200 --> 01:53:37,360
to jumper here

1631
01:53:38,660 --> 01:53:42,600
when the scanning through lists

1632
01:53:42,670 --> 01:53:44,070
risk skip b

1633
01:53:44,080 --> 01:53:47,690
fourteen to some memory to some temporary

1634
01:53:47,700 --> 01:53:52,450
this one hundred seventy five and point to this place where this one hundred thirty

1635
01:53:52,450 --> 01:53:54,270
five years

1636
01:53:54,300 --> 01:53:59,560
then we are taking numbers from this list five six eight nine seventy eight

1637
01:53:59,570 --> 01:54:02,920
and then we are coming to offset in the least

1638
01:54:02,930 --> 01:54:05,280
equal to said that wasn't

1639
01:54:05,290 --> 01:54:07,960
one thirty five

1640
01:54:08,010 --> 01:54:10,220
and in this case what we're doing

1641
01:54:10,230 --> 01:54:13,800
we are not put so of course is that the coordinates the

1642
01:54:14,850 --> 01:54:17,200
in this case we don't need to port

1643
01:54:17,210 --> 01:54:20,920
so we know that we need to put this previous number we put in one

1644
01:54:20,920 --> 01:54:22,180
hundred seventy five

1645
01:54:23,740 --> 01:54:28,530
and we are put into our temporary sixty six and next point

1646
01:54:28,550 --> 01:54:30,480
and and then accounting

1647
01:54:30,510 --> 01:54:36,390
four three nine thirty eight previous now and then we

1648
01:54:36,460 --> 01:54:37,560
come to that

1649
01:54:37,560 --> 01:54:42,210
next place where we have point to really got number from this place and so

1650
01:54:45,240 --> 01:54:51,260
it's comprehensive because except two first numbers of numbers are actually there

1651
01:54:51,270 --> 01:54:56,190
it's results step from previous and next note

1652
01:54:56,240 --> 01:54:58,090
and here we go

1653
01:54:58,110 --> 01:54:59,740
you can imagine it's like

1654
01:54:59,770 --> 01:55:02,040
two release of alpha cuts

1655
01:55:02,060 --> 01:55:09,380
one the are all numbers that are included another of pointers keep the numbers of

1656
01:55:09,380 --> 01:55:12,790
point of this skip list

