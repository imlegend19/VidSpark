1
00:00:00,000 --> 00:00:03,760
in fact if you look at his three students gets better than the teachers

2
00:00:03,800 --> 00:00:06,520
so in the theory something is completely wrong

3
00:00:06,530 --> 00:00:12,570
and it's completely aware of that and very annoyed by this fact

4
00:00:12,590 --> 00:00:14,650
so inductive inference

5
00:00:14,700 --> 00:00:17,430
and you start with the pattern recognition problem

6
00:00:17,480 --> 00:00:22,900
that draws the fruit fly the simplest problem that contains the learning

7
00:00:24,170 --> 00:00:30,100
four were just slows the point

8
00:00:30,110 --> 00:00:34,560
so you are facts observations about this but then exit

9
00:00:34,570 --> 00:00:38,920
and class is the one that became minus one plus one just two clusters

10
00:00:38,940 --> 00:00:41,550
it's really the simplest problem you can think

11
00:00:41,670 --> 00:00:45,890
and you want to find the classification will find y as a function of x

12
00:00:45,910 --> 00:00:48,430
which is not always possible

13
00:00:48,440 --> 00:00:53,260
but you want to find the function that works best

14
00:00:56,820 --> 00:01:02,290
although the first part of letting this work was concerned about the complexity problem

15
00:01:02,350 --> 00:01:05,120
and it has been known for long time

16
00:01:05,140 --> 00:01:06,660
that the complexity

17
00:01:07,520 --> 00:01:09,640
the solution to find

18
00:01:09,650 --> 00:01:12,630
determines how

19
00:01:12,650 --> 00:01:15,050
credibility are much interest

20
00:01:15,090 --> 00:01:19,070
and the something like in the twentieth century there was all kinds result

21
00:01:19,220 --> 00:01:22,760
entities are not be multiplied beyond necessity that support

22
00:01:22,780 --> 00:01:25,800
it was written that way in latin

23
00:01:25,810 --> 00:01:28,450
and entity means the thing

24
00:01:28,460 --> 00:01:30,060
and then you see things

25
00:01:30,080 --> 00:01:32,650
is a viable

26
00:01:32,670 --> 00:01:36,110
and it's going to show this is not exactly the right formulation

27
00:01:36,150 --> 00:01:39,020
and beyond necessity means that

28
00:01:40,210 --> 00:01:44,570
i don't need to multiply these entities beyond what we need to explain

29
00:01:44,620 --> 00:01:48,180
the observed facts

30
00:01:48,230 --> 00:01:52,000
so that was the complexity concept in the twelfth century

31
00:01:52,730 --> 00:01:55,160
if you apply the to physical sciences

32
00:01:55,170 --> 00:01:58,520
and as i told before learning is very interested in the way

33
00:01:58,530 --> 00:02:00,540
i fields theory

34
00:02:02,110 --> 00:02:06,180
physics is very example of successful science in fact a lot of the world in

35
00:02:06,180 --> 00:02:07,650
which we live as been

36
00:02:07,700 --> 00:02:12,140
produced by our understanding of the physical world

37
00:02:12,150 --> 00:02:16,890
and so we take some quotes the first quote is a quote from understand i

38
00:02:16,890 --> 00:02:20,350
stand said when the solution is simple god is answering

39
00:02:20,420 --> 00:02:24,380
but you also say to to explain this when the number of fractals coming into

40
00:02:26,110 --> 00:02:27,770
is too large

41
00:02:27,780 --> 00:02:29,610
thank you for

42
00:02:29,660 --> 00:02:31,850
the same thing has been said by land are

43
00:02:31,860 --> 00:02:35,130
it was the most version is this

44
00:02:35,140 --> 00:02:38,720
with four free parameters one can draw an elephant

45
00:02:38,760 --> 00:02:39,900
we five

46
00:02:39,950 --> 00:02:42,660
one can do on the front of the thing is there

47
00:02:42,710 --> 00:02:46,850
and that means that if you multiply the parameters in physics well the

48
00:02:46,860 --> 00:02:50,260
the prosody of physics of the physical sciences

49
00:02:50,310 --> 00:02:53,120
do not work so well

50
00:02:54,520 --> 00:02:57,800
but the commission deals with the complex well because we we know how to do

51
00:02:57,810 --> 00:03:00,960
but and commissioned with a lot more common and

52
00:03:00,980 --> 00:03:05,010
well in for the tall second show examples where we have used the parameters and

53
00:03:05,010 --> 00:03:07,330
we can do it

54
00:03:07,380 --> 00:03:12,890
so there is some discrepancy there between the way we build our understanding of physics

55
00:03:12,900 --> 00:03:14,450
and what we are able to do

56
00:03:14,460 --> 00:03:16,570
in pattern recognition

57
00:03:16,620 --> 00:03:20,170
and why is it so

58
00:03:20,210 --> 00:03:25,210
so then they about nature and the series the back from the sixties

59
00:03:25,320 --> 00:03:27,480
in the sixties and the there was no

60
00:03:27,570 --> 00:03:31,930
showing or show there was no real computers where you could actually experiment so when

61
00:03:32,180 --> 00:03:33,990
you start thinking about learning

62
00:03:34,040 --> 00:03:37,040
while computers were distant possibility

63
00:03:37,090 --> 00:03:41,170
i was really interested in the fundamental phenomenon of learning

64
00:03:41,220 --> 00:03:42,620
in machines may be

65
00:03:42,630 --> 00:03:45,660
in humans for

66
00:03:45,670 --> 00:03:50,230
so for any algorithm that selects one function from the set of functions which was

67
00:03:50,230 --> 00:03:51,290
up really

68
00:03:51,340 --> 00:03:53,710
there are two and only two fractals

69
00:03:53,720 --> 00:03:55,960
pick standardisation

70
00:03:56,040 --> 00:04:00,060
the first one is empirical laws that is the number of training are also made

71
00:04:00,060 --> 00:04:03,390
by the functions number also training set

72
00:04:03,400 --> 00:04:07,950
and the second one is the complexity measure now several people with the VC dimension

73
00:04:08,210 --> 00:04:10,810
VC entropy is much better than

74
00:04:10,900 --> 00:04:13,990
and if the complexity of the set of functions

75
00:04:14,060 --> 00:04:16,390
i mean is not the complexity of the solution

76
00:04:16,400 --> 00:04:20,540
it's not because you find the solution and you look at the solution states complicated

77
00:04:21,290 --> 00:04:23,720
like was just a all kinds result

78
00:04:23,740 --> 00:04:26,970
is the complexity of the set of functions from which you search

79
00:04:26,980 --> 00:04:33,670
the right answer

80
00:04:34,400 --> 00:04:36,550
i'm going to go over the

81
00:04:36,570 --> 00:04:41,130
capacity the notion of capacity of learning defined it in the sixties

82
00:04:41,140 --> 00:04:44,090
so you have these patterns x and you have set

83
00:04:44,100 --> 00:04:47,070
of indicator function the function that next

84
00:04:47,120 --> 00:04:51,110
as going to say same minus one plus one depending on which class you believe

85
00:04:51,260 --> 00:04:53,380
x belongs to

86
00:04:53,480 --> 00:04:57,010
and x one xn is supposed to be in the

87
00:04:57,020 --> 00:05:04,400
identically distributed samples of independent examples from the certain distribution that you don't know about

88
00:05:04,490 --> 00:05:08,020
so there is and it's very important that the cause and

89
00:05:08,030 --> 00:05:11,250
which is the number of separations can do about this

90
00:05:11,260 --> 00:05:12,680
but you at them

91
00:05:12,730 --> 00:05:16,260
with function from that city

92
00:05:16,280 --> 00:05:19,250
and this number is of course more than two to the l which is the

93
00:05:19,290 --> 00:05:21,770
total number of separation could think of

94
00:05:21,790 --> 00:05:26,330
basically there are some partitions of these examples but you cannot represent with the function

95
00:05:26,330 --> 00:05:27,230
from that

96
00:05:27,240 --> 00:05:30,960
so it's going to be less

97
00:05:30,970 --> 00:05:32,510
and there is the quantity

98
00:05:32,520 --> 00:05:34,190
it's called VC entropy

99
00:05:34,210 --> 00:05:38,070
which is the logo even of the expectation of that number

100
00:05:39,300 --> 00:05:43,450
the possible drawing office and

101
00:05:43,600 --> 00:05:46,310
so that's the important quantity

102
00:05:46,320 --> 00:05:49,490
there is the quantity that because the growth function

103
00:05:49,500 --> 00:05:52,280
well you're plays the expectation by maximum

104
00:05:52,290 --> 00:05:57,510
so this is an upper bound of

105
00:05:57,520 --> 00:06:00,170
and i think the breakthrough was this

106
00:06:00,170 --> 00:06:03,520
automatic relevance determination anybody one

107
00:06:03,540 --> 00:06:05,750
when you have

108
00:06:05,770 --> 00:06:06,400
all right

109
00:06:08,270 --> 00:06:11,150
maybe i'll do this and then you know

110
00:06:11,190 --> 00:06:12,310
we can take it

111
00:06:12,370 --> 00:06:15,890
break in the die-hard people consider around kind more

112
00:06:19,150 --> 00:06:21,350
in feature selection

113
00:06:21,400 --> 00:06:26,670
so the standard method for doing feature selection in a bayesian setting is called automatic

114
00:06:26,670 --> 00:06:31,560
relevance determination and the reason is nice is that it avoids this

115
00:06:31,600 --> 00:06:35,580
combinatorial search over to the possible subsets

116
00:06:35,600 --> 00:06:41,120
of features instead of that it attaches continuous

117
00:06:41,120 --> 00:06:43,130
number to each dimension

118
00:06:43,150 --> 00:06:46,870
which is the relevance of the dimension and then tries to

119
00:06:46,920 --> 00:06:53,100
infer optimise that continuous number and is usually much easier to optimize one continues d

120
00:06:53,100 --> 00:06:54,420
dimensional vector

121
00:06:54,460 --> 00:06:55,230
ten two

122
00:06:55,230 --> 00:07:01,790
search the corners of a d dimensional hypercube because there two the the corners

123
00:07:04,900 --> 00:07:08,370
this was developed in the context of bayesian neural networks

124
00:07:09,170 --> 00:07:12,690
you have it doesn't matter can apply to lots of other things he has some

125
00:07:13,790 --> 00:07:16,830
inputs and outputs as they were doing regression

126
00:07:16,850 --> 00:07:19,710
because what we could do classification or anything else we want

127
00:07:19,770 --> 00:07:24,020
parameters theta which are the weights and your neural network weights

128
00:07:24,080 --> 00:07:29,310
from inputs to hidden variables and from the very beginning stu outputs

129
00:07:29,350 --> 00:07:33,750
then we define a prior on your parameters posterior under parameters

130
00:07:33,790 --> 00:07:34,850
is a

131
00:07:34,850 --> 00:07:39,090
o point guard from bayes rule all these things come from the basic statistics of

132
00:07:39,270 --> 00:07:42,350
what you define the prior then everything else

133
00:07:42,440 --> 00:07:45,080
i don't need to go into the details

134
00:07:45,120 --> 00:07:48,830
the important thing is the prior the the prior

135
00:07:49,810 --> 00:07:51,690
has a vector

136
00:07:52,750 --> 00:07:54,230
of hyperparameters

137
00:07:54,230 --> 00:07:56,520
and there is an alpha

138
00:07:56,520 --> 00:08:01,020
element for each input dimension OK so here we have three input dimensions and one

139
00:08:01,020 --> 00:08:05,190
output dimensions so would alpha one alpha two alpha three

140
00:08:05,210 --> 00:08:06,370
and what these

141
00:08:06,400 --> 00:08:08,210
hyperparameters do

142
00:08:11,330 --> 00:08:15,750
they essentially determines the relevance of this input

143
00:08:15,750 --> 00:08:18,190
in the whole prediction problem

144
00:08:18,210 --> 00:08:20,150
let's try to understand how

145
00:08:20,150 --> 00:08:22,770
so actually what they do is

146
00:08:22,850 --> 00:08:26,670
they control the the variance of the weights

147
00:08:26,920 --> 00:08:30,330
coming out of each one of these input

148
00:08:30,370 --> 00:08:32,020
and it's OK so

149
00:08:32,040 --> 00:08:33,690
that means

150
00:08:34,730 --> 00:08:37,290
input unit xd

151
00:08:37,400 --> 00:08:39,750
there are some weights coming out of this

152
00:08:39,830 --> 00:08:44,920
w dj is the weight from units input units the

153
00:08:44,940 --> 00:08:46,560
two hidden unit j

154
00:08:46,650 --> 00:08:48,730
OK so this this way here

155
00:08:49,440 --> 00:08:52,060
y alpha d does

156
00:08:52,080 --> 00:08:56,690
is it controls the variance the prior variance of this way

157
00:08:56,730 --> 00:09:04,650
so the prior assumption is that the distribution of wtj given alpha d is normal

158
00:09:04,650 --> 00:09:09,170
with zero mean and variance one property

159
00:09:11,040 --> 00:09:13,400
let's think about this

160
00:09:13,420 --> 00:09:17,730
when alpha d goes to infinity

161
00:09:19,920 --> 00:09:24,310
variance here goes to zero because one over infinity

162
00:09:24,960 --> 00:09:27,290
the weights coming out of input

163
00:09:28,770 --> 00:09:34,270
all of the weights coming out of the deal have zero mean and zero variance

164
00:09:34,290 --> 00:09:38,190
in other words all the weights coming out of the d going to zero

165
00:09:38,190 --> 00:09:41,480
therefore no matter what input is

166
00:09:41,520 --> 00:09:46,600
it has no effect on the hidden variables and therefore no effect on the output

167
00:09:47,540 --> 00:09:49,620
when alpha d

168
00:09:49,620 --> 00:09:51,130
is infinity

169
00:09:51,750 --> 00:09:54,690
it would be is irrelevant

170
00:09:54,690 --> 00:09:57,870
when alpha d is finite

171
00:09:57,900 --> 00:10:03,100
then the weights coming out any finite meaning not in or close to infinity six

172
00:10:03,100 --> 00:10:05,560
like some reasonable small number

173
00:10:05,560 --> 00:10:07,600
then the weight here

174
00:10:07,620 --> 00:10:09,460
can vary

175
00:10:09,480 --> 00:10:14,500
which means that this input x d can have an impact on the output

176
00:10:17,000 --> 00:10:20,790
alpha d continuously controls the relevant

177
00:10:21,750 --> 00:10:23,630
input xt

178
00:10:23,650 --> 00:10:29,540
so the idea behind automatic relevance determination is simply to infer the relevance alpha

179
00:10:29,560 --> 00:10:32,170
from the data it is you're trying to learn

180
00:10:32,230 --> 00:10:36,620
this alpha vector from the data and using the same idea of marginal likelihood and

181
00:10:36,620 --> 00:10:40,080
so on to do that so often what we can do is we can just

182
00:10:40,080 --> 00:10:41,960
optimise of

183
00:10:41,980 --> 00:10:43,580
as a function

184
00:10:43,620 --> 00:10:45,650
we can optimize

185
00:10:47,560 --> 00:10:51,230
probability of all of the observed the outputs given the inputs

186
00:10:51,270 --> 00:10:53,730
as a function of alpha

187
00:10:53,730 --> 00:10:57,460
and then when we do that optimisation what we find in practice is that a

188
00:10:57,460 --> 00:11:02,010
so should start or ten minutes or

189
00:11:02,020 --> 00:11:08,100
so which just left by this very and saying that he five gonna contraction and

190
00:11:08,100 --> 00:11:12,690
we have seen that those properties some previous slides for pretty handy in compete in

191
00:11:16,190 --> 00:11:17,630
here in

192
00:11:17,630 --> 00:11:20,510
why is the unique fixed point if you buy

193
00:11:20,560 --> 00:11:25,400
it iteration is converging to be of and that matrix is invertible the rate of

194
00:11:25,400 --> 00:11:28,170
convergence is is exponentially

195
00:11:28,220 --> 00:11:28,830
so all

196
00:11:28,830 --> 00:11:32,150
why is this case so this would turn out to be

197
00:11:32,150 --> 00:11:33,260
important for

198
00:11:33,280 --> 00:11:40,550
for some reasons so that they are given that we require that you can have

199
00:11:40,820 --> 00:11:43,210
any policy of interest of you

200
00:11:43,260 --> 00:11:47,930
but other than that this doesn't seem to be directly useful for our purposes right

201
00:11:47,930 --> 00:11:53,320
now so what's next is how do we really compute an optimal

202
00:11:55,250 --> 00:11:58,910
you are to do this so we are just one step of a doing that

203
00:11:59,030 --> 00:12:00,900
so we define this

204
00:12:00,920 --> 00:12:04,930
operator that's called after ban on the battle of canonical operator

205
00:12:04,950 --> 00:12:10,250
so this is pretty much the same thing as the operator that we used before

206
00:12:10,330 --> 00:12:13,040
so the difference is that

207
00:12:13,090 --> 00:12:16,190
there is is no policy here since enough

208
00:12:16,250 --> 00:12:18,590
having a policy iran

209
00:12:18,640 --> 00:12:20,060
what we have

210
00:12:21,530 --> 00:12:24,480
we have the maximum overreactions

211
00:12:24,530 --> 00:12:29,480
so if we didn't have maximum body actions were chosen by some policy this would

212
00:12:29,480 --> 00:12:32,950
be just the police abolition operator here by

213
00:12:32,970 --> 00:12:37,500
so why is this change so remember that we are looking for something some policy

214
00:12:37,510 --> 00:12:39,560
that maximizes the reward

215
00:12:39,640 --> 00:12:40,810
it's sort of

216
00:12:40,840 --> 00:12:43,430
i can understand it makes sense to

217
00:12:46,420 --> 00:12:48,250
specific action choice

218
00:12:49,170 --> 00:12:55,610
choosing the action that gives you is this good our lives factors

219
00:12:55,620 --> 00:12:57,840
and so what happens is

220
00:12:57,930 --> 00:13:00,170
but if you define this policy

221
00:13:00,220 --> 00:13:02,590
OK you need one more definition before

222
00:13:02,620 --> 00:13:04,170
i think the next proof

223
00:13:04,170 --> 00:13:05,630
so we say

224
00:13:05,650 --> 00:13:07,490
that policy pi

225
00:13:07,500 --> 00:13:10,940
his greedy respect some function

226
00:13:11,000 --> 00:13:12,590
the director b

227
00:13:12,590 --> 00:13:16,000
fifty five because you have to be

228
00:13:16,000 --> 00:13:20,800
so what is the meaning of this this this is very very compact notation of

229
00:13:21,520 --> 00:13:24,770
saying something very simple

230
00:13:24,780 --> 00:13:26,960
so that simple saying is that

231
00:13:26,990 --> 00:13:31,720
policy is greatest respect to some value function v

232
00:13:31,770 --> 00:13:34,470
if it works in such a way

233
00:13:34,560 --> 00:13:36,780
that maximizes

234
00:13:36,800 --> 00:13:39,190
the sum of the parts

235
00:13:39,210 --> 00:13:42,570
some of the immediate reward for the first time step

236
00:13:42,590 --> 00:13:47,030
plus it matches that from that step on

237
00:13:47,090 --> 00:13:53,030
the put that expected return is going to be defined by p

238
00:13:53,070 --> 00:13:57,900
so if you imagine this and you still have the the action that just

239
00:13:57,930 --> 00:13:59,930
it maximizes this site

240
00:13:59,990 --> 00:14:03,410
you get that policy that's previous respectively

241
00:14:04,520 --> 00:14:06,650
so let's

242
00:14:06,710 --> 00:14:09,910
the definition of greedy policy

243
00:14:09,930 --> 00:14:13,320
and then it turns out that these are gonna contraction so why do we know

244
00:14:13,320 --> 00:14:15,770
that these are coming attractions

245
00:14:15,780 --> 00:14:17,160
the game so

246
00:14:17,190 --> 00:14:18,430
you start by

247
00:14:18,440 --> 00:14:20,780
by decomposing it or looking at the

248
00:14:20,810 --> 00:14:23,570
component defining this operator

249
00:14:23,590 --> 00:14:27,030
and again you see that you have a matrix

250
00:14:27,050 --> 00:14:30,940
vector multiplication so that you see is some of the advice

251
00:14:30,960 --> 00:14:34,770
the have a vector matrix multiplication

252
00:14:35,560 --> 00:14:41,680
and that's this respect probability transition matrix so that's anon expansion and then you multiply

253
00:14:41,720 --> 00:14:42,970
grandmaster that

254
00:14:43,020 --> 00:14:44,500
makes the contraction

255
00:14:44,520 --> 00:14:46,120
and and then you should have

256
00:14:46,120 --> 00:14:49,210
the whole saying by this immediate reward

257
00:14:49,220 --> 00:14:52,250
and that still makes it

258
00:14:53,620 --> 00:14:57,470
contraction OK because it's expansion shifting something

259
00:14:57,570 --> 00:15:01,250
and then you take the max with respectively and we have seen that

260
00:15:01,270 --> 00:15:06,120
so before taking the max what you had that you started this function that was

261
00:15:06,120 --> 00:15:08,050
defined over the state

262
00:15:08,120 --> 00:15:10,340
and you end up with the functions

263
00:15:10,370 --> 00:15:14,000
what are the free variables if if i don't have

264
00:15:14,000 --> 00:15:15,800
max over a here

265
00:15:15,870 --> 00:15:18,500
so the free variables are

266
00:15:18,510 --> 00:15:20,940
x a set of function

267
00:15:21,000 --> 00:15:25,920
over state action pair so we started with the space of

268
00:15:25,930 --> 00:15:28,180
functions so

269
00:15:28,240 --> 00:15:32,160
states and we ended up in the space of functions

270
00:15:33,010 --> 00:15:34,700
the state action pairs

271
00:15:35,490 --> 00:15:39,800
and then we take the maximum we suspected the actions

272
00:15:39,830 --> 00:15:42,930
this brings us back to the space of functions

273
00:15:43,010 --> 00:15:45,750
it is defined just over the state space

274
00:15:47,790 --> 00:15:49,310
the course of

275
00:15:49,330 --> 00:15:50,730
the struggle is

276
00:15:50,750 --> 00:15:52,290
you start this

277
00:15:54,640 --> 00:15:56,630
now you've got to be f

278
00:15:56,640 --> 00:16:01,810
x time a survivor access time is just the product space

279
00:16:01,870 --> 00:16:06,380
and you go back to the facts and this was the operator

280
00:16:06,630 --> 00:16:11,020
and so in the previous slide two slides before we have seen that that operator

281
00:16:11,020 --> 00:16:15,940
is anon expansion so all together the whole operator is is gonna contraction

282
00:16:19,490 --> 00:16:21,980
because every

283
00:16:37,370 --> 00:16:43,960
so it appears someone side

284
00:16:43,990 --> 00:16:47,710
OK so maybe i should spread out

285
00:16:50,020 --> 00:16:53,370
so that definition states

286
00:16:58,820 --> 00:17:03,000
this this is this so by is the greatest respectively

287
00:17:03,690 --> 00:17:08,960
the following happens if you take

288
00:17:09,050 --> 00:17:12,570
this thing here

289
00:17:12,580 --> 00:17:13,730
at some

290
00:17:13,740 --> 00:17:15,980
actually this is the by

291
00:17:16,050 --> 00:17:17,560
applied to the

292
00:17:17,560 --> 00:17:19,200
and a weighted MAX

293
00:17:20,460 --> 00:17:25,520
so keep by visa backer we can take a look at one of its components

294
00:17:25,540 --> 00:17:30,070
so the components are indexed by state variables so that's big axe

295
00:17:30,810 --> 00:17:32,010
so this is

296
00:17:32,100 --> 00:17:32,790
at the

297
00:17:32,800 --> 00:17:34,180
the left hand side

298
00:17:34,230 --> 00:17:36,550
a that

299
00:17:36,620 --> 00:17:39,080
and the right hand side

300
00:17:39,130 --> 00:17:40,610
is what

301
00:17:40,620 --> 00:17:41,620
you have

302
00:17:41,630 --> 00:17:46,200
almost the same exact almost the same expression

303
00:17:46,250 --> 00:17:49,250
except that he's denied having five

304
00:17:49,250 --> 00:17:50,430
in patient

305
00:17:50,440 --> 00:17:53,310
you have the maximum actions

306
00:17:53,310 --> 00:17:56,440
so what they say is

307
00:17:56,450 --> 00:17:59,540
it is

308
00:17:59,700 --> 00:18:02,790
that the policy

309
00:18:02,790 --> 00:18:06,780
which basically says you just going to do

310
00:18:06,830 --> 00:18:12,410
the pass greedy algorithm right to get the solution when we optimize the

311
00:18:12,420 --> 00:18:14,480
benefit cost ratio

312
00:18:14,490 --> 00:18:19,780
and then we get a separate solution that we just tend not considered cost

313
00:18:19,830 --> 00:18:22,850
and then take the best of the two solutions

314
00:18:22,890 --> 00:18:24,540
then the thing works

315
00:18:24,550 --> 00:18:29,190
so then you can prove that again the year optimal meaning we are we get

316
00:18:29,200 --> 00:18:31,250
a constant factor approximation

317
00:18:33,850 --> 00:18:37,070
and the other thing that i would like to know is

318
00:18:37,080 --> 00:18:42,370
how far from optimal right we know we can go and find optimal solution because

319
00:18:42,370 --> 00:18:47,280
that requires the enumeration of all possible subsets in that sense so

320
00:18:48,200 --> 00:18:49,280
the question is

321
00:18:49,290 --> 00:18:54,580
how far in the traditional by this constant factor about that you can define how

322
00:18:54,580 --> 00:18:58,990
how bad so even before looking at the data before running the algorithm you know

323
00:18:58,990 --> 00:19:03,320
how far from optimal you right it says that was you will be at sixty

324
00:19:03,320 --> 00:19:07,160
three percent so you don't even need to look at the data and you know

325
00:19:07,180 --> 00:19:10,480
how how how far how far you could be

326
00:19:10,500 --> 00:19:12,070
in which case so

327
00:19:12,910 --> 00:19:15,270
what we also have is is tighter bound

328
00:19:15,280 --> 00:19:16,820
which basically tells us

329
00:19:16,850 --> 00:19:22,230
how far from optimal worst-case time and the idea is that since this marginal gains

330
00:19:22,230 --> 00:19:23,360
are decreasing

331
00:19:23,370 --> 00:19:24,210
you can go

332
00:19:24,230 --> 00:19:26,620
and get the better

333
00:19:27,890 --> 00:19:29,060
so i was

334
00:19:30,460 --> 00:19:32,120
and the at finish early

335
00:19:35,510 --> 00:19:39,680
so we have two case studies and i just show you want

336
00:19:40,780 --> 00:19:41,740
one is on

337
00:19:41,750 --> 00:19:46,670
on blog network the same that i was talking about before we have this one

338
00:19:46,890 --> 00:19:51,260
blogs for one year and we asked which blogs should one read to detect information

339
00:19:51,260 --> 00:19:57,250
as quickly as possible and the other one is the water distribution network have a

340
00:19:57,280 --> 00:19:59,510
city water distribution network

341
00:19:59,520 --> 00:20:04,050
and we have a realistic simulator simulates how the water is then caught over time

342
00:20:04,050 --> 00:20:08,540
and we would like to find locations for sensors to detect this things skip this

343
00:20:08,540 --> 00:20:13,520
as effectively as possible and i will show the the water network example so we

344
00:20:13,520 --> 00:20:17,960
have looked network has ten to one thousand nodes or houses and we have twenty

345
00:20:17,960 --> 00:20:20,410
five thousand bikes connecting them

346
00:20:20,490 --> 00:20:23,270
so here's a picture of one of these networks

347
00:20:23,280 --> 00:20:28,140
and then we simulated three point six million epidemic scenarios

348
00:20:28,180 --> 00:20:29,420
o what is that

349
00:20:29,490 --> 00:20:33,960
so for any time of the day for for every house

350
00:20:33,980 --> 00:20:37,690
you you pretend you there was an outbreak started to the polls and then you

351
00:20:38,620 --> 00:20:42,360
how how the how the poisonous water would spread over the net

352
00:20:42,410 --> 00:20:45,230
and that gives you a decent amount of data

353
00:20:45,250 --> 00:20:50,600
and then if you're smart you can by exploiting sparsity we we can get down

354
00:20:50,600 --> 00:20:53,430
to sixteen eggs and fitted into the main memory

355
00:20:53,440 --> 00:20:55,700
and then we can run our optimisation

356
00:20:55,730 --> 00:20:57,530
and on

357
00:20:57,540 --> 00:21:01,730
the first question is how far is our solution from the optimal solution

358
00:21:01,820 --> 00:21:03,860
and the

359
00:21:03,880 --> 00:21:07,910
the previous result would tell you that the optimal solution

360
00:21:07,960 --> 00:21:10,480
it lies somewhere between what we have

361
00:21:10,490 --> 00:21:14,710
and this so we know that in worst-case optimally

362
00:21:14,880 --> 00:21:16,040
is of there

363
00:21:16,060 --> 00:21:21,530
the new bound to realize that looks so that they can looks at the algorithm

364
00:21:21,560 --> 00:21:26,080
is much tighter so this is what is basically tells us is in worst case

365
00:21:26,140 --> 00:21:30,370
this is the margin this is how far from optimal so instead of being fifty

366
00:21:30,370 --> 00:21:35,070
seven percent away from of tomorrow we are ten percent from just gives us a

367
00:21:35,070 --> 00:21:36,300
better estimate of

368
00:21:36,330 --> 00:21:39,700
how far is our solution from unknown optimal solution

369
00:21:39,710 --> 00:21:44,110
then what you can also going to ask is what

370
00:21:44,150 --> 00:21:48,820
what if forget about how viruses spread over the network can do these optimizations but

371
00:21:48,820 --> 00:21:49,780
let's just say

372
00:21:49,780 --> 00:21:54,680
let's just heuristic just the locations so for example what i'm showing here is the

373
00:21:55,530 --> 00:21:59,270
the quality of the solution as more sensors are added to the to the to

374
00:21:59,270 --> 00:22:00,660
the network

375
00:22:00,670 --> 00:22:02,830
more than monitoring stations

376
00:22:02,830 --> 00:22:06,310
and these are the other curves are

377
00:22:07,570 --> 00:22:11,820
how good to be would get if you would point for example the sensors randomly

378
00:22:11,820 --> 00:22:14,800
in the network that the black one or even if we were to put it

379
00:22:14,840 --> 00:22:18,230
within based on the population so that's a moot point

380
00:22:18,250 --> 00:22:21,900
the more people do with the sense of the locations where there are more people

381
00:22:21,900 --> 00:22:25,150
or for example you could say i put it put them on the on the

382
00:22:25,150 --> 00:22:29,480
main pipes the pipes when the flow is highest or whether they are not of

383
00:22:29,480 --> 00:22:31,990
the pipes is high but you can see that

384
00:22:32,230 --> 00:22:37,350
if you really going to know how the virus propagates over the network and optimize

385
00:22:37,410 --> 00:22:41,790
then you can do much much better than using any other heuristic selection

386
00:22:41,810 --> 00:22:43,880
any other criteria that does not

387
00:22:43,930 --> 00:22:46,290
we consider the virus

388
00:22:46,300 --> 00:22:53,050
and what they also want to show is that given different different different objective functions

389
00:22:53,050 --> 00:22:54,430
of different criteria

390
00:22:54,440 --> 00:22:57,620
you get different placements so for example

391
00:22:57,620 --> 00:22:59,390
this is the

392
00:22:59,410 --> 00:23:04,150
the water distribution network the dots side of the sensor placements the decided to be

393
00:23:04,150 --> 00:23:07,870
found in human

394
00:23:07,880 --> 00:23:12,200
trying to optimize population affected so the idea here is to have a placement that

395
00:23:12,200 --> 00:23:13,300
would detect

396
00:23:13,760 --> 00:23:20,130
the outbreak of so that the least number of people with the the poisonous one

397
00:23:20,130 --> 00:23:22,780
and you know it

398
00:23:22,790 --> 00:23:29,550
but also this particular presentation was given by michael

399
00:23:29,700 --> 00:23:43,100
is one

400
00:23:43,260 --> 00:23:54,410
copied with languor chunk of text

401
00:23:54,440 --> 00:23:56,590
but we do is

402
00:23:56,750 --> 00:23:59,480
i mean it

403
00:23:59,530 --> 00:24:02,050
that's because it for

404
00:24:02,100 --> 00:24:03,980
so will tell

405
00:24:04,090 --> 00:24:07,900
this is all these things through this follows

406
00:24:07,900 --> 00:24:10,510
and what has to be true so all this

407
00:24:10,520 --> 00:24:12,090
it has to

408
00:24:12,150 --> 00:24:13,220
if this

409
00:24:13,230 --> 00:24:17,260
this is a presentation i don't is the story

410
00:24:17,270 --> 00:24:19,340
the place is given

411
00:24:19,390 --> 00:24:21,600
this tutorial is actually given

412
00:24:21,650 --> 00:24:24,020
this particular presentation

413
00:24:24,200 --> 00:24:27,640
the tories make

414
00:24:27,750 --> 00:24:29,110
and the tutorial is

415
00:24:29,130 --> 00:24:30,300
subgroup of

416
00:24:30,350 --> 00:24:35,380
this one actually working here

417
00:24:35,400 --> 00:24:37,530
so the this topic

418
00:24:41,460 --> 00:24:43,090
who was done

419
00:24:52,650 --> 00:25:01,940
here presenter

420
00:25:02,050 --> 00:25:04,270
so we need there's a person

421
00:25:04,300 --> 00:25:07,980
and who is presentation so this is what this

422
00:25:08,000 --> 00:25:11,080
it assertions he hated tells so

423
00:25:11,160 --> 00:25:14,840
being given a presentation by person

424
00:25:23,230 --> 00:25:24,800
then this person

425
00:25:24,850 --> 00:25:27,400
experts in this field

426
00:25:31,720 --> 00:25:44,850
and also

427
00:25:44,890 --> 00:25:48,670
four medically applied to all the

428
00:25:48,690 --> 00:25:49,820
it for

429
00:25:49,840 --> 00:25:51,530
materialize this rule

430
00:26:07,470 --> 00:26:09,380
let's see how this works

431
00:26:21,710 --> 00:26:25,900
OK so now we sort of this thing in the knowledge base this

432
00:26:25,910 --> 00:26:29,360
what do we know about michael work

433
00:26:29,410 --> 00:26:33,190
and we see that he's expert in the

434
00:26:33,330 --> 00:26:36,660
this if you know we should have the article was under the some of the

435
00:26:36,660 --> 00:26:38,690
point because of

436
00:26:38,700 --> 00:26:40,950
sort of it

437
00:26:41,100 --> 00:26:53,500
we can try to contract

438
00:26:53,580 --> 00:26:59,440
you assert that

439
00:27:08,520 --> 00:27:12,700
i am a person

440
00:27:12,870 --> 00:27:20,820
but this

441
00:27:20,870 --> 00:27:24,010
but does it no it also

442
00:27:24,100 --> 00:27:25,520
so this this

443
00:27:25,530 --> 00:27:28,260
and think it

444
00:27:28,270 --> 00:27:54,760
you can use to

445
00:28:01,400 --> 00:28:04,040
i think second this should automatically

446
00:28:16,810 --> 00:28:17,790
so it's

447
00:28:18,750 --> 00:28:20,950
four so

448
00:28:21,010 --> 00:28:23,640
in fact my probably person was be

449
00:28:23,650 --> 00:28:25,670
this is because

450
00:28:25,680 --> 00:28:28,800
there must be a person

451
00:28:40,910 --> 00:28:45,400
maybe sort apparently attempting if you do this

452
00:28:45,480 --> 00:28:52,100
here we the markers person and the next

453
00:28:52,220 --> 00:28:56,780
it was checking to see that for this is for medical presenter

454
00:28:56,870 --> 00:28:58,650
the constraints are the

455
00:29:00,310 --> 00:29:04,570
the second argument has to be a partner from this it automatically

456
00:29:30,280 --> 00:29:32,020
OK so this is

457
00:29:32,290 --> 00:29:51,870
your cities

458
00:29:51,870 --> 00:29:56,870
so because full of for those of you who basically

459
00:29:56,910 --> 00:30:00,760
we don't know anything on the calorimeters are we really very briefly

460
00:30:00,780 --> 00:30:05,250
in the first like twenty minutes from now the multicolour principles OK

461
00:30:05,310 --> 00:30:06,990
then we move on

462
00:30:07,010 --> 00:30:12,690
basically by discussing introducing the class of markov chain monte carlo methods on their i

463
00:30:12,690 --> 00:30:18,050
will reduce the such as sampling of mental processing factories on that i will discuss

464
00:30:18,050 --> 00:30:23,010
more sophisticated techniques such as slice sampling hamiltonian MCMC

465
00:30:23,010 --> 00:30:27,560
all part of the empirical so that you take this one on off OK so

466
00:30:27,560 --> 00:30:31,170
we will lose all place anytime

467
00:30:31,180 --> 00:30:36,790
during basically this morning after that after the boy we really start discussing the class

468
00:30:36,820 --> 00:30:41,520
of sequential monte carlo methods which is the so-called essentially a large class of multicolored

469
00:30:41,520 --> 00:30:47,560
methods also known in the literature time as particle filters because so i do

470
00:30:47,570 --> 00:30:52,100
OK first i first are one of MCMC sigma

471
00:30:52,100 --> 00:30:56,030
as smc on that's basically to

472
00:30:56,040 --> 00:31:01,200
once you have understood all those techniques with MCMC on SMC then we will describe

473
00:31:01,260 --> 00:31:08,540
essentially mother calorimeters which is essentially a combination of both MCMC on sentiment on this

474
00:31:08,540 --> 00:31:10,010
will include actually

475
00:31:10,030 --> 00:31:15,060
very recent material that have been published actually last year or even as it we

476
00:31:15,060 --> 00:31:19,310
should be published actually very significant so for those of you are really especially some

477
00:31:19,310 --> 00:31:23,650
of the calorimeters i'm afraid you're not going to let me much things this morning

478
00:31:23,680 --> 00:31:30,030
OK but definitely to more like this you should basically get lonely few new nutrients

479
00:31:30,030 --> 00:31:31,780
in this field so

480
00:31:31,810 --> 00:31:37,060
let's start with a little bit of motivation why am i interesting mccallum elements were

481
00:31:37,060 --> 00:31:43,900
essentially means coming from my interest in bayesian statistics OK so essentially a lot of

482
00:31:43,920 --> 00:31:49,170
prior and likelihood of use the bayes rule only we go you have the posterior

483
00:31:49,170 --> 00:31:53,930
distribution of the data given the observation first type or this very nice for slide

484
00:31:53,930 --> 00:32:00,150
define the type of this is why this is notable style OK this is this

485
00:32:00,150 --> 00:32:05,590
is was the distribution of essentially as soon as you have basically model which are

486
00:32:05,590 --> 00:32:10,190
little bit complex you don't have a closed form expression for the posterior distribution so

487
00:32:10,190 --> 00:32:14,360
you need to approximate it on the tools of shows in the states community on

488
00:32:14,360 --> 00:32:19,680
increasingly in the machine learning community consists of using multicolour methods OK

489
00:32:20,780 --> 00:32:24,770
that's why we will be interested in that similarly when you don't want to do

490
00:32:24,770 --> 00:32:29,280
so some kind of model selection in a bayesian framework you have essentially to compute

491
00:32:29,280 --> 00:32:34,780
the marginal likelihood of the salvation which once more so i'm unsure integral typically more

492
00:32:35,990 --> 00:32:43,110
OK so everything you show you interesting computing estimates for complete data bayesian models say

493
00:32:43,110 --> 00:32:45,580
you want to compute the posterior distribution

494
00:32:46,000 --> 00:32:51,870
the probability to find a posteriori was televised one more you i mentioned in schools

495
00:32:51,940 --> 00:32:56,810
so basically multicolour meant that what you need to use this issue as you know

496
00:32:56,810 --> 00:32:58,150
like complex models

497
00:32:58,160 --> 00:33:00,210
OK so

498
00:33:00,220 --> 00:33:01,620
i'm not going to go

499
00:33:01,780 --> 00:33:06,650
like to give much more detail about that you will ever caused by peter noreen

500
00:33:06,650 --> 00:33:07,940
basically tool

501
00:33:07,960 --> 00:33:14,330
on bayesian modeling each time you doing essentially bayesian modelling value adding nonsense non slgnol

502
00:33:14,330 --> 00:33:16,090
model or you're going to have to deal

503
00:33:16,110 --> 00:33:20,640
and we the element to it so as to compute marginal distribution or if you're

504
00:33:20,640 --> 00:33:28,370
interested say in computing center predictive distribution over new acceleration even the call observation one

505
00:33:28,370 --> 00:33:32,370
small this is an idea want in order to go once more you have to

506
00:33:32,370 --> 00:33:37,030
use the kalman you have to use demonstrated sold

507
00:33:37,050 --> 00:33:38,370
that's essentially

508
00:33:38,380 --> 00:33:40,280
my motivation

509
00:33:40,300 --> 00:33:43,120
bayesian statistics i want to approximate

510
00:33:43,120 --> 00:33:47,840
like i i'm unsure percent distribution all over

511
00:33:49,750 --> 00:33:54,210
all the remaining the rest of this talk i will not put any of is

512
00:33:56,080 --> 00:34:02,360
bayesian statistics i will just assume that i'm interested in a given target distribution poisson

513
00:34:02,360 --> 00:34:08,200
distribution to result from the application point of view it's typically we're still distribution but

514
00:34:08,200 --> 00:34:12,940
all it was obvious describing can be generally use uses can be used in the

515
00:34:12,950 --> 00:34:15,430
much much wider context

516
00:34:16,660 --> 00:34:20,800
i say for those of you who have never seen a mccallum i'm just you

517
00:34:20,800 --> 00:34:27,320
know we you essentially very briefly the main idea behind multicolour so

518
00:34:27,370 --> 00:34:32,190
let's start with the kind of silly to example the sea like simplest you can

519
00:34:32,190 --> 00:34:37,880
think so we we can see that we have essentially a circle inscribed in a

520
00:34:37,880 --> 00:34:39,620
rectangle OK

521
00:34:39,620 --> 00:34:42,630
basically i'm supposing

522
00:34:42,700 --> 00:34:47,980
OK so that's really silly example that basically going to utilize the rain that falls

523
00:34:49,110 --> 00:34:50,760
on the square

524
00:34:51,580 --> 00:34:54,220
the blue square here

525
00:34:55,330 --> 00:34:58,100
i'm interested basically

526
00:34:58,120 --> 00:35:00,620
in computing the probability

527
00:35:00,640 --> 00:35:04,510
forward waterfall in a given region a

528
00:35:05,180 --> 00:35:09,190
so which is proportional obviously to the our you have a because i assume that

529
00:35:09,190 --> 00:35:12,850
the drops falls uniformly basically on the square

530
00:35:14,750 --> 00:35:17,410
basically in this case you don't need to do

531
00:35:17,430 --> 00:35:21,490
to do anything basically you know that you should be fine as the the on

532
00:35:21,490 --> 00:35:24,930
the that represent the location of the drug

533
00:35:24,950 --> 00:35:29,620
on a region of interest status article in my case in the previous slide OK

534
00:35:29,620 --> 00:35:37,400
In the thank

535
00:35:38,840 --> 00:35:46,230
I've been talking and then multiplying matrices already but certainly time for me to discuss

536
00:35:46,280 --> 00:35:49,460
the rules for matrix multiplication and

537
00:35:50,070 --> 00:35:51,470
them into

538
00:35:51,500 --> 00:35:55,760
the body of the many ways you can do it and they all give the

539
00:35:55,760 --> 00:35:58,480
same so as and all-important

540
00:35:58,970 --> 00:36:02,180
the matrix multiplication and a

541
00:36:02,210 --> 00:36:08,240
come in Delaware and we mentioned the inverse of a matrix but there is that

542
00:36:08,320 --> 00:36:12,000
than what do about inverses and how to find

543
00:36:12,590 --> 00:36:21,790
OK so not beginning with multiplying 2 matrices birth OK so suppose I have a

544
00:36:24,130 --> 00:36:28,120
a multiplying the matrices B

545
00:36:29,770 --> 00:36:41,070
and giving me a result well I following the anytime me OK and so that

546
00:36:41,230 --> 00:36:42,490
be just review of

547
00:36:43,210 --> 00:36:46,760
rules for the for this

548
00:36:47,550 --> 00:36:54,030
at the entry into rule i and column j so that the ICA anything right

549
00:36:54,030 --> 00:36:59,070
there is the idea we always write the row numbers

550
00:36:59,120 --> 00:37:03,620
and then the columns so I might my might maybe I take it the 3

551
00:37:04,870 --> 00:37:11,820
just make it set so instead of by Jeremy use numbers the the word that

552
00:37:11,820 --> 00:37:14,170
comes from the 3

553
00:37:14,390 --> 00:37:18,290
it comes from both 3

554
00:37:18,310 --> 00:37:22,700
row 3 and column for

555
00:37:23,900 --> 00:37:25,060
as you know

556
00:37:25,100 --> 00:37:29,950
problem and then I just write down the can we write down the formula for

557
00:37:31,170 --> 00:37:32,450
3 4

558
00:37:34,630 --> 00:37:38,610
if we look at the whole role in the whole whole column the quick way

559
00:37:38,610 --> 00:37:46,050
for me to say it is rolled 3 of I could use that . 4

560
00:37:46,060 --> 00:37:51,690
. product often use that actually done column for

561
00:37:51,720 --> 00:37:52,930
of the

562
00:37:54,050 --> 00:38:00,240
and but this gives us a chance that just like you a little matrix notation

563
00:38:00,930 --> 00:38:05,610
what are the entries what this 1st entry in 3

564
00:38:05,930 --> 00:38:13,310
the 1st the 1st that number that's sitting right there is a need so it's

565
00:38:13,310 --> 00:38:18,630
got to indices and what are really want other than any

566
00:38:18,680 --> 00:38:24,310
3 1 there now what's the 1st guy at the top of a column for

567
00:38:24,430 --> 00:38:31,060
so what was sitting up there will be 1 4 right

568
00:38:31,500 --> 00:38:38,370
so that this not a product starts with an a 3 1 times the 1

569
00:38:39,330 --> 00:38:44,720
and then went on to the next so this is like I'm accumulating the sun

570
00:38:44,810 --> 00:38:47,550
then comes the next guy 3

571
00:38:47,610 --> 00:38:53,620
2nd column times be tuned for 2nd row

572
00:38:53,830 --> 00:38:56,440
so as to be anything really new

573
00:38:56,610 --> 00:38:58,810
the 2 more and stuff

574
00:39:00,610 --> 00:39:07,160
just practice within this all let let me practice with a summation forms

575
00:39:08,330 --> 00:39:10,190
so this is an

576
00:39:10,650 --> 00:39:13,210
most of the costs use

577
00:39:13,270 --> 00:39:20,940
all vectors and very seldom not getting down to the details of the particular entries

578
00:39:20,940 --> 00:39:21,310
but here

579
00:39:21,810 --> 00:39:28,460
we better do so I'm in some kind of some right of names in rows

580
00:39:29,900 --> 00:39:35,220
column k of times things it

581
00:39:36,170 --> 00:39:40,490
I wrote a column for using that

582
00:39:40,650 --> 00:39:43,730
that's what we're seeing here this is same 1

583
00:39:44,370 --> 00:39:47,790
here claim to follow along so well

584
00:39:47,970 --> 00:39:49,290
so that's not

585
00:39:49,310 --> 00:39:55,260
all the way along the row and down the columns say 1 that in so

586
00:39:55,260 --> 00:39:57,370
that's what the agent

587
00:39:57,470 --> 00:40:02,700
the C. 3 4 and 3 like a son of a free trade be paid

588
00:40:03,370 --> 00:40:06,730
this takes a little practice that

589
00:40:06,740 --> 00:40:09,990
OK and

590
00:40:10,240 --> 00:40:15,120
all well maybe I should say when are we allowed to multiply the matrices 1

591
00:40:15,120 --> 00:40:17,580
of the shape of this thing

592
00:40:17,700 --> 00:40:23,760
the shapes the if we allow them to be not necessarily square mesh

593
00:40:24,210 --> 00:40:31,070
if there for the the same stuff if the rectangular they're not the same so

594
00:40:31,070 --> 00:40:38,710
if the rectangle this might be well I always think of a and my aunt

595
00:40:38,710 --> 00:40:46,410
and roles and so that growth and now what's the point commonly rose B

596
00:40:47,370 --> 00:40:52,810
and on the number of rows in and be the number of guys that will

597
00:40:52,810 --> 00:40:57,120
be coming down estimates the number 1 of the product that we will have to

598
00:40:57,120 --> 00:41:00,740
reduces the number a twenty two derivatives

599
00:41:00,790 --> 00:41:03,180
and and or some here

600
00:41:03,180 --> 00:41:06,030
this result

601
00:41:06,040 --> 00:41:08,370
would say is that when i take the derivative

602
00:41:08,430 --> 00:41:11,160
i lose the power of t

603
00:41:11,180 --> 00:41:13,870
in the end i don't want any powers of t

604
00:41:13,950 --> 00:41:16,390
very clear i got to start with

605
00:41:16,450 --> 00:41:19,830
function that looks like the square

606
00:41:19,910 --> 00:41:21,890
this we want to take two derivatives

607
00:41:21,920 --> 00:41:23,580
the naughty left

608
00:41:23,620 --> 00:41:27,010
unfortunately we know this is not the right answer because if you take the first

609
00:41:27,010 --> 00:41:28,990
derivative i get to t

610
00:41:29,010 --> 00:41:31,790
the second derivative i get to

611
00:41:31,870 --> 00:41:33,310
i want to get a

612
00:41:33,370 --> 00:41:34,740
and not too

613
00:41:34,740 --> 00:41:36,160
and it's very clear

614
00:41:36,180 --> 00:41:37,080
you find

615
00:41:37,100 --> 00:41:38,260
that's it up

616
00:41:38,270 --> 00:41:40,600
it's you multiplied by this constant

617
00:41:40,620 --> 00:41:43,060
now we are all set

618
00:41:43,100 --> 00:41:46,180
this function will have the right second it

619
00:41:46,260 --> 00:41:48,140
the certainly describes the particle

620
00:41:48,160 --> 00:41:50,120
whose activation is it

621
00:41:50,200 --> 00:41:54,640
it is not dependent on time

622
00:41:54,660 --> 00:41:57,530
but the question is is this the most

623
00:41:57,540 --> 00:41:59,810
general answer

624
00:41:59,830 --> 00:42:01,870
this is just one answer

625
00:42:01,890 --> 00:42:04,930
i think you all know that this is not the most general and so this

626
00:42:04,930 --> 00:42:06,180
one answer

627
00:42:06,200 --> 00:42:10,700
but i can add to this some number like ninety six

628
00:42:10,700 --> 00:42:13,910
still have the property that if you take two derivatives you're going to get the

629
00:42:13,910 --> 00:42:15,640
same acceleration

630
00:42:15,640 --> 00:42:18,870
so ninety six now is the typical

631
00:42:18,870 --> 00:42:22,040
constance i'm going to give the name c

632
00:42:24,720 --> 00:42:28,180
everyone knows calculus that if you if i find a function

633
00:42:28,200 --> 00:42:32,290
what would you know only the derivative you can always add a constant to

634
00:42:32,330 --> 00:42:35,370
one person answer without changing anything

635
00:42:35,430 --> 00:42:39,350
but i think here you know you can do more

636
00:42:39,370 --> 00:42:41,870
you can add something else to be answered

637
00:42:41,950 --> 00:42:43,640
without invalidating

638
00:42:43,680 --> 00:42:44,760
and that is

639
00:42:45,990 --> 00:42:50,390
with one power of p

640
00:42:50,470 --> 00:42:54,890
because if you take one derivative it'll survive but if you take two derivatives

641
00:42:54,890 --> 00:42:58,740
it you begin to get wiped out

642
00:42:58,790 --> 00:43:04,620
now it's not obvious but it is true that you cannot add to this anymore

643
00:43:04,640 --> 00:43:05,930
the basic idea

644
00:43:05,950 --> 00:43:10,290
solving these equations in integrating is you find one answer

645
00:43:10,310 --> 00:43:14,160
the many taking derivatives the function does what it's supposed to do

646
00:43:14,220 --> 00:43:18,290
but then having found one and so you can add to with anything

647
00:43:18,310 --> 00:43:21,680
that gets killed by the act of taking derivatives

648
00:43:21,810 --> 00:43:24,120
the only one that is can have a constant

649
00:43:24,140 --> 00:43:28,450
taking to that you can constant and something linear in t

650
00:43:28,470 --> 00:43:32,080
if you knew only the third derivative the function you can have something quadratic without

651
00:43:32,080 --> 00:43:33,490
changing the outcome

652
00:43:33,510 --> 00:43:35,330
so this is the

653
00:43:35,370 --> 00:43:38,310
was general

654
00:43:38,310 --> 00:43:42,180
position for particle of constant acceleration

655
00:43:43,950 --> 00:43:45,430
you must remember that

656
00:43:45,430 --> 00:43:47,870
this describes the particle going side-to-side

657
00:43:47,910 --> 00:43:51,490
i can also describe particle going up and down

658
00:43:51,490 --> 00:43:52,790
if i do that

659
00:43:52,810 --> 00:43:56,010
i was i would like to call that coordinate y

660
00:43:56,060 --> 00:43:59,850
and i would the same thing

661
00:43:59,850 --> 00:44:02,200
you've got to realize that in calculus

662
00:44:02,200 --> 00:44:03,740
the symbols that you call

663
00:44:03,740 --> 00:44:06,830
x and y are completely arbitrary

664
00:44:06,870 --> 00:44:09,040
if you know the second derivative y

665
00:44:09,140 --> 00:44:12,370
to be a and the answer looks like this if you know the second it

666
00:44:12,370 --> 00:44:16,470
directs the absolutely

667
00:44:16,470 --> 00:44:18,450
now we have asked

668
00:44:18,450 --> 00:44:20,030
what these numbers

669
00:44:20,040 --> 00:44:22,810
b and c

670
00:44:22,870 --> 00:44:24,660
so let me go back now

671
00:44:24,680 --> 00:44:25,990
this expression

672
00:44:25,990 --> 00:44:27,620
x t

673
00:44:28,830 --> 00:44:32,080
it is where over two

674
00:44:32,140 --> 00:44:35,080
let's see

675
00:44:35,080 --> 00:44:39,180
let's pt

676
00:44:39,200 --> 00:44:43,030
it's true mathematical mathematically you can add two numbers you gotta ask yourself what am

677
00:44:43,030 --> 00:44:44,970
i doing as a physicist

678
00:44:45,010 --> 00:44:47,470
when i add these two numbers what am i supposed to do with a and

679
00:44:47,470 --> 00:44:49,790
b with this BNC

680
00:44:49,810 --> 00:44:52,700
what value should i think

681
00:44:52,720 --> 00:44:56,870
the answer is that simply knowing the particle has that acceleration is not enough to

682
00:44:56,870 --> 00:44:57,790
tell you

683
00:44:57,850 --> 00:44:59,640
when the particle will be

684
00:44:59,680 --> 00:45:00,680
for example

685
00:45:00,790 --> 00:45:02,470
let's take the case where

686
00:45:02,490 --> 00:45:05,100
the particles falling under gravity

687
00:45:05,120 --> 00:45:07,200
then you guys know just me

688
00:45:07,220 --> 00:45:09,310
acceleration is minus

689
00:45:09,330 --> 00:45:10,910
nine point eight

690
00:45:12,240 --> 00:45:16,180
twenty nine point because minus because it's actually writing down

691
00:45:16,200 --> 00:45:19,120
and up was taken to be the positive direction

692
00:45:19,120 --> 00:45:22,120
in that case why the

693
00:45:22,180 --> 00:45:25,560
will be minus one half the square

694
00:45:25,580 --> 00:45:26,660
let's see

695
00:45:26,700 --> 00:45:29,450
let's be

696
00:45:29,470 --> 00:45:32,240
the point is every object falling under gravity

697
00:45:32,270 --> 00:45:36,060
is given by the same formula but there are many many objects that can have

698
00:45:36,060 --> 00:45:39,140
many histories of falling under gravity

699
00:45:39,200 --> 00:45:42,890
and what's different from one object the other object is

700
00:45:42,950 --> 00:45:47,990
when was it drop from what i can with what need speak

701
00:45:48,010 --> 00:45:50,870
that that's what these numbers are going to tell us and we can verify that

702
00:45:50,870 --> 00:45:51,680
as follows

703
00:45:51,740 --> 00:45:53,740
you want to know what the number c is

704
00:45:53,770 --> 00:45:57,890
you say let's before time t equal to zero

705
00:45:57,930 --> 00:46:00,560
in fact let me go back to this equation if

706
00:46:00,640 --> 00:46:02,790
what time t equal to zero

707
00:46:02,830 --> 00:46:04,410
x zero

708
00:46:04,430 --> 00:46:07,290
it doesn't have the strength doesn't have this term

709
00:46:07,310 --> 00:46:08,450
and c

710
00:46:08,520 --> 00:46:11,240
so i realize that the constancy

711
00:46:11,290 --> 00:46:13,810
o is the initial location of the object

712
00:46:13,870 --> 00:46:15,450
and it's very common

713
00:46:15,470 --> 00:46:16,910
they're not that by

714
00:46:16,930 --> 00:46:20,410
x not

715
00:46:20,410 --> 00:46:22,740
so the meaning of the constant is

716
00:46:22,760 --> 00:46:25,640
where was the object at the initial time

717
00:46:25,660 --> 00:46:27,200
it could have been anywhere

718
00:46:27,220 --> 00:46:29,930
so knowing the acceleration is starting enough to tell you

719
00:46:29,930 --> 00:46:35,010
but it was initial you get the big but it was the initial

720
00:46:35,060 --> 00:46:37,330
then define the meaning of b

721
00:46:37,330 --> 00:46:40,870
we take one derivative of this

722
00:46:40,890 --> 00:46:43,140
that's the last is a function of time

723
00:46:43,200 --> 00:46:45,160
if the derivative this guy

724
00:46:45,350 --> 00:46:46,890
i find it t

725
00:46:46,910 --> 00:46:50,040
plus b

726
00:46:50,140 --> 00:46:52,220
if the velocity of the object

727
00:46:52,260 --> 00:46:54,010
and you can then understand

728
00:46:54,060 --> 00:46:56,270
that we have zero

729
00:46:56,290 --> 00:46:58,060
is what these

730
00:46:58,060 --> 00:47:00,010
which we write as

731
00:47:06,510 --> 00:47:08,510
OK so the final answer

732
00:47:08,560 --> 00:47:11,410
is that except t

733
00:47:11,430 --> 00:47:12,310
looks like

734
00:47:12,310 --> 00:47:14,040
x not

735
00:47:14,040 --> 00:47:16,180
let b not the

736
00:47:16,180 --> 00:47:21,700
it is my impression is that the second movement towards HDP based URI as the

737
00:47:21,700 --> 00:47:23,210
can also make

738
00:47:23,220 --> 00:47:26,190
l is ID's you reference above

739
00:47:26,200 --> 00:47:30,120
over http just the technical question like

740
00:47:30,140 --> 00:47:32,550
there's a identification she might down

741
00:47:32,570 --> 00:47:34,780
on one level and the dereferencing

742
00:47:34,790 --> 00:47:35,490
the text

743
00:47:35,590 --> 00:47:42,040
on another level missing people are moving towards making this idea is also the difference

744
00:47:44,540 --> 00:47:46,680
the idea of linked data is

745
00:47:46,690 --> 00:47:49,070
keep it simple simple

746
00:47:49,090 --> 00:47:53,750
you just have to three technologies and uses regional colleges you can solve most of

747
00:47:53,750 --> 00:48:00,670
your problems with histories technologies like http RDF and links

748
00:48:02,070 --> 00:48:08,550
if people want to do is build proprietary data space for puppetry standards sure and

749
00:48:08,550 --> 00:48:15,570
this idea is also have since URI is the library communities which require specific infrastructure

750
00:48:15,940 --> 00:48:18,050
to do exactly the same thing

751
00:48:18,150 --> 00:48:20,960
just declined wants to get some data that's not nothing

752
00:48:21,940 --> 00:48:28,280
you have been

753
00:48:36,280 --> 00:48:43,090
i would say everything dies and this move what the reference http wise

754
00:48:43,130 --> 00:48:49,630
and from my impression maybe people should have

755
00:48:53,400 --> 00:48:55,750
this is due to the same impression

756
00:49:15,960 --> 00:49:22,070
you notice

757
00:49:22,090 --> 00:49:24,520
this is not too low special thing is

758
00:49:24,570 --> 00:49:26,230
plain HTTP

759
00:49:26,280 --> 00:49:29,280
meaning that apache does most of the time

760
00:49:29,340 --> 00:49:34,050
as you busy just take part like that to the talk about more details

761
00:49:34,070 --> 00:49:35,500
what does but basically

762
00:49:35,500 --> 00:49:41,820
it's http they can use any http server and the thing is you just

763
00:49:41,820 --> 00:49:43,590
when you do your friends URI

764
00:49:43,590 --> 00:49:44,520
you asked

765
00:49:44,570 --> 00:49:49,020
all content type rdf not for content type x amount

766
00:49:49,020 --> 00:49:50,770
so it builds on

767
00:49:50,820 --> 00:49:53,590
paul over classic infrastructure

768
00:49:53,650 --> 00:49:56,460
it's going to

769
00:49:56,500 --> 00:49:59,520
so what happens

770
00:49:59,550 --> 00:50:00,750
you are

771
00:50:26,190 --> 00:50:27,900
also nice because you can

772
00:50:27,920 --> 00:50:32,690
we use of the technologies like access control and stuff that

773
00:50:32,690 --> 00:50:35,920
have proven useful

774
00:50:35,920 --> 00:50:40,090
four ATP four years now and the people can we use that knowledge to already

775
00:50:40,090 --> 00:50:42,570
have about these technologies and

776
00:50:42,800 --> 00:50:44,380
as ideas

777
00:50:44,400 --> 00:50:48,800
scoring from a scientific point of view but i think pretty compelling from a practical

778
00:50:48,800 --> 00:50:53,920
point of view just don't build new in new infrastructure for stuff that already works

779
00:50:57,520 --> 00:51:00,730
used in the world

780
00:51:02,050 --> 00:51:02,710
the thing

781
00:51:02,730 --> 00:51:04,210
the other others

782
00:51:06,750 --> 00:51:08,300
the new

783
00:51:12,480 --> 00:51:20,020
know this is that you know the topic it's coming later in this tutorial

784
00:51:22,340 --> 00:51:24,820
the kind of the tricks of the whole thing like

785
00:51:24,860 --> 00:51:28,320
s on the that explicitly have to set links

786
00:51:28,340 --> 00:51:30,610
so what people usually do is say

787
00:51:30,630 --> 00:51:34,820
you know about some data sources they look the status of clouds that we also

788
00:51:34,820 --> 00:51:39,340
have list the on the in the wiki about all the data sources they select

789
00:51:39,360 --> 00:51:41,540
some noticing interesting

790
00:51:41,550 --> 00:51:43,000
and then they usually

791
00:51:43,000 --> 00:51:46,190
the data sets a big like the data set was

792
00:51:46,210 --> 00:51:47,800
o records about

793
00:51:47,840 --> 00:51:48,500
it was

794
00:51:48,500 --> 00:51:49,940
twenty so bands

795
00:51:49,940 --> 00:51:52,250
hundreds of thousands of albums

796
00:51:52,270 --> 00:51:58,520
usually use an automatic interlinking charisma some similarity matching i reason to figure out your

797
00:52:00,110 --> 00:52:08,440
could just means you have to talk about this in detail later

798
00:52:14,840 --> 00:52:19,500
this is because of the

799
00:52:19,600 --> 00:52:20,840
much of

800
00:52:22,210 --> 00:52:29,440
but strongly being done is people don't care too much about ontologies

801
00:52:29,440 --> 00:52:30,180
so now you see

802
00:52:30,740 --> 00:52:32,720
hopefully connected to this notion of

803
00:52:33,130 --> 00:52:34,700
if how we control function from

804
00:52:35,300 --> 00:52:36,230
from a jeep e

805
00:52:37,140 --> 00:52:39,780
so the step so this should be clear that is the only

806
00:52:40,370 --> 00:52:43,330
this is the only code snippet i will give you throughout the course but it's

807
00:52:43,430 --> 00:52:47,200
but it's but it's only it's only one line and dual one packed more

808
00:52:49,140 --> 00:52:51,930
okay so how do i get how to actually get the straw

809
00:52:52,830 --> 00:52:55,950
so to do that's i take a whole bunch of index points

810
00:52:56,670 --> 00:52:57,480
a finite number

811
00:52:58,060 --> 00:52:59,620
so here i took

812
00:53:00,130 --> 00:53:02,700
the integer index points between zero and five hundred

813
00:53:05,060 --> 00:53:08,560
i evaluate the kernel function just like we do with those three index points

814
00:53:08,950 --> 00:53:12,040
i evaluate the kernel function building the sum in the sum

815
00:53:12,920 --> 00:53:14,550
five hundred by five hundred matrix can

816
00:53:16,580 --> 00:53:17,680
and then i can take a draw

817
00:53:18,380 --> 00:53:20,690
from the guassian with zero mean and the covariance

818
00:53:21,620 --> 00:53:25,740
so how that's actually done right and this this would be here matlab code

819
00:53:27,310 --> 00:53:29,310
how that's actually done is is according to this

820
00:53:30,740 --> 00:53:31,670
so they will give u

821
00:53:32,130 --> 00:53:34,120
this this procedure will give you this nice

822
00:53:34,550 --> 00:53:35,730
will give you this nice strong

823
00:53:38,260 --> 00:53:43,160
okay so there is one drawing green now taking four draws at end and belaboring

824
00:53:43,160 --> 00:53:47,150
this point just so just that we remind ourselves that draw from guassian process gives

825
00:53:47,150 --> 00:53:50,190
you a function so four draws from now give is nice for functions

826
00:53:53,650 --> 00:53:56,310
before when we are evaluating the kernel matrix

827
00:53:56,900 --> 00:54:00,680
we we messed around with the hyperparameters a little bit and see how that changed

828
00:54:01,550 --> 00:54:03,560
to see how the change the covariance matrix

829
00:54:03,970 --> 00:54:05,440
let's do that here in pictures

830
00:54:05,890 --> 00:54:06,930
so here is

831
00:54:07,310 --> 00:54:08,370
sigma alpha ten

832
00:54:08,780 --> 00:54:09,900
and the lengthscale fifty

833
00:54:11,850 --> 00:54:14,370
if i change and leave the lengthscale fifty

834
00:54:14,760 --> 00:54:19,580
but not change the power the envelope about from ten down before r and you

835
00:54:19,580 --> 00:54:20,620
see what's happened is that the

836
00:54:22,250 --> 00:54:24,390
the onlooker shrunk and the draws a strong

837
00:54:24,780 --> 00:54:25,470
not surprising

838
00:54:26,680 --> 00:54:27,320
if i change

839
00:54:28,270 --> 00:54:31,500
if i change the lengthscale fifty to a length scale of ten

840
00:54:32,060 --> 00:54:33,390
right so what this is saying is

841
00:54:34,430 --> 00:54:35,970
has two points get further apart

842
00:54:36,480 --> 00:54:39,910
they fall off the their correlation falls off more quickly

843
00:54:40,410 --> 00:54:43,020
so accordingly you get more weakly draws

844
00:54:47,640 --> 00:54:48,970
one final point on this is this

845
00:54:49,390 --> 00:54:53,540
we should feel a whole lot like when we are evaluating those those covariance matrices

846
00:54:53,810 --> 00:54:55,220
in other words this is easy

847
00:54:56,040 --> 00:54:57,060
is it so is that

848
00:54:57,580 --> 00:55:04,770
closer to the identity matrix the covariance matrix is closely and stand for example this which has much longer-range correlations

849
00:55:06,620 --> 00:55:12,850
okay so i mentioned that's what we've been looking at regression in time and taking draws that are nice

850
00:55:13,330 --> 00:55:18,810
temporal functions as easy to look at and familiar to us are you can also have a dimensional input

851
00:55:20,640 --> 00:55:21,470
i wanna be

852
00:55:22,100 --> 00:55:28,720
one of the the histories the application areas where gas process even used a lot is in geostatistics andtheir

853
00:55:29,410 --> 00:55:30,470
there are often interested in

854
00:55:31,290 --> 00:55:37,290
since spatial guassian processes so for example latitude longitude instead of regression and time you

855
00:55:37,290 --> 00:55:40,370
want regress on latitude longitude so do that we can make

856
00:55:41,540 --> 00:55:42,890
we can make each input

857
00:55:43,430 --> 00:55:47,040
instead of instead of a single real a real valued number a pair of numbers

858
00:55:47,950 --> 00:55:48,350
so now

859
00:55:49,000 --> 00:55:52,600
we've got some guassian process here for the same gene he

860
00:55:53,080 --> 00:55:57,200
and the kernel is almost exactly the same except here is just an extra term

861
00:55:58,950 --> 00:56:03,000
there's there's there's there's a squared term for each dimension about casting process

862
00:56:03,450 --> 00:56:07,220
so what might a draw from not look like instead of one single function

863
00:56:07,930 --> 00:56:12,640
what you get now is a field over over latitude longitude so that a random

864
00:56:12,640 --> 00:56:14,120
function is drawn over the field

865
00:56:17,830 --> 00:56:18,180
i suppose

866
00:56:19,040 --> 00:56:24,120
shameless plug if you are staying free cystatin you wanna see a multidimensional cheapy in action

867
00:56:24,930 --> 00:56:27,640
we've got a we've got a paper on the and i'm sure there will be

868
00:56:27,640 --> 00:56:29,370
other papers which appears as well

869
00:56:31,140 --> 00:56:31,870
okay so

870
00:56:32,950 --> 00:56:33,890
we've got the same model

871
00:56:34,410 --> 00:56:35,390
there we just been dealing with

872
00:56:35,810 --> 00:56:37,520
and now let's let's gather some data

873
00:56:39,660 --> 00:56:42,740
so this is that this is this is our model the jeep model

874
00:56:45,160 --> 00:56:45,910
we've got a prior

875
00:56:47,750 --> 00:56:50,950
and we can go and take we can go and take a data point so

876
00:56:50,950 --> 00:56:53,350
on so i say i gathered data point in time

877
00:56:54,390 --> 00:56:55,500
two hundred four r

878
00:56:56,620 --> 00:56:57,310
so what i know

879
00:56:57,950 --> 00:57:00,890
is there i can evaluate wife two hundred four r

880
00:57:01,220 --> 00:57:03,040
and i know that this is according to the model

881
00:57:03,450 --> 00:57:07,350
this is calcium distributed according to mean zero and came why

882
00:57:08,350 --> 00:57:12,450
evaluated to afford four coming to the fore that's a simple univariate gassing

883
00:57:13,890 --> 00:57:17,950
this is all just pulling this right out of right out the definition of f

884
00:57:18,120 --> 00:57:19,330
of what other jeep peay

885
00:57:19,740 --> 00:57:21,370
and i'm just evaluating the kernel matrix

886
00:57:24,720 --> 00:57:29,750
then we can use conditioning to update the posterior r so here i still got the prior there

887
00:57:30,270 --> 00:57:33,140
but not got this data observation so i can use

888
00:57:33,600 --> 00:57:35,720
i can use the inference rule that we talked about

889
00:57:36,350 --> 00:57:37,930
and evaluate why to

890
00:57:38,520 --> 00:57:40,680
so i take this data point why to afor

891
00:57:41,240 --> 00:57:42,410
and run this equation

892
00:57:43,850 --> 00:57:44,680
and what is this give me

893
00:57:45,410 --> 00:57:47,020
well now this is refined

894
00:57:47,720 --> 00:57:51,450
my posterior estimate of so there is no longer flat mean function

895
00:57:51,890 --> 00:57:54,290
but i think the mean function comes down here and goes to ask

896
00:57:54,850 --> 00:57:56,660
furthermore because there are choice

897
00:57:57,240 --> 00:58:03,640
f of noise parameters you can see that what's happened here is right around two afor i'm awfully short

898
00:58:04,100 --> 00:58:05,040
my my my my

899
00:58:05,930 --> 00:58:09,810
covariance outlook has collapsed and of assure that that the measurement is around there

900
00:58:10,450 --> 00:58:12,120
but as soon as i get further away

901
00:58:12,740 --> 00:58:16,020
well i forget because because i don't think that what happens in four hundred is

902
00:58:16,020 --> 00:58:19,290
particularly related to what happened to the fore so by the time i get over

903
00:58:19,290 --> 00:58:21,140
here and basically back to the prior

904
00:58:24,950 --> 00:58:26,470
a small change is there

905
00:58:26,930 --> 00:58:31,180
here we are looking at the posterior this this i think is important it is

906
00:58:31,180 --> 00:58:34,700
important a hang on just for a second because often when you look at

907
00:58:35,290 --> 00:58:39,290
geeky work it's not clear whether people are talking about whether whether showing that the

908
00:58:39,290 --> 00:58:42,040
posterior or they are showing a predictive model so

909
00:58:43,160 --> 00:58:48,540
this on the poster and this here is the predictive the predictive distribution so you're

910
00:58:48,540 --> 00:58:51,560
just know this is a very small change here but this this this very simple

911
00:58:51,560 --> 00:58:55,620
it has just increased slightly ever-so-slightly because we think that on top of their

912
00:58:55,620 --> 00:59:00,050
and thank you again to the person loaned me

913
00:59:00,090 --> 00:59:02,480
laser pointer and so excited

914
00:59:04,410 --> 00:59:07,340
so in the second part of the tutorial

915
00:59:07,350 --> 00:59:09,410
i'm going to focus on

916
00:59:09,430 --> 00:59:13,890
other ways of understanding adaboost so

917
00:59:13,940 --> 00:59:15,760
so far i've described

918
00:59:16,050 --> 00:59:19,690
talk about boosting in terms of

919
00:59:19,750 --> 00:59:25,500
as a boosting algorithm in this pack sense and i talked about the margins theory

920
00:59:25,530 --> 00:59:28,080
as a way of understanding

921
00:59:28,140 --> 00:59:32,140
why adaboost often does not overfit and let me say i don't think i said

922
00:59:32,140 --> 00:59:36,060
it so far but of course adaboost sometimes does overfit certainly

923
00:59:36,100 --> 00:59:39,680
but in many cases often does not

924
00:59:39,700 --> 00:59:43,780
it turns out that there are many other ways of thinking about adaboost

925
00:59:44,070 --> 00:59:47,570
and in the second part of the talk i want to talk about three of

926
00:59:48,280 --> 00:59:52,200
the first one is a game theoretic perspective on

927
00:59:52,490 --> 00:59:56,100
boosting in general the second one is viewing

928
00:59:56,110 --> 01:00:02,130
boosting has lost minimisation algorithm so in other words in terms of optimisation

929
01:00:02,190 --> 01:00:03,660
and then the final view

930
01:00:03,660 --> 01:00:11,610
which is closely related to the loss minimisation is a view based on information geometry

931
01:00:11,630 --> 01:00:14,410
so let me start with the first of these

932
01:00:15,850 --> 01:00:21,440
we can think about boosting has a game that is as of formal interaction

933
01:00:21,470 --> 01:00:23,270
between the two

934
01:00:24,740 --> 01:00:29,690
in this setting the two players are the boosting algorithm are the booster

935
01:00:29,710 --> 01:00:33,130
and the weak learning algorithm or the weak learner

936
01:00:33,160 --> 01:00:35,830
so on each round t

937
01:00:35,880 --> 01:00:38,940
each of these players is making the move

938
01:00:38,960 --> 01:00:44,670
so the booster chooses distribution dt that's the move that the booster makes and the

939
01:00:44,670 --> 01:00:50,580
weak learner responds by choosing a weak classifier ht

940
01:00:50,580 --> 01:00:53,710
so we can think of this as the game booster makes move and the weak

941
01:00:53,710 --> 01:00:56,850
learner makes move and that repeats

942
01:00:56,860 --> 01:01:00,940
so we're going to study this in terms of game theory or think about in

943
01:01:00,940 --> 01:01:03,470
terms of game theory so game theory

944
01:01:03,490 --> 01:01:09,080
is the field of mathematics that studies ordinary games things like you know checkers tic-tac-toe

945
01:01:09,080 --> 01:01:10,210
and so on

946
01:01:10,240 --> 01:01:16,140
but it also studies all kinds of interactions between all sorts of players between people

947
01:01:16,140 --> 01:01:19,590
and corporations and animals

948
01:01:19,640 --> 01:01:21,490
computer agents

949
01:01:21,520 --> 01:01:23,610
and here we're going to be thinking about

950
01:01:23,620 --> 01:01:27,610
boosting as a game between these two players the booster and a weak learner

951
01:01:27,620 --> 01:01:31,890
and this is going to lead has will see this is going to lead to

952
01:01:31,890 --> 01:01:34,930
many insights about boosting we're going to see that

953
01:01:34,950 --> 01:01:39,740
the boosting problem itself is very closely connected to game theory

954
01:01:39,760 --> 01:01:43,590
two key notions of game theory

955
01:01:44,150 --> 01:01:45,480
OK so

956
01:01:45,490 --> 01:01:48,860
to get started let me give a little background on game theory

957
01:01:49,980 --> 01:01:54,170
for our purposes the game is defined by a matrix m

958
01:01:54,180 --> 01:01:59,120
so here's the game matrix for the game rock paper scissors which hopefully everybody

959
01:01:59,170 --> 01:02:01,620
nose in principle

960
01:02:01,640 --> 01:02:03,080
any game can be

961
01:02:03,140 --> 01:02:08,090
any zero sum game can be put into this form as matrix although in general

962
01:02:08,090 --> 01:02:09,020
the matrix might be

963
01:02:09,490 --> 01:02:11,680
extremely large

964
01:02:11,700 --> 01:02:15,080
but in this case the game matrix is just three by three

965
01:02:15,140 --> 01:02:20,240
so in this game there are two players the row player in the column player

966
01:02:20,290 --> 01:02:24,360
to play the game the row player chooses one of these rows and the columns

967
01:02:24,360 --> 01:02:26,430
player chooses one of these columns

968
01:02:26,460 --> 01:02:29,400
and the resulting loss

969
01:02:29,420 --> 01:02:33,490
to the row player is just whatever entry is chosen up here

970
01:02:33,520 --> 01:02:36,290
so for instance if the row player chooses

971
01:02:37,300 --> 01:02:40,200
and the column player chooses scissors

972
01:02:40,860 --> 01:02:42,620
scissors beats paper

973
01:02:42,640 --> 01:02:46,430
and so the loss to the row player is one of his were always going

974
01:02:46,430 --> 01:02:49,650
to be taking the view of the row player you can think of the column

975
01:02:50,140 --> 01:02:51,890
wanting to maximize

976
01:02:51,900 --> 01:02:55,740
the loss suffered by the row player so in that sense the game is set

977
01:02:55,740 --> 01:02:58,080
to be zero sum

978
01:02:58,080 --> 01:03:01,610
meaning whatever one player wins the other player loses

979
01:03:04,110 --> 01:03:06,420
OK so

980
01:03:06,550 --> 01:03:11,930
so usually we allow the players to make randomized choices in their play

981
01:03:13,100 --> 01:03:16,720
up until now is in the row player chooses a single row and column player

982
01:03:16,720 --> 01:03:18,490
chooses a single column

983
01:03:18,700 --> 01:03:23,750
but instead we could allow the row player to choose the distribution over rows and

984
01:03:23,760 --> 01:03:28,990
the columns player to choose the distribution over columns with the understanding that the actual

985
01:03:28,990 --> 01:03:30,610
choice of rho

986
01:03:30,660 --> 01:03:33,490
will be selected at random according to p

987
01:03:33,490 --> 01:03:36,980
and likewise the actual choice of column will be chosen according

988
01:03:36,990 --> 01:03:38,900
the distribution q

989
01:03:38,920 --> 01:03:41,500
and then we can compute the

990
01:03:41,540 --> 01:03:46,800
row player's expected loss which would just be the sum over rows and columns of

991
01:03:46,800 --> 01:03:52,380
the probability of choosing their own column times the actual loss that would be suffered

992
01:03:52,680 --> 01:03:54,600
by j

993
01:03:54,650 --> 01:03:57,720
we can rewrite that in this nice matrix form

994
01:03:57,740 --> 01:04:01,220
or i'm going to be using this notation pq

995
01:04:01,260 --> 01:04:04,800
i to mean the loss suffered if the row player chooses p on the column

996
01:04:04,800 --> 01:04:05,940
player chooses

997
01:04:06,990 --> 01:04:09,760
OK so this is really an expectation

998
01:04:09,810 --> 01:04:12,980
but i'm going to forget the expectation part and we're just going to think of

999
01:04:12,980 --> 01:04:14,720
this as the actual

1000
01:04:14,740 --> 01:04:17,760
loss suffered by the row player OK

1001
01:04:17,800 --> 01:04:21,110
i'm also going to write things like m p j

1002
01:04:21,110 --> 01:04:27,170
when q is just concentrated on a single column j

1003
01:04:27,610 --> 01:04:29,160
OK so

1004
01:04:29,190 --> 01:04:31,050
the fundamental theorem

1005
01:04:31,090 --> 01:04:36,730
zero sum games is called the minimax theorem it's divine ointment

1006
01:04:36,740 --> 01:04:41,380
and in symbols it says that if you take the men over distributions p

1007
01:04:41,400 --> 01:04:42,510
over rows

1008
01:04:42,540 --> 01:04:47,200
of the max over column distributions q of the loss that would result

1009
01:04:47,220 --> 01:04:49,560
that's equal to the maximum

1010
01:04:49,610 --> 01:04:53,740
so you can just take the minimax in reverse get the maximum

1011
01:04:53,740 --> 01:04:57,930
and this common value is called the value of the game and i'm going to

1012
01:04:57,930 --> 01:05:02,000
write it as be OK so this is just a lot of mathematical symbols what

1013
01:05:02,000 --> 01:05:04,030
does it actually mean

1014
01:05:04,150 --> 01:05:09,340
well the fact that the minimax is equal to v

1015
01:05:09,350 --> 01:05:15,130
that means that the row player has what's called the minmax strategy p star

1016
01:05:15,790 --> 01:05:21,870
so that no matter how the column player plays the matter what strategy distribution q

1017
01:05:21,870 --> 01:05:24,930
is chosen by the column player the loss

1018
01:05:25,470 --> 01:05:27,170
will be at most the

1019
01:05:27,180 --> 01:05:30,740
that's what meant minimax equals b means

1020
01:05:32,110 --> 01:05:36,730
and the fact that max equals maximum in the fact that the maximum is also

1021
01:05:36,730 --> 01:05:38,470
equal to the value v

1022
01:05:38,490 --> 01:05:41,010
it means that p star

1023
01:05:41,030 --> 01:05:42,360
it is optimal

1024
01:05:42,370 --> 01:05:47,060
its optimal in the sense that the column player has its own strategy q star

1025
01:05:47,060 --> 01:05:49,050
called the maximum strategy

1026
01:05:49,070 --> 01:05:53,350
so that no no matter how the row player plays its loss will be at

1027
01:05:53,350 --> 01:05:59,220
debating what what exactly what function for use on this of exponential decay function is

1028
01:05:59,220 --> 01:06:03,300
this happens there is a common one that seems reasonable choice

1029
01:06:03,310 --> 01:06:05,860
you can actually pick another functions as well

1030
01:06:06,640 --> 01:06:11,420
the image and the second is that for those who are familiar with the sum

1031
01:06:11,420 --> 01:06:12,310
of the

1032
01:06:12,350 --> 01:06:18,700
normal distribution of the gaussians distribution say this what this form written out here you

1033
01:06:18,700 --> 01:06:24,150
know it cosmetically lots of the gaussians distribution but this is that actually does actually

1034
01:06:24,150 --> 01:06:28,990
has absolutely nothing to do with gauss distribution on this is not

1035
01:06:28,990 --> 01:06:34,450
it's not that you probably excised calcium whatever is no no such interpretation this is

1036
01:06:34,450 --> 01:06:39,640
just a convenient function that happens to be bell-shaped function and elders of any calcium

1037
01:06:41,770 --> 01:06:44,350
as the fact

1038
01:06:44,370 --> 01:06:49,550
if you're the familiar bell-shaped calcium on again just

1039
01:06:49,590 --> 01:06:53,020
the way to the end of associating with these points is that some

1040
01:06:53,030 --> 01:06:54,890
if you may imagine

1041
01:06:54,950 --> 01:06:59,010
but the sort the bell shape by centred around the position of where you want

1042
01:06:59,020 --> 01:07:03,800
to value hypothesis it then this is saying this point here i'll give

1043
01:07:03,810 --> 01:07:05,690
a weight is proportional

1044
01:07:05,720 --> 01:07:09,620
so the height the gaussians used to the height of the bell shape function

1045
01:07:09,640 --> 01:07:11,540
evaluated at this point

1046
01:07:11,550 --> 01:07:16,650
and the way to get to this point to this training example will be proportional

1047
01:07:16,650 --> 01:07:17,760
to that right

1048
01:07:17,810 --> 01:07:19,030
and so on

1049
01:07:19,050 --> 01:07:22,580
so the training examples are really far away

1050
01:07:22,600 --> 01:07:24,750
a very small way

1051
01:07:28,340 --> 01:07:34,440
one last small generalisation to this is that normally is one of the parameter that

1052
01:07:34,440 --> 01:07:36,010
is out of

1053
01:07:36,010 --> 01:07:38,390
this which i don't know how

1054
01:07:38,410 --> 01:07:43,820
on again this looks suspiciously like variance of the gaussians but this is not is

1055
01:07:43,830 --> 01:07:47,770
convenient form of function this parameter tau

1056
01:07:47,780 --> 01:07:52,190
it's called the bandwidth

1057
01:07:54,790 --> 01:07:59,280
and so

1058
01:07:59,390 --> 01:08:08,340
if only it controls how fast the weights for with distance so

1059
01:08:08,360 --> 01:08:10,130
my diagram from

1060
01:08:10,140 --> 01:08:11,220
the one

1061
01:08:14,540 --> 01:08:21,000
so on towers very small category x

1062
01:08:21,030 --> 01:08:26,360
you end up choosing very narrow calcium using a very narrow bell-shaped so that's on

1063
01:08:26,370 --> 01:08:29,160
the way point far we are rapidly

1064
01:08:29,180 --> 01:08:30,570
whereas if fun

1065
01:08:30,580 --> 01:08:33,250
tau is large

1066
01:08:33,550 --> 01:08:41,900
if tau is largely that using

1067
01:08:42,590 --> 01:08:48,570
a weighting function to falls relatively slowly with distance from the aquarium

1068
01:08:54,250 --> 01:08:56,720
i hope you can therefore see that

1069
01:08:56,730 --> 01:09:06,660
if you apply locally weighted linear regression today the second outside this

1070
01:09:06,740 --> 01:09:07,840
then two

1071
01:09:07,860 --> 01:09:11,080
ask what you hypothesis is output is in the point like this can be in

1072
01:09:11,080 --> 01:09:12,250
a straight line

1073
01:09:12,290 --> 01:09:13,780
making that prediction

1074
01:09:13,790 --> 01:09:18,910
on the last week classes output is that value in a straight line there

1075
01:09:19,010 --> 01:09:21,570
you think that value

1076
01:09:21,580 --> 01:09:25,580
and since every time you are you trying to value hypothesis every time you are

1077
01:09:25,660 --> 01:09:29,050
going out to make a prediction before you know how much the new health path

1078
01:09:29,080 --> 01:09:31,100
whatever you need to fit

1079
01:09:31,130 --> 01:09:34,770
you need to run new fitting procedure and then about

1080
01:09:34,770 --> 01:09:36,770
on this line defects

1081
01:09:36,780 --> 01:09:38,860
just at the position of

1082
01:09:38,890 --> 01:09:42,810
on the value of x is the position of the query where you tried

1083
01:09:42,820 --> 01:09:46,850
but if you do this you have every point along the exact is then you

1084
01:09:46,850 --> 01:09:51,300
find that not everyone regression is is able to treat all the full very non

1085
01:09:51,300 --> 01:09:52,660
linear kernel

1086
01:09:52,690 --> 01:09:54,510
it is said that

1087
01:09:58,780 --> 01:10:03,080
in the problem that western that you hear rumor is our from so i won't

1088
01:10:03,080 --> 01:10:04,970
say too much more about it here

1089
01:10:04,970 --> 01:10:06,390
this visualization

1090
01:10:06,400 --> 01:10:11,670
techniques and pretty should depending on the application people may do different things

1091
01:10:11,690 --> 01:10:15,680
so this is a software tool that is applied for

1092
01:10:16,960 --> 01:10:18,530
cell phone calls

1093
01:10:18,590 --> 01:10:19,960
and in this

1094
01:10:20,020 --> 01:10:21,400
and this is o

1095
01:10:21,510 --> 01:10:24,470
for detecting go

1096
01:10:24,480 --> 01:10:27,000
telecommunications fraud and people try to

1097
01:10:28,260 --> 01:10:29,450
data using

1098
01:10:29,460 --> 01:10:31,810
different different graphs

1099
01:10:33,010 --> 01:10:35,700
phone calls that

1100
01:10:35,740 --> 01:10:37,840
from camp

1101
01:10:39,130 --> 01:10:42,780
because actually every not crisp on the cellphone actually

1102
01:10:42,810 --> 01:10:44,910
lines correspond to which

1103
01:10:44,920 --> 01:10:49,410
phone call you made so basically if you have the line here and here that

1104
01:10:49,410 --> 01:10:54,120
means this for some medical this person so they use the colors

1105
01:10:54,200 --> 01:10:58,690
represent different anomaly in this case i think

1106
01:10:58,700 --> 01:11:04,420
below correspond to quite normal behavior and then red green and red correspond to anomalous

1107
01:11:05,400 --> 01:11:09,980
and then use also the kind of the histograms to represent what can be

1108
01:11:10,030 --> 01:11:11,420
number so

1109
01:11:11,420 --> 01:11:17,270
again this visualization techniques mostly depend on the application mostly depend on the

1110
01:11:17,340 --> 01:11:19,210
the problem so

1111
01:11:19,250 --> 01:11:25,970
people end up using different visualization tools different approaches to visualizing the data in multi

1112
01:11:27,890 --> 01:11:33,410
so now we will talk briefly about all this approach is here contextual anomaly collect

1113
01:11:33,410 --> 01:11:40,190
collect detection online and distributed anomaly detection

1114
01:11:40,240 --> 01:11:42,000
so as i mentioned earlier

1115
01:11:42,010 --> 01:11:48,920
in contextual anomaly detection you have to put the context in into the game so

1116
01:11:48,950 --> 01:11:51,510
the key assumption that

1117
01:11:51,560 --> 01:11:53,160
if you don't do this

1118
01:11:53,160 --> 01:11:57,140
all data records made of the same only managed to do this concept

1119
01:11:57,150 --> 01:12:01,700
the context then you will be able to see

1120
01:12:01,790 --> 01:12:04,420
what is normal and what is not

1121
01:12:04,420 --> 01:12:08,930
so the general approach in this kind of techniques is to identify the context first

1122
01:12:08,980 --> 01:12:13,930
using a set of contextual attributes this can be like spatial domain or temporal domain

1123
01:12:13,930 --> 01:12:15,420
to put them into place

1124
01:12:15,430 --> 01:12:20,170
and then you have to to determine if the test data record is anomalous the

1125
01:12:20,170 --> 01:12:21,930
thing that specific on

1126
01:12:21,980 --> 01:12:23,470
o thing to provide some

1127
01:12:24,090 --> 01:12:26,240
four inside

1128
01:12:26,280 --> 01:12:27,950
so obvious

1129
01:12:29,280 --> 01:12:33,160
but of course they can detect anomalies that are hard to see the new

1130
01:12:33,200 --> 01:12:34,300
ignoring this

1131
01:12:34,320 --> 01:12:35,990
contextual information

1132
01:12:36,000 --> 01:12:42,440
but some of the challenges included sometimes identifying a good set of compassion for the

1133
01:12:42,880 --> 01:12:45,680
features may not be trivial task

1134
01:12:48,430 --> 01:12:55,050
how to determine the parts of the context then you find this conversion activities also

1135
01:12:56,140 --> 01:12:57,940
very challenging task

1136
01:12:59,000 --> 01:13:06,400
as i said i set confessional debate attributes usually defined neighborhood for each data record

1137
01:13:06,400 --> 01:13:09,410
so they may include spatial context for example

1138
01:13:09,420 --> 01:13:12,610
we you the latitude longitude certain data records

1139
01:13:12,660 --> 01:13:17,630
the graph context they include edges and waits for sequential position in time

1140
01:13:17,720 --> 01:13:24,030
four profile use some kind of demographics and so on

1141
01:13:24,430 --> 01:13:29,220
so usually what you're trying to do

1142
01:13:29,270 --> 01:13:32,620
you first try to use this had to identify these

1143
01:13:32,620 --> 01:13:37,010
contextual attributes and to reduce the problem to point anomaly detection

1144
01:13:37,980 --> 01:13:41,550
one of the first step is to segment to cluster the data using only this

1145
01:13:41,550 --> 01:13:43,480
contextual information

1146
01:13:43,490 --> 01:13:44,220
and then

1147
01:13:44,240 --> 01:13:47,210
you can of

1148
01:13:47,210 --> 01:13:50,170
apply this traditional anomaly detection techniques

1149
01:13:50,220 --> 01:13:55,520
within each context using other types attributes we call them behavioral attributes

1150
01:13:57,700 --> 01:14:00,050
again the problem is sometimes

1151
01:14:00,080 --> 01:14:06,200
this context like this cannot be saying that and easily that maybe the problem so

1152
01:14:06,270 --> 01:14:10,260
in this case usually you're trying to

1153
01:14:10,310 --> 01:14:11,750
exploit some

1154
01:14:12,140 --> 01:14:13,790
the models

1155
01:14:13,890 --> 01:14:18,960
the well-known in contextual domain for example for dealing with temporal data you trying to

1156
01:14:18,960 --> 01:14:24,360
use for example time series for the sake arima moving average out regression models

1157
01:14:25,300 --> 01:14:28,150
to do something with this can actually did to segment them to buy some more

1158
01:14:29,490 --> 01:14:33,490
and then you can try to do this point anomaly detection

1159
01:14:33,510 --> 01:14:35,920
and and also

1160
01:14:35,940 --> 01:14:40,900
you're trying to optimise using that one elegant writer and automatically analyse

1161
01:14:40,960 --> 01:14:46,340
data instances with respect to the context

1162
01:14:46,400 --> 01:14:49,050
OK this is one of the approaches

1163
01:14:49,050 --> 01:14:53,050
proposed by sundaram come from is to florida

1164
01:14:53,090 --> 01:14:55,650
so they're trying basically to do this conditional

1165
01:14:55,650 --> 01:15:02,690
anomaly detection this term they introduce incentive contextual using word conditional anomaly detection

1166
01:15:03,990 --> 01:15:09,610
basically they represent everyday to record as a pair of attributes x and y x

1167
01:15:09,610 --> 01:15:11,760
denotes the contextual attributes

1168
01:15:11,780 --> 01:15:15,550
and y denotes the behavioral attributes

1169
01:15:15,550 --> 01:15:17,440
so a

1170
01:15:17,460 --> 01:15:19,440
right now they are trying to do

1171
01:15:19,440 --> 01:15:21,690
mixture of

1172
01:15:21,740 --> 01:15:28,090
and you go the model then basically model correspond to contextual data

1173
01:15:28,090 --> 01:15:30,710
very similar to a mixture and the

1174
01:15:30,720 --> 01:15:35,670
of course models correspond to behavioral data so basically they're trying to learn mixture of

1175
01:15:35,670 --> 01:15:37,090
course among for both

1176
01:15:37,170 --> 01:15:40,300
contextual attributes and for here

1177
01:15:40,340 --> 01:15:43,690
so i know that second the mapping

1178
01:15:48,570 --> 01:15:53,480
is basically learned that indicates the probability of the care about to be part to

1179
01:15:53,480 --> 01:15:56,070
be generated by component vj

1180
01:15:56,110 --> 01:16:00,090
when the contextual part is generated by component u j what that actually

1181
01:16:01,210 --> 01:16:03,280
we're are trying to see basically

1182
01:16:03,300 --> 01:16:07,050
how likely is the contextual part to be the component

1183
01:16:07,150 --> 01:16:09,530
of URI basically that's

1184
01:16:09,550 --> 01:16:13,380
what we have here so this is the parameters used in all the actions this

1185
01:16:13,380 --> 01:16:14,440
is used

1186
01:16:14,440 --> 01:16:16,720
as anomaly detection score

1187
01:16:16,740 --> 01:16:17,710
so first

1188
01:16:17,720 --> 01:16:21,190
here this space basically trying to compute

1189
01:16:21,240 --> 01:16:22,820
as a so-called likely

1190
01:16:22,880 --> 01:16:27,360
this data could only the contextual part to be generated by

1191
01:16:27,440 --> 01:16:29,820
this is a distribution

1192
01:16:29,860 --> 01:16:31,070
you i here

1193
01:16:31,090 --> 01:16:32,670
so the next step

1194
01:16:32,740 --> 01:16:35,780
what is probability

1195
01:16:35,780 --> 01:16:39,530
the behavioral part to be generated by the distribution of the j

1196
01:16:39,550 --> 01:16:40,990
and finally

1197
01:16:41,050 --> 01:16:42,720
given this fact

1198
01:16:42,780 --> 01:16:46,010
the data records from contextual q i

1199
01:16:46,010 --> 01:16:50,670
what is the probability that it is the most likely component vj

1200
01:16:50,720 --> 01:16:56,280
that will generate the behavioral part so basically it was first tried to

1201
01:16:56,320 --> 01:16:58,900
estimated distribution for contextual part

1202
01:16:58,920 --> 01:17:01,280
and then to learn from the history

1203
01:17:01,340 --> 01:17:05,190
what kind of behavior can correspond to this confessional part

1204
01:17:06,030 --> 01:17:07,960
then you go to the test data

1205
01:17:08,010 --> 01:17:09,920
you identify contextual part

1206
01:17:09,980 --> 01:17:13,400
and and if the pitcher paul does not fit this piece

1207
01:17:13,440 --> 01:17:17,420
then you will treat this as an online so that's like some kind of tool

1208
01:17:17,760 --> 01:17:21,490
step approach when you're taking this kind of anomalies that's probably

1209
01:17:21,490 --> 01:17:22,840
the only

1210
01:17:22,860 --> 01:17:26,800
actually one of the rare papers that deal with this kind of contextual in conditional

1211
01:17:26,800 --> 01:17:28,170
anomaly detection

1212
01:17:28,260 --> 01:17:35,030
they also mentioned earlier this collective anomaly detection techniques

1213
01:17:35,090 --> 01:17:38,840
and in this approach is

1214
01:17:38,900 --> 01:17:44,590
is you exploit the relationship among data instances in order to detect this data records

1215
01:17:44,610 --> 01:17:47,650
and as i said this is mostly applicable to

1216
01:17:51,170 --> 01:17:52,690
two spatial domains

1217
01:17:52,710 --> 01:17:54,860
and some graph anomaly detection

1218
01:17:54,920 --> 01:17:56,570
of course

1219
01:17:56,590 --> 01:18:00,960
if you have the time series data also sequence a sequence of symbols

1220
01:18:00,960 --> 01:18:06,990
with the same frequency in that in that sequence and it even every subsequence appears

1221
01:18:06,990 --> 01:18:09,400
with the same frequency in that sequence

1222
01:18:09,430 --> 01:18:13,470
this way if you look at how many times thirty five orcas in this

1223
01:18:13,910 --> 01:18:18,140
expansion you would have exactly one

1224
01:18:18,160 --> 01:18:22,590
over hundreds

1225
01:18:24,110 --> 01:18:28,640
so you could say well it's just randomly like if i if i sample one

1226
01:18:29,340 --> 01:18:30,920
in figures

1227
01:18:31,340 --> 01:18:37,620
independently so if i just put numbers i strode dies with ten faces

1228
01:18:37,710 --> 01:18:38,860
and i write down

1229
01:18:38,870 --> 01:18:43,360
which are the numbers that come out i could ever sequence that looks like this

1230
01:18:43,400 --> 01:18:45,630
at least in terms of frequencies

1231
01:18:45,640 --> 01:18:49,390
so why would this sequence b not random

1232
01:18:49,410 --> 01:18:56,450
different difficult thing to say it's not random because here we see a simple way

1233
01:18:56,540 --> 01:19:01,210
to continue the sequence and this and simple in the sense that we can express

1234
01:19:01,210 --> 01:19:02,360
an algorithm

1235
01:19:02,370 --> 01:19:03,980
that would

1236
01:19:04,010 --> 01:19:06,940
produces a sequence and continue

1237
01:19:06,950 --> 01:19:11,360
this algorithm would be the algorithm that computes the this man expansion of can maybe

1238
01:19:11,360 --> 01:19:14,740
it's not a single organism but at least it's an aneurysm

1239
01:19:14,850 --> 01:19:18,660
and it's not just a random phenomenon

1240
01:19:21,060 --> 01:19:22,600
that shows you that

1241
01:19:22,640 --> 01:19:25,820
in this process when we try to

1242
01:19:25,860 --> 01:19:29,780
observe something and continuing to predict what comes next

1243
01:19:29,800 --> 01:19:34,950
usually we look for but turns off for good reasons or for something that we

1244
01:19:34,950 --> 01:19:35,960
can express

1245
01:19:36,040 --> 01:19:39,000
in simple and operated terms

1246
01:19:39,020 --> 01:19:41,370
that would produce the sequence

1247
01:19:41,380 --> 01:19:43,690
and continued

1248
01:19:46,680 --> 01:19:49,220
the question naturally is

1249
01:19:49,240 --> 01:19:53,490
is there some kind of unique way of doing this and i think i have

1250
01:19:53,490 --> 01:19:57,140
convinced you i hope i have convinced you that it's not the case

1251
01:19:57,150 --> 01:19:58,940
and there can be

1252
01:20:00,250 --> 01:20:02,320
many answers to such questions

1253
01:20:02,340 --> 01:20:04,680
for example

1254
01:20:04,700 --> 01:20:09,860
if you think in terms of algorithms if you have a programming language that

1255
01:20:09,870 --> 01:20:14,320
that is some pretty primitive that allows you to naturally make

1256
01:20:14,340 --> 01:20:20,270
o multiplications i ditions and so on may be expressing the algorithm that computes the

1257
01:20:20,300 --> 01:20:24,000
this is expansion of by simple and you can try the short program that does

1258
01:20:24,000 --> 01:20:28,610
it but if you don't have these primitives in your language i don't know if

1259
01:20:28,610 --> 01:20:30,160
you have a

1260
01:20:30,200 --> 01:20:36,940
maybe approach prolog programming language or something new language that doesn't have arithmetic primitives then

1261
01:20:36,980 --> 01:20:40,820
you might have a much harder time expressing this

1262
01:20:42,880 --> 01:20:48,520
and maybe you would try to create a program that expense or continues the sequence

1263
01:20:48,520 --> 01:20:52,900
in a different way than computing the decimal expansion of pi

1264
01:20:52,910 --> 01:20:56,460
because it's all it's all a matter of what are the things that you think

1265
01:20:56,680 --> 01:21:01,470
are simple and what you are the things that you think are complicated

1266
01:21:02,190 --> 01:21:09,470
like this example of group series versus number theorists so number theorists see primes everywhere

1267
01:21:09,470 --> 01:21:11,300
group series c c

1268
01:21:11,340 --> 01:21:15,750
groups everywhere right so it's just

1269
01:21:16,260 --> 01:21:21,410
yeah its simplicity is not an objective notion that's the key point

1270
01:21:22,550 --> 01:21:24,900
unfortunately this is

1271
01:21:24,920 --> 01:21:27,320
i i would say that main

1272
01:21:27,340 --> 01:21:28,430
the main thing to

1273
01:21:29,490 --> 01:21:32,010
about this is that

1274
01:21:32,030 --> 01:21:34,190
there is no principle four

1275
01:21:34,200 --> 01:21:35,980
induction unfortunately

1276
01:21:36,960 --> 01:21:42,210
i could just of the lecture now but they continually of it

1277
01:21:45,550 --> 01:21:51,540
reasoning that you might think of to justify induction based on simplicity

1278
01:21:51,600 --> 01:21:54,210
cannot so can just failed because

1279
01:21:54,230 --> 01:22:00,200
simplicity cannot be defined in an objective way

1280
01:22:02,300 --> 01:22:05,340
what can we do this

1281
01:22:05,340 --> 01:22:06,860
so now i want to

1282
01:22:06,870 --> 01:22:12,090
talk a little bit about probability because probability is often used in this context to

1283
01:22:12,090 --> 01:22:15,630
justify induction or two

1284
01:22:15,700 --> 01:22:18,550
derive algorithm for induction

1285
01:22:18,600 --> 01:22:21,210
and i want to tell you

1286
01:22:21,240 --> 01:22:25,210
a few things about probability just two

1287
01:22:25,220 --> 01:22:28,210
two have a common understanding of this

1288
01:22:28,220 --> 01:22:29,760
so probability is

1289
01:22:29,780 --> 01:22:36,950
the nice theoretical tool mathematical tools for dealing with reasoning with and under uncertainty

1290
01:22:36,970 --> 01:22:40,300
it allows you to formulate

1291
01:22:40,320 --> 01:22:42,940
imprecise statements knowing

1292
01:22:43,950 --> 01:22:44,840
at the heart

1293
01:22:44,870 --> 01:22:48,890
of the probability are the core there are these actions

1294
01:22:48,910 --> 01:22:51,470
so probably you you can deduce more less

1295
01:22:51,490 --> 01:22:52,670
all the rest

1296
01:22:52,680 --> 01:22:56,420
all the all the other properties of probabilities

1297
01:22:57,010 --> 01:22:58,850
but the question is

1298
01:22:58,870 --> 01:23:05,710
how do you relate these abstract mathematical objects that are the probability is too

1299
01:23:05,760 --> 01:23:10,160
the real work how do you measure these quantities do the probabilities

1300
01:23:10,160 --> 01:23:11,480
actually exists

1301
01:23:11,510 --> 01:23:13,760
what do they mean in the real world

1302
01:23:14,940 --> 01:23:19,200
there are several answers to this question actually it again

1303
01:23:19,220 --> 01:23:23,360
a somewhat philosophical question and there

1304
01:23:23,400 --> 01:23:29,070
at least three different points of view on how to answer this question

1305
01:23:29,130 --> 01:23:32,710
one very classical one is too

1306
01:23:32,920 --> 01:23:35,670
i don't the frequentist point of view

1307
01:23:35,670 --> 01:23:41,030
and in this point of view frequent sorry probabilities are

1308
01:23:41,210 --> 01:23:43,650
just taken as needs

1309
01:23:45,110 --> 01:23:49,710
the observed frequencies so you just throw dice a certain number of times you count

1310
01:23:49,710 --> 01:23:55,130
how many times it's ends up on one or two or three and then you

1311
01:23:55,150 --> 01:23:56,970
do that enough times

1312
01:23:58,220 --> 01:23:59,570
the the the

1313
01:23:59,590 --> 01:24:02,490
frequency that you have converges to some value

1314
01:24:02,490 --> 01:24:05,110
and you would say the probability that my dies

1315
01:24:05,130 --> 01:24:07,650
lens on the one

1316
01:24:07,650 --> 01:24:10,150
is this frequency

1317
01:24:11,320 --> 01:24:15,420
and then there's is the objectivist point of view

1318
01:24:15,440 --> 01:24:17,110
that's a

1319
01:24:17,110 --> 01:24:21,040
the case of trees we have to estimate the accuracy evaluation

1320
01:24:21,790 --> 01:24:23,350
when we the

1321
01:24:25,510 --> 01:24:30,000
this is the ultimate value and then the corresponding model has one two three four

1322
01:24:31,610 --> 01:24:35,510
nonzero coefficients and one two or three

1323
01:24:35,520 --> 01:24:38,550
that still see what the point

1324
01:24:38,560 --> 01:24:41,330
and so

1325
01:24:41,490 --> 01:24:43,620
reference here and so in our

1326
01:24:43,630 --> 01:24:48,300
it is also the the score something called the largest package in r and so

1327
01:24:48,300 --> 01:24:49,580
you can get

1328
01:24:49,630 --> 01:24:54,810
this that said i believe it might even be included with the large package and

1329
01:24:54,810 --> 01:24:58,540
you can't really do this picture and and even compare the

1330
01:24:58,570 --> 01:25:00,140
this solution has

1331
01:25:00,160 --> 01:25:08,260
four coefficients versus the ordinary linear regression solution with the cerenkov aphasia and and and

1332
01:25:08,260 --> 01:25:12,740
show in this particular case that there are right i solution has a later lower

1333
01:25:12,740 --> 01:25:16,650
case ever

1334
01:25:16,670 --> 01:25:18,290
so to summarize

1335
01:25:18,610 --> 01:25:22,710
two two is light summaries on on

1336
01:25:22,780 --> 01:25:27,870
globalization so what is the decision what we've been talking or i got this quote

1337
01:25:27,870 --> 01:25:34,400
from these cases this recent phd thesis is that this is so any part of

1338
01:25:34,400 --> 01:25:40,630
model building which takes into account implicitly or explicitly the finiteness and imperfection of the

1339
01:25:41,790 --> 01:25:47,960
and the limited information in it which we can term but instead an abstract sense

1340
01:25:47,980 --> 01:25:49,850
so that's where decision

1341
01:25:49,870 --> 01:25:55,260
how do we know that i swear to they win or lose of three different

1342
01:25:55,260 --> 01:25:57,010
ways of regularized the

1343
01:25:57,030 --> 01:25:58,080
one is

1344
01:25:58,100 --> 01:26:02,340
having explicit constraints which is what we do with the lasso

1345
01:26:02,500 --> 01:26:04,410
we can do all

1346
01:26:04,430 --> 01:26:10,300
implicitly through incremental model building so that is and if they

1347
01:26:10,310 --> 01:26:16,150
epsilon stagewise for work that were built in the modernist slowly little by little daddy

1348
01:26:16,170 --> 01:26:20,330
some form of regularisation and that's going to play a key

1349
01:26:20,360 --> 01:26:24,350
in today's understanding of how boasting work

1350
01:26:25,910 --> 01:26:28,570
you can add to regularize the rule

1351
01:26:28,570 --> 01:26:33,890
a choice of they are was loss function in so doing it is my somewhere

1352
01:26:33,900 --> 01:26:36,040
like what does the lost interest

1353
01:26:36,090 --> 01:26:40,810
decision well so one of the sources of finance has to be with outliers right

1354
01:26:40,820 --> 01:26:42,710
so to the extent that you have

1355
01:26:42,710 --> 01:26:49,230
loss function that is less sensitive to outliers URI using body and and so you

1356
01:26:49,230 --> 01:26:51,800
are doing meditation

1357
01:26:51,810 --> 01:26:55,960
and then let's let's look at the picture that we had before

1358
01:26:55,970 --> 01:26:58,760
and in the realization side to it

1359
01:26:58,810 --> 01:27:00,960
so this is what we had before

1360
01:27:00,970 --> 01:27:07,370
and which is great so now what we don't is we have restricted already know

1361
01:27:07,380 --> 01:27:11,610
more space so so when increasing the bias

1362
01:27:11,670 --> 01:27:14,570
that's that's increasing bias that we took

1363
01:27:14,580 --> 01:27:16,890
so this is the motor bias and this

1364
01:27:16,900 --> 01:27:21,300
now it's what it called estimation by this

1365
01:27:21,310 --> 01:27:24,350
and so the closest to the truth

1366
01:27:24,370 --> 01:27:28,240
in the restricted model is this one right so so that's that's what i mean

1367
01:27:28,240 --> 01:27:30,890
that we have think remain

1368
01:27:30,900 --> 01:27:34,250
increase the value and so they

1369
01:27:35,030 --> 01:27:38,550
regularized feed the f had lasso

1370
01:27:38,560 --> 01:27:42,480
so remember where we were there will arise in these have had so it's going

1371
01:27:42,490 --> 01:27:47,380
to be the closest in this restricted model space to that guy

1372
01:27:48,550 --> 01:27:53,030
the right hands is going to be a lot smaller and that's that that's what

1373
01:27:53,730 --> 01:27:55,930
that's the intuition and so

1374
01:27:55,990 --> 01:27:59,060
when the increasing by us

1375
01:27:59,220 --> 01:28:03,350
or where there shown body and so we went from these by audiences what when

1376
01:28:03,350 --> 01:28:09,480
the reduction in body and it it's larger than the increasing by yes we need

1377
01:28:09,490 --> 01:28:14,580
something useful and we have a lot of water

1378
01:28:14,590 --> 01:28:16,020
OK so now

1379
01:28:16,580 --> 01:28:19,160
switch topics

1380
01:28:19,180 --> 01:28:23,510
two perhaps what you're he

1381
01:28:23,860 --> 01:28:27,160
and so

1382
01:28:27,180 --> 01:28:31,000
in the abstract we said that

1383
01:28:32,200 --> 01:28:37,200
one one of the things that we're going to talk about his importance sampling that

1384
01:28:37,200 --> 01:28:43,700
goes by the acronym years elite distance or importance sampling learning ensembles this is reason

1385
01:28:43,700 --> 01:28:44,740
worked on

1386
01:28:44,750 --> 01:28:46,210
from donut

1387
01:28:46,220 --> 01:28:52,070
by professor friedman stand for which as we say in the have of the allow

1388
01:28:52,080 --> 01:28:58,620
us to view the traditional and simply methods of bagging random forests balls on march

1389
01:28:58,850 --> 01:29:00,810
as special cases of

1390
01:29:00,840 --> 01:29:06,750
a single organism not only that it it does suggest ways of improving

1391
01:29:06,760 --> 01:29:11,430
each of these organisms so if you are currently using bagging you you you will

1392
01:29:11,430 --> 01:29:17,070
find ways to provide you know you use random forests and so on so

1393
01:29:17,090 --> 01:29:22,060
they approach it is so we're going to come from the top assume you don't

1394
01:29:22,060 --> 01:29:26,030
know anything about and symbols and you're you're going to be described is generic and

1395
01:29:26,030 --> 01:29:29,110
simple generation algorithm and then will

1396
01:29:29,150 --> 01:29:32,640
but that template next to the template the of

1397
01:29:32,700 --> 01:29:34,530
they they

1398
01:29:34,870 --> 01:29:40,720
previously existing methods and see how each of them fit is generic template

1399
01:29:41,150 --> 01:29:45,990
so there

1400
01:29:46,010 --> 01:29:50,770
the model that we have so let's start with what is the model that we're

1401
01:29:50,770 --> 01:29:51,990
so optimal

1402
01:29:53,520 --> 01:29:55,520
was the paper from two thousand two

1403
01:29:55,520 --> 01:29:57,770
sort of pretty

1404
01:29:57,790 --> 01:30:04,220
well cited paper now on that analyse this regret minimisation problem for the multi armed

1405
01:30:06,720 --> 01:30:11,020
i'm just going to tell you the algorithm is extremely simple so every moment in

1406
01:30:11,970 --> 01:30:16,370
we're going to have some statistics we've collected so we've pulled a bunch of arms

1407
01:30:16,370 --> 01:30:17,600
q way

1408
01:30:17,640 --> 01:30:21,310
is going to be the average payoff that we observe for army

1409
01:30:21,330 --> 01:30:25,910
right so we pulled arm a some number of times QA is the payoff that

1410
01:30:25,910 --> 01:30:28,100
we've observed on average

1411
01:30:28,100 --> 01:30:31,180
and a is the number of times we pulled arm

1412
01:30:32,220 --> 01:30:36,180
so we we can store these things easily

1413
01:30:36,180 --> 01:30:39,850
and now the question is what are reliable next

1414
01:30:39,890 --> 01:30:43,930
UCB for upper confidence bound is going to pull

1415
01:30:43,950 --> 01:30:45,580
this army a star

1416
01:30:45,600 --> 01:30:48,970
that maximizes this quantity

1417
01:30:49,720 --> 01:30:55,150
but about this quantity in the moment and this assumes that the payoff an zero

1418
01:30:56,720 --> 01:31:02,390
there and zero to be be square there are but this is the quantity that

1419
01:31:02,450 --> 01:31:06,520
is going to maximize going to pull that are

1420
01:31:07,290 --> 01:31:09,830
the basic theorem is that

1421
01:31:09,850 --> 01:31:16,520
the expected regret so the difference between the sum of rewards that we get if

1422
01:31:16,520 --> 01:31:19,430
we pull the over the the optimal arm

1423
01:31:19,450 --> 01:31:23,870
and the sum of rewards we get according to strategy that's the expected regret the

1424
01:31:23,870 --> 01:31:26,680
difference in the sums up expectations

1425
01:31:26,700 --> 01:31:28,790
is going to be bounded by

1426
01:31:28,810 --> 01:31:31,100
log and the number of our polls

1427
01:31:31,120 --> 01:31:36,950
this is a pretty pretty good bound in in fact is actually a lower bound

1428
01:31:37,010 --> 01:31:38,620
that matches this

1429
01:31:38,640 --> 01:31:43,140
so says that the sum of our poles that i get for my strategy compared

1430
01:31:43,140 --> 01:31:46,510
to some of our pulls if i always pulled the optimal arm is going to

1431
01:31:46,510 --> 01:31:48,890
be bounded by this particular

1432
01:31:48,910 --> 01:31:51,790
formerly over time it's not an asymptotic result

1433
01:31:51,810 --> 01:31:55,020
so if you think about what is the average

1434
01:31:55,080 --> 01:31:58,600
regret right you're going to divide this by and

1435
01:31:58,620 --> 01:32:00,140
and so on average

1436
01:32:00,140 --> 01:32:03,010
your your regret your payoff

1437
01:32:03,010 --> 01:32:06,410
it's going to be raag again divided by and away from optimal

1438
01:32:06,540 --> 01:32:10,470
so that's pretty good that goes to zero quite quickly

1439
01:32:10,470 --> 01:32:15,990
so that's the issue pretty famous result this problem was to find in the sixties

1440
01:32:15,990 --> 01:32:18,950
and it's just two thousand two that this result

1441
01:32:18,970 --> 01:32:22,560
came around to score in

1442
01:32:24,200 --> 01:32:28,450
so so those results so

1443
01:32:28,510 --> 01:32:34,520
so the original scoring rules require quite a bit of computation the the indices the

1444
01:32:34,520 --> 01:32:40,640
this requires practically no computation has simple statistics they had some later work so they

1445
01:32:40,640 --> 01:32:46,310
could prove bounds of log for those but they were asymptotic as an gets goes

1446
01:32:46,310 --> 01:32:49,950
to infinity that they had some

1447
01:32:49,970 --> 01:32:55,280
easier to compute versions and also the bounds for asymptotic this was the first found

1448
01:32:55,280 --> 01:33:00,850
that was not that this actually every single and it satisfies the property that none

1449
01:33:00,850 --> 01:33:04,060
of these are

1450
01:33:04,080 --> 01:33:08,200
i don't know about that

1451
01:33:08,250 --> 01:33:13,930
i think they have to think about that so i forget about this system animation

1452
01:33:14,290 --> 01:33:18,410
problems so let's look at this this

1453
01:33:18,430 --> 01:33:23,250
rule this is our algorithm our decision rule

1454
01:33:23,270 --> 01:33:26,470
so what it is is a value term

1455
01:33:27,620 --> 01:33:30,290
so is that have high value

1456
01:33:30,310 --> 01:33:34,100
or we're going to be more inclined to pull because the some will be larger

1457
01:33:34,330 --> 01:33:38,520
armies that have looked really bad in the past will be less inclined

1458
01:33:38,580 --> 01:33:44,040
but remember we always have to make sure we explore a summary because maybe get

1459
01:33:44,040 --> 01:33:49,810
unlucky with the optimal arm in the beginning you know a fourteen forever so we

1460
01:33:49,810 --> 01:33:53,180
have this exploration term

1461
01:33:53,180 --> 01:33:58,200
the in what this is going to do is to notice that lol again is

1462
01:33:58,200 --> 01:34:00,470
in the numerator of this term

1463
01:34:01,100 --> 01:34:03,560
remember n is the total number of poles

1464
01:34:03,580 --> 01:34:07,060
so is the total number of poles increases

1465
01:34:07,060 --> 01:34:10,490
and i say we don't pull this are so in a stays the same this

1466
01:34:10,490 --> 01:34:14,270
term is going to keep getting larger and larger and larger and eventually you're going

1467
01:34:14,270 --> 01:34:16,180
to pull that are again

1468
01:34:16,200 --> 01:34:19,310
now when you pull it again

1469
01:34:19,540 --> 01:34:24,100
this is going to increase by one it doesn't take too many samples for

1470
01:34:24,120 --> 01:34:29,290
this to sort of dominate the log and and if you pull amalie this basically

1471
01:34:29,290 --> 01:34:34,180
goes to zero so as alarms that pulled lot are basically dominated by this term

1472
01:34:34,180 --> 01:34:38,390
answer to pull the little dominated by this term in the only pull them

1473
01:34:38,410 --> 01:34:39,620
a little bit

1474
01:34:39,640 --> 01:34:42,080
now of

1475
01:34:42,390 --> 01:34:47,810
sort of the the fundamental result of that paper the following

1476
01:34:48,450 --> 01:34:51,240
the expected number of times that we're going to paul

1477
01:34:51,240 --> 01:34:53,080
the sub optimal arm

1478
01:34:53,120 --> 01:34:58,240
so so there are now going to find it in the case of the expected

1479
01:34:58,240 --> 01:35:03,810
number of times we poland a sub optimal arm after and total poles is going

1480
01:35:03,810 --> 01:35:05,660
to be bounded by this

1481
01:35:05,680 --> 01:35:10,040
so so notice first it's log n so if an arm is suboptimal

1482
01:35:10,060 --> 01:35:13,350
asymptotically and actually in every moment

1483
01:35:13,350 --> 01:35:17,520
is dominated by bloggers who are not going to pull it very much because logan

1484
01:35:17,520 --> 01:35:19,520
is a lot smaller than

1485
01:35:21,470 --> 01:35:22,890
what is this

1486
01:35:23,720 --> 01:35:27,600
is the difference between the expected value of the optimal arm

1487
01:35:27,600 --> 01:35:29,430
and our a

1488
01:35:30,140 --> 01:35:35,410
and notice if our a is really bad compared to the optimal arm

1489
01:35:35,470 --> 01:35:38,020
this term is going to be very small

1490
01:35:39,080 --> 01:35:42,930
whereas if are may is very close to optimal

1491
01:35:42,970 --> 01:35:45,160
this concert will be large

1492
01:35:45,180 --> 01:35:48,490
in some sense that's OK because it's close to optimal

1493
01:35:48,490 --> 01:35:52,600
we're happy to pull in more than an arm that is really bad and so

1494
01:35:52,600 --> 01:35:55,040
this is the fundamental result in

1495
01:35:55,040 --> 01:35:57,680
basically if you multiply this quantity by

1496
01:35:57,700 --> 01:36:01,160
this difference to get the expected regret of time

1497
01:36:01,180 --> 01:36:03,740
and so on

1498
01:36:03,770 --> 01:36:06,640
so you just get one of these and in the denominator

1499
01:36:06,700 --> 01:36:10,120
all right so

1500
01:36:10,140 --> 01:36:15,060
so that's so it doesn't waste time on bad arms sort of fundamental result from

1501
01:36:15,060 --> 01:36:16,890
the theory of multi armed

1502
01:36:16,910 --> 01:36:22,120
bandits so so now how can we apply this result

1503
01:36:22,120 --> 01:36:23,750
we have about twenty minutes

1504
01:36:23,750 --> 01:36:25,060
here because it's

1505
01:36:25,090 --> 01:36:30,280
it's the sound standard normal mean easier

1506
01:36:31,060 --> 01:36:36,040
this is the kind of processes that may represent some

1507
01:36:36,060 --> 01:36:39,450
stock markets areas like this i am IBM

1508
01:36:45,950 --> 01:36:49,680
statistical review what what's

1509
01:36:49,690 --> 01:36:53,890
interesting is trying to estimate

1510
01:36:53,910 --> 01:36:58,490
the parameters of this model here we have three parameters beta

1511
01:36:59,590 --> 01:37:01,520
and sigma

1512
01:37:01,530 --> 01:37:03,160
and observations

1513
01:37:04,200 --> 01:37:07,100
well we have about five hundred observations

1514
01:37:07,130 --> 01:37:09,580
OK so we have the whiteys from one

1515
01:37:09,600 --> 01:37:13,590
two five hundred and one to produce

1516
01:37:13,600 --> 01:37:16,470
the posterior distribution on beta

1517
01:37:17,210 --> 01:37:18,890
and sigma

1518
01:37:20,970 --> 01:37:24,760
we cannot really look directly at this posterior distribution

1519
01:37:24,790 --> 01:37:31,580
because the likelihood is this five hundred dimensional integral we need to integrate out

1520
01:37:32,730 --> 01:37:36,810
OK because of the y t

1521
01:37:36,830 --> 01:37:42,490
so the whiteys of the marginal of the joint whiteys and xt

1522
01:37:42,490 --> 01:37:45,290
so the distribution of x of y t

1523
01:37:45,300 --> 01:37:49,730
she going from one to five hundred is into world of the joint distribution of

1524
01:37:49,740 --> 01:37:51,070
y t xt

1525
01:37:51,090 --> 01:37:56,970
she going from one to five hundred so we need to integrate five hundred sixty

1526
01:37:57,670 --> 01:37:59,320
and we can do it

1527
01:37:59,340 --> 01:38:04,030
in closed form therefore what we want to do is is

1528
01:38:04,050 --> 01:38:08,300
to approximate not only the dispatcher distribution of

1529
01:38:09,820 --> 01:38:10,830
and sigma

1530
01:38:10,850 --> 01:38:14,650
but the joint posterior distribution of beta

1531
01:38:14,650 --> 01:38:20,570
phi sigma and vxt so we just plug txt is into the unknown

1532
01:38:20,600 --> 01:38:25,410
and in terms of of distributions to get the same treatment as parameters because there

1533
01:38:25,410 --> 01:38:26,140
are no

1534
01:38:26,990 --> 01:38:29,490
so now we have a distribution

1535
01:38:29,630 --> 01:38:34,150
joint commission on the expertise and why it is that that look like that this

1536
01:38:34,150 --> 01:38:38,490
is like the likelihood if you had observed also

1537
01:38:38,500 --> 01:38:39,750
next season

1538
01:38:39,770 --> 01:38:44,670
so this is usually one to cement for given the parameters

1539
01:38:44,740 --> 01:38:49,810
and we have a distribution for the distribution of the parameter so this makes the

1540
01:38:49,810 --> 01:38:54,520
distribution to simulate in dimension five hundred and three

1541
01:38:56,300 --> 01:39:00,790
that is the probability dimensional problem because we simulate loss

1542
01:39:02,740 --> 01:39:05,390
the exterior of course links

1543
01:39:05,410 --> 01:39:11,130
together so it's not like simply a five hundred points independently

1544
01:39:11,150 --> 01:39:15,440
and the shape of the distribution is not exactly standards

1545
01:39:17,560 --> 01:39:19,200
get something that is really

1546
01:39:19,200 --> 01:39:21,980
no more like but here

1547
01:39:22,040 --> 01:39:24,580
we have an exponential growth and exponential

1548
01:39:24,600 --> 01:39:25,750
that's not

1549
01:39:27,030 --> 01:39:28,750
and we need to deal with this

1550
01:39:29,790 --> 01:39:35,570
what to do to simulate this drug distribution and beta phi sigma NTXT this to

1551
01:39:35,570 --> 01:39:39,780
replace these difficulties since distribution

1552
01:39:39,800 --> 01:39:43,090
we have a simpler distribution

1553
01:39:43,100 --> 01:39:46,510
and why is usually to get

1554
01:39:47,730 --> 01:39:54,810
rate of the exponential by linear transform like this kind of terror expansion

1555
01:39:54,920 --> 01:40:00,930
and therefore we have a huge gaussian distribution of dimension five hundred to simulate

1556
01:40:00,930 --> 01:40:02,220
the xt

1557
01:40:02,240 --> 01:40:03,640
OK with some

1558
01:40:03,670 --> 01:40:05,140
mean environs that

1559
01:40:05,160 --> 01:40:07,260
we can compute

1560
01:40:11,830 --> 01:40:16,590
now we have a gaussian we can do it one term at the time

1561
01:40:16,600 --> 01:40:20,900
and we get an importance weight which is the ratio of the truth all over

1562
01:40:21,240 --> 01:40:22,990
the wrong

1563
01:40:23,010 --> 01:40:25,170
well the importance distribution

1564
01:40:25,170 --> 01:40:27,360
we have

1565
01:40:28,600 --> 01:40:31,380
because the size of the problem

1566
01:40:31,490 --> 01:40:37,040
most of the weights will be very very small because it is ground

1567
01:40:37,050 --> 01:40:39,200
we normalised of the mapping

1568
01:40:39,210 --> 01:40:41,990
the way you can see that

1569
01:40:42,020 --> 01:40:43,030
the bill

1570
01:40:43,080 --> 01:40:46,250
that correspond to the highest weight

1571
01:40:46,250 --> 01:40:51,430
it is very similar to young get something that is very nice and climbing up

1572
01:40:51,440 --> 01:40:56,040
no it's and centred at some value and you have difference of

1573
01:40:56,050 --> 01:41:01,650
of two magnitudes in log of ten ten images in logs

1574
01:41:02,880 --> 01:41:06,190
number one you take the highest

1575
01:41:09,110 --> 01:41:13,420
the sequence with the highest weight you don't necessarily get something that is very very

1576
01:41:13,420 --> 01:41:16,000
close to the troops in

1577
01:41:16,020 --> 01:41:20,510
you can see something and is the blue line is the true volatility the true

1578
01:41:20,520 --> 01:41:24,820
sequence of is and the red dotted line

1579
01:41:25,800 --> 01:41:27,310
on the simulation

1580
01:41:27,330 --> 01:41:31,570
the sequence and that got the highest weight

1581
01:41:31,590 --> 01:41:34,010
because the degeneracy of the weights

1582
01:41:35,320 --> 01:41:38,010
i have to wait doesn't necessarily mean

1583
01:41:39,410 --> 01:41:43,560
simulation close to the troops

1584
01:41:43,590 --> 01:41:46,380
OK and here is another example of the same thing

1585
01:41:46,390 --> 01:41:49,580
except that includes the range of all

1586
01:41:49,610 --> 01:41:54,740
the consequences of of simulated and you can maybe see that it this is true

1587
01:41:55,650 --> 01:41:57,030
the range

1588
01:41:58,170 --> 01:42:03,390
well if you made can be very very different from the truth

1589
01:42:04,820 --> 01:42:11,270
so this is the sampling and

1590
01:42:11,290 --> 01:42:14,290
i tried to come back

1591
01:42:14,300 --> 01:42:16,030
to this method after

1592
01:42:16,050 --> 01:42:19,670
after about MCMC methods because there is kind of

1593
01:42:19,680 --> 01:42:23,380
convergence of both methods in in more

1594
01:42:25,800 --> 01:42:29,240
the techniques that we call population monte carlo

1595
01:42:29,250 --> 01:42:31,040
which is up

1596
01:42:31,080 --> 01:42:34,930
synonym of particle filter method

1597
01:42:35,920 --> 01:42:37,160
just before

1598
01:42:37,170 --> 01:42:42,130
going to to markov chains i want to talk about a few points

1599
01:42:43,810 --> 01:42:46,750
monte carlo method

1600
01:42:47,890 --> 01:42:50,280
improving the monte-carlo methods

1601
01:42:50,300 --> 01:42:55,990
it's trying to reduce the violence that few tricks that can help with reducing the

1602
01:42:57,380 --> 01:43:04,420
and one of these traces is whenever you can produce negative correlations in your simulation

1603
01:43:04,530 --> 01:43:07,390
or rather in the age of x are

1604
01:43:07,400 --> 01:43:10,080
you're edges of our

1605
01:43:10,110 --> 01:43:11,330
OK this seems

1606
01:43:11,340 --> 01:43:14,970
so to impose correlation but actually

1607
01:43:15,390 --> 01:43:18,040
if instead

1608
01:43:18,070 --> 01:43:20,060
after producing two

1609
01:43:20,080 --> 01:43:22,460
independent samples

1610
01:43:23,250 --> 01:43:25,940
are of x i and y j

1611
01:43:25,950 --> 01:43:28,210
and using this to sample

1612
01:43:28,220 --> 01:43:33,140
cumulatively to approximate the integral i

1613
01:43:33,160 --> 01:43:35,040
if you can produce

1614
01:43:35,070 --> 01:43:36,800
correlated samples

1615
01:43:36,810 --> 01:43:38,960
so that the covariance

1616
01:43:40,250 --> 01:43:45,890
i once had and i two had is negative then you get of course is

1617
01:43:45,890 --> 01:43:49,610
more violent than using and i i d

1618
01:43:51,850 --> 01:43:53,320
now it is not

1619
01:43:53,380 --> 01:43:59,180
always obvious to realize that they should trick that if the function is symmetric

1620
01:43:59,190 --> 01:43:59,920
you can

1621
01:43:59,940 --> 01:44:02,130
use the symmetry of the symmetry

1622
01:44:02,140 --> 01:44:05,470
in function if you know for group action

1623
01:44:06,830 --> 01:44:09,080
the space you're working on

1624
01:44:09,080 --> 01:44:13,650
no no this one more thing perhaps about PCA

1625
01:44:13,670 --> 01:44:20,480
so peace is really good for presenting presenting this

1626
01:44:20,740 --> 01:44:23,010
this kind of distributions as we have here

1627
01:44:23,020 --> 01:44:28,070
which are characterized by the fact that well for example by many facts actually well

1628
01:44:28,070 --> 01:44:34,160
but basically have something like an edit for something like an ellipse that the data

1629
01:44:34,160 --> 01:44:39,450
from the club is something similar to with then some points here and there

1630
01:44:39,470 --> 01:44:44,720
so let's think of many different kinds of data sets

1631
01:44:45,030 --> 01:44:50,320
for example something like something that's a bit like a band shape

1632
01:44:50,330 --> 01:44:51,560
so you have

1633
01:44:51,900 --> 01:44:53,760
data points

1634
01:44:53,780 --> 01:44:58,660
like this

1635
01:44:58,680 --> 01:45:01,990
now if you do PC this kind of data

1636
01:45:02,040 --> 01:45:03,880
you get into trouble

1637
01:45:03,970 --> 01:45:09,670
well because i mean what is the PCA uses this kind of the coordinates well

1638
01:45:09,670 --> 01:45:14,660
let's make it even more weird the PCA doesn't work

1639
01:45:15,920 --> 01:45:19,260
so what PCA would do

1640
01:45:19,270 --> 01:45:22,840
well let's face it would give you one

1641
01:45:22,850 --> 01:45:24,820
component one linear

1642
01:45:26,240 --> 01:45:30,550
beginning from the being so let's suppose the means here

1643
01:45:30,600 --> 01:45:31,680
so then

1644
01:45:31,700 --> 01:45:39,310
the principal component might be i don't know maybe something like this

1645
01:45:40,730 --> 01:45:42,240
well the

1646
01:45:42,260 --> 01:45:48,280
certainly this single called analogous to describe your data so well not in the same

1647
01:45:48,280 --> 01:45:53,200
way as the singing composing component in in that case

1648
01:45:53,220 --> 01:45:54,670
i think

1649
01:45:55,460 --> 01:46:00,820
this may not be the best example possible but in case you see is that

1650
01:46:00,820 --> 01:46:05,960
this kind of thing is very natural for that kind of ellipse shape stuff because

1651
01:46:05,960 --> 01:46:09,750
if you have an ellipse than how to represent you can easily represented by this

1652
01:46:09,750 --> 01:46:10,960
and these

1653
01:46:11,260 --> 01:46:14,860
these two continents but for this kind of data what you actually want to do

1654
01:46:14,860 --> 01:46:20,150
is to use some kind of feeling sorry some kind of nonlinear coordinates some kind

1655
01:46:20,150 --> 01:46:24,520
of nonlinear manifolds well i mean the idea way of representing this kind of data

1656
01:46:24,830 --> 01:46:28,910
would be that you have one coordinate that goes something like this

1657
01:46:28,920 --> 01:46:34,080
actually you don't really need another coordinate because there's you have only very little noise

1658
01:46:34,080 --> 01:46:40,400
in this in the orthogonal to this one you just this one coordinate one accordance

1659
01:46:40,400 --> 01:46:45,550
with very well described it so this is the case where people would like to

1660
01:46:45,550 --> 01:46:48,950
use some kind of non linear PCM

1661
01:46:48,960 --> 01:46:53,130
this is not a case where you would like to use ICA ICA addresses completely

1662
01:46:53,130 --> 01:46:56,710
different problems as we will see later probably tomorrow

1663
01:46:56,820 --> 01:46:58,740
but this is

1664
01:46:58,760 --> 01:47:02,860
this is this is related to the linearity of BBC

1665
01:47:02,880 --> 01:47:08,300
OK maybe that's enough but PCA for the

1666
01:47:08,400 --> 01:47:11,010
and let's go

1667
01:47:15,590 --> 01:47:17,300
so short reminder of how

1668
01:47:17,320 --> 01:47:20,230
you estimate statistical models

1669
01:47:20,510 --> 01:47:24,900
in particular what is the maximum likelihood estimation

1670
01:47:24,910 --> 01:47:31,280
so what is the dimension means we have basically three different words which sound very

1671
01:47:31,280 --> 01:47:34,750
much the same but which have slightly different meanings which is

1672
01:47:34,800 --> 01:47:38,250
one the fundamental thing is destination

1673
01:47:38,310 --> 01:47:44,900
estimation can be defined in an extremely boring and abstract way in in mathematical statistics

1674
01:47:44,920 --> 01:47:50,160
but here we define it in a more intuitive manner which means that we have

1675
01:47:50,160 --> 01:47:52,480
some kind of parameters

1676
01:47:52,500 --> 01:47:59,040
something which correspond to usually one sponsor something into like for example the mean value

1677
01:47:59,040 --> 01:48:03,970
of all of the population of perhaps the standard deviation of the variance of the

1678
01:48:03,980 --> 01:48:09,240
population and now estimation means finding an approximate value for

1679
01:48:09,250 --> 01:48:14,850
that parameter when we have a random sample of of the population

1680
01:48:15,200 --> 01:48:20,290
so this the idea of of in statistical estimation is always there's some kind of

1681
01:48:20,650 --> 01:48:22,480
theoretical population

1682
01:48:22,500 --> 01:48:26,800
and we only have access to a random so it's my small sample of the

1683
01:48:27,860 --> 01:48:32,670
so this is very different from things like data miners from some some aspects of

1684
01:48:32,670 --> 01:48:36,200
data mining of some approaches to data mining where you just have you know a

1685
01:48:37,090 --> 01:48:41,640
and that's all people don't assume that it's a sample of something

1686
01:48:41,650 --> 01:48:46,750
well some people do but some people not in statistical we always have an underlying

1687
01:48:46,750 --> 01:48:53,040
population which is actually model is represented by statistical distribution of random variable are random

1688
01:48:53,910 --> 01:48:59,930
now we estimate is the concrete function or in computer science terms it might be

1689
01:48:59,930 --> 01:49:06,550
an algorithm that really gives you computes this absolute value given this sample

1690
01:49:06,760 --> 01:49:13,120
so you might also uses the floor of the sample

1691
01:49:13,130 --> 01:49:17,760
which is to be kind of signal processing terminology in in statistics when we have

1692
01:49:18,140 --> 01:49:21,880
a number of measurements that's one sample

1693
01:49:21,900 --> 01:49:25,130
but as in signal processing you would perhaps there is some people will say that

1694
01:49:25,420 --> 01:49:30,130
each of those measurements is one sample which is again a bit confusing

1695
01:49:30,190 --> 01:49:35,990
but in any case so in statistical problems so we have a sample from which

1696
01:49:35,990 --> 01:49:42,510
we compute this approximation and estimates now is the numerical value given by the estimator

1697
01:49:42,680 --> 01:49:47,460
for one particular sample so here my cause set of samples but in statistics we

1698
01:49:47,460 --> 01:49:50,830
would say that the value given by a single source

1699
01:49:50,850 --> 01:49:54,440
so basically you know like the population could be like all

1700
01:49:54,440 --> 01:49:58,500
to be represented as a very sparse long binary vector

1701
01:50:01,750 --> 01:50:06,880
if we give the query two of our favorite people alex smola virtual can't remember

1702
01:50:06,880 --> 01:50:10,310
these are newspapers eighty seven ninety nine

1703
01:50:10,310 --> 01:50:11,750
things may have changed

1704
01:50:11,790 --> 01:50:17,940
by the party members that there are the following people including you know more paper

1705
01:50:17,940 --> 01:50:19,460
people like and are

1706
01:50:19,500 --> 01:50:26,480
and the top words associated with that that are the words back to support colonel

1707
01:50:26,960 --> 01:50:29,570
have no idea what pages here

1708
01:50:29,590 --> 01:50:32,210
maybe there are lots of pages

1709
01:50:32,250 --> 01:50:38,310
machine quadratic of regularisation minimizing

1710
01:50:38,310 --> 01:50:44,560
so if we give a large following from the accolade we're you probabilistic models around

1711
01:50:44,560 --> 01:50:45,790
the same time

1712
01:50:45,830 --> 01:50:50,520
here's the list of people that we get other people working on probabilistic model

1713
01:50:50,570 --> 01:50:58,480
and the top words are log likelihood models mixture conditional

1714
01:50:59,360 --> 01:51:06,060
and during in red button we're working on a lot of reinforcement learning and decision

1715
01:51:06,060 --> 01:51:07,380
making models

1716
01:51:07,430 --> 01:51:15,630
we get other people report learning and decision making and these are the top words

1717
01:51:15,650 --> 01:51:20,650
here's here's something i would actually find really useful if somebody provided this service based

1718
01:51:20,650 --> 01:51:21,520
on the

1719
01:51:21,540 --> 01:51:25,130
we haven't done it but you know the next if somebody did

1720
01:51:25,170 --> 01:51:27,980
often we we're doing the literature search

1721
01:51:28,940 --> 01:51:33,500
we may have a couple of examples of papers we know are on the topic

1722
01:51:33,500 --> 01:51:34,840
we're interested in

1723
01:51:34,860 --> 01:51:38,920
and what we really want to do is we want to find other papers

1724
01:51:39,840 --> 01:51:43,440
that are somehow on the same topic so instead of typing in keywords which is

1725
01:51:43,440 --> 01:51:44,650
what we had to do

1726
01:51:44,650 --> 01:51:48,440
which seems incredibly stupid if i have to incredibly

1727
01:51:48,460 --> 01:51:50,750
big rich document

1728
01:51:50,770 --> 01:51:54,020
why would i have to come up with keyword to try to represent what can

1729
01:51:54,020 --> 01:51:57,380
i give my two documents as the query

1730
01:51:58,170 --> 01:52:02,960
so here's what we did we had a system for searching literature where

1731
01:52:02,980 --> 01:52:04,230
our query

1732
01:52:04,230 --> 01:52:05,810
it was not just the

1733
01:52:05,840 --> 01:52:07,860
title here we have the title

1734
01:52:07,880 --> 01:52:10,110
and the authors of the

1735
01:52:10,840 --> 01:52:16,210
in this paper but query whether fact a representation of the whole document in terms

1736
01:52:16,210 --> 01:52:17,400
of the

1737
01:52:17,560 --> 01:52:22,810
in this case the bag of words representation of the document in terms of words

1738
01:52:22,920 --> 01:52:28,880
OK but we binarize the bag of words so that represents each document by overabundance

1739
01:52:28,880 --> 01:52:31,230
of certain words

1740
01:52:31,230 --> 01:52:34,610
and then the results are other documents

1741
01:52:35,110 --> 01:52:41,790
the interesting thing is that both of these documents were are about gas prices are

1742
01:52:41,810 --> 01:52:46,360
both papers about casting process these all these papers are about casting processes

1743
01:52:46,440 --> 01:52:50,250
including for example this one computing with infinite network

1744
01:52:50,250 --> 01:52:52,790
which if you just look at the title the

1745
01:52:52,840 --> 01:52:57,650
you know mentioned but if you look at the actual at the cutting process is

1746
01:52:59,150 --> 01:53:02,060
at the top words are

1747
01:53:03,040 --> 01:53:05,230
yes the process related work

1748
01:53:05,230 --> 01:53:07,330
chris williams

1749
01:53:11,630 --> 01:53:15,420
OK i have to take a look at that the second thing except

1750
01:53:15,440 --> 01:53:18,270
and it's not in the light of that but you have to take a look

1751
01:53:18,270 --> 01:53:21,940
at that

1752
01:53:25,150 --> 01:53:29,420
and query times are very fast and this is just running on the left

1753
01:53:29,440 --> 01:53:32,610
you know different have that with

1754
01:53:32,840 --> 01:53:38,940
you know matrices there are thirty thousand by two thousand doing the really naive matlab

1755
01:53:38,940 --> 01:53:44,880
implementation each query takes about the second process or less if you didn't mark level

1756
01:53:44,880 --> 01:53:46,770
clever implementation

1757
01:53:46,790 --> 01:53:50,840
i am sure it it can be sped up by in fact we know how

1758
01:53:50,840 --> 01:53:53,360
to speed up by even more than that

1759
01:53:53,360 --> 01:53:56,630
so we can use this to be content based image retrieval let me show you

1760
01:53:56,690 --> 01:53:58,920
how to based image retrieval and that

1761
01:53:58,920 --> 01:54:02,270
and try to wrap up and we can take a little break

1762
01:54:04,250 --> 01:54:06,630
in our content based image retrieval

1763
01:54:09,210 --> 01:54:12,480
we have a we have the following scenario

1764
01:54:12,710 --> 01:54:16,440
we want to search a large collection of unlabeled images

1765
01:54:16,480 --> 01:54:19,540
we imagine entering the word

1766
01:54:20,460 --> 01:54:23,400
we can also hear subset of images

1767
01:54:24,330 --> 01:54:27,880
and the basic idea of entering the word is that we have some small set

1768
01:54:27,900 --> 01:54:29,420
of labeled images

1769
01:54:29,460 --> 01:54:32,710
so when we enter the word penguins what we can do is we can

1770
01:54:32,770 --> 01:54:37,060
find the labelled images that are labeled penguins

1771
01:54:37,110 --> 01:54:41,690
and present that as a query set q this is the

1772
01:54:41,750 --> 01:54:43,310
that's going to retrieve

1773
01:54:43,360 --> 01:54:45,630
unlabeled images

1774
01:54:45,630 --> 01:54:48,940
the query set is the set of images

1775
01:54:48,960 --> 01:54:51,290
all of which have the label penguins

1776
01:54:51,360 --> 01:54:56,190
and we're going into the data set of unlabeled images

1777
01:54:56,210 --> 01:54:59,920
and finding other images that match the

1778
01:54:59,920 --> 01:55:03,690
purely content based visual features

1779
01:55:03,710 --> 01:55:05,060
so we have here

1780
01:55:05,070 --> 01:55:11,750
label that ten thousand images from unlabelled set of twenty two thousand images and about

1781
01:55:11,750 --> 01:55:15,060
two thousand different keywords that this can

1782
01:55:15,150 --> 01:55:20,560
can run on this many keywords you have but we have about two thousand

1783
01:55:20,570 --> 01:55:25,790
so the features that we use are incredibly simple and stupid there's things like

1784
01:55:25,840 --> 01:55:27,750
texture features

1785
01:55:27,770 --> 01:55:34,730
i like for texture features these things are more features and just global color histogram

1786
01:55:35,840 --> 01:55:39,000
saturation value we we

1787
01:55:39,210 --> 01:55:43,630
this implies that we represent the probability that a feature

1788
01:55:43,650 --> 01:55:47,190
and we binarize our data by three feature we

1789
01:55:47,250 --> 01:55:52,650
take a continuous feature we compute the cube that feature we take the

1790
01:55:52,730 --> 01:55:58,020
the heavier tail that every giver one value with some of the heavier tail and

1791
01:55:58,130 --> 01:56:00,060
zero value otherwise

1792
01:56:00,110 --> 01:56:06,770
so we end up with the two hundred forty binary features very different

1793
01:56:06,790 --> 01:56:11,520
yeah it's pretty stupid but you'll see it a can actually work quite well

1794
01:56:11,560 --> 01:56:17,560
the texture features are a little more clever clever in that we take the image

1795
01:56:17,670 --> 01:56:19,730
it's still pretty embarrassingly

1796
01:56:19,730 --> 01:56:25,250
so we get divided into quadrants we compute texture features and each of the

1797
01:56:25,270 --> 01:56:26,770
o actually nine

1798
01:56:26,770 --> 01:56:30,290
the following content is provided under creative commons license

1799
01:56:30,310 --> 01:56:36,140
your support will help MIT opencourseware continue to offer high quality educational resources for free

1800
01:56:36,140 --> 01:56:40,700
to make a donation or to view additional materials from hundreds of MIT courses visit

1801
01:56:40,700 --> 01:56:45,120
MIT opencourseware OCW that MIT that EDU

1802
01:56:48,970 --> 01:56:51,640
again welcome to eighteen o one

1803
01:56:51,650 --> 01:56:53,620
we're getting started today

1804
01:56:54,820 --> 01:56:56,440
what we're calling

1805
01:56:56,490 --> 01:56:57,520
unit one

1806
01:56:57,530 --> 01:56:59,310
highly imaginative

1807
01:56:59,350 --> 01:57:02,310
topic i highly imaginative titles

1808
01:57:09,710 --> 01:57:12,630
so let me first tell you briefly

1809
01:57:12,680 --> 01:57:16,070
what's in store in the next couple of weeks

1810
01:57:16,080 --> 01:57:18,180
the main topic today

1811
01:57:19,230 --> 01:57:20,610
what is

1812
01:57:20,660 --> 01:57:23,280
a derivative

1813
01:57:30,180 --> 01:57:34,960
we're going to

1814
01:57:34,970 --> 01:57:37,930
look at this from several different points of view

1815
01:57:37,980 --> 01:57:42,050
and the first one is the geometric interpretation

1816
01:57:42,110 --> 01:57:48,330
and that's what will spend most of today on

1817
01:57:51,170 --> 01:57:53,320
we'll also talk about physical

1818
01:57:55,590 --> 01:58:03,980
i what derivative is

1819
01:58:09,170 --> 01:58:11,680
there's going to be something else which

1820
01:58:11,700 --> 01:58:15,460
i guess is maybe the reason why calculus is so fundamental and why we always

1821
01:58:15,460 --> 01:58:16,790
start with it

1822
01:58:18,030 --> 01:58:23,480
in most science and engineering schools which is the important

1823
01:58:24,400 --> 01:58:26,930
of derivatives

1824
01:58:26,940 --> 01:58:29,370
the of this too

1825
01:58:29,420 --> 01:58:34,360
all measurements

1826
01:58:34,460 --> 01:58:39,720
so that means pretty much every place that means in science

1827
01:58:39,770 --> 01:58:41,160
in engineering

1828
01:58:41,310 --> 01:58:43,830
in economics

1829
01:58:43,880 --> 01:58:47,500
in political science

1830
01:58:47,520 --> 01:58:52,370
federer polling

1831
01:58:52,430 --> 01:58:56,290
lots of commercial applications just just about everything

1832
01:58:57,420 --> 01:58:59,780
so that's what we'll be getting started with

1833
01:58:59,790 --> 01:59:00,560
and then

1834
01:59:00,610 --> 01:59:01,830
there's another

1835
01:59:01,850 --> 01:59:03,290
o thing that we're going to do

1836
01:59:03,330 --> 01:59:05,590
in this unit

1837
01:59:05,600 --> 01:59:06,950
which is

1838
01:59:06,960 --> 01:59:09,430
we're going to explain how

1839
01:59:09,440 --> 01:59:12,400
to differentiate any

1840
01:59:15,480 --> 01:59:20,840
so how to differentiate any function you know

1841
01:59:28,200 --> 01:59:30,650
and that's kind of tall order

1842
01:59:30,670 --> 01:59:32,740
but let me just give you an example

1843
01:59:32,790 --> 01:59:36,550
if you want to take the derivative this will see today is the notation for

1844
01:59:36,550 --> 01:59:37,920
the derivative something

1845
01:59:37,970 --> 01:59:43,570
of some messy function like e to the x arctan

1846
01:59:46,330 --> 01:59:47,890
i will work this out

1847
01:59:48,000 --> 01:59:49,930
by the

1848
01:59:49,950 --> 01:59:52,440
and of this unit

1849
01:59:52,450 --> 01:59:55,780
right so anything you can think of anything you can write down

1850
01:59:55,830 --> 01:59:58,890
we can differentiate

1851
01:59:58,940 --> 02:00:01,250
right so that's what we're going to do

1852
02:00:03,150 --> 02:00:04,460
as i said

1853
02:00:04,510 --> 02:00:06,420
we're going to spend most of our time

1854
02:00:06,440 --> 02:00:07,330
on this

1855
02:00:07,400 --> 02:00:11,330
geometric interpretation

1856
02:00:11,340 --> 02:00:16,770
so let's let's begin with that

1857
02:00:16,790 --> 02:00:20,190
so here we go with the geometric

1858
02:00:30,460 --> 02:00:35,510
what we're going to do

1859
02:00:35,530 --> 02:00:39,460
is just as the geometric problems finding

1860
02:00:39,500 --> 02:00:42,980
the tangent line

1861
02:00:51,440 --> 02:00:54,060
graph of some function

1862
02:00:54,110 --> 02:00:55,860
at some point

1863
02:00:55,880 --> 02:01:00,080
which i think you're right

1864
02:01:00,140 --> 02:01:01,920
that's the problem

1865
02:01:01,970 --> 02:01:04,330
they were addressing here

1866
02:01:04,680 --> 02:01:07,250
the station

