1
00:00:00,000 --> 00:00:04,790
including the amplitude a including the phase angle phi

2
00:00:06,560 --> 00:00:08,950
that first calculate omega

3
00:00:08,990 --> 00:00:11,360
that is the square root of k over n

4
00:00:11,470 --> 00:00:15,500
that will be ten radiance per second

5
00:00:16,780 --> 00:00:18,730
the period t

6
00:00:18,740 --> 00:00:21,310
which is two pi divided by omega

7
00:00:21,370 --> 00:00:23,920
it would be roughly

8
00:00:23,960 --> 00:00:25,880
six point

9
00:00:25,930 --> 00:00:27,870
two eight seconds

10
00:00:27,890 --> 00:00:30,240
and the frequency f

11
00:00:30,290 --> 00:00:31,520
would be about

12
00:00:31,570 --> 00:00:33,420
o point one six

13
00:00:33,450 --> 00:00:36,130
just to get

14
00:00:36,220 --> 00:00:37,280
some numbers

15
00:00:37,330 --> 00:00:39,760
one point six sort

16
00:00:41,250 --> 00:00:42,390
this is not my

17
00:00:42,440 --> 00:00:44,770
they this is o point

18
00:00:44,820 --> 00:00:45,980
six to eight

19
00:00:46,020 --> 00:00:47,930
and this is one point

20
00:00:47,980 --> 00:00:50,750
six words

21
00:00:50,850 --> 00:00:51,880
two pi

22
00:00:51,900 --> 00:00:53,780
divided omega you can see

23
00:00:53,780 --> 00:00:57,930
this is then six divided by ten is about o point six

24
00:00:57,940 --> 00:00:59,420
all right so now

25
00:00:59,430 --> 00:01:01,860
i know that equals zero

26
00:01:01,880 --> 00:01:03,800
x equals zero

27
00:01:03,880 --> 00:01:06,110
so i see my solution right there

28
00:01:06,120 --> 00:01:07,920
right here

29
00:01:07,920 --> 00:01:09,780
i put it equal zero

30
00:01:09,790 --> 00:01:11,700
and in order to access zero

31
00:01:11,750 --> 00:01:13,280
so i get zero

32
00:01:13,290 --> 00:01:14,570
he calls eight

33
00:01:14,600 --> 00:01:16,170
times the cosine

34
00:01:16,210 --> 00:01:18,300
o five

35
00:01:19,690 --> 00:01:20,940
a is zero

36
00:01:20,980 --> 00:01:24,340
if i really the thing at equilibrium and i give the velocity of three meters

37
00:01:24,340 --> 00:01:27,220
per second is going to oscillate so a is not zero

38
00:01:27,240 --> 00:01:30,030
so the only solution is the cosine phi is zero

39
00:01:30,050 --> 00:01:31,890
so that leaves me with

40
00:01:31,940 --> 00:01:33,650
five by over two

41
00:01:34,570 --> 00:01:35,450
three by

42
00:01:35,450 --> 00:01:37,250
o five three by over

43
00:01:37,970 --> 00:01:39,650
the only two possibilities

44
00:01:39,740 --> 00:01:41,650
now i go to my next

45
00:01:41,680 --> 00:01:44,840
initial condition that the velocities minus three

46
00:01:44,900 --> 00:01:47,480
here you see equation for the velocity

47
00:01:47,520 --> 00:01:49,520
this is my three

48
00:01:49,670 --> 00:01:51,240
people zero

49
00:01:51,240 --> 00:01:53,260
so my story

50
00:01:54,150 --> 00:01:56,920
mine is a in use

51
00:01:56,920 --> 00:01:57,860
we don't know yet

52
00:01:57,970 --> 00:01:59,870
i is a

53
00:01:59,880 --> 00:02:02,570
and there we have only just created

54
00:02:02,590 --> 00:02:05,490
omega story which is ten

55
00:02:05,530 --> 00:02:06,640
he is zero

56
00:02:06,650 --> 00:02:10,000
i get the sign of phi

57
00:02:11,340 --> 00:02:12,830
if i by over two

58
00:02:12,840 --> 00:02:14,850
the sign of phi is one

59
00:02:14,970 --> 00:02:20,790
so you find immediately that a course plus o point three

60
00:02:20,850 --> 00:02:22,500
so the solution now

61
00:02:22,500 --> 00:02:24,720
which includes now a

62
00:02:24,760 --> 00:02:26,180
is that x

63
00:02:26,190 --> 00:02:28,610
he calls plus o point three

64
00:02:28,720 --> 00:02:30,850
the cosine

65
00:02:32,450 --> 00:02:33,790
which is ten t

66
00:02:34,290 --> 00:02:35,100
plus y

67
00:02:35,110 --> 00:02:37,230
over two

68
00:02:37,230 --> 00:02:38,380
so you see that the

69
00:02:38,510 --> 00:02:40,260
initial conditions

70
00:02:40,270 --> 00:02:44,560
but the conditions i t people zero they determine my age and it's human my

71
00:02:44,600 --> 00:02:45,530
face angle

72
00:02:45,540 --> 00:02:47,350
if you had chosen

73
00:02:47,440 --> 00:02:50,780
this as the phase angle three by over two

74
00:02:51,530 --> 00:02:52,900
that would be fine

75
00:02:52,950 --> 00:02:56,280
you would have found a minus sign here and that's exactly the same

76
00:02:56,290 --> 00:02:57,390
so you would have found

77
00:03:02,080 --> 00:03:04,580
i want to demonstrate to you that the

78
00:03:04,590 --> 00:03:07,960
period of oscillations nonintuitive as that may be

79
00:03:08,000 --> 00:03:09,230
is independent

80
00:03:09,240 --> 00:03:12,430
of the

81
00:03:12,470 --> 00:03:14,700
amplitude that i give the object

82
00:03:14,790 --> 00:03:17,800
and i want to do that here was this track

83
00:03:17,810 --> 00:03:20,820
i have a an object here

84
00:03:20,870 --> 00:03:24,500
this object has a mass

85
00:03:24,500 --> 00:03:26,500
one hundred eighty

86
00:03:26,540 --> 00:03:31,960
six plus or minus one gram

87
00:03:31,960 --> 00:03:35,060
court and one

88
00:03:35,090 --> 00:03:40,090
i'm going to also made it

89
00:03:40,100 --> 00:03:41,960
every going to measure the period

90
00:03:41,990 --> 00:03:46,690
but instead of measuring long period i'm going to measure ten periods because that gives

91
00:03:46,690 --> 00:03:48,070
me a smaller

92
00:03:48,130 --> 00:03:51,530
uncertainty is small relative error in my measurements

93
00:03:51,570 --> 00:03:54,020
i'm going to do it as an amplitude

94
00:03:54,060 --> 00:03:56,780
which is fifteen centimetres let's make twenty

95
00:03:58,620 --> 00:04:00,380
so i get ten t

96
00:04:00,430 --> 00:04:02,090
get a certain number

97
00:04:02,110 --> 00:04:04,510
and i get an error which is my reaction error

98
00:04:04,520 --> 00:04:07,200
which is about a tenth of a second

99
00:04:07,260 --> 00:04:12,420
that's about the reaction that we all have roughly then i'll do it forty centimeters

100
00:04:12,450 --> 00:04:13,790
we get ten t

101
00:04:13,850 --> 00:04:16,300
and we get again

102
00:04:16,350 --> 00:04:20,360
plus and minus one point o point one second

103
00:04:20,400 --> 00:04:21,970
and we'll see

104
00:04:22,000 --> 00:04:23,120
how much they differ

105
00:04:23,180 --> 00:04:25,830
they should be the same if this is an ideal spring

106
00:04:25,890 --> 00:04:28,390
within the uncertainty of my measurements

107
00:04:28,440 --> 00:04:31,250
we see defining there

108
00:04:31,300 --> 00:04:34,010
i'm going to give it a twenty centimeter offset which is

109
00:04:35,070 --> 00:04:38,060
and then i will started when it comes back here so i will allow it

110
00:04:38,060 --> 00:04:39,750
one oscillation first

111
00:04:39,760 --> 00:04:42,060
it's easier for me to see it stands still

112
00:04:42,080 --> 00:04:43,180
when i started

113
00:04:43,180 --> 00:04:44,540
there we go

114
00:05:01,730 --> 00:05:04,140
what we see

115
00:05:04,200 --> 00:05:08,490
fifteen point one six

116
00:05:10,010 --> 00:05:13,110
o point one six and by the way you can derive the spring constant from

117
00:05:13,110 --> 00:05:13,990
this now

118
00:05:14,040 --> 00:05:15,340
because you know to mass

119
00:05:15,360 --> 00:05:17,110
you know the time

120
00:05:17,120 --> 00:05:22,810
not going to give you the displacement amplitude which is twice as high

121
00:05:22,850 --> 00:05:25,110
so i think it forty centimeters

122
00:05:25,150 --> 00:05:27,060
but ten

123
00:05:27,070 --> 00:05:31,210
forty centimeters huge displacement

124
00:05:48,100 --> 00:05:53,150
fifteen forty one three

125
00:05:53,230 --> 00:05:58,540
fantastic agreement within the uncertainty of my measurements

126
00:05:58,560 --> 00:06:00,540
there three hundred and the second

127
00:06:00,580 --> 00:06:04,190
of course if you tried many times you will always get that close because my

128
00:06:04,190 --> 00:06:08,360
reaction time is really not much better than the tenth of the second

129
00:06:09,850 --> 00:06:11,970
i'll show you something else

130
00:06:11,970 --> 00:06:13,790
which is quite interesting and that is

131
00:06:14,390 --> 00:06:16,130
the behaviour of the period is

132
00:06:16,140 --> 00:06:17,310
on the

133
00:06:17,320 --> 00:06:20,290
on the mass of the object

134
00:06:20,290 --> 00:06:22,700
i have here another car

135
00:06:22,720 --> 00:06:25,020
which weighs roughly the same

136
00:06:25,080 --> 00:06:27,540
i'm going to add the two together

137
00:06:27,580 --> 00:06:29,420
and so we get an two

138
00:06:29,420 --> 00:06:31,700
it's about three hundred seventy two

139
00:06:31,810 --> 00:06:35,690
one minus one grand plus and minus one comes in because our scale is no

140
00:06:35,690 --> 00:06:38,740
more accurate than one gram so we put them both on the scale

141
00:06:38,750 --> 00:06:40,570
and we find this to be

142
00:06:40,590 --> 00:06:42,620
the uncertainty

143
00:06:42,710 --> 00:06:43,730
so no

144
00:06:43,740 --> 00:06:47,200
i'm going to measure the ten periods

145
00:06:47,310 --> 00:06:48,780
of this

146
00:06:48,780 --> 00:06:55,180
keep moving an equal and opposite directions because of the conservation of momentum

147
00:06:56,320 --> 00:07:02,280
other leptons the mu on and time you want or talent and i tell

148
00:07:02,290 --> 00:07:05,150
and that happens roughly half the time

149
00:07:05,190 --> 00:07:09,190
or roughly one percent of that have

150
00:07:11,290 --> 00:07:13,230
something else happens

151
00:07:13,310 --> 00:07:14,570
namely you have

152
00:07:14,580 --> 00:07:16,600
the electron and positron

153
00:07:16,630 --> 00:07:20,310
or the new unknown at time yuan or the talent and i tell accompanied by

154
00:07:20,310 --> 00:07:22,670
photon another bit of radiation

155
00:07:26,200 --> 00:07:31,980
that's the body and soul of quantum electrodynamics you see charged particles

156
00:07:32,770 --> 00:07:35,390
the things that photons respond to

157
00:07:35,400 --> 00:07:38,310
photons respond to them they get radiated

158
00:07:38,350 --> 00:07:43,590
and by consistent application just knowing the basic interaction

159
00:07:43,610 --> 00:07:46,590
that basic act of radiation

160
00:07:46,650 --> 00:07:51,570
and by consistently applying the rules of quantum mechanics and special relativity

161
00:07:51,640 --> 00:07:54,730
you can build up all of quantum electrodynamics

162
00:07:54,840 --> 00:07:56,920
and you can test that you've got it right

163
00:07:56,960 --> 00:08:02,400
by looking to see that the number of photons radiated at different energies that the

164
00:08:02,440 --> 00:08:08,520
angular distribution the antenna pattern has to study many many events is consistently described by

165
00:08:08,520 --> 00:08:09,750
the fundamental

166
00:08:09,770 --> 00:08:16,770
vertex that you get from the symmetry and structure of quantum electrodynamics

167
00:08:16,780 --> 00:08:20,750
the other half the time when you don't see electrons and photons what you see

168
00:08:20,750 --> 00:08:22,430
is more like this

169
00:08:22,450 --> 00:08:25,010
that is you see sprays of particles

170
00:08:25,050 --> 00:08:26,140
now actually

171
00:08:26,150 --> 00:08:27,810
more common than these

172
00:08:27,820 --> 00:08:31,440
events we see three sprays are events where you see two sprays

173
00:08:31,450 --> 00:08:36,170
going in opposite directions

174
00:08:36,220 --> 00:08:40,170
those occur roughly half the time but ten percent of that half roughly you see

175
00:08:40,170 --> 00:08:41,700
instead three sprays

176
00:08:41,760 --> 00:08:46,480
well this sounds like a story very much like the story i just told you

177
00:08:46,560 --> 00:08:51,500
about quantum electrodynamics but now instead of having a

178
00:08:51,540 --> 00:08:54,760
single particles and particles that are

179
00:08:54,820 --> 00:09:00,650
leptons and photons you see sprays of particles and these sprays when you look into

180
00:09:00,650 --> 00:09:03,860
them consists of strongly interacting particles like

181
00:09:03,890 --> 00:09:06,910
protons neutrons which we're trying to get at

182
00:09:06,960 --> 00:09:13,930
rho mesons other mesons and their antiparticles all these strongly interacting particles that are

183
00:09:15,440 --> 00:09:16,930
produced abundantly

184
00:09:16,930 --> 00:09:19,300
that accelerators

185
00:09:22,280 --> 00:09:24,500
just looking at these patterns

186
00:09:24,520 --> 00:09:29,230
you be tempted to make a direct analogy between

187
00:09:29,280 --> 00:09:34,740
this is the well known structures of quantum electrodynamics and the structures that you're seeing

188
00:09:35,570 --> 00:09:40,600
in the strong in this class of events that are strongly interacting particles

189
00:09:40,660 --> 00:09:44,080
if you squint a little or if you're of a certain age and just take

190
00:09:44,080 --> 00:09:49,650
off his glasses you would have a hard time distinguishing them

191
00:09:49,700 --> 00:09:50,570
and so

192
00:09:50,590 --> 00:09:52,960
one would be tempted

193
00:09:52,960 --> 00:09:54,440
and it turns out to be

194
00:09:55,880 --> 00:09:58,770
consistency cheque which i mentioned in the moment

195
00:09:58,810 --> 00:10:02,280
one would be tempted to identify

196
00:10:02,950 --> 00:10:07,580
jets if you knew about quantum electrodynamics as the quarks and gluons

197
00:10:08,920 --> 00:10:12,770
of course they can be literally identified with the quarks and gluons because for one

198
00:10:12,770 --> 00:10:15,050
thing the not single particles

199
00:10:15,100 --> 00:10:18,870
but we can if we are willing to compromise a little bit and say

200
00:10:18,880 --> 00:10:23,000
it's not the quarks and gluons but it has the energy and momentum of quarks

201
00:10:23,000 --> 00:10:24,120
and gluons

202
00:10:24,130 --> 00:10:26,740
then we have a testable hypothesis

203
00:10:26,740 --> 00:10:31,880
because then we can test which is similar to the two in electrodynamics whether the

204
00:10:31,880 --> 00:10:35,990
antenna patterns as a function of energy is a function of angle

205
00:10:36,010 --> 00:10:40,960
we can also see events with four jet occasionally ten percent of the ten percent

206
00:10:42,330 --> 00:10:44,090
we have even an extra

207
00:10:44,100 --> 00:10:46,900
radiation event four

208
00:10:48,190 --> 00:10:54,640
we can test and all details the basic fundamental interaction between quarks and gluons which

209
00:10:54,640 --> 00:10:58,340
from which the whole theory can be built up

210
00:10:58,400 --> 00:11:01,860
so it's in that sense that i said so we see the body of

211
00:11:02,630 --> 00:11:04,500
the body of

212
00:11:04,510 --> 00:11:05,750
but qcd

213
00:11:05,770 --> 00:11:08,650
we see the quarks and gluons and

214
00:11:08,700 --> 00:11:12,960
their joint so to speak the limbs and joints because we can also see how

215
00:11:12,960 --> 00:11:16,150
they interact and that's sufficient to

216
00:11:16,250 --> 00:11:23,170
build up the entire theory in principle just using quantum mechanics and special relativity

217
00:11:24,890 --> 00:11:29,910
consistency checks work the distribution is right the number of particles is function of energy

218
00:11:29,910 --> 00:11:30,870
is right

219
00:11:31,120 --> 00:11:34,470
as dictated by that thought

220
00:11:36,370 --> 00:11:42,190
there's the missing ingredient was all the profane here that is identifying the energy and

221
00:11:43,600 --> 00:11:47,530
carried by these conglomerates of particles with

222
00:11:48,230 --> 00:11:52,560
energy and momentum of some underlying structure that you don't actually see

223
00:11:52,590 --> 00:11:57,160
the quarks and gluons how can you justify that if they really if the quarks

224
00:11:57,160 --> 00:12:01,850
and gluons shopping energy why don't they show up as individual particles why they choose

225
00:12:01,850 --> 00:12:04,750
to show up in this peculiar way

226
00:12:04,750 --> 00:12:08,110
and that's where the soul of QCD comes in

227
00:12:08,140 --> 00:12:13,260
so this is the body but there's also the sole which is how you breathe

228
00:12:13,260 --> 00:12:16,720
from quarks breathe life into these many jets

229
00:12:16,730 --> 00:12:21,120
many particles that make up jets and that's where this property of asymptotic freedom that

230
00:12:21,120 --> 00:12:25,140
mark so kindly explained comes in

231
00:12:25,640 --> 00:12:28,550
asymptotic freedom directly in the phenomenon

232
00:12:28,590 --> 00:12:30,310
is simply the fact

233
00:12:31,210 --> 00:12:33,810
radiation events

234
00:12:35,080 --> 00:12:40,780
involved only small transfers of energy and momentum so don't disturb the overall flow of

235
00:12:40,780 --> 00:12:43,620
energy momentum are quite common

236
00:12:44,910 --> 00:12:51,440
radiation events like this radiation of an extra gluon in substantially different directions which involves

237
00:12:52,520 --> 00:12:56,640
transfers of energy and momentum are rare

238
00:12:57,230 --> 00:13:01,030
then we have our cake and eat it too namely we can see

239
00:13:01,050 --> 00:13:02,070
why there are

240
00:13:02,490 --> 00:13:07,320
occasionally what we can see that there are radiation events that are discernible only have

241
00:13:07,350 --> 00:13:10,610
large disturbances of the flow of energy momentum

242
00:13:10,630 --> 00:13:11,720
and yet

243
00:13:11,730 --> 00:13:13,760
each time we have such a thing

244
00:13:14,070 --> 00:13:17,320
the individual quarks and gluons

245
00:13:17,360 --> 00:13:22,590
radiate softly that is how radiation events that don't change the overall flow of energy

246
00:13:24,700 --> 00:13:26,260
and that produces

247
00:13:26,270 --> 00:13:29,780
that is the way you go from a single particle to the observed pattern of

248
00:13:29,780 --> 00:13:34,130
many particles in the jet

249
00:13:35,830 --> 00:13:41,020
that is calculable that property of asymptotic freedom is calculable property

250
00:13:41,110 --> 00:13:46,030
of the fundamental theory of QCD it involves calculating the action of

251
00:13:46,040 --> 00:13:52,390
the of what we call empty space the virtual particles and reacting to charge and

252
00:13:53,590 --> 00:13:58,060
charge the talk about that little bit more detail later

253
00:13:59,630 --> 00:14:03,900
since the calculable quantity we can calculate exactly how it

254
00:14:04,010 --> 00:14:08,530
turns on that is how the effective charge the

255
00:14:08,540 --> 00:14:14,050
the ability of particles to radiate gets smaller and smaller as the amount of energy

256
00:14:14,050 --> 00:14:18,810
momentum that has to be is conveyed in the radiation gets larger and larger

257
00:14:19,600 --> 00:14:23,950
by literally by studying such jet events one can start to trace out

258
00:14:23,950 --> 00:14:25,200
so for every

259
00:14:25,220 --> 00:14:30,160
covariate acts i input actually going to have a vector of outputs vector of responses

260
00:14:30,160 --> 00:14:33,010
that's multivariate regression

261
00:14:33,010 --> 00:14:38,110
OK we're going to formulate this problem as the second order cone problem an SOCP

262
00:14:38,160 --> 00:14:43,260
compare that to the last which is just l one shrinkage l one regression

263
00:14:43,280 --> 00:14:46,680
done independently on every one of the response components

264
00:14:46,720 --> 00:14:49,110
see if we can get any benefits from them

265
00:14:49,150 --> 00:14:55,610
the SOCP which cost more computationally we also compare a couple of different procedures get

266
00:14:55,660 --> 00:14:56,820
cheap method

267
00:14:56,840 --> 00:15:03,260
and then a computationally more expensive method sitting in difference in rates here

268
00:15:03,280 --> 00:15:06,910
OK so this is just the classical set up of the last so

269
00:15:06,950 --> 00:15:08,660
how many other kind of procedures

270
00:15:09,110 --> 00:15:13,700
dancing selector and so on and so forth gives you neither aggression there's the design

271
00:15:13,700 --> 00:15:16,720
matrix is the parameter vector noise perhaps

272
00:15:16,740 --> 00:15:21,880
and you know if you need to the usual approach is the regularized least squares

273
00:15:21,880 --> 00:15:24,090
problem quadratic programme

274
00:15:24,130 --> 00:15:28,240
they have a square error term and they have a regularizer

275
00:15:28,240 --> 00:15:33,470
and l two regularizer is called ridge regression l one

276
00:15:33,490 --> 00:15:39,970
is is l one l zero which is the count of the number of

277
00:15:39,990 --> 00:15:43,280
components which are nonzero that leads to an NP hard problem

278
00:15:43,300 --> 00:15:47,990
and then there's also no attempts to look at sort of convex nonconvex problems where

279
00:15:49,200 --> 00:15:53,910
the exponent is between zero and one might say the l one has been focused

280
00:15:53,910 --> 00:15:57,820
almost all the attention because there's surprisingly strong results available for l one it acts

281
00:15:57,820 --> 00:16:03,010
kind of like zero in certain situations

282
00:16:03,110 --> 00:16:06,010
now there is a huge literature on this and it's important to kind of divided

283
00:16:06,010 --> 00:16:09,910
the literature and what the goals are installed at all session theoretic there's different choices

284
00:16:09,910 --> 00:16:11,320
of loss functions

285
00:16:11,340 --> 00:16:14,840
so some people are interested in squared error is the regression after all it wouldn't

286
00:16:14,840 --> 00:16:20,490
be too surprising but the eastern square for the predictions right about the parameter estimates

287
00:16:20,550 --> 00:16:24,090
OK so you you might get you might use user plugin or something but you

288
00:16:24,090 --> 00:16:26,760
get the parameters to me plug that any make a prediction

289
00:16:26,840 --> 00:16:30,220
and you compare that to the new data you might get and you take on

290
00:16:30,220 --> 00:16:31,700
your loss to be

291
00:16:31,720 --> 00:16:33,360
than expected air

292
00:16:33,380 --> 00:16:36,490
OK so that's the loss function has an expectation there

293
00:16:36,530 --> 00:16:40,570
now look at that loss over multiple repeated datasets and that would be the frequentist

294
00:16:40,570 --> 00:16:44,240
risk you would be calculating there

295
00:16:44,260 --> 00:16:47,130
OK so that's prediction a lot of people have been looking that

296
00:16:47,130 --> 00:16:50,450
the more classical in is just look at the squared error in terms of parameter

297
00:16:52,110 --> 00:16:54,970
if you really care about the parameters which you do in any situation he might

298
00:16:54,970 --> 00:17:01,200
carry the one will focus on is what's called support recovery which is the indices

299
00:17:01,200 --> 00:17:05,220
in which you have nonzero parameterized case i'm going to make an estimate

300
00:17:05,240 --> 00:17:09,820
and some of those slots the indices will be have non zero values and i'll

301
00:17:09,820 --> 00:17:15,180
i'll return support as my estimate after i've done that i could parameter estimates just

302
00:17:15,180 --> 00:17:18,280
restricted to my support and that would be a way to actually do parameter estimation

303
00:17:18,280 --> 00:17:21,070
is the two stage procedure

304
00:17:21,680 --> 00:17:25,150
OK so you'd like to hear your risk now will be the probability that you

305
00:17:25,150 --> 00:17:27,010
don't get the right support

306
00:17:27,030 --> 00:17:32,470
so the different risk functional than the one in the first two cases

307
00:17:32,490 --> 00:17:36,410
OK so now let's talk about this multivariate regression problem

308
00:17:36,430 --> 00:17:39,010
OK so a little bit different so now

309
00:17:39,030 --> 00:17:40,450
the y

310
00:17:40,450 --> 00:17:45,510
is a matrix further and data points but every row of this thing is a

311
00:17:45,510 --> 00:17:46,800
whole vector

312
00:17:46,900 --> 00:17:48,660
so for every x

313
00:17:48,680 --> 00:17:52,530
you get a you know every x rho here you get to y rho as

314
00:17:52,530 --> 00:17:57,390
well so it's of x y pairs are both vectors

315
00:17:58,590 --> 00:18:03,950
therefore the b may be is no longer vector b is the whole matrix

316
00:18:04,030 --> 00:18:07,030
right and you can think of each column of this thing

317
00:18:07,090 --> 00:18:09,990
as all the coefficients appropriate too

318
00:18:10,030 --> 00:18:15,180
one of the y components

319
00:18:17,990 --> 00:18:24,910
OK now you might hope to be able to do a good regression in this

320
00:18:24,910 --> 00:18:26,930
situation when

321
00:18:27,450 --> 00:18:30,880
these regression to have something to do with each other related regressions

322
00:18:30,970 --> 00:18:34,680
so regression for component why one might be related to one for y two in

323
00:18:34,680 --> 00:18:37,260
particular might share their support

324
00:18:37,280 --> 00:18:39,570
i e there might be some relevant features

325
00:18:39,590 --> 00:18:44,220
relevant axes which are relevant across multiple regression you might get more power by doing

326
00:18:44,220 --> 00:18:47,450
the regression simultaneously in that situation

327
00:18:47,450 --> 00:18:48,930
so that the intuition

328
00:18:48,930 --> 00:18:52,430
and you can now go on sort of bayesian story if i put on bayesian

329
00:18:52,430 --> 00:18:56,280
had kind of a hierarchical bayesian model of some kind to try to capture that

330
00:18:56,280 --> 00:19:00,090
we're not going get this talk be frequentists org have developed some kind of loss

331
00:19:00,090 --> 00:19:02,530
function which captures that

332
00:19:03,380 --> 00:19:07,300
i'll get ready some tree shrinkage of parameters which we hope might be related towards

333
00:19:07,300 --> 00:19:12,110
each other and not distributive everything towards its for zero or towards a mean which

334
00:19:12,110 --> 00:19:13,950
is what the last so node

335
00:19:14,780 --> 00:19:18,010
OK so we're going to have design matrix

336
00:19:18,010 --> 00:19:21,700
matrix operations and we're going to look at all these parameters going to infinity and

337
00:19:21,700 --> 00:19:24,510
try to say something theoretical situation

338
00:19:24,510 --> 00:19:32,700
OK so our first but it worked on this is ue obozinski young's work started

339
00:19:32,700 --> 00:19:37,740
with block regularisation second order cone problems so here's could be a regularisation problem we've

340
00:19:37,740 --> 00:19:40,430
censorship in various ways

341
00:19:40,760 --> 00:19:43,570
so this a really nice example of what happened to me

342
00:19:43,770 --> 00:19:47,090
and after that i must say stopped working with this company was that was not

343
00:19:47,100 --> 00:19:51,240
the first time that something stupid let me tell you a story

344
00:19:51,740 --> 00:19:53,880
so this company

345
00:19:54,550 --> 00:19:56,960
some device wanted to figure out

346
00:19:57,000 --> 00:19:58,180
based on some

347
00:19:58,190 --> 00:20:00,060
information about the blood status

348
00:20:02,510 --> 00:20:04,940
old main have prostate cancer

349
00:20:05,990 --> 00:20:10,810
so that it's apparently fairly easy to get started from sick old man who had

350
00:20:10,810 --> 00:20:12,140
prostate cancer

351
00:20:12,210 --> 00:20:15,700
the reason being that usually when people get sick they will say OK you should

352
00:20:15,700 --> 00:20:20,240
take blood and it is actually interested that while other people will be diagnosed what

353
00:20:20,240 --> 00:20:24,350
is actually and will quickly and bits and whatever you should do it

354
00:20:24,380 --> 00:20:28,070
six people are usually very willing to give you that

355
00:20:28,120 --> 00:20:30,370
healthy people aren't

356
00:20:30,390 --> 00:20:32,110
so that this big stack of

357
00:20:32,120 --> 00:20:35,190
data from sick pay will see goldman

358
00:20:35,420 --> 00:20:39,750
and that's the problem OK we have a little bit of data from healthy people

359
00:20:39,750 --> 00:20:42,260
but we don't have enough

360
00:20:43,380 --> 00:20:47,340
that's that's so they got at the had this really brilliant idea

361
00:20:47,380 --> 00:20:48,740
they went of

362
00:20:48,760 --> 00:20:53,000
because this is a this company was to start up on campus

363
00:20:53,660 --> 00:20:56,090
this so OK well you know

364
00:20:56,120 --> 00:21:00,160
who's going to give us the bloodless easily while students of course that you give

365
00:21:00,160 --> 00:21:02,990
money to give blood

366
00:21:03,100 --> 00:21:08,130
and so they went off and got all the blood samples from healthy young active

367
00:21:08,140 --> 00:21:12,310
probably slightly drunk male students

368
00:21:12,360 --> 00:21:15,480
and came to be me and said alex can classify

369
00:21:15,740 --> 00:21:19,150
so i told him well i guess i'll probably be able to build the wonderful

370
00:21:19,150 --> 00:21:23,820
estimated it'll probably separated two classes but it will be no good

371
00:21:23,990 --> 00:21:27,030
and so they wondered why

372
00:21:27,050 --> 00:21:31,530
well the point is quite simply that of course it would take all sorts of

373
00:21:31,530 --> 00:21:36,640
things probably the hormonal status will be different from the the amount of activity would

374
00:21:36,640 --> 00:21:37,920
be different

375
00:21:38,850 --> 00:21:42,320
demand of media they probably drink would be different so there's a lot of things

376
00:21:42,320 --> 00:21:43,920
that will be very different

377
00:21:43,970 --> 00:21:48,110
that probably wouldn't pick up on the prostate cancer center in feature

378
00:21:48,210 --> 00:21:49,960
so we need to make sure

379
00:21:50,900 --> 00:21:52,410
you capture the bias

380
00:21:52,420 --> 00:21:54,180
problems otherwise

381
00:21:54,190 --> 00:21:55,460
you'll get really

382
00:21:55,470 --> 00:21:57,520
bad example

383
00:21:57,830 --> 00:22:03,160
there's another example that happened to the US army in the seventies was

384
00:22:03,220 --> 00:22:06,430
but i think late seventies when the network started getting popular

385
00:22:06,480 --> 00:22:07,760
so they wanted to

386
00:22:07,770 --> 00:22:11,590
build the system which can detect with tanks in the forest

387
00:22:12,780 --> 00:22:17,190
so they decide to do the right thing and do actually experiments

388
00:22:17,200 --> 00:22:20,700
so i went off and draft attacks into the forest

389
00:22:20,710 --> 00:22:22,260
and the flow of the forest the

390
00:22:22,280 --> 00:22:25,250
with the plane with the camera to pictures

391
00:22:25,260 --> 00:22:26,120
and that

392
00:22:26,120 --> 00:22:28,410
the tax drove them out of the forest

393
00:22:28,490 --> 00:22:30,450
for the first game

394
00:22:31,590 --> 00:22:33,140
build a classifier

395
00:22:33,240 --> 00:22:35,240
they worked flawlessly

396
00:22:35,990 --> 00:22:37,170
what went wrong

397
00:22:37,190 --> 00:22:40,700
has anybody got an idea

398
00:22:41,090 --> 00:22:43,920
you're pretty close

399
00:22:45,670 --> 00:22:46,970
so the phrase

400
00:22:46,980 --> 00:22:50,060
pretty much what happened is that draft attacks into the forest

401
00:22:50,170 --> 00:22:54,740
and this was in the morning when the taxman before so that long shadows

402
00:22:54,780 --> 00:22:57,470
by the time the tax that the first it was high

403
00:22:57,480 --> 00:22:59,260
there were no shadows

404
00:22:59,280 --> 00:23:03,570
so what the classifier actually do this it did take the chalice was munchausen did

405
00:23:03,630 --> 00:23:06,400
that really well simple problem

406
00:23:06,400 --> 00:23:10,030
and they spent a lot of money on it

407
00:23:11,160 --> 00:23:13,920
if you do machine learning problem here one

408
00:23:13,960 --> 00:23:18,450
you really really really need to make sure that you about to explain

409
00:23:18,500 --> 00:23:21,120
if it isn't there are ways how to

410
00:23:21,160 --> 00:23:25,280
fix things if they are not too bad if they have some amount of wire

411
00:23:25,380 --> 00:23:28,740
so this goes under the moniker of covariate shift correction

412
00:23:29,420 --> 00:23:30,670
i mean that tools

413
00:23:31,420 --> 00:23:32,690
to deal with this

414
00:23:32,700 --> 00:23:36,120
at least to some extent but you need to be very careful

415
00:23:36,160 --> 00:23:38,840
and if you just naively run it

416
00:23:38,850 --> 00:23:40,430
o thing you'll be screwed

417
00:23:40,460 --> 00:23:41,670
another example

418
00:23:41,680 --> 00:23:44,230
a bit more recently on netflix

419
00:23:44,600 --> 00:23:47,010
if you look at the ratings

420
00:23:47,080 --> 00:23:51,330
i think about two or three years ago the ratings changed

421
00:23:51,990 --> 00:23:53,380
the average ratings

422
00:23:53,380 --> 00:23:56,090
jumped up by half a point

423
00:23:56,140 --> 00:23:58,420
so what happened well

424
00:23:58,470 --> 00:24:00,280
when you basically gone whether

425
00:24:00,300 --> 00:24:03,700
how much you like the move you're not and this is the initial scale went

426
00:24:03,700 --> 00:24:06,880
from i hated it to i loved it

427
00:24:07,760 --> 00:24:13,570
after that time the scale went from i didn't like its two i loved it

428
00:24:13,570 --> 00:24:16,670
so all the some the ratings changed

429
00:24:16,730 --> 00:24:21,890
travis intend in in the competition we had to figure that out on you

430
00:24:25,710 --> 00:24:28,650
let's just stop a little bit what we had in terms of stuff

431
00:24:28,680 --> 00:24:31,270
well we could encounter vectors

432
00:24:32,600 --> 00:24:35,990
all strings like from takes the biological sequences

433
00:24:36,010 --> 00:24:38,500
structured documents

434
00:24:39,340 --> 00:24:41,670
graph like internet

435
00:24:41,700 --> 00:24:43,420
gene regulatory networks

436
00:24:43,430 --> 00:24:47,320
you might have some tertiary structure in your proteins themselves

437
00:24:47,360 --> 00:24:49,160
and well

438
00:24:49,180 --> 00:24:52,880
what can happen well you could have incomplete doctor

439
00:24:52,920 --> 00:24:55,720
data may actually come from different sources

440
00:24:55,740 --> 00:25:01,370
as in the same information in different places like you have three labs actually cooperating

441
00:25:01,370 --> 00:25:03,960
on some large scale micro study

442
00:25:03,970 --> 00:25:06,630
the doctor may happen to be biased

443
00:25:06,630 --> 00:25:08,110
so that's

444
00:25:08,120 --> 00:25:09,130
OK case

445
00:25:09,160 --> 00:25:11,350
or it may actually be ill-defined

446
00:25:11,370 --> 00:25:15,370
so this is what happens to you actually quite frequently if you talk to somebody

447
00:25:15,370 --> 00:25:16,950
in the in practice

448
00:25:17,000 --> 00:25:20,140
so they will tell you that they will have some idea of what you should

449
00:25:20,140 --> 00:25:21,860
be able to extract

450
00:25:21,900 --> 00:25:25,070
but it may not actually be very close to what really helps them this so

451
00:25:25,160 --> 00:25:29,310
some the very maybe actually to tease out what really solve the problem

452
00:25:29,310 --> 00:25:32,460
or even what the problem is

453
00:25:32,810 --> 00:25:35,090
this is a particularly nasty one

454
00:25:35,130 --> 00:25:38,300
environment may actually react to what you're doing

455
00:25:38,390 --> 00:25:41,410
and reinforcement learning deals with that

456
00:25:41,430 --> 00:25:42,710
so for instance

457
00:25:42,730 --> 00:25:46,350
people at some point started designing butterfly portfolios

458
00:25:46,350 --> 00:25:51,190
so the idea was well you pick some good combination of stocks that

459
00:25:51,240 --> 00:25:53,880
when one goes up the other one goes down

460
00:25:53,920 --> 00:25:55,470
he picked on those well

461
00:25:55,490 --> 00:26:00,360
why would you bet on something that was value is essentially constant well you buy

462
00:26:00,360 --> 00:26:02,640
low sell high right

463
00:26:02,660 --> 00:26:06,210
trouble is as soon as people figure out what you're betting on they will be

464
00:26:06,220 --> 00:26:07,140
against you

465
00:26:07,150 --> 00:26:09,660
so that arbitrage opportunities appears

466
00:26:09,710 --> 00:26:14,170
so at some point as butterfly portfolios are not so useful anymore

467
00:26:14,180 --> 00:26:17,370
then people went to france in france in fancy ones

468
00:26:17,370 --> 00:26:18,470
so this is why

469
00:26:18,490 --> 00:26:21,460
a lot of traders wouldn't really tell you what the strategy is

470
00:26:21,470 --> 00:26:24,140
it's going to compete against them

471
00:26:25,550 --> 00:26:28,960
any questions about that

472
00:26:31,740 --> 00:26:36,600
the next thing is a little bit what can we actually do with start

473
00:26:36,600 --> 00:26:37,800
and i'm

474
00:26:37,820 --> 00:26:41,300
roughly distinguishing between unsupervised supervised learning

475
00:26:41,300 --> 00:26:43,570
with lines at different heights

476
00:26:43,610 --> 00:26:48,950
and then at each level for you know

477
00:26:49,010 --> 00:26:53,140
how many correct components are about like how many peaks at above that line

478
00:26:53,180 --> 00:26:55,130
and the biggest cluster

479
00:26:55,140 --> 00:27:05,160
so for example if i hope you will see one peak two

480
00:27:05,160 --> 00:27:08,510
three four five

481
00:27:08,820 --> 00:27:11,490
but if the threshold lower like here

482
00:27:11,510 --> 00:27:13,530
i may seem to be

483
00:27:13,550 --> 00:27:17,570
which means that at this level that are all in one place here i have

484
00:27:17,610 --> 00:27:19,860
i love this place the

485
00:27:19,870 --> 00:27:22,530
and at this level here

486
00:27:22,550 --> 00:27:25,820
i have only one left

487
00:27:27,470 --> 00:27:34,530
and so that could produce is not just one clustering but basically all possible clusterings

488
00:27:34,530 --> 00:27:38,320
when you move this line from zero to

489
00:27:38,340 --> 00:27:40,640
one the the highest peak distribution

490
00:27:48,300 --> 00:27:56,070
o thing because this is a very good question in fact i don't know if

491
00:27:56,070 --> 00:27:59,610
you look at the bottom of the page this doesn't work so used you want

492
00:27:59,630 --> 00:28:01,110
to this the

493
00:28:01,110 --> 00:28:02,410
this is why this this

494
00:28:02,430 --> 00:28:04,890
can't really work in high dimensions

495
00:28:04,890 --> 00:28:08,320
and one of them is that actually kernel density estimation

496
00:28:09,160 --> 00:28:12,640
it doesn't work in high dimensions two

497
00:28:12,660 --> 00:28:17,490
basically for several reasons so this will work in

498
00:28:17,550 --> 00:28:20,700
and the second problem so everything that in green is

499
00:28:23,450 --> 00:28:26,490
are difficult in this so

500
00:28:26,510 --> 00:28:34,200
first of all it's hard to get a higher current density estimation in high dimensions

501
00:28:34,220 --> 00:28:36,910
can you see why

502
00:28:36,970 --> 00:28:39,840
because typically in high dimensions

503
00:28:43,840 --> 00:28:50,390
let's say in any impediment secure

504
00:28:50,410 --> 00:28:55,050
break every direction into two you have about to the tens of thousands

505
00:28:55,070 --> 00:28:56,530
little q

506
00:28:56,550 --> 00:28:58,840
we should just break the dimension to left and right

507
00:28:58,930 --> 00:29:01,610
and so if you want to have a data point in each of these you

508
00:29:01,610 --> 00:29:03,720
need a thousand points

509
00:29:03,760 --> 00:29:07,260
of course if the data is clustered and it will not be spread so basically

510
00:29:08,010 --> 00:29:13,410
a lot of data to fill the space most of the space is empty and

511
00:29:13,410 --> 00:29:15,390
lots of point very far apart

512
00:29:15,450 --> 00:29:20,070
and so was is on top of the points this not overlap very much on

513
00:29:20,070 --> 00:29:24,180
the data is really what

514
00:29:24,200 --> 00:29:27,680
and so it's very hard to construct

515
00:29:27,680 --> 00:29:30,660
to adjust the size of the crawler to construct

516
00:29:30,840 --> 00:29:35,760
so these are just

517
00:29:36,320 --> 00:29:43,680
the growth and so there are some to like we actually put but serious

518
00:29:43,700 --> 00:29:46,030
the cover several

519
00:29:46,030 --> 00:29:51,930
several things but not

520
00:29:52,180 --> 00:30:00,800
so both desire as far as they understood understood that the just it's not understood

521
00:30:00,800 --> 00:30:03,700
what the asymptotic properties that doesn't come the that

522
00:30:03,910 --> 00:30:08,070
like you you're doing this it estimation statisticians want to know it converges and so

523
00:30:08,070 --> 00:30:10,570
on this

524
00:30:10,590 --> 00:30:13,760
they are

525
00:30:14,090 --> 00:30:20,970
four and a

526
00:30:31,180 --> 00:30:34,490
o because the because then you want see peaks

527
00:30:34,510 --> 00:30:40,050
if you think about once and then what you see the cluster sensation yes you

528
00:30:40,050 --> 00:30:44,360
can think basically have to make the bomb almost this last letters all the data

529
00:30:44,360 --> 00:30:46,110
in one dimension

530
00:30:47,630 --> 00:30:53,550
also there is another reason when the data in high dimensions very often

531
00:30:53,610 --> 00:30:57,490
there are two dimensions of the dimension of the space so they lie along strings

532
00:30:57,490 --> 00:31:03,160
in this space in order to lie along lower dimensional curved manifolds and so you

533
00:31:03,160 --> 00:31:06,700
can you puts here's the sphere contains

534
00:31:06,760 --> 00:31:09,090
a lot of empty space

535
00:31:09,140 --> 00:31:11,390
so that's another reason why

536
00:31:12,720 --> 00:31:15,320
kernel density estimation in this in this

537
00:31:15,340 --> 00:31:20,070
that was almost certainly don't work in

538
00:31:20,110 --> 00:31:23,140
you have actually put the bombs

539
00:31:23,180 --> 00:31:27,010
make them very elongated in the direction of the data line

540
00:31:29,840 --> 00:31:34,960
this simple kernel density estimation doesn't go far enough in the and also kind of

541
00:31:34,960 --> 00:31:38,110
finding level features like these if you are in more than

542
00:31:38,280 --> 00:31:39,860
in one dimension

543
00:31:39,870 --> 00:31:41,700
in two dimensions there are

544
00:31:41,720 --> 00:31:45,280
if you go for level set in three and four and five images you see

545
00:31:45,280 --> 00:31:49,320
that it's much harder the actually this can be fixed

546
00:31:49,320 --> 00:31:53,860
and you about actually looking only at the values of

547
00:31:53,860 --> 00:31:57,170
so the more

548
00:32:09,380 --> 00:32:12,790
that's like for its

549
00:32:12,790 --> 00:32:14,180
show you

550
00:32:14,200 --> 00:32:21,250
and they say they believe that and also show you

551
00:32:31,340 --> 00:32:38,120
more than

552
00:32:39,520 --> 00:32:42,940
so is

553
00:32:51,450 --> 00:33:01,440
one of the key reason is that

554
00:33:03,970 --> 00:33:08,910
the thing

555
00:33:08,940 --> 00:33:11,920
four classes

556
00:33:15,450 --> 00:33:18,140
well structure real

557
00:33:26,090 --> 00:33:31,050
so i'm sitting here in the

558
00:33:34,200 --> 00:33:39,020
and by matrix

559
00:33:39,030 --> 00:33:41,340
and you will see some examples in a minute

560
00:33:41,360 --> 00:33:46,720
and basically with these large typically very sparse matrices all kinds of problems i mean

561
00:33:46,720 --> 00:33:50,340
there are two large data complicated missing entries the noisy entries

562
00:33:50,390 --> 00:33:54,340
and this in general we feel there might be some underlying structure which we don't

563
00:33:54,340 --> 00:33:57,920
see on the surface but just looking at these matrices

564
00:33:58,660 --> 00:34:03,670
what be looking for some then is a simple way in the sense to explain

565
00:34:04,170 --> 00:34:06,900
these entries entries in the matrix

566
00:34:06,950 --> 00:34:08,450
by using a model

567
00:34:08,490 --> 00:34:12,800
and then also uncover what you might call latent structure

568
00:34:12,830 --> 00:34:13,620
of the day

569
00:34:13,670 --> 00:34:15,680
this is the general goal here

570
00:34:15,740 --> 00:34:19,300
and in particular the types of methods that i will talk about

571
00:34:19,420 --> 00:34:25,330
are closely related to a matrix decomposition to what i call matrix decomposition here is

572
00:34:25,330 --> 00:34:27,960
simply a factorisation of the matrix

573
00:34:28,020 --> 00:34:32,580
so we have a matrix a we want to approximate it by some matrix had

574
00:34:32,580 --> 00:34:37,360
which can be written as a product of two matrices are and these two

575
00:34:37,420 --> 00:34:39,110
matrices are

576
00:34:39,210 --> 00:34:43,110
typically constrained to be things so you know if a is an m by n

577
00:34:44,080 --> 00:34:48,240
you know how much has been owned by q so you know very thin matrix

578
00:34:48,240 --> 00:34:51,990
and then we might have our and we might have a diagonal matrix in between

579
00:34:52,010 --> 00:34:53,830
and the idea is

580
00:34:53,850 --> 00:34:58,520
right to find these types of factorizations of matrices and and i'll try to explain

581
00:34:58,520 --> 00:35:01,890
how that relates to something like latent structure

582
00:35:03,920 --> 00:35:06,510
it's just a brief introduction we're going

583
00:35:06,520 --> 00:35:12,790
OK so let me now talk a little bit about a particular application information retrieval

584
00:35:12,790 --> 00:35:18,450
and this technique called latent semantic indexing which was developed in the late nineties

585
00:35:19,680 --> 00:35:21,020
so first of all

586
00:35:21,020 --> 00:35:22,670
you know

587
00:35:22,770 --> 00:35:26,760
try to convince you that information so is important well we all know that right

588
00:35:26,770 --> 00:35:30,900
in order to access information the most popular way of doing that is

589
00:35:30,920 --> 00:35:34,420
to use box like that you know the box button

590
00:35:34,460 --> 00:35:37,300
and then you know we have all the knowledge of the world out there and

591
00:35:37,300 --> 00:35:42,700
certainly somehow magically you know we will get access to the knowledge and indeed

592
00:35:42,950 --> 00:35:47,890
you know there some magic other names here like google and if you look you

593
00:35:47,890 --> 00:35:51,790
know on the web there's no website basically that doesn't have such functionality right to

594
00:35:51,790 --> 00:35:57,550
go to CNN or medlineplus you wherever you go everywhere is basically search box like

595
00:35:59,610 --> 00:36:03,290
apparently it's it's an interesting problem so we would be good if we could somehow

596
00:36:03,290 --> 00:36:07,930
improve search right so that's just in terms of the relevance of the problem

597
00:36:09,020 --> 00:36:10,090
but really

598
00:36:10,100 --> 00:36:14,170
some of course we all know that and we also with particular we know that

599
00:36:14,170 --> 00:36:18,300
because very often things don't work out the way we expect them to work out

600
00:36:19,150 --> 00:36:23,800
you know it's really this idea that we can identify relevant documents based on you

601
00:36:23,800 --> 00:36:26,380
know very short ambiguous and incomplete query

602
00:36:26,400 --> 00:36:27,970
it's a bit of

603
00:36:27,980 --> 00:36:31,490
you know we have very high expectations here

604
00:36:31,550 --> 00:36:32,920
and are

605
00:36:33,610 --> 00:36:37,100
but but nevertheless in a very popular have the numbers you know how many people

606
00:36:37,100 --> 00:36:41,310
use it and we all know that and we certainly use

607
00:36:41,410 --> 00:36:42,980
i would assume

608
00:36:43,000 --> 00:36:45,280
search on the web for instance every day

609
00:36:45,290 --> 00:36:49,340
and more and more people use it in their professional and personal

610
00:36:49,360 --> 00:36:51,620
OK now let me

611
00:36:51,650 --> 00:36:53,670
try to show how

612
00:36:53,680 --> 00:36:58,630
a matrix comes into play here to connect with what i said for my first

613
00:36:59,610 --> 00:37:03,870
OK so the the typical matrix we're talking about here is called the document term

614
00:37:03,870 --> 00:37:08,550
matrix or term document matrix depending on which way transpose the matrix

615
00:37:08,600 --> 00:37:11,980
so we have a document here have term

616
00:37:11,980 --> 00:37:17,290
and then what we what we typically do many other information retrieval applications we reduce

617
00:37:17,320 --> 00:37:19,230
the data to

618
00:37:19,240 --> 00:37:21,350
this matrix which

619
00:37:21,460 --> 00:37:25,780
as an entry has just frequency count of how often a particular word occurred in

620
00:37:25,780 --> 00:37:27,560
the talk

621
00:37:27,610 --> 00:37:32,040
so then we can take a particular row of this matrix

622
00:37:32,550 --> 00:37:36,880
and then we can use that basically is the representation of a particular stock so

623
00:37:36,880 --> 00:37:39,480
if we have many terms so we have tens of thousands of terms it will

624
00:37:39,480 --> 00:37:43,040
be very high dimensional representation for each document

625
00:37:43,090 --> 00:37:44,110
and of course

626
00:37:44,120 --> 00:37:47,360
you know there will be lots of zeros and so on and so forth and

627
00:37:47,360 --> 00:37:53,670
a lot of information retrieval techniques are based on these document representation just by hundred

628
00:37:53,670 --> 00:37:55,110
sixty right

629
00:37:55,110 --> 00:37:59,360
and and the reason why it works at least to some extent is that indeed

630
00:37:59,360 --> 00:38:04,730
in the the words that occur in the document are good indicators in some sense

631
00:38:04,730 --> 00:38:07,960
right of whether document to be relevant to a particular query

632
00:38:07,980 --> 00:38:14,460
so that a lot of the deeper maybe you know syntactic analysis and linguistic analysis

633
00:38:14,460 --> 00:38:17,090
is very often not necessary on on

634
00:38:17,130 --> 00:38:21,530
service level to find out whether document could be relevant or not because that depends

635
00:38:22,300 --> 00:38:25,300
what you want you want question answering systems and you need to look more into

636
00:38:25,300 --> 00:38:27,680
the structure of sentences

637
00:38:27,800 --> 00:38:31,350
and in practice what people often do is they are they multiply these entries by

638
00:38:31,350 --> 00:38:34,420
some smart term weighting like you you know the idea of waiting and then you

639
00:38:35,100 --> 00:38:37,670
as TFIDF and things like that

640
00:38:37,670 --> 00:38:44,590
and this is the foreground star but everything else background galaxies and as you see

641
00:38:44,640 --> 00:38:46,810
these objects are

642
00:38:46,830 --> 00:38:52,160
much more disturbing any irregular from the one that they show you before but remember

643
00:38:52,160 --> 00:38:55,680
you have seen this object you are looking so far away that you've seen this

644
00:38:55,680 --> 00:38:59,120
object when the universe was only a fraction of its age

645
00:38:59,160 --> 00:39:02,390
so you see in the universe actually forming galaxies

646
00:39:02,400 --> 00:39:06,530
that's why they are disturbed ball

647
00:39:06,580 --> 00:39:11,840
so cosmology is fortunately or unfortunately depending on your point of view not only pretty

648
00:39:13,060 --> 00:39:17,660
so nature is written in the mathematical language this come from my countrymen galileo and

649
00:39:17,660 --> 00:39:21,430
so what we're saying here we can study the laws of physics here on earth

650
00:39:21,430 --> 00:39:25,230
and that's them and then apply them to describe the entire universe this is an

651
00:39:25,230 --> 00:39:28,410
anonymous extrapolation of scale remember

652
00:39:28,460 --> 00:39:31,560
the first one of the first transparency is a show you

653
00:39:31,660 --> 00:39:35,450
and so we are doing an enormous extrapolation here right we are testing the laws

654
00:39:35,450 --> 00:39:37,490
of physics and scales of

655
00:39:37,550 --> 00:39:38,300
you know

656
00:39:38,310 --> 00:39:44,790
kilometres of in this case of the local solar system and then extrapolating those low

657
00:39:44,800 --> 00:39:50,630
tense order of magnitude of so

658
00:39:50,670 --> 00:39:53,800
what physical you know that you can extrapolate that many of the of money to

659
00:39:53,800 --> 00:39:58,900
then it works without weakening

660
00:39:58,980 --> 00:40:03,530
so this is not a step we are taking it as an assumption

661
00:40:03,560 --> 00:40:10,010
and also we assume that the universe is comprehensible by as and you need physics

662
00:40:10,010 --> 00:40:17,600
and mathematics is sometimes called physical cosmology and these deep from links to fundamental physics

663
00:40:17,620 --> 00:40:22,920
so the kind of fundamental physics so low that database studied under your feet here

664
00:40:22,930 --> 00:40:23,540
at cern

665
00:40:24,120 --> 00:40:28,650
at the same kind of physicality and testing by looking up at the universe and

666
00:40:28,650 --> 00:40:34,290
this has become clear in the past to say five or ten years basically

667
00:40:34,320 --> 00:40:40,780
astronomy cosmology and particle physicist as start asking exactly the same question but they're trying

668
00:40:40,780 --> 00:40:45,220
to find the answer from two different approaches in some people go underground and some

669
00:40:45,220 --> 00:40:47,050
people of telescopes

670
00:40:47,070 --> 00:40:51,840
but you know the the question is the same and well the answer ought to

671
00:40:51,840 --> 00:40:58,040
be the same if we understand you know the entire no physical picture behind

672
00:41:02,610 --> 00:41:06,030
this this is in cosmology are very difficult to measure

673
00:41:06,080 --> 00:41:10,580
so in the the graph before i told you know that mentioned that was going

674
00:41:10,580 --> 00:41:15,470
out that way was the distance and then they will see in a second distances

675
00:41:15,470 --> 00:41:19,030
in cosmology are very difficult to measure you can't go with the ruler in all

676
00:41:19,030 --> 00:41:19,980
the way

677
00:41:19,990 --> 00:41:25,780
to the next galaxy already had enough to measure the distance to the nearby stars

678
00:41:25,820 --> 00:41:28,000
but the velocity is easy

679
00:41:28,030 --> 00:41:30,440
and thank you to edwin hubble

680
00:41:30,440 --> 00:41:35,220
that's edwin hubble was of in a small with some telescope and here's morgan described

681
00:41:35,270 --> 00:41:37,260
then we can make a connection

682
00:41:37,280 --> 00:41:44,030
between velocities and distances and that's what allows us to measure distances out to basically

683
00:41:44,030 --> 00:41:46,250
the edge of the so

684
00:41:46,260 --> 00:41:47,780
how does it work

685
00:41:47,830 --> 00:41:52,990
i mean i'm sure you're all familiar with the effect of light bank shifted the

686
00:41:52,990 --> 00:41:54,270
blue shift

687
00:41:54,280 --> 00:41:57,320
if you have a light source the moves away from you

688
00:41:57,470 --> 00:42:02,730
wavelength is going to be redshifted while if the light source go moves towards you

689
00:42:02,730 --> 00:42:05,370
these wavelengths is going to be blue shifted

690
00:42:05,430 --> 00:42:10,820
so velocity relatively easy to measure in cosmology because you just have to measure the

691
00:42:10,820 --> 00:42:16,840
light come from one object to recognise the song featuring its spectrum and then compare

692
00:42:16,840 --> 00:42:21,670
that to build the wavelength that feature we we've learned that you measure on earth

693
00:42:21,670 --> 00:42:24,780
when the object is at rest

694
00:42:24,820 --> 00:42:27,200
so this is how

695
00:42:27,200 --> 00:42:31,530
so cosmology talk about redshift and we'll see what the blue shift and we see

696
00:42:31,530 --> 00:42:36,570
why nominator but let's just use the nomenclature for now that is called fresh

697
00:42:37,950 --> 00:42:41,790
we define relationship with the latter the that these days

698
00:42:41,850 --> 00:42:46,990
a set of wavelengths minus the emitted wavelength divided by the emitted wavelengths or one

699
00:42:46,990 --> 00:42:48,330
plus the

700
00:42:48,350 --> 00:42:55,610
this set of wavelet divided by the to the women and this is some

701
00:42:55,630 --> 00:43:00,520
artistically reinterpreting the picture of what happened to us back to this is the spectrum

702
00:43:00,520 --> 00:43:05,710
you see there is some continues that the wavelet changes from violet to the red

703
00:43:05,790 --> 00:43:13,270
and this black line here represent as ocean sometimes the absorption lines spectrum and saw

704
00:43:13,730 --> 00:43:18,350
well this may be when the object moves away from you then this feature ends

705
00:43:18,350 --> 00:43:23,440
up there are this feature and there so the whole spectrum has been shift

706
00:43:23,450 --> 00:43:27,630
and so if you've seen relativity you know that they can be very ten in

707
00:43:27,630 --> 00:43:32,170
this standard form and four smaller velocity because will always be feeling

708
00:43:32,180 --> 00:43:35,950
almost always be dealing with philosophy that are much smaller than the speed of light

709
00:43:35,950 --> 00:43:40,070
and then you can do the approximation that actually fits basically the velocity along the

710
00:43:40,070 --> 00:43:44,370
line of sight divided by the speed of light but in reality the actual expression

711
00:43:44,540 --> 00:43:50,030
these when you rejected your velocity in the direction along the line of sight

712
00:43:50,030 --> 00:43:52,040
what say

713
00:44:02,330 --> 00:44:05,100
two years

714
00:44:05,120 --> 00:44:09,900
i was right

715
00:44:09,910 --> 00:44:14,920
the rose

716
00:44:19,560 --> 00:44:22,790
in fact there are

717
00:44:45,130 --> 00:44:48,960
well i don't know know

718
00:44:57,500 --> 00:44:59,010
they were

719
00:46:02,110 --> 00:46:05,560
these are

720
00:46:14,830 --> 00:46:16,510
part of

721
00:46:32,690 --> 00:46:38,130
i have here

722
00:46:52,090 --> 00:46:56,220
one is the only thing

723
00:47:20,940 --> 00:47:26,390
i don't think

724
00:48:04,030 --> 00:48:13,700
i mean i

725
00:48:23,820 --> 00:48:26,620
each person

726
00:48:46,690 --> 00:48:51,090
five times that

727
00:49:14,420 --> 00:49:16,960
i was thinking

728
00:49:21,650 --> 00:49:24,050
and now

729
00:49:24,330 --> 00:49:32,200
to be the

730
00:49:32,390 --> 00:49:38,840
there are are

731
00:49:42,980 --> 00:49:46,630
there are two

732
00:49:52,800 --> 00:49:56,970
one they were

733
00:50:14,620 --> 00:50:19,280
you are

734
00:50:19,290 --> 00:50:20,950
so in

735
00:50:20,950 --> 00:50:25,180
eight years of

736
00:50:28,360 --> 00:50:30,700
the point here

737
00:50:30,700 --> 00:50:33,230
the they

738
00:50:46,970 --> 00:50:51,420
or three five one

739
00:51:12,530 --> 00:51:15,130
all right

740
00:51:15,170 --> 00:51:21,760
all right here saying

741
00:51:25,830 --> 00:51:35,470
what is more

742
00:51:36,540 --> 00:51:38,000
he teaches

743
00:51:38,540 --> 00:51:42,940
on you

744
00:51:45,400 --> 00:51:51,220
you might think that one

745
00:51:52,400 --> 00:51:55,640
is the one which

746
00:52:04,460 --> 00:52:07,310
you know

747
00:52:16,680 --> 00:52:20,600
generative model we say

748
00:52:21,260 --> 00:52:24,800
it is it to

749
00:52:24,830 --> 00:52:29,350
what you

750
00:52:29,360 --> 00:52:33,010
trying to

751
00:52:33,060 --> 00:52:36,840
just to the math

752
00:52:49,400 --> 00:52:55,600
is that they do

753
00:52:55,640 --> 00:53:00,400
we've seen

754
00:53:14,250 --> 00:53:19,680
this is

755
00:54:47,830 --> 00:54:50,470
so that

756
00:54:53,920 --> 00:54:57,390
a few simple terms

757
00:55:03,100 --> 00:55:07,480
so what we do

758
00:55:11,970 --> 00:55:18,330
yes you will be able to

759
00:55:18,340 --> 00:55:26,500
she would you like

760
00:55:44,780 --> 00:55:46,060
one year

761
00:55:46,060 --> 00:55:47,330
this film

762
00:55:47,620 --> 00:55:50,290
every nine months

763
00:55:50,520 --> 00:55:53,410
as as in the nine months before

764
00:55:53,430 --> 00:55:54,850
much more aggressive growth

765
00:55:54,850 --> 00:55:56,080
then we love

766
00:55:56,100 --> 00:56:01,180
the man is exploding every business today is essentially a business well it knows it

767
00:56:01,220 --> 00:56:02,160
or not

768
00:56:02,210 --> 00:56:08,210
which means as a corollary every business generating humungous amounts of data

769
00:56:08,230 --> 00:56:09,590
towards the end of the day

770
00:56:09,690 --> 00:56:12,980
a bit about which is an extreme that

771
00:56:15,400 --> 00:56:19,170
science in science scientific instrumentation has been exploring

772
00:56:19,190 --> 00:56:23,500
so scientists can now easily together

773
00:56:23,520 --> 00:56:26,100
one side of the scientists can get

774
00:56:26,490 --> 00:56:30,660
automated more the decade

775
00:56:30,670 --> 00:56:34,770
this is also government security applications

776
00:56:34,780 --> 00:56:37,110
politics et cetera

777
00:56:37,120 --> 00:56:38,660
the internet

778
00:56:38,670 --> 00:56:41,930
and the ubiquity of the web have actually

779
00:56:41,980 --> 00:56:46,590
been a huge contributor to data driven approach to business especially in the marketing side

780
00:56:46,600 --> 00:56:50,050
of it on was talk about that on tuesday

781
00:56:50,160 --> 00:56:54,180
finally the most important part of the talent shortage we can

782
00:56:54,360 --> 00:56:59,350
but rather actually has this kind of number of phds versus amount of storage

783
00:57:00,520 --> 00:57:02,120
he shows again this

784
00:57:02,140 --> 00:57:06,420
increasing dramatically every year

785
00:57:06,420 --> 00:57:08,460
so what i mean by mining

786
00:57:08,540 --> 00:57:11,500
is finding interesting structure in data

787
00:57:11,520 --> 00:57:13,900
structure structure refers to

788
00:57:13,920 --> 00:57:19,240
statistical patterns predictive models and in relationships

789
00:57:20,580 --> 00:57:26,190
difficulty in this definition we can define everything fairly accurately except for interest

790
00:57:26,780 --> 00:57:27,770
we don't know

791
00:57:27,790 --> 00:57:30,350
still to this day i believe

792
00:57:30,420 --> 00:57:35,750
how to define what's interesting obviously machine continue many many many more patterns of data

793
00:57:37,230 --> 00:57:39,920
so the problem is three

794
00:57:39,930 --> 00:57:44,380
and pretty important from an algorithmic perspective seems to discover

795
00:57:44,400 --> 00:57:47,950
you really want them to have some common sense as to what is interesting patterns

796
00:57:47,950 --> 00:57:50,420
and what is now in one

797
00:57:50,420 --> 00:57:54,540
i believe should be maybe

798
00:57:54,570 --> 00:57:56,890
pointers to that

799
00:57:56,950 --> 00:58:03,150
it beyond just simple data about scaling the analysis to very large data bases because

800
00:58:03,150 --> 00:58:05,000
humans can go there

801
00:58:05,010 --> 00:58:08,920
it's about automated search machines are trying a different

802
00:58:08,930 --> 00:58:14,180
models instead of people type hypothesizing

803
00:58:14,190 --> 00:58:18,920
one his understandable models that we can figure out what they're saying to us this

804
00:58:18,920 --> 00:58:20,300
is just the machine

805
00:58:20,410 --> 00:58:24,020
guessing what is going on predicting blindly

806
00:58:24,060 --> 00:58:26,090
and scaling highdimensional data

807
00:58:26,110 --> 00:58:30,260
my i dimensions is a huge huge challenge

808
00:58:30,430 --> 00:58:33,000
theoretically and practically

809
00:58:33,660 --> 00:58:35,250
it doesn't mean it's impossible

810
00:58:35,270 --> 00:58:39,090
in fact you can explain a lot about the data to do something and hopefully

811
00:58:39,090 --> 00:58:43,370
which was an example examples of that

812
00:58:43,420 --> 00:58:46,020
that's where databases the real problem here

813
00:58:46,880 --> 00:58:48,120
many queries

814
00:58:48,420 --> 00:58:50,900
very valid state

815
00:58:50,920 --> 00:58:52,270
i would like to say

816
00:58:52,280 --> 00:58:55,300
which was represent fraudulent transactions

817
00:58:55,340 --> 00:58:58,960
which is what like to refer this product

818
00:58:58,970 --> 00:59:02,740
what's a good customers is my database

819
00:59:02,790 --> 00:59:04,030
the problem is

820
00:59:04,060 --> 00:59:08,990
database contains information but we don't know how to state is the language of databases

821
00:59:08,990 --> 00:59:10,590
today which is why

822
00:59:10,640 --> 00:59:12,380
when data mining licences

823
00:59:12,400 --> 00:59:17,920
which hopefully you know all of this i just wanted to get one page

824
00:59:17,930 --> 00:59:19,680
so in data mining and

825
00:59:19,700 --> 00:59:23,040
right this was an early three maybe you know

826
00:59:23,070 --> 00:59:25,080
ten fifteen years ago

827
00:59:25,110 --> 00:59:28,070
you know you come up some browsers

828
00:59:28,130 --> 00:59:31,040
in the morning is sort of ask what's new

829
00:59:31,040 --> 00:59:35,020
what's interesting or predict something more

830
00:59:35,060 --> 00:59:40,750
we are very very far from this although some interesting instances coming close to having

831
00:59:40,750 --> 00:59:45,190
a like this we actually have this kind of interface to a database

832
00:59:45,240 --> 00:59:48,040
and establish more applications and plug them in

833
00:59:48,040 --> 00:59:48,870
but that is

834
00:59:48,970 --> 00:59:51,890
one of the grand vision of what we want to be

835
00:59:51,930 --> 00:59:55,450
now i would argue is still very very

836
00:59:55,470 --> 00:59:59,730
sure having this calling in a general sense

837
00:59:59,800 --> 01:00:01,640
so what's missing

838
01:00:03,490 --> 01:00:06,780
companies have built up some large and impressive data warehousing

839
01:00:06,790 --> 01:00:08,960
the mining is visible nowadays

840
01:00:08,990 --> 01:00:11,460
large corporations not to do it

841
01:00:11,520 --> 01:00:17,740
two is an application to discover information enterprise data

842
01:00:17,760 --> 01:00:19,370
the truth is

843
01:00:19,390 --> 01:00:22,950
in most organizations data shambles

844
01:00:22,970 --> 01:00:30,520
most data mining efforts end up not benefiting from existing infrastructure all those are famous

845
01:00:30,540 --> 01:00:32,420
o operations carried out the DNA

846
01:00:32,420 --> 01:00:37,190
are obsessed with customer behavior understanding and they talk a lot about it

847
01:00:37,210 --> 01:00:38,760
but very very few

848
01:00:38,770 --> 01:00:40,500
actually end up doing

849
01:00:40,510 --> 01:00:43,920
very very few end up being data

850
01:00:43,930 --> 01:00:48,500
and the successful efforts typically one of not like we can take the form of

851
01:00:48,500 --> 01:00:49,580
an application

852
01:00:49,610 --> 01:00:52,700
that's a real problem for us there is a huge

853
01:00:52,890 --> 01:00:57,570
issue for me we will not become serious engineering field figure

854
01:00:57,660 --> 01:01:01,040
to this i have to about the battle of success

855
01:01:01,050 --> 01:01:05,130
and i'm thinking across organization

856
01:01:05,140 --> 01:01:06,540
i used to live

857
01:01:06,550 --> 01:01:11,050
actually maybe it nine years ago and jim gray invited to speak at the national

858
01:01:11,050 --> 01:01:12,640
research council he said to me

859
01:01:13,130 --> 01:01:15,690
try to explain to these folks

860
01:01:15,710 --> 01:01:18,890
what sustainability today

861
01:01:18,930 --> 01:01:20,650
so the way i it is

862
01:01:20,660 --> 01:01:26,630
our data navigation exploration and exploitation technology is very very

863
01:01:26,640 --> 01:01:28,640
and then i came

864
01:01:31,060 --> 01:01:35,610
you know we have this huge data storage was are right on this to one

865
01:01:35,810 --> 01:01:39,290
i determine mean every together

866
01:01:39,310 --> 01:01:40,970
so to me that's basically

867
01:01:41,020 --> 01:01:42,870
is that the tools

868
01:01:42,890 --> 01:01:48,400
and then i started these ancient right we know how to build a very fancy

869
01:01:49,900 --> 01:01:53,960
a lot of companies dragging my we're bigger than yours

870
01:01:54,170 --> 01:01:58,020
but we really haven't figured out how to really put it to use and that's

871
01:01:58,020 --> 01:02:02,380
a huge challenge for us what it mean that distance falls today even though i

872
01:02:02,380 --> 01:02:07,240
actually you know you think it uses along

873
01:02:07,290 --> 01:02:08,610
so shift over time

874
01:02:08,620 --> 01:02:10,040
is my story

875
01:02:10,190 --> 01:02:11,840
my personal side of here

876
01:02:13,010 --> 01:02:18,370
started out in pure research as a professional students wanted to be use for

877
01:02:18,420 --> 01:02:22,090
and although i was in algorithms

878
01:02:22,110 --> 01:02:24,620
and taking summer jobs once in while

879
01:02:24,640 --> 01:02:27,740
this was sort of my presentation of the world

880
01:02:28,590 --> 01:02:31,460
the world is really about algorithms and theory is

881
01:02:31,510 --> 01:02:34,750
there something in the background of the database

882
01:02:34,770 --> 01:02:37,860
and systems in some way you have to worry about

883
01:02:37,880 --> 01:02:39,580
that was

884
01:02:40,520 --> 01:02:42,660
well years

885
01:02:42,660 --> 01:02:45,530
is that as you become a practitioner era view changes

886
01:02:45,540 --> 01:02:48,940
right there's something important to the customer and what they need

887
01:02:48,950 --> 01:02:50,660
not really about the algorithm

888
01:02:50,670 --> 01:02:54,660
so moving to the side becoming smaller importance

889
01:02:54,700 --> 01:02:57,800
systems and integration this you become huge

890
01:02:59,130 --> 01:03:02,820
this is becoming something really worry about a lot more

891
01:03:02,840 --> 01:03:04,790
right as you practice

892
01:03:05,780 --> 01:03:09,770
if you're all the way on the business side

893
01:03:09,790 --> 01:03:12,580
i'd rather than zero

894
01:03:13,250 --> 01:03:15,240
OK it should be imprisoned

895
01:03:16,880 --> 01:03:19,110
systems and there is an important

896
01:03:19,120 --> 01:03:22,820
but you know is a big worry about dollars and how feasible this is and

897
01:03:22,820 --> 01:03:25,110
can you make it work itself

898
01:03:25,130 --> 01:03:28,820
now this is shift is very important to keep in our minds right

899
01:03:28,870 --> 01:03:31,680
because it really helps us figure

900
01:03:31,760 --> 01:03:32,840
how to that

901
01:03:32,990 --> 01:03:36,800
attention to the right problems to solve the problem

902
01:03:36,870 --> 01:03:40,360
if you spend more time worrying about this tags

903
01:03:40,570 --> 01:03:44,990
data mining and you don't worry about the systems databases

904
01:03:45,000 --> 01:03:48,400
you're what's really going on with the very light field

905
01:03:48,420 --> 01:03:50,780
you know also need to work

906
01:03:51,270 --> 01:03:56,320
it was not

907
01:03:56,470 --> 01:03:59,460
one might think that are being in

908
01:03:59,460 --> 01:04:03,650
has the highest objective score with respect to our users selected objective

909
01:04:05,110 --> 01:04:06,090
so we

910
01:04:06,120 --> 01:04:10,250
the resort to this kind of mining problem so instead of generate the complete pattern

911
01:04:10,990 --> 01:04:13,370
we aimed at generating

912
01:04:13,390 --> 01:04:16,640
optimal graph patterns with respect to

913
01:04:16,650 --> 01:04:19,070
objective function

914
01:04:19,080 --> 01:04:21,460
and that this mining framework is very

915
01:04:21,480 --> 01:04:25,070
very useful and it is very general and we hope this mining framework is

916
01:04:25,070 --> 01:04:27,440
not specific to one objective score

917
01:04:27,440 --> 01:04:31,000
or one objective function. it should be available to

918
01:04:31,020 --> 01:04:33,690
all kinds of objective functions

919
01:04:33,730 --> 01:04:38,140
and once this mining framework is developed to same we can extend them to do

920
01:04:38,140 --> 01:04:41,540
a lot of other interesting pattern mining tasks

921
01:04:41,570 --> 01:04:44,250
for example top k optimal graph pattern mining

922
01:04:44,320 --> 01:04:46,710
redundancy aware graph pattern mining

923
01:04:46,750 --> 01:04:48,020
it's going to

924
01:04:48,120 --> 01:04:50,860
patterns for classification

925
01:04:50,920 --> 01:04:55,650
so here is mining framework we would like to propose. we would like to directly

926
01:04:55,650 --> 01:04:59,090
mine those optimal graph patterns from the raw

927
01:04:59,090 --> 01:05:00,210
graph database

928
01:05:00,210 --> 01:05:04,650
and at the same time we do not want to set any minimal

929
01:05:04,710 --> 01:05:08,420
frequency threshold because it's very hard to justify

930
01:05:08,420 --> 01:05:11,480
so this is our goal to

931
01:05:11,500 --> 01:05:17,000
to propose directed pattern mining framework

932
01:05:17,020 --> 01:05:18,810
so we recognise that

933
01:05:18,820 --> 01:05:26,210
OK also these objective functions are not N-term monotonic to frequency measure

934
01:05:26,230 --> 01:05:29,820
but we can derive an upper bound function which

935
01:05:29,860 --> 01:05:32,840
is monotonic and monotonic

936
01:05:33,000 --> 01:05:38,670
to the frequency and monotonic to the graph size, and monotonic to the frequency let's take

937
01:05:38,670 --> 01:05:41,230
G-test score as an example

938
01:05:41,250 --> 01:05:45,380
here G-test score can be write as a function of

939
01:05:45,400 --> 01:05:47,630
p and q where p and q

940
01:05:47,630 --> 01:05:51,000
are the frequency of the graph pattern in the positive dataset and

941
01:05:51,020 --> 01:05:55,730
negative dataset. we can write the G-test score as this formula

942
01:05:55,730 --> 01:05:59,940
here i omit some constant function, constant value

943
01:05:59,960 --> 01:06:04,250
and and then we can calculate the derivative of gt with respect to p

944
01:06:04,250 --> 01:06:07,540
and with respect to q

945
01:06:07,590 --> 01:06:11,840
and then with some simple calculation we can make as the following conclusion

946
01:06:11,900 --> 01:06:16,650
if p is greater than q then the derivative of t with respect to p

947
01:06:16,650 --> 01:06:18,810
is greater than zero

948
01:06:18,860 --> 01:06:23,420
the derivative of gt with respect to q is less than zero and vice versa

949
01:06:23,420 --> 01:06:24,820
for the case if

950
01:06:24,880 --> 01:06:27,460
p is less than q

951
01:06:27,520 --> 01:06:32,070
this conclusion is not only valid four to G-test score if we check all

952
01:06:32,090 --> 01:06:37,540
the other objective functions i mentioned including information gain and Fischer score

953
01:06:37,550 --> 01:06:42,210
you will find that this conclusion is always valid for these objective functions

954
01:06:42,250 --> 01:06:45,270
so actually there is a hidden rule behind these

955
01:06:46,500 --> 01:06:49,090
that means if the frequency difference

956
01:06:49,110 --> 01:06:53,520
of a graph pattern in the positive dataset and the negative dataset increases

957
01:06:53,570 --> 01:06:56,630
the pattern becomes more interesting

958
01:06:56,630 --> 01:07:00,730
so using this rule actually we can derive the upper bound function

959
01:07:00,750 --> 01:07:02,480
of an objective function

960
01:07:02,520 --> 01:07:05,400
and this upper bound function is monotonic to p

961
01:07:05,420 --> 01:07:08,020
and is monotonically to q

962
01:07:08,070 --> 01:07:10,940
and because they are monotonic to the frequencies

963
01:07:10,980 --> 01:07:12,840
that means we can reuse

964
01:07:12,860 --> 01:07:14,110
the existing

965
01:07:14,130 --> 01:07:16,150
frequent graph pattern mining algorithm

966
01:07:16,170 --> 01:07:18,050
to generate these

967
01:07:18,100 --> 01:07:23,250
graph patterns with respect to other objective functions that is we can recycle

968
01:07:23,320 --> 01:07:25,630
existing graph mining algorithms

969
01:07:25,650 --> 01:07:26,960
to accommodate

970
01:07:26,960 --> 01:07:29,400
non monotonic functions

971
01:07:29,400 --> 01:07:34,500
let me use one slide to illustrate the detail

972
01:07:34,540 --> 01:07:39,000
here is a search tree a pattern search tree we enumerate the patterns from small

973
01:07:39,000 --> 01:07:40,480
size to larger size

974
01:07:40,520 --> 01:07:41,730
and each node

975
01:07:41,730 --> 01:07:44,320
represents a graph pattern

976
01:07:44,380 --> 01:07:48,860
and the child nodes contains a supergraph of its parent node

977
01:07:48,900 --> 01:07:51,290
during the enumeration you will find that OK

978
01:07:51,340 --> 01:07:54,020
the frequency of this pattern decreases

979
01:07:54,040 --> 01:07:59,340
and the upper bound value of this pattern decreases if to some

980
01:07:59,360 --> 01:08:03,090
point we find that so upper bound function score

981
01:08:03,110 --> 01:08:07,820
is less than the best pattern we discovered so far so basically we can

982
01:08:08,710 --> 01:08:10,290
the whole branch

983
01:08:10,310 --> 01:08:14,250
above that pattern. that branch becomes homeless because it were not to generate an

984
01:08:14,250 --> 01:08:16,820
any graph pattern that is

985
01:08:16,840 --> 01:08:18,900
that has objective score

986
01:08:18,940 --> 01:08:23,650
greater than the best pattern discovered so far so we call this method would

987
01:08:23,650 --> 01:08:25,040
vertical pruning

988
01:08:25,130 --> 01:08:30,570
with a careful examination we find that we actually can do these pruning more radically

989
01:08:30,840 --> 01:08:33,520
by introducing a horizontal pruning

990
01:08:33,540 --> 01:08:35,650
again this is part of the search tree

991
01:08:35,730 --> 01:08:39,810
and suppose we have two patterns g prime and g double prime

992
01:08:39,860 --> 01:08:44,270
both of them share the same graph pattern g

993
01:08:44,270 --> 01:08:47,650
and if the structure g-prime and g-double-prime

994
01:08:47,750 --> 01:08:50,130
is similar to each other then likely

995
01:08:50,150 --> 01:08:55,480
the objective score of g prime and g double prime will be similar to each other

996
01:08:55,520 --> 01:08:57,110
if we find that

997
01:08:57,110 --> 01:08:58,440
in this branch

998
01:08:58,460 --> 01:09:00,250
none of

999
01:09:00,290 --> 01:09:01,940
g primes children

1000
01:09:02,050 --> 01:09:05,020
can generate best objective score

1001
01:09:05,040 --> 01:09:06,340
there likely

1002
01:09:06,360 --> 01:09:07,440
this branch

1003
01:09:07,440 --> 01:09:08,400
will not

1004
01:09:08,420 --> 01:09:11,590
have the opportunity to generate the best possible score

1005
01:09:11,610 --> 01:09:13,340
what does this mean this means

1006
01:09:13,360 --> 01:09:18,630
as long as we finished this search tree search branch g prime it is not

1007
01:09:18,630 --> 01:09:20,900
necessary to search the

1008
01:09:23,480 --> 01:09:24,750
like g double prime

1009
01:09:24,770 --> 01:09:26,440
so we can cut to the search

1010
01:09:26,550 --> 01:09:29,320
branch to g double prime

1011
01:09:29,340 --> 01:09:32,650
as you can see that it is a horizontal pruning

1012
01:09:32,750 --> 01:09:36,840
so this is a major method that we propose in our past disagreement was

1013
01:09:36,840 --> 01:09:41,980
on the order paper propose master which can do not only are vertical pruning but

1014
01:09:42,130 --> 01:09:45,420
also it can do horizontal pruning

1015
01:09:45,440 --> 01:09:48,420
and we applied our method to

1016
01:09:48,440 --> 01:09:51,040
a set of chemical compounds in a sense

1017
01:09:51,130 --> 01:09:55,790
try to see its performance in comparison with the origin of graph frequent pattern mining algorithms

1018
01:09:55,790 --> 01:09:58,210
and vertical pruning methods

1019
01:09:58,270 --> 01:09:59,710
and we collected

1020
01:09:59,730 --> 01:10:03,940
eleven chemical compound datasets from NCI website

1021
01:10:03,960 --> 01:10:07,670
and this is the data sets containing each of them contain

1022
01:10:09,050 --> 01:10:12,460
more than solvents chemical compounds

1023
01:10:12,520 --> 01:10:16,040
if you are interested you can contact me later and and we have all these

1024
01:10:16,040 --> 01:10:18,650
datasets clean

1025
01:10:18,670 --> 01:10:26,000
and here's the performance comparison between our method BB means vertical pruning and leap means vertical

1026
01:10:26,000 --> 01:10:29,520
pruning plus horizontal pruning remember that

1027
01:10:29,540 --> 01:10:35,320
this vertical pruning is already better than the traditional frequent graph pattern mining algorithm

1028
01:10:35,340 --> 01:10:37,810
here is the different dataset. here is

1029
01:10:37,820 --> 01:10:39,820
a runtime comparison

1030
01:10:39,820 --> 01:10:45,810
equivalent the very obvious reason that the relations between a highly non linear

1031
01:10:45,820 --> 01:10:48,850
if it's principle

1032
01:10:48,890 --> 01:10:50,270
gene ontology

1033
01:10:50,270 --> 01:10:53,680
the attempt to as catalogue

1034
01:10:53,690 --> 01:11:00,110
what all the genes are doing will fail without higher level insight there are close

1035
01:11:01,080 --> 01:11:08,530
programs in existence to annotate the gene the them the most famous is the gene

1036
01:11:08,530 --> 01:11:14,250
ontology project g that by michael ashton in cambridge is very interesting to have a

1037
01:11:14,250 --> 01:11:17,260
look at the gene ontology website

1038
01:11:17,320 --> 01:11:19,030
you will find

1039
01:11:19,040 --> 01:11:23,450
that they exclude all forms of higher level

1040
01:11:26,050 --> 01:11:29,030
even to the extent of excluding the idea

1041
01:11:29,160 --> 01:11:34,410
but you can label certain genes oncogenes as cancer genes the gene ontology project there

1042
01:11:34,410 --> 01:11:40,190
are no cancer genes and very simple and correct point but surely that's not why

1043
01:11:40,190 --> 01:11:42,830
those genes are selected in the first place

1044
01:11:42,880 --> 01:11:46,580
and as they put it on their website cancer is not so

1045
01:11:46,670 --> 01:11:49,850
a selected function g

1046
01:11:51,550 --> 01:11:53,500
there also

1047
01:11:53,540 --> 01:11:55,020
on the website

1048
01:11:55,030 --> 01:11:57,750
an interesting list of exclusions

1049
01:11:57,760 --> 01:12:01,580
virtually the whole of physiology is excluded

1050
01:12:01,610 --> 01:12:06,720
and actually the whole of the evolutionary biology is excluded and the reason for that

1051
01:12:07,080 --> 01:12:12,070
i think they're right to exclusivity is the reason for that is that while it

1052
01:12:12,120 --> 01:12:17,030
is possible to annotate the genome in terms of safe which proteins are coded for

1053
01:12:17,550 --> 01:12:23,860
which genes and what the biochemical functions as those proteins might be will be they

1054
01:12:23,880 --> 01:12:30,420
enzyme or structural or whatever it's extremely difficult as those who try to reconstruct the

1055
01:12:30,420 --> 01:12:32,480
complexity of biological systems no

1056
01:12:32,580 --> 01:12:38,390
to go much further beyond their from the genome information itself the gene if is

1057
01:12:38,410 --> 01:12:40,390
not the book of life

1058
01:12:40,420 --> 01:12:41,760
or it is a book

1059
01:12:41,770 --> 01:12:46,100
then it's about because it was such a huge gaps in it you can't read

1060
01:12:47,500 --> 01:12:50,390
but i think it's better to get rid of that metaphore

1061
01:12:50,850 --> 01:12:56,200
and replace it with the idea that the gene is simply a database

1062
01:12:56,210 --> 01:13:04,540
six principle it follows from what i've said that it was a nice metaphor when

1063
01:13:04,540 --> 01:13:06,170
it was introduced by

1064
01:13:06,200 --> 01:13:08,260
mono and yet

1065
01:13:08,280 --> 01:13:13,990
way back in the sixties there is there are no genetic programs

1066
01:13:16,290 --> 01:13:20,970
chairman refers to the fact that i did simulation

1067
01:13:20,970 --> 01:13:24,910
of rhythm and the hard way back in nineteen sixty

1068
01:13:25,450 --> 01:13:28,480
and in those days you know you have to use

1069
01:13:31,360 --> 01:13:37,540
valve computer to do the reconstruction and a very very slow

1070
01:13:37,560 --> 01:13:39,730
and had feed paper tape

1071
01:13:39,740 --> 01:13:41,070
and later on

1072
01:13:41,090 --> 01:13:46,100
these cards your program course was a set of instructions on that page protection

1073
01:13:46,110 --> 01:13:47,720
that's the model

1074
01:13:47,730 --> 01:13:53,530
the monomaniacal putting nine because they were introducing the concept of the genetic program way

1075
01:13:53,530 --> 01:13:59,840
back in the sixties the idea that is some separate k

1076
01:13:59,870 --> 01:14:05,810
for program then determines what living organism does there is no separate

1077
01:14:05,820 --> 01:14:11,930
and there's a lovely quote here sorry moving on too quickly and just go back

1078
01:14:13,280 --> 01:14:19,330
OK beautiful book in big companies plant geneticist and his his book the art of

1079
01:14:19,330 --> 01:14:25,270
genes is an absolute delight worth reading organisms are not simply manufactured according to a

1080
01:14:25,270 --> 01:14:30,920
set of instructions there's no easy way to separate instructions press carrying out to distinguish

1081
01:14:30,920 --> 01:14:32,540
plan from execution

1082
01:14:32,710 --> 01:14:36,430
just a pacemaker them again as an example

1083
01:14:36,440 --> 01:14:38,330
the only thing that

1084
01:14:38,360 --> 01:14:43,390
that could conceivably be regarded as the program is the set of interactions to include

1085
01:14:43,390 --> 01:14:48,580
in the computer model between the genes coding for proteins that interact with the cell

1086
01:14:49,020 --> 01:14:52,170
potential to produce pacemaker is

1087
01:14:52,170 --> 01:14:57,670
but that interaction is the system itself is not appropriate for the system

1088
01:14:57,720 --> 01:15:02,350
the concept of the program is completely redundant and you can do the same with

1089
01:15:02,350 --> 01:15:07,010
nearly all of the biological functions try to think of about biological function for which

1090
01:15:07,010 --> 01:15:11,120
it is the case that there is a separate program code for that is not

1091
01:15:11,200 --> 01:15:12,520
the function itself

1092
01:15:15,820 --> 01:15:17,920
i'm going to show one two

1093
01:15:18,560 --> 01:15:24,820
simulations at a higher level to illustrate the point i made earlier on about multi

1094
01:15:24,820 --> 01:15:26,710
level functionality

1095
01:15:26,710 --> 01:15:29,250
OK hailed

1096
01:15:29,270 --> 01:15:31,960
so i will

1097
01:15:31,970 --> 01:15:33,310
talk some more

1098
01:15:33,330 --> 01:15:38,310
sometime about ontology design practices

1099
01:15:38,320 --> 01:15:44,970
this is also war that is made in the context of the new products and

1100
01:15:45,880 --> 01:15:50,530
so i have to thank the members of my newly found the semantic technology lab

1101
01:15:52,620 --> 01:15:54,470
and in particular valentina

1102
01:15:54,490 --> 01:15:59,220
so outline of talk briefly about the world of ontology design

1103
01:15:59,250 --> 01:16:05,020
and then the relation between design our logical layers are assumed

1104
01:16:05,040 --> 01:16:06,940
to operate in wind

1105
01:16:06,950 --> 01:16:08,840
representing ontologies

1106
01:16:09,790 --> 01:16:13,050
ontology design patterns

1107
01:16:13,110 --> 01:16:18,510
some simple issues and unit test that can be applied for use in design jointly

1108
01:16:18,510 --> 01:16:20,580
with evaluation

1109
01:16:20,590 --> 01:16:28,150
so the partisans were we use some examples that were recommended by me by john

1110
01:16:28,480 --> 01:16:35,560
last year about the semantic web community ontology was used is have been using also

1111
01:16:35,560 --> 01:16:39,120
this is the only last year

1112
01:16:39,980 --> 01:16:44,870
still still in this series and operating in this conference looking

1113
01:16:44,890 --> 01:16:50,840
so until as the world include requirements of course

1114
01:16:50,890 --> 01:16:55,090
so for example i want to attend my ideal talk k so what ontology it

1115
01:16:55,090 --> 01:16:56,030
is stable

1116
01:16:56,030 --> 01:17:01,580
two which can be used for knowledge base joined the knowledge base that can answer

1117
01:17:01,580 --> 01:17:02,530
my greedy

1118
01:17:02,550 --> 01:17:09,950
to be logical constructs like subclass of restriction as you've seen from shows tutorial

1119
01:17:09,980 --> 01:17:15,390
and the bunch of existing ontologies like friend of a friend the cause the semantic

1120
01:17:15,390 --> 01:17:18,450
web community ontology or

1121
01:17:18,500 --> 01:17:21,690
the foundation reusable ontologies like

1122
01:17:21,700 --> 01:17:29,090
and the informal knowledge resources like citeseer or this topic catalogue there not exactly ontologies

1123
01:17:29,090 --> 01:17:33,310
in the sense of being explained today about

1124
01:17:33,340 --> 01:17:36,300
they can be used as ontology this is the

1125
01:17:36,360 --> 01:17:38,450
part of the current practices

1126
01:17:38,460 --> 01:17:43,640
and a lot of conventions and practices for example how to name elements or to

1127
01:17:43,640 --> 01:17:47,550
make your eyes to to work in the semantic web of

1128
01:17:47,570 --> 01:17:50,270
how to make it was joined cowering in

1129
01:17:50,290 --> 01:17:52,760
you know or how to verify

1130
01:17:52,890 --> 01:17:57,290
relations that cannot be expressed in within this pursuit of all of a how to

1131
01:17:58,100 --> 01:18:03,640
things like a transitive part of the or not as the ones or relations between

1132
01:18:03,640 --> 01:18:10,330
roles and tasks or these conventions and practices belong to different areas of design and

1133
01:18:10,330 --> 01:18:16,910
will be treated separately and examples presented in this book and of course the tools

1134
01:18:16,920 --> 01:18:22,480
so i those reasons translators probably most of you have already been faced with many

1135
01:18:22,480 --> 01:18:29,010
interfaces the many function that are contained in current semantic web and web of data

1136
01:18:33,170 --> 01:18:38,390
talking about this i will talk about good practices are best practices and of course

1137
01:18:38,480 --> 01:18:44,860
is a very old question about holt is a well designed ontology pragmatically speaking i

1138
01:18:44,860 --> 01:18:47,110
think that after so many years of a

1139
01:18:47,200 --> 01:18:53,450
incredibly long discussions with people from all areas interdisciplinary contest and that

1140
01:18:53,910 --> 01:18:59,550
basic things could be singled out and should be true anyway so

1141
01:18:59,600 --> 01:19:00,890
given that you want to do

1142
01:19:00,920 --> 01:19:05,270
develop an ontology within a large organisation with a team of developers or you want

1143
01:19:05,270 --> 01:19:11,360
to develop it in anybody can like for example recently and was this launch of

1144
01:19:11,390 --> 01:19:16,820
and no campaign which can be used as backup but develop vocabularies or and they

1145
01:19:16,850 --> 01:19:20,700
want to develop by alone on the on the peak of the mountain OK i

1146
01:19:20,700 --> 01:19:25,790
think that this can questions should be anyway take into account so what are we

1147
01:19:25,790 --> 01:19:30,320
talking about why do we want to talk about it and where to find reusable

1148
01:19:30,320 --> 01:19:32,510
knowledge to talk about

1149
01:19:32,540 --> 01:19:37,360
and also maybe do we have the resources to maintain it so sometimes the sustainability

1150
01:19:37,360 --> 01:19:38,480
is an issue

1151
01:19:38,490 --> 01:19:42,960
so these were also the images that we have singled out in ontology social work

1152
01:19:42,960 --> 01:19:44,350
years ago

1153
01:19:44,380 --> 01:19:47,400
so this was wise words constitute the

1154
01:19:47,410 --> 01:19:50,610
it's called the problem space of an ontology project

1155
01:19:50,640 --> 01:19:54,480
and the designers need to find solutions from a solution space

1156
01:19:54,480 --> 01:19:57,620
after prime x

1157
01:19:57,630 --> 01:19:59,170
like crime

1158
01:20:01,960 --> 01:20:03,680
the derivatives

1159
01:20:03,690 --> 01:20:04,840
inside here

1160
01:20:04,850 --> 01:20:10,080
again is just wj prime

1161
01:20:10,100 --> 01:20:14,670
so the derivative with respect to

1162
01:20:17,870 --> 01:20:19,410
this song

1163
01:20:19,430 --> 01:20:22,830
it's going to be fj prime

1164
01:20:22,870 --> 01:20:27,900
for the and we just pick out whichever j we're talking about

1165
01:20:34,700 --> 01:20:38,020
x y prime

1166
01:20:51,130 --> 01:20:53,820
it's actually

1167
01:20:53,860 --> 01:20:55,440
a lot

1168
01:20:55,490 --> 01:21:00,550
no it can't because

1169
01:21:02,850 --> 01:21:06,430
what i can do is bring dizzy inside the cell

1170
01:21:06,470 --> 01:21:10,380
but i can't but i don't think i can

1171
01:21:10,380 --> 01:21:12,430
cancel it out

1172
01:21:12,460 --> 01:21:15,130
of my

1173
01:21:17,080 --> 01:21:19,140
this is

1174
01:21:19,150 --> 01:21:21,850
i'm going to write it like this some

1175
01:21:21,900 --> 01:21:26,440
of the life crime

1176
01:21:27,940 --> 01:21:29,840
the cell

1177
01:21:29,850 --> 01:21:33,160
the premises here like this

1178
01:21:33,170 --> 01:21:34,670
so i'm going to write

1179
01:21:36,040 --> 01:21:38,950
j x and y prime

1180
01:21:38,960 --> 01:21:43,350
just moving this first and then i'm going to move disease

1181
01:21:50,420 --> 01:21:53,600
and act

1182
01:21:53,650 --> 01:21:57,180
some of the j prime

1183
01:21:57,190 --> 01:22:01,210
wj prime j prime x

1184
01:22:01,230 --> 01:22:04,090
why prime

1185
01:22:04,930 --> 01:22:06,270
here i just

1186
01:22:06,500 --> 01:22:11,440
swap the order of these two things in the summer move dizzy inside

1187
01:22:15,390 --> 01:22:16,610
comes the

1188
01:22:16,630 --> 01:22:18,480
the interesting part

1189
01:22:18,510 --> 01:22:19,980
so i can

1190
01:22:20,010 --> 01:22:24,330
i can recognise this is being something well known

1191
01:22:24,330 --> 01:22:25,600
what is

1192
01:22:28,760 --> 01:22:30,800
we've seen

1193
01:22:30,850 --> 01:22:34,950
it's a p of why prime given x

1194
01:22:36,580 --> 01:22:39,100
why prime given x

1195
01:22:39,150 --> 01:22:41,470
and also

1196
01:22:42,660 --> 01:22:46,720
but the w is not important now

1197
01:22:52,780 --> 01:22:56,730
so now if i go back to here

1198
01:23:07,420 --> 01:23:15,460
divided every j log p

1199
01:23:17,220 --> 01:23:18,120
s j

1200
01:23:18,150 --> 01:23:20,170
x y

1201
01:23:24,180 --> 01:23:30,240
the expected value so this is an expectation

1202
01:23:36,700 --> 01:23:40,190
why prime

1203
01:23:44,180 --> 01:23:50,110
why prime follows the distribution p of y prime given x

1204
01:23:50,130 --> 01:23:52,230
and w

1205
01:23:52,860 --> 01:23:54,710
see this this is

1206
01:23:54,770 --> 01:24:00,190
the definition of an expectation is a weighted average of the FJ values for all

1207
01:24:00,190 --> 01:24:01,470
y crimes

1208
01:24:01,480 --> 01:24:07,700
with each why prime weighted by the probability that white pride so its expected value

1209
01:24:08,630 --> 01:24:11,200
f j x and y prime

1210
01:24:12,120 --> 01:24:13,430
what it's saying

1211
01:24:13,450 --> 01:24:14,540
is that

1212
01:24:14,550 --> 01:24:18,640
the observed value of f x y

1213
01:24:18,670 --> 01:24:21,370
for the

1214
01:24:21,420 --> 01:24:24,410
training example x y

1215
01:24:24,430 --> 01:24:25,820
has to equal

1216
01:24:25,820 --> 01:24:33,170
the expected value of the same feature functions where the white crimes all possible labels

1217
01:24:33,170 --> 01:24:37,560
and now i have three different microphones each of which is a different setting

1218
01:24:38,170 --> 01:24:42,680
no matter where i stand in the room we will either have feedback shadows

1219
01:24:42,700 --> 01:24:46,690
that until these guys fix it we hope that will get taken care of

1220
01:24:46,740 --> 01:24:49,230
and i'm hoping of course they will cut that out of the

1221
01:24:49,240 --> 01:24:51,050
webcasts will say

1222
01:24:52,120 --> 01:24:56,530
every every time we've done this either here or several of the times at the

1223
01:24:56,530 --> 01:24:58,420
summer school itself

1224
01:24:58,440 --> 01:25:01,370
i have had the privilege to be the person who give the talk which is

1225
01:25:01,370 --> 01:25:03,280
the introduction to

1226
01:25:03,290 --> 01:25:05,100
the introduction to

1227
01:25:05,160 --> 01:25:06,930
the semantic web tutorial

1228
01:25:08,670 --> 01:25:09,450
i guess

1229
01:25:09,460 --> 01:25:14,100
i used to think it was very flattering entire high-rise was just that i was

1230
01:25:14,100 --> 01:25:18,910
the oldest one had been doing it the longest so they figure get him up

1231
01:25:18,910 --> 01:25:20,240
front quickly

1232
01:25:20,250 --> 01:25:21,750
the way

1233
01:25:23,340 --> 01:25:26,820
this thing we've been calling the semantic web

1234
01:25:26,820 --> 01:25:29,280
has various different aspects comes

1235
01:25:29,310 --> 01:25:35,390
back and forth at various times

1236
01:25:35,410 --> 01:25:36,580
he has

1237
01:25:36,660 --> 01:25:39,380
is that set

1238
01:25:39,410 --> 01:25:42,660
you have to talk to him today

1239
01:25:42,670 --> 01:25:44,190
and this is the same

1240
01:25:44,580 --> 01:25:46,750
is this same slide you have in your hand

1241
01:25:53,160 --> 01:25:56,860
i sent john two or three different things i want to make sure we all

1242
01:25:56,860 --> 01:25:59,110
around using the same stack

1243
01:25:59,910 --> 01:26:00,800
they can't tell

1244
01:26:08,420 --> 01:26:12,360
second one and

1245
01:26:14,740 --> 01:26:18,110
the only difference between this slide back and what you've been handed out this one

1246
01:26:18,110 --> 01:26:21,220
should have the title introduction to the introduction to

1247
01:26:21,240 --> 01:26:24,140
and you should all just a introduction to

1248
01:26:24,770 --> 01:26:28,060
if i if i start showing slides

1249
01:26:28,820 --> 01:26:32,940
anyone who is trying to follow that has and i made worried by the fact

1250
01:26:32,940 --> 01:26:36,130
that john said you all have copies of the slide

1251
01:26:36,190 --> 01:26:39,540
and everyone not it but when i ask if this was the same size no

1252
01:26:39,540 --> 01:26:42,480
one has responded do you have copies of our slide

1253
01:26:42,530 --> 01:26:45,730
you electronic once again

1254
01:26:45,840 --> 01:26:47,780
there's is anyone actually had them opened

1255
01:26:47,790 --> 01:26:49,650
are you reading email

1256
01:26:51,560 --> 01:26:54,100
well then then it will matter because then you won't be able to tell if

1257
01:26:54,100 --> 01:26:56,500
i give you the same ones are not so then i'm fine

1258
01:26:58,680 --> 01:27:02,940
we we tend to teach this course

1259
01:27:02,940 --> 01:27:07,590
from the point of view something called ontologies which we will get into

1260
01:27:07,600 --> 01:27:09,350
a little bit in this talk

1261
01:27:09,450 --> 01:27:11,320
a lot in some later talks

1262
01:27:11,340 --> 01:27:14,200
and what i want to do is start to set up a little bit of

1263
01:27:16,250 --> 01:27:19,930
the the picture of how these things are being used on the web

1264
01:27:19,940 --> 01:27:24,600
because there really are several different ways things are being used and it's important to

1265
01:27:24,600 --> 01:27:26,470
understand some of that

1266
01:27:26,510 --> 01:27:31,220
the the talk i gave that i started talking about this was called

1267
01:27:31,250 --> 01:27:34,820
the the fellowship of the semantic web the two towers and i call it the

1268
01:27:34,820 --> 01:27:36,100
two towers

1269
01:27:36,100 --> 01:27:38,820
and you will see in the talk why

1270
01:27:38,820 --> 01:27:42,530
that is but it's essentially so there's really

1271
01:27:42,540 --> 01:27:46,350
if if we're careful there are many different views of ontology but you will see

1272
01:27:47,470 --> 01:27:51,350
things that really arise from two

1273
01:27:51,370 --> 01:27:57,120
two different views of the same technology and it's important sometimes to understand views

1274
01:27:57,130 --> 01:28:00,850
so that when the presenters show you things you can try to

1275
01:28:00,900 --> 01:28:04,010
make sure you are seeing it from the right point of view

1276
01:28:06,870 --> 01:28:09,430
the way i like to describe this slightly

1277
01:28:09,440 --> 01:28:11,530
it is that we have this very funny

1278
01:28:11,540 --> 01:28:14,180
ontological conundrum is a college

1279
01:28:14,190 --> 01:28:15,950
which is that

1280
01:28:15,970 --> 01:28:21,500
one of the things that has her adoption of the semantic web for while it's

1281
01:28:21,500 --> 01:28:24,880
not going quite well is also used but one of the things that was the

1282
01:28:24,880 --> 01:28:31,230
problem was two different viewpoint so what would happen if somebody would here talking say

1283
01:28:31,310 --> 01:28:34,510
oh i get it this is what i need for my company

1284
01:28:34,530 --> 01:28:37,180
and then they go to different talk to

1285
01:28:37,190 --> 01:28:38,650
and here's something was said

1286
01:28:38,660 --> 01:28:42,870
no no everything you learned that other talk was wrong you need this other thing

1287
01:28:42,880 --> 01:28:45,660
and they say wait wait wait now i'm confused

1288
01:28:45,720 --> 01:28:49,360
and very often that was hurting peoples adoption

1289
01:28:49,370 --> 01:28:54,180
and what started to change now is is becoming easier to find schools

1290
01:28:54,190 --> 01:28:55,670
and techniques

1291
01:28:55,680 --> 01:28:57,560
and tutorials

1292
01:28:57,580 --> 01:29:02,100
but still pretty much anyone you take will be from one point of view or

1293
01:29:02,100 --> 01:29:05,820
another and this one you will get to that you actually can see both points

1294
01:29:05,820 --> 01:29:07,560
of view as we go along

1295
01:29:07,560 --> 01:29:11,500
and part of the reason this is important is that both of these

1296
01:29:11,560 --> 01:29:14,570
are ways that people are using this technology

1297
01:29:14,580 --> 01:29:19,710
one which looks more like a traditional AI technology and one which looks more like

1298
01:29:19,710 --> 01:29:25,920
a traditional web technology so it's not so much surprising that technology that wants to

1299
01:29:25,930 --> 01:29:28,440
really come from the perspective of

1300
01:29:28,450 --> 01:29:31,060
artificial intelligence on the web

1301
01:29:31,090 --> 01:29:35,070
is going to have tension between those two very different things

1302
01:29:36,000 --> 01:29:37,810
so one of the

1303
01:29:37,830 --> 01:29:39,590
towers i talk about

1304
01:29:39,650 --> 01:29:43,650
is the tower of expressive ontology and some of the talks you'll here today are

1305
01:29:43,650 --> 01:29:46,560
about ontology engineering

1306
01:29:46,570 --> 01:29:49,690
and getting your ontology right

1307
01:29:49,710 --> 01:29:54,080
and i like to think of that surrounds tower from lord of the rings

1308
01:29:54,110 --> 01:29:56,810
the tremendous amount of power

1309
01:29:56,820 --> 01:30:01,460
but you got to have a lot of or works around the outside garden because

1310
01:30:01,460 --> 01:30:03,630
if a little hobbit cats

1311
01:30:03,650 --> 01:30:06,440
you know you can drop your wedding ring in the crack of doom and the

1312
01:30:06,440 --> 01:30:10,050
whole thing could collapse i'm assuming you know the story but now if you don't

1313
01:30:10,050 --> 01:30:13,120
know the story i don't know where you been living in the past

1314
01:30:13,130 --> 01:30:15,690
twenty years but roughly speaking

1315
01:30:15,690 --> 01:30:18,580
right very very powerful but

1316
01:30:18,590 --> 01:30:21,560
you need a lot of control and in reality

1317
01:30:21,570 --> 01:30:26,610
there lots of views of ontologies

1318
01:30:26,630 --> 01:30:29,650
i can actually put a number of different names out there

1319
01:30:29,700 --> 01:30:33,120
but that come from this point of view which says we want to make the

1320
01:30:34,480 --> 01:30:40,990
very very explicit very very carefully engineered using some very powerful tools

1321
01:30:42,260 --> 01:30:45,370
we have to also make sure that the information

1322
01:30:45,380 --> 01:30:48,890
is correct and this is the technical sense of correct

1323
01:30:49,810 --> 01:30:54,130
inconsistencies coming into logic based system

1324
01:30:54,140 --> 01:30:56,820
unless they are carefully controlled

1325
01:30:56,840 --> 01:31:00,880
can destroy the logic in traditional logic system that

1326
01:31:00,900 --> 01:31:04,080
was just if we tried to use pure logic

1327
01:31:04,090 --> 01:31:08,420
to do almost any kind of reasoning if one if you have a way you

1328
01:31:08,420 --> 01:31:09,890
can prove that

1329
01:31:09,920 --> 01:31:12,760
and you have a way you can prove the inverse of that

1330
01:31:12,790 --> 01:31:16,840
then you have a rule that says anything that causes you to believe x and

1331
01:31:16,840 --> 01:31:19,180
not x must be wrong

1332
01:31:19,190 --> 01:31:20,480
so given that

1333
01:31:20,500 --> 01:31:24,140
if i was to go out on the web and say hey this website once

1334
01:31:24,170 --> 01:31:29,550
obama to win the presidency on this website wants mccain to win the presidency in

1335
01:31:29,550 --> 01:31:30,500
the US

1336
01:31:33,820 --> 01:31:37,230
the fact is that those are not the same person

1337
01:31:38,700 --> 01:31:43,440
there is a contradiction in a sense if one of the more or less safe

1338
01:31:43,440 --> 01:31:46,770
as he will be the next president and he will be the next president of

1339
01:31:46,770 --> 01:31:48,570
both cannot be true

1340
01:31:48,580 --> 01:31:51,700
so it follows from that inconsistency

1341
01:31:51,710 --> 01:31:54,340
that you owe me a lot of money

1342
01:31:54,360 --> 01:31:57,710
right so first of all i feel free to put it in box by the

1343
01:31:57,710 --> 01:32:01,060
door as you leave today i'll happily take your contributions but

1344
01:32:01,070 --> 01:32:03,090
clearly as humans you would say

1345
01:32:03,110 --> 01:32:05,940
something is very broken in the system

1346
01:32:05,940 --> 01:32:07,870
graphical models

1347
01:32:07,900 --> 01:32:09,570
i hope most of you

1348
01:32:09,590 --> 01:32:11,870
i don't know a a lot about it because if not going to to be

1349
01:32:13,110 --> 01:32:14,100
what i'm doing

1350
01:32:14,140 --> 01:32:16,650
it's not too easy

1351
01:32:18,210 --> 01:32:20,200
i will first give a quick overview

1352
01:32:20,210 --> 01:32:23,550
what traffic among our standard i will present

1353
01:32:23,640 --> 01:32:25,580
directed graphical models

1354
01:32:25,620 --> 01:32:28,530
a specific kind of customers

1355
01:32:28,550 --> 01:32:31,780
we talk a bit about

1356
01:32:31,800 --> 01:32:36,460
conditional independence

1357
01:32:36,580 --> 01:32:39,040
so what are produced

1358
01:32:39,060 --> 01:32:42,140
graphical models the name of graphic

1359
01:32:42,140 --> 01:32:49,420
there are diagrams for representing the probability distribution over a set of random variables

1360
01:32:49,460 --> 01:32:56,780
is it's generic name for several kind of of

1361
01:32:56,790 --> 01:33:03,000
directed graphical models which are also called bayesian networks

1362
01:33:03,030 --> 01:33:05,270
the and directed graphical mothers

1363
01:33:05,280 --> 01:33:09,350
which are also called markov random fields

1364
01:33:09,380 --> 01:33:14,820
and the factor graph so that the directed graph can easily be transformed into

1365
01:33:14,830 --> 01:33:17,270
and directed graph so it's all

1366
01:33:17,280 --> 01:33:21,630
make it a different way of representing this probability distribution

1367
01:33:23,050 --> 01:33:26,560
nodes and links

1368
01:33:27,170 --> 01:33:32,080
and factor graph that was directed and undirected graphs

1369
01:33:32,130 --> 01:33:38,050
can be represented as a factor graph

1370
01:33:43,240 --> 01:33:53,920
so what are these are a graphical representation for

1371
01:33:56,030 --> 01:33:59,520
in fact there a way of

1372
01:33:59,530 --> 01:34:05,020
decomposing graphically to traffic

1373
01:34:05,030 --> 01:34:06,530
topographic view

1374
01:34:06,550 --> 01:34:09,960
on how to decompose the joint probability distribution

1375
01:34:09,970 --> 01:34:13,060
into product of factors

1376
01:34:13,080 --> 01:34:15,400
it's depending in the subset

1377
01:34:15,440 --> 01:34:17,600
of variables

1378
01:34:17,760 --> 01:34:23,600
having that is useful to understand how the variables are connected

1379
01:34:23,600 --> 01:34:24,850
to each other

1380
01:34:24,870 --> 01:34:25,680
that's work

1381
01:34:25,780 --> 01:34:28,910
usually called conditional independence

1382
01:34:28,910 --> 01:34:34,380
having this decomposition of course you see that it will simplify computations

1383
01:34:34,430 --> 01:34:37,690
inference and learning

1384
01:34:37,700 --> 01:34:41,650
and it's it's in fact the way of generalizing

1385
01:34:44,510 --> 01:34:48,530
this simplification this way of understanding

1386
01:34:48,560 --> 01:34:50,940
different mothers

1387
01:34:50,970 --> 01:34:53,440
new mothers you do we use this

1388
01:34:53,470 --> 01:34:56,220
they agrams

1389
01:34:56,230 --> 01:35:00,290
the first let's come back to their easy example

1390
01:35:00,340 --> 01:35:01,750
query remember

1391
01:35:01,820 --> 01:35:05,030
we have two boxes and experiment is selected

1392
01:35:05,030 --> 01:35:10,660
one block one box then pick fruit in the box and then replace different

1393
01:35:10,660 --> 01:35:14,530
and so we have these two random variables which are

1394
01:35:14,530 --> 01:35:16,910
the identity of the box and the

1395
01:35:16,910 --> 01:35:19,570
the identity of the foot feet

1396
01:35:23,150 --> 01:35:25,340
i have two ways of writing

1397
01:35:25,380 --> 01:35:27,750
of the composing the

1398
01:35:29,180 --> 01:35:33,980
joint probability of one books and one foot

1399
01:35:34,070 --> 01:35:38,910
or i can see as the probability of the books times the probability of different

1400
01:35:38,910 --> 01:35:40,040
given the box

1401
01:35:40,060 --> 01:35:41,060
i can see it

1402
01:35:41,060 --> 01:35:44,410
as the probability of the four times the probability of the

1403
01:35:44,440 --> 01:35:46,320
books given different

1404
01:35:46,320 --> 01:35:49,350
however this first one

1405
01:35:50,090 --> 01:35:53,790
it's much much more natural in the way

1406
01:35:53,810 --> 01:35:56,820
the experiment is his career

1407
01:35:58,650 --> 01:36:01,100
it's also the kind of of

1408
01:36:01,150 --> 01:36:03,410
we have this going this

1409
01:36:03,450 --> 01:36:05,680
this conditional distribution of y

1410
01:36:05,690 --> 01:36:08,190
this one we could compute them but

1411
01:36:08,370 --> 01:36:11,470
it's not what we have in the first place

1412
01:36:11,970 --> 01:36:12,820
so that

1413
01:36:12,840 --> 01:36:17,160
we give this kind of graphical models in which we have

1414
01:36:17,280 --> 01:36:21,350
one not for the book for the identity of the books one

1415
01:36:22,150 --> 01:36:25,320
note for the the eighteenth of different

1416
01:36:26,300 --> 01:36:28,570
link is a direct link between

1417
01:36:28,580 --> 01:36:31,130
these two

1418
01:36:31,130 --> 01:36:34,010
kind of representing

1419
01:36:34,020 --> 01:36:39,190
that the the fruits are a product of the box that

1420
01:36:39,200 --> 01:36:43,130
let's take a more dramatic example

1421
01:36:44,770 --> 01:36:45,500
let's say

1422
01:36:45,500 --> 01:36:48,440
male one female one friend one can

1423
01:36:48,860 --> 01:36:53,440
family member so full people in each other three main circles fifteen fifty eight hundred

1424
01:36:53,460 --> 01:36:54,630
fifty and right in this way

1425
01:36:57,480 --> 01:37:01,190
the key things that interest here is the more things to sharing common the top graph

1426
01:37:02,500 --> 01:37:09,340
the more emotionally close to filter somebody and indeed people in they close layers to share more

1427
01:37:10,510 --> 01:37:11,900
traits with these types

1428
01:37:12,770 --> 01:37:16,250
you also more altruistic towards and here's a very simple one would you donate a

1429
01:37:16,250 --> 01:37:21,300
kidney to them irrespective layer the more try to sharing common more willing you want

1430
01:37:21,300 --> 01:37:22,130
to to to

1431
01:37:22,670 --> 01:37:24,050
be altruistic towards them

1432
01:37:25,270 --> 01:37:29,030
it turns out there are six key sector interesting on these

1433
01:37:29,460 --> 01:37:30,840
or personality traits

1434
01:37:31,480 --> 01:37:34,710
they're all things that are culturally determined and therefore change with time

1435
01:37:35,510 --> 01:37:37,730
and that's probably explains why friendships

1436
01:37:38,750 --> 01:37:43,900
gradually drift apart because over time you don't see each other ure interest change their things like

1437
01:37:44,480 --> 01:37:48,030
sharing the same language coming from the same place showing the same hobbies

1438
01:37:48,820 --> 01:37:50,280
and interest having the same

1439
01:37:50,750 --> 01:37:54,190
will view the same political views same religion those kinds things

1440
01:37:57,090 --> 01:37:58,460
there's one more and then i forgot what it

1441
01:37:58,980 --> 01:38:03,010
it is for the moment but the key thing the six one is human and

1442
01:38:03,150 --> 01:38:05,710
shared humour turns out to be the most important

1443
01:38:06,170 --> 01:38:07,130
these six core

1444
01:38:08,880 --> 01:38:10,610
dimensions a french if you like

1445
01:38:11,480 --> 01:38:13,750
and just to show you how important that is we then

1446
01:38:16,820 --> 01:38:18,800
what is collections one hundred best jokes

1447
01:38:19,250 --> 01:38:20,800
we have various people rate them

1448
01:38:21,380 --> 01:38:22,730
we took be twenty

1449
01:38:23,610 --> 01:38:26,150
jokes that people disagreed about most

1450
01:38:27,360 --> 01:38:31,480
am then had a large sample on the internet rate them before

1451
01:38:32,230 --> 01:38:36,590
ah their preference and do like this joke or not simple yes no

1452
01:38:39,940 --> 01:38:41,750
about two weeks later we went back to them

1453
01:38:42,380 --> 01:38:45,730
and said oh by the way we've got somebody here with their and his

1454
01:38:46,610 --> 01:38:50,110
that's your job profile profile is his their joke profile

1455
01:38:51,840 --> 01:38:54,110
you know who are would you be interested in

1456
01:38:54,920 --> 01:38:57,860
meeting them with a kind person you'd you'd be happy to go

1457
01:38:58,650 --> 01:39:00,960
you spend an evening with all or what have you

1458
01:39:02,170 --> 01:39:05,690
what they didn't know was that the joke profile being given back was actually the

1459
01:39:05,690 --> 01:39:08,650
rather but in a place that i the shared to activating

1460
01:39:09,130 --> 01:39:12,630
jokes or after sixteen activating jokes the same as the

1461
01:39:13,500 --> 01:39:13,960
you can see

1462
01:39:14,880 --> 01:39:18,280
the more jokes the most similar your attack profiles are

1463
01:39:18,800 --> 01:39:19,300
the more

1464
01:39:19,920 --> 01:39:25,750
you are likely to feel like person and indeed the more altruistic you say would

1465
01:39:25,750 --> 01:39:27,570
be towards them if you asked me

1466
01:39:28,010 --> 01:39:28,880
altruistic toward

1467
01:39:30,590 --> 01:39:33,420
okay so what actually makes relationships work

1468
01:39:34,090 --> 01:39:36,980
the model we work on this one derive from primates

1469
01:39:37,420 --> 01:39:39,960
primate social relationships appear to be able to

1470
01:39:40,570 --> 01:39:44,770
process mechanism they depend partly on an emotionally intense component which is

1471
01:39:45,210 --> 01:39:46,190
produced by grooming

1472
01:39:46,920 --> 01:39:51,360
explain one-minute and then a cognitive component which is where the brain size comes into

1473
01:39:51,400 --> 01:39:53,210
play so essentially what is happening is

1474
01:39:53,730 --> 01:39:56,780
you have grooming producing estimation intense component

1475
01:39:57,250 --> 01:39:59,780
which provides a kind of psychological platform

1476
01:40:00,780 --> 01:40:06,090
of which you can build a combative relationship a relationship trust and and obligations so

1477
01:40:06,530 --> 01:40:07,340
looking into

1478
01:40:08,050 --> 01:40:08,630
very quickly

1479
01:40:09,920 --> 01:40:13,980
cognitive that first what we understand about the cognitive component is this kind of stuff

1480
01:40:15,110 --> 01:40:21,730
so call mind-reading or mentalizing ability to understand what somebody else is thinking and how they're seeing the world

1481
01:40:23,380 --> 01:40:26,030
and this sort of forms a natural hierarchy

1482
01:40:27,320 --> 01:40:28,510
reflexive hierarchy

1483
01:40:29,190 --> 01:40:32,920
where jack here on the left is in first order intentionality as is known

1484
01:40:33,570 --> 01:40:36,650
he believes that something is the case being a general

1485
01:40:37,050 --> 01:40:40,380
philosophical term four mind-states belief states as were

1486
01:40:40,860 --> 01:40:47,000
including things like intentions like second order she believes that at least some case jackson

1487
01:40:47,000 --> 01:40:49,420
third automatically believes you will is actually

1488
01:40:49,980 --> 01:40:52,050
something's gotta principle this goes on forever

1489
01:40:53,610 --> 01:40:54,500
the second order

1490
01:40:55,320 --> 01:40:55,920
level here

1491
01:40:57,030 --> 01:40:57,900
the above jill

1492
01:40:58,460 --> 01:41:03,690
is known as the mind it's when he finally start to understand somebody else's mind is different to yours

1493
01:41:04,190 --> 01:41:09,590
that's say they may see the world in a different way children go through the benchmark about page five

1494
01:41:10,460 --> 01:41:13,690
that's the point at which they finally learn how to lie convincingly

1495
01:41:14,170 --> 01:41:16,820
because they understand how information they feed u

1496
01:41:17,530 --> 01:41:22,130
will affect your behavior prior to that i don't understand why you behave the way you do

1497
01:41:24,380 --> 01:41:26,210
so when we fast people

1498
01:41:26,860 --> 01:41:28,420
these are normally done little vignettes

1499
01:41:28,880 --> 01:41:32,980
little stories about people trying to make a date or negotiate something

1500
01:41:35,480 --> 01:41:40,460
and then you can ask you know about these stories about questions about the mind

1501
01:41:40,460 --> 01:41:43,320
states as individuals involved in the story when you do that

1502
01:41:43,960 --> 01:41:49,500
find typically at a small models have an upper limit out this order intentionality

1503
01:41:52,360 --> 01:41:54,380
three orders about what five olds can do

1504
01:41:55,280 --> 01:41:56,570
that's quite complicated

1505
01:41:57,420 --> 01:42:00,460
but more interestingly when you're also of the same people

1506
01:42:01,030 --> 01:42:02,300
did to list out there

1507
01:42:02,780 --> 01:42:04,150
in a core clique size

1508
01:42:04,690 --> 01:42:10,460
he finds a nice if messy correlation between the two we replicated this several times now so it's quite robust

1509
01:42:11,280 --> 01:42:14,130
the later on messy it's highly significant it's messy

1510
01:42:14,570 --> 01:42:15,250
it's messy because

1511
01:42:15,960 --> 01:42:16,730
sex difference

1512
01:42:17,170 --> 01:42:19,570
women are better unintentional these and these social

1513
01:42:20,030 --> 01:42:21,090
cognitive abilities

1514
01:42:21,880 --> 01:42:25,980
mind-reading abilities they also tend to have slightly bigger clique sizes as a result

1515
01:42:26,650 --> 01:42:29,610
there are also personality differences and this extroverts

1516
01:42:30,030 --> 01:42:30,380
tend be

1517
01:42:31,050 --> 01:42:32,360
have bigger clique sizes

1518
01:42:32,780 --> 01:42:33,750
than introverts

1519
01:42:35,110 --> 01:42:41,460
we knew what people high on neuroticism scale have smaller clique sizes than those who have

1520
01:42:41,570 --> 01:42:43,650
school level on the neuroticism scale

1521
01:42:44,480 --> 01:42:45,130
but the point is

1522
01:42:46,130 --> 01:42:49,090
is a nice correlation between the two and essentially what you've got here

1523
01:42:49,770 --> 01:42:52,860
is on the one access bitter behavior on the x axis

1524
01:42:53,400 --> 01:42:54,190
a bit software

1525
01:42:54,650 --> 01:42:55,630
let's say cognition

1526
01:42:56,320 --> 01:42:57,570
correlation between cognition

1527
01:42:58,130 --> 01:42:58,780
in the software

1528
01:42:59,360 --> 01:43:00,770
and the behavior yet what

1529
01:43:00,770 --> 01:43:04,950
have my friends enemies my so i would put the miners right and so on

1530
01:43:04,970 --> 01:43:06,160
and i could labels

1531
01:43:06,180 --> 01:43:11,450
what do i think about what kind of time with for each of these sixteen

1532
01:43:11,450 --> 01:43:13,040
different cases

1533
01:43:13,060 --> 01:43:16,600
OK and if i do this and then ask okay how often does do what

1534
01:43:16,600 --> 01:43:21,220
does this theory make correct predictions it makes a correct predictions made out of sixteen

1535
01:43:21,220 --> 01:43:26,500
cases and for example it's interesting that in this first case where i have pluses

1536
01:43:26,540 --> 01:43:32,240
so i have four months of the form where i have to process and the

1537
01:43:32,240 --> 01:43:36,770
predictive class this is how people behave in this case but in this case where

1538
01:43:37,180 --> 01:43:42,520
you have the same network structure just adjust their directions of people in the was

1539
01:43:42,520 --> 01:43:47,070
their actions and science of class because they don't to point to them to put

1540
01:43:47,070 --> 01:43:50,950
the minus here which is which is something that is in contradiction with what structure

1541
01:43:50,950 --> 01:43:53,330
that's so what they want to

1542
01:43:53,390 --> 01:43:57,270
say is you know is that a better explanation and as can see that fuel

1543
01:43:57,350 --> 01:44:02,680
that this provides a much better explanation about how people can create signed edges in

1544
01:44:03,220 --> 01:44:08,020
in this media sites so that's that's the that's the that's the plan so now

1545
01:44:08,020 --> 01:44:12,060
what i want to show you how to establish the yes studios that this is

1546
01:44:12,060 --> 01:44:13,580
a better explanation

1547
01:44:13,640 --> 01:44:18,890
OK so here is here is our experimental settings so the way to do this

1548
01:44:18,890 --> 01:44:22,500
is i will assume that links are directed and created over time

1549
01:44:22,980 --> 01:44:26,680
i have the status theory with which the the way i believe what is say

1550
01:44:26,680 --> 01:44:30,410
a user plus to someone of higher status and a gives the minus to someone

1551
01:44:30,410 --> 01:44:31,750
of lower status

1552
01:44:31,770 --> 01:44:37,700
and as a said before the two the two theories can give different predictions

1553
01:44:37,720 --> 01:44:41,980
the way i think about this now is to say i had a is about

1554
01:44:41,980 --> 01:44:43,180
to evaluate the

1555
01:44:43,200 --> 01:44:46,870
this is how do edge a b is embedded in the network that they may

1556
01:44:46,870 --> 01:44:50,620
maybe this common for indexing and this is the fraction of edges and science of

1557
01:44:50,620 --> 01:44:52,290
edges towards x how

1558
01:44:52,680 --> 01:44:54,950
hi what happened to this page

1559
01:44:54,950 --> 01:44:56,600
right so

1560
01:44:56,620 --> 01:45:00,080
so to be able to establish is there there are two things that they need

1561
01:45:00,080 --> 01:45:04,060
to formalize the first link the first thing to formalize that sort of realised that

1562
01:45:04,330 --> 01:45:08,620
links so these red links that i'm interested in are embedded in the network right

1563
01:45:08,620 --> 01:45:11,750
so they are embedded in this kind of try trying to write so every common

1564
01:45:11,750 --> 01:45:16,270
neighbors between a and b create some kind of embedding of that link in the

1565
01:45:16,270 --> 01:45:20,250
network so the first thing i want to know is how this people's behavior change

1566
01:45:20,250 --> 01:45:24,250
depending on if i know how the structure of the of this context is well

1567
01:45:24,290 --> 01:45:28,560
justified nor the context OK so i want to say this is my based behavior

1568
01:45:28,560 --> 01:45:32,470
when i don't really care what are the directions and size of these links and

1569
01:45:32,470 --> 01:45:36,020
i say what what is a doing in this case one says if i conditioned

1570
01:45:36,020 --> 01:45:40,750
on the context OK i conditioning what is the directions and signs of of the

1571
01:45:40,750 --> 01:45:44,020
edges towards a common friend between

1572
01:45:44,040 --> 01:45:49,530
so so that the first thing so try triads provide context for the region is

1573
01:45:49,530 --> 01:45:53,850
at science created and the second thing is that users are considered genius in the

1574
01:45:53,850 --> 01:45:57,930
early evening became what i mean by that is that some people are are just

1575
01:45:57,930 --> 01:46:02,140
overall more positive and some people are overall more negative so i want to sort

1576
01:46:02,140 --> 01:46:06,100
of instead of saying what people do i want to see how the behavior change

1577
01:46:06,100 --> 01:46:08,310
from the baseline behavior OK

1578
01:46:08,980 --> 01:46:15,100
first about context as i said before there are sixteen different contexts in which a

1579
01:46:15,100 --> 01:46:21,870
sort can appear either sixteen different labelled directed private lives in which i think i

1580
01:46:21,870 --> 01:46:23,200
don't think can appear

1581
01:46:23,200 --> 01:46:27,500
and this is basically out tells made there sort sort of sixteen different ways on

1582
01:46:27,500 --> 01:46:31,140
which i can condition my behaviour i can sort of ignore the context and there

1583
01:46:31,200 --> 01:46:34,250
are sixteen different ways in which

1584
01:46:34,270 --> 01:46:37,350
the link between a and b that you are interested in x

1585
01:46:37,370 --> 01:46:39,020
and now

1586
01:46:39,020 --> 01:46:42,690
so sort of the second thing is how do i address the originating the behavior

1587
01:46:42,690 --> 01:46:47,060
of users so the point is that some users are more positive and some users

1588
01:46:47,060 --> 01:46:51,770
are less positive and some users are more popular some users are less popular so

1589
01:46:52,060 --> 01:46:54,180
i will call these baselines and scenario

1590
01:46:54,180 --> 01:46:58,890
introduce a notion of generative baseline which is like how what fraction of all the

1591
01:46:58,890 --> 01:47:05,290
edges are created plus and i will also introduce generic also the receptive baseline which

1592
01:47:05,290 --> 01:47:10,160
is what is what fraction of all users receive a plus right so generative is

1593
01:47:10,250 --> 01:47:15,200
you're you're positive out of reflection in receptive is what fraction of

1594
01:47:15,250 --> 01:47:19,560
of edges that you received positive right so one is what you create and the

1595
01:47:19,560 --> 01:47:23,390
other one is what you see hot so i have this so now i want

1596
01:47:23,390 --> 01:47:28,020
to say OK how do different link contexts cause users to deviate from these baselines

1597
01:47:28,020 --> 01:47:31,680
right so what you want to say is if i don't know what is what

1598
01:47:31,680 --> 01:47:32,890
is the context in which

1599
01:47:32,910 --> 01:47:34,350
in which is something appears

1600
01:47:34,370 --> 01:47:38,540
what's the matter was the probability of a giving up blast and was the probability

1601
01:47:38,540 --> 01:47:41,100
of receiving a plus one says if i know

1602
01:47:41,120 --> 01:47:43,740
that this edge will be created

1603
01:47:43,790 --> 01:47:47,930
it is embedded in the network in this kind of structure what happens to a

1604
01:47:47,930 --> 01:47:50,600
giving up giving a positive evaluation

1605
01:47:50,620 --> 01:47:54,680
and what happens to be receiving a positive evaluation so this is what i would

1606
01:47:54,680 --> 01:47:59,640
like to like to call a sort of capture is that basically this that because

1607
01:47:59,640 --> 01:48:04,700
of links appear in the context this makes nodes a and b deviates from the

1608
01:48:04,700 --> 01:48:10,930
generative and receptive baseline right so eight gen deviate from its generative baselines and the

1609
01:48:11,970 --> 01:48:19,080
we deviate from its receptive baseline OK so that's so here are two two basic

1610
01:48:19,100 --> 01:48:23,780
examples right so how would i expect people to deviate here so for example if

1611
01:48:23,780 --> 01:48:28,120
this is something that happens in status theory is right there before my prediction was

1612
01:48:28,120 --> 01:48:32,700
that this one minus right so it would never to x x points negative one

1613
01:48:32,700 --> 01:48:37,740
so i would imagine that a good evaluation may be negatively so i would imagine

1614
01:48:37,740 --> 01:48:42,750
that the deviation of a would be the generative deviation of would be negative likely

1615
01:48:42,770 --> 01:48:47,200
to be more negative than what he usually is right from his baseline and also

1616
01:48:47,370 --> 01:48:49,870
is now receiving the negative and so we would expect the

1617
01:48:50,060 --> 01:48:57,140
deviations in receptive baseline of is also negative right and gives us some sort of

1618
01:48:57,140 --> 01:48:59,950
similar example where again i would expect deviations

1619
01:49:00,020 --> 01:49:04,390
of a big negative deviation of the right so here it would be more likely

1620
01:49:04,390 --> 01:49:08,290
to give the miners then what the ring comparing to his very long baseline and

1621
01:49:08,290 --> 01:49:12,950
they would also be more likely to receive minus minus compared to talk is based

1622
01:49:13,000 --> 01:49:17,890
OK so now how do i say that something is how do i make predictions

1623
01:49:17,890 --> 01:49:22,330
based resource that is still sort of this was implicit throughout the talk so far

1624
01:49:22,330 --> 01:49:25,310
so i will just make it more it's more precise right so the way i

1625
01:49:25,310 --> 01:49:29,890
mean this is a hard i've been assigned status of zero to x now i

1626
01:49:29,890 --> 01:49:35,910
say based on the directions in science of researchers assigned stepsister and here that has

1627
01:49:35,910 --> 01:49:40,200
led to that the zero he puts positive points positive weight so he has higher

1628
01:49:40,200 --> 01:49:45,640
standards than x so that's a plus one x points negatively to to to be

1629
01:49:45,640 --> 01:49:46,390
which means

1630
01:49:46,410 --> 01:49:53,180
x because negative status minus one so my prediction here would be the the direction

1631
01:49:53,180 --> 01:49:57,330
of the the sign of this actually being write his high status because

1632
01:49:57,350 --> 01:50:01,480
right now i would say that

1633
01:50:01,480 --> 01:50:03,480
the probability one two

1634
01:50:03,510 --> 01:50:06,450
it's only one possible outcome that would be one six

1635
01:50:06,470 --> 01:50:10,810
and then i have to divide by the probability of n being even

1636
01:50:10,810 --> 01:50:16,800
so that's that's three six about it then the conditional probability to all three given

1637
01:50:16,800 --> 01:50:19,860
that and is even would be one thirty

1638
01:50:19,880 --> 01:50:23,330
OK well actually do quite a bit with conditional probability

1639
01:50:23,340 --> 01:50:26,820
in the in the next four days

1640
01:50:26,930 --> 01:50:28,950
OK closely related property

1641
01:50:28,970 --> 01:50:31,560
it is the property of independence

1642
01:50:31,570 --> 01:50:34,430
two subsets are said to be independent

1643
01:50:34,440 --> 01:50:39,260
if the probability that the outcome is in a and in b so the intersection

1644
01:50:39,360 --> 01:50:44,940
factorizes into the probability for a time the probability for b

1645
01:50:44,950 --> 01:50:49,530
by this time thinking oh i all these definitions what what does that have to

1646
01:50:49,530 --> 01:50:51,480
do with independence

1647
01:50:51,500 --> 01:50:55,510
and you can see somehow that's related to the idea of independence if you look

1648
01:50:55,510 --> 01:50:57,190
at the conditional probability

1649
01:50:57,250 --> 01:50:58,940
for a given the

1650
01:50:58,960 --> 01:51:01,750
now if a and b are independent

1651
01:51:01,770 --> 01:51:03,430
that means that was

1652
01:51:03,430 --> 01:51:05,050
o thing in the numerator

1653
01:51:05,060 --> 01:51:09,510
factorizes into the probability of a times the probability of b

1654
01:51:09,530 --> 01:51:11,920
and the probability of the councils

1655
01:51:11,930 --> 01:51:15,900
and i simply get back the probability of a so what that means is that

1656
01:51:17,000 --> 01:51:18,710
the condition of b

1657
01:51:18,730 --> 01:51:22,910
i had no influence on the probability for a so that you can kind of

1658
01:51:22,910 --> 01:51:27,010
see that this word independence makes sense and you can easily show that it works

1659
01:51:27,180 --> 01:51:32,000
better as well as the probability of b given a would simply be the probability

1660
01:51:32,000 --> 01:51:32,810
of b

1661
01:51:32,830 --> 01:51:37,780
imposing the condition would make no difference if the two subsets are independent

1662
01:51:37,800 --> 01:51:43,500
so don't confuse that with independence with disjoint is the difference it's that's not the

1663
01:51:43,500 --> 01:51:49,600
same thing as having two disjoint subsets that means that the intersection doesn't overlap

1664
01:51:49,610 --> 01:51:52,850
the intersections and also

1665
01:51:52,860 --> 01:51:54,310
OK let's keep going

1666
01:51:54,330 --> 01:51:59,530
now up to now there has been relatively abstract definition i haven't insisted on what

1667
01:51:59,540 --> 01:52:05,830
elements of the subspace sample space have to represent and i haven't even specified

1668
01:52:05,910 --> 01:52:09,150
in this sort of prescription for assigning numerical values

1669
01:52:09,170 --> 01:52:13,180
to the probabilities in order to do that you have to say something about the

1670
01:52:13,180 --> 01:52:15,690
probability means how you interpret it

1671
01:52:15,700 --> 01:52:19,840
and here is more than one school of thought there are more than one way

1672
01:52:19,840 --> 01:52:22,830
of interpreting probability and the two

1673
01:52:22,850 --> 01:52:24,860
important ways

1674
01:52:24,880 --> 01:52:26,760
and those are

1675
01:52:26,770 --> 01:52:31,410
roughly speaking calls frequentist probability subjective probability

1676
01:52:31,500 --> 01:52:38,770
and and those are the basis of two in many ways compete competing schools statistical

1677
01:52:39,890 --> 01:52:43,330
and will spend some time talking about each

1678
01:52:43,350 --> 01:52:44,960
so let me introduce these two

1679
01:52:44,970 --> 01:52:52,320
interpretations when data interpret the probability is to say that the elements of the sample

1680
01:52:52,320 --> 01:52:55,260
space are the outcomes of a repeatable experiment

1681
01:52:55,280 --> 01:52:56,320
and in

1682
01:52:56,330 --> 01:53:02,290
limit i repeat the experiment a very large number of times the fraction of times

1683
01:53:02,290 --> 01:53:05,070
that the outcome is in the sense that a

1684
01:53:05,190 --> 01:53:09,220
is that's what i mean by the probability of a

1685
01:53:09,280 --> 01:53:09,930
now what

1686
01:53:09,940 --> 01:53:11,800
mathematicians look at this limits

1687
01:53:11,810 --> 01:53:13,430
here i think that's

1688
01:53:13,530 --> 01:53:17,160
that has nothing to do with the proper mathematical limit how can you repeat

1689
01:53:17,170 --> 01:53:19,500
an experiment an infinite number of times

1690
01:53:19,510 --> 01:53:24,970
but as this is the basic idea of probability that we have in mind for

1691
01:53:24,970 --> 01:53:32,460
example in quantum mechanics in the ordinary copenhagen interpretation of quantum mechanics and certainly natural

1692
01:53:32,460 --> 01:53:34,630
in particle physics because we had this idea

1693
01:53:34,760 --> 01:53:35,800
but you can

1694
01:53:35,810 --> 01:53:37,120
collide together

1695
01:53:38,320 --> 01:53:41,320
as many times as you want it doesn't appear to be any natural limit to

1696
01:53:41,320 --> 01:53:46,390
the number of times that you could repeat such an experiment or similarly with radioactive

1697
01:53:46,390 --> 01:53:50,380
decay if you have a very very large number of radio of of nuclear

1698
01:53:50,400 --> 01:53:56,310
and then repeating observation many times seems very natural

1699
01:53:56,310 --> 01:53:58,830
there other situations however in which

1700
01:53:58,840 --> 01:54:02,230
being able to repeat observation seems highly contrived

1701
01:54:02,900 --> 01:54:04,090
the probability

1702
01:54:05,010 --> 01:54:06,980
it's going to rain tomorrow

1703
01:54:06,990 --> 01:54:09,590
the probability that what i just read

1704
01:54:09,620 --> 01:54:13,330
what is the probability that the first person to go to mars will come back

1705
01:54:14,460 --> 01:54:20,060
he he or she is will about and you can repeat that somehow

1706
01:54:22,120 --> 01:54:27,140
it is natural under certain circumstances to define what is called subjective probability

1707
01:54:27,970 --> 01:54:34,630
in this game the elements of the sample space of hypotheses and here we have

1708
01:54:34,630 --> 01:54:37,860
a hypothesis i simply mean a statement that is either true

1709
01:54:37,880 --> 01:54:39,160
or false

1710
01:54:39,170 --> 01:54:44,810
and then sometimes in the sample space in this context is called the hypothesis space

1711
01:54:44,810 --> 01:54:49,410
and the probability of a certain subset is simply

1712
01:54:49,420 --> 01:54:51,600
interpreted as the degree of belief

1713
01:54:51,640 --> 01:54:54,600
that hypothesis is true

1714
01:54:54,610 --> 01:54:57,310
now both interpretations are consistent

1715
01:54:57,330 --> 01:55:02,380
with the kolmogorov axioms so there are legitimate uses of probability

1716
01:55:02,390 --> 01:55:06,950
and i think it's fair to say that in particle physics the frequency interpretation is

1717
01:55:06,950 --> 01:55:08,700
is most often used

1718
01:55:08,710 --> 01:55:12,200
but subjective probability can nevertheless

1719
01:55:12,250 --> 01:55:19,020
provide a more natural interpretation for for many things are for for systematic uncertainties for

1720
01:55:19,020 --> 01:55:24,080
the things like what is the probability that the higgs boson exists would either does

1721
01:55:24,080 --> 01:55:28,140
or doesn't no repetition of of any experiment will change that

1722
01:55:29,380 --> 01:55:31,150
last week i was at a meeting

1723
01:55:31,160 --> 01:55:36,250
of of cosmologists who were interested in in statistical matters and then it was interesting

1724
01:55:36,250 --> 01:55:37,650
that although

1725
01:55:37,690 --> 01:55:43,420
particle physicists more often use the frequency interpretation apparently for cosmologists seek is the exact

1726
01:55:43,420 --> 01:55:47,970
opposite that they are more often use subjective probability in fact if one universal work

1727
01:55:47,970 --> 01:55:51,410
with i think that's the basis

1728
01:55:51,420 --> 01:55:54,550
OK now regardless of which interpretation

1729
01:55:54,560 --> 01:55:58,140
you use anything called the probability has to obey

1730
01:55:58,150 --> 01:55:59,510
an important theorem

1731
01:55:59,510 --> 01:56:01,470
called bayes there

1732
01:56:01,490 --> 01:56:05,050
and is there is very easy to derive so let's just do it

1733
01:56:05,080 --> 01:56:12,250
we have from the definition of conditional probability is the conditional probability of a given

1734
01:56:12,250 --> 01:56:16,120
i see is one bigger less than three it smaller so i put it over

1735
01:56:17,480 --> 01:56:21,870
when i insert eight that's bigger than three to get some relief over here

1736
01:56:21,920 --> 01:56:26,660
and the answer to that it's between one and three so it would fall off

1737
01:56:26,660 --> 01:56:30,410
the right child of one i had two there

1738
01:56:31,430 --> 01:56:33,290
beginning three less than eight

1739
01:56:34,620 --> 01:56:36,210
it's here

1740
01:56:37,110 --> 01:56:40,340
bigger than three less than a bigger than six

1741
01:56:40,380 --> 01:56:42,470
so it goes here

1742
01:56:44,170 --> 01:56:45,960
it's in between three and five

1743
01:56:45,970 --> 01:56:49,100
three insects rather

1744
01:56:49,150 --> 01:56:51,700
so that's the binary search tree that i get

1745
01:56:51,720 --> 01:56:53,880
then i run in order to reversal

1746
01:56:53,900 --> 01:56:56,910
which will present one two three

1747
01:56:56,950 --> 01:56:58,670
five six seven

1748
01:56:59,600 --> 01:57:03,480
can run that quickly my head because i've got a big stack a little bit

1749
01:57:03,480 --> 01:57:07,630
careful because you should check that they come out in sort of one two

1750
01:57:07,640 --> 01:57:10,500
three five six seven

1751
01:57:11,460 --> 01:57:16,530
and if you have the big stack you can go and buy one

1752
01:57:16,580 --> 01:57:18,180
i was useful

1753
01:57:18,200 --> 01:57:22,900
memory costs are going up these days are going down there should be because of

1754
01:57:27,090 --> 01:57:28,030
price fixing

1755
01:57:30,740 --> 01:57:34,900
the question is what's the running time of this algorithm and there's no

1756
01:57:34,950 --> 01:57:38,120
we here this is one of those answers where it depends

1757
01:57:38,140 --> 01:57:41,420
the parts is easy to through

1758
01:57:41,470 --> 01:57:44,010
analyzer well initialisation

1759
01:57:44,100 --> 01:57:47,580
the inorder tree walk columns that take

1760
01:57:49,450 --> 01:57:51,180
order and the walk

1761
01:57:51,200 --> 01:57:55,830
the initialisation constant

1762
01:57:55,850 --> 01:58:00,350
the question is how long does it take me to do and entry into service

1763
01:58:00,360 --> 01:58:11,380
anyone want to guess any kind of answer to that question

1764
01:58:11,400 --> 01:58:13,490
other than it depends on

1765
01:58:13,500 --> 01:58:18,150
so on the thunder there

1766
01:58:21,040 --> 01:58:25,660
big omega log and that's good it's at least and again

1767
01:58:44,000 --> 01:58:47,270
right so you give two reasons the first one is because of the

1768
01:58:47,280 --> 01:58:50,240
decision tree lower bound that doesn't actually

1769
01:58:50,260 --> 01:58:51,470
proof of this

1770
01:58:51,520 --> 01:58:54,820
a bit careful this is the claim that something and again

1771
01:58:54,840 --> 01:58:56,950
in every all the time

1772
01:58:57,000 --> 01:59:01,710
it's certainly again like in the worst case returning every comparison based learning how some

1773
01:59:01,710 --> 01:59:03,640
again like in the worst case

1774
01:59:03,650 --> 01:59:08,080
it's also and again every single time i may and again

1775
01:59:08,090 --> 01:59:09,860
because of the second reason gave

1776
01:59:09,870 --> 01:59:13,260
which is the best case the best thing that can happen is we have a

1777
01:59:13,260 --> 01:59:15,060
perfectly balanced tree

1778
01:59:15,810 --> 01:59:20,280
this is the thing that i have drawn the most

1779
01:59:20,330 --> 01:59:24,920
on blackboard my life perfect three on

1780
01:59:25,070 --> 01:59:28,270
fifteen nodes i guess

1781
01:59:28,320 --> 01:59:31,680
so if we're lucky we have this and if you add up all the depths

1782
01:59:31,680 --> 01:59:34,120
of the nodes here which gives you the

1783
01:59:34,140 --> 01:59:36,130
the search for cost in particular

1784
01:59:36,140 --> 01:59:39,530
these and other two nodes in the bottom each have high

1785
01:59:39,640 --> 01:59:42,030
each have depth log n

1786
01:59:42,040 --> 01:59:45,540
and therefore you're going to have to pay at least an organ for those

1787
01:59:45,590 --> 01:59:49,820
and if you're less balance is going to be even worse that takes improving but

1788
01:59:49,830 --> 01:59:50,610
it's true

1789
01:59:50,630 --> 01:59:54,110
so it's actually make it and log and all the time

1790
01:59:54,130 --> 02:00:01,730
you see that are some cases like if you know that the elements are almost

1791
02:00:01,730 --> 02:00:05,990
already in order you can do it in linear linear number comparisons

1792
02:00:06,120 --> 02:00:08,000
here you can

1793
02:00:08,010 --> 02:00:09,250
any other

1794
02:00:09,270 --> 02:00:10,370
guess is that

1795
02:00:10,390 --> 02:00:13,600
and the answer to this question

1796
02:00:14,440 --> 02:00:18,030
go and square

1797
02:00:19,720 --> 02:00:30,080
right we're doing and things and each node has depth most and so the number

1798
02:00:30,080 --> 02:00:33,830
of comparisons are making per element we insert is the most and so that's

1799
02:00:33,990 --> 02:00:37,610
most and square

1800
02:00:38,650 --> 02:00:43,820
any other answers

1801
02:00:43,840 --> 02:00:47,070
is it possible for this to take m squared times

1802
02:00:47,090 --> 02:00:53,320
is it are the instances where it takes a fair and square

1803
02:00:53,370 --> 02:00:57,410
it's already sorted out pretty bad

1804
02:01:04,330 --> 02:01:07,810
sorry sorry no offense reverse sorted

1805
02:01:07,860 --> 02:01:11,440
during about shape

1806
02:01:11,450 --> 02:01:15,650
because then you get a tree like this this sort of case

1807
02:01:16,710 --> 02:01:21,490
and your computer so that the total cost the time in general is going to

1808
02:01:22,140 --> 02:01:25,850
the sum of the depths of the nodes

1809
02:01:25,900 --> 02:01:29,000
for each node x three

1810
02:01:29,190 --> 02:01:34,150
and in this case is one pos two pos three pos for this arithmetic series

1811
02:01:34,150 --> 02:01:35,250
is and of

1812
02:01:35,260 --> 02:01:40,800
so this is the and were like this one over two

1813
02:01:42,220 --> 02:01:45,770
that's bad news some the worst case running time of this algorithm is

1814
02:01:45,830 --> 02:01:49,430
is an square

1815
02:01:49,470 --> 02:01:51,170
so familiar at all

1816
02:01:51,180 --> 02:01:56,950
algorithm worst case running time and in particular already sort case

1817
02:01:56,960 --> 02:01:58,560
but if we're lucky

1818
02:01:58,570 --> 02:02:00,680
the lucky case to be the

1819
02:02:00,690 --> 02:02:04,870
as we said it's already a balanced tree

1820
02:02:04,880 --> 02:02:07,870
the mapping or anything

1821
02:02:07,920 --> 02:02:09,670
with omega log n high

1822
02:02:11,830 --> 02:02:14,940
will give us a sorting out from

1823
02:02:14,960 --> 02:02:16,170
in my again

1824
02:02:17,180 --> 02:02:23,040
so lucky case where n log n

1825
02:02:23,120 --> 02:02:25,760
and in the unlikely case where square

1826
02:02:25,780 --> 02:02:27,360
and what he is

1827
02:02:27,370 --> 02:02:31,630
sort it

1828
02:02:31,650 --> 02:02:34,960
mind you many algorithm we've seen before

1829
02:02:34,980 --> 02:02:40,130
clicks are

1830
02:02:43,120 --> 02:02:48,450
it turns out

1831
02:02:48,460 --> 02:02:53,250
the running time of this algorithm is the same as the running time of quicksort

1832
02:02:53,270 --> 02:02:55,730
in a very strong sense

1833
02:02:55,750 --> 02:02:58,590
turns out the comparisons that this algorithm makes

1834
02:02:58,600 --> 02:03:00,930
are exactly the same comparisons

1835
02:03:00,940 --> 02:03:04,180
quicksort makes makes them in a different order

1836
02:03:04,190 --> 02:03:06,340
but it's really the same algorithm

1837
02:03:06,350 --> 02:03:11,920
in disguise

1838
02:03:11,960 --> 02:03:16,400
that's the

1839
02:03:16,460 --> 02:03:18,340
surprise here

