1
00:00:00,000 --> 00:00:04,920
property as soon as the number of measurements is bigger than this as slogan over

2
00:00:04,920 --> 00:00:09,920
as factors and that's what explains the kind of the

3
00:00:09,940 --> 00:00:16,330
the non-recourse results you've seen before just by taking the matrix is idea entry they

4
00:00:16,360 --> 00:00:19,730
naturally have sparse nearly

5
00:00:19,750 --> 00:00:21,860
nearly orthogonal columns

6
00:00:21,880 --> 00:00:25,590
and that's what i call the blessing of i dimensionality when we think about high

7
00:00:25,590 --> 00:00:28,500
dimensions we we think that it occurs

8
00:00:28,520 --> 00:00:32,610
because you know the space is empty and so on and so forth it's precisely

9
00:00:32,610 --> 00:00:37,440
because the space is empty because precisely this year is empty that i can pack

10
00:00:37,540 --> 00:00:41,110
lots of columns maintaining approximate orthogonality

11
00:00:41,130 --> 00:00:46,590
which is something that people go in quantum information theory all the time

12
00:00:46,590 --> 00:00:50,360
so renormalisation makes life is not going to go back to your comments about how

13
00:00:50,360 --> 00:00:52,290
this this play was incoherent

14
00:00:52,380 --> 00:00:58,560
and so there is another way of finding matrices which is practically much more relevant

15
00:00:58,560 --> 00:01:02,360
than then matrices because the problem is when we do

16
00:01:02,380 --> 00:01:05,830
maybe as if you were the genesis tutorial when we try to do

17
00:01:06,000 --> 00:01:11,590
real x real devices based on compressed sensing there's no way i'm going to store

18
00:01:11,590 --> 00:01:17,840
and i'd just images text suppose we doing imaging where ten megapixel the five hundred

19
00:01:17,840 --> 00:01:22,590
thousand measurements on the sample by twenty there's no way i'm going to store five

20
00:01:22,590 --> 00:01:24,860
hundred thousand by ten mega

21
00:01:24,860 --> 00:01:27,000
by ten in matrix and its

22
00:01:27,020 --> 00:01:32,310
it takes thousands of DVD stephen wright the matrix so to apply it's impossible

23
00:01:32,370 --> 00:01:35,560
and that's what is very important in the field to be able to have structured

24
00:01:35,560 --> 00:01:40,630
matrices databases is restricted isometry property is that i don't need to store and for

25
00:01:40,630 --> 00:01:42,560
which i have fast algorithms

26
00:01:42,570 --> 00:01:45,060
and one way of doing this is very simple

27
00:01:45,070 --> 00:01:49,230
it is to start with the unitary matrix u

28
00:01:50,250 --> 00:01:56,310
like the fourier matrix that you've seen before and just to select rows around

29
00:01:56,340 --> 00:02:00,270
so i could take a unitary matrix select rows at random

30
00:02:00,290 --> 00:02:04,540
and that will bayes are restricted isometry property is very deep

31
00:02:04,590 --> 00:02:09,330
theorem that goes back to well that was proved by myself and terence tao

32
00:02:09,570 --> 00:02:13,380
also by which is an inversion but the goes back to ideas from burgas

33
00:02:13,400 --> 00:02:18,250
so this result would say something like this take an orthonormal transform let's say for

34
00:02:19,630 --> 00:02:23,670
look at its maximum entry squared multiplied by ten

35
00:02:23,690 --> 00:02:27,500
so in the case of the fourier matrix is one

36
00:02:27,520 --> 00:02:32,900
and then if you select rows at random

37
00:02:32,920 --> 00:02:37,520
ips from this matrix it will be the restricted isometry property as long as the

38
00:02:37,520 --> 00:02:44,190
number of measurement is essentially the coherence time the sparsity times along fact

39
00:02:45,520 --> 00:02:47,770
and one versioning tells me

40
00:02:47,790 --> 00:02:52,400
but if you can laws exponent from five to one

41
00:02:52,420 --> 00:02:56,000
then apparently you are in the fields medal because he proved along the one conjecture

42
00:02:56,000 --> 00:03:01,060
which is a big thing in numbers here

43
00:03:01,090 --> 00:03:05,920
our focus now in the last ten minutes of part two and part three and

44
00:03:05,920 --> 00:03:07,520
make it very short

45
00:03:07,540 --> 00:03:10,940
in the last ten minutes of part two i would like to discuss applications and

46
00:03:10,940 --> 00:03:15,570
opportunities so there's a lot of people working in this field as i mentioned earlier

47
00:03:15,590 --> 00:03:18,980
and maybe i need to switch to

48
00:03:21,130 --> 00:03:22,860
right so we can discuss

49
00:03:29,770 --> 00:03:34,500
several efforts are underway to kind of leverages

50
00:03:34,520 --> 00:03:39,710
techniques in hardware and and so the first stop is the discussion of new analog

51
00:03:39,730 --> 00:03:43,250
to digital converters that happen to be very involved

52
00:03:43,270 --> 00:03:44,920
OK so

53
00:03:44,940 --> 00:03:49,400
roughly speaking when we think about how to do a conversion everything goes back to

54
00:03:50,290 --> 00:03:55,730
and shannon says well suppose you want to digitize bandlimited signal that is you want

55
00:03:55,730 --> 00:04:00,210
to kind of digital signal living in a certain bandwidth so for example all signals

56
00:04:00,210 --> 00:04:03,130
between zero and two gigahertz say

57
00:04:03,150 --> 00:04:07,340
then shannon says of course you need to sample at twice the rate so we

58
00:04:07,360 --> 00:04:10,170
need to sample twice the highest frequency

59
00:04:10,170 --> 00:04:14,110
and so we need two points per cycle so if i want to digitize signal

60
00:04:14,460 --> 00:04:20,340
in about two gigahertz bandwidths i need to sample four billion times per second

61
00:04:20,920 --> 00:04:25,250
and that's a bit of a problem because today signals to get wider and wider

62
00:04:25,250 --> 00:04:30,900
band and so for example this is the united states frequency allocations map the united

63
00:04:30,900 --> 00:04:36,590
states has already located about three hundred gigahertz worse than weights and so this is

64
00:04:36,590 --> 00:04:39,130
reflecting the fact that signals

65
00:04:39,150 --> 00:04:43,270
very wide in the frequency domain

66
00:04:43,270 --> 00:04:47,750
OK for example if you go and you care about military application people can make

67
00:04:47,830 --> 00:04:53,190
radar pulses that span several years at the moment because it just a short

68
00:04:54,380 --> 00:04:59,150
so signals are wider and wider band and if we look at what the industry

69
00:04:59,150 --> 00:05:03,000
is doing there's more flying in

70
00:05:03,000 --> 00:05:07,380
in computing of course but it is also more law in analog to digital conversion

71
00:05:07,380 --> 00:05:09,880
that says something like this that

72
00:05:09,880 --> 00:05:14,040
if you have it in analog to digital converters that sample signals at two hundred

73
00:05:14,040 --> 00:05:17,560
megahertz how long they have to wait

74
00:05:17,590 --> 00:05:21,790
with a given accuracy let's say twelve bits of accuracy on the on the data

75
00:05:21,810 --> 00:05:25,090
how long they have to wait until you see

76
00:05:25,110 --> 00:05:28,020
the same accuracy but at twice that speed

77
00:05:28,040 --> 00:05:29,940
at four hundred megahertz

78
00:05:29,960 --> 00:05:31,360
and when you look at

79
00:05:31,380 --> 00:05:36,020
the tripoli literature what people put out on the market it see that you have

80
00:05:36,020 --> 00:05:37,830
to wait about eight years

81
00:05:37,840 --> 00:05:42,000
so to double the speed i need to wait about a years is now when

82
00:05:42,000 --> 00:05:45,960
i say well how long have to wait until i see your ship is actually

83
00:05:45,960 --> 00:05:51,270
sampling the radio frequency band in the two point five gigahertz bandwidth with twelve bits

84
00:05:51,270 --> 00:05:52,360
of accuracy

85
00:05:52,380 --> 00:05:55,570
well i have to wait seventy years

86
00:05:57,900 --> 00:06:02,150
a big problem because lots of people are interested in since signals are wondering wider

87
00:06:02,150 --> 00:06:06,090
than lots of people interested in digitizing signals

88
00:06:06,110 --> 00:06:08,250
you know why you know what

89
00:06:09,110 --> 00:06:10,590
now the thing is

90
00:06:10,590 --> 00:06:14,990
heavy elements of got lots of protons or positively charged packed together

91
00:06:15,010 --> 00:06:20,480
feeling electrical repulsion the nucleus would apparently want to blow apart clearly doesn't otherwise you

92
00:06:20,480 --> 00:06:21,970
wouldn't be here

93
00:06:22,000 --> 00:06:23,090
so from that

94
00:06:23,100 --> 00:06:26,850
they deduce there must be a very powerful attractive force to work inside the nucleus

95
00:06:26,850 --> 00:06:30,730
to hold it together that was called the strong force

96
00:06:30,780 --> 00:06:36,030
the third force called weak by comparison with strong is what caused radioactivity assert that

97
00:06:36,080 --> 00:06:38,290
radioactivity beta decay

98
00:06:38,330 --> 00:06:42,170
which changes one element into another is very important in the centre of the sun

99
00:06:42,240 --> 00:06:47,450
fusing hydrogen into helium and start building up heavier elements which eventually

100
00:06:47,450 --> 00:06:48,650
and up inside us

101
00:06:48,660 --> 00:06:50,170
so those the three

102
00:06:50,180 --> 00:06:55,970
forces known in nineteen fifty five

103
00:06:56,010 --> 00:06:57,660
there were however

104
00:06:57,660 --> 00:07:02,510
weird things happening experiment the theorist i have to confess to his actually experiment which

105
00:07:02,510 --> 00:07:04,200
tells us how things work

106
00:07:04,250 --> 00:07:04,920
i know

107
00:07:04,920 --> 00:07:07,550
we theories like to tell you that we've got it all sorted out and the

108
00:07:07,550 --> 00:07:09,760
will of the people here claim claiming string theory

109
00:07:09,790 --> 00:07:14,220
explain saying this then introduced me wakes up at this point

110
00:07:14,220 --> 00:07:17,750
the experiment will tell us whether that's right or wrong and experiment has a habit

111
00:07:17,750 --> 00:07:20,730
of showing things that we had anticipated

112
00:07:20,790 --> 00:07:23,200
and in the nineteen forties

113
00:07:23,230 --> 00:07:27,410
the way the experiment was going was

114
00:07:27,460 --> 00:07:31,540
looking cosmic rays sending balloons high up in the atmosphere

115
00:07:31,570 --> 00:07:34,730
because out there in space nature

116
00:07:34,790 --> 00:07:40,800
is providing us with naturally produced high-energy particles exploding stars

117
00:07:40,850 --> 00:07:43,390
phenomena that we don't totally understand even now

118
00:07:44,460 --> 00:07:45,850
particles in the space

119
00:07:47,080 --> 00:07:51,500
the magnetic field in space move around and some of them hit the upper atmosphere

120
00:07:51,540 --> 00:07:53,370
and in doing so

121
00:07:54,450 --> 00:07:56,320
the atomic nuclei

122
00:07:56,380 --> 00:07:57,300
in the

123
00:07:57,320 --> 00:08:02,150
as into the atmosphere into pieces and showers of particles come

124
00:08:02,190 --> 00:08:05,480
falling down and in fact passing through is right now

125
00:08:05,500 --> 00:08:10,200
so by sending detectors in blue into the high atmosphere you could

126
00:08:10,730 --> 00:08:13,730
these primary cosmic rays

127
00:08:13,750 --> 00:08:15,130
and in doing so

128
00:08:15,140 --> 00:08:20,480
they found not just the ordinary particles that went like electrons protons and photons

129
00:08:21,230 --> 00:08:25,900
particles are very strange properties which became known as strange particles

130
00:08:25,940 --> 00:08:30,170
and nobody anticipated these what they were nobody knew

131
00:08:30,230 --> 00:08:34,940
and it was in part a desire to understand what

132
00:08:34,960 --> 00:08:36,980
nature revealing from out there

133
00:08:36,990 --> 00:08:40,220
there's more going on that we have previously known down here

134
00:08:40,290 --> 00:08:43,720
that was the birth of particle physics and the way it was

135
00:08:43,770 --> 00:08:45,260
in the nineteen fifties

136
00:08:45,270 --> 00:08:51,770
the desire to be able to replicate the cosmic rays in the experiments

137
00:08:51,830 --> 00:08:53,740
so the top picture shows

138
00:08:53,790 --> 00:08:56,960
a picture coming from the left probably an atom

139
00:08:56,960 --> 00:09:00,720
of an iron nucleus is coming the cosmic rays detected

140
00:09:00,770 --> 00:09:03,080
in some emulsions hitting

141
00:09:03,100 --> 00:09:08,200
and at the atom and shattering into pieces which are sharing along at the bottom

142
00:09:08,470 --> 00:09:10,270
you have a picture

143
00:09:10,310 --> 00:09:16,220
from many years later an experiment where beam atomic nuclei smashing to target and produce

144
00:09:16,250 --> 00:09:22,390
secondary particles the point being that the observation of cosmic rays the observation discovery of

145
00:09:22,390 --> 00:09:24,850
unusual particles in them

146
00:09:24,870 --> 00:09:27,910
led to the desired to replicator

147
00:09:27,930 --> 00:09:30,430
cosmic ray collisions in the laboratory

148
00:09:30,470 --> 00:09:33,350
under controlled conditions to see if we can understand what was going on and that

149
00:09:33,350 --> 00:09:36,890
was the beginnings of modern high energy physics

150
00:09:36,890 --> 00:09:39,180
the result fifty years later

151
00:09:39,180 --> 00:09:42,250
is that the basic structure of matter

152
00:09:42,270 --> 00:09:46,040
as far electron concerns we know nothing deeper than it

153
00:09:46,040 --> 00:09:49,950
as far as the proton and neutron in the central nucleus we certainly do know

154
00:09:50,220 --> 00:09:51,910
something deeper than it

155
00:09:52,000 --> 00:09:52,680
we know

156
00:09:52,700 --> 00:09:57,680
the protons and neutrons are actually it'll clouds of quarks

157
00:09:57,700 --> 00:09:58,830
and that

158
00:09:58,870 --> 00:10:03,700
these many different varieties of particle that were discovered in cosmic rays and

159
00:10:03,810 --> 00:10:10,450
when they started in the accelerator experiments they found hundreds of varieties of such things

160
00:10:12,080 --> 00:10:15,160
different combinations of the quarks linked together just like

161
00:10:15,160 --> 00:10:17,290
different combinations of

162
00:10:17,330 --> 00:10:20,330
electrons make different types of elements with

163
00:10:20,390 --> 00:10:21,830
excited levels

164
00:10:21,830 --> 00:10:24,580
so different combination of quarks make different

165
00:10:24,600 --> 00:10:26,040
collective particles

166
00:10:26,040 --> 00:10:27,580
with excited levels

167
00:10:27,620 --> 00:10:31,410
so these hundreds of varieties of particles that have been discovered in cosmic rays and

168
00:10:31,410 --> 00:10:35,600
particle physics during the nineteen fifties and sixties we now know

169
00:10:35,640 --> 00:10:39,560
are all due to a deeper layer of reality namely the quarks

170
00:10:39,620 --> 00:10:40,580
and today

171
00:10:40,600 --> 00:10:42,720
to the best experiment we can do

172
00:10:42,730 --> 00:10:45,160
electrons and quarks appear to be the basic

173
00:10:45,160 --> 00:10:48,290
letters of nature's alphabet

174
00:10:48,310 --> 00:10:51,910
no i stress when i say to the best experiment we can do

175
00:10:51,970 --> 00:10:54,040
we've seen this morning how

176
00:10:54,060 --> 00:10:56,350
energy scales of TV

177
00:10:56,350 --> 00:10:59,890
which is the highest that we can produce in the laboratory

178
00:10:59,970 --> 00:11:04,410
correspond to distance scales of ten to the minus eighteen metres

179
00:11:04,410 --> 00:11:07,660
so what we really saying is that on the distance scales

180
00:11:07,660 --> 00:11:12,220
ten to the minus eighty meters electrons and quarks appear to have

181
00:11:12,310 --> 00:11:13,830
no structure

182
00:11:13,890 --> 00:11:17,540
but if we do experiments a hundred TV

183
00:11:17,560 --> 00:11:20,290
and pro-business is a thousand times smaller

184
00:11:20,330 --> 00:11:24,850
we might discover that in fact electrons and quarks have an internal structure

185
00:11:24,870 --> 00:11:28,990
just a repeat of what we've seen over the last century when atoms were thought

186
00:11:28,990 --> 00:11:30,080
to be elementary

187
00:11:30,180 --> 00:11:34,370
and then you discover the structure of the atomic nucleus is thought to be elementary

188
00:11:34,410 --> 00:11:39,100
and you discover the structure why not the same thing again so fair question

189
00:11:39,140 --> 00:11:41,700
i will discuss it as we go along and you here

190
00:11:41,700 --> 00:11:45,270
reasons why we think it isn't that simple or maybe it is we don't know

191
00:11:45,270 --> 00:11:47,220
but experimentally

192
00:11:47,230 --> 00:11:50,930
all we know today is that at this distance scale electrons and quarks appear to

193
00:11:50,930 --> 00:11:52,520
be the basic seats

194
00:11:52,520 --> 00:11:57,310
there are many profound similarities between electrons and quarks will meet

195
00:11:57,370 --> 00:11:58,810
this suggests that

196
00:11:58,850 --> 00:12:02,950
if they are not fundamental there's something very special this time around

197
00:12:03,120 --> 00:12:05,660
nature is conspiring as if there

198
00:12:05,720 --> 00:12:08,680
and this is one of the conundrums science to try to build a theory that

199
00:12:08,680 --> 00:12:10,850
unifies electrons

200
00:12:10,850 --> 00:12:17,850
hi everyone my name is ready and working with doctor richard green he's my supervisor

201
00:12:17,890 --> 00:12:22,460
and away from the university of canterbury in new zealand let me tell you a

202
00:12:22,460 --> 00:12:27,360
new zealand has an eleven hour time difference so it's about eleven PM right now

203
00:12:27,580 --> 00:12:33,050
so frivolously it's not your fault okay let's get it going

204
00:12:33,060 --> 00:12:40,350
boundary detection boundary detection is sometimes used as an essential first step to image interpretation

205
00:12:40,370 --> 00:12:45,430
in fact there are many computer vision algorithms which rely on the boundary detection

206
00:12:45,440 --> 00:12:47,110
now this here

207
00:12:47,120 --> 00:12:53,650
the global probability of boundary detector is one of the best boundary detectors right now

208
00:12:53,750 --> 00:12:57,550
and so that's what boundary detector looks like

209
00:12:57,580 --> 00:13:02,370
so what is the boundary i'm sure it's common sense to all of you and

210
00:13:02,370 --> 00:13:09,360
this here that's what humans think is where the boundaries that it had boundary

211
00:13:09,370 --> 00:13:12,080
separates different areas of an image

212
00:13:12,090 --> 00:13:13,370
it's that simple

213
00:13:13,380 --> 00:13:18,930
the way computer the way of boundary detector works as it tries to find five

214
00:13:19,060 --> 00:13:26,570
boundaries by finding significant differences between neighbouring areas areas of the same image

215
00:13:26,600 --> 00:13:28,660
so what that would look like

216
00:13:28,710 --> 00:13:33,600
as we look at one particular region then it would look at region next to

217
00:13:33,880 --> 00:13:39,790
it would say oh these are significantly different there must be a boundary between and

218
00:13:39,790 --> 00:13:43,040
so that is how the boundary detector works

219
00:13:43,050 --> 00:13:44,100
but it's not

220
00:13:44,130 --> 00:13:46,520
that simple because in the real world

221
00:13:46,540 --> 00:13:50,460
saying these two areas are significantly different

222
00:13:50,460 --> 00:13:52,660
is not simple

223
00:13:52,660 --> 00:13:54,180
why not well

224
00:13:54,240 --> 00:14:00,040
fixtures most images in the real world contain texture

225
00:14:01,320 --> 00:14:04,320
is made up of repeating variations

226
00:14:04,380 --> 00:14:09,770
and these variations are what make boundary detection quite difficult

227
00:14:09,940 --> 00:14:13,630
so this here this is a little example of why

228
00:14:13,680 --> 00:14:19,940
how that might look like the canny edge detector does not consider texture information and

229
00:14:19,940 --> 00:14:21,660
so are one it's done

230
00:14:21,670 --> 00:14:25,640
is it's looked at the end

231
00:14:25,690 --> 00:14:28,290
and the barebones face

232
00:14:28,290 --> 00:14:32,350
and decided all those little variations are significant

233
00:14:32,350 --> 00:14:35,760
and so a naive boundary detector like this

234
00:14:35,760 --> 00:14:42,290
can easily confuse these repeating variations for significant differences

235
00:14:42,340 --> 00:14:43,810
so that's the problem

236
00:14:44,910 --> 00:14:47,010
that has been quite

237
00:14:47,030 --> 00:14:50,540
pretty much solved most images contain texture

238
00:14:50,560 --> 00:14:51,910
four boundary two

239
00:14:51,940 --> 00:14:55,510
the boundary detector to be good on my images therefore

240
00:14:55,560 --> 00:14:58,200
that must utilise texture information

241
00:14:58,220 --> 00:15:03,690
so this is very important and right here this is a boundary detector which does

242
00:15:03,730 --> 00:15:07,030
not consider texture and so you can see how it's

243
00:15:07,040 --> 00:15:10,850
improve the results greatly now this actually happens to be

244
00:15:10,880 --> 00:15:12,390
our particular

245
00:15:12,410 --> 00:15:16,920
algorithms i'll go along and i'll tell you about today

246
00:15:18,470 --> 00:15:23,940
although texture people have found ways of handling the main problem right now is is

247
00:15:23,940 --> 00:15:27,630
the texture is slow

248
00:15:27,670 --> 00:15:34,010
most texture boundary detectors are too slow to be any use in real time

249
00:15:34,010 --> 00:15:36,410
so generally they take

250
00:15:36,420 --> 00:15:40,480
more than ten seconds and sometimes even three minutes per image

251
00:15:40,610 --> 00:15:45,110
so it's not even an option for a real-time algorithm a lot of the time

252
00:15:45,420 --> 00:15:48,500
to use the texture boundary detectors

253
00:15:48,810 --> 00:15:53,880
there are a few texture boundary detectors which to work in real time but they're

254
00:15:54,190 --> 00:15:59,420
they're quite low quality this is too you may have heard i consider this to

255
00:15:59,420 --> 00:16:00,170
be the

256
00:16:00,190 --> 00:16:04,850
state of the art the second moment matrix and surround suppression

257
00:16:06,230 --> 00:16:09,130
real time texture boundary detectors like this

258
00:16:09,140 --> 00:16:12,170
the like quality they can be improved a lot

259
00:16:12,190 --> 00:16:13,720
and so what we need

260
00:16:13,730 --> 00:16:18,380
so we need it takes the boundary detector which can work in real time

261
00:16:18,420 --> 00:16:24,280
and so our preferred solution is the standard deviation ridge detector

262
00:16:24,290 --> 00:16:25,820
so this detector

263
00:16:25,840 --> 00:16:30,860
is capable of three things that can detect boundaries

264
00:16:31,350 --> 00:16:34,960
it can ignore variations in texture

265
00:16:35,000 --> 00:16:39,580
and most importantly it can do all of this in real time

266
00:16:39,590 --> 00:16:46,100
so how we manage to do this well this algorithm works by finding ridges in

267
00:16:46,110 --> 00:16:48,530
the standard deviation space

268
00:16:48,540 --> 00:16:51,270
so there are four steps the algorithm

269
00:16:51,290 --> 00:16:56,470
and i'm going to go through the right after the slide but keep in mind

270
00:16:56,470 --> 00:16:58,660
the biggest difficulty is

271
00:16:58,680 --> 00:17:03,280
how can you ignore these variations in texture in real time

272
00:17:03,290 --> 00:17:07,790
it's quite difficult so let's see how it's done

273
00:17:07,800 --> 00:17:09,090
OK so one

274
00:17:09,100 --> 00:17:12,240
quite simple the standard deviation transport

275
00:17:12,290 --> 00:17:14,450
for each pixel

276
00:17:14,470 --> 00:17:18,620
calculate the standard deviation of the local area

277
00:17:18,650 --> 00:17:21,290
and then repeat for all pixels

278
00:17:21,290 --> 00:17:23,160
so you can follow that much

279
00:17:23,180 --> 00:17:25,120
OK but

280
00:17:25,230 --> 00:17:26,850
you probably wondering why

281
00:17:26,860 --> 00:17:28,730
do we do this

282
00:17:28,740 --> 00:17:35,110
well the reason one there are two reasons only reason one right now standard deviation

283
00:17:35,110 --> 00:17:41,850
is approximately equal for different areas of the same texture

284
00:17:41,860 --> 00:17:46,430
if we look at the standard deviation transform for this

285
00:17:46,460 --> 00:17:47,670
there have been many

286
00:17:47,700 --> 00:17:53,290
you'll see if this is quite true in this case the for has

287
00:17:53,290 --> 00:17:57,280
approximately the same standard deviation throughout

288
00:17:57,310 --> 00:17:58,480
and so

289
00:17:58,490 --> 00:18:02,360
if we can design an algorithm which can detect this

290
00:18:02,370 --> 00:18:07,490
then we can find a way to suppress the variations in texture

291
00:18:07,530 --> 00:18:11,700
how do we do that well that is the next step

292
00:18:11,720 --> 00:18:12,610
so far

293
00:18:12,620 --> 00:18:14,550
but still too

294
00:18:14,560 --> 00:18:16,290
the gradient transform

295
00:18:16,300 --> 00:18:19,660
looks at each pixel individually

296
00:18:19,680 --> 00:18:22,600
so let's say we're looking at this particular pixels

297
00:18:22,610 --> 00:18:25,390
o call it the point of interest

298
00:18:25,400 --> 00:18:32,790
so what happens is the surrounding pixels pull the point of interest in their direction

299
00:18:32,790 --> 00:18:38,590
now we sit up the system so that pixels with a higher standard deviation have

300
00:18:38,590 --> 00:18:40,420
a stronger pull

301
00:18:40,460 --> 00:18:44,840
so if all these pixels on the left have greater standard deviation

302
00:18:44,890 --> 00:18:46,990
and they all have more full

303
00:18:47,000 --> 00:18:50,460
now of course you can probably see what we do next we take the sum

304
00:18:50,460 --> 00:18:51,930
of all forces

305
00:18:52,050 --> 00:18:57,720
and so that's the gradient so the overall gradient is calculated for each pixel

306
00:18:57,730 --> 00:19:03,850
so naturally the gradient will point to the area of strongest standard deviation

307
00:19:03,850 --> 00:19:05,370
now let's look at

308
00:19:05,430 --> 00:19:08,100
but the real magic of the algorithm

309
00:19:08,100 --> 00:19:11,240
this is a continuation of the task

310
00:19:11,490 --> 00:19:14,620
workshop and very

311
00:19:21,350 --> 00:19:23,010
about something that

312
00:19:23,030 --> 00:19:24,630
future and if there is

313
00:19:24,650 --> 00:19:27,990
graph g

314
00:19:42,920 --> 00:19:46,670
and what about

315
00:19:53,860 --> 00:20:00,360
i think it's going

316
00:20:01,130 --> 00:20:08,500
OK so so this and going to talk about some more sort of been doing

317
00:20:08,500 --> 00:20:10,560
this for the last

318
00:20:10,560 --> 00:20:14,770
for a while now and this

319
00:20:14,770 --> 00:20:17,890
so most of what i'm going to talk about it

320
00:20:17,910 --> 00:20:28,860
joint work with our new engine parts of the input space consequently inria willow expert

321
00:20:28,880 --> 00:20:33,660
and so basically i would like to to start with the sort of the geometric

322
00:20:38,270 --> 00:20:44,880
the notion of the sort of underlying geometric notions notion of manifold and the less

323
00:20:46,410 --> 00:20:49,660
sort of thing was going were interested in

324
00:20:50,880 --> 00:20:57,660
and what does the data is generated by an open source you know maybe ten

325
00:20:57,660 --> 00:20:58,860
written digits

326
00:20:59,750 --> 00:21:06,700
which are generated by what we don't know exactly what generated by the some process

327
00:21:06,720 --> 00:21:10,080
to generate an

328
00:21:10,110 --> 00:21:12,050
c perfectly well

329
00:21:12,060 --> 00:21:17,340
OK well in some way to get an example those very explicitly it's clear that

330
00:21:17,340 --> 00:21:25,050
they have some parameters trequenu parameter representation in other domains well maybe

331
00:21:25,070 --> 00:21:30,200
slightly less clear but pretty much i would say for all sorts of high dimensional

332
00:21:31,050 --> 00:21:34,480
we can say that the true dimensionality is far far

333
00:21:34,490 --> 00:21:40,140
lower than their sort baron dimensionality of the data and in fact one may argue

334
00:21:40,140 --> 00:21:41,990
that you know if

335
00:21:42,070 --> 00:21:43,050
you know

336
00:21:43,080 --> 00:21:46,240
one hundred and just tried to number of pixels

337
00:21:46,340 --> 00:21:47,200
you know

338
00:21:47,270 --> 00:21:52,700
hundreds of thousands and basically if the two dimensional to the is really a hundred

339
00:21:54,340 --> 00:21:56,240
we will never be able to that

340
00:21:56,260 --> 00:21:57,510
it's impossible

341
00:21:57,520 --> 00:22:00,980
so it's really is the number of true

342
00:22:00,990 --> 00:22:07,200
i mean by that is following the dimensionality of the data

343
00:22:10,170 --> 00:22:15,140
another situation is much of the data is quite meaning and give a couple of

344
00:22:16,920 --> 00:22:24,400
so i mean linear methods are nice but they work well in many applications but

345
00:22:24,420 --> 00:22:28,650
they don't necessarily reflect the structure of the data

346
00:22:28,700 --> 00:22:33,460
a simple example to think about it just a mixture of two gaussians very simple

347
00:22:33,460 --> 00:22:40,110
parametric distributions you probably can do so using a linear map

348
00:22:40,110 --> 00:22:43,360
well maybe you can separate them

349
00:22:46,740 --> 00:22:53,680
now manifold forward to manifolds and manifolds in the fall so it slightly more general

350
00:22:53,680 --> 00:22:55,800
motion so

351
00:22:56,770 --> 00:23:02,860
suppose we have some sort of manifold which is a first class with probability distribution

352
00:23:02,860 --> 00:23:04,010
on the surface

353
00:23:04,050 --> 00:23:08,540
so the probability density may be different in different parts of the surface plus you

354
00:23:08,540 --> 00:23:13,070
may think there is some maybe some noise of some so you know maybe it's

355
00:23:13,070 --> 00:23:16,620
not there is not exactly on the manifold but little bit off

356
00:23:16,640 --> 00:23:22,260
and this is probably one of the most general model for data that you can

357
00:23:22,300 --> 00:23:25,750
so you probably to check

358
00:23:25,780 --> 00:23:30,960
so for example to give example of mixture of gaussians satisfies the definition manifold is

359
00:23:30,960 --> 00:23:33,270
just essentially thousands

360
00:23:33,280 --> 00:23:35,270
plus you have noise which is

361
00:23:37,940 --> 00:23:43,430
this is quite quite general for zero dimensional manifold wasn't as mixture

362
00:23:43,460 --> 00:23:48,570
OK so let me give you a couple of examples so speech for example we

363
00:23:48,570 --> 00:23:54,020
can model speech by a sequence of data elements in models which is followed by

364
00:23:54,020 --> 00:23:56,080
a sequence of tubes

365
00:23:56,430 --> 00:24:02,260
and while the conservatives so with bunch of two well actually citizen

366
00:24:02,320 --> 00:24:10,580
you know exactly what the issue was and that's

367
00:24:10,840 --> 00:24:15,630
that is clearly parameter model the speech itself is an infinite dimensional space right of

368
00:24:15,630 --> 00:24:18,900
high dimensional have use this

369
00:24:19,840 --> 00:24:22,150
but the students

370
00:24:22,200 --> 00:24:24,780
fixed parameter something like

371
00:24:24,830 --> 00:24:26,580
for example what you

372
00:24:27,070 --> 00:24:30,780
i think about this for example and you just have a little by this moving

373
00:24:30,780 --> 00:24:36,100
around the space here if you think of the visual field and you're just moving

374
00:24:36,100 --> 00:24:40,330
this little about the number of pixels here arbitrarily large but there are just two

375
00:24:40,330 --> 00:24:41,980
degrees of freedom

376
00:24:42,000 --> 00:24:49,580
two-dimensional manifolds and finally something like robotics you may have something new manipulate for example

377
00:24:49,580 --> 00:24:52,390
many many degrees of freedom

378
00:24:52,660 --> 00:24:57,650
and we would like to study so this is not an example of an embedded

379
00:24:57,650 --> 00:25:02,450
manifold we may want to study what how the location of the

380
00:25:02,950 --> 00:25:05,100
also of robotics

381
00:25:05,120 --> 00:25:09,480
relates to the degrees of freedom and more generally in the house great

382
00:25:09,500 --> 00:25:15,010
relates to all these degrees of freedom for example the quality of the group in

383
00:25:15,010 --> 00:25:21,450
england configuration of robotics on the interesting function it's linear manifold to function on the

384
00:25:22,960 --> 00:25:28,820
so finally so so how do we sort of what is the connection graphs and

385
00:25:28,820 --> 00:25:30,530
what sort of

386
00:25:30,590 --> 00:25:32,120
what they did

387
00:25:32,830 --> 00:25:37,460
we believe that they can't list in many machine learning problems we believe they come

388
00:25:37,470 --> 00:25:42,070
from a probability distribution so that is probability distribution and there is a discrete version

389
00:25:42,070 --> 00:25:43,940
of of this probability distribution which

390
00:25:47,560 --> 00:25:50,390
the probability distribution if

391
00:25:50,400 --> 00:25:52,910
i think it's meant so

392
00:25:52,930 --> 00:25:57,460
then what is the point in structure or debates itself

393
00:25:57,520 --> 00:25:59,830
and one may argue that

394
00:25:59,840 --> 00:26:06,230
graph based representation of the data actually provides a nice model for the underlying many

395
00:26:06,290 --> 00:26:11,720
so one may think graph is being discrete meaningful in the sense is that it's

396
00:26:11,720 --> 00:26:16,080
sort of discretizations is a representation of the probability distribution

397
00:26:16,200 --> 00:26:19,430
so kind of like

398
00:26:20,080 --> 00:26:24,270
this is the samples from the manifold and a construct some graphs

399
00:26:24,280 --> 00:26:29,700
and this is a graph of the presentation for

400
00:26:29,700 --> 00:26:36,870
so in a sense it's sort of constructs underlying structure OK

401
00:26:37,090 --> 00:26:40,540
now i think about actually problems in machine learning

402
00:26:42,180 --> 00:26:45,450
how this applies to the problems of russia

403
00:26:47,510 --> 00:26:53,090
of classical problems of machine learning one is that of course ification regression well we

404
00:26:53,090 --> 00:26:55,480
know that this a bunch

405
00:26:55,500 --> 00:26:58,020
examples and we

406
00:26:58,020 --> 00:27:01,790
based on this example is going to build the classifier to classify data

407
00:27:01,810 --> 00:27:03,770
classify speech

408
00:27:04,410 --> 00:27:08,720
something like that of the regression you info

409
00:27:08,770 --> 00:27:11,310
you know what the stock market is going

410
00:27:11,320 --> 00:27:13,080
to be tomorrow from

411
00:27:13,100 --> 00:27:13,830
you know what to do

412
00:27:13,840 --> 00:27:17,750
yesterday could rich

413
00:27:19,590 --> 00:27:23,140
then there is the representation dimensionality reduction

414
00:27:23,150 --> 00:27:27,470
which has several purposes one purposes of visualization the

415
00:27:27,480 --> 00:27:29,940
i like to look at the data understand what it's doing

416
00:27:29,960 --> 00:27:34,080
another the purpose is sort of data quantization clustering

417
00:27:34,080 --> 00:27:38,830
the same

418
00:27:48,430 --> 00:27:50,880
we have to

419
00:28:08,200 --> 00:28:10,060
she she

420
00:28:10,220 --> 00:28:11,280
time t

421
00:28:11,290 --> 00:28:12,610
when t

422
00:28:20,630 --> 00:28:24,160
it was

423
00:28:24,160 --> 00:28:25,830
you should

424
00:28:28,940 --> 00:28:33,020
he said rules

425
00:28:45,540 --> 00:28:48,860
seventy two

426
00:28:49,220 --> 00:28:51,850
the term

427
00:28:56,320 --> 00:29:00,610
the result was

428
00:29:13,220 --> 00:29:16,500
you also

429
00:29:23,160 --> 00:29:25,970
and this

430
00:29:25,980 --> 00:29:28,930
we used to

431
00:29:29,150 --> 00:29:31,350
it's the

432
00:29:36,280 --> 00:29:39,100
can use the

433
00:29:51,950 --> 00:29:54,910
all men

434
00:30:01,940 --> 00:30:05,620
the way the

435
00:30:05,660 --> 00:30:12,130
so that all these

436
00:30:12,150 --> 00:30:17,200
it also

437
00:30:20,470 --> 00:30:33,290
so there are all the same reasons

438
00:30:42,250 --> 00:30:44,790
you reach the

439
00:30:54,600 --> 00:30:55,900
i don't know

440
00:30:55,910 --> 00:30:58,060
it all

441
00:31:02,190 --> 00:31:08,220
you want to do

442
00:31:14,060 --> 00:31:16,380
one of the most

443
00:31:19,750 --> 00:31:23,100
all these things

444
00:31:28,560 --> 00:31:31,550
it to be

445
00:31:32,170 --> 00:31:37,060
the first thing to say

446
00:31:37,060 --> 00:31:42,940
so what you get is this

447
00:31:46,170 --> 00:31:48,230
so we get

448
00:31:50,380 --> 00:31:54,380
this is the only two

449
00:31:58,870 --> 00:32:03,560
i used

450
00:32:20,080 --> 00:32:25,380
i love you in

451
00:32:25,440 --> 00:32:26,750
he here

452
00:32:34,060 --> 00:32:42,130
so there is the

453
00:32:42,270 --> 00:32:45,000
you need to

454
00:32:58,320 --> 00:33:04,180
for the

455
00:33:04,980 --> 00:33:08,170
is huge

456
00:33:08,190 --> 00:33:12,210
he he is

457
00:33:12,290 --> 00:33:13,130
two years

458
00:33:17,240 --> 00:33:20,560
we see

459
00:33:20,570 --> 00:33:22,290
we are

460
00:33:22,430 --> 00:33:24,610
it's a

461
00:33:29,960 --> 00:33:35,880
and for all

462
00:33:48,450 --> 00:33:51,990
one is that the

463
00:33:52,130 --> 00:33:57,250
need to

464
00:34:01,940 --> 00:34:05,820
this is

465
00:34:12,440 --> 00:34:13,940
this is

466
00:34:24,050 --> 00:34:26,820
the whole was

467
00:34:26,820 --> 00:34:31,280
magnetic flux that is created

468
00:34:31,290 --> 00:34:35,360
when the current was running happily here was better in place

469
00:34:35,410 --> 00:34:37,080
the current was let's say

470
00:34:37,100 --> 00:34:38,140
all the time

471
00:34:38,140 --> 00:34:40,060
i max

472
00:34:40,080 --> 00:34:42,770
very close to the values v over are

473
00:34:42,820 --> 00:34:46,020
so all the time there was he produced in the resist

474
00:34:46,070 --> 00:34:48,250
i squared r joules per second

475
00:34:48,250 --> 00:34:52,120
or providing that's energy well of course the battery

476
00:34:52,140 --> 00:34:53,070
but now

477
00:34:53,100 --> 00:34:56,160
when i take the battery out

478
00:34:56,160 --> 00:34:58,540
there still current running

479
00:34:58,600 --> 00:34:59,680
and that means

480
00:34:59,680 --> 00:35:02,160
while the current is dying

481
00:35:02,200 --> 00:35:04,530
there still he produced in that was this

482
00:35:04,550 --> 00:35:07,790
that he slowly comes out until the current ultimately

483
00:35:07,810 --> 00:35:09,070
become zero

484
00:35:09,090 --> 00:35:11,430
so where does that energy come from

485
00:35:11,430 --> 00:35:15,310
well the energy must come from the magnetic field that is present in

486
00:35:15,370 --> 00:35:17,250
the solenoid

487
00:35:17,350 --> 00:35:19,390
this idea here

488
00:35:19,430 --> 00:35:20,850
that we have

489
00:35:21,950 --> 00:35:24,050
that comes out in the form of heat

490
00:35:24,070 --> 00:35:27,700
which really was there earlier in the form of the magnetic field

491
00:35:27,740 --> 00:35:30,240
it allows us to evaluate

492
00:35:30,310 --> 00:35:31,910
what we call

493
00:35:31,950 --> 00:35:34,050
the magnetic energy

494
00:35:34,080 --> 00:35:36,210
field density

495
00:35:36,210 --> 00:35:38,360
let me first calculate

496
00:35:38,430 --> 00:35:39,500
how much

497
00:35:39,510 --> 00:35:41,010
he is produced

498
00:35:41,030 --> 00:35:42,200
as the current

499
00:35:42,240 --> 00:35:47,780
goes from the maximum value down to zero

500
00:35:47,860 --> 00:35:50,560
and have to raise something

501
00:35:50,580 --> 00:35:54,000
i raise this part

502
00:35:54,000 --> 00:35:56,960
so at any moment in time

503
00:35:57,010 --> 00:35:59,900
the current

504
00:35:59,970 --> 00:36:02,490
is producing

505
00:36:02,540 --> 00:36:04,710
heat in the resistor

506
00:36:04,710 --> 00:36:06,120
and so

507
00:36:06,170 --> 00:36:07,480
if i

508
00:36:07,590 --> 00:36:10,960
my voltage becomes zero at time t equals zero

509
00:36:11,620 --> 00:36:13,520
this is the amount

510
00:36:13,570 --> 00:36:16,200
of heat

511
00:36:16,640 --> 00:36:18,830
those straight here

512
00:36:18,900 --> 00:36:23,140
i swear i integrated from zero to infinity is the total heat

513
00:36:23,150 --> 00:36:26,210
that is produced as the current guys out

514
00:36:26,220 --> 00:36:30,600
but i know what is current was just to raise it to remember it

515
00:36:30,620 --> 00:36:34,470
so i can bring i max outside

516
00:36:34,480 --> 00:36:36,740
and i can bring the resistance outside

517
00:36:36,750 --> 00:36:37,840
and then i get the

518
00:36:37,870 --> 00:36:40,200
the integral from zero to infinity

519
00:36:40,200 --> 00:36:42,120
two minus are

520
00:36:42,140 --> 00:36:44,100
over l times the

521
00:36:47,160 --> 00:36:50,870
and this is the trivial integral

522
00:36:50,920 --> 00:36:54,640
this integral is well defined by

523
00:36:54,690 --> 00:36:56,010
two are

524
00:36:56,030 --> 00:36:59,580
o by the way it is i squared to over two here

525
00:36:59,590 --> 00:37:00,970
very important

526
00:37:00,970 --> 00:37:02,930
don't forget the two

527
00:37:02,940 --> 00:37:06,110
so that the integral is l divided by two are

528
00:37:06,250 --> 00:37:08,360
and so if now

529
00:37:09,540 --> 00:37:14,070
look at the product of ice cream maximum are l divided by two are i

530
00:37:14,070 --> 00:37:15,200
get one half

531
00:37:16,460 --> 00:37:19,160
i maximum

532
00:37:19,180 --> 00:37:24,280
great so this comes out in the form of heat

533
00:37:24,360 --> 00:37:27,240
and i'm axis and the maximum current that we had

534
00:37:27,280 --> 00:37:29,340
when the current was

535
00:37:29,420 --> 00:37:34,490
flowing after a long time

536
00:37:34,500 --> 00:37:37,230
i can now by manipulating numbers

537
00:37:37,240 --> 00:37:38,230
i can now

538
00:37:38,250 --> 00:37:42,010
calculate how much

539
00:37:42,040 --> 00:37:46,210
and this there was in that field per cubic meter because the magnetic field

540
00:37:46,250 --> 00:37:50,200
i was exclusively inside that solenoidal

541
00:37:50,250 --> 00:37:54,030
and if i know that the energy that is ultimately coming out is one have

542
00:37:54,080 --> 00:37:56,010
allies claimed

543
00:37:56,060 --> 00:37:59,040
that he i

544
00:37:59,060 --> 00:38:02,240
so i can replace that either by b

545
00:38:02,280 --> 00:38:03,890
divided by new zero

546
00:38:03,950 --> 00:38:05,910
james l divided by n

547
00:38:05,920 --> 00:38:08,590
and you have l

548
00:38:08,680 --> 00:38:11,450
and so if i substitute in here

549
00:38:11,500 --> 00:38:14,530
the value for l that we have on the blackboard there

550
00:38:14,560 --> 00:38:18,590
and we substitute for either value that we have that you can drop the maximum

551
00:38:19,540 --> 00:38:23,590
simply tell you them at any moment in time that have the current i running

552
00:38:23,590 --> 00:38:24,940
through is solenoid

553
00:38:24,990 --> 00:38:25,830
that the

554
00:38:25,850 --> 00:38:29,950
and energy it is available in this order forms of magnetic energy is one of

555
00:38:30,010 --> 00:38:31,790
our square

556
00:38:31,850 --> 00:38:33,270
so when you do that

557
00:38:33,280 --> 00:38:35,220
we substitute capital l

558
00:38:35,220 --> 00:38:36,480
and capital

559
00:38:36,490 --> 00:38:38,280
i you'll find

560
00:38:38,330 --> 00:38:39,670
that one half

561
00:38:39,730 --> 00:38:43,020
ellis created

562
00:38:43,080 --> 00:38:44,740
then becomes

563
00:38:44,740 --> 00:38:47,800
the squared

564
00:38:47,820 --> 00:38:51,000
over two new zero

565
00:38:51,000 --> 00:38:53,240
and my little are cleared

566
00:38:53,320 --> 00:38:54,260
times l

567
00:38:54,270 --> 00:38:56,210
checked that at home

568
00:38:56,310 --> 00:38:58,560
simply substitution

569
00:38:58,680 --> 00:39:00,200
but this

570
00:39:00,250 --> 00:39:02,920
is the volume of the solenoid

571
00:39:02,930 --> 00:39:07,100
where the magnetic field exists then we have assumed that the magnetic field is zero

572
00:39:07,100 --> 00:39:09,320
everywhere outside

573
00:39:09,510 --> 00:39:11,350
and if you accept that

574
00:39:11,470 --> 00:39:12,540
then you see

575
00:39:12,590 --> 00:39:14,250
that we now have a

576
00:39:14,260 --> 00:39:15,590
results for

577
00:39:15,600 --> 00:39:17,990
the magnetic

578
00:39:19,350 --> 00:39:23,090
energy density

579
00:39:23,110 --> 00:39:26,750
that is how much energy it is a cubic metre

580
00:39:26,840 --> 00:39:29,370
that is of course this value

581
00:39:29,420 --> 00:39:32,400
because this is the total energy of the magnetic field

582
00:39:32,460 --> 00:39:33,820
if we know the current

583
00:39:33,840 --> 00:39:36,480
and this is the volume of the magnetic field

584
00:39:36,480 --> 00:39:38,990
so the magnetic field energy density

585
00:39:39,010 --> 00:39:40,720
is then be grid

586
00:39:40,770 --> 00:39:43,060
divided by two new zero

587
00:39:43,120 --> 00:39:46,110
this is in joules per cubic metre

588
00:39:46,150 --> 00:39:47,310
so in principle

589
00:39:47,330 --> 00:39:50,500
if you're not to magnetic field everywhere in space

590
00:39:50,570 --> 00:39:52,720
you can integrate over all space

591
00:39:52,720 --> 00:39:55,260
you can calculate how much energy

592
00:39:56,710 --> 00:39:58,790
present in the magnetic field

593
00:39:58,850 --> 00:40:02,790
and earlier in this course we did something similar for electric field

594
00:40:02,840 --> 00:40:07,580
we calculated electric field energy density

595
00:40:07,590 --> 00:40:10,200
if you remember what it was it was one half

596
00:40:10,300 --> 00:40:12,490
actually non-zero kappa

597
00:40:12,540 --> 00:40:15,930
time is credited with also in jules

598
00:40:15,940 --> 00:40:17,770
a cubic metre

599
00:40:17,790 --> 00:40:21,060
now in the case of an electric field

600
00:40:22,430 --> 00:40:26,150
it represents the work that i have to do to arrange the charges in a

601
00:40:26,150 --> 00:40:29,670
this very simple information can be enough

602
00:40:29,720 --> 00:40:32,580
to detect attacks

603
00:40:32,610 --> 00:40:35,140
and think about this

604
00:40:35,140 --> 00:40:38,010
this is the kind of that is that

605
00:40:38,030 --> 00:40:41,080
once you have the points in the data and

606
00:40:41,090 --> 00:40:42,890
metric space

607
00:40:42,890 --> 00:40:46,740
there are many ways to connect similarity between those points

608
00:40:46,750 --> 00:40:48,170
and this is something about that

609
00:40:48,220 --> 00:40:51,460
hasn't been very wide

610
00:40:51,480 --> 00:40:53,190
used in machine learning

611
00:40:54,550 --> 00:40:56,720
most people have used corals

612
00:40:56,720 --> 00:40:58,140
that correspond to the sensors

613
00:40:58,270 --> 00:40:59,960
the euclidean distance

614
00:40:59,960 --> 00:41:02,170
this is basically a simple expression for kind of

615
00:41:02,180 --> 00:41:03,790
like the product of the

616
00:41:03,850 --> 00:41:06,400
two very large but sparse vector

617
00:41:06,440 --> 00:41:09,830
but there are also other ways too

618
00:41:09,860 --> 00:41:11,140
measure similarity

619
00:41:11,150 --> 00:41:12,500
that turned out

620
00:41:12,510 --> 00:41:16,200
to be not so difficult to implement from the technical point of view

621
00:41:16,290 --> 00:41:18,590
you can use various metric distances

622
00:41:18,600 --> 00:41:19,450
i in

623
00:41:19,450 --> 00:41:22,410
so this is there are having taken

624
00:41:22,760 --> 00:41:27,220
there are also some interesting on similar cases that have been known from the fifties

625
00:41:27,240 --> 00:41:30,890
sixties and various other fields

626
00:41:30,910 --> 00:41:36,120
of data analysis and it turns out that some of them very interesting characteristics of

627
00:41:36,120 --> 00:41:37,390
some very

628
00:41:37,400 --> 00:41:41,680
shows the performance on the net

629
00:41:43,190 --> 00:41:44,530
in order to deal with it

630
00:41:45,550 --> 00:41:49,260
similarity measures efficiently we have come out

631
00:41:52,080 --> 00:41:54,410
compute the

632
00:41:54,420 --> 00:41:56,090
similarity measures

633
00:41:56,110 --> 00:41:58,420
in a unified fashion

634
00:41:58,430 --> 00:42:00,580
so basically think about it

635
00:42:00,600 --> 00:42:02,610
computing all fun

636
00:42:02,710 --> 00:42:06,480
all down to some

637
00:42:06,490 --> 00:42:12,660
how does that runs over some operations all on a single word

638
00:42:14,470 --> 00:42:17,190
because of that one show that

639
00:42:17,220 --> 00:42:19,780
you have a general

640
00:42:22,460 --> 00:42:25,410
performs an operation on elementary

641
00:42:26,650 --> 00:42:30,860
o operations and you can define the operations

642
00:42:32,200 --> 00:42:33,810
in the final operation

643
00:42:33,840 --> 00:42:36,690
in a meaningful way that very simple

644
00:42:37,050 --> 00:42:41,200
so this is basically what you have to do one single currency in order to

645
00:42:41,200 --> 00:42:44,460
add at the top end the abstract

646
00:42:44,470 --> 00:42:47,590
abstract framework

647
00:42:48,410 --> 00:42:50,590
what almost until his death

648
00:42:50,650 --> 00:42:54,010
essentially we like to to design efficient algorithms for

649
00:42:54,060 --> 00:42:55,950
now algorithms that run in

650
00:42:55,950 --> 00:42:57,230
linear time

651
00:42:57,250 --> 00:43:00,570
that have efficient indexing of all the dimensions

652
00:43:03,910 --> 00:43:08,660
the the space from that data into extremely high demand for thank

653
00:43:08,710 --> 00:43:14,020
in all possible combinations of and random high dimensional space can be very large numbers

654
00:43:14,040 --> 00:43:18,680
but you only have a small number of possible and ground in the final data

655
00:43:18,720 --> 00:43:20,730
if you are able to index this

656
00:43:24,800 --> 00:43:26,790
the subset of features and efficient way

657
00:43:26,800 --> 00:43:29,300
we can

658
00:43:29,320 --> 00:43:31,100
efficient around the best

659
00:43:31,110 --> 00:43:32,170
and the many

660
00:43:32,210 --> 00:43:38,120
data structures that can be used for that we have what we have developed for

661
00:43:38,510 --> 00:43:44,240
the particular hash tables tries suffix trees and some more advanced features

662
00:43:44,500 --> 00:43:47,050
recently there are four

663
00:43:47,060 --> 00:43:52,350
tells you how it works in one case so you can try this

664
00:43:52,410 --> 00:43:54,920
some of them may know

665
00:43:54,940 --> 00:43:55,760
it's actually

666
00:43:55,870 --> 00:43:59,440
very classical data back to the sixties and seventies

667
00:43:59,440 --> 00:44:01,970
and it basically boils down to

668
00:44:02,460 --> 00:44:05,060
storing the dictionaries of words

669
00:44:05,070 --> 00:44:07,840
in the compact representation

670
00:44:08,850 --> 00:44:11,070
you can see here for example

671
00:44:11,310 --> 00:44:13,610
i think by high card

672
00:44:13,640 --> 00:44:16,360
can be stored basically no

673
00:44:16,410 --> 00:44:18,010
where you share

674
00:44:18,110 --> 00:44:19,990
the car card

675
00:44:20,180 --> 00:44:21,740
same representation

676
00:44:21,750 --> 00:44:24,180
and you can store some additional information

677
00:44:24,220 --> 00:44:25,850
that allows the can

678
00:44:25,850 --> 00:44:27,080
tell you

679
00:44:27,490 --> 00:44:29,450
the frequency

680
00:44:29,600 --> 00:44:32,510
individual time

681
00:44:32,510 --> 00:44:33,470
so you can

682
00:44:33,540 --> 00:44:35,490
store the number of assertions

683
00:44:35,490 --> 00:44:37,490
this is an example

684
00:44:37,540 --> 00:44:38,930
what card

685
00:44:38,950 --> 00:44:41,560
has been studied

686
00:44:42,620 --> 00:44:46,720
let's see has four times three times on one card

687
00:44:46,770 --> 00:44:48,560
and one from the work of

688
00:44:48,620 --> 00:44:49,490
and so on

689
00:44:49,580 --> 00:44:51,560
you also count the number of leaves

690
00:44:51,580 --> 00:44:53,890
which are the node

691
00:44:54,040 --> 00:44:57,350
that correspond to single words in the

692
00:44:57,370 --> 00:44:59,560
in this case

693
00:44:59,580 --> 00:45:01,220
this branch has one only

694
00:45:01,220 --> 00:45:03,160
which part of by

695
00:45:03,160 --> 00:45:04,350
and this branch

696
00:45:05,470 --> 00:45:06,700
two leaves

697
00:45:06,750 --> 00:45:10,910
in all nodes on the path to car because this was one word

698
00:45:10,950 --> 00:45:15,240
and then one the corresponding to work on

699
00:45:15,290 --> 00:45:20,200
so by using information you can essentially compact have of trees

700
00:45:20,660 --> 00:45:23,870
in some sort of parallel traversal algorithm

701
00:45:23,970 --> 00:45:26,020
basically works for

702
00:45:26,070 --> 00:45:29,120
for free from top down

703
00:45:29,120 --> 00:45:32,720
each time taken for the similarity of nodes

704
00:45:32,970 --> 00:45:36,020
individual no labels not correspond

705
00:45:37,450 --> 00:45:39,580
we find in this mismatch

706
00:45:39,600 --> 00:45:42,890
so you find that

707
00:45:43,060 --> 00:45:44,390
keep going

708
00:45:44,750 --> 00:45:48,470
and you also have something called local mismatches

709
00:45:48,510 --> 00:45:53,080
which is the local node that you have matching symbols

710
00:45:53,100 --> 00:45:57,080
but i have course on the difference in the corresponding labels

711
00:45:57,140 --> 00:46:00,680
this one does have a leaf labels and this one does not

712
00:46:00,700 --> 00:46:03,330
there is also counted as in this match

713
00:46:03,430 --> 00:46:05,740
finally we propose

714
00:46:05,790 --> 00:46:08,910
further down the line that

715
00:46:08,950 --> 00:46:14,660
turns out linear runtime complexity is linear and this is what what we need

716
00:46:14,660 --> 00:46:15,660
OK so you can

717
00:46:15,700 --> 00:46:17,470
we have in this case

718
00:46:17,540 --> 00:46:19,740
dependent on the k is

719
00:46:19,770 --> 00:46:21,100
number of cases

720
00:46:21,120 --> 00:46:23,640
you can think of

721
00:46:25,350 --> 00:46:28,680
the decay constant not interested in

722
00:46:28,720 --> 00:46:30,640
extremely large round

723
00:46:30,640 --> 00:46:36,340
the following content is provided under creative commons license your support will help MIT opencourseware

724
00:46:36,340 --> 00:46:41,790
continue to offer high quality educational resources for free to make a donation or view

725
00:46:41,790 --> 00:46:47,890
additional materials from hundreds of MIT courses visit MIT opencourseware at OCW MIT

726
00:46:49,800 --> 00:46:51,390
that u

727
00:46:52,970 --> 00:46:54,340
i want to pick up

728
00:46:54,420 --> 00:46:56,780
exactly where i left off last time

729
00:46:56,830 --> 00:47:01,750
when i was talking about various scenes one can commit with statistics

730
00:47:01,760 --> 00:47:06,760
and i have been talking about this in the data enhancement

731
00:47:09,530 --> 00:47:19,620
where the basic idea there is you take a piece of data

732
00:47:19,630 --> 00:47:24,060
and you really much more into it that it implies in particular

733
00:47:24,110 --> 00:47:27,070
a very common thing people do with data

734
00:47:27,110 --> 00:47:29,710
is a extrapolate

735
00:47:29,720 --> 00:47:36,610
i given you a couple of examples

736
00:47:36,630 --> 00:47:42,310
in the real world

737
00:47:43,260 --> 00:47:45,450
it's often not

738
00:47:45,460 --> 00:47:47,030
desirable to say

739
00:47:47,750 --> 00:47:50,100
i have a point here

740
00:47:50,190 --> 00:47:54,500
and the point here there for the next point will surely be here

741
00:47:54,510 --> 00:47:57,890
and we can just extrapolating a straight line

742
00:47:57,940 --> 00:48:02,380
we before soft some examples where i had an algorithm to generate points and we

743
00:48:02,380 --> 00:48:06,860
fit the curve to it use the curve to predict future points and discovered it

744
00:48:06,860 --> 00:48:09,410
was nowhere close

745
00:48:09,420 --> 00:48:16,370
unfortunately we often see people do this sort of thing one of my sister favorite

746
00:48:16,370 --> 00:48:18,860
stories william ruggles house

747
00:48:18,870 --> 00:48:19,720
who was

748
00:48:19,820 --> 00:48:25,310
head of the environmental protection agency in the early nineteen seventies

749
00:48:25,380 --> 00:48:26,880
and he had a

750
00:48:26,920 --> 00:48:30,780
press conference spoke about the increased use of cars

751
00:48:30,790 --> 00:48:33,970
and the decreased amount of carpooling

752
00:48:33,990 --> 00:48:37,470
he was trying to give in people the carpool the time carpooling was on the

753
00:48:37,470 --> 00:48:39,180
way down

754
00:48:39,230 --> 00:48:41,230
and i now quote

755
00:48:41,240 --> 00:48:43,560
each car entering the central city

756
00:48:43,580 --> 00:48:46,360
started in nineteen sixty he said

757
00:48:46,410 --> 00:48:51,150
each car entering central city had one point seven people in it

758
00:48:51,220 --> 00:48:56,170
by nineteen seventy this had dropped to less than one point two

759
00:48:56,210 --> 00:48:58,220
if present trends continue

760
00:48:58,230 --> 00:48:59,870
by nineteen eighty

761
00:48:59,890 --> 00:49:06,780
more than one out of every ten cars entering the city will have no driver

762
00:49:06,860 --> 00:49:11,100
amazingly enough the press reported this is a straight story

763
00:49:11,120 --> 00:49:15,700
and talked about how we would be dramatically dropping

764
00:49:15,780 --> 00:49:16,990
i have course

765
00:49:17,000 --> 00:49:20,250
as it happened it didn't happen didn't occur

766
00:49:20,270 --> 00:49:22,870
but it's just an example of

767
00:49:22,930 --> 00:49:27,310
how much trouble you can get into my extrapolating

768
00:49:27,320 --> 00:49:31,050
the final scene i want to talk about is is probably

769
00:49:31,100 --> 00:49:34,280
the most common

770
00:49:34,380 --> 00:49:38,090
and it's called the texas sharpshooter

771
00:49:41,590 --> 00:49:47,990
now for a given session is are immediate from texas

772
00:49:48,040 --> 00:49:49,790
our right is going to be offended

773
00:49:49,800 --> 00:49:54,460
i mean things can anybody here from oklahoma

774
00:49:54,510 --> 00:49:57,740
you like it dump on oklahoma would be much better than i will talk about

775
00:49:57,740 --> 00:50:01,120
the oklahoma sharpshooter fallacy

776
00:50:01,170 --> 00:50:06,740
we will talk about the BCS rankings

777
00:50:06,830 --> 00:50:11,360
so the idea here is pretty simple one

778
00:50:11,410 --> 00:50:13,720
this is a famous marksman

779
00:50:13,730 --> 00:50:17,000
who fires his gun randomly at the side of the bond

780
00:50:17,010 --> 00:50:24,150
has a bunch of holes in it then goes and takes a piece of of

781
00:50:24,150 --> 00:50:25,780
can of paint

782
00:50:25,860 --> 00:50:28,430
and beauxis

783
00:50:28,440 --> 00:50:33,160
around all the places but what's happened here

784
00:50:33,210 --> 00:50:38,300
and people walk by the bonnet say god he is good

785
00:50:38,350 --> 00:50:40,190
so obviously

786
00:50:40,230 --> 00:50:41,790
not a good thing

787
00:50:43,230 --> 00:50:46,790
amazingly easy to fall into this trap

788
00:50:47,050 --> 00:50:49,200
so here's another example in

789
00:50:49,250 --> 00:50:51,960
august two thousand one

790
00:50:52,010 --> 00:50:53,550
a paper

791
00:50:53,560 --> 00:50:55,460
which people took seriously

792
00:50:55,500 --> 00:51:00,240
appeared in moderately serious journal called the new scientist

793
00:51:00,260 --> 00:51:03,640
and announced that researchers in scotland

794
00:51:03,680 --> 00:51:05,640
had proven

795
00:51:05,690 --> 00:51:07,200
the anorexics

796
00:51:07,210 --> 00:51:12,530
are likely to have been born in june

797
00:51:12,540 --> 00:51:14,640
i'm sure you all knew that

798
00:51:14,660 --> 00:51:18,150
how did he had they proved this or demonstrate this

799
00:51:18,200 --> 00:51:25,150
they study four hundred forty six women

800
00:51:25,210 --> 00:51:35,390
each of whom had been diagnosed is anorexic

801
00:51:35,400 --> 00:51:37,660
and they observed

802
00:51:39,870 --> 00:51:42,160
about thirty percent

803
00:51:42,710 --> 00:51:46,340
more than average

804
00:51:46,360 --> 00:51:54,650
born in june

805
00:52:04,270 --> 00:52:06,890
now since the monthly average of births

806
00:52:06,910 --> 00:52:10,100
divide this by twelve is about thirty seven

807
00:52:10,160 --> 00:52:11,560
that tells us

808
00:52:11,610 --> 00:52:13,780
that forty eight

809
00:52:13,790 --> 00:52:17,720
we're born in june

810
00:52:17,780 --> 00:52:22,590
so i first sight this seems significant and in fact if you run tests

811
00:52:22,600 --> 00:52:25,100
and ask what's the likelihood

812
00:52:25,150 --> 00:52:28,350
of that many more being born in one month

813
00:52:28,400 --> 00:52:33,770
you'll find it it's quite unlikely

814
00:52:34,190 --> 00:52:39,260
in fact you'll find the probability of this happening

815
00:52:39,330 --> 00:52:42,120
is only about three percent

816
00:52:42,120 --> 00:52:46,080
which generates mappings between these

817
00:52:46,120 --> 00:52:49,430
different representations and basically are present

818
00:52:49,450 --> 00:52:53,620
information in sort of modality neutral way

819
00:52:53,950 --> 00:52:58,540
well let's see deal with images and text then we are able to such a

820
00:52:58,540 --> 00:53:05,140
representation tool transition from images into text and back getting good work we will get

821
00:53:05,310 --> 00:53:07,030
back images

822
00:53:07,050 --> 00:53:12,470
so quite efficient technique

823
00:53:12,470 --> 00:53:19,390
last three hear presentations which have just briefly mentions of first one would be this

824
00:53:19,390 --> 00:53:21,680
collaborative tagging which appeared

825
00:53:21,760 --> 00:53:28,180
so the relevant representation on the recently with this appearance of web two point zero

826
00:53:28,580 --> 00:53:34,930
and physically collaborative tagging is a process of adding metadata to annotate some kind of

827
00:53:34,930 --> 00:53:40,850
content and this content can be documents web sites photos and so on are most

828
00:53:40,850 --> 00:53:43,600
often we have these annotations in the form of

829
00:53:43,640 --> 00:53:49,240
actually keywords and what's important is that this is being done in a collaborative way

830
00:53:49,240 --> 00:53:55,620
by many users from larger community so here we discuss about this long tail and

831
00:53:55,620 --> 00:54:00,770
so on which of these big community has collectively a lot of knowledge and somehow

832
00:54:00,770 --> 00:54:07,160
we would try to align all the participants of the community is situated they express

833
00:54:07,160 --> 00:54:11,470
their knowledge much less in the same language and this language would be this keyword

834
00:54:12,600 --> 00:54:18,330
most of us are designed to get this annotated data small for the low relatively

835
00:54:18,330 --> 00:54:20,530
low cost and then

836
00:54:20,810 --> 00:54:26,280
we can all this operate with this that here we have two examples so from

837
00:54:27,310 --> 00:54:32,370
if you're gonna flickr so here i have photos which certainly it's hard to do

838
00:54:33,010 --> 00:54:35,100
images our

839
00:54:35,100 --> 00:54:37,260
everybody who uploads

840
00:54:37,280 --> 00:54:41,570
photos at this tax a

841
00:54:41,580 --> 00:54:48,050
in the form of keywords solely this key these photos are getting comparable because we

842
00:54:48,050 --> 00:54:53,430
can just compare just the keywords and this person quite well actually

843
00:54:53,450 --> 00:54:55,720
in the same way this would work for

844
00:54:55,780 --> 00:55:03,180
the so-called delicious so here we annotating photos on flickr delicious is is this folksonomy

845
00:55:03,890 --> 00:55:07,080
other people

846
00:55:07,100 --> 00:55:12,030
at are annotated websites so this is the

847
00:55:12,070 --> 00:55:15,850
search for text mining can hear more less you get all kind of text mining

848
00:55:15,850 --> 00:55:23,870
websites and since they share this text mining keywords and then we get these results

849
00:55:23,950 --> 00:55:29,120
i think we would have a little bit more problems by getting such relatively clean

850
00:55:29,120 --> 00:55:30,800
clean list of websites

851
00:55:33,800 --> 00:55:37,050
the idea behind this collaborative work is basically

852
00:55:37,080 --> 00:55:40,660
still a lot of people around the world which have enough free time to add

853
00:55:40,660 --> 00:55:48,180
this information to explore exploiting this good in three times p

854
00:55:48,200 --> 00:55:53,830
templates frames so this would be a little bit more sophisticated techniques o

855
00:55:54,050 --> 00:55:59,140
and the idea here is to extract

856
00:55:59,160 --> 00:56:05,300
information from text by using some kind of domain specific linguistic better here i will

857
00:56:05,300 --> 00:56:09,350
show some of these batteries

858
00:56:10,490 --> 00:56:12,330
let's say

859
00:56:12,350 --> 00:56:13,370
this would be

860
00:56:13,370 --> 00:56:16,160
from this system called know it all

861
00:56:16,160 --> 00:56:23,260
this will be set of better of the show basically how this works

862
00:56:23,330 --> 00:56:26,100
let's see if it if we put in a will

863
00:56:26,120 --> 00:56:27,300
better like this

864
00:56:27,310 --> 00:56:30,030
cities such such as

865
00:56:30,080 --> 00:56:33,030
now we get a list of documents in most likely

866
00:56:33,030 --> 00:56:36,760
whenever this cities such as appears

867
00:56:37,120 --> 00:56:40,830
will be behind behind it will be the name of the city

868
00:56:40,830 --> 00:56:42,800
and so we recognise

869
00:56:42,850 --> 00:56:45,760
one is the type of the word just by

870
00:56:45,800 --> 00:56:49,470
applying this linguistic better

871
00:56:49,530 --> 00:56:51,470
on the

872
00:56:51,660 --> 00:56:54,930
of but by using this linguistic pattern to for the search

873
00:56:55,330 --> 00:56:57,890
or the two cities such as glasgow

874
00:56:58,100 --> 00:57:05,870
and so on so in the same way we can we can develop or even

875
00:57:06,950 --> 00:57:11,640
linguistic patterns for which are domain specific and we can capture a lot of information

876
00:57:11,700 --> 00:57:13,180
about the this is

877
00:57:13,240 --> 00:57:16,600
an interesting technique which is

878
00:57:16,640 --> 00:57:21,370
recently used guess in semantic web a lot because with this kind of veterans we

879
00:57:21,370 --> 00:57:24,760
can model different relationships and concepts and so on

880
00:57:24,780 --> 00:57:30,010
and the last one hundred thousand is smallest at the end of the the borel

881
00:57:30,010 --> 00:57:34,570
ontologies so here's the idea is really to represents the

882
00:57:34,580 --> 00:57:40,700
the content of the text in the first logic for first order logic and ontologies

883
00:57:40,700 --> 00:57:49,310
more less vocabulary which carries this information and the relationships between pieces of information

884
00:57:49,330 --> 00:57:53,930
so i'll skip the details here because we'll touch semantic that the DNC maybe just

885
00:57:54,180 --> 00:57:58,140
to show you an example of how text can be translated in the first order

886
00:57:58,140 --> 00:58:06,200
logic let's this is an example from cyc system which was afterwards so if we

887
00:58:06,200 --> 00:58:10,720
have a sentence like this there is groups are capable of directing because this nation

888
00:58:10,780 --> 00:58:14,910
so this can be translated to first order logic so that

889
00:58:14,930 --> 00:58:16,740
if something which

890
00:58:16,890 --> 00:58:23,470
variable groups if something is terrorist group then this implies that this group is also

891
00:58:23,490 --> 00:58:27,950
behavioural so in the relationship here kappa capable weights

892
00:58:27,970 --> 00:58:33,080
as estimating something so these are concepts from the ontology and so i don't know

893
00:58:33,080 --> 00:58:36,760
about this so this is also sort of relationship and here's a little bit longer

894
00:58:36,760 --> 00:58:40,430
sentence which is translated in the first order theory so this is

895
00:58:40,470 --> 00:58:41,200
this is

896
00:58:41,220 --> 00:58:47,050
something which we would like to out myself a lot but it is still difficult

897
00:58:47,070 --> 00:58:49,180
to the moment

898
00:58:49,180 --> 00:58:53,470
at the end of the show how these kind of representations can be used for

899
00:58:53,470 --> 00:58:56,990
a very complex question answering

900
00:58:58,950 --> 00:59:03,030
OK so this so much about the representations about text now

901
00:59:03,050 --> 00:59:05,720
once we have text presented in such a way

902
00:59:05,740 --> 00:59:08,120
what can we do with it

903
00:59:08,530 --> 00:59:14,680
so here i will show just the most typical tasks which we can perform many

904
00:59:14,680 --> 00:59:17,370
more tasks but

905
00:59:17,370 --> 00:59:19,930
but it's just a couple of

906
00:59:19,950 --> 00:59:21,100
the main one

907
00:59:21,120 --> 00:59:24,050
for document summarisation this is

908
00:59:24,050 --> 00:59:28,700
so summarisation is quite on text is quite hard problem

909
00:59:29,430 --> 00:59:31,120
nobody really succeeded

910
00:59:31,120 --> 00:59:33,220
to do it properly

911
00:59:37,450 --> 00:59:41,220
so what the task was given the task is very simple just to produce shorter

912
00:59:41,220 --> 00:59:45,220
summary version of an original document nothing more than this

913
00:59:45,240 --> 00:59:50,470
and they have two main approaches to this one of the so-called selection based on

914
00:59:50,510 --> 00:59:55,130
over the summer just a selection of sentences from an original document and this is

915
00:59:55,130 --> 00:59:59,660
something which you can get in microsoft word or in some of these

916
00:59:59,680 --> 01:00:02,910
commercial products

917
01:00:02,910 --> 01:00:08,000
tuesday is market day before yesterday

918
01:00:10,390 --> 01:00:16,660
here you have examples of vector f u v w c

919
01:00:16,700 --> 01:00:20,750
and what can i say about that so

920
01:00:20,770 --> 01:00:23,160
so those are victorious

921
01:00:23,360 --> 01:00:25,330
the spaces are two

922
01:00:25,390 --> 01:00:30,280
and for instance if you do the difference between

923
01:00:30,290 --> 01:00:31,790
u and v

924
01:00:31,840 --> 01:00:33,120
this vector

925
01:00:33,140 --> 01:00:34,860
which is w

926
01:00:34,910 --> 01:00:38,400
and if you remember what you did when you were in high school

927
01:00:38,830 --> 01:00:44,830
it doesn't matter whether these w w is here or here or here

928
01:00:44,890 --> 01:00:46,560
this is exactly the same victory

929
01:00:46,570 --> 01:00:49,980
you don't care about the precise location where it is

930
01:00:50,130 --> 01:00:52,790
if you do the sum of u and v

931
01:00:52,820 --> 01:00:55,400
you have these long vector here

932
01:00:55,440 --> 01:00:57,170
which is just obtained by

933
01:00:57,640 --> 01:01:03,540
i think you just moved to along this vector and then you had this victory

934
01:01:03,540 --> 01:01:07,270
to the end of this vector so you do this path and then you end

935
01:01:07,270 --> 01:01:08,050
up with

936
01:01:08,110 --> 01:01:10,180
these these long vectors

937
01:01:10,190 --> 01:01:12,750
and if you divided by two

938
01:01:13,530 --> 01:01:16,750
in the middle of these two points

939
01:01:16,920 --> 01:01:22,590
and the the last thing i don't even know if it's necessary to talk to

940
01:01:22,590 --> 01:01:23,790
you about that but

941
01:01:23,830 --> 01:01:28,530
let's talk to you about that if you have a scholar here and which is

942
01:01:28,530 --> 01:01:31,370
between zero and one

943
01:01:31,410 --> 01:01:36,140
then the resulting vector the multiplication of land by

944
01:01:36,160 --> 01:01:37,260
he is

945
01:01:37,330 --> 01:01:38,250
this vector

946
01:01:38,260 --> 01:01:42,960
so it's a vector which is between which is which is based on these on

947
01:01:42,960 --> 01:01:44,660
this line

948
01:01:44,660 --> 01:01:56,410
and depending of the value of mandates closer to here or here

949
01:01:58,710 --> 01:02:03,790
in a product so in products if you if you if you if you missed

950
01:02:03,790 --> 01:02:07,200
the slide if you don't understand the slide

951
01:02:07,250 --> 01:02:10,170
it's done for you

952
01:02:12,370 --> 01:02:15,620
what i'm going to talk to you about for the remaining of this talk is

953
01:02:15,620 --> 01:02:18,300
just how you deal with his

954
01:02:18,320 --> 01:02:23,560
inner product or dot products maybe a little bit more elaborate it into an elaborate

955
01:02:23,990 --> 01:02:25,650
dot products but

956
01:02:25,690 --> 01:02:27,570
i'm just going to talk about

957
01:02:27,580 --> 01:02:29,830
dot products so

958
01:02:30,030 --> 01:02:35,830
so again this is something that you learn in high school

959
01:02:35,890 --> 01:02:41,670
so this is something that is symmetric which means that you you take the argument

960
01:02:41,680 --> 01:02:47,560
against swedes then you still have the same value its billionaire

961
01:02:50,460 --> 01:02:55,000
meaning that if you take one victory taking the inner product by by himself you

962
01:02:55,030 --> 01:02:57,740
have a value that is non negative

963
01:02:57,760 --> 01:03:00,390
and define it meaning that if

964
01:03:01,260 --> 01:03:06,440
it's kind of a product of a vector brain cells is equal to zero then

965
01:03:06,440 --> 01:03:09,500
it means that you is zero itself

966
01:03:09,520 --> 01:03:14,310
and so what

967
01:03:15,400 --> 01:03:20,740
an inner product or dot product tells tell tell us about the the space

968
01:03:20,840 --> 01:03:26,940
it provides the structure of the space where you leaving actually t provides structure because

969
01:03:28,460 --> 01:03:34,920
if you do a little bit of topology the main tool is distance and

970
01:03:34,930 --> 01:03:36,430
scalar product

971
01:03:36,440 --> 01:03:38,060
induces the distance

972
01:03:38,080 --> 01:03:41,160
so there's is a here you can see the norm

973
01:03:41,250 --> 01:03:46,800
and from this normally can you can derive a distance which is already related

974
01:03:47,000 --> 01:03:51,410
so if you don't like the

975
01:03:51,560 --> 01:03:54,040
mathematical parts of of

976
01:03:54,060 --> 01:04:00,630
of things just think of an inner product as the similarity of similarity functions and

977
01:04:00,630 --> 01:04:02,340
you happy so

978
01:04:02,370 --> 01:04:05,980
this is one of the first message that you have to take home today

979
01:04:06,030 --> 01:04:11,380
so it's it's the beginning of the talk but still it's very important message message

980
01:04:11,380 --> 01:04:16,030
if you have a similarity function between the object you you're the objects that you're

981
01:04:16,030 --> 01:04:18,430
going to work on the new happy

982
01:04:18,450 --> 01:04:21,290
you can do a lot of things

983
01:04:21,300 --> 01:04:27,630
there's a slight thing that i don't say when i say that because there's some

984
01:04:27,650 --> 01:04:32,790
heritage details on the similarities but OK just just say that you have a similarity

985
01:04:32,790 --> 01:04:35,070
you're almost happen

986
01:04:35,090 --> 01:04:41,960
and the last thing that is very important is that if you want to compute

987
01:04:41,960 --> 01:04:44,150
the norm that the length of

988
01:04:44,230 --> 01:04:48,210
vector he just taking the inner product of you

989
01:04:48,240 --> 01:04:49,050
and you

990
01:04:49,210 --> 01:04:53,770
and if take the square root and give you the distance the norm of the

991
01:04:55,030 --> 01:04:59,050
so let's stay at the high school level

992
01:04:59,940 --> 01:05:03,860
in our two you have one victory with the two coordinates

993
01:05:03,870 --> 01:05:06,790
and another one and the prior to the

994
01:05:06,810 --> 01:05:11,660
the natural inner product is just the sum of the product between of the corresponding

995
01:05:19,910 --> 01:05:25,930
now we are back with a little sketch here so what is important well what

996
01:05:25,930 --> 01:05:29,180
i want you to show is how

997
01:05:29,240 --> 01:05:35,160
the inner product is core correlated to the fact that two vectors

998
01:05:35,580 --> 01:05:36,390
o point

999
01:05:36,410 --> 01:05:39,260
to the same direction or not

1000
01:05:40,080 --> 01:05:42,910
here if you look at these vector

1001
01:05:42,940 --> 01:05:47,200
you minus the this this vector or detector

1002
01:05:47,230 --> 01:05:49,270
and you look at each

1003
01:05:49,410 --> 01:05:51,520
then you see that they

1004
01:05:52,420 --> 01:05:53,200
o point

1005
01:05:53,210 --> 01:05:54,890
to the same direction

1006
01:05:54,920 --> 01:05:58,830
two the upper left corner core of these what

1007
01:05:58,860 --> 01:06:02,240
and when it is the case then you have the inner product on the of

1008
01:06:02,240 --> 01:06:06,140
those those two guys which is positive

1009
01:06:06,290 --> 01:06:13,790
on the other hand hand if you take the if you look at

1010
01:06:13,870 --> 01:06:16,270
your mind is v and g

1011
01:06:16,310 --> 01:06:18,580
they're pointing towards

1012
01:06:18,610 --> 01:06:20,940
opposing opposite directions

1013
01:06:20,950 --> 01:06:27,530
and in this situation the dot product is is negative

1014
01:06:27,660 --> 01:06:33,530
otherwise if the two vectors are perpendicular or or to low then the the dot

1015
01:06:33,530 --> 01:06:39,070
product is equal to zero

1016
01:06:39,160 --> 01:06:40,360
so that's it

1017
01:06:40,360 --> 01:06:41,980
you know

1018
01:06:42,030 --> 01:06:48,190
develop new operations because thanks completely forgot to verify that the information could be maintained

1019
01:06:49,550 --> 01:06:52,400
so you want to make sure that you've done all those so usually you have

1020
01:06:52,400 --> 01:06:53,320
to play

1021
01:06:53,320 --> 01:07:01,900
with the actions

1022
01:07:06,320 --> 01:07:08,690
in between step

1023
01:07:08,790 --> 01:07:14,480
it's not just a

1024
01:07:14,500 --> 01:07:17,360
do this to this to this

1025
01:07:17,480 --> 01:07:21,300
they were going to do now a more complicated data structure

1026
01:07:21,320 --> 01:07:26,690
not that much more complicated but its correctness is actually kind of challenges

1027
01:07:36,210 --> 01:07:38,090
answer is actually very

1028
01:07:38,090 --> 01:07:40,670
practically useful data structure

1029
01:07:40,730 --> 01:07:44,130
o amazed at how many people

1030
01:07:44,840 --> 01:07:46,320
i'm aware that there are

1031
01:07:46,320 --> 01:07:49,320
these structures of this nature they are useful for them

1032
01:07:49,380 --> 01:07:52,900
when i see people writing a really slow code

1033
01:07:52,920 --> 01:07:58,670
OK and so the example random interval trees

1034
01:08:03,480 --> 01:08:07,480
and the idea this is that we want to maintain

1035
01:08:11,630 --> 01:08:18,800
for example time intervals

1036
01:08:23,070 --> 01:08:28,690
so the whole database of time intervals that i'm trying to maintain

1037
01:08:28,840 --> 01:08:32,440
so let's just do an example here

1038
01:08:36,900 --> 01:08:41,250
that's it

1039
01:09:05,110 --> 01:09:08,190
this is going from seven to ten

1040
01:09:08,250 --> 01:09:11,190
five to two eleven

1041
01:09:11,230 --> 01:09:14,440
four eight

1042
01:09:14,460 --> 01:09:18,050
from fifteen eighteen

1043
01:09:18,070 --> 01:09:20,630
seventeen nineteen

1044
01:09:20,670 --> 01:09:23,820
twenty one to twenty three

1045
01:09:23,820 --> 01:09:27,340
OK so is the set of intervals

1046
01:09:27,340 --> 01:09:29,630
OK and

1047
01:09:29,650 --> 01:09:31,360
if we have in our life

1048
01:09:31,380 --> 01:09:34,000
since this interval i

1049
01:09:34,000 --> 01:09:35,820
which is seven karma

1050
01:09:37,790 --> 01:09:39,050
and we're going to call

1051
01:09:39,090 --> 01:09:41,750
this endpoint we're going to call the low

1052
01:09:41,860 --> 01:09:44,380
and point by i

1053
01:09:44,400 --> 01:09:46,190
and this we're going to call the high

1054
01:09:46,500 --> 01:09:52,800
o point of i resign is low and high rather than left to right

1055
01:09:52,840 --> 01:09:54,770
is because

1056
01:09:54,800 --> 01:09:58,400
never tree and ran what the left subtree in the right subtree

1057
01:09:58,420 --> 01:10:00,250
if i start saying

1058
01:10:00,300 --> 01:10:05,960
left and right free intervals and left and right for trees were really really confused

1059
01:10:05,960 --> 01:10:09,750
this is also attempt let me say when you're coding

1060
01:10:09,800 --> 01:10:13,480
you really have to think hard sometimes about the words that you're using for things

1061
01:10:13,480 --> 01:10:18,840
especially things like left and right because they get so over used throughout

1062
01:10:19,880 --> 01:10:20,880
so you

1063
01:10:20,900 --> 01:10:23,860
you know it's a good idea to come up with the whole wealth of synonyms

1064
01:10:23,860 --> 01:10:27,320
for different situations so it's clear in any piece of code

1065
01:10:27,400 --> 01:10:31,520
when you're talking for example about the intervals versus the trees is around both going

1066
01:10:31,520 --> 01:10:33,320
on here

1067
01:10:33,360 --> 01:10:37,170
OK and what we do is we want to have a we want to support

1068
01:10:37,170 --> 01:10:39,670
insertion and deletion of

1069
01:10:39,740 --> 01:10:44,530
the intervals here

1070
01:10:44,550 --> 01:10:49,980
and i would have a query which is going to be the new operation developed

1071
01:10:49,980 --> 01:10:51,840
which is going to be defined

1072
01:10:53,380 --> 01:10:55,520
an interval any interval

1073
01:10:57,530 --> 01:10:58,520
in the set

1074
01:10:58,690 --> 01:11:05,940
the overlaps

1075
01:11:10,880 --> 01:11:14,360
the query interval

1076
01:11:14,360 --> 01:11:22,770
OK so i give you a query interval like say

1077
01:11:22,770 --> 01:11:27,000
a let's say

1078
01:11:27,050 --> 01:11:28,920
you know six

1079
01:11:30,340 --> 01:11:32,440
and you can return

1080
01:11:32,440 --> 01:11:33,380
you know

1081
01:11:33,440 --> 01:11:35,670
this guy is this guy

1082
01:11:35,690 --> 01:11:36,750
this guy

1083
01:11:36,750 --> 01:11:38,290
could return any of these

1084
01:11:38,300 --> 01:11:40,590
because these are all less than fourteen

1085
01:11:40,630 --> 01:11:41,940
so whatever

1086
01:11:41,960 --> 01:11:45,340
i can return any one of those i only to return one

1087
01:11:45,360 --> 01:11:48,630
this to find one guy that overlaps

1088
01:11:50,480 --> 01:11:52,550
any question about

1089
01:11:52,590 --> 01:11:53,550
what we're going to be

1090
01:11:53,550 --> 01:11:55,400
setting up here

1091
01:11:56,440 --> 01:11:57,900
so our methodology

1092
01:11:58,770 --> 01:11:59,820
is we're going

1093
01:12:03,190 --> 01:12:07,300
first of all step one

1094
01:12:07,320 --> 01:12:09,670
here's our methodology

1095
01:12:09,690 --> 01:12:18,500
OK so step one is we're going to pick a

1096
01:12:18,520 --> 01:12:20,500
two underlying data structures

1097
01:12:20,580 --> 01:12:24,920
anybody ever suggestions to what the structure we only use here

1098
01:12:24,920 --> 01:12:27,090
the support interval tree

1099
01:12:27,110 --> 01:12:38,920
but this structure should try start with

1100
01:12:38,960 --> 01:12:42,710
this support interval tree is

1101
01:12:42,770 --> 01:12:46,710
interval tree is

1102
01:12:46,750 --> 01:12:49,300
have an idea

1103
01:12:49,480 --> 01:12:53,920
a red black tree binary search tree a red black tree really is a red

1104
01:12:53,920 --> 01:12:55,960
black tree

1105
01:12:55,960 --> 01:13:02,570
so i got to say what is he doing

1106
01:13:02,590 --> 01:13:08,460
so what's going to be the key for my red black trees so for each

1107
01:13:08,460 --> 01:13:12,840
interval which lay uses the he

1108
01:13:12,840 --> 01:13:16,380
this is this is where this is

1109
01:13:16,380 --> 01:13:19,750
because the lunch options here right

1110
01:13:24,150 --> 01:13:28,440
are some ideas i always better to branch than it is to prove

1111
01:13:28,610 --> 01:13:30,440
i crime later but if you don't

1112
01:13:30,500 --> 01:13:33,000
branch and never get a chance to

1113
01:13:33,020 --> 01:13:36,920
so the generation of ideas you need that when you're doing the design phase in

1114
01:13:36,920 --> 01:13:40,590
doing the the take-home exams

1115
01:13:43,460 --> 01:13:45,270
so we're kind of low

1116
01:13:45,270 --> 01:13:47,330
blackboard can i raised

1117
01:13:47,360 --> 01:13:50,600
create a little bit to do it

1118
01:13:50,650 --> 01:13:54,150
just to give you an idea of what is the simplest complex many of you

1119
01:13:54,170 --> 01:13:57,150
know what it is but maybe some of you don't so i just just like

1120
01:13:57,150 --> 01:13:58,690
to draw a few pictures

1121
01:13:58,750 --> 01:14:01,310
so what it is i don't think

1122
01:14:01,320 --> 01:14:04,100
i think

1123
01:14:05,380 --> 01:14:12,230
the simplest complexes typically a geometric object which is formed from

1124
01:14:12,310 --> 01:14:19,310
from simple geometric components for example two dimensional simplicial complex would be formed from triangles

1125
01:14:19,320 --> 01:14:24,460
and of course the edges of the triangles belong to the vertices and these geometric

1126
01:14:24,460 --> 01:14:29,890
objects are nicely put together so that the and the boundaries of these triangles are

1127
01:14:29,890 --> 01:14:33,370
glued in nice way maybe we had that we can have a triangle sticking out

1128
01:14:33,370 --> 01:14:37,350
like this but it can stick out like this because the vertex has degree two

1129
01:14:37,350 --> 01:14:44,060
vertex for simplest complexes are geometric objects consist of which consists of simple geometric shapes

1130
01:14:44,430 --> 01:14:50,080
triangle in the two dimensional case that he in the three dimensional case

1131
01:14:50,170 --> 01:14:56,560
and the more generally what is that regular within group together along the the

1132
01:14:56,880 --> 01:15:02,360
for surfaces and more generally if implicit in high dimensions a simplex is just a

1133
01:15:02,430 --> 01:15:07,650
higher dimensional analogue of these things that we have over here in dimensions and three

1134
01:15:07,690 --> 01:15:12,820
well general general set of compromises something a little bit more general so it's official

1135
01:15:12,850 --> 01:15:19,420
complex really has these local triangular shapes while so complex can also have for example

1136
01:15:19,420 --> 01:15:24,680
hexagonal shape polygonal shapes in general and it can also maybe have curved shapes like

1137
01:15:24,680 --> 01:15:29,440
this this would be awful of complex for this kind of geometric structures are used

1138
01:15:29,440 --> 01:15:35,500
to model this data to to achieve this solve this problem

1139
01:15:35,510 --> 01:15:41,000
i would like to five to give general clouds of points structure so this idea

1140
01:15:41,000 --> 01:15:47,690
of using objects that this of this structure data both very far back it goes

1141
01:15:47,690 --> 01:15:52,040
back to the clear which lived in the nineteenth century and the voronoi in the

1142
01:15:52,040 --> 01:15:57,140
beginning of the twentieth century the linear in the thirties so these are people who

1143
01:15:57,140 --> 01:16:03,020
are already used this idea of what to come up with the basic models and

1144
01:16:03,020 --> 01:16:05,990
then there are a lot of people who work in this nowadays and i just

1145
01:16:05,990 --> 01:16:10,640
put here down to names because they were sort of made especially impression to me

1146
01:16:10,640 --> 01:16:14,680
but there's a lot of people who work in the field of computational topology who

1147
01:16:15,060 --> 01:16:23,270
well who designed algorithms for solving problems like this for constructive simplicial complexes on a

1148
01:16:23,270 --> 01:16:28,540
finite set of points this is typically computer science called triangulation so we're talking about

1149
01:16:28,540 --> 01:16:30,070
three years here

1150
01:16:30,120 --> 01:16:35,390
so as i said there's a number of algorithms number of implementations libraries and so

1151
01:16:35,390 --> 01:16:39,290
on for a lot of people work in this field and this is quite well

1152
01:16:39,290 --> 01:16:42,520
represented in most of these

1153
01:16:42,580 --> 01:16:47,710
for example algorithms and so on in most of these models that exist the prevailing

1154
01:16:47,710 --> 01:16:52,830
model which is used as the simplest complex for triangulation or maybe already are so

1155
01:16:52,840 --> 01:16:56,660
complex and there's a little problem with this model and the problem is that it's

1156
01:16:56,660 --> 01:17:00,880
a huge thing because we are talking about different simplest of complex we want to

1157
01:17:00,880 --> 01:17:04,700
list all the elements of the triangulation want to list for example in this case

1158
01:17:05,000 --> 01:17:10,380
the trial this part of it should be re-set the triangles as well as the

1159
01:17:10,380 --> 01:17:15,880
edges as well vertices so there's a lot of data to list of when talking

1160
01:17:15,880 --> 01:17:21,230
about simplicial complexes so the problem is that these models are sometimes not as effective

1161
01:17:21,230 --> 01:17:23,380
as they won what was deemed to be

1162
01:17:23,390 --> 01:17:27,130
but on the other hand using these kind of models on the other hand we

1163
01:17:27,130 --> 01:17:31,270
we have policy into practice policy which is what it's not very old field but

1164
01:17:31,270 --> 01:17:36,730
there's a whole arsenal of various topological invariants which we can use point to stand

1165
01:17:36,750 --> 01:17:40,220
this object to come up with so we have this

1166
01:17:40,240 --> 01:17:44,960
cloud point of data we construct for example a simplicial complex on it and now

1167
01:17:45,090 --> 01:17:50,970
use all these topological invariants like for example homology groups homotopy groups and so on

1168
01:17:51,260 --> 01:17:57,570
to understand what kind of an object this this this model of the reconstructed is

1169
01:17:57,920 --> 01:18:02,880
understand its shape and its properties better

1170
01:18:02,890 --> 01:18:08,010
so as i mentioned already one of the problems with this approach is that the

1171
01:18:08,010 --> 01:18:12,180
plot to commence mothers which we come up with are often very very big so

1172
01:18:12,180 --> 01:18:17,620
here's an example of this is a simplicial complex was constructed in our lab maybe

1173
01:18:17,620 --> 01:18:18,400
a year or two ago

1174
01:18:18,840 --> 01:18:21,570
so this is the triangulation of

1175
01:18:21,590 --> 01:18:26,320
the parts of space where the set of

1176
01:18:26,370 --> 01:18:30,870
the art of sailing boat is taken out and the reason for this triangulation was

1177
01:18:30,870 --> 01:18:35,270
that we're doing some modelling on fluid flow around the shape of the on around

1178
01:18:35,270 --> 01:18:39,150
the whole of the sailing boat well in order to do that you have to

1179
01:18:39,150 --> 01:18:42,840
have a very very very big triangulation so this one for example i think has

1180
01:18:42,840 --> 01:18:49,150
more than a million triangles so it's really huge well of course for our experiments

1181
01:18:49,150 --> 01:18:53,280
we need a lot of triangles so in this case it wouldn't make sense to

1182
01:18:53,280 --> 01:18:58,330
make a smaller object but in the sense of understanding the shape of this year

1183
01:18:58,380 --> 01:19:02,730
of the how well a million triangles is obviously or to to hear stories obviously

1184
01:19:02,730 --> 01:19:07,750
too much so we could simplify the strangulation a lot for that purpose well anyway

1185
01:19:07,750 --> 01:19:12,590
as we see the fact that we have very very large model scores for implementation

1186
01:19:12,840 --> 01:19:19,650
are efficient that have implementations of things which are simpler than simplicial complexes for example

1187
01:19:19,650 --> 01:19:27,900
typically see the CW complexes are a nicer way to describe the logical spaces and

1188
01:19:27,920 --> 01:19:32,470
we would like to have a few thousand possible to still grasp the metadata that

1189
01:19:32,470 --> 01:19:37,050
we need of course and on the other hand we need efficient algorithms to do

1190
01:19:37,090 --> 01:19:39,010
with things like this

1191
01:19:39,020 --> 01:19:44,000
OK so these are two major problems which are addressed in computational topology now here's

1192
01:19:44,000 --> 01:19:48,670
another problem the problem the other problem is that well we have this

1193
01:19:48,670 --> 01:19:52,200
the first

1194
01:19:52,260 --> 01:19:56,090
will actually see that a lot of the mechanism is very similar to what we

1195
01:19:56,090 --> 01:19:57,270
had before

1196
01:19:57,300 --> 01:19:58,230
it's just

1197
01:19:58,270 --> 01:20:02,190
you know reapplying the same tricks again

1198
01:20:03,010 --> 01:20:06,780
the first thing that we had all the exponential family

1199
01:20:06,780 --> 01:20:08,590
model so p of x

1200
01:20:08,600 --> 01:20:10,800
parameterized by theta

1201
01:20:10,890 --> 01:20:12,390
given by

1202
01:20:12,440 --> 01:20:15,120
e two

1203
01:20:15,140 --> 01:20:17,760
five with theta

1204
01:20:17,820 --> 01:20:22,980
minus gl later

1205
01:20:23,520 --> 01:20:29,090
and this is why is called the sufficient statistics

1206
01:20:29,170 --> 01:20:34,590
this was called the natural parameter

1207
01:20:34,590 --> 01:20:40,150
and this was the

1208
01:20:40,160 --> 01:20:43,020
cumulant generating function

1209
01:20:47,970 --> 01:20:49,020
and we

1210
01:20:49,040 --> 01:20:54,220
then concluded after a bit of the derivation that if we want to

1211
01:20:54,300 --> 01:20:56,890
right the negative log posterior

1212
01:20:56,940 --> 01:20:58,310
so minus

1213
01:20:59,640 --> 01:21:00,890
of p of

1214
01:21:02,120 --> 01:21:05,120
given the doctor solvent of observations

1215
01:21:05,140 --> 01:21:06,760
this would be

1216
01:21:06,810 --> 01:21:10,030
in general given by

1217
01:21:10,060 --> 01:21:11,660
m tom's

1218
01:21:11,670 --> 01:21:13,030
g of data

1219
01:21:17,140 --> 01:21:21,030
one m

1220
01:21:21,090 --> 01:21:24,080
some i going from one

1221
01:21:25,360 --> 01:21:28,370
five of x i

1222
01:21:28,390 --> 01:21:32,120
inner product with data

1223
01:21:33,590 --> 01:21:36,650
one of the two sigma squared

1224
01:21:36,660 --> 01:21:43,800
norm over theta squared plus some constant which is independent data

1225
01:21:45,750 --> 01:21:47,600
and so this was basically

1226
01:21:47,620 --> 01:21:53,770
trying to estimate with their parameter from the exponential family with corresponding

1227
01:21:53,780 --> 01:22:02,160
critical organisation of data we saw that there are connections to agustin process

1228
01:22:02,210 --> 01:22:06,330
and i'm not going to go into that very much more detailed and that there

1229
01:22:06,330 --> 01:22:09,620
are connections to extremes

1230
01:22:09,630 --> 01:22:11,910
so that's what we did not find

1231
01:22:13,770 --> 01:22:18,250
read if we looked at something like a like iteration and we derive the margin

1232
01:22:18,260 --> 01:22:22,120
and their connections to novelty detection

1233
01:22:27,830 --> 01:22:31,490
the next six makes that we do is we said well actually rather than building

1234
01:22:31,490 --> 01:22:32,960
a generative model

1235
01:22:34,080 --> 01:22:38,040
we might want to look at the discriminative models

1236
01:22:38,050 --> 01:22:42,120
in other words i might want to look at or conditional models we have p

1237
01:22:42,120 --> 01:22:43,530
of y

1238
01:22:44,990 --> 01:22:47,120
and parameterized by theta

1239
01:22:47,140 --> 01:22:52,450
would be given in the very same form as above by the two them

1240
01:22:52,470 --> 01:22:55,890
in approach between forth ics and why

1241
01:22:55,940 --> 01:22:57,780
and theta

1242
01:22:57,800 --> 01:23:06,730
and now the normalisation will depend on the location so minus two data given x

1243
01:23:06,740 --> 01:23:08,720
and again this will be

1244
01:23:09,580 --> 01:23:12,830
government generating function but not for conditional models

1245
01:23:12,850 --> 01:23:16,910
this to be sufficient statistics in x and y natural parameters

1246
01:23:17,460 --> 01:23:20,940
and just incomplete similarity to about here

1247
01:23:20,950 --> 01:23:24,710
we have the corresponding map problem

1248
01:23:24,840 --> 01:23:31,290
of minus log

1249
01:23:32,140 --> 01:23:33,040
p of

1250
01:23:33,930 --> 01:23:36,510
given x and y

1251
01:23:36,530 --> 01:23:38,640
being given y

1252
01:23:38,650 --> 01:23:41,560
and now we had

1253
01:23:41,580 --> 01:23:43,290
well some over

1254
01:23:43,300 --> 01:23:47,250
are you going from one m

1255
01:23:47,260 --> 01:23:48,810
g of data

1256
01:23:48,830 --> 01:23:54,450
given x y somebody actually reminded me that i had forgotten to break their minus

1257
01:23:55,860 --> 01:23:57,340
inner product between

1258
01:23:57,390 --> 01:24:00,530
five of x i y i

1259
01:24:00,540 --> 01:24:04,600
and theta

1260
01:24:06,100 --> 01:24:07,100
one of the

1261
01:24:07,110 --> 01:24:09,030
two sigma squared

1262
01:24:09,090 --> 01:24:10,120
norm of

1263
01:24:10,130 --> 01:24:11,820
o thing to win

1264
01:24:11,830 --> 01:24:13,720
plus the concert

1265
01:24:16,760 --> 01:24:22,000
and what we saw is that depending on what model you pick for y

1266
01:24:22,010 --> 01:24:27,860
like which domain which type of sufficient statistics you could actually recover a couple of

1267
01:24:29,900 --> 01:24:34,890
estimation procedures for discussing process classification will come up with a special case

1268
01:24:34,940 --> 01:24:38,250
now what we're going to do today is

1269
01:24:38,270 --> 01:24:43,630
we're going to exploit the connection between this and the exponential and the graphical model

1270
01:24:43,640 --> 01:24:45,670
a bit further

1271
01:24:47,770 --> 01:24:56,490
the third ingredient that we had was if we have some graphical model

1272
01:24:56,580 --> 01:24:59,550
then there's the difficult theorem

1273
01:24:59,700 --> 01:25:02,150
which tells us

1274
01:25:03,510 --> 01:25:04,690
p of x

1275
01:25:04,740 --> 01:25:07,740
has to be written

1276
01:25:08,800 --> 01:25:12,480
well that's just some normalisation the product

1277
01:25:12,490 --> 01:25:15,850
overseeing the set of maximal cliques

1278
01:25:15,870 --> 01:25:18,010
of so c

1279
01:25:18,020 --> 01:25:18,990
of six

1280
01:25:22,640 --> 01:25:29,670
and basically what it means is that for any distribution with certain conditional independence structure

1281
01:25:29,690 --> 01:25:32,520
which can be nice to represented by a graph

1282
01:25:33,830 --> 01:25:38,360
density the joint density in or distribution over all the random variables

1283
01:25:38,380 --> 01:25:41,850
assuming that doesn't vanishingly where

1284
01:25:43,450 --> 01:25:48,520
i can be written as a product of functions on the maximal cliques

1285
01:25:48,530 --> 01:25:53,880
again this has the consequence if we combine it with our exponential families notation

1286
01:25:55,440 --> 01:25:57,150
well of function

1287
01:25:57,170 --> 01:25:59,260
why of fakes

1288
01:25:59,280 --> 01:26:00,540
had to be written

1289
01:26:00,550 --> 01:26:04,190
as some form some functions phi c

1290
01:26:04,210 --> 01:26:05,330
of c

1291
01:26:05,330 --> 01:26:09,990
the ice is the example of variances is of the axis of the sample variances

1292
01:26:09,990 --> 01:26:12,680
which is what these things are then

1293
01:26:12,710 --> 01:26:14,150
you get

1294
01:26:14,180 --> 01:26:16,460
the some of these and these

1295
01:26:16,470 --> 01:26:19,900
there are the sample variances of the principal components of the ones

1296
01:26:19,920 --> 01:26:20,960
so that's

1297
01:26:20,980 --> 01:26:23,190
that's why one

1298
01:26:23,220 --> 01:26:27,170
so if you take the vector y one of the y two also always one

1299
01:26:27,170 --> 01:26:29,780
and then you get the same total

1300
01:26:29,810 --> 01:26:33,220
as if you take x one so sec it's variance in order additive

1301
01:26:34,190 --> 01:26:38,350
so let's say you can think of this as

1302
01:26:38,360 --> 01:26:40,180
in some ways

1303
01:26:40,210 --> 01:26:44,710
decomposing these variances into an nice form

1304
01:26:44,710 --> 01:26:50,770
so we end up with

1305
01:26:50,810 --> 01:26:52,680
what we're doing this procedure

1306
01:26:52,700 --> 01:26:56,550
two the iris data

1307
01:26:56,710 --> 01:26:59,420
you can see that

1308
01:26:59,430 --> 01:27:02,560
y three y for the third and fourth principal components

1309
01:27:02,600 --> 01:27:04,420
was no

1310
01:27:04,430 --> 01:27:05,890
because pounds

1311
01:27:05,920 --> 01:27:08,860
involving one three are constantly evolving one four

1312
01:27:08,880 --> 01:27:11,000
the second one one

1313
01:27:11,000 --> 01:27:13,470
got one one one one seems to give me too

1314
01:27:16,410 --> 01:27:22,000
so some sort of this is second

1315
01:27:22,020 --> 01:27:23,970
but focus

1316
01:27:24,000 --> 01:27:27,740
well this depends on

1317
01:27:27,770 --> 01:27:33,040
and how much is going to some people would argue that sort of bounded and

1318
01:27:33,040 --> 01:27:38,030
the weights of people argue that with me

1319
01:27:38,280 --> 01:27:39,080
also feel

1320
01:27:39,080 --> 01:27:41,050
people who basically

1321
01:27:42,250 --> 01:27:43,520
well before

1322
01:27:43,550 --> 01:27:47,860
the information in the two clusters spread all over the diagram

1323
01:27:47,860 --> 01:27:54,080
well to this all

1324
01:27:54,110 --> 01:27:58,170
the picture tells you about sampling cells are two

1325
01:27:59,970 --> 01:28:04,130
what's the problem

1326
01:28:04,140 --> 01:28:06,850
length tells the

1327
01:28:06,860 --> 01:28:07,910
two really

1328
01:28:07,910 --> 01:28:10,000
which tells us to

1329
01:28:10,020 --> 01:28:15,680
and i think you see some of the problems with

1330
01:28:17,620 --> 01:28:22,520
but combinations of who said what you can see

1331
01:28:22,550 --> 01:28:26,300
so what does anyone basic structure that the two groups

1332
01:28:26,640 --> 01:28:30,070
so what we have actually managed to achieve

1333
01:28:31,140 --> 01:28:36,020
the basic structure is now confined to just one variable

1334
01:28:36,040 --> 01:28:40,550
a lot not for the labels it was before so we will concentrate the solution

1335
01:28:40,550 --> 01:28:43,900
to this although here might argue the

1336
01:28:43,920 --> 01:28:49,020
i want to white so basically all the basic variational bayes

1337
01:28:49,050 --> 01:28:53,610
structure that information is not one one so we could discuss y two y three

1338
01:28:53,610 --> 01:28:56,360
y four because they're just one fluctuations

1339
01:28:56,370 --> 01:28:59,260
that's the thought behind it

1340
01:28:59,270 --> 01:29:05,680
so you do that way by looking at the pictures i will want to be

1341
01:29:05,740 --> 01:29:08,770
but it also

1342
01:29:08,770 --> 01:29:10,730
trying decide how many

1343
01:29:10,740 --> 01:29:16,860
dimensions you have otherwise effective dimensionality various sort of ad hoc methods of doing this

1344
01:29:16,860 --> 01:29:18,270
is the formal

1345
01:29:18,270 --> 01:29:21,990
correct one but this is what people do

1346
01:29:22,040 --> 01:29:25,170
so the first one is

1347
01:29:25,210 --> 01:29:29,920
remember is that this is quantity for example on this but if the same as

1348
01:29:29,920 --> 01:29:32,400
the whole of the variance for the axes

1349
01:29:33,230 --> 01:29:36,670
you can say well what proportion of variance

1350
01:29:37,590 --> 01:29:44,540
picked so if i just take the first are composed of photographers two is what

1351
01:29:44,540 --> 01:29:50,420
proportions that mix of looking at the situation five lambda one lambda two divided by

1352
01:29:50,450 --> 01:29:55,580
the sum of all the wonders can say ninety four seventy percent but i would

1353
01:29:55,580 --> 01:29:59,110
tend to think that was a good representation of the data

1354
01:29:59,170 --> 01:30:03,570
if i was looking at a picture of what if we also so that's one

1355
01:30:03,580 --> 01:30:05,050
do people use

1356
01:30:05,080 --> 01:30:08,360
one is this

1357
01:30:09,210 --> 01:30:09,870
the no

1358
01:30:09,890 --> 01:30:13,590
falsifications just this is what people think works

1359
01:30:14,450 --> 01:30:16,450
so it's important variance

1360
01:30:16,480 --> 01:30:20,490
if you take the average of the wonders

1361
01:30:20,520 --> 01:30:21,890
that the

1362
01:30:21,900 --> 01:30:23,020
if you see

1363
01:30:23,020 --> 01:30:26,460
so there is less than the average and variation

1364
01:30:26,470 --> 01:30:33,330
if you find lot less than the average number but that's really saying that if

1365
01:30:33,330 --> 01:30:40,920
you can actually read this beta which had swept you two one which is where

1366
01:30:40,960 --> 01:30:41,500
the vision

1367
01:30:41,520 --> 01:30:45,670
there you can rewrite is something nice

1368
01:30:45,670 --> 01:30:47,100
and prime

1369
01:30:47,120 --> 01:30:52,310
is the proportion of positive that so he might be present and what you get

1370
01:30:52,330 --> 01:30:55,600
if you rewrite like this is

1371
01:30:55,620 --> 01:30:58,560
a difference where where

1372
01:30:58,730 --> 01:31:04,730
s trying now takes the role the true probability this is the proportion of positives

1373
01:31:04,830 --> 01:31:06,940
second that you're talking about

1374
01:31:06,960 --> 01:31:11,020
the second here but it is it is referred to as the

1375
01:31:11,080 --> 01:31:12,690
he had the story

1376
01:31:12,710 --> 01:31:20,160
it's in your best gnx but only talks about the slope of process

1377
01:31:20,170 --> 01:31:23,790
for a long story short you can do this decomposition

1378
01:31:23,810 --> 01:31:28,100
exactly which is different from for fear of the

1379
01:31:28,100 --> 01:31:29,810
two with approximately

1380
01:31:30,790 --> 01:31:32,690
dividing this quantity will be

1381
01:31:32,890 --> 01:31:34,580
using the wrong

1382
01:31:34,600 --> 01:31:42,920
you could have two debates inside the boundaries you can examine close these two things

1383
01:31:42,940 --> 01:31:46,870
this not something very useful but because

1384
01:31:46,920 --> 01:31:53,210
if this decomposition gives you much more information than the previous four games with itself

1385
01:31:53,210 --> 01:31:54,140
is you

1386
01:31:54,270 --> 01:31:56,390
to show that

1387
01:31:56,390 --> 01:32:01,810
i have a problem here also some actual models that we

1388
01:32:02,210 --> 01:32:05,770
basically this is doubtful cross validation so

1389
01:32:05,900 --> 01:32:08,250
this is a performance

1390
01:32:09,690 --> 01:32:17,850
each of the ten fold so i wanted to look at is

1391
01:32:17,870 --> 01:32:21,600
these points here i left

1392
01:32:21,620 --> 01:32:22,420
which is

1393
01:32:22,460 --> 01:32:24,140
clearly can

1394
01:32:25,890 --> 01:32:28,520
calculate probability just go right

1395
01:32:28,540 --> 01:32:32,750
binary numbers so it is not not surprised that this is not going to

1396
01:32:33,020 --> 01:32:36,750
but by doing the ROC convex hull penetrations

1397
01:32:36,770 --> 01:32:39,170
we can get the calibration loss

1398
01:32:39,190 --> 01:32:41,250
close to zero

1399
01:32:41,270 --> 01:32:42,330
but if we in

1400
01:32:42,350 --> 01:32:46,560
have a little bit refinement loss because are creating the convex hull

1401
01:32:47,310 --> 01:32:52,440
many previously we've only horizontal and vertical segments which means that we find a lot

1402
01:32:52,530 --> 01:32:53,310
of zero

1403
01:32:53,500 --> 01:32:55,640
by introducing times

1404
01:32:55,660 --> 01:32:59,640
we will not get segments so we

1405
01:32:59,870 --> 01:33:06,790
the calibration of close to zero at the expense of a slight increase of of

1406
01:33:06,790 --> 01:33:12,640
the refined and is another example here we have the calibration with the same as

1407
01:33:12,640 --> 01:33:14,580
the naive bayes

1408
01:33:14,600 --> 01:33:18,420
and you could argue that may be in three four

1409
01:33:18,440 --> 01:33:20,920
in this celebration doesn't make an awful

1410
01:33:20,920 --> 01:33:25,170
so it's really interesting because these are important in machine learning the first ones i

1411
01:33:25,170 --> 01:33:31,840
haven't seen with BPP any results related to machine learning i was just showing this

1412
01:33:31,840 --> 01:33:36,360
because that's the idea of one of the national ideas of randomness is to do

1413
01:33:37,200 --> 01:33:39,370
that idea is to say

1414
01:33:39,380 --> 01:33:41,690
take it totally

1415
01:33:41,710 --> 01:33:43,280
asymmetric view

1416
01:33:43,300 --> 01:33:45,880
which is the same well we want some

1417
01:33:46,130 --> 01:33:53,750
nondeterministic of course machines but this time either no computation accepts

1418
01:33:53,760 --> 01:33:58,360
OK no computation is you can't accept that we know or half of them do

1419
01:33:58,900 --> 01:34:03,160
now what's the difference between the thing before

1420
01:34:03,180 --> 01:34:06,460
and come to say it's not sure we'll come back to that later

1421
01:34:06,470 --> 01:34:10,120
so for the following with this so this is really the number of computations are

1422
01:34:10,130 --> 01:34:12,160
not taking the different steps

1423
01:34:12,220 --> 01:34:15,980
which i'm doing it i really want half of the computations being able to say

1424
01:34:15,990 --> 01:34:20,680
that half the computations work or at least some fraction

1425
01:34:20,740 --> 01:34:24,490
larger than zero is going to is going to work

1426
01:34:24,510 --> 01:34:28,080
what people believe is in our opinion believe is not

1427
01:34:30,500 --> 01:34:36,330
and the is sorry yes i believe it is in our

1428
01:34:36,350 --> 01:34:39,320
it is in our people without being now what i'm saying is to not be

1429
01:34:39,320 --> 01:34:44,620
but not p OK policy product polynomial in equivalence

1430
01:34:44,640 --> 01:34:46,190
so in equivalence

1431
01:34:46,210 --> 01:34:49,330
so the idea is you've got some polynomials

1432
01:34:49,370 --> 01:34:57,600
kx one plus three x say it right next to the squared plus three multiplied

1433
01:34:57,600 --> 01:34:58,540
by often

1434
01:34:58,810 --> 01:35:03,970
another one will be explored seven and here's some more polynomials and the question is

1435
01:35:03,970 --> 01:35:04,420
if i do

1436
01:35:04,840 --> 01:35:09,910
the product of these polynomials and here the product of these polynomials

1437
01:35:09,920 --> 01:35:12,630
do we obtain something different

1438
01:35:17,010 --> 01:35:20,230
so i suppose we've got this polynomial this

1439
01:35:20,260 --> 01:35:21,450
this part of

1440
01:35:21,460 --> 01:35:24,290
i know about this one this one this one this one this one so don't

1441
01:35:24,290 --> 01:35:25,790
have to be the same length

1442
01:35:25,800 --> 01:35:27,180
the question is

1443
01:35:27,200 --> 01:35:31,810
do if i by all this together and i'm going play all this together as

1444
01:35:31,810 --> 01:35:33,280
far as polynomials go

1445
01:35:33,290 --> 01:35:36,930
are they are they different

1446
01:35:36,940 --> 01:35:39,930
so how would you solve that problem

1447
01:35:39,940 --> 01:35:43,330
the basic way of solving the problem is just to develop

1448
01:35:44,180 --> 01:35:45,760
this into a some

1449
01:35:45,810 --> 01:35:49,680
of monomials this into a sum of monomials and see if you do the same

1450
01:35:49,680 --> 01:35:53,530
if you think that clearly gives us nazis you obtain the same that you're done

1451
01:35:53,560 --> 01:35:56,070
you say yes sorry say no

1452
01:35:56,080 --> 01:35:59,970
they are not different and if you think something different than you say yes there

1453
01:35:59,970 --> 01:36:03,230
are different what's the problem with that approach

1454
01:36:03,250 --> 01:36:06,420
problem with that approach is that it is too long

1455
01:36:06,430 --> 01:36:13,470
because if you start taking all these products thinking about the big problems here you're

1456
01:36:13,470 --> 01:36:19,150
going to be in size not polynomial so the algorithm itself that you could be

1457
01:36:19,150 --> 01:36:22,940
thinking about of developing all this is not

1458
01:36:25,240 --> 01:36:30,930
so that doesn't work nevertheless why is this problem in our p

1459
01:36:30,980 --> 01:36:34,850
because you can prove quite easily that if you take

1460
01:36:34,900 --> 01:36:37,270
some number

1461
01:36:37,290 --> 01:36:41,930
right and you just apply this part of the of each one of these polynomials

1462
01:36:41,940 --> 01:36:42,990
with this number

1463
01:36:43,000 --> 01:36:45,350
and then building the product

1464
01:36:45,370 --> 01:36:47,010
of what you obtain

1465
01:36:47,020 --> 01:36:48,520
do the same here

1466
01:36:48,580 --> 01:36:53,980
well case number one is they are they are equal meaning by that the answer

1467
01:36:53,980 --> 01:36:57,640
should be no when you're always going to obtain the same answer so just answer

1468
01:36:57,640 --> 01:36:59,010
no that's no problem

1469
01:36:59,020 --> 01:37:02,060
case number two is that these polynomials

1470
01:37:02,070 --> 01:37:03,430
the result

1471
01:37:03,460 --> 01:37:09,060
is sort of the the product of these polynomials is different to product this polynomials

1472
01:37:09,850 --> 01:37:16,650
apart from in a finite number of cases you should be updating something different

1473
01:37:16,660 --> 01:37:19,810
OK if i give you a fuzzy calculator

1474
01:37:19,910 --> 01:37:22,070
the value zero well

1475
01:37:22,210 --> 01:37:25,480
for value zero might be OK but i mean i would be OK for a

1476
01:37:25,480 --> 01:37:29,310
certain number of possible routes and here press officer

1477
01:37:29,320 --> 01:37:33,810
a number of possible but in most cases i would be different

1478
01:37:33,870 --> 01:37:37,110
so so in fact in that case i will be obtaining yes

1479
01:37:37,130 --> 01:37:39,370
very often so i've got a nice

1480
01:37:39,380 --> 01:37:48,110
randomized algorithm that solves in polynomial time this this problem whereas i do not have

1481
01:37:48,110 --> 01:37:51,180
it is not approved of course the problem is not p

1482
01:37:51,200 --> 01:37:54,680
if we could prove the problem is not in p then we would have typical

1483
01:37:54,700 --> 01:37:56,990
prove that p is

1484
01:37:57,090 --> 01:38:01,730
different from NP but you can see why people believe that is not in PC

1485
01:38:01,730 --> 01:38:03,610
and believe that it is in our p

1486
01:38:04,610 --> 01:38:20,010
OK so there's a special class called zpzp p is zero error probabilistic polynomial time

1487
01:38:20,310 --> 01:38:25,330
that is just to the intersection of four and co RP is saying well perhaps

1488
01:38:25,330 --> 01:38:26,340
i can

1489
01:38:26,360 --> 01:38:31,050
so make mistakes on one side small and on the other side small run both

1490
01:38:31,050 --> 01:38:32,590
algorithms and parallel

1491
01:38:32,610 --> 01:38:38,360
and notice that if i you know if i repeat experiments various times we taking

1492
01:38:38,360 --> 01:38:41,370
random numbers i'm i'm pretty sure

1493
01:38:41,380 --> 01:38:46,130
that's pretty sure means i can get the probability of making an error down to

1494
01:38:46,130 --> 01:38:49,250
rule something very very very small

1495
01:38:49,340 --> 01:38:55,220
so this sort of algorithms which basically i just keep on repeating a certain number

1496
01:38:55,220 --> 01:38:56,800
of times the

1497
01:38:58,600 --> 01:39:04,030
the experiment until i'm happy about things or usually called las vegas

1498
01:39:04,040 --> 01:39:07,610
they the

1499
01:39:07,620 --> 01:39:11,080
they are always right because of the end of the day i'm really going to

1500
01:39:11,080 --> 01:39:16,060
be able to to reach short conclusion but what's going to happen is that there

1501
01:39:16,060 --> 01:39:20,730
might be worse case where really got keep on going until i get this conclusion

1502
01:39:20,730 --> 01:39:24,910
i need those words case absolutely horrible what happens in the worst case because i

1503
01:39:24,910 --> 01:39:29,300
mean the randomized situation are going to be so false so

1504
01:39:29,320 --> 01:39:35,390
so unusual that it's only in average i'm going to be college

1505
01:39:35,400 --> 01:39:39,280
so let's have a guess in quite the same as the monte carlo

1506
01:39:39,300 --> 01:39:40,560
OK let's see

1507
01:39:40,570 --> 01:39:42,590
quickly yes

1508
01:39:42,980 --> 01:39:46,830
two examples some examples of stochastic algorithms just for the sake of it

1509
01:39:46,900 --> 01:39:49,090
monte carlo algorithms

1510
01:39:49,090 --> 01:39:52,240
really folks for

1511
01:40:02,950 --> 01:40:05,270
the response

1512
01:40:09,370 --> 01:40:14,820
he that's sent ten percent of

1513
01:40:15,410 --> 01:40:16,900
you are

1514
01:40:18,410 --> 01:40:21,910
suppose now twenty one three

1515
01:40:23,050 --> 01:40:28,660
for the one that really wish

1516
01:40:28,670 --> 01:40:31,060
so this is more

1517
01:40:32,280 --> 01:40:34,150
the rest

1518
01:40:47,510 --> 01:40:50,360
have to just

1519
01:41:01,010 --> 01:41:03,160
or you were

1520
01:41:03,160 --> 01:41:06,810
i think one

1521
01:41:11,730 --> 01:41:14,730
so that's all

1522
01:41:15,540 --> 01:41:17,270
or the area

1523
01:41:17,310 --> 01:41:19,950
this is

1524
01:41:19,960 --> 01:41:22,990
so the important

1525
01:41:26,880 --> 01:41:29,940
threshold so it's an

1526
01:41:29,940 --> 01:41:31,370
he was

1527
01:41:34,000 --> 01:41:36,650
that's right

1528
01:41:40,290 --> 01:41:41,450
so well

1529
01:41:47,630 --> 01:41:50,210
probability metrics

1530
01:41:52,620 --> 01:41:55,260
and one one

1531
01:42:02,960 --> 01:42:07,150
so that that's where

1532
01:42:07,160 --> 01:42:09,210
there is a small

1533
01:42:09,760 --> 01:42:17,230
or one five to three

1534
01:42:18,980 --> 01:42:22,920
things like this one

1535
01:42:22,940 --> 01:42:25,570
so that means that he

1536
01:42:25,760 --> 01:42:28,850
what i want to track

1537
01:42:28,950 --> 01:42:35,500
harry potter

1538
01:42:35,520 --> 01:42:37,540
so for have

1539
01:42:38,600 --> 01:42:41,230
it's you know it was

1540
01:42:41,480 --> 01:42:45,410
we're going to see things

1541
01:42:46,800 --> 01:42:48,520
based on

1542
01:42:48,530 --> 01:42:54,250
so you see this for a year or any sort of g

1543
01:42:54,270 --> 01:42:57,970
just like in the

1544
01:43:00,410 --> 01:43:02,310
the one

1545
01:43:02,330 --> 01:43:04,370
i think it's wrong

1546
01:43:05,730 --> 01:43:10,880
the brain does not work so

1547
01:43:10,900 --> 01:43:13,370
she three

1548
01:43:13,390 --> 01:43:17,620
use it to model selection

1549
01:43:17,660 --> 01:43:20,730
the second half still

1550
01:43:21,230 --> 01:43:22,330
and he

1551
01:43:22,340 --> 01:43:26,170
she that it would

1552
01:43:34,320 --> 01:43:37,600
based on that

1553
01:43:40,960 --> 01:43:45,910
three were

1554
01:43:54,850 --> 01:43:57,390
you know

1555
01:43:59,570 --> 01:44:02,950
twenty years

1556
01:44:06,020 --> 01:44:10,390
while you were problems

1557
01:44:10,410 --> 01:44:13,940
all the problems in the world

1558
01:44:15,340 --> 01:44:16,940
i don't

1559
01:44:17,150 --> 01:44:19,260
at last

1560
01:44:32,830 --> 01:44:34,990
are one problem

1561
01:44:35,000 --> 01:44:37,000
on the day when the

1562
01:44:38,850 --> 01:44:40,480
but problem

1563
01:44:42,230 --> 01:44:49,990
for his work

1564
01:44:52,820 --> 01:44:54,030
i don't know

1565
01:44:56,010 --> 01:44:58,580
i don't want to be

1566
01:45:09,440 --> 01:45:12,550
you did

1567
01:45:18,980 --> 01:45:24,720
it's really a sufficient condition for a long

1568
01:45:28,480 --> 01:45:30,120
and then it

1569
01:45:31,650 --> 01:45:36,260
one is

1570
01:45:44,190 --> 01:45:48,040
there also

1571
01:45:48,310 --> 01:45:50,690
what is it

1572
01:46:04,490 --> 01:46:07,640
and so

1573
01:46:09,730 --> 01:46:13,370
cells so the

1574
01:46:31,890 --> 01:46:34,450
he was

1575
01:46:34,900 --> 01:46:37,640
what is self

1576
01:46:40,560 --> 01:46:44,430
but the average all

1577
01:46:44,490 --> 01:46:47,370
eighty nine problems so

1578
01:46:47,370 --> 01:46:51,180
to your benefit and use

1579
01:46:51,230 --> 01:46:54,940
well i turn out solutions

1580
01:46:54,960 --> 01:47:00,810
only on an individual basis in the fashion that i just described i found that

1581
01:47:00,810 --> 01:47:04,690
if i write down the solution to each of these problems you don't do them

1582
01:47:04,970 --> 01:47:08,120
and you say ah so that's how you do that throat and if i not

1583
01:47:08,120 --> 01:47:10,650
look at it until the night before the quiz

1584
01:47:11,450 --> 01:47:16,250
there were not solutions handed out others and corrections on an individual basis on your

1585
01:47:17,620 --> 01:47:19,850
so said all i i wanted to say

1586
01:47:19,870 --> 01:47:22,130
i think that's about all

1587
01:47:22,140 --> 01:47:24,570
for the formalities

1588
01:47:24,620 --> 01:47:28,460
actually i should

1589
01:47:28,470 --> 01:47:32,450
had one postscript they say the the causes don't

1590
01:47:32,500 --> 01:47:36,800
how they excuse me the problem sets don't count anything put your final grade

1591
01:47:36,850 --> 01:47:44,110
they do in one minor sense when you have a large class in new plotted

1592
01:47:44,110 --> 01:47:49,340
the grades and there are no obvious with gaps in between the comes a point

1593
01:47:49,340 --> 01:47:53,250
where you have to separate one group from another

1594
01:47:53,510 --> 01:47:56,900
and if you'd done well on the quizzes and you've done well on the problem

1595
01:47:56,900 --> 01:48:01,360
sets and there's just one quiz it's a little bit though it's OK here she

1596
01:48:01,360 --> 01:48:05,650
had a bad day that afternoon and i'll give you the benefit of the doubt

1597
01:48:05,710 --> 01:48:09,530
and even though i'm not a vindictive sort if you're right on the fence you

1598
01:48:09,530 --> 01:48:12,160
haven't done any other problems

1599
01:48:12,210 --> 01:48:14,050
then without malice i say

1600
01:48:14,100 --> 01:48:18,620
gotcha you about how whole and those side of the barricade and i think that's

1601
01:48:18,620 --> 01:48:24,600
only a natural indication because there are some cases where with solomon judgement you have

1602
01:48:24,600 --> 01:48:26,740
to decide who

1603
01:48:26,790 --> 01:48:29,940
it's what green

1604
01:48:29,960 --> 01:48:35,430
OK let me say have a little bit about two

1605
01:48:37,440 --> 01:48:39,960
there are

1606
01:48:39,980 --> 01:48:45,780
a number of books that deal with crystallography

1607
01:48:45,790 --> 01:48:49,820
for the most part though they

1608
01:48:49,870 --> 01:48:56,040
it consists of an introductory chapter every single book on the solid state feels compelled

1609
01:48:56,310 --> 01:49:03,050
to write some sort of half-baked chapter crystal structure or crystallography

1610
01:49:03,070 --> 01:49:06,270
usually these chapters consist of big tables

1611
01:49:06,280 --> 01:49:08,920
and they say there are

1612
01:49:08,930 --> 01:49:14,190
fourteen of these there are seventeen of these there were thirty two of these there

1613
01:49:14,190 --> 01:49:20,230
were two hundred thirty of these in one thousand one hundred seventy of these

1614
01:49:20,240 --> 01:49:21,040
and then

1615
01:49:21,050 --> 01:49:26,540
that has all the excitement and stimulation of reading the telephone directory

1616
01:49:26,620 --> 01:49:30,030
you know it's the crazy cats the characters but it's awfully hard to to see

1617
01:49:30,030 --> 01:49:31,740
the plot

1618
01:49:31,780 --> 01:49:33,620
so what we will do

1619
01:49:33,670 --> 01:49:38,510
all the way through is derived everything so you can not only see how it

1620
01:49:38,510 --> 01:49:42,150
turns out why it has to be that way

1621
01:49:42,230 --> 01:49:47,800
and that's the way in my opinion one really wants this material

1622
01:49:47,850 --> 01:49:52,710
a couple of other very pedantic comments about the material

1623
01:49:52,730 --> 01:49:56,040
in the early part of the term in the first half in fact we're gonna

1624
01:49:56,040 --> 01:49:59,010
use plain old geometry

1625
01:49:59,710 --> 01:50:05,730
geometry really doesn't cut much mustard around the institute they if you can integrated or

1626
01:50:05,730 --> 01:50:11,480
take its fourier transform that's the mathematics you don't have to take seriously

1627
01:50:11,530 --> 01:50:17,040
geometry is a perfectly valid branch of mathematics and one can do what we're going

1628
01:50:17,040 --> 01:50:17,830
to do

1629
01:50:17,840 --> 01:50:22,490
in more complex terms using the language of group theory and we will in fact

1630
01:50:22,490 --> 01:50:27,330
use a little bit that later on but for the most part just diagrams with

1631
01:50:27,340 --> 01:50:32,070
simple geometry are they going to be one of the principal tools in the initial

1632
01:50:32,070 --> 01:50:34,090
part of the class

1633
01:50:34,100 --> 01:50:38,600
about halfway through will switch over to something that is much more mathematical in the

1634
01:50:38,600 --> 01:50:44,910
traditional sense here a little bit of linear algebra and matrix algebra will help you

1635
01:50:44,910 --> 01:50:48,810
if you have had better have a look at for a while will build up

1636
01:50:48,810 --> 01:50:52,950
from ground zero so that you will be able to fully understand it

1637
01:50:52,970 --> 01:50:57,620
will hit if you igon value problems towards the end of the term is that

1638
01:50:57,710 --> 01:51:02,680
it doesn't to get your adrenaline pumping that will be developed in the physical context

1639
01:51:02,680 --> 01:51:06,250
so that you're doing this sort of problem before you even know what it's called

1640
01:51:06,260 --> 01:51:09,880
so it's going to be user-friendly cause that doesn't rely on something that you may

1641
01:51:09,880 --> 01:51:12,720
have had two or three years ago

1642
01:51:14,170 --> 01:51:15,130
OK but

1643
01:51:15,140 --> 01:51:19,160
the other thing that i wanted to say was that this

1644
01:51:20,130 --> 01:51:23,890
it is not like many classes and that you talk about something for one week

1645
01:51:24,280 --> 01:51:28,780
and then you could decide to talk about something completely different the next week or

1646
01:51:28,780 --> 01:51:35,340
first half of the course will be one of long process of synthesis

1647
01:51:35,390 --> 01:51:41,610
we're going to start out very very simply with the mapping transformations this is picked

1648
01:51:41,650 --> 01:51:46,110
up and rotated over to here and you say all on let's get on with

1649
01:51:46,110 --> 01:51:49,430
it come and go faster but

1650
01:51:49,480 --> 01:51:53,930
will build on this and then build on what we've just done to what comes

1651
01:51:53,930 --> 01:51:58,220
next and unlike most of the classes in science that you take

1652
01:51:58,390 --> 01:52:06,220
where you start with general terms and you zero in on some little nugget like

1653
01:52:06,220 --> 01:52:08,960
f equals m a equals

1654
01:52:08,970 --> 01:52:11,100
mc squared

1655
01:52:11,150 --> 01:52:13,480
lambda equal to decide

1656
01:52:13,490 --> 01:52:17,630
a little nugget like the billion cubic can drop in your pocket and then when

1657
01:52:17,630 --> 01:52:20,770
you need it later on you pull it out and add

1658
01:52:20,790 --> 01:52:25,070
hot water and then you have the tool that you can use

1659
01:52:25,120 --> 01:52:27,470
we will do something that's completely

1660
01:52:27,480 --> 01:52:32,380
different and its structure will start out simple it will grow it will blossom like

1661
01:52:32,380 --> 01:52:38,720
an elegant villa agreed structure that gets more and more complicated and diverges rather than

1662
01:52:38,720 --> 01:52:41,720
converging to an nice title market

1663
01:52:41,730 --> 01:52:43,900
it's going to get very very complicated

1664
01:52:43,980 --> 01:52:48,070
and the reason for doing this gradually and thoroughly is so that you can understand

1665
01:52:48,070 --> 01:52:51,140
the complexity and we're comes from

1666
01:52:51,260 --> 01:52:55,490
OK so my moral here is keep up it may seem easy when you start

1667
01:52:55,490 --> 01:52:59,590
but we can assume that you've got that down cold before we go on to

1668
01:52:59,590 --> 01:53:02,300
the next step

1669
01:53:02,320 --> 01:53:06,370
OK texts

1670
01:53:06,380 --> 01:53:12,350
how apart from these have big treatments which i just keeps going on one of

1671
01:53:12,350 --> 01:53:15,040
apply as human see what's happening

1672
01:53:15,050 --> 01:53:19,170
and starting from that

1673
01:53:19,960 --> 01:53:25,350
we often read in the literature that has been was presented as the leading classification

1674
01:53:26,490 --> 01:53:30,330
however in some situations

1675
01:53:30,340 --> 01:53:35,160
in my experiment but also in other experiments in the literature we observed naive bayes

1676
01:53:35,170 --> 01:53:37,930
can also classifiers to

1677
01:53:37,950 --> 01:53:41,780
twenty outperform as you

1678
01:53:41,790 --> 01:53:44,100
so why in which settings

1679
01:53:44,110 --> 01:53:47,490
how can we avoid this situation of

1680
01:53:47,520 --> 01:53:48,890
if so

1681
01:53:48,910 --> 01:53:51,970
how can we take discovered is

1682
01:53:52,020 --> 01:53:56,870
to answer this question we must develop better understanding and that fits quite well in

1683
01:53:56,910 --> 01:54:00,200
in the work which was done initiated in

1684
01:54:00,210 --> 01:54:01,790
in both together

1685
01:54:01,830 --> 01:54:04,920
because courses are working with the learning so they want to get a lot of

1686
01:54:05,400 --> 01:54:06,630
of knowledge from

1687
01:54:06,650 --> 01:54:11,400
what working on and then to try to apply given his knowledge

1688
01:54:11,450 --> 01:54:14,400
try to play successfully some techniques

1689
01:54:14,410 --> 01:54:16,910
so that remains

1690
01:54:17,050 --> 01:54:20,810
in the initial target

1691
01:54:21,500 --> 01:54:24,340
a quick recall to frame a bit everything

1692
01:54:24,370 --> 01:54:29,160
introduction to text classification what kind of data set by using

1693
01:54:29,160 --> 01:54:32,160
what is after datasets publicly available

1694
01:54:32,160 --> 01:54:36,760
and austin all about fifty thousand medical abstracts

1695
01:54:36,780 --> 01:54:39,840
of cardiovascular disease

1696
01:54:41,210 --> 01:54:42,820
data set i'm using

1697
01:54:42,910 --> 01:54:49,320
twenty news groups about twenty thousand images of varying things

1698
01:54:49,330 --> 01:54:57,790
this spread into twenty categories twenty newsgroups writers i also using twenty newsgroup it was

1699
01:54:57,790 --> 01:55:00,460
initially does it actually

1700
01:55:00,480 --> 01:55:05,570
writers to couple plus all this this is the rate descent try to school this

1701
01:55:06,080 --> 01:55:07,390
published two years ago

1702
01:55:08,740 --> 01:55:10,150
data transformation

1703
01:55:10,160 --> 01:55:13,680
and you can perform stemming you can

1704
01:55:13,730 --> 01:55:19,020
compute and round TFIDF and actually i don't do anything on the dataset just use

1705
01:55:19,020 --> 01:55:21,290
it as it is

1706
01:55:21,310 --> 01:55:28,410
then binary classification task because we could consider multiclass categorisation

1707
01:55:30,460 --> 01:55:31,910
i won't discuss now

1708
01:55:31,910 --> 01:55:34,200
why but it is motivated

1709
01:55:34,840 --> 01:55:38,770
we can discuss it after discussion that interest you

1710
01:55:38,780 --> 01:55:42,070
i'm going to get a by using as sources where

1711
01:55:42,090 --> 01:55:46,900
used in the previous previous work and we especially focus

1712
01:55:47,540 --> 01:55:51,810
i mean i support vector machines

1713
01:55:51,860 --> 01:55:52,910
we can study

1714
01:55:52,910 --> 01:56:00,080
and the different viewpoints those classification techniques we can either reduce the number of columns

1715
01:56:00,080 --> 01:56:02,640
of your data metrics also number flies

1716
01:56:02,650 --> 01:56:07,860
so to reduce the number of columns you perform some feature selection construction

1717
01:56:07,910 --> 01:56:10,500
in our case today

1718
01:56:10,540 --> 01:56:12,370
it is not topic two

1719
01:56:12,460 --> 01:56:16,980
comprises techniques we just select information gain which was demonstrated at

1720
01:56:16,990 --> 01:56:19,000
good in the first one

1721
01:56:19,030 --> 01:56:21,730
but when performing

1722
01:56:23,250 --> 01:56:28,120
in terms of resampling

1723
01:56:28,820 --> 01:56:33,910
on the other hand we reduce the number of documents of the train set but

1724
01:56:33,910 --> 01:56:40,250
then we systematically calibrate so which means we have systematically as many positive and negative

1725
01:56:40,250 --> 01:56:42,250
documents the train set

1726
01:56:42,260 --> 01:56:43,510
like if i have

1727
01:56:43,540 --> 01:56:45,110
transit of sixty four

1728
01:56:45,120 --> 01:56:48,310
in the following figures then it means thirty two city two

1729
01:56:48,330 --> 01:56:53,450
and so on five hundred twelve and so on

1730
01:56:53,460 --> 01:57:01,020
for evaluating a user classical f one measure which is the harmonic mean between precision

1731
01:57:01,020 --> 01:57:02,750
and recall

1732
01:57:03,180 --> 01:57:09,050
i give formula to give you some idea and that's what's used use information retrieval

1733
01:57:09,050 --> 01:57:11,030
and text classification so it's not

1734
01:57:11,070 --> 01:57:12,790
fancy machine

1735
01:57:14,000 --> 01:57:15,260
what we also

1736
01:57:15,300 --> 01:57:19,580
because the train at this time but i don't speak about it today

1737
01:57:19,590 --> 01:57:26,660
given our focus which was really two to compare widely on a wide range of

1738
01:57:26,660 --> 01:57:28,260
experiments settings

1739
01:57:28,270 --> 01:57:29,760
we draw

1740
01:57:29,770 --> 01:57:35,670
many learning curves and then we try to see OK what's happening

1741
01:57:35,770 --> 01:57:37,530
so it was quite

1742
01:57:37,540 --> 01:57:44,610
heavy work because we had spent hundreds of classification task which we then

1743
01:57:44,620 --> 01:57:46,300
where we will

1744
01:57:46,320 --> 01:57:47,310
we major

1745
01:57:47,320 --> 01:57:48,440
o cos

1746
01:57:49,160 --> 01:57:51,400
a lot of work

1747
01:57:51,410 --> 01:57:57,080
and in the beautiful world what you would expect is to have really nicely look

1748
01:57:57,080 --> 01:58:10,390
she was

1749
01:58:28,590 --> 01:58:32,210
the wireless

1750
01:58:32,220 --> 01:58:35,430
i don't know whether this makes any difference OK

1751
01:58:35,560 --> 01:58:40,300
so the wireless network works again also the wiki

1752
01:58:40,470 --> 01:58:45,070
works again in case you don't know this this some kind of online collaboration tools

1753
01:58:45,490 --> 01:58:47,980
and in particular if you don't have the

1754
01:58:47,990 --> 01:58:51,460
slides of two myself months talk in the

1755
01:58:51,510 --> 01:58:54,800
i think he will hand the model he has and the amount then you can

1756
01:58:54,800 --> 01:58:56,190
find them on the wiki

1757
01:58:58,200 --> 01:58:59,340
so this is the

1758
01:58:59,360 --> 01:59:03,120
what can i was just pointing out that the IP number so there is no

1759
01:59:03,120 --> 01:59:06,750
name for it if you type this into your browser if you connected to the

1760
01:59:06,750 --> 01:59:10,410
wireless network you should be able to access it and you find a lot of

1761
01:59:10,490 --> 01:59:14,740
stuff on that you can find some pictures of people have taken during the summer

1762
01:59:17,170 --> 01:59:19,070
anything else

1763
01:59:19,080 --> 01:59:23,320
OK so i think i want to do more time from thomas and i look

1764
01:59:23,320 --> 01:59:25,100
forward to his tutorial

1765
01:59:25,120 --> 01:59:33,310
OK thank you that introduction

1766
01:59:34,230 --> 01:59:39,110
right actually i just noticed that there is no clock and i have no

1767
01:59:39,130 --> 01:59:43,250
wristwatch so maybe someone could give me a sign you know after forty minutes or

1768
01:59:43,250 --> 01:59:45,810
so i can you know five minutes to

1769
01:59:45,860 --> 01:59:47,130
one break in

1770
01:59:47,250 --> 01:59:48,790
OK so

1771
01:59:49,420 --> 01:59:54,730
i just arrived an and an overnight flight from the from these coast so i'm

1772
01:59:54,730 --> 02:00:01,470
not a hundred percent clear maybe it's because of that but i will talk about

1773
02:00:01,470 --> 02:00:03,970
machine learning information retrieval

1774
02:00:03,990 --> 02:00:08,400
and you know there will be some technical content but also try to provide some

1775
02:00:09,590 --> 02:00:14,940
you know what problems in information retrieval when machine learning can be useful and you

1776
02:00:14,940 --> 02:00:19,580
know that should give the motivation to learn more about machine learning but also talk

1777
02:00:19,580 --> 02:00:22,250
about specific machine learning methods here

1778
02:00:22,340 --> 02:00:25,540
OK so let me begin with part zero

1779
02:00:28,040 --> 02:00:29,600
you know

1780
02:00:29,610 --> 02:00:35,690
we can look back in the history of mankind you know and ask ourselves in

1781
02:00:35,700 --> 02:00:40,970
the issue of dealing with knowledge and text right and we would see that you

1782
02:00:40,970 --> 02:00:46,700
know the very early stage of the of the culture all civilizations

1783
02:00:46,720 --> 02:00:51,020
it was important basically to preserve the knowledge and the way it is typically happened

1784
02:00:51,020 --> 02:00:55,740
was in libraries right and so you know there are some famous ones on here

1785
02:00:55,840 --> 02:00:57,490
library of alexandria

1786
02:00:57,540 --> 02:01:01,590
and then in more modern times british museum and library of congress of course you

1787
02:01:01,590 --> 02:01:06,940
can see that the number of documents number whatever books or scrolls in the early

1788
02:01:06,940 --> 02:01:12,610
days you know increases dramatically in particular over the last couple of years now

1789
02:01:12,630 --> 02:01:21,890
this of course is mainly due to the fact the very recent increase in information

1790
02:01:21,890 --> 02:01:23,850
that space available in

1791
02:01:23,910 --> 02:01:26,740
the repositories due to

1792
02:01:26,750 --> 02:01:32,020
the fact that a lot of documents are now in digital format right and the

1793
02:01:32,240 --> 02:01:37,840
kind of the all-encompassing repository of course the world wide web which today has seen

1794
02:01:37,840 --> 02:01:44,210
several ten billion documents but you know and their digital libraries we have special purpose

1795
02:01:44,210 --> 02:01:48,310
content providers like lexisnexis to provide almost as much content as there is on the

1796
02:01:48,310 --> 02:01:54,780
web so there a huge content repositories you know companies have their own intranets and

1797
02:01:54,780 --> 02:02:01,220
what's called digital assets basically right documents in chile that that somehow encoded the knowledge

1798
02:02:02,020 --> 02:02:03,280
in a company

1799
02:02:03,290 --> 02:02:08,600
and we have scientific literature databases medical information portals and so on and so forth

1800
02:02:08,680 --> 02:02:12,650
so there's a lot of that stuff out there a lot of digital content

1801
02:02:15,310 --> 02:02:19,050
of course it's not enough to just store the information right so over the last

1802
02:02:19,050 --> 02:02:21,360
decade a lot was invested into

1803
02:02:21,370 --> 02:02:26,230
creating these content repositories and so now the content is available

1804
02:02:26,240 --> 02:02:31,430
but it doesn't mean that it's useful right just because it's it's there in some

1805
02:02:31,430 --> 02:02:37,180
say database or in the digital library so there is the question of

1806
02:02:37,200 --> 02:02:39,690
you know how can we actually support two

1807
02:02:39,750 --> 02:02:46,610
get access to the relevant information given these repositories and this is actually dates back

1808
02:02:46,670 --> 02:02:48,780
quite some time so here

1809
02:02:48,800 --> 02:02:52,980
actually in the early days of the computers vannevar bush

1810
02:02:54,600 --> 02:02:59,520
i thought about this machine which was which he called mimics which was is basically

1811
02:02:59,520 --> 02:03:04,560
like you know personal computers so basically the idea that you would have access to

1812
02:03:04,560 --> 02:03:07,620
all the information from your desk

1813
02:03:07,670 --> 02:03:12,080
right and and you know and he is kind of

1814
02:03:12,100 --> 02:03:15,160
you know a picture and there was also a prototype i think that was still

1815
02:03:15,170 --> 02:03:17,390
there was not never really successful

1816
02:03:17,400 --> 02:03:18,990
but the idea

1817
02:03:19,000 --> 02:03:24,660
you know of course now this is trivial he also set

1818
02:03:25,790 --> 02:03:28,290
you know this is quotation here

1819
02:03:28,300 --> 02:03:31,510
you know there's a problem because

1820
02:03:31,530 --> 02:03:35,560
you know the publication as he says has been extended far beyond our present ability

1821
02:03:35,560 --> 02:03:37,580
to make real use of the record

1822
02:03:37,620 --> 02:03:41,570
and you know that the techniques that we are using are still

1823
02:03:41,580 --> 02:03:46,160
you know from the days of square rigged ships so basically there is a mismatch

1824
02:03:47,000 --> 02:03:50,090
you know in in that time between

1825
02:03:50,100 --> 02:03:54,500
the all the publication of the contents that's out there

1826
02:03:54,510 --> 02:03:59,310
and the technology that's available to make use of that content right to efficiently access

1827
02:04:01,740 --> 02:04:05,590
you know what he called make use of the record and he calls the major

1828
02:04:05,590 --> 02:04:09,840
civilization a challenge to deal with that's actually after the second world war where he

1829
02:04:09,840 --> 02:04:14,010
was looking for new challenges are scientists right he thought that was one of the

1830
02:04:14,010 --> 02:04:20,700
main challenges people should look into so somewhat later in the sixties also pioneers in

1831
02:04:20,700 --> 02:04:30,150
information retrieval more includes define information retrieval known as adequately identifying the in asian content

1832
02:04:30,160 --> 02:04:33,700
of documentary data right so

1833
02:04:33,960 --> 02:04:37,990
whatever adequately means but you know we have to get some to the information content

1834
02:04:37,990 --> 02:04:40,720
rights not just enough to store and to preserve

1835
02:04:40,740 --> 02:04:45,760
all the documentary data now there are many facets that we see today of information

1836
02:04:45,760 --> 02:04:47,670
retrieval and i want give you

1837
02:04:47,680 --> 02:04:51,410
like a complete overview of information retrieval you know this is not the one one

1838
02:04:51,450 --> 02:04:55,490
one tory information retrieval but still let me mention you know most of these should

1839
02:04:56,100 --> 02:04:59,180
familiar to you so there's no query based search

1840
02:04:59,190 --> 02:05:04,510
we'll talk about this in the first part of categorising and annotating documents

1841
02:05:04,520 --> 02:05:10,830
organising and managing information repositories of things like assessing the quality

1842
02:05:10,840 --> 02:05:16,350
of information sources which is very important context of things like the web understanding users

1843
02:05:16,350 --> 02:05:17,790
information need

1844
02:05:17,840 --> 02:05:24,400
techniques like collaborative filtering recommender systems and then of course also this nowadays we also

1845
02:05:24,410 --> 02:05:30,270
things like you know multimedia retrieval distributed retrieval and many other aspects

1846
02:05:31,070 --> 02:05:34,710
so what i will talk about in this tutorial then is you know how can

1847
02:05:34,710 --> 02:05:36,360
machine learning helps

1848
02:05:36,380 --> 02:05:40,200
and the general idea is that

1849
02:05:40,220 --> 02:05:45,550
the information content so what moroni includes called you know the information content with you

1850
02:05:45,550 --> 02:05:49,070
know is what we want to get at is not directly observable right to give

1851
02:05:49,070 --> 02:05:52,540
you a text you know you don't know exactly what is the information content in

1852
02:05:52,580 --> 02:05:54,970
was the relevant information in there

