1
00:00:00,000 --> 00:00:06,130
distribution seems to the store for the distributed to the ship every this kind of

2
00:00:06,130 --> 00:00:07,730
distribution is

3
00:00:07,730 --> 00:00:11,620
but key distribution means you are not just you want to do so many things

4
00:00:11,900 --> 00:00:15,310
the whole track you may go down there so

5
00:00:15,330 --> 00:00:16,320
with this

6
00:00:16,340 --> 00:00:18,870
what about instead of thinking there

7
00:00:18,880 --> 00:00:23,550
one so the idea is if the ship at the same time what we

8
00:00:23,610 --> 00:00:28,200
but we think they are just one idea and that idea we

9
00:00:28,200 --> 00:00:32,940
actually say instead of thinking there are so many things we have to register

10
00:00:32,990 --> 00:00:40,280
we're thinking we invent one we called generalised i've been using this IP actually compact

11
00:00:40,280 --> 00:00:45,090
loss of our bodies inside because they move together so we get the human to

12
00:00:45,120 --> 00:00:46,520
generalize i

13
00:00:46,570 --> 00:00:51,120
then a reasoning you may have tensile record if they move together we just need

14
00:00:52,600 --> 00:00:57,330
that's why this generosity becomes quite useful

15
00:00:57,370 --> 00:01:00,530
then you also have to think in this way

16
00:01:00,530 --> 00:01:05,540
actually things and not moving in one way this keep splitting

17
00:01:05,550 --> 00:01:08,430
actually in many cases they

18
00:01:08,430 --> 00:01:13,950
keep merging and split in many different ways you think about this supposed in the

19
00:01:13,950 --> 00:01:20,010
real case you think about this one suppose is the producer somewhere suppose in china

20
00:01:20,200 --> 00:01:25,320
OK this one supposes big port like in shanghai so there are many many things

21
00:01:25,320 --> 00:01:31,200
produce here different factories produce different things the ship bound to this bigger pause the

22
00:01:31,200 --> 00:01:34,870
bigger pillars financing ship to the biggest seaport

23
00:01:35,110 --> 00:01:40,030
a lot of these different things packed into this ocean liner this alternative down to

24
00:01:40,030 --> 00:01:44,770
here suppose loss and is in this list and then they keep distributing to different

25
00:01:45,960 --> 00:01:49,990
OK you have march and then split

26
00:01:50,000 --> 00:01:52,620
OK you may merge and split in many ways

27
00:01:54,650 --> 00:01:56,290
for the RFID

28
00:01:56,300 --> 00:01:58,790
how could you compress

29
00:01:58,810 --> 00:02:05,600
OK what our claim is for example this ocean line you shipped millions or you

30
00:02:05,600 --> 00:02:11,370
can even say i should hundreds of thousands of kinds of items from here here

31
00:02:11,400 --> 00:02:12,890
how many ideas i need

32
00:02:13,010 --> 00:02:15,580
actually what we do

33
00:02:15,660 --> 00:02:20,730
her you is what you only need is what you just need one idea

34
00:02:20,730 --> 00:02:26,220
go to register here shipping this ocean liner from the show had los and you

35
00:02:26,220 --> 00:02:28,010
don't need that many things

36
00:02:28,020 --> 00:02:32,630
and we also carry you you don't lose any information

37
00:02:34,050 --> 00:02:37,050
another thing of course you can have

38
00:02:38,070 --> 00:02:44,710
generalization this generalisation we call is lossy compression and if you have a particular features

39
00:02:44,710 --> 00:02:47,130
because it is our shores OK

40
00:02:47,150 --> 00:02:52,720
this is called the children for women for men to find shirt sure just go

41
00:02:52,750 --> 00:02:56,290
to our our school to close it if you do this

42
00:02:56,290 --> 00:02:59,730
you still can do a lot of reasoning but this kind of thing do this

43
00:02:59,730 --> 00:03:06,200
reasoning the the interesting level of reasoning it is lossy compression basically you

44
00:03:06,220 --> 00:03:10,260
well this it our you don't know whether it's jacket is sure

45
00:03:10,280 --> 00:03:15,320
and another way to the lossy compression actually is

46
00:03:15,370 --> 00:03:18,430
compress in the pattern along the perhaps

47
00:03:18,470 --> 00:03:20,180
if you think about this case

48
00:03:20,220 --> 00:03:21,210
if people

49
00:03:21,230 --> 00:03:24,790
suppose manager or be only interest in this part

50
00:03:24,800 --> 00:03:27,890
into the storm entering only interest in this part

51
00:03:28,080 --> 00:03:33,930
you can compress all the transportation no matter from which distribution centre to which try

52
00:03:34,000 --> 00:03:37,440
and finally getting to store that part can be compressed

53
00:03:37,560 --> 00:03:43,980
but for transportation they can compare this part the can only watching this so of

54
00:03:43,980 --> 00:03:48,940
course this is lossy compression if you trace particular jagger of milk how they go

55
00:03:48,940 --> 00:03:52,090
wrong OK so there you have two

56
00:03:52,120 --> 00:03:54,080
the campus

57
00:03:55,450 --> 00:03:58,210
our model is this

58
00:03:58,240 --> 00:04:00,010
you get into

59
00:04:00,030 --> 00:04:04,720
the other part is similar to your typical data warehouse

60
00:04:04,730 --> 00:04:10,940
you have to those different products different information different manufacturers of these is very similar

61
00:04:10,940 --> 00:04:12,730
to the typical data warehouse

62
00:04:12,780 --> 00:04:15,690
but the interesting thing you can see is

63
00:04:15,740 --> 00:04:19,560
many things can be compressed as long as they

64
00:04:19,970 --> 00:04:25,250
stay on well together if they stay on all together they can be compressed into

65
00:04:25,250 --> 00:04:26,580
one GID

66
00:04:26,610 --> 00:04:29,230
the only thing is your GID

67
00:04:29,280 --> 00:04:31,140
may have hierarchies

68
00:04:31,160 --> 00:04:35,500
what used your your whole movement or state

69
00:04:35,500 --> 00:04:40,050
this one record if they share the same locations or one rec

70
00:04:40,070 --> 00:04:45,480
for example you sinking those ocean liner OK this ocean liners may store millions of

71
00:04:45,480 --> 00:04:50,180
things to find use one GID because the more exactly the same way

72
00:04:50,190 --> 00:04:52,250
OK then here

73
00:04:52,280 --> 00:04:55,240
you have hierarchy of your GI these

74
00:04:55,260 --> 00:04:59,550
OK with this hierarchy of jedi you do not lose any piece of information because

75
00:04:59,550 --> 00:05:04,680
this one jedi made many many such these of subject is finally getting to the

76
00:05:04,690 --> 00:05:05,710
are five

77
00:05:05,730 --> 00:05:12,470
so with this structure you actually can do most of data warehousing using much more

78
00:05:12,680 --> 00:05:15,700
compress they but you do not lose information

79
00:05:15,760 --> 00:05:17,780
so you can see

80
00:05:17,830 --> 00:05:23,460
what you need for the for your schema our data warehouse design the only thing

81
00:05:23,460 --> 00:05:24,710
is you can see

82
00:05:24,710 --> 00:05:29,080
this is a very new part because you haven't yet these factors

83
00:05:29,150 --> 00:05:33,250
otherwise all the others using his more or less like

84
00:05:33,250 --> 00:05:34,780
no not too much data

85
00:05:34,790 --> 00:05:36,910
you can play with it

86
00:05:37,050 --> 00:05:39,120
but how do you get this you

87
00:05:39,150 --> 00:05:44,780
OK are our was basically you scan data only once for your whole body scan

88
00:05:44,780 --> 00:05:45,780
data once

89
00:05:45,780 --> 00:05:51,050
you actually can construct all these map paper and you the state people

90
00:05:51,070 --> 00:05:52,690
in a very systematic way

91
00:05:52,820 --> 00:05:56,870
how to do it you can see this is your EPC called RFID

92
00:05:56,900 --> 00:06:01,260
this is a tiny and time out this is your location or something this is

93
00:06:02,240 --> 00:06:03,510
no information

94
00:06:03,530 --> 00:06:09,400
and with this information what can once again this you can see if they share

95
00:06:09,400 --> 00:06:14,610
the same location same time in time out this would become president one so that's

96
00:06:14,610 --> 00:06:18,270
the reason you actually can go so they want to get all the jedi sort

97
00:06:18,270 --> 00:06:18,960
it out

98
00:06:19,130 --> 00:06:23,500
so that's the part we can say is very fast you can get this one

99
00:06:24,280 --> 00:06:25,970
and with this

100
00:06:25,990 --> 00:06:32,530
banned use your GID comparing two EPC this size is far far smaller

101
00:06:32,540 --> 00:06:33,820
so the

102
00:06:33,880 --> 00:06:36,710
you're you're you're dataset you try to play

103
00:06:36,710 --> 00:06:38,790
it would be much smaller thing you can place

104
00:06:38,800 --> 00:06:43,400
so another thing is you came name

105
00:06:43,410 --> 00:06:49,250
your CID because the IP is under your control you actually can put a lot

106
00:06:49,250 --> 00:06:54,010
of information into the IDE for example you can you can put your past information

107
00:06:54,030 --> 00:06:59,270
tied they'll have to go back to track past you actually come in colonial past

108
00:06:59,270 --> 00:07:00,510
into your GID

109
00:07:00,550 --> 00:07:06,250
then a lot of information can do reasoning just grab idea and allow reasoning already

110
00:07:06,320 --> 00:07:09,990
so that's why it's very efficient and it's quite possible

111
00:07:11,340 --> 00:07:16,500
the essentially you can see that you is your apparent ninety plus you're unique IP

112
00:07:16,500 --> 00:07:20,880
you use the structure you can check all the next to structures and checks over

113
00:07:20,880 --> 00:07:22,960
the past as well

114
00:07:24,620 --> 00:07:29,090
then there are many other ways to generalize for example you can generalize based on

115
00:07:29,090 --> 00:07:33,840
the modular hierarchical structure and you can generalize with within the past

116
00:07:33,890 --> 00:07:36,040
and that's the

117
00:07:36,050 --> 00:07:39,000
paper what you need needed just information tables

118
00:07:39,010 --> 00:07:40,210
which register

119
00:07:40,230 --> 00:07:45,200
the jedi with different attributes like whether this is the shared with others for four

120
00:07:45,210 --> 00:07:48,950
woman with the size of a witch who is producing it

121
00:07:49,050 --> 00:07:54,290
the information is your information table but you have to stay table justice

122
00:07:54,300 --> 00:07:59,190
two new g end there is a hollow almost a all the book

123
00:07:59,220 --> 00:08:04,260
the map favoritism nested structure what you really need to just these three major papers

124
00:08:04,290 --> 00:08:08,900
you can construct a data warehouse to do the money in very systematic way

125
00:08:08,900 --> 00:08:10,850
but anyway

126
00:08:10,930 --> 00:08:12,680
but i'm going to do now is

127
00:08:12,710 --> 00:08:16,040
stop that one you could quite easily write it yourself

128
00:08:16,050 --> 00:08:17,640
i mean if they just

129
00:08:17,800 --> 00:08:21,800
using all the framework that we introduced in the previous section

130
00:08:21,820 --> 00:08:23,680
and now find

131
00:08:23,690 --> 00:08:26,920
i guess one of the key points departure from

132
00:08:26,930 --> 00:08:29,900
what you would find it useful to take care

133
00:08:29,920 --> 00:08:35,210
but there are people are usually referred to estimate the proper function

134
00:08:35,230 --> 00:08:40,850
there are interested in estimating p of its franchise on the

135
00:08:40,860 --> 00:08:45,380
and they want to make something

136
00:08:45,390 --> 00:08:49,190
that is fairly well established in local

137
00:08:49,230 --> 00:08:50,510
and the

138
00:08:50,550 --> 00:08:55,930
key difficulty in this context what compute the function g

139
00:08:55,940 --> 00:08:57,660
twelve songs

140
00:08:57,670 --> 00:08:59,190
fourteen collection

141
00:09:00,460 --> 00:09:03,870
all my

142
00:09:03,890 --> 00:09:06,150
all the

143
00:09:07,500 --> 00:09:10,280
with the

144
00:09:12,440 --> 00:09:17,400
specific some measure

145
00:09:17,650 --> 00:09:19,860
maybe some of the go

146
00:09:19,910 --> 00:09:24,400
o thing

147
00:09:27,510 --> 00:09:30,210
how much statistical physics book

148
00:09:31,540 --> 00:09:35,460
this function is referred to as they

149
00:09:35,470 --> 00:09:37,980
are actually love this

150
00:09:40,050 --> 00:09:44,890
there are one hundred one different ways how you would actually going to compute this

151
00:09:44,970 --> 00:09:47,090
useful things with it

152
00:09:47,100 --> 00:09:49,950
seems a little bit of what you can

153
00:09:49,960 --> 00:09:53,230
first of all trying to do with

154
00:09:53,250 --> 00:09:57,290
consider the light into so effectively

155
00:09:57,310 --> 00:10:00,150
i will take a slightly two

156
00:10:00,210 --> 00:10:03,680
this is well known well studied

157
00:10:05,010 --> 00:10:08,680
well we can actually also have the following thing we can try to estimate conditional

158
00:10:09,980 --> 00:10:12,170
if you want to get

159
00:10:12,180 --> 00:10:15,650
parameterized by theta

160
00:10:15,670 --> 00:10:18,610
well to write this

161
00:10:18,620 --> 00:10:21,650
in the same way as in

162
00:10:21,690 --> 00:10:25,970
politics and why would they want

163
00:10:25,990 --> 00:10:27,990
do you think now

164
00:10:31,460 --> 00:10:33,620
and where the

165
00:10:33,630 --> 00:10:36,690
do they

166
00:10:36,710 --> 00:10:41,240
given by the integral of my otherwise

167
00:10:45,680 --> 00:10:48,710
on why

168
00:11:02,810 --> 00:11:05,840
so that's the way we depart from

169
00:11:05,880 --> 00:11:09,210
standard statistics textbooks

170
00:11:09,250 --> 00:11:12,910
and what's the reason why you might want to do this

171
00:11:12,930 --> 00:11:16,250
well this domain can be really nasty to integrate over

172
00:11:16,250 --> 00:11:17,740
can be expensive

173
00:11:17,780 --> 00:11:19,960
he might not want to do this

174
00:11:19,970 --> 00:11:22,870
is that the main can be considerably smaller

175
00:11:22,880 --> 00:11:26,340
for for binary classification

176
00:11:26,370 --> 00:11:31,840
why will just take on values one minus one so apples and oranges say

177
00:11:31,870 --> 00:11:34,050
for regression while

178
00:11:34,060 --> 00:11:37,960
you might have why being on the real line

179
00:11:37,990 --> 00:11:43,840
however it could be that well this distribution he has a particularly nice form like

180
00:11:43,840 --> 00:11:46,780
it's normal distribution and so on

181
00:11:46,780 --> 00:11:47,650
so we get

182
00:11:47,680 --> 00:11:50,240
so all this in more detail as we go along

183
00:11:50,250 --> 00:11:53,170
i just want to give you a little bit of an idea of where this

184
00:11:53,170 --> 00:11:55,240
is going to go to

185
00:11:55,280 --> 00:11:59,020
so depending on how we set this up will recover a lot of

186
00:11:59,030 --> 00:12:09,030
what people typically recall supervised learning problems

187
00:12:09,030 --> 00:12:14,710
or sometimes people talk about as discriminative learning

188
00:12:14,720 --> 00:12:19,560
and i mean it's actually pretty simple reason why this might be a good idea

189
00:12:19,560 --> 00:12:21,490
so if you want to

190
00:12:21,490 --> 00:12:24,240
distinguish between cats and dogs i mean

191
00:12:24,250 --> 00:12:26,770
i imagine that each of us in the room will be able to to look

192
00:12:26,770 --> 00:12:28,310
at apart from the dog

193
00:12:28,370 --> 00:12:33,140
if i wanted to ask who of us can actually draw cats and dogs in

194
00:12:33,140 --> 00:12:37,320
such a way that they look like cats and dogs while i

195
00:12:37,350 --> 00:12:40,230
what at least that i don't know how to do it really well

196
00:12:40,260 --> 00:12:42,860
and for most of you and other

197
00:12:44,470 --> 00:12:48,100
building a generative model of cats and dogs

198
00:12:48,110 --> 00:12:49,790
it's really hard

199
00:12:50,130 --> 00:12:55,990
building a discriminative model of cats and dogs can actually be pretty easy

200
00:12:55,990 --> 00:13:00,620
for instance we don't need to know the entire distribution of those axis which might

201
00:13:00,620 --> 00:13:05,810
be pictures of cats and dogs but just look at some discriminative features of it

202
00:13:05,830 --> 00:13:10,200
for instance one discriminative feature will be the whiskers

203
00:13:11,470 --> 00:13:14,860
the beast has whiskers probably a cat otherwise it's all

204
00:13:14,920 --> 00:13:18,920
so of course just that by itself doesn't make a good model of a cat

205
00:13:19,160 --> 00:13:22,990
or dog but it's a pretty good discriminative one

206
00:13:23,040 --> 00:13:26,670
so you can already see that you may have to estimate may need to estimate

207
00:13:26,670 --> 00:13:28,790
only something considerably simpler

208
00:13:28,810 --> 00:13:33,670
and that you know why want out to do

209
00:13:34,970 --> 00:13:37,680
if the wise

210
00:13:39,390 --> 00:13:41,250
plus minus one

211
00:13:41,540 --> 00:13:44,550
and we get binary classification

212
00:13:44,560 --> 00:13:53,430
if the wise

213
00:13:53,490 --> 00:13:55,100
and the set from one

214
00:13:56,370 --> 00:14:02,110
we get multiclass

215
00:14:02,120 --> 00:14:04,140
if the wiser say

216
00:14:04,160 --> 00:14:06,200
in our

217
00:14:06,220 --> 00:14:07,660
the market regulation

218
00:14:07,740 --> 00:14:13,120
of some sort

219
00:14:13,140 --> 00:14:16,050
the wise for instance

220
00:14:16,060 --> 00:14:17,330
in theory

221
00:14:17,360 --> 00:14:19,870
we have have to measure one or

222
00:14:19,880 --> 00:14:24,270
why factorial the negative class or regression

223
00:14:28,850 --> 00:14:33,670
there's another thing that i could have namely that the y sample of structure

224
00:14:33,720 --> 00:14:37,880
this was what little lead us to graphical models

225
00:14:37,890 --> 00:14:40,600
that's actually quite the powerful connection

226
00:14:40,610 --> 00:14:42,140
good will allow us to

227
00:14:42,170 --> 00:14:45,350
find reasonably nonparametric

228
00:14:45,370 --> 00:14:48,990
representations of a graphical model quite easily

229
00:14:48,990 --> 00:14:51,600
so in this case the whites could be

230
00:14:51,610 --> 00:14:55,680
going from sequences of plus minus ones to the in

231
00:14:55,690 --> 00:14:58,770
but with the structure

232
00:14:58,820 --> 00:15:04,680
and i'll talk about dependence independence and all that a little bit later

233
00:15:04,690 --> 00:15:08,180
but that's a little bit the program of what we're going to look at

234
00:15:09,360 --> 00:15:12,490
and probably since my commitment is still stuck in a snowstorm

235
00:15:12,490 --> 00:15:15,750
also on wednesday

236
00:15:15,770 --> 00:15:20,620
OK so little depend on how bad the snowstorm in north america is how much

237
00:15:20,620 --> 00:15:23,580
you'll see if the grafted models

238
00:15:23,610 --> 00:15:26,450
OK good

239
00:15:26,450 --> 00:15:30,440
they get are

240
00:15:49,040 --> 00:15:55,470
all right my

241
00:16:30,440 --> 00:16:37,140
you can't

242
00:17:42,230 --> 00:18:00,190
what is about

243
00:18:11,600 --> 00:18:14,410
a lot

244
00:18:14,440 --> 00:18:24,170
o five

245
00:19:00,160 --> 00:19:08,780
how does one

246
00:19:08,800 --> 00:19:11,160
number here

247
00:19:11,160 --> 00:19:12,440
his last

248
00:19:12,450 --> 00:19:14,380
then what it would be

249
00:19:14,390 --> 00:19:20,750
if i let those two hydrogen atom wavefunctions destructively interfere

250
00:19:20,770 --> 00:19:22,260
and they have the key

251
00:19:22,270 --> 00:19:24,460
in this chemical plant

252
00:19:24,510 --> 00:19:28,380
is this right here in this term

253
00:19:28,460 --> 00:19:34,600
that makes all the difference this is our constructive interference causes the

254
00:19:34,610 --> 00:19:37,090
extra density

255
00:19:38,410 --> 00:19:43,030
because those two wavefunctions constructively interfere

256
00:19:43,220 --> 00:19:46,810
what is this pile up of what lecture

257
00:19:46,820 --> 00:19:50,030
electron density right in the centre

258
00:19:50,040 --> 00:19:53,560
it is the cool in this chemical bonds

259
00:19:53,580 --> 00:19:58,210
they make sense

260
00:19:58,950 --> 00:20:01,180
all right

261
00:20:04,660 --> 00:20:08,110
we are now a molecular weight function

262
00:20:08,130 --> 00:20:13,210
by the linear combination of atomic weight function

263
00:20:13,230 --> 00:20:17,500
now what happens to the energy when we do that

264
00:20:18,340 --> 00:20:22,880
i i mean what happens to the energy the energy goes down

265
00:20:22,900 --> 00:20:26,600
we're going to form a new molecular state

266
00:20:26,620 --> 00:20:29,590
let's draw an energy level diagram

267
00:20:29,610 --> 00:20:33,500
all right so energy going up here

268
00:20:33,520 --> 00:20:38,560
and let me represented here by these two lines energy levels

269
00:20:38,580 --> 00:20:42,030
these separated hydrogen atoms

270
00:20:42,040 --> 00:20:47,250
so i'm going to call this one as the enemy

271
00:20:47,260 --> 00:20:49,580
the one st and and the

272
00:20:49,690 --> 00:20:51,160
the the same energy

273
00:20:51,170 --> 00:20:53,810
i can tell them apart

274
00:20:53,960 --> 00:20:57,450
and each one of them has an electron

275
00:20:59,180 --> 00:21:00,400
once we

276
00:21:01,170 --> 00:21:05,380
those two wavefunctions to constructively interfere

277
00:21:05,390 --> 00:21:08,250
we're going to produce a new state

278
00:21:08,260 --> 00:21:10,650
we're going to call their state

279
00:21:10,670 --> 00:21:12,220
sigma one as

280
00:21:12,230 --> 00:21:16,190
and that state is going to be lower in energy here

281
00:21:16,240 --> 00:21:18,140
the and

282
00:21:18,150 --> 00:21:19,660
that's too

283
00:21:20,960 --> 00:21:23,940
from the individual hydrogen

284
00:21:23,950 --> 00:21:29,640
and that's good because it's lowering energy we got a hydrogen atom stuck together

285
00:21:29,650 --> 00:21:36,900
right we want chemical bonds two hydrogen atoms are why are want do

286
00:21:36,910 --> 00:21:38,570
thank you

287
00:21:38,620 --> 00:21:42,800
and therefore do two electrons here

288
00:21:42,840 --> 00:21:47,040
one from each hydrogen atom are sitting down the molecular state

289
00:21:47,060 --> 00:21:50,070
we are calling sigma one

290
00:21:50,210 --> 00:21:54,180
the weight function then that describes the electron mass

291
00:21:55,270 --> 00:21:59,680
sigma one molecular weight function

292
00:22:00,710 --> 00:22:01,990
that's right

293
00:22:02,040 --> 00:22:04,420
everything looks school

294
00:22:08,350 --> 00:22:09,880
these away

295
00:22:09,930 --> 00:22:11,950
so not only in ways

296
00:22:11,960 --> 00:22:14,110
destructively interfere

297
00:22:14,120 --> 00:22:17,430
they destructively interfere too

298
00:22:17,510 --> 00:22:20,400
right think here there is now

299
00:22:20,410 --> 00:22:24,640
the case of the structure of atoms

300
00:22:24,660 --> 00:22:27,560
well what's the

301
00:22:27,760 --> 00:22:35,410
right in this case

302
00:22:35,420 --> 00:22:37,160
what's going to happen well

303
00:22:38,070 --> 00:22:41,150
everyone has a weight function

304
00:22:41,170 --> 00:22:42,640
and i want

305
00:22:42,660 --> 00:22:45,200
the weight function

306
00:22:45,270 --> 00:22:46,580
and now

307
00:22:46,590 --> 00:22:48,950
what i'm going to bring these two together

308
00:22:48,960 --> 00:22:50,840
the equilibrium

309
00:22:50,890 --> 00:22:52,600
but i'm going to be

310
00:22:53,810 --> 00:22:59,610
what's going to happen is in destructive interference

311
00:22:59,620 --> 00:23:01,250
this weight function

312
00:23:01,260 --> 00:23:05,340
it's going to cancel this part of the wave function

313
00:23:06,170 --> 00:23:07,820
when these overlap

314
00:23:07,840 --> 00:23:10,310
and i take the minus sign here

315
00:23:10,330 --> 00:23:15,430
or i'm superimposing two wave functions with different sign

316
00:23:15,450 --> 00:23:19,390
it had destructive interference and the result is

317
00:23:19,510 --> 00:23:22,570
at the molecular weight function

318
00:23:22,590 --> 00:23:24,670
that kind of look like

319
00:23:24,680 --> 00:23:26,870
like this

320
00:23:26,880 --> 00:23:30,950
right in the centre here hey there's going to be you know

321
00:23:30,970 --> 00:23:31,760
there's no old

322
00:23:32,850 --> 00:23:37,690
this part of the weight function exactly cancel this part of the wave function they

323
00:23:37,790 --> 00:23:41,260
there's no way function right here

324
00:23:41,340 --> 00:23:45,560
right we have destructive interference

325
00:23:45,600 --> 00:23:48,580
powergen label this function

326
00:23:48,590 --> 00:23:53,160
well we're all going to call it a signal wave function and sigma that symmetry

327
00:23:54,680 --> 00:23:58,700
cylindrically symmetric around the bond axis

328
00:23:59,580 --> 00:24:02,580
but because

329
00:24:02,630 --> 00:24:04,310
it is a

330
00:24:04,320 --> 00:24:06,200
linear combination

331
00:24:09,430 --> 00:24:15,970
is a consequence of these structures here i'm going to call this big star

332
00:24:15,990 --> 00:24:18,740
i think most are one and

333
00:24:18,790 --> 00:24:25,660
and i'm going to call that sigma star what has happened and tight bonding orbital

334
00:24:25,680 --> 00:24:28,360
four antibonding

335
00:24:33,350 --> 00:24:35,490
how do i know there well

336
00:24:37,470 --> 00:24:41,230
let's also calculate the probability density

337
00:24:41,250 --> 00:24:47,630
here for sigma one at a time you that's going to be to be interesting

338
00:24:47,680 --> 00:24:53,120
so we got the probability density sigma one garden

339
00:24:54,630 --> 00:24:56,250
OK so i am

340
00:24:56,260 --> 00:25:00,860
now one as a minus one as

341
00:25:00,950 --> 00:25:03,940
times one as a minus

342
00:25:03,980 --> 00:25:05,650
one as

343
00:25:05,650 --> 00:25:08,210
multiply that out

344
00:25:08,240 --> 00:25:11,000
so i get one as a

345
00:25:11,060 --> 00:25:14,040
one these square plus one SB

346
00:25:14,120 --> 00:25:18,950
twenty square minus two times one

347
00:25:18,990 --> 00:25:22,000
times one as he he

348
00:25:22,050 --> 00:25:23,930
now going by

349
00:25:23,940 --> 00:25:26,950
in my career here think

350
00:25:26,970 --> 00:25:30,130
this is what i wanted write on my

351
00:25:30,150 --> 00:25:33,240
graph of the probability density

352
00:25:33,260 --> 00:25:35,730
when i do that well OK

353
00:25:35,730 --> 00:25:37,500
way out here

354
00:25:37,530 --> 00:25:41,980
right on one end of the molecule probability is pretty low

355
00:25:41,980 --> 00:25:46,470
probability density goes to maximum right at the nucleus

356
00:25:46,490 --> 00:25:52,430
and then sure deployment right and right at the centre here

357
00:25:52,430 --> 00:25:57,430
right at the centre here the probability density is zero because the function zero

358
00:25:57,480 --> 00:26:01,040
because we had destructive interference

359
00:26:01,050 --> 00:26:04,990
OK i want you to notice something

360
00:26:05,050 --> 00:26:08,690
probability density here is zero

361
00:26:08,700 --> 00:26:10,740
it is even lower

362
00:26:10,750 --> 00:26:12,570
then in this case

363
00:26:12,580 --> 00:26:13,930
in this case

364
00:26:13,970 --> 00:26:14,930
which is just

365
00:26:14,930 --> 00:26:20,350
on new data so this is what we would think of as a generalization

366
00:26:22,230 --> 00:26:26,250
so the first point here is just that example i gave

367
00:26:26,270 --> 00:26:30,710
if it was classifications of this loss function is one of the two outputs disagree

368
00:26:30,710 --> 00:26:32,630
and zero otherwise

369
00:26:32,670 --> 00:26:36,210
well for regression it could be the square of the difference between

370
00:26:36,220 --> 00:26:39,470
the application of the function to the input and the true

371
00:26:40,410 --> 00:26:44,050
you know if it's regression we can't say

372
00:26:44,080 --> 00:26:47,220
you know a lot of one event article

373
00:26:47,230 --> 00:26:50,830
because they're always going to be some error but is no need to measure the

374
00:26:50,830 --> 00:26:52,670
difference between the two

375
00:26:52,720 --> 00:26:53,560
so this

376
00:26:53,610 --> 00:26:54,910
random variable

377
00:26:54,940 --> 00:27:01,190
defined appropriately for whatever task we're trying to solve is a measure of generalisation of

378
00:27:01,190 --> 00:27:02,080
the learner

379
00:27:02,200 --> 00:27:06,480
and that's what we'd like to understand and control

380
00:27:06,490 --> 00:27:08,640
well i'm just going to go into

381
00:27:08,660 --> 00:27:12,410
this talk is trying to give you a sort of feel for these things and

382
00:27:12,800 --> 00:27:15,790
you know intuition behind what's going on

383
00:27:16,070 --> 00:27:19,720
so i'm not going to go into details of the kind of analysis at all

384
00:27:19,720 --> 00:27:25,220
but in the case of classification the critical method for doing controlling generalisation

385
00:27:25,230 --> 00:27:29,750
is to force a large margin on the training data so here's your positive the

386
00:27:29,750 --> 00:27:34,690
negative margin is if you want to hear somehow got lost in the transcription sure

387
00:27:34,690 --> 00:27:35,920
they went to but anyway

388
00:27:37,000 --> 00:27:40,210
when you pick a line that separates these two

389
00:27:40,210 --> 00:27:44,240
there are many lines you could pick up line like this like this

390
00:27:44,290 --> 00:27:46,160
now the large margin

391
00:27:46,200 --> 00:27:50,780
criterion says pick the line that has the furthest distance

392
00:27:50,800 --> 00:27:56,930
between the line and the nearest points on either side so here's the two negative

393
00:27:56,930 --> 00:28:00,880
points and here's the positive point nearest to the hyperplane

394
00:28:01,820 --> 00:28:06,990
this hyperplanes been chosen so that this these distances are maximal

395
00:28:07,820 --> 00:28:11,950
so that's the sort of method know why

396
00:28:11,950 --> 00:28:14,160
why is that going to work well

397
00:28:14,170 --> 00:28:16,670
these are intuitive explanations that you can

398
00:28:16,710 --> 00:28:20,840
and they're all different but they all say the same thing so you could say

399
00:28:20,840 --> 00:28:21,440
that the

400
00:28:22,020 --> 00:28:28,100
in fact we kept the hyperplane away from the examples makes classification robust to uncertainties

401
00:28:28,100 --> 00:28:32,470
in the inputs you mentioned we call the inputs of some sort of distribution of

402
00:28:32,470 --> 00:28:36,270
things that might happen around those points they're still going to be correctly classified because

403
00:28:36,270 --> 00:28:38,560
this nice but

404
00:28:38,580 --> 00:28:40,340
so that's sort of intuitive

405
00:28:40,350 --> 00:28:44,940
o thing you can do is another way of thinking about it which is two

406
00:28:44,950 --> 00:28:51,540
randomly project into low dimensional spaces and if you do random projections things don't actually

407
00:28:51,540 --> 00:28:54,390
change their inner products that much

408
00:28:54,490 --> 00:28:59,410
so if you've got this nice margin even after you've done this random projection they're

409
00:28:59,440 --> 00:29:00,530
still going to be

410
00:29:00,550 --> 00:29:02,740
with high probability but separation

411
00:29:03,990 --> 00:29:08,210
you you've effectively got separation in the low dimensional space

412
00:29:08,230 --> 00:29:10,000
is random projection space

413
00:29:10,020 --> 00:29:11,250
and so

414
00:29:11,260 --> 00:29:15,270
we know that a low dimensional space there is low capacity by the previous theorem

415
00:29:15,270 --> 00:29:19,730
that i mentioned so you know although you high dimensional space the fact that you've

416
00:29:19,730 --> 00:29:21,930
got a large margin means you actually

417
00:29:21,940 --> 00:29:24,540
effectively work in a low dimensional space

418
00:29:24,600 --> 00:29:29,490
and again you can apply rigorous statistical analysis shows effective dimension

419
00:29:29,500 --> 00:29:32,750
it is one of the gamma squared when gamma

420
00:29:32,750 --> 00:29:35,140
is this margin quantity

421
00:29:35,160 --> 00:29:40,460
so you know here is exactly what i was alluding to before when i said

422
00:29:40,460 --> 00:29:42,430
you know you want this neither

423
00:29:42,450 --> 00:29:43,800
but you can

424
00:29:43,830 --> 00:29:46,590
control your capacity with

425
00:29:46,680 --> 00:29:50,900
OK the lever is the marginal OK so what we're saying is if we can

426
00:29:50,900 --> 00:29:52,740
keep the margin big

427
00:29:52,930 --> 00:29:55,590
they're effectively controlling

428
00:29:55,610 --> 00:29:56,670
the capacity

429
00:29:56,670 --> 00:29:59,390
although we working on this very high dimensional space

430
00:29:59,400 --> 00:30:02,910
capacity potentially at our disposal was very high

431
00:30:02,920 --> 00:30:08,220
like sort of cranking up the sleeve we're keeping it under control

432
00:30:08,840 --> 00:30:10,910
this is you know sort of

433
00:30:10,950 --> 00:30:16,140
a different way of controlling capacity than just specifying the space

434
00:30:16,140 --> 00:30:26,280
we see entropy

435
00:30:26,410 --> 00:30:30,570
OK so

436
00:30:30,580 --> 00:30:33,260
so if we had an example in the function

437
00:30:33,270 --> 00:30:37,620
you can work out what's the loss function on the examples of the same quantity

438
00:30:37,620 --> 00:30:40,630
that we used to call that we also called XII

439
00:30:40,640 --> 00:30:44,600
the other cases this quantity can be zero one

440
00:30:44,620 --> 00:30:47,080
likewise if you have a whole sample of points

441
00:30:47,090 --> 00:30:51,410
you get a whole set of of loss vectors which are zero one vectors of

442
00:30:52,180 --> 00:30:58,750
the corners of the hypercube m dimensional hypercube

443
00:31:01,580 --> 00:31:04,100
a fixed set of functions one

444
00:31:04,130 --> 00:31:06,790
a set of training points

445
00:31:07,330 --> 00:31:09,070
we get some numbers

446
00:31:09,080 --> 00:31:12,330
of such output vectors

447
00:31:12,340 --> 00:31:18,590
the VC entropy is then defined as the expectation of the logarithm of the twenty

448
00:31:18,630 --> 00:31:20,650
news expectation taking over

449
00:31:20,680 --> 00:31:22,120
drawing many

450
00:31:26,220 --> 00:31:32,560
training sets so you can for typical training set how many possible predictors can generate

451
00:31:32,660 --> 00:31:34,020
how many possible

452
00:31:34,020 --> 00:31:39,960
the logarithm of the vectors cannot generate so over the number of was not vectors

453
00:31:39,970 --> 00:31:42,400
this quantity is called the VC entropy

454
00:31:42,430 --> 00:31:43,700
and it turns out

455
00:31:45,390 --> 00:31:46,260
one can

456
00:31:46,270 --> 00:31:50,130
so i said before the uniform convergence of risk is what we are interested in

457
00:31:50,130 --> 00:31:54,700
terms of the uniform convergence of risk takes place if and only if

458
00:31:54,720 --> 00:31:56,190
the VC entropy

459
00:31:56,200 --> 00:32:00,040
some the states sublinearly with number of

460
00:32:00,090 --> 00:32:03,890
training observations

461
00:32:03,900 --> 00:32:07,500
so this is this is something that depends on the underlying distribution

462
00:32:07,510 --> 00:32:11,770
we have this expectation of the distribution here which is nice takes into account the

463
00:32:11,770 --> 00:32:15,010
problem but is that since we don't know the distribution

464
00:32:15,650 --> 00:32:19,600
we want to we don't know the distribution and that's why i want to be

465
00:32:19,600 --> 00:32:25,070
keywords with is dimension which is possible for all which is south of on possible

466
00:32:25,070 --> 00:32:30,520
distributions in this sense worst case theory

467
00:32:30,530 --> 00:32:33,080
but this was case because we don't know beforehand

468
00:32:33,090 --> 00:32:37,960
we can see because could imagine things in between

469
00:32:38,810 --> 00:32:39,820
any more

470
00:32:44,960 --> 00:32:57,180
this is a

471
00:32:57,190 --> 00:33:01,980
this is

472
00:33:08,940 --> 00:33:13,090
this is

473
00:33:13,100 --> 00:33:17,990
so the question is to assume that the data distribution is the same training and

474
00:33:17,990 --> 00:33:21,530
test sets the answer is emphatically yes

475
00:33:22,280 --> 00:33:25,920
if the data distribution is different between training and test set

476
00:33:26,080 --> 00:33:28,610
things can go up the the tree right

477
00:33:28,620 --> 00:33:33,060
so now it's interesting question to think of the interesting cases in between

478
00:33:33,710 --> 00:33:34,870
four in a

479
00:33:34,910 --> 00:33:37,730
what if they did this because i'm not saying but they have something in common

480
00:33:37,730 --> 00:33:41,230
from the point is what the

481
00:33:41,250 --> 00:33:42,830
distribution over x

482
00:33:42,840 --> 00:33:48,540
is different but the distribution of y given x the conditional distribution is the same

483
00:33:48,560 --> 00:33:51,840
one would be tempted to think

484
00:33:51,860 --> 00:33:53,810
in this case

485
00:33:53,880 --> 00:33:58,450
we should still be fine but actually it's it's more complicated even in this case

486
00:33:58,460 --> 00:34:02,420
a lot of things can go wrong so let's call could very cheap so it

487
00:34:02,740 --> 00:34:06,330
so the answer if there's a general change have some p of x and y

488
00:34:06,330 --> 00:34:10,340
in the twenty k some of p of x and y in test case can

489
00:34:10,340 --> 00:34:14,880
go up the tree wrong but it's interesting problem to think about

490
00:34:14,900 --> 00:34:19,830
how much change in the law and can we still say something and this very

491
00:34:19,830 --> 00:34:22,660
little research that almost none so

492
00:34:22,670 --> 00:34:25,560
it's interesting topic

493
00:34:25,620 --> 00:34:28,780
and so there are two questions here i don't know was first

494
00:34:47,450 --> 00:34:56,910
so actually it's a little bit more complicated i was glossing over some details and

495
00:34:56,910 --> 00:34:58,040
it's even

496
00:34:58,050 --> 00:35:05,380
so it took if you want to get the equivalence between uniform convergence and consistency

497
00:35:05,380 --> 00:35:11,030
so if you want uniform convergence as the replacement of consistency then you have to

498
00:35:11,030 --> 00:35:12,280
use some

499
00:35:12,290 --> 00:35:18,810
slightly strange notion of consistency with the dominican school nontrivial consistency in which

500
00:35:18,850 --> 00:35:20,060
it took them

501
00:35:20,080 --> 00:35:24,410
twenty years to come up with this is this part of the learning theories was

502
00:35:24,410 --> 00:35:27,000
only published in the late eighties

503
00:35:27,830 --> 00:35:33,540
efficiency should say maybe not not everybody is happy with this definition of nontrivial consistency

504
00:35:33,550 --> 00:35:36,510
you could you could even say sensitive it's definition which is

505
00:35:36,570 --> 00:35:41,480
made such that these two quantities of these two things become equivalent

506
00:35:41,500 --> 00:35:43,900
it's something like that

507
00:35:43,910 --> 00:35:45,990
you have to remove

508
00:35:46,010 --> 00:35:49,440
those functions from the class

509
00:35:49,740 --> 00:35:54,180
then you have some from the class which are uniformly better than all the rest

510
00:35:54,200 --> 00:35:59,330
then this can screw up so if you had one function which is so clearly

511
00:35:59,330 --> 00:36:03,010
better than everything else that you can already be identified from the first training point

512
00:36:03,010 --> 00:36:03,940
that you see

513
00:36:03,960 --> 00:36:06,940
then becomes

514
00:36:06,960 --> 00:36:11,860
this is a second get this equivalence anymore so is this a little bit difficult

515
00:36:11,870 --> 00:36:14,750
to answer but i just want to point out

516
00:36:15,580 --> 00:36:18,760
it's interesting to to look into the same thing to look at this notion of

517
00:36:18,760 --> 00:36:24,200
nontrivial consistency which requires that even after removing the best functions

518
00:36:24,210 --> 00:36:29,350
this fraction of functions in this has to work both fractions you still want to

519
00:36:29,350 --> 00:36:35,400
be consistent afterwards letters from the condition than the usual consistency

520
00:36:36,670 --> 00:36:38,000
over here

521
00:36:42,300 --> 00:36:50,530
OK so the question is why we're symmetrisation

522
00:36:50,550 --> 00:36:53,940
important in the proof so the symmetrisation

523
00:36:54,070 --> 00:36:59,960
i was replacing

524
00:37:00,650 --> 00:37:03,130
symmetrisation was replacing

525
00:37:03,730 --> 00:37:05,130
this difference

526
00:37:06,000 --> 00:37:09,920
this one or upper bounding in terms of something like this one

527
00:37:09,940 --> 00:37:13,130
the nice thing about the right hand side is

528
00:37:13,150 --> 00:37:17,700
here we have in here so this refers to m points in training points you

529
00:37:17,700 --> 00:37:22,830
want this is another set of training points of function class

530
00:37:22,840 --> 00:37:26,730
o functions take values outputs plus or minus one

531
00:37:26,740 --> 00:37:29,620
so if we only have twelve points

532
00:37:29,630 --> 00:37:34,340
and the outputs of plus or minus one there only two to the two m

533
00:37:34,370 --> 00:37:38,010
possible output vector is that the functions can generate

534
00:37:38,060 --> 00:37:40,960
so even if we have infinitely many functions

535
00:37:40,960 --> 00:37:41,960
it's enough

536
00:37:41,970 --> 00:37:43,210
if we instead

537
00:37:43,230 --> 00:37:49,340
consider sets of functions that have size at most two to the pol two

538
00:37:50,050 --> 00:37:52,470
everything that happens way

539
00:37:53,680 --> 00:37:56,380
these two sets of points that

540
00:37:56,400 --> 00:38:01,010
going to hear it doesn't matter for us so it's kind of factorisation of an

541
00:38:01,010 --> 00:38:03,100
infinite class of functions

542
00:38:03,110 --> 00:38:05,050
into equivalence classes

543
00:38:05,060 --> 00:38:09,800
and only two to the path to an equivalence classes and it's a good question

544
00:38:09,800 --> 00:38:14,450
because in the sense that the the central idea of all of learning theory

545
00:38:14,450 --> 00:38:17,520
i want to do is use that data to tell

546
00:38:19,990 --> 00:38:23,390
weighted sum of basis functions or

547
00:38:23,410 --> 00:38:25,130
other estimator

548
00:38:25,190 --> 00:38:27,200
how to behave

549
00:38:27,300 --> 00:38:28,780
in between

550
00:38:28,840 --> 00:38:30,270
sample points

551
00:38:30,350 --> 00:38:35,020
we do this through a method which is generally known as regularisation

552
00:38:35,050 --> 00:38:38,740
statisticians tend to refer to as penalisation

553
00:38:38,850 --> 00:38:42,240
OK but they both mean the same thing

554
00:38:42,880 --> 00:38:49,260
what we do is place the penalty cost on the roughness of use the

555
00:38:49,260 --> 00:38:51,910
quotation marks because

556
00:38:51,950 --> 00:38:54,920
the number ways of

557
00:38:55,760 --> 00:38:58,060
defining what we mean by

558
00:38:59,630 --> 00:39:03,750
so if we place a penalty on that typically will just simply add it

559
00:39:04,810 --> 00:39:09,380
the cost function that we're trying to work with in the first place

560
00:39:09,420 --> 00:39:13,230
as an example if we look at some squares are so we're looking doing the

561
00:39:13,550 --> 00:39:16,010
least squares fit

562
00:39:17,280 --> 00:39:22,360
the first term there just the the ordinary selves squares and we seek to minimize

563
00:39:22,360 --> 00:39:24,280
by choosing a set of weights

564
00:39:25,330 --> 00:39:26,780
by some method

565
00:39:26,800 --> 00:39:31,360
and we have to that rose just some

566
00:39:31,380 --> 00:39:33,340
nonnegative constant

567
00:39:33,350 --> 00:39:35,010
times q

568
00:39:35,060 --> 00:39:36,840
o where q is a

569
00:39:36,930 --> 00:39:41,780
it's something stands in for how rough the function is

570
00:39:42,980 --> 00:39:47,620
and then instead of minimizing just the mean squared error of the sum of squares

571
00:39:47,620 --> 00:39:51,700
error we minimize that entire thing

572
00:39:51,750 --> 00:39:53,700
and that gives a bit of control

573
00:39:54,520 --> 00:39:56,720
what about when q is for the

574
00:39:56,770 --> 00:39:59,270
because rho than if it is zero

575
00:39:59,450 --> 00:40:03,180
basically brings us back to original problem and says OK just minimize the sum of

576
00:40:03,180 --> 00:40:07,310
squares can be the basically square solution

577
00:40:07,360 --> 00:40:08,510
if i make row

578
00:40:08,520 --> 00:40:10,570
very large it says

579
00:40:10,590 --> 00:40:11,280
give me

580
00:40:11,290 --> 00:40:13,540
very very smooth

581
00:40:13,550 --> 00:40:15,780
fit to the data

582
00:40:16,500 --> 00:40:22,500
so basically it you're you're making the smoothness of the problem that is

583
00:40:22,960 --> 00:40:26,110
try to minimize the roughness

584
00:40:26,530 --> 00:40:30,080
the major component of your optimisation

585
00:40:30,190 --> 00:40:32,110
as of course

586
00:40:32,170 --> 00:40:35,450
again a good engineer what i'm looking for is the trade-off

587
00:40:35,460 --> 00:40:36,890
which tells me

588
00:40:37,010 --> 00:40:40,330
i get an acceptable level of fit to the data

589
00:40:40,350 --> 00:40:42,490
with the smoothest

590
00:40:43,890 --> 00:40:47,020
a function that i can get away with

591
00:40:47,070 --> 00:40:51,690
so the strategy i take a very flexible

592
00:40:51,690 --> 00:40:53,740
structure a lot of

593
00:40:53,780 --> 00:40:56,180
radial basis functions with

594
00:40:56,230 --> 00:40:57,970
quite narrow

595
00:40:58,580 --> 00:41:02,270
which is for instance

596
00:41:02,350 --> 00:41:06,850
nasty rough thing that is capable of doing all sorts of weird things

597
00:41:06,860 --> 00:41:07,930
and then

598
00:41:08,980 --> 00:41:12,100
rho here to choose

599
00:41:13,420 --> 00:41:15,980
how flexible i let the final

600
00:41:18,490 --> 00:41:24,780
so what we want is something that is very

601
00:41:24,790 --> 00:41:26,350
simple here

602
00:41:26,370 --> 00:41:27,430
and nice

603
00:41:27,430 --> 00:41:28,850
to deal with

604
00:41:28,880 --> 00:41:36,020
a good measure of roughness or something very strongly associated with roughness of the

605
00:41:36,020 --> 00:41:40,360
one of the function is it's second derivative with respect to the

606
00:41:40,410 --> 00:41:42,840
so the argument to the x's

607
00:41:45,720 --> 00:41:47,210
because that's the curvature

608
00:41:47,220 --> 00:41:49,800
the functions areas of high curvature

609
00:41:49,860 --> 00:41:53,020
are areas which are doing this kind of

610
00:41:55,450 --> 00:41:59,080
that's a nice thing to use and you can use it formally it's what ultimately

611
00:41:59,080 --> 00:42:00,470
leads to

612
00:42:00,490 --> 00:42:02,140
pspline fits

613
00:42:02,280 --> 00:42:04,070
OK that's where it all comes from

614
00:42:04,120 --> 00:42:06,210
the optimal choice

615
00:42:06,210 --> 00:42:07,670
to minimize

616
00:42:07,720 --> 00:42:09,780
sum of squares error and

617
00:42:12,020 --> 00:42:13,710
a measure of curvature

618
00:42:13,710 --> 00:42:15,520
penalize curvature

619
00:42:15,580 --> 00:42:18,780
they just to explain it but i'm going to talk about that

620
00:42:18,870 --> 00:42:23,180
what happens then is you get rather complex

621
00:42:24,770 --> 00:42:26,960
the solution to the optimisation problem

622
00:42:26,970 --> 00:42:32,960
there are number of other possibilities but one choice which is very very nice and

623
00:42:32,960 --> 00:42:35,520
leads to a very very simple solution

624
00:42:35,580 --> 00:42:38,810
is this one we like you just equal to the sum of the squares of

625
00:42:38,810 --> 00:42:40,290
all the weights

626
00:42:40,340 --> 00:42:42,600
in our estimator

627
00:42:43,320 --> 00:42:45,170
i've used the double

628
00:42:46,120 --> 00:42:48,180
index because

629
00:42:48,180 --> 00:42:53,210
sometimes we have multiple layers of things to come after that

630
00:42:53,260 --> 00:42:57,970
anyway just think about the sum of the squares of all the weights in the

631
00:42:58,130 --> 00:43:01,710
the estimator we

632
00:43:01,780 --> 00:43:06,280
do that it turns out in this linear in the parameters case

633
00:43:06,320 --> 00:43:11,060
OK that we just have very simple modifications to

634
00:43:12,170 --> 00:43:13,960
OK we have w hat

635
00:43:13,970 --> 00:43:18,380
now the estimated set the weight is equal to phi transpose phi

636
00:43:18,440 --> 00:43:23,520
plus times the identity matrix was phi transposed times that

637
00:43:23,530 --> 00:43:25,410
pretty much the same as we had before

638
00:43:25,430 --> 00:43:28,500
and when rho is equal to zero of course we get

639
00:43:28,560 --> 00:43:32,240
the same answer i should say inverse there sorry

640
00:43:32,750 --> 00:43:36,640
sorry is an inverse missing

641
00:43:36,640 --> 00:43:41,360
so what we are doing what is the advantage the the advantage is that we

642
00:43:41,360 --> 00:43:44,870
are reducing the number of the compositions

643
00:43:44,880 --> 00:43:48,440
and number of times when become aware of

644
00:43:49,200 --> 00:43:53,700
ideas ideas of documents

645
00:43:53,760 --> 00:43:56,180
OK so we can use this list

646
00:43:56,200 --> 00:43:58,200
so we

647
00:43:58,260 --> 00:44:02,000
we discussed in this construction yesterday about

648
00:44:03,520 --> 00:44:05,260
this structure

649
00:44:05,300 --> 00:44:07,020
seems to be

650
00:44:07,030 --> 00:44:10,530
october so how we can be because

651
00:44:10,540 --> 00:44:12,980
there are some pointers and it's not clear

652
00:44:12,990 --> 00:44:18,930
when we talked about compression everything seems very straightforward we are taking one number next

653
00:44:18,930 --> 00:44:21,080
number in it and

654
00:44:21,110 --> 00:44:26,970
in the end encoding sigma delta between this numbers repeating this separation is

655
00:44:27,010 --> 00:44:29,940
a more complex structure

656
00:44:29,970 --> 00:44:30,900
so the same

657
00:44:32,050 --> 00:44:36,660
what do you have put into some key way the temporary very well and we

658
00:44:36,660 --> 00:44:39,530
need to keep in memory so her

659
00:44:39,530 --> 00:44:42,420
four out input information

660
00:44:42,470 --> 00:44:47,250
so at the beginning of asking for a zero we don't know what you of

661
00:44:48,410 --> 00:44:50,540
and they put into temporary buffer

662
00:44:51,150 --> 00:44:52,170
is that

663
00:44:53,730 --> 00:44:55,720
but this possible

664
00:44:55,730 --> 00:44:57,450
when this buffer

665
00:44:58,850 --> 00:45:01,680
for the next number to keep for while

666
00:45:03,040 --> 00:45:06,910
four to skip way and output it to the hour

667
00:45:07,010 --> 00:45:10,630
five or what what what what they're going to be

668
00:45:10,650 --> 00:45:12,470
this place

669
00:45:12,520 --> 00:45:16,800
fourteen of of course posting list

670
00:45:16,820 --> 00:45:20,410
then after this is right temporary buffer

671
00:45:20,430 --> 00:45:23,440
clean repeat this separation

672
00:45:23,450 --> 00:45:26,350
so it's is pretty simple

673
00:45:26,360 --> 00:45:29,500
to create a a portal is keep

674
00:45:29,740 --> 00:45:35,950
there's only one problem that the during according need this temporary buffer ski way

675
00:45:36,980 --> 00:45:38,760
i want to

676
00:45:39,560 --> 00:45:43,900
one hundred percent it also in use such structure

677
00:45:43,910 --> 00:45:46,580
and you can see it

678
00:45:46,600 --> 00:45:50,630
not a lot of some number of research papers

679
00:45:50,640 --> 00:45:52,290
this structure called two

680
00:45:52,310 --> 00:45:55,680
two of this keep harmonies keeps in a

681
00:45:55,700 --> 00:46:00,050
but usually it is also the simplest approach is to

682
00:46:01,030 --> 00:46:03,770
for this particular posting lists

683
00:46:03,810 --> 00:46:10,650
the size of these temporary buffer because size of these jumps around school where root

684
00:46:11,130 --> 00:46:18,370
from the length of the postings lists and usually it works pretty well

685
00:46:18,380 --> 00:46:21,560
another approach toward

686
00:46:21,620 --> 00:46:23,080
ski please

687
00:46:23,100 --> 00:46:24,580
it is

688
00:46:24,630 --> 00:46:29,900
is very simple we talked about block compression and i said that block compression could

689
00:46:30,470 --> 00:46:32,700
extremely effective

690
00:46:32,780 --> 00:46:35,030
and football competitions can

691
00:46:35,030 --> 00:46:36,110
but we can

692
00:46:36,120 --> 00:46:38,510
look at the block compression

693
00:46:38,530 --> 00:46:44,050
and as an implementation of its key players

694
00:46:44,070 --> 00:46:48,630
because we have blocked what we can do remember interpolated

695
00:46:48,650 --> 00:46:52,660
according to its always number and delta

696
00:46:52,670 --> 00:46:57,160
so imagine that you are looking for i don't know what thing number

697
00:46:57,180 --> 00:46:58,900
two hundred

698
00:46:58,900 --> 00:47:01,800
so without looking at first block

699
00:47:01,820 --> 00:47:05,240
and we're looking that his block stars from five

700
00:47:05,250 --> 00:47:08,120
and delta length of this block the

701
00:47:08,150 --> 00:47:12,900
the difference between smallest IDM biggest estate is one hundred thirty four

702
00:47:12,920 --> 00:47:17,750
so it means that the last posting in this in this block

703
00:47:17,760 --> 00:47:23,350
is one hundred fifty six thirty nine so it's less than two hundred

704
00:47:23,370 --> 00:47:26,260
we don't need to incorporate something from his blog

705
00:47:26,270 --> 00:47:31,010
they can simply jump over this block start looking at the next block

706
00:47:31,030 --> 00:47:33,480
and for an example of feel stupid be

707
00:47:33,520 --> 00:47:37,530
one thirty nine plus eleven so it

708
00:47:39,320 --> 00:47:41,940
current sixty and we we can see that

709
00:47:45,420 --> 00:47:49,800
is supposed to be somewhere in this block and begin to compress it and look

710
00:47:49,810 --> 00:47:51,980
these two on a

711
00:47:53,430 --> 00:47:55,200
block according

712
00:47:55,210 --> 00:47:57,330
it's automatic skip lists

713
00:47:57,360 --> 00:47:58,970
and also it

714
00:47:58,980 --> 00:48:00,190
very high

715
00:48:00,210 --> 00:48:02,670
compression ratio as i said

716
00:48:02,720 --> 00:48:05,820
only one disadvantage that is pretty small

717
00:48:05,840 --> 00:48:10,830
if we need to decompress we always take it always takes a lot of time

718
00:48:10,840 --> 00:48:14,110
therefore it's not very and also it's very complex

719
00:48:14,160 --> 00:48:20,000
therefore it is not very common algorithmic implementation of inverted file

720
00:48:20,020 --> 00:48:28,210
another another situation is really is implementation of inverted index over the tree

721
00:48:28,220 --> 00:48:29,820
the is very good

722
00:48:29,950 --> 00:48:32,410
structure was keeping

723
00:48:33,780 --> 00:48:39,140
we all the also dividing our policies listening to number of blocks

724
00:48:39,160 --> 00:48:41,110
and more than that in

725
00:48:41,130 --> 00:48:42,240
the treaty

726
00:48:42,240 --> 00:48:46,550
we can do not simply keep or one block

727
00:48:46,570 --> 00:48:50,530
b because their order it and we can do so much

728
00:48:50,580 --> 00:48:52,690
one assertion this knowledge

729
00:48:52,720 --> 00:48:53,570
we can do

730
00:48:54,960 --> 00:48:59,260
much faster much faster because we can keep and also told

731
00:48:59,280 --> 00:49:04,590
from from the root note start positions in every in every node

732
00:49:04,600 --> 00:49:06,740
and and we can do along this keeps

733
00:49:06,740 --> 00:49:07,670
there is

734
00:49:07,690 --> 00:49:09,040
not that article

735
00:49:11,090 --> 00:49:15,270
conference three w from to solve all

736
00:49:15,280 --> 00:49:19,330
three i believe that people implemented

737
00:49:20,880 --> 00:49:25,880
so much of our the trees and they call this very simple and they keep

738
00:49:25,890 --> 00:49:30,410
not do during marriage ended airing during

739
00:49:30,450 --> 00:49:36,260
building of results they called it exact choice because this keeping doing this

740
00:49:36,270 --> 00:49:38,420
john's in the tree

741
00:49:39,510 --> 00:49:41,700
to plants and bacteria

742
00:49:41,710 --> 00:49:44,090
leaves of this tree

743
00:49:44,110 --> 00:49:48,060
so this is an example of another automatic skip lists

744
00:49:48,080 --> 00:49:50,120
and if you

745
00:49:50,130 --> 00:49:55,180
are good programmer and the creator good implementations

746
00:49:55,180 --> 00:49:57,230
of these

747
00:49:57,240 --> 00:49:58,550
o thing

748
00:49:58,610 --> 00:50:02,240
you can achieve pretty good performance

749
00:50:02,270 --> 00:50:03,670
but the stu

750
00:50:03,720 --> 00:50:05,760
very unlikely

751
00:50:05,780 --> 00:50:08,310
in this situation where you have

752
00:50:08,330 --> 00:50:10,810
query this every is

753
00:50:10,820 --> 00:50:13,320
a very popular before

754
00:50:14,370 --> 00:50:16,610
you cannot keep anything

755
00:50:17,030 --> 00:50:22,960
because all you need in a case you need to comparable to position

756
00:50:22,970 --> 00:50:26,670
imagine that we also like that we have query

757
00:50:26,720 --> 00:50:29,220
for example zero one five

758
00:50:29,240 --> 00:50:33,230
and a lot of documents can contain such

759
00:50:33,270 --> 00:50:35,270
number far

760
00:50:35,270 --> 00:50:37,050
and all of these numbers

761
00:50:37,070 --> 00:50:40,120
this huge force base

762
00:50:41,530 --> 00:50:43,810
how we can improve speed

763
00:50:43,830 --> 00:50:47,810
in this case

764
00:50:48,560 --> 00:50:53,010
it can be improved using some skip police or something else

765
00:50:53,050 --> 00:50:55,890
because we need to check relative position

766
00:50:55,940 --> 00:50:58,480
the only one solution is to a

767
00:50:58,500 --> 00:51:00,010
help us during

768
00:51:01,090 --> 00:51:03,260
building of the index

769
00:51:03,270 --> 00:51:04,250
we can

770
00:51:04,250 --> 00:51:08,260
do this intersection of post lists in advance

771
00:51:08,260 --> 00:51:10,380
now basically

772
00:51:10,390 --> 00:51:14,700
and the such interesting bayesian stats and as such a model

773
00:51:15,860 --> 00:51:19,180
although on the bottom interested basically

774
00:51:19,760 --> 00:51:23,340
the failure rate on that one too long that ten on the additional

775
00:51:23,360 --> 00:51:25,320
i feel parameter beta

776
00:51:26,740 --> 00:51:30,180
for the bayesian point of view it means that what i'm interested in interest in

777
00:51:30,180 --> 00:51:32,820
the posterior distribution on on that one too long that ten

778
00:51:33,260 --> 00:51:37,720
beattie given basically the observation time on the number of fellows

779
00:51:37,740 --> 00:51:41,090
so it's very simple or is proportional simply

780
00:51:41,110 --> 00:51:45,340
two disperse percent solution is proportional to the point

781
00:51:46,390 --> 00:51:48,160
on the playoff for all

782
00:51:48,180 --> 00:51:51,490
gamma distribution for each parameter

783
00:51:51,550 --> 00:51:52,930
even beta

784
00:51:52,950 --> 00:51:56,740
OK so this is why those terms coming from the gamma pi all these things

785
00:51:56,740 --> 00:51:58,880
coming basically

786
00:51:58,880 --> 00:52:03,680
the apply on the top on this is essentially the like to tell corresponding to

787
00:52:03,680 --> 00:52:10,070
my question assumption so i'm interested in is the solution OK is six really simple

788
00:52:10,070 --> 00:52:14,640
models on still have no clue essentially to approximate

789
00:52:14,910 --> 00:52:19,990
in doing like deterministic approximation you would really accelerometer with the pain

790
00:52:20,010 --> 00:52:23,930
OK when you try to do addiction something you need to come up with a

791
00:52:23,930 --> 00:52:28,030
kind of like probability distribution on what to do about them is too high dimensional

792
00:52:28,220 --> 00:52:30,930
to kind of the public distribution

793
00:52:30,950 --> 00:52:36,680
it's appeal because essentially these bodies but this essentially this column seems to have a

794
00:52:36,680 --> 00:52:38,340
lot of structure

795
00:52:38,360 --> 00:52:40,030
in particular

796
00:52:40,030 --> 00:52:45,050
o xo the joint distribution of lambda beta units allegations quite complicated

797
00:52:46,410 --> 00:52:50,470
the some expose some conditional distribution of a much simpler form so if you look

798
00:52:50,470 --> 00:52:52,050
at the conditional

799
00:52:52,070 --> 00:52:56,030
distribution of the parliament along that given south asia

800
00:52:56,050 --> 00:53:01,140
well by using element articulation and you see that this thing factorizes according of gamma

801
00:53:01,140 --> 00:53:03,680
distribution is quite simple

802
00:53:03,760 --> 00:53:06,990
similarly if you look at the conditional distribution

803
00:53:07,010 --> 00:53:13,200
peter argue end up salvation on the parliament along this thing is seen before

804
00:53:13,220 --> 00:53:17,010
gamma distribution well so the joint distribution is the nightmare

805
00:53:17,070 --> 00:53:22,990
but the conditional distribution of the get along that you beta beta you quite simple

806
00:53:25,590 --> 00:53:29,910
what you propose going to try to look get the is which is called actually

807
00:53:30,490 --> 00:53:35,200
a gibbs sampler we try to justify later on so and so what i'm proposing

808
00:53:35,200 --> 00:53:40,990
so as to approximate the posterior distribution of blonde on beta given the salvation is

809
00:53:40,990 --> 00:53:43,090
the following iterative algorithm

810
00:53:43,110 --> 00:53:47,530
on is known in the literature the gibbs sampler so that the algorithm is to

811
00:53:47,530 --> 00:53:51,110
be like rejection sampling OK i'm not going to go until late on to have

812
00:53:51,200 --> 00:53:53,450
sample exactly is going to percent

813
00:53:53,570 --> 00:53:58,050
it's an iterative algorithm repositories follows so similar iteration i

814
00:54:00,620 --> 00:54:02,510
you have a set of parliament

815
00:54:02,530 --> 00:54:05,800
these are the current value of the parliament along the on the current value of

816
00:54:06,530 --> 00:54:11,510
on what you're proposing to do at iteration i plus one of your iterative algorithm

817
00:54:11,570 --> 00:54:15,660
is that conditional upon the current value of beta

818
00:54:15,680 --> 00:54:17,120
use on paul

819
00:54:17,140 --> 00:54:21,640
the parameter lambda according to the conditional distribution

820
00:54:21,640 --> 00:54:24,610
this is the part of the gamma distribution OK

821
00:54:24,610 --> 00:54:27,030
on once you basically

822
00:54:27,050 --> 00:54:30,660
when you realize values for the parameters

823
00:54:31,700 --> 00:54:33,840
you data basically

824
00:54:33,860 --> 00:54:38,220
beta apartment to be done by sampling it for according to its conditional distribution given

825
00:54:38,240 --> 00:54:42,700
the current value of lambda on the observation OK so that i know because the

826
00:54:42,700 --> 00:54:45,360
conditional distribution of mary's

827
00:54:45,380 --> 00:54:48,140
so that's good so i this kind of algorithms

828
00:54:48,180 --> 00:54:53,410
we basically instead of going to knowledge in this area and emotional space i just

829
00:54:53,410 --> 00:55:00,280
need to solve for c paul essentially is won the valuable distributed according to start

830
00:55:00,430 --> 00:55:03,200
gamma i can do that using matlab say OK

831
00:55:03,220 --> 00:55:04,320
so you can do that

832
00:55:04,340 --> 00:55:10,700
OK on you could also they thing formerly on this question is basically where

833
00:55:10,720 --> 00:55:14,030
you do have to sample from this conditional distribution

834
00:55:14,050 --> 00:55:18,680
i think is going to give you basically sold for approximately distributed according to the

835
00:55:18,680 --> 00:55:22,970
joint distribution of interest it seems like it's a computer stick for the time being

836
00:55:22,990 --> 00:55:28,550
on the fifth basically the chase you basically is iterative algorithm converged in some sense

837
00:55:28,550 --> 00:55:31,590
conditioning on an impossibilities less the technical

838
00:55:31,620 --> 00:55:33,680
condition here

839
00:55:33,700 --> 00:55:40,350
also x is independent of y given v if the probability of x and y

840
00:55:40,370 --> 00:55:41,580
given the

841
00:55:41,590 --> 00:55:45,030
factors into the probability of x given b

842
00:55:45,060 --> 00:55:47,800
and the probability of y given p

843
00:55:50,340 --> 00:55:55,200
so in general we can think of conditional independence between sets of variables

844
00:55:55,210 --> 00:55:59,900
so users are currently notation for sex sets

845
00:55:59,950 --> 00:56:02,740
x is independent of y given v

846
00:56:02,750 --> 00:56:06,380
it is the sort of factorizations

847
00:56:07,770 --> 00:56:10,010
marginal independence the sort of

848
00:56:10,080 --> 00:56:12,080
usual for independence that

849
00:56:12,080 --> 00:56:18,700
you know we first learn about is just independence with conditioning on the empty set

850
00:56:18,950 --> 00:56:21,320
OK so x is independent of y

851
00:56:21,330 --> 00:56:26,070
it the same as conditional independence given nothing and that just means that the joint

852
00:56:26,070 --> 00:56:30,700
distribution of x and y factors vision of x times the distribution of y

853
00:56:30,750 --> 00:56:38,520
OK so why conditional independence important why is marginal independence important well it tells us

854
00:56:38,520 --> 00:56:42,070
what things depend on what other things and what things we can ignore

855
00:56:42,080 --> 00:56:46,390
and part of doing things efficiently is being able to ignore as many things as

856
00:56:47,940 --> 00:56:49,990
so conditional independence

857
00:56:50,010 --> 00:56:52,320
happens in all sorts of

858
00:56:52,330 --> 00:56:56,050
nice intuitive places and i want to highlight

859
00:56:56,060 --> 00:56:58,660
conditional independence

860
00:56:58,680 --> 00:57:01,010
as opposed to marginal independence

861
00:57:01,760 --> 00:57:04,220
so let's look at some of these examples

862
00:57:04,240 --> 00:57:07,830
so the amount of speeding fine

863
00:57:07,900 --> 00:57:13,560
is conditionally independent from the type of car that you're driving given the speed that

864
00:57:13,560 --> 00:57:15,510
you're caught driving OK

865
00:57:15,530 --> 00:57:19,310
so if you're driving if there are any at fifty six miles per hour you

866
00:57:19,310 --> 00:57:24,200
should get the same speeding fine as if you're driving a fiat fifty six miles

867
00:57:24,200 --> 00:57:25,140
per hour

868
00:57:25,160 --> 00:57:26,710
of course

869
00:57:26,720 --> 00:57:33,200
this is a good example thinking about marginal independence if you just randomly went out

870
00:57:33,200 --> 00:57:36,400
and measured speeding fines

871
00:57:36,430 --> 00:57:37,570
and you

872
00:57:38,370 --> 00:57:43,130
measured types of cars the people were driving you would not find these things are

873
00:57:43,130 --> 00:57:48,320
independent right ferrari drivers will get bigger speeding fines than feel drivers

874
00:57:51,510 --> 00:57:56,680
again a classic example is for example smoking

875
00:57:56,690 --> 00:58:02,180
tends to make people's teeth yellow and is associated with lung cancer but

876
00:58:04,700 --> 00:58:09,660
lung cancer and yellow teeth are conditionally independent given y

877
00:58:09,680 --> 00:58:12,950
if you know whether somebody is a smoker or not

878
00:58:12,970 --> 00:58:17,740
now these are sort of classic things but you can imagine conditional independence being used

879
00:58:17,740 --> 00:58:24,190
in all sorts of other settings where maybe you're sort of doing something like robotics

880
00:58:24,200 --> 00:58:29,460
or tracking OK so it's you're modelling the dynamics of the system and you measure

881
00:58:29,460 --> 00:58:33,510
the position and velocity and acceleration of the system over time

882
00:58:33,520 --> 00:58:36,640
and you draw out model

883
00:58:36,640 --> 00:58:40,120
and then you could say well the state of the system the position and velocity

884
00:58:40,130 --> 00:58:44,080
at time t plus one is actually independent of the position and velocity at t

885
00:58:44,080 --> 00:58:49,060
minus one given the state the position and velocity at time t and the acceleration

886
00:58:49,060 --> 00:58:50,450
at time t

887
00:58:51,740 --> 00:58:54,590
you could be doing

888
00:58:54,710 --> 00:59:03,200
genetic pedigree analysis and looking at children's genes

889
00:59:03,210 --> 00:59:07,710
parents genes their grandparents genes and then you would want to use the conditional independence

890
00:59:07,710 --> 00:59:13,970
structure to be able to infer you know the genotype of a particular individual given

891
00:59:13,990 --> 00:59:15,770
their ancestry

892
00:59:15,880 --> 00:59:20,060
and in the nice example the chris showed

893
00:59:20,070 --> 00:59:24,620
if we have two teams of players playing against each other

894
00:59:24,640 --> 00:59:27,020
maybe have three or priority

895
00:59:27,030 --> 00:59:33,380
their abilities are independent but conditioned on the outcome of the game a versus b

896
00:59:33,400 --> 00:59:37,270
the ability of team and the ability of team b become

897
00:59:37,290 --> 00:59:42,710
conditionally dependent so knowledge of the outcome informs us of

898
00:59:42,720 --> 00:59:45,300
both of these things in there

899
00:59:45,320 --> 00:59:46,770
depend on each other

900
00:59:47,370 --> 00:59:49,610
any questions about that yet

901
00:59:49,630 --> 00:59:54,040
what we condition and what they want stay in

902
00:59:54,050 --> 00:59:57,000
o belief five was

903
00:59:57,030 --> 01:00:02,790
but but but the question is it amount of speeding car

904
01:00:02,810 --> 01:00:07,530
doesn't depend on the type of of sorry amount of speeding fine doesn't depend on

905
01:00:07,530 --> 01:00:09,110
type of car

906
01:00:10,500 --> 01:00:13,960
you when you say that you have to say under what assumptions

907
01:00:13,980 --> 01:00:17,560
OK so what you know about

908
01:00:17,580 --> 01:00:22,270
what you know about the situation if you know that

909
01:00:22,320 --> 01:00:25,530
if you know the speed it doesn't depend but if you just go and measure

910
01:00:25,540 --> 01:00:27,130
these things

911
01:00:27,150 --> 01:00:31,220
then there is the dependency you can plot the dependencies between the two

912
01:00:32,250 --> 01:00:33,770
so all

913
01:00:33,800 --> 01:00:39,190
notions of dependence and independence are really conditioned on the state of knowledge

914
01:00:39,230 --> 01:00:45,460
in some ways of these are representations of state of knowledge not representations of necessarily

915
01:00:45,460 --> 01:00:48,750
physical things out there OK and

916
01:00:48,780 --> 01:00:52,450
things can become dependent once you know something else

917
01:00:53,430 --> 01:00:58,320
so let me start talking about factor graphs again you've heard about this from chris

918
01:00:58,320 --> 01:01:00,670
i'm going to go pretty quickly hopefully

919
01:01:01,630 --> 01:01:04,450
you have two types of nodes

920
01:01:04,460 --> 01:01:06,910
in a factor graph

921
01:01:06,970 --> 01:01:12,700
the circles in a factor graph represent random variables the squares are filled dots represent

922
01:01:12,700 --> 01:01:15,060
factors in the joint distribution

923
01:01:15,080 --> 01:01:19,350
so here are two different factor graphs over five variables a b c d and

924
01:01:19,350 --> 01:01:22,600
e and factor graph eighty represents

925
01:01:22,620 --> 01:01:25,820
this factorisation of the joint distribution

926
01:01:27,040 --> 01:01:30,110
so for example g one is the factor

927
01:01:30,120 --> 01:01:32,780
that relates a and c

928
01:01:32,790 --> 01:01:39,550
g two relates b c and d in g three relate c d and e

929
01:01:39,820 --> 01:01:44,960
this thing at the beginning here is just a normalizing constant

930
01:01:47,410 --> 01:01:52,120
what we have here is the probability distribution over these five quantities if we sum

931
01:01:52,120 --> 01:01:56,520
or integrate over all the variables it has two summer integrate to one so we

932
01:01:56,520 --> 01:02:01,560
need a normalising constant to make sure that when we multiply the factors together and

933
01:02:01,560 --> 01:02:03,800
we some out some to one as well

934
01:02:04,620 --> 01:02:09,190
what are the factors the factors are functions of their arguments OK

935
01:02:09,200 --> 01:02:13,120
so if ANC are binary variables

936
01:02:13,130 --> 01:02:16,660
this factor is a two by two tables

937
01:02:16,680 --> 01:02:18,690
so two by two table

938
01:02:18,700 --> 01:02:21,370
with nonnegative entries

939
01:02:21,400 --> 01:02:27,350
we don't want negative probabilities otherwise nothing makes much sense k but we can allow

940
01:02:27,410 --> 01:02:33,630
the factors to have any nonnegative number in because then we have a normalizing constant

941
01:02:33,650 --> 01:02:35,700
that ensures the thing sum to one

942
01:02:38,400 --> 01:02:42,740
this factor graph represents this factorizations

943
01:02:42,770 --> 01:02:46,220
the factor graph b represents this factorizations

944
01:02:47,940 --> 01:02:51,870
and i think i've said all of these things

945
01:02:51,870 --> 01:02:54,270
has a roof you just need to move around so that is the right

946
01:02:54,270 --> 01:02:57,850
place so that's what i call energy-based learning and and

947
01:02:58,340 --> 01:03:01,990
yeah ok probably sick a this this emission is kind of

948
01:03:02,500 --> 01:03:06,170
so special case of this if you want you can sort of the formulated

949
01:03:06,180 --> 01:03:09,560
in those terms but but those this technique includes things are

950
01:03:09,570 --> 01:03:11,780
not probabilistic in nature and general

951
01:03:12,130 --> 01:03:14,140
tso can seven different methods to

952
01:03:14,450 --> 01:03:16,510
strategies to shape the energy function

953
01:03:16,980 --> 01:03:21,230
in the right way in such a way that data points lower energy than

954
01:03:21,230 --> 01:03:24,620
things outside the manifold of high this tendency to essentially

955
01:03:24,620 --> 01:03:27,540
the first one is to build machines such a way that the volume

956
01:03:27,540 --> 01:03:30,480
of low energy stuff is constant so basically the same normalized

957
01:03:30,480 --> 01:03:33,430
distribution ok so the energy function did negative log

958
01:03:33,880 --> 01:03:38,150
probabilities synonymized distribution this finite fixed volume

959
01:03:38,150 --> 01:03:40,990
of stuff they can have low energy because the distribution is

960
01:03:40,990 --> 01:03:45,600
normalized but there are other non obviously models have this property

961
01:03:45,610 --> 01:03:50,080
pch or k-means or yes mixture models or square here stuff that

962
01:03:50,580 --> 01:03:54,070
the second strategy is to push down the energy of data points push

963
01:03:54,070 --> 01:03:57,480
up everywhere else so that's basically what maximum-likelihood

964
01:03:57,700 --> 01:04:01,090
does when your function has an explicit normalised are when when

965
01:04:01,090 --> 01:04:04,260
you're so you probably just vision if you density estimation

966
01:04:04,260 --> 01:04:05,750
as an explicit normalization

967
01:04:08,040 --> 01:04:12,720
and that doesn't quite work all the time because it's possible that

968
01:04:12,720 --> 01:04:16,120
the the this normalization on the partition function would be intractable

969
01:04:16,120 --> 01:04:18,520
or gradient with the intractable and so

970
01:04:18,520 --> 01:04:21,420
a lot of situations we can really do this is a lot papers in

971
01:04:21,420 --> 01:04:24,370
machine learning community increase by people in this room

972
01:04:24,370 --> 01:04:27,740
that have to do with how do you kind of make approximations of this

973
01:04:27,740 --> 01:04:30,910
that it's to works by some approximations something whatever

974
01:04:32,030 --> 01:04:35,310
for method push on the energy of data points push up on chosen

975
01:04:35,310 --> 01:04:39,350
locations sometimes close to the point so you know living outside

976
01:04:39,630 --> 01:04:43,330
so things like contrastive divergence ratio matching noise contrastive

977
01:04:43,330 --> 01:04:46,420
estimation you know every time as this this slide people to pictures

978
01:04:46,420 --> 01:04:49,490
of it and you know i should probably this paper some point

979
01:04:54,260 --> 01:04:58,470
so as other other methods this site so protest divergence is you

980
01:04:58,470 --> 01:05:01,560
take a point and then you would be away from it preferentially in

981
01:05:01,560 --> 01:05:03,520
directions where the energy goes down

982
01:05:03,520 --> 01:05:06,100
and then you push up on that point and what does that creates

983
01:05:06,100 --> 01:05:09,480
local groove in the energy surface it doesn't trying to shape

984
01:05:09,480 --> 01:05:12,130
the energy surface far away from data but you know

985
01:05:12,430 --> 01:05:17,450
as problem number for our you might gradient maximize the curvature

986
01:05:17,450 --> 01:05:19,510
around data point this what's called score matching

987
01:05:19,510 --> 01:05:22,280
that's actually pretty much practical complex models

988
01:05:22,940 --> 01:05:25,960
fifth method you train a dynamical system so that danny's

989
01:05:25,960 --> 01:05:29,500
goes the manifold this you know you know you could was very popular

990
01:05:29,500 --> 01:05:31,020
around 'trial but

991
01:05:33,640 --> 01:05:34,650
you know and beyond

992
01:05:36,910 --> 01:05:43,100
but talk much about this this much everyone so if we use regular

993
01:05:43,100 --> 01:05:44,930
as and this is your everyone to by the way

994
01:05:44,930 --> 01:05:47,320
for most of you least even if you don't know

995
01:05:47,320 --> 01:05:50,470
which is to use regular as that limits the volume of space that has

996
01:05:50,470 --> 01:05:53,800
low energy and that includes things like dictionary learning for

997
01:05:53,800 --> 01:05:57,320
sparse coding so dictionary for costs sparse coding essentially

998
01:05:57,780 --> 01:06:01,530
you could think of the sparsity term as when you do dictionary learning

999
01:06:01,710 --> 01:06:05,860
as a way of limiting the volume of space that can be properly constructed

1000
01:06:06,340 --> 01:06:09,950
ok and if you use the reconstruction error as the energy

1001
01:06:09,960 --> 01:06:14,080
function then that means you can create the move that with the

1002
01:06:14,090 --> 01:06:17,630
volume around data points that manifold or through the

1003
01:06:17,960 --> 01:06:23,410
training one to talk that ok so pc ok so if you which are points

1004
01:06:23,410 --> 01:06:26,730
drawn from this spiral to pc with one dimension you get this is

1005
01:06:26,730 --> 01:06:29,840
a terrible approximation but if you have some you know the energy

1006
01:06:29,840 --> 01:06:33,330
function is zero on the black and sort of increases increases with

1007
01:06:34,410 --> 01:06:37,790
intensity here to easy you'll k-means if you train

1008
01:06:38,720 --> 01:06:45,160
with little quadratic welds are all around this the surface

1009
01:06:45,770 --> 01:06:48,990
seems that it's great except doesn't scalar in high dimension

1010
01:06:53,930 --> 01:06:56,380
and so this this is just getting sparse coding

1011
01:06:56,670 --> 01:07:00,210
sparse modeling india a when you when you learn dictionary learning

1012
01:07:00,470 --> 01:07:04,050
is you have an energy function this type where you have the square

1013
01:07:04,050 --> 01:07:06,330
reconstruction error where y is observation

1014
01:07:06,550 --> 01:07:09,220
then you is dictionary matrix z is sparse vector

1015
01:07:09,670 --> 01:07:13,480
which by when you multiply but dictionary matrix is meant to reconstruct

1016
01:07:13,480 --> 01:07:17,260
the input or his approximately with a square penalty function and

1017
01:07:17,270 --> 01:07:22,680
then you have this sparsity term so elector represent this as

1018
01:07:22,680 --> 01:07:26,050
kind of funny type factor graph where those are variables or this

1019
01:07:26,050 --> 01:07:28,850
one is observed this one is latent has to be inferred

1020
01:07:28,850 --> 01:07:32,130
and then you have deterministic functions that those symbols those

1021
01:07:32,130 --> 01:07:36,390
factors which are kind of times in the energy function or enter additively

1022
01:07:36,390 --> 01:07:38,820
in the energy function or multiplicatively in the likelihood

1023
01:07:38,820 --> 01:07:41,720
function if you take exponentials so the problem with

1024
01:07:42,150 --> 01:07:44,580
sparse coding is that even once you've learned

1025
01:07:44,890 --> 01:07:48,000
dictionary inference is relatively expensive

1026
01:07:48,080 --> 01:07:51,620
is also efficient algorithms we heard about one this morning in the

1027
01:07:51,620 --> 01:07:56,050
first talk cht is is fista whatever according to descend is

1028
01:07:56,050 --> 01:07:58,390
all kinds of efficient algorithms for this but this just

1029
01:07:58,390 --> 01:08:01,780
so if you want to do sparse coding with four thousand dimension

1030
01:08:01,860 --> 01:08:04,520
on a window in images going to take a few seconds

1031
01:08:04,910 --> 01:08:08,440
and tricks autres and you have with to do this approximately as

1032
01:08:08,450 --> 01:08:12,200
fast but but you know it's it's going expensive

1033
01:08:12,510 --> 01:08:16,140
now sparse coding essentially makes assumption that data is sort

1034
01:08:16,150 --> 01:08:19,030
of union of planes for the energy function really has

1035
01:08:19,240 --> 01:08:22,480
a kind of linear grooves of rule relatively low dimension and

1036
01:08:22,480 --> 01:08:24,480
then you can add them up and you get this sort of

1037
01:08:24,480 --> 01:08:30,030
approximation of the of the data so so one thing that we cannot

1038
01:08:30,040 --> 01:08:33,560
with back two thousand eight or seven well as to

1039
01:08:33,800 --> 01:08:38,580
learn quick approximations of sparse coding inference so instead

1040
01:08:38,590 --> 01:08:42,230
of learning ista our d you see here i ratio whatever our

1041
01:08:43,310 --> 01:08:47,380
we're going to train yeah what amounts to no net essentially to

1042
01:08:47,380 --> 01:08:49,230
produce a good approximation of the

1043
01:08:49,230 --> 01:08:51,820
ultimate solution sparse coding inference problem

1044
01:08:51,820 --> 01:08:55,010
and so that these are what's called sparse but encoder so you

1045
01:08:55,010 --> 01:08:58,020
have the top is just getting are the same

1046
01:08:58,300 --> 01:09:02,120
the same as i to originally this thing at the bottom here which

1047
01:09:02,120 --> 01:09:03,970
is a non not parameterised function

1048
01:09:04,080 --> 01:09:08,460
which predicts try to predict what the optimal code is for particular

1049
01:09:08,470 --> 01:09:11,370
why ok so you can white you find z that

1050
01:09:11,370 --> 01:09:14,800
x one x is zero you find the potential to zero

1051
01:09:14,810 --> 01:09:16,020
and when x is

1052
01:09:16,040 --> 01:09:17,850
one centimeter

1053
01:09:17,910 --> 01:09:20,560
you find the potential to be thousand volts

1054
01:09:20,610 --> 01:09:21,860
and that then

1055
01:09:21,920 --> 01:09:24,710
goes together with the electric field

1056
01:09:24,770 --> 01:09:26,530
equals minus

1057
01:09:26,620 --> 01:09:28,540
and the this

1058
01:09:28,610 --> 01:09:31,160
in this direction

1059
01:09:31,310 --> 01:09:33,600
it is extremely physical

1060
01:09:33,610 --> 01:09:36,230
this is something that you would have whenever we deal

1061
01:09:36,270 --> 01:09:41,500
with parallel plates

1062
01:09:41,630 --> 01:09:42,550
as long as

1063
01:09:42,550 --> 01:09:46,680
there is no charge moving

1064
01:09:46,700 --> 01:09:49,080
and we're dealing with

1065
01:09:49,140 --> 01:09:51,560
solid conductors

1066
01:09:51,560 --> 01:09:53,270
so we have static

1067
01:09:53,340 --> 01:09:55,140
electric fields

1068
01:09:55,180 --> 01:09:57,040
charges are not heavily moving

1069
01:09:57,120 --> 01:09:59,560
in the field inside the a conductor

1070
01:09:59,570 --> 01:10:00,880
is always zero

1071
01:10:00,900 --> 01:10:02,100
not the case

1072
01:10:02,110 --> 01:10:07,700
in non conducted only in the conductor because conductors have free electrons move

1073
01:10:07,740 --> 01:10:11,740
and if these free electrons seeing electric fields inside which they

1074
01:10:11,750 --> 01:10:14,070
they start to move until they

1075
01:10:14,090 --> 01:10:18,200
experience no longer force thereby they kill the electric field inside

1076
01:10:18,350 --> 01:10:22,520
so the charge the conductor always rearranges itself

1077
01:10:22,520 --> 01:10:25,470
so that the electric field becomes zero

1078
01:10:25,520 --> 01:10:30,600
if the field is static field not rapidly changing

1079
01:10:30,640 --> 01:10:31,820
so now

1080
01:10:31,860 --> 01:10:34,070
i want to evaluate with you

1081
01:10:34,150 --> 01:10:35,590
the situation

1082
01:10:35,640 --> 01:10:38,550
that i'm going to charge a solid

1083
01:10:40,170 --> 01:10:43,620
and i asked myself the question where the charge

1084
01:10:46,310 --> 01:10:48,300
in honor of

1085
01:10:48,340 --> 01:10:49,980
valentine's day

1086
01:10:50,010 --> 01:10:52,510
let's take a solid hard

1087
01:10:54,410 --> 01:10:57,510
it's all all the way throughout

1088
01:10:57,550 --> 01:10:59,660
this is a solid conductor

1089
01:10:59,710 --> 01:11:03,970
and i bring on this conductor chart from the outside

1090
01:11:04,060 --> 01:11:09,770
the bottom line is not just a plus for now

1091
01:11:09,790 --> 01:11:12,210
so the question that i'm asking you know

1092
01:11:12,220 --> 01:11:15,840
this is the conductor this is not an insulated the story for insulators is totally

1093
01:11:17,210 --> 01:11:20,750
it has three moving electrons inside

1094
01:11:20,770 --> 01:11:23,960
i'm asking you know if i touch this

1095
01:11:24,010 --> 01:11:25,530
conducting hard

1096
01:11:25,590 --> 01:11:28,370
by the way you are very good conductors

1097
01:11:28,730 --> 01:11:33,130
if you touch this conducting hard where with discharge

1098
01:11:33,140 --> 01:11:35,030
and up

1099
01:11:35,080 --> 01:11:37,960
where we go to

1100
01:11:37,960 --> 01:11:41,040
and i leave you with three choices

1101
01:11:41,090 --> 01:11:43,650
i have vote on that

1102
01:11:43,700 --> 01:11:49,700
the first choices that the charges were uniformly distributed for throughout

1103
01:11:49,710 --> 01:11:52,420
the possibility

1104
01:11:52,450 --> 01:11:54,930
the second possibility

1105
01:11:55,020 --> 01:11:56,500
slightly i think

1106
01:11:56,550 --> 01:11:59,970
that all the charge go to one place there

1107
01:12:00,070 --> 01:12:03,480
i don't know which place that would be maybe

1108
01:12:03,550 --> 01:12:07,000
and then the third possibility is that maybe the charge

1109
01:12:08,150 --> 01:12:10,580
uniformly distributed itself

1110
01:12:10,590 --> 01:12:13,580
only on the outer surface

1111
01:12:13,620 --> 01:12:15,730
and then the fourth possibilities

1112
01:12:15,800 --> 01:12:17,860
none of the above

1113
01:12:17,970 --> 01:12:21,700
all the suggestions i made were wrong

1114
01:12:21,710 --> 01:12:23,420
i i think that the charge

1115
01:12:23,470 --> 01:12:29,030
might uniformly distributed throughout the conductor

1116
01:12:29,040 --> 01:12:31,140
i see one two hands

1117
01:12:31,200 --> 01:12:33,700
that's good don't feel ashamed

1118
01:12:33,710 --> 01:12:37,000
raise your hands in the worst case you're wrong i have been so many times

1119
01:12:37,010 --> 01:12:38,500
wrong when it comes

1120
01:12:38,510 --> 01:12:40,490
don't feel bad about that

1121
01:12:40,530 --> 01:12:46,650
who thinks that the charge will go to one point in the hard

1122
01:12:47,370 --> 01:12:51,970
you have the courage to think about one point charge repel it's alright that doesn't

1123
01:12:51,970 --> 01:12:58,200
seem likely with things that will uniformly distributed itself on the outer surface

1124
01:12:58,240 --> 01:13:02,350
o things none of the above

1125
01:13:02,370 --> 01:13:03,810
very good well

1126
01:13:03,820 --> 01:13:08,740
those who suggested that it might be uniform on the outside i would still give

1127
01:13:08,740 --> 01:13:10,740
them abu

1128
01:13:10,790 --> 01:13:12,750
but is not uniform as you will see

1129
01:13:12,800 --> 01:13:13,950
but i will go

1130
01:13:15,220 --> 01:13:19,560
to the outside and i will prove that now to you

1131
01:13:19,640 --> 01:13:21,000
the first

1132
01:13:21,040 --> 01:13:25,410
look for that ridiculous possibility that the charge would somehow end up

1133
01:13:25,460 --> 01:13:28,110
in the conductive itself

1134
01:13:28,130 --> 01:13:29,500
i think here

1135
01:13:29,530 --> 01:13:32,410
gulshan surface which is a closed surface

1136
01:13:32,430 --> 01:13:36,560
i know inside the conductor we electrostatic fields

1137
01:13:36,630 --> 01:13:41,560
not fast moving charges but it's static field i know that field everywhere must be

1138
01:13:41,560 --> 01:13:43,530
zero on the surface

1139
01:13:43,530 --> 01:13:46,880
this is closed surface so the integral of adopt a

1140
01:13:46,900 --> 01:13:50,780
equation one is zero that means the charge inside my

1141
01:13:50,780 --> 01:13:52,090
sphere is zero

1142
01:13:52,110 --> 01:13:53,920
and so there cannot be in charge

1143
01:13:53,970 --> 01:13:58,450
so gauss law immediately kills the possibility that there would be any charge

1144
01:13:59,550 --> 01:14:02,540
this conductor so that's out of the question

1145
01:14:02,590 --> 01:14:05,970
at least you only was one choice that is on at the surface

1146
01:14:05,970 --> 01:14:07,360
charge must be

1147
01:14:07,380 --> 01:14:08,990
at the surface

1148
01:14:08,990 --> 01:14:12,080
and later in the later lecture i will

1149
01:14:12,100 --> 01:14:13,450
discuss with you

1150
01:14:13,450 --> 01:14:14,560
the details

1151
01:14:14,570 --> 01:14:20,430
why that charge is not uniformly distributed it would be uniformly distributed the surface it

1152
01:14:20,460 --> 01:14:21,900
is more here

1153
01:14:22,640 --> 01:14:27,800
if it has this funny shape but it will at the surface

1154
01:14:29,320 --> 01:14:31,290
i'm going to make this heart

1155
01:14:31,340 --> 01:14:33,130
very special heart

1156
01:14:33,140 --> 01:14:34,970
more like a real hard

1157
01:14:34,990 --> 01:14:36,360
is open here

1158
01:14:36,410 --> 01:14:39,300
one of the solid here

1159
01:14:39,380 --> 01:14:42,060
this is a conducting heart muscle

1160
01:14:42,080 --> 01:14:45,030
and here open there's nothing you

1161
01:14:45,030 --> 01:14:47,780
and again i'm going to charge

1162
01:14:47,800 --> 01:14:50,780
bring charges on the outside

1163
01:14:52,100 --> 01:14:56,060
it's obvious that we don't expect that there is any chance it will be

1164
01:14:56,100 --> 01:15:00,380
inside the conductor that clear the same argument holds with the gauss law argument

1165
01:15:00,420 --> 01:15:01,260
but now

1166
01:15:01,260 --> 01:15:03,060
is it perhaps possible

1167
01:15:03,130 --> 01:15:05,130
that some of the positive charge

1168
01:15:05,200 --> 01:15:07,100
will go on the inside

1169
01:15:07,150 --> 01:15:10,770
of the surface and some on the outside

1170
01:15:10,770 --> 01:15:12,960
o things that maybe some will now

1171
01:15:12,970 --> 01:15:16,810
go on the inside because now the situation is different right there is no

1172
01:15:16,820 --> 01:15:19,510
now the whole conduct

1173
01:15:19,550 --> 01:15:22,260
anyone in favor of some of the charge maybe

1174
01:15:22,280 --> 01:15:24,570
going on the inside

1175
01:15:24,630 --> 01:15:26,430
one hand two hands

1176
01:15:26,530 --> 01:15:31,740
who says no it's not possible montgomery inside the building outside

1177
01:15:31,790 --> 01:15:36,050
for most of you are very careful now you don't vote anymore

1178
01:15:36,090 --> 01:15:40,130
it cannot go to the inside why can not go to the inside

1179
01:15:40,220 --> 01:15:43,470
let this be my girl so

1180
01:15:43,500 --> 01:15:47,070
closed surface think of the three-dimensional

1181
01:15:47,120 --> 01:15:51,710
everywhere on that line the electric field is zero because you're inside the conductor

1182
01:15:51,750 --> 01:15:54,020
so the surface integral is also zero

1183
01:15:54,070 --> 01:15:57,500
gauss law says there cannot be any charge inside

1184
01:15:57,540 --> 01:15:59,280
that box

1185
01:16:00,050 --> 01:16:02,280
again the charge has to go

1186
01:16:02,290 --> 01:16:05,330
to the outer surface and nothing will go

1187
01:16:05,350 --> 01:16:06,260
to the

1188
01:16:06,320 --> 01:16:07,830
in a surface

1189
01:16:07,850 --> 01:16:10,060
and so the conclusion then is

1190
01:16:10,060 --> 01:16:14,230
that the electric field is zero in the conductor with the electric field is also

1191
01:16:14,230 --> 01:16:17,950
zero in his opening there's never any charges there

1192
01:16:18,000 --> 01:16:20,920
and so the whole heart including the cavity

1193
01:16:20,970 --> 01:16:26,830
is a potential there's never any electric field anywhere the only electric fields outside

1194
01:16:26,920 --> 01:16:27,800
the heart

1195
01:16:27,800 --> 01:16:29,740
and there are field lines

1196
01:16:29,790 --> 01:16:31,320
and these field lines

1197
01:16:32,300 --> 01:16:35,270
are perpendicular to the surface of the heart

1198
01:16:35,280 --> 01:16:36,450
because the heart

1199
01:16:36,560 --> 01:16:38,030
the equipotential

1200
01:16:39,280 --> 01:16:41,240
you get very funny field lines

1201
01:16:41,360 --> 01:16:43,020
go like this

1202
01:16:43,030 --> 01:16:47,310
have to be perpendicular locally where

1203
01:16:47,310 --> 01:16:50,760
this is it's kind of like magic but it's it's pretty cool so

1204
01:16:50,760 --> 01:16:54,780
let's go back to the old equations that we that we that we had before

1205
01:16:54,780 --> 01:16:59,790
so here with the lucas kanade i had this forward additive idea so i've got

1206
01:16:59,790 --> 01:17:04,290
my source image my delta p is where of following want to be this is

1207
01:17:04,290 --> 01:17:07,410
this is like where the actual line image is going to be

1208
01:17:07,430 --> 01:17:09,010
of course got the actual

1209
01:17:09,020 --> 01:17:10,280
my current guess

1210
01:17:10,290 --> 01:17:13,990
o vary in size from the open sea face detector not really sure

1211
01:17:14,020 --> 01:17:19,580
and then i apply the chain to get this to protein drug

1212
01:17:19,660 --> 01:17:21,790
but with the forward compositional

1213
01:17:21,890 --> 01:17:24,370
what i instead of

1214
01:17:25,950 --> 01:17:27,720
i want to essentially

1215
01:17:29,970 --> 01:17:31,180
it's like this

1216
01:17:31,180 --> 01:17:35,290
and this image and this image should be exactly the same thing but how why

1217
01:17:36,100 --> 01:17:38,930
estimate the what a different my delta p here

1218
01:17:38,950 --> 01:17:44,890
will be different for delta b here because i am essentially of got my coordinates

1219
01:17:44,910 --> 01:17:48,850
what by delta p and then those coordinates i award by

1220
01:17:48,850 --> 01:17:51,330
so it's actually composed

1221
01:17:51,330 --> 01:17:55,280
what i what i then do is about the original source image and this is

1222
01:17:55,280 --> 01:17:58,080
the magic pot so this is why i was so

1223
01:17:58,100 --> 01:18:01,970
i was pointing out that the nominator here about the gradient so here

1224
01:18:01,990 --> 01:18:05,680
what we're doing is we're estimating gradients from the source image

1225
01:18:05,700 --> 01:18:09,640
and and then we'll warping the x and y gradients

1226
01:18:09,680 --> 01:18:12,740
to get this so we so we have to estimate the gradients in the source

1227
01:18:12,740 --> 01:18:17,450
image first and then will be to the to get this these image gradients

1228
01:18:17,450 --> 01:18:21,870
here what i did because i'm actually doing this with respect to

1229
01:18:21,890 --> 01:18:24,390
not p one to p has been removed

1230
01:18:24,410 --> 01:18:25,620
i will

1231
01:18:25,680 --> 01:18:31,740
p of what the source image for p tackling the gradients on that will damage

1232
01:18:31,780 --> 01:18:33,970
and this is this is what this is indicated here

1233
01:18:33,990 --> 01:18:37,810
that i'm not doing it from p of doing it what he has been removed

1234
01:18:37,810 --> 01:18:43,100
because because this is zero this anymore and then apply the exact same will

1235
01:18:43,200 --> 01:18:48,540
to be completely correct the jacobian differently but because the

1236
01:18:48,560 --> 01:18:50,930
my what constraints typically linear

1237
01:18:50,950 --> 01:18:53,260
it doesn't matter what my peers here

1238
01:18:53,310 --> 01:18:57,990
this this is essentially a matrix in ways that the society but this point before

1239
01:18:58,100 --> 01:18:59,580
we calculate the delta p

1240
01:19:01,430 --> 01:19:05,350
this is just the kind of like this is kind of a MAP here about

1241
01:19:05,350 --> 01:19:08,010
just the kind of explaining what's going on

1242
01:19:08,080 --> 01:19:12,410
this image on doing the exact same approximation this image and this image should be

1243
01:19:12,410 --> 01:19:13,790
exactly the same

1244
01:19:13,830 --> 01:19:16,330
my delta p here delta p

1245
01:19:18,240 --> 01:19:20,350
now this is just the forward so

1246
01:19:20,390 --> 01:19:24,640
computationally not getting any benefit here at all

1247
01:19:26,120 --> 01:19:31,910
this and again the actual operation of and i always always keeping so this is

1248
01:19:31,910 --> 01:19:35,600
that this is the paper put forward by harry shum image with the last thing

1249
01:19:35,870 --> 01:19:41,240
and what around the show was essentially you've before compositional output

1250
01:19:42,910 --> 01:19:47,410
you again the hessian but this is the forward compose hessian the jacobian which is

1251
01:19:47,410 --> 01:19:52,390
the four composed jacobian and the only difference is that i'm taking the gradients one

1252
01:19:52,430 --> 01:19:57,390
siemens has been what from p instead of taking the gradient at p then

1253
01:19:57,390 --> 01:20:01,850
and then of got the image differences i get my current delta p update and

1254
01:20:01,850 --> 01:20:07,240
because i'm working at the delta p for composers wall rather than an added what

1255
01:20:07,350 --> 01:20:09,470
i have to oppose

1256
01:20:09,510 --> 01:20:12,790
so i have that IP not by adding delta p

1257
01:20:13,580 --> 01:20:15,830
composing p with this delta p

1258
01:20:15,870 --> 01:20:17,740
and this is my car

1259
01:20:17,790 --> 01:20:22,560
and i keep on applying these steps until p convergence again

1260
01:20:22,580 --> 01:20:23,510
this is so

1261
01:20:23,540 --> 01:20:25,930
can see j ford compositional

1262
01:20:25,970 --> 01:20:29,390
is taken with respect to zero not p

1263
01:20:29,430 --> 01:20:33,600
and looks at all but it's really important

1264
01:20:33,600 --> 01:20:34,410
and so

1265
01:20:34,680 --> 01:20:38,240
in this composition this is this is called the core

1266
01:20:38,310 --> 01:20:42,850
so four compositions still essentially have really boring thing here it's got it's kind of

1267
01:20:42,850 --> 01:20:47,540
nice in the sense that i might be able to save some slight computations on

1268
01:20:47,540 --> 01:20:52,830
if gradients cost me more than tackling the gradient across the entire image cost more

1269
01:20:52,830 --> 01:20:55,560
than k great small area

1270
01:20:55,580 --> 01:20:58,160
excuse me i might be able to buy mister something but i still going to

1271
01:20:58,180 --> 01:21:01,240
do some extra walls as opposed to the other one way i you have to

1272
01:21:01,240 --> 01:21:02,120
do the one more

1273
01:21:02,680 --> 01:21:05,510
has only brought me anything computationally

1274
01:21:06,350 --> 01:21:11,640
if we assume the template is not static we can invert our problem using approximation

1275
01:21:11,640 --> 01:21:16,970
so let's now invert so let's say that all i want to actually approximate my

1276
01:21:16,970 --> 01:21:22,580
car poorly positioned image so so so so the image that badly position that are

1277
01:21:22,680 --> 01:21:24,120
there are no

1278
01:21:24,140 --> 01:21:28,220
of the template which is which is the image which is the temple trying to

1279
01:21:28,220 --> 01:21:31,350
match with and lines instead take gradients

1280
01:21:31,370 --> 01:21:34,040
of the tenth not of the source image

1281
01:21:34,120 --> 01:21:39,700
and because the implied i know is in the ground truth that always with respect

1282
01:21:40,350 --> 01:21:43,780
this zero to so there's no p there at all

1283
01:21:43,790 --> 01:21:46,240
and of course to cope

1284
01:21:46,290 --> 01:21:52,540
and so basically this the inverse compositional approach the big advantage here

1285
01:21:52,560 --> 01:21:57,430
it's essentially i only have to like this

1286
01:21:57,470 --> 01:21:58,330
and this

1287
01:21:59,790 --> 01:22:01,990
and i can actually precomputed

1288
01:22:01,990 --> 01:22:04,350
i don't have to compute at runtime

1289
01:22:04,370 --> 01:22:10,760
so the big advantage is that instead of having two i image gradients b

1290
01:22:10,760 --> 01:22:12,440
immediately hard

1291
01:22:12,440 --> 01:22:14,590
apparent why should be hard to say

1292
01:22:14,610 --> 01:22:19,590
there are here two reasons why you might be interested normalising constants

1293
01:22:19,590 --> 01:22:20,960
one is

1294
01:22:20,960 --> 01:22:22,800
the probability of data

1295
01:22:22,820 --> 01:22:26,480
given the model is that the marginal likelihood of the evidence for the model

1296
01:22:26,490 --> 01:22:32,510
this is an integral it's an expectation you average parameters of your models drawn from

1297
01:22:32,510 --> 01:22:34,900
the prior you average under that distribution

1298
01:22:34,920 --> 01:22:39,090
and you just a each those parameters drawn from my prior how probable if the

1299
01:22:39,090 --> 01:22:41,070
data to happen

1300
01:22:41,210 --> 01:22:43,610
so it looks as though there is an obvious simple

1301
01:22:43,650 --> 01:22:48,630
monte carlo solution to it you just draw parameters from your prior and you evaluate

1302
01:22:48,630 --> 01:22:52,240
how probable that data was under each of those in your average

1303
01:22:52,260 --> 01:22:59,070
and that's actually a terrible algorithm for most data modeling applications because

1304
01:22:59,130 --> 01:23:03,690
you've just written down a bit complicated probabilistic model and you often quite vague priors

1305
01:23:03,690 --> 01:23:05,320
on lots of things that you don't know

1306
01:23:05,340 --> 01:23:08,610
and as soon as you see a bit of data you some learn an awful

1307
01:23:08,610 --> 01:23:10,130
lot about the parameters

1308
01:23:10,210 --> 01:23:13,150
i mean if you doing something as simple as regression as soon as you see

1309
01:23:13,150 --> 01:23:15,320
about three points you know there's whole

1310
01:23:15,340 --> 01:23:20,510
load possible slopes if your line pretty invisible and if you have more flexible model

1311
01:23:20,510 --> 01:23:21,510
and that then

1312
01:23:21,610 --> 01:23:24,900
different bits of your data we'll pin down different parameters in different ways but the

1313
01:23:24,900 --> 01:23:28,050
result is that if you just draw random model

1314
01:23:28,070 --> 01:23:32,510
and draw data not gonna look anything like the data that you actually have a

1315
01:23:32,530 --> 01:23:36,820
what's going to happen if you do the simple monte carlo algorithm is that almost

1316
01:23:36,820 --> 01:23:41,030
all of the parameters you draw will be complete junk this will be about zero

1317
01:23:41,050 --> 01:23:46,110
and then if eventually my draw sensible fitting the parameters and that one sample would

1318
01:23:46,110 --> 01:23:52,380
completely dominate your estimate for the bad algorithm for estimating the article

1319
01:23:52,380 --> 01:23:56,260
another reason you might want normalizing constant is to find the normalizing constant of an

1320
01:23:56,260 --> 01:23:57,690
undirected graphical model

1321
01:23:57,740 --> 01:24:04,570
so a johansen next week we'll talk a lot about various undirected graphical models

1322
01:24:04,590 --> 01:24:07,440
even explained you have some

1323
01:24:07,440 --> 01:24:11,760
function you can evaluate that the product potentials and then there's the normalizing constant which

1324
01:24:11,760 --> 01:24:14,960
is the sum of all the unnormalized probabilities now

1325
01:24:16,110 --> 01:24:20,940
z the normalising constant can sometimes be very useful for model comparison and various other

1326
01:24:20,940 --> 01:24:23,960
tasks but it's a very large sum

1327
01:24:25,150 --> 01:24:29,440
the unnormalized functions defined by an undirected graph

1328
01:24:29,460 --> 01:24:32,030
now if you fitted this graph to data

1329
01:24:32,050 --> 01:24:33,920
you've made it so that

1330
01:24:33,940 --> 01:24:40,940
along the finns skinny weird manifold that looks like a reasonable dataset in your world

1331
01:24:40,990 --> 01:24:45,860
the probability the high and everywhere else in the exponentially huge state space where

1332
01:24:45,880 --> 01:24:49,710
things just look like junk and like reasonable data the probability is very likely so

1333
01:24:49,740 --> 01:24:54,090
this something is completely dominated by the skin the manifold which can be quite hard

1334
01:24:54,090 --> 01:24:57,300
to find a new not confined by very simple sample

1335
01:24:59,280 --> 01:25:01,440
personally i'm actually more interested in

1336
01:25:01,460 --> 01:25:05,400
the bayesian application i'm going to use this undirected graphical model

1337
01:25:05,420 --> 01:25:09,990
example is the running example for computing that because i can draw i can very

1338
01:25:09,990 --> 01:25:12,460
easily draw some pretty pictures for again

1339
01:25:12,460 --> 01:25:15,570
talk about just computing this one scalar

1340
01:25:15,570 --> 01:25:22,460
that will normalize the distribution is you've defined up to a constant

1341
01:25:22,480 --> 01:25:23,980
say a

1342
01:25:24,030 --> 01:25:30,360
to motivate his a benchmark application of why you'd want to compute that i've got

1343
01:25:30,360 --> 01:25:35,030
a probabilistic model which is the boltzmann machine which is the model of binary images

1344
01:25:35,030 --> 01:25:40,170
so the toy vision applications at this stage and

1345
01:25:41,190 --> 01:25:45,570
restricted boltzmann machine defines the probability that images up to constant and i don't know

1346
01:25:47,070 --> 01:25:50,940
and what i want to do is compare this probabilistic model to other models i

1347
01:25:50,940 --> 01:25:56,460
want to say is this model here actually better than something standard like a simple

1348
01:25:56,460 --> 01:26:00,380
mixture model which looks worse but maybe this model is just a fitted

1349
01:26:00,400 --> 01:26:04,710
it's actually better probabilistic models so i want to evaluate probabilities on the test and

1350
01:26:05,090 --> 01:26:06,210
actually this

1351
01:26:06,300 --> 01:26:09,090
evaluated things better on the test and say

1352
01:26:09,090 --> 01:26:12,630
if i have got complicated training procedures and i want to evaluate whether model is

1353
01:26:12,650 --> 01:26:13,460
a good idea

1354
01:26:13,510 --> 01:26:17,900
i want to be able to evaluate the probability to say yes this hasn't it's

1355
01:26:17,900 --> 01:26:20,510
actually about better probabilistic model

1356
01:26:20,530 --> 01:26:21,880
so how many going to

1357
01:26:21,880 --> 01:26:23,860
do this when

1358
01:26:23,860 --> 01:26:27,900
p started only going to be large the very small number of binary images that

1359
01:26:27,900 --> 01:26:30,300
look like digits and most of them

1360
01:26:30,300 --> 01:26:32,400
it's going to be very small

1361
01:26:32,400 --> 01:26:35,360
i certainly don't want to do it by importance sampling

1362
01:26:35,360 --> 01:26:37,840
from a simple distribution

1363
01:26:38,720 --> 01:26:40,460
if i sample

1364
01:26:40,530 --> 01:26:43,070
uniformly random binary images

1365
01:26:43,070 --> 01:26:45,070
they look like this

1366
01:26:45,090 --> 01:26:48,710
OK i get away to very very long time and for one of these looks

1367
01:26:48,710 --> 01:26:51,150
like integer

1368
01:26:51,170 --> 01:26:54,820
and so if i'm going to

1369
01:26:54,860 --> 01:26:58,980
evaluate the probability of each of these images under my model of digit and then

1370
01:26:58,980 --> 01:27:02,670
average all its probabilities together that's not going to

1371
01:27:02,710 --> 01:27:05,070
tell me anything meaningful about

1372
01:27:05,070 --> 01:27:08,240
some attempts actually matter because i'm never going to see any of the terms of

1373
01:27:09,920 --> 01:27:16,090
you might think that the the obvious thing to do is to just sample from

1374
01:27:16,090 --> 01:27:18,990
a different distribution one way you get the chance we know one of the things

1375
01:27:18,990 --> 01:27:21,800
we could sample from the model itself

1376
01:27:21,800 --> 01:27:24,840
now we can sample directly from the model because we only know it up to

1377
01:27:24,840 --> 01:27:27,090
a constant but we could run MCMC

1378
01:27:28,130 --> 01:27:29,460
you could imagine

1379
01:27:29,460 --> 01:27:30,980
let's sample

1380
01:27:30,990 --> 01:27:32,880
using gibbs sampling

1381
01:27:32,920 --> 01:27:37,630
from this probabilistic model of images and generate random images and this these are genuine

1382
01:27:37,630 --> 01:27:41,380
sample drawn from the model and they all look like this and they all have

1383
01:27:41,380 --> 01:27:44,780
high probability under this model

1384
01:27:44,820 --> 01:27:47,110
but this does actually help either

1385
01:27:47,180 --> 01:27:53,170
they might not z is the sum of the unnormalized probabilities over all images

1386
01:27:53,210 --> 01:27:57,530
and here i got some images that we know have high probability

1387
01:27:57,550 --> 01:28:01,300
so i can evaluate p start x these particular images

1388
01:28:01,340 --> 01:28:04,360
but what i need to know to know how big the sum is

1389
01:28:04,420 --> 01:28:06,920
is how many images like this

1390
01:28:06,980 --> 01:28:08,690
not i need to know

1391
01:28:08,710 --> 01:28:12,130
how how probable the problem images but i need to know how many of them

1392
01:28:13,150 --> 01:28:18,280
to approximate the sub and the samples don't tell me that

1393
01:28:18,380 --> 01:28:22,420
you think l but you you just important or something redistribution if you write down

1394
01:28:22,420 --> 01:28:27,880
the importance sampling estimator turns out to tell you actually need to know that say

1395
01:28:27,880 --> 01:28:31,630
it it doesn't actually help you with that that is approximately equal to z so

1396
01:28:31,630 --> 01:28:34,420
i'm just write that down and you're done which is useful

1397
01:28:34,440 --> 01:28:38,070
so a

1398
01:28:38,090 --> 01:28:44,150
don't kind textbook has a really nice discussion of

1399
01:28:44,190 --> 01:28:48,570
what's going on in these algorithms and why findings at its heart it says

1400
01:28:48,670 --> 01:28:52,940
imagine that was the volume of the lake so now you state space you're wandering

1401
01:28:52,940 --> 01:28:57,590
around the area of the lake in your unnormalized functions is that the like and

1402
01:28:57,590 --> 01:29:01,440
obviously the volume of the integral of the unnormalized functions over here

1403
01:29:01,440 --> 01:29:02,570
like the base

1404
01:29:02,590 --> 01:29:04,010
sales around

1405
01:29:04,070 --> 01:29:06,320
so is something from the prior

1406
01:29:06,340 --> 01:29:10,720
this is the lake with very spiky canyons because we want to model only put

1407
01:29:10,740 --> 01:29:15,010
extreme values of the function on very skinny canyons look like they

1408
01:29:15,050 --> 01:29:18,920
so as you sail around most of the time you're not about the canyon they

1409
01:29:18,920 --> 01:29:22,800
sample from the prior would just be solar around random ignoring how do like it

1410
01:29:22,800 --> 01:29:25,030
and occasionally look at how they like

1411
01:29:25,030 --> 01:29:29,920
that's the bad algorithm because you're not going to spend any time over the canyon

1412
01:29:29,960 --> 01:29:32,110
sampling from the posterior

1413
01:29:32,130 --> 01:29:33,800
swim around the lake

1414
01:29:33,820 --> 01:29:36,630
and sample uniformly from

1415
01:29:36,650 --> 01:29:40,800
the volume and the like and then see how deep you optimize the volume within

1416
01:29:40,800 --> 01:29:45,030
these massive deep canyons they'll be in these and you can see how deep they

1417
01:29:46,070 --> 01:29:48,490
but in order to know the volume of the like you know you need to

1418
01:29:48,490 --> 01:29:49,860
know how deep

1419
01:29:49,880 --> 01:29:53,820
the can table you need to know that sort of cross sectional area of hyperbolic

1420
01:29:53,960 --> 01:29:58,380
is very difficult to estimate hyperbolic instance breakpoint

1421
01:29:58,420 --> 01:30:02,880
so neither of these algorithms give you the information you need

1422
01:30:02,900 --> 01:30:06,590
so this is an example of a problem where

1423
01:30:06,590 --> 01:30:10,210
just doing MCMC isn't the answer to the question like it is an expectation that

1424
01:30:10,210 --> 01:30:11,300
is not

1425
01:30:11,320 --> 01:30:13,650
usefully obvious expectation that we know

1426
01:30:13,670 --> 01:30:17,960
how what sensible distribution to sample from

1427
01:30:18,050 --> 01:30:25,710
and there are several ways around this problem and i have

1428
01:30:25,760 --> 01:30:28,920
several references i've got a section references at the end of the slide switch on

1429
01:30:28,920 --> 01:30:31,920
real generative processes out there in the world

1430
01:30:31,930 --> 01:30:36,660
we should assume that they come from something simple because then will definitely be wrong

1431
01:30:38,030 --> 01:30:42,850
essentially we are not to limit the complexity of our model a priori

1432
01:30:43,240 --> 01:30:47,200
since we don't believe that the real data was actually generated from a statistical model

1433
01:30:47,200 --> 01:30:50,490
with a small number of parameters right

1434
01:30:50,500 --> 01:30:54,500
so therefore regardless of how much training data we have we should consider models with

1435
01:30:54,500 --> 01:30:58,480
as many parameters as we can handle computationally

1436
01:30:59,640 --> 01:31:03,960
we all have to do the selecting of model order we don't have to say

1437
01:31:04,010 --> 01:31:08,710
all rights use five gaussians are six gaussians and my mixture model

1438
01:31:08,720 --> 01:31:13,660
in fact we can have infinitely many of you know the parameters if we want

1439
01:31:13,780 --> 01:31:17,200
and the key thing is that we can have infinitely many parameters as long as

1440
01:31:17,200 --> 01:31:18,870
we don't optimize them

1441
01:31:18,920 --> 01:31:20,340
we optimize the

1442
01:31:22,370 --> 01:31:23,430
we're going to

1443
01:31:23,840 --> 01:31:26,070
overfit the data

1444
01:31:26,870 --> 01:31:30,760
the kinds of models that i'm going to to talk about these nonparametric models including

1445
01:31:30,760 --> 01:31:36,410
gassing processes is shown about talk about our models with an infinite number of parameters

1446
01:31:41,620 --> 01:31:44,850
there are two ways of understanding guassian process

1447
01:31:46,910 --> 01:31:51,610
so the first way starts from a multivariate gaussians

1448
01:31:51,620 --> 01:32:00,240
and the second we start from linear regression so then we try to explain

1449
01:32:04,800 --> 01:32:06,730
let's start from

1450
01:32:06,750 --> 01:32:10,270
dallas in the solution OK so this is it's the univariate gas in this mission

1451
01:32:10,270 --> 01:32:12,180
was zero mean right

1452
01:32:12,190 --> 01:32:15,380
this is the multivariate gaussians density

1453
01:32:15,560 --> 01:32:21,720
it's t is the vector with elements t one t and

1454
01:32:21,810 --> 01:32:23,980
OK with zero mean again

1455
01:32:24,030 --> 01:32:27,990
now let's look at this expression for multivariate gaussians density

1456
01:32:28,000 --> 01:32:31,210
look in particular at this matrix sigma

1457
01:32:32,010 --> 01:32:33,250
so sigma

1458
01:32:34,140 --> 01:32:38,300
and n by n covariance matrix

1459
01:32:39,340 --> 01:32:44,250
imagine the following imagine it

1460
01:32:44,300 --> 01:32:46,410
sigma i j

1461
01:32:46,470 --> 01:32:47,980
depended on

1462
01:32:49,660 --> 01:32:51,350
on i and j

1463
01:32:52,110 --> 01:32:53,780
so imagine that

1464
01:32:54,930 --> 01:32:57,670
if i and j are close to each other

1465
01:32:57,680 --> 01:32:59,870
then sigma i j is large

1466
01:32:59,880 --> 01:33:00,950
these areas

1467
01:33:00,960 --> 01:33:03,240
a lot of covariance between

1468
01:33:03,320 --> 01:33:04,380
t i

1469
01:33:05,420 --> 01:33:07,230
i pos one let's say

1470
01:33:08,190 --> 01:33:13,180
there's less imagine if there's less covariance between t i and t i was ten

1471
01:33:16,330 --> 01:33:19,720
if we were to do this

1472
01:33:19,770 --> 01:33:21,360
uh no better

1473
01:33:21,410 --> 01:33:23,940
this is probably not going to work

1474
01:33:24,810 --> 01:33:55,530
OK with high probability this is not going to work so don't care of

1475
01:33:55,590 --> 01:34:13,830
what happens

1476
01:34:14,260 --> 01:34:16,280
forget it does not work

1477
01:34:29,520 --> 01:34:30,870
very sorry about this

1478
01:34:33,620 --> 01:34:36,250
show picture i think that picture on it

1479
01:34:36,300 --> 01:34:38,770
slide later on day

1480
01:34:38,790 --> 01:34:40,800
so here's

1481
01:34:42,460 --> 01:34:45,790
imagine that t i

1482
01:34:45,840 --> 01:34:48,590
t one through t one hundred

1483
01:34:48,600 --> 01:34:50,980
we're were lined along here

1484
01:34:54,140 --> 01:34:58,520
so i imagine that the index one three hundred was line along here and the

1485
01:34:58,520 --> 01:35:00,990
values plotted here where t one

1486
01:35:01,120 --> 01:35:03,710
thirty one hundred now if there

1487
01:35:03,720 --> 01:35:08,440
these four samples from that multivariate gaussians distribution

1488
01:35:08,690 --> 01:35:13,010
what you see is that nearby samples are very close to each other

1489
01:35:13,790 --> 01:35:16,850
nearby samples are very close to each other

1490
01:35:17,100 --> 01:35:22,540
nearby points t in input space are are very similar to each other but far

1491
01:35:22,540 --> 01:35:23,720
away points

1492
01:35:23,760 --> 01:35:25,920
are less correlated

1493
01:35:35,570 --> 01:35:39,900
in the gaussianprocess what you assume is that the covariance

1494
01:35:41,260 --> 01:35:46,260
sigma depends on some say the covariance between t and t j

1495
01:35:46,270 --> 01:35:47,610
depends on some

1496
01:35:47,620 --> 01:35:50,240
input variables

1497
01:35:50,290 --> 01:35:53,820
which here we're calling i and j but in general could be

1498
01:35:53,850 --> 01:35:56,210
indexed by space for example

1499
01:35:56,220 --> 01:36:00,960
so two points that are close to each other in space have the same value

1500
01:36:00,960 --> 01:36:01,660
of t

1501
01:36:01,700 --> 01:36:03,450
a similar values of t

1502
01:36:03,460 --> 01:36:06,560
two points that are far away and have quite different values of t

1503
01:36:07,200 --> 01:36:14,400
essentially it's infinite dimensional generalization of the multivariate gaussians where instead of this finite index

1504
01:36:15,420 --> 01:36:18,400
of integers one two and i now

1505
01:36:18,410 --> 01:36:20,420
indexed this by space

1506
01:36:20,440 --> 01:36:23,820
call it acts OK

1507
01:36:23,830 --> 01:36:27,960
and instead of the covariance function which is the so instead of the covariance matrix

1508
01:36:27,960 --> 01:36:30,330
need is a domain

1509
01:36:30,350 --> 01:36:34,730
it must be a nonempty domain otherwise it doesn't make much sense we call this

1510
01:36:35,690 --> 01:36:44,920
delta delta four domain and it is associated with a particular interpretation i and then

1511
01:36:44,940 --> 01:36:50,730
we need a function that draws the diagrams a function that gives us the set

1512
01:36:50,730 --> 01:36:53,430
associated with the

1513
01:36:53,450 --> 01:36:55,090
names always the

1514
01:36:55,120 --> 01:37:00,310
concept descriptions that we might come up with using some nice syntax and so this

1515
01:37:00,310 --> 01:37:06,720
is usually written as this take something here and interpreted so we write it

1516
01:37:06,730 --> 01:37:07,590
this way

1517
01:37:11,830 --> 01:37:14,720
and as we said our intention is to

1518
01:37:14,720 --> 01:37:20,520
map the names to some subset of the domain i think you remember the diagrams

1519
01:37:20,520 --> 01:37:21,910
take the domains

1520
01:37:23,060 --> 01:37:26,040
the name so a subset of this domain

1521
01:37:26,050 --> 01:37:28,520
and four

1522
01:37:28,520 --> 01:37:34,330
role descriptions we are not talking about single object but pairs of objects

1523
01:37:35,730 --> 01:37:38,480
considering plans in the works for

1524
01:37:39,470 --> 01:37:40,870
if we

1525
01:37:40,930 --> 01:37:45,010
i want to draw one to draw the diagram actually it's the a set of

1526
01:37:45,010 --> 01:37:47,020
pairs i don't i didn't

1527
01:37:47,050 --> 01:37:52,090
daud here but i think it's obvious so it's easy to understand that a particle

1528
01:37:53,120 --> 01:37:58,300
role description should be a subset of the cross product of the domain with itself

1529
01:37:58,310 --> 01:38:03,600
pairs of a set of pairs of objects that's pretty easy to understand now let

1530
01:38:03,600 --> 01:38:05,330
me just summarised

1531
01:38:05,350 --> 01:38:07,960
the idea about the

1532
01:38:07,960 --> 01:38:10,550
operators here so the

1533
01:38:10,620 --> 01:38:12,860
conjunction operator

1534
01:38:12,910 --> 01:38:15,500
it is then

1535
01:38:16,250 --> 01:38:23,070
the semantics of the conjunction operator is defined with respect to set intersection

1536
01:38:23,090 --> 01:38:28,940
the disjunction is defined with respect to union this is exactly what we have seen

1537
01:38:28,990 --> 01:38:34,770
in the drawings before complement is an interesting thing if i have the

1538
01:38:34,770 --> 01:38:37,100
the main here and

1539
01:38:37,120 --> 01:38:38,530
these are the set

1540
01:38:38,540 --> 01:38:39,170
this is

1541
01:38:39,170 --> 01:38:47,120
these are the students a then the complement of the student is all the rest

1542
01:38:47,140 --> 01:38:51,250
and if we have a very complex concept descriptions then

1543
01:38:51,280 --> 01:38:53,310
we can apply the same modelling

1544
01:38:54,210 --> 01:38:56,200
this is very

1545
01:38:56,210 --> 01:39:02,160
interesting there are restricted languages that don't allow you to do this

1546
01:39:02,210 --> 01:39:07,710
then you sacrifice modeling power for the same computational properties

1547
01:39:08,380 --> 01:39:12,430
turnout inference problem to be less complex

1548
01:39:12,450 --> 01:39:16,850
this looks a little bit strange for instance we discussed some

1549
01:39:16,880 --> 01:39:18,020
works for

1550
01:39:18,040 --> 01:39:23,920
organization this can be an arbitrary complex concept descriptions it could write down using your

1551
01:39:23,920 --> 01:39:29,360
favourite language this is the schema here then a set of domain objects for which

1552
01:39:29,360 --> 01:39:33,600
there exists something on the right-hand side and this thing on the right hand side

1553
01:39:33,600 --> 01:39:38,360
must satisfy certain restrictions

1554
01:39:38,370 --> 01:39:42,850
and this was the same for all very different different different

1555
01:39:46,390 --> 01:39:48,790
any question about the

1556
01:39:48,840 --> 01:39:50,020
i mean

1557
01:39:50,020 --> 01:39:51,980
it's not really necessary to

1558
01:39:53,020 --> 01:39:58,960
really understand all this mafia i i think i have drawn diagrams and you got

1559
01:39:58,970 --> 01:39:59,810
the idea

1560
01:39:59,910 --> 01:40:02,660
four four in some

1561
01:40:02,680 --> 01:40:04,340
cases you want to say

1562
01:40:04,480 --> 01:40:10,460
that a particular professor

1563
01:40:10,500 --> 01:40:13,590
must not teach two courses or

1564
01:40:13,610 --> 01:40:19,630
some professor usually a younger professor must teach at least three courses otherwise something is

1565
01:40:19,630 --> 01:40:27,370
wrong by research prof or know the professor can teach at most one cause obviously

1566
01:40:27,370 --> 01:40:29,270
right so

1567
01:40:29,290 --> 01:40:34,910
clear that one probably wants to restrict the set of all objects that one might

1568
01:40:34,910 --> 01:40:40,510
find for one particular object on the right hand side so students must work at

1569
01:40:40,510 --> 01:40:44,540
at least four four companies in these days

1570
01:40:44,570 --> 01:40:49,670
o five was one company if one has strict

1571
01:40:49,680 --> 01:40:54,260
the government

1572
01:40:55,060 --> 01:41:00,960
it's obvious that there is the need to introduce new concept forming operators that

1573
01:41:00,970 --> 01:41:06,840
we can use them to describe these sets in this of secure language

1574
01:41:06,870 --> 01:41:09,550
we can do this using these

1575
01:41:09,560 --> 01:41:15,840
access these at least restrictions and this is the qualification you could say

1576
01:41:15,850 --> 01:41:19,310
works for at least

1577
01:41:22,060 --> 01:41:24,980
o for at most one

1578
01:41:25,020 --> 01:41:26,200
this is the number

1579
01:41:32,220 --> 01:41:33,450
operators long

1580
01:41:33,460 --> 01:41:35,470
known in description logics

1581
01:41:35,520 --> 01:41:40,050
but has only recently been added to the old two standard

1582
01:41:40,070 --> 01:41:45,800
but it's now it's very very nice that this was also adopted

1583
01:41:45,940 --> 01:41:54,390
i talked about interpretations i talked about this relational structures that we might want to

1584
01:41:54,390 --> 01:41:57,990
define now we can have a look at the particle or structure

1585
01:41:58,030 --> 01:42:00,670
we can

1586
01:42:00,720 --> 01:42:03,390
take some particular object

1587
01:42:03,410 --> 01:42:06,320
of a domain let's take

1588
01:42:06,360 --> 01:42:08,280
object that i call p one

1589
01:42:08,290 --> 01:42:10,330
let's take object p two

1590
01:42:10,340 --> 01:42:11,430
p three

1591
01:42:11,430 --> 01:42:13,780
p four maybe

1592
01:42:13,800 --> 01:42:15,020
and then

1593
01:42:15,030 --> 01:42:20,950
we might want to talk about courses there's the description logic course database course maybe

1594
01:42:20,950 --> 01:42:26,060
a semantic web course so these are objects of the domain

1595
01:42:26,080 --> 01:42:30,370
i gave them the name but you can think of these circles here is representing

1596
01:42:30,370 --> 01:42:34,740
objects of the domain and we can see that those objects into relation to one

1597
01:42:34,740 --> 01:42:36,040
another so

1598
01:42:36,040 --> 01:42:36,850
we say

1599
01:42:36,850 --> 01:42:40,080
the relation takescourse consists of this

1600
01:42:40,080 --> 01:42:41,490
o p one

1601
01:42:44,510 --> 01:42:46,040
p two p l

1602
01:42:46,040 --> 01:42:48,720
p two d p three seven

1603
01:42:48,850 --> 01:42:53,660
as you see i write down

1604
01:42:53,660 --> 01:42:54,690
the variance

1605
01:42:54,700 --> 01:42:57,790
on our dependent variable is pretty large

1606
01:42:57,820 --> 01:43:00,780
there are other values

1607
01:43:00,790 --> 01:43:02,040
so here

1608
01:43:02,100 --> 01:43:03,240
but the variance

1609
01:43:03,250 --> 01:43:06,690
it's pretty small

1610
01:43:06,700 --> 01:43:10,710
this is annoying as well

1611
01:43:10,790 --> 01:43:13,240
another issue

1612
01:43:13,250 --> 01:43:16,080
let's take this area here for the

1613
01:43:16,090 --> 01:43:18,790
independent variable

1614
01:43:18,820 --> 01:43:20,650
and this look at this data

1615
01:43:24,050 --> 01:43:26,490
and others assume the difference between cone

1616
01:43:26,540 --> 01:43:31,120
it is zero and forty percent should be that large

1617
01:43:31,190 --> 01:43:36,440
but as have been sitting in this room heating up before we came

1618
01:43:36,490 --> 01:43:41,550
you may remember that normally you talked about

1619
01:43:41,570 --> 01:43:42,470
he CA

1620
01:43:43,470 --> 01:43:45,820
there's notion of fuzzy sets

1621
01:43:45,870 --> 01:43:48,360
in fuzzy sets terms

1622
01:43:48,440 --> 01:43:52,040
talk about replacing rate we could argue in place and rate

1623
01:43:52,050 --> 01:43:55,120
which is that low point four

1624
01:43:55,170 --> 01:43:58,150
if you got only forty percent of former salary

1625
01:43:58,240 --> 01:44:02,950
you're probably quite eager to get the job in this area

1626
01:44:02,970 --> 01:44:05,800
in this sector

1627
01:44:06,440 --> 01:44:10,710
we could argue is the differences here are that large

1628
01:44:10,760 --> 01:44:13,050
music playing over here

1629
01:44:13,130 --> 01:44:15,630
where it it makes a difference whether you're

1630
01:44:15,690 --> 01:44:17,550
above or below

1631
01:44:17,630 --> 01:44:20,740
alternative wage

1632
01:44:21,020 --> 01:44:22,870
and if we take this

1633
01:44:23,790 --> 01:44:25,780
one part of

1634
01:44:27,120 --> 01:44:31,470
of a replacement rates across countries

1635
01:44:31,480 --> 01:44:34,870
and we see something that we call the wedge

1636
01:44:39,980 --> 01:44:41,360
has this form

1637
01:44:41,490 --> 01:44:43,590
so should thing here

1638
01:44:43,600 --> 01:44:45,490
which shape form

1639
01:44:45,540 --> 01:44:50,240
would be something like this

1640
01:44:50,270 --> 01:44:53,020
so variations here

1641
01:44:53,080 --> 01:44:55,250
this area is much larger

1642
01:44:55,250 --> 01:44:56,800
on the dependent variable

1643
01:44:56,820 --> 01:45:03,200
then variation in this area

1644
01:45:03,260 --> 01:45:04,670
which shape

1645
01:45:04,690 --> 01:45:08,150
another alarming signals

1646
01:45:08,170 --> 01:45:09,480
it tells us

1647
01:45:09,510 --> 01:45:10,750
at least

1648
01:45:10,800 --> 01:45:13,260
we cannot answer for our

1649
01:45:13,290 --> 01:45:16,760
significance tests

1650
01:45:16,850 --> 01:45:18,620
and at worst it tells us

1651
01:45:18,630 --> 01:45:27,550
the model specification is utterly wrong

1652
01:45:27,630 --> 01:45:30,150
so the straight line here

1653
01:45:30,210 --> 01:45:31,740
a nice summary

1654
01:45:31,780 --> 01:45:39,510
with artfully painted it into data about are standard for us

1655
01:45:41,820 --> 01:45:45,910
it summarizes something indeed if you look at the big picture

1656
01:45:45,970 --> 01:45:47,990
then we can say

1657
01:45:48,020 --> 01:45:51,870
for low rates of the replacement rate indeed on average

1658
01:45:51,920 --> 01:45:54,490
employment seems to be lower

1659
01:45:54,530 --> 01:45:56,020
for higher

1660
01:45:56,030 --> 01:45:57,700
levels of

1661
01:45:57,720 --> 01:45:59,570
the replacement rate

1662
01:46:00,990 --> 01:46:03,600
employment in the private sector consumer services

1663
01:46:03,630 --> 01:46:04,350
is low

1664
01:46:06,870 --> 01:46:08,580
in general

1665
01:46:08,620 --> 01:46:12,440
the association that we expect it

1666
01:46:12,490 --> 01:46:14,250
can be seen

1667
01:46:14,320 --> 01:46:16,070
but there are lots of

1668
01:46:18,250 --> 01:46:19,870
it's about that

1669
01:46:19,880 --> 01:46:23,500
we'll talk during the next two weeks

1670
01:46:23,550 --> 01:46:24,770
because this is simple

1671
01:46:24,780 --> 01:46:27,190
you will be able to draw this picture

1672
01:46:27,240 --> 01:46:31,140
by the end of tomorrow

1673
01:46:31,580 --> 01:46:34,710
most people go home or good party

1674
01:46:37,250 --> 01:46:39,410
but i will recommend to stay on the wow

1675
01:46:39,490 --> 01:46:41,380
and two

1676
01:46:41,390 --> 01:46:46,130
also look into those bits that sketched

1677
01:46:46,140 --> 01:46:49,090
to make

1678
01:46:49,130 --> 01:46:51,600
better use of what you're doing

1679
01:46:52,370 --> 01:46:54,490
the data

1680
01:46:54,500 --> 01:46:59,840
so let's go back to the art of summarizing relationships

1681
01:46:59,850 --> 01:47:01,610
this is

1682
01:47:01,620 --> 01:47:06,920
what you will be able to run in our tomorrow

1683
01:47:07,030 --> 01:47:08,540
we have a relationship

1684
01:47:09,750 --> 01:47:10,950
it's related

1685
01:47:10,970 --> 01:47:13,950
to x

1686
01:47:13,990 --> 01:47:15,670
these are vectors

1687
01:47:15,750 --> 01:47:18,620
so we have here

1688
01:47:18,670 --> 01:47:21,350
a set of values for

1689
01:47:21,390 --> 01:47:25,190
a dependent variable

1690
01:47:25,200 --> 01:47:30,670
in our case it was employment private consumer services

1691
01:47:30,790 --> 01:47:33,650
and here this is a matrix

1692
01:47:33,660 --> 01:47:34,780
which means

1693
01:47:34,790 --> 01:47:36,480
a set of vectors

1694
01:47:36,530 --> 01:47:38,040
each representing

1695
01:47:38,050 --> 01:47:39,700
the independent variable

1696
01:47:39,720 --> 01:47:41,710
the previous picture

1697
01:47:41,720 --> 01:47:45,220
i've had one independent variable the replacement rate

1698
01:47:45,260 --> 01:47:48,050
things get nasty as soon as more than one

1699
01:47:48,120 --> 01:47:51,420
independent variable to draw

1700
01:47:51,820 --> 01:47:56,150
however this is nice representation

1701
01:47:56,200 --> 01:47:57,400
so here it is

1702
01:47:57,410 --> 01:48:01,750
o thing you must consider as a data matrix with parables

1703
01:48:03,260 --> 01:48:04,230
the columns

1704
01:48:05,140 --> 01:48:08,320
the names of the

1705
01:48:08,380 --> 01:48:09,860
unit of analysis

1706
01:48:09,870 --> 01:48:12,250
in the rows

1707
01:48:12,280 --> 01:48:14,240
here we have a vector

1708
01:48:14,370 --> 01:48:16,250
of coefficients

1709
01:48:16,280 --> 01:48:17,920
telling us

1710
01:48:18,010 --> 01:48:20,130
but how many steps

1711
01:48:20,140 --> 01:48:23,450
the dependent variable will change

1712
01:48:23,500 --> 01:48:26,300
and also to what direction

1713
01:48:27,420 --> 01:48:32,870
we move one step on our independent variable

1714
01:48:34,090 --> 01:48:36,920
got the residual term

1715
01:48:36,980 --> 01:48:42,490
in which all the noise nuisance is hidden

1716
01:48:42,500 --> 01:48:45,370
all the rest

1717
01:48:45,380 --> 01:48:47,020
and the assumption

1718
01:48:47,070 --> 01:48:48,770
otherwise we can not

1719
01:48:48,840 --> 01:48:52,450
calculate these coefficients

1720
01:48:52,460 --> 01:48:56,090
this is all well behaved i'll tell you what will be a means in a

1721
01:48:56,090 --> 01:48:59,530
few minutes

1722
01:48:59,550 --> 01:49:02,210
now with some algebraic

1723
01:49:06,330 --> 01:49:08,890
you get after a while

1724
01:49:08,890 --> 01:49:10,480
this expression

1725
01:49:10,480 --> 01:49:11,650
of the two

1726
01:49:11,670 --> 01:49:13,110
divided by design

1727
01:49:13,130 --> 01:49:15,340
of the of two

1728
01:49:15,390 --> 01:49:17,670
now i square

1729
01:49:18,480 --> 01:49:20,990
i have will greatly increase

1730
01:49:21,050 --> 01:49:23,130
so what you see now is

1731
01:49:23,170 --> 01:49:24,820
that's little d

1732
01:49:24,820 --> 01:49:27,840
it is always larger than rapidly

1733
01:49:27,840 --> 01:49:30,380
you're going to see that the maxima

1734
01:49:30,400 --> 01:49:31,840
o which comes from this

1735
01:49:31,960 --> 01:49:36,050
equation i being modulated by this one

1736
01:49:36,070 --> 01:49:36,920
and so

1737
01:49:36,940 --> 01:49:38,130
if this were

1738
01:49:39,420 --> 01:49:43,820
by capital the was the opening each individual group

1739
01:49:43,880 --> 01:49:48,170
and if the grading one-fifth of course is zero or maximum always here

1740
01:49:48,210 --> 01:49:51,650
and if the first or maximum of the degrading fall here

1741
01:49:51,650 --> 01:49:54,920
and the second order one full year and the third here

1742
01:49:54,960 --> 01:49:57,300
and this is the price you pay

1743
01:49:57,320 --> 01:49:58,300
before the fact

1744
01:49:58,320 --> 01:50:03,690
that these groups have an opening so you see and modulation in the strength

1745
01:50:03,740 --> 01:50:04,740
of your

1746
01:50:04,780 --> 01:50:09,740
first second third fourth order and so on

1747
01:50:09,780 --> 01:50:12,670
so when you look carefully at the grading

1748
01:50:12,720 --> 01:50:14,110
lights and our

1749
01:50:14,150 --> 01:50:18,070
demonstrate that the you they are not all

1750
01:50:18,090 --> 01:50:21,900
sixteen times i zero if you have any calls for and if you have any

1751
01:50:21,900 --> 01:50:24,720
equal thousand not all a million times

1752
01:50:24,740 --> 01:50:25,720
i zero

1753
01:50:25,740 --> 01:50:26,900
but they have this

1754
01:50:26,900 --> 01:50:28,880
overall envelope

1755
01:50:28,940 --> 01:50:31,170
which modulated and that's the result

1756
01:50:31,220 --> 01:50:36,340
of the opening finite opening of the growth

1757
01:50:37,300 --> 01:50:40,630
they could to make sure that you understand the difference between the

1758
01:50:40,630 --> 01:50:42,490
and the two days

1759
01:50:42,590 --> 01:50:48,900
this is my great

1760
01:50:48,920 --> 01:50:51,860
and this is the open area

1761
01:50:51,900 --> 01:50:52,760
this is

1762
01:50:53,300 --> 01:50:55,780
when the light can not go through

1763
01:50:55,780 --> 01:50:57,210
and the definition

1764
01:50:57,280 --> 01:50:59,340
of these this

1765
01:50:59,380 --> 01:51:02,690
and the definition of this the

1766
01:51:02,800 --> 01:51:05,340
that the shows up in here

1767
01:51:05,340 --> 01:51:08,880
and this capital these shows up

1768
01:51:08,900 --> 01:51:12,690
so this is the

1769
01:51:12,690 --> 01:51:15,280
a single slit diffraction

1770
01:51:15,280 --> 01:51:17,960
this is the most puzzling interference

1771
01:51:18,010 --> 01:51:19,300
there you see again

1772
01:51:19,300 --> 01:51:21,360
we make the distinction in water

1773
01:51:21,360 --> 01:51:22,400
but that has no

1774
01:51:22,420 --> 01:51:26,490
meaning because it's all the fraction of course

1775
01:51:26,630 --> 01:51:28,530
somehow be

1776
01:51:28,550 --> 01:51:33,550
were approximately divided by five and it just so happens we have grading

1777
01:51:33,570 --> 01:51:36,070
for with that is the case

1778
01:51:37,110 --> 01:51:38,240
the first

1779
01:51:38,260 --> 01:51:40,530
order maximum of the great

1780
01:51:41,880 --> 01:51:42,990
it's going to be

1781
01:51:44,320 --> 01:51:45,820
because that is

1782
01:51:45,860 --> 01:51:48,420
when this function becomes zero

1783
01:51:48,480 --> 01:51:50,240
so you will see that

1784
01:51:50,280 --> 01:51:54,490
zero order one two three four five will be killed here and then they would

1785
01:51:54,490 --> 01:51:59,030
build up a little again and then ultimately of course they will peter out

1786
01:51:59,070 --> 01:52:00,820
so if i show you is

1787
01:52:00,880 --> 01:52:03,130
a spectrum of great

1788
01:52:03,190 --> 01:52:08,690
you can actually roughly estimate what the ratio capital be of little these by seeing

1789
01:52:09,630 --> 01:52:13,190
sure that modulation

1790
01:52:13,210 --> 01:52:15,820
and so that's what i want you to see now

1791
01:52:15,860 --> 01:52:17,460
it's not so exciting

1792
01:52:17,460 --> 01:52:20,760
many of you were observed may have seen any

1793
01:52:20,760 --> 01:52:25,240
because it was every time there even when i showed my own

1794
01:52:25,300 --> 01:52:27,630
so we're going to make it

1795
01:52:27,670 --> 01:52:29,710
quite dark for this

1796
01:52:29,780 --> 01:52:32,170
i know this is the wrong wrongs which

1797
01:52:32,190 --> 01:52:36,110
so many switches here there we go

1798
01:52:36,150 --> 01:52:39,050
this is a great thing that i purposely offset

1799
01:52:39,980 --> 01:52:42,320
i purposely offset sort of here is the

1800
01:52:42,400 --> 01:52:43,840
zero order

1801
01:52:44,440 --> 01:52:45,280
make sure

1802
01:52:45,530 --> 01:52:49,130
this is the zero order i think that well actually

1803
01:52:49,210 --> 01:52:52,130
test when my zero it was

1804
01:52:52,300 --> 01:52:53,760
this is the zero order

1805
01:52:53,820 --> 01:52:56,090
we aim exactly here

1806
01:52:56,170 --> 01:52:57,710
and so this is the first order

1807
01:52:57,720 --> 01:53:00,490
second order third or fourth order

1808
01:53:00,490 --> 01:53:02,840
look at this soccer almost gone

1809
01:53:02,880 --> 01:53:06,320
that is the result of the fact the single slip into

1810
01:53:06,340 --> 01:53:09,800
and then it comes up again and the reason why it comes up again

1811
01:53:09,860 --> 01:53:12,920
because now you enter this little many

1812
01:53:13,900 --> 01:53:17,340
in the singles that's so you see here quite well sometimes

1813
01:53:17,360 --> 01:53:19,720
grading you can see remarkably well

1814
01:53:19,740 --> 01:53:20,800
all the time

1815
01:53:20,820 --> 01:53:23,490
it is hard to see it depends of course on

1816
01:53:23,650 --> 01:53:24,710
how many

1817
01:53:24,740 --> 01:53:26,550
maximize you have

1818
01:53:26,760 --> 01:53:29,190
you seem quite well the modulation

1819
01:53:29,300 --> 01:53:32,460
see comes up here again in this little point here

1820
01:53:32,510 --> 01:53:37,710
would then be somewhere here in this axis

1821
01:53:37,720 --> 01:53:42,400
so this is then the complete equation that combines single slit diffraction

1822
01:53:42,440 --> 01:53:43,820
with multiple

1823
01:53:43,840 --> 01:53:48,590
slit interference

1824
01:53:48,610 --> 01:53:50,670
if we change

1825
01:53:50,670 --> 01:53:52,650
the single opening

1826
01:53:52,690 --> 01:53:54,880
from a senate

1827
01:53:54,900 --> 01:53:57,820
there was circles

1828
01:53:57,880 --> 01:54:00,910
circle you policies circle

1829
01:54:01,000 --> 01:54:05,830
then very little changes except of course if you have a circular opening

1830
01:54:05,840 --> 01:54:08,410
everything is now actual symmetry

1831
01:54:08,410 --> 01:54:10,510
so you get circle

1832
01:54:10,530 --> 01:54:13,170
the thing become circles

1833
01:54:13,190 --> 01:54:14,380
and then

1834
01:54:14,400 --> 01:54:15,740
which is not so obvious

1835
01:54:15,760 --> 01:54:16,840
that is that

1836
01:54:16,860 --> 01:54:21,550
this minimum doesn't for of land over two in terms of angular dimensions with one

1837
01:54:21,550 --> 01:54:24,190
point two times level to

1838
01:54:24,200 --> 01:54:25,910
and if you want to use one two

1839
01:54:25,950 --> 01:54:29,440
for an approximation that is fine enough so it's a little larger

1840
01:54:29,440 --> 01:54:33,600
for all perpetrates but this very rigid model

1841
01:54:33,620 --> 01:54:37,000
one of two alternatives does not hold either

1842
01:54:37,020 --> 01:54:38,730
so let's take a look at the real data

1843
01:54:38,780 --> 01:54:42,300
the data comes from thomas hunt morgan

1844
01:54:42,370 --> 01:54:47,880
a developmental biologist to be eventually became one of the great geneticists of the century

1845
01:54:48,980 --> 01:54:54,420
columbia university studying fruit flies and he studied for flies rather than peas

1846
01:54:54,470 --> 01:54:58,730
there many good reasons why makes sense the flies than p is

1847
01:55:00,730 --> 01:55:03,470
as four chromosomes is the seven

1848
01:55:03,480 --> 01:55:05,480
o four seven

1849
01:55:05,490 --> 01:55:12,710
anybody been to columbia university

1850
01:55:13,170 --> 01:55:14,810
i mean it's really

1851
01:55:17,840 --> 01:55:22,360
also what else is wrong with studying these

1852
01:55:22,380 --> 01:55:25,960
they take too long how many generations piece you can get a year and half

1853
01:55:25,970 --> 01:55:27,310
so many

1854
01:55:27,440 --> 01:55:29,950
from flies how long did it take

1855
01:55:30,000 --> 01:55:33,330
a couple weeks ago get a generation every couple weeks

1856
01:55:33,420 --> 01:55:36,470
if you actually want to write some papers i mean if you have a day

1857
01:55:36,470 --> 01:55:39,980
job is among you can you can do these p things that take a long

1858
01:55:40,640 --> 01:55:43,470
but for example if you're trying to get tenure at columbia

1859
01:55:43,530 --> 01:55:47,490
you might want to actually do something that you could get a couple generations every

1860
01:55:47,490 --> 01:55:50,930
month or something like that so the fruit fly was much better they also you

1861
01:55:50,930 --> 01:55:53,960
know they don't take feels and things you grow them in little files

1862
01:55:55,720 --> 01:55:56,620
with some

1863
01:55:56,660 --> 01:56:01,700
food at the bottom these are medium at the bottom and little cotton stopper at

1864
01:56:01,700 --> 01:56:05,860
the top and very quickly you can grow zillions and zillions of fruit flies to

1865
01:56:05,860 --> 01:56:06,370
the the

1866
01:56:06,380 --> 01:56:10,210
the flight was chosen easy short generation time et cetera et cetera and there are

1867
01:56:10,210 --> 01:56:15,880
a lot of natural variations other geneticists love to choose organisms that is easy to

1868
01:56:15,880 --> 01:56:17,850
work with you can do a lot of work

1869
01:56:17,970 --> 01:56:23,760
so and provide four chromosomes but i do so any calls for that four pairs

1870
01:56:23,760 --> 01:56:25,390
of chromosomes

1871
01:56:28,770 --> 01:56:30,380
so he said across

1872
01:56:30,560 --> 01:56:35,760
the have zero cross was between a normal flight and the way we see norman

1873
01:56:35,760 --> 01:56:37,660
genetics is wild type

1874
01:56:37,670 --> 01:56:42,880
wild type that is the type in the wild it actually doesn't mean that it

1875
01:56:42,880 --> 01:56:46,530
is the type in the wild it means it's whatever type the geneticist was chosen

1876
01:56:46,530 --> 01:56:51,320
as his or reference train but it's called wild type and he said across

1877
01:56:51,340 --> 01:56:55,850
between the wild type fly by flight had two interesting properties its body

1878
01:56:55,860 --> 01:56:57,630
was black

1879
01:56:57,640 --> 01:56:59,780
and its wings

1880
01:56:59,790 --> 01:57:05,430
were in bad shape and they were called vestigial the funny little when things that

1881
01:57:05,430 --> 01:57:11,290
weren't warned that didn't work and grown outright et cetera so instead of the normal

1882
01:57:11,290 --> 01:57:15,360
flight body color which is kind ten around the middle it was black all around

1883
01:57:15,360 --> 01:57:21,350
its middle and its wings were very short the hypothesis is that there genes controlling

1884
01:57:21,350 --> 01:57:26,480
in fact by demonstrating mendelian inheritance black was the single mendelian trait which was recessive

1885
01:57:26,500 --> 01:57:30,160
to the normal body color as the gill was the single mendelian trait which is

1886
01:57:30,180 --> 01:57:38,500
recessive to the normal body shape and the genotype of wild type was homozygous

1887
01:57:38,560 --> 01:57:43,600
normal which already possible plus now geneticists actually prefer plus terms rather than big or

1888
01:57:43,600 --> 01:57:47,740
small plus over plus and will also a female

1889
01:57:47,750 --> 01:57:51,130
and we'll cross her to a male

1890
01:57:51,150 --> 01:57:52,920
who homozygous

1891
01:57:52,930 --> 01:57:57,670
for the gene that controls the body color there and this gene that controls wing

1892
01:57:59,230 --> 01:58:03,660
and we'll look at the offspring so

1893
01:58:03,680 --> 01:58:10,440
makes f one f one have what genotype there plus over black class over this

1894
01:58:10,470 --> 01:58:11,570
the jail

1895
01:58:11,690 --> 01:58:13,160
f one

1896
01:58:14,070 --> 01:58:15,140
so that

1897
01:58:15,160 --> 01:58:16,980
what he does is he takes

1898
01:58:16,990 --> 01:58:19,080
say these males

1899
01:58:19,140 --> 01:58:21,050
and he crosses them

1900
01:58:23,510 --> 01:58:31,560
these flies here that have the doubly recessive phenotype doing what we call a test

1901
01:58:32,550 --> 01:58:36,350
now the name of conclusion is more is known as a test cross when you

1902
01:58:36,420 --> 01:58:39,020
cross back to the home of zygote

1903
01:58:39,030 --> 01:58:42,280
for the recessive phenotype and

1904
01:58:42,320 --> 01:58:46,180
when he gets out the same exact picture i drew before but we're just getting

1905
01:58:46,180 --> 01:58:51,580
used the nomenclature and getting used to a slightly different nomenclatures here he could either

1906
01:58:52,230 --> 01:58:56,470
you always get blacklisted URL blacklisted job black stage or

1907
01:58:56,520 --> 01:58:57,900
black the job

1908
01:59:00,630 --> 01:59:04,070
parent on the right and here you can get plus plus

1909
01:59:04,220 --> 01:59:07,180
could get blacklisted joel he could get

1910
01:59:07,190 --> 01:59:09,050
black plus

1911
01:59:09,050 --> 01:59:12,730
or you could get plus stage all and

1912
01:59:12,780 --> 01:59:14,780
as we said over there

1913
01:59:14,780 --> 01:59:19,150
that as then doing this random changes i want to get better and better models

1914
01:59:19,150 --> 01:59:23,460
so one way to think of this is sort of like doing the simulated annealing

1915
01:59:23,460 --> 01:59:24,860
over all these things

1916
01:59:24,870 --> 01:59:26,190
you can think of it this way

1917
01:59:26,260 --> 01:59:31,400
and then you know with some probability of course you also accept sort of changes

1918
01:59:31,400 --> 01:59:34,710
that lead us to bend towards more in some sense

1919
01:59:34,830 --> 01:59:38,210
so that's the way how we can estimate the model

1920
01:59:38,280 --> 01:59:45,370
and what do the authors use this model for link prediction for three very small

1921
01:59:45,370 --> 01:59:51,580
networks where what what is shown here is the probability that the fraction of things

1922
01:59:51,580 --> 01:59:54,920
that i removed from the network so basically and

1923
01:59:55,350 --> 01:59:58,120
so basically or it is the

1924
01:59:58,130 --> 02:00:02,370
one minus the size of my training or it's the it's mind one minus that

1925
02:00:02,540 --> 02:00:05,120
is the size of my training data set where

1926
02:00:05,160 --> 02:00:08,710
here for example i'm pretty much removing almost all the links from the network and

1927
02:00:08,710 --> 02:00:11,000
i then i try to to predict them

1928
02:00:11,020 --> 02:00:16,820
and the red curve is this hierarchical method and then the other curves are sort

1929
02:00:16,820 --> 02:00:20,060
of competing methods this is the random

1930
02:00:20,080 --> 02:00:21,560
or chance

1931
02:00:21,570 --> 02:00:27,360
the evaluation is done by area under the curve measures so not comparing to random

1932
02:00:27,370 --> 02:00:28,440
and you can see

1933
02:00:28,490 --> 02:00:32,880
that this works toward this tends to work well for small terrorist network so i

1934
02:00:32,880 --> 02:00:37,820
think this is like nine eleven terrorist network also about it for example for the

1935
02:00:37,870 --> 02:00:39,350
metabolic network

1936
02:00:39,350 --> 02:00:43,980
what tends to work better is for example short spats so predict

1937
02:00:44,000 --> 02:00:48,830
link based on the on the distance of the shortest span between the nodes or

1938
02:00:48,870 --> 02:00:53,030
by the product of their degrees so this would be like a preferential attachment hypothesis

1939
02:00:53,030 --> 02:00:58,370
that this is the food web from some grassland echoes ecosystem where again this sort

1940
02:00:58,370 --> 02:01:02,700
of this hierarchical model tends to work really well

1941
02:01:05,440 --> 02:01:10,000
a different way to model graphs and also sort of to reason about links comes

1942
02:01:10,000 --> 02:01:14,940
more from the social sciences and this is called exponential random graphs are based on

1943
02:01:15,820 --> 02:01:19,160
and the idea is

1944
02:01:19,160 --> 02:01:20,810
somehow describes

1945
02:01:20,860 --> 02:01:23,400
the graph by r

1946
02:01:23,450 --> 02:01:29,580
by some kind of statistics or some kind of numerical somebody measures and then throw

1947
02:01:29,620 --> 02:01:31,900
throat a log linear model over that

1948
02:01:31,940 --> 02:01:36,110
and that's very nice for very useful for hypothesis testing

1949
02:01:36,120 --> 02:01:40,690
so what i mean by by that is the following a probability of missing graph

1950
02:01:40,690 --> 02:01:47,000
y is just some big nasty normalizing constant and these are my and then

1951
02:01:47,020 --> 02:01:52,450
the my linear building combination of my summary measures like this is my summary measures

1952
02:01:52,620 --> 02:01:56,920
times the parameters and what they want to learn is this but on this that

1953
02:01:56,940 --> 02:01:58,610
i OK

1954
02:01:58,620 --> 02:02:02,610
i can be a bit more concrete how can i designed this

1955
02:02:02,630 --> 02:02:08,090
summary measures this statistics you know i can say the the number of nodes the

1956
02:02:08,090 --> 02:02:09,190
number of edges

1957
02:02:09,240 --> 02:02:13,670
the number of open triads number of closed triads and so on so these are

1958
02:02:13,730 --> 02:02:17,740
these are the kind of kinds of summary statistics i can use

1959
02:02:18,240 --> 02:02:23,990
and yet the nice thing here is that i can condition this summary statistics based

1960
02:02:23,990 --> 02:02:28,860
on some kind of know that if see the like activity or individual's characteristics

1961
02:02:28,940 --> 02:02:34,000
so in this way then i can do hypothesis testing consumer whether certain types of

1962
02:02:34,000 --> 02:02:39,350
nodes are more likely you know to participate in such structures was as structures where

1963
02:02:39,350 --> 02:02:43,580
the edges are reciprocated or you know a particular sets of nodes more likely to

1964
02:02:43,580 --> 02:02:49,030
cause two to participating triangles or you know in open in open i

1965
02:02:49,080 --> 02:02:51,330
so that's basically the idea

1966
02:02:51,350 --> 02:02:55,650
so the first such models so here's an example is called the one and it's

1967
02:02:55,650 --> 02:03:00,350
on edge of diet independence model and what it says is the probability of of

1968
02:03:00,350 --> 02:03:04,060
seeing the graph is simply this is done

1969
02:03:04,070 --> 02:03:07,210
this so i have like four different parameters

1970
02:03:07,240 --> 02:03:09,460
the first parameter rho

1971
02:03:09,480 --> 02:03:14,430
is basically telling me what was the tendency to reciprocate edge right so i have

1972
02:03:14,490 --> 02:03:19,450
multiplied by j and j j i element of adjacency matrix so whenever they here

1973
02:03:19,450 --> 02:03:25,250
and counting how many times a french is mutual goes both ways then i'm asking

1974
02:03:25,530 --> 02:03:29,920
how many edges so there so phi is the density parameter

1975
02:03:29,950 --> 02:03:32,030
and then i have a sort of

1976
02:03:32,040 --> 02:03:35,240
the out out degree parameter in the parameters so

1977
02:03:35,240 --> 02:03:39,520
in social sciences they were called is called a separate activity of and also how

1978
02:03:39,520 --> 02:03:43,950
many edges it creates and attractiveness of an old how how likely is the node

1979
02:03:43,950 --> 02:03:45,560
to this evening

1980
02:03:46,690 --> 02:03:49,650
and i can go to feed this

1981
02:03:49,690 --> 02:03:52,540
using maximum MCMC

1982
02:03:52,570 --> 02:03:54,090
the reason for that is that

1983
02:03:54,110 --> 02:03:58,740
calculating the normalizing constant here according to coppola's policy is basically a sum over all

1984
02:03:58,740 --> 02:04:04,310
possible graphs times the probability of the graph right and for this little network this

1985
02:04:04,310 --> 02:04:07,530
is the number of graphs i would have to some some of so it some

1986
02:04:07,530 --> 02:04:12,130
it's somewhat too many so people do MCMC and since i'm running short on time

1987
02:04:12,130 --> 02:04:14,190
i want to explain how to do that

1988
02:04:14,200 --> 02:04:19,560
but here is example what you can do so one of sort of again very

1989
02:04:19,560 --> 02:04:25,710
very used networks in social networks analysis is this florentine families

1990
02:04:26,230 --> 02:04:30,530
this is the network these are the sixteen florentine families and i think every edge

1991
02:04:30,660 --> 02:04:36,020
that means that means that families are somehow related either by business relation or marriage

1992
02:04:36,020 --> 02:04:40,450
or something like that and you know miniature something i think this node and so

1993
02:04:40,450 --> 02:04:45,250
on OK so what you can do then is here community network

