1
00:00:00,000 --> 00:00:01,470
for instance in

2
00:00:01,490 --> 00:00:08,180
this is situation you see that i arranged these features these are the coordinates these

3
00:00:08,180 --> 00:00:12,080
are three color channels and the gradients that

4
00:00:12,090 --> 00:00:14,780
the course is symmetric and the

5
00:00:14,800 --> 00:00:16,620
the vertical axis same

6
00:00:16,650 --> 00:00:18,240
arm and

7
00:00:18,250 --> 00:00:25,330
each each cell here represents for instance the covariance between six and the chairman

8
00:00:25,350 --> 00:00:27,840
or channel and

9
00:00:27,850 --> 00:00:30,880
next to this is the function

10
00:00:32,320 --> 00:00:38,900
symmetric positive semidefinite so they live in be group so i can apply my the

11
00:00:38,900 --> 00:00:40,770
stuff that i showed you before

12
00:00:40,790 --> 00:00:42,350
the dimension of

13
00:00:42,360 --> 00:00:46,410
they are symmetric about in the previous slide on several features strength

14
00:00:46,670 --> 00:00:50,520
and i need only to coefficients to represent

15
00:00:51,800 --> 00:00:52,840
like many

16
00:00:52,930 --> 00:00:58,670
pixies on two between a coefficients i'm using color histogram which is only sixteen b

17
00:00:58,670 --> 00:01:03,630
is of histogram by the way i end up with much more coefficients so he

18
00:01:03,700 --> 00:01:05,080
called it

19
00:01:05,080 --> 00:01:09,530
its natural way of fusing different kind of features i don't care what is called

20
00:01:09,530 --> 00:01:14,720
and the other one is called the gradient i can define in my life

21
00:01:14,870 --> 00:01:18,540
and then you have to rotation scale changes

22
00:01:18,550 --> 00:01:23,830
and if find transformations and illumination variances

23
00:01:23,850 --> 00:01:26,000
and that is an illustration

24
00:01:26,600 --> 00:01:32,840
so these are my images and these is actually color-coded going metastasis corresponding to these

25
00:01:32,850 --> 00:01:36,120
images kept pictures of pictures

26
00:01:36,150 --> 00:01:37,970
at the same kind of

27
00:01:38,030 --> 00:01:40,540
look like these so we are

28
00:01:40,560 --> 00:01:42,340
kind of presenting this

29
00:01:42,420 --> 00:01:44,030
just need to twenty

30
00:01:44,040 --> 00:01:49,200
coefficients in this example and you know that structure the league

31
00:01:49,350 --> 00:01:52,710
so one of the first example i want to show is this search on the

32
00:01:52,720 --> 00:01:55,760
manifold which means that this is the points

33
00:01:55,770 --> 00:01:57,090
like the pictures

34
00:01:57,110 --> 00:01:59,590
o kind of like human pictures here

35
00:01:59,640 --> 00:02:04,690
and i compute covariance metastasis so the spider on the manifold that i want to

36
00:02:04,700 --> 00:02:10,720
kind of computable distance between them and you know how to compute the distance stars

37
00:02:10,830 --> 00:02:15,870
and the multiplication in bayes about picture find these that

38
00:02:15,900 --> 00:02:17,110
and i'm sure you

39
00:02:17,110 --> 00:02:22,150
o is located in is here but so a computer to do this thing it

40
00:02:22,150 --> 00:02:24,510
is to compute the know the whole thing

41
00:02:24,560 --> 00:02:29,680
it can compute everything and search is time consuming and this is why you need

42
00:02:29,750 --> 00:02:32,520
compact representations for businesses

43
00:02:32,620 --> 00:02:34,070
one thing

44
00:02:34,070 --> 00:02:37,990
and i think it was also this publication

45
00:02:38,090 --> 00:02:44,890
we constructed multiple queries metastasis wanted to be robust to force the

46
00:02:45,100 --> 00:02:50,260
conclusions so the original thing and then the like the covariance matrix for the whole

47
00:02:50,260 --> 00:02:55,070
thing that down portions of the image forces here we have for the last one

48
00:02:55,070 --> 00:02:57,560
right on top of and

49
00:02:57,690 --> 00:03:02,410
this is how we define our little features in this case nine

50
00:03:02,450 --> 00:03:04,550
the search image for

51
00:03:04,570 --> 00:03:09,720
region having similar calling semantics but we do we cannot be the image copied covariance

52
00:03:09,720 --> 00:03:15,040
matrix that compute the distance between the two and it's like these metal metastasis that

53
00:03:15,050 --> 00:03:16,900
we are

54
00:03:16,920 --> 00:03:21,550
in one case because we don't know the scale i is the application this is

55
00:03:21,550 --> 00:03:27,640
just an application example of this is the kind of skin affected by scandals they

56
00:03:27,640 --> 00:03:33,240
kept two towers of best match and in the second is that the the first

57
00:03:33,240 --> 00:03:38,240
one was just using the whole thing and the second one using the five of

58
00:03:38,800 --> 00:03:45,380
two just kind of accomplished some speedup and robustness that these are the results

59
00:03:45,440 --> 00:03:52,530
so what you see nearly in the given image someone like these regions

60
00:03:52,610 --> 00:03:55,390
this is about the earlier base and the flower

61
00:03:55,530 --> 00:03:59,530
and this is what you end up

62
00:03:59,530 --> 00:04:01,100
and these are the best images

63
00:04:01,120 --> 00:04:04,920
i mean i assume that the objective is going to be in the picture but

64
00:04:04,920 --> 00:04:10,370
this is my example that i can do anything and it was kind of a

65
00:04:10,640 --> 00:04:16,760
that occur in space distribution systems best the result you get

66
00:04:18,140 --> 00:04:19,210
the detected but

67
00:04:19,220 --> 00:04:24,610
kind of like this thing is missing using this intensity different sector to enter is

68
00:04:24,610 --> 00:04:27,950
different here is another example is the interface

69
00:04:27,970 --> 00:04:35,190
recognition algorithm of course but you this this is it's not also structural members structure

70
00:04:35,500 --> 00:04:39,280
is going to be representation again

71
00:04:42,130 --> 00:04:48,100
these are the results you get four histogram features another example this about a year

72
00:04:48,100 --> 00:04:53,090
and in this picture is the region it's fun and if you use histogram this

73
00:04:53,090 --> 00:04:55,000
what you will get so

74
00:04:55,100 --> 00:05:01,990
it is a performing then is a simple discrete but computing distances on manifold

75
00:05:02,000 --> 00:05:03,880
this slide can it

76
00:05:04,440 --> 00:05:10,510
about this kind of got another search application is set to the nearest neighbour because

77
00:05:10,520 --> 00:05:17,360
wireless are manifold in this example we use brodatz dataset hundred twelve classes and these

78
00:05:17,360 --> 00:05:24,800
in the time interval from to right to I + 1 is approximately not exactly

79
00:05:24,800 --> 00:05:30,070
because the damping the dumping rate isn't even constant within this time interval

80
00:05:30,820 --> 00:05:35,820
but it's approximately the dumping rate times the time

81
00:05:35,860 --> 00:05:39,820
for which the dumping was taking place

82
00:05:41,400 --> 00:05:42,650
that's what I mean by that

83
00:05:44,040 --> 00:05:47,720
and it gets more and more accurate the small time interval

84
00:05:50,320 --> 00:05:55,620
OK now here's my problem the problem is

85
00:05:55,640 --> 00:05:59,950
we start dumping

86
00:05:59,970 --> 00:06:01,840
at time t equals 0

87
00:06:03,140 --> 00:06:06,980
at time t equals t

88
00:06:06,990 --> 00:06:12,120
how much radioactive waste is in the file

89
00:06:23,900 --> 00:06:27,340
no they subproblems slightly complicated is

90
00:06:28,260 --> 00:06:34,660
radioactive waste OK if I put something at certain day and then go back several

91
00:06:34,660 --> 00:06:40,080
months later I don't and nothing's happened in between I don't have the same amount

92
00:06:40,080 --> 00:06:44,340
that I don't because all some of a fraction of the have left

93
00:06:46,700 --> 00:06:52,100
answers to the problem must take account of for each piece of wastes how long

94
00:06:52,100 --> 00:06:55,910
it has been in the pile because that takes account of how

95
00:06:55,950 --> 00:07:02,360
long it decay and when and where it ends up there so the calculation

96
00:07:02,600 --> 00:07:07,860
the central part of the calculation will be that if you have

97
00:07:07,880 --> 00:07:13,210
on initial amount of substance

98
00:07:13,230 --> 00:07:17,210
and it decays

99
00:07:17,230 --> 00:07:19,170
at time t

100
00:07:19,190 --> 00:07:25,120
this is the amount left at time t

101
00:07:25,120 --> 00:07:34,360
you know there's a lot radioactive decay you knew that coming into 1803

102
00:07:34,450 --> 00:07:38,170
although it's of course simple differential equation which reduces the velocities of the know the

103
00:07:38,170 --> 00:07:42,990
answer OK depends on the material so I'm going to assume that the

104
00:07:43,640 --> 00:07:49,930
nuclear plant dumps the same radioactive substance each time is only 1 substance calculating and

105
00:07:50,450 --> 00:07:56,650
itself assume k that I don't have to change from 1 k from one material

106
00:07:56,650 --> 00:07:59,170
to care for another because it's mixing of

107
00:08:00,720 --> 00:08:02,180
just 1 material

108
00:08:02,590 --> 00:08:08,980
OK now let's calculate here the idea all take the t axis but now I'm

109
00:08:08,980 --> 00:08:13,350
going to change its name to the you actually using why the 2nd

110
00:08:13,410 --> 00:08:15,670
starts at 0

111
00:08:15,690 --> 00:08:19,640
I'm interested in what's happening at the time t

112
00:08:19,640 --> 00:08:23,560
how much is left at time T so I'm going to divide up the interval

113
00:08:23,560 --> 00:08:25,470
from 0 to t

114
00:08:25,490 --> 00:08:27,620
on this time axis

115
00:08:27,650 --> 00:08:32,620
into well here's you 0 0 starting point you want you to

116
00:08:32,750 --> 00:08:35,670
of let's make this you want you

117
00:08:36,600 --> 00:08:39,300
focuses you want

118
00:08:39,520 --> 00:08:43,300
you to so on let's call this

119
00:08:43,380 --> 00:08:45,060
you have

120
00:08:45,860 --> 00:08:51,480
relentless about that

121
00:08:53,360 --> 00:08:55,180
that the amount

122
00:08:55,720 --> 00:08:58,960
the other

123
00:09:00,690 --> 00:09:04,370
so I'm going to do is look at the amount that take a time interval

124
00:09:04,370 --> 00:09:10,750
from you URI for URI plus 1 is the time interval delta T delta you

125
00:09:10,790 --> 00:09:15,100
divided up into equal time intervals

126
00:09:15,620 --> 00:09:17,860
so the amount of dumped

127
00:09:19,200 --> 00:09:26,720
in the time interval from you GUI + 1 is equal to approximately half of

128
00:09:26,720 --> 00:09:32,260
you dumping function there times delta you we calculate that before

129
00:09:32,270 --> 00:09:33,690
that's what the meaning of the dumping rate

130
00:09:36,980 --> 00:09:40,460
by trying to see how much is case

131
00:09:40,520 --> 00:09:47,500
it has carried how much is left in other words

132
00:09:51,740 --> 00:09:54,140
well this is the starting amount

133
00:09:55,000 --> 00:10:00,300
so the answer is going to be its efforts you URI times delta you

134
00:10:00,310 --> 00:10:05,060
flying this factor which tells us how

135
00:10:05,060 --> 00:10:07,100
how much it decays now so

136
00:10:07,120 --> 00:10:12,270
time so this is the starting amount that's fine you are

137
00:10:12,460 --> 00:10:15,620
that's what it was 1st done and this is the amount of

138
00:10:16,620 --> 00:10:21,720
time multiply that by e to the minus k times now what should I put

139
00:10:21,730 --> 00:10:22,640
up there

140
00:10:22,830 --> 00:10:25,480
I have to put the length of time that

141
00:10:26,160 --> 00:10:32,500
had to decay what is the length of time that has the case was dumped

142
00:10:32,510 --> 00:10:35,510
I'm looking at time t it decayed for

143
00:10:36,700 --> 00:10:41,980
the time length T minus 2

144
00:10:42,610 --> 00:10:46,000
of the

145
00:10:47,500 --> 00:10:51,220
when the time to do that

146
00:10:52,370 --> 00:11:02,020
today on

147
00:11:02,440 --> 00:11:06,340
so the stuff that was dumped this time interval

148
00:11:06,350 --> 00:11:09,960
when i at time t when I come to look at it this is how

149
00:11:09,960 --> 00:11:13,750
much of it is less and now all I have to do is add up

150
00:11:13,770 --> 00:11:19,120
that that quantity for this time the stuff that was this time interval of stuff

151
00:11:19,250 --> 00:11:22,210
down then once and so on all the way up to the stuff that was

152
00:11:22,210 --> 00:11:25,370
done yesterday and the answer will be there for

153
00:11:25,860 --> 00:11:30,760
the answer will be the total amount

154
00:11:31,270 --> 00:11:38,330
left at time t that is that is not yet the case

155
00:11:40,740 --> 00:11:47,100
will be approximately you add up the amount coming from the 1st time interval was

156
00:11:47,100 --> 00:11:51,620
the and coming and so on so the effort you all all say save delta

157
00:11:51,620 --> 00:11:57,680
you for the n times e to the minus k minus minus I find delta

158
00:11:57,680 --> 00:12:02,200
you so these 2 represent the amount and this is the case factor

159
00:12:02,960 --> 00:12:08,020
and I add those up as a runs from well we start from 1

160
00:12:08,840 --> 00:12:11,340
the yeah

161
00:12:11,540 --> 00:12:14,020
and now let delta T goes to 0

162
00:12:14,060 --> 00:12:19,230
In other words make this more like delta you go 0 make this more accurate

163
00:12:19,250 --> 00:12:23,600
by taking finer and finer some analysis of looking every month to see how much

164
00:12:23,600 --> 00:12:27,050
the population biology evolutionary epidemiology

165
00:12:27,060 --> 00:12:30,430
the likelihood function is intractable

166
00:12:30,440 --> 00:12:35,910
so we don't know likelihood function properties data given theta we can't use MCMC

167
00:12:35,960 --> 00:12:40,030
he gave sample many of the monte carlo algorithms

168
00:12:40,050 --> 00:12:42,020
that these and other approach

169
00:12:44,950 --> 00:12:46,070
so the

170
00:12:46,080 --> 00:12:52,330
basic algorithm is based upon the rejection algorithm which was first proposed by bono monopoly

171
00:12:52,330 --> 00:12:53,770
back in the fifties

172
00:12:53,820 --> 00:13:00,450
it's a complete what follows we pick a parameter values because of my prior distribution

173
00:13:00,450 --> 00:13:04,820
on the set of parameter values with probability proportional to the likelihood

174
00:13:04,820 --> 00:13:09,090
so divide by some constant here along with everything less than one

175
00:13:09,140 --> 00:13:15,350
but what i find that gives is independent draws from the posterior distribution of the

176
00:13:15,350 --> 00:13:17,330
parameter is that works

177
00:13:17,340 --> 00:13:21,410
but what he said we don't know the likelihood function

178
00:13:21,430 --> 00:13:27,890
so what david building calls the mechanical version of this

179
00:13:27,950 --> 00:13:32,890
second mechanical version of based so we draw theta from prior

180
00:13:32,890 --> 00:13:35,660
we simulate the data from the model

181
00:13:35,700 --> 00:13:40,620
b prime and we get from the value of the simulated data the prime exactly

182
00:13:40,620 --> 00:13:42,820
matches real day-to-day

183
00:13:44,580 --> 00:13:46,660
is a mechanical version

184
00:13:46,710 --> 00:13:51,570
but you computable to anywhere in the background producing we simulated data sets

185
00:13:51,580 --> 00:13:55,590
i mean it belongs to exactly match a real dataset

186
00:13:55,600 --> 00:13:59,940
now obviously see the animal very complexity here

187
00:13:59,950 --> 00:14:05,780
this is just not going happen i mean if you continue data that you never

188
00:14:05,780 --> 00:14:09,080
going to exactly simulate your hard to should

189
00:14:09,090 --> 00:14:12,750
so other approximation comes in

190
00:14:12,780 --> 00:14:16,760
so what we do instead is pick up from its the former prime

191
00:14:16,820 --> 00:14:18,260
simulation data

192
00:14:18,260 --> 00:14:22,820
i mean that's that the parameter if the simulated data is close to the real

193
00:14:23,660 --> 00:14:30,520
and by close i mean we're going to have a metric rho and

194
00:14:30,550 --> 00:14:32,300
tolerance epsilon

195
00:14:32,310 --> 00:14:37,340
and this work in the following sense that the first of all this is an

196
00:14:37,340 --> 00:14:44,020
approximation obviously and this is what was sampling from with this and it can make

197
00:14:44,030 --> 00:14:45,260
sense in the following way

198
00:14:45,780 --> 00:14:49,160
if epsilon is infinity here

199
00:14:49,190 --> 00:14:51,320
we cannot accept every single

200
00:14:51,320 --> 00:14:52,890
value of theta

201
00:14:52,950 --> 00:14:55,330
so we're left with is prior distribution

202
00:14:55,570 --> 00:15:01,280
it's island zero here we're only going to accept theta if we simulated data matches

203
00:15:01,300 --> 00:15:05,090
the real data metric we get a posterior distribution

204
00:15:05,140 --> 00:15:06,070
OK so

205
00:15:06,080 --> 00:15:08,090
epsilon controls accuracy

206
00:15:08,100 --> 00:15:13,460
the same time controls the efficiency of the computability so we not very small we

207
00:15:13,580 --> 00:15:18,390
accept very few feet tall very large amounts of lots of the decision of the

208
00:15:18,390 --> 00:15:22,870
trade-off represents the tension between computability and accuracy

209
00:15:27,320 --> 00:15:30,880
well there is another kind of just another layer of complexity

210
00:15:32,430 --> 00:15:36,760
even using the approximation this kind of model for talking about typically have a very

211
00:15:36,760 --> 00:15:44,070
high dimensional output so you're talking about genocide canadian expression data they highdimensional and even

212
00:15:44,070 --> 00:15:46,390
that been a good so

213
00:15:46,440 --> 00:15:51,560
is it really so much data some lower dimensional space kind of summary of what

214
00:15:51,560 --> 00:15:54,630
we do is compared to some of the real data with summary of the simulation

215
00:15:54,900 --> 00:15:58,300
output that close to set the parameters

216
00:16:00,550 --> 00:16:04,100
if s is sufficient statistic here

217
00:16:04,120 --> 00:16:08,560
as i mean sufficient in the physical sense of political information about the in the

218
00:16:10,010 --> 00:16:14,510
it is sufficient that this algorithm is equivalent to the previous slide

219
00:16:14,560 --> 00:16:21,960
of course you say if the likelihood function is no we can never know whether

220
00:16:21,960 --> 00:16:26,120
any statistic sufficient to not to find anything difficult

221
00:16:28,440 --> 00:16:31,260
OK is the simplest example

222
00:16:32,940 --> 00:16:37,730
so let me just say that the accuracy of these algorithms is unknown need very

223
00:16:37,730 --> 00:16:39,470
near the first proposed in

224
00:16:39,480 --> 00:16:44,820
ninety nine we think by it's hard to tell and then mark beaumont bolding in

225
00:16:44,820 --> 00:16:48,570
two thousand one made many well-known suggested community

226
00:16:48,610 --> 00:16:55,240
and at the moment is not known how the scales so for a simple example

227
00:16:55,370 --> 00:16:59,590
for gas distribution data comes from a normal distribution

228
00:16:59,600 --> 00:17:01,250
but then is no

229
00:17:01,270 --> 00:17:02,180
i mean

230
00:17:02,190 --> 00:17:03,660
we give a flat

231
00:17:03,750 --> 00:17:06,560
improper prior to the mean

232
00:17:07,290 --> 00:17:11,190
well the generality we assume that we observe data that has the sample mean is

233
00:17:11,200 --> 00:17:13,830
zero that's real data observations

234
00:17:13,850 --> 00:17:17,730
the album to follows would pick a parameter from a prior distribution

235
00:17:17,790 --> 00:17:20,310
simulation data using the parameter value

236
00:17:20,320 --> 00:17:25,560
well except that parameter if the simulated data is close to zero which observation

237
00:17:25,620 --> 00:17:29,170
and in this case we actually calculate what the approximation is analytically

238
00:17:29,210 --> 00:17:33,700
know is is that the variance of the approximation

239
00:17:33,720 --> 00:17:35,770
it's the true posterior

240
00:17:35,820 --> 00:17:40,720
plus epsilon spread over three can

241
00:17:40,730 --> 00:17:46,190
and so this is overdispersion the approximations overdispersion and the true posterior

242
00:17:46,200 --> 00:17:50,520
i mean it is true in general that the estimates are conservative dispersed

243
00:17:50,870 --> 00:17:52,480
well this one should be

244
00:17:52,490 --> 00:17:56,690
so here the picture you've got the solid lines if you can see this and

245
00:17:56,690 --> 00:17:58,020
the true posterior

246
00:17:58,070 --> 00:18:03,600
the dotted lines the approximation and we'll epsilon decreasing in the approximation gets better

247
00:18:03,670 --> 00:18:07,580
have to do more work do it each time

248
00:18:10,780 --> 00:18:13,470
the problem with the algorithms

249
00:18:13,490 --> 00:18:14,790
and it so far

250
00:18:14,810 --> 00:18:16,690
that the very inefficient

251
00:18:16,780 --> 00:18:20,900
so you probably spotted with sampling repeatedly from the prior distribution

252
00:18:20,920 --> 00:18:27,210
so if you got a highly multivariate promise i motivate parameter can dimensions or whatever

253
00:18:27,530 --> 00:18:30,410
you're looking for a regional high probability in

254
00:18:31,380 --> 00:18:36,650
a multidimensional haystack very hard to find way looking for

255
00:18:37,350 --> 00:18:43,690
the idea behind MCMC of course is that by correlating successive observations we can spend

256
00:18:43,690 --> 00:18:47,320
more time in regions of high likelihood

257
00:18:47,340 --> 00:18:53,060
well we can't do MCMC according likelihood but there is an approximate version of it

258
00:18:53,080 --> 00:18:59,870
so we have usual proposal density q we the following because theta we propose move

259
00:18:59,880 --> 00:19:01,000
the prime

260
00:19:01,020 --> 00:19:02,370
thank you

261
00:19:02,430 --> 00:19:04,710
and then we simulation data using that parameter

262
00:19:04,730 --> 00:19:07,790
if the simulated data close again

263
00:19:07,830 --> 00:19:11,240
we then calculate the metropolis hastings acceptance ratio

264
00:19:11,270 --> 00:19:14,530
so usually because the ratio of likelihoods in here

265
00:19:14,540 --> 00:19:18,120
but that is being approximated by this acceptance step

266
00:19:18,120 --> 00:19:20,530
dominance of this

267
00:19:20,530 --> 00:19:23,580
two clusters actually can proceed to class

268
00:19:23,590 --> 00:19:28,880
this is what i get because like construction of course the similarities is just the

269
00:19:28,880 --> 00:19:33,930
usual why don't we take the euclidean distance between points

270
00:19:33,990 --> 00:19:36,810
the city

271
00:19:36,860 --> 00:19:39,310
the idea is called the j

272
00:19:39,330 --> 00:19:43,220
the distance between two points in the similarity to this one

273
00:19:47,810 --> 00:19:53,660
this the one and as i said by construction dominance that we're find complex structure

274
00:19:53,700 --> 00:19:58,810
so the fine for find this dominance over sigma

275
00:19:58,860 --> 00:20:00,840
data and this is no way

276
00:20:00,850 --> 00:20:02,310
let's do something

277
00:20:04,620 --> 00:20:06,970
captured along structure

278
00:20:07,030 --> 00:20:08,410
so there is the standard

279
00:20:08,790 --> 00:20:11,610
normalized got worse

280
00:20:14,420 --> 00:20:17,430
and in fact it shows what happens

281
00:20:17,450 --> 00:20:20,070
here is another example

282
00:20:20,080 --> 00:20:21,160
of course

283
00:20:21,180 --> 00:20:26,250
of course i can do i can adjust the scale we have escaped parameter here

284
00:20:27,670 --> 00:20:29,960
but of course it doesn't work because the scale

285
00:20:29,970 --> 00:20:31,600
just to the

286
00:20:31,600 --> 00:20:37,510
the size of the class become larger get into all sorts of point for example

287
00:20:37,570 --> 00:20:39,130
because consider this

288
00:20:39,190 --> 00:20:41,810
we would like to extract this class

289
00:20:41,850 --> 00:20:47,640
and if we ground the replicator and with k parameter we get this

290
00:20:47,660 --> 00:20:51,160
c so it over segments data

291
00:20:51,180 --> 00:20:52,490
and then we might

292
00:20:52,510 --> 00:20:54,390
this is what it does actually

293
00:20:55,870 --> 00:20:59,660
complex structured data we can increase the the scale

294
00:20:59,680 --> 00:21:04,840
but then of course we get everything but of course we also have lot

295
00:21:04,890 --> 00:21:08,470
so there is a simple trick to avoid this problem

296
00:21:08,530 --> 00:21:11,700
and actually this is quite standard for

297
00:21:11,720 --> 00:21:14,550
those are ones which which by definition

298
00:21:14,570 --> 00:21:16,660
extract complex structure

299
00:21:16,680 --> 00:21:21,930
the idea is to perform some operations of the similarity metrics in order to

300
00:21:21,970 --> 00:21:25,490
he able to extract a chain structure in the

301
00:21:26,700 --> 00:21:29,800
but we show this with a simple example just the

302
00:21:30,030 --> 00:21:36,010
mentioned the basic idea

303
00:21:38,740 --> 00:21:43,800
because here

304
00:21:43,800 --> 00:21:45,660
the set of lines

305
00:21:45,720 --> 00:21:47,470
and suppose that

306
00:21:47,610 --> 00:21:53,720
calculate the similarity between by using standard approach like this one

307
00:21:53,760 --> 00:21:59,240
based on euclidean distance and then i have a threshold which says well this too

308
00:21:59,570 --> 00:22:03,700
are similar or not seen above the shoulders down

309
00:22:03,740 --> 00:22:05,660
so we may probably that

310
00:22:05,680 --> 00:22:09,620
with an unweighted graph talking about with

311
00:22:09,620 --> 00:22:10,870
this problem in this way

312
00:22:10,970 --> 00:22:17,510
because course these two are similar but this two are not seen in this way

313
00:22:17,570 --> 00:22:20,340
now if i ran the replicator dynamics here

314
00:22:20,410 --> 00:22:23,180
of course now we are the zero one case

315
00:22:23,220 --> 00:22:25,370
dominant sets to coincide

316
00:22:25,570 --> 00:22:27,740
with the maximal cliques

317
00:22:27,740 --> 00:22:30,280
we're fine i did this maximum weight

318
00:22:30,320 --> 00:22:32,430
all this maximum there's no way

319
00:22:32,450 --> 00:22:34,090
you can get rid

320
00:22:34,090 --> 00:22:36,950
so in which case this simple trick two

321
00:22:36,990 --> 00:22:40,870
let the replicator in works well is to close

322
00:22:41,470 --> 00:22:42,780
the graph

323
00:22:42,780 --> 00:22:44,820
if i give you a graph

324
00:22:44,840 --> 00:22:48,370
the closure of the graph is the new graph which is the same that is

325
00:22:48,370 --> 00:22:49,550
p one

326
00:22:49,570 --> 00:22:54,160
and the reason that between two vertices if and only if the two parties in

327
00:22:54,180 --> 00:22:57,660
the original graph are connected by

328
00:22:57,660 --> 00:23:01,390
this is the closure standard notion from

329
00:23:01,430 --> 00:23:03,610
so if i closed this graph

330
00:23:03,620 --> 00:23:06,840
it is easy to see the end up with a complete

331
00:23:06,840 --> 00:23:08,890
because of course this graph is connected

332
00:23:08,910 --> 00:23:10,990
if i close the graph

333
00:23:11,030 --> 00:23:13,140
so this becomes a large

334
00:23:13,160 --> 00:23:14,660
well actually means

335
00:23:15,620 --> 00:23:20,740
so if a if a around the replicator dynamics over the closure of this i

336
00:23:20,740 --> 00:23:21,760
will get

337
00:23:23,200 --> 00:23:25,800
the class was

338
00:23:27,970 --> 00:23:30,640
the track this is called the transitive closure

339
00:23:30,700 --> 00:23:34,320
now simple observation the transitive closure of the graph

340
00:23:34,340 --> 00:23:38,660
we obtain considering the powers of which is that

341
00:23:38,680 --> 00:23:39,760
in general

342
00:23:39,780 --> 00:23:41,870
if i give you

343
00:23:41,870 --> 00:23:45,090
the adjacency matrix of the graph and

344
00:23:45,110 --> 00:23:47,700
and i compute a power of

345
00:23:47,840 --> 00:23:50,930
and an integer

346
00:23:51,140 --> 00:23:55,370
components in the same way i j k

347
00:23:55,390 --> 00:23:56,680
this number here

348
00:23:56,720 --> 00:23:57,640
as you

349
00:23:57,640 --> 00:23:59,320
the number of

350
00:23:59,340 --> 00:24:01,030
as after of length k

351
00:24:01,050 --> 00:24:03,350
connect i and j

352
00:24:05,300 --> 00:24:06,320
if we

353
00:24:09,510 --> 00:24:16,140
the some is our so a plus a two

354
00:24:16,160 --> 00:24:18,620
so we have a way to we have actually

355
00:24:18,660 --> 00:24:22,990
simple what we need to compute transitive closure

356
00:24:24,280 --> 00:24:27,870
so now let's consider the way the case by analogy

357
00:24:27,910 --> 00:24:34,070
one idea for closing what we call a weighted graph that we

358
00:24:34,120 --> 00:24:37,090
is to consider this magic here

359
00:24:37,090 --> 00:24:39,400
these are all things that live in the air and then you have to the

360
00:24:39,400 --> 00:24:43,240
set of things that live on land the leopard alligator pie fun and that's one

361
00:24:43,300 --> 00:24:46,780
and here's grasshopper and b she monkey gone i said what's the difference between the

362
00:24:46,780 --> 00:24:48,490
two ones that live on land

363
00:24:48,490 --> 00:24:54,400
like whether you would want to encounter them

364
00:24:54,450 --> 00:24:56,940
in the dark jungle ali or something

365
00:24:56,990 --> 00:25:00,090
these are the theories what

366
00:25:00,110 --> 00:25:04,170
well if you're brave hunter course you're going out looking for the leopards alligators and

367
00:25:06,300 --> 00:25:08,320
and these are the mostly harmless

368
00:25:08,900 --> 00:25:13,760
so basically what i discovered is an organisation that that's in terms of the ecological

369
00:25:13,760 --> 00:25:15,440
niche land air and sea

370
00:25:15,450 --> 00:25:20,670
crossed with whether or not you're predator prey basically with with more data more features

371
00:25:20,670 --> 00:25:24,070
and more animals we should be able to pick those apart probably pick up predator

372
00:25:24,070 --> 00:25:27,860
and prey as this as as a separate orthogonal dimensions from land air and sea

373
00:25:27,860 --> 00:25:29,920
but here we don't have enough data for that

374
00:25:29,920 --> 00:25:32,470
and if you if you look at the features that are supported by this is

375
00:25:32,470 --> 00:25:36,050
exactly the ones in this spectrum so these guys all had features like is ferocious

376
00:25:36,050 --> 00:25:39,650
is dangerous is a carnivore and those are the main things distinguishing them from he's

377
00:25:39,650 --> 00:25:43,010
mostly harmless levels

378
00:25:44,510 --> 00:25:50,510
this is this one is in the hands free

379
00:25:50,530 --> 00:25:59,110
so who was it's true i said well you should you basically just ignore that

380
00:25:59,110 --> 00:26:01,920
happens to be in this dataset but there's a few

381
00:26:01,970 --> 00:26:05,650
the features are just predicates you can say in language like one of the things

382
00:26:05,650 --> 00:26:07,920
you can say about the monkey is that a mammal another thing you can say

383
00:26:07,920 --> 00:26:10,990
is that it's formula another thing you can say is it has both there's a

384
00:26:10,990 --> 00:26:15,010
small number of these features which happened to be in english the labels for these

385
00:26:15,010 --> 00:26:19,700
taxonomic categories but that's i mean that's just as far as the models concerned doesn't

386
00:26:19,700 --> 00:26:22,970
know that it's just you know there's no difference here

387
00:26:25,130 --> 00:26:27,740
being so is a mammal is this feature

388
00:26:27,740 --> 00:26:31,610
it's one that's true just the mammals not the other and here's one lays eggs

389
00:26:31,610 --> 00:26:34,700
which is true of everything except the mammals as far as the models concerned those

390
00:26:34,700 --> 00:26:37,490
are just you know the the inverses of each other that no one one is

391
00:26:37,490 --> 00:26:41,030
no more special other just so happens in this dataset you know

392
00:26:41,050 --> 00:26:41,860
as in

393
00:26:41,920 --> 00:26:46,010
much of the animal kingdom mammals don't mammals give birth to live young and everything

394
00:26:46,010 --> 00:26:46,840
else doesn't

395
00:26:46,880 --> 00:26:49,650
so that makes sense

396
00:26:53,420 --> 00:26:59,170
so if when when it when it when a child learns the words mammal right

397
00:26:59,320 --> 00:27:01,590
like what what in what sense today

398
00:27:01,630 --> 00:27:04,570
learn it right or if you learn the word dog right i mean you may

399
00:27:05,320 --> 00:27:08,170
have somebody telling you OK this is a mammal this as you can observe this

400
00:27:08,170 --> 00:27:14,030
has various other properties right i mean it's true that that the word mammal

401
00:27:14,240 --> 00:27:17,920
has it has a deeper meaning in some sense in terms of how you want

402
00:27:17,920 --> 00:27:21,880
to organize objects but this but children don't necessarily have to know that initially another

403
00:27:21,880 --> 00:27:25,920
does this model it's just one of many things you can say about animal rights

404
00:27:25,920 --> 00:27:32,740
like has the first right i mean you have a is the fact that there

405
00:27:32,740 --> 00:27:36,700
are mammals and what it so if you took there's only about four five features

406
00:27:36,700 --> 00:27:39,400
in here that are of that sort so there's like is a mammal

407
00:27:39,650 --> 00:27:44,090
there's also there's also is a canine which actually

408
00:27:44,200 --> 00:27:45,820
of these things canine

409
00:27:45,840 --> 00:27:47,590
i mean it's sort of

410
00:27:47,630 --> 00:27:50,220
those are basically like noise i mean

411
00:27:50,240 --> 00:27:52,970
there they happened or

412
00:27:53,170 --> 00:27:59,530
and basically the so the dataset has this organisation and there are a bunch of

413
00:27:59,530 --> 00:28:02,630
mammals and here there are a bunch of birds there are a bunch of insects

414
00:28:02,630 --> 00:28:03,880
and so on right

415
00:28:03,900 --> 00:28:08,170
but the fact that there's a feature called is a mammal is irrelevant pretty much

416
00:28:08,220 --> 00:28:11,090
there is that it happens to be that it's clean with respect to partition but

417
00:28:11,090 --> 00:28:15,300
so are lots of others right like laying eggs is also just as clear

418
00:28:15,340 --> 00:28:16,990
o point of view and

419
00:28:17,010 --> 00:28:17,590
the a

420
00:28:17,610 --> 00:28:22,300
which is called the importance of queensland's left yes

421
00:28:22,320 --> 00:28:26,780
well they are the clean well the

422
00:28:26,820 --> 00:28:29,590
well it is not

423
00:28:29,610 --> 00:28:32,320
if you took them out you get the same answer

424
00:28:32,380 --> 00:28:35,720
there's not very many of them there's like four

425
00:28:35,780 --> 00:28:38,970
there's a hundred features

426
00:28:39,200 --> 00:28:41,530
so if you look at the clean features

427
00:28:41,550 --> 00:28:43,590
most of them are not is a mammal

428
00:28:43,650 --> 00:28:47,400
is that the that they are you saying it's hardly surprising given that there's a

429
00:28:47,400 --> 00:28:50,220
lot of features which are clean with respect to the taxonomic categories that you should

430
00:28:50,220 --> 00:28:54,260
get the taxonomic categories using

431
00:28:54,280 --> 00:28:57,320
so has right so

432
00:28:57,630 --> 00:28:59,150
among all right

433
00:28:59,170 --> 00:29:04,630
just because i mean the clean the ones which are strictly strictly clean are you

434
00:29:04,630 --> 00:29:10,220
know there's probably an right so OK so the so

435
00:29:10,240 --> 00:29:15,050
yes in some sense it's hardly surprising given these features but these are so i

436
00:29:15,050 --> 00:29:18,340
didn't really tell you these features come from other companies asking a bunch of asking

437
00:29:18,340 --> 00:29:22,260
separate subject to just name features of animals so

438
00:29:26,990 --> 00:29:30,180
i mean in some sense what you're saying what applied to i guess far as

439
00:29:30,180 --> 00:29:35,220
i know any unsupervised learning and if you say well your model found some clusters

440
00:29:35,220 --> 00:29:38,630
and yet look there are some features which are really clean with respect to those

441
00:29:38,630 --> 00:29:40,740
clusters well this sort of

442
00:29:40,760 --> 00:29:43,940
if if there weren't any such features it wasn't going to learn anything

443
00:29:43,950 --> 00:29:47,630
right in the middle of this

444
00:29:47,650 --> 00:29:50,300
this is the

445
00:29:50,320 --> 00:29:54,840
well so for example that that's what was going on here i mean again the

446
00:29:54,840 --> 00:29:59,110
people who listen the features were not told why they releasing the features the

447
00:30:01,820 --> 00:30:05,590
and we were pleasantly surprised and we really didn't know what was going to come

448
00:30:05,590 --> 00:30:09,990
out we were pleasantly surprised that this model was able to discover a totally different

449
00:30:09,990 --> 00:30:15,110
way of organizing these animals which is relevant for some of the

450
00:30:15,130 --> 00:30:19,090
some of the ways we think about animals particularly for not biologists but if we

451
00:30:19,090 --> 00:30:24,970
actually care about interacting with the natural world and maybe if you like

452
00:30:24,990 --> 00:30:28,860
we the criticism you have is more criticism of most conventional

453
00:30:28,900 --> 00:30:34,050
kinds of clustering where you're only looking for one clustering and basically it's whatever clustering

454
00:30:34,050 --> 00:30:36,700
you come up with if few models doing well is going to be driven by

455
00:30:38,470 --> 00:30:42,680
you have most features four like the reason why the conventional mixture model find this

456
00:30:42,680 --> 00:30:46,200
clustering as opposed the other one is that most of the features you know the

457
00:30:46,200 --> 00:30:49,530
majority of the features that support any clustering support that one

458
00:30:49,530 --> 00:30:53,330
as a dot product and that's a little bit more complicated and we need several

459
00:30:53,350 --> 00:31:00,170
elementary things along the way so so let's take another set of questions and play

460
00:31:00,170 --> 00:31:04,650
with them a bit it's not embarrassing if you don't solve them but it's good

461
00:31:04,690 --> 00:31:09,110
to play with the definition to learn and understand it and get a feeling for it

462
00:31:09,110 --> 00:31:16,110
so I will also give you some some hints how to solve these problems

463
00:31:16,110 --> 00:31:22,770
so so on the last light we started with the sorry we started with the

464
00:31:22,770 --> 00:31:28,610
feature not being constructed kernels from it this time we start with kernels and

465
00:31:28,690 --> 00:31:33,350
look at some properties of kernels so let's assume all the Ks on this light

466
00:31:33,360 --> 00:31:40,950
are positive definite all the points are from our domain X  and the first property

467
00:31:40,950 --> 00:31:48,310
to prove is that if I substitute the same element twice into the kernel function

468
00:31:48,310 --> 00:31:57,070
I get a non-negative number so which means that that diagonal elements of a kernel matrix

469
00:31:57,120 --> 00:32:03,270
are always non-negative okay that's the first property  I wanted to prove that this so maybe

470
00:32:03,270 --> 00:32:09,510
if you haven't done so yet it would be good to copy this inequality and you

471
00:32:09,510 --> 00:32:14,230
can even directly substitute this kernel in here because that's the thing that you will always

472
00:32:14,230 --> 00:32:21,350
have to that you usually have to check or prove so so I'll leave it up for

473
00:32:21,350 --> 00:32:30,030
a few seconds and puff okay so so now you know this equality

474
00:32:30,030 --> 00:32:34,850
is true no matter what A I you choose no matter what points X you choose

475
00:32:34,860 --> 00:32:39,830
so you are free to choose what you want and I want you to show that

476
00:32:39,830 --> 00:32:45,710
this is a consequence of that inequality  so it follows from this definition the second

477
00:32:45,710 --> 00:32:49,890
point so this is already it's a reasonably easy one even though you need a little

478
00:32:49,890 --> 00:32:54,510
idea how to solve it the second one is a little bit more complicated that's

479
00:32:54,510 --> 00:32:57,930
why I give you a hint so in the second  case so here we have the

480
00:32:57,930 --> 00:33:02,130
simplest we have only one point if you want we have a kernel matrix with

481
00:33:02,130 --> 00:33:08,030
only one element in the second case we are  effectively dealing with two points so with

482
00:33:08,030 --> 00:33:12,550
the kernel matrix that's a two by two matrix so if we compute all possible

483
00:33:13,150 --> 00:33:18,550
comparisons of X and X prime we we are looking at the two by two matrix

484
00:33:18,550 --> 00:33:24,630
and the hints to prove this this is generalized Cauchy-Schwarz inequalities to compute

485
00:33:24,630 --> 00:33:30,510
the determinant of this two by two matrix so write down the matrix compute the determinant

486
00:33:30,510 --> 00:33:37,850
and then you will be able to prove this now the next one  is

487
00:33:37,850 --> 00:33:42,310
a little bit more complicated but the proof is relatively easy but the statement is

488
00:33:42,310 --> 00:33:48,190
more complicated so the statement says if the diagonal elements are zero so remember

489
00:33:48,190 --> 00:33:54,210
we proved before that they are non-negative so here I am saying if they are zero if

490
00:33:54,210 --> 00:34:00,290
all diagonal elements are zero for all X then actually the kernel is zero everywhere so

491
00:34:00,290 --> 00:34:03,010
if the diagonal is zero the kernel is zero

492
00:34:03,360 --> 00:34:07,510
and the hint for this one is that you should be using the Cauchy-Schwarz inequality

493
00:34:07,510 --> 00:34:13,660
to prove that and finally maybe we don't have to do all of

494
00:34:13,670 --> 00:34:20,310
them but prove as many as you can was there a question no prove as many

495
00:34:20,510 --> 00:34:24,370
as you can so the first one is if you have a kernel K positive definite

496
00:34:24,370 --> 00:34:29,010
you multiply it with a number  that's not negative you get another positive

497
00:34:29,010 --> 00:34:36,350
definite kernel likewise if you have two positive definite kernels take their sum you

498
00:34:36,350 --> 00:34:38,270
get another positive definite kernel

499
00:34:38,790 --> 00:34:43,550
if you have a sequence of positive definite kernels you take their limits you

500
00:34:43,550 --> 00:34:49,330
get another positive definite kernel provided the limit exists the next one I don't think

501
00:34:49,350 --> 00:34:54,250
I don't think you'll be able to prove that here this is this is rity pretty difficult

502
00:34:54,460 --> 00:34:59,870
if you take the product of two positive definite kernels you get a  positive definite kernel so you

503
00:34:59,870 --> 00:35:04,570
might be aware that there's a corresponding result of positive definite matrices so if you

504
00:35:04,570 --> 00:35:08,330
take two matrices that's are both positive definite and they have the same size and take

505
00:35:08,330 --> 00:35:13,130
the element wise product and you get a positive definite matrix which which is quite

506
00:35:13,130 --> 00:35:22,090
surprising  this is a difficult result you to show and  so I think  you in

507
00:35:22,090 --> 00:35:25,930
principle you could get up to here but let's let's see let's see how far you get so

508
00:35:25,940 --> 00:35:30,050
you can do them in any  sequence you like and I'll take a break and then

509
00:35:30,050 --> 00:35:34,540
we'll see how it goes so this is all correct however I would like another

510
00:35:34,540 --> 00:35:38,810
solution for the first one and maybe someone else can give me that

511
00:35:38,820 --> 00:35:42,830
and the reason  so maybe I should first just repeat because he didn't have a microphone

512
00:35:42,840 --> 00:35:48,490
so he was saying okay K's positive definite therefore we can represent it as a dot product and now

513
00:35:48,490 --> 00:35:52,060
if we plug in the same vector twice  we just get a dot product if we it

514
00:35:52,150 --> 00:35:58,690
itself which is non-negative the second problem he was saying that's right down the two by

515
00:35:58,690 --> 00:36:05,370
two matrix of K on points X and X  prime and then we compute the determinant

516
00:36:05,370 --> 00:36:11,050
determinant determinant is the product of these two minus the product of these two these

517
00:36:11,050 --> 00:36:15,650
two are identical because K is symmetric with  K of X X prime is the same

518
00:36:15,650 --> 00:36:23,310
of K as K X prime X so therefore we get this times this minus

519
00:36:23,310 --> 00:36:29,070
the square of this  of diagonal element moreover since this is a positive definite

520
00:36:29,070 --> 00:36:30,260
the second mode

521
00:36:31,630 --> 00:36:33,420
we don't know

522
00:36:34,130 --> 00:36:37,800
in the long after something which is useful

523
00:36:39,130 --> 00:36:41,700
that needs to be some kind of

524
00:36:41,740 --> 00:36:44,650
democracy like john two

525
00:36:44,700 --> 00:36:46,550
you have been knowledge about the

526
00:36:46,550 --> 00:36:48,280
organic beings

527
00:36:49,130 --> 00:36:51,110
or at least

528
00:36:51,110 --> 00:36:53,130
needs the market

529
00:36:55,300 --> 00:37:02,170
the crucial thing here i know what is being debated during the winter because of

530
00:37:02,270 --> 00:37:05,090
that said that if he

531
00:37:05,130 --> 00:37:06,720
can made

532
00:37:06,720 --> 00:37:08,970
which is now

533
00:37:10,590 --> 00:37:16,450
around the world especially when the one you know expected king

534
00:37:18,970 --> 00:37:21,820
the first was is open

535
00:37:21,820 --> 00:37:23,550
we can that

536
00:37:23,570 --> 00:37:26,240
and we are

537
00:37:26,280 --> 00:37:27,530
and on the

538
00:37:29,300 --> 00:37:33,550
because this was changed

539
00:37:33,570 --> 00:37:34,840
before the war

540
00:37:35,530 --> 00:37:38,320
it is that this is about we've

541
00:37:39,360 --> 00:37:42,590
you will be about european legacy

542
00:37:42,680 --> 00:37:45,700
and so on you should

543
00:37:45,740 --> 00:37:48,240
because you know the image is

544
00:37:49,070 --> 00:37:50,470
and so on

545
00:37:50,720 --> 00:37:54,180
the model which is

546
00:37:54,200 --> 00:37:57,030
it to be

547
00:37:59,420 --> 00:38:02,240
to these he got for model of

548
00:38:02,260 --> 00:38:03,700
well that

549
00:38:05,570 --> 00:38:07,720
why is this

550
00:38:07,780 --> 00:38:12,860
something genuinely you because you know

551
00:38:12,860 --> 00:38:14,470
that's it

552
00:38:14,490 --> 00:38:19,670
all he change three years the no example

553
00:38:19,700 --> 00:38:27,550
here i don't think there is pressure democracy growing trying to get to know what

554
00:38:27,590 --> 00:38:28,440
i'd call

555
00:38:28,440 --> 00:38:30,860
what i think they

556
00:38:30,880 --> 00:38:32,180
if the

557
00:38:32,200 --> 00:38:35,400
he invented something new

558
00:38:35,420 --> 00:38:38,360
which is short for us

559
00:38:38,380 --> 00:38:41,150
western cape

560
00:38:41,150 --> 00:38:44,400
which if anything is even more

561
00:38:44,420 --> 00:38:50,220
more than five to one but

562
00:38:50,240 --> 00:38:51,740
but we can fix

563
00:38:51,740 --> 00:38:54,150
well that will

564
00:38:56,360 --> 00:38:59,170
this is the first

565
00:38:59,220 --> 00:39:01,240
i think we should be able to come

566
00:39:02,090 --> 00:39:03,820
movie police of

567
00:39:04,130 --> 00:39:07,670
multiplicity of the market and

568
00:39:14,240 --> 00:39:15,760
proposals not

569
00:39:18,360 --> 00:39:20,550
we in iraq

570
00:39:20,590 --> 00:39:21,900
and to be a little

571
00:39:21,920 --> 00:39:26,610
who in the sense in relation to the united states

572
00:39:26,680 --> 00:39:27,940
and so on

573
00:39:27,950 --> 00:39:28,720
we are

574
00:39:28,740 --> 00:39:32,860
the station building and so the police

575
00:39:34,530 --> 00:39:37,090
the europe want to play

576
00:39:37,970 --> 00:39:42,550
we already both twenty five dollars

577
00:39:43,390 --> 00:39:44,720
is there

578
00:39:44,740 --> 00:39:50,010
when we can see that we leave the united states

579
00:39:50,510 --> 00:39:57,260
by the end the war one i think it's going to be the first part

580
00:39:57,260 --> 00:39:58,800
was when

581
00:39:59,530 --> 00:40:01,800
it is a united states

582
00:40:01,820 --> 00:40:04,340
it's of the three

583
00:40:04,360 --> 00:40:08,570
he call his book because that could lead

584
00:40:08,590 --> 00:40:10,900
the for example

585
00:40:10,920 --> 00:40:14,320
we were not less than americans for

586
00:40:14,340 --> 00:40:17,130
so called islam

587
00:40:17,130 --> 00:40:21,670
but you know for a couple of the united states

588
00:40:22,760 --> 00:40:26,340
one of the second then we can talk about

589
00:40:26,360 --> 00:40:27,110
there is no

590
00:40:27,180 --> 00:40:28,990
invasion of iraq but

591
00:40:28,990 --> 00:40:31,150
basically we like to play so

592
00:40:31,170 --> 00:40:33,030
so the which

593
00:40:35,800 --> 00:40:38,650
do not think he invented

594
00:40:38,780 --> 00:40:41,510
in that sense we have now

595
00:40:41,570 --> 00:40:44,280
apart from the dominican republic and so on

596
00:40:44,300 --> 00:40:48,070
of which i don't think we can close

597
00:40:48,070 --> 00:40:51,440
travis morgan morality problem

598
00:40:52,450 --> 00:40:53,900
i don't think

599
00:40:53,920 --> 00:40:54,940
something new

600
00:40:54,950 --> 00:40:58,200
we have two basic model

601
00:40:59,700 --> 00:41:00,800
more than

602
00:41:00,840 --> 00:41:02,840
american state liberal democrat

603
00:41:02,840 --> 00:41:07,990
that you have many gaps in a few nuclear for example here he turned seventy

604
00:41:09,070 --> 00:41:14,010
you have gaps everywhere for put all blatant paula chip so

605
00:41:14,050 --> 00:41:16,300
positive and negative information

606
00:41:16,320 --> 00:41:19,910
so it seems that you have the potential energy like that and you can have

607
00:41:19,910 --> 00:41:21,010
stayed here

608
00:41:21,030 --> 00:41:24,680
over its state and paul h paul h

609
00:41:24,700 --> 00:41:26,450
so quite existence of

610
00:41:26,590 --> 00:41:28,490
different shapes

611
00:41:28,570 --> 00:41:31,410
so they have to form a lot of experiments folks

612
00:41:31,470 --> 00:41:36,400
and especially using these exotic secondary be most people hard

613
00:41:36,410 --> 00:41:43,360
again even if they had done multistep coulomb excitation so they populated meeting

614
00:41:43,360 --> 00:41:45,780
different states have two eight three state

615
00:41:45,800 --> 00:41:47,510
and that look at the

616
00:41:47,530 --> 00:41:51,950
the different properties of this excited state for example they are found that these two

617
00:41:52,030 --> 00:41:53,910
states of the first sorry

618
00:41:53,950 --> 00:41:56,970
this zero posted the second european state

619
00:41:57,820 --> 00:41:58,760
it has

620
00:41:58,760 --> 00:42:03,300
the opposite sign for the deformation compared to the ground state so you really have

621
00:42:03,300 --> 00:42:04,510
to states

622
00:42:04,570 --> 00:42:07,740
the same energy almost the same energy

623
00:42:07,760 --> 00:42:10,110
that have different

624
00:42:12,140 --> 00:42:14,280
so this is what is compared here

625
00:42:14,320 --> 00:42:17,320
you have

626
00:42:17,530 --> 00:42:23,360
you have for ten seventy four

627
00:42:23,380 --> 00:42:27,490
you have these experimenter is some theory

628
00:42:27,510 --> 00:42:31,840
and you had with the experiment a lot of levels and with the help of

629
00:42:31,840 --> 00:42:37,760
the theory we can have construct some bands and uses the grandstand and paul banned

630
00:42:37,760 --> 00:42:39,070
all bled bands

631
00:42:39,090 --> 00:42:42,410
and here you've got my vibration high exit

632
00:42:42,430 --> 00:42:47,410
so i mean that you have a new place that can be prolate oblate and

633
00:42:47,550 --> 00:42:52,160
biaxiality is how do you can go from point a to in a continuous way

634
00:42:52,180 --> 00:42:55,930
so you have stayed associated with that movement here

635
00:42:55,970 --> 00:42:58,660
and it seemed that this explanation

636
00:42:58,680 --> 00:43:03,410
works quite well so people now do the same kind

637
00:43:03,430 --> 00:43:05,840
of calculation

638
00:43:05,840 --> 00:43:13,340
in selenium and so the neighbouring nuclei to see whether this phenomenon happens or not

639
00:43:13,360 --> 00:43:17,930
so you can see the shape coexistence when you look at the way function

640
00:43:17,930 --> 00:43:20,990
so this is the function for the different states

641
00:43:21,010 --> 00:43:26,430
so here you have fallen information here is overblown information and in between these three

642
00:43:28,070 --> 00:43:32,320
so you see that the first band here is mostly played

643
00:43:32,340 --> 00:43:37,140
the second and is mostly oblate and that you have try xntp

644
00:43:37,160 --> 00:43:38,900
for the other band

645
00:43:38,910 --> 00:43:40,030
this is one way

646
00:43:40,050 --> 00:43:44,990
to know whether what is information

647
00:43:44,990 --> 00:43:45,740
so now

648
00:43:45,840 --> 00:43:49,470
what about new structure in nuclei

649
00:43:49,530 --> 00:43:54,410
so i told you about this leads him into the nucleus where they have observed

650
00:43:54,410 --> 00:43:57,590
that these nucleus do have a very large

651
00:43:59,820 --> 00:44:05,220
compared to the other one and rapidly polio that it's about minute so why is

652
00:44:05,220 --> 00:44:08,760
because you have medium nine plus two neutrons

653
00:44:08,820 --> 00:44:12,780
so if you take away one of the subsystem for example you take away these

654
00:44:12,780 --> 00:44:16,180
region nine you have two neutral it's bound

655
00:44:16,200 --> 00:44:20,880
if you take away when you try new medium then and it's bound

656
00:44:21,550 --> 00:44:24,160
that's why these medium it is found another

657
00:44:24,340 --> 00:44:28,590
neighbouring so that's why when you increase the neutron number here

658
00:44:28,610 --> 00:44:30,590
you have need to mine

659
00:44:30,610 --> 00:44:33,740
nothing here lead to elephant

660
00:44:33,760 --> 00:44:39,380
so all these properties are understood here to see whether

661
00:44:39,950 --> 00:44:45,430
what are the as the properties of the external no problems if there should be

662
00:44:45,430 --> 00:44:51,240
what are the most probable quantum number they should have in order to be

663
00:44:52,240 --> 00:44:55,660
hello nicky form handle so these

664
00:44:55,660 --> 00:44:57,260
picture now

665
00:44:57,280 --> 00:44:59,900
if if the rank are

666
00:44:59,920 --> 00:45:01,740
was two

667
00:45:01,760 --> 00:45:05,350
this is the this is the number of

668
00:45:05,390 --> 00:45:07,300
payment variables right

669
00:45:07,320 --> 00:45:09,250
because the count of the payments

670
00:45:09,370 --> 00:45:14,800
so how many free variables

671
00:45:14,810 --> 00:45:17,800
well you know it's true right

672
00:45:17,810 --> 00:45:25,300
what is the fourier matrix that's and rows and columns and variables that means

673
00:45:25,320 --> 00:45:27,910
with the rank are

674
00:45:27,940 --> 00:45:29,120
how many

675
00:45:29,130 --> 00:45:31,270
three variables and we got left

676
00:45:31,280 --> 00:45:35,630
if you are of the variables are pivot variables we have n

677
00:45:37,520 --> 00:45:39,000
in this case

678
00:45:39,020 --> 00:45:40,980
four minus two

679
00:45:41,030 --> 00:45:46,880
three merit

680
00:45:46,980 --> 00:45:50,040
you see that

681
00:45:50,060 --> 00:45:53,520
first of all

682
00:45:53,570 --> 00:45:56,240
we get a night we get clean answers here

683
00:45:56,300 --> 00:45:57,870
we get are

684
00:45:57,920 --> 00:46:03,070
and variables so there really were are equations here

685
00:46:03,080 --> 00:46:08,580
they looked like three equations but there are really only two independent equation

686
00:46:09,630 --> 00:46:12,400
there were and minus or

687
00:46:12,460 --> 00:46:14,790
variables that we could choose freely

688
00:46:14,800 --> 00:46:20,000
and we gave them no special zero one of its values and we got the

689
00:46:20,000 --> 00:46:22,130
special solution

690
00:46:25,130 --> 00:46:28,530
for me we could stop at that point

691
00:46:28,600 --> 00:46:31,870
that gives you a complete algorithm

692
00:46:33,710 --> 00:46:37,560
all the solutions to a x equals zero

693
00:46:41,120 --> 00:46:44,550
again you do elimination

694
00:46:44,590 --> 00:46:47,780
going on word when the column

695
00:46:47,790 --> 00:46:53,420
when when there's nothing to be done on one column you just continue

696
00:46:53,440 --> 00:46:58,790
that this number are the number of that is crucial

697
00:46:59,890 --> 00:47:03,600
leaves and minus are free variables

698
00:47:03,610 --> 00:47:05,870
which you give values are one two

699
00:47:05,890 --> 00:47:10,950
i would like to take one more step

700
00:47:11,030 --> 00:47:14,890
i would like to clean up this matrix even more

701
00:47:14,950 --> 00:47:17,940
so now i'm going to go to this is in its

702
00:47:17,990 --> 00:47:20,940
this is in echelon form

703
00:47:20,990 --> 00:47:23,070
upper triangular if you like

704
00:47:23,080 --> 00:47:27,440
i want to go one more step to make it as good as it can

705
00:47:29,160 --> 00:47:32,980
so now i'm going to speak about the reduced

706
00:47:34,880 --> 00:47:38,890
OK so now i'm going to speak about the matrix are which is the

707
00:47:47,620 --> 00:47:49,520
so what does that mean

708
00:47:49,580 --> 00:47:51,910
that means i just

709
00:47:51,940 --> 00:47:56,480
i can i can do work harder on you so let me start

710
00:47:56,490 --> 00:48:00,170
i suppose i got as far as you

711
00:48:00,250 --> 00:48:04,910
which was good

712
00:48:11,100 --> 00:48:12,850
notice how the

713
00:48:12,900 --> 00:48:16,580
the role of zeros appeared i didn't comment on that

714
00:48:16,580 --> 00:48:18,590
but i think i should

715
00:48:18,640 --> 00:48:22,230
the role of zeros appearing because

716
00:48:22,290 --> 00:48:26,360
row three with a combination of rows one and two

717
00:48:26,370 --> 00:48:30,200
and the elimination discovered that

718
00:48:30,220 --> 00:48:33,320
when we get row of zeros that's telling us

719
00:48:33,340 --> 00:48:38,540
that they are the that of the original role that was there

720
00:48:38,590 --> 00:48:41,490
it was a combination of other rows

721
00:48:42,430 --> 00:48:45,250
elimination not

722
00:48:45,300 --> 00:48:47,530
OK so we got this far

723
00:48:47,580 --> 00:48:50,070
now how can i clean it up further

724
00:48:50,090 --> 00:48:51,350
i can do

725
00:48:51,470 --> 00:48:54,310
elimination upwards

726
00:48:54,330 --> 00:48:57,300
i can get a zero above the pivot

727
00:48:57,320 --> 00:49:02,350
so this reduced row echelon form has zero

728
00:49:03,340 --> 00:49:06,690
and below

729
00:49:06,720 --> 00:49:09,410
the payments

730
00:49:09,470 --> 00:49:14,330
so let me do that

731
00:49:14,380 --> 00:49:19,270
so now subtract one of this from the robot that will leave

732
00:49:19,280 --> 00:49:22,980
was zero and minus two in there

733
00:49:24,820 --> 00:49:27,890
that's good

734
00:49:31,050 --> 00:49:36,200
and i can clean it up even one more step i can make the payments

735
00:49:36,230 --> 00:49:39,390
the payments i'm going to make it will one

736
00:49:39,440 --> 00:49:42,140
because i can divide equation two

737
00:49:42,160 --> 00:49:44,250
by the pivot

738
00:49:44,300 --> 00:49:45,130
that will

739
00:49:45,160 --> 00:49:47,030
change the solutions

740
00:49:47,030 --> 00:49:48,810
so let me do that

741
00:49:48,860 --> 00:49:51,870
and then i really i'm ready to stop one two

742
00:49:51,880 --> 00:49:53,950
zero minus two

743
00:49:54,020 --> 00:49:58,340
zero zero one two i divided

744
00:49:58,370 --> 00:50:01,420
the second equation by two

745
00:50:01,440 --> 00:50:03,480
because now i have all one

746
00:50:03,490 --> 00:50:05,450
in the paper

747
00:50:06,630 --> 00:50:08,280
zero small

748
00:50:09,230 --> 00:50:17,650
this is my matrix are

749
00:50:17,700 --> 00:50:19,570
i guess i'm hoping

750
00:50:19,570 --> 00:50:22,130
sign of linear combination of the weak classifiers

751
00:50:22,140 --> 00:50:26,760
OK so i need three pieces of notation

752
00:50:26,770 --> 00:50:29,830
so you have to remember what the string three things are

753
00:50:29,880 --> 00:50:31,820
number one is the matrix

754
00:50:31,840 --> 00:50:35,410
in the matrix and the binary matrix all the entries are either plus one to

755
00:50:35,410 --> 00:50:39,630
minus one and the matrix is going to contain all the information we need about

756
00:50:39,630 --> 00:50:43,170
the weak learning algorithm and about the training data so

757
00:50:43,220 --> 00:50:46,010
in our case the rural access

758
00:50:46,030 --> 00:50:50,130
it's going to be the training example axis so we get sort of one

759
00:50:50,160 --> 00:50:51,720
news article for each

760
00:50:55,270 --> 00:50:56,630
on the column axis

761
00:50:56,650 --> 00:50:59,250
we get the weak classifiers

762
00:50:59,270 --> 00:51:01,270
so let me just tell you

763
00:51:01,280 --> 00:51:05,830
again what i'm doing i'm taking the weak learning algorithm and everything is inside

764
00:51:05,850 --> 00:51:10,610
and i'm enumerating every possible weak classifier which the weak learning algorithm can produce

765
00:51:10,630 --> 00:51:14,650
so there could be a huge number of these weak classifiers

766
00:51:14,670 --> 00:51:19,310
and in fact in many to actually be enumerated so i'm never going actually enumerate

767
00:51:19,330 --> 00:51:20,960
this matrix in practice

768
00:51:21,000 --> 00:51:25,120
but since i'm doing some theoretical work work i can now consider the the matrix

769
00:51:25,120 --> 00:51:27,830
just i'm going to use the sort of tool

770
00:51:27,850 --> 00:51:32,750
OK and because this matrix acts as the only input to adaboost that contains all

771
00:51:32,750 --> 00:51:36,550
the information we need about the weak learning algorithm about the training data and the

772
00:51:36,550 --> 00:51:39,270
entries of this matrix are plus one to minus one

773
00:51:39,290 --> 00:51:42,410
the entries plus one if we classifier j

774
00:51:42,420 --> 00:51:45,720
classifies training example i correctly negative one otherwise

775
00:51:45,760 --> 00:51:47,140
so for example

776
00:51:47,160 --> 00:51:49,230
if we have this news article right here

777
00:51:49,250 --> 00:51:53,680
and it it but it contains the term actor but it's not an article about

778
00:51:53,680 --> 00:51:59,380
entertainment that we missed so in my mind one so if the article does contain

779
00:51:59,380 --> 00:52:01,480
the term actor and

780
00:52:01,500 --> 00:52:06,540
the article really is about entertainment imagine plus one and there are two more

781
00:52:06,550 --> 00:52:10,180
there are two more things that two more examples i can state

782
00:52:11,840 --> 00:52:16,600
they had matrix and now i'm going to introduce the this notation dt dt are

783
00:52:16,600 --> 00:52:20,930
the distribution with over the training examples at twenty third member at each iteration we

784
00:52:20,930 --> 00:52:24,160
have a set of weights one week for each training examples i'm just going to

785
00:52:24,160 --> 00:52:28,000
write this without the which are always nonnegative and add up to one for the

786
00:52:28,120 --> 00:52:31,470
discrete probability distribution over the training examples

787
00:52:31,490 --> 00:52:36,640
OK so the last thing i want to introduce is the vector lambda

788
00:52:36,660 --> 00:52:41,220
and the windows are the coefficients of the weak classifiers for this linear combination

789
00:52:41,230 --> 00:52:45,010
now i remember my goal here is to construct this function so that

790
00:52:45,030 --> 00:52:47,060
its decision boundary

791
00:52:47,080 --> 00:52:49,100
is it good you know

792
00:52:49,160 --> 00:52:50,270
is a good

793
00:52:50,280 --> 00:52:55,250
so the decision boundaries good decision boundary so what i'm trying to do really is

794
00:52:55,250 --> 00:52:59,510
i'm trying to find these lambda so that the function f has a finite decision

795
00:53:00,730 --> 00:53:03,030
OK so just to recap all the notation

796
00:53:03,050 --> 00:53:04,450
so the

797
00:53:04,470 --> 00:53:08,070
the matrix is this very large binary matrix which

798
00:53:08,150 --> 00:53:13,050
which has all the information we need about the weak classifiers and about the training

799
00:53:14,710 --> 00:53:16,280
and at each iteration

800
00:53:16,320 --> 00:53:19,060
we just two things in adaboost

801
00:53:19,080 --> 00:53:21,600
we just away from the training instances

802
00:53:21,620 --> 00:53:26,230
and we just the coefficients on the weak classifiers to to form the combined classifier

803
00:53:26,250 --> 00:53:30,440
so we iterate until the blue in the face adjusting these two quantities at every

804
00:53:31,430 --> 00:53:35,710
and then the and we sped the coefficients for the final combined classifier

805
00:53:37,900 --> 00:53:42,840
i'm trying to find the final combine classifiers

806
00:53:43,430 --> 00:53:47,690
because know that's my goal i want to have a final classifier so you think

807
00:53:47,830 --> 00:53:49,560
be interested in these lambda t

808
00:53:49,570 --> 00:53:53,570
because i want to know how the entities convert to what they converge to one

809
00:53:54,150 --> 00:53:56,110
whether the

810
00:53:56,130 --> 00:53:59,470
the decision whether function

811
00:53:59,530 --> 00:54:04,330
made from lambda final has the maximum margin solution i don't know whether whether defined

812
00:54:04,330 --> 00:54:06,230
going to give me maximum margin solution

813
00:54:06,250 --> 00:54:10,520
but the problem is that that entities are really really difficult

814
00:54:10,570 --> 00:54:13,980
try understand the convergence properties of those lambda t is not something i want to

815
00:54:13,980 --> 00:54:19,820
do because they spiral up to infinity and they just there there's a big mess

816
00:54:19,880 --> 00:54:24,860
so what i found was that studying the DT's it's actually much easier

817
00:54:24,870 --> 00:54:26,430
and much more

818
00:54:26,450 --> 00:54:30,180
much much more elegant because they do some very nice things

819
00:54:30,200 --> 00:54:34,220
so what you should expect is that i'm going to be studying the evolution of

820
00:54:34,220 --> 00:54:37,440
the DT is to try to understand what is going to do

821
00:54:37,460 --> 00:54:39,890
and in fact

822
00:54:39,920 --> 00:54:41,670
the DT's are going to

823
00:54:41,680 --> 00:54:45,750
two things like cycle around

824
00:54:45,790 --> 00:54:50,110
and in the case cases where the duty cycle around the lambda is the normalized

825
00:54:50,110 --> 00:54:52,790
versions of the windows are going to converge so i'm going to be able to

826
00:54:52,790 --> 00:54:57,750
understand completely how my are converging by understanding the convergence of the deities

827
00:54:57,770 --> 00:55:00,520
so she expected data cycle in the limit

828
00:55:00,540 --> 00:55:02,630
normalized lambda to converge

829
00:55:02,660 --> 00:55:06,730
OK so this one more bit of notation i want to tell you which is

830
00:55:06,730 --> 00:55:10,310
the edge not going to find it for you but the edge is can be

831
00:55:10,310 --> 00:55:17,000
made from DT and from and the edges the number between zero and one

832
00:55:20,290 --> 00:55:25,120
so we have the candidate from from data from and told you that tells us

833
00:55:25,120 --> 00:55:27,630
how well we classifiers doing in each iteration

834
00:55:27,650 --> 00:55:31,430
so if the weak classifiers did particularly well at iteration t

835
00:55:32,570 --> 00:55:34,600
with respect to the way the training examples

836
00:55:34,610 --> 00:55:37,610
and the edges can be large otherwise it is going to be small

837
00:55:37,660 --> 00:55:42,100
so the edge tells us how well we classifiers doing at each iteration

838
00:55:45,640 --> 00:55:49,080
so i remember what we're trying to do we're trying to construct we're trying to

839
00:55:49,080 --> 00:55:55,180
find trying to understand lambda finals because our decision function is made from a linear

840
00:55:56,440 --> 00:55:58,830
you know the vector lambda final

841
00:55:59,000 --> 00:56:02,650
that's because of coefficients for the linear combination we want to understand what they converge

842
00:56:02,660 --> 00:56:05,980
to in fact what we want to find is whether or not at this produces

843
00:56:05,980 --> 00:56:07,570
the maximum margin solution

844
00:56:07,580 --> 00:56:09,330
so in other words what we want to do this one

845
00:56:09,340 --> 00:56:13,820
finally in the final and we want to understand whether or not it maximizes the

846
00:56:13,820 --> 00:56:16,530
margin and the margin is defined this way

847
00:56:18,340 --> 00:56:22,030
i just want to tell you a a little bit about this notation system hx

848
00:56:22,030 --> 00:56:25,120
this is a vector so when you multiply them together you get infected

849
00:56:25,170 --> 00:56:28,800
this is the i th component that so this is just a number

850
00:56:28,800 --> 00:56:33,690
so i want to find out whether or not adaboost produces the maximum lambda of

851
00:56:33,710 --> 00:56:37,130
this quantity i want to know whether it maximizes the margin

852
00:56:38,190 --> 00:56:40,420
OK so what i'm going to do now

853
00:56:40,490 --> 00:56:44,520
i'm going to change topics and i'm going to prove this theorem for you

854
00:56:44,520 --> 00:56:49,370
released give some intuition about about the truth

855
00:56:52,180 --> 00:56:56,130
as i mentioned that this is difficult to analyse because the margin was increased

856
00:56:56,180 --> 00:56:59,390
so what we're going to do is we're going to take the dynamical systems approach

857
00:56:59,390 --> 00:57:04,270
and reduce adaboost to a dynamical system in order to understand its convergence properties and

858
00:57:04,270 --> 00:57:08,050
when we analyse the dynamical system in simple cases we're going to find these remarkable

859
00:57:08,050 --> 00:57:14,060
stable cycles and luckily for when we find stable cycles of the convergence properties can

860
00:57:14,060 --> 00:57:16,110
be completely understood

861
00:57:16,170 --> 00:57:19,470
OK so the key to answering the question was really of example

862
00:57:19,480 --> 00:57:23,610
in which at this convergence properties can be completely understood that something that we just

863
00:57:23,610 --> 00:57:26,990
didn't have before and this was kind of like a black box you know you

864
00:57:26,990 --> 00:57:28,670
just stuff into it and then

865
00:57:28,790 --> 00:57:32,050
i mean you have all these no guarantees the right told you about but as

866
00:57:32,050 --> 00:57:33,820
far as the marginal goes

867
00:57:33,820 --> 00:57:35,840
all bets are off

868
00:57:35,860 --> 00:57:39,840
OK so here is the dynamical system that promise you

869
00:57:39,860 --> 00:57:41,530
it looks like this

870
00:57:41,570 --> 00:57:44,490
does that look anything like the original adaboost algorithm

871
00:57:44,490 --> 00:57:46,180
is blocked by the

872
00:57:46,200 --> 00:57:52,700
so that sounds a bit intuitive still we're trying to think of dependency is passing

873
00:57:52,700 --> 00:57:54,260
from x to y

874
00:57:54,260 --> 00:57:57,890
and if every path gets blocked by the

875
00:57:58,700 --> 00:58:01,120
x and y are independent of each other

876
00:58:02,330 --> 00:58:03,850
so now let's look at sea

877
00:58:04,830 --> 00:58:08,790
dependency is blocked by d

878
00:58:08,810 --> 00:58:11,390
a path is blocked by p

879
00:58:12,410 --> 00:58:15,370
there's is a node w on the path

880
00:58:15,390 --> 00:58:20,200
such that it satisfies one of two conditions

881
00:58:20,620 --> 00:58:24,010
the first condition is that

882
00:58:24,060 --> 00:58:27,830
w has converging arrows along the path

883
00:58:27,850 --> 00:58:29,830
in other words

884
00:58:29,850 --> 00:58:35,890
the path goes down to w and then from w

885
00:58:35,950 --> 00:58:41,330
and then neither w nor its descendants are observed in other words are in the

886
00:58:41,330 --> 00:58:43,080
set b

887
00:58:43,100 --> 00:58:47,990
that will block a path

888
00:58:49,640 --> 00:58:53,930
if w does not have converging arrows along the path

889
00:58:53,970 --> 00:58:55,510
so you go

890
00:58:55,540 --> 00:58:58,970
i right through w this way or that way

891
00:59:01,120 --> 00:59:02,470
w is

892
00:59:04,620 --> 00:59:06,290
w is

893
00:59:06,490 --> 00:59:08,910
in the set b

894
00:59:10,470 --> 00:59:13,120
if that's the case

895
00:59:13,140 --> 00:59:14,950
then the

896
00:59:14,970 --> 00:59:18,390
separate x from y

897
00:59:20,240 --> 00:59:22,450
the core area of this is

898
00:59:23,430 --> 00:59:28,080
we can find out what the markov boundary of x is in the markov boundary

899
00:59:28,080 --> 00:59:29,390
effects is

900
00:59:29,410 --> 00:59:32,910
the union of the parent of x

901
00:59:32,910 --> 00:59:34,390
the children of that

902
00:59:34,390 --> 00:59:37,330
and the parents of the children of x

903
00:59:37,620 --> 00:59:42,430
this is going to take a couple minutes how many people have encountered the separation

904
00:59:44,100 --> 00:59:47,600
OK the same people who don't have a lot of good sign

905
00:59:47,600 --> 00:59:53,160
so let's look at some examples of the separation

906
00:59:53,260 --> 00:59:56,450
again just looking at this graph

907
00:59:56,490 --> 00:59:57,580
all right

908
00:59:59,890 --> 01:00:02,350
it is independent of the

909
01:00:02,350 --> 01:00:05,350
it is marginally independent of b

910
01:00:05,370 --> 01:00:08,970
conditioned on the being the empty set

911
01:00:11,390 --> 01:00:15,040
let's look at every path between a and b

912
01:00:15,160 --> 01:00:21,810
so essentially the same and is all the path between a and b are blocked

913
01:00:26,330 --> 01:00:27,810
the past

914
01:00:30,060 --> 01:00:32,640
is blocked by c

915
01:00:32,660 --> 01:00:33,910
why is that

916
01:00:33,970 --> 01:00:35,850
let's look at this

917
01:00:40,350 --> 01:00:43,060
has converging arrows along the path

918
01:00:43,100 --> 01:00:45,260
great eight db

919
01:00:45,350 --> 01:00:50,970
and neither seen nor his descendants are observed are in the set b remember the

920
01:00:50,970 --> 01:00:52,700
set that b is the empty set

921
01:00:55,640 --> 01:00:58,700
that only block the path between a and b

922
01:00:58,720 --> 01:01:01,330
the direct path between a and b

923
01:01:01,390 --> 01:01:02,200
but now

924
01:01:03,540 --> 01:01:07,080
we we really need to look at all paths between a and b so

925
01:01:07,100 --> 01:01:09,970
if we examine the past eight the

926
01:01:11,540 --> 01:01:13,890
a b b b

927
01:01:16,430 --> 01:01:20,910
the path between a and b

928
01:01:20,930 --> 01:01:25,910
along a b b is the has converging arrows

929
01:01:25,970 --> 01:01:29,810
and neither d nor and foreign set b

930
01:01:29,830 --> 01:01:31,010
all right

931
01:01:36,040 --> 01:01:41,390
by examining these we can tell that a and b are marginally independent let's look

932
01:01:41,390 --> 01:01:43,260
at another

933
01:01:44,830 --> 01:01:47,890
it is not the case for this graph that

934
01:01:50,100 --> 01:01:53,660
it is independent of b given c

935
01:01:53,700 --> 01:01:55,990
and let's consider that well

936
01:01:58,260 --> 01:01:59,990
the b

937
01:02:00,010 --> 01:02:02,260
in is path between

938
01:02:02,280 --> 01:02:04,220
a and b

939
01:02:04,280 --> 01:02:07,470
and that path is not blocked

940
01:02:09,180 --> 01:02:10,390
so therefore

941
01:02:13,760 --> 01:02:15,080
between a and b

942
01:02:15,080 --> 01:02:17,680
you can think of it that way in europe with

943
01:02:18,450 --> 01:02:21,240
intuitively from our rain sprinkler example

944
01:02:21,260 --> 01:02:22,470
if you observe

945
01:02:22,490 --> 01:02:25,060
the effect

946
01:02:25,080 --> 01:02:27,410
two independent causes

947
01:02:28,490 --> 01:02:30,620
conditional that observation

948
01:02:30,640 --> 01:02:31,560
you know

949
01:02:31,580 --> 01:02:34,990
the cost become dependent on each other

950
01:02:34,990 --> 01:02:39,180
here's another intuitive way of doing that

951
01:02:39,240 --> 01:02:41,370
if if i tell you

952
01:02:41,410 --> 01:02:43,450
i'm gonna draw

953
01:02:46,010 --> 01:02:51,100
two random numbers between one and ten independently

954
01:02:51,100 --> 01:02:57,220
is because most of mainstream Bayesian statistics is about variables that uncertain variables parameters

955
01:02:57,220 --> 01:03:02,200
that are real valued and this this message-passing type of algorithms seem to be much more

956
01:03:02,200 --> 01:03:05,390
suitable when you have discrete systems but we do use them we do know

957
01:03:05,420 --> 01:03:11,260
a bit about them and we use them in a probability propagation for example in forensic

958
01:03:11,260 --> 01:03:18,900
genetics so this this is one particular class of model of computations that I use quite a

959
01:03:18,900 --> 01:03:26,680
bit if you have a graphical model and it's what we call decomposable

960
01:03:26,780 --> 01:03:35,520
decomposable means that there are no loops of four or more vertices which don't have a caught

961
01:03:35,520 --> 01:03:39,540
so this for example here is the harish one there is a half there is indeed a

962
01:03:39,540 --> 01:03:46,360
circuit there or for but it's got a cord across sure cuts it such gross

963
01:03:46,400 --> 01:03:51,810
very you know very conveniently handled in in message-passing type algorithms because there's a

964
01:03:51,940 --> 01:04:00,520
a sort of dual graph called the the junction tree which hello yes

965
01:04:00,520 --> 01:04:05,900
yes exactly yes the same thing and what this graph is by the way this graph is a

966
01:04:05,900 --> 01:04:10,160
graph that encodes the conditional independences in the model I've been trying not to say

967
01:04:10,160 --> 01:04:13,980
too much about graphical modeling because you're gonna get a lot about that next

968
01:04:13,980 --> 01:04:20,620
week but this is dual graph essentially is is captures the structure of the

969
01:04:20,620 --> 01:04:25,440
graph in a in a in a slightly abstract way in a new graph whose nodes

970
01:04:25,440 --> 01:04:30,800
are subsets of vertices clicks in fact of the original graph and then on such graphs

971
01:04:31,280 --> 01:04:36,600
there are nice algorithms it's sometimes called the junction tree algorithm or the Lauritzen

972
01:04:36,600 --> 01:04:43,160
Spiegel halter algorithm which allow you to propagate probabilities around these graphs in discrete systems and

973
01:04:43,160 --> 01:04:51,900
that's how base nets work and you got is very nice simple computations which involve marginalizing multiplying and

974
01:04:51,900 --> 01:04:57,120
then the after it properly schedule the messages around the graph and you get the essentially the

975
01:04:57,120 --> 01:05:04,100
exact computation in Bayesian systems where the variables are all discrete and such it

976
01:05:04,120 --> 01:05:08,280
you may well of met these algorithms in a more general context and their their

977
01:05:08,280 --> 01:05:12,520
their ideas like  people leave propagation and so on where algorithm

978
01:05:12,520 --> 01:05:18,400
is bit like this I used in situations where they don't converge even in

979
01:05:18,500 --> 01:05:24,180
the convergen of final time but but those are the methods are not mainstream in in

980
01:05:24,180 --> 01:05:30,060
Bayesian statistics because of the discreteness finally I'll say a few rude things about variational

981
01:05:30,060 --> 01:05:41,140
methods excuse me I'm slightly joking but the obviously variational

982
01:05:41,140 --> 01:05:48,660
methods are capable of producing a powerful inferences very fast and and there are

983
01:05:48,660 --> 01:05:57,120
fantastic thing to have available to us and and yeah they in any competition in

984
01:05:57,120 --> 01:06:01,020
any model in which you can do both the variational calculation or an MCMC

985
01:06:01,020 --> 01:06:05,380
calculation yeah we realy don't need to have a cometition it's obvious who's gonna

986
01:06:05,380 --> 01:06:12,180
win but I just have a couple of concerns about it such such

987
01:06:12,180 --> 01:06:19,920
methods are sometimes characterize as being approximations to the Bayesian inference and indeed well there're various

988
01:06:19,920 --> 01:06:24,440
versions of these but one I mean one one one standard one is to essentially minimize

989
01:06:24,440 --> 01:06:29,380
the call back liable distance between the true posterior and the best approximation in a certain

990
01:06:29,380 --> 01:06:38,000
function class and that's the the classic mean-field type approximation well it's several points about that

991
01:06:38,000 --> 01:06:42,740
first of all this this is indeed a best approximation but it's the best approximation in terms

992
01:06:42,740 --> 01:06:46,960
of the value of the function and when we are doing inference what we're looking for is

993
01:06:46,960 --> 01:06:52,560
is essentially something in the horizontal dimension of that graph and the best of

994
01:06:52,560 --> 01:06:58,840
my knowledge typically that approximation results don't quantify how far away for example the

995
01:06:58,840 --> 01:07:06,500
maximum is of the approximation compared to the maximum of the posterior so somehow

996
01:07:06,500 --> 01:07:09,640
try to find well calibrated means but

997
01:07:09,690 --> 01:07:11,120
i will say that

998
01:07:13,230 --> 01:07:14,940
OK so

999
01:07:14,960 --> 01:07:16,710
so much

1000
01:07:17,420 --> 01:07:19,420
let x one

1001
01:07:19,480 --> 01:07:25,290
the x and the test examples

1002
01:07:25,870 --> 01:07:28,980
where is

1003
01:07:29,060 --> 01:07:32,390
predicted probabilities

1004
01:07:34,560 --> 01:07:36,500
probability of

1005
01:07:36,520 --> 01:07:40,330
y equals one given x y

1006
01:07:40,390 --> 01:07:44,080
he calls the i

1007
01:07:53,060 --> 01:07:56,520
as the the sum of the i

1008
01:07:56,540 --> 01:07:58,640
of the i

1009
01:08:01,020 --> 01:08:02,730
let t e

1010
01:08:02,770 --> 01:08:04,960
the actual

1011
01:08:08,710 --> 01:08:13,170
y equals one cases

1012
01:08:13,170 --> 01:08:16,100
in the set

1013
01:08:16,140 --> 01:08:19,120
one day

1014
01:08:19,330 --> 01:08:21,100
so much so

1015
01:08:21,170 --> 01:08:22,560
we have

1016
01:08:22,580 --> 01:08:24,830
a hundred test examples

1017
01:08:25,620 --> 01:08:27,230
make predictions

1018
01:08:27,230 --> 01:08:29,960
the i three test examples

1019
01:08:29,980 --> 01:08:31,000
and then we

1020
01:08:31,020 --> 01:08:34,620
add up all those predicted probabilities

1021
01:08:35,520 --> 01:08:40,410
then later on we get to find out the truth about the test examples

1022
01:08:40,440 --> 01:08:45,270
and we find and we discovered that among these and test examples t was the

1023
01:08:45,270 --> 01:08:46,640
actual number of

1024
01:08:48,480 --> 01:08:53,210
so what can we say about the relationship between s and t

1025
01:08:53,250 --> 01:08:59,250
if the probabilities are well calibrated

1026
01:08:59,310 --> 01:09:00,210
should be

1027
01:09:01,730 --> 01:09:03,140
plus or minus

1028
01:09:03,190 --> 01:09:04,810
random noise

1029
01:09:04,830 --> 01:09:06,140
so the

1030
01:09:06,140 --> 01:09:11,830
the expected value of t

1031
01:09:13,810 --> 01:09:17,230
and this is

1032
01:09:17,230 --> 01:09:20,080
and this is something that you can only say

1033
01:09:20,850 --> 01:09:22,910
the probabilities are well calibrated

1034
01:09:22,910 --> 01:09:30,250
and so this is a big advantage of having well calibrated examples and the the

1035
01:09:30,270 --> 01:09:35,270
scenario that i think illustrates very well so you know

1036
01:09:35,290 --> 01:09:38,190
airlines always overbooked flights

1037
01:09:38,210 --> 01:09:39,810
and my

1038
01:09:39,830 --> 01:09:44,250
but then they need to make a decision of how much the overbooked flight

1039
01:09:44,250 --> 01:09:47,560
if a hundred seats on the plane should set one hundred ten tickets also should

1040
01:09:47,560 --> 01:09:49,290
use one hundred twenty tickets

1041
01:09:49,310 --> 01:09:52,140
and love

1042
01:09:52,150 --> 01:09:59,910
so in order to make that decision they really need to estimate for each flight

1043
01:09:59,910 --> 01:10:01,100
how many people

1044
01:10:01,190 --> 01:10:03,060
they're going to be no shows

1045
01:10:04,890 --> 01:10:09,190
so one way to do that and because they have lots of training data about

1046
01:10:09,190 --> 01:10:13,480
your millions of past passengers and freight sparse passenger they know whether that person

1047
01:10:13,520 --> 01:10:14,710
was the notion

1048
01:10:14,730 --> 01:10:21,310
and they know there is characteristics of these passengers in particular they no behavioral characteristics

1049
01:10:21,310 --> 01:10:26,120
like the passenger buys tickets the same day the previous week the previous month more

1050
01:10:26,120 --> 01:10:27,910
than a month earlier and so on

1051
01:10:27,960 --> 01:10:29,310
and so

1052
01:10:29,350 --> 01:10:31,540
what they can do is among

1053
01:10:31,600 --> 01:10:33,270
build a classifier

1054
01:10:33,290 --> 01:10:37,810
they can train a classifier to try and predict for each person for each flight

1055
01:10:37,810 --> 01:10:40,370
is this person going to be no show

1056
01:10:43,270 --> 01:10:45,770
and i'm sure they could train decision trees

1057
01:10:45,790 --> 01:10:48,440
and get some reasonable accuracy

1058
01:10:48,460 --> 01:10:51,170
that you know if it's a business process

1059
01:10:51,170 --> 01:10:53,370
and they bought their tickets

1060
01:10:53,370 --> 01:10:54,870
a day before

1061
01:10:54,890 --> 01:10:57,460
then they are more likely to be a no show

1062
01:10:57,480 --> 01:11:01,870
and if they bought their ticket one hour before they're less likely to be show

1063
01:11:01,870 --> 01:11:04,750
so you could come up with some pretty good predictions about who is going to

1064
01:11:04,750 --> 01:11:05,810
be you know show

1065
01:11:06,600 --> 01:11:08,920
then i don't really care about that

1066
01:11:08,920 --> 01:11:12,690
they don't really care about who know show they just care about how many no-shows

1067
01:11:12,690 --> 01:11:13,690
there are

1068
01:11:16,520 --> 01:11:19,670
this is so this is the number they really care about

1069
01:11:19,690 --> 01:11:24,270
and then this is the estimate they would like to get so what they really

1070
01:11:24,270 --> 01:11:26,210
want to do is

1071
01:11:26,270 --> 01:11:29,410
run the predictive model on every passenger

1072
01:11:29,420 --> 01:11:34,040
was booked on the plane then just add up the probably being no-shows and that's

1073
01:11:34,060 --> 01:11:36,960
the estimate of the total number of no-shows

1074
01:11:37,000 --> 01:11:39,100
which is what really matters

1075
01:11:39,870 --> 01:11:44,250
but they can only do this if the

1076
01:11:44,250 --> 01:11:46,810
classifier is

1077
01:11:46,830 --> 01:11:51,560
giving well calibrated probabilities you you can't add up the numerical scores you get a

1078
01:11:51,560 --> 01:11:54,980
support vector machine and get an estimate of the number of no-shows

1079
01:12:04,890 --> 01:12:11,100
so there is a fair amount of research on how to convert the output of

1080
01:12:11,100 --> 01:12:19,170
an SVM classifier into being well calibrated probability and the most widely used method is

1081
01:12:19,940 --> 01:12:22,410
take the output of the SVM

1082
01:12:22,480 --> 01:12:24,080
use it

1083
01:12:25,790 --> 01:12:29,920
a single feature so represented example by just one feature

1084
01:12:29,920 --> 01:12:34,730
which is actually an output and then train a one-dimensional just regression on top of

1085
01:12:35,640 --> 01:12:39,520
and that works some quite well in practice

1086
01:12:48,910 --> 01:12:53,410
yes so linear SVM the very closely related logistic regression

1087
01:12:53,460 --> 01:12:57,120
in fact they use the same month

1088
01:12:57,140 --> 01:13:00,960
sort of a linear transformation of the input examples

1089
01:13:01,020 --> 01:13:05,330
but then the key difference is that they actually use a different objective function for

1090
01:13:05,330 --> 01:13:08,080
training so for for training

1091
01:13:08,080 --> 01:13:11,620
with just regression with doing maximum conditional likelihood

1092
01:13:11,690 --> 01:13:16,370
the support vector machines are doing maximum something else

1093
01:13:16,460 --> 01:13:21,770
which is the loss function that involves the loss function is called the hinge loss

1094
01:13:21,770 --> 01:13:23,120
so actually

1095
01:13:24,000 --> 01:13:26,790
support vector machines are good illustration

1096
01:13:26,920 --> 01:13:29,080
of how

1097
01:13:29,080 --> 01:13:33,540
conditional likelihood maximisation i'm not the only possible induction principles

1098
01:13:33,560 --> 01:13:36,750
they make a lot of sense that widely used very useful

1099
01:13:36,770 --> 01:13:40,790
but i had an example with a mixture of two gaussians a show the conditional

1100
01:13:40,870 --> 01:13:46,540
maximum likelihood has pathologies and support vector machines show there is a reasonable alternative

1101
01:13:47,320 --> 01:13:51,790
but support vector machines don't give you well calibrated probabilities and so that you can

1102
01:13:51,790 --> 01:13:54,520
put it just rational top of the support vector machine

1103
01:13:54,540 --> 01:13:58,670
and that works well in practice the doing the just regression directly works well in

1104
01:13:58,670 --> 01:14:00,210
practice also

1105
01:14:00,270 --> 01:14:05,080
so that

1106
01:14:06,330 --> 01:14:08,890
i thought there was one more thing i wanted to say

1107
01:14:12,640 --> 01:14:15,120
so actually so

1108
01:14:16,210 --> 01:14:19,140
a fourth advantage of the just regression

1109
01:14:19,150 --> 01:14:22,620
is that it handles

1110
01:14:22,620 --> 01:14:28,190
unbalanced training data

1111
01:14:28,960 --> 01:14:30,710
there are lots of

1112
01:14:30,730 --> 01:14:33,790
machine learning applications where

1113
01:14:33,810 --> 01:14:36,370
we have the positive class the negative class

1114
01:14:36,370 --> 01:14:39,520
good afternoon everybody

1115
01:14:40,250 --> 01:14:44,830
this afternoon with and for about some aspects of group theory

1116
01:14:44,890 --> 01:14:47,270
in machine learning

1117
01:14:47,580 --> 01:14:50,660
they were sent maybe in the last two

1118
01:14:50,680 --> 01:14:54,160
years i have been following this time

1119
01:14:54,170 --> 01:14:56,460
it's a search

1120
01:14:56,460 --> 01:14:59,780
interest in which the algebraic methods

1121
01:15:02,370 --> 01:15:04,330
elements from group theory

1122
01:15:04,350 --> 01:15:05,710
in machine learning

1123
01:15:07,350 --> 01:15:10,450
i decided to to make a series of

1124
01:15:10,470 --> 01:15:11,960
small talks

1125
01:15:11,960 --> 01:15:15,210
about those recent developments

1126
01:15:15,320 --> 01:15:18,540
but the problem i i notice

1127
01:15:19,380 --> 01:15:21,770
regarding machine learning people

1128
01:15:21,780 --> 01:15:27,950
and these methods is that they have a certain difficulty in understanding the some some

1129
01:15:27,950 --> 01:15:30,570
of the notation some of the

1130
01:15:30,650 --> 01:15:32,590
the formalism

1131
01:15:32,600 --> 01:15:36,700
that's involved in this group theory was

1132
01:15:36,750 --> 01:15:40,020
as it as it is thought to usually

1133
01:15:40,030 --> 01:15:44,450
so i decided to try to make something more simple

1134
01:15:44,510 --> 01:15:48,370
and something a bit more based on examples

1135
01:15:48,450 --> 01:15:51,340
so those ideas could be

1136
01:15:52,200 --> 01:15:53,870
more easily grasped

1137
01:15:53,900 --> 01:16:01,790
so the this this connection between group theory and probability statistics is not new it's

1138
01:16:01,790 --> 01:16:05,310
quite maybe two decades or more

1139
01:16:05,350 --> 01:16:07,200
and by this has been

1140
01:16:07,210 --> 01:16:11,370
known for while in the statistic statistics community

1141
01:16:11,400 --> 01:16:14,590
but not so much in the machine learning community

1142
01:16:15,650 --> 01:16:21,840
and the person that developed most of the SS both papers connecting these two areas

1143
01:16:21,840 --> 01:16:24,700
of group theory and statistics

1144
01:16:24,710 --> 01:16:30,140
it's a man called the procedure icons these statisticians in

1145
01:16:30,150 --> 01:16:31,680
united states

1146
01:16:31,760 --> 01:16:34,890
and his two

1147
01:16:34,930 --> 01:16:41,200
phd students which became quite famous well rock more and muslin

1148
01:16:41,200 --> 01:16:42,990
and of course from that

1149
01:16:43,010 --> 01:16:43,730
you have

1150
01:16:43,750 --> 01:16:45,990
a whole bunch of people that do

1151
01:16:46,000 --> 01:16:46,840
that have

1152
01:16:46,870 --> 01:16:51,250
quite good work as well as suppose

1153
01:16:51,270 --> 01:16:53,530
it's quite important to be accessible because

1154
01:16:53,530 --> 01:16:55,680
the group theory that

1155
01:16:55,710 --> 01:16:59,960
material on group theory normals and it can be quite

1156
01:17:00,150 --> 01:17:03,630
counting to to start somewhere

1157
01:17:04,500 --> 01:17:09,440
keep in mind is named percy diaconis and this is the

1158
01:17:09,490 --> 01:17:13,520
the major refernce for this talk

1159
01:17:14,710 --> 01:17:27,550
all four

1160
01:17:27,810 --> 01:17:35,470
the model has to be here

1161
01:17:46,690 --> 01:17:49,490
OK so the outline of this talk

1162
01:17:51,520 --> 01:17:59,400
so the first thing a bit of group theory very basic representation

1163
01:17:59,430 --> 01:18:02,720
group theory it's very abstract thing

1164
01:18:02,740 --> 01:18:05,090
it's based on set theory

1165
01:18:05,090 --> 01:18:11,660
and the elements of the group it's normally abstract doesn't have

1166
01:18:11,680 --> 01:18:14,460
it can be you can have

1167
01:18:14,490 --> 01:18:18,110
different types of actions so only realise

1168
01:18:18,120 --> 01:18:22,370
the action of an element of the group by the finer representation for

1169
01:18:22,460 --> 01:18:26,860
so this is what we're going to have to deal with representation later this is

1170
01:18:26,860 --> 01:18:28,210
the basic

1171
01:18:28,340 --> 01:18:32,810
construction for of understanding and monica analysis on the group

1172
01:18:32,840 --> 01:18:36,740
later i'll talk about after take this

1173
01:18:36,870 --> 01:18:41,610
one cannot be more or less how for almost two first box

1174
01:18:41,680 --> 01:18:46,870
and then some applications invariant shape analysis group theory plays some role

1175
01:18:46,910 --> 01:18:51,090
and some things i'm doing recently

1176
01:18:51,180 --> 01:18:53,290
so these are the papers

1177
01:18:53,310 --> 01:18:56,320
that i mentioned that i followed recently

1178
01:18:56,420 --> 01:18:57,910
from the

1179
01:18:57,930 --> 01:19:01,540
flicking this this paper of the trees

1180
01:19:01,580 --> 01:19:03,860
from maybe

1181
01:19:03,910 --> 01:19:05,880
at the beginning of the nineties

1182
01:19:05,890 --> 01:19:09,940
so it's one of the this page is one of the

1183
01:19:11,000 --> 01:19:14,730
this diaconis boyd et al

1184
01:19:14,750 --> 01:19:17,500
this makes a markov chain with symmetry

1185
01:19:17,560 --> 01:19:20,060
it was in two thousand six

1186
01:19:20,100 --> 01:19:21,900
some applications of groups

1187
01:19:23,010 --> 01:19:24,880
theory in the ideas of

1188
01:19:24,890 --> 01:19:26,930
one of analysis

1189
01:19:27,370 --> 01:19:30,110
in order to x

1190
01:19:30,120 --> 01:19:33,440
the mixing rate of the markov chain

1191
01:19:35,000 --> 01:19:37,200
and the problem of finding

1192
01:19:37,250 --> 01:19:39,930
watching that mixes fast

1193
01:19:39,930 --> 01:19:41,270
this is

1194
01:19:41,410 --> 01:19:46,930
japanese has two papers of these one without symmetry one general another one with symmetry

1195
01:19:46,990 --> 01:19:48,180
his recent

1196
01:19:48,180 --> 01:19:50,660
paper thanks to the the seven

1197
01:19:50,700 --> 01:19:53,260
comparing partial ranks

1198
01:19:53,270 --> 01:19:57,950
o explains actly what partial ranking lists

1199
01:19:58,020 --> 01:20:01,680
guy lebanon nips two thousand seven

1200
01:20:05,140 --> 01:20:12,680
he extended the results of production flickinger to partial rankings

1201
01:20:16,420 --> 01:20:18,370
my is

1202
01:20:18,380 --> 01:20:19,870
the thing that's

1203
01:20:20,330 --> 01:20:22,970
that's unit in the cell

1204
01:20:22,980 --> 01:20:29,200
he started with a series of papers in NIPS

1205
01:20:29,220 --> 01:20:31,990
in one of the application of symmetric group

1206
01:20:32,070 --> 01:20:37,430
dealing with MOAT motor tracking of planes in airport

1207
01:20:37,480 --> 01:20:41,880
it was the first application in the real world applications found for

1208
01:20:41,930 --> 01:20:44,250
efficient computation in the

1209
01:20:44,290 --> 01:20:46,800
symmetry group

1210
01:20:46,850 --> 01:20:50,110
there are some of the papers from him as well

1211
01:20:50,270 --> 01:20:54,930
and presenting them as in the last NIPS there there were these

1212
01:20:54,930 --> 01:20:59,740
this workshop and symposium on

1213
01:20:59,810 --> 01:21:01,430
as you recommend

1214
01:21:02,440 --> 01:21:04,370
group theory in machine learning

1215
01:21:04,370 --> 01:21:06,580
it needs to know the name

1216
01:21:06,600 --> 01:21:10,880
and he is of a little bit of order this paper he was in the

1217
01:21:10,890 --> 01:21:12,080
it was the follow-up

1218
01:21:12,100 --> 01:21:14,620
that the one for NIPS two thousand six

1219
01:21:14,670 --> 01:21:18,870
and this two thousand seven was some improvement on the

1220
01:21:18,880 --> 01:21:24,940
results that we really got in two thousand six and is one here only the

1221
01:21:24,950 --> 01:21:27,770
thing that they do the which improves along the result

1222
01:21:27,800 --> 01:21:28,690
it's too

1223
01:21:28,700 --> 01:21:32,140
instead of dealing with

1224
01:21:32,170 --> 01:21:36,950
when they do the inverse transform i'll talk about which transforms into some

1225
01:21:36,970 --> 01:21:41,120
but here they when they invaded the inverse transform

1226
01:21:41,130 --> 01:21:44,940
you have to project back to the to the marginal polytope

1227
01:21:44,950 --> 01:21:48,670
and one thing that wasn't done before and with this project and you get much

1228
01:21:50,680 --> 01:21:52,880
this nation

1229
01:21:56,580 --> 01:22:00,800
so all those papers they are they were talking about things like this mela models

1230
01:22:00,800 --> 01:22:05,490
which is the probability model on this the group on the permutations

1231
01:22:05,490 --> 01:22:10,390
and i want to analysis on manifolds that she's

1232
01:22:10,410 --> 01:22:11,990
group is

1233
01:22:12,000 --> 01:22:14,420
what i mean by monarch analysis

1234
01:22:14,490 --> 01:22:19,990
of all things and when the canal is we're going to focus on fourier transform

1235
01:22:19,990 --> 01:22:33,040
last time i mentioned you

1236
01:22:34,250 --> 01:22:36,280
charge resides

1237
01:22:36,330 --> 01:22:38,990
at the surface of solid conductors

1238
01:22:38,990 --> 01:22:42,940
but that is not uniformly distributed but you remember that when that happens to be

1239
01:22:43,950 --> 01:22:46,290
one of the so that today

1240
01:22:46,350 --> 01:22:48,530
if i had a solid conductor

1241
01:22:48,580 --> 01:22:53,120
which they had this shape

1242
01:22:53,190 --> 01:22:56,580
i want to convince you convey the right here

1243
01:22:56,650 --> 01:23:00,520
the surface charge density will be higher than their

1244
01:23:00,540 --> 01:23:01,720
because the curvature

1245
01:23:01,730 --> 01:23:05,970
stronger than it is here

1246
01:23:05,970 --> 01:23:09,470
anyway i want to approach that is as follows

1247
01:23:09,510 --> 01:23:12,070
suppose i have here

1248
01:23:12,110 --> 01:23:14,890
a solid conductor a

1249
01:23:14,940 --> 01:23:17,900
it has radius r of a

1250
01:23:17,930 --> 01:23:20,520
and very very far away

1251
01:23:20,540 --> 01:23:23,250
maybe tens of metres away

1252
01:23:23,260 --> 01:23:24,550
i have a solid

1253
01:23:24,570 --> 01:23:26,490
conductor b

1254
01:23:26,510 --> 01:23:28,820
it was radius our of the

1255
01:23:28,850 --> 01:23:32,430
and they are connected through a conducting wire that's essential

1256
01:23:32,490 --> 01:23:35,160
if they are connected through the conducting wire

1257
01:23:35,180 --> 01:23:40,880
then this equipotential all at the same potential

1258
01:23:40,880 --> 01:23:43,610
i'm going to charge them up

1259
01:23:43,660 --> 01:23:46,520
so i can charge distribution

1260
01:23:46,540 --> 01:23:48,150
q a year

1261
01:23:48,220 --> 01:23:51,870
and i get to be there

1262
01:23:51,880 --> 01:23:55,610
the potential of eighteen

1263
01:23:56,720 --> 01:23:59,470
about the same it would be if b

1264
01:23:59,490 --> 01:24:01,050
we're not there

1265
01:24:01,050 --> 01:24:03,430
b so far away

1266
01:24:03,490 --> 01:24:06,820
that if i come with some charge from infinity in in my pocket

1267
01:24:06,910 --> 01:24:09,930
the work that i have to do to reach a

1268
01:24:10,020 --> 01:24:11,290
unit charge

1269
01:24:11,300 --> 01:24:15,050
is independent of whether these are not be as far away

1270
01:24:15,070 --> 01:24:18,040
tens of meters you can make it to my like you want to

1271
01:24:18,110 --> 01:24:22,160
and so the potential of a is in charge on a

1272
01:24:22,210 --> 01:24:23,910
divided by four five

1273
01:24:23,930 --> 01:24:25,460
actually non-zero

1274
01:24:25,520 --> 01:24:28,960
the radius of a but since it is only people can still because it's all

1275
01:24:30,120 --> 01:24:31,580
this must be also

1276
01:24:31,580 --> 01:24:32,670
the potential

1277
01:24:32,730 --> 01:24:35,170
of the sphere b and then is

1278
01:24:35,210 --> 01:24:36,640
the charge on b

1279
01:24:36,650 --> 01:24:38,370
divided by four five

1280
01:24:38,370 --> 01:24:39,650
actually non-zero

1281
01:24:39,650 --> 01:24:41,340
are these

1282
01:24:41,360 --> 01:24:43,180
so you see immediately

1283
01:24:44,180 --> 01:24:46,420
q the charge on the

1284
01:24:46,430 --> 01:24:47,920
divided by

1285
01:24:47,960 --> 01:24:49,580
the radius of the

1286
01:24:49,580 --> 01:24:53,950
if the charge on a divided by the radius on

1287
01:24:55,140 --> 01:24:57,080
the radius of the

1288
01:24:57,090 --> 01:25:01,310
we're for instance five times larger

1289
01:25:01,330 --> 01:25:02,990
then the radius of a

1290
01:25:03,030 --> 01:25:08,930
there would be five times more charge on b then there would be on a

1291
01:25:11,960 --> 01:25:14,740
as five times larger radius

1292
01:25:14,800 --> 01:25:16,780
then its surface area

1293
01:25:16,830 --> 01:25:18,420
is twenty five

1294
01:25:20,950 --> 01:25:22,430
and and since

1295
01:25:22,490 --> 01:25:26,770
the surface charge density sigma

1296
01:25:26,780 --> 01:25:28,810
it is the charge on the sphere

1297
01:25:28,830 --> 01:25:31,360
divided by the surface area

1298
01:25:31,400 --> 01:25:32,770
of often here

1299
01:25:32,840 --> 01:25:34,650
it is now clear that if

1300
01:25:34,740 --> 01:25:38,610
the radius of these five times larger than a

1301
01:25:38,610 --> 01:25:39,620
it's true

1302
01:25:39,640 --> 01:25:42,680
that the charge on the

1303
01:25:42,740 --> 01:25:45,360
five times the charge on

1304
01:25:45,400 --> 01:25:47,830
but the surface charge density on

1305
01:25:47,870 --> 01:25:49,830
is now only one thing

1306
01:25:49,870 --> 01:25:50,620
the two

1307
01:25:50,650 --> 01:25:54,550
charge density of a because it's area is twenty five times larger

1308
01:25:54,680 --> 01:25:55,870
so you have this

1309
01:25:55,930 --> 01:25:56,950
the highest

1310
01:25:56,960 --> 01:25:59,140
the surface charge density

1311
01:26:00,250 --> 01:26:01,080
you have

1312
01:26:01,080 --> 01:26:03,150
b five times higher

1313
01:26:03,170 --> 01:26:05,580
the surface charge density in there

1314
01:26:05,590 --> 01:26:09,770
and i hope that convinces you that if we have a solid conductor like this

1315
01:26:10,200 --> 01:26:11,080
even though

1316
01:26:11,110 --> 01:26:15,210
it's not ideal as we have here with these two species far apart that the

1317
01:26:15,210 --> 01:26:18,830
surface charge density here will be larger than their

1318
01:26:18,900 --> 01:26:19,900
because it has

1319
01:26:19,900 --> 01:26:22,990
a smaller radius is basically the same

1320
01:26:25,710 --> 01:26:27,550
so you expect

1321
01:26:27,610 --> 01:26:29,770
the high surface charge density

1322
01:26:30,900 --> 01:26:34,400
the curvature is the highest

1323
01:26:34,490 --> 01:26:36,180
the smallest radius

1324
01:26:36,240 --> 01:26:41,130
and that means that also the electric field will be stronger there

1325
01:26:41,220 --> 01:26:44,030
it follows immediately from gauss is small

1326
01:26:44,070 --> 01:26:45,300
if this is

1327
01:26:45,300 --> 01:26:47,490
the surface of the conductor

1328
01:26:47,500 --> 01:26:50,300
any conductor solid conductor

1329
01:26:50,350 --> 01:26:54,300
when the field is zero inside the conductor

1330
01:26:54,300 --> 01:26:57,040
and the surface charge here

1331
01:26:57,090 --> 01:27:01,120
what i'm going to do is i'm going to make a gaussian pillbox

1332
01:27:01,180 --> 01:27:02,970
this surface is parallel

1333
01:27:02,980 --> 01:27:04,640
good conduct

1334
01:27:04,690 --> 01:27:07,680
i go in the conductor

1335
01:27:07,690 --> 01:27:09,070
and this now is my

1336
01:27:09,170 --> 01:27:11,550
gaussian surface

1337
01:27:11,550 --> 01:27:17,640
the proportion of opponents of year essentially going to put discretizes based on

1338
01:27:17,710 --> 01:27:19,060
thing the points

1339
01:27:19,080 --> 01:27:21,810
in the right sort of the space

1340
01:27:21,820 --> 01:27:23,960
that's really the idea

1341
01:27:24,630 --> 01:27:26,540
the account is just

1342
01:27:26,560 --> 01:27:30,860
very clever discretisation of the space is automatically

1343
01:27:30,910 --> 01:27:33,440
you put point where it doesn't matter

1344
01:27:33,460 --> 01:27:37,150
you could send me of this the low dimensional space

1345
01:27:37,200 --> 01:27:40,690
well it seems that is not very relevant

1346
01:27:41,840 --> 01:27:47,100
instead of doing that you could do something in numerical approximations using the sum of

1347
01:27:47,110 --> 01:27:50,140
the masses well which would be much simpler

1348
01:27:50,190 --> 01:27:52,210
that have to do with the yellow

1349
01:27:52,230 --> 01:27:55,830
so we do in this case you could for example

1350
01:27:55,840 --> 01:27:58,330
in the binary you could maybe

1351
01:27:58,390 --> 01:28:01,760
just to get them agree with the points

1352
01:28:01,770 --> 01:28:06,260
at each node you the great the dimensional great OK that's what we could do

1353
01:28:06,310 --> 01:28:09,750
on then approximate apology density function

1354
01:28:09,760 --> 01:28:11,700
by the system

1355
01:28:11,750 --> 01:28:13,700
it on that and

1356
01:28:13,750 --> 01:28:14,970
we go to

1357
01:28:14,980 --> 01:28:21,540
plus infinity though this all consistent approximation of what that something can do know dimensional

1358
01:28:21,540 --> 01:28:24,030
space it doesn't make sense to do that

1359
01:28:24,190 --> 01:28:27,080
the problem is for us

1360
01:28:27,130 --> 01:28:29,230
you see that most of the point

1361
01:28:29,310 --> 01:28:33,210
on greater degree that you know i just knew well they have mass which is

1362
01:28:33,210 --> 01:28:34,770
essentially zero

1363
01:28:34,780 --> 01:28:37,250
so it's kind of west to find work to compute

1364
01:28:37,300 --> 01:28:43,240
basically the posterior distribution at this point because it's zero to deal with time

1365
01:28:43,260 --> 01:28:46,240
more and more importantly that

1366
01:28:47,470 --> 01:28:50,550
if you are not very high dimensional space

1367
01:28:51,890 --> 01:28:55,620
assume that you're trying to make the point on the function

1368
01:28:55,670 --> 01:28:56,800
in our in

1369
01:28:57,140 --> 01:29:01,290
i assume you're trying to do that using some kind of some kind type of

1370
01:29:01,290 --> 01:29:04,290
course so let's say

1371
01:29:05,350 --> 01:29:07,190
you're going to do

1372
01:29:08,410 --> 01:29:13,320
so x x x or xk belongs to are you interested in

1373
01:29:13,370 --> 01:29:18,940
in a meeting of the distributions the following distribution lies on our end you want

1374
01:29:18,940 --> 01:29:19,960
to degrade

1375
01:29:20,020 --> 01:29:23,130
so what i'm doing i'm opposing discretize each

1376
01:29:23,180 --> 01:29:26,530
the space of each state company xk

1377
01:29:26,580 --> 01:29:28,070
using p value

1378
01:29:28,400 --> 01:29:34,620
p value with you do that you discretize storyline each component but use

1379
01:29:34,630 --> 01:29:36,010
and you see that the

1380
01:29:36,020 --> 01:29:39,820
the point is that the number of points in the grid will be directly to

1381
01:29:39,830 --> 01:29:40,880
the and

1382
01:29:40,900 --> 01:29:48,290
that would actually this exponential of because i'm particularly interesting problem where n is of

1383
01:29:48,290 --> 01:29:53,730
all the one that you see that even if you are doing the work for

1384
01:29:55,340 --> 01:29:58,530
it they company by two values you we need

1385
01:29:58,540 --> 01:30:03,240
i like to need to deploy one on the hot that's ridiculous

1386
01:30:03,280 --> 01:30:08,220
this is useless so all these kind of green met when you're trying to work

1387
01:30:08,260 --> 01:30:12,900
supports nearly i'm unsure point distribution it's just not workable

1388
01:30:13,190 --> 01:30:15,640
but that's the only way we can do it

1389
01:30:15,690 --> 01:30:20,000
so that's why i monte the government the right things much more because we see

1390
01:30:20,000 --> 01:30:21,740
that automatically

1391
01:30:21,800 --> 01:30:26,330
don't really care about the dimension of the base that automatically putting the point where

1392
01:30:26,330 --> 01:30:30,480
it doesn't matter

1393
01:30:35,580 --> 01:30:38,430
it's not only possible and if you know if you have is that in our

1394
01:30:40,650 --> 01:30:49,850
again is what with you are you going to the mass of the wall are

1395
01:30:49,860 --> 01:30:52,340
the uniform distribution

1396
01:31:02,750 --> 01:31:08,970
i mean you get a lot of application you may have seen cases where you

1397
01:31:08,970 --> 01:31:12,370
could you could have like a significant like

1398
01:31:12,390 --> 01:31:18,010
the clever discretisation space

1399
01:31:18,200 --> 01:31:21,600
but this is what happens with happening with whatever

1400
01:31:22,760 --> 01:31:24,070
OK so

1401
01:31:24,080 --> 01:31:27,510
now because i'm telling you intuitively these these

1402
01:31:27,520 --> 01:31:29,640
multicolored of nations

1403
01:31:29,690 --> 01:31:34,210
this is just an empirical measure of distributed according to p

1404
01:31:34,220 --> 01:31:35,250
it makes sense

1405
01:31:35,620 --> 01:31:37,830
that makes sense to everything

1406
01:31:37,890 --> 01:31:40,990
it is just information well consider

1407
01:31:41,040 --> 01:31:46,360
consider that you are interested in estimating an expectation function phi

1408
01:31:46,370 --> 01:31:47,770
respect to

1409
01:31:47,820 --> 01:31:49,630
the distribution of interest

1410
01:31:51,440 --> 01:31:54,870
so to compute an approximation of these guys

1411
01:31:54,890 --> 01:31:57,370
using the monte carlo approximation of p

1412
01:31:57,380 --> 01:32:00,940
it's very but what is very good thing about this sort of nature

1413
01:32:00,960 --> 01:32:04,130
so what you do your interest approximating the i

1414
01:32:06,060 --> 01:32:07,940
well i've got

1415
01:32:08,810 --> 01:32:10,820
o point distribution here

1416
01:32:10,830 --> 01:32:13,370
you substitute to it the empirical

1417
01:32:13,760 --> 01:32:16,310
nature is the sum of the

1418
01:32:17,580 --> 01:32:23,570
at the sampled values are not used simply simple properties of

1419
01:32:24,680 --> 01:32:28,630
the that some function that is if you have

1420
01:32:28,680 --> 01:32:34,500
f of fire that there that you all act is equal to five x nine

1421
01:32:34,510 --> 01:32:38,260
it's not x OK so what using here

1422
01:32:38,300 --> 01:32:42,400
so you end up that you're multicolored which may be just the sort of the

1423
01:32:42,400 --> 01:32:47,310
best the idea is to function it may be that of course but this is

1424
01:32:47,320 --> 01:32:49,010
an estimate of

1425
01:32:49,060 --> 01:32:55,560
people have been on to slide because they want to make you deterministic approximation this

1426
01:32:55,560 --> 01:33:00,330
is one of the approximation the one then this coming under the distribution that these

1427
01:33:00,330 --> 01:33:03,470
guys are sampled values that one of

1428
01:33:03,540 --> 01:33:06,830
and point here

1429
01:33:06,850 --> 01:33:09,000
OK so now

1430
01:33:09,020 --> 01:33:11,860
what happens is it is it with you

1431
01:33:11,880 --> 01:33:13,940
this is the most part

1432
01:33:15,530 --> 01:33:17,280
by the law of large numbers

1433
01:33:17,300 --> 01:33:19,110
essentially we know

1434
01:33:19,130 --> 01:33:24,170
that that on the left or the right to buy

1435
01:33:24,170 --> 01:33:26,000
i argued

1436
01:33:26,040 --> 01:33:28,260
was the statesman b

1437
01:33:29,170 --> 01:33:32,710
of literally transforming human nature

1438
01:33:32,750 --> 01:33:34,890
as rousseau maintains

1439
01:33:34,950 --> 01:33:38,490
or is the sovereign a more or less

1440
01:33:41,090 --> 01:33:43,170
in the manner of the modern say

1441
01:33:43,230 --> 01:33:45,170
martin CEO

1442
01:33:45,330 --> 01:33:48,350
as for example someone like hobbes

1443
01:33:48,400 --> 01:33:52,680
it seems to believe all of our text that we will read the republic the

1444
01:33:52,680 --> 01:33:55,630
politics the prince the social contract

1445
01:33:55,640 --> 01:33:57,170
have different views

1446
01:33:57,180 --> 01:34:03,110
on the qualities of statecraft and what is not what those qualities necessary to found

1447
01:34:03,110 --> 01:34:08,480
and maintain states that we will we will be considering

1448
01:34:08,520 --> 01:34:10,260
all of this in a way

1449
01:34:10,280 --> 01:34:14,650
this is another way of saying or at least implying OK

1450
01:34:14,710 --> 01:34:17,030
that political philosophy

1451
01:34:17,060 --> 01:34:18,610
is an imminently

1452
01:34:18,640 --> 01:34:22,230
practical support practical field

1453
01:34:22,270 --> 01:34:26,030
its purpose is not simply contemplation

1454
01:34:26,040 --> 01:34:29,730
its purpose is not a reflection alone

1455
01:34:29,750 --> 01:34:32,300
it is advice giving

1456
01:34:32,350 --> 01:34:36,250
all of the people none of the none of the people we will study this

1457
01:34:37,260 --> 01:34:43,140
were cloistered scholars detached from the world of this is a very common

1458
01:34:43,210 --> 01:34:44,820
prejudice against

1459
01:34:44,820 --> 01:34:47,430
political philosophy it somehow

1460
01:34:47,450 --> 01:34:50,550
uniquely sort of pi in the sky

1461
01:34:50,660 --> 01:34:53,180
and detached from the world

1462
01:34:53,240 --> 01:34:55,620
but the great thinkers were very far

1463
01:34:55,650 --> 01:35:01,940
from being just so to speak detached intellectuals play-doh undertook three

1464
01:35:01,990 --> 01:35:04,860
long and dangerous voyages

1465
01:35:04,880 --> 01:35:06,520
to sicily

1466
01:35:06,520 --> 01:35:07,840
in order to

1467
01:35:07,880 --> 01:35:11,870
advise the king and ICS

1468
01:35:11,920 --> 01:35:16,390
aristotle famously was the tutor of alexander the great

1469
01:35:16,550 --> 01:35:20,900
marke valley spent a large part of his career in the foreign service

1470
01:35:20,910 --> 01:35:22,660
of his native flora

1471
01:35:22,710 --> 01:35:27,350
and wrote is an adviser to the metagene

1472
01:35:27,350 --> 01:35:30,330
hobbs was the tutor the royal household

1473
01:35:30,370 --> 01:35:32,040
who follow the king

1474
01:35:32,060 --> 01:35:33,380
into exile

1475
01:35:33,400 --> 01:35:35,990
during the english civil war

1476
01:35:36,020 --> 01:35:36,930
and lock

1477
01:35:36,950 --> 01:35:39,750
it was associated with shaftesbury circle

1478
01:35:39,760 --> 01:35:40,820
who also

1479
01:35:40,830 --> 01:35:42,620
was forced into exile

1480
01:35:42,650 --> 01:35:46,260
after being accused of plotting against

1481
01:35:46,340 --> 01:35:48,850
english came

1482
01:35:48,870 --> 01:35:50,320
rousseau had no

1483
01:35:50,320 --> 01:35:55,160
official political connections but he signed his name always

1484
01:35:55,180 --> 01:35:57,180
jean-jacques rousseau citizen

1485
01:35:57,190 --> 01:35:58,390
of geneva

1486
01:35:58,530 --> 01:36:02,290
and was approached to write constitutions for poland

1487
01:36:02,300 --> 01:36:04,630
and for the island of corsica

1488
01:36:04,660 --> 01:36:08,590
and total with was a member of the french national assembly

1489
01:36:08,600 --> 01:36:11,100
his experience of american democracy

1490
01:36:11,110 --> 01:36:12,800
deeply affected the way

1491
01:36:12,800 --> 01:36:13,630
he saw

1492
01:36:13,650 --> 01:36:16,010
the future of europe

1493
01:36:16,050 --> 01:36:21,900
so the great political thinkers were typically engaged in the politics of their times

1494
01:36:21,930 --> 01:36:24,850
and help in that way to provide us OK

1495
01:36:24,870 --> 01:36:26,090
with models

1496
01:36:26,090 --> 01:36:26,860
for how

1497
01:36:26,880 --> 01:36:28,940
we might think about

1498
01:36:39,510 --> 01:36:50,080
before break for refreshments

1499
01:36:50,090 --> 01:36:53,100
but this goes in a slightly different direction as well

1500
01:36:53,160 --> 01:36:56,620
not only is the study of the regime

1501
01:36:56,630 --> 01:36:59,080
as we've seen as just tried to indicate

1502
01:36:59,080 --> 01:37:00,270
rooted in

1503
01:37:00,280 --> 01:37:02,990
in many ways the practical experience

1504
01:37:03,000 --> 01:37:05,780
of the thinkers will be looking at but the study

1505
01:37:05,830 --> 01:37:10,540
of regime politics either implicitly or explicitly

1506
01:37:10,550 --> 01:37:13,680
raises a question that goes beyond

1507
01:37:13,730 --> 01:37:18,020
the boundary of any any given society

1508
01:37:18,110 --> 01:37:22,580
regime as i said constitutes a people's way of life what

1509
01:37:22,590 --> 01:37:25,820
they believe makes their life worth living

1510
01:37:25,850 --> 01:37:28,900
or to put it again slightly different what a people

1511
01:37:28,960 --> 01:37:32,110
i stand for

1512
01:37:32,120 --> 01:37:35,000
although we are most familiar

1513
01:37:35,770 --> 01:37:39,990
the character of the modern democratic regimes such as ours

1514
01:37:40,060 --> 01:37:42,320
the study of political philosophy

1515
01:37:42,360 --> 01:37:47,010
is in many ways a kind of immersion into what we might call today comparative

1516
01:37:47,010 --> 01:37:52,340
politics that is to say is it opens up towards the variety of regimes

1517
01:37:52,350 --> 01:37:56,660
each with its own distinctive set of claims or principles

1518
01:37:56,680 --> 01:38:00,230
each vying and potentially in conflict

1519
01:38:00,270 --> 01:38:02,410
with all the others of OK

1520
01:38:02,440 --> 01:38:07,200
underlying this took coffin a of regimes is the question

1521
01:38:07,210 --> 01:38:11,670
which of these regimes is best

1522
01:38:11,690 --> 01:38:13,850
what has or ought to have

1523
01:38:15,080 --> 01:38:16,640
on our loyalty

1524
01:38:16,690 --> 01:38:18,830
and rational consent

1525
01:38:18,830 --> 01:38:20,300
political philosophy

1526
01:38:20,360 --> 01:38:22,650
is always guided by the question

1527
01:38:22,710 --> 01:38:25,390
one of the best machine

1528
01:38:25,440 --> 01:38:27,750
but what is the best regime

1529
01:38:27,790 --> 01:38:30,870
event to raise the set of questions seems to

1530
01:38:30,890 --> 01:38:32,370
always insuperable

1531
01:38:33,630 --> 01:38:35,120
isn't that

1532
01:38:35,200 --> 01:38:43,210
completely subjective judgement what one thinks is the best regime how could one began

1533
01:38:43,250 --> 01:38:45,870
such a study

1534
01:38:45,890 --> 01:38:47,600
it is the best regime

1535
01:38:47,650 --> 01:38:49,170
as the ancients

1536
01:38:49,180 --> 01:38:53,180
tended to believe it aristotle and others

1537
01:38:53,240 --> 01:38:54,040
is there

1538
01:38:54,050 --> 01:38:56,690
is it in aristocratic republic

1539
01:38:56,750 --> 01:39:00,670
in which only the few best bitterly rule

1540
01:39:00,690 --> 01:39:03,810
or is the best regime as the moderns believe

1541
01:39:03,880 --> 01:39:07,770
democratic republic wherein principle political office

1542
01:39:07,770 --> 01:39:09,430
it is open to all

1543
01:39:09,450 --> 01:39:14,980
by virtue of their membership in society alone

1544
01:39:14,980 --> 01:39:18,180
well the best regime be small

1545
01:39:18,210 --> 01:39:21,480
closed society two generations

1546
01:39:21,520 --> 01:39:24,340
has made the supreme sacrifice

1547
01:39:24,350 --> 01:39:27,690
towards self perfection

1548
01:39:27,760 --> 01:39:29,730
think of that

1549
01:39:29,780 --> 01:39:34,430
well the best regime be of large cosmopolitan order

1550
01:39:34,450 --> 01:39:36,700
embracing all human beings

1551
01:39:36,710 --> 01:39:40,400
perhaps even a kind of universal league of nations

1552
01:39:40,400 --> 01:39:43,280
rest of

1553
01:40:02,770 --> 01:40:08,950
so this is a joint work with you know who you don't

1554
01:40:15,530 --> 01:40:16,930
so this is

1555
01:40:17,690 --> 01:40:20,230
this was some that

1556
01:40:20,280 --> 01:40:22,580
and we want

1557
01:40:22,590 --> 01:40:25,890
with the addition of ten

1558
01:40:26,040 --> 01:40:28,240
want to do this

1559
01:40:29,170 --> 01:40:30,230
that's all

1560
01:40:30,240 --> 01:40:34,290
so this is probably going the vision wasn't

1561
01:40:34,300 --> 01:40:36,680
basically because the strain

1562
01:40:36,700 --> 01:40:39,080
to shown

1563
01:40:39,090 --> 01:40:42,450
so the solution somewhat sequence

1564
01:40:42,470 --> 01:40:48,040
we can be reduced so that the example space

1565
01:40:48,720 --> 01:40:51,850
the station is going to

1566
01:40:54,890 --> 01:40:56,630
we had great

1567
01:41:03,240 --> 01:41:05,530
the first one is to do this

1568
01:41:05,540 --> 01:41:09,380
so that is very easy for you

1569
01:41:09,420 --> 01:41:11,090
points are

1570
01:41:11,110 --> 01:41:13,660
you can also the look as

1571
01:41:13,670 --> 01:41:19,300
this is the same thing

1572
01:41:24,560 --> 01:41:27,890
so the difference is also here for example

1573
01:41:30,980 --> 01:41:33,520
so to get to the

1574
01:41:34,910 --> 01:41:42,910
you can do is we can also be seen when we

1575
01:41:42,930 --> 01:41:44,690
that's one of the sequences

1576
01:41:44,710 --> 01:41:49,530
we know that we use for example but also give you can

1577
01:41:49,550 --> 01:41:54,540
actually use a prior to that she was

1578
01:41:54,560 --> 01:41:58,280
so what we want is one of the first system

1579
01:41:58,290 --> 01:42:00,130
well the world

1580
01:42:00,140 --> 01:42:07,390
i want to use these for about one centimeter

1581
01:42:07,410 --> 01:42:14,910
it is but you can just use england here is

1582
01:42:15,960 --> 01:42:20,960
for the square loss of people

1583
01:42:20,970 --> 01:42:23,380
basically because

1584
01:42:23,400 --> 01:42:27,250
and it's not going to be

1585
01:42:28,360 --> 01:42:34,450
so basically not everybody has the legal system

1586
01:42:34,460 --> 01:42:38,780
nowadays there are still

1587
01:42:38,800 --> 01:42:40,120
so this

1588
01:42:41,560 --> 01:42:43,150
basically we

1589
01:42:43,190 --> 01:42:47,150
they a simple motion

1590
01:42:47,170 --> 01:42:53,690
one of the following phases uses models who was

1591
01:42:59,080 --> 01:43:00,940
this is one

1592
01:43:00,960 --> 01:43:04,460
the first one to use

1593
01:43:04,470 --> 01:43:06,210
that's right

1594
01:43:06,230 --> 01:43:09,660
four so that we can the

1595
01:43:13,650 --> 01:43:14,740
we have

1596
01:43:16,550 --> 01:43:20,280
some people were

1597
01:43:21,230 --> 01:43:26,410
the same thing for the first

1598
01:43:26,430 --> 01:43:30,250
what to change

1599
01:43:30,260 --> 01:43:31,850
you know what you're

1600
01:43:31,860 --> 01:43:34,080
you must be

1601
01:43:34,090 --> 01:43:36,880
the club generating station

1602
01:43:36,890 --> 01:43:39,220
in the world

1603
01:43:40,380 --> 01:43:44,760
you're the decision is to be a

1604
01:43:44,810 --> 01:43:46,780
they are

1605
01:43:50,560 --> 01:43:51,660
so we see

1606
01:43:56,660 --> 01:44:03,590
and so what we assume is that wasn't going to let many of the original

1607
01:44:07,140 --> 01:44:13,520
and around twice in terms of people should

1608
01:44:13,570 --> 01:44:20,870
and we can use easily image space is a space

1609
01:44:20,880 --> 01:44:23,300
one is the density function

1610
01:44:24,230 --> 01:44:29,690
so we can use these as prior to say that this process is of

1611
01:44:31,660 --> 01:44:37,600
number one is going saying

1612
01:44:37,620 --> 01:44:40,730
so this is what

1613
01:44:40,750 --> 01:44:41,920
so you was

1614
01:44:43,470 --> 01:44:45,900
so to begin with

1615
01:44:45,920 --> 01:44:48,460
based on

1616
01:44:49,560 --> 01:44:53,370
but basically there are too simple too

1617
01:44:53,390 --> 01:44:56,000
that's the motion

1618
01:44:59,930 --> 01:45:04,340
well known

1619
01:45:04,380 --> 01:45:06,460
i believe that there is

1620
01:45:06,470 --> 01:45:09,750
hundreds of thousands of

1621
01:45:11,440 --> 01:45:13,800
and we want to track each

1622
01:45:13,810 --> 01:45:17,410
and you try to receive all of this

1623
01:45:17,430 --> 01:45:19,160
these schools

1624
01:45:19,180 --> 01:45:23,200
then they want to

1625
01:45:23,870 --> 01:45:29,050
as i said before there is a lot of things

1626
01:45:29,060 --> 01:45:31,400
so this is

1627
01:45:35,100 --> 01:45:39,700
the mission you or

1628
01:45:41,480 --> 01:45:43,370
this is

1629
01:45:43,390 --> 01:45:47,630
this suggests that the elimination of

1630
01:45:47,650 --> 01:45:50,770
two of the that is the

1631
01:45:50,820 --> 01:45:52,790
probably not

1632
01:45:52,810 --> 01:45:55,190
so to use

1633
01:45:59,100 --> 01:46:01,540
and you need to use

1634
01:46:01,540 --> 01:46:04,540
this is the kind of an adaptive control algorithms

1635
01:46:04,900 --> 01:46:10,890
and similarly if you know the the area of our web intelligence is again a

1636
01:46:10,890 --> 01:46:12,940
huge data of this

1637
01:46:12,960 --> 01:46:14,630
the kind of community people

1638
01:46:14,630 --> 01:46:19,520
and then you have machine learning data mining and also in the theoretical aspect of

1639
01:46:19,610 --> 01:46:24,580
machine learning for all of these things are fairly well connected and statistics another big

1640
01:46:24,580 --> 01:46:26,400
area that belongs to this area

1641
01:46:26,460 --> 01:46:28,460
a lot of the ice you know like you

1642
01:46:28,480 --> 01:46:29,870
if you're going to get

1643
01:46:29,870 --> 01:46:31,110
computer vision

1644
01:46:31,130 --> 01:46:36,940
again this a little bit of signal processing

1645
01:46:37,020 --> 01:46:42,150
i should celebrate a substantial about of signal processing and a lot of machine learning

1646
01:46:42,170 --> 01:46:45,250
data mining and in these areas

1647
01:46:45,310 --> 01:46:49,370
in fact all in my view one big area you're dealing with that in some

1648
01:46:49,370 --> 01:46:53,580
sense of course if the data has a particular properties then then you need to

1649
01:46:53,580 --> 01:46:55,460
know particulars set problems

1650
01:46:55,480 --> 01:47:00,710
so that's what you know definition of these things and and what you generally find

1651
01:47:00,730 --> 01:47:03,380
is if you if you do well in one area

1652
01:47:03,420 --> 01:47:06,440
you just go and sit in their area and between

1653
01:47:06,500 --> 01:47:10,400
within one hour you tend to understand it completely to some extent

1654
01:47:10,400 --> 01:47:14,230
i mean there will be always difficulties in terms of terms that they use but

1655
01:47:14,230 --> 01:47:16,480
once you understand the terminology

1656
01:47:16,480 --> 01:47:22,250
it's not these are very difficult to follow most of the literature very very quickly

1657
01:47:22,920 --> 01:47:26,630
this thing algorithms you know like in the in the in the community of information

1658
01:47:27,650 --> 01:47:29,790
you know that's again a huge

1659
01:47:29,810 --> 01:47:31,900
data mining people because they are

1660
01:47:31,920 --> 01:47:34,870
you can't build simple

1661
01:47:34,880 --> 01:47:37,150
machine learning algorithms in some sense

1662
01:47:37,170 --> 01:47:39,630
what you're trying to do is you trying to come up with some kind of

1663
01:47:39,630 --> 01:47:41,080
scoring functions

1664
01:47:41,210 --> 01:47:43,000
so that you can you can see

1665
01:47:43,020 --> 01:47:45,520
whether a document that is created

1666
01:47:45,600 --> 01:47:48,750
so you're trying to come up with a function that some of the

1667
01:47:48,790 --> 01:47:53,940
in some sense kind of the classification problem except the training data set is minuscule

1668
01:47:54,330 --> 01:47:55,710
don't have it all

1669
01:47:55,880 --> 01:48:00,770
so therefore what you're trying to do is to trying to hypothesize some mathematical modeling

1670
01:48:00,980 --> 01:48:02,040
this is how the

1671
01:48:02,060 --> 01:48:04,750
document mathematically represented

1672
01:48:04,770 --> 01:48:09,170
and because of that i can save my if mike really matters in terms of

1673
01:48:09,170 --> 01:48:14,600
the model parameters what someone matches then i see this document is like one so

1674
01:48:14,600 --> 01:48:19,400
you can see lots of connections and recently we've been working at nine hundred can

1675
01:48:19,400 --> 01:48:21,600
incorporate machine learning in two

1676
01:48:21,650 --> 01:48:24,110
entered into information retrieval

1677
01:48:24,130 --> 01:48:25,500
and you find that you

1678
01:48:25,500 --> 01:48:29,630
it just cannot just use machine learning because the real problem is that

1679
01:48:29,650 --> 01:48:30,960
we don't have

1680
01:48:31,580 --> 01:48:35,500
training data we have huge about you know the whole web is evolving for you

1681
01:48:35,540 --> 01:48:36,810
but we don't have

1682
01:48:36,830 --> 01:48:40,310
find that i don't think it even fits into

1683
01:48:40,330 --> 01:48:43,040
semi supervised learning in my view because

1684
01:48:44,480 --> 01:48:47,230
unsupervised parties so large

1685
01:48:47,250 --> 01:48:50,100
it's not going to help us in some sense

1686
01:48:50,100 --> 01:48:56,730
but we can do some kind of learning in incrementally learning you what you can

1687
01:48:56,730 --> 01:48:58,100
do that

1688
01:48:58,520 --> 01:49:04,040
and connects to work in inductive logic programming is holding technology program that i was

1689
01:49:04,040 --> 01:49:06,060
talking earlier for example

1690
01:49:06,790 --> 01:49:08,940
to understand the underlying

1691
01:49:08,960 --> 01:49:12,130
syntax of the fifth of language

1692
01:49:12,290 --> 01:49:16,630
that could be potentially down through inductive logic programming

1693
01:49:16,690 --> 01:49:20,770
and they could be there also tree based contrasts you can build

1694
01:49:20,770 --> 01:49:23,170
contrast that to look like tree structures

1695
01:49:23,190 --> 01:49:26,290
and then it those to start with nothing but

1696
01:49:26,420 --> 01:49:29,330
like nested if then function

1697
01:49:29,330 --> 01:49:33,380
so you can you can do those things and change in

1698
01:49:33,440 --> 01:49:38,190
changes in data streams again this is almost and this specialized area

1699
01:49:38,250 --> 01:49:41,460
in the data mining and frequent pattern algorithms

1700
01:49:41,480 --> 01:49:45,830
lots of frequent pattern algorithms that are written

1701
01:49:45,850 --> 01:49:48,980
so if you go to typically in ICDM conference

1702
01:49:49,000 --> 01:49:50,810
in a video you see some

1703
01:49:50,830 --> 01:49:53,330
three of four or five different kinds of

1704
01:49:53,330 --> 01:49:55,460
frequent pattern mining algorithms

1705
01:49:55,460 --> 01:49:58,440
it's quite rigorous in the mathematical sense

1706
01:49:58,540 --> 01:50:03,920
and i do you see some theoretical results of violence or sometimes know new definition

1707
01:50:03,920 --> 01:50:07,270
of a much better definition of frequent patterns

1708
01:50:07,290 --> 01:50:08,370
you know

1709
01:50:08,420 --> 01:50:09,870
putting some constraints

1710
01:50:09,880 --> 01:50:11,460
and and so forth so to me

1711
01:50:11,480 --> 01:50:14,880
the whole data mining is nothing but you know you have a model

1712
01:50:14,900 --> 01:50:17,730
and you're trying to either maximize or minimize

1713
01:50:17,960 --> 01:50:19,810
with respect to some cost

1714
01:50:19,850 --> 01:50:21,810
and that's what you're trying to do so

1715
01:50:21,870 --> 01:50:25,650
so to really do well in this space is to learn

1716
01:50:25,670 --> 01:50:28,400
statistics about probability rho

1717
01:50:28,420 --> 01:50:31,100
and also learn optimisation

1718
01:50:31,110 --> 01:50:32,690
as much as possible

1719
01:50:33,060 --> 01:50:35,610
you know convex programming is a among

1720
01:50:35,630 --> 01:50:37,400
and and also

1721
01:50:37,440 --> 01:50:40,460
in the standard of them this will give you

1722
01:50:40,500 --> 01:50:41,250
in know good

1723
01:50:41,250 --> 01:50:44,020
and much better momentum

1724
01:50:44,060 --> 01:50:48,020
for you to get moving to this area

1725
01:50:48,040 --> 01:50:52,560
and there's also connection with the granular computing and rough sets and also we don't

1726
01:50:52,560 --> 01:50:54,190
have to

1727
01:50:54,210 --> 01:50:58,810
OK so outline of my presentation is based annotations

1728
01:50:58,810 --> 01:51:00,580
and then

1729
01:51:00,600 --> 01:51:03,770
compare with a univariate contrasts

1730
01:51:03,790 --> 01:51:05,770
patterns and rule based contrasts

1731
01:51:05,790 --> 01:51:08,130
contrast pattern based classification

1732
01:51:08,130 --> 01:51:10,790
in contrast for a class

1733
01:51:10,790 --> 01:51:11,900
data sets

1734
01:51:11,900 --> 01:51:13,650
data cube contrasts

1735
01:51:13,650 --> 01:51:20,560
sequence based contrasts grow this contrast more with contrast and common themes open problems and

1736
01:51:20,560 --> 01:51:22,670
then i'll summarize then give you

1737
01:51:22,850 --> 01:51:25,000
lots of our

1738
01:51:28,420 --> 01:51:29,920
OK so

1739
01:51:30,080 --> 01:51:34,810
this the notion of contrast pattern is is quite old in some sense you know

1740
01:51:34,810 --> 01:51:36,710
the very feature selection

1741
01:51:36,790 --> 01:51:39,670
is a kind of contrast so what you're trying to do is

1742
01:51:39,730 --> 01:51:41,690
you know in order to do

1743
01:51:41,710 --> 01:51:45,540
you know the he's is a given data sets given

1744
01:51:45,560 --> 01:51:49,520
tell me what are the most important features of this particular dataset

1745
01:51:49,540 --> 01:51:53,980
you might have a data set containing let's say ten thousand features

1746
01:51:54,020 --> 01:51:57,480
and you know that there are too many know because of the difficulty you can't

1747
01:51:57,480 --> 01:51:58,290
use them

1748
01:51:58,290 --> 01:52:04,230
you want small subset that you know you might use some principal component analysis and

1749
01:52:04,230 --> 01:52:05,170
say OK

1750
01:52:05,170 --> 01:52:06,170
these are the

1751
01:52:06,190 --> 01:52:07,580
they can talk

1752
01:52:07,600 --> 01:52:08,520
you know

1753
01:52:08,630 --> 01:52:10,380
features that i can

1754
01:52:10,520 --> 01:52:12,750
reduce them and when i say

1755
01:52:12,750 --> 01:52:15,610
can it not necessarily can from these

1756
01:52:15,730 --> 01:52:19,940
what about ten thousand feet as i said it could be combination of them but

1757
01:52:19,960 --> 01:52:21,100
the reduced to

1758
01:52:21,850 --> 01:52:23,520
then after

1759
01:52:23,520 --> 01:52:26,330
kind of features the projected in

1760
01:52:26,330 --> 01:52:30,620
we collect all the outcome then there is

1761
01:52:30,680 --> 01:52:34,970
and we observe that k of these are our heads

1762
01:52:35,530 --> 01:52:38,600
so now the likelihood for the for the

1763
01:52:38,630 --> 01:52:39,740
for the entire

1764
01:52:39,770 --> 01:52:43,530
experiments each of the experiments are independent

1765
01:52:43,550 --> 01:52:45,130
from each other

1766
01:52:45,140 --> 01:52:47,000
because the coin

1767
01:52:47,000 --> 01:52:48,330
to me that the what

1768
01:52:48,360 --> 01:52:50,350
the outcome of the coin doesn't doesn't

1769
01:52:50,400 --> 01:52:53,690
it doesn't have any impact on what's going happen in the next

1770
01:52:53,690 --> 01:52:56,560
the experiment

1771
01:52:56,650 --> 01:52:59,150
now the problem here is going to be

1772
01:52:59,390 --> 01:53:02,200
we have k times will have property pi

1773
01:53:02,510 --> 01:53:05,910
and then it will have probably one minus

1774
01:53:05,970 --> 01:53:09,130
OK so that's the the the difference here

1775
01:53:09,190 --> 01:53:12,290
about whether you think of this as being an ordered set or you think of

1776
01:53:12,300 --> 01:53:15,030
it being an unordered set shown that

1777
01:53:15,040 --> 01:53:17,790
strictly only in the the number and the number of days

1778
01:53:17,820 --> 01:53:21,790
might be sort of like a combinatorial factor in front of it

1779
01:53:21,790 --> 01:53:28,760
so i'd like to believe that the way you think thinking about that particular sequence

1780
01:53:29,200 --> 01:53:33,060
you be that the likelihood function looks like this

1781
01:53:33,180 --> 01:53:38,220
that's an interesting thing that will come back to that the likelihood function

1782
01:53:38,220 --> 01:53:41,550
is the probability distribution over data over observations

1783
01:53:41,580 --> 01:53:45,280
not a problem with this is not the

1784
01:53:45,370 --> 01:53:50,340
the probability distribution over these things but this is not necessarily a random variable y

1785
01:53:50,340 --> 01:53:52,160
is just parameter

1786
01:53:52,220 --> 01:53:56,790
that would be about the important when we come to do

1787
01:53:56,840 --> 01:53:58,240
when it comes to inference

1788
01:53:58,440 --> 01:54:05,270
OK so let's say well how do we actually do inference in

1789
01:54:05,710 --> 01:54:08,510
in this model

1790
01:54:08,570 --> 01:54:11,390
so the class inference based on

1791
01:54:11,410 --> 01:54:15,490
the idea of estimators so if you want to if you want to make inference

1792
01:54:15,490 --> 01:54:18,790
about something and the first thing you do is you need to to invent an

1793
01:54:20,070 --> 01:54:22,660
and one of the

1794
01:54:23,190 --> 01:54:27,740
and then it was often used so-called maximum likelihood estimator

1795
01:54:27,750 --> 01:54:29,720
basically says well

1796
01:54:29,730 --> 01:54:33,470
that use an estimate of the parameter

1797
01:54:33,520 --> 01:54:36,470
the parameter that actually maximizes the probability

1798
01:54:36,480 --> 01:54:40,000
of the outcome that we actually observe

1799
01:54:40,130 --> 01:54:42,430
this sounds like a pretty reasonable

1800
01:54:42,470 --> 01:54:44,690
a part of

1801
01:54:45,690 --> 01:54:51,310
but notice that we could have chosen another estimate well so the maximum likelihood estimator

1802
01:54:51,330 --> 01:54:54,610
it one one kind of made

1803
01:54:54,660 --> 01:54:56,660
but in physics

1804
01:54:56,680 --> 01:54:58,000
people were

1805
01:54:58,010 --> 01:54:59,500
we're on proving

1806
01:54:59,520 --> 01:55:04,050
various properties of the estimated you can maybe can prove something about

1807
01:55:04,070 --> 01:55:07,190
a particular kind of estimator has some nice properties

1808
01:55:07,190 --> 01:55:09,970
and that would then be reason why this is a good estimate

1809
01:55:10,880 --> 01:55:15,750
that this matter although it sounds pretty reasonable it doesn't really come from anywhere i

1810
01:55:15,760 --> 01:55:17,700
just mentioned

1811
01:55:17,710 --> 01:55:20,480
like somebody thought lot about me

1812
01:55:20,490 --> 01:55:25,120
and this is you can find that military you can find proposal of

1813
01:55:25,140 --> 01:55:29,810
you know various kinds of estimators for various properties in various complicated models

1814
01:55:30,810 --> 01:55:32,580
what you do your classical

1815
01:55:32,620 --> 01:55:34,870
vision reinvent system

1816
01:55:34,990 --> 01:55:37,060
who thinks about the

1817
01:55:37,150 --> 01:55:45,790
well we different estimates for different estimator would be things like penalized maximum likelihood estimation

1818
01:55:45,890 --> 01:55:48,780
so you say well i want to maximize likelihood but also have some of the

1819
01:55:48,780 --> 01:55:51,040
constraints but i also want

1820
01:55:51,110 --> 01:55:54,760
or could be things like leave one out estimators you can you can estimate you

1821
01:55:54,760 --> 01:55:58,390
can manipulate you're your datasets are or

1822
01:55:58,810 --> 01:56:03,860
estimators based on resampling ideas of bootstrapping estimate somehow

1823
01:56:03,880 --> 01:56:07,370
you know you to manipulate the data in various ways and then you do inference

1824
01:56:07,370 --> 01:56:08,180
based on

1825
01:56:08,190 --> 01:56:11,710
on on what happened in the it

1826
01:56:11,760 --> 01:56:14,580
and it became something like you can you can

1827
01:56:14,610 --> 01:56:19,970
you can invent any kind of i can say my estimate fifty

1828
01:56:19,970 --> 01:56:20,990
no there no

1829
01:56:21,010 --> 01:56:21,730
there's no

1830
01:56:21,740 --> 01:56:25,560
there is no formal requirement has to be related to data like any procedure

1831
01:56:25,580 --> 01:56:27,690
we you can computing this is

1832
01:56:27,740 --> 01:56:31,250
the balance may not be might not have good properties

1833
01:56:34,760 --> 01:56:38,750
all right so what would happen if we actually effective by to talk to maximize

1834
01:56:38,750 --> 01:56:42,140
the likelihood in this case

1835
01:56:42,200 --> 01:56:47,000
we can we can simply you know compute the the

1836
01:56:47,040 --> 01:56:48,940
the maximum here we

1837
01:56:48,990 --> 01:56:50,790
predictive of the of the

1838
01:56:50,810 --> 01:56:52,550
likelihood of the log likelihood

1839
01:56:54,390 --> 01:56:57,620
and that they are one of the high then you get high should be

1840
01:57:00,480 --> 01:57:05,190
so this is a good answer

1841
01:57:05,190 --> 01:57:06,830
anybody have

1842
01:57:06,860 --> 01:57:09,890
it seems like the right you if you roll the dice

1843
01:57:09,900 --> 01:57:12,690
one hundred times in fifty six times comes out

1844
01:57:12,700 --> 01:57:15,930
or you a kind

1845
01:57:15,950 --> 01:57:18,580
times six times comes up heads

1846
01:57:18,610 --> 01:57:19,620
then the the

1847
01:57:19,710 --> 01:57:21,140
but think

1848
01:57:23,520 --> 01:57:24,510
very good

1849
01:57:34,110 --> 01:57:36,440
right that that's good at that

1850
01:57:36,480 --> 01:57:40,290
that's an important that say i flip a coin twice

1851
01:57:40,350 --> 01:57:41,910
and about two heads

1852
01:57:41,960 --> 01:57:44,120
now what's the probability

1853
01:57:44,180 --> 01:57:45,640
the zero

1854
01:57:45,690 --> 01:57:51,210
OK and zero means i'm absolutely certain that will happen

1855
01:57:51,290 --> 01:57:53,100
but that doesn't seem right

1856
01:57:53,120 --> 01:57:58,520
this something look at all about that and there are ways to fix

1857
01:57:58,520 --> 01:58:02,750
the properties of the system so maximum likelihood estimators are generally

1858
01:58:02,770 --> 01:58:04,460
if you have a very small datasets

1859
01:58:04,550 --> 01:58:07,760
there are are some problems there are you should you should worry about that

1860
01:58:07,970 --> 01:58:11,290
there are ways to fix these things conceptually

1861
01:58:11,300 --> 01:58:14,510
this is the kind of this kind of scheme you have to go through then

1862
01:58:14,510 --> 01:58:17,600
you have to think about an estimator that has

1863
01:58:17,610 --> 01:58:19,050
the property

1864
01:58:19,050 --> 01:58:19,910
one way

1865
01:58:19,940 --> 01:58:22,060
of of inventing the property

1866
01:58:22,080 --> 01:58:23,100
it's say well

1867
01:58:23,110 --> 01:58:24,400
let's pretend

1868
01:58:24,410 --> 01:58:25,570
we have

1869
01:58:25,610 --> 01:58:29,030
actually observe the head and tail before we start

1870
01:58:29,070 --> 01:58:31,790
you can sort introduces all pseudo observations

1871
01:58:32,640 --> 01:58:34,860
that we will

1872
01:58:34,960 --> 01:58:39,950
prevent this thing from ever reaching one as you you say well no matter how

1873
01:58:39,950 --> 01:58:41,910
many tales i observe

1874
01:58:41,930 --> 01:58:45,520
it could still happen but still come right in the probability if you only have

1875
01:58:45,520 --> 01:58:50,220
to make an observation then you have this sort of extra pseudocount then

