1
00:00:00,000 --> 00:00:01,270
give me the crap

2
00:00:01,290 --> 00:00:04,250
and the same thing teen clothing

3
00:00:04,300 --> 00:00:05,790
two thousand six

4
00:00:07,320 --> 00:00:09,090
the prophets ten k

5
00:00:09,690 --> 00:00:13,940
so this is a kind of very interesting contrast because what happened is that

6
00:00:14,000 --> 00:00:18,110
almost i did you know identical for product same time

7
00:00:18,150 --> 00:00:20,940
the only thing is in two different cities

8
00:00:21,290 --> 00:00:23,060
you know the two states

9
00:00:23,590 --> 00:00:25,860
two provinces

10
00:00:26,210 --> 00:00:27,730
you know the profits

11
00:00:28,520 --> 00:00:30,040
incredibly different

12
00:00:30,040 --> 00:00:32,500
it could be the local managers that

13
00:00:33,670 --> 00:00:34,980
you know the

14
00:00:35,000 --> 00:00:38,250
the culture of thirteen years is entirely different

15
00:00:38,270 --> 00:00:39,570
or it could be

16
00:00:39,590 --> 00:00:44,130
you know one place is very warm and all the clothes made was impossible to

17
00:00:45,190 --> 00:00:48,940
is and pennsylvania maybe it's called it's quite good so this is the kind of

18
00:00:48,940 --> 00:00:50,880
information they want to get

19
00:00:50,920 --> 00:00:55,360
so that you see that is basically multidimensional except that we have the notion of

20
00:00:55,360 --> 00:01:00,820
aggregation and that's what makes interesting you need to do the aggregation of these values

21
00:01:01,090 --> 00:01:04,820
depending on how you putting this fast and then you have to mine that's what

22
00:01:04,820 --> 00:01:08,630
makes this is a really interesting and also complicated

23
00:01:08,690 --> 00:01:11,130
so i'm not going to give you the algorithms

24
00:01:11,150 --> 00:01:12,710
and then

25
00:01:12,750 --> 00:01:15,500
the sequence based contrasts patterns

26
00:01:18,900 --> 00:01:23,840
it's really interesting this is done mainly in the in the context

27
00:01:24,150 --> 00:01:26,290
in genomic kind of stuff

28
00:01:27,520 --> 00:01:28,560
so basically

29
00:01:28,570 --> 00:01:31,110
well you know when we did some mining

30
00:01:31,290 --> 00:01:35,880
two protein families whatever i mean i have no idea what this protein families that

31
00:01:36,170 --> 00:01:39,920
but what we found is that particular group of graph

32
00:01:41,170 --> 00:01:43,040
are very prevalent

33
00:01:43,940 --> 00:01:45,750
in ICDE two hits two

34
00:01:46,770 --> 00:01:48,380
and never appeared in the

35
00:01:48,400 --> 00:01:50,070
the other sequences

36
00:01:50,110 --> 00:01:54,150
so this is you know when you do this kind things biologists are interesting because

37
00:01:54,150 --> 00:01:54,920
they want to know

38
00:01:55,420 --> 00:01:59,210
you know this is the kind of biomarker for them

39
00:01:59,290 --> 00:02:01,060
so this analysis becomes

40
00:02:02,290 --> 00:02:04,320
interesting and important

41
00:02:04,400 --> 00:02:08,980
i and you know you know this is similar to the was trying to to

42
00:02:09,500 --> 00:02:10,710
the bible i think

43
00:02:10,770 --> 00:02:13,000
so what they found is that

44
00:02:13,020 --> 00:02:16,170
you know the one of the things we say gaps means

45
00:02:16,190 --> 00:02:17,920
the worst can appear

46
00:02:17,940 --> 00:02:21,020
a certain distance you know you can constrain gaps

47
00:02:21,020 --> 00:02:22,730
how much capital i

48
00:02:23,500 --> 00:02:26,960
so for example the terms like having horns

49
00:02:27,000 --> 00:02:28,380
this worship

50
00:02:28,570 --> 00:02:30,460
stones price

51
00:02:30,520 --> 00:02:34,020
and ornaments price appear multiple times

52
00:02:34,020 --> 00:02:36,920
in the sentences in the book of revelation

53
00:02:36,920 --> 00:02:39,090
but never in the book of genesis

54
00:02:40,290 --> 00:02:42,960
so if you are guillotine maybe this is interesting

55
00:02:43,520 --> 00:02:47,730
why they know these these kinds of things

56
00:02:47,770 --> 00:02:53,690
so so they found lots of these things by the way this paper won the

57
00:02:53,690 --> 00:02:57,420
best paper award in and ICDM conference

58
00:02:57,420 --> 00:03:01,650
because it's quite tricky very difficult and tricky algorithms

59
00:03:03,060 --> 00:03:05,040
which is by why the is given here

60
00:03:07,630 --> 00:03:09,630
so the idea here is that

61
00:03:10,380 --> 00:03:14,650
i just spend a few minutes on this because we want how much time

62
00:03:14,650 --> 00:03:18,820
let's say you are given a sequence e one e two e n

63
00:03:18,860 --> 00:03:21,960
and each of these elements i

64
00:03:22,020 --> 00:03:26,070
it could be a particular of so for example if the DNA

65
00:03:26,110 --> 00:03:28,440
they come from let's say

66
00:03:28,880 --> 00:03:31,480
a CGT these are the

67
00:03:31,480 --> 00:03:34,230
and amino

68
00:03:37,000 --> 00:03:38,690
nuclear that is

69
00:03:40,590 --> 00:03:45,150
and then we say is a subsequence of AGCA

70
00:03:45,170 --> 00:03:46,840
notice that c

71
00:03:46,880 --> 00:03:48,560
doesn't appear

72
00:03:48,570 --> 00:03:51,710
one after the other you can have a gap so for example

73
00:03:51,750 --> 00:03:53,960
you know g appears

74
00:03:54,070 --> 00:03:56,900
in the main main structure AGCA

75
00:03:57,090 --> 00:04:02,900
if we subsequent manages he's substrings substring means that you don't expect any

76
00:04:02,960 --> 00:04:05,210
spruce things appearing in between

77
00:04:06,360 --> 00:04:09,270
so when we do the is a subsequence

78
00:04:09,320 --> 00:04:16,150
you might have some constraints some gap that means you you allow was listed one

79
00:04:16,170 --> 00:04:19,820
one element to appear in between it a gap of one

80
00:04:19,820 --> 00:04:20,960
OK so

81
00:04:20,960 --> 00:04:22,790
so if the gap zero then the

82
00:04:22,800 --> 00:04:25,130
we can automatically substrings

83
00:04:25,230 --> 00:04:27,560
it is a an example

84
00:04:27,640 --> 00:04:31,170
let's say you are given a sequence a c

85
00:04:31,210 --> 00:04:33,900
a c b c b

86
00:04:33,940 --> 00:04:35,500
and now we try to see

87
00:04:35,500 --> 00:04:36,290
tell me

88
00:04:36,300 --> 00:04:41,570
busy appearing so first of all what we do is that we index these elements

89
00:04:41,570 --> 00:04:43,070
in the first phase one

90
00:04:43,070 --> 00:04:44,440
season two

91
00:04:44,460 --> 00:04:47,460
the next phase three three and so on

92
00:04:47,520 --> 00:04:49,980
so if you look at the first one one five

93
00:04:50,040 --> 00:04:52,210
what it says is a year

94
00:04:52,250 --> 00:04:54,560
at position one

95
00:04:54,570 --> 00:05:01,070
that is b appears at position five so still it is the subsequence but the

96
00:05:01,070 --> 00:05:02,920
gap four

97
00:05:02,960 --> 00:05:05,480
OK between those

98
00:05:05,480 --> 00:05:07,130
and then

99
00:05:07,650 --> 00:05:11,170
the next sequence you could also say is one seven

100
00:05:13,070 --> 00:05:15,170
the first phase that the first thing

101
00:05:15,190 --> 00:05:16,650
and the second

102
00:05:16,670 --> 00:05:18,670
can be

103
00:05:18,690 --> 00:05:20,560
appeared in seventh position

104
00:05:20,610 --> 00:05:21,570
so four

105
00:05:21,570 --> 00:05:25,400
so now i can go on to the next day which is location three

106
00:05:25,400 --> 00:05:28,270
and then try to match the then it appears five

107
00:05:28,290 --> 00:05:29,000
and the

108
00:05:29,290 --> 00:05:31,590
and then i can also have several

109
00:05:31,630 --> 00:05:34,320
so similarly if you're looking for a CB

110
00:05:34,340 --> 00:05:36,250
we can build these sequences

111
00:05:36,300 --> 00:05:40,070
so now what we can do is you put some constraints on the distances between

112
00:05:40,070 --> 00:05:44,150
these elements and that's what we call this gap constraint

113
00:05:44,170 --> 00:05:45,290
the game

114
00:05:45,290 --> 00:05:47,840
you know these are interesting for

115
00:05:47,860 --> 00:05:50,110
biology and other kinds of places

116
00:05:50,170 --> 00:05:55,060
so it is again a very tricky algorithm and then you can define

117
00:05:55,110 --> 00:05:58,040
this constraint on this for these things

118
00:05:58,040 --> 00:06:01,790
the minimum frequency like you know once you find these sequences you might want to

119
00:06:01,790 --> 00:06:05,590
say how many times they have to appear that is the minimum support and you

120
00:06:06,320 --> 00:06:10,480
how that you know if you contest with another sequence said

121
00:06:10,500 --> 00:06:14,900
what is the maximum that there can be allowed so this is actually our support

122
00:06:15,900 --> 00:06:20,750
and then we develop a condition is there is no subsequence of p satisfying one

123
00:06:20,750 --> 00:06:22,400
product or norm

124
00:06:22,410 --> 00:06:26,080
the product of two functions in two

125
00:06:26,090 --> 00:06:29,380
in the interwar of their products

126
00:06:29,390 --> 00:06:35,450
and it's equivalent to your usual dot product in our here

127
00:06:35,460 --> 00:06:40,280
which would be easy for you humans the vector u one

128
00:06:41,460 --> 00:06:44,220
and the reason he one v two

129
00:06:44,250 --> 00:06:47,140
then you're not products

130
00:06:47,150 --> 00:06:50,810
is there some u one

131
00:06:50,830 --> 00:06:52,520
u one class

132
00:06:52,680 --> 00:06:56,040
u two two v two

133
00:06:56,140 --> 00:06:58,490
so that product in it two

134
00:06:58,510 --> 00:07:04,950
user norm which is the the square root of the integral of the square of

135
00:07:04,960 --> 00:07:10,030
and that is going to be useful because you you defined with which is the

136
00:07:10,040 --> 00:07:16,310
product norm you're going to be able to define things that are orthogonal and decomposing

137
00:07:16,830 --> 00:07:21,760
vectors on sets of also in the thing is going to make your life easier

138
00:07:21,830 --> 00:07:26,850
and the next thing you have been in two of our which is important is

139
00:07:26,850 --> 00:07:34,030
its closeness so if you take a sequence of function f and that converges to

140
00:07:34,080 --> 00:07:36,810
four which is the number of

141
00:07:36,890 --> 00:07:45,580
different scholars in just two zero then the limit function is is in two

142
00:07:45,590 --> 00:07:50,520
it's not always true right and sets are not always born

143
00:07:50,530 --> 00:07:54,470
close issues take for example

144
00:07:54,500 --> 00:08:02,850
it's use that is

145
00:08:03,400 --> 00:08:08,950
the interval between zero and one but not continues you all one

146
00:08:08,950 --> 00:08:10,510
and you can find

147
00:08:10,520 --> 00:08:13,760
if you take x and

148
00:08:13,760 --> 00:08:21,460
the sequence which is one of and for each and then x goes to zero

149
00:08:21,490 --> 00:08:28,160
but you know is not in your

150
00:08:28,330 --> 00:08:30,180
your yourself as

151
00:08:30,190 --> 00:08:31,890
so which means that

152
00:08:31,900 --> 00:08:35,490
you may have limit of things that are already in s

153
00:08:35,510 --> 00:08:40,160
which is the limit is not in s and that's one has not closed

154
00:08:41,070 --> 00:08:46,400
well to are as it is this ability that if you have a convergent sequence

155
00:08:47,080 --> 00:08:48,580
are sure that the

156
00:08:48,590 --> 00:08:54,580
images in order to steal and that's important

157
00:08:54,640 --> 00:08:56,620
OK so

158
00:08:56,630 --> 00:09:02,630
go to a formal different definition of vector spaces which is just summarizing what was

159
00:09:02,640 --> 00:09:03,830
said so far

160
00:09:03,840 --> 00:09:09,040
if you've said as it's called the vector space if the

161
00:09:09,520 --> 00:09:12,350
additionally the war operations

162
00:09:12,500 --> 00:09:19,280
if the addition is the addition is commutative and distributive and associated

163
00:09:21,100 --> 00:09:25,530
as most of an known element for the addition

164
00:09:25,560 --> 00:09:30,960
and every element of asthma be invertible for the addition so basically you have an

165
00:09:30,960 --> 00:09:37,490
opposite minus an operation which is the minus

166
00:09:37,500 --> 00:09:43,220
in addition if you want a real vector space you need to have

167
00:09:43,240 --> 00:09:46,900
as which is favoured by multiplication by any real

168
00:09:48,580 --> 00:09:52,030
or any skater either within the skater

169
00:09:52,140 --> 00:09:54,060
learn that

170
00:09:54,120 --> 00:09:59,940
and the multiplication by a scalar must have the property that it does it is

171
00:09:59,940 --> 00:10:03,160
associative and distributive over addition

172
00:10:03,320 --> 00:10:09,150
so all of this is basically just saying you have an addition

173
00:10:09,160 --> 00:10:12,210
if you have two vectors it's still vector

174
00:10:12,240 --> 00:10:16,610
OK you have to do the opposite of the vector does exists and the multiplication

175
00:10:16,610 --> 00:10:18,520
of the vector of ice skater

176
00:10:18,760 --> 00:10:22,520
is vector as well

177
00:10:22,540 --> 00:10:28,120
so you thing when you want to see what's going on as a vector spaces

178
00:10:28,120 --> 00:10:30,440
to divided into smaller spaces

179
00:10:30,470 --> 00:10:36,370
and so subspaces basically subset of s which have the same property so they are

180
00:10:36,370 --> 00:10:38,020
stable by addition

181
00:10:38,060 --> 00:10:41,610
and still by matthew to plication may escape

182
00:10:42,960 --> 00:10:55,140
nonlinear subspaces that of

183
00:10:55,980 --> 00:10:58,510
help your article falls

184
00:10:58,520 --> 00:11:06,410
your vector spaces in and also because of promontory subspaces

185
00:11:06,420 --> 00:11:08,820
so inside i think two again

186
00:11:08,840 --> 00:11:19,800
in my subspace as it is the one that is defined by my my friends

187
00:11:19,800 --> 00:11:21,380
we have to make sure

188
00:11:21,420 --> 00:11:24,290
so you want your

189
00:11:24,300 --> 00:11:27,140
then if you take any other

190
00:11:27,230 --> 00:11:31,290
vector that is not quality near to you one so that that is not in

191
00:11:31,290 --> 00:11:36,190
the same direction as you one for example this one there v

192
00:11:36,210 --> 00:11:39,820
the user subspace which is the

193
00:11:40,000 --> 00:11:43,790
the set of all the them the

194
00:11:43,800 --> 00:11:46,300
the vectors that are proportional to v

195
00:11:46,360 --> 00:11:49,520
those who subspace there are

196
00:11:49,530 --> 00:11:52,860
liquid crystal display in f one and f two

197
00:11:52,870 --> 00:11:56,220
i actually see supplementary subspaces

198
00:11:56,230 --> 00:11:59,610
which means that for any

199
00:11:59,640 --> 00:12:01,720
and so that you have in our two

200
00:12:01,740 --> 00:12:05,210
then you may write it as the sum of

201
00:12:06,400 --> 00:12:09,150
so for the cause

202
00:12:09,210 --> 00:12:10,740
projections there

203
00:12:15,990 --> 00:12:23,440
so your vector x is that is can be written as

204
00:12:23,460 --> 00:12:33,180
x one times the first vector u one this x two times the second vector

205
00:12:33,240 --> 00:12:38,760
and actually is to to subspaces are said to be suppressed promontory if such a

206
00:12:38,760 --> 00:12:41,150
decomposition always exists

207
00:12:46,110 --> 00:12:52,250
train the notations are varied with this new thing with the x of s one

208
00:12:52,260 --> 00:12:54,610
and the thing would be x

209
00:12:54,650 --> 00:12:55,910
f two

210
00:13:02,490 --> 00:13:07,410
now this space here f one generated by u one

211
00:13:07,420 --> 00:13:12,440
and similarly if you take a family of vectors in as you can generate the

212
00:13:12,460 --> 00:13:14,390
vector subspace

213
00:13:14,390 --> 00:13:20,010
the new permutation has higher likelihood than the old one i always accept experimentation

214
00:13:20,030 --> 00:13:23,390
otherwise i will accept it with this probability OK

215
00:13:23,390 --> 00:13:26,620
this is you can think of this is some kind of similar to annealing great

216
00:13:26,800 --> 00:13:31,160
i tried i have my current mediation i swapped elements if this is if there

217
00:13:31,160 --> 00:13:35,320
is better attention more like a permutation and keep it if it doesn't

218
00:13:36,070 --> 00:13:40,070
with some with some probability right i was thinking so this is this is the

219
00:13:40,070 --> 00:13:44,910
segmentation of the body that so i'm just doing this swapping if it gets me

220
00:13:44,910 --> 00:13:49,890
if it gets too better permutations right more likely permutation psyche that if you get

221
00:13:49,890 --> 00:13:56,220
it was permutations i will keep it with some probability with this you taken from

222
00:13:57,470 --> 00:14:04,280
she made this is just no i mean

223
00:14:04,300 --> 00:14:05,930
just this just intonation

224
00:14:05,930 --> 00:14:08,740
OK so the idea is if it if the if the and

225
00:14:08,890 --> 00:14:13,260
change gets more likely to better part of the space of a given that that

226
00:14:13,260 --> 00:14:19,430
otherwise just keep with some probability of i will show

227
00:14:21,320 --> 00:14:24,620
so this is this is what you are doing the other thing that's very nice

228
00:14:24,620 --> 00:14:28,580
here is right now we can do local changes in my distribution and then just

229
00:14:28,760 --> 00:14:33,030
a asserting about my presentation in just swapping elements right so this is my

230
00:14:33,050 --> 00:14:39,100
the adjacency matrix i that says what basically doing swapping the rows and columns of

231
00:14:39,100 --> 00:14:44,660
this adjacency matrix sites so if i sort of elements jnk this means i'm swapping

232
00:14:44,720 --> 00:14:46,240
jason case

233
00:14:46,240 --> 00:14:49,570
o and call right so just swapping

234
00:14:49,580 --> 00:14:54,740
this total single OK but what this means is that a large portion of the

235
00:14:54,850 --> 00:14:59,370
adjacency matrix remains constant right so when i will get when i began creating this

236
00:14:59,370 --> 00:15:03,260
ratio of this parts that we can slow

237
00:15:03,260 --> 00:15:07,720
right so i don't really need to given the permutation i don't need to go

238
00:15:07,720 --> 00:15:10,660
and evaluate everything i just need to invent

239
00:15:10,680 --> 00:15:12,720
this tour also in columns

240
00:15:12,740 --> 00:15:13,950
does it make sense

241
00:15:19,340 --> 00:15:22,570
if you your

242
00:15:22,740 --> 00:15:25,660
class lot of

243
00:15:25,680 --> 00:15:28,120
and the changes

244
00:15:29,550 --> 00:15:35,990
sure should i mean there are many ways how you could do this and nobody

245
00:15:35,990 --> 00:15:42,120
says you want to do local local changes on your on your on your own

246
00:15:42,160 --> 00:15:47,910
limitation because this would be this will be bad in the sense that it would

247
00:15:47,910 --> 00:15:52,800
take long time to get to get completely random permutation for example that the strategy

248
00:15:52,800 --> 00:15:58,240
would be to pick up some element element to transport and inserted at some other

249
00:15:58,240 --> 00:16:01,680
random spotlight and this would then shift all the other elements by one and this

250
00:16:01,680 --> 00:16:07,700
is much better shuffling strategy the problem with that some shopping strategy that it's a

251
00:16:07,700 --> 00:16:14,120
better strategy so it needs to be more efficient or less less less samples to

252
00:16:14,970 --> 00:16:18,680
or two to get that sample from the distribution but the problem is that it's

253
00:16:18,680 --> 00:16:22,840
you it's not local so everything will change so we have to evaluate much more

254
00:16:22,870 --> 00:16:25,030
i think the i mean it's not clear

255
00:16:25,050 --> 00:16:27,930
it's not entirely clear why this would be obvious

256
00:16:27,990 --> 00:16:32,260
i mean why is it obvious that that this here elements we can see why

257
00:16:33,340 --> 00:16:40,580
because if i if i now slob this so it means i'm is taking this

258
00:16:40,580 --> 00:16:44,780
one is i think it here then the whole documents one to the right so

259
00:16:44,780 --> 00:16:46,720
we can use all

260
00:16:46,740 --> 00:16:48,800
like one

261
00:16:48,800 --> 00:16:52,760
it's better for getting it's about shuffling strategy

262
00:16:52,780 --> 00:16:54,510
but because one

263
00:16:54,530 --> 00:16:56,180
because it

264
00:16:57,550 --> 00:17:01,180
i have all of the

265
00:17:01,200 --> 00:17:04,100
because if you start you can show that i know

266
00:17:04,120 --> 00:17:10,080
so now i'm just talking about but memory but i think it's started

267
00:17:10,100 --> 00:17:14,100
but like all the permutation and you ask how long how many shuffles we need

268
00:17:14,120 --> 00:17:18,800
to get to get to a random permutation for for this thing you i think

269
00:17:18,800 --> 00:17:22,850
you will like water and square swaps well if you take and the one circuit

270
00:17:22,850 --> 00:17:25,680
you will need like water and logan or something

271
00:17:25,700 --> 00:17:29,490
so it's much better in this so we need less to get

272
00:17:29,510 --> 00:17:33,200
to sample the whole space the problem is that you won't have this locality so

273
00:17:33,200 --> 00:17:38,780
we have to every at every iteration it would cost more than OK but

274
00:17:41,260 --> 00:17:44,320
so this is the idea how to get how to sample

275
00:17:44,740 --> 00:17:46,740
how to sample these

276
00:17:46,760 --> 00:17:51,720
limitations from the from distribution using these local moves this works quickly and the other

277
00:17:51,720 --> 00:17:55,070
one is this this observation that makes everything given fast

278
00:17:55,120 --> 00:17:57,530
OK now the other questions

279
00:17:57,550 --> 00:17:59,300
so now i have this

280
00:17:59,320 --> 00:18:03,010
so i have the parameters i have made i have the how they how they

281
00:18:04,070 --> 00:18:09,410
how they calculate the probability and before we were so if you do you will

282
00:18:09,410 --> 00:18:12,390
do it maybe you would have to evaluate every cell

283
00:18:12,410 --> 00:18:17,780
of these adjacency matrix which takes children square but what you can notice is that

284
00:18:17,800 --> 00:18:23,430
every element here we have is that this for so so if i have my

285
00:18:23,430 --> 00:18:26,700
capital state that the parameter matrix which is let's say two by two then i

286
00:18:26,700 --> 00:18:31,850
have four parameters so this is for free and then

287
00:18:31,870 --> 00:18:33,740
the problem here will be like

288
00:18:35,120 --> 00:18:39,100
so we first parameter some power second parameter some power and so on

289
00:18:39,100 --> 00:18:43,910
in the sum of this parameter and this this this this this should sum to

290
00:18:43,910 --> 00:18:48,780
some constant like let's say OK so here this this one of the things i

291
00:18:48,780 --> 00:18:52,030
want to the k and then a b and c and d will be the

292
00:18:52,030 --> 00:18:55,530
and and so on right so basically having

293
00:18:55,550 --> 00:18:57,800
if you look carefully you get

294
00:18:57,800 --> 00:19:03,520
really conditional distributions taken from these from joint models so

295
00:19:04,350 --> 00:19:08,270
what i want do in the last third of forty five minutes here is to

296
00:19:08,270 --> 00:19:12,080
try and give you a look at one way

297
00:19:12,090 --> 00:19:18,740
of compact representing joint probability distributions and working with them they should always keeping in

298
00:19:18,740 --> 00:19:23,670
mind the motivation for studying these joint probability distributions is this whole set up of

299
00:19:23,670 --> 00:19:28,560
machine learning in the probabilistic approach to machine learning that just gave you but now

300
00:19:28,580 --> 00:19:32,250
we're going to move to a more general discussion which is just about the joint

301
00:19:32,250 --> 00:19:37,980
probability distributions and i'll try and try and go through that to give you some

302
00:19:37,980 --> 00:19:40,890
sense of what this with this

303
00:19:40,900 --> 00:19:43,940
field of graphical models really about

304
00:19:44,680 --> 00:19:48,070
one of our goals here is to represent the joint distribution

305
00:19:48,080 --> 00:19:52,220
compactly even when there are many many variables

306
00:19:52,230 --> 00:19:56,320
so remember we said that if you have say variables with two hundred fifty five

307
00:19:56,320 --> 00:20:00,300
settings are two hundred fifty six settings you have thousands of them then those two

308
00:20:00,300 --> 00:20:01,830
hundred fifty six to the

309
00:20:01,850 --> 00:20:05,160
thousand or to the ten thousand possible

310
00:20:05,210 --> 00:20:11,740
inputs and so just to place the most general distribution you could which is a

311
00:20:11,770 --> 00:20:17,960
just a histogram over those possible settings has far too many degrees of freedom

312
00:20:17,970 --> 00:20:23,680
so that's one more compact representation and the second goal is to efficiently calculate marginal

313
00:20:23,680 --> 00:20:24,960
and conditional

314
00:20:24,990 --> 00:20:29,990
distributions of these joints given our representation

315
00:20:30,010 --> 00:20:31,510
and that goal might

316
00:20:31,520 --> 00:20:33,020
be a bit less

317
00:20:33,050 --> 00:20:37,740
clearly motivated for you but i think just in a couple of equations here i

318
00:20:37,740 --> 00:20:38,870
can convince you

319
00:20:38,970 --> 00:20:41,700
why we care about that for example

320
00:20:41,720 --> 00:20:47,690
let's say that you do manage to learn a joint distribution p of x y

321
00:20:47,700 --> 00:20:50,210
all these are the inputs

322
00:20:50,240 --> 00:20:53,330
of your learning machine and these easily output

323
00:20:53,620 --> 00:20:58,920
let's say you did manage to learn this distribution from your training data which are

324
00:20:58,920 --> 00:21:01,590
just samples from the joint distribution

325
00:21:01,600 --> 00:21:04,210
what would you do if joint distribution

326
00:21:04,220 --> 00:21:09,470
to answer question test time like what do you think the answer is the output

327
00:21:09,470 --> 00:21:13,310
is for a new input

328
00:21:15,110 --> 00:21:18,120
the best thing you can do in this sort of obvious thing is to look

329
00:21:18,120 --> 00:21:19,990
at the conditional distribution

330
00:21:20,000 --> 00:21:22,860
p of y given x

331
00:21:24,560 --> 00:21:28,590
this is actually your machines

332
00:21:28,600 --> 00:21:32,730
your machine is the conditional distribution you put the new input in here and your

333
00:21:32,730 --> 00:21:36,880
machine tells you all for that input that you put in here i think that

334
00:21:37,310 --> 00:21:41,500
the output is one with probability point four

335
00:21:42,190 --> 00:21:46,420
it's actually better than a deterministic function deterministic functions gives you an answer i don't

336
00:21:46,420 --> 00:21:51,390
know how it about that but this thing is uniform distribution over the outputs given

337
00:21:51,390 --> 00:21:55,040
the inputs so you can see very simply that if you manage to learn the

338
00:21:55,060 --> 00:21:59,820
joint distribution and you had an efficient way of calculating the conditional but you would

339
00:21:59,820 --> 00:22:03,500
be able to answer queries at test time very nice

340
00:22:03,550 --> 00:22:06,580
OK so that's the the motivation

341
00:22:06,600 --> 00:22:07,770
OK so

342
00:22:07,780 --> 00:22:12,050
the observation that we already made is that the arbitrary

343
00:22:13,500 --> 00:22:18,910
naive representation of the joint distribution in the case of categorical variables is huge

344
00:22:18,920 --> 00:22:21,570
it requires a huge number of entries

345
00:22:21,580 --> 00:22:27,260
so we need to make some assumptions about the distribution and one assumption is

346
00:22:27,300 --> 00:22:30,770
very very strong assumption it's complete factorizations

347
00:22:30,820 --> 00:22:35,610
OK it says that the joint distribution is just the product of marginal distributions on

348
00:22:35,610 --> 00:22:41,240
all the entries so that would be like saying the probability distribution over an image

349
00:22:41,270 --> 00:22:46,600
is just the product of some individual probability distributions on each pixel

350
00:22:47,330 --> 00:22:52,680
that's a very crazy distribution over images in particular tells you that knowing all the

351
00:22:52,680 --> 00:22:58,060
pixels in a huge block that surround this one little pixel centre tells you nothing

352
00:22:58,060 --> 00:22:59,370
about this pixel

353
00:22:59,390 --> 00:23:01,340
it doesn't help predict that pixel at all

354
00:23:01,360 --> 00:23:04,960
of course we all know the real world images this is true right

355
00:23:05,010 --> 00:23:09,770
so image is represented by this kind of completely factorized distribution look like the snow

356
00:23:09,780 --> 00:23:15,170
that you get when you tune your television to the incorrect just completely random

357
00:23:15,180 --> 00:23:17,640
collections of pixels uncorrelated in space

358
00:23:17,650 --> 00:23:22,440
but it has the advantage that very very compact right

359
00:23:22,450 --> 00:23:26,950
so if you wanted to represent the distribution over binary images

360
00:23:26,960 --> 00:23:34,010
with n pixels using this distribution how many numbers would you need

361
00:23:34,020 --> 00:23:38,190
we just need it one number for every pixel which tells you what's the probability

362
00:23:38,200 --> 00:23:40,520
that pixel is going to be on

363
00:23:41,250 --> 00:23:46,450
so it is obviously much smaller than two d and minus one which is the

364
00:23:46,450 --> 00:23:51,800
number of parameters you need to represent an arbitrary distribution over all possible binary images

365
00:23:53,220 --> 00:23:57,110
OK so there's a huge gap from exponential to the the end delta and which

366
00:23:57,110 --> 00:24:01,630
is factorized and the question is is there anything that lives inside that gap from

367
00:24:01,630 --> 00:24:05,670
an arbitrary distribution over images which has so many parameters could never hope to learn

368
00:24:05,670 --> 00:24:10,400
it to a factorized distribution which only has an parameters but is soul

369
00:24:10,440 --> 00:24:12,770
restrictive that it's basically useless

370
00:24:12,780 --> 00:24:15,780
so what's in the middle and that's what we're going to study today is what

371
00:24:15,780 --> 00:24:21,360
is in the middle so rather than independence assumptions complete factorisation we can make what's

372
00:24:21,360 --> 00:24:24,570
called conditional independence assumptions

373
00:24:25,430 --> 00:24:28,840
so just a quick review for those of you who don't oh by the way

374
00:24:28,840 --> 00:24:30,120
i should say that

375
00:24:30,140 --> 00:24:32,040
i think the

376
00:24:32,090 --> 00:24:36,390
in the notes there should be a quick probability and statistics review

377
00:24:36,400 --> 00:24:38,340
for those of you who don't

378
00:24:38,360 --> 00:24:42,390
no it so could look through those things in but

379
00:24:42,410 --> 00:24:44,030
just very quickly

380
00:24:44,050 --> 00:24:48,120
the notation for conditional independencies this inverted t

381
00:24:48,130 --> 00:24:54,710
it says that a variable x is conditionally independent of another variable x given xc

382
00:24:54,730 --> 00:25:00,010
and what that means is that knowing x c

383
00:25:01,200 --> 00:25:03,000
knowing x b

384
00:25:03,010 --> 00:25:08,480
does not tell you anything more about x a then you already knew

385
00:25:08,500 --> 00:25:10,710
from having measured x

386
00:25:10,760 --> 00:25:12,670
that's what this means

387
00:25:12,680 --> 00:25:15,250
it means that if i tell you xie

388
00:25:15,260 --> 00:25:16,880
only if i tell you see

389
00:25:17,870 --> 00:25:21,500
x eight doesn't give you any more sorry XB doesn't give you any more information

390
00:25:21,500 --> 00:25:22,650
about this

391
00:25:22,690 --> 00:25:26,670
and that's the idea of conditional independence and you can write that down in two

392
00:25:26,670 --> 00:25:32,150
ways you can write it down by saying the joint distribution over a and b

393
00:25:32,360 --> 00:25:37,800
given x e is just the product of the conditional distributions for all settings of

394
00:25:37,800 --> 00:25:41,650
c or you can write it down in this other re which just says that

395
00:25:41,650 --> 00:25:46,020
the distribution of a given both b and c is the same as the distribution

396
00:25:46,020 --> 00:25:49,120
of a given c real care about b

397
00:25:51,060 --> 00:25:57,350
obviously for any particular independence assumption that we make might say oh pixel thirty is

398
00:25:57,350 --> 00:26:01,910
conditionally independent of pixel eighteen given pixel twenty four

399
00:26:01,930 --> 00:26:06,640
OK that some conditional independence then we can make for any particular independent state we

400
00:26:06,640 --> 00:26:13,530
make like that only a tiny subset of all distributions satisfy that assumption

401
00:26:14,210 --> 00:26:20,680
so the idea of graphical models are probabilistic graphical models

402
00:26:20,690 --> 00:26:26,600
there is a very powerful and elegant way to specify a set of conditional independence

403
00:26:26,600 --> 00:26:32,910
assumptions and thus specify a family restricted family of probability distributions

404
00:26:33,100 --> 00:26:40,230
so the way probabilistic graphical models work is they represent large

405
00:26:40,230 --> 00:26:42,440
and we even measured the period

406
00:26:42,450 --> 00:26:45,500
and we found very very good agreement with the

407
00:26:45,570 --> 00:26:49,500
theoretical predictions

408
00:26:49,500 --> 00:26:52,720
you could ask not what is the kinetic energy of rotation

409
00:26:52,770 --> 00:26:56,710
of these rods which changes with time

410
00:26:56,720 --> 00:26:58,800
kinetic energy of rotation

411
00:26:58,860 --> 00:27:01,490
is one half i omega squid

412
00:27:01,520 --> 00:27:04,960
remember the linear kinetic energy is one half

413
00:27:04,960 --> 00:27:07,730
MV squared

414
00:27:07,740 --> 00:27:10,580
and becomes i you gotta rotation

415
00:27:11,380 --> 00:27:13,790
becomes omega

416
00:27:13,840 --> 00:27:17,000
so the kinetic energy of rotation

417
00:27:17,020 --> 00:27:23,190
equals one half i omega scripts you'll find this equation

418
00:27:23,240 --> 00:27:25,440
on the exam

419
00:27:25,480 --> 00:27:29,360
this is i about four p

420
00:27:29,420 --> 00:27:32,270
you know what fair use as a function of time

421
00:27:34,560 --> 00:27:35,470
he calls

422
00:27:35,480 --> 00:27:37,220
the theta DT

423
00:27:37,270 --> 00:27:39,250
so you can find with omega is

424
00:27:39,290 --> 00:27:41,890
you know what i of these which is calculated

425
00:27:41,910 --> 00:27:42,910
so you know

426
00:27:42,950 --> 00:27:47,090
what the kinetic energy of rotation is at any moment in time

427
00:27:47,140 --> 00:27:48,570
it will change

428
00:27:48,620 --> 00:27:51,860
it will be zero when this comes to a halt

429
00:27:51,910 --> 00:27:53,980
and it will be a maximum

430
00:27:54,110 --> 00:27:57,590
when it is here

431
00:27:57,670 --> 00:28:03,520
it's a continuous conversion from gravitational potential energy which is the maximum here

432
00:28:03,540 --> 00:28:05,730
to kinetic energy

433
00:28:05,730 --> 00:28:07,730
which is the maximum

434
00:28:07,830 --> 00:28:09,020
so this will

435
00:28:12,000 --> 00:28:13,310
we time connecticut

436
00:28:13,340 --> 00:28:16,330
energy of rotation

437
00:28:16,380 --> 00:28:17,940
i thought problem

438
00:28:17,950 --> 00:28:20,510
eight one wasn't nice example

439
00:28:20,560 --> 00:28:23,660
of how to apply

440
00:28:23,670 --> 00:28:26,480
couple lost newton's

441
00:28:26,550 --> 00:28:28,250
laws of gravity

442
00:28:28,250 --> 00:28:30,410
in the case that we have

443
00:28:30,450 --> 00:28:33,310
an elliptical orbit

444
00:28:33,370 --> 00:28:34,280
you we have

445
00:28:37,210 --> 00:28:40,330
are it's not rotating it has no atmosphere

446
00:28:40,370 --> 00:28:45,800
grant you a little bit artificial

447
00:28:45,860 --> 00:28:47,890
we launch satellites

448
00:28:47,990 --> 00:28:51,200
and the satellite

449
00:28:51,240 --> 00:28:53,010
that's the velocity here

450
00:28:53,020 --> 00:28:55,580
it was called the zero

451
00:28:55,600 --> 00:28:58,050
as it is right here at the surface

452
00:28:58,060 --> 00:28:59,990
this angle

453
00:28:59,990 --> 00:29:01,420
twenty degrees

454
00:29:01,470 --> 00:29:02,750
and we were told

455
00:29:03,680 --> 00:29:06,310
the point farthest away was five

456
00:29:06,330 --> 00:29:09,110
planet radio waves

457
00:29:10,320 --> 00:29:11,380
if i try to

458
00:29:11,390 --> 00:29:13,990
sketch the elliptical orbit assuming

459
00:29:14,050 --> 00:29:18,760
that the velocities instantaneously reached mean this point must also be a point of the

460
00:29:20,250 --> 00:29:22,470
if i try to make a sketch

461
00:29:23,520 --> 00:29:26,390
that elliptical orbit

462
00:29:26,440 --> 00:29:29,540
would look something like this

463
00:29:29,570 --> 00:29:36,630
something like this

464
00:29:36,710 --> 00:29:38,350
and then this distance

465
00:29:38,380 --> 00:29:40,980
five four

466
00:29:41,140 --> 00:29:42,660
i call this point eight

467
00:29:42,660 --> 00:29:46,790
one that's farthest away from the sun

468
00:29:46,880 --> 00:29:52,090
it's clear that the satellite will crashed back onto the planet

469
00:29:52,140 --> 00:29:56,030
but that's no concern to us

470
00:29:56,040 --> 00:29:57,770
when i draw this ellipse

471
00:29:57,780 --> 00:29:59,520
i made the assumption

472
00:30:01,220 --> 00:30:06,040
all the mass of the planet was inside the ellipse for instance somewhere here

473
00:30:06,060 --> 00:30:07,920
so that is not the case

474
00:30:07,980 --> 00:30:10,660
so only this part of the ellipse

475
00:30:10,710 --> 00:30:11,850
is realistic

476
00:30:11,870 --> 00:30:16,180
and this part of course is not

477
00:30:16,220 --> 00:30:17,770
which you're being asked

478
00:30:18,440 --> 00:30:22,830
to calculate what the zero was only based on the information of the five are

479
00:30:22,850 --> 00:30:24,300
on the twenty degrees

480
00:30:24,310 --> 00:30:25,670
and all the mass

481
00:30:25,670 --> 00:30:27,910
and i can add to it

482
00:30:27,920 --> 00:30:29,920
what is the velocity he a

483
00:30:29,930 --> 00:30:31,620
and that's farthest away

484
00:30:31,680 --> 00:30:34,370
and i can even add to it

485
00:30:34,430 --> 00:30:35,520
what is the

486
00:30:35,580 --> 00:30:38,140
so my major axis

487
00:30:38,180 --> 00:30:41,000
where this is to a all that

488
00:30:41,060 --> 00:30:42,220
comes for free

489
00:30:42,220 --> 00:30:43,230
was that

490
00:30:45,030 --> 00:30:46,850
and the knowledge that it goes out

491
00:30:46,860 --> 00:30:49,700
five planet radio

492
00:30:49,740 --> 00:30:52,530
and you meant to melt

493
00:30:52,590 --> 00:30:54,350
articles are

494
00:30:56,440 --> 00:30:57,960
and you know omentum

495
00:30:57,970 --> 00:31:01,290
this conserved for these objects in orbit

496
00:31:01,310 --> 00:31:03,600
mass little and but only

497
00:31:03,640 --> 00:31:04,980
if you

498
00:31:05,020 --> 00:31:06,620
take the angular momentum

499
00:31:06,630 --> 00:31:08,830
relative to this point

500
00:31:08,880 --> 00:31:11,140
i don't know what i call the point

501
00:31:11,230 --> 00:31:13,470
looks like i call the he

502
00:31:13,520 --> 00:31:16,730
i can hardly read quality for now

503
00:31:16,740 --> 00:31:19,850
the angular momentum of this object in this orbit

504
00:31:19,870 --> 00:31:22,360
is not conserved relative to any point

505
00:31:22,380 --> 00:31:26,290
the only conservative to this point

506
00:31:26,300 --> 00:31:29,520
sorry angular momentum relative to point c

507
00:31:29,530 --> 00:31:31,530
at this moment in time

508
00:31:31,610 --> 00:31:35,000
is the position vector which is capitol are

509
00:31:35,050 --> 00:31:36,770
times the velocity

510
00:31:37,580 --> 00:31:40,950
i have to multiplied by the sine of twenty degrees

511
00:31:41,040 --> 00:31:44,210
so it is and

512
00:31:44,240 --> 00:31:45,130
it is

513
00:31:45,190 --> 00:31:48,270
and the right so we get a zero here

514
00:31:48,290 --> 00:31:50,030
we get the capitol are here

515
00:31:50,110 --> 00:31:51,760
and we get the sign

516
00:31:51,870 --> 00:31:54,560
of twenty

517
00:31:54,600 --> 00:31:55,710
are times

518
00:31:55,710 --> 00:31:56,830
in the

519
00:31:56,850 --> 00:31:58,290
that must also be

520
00:31:58,420 --> 00:31:59,710
and the momentum

521
00:31:59,720 --> 00:32:03,600
right here the the fact that the omentum is in the blackboard is no concern

522
00:32:03,630 --> 00:32:06,290
to me i just want to know the magnitude

523
00:32:06,310 --> 00:32:08,080
when the object is here

524
00:32:08,100 --> 00:32:12,690
angular momentum is the position vector which is legs five or

525
00:32:12,720 --> 00:32:14,610
the velocities v of eight

526
00:32:14,630 --> 00:32:18,460
the angle is now ninety degrees of the sign of the angle is one

527
00:32:18,500 --> 00:32:20,240
so i get an

528
00:32:20,250 --> 00:32:22,140
ninety eight

529
00:32:22,170 --> 00:32:23,390
nine five

530
00:32:23,520 --> 00:32:28,240
i lose my deal and and i one equation with two unknowns

531
00:32:28,240 --> 00:32:29,970
i don't know what the zero is

532
00:32:30,010 --> 00:32:32,190
and i don't know what to ways

533
00:32:32,240 --> 00:32:34,200
it is not a place

534
00:32:34,210 --> 00:32:37,260
well these are conservative forces were dealing with gravity

535
00:32:38,240 --> 00:32:41,500
mechanical energy must be conserved

536
00:32:42,330 --> 00:32:46,620
what we used to use the same total energy of the system is conserved

537
00:32:46,670 --> 00:32:48,360
the total energy of the system

538
00:32:48,410 --> 00:32:51,950
is kinetic energy plus potential

539
00:32:51,960 --> 00:32:54,570
must be conserved must be the same here

540
00:32:54,630 --> 00:32:56,980
as it is there

541
00:32:57,020 --> 00:32:58,990
one is the total energy here

542
00:32:59,030 --> 00:33:00,360
kinetic energy

543
00:33:00,380 --> 00:33:02,010
is one half

544
00:33:02,020 --> 00:33:03,810
the zero square

545
00:33:03,910 --> 00:33:05,830
what is the potential energy

546
00:33:05,880 --> 00:33:07,380
this miners is and

547
00:33:07,510 --> 00:33:09,440
and g

548
00:33:09,440 --> 00:33:10,690
divided by

549
00:33:10,700 --> 00:33:12,490
its distance to centre

550
00:33:12,540 --> 00:33:14,720
which is capital are

551
00:33:14,790 --> 00:33:16,550
that is the total energy

552
00:33:16,630 --> 00:33:17,790
right here

553
00:33:17,830 --> 00:33:19,230
the total energy here

554
00:33:19,230 --> 00:33:21,930
could not have changed must therefore be

555
00:33:21,940 --> 00:33:23,760
one half

556
00:33:23,800 --> 00:33:25,800
if you have a scribd

557
00:33:27,130 --> 00:33:29,030
and g

558
00:33:29,040 --> 00:33:30,330
but now to these things

559
00:33:30,340 --> 00:33:33,540
call five or

560
00:33:33,600 --> 00:33:36,270
apart from the fact that i lose my little

561
00:33:36,310 --> 00:33:38,360
notice i have the second equation

562
00:33:38,410 --> 00:33:39,640
i didn't have any

563
00:33:39,670 --> 00:33:40,750
well known

564
00:33:40,760 --> 00:33:44,300
so i have two equations with two unknowns is zero

565
00:33:44,350 --> 00:33:45,580
in view of a

566
00:33:45,630 --> 00:33:48,570
and you can solve for that that you can find both

567
00:33:48,580 --> 00:33:50,260
interestingly enough

568
00:33:50,260 --> 00:33:52,790
you can also find the same major axis

569
00:33:52,870 --> 00:33:54,940
because the total energy of the system

570
00:33:54,940 --> 00:33:58,330
it's also minus and and

571
00:33:58,350 --> 00:34:00,290
divided by

572
00:34:00,310 --> 00:34:01,030
two a

573
00:34:01,080 --> 00:34:03,310
this equation is also

574
00:34:03,360 --> 00:34:05,160
on your exam

575
00:34:05,210 --> 00:34:08,290
and so if you know v zero or you know v eight

576
00:34:08,320 --> 00:34:15,450
then you can be and cancelled you can also calculate the so my major axis

577
00:34:16,600 --> 00:34:19,280
doppler shift

578
00:34:19,330 --> 00:34:21,970
let's do something about the operation

579
00:34:22,030 --> 00:34:23,810
first doppler shift

580
00:34:23,830 --> 00:34:26,480
of electromagnetic radiation lights

581
00:34:26,520 --> 00:34:28,050
radio waves

582
00:34:30,660 --> 00:34:33,770
a star moving relative to us

583
00:34:33,780 --> 00:34:36,840
was the velocity v

584
00:34:36,930 --> 00:34:38,470
since we're dealing with

585
00:34:38,480 --> 00:34:43,140
electromagnetic radiation we don't have to ask whether the star is moving relative to us

586
00:34:43,210 --> 00:34:47,280
we are moving relative to the stars the question is a meaningless question in special

587
00:34:49,560 --> 00:34:51,930
you're here

588
00:34:51,930 --> 00:34:54,800
you are receiving the frequency of prime

589
00:34:54,850 --> 00:34:56,520
he was that star

590
00:34:56,520 --> 00:35:00,670
and this the task is usually referred to as the sequence labeling

591
00:35:00,680 --> 00:35:05,580
so we are given an input sequence a sequence x we want to reconstruct the

592
00:35:05,650 --> 00:35:12,230
associated label sequence y where about the x and y have the equal length and

593
00:35:12,680 --> 00:35:18,660
this of course sequence labelling can be used not only for named entity recognition before

594
00:35:18,690 --> 00:35:20,710
many other tasks such as that

595
00:35:20,760 --> 00:35:24,760
gene finding in computational biology for example

596
00:35:24,770 --> 00:35:31,900
another task that involves structure that and we all analyse it is biological sequence alignment

597
00:35:32,060 --> 00:35:37,600
and biological sequence alignment is especially useful when no one wanted to remind the similarity

598
00:35:37,690 --> 00:35:44,710
between biological sequences in particular in this talk we concentrate my attention on global alignment

599
00:35:44,720 --> 00:35:48,110
so we are given two sequences s one and s two

600
00:35:48,120 --> 00:35:53,600
and the global alignment is an assignment of gaps such as to line up each

601
00:35:53,600 --> 00:35:59,860
letter in one sequence either with a gap or a letter in the other sequence

602
00:35:59,870 --> 00:36:01,100
this problem

603
00:36:01,120 --> 00:36:06,590
can be modeled mathematically in this way we are given a sequence the player x

604
00:36:06,950 --> 00:36:09,620
we want to predict the correct sequence y

605
00:36:09,670 --> 00:36:14,770
of alignment operations in the simplest model we should just count the number of matches

606
00:36:14,770 --> 00:36:19,200
mismatches and in its alignment and

607
00:36:19,250 --> 00:36:25,700
alignments can be represented as you all probably know the park in the alignment graph

608
00:36:25,780 --> 00:36:29,720
but that flows from the upper left to the lower right corner

609
00:36:29,740 --> 00:36:32,310
in the graph

610
00:36:32,360 --> 00:36:38,840
the problem that we analyse that involves structure that is the RNA secondary structure prediction

611
00:36:38,860 --> 00:36:45,170
if the problem that we have an RNA sequence in the primary sector want to

612
00:36:45,170 --> 00:36:48,230
predict the most likely secondary structure

613
00:36:48,250 --> 00:36:51,020
and the secondary structure

614
00:36:51,690 --> 00:36:58,340
his former since the sequence for the weights alpha two four f of the hydrogen

615
00:36:58,340 --> 00:37:05,580
bomb and it's the study of her a secondary structure is particularly important in understanding

616
00:37:05,580 --> 00:37:07,740
the function of around me

617
00:37:07,750 --> 00:37:12,090
how do we model this problem when one of the problem of the sequence starting

618
00:37:12,090 --> 00:37:17,930
problems so we are given an input sequence x i want to remind set part

619
00:37:17,930 --> 00:37:23,080
three y if we have to find an appropriate context free grammar

620
00:37:23,100 --> 00:37:27,080
as you probably already know i want to bring summaries

621
00:37:27,220 --> 00:37:32,860
four two of object so we have a set of nonterminal symbols a set of

622
00:37:32,860 --> 00:37:35,690
terminal symbols the set of rules that

623
00:37:35,710 --> 00:37:42,090
connect the terminal and nonterminal symbols and starting symbol that should belong to the set

624
00:37:42,090 --> 00:37:43,430
of non terminals

625
00:37:43,450 --> 00:37:49,480
and for example on the right side of the screen we have an RNA sequence

626
00:37:49,590 --> 00:37:54,770
we can afford it we can model the folding in that way

627
00:37:54,780 --> 00:38:00,420
using the glamour that we have defined on the left of the screen

628
00:38:00,440 --> 00:38:05,070
and for all the three problems that i have shown to you

629
00:38:05,090 --> 00:38:10,520
no there is a generative a generative approach is possible

630
00:38:10,540 --> 00:38:17,810
for example i will consider the most simple case in sequence labeling traditionally hidden markov

631
00:38:17,810 --> 00:38:23,690
models and has been used for sequence labeling so but there are too many two

632
00:38:23,690 --> 00:38:30,140
main drawback in using a HMM for this task the first drawback is that hmm

633
00:38:30,140 --> 00:38:38,940
relies on independence assumption and so yes we assume that each observation is dependent only

634
00:38:38,940 --> 00:38:45,350
on the current state and its state dependent only on the previous state and in

635
00:38:45,350 --> 00:38:46,980
this way we can not model

636
00:38:47,020 --> 00:38:54,050
long term the interaction between objects between was for example in a man the recognition

637
00:38:54,070 --> 00:39:02,030
second drawback that we try to avoid is still is that it hmm if they

638
00:39:02,030 --> 00:39:05,890
are typically trained by maximum likelihood estimation

639
00:39:07,420 --> 00:39:14,160
this is suboptimal way of training has since the maximum likelihood criterium is not directly

640
00:39:14,160 --> 00:39:19,000
related to the prediction accuracy of the model so we should try to devise a

641
00:39:19,000 --> 00:39:23,330
method that is more directly related to the prediction accuracy

642
00:39:23,350 --> 00:39:24,460
how to do that

643
00:39:24,480 --> 00:39:32,520
recently discriminative models have been introduced and discriminative models essentially fifty five the probability of

644
00:39:32,520 --> 00:39:34,190
possible output y

645
00:39:34,210 --> 00:39:41,830
even an observation so they consider the conditional probability rather than joint probability and we

646
00:39:41,850 --> 00:39:46,070
discriminative bottom models we do not have a problem of

647
00:39:46,150 --> 00:39:54,440
independence assumptions like into narrative more and more of we can consider arbitrary features of

648
00:39:54,440 --> 00:40:00,980
the observations and this can help us to model more complex problems in a more

649
00:40:00,980 --> 00:40:02,340
effective way

650
00:40:02,360 --> 00:40:05,620
it is a sample of conditional random fields in

651
00:40:05,640 --> 00:40:12,710
an example is the chain conditional random fields that can show this morning

652
00:40:12,720 --> 00:40:20,140
and so i already said that a lot of discriminative algorithms have emerged in the

653
00:40:20,180 --> 00:40:21,890
last few years two

654
00:40:21,910 --> 00:40:23,590
to solve this

655
00:40:23,600 --> 00:40:30,000
the problem of complex structure prediction and you i would like to show how our

656
00:40:30,000 --> 00:40:36,130
approaches for structure prediction and i will concentrate on the problem that i showed you

657
00:40:36,130 --> 00:40:37,200
before so

658
00:40:37,220 --> 00:40:41,670
the question that i have to answer are given a training set of correct pairs

659
00:40:41,670 --> 00:40:47,150
of sentences and their associated entity types can we learn to extract entities from a

660
00:40:47,150 --> 00:40:49,730
new word for new sentences

661
00:40:49,750 --> 00:40:55,990
or given a training set of correct biological alignments can will to align two unknown

662
00:40:56,000 --> 00:41:03,660
sequences and finally given a training set of corrects RNA secondary structures can we learn

663
00:41:03,660 --> 00:41:08,570
to deter mine the secondary structure so for a new RNA sequence

664
00:41:08,590 --> 00:41:14,900
of course this is not an exhaustive list of possible applications for example he managed

665
00:41:14,910 --> 00:41:16,970
to make segmentation could be

666
00:41:16,980 --> 00:41:24,900
another application or like gene gene finding in bioinformatics that there's a lot of applications

667
00:41:24,900 --> 00:41:27,840
for this framework

668
00:41:27,840 --> 00:41:32,340
well it's same thing you can use for selecting kernels

669
00:41:32,840 --> 00:41:37,630
so this is a little bit about the current situation of SVM kernels

670
00:41:38,150 --> 00:41:44,060
well so we we learn about the radial basis function kernel recalls her when only

671
00:41:44,980 --> 00:41:47,820
there are quite a few a few other kernels

672
00:41:47,900 --> 00:41:54,730
and you can even do combinations actually which is a bit later so so we

673
00:41:55,030 --> 00:42:00,900
essentially physical situations so only only one we don't want to have to current

674
00:42:01,690 --> 00:42:05,880
four uses they have trouble on selecting suitable kernel

675
00:42:06,010 --> 00:42:13,330
but for researchers always singular which is designed kernels particulars the suitable for our target

676
00:42:14,620 --> 00:42:16,920
you want knowledge

677
00:42:16,930 --> 00:42:23,410
you should i those which should better cooperate domain knowledge as your current product

678
00:42:23,420 --> 00:42:26,060
so it like we have two

679
00:42:26,190 --> 00:42:28,110
contradictory goals

680
00:42:28,440 --> 00:42:36,440
so that's why i think that those who thinks of contradicting his salary but practically

681
00:42:36,920 --> 00:42:42,120
i think they the this is OK for to do is we actually very few

682
00:42:42,120 --> 00:42:45,170
general kernels by general kernels

683
00:42:45,230 --> 00:42:46,300
i mean

684
00:42:46,310 --> 00:42:49,580
those are very very common basically what's

685
00:42:49,770 --> 00:42:55,650
if you look at the four point of view is what related

686
00:42:55,670 --> 00:43:00,860
so what do we do with the extension of the current so we see those

687
00:43:01,080 --> 00:43:06,970
polynomial ten x i x j right so this a it's actually the middle son

688
00:43:06,980 --> 00:43:11,230
of of going on so this somewhat related

689
00:43:12,660 --> 00:43:17,480
so most to change the so-called generative model of similar types

690
00:43:17,490 --> 00:43:23,120
so in this is we don't know many such kind of therefore for making this

691
00:43:23,140 --> 00:43:27,580
they don't have many choices so that's good they don't have to worry so much

692
00:43:27,590 --> 00:43:31,100
this is try some of them but

693
00:43:31,110 --> 00:43:36,590
the researchers are also actively designing stationary kernels

694
00:43:36,960 --> 00:43:43,970
please use this string string they so you're you're the instances and euclidean vectors any

695
00:43:43,970 --> 00:43:51,440
more than you want to have some spatial kernels string kernels for such as it

696
00:43:51,620 --> 00:43:54,670
it supports

697
00:43:54,690 --> 00:43:56,550
i think without so

698
00:43:56,940 --> 00:44:02,440
for beginners will just try to sequence but once you you know more than you

699
00:44:02,440 --> 00:44:07,270
tried to to design your own kernels

700
00:44:07,560 --> 00:44:11,290
so this is what saying that if you are

701
00:44:11,290 --> 00:44:16,810
if you think we don't know yet you you have so that's why they are

702
00:44:16,940 --> 00:44:19,280
so some of you are being

703
00:44:19,650 --> 00:44:25,500
then you can use any of this function kernel for this reason is you look

704
00:44:25,760 --> 00:44:32,760
at the end of the day you don't have any of these are used for

705
00:44:32,810 --> 00:44:38,920
well this is a linear kernel then what i think is it is this is

706
00:44:38,920 --> 00:44:44,830
a special case of you can so we have a theoretical results proving that the

707
00:44:44,830 --> 00:44:48,750
performance of using exactly the same

708
00:44:48,820 --> 00:44:51,970
you could under certain parameters

709
00:44:51,980 --> 00:44:57,690
but use the internet for this year only for the last penalty parameter c but

710
00:44:57,690 --> 00:45:01,960
these are the kind of these two parameters you don't need to tell you what

711
00:45:01,970 --> 00:45:09,730
it was kernel parameters got so of course shows for the new kernel others and

712
00:45:09,730 --> 00:45:14,110
see is a example of different kernels

713
00:45:14,120 --> 00:45:16,310
we started seeing got

714
00:45:18,150 --> 00:45:23,500
he says he tried parameters to set the parameters for kernel

715
00:45:23,520 --> 00:45:29,090
well then you are guaranteed to perform at least as good as using these kernels

716
00:45:29,090 --> 00:45:32,610
so that's assuming control you

717
00:45:32,610 --> 00:45:34,810
because was

718
00:45:34,820 --> 00:45:42,110
the issues here that this situation that i think still on the training in

719
00:45:42,870 --> 00:45:49,210
and polynomial kernels with the reason of using it is because numerical difficulties

720
00:45:50,050 --> 00:45:53,920
we doing this work curating available to

721
00:45:54,890 --> 00:45:57,220
the degree of d which

722
00:45:57,250 --> 00:46:02,370
did the inside of processes is actually smaller than one in

723
00:46:02,370 --> 00:46:06,320
degrees small that this very is very very close to zero

724
00:46:06,820 --> 00:46:12,360
the unconscious if it's quite one and the degrees all the

725
00:46:12,360 --> 00:46:15,270
it becomes a very large so

726
00:46:15,350 --> 00:46:20,200
in something you do it is easy interface numerical difficulties

727
00:46:20,200 --> 00:46:24,320
one of the issues here is that the

728
00:46:24,340 --> 00:46:31,370
many of these these models grow quite large and become unmanageable in terms of the

729
00:46:31,370 --> 00:46:36,350
the overall size of the model for some of the common techniques applied for learning

730
00:46:36,350 --> 00:46:41,030
joint models a so-called search and score methods and so it's more likely that what

731
00:46:41,030 --> 00:46:46,850
are called constraint based methods these methods to look for local little evidence that is

732
00:46:46,850 --> 00:46:52,720
valid and for the global model is a is going to be the better way

733
00:46:52,720 --> 00:46:54,490
to go because

734
00:46:54,510 --> 00:47:02,090
building and learning the structure and parameters of really large models large relational spatial and

735
00:47:02,090 --> 00:47:06,370
temporal models seems to me unlikely to be tractable

736
00:47:06,640 --> 00:47:10,010
finally i think social scientists again are going to have something to say about what

737
00:47:10,010 --> 00:47:13,970
sort of algorithms we would want to use here there are unlikely to be willing

738
00:47:13,970 --> 00:47:18,220
to turn over complete control to some algorithm and we talked for many years in

739
00:47:18,220 --> 00:47:20,450
this field about

740
00:47:20,470 --> 00:47:25,390
having interactive systems but i think social scientists with the for working with their going

741
00:47:25,390 --> 00:47:29,720
to look at and so we really want this to be seriously interactive and also

742
00:47:29,720 --> 00:47:32,970
not going to be able to encode all their domain knowledge in machine readable form

743
00:47:32,970 --> 00:47:36,550
and so it's going to have to be more like a design environment then it

744
00:47:36,550 --> 00:47:42,600
is a system for just dumping in your data and getting it last topic briefly

745
00:47:42,600 --> 00:47:46,870
i want to talk about his privacy it's a kind of overriding issued all of

746
00:47:46,870 --> 00:47:52,050
these things this is where the the snow story in our story diverge

747
00:47:52,050 --> 00:47:57,140
snow was able to access large amounts of data about individuals because of the different

748
00:47:57,140 --> 00:48:04,070
historical situation that we have today today's computational social scientists have a very different environment

749
00:48:04,070 --> 00:48:04,410
was of

750
00:48:04,760 --> 00:48:11,340
a paper published last year brief paper about computational social science in science magazine and

751
00:48:11,410 --> 00:48:13,370
one of the quotes in that

752
00:48:13,390 --> 00:48:18,410
i was about to perhaps what they call the thorniest challenges to computational social science

753
00:48:18,870 --> 00:48:24,390
is about access to data and privacy of the individuals in the data

754
00:48:24,410 --> 00:48:27,950
computational social sciences usually about people

755
00:48:27,970 --> 00:48:35,600
and about their level actions and interactions and that is difficult to get access to

756
00:48:35,600 --> 00:48:43,340
and unfortunately is now being increasingly silent individual organisations that can analyse their own data

757
00:48:43,340 --> 00:48:48,580
but not go outside and the concern is that really good work in computational social

758
00:48:48,580 --> 00:48:53,680
science will end up being all about a single organisations working only on the data

759
00:48:53,680 --> 00:48:55,840
they have access to

760
00:48:56,280 --> 00:49:01,050
now the news that i have on this is not a particularly good unfortunately but

761
00:49:01,050 --> 00:49:04,510
it's some very recent work one of my students michael hay

762
00:49:05,530 --> 00:49:12,120
has looked at networks and said well how does the privacy equation change in networks

763
00:49:12,490 --> 00:49:16,570
if this is the original network we can imagine naively anonymizing by just taking off

764
00:49:16,600 --> 00:49:23,870
identity and if we just remove identity then really it should be particularly identifiable unfortunately

765
00:49:23,870 --> 00:49:27,990
what is shown is that if you have a little bit of information about node

766
00:49:28,030 --> 00:49:34,070
say how many friends they have many friends the friends have you can very frequently

767
00:49:34,070 --> 00:49:39,570
re identify individuals within the network this is true just of notional networks it's true

768
00:49:39,580 --> 00:49:42,950
very large real networks here's the for instance

769
00:49:43,100 --> 00:49:49,010
o point five million nodes if you know how many friends and individual house that's

770
00:49:49,010 --> 00:49:53,350
what we call h one knowledge and it's only possible to return people in a

771
00:49:53,350 --> 00:49:58,260
very small number of cases but if you know how many friends their friends how

772
00:49:58,260 --> 00:50:04,350
you could really identify uniquely about half of the individuals in the network that network

773
00:50:04,370 --> 00:50:09,910
signature provides of a quasi identifier for individuals

774
00:50:10,140 --> 00:50:15,760
about the only positive thing that i can say right now is first of all

775
00:50:15,760 --> 00:50:20,990
michael has worked out of several individual kinds of analyses that you can do meeting

776
00:50:20,990 --> 00:50:27,370
very strong privacy definitions including looking at the degree distribution degree histogram but in terms

777
00:50:27,370 --> 00:50:31,350
of many other kinds of analyses done in this community it's very hard to do

778
00:50:31,350 --> 00:50:34,670
driven by the the observations

779
00:50:34,830 --> 00:50:38,630
when you put that together with say this observation of one blue marble in one

780
00:50:38,630 --> 00:50:41,930
bag what this is saying is well i know there's lots of different colours out

781
00:50:41,930 --> 00:50:43,200
there but

782
00:50:43,210 --> 00:50:48,410
mostly women in estimating that theta i should go with the observations since the observation

783
00:50:48,410 --> 00:50:49,100
is blue

784
00:50:49,160 --> 00:50:52,160
but i'm going to pay most of the way there and and and i'm not

785
00:50:52,160 --> 00:50:54,130
going to get strictly just

786
00:50:54,140 --> 00:50:58,410
spike on blue just delta function there because i have some role the priors like

787
00:50:58,410 --> 00:51:03,170
get smoothing but my best guess is probably the next draws rogan the blue whereas

788
00:51:03,170 --> 00:51:06,330
if i had this situation here just to contrast you would have done is just

789
00:51:07,550 --> 00:51:11,720
all the bands from the same marble which just permute them across bags shuffle them

790
00:51:11,720 --> 00:51:15,650
up so that all the bags are kind of look about the same intuitively or

791
00:51:15,650 --> 00:51:19,470
he like all of the variation in the population of balls is now

792
00:51:19,720 --> 00:51:24,060
on average there in any one individual bags

793
00:51:24,070 --> 00:51:26,400
and then of course our status are going to be

794
00:51:26,420 --> 00:51:31,020
much more blurred out but that's how the way we kept higher level is with

795
00:51:31,020 --> 00:51:34,000
an alpha value now that's much greater than one is supposed to much less than

796
00:51:34,000 --> 00:51:38,600
one that saying that for any for any given bad we expect if you like

797
00:51:38,600 --> 00:51:44,770
the the prior to exert relatively much more weight than any one individual draw

798
00:51:44,800 --> 00:51:47,460
so that when we see just this one little thing here well you know we

799
00:51:47,460 --> 00:51:48,220
we do

800
00:51:48,240 --> 00:51:52,330
so i think it's slightly more likely to be bluer than anything else but most

801
00:51:52,330 --> 00:51:54,010
of our inference is based on

802
00:51:54,820 --> 00:51:58,360
the prototype coming from the prior so we don't in the this is the case

803
00:51:58,360 --> 00:52:01,530
in which we're not going to make a strong inference about what else coming because

804
00:52:01,530 --> 00:52:04,910
i've seen basically the bags could be just about anything

805
00:52:04,920 --> 00:52:10,600
OK so so is essentially what our model says here is we're going to we're

806
00:52:10,600 --> 00:52:13,740
going to get out apply this disclaimer was question

807
00:52:17,440 --> 00:52:19,070
this one

808
00:52:19,080 --> 00:52:20,970
since we are

809
00:52:21,230 --> 00:52:29,200
it is going to have to

810
00:52:33,310 --> 00:52:38,300
yes it is one of the

811
00:52:39,220 --> 00:52:41,430
apart from that of

812
00:52:41,450 --> 00:52:43,920
a lot of

813
00:52:47,820 --> 00:52:50,800
our article

814
00:52:50,990 --> 00:52:53,910
yes it so that so

815
00:52:56,250 --> 00:53:00,370
well probably the many possible and to that i would say that one of the

816
00:53:00,370 --> 00:53:04,030
things we want to do here if you want to be able to learn an

817
00:53:04,030 --> 00:53:09,010
inductive bias that's going to generalize to new categories and that we're going to do

818
00:53:09,010 --> 00:53:12,640
this take a very simple representation in which will have the dimension of shape and

819
00:53:12,640 --> 00:53:16,930
the dimension of color dimension of material essentially and it's high at

820
00:53:16,940 --> 00:53:19,410
a many valued discrete

821
00:53:19,410 --> 00:53:23,700
dimension like you know a multinomial with many bands which is kind of like constantly

822
00:53:23,700 --> 00:53:29,300
thinking about basic level categories you know imagine that are our perceptual system gives us

823
00:53:29,300 --> 00:53:33,600
two thousand ten thousand basic level shape categories as well as basic level color and

824
00:53:33,600 --> 00:53:35,580
other sorts of things

825
00:53:35,590 --> 00:53:38,590
then we want to be able to learn something which is going to generalize across

826
00:53:38,590 --> 00:53:41,460
different values of this multinomial

827
00:53:41,480 --> 00:53:44,480
and the pretty much the only way we can do that is if we have

828
00:53:44,480 --> 00:53:51,160
some parameter which captures the abstract knowledge independent of any particular value that could be

829
00:53:52,120 --> 00:53:55,990
it's just as all show you want i get to this concrete example we want

830
00:53:55,990 --> 00:53:58,800
something which is going to generalize to

831
00:53:58,850 --> 00:54:00,560
new values of

832
00:54:00,610 --> 00:54:05,390
or you know values of this variable haven't even been observed

833
00:54:05,470 --> 00:54:06,590
is that

834
00:54:06,600 --> 00:54:07,830
help make sense

835
00:54:07,880 --> 00:54:09,660
but OK

836
00:54:09,680 --> 00:54:14,740
so so here's concretely how we apply this model to this case we say we're

837
00:54:14,740 --> 00:54:21,350
gonna represent the objects with various dimensions which are going to be these each dimension

838
00:54:21,350 --> 00:54:24,250
if you like is a high dimensional discrete variable so that i think in this

839
00:54:24,250 --> 00:54:28,920
case there were no how high this is something like twenty or fifty shape categories

840
00:54:28,920 --> 00:54:33,150
and various texture and colour categories and size categories

841
00:54:35,090 --> 00:54:36,600
we we observed

842
00:54:37,460 --> 00:54:41,780
just just as an experiment two objects in each of these four types and there

843
00:54:41,780 --> 00:54:46,940
are those that to have the same shape value in each category but different values

844
00:54:46,940 --> 00:54:51,200
of texture and color and so on and indeed also different values of shape across

845
00:54:51,200 --> 00:54:54,210
categories and different values of texture and color

846
00:54:54,230 --> 00:54:58,870
and then we're going to be given during testing some new object here

847
00:54:58,880 --> 00:55:04,640
let's which let's say we're told is in category five also has a new shape

848
00:55:04,640 --> 00:55:07,850
and also has a new texture also the nucleus it's different than the things we've

849
00:55:07,850 --> 00:55:08,920
seen on every

850
00:55:09,800 --> 00:55:12,830
but we want to be able to learn something which is going to generalize in

851
00:55:12,830 --> 00:55:14,210
this case basically

852
00:55:14,240 --> 00:55:17,450
so that's the way we do this is we're going to learn one of these

853
00:55:17,450 --> 00:55:23,210
dirichlet multinomial models for each of these dimensions sort separately so we're basically we're trying

854
00:55:23,210 --> 00:55:24,520
to learn for each dimension

855
00:55:24,530 --> 00:55:27,480
how does the variation across the population

856
00:55:27,620 --> 00:55:31,100
concentrate in bags versus across banks

857
00:55:31,140 --> 00:55:36,520
and that allows us essentially to learn that the shape dimension looks like this so

858
00:55:36,520 --> 00:55:41,810
it's it's very homogeneous within bags whereas the other dimensions look like this

859
00:55:41,820 --> 00:55:43,650
and that's what i see in new

860
00:55:43,680 --> 00:55:47,510
object it's only along the dimension shape and i'm going to be able to make

861
00:55:47,510 --> 00:55:49,310
a strong generalizations about the

862
00:55:49,310 --> 00:55:53,450
of hydrogen which is the fundamental spectrum of all time and then after that G

863
00:55:53,450 --> 00:55:58,950
H cause they're out of ideas so that that was it so

864
00:55:59,100 --> 00:56:04,010
now let's take a look at what l equals 0 l equals 0 this means

865
00:56:04,010 --> 00:56:06,390
you have a satirical or circular

866
00:56:06,540 --> 00:56:12,220
a spherical orbital a circular orbit and that's easy to remember because all is think

867
00:56:12,220 --> 00:56:13,970
of it as perfect spheres

868
00:56:14,010 --> 00:56:17,970
right when l equals 1 this is elliptical

869
00:56:17,990 --> 00:56:24,240
you have an elliptical orbit you can think of that 0 is 0 and 1

870
00:56:24,240 --> 00:56:30,140
can think of that as there's some elongated OK and when l equals to 1

871
00:56:30,140 --> 00:56:34,100
l equals two I'll show you these later in the in about a week's time

872
00:56:34,180 --> 00:56:38,450
they're just much more complex you get dumbbell shape doughnut shaped and all sorts of

873
00:56:38,450 --> 00:56:41,870
things just say the more complex

874
00:56:42,710 --> 00:56:48,650
so those are the numbers associated with certain shapes of the

875
00:56:48,660 --> 00:56:54,330
orbital then the next quantum number he gave us was m m which is the

876
00:56:54,330 --> 00:57:00,680
magnetic quantum number it's the magnetic quantum number and it speaks to the question of

877
00:57:00,680 --> 00:57:04,510
orientation orientation

878
00:57:04,930 --> 00:57:09,710
and the admissible values of M . L

879
00:57:09,870 --> 00:57:13,890
l minus 1 3 0 0 down to

880
00:57:14,180 --> 00:57:16,220
line itself so

881
00:57:16,510 --> 00:57:21,660
spanning from plus delta minus across l is subject to the values of and so

882
00:57:22,160 --> 00:57:27,040
in the case of n equals 1 when n equals 1

883
00:57:27,060 --> 00:57:34,060
Elkan people only 0 so was 0 than this value and must always be zero

884
00:57:35,070 --> 00:57:41,690
and indeed this confirms the observations there was no line splitting magnetic field for the

885
00:57:41,690 --> 00:57:44,310
ground state wasn't

886
00:57:44,430 --> 00:57:49,950
what happens when n equals 2 when equals to well l can take values of

887
00:57:49,950 --> 00:57:52,260
1 and 0

888
00:57:53,720 --> 00:57:57,990
1 and 0 so what do we have we have now an equals to l

889
00:57:57,990 --> 00:58:05,350
equals 1 so can now go 1 0 and minus 1 1 0 and minus

890
00:58:06,370 --> 00:58:10,810
so this is the 1 case where there is some value and thinking about the

891
00:58:10,810 --> 00:58:15,780
Cartesian equivalent of this is very dangerous when you're talking about the a quantum ideas

892
00:58:15,780 --> 00:58:21,220
to try to scribe physical attributes to them because in many cases just doesn't doesn't

893
00:58:21,220 --> 00:58:25,350
work but in this case I think it does so let's look at 3 values

894
00:58:25,350 --> 00:58:29,330
of the magnetic quantum number when we have an elliptical orbit

895
00:58:29,510 --> 00:58:30,930
right so

896
00:58:30,990 --> 00:58:36,160
1 way of thinking about it is here the 3 principal coordinate directions x y

897
00:58:36,160 --> 00:58:37,120
and z

898
00:58:37,220 --> 00:58:41,120
no I don't know where real X lies but once I choose x I can

899
00:58:41,120 --> 00:58:44,450
use the right hand rule and I know where y and z have to be

900
00:58:44,450 --> 00:58:45,830
there orthogonal

901
00:58:45,830 --> 00:58:48,100
so this is X is why this is the

902
00:58:48,890 --> 00:58:57,330
I have a non spherical nonaxisymmetric orbital so for simplicity just make it

903
00:58:59,510 --> 00:59:04,770
well what the say it says can go 1 0 minus 1 which to my

904
00:59:04,770 --> 00:59:10,370
mind represents the fact that there are 3 principal coordinate directions in which you can

905
00:59:10,370 --> 00:59:17,390
orient so I think this is a nice example of mathematics imitating reality OK after

906
00:59:17,390 --> 00:59:22,730
that it gets tickets and then the last quantum number this is where this is

907
00:59:22,730 --> 00:59:24,550
where I

908
00:59:24,560 --> 00:59:32,890
Sommerfeld left it but I'll finish by introducing the last quantum number last quantum number

909
00:59:32,890 --> 00:59:40,570
came a little bit later it's lower case s which is the spin quantum number

910
00:59:40,570 --> 00:59:45,870
in it represents the fact that the electron is considered have spent and it takes

911
00:59:45,870 --> 00:59:48,900
values plus or minus 1 half

912
00:59:49,330 --> 00:59:55,370
and here's the here's the basis for this value here I want to put this

913
00:59:55,370 --> 00:59:57,870
all together so that I'm doing a little bit of

914
00:59:57,870 --> 01:00:03,660
back to the future because this is 960 that's for Sommerfeld leaves this comes about

915
01:00:03,660 --> 01:00:07,970
1925 but I want you to have all quantum numbers and then we can move

916
01:00:07,970 --> 01:00:11,100
forward and what introduced this was

917
01:00:11,350 --> 01:00:15,690
an experiment conducted by Stern Gerlach in front

918
01:00:17,700 --> 01:00:20,510
and that was done around 19 21

919
01:00:20,510 --> 01:00:25,330
you can make this up will have a look at all this is cute this

920
01:00:25,330 --> 01:00:32,160
is what I think is unilaterally image here's this is this is Sommerfeld on the

921
01:00:32,160 --> 01:00:37,890
occasion of his 80th birthday and as is the custom for about a Berthier so

922
01:00:38,370 --> 01:00:43,200
and there's people from all over the world get together for a party and the

923
01:00:43,200 --> 01:00:47,120
accompany this with the 2 or 3 affair in which people give scientific papers in

924
01:00:47,120 --> 01:00:52,850
honor of the celebrate so this is taken from the book that was published on

925
01:00:52,850 --> 01:00:57,680
the occasion of Sommerfeld 80th birthday and since Sommerfeld was the 1 that propose that

926
01:00:57,680 --> 01:01:03,660
orbits could be spherical or elliptical they as a as a gag said that well

927
01:01:03,680 --> 01:01:09,060
here's Sommerfeld here Sommerfeld somewhat the standard so this is l equals 0 and this

928
01:01:09,060 --> 01:01:15,330
is l equals 1 and this was accompanied by the inscription said Sommerfeld was the

929
01:01:15,330 --> 01:01:19,010
1 who taught us that the circle is the degenerate form of the ellipse

930
01:01:19,560 --> 01:01:25,080
and that was really funny that's that's 19 twenties geek humor that was off roaring

931
01:01:25,120 --> 01:01:30,080
cities but clearly it doesn't age well you're not you're not you're not sensing that's

932
01:01:30,200 --> 01:01:35,680
OK here's the Stern Gerlach experiment very very cool experiment they were doing studies in

933
01:01:35,680 --> 01:01:40,890
the early twenties to try to validate Maxwell's equations so they got over here is

934
01:01:40,890 --> 01:01:45,740
a furnace with a crucible of liquid metal and the metal has obviously vapor pressure

935
01:01:45,740 --> 01:01:48,430
above and there's a sled here and I'm gonna

936
01:01:48,430 --> 01:01:53,420
we have to answer the question what howard hughes del threats directly we let's imagine

937
01:01:53,420 --> 01:01:56,310
that we've we've sold out in a sense more question to is

938
01:01:57,030 --> 01:01:57,880
how do you choose

939
01:01:59,460 --> 01:02:00,930
between different measures the goodness-of-fit

940
01:02:01,810 --> 01:02:03,530
i'm not saying you know these is a good

941
01:02:04,120 --> 01:02:05,680
measure just the first one sort of

942
01:02:06,380 --> 01:02:08,480
has anyone got another approach to

943
01:02:09,010 --> 01:02:09,380
to this

944
01:02:11,180 --> 01:02:16,820
methods to be more to optimize gradient descent which suggested that is not too important

945
01:02:16,820 --> 01:02:18,360
how we optimize as long as we find

946
01:02:18,800 --> 01:02:22,240
the optimal when we've done optimization it will spit out i guess

947
01:02:22,720 --> 01:02:25,430
the animals that at the best fit value of lambda

948
01:02:27,230 --> 01:02:27,900
there's anyone have

949
01:02:28,620 --> 01:02:33,260
another approach because there are many approaches you get taught when you're being taught

950
01:02:33,740 --> 01:02:35,050
ad hoc statistical methods

951
01:02:36,090 --> 01:02:39,540
and i'm sure some of you sort of methods that don't look like this or this

952
01:02:44,570 --> 01:02:45,640
product again since you say

953
01:02:46,510 --> 01:02:48,670
so what's the problem gaussians approach to

954
01:02:49,080 --> 01:02:50,510
answering the question what is lambda

955
01:02:57,290 --> 01:02:57,960
okay so

956
01:02:59,600 --> 01:03:02,400
what since you have in mind for this this problem

957
01:03:04,620 --> 01:03:07,350
okay data points that's x-coordinates

958
01:03:08,400 --> 01:03:12,310
okay the x coordinates all these guys are exponentially distributed maybe i need to make

959
01:03:12,370 --> 01:03:15,300
it clear what the assumption is a accent

960
01:03:15,920 --> 01:03:16,580
given lambda

961
01:03:18,230 --> 01:03:20,090
the window edges eh

962
01:03:22,670 --> 01:03:24,130
when i say exponential it's

963
01:03:24,510 --> 01:03:25,110
one of these

964
01:03:27,130 --> 01:03:28,580
this thing it's minus

965
01:03:35,440 --> 01:03:36,450
x is

966
01:03:38,700 --> 01:03:40,060
between three

967
01:03:41,190 --> 01:03:41,540
and this

968
01:03:46,780 --> 01:03:47,400
in the wrong order

969
01:03:50,280 --> 01:03:53,280
where z is the normalisation constant z is integral

970
01:03:53,690 --> 01:03:55,110
from i to be

971
01:03:55,830 --> 01:03:56,370
it minus

972
01:03:58,150 --> 01:04:01,920
so the actual probability density at each x if we knew lambda is

973
01:04:02,330 --> 01:04:03,060
an exponential

974
01:04:05,180 --> 01:04:08,850
there are possibly some gassings running around in this for example

975
01:04:09,360 --> 01:04:11,170
if i define a bin

976
01:04:11,840 --> 01:04:14,830
and if we measure the count of how many points arrived in not being

977
01:04:15,710 --> 01:04:18,680
it may be true that the probability distribution in fact count

978
01:04:19,410 --> 01:04:25,320
which is actually a distribution over integers it may be reasonably well approximated as a gas so that's a possible

979
01:04:25,850 --> 01:04:29,140
direction you could go in the does that definitely does involve

980
01:04:29,630 --> 01:04:30,200
gas fumes

981
01:04:31,090 --> 01:04:33,500
but there's anyone else have a completely different

982
01:04:33,910 --> 01:04:34,780
approach to this

983
01:04:37,000 --> 01:04:38,890
what you know did anyone studied physics

984
01:04:41,720 --> 01:04:42,380
one of the

985
01:04:42,460 --> 01:04:44,180
yeah the physicist over there

986
01:04:44,770 --> 01:04:46,130
isn't it the case that when

987
01:04:47,140 --> 01:04:49,570
little young physicists setosa to do stuff with the data

988
01:04:50,270 --> 01:04:52,330
they're told you get the data

989
01:04:53,190 --> 01:04:57,650
rearrange the equation that predicts the data given the parameters in such a way that

990
01:04:57,660 --> 01:04:58,510
you get a straight line graph

991
01:04:59,000 --> 01:05:02,240
then for the straight line is not one of the rules of how to do

992
01:05:02,980 --> 01:05:03,800
statistical inference

993
01:05:06,900 --> 01:05:08,540
let me run the idea

994
01:05:12,550 --> 01:05:13,140
this will be

995
01:05:14,640 --> 01:05:16,040
given lambda i am be

996
01:05:20,600 --> 01:05:22,670
this is minus one lambda

997
01:05:23,780 --> 01:05:24,490
divided by z

998
01:05:25,750 --> 01:05:27,660
z depends on lambda be

999
01:05:30,160 --> 01:05:32,220
if we points x

1000
01:05:33,290 --> 01:05:37,330
how can we turn this law into some sort of straight line well

1001
01:05:39,110 --> 01:05:40,600
first multiplied by a little window

1002
01:05:41,020 --> 01:05:44,820
as we have people who have been side effects and given lambda be

1003
01:05:45,440 --> 01:05:46,720
times delta x

1004
01:05:47,830 --> 01:05:48,630
that's now

1005
01:05:48,630 --> 01:05:51,420
to the whole map of the sky

1006
01:05:51,800 --> 01:05:54,460
this mountain here if you look closely is

1007
01:05:55,110 --> 01:06:00,220
blue words which are kind of context what would seem

1008
01:06:00,230 --> 01:06:05,720
you would see that these are basically and threats from the texts so this is

1009
01:06:05,720 --> 01:06:07,830
the first works pretty well

1010
01:06:15,690 --> 01:06:19,850
and it's very what's even more important is very efficient to calculate

1011
01:06:19,870 --> 01:06:23,190
because music and we just need to

1012
01:06:23,210 --> 01:06:26,730
if you have to sort of vectors then we just need to look at the

1013
01:06:26,780 --> 01:06:29,730
intersection of the words which is

1014
01:06:29,740 --> 01:06:32,320
which can be calculated pretty fast by

1015
01:06:32,340 --> 01:06:34,790
using this low-level machine

1016
01:06:34,800 --> 01:06:36,480
o operations

1017
01:06:36,490 --> 01:06:41,430
and the end we get a similarity between zero and one so

1018
01:06:41,440 --> 01:06:46,420
zero meaning orthogonal documents in one more of the same

1019
01:06:48,640 --> 01:06:55,050
so what what the typical scenarios for visualisation

1020
01:06:56,440 --> 01:07:00,300
by having this so-called bag of words representation so this is the one which i

1021
01:07:00,300 --> 01:07:01,540
just explained

1022
01:07:01,560 --> 01:07:07,360
usually performed some kind of clustering algorithm so we try to cluster

1023
01:07:07,800 --> 01:07:12,760
group points which are kind of more similar together and then we reported by the

1024
01:07:12,760 --> 01:07:15,820
clusters so if he we would both rules

1025
01:07:15,840 --> 01:07:19,320
most of these position approaches we would

1026
01:07:19,330 --> 01:07:23,360
i see that most of them basically do clustering in some way

1027
01:07:23,380 --> 01:07:28,860
and then we basically metadata clusters into two d and three d space before we

1028
01:07:29,460 --> 01:07:31,460
one three example but we

1029
01:07:31,470 --> 01:07:35,180
we have also many other two d example

1030
01:07:37,970 --> 01:07:42,280
we to because the narrow which gives you a little bit later is also

1031
01:07:42,300 --> 01:07:46,940
visualizing text based on this score occurrences of words

1032
01:07:46,950 --> 01:07:48,730
and phrases

1033
01:07:48,740 --> 01:07:52,250
from the text so this is also the way how we can see certain certain

1034
01:07:53,600 --> 01:07:59,770
and so some of these people because in areas are is asian of document collections

1035
01:08:00,410 --> 01:08:04,950
before we had such an example of visualisation of search results from c

1036
01:08:04,960 --> 01:08:10,030
one examples like this and one example of was see also on the

1037
01:08:10,050 --> 01:08:14,510
position of the document timeline so let's in news stories and so

1038
01:08:17,300 --> 01:08:20,480
now i go briefly through a couple of

1039
01:08:20,490 --> 01:08:25,750
examples so maybe i can see the actual algorithm so this is to first i

1040
01:08:25,750 --> 01:08:27,590
was going to

1041
01:08:27,620 --> 01:08:30,050
systems are two algorithms which

1042
01:08:30,060 --> 01:08:33,100
we're being here in ghana

1043
01:08:33,120 --> 01:08:38,560
basically the first one is based on the

1044
01:08:38,570 --> 01:08:44,960
visualizing texts as so first we build a graph out of the document collection then

1045
01:08:44,970 --> 01:08:47,710
we basically visualizing the graph

1046
01:08:49,780 --> 01:08:53,010
me to give you an example so let's say if the

1047
01:08:53,030 --> 01:08:59,370
in this case we have a collection of seventeen hundred documents project descriptions from some

1048
01:08:59,510 --> 01:09:02,830
kind of project descriptions and here the

1049
01:09:03,910 --> 01:09:06,110
generate two clusters

1050
01:09:06,130 --> 01:09:07,770
and so

1051
01:09:07,780 --> 01:09:13,480
each cluster is represented by the most representative works of by by the highly weighted

1052
01:09:13,500 --> 01:09:20,310
words of the from which is basically the must point of that particular

1053
01:09:20,330 --> 01:09:23,020
last so

1054
01:09:23,030 --> 01:09:28,430
having just to cluster we don't see much we just see that there's a good

1055
01:09:28,440 --> 01:09:30,030
set of

1056
01:09:32,010 --> 01:09:36,180
documents of projects within the collection about services

1057
01:09:36,190 --> 01:09:40,070
that's where the system is so basically it's very general here

1058
01:09:40,080 --> 01:09:44,200
and the other one is about learning quantum so these are several topics are hidden

1059
01:09:44,210 --> 01:09:47,200
in this class if you go from two to three

1060
01:09:50,080 --> 01:09:52,310
topic kind leads into

1061
01:09:52,330 --> 01:09:55,100
two top topics then the future is clear

1062
01:09:55,110 --> 01:09:56,180
if you go

1063
01:09:56,200 --> 01:09:57,480
let's a two

1064
01:09:57,530 --> 01:10:00,220
then we already see the whole picture

1065
01:10:00,240 --> 01:10:04,470
again a little bit here here we can see

1066
01:10:04,490 --> 01:10:11,560
cultural heritage education museum studies of project which europe finding missing from this area education

1067
01:10:11,560 --> 01:10:12,470
in in

1068
01:10:12,480 --> 01:10:16,520
cultural heritage and so on so is one thing

1069
01:10:16,940 --> 01:10:19,640
clustering we can see also the

1070
01:10:20,280 --> 01:10:24,140
wikimedia projects in speech and so on and so

1071
01:10:24,160 --> 01:10:25,730
what we basically do

1072
01:10:25,740 --> 01:10:27,710
we take the texts

1073
01:10:27,780 --> 01:10:29,880
we perform k means clustering

1074
01:10:30,390 --> 01:10:33,240
and the connected clusters which are kind

1075
01:10:34,180 --> 01:10:41,230
more similar to each other the cause and this things the front between the mouth

1076
01:10:41,280 --> 01:10:42,420
points of the

1077
01:10:42,440 --> 01:10:43,680
each cluster

1078
01:10:43,770 --> 01:10:49,040
n but reconstructed the graph and then we just draw graphs and this is the

1079
01:10:50,660 --> 01:10:52,670
procedure here and then we already

1080
01:10:52,680 --> 01:10:54,120
sort of structure

1081
01:10:55,220 --> 01:10:56,820
so here think

1082
01:10:57,010 --> 01:11:00,920
musicians to twenty groups of the same got document collection here

1083
01:11:00,940 --> 01:11:03,610
you can see

1084
01:11:03,620 --> 01:11:07,270
to already about the details but let's say this is

1085
01:11:07,290 --> 01:11:13,250
medical project is this you station and and cultural heritage

1086
01:11:13,970 --> 01:11:18,920
and so on

1087
01:11:20,200 --> 01:11:22,200
another similar approach

1088
01:11:22,790 --> 01:11:26,880
again is based on the clustering but uses slightly different idea

1089
01:11:26,900 --> 01:11:29,340
o is

1090
01:11:29,410 --> 01:11:31,690
something what we call filing

1091
01:11:31,710 --> 01:11:35,870
based musician some some some other people you use the word three three men as

1092
01:11:36,770 --> 01:11:38,640
so the idea here is

1093
01:11:38,650 --> 01:11:43,710
again pretty similar as before but we use here instead of k means clustering so

1094
01:11:44,560 --> 01:11:46,720
flat clustering scheme uses

1095
01:11:46,740 --> 01:11:49,380
hierarchical class class

1096
01:11:49,400 --> 01:11:54,910
and basically on the first step we split all the documents which have in our

1097
01:11:54,910 --> 01:11:56,610
collection into

1098
01:11:57,240 --> 01:12:01,220
two clusters like before

1099
01:12:01,240 --> 01:12:04,840
and split also the area

1100
01:12:04,850 --> 01:12:09,070
the air drawing area was in the same proportions

1101
01:12:09,150 --> 01:12:16,740
how in the same proportion how many projects documents fell into one group or the

1102
01:12:18,080 --> 01:12:22,120
so obviously it is slightly more like and the next step

1103
01:12:23,180 --> 01:12:26,170
simply because perform clustering

1104
01:12:26,190 --> 01:12:31,040
for each of these groups so for the next step would be the discrete group

1105
01:12:31,050 --> 01:12:32,620
is split down into

1106
01:12:32,620 --> 01:12:37,490
distance between them then the affinity between w is going to be high is going

1107
01:12:37,490 --> 01:12:40,000
to be close to one in this case

1108
01:12:40,030 --> 01:12:41,920
if s and s are

1109
01:12:41,950 --> 01:12:49,670
the distant from each other then w and is going to be close to zero

1110
01:12:49,720 --> 01:12:53,390
not volume or node degree is the sum

1111
01:12:53,520 --> 01:12:54,990
all of the edges

1112
01:12:55,020 --> 01:12:58,160
that are connected to the nodes from the end

1113
01:12:58,170 --> 01:13:01,150
in the end is the creation of the critical quantity the corsican quantity and what

1114
01:13:01,540 --> 01:13:04,640
so it's important to remember the end

1115
01:13:04,660 --> 01:13:06,460
it is the sum of all edges

1116
01:13:06,500 --> 01:13:08,040
that are connected

1117
01:13:08,060 --> 01:13:10,340
to the end nodes

1118
01:13:10,350 --> 01:13:14,980
this is the no this is the volume of the volume of a cluster is

1119
01:13:14,990 --> 01:13:17,350
to compute the value of got to what we need to do is simply go

1120
01:13:17,420 --> 01:13:22,040
over all of the nodes in the cluster in some of the volumes

1121
01:13:23,030 --> 01:13:28,290
we basically go over each node in some of the edges that are connected

1122
01:13:28,310 --> 01:13:29,600
to that node

1123
01:13:29,780 --> 01:13:34,540
the cut between two clusters to subgraphs with this is a graph and one of

1124
01:13:34,680 --> 01:13:39,040
partition the graph into one subgraph in the second subgraph then the cut

1125
01:13:39,040 --> 01:13:40,720
it is the sum of the edges

1126
01:13:40,720 --> 01:13:42,740
but we need to take out

1127
01:13:42,750 --> 01:13:44,300
together partitioning

1128
01:13:49,660 --> 01:13:52,770
now we can think about different kind of graph cuts or a different kind of

1129
01:13:52,770 --> 01:13:57,100
criteria so to define the graph cut or to use the graph to find partitioning

1130
01:13:57,350 --> 01:14:01,850
one would simply be to find the partition we want to cluster made or putting

1131
01:14:02,040 --> 01:14:06,170
a partition the graph we can look for the partitioning of that

1132
01:14:06,180 --> 01:14:10,110
minimizes the cost to british and this is the graph we're looking for the partition

1133
01:14:10,110 --> 01:14:11,800
that minimizes

1134
01:14:11,830 --> 01:14:16,000
the edges of the weights of the edges to pick out get partitioning in this

1135
01:14:16,850 --> 01:14:19,860
what it means is that we need to think about this page

1136
01:14:19,890 --> 01:14:23,340
and if we want to partition the data this graph into two subgraphs

1137
01:14:23,350 --> 01:14:25,980
the end up having one subgraph theory

1138
01:14:26,000 --> 01:14:28,850
and the other one being the only the soul

1139
01:14:29,550 --> 01:14:30,480
that aside

1140
01:14:30,980 --> 01:14:35,990
this is this is not very good criteria to have a reasonable partitioning of graphs

1141
01:14:36,230 --> 01:14:39,740
a good partitioning of a graph and the reason here is that the reason is

1142
01:14:39,740 --> 01:14:44,720
that it tends to come out nodes that are on the edge

1143
01:14:44,740 --> 01:14:47,980
of the dataset and if you have an accuracy of one point which is far

1144
01:14:48,910 --> 01:14:52,280
then through and you will end up if you implement this if you this going

1145
01:14:52,340 --> 01:14:56,090
you you can end up separating between this outlier and the rest of the data

1146
01:14:57,600 --> 01:15:01,800
an alternative would be the normalized cut

1147
01:15:01,890 --> 01:15:04,960
the normalized cut is using the cut itself

1148
01:15:05,020 --> 01:15:07,350
this is exactly same cut this year

1149
01:15:07,400 --> 01:15:10,710
but the protein but it also introduce the trade in it

1150
01:15:10,720 --> 01:15:14,790
what about the cut made this quantity here now this quantity is the sum of

1151
01:15:14,790 --> 01:15:17,720
one over the volume of the first cluster

1152
01:15:17,770 --> 01:15:21,240
plus one over the volume of the second cluster

1153
01:15:21,250 --> 01:15:23,850
why why is it doing anything

1154
01:15:23,900 --> 01:15:27,850
which we might consider good because that if we look at the volume of a

1155
01:15:28,750 --> 01:15:33,330
if this cluster is a small number of nodes that it includes are small and

1156
01:15:33,340 --> 01:15:37,140
the volume of this cluster is small then the this quantity is going to be

1157
01:15:40,100 --> 01:15:43,860
and the trade-off between uris before between having

1158
01:15:43,910 --> 01:15:47,270
between picking out a small number of edges

1159
01:15:47,290 --> 01:15:53,530
but not ending up in same cluster clusters which volume that is too small get

1160
01:15:54,050 --> 01:15:55,410
balance between

1161
01:15:55,420 --> 01:16:02,240
making as few edges as we can but still think clusters which are relatively large

1162
01:16:02,470 --> 01:16:05,940
if you implement for example if do if you human this criterion if you optimize

1163
01:16:06,100 --> 01:16:12,720
this quality criterion you can end up having this kind of partitioning of data

1164
01:16:12,730 --> 01:16:21,550
now finding the solution for

1165
01:16:21,730 --> 01:16:26,110
for minimizing or the solution the minimizes to find a good source low normalized cut

1166
01:16:26,120 --> 01:16:30,460
is combinatorial this is the broken combinatorial problem so to find the solution is NP

1167
01:16:31,550 --> 01:16:35,550
and we know that we usually want to run away from NPR problems in one

1168
01:16:35,550 --> 01:16:40,940
five the approximation the partition the about here is the spectral graph partitioning

1169
01:16:40,960 --> 01:16:43,720
this is basically an approximation for this problem

1170
01:16:43,770 --> 01:16:48,480
i'm going to see different ways of looking at spectral clustering as an approximation

1171
01:16:48,510 --> 01:16:53,240
for minimizing the normalized cut

1172
01:16:53,250 --> 01:17:00,250
so what is expected clustering

1173
01:17:00,260 --> 01:17:04,100
one third somebody told me that so well with the with the loss of spectral

1174
01:17:04,100 --> 01:17:08,260
clustering is at the north pole clustering is take some affinity matrix you find tagging

1175
01:17:09,270 --> 01:17:12,380
and the k means over there and that's it

1176
01:17:12,390 --> 01:17:14,730
and his right i mean you a lot

1177
01:17:14,740 --> 01:17:17,500
o five or and you have a lot of criteria then you have a lot

1178
01:17:17,500 --> 01:17:22,110
of different ways of initializing there are many papers about the clustering and they lot

1179
01:17:22,130 --> 01:17:26,210
differences between them but the basic thing the basic idea in all of these methods

1180
01:17:26,420 --> 01:17:29,600
is that you're going to represent data is the graph

1181
01:17:29,640 --> 01:17:34,860
and instead of putting edges you can simply summarize it in the matrix

1182
01:17:34,870 --> 01:17:36,680
and then you're going to

1183
01:17:36,690 --> 01:17:42,140
find some other matrix this matrix w is the metrics that the n n entry

1184
01:17:42,340 --> 01:17:45,000
of this matrix would be the affinity between

1185
01:17:45,060 --> 01:17:48,010
the end in the end nodes

1186
01:17:48,020 --> 01:17:51,350
so we're going to take the data we can define some graph vary by

1187
01:17:51,360 --> 01:17:56,190
plugging all the affinity values in the matrix is defined new matrix a

1188
01:17:56,190 --> 01:17:56,960
if you look at

1189
01:17:58,840 --> 01:18:00,280
other people publishing about

1190
01:18:01,030 --> 01:18:03,550
potentially far more interesting topics and amazon

1191
01:18:04,320 --> 01:18:05,860
there's a fair number of interest

1192
01:18:06,780 --> 01:18:07,980
on making money into the air

1193
01:18:08,400 --> 01:18:10,320
you over a variety of forms and

1194
01:18:11,320 --> 01:18:12,400
before which all it

1195
01:18:15,690 --> 01:18:16,320
when we apply

1196
01:18:17,110 --> 01:18:18,630
machine learning techniques to

1197
01:18:19,940 --> 01:18:21,820
the interest from what we actually

1198
01:18:22,650 --> 01:18:26,920
look at whether which tried track and the i'm sure some people here

1199
01:18:27,530 --> 01:18:27,940
i gave

1200
01:18:28,380 --> 01:18:30,190
i gotta be complaining to me about

1201
01:18:30,610 --> 01:18:31,730
who is found so images

1202
01:18:33,230 --> 01:18:33,820
this guy that

1203
01:18:34,730 --> 01:18:36,980
brought understand we see it here so

1204
01:18:38,300 --> 01:18:39,590
the first categories they

1205
01:18:40,280 --> 01:18:42,230
the applies not mentioned so

1206
01:18:43,020 --> 01:18:47,090
so this is very much like emails from somebody's going to tag directly

1207
01:18:47,800 --> 01:18:50,480
that's right through something probably clicking the link

1208
01:18:54,050 --> 01:18:55,230
with transparent

1209
01:18:56,690 --> 01:18:58,500
so people tried to monetize

1210
01:18:59,280 --> 01:19:00,800
search is very similar to

1211
01:19:01,210 --> 01:19:03,050
people trying to monetize web searches

1212
01:19:04,230 --> 01:19:06,300
andrew if all spam which

1213
01:19:06,820 --> 01:19:08,440
spam it right to essentially

1214
01:19:08,940 --> 01:19:11,480
build out the following the follower network

1215
01:19:12,250 --> 01:19:16,520
and then sell it for money forum spam to them sometime in the future

1216
01:19:18,760 --> 01:19:20,380
so if you look at those dead spam

1217
01:19:21,480 --> 01:19:22,900
they can be classified into

1218
01:19:23,900 --> 01:19:27,980
low-dimensional stands short-term interests so they follow families

1219
01:19:28,840 --> 01:19:32,300
probably long-term interest because it takes time to build networks

1220
01:19:33,860 --> 01:19:36,380
and those guys are very much shorter

1221
01:19:37,110 --> 01:19:39,150
so as long as they can monetize things

1222
01:19:39,530 --> 01:19:40,280
pretty fast

1223
01:19:41,860 --> 01:19:43,150
is is a quick going for them

1224
01:19:45,250 --> 01:19:49,730
so the question is like can we actually the fact that the interesting patterns of things

1225
01:19:51,340 --> 01:19:53,300
for spam detection and don't

1226
01:19:54,400 --> 01:19:56,300
just a show you a couple of examples

1227
01:19:58,360 --> 01:20:00,550
probably not very visible here but

1228
01:20:02,750 --> 01:20:04,780
what this shows is essentially is they

1229
01:20:05,630 --> 01:20:07,190
patterns of user interaction

1230
01:20:08,360 --> 01:20:09,710
in terms of applying

1231
01:20:10,570 --> 01:20:12,020
four three users

1232
01:20:13,960 --> 01:20:18,460
you probably don't see those kinds of connections here but what is interesting here is like a lot of

1233
01:20:19,150 --> 01:20:20,210
normal users

1234
01:20:21,000 --> 01:20:22,250
quote unquote inter

1235
01:20:23,530 --> 01:20:27,110
communicate primality locally so people are friends

1236
01:20:27,880 --> 01:20:28,650
people they follow

1237
01:20:29,780 --> 01:20:30,820
in more or less

1238
01:20:31,320 --> 01:20:32,800
the same geographical location

1239
01:20:36,280 --> 01:20:43,070
if you look at the patterns found which probably not all the visible here but we should be seeing here

1240
01:20:43,750 --> 01:20:46,710
is that a lot of long term collections

1241
01:20:48,360 --> 01:20:49,840
coming from various places

1242
01:20:51,630 --> 01:20:54,150
and also many connections corresponding to

1243
01:20:55,110 --> 01:20:59,500
people living in the same area so those mentions essentially corresponding to

1244
01:21:00,440 --> 01:21:01,550
from interactions

1245
01:21:02,360 --> 01:21:03,750
so the the data

1246
01:21:04,690 --> 01:21:05,480
related to be

1247
01:21:06,150 --> 01:21:10,380
interaction and the network which is very meaningful for detecting spam

1248
01:21:16,020 --> 01:21:16,570
just to

1249
01:21:17,280 --> 01:21:19,860
wrap up really wanted to give credit to a lot of people

1250
01:21:20,610 --> 01:21:24,020
which were wear on building out infrastructure

1251
01:21:25,900 --> 01:21:27,000
from various size

1252
01:21:27,940 --> 01:21:31,440
both the core machine learning and the building pipelines

1253
01:21:32,250 --> 01:21:35,000
think feature extraction so you can create all of them

1254
01:21:36,210 --> 01:21:38,400
but those collective mind the primary

1255
01:21:39,000 --> 01:21:40,250
creators of the system

1256
01:21:42,920 --> 01:21:44,030
about half of the two

1257
01:21:44,860 --> 01:21:49,110
and the boston precautions i

1258
01:21:49,380 --> 01:21:53,320
so you may be interesting comment when you talked about what definition of big data

1259
01:21:53,320 --> 01:21:55,530
is in the definition with something like

1260
01:21:55,940 --> 01:21:57,460
if you have to have a data warehouse

1261
01:21:58,480 --> 01:21:59,300
that it's big data

1262
01:21:59,840 --> 01:22:00,840
and the like question is

1263
01:22:02,250 --> 01:22:08,730
researchers like us who don't happen to have data warehouses lying around actually ever do research on big data

1264
01:22:11,590 --> 01:22:12,190
i think so

1265
01:22:12,670 --> 01:22:15,690
i mean they interesting thing about the big data is

1266
01:22:17,320 --> 01:22:22,570
well to analyze various problems you very rarely need all the data at the same time

1267
01:22:23,630 --> 01:22:25,650
so if i say that really needs they

1268
01:22:26,320 --> 01:22:27,940
warehouse the processes they got

1269
01:22:28,730 --> 01:22:31,780
it means that all the components of the service

1270
01:22:32,210 --> 01:22:35,520
needed but these are the components and all parties graph

1271
01:22:36,090 --> 01:22:37,500
only a fraction of the data

1272
01:22:38,170 --> 01:22:39,960
so from the user's perspective

1273
01:22:40,420 --> 01:22:40,690
you can

1274
01:22:41,440 --> 01:22:42,020
look at

1275
01:22:43,320 --> 01:22:47,630
data repositories but no maybe which fit on on lot data center

1276
01:22:50,190 --> 01:22:54,380
can you may be talk about how you did the recommendations for the who to follow

1277
01:22:56,300 --> 01:22:57,320
task because this

1278
01:22:57,920 --> 01:22:59,230
doesn't really fit in but u

1279
01:23:00,110 --> 01:23:03,420
said so far i mean you can do it with the methods features

1280
01:23:04,210 --> 01:23:05,460
mentioned but maybe there

1281
01:23:06,000 --> 01:23:07,730
but the ones that most

1282
01:23:09,280 --> 01:23:12,630
well i can give you all the details on how it works

1283
01:23:16,630 --> 01:23:20,670
you know who to follow as a problem involving essentially commendation over a graph

1284
01:23:21,630 --> 01:23:22,860
so you might say about

1285
01:23:23,090 --> 01:23:25,210
are there variety of all

1286
01:23:26,210 --> 01:23:28,070
relevance propagation on the graph

1287
01:23:28,690 --> 01:23:30,800
we don't require machine learning

1288
01:23:32,440 --> 01:23:34,820
but i'm just going to say that if u

1289
01:23:35,360 --> 01:23:39,400
take the same signals which you can build you know heuristically or in

1290
01:23:40,090 --> 01:23:41,210
there is some formalism

1291
01:23:41,800 --> 01:23:46,020
and using in features in machine learning process we have labels from

1292
01:23:46,500 --> 01:23:47,610
using interactions

1293
01:23:48,150 --> 01:23:50,800
you can probably improve on this quite a bit

1294
01:23:53,980 --> 01:23:55,880
so hopefully up kind of request

1295
01:24:01,070 --> 01:24:04,570
they have indicators of how this is impacting in users

1296
01:24:05,980 --> 01:24:06,900
how is it impacting

1297
01:24:07,570 --> 01:24:09,230
on to decisions i mean how the

1298
01:24:10,130 --> 01:24:11,150
perceiving the benefit

1299
01:24:11,610 --> 01:24:12,760
of these different techniques and the

1300
01:24:14,190 --> 01:24:16,000
in the daily use of of of

1301
01:24:18,360 --> 01:24:22,150
i was sure i mean i cannot tell you well it's improving by x percent

1302
01:24:22,150 --> 01:24:23,820
very soon GPU

1303
01:24:25,150 --> 01:24:27,350
i will look into the catalogue

1304
01:24:27,360 --> 01:24:32,480
and you will see that there is no there is no such

1305
01:24:32,490 --> 01:24:35,490
compute that match demand

1306
01:24:35,540 --> 01:24:40,240
well maybe there but it's a

1307
01:24:40,250 --> 01:24:42,240
it's quite right

1308
01:24:42,290 --> 01:24:45,860
six in order to compute and that's it

1309
01:24:45,860 --> 01:24:48,400
it's very very when you don't need

1310
01:24:50,350 --> 01:24:54,030
another approach what you can do you can divide it up to a number of

1311
01:24:55,770 --> 01:24:58,320
and this is horizontal scale

1312
01:24:58,330 --> 01:25:00,730
and to be very very

1313
01:25:00,730 --> 01:25:03,370
naive and stupid judge just

1314
01:25:03,400 --> 01:25:05,250
a rule for summarisation

1315
01:25:05,280 --> 01:25:07,700
vertical scanning because the

1316
01:25:07,720 --> 01:25:10,040
horizontal scaling physical

1317
01:25:10,060 --> 01:25:13,480
so if you're developing an essay i need

1318
01:25:13,510 --> 01:25:18,930
OK so with that because you always you always have elements

1319
01:25:18,940 --> 01:25:22,660
elements of the modern state of the art hardware

1320
01:25:22,700 --> 01:25:27,780
and this limited things the kind but actually it's very easy to him

1321
01:25:27,790 --> 01:25:30,190
and when you can create

1322
01:25:30,190 --> 01:25:32,570
it is on the scale

1323
01:25:32,610 --> 01:25:33,850
it's always who

1324
01:25:33,850 --> 01:25:37,070
because he always kennedy

1325
01:25:37,120 --> 01:25:42,010
from secondhand cheap computation for the grand prize

1326
01:25:42,020 --> 01:25:46,930
and create huge data because way much more fully three and right now it's another

1327
01:25:46,930 --> 01:25:49,010
problem gases so

1328
01:25:49,060 --> 01:25:55,820
but at least you don't have radical colima close to you

1329
01:25:55,830 --> 01:25:57,790
how can do horizontal

1330
01:25:58,820 --> 01:26:01,470
we can do it in two dimensions

1331
01:26:01,510 --> 01:26:02,510
we can

1332
01:26:02,540 --> 01:26:07,780
because you know we always have data and algorithms data function

1333
01:26:07,820 --> 01:26:08,860
again group

1334
01:26:08,860 --> 01:26:12,990
data by function and became who function by date

1335
01:26:13,020 --> 01:26:15,980
about our search engine what does it mean

1336
01:26:16,040 --> 01:26:18,900
we can say OK

1337
01:26:18,940 --> 01:26:23,540
we don't want to remove documents from from computer to computer

1338
01:26:24,320 --> 01:26:28,530
implement all indexing on this the on

1339
01:26:28,540 --> 01:26:29,900
on this

1340
01:26:31,440 --> 01:26:36,280
and the do all snippet generation of this computer therefore this computer

1341
01:26:36,320 --> 01:26:39,270
keep all information about documents

1342
01:26:39,320 --> 01:26:43,680
but this candidate is not powerful enough to keep all index

1343
01:26:43,690 --> 01:26:46,070
so the whole all

1344
01:26:46,110 --> 01:26:51,400
of all of the form of the following this to another

1345
01:26:51,440 --> 01:26:53,350
all we can

1346
01:26:54,530 --> 01:27:01,310
another another way can simply divide collections to all computers and do inference

1347
01:27:01,320 --> 01:27:02,730
so it depends

1348
01:27:02,750 --> 01:27:08,140
but you always have these these two options how to solve this problem we always

1349
01:27:08,140 --> 01:27:09,280
had this charge

1350
01:27:09,320 --> 01:27:13,230
and of course the allow them all

1351
01:27:13,250 --> 01:27:15,560
right approach is something in the middle

1352
01:27:15,600 --> 01:27:19,600
sometimes moving functions sometimes moving day

1353
01:27:22,490 --> 01:27:24,400
the problem is why

1354
01:27:24,410 --> 01:27:28,020
so we are talking about the article scaling is good but

1355
01:27:28,060 --> 01:27:31,030
OK let's let's use only on the

1356
01:27:31,400 --> 01:27:32,530
horizontal sky

1357
01:27:33,150 --> 01:27:36,140
they don't use only for example scala

1358
01:27:36,150 --> 01:27:38,540
because it's not linear

1359
01:27:38,570 --> 01:27:44,810
you can always divide your original simply any number of parallel algorithms

1360
01:27:44,860 --> 01:27:47,400
and everything will be OK

1361
01:27:49,250 --> 01:27:50,970
simply because

1362
01:27:50,990 --> 01:27:52,310
you need to

1363
01:27:54,080 --> 01:27:58,770
so what if your charisma always not linear

1364
01:27:58,780 --> 01:28:03,060
if you are doing indicating a practicing all your documents in the number of computers

1365
01:28:03,060 --> 01:28:04,230
in parallel

1366
01:28:04,250 --> 01:28:08,690
there are some moments maybe in the future but you it's unavoidable when you merge

1367
01:28:08,690 --> 01:28:11,110
everything into one index

1368
01:28:11,110 --> 01:28:15,610
if you're doing so much you distribute your request with the class sunlight you need

1369
01:28:15,610 --> 01:28:17,530
to be combined together

1370
01:28:17,530 --> 01:28:18,450
well maybe

1371
01:28:18,480 --> 01:28:23,570
during the execution introduce some operations about the

1372
01:28:23,580 --> 01:28:25,160
this is the first problem

1373
01:28:25,180 --> 01:28:28,930
the second problem is that you when you create set

1374
01:28:28,980 --> 01:28:31,330
not all resources multiply

1375
01:28:31,360 --> 01:28:33,780
if you have to compute

1376
01:28:33,820 --> 01:28:38,350
speed of the network is maybe one gigabit per second

1377
01:28:38,360 --> 01:28:42,060
if you have three computers of for compute

1378
01:28:42,100 --> 01:28:46,530
you resources talking about CPU memory and all the time

1379
01:28:47,270 --> 01:28:49,240
committed twice

1380
01:28:49,270 --> 01:28:53,720
but the speed of your network if you want to go by second in you

1381
01:28:53,720 --> 01:28:55,640
one usually last

1382
01:28:55,650 --> 01:28:59,480
because there are collisions in network

1383
01:28:59,490 --> 01:29:03,070
so this is a principle limitation

1384
01:29:05,080 --> 01:29:10,070
what developer said that trying to find this limitation they're trying to

1385
01:29:12,250 --> 01:29:18,030
not what we don't need all synchronization and all communication based on the number of

1386
01:29:18,060 --> 01:29:20,150
big number of computers

1387
01:29:20,190 --> 01:29:22,280
they trying

1388
01:29:22,310 --> 01:29:26,200
not to use the resources that can be scale

1389
01:29:27,120 --> 01:29:32,610
combine this with sources such way that they can be used in a scalable way

1390
01:29:34,700 --> 01:29:37,320
we need a mathematical model because

1391
01:29:37,480 --> 01:29:42,150
as i said in the beginning a lot of people here the programmers in the

1392
01:29:42,150 --> 01:29:48,540
system is it's only considers two players because it was the chess and it in

1393
01:29:48,540 --> 01:29:52,890
a way it really disregard draws it's not very good at handling draws it doesn't

1394
01:29:52,890 --> 01:29:59,100
have this epsilon delta there the ideas that we model the probability that player i

1395
01:29:59,100 --> 01:30:04,140
wins over player j in a very similar way as we just did using a

1396
01:30:04,140 --> 01:30:09,800
cumulative calcium here this just the model that you have got for one player standard

1397
01:30:09,800 --> 01:30:13,950
at it that is skilled accounting for the other player in the sampling model is

1398
01:30:13,950 --> 01:30:19,280
that you you draw in number from each of these gaussians and it's then the

1399
01:30:19,280 --> 01:30:23,500
probability distribution of one of these numbers being greater than the other

1400
01:30:23,500 --> 01:30:26,830
so it's pretty much like our assumption was

1401
01:30:26,850 --> 01:30:31,190
but now in the user moving average estimators so

1402
01:30:31,210 --> 01:30:35,100
the probability is further out the first game

1403
01:30:35,110 --> 01:30:39,520
it is updated by just using the old estimator for that

1404
01:30:39,520 --> 01:30:42,100
and weighted by one minus of

1405
01:30:42,120 --> 01:30:46,830
and take with all the new result into account

1406
01:30:46,840 --> 01:30:51,070
so that's the underlying equation and in addition to make that work you have to

1407
01:30:51,070 --> 01:30:56,150
assume that a constant average skill so each game is really is zero sum game

1408
01:30:56,340 --> 01:31:00,360
is if two players play if one goes up and the other one has to

1409
01:31:00,360 --> 01:31:03,090
go down by the corresponding amount

1410
01:31:03,180 --> 01:31:08,380
also in you don't maintain the distribution over these assets but must represent the number

1411
01:31:10,010 --> 01:31:14,560
in addition in the there's a linear approximation made

1412
01:31:14,570 --> 01:31:17,600
because you have to use the phi function here

1413
01:31:17,610 --> 01:31:23,250
you approximate that phi functions in the early around zero

1414
01:31:23,270 --> 01:31:25,340
and you see that this is good in the

1415
01:31:25,400 --> 01:31:27,110
in a certain area

1416
01:31:27,130 --> 01:31:31,940
but becomes quite bad outside this area

1417
01:31:32,000 --> 01:31:36,520
but it makes the calculation much easier and

1418
01:31:36,520 --> 01:31:38,560
yes the result of that

1419
01:31:39,740 --> 01:31:43,570
what you see here is again the difference in skills if you like of the

1420
01:31:43,570 --> 01:31:49,880
two players and and he the update that results from the game

1421
01:31:49,910 --> 01:31:54,930
if one player wins against the other and what you see here is that again

1422
01:31:54,930 --> 01:32:00,240
there is relatively little happening in this regime which is no surprise regime but in

1423
01:32:00,240 --> 01:32:06,040
the surprise regime you see that the e update here and here

1424
01:32:06,130 --> 01:32:12,480
kind of thresholds so if you beat a much stronger player nominally stronger player then

1425
01:32:12,480 --> 01:32:17,030
you get only a very limited reward for that and that's the problem with you

1426
01:32:17,030 --> 01:32:20,750
look at the same time so this is in comparison our function at the same

1427
01:32:20,750 --> 01:32:27,390
time of course this form here create a certain robustness of the system because there

1428
01:32:27,390 --> 01:32:33,150
are certain outrageous game results do not spoil the statistics

1429
01:32:33,180 --> 01:32:37,120
so this is the update equation for you know which is really pretty straightforward if

1430
01:32:37,120 --> 01:32:40,900
you have a table of the phi function here and that's also used in many

1431
01:32:40,900 --> 01:32:42,340
video games

1432
01:32:42,360 --> 01:32:47,870
and just to summarise some of the properties you can view it as an approximation

1433
01:32:47,870 --> 01:32:54,380
of this music system but due to the linear approximation it deteriorates matchmaking using this

1434
01:32:54,380 --> 01:32:59,930
after this approximation was very good around zero when people had very similar nominal playing

1435
01:32:59,930 --> 01:33:04,520
strength but it became bad at this on the sides right so you need very

1436
01:33:04,520 --> 01:33:09,710
good match making you can just pair kasparov for example with the very weak chess

1437
01:33:09,710 --> 01:33:14,850
player that would lead the chances are still for example that they draw and that

1438
01:33:14,850 --> 01:33:18,610
would pull down kasparov in you know quite a bit so that's one of the

1439
01:33:19,930 --> 01:33:22,250
it was only when teams one

1440
01:33:22,600 --> 01:33:28,340
one point of the distribution and can essentially be viewed as a fixed stepsize version

1441
01:33:31,150 --> 01:33:38,020
there are some aspects of course you you know it's designed for two players but

1442
01:33:38,150 --> 01:33:42,030
so is is quite limited in in that respect

1443
01:33:42,050 --> 01:33:47,490
so there's a ranking in halo two as well but i would like to

1444
01:33:47,540 --> 01:33:51,950
the reason i would like to move on to some results because we tried out

1445
01:33:51,960 --> 01:33:56,510
the system on on the data that we obtain from this beta testing and we

1446
01:33:56,510 --> 01:33:59,960
just wanted to know what was going on that we didn't became first

1447
01:34:00,020 --> 01:34:05,690
and so there are five different so-called hoppers those different games time game types that

1448
01:34:05,810 --> 01:34:10,390
being played and for example in the free all hopper that's the one we just

1449
01:34:10,390 --> 01:34:15,380
want to ramp up as many kills as you possibly can there were sixty thousand

1450
01:34:15,380 --> 01:34:20,250
games there with about six thousand players and another hopper which is one where versus

1451
01:34:20,250 --> 01:34:24,960
one that's more like it dueling situation where one player a plays against one another

1452
01:34:26,270 --> 01:34:31,290
they played on different maps and they different games being played and since there were

1453
01:34:31,290 --> 01:34:36,160
not so many people the matchmaking had to be relatively relaxed so it happened that

1454
01:34:36,160 --> 01:34:41,710
you were paired with much stronger with much weaker players that of course it always

1455
01:34:41,710 --> 01:34:46,930
gets better if you have a larger population than you can always find better matches

1456
01:34:46,980 --> 01:34:54,890
now here's the plots for two players that we looked at charis is a hamlet

1457
01:34:54,890 --> 01:34:56,340
two legend

1458
01:34:56,360 --> 01:34:59,980
people regularly think that he cheats because he's so good

1459
01:34:59,980 --> 01:35:06,510
and the sequel wildman is the guy who won the freefall hopper

1460
01:35:06,520 --> 01:35:13,050
he's obviously working in the sequence of a group microsoft so here's the number of

1461
01:35:13,050 --> 01:35:18,610
games played and what you see here is the value of mu plus minus one

1462
01:35:18,610 --> 01:35:21,930
standard deviation for for these two players

1463
01:35:21,950 --> 01:35:27,130
and this is the typical kind of plot development over games that you can make

1464
01:35:27,130 --> 01:35:32,690
as he was looking inside the data space so this is a sensible can function

1465
01:35:32,690 --> 01:35:34,030
for this application

1466
01:35:34,470 --> 01:35:37,190
as you expand and again

1467
01:35:39,220 --> 01:35:42,900
so now we are here

1468
01:35:42,920 --> 01:35:45,920
if i correctly then you just stay where he was

1469
01:35:45,940 --> 01:35:47,630
as you leave the area

1470
01:35:47,650 --> 01:35:49,240
they fetching post

1471
01:35:49,260 --> 01:35:51,630
lessons from this is

1472
01:35:51,740 --> 01:35:52,970
of yourself

1473
01:35:53,470 --> 01:35:57,530
on the web for people is the set

1474
01:35:59,240 --> 01:36:01,720
you can see most slide them in there

1475
01:36:02,150 --> 01:36:03,970
you can sort of animating

1476
01:36:03,970 --> 01:36:06,340
but it is not

1477
01:36:06,340 --> 01:36:10,950
considering that the four hundred and five hundred sixty dimensional space you're trying to

1478
01:36:11,380 --> 01:36:14,380
this two-dimensional embedding in business

1479
01:36:14,510 --> 01:36:17,240
sort of constraints on what he

1480
01:36:20,220 --> 01:36:23,070
so i guess i'll just show quick digits one

1481
01:36:23,070 --> 01:36:26,190
there is a quite fun as well so we can do the same thing with

1482
01:36:26,210 --> 01:36:30,170
their digits dataset

1483
01:36:30,340 --> 01:36:31,360
this is a

1484
01:36:31,420 --> 01:36:32,920
let's look at this

1485
01:36:33,010 --> 01:36:37,170
these demos are on the web every experiment is on the web you can recreate

1486
01:36:37,170 --> 01:36:41,170
all this in the convenience of your own home

1487
01:36:41,190 --> 01:36:46,940
get these are the USPS digits i show the results from the small

1488
01:36:46,950 --> 01:36:49,760
so they had some three is over

1489
01:36:49,800 --> 01:36:51,470
they change shape

1490
01:36:51,470 --> 01:36:54,970
here's some to use the skinny too fat too

1491
01:36:54,990 --> 01:36:56,860
as the ones in here

1492
01:36:56,860 --> 01:37:02,320
so small associated with the ones that can move from the twos to the threes

1493
01:37:02,320 --> 01:37:06,300
and you get some sort of change the morphing into a three not necessary in

1494
01:37:06,300 --> 01:37:09,880
any way you might expect i think it's quite nice move to the falls on

1495
01:37:09,880 --> 01:37:13,550
there is at some point maybe it's not this visualization from the zero is where

1496
01:37:13,550 --> 01:37:15,720
you get the zeros with missing talks

1497
01:37:17,090 --> 01:37:19,090
so you go

1498
01:37:19,110 --> 01:37:23,240
unique solution has many different solutions that would be appropriate for this kernel is just

1499
01:37:23,240 --> 01:37:29,190
a visualisation you could run again with different initialisation into get something completely different

1500
01:37:29,190 --> 01:37:33,490
so you can evaluate as an advantage when it comes to you claim it's a

1501
01:37:33,510 --> 01:37:39,150
disadvantage what you clever you claim it's an advantage because you say things like

1502
01:37:39,170 --> 01:37:43,240
which would project into two spaces in latent space is one of the bank enterprise

1503
01:37:43,300 --> 01:37:45,510
you one for a tree branch

1504
01:37:45,550 --> 01:37:51,840
and for multimodality in the latent variable space which you can do with kernel PCA

1505
01:37:52,050 --> 01:37:57,130
you can do with this guy seems to depend on your application

1506
01:37:57,210 --> 01:38:06,090
space we observe space here

1507
01:38:08,260 --> 01:38:10,340
yes each x being given by

1508
01:38:10,420 --> 01:38:12,940
individual casting process

1509
01:38:12,950 --> 01:38:15,320
independent gaussian process

1510
01:38:15,320 --> 01:38:19,400
and that's what PCA tells us PCA is doing with linear

1511
01:38:19,920 --> 01:38:22,220
that's impressive

1512
01:38:23,220 --> 01:38:25,610
and said that PCA

1513
01:38:25,610 --> 01:38:29,720
but i think people have done this GTM as well is that we can consider

1514
01:38:29,720 --> 01:38:31,820
different types of models are here

1515
01:38:31,840 --> 01:38:36,820
we've got we've been talking that you know it's so far should move quickly on

1516
01:38:36,840 --> 01:38:38,340
so this isn't

1517
01:38:38,380 --> 01:38:41,240
which is binary a by images

1518
01:38:42,880 --> 01:38:45,220
database that we can either do

1519
01:38:45,220 --> 01:38:50,760
you know if you want to that i mean who knows what

1520
01:38:50,780 --> 01:38:54,800
i'm going to take advantage of the probabilistic

1521
01:38:54,800 --> 01:38:57,970
nature of the model and see what happens

1522
01:39:00,220 --> 01:39:06,610
which is previously seen before i delete pixels to reconstruct the

1523
01:39:06,630 --> 01:39:09,340
can reconstructing even with the united

1524
01:39:09,360 --> 01:39:13,170
constructing the threshold over what output is or

1525
01:39:13,300 --> 01:39:15,590
it is given one or zero

1526
01:39:15,720 --> 01:39:20,530
still have included let's just assume all missing pixels i now PCD

1527
01:39:20,740 --> 01:39:26,650
it is a sample of than the ones who pixels removed to use the pixels

1528
01:39:26,650 --> 01:39:33,090
removed is the reconstruction from noise is the reconstruction from the newly really nice

1529
01:39:33,150 --> 01:39:36,190
on the twenty three percent it's hard with to go

1530
01:39:36,260 --> 01:39:40,710
and i said this is that i mean the interpretation is that probabilistic model

1531
01:39:40,710 --> 01:39:42,240
and you can see

1532
01:39:42,260 --> 01:39:46,720
i mean is well known to british but typically the same

1533
01:39:47,150 --> 01:39:48,380
king these

1534
01:39:48,490 --> 01:39:50,320
which led to the tall

1535
01:39:50,340 --> 01:39:51,490
like this

1536
01:39:51,530 --> 01:39:52,510
what is the

1537
01:39:53,260 --> 01:39:56,260
being here which i think probably are

1538
01:39:56,360 --> 01:39:58,780
looks like it two which is what you would hope

1539
01:40:04,360 --> 01:40:09,530
i think this is the data set which has all the values so we're used

1540
01:40:09,530 --> 01:40:12,470
to getting with one or regression

1541
01:40:12,490 --> 01:40:13,210
i hope

1542
01:40:14,260 --> 01:40:16,490
so now

1543
01:40:16,630 --> 01:40:18,650
integrated missing

1544
01:40:18,740 --> 01:40:19,900
we can do

1545
01:40:19,900 --> 01:40:21,780
which is just a regular PCA

1546
01:40:22,820 --> 01:40:23,630
that's great

1547
01:40:23,650 --> 01:40:26,440
move away from the origin that's what we expect

1548
01:40:26,450 --> 01:40:28,590
and it's p and b

1549
01:40:28,610 --> 01:40:32,130
on the interesting things is probably too small

1550
01:40:32,280 --> 01:40:35,970
this is a clinic

1551
01:40:37,150 --> 01:40:39,130
or they were killed

1552
01:40:41,550 --> 01:40:42,670
i can see

1553
01:40:42,690 --> 01:40:45,320
i mean if you visual things in this world

1554
01:40:45,320 --> 01:40:48,260
if there is a region

1555
01:40:48,260 --> 01:40:50,170
well whole typically

1556
01:40:50,230 --> 01:40:55,760
that killed it so you might suggest that he might reconsider when he

1557
01:40:55,780 --> 01:40:57,490
he kills the horse

1558
01:40:59,740 --> 01:41:04,420
the GTM can have high dimensional spaces with TTN you have to increase the number

1559
01:41:04,420 --> 01:41:08,340
of points also with st network b

1560
01:41:08,610 --> 01:41:16,470
partitions that set different classifications different partitions the pattern is the same for each of

1561
01:41:16,470 --> 01:41:19,690
these but some of the partitions the harder to predict them out so this is

1562
01:41:19,690 --> 01:41:22,590
a significant so it in

1563
01:41:22,610 --> 01:41:24,690
as you do this kind of

1564
01:41:24,740 --> 01:41:27,860
you get a decrease in error rate so there is some validity in going to

1565
01:41:27,860 --> 01:41:30,320
harvard in two dimensions for that date

1566
01:41:31,670 --> 01:41:35,490
i probably won't have much time for discussion to one of the nice things about

1567
01:41:35,490 --> 01:41:36,670
this work

1568
01:41:36,690 --> 01:41:38,530
if someone else

1569
01:41:38,550 --> 01:41:40,760
downloaded from the web

1570
01:41:40,780 --> 01:41:45,510
and use it in inverse kinematics so since then

1571
01:41:45,630 --> 01:41:48,740
especially the united states

1572
01:41:49,940 --> 01:41:53,260
there is no

1573
01:41:56,920 --> 01:42:01,800
we then they are also human

1574
01:42:01,820 --> 01:42:05,260
it's not just on this

1575
01:42:05,280 --> 01:42:08,510
all that

1576
01:42:09,840 --> 01:42:12,070
in the real

1577
01:42:12,110 --> 01:42:15,690
this is lot

1578
01:42:16,340 --> 01:42:17,670
we don't

1579
01:42:19,050 --> 01:42:22,760
the model

1580
01:42:22,780 --> 01:42:24,110
show here

1581
01:42:24,400 --> 01:42:28,720
it's here

1582
01:42:30,860 --> 01:42:33,470
there are three

1583
01:42:33,470 --> 01:42:34,960
are not so the or

1584
01:42:34,980 --> 01:42:40,540
the people think it's only PCA is also something unsupervised and it's not difficult also

1585
01:42:40,720 --> 01:42:44,400
or you could say density estimation is also unsupervised to just get points and do

1586
01:42:44,400 --> 01:42:48,580
something with your points they're just very clearly find rest masses what you could do

1587
01:42:49,480 --> 01:42:50,620
that can be on

1588
01:42:50,620 --> 01:42:54,710
OK of course the second thing

1589
01:42:54,800 --> 01:42:56,470
that's the thing i want to go into

1590
01:42:56,480 --> 01:42:57,700
after the slides

1591
01:42:57,710 --> 01:43:01,720
that we don't really know what the ground truth is so

1592
01:43:01,740 --> 01:43:06,590
i said in density estimation its unsupervised but of course this the true density which

1593
01:43:06,590 --> 01:43:11,340
which generated our data and we know that even though we don't have any we

1594
01:43:11,340 --> 01:43:15,110
don't get hints about this density directly and we know that this is the thing

1595
01:43:15,110 --> 01:43:18,660
we want to achieve in the and so somehow even though we don't know what

1596
01:43:18,660 --> 01:43:21,450
this the ground truth in the back which might help us

1597
01:43:21,480 --> 01:43:25,420
to get our results are two to prove that certain evidence to what we want

1598
01:43:25,420 --> 01:43:26,520
to do whatever

1599
01:43:26,570 --> 01:43:31,790
and of course that's not the case in clustering because it's really hard to define

1600
01:43:31,830 --> 01:43:35,630
but OK we would see different definitions later on and one could say OK

1601
01:43:35,650 --> 01:43:40,880
now i give the definition so now is it easier

1602
01:43:40,900 --> 01:43:43,890
and then i think this is a very important point

1603
01:43:43,930 --> 01:43:47,550
which just sort of figured out the last half year so

1604
01:43:49,740 --> 01:43:52,270
in practice the thing which makes clustering

1605
01:43:52,310 --> 01:43:53,860
so difficult is that

1606
01:43:53,870 --> 01:43:56,640
the clustering of the data set something global

1607
01:43:56,650 --> 01:44:06,630
so somewhere in classification what you can do is to get a certain dataset

1608
01:44:07,420 --> 01:44:09,810
so you get a data set was

1609
01:44:10,150 --> 01:44:12,650
a positive points

1610
01:44:12,660 --> 01:44:15,060
the negative points

1611
01:44:15,100 --> 01:44:18,440
now someone asks you OK i guess this new point

1612
01:44:20,260 --> 01:44:22,600
which labelings should give to the this new point

1613
01:44:22,700 --> 01:44:26,650
and then you would say OK i mean you build classifier inference on but essentially

1614
01:44:26,650 --> 01:44:30,700
what this classifier what do it

1615
01:44:32,730 --> 01:44:37,880
so what is classified do is someone would looking local

1616
01:44:37,880 --> 01:44:41,100
neighborhood of your data points

1617
01:44:41,110 --> 01:44:42,480
and site

1618
01:44:42,510 --> 01:44:46,120
decide what this label should get based on the on the neighborhood of all the

1619
01:44:46,910 --> 01:44:50,620
and you and and like to design a classification algorithm in the end it's the

1620
01:44:50,620 --> 01:44:55,510
same you just have to use k nearest neighbour you just saying look at the

1621
01:44:55,580 --> 01:44:59,370
k nearest neighbour points and give them the label so somehow if i want to

1622
01:44:59,370 --> 01:45:01,860
solve the classification problem

1623
01:45:01,940 --> 01:45:05,800
the question which label i give to this data point has no influence whatsoever on

1624
01:45:05,800 --> 01:45:09,290
the question whether give the data so if i have enough data can just look

1625
01:45:09,290 --> 01:45:14,110
at my small pointed out and i can optimize my classifier locally that's a very

1626
01:45:14,110 --> 01:45:15,400
important point

1627
01:45:15,400 --> 01:45:20,320
and then clustering now are different because it might very well influence like if i

1628
01:45:20,320 --> 01:45:24,210
have a data point here for example which is in the middle

1629
01:45:24,490 --> 01:45:30,190
so clustering some the global property and just say i want to have groups of

1630
01:45:30,190 --> 01:45:33,950
points which are which has something to do with each other and if i decided

1631
01:45:33,950 --> 01:45:37,910
that maybe this point and this point has nothing to do with

1632
01:45:37,930 --> 01:45:40,130
then it would be stupid maybe to say

1633
01:45:40,130 --> 01:45:43,450
and i also have to point that those two point has something to do with

1634
01:45:43,580 --> 01:45:49,300
it some of their more global influences so interface i some optimisation problem like the

1635
01:45:49,300 --> 01:45:50,390
graph cut we

1636
01:45:50,410 --> 01:45:51,950
we just discussed yesterday

1637
01:45:51,970 --> 01:45:57,220
they just can't edges between different points you can just go to one data point

1638
01:45:57,230 --> 01:46:00,560
ignore the rest of the of your class of data points and say OK i

1639
01:46:01,410 --> 01:46:05,380
the the cluster label to decide which cluster this point is just look at the

1640
01:46:05,380 --> 01:46:09,320
local level that can do this because i need to look at everything

1641
01:46:09,330 --> 01:46:12,000
and that's why

1642
01:46:12,020 --> 01:46:16,190
and i think that's really important point that's why why clustering

1643
01:46:16,210 --> 01:46:18,180
in the end is

1644
01:46:18,190 --> 01:46:21,490
like optimisation problems to come up with much much harder

1645
01:46:21,510 --> 01:46:25,330
and that's what we had yesterday there always turn out to be NP hard whatever

1646
01:46:25,330 --> 01:46:25,970
you do

1647
01:46:26,020 --> 01:46:29,230
and it's not so easy to solve this thing that's

1648
01:46:29,260 --> 01:46:33,310
very important reason

1649
01:46:34,360 --> 01:46:39,330
OK now let's try how to come up so with different ways of defining what

1650
01:46:39,330 --> 01:46:44,430
the clustering so the simplest way we that's also what we did yesterday yesterday's

1651
01:46:44,450 --> 01:46:47,910
OK just invent your own quality funds and you say

1652
01:46:47,920 --> 01:46:50,660
clustering should minimize the normalized cut

1653
01:46:50,670 --> 01:46:54,840
normalized cut is a well-defined function some all and then you can go ahead and

1654
01:46:54,840 --> 01:46:56,320
normalize it and

1655
01:46:56,330 --> 01:47:02,150
everything is fine tunnels so that's a clear way of defining what clustering is

1656
01:47:02,160 --> 01:47:05,540
but of course it's also it's pretty ad hoc i mean you

1657
01:47:05,610 --> 01:47:11,540
just you just come up with some quality function and say k means to minimize

1658
01:47:11,580 --> 01:47:16,080
minimize something like so case the number of

1659
01:47:16,090 --> 01:47:17,790
o point in your clusters

1660
01:47:17,810 --> 01:47:21,330
thanks i minus exchange square and

1661
01:47:21,350 --> 01:47:25,230
now we take square area but we also could take the absolute value we could

1662
01:47:25,270 --> 01:47:30,230
look at different norm we could solve this normalizations i mean it's it's really something

1663
01:47:30,230 --> 01:47:31,980
pretty arbitrary so

1664
01:47:32,570 --> 01:47:34,900
so if you would like

1665
01:47:34,920 --> 01:47:38,250
if you would like to come up with a definition of clustering which would set

1666
01:47:38,250 --> 01:47:42,710
in fact satisfy everybody someone would really be hard to design objective function

1667
01:47:42,750 --> 01:47:44,500
everybody would agree to it

1668
01:47:47,250 --> 01:47:49,840
another approach to define what clustering is

1669
01:47:49,900 --> 01:47:54,540
it's based on density estimation so

1670
01:47:54,550 --> 01:47:58,360
it's more this in the statistics communities so people say OK we were in in

1671
01:47:58,360 --> 01:48:02,570
this setting that all data points have been drawn according to some probability distribution

1672
01:48:02,620 --> 01:48:09,220
and maybe if we would know the probability distribution it would look like this

1673
01:48:09,250 --> 01:48:10,990
in clusters on areas

1674
01:48:11,010 --> 01:48:15,230
where we have lots of density so because you have many points which

1675
01:48:15,250 --> 01:48:19,130
a very small distance to each other somehow we would say they form cluster because

1676
01:48:19,130 --> 01:48:21,190
they related to each other

1677
01:48:21,200 --> 01:48:23,390
then there's nothing in between

1678
01:48:23,390 --> 01:48:26,040
and then we have another cluster where we have lots of points which are very

1679
01:48:26,040 --> 01:48:28,090
close to each other

1680
01:48:28,100 --> 01:48:31,630
so somewhere you could say OK just defined classes two

1681
01:48:31,660 --> 01:48:34,710
sort of correspond to all clubs in identity

1682
01:48:34,740 --> 01:48:39,300
and that's what actually many people doing what many people work

1683
01:48:39,340 --> 01:48:43,670
of course we need to make it a bit modified precisely you could for example

1684
01:48:44,240 --> 01:48:48,080
clusters are density level sets to level set of identity

1685
01:48:48,090 --> 01:48:52,320
something very simple

1686
01:48:52,330 --> 01:48:53,720
at least in theory

1687
01:48:53,770 --> 01:48:56,230
so if you have a density

1688
01:48:56,230 --> 01:48:57,500
their probability

1689
01:48:58,210 --> 01:49:01,350
so we sum i have to make in order to marginalize some i have to

1690
01:49:02,080 --> 01:49:05,430
this three-dimensional matter applicable to this two-dimensional set

1691
01:49:05,440 --> 01:49:07,730
in a way that gives is exactly the value

1692
01:49:07,750 --> 01:49:09,830
that the marginal whitaker

1693
01:49:09,850 --> 01:49:14,060
OK and the way we do that is we take the preimage under projection now

1694
01:49:14,060 --> 01:49:15,960
what does that mean all the points

1695
01:49:16,020 --> 01:49:18,370
the put project to the set

1696
01:49:18,660 --> 01:49:21,330
and since the projection is axis parallel

1697
01:49:21,350 --> 01:49:23,540
that is the set which simply

1698
01:49:23,560 --> 01:49:28,940
we continue the same here

1699
01:49:28,940 --> 01:49:30,670
think of taking the set

1700
01:49:30,690 --> 01:49:34,460
and just moving in in both directions

1701
01:49:34,480 --> 01:49:36,140
up to infinity

1702
01:49:36,160 --> 01:49:38,290
so it's like if this will be a circle

1703
01:49:38,370 --> 01:49:40,600
and this set would be useful in

1704
01:49:41,730 --> 01:49:45,910
and they actually called probability actually called cylinder sets for exactly that reason

1705
01:49:45,910 --> 01:49:52,250
these concepts which are images production so if this is a then the set

1706
01:49:52,270 --> 01:49:56,430
is the preimage under the projection

1707
01:49:56,480 --> 01:49:59,830
o thing

1708
01:50:00,770 --> 01:50:07,350
OK and now we have we can we can compute are marginal down here

1709
01:50:07,370 --> 01:50:09,660
in terms of the measure mu j

1710
01:50:10,460 --> 01:50:12,020
by simply applying u j

1711
01:50:12,040 --> 01:50:14,080
to this

1712
01:50:16,160 --> 01:50:19,140
how does that fit in with the with the probability that we're are used to

1713
01:50:19,140 --> 01:50:22,980
what we have to do when you marginalized in terms of density

1714
01:50:23,120 --> 01:50:26,670
and one variable that you want to marginalize out you have to integrate right you

1715
01:50:26,670 --> 01:50:28,500
have to integrate the variable

1716
01:50:28,560 --> 01:50:30,540
and basically

1717
01:50:30,540 --> 01:50:33,430
integrating out the certain variables

1718
01:50:33,440 --> 01:50:34,890
corresponds to

1719
01:50:34,910 --> 01:50:40,440
if you if you think of integration as something something i walk along c

1720
01:50:40,460 --> 01:50:43,710
some set and summing up over the set and what you're doing is you're you're

1721
01:50:43,710 --> 01:50:45,250
walking in this direction

1722
01:50:59,790 --> 01:51:03,640
but the

1723
01:51:03,690 --> 01:51:06,310
this is the probability measure

1724
01:51:06,350 --> 01:51:08,390
and the overall space

1725
01:51:08,430 --> 01:51:11,040
justice has measure one total

1726
01:51:11,540 --> 01:51:13,540
so this might be infinitely large

1727
01:51:13,540 --> 01:51:15,410
but it does mean it has infinite measure

1728
01:51:15,520 --> 01:51:32,140
so some more green is that we are all the same is true is not

1729
01:51:32,980 --> 01:51:39,120
because you could have some somebody maybe like this and project to the set

1730
01:51:39,140 --> 01:51:42,940
but the preimage is much longer than that potentially

1731
01:51:44,710 --> 01:51:47,910
my name is

1732
01:51:50,690 --> 01:51:55,160
of the five

1733
01:51:55,160 --> 01:51:58,790
in my view be

1734
01:52:09,040 --> 01:52:11,410
that's right

1735
01:52:11,430 --> 01:52:15,460
so it's up product space if you will stop product of this product which which

1736
01:52:15,460 --> 01:52:16,730
defines something and j

1737
01:52:16,790 --> 01:52:19,120
this is the same

1738
01:52:26,160 --> 01:52:28,210
by one

1739
01:52:34,440 --> 01:52:38,140
you would have to delete in the two cases you would have to delete different

1740
01:52:39,190 --> 01:52:41,730
so if if if these two

1741
01:52:41,750 --> 01:52:44,350
if these two entries correspond to i one

1742
01:52:44,370 --> 01:52:46,390
and these structures correspond to i two

1743
01:52:46,440 --> 01:52:50,190
and the projection of onto i one would be to delete this are one and

1744
01:52:50,190 --> 01:52:55,600
the projection onto i two would be to first

1745
01:52:55,620 --> 01:52:59,520
it's really important to distinguish these different sets of letters of texas labels and not

1746
01:52:59,520 --> 01:53:02,430
just look at the dimension of the county dimensions

1747
01:53:02,440 --> 01:53:09,370
because they can overlap and we might have different marginal distributions on

1748
01:53:29,440 --> 01:53:33,560
i have about thirty five minutes i think i can make this mistake probably

1749
01:53:33,560 --> 01:53:35,940
three of one more time

1750
01:53:35,960 --> 01:53:37,370
OK now

1751
01:53:37,390 --> 01:53:44,190
we already said before that that if we if we have a stochastic process that

1752
01:53:44,190 --> 01:53:46,120
we take the marginals

1753
01:53:46,120 --> 01:53:48,600
then these marginals must sometimes be consistent

1754
01:53:48,620 --> 01:53:49,710
there must be

1755
01:53:49,730 --> 01:53:53,480
usually quite the same under marginalization and if we have this protection here

1756
01:53:53,640 --> 01:53:56,790
then we can formalise this in this manner he that we say

1757
01:53:59,810 --> 01:54:01,410
the marginal

1758
01:54:01,430 --> 01:54:02,960
so if we

1759
01:54:02,980 --> 01:54:05,730
we we posit measure mu j on each

1760
01:54:05,750 --> 01:54:09,040
on each finite dimensional subspace for each day

1761
01:54:09,040 --> 01:54:10,690
now if we

1762
01:54:10,710 --> 01:54:13,730
we can compute we know how to compute the marginal

1763
01:54:13,770 --> 01:54:16,230
on some subspace on the guy

1764
01:54:16,370 --> 01:54:21,210
another condition that we have is that the the the the measures that we start

1765
01:54:21,210 --> 01:54:25,000
with to define or process on the finite dimensional subspaces must be all marginals of

1766
01:54:25,000 --> 01:54:25,790
each other

1767
01:54:25,790 --> 01:54:27,060
and that is simply say

1768
01:54:27,790 --> 01:54:32,810
new JMU i are two members of my family and i take the and and

1769
01:54:32,810 --> 01:54:35,270
on the guy is the subspace of omega j

1770
01:54:35,290 --> 01:54:39,440
and it takes this projection which is marginalisation then have to end up with exactly

1771
01:54:39,440 --> 01:54:40,480
the space

1772
01:54:40,500 --> 01:54:43,540
with exactly the measure that i defined before

1773
01:54:43,580 --> 01:54:46,390
on my subspace omega i simply

1774
01:54:46,410 --> 01:54:49,600
consistency and under marginalisation

1775
01:54:50,750 --> 01:54:55,480
there are different names for such systems of probability distribution some people in probability called

1776
01:54:55,540 --> 01:54:58,580
consistent and some people called the projector

1777
01:54:58,640 --> 01:55:00,230
and i

1778
01:55:00,270 --> 01:55:05,480
i advertise calling them projective because we also doing lot statistics here

1779
01:55:05,480 --> 01:55:07,290
and the

1780
01:55:07,290 --> 01:55:11,730
in statistics were consistent simply has a a lot of different meanings and we don't

1781
01:55:11,730 --> 01:55:13,000
need to add another

1782
01:55:13,020 --> 01:55:14,520
another one right

1783
01:55:15,190 --> 01:55:18,770
i call this the projective projective family

1784
01:55:18,830 --> 01:55:24,190
and now what this this serum that i mentioned the congo extension theorem tells us

1785
01:55:24,230 --> 01:55:26,940
if we have a project family of measures

1786
01:55:26,980 --> 01:55:30,710
on all the finite dimensional subspaces of our space

1787
01:55:30,730 --> 01:55:35,330
then there exists a unique measure on the on the overall space

1788
01:55:35,370 --> 01:55:37,810
which has these as its marginal

1789
01:55:40,580 --> 01:55:45,690
OK let's let's think for

1790
01:55:52,020 --> 01:55:55,250
the intuition i would like to convey about this theorem is

1791
01:55:55,310 --> 01:55:56,850
it's not

1792
01:55:56,850 --> 01:56:00,080
you know you seem to have to wait too long either you run out of

1793
01:56:00,080 --> 01:56:03,910
memory can do anything you know you wait for three months and then finally

1794
01:56:03,930 --> 01:56:07,280
pc tells aidan of memory you know that

1795
01:56:07,590 --> 01:56:11,330
so so that's why it is limits you put in

1796
01:56:11,350 --> 01:56:13,150
from microarray data

1797
01:56:13,160 --> 01:56:15,390
there are thousands of genes you know they

1798
01:56:15,420 --> 01:56:16,290
you know

1799
01:56:16,320 --> 01:56:17,650
just saying no

1800
01:56:17,660 --> 01:56:19,780
up to two hundred

1801
01:56:19,830 --> 01:56:20,660
you know

1802
01:56:22,830 --> 01:56:25,090
my the probes could be put in there

1803
01:56:25,130 --> 01:56:28,390
so for all practical purposes there might be able to find

1804
01:56:29,510 --> 01:56:32,150
expressions in very very soon

1805
01:56:33,210 --> 01:56:34,570
the data sets

1806
01:56:35,790 --> 01:56:37,440
is going crazy

1807
01:56:37,450 --> 01:56:39,830
and typically proteins acting in humans

1808
01:56:39,840 --> 01:56:41,930
probably two million of them

1809
01:56:41,940 --> 01:56:43,100
so you can

1810
01:56:43,110 --> 01:56:46,110
get lots of important fourteen expressions

1811
01:56:46,160 --> 01:56:47,870
quite easily

1812
01:56:48,190 --> 01:56:53,550
so the problems are becoming harder and and how to deal with these things

1813
01:56:54,710 --> 01:56:57,790
formica my courage with did that so so the way

1814
01:56:58,230 --> 01:57:00,130
and i don't

1815
01:57:00,140 --> 01:57:02,320
was done what they've done is

1816
01:57:02,340 --> 01:57:04,540
they they did some initial

1817
01:57:04,550 --> 01:57:06,950
preprocessing before they went to

1818
01:57:06,960 --> 01:57:09,850
my fear is that it very simple

1819
01:57:09,870 --> 01:57:13,190
know entropy kind of based correlations to look back

1820
01:57:13,200 --> 01:57:16,500
then they said OK these genes seems to

1821
01:57:16,560 --> 01:57:18,770
behaved morally similarly

1822
01:57:18,810 --> 01:57:24,840
we reduce the dimensionality by looking at similarity groups and sometimes it's also possible that

1823
01:57:24,840 --> 01:57:29,760
the you know in the given discipline people might know that how the regulatory system

1824
01:57:29,760 --> 01:57:31,270
works or four

1825
01:57:31,340 --> 01:57:33,280
it's a for the humans

1826
01:57:33,290 --> 01:57:37,380
i and they can group proteins together because they have prior knowledge

1827
01:57:37,390 --> 01:57:42,090
so using those things you can reduce the dimensionality by grouping those and that's what

1828
01:57:42,090 --> 01:57:46,820
he has done when they did that they were able to actually mind the

1829
01:57:46,840 --> 01:57:51,490
these contrast patterns which i shown you earlier today

1830
01:57:51,590 --> 01:57:55,580
following cancer that's what they've done

1831
01:57:56,820 --> 01:58:00,550
so they could a gene club is a set of k genes strongly correlated with

1832
01:58:00,550 --> 01:58:04,150
a given gene and the classes so that that's how to do that

1833
01:58:04,440 --> 01:58:10,380
some emerging patterns discovered using this method were shown earlier and in is a set

1834
01:58:10,480 --> 01:58:12,920
the following patterns that are

1835
01:58:13,030 --> 01:58:16,410
hundred per cent appearing in one tissue

1836
01:58:16,420 --> 01:58:17,630
one kind of tissue

1837
01:58:17,650 --> 01:58:21,330
i'm almost no presence

1838
01:58:21,350 --> 01:58:23,830
in the other tissues

1839
01:58:23,880 --> 01:58:26,420
you can use for example

1840
01:58:26,470 --> 01:58:28,540
tree structures

1841
01:58:28,590 --> 01:58:30,660
again there's a guy

1842
01:58:30,710 --> 01:58:32,330
jerry how is now

1843
01:58:34,310 --> 01:58:36,170
university of illinois

1844
01:58:36,180 --> 01:58:39,170
he became incredibly families

1845
01:58:40,280 --> 01:58:42,180
by basically one paper

1846
01:58:42,200 --> 01:58:43,030
which is

1847
01:58:43,040 --> 01:58:44,150
if we treat

1848
01:58:44,160 --> 01:58:48,050
so once you become families then you especially if you're in good university

1849
01:58:48,060 --> 01:58:51,980
you become even more because you attract really branch to

1850
01:58:53,000 --> 01:58:57,330
the secret to become a families is you are not right when you were able

1851
01:58:57,330 --> 01:58:59,300
to get right

1852
01:58:59,320 --> 01:59:00,240
OK so

1853
01:59:01,190 --> 01:59:05,160
so there that there's so little circularity but you have to be good to get

1854
01:59:05,250 --> 01:59:06,770
good students as well but

1855
01:59:06,820 --> 01:59:11,280
but the real amplification comes with assurance so remember that

1856
01:59:12,820 --> 01:59:17,280
that's why the lower phd students and that's the right

1857
01:59:18,740 --> 01:59:25,170
so anyway this guy became very families because they did this african pattern three so

1858
01:59:25,170 --> 01:59:28,370
what have done is that they have taken the dataset

1859
01:59:29,200 --> 01:59:32,570
built a perfect three prefix tree means that

1860
01:59:33,150 --> 01:59:34,720
you know you take data set

1861
01:59:34,770 --> 01:59:36,800
and then you construct the branch

1862
01:59:36,850 --> 01:59:39,170
and you take the next data points

1863
01:59:39,180 --> 01:59:41,760
and then if it shares many of these nodes

1864
01:59:41,790 --> 01:59:44,350
you just shared the part of the structure

1865
01:59:44,400 --> 01:59:47,250
so in order to do that what you have to do is you have to

1866
01:59:47,250 --> 01:59:51,050
do a little bit of analysis is simple analysis like

1867
01:59:51,090 --> 01:59:54,180
the frequency of single items

1868
01:59:54,230 --> 01:59:57,190
so what you do is

1869
01:59:57,200 --> 01:59:58,840
in this slide

1870
01:59:59,540 --> 02:00:01,970
OK so typically what you do is

1871
02:00:02,270 --> 02:00:05,130
so let's assume we this is my

1872
02:00:05,150 --> 02:00:08,680
that's like saying a b c d

1873
02:00:08,720 --> 02:00:13,130
and the other one let's let's take a b c d e

1874
02:00:13,180 --> 02:00:16,100
the other one is like say a

1875
02:00:20,900 --> 02:00:23,380
f or g or something like that

1876
02:00:23,390 --> 02:00:27,060
so so what i do is a very simple analysis first of all

1877
02:00:27,080 --> 02:00:28,910
the items i have is

1878
02:00:28,960 --> 02:00:30,790
a b c d

1879
02:00:30,840 --> 02:00:32,920
then we have e

1880
02:00:35,870 --> 02:00:37,200
right so

1881
02:00:37,240 --> 02:00:40,580
they appeared in our database three times

1882
02:00:40,630 --> 02:00:43,190
and b appeared twice

1883
02:00:43,240 --> 02:00:44,730
see if you

1884
02:00:44,780 --> 02:00:46,730
three times

1885
02:00:48,360 --> 02:00:50,560
the other ones

1886
02:00:50,570 --> 02:00:52,030
he wants

