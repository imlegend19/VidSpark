1
00:00:00,000 --> 00:00:04,960
we can construct a lots lots of algorithms that look like or that are dealing with

2
00:00:04,960 --> 00:00:10,480
nonlinear function classes and but that correspond to linear things or lines geometric things

3
00:00:10,480 --> 00:00:16,730
in high-dimensional space secondly  if we have  data that are  not vectorial

4
00:00:16,730 --> 00:00:20,940
to begin with so we might not know how to we cannot compute

5
00:00:20,940 --> 00:00:27,260
a dot  product between our input  data if we are able to construct a kernel

6
00:00:27,270 --> 00:00:32,400
then we will get our dot product space representation for free because in this

7
00:00:32,400 --> 00:00:38,410
case if we I mean  there's no condition here that our input points come from a

8
00:00:38,410 --> 00:00:43,160
vector space so we might remember I told you before I will only assume that this X is

9
00:00:43,160 --> 00:00:48,360
a non empty set so it could be anything but if we can define such

10
00:00:48,360 --> 00:00:55,200
a positive definite kernel on this set then it turns out that we will be

11
00:00:55,200 --> 00:00:59,000
able to construct a feature space such that the kernel computes the dot product in

12
00:00:59,000 --> 00:01:03,220
that feature space so in that sense we will get a vector space representation of

13
00:01:03,220 --> 00:01:12,090
the data yeah so maybe I should be more explicit so what I'm saying is the

14
00:01:12,090 --> 00:01:17,680
class of kernels that are positive definite is exactly the class of kernels that have

15
00:01:17,680 --> 00:01:25,020
this property that they can be let's see  which one do we use that that

16
00:01:25,020 --> 00:01:30,120
have this property that they can be written as a dot product phi of X

17
00:01:30,140 --> 00:01:33,960
phi of X prime in some other space that phi maps into so we will

18
00:01:33,960 --> 00:01:39,510
prove this so we'll get back to this  okay so we can think of this

19
00:01:39,510 --> 00:01:45,200
kernel as a kind of a nonlinear similarity measure and here there's  some examples of

20
00:01:45,200 --> 00:01:50,330
kernels we've already seen the polynomial that we can also add this positive constant in here

21
00:01:50,360 --> 00:01:55,600
in which case the polynomial kernel computes all monomials up to a degree D so

22
00:01:55,600 --> 00:02:00,280
before I showed you only the case where C is equal zero where we compute all monomials

23
00:02:00,290 --> 00:02:05,020
of exactly degree D we can also use a Gaussian kernel and

24
00:02:05,020 --> 00:02:12,960
a lot of other kernels so that's so my experience is if I see a new definition

25
00:02:12,980 --> 00:02:17,580
mathematical definition I have to work with it dit a bit  before I understand it so

26
00:02:17,580 --> 00:02:25,400
I usually try to get everybody involved and and try that everybody plays a bit with this definition so I

27
00:02:25,400 --> 00:02:31,440
hope you all have some paper with you and I have some some exercises that we'll do

28
00:02:31,440 --> 00:02:36,380
or that maybe if you first do and then we do them together and

29
00:02:36,380 --> 00:02:44,110
I would say we start with these and they all such that once you

30
00:02:44,110 --> 00:02:47,420
see the solution you will say they're easy but if you don't if you

31
00:02:47,420 --> 00:02:51,640
haven't seen the definition before nevertheless it's nontrivial to get into it and think

32
00:02:51,640 --> 00:02:57,700
about these things okay so the first two properties  in the first two cases we

33
00:02:57,700 --> 00:03:02,760
start with a feature map and then  want to prove that it leads to

34
00:03:02,760 --> 00:03:07,400
a positive definite kernel so we start off with the so I told you before that the class

35
00:03:07,400 --> 00:03:13,220
of kernels that are positives definite are exactly identical to the class of kernels that can

36
00:03:13,220 --> 00:03:18,120
be represented this way this means whenever we have such a mapping phi and we compute

37
00:03:18,120 --> 00:03:21,480
this thing we end up with a positive definite kernel and whenever we have a

38
00:03:21,480 --> 00:03:27,200
positive different kernel we can construct or we can prove that there exists some phi

39
00:03:27,200 --> 00:03:34,140
such that this positive definite kernel is can be represented this way so will prove

40
00:03:34,180 --> 00:03:37,340
that this is true and one of the directions is the easy one this

41
00:03:37,340 --> 00:03:41,560
is the easy one we start with phi  look at this thing and prove

42
00:03:41,560 --> 00:03:47,360
that this kernel is positive definite afterwards we will prove the difficult ones so

43
00:03:47,360 --> 00:03:53,240
please try to prove this what does it mean to prove this it means you

44
00:03:53,240 --> 00:03:58,620
define a function K like this you don't know yet if it's a positive definite kernel

45
00:03:58,620 --> 00:04:08,100
and then you try to verify that this such defined kernel satisfies this inequality

46
00:04:08,100 --> 00:04:14,210
here no matter what X we use no matter what A we use so take take

47
00:04:14,220 --> 00:04:20,300
the kernel on the other slide substitute it in here and see if you can prove that

48
00:04:20,580 --> 00:04:25,240
and once you've done that you can try to prove the second thing so maybe I'll

49
00:04:25,240 --> 00:04:28,500
say a word about the second thing as well and then you can get going so

50
00:04:28,500 --> 00:04:34,260
the sec second thing is already it's also easy but it already involves

51
00:04:34,260 --> 00:04:39,980
a little bit more  in the second thing we construct a kernel on

52
00:04:39,980 --> 00:04:46,900
sets rather than input points so this is saying that if we have a kernel on

53
00:04:46,900 --> 00:04:52,080
on the points of X then we can construct a simple kernel on sets of

54
00:04:52,080 --> 00:04:56,700
points of X by just summing over  all kernel values that we get if

55
00:04:56,700 --> 00:05:01,540
we compare elements of the one side to elements of the other side and  one

56
00:05:01,540 --> 00:05:07,400
can prove that this is a positive definite kernel by constructing a feature map from

57
00:05:07,400 --> 00:05:12,360
reasonable to assume infinite exchangeability if we if somebody tells you that my data

58
00:05:12,360 --> 00:05:18,460
set the model my model should not depend on the way my data items are labelled

59
00:05:18,460 --> 00:05:24,740
then you can directly say that you have to model your your data set using

60
00:05:24,740 --> 00:05:30,320
a Bayesian model with an unknown latent variable G with some prior which you don't

61
00:05:30,320 --> 00:05:36,500
know you have to construct it and then given G your data items are gonna be IID so it's kind of

62
00:05:36,540 --> 00:05:41,840
directly goes from this very innocuous assumption of infinite exchangeability directly to a Bayesian

63
00:05:41,840 --> 00:05:48,980
model which is given by this so how does this have to do with the

64
00:05:49,010 --> 00:05:54,860
Dirichlet processes and the fact that CRPs are projective and exchangeable because the CRP

65
00:05:54,860 --> 00:06:00,500
is projective and exchangeable  we can define such an infinitely exchangeable  sequence in the following way

66
00:06:00,500 --> 00:06:07,840
so we first can sample a partition of all natural numbers according to a CRP

67
00:06:07,840 --> 00:06:12,240
distribution parameterized by alpha I can't really say how you could do this but you can

68
00:06:12,250 --> 00:06:18,000
imagine that you can do this in an iterative manner in the CRP basically every customer

69
00:06:18,000 --> 00:06:23,580
that comes in you can assign into some table and if we imagine this process is running

70
00:06:23,580 --> 00:06:29,260
infinitely long  than that gives you a partition of the natural numbers and that

71
00:06:29,260 --> 00:06:35,500
would be a partition of the natural numbers then for every cluster in our partition

72
00:06:35,670 --> 00:06:44,600
we can sample a Y a variable Y C IID from our base distribution H

73
00:06:44,600 --> 00:06:49,520
and then we simply set  XI to be equal to Y C where C is the

74
00:06:49,530 --> 00:06:54,200
cluster that I belongs to so we can visualize in the following way so

75
00:06:54,200 --> 00:06:58,700
we have customers x one to x nine and we're simply gonna say that X one

76
00:06:58,700 --> 00:07:02,980
X three X six all take on the same value given by Y one X

77
00:07:02,980 --> 00:07:07,980
two and seven  are gonna take on the same value given by Y two and so forth

78
00:07:07,980 --> 00:07:11,720
and because the CRP is projective and exchangeable you can show that this sequence

79
00:07:11,720 --> 00:07:17,860
of X one X two  X three and so forth will be infinitely exchangeable so we can

80
00:07:17,860 --> 00:07:23,440
apply De Finetti's theorem and say that there will be some underlying G that makes

81
00:07:23,440 --> 00:07:31,260
all our data items IID and in the case of the CRP here the underlying

82
00:07:31,260 --> 00:07:36,790
De Finetti measure the G will be a Dirichlet process okay so this kind of allows us a

83
00:07:36,790 --> 00:07:44,200
different way of constructing a Dirichlet process from a Chinese restaurant process right I guess I've talked

84
00:07:44,210 --> 00:07:49,220
about this before basically if you assume that your model shouldn't depend on the

85
00:07:49,220 --> 00:07:54,980
number of test items and shouldn't depend on the way that your data items are labeled

86
00:07:54,980 --> 00:08:01,380
then you should work with the infinitely exchangeable model and of course they are

87
00:08:01,380 --> 00:08:07,000
scenarios where infinite  exchangeability is not suitable for example if you know that

88
00:08:07,000 --> 00:08:12,440
your data your sequence of random variables is coming from some time series or

89
00:08:12,450 --> 00:08:18,960
maybe  if so yeah that's the typical case in which you don't have

90
00:08:18,970 --> 00:08:26,080
infinite  exchangeability another case in which you don't have infinite exchangeability is if

91
00:08:26,080 --> 00:08:32,880
you know that there will be a fixed number of data items for your model

92
00:08:32,880 --> 00:08:38,500
so for example if you're working with images and you know that the image's two fifty six

93
00:08:38,500 --> 00:08:42,700
by two fifty six than you know that's the number of random variables that we have that's the

94
00:08:42,700 --> 00:08:46,980
number of pixels that you observe okay in that case there's no you can't really think

95
00:08:46,980 --> 00:08:52,320
about infinite exchangeability in that case because the model has to be only over a finite number

96
00:08:52,320 --> 00:09:01,200
of pixels okay as I say exchangeability is quite important in bayesian statistics and there are generalizations of

97
00:09:01,200 --> 00:09:07,940
infinite exchangeability to kind of other sort of settings so for example you can think you

98
00:09:07,940 --> 00:09:14,280
can think about exchangeability in networks and in relational data so in in something

99
00:09:14,280 --> 00:09:19,600
that collaborative filtering  we had these users going down the rows of a matrix

100
00:09:19,600 --> 00:09:26,080
and movies going down the columns it's quite reasonable to a sum to assume

101
00:09:26,090 --> 00:09:30,420
that users are exchangeable right so you can permute the users and that shouldn't change

102
00:09:30,420 --> 00:09:35,580
your model you can permute the movies and that shouldn't change your model either so that's

103
00:09:35,580 --> 00:09:48,920
what's called a relational exchangeability and there are like representations there are  generalizations

104
00:09:48,920 --> 00:09:56,340
of De Finetti's theorem which applies to this more general notions of exchangeability

105
00:09:56,340 --> 00:10:02,840
so there are hierarchical and hierarchical bayesian models and Markov exchangeability  as well okay

106
00:10:02,840 --> 00:10:08,880
so coming back to the Chinese restaurant process now basically in the case of the Pitman

107
00:10:08,880 --> 00:10:15,340
Yor process it also has a Chinese restaurant process representation but now this CRP representation

108
00:10:15,340 --> 00:10:19,980
has two parameters it has the original alpha parameter and it has another parameter

109
00:10:20,000 --> 00:10:27,280
called D okay which we call the discount parameter and the generative process

110
00:10:27,280 --> 00:10:32,120
basically goes as follows the probability of a new customer sitting at  table C is gonna

111
00:10:32,120 --> 00:10:39,360
be proportional to N C  the number of customers already sitting minus D where this D

112
00:10:39,390 --> 00:10:43,720
parameter has to be between zero and one and D is zero we get the original

113
00:10:43,860 --> 00:10:52,080
DP when D is greater than zero then we get a Pitman Yor process so we've subtracted

114
00:10:52,080 --> 00:10:57,920
probability mass of D from each of this four tables we then need to

115
00:10:57,920 --> 00:11:03,090
so at four times D to the probability of the new customer sitting at

116
00:11:03,100 --> 00:11:08,230
a new table okay so the probability there is gonna be proportional to alpha plus

117
00:11:08,240 --> 00:11:19,040
D times the number of tables that currently occupied so the idea is that

118
00:11:19,040 --> 00:11:23,760
if we subtract what a D from each of this four tables then the probability

119
00:11:23,760 --> 00:11:27,640
of a new table is gonna be lpha plus four D and if we have lots of

120
00:11:27,640 --> 00:11:32,520
tables that are occupied then the probability of a new customer coming in and sitting at

121
00:11:32,520 --> 00:11:46,160
a new table is also gonna grow as well sorry okay so the difference

122
00:11:46,160 --> 00:11:52,000
as from before is that we have this editional discount parameter  and it's kind

123
00:11:52,000 --> 00:11:57,300
of a nice parameter to have because it kind of  says that  if there are

124
00:11:57,300 --> 00:12:01,050
we we should really do is average the predictions about these different classifiers

125
00:12:01,490 --> 00:12:05,370
that's where the sum and product will give you but averaging even in parameter space

126
00:12:05,370 --> 00:12:07,660
for this very well behaved problem

127
00:12:08,160 --> 00:12:10,470
gives us different solutions to

128
00:12:11,410 --> 00:12:16,230
what we get from max margin and really you can you can visualizes all very geometrically

129
00:12:16,730 --> 00:12:18,260
u have data

130
00:12:18,860 --> 00:12:19,290
and then

131
00:12:19,850 --> 00:12:24,400
the data constrains in parameter space the feasible set of solutions

132
00:12:24,840 --> 00:12:29,640
these maximum margin gives you a particular point in the feasible set

133
00:12:31,530 --> 00:12:33,460
which has the largest enclosing ball

134
00:12:34,140 --> 00:12:35,360
and the base point

135
00:12:35,830 --> 00:12:37,400
would try the average

136
00:12:38,200 --> 00:12:40,260
over the feasible set in a different way

137
00:12:42,320 --> 00:12:46,420
this is useful to illustrate this also i do wanna make the point that a

138
00:12:46,420 --> 00:12:50,810
lot of us and you'll hear a lot more about it work on bayesian discriminative

139
00:12:50,810 --> 00:12:53,840
models things like calcium process classifiers and things like that

140
00:12:58,190 --> 00:13:02,800
again just to reinforce this kind difference between parametric and nonparametric models

141
00:13:06,420 --> 00:13:08,010
and i think i sort of

142
00:13:08,490 --> 00:13:16,030
i make editorial commentary in that this slide okay so parametric models assume a finite set of parameters

143
00:13:16,590 --> 00:13:20,760
in other words if you wanna make predictions given those parameters that

144
00:13:21,400 --> 00:13:23,910
the rest of your training data is irrelevant

145
00:13:24,930 --> 00:13:31,140
that's what this this is a conditional independence statements saying given the parameters the training data it can be dropped

146
00:13:32,140 --> 00:13:33,880
in terms of making predictions about x

147
00:13:34,440 --> 00:13:38,660
therefore you parameters capture everything there is to know about the data in a sense

148
00:13:38,900 --> 00:13:45,620
your parameters are in a very strict and formal sense your parameters are information channel

149
00:13:46,090 --> 00:13:47,210
from your past data

150
00:13:47,620 --> 00:13:48,840
here future predictions

151
00:13:50,360 --> 00:13:51,360
and all

152
00:13:51,890 --> 00:13:54,410
the power of your past data goes through that's

153
00:13:56,460 --> 00:13:57,170
other information

154
00:13:57,830 --> 00:14:00,660
and the complexity of the channel is bounded by

155
00:14:01,100 --> 00:14:02,700
the number of parameters that you have

156
00:14:04,460 --> 00:14:07,930
this makes them not very flexible even if you have infinitely much data

157
00:14:08,400 --> 00:14:12,170
you can only learn a finite amount of information from the data

158
00:14:13,330 --> 00:14:17,120
non parametric models assume the data distribution cannot be defined in terms of such a

159
00:14:17,120 --> 00:14:22,740
finite set of parameters but generally it means that it can be defined in terms

160
00:14:22,740 --> 00:14:25,310
of some infinite dimensional parameter space

161
00:14:26,350 --> 00:14:27,100
we can think of the

162
00:14:27,990 --> 00:14:31,310
data as a function or an unknown distribution or something like that

163
00:14:32,460 --> 00:14:35,770
the nice thing about this is that the amount of information

164
00:14:36,320 --> 00:14:41,850
about past data that can be used for future prediction grows with the amount of past data that we have

165
00:14:42,840 --> 00:14:44,070
and that makes them more flexible

166
00:14:45,580 --> 00:14:47,390
another way to think about it is

167
00:14:49,980 --> 00:14:53,430
parametric models are really model-based the really like

168
00:14:53,830 --> 00:14:57,100
you know you write down any particular model you learn the parameters of the model

169
00:14:58,050 --> 00:15:04,850
nonparametric methods tend to be memory based because to make predictions you need actually store

170
00:15:05,660 --> 00:15:08,170
the past data in some form or store

171
00:15:08,750 --> 00:15:12,030
a growing amount of statistics the past data in some form

172
00:15:12,560 --> 00:15:15,250
in order to make predictions so this sort of the model-based memory

173
00:15:15,960 --> 00:15:17,380
based distinction

174
00:15:17,840 --> 00:15:18,450
and i think

175
00:15:18,960 --> 00:15:19,850
on you

176
00:15:20,420 --> 00:15:25,270
you know more important i talk about the bayesian versus more classical approaches

177
00:15:26,920 --> 00:15:32,050
thing more important than the distinction is this extinction between parametric and nonparametric models

178
00:15:32,580 --> 00:15:39,430
a lot of the most successful things that happen in machine learning so that the whole kernel revolution

179
00:15:40,550 --> 00:15:44,570
and with support vector machines and all the other kinds of kernel methods

180
00:15:45,280 --> 00:15:47,060
has really been based on

181
00:15:47,460 --> 00:15:49,110
the success of non parametric

182
00:15:50,870 --> 00:15:52,150
okay it's it's the

183
00:15:52,820 --> 00:15:55,260
power and flexibility of nonparametric approaches

184
00:15:55,720 --> 00:15:59,700
that has made us able to do a lot better than what we use to

185
00:15:59,700 --> 00:16:01,470
be able to do with are small models

186
00:16:02,000 --> 00:16:03,760
and even things like neural networks

187
00:16:04,840 --> 00:16:09,260
like these large real networks are something like a deep belief network these models have

188
00:16:10,970 --> 00:16:14,060
hundreds of thousands of parameters effectively

189
00:16:14,480 --> 00:16:17,350
there are basically nonparametric models there's are so flexible

190
00:16:17,910 --> 00:16:19,760
that's you know they can actually learn

191
00:16:20,190 --> 00:16:22,070
whatever is in the data they let the data

192
00:16:22,550 --> 00:16:24,220
it's code speaks for itself

193
00:16:24,750 --> 00:16:28,180
and that's that's sort of an idea that that i think is quite powerful

194
00:16:29,260 --> 00:16:30,510
okay any questions about their

195
00:16:35,700 --> 00:16:38,170
so just as a preview

196
00:16:38,910 --> 00:16:41,130
and i think john cunningham is gonna be talking about

197
00:16:42,100 --> 00:16:44,380
nonlinear regression calcium processes

198
00:16:45,460 --> 00:16:48,530
i could actually skip over this so that i can get to

199
00:16:50,130 --> 00:16:52,130
the something making a very loud noise

200
00:16:52,960 --> 00:16:55,130
the projectors overheating or something

201
00:16:57,270 --> 00:16:58,560
alright interesting

202
00:16:59,200 --> 00:17:02,410
we have a spare project about what we're gonna find a cave that's good

203
00:17:03,770 --> 00:17:06,390
so i'm gonna skip over gaussianprocess processes just say

204
00:17:07,460 --> 00:17:10,720
gassing processes define distributions over functions

205
00:17:11,300 --> 00:17:14,020
her forget my polynomials example

206
00:17:14,780 --> 00:17:19,260
as a practical thing you really wanna be fitting polynomials the data because you don't

207
00:17:19,320 --> 00:17:21,320
really believe anything in the real world

208
00:17:21,780 --> 00:17:23,020
comes from polynomial

209
00:17:24,560 --> 00:17:30,350
if you want more flexible function approximators and gas processes are way of defining flexible

210
00:17:31,100 --> 00:17:32,290
functions from data

211
00:17:34,270 --> 00:17:37,170
you've heard about dirichlet process mixtures just now

212
00:17:39,930 --> 00:17:41,300
maybe i'll just skip over this

213
00:17:42,450 --> 00:17:43,990
because you've already heard about it

214
00:17:46,000 --> 00:17:47,670
o point i wanna make is

215
00:17:48,340 --> 00:17:52,640
the importance of distinguishing in this framework between

216
00:17:55,700 --> 00:17:57,150
and an algorithm okay

217
00:17:57,570 --> 00:18:00,120
so dirichlet process mixture r

218
00:18:01,070 --> 00:18:01,890
is a model

219
00:18:02,790 --> 00:18:03,230
of data

220
00:18:04,600 --> 00:18:06,470
and now once to define the model

221
00:18:06,880 --> 00:18:10,260
you can think of many different inference algorithms

222
00:18:10,710 --> 00:18:11,210
so far

223
00:18:11,900 --> 00:18:12,100
you know

224
00:18:16,550 --> 00:18:18,650
predictive probabilities from your model

225
00:18:19,380 --> 00:18:21,820
or r inferring hidden variables et cetera

226
00:18:22,690 --> 00:18:23,260
and so

227
00:18:23,260 --> 00:18:24,440
part of

228
00:19:06,090 --> 00:19:11,580
all just in time series of their

229
00:19:17,430 --> 00:19:23,040
so now we

230
00:19:24,830 --> 00:19:30,260
most find of them

231
00:19:31,690 --> 00:19:34,040
one might be

232
00:19:48,040 --> 00:19:52,800
we want to all

233
00:19:59,580 --> 00:20:01,450
not all

234
00:20:10,240 --> 00:20:14,700
you know

235
00:20:23,350 --> 00:20:25,570
both of

236
00:21:10,730 --> 00:21:12,570
the wish

237
00:21:12,620 --> 00:21:15,680
it was a lot of them

238
00:21:29,330 --> 00:21:35,260
well you right

239
00:22:02,470 --> 00:22:07,450
now he shouldn't

240
00:22:17,450 --> 00:22:19,470
according to the

241
00:22:21,700 --> 00:22:24,240
and then

242
00:22:29,550 --> 00:22:31,810
there you go

243
00:22:41,870 --> 00:22:43,290
it was you know

244
00:22:47,470 --> 00:22:48,620
all o

245
00:22:48,640 --> 00:22:53,480
if you can

246
00:22:54,680 --> 00:22:55,760
it is

247
00:23:09,850 --> 00:23:11,150
read thing

248
00:23:19,800 --> 00:23:22,800
i have a

249
00:23:34,520 --> 00:23:38,240
the problem

250
00:23:41,620 --> 00:23:44,950
o one

251
00:24:07,860 --> 00:24:12,530
it is called

252
00:24:13,440 --> 00:24:19,550
and what you

253
00:24:32,030 --> 00:24:34,790
let me

254
00:25:10,910 --> 00:25:13,570
all right now

255
00:25:22,650 --> 00:25:23,420
a one

256
00:25:39,300 --> 00:25:47,250
i think that

257
00:25:47,350 --> 00:25:51,080
we do not change

258
00:26:00,600 --> 00:26:06,910
you need to

259
00:26:20,140 --> 00:26:26,400
my office or vote on these forums

260
00:26:33,230 --> 00:26:37,240
there are these

261
00:26:46,060 --> 00:26:46,670
we do not

262
00:26:51,860 --> 00:26:58,790
and the

263
00:27:01,180 --> 00:27:02,060
it was

264
00:27:02,060 --> 00:27:05,920
i to be the score that you give

265
00:27:05,940 --> 00:27:10,890
two xl using the weight vector corresponding to the correct label so this will be

266
00:27:12,000 --> 00:27:15,980
so this is our f of x i y i and then you will look

267
00:27:16,710 --> 00:27:19,770
what score you give to all the other

268
00:27:19,950 --> 00:27:23,140
one possibility is that are incorrect

269
00:27:23,160 --> 00:27:24,360
right and

270
00:27:24,360 --> 00:27:29,850
the margin will be determined to be the minimum of such difference

271
00:27:30,760 --> 00:27:33,170
a y that is an incorrect y

272
00:27:33,190 --> 00:27:37,440
and that means basically that if you know if we the minimum over all possible

273
00:27:37,440 --> 00:27:41,170
y that means that we should we should choose this part in a way that

274
00:27:41,180 --> 00:27:46,130
we use y had which is the argmax over all possible incorrect y

275
00:27:46,150 --> 00:27:47,340
in that

276
00:27:47,350 --> 00:27:49,030
passion he writes

277
00:27:49,040 --> 00:27:54,400
basically what you would surely also you know diagram to explain this you take

278
00:27:54,990 --> 00:28:00,570
the y hat that is the best which is different from why i write it

279
00:28:00,570 --> 00:28:04,140
can either be the best if it if it is even better than y i

280
00:28:04,140 --> 00:28:07,420
if y is the best it is it will be based in the second best

281
00:28:07,420 --> 00:28:11,250
solution that you have and you will define the margin to be the difference in

282
00:28:11,250 --> 00:28:12,980
score between the two

283
00:28:13,030 --> 00:28:18,120
okay note that if this is positive then it means that you know your score

284
00:28:18,130 --> 00:28:22,630
for why i will be higher than the score before we ever achieve the maximum

285
00:28:22,630 --> 00:28:25,970
score with the rest sold also be higher than all the rest

286
00:28:25,990 --> 00:28:29,380
right if you think the max

287
00:28:31,130 --> 00:28:34,190
so here's here's a simple illustration

288
00:28:34,210 --> 00:28:36,290
four that so

289
00:28:36,290 --> 00:28:38,560
y value

290
00:28:38,970 --> 00:28:43,640
here the hate is kind of the score the circles are the

291
00:28:43,670 --> 00:28:48,530
labels if you like and in the field circle is that

292
00:28:48,540 --> 00:28:52,670
correct one case we look at different labels and this score and there are three

293
00:28:52,670 --> 00:28:57,630
cases that are outlined his case one would be one where the correct label gets

294
00:28:57,630 --> 00:29:01,790
the highest score and there's a certain margin that you also

295
00:29:01,810 --> 00:29:07,500
one to obtain let's say maybe it's this right margin of don't one something and

296
00:29:07,500 --> 00:29:08,820
that's the case where

297
00:29:08,850 --> 00:29:13,090
or let's say you want more one way you would be to achieve margin of

298
00:29:13,860 --> 00:29:17,140
in fact a little bit more in the way you would measure the margin is

299
00:29:17,140 --> 00:29:21,710
the difference in score between this and the next best example the second best which

300
00:29:21,710 --> 00:29:25,550
would be this one right if this one word here then you would measure the

301
00:29:25,550 --> 00:29:28,410
margin to the the to the circle right

302
00:29:28,430 --> 00:29:31,550
then you can have a case where

303
00:29:32,160 --> 00:29:34,990
the correct label still comes out on top

304
00:29:35,050 --> 00:29:39,710
but it doesn't have sufficient margin to the next best one so this is here

305
00:29:40,060 --> 00:29:42,610
the difference in score between this and this

306
00:29:42,630 --> 00:29:44,810
and it's less than this a one here

307
00:29:44,820 --> 00:29:49,660
and so then used to classify correctly but the margin into insufficient right and if

308
00:29:49,660 --> 00:29:55,300
you look at the case here we're actually something else comes out on top and

309
00:29:55,300 --> 00:29:57,230
the correct one is down here

310
00:29:58,890 --> 00:30:04,390
you're margin actually turns out to be negative and that sort of thing so this

311
00:30:04,390 --> 00:30:08,610
is so i don't know work with you you already seen this but this is

312
00:30:08,610 --> 00:30:11,500
basically a way that you can introduce the margin

313
00:30:11,520 --> 00:30:18,320
in the multiclass type of decision setting without necessarily thinking about things being on the

314
00:30:18,320 --> 00:30:21,780
other side of the hyperplane can there's no sort of other side of side of

315
00:30:21,780 --> 00:30:26,220
the hyperplane this is no more simply about the distance between the top the all

316
00:30:26,230 --> 00:30:28,880
the correct one and the next best

317
00:30:30,640 --> 00:30:32,750
OK so

318
00:30:32,770 --> 00:30:34,870
and then you can

319
00:30:34,890 --> 00:30:41,170
you can basically take part the two things together so what i talked about the

320
00:30:41,420 --> 00:30:44,130
perceptron learning

321
00:30:44,250 --> 00:30:46,070
four and

322
00:30:46,080 --> 00:30:48,400
and then the

323
00:30:48,800 --> 00:30:54,350
basically what i just said about the margin and come up with a large margin

324
00:30:54,350 --> 00:30:59,780
formulation for multi class problems so this is all well known so basically you think

325
00:30:59,780 --> 00:31:03,420
of your weight vector as being a concatenation

326
00:31:03,450 --> 00:31:06,960
all of you know all of the weight vectors that you decline for the different

327
00:31:06,960 --> 00:31:11,630
classes so this would be transposition OK so you take all your column vectors and

328
00:31:11,630 --> 00:31:13,340
just stack them together

329
00:31:13,340 --> 00:31:18,000
and then you do the usual things so if you do a soft margin SVM

330
00:31:18,020 --> 00:31:24,770
you minimize quadratic objective in w namely it w just enters in a norm of

331
00:31:26,100 --> 00:31:29,730
so note that the norm of w now basically is the sum of the norms

332
00:31:29,730 --> 00:31:36,520
so you came here to be expected another soothing talk neil lawrence about faces and

333
00:31:36,740 --> 00:31:38,530
now here first try

334
00:31:38,540 --> 00:31:42,330
we're starting with statistical learning theory this morning

335
00:31:42,750 --> 00:31:46,590
maybe fresh still and

336
00:31:49,750 --> 00:31:51,780
rather informal

337
00:31:51,800 --> 00:31:55,390
so the first twenty minutes may be required before and then

338
00:31:55,430 --> 00:31:56,460
we have some

339
00:31:57,640 --> 00:31:59,500
but actually falls out

340
00:31:59,550 --> 00:32:03,610
maybe just some personal remarks about this discussion

341
00:32:05,890 --> 00:32:09,240
i got the impression from the discussion and cognitive science

342
00:32:09,250 --> 00:32:11,400
the weights right now is

343
00:32:11,430 --> 00:32:16,320
it's about studying hall human animal cognition works

344
00:32:16,400 --> 00:32:20,060
hello OK

345
00:32:20,070 --> 00:32:23,600
so i got the impression about studying how

346
00:32:23,610 --> 00:32:26,560
cooperation in animals works

347
00:32:27,610 --> 00:32:30,820
i should say this is also what brought me into this field so i was

348
00:32:30,820 --> 00:32:34,760
interested in theoretical neuroscience at some point but

349
00:32:34,780 --> 00:32:39,420
i agree me have come to realize that for me is actually much more interesting

350
00:32:39,420 --> 00:32:42,760
to think about the future of public goods cognition converge

351
00:32:42,780 --> 00:32:45,680
more generally in

352
00:32:45,700 --> 00:32:51,070
first of all animals systems in possible artificial systems maybe also what's the theory that

353
00:32:51,070 --> 00:32:56,200
should be underlined such field of cognitive science so in that sense i think statistical

354
00:32:56,200 --> 00:32:58,200
learning theory is one such

355
00:32:58,230 --> 00:33:03,430
ten to construct a theory of some aspects of computer science

356
00:33:03,450 --> 00:33:05,000
for me that's even more

357
00:33:05,010 --> 00:33:07,820
fascinating and studying cognition in

358
00:33:07,840 --> 00:33:12,220
and also humans even though i still think this is an amazing inspiration to look

359
00:33:12,220 --> 00:33:17,170
at such problems for instance as discussed yesterday by chris

360
00:33:19,480 --> 00:33:23,590
we have left cuts far from having a theory of cognition but

361
00:33:23,900 --> 00:33:28,810
let's start with what we have and some statistical learning theory and i think even

362
00:33:28,810 --> 00:33:33,170
though most of you probably won't work on statistical learning theory and actually i wasn't

363
00:33:33,170 --> 00:33:38,320
working on currently myself i still think it's something that needs no one works in

364
00:33:38,320 --> 00:33:39,850
this field so

365
00:33:39,860 --> 00:33:43,980
i'm trying to today i'll try to give you an need how one can derive

366
00:33:44,010 --> 00:33:47,140
statistical learning theory found so

367
00:33:47,160 --> 00:33:50,570
just to make sure that if you see them in the future you're not intimidated

368
00:33:50,570 --> 00:33:53,540
because the basic idea is actually not that difficult

369
00:33:53,550 --> 00:33:57,080
but it starts very basic

370
00:33:57,520 --> 00:34:03,270
so from the beginning from the empirical inference department of the max planck institute for

371
00:34:03,270 --> 00:34:11,130
biological cybernetics by empirical inference i mean the process of drawing conclusions from empirical data

372
00:34:11,130 --> 00:34:16,940
so the state could be observations and measurements and scientists were doing their friends all

373
00:34:16,940 --> 00:34:22,010
the time with scientific inference so we might be measuring tools of x and y

374
00:34:22,040 --> 00:34:27,200
which are related in this fashion and we might then be willing to infer

375
00:34:27,200 --> 00:34:31,580
the there's linear law explaining these data points

376
00:34:32,190 --> 00:34:35,420
but we might equally well

377
00:34:35,450 --> 00:34:37,850
he willing to or actually we might

378
00:34:38,480 --> 00:34:42,800
so it's it's already pointed out even if we scattered points of in

379
00:34:42,820 --> 00:34:48,070
randomly on a piece of paper references but taking a quill pen we can still

380
00:34:48,070 --> 00:34:53,570
find some mathematical expression that explains the states have what might be more complicated expressions

381
00:34:53,580 --> 00:34:57,990
listed looks more complicated to us but it's also

382
00:34:58,010 --> 00:34:59,240
a lot and

383
00:34:59,260 --> 00:35:00,670
question is

384
00:35:00,700 --> 00:35:03,890
why do we believe in the first law in this linear law

385
00:35:03,940 --> 00:35:07,640
why do we not believe in the second law so what's the difference between these

386
00:35:08,770 --> 00:35:10,940
so that's something that has been

387
00:35:10,980 --> 00:35:15,450
it like this has thought about our mathematicians have also thought about it coming by

388
00:35:15,730 --> 00:35:18,170
and try to be more recently

389
00:35:18,190 --> 00:35:22,850
and it seems to be unclear what makes equations simple

390
00:35:22,850 --> 00:35:27,390
what makes difficult the physicist robert full took a pragmatic view

391
00:35:27,390 --> 00:35:30,660
he said that if this law it should be evident from the data

392
00:35:30,730 --> 00:35:37,790
so he said if your experiment statistics you ought to know better experiment

393
00:35:37,890 --> 00:35:43,260
so i tried to convince the interesting inference problems that are non-trivial

394
00:35:43,270 --> 00:35:46,970
the problems were better experiments are not enough and i think there are lots of

395
00:35:46,970 --> 00:35:51,300
sites from biology for instance in perception

396
00:35:51,610 --> 00:35:55,690
let me show you some images of handwritten digits

397
00:35:55,710 --> 00:36:00,660
so this looks like another easy problem of empirical inference with obvious to which is

398
00:36:00,660 --> 00:36:03,160
which did you have no problem recognizing them

399
00:36:03,170 --> 00:36:08,550
but it's the simple transformation of the images just apply a fixed permutation of pixels

400
00:36:08,730 --> 00:36:14,480
so we just three all the pixels mathematically in computers we represent such images as

401
00:36:14,670 --> 00:36:15,630
to tools

402
00:36:15,650 --> 00:36:20,670
so this limitation is we are it just means we are really axis so for

403
00:36:20,670 --> 00:36:26,430
computer and for many recognition algorithms these are actually the same kind of data so

404
00:36:26,440 --> 00:36:30,230
for the computer this problem was not harder than this one you with a lot

405
00:36:32,610 --> 00:36:34,560
it seems to be a paradox

406
00:36:34,610 --> 00:36:40,120
and the point is of course the original only easy to us because we have

407
00:36:40,120 --> 00:36:44,500
been trained on such images on our lives so actually the two problems in some

408
00:36:44,500 --> 00:36:49,880
sense almost the the same difficulty that we've been trained all of life of brain

409
00:36:49,970 --> 00:36:54,040
has been extracting statistical regularities of this kind of data

410
00:36:54,190 --> 00:37:01,040
in the words of horace barlow was physiologist brings nothing but statistical decision or

411
00:37:01,080 --> 00:37:04,040
so that's good because it means if we want to do

412
00:37:04,040 --> 00:37:07,050
brain theory we have to statistical learning theory

413
00:37:07,180 --> 00:37:09,460
and things like that

414
00:37:09,500 --> 00:37:16,580
the most of them don't exist yet statistical learning theory is we know some things

415
00:37:16,620 --> 00:37:19,560
so so far it looks like

416
00:37:19,610 --> 00:37:20,840
there are some

417
00:37:20,850 --> 00:37:25,350
trivial inference problems in science like finding a straight line to be obvious from the

418
00:37:25,350 --> 00:37:30,460
data and there are some difficult inference problems in biology

419
00:37:30,620 --> 00:37:34,000
but actually it's not the division also

420
00:37:34,050 --> 00:37:38,280
many interesting difficult inference problems in science

421
00:37:38,290 --> 00:37:40,860
and actually which is

422
00:37:41,850 --> 00:37:44,650
study in the growth of knowledge

423
00:37:44,680 --> 00:37:48,580
was applying machine learning methods in computational biology

424
00:37:48,610 --> 00:37:52,130
so i'm going to be on this problem

425
00:37:52,130 --> 00:37:56,250
they ask trying to see how people would label now the friends interface will create

426
00:37:56,250 --> 00:38:00,540
sort of strength of the connection i can let the left side with the left

427
00:38:00,540 --> 00:38:04,600
side of the bars means i barely know that i would never ask them to

428
00:38:04,600 --> 00:38:08,810
lend money and so on and the right hand side means all very close to

429
00:38:08,810 --> 00:38:12,660
me i would be very comfortable asking them for money i would be very helpful

430
00:38:12,660 --> 00:38:15,360
finding had to find a job and so OK

431
00:38:15,370 --> 00:38:16,340
so now

432
00:38:16,360 --> 00:38:18,960
now this is that this is the first part of these are now the labels

433
00:38:18,960 --> 00:38:21,730
they ask people to generate labels and now

434
00:38:21,790 --> 00:38:25,950
the question is OK i don't want to connect the features for each for each

435
00:38:25,950 --> 00:38:30,220
french so that then be tried use it for learning to try to predict what

436
00:38:30,220 --> 00:38:33,950
the strength of the friendship is right and they have they have this four

437
00:38:33,990 --> 00:38:37,500
four sets of features that were mentioned in the granovetter called the they can write

438
00:38:37,500 --> 00:38:41,540
and they say how was density of the relationship like what is the number of

439
00:38:41,550 --> 00:38:47,360
words in the world that were exchanged how many wall posts were between the front

440
00:38:47,490 --> 00:38:51,710
between the pair of friends and so on then they have intimacy types of features

441
00:38:51,710 --> 00:38:55,450
how many friends you have in common how how how much time has spent his

442
00:38:55,450 --> 00:39:01,500
life last communication how how many miles between the hometowns and so on

443
00:39:02,210 --> 00:39:07,410
sort of other classes of features that they consider about social distance meaning what is

444
00:39:07,410 --> 00:39:14,480
the age difference what is the difference in education political views the reciprocal services which

445
00:39:14,480 --> 00:39:19,350
is like how much information exchange was going on some structural features like number of

446
00:39:19,350 --> 00:39:25,170
common friends groups that they have in common cosine similarity between vector of interests in

447
00:39:25,170 --> 00:39:31,230
their profile and also some what they called emotional support we just count of positively

448
00:39:31,230 --> 00:39:36,240
and negatively emotional words in the messages exchanged by between a pair of people right

449
00:39:36,240 --> 00:39:40,170
now so we have this feature we have the ground truth because we ask people

450
00:39:40,170 --> 00:39:44,470
what they think about each each each of their friendships and we try to try

451
00:39:44,470 --> 00:39:49,450
to make predictions about what the price is right so here they train the linear

452
00:39:49,450 --> 00:39:54,150
regression model and ask how strong is your relationship right now here i have different

453
00:39:54,150 --> 00:39:57,960
sets of features and what you see that sort of intimacy which is like last

454
00:39:57,960 --> 00:40:04,830
communication number of common friends number of intimate words was sort of the most important

455
00:40:05,960 --> 00:40:11,290
the future in predicting device and that sort of followed by ten which is not

456
00:40:11,290 --> 00:40:15,230
how many words did they exchange on the wall how many posts went in each

457
00:40:15,230 --> 00:40:18,740
direction what was the depth of the thread of discussion

458
00:40:18,760 --> 00:40:24,000
for example duration is another important thing then then on the fourth

459
00:40:24,010 --> 00:40:30,590
this spot by important part is the social distance measures like on time difference in

460
00:40:30,640 --> 00:40:36,690
political opinion difference and so on and then more emotional and structural features seemed to

461
00:40:36,690 --> 00:40:37,760
be at the bottom

462
00:40:37,770 --> 00:40:38,930
OK so now

463
00:40:38,940 --> 00:40:44,020
now we sort of you examine what are the most predictive features of what people

464
00:40:44,830 --> 00:40:50,250
consider as a strong relationship it is like negatively correlated with the they seems last

465
00:40:50,250 --> 00:40:53,890
communication but positively correlated with the

466
00:40:53,900 --> 00:40:58,680
how long have you know the person so the base since first communication and then

467
00:40:58,980 --> 00:41:02,770
how many words they exchanged on the wall and so on and so forth right

468
00:41:02,770 --> 00:41:06,910
so what is sort of the the nice about this piece of work is that

469
00:41:06,970 --> 00:41:10,370
by obtaining a small label set of number of

470
00:41:10,410 --> 00:41:15,660
of edges now contains kinds of fathers and possibly labelled the whole that the whole

471
00:41:15,660 --> 00:41:20,190
facebook network we want to the strong ties and what does what sort of strong

472
00:41:20,210 --> 00:41:25,000
more stronger relationships and what we relationships right so so this is sort of the

473
00:41:25,000 --> 00:41:30,100
first part is going beyond just nodes and edges right so here it was let's

474
00:41:30,100 --> 00:41:35,450
try to see if we can somehow automatically predict what the what the relationship what

475
00:41:35,450 --> 00:41:39,940
is the strand two relationship of a relationship between a pair of people now i

476
00:41:39,940 --> 00:41:42,830
sort of want to move on and not just say what is the same but

477
00:41:42,830 --> 00:41:47,370
actually is this sort of strength positive or negative right so what i want to

478
00:41:47,400 --> 00:41:51,760
what i want to talk about now is to think about links in networks as

479
00:41:51,760 --> 00:41:55,780
positive and negative right so the way we think about this is that i have

480
00:41:55,780 --> 00:41:58,900
some network that every edge has assigned

481
00:41:58,920 --> 00:42:03,940
it has a positive sign or negative side right positive means something good something friendly

482
00:42:03,940 --> 00:42:09,290
something that's what the negative sign is the opposite of the right and wanted to

483
00:42:09,290 --> 00:42:13,990
look at is how how these signs in the network structure how do they relate

484
00:42:14,020 --> 00:42:16,220
and they will also then you got

485
00:42:16,240 --> 00:42:20,390
whether we can predict for example whether this of these two people that are connected

486
00:42:20,390 --> 00:42:22,770
by an edge are the friends or enemies

487
00:42:22,790 --> 00:42:24,160
OK so

488
00:42:24,200 --> 00:42:27,620
the task will be not just a comment what the people i mean but actually

489
00:42:27,870 --> 00:42:31,510
tell me what i think of the way you know would they be my friends

490
00:42:31,510 --> 00:42:34,810
are within my own because in facebook you can imagine that in many cases where

491
00:42:34,830 --> 00:42:39,710
you have lots of common friends with someone but you may not want to french

492
00:42:39,710 --> 00:42:44,460
right so this is what it's like to distinguish OK so that's the idea so

493
00:42:44,740 --> 00:42:49,680
the first question is whether we find networks that can that can be explicit signs

494
00:42:49,680 --> 00:42:54,080
that actually are unlabelled pluses and minuses and here i talk i'll i'll focus on

495
00:42:54,080 --> 00:42:56,540
three different networks for every edge

496
00:42:56,600 --> 00:43:01,840
has explicit plus or minus late so in all these cases there is no way

497
00:43:01,840 --> 00:43:05,700
to create an edge without labelling it with a plus or minus so the first

498
00:43:05,700 --> 00:43:11,670
one is opinions productivity websites where i basically just means i trust your product reviews

499
00:43:11,670 --> 00:43:15,870
that's the positive edge and negative edge means i don't trust your product right so

500
00:43:15,870 --> 00:43:17,450
here this is us

501
00:43:17,490 --> 00:43:19,110
it's directly

502
00:43:20,200 --> 00:43:26,460
the data comes from wikipedia is election day so wikipedia is any ship promotion process

503
00:43:26,460 --> 00:43:31,150
right after being on wikipedia for a few years you can you can be promoted

504
00:43:31,160 --> 00:43:35,090
or you could be nominated to be promoted to become an administrator and if that

505
00:43:35,090 --> 00:43:39,290
happens there is a public election where people come and say i support your nomination

506
00:43:39,290 --> 00:43:44,330
i oppose your nomination right so now we have signed network plus means i voted

507
00:43:44,330 --> 00:43:49,290
in support of your becoming an administrator minus means i oppose you becoming an administrator

508
00:43:49,290 --> 00:43:53,830
right and the dataset we look at is slow datasets of this less of these

509
00:43:53,830 --> 00:43:57,810
technological blog that has a social network attached to it which is called slashdot so

510
00:43:58,160 --> 00:44:02,430
and then you can sort of label people as friends or enemies with friends means

511
00:44:02,430 --> 00:44:06,810
i like your comments on slashdot and in your phone means i don't like slashdot

512
00:44:06,810 --> 00:44:10,540
comments so what i should point out here is the following these datasets are in

513
00:44:10,540 --> 00:44:16,410
some sense very much very different i mean opinions the interesting thing is that you

514
00:44:16,410 --> 00:44:20,690
only your trust edges are exposed to the rest of the population while nobody is

515
00:44:20,690 --> 00:44:25,210
one of the people who distrust on opinions there is no social cost of distrusting

516
00:44:25,210 --> 00:44:29,660
everyone right nobody will see how people who distrust of how many people who don't

517
00:44:29,660 --> 00:44:34,270
like and nobody who they are so you can be expressed would like know but

518
00:44:34,270 --> 00:44:39,640
there is no sort of nobody will look at strange because nobody right but people

519
00:44:39,680 --> 00:44:44,310
do not know who you trust so here only positive links are visible in wikipedia

520
00:44:44,330 --> 00:44:49,000
it's different because here its election and its public collections of everyone gets to see

521
00:44:49,000 --> 00:44:52,870
how people vote so it's sort of and it's a very serious process because after

522
00:44:52,870 --> 00:44:56,410
you know two years of hard work on wikipedia you may be promoted and there

523
00:44:56,430 --> 00:45:00,080
is the election so here it's very important what this outside and so on and

524
00:45:00,080 --> 00:45:06,770
everybody signed the public archive and so so intensely in some sense it's it's the

525
00:45:06,770 --> 00:45:09,640
most fun of the all because it's not really clear what does it mean to

526
00:45:09,950 --> 00:45:13,230
be a friend or someone enough of of someone maybe you're just using someone if

527
00:45:13,230 --> 00:45:17,930
you label them as well and so OK so very different but again every edges

528
00:45:17,930 --> 00:45:23,480
either plus or minus datasets have around hundreds of thousands of nodes and between target

529
00:45:23,480 --> 00:45:28,680
i will obtain the function that will fight together these two functions

530
00:45:28,700 --> 00:45:31,780
i would think some function

531
00:45:31,790 --> 00:45:33,140
of x a

532
00:45:33,380 --> 00:45:35,110
x being

533
00:45:35,160 --> 00:45:38,340
because i'm summing over see here

534
00:45:38,380 --> 00:45:42,660
but this function will be factorized separately

535
00:45:42,670 --> 00:45:44,550
in general

536
00:45:44,550 --> 00:45:49,600
can not having guarantee that

537
00:45:49,680 --> 00:45:53,490
the power

538
00:45:53,490 --> 00:45:55,080
the only

539
00:46:00,490 --> 00:46:04,850
between the available to get if you don't have any observations

540
00:46:04,910 --> 00:46:06,010
and if

541
00:46:06,010 --> 00:46:09,490
basically you have for example i spent three

542
00:46:09,590 --> 00:46:12,800
just assume that it might mark mark of in the future

543
00:46:12,810 --> 00:46:16,130
it is a spend three so there is no unconnected nodes

544
00:46:17,140 --> 00:46:19,510
and you have not also

545
00:46:19,550 --> 00:46:21,220
any know

546
00:46:21,270 --> 00:46:26,390
so you cannot state that there is independence between any pair of not

547
00:46:26,430 --> 00:46:29,860
i was there is a pattern that's not what

548
00:46:29,880 --> 00:46:34,220
well as a matter of fact i didn't go into details into that because

549
00:46:34,260 --> 00:46:36,540
that's actually

550
00:46:38,020 --> 00:46:40,520
exactly matches intuition

551
00:46:40,530 --> 00:46:44,170
we had to go through all the details for the vision advocates the separation and

552
00:46:44,170 --> 00:46:47,010
all that but in case of markov random fields

553
00:46:48,080 --> 00:46:52,300
we saw the initial condition depends but independent basically means

554
00:46:52,340 --> 00:46:54,580
there's no path

555
00:46:54,580 --> 00:46:58,370
if you don't need to condition well the party cases when the

556
00:46:58,400 --> 00:47:01,150
set c is the empty set

557
00:47:01,330 --> 00:47:03,230
even if that

558
00:47:03,290 --> 00:47:04,660
you don't

559
00:47:04,820 --> 00:47:08,690
if you condition on the empty set and there is no plan that

560
00:47:08,750 --> 00:47:11,480
then it's independent

561
00:47:11,500 --> 00:47:13,950
it means that

562
00:47:13,970 --> 00:47:15,590
maybe i don't think so

563
00:47:15,640 --> 00:47:19,310
so basically kris that's the reason OK

564
00:47:22,040 --> 00:47:25,130
this is something that we should think about

565
00:47:25,150 --> 00:47:29,750
also in the important concepts i mean they're theoretical concept but

566
00:47:29,760 --> 00:47:33,070
i mean it seems since this is not the type of thing you will see

567
00:47:33,070 --> 00:47:37,100
any graphical model for or in most graphical model for

568
00:47:37,160 --> 00:47:40,410
i decided just to bring these up here

569
00:47:43,380 --> 00:47:46,060
we can define what you call i maps

570
00:47:46,070 --> 00:47:49,360
d maps and p maps

571
00:47:50,340 --> 00:47:54,840
is that this is just informative but is important information

572
00:47:56,180 --> 00:47:59,840
and and these are just labels that we give to a given graph

573
00:47:59,870 --> 00:48:02,420
with respect to a given distribution

574
00:48:03,610 --> 00:48:07,540
a graph will also be said to be an i map for the map or

575
00:48:07,580 --> 00:48:11,040
p map with respect to a distribution

576
00:48:13,980 --> 00:48:17,780
we see that the graph is that the map

577
00:48:17,830 --> 00:48:19,580
of the distribution

578
00:48:19,600 --> 00:48:24,770
if every conditional independence statement satisfied by the distribution

579
00:48:24,810 --> 00:48:26,610
he is reflected

580
00:48:26,670 --> 00:48:28,060
in the graph

581
00:48:28,100 --> 00:48:29,280
in terms of

582
00:48:29,280 --> 00:48:34,130
we have separation

583
00:48:36,420 --> 00:48:38,490
a graph is said to be an i map

584
00:48:38,620 --> 00:48:42,250
if the opposite holds if

585
00:48:42,310 --> 00:48:43,790
if you take

586
00:48:43,840 --> 00:48:46,150
at every conditional independence

587
00:48:46,160 --> 00:48:49,380
the given distribution satisfies

588
00:48:49,410 --> 00:48:51,630
is implied

589
00:48:52,420 --> 00:48:55,320
by the graph is simple is satisfied

590
00:48:55,320 --> 00:48:56,690
so what they say

591
00:48:56,720 --> 00:49:00,240
the graph is said to be an i map of a distribution if every conditional

592
00:49:00,240 --> 00:49:01,950
independences implied by the graph

593
00:49:02,010 --> 00:49:09,680
he said fighting in the distribution so basically what you have here is

594
00:49:09,740 --> 00:49:13,650
you have

595
00:49:17,790 --> 00:49:22,250
and distribution

596
00:49:24,350 --> 00:49:28,010
distributions may imply a set of conditional

597
00:49:28,010 --> 00:49:30,400
independence statements

598
00:49:31,230 --> 00:49:32,920
and graphs

599
00:49:32,980 --> 00:49:35,470
i mean by another set of conditions

600
00:49:39,500 --> 00:49:41,570
so let's write

601
00:49:41,640 --> 00:49:47,610
set of conditional independence statements here in the set of conditional independence statements

602
00:49:49,950 --> 00:49:55,580
if every conditional independence statement satisfied by the distribution

603
00:49:55,620 --> 00:49:59,930
if every one of these guys

604
00:49:59,980 --> 00:50:06,530
is present here

605
00:50:06,550 --> 00:50:07,520
we have

606
00:50:07,530 --> 00:50:08,800
but these graphs

607
00:50:09,580 --> 00:50:11,350
the math

608
00:50:12,730 --> 00:50:16,570
the map

609
00:50:16,620 --> 00:50:18,160
or the

610
00:50:27,530 --> 00:50:34,040
likewise he should have

611
00:50:34,050 --> 00:50:35,240
if you have

612
00:50:36,800 --> 00:50:38,950
possible conditional independence

613
00:50:38,970 --> 00:50:41,740
that arises from graph separation these graphs

614
00:50:41,760 --> 00:50:45,570
he is respected by the distribution

615
00:50:45,620 --> 00:50:51,200
you say that these graphs is an i map for the

616
00:50:52,520 --> 00:50:57,490
another class of really interesting graph which is this one

617
00:50:57,500 --> 00:51:01,280
a graph is said to be a perfect map of a distribution if it is

618
00:51:01,280 --> 00:51:04,840
both a d map and an i map for this

619
00:51:05,900 --> 00:51:08,390
you basically have

620
00:51:08,410 --> 00:51:15,770
he said in the set are exactly the same

621
00:51:15,770 --> 00:51:18,570
so look at how interesting this is

622
00:51:18,580 --> 00:51:22,050
you may

623
00:51:22,090 --> 00:51:25,790
you may come up with a set of conditional independence statements

624
00:51:26,730 --> 00:51:29,780
and that set of conditional independence statements

625
00:51:29,790 --> 00:51:32,180
is the entire set

626
00:51:32,240 --> 00:51:35,750
that is

627
00:51:35,770 --> 00:51:37,930
implied by a given graph

628
00:51:39,470 --> 00:51:44,100
and now we have to prove that the distribution that includes that set of conditional

629
00:51:44,100 --> 00:51:47,900
independence statements has a bunch of for the

630
00:51:52,100 --> 00:51:54,810
that may be OK for you but maybe

631
00:51:54,810 --> 00:51:57,590
some of the others who are not interested in

632
00:51:57,590 --> 00:52:01,590
i said we look at every and in fact we get a feel for what

633
00:52:01,590 --> 00:52:04,820
kind of knowledge has been found to be most useful and then

634
00:52:04,870 --> 00:52:11,000
those are all goes that so to

635
00:52:13,230 --> 00:52:17,860
so the kinds of guidance that people were writing in this cave planning track and

636
00:52:17,860 --> 00:52:22,720
this man jack in one two types of once one was deactivated knowledge for example

637
00:52:22,790 --> 00:52:26,960
the collective knowledge about what are the solutions that have good water solutions that that's

638
00:52:26,990 --> 00:52:29,110
not all plants are created equal

639
00:52:29,130 --> 00:52:32,810
OK so for example my initial state was in phoenix my goal state was to

640
00:52:32,810 --> 00:52:38,050
be in canberra and i can not in all probability was not particularly bright could

641
00:52:38,050 --> 00:52:41,930
say you know what from phoenix to san diego and by from san diego to

642
00:52:41,930 --> 00:52:46,460
los angeles and then you know take a flight from los angeles tools and yet

643
00:52:46,610 --> 00:52:49,610
but yesterday i was talking to my my son and he came up to me

644
00:52:49,610 --> 00:52:50,600
and you

645
00:52:50,610 --> 00:52:54,450
which involves something like the extreme through increase my number of frequent flyer miles that

646
00:52:54,450 --> 00:52:58,010
included at least going to a a couple of times and that would be a

647
00:52:58,770 --> 00:53:02,940
it would be if you started from the initial state in

648
00:53:02,960 --> 00:53:04,720
it's just that we may not like

649
00:53:04,730 --> 00:53:06,510
OK so

650
00:53:06,560 --> 00:53:11,270
people actually brought expert knowledge on top of the main physics saying what sorts of

651
00:53:12,300 --> 00:53:14,150
humans implicitly

652
00:53:14,230 --> 00:53:19,300
OK so these are the desirable plants that might last now they could write it

653
00:53:19,510 --> 00:53:23,120
in terms of extra constraints such as you know these concerns might be satisfied by

654
00:53:23,120 --> 00:53:29,360
this planners they could write what sort of state sequences remember when you're doing a

655
00:53:29,360 --> 00:53:33,680
plan starting from the initial state as you do actions you're going through different states

656
00:53:33,680 --> 00:53:36,750
of the world so you can think of the plan as a sequence of actions

657
00:53:36,790 --> 00:53:39,190
i can think of it as a sequence of

658
00:53:40,360 --> 00:53:41,490
and i might say

659
00:53:41,510 --> 00:53:46,600
i have references on what's sequences of states i like and what from what sequence

660
00:53:47,940 --> 00:53:52,320
OK so i can write knowledge in terms of my domain in terms of what

661
00:53:52,320 --> 00:53:56,410
sorts of sequence of states good and what sorts of sequence of states that

662
00:53:56,420 --> 00:54:01,470
and if my planet has that and being procedural and from being

663
00:54:01,480 --> 00:54:05,160
progress and planning which is going forward then i can say OK i got the

664
00:54:05,190 --> 00:54:10,210
steady state is state in this plan look this sequence is violating this rule about

665
00:54:10,240 --> 00:54:11,810
bad sequences

666
00:54:11,820 --> 00:54:14,970
so i want to go forward and this one

667
00:54:14,980 --> 00:54:17,290
it turns out that one of the highly

668
00:54:17,300 --> 00:54:19,660
effective knowledge based planners

669
00:54:19,710 --> 00:54:24,430
game plan but the plan and i'll show you the kind of light and it's

670
00:54:24,430 --> 00:54:31,110
pretty it looks quite complex and they were able to write that knowledge and they

671
00:54:31,110 --> 00:54:33,710
were able to write an engine which incrementally

672
00:54:33,770 --> 00:54:39,600
evaluate these formulas to decide if any particular brand is violating any of these

673
00:54:39,620 --> 00:54:44,940
you know complex state sequence rules as soon as it violates the killing

674
00:54:44,990 --> 00:54:48,090
getting back to the ground just by doing that they were able to get a

675
00:54:48,100 --> 00:54:49,750
much faster solution

676
00:54:51,370 --> 00:54:56,980
and then there is another way of doing it declarative grammar useful solution is actually

677
00:54:56,980 --> 00:55:01,540
an interesting way of doing this which is suppose i don't value

678
00:55:01,550 --> 00:55:06,480
what of the constraints of one of the sequences what i tell you that i

679
00:55:06,480 --> 00:55:10,320
like a plan as long as it can be passed by this grammar that i

680
00:55:10,320 --> 00:55:11,960
have that been

681
00:55:11,980 --> 00:55:16,530
OK i think of the plans as primitives and then my grammar basically

682
00:55:16,540 --> 00:55:20,940
it tells you these primitives together can be mapped these non non primitives and these

683
00:55:21,080 --> 00:55:24,610
is going to map these non and then finally there's the root note

684
00:55:24,620 --> 00:55:30,440
OK so if a plan is considered desirable if it has such a plus

685
00:55:30,450 --> 00:55:32,250
it's not it's not desirable

686
00:55:32,270 --> 00:55:37,770
this is sort of like saying a program is considered syntactically correct if the ground

687
00:55:37,770 --> 00:55:40,940
grandmother actually passes the program

688
00:55:40,960 --> 00:55:43,740
they don't tell you what it is that it was impractical to just give the

689
00:55:43,740 --> 00:55:48,920
them and we have done this in in in the planning community that's called HTN

690
00:55:48,920 --> 00:55:52,780
planning how to fast network planning where you can think of the knowledge that it

691
00:55:52,780 --> 00:55:58,390
stands as a implicit grammar desirable solutions

692
00:55:58,490 --> 00:56:02,770
OK and they will be inclined to now these are all essentially the quality of

693
00:56:02,780 --> 00:56:08,220
in the sense that you don't change the inside of the customizable plans

694
00:56:08,240 --> 00:56:11,660
it is extra knowledge this is given apart from the the main physics

695
00:56:11,670 --> 00:56:14,960
OK and if the player wants to use it can use it if my account

696
00:56:14,970 --> 00:56:19,000
is it and some some planet somewhat better uses certain types of knowledge some plants

697
00:56:19,050 --> 00:56:22,620
what it used to but it's just given outside

698
00:56:22,660 --> 00:56:27,150
the other possibility is procedural control knowledge which is sort of

699
00:56:27,240 --> 00:56:32,800
either writing you know i hate level language programs for solving this problem

700
00:56:32,820 --> 00:56:34,950
OK i

701
00:56:35,000 --> 00:56:39,700
learning for control rules which are very specific plan so for example you might say

702
00:56:39,700 --> 00:56:44,060
if you are looking at at the the following open condition and it has the

703
00:56:44,060 --> 00:56:49,170
plan of the have the following unsafe feelings then don't try to establish this open

704
00:56:49,170 --> 00:56:52,160
conditions using this choice

705
00:56:52,200 --> 00:56:55,370
OK that's very specific it only makes sense that

706
00:56:55,390 --> 00:56:56,970
and you can write those things to

707
00:56:57,020 --> 00:57:01,200
obviously as you might expect humans are not very good at writing those and actually

708
00:57:01,200 --> 00:57:05,830
much easier to learn automatically because you have to analyse the failures of learners you

709
00:57:05,830 --> 00:57:09,080
can learn how to do that kind of search

710
00:57:10,240 --> 00:57:14,230
so these are the kinds of guidance that have been tried in the knowledge base

711
00:57:14,240 --> 00:57:19,220
planning and interesting of course the point of this briefly put is

712
00:57:19,230 --> 00:57:21,140
depending on who you know

713
00:57:21,150 --> 00:57:26,130
what i what i was writing the knowledge and then they are using you know

714
00:57:26,130 --> 00:57:29,580
you have different kinds of planets between different kinds of planet so at least you

715
00:57:29,580 --> 00:57:33,770
know as i some years back you know the it's the planners were able to

716
00:57:33,770 --> 00:57:38,250
to be managed planners but the plan was able to be you know some of

717
00:57:38,250 --> 00:57:39,540
the more interesting

718
00:57:39,560 --> 00:57:44,750
reachability heuristic based plan is and then there is a sharp ridges and which actually

719
00:57:44,750 --> 00:57:47,780
write sort of a procedural knowledge about domains

720
00:57:47,780 --> 00:57:49,450
we haven't yet proved uh

721
00:57:50,120 --> 00:57:52,170
there's the architectures something so far we are

722
00:57:52,750 --> 00:57:55,690
now these results we just working with the properties are positive

723
00:57:56,080 --> 00:57:57,220
the difference of the kernel

724
00:57:58,110 --> 00:58:00,570
we're going to use them to construct they are so it's nice to

725
00:58:01,950 --> 00:58:02,380
prove them

726
00:58:03,120 --> 00:58:07,140
in by elementary means what we know we already and then we can

727
00:58:08,120 --> 00:58:12,040
continue with the architects but every once we have the architects many things become

728
00:58:12,700 --> 00:58:13,670
much easier to prove

729
00:58:15,480 --> 00:58:16,170
okay so this one

730
00:58:16,630 --> 00:58:19,390
maybe it's too easy and they don't want to answer any any

731
00:58:20,080 --> 00:58:21,180
thoughts had approved that's

732
00:58:24,820 --> 00:58:25,500
so you put in

733
00:58:30,810 --> 00:58:31,930
practice so you have this

734
00:58:32,610 --> 00:58:33,440
we have this condition

735
00:58:41,790 --> 00:58:42,930
you know that this is positive

736
00:58:43,810 --> 00:58:47,140
therefore we also know that this is positive if phi is nonnegative

737
00:58:48,330 --> 00:58:49,800
now let's move on fine here

738
00:58:51,020 --> 00:58:53,460
and then we have our original formula again

739
00:58:54,860 --> 00:58:56,520
and then we have other five times

740
00:59:01,230 --> 00:59:02,010
we know it's still

741
00:59:02,420 --> 00:59:03,790
the positive

742
00:59:04,450 --> 00:59:06,970
and what is this thing this is actually the same as

743
00:59:07,420 --> 00:59:08,450
maybe now this is like

744
00:59:09,330 --> 00:59:10,750
like a mathematician would write it

745
00:59:11,340 --> 00:59:14,340
this is kind of our time scale value x

746
00:59:16,250 --> 00:59:19,400
so now we know i've five times case a positive definite kernel

747
00:59:21,000 --> 00:59:22,120
okay and basically

748
00:59:22,830 --> 00:59:25,840
okay one plus came to works exactly the same point so this

749
00:59:26,390 --> 00:59:28,230
sums is linear not only

750
00:59:28,770 --> 00:59:33,420
products but also four sums so we can do the same thing to prove that this is positive definite

751
00:59:34,440 --> 00:59:35,430
and this one

752
00:59:36,060 --> 00:59:37,450
it's also relatively elementary

753
00:59:38,110 --> 00:59:41,850
this one is the hardest one if you want to play around with it you can try tonight

754
00:59:42,930 --> 00:59:43,330
this is

755
00:59:43,980 --> 00:59:45,580
essentially a result of shawl

756
00:59:46,460 --> 00:59:49,610
four put positive definite matrices social proof that

757
00:59:50,170 --> 00:59:52,770
we have two positive definite matrices of the same size

758
00:59:53,600 --> 00:59:55,510
and you take the element wise product

759
00:59:56,000 --> 00:59:57,080
don't start matlab

760
00:59:57,820 --> 01:00:02,430
you get a positive definite matrix so it's a pity if you don't know that it's a pretty surprising result

761
01:00:03,070 --> 01:00:04,210
it's not trivial to prove

762
01:00:05,640 --> 01:00:06,820
but it's also not impossible

763
01:00:07,650 --> 01:00:09,380
i don't know the proof by heart but it's

764
01:00:10,000 --> 01:00:11,350
maybe a page or something like that

765
01:00:11,980 --> 01:00:13,200
so it can be done them

766
01:00:13,900 --> 01:00:16,560
so if you want to try but we're not going to use it end

767
01:00:17,190 --> 01:00:18,090
from that you can also

768
01:00:18,850 --> 01:00:21,370
prove some results about tensor products and other things

769
01:00:22,110 --> 01:00:23,210
and we wanted others

770
01:00:24,600 --> 01:00:24,960
okay so

771
01:00:25,630 --> 01:00:27,250
any questions about this so far

772
01:00:28,770 --> 01:00:32,300
so even if you haven't solved all them more even if it happens after seeing

773
01:00:32,310 --> 01:00:35,040
one of them if you have worked with the definition and that was the purpose

774
01:00:35,360 --> 01:00:36,160
of the exercise

775
01:00:37,480 --> 01:00:37,930
so u

776
01:00:38,710 --> 01:00:41,000
might remember this definition are what it's about

777
01:00:41,600 --> 01:00:42,520
and now we will

778
01:00:43,330 --> 01:00:45,770
use some these facts to to construct the feature space

779
01:00:48,220 --> 01:00:51,510
so the basic idea this is the overview of this construction

780
01:00:54,190 --> 01:00:56,790
the momentary construction the basic ideas

781
01:00:57,480 --> 01:01:00,250
we have to define the feature maps and now we want to prove

782
01:01:00,770 --> 01:01:04,130
given a positive definite kernel there is a feature map such that the kernel is

783
01:01:04,130 --> 01:01:05,830
the dot product after the feature map

784
01:01:06,770 --> 01:01:08,260
we want to prove given kay

785
01:01:09,610 --> 01:01:12,250
there is a map phi such that this equality is true

786
01:01:13,140 --> 01:01:17,340
so let's define mephi hand and show that it doesn't what we want to do

787
01:01:17,620 --> 01:01:20,090
so we have to define the find some space we have to

788
01:01:20,900 --> 01:01:24,830
construct a dot product in that space so because we have both define phi in

789
01:01:24,830 --> 01:01:26,210
the input space into which goes

790
01:01:27,070 --> 01:01:29,890
and then we have to show that the product together with phi

791
01:01:31,170 --> 01:01:32,440
satisfies this equality

792
01:01:32,880 --> 01:01:34,200
four hour given concave

793
01:01:35,260 --> 01:01:36,060
okay so we're going to

794
01:01:36,660 --> 01:01:39,970
actually use a spacer functions like someone here in the back

795
01:01:40,880 --> 01:01:46,580
i was mentioning before so one nice way of constructing its businesses today with spaces of functions

796
01:01:47,500 --> 01:01:50,090
and the functions and we're going to use will be functions

797
01:01:50,540 --> 01:01:51,840
defined on this domain x

798
01:01:52,880 --> 01:01:57,330
functions in one variable which we get by taking the common substituting x

799
01:01:58,840 --> 01:02:03,210
the second of the first symmetric doesn't matter but let's into the second arguement of

800
01:02:04,950 --> 01:02:08,630
so now the first arguement still open which is why this is a function x

801
01:02:09,030 --> 01:02:09,220
so the

802
01:02:09,840 --> 01:02:13,320
the function and the argument of a function is plugged into you whether it is written

803
01:02:14,870 --> 01:02:18,750
and visually forms as you can imagine if a user accounts and kernel

804
01:02:19,760 --> 01:02:22,190
and are squared exponential kernel

805
01:02:23,190 --> 01:02:25,120
exponential square whatever right

806
01:02:26,080 --> 01:02:31,550
exponentiated quadratic so that the girls in process people have to have many words for concepts

807
01:02:33,360 --> 01:02:34,910
we were talking about this the other day and

808
01:02:35,630 --> 01:02:38,010
it's like the eskimos having lots of words for snow

809
01:02:40,120 --> 01:02:44,610
pointed out like that the finnish having lots of different words for independent component analysis

810
01:02:47,210 --> 01:02:49,860
so you take one of these kernels you map into that space

811
01:02:50,620 --> 01:02:51,270
the end

812
01:02:51,810 --> 01:02:54,870
the image of a point x in that space of functions will be

813
01:02:55,650 --> 01:02:58,550
a gaussian kernel centered on the point x

814
01:02:59,560 --> 01:03:04,630
the image over point ex-prime in that space would simply be causing kernel centered ex-prime so we have these different

815
01:03:06,110 --> 01:03:07,410
these are functions

816
01:03:07,860 --> 01:03:09,310
mapping x into are

817
01:03:09,790 --> 01:03:10,870
because we subsequently

818
01:03:11,340 --> 01:03:14,430
whenever we put some x into them they give us a real number

819
01:03:15,160 --> 01:03:18,950
this is a compact notation for writing the space of all functions mapping x two are

820
01:03:20,550 --> 01:03:21,920
so this is a mapping

821
01:03:22,450 --> 01:03:25,360
we can do that it's not yet into a vector space

822
01:03:25,940 --> 01:03:28,200
but we can turn it into the space required

823
01:03:29,910 --> 01:03:33,670
we can then endow it with a dot product that satisfies this equality

824
01:03:34,280 --> 01:03:34,620
this is the

825
01:03:35,080 --> 01:03:37,710
nontrivial step this step where we're going to

826
01:03:38,280 --> 01:03:40,650
exploit the fact that case positive definite

827
01:03:42,060 --> 01:03:43,460
and the last step is actually

828
01:03:44,160 --> 01:03:47,070
i'll skip that step that's sort of a mathematical details

829
01:03:51,500 --> 01:03:52,520
so the first thing is

830
01:03:53,340 --> 01:03:55,960
we have to into a vector space orally in space

831
01:03:56,580 --> 01:03:58,290
and we simply do this by saying

832
01:03:59,210 --> 01:04:02,360
well first of all i so i told you that this kind of bumps these

833
01:04:02,360 --> 01:04:05,200
individual things there are elements of all space the images of

834
01:04:05,860 --> 01:04:07,960
input points under all feature map phi

835
01:04:09,560 --> 01:04:13,110
we will simply declare that if we take some sort of such bonds and actually in

836
01:04:13,910 --> 01:04:17,610
if we multiply them with some real numbers before taking the sum so if we take general

837
01:04:18,040 --> 01:04:20,220
linear combinations of such pumps people simply say

838
01:04:20,620 --> 01:04:25,440
these are also elements not space so that's the definition of of all space that contains a

839
01:04:25,470 --> 01:04:27,940
all functions of this form that we get by taking these

840
01:04:29,840 --> 01:04:35,130
bonds end taking in combinations so typical functions our space so this form like this if are

841
01:04:38,060 --> 01:04:39,700
so you just stop me whenever you want

842
01:04:41,430 --> 01:04:43,040
so i'm a bit are real numbers

843
01:04:43,690 --> 01:04:46,780
these are from all the main these are finite sums

844
01:04:49,030 --> 01:04:52,220
now we have to define a dot product on such functions

845
01:04:53,410 --> 01:04:57,650
well and we actually don't have much choice how we can define the dot product

846
01:04:58,120 --> 01:05:01,030
because we want the dot product to be such that if we take two to

847
01:05:01,480 --> 01:05:02,440
elementary pumps

848
01:05:03,140 --> 01:05:04,600
just images of input points

849
01:05:05,260 --> 01:05:07,470
and we want the dot product to be equal to the kernel

850
01:05:07,940 --> 01:05:08,590
so remember

851
01:05:09,470 --> 01:05:10,230
we want the

852
01:05:11,100 --> 01:05:12,000
we want this to be true

853
01:05:13,290 --> 01:05:15,610
now we have defined on mapping to it

854
01:05:16,100 --> 01:05:18,260
to take x into a kernel centered on x

855
01:05:18,260 --> 01:05:26,950
let's crew something easy here

856
01:05:28,550 --> 01:05:33,630
sort of the aero version about respondents sometimes called assertion

857
01:05:33,670 --> 01:05:36,900
so we have a hypothesis

858
01:05:36,900 --> 01:05:39,500
and we give it the subscript

859
01:05:41,550 --> 01:05:44,590
and what we're going to do is keep track

860
01:05:44,630 --> 01:05:52,280
o lines in the proof or the hypothesis of the other hypotheses are used

861
01:05:52,380 --> 01:05:56,030
OK we want to prove this

862
01:05:56,070 --> 01:06:00,400
so we introduce that hypothesis to prove this so we want this and then use

863
01:06:00,420 --> 01:06:05,670
implication introduction to get whole that's what we want is our goal here is a

864
01:06:05,670 --> 01:06:07,800
narrow b

865
01:06:17,150 --> 01:06:22,000
we introduce a or b is the antecedent of that we use it into implication

866
01:06:23,210 --> 01:06:26,070
ah to get that if we get to prove b

867
01:06:26,170 --> 01:06:28,250
OK so

868
01:06:28,300 --> 01:06:31,590
the next thing we do

869
01:06:31,630 --> 01:06:34,710
again you might be used to think systems where you don't have to do this

870
01:06:35,170 --> 01:06:36,940
we just reiterate one

871
01:06:37,000 --> 01:06:38,800
so we can use it to do

872
01:06:38,820 --> 01:06:44,030
a modus ponens and implication elimination

873
01:06:44,070 --> 01:06:48,110
so we reiterate one bring them

874
01:06:51,530 --> 01:06:54,570
from two and three

875
01:06:57,480 --> 01:06:58,860
get that now

876
01:06:58,880 --> 01:07:01,750
what subscript e we put down on the what do you reckon

877
01:07:01,760 --> 01:07:04,820
what we use to get b

878
01:07:04,860 --> 01:07:07,280
which hypothesis

879
01:07:07,320 --> 01:07:08,780
just to

880
01:07:08,780 --> 01:07:09,900
and one

881
01:07:09,960 --> 01:07:14,130
so i thought about this is why the segmentation

882
01:07:14,320 --> 01:07:26,630
that's the set of premises used

883
01:07:26,860 --> 01:07:30,250
now we can

884
01:07:37,000 --> 01:07:39,030
now what

885
01:07:39,090 --> 01:07:47,070
subscript you think we put on on air of b

886
01:07:47,090 --> 01:07:53,820
we discharge the hypothesis somebody that we don't discharging hypotheses

887
01:07:53,820 --> 01:07:55,840
we remove it

888
01:07:55,940 --> 01:07:58,690
from the dependency list

889
01:07:58,730 --> 01:08:00,980
so what's are

890
01:08:01,030 --> 01:08:03,840
where we put on the subscript

891
01:08:03,880 --> 01:08:08,500
it's just one

892
01:08:08,760 --> 01:08:13,920
in order to discharge it had to be in there

893
01:08:13,940 --> 01:08:17,860
this has to be in here to use implication introduction

894
01:08:17,880 --> 01:08:24,670
that's the important thing it has to really be used

895
01:08:24,690 --> 01:08:26,280
we couldn't just

896
01:08:26,300 --> 01:08:30,500
we couldn't do

897
01:08:39,340 --> 01:08:50,710
we could do this

898
01:08:50,730 --> 01:08:53,840
where we just brought down and then discharged

899
01:08:53,860 --> 01:08:58,210
because two wasn't in here

900
01:08:58,250 --> 01:09:00,570
we have to find some way of getting it in

901
01:09:00,630 --> 01:09:03,860
in order to discharge

902
01:09:15,570 --> 01:09:21,420
so now one is in and so we can discharge

903
01:09:28,940 --> 01:09:33,090
OK now what subscript if we put down here

904
01:09:33,110 --> 01:09:37,340
the discharge once we remove it so what we got left

905
01:09:37,670 --> 01:09:39,730
so you can leave it blank

906
01:09:39,730 --> 01:09:43,190
often empty set

907
01:09:43,210 --> 01:09:45,800
and that tells us that there

908
01:09:45,960 --> 01:09:48,420
and it's not easy

909
01:09:48,530 --> 01:09:50,980
some of the rules get a little fiddly

910
01:09:51,070 --> 01:09:58,150
show you want the conjunction rules in particular

911
01:09:58,190 --> 01:09:59,230
so that

912
01:09:59,230 --> 01:10:01,250
shows you what the

913
01:10:01,300 --> 01:10:04,780
the implication introduction and elimination rules are

914
01:10:05,480 --> 01:10:07,260
the introduction rule

915
01:10:07,280 --> 01:10:12,000
looks the same as classical works every got to keep track subscripts and the

916
01:10:12,030 --> 01:10:16,150
hypothesis they are discharging has to really be an

917
01:10:16,170 --> 01:10:18,380
the last subscript there

918
01:10:18,420 --> 01:10:21,070
in the in the subproof

919
01:10:21,090 --> 01:10:23,090
the answer is you can do it

920
01:10:23,110 --> 01:10:29,400
gotta be really in there to remove

921
01:10:47,800 --> 01:10:50,070
OK now that thing

922
01:10:50,070 --> 01:10:51,130
is problem

923
01:10:51,150 --> 01:10:58,030
we added to relevant logic it's going to give us classical logic we don't want

924
01:10:58,050 --> 01:11:02,230
we really don't want and as i told you before it violates our intuitions are

925
01:11:02,230 --> 01:11:08,460
relevant intuitions relevance intuitions about meaning of the word implication

926
01:11:08,480 --> 01:11:11,190
so what do how do we get rid of it

927
01:11:12,150 --> 01:11:26,260
if you look at this using just implication

928
01:11:26,480 --> 01:11:35,690
as i said before

929
01:11:35,750 --> 01:11:39,610
we can't

930
01:11:39,610 --> 01:11:44,300
they were discharge that we can't do anything

931
01:11:44,300 --> 01:11:46,400
now what happens if we add conjunction

932
01:11:46,420 --> 01:11:47,900
and i think

933
01:11:47,920 --> 01:11:50,360
the problem

934
01:11:50,380 --> 01:11:52,400
has anybody studied lemon spokane

935
01:11:53,690 --> 01:11:54,750
beginning logic

936
01:11:54,760 --> 01:11:58,320
people don't tend not to any more because it's very out of date and has

937
01:11:58,320 --> 01:12:03,150
very sexist examples but it's a nice system it's for classical logic but it's almost

938
01:12:03,150 --> 01:12:09,360
for relevant logic but he has a derivation of this that is conjunction

939
01:12:09,360 --> 01:12:12,550
so just with the implication rules we can derive in the fact

940
01:12:12,590 --> 01:12:16,380
we have to have a conjunction rules to do this

941
01:12:16,400 --> 01:12:23,230
and you might think the conjunction rules allow this

942
01:12:31,360 --> 01:13:35,020
OK now looks like a good proof or we really used

943
01:13:35,030 --> 01:13:36,800
so we brought in

944
01:13:37,780 --> 01:13:41,880
and brought down a and just can join them together

945
01:13:41,880 --> 01:13:44,090
on the basis of premise one two

946
01:13:44,150 --> 01:13:44,960
we have

947
01:13:45,020 --> 01:13:49,000
information and we have the information to be one of the conjunction

948
01:13:49,020 --> 01:13:51,650
rather logic doesn't allow you to do that

949
01:13:51,710 --> 01:13:55,300
they have to have in order to use conjunction introduction

950
01:13:55,300 --> 01:13:55,940
and secondly

951
01:13:59,170 --> 01:14:00,060
we don't know z

952
01:14:04,060 --> 01:14:04,870
but it is

953
01:14:06,630 --> 01:14:07,660
of interest

954
01:14:08,190 --> 01:14:09,420
if someone tell us

955
01:14:10,380 --> 01:14:11,340
be delighted to

956
01:14:12,170 --> 01:14:13,740
no it costs

957
01:14:14,220 --> 01:14:18,010
remember always that's what this is what we need to do model comparison so you

958
01:14:18,010 --> 01:14:20,700
want to compare very small clusters

959
01:14:21,330 --> 01:14:25,680
in data with one that says that just as you need to know the normalizing constant

960
01:14:26,080 --> 01:14:26,410
those two

961
01:14:27,340 --> 01:14:28,860
policies and then you can actually

962
01:14:29,630 --> 01:14:30,190
the expression

963
01:14:31,930 --> 01:14:35,450
we can evaluate the star we don't know z we like will

964
01:14:36,680 --> 01:14:40,170
you won't complain if we don't know whether it

965
01:14:41,710 --> 01:14:45,660
what column that is going to be talking about get is bonus

966
01:14:47,950 --> 01:14:53,480
in general i'm gonna generalize and say that think that is the probability distribution over

967
01:14:54,470 --> 01:14:55,160
this thing here

968
01:14:55,500 --> 01:14:58,760
this is unknown parameters and assignments

969
01:15:00,830 --> 01:15:01,630
i'm gonna give them they

970
01:15:02,200 --> 01:15:04,780
and with apologies the canonical texts now

971
01:15:06,150 --> 01:15:07,110
so this is the thing

972
01:15:14,420 --> 01:15:19,210
the article that's why we talk about problem and x is the name if we're

973
01:15:19,230 --> 01:15:22,860
actually talking about string x is the name for the unknown means and standard deviations

974
01:15:23,710 --> 01:15:24,500
assignment of

975
01:15:25,000 --> 01:15:25,600
data points

976
01:15:26,020 --> 01:15:26,620
two clusters

977
01:15:30,250 --> 01:15:34,000
what else to say about the rules of the game well what we actually want to know

978
01:15:34,490 --> 01:15:37,010
we've got a probability distribution we are interested in

979
01:15:38,120 --> 01:15:40,060
we can evaluate it any particular

980
01:15:44,160 --> 01:15:47,200
argument so you can think and act and we can go there

981
01:15:47,750 --> 01:15:49,620
well given the value of

982
01:15:51,240 --> 01:15:53,550
and work out what this is all expression

983
01:15:54,970 --> 01:15:55,760
come back to

984
01:15:57,530 --> 01:15:58,580
and what we want to do

985
01:16:05,940 --> 01:16:09,410
what we want to do is we solve these two problems

986
01:16:13,290 --> 01:16:19,130
so just a recap notation gotta read distribution always right nasty distribution and red

987
01:16:21,520 --> 01:16:22,020
can be written

988
01:16:22,870 --> 01:16:26,910
also the artery at the minus e of x divided by z

989
01:16:27,960 --> 01:16:30,590
it was just taking the log of star

990
01:16:31,420 --> 01:16:35,980
so the assumption is that we can evaluate or equivalently evaluate

991
01:16:36,650 --> 01:16:40,580
yeah that's child also has the energy that's very

992
01:16:41,720 --> 01:16:45,390
so we can evaluate this what we want do we solve problems one

993
01:16:45,800 --> 01:16:46,750
and two ideally

994
01:16:47,480 --> 01:16:51,040
the problem is we would like to draw samples from this

995
01:16:52,950 --> 01:16:53,700
what does that mean

996
01:16:54,500 --> 01:17:00,680
i means i want to take this posterior distribution come up with some plausible representatives of it

997
01:17:02,050 --> 01:17:04,730
i want to draw a few times on and say well i don't know what

998
01:17:04,730 --> 01:17:09,220
the means sample assignments other his one credible hope is another and another

999
01:17:09,560 --> 01:17:11,230
that means that could be there

1000
01:17:11,960 --> 01:17:13,560
you in this way the way you know

1001
01:17:15,810 --> 01:17:16,570
that's that's one

1002
01:17:17,180 --> 01:17:22,640
right now and sandro examples please don't get confused and think i'm growing points in this

1003
01:17:26,100 --> 01:17:27,210
i taking a risk

1004
01:17:28,090 --> 01:17:29,730
this is now the name or

1005
01:17:30,570 --> 01:17:31,560
unknown variables

1006
01:17:32,890 --> 01:17:37,270
the data is given it fits in our wandering around in hypothesis space and

1007
01:17:37,670 --> 01:17:38,650
coming up samples

1008
01:17:39,300 --> 01:17:40,390
posterior distribution over

1009
01:17:41,550 --> 01:17:41,790
all right

1010
01:17:43,500 --> 01:17:44,620
from two is

1011
01:17:45,030 --> 01:17:49,230
we would like to estimate the expectation of functions on the this distribution

1012
01:17:49,790 --> 01:17:52,020
what i mean let's give you an example

1013
01:17:53,080 --> 01:17:55,810
let's imagine that we are very interested in knowing

1014
01:17:57,250 --> 01:17:58,720
two particular characters

1015
01:17:59,150 --> 01:18:01,740
were written by the same scribe or not

1016
01:18:02,290 --> 01:18:03,190
is it can result

1017
01:18:03,730 --> 01:18:06,250
can you at the beginning something

1018
01:18:08,290 --> 01:18:10,030
so what would mean you want to know

1019
01:18:10,740 --> 01:18:12,390
well these are the same right

1020
01:18:13,160 --> 01:18:14,270
i mean you want to know

1021
01:18:15,650 --> 01:18:17,260
good to data points

1022
01:18:20,540 --> 01:18:21,550
and then use

1023
01:18:24,190 --> 01:18:27,140
all the seas equal to each other or not

1024
01:18:28,400 --> 01:18:29,190
when you answers

1025
01:18:29,300 --> 01:18:31,670
i don't know the name the data we can infer

1026
01:18:32,060 --> 01:18:33,670
probabilities and they are the same

1027
01:18:34,560 --> 01:18:37,390
so what want to do is take the posterior probability

1028
01:18:39,350 --> 01:18:40,240
all everything

1029
01:18:41,410 --> 01:18:43,100
it is this is given the data

1030
01:18:44,720 --> 01:18:46,400
and we want to know the average

1031
01:18:49,390 --> 01:18:50,740
averaging over theta

1032
01:18:51,510 --> 01:18:52,260
summing over

1033
01:18:53,920 --> 01:18:56,180
all possible theories about simon

1034
01:18:56,590 --> 01:18:57,100
we want to know

1035
01:18:58,210 --> 01:18:59,430
how probabilities

1036
01:18:59,920 --> 01:19:03,700
that's this proposition is true or one function that says

1037
01:19:04,250 --> 01:19:07,720
this is true is one one is that it is true

1038
01:19:08,590 --> 01:19:12,600
and we want to know the average and the posterior distribution this function which is

1039
01:19:12,660 --> 01:19:15,560
one when the two sees recall this zero otherwise

1040
01:19:16,290 --> 01:19:17,730
if we can compute this

1041
01:19:18,450 --> 01:19:19,490
expectation here

1042
01:19:20,190 --> 01:19:24,290
then we answer to the question what's the probability that those two letters written

1043
01:19:24,480 --> 01:19:25,190
six right

1044
01:19:26,840 --> 01:19:31,630
so what i said i'm saying that question problems that these to see the same

1045
01:19:32,110 --> 01:19:34,160
can be written as an expectation

1046
01:19:37,570 --> 01:19:39,940
another thing you might want to know if you're doing clustering

1047
01:19:42,630 --> 01:19:43,360
you might want to know

1048
01:19:45,480 --> 01:19:47,970
how probable is it that one of the means

1049
01:19:48,430 --> 01:19:52,010
it is somewhere in this book is is in this box

1050
01:19:52,010 --> 01:19:55,580
what's the role of the feature f

1051
01:19:55,600 --> 01:19:58,560
is there some feature

1052
01:19:58,570 --> 01:20:03,070
that the physical is cannot explain and so we need to appeal to something extra

1053
01:20:03,070 --> 01:20:05,860
physical to explain it

1054
01:20:07,090 --> 01:20:11,130
the physical this can only do a rotten job of explaining and if we were

1055
01:20:11,130 --> 01:20:14,180
to repeat like a demon theory did and we have more appeal to

1056
01:20:14,610 --> 01:20:19,350
something non-physical we do a better job of explaining

1057
01:20:19,370 --> 01:20:23,400
if we could find the right path

1058
01:20:23,410 --> 01:20:27,650
and make out the argument the physical is can't explain it or does a better

1059
01:20:27,660 --> 01:20:31,600
job of explaining it and the dual is does a better job of explaining we

1060
01:20:31,600 --> 01:20:34,660
have reason to believe in the soul

1061
01:20:35,540 --> 01:20:39,860
all arguments in philosophy it would be a tentative

1062
01:20:39,880 --> 01:20:41,990
argument is the ability we

1063
01:20:42,000 --> 01:20:46,290
we have some reason to believe the soul until we sort of see what next

1064
01:20:46,290 --> 01:20:49,810
argument comes down the road but at least give us some reason to believe in

1065
01:20:49,810 --> 01:20:53,400
the cell

1066
01:20:53,420 --> 01:21:00,180
so what i want to do is ask what might feature f b

1067
01:21:00,190 --> 01:21:02,790
is there any such feature f and

1068
01:21:02,800 --> 01:21:07,580
it's probably also worth underlining the fact that what i'm really doing is running through

1069
01:21:07,600 --> 01:21:13,980
a series of arguments inference to the best explanation is not a single

1070
01:21:13,980 --> 01:21:15,780
the argument for the soul

1071
01:21:15,820 --> 01:21:19,830
it's rather the name for this kind of argument

1072
01:21:19,850 --> 01:21:24,630
and depending on what have you fill in the blank with no one

1073
01:21:24,640 --> 01:21:29,430
pet feature that you're trying to explain by appeal to the so you get a

1074
01:21:29,430 --> 01:21:35,080
different argument so let's ask ourselves are the things that we need to appeal to

1075
01:21:35,080 --> 01:21:36,320
the pole

1076
01:21:36,320 --> 01:21:38,550
in order to to in order to

1077
01:21:39,870 --> 01:21:44,290
these things about as well as the first try

1078
01:21:44,300 --> 01:21:50,010
actually let me start by saying i'm going to distinguish two broad families of characteristics

1079
01:21:50,010 --> 01:21:52,020
we might appeal to the we might say

1080
01:21:52,160 --> 01:21:55,750
one set of approaches

1081
01:21:55,750 --> 01:22:00,760
focus on ordinary familiar every day

1082
01:22:00,770 --> 01:22:02,770
facts about us

1083
01:22:02,780 --> 01:22:07,260
that we love the fact that we think the fact that you know experience emotions

1084
01:22:07,260 --> 01:22:09,080
what have you

1085
01:22:09,130 --> 01:22:13,650
it is an ordinary features of us and then let's start with those and then

1086
01:22:13,650 --> 01:22:15,910
i'll turn eventually two

1087
01:22:15,930 --> 01:22:20,330
another set of possible things that might need explaining which you might think of as

1088
01:22:20,360 --> 01:22:22,160
extra ordinary

1089
01:22:22,210 --> 01:22:27,670
supernatural things you maybe there are certain supernatural things about communication from the from the

1090
01:22:27,670 --> 01:22:28,990
dead or

1091
01:22:29,000 --> 01:22:33,280
near-death experiences that need to be explained in terms of the soul so so we'll

1092
01:22:33,280 --> 01:22:35,530
get to those will start with

1093
01:22:35,580 --> 01:22:37,120
so the ordinary

1094
01:22:37,130 --> 01:22:38,090
every day

1095
01:22:38,110 --> 01:22:43,000
humdrum facts about even though the ordinary familiar x still could turn out that we

1096
01:22:43,000 --> 01:22:48,030
need to appeal to cells in order to explain the so to start

1097
01:22:48,090 --> 01:22:50,360
how about this

1098
01:22:50,410 --> 01:22:54,060
start with the familiar factors have already drawn your attention to a a couple of

1099
01:22:54,060 --> 01:22:57,440
times that you know you can have a body

1100
01:22:57,460 --> 01:23:01,050
that day had never corpse

1101
01:23:01,100 --> 01:23:04,830
and that's clearly not the person

1102
01:23:04,830 --> 01:23:07,590
living beings out person

1103
01:23:07,640 --> 01:23:10,520
it doesn't do anything it just live there

1104
01:23:10,520 --> 01:23:13,870
whereas your body my body

1105
01:23:13,900 --> 01:23:19,430
it is is animated right i move my hands around my mouth is going up

1106
01:23:19,430 --> 01:23:24,660
and down it walks from one part of the stage to the other parties stage

1107
01:23:24,710 --> 01:23:26,810
maybe we need

1108
01:23:26,820 --> 01:23:29,600
there are to appeal to the soul

1109
01:23:29,630 --> 01:23:32,150
in order to explain sort of one

1110
01:23:32,160 --> 01:23:33,810
and animates

1111
01:23:33,810 --> 01:23:35,620
the body

1112
01:23:35,650 --> 01:23:37,010
and thought would be

1113
01:23:37,080 --> 01:23:41,560
know when the soul and the body had been separated sets the dualistic planes the

1114
01:23:41,570 --> 01:23:44,540
soul has lost its ability to

1115
01:23:44,810 --> 01:23:47,340
give commands to the body

1116
01:23:47,370 --> 01:23:52,060
and so the body no longer animated so got a possible explanation of the difference

1117
01:23:52,060 --> 01:23:57,080
between an animated and and on animated or inanimate body

1118
01:23:57,090 --> 01:24:01,730
two it is the sole contact of the right sort with the body there is

1119
01:24:01,730 --> 01:24:03,930
a possible explanation

1120
01:24:03,940 --> 01:24:07,900
you might say look the physical is can't tell us that because all the physical

1121
01:24:07,900 --> 01:24:10,680
parts are still there when you've got the corpse

1122
01:24:10,730 --> 01:24:13,330
o if it's a fresh corpse

1123
01:24:13,350 --> 01:24:17,100
before the decay has set in

1124
01:24:18,300 --> 01:24:21,500
we need to appeal to the existence of the soul in order to explain the

1125
01:24:21,500 --> 01:24:22,880
animation of

1126
01:24:22,940 --> 01:24:26,190
bodies like the ones that you and i have

1127
01:24:28,220 --> 01:24:32,840
i said i'm going to run through a series of arguments

1128
01:24:32,860 --> 01:24:34,530
but that doesn't mean

1129
01:24:34,570 --> 01:24:42,730
doesn't mean that that lies just another and what does it mean that the

1130
01:24:42,750 --> 01:24:45,790
but rather it is that it doesn't mean that i think the arguments all work

1131
01:24:45,900 --> 01:24:50,380
now i know in the first class then i don't myself believe in the existence

1132
01:24:50,380 --> 01:24:51,750
of us all

1133
01:24:51,770 --> 01:24:54,760
and as such it shouldn't be any surprise you that what i'm going to do

1134
01:24:54,760 --> 01:24:57,410
and so it is zero

1135
01:24:57,670 --> 01:24:58,630
trouble with

1136
01:24:58,800 --> 01:25:03,830
they might even roots the in the first half of the twentieth century in that

1137
01:25:04,340 --> 01:25:06,320
it would very good

1138
01:25:07,320 --> 01:25:09,080
around the same time

1139
01:25:09,830 --> 01:25:12,340
there's hole in the middle here now

1140
01:25:12,410 --> 01:25:15,330
the invisible any

1141
01:25:15,340 --> 01:25:17,090
the issues

1142
01:25:20,150 --> 01:25:33,520
actually i think mean it

1143
01:25:37,080 --> 01:25:41,270
it's the one

1144
01:25:41,280 --> 01:25:48,100
this is an extremely who did

1145
01:25:48,110 --> 01:25:49,890
o four months

1146
01:25:50,920 --> 01:25:52,780
i mean that's a

1147
01:25:52,800 --> 01:25:55,240
so i think there was kind of

1148
01:25:55,340 --> 01:25:58,430
try to find a black and white

1149
01:25:58,840 --> 01:26:07,030
so there are a world of any not something that really BCE until you can

1150
01:26:07,080 --> 01:26:11,720
do tend to the other

1151
01:26:14,090 --> 01:26:15,710
during this is the

1152
01:26:15,720 --> 01:26:18,860
friendship relations among thirty four members of the university

1153
01:26:18,890 --> 01:26:20,990
right so

1154
01:26:21,010 --> 01:26:25,940
several times as slow

1155
01:26:28,150 --> 01:26:29,460
how to get into this

1156
01:26:31,190 --> 01:26:32,330
is very

1157
01:26:32,360 --> 01:26:34,920
social and technological networks as you

1158
01:26:34,960 --> 01:26:41,780
to information systems with intrinsic structure right to see that things like facebook and myspace

1159
01:26:44,870 --> 01:26:50,160
well i was not designed to use the media and the intrinsic of structure but

1160
01:26:50,570 --> 01:26:52,240
the point

1161
01:26:52,250 --> 01:26:55,520
this is the current context is there

1162
01:26:55,530 --> 01:27:00,510
well think the social networks as facebook or something that appears and creator of past

1163
01:27:00,510 --> 01:27:05,400
few years i see some water pre-tax things

1164
01:27:05,400 --> 01:27:08,650
for a long time and so what

1165
01:27:08,710 --> 01:27:13,730
these different fields learn from each of the convergence here is what can appears science

1166
01:27:13,730 --> 01:27:14,760
and social sciences

1167
01:27:14,810 --> 01:27:15,770
from each other

1168
01:27:17,450 --> 01:27:19,150
one of the learning sciences

1169
01:27:20,030 --> 01:27:21,360
the measurement social networks

1170
01:27:21,400 --> 01:27:24,940
mining social networks really

1171
01:27:24,960 --> 01:27:26,930
this is

1172
01:27:26,940 --> 01:27:28,520
so we get a

1173
01:27:28,540 --> 01:27:30,360
examples here but

1174
01:27:30,440 --> 01:27:32,830
two years later i was in

1175
01:27:33,190 --> 01:27:35,460
thirty one network

1176
01:27:35,520 --> 01:27:40,430
french citizens with his phd work

1177
01:27:40,450 --> 01:27:45,790
temple university in new york nineteen seventies and the idea was simply take a

1178
01:27:45,800 --> 01:27:48,980
so you enter politics take a very

1179
01:27:49,000 --> 01:27:56,150
constraints is very close to one membership and limited kinds of social ties

1180
01:27:56,160 --> 01:27:57,300
prior his

1181
01:27:57,820 --> 01:28:01,800
it's time for him and

1182
01:28:01,820 --> 01:28:03,070
to develop

1183
01:28:03,250 --> 01:28:06,860
one snapshot and whether he

1184
01:28:06,880 --> 01:28:09,070
planned in this way it is

1185
01:28:09,080 --> 01:28:11,690
that's the science division chance favors

1186
01:28:11,700 --> 01:28:12,620
here my

1187
01:28:13,210 --> 01:28:15,480
course these two years he was looking at a

1188
01:28:15,490 --> 01:28:18,930
five club actually split

1189
01:28:18,950 --> 01:28:21,830
nine hundred thirty four

1190
01:28:21,900 --> 01:28:23,790
she remembers here

1191
01:28:23,800 --> 01:28:25,790
why does

1192
01:28:25,850 --> 01:28:31,520
the founder of the students at the university went unnamed the story

1193
01:28:31,750 --> 01:28:38,680
number one here right or not there was constructed by the criteria very charismatic figure

1194
01:28:38,940 --> 01:28:45,390
he started throwing things to him away from the original core principles close and focus

1195
01:28:45,430 --> 01:28:46,530
was right

1196
01:28:46,540 --> 01:28:48,530
founder versus strong

1197
01:28:48,540 --> 01:28:53,240
ten five social ties between this we can and found

1198
01:28:53,350 --> 01:28:55,750
of taking up first with

1199
01:28:55,790 --> 01:28:57,360
they actually splits

1200
01:28:58,640 --> 01:29:01,210
tactile sensors thing really

1201
01:29:01,260 --> 01:29:02,860
well founded

1202
01:29:04,410 --> 01:29:08,270
the structure of the other way so tied the middle and in fact

1203
01:29:09,870 --> 01:29:11,200
politics he went into the

1204
01:29:11,210 --> 01:29:13,780
the literature on

1205
01:29:15,020 --> 01:29:20,410
algorithms and time was almost no reasonable all

1206
01:29:20,580 --> 01:29:23,020
so that you is going to be able to explain

1207
01:29:23,110 --> 01:29:25,480
minimum cut in social network mainly

1208
01:29:25,520 --> 01:29:27,910
we're told the fewest number social

1209
01:29:27,930 --> 01:29:29,070
this was

1210
01:29:29,100 --> 01:29:31,030
the way the patterns were

1211
01:29:31,370 --> 01:29:34,270
september nine years

1212
01:29:34,300 --> 01:29:36,360
which user apologize

1213
01:29:37,390 --> 01:29:41,880
three weeks away from for your first black belt in karate and could only to

1214
01:29:41,880 --> 01:29:45,100
the instructor was forced to return to

1215
01:29:45,120 --> 01:29:48,950
well i don't know what is really a

1216
01:29:49,060 --> 01:29:50,260
something that we were

1217
01:29:50,270 --> 01:29:52,040
we have to think

1218
01:29:52,180 --> 01:29:57,630
mining social network data from the nineteen seventies it took him three years after collect

1219
01:29:57,700 --> 01:30:00,190
this thirty four network and one

1220
01:30:02,080 --> 01:30:03,380
what do we bring to the table

1221
01:30:03,390 --> 01:30:08,420
if if if if all this is one of course is a matter of scale

1222
01:30:08,430 --> 01:30:12,370
right so it's not just a small things like club

1223
01:30:13,170 --> 01:30:19,630
many many others to i try to go and how it actually here think about

1224
01:30:19,630 --> 01:30:22,020
some of the social network to use computers

1225
01:30:22,380 --> 01:30:24,530
this is a very simple form

1226
01:30:24,530 --> 01:30:25,680
in the network

1227
01:30:25,700 --> 01:30:28,170
if you have mountain

1228
01:30:30,520 --> 01:30:33,830
what makes it harder to two thousand three

1229
01:30:33,890 --> 01:30:41,630
the last temperature on computers show that for now

1230
01:30:41,650 --> 01:30:42,770
network who

1231
01:30:43,140 --> 01:30:44,150
you know two

1232
01:30:44,170 --> 01:30:46,540
here's the first doing business

1233
01:30:46,550 --> 01:30:48,850
what the in two thousand six

1234
01:30:48,860 --> 01:30:52,890
and it's going to get into the realm of making the invisible visible right social

1235
01:30:53,530 --> 01:30:58,640
analysis of the history of exposing things that are all around us is

1236
01:30:58,740 --> 01:31:05,260
scale really minor imagined twenty years the able watch in time female to

1237
01:31:05,270 --> 01:31:08,030
in fact hundreds

1238
01:31:08,660 --> 01:31:11,870
four million people one would like want

1239
01:31:12,010 --> 01:31:13,790
this will be something like this

1240
01:31:13,810 --> 01:31:20,520
so much like my space in the past few months just to report by the

1241
01:31:20,520 --> 01:31:24,450
no no i was about to say and now for something completely different

1242
01:31:24,520 --> 01:31:29,760
let me start by saying that it's an honor to be here

1243
01:31:29,780 --> 01:31:31,790
in front of such august

1244
01:31:31,810 --> 01:31:34,310
company and such a lovely setting

1245
01:31:34,330 --> 01:31:40,520
thank you all for sticking it out so late in the day

1246
01:31:40,540 --> 01:31:46,400
so my talk is going to have two parts the first part is

1247
01:31:46,520 --> 01:31:49,020
generally historical

1248
01:31:49,060 --> 01:31:54,880
because a couple of years ago during the kernel centennial i was invited to

1249
01:31:54,910 --> 01:32:00,240
give a talk on girls which can be a little bit worried at the time

1250
01:32:00,380 --> 01:32:02,810
one thing i'm not a historian

1251
01:32:02,840 --> 01:32:08,500
so don't claim to have any great historical insight but even more worrying to me

1252
01:32:08,500 --> 01:32:14,070
was the fact that the time i didn't feel any great affinity to to good

1253
01:32:14,070 --> 01:32:18,700
so i was raised in the whole proof theoretic tradition

1254
01:32:18,740 --> 01:32:24,290
of of working in starting formal axiomatic systems and even though as well as i'll

1255
01:32:24,290 --> 01:32:30,150
explain kernel himself work from within that tradition i always felt tension in him that

1256
01:32:30,400 --> 01:32:31,320
made me

1257
01:32:31,320 --> 01:32:32,990
a little bit easy

1258
01:32:33,040 --> 01:32:36,570
and so what decided to do at the time was you know read and girls

1259
01:32:36,570 --> 01:32:38,570
work in writing and and

1260
01:32:38,670 --> 01:32:43,620
remarks he made to how long and and and and work through it and so

1261
01:32:43,620 --> 01:32:48,040
actually the first part is going to be a somewhat personal

1262
01:32:48,060 --> 01:32:52,540
a view of what was going on that tension

1263
01:32:52,920 --> 01:32:57,430
and also to explain you know what i take girls ambivalence the metamathematical tradition to

1264
01:32:58,340 --> 01:33:02,150
and then what i'd like to do is discuss some things are going on in

1265
01:33:02,150 --> 01:33:06,680
contemporary work and metamathematics proof theory that i find particularly interesting

1266
01:33:06,920 --> 01:33:11,010
like to discuss those topics in light of course concerns

1267
01:33:11,020 --> 01:33:15,040
OK so i would like to start though

1268
01:33:15,120 --> 01:33:19,760
before tell you about a girl my talking about health risks

1269
01:33:19,790 --> 01:33:26,900
so even though the mature formulation of public program didn't appear

1270
01:33:26,900 --> 01:33:31,900
until nineteen twenty two and nineteen twenty three he interested logic and foundations

1271
01:33:32,340 --> 01:33:33,780
began much earlier

1272
01:33:33,800 --> 01:33:39,460
so his eighteen ninety nine belong in the geometry given the axiomatic treatment of geometry

1273
01:33:39,460 --> 01:33:42,400
that was revolutionary

1274
01:33:42,460 --> 01:33:48,020
the nineteen hundred paper there gives the maximisation of the real numbers of his famous

1275
01:33:48,020 --> 01:33:56,120
list of twenty three problems presented at the international congress in mathematics in nineteen hundred

1276
01:33:56,400 --> 01:34:01,530
three of them had a distinctly foundational character having to do with countries continuum problem

1277
01:34:01,530 --> 01:34:07,220
the consistency of arithmetic and mathematical treatment of the axioms of physics

1278
01:34:07,280 --> 01:34:14,530
then in nineteen four he gave an early attempt to and flawed attempt to treat

1279
01:34:14,530 --> 01:34:18,000
the consistency problem in syntactic terms

1280
01:34:18,020 --> 01:34:23,680
and then after nineteen o four largely left the topic until nineteen twenty two

1281
01:34:23,710 --> 01:34:30,280
say for a lecture on axiomatic semantics is thinking in nineteen eighteen but his lecture

1282
01:34:30,280 --> 01:34:34,930
notes from the time show that he was actively engaged in in these issues in

1283
01:34:34,930 --> 01:34:38,030
much in the from much of the intervening period

1284
01:34:38,120 --> 01:34:41,660
for example lecturing them annually

1285
01:34:41,680 --> 01:34:47,740
so by nineteen twenty two the legal like constraints resulting from was intuitionistic challenge was

1286
01:34:47,740 --> 01:34:49,970
gathering steam and that's when hulbert

1287
01:34:50,520 --> 01:34:54,990
presented his program to secure the foundations of mathematics

1288
01:34:55,060 --> 01:34:57,620
the general features of his program are

1289
01:34:57,810 --> 01:34:59,110
now well known

1290
01:34:59,210 --> 01:35:03,490
the idea was that you were supposed to characterize the methods of modern mathematics so

1291
01:35:03,490 --> 01:35:06,110
including nonconstructive infinitary

1292
01:35:06,180 --> 01:35:11,930
reasoning is to characterize those methods and formal axiomatic systems and then prove the systems

1293
01:35:11,930 --> 01:35:17,110
consistent using secure safe finitary methods

1294
01:35:17,120 --> 01:35:21,930
now this program is often taken to presuppose the position whereby mathematics is taken to

1295
01:35:21,930 --> 01:35:27,270
be nothing more than a game of symbols with no meaning beyond that is that

1296
01:35:27,280 --> 01:35:32,390
beyond that given by the prescribed rules and so you can find such characterizations of

1297
01:35:32,390 --> 01:35:36,920
formalism for example in prowess inaugural address the university of amsterdam as early as nineteen

1298
01:35:37,860 --> 01:35:43,780
and it also ramses and mathematical logic of nineteen twenty six

1299
01:35:43,790 --> 01:35:49,330
and it's true that will help but is is emphasizing six the syntactic nature of

1300
01:35:49,330 --> 01:35:53,650
his program his language often does suggest such

1301
01:35:54,520 --> 01:35:57,960
but i think it's it's silly to take this

1302
01:35:58,010 --> 01:36:02,780
this to characterize the mathematics more generally any of course but we respect for mathematics

1303
01:36:02,780 --> 01:36:06,590
and he's the last person thing to to hold the view that mathematics is meaningless

1304
01:36:06,890 --> 01:36:08,380
symbol game

1305
01:36:08,460 --> 01:36:09,910
and when you

1306
01:36:09,940 --> 01:36:12,000
look past the rhetoric

1307
01:36:12,010 --> 01:36:18,590
and put his remarks developing context we are left with are just two simple

1308
01:36:19,850 --> 01:36:26,040
the first is that with syntactic characterization of infantry mathematical reasoning in hand the question

1309
01:36:26,040 --> 01:36:29,520
of consistency becomes a purely mathematical questions

1310
01:36:29,530 --> 01:36:31,170
this is the first observation

1311
01:36:31,340 --> 01:36:35,530
and second of all the consistency proof using restricted trusted body mass this would provide

1312
01:36:35,530 --> 01:36:40,450
solid re-assurance to anyone concerned that infantry methods might be unsound

1313
01:36:40,460 --> 01:36:41,950
and so i take the

1314
01:36:41,960 --> 01:36:48,590
the core methodological presuppositions of program to be embodied in the following simple outline so

1315
01:36:48,610 --> 01:36:52,840
first of all there is the claim the formal axiomatic systems provide faithful representations of

1316
01:36:52,840 --> 01:36:54,390
mathematical argumentation

1317
01:36:54,450 --> 01:36:58,410
and in the sense that once you have a mathematical proof if you can you

1318
01:36:59,010 --> 01:37:03,350
develop a formal representation of it

1319
01:37:03,360 --> 01:37:08,190
then with this representation and had at least some foundational not an epistemological questions can

1320
01:37:08,190 --> 01:37:12,890
be formulated in mathematical terms it's specifically was was

1321
01:37:12,940 --> 01:37:14,330
at least

1322
01:37:14,340 --> 01:37:15,820
most sales

1323
01:37:15,830 --> 01:37:18,130
concerned with the question of consistency

1324
01:37:18,140 --> 01:37:23,510
and then finally a finitary syntactic perspective makes it possible to address such questions without

1325
01:37:23,510 --> 01:37:28,220
presupposing substantial portions of the body mathematics under investigation by so if you're trying to

1326
01:37:28,220 --> 01:37:35,140
reason about the trying to understand methods reasoning about the infinite many observations here you

1327
01:37:35,140 --> 01:37:39,690
can you can study our methods of reasoning about the infinite without presupposing anything about

1328
01:37:39,690 --> 01:37:41,960
the infinite at all

1329
01:37:41,960 --> 01:37:42,800
so you see

1330
01:37:42,850 --> 01:37:48,090
from fifteen centimeters really goes to one million

1331
01:37:48,140 --> 01:37:52,320
here is our five hundred kv electrons

1332
01:37:52,340 --> 01:37:54,050
and notice that i

1333
01:37:54,110 --> 01:37:55,480
the the

1334
01:37:55,520 --> 01:37:57,510
calculation correctly

1335
01:37:57,560 --> 01:38:01,750
this is relativistic the corrected now you get two point six times ten thirty eight

1336
01:38:01,770 --> 01:38:03,260
meters per second

1337
01:38:03,290 --> 01:38:06,570
by applying the formalism that you see there

1338
01:38:06,590 --> 01:38:08,260
i will leave this

1339
01:38:08,270 --> 01:38:10,120
throughout this lecture

1340
01:38:10,150 --> 01:38:12,030
because i will return to this

1341
01:38:12,100 --> 01:38:16,570
several times

1342
01:38:16,630 --> 01:38:18,740
i want to sell you a

1343
01:38:18,750 --> 01:38:21,860
acute demonstration

1344
01:38:21,900 --> 01:38:25,410
i have an electron gun here

1345
01:38:25,470 --> 01:38:28,610
an electron gun

1346
01:38:28,850 --> 01:38:31,070
comes like so

1347
01:38:31,130 --> 01:38:33,490
this is the velocity of the electrons

1348
01:38:33,500 --> 01:38:36,440
i wouldn't mind signed to remind you

1349
01:38:36,460 --> 01:38:39,620
they are electrons are the electrons going this direction

1350
01:38:39,630 --> 01:38:41,980
currently in that direction

1351
01:38:41,990 --> 01:38:44,810
and so if now i have the magnetic field

1352
01:38:46,440 --> 01:38:51,060
let's assume the magnetic field is in the blackboards

1353
01:38:51,120 --> 01:38:53,480
this is b

1354
01:38:54,300 --> 01:38:57,560
i crosby's direction of the force

1355
01:38:57,570 --> 01:38:59,490
i think these are actually

1356
01:38:59,500 --> 01:39:01,400
using the blackboard

1357
01:39:01,490 --> 01:39:03,280
if i'm not mistaken

1358
01:39:03,290 --> 01:39:05,730
i think the forces in this direction

1359
01:39:05,770 --> 01:39:07,150
and so

1360
01:39:07,170 --> 01:39:08,070
you'll see

1361
01:39:08,090 --> 01:39:10,760
but it starts to bend this direction

1362
01:39:10,800 --> 01:39:11,840
if you change

1363
01:39:11,850 --> 01:39:14,260
the direction of the

1364
01:39:14,360 --> 01:39:16,030
magnetic field

1365
01:39:16,040 --> 01:39:18,340
if you just coming out of the blackboard

1366
01:39:18,380 --> 01:39:19,470
and the electron

1367
01:39:20,250 --> 01:39:22,800
this i can show you that you

1368
01:39:22,810 --> 01:39:28,650
it is not too different from the distortion experiment i did when i had the

1369
01:39:28,660 --> 01:39:33,240
television program there and and i had this strong magnet and we distorted the image

1370
01:39:33,250 --> 01:39:34,970
but this of course is a little bit

1371
01:39:37,770 --> 01:39:42,390
so i'm going to see the image there

1372
01:39:42,400 --> 01:39:43,930
and we

1373
01:39:43,940 --> 01:39:49,570
i want to make it quite dark in the room

1374
01:39:51,660 --> 01:39:54,400
two and on the

1375
01:39:54,470 --> 01:39:55,800
electron gun

1376
01:39:55,810 --> 01:39:59,640
using the electron gun it strikes a fluorescent screen

1377
01:39:59,640 --> 01:40:01,810
and that's how you can see it

1378
01:40:01,870 --> 01:40:05,880
and i have here bar magnet if i hold the bar magnet behind it

1379
01:40:05,920 --> 01:40:08,010
and i can create more or less

1380
01:40:08,120 --> 01:40:10,840
situations like this i can flip over the magnet

1381
01:40:10,850 --> 01:40:12,100
and then

1382
01:40:12,140 --> 01:40:16,510
the direction of bending should change so the outcome was the magnet

1383
01:40:16,560 --> 01:40:17,650
you see

1384
01:40:17,690 --> 01:40:19,670
curve that electrons

1385
01:40:19,730 --> 01:40:21,350
i turn the magnet over

1386
01:40:21,480 --> 01:40:24,170
and i come in again and the curve

1387
01:40:24,180 --> 01:40:25,490
very straightforward

1388
01:40:25,500 --> 01:40:36,160
very simple

1389
01:40:36,520 --> 01:40:47,780
there is a fantastic

1390
01:40:47,790 --> 01:40:49,610
the way in

1391
01:40:50,970 --> 01:40:53,210
that we can separate

1392
01:40:54,470 --> 01:40:57,640
one and the same element

1393
01:40:57,650 --> 01:40:59,140
if we for instance

1394
01:41:01,800 --> 01:41:03,510
and uranium

1395
01:41:03,520 --> 01:41:05,630
when you find it

1396
01:41:05,670 --> 01:41:06,940
two ninety nine

1397
01:41:06,960 --> 01:41:09,020
o point three percent

1398
01:41:09,030 --> 01:41:11,860
uranium two thirty eight

1399
01:41:11,870 --> 01:41:16,290
that means it has ninety two protons otherwise it wouldn't be iranian

1400
01:41:16,310 --> 01:41:18,980
and it has a hundred and forty six new pond

1401
01:41:18,990 --> 01:41:20,390
nine nine

1402
01:41:20,430 --> 01:41:23,850
o point three percent

1403
01:41:23,890 --> 01:41:25,780
o point seven percent

1404
01:41:25,900 --> 01:41:28,270
uranium two thirty five

1405
01:41:28,350 --> 01:41:31,340
again ninety two protons otherwise it wouldn't be

1406
01:41:32,350 --> 01:41:35,140
but only a hundred and forty three neutrons

1407
01:41:35,140 --> 01:41:37,760
and that you'll find in nature

1408
01:41:37,790 --> 01:41:41,240
o point seven percent you got chemist

1409
01:41:41,260 --> 01:41:45,110
and you get the cameras the uranium and you say would you please separate

1410
01:41:45,130 --> 01:41:47,190
these two isotopes for me

1411
01:41:47,270 --> 01:41:49,220
and he of course would laugh at you

1412
01:41:49,270 --> 01:41:53,410
and he would say go fly a kite because the chemical properties are exactly the

1413
01:41:53,410 --> 01:41:54,890
same for the two

1414
01:41:54,890 --> 01:42:01,370
because uranium as uranium neutral urania ninety two electrons and neutral during the nineteen two

1415
01:42:01,370 --> 01:42:05,980
electoral but there's no way that they could separate out

1416
01:42:06,060 --> 01:42:07,820
i'll show you know

1417
01:42:07,840 --> 01:42:12,840
how they can be separated which is what we call a mass spectrometer

1418
01:42:12,850 --> 01:42:14,870
during the uranium

1419
01:42:14,880 --> 01:42:19,120
sort of that ionizers let's assume is ionized one so it loses one electron so

1420
01:42:19,120 --> 01:42:20,620
it's positively

1421
01:42:20,660 --> 01:42:21,890
charge with one

1422
01:42:21,900 --> 01:42:26,370
unit charge one of those charges that you see here

1423
01:42:28,510 --> 01:42:32,190
we now accelerated over certain potential difference o

1424
01:42:32,210 --> 01:42:36,690
these uranium atoms to thirty five to thirty eight get certain

1425
01:42:36,740 --> 01:42:39,140
speed in the coming here

1426
01:42:39,160 --> 01:42:40,640
with the speed

1427
01:42:43,850 --> 01:42:45,660
so the positively charged

1428
01:42:45,710 --> 01:42:48,340
and let's assume that we have a magnetic field

1429
01:42:48,390 --> 01:42:50,130
that is uniform

1430
01:42:50,150 --> 01:42:52,000
and that is in this direction

1431
01:42:52,010 --> 01:42:54,300
comes out of the blackboard

1432
01:42:54,360 --> 01:42:56,580
so what will happen is that the

1433
01:42:56,620 --> 01:43:00,880
charged particles which are positively charged now

1434
01:43:00,940 --> 01:43:02,590
one unit charge

1435
01:43:02,610 --> 01:43:06,580
i going to go around the circle

1436
01:43:06,630 --> 01:43:10,930
and hit you

1437
01:43:11,040 --> 01:43:13,000
the radius

1438
01:43:13,010 --> 01:43:15,860
but if you look at these equations

1439
01:43:15,910 --> 01:43:19,270
so you will see that the rate is proportional was the square root of the

1440
01:43:19,270 --> 01:43:21,120
mass of the particles

1441
01:43:21,130 --> 01:43:25,510
and the mass of two thirty eight is one point two percent larger

1442
01:43:25,560 --> 01:43:28,790
then the mass of two thirty five

1443
01:43:28,840 --> 01:43:30,500
so with one point

1444
01:43:30,500 --> 01:43:33,860
two percent larger since we have the square there

1445
01:43:33,880 --> 01:43:36,970
he describe it

1446
01:43:36,970 --> 01:43:41,490
we accelerate them over the same potential difference and this one doesn't change

1447
01:43:41,500 --> 01:43:43,320
this is the only thing that changes

1448
01:43:43,320 --> 01:43:47,610
but it may be sufficient for the task of language identification

1449
01:43:47,650 --> 01:43:48,910
OK so

1450
01:43:48,930 --> 01:43:50,910
these are parameters

1451
01:43:50,930 --> 01:43:54,050
these are the values of the parameters that i was going to change the notation

1452
01:43:54,050 --> 01:43:59,150
one more time i just want to emphasise here these are just numbers here

1453
01:43:59,160 --> 01:44:02,200
right we don't have to call the probabilities at all of this point

1454
01:44:03,030 --> 01:44:04,340
we're defining

1455
01:44:04,360 --> 01:44:10,320
p of p of a string here forces as being the product of these numbers

1456
01:44:10,320 --> 01:44:14,010
i think i picked to stanford try grammar maybe for table

1457
01:44:14,010 --> 01:44:17,700
because you look these up in a hash table for example or re

1458
01:44:17,720 --> 01:44:18,990
so if you want to know

1459
01:44:20,390 --> 01:44:23,260
what the probability is either given or in as

1460
01:44:23,260 --> 01:44:26,630
associated with this program you look

1461
01:44:26,650 --> 01:44:28,360
and here's the number you get

1462
01:44:28,380 --> 01:44:30,700
and how do you get it will get it by

1463
01:44:30,760 --> 01:44:37,910
i mean we're that come from it came from counting things of the problems

1464
01:44:37,950 --> 01:44:40,390
so here's our prior

1465
01:44:40,410 --> 01:44:43,320
given a particular set of values of the parameters

1466
01:44:43,340 --> 01:44:45,050
we get the definition of p

1467
01:44:45,070 --> 01:44:47,930
and what can we do with our definition of a one thing is we can

1468
01:44:47,930 --> 01:44:49,220
roll dies

1469
01:44:49,240 --> 01:44:52,090
so we can say let's generate tags according to this flow

1470
01:44:52,110 --> 01:44:55,410
that's actually a good thing to do when you build them we'll do this happening

1471
01:44:55,570 --> 01:44:59,060
when he went to build a model of language you should see

1472
01:44:59,090 --> 01:45:02,090
what that looks like an end in well in this case we saw that with

1473
01:45:02,240 --> 01:45:06,390
trying to model it started to be pronounced all with the foreground model and started

1474
01:45:06,410 --> 01:45:12,340
have words with seventeen grandma what would i would probably just produced genesis

1475
01:45:12,410 --> 01:45:16,950
because for that particular set of sixteen characters

1476
01:45:16,970 --> 01:45:22,610
there's probably only appeared one time in genesis and there's only one character that follow

1477
01:45:22,610 --> 01:45:28,490
so just reproduce what happened in genesis that point that model has no no bias

1478
01:45:28,490 --> 01:45:33,660
at all it's exactly reproducing genesis but it has very high variance model of english

1479
01:45:33,660 --> 01:45:38,780
because it always gives you know completely dependent on this corpus the estimated from

1480
01:45:38,820 --> 01:45:43,860
if you really wanted to seventeen grand model then you need an astronomical texts

1481
01:45:43,860 --> 01:45:48,780
in order to be able to generate something that looked like english generally and not

1482
01:45:48,780 --> 01:45:51,530
just like the sampled you estimate

1483
01:45:51,550 --> 01:45:54,380
so once you've got he can generate

1484
01:45:54,410 --> 01:45:59,970
and more important for our purposes given that like a sentence you can find out

1485
01:45:59,970 --> 01:46:01,660
what the probability is

1486
01:46:01,680 --> 01:46:03,470
and in language i d

1487
01:46:03,490 --> 01:46:05,760
we say we've got the same model

1488
01:46:05,760 --> 01:46:07,360
it's got the same structure

1489
01:46:07,380 --> 01:46:09,700
we just have different parameters

1490
01:46:09,700 --> 01:46:13,990
we've got our english program numbers and our polish striker numbers

1491
01:46:14,010 --> 01:46:15,510
what comes out from

1492
01:46:15,530 --> 01:46:19,070
is two different

1493
01:46:19,070 --> 01:46:24,150
two different probability functions and then given some data access

1494
01:46:24,160 --> 01:46:27,320
since we can compute p of x in qx

1495
01:46:27,340 --> 01:46:30,610
and decide which one is

1496
01:46:30,630 --> 01:46:32,320
so far so good

1497
01:46:36,780 --> 01:46:40,340
i want to spend a few minutes now just talking about

1498
01:46:41,470 --> 01:46:46,200
just just talking about notation making it a little bit more precise because people sometimes

1499
01:46:46,200 --> 01:46:48,240
get confused about this so

1500
01:46:48,240 --> 01:46:55,280
this axis here is an element of some event space

1501
01:46:55,320 --> 01:46:59,260
for example the space of all races through the space of all sentences if the

1502
01:46:59,260 --> 01:47:01,720
that's the whole text

1503
01:47:01,740 --> 01:47:05,150
so we want to know the probability of the corpus

1504
01:47:05,150 --> 01:47:08,130
well that's the problem believe all the sentences together

1505
01:47:08,160 --> 01:47:12,130
and if we want that it's pretty reasonable

1506
01:47:12,130 --> 01:47:14,700
to take the general

1507
01:47:14,720 --> 01:47:19,450
in fact so if we want the probability the whole text we said let's take

1508
01:47:19,990 --> 01:47:25,220
let's pretend all the sentences are and multiply the probabilities of the individual sentences together

1509
01:47:25,220 --> 01:47:27,950
so is another example

1510
01:47:27,970 --> 01:47:34,050
well i've been repeatedly doing it

1511
01:47:34,070 --> 01:47:36,860
is this actually is this assumption actually

1512
01:47:37,910 --> 01:47:42,220
is it the case that sense to really is independent of sense

1513
01:47:42,300 --> 01:47:43,740
probably not

1514
01:47:43,760 --> 01:47:49,260
if i talk about brain sentence one maybe i'll talk about bread-and-butter instance two

1515
01:47:49,360 --> 01:47:54,740
these things are related sense two is unlikely to have here is you know unless

1516
01:47:54,740 --> 01:47:58,130
i mention the person of the appropriate gender instance one

1517
01:47:58,150 --> 01:48:02,930
or elsewhere in sentences

1518
01:48:02,950 --> 01:48:05,430
but you know this is

1519
01:48:05,430 --> 01:48:07,240
this is not a terrible such

1520
01:48:07,240 --> 01:48:08,970
it's common

1521
01:48:08,990 --> 01:48:11,910
all right so i've said that x is an element of some of the space

1522
01:48:11,910 --> 01:48:14,280
so often have a sequence of letters

1523
01:48:14,300 --> 01:48:17,660
well that's kind of funny because

1524
01:48:17,680 --> 01:48:20,880
this doesn't look like an element of an event is right this looks like the

1525
01:48:21,950 --> 01:48:24,450
of different sets of events

1526
01:48:24,470 --> 01:48:27,890
this is the the set of all sentences whose letters e

1527
01:48:28,130 --> 01:48:34,590
is we're taking intersection of sentences was first letters age with those who secularism with

1528
01:48:34,590 --> 01:48:36,200
those used the letters are

1529
01:48:36,220 --> 01:48:44,740
when we take this intersection we get all sentences whose first six letters spell horses

1530
01:48:45,800 --> 01:48:51,720
what is this notation really mean kind of content that before

1531
01:48:51,740 --> 01:48:54,950
so x two was supposed to be the second letter in the sequence going to

1532
01:48:54,950 --> 01:49:00,650
stick my story here they are space is something like the set of all sentences

1533
01:49:00,660 --> 01:49:02,320
every sentence is the that

1534
01:49:02,390 --> 01:49:05,160
x two is the second level here's another example

1535
01:49:05,200 --> 01:49:08,160
are there is a sequence of three point lines

1536
01:49:08,240 --> 01:49:11,510
but you might write something like the probability that the number of heads is to

1537
01:49:11,510 --> 01:49:16,570
remind previous in one of these ways

1538
01:49:16,570 --> 01:49:18,390
so what we mean by

1539
01:49:18,410 --> 01:49:24,160
over here the events of race but again we're writing credits

1540
01:49:24,200 --> 01:49:27,410
which pick out some subset of the set of races

1541
01:49:27,410 --> 01:49:28,950
so what's going on here

1542
01:49:28,970 --> 01:49:30,160
the answer is that

1543
01:49:30,180 --> 01:49:33,660
variable is really a function of the event as i hinted before

1544
01:49:34,550 --> 01:49:36,700
really the right way of writing this

1545
01:49:36,720 --> 01:49:38,340
it's like this

1546
01:49:38,340 --> 01:49:41,070
event is a sequence of letters

1547
01:49:41,110 --> 01:49:45,590
any event this so any is a sequence x two of that is the second

1548
01:49:45,590 --> 01:49:47,610
element of the sequence

1549
01:49:47,610 --> 01:49:50,650
we traditionally suppressed this part of the notation

1550
01:49:50,660 --> 01:49:54,890
what we have the probability that ages to what we need is the probability that

1551
01:49:54,890 --> 01:49:58,660
we have some of the whose age is two

1552
01:49:58,680 --> 01:50:01,280
in the images like a method call

1553
01:50:01,280 --> 01:50:05,340
which tells you back some property of in this case

1554
01:50:05,360 --> 01:50:09,220
the company has worked in that

1555
01:50:10,700 --> 01:50:14,970
races might be amazingly complicated events they might talk about you know if you know

1556
01:50:14,970 --> 01:50:19,530
the race you might be able to inspect the race and find out who the

1557
01:50:19,530 --> 01:50:24,430
jockey was and what phase the moon was whether the track was money and so

1558
01:50:24,430 --> 01:50:28,470
forth in this case we're just looking at one property of the race namely whether

1559
01:50:28,470 --> 01:50:31,360
the weather

1560
01:50:31,410 --> 01:50:35,930
so that's true some that's it's possible others what they think about the notation is

1561
01:50:35,930 --> 01:50:40,240
that here we have said that the events races but we could get more complicated

1562
01:50:40,240 --> 01:50:43,290
and say that the event was you know some even more complicated state of the

1563
01:50:43,290 --> 01:50:46,260
world and we can still pick fact

1564
01:50:46,290 --> 01:50:47,280
asked whether

1565
01:50:47,330 --> 01:50:50,290
what value of this function on the state of the world so we don't really

1566
01:50:50,290 --> 01:50:53,960
have to commit usually what the that space was we only have to that we

1567
01:50:54,030 --> 01:50:56,910
were working on our models one

1568
01:50:56,920 --> 01:51:02,960
things we have to be able to compute of events

1569
01:51:02,960 --> 01:51:06,260
OK so

1570
01:51:06,280 --> 01:51:09,840
just to do this for a couple of couple of examples we've looked at

1571
01:51:10,280 --> 01:51:14,210
when we ask of probability the number of edges two

1572
01:51:14,210 --> 01:51:15,570
what we need is

1573
01:51:15,580 --> 01:51:18,290
the total probability of all of this

1574
01:51:18,320 --> 01:51:22,180
that have two heads so that the total probability the set

1575
01:51:22,210 --> 01:51:25,930
now we can divide this set up into three pieces right

1576
01:51:25,950 --> 01:51:28,040
because these are disjoint subsets

1577
01:51:28,050 --> 01:51:33,420
so the probability of heads and tails heads tails heads and tails heads heads and

1578
01:51:33,420 --> 01:51:41,210
the total probability those things is

1579
01:51:41,210 --> 01:51:44,660
calculating expectation

1580
01:51:44,690 --> 01:51:46,320
is larger than one

1581
01:51:46,320 --> 01:51:48,640
probably you are very happy

1582
01:51:48,660 --> 01:51:52,330
if you are allowed to play a game

1583
01:51:52,380 --> 01:51:55,680
we do but as

1584
01:51:55,740 --> 01:51:59,290
because the expectation is larger than one then

1585
01:51:59,310 --> 01:52:02,120
you expand that each round

1586
01:52:05,650 --> 01:52:07,700
you may calculate

1587
01:52:07,760 --> 01:52:10,060
the expectation

1588
01:52:10,060 --> 01:52:16,520
all for the price of the asset finance which is the product of the previous

1589
01:52:18,130 --> 01:52:19,390
and you know

1590
01:52:19,400 --> 01:52:20,500
that the

1591
01:52:20,520 --> 01:52:28,400
the expectation of indeed all the product of independent random variables is

1592
01:52:29,720 --> 01:52:32,510
the product of the expectation

1593
01:52:32,560 --> 01:52:33,880
each fact or

1594
01:52:33,890 --> 01:52:40,710
he had expectation five or four but what together we have that feature

1595
01:52:40,740 --> 01:52:46,930
all the price goes exponentially it's fine

1596
01:52:48,580 --> 01:52:51,250
yeah but this is the

1597
01:52:52,330 --> 01:52:55,860
misunderstanding and danger

1598
01:52:55,900 --> 01:52:57,470
what i would like to

1599
01:52:57,490 --> 01:53:01,070
show you

1600
01:53:06,100 --> 01:53:08,260
you may ask

1601
01:53:08,290 --> 01:53:15,150
what are the typical values of the prize of the random prize

1602
01:53:17,120 --> 01:53:19,460
what is the

1603
01:53:20,090 --> 01:53:25,410
as input because of the growth rate

1604
01:53:25,420 --> 01:53:30,180
and and

1605
01:53:30,220 --> 01:53:33,940
for the asymptotics of the growth rate of

1606
01:53:33,990 --> 01:53:38,080
you calculate the average growth rate

1607
01:53:38,120 --> 01:53:42,440
this is the average of the

1608
01:53:42,500 --> 01:53:46,910
and here the average is also

1609
01:53:46,950 --> 01:53:51,250
an average of independent random variables

1610
01:53:51,260 --> 01:53:55,680
the average growth but the expectation

1611
01:53:55,730 --> 01:54:01,440
and if you can only be expectation one half time a lot though

1612
01:54:01,460 --> 01:54:06,610
last one half the log one half is zero

1613
01:54:06,670 --> 01:54:12,320
so if you just keep your money in these artificial park

1614
01:54:12,360 --> 01:54:16,430
then you have their growth rate

1615
01:54:16,490 --> 01:54:25,830
and it is it is shocking if the final guy can

1616
01:54:25,910 --> 01:54:27,770
i understand

1617
01:54:28,640 --> 01:54:30,340
from the people garden

1618
01:54:30,350 --> 01:54:32,200
he learned

1619
01:54:32,220 --> 01:54:39,850
that a random variable is around its expectation

1620
01:54:39,850 --> 01:54:43,020
and it is not the case here

1621
01:54:43,080 --> 01:54:46,290
because the the expectation

1622
01:54:47,320 --> 01:54:50,670
exponentially to infinity

1623
01:54:50,690 --> 01:54:53,290
and the random variable

1624
01:54:53,330 --> 01:54:57,520
it is typically around one

1625
01:54:57,680 --> 01:54:59,210
so it

1626
01:54:59,220 --> 01:55:01,500
it means

1627
01:55:01,510 --> 01:55:05,360
that if i applaud the density

1628
01:55:05,370 --> 01:55:07,090
of as an

1629
01:55:07,100 --> 01:55:11,800
then the the we large probability

1630
01:55:12,940 --> 01:55:15,960
the method used in this range

1631
01:55:15,970 --> 01:55:19,250
the expectation

1632
01:55:19,940 --> 01:55:21,810
is much much larger

1633
01:55:21,880 --> 01:55:24,920
then the typical values of these random variables

1634
01:55:25,110 --> 01:55:28,840
and the reason is that these random variables

1635
01:55:28,850 --> 01:55:31,700
it is the product

1636
01:55:31,760 --> 01:55:34,170
of some random variables

1637
01:55:34,180 --> 01:55:38,160
and the problem theory is on the average

1638
01:55:38,210 --> 01:55:43,810
on the top of random variables and not on the product of random variables if

1639
01:55:44,360 --> 01:55:50,870
you see a problem on the product of random variables then you have to transform

1640
01:55:50,920 --> 01:55:52,840
your problem in full

1641
01:55:52,840 --> 01:55:57,300
the number of random variables in order to

1642
01:55:57,350 --> 01:56:00,920
i have a right picture on the problem

1643
01:56:01,720 --> 01:56:05,660
the fear

1644
01:56:05,830 --> 01:56:12,990
i if you win the one dollar

1645
01:56:13,050 --> 01:56:17,160
the return means that investing one dollar what is the reader

1646
01:56:17,210 --> 01:56:21,540
even that one dollar that either you have to derive

1647
01:56:21,550 --> 01:56:26,750
it probably you one half or half dollar we probably you want

1648
01:56:35,790 --> 01:56:43,610
i don't understand

1649
01:56:48,090 --> 01:56:58,360
you you you've seen at this point that this is a misleading calculation

1650
01:56:58,370 --> 01:56:59,850
this is what you are seeing

1651
01:57:11,960 --> 01:57:18,000
that reaches the final four

1652
01:57:18,120 --> 01:57:31,170
in expectation and i gain

1653
01:57:31,230 --> 01:57:33,690
one over four dollars

1654
01:57:33,730 --> 01:57:36,560
in each round

1655
01:57:36,600 --> 01:57:38,290
investing one dollar

1656
01:57:38,310 --> 01:57:40,620
the return is filed before

1657
01:57:40,690 --> 01:57:44,390
the game is one or four

1658
01:57:44,390 --> 01:57:46,250
the point he is

1659
01:57:46,270 --> 01:57:50,730
that you have to take it at interest level

1660
01:57:50,750 --> 01:57:55,390
growth rate and in it the level

1661
01:57:55,400 --> 01:57:56,350
it is

1662
01:57:56,350 --> 01:57:58,480
random walk

1663
01:57:58,580 --> 01:58:00,390
is the

1664
01:58:00,420 --> 01:58:03,940
if it log base two

1665
01:58:03,940 --> 01:58:05,710
then it is last one

1666
01:58:05,750 --> 01:58:07,190
n minus one

1667
01:58:07,190 --> 01:58:10,060
but the in that level

1668
01:58:10,100 --> 01:58:13,620
if the random walk with me is there

1669
01:58:13,690 --> 01:58:16,790
the result in average growth there

1670
01:58:17,620 --> 01:58:22,460
but this is the the first of all in the second component

1671
01:58:22,460 --> 01:58:23,850
is the cash

1672
01:58:24,040 --> 01:58:26,100
that has there great

1673
01:58:28,400 --> 01:58:31,980
then i make a portfolio

1674
01:58:31,980 --> 01:58:36,770
forty two as i have to ask that both of them

1675
01:58:38,000 --> 01:58:39,750
zero growth rate

1676
01:58:39,750 --> 01:58:42,580
but they use the portfolio for the

1677
01:58:42,600 --> 01:58:45,900
thought of their growth rate

1678
01:58:45,920 --> 01:58:48,920
the portfolio mies

1679
01:58:48,980 --> 01:58:52,460
probability distribution so need

1680
01:58:52,460 --> 01:58:54,460
having first component b

1681
01:58:54,480 --> 01:58:58,540
the other component is one minus b

1682
01:58:58,540 --> 01:59:04,860
kernel methods methods just provide a principled way to address this kind of problems

1683
01:59:04,880 --> 01:59:06,850
with these kind of

1684
01:59:06,880 --> 01:59:08,560
of models

1685
01:59:08,570 --> 01:59:13,780
and this one is a very powerful on the kernel methods

1686
01:59:13,790 --> 01:59:16,800
so the idea is the kernel trick

1687
01:59:16,810 --> 01:59:19,820
i'm sure a lot of stuff

1688
01:59:19,870 --> 01:59:22,770
you guys know about the kernel trick but

1689
01:59:22,810 --> 01:59:24,100
it is

1690
01:59:24,160 --> 01:59:26,040
the crux

1691
01:59:26,040 --> 01:59:28,340
so the kernel trick

1692
01:59:28,390 --> 01:59:29,550
here it is

1693
01:59:29,550 --> 01:59:32,260
in a very it's simple way

1694
01:59:32,820 --> 01:59:37,830
you have a early separable dataset as

1695
01:59:37,840 --> 01:59:40,900
on one side and on the other side you know that you have a very

1696
01:59:40,900 --> 01:59:42,500
simple algorithm

1697
01:59:42,510 --> 01:59:48,000
let's say not simple but just linear algorithm you have something that is capable two

1698
01:59:48,380 --> 01:59:54,290
two to compute the hyperplane so your focus is hyperplane you know how to compute

1699
01:59:54,290 --> 01:59:55,740
a hyperplane

1700
01:59:56,610 --> 02:00:01,980
now what you want to do is to use your your favourite algorithm

1701
02:00:02,060 --> 02:00:09,190
to compute hyperplanes in the situation where hyperplanes are not sufficient to do classification

1702
02:00:09,250 --> 02:00:17,080
so the idea behind the kernel trick and the idea is a little bit reversed

1703
02:00:19,570 --> 02:00:23,370
the way i am going to present it is a little bit reversed with what

1704
02:00:23,370 --> 02:00:28,920
is actually meant by by kernel trick but it helps understand how

1705
02:00:28,940 --> 02:00:31,640
how how how it works so

1706
02:00:31,660 --> 02:00:37,880
the idea is that if you have a nonlinear nonlinear and a non-linear problem is

1707
02:00:38,050 --> 02:00:43,680
but you may want to try to map all your data into another space

1708
02:00:43,700 --> 02:00:47,550
usually high dimensional space where

1709
02:00:48,100 --> 02:00:51,900
this is the linear step separation can be

1710
02:00:51,940 --> 02:00:55,760
can be solved for

1711
02:01:00,250 --> 02:01:08,360
so these these mappings from the input space towards these special high dimensional space which

1712
02:01:08,360 --> 02:01:11,460
we're we're going to call the feature space

1713
02:01:11,520 --> 02:01:14,670
it is usually denoted as five

1714
02:01:14,690 --> 02:01:18,080
so these these guys here

1715
02:01:18,080 --> 02:01:25,610
and what you want is that these are the space age

1716
02:01:25,640 --> 02:01:29,690
you want to be equipped with a dot product because

1717
02:01:29,710 --> 02:01:33,190
if it's equipped with the dot product then you can work in this space try

1718
02:01:33,200 --> 02:01:39,240
to find a linear separator in this space and then only work with the images

1719
02:01:39,240 --> 02:01:40,840
of the training data

1720
02:01:40,860 --> 02:01:41,840
by bye

1721
02:01:41,850 --> 02:01:48,730
and on the the the training data by by themselves and

1722
02:01:48,730 --> 02:01:53,280
i think that this better summarize the idea is that you have these non linear

1723
02:01:53,280 --> 02:01:56,020
separable problems use

1724
02:01:56,040 --> 02:01:58,040
the mapping phi

1725
02:01:58,090 --> 02:02:01,950
that goes from x to some feature space h

1726
02:02:02,010 --> 02:02:03,710
and hearing have all the

1727
02:02:03,730 --> 02:02:05,090
images of the

1728
02:02:05,100 --> 02:02:06,330
right points

1729
02:02:06,340 --> 02:02:10,830
in this this space and the images of the blue point in this space and

1730
02:02:10,830 --> 02:02:13,910
in this space you can do linear separation

1731
02:02:14,000 --> 02:02:20,110
and again the the crux is that you assume that there is an inner product

1732
02:02:20,110 --> 02:02:23,960
in the space because if there is an inner product we just so that it's

1733
02:02:23,960 --> 02:02:26,500
possible to find a classifier

1734
02:02:26,510 --> 02:02:30,780
it's it's just easy if you have an dot product in the space you walk

1735
02:02:30,820 --> 02:02:32,410
in two two

1736
02:02:32,430 --> 02:02:37,340
to derive an two to compute the decision surface so this this is the idea

1737
02:02:37,340 --> 02:02:39,140
of the

1738
02:02:39,150 --> 02:02:41,340
of the

1739
02:02:41,520 --> 02:02:47,000
the kernel trick but it's not precisely the idea i i'll come to it in

1740
02:02:47,000 --> 02:02:48,340
a bit

1741
02:02:48,360 --> 02:02:50,400
so if

1742
02:02:50,420 --> 02:02:53,770
i'm going back to the previous algorithm

1743
02:02:53,780 --> 02:02:56,760
the very simple linear algorithms

1744
02:02:56,810 --> 02:03:00,100
they means that

1745
02:03:00,120 --> 02:03:05,270
instead of having the dot product between between the exercise and x

1746
02:03:05,310 --> 02:03:09,120
then you will have the dot product between the phi fixi i

1747
02:03:09,140 --> 02:03:11,570
and the five x

1748
02:03:11,610 --> 02:03:16,210
so if you have your mapping phi and you can work with the mapping

1749
02:03:16,250 --> 02:03:21,680
then it's just very easy to compute a non-linear

1750
02:03:23,110 --> 02:03:26,360
by using exactly the same algorithm as before

1751
02:03:26,550 --> 02:03:29,710
provided that you have

1752
02:03:29,730 --> 02:03:35,140
and in the product in the space age

1753
02:03:35,200 --> 02:03:39,730
and here is actually the kernel trick

1754
02:03:43,860 --> 02:03:45,600
the bits

1755
02:03:45,610 --> 02:03:49,170
OK this is just the step before the actual work

1756
02:03:49,180 --> 02:03:50,130
the kernel trick

1757
02:03:51,320 --> 02:03:56,760
the kernel trick can be applied so this is trick again i'm going to to

1758
02:03:56,760 --> 02:03:59,860
take it and go into the

1759
02:04:02,240 --> 02:04:07,370
so the kernel trick is just a very simple thing which say that if you

1760
02:04:07,370 --> 02:04:12,520
have a linear ordering that works with dot product that you do something very silly

1761
02:04:12,520 --> 02:04:16,810
but very powerful every time you see a dot product review replace it by

1762
02:04:17,210 --> 02:04:19,580
a kernel function k

1763
02:04:19,590 --> 02:04:21,710
because if the kernel function k

1764
02:04:21,730 --> 02:04:24,830
then you know that it's using kernel function k

1765
02:04:24,840 --> 02:04:29,980
it is precisely like as if you were mapping or your data from an input

1766
02:04:29,980 --> 02:04:33,250
space to work well designed

1767
02:04:33,700 --> 02:04:35,380
feature space

1768
02:04:35,440 --> 02:04:40,530
so why is that the way that because what you're going to use is

1769
02:04:40,810 --> 02:04:45,210
mercer mercer or positive definite kernels

1770
02:04:45,270 --> 02:04:50,420
positive definite kernels or or functional

1771
02:04:50,450 --> 02:04:54,600
taking two arguments and outputting the value or

1772
02:04:54,620 --> 02:04:56,990
you know in the real sense

1773
02:04:57,020 --> 02:05:01,980
and so if it is positive definite if

1774
02:05:02,030 --> 02:05:04,200
compute computing

1775
02:05:04,260 --> 02:05:09,130
k of UV is equivalent to first mapping u

1776
02:05:09,150 --> 02:05:14,770
and v two some hidden space and then to compute the dot product between those

1777
02:05:14,770 --> 02:05:18,300
not those mapped instances

1778
02:05:18,300 --> 02:05:22,740
and the key points actually on on the the way i'm going to i'm trying

1779
02:05:22,740 --> 02:05:30,600
to to bring into the kernel kernel trick is that the end facets

1780
02:05:30,960 --> 02:05:34,650
is usually when you want to use the kernel trick and the idea of mapping

1781
02:05:35,250 --> 02:05:40,290
actually the the idea the emphasis on the very the important thing is not to

1782
02:05:40,290 --> 02:05:46,860
find the phi transform to the phi transformed the fine mapping but actually it's more

1783
02:05:46,860 --> 02:05:47,600
to find the

1784
02:05:47,600 --> 02:05:49,420
but then he meets

1785
02:05:49,490 --> 02:05:51,980
someone by the name of

1786
02:05:52,030 --> 02:05:53,930
what we have we have to follow

1787
02:05:53,990 --> 02:05:57,430
and he meets

1788
02:05:57,470 --> 02:06:01,120
this man wrote to partner

1789
02:06:01,130 --> 02:06:05,310
and this is where probability theory apparently comes in the two-parter now

1790
02:06:05,330 --> 02:06:09,490
tells now that he knows the science of gambling and he

1791
02:06:09,540 --> 02:06:10,980
well teach two

1792
02:06:12,020 --> 02:06:17,280
but it has to be done by whispering in his ear because it's deep and

1793
02:06:17,280 --> 02:06:18,910
extreme secret

1794
02:06:18,930 --> 02:06:20,860
and now is skeptical

1795
02:06:20,870 --> 02:06:23,760
how does the two two-pronged know how to to gamble

1796
02:06:23,900 --> 02:06:25,200
and so

1797
02:06:25,210 --> 02:06:29,000
the two parent tries to prove to him his abilities and he says

1798
02:06:29,020 --> 02:06:30,530
see that tree there

1799
02:06:30,540 --> 02:06:33,910
i can estimate how many leaves there are on that tree

1800
02:06:33,920 --> 02:06:39,430
by counting one believes on one branch over two partner looked at one branch

1801
02:06:39,440 --> 02:06:42,650
and then estimate the number of leaves in the tree

1802
02:06:42,660 --> 02:06:45,240
and now was

1803
02:06:45,250 --> 02:06:46,300
step to call

1804
02:06:47,170 --> 02:06:52,630
he stayed up all night and counted every leaf and the tree came very close

1805
02:06:52,630 --> 02:06:55,670
to what now are two parents

1806
02:06:55,720 --> 02:06:57,720
and so

1807
02:06:57,730 --> 02:07:00,700
he next morning believe now

1808
02:07:00,710 --> 02:07:02,640
they believe it to partner

1809
02:07:02,980 --> 02:07:08,670
now this is interesting hacking says because it shows that sampling theory was part of

1810
02:07:08,700 --> 02:07:10,290
knowledge theory

1811
02:07:10,340 --> 02:07:13,050
if you don't have to cover all the leaves in the tree you take a

1812
02:07:14,150 --> 02:07:16,870
and you can that and then you multiply

1813
02:07:16,920 --> 02:07:23,190
so anyway the story and now goes back and now armed with probability theory we

1814
02:07:23,920 --> 02:07:29,200
he goes back and gamble's again he has nothing left to wager except his wife

1815
02:07:29,210 --> 02:07:30,420
so he

1816
02:07:30,430 --> 02:07:36,200
he puts her and gambles her remembering now he knows what he's doing

1817
02:07:36,480 --> 02:07:42,390
and so he really wasn't gambling his wife he really very pure and arable land

1818
02:07:42,440 --> 02:07:48,060
so he won back the entire kingdom and that's the end so anyway that shows

1819
02:07:48,060 --> 02:07:51,340
that i think probability theory does have a long history

1820
02:07:51,350 --> 02:07:56,450
but it not being an intellectual disciplines it didn't really informed

1821
02:07:57,490 --> 02:07:59,480
a generation of

1822
02:07:59,500 --> 02:08:01,780
finance when you don't have a theory

1823
02:08:01,830 --> 02:08:04,290
then you don't have the way to be rigorous

1824
02:08:05,240 --> 02:08:13,200
it was in the sixteen hundred that probability theory started to get written down as

1825
02:08:13,200 --> 02:08:14,110
a theory

1826
02:08:15,050 --> 02:08:18,070
many things that happened in that century

1827
02:08:18,090 --> 02:08:21,930
which i think are precursors both to finance and insurance

1828
02:08:21,940 --> 02:08:24,340
one was in the sixteen hundreds

1829
02:08:24,360 --> 02:08:27,410
people started constructing life tables

1830
02:08:27,430 --> 02:08:28,890
what is the life table

1831
02:08:28,930 --> 02:08:34,020
it's a table showing the probability of dying at each age for each age and

1832
02:08:35,580 --> 02:08:39,250
that's what you need to know if you're going to do life insurance

1833
02:08:39,300 --> 02:08:42,030
so they started to do collecting of data

1834
02:08:42,040 --> 02:08:43,580
and mortality

1835
02:08:43,600 --> 02:08:50,180
and they developed something called actuarial science which is estimating the probability of people

1836
02:08:51,490 --> 02:08:53,720
and so

1837
02:08:53,730 --> 02:08:59,120
that then became the basis for

1838
02:08:59,140 --> 02:09:00,700
in france

1839
02:09:00,710 --> 02:09:04,420
now actually insurance goes back to ancient rome

1840
02:09:04,430 --> 02:09:05,950
in some forms

1841
02:09:05,970 --> 02:09:10,350
in ancient rome they had something called burial insurance

1842
02:09:10,370 --> 02:09:11,670
you could buy

1843
02:09:12,830 --> 02:09:15,380
that protected you against

1844
02:09:15,390 --> 02:09:20,000
your family not having the money to bury you if you died

1845
02:09:20,020 --> 02:09:23,170
and in ancient culture people were a great deal

1846
02:09:23,180 --> 02:09:25,290
about being properly buried

1847
02:09:25,300 --> 02:09:29,600
so that's an interesting concept they were selling that in ancient rome

1848
02:09:29,650 --> 02:09:31,630
but you might think

1849
02:09:31,650 --> 02:09:36,050
why just for burial why not going to make it into a full-blown life insurance

1850
02:09:36,050 --> 02:09:38,120
and you can wonder why they didn't

1851
02:09:38,130 --> 02:09:41,440
i think it may be because they didn't have the concepts that

1852
02:09:41,500 --> 02:09:43,770
in renaissance italy

1853
02:09:43,790 --> 02:09:46,490
they started writing insurance policies

1854
02:09:46,500 --> 02:09:50,250
but i read one of the insurance policies it's in the a journal of risk

1855
02:09:50,250 --> 02:09:54,410
and insurance they translated a red sox insurance policy

1856
02:09:54,640 --> 02:09:59,570
and very hard to understand what this policy was so i guess they didn't have

1857
02:09:59,570 --> 02:10:02,130
a language they didn't it was

1858
02:10:02,590 --> 02:10:07,770
there were intuitively halfway there but they could express i think the

1859
02:10:07,790 --> 02:10:11,100
industry didn't get really started

1860
02:10:11,730 --> 02:10:14,130
i think it's the invention of

1861
02:10:14,140 --> 02:10:18,790
probability theory that really starts and that's why i think theory is very important in

1862
02:10:18,790 --> 02:10:20,600
finance i

1863
02:10:23,360 --> 02:10:24,340
some people date

1864
02:10:24,350 --> 02:10:28,400
fire insurance with the fire of london in sixteen sixty six

1865
02:10:28,440 --> 02:10:30,130
the whole city burned down

1866
02:10:30,140 --> 02:10:32,810
practically in a terrible fire

1867
02:10:32,830 --> 02:10:38,250
and fire insurance started to proliferate right after that in london

1868
02:10:38,270 --> 02:10:43,380
but you kind of wonder if that's a good example for fire insurance because

1869
02:10:43,390 --> 02:10:48,370
if the whole city burns down insurance companies would go bankrupt anywhere here right london

1870
02:10:48,370 --> 02:10:50,140
insurance companies would because

1871
02:10:50,150 --> 02:10:55,280
the whole concept of insurance is pulling your problem of independent probabilities

1872
02:10:55,370 --> 02:10:57,580
and so

1873
02:10:57,630 --> 02:11:00,270
but nonetheless that was the beginning

1874
02:11:00,290 --> 02:11:04,940
we're also going to recognise however that insurance

1875
02:11:04,990 --> 02:11:07,250
got a slow start

1876
02:11:07,260 --> 02:11:10,430
because and i believe it's because people

1877
02:11:10,500 --> 02:11:11,410
could not

1878
02:11:11,430 --> 02:11:12,820
i understand

1879
02:11:12,830 --> 02:11:14,660
the concept of probability

1880
02:11:14,680 --> 02:11:16,390
he didn't have

1881
02:11:16,440 --> 02:11:22,650
they didn't have the concept firmly in mind there's a lot of aspects to it

1882
02:11:22,670 --> 02:11:23,580
you know

1883
02:11:23,590 --> 02:11:28,410
there's an ad it in order to understand probability have to take things as

1884
02:11:28,450 --> 02:11:30,870
coming from a random event

1885
02:11:31,670 --> 02:11:32,690
and people

1886
02:11:32,710 --> 02:11:37,770
don't clearly have that in their mind from an intuitive standpoint they have maybe a

1887
02:11:38,630 --> 02:11:42,220
that i can influence events by willing or wishing

1888
02:11:42,500 --> 02:11:46,530
and if i think that if i have kind of mystical side to me

1889
02:11:46,550 --> 02:11:50,400
then probabilities don't have a clear meaning

1890
02:11:50,450 --> 02:11:53,640
and it's been shown that even today

1891
02:11:53,690 --> 02:12:00,700
people seem to think that they don't really take at an intuitive level probabilities

1892
02:12:00,790 --> 02:12:05,980
very as objective

1893
02:12:06,000 --> 02:12:09,110
so for example if you ask people how much they would be willing to bet

1894
02:12:09,300 --> 02:12:10,990
on a coin toss

1895
02:12:11,030 --> 02:12:15,040
they were they will typically that more if they can toss the coin

