1
00:00:00,000 --> 00:00:02,370
all right

2
00:00:03,940 --> 00:00:06,460
and also

3
00:00:22,640 --> 00:00:31,500
she is wrong

4
00:00:37,600 --> 00:00:42,290
so far

5
00:02:22,370 --> 00:02:26,890
there are small

6
00:04:08,810 --> 00:04:12,070
well to

7
00:04:25,400 --> 00:04:28,090
o were

8
00:04:28,090 --> 00:04:28,760
it's a

9
00:04:28,760 --> 00:04:31,550
the expected value of AUC

10
00:04:31,570 --> 00:04:35,550
in that case if i tell you here is the value of accuracy with a

11
00:04:35,550 --> 00:04:41,840
uniform distribution your best answer would probably be well i guess you see is the

12
00:04:43,130 --> 00:04:46,430
but again is lot better

13
00:04:46,450 --> 00:04:50,660
because it was possible rule that point be this one

14
00:04:50,660 --> 00:04:53,930
which as you see is square

15
00:04:53,950 --> 00:04:55,650
the best possible wrong because

16
00:04:55,660 --> 00:05:00,050
the point is this one which has a UC one minus

17
00:05:00,070 --> 00:05:02,340
one of the square one

18
00:05:04,070 --> 00:05:14,050
so again there is some some new relationship but because the we we exploit this

19
00:05:15,510 --> 00:05:19,360
if we have situations like

20
00:05:19,380 --> 00:05:27,700
adaboost seemed to optimize a senior SVM what the montecito we need model specific to

21
00:05:27,740 --> 00:05:33,230
to explain the point and as i mentioned was very nice here which is that

22
00:05:33,320 --> 00:05:36,410
for SVM

23
00:05:37,380 --> 00:05:42,860
so moving into the final which is about relation between

24
00:05:44,130 --> 00:05:48,220
probability estimation

25
00:05:48,570 --> 00:05:52,150
what need help probability s

26
00:05:52,840 --> 00:05:58,180
i like to say that if my probability estimates are not calibrated in some sense

27
00:05:58,340 --> 00:06:01,780
in fact we don't have a problem with this image though we have already

28
00:06:01,800 --> 00:06:06,340
i like to say that space is is not the probability estimate that happens to

29
00:06:06,340 --> 00:06:07,360
have number

30
00:06:07,380 --> 00:06:08,680
zero and one

31
00:06:08,680 --> 00:06:09,470
one day

32
00:06:10,220 --> 00:06:13,340
if you want probabilities you

33
00:06:13,360 --> 00:06:16,930
so that's what we're going to talk about that here

34
00:06:16,950 --> 00:06:22,630
and the reason why you want to write a probability estimate is that you get

35
00:06:22,630 --> 00:06:26,570
your justification differences which are special point four

36
00:06:26,590 --> 00:06:31,500
if you probability estimates are not the right there is no justification for such a

37
00:06:31,500 --> 00:06:35,780
special afternoon session

38
00:06:35,780 --> 00:06:42,630
so what you essentially means is that because i have my model and that's a

39
00:06:42,630 --> 00:06:48,050
report of the predictions of the model that and now i look at the examples

40
00:06:48,050 --> 00:06:51,820
that have particular predicted probability

41
00:06:51,840 --> 00:06:55,760
it was generated by the proportion of

42
00:06:55,780 --> 00:06:56,860
what this

43
00:06:56,890 --> 00:06:59,740
next to correspond

44
00:06:59,760 --> 00:07:05,660
and the fact they should be examples of the problem five

45
00:07:05,680 --> 00:07:08,450
for one classifier expect

46
00:07:08,470 --> 00:07:09,800
as many positive

47
00:07:09,820 --> 00:07:12,300
the next

48
00:07:12,300 --> 00:07:17,240
and actually without going in the world details

49
00:07:17,260 --> 00:07:23,840
basically this proportion of positive to negative is essentially the sole of wrong

50
00:07:23,860 --> 00:07:29,240
so well calibrated classifier which uses it for all the segments

51
00:07:29,280 --> 00:07:34,130
of the ROC curve is the score corresponds to the slope of the second

52
00:07:34,160 --> 00:07:36,430
that means that convex

53
00:07:36,450 --> 00:07:41,110
because this corresponds to increasing along the and then

54
00:07:41,160 --> 00:07:43,180
slopes monotonically

55
00:07:46,450 --> 00:07:50,010
this is a very simple you

56
00:07:50,700 --> 00:07:56,160
because there are many is convex constructed

57
00:07:56,220 --> 00:08:01,240
whole which essentially means introducing ties in the ranking and then we can just read

58
00:08:01,880 --> 00:08:06,090
the probability from the cell cycle

59
00:08:06,110 --> 00:08:08,110
now this is what is wrong with machine

60
00:08:08,220 --> 00:08:11,470
so what this the the machine learning that is easy to pick out the good

61
00:08:11,720 --> 00:08:13,030
news machine learning because

62
00:08:13,240 --> 00:08:18,240
get invented reinvented over and over again this is all the more so

63
00:08:18,240 --> 00:08:24,740
one is not only by itself that is if you already exists under the name

64
00:08:24,740 --> 00:08:28,380
of these regression which why

65
00:08:28,390 --> 00:08:37,700
that by itself to get some fifty to sixty was introduced to machine by charles

66
00:08:39,530 --> 00:08:41,160
the connection

67
00:08:41,160 --> 00:08:43,610
with the royal of was

68
00:08:44,630 --> 00:08:52,030
first of all alex niculescu in the rest of the band which she during the

69
00:08:52,160 --> 00:08:59,930
just beat me to it on honestly the same thing they get because the

70
00:08:59,930 --> 00:09:02,640
one of the things i like to do this if i see something that's against

71
00:09:02,640 --> 00:09:06,720
my opinions then then i'd like to try and evaluate completely my opinions and become

72
00:09:06,720 --> 00:09:09,160
very uncertain the

73
00:09:09,220 --> 00:09:13,410
the way these models bayesian inference will do that but not with most of the

74
00:09:13,410 --> 00:09:17,470
classically constructed model most of the time get more confident as you see data so

75
00:09:17,470 --> 00:09:21,930
that something can be quite wary of and the actual property is the log concavity

76
00:09:21,930 --> 00:09:25,890
of the likelihood if the likelihood is log concave that's going to happen if you've

77
00:09:25,890 --> 00:09:30,040
got in on log concave likelihood you can get this nice effect and for example

78
00:09:30,040 --> 00:09:34,500
robust regression is an example of that is in robust regression if you see a

79
00:09:34,500 --> 00:09:35,700
data points

80
00:09:35,700 --> 00:09:37,310
a lot further away

81
00:09:37,330 --> 00:09:39,870
from where you expect it to be

82
00:09:39,890 --> 00:09:44,660
then you actually increase your uncertainty about what's going on because the like this user

83
00:09:44,660 --> 00:09:46,270
not log concave

84
00:09:46,290 --> 00:09:51,790
so the final example is because there was no election on last night and this

85
00:09:51,790 --> 00:09:54,160
is how left or right wing you are

86
00:09:54,220 --> 00:09:57,270
so on the left wing your voting labour

87
00:09:57,290 --> 00:10:01,600
this is you probably know these parties if you know english middle little americans may

88
00:10:01,600 --> 00:10:05,520
be surprised to see that with the liberal party is in the middle in the

89
00:10:05,520 --> 00:10:11,470
united kingdom and most of europe norway on the left the liberal party and the

90
00:10:11,470 --> 00:10:13,220
conservative party

91
00:10:13,240 --> 00:10:16,470
the observation in this case you think someone is left with but then they say

92
00:10:16,470 --> 00:10:18,200
they voted liberal

93
00:10:18,220 --> 00:10:20,850
and so you observe this medal thing

94
00:10:20,850 --> 00:10:23,370
so you want your opinion they become more right

95
00:10:23,370 --> 00:10:24,390
and again

96
00:10:24,410 --> 00:10:29,020
that's a nongaussian like this you can update it with the gas

97
00:10:32,560 --> 00:10:36,910
i was really hoping that i was going to mention kernels in his talk given

98
00:10:37,100 --> 00:10:39,200
kernel methods in the title

99
00:10:41,180 --> 00:10:44,310
and he wrote the book on it but

100
00:10:44,330 --> 00:10:48,250
he did mention kernel methods so

101
00:10:48,270 --> 00:10:50,430
i'll have to

102
00:10:50,500 --> 00:10:55,480
have introduced a little bit on this respects the kernel method

103
00:10:55,500 --> 00:10:58,700
so the likelihood we have the full regression what we can do is we can

104
00:10:58,700 --> 00:11:01,890
combine it with the prior density of the parameters in this prize just saying we

105
00:11:01,890 --> 00:11:03,810
don't know very very slow

106
00:11:04,350 --> 00:11:08,950
the product is equally likely to be the same size the spherical gaussians prior to

107
00:11:08,950 --> 00:11:13,770
the identity matrix and the covariance independent and based some scale of these parameters that

108
00:11:13,770 --> 00:11:17,040
will eventually affect scalar function now

109
00:11:17,060 --> 00:11:20,580
what i'm not going to show you what you can do this integral

110
00:11:20,600 --> 00:11:21,640
and you recover

111
00:11:21,640 --> 00:11:24,100
because of counting dousing doused in fact

112
00:11:24,120 --> 00:11:26,790
you recover the marginal likelihood for model

113
00:11:26,810 --> 00:11:28,430
is calcium

114
00:11:28,480 --> 00:11:30,430
and you're recovery projections

115
00:11:30,450 --> 00:11:34,000
the elements of this new covariance is zero mean

116
00:11:34,020 --> 00:11:38,970
because your prior over w zero means even though you had to me here

117
00:11:39,020 --> 00:11:43,060
this prior is zero mean and effective that is to make the final marginal likelihood

118
00:11:43,080 --> 00:11:47,890
zero mean that this was independent and this independency but looking here this is not

119
00:11:47,890 --> 00:11:53,680
independent so basically this is the covariance matrix that says what the correlation between different

120
00:11:53,680 --> 00:11:57,330
observations are and in fact it depends on the axis so

121
00:11:57,430 --> 00:12:02,330
elements of this covariance matrix which is giving you the correlations between your also the

122
00:12:02,330 --> 00:12:05,350
training points are given by

123
00:12:05,370 --> 00:12:09,370
this inner product between your basis functions and this is where i hope not

124
00:12:09,390 --> 00:12:12,450
i talked about kernel methods because

125
00:12:12,500 --> 00:12:15,680
we talked about these basis functions as fixed set

126
00:12:15,720 --> 00:12:18,500
if you remember that i was using three

127
00:12:18,520 --> 00:12:21,100
the polynomial all

128
00:12:22,430 --> 00:12:25,640
the RBF squared exponential basis

129
00:12:25,720 --> 00:12:28,470
and then we got some coming from the noise so it's some sort of ten

130
00:12:28,470 --> 00:12:33,240
coming from these some across these basis functions times the scale parameter here

131
00:12:33,240 --> 00:12:37,330
she's giving us the overall size covariance plus some noise

132
00:12:37,350 --> 00:12:40,830
now this is what i would call accounting process but you could also think of

133
00:12:40,830 --> 00:12:42,080
this as the can

134
00:12:43,810 --> 00:12:48,220
this is an inner product in infinite dimensional products and kernels then we wouldn't be

135
00:12:48,290 --> 00:12:50,310
one of the clever tricks you can do

136
00:12:50,350 --> 00:12:55,770
you can replace the inner product with the so-called covariance functions all kernels mercer kernels

137
00:12:55,830 --> 00:12:58,040
which allows you to consider

138
00:12:58,430 --> 00:13:01,720
infinite basis functions that's really fun

139
00:13:01,890 --> 00:13:05,640
because instead of having to choose the number of basis which is one question my

140
00:13:05,680 --> 00:13:09,500
ask how many bases you're going to use you just say i'm going to use

141
00:13:09,500 --> 00:13:10,790
infinity of them

142
00:13:10,790 --> 00:13:15,520
you never have to compute these infinity basis functions because it's an inner product between

143
00:13:15,520 --> 00:13:19,680
two infinite things and you can prove this relationship that's what mercer's theorem allows you

144
00:13:19,680 --> 00:13:21,040
to do

145
00:13:21,060 --> 00:13:23,020
so this is the way of thinking about

146
00:13:23,040 --> 00:13:28,970
the your current account is sum of infinite basis functions computed at

147
00:13:29,020 --> 00:13:31,040
these two different training points

148
00:13:31,040 --> 00:13:36,100
it's the covariance of your training data so the covariance between two different training points

149
00:13:36,240 --> 00:13:40,500
is the evaluation of all these infinite basis functions of those two different training points

150
00:13:40,500 --> 00:13:45,330
the thing about that i think is a bit of mindset change for some machine

151
00:13:45,330 --> 00:13:48,660
learning people is when we so and i emphasise before

152
00:13:48,680 --> 00:13:50,540
noise is i i d

153
00:13:50,560 --> 00:13:53,640
noise is i i d

154
00:13:54,600 --> 00:13:59,060
the noise is i i d because there's no crossed dependencies the kronecker delta that

155
00:13:59,060 --> 00:14:01,730
so just to

156
00:14:01,750 --> 00:14:06,940
summarize we've got a distance which is something which is saying i take some elements

157
00:14:06,940 --> 00:14:09,520
and the closer the smaller

158
00:14:09,540 --> 00:14:12,810
and then we got a metric which is what we got

159
00:14:12,850 --> 00:14:17,600
i reduce the five equations into three because they can and you obtain these three

160
00:14:20,580 --> 00:14:22,940
OK so what can we do with strings

161
00:14:22,960 --> 00:14:27,210
that's you know just so codeword would of conclusion pros and cons a distance in

162
00:14:27,210 --> 00:14:31,770
the general sense is more flexible when people are working on applications people are going

163
00:14:31,770 --> 00:14:36,690
to try and find you know things that seem to work that corresponds to what

164
00:14:36,730 --> 00:14:41,350
you what they feel the problem is is that that was mentioning earlier on how

165
00:14:41,710 --> 00:14:46,060
you are choosing the features so that the corresponded to what your intuition of the

166
00:14:46,060 --> 00:14:47,100
problem was

167
00:14:47,100 --> 00:14:51,870
so the same here choosing the distance so that it corresponds to what you want

168
00:14:51,870 --> 00:14:56,730
the same with kernels kernels you often choose because you really think this is sort

169
00:14:56,750 --> 00:15:01,230
of property of the kernel you need for this application should have

170
00:15:01,270 --> 00:15:05,350
but then at the same time if you need algorithmics while you're probably going to

171
00:15:05,350 --> 00:15:10,980
have to have some symmetric somewhere

172
00:15:11,000 --> 00:15:14,330
so if we back to big back to strings we're going to say that there

173
00:15:14,330 --> 00:15:18,710
are four types of distances and that there's only three points no these four types

174
00:15:18,710 --> 00:15:20,020
and this is the first

175
00:15:20,120 --> 00:15:24,870
so we're going to go through four different ways of building distances the first way

176
00:15:24,900 --> 00:15:30,040
is to say well i'm going to try and see how many modifications i need

177
00:15:30,080 --> 00:15:35,480
to go from string x to string why somebody count how many changes how many

178
00:15:35,480 --> 00:15:40,790
individual changes i need to be able to get to transform string x to string

179
00:15:40,790 --> 00:15:45,080
why if you're more interesting trees you can think trees more interesting graphs you can

180
00:15:45,080 --> 00:15:50,870
think graphs on long vectors of numbers that's clearly not offering numbers that's clearly not

181
00:15:50,870 --> 00:15:54,020
indicated but on what we're doing it is

182
00:15:54,460 --> 00:15:58,560
so you're going to try and do this and you might think about normalizing say

183
00:15:58,560 --> 00:16:03,290
yes but i mean obviously if my string is of length several million then i

184
00:16:03,290 --> 00:16:07,480
mean i have to do some changes i would like this to be normalized taking

185
00:16:07,480 --> 00:16:10,140
into account the length of the string

186
00:16:10,170 --> 00:16:13,670
that's the sort of idea and the most typical and the one that i want

187
00:16:13,670 --> 00:16:20,420
to tell you about all of these distances is called the edit distance

188
00:16:20,480 --> 00:16:23,600
but before tell you about the edit distance and you can run through the other

189
00:16:23,830 --> 00:16:28,540
the second one is to say well we're going to be able to compute some

190
00:16:28,540 --> 00:16:30,790
sort of similarity

191
00:16:30,810 --> 00:16:36,310
using another way time looks word about the kernel if you have an answer is

192
00:16:36,310 --> 00:16:41,940
something even looser a similarity between two strings that idea of similarity being

193
00:16:41,960 --> 00:16:44,520
the more things are in common then

194
00:16:44,540 --> 00:16:47,750
then the larger the similarity

195
00:16:47,830 --> 00:16:51,850
and then i'll try and find some way to be able to convert this similarity

196
00:16:51,850 --> 00:16:54,350
between two strings into

197
00:16:54,400 --> 00:16:56,290
a distance

198
00:16:56,520 --> 00:17:00,600
and this leads to methods i know of that can can can do this

199
00:17:00,660 --> 00:17:05,210
so the first method is to do the following is to just take as the

200
00:17:05,210 --> 00:17:10,410
distance between a and b two to the power set minus the similarity in a

201
00:17:10,410 --> 00:17:12,120
of a and b

202
00:17:12,140 --> 00:17:16,250
in this way so the more they similar to minus is going to make it

203
00:17:17,440 --> 00:17:21,460
so we're going to have distances that are really going to be a

204
00:17:21,460 --> 00:17:25,940
of this sort one over two one over four one of the great one of

205
00:17:25,980 --> 00:17:31,620
six in of mathematical made reasons this is OK for practical reasons is not it's

206
00:17:31,620 --> 00:17:36,420
never used romanticising saying it does exist and we relate with some sort of kernels

207
00:17:37,620 --> 00:17:42,000
so typically the people that use the prefixes well i would say well how what's

208
00:17:42,000 --> 00:17:46,270
the length of the longest common prefix of got between two strings we take that

209
00:17:46,270 --> 00:17:49,330
in common we look at the length of this prefix take two

210
00:17:49,330 --> 00:17:53,210
two the powerset minus this length and we obtain the distance

211
00:17:53,230 --> 00:17:57,640
o with just one city located everything is in common then we would say that

212
00:17:59,190 --> 00:18:03,770
things uncommon and then we say d if a if b is equal to zero

213
00:18:03,810 --> 00:18:07,670
which allows us to student to find what want

214
00:18:10,460 --> 00:18:11,560
if you could

215
00:18:11,580 --> 00:18:16,940
if a equals b then div a b would be something like two minus s

216
00:18:16,940 --> 00:18:18,190
of infinity

217
00:18:18,250 --> 00:18:19,480
so about

218
00:18:23,640 --> 00:18:26,960
that's why we're doing it that way but i mean saying is infinite the similarity

219
00:18:26,960 --> 00:18:32,750
being infinite on finite strings doesn't make much it's more like a trick to be

220
00:18:32,750 --> 00:18:37,620
able to do a deal with this which is going to decide that it's simple

221
00:18:37,670 --> 00:18:41,120
you should be saying OK ways the length of the string itself

222
00:18:41,140 --> 00:18:45,660
but so is too but that would make it known not equal to zero

223
00:18:45,750 --> 00:18:49,460
so that's the first method the second method which is the one that is used

224
00:18:49,460 --> 00:18:53,830
more like by kernels i will check this i actually wanted to check it but

225
00:18:53,830 --> 00:18:58,230
the idea is the following do the distance got as well which would be some

226
00:18:58,230 --> 00:18:59,560
sort of kernel

227
00:18:59,560 --> 00:19:03,660
typically the people that use kernels do this sort of way and what they do

228
00:19:03,660 --> 00:19:04,810
is they

229
00:19:04,850 --> 00:19:08,770
well they take the similarity between a and a plus the similarity between b and

230
00:19:08,770 --> 00:19:14,750
b and then you've got to abstract two times similarity between a and b it

231
00:19:14,770 --> 00:19:18,310
symmetrical and if not i just in this way like this

232
00:19:19,330 --> 00:19:24,080
so this is sort of way that is used if you're writing it down put

233
00:19:24,080 --> 00:19:29,040
a question mark because i had to check it on some slides by somebody who

234
00:19:29,060 --> 00:19:36,420
possibly this i didn't check also the conditions about this yes is the x is

235
00:19:36,420 --> 00:19:41,850
equals y and yes this the triangle is the triangular inequality only holds when certain

236
00:19:41,850 --> 00:19:46,710
special conditions over the summer activity held but people for the people that to do

237
00:19:46,710 --> 00:19:52,440
kernels will try and transform the if needed the kernel into a distance

238
00:19:52,460 --> 00:19:57,900
if somebody you can look at that also in an exercise session

239
00:19:57,920 --> 00:20:03,270
so the third way of doing that distances right is to extract some features so

240
00:20:03,290 --> 00:20:06,920
we saw that this morning and to say well we don't want to manipulate strings

241
00:20:06,920 --> 00:20:10,210
tall because what we really know how to manipulate of vectors

242
00:20:10,230 --> 00:20:12,190
so we're going to take these vectors

243
00:20:12,210 --> 00:20:14,230
and we are going to

244
00:20:15,560 --> 00:20:20,460
measures some of the system takes measurable features to decide on the length of the

245
00:20:20,460 --> 00:20:25,310
vector and we're just going to compute this numerical vector for a another numerical vector

246
00:20:25,310 --> 00:20:30,080
for b and then we can use some distance between the many distances many norms

247
00:20:30,080 --> 00:20:35,790
and got over or to the end i worked directly on the vector representation of

248
00:20:35,790 --> 00:20:39,750
the strings so you can think well you know why study westerns if we can

249
00:20:39,750 --> 00:20:44,330
do this the problem is that usually by doing that by choosing this the

250
00:20:44,390 --> 00:20:48,960
the features we to do something about about about the strings themselves

251
00:20:48,960 --> 00:20:52,810
so there are cases where you want what you call the people that are working

252
00:20:52,810 --> 00:20:59,130
on say a natural language processing on translation issues they're going to need long-term dependencies

253
00:20:59,130 --> 00:21:02,670
things like that which are usually going to be lost when we go and extract

254
00:21:02,670 --> 00:21:08,660
thanks for inviting me happy to be here

255
00:21:08,670 --> 00:21:10,550
i'm going to be talking about

256
00:21:10,580 --> 00:21:12,930
general purpose image segmentation

257
00:21:13,720 --> 00:21:17,350
using random walks we've heard a lot about random walks on graphs

258
00:21:17,360 --> 00:21:19,050
over the course of this

259
00:21:19,380 --> 00:21:21,120
the conference

260
00:21:21,150 --> 00:21:24,600
i'm going to be continuing that and

261
00:21:24,620 --> 00:21:28,970
talking about uses force image segmentation specifically

262
00:21:29,320 --> 00:21:33,330
so here is the outline of organ be talking about

263
00:21:33,370 --> 00:21:38,440
first as francisco mentioned i come from siemens corporate search one to give a few

264
00:21:38,440 --> 00:21:41,480
slides but who we are and what we do

265
00:21:41,510 --> 00:21:45,480
then talk about what i mean by general purpose segmentation

266
00:21:45,570 --> 00:21:49,820
and then introduce you to a particular algorithm that have been developing for the last

267
00:21:49,820 --> 00:21:55,030
few years now called the random walker algorithm the going to go through the the

268
00:21:55,030 --> 00:21:59,230
concept the properties of the behaviour of the algorithm the theory

269
00:21:59,250 --> 00:22:04,470
numerics look at some results and then talk about some more recent work

270
00:22:04,510 --> 00:22:06,950
the up of conclusions

271
00:22:06,980 --> 00:22:13,780
SCR are siemens corporate search is located in princeton new jersey USA

272
00:22:13,790 --> 00:22:17,780
we have about two hundred full-time research staff and about

273
00:22:17,840 --> 00:22:20,280
the size of a little bit about

274
00:22:20,330 --> 00:22:22,230
eighty two hundred people

275
00:22:22,230 --> 00:22:24,980
at the phd level working on computer vision

276
00:22:25,060 --> 00:22:29,980
basically siemens we are trying to solve

277
00:22:30,000 --> 00:22:35,590
practical computer vision problems mainly in the area of medical image analysis

278
00:22:35,610 --> 00:22:37,890
are funding models such that we

279
00:22:38,640 --> 00:22:45,140
about one-third of our time and basic research about two-thirds of on actual product development

280
00:22:45,780 --> 00:22:48,610
the goal of clinical imaging of

281
00:22:48,620 --> 00:22:52,200
in computer vision the medical domain is generally do

282
00:22:54,310 --> 00:22:59,310
enhance therapy or provide more information to

283
00:22:59,330 --> 00:23:00,890
radiologists that would be

284
00:23:00,900 --> 00:23:03,590
look otherwise looking at the images

285
00:23:03,610 --> 00:23:08,250
and obviously ultimately to save time and reduce costs

286
00:23:08,260 --> 00:23:11,040
are our core interests basically

287
00:23:11,080 --> 00:23:12,410
fall on the three

288
00:23:12,420 --> 00:23:17,610
major lines the medical imaging community segmentation registration and visualization

289
00:23:17,730 --> 00:23:22,040
we also work on interventional imaging

290
00:23:22,080 --> 00:23:30,130
in which we can put people inside MRI's and tried to help surgeons work in

291
00:23:30,130 --> 00:23:32,430
real time in order to

292
00:23:34,670 --> 00:23:39,140
extract organs and and tumors and things like that

293
00:23:39,160 --> 00:23:44,110
there's a lot of problems here associated with segmentation registration and real time analysis

294
00:23:44,140 --> 00:23:49,830
so that that's who we are it seems copper research and we're always taking

295
00:23:49,890 --> 00:23:51,640
interns and

296
00:23:51,660 --> 00:23:57,160
staff members and if anybody is interested in sending students or or common cells you

297
00:23:57,160 --> 00:23:58,410
know please be in touch

298
00:23:58,420 --> 00:24:00,820
OK so now

299
00:24:00,830 --> 00:24:04,860
my research i'm going to be talking about general purpose segmentation what i mean by

300
00:24:06,510 --> 00:24:10,860
the goal with general purpose segmentation is

301
00:24:10,920 --> 00:24:12,980
simple or sounds simple

302
00:24:12,990 --> 00:24:16,520
important image and output the desired segmentation

303
00:24:16,670 --> 00:24:19,850
obviously the problem is

304
00:24:19,890 --> 00:24:24,330
with this desired segmentation because two different people

305
00:24:24,350 --> 00:24:29,550
two different users may want different objects to be extracted of an image

306
00:24:29,580 --> 00:24:36,200
so from our standpoint sometimes with actually like to provide a general-purpose tool like the

307
00:24:36,910 --> 00:24:39,230
magic wand and photoshop

308
00:24:39,270 --> 00:24:41,980
that allows somebody to

309
00:24:41,990 --> 00:24:44,830
on any given day extract you know

310
00:24:45,580 --> 00:24:50,170
cousin from the photograph put them somewhere else or

311
00:24:50,700 --> 00:24:57,960
especially for product standpoint if we build a segmentation engine to segment say livers and

312
00:24:58,010 --> 00:25:00,880
tomorrow we need to segment kidneys

313
00:25:00,920 --> 00:25:05,450
we'd like to be able to reuse the core segmentation engine and not start from

314
00:25:08,670 --> 00:25:11,260
this problem though of trying to

315
00:25:11,300 --> 00:25:17,510
return the object that a particular individuals looking for

316
00:25:17,540 --> 00:25:19,730
requires some interaction

317
00:25:19,740 --> 00:25:21,180
in other words

318
00:25:21,190 --> 00:25:25,210
if we really have a general purpose segmentation tool

319
00:25:26,190 --> 00:25:30,440
the user has to indicate to the algorithm which object it is they want to

320
00:25:30,440 --> 00:25:33,190
extract or reduce the desired segmentation

321
00:25:33,210 --> 00:25:37,600
and there are basically three approaches for this in the literature there sort of the

322
00:25:37,600 --> 00:25:43,780
which might call classical approach in which all the objects are segmented automatically from the

323
00:25:43,780 --> 00:25:45,210
out from the image

324
00:25:45,360 --> 00:25:49,940
and then a user just go through and select the object that they were interested

325
00:25:52,060 --> 00:25:54,620
another approach is to

326
00:25:55,340 --> 00:25:57,780
provide pieces of boundary of

327
00:25:57,790 --> 00:26:01,690
an object or a nearby boundary in and of all that

328
00:26:01,720 --> 00:26:04,490
boundary to the desired boundary

329
00:26:04,620 --> 00:26:06,480
and that's the approach of

330
00:26:06,530 --> 00:26:10,560
so stakes level sets intelligent scissors

331
00:26:10,870 --> 00:26:17,320
more recently people have used this approach in which

332
00:26:17,990 --> 00:26:23,480
mark some interior parts of the object and some exterior parts of the object and

333
00:26:23,480 --> 00:26:28,730
then allow the the system to generate where the boundaries

334
00:26:28,750 --> 00:26:32,720
this is the approach of graph cuts at the magic wand in photoshop or region

335
00:26:36,130 --> 00:26:40,920
o thing about these atomic methods the ones that automatically segment all the objects in

336
00:26:40,920 --> 00:26:43,220
the image is that

337
00:26:43,270 --> 00:26:45,610
the atoms must be

338
00:26:45,620 --> 00:26:47,860
subset of the true segmentation

339
00:26:47,870 --> 00:26:50,430
if they're not then if the user wants

340
00:26:50,860 --> 00:26:54,770
half of one of the objects that's been segmented they need to come up with

341
00:26:54,770 --> 00:26:57,480
some way of supporting those objects

342
00:26:57,490 --> 00:27:01,750
what they want multiple objects any to come up with ways of merging

343
00:27:01,800 --> 00:27:08,400
the boundary methods have typically been easier to incorporate shape priors and they can be

344
00:27:08,400 --> 00:27:10,850
used to improve the segmentation algorithm

345
00:27:13,000 --> 00:27:17,520
traditionally what's in the literature has been iterative local minima

346
00:27:18,670 --> 00:27:22,230
it's very difficult to initialize boundary methods

347
00:27:22,230 --> 00:27:25,710
in higher dimensions or an abstract spaces

348
00:27:25,750 --> 00:27:29,110
in contrast the seeding methods for

349
00:27:29,120 --> 00:27:31,570
the third kind here

350
00:27:32,630 --> 00:27:36,970
generally leads naturally to steady state and graph based algorithms

351
00:27:36,980 --> 00:27:41,050
and it's easy to see to provide points that are inside and outside and objectivity

352
00:27:41,050 --> 00:27:42,460
in arbitrary dimension

353
00:27:42,500 --> 00:27:44,100
or other

354
00:27:44,110 --> 00:27:48,970
data structures for example if you want to segment the surface mesh or non uniformly

355
00:27:48,970 --> 00:27:50,100
sampled data

356
00:27:51,190 --> 00:27:53,960
some general clustering problem

357
00:27:53,970 --> 00:27:55,530
so it's this third type

358
00:27:55,550 --> 00:27:59,550
seeded methods that we're are going to be looking at

359
00:27:59,560 --> 00:28:05,550
the classic seeded algorithm is is region growing

360
00:28:05,610 --> 00:28:09,980
basically you start one seed and you grow

361
00:28:12,160 --> 00:28:14,730
a measure of distance of contrast

362
00:28:14,820 --> 00:28:19,180
is is meant the nice thing about region growing is that it's

363
00:28:19,610 --> 00:28:22,060
simple to code and it's fast

364
00:28:22,060 --> 00:28:22,960
many millions

365
00:28:23,560 --> 00:28:29,060
practice so many millions and patterns of zeros and non-zeros on the off diagonal the precision matrix

366
00:28:30,940 --> 00:28:33,580
and the nonzero elements across the time varying

367
00:28:34,330 --> 00:28:39,750
we evaluate posterior probabilities approximate posterior probabilities and this is a heat map those probabilities

368
00:28:40,190 --> 00:28:43,710
you can see that a lot of these edges have black very surely there

369
00:28:44,730 --> 00:28:49,190
there's a bit of red and yellow there are few uncertain edges this is probability point to deprive

370
00:28:49,920 --> 00:28:52,630
and the rest is white so they're really not there at all

371
00:28:53,150 --> 00:28:59,170
so this is this is a region of the posterior over the structure of this particular dynamic bayesian model

372
00:29:00,560 --> 00:29:01,710
in terms of sparsity

373
00:29:02,750 --> 00:29:03,130
which is

374
00:29:03,580 --> 00:29:05,880
really finding lots and lots and lots of

375
00:29:06,360 --> 00:29:07,170
model structures

376
00:29:07,810 --> 00:29:09,230
that have the same components

377
00:29:13,000 --> 00:29:13,630
and this graph

378
00:29:15,730 --> 00:29:16,100
is really

379
00:29:16,770 --> 00:29:18,290
and i s wave summarizing

380
00:29:19,810 --> 00:29:20,670
the statistical

381
00:29:21,880 --> 00:29:24,900
probabilistic and the decision implications

382
00:29:26,480 --> 00:29:30,730
office building sparsity into dynamic models in this kind of context it's a nice picture

383
00:29:32,190 --> 00:29:33,500
what we have here is just

384
00:29:34,150 --> 00:29:35,480
two hundred graphs these that

385
00:29:36,100 --> 00:29:38,730
a collection of two hundred high probability graphs

386
00:29:40,500 --> 00:29:46,940
log probability okay so these guys appear really dominant this is a huge there and huge market these are negligible

387
00:29:49,190 --> 00:29:53,420
what we have a here is sparsity in terms of the percentage if you have a quite

388
00:29:54,830 --> 00:29:57,500
i'm sorry the percentage black the number of edges that are in

389
00:29:58,100 --> 00:29:59,020
relative to the total

390
00:29:59,830 --> 00:30:05,400
so these graphs so lived sort in that ten to fifteen percent sparse ranged is a highly sparse graphs

391
00:30:06,400 --> 00:30:09,580
so there's a lot of zeros in the in this precision matrix

392
00:30:10,980 --> 00:30:11,940
this fellow at the back

393
00:30:13,620 --> 00:30:17,080
is down here to about minus two thousand on the log probability scale

394
00:30:17,580 --> 00:30:23,620
it's on the backplane and sparsity that's when you have no no no no no zeros in the precision matrix

395
00:30:24,000 --> 00:30:25,690
that's not a very interesting model structure

396
00:30:28,190 --> 00:30:34,270
this axis shows this risk these models are used in one step forecasting to with the same portfolio utility function

397
00:30:34,850 --> 00:30:37,520
to revise portfolios and see how much money was made

398
00:30:38,460 --> 00:30:40,670
and on this axis we've got risk on the

399
00:30:41,230 --> 00:30:43,600
standard deviation one step ahead portfolios

400
00:30:44,210 --> 00:30:45,060
i risk is bad

401
00:30:45,620 --> 00:30:47,250
i don't like to gamble my own money

402
00:30:48,270 --> 00:30:49,310
if it's in my pension

403
00:30:49,960 --> 00:30:51,230
find i don't like to gambling

404
00:30:53,100 --> 00:30:56,290
low risk is is generally good so low-risk portfolios

405
00:30:56,810 --> 00:30:58,350
have high posterior probability

406
00:30:58,770 --> 00:31:00,880
and tend to be associated with sparse grass

407
00:31:01,480 --> 00:31:03,380
sparsity is good from both the point-of-view

408
00:31:03,770 --> 00:31:04,920
the statistical point of view

409
00:31:05,380 --> 00:31:06,080
i probability

410
00:31:06,710 --> 00:31:08,670
and a piece the decision theory point of view

411
00:31:09,460 --> 00:31:10,420
lower risk okay

412
00:31:12,920 --> 00:31:14,730
the color coding is the fourth dimension

413
00:31:15,500 --> 00:31:16,250
the colour coding

414
00:31:19,040 --> 00:31:23,400
however this period of well we we did an out-of-sample predictive portfolio analysis

415
00:31:23,810 --> 00:31:25,650
o repeat whatever period of time we talk

416
00:31:26,100 --> 00:31:28,500
this shows that the risk the return on the portfolio

417
00:31:29,080 --> 00:31:30,620
for each of these model structures

418
00:31:32,310 --> 00:31:35,850
so red is really good so you see if you don't mind taking a lot

419
00:31:35,850 --> 00:31:39,850
of risk you can make a lot of money with sparse graphs but they don't

420
00:31:39,850 --> 00:31:41,190
tend to be very high probability

421
00:31:44,060 --> 00:31:46,960
there's a lot of red appears so there are a lot of high probability graphs

422
00:31:46,960 --> 00:31:49,270
that that that that make money

423
00:31:49,810 --> 00:31:52,250
that a risk these are interesting model structures

424
00:31:53,000 --> 00:31:53,560
there are also

425
00:31:54,460 --> 00:31:58,630
some graphs appeared that don't make so much money that have high probability it so it's not a given

426
00:31:59,210 --> 00:32:00,440
the posterior probability

427
00:32:00,880 --> 00:32:04,400
is the way to search over model structures if what you want to do is select a model

428
00:32:04,850 --> 00:32:06,100
all select a few models

429
00:32:08,250 --> 00:32:11,850
if part of what you are interested in is is using these models for decision

430
00:32:11,850 --> 00:32:14,330
making then you've got a factor in utility functions as well

431
00:32:14,750 --> 00:32:17,250
and they can be due in different parts of the model space

432
00:32:22,440 --> 00:32:27,250
okay to more check

433
00:32:28,480 --> 00:32:28,850
that's good

434
00:32:29,850 --> 00:32:30,770
that's going on one time

435
00:32:31,420 --> 00:32:35,880
a couple slides and i'm now fast-forward and even more

436
00:32:37,250 --> 00:32:40,670
just to point out a couple of areas based in dynamic model

437
00:32:41,380 --> 00:32:45,500
modeling that are topical in that i know a lot of people are interested in them and working on them

438
00:32:46,170 --> 00:32:47,380
am i find topical

439
00:32:48,900 --> 00:32:49,920
on the sparsity theme

440
00:32:51,150 --> 00:32:54,040
so what we've been talking about there is taking a model structure

441
00:32:55,600 --> 00:32:59,730
whether it's not autoregressive parameter matrix of the precision matrix

442
00:33:00,810 --> 00:33:02,250
a volatility model

443
00:33:03,210 --> 00:33:06,310
or dynamic regression coefficient matrix or something else

444
00:33:07,980 --> 00:33:09,690
anna's dimension explodes

445
00:33:10,080 --> 00:33:14,310
we want constraints and some of those constraints will be saying that's zeros in their

446
00:33:14,310 --> 00:33:17,250
let's find out whether the data doesn't argue against their

447
00:33:19,040 --> 00:33:20,880
well i think that's a good strategy and

448
00:33:21,540 --> 00:33:25,040
we all do it we'll do bayesian model selection and d

449
00:33:25,080 --> 00:33:29,400
use prior distributions that white parameters the zero if if they don't look interesting

450
00:33:30,080 --> 00:33:34,850
but in a dynamic context in a time series context this time as a covariance

451
00:33:35,860 --> 00:33:40,230
you might have a model where you've got a particular set of zeros in the precision matrix

452
00:33:40,940 --> 00:33:42,330
and it's been working fine

453
00:33:43,810 --> 00:33:45,080
but circumstances change

454
00:33:45,080 --> 00:33:48,300
and this is part of the power of language

455
00:33:48,960 --> 00:33:53,980
the syntactic rules are complicated and one of the puzzles of syntactic rules are one

456
00:33:53,980 --> 00:33:58,210
of the big issues of them is that different rules

457
00:33:58,230 --> 00:34:01,990
can conspire to create the same sentence

458
00:34:02,010 --> 00:34:03,830
so you the sentence

459
00:34:03,840 --> 00:34:07,920
why this is the classic white from growth remarks i want to an elephant in

460
00:34:07,920 --> 00:34:09,670
my pajamas

461
00:34:09,680 --> 00:34:12,080
how would got to my pajamas i'll never know

462
00:34:12,090 --> 00:34:14,520
and the humor such that it is

463
00:34:14,560 --> 00:34:19,160
revolves around the ambiguity of of rules are generated like this

464
00:34:19,220 --> 00:34:20,910
first is like this

465
00:34:21,880 --> 00:34:26,310
to illustrate issues of ambiguity people have collected

466
00:34:26,320 --> 00:34:33,890
poorly thought-out headlines in newspaper reports that play on that inadvertently

467
00:34:34,770 --> 00:34:37,070
have ambiguity

468
00:34:37,080 --> 00:34:39,980
complaints about NBA referees growing ugly

469
00:34:40,680 --> 00:34:43,550
universe structure

470
00:34:43,550 --> 00:34:45,920
kids make nutritious snacks

471
00:34:45,950 --> 00:34:50,670
no one was injured in the blast which was attributed to the belief that one

472
00:34:50,670 --> 00:34:51,890
town official

473
00:34:54,890 --> 00:34:57,570
last summer i was in seoul

474
00:34:58,020 --> 00:35:04,110
visiting the visiting korea university and the big headline there on front page was

475
00:35:04,120 --> 00:35:07,810
general arrested for funding private

476
00:35:10,190 --> 00:35:12,170
there actually is

477
00:35:14,270 --> 00:35:17,900
ambiguity is that it is actually quite difficult to avoid

478
00:35:17,960 --> 00:35:20,660
in the construction understanding of sentences

479
00:35:20,670 --> 00:35:23,810
it's one of the one of the ways in which is often difficult to write

480
00:35:24,840 --> 00:35:26,400
and in fact as a whole

481
00:35:26,410 --> 00:35:28,890
subfield of the law

482
00:35:30,100 --> 00:35:35,430
the use of linguistic theory to disambiguates sentences both in

483
00:35:35,460 --> 00:35:36,940
in the constitution

484
00:35:36,960 --> 00:35:40,510
in in in legislation and legislation

485
00:35:40,530 --> 00:35:42,970
as well as in some criminal cases

486
00:35:44,000 --> 00:35:45,980
there was

487
00:35:45,990 --> 00:35:48,660
several years ago very serious

488
00:35:48,660 --> 00:35:51,660
criminal case that rested on the sentence

489
00:35:51,710 --> 00:35:53,150
and here's what happened

490
00:35:53,150 --> 00:35:55,950
there were two brothers one of them retarded

491
00:35:56,010 --> 00:35:58,200
and they get into a robbery

492
00:35:59,410 --> 00:36:00,900
police officer

493
00:36:00,930 --> 00:36:03,570
system and points the gun at the

494
00:36:04,900 --> 00:36:06,570
one of the brothers

495
00:36:06,610 --> 00:36:07,860
points the gun

496
00:36:07,860 --> 00:36:09,630
at the police officer

497
00:36:09,680 --> 00:36:12,340
the police officer shouts from the brother

498
00:36:12,360 --> 00:36:14,320
the non retarded brother

499
00:36:14,340 --> 00:36:16,630
to drop the gun

500
00:36:16,630 --> 00:36:19,220
she said give me the gun

501
00:36:19,220 --> 00:36:22,910
the retarded brother showed

502
00:36:22,930 --> 00:36:25,050
we don't have it

503
00:36:25,070 --> 00:36:28,680
where upon brother shot and killed a police officer

504
00:36:28,700 --> 00:36:32,050
but rather that the shooting was planned murder

505
00:36:32,130 --> 00:36:35,470
what about the brother who show to let them have it

506
00:36:37,180 --> 00:36:38,800
it depends

507
00:36:38,820 --> 00:36:41,740
on what it on on how you interpret that sentence

508
00:36:42,050 --> 00:36:44,720
sentence is beautifully ambiguous

509
00:36:44,740 --> 00:36:46,970
it could mean

510
00:36:46,990 --> 00:36:48,950
should have learned

511
00:36:48,970 --> 00:36:53,380
or it could mean given the gun let them have

512
00:36:53,400 --> 00:36:57,220
and in fact the try which i think somebody could if people that know about

513
00:36:57,220 --> 00:37:00,970
this please send me an email my understanding was he was found guilty

514
00:37:00,970 --> 00:37:05,680
but a lot to turn on the ambiguity of the sentence

515
00:37:05,680 --> 00:37:07,510
i want to shift now

516
00:37:07,530 --> 00:37:08,990
and talk about where

517
00:37:10,050 --> 00:37:12,150
all this knowledge come from

518
00:37:12,150 --> 00:37:18,110
but all stop and answer any questions about material so far what are your questions

519
00:37:22,490 --> 00:37:27,610
syntax and the question is how is syntax different from grammar they're exactly the same

520
00:37:27,970 --> 00:37:34,860
syntax is more technical terms but typical but it means the same thing as ram

521
00:37:53,340 --> 00:37:56,340
yes i'm glad i'm glad you actually asked me about that because as i said

522
00:37:56,340 --> 00:37:58,510
i realized it wasn't quite right

523
00:37:58,530 --> 00:38:00,840
the point i was just raise serious

524
00:38:00,880 --> 00:38:05,590
i've said before everybody neurologically normal comes to acquire

525
00:38:05,610 --> 00:38:07,360
and learn language

526
00:38:07,380 --> 00:38:09,260
but what about people who

527
00:38:09,260 --> 00:38:12,990
are neurologically normal but they don't have language around

528
00:38:13,050 --> 00:38:16,510
and in fact have been historically some cases of this

529
00:38:16,990 --> 00:38:22,150
there's been a probably apocryphal stories about children who are raised by wolves are by

530
00:38:23,410 --> 00:38:28,400
there are stories horrible stories some in some in the twentieth century about children were

531
00:38:28,400 --> 00:38:30,220
locked away

532
00:38:30,260 --> 00:38:32,860
by insane or evil parents

533
00:38:32,910 --> 00:38:34,820
and you never learn to speak

534
00:38:34,840 --> 00:38:36,800
there are stories of deaf people

535
00:38:36,800 --> 00:38:39,430
who are within certain societies were nobody

536
00:38:39,450 --> 00:38:40,950
science to

537
00:38:40,950 --> 00:38:43,400
and so there what's known as linguistic isolates

538
00:38:43,410 --> 00:38:46,200
and they themselves never learn to speak

539
00:38:46,200 --> 00:38:50,340
the probability of the the next variables given the previous one like it always i

540
00:38:50,340 --> 00:38:52,920
can always back to factorize the distribution

541
00:38:52,960 --> 00:38:58,020
in that way that doesn't require any particular properties of this

542
00:38:58,040 --> 00:38:59,970
now i have written

543
00:39:00,010 --> 00:39:01,460
my joint distribution

544
00:39:01,460 --> 00:39:04,280
the product of some one-dimensional

545
00:39:04,340 --> 00:39:06,430
distributions may that's helpful

546
00:39:06,430 --> 00:39:07,640
o point understand

547
00:39:08,760 --> 00:39:11,140
what is it that give rise to particular

548
00:39:11,570 --> 00:39:14,690
form here

549
00:39:16,940 --> 00:39:22,190
but not use this in a constructive way that you construct the joint distribution here

550
00:39:22,240 --> 00:39:23,230
by simply

551
00:39:23,760 --> 00:39:26,750
i simply writing out the terms of support

552
00:39:26,810 --> 00:39:29,000
what with the individual terms look like

553
00:39:29,840 --> 00:39:33,570
the individual terms are just conditional distributions

554
00:39:33,580 --> 00:39:35,920
all single variable given

555
00:39:35,990 --> 00:39:37,760
given previous variables

556
00:39:37,790 --> 00:39:39,410
the conditional distribution

557
00:39:39,410 --> 00:39:43,970
of the joint gaston is again again

558
00:39:45,940 --> 00:39:50,070
the actual rate expression here the joint distribution

559
00:39:50,090 --> 00:39:53,180
the joint distribution again this particular form

560
00:39:53,190 --> 00:39:55,030
then the conditional distribution of x

561
00:39:55,080 --> 00:39:56,380
given y

562
00:39:56,380 --> 00:40:02,460
as this sort of longish only looking expression but it's just it's just come out

563
00:40:02,480 --> 00:40:05,400
so we can actually we can actually write down these things

564
00:40:05,570 --> 00:40:07,710
because they only depend on

565
00:40:07,810 --> 00:40:11,980
that means that the entries in the query

566
00:40:11,990 --> 00:40:20,140
OK let's try and let's try actually do this

567
00:40:25,280 --> 00:40:33,570
OK so so in the those in this but i'm going to try to support

568
00:40:33,590 --> 00:40:36,810
a random realisation of political process

569
00:40:36,860 --> 00:40:42,210
again i'm going to have a one-dimensional input x on this axis and the function

570
00:40:42,210 --> 00:40:44,780
values on the other hand

571
00:40:46,940 --> 00:40:50,540
anyway i'm going to do is i'm going to select xis doing was going select

572
00:40:51,960 --> 00:40:53,450
and then i'm going to talk

573
00:40:53,500 --> 00:40:54,890
what the

574
00:40:55,140 --> 00:41:02,210
distribution of the corresponding function values are given that x and given the previous value

575
00:41:02,260 --> 00:41:04,020
and one start from nothing

576
00:41:09,230 --> 00:41:14,910
he and number to use this one is the part right now there's no there's

577
00:41:14,940 --> 00:41:19,460
function yet but the the function will be drawn in here

578
00:41:19,500 --> 00:41:20,310
and the

579
00:41:23,020 --> 00:41:24,830
so here is going to give me

580
00:41:24,840 --> 00:41:30,570
the area in which is likely to find the function value

581
00:41:30,680 --> 00:41:35,000
the first thing i choose the next random happen to be an here

582
00:41:35,950 --> 00:41:39,020
now just indicate where the x by this red line

583
00:41:39,050 --> 00:41:40,600
now i want to sample

584
00:41:40,610 --> 00:41:42,270
from the conditional distributions

585
00:41:42,300 --> 00:41:44,730
of the function value

586
00:41:44,740 --> 00:41:46,180
given the previous one

587
00:41:46,190 --> 00:41:49,510
which of which there are not not even anything

588
00:41:49,530 --> 00:41:51,000
so i just one sample

589
00:41:51,010 --> 00:41:52,710
values at random

590
00:41:52,760 --> 00:41:58,790
from this skeleton distribution happens to be concentration has done division of one

591
00:41:58,800 --> 00:42:04,950
i think the ninety five percent confidence intervals which is roughly between minus two into

592
00:42:04,990 --> 00:42:07,990
but i know i draw a random sample

593
00:42:08,000 --> 00:42:11,260
from the distribution

594
00:42:11,280 --> 00:42:13,240
so i happened to draw something

595
00:42:16,000 --> 00:42:20,490
i know that my function has to go through the point

596
00:42:20,560 --> 00:42:23,240
so now i move on to the next variables

597
00:42:23,310 --> 00:42:29,790
the next one i happen to choose was over here

598
00:42:29,850 --> 00:42:31,690
now you can see that the

599
00:42:31,800 --> 00:42:34,980
the conditional distribution of of the

600
00:42:34,990 --> 00:42:36,270
the new variable

601
00:42:36,290 --> 00:42:37,990
now conditions on

602
00:42:37,990 --> 00:42:39,030
the other parable

603
00:42:39,090 --> 00:42:41,510
i'd like to change ever-so-slightly

604
00:42:43,410 --> 00:42:47,550
because of the relationship between the two function values through the

605
00:42:47,580 --> 00:42:50,970
correlation which is induced by the covariance

606
00:42:51,010 --> 00:42:53,110
draw random sample of value

607
00:42:53,130 --> 00:42:55,390
i happen to do something

608
00:42:55,400 --> 00:42:59,810
of the random which was larger than the mean

609
00:42:59,910 --> 00:43:02,050
and in the that process

610
00:43:02,060 --> 00:43:03,410
now i notice that

611
00:43:03,450 --> 00:43:07,540
this is actually change the distribution over here right now it's very unlikely that we

612
00:43:07,540 --> 00:43:10,130
get but i'm going to talk about value down here

613
00:43:10,140 --> 00:43:13,720
five because conditional on already having observed the functions

614
00:43:13,770 --> 00:43:20,160
we rather positive function values would be very unlikely to get something which was a

615
00:43:20,160 --> 00:43:24,690
the good the portfolio vector for we'll be banned

616
00:43:24,730 --> 00:43:27,830
i strongly on the priors p

617
00:43:27,960 --> 00:43:35,530
here's the log optimal portfolio vector is characterized as well

618
00:43:35,880 --> 00:43:44,800
john was an arbitrary functions you're of and mine with one variable

619
00:43:44,820 --> 00:43:46,600
and calculate

620
00:43:46,650 --> 00:43:51,000
the conditional expectation of the lottery or b

621
00:43:51,040 --> 00:43:53,210
the portfolio choice

622
00:43:53,220 --> 00:43:59,100
given the parts they played means that x one x two x minus one is

623
00:43:59,110 --> 00:44:02,710
the shorthand notation

624
00:44:02,710 --> 00:44:05,780
so for any function b

625
00:44:05,800 --> 00:44:09,720
let's get qualities is conditional expectation

626
00:44:09,730 --> 00:44:11,700
and search

627
00:44:11,710 --> 00:44:14,650
for the function b

628
00:44:14,660 --> 00:44:18,710
which maximizes the conditional expectation

629
00:44:18,770 --> 00:44:23,710
the conditional expectation q one x one x two x minus one

630
00:44:23,750 --> 00:44:27,840
is a function of x one x two x minus one

631
00:44:27,840 --> 00:44:34,380
so for any function i can probably needs to be a random variable

632
00:44:34,400 --> 00:44:40,210
but for any function it is a random variable depending on the part of the

633
00:44:42,770 --> 00:44:44,390
function b

634
00:44:44,400 --> 00:44:50,840
which maximizes the conditional expectation is the the definition of the start these part the

635
00:44:50,850 --> 00:44:55,570
function which maximizes the conditional expectation

636
00:44:55,590 --> 00:45:00,960
if the market process is independent identically distributed

637
00:45:02,960 --> 00:45:07,100
the conditional expectation is equal to the expectation

638
00:45:07,170 --> 00:45:07,940
he is

639
00:45:07,950 --> 00:45:14,410
OK as get back the original definition of the log optimal portfolio then we we

640
00:45:14,420 --> 00:45:21,090
have to find a big portfolio vector in a big vector which maximizes the expectation

641
00:45:21,150 --> 00:45:26,890
but if we had just stationary market processes that we can do

642
00:45:26,900 --> 00:45:32,670
much better but it is that a random variable is a function we make a

643
00:45:34,800 --> 00:45:36,790
within the

644
00:45:36,880 --> 00:45:39,610
functions of mind one variable

645
00:45:39,640 --> 00:45:46,020
at the start is the function of each maximizes its conditional expectation later on i

646
00:45:46,020 --> 00:45:51,950
will return to the this definition because it it looks great but otherwise people

647
00:45:52,040 --> 00:45:57,220
but the first round it's not so easy to be

648
00:45:57,250 --> 00:46:02,090
why you should be a good idea through

649
00:46:05,020 --> 00:46:09,620
that if it is optimal

650
00:46:10,760 --> 00:46:15,440
but this has been discovered by paul on the phone call over

651
00:46:15,450 --> 00:46:17,470
is that for

652
00:46:17,540 --> 00:46:23,140
and optimality is characterized this

653
00:46:23,180 --> 00:46:27,660
let s start the new

654
00:46:27,680 --> 00:46:32,550
the amount of money at the end of the

655
00:46:32,570 --> 00:46:35,600
and creating video using

656
00:46:36,450 --> 00:46:37,900
b star

657
00:46:37,940 --> 00:46:40,230
or that are

658
00:46:41,550 --> 00:46:43,670
less as and

659
00:46:43,680 --> 00:46:45,130
and every theory

660
00:46:45,160 --> 00:46:50,650
they can be solved

661
00:46:50,690 --> 00:46:56,890
four and very portfolio strategies

662
00:46:56,920 --> 00:47:00,070
and then compare

663
00:47:00,080 --> 00:47:02,580
the average growth rate

664
00:47:02,640 --> 00:47:04,600
of the fire

665
00:47:06,150 --> 00:47:07,280
the other

666
00:47:07,340 --> 00:47:11,470
the average growth rate

667
00:47:12,300 --> 00:47:18,140
here it is not obvious that any of the

668
00:47:18,500 --> 00:47:22,680
the growth rate average growth rate

669
00:47:23,180 --> 00:47:26,440
our coverage

670
00:47:26,500 --> 00:47:29,100
here it that

671
00:47:29,140 --> 00:47:33,750
that if you compare to average growth rate

672
00:47:33,770 --> 00:47:35,820
then we have very

673
00:47:35,880 --> 00:47:41,600
the growth rate of the very portfolio selection

674
00:47:41,610 --> 00:47:43,580
any strategy

675
00:47:43,600 --> 00:47:46,070
the average growth rate

676
00:47:46,080 --> 00:47:49,240
the average growth rate for all around

677
00:47:49,280 --> 00:47:50,820
can be larger

678
00:47:50,900 --> 00:47:54,010
the average growth rate of the other one

679
00:47:54,030 --> 00:47:58,770
if you have a lot of large numbers if if the sequence is an ergodic

680
00:47:59,790 --> 00:48:05,810
it is possible to calculate the best possible average growth rate but is so complicated

681
00:48:05,810 --> 00:48:07,470
we have

682
00:48:09,870 --> 00:48:11,700
i it

683
00:48:11,750 --> 00:48:13,700
it was full of expectation

684
00:48:13,790 --> 00:48:15,960
i couldn't sleep all night

685
00:48:16,070 --> 00:48:18,400
there's the elephant

686
00:48:18,450 --> 00:48:21,530
there is no evidence whatsoever that p of l

687
00:48:21,630 --> 00:48:24,190
really large for the elephant for the mouse

688
00:48:24,230 --> 00:48:26,020
vertical bars indicate

689
00:48:26,050 --> 00:48:28,860
my uncertainty in measurement of thickness

690
00:48:28,880 --> 00:48:32,030
and the horizontal tail which is a logarithmic scale

691
00:48:32,030 --> 00:48:34,310
the uncertainty of the length measurement

692
00:48:34,540 --> 00:48:36,970
it is in the thickness of the red and so there's no

693
00:48:37,020 --> 00:48:39,820
need for me to indicate that and further

694
00:48:39,880 --> 00:48:41,850
and here you have your measurements

695
00:48:41,900 --> 00:48:44,010
in case you want to check

696
00:48:44,050 --> 00:48:47,980
and look again at the mouth and look at the end of

697
00:48:48,020 --> 00:48:50,400
the mouth

698
00:48:50,470 --> 00:48:52,130
as the only

699
00:48:52,180 --> 00:48:54,350
one centimeter length of the femur

700
00:48:54,370 --> 00:48:58,480
and the elephant is indeed on hundred times longer so the first gaining argument

701
00:48:58,520 --> 00:49:00,740
that s is proportional to l

702
00:49:01,930 --> 00:49:05,800
what you expect to live in this about larger in size

703
00:49:05,810 --> 00:49:07,690
but when you go to be over l

704
00:49:07,690 --> 00:49:09,100
you see it's all over

705
00:49:09,120 --> 00:49:10,900
the overall for the model

706
00:49:10,990 --> 00:49:13,490
it's really not all that different from elephant

707
00:49:13,500 --> 00:49:14,800
and you would expect it

708
00:49:16,430 --> 00:49:17,900
with the square root

709
00:49:19,700 --> 00:49:23,250
one hundred thirty were expected to be ten times more

710
00:49:23,260 --> 00:49:24,760
instead of about

711
00:49:24,800 --> 00:49:26,870
the same

712
00:49:26,880 --> 00:49:29,010
and i want to discuss with you

713
00:49:29,030 --> 00:49:31,220
what we call in physics

714
00:49:32,500 --> 00:49:36,040
and that was it

715
00:49:36,090 --> 00:49:38,540
i will ask myself the question

716
00:49:38,540 --> 00:49:40,180
if i drop

717
00:49:40,260 --> 00:49:41,200
and appl

718
00:49:41,210 --> 00:49:43,800
from a certain height

719
00:49:43,850 --> 00:49:46,330
and i changed the height

720
00:49:46,390 --> 00:49:48,940
what will happen with time

721
00:49:48,970 --> 00:49:50,390
forty apple

722
00:49:50,440 --> 00:49:53,470
two full

723
00:49:54,870 --> 00:49:56,720
i dropped the apple from

724
00:49:56,950 --> 00:49:58,750
hi h

725
00:49:58,820 --> 00:50:00,560
i want to know

726
00:50:00,570 --> 00:50:05,470
what happened was the time when i change a ge

727
00:50:05,510 --> 00:50:09,730
so i said to myself well the time it takes must be proportional

728
00:50:09,740 --> 00:50:12,550
the height of the ball for

729
00:50:12,610 --> 00:50:16,750
pretty reasonable if i make the height larger we all know that it's longer forty

730
00:50:20,080 --> 00:50:23,710
i said to myself well if the apple has mass and

731
00:50:23,750 --> 00:50:25,970
probably is also proportional

732
00:50:25,990 --> 00:50:27,110
the massive

733
00:50:29,040 --> 00:50:32,730
i said to myself if something is more mass

734
00:50:32,740 --> 00:50:34,580
probably take less time

735
00:50:34,600 --> 00:50:35,880
so maybe

736
00:50:35,900 --> 00:50:40,180
and some power but i don't know if i don't know better

737
00:50:40,190 --> 00:50:44,830
then i suggest there's also something like gravity is gravitational pull

738
00:50:44,930 --> 00:50:47,480
the gravitational acceleration

739
00:50:47,560 --> 00:50:49,680
so let introduce that two

740
00:50:49,720 --> 00:50:50,840
let's assume

741
00:50:50,860 --> 00:50:52,970
at that time is also proportional

742
00:50:52,970 --> 00:50:57,520
the gravitational acceleration is an acceleration learn a lot more about that

743
00:50:57,570 --> 00:51:00,430
the power government

744
00:51:00,470 --> 00:51:03,050
having said this

745
00:51:03,110 --> 00:51:10,890
we can now do what's called in physics a dimensional and that

746
00:51:10,940 --> 00:51:12,530
on the left

747
00:51:12,540 --> 00:51:14,430
we have time

748
00:51:14,470 --> 00:51:17,760
and if we have left on the left side of time on the right side

749
00:51:17,760 --> 00:51:19,590
we must also have

750
00:51:19,700 --> 00:51:20,900
you cannot have

751
00:51:20,910 --> 00:51:25,340
coconuts on one side and and oranges only other you cannot have seconds on one

752
00:51:26,230 --> 00:51:28,470
and meters per second

753
00:51:28,480 --> 00:51:31,620
sort of dimensions left and and right have to be the same

754
00:51:31,640 --> 00:51:33,730
one of them mention here

755
00:51:33,760 --> 00:51:34,990
that is cheap

756
00:51:35,080 --> 00:51:38,340
that he

757
00:51:38,350 --> 00:51:40,130
it must be the same

758
00:51:42,600 --> 00:51:45,530
the power of

759
00:51:45,570 --> 00:51:47,590
time math

760
00:51:47,660 --> 00:51:50,340
the power data

761
00:51:53,430 --> 00:51:55,720
remember still there on the blackboard

762
00:51:55,760 --> 00:51:58,200
as i mentioned l

763
00:51:58,320 --> 00:52:01,340
abided by times

764
00:52:01,440 --> 00:52:05,610
and the whole thing could have all gone however godlike have gone

765
00:52:05,660 --> 00:52:07,810
this site must have the same dimension

766
00:52:07,830 --> 00:52:11,190
that is non-negotiable physics

767
00:52:11,190 --> 00:52:12,680
OK there we go

768
00:52:12,730 --> 00:52:15,270
there is no and is only one year

769
00:52:15,350 --> 00:52:17,540
so bad must be zero

770
00:52:17,580 --> 00:52:20,910
there is here

771
00:52:20,930 --> 00:52:24,790
well to the power of l the bubblegum others no l year

772
00:52:24,800 --> 00:52:27,590
sewell was this year of

773
00:52:27,750 --> 00:52:31,070
gamma must be zero

774
00:52:31,500 --> 00:52:33,380
people power one year

775
00:52:33,430 --> 00:52:38,790
and there is here to the power miners to go my mind because downstairs

776
00:52:38,800 --> 00:52:39,620
so one

777
00:52:39,660 --> 00:52:41,890
must equal minus

778
00:52:43,080 --> 00:52:44,340
it means grandma

779
00:52:44,350 --> 00:52:46,540
but minus one half

780
00:52:46,580 --> 00:52:48,800
the gamma is minus one

781
00:52:49,080 --> 00:52:51,180
of the world

782
00:52:54,300 --> 00:52:56,900
and my dimension that

783
00:52:56,910 --> 00:52:58,040
i therefore

784
00:52:59,330 --> 00:53:01,030
the time that takes

785
00:53:01,030 --> 00:53:02,930
for an object fall

786
00:53:03,020 --> 00:53:06,630
equal to some constant which i do not know

787
00:53:06,720 --> 00:53:08,340
content has no dimension

788
00:53:08,400 --> 00:53:10,690
i don't know what it is

789
00:53:12,330 --> 00:53:14,250
this there

790
00:53:15,320 --> 00:53:19,130
defined by g

791
00:53:19,140 --> 00:53:21,720
data is zero there is no mass

792
00:53:21,750 --> 00:53:24,410
eight or one half of the that year

793
00:53:24,430 --> 00:53:26,540
and you have a minus one

794
00:53:27,480 --> 00:53:28,790
is proportional

795
00:53:28,800 --> 00:53:29,850
this rare

796
00:53:29,860 --> 00:53:33,850
eight is given and c is given and i don't know c

797
00:53:33,890 --> 00:53:35,540
i make no pretence

798
00:53:35,620 --> 00:53:38,960
i can predict how long it will take forty four

799
00:53:39,010 --> 00:53:40,240
all i'm saying is

800
00:53:40,250 --> 00:53:42,820
i can compare two different heights

801
00:53:42,870 --> 00:53:45,010
i can drop an apple from eight meters

802
00:53:45,020 --> 00:53:46,760
and another one from the leaders

803
00:53:46,770 --> 00:53:50,490
and one from a just will take two times longer

804
00:53:50,500 --> 00:53:51,780
and the one from

805
00:53:51,830 --> 00:53:52,710
two meters

806
00:53:54,910 --> 00:53:55,900
h two

807
00:53:55,910 --> 00:53:59,880
for which we put one right if i drop one of eight metres

808
00:53:59,890 --> 00:54:02,980
another one

809
00:54:02,990 --> 00:54:05,960
and the difference in time will be the square root

810
00:54:06,020 --> 00:54:08,800
of the ratio will be twice as long

811
00:54:08,820 --> 00:54:10,760
and that i want to bring

812
00:54:10,760 --> 00:54:12,560
two test

813
00:54:14,580 --> 00:54:16,550
we have set up here

814
00:54:16,560 --> 00:54:19,800
we have an apple there the height of three metres

815
00:54:19,990 --> 00:54:21,520
we know that q

816
00:54:21,600 --> 00:54:25,130
thanks to an accuracy the hyperbolic three millimeters better

817
00:54:25,180 --> 00:54:28,250
and here we have set up by the apple is about one and a half

818
00:54:28,250 --> 00:54:30,080
metres about above the ground

819
00:54:30,140 --> 00:54:31,650
and we know that toolbar

820
00:54:31,700 --> 00:54:33,840
also in that is no better

821
00:54:38,440 --> 00:54:40,540
so let's set it up

822
00:54:40,540 --> 00:54:45,960
i have here

823
00:54:45,990 --> 00:54:48,800
something that's going to be a prediction

824
00:54:48,860 --> 00:54:51,470
prediction of

825
00:54:51,510 --> 00:54:52,540
the prime

826
00:54:52,660 --> 00:54:56,850
it takes one of the line by the time that it takes

827
00:54:56,860 --> 00:54:59,020
we at all

828
00:54:59,030 --> 00:55:01,110
h one

829
00:55:01,130 --> 00:55:03,230
three metres

830
00:55:03,270 --> 00:55:06,100
but i mean there an uncertainty of about

831
00:55:06,160 --> 00:55:07,250
three millimeters

832
00:55:07,250 --> 00:55:08,810
and any better

833
00:55:08,860 --> 00:55:10,950
and h two

834
00:55:10,950 --> 00:55:13,230
one point five metres

835
00:55:13,280 --> 00:55:15,530
and was going to be

836
00:55:15,590 --> 00:55:20,350
the many

837
00:55:20,450 --> 00:55:25,690
so the ratio it's one of eight two

838
00:55:25,740 --> 00:55:28,520
point zero zero zero

839
00:55:28,610 --> 00:55:29,590
now we have to

840
00:55:29,590 --> 00:55:31,960
that is in response to us

841
00:55:31,990 --> 00:55:35,620
since sit situation or stimulus

842
00:55:37,140 --> 00:55:41,550
and a lot is known about that in the typical textbook on a i

843
00:55:41,560 --> 00:55:42,640
will have

844
00:55:42,780 --> 00:55:47,450
long sections about various modern forms of machine learning

845
00:55:47,460 --> 00:55:49,410
and you can look at those and

846
00:55:49,430 --> 00:55:51,510
i'm sure that those are useful

847
00:55:51,520 --> 00:55:54,360
like before i say anything more

848
00:55:54,390 --> 00:55:57,290
what i want to say is

849
00:55:57,300 --> 00:55:59,350
that i may sound very critical

850
00:55:59,440 --> 00:56:01,550
about these things

851
00:56:01,550 --> 00:56:03,240
how come it in white

852
00:56:03,280 --> 00:56:06,430
but in fact all of these methods

853
00:56:06,450 --> 00:56:09,590
are very useful for certain problems

854
00:56:09,650 --> 00:56:12,290
what we don't know in in general is

855
00:56:12,290 --> 00:56:17,090
for what kind of problem is it good to use statistical inference system

856
00:56:17,130 --> 00:56:19,630
for what kinds of problems

857
00:56:19,640 --> 00:56:25,850
what kinds of problems are ones that can be learned and handled by neural networks

858
00:56:25,850 --> 00:56:29,210
or by genetic algorithms or by

859
00:56:34,290 --> 00:56:36,880
what i'm proposing along with

860
00:56:36,900 --> 00:56:40,080
gerry sussman and how labels and

861
00:56:40,090 --> 00:56:43,740
is to develop a new kind of a i system

862
00:56:43,980 --> 00:56:49,220
it has places in which we can insert all of the useful results that

863
00:56:49,220 --> 00:56:53,220
tens of thousands of a i researchers have made

864
00:56:53,270 --> 00:56:57,660
this hasn't been done because of commercial reasons largely namely

865
00:56:57,730 --> 00:56:59,370
each researcher

866
00:56:59,430 --> 00:57:02,900
find themselves trying to show that

867
00:57:02,990 --> 00:57:07,600
this particular learning algorithm is very general and if it's run long enough on the

868
00:57:07,610 --> 00:57:09,160
big enough machine

869
00:57:09,190 --> 00:57:12,230
we'll do everything you want

870
00:57:12,250 --> 00:57:14,040
and because each person

871
00:57:14,150 --> 00:57:17,740
has the goal of showing that he has the

872
00:57:17,740 --> 00:57:21,390
a great new unified theory of intelligence

873
00:57:21,690 --> 00:57:25,980
it isn't true

874
00:57:26,110 --> 00:57:31,270
well each of these systems have limitations and there's been very little theory

875
00:57:31,310 --> 00:57:33,310
about where they fail

876
00:57:35,140 --> 00:57:36,820
however but if you have

877
00:57:36,830 --> 00:57:43,270
investment capitalists supporting your researcher companies financing it

878
00:57:43,300 --> 00:57:48,300
you feel uncomfortable about say by the way this will work or this will work

879
00:57:48,300 --> 00:57:50,040
for x or y

880
00:57:50,110 --> 00:57:52,550
and so we're having a little trouble

881
00:57:52,560 --> 00:57:54,050
in that area

882
00:57:54,100 --> 00:57:57,640
anyway each of these methods works in certain areas

883
00:57:57,660 --> 00:58:01,150
and my goal is to find a way to combine them all so that we

884
00:58:01,150 --> 00:58:06,000
can get a machine that can do the broad range of things that children can

885
00:58:06,910 --> 00:58:09,070
let's take one example

886
00:58:09,430 --> 00:58:14,050
a lot of people are interested in the idea that if you have a problem

887
00:58:14,050 --> 00:58:15,090
to solve

888
00:58:15,100 --> 00:58:16,800
why not make a

889
00:58:16,880 --> 00:58:20,200
and evolutionary ecology

890
00:58:20,200 --> 00:58:23,720
program a lot of machines to try to solve the problem

891
00:58:23,740 --> 00:58:27,470
maybe have a marketplace where they compete you give them a lot of these little

892
00:58:28,980 --> 00:58:34,030
which eventually replaced by successively more difficult problems

893
00:58:34,050 --> 00:58:34,940
and the

894
00:58:34,980 --> 00:58:41,660
programs that get the solution are allowed to survive and make mutated copies of themselves

895
00:58:41,680 --> 00:58:44,300
and the ones that can solve it are wiped out

896
00:58:44,360 --> 00:58:48,120
that's evolution by darwinian selection

897
00:58:48,140 --> 00:58:50,980
and there's a lot of excitement about that because

898
00:58:52,020 --> 00:58:55,850
many such programs have solve problems that look pretty hard

899
00:58:55,910 --> 00:58:59,340
there are some designed little circuits that were

900
00:58:59,350 --> 00:59:02,950
i used fewer components than anybody expected

901
00:59:02,990 --> 00:59:04,470
for example

902
00:59:04,540 --> 00:59:09,940
well it's evolution a good thing

903
00:59:10,010 --> 00:59:14,340
no it's a very powerful thing and given a hundred million years

904
00:59:14,370 --> 00:59:16,000
it's amazing

905
00:59:16,010 --> 00:59:19,060
and quint billions of individuals

906
00:59:19,080 --> 00:59:22,720
all but a few of whom die horrible deaths

907
00:59:23,060 --> 00:59:26,690
it's remarkably successful

908
00:59:26,700 --> 00:59:28,820
but we cannot usually afford

909
00:59:28,880 --> 00:59:33,080
that kind of time and energy

910
00:59:33,960 --> 00:59:35,630
there's actually something

911
00:59:35,690 --> 00:59:38,310
several things fundamentally weak

912
00:59:38,430 --> 00:59:40,990
about darwinian evolution

913
00:59:40,990 --> 00:59:46,500
one thing is that they don't know what they're doing they don't have goals

914
00:59:46,640 --> 00:59:49,640
so most mutations are bad

915
00:59:49,640 --> 00:59:53,760
and we computationally flat in the visual cortex so this is a region of cortex

916
00:59:53,760 --> 01:00:01,540
around the one first primary for the first station of cortical processing visual information

917
01:00:01,630 --> 01:00:07,780
and you can see here the the activation of course in this class cortical representation

918
01:00:08,830 --> 01:00:12,070
kind of amulet stimulus

919
01:00:12,120 --> 01:00:15,830
and here are the different boundaries between the different visual regions and now we can

920
01:00:15,830 --> 01:00:20,650
zoom into one of these regions and look at the orientation preference of each work

921
01:00:21,230 --> 01:00:26,100
on the flattened cortical surface and you can see that here so the grayscale codes

922
01:00:26,110 --> 01:00:30,200
whether voxel prefers the left or the right of the orientation in which you can

923
01:00:30,200 --> 01:00:32,040
see here is that although

924
01:00:32,080 --> 01:00:38,020
the scale of the orientation encoding and the fmri signals is is quite different is

925
01:00:38,020 --> 01:00:40,570
about a factor of eight to ten

926
01:00:40,590 --> 01:00:46,200
still were able to capture some kind of pattern signal that we can then analyse

927
01:00:46,200 --> 01:00:51,370
using multivariate techniques and one of them is of course a pattern recognition but

928
01:00:51,430 --> 01:00:56,920
in this case is just two times orientations but i mean others have done with

929
01:00:56,920 --> 01:01:00,730
more than two types of orientation from that you can you can increase the number

930
01:01:00,730 --> 01:01:04,580
of orientations with problem and it works quite similar it's just not so easy to

931
01:01:04,580 --> 01:01:07,040
you have to use different color featuring tation

932
01:01:07,060 --> 01:01:11,230
people have done with more more than two orientations but i mean these are it's

933
01:01:11,230 --> 01:01:15,590
important to these aliasing patterns is like an old computer screen where you basically the

934
01:01:15,590 --> 01:01:20,810
screen didn't quite match the display grid by the graphics card and then you have

935
01:01:20,810 --> 01:01:23,880
this kind of more patterns and stuff like that so it's a very similar process

936
01:01:24,370 --> 01:01:25,680
that we're using here

937
01:01:25,700 --> 01:01:31,020
for this kind of sampling patterns you can observe the other cases as well so

938
01:01:31,020 --> 01:01:37,480
we've observed in visual cortex also so here red and blue ridge refer to cases

939
01:01:40,150 --> 01:01:44,850
steve subjects seeing the blue versus red stimulus and you can see again but there

940
01:01:44,850 --> 01:01:46,940
seems to be some kind of information

941
01:01:47,010 --> 01:01:52,130
individual voxels related to that already in early visual cortex so this is the region

942
01:01:52,130 --> 01:01:56,400
of the one again the first processing stage so there's information related to the color

943
01:01:56,530 --> 01:02:01,790
and you can see that carries up into the visual stream so if we go

944
01:02:01,790 --> 01:02:05,740
up the visual stream from the central region here we can still see that we

945
01:02:05,740 --> 01:02:11,570
still get these biases so we get a pattern reliable factor also for colour selectivity

946
01:02:11,610 --> 01:02:17,000
and we also get some robust pattern for i selectivity for example in the we

947
01:02:17,000 --> 01:02:21,430
know on the and that there are layers the monocular layers and if we sample

948
01:02:21,430 --> 01:02:25,300
these monocular layers using the fmri voxels what we can see

949
01:02:25,410 --> 01:02:30,040
is that if we stimulate with the right or left i ultimately what we see

950
01:02:30,040 --> 01:02:34,320
is that these voxels have a preference to either the right or left eye and

951
01:02:34,510 --> 01:02:38,520
what the voxel preferences randomly distributed we can get a handle on all these fine

952
01:02:38,520 --> 01:02:42,270
grained neural topographies

953
01:02:42,310 --> 01:02:46,770
one thing to bear in mind when doing this is that we're seeing this

954
01:02:46,780 --> 01:02:49,030
map through the vascular

955
01:02:49,220 --> 01:02:54,690
chair because as i've shown you on one thing we're measuring nearly measuring vascular signal

956
01:02:54,690 --> 01:02:58,230
it's to do with the oxygenation of the blood so ultimately we have to still

957
01:02:58,600 --> 01:03:03,300
feed this through some kind of model of the vasculature which is going to again

958
01:03:03,300 --> 01:03:07,590
downsample the resolution of the of this map

959
01:03:08,450 --> 01:03:15,110
we might increase the lower spatial frequencies for example another possible source

960
01:03:15,260 --> 01:03:21,110
i showed you this model of kind of by sampling model of how we obtain

961
01:03:21,150 --> 01:03:27,090
information pattern information of these high fine grained topographies there are other sources that might

962
01:03:27,090 --> 01:03:31,730
contribute to such biases for example one hypothesis is the core development the functional and

963
01:03:31,730 --> 01:03:35,410
vascular architecture so the idea is that the vessel followed similar

964
01:03:35,430 --> 01:03:37,510
columns with similar preferences

965
01:03:37,530 --> 01:03:42,250
another is large scale biases for example there are people who claim that some of

966
01:03:42,250 --> 01:03:47,970
these orientation by biases are actually due to large-scale biases like radio tangential effect

967
01:03:48,050 --> 01:03:51,390
visual processing but i mean the key thing to bear in mind here is that

968
01:03:51,390 --> 01:03:55,240
we can get the using these larger fmri voxels

969
01:03:55,260 --> 01:03:58,040
OK so the next i'm going to show you how we can

970
01:03:58,050 --> 01:04:00,910
compare the output of the decoder so what we're trying to do is we're trying

971
01:04:00,910 --> 01:04:08,520
to decode orientation perception using pattern recognition algorithms and comparing that to the perceptual accuracy

972
01:04:08,520 --> 01:04:10,150
of the subject

973
01:04:10,170 --> 01:04:14,800
now the problem here is that of course the decoder the fmri decoder will only

974
01:04:14,800 --> 01:04:20,440
give you a lower bound of the information present in the neural system because ultimately

975
01:04:20,440 --> 01:04:26,030
we down sampling the neural systems seem through very coarse resampling grid so it's not

976
01:04:26,030 --> 01:04:30,470
about the information so if we know this information then we can say that likely

977
01:04:30,470 --> 01:04:32,450
and that's equal to m

978
01:04:32,500 --> 01:04:34,370
they are

979
01:04:34,430 --> 01:04:35,830
which is just

980
01:04:35,890 --> 01:04:38,410
the size of a ge divided by

981
01:04:41,750 --> 01:04:45,000
that completes the proof

982
01:04:48,910 --> 01:04:56,520
so there are other universal constructions but this is a particularly elegant line

983
01:04:56,580 --> 01:04:59,040
OK so the point is that

984
01:04:59,080 --> 01:05:03,080
i have m plus one

985
01:05:03,290 --> 01:05:07,850
so are plus one degrees of freedom where each degree of freedom i have m

986
01:05:10,000 --> 01:05:13,020
but if i want to collide

987
01:05:13,040 --> 01:05:15,660
they once i picked

988
01:05:16,950 --> 01:05:18,120
any of the

989
01:05:20,120 --> 01:05:22,160
what i picked

990
01:05:23,600 --> 01:05:25,520
are those

991
01:05:25,540 --> 01:05:28,470
possible choices for the last one is forced

992
01:05:28,490 --> 01:05:31,020
if i want to climb

993
01:05:32,500 --> 01:05:35,220
so therefore the set of functions which to clyde's

994
01:05:35,310 --> 01:05:37,200
there's only one of them

995
01:05:37,250 --> 01:05:38,930
a very slick

996
01:05:40,490 --> 01:05:41,640
very slick

997
01:05:41,680 --> 01:05:43,500
everybody with me here

998
01:05:43,520 --> 01:05:54,560
delist too many people question

999
01:05:54,750 --> 01:05:57,970
part of it is actually this is a quite common type of thing to be

1000
01:05:57,970 --> 01:06:02,220
doing actually if you take class so we're following classes in

1001
01:06:04,410 --> 01:06:07,220
and so forth and this kind of thing of taking

1002
01:06:07,490 --> 01:06:11,290
dot products modulo m

1003
01:06:11,310 --> 01:06:16,850
you know and also get what fields which are are particularly simple finite fields

1004
01:06:16,890 --> 01:06:20,270
and things like that people play with these all the time OK

1005
01:06:20,290 --> 01:06:23,470
so galaxy also like using exorcist here

1006
01:06:23,470 --> 01:06:26,730
so it's the same sort of thing this except these two

1007
01:06:26,790 --> 01:06:30,180
OK and so there's a lot of study this sort of thing so people understand

1008
01:06:30,180 --> 01:06:31,850
these kinds of properties

1009
01:06:32,080 --> 01:06:34,750
but you know it's it's like what's the algorithm four

1010
01:06:34,790 --> 01:06:38,700
having a brilliant insight into our site

1011
01:06:39,600 --> 01:06:45,890
i wish i know that i just turn the crank i

1012
01:06:45,930 --> 01:06:47,270
but for that easy

1013
01:06:48,410 --> 01:06:52,600
i would be standing up here today

1014
01:06:57,120 --> 01:07:00,870
so now i want try take on another topic which is also i find it

1015
01:07:00,890 --> 01:07:02,640
i think this is astounding

1016
01:07:02,680 --> 01:07:05,520
it's is beautiful beautiful

1017
01:07:05,600 --> 01:07:08,660
mathematics and big impact on

1018
01:07:08,750 --> 01:07:10,540
you know your ability to

1019
01:07:10,560 --> 01:07:13,160
build good hash functions

1020
01:07:13,200 --> 01:07:18,580
now i want to talk about another one topic which is related

1021
01:07:18,620 --> 01:07:20,720
which is the topic of

1022
01:07:21,270 --> 01:07:24,580
perfect passion

1023
01:07:34,490 --> 01:07:41,470
everything we've done so far does expected time performance hashing is good and expected

1024
01:07:43,180 --> 01:07:47,350
a perfect hashing addresses the following question suppose that i gave you

1025
01:07:47,350 --> 01:07:48,950
a set of keys

1026
01:07:48,970 --> 01:07:52,870
i suggest build me a static table

1027
01:07:52,870 --> 01:07:54,810
so i can look up

1028
01:07:55,750 --> 01:07:57,970
the keys in the table

1029
01:07:59,310 --> 01:08:00,950
with the worst case

1030
01:08:03,020 --> 01:08:06,020
good worst-case time

1031
01:08:06,060 --> 01:08:08,910
so i have a fixed set of keys

1032
01:08:08,930 --> 01:08:11,640
it might be something like for example

1033
01:08:12,430 --> 01:08:20,270
you know a hundred most common thousand most common words in english

1034
01:08:20,310 --> 01:08:23,830
and when i get a word i want to check quickly in this table

1035
01:08:23,890 --> 01:08:27,140
OK is that what is the word that i've got one of the most common

1036
01:08:27,140 --> 01:08:28,890
words in english

1037
01:08:28,890 --> 01:08:31,830
i would like to do that not with expected

1038
01:08:31,870 --> 01:08:35,250
performance guarantee worst-case performance

1039
01:08:35,270 --> 01:08:38,520
is there a way of building it so that

1040
01:08:38,580 --> 01:08:41,970
so that i can find this

1041
01:08:43,540 --> 01:08:45,230
the problem given

1042
01:08:45,250 --> 01:08:47,270
and he is

1043
01:08:47,450 --> 01:08:55,970
static hash table

1044
01:08:56,060 --> 01:09:01,120
there's no insertion deletion or come put the elements in there

1045
01:09:03,560 --> 01:09:12,060
he calls an

1046
01:09:12,100 --> 01:09:15,910
so i don't want it to be a huge table

1047
01:09:15,950 --> 01:09:17,060
i want to be a spy

1048
01:09:17,060 --> 01:09:20,640
table that is the size of my keys

1049
01:09:20,680 --> 01:09:25,410
table of size m equals order and

1050
01:09:25,410 --> 01:09:26,700
such that

1051
01:09:27,700 --> 01:09:35,370
it takes o one time

1052
01:09:35,450 --> 01:09:40,100
the worst case

1053
01:09:46,390 --> 01:09:48,200
so there's no place

1054
01:09:48,220 --> 01:09:50,350
in the table were going to have

1055
01:09:50,450 --> 01:09:54,470
i know in the average case that's not too hard to do

1056
01:09:54,520 --> 01:09:56,220
but in the worst case

1057
01:09:56,230 --> 01:10:00,700
or make sure that there is no particular spot where

1058
01:10:01,290 --> 01:10:07,060
the number of keys piles up to be a large number

1059
01:10:07,080 --> 01:10:10,560
OK i in no spot should that happen every single search i do should take

1060
01:10:10,560 --> 01:10:12,370
water one time

1061
01:10:12,390 --> 01:10:15,100
they shouldn't be any statistical variation

1062
01:10:15,220 --> 01:10:18,060
in terms of how long it takes me

1063
01:10:18,140 --> 01:10:21,220
to get something

1064
01:10:21,230 --> 01:10:24,500
OK so really understand what the puzzle is

1065
01:10:25,140 --> 01:10:29,020
this is great because this actually ends up having a lot of things

1066
01:10:30,180 --> 01:10:32,770
you know you want to build the table for something

1067
01:10:32,830 --> 01:10:35,560
and you know what the values are the company

1068
01:10:35,700 --> 01:10:37,790
you don't want to spend lot space on

1069
01:10:37,790 --> 01:10:39,230
and so forth

1070
01:10:39,250 --> 01:10:43,950
so the idea here is actually going to be to use the two level scheme

1071
01:10:45,000 --> 01:10:52,060
the idea is raise it to level

1072
01:10:58,540 --> 01:10:59,750
with universal

1073
01:11:02,140 --> 01:11:08,000
at both levels

1074
01:11:10,790 --> 01:11:13,390
so the idea is we're going to harsh

1075
01:11:13,390 --> 01:11:18,020
the hash table and hash into slots rather than using chaining

1076
01:11:18,060 --> 01:11:22,290
we have another hashtable there

1077
01:11:22,350 --> 01:11:24,310
reduce second-hand

1078
01:11:24,370 --> 01:11:27,020
into the second hashtable

1079
01:11:27,810 --> 01:11:30,370
and the idea is there when have

1080
01:11:30,450 --> 01:11:34,080
do it in such a way that we have no collisions

1081
01:11:36,430 --> 01:11:37,580
at level two

1082
01:11:40,410 --> 01:11:45,100
may have collisions at level one will take anything that clyde level one and put

1083
01:11:45,100 --> 01:11:46,500
them into

1084
01:11:46,560 --> 01:11:48,180
hash hashtable

1085
01:11:48,200 --> 01:11:50,680
the second level hashtable but that

1086
01:11:50,700 --> 01:11:52,850
hashtable no collisions

1087
01:11:52,870 --> 01:11:54,870
well we're just gonna

1088
01:11:54,870 --> 01:11:56,540
cash right in

1089
01:11:56,600 --> 01:11:58,660
OK just go boom to its

1090
01:11:58,680 --> 01:12:01,660
was so struck picture this

1091
01:12:02,270 --> 01:12:07,520
illustrate schema

1092
01:12:07,520 --> 01:12:13,910
so we of

1093
01:12:14,000 --> 01:12:17,810
zero one six

1094
01:12:17,810 --> 01:12:19,370
because there's the logicians

1095
01:12:19,380 --> 01:12:22,360
and there's the statisticians and there's not much in the middle

1096
01:12:22,400 --> 01:12:26,390
this is in itself a problem because you the that's the people at the technology

1097
01:12:26,390 --> 01:12:27,760
don't care about it

1098
01:12:27,770 --> 01:12:29,400
just use bayesian networks and made

1099
01:12:29,460 --> 01:12:30,800
do the job

1100
01:12:30,860 --> 01:12:33,330
they can do the job to some kinds of problems

1101
01:12:34,930 --> 01:12:39,130
who forgetting about the uncertainty is the logic and solve some kinds of problems

1102
01:12:39,140 --> 01:12:41,140
you smashing together

1103
01:12:41,970 --> 01:12:45,640
one of the things happening at the moment in fact machine learning leading this

1104
01:12:45,740 --> 01:12:51,830
there's a field called statistical relational learning cities statistical means that graphical modeling population means

1105
01:12:51,850 --> 01:12:53,160
logic part

1106
01:12:53,180 --> 01:12:55,260
and learning is putting together

1107
01:12:55,280 --> 01:12:57,280
so there's a community this working

1108
01:12:57,340 --> 01:12:58,970
in between those two

1109
01:12:58,980 --> 01:13:02,440
in fact the the part of the community is really exciting stuff because

1110
01:13:02,550 --> 01:13:05,140
you need the logic for knowledge representation

1111
01:13:05,140 --> 01:13:08,210
in a dance the probability for handling uncertainty

1112
01:13:08,220 --> 01:13:12,830
but in the formalism to put them together in this way

1113
01:13:18,690 --> 01:13:32,470
i yes i think i is waxed and waned

1114
01:13:32,510 --> 01:13:34,680
there's no doubt that there was

1115
01:13:34,690 --> 01:13:39,010
there was the point where many big companies had dozens or scores of people doing

1116
01:13:39,010 --> 01:13:40,900
i research this was

1117
01:13:41,000 --> 01:13:42,440
in the early eighties

1118
01:13:42,490 --> 01:13:45,050
and there was thing called the winter

1119
01:13:45,120 --> 01:13:47,350
they didn't deliver died

1120
01:13:49,600 --> 01:13:51,270
i think

1121
01:13:51,280 --> 01:13:54,560
the future is extremely promising first of all

1122
01:13:54,580 --> 01:13:58,520
if you add up all the research that the gained all the AI conferences

1123
01:13:58,530 --> 01:14:01,310
from from this the AI conference in machine learning to

1124
01:14:01,340 --> 01:14:02,540
national register the

1125
01:14:02,630 --> 01:14:06,910
there are thousands and thousands and thousands of feels extremely strong

1126
01:14:06,920 --> 01:14:09,250
the second thing is that

1127
01:14:09,300 --> 01:14:13,380
what we talk about human level intelligence and all the rest of the

1128
01:14:13,430 --> 01:14:17,380
we are very far from the what's actually happening is the technology is getting embedded

1129
01:14:17,380 --> 01:14:19,230
in the systems being used

1130
01:14:19,270 --> 01:14:22,480
let's take a typical example that was developed in this campus

1131
01:14:22,520 --> 01:14:24,790
this technology the traction right

1132
01:14:24,800 --> 01:14:27,810
it's going to say the killings of running industry when they

1133
01:14:27,830 --> 01:14:30,040
so the traction rising metallurgy

1134
01:14:30,120 --> 01:14:32,370
just going off to sleep

1135
01:14:32,440 --> 01:14:35,590
it intervene somehow

1136
01:14:35,650 --> 01:14:40,030
so that's the vision technology this cameras is watching this is a technology was developed

1137
01:14:40,030 --> 01:14:42,130
by vision people think on this campus

1138
01:14:42,210 --> 01:14:43,910
other places

1139
01:14:44,650 --> 01:14:49,600
people i think about this as being i i that technology embedded into systems there

1140
01:14:49,600 --> 01:14:52,140
are lots of examples like systems that

1141
01:14:52,190 --> 01:14:55,290
the test when you try and get your credit card

1142
01:14:55,350 --> 01:14:56,760
many liberals

1143
01:14:56,920 --> 01:15:00,860
answer questions to see whether any credit or whatever else is computed in deciding whether

1144
01:15:00,860 --> 01:15:02,890
to give to you

1145
01:15:02,940 --> 01:15:05,680
so there's lots and lots of applications like

1146
01:15:06,780 --> 01:15:08,260
at the level of

1147
01:15:08,310 --> 01:15:12,540
human level intelligence itself it's lovely stuff you can talk about the idea professors against

1148
01:15:12,540 --> 01:15:14,760
you can talk about it was a long way off

1149
01:15:14,780 --> 01:15:16,490
in terms of practicality

1150
01:15:16,500 --> 01:15:19,030
i i was having a big influence

1151
01:15:19,870 --> 01:15:20,910
i think

1152
01:15:20,960 --> 01:15:22,840
i think the answer is that

1153
01:15:22,900 --> 01:15:27,560
i i arrived it's proved to be successful and

1154
01:15:28,940 --> 01:15:32,540
most of the interesting applications we want to solve in the future like to be

1155
01:15:32,540 --> 01:15:35,020
i kinds of things

1156
01:15:35,100 --> 01:15:37,460
so my guess is probably the features

1157
01:15:37,470 --> 01:15:39,790
pretty rising

1158
01:15:45,470 --> 01:15:49,000
before i get into philosophy

1159
01:15:50,180 --> 01:15:53,390
the other thing about i always sits at the intersection

1160
01:15:53,400 --> 01:15:55,640
a whole bunch of other fields

1161
01:15:55,640 --> 01:15:58,680
in fact if you get to when i are conference specializes conference

1162
01:15:58,740 --> 01:16:00,220
you'll see this

1163
01:16:00,590 --> 01:16:03,860
people come to this from lots of different directions

1164
01:16:03,870 --> 01:16:08,540
and big influence from all of the fields that i mentioned there

1165
01:16:10,300 --> 01:16:13,380
i mean obviously

1166
01:16:13,380 --> 01:16:16,050
on the one hand i i has

1167
01:16:16,160 --> 01:16:20,030
benefited enormously from things i mean

1168
01:16:20,040 --> 01:16:22,980
done in other fields

1169
01:16:23,000 --> 01:16:24,900
on the other hand

1170
01:16:24,960 --> 01:16:30,160
it's made a huge contribution itself and the contributions come from my eyes the computational

1171
01:16:32,460 --> 01:16:33,720
many other people

1172
01:16:33,790 --> 01:16:36,410
work in these different things for example statisticians

1173
01:16:36,730 --> 01:16:39,370
but the statisticians do

1174
01:16:39,750 --> 01:16:43,680
so theoretical kinds of things

1175
01:16:43,740 --> 01:16:46,970
and they do some calculations but perhaps not a lot

1176
01:16:47,010 --> 01:16:49,940
the computer scientist the statistician got together

1177
01:16:49,960 --> 01:16:53,470
and what are very heavy computation component

1178
01:16:53,480 --> 01:16:57,300
and change statistics the same is true of physics is the field now called computational

1179
01:16:59,290 --> 01:17:02,540
is to be experimental and theoretical

1180
01:17:02,750 --> 01:17:04,860
this computational

1181
01:17:05,040 --> 01:17:10,230
admittedly this is probably not much of so the computational components really important

1182
01:17:10,240 --> 01:17:13,000
so if you take this fields for example game theory

1183
01:17:13,030 --> 01:17:17,440
thanks very was lively development economist

1184
01:17:18,400 --> 01:17:21,150
in the nineteen forties

1185
01:17:21,190 --> 01:17:23,780
online and people like that

1186
01:17:23,790 --> 01:17:29,120
but the the agents communities brought very heavy computational components that and the using game

1187
01:17:29,120 --> 01:17:32,610
theory computational game theory regions

1188
01:17:32,620 --> 01:17:34,410
so if you get down like going

1189
01:17:34,420 --> 01:17:37,530
i mean the community so i won't get through that almost feels but

1190
01:17:37,590 --> 01:17:39,250
and all of those cases

1191
01:17:39,280 --> 01:17:43,490
you can see that there are big ingredients that have come from those different fields

1192
01:17:43,500 --> 01:17:45,400
and the

1193
01:17:45,520 --> 01:17:49,360
artificial intelligence and a few more generally

1194
01:17:50,780 --> 01:17:52,920
use those things in the system

1195
01:17:52,940 --> 01:18:02,460
well there probably relates back to the BDI

1196
01:18:02,510 --> 01:18:04,380
so the practical reasoning

1197
01:18:04,430 --> 01:18:07,010
insensitive psychological theory

1198
01:18:07,060 --> 01:18:08,610
how humans do

1199
01:18:08,620 --> 01:18:13,660
practical reason

1200
01:18:14,020 --> 01:18:18,000
well this going backwards and forwards

1201
01:18:18,050 --> 01:18:22,520
with mention something else e

1202
01:18:22,580 --> 01:18:26,040
this two from eyes looking at i i you can have two different view points

1203
01:18:26,040 --> 01:18:27,290
of view

1204
01:18:27,350 --> 01:18:30,640
you can either be interested in

1205
01:18:30,670 --> 01:18:33,140
building intelligent systems

1206
01:18:33,150 --> 01:18:36,510
and getting hints from psychology and other fields

1207
01:18:36,510 --> 01:18:40,280
or you can one and how the brain works

1208
01:18:40,290 --> 01:18:43,280
so be inspired by the cognitive science

1209
01:18:43,340 --> 01:18:46,590
so the psychologist one and how the brain works

1210
01:18:46,640 --> 01:18:49,010
benton actually build intelligent systems

1211
01:18:49,050 --> 01:18:51,040
the i engineering kind person

1212
01:18:51,050 --> 01:18:53,610
most of the intelligence system and it is what the way the brain works OK

1213
01:18:54,890 --> 01:18:58,280
OK but there is an it is an interaction between these two

1214
01:18:59,260 --> 01:19:04,000
this the computational science psychology because you can have cognitive models

1215
01:19:04,010 --> 01:19:06,270
that the computationally driven

1216
01:19:06,330 --> 01:19:09,860
somehow model the process the going back in fact all right at the bottom or

1217
01:19:09,860 --> 01:19:11,520
by the end of the neuron

1218
01:19:11,540 --> 01:19:12,460
in now building

1219
01:19:12,510 --> 01:19:14,050
computational models

1220
01:19:15,990 --> 01:19:20,410
after thousands of neurons making tens of thousands of low-level creatures

1221
01:19:20,430 --> 01:19:21,770
low-level organisms

1222
01:19:21,790 --> 01:19:24,870
trying model so the doing computationally so this

1223
01:19:24,870 --> 01:19:28,810
in this way the more general case where psi is to say some of l

1224
01:19:28,810 --> 01:19:33,200
two like you get grouplasso if you got a bunch of separable groups in on

1225
01:19:33,200 --> 01:19:35,020
denoting the group c by

1226
01:19:35,040 --> 01:19:37,200
square brackets little g

1227
01:19:37,210 --> 01:19:39,870
where g belongs to some index big g

1228
01:19:39,890 --> 01:19:41,140
so if the

1229
01:19:41,150 --> 01:19:42,650
if the

1230
01:19:42,700 --> 01:19:48,930
the the subgroups are disjoint if that if you got non overlapping subvectors then it

1231
01:19:48,930 --> 01:19:50,590
is equally easy to

1232
01:19:50,770 --> 01:19:56,150
so the shrinking operator explicitly to evaluate it explicitly in the formulas almost identical to

1233
01:19:56,150 --> 01:19:58,270
the one i just giving you

1234
01:19:58,320 --> 01:20:03,070
OK so it's just as easy and non overlapping group

1235
01:20:03,120 --> 01:20:05,140
the last case

1236
01:20:05,150 --> 01:20:08,020
moving to matrix unknowns

1237
01:20:08,030 --> 01:20:11,960
if you're dealing with the matrix and on fuzzy and if you're shrinking if you're

1238
01:20:11,960 --> 01:20:17,640
a regularisation operator is the nuclear norm as the sum of singular values then the

1239
01:20:17,650 --> 01:20:19,970
shrinkage operator looks like this

1240
01:20:19,990 --> 01:20:22,780
the two norm gets replaced by the frobenius norm

1241
01:20:22,810 --> 01:20:27,750
and his you're a regularisation term it turns out this has an explicit solution you

1242
01:20:27,750 --> 01:20:28,590
can get

1243
01:20:28,600 --> 01:20:31,700
by doing the SVD of like you

1244
01:20:31,720 --> 01:20:35,080
sigma the transpose u and v are orthogonal

1245
01:20:35,140 --> 01:20:37,010
orthonormal at least and

1246
01:20:37,290 --> 01:20:41,180
and sigma is the matrix of the singular values

1247
01:20:41,200 --> 01:20:43,790
given that you can operate on

1248
01:20:43,820 --> 01:20:47,500
singular values in much the same way as you do in the l one case

1249
01:20:47,500 --> 01:20:51,540
now you subtract out times alpha from all of them and cut them off if

1250
01:20:51,540 --> 01:20:53,450
they go below zero

1251
01:20:53,450 --> 01:20:56,780
and you get modified sigma you plug it back into

1252
01:20:56,790 --> 01:21:00,770
multiply by you and the transpose and you get the solution to this problem so

1253
01:21:00,770 --> 01:21:06,000
this is not necessarily cheap to compute certainly straightforward but it's not cheap because of

1254
01:21:06,000 --> 01:21:09,390
course this operation the SVD can be quite expensive

1255
01:21:09,410 --> 01:21:12,640
but you can save some money by

1256
01:21:12,660 --> 01:21:17,710
doing some sort of restricted SVD using some iterative approximate SVD

1257
01:21:17,720 --> 01:21:22,180
just focusing on the singular values that for which the sigma i minus ten times

1258
01:21:22,710 --> 01:21:26,380
it's still going to be positive OK you really don't have to go to the

1259
01:21:26,390 --> 01:21:30,250
trouble of computing the sigma rise if you're going to chop them offered zero anyway

1260
01:21:30,260 --> 01:21:32,620
so you just focus on the positive ones

1261
01:21:32,630 --> 01:21:35,900
so the paper by candes and some collaborators with a

1262
01:21:36,060 --> 01:21:38,640
take that sort of approach

1263
01:21:38,650 --> 01:21:43,640
so how does generalise what i've been talking about already well very very closely related

1264
01:21:43,640 --> 01:21:46,580
to the methods that i talked about in the first part of the talk

1265
01:21:46,580 --> 01:21:53,740
you can recover gradient methods for unconstrained optimisation simply by setting the regularizer to zero

1266
01:21:53,890 --> 01:21:59,180
then what you get is simply a gradient descent method

1267
01:21:59,530 --> 01:22:05,240
it's also related to methods for the constrained problem if i take the regularizer psi

1268
01:22:05,260 --> 01:22:10,030
to be the indicator function for these constraints it may indicator function is the thing

1269
01:22:10,030 --> 01:22:14,600
that zero on omega infinity everywhere else in if i plug that into the definition

1270
01:22:14,600 --> 01:22:17,870
of the shrink operator what i get is the projection operator

1271
01:22:17,880 --> 01:22:20,140
OK projection operator of war

1272
01:22:20,180 --> 01:22:24,270
the projection of y onto omega is just the vector in america its closest to

1273
01:22:24,270 --> 01:22:26,200
y the two norm

1274
01:22:26,240 --> 01:22:27,760
then you get exactly that

1275
01:22:27,770 --> 01:22:29,580
and so

1276
01:22:29,590 --> 01:22:31,830
but really this regularisation setting

1277
01:22:31,890 --> 01:22:34,750
in a sense is a generalisation of a couple of things that we've looked at

1278
01:22:34,750 --> 01:22:35,940
so far

1279
01:22:35,950 --> 01:22:37,580
looked already

1280
01:22:37,640 --> 01:22:42,520
i've already mentioned that last so and this was the subject of this morning's tutorial

1281
01:22:42,570 --> 01:22:46,520
there was a lot of talk about the constrained version of this name the first

1282
01:22:46,520 --> 01:22:47,520
one here

1283
01:22:47,570 --> 01:22:52,390
we put you minimize the sum of squares objective subject to a ban on on

1284
01:22:52,390 --> 01:22:55,970
the one norm of x and as was mentioned there you can come up with

1285
01:22:55,970 --> 01:22:59,880
an equivalent formulation which is the so-called l two l one formulation

1286
01:22:59,890 --> 01:23:03,360
we put the sum of squares and the one norm into the same objective and

1287
01:23:03,360 --> 01:23:06,390
wipe them off by this thing ten

1288
01:23:06,400 --> 01:23:11,510
it's actually kind of hard relate how to t but theoretically you can do that

1289
01:23:11,520 --> 01:23:16,460
if you doing grouplasso you replaced the one norm with the sum of groups

1290
01:23:17,120 --> 01:23:20,880
i just mentioned it when groups overlap is a technique that cropped up in the

1291
01:23:20,880 --> 01:23:24,940
literature in the last few years when groups overlap

1292
01:23:24,960 --> 01:23:28,800
one thing you can do is just to replicate variables that appear in different groups

1293
01:23:29,140 --> 01:23:34,310
and get get yourself an expanded variable set in which the groups don't overlap anymore

1294
01:23:34,310 --> 01:23:38,430
and then you can use these techniques on

1295
01:23:38,460 --> 01:23:42,110
compressed sensing is another thing i think there was a short tutorial by candice two

1296
01:23:42,110 --> 01:23:45,720
or three years ago in this session where you are

1297
01:23:45,990 --> 01:23:51,110
it's the last time a problem but the matrix a has is assumed to have

1298
01:23:51,140 --> 01:23:52,620
a very special structure

1299
01:23:52,640 --> 01:23:54,990
it's assumed to have this restricted isometry

1300
01:23:55,010 --> 01:23:56,890
property and that is that all

1301
01:23:56,910 --> 01:24:00,460
a tall thin column submatrix of a

1302
01:24:00,500 --> 01:24:04,420
where the number of columns in the matrix some pre-defined quantity

1303
01:24:04,480 --> 01:24:07,260
there are assumed to be nearly orthonormal

1304
01:24:07,270 --> 01:24:12,620
and it's known that certain random matrices have this property matrices where the elements are

1305
01:24:12,770 --> 01:24:18,390
i i d guassian all matrices the random roads from the discrete cosine transformation or

1306
01:24:18,390 --> 01:24:21,800
something like that tend to have this property

1307
01:24:21,810 --> 01:24:27,050
the algorithm compares a bit different in compressed sensing because there you usually dealing with

1308
01:24:27,050 --> 01:24:30,680
you want to know what's the performance costumes music and it turns out you can

1309
01:24:30,680 --> 01:24:33,110
bet this at ceylon

1310
01:24:34,760 --> 01:24:36,550
and this result tells you

1311
01:24:36,570 --> 01:24:41,760
how many times we need to observe certain transitions and observation

1312
01:24:41,860 --> 01:24:44,720
in order to have epsilon precision

1313
01:24:44,740 --> 01:24:46,340
so don't worry about the

1314
01:24:46,360 --> 01:24:50,670
details of the expression but you can use this result to figure out how many

1315
01:24:50,670 --> 01:24:52,360
times you need to see

1316
01:24:52,380 --> 01:24:53,680
different counts

1317
01:24:53,700 --> 01:24:57,010
to get an epsilon optimal solution

1318
01:24:57,030 --> 01:25:02,530
in general the answer is a lot of these are really who's found you need

1319
01:25:02,530 --> 01:25:08,010
a lot of counts before these things you really get abseil on approximate solutions let

1320
01:25:08,630 --> 01:25:11,130
tell you a little bit about how we can make these

1321
01:25:11,130 --> 01:25:17,570
a bit more practical in terms of real algorithms for tracking the belief and planning

1322
01:25:17,630 --> 01:25:19,200
the first problem

1323
01:25:19,240 --> 01:25:23,470
is how do we maintain this belief how do we deal with this mixture

1324
01:25:23,490 --> 01:25:25,280
to reach like components

1325
01:25:25,300 --> 01:25:29,400
in particular the different possible count vectors

1326
01:25:31,440 --> 01:25:32,180
and we

1327
01:25:32,200 --> 01:25:38,030
use standard machinery for approximating complex distributions over beliefs

1328
01:25:38,050 --> 01:25:42,200
particle filters are really the way to go here

1329
01:25:42,260 --> 01:25:46,780
the only thing is we need to figure out how much of these different count

1330
01:25:46,780 --> 01:25:51,070
vectors to maintain it's really this is the number of count vectors you need to

1331
01:25:51,070 --> 01:25:56,590
maintain that grows exponentially so we're going to limit that decay different count vectors

1332
01:25:56,630 --> 01:26:00,380
and if you use a simple monte carlo the ability to perform belief update will

1333
01:26:00,380 --> 01:26:05,260
start with a mixture with key components will protect them forward one state according to

1334
01:26:05,260 --> 01:26:09,050
action and observation that was see and then we'll pick keep k of those so

1335
01:26:09,050 --> 01:26:12,700
we always keep k samples that every time it grows but we always bring it

1336
01:26:12,700 --> 01:26:15,920
down to k of them and you just randomly pick up

1337
01:26:15,950 --> 01:26:21,300
if you want to be a little bit more sophisticated instead of keeping take a

1338
01:26:21,300 --> 01:26:25,300
random keep the k with the highest probability

1339
01:26:25,320 --> 01:26:28,670
always a good thing to do it doesn't cost too much to calculate the weight

1340
01:26:29,010 --> 01:26:31,170
and to keep that

1341
01:26:31,170 --> 01:26:36,300
but since we spent as defined in particular spend so much time working out these

1342
01:26:36,300 --> 01:26:38,320
theoretical results

1343
01:26:38,320 --> 01:26:41,070
maybe this would be a good place to use of

1344
01:26:41,110 --> 01:26:45,900
so what the distance metric does is it says OK

1345
01:26:45,940 --> 01:26:48,610
i have my k different count vectors

1346
01:26:48,630 --> 01:26:52,530
i project them forward based on a possible action observation

1347
01:26:52,530 --> 01:26:54,740
i have a lot more of them but i want to keep only care of

1348
01:26:56,110 --> 01:26:57,590
they can

1349
01:26:57,610 --> 01:26:59,950
which best fit the posterior

1350
01:26:59,990 --> 01:27:02,550
according to the distance metric

1351
01:27:02,570 --> 01:27:04,240
we had in theorem one

1352
01:27:04,260 --> 01:27:07,360
so all of these different terms of the counts

1353
01:27:07,380 --> 01:27:10,450
look at these part are these more than k particles

1354
01:27:10,510 --> 01:27:15,420
and pick the ones of them that best span the space

1355
01:27:15,470 --> 01:27:19,760
and theorem what we had was the distance metric in terms of different count vector

1356
01:27:19,780 --> 01:27:22,170
so we can use that readily here

1357
01:27:22,170 --> 01:27:24,510
this is more expensive to compute

1358
01:27:24,530 --> 01:27:29,050
not more by the exponential factor there some kind of linear factor but it is

1359
01:27:29,050 --> 01:27:31,800
definitely more expensive than any of these

1360
01:27:31,800 --> 01:27:35,840
maybe it means we can maintain fewer particles but maybe it gives us much better

1361
01:27:36,920 --> 01:27:41,300
remember that here we're using this belief for planning

1362
01:27:41,320 --> 01:27:46,450
so it's not a standard particle filter where you really need to keep good density

1363
01:27:46,470 --> 01:27:50,720
of you believe you really need to keep the set of beliefs which characterize the

1364
01:27:50,720 --> 01:27:54,280
possible things that could happen so that then you can create a good plan

1365
01:27:54,280 --> 01:27:59,700
so this is what this does is really characterizes the distribution not necessarily proportional but

1366
01:27:59,700 --> 01:28:03,200
it characterizes in way that useful for planning

1367
01:28:03,240 --> 01:28:05,150
now that we have a way to do

1368
01:28:06,950 --> 01:28:12,610
belief maintenance in the system we also knows need some way to do planning

1369
01:28:12,670 --> 01:28:18,920
there's a lot of pompey planning machinery out there has been developed in the last

1370
01:28:18,920 --> 01:28:19,860
few years

1371
01:28:19,880 --> 01:28:25,240
we use something a little bit simpler which is just similar to receding horizon control

1372
01:28:25,260 --> 01:28:27,740
we assume that we do forward search

1373
01:28:27,760 --> 01:28:30,860
this is just the forward heuristic search

1374
01:28:30,880 --> 01:28:33,530
in the space of beliefs

1375
01:28:33,550 --> 01:28:37,150
well when you think believe you have to think physical state and counts

1376
01:28:37,150 --> 01:28:40,440
so if i have a certain physical state and initial counts i could do this

1377
01:28:41,450 --> 01:28:46,400
then also this observation and they'll take me to this distribution of counts and physical

1378
01:28:47,610 --> 01:28:51,510
so this is the forward search in the space of these beliefs

1379
01:28:51,570 --> 01:28:55,610
very efficient depending on the branching factor you may not be able to go very

1380
01:28:55,610 --> 01:28:57,990
far and in this is often what happens to us

1381
01:28:58,010 --> 01:29:00,720
but the belief tracking

1382
01:29:00,740 --> 01:29:04,400
approximate method i showed the previous slide can be used to cool off some of

1383
01:29:04,400 --> 01:29:06,470
these branches

1384
01:29:06,490 --> 01:29:10,680
so this is what we do this hasn't quite made it onto the robot or

1385
01:29:10,700 --> 01:29:15,420
the medical devices yet we worked with a very

1386
01:29:15,420 --> 01:29:20,700
simplified problem inspired by human robot interaction the idea here is that the robot has

1387
01:29:20,700 --> 01:29:24,150
the following individual within some kind of known

1388
01:29:24,170 --> 01:29:29,150
discretized environment but it turns out there's two different individuals that the robot can follow

1389
01:29:29,300 --> 01:29:32,990
in these individual really differ in terms of their motion model one of them is

1390
01:29:33,010 --> 01:29:37,610
graceful able to walk that walk good speed and the other one is more handling

1391
01:29:37,610 --> 01:29:39,970
and walking much slower

1392
01:29:39,990 --> 01:29:44,490
robot knows two individuals but the robot doesn't have the preset

1393
01:29:44,490 --> 01:29:47,050
model of how these people

1394
01:29:47,940 --> 01:29:50,700
so the robot simultaneously has to

1395
01:29:50,760 --> 01:29:54,420
track this individual without losing track of

1396
01:29:54,420 --> 01:29:59,540
learn what is the motion model of this individual also so take actions that are

1397
01:29:59,540 --> 01:30:01,110
able to do that

1398
01:30:01,130 --> 01:30:04,110
you might not want to learn the model perfectly if it happens that the two

1399
01:30:04,110 --> 01:30:07,830
of them move in similar ways that you can aggregate model but somehow do this

1400
01:30:07,830 --> 01:30:13,990
simultaneously learning and planning and so the bayesian framework is quite good for this

1401
01:30:14,050 --> 01:30:16,740
what i show you here is

1402
01:30:17,030 --> 01:30:20,880
the effect of the the choice of belief

1403
01:30:20,930 --> 01:30:24,760
belief maintenance approximation algorithm

1404
01:30:24,780 --> 01:30:28,570
so what happens if you have the exact model

1405
01:30:28,590 --> 01:30:32,590
and you use simple from the planning on this site that bigger problem you can

1406
01:30:32,590 --> 01:30:34,070
do that quite readily

1407
01:30:34,110 --> 01:30:37,220
and this is the optimal return

1408
01:30:37,220 --> 01:30:41,010
now if you take your prior model and don't do any learning

1409
01:30:41,030 --> 01:30:43,820
and just plan based on your prior model

1410
01:30:43,840 --> 01:30:45,720
this is how well you're going to do

1411
01:30:45,720 --> 01:30:48,390
model on

1412
01:30:49,630 --> 01:30:55,400
twenty news group data four categories autos models that we have

1413
01:30:55,930 --> 01:30:58,900
each taking on what is the document

1414
01:30:58,920 --> 01:31:03,640
and in this case politics classes so we know the class location but we don't

1415
01:31:03,640 --> 01:31:07,270
have a lot of about classification and we get a reasonable

1416
01:31:07,280 --> 01:31:08,900
recovery of the

1417
01:31:08,970 --> 01:31:11,840
extra classes based on the clusters

1418
01:31:11,850 --> 01:31:13,850
so it's not perfect

1419
01:31:14,060 --> 01:31:17,120
which should answer can be

1420
01:31:17,130 --> 01:31:22,240
another markets there there is a similar model for this was developed at the same

1421
01:31:22,240 --> 01:31:28,710
time the group of john blanton but they instead of using the

1422
01:31:28,770 --> 01:31:34,010
marking the location they face model in the truncated stick breaking

1423
01:31:34,030 --> 01:31:35,450
the procedure

1424
01:31:35,470 --> 01:31:42,350
so slightly different unfortunately it didn't compare it to clean the different approaches

1425
01:31:44,440 --> 01:31:51,980
so there are a lot more work also in machine learning community about these

1426
01:31:54,010 --> 01:31:58,930
one is again focusing on the issue of determining

1427
01:31:58,940 --> 01:32:07,020
automatically determining the right number of clusters in the domain illustrated previously in this example

1428
01:32:07,040 --> 01:32:09,170
the gaussian mixture models

1429
01:32:09,190 --> 01:32:15,980
and this has been explored the first by congressman who infected this infinite mixture of

1430
01:32:18,130 --> 01:32:19,210
in a minute

1431
01:32:20,070 --> 01:32:26,560
then this can be extended to infinite mixture of many things like also processes which

1432
01:32:26,830 --> 01:32:31,790
was the paper as most money and also in markov networks

1433
01:32:31,810 --> 01:32:33,920
the money

1434
01:32:33,940 --> 01:32:36,030
in two

1435
01:32:36,080 --> 01:32:37,490
so you again

1436
01:32:37,930 --> 01:32:43,570
it was not so much the sharing states effect it was small the clustering

1437
01:32:43,590 --> 01:32:48,300
one is interested in

1438
01:32:48,310 --> 01:32:51,600
and then our own group has

1439
01:32:51,660 --> 01:32:59,700
studying the application of this idea to relational modeling so we see later modelling is

1440
01:32:59,880 --> 01:33:05,810
sort of a generalisation of a hierarchical bayesian approach and the idea of a relational

1441
01:33:05,810 --> 01:33:12,050
modeling is it useful to find the truth for representation of data data structures represented

1442
01:33:12,050 --> 01:33:14,530
for example in relational databases

1443
01:33:14,540 --> 01:33:17,350
so you want to some of the

1444
01:33:17,360 --> 01:33:19,740
the probability of relation between objects

1445
01:33:19,760 --> 01:33:27,840
you want to predict attributes of objects sometimes also as discussed by thomas hofmann

1446
01:33:29,200 --> 01:33:34,540
tell you much more information about an object then the properties of the objects which

1447
01:33:34,540 --> 01:33:37,510
is for example used in ranking algorithms

1448
01:33:37,530 --> 01:33:41,520
search engines and we applied to

1449
01:33:41,550 --> 01:33:45,470
two to medical domain where you want to predict the

1450
01:33:45,490 --> 01:33:53,420
assignment of patients who procedures of patient diagnosis based on primary diagnosis and procedures which

1451
01:33:53,420 --> 01:33:59,250
have been described already we've got quite good design so we were most interested in

1452
01:33:59,380 --> 01:34:05,380
shown strength suspect that if we have similar patients in the database we because expect

1453
01:34:05,420 --> 01:34:11,410
they also tell us something about the procedures and diagnosis for the active

1454
01:34:11,460 --> 01:34:16,500
we're trying to predict for that another group

1455
01:34:16,520 --> 01:34:22,390
and obama co-workers have tried to use this

1456
01:34:22,410 --> 01:34:29,250
as the clustering approach to the clustering effect of these processes to cluster database based

1457
01:34:29,250 --> 01:34:33,540
on the relations of the data center again that is not that actually works out

1458
01:34:34,120 --> 01:34:40,040
meaningful but maybe relations between objects are more relevant also for determining

1459
01:34:40,080 --> 01:34:41,890
think this is a very

1460
01:34:43,940 --> 01:34:45,260
and research

1461
01:34:45,270 --> 01:34:46,110
in particular

1462
01:34:46,120 --> 01:34:48,560
commit combining these relational models

1463
01:34:48,570 --> 01:34:56,460
was prominent modelling be quite interesting idea because you have a lot of relations you

1464
01:34:56,460 --> 01:35:01,330
can explore and a lot of the prime minister he inherited between the different stations

1465
01:35:01,340 --> 01:35:03,850
of the parameters

1466
01:35:03,860 --> 01:35:13,490
the basic approach is based on the probabilistic relational model approach that color which showed

1467
01:35:14,300 --> 01:35:21,900
translate these data from relational database into probabilistic model

1468
01:35:21,910 --> 01:35:24,470
o or already conclusions

1469
01:35:24,480 --> 01:35:33,760
so one point is that nonparametric bayes modelling allows for a flexible models

1470
01:35:33,780 --> 01:35:35,460
and development

1471
01:35:35,480 --> 01:35:40,700
hierarchical modeling effect showing strains between models and also

1472
01:35:41,210 --> 01:35:44,170
they have been application is the

1473
01:35:44,260 --> 01:35:51,150
clustering idea of them executed being able to automatically determine the correct number of clusters

1474
01:35:51,270 --> 01:35:55,010
the most important thing is usually actually a mixture model

1475
01:35:55,020 --> 01:35:57,520
we don't observe samples of

1476
01:35:58,880 --> 01:36:03,530
itself derived quantities like data

1477
01:36:03,540 --> 01:36:05,710
the model

1478
01:36:06,820 --> 01:36:09,390
a large set of history on

1479
01:36:09,410 --> 01:36:11,950
nonparametric modeling

1480
01:36:11,970 --> 01:36:15,220
and statistics and this is something

1481
01:36:15,230 --> 01:36:17,580
one can explore further

1482
01:36:17,590 --> 01:36:20,830
the only thing conferences

1483
01:36:20,880 --> 01:36:27,420
but based on decisions are typically concerned with a low dimensional problems and the challenges

1484
01:36:27,430 --> 01:36:31,040
in machine learning sort of to get these things to work also for

1485
01:36:31,050 --> 01:36:36,710
for high dimensional problems and that some some problems

1486
01:36:36,730 --> 01:36:38,930
the machine learning community and the

1487
01:36:38,950 --> 01:36:40,260
o thing

1488
01:36:40,280 --> 01:36:43,070
actually i think that the new was the first one two

1489
01:36:43,120 --> 01:36:45,730
pay attention to these type of models and that

1490
01:36:45,740 --> 01:36:48,800
rasmussen and williams

1491
01:36:48,810 --> 01:36:55,170
the money these people develop is infinite mixture models and then and now it's becoming

1492
01:36:55,170 --> 01:36:56,670
more and more

1493
01:36:56,820 --> 01:37:01,580
i don't think it's become mainstream but the larger group of people interested in this

1494
01:37:01,800 --> 01:37:08,680
case they were there was a makers on the last NIPS conference which you can

1495
01:37:09,760 --> 01:37:12,220
on the web and which is

1496
01:37:16,890 --> 01:37:19,160
a lot of references to different

1497
01:37:19,180 --> 01:37:24,250
work in statistics and there's also a good time by so big

1498
01:37:24,270 --> 01:37:26,600
money available on the web

1499
01:37:26,620 --> 01:37:29,510
which is a little bit

1500
01:37:29,520 --> 01:37:31,640
easier to follow

1501
01:37:31,690 --> 01:37:33,960
the most comprehensive

1502
01:37:33,980 --> 01:37:36,210
there's also a paper

1503
01:37:36,230 --> 01:37:39,780
three quarters of introduction to this

1504
01:37:39,820 --> 01:37:41,540
the main

1505
01:37:41,550 --> 01:37:45,760
so it seems to be the still growing into

1506
01:37:45,780 --> 01:37:50,790
businesses going and of course people of a creative develop a lot of

1507
01:37:50,800 --> 01:37:53,980
different variations of existing approaches

1508
01:37:54,000 --> 01:37:58,180
one big area of research believe

1509
01:38:03,100 --> 01:38:06,040
to be able to provide the same thing

1510
01:38:06,060 --> 01:38:07,600
which in

1511
01:38:07,610 --> 01:38:11,860
many interesting one very difficult to to to mix well

1512
01:38:11,860 --> 01:38:12,840
and now

1513
01:38:13,300 --> 01:38:16,050
what we what we

1514
01:38:20,710 --> 01:38:21,880
OK with

1515
01:38:21,920 --> 01:38:25,820
wrong here this be of HIV given

1516
01:38:25,940 --> 01:38:28,130
hi h the

1517
01:38:32,170 --> 01:38:37,980
the reverse of the of the problem here your h i for about

1518
01:38:38,150 --> 01:38:40,770
should here it i even be the

1519
01:38:50,360 --> 01:38:58,400
i had about this just happened to be the posterior probability for the parameters for

1520
01:38:58,400 --> 01:38:59,380
each model

1521
01:38:59,420 --> 01:39:04,920
OK so the same expression as before except that now everywhere conditioning on a

1522
01:39:06,840 --> 01:39:07,570
and now

1523
01:39:08,170 --> 01:39:16,130
the normalizing constant of the we had four here would just be given a child

1524
01:39:16,130 --> 01:39:20,070
which again is called the marginal likelihood or evidence

1525
01:39:20,110 --> 01:39:23,400
four four model h i

1526
01:39:23,420 --> 01:39:25,750
and what i want to know is i want to know

1527
01:39:25,770 --> 01:39:28,270
what is the probability of the different models

1528
01:39:29,610 --> 01:39:31,630
my observations

1529
01:39:31,670 --> 01:39:34,840
and i use bayes rule against the swap around

1530
01:39:35,300 --> 01:39:38,940
the median age was going to be the

1531
01:39:39,520 --> 01:39:42,340
marginal likelihood

1532
01:39:42,380 --> 01:39:43,610
for each i

1533
01:39:43,670 --> 01:39:47,030
now he of HIV one of my prior

1534
01:39:47,050 --> 01:39:50,320
over time but maybe that before i start

1535
01:39:50,320 --> 01:39:52,570
i actually think that one of the one of the model

1536
01:39:52,570 --> 01:39:55,400
there's a lot more likely to be true then the other one

1537
01:39:55,420 --> 01:39:58,340
they can rule the other one out maybe the data

1538
01:39:58,360 --> 01:40:01,320
really say one then maybe i would change my mind

1539
01:40:01,400 --> 01:40:02,750
i can encode

1540
01:40:02,770 --> 01:40:05,420
again my my

1541
01:40:05,420 --> 01:40:09,320
why prior beliefs using

1542
01:40:09,480 --> 01:40:12,270
using the prior year

1543
01:40:12,290 --> 01:40:17,150
and the and the and the marginal likelihood here is just the normalizing constant

1544
01:40:17,230 --> 01:40:18,150
of the

1545
01:40:18,170 --> 01:40:21,170
all the previous stages of inference

1546
01:40:21,210 --> 01:40:27,710
and the predictive inference just just the normalizing constant of the likelihood that the part

1547
01:40:27,730 --> 01:40:34,460
we're trying to do that an example

1548
01:40:35,290 --> 01:40:36,070
OK so

1549
01:40:36,090 --> 01:40:41,030
so this is going to be this boring example again we cannot compute everything

1550
01:40:41,270 --> 01:40:42,650
now we have two

1551
01:40:42,670 --> 01:40:44,000
different models

1552
01:40:44,020 --> 01:40:45,610
two different learners

1553
01:40:45,650 --> 01:40:48,340
i call them to do to be able to machine learning

1554
01:40:48,460 --> 01:40:53,440
the one that i believe that the coin is fair actually not really a very

1555
01:40:53,500 --> 01:40:54,440
good morning

1556
01:40:54,480 --> 01:40:57,520
just believe that the coin is fair

1557
01:40:57,520 --> 01:41:03,150
one of the i believe something to learn something to believe that

1558
01:41:03,520 --> 01:41:08,420
all possible that all of that is up i mean one

1559
01:41:08,500 --> 01:41:10,960
are are equally plausible

1560
01:41:12,230 --> 01:41:18,050
the particular kind of prior belief that the right belief written as the probability distribution

1561
01:41:18,050 --> 01:41:22,230
would look like this all about higher equally likely

1562
01:41:28,090 --> 01:41:34,030
it turns out that it's convenient to write the prior distribution in terms of of

1563
01:41:34,170 --> 01:41:35,840
the probability distribution

1564
01:41:35,860 --> 01:41:38,730
turns out that uniform on the interval like this

1565
01:41:38,750 --> 01:41:40,920
exactly what don't answer

1566
01:41:40,920 --> 01:41:45,570
so beta one one distribution or the distribution are written down expression for beta distribution

1567
01:41:46,520 --> 01:41:50,840
was very closely related to the the the and only random variable

1568
01:41:50,860 --> 01:41:55,380
so it is the normalizing constant constant here depend on high

1569
01:41:55,480 --> 01:41:56,700
it just makes

1570
01:41:57,520 --> 01:42:00,340
probability over time integral

1571
01:42:00,940 --> 01:42:03,900
one of the problem problem

1572
01:42:03,900 --> 01:42:07,190
and then eventually hide some power

1573
01:42:07,210 --> 01:42:10,440
minus one where the problem of the distribution

1574
01:42:10,440 --> 01:42:12,250
one modified power

1575
01:42:12,270 --> 01:42:15,400
so what want to talk rather than b

1576
01:42:15,420 --> 01:42:20,070
these distributions can look in various ways it by five

1577
01:42:22,420 --> 01:42:25,270
you know any value in in the central area

1578
01:42:25,270 --> 01:42:27,520
it is reasonably reasonably

1579
01:42:28,880 --> 01:42:33,110
and that values below a o point one or o point nine

1580
01:42:33,170 --> 01:42:34,880
quite unlikely

1581
01:42:34,880 --> 01:42:36,000
you can also have

1582
01:42:36,000 --> 01:42:41,500
you can hear think that that is about o point eight quite like the sort

1583
01:42:41,550 --> 01:42:44,270
of things are

1584
01:42:45,300 --> 01:42:50,380
so the the the beta distribution here is called the conjugate prior

1585
01:42:50,420 --> 01:42:51,940
because it has the same form

1586
01:42:51,960 --> 01:42:53,860
that makes analysis

1587
01:42:53,880 --> 01:42:57,460
are you

1588
01:42:58,840 --> 01:42:59,500
all right

1589
01:42:59,550 --> 01:43:01,460
so let's try to make them up

1590
01:43:01,480 --> 01:43:05,020
so i made observations i got

1591
01:43:05,090 --> 01:43:07,340
may ten observations of b

1592
01:43:07,360 --> 01:43:09,170
we had been

1593
01:43:09,210 --> 01:43:11,190
and seven tales

1594
01:43:12,610 --> 01:43:18,090
now question is what to my to learn what they want to do

1595
01:43:18,090 --> 01:43:25,400
there are no it which was really not perfect half again the north central

1596
01:43:25,800 --> 01:43:28,670
what about what about one of the

1597
01:43:28,730 --> 01:43:30,800
i should like to do is actually work out the

1598
01:43:30,820 --> 01:43:32,730
the posterior distribution

1599
01:43:33,860 --> 01:43:36,000
the posterior distribution

1600
01:43:36,030 --> 01:43:37,570
i got by multiplying the

1601
01:43:38,650 --> 01:43:40,840
which was that beta one one

1602
01:43:40,860 --> 01:43:42,460
with the

1603
01:43:42,880 --> 01:43:44,550
with with the likelihood

1604
01:43:44,570 --> 01:43:45,940
the likelihood here

1605
01:43:45,940 --> 01:43:50,070
it's going to be bernoulli likelihood so it's high

1606
01:43:50,110 --> 01:43:52,840
the power three

1607
01:43:56,320 --> 01:44:03,570
woman by the power set

1608
01:44:03,570 --> 01:44:04,420
the product

1609
01:44:06,520 --> 01:44:10,480
amounts to saying that k is what people call a reproducing kernel

1610
01:44:10,500 --> 01:44:11,760
so you

1611
01:44:11,780 --> 01:44:15,580
take the dot product with k between k itself and you can get k

1612
01:44:15,590 --> 01:44:20,860
it's the reproducing kernel property and that's why these spaces will be called reproducing kernel

1613
01:44:20,860 --> 01:44:22,840
hilbert spaces

1614
01:44:22,860 --> 01:44:27,540
so these are some interesting special cases now let's go back to

1615
01:44:27,560 --> 01:44:31,270
proving that the dot product

1616
01:44:31,400 --> 01:44:35,720
this is something that i'll show you on this slide it's a little bit more

1617
01:44:35,720 --> 01:44:38,920
tricky so i wouldn't put this one is an exercise

1618
01:44:38,960 --> 01:44:43,690
and it works as follows we first show that this thing is a positive definite

1619
01:44:45,080 --> 01:44:47,160
on the set of functions

1620
01:44:47,160 --> 01:44:50,910
and then since we know some properties of positive definite kernels

1621
01:44:50,980 --> 01:44:54,010
then we can show that is actually

1622
01:44:54,320 --> 01:44:59,740
dot product which is a little bit more than a positive definite kernel

1623
01:44:59,880 --> 01:45:01,220
OK so

1624
01:45:01,220 --> 01:45:05,040
i define a set of functions which looks like this

1625
01:45:05,060 --> 01:45:08,460
so linear combinations of kernels sitting on on

1626
01:45:08,760 --> 01:45:09,860
some points

1627
01:45:12,160 --> 01:45:14,090
o six o

1628
01:45:14,110 --> 01:45:19,270
compute this quantity here so i'll take some different functions f five j coefficients gamma

1629
01:45:19,270 --> 01:45:25,520
i gamma j i think before when i find positive definite kernels i was using

1630
01:45:25,520 --> 01:45:27,100
a different notation

1631
01:45:27,240 --> 01:45:29,250
let's just go back

1632
01:45:29,260 --> 01:45:31,090
this show is the same

1633
01:45:35,280 --> 01:45:41,870
and inequality like this one so substituting the kernel that that have to inequality and

1634
01:45:41,870 --> 01:45:44,760
my positions are not common

1635
01:45:44,800 --> 01:45:47,490
and we have a situation where

1636
01:45:47,490 --> 01:45:57,580
my set x is actually a function space but nothing prevents me from doing that

1637
01:45:57,740 --> 01:46:03,680
so my my set x minu ctx x would be this function space don't confuse

1638
01:46:03,710 --> 01:46:04,550
with this one

1639
01:46:04,780 --> 01:46:06,970
and i'm going to check

1640
01:46:06,990 --> 01:46:08,670
this quantity here

1641
01:46:08,690 --> 01:46:11,440
so first of all end

1642
01:46:11,450 --> 01:46:14,910
very much a laser pointers dying again this quantity here

1643
01:46:16,940 --> 01:46:21,450
this angular brackets denote the thing that defined before

1644
01:46:21,470 --> 01:46:25,730
this thing here so far we only know it's symmetric by linear form

1645
01:46:25,740 --> 01:46:28,580
so i'm going to use this image by linear form

1646
01:46:28,630 --> 01:46:33,460
substitute the function five train hit a coefficients gamma i gamma j

1647
01:46:33,470 --> 01:46:38,090
i know it's by linear so i can put these coefficients insight into some inside

1648
01:46:38,110 --> 01:46:40,220
and then again

1649
01:46:40,220 --> 01:46:42,800
i get an expression like this

1650
01:46:43,120 --> 01:46:47,930
now this is a certain function that's actually the same function written with different

1651
01:46:47,940 --> 01:46:51,860
some summation index or call this function f

1652
01:46:51,910 --> 01:46:54,400
and this is this is actually

1653
01:46:54,410 --> 01:46:58,120
by linear form evaluated on f and f

1654
01:46:58,150 --> 01:46:59,250
now we know

1655
01:46:59,260 --> 01:47:03,310
these functions are my set of functions can be expanded in terms of kernels

1656
01:47:03,330 --> 01:47:07,190
so i'm going to expand the function f in terms of kernels

1657
01:47:07,200 --> 01:47:10,970
expand this function and i'll expand this function

1658
01:47:11,060 --> 01:47:15,790
same function were named one of the in this is in the j

1659
01:47:15,820 --> 01:47:17,420
and then again

1660
01:47:17,460 --> 01:47:21,180
use the linearity of this idea of form

1661
01:47:21,230 --> 01:47:23,100
and again this is this

1662
01:47:23,120 --> 01:47:25,070
quantity here so

1663
01:47:25,360 --> 01:47:27,300
converted this

1664
01:47:27,340 --> 01:47:29,980
top thing

1665
01:47:30,000 --> 01:47:31,760
into this one here

1666
01:47:31,780 --> 01:47:34,880
and this one of course i know is nonnegative because

1667
01:47:34,900 --> 01:47:37,650
i'm assuming that k is a positive definite kernel

1668
01:47:37,710 --> 01:47:42,820
so from the fact that k is a positive definite kernel it follows that also

1669
01:47:42,820 --> 01:47:48,070
this thing here that so far we've got the symmetry by linear form is positive

1670
01:47:48,070 --> 01:47:49,050
definite kernel

1671
01:47:49,070 --> 01:47:54,650
the only difference being that k is a positive definite kernel on this domain x

1672
01:47:55,880 --> 01:47:58,550
this thing is a positive definite kernel

1673
01:47:58,570 --> 01:48:01,070
one some function space

1674
01:48:02,030 --> 01:48:05,280
in a little bit confusing but actually it's quite elementary

1675
01:48:05,340 --> 01:48:10,600
now that's nice but let's go a bit further so now we know it is

1676
01:48:10,600 --> 01:48:13,780
symmetric bilinear form and the positive definite kernel

1677
01:48:13,840 --> 01:48:17,360
for the product we just need a little bit more we need the fact that

1678
01:48:17,360 --> 01:48:22,650
it strictly positive definite so we need to show that if there is a function

1679
01:48:23,400 --> 01:48:24,670
when we

1680
01:48:24,670 --> 01:48:26,900
take this

1681
01:48:26,900 --> 01:48:28,530
symmetric by linear

1682
01:48:28,530 --> 01:48:31,940
come up with itself

1683
01:48:31,960 --> 01:48:35,860
it gives the value zero then this function has to be identically zero

1684
01:48:35,860 --> 01:48:39,670
if we can show that then we know it is strictly positive

1685
01:48:39,670 --> 01:48:40,380
and to

1686
01:48:40,460 --> 01:48:43,920
to show this we proceed as follows we compute the value of the function at

1687
01:48:43,920 --> 01:48:46,710
some point x in square it

1688
01:48:46,730 --> 01:48:50,480
next i'm going to use this reproducing property so

1689
01:48:50,500 --> 01:48:54,980
the value of the function at some point x can be written as the

1690
01:48:55,000 --> 01:48:59,570
these angular brackets of the function with the kernel centered on x

1691
01:48:59,590 --> 01:49:02,250
i'm going to do this and the next slide

1692
01:49:02,280 --> 01:49:04,130
right f of x is this

1693
01:49:04,130 --> 01:49:08,030
the angular brackets thing that i'm not allowed to call the dot product

1694
01:49:08,250 --> 01:49:13,360
and because i still have the square next i'm going to use the local she

1695
01:49:13,360 --> 01:49:16,320
schwartz inequality that you've proven before

1696
01:49:16,320 --> 01:49:21,190
so the question is what inequality gives the way of bonding this quantity here

1697
01:49:22,500 --> 01:49:24,940
these records of f with itself

1698
01:49:24,960 --> 01:49:28,000
and the facts between the kernel and the kernel

1699
01:49:28,030 --> 01:49:30,960
now the back between the kernel and the kernel

1700
01:49:31,020 --> 01:49:33,750
again just gives you kernel

1701
01:49:33,760 --> 01:49:38,150
this the second problem the second property in this slide can with kernel gives you

1702
01:49:39,030 --> 01:49:41,320
so i will do that

1703
01:49:41,340 --> 01:49:43,000
to get to kernel here

1704
01:49:43,130 --> 01:49:47,170
and now let's step back for a second

1705
01:49:47,760 --> 01:49:49,150
what does this show

1706
01:49:49,170 --> 01:49:50,750
this shows that whenever

1707
01:49:50,760 --> 01:49:53,880
the angular brackets of f with itself

1708
01:49:53,900 --> 01:49:55,570
is zero

1709
01:49:55,590 --> 01:50:01,460
and the right inside his completely zero then f of x squared is zero

1710
01:50:01,480 --> 01:50:03,570
therefore f x is zero

1711
01:50:03,670 --> 01:50:08,250
and this is independent of which point x i choose to four points x i

1712
01:50:08,250 --> 01:50:11,020
could choose f of x is still

1713
01:50:11,020 --> 01:50:16,500
so this means whenever this quantity is zero the function f is identically zero

1714
01:50:16,530 --> 01:50:21,420
this means it strictly positive definite so the dot product

1715
01:50:21,440 --> 01:50:26,820
so we have now constructed let's just summarize little bit we've

1716
01:50:26,840 --> 01:50:30,820
difference with defined and that in the space of functions

1717
01:50:31,050 --> 01:50:36,880
we defined the product on this map the dot product has the property that

1718
01:50:36,900 --> 01:50:42,320
the kernel corresponds to the dot product between two points mapped into that space

1719
01:50:42,630 --> 01:50:45,650
the problem that the dot product

1720
01:50:45,670 --> 01:50:49,170
so we are now in a dot product space

1721
01:50:49,190 --> 01:50:52,710
and the only thing that's left and we're not going to worry about this is

1722
01:50:52,710 --> 01:50:55,610
to complete the space in the corresponding norm

1723
01:50:55,690 --> 01:51:01,710
and so each dot product uses on and then you get what's called

1724
01:51:01,710 --> 01:51:02,690
hilbert space

1725
01:51:02,710 --> 01:51:05,020
and if the reproducing kernel hilbert space so

1726
01:51:05,020 --> 01:51:09,900
so this is a particular kind of hilbert spaces the space point evaluation is a

1727
01:51:09,900 --> 01:51:14,620
continuous function and can therefore be represented as a dot product

1728
01:51:14,630 --> 01:51:20,590
so we'll come back to use this later on this kind of point evaluation expression

1729
01:51:20,600 --> 01:51:24,520
now let's go back to this example from before and do the same thing in

1730
01:51:24,520 --> 01:51:26,340
the feature space

1731
01:51:26,360 --> 01:51:32,130
so now we want to classify points match into the feature space again by computing

1732
01:51:32,150 --> 01:51:34,290
distances between a test point

1733
01:51:34,310 --> 01:51:38,820
and the mean of the positive class in the middle of the negative class if

1734
01:51:38,820 --> 01:51:41,840
we do this and we work it out again so this is the separating hyperplane

1735
01:51:41,840 --> 01:51:45,380
this time in the feature space in the properties and kernel hilbert space and if

1736
01:51:45,380 --> 01:51:49,840
we work it out terms of this is a nice expression in terms of kernels

1737
01:51:49,850 --> 01:51:52,140
the decision function will be

1738
01:51:52,180 --> 01:51:59,290
thresholded version of a kernel expansion with kernels sitting on the positive and negative points

1739
01:51:59,380 --> 01:52:01,140
and it would be such that here

1740
01:52:01,190 --> 01:52:05,910
this thing if k is the density differences and we can think of this as

1741
01:52:05,910 --> 01:52:09,830
a thousand windows estimate of the positive class and we can think of this as

1742
01:52:09,850 --> 01:52:12,620
possible in estimate of the negative class

1743
01:52:12,640 --> 01:52:18,380
so this is the parzen windows classifier based on a plug-in estimates

1744
01:52:18,390 --> 01:52:22,610
one thousand windows of the two classes

1745
01:52:22,780 --> 01:52:24,190
the support vector machine is

1746
01:52:24,200 --> 01:52:28,740
quite similar it's also separating hyperplane is such a feature space

1747
01:52:28,760 --> 01:52:33,980
which solves the nonlinear classification problem in the input domain and

1748
01:52:34,070 --> 01:52:38,420
depending on what kind of come we use if we turn off the nonlinearity the

1749
01:52:38,420 --> 01:52:46,910
decision boundary in the input space will become increasingly nonlinear

1750
01:52:50,980 --> 01:52:55,450
so you can see the behaviour is a little bit similar to these behaviors that

1751
01:52:55,450 --> 01:52:56,660
you get if you change

1752
01:52:56,710 --> 01:53:03,170
regularisation constant some of you who know this kind of work

1753
01:53:04,770 --> 01:53:07,270
so far so good

1754
01:53:07,280 --> 01:53:09,660
let's talk about some applications

1755
01:53:09,680 --> 01:53:12,220
hopefully this will get a little bit more interesting field

1756
01:53:12,680 --> 01:53:16,360
so the application that i want to focus on in the next ten minutes or

1757
01:53:16,360 --> 01:53:21,240
so is a applications in computer graphics starting with the problem of inferring implicit surface

1758
01:53:22,520 --> 01:53:25,610
so here we have a sampling of the surface

1759
01:53:25,660 --> 01:53:31,290
surface to typically living in r three and sometimes we have crisp ending so surface

1760
01:53:31,290 --> 01:53:37,870
normals and would like to construct a function kernel expansion whose zero level approximates the

1761
01:53:37,870 --> 01:53:42,910
surface where the surface is the function is supposed to take the value zero inside

1762
01:53:42,920 --> 01:53:47,490
of the object which is delimited by the surface the function should be negative should

1763
01:53:47,490 --> 01:53:49,170
be positive

1764
01:53:49,180 --> 01:53:53,900
and a number of support vector approaches have been developed for this problem some building

1765
01:53:53,900 --> 01:53:58,840
on one class support vector machines some of the building support vector regression and it's

1766
01:53:58,840 --> 01:54:04,290
possible to approximate objects with this kind of approach and also to estimate was called

1767
01:54:04,290 --> 01:54:09,800
the signed distance function so the sign distance function is a particular such function with

1768
01:54:09,800 --> 01:54:14,040
the additional property that the value of f should ever always be equal to the

1769
01:54:15,110 --> 01:54:20,860
from the surface multiplied with the minus one if we inside the object

1770
01:54:20,900 --> 01:54:25,970
once we have such implicit representations we can do funny things like intersections of objects

1771
01:54:25,970 --> 01:54:31,970
and things like that and one can solve such problems also in on fairly large

1772
01:54:31,970 --> 01:54:38,080
scale so this is some work of of species and christian walder who has sold

1773
01:54:38,080 --> 01:54:39,370
for instance this

1774
01:54:39,490 --> 01:54:45,050
approximate this object which is a standard benchmark in computer graphics that has fourteen million

1775
01:54:45,050 --> 01:54:48,320
training points and it so that's pretty fast if you also

1776
01:54:48,420 --> 01:54:55,260
and he developed a multiscale approach where we don't just have a single scale of

1777
01:54:55,260 --> 01:55:00,110
kernel which but you have multiple scales so you can you can do things like

1778
01:55:00,110 --> 01:55:03,600
fuels where you don't have the time if you have the skills accordingly and so

1779
01:55:05,790 --> 01:55:10,000
so there's another few examples all these things are

1780
01:55:10,020 --> 01:55:12,770
with with a grain of salt

1781
01:55:12,780 --> 01:55:17,970
hyperplanes in some feature space associated with the company in the input space that look

1782
01:55:17,970 --> 01:55:21,830
like this complex objects

1783
01:55:21,840 --> 01:55:27,080
now if you try to apply this to videos the first thing that comes to

1784
01:55:27,080 --> 01:55:32,550
mind is to take a video and you fit such an implicit surface representation frame

1785
01:55:34,110 --> 01:55:38,900
and if you do that you get this kind of solutions

1786
01:55:38,910 --> 01:55:40,370
so you see each

1787
01:55:40,400 --> 01:55:46,260
picture is reasonable but from picture to picture this and jumps which is because you

1788
01:55:46,270 --> 01:55:50,010
do in in each of them separately newtown regularized across time

1789
01:55:50,310 --> 01:55:53,820
and if you extend this approach to it for the

1790
01:55:53,840 --> 01:55:56,980
you get a solution which is not smooth in time

1791
01:55:57,390 --> 01:56:05,300
and which also has some interesting properties in terms of interpolation with christian tried see

1792
01:56:05,300 --> 01:56:08,050
deleted some frames in this video

1793
01:56:08,090 --> 01:56:12,790
the shown in red so if you see something red there were no training points

1794
01:56:12,790 --> 01:56:25,070
and the right thing seen is purely filled in by the system

1795
01:56:25,170 --> 01:56:29,140
now one could come up with the idea of trying to use this to interpolate

1796
01:56:29,140 --> 01:56:33,070
between different shapes not just the same person at different times but really different shapes

1797
01:56:33,070 --> 01:56:34,800
like princes this monkey in this human

1798
01:56:35,420 --> 01:56:39,210
but it turns out this doesn't work very well

1799
01:56:41,460 --> 01:56:45,230
so the deformation states are pretty ugly and i don't want to claim that the

1800
01:56:45,230 --> 01:56:48,930
final human is beautiful but the things in between probably even worse

1801
01:56:49,090 --> 01:56:50,420
so we

1802
01:56:50,440 --> 01:56:56,620
we try to think of something different and some different approach that more directly tackle

