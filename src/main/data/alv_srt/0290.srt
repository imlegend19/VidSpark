1
00:00:00,000 --> 00:00:02,720
only practical

2
00:00:07,770 --> 00:00:13,840
how can you

3
00:00:14,000 --> 00:00:20,540
a lot of the rules

4
00:00:28,970 --> 00:00:34,940
not bad

5
00:00:39,730 --> 00:00:43,600
you are

6
00:00:45,700 --> 00:00:47,180
the other

7
00:01:02,710 --> 00:01:03,760
you know

8
00:01:03,840 --> 00:01:06,880
i have

9
00:01:23,330 --> 00:01:25,880
these are

10
00:02:05,530 --> 00:02:17,040
it's true

11
00:02:30,350 --> 00:02:31,470
he is

12
00:02:47,190 --> 00:02:52,430
and it would be

13
00:02:54,930 --> 00:02:58,170
in march

14
00:02:58,420 --> 00:03:05,570
there are all these people

15
00:03:05,570 --> 00:03:07,980
OK case

16
00:03:26,740 --> 00:03:31,630
why are we

17
00:03:31,650 --> 00:03:33,090
which is better

18
00:03:33,110 --> 00:03:36,860
and that was

19
00:03:36,880 --> 00:03:38,820
is that

20
00:03:41,260 --> 00:03:42,530
part of

21
00:03:51,450 --> 00:03:57,880
you will need

22
00:03:57,880 --> 00:04:00,290
in the

23
00:04:01,650 --> 00:04:02,670
i decided to be

24
00:04:02,800 --> 00:04:04,740
the the help me

25
00:04:04,790 --> 00:04:10,010
and took good job in NASA JPL which was actually an amazing just sort of

26
00:04:10,010 --> 00:04:11,210
kept me

27
00:04:11,230 --> 00:04:13,920
quite look into the academic community

28
00:04:13,980 --> 00:04:15,120
give me the opportunity work

29
00:04:15,130 --> 00:04:16,990
the top scientists

30
00:04:17,330 --> 00:04:21,360
and a lot of stuff happened

31
00:04:21,380 --> 00:04:25,530
then i moved out of the time with a lot of people

32
00:04:25,550 --> 00:04:27,630
i would leave such a great

33
00:04:30,710 --> 00:04:32,960
only seventy one

34
00:04:33,150 --> 00:04:36,800
but in the context of one of the most successful projects i worked on while

35
00:04:36,800 --> 00:04:38,750
i was there

36
00:04:39,130 --> 00:04:43,810
so let me know that it's kind of project which was about classifying about two

37
00:04:43,810 --> 00:04:45,510
billion objects

38
00:04:45,520 --> 00:04:46,960
to start galaxies

39
00:04:46,990 --> 00:04:48,320
and most of these objects

40
00:04:48,330 --> 00:04:50,920
look like this

41
00:04:50,930 --> 00:04:52,880
that's what the images were

42
00:04:52,920 --> 00:04:55,760
in that in the survey which took about seven years to do

43
00:04:56,080 --> 00:04:59,030
and finished in ninety five

44
00:04:59,040 --> 00:05:04,410
it has images of of the images that show now i'm showing this

45
00:05:04,420 --> 00:05:05,750
two things one

46
00:05:05,770 --> 00:05:08,190
is one of the data on the top

47
00:05:08,810 --> 00:05:09,960
it looks like

48
00:05:09,980 --> 00:05:11,420
no resolution

49
00:05:11,440 --> 00:05:16,410
an astronomer looking at and actually a lot of methods in astronomy trying to understand

50
00:05:16,410 --> 00:05:18,320
these images

51
00:05:18,380 --> 00:05:20,430
i can tell the difference pretty much

52
00:05:20,440 --> 00:05:22,500
now that i can tell you what

53
00:05:22,530 --> 00:05:24,380
which one is the start which of the galaxy

54
00:05:24,400 --> 00:05:25,630
if you look at the bottom

55
00:05:25,670 --> 00:05:28,660
an astronomer will readily tell you it's very clear

56
00:05:28,660 --> 00:05:29,460
which one is this

57
00:05:29,520 --> 00:05:30,660
which one is about

58
00:05:30,670 --> 00:05:33,150
very very certain about

59
00:05:33,820 --> 00:05:37,190
turns out many of astronomers

60
00:05:37,210 --> 00:05:40,250
the big one year is not guaranteed to start

61
00:05:40,270 --> 00:05:42,440
and that's clearly is

62
00:05:44,790 --> 00:05:45,920
the point is

63
00:05:45,950 --> 00:05:49,790
we were able to solve this problem with amazing accuracy using data mining and machine

64
00:05:52,410 --> 00:05:53,930
blew me away

65
00:05:55,440 --> 00:05:56,580
because this is a group of

66
00:05:56,590 --> 00:06:00,660
this is the community very very smart people dedicated

67
00:06:00,670 --> 00:06:04,420
they can spend all my lifetime studying one

68
00:06:04,440 --> 00:06:05,690
one problem

69
00:06:05,730 --> 00:06:08,360
first of all secondary

70
00:06:08,390 --> 00:06:09,760
one instrument

71
00:06:09,770 --> 00:06:11,990
i know it inside out

72
00:06:12,170 --> 00:06:14,690
they really are very familiar with the idea

73
00:06:15,750 --> 00:06:17,890
expected in real life

74
00:06:17,970 --> 00:06:19,990
so my first lesson

75
00:06:20,830 --> 00:06:25,790
even when you have a lot more people focused on very specific problem but was

76
00:06:25,790 --> 00:06:28,180
was very successful actresses work

77
00:06:28,190 --> 00:06:29,720
much much higher than we thought

78
00:06:29,730 --> 00:06:31,800
in the nineteen these objects

79
00:06:31,830 --> 00:06:34,750
so it it was really like a magic box and it led to a bunch

80
00:06:34,750 --> 00:06:37,280
of discoveries in astronomy

81
00:06:37,280 --> 00:06:38,600
a lot of stuff

82
00:06:38,630 --> 00:06:41,590
that happened from that

83
00:06:41,610 --> 00:06:45,110
so that's basically microsoft's biggest

84
00:06:45,680 --> 00:06:48,800
get me to leave GPL and go to microsoft was to ask me a simple

85
00:06:49,930 --> 00:06:52,110
which is a group of very smart people

86
00:06:52,140 --> 00:06:54,300
who can dedicate their life to one dataset set

87
00:06:54,350 --> 00:06:57,340
could not solve this problem but in many parts of

88
00:06:57,360 --> 00:06:59,810
imagine what it could do in the business world

89
00:06:59,820 --> 00:07:01,940
and part of the building efforts

90
00:07:01,940 --> 00:07:05,540
want to introduce me to see what was happening in the business world in corporate

91
00:07:05,540 --> 00:07:06,980
america and

92
00:07:07,030 --> 00:07:08,790
basically very

93
00:07:10,090 --> 00:07:13,050
the reason why things work well by the way

94
00:07:13,060 --> 00:07:15,600
it's not magical

95
00:07:17,270 --> 00:07:18,440
humans then

96
00:07:18,450 --> 00:07:21,740
because this problem was inherently high dimensional

97
00:07:21,750 --> 00:07:23,600
what do i mean by that

98
00:07:23,630 --> 00:07:25,040
this is in that respect

99
00:07:25,110 --> 00:07:27,430
measurements have to solve the problems

100
00:07:27,470 --> 00:07:30,850
we found that out of fifty that was we were dealing

101
00:07:30,870 --> 00:07:34,800
and today by the way is this part of the fifty sounds very small right

102
00:07:34,800 --> 00:07:37,890
because a lot of us deal with a hundred thousand

103
00:07:37,900 --> 00:07:39,870
and millions of light

104
00:07:39,890 --> 00:07:41,950
but fifty years

105
00:07:41,990 --> 00:07:43,780
it essential

106
00:07:43,800 --> 00:07:46,500
we didn't know which way ahead of time

107
00:07:46,820 --> 00:07:48,560
o thirty years

108
00:07:48,580 --> 00:07:52,970
he's very smart people working with a very smart it's not only in the right

109
00:07:53,630 --> 00:07:54,580
this problem

110
00:07:54,590 --> 00:07:57,440
pretty bad property which is the removal of the

111
00:07:57,640 --> 00:08:02,170
think this is good dramatic actually had to discover the right

112
00:08:02,180 --> 00:08:06,430
and what they do they were themselves up to four five six dimensions

113
00:08:06,450 --> 00:08:09,750
but that didn't help they just couldn't get out by humans

114
00:08:09,760 --> 00:08:12,200
designed to get up to about eight dimension

115
00:08:12,250 --> 00:08:14,690
which would solve the problem had a normal life

116
00:08:17,260 --> 00:08:21,430
microsoft research was a major shift in

117
00:08:21,500 --> 00:08:24,200
direction and along with the

118
00:08:24,220 --> 00:08:25,770
in scientists

119
00:08:25,790 --> 00:08:29,130
but i was worried about scalability and systems and and i learned a lot of

120
00:08:29,130 --> 00:08:33,260
about databases from a lot of people

121
00:08:33,350 --> 00:08:36,520
began to worry about what it takes to product starts to put it in the

122
00:08:38,850 --> 00:08:41,250
and actually had the fortune to work

123
00:08:41,350 --> 00:08:45,780
research group and the product was pretty interesting

124
00:08:45,790 --> 00:08:52,990
so this is my second lesson came up and ended up leaving microsoft with two

125
00:08:52,990 --> 00:08:56,620
colleagues microsoft to do is to

126
00:08:56,630 --> 00:08:59,060
now i do that

127
00:08:59,220 --> 00:09:02,300
even though the environment with microsoft still

128
00:09:02,330 --> 00:09:04,580
like to come

129
00:09:04,600 --> 00:09:10,030
we did that because i discovered the real problem is not technology from

130
00:09:10,030 --> 00:09:13,790
the problems data management problems and people

131
00:09:13,810 --> 00:09:16,120
people didn't understand it

132
00:09:16,170 --> 00:09:20,670
so what we did is and by the way the world is pretty much worrying

133
00:09:20,670 --> 00:09:24,920
about how do i make a database with finite memory and find this

134
00:09:24,930 --> 00:09:26,820
one very large

135
00:09:28,250 --> 00:09:31,680
a lot of the work being done today is referred to as essentially most of

136
00:09:31,680 --> 00:09:35,530
the time in the history right you only one person so there you never want

137
00:09:35,770 --> 00:09:37,340
more than one

138
00:09:37,360 --> 00:09:41,520
and if you take your password to come up with the best possible results

139
00:09:43,430 --> 00:09:45,610
let's start with the a good work

140
00:09:46,710 --> 00:09:48,030
didn't read

141
00:09:48,880 --> 00:09:50,780
didn't get used a lot

142
00:09:50,780 --> 00:09:53,600
and the reason for that is people do not use

143
00:09:53,620 --> 00:09:58,360
and the reason for that

144
00:09:58,390 --> 00:10:00,420
and this is i think microsoft word

145
00:10:00,420 --> 00:10:04,230
is discovering this sort of shift between

146
00:10:04,250 --> 00:10:06,510
business challenges technical two

147
00:10:08,840 --> 00:10:15,240
we in this community think about algorithms and technical tools how to scale of stuff

148
00:10:15,240 --> 00:10:18,720
and what you have here this challenge is how

149
00:10:18,760 --> 00:10:22,820
people who will ultimately use algorithms to think of the

150
00:10:22,850 --> 00:10:25,280
and in between there there is no clear

151
00:10:25,280 --> 00:10:26,780
i had to basically

152
00:10:30,620 --> 00:10:33,500
was statisticians extremely hard to find

153
00:10:33,510 --> 00:10:38,570
in the nineteen fifties TV is consultant it becomes very expensive just interested between the

154
00:10:39,470 --> 00:10:45,360
that's a lot of misunderstandings system was that company basically sensible simple idea

155
00:10:45,380 --> 00:10:47,290
let's do it has been

156
00:10:47,300 --> 00:10:50,920
well we did we operate on it and we do the data mining

157
00:10:50,920 --> 00:10:55,780
and companies just give us the

158
00:10:58,110 --> 00:10:59,890
now addressing these

159
00:10:59,890 --> 00:11:03,510
second lecture at the second week of machine learning summer school

160
00:11:03,530 --> 00:11:08,320
and today we're going to have a new speaker which is the u i t

161
00:11:08,340 --> 00:11:10,550
from the gatsby computational unit

162
00:11:10,560 --> 00:11:11,680
he's going to be there

163
00:11:11,690 --> 00:11:12,950
talk to

164
00:11:12,970 --> 00:11:17,940
got talk today and then two practice sessions today and tomorrow

165
00:11:17,950 --> 00:11:24,730
so you which he did his undergraduate in mathematics and waterloo and then he did

166
00:11:24,730 --> 00:11:29,110
his graduate students with geoff hinton cannot be is now

167
00:11:29,130 --> 00:11:34,710
in two thousand three he finished his phd and immediately after he post-doc with michael

168
00:11:34,710 --> 00:11:36,530
jordan UC berkeley

169
00:11:36,550 --> 00:11:38,350
and the lee

170
00:11:38,370 --> 00:11:39,390
qu one u

171
00:11:39,510 --> 00:11:41,780
i don't sing about that

172
00:11:41,800 --> 00:11:49,840
OK and since two thousand seven this electrode the gatsby computational so you reduce expert

173
00:11:49,840 --> 00:11:53,710
in based bayesian techniques and approximate inference

174
00:11:53,720 --> 00:11:55,610
and unsupervised learning

175
00:11:55,630 --> 00:12:00,880
and today is going to talk about usually processes and forward

176
00:12:00,900 --> 00:12:03,200
the lecture OK thank you

177
00:12:10,300 --> 00:12:11,600
is it working

178
00:12:12,970 --> 00:12:18,810
really ought to be here to tell you about the process of how i am

179
00:12:18,840 --> 00:12:20,370
very excited about this

180
00:12:20,380 --> 00:12:23,180
o area is not currently

181
00:12:23,200 --> 00:12:26,320
i'm really happy to be talking here about this so

182
00:12:29,370 --> 00:12:34,900
i guess what doing today and tomorrow is that

183
00:12:34,920 --> 00:12:36,680
is that

184
00:12:36,690 --> 00:12:40,420
a tutorial right now and that the paper will cause today in the afternoon and

185
00:12:40,420 --> 00:12:41,800
more useful

186
00:12:41,850 --> 00:12:47,940
so let's start with how does this dirichlet processes basically

187
00:12:48,070 --> 00:12:50,710
what's called bayesian nonparametrics

188
00:12:50,790 --> 00:12:53,410
a model so as

189
00:12:53,460 --> 00:12:58,550
congress with the value of the constant positive that's another class of bayesian nonparametric models

190
00:12:58,590 --> 00:13:02,850
and this model which are not from parametric in the sense that

191
00:13:03,320 --> 00:13:08,220
they generally have a very large or infinite number of parameters in the

192
00:13:08,240 --> 00:13:12,300
and so in order to prevent overfitting is this model you

193
00:13:12,440 --> 00:13:18,270
you basically have to be paid to integrate all this infinitely many

194
00:13:18,310 --> 00:13:24,670
and dirichlet processes has become pretty popular nowadays in both the machine learning

195
00:13:25,990 --> 00:13:28,210
so how can i like the

196
00:13:28,230 --> 00:13:34,350
general applications that have that has been used in both in both communities

197
00:13:34,360 --> 00:13:37,740
that the estimation

198
00:13:38,070 --> 00:13:41,500
the estimation so that basically estimating

199
00:13:41,550 --> 00:13:47,790
that use of distribution basically and is also being used a lot in what's called

200
00:13:48,760 --> 00:13:50,410
semiparametric modelling

201
00:13:50,420 --> 00:13:52,760
and it's useful are in biostatistics

202
00:13:52,800 --> 00:13:56,970
and finally in machine learning is since the dirichlet processes that mostly used at the

203
00:13:56,970 --> 00:14:01,070
way of might like that being model selection and averaging

204
00:14:01,400 --> 00:14:08,050
so it would on the process was followed by a practical cost on implementing a

205
00:14:08,050 --> 00:14:09,490
dirichlet process mixture

206
00:14:09,500 --> 00:14:12,450
i will try to cross the NIPS papers if

207
00:14:12,500 --> 00:14:14,320
if you manage

208
00:14:14,340 --> 00:14:18,180
to get that the quite interesting

209
00:14:18,190 --> 00:14:23,640
i presume that everybody here is somewhat familiar with the paradigm

210
00:14:23,750 --> 00:14:27,580
and you should know this from the open call

211
00:14:27,650 --> 00:14:31,850
cost as well among the previous back across the

212
00:14:31,860 --> 00:14:33,880
in and market

213
00:14:33,890 --> 00:14:38,370
so this of course been quite a number of other foreign countries the processes both

214
00:14:38,410 --> 00:14:40,610
in machine learning and statistics

215
00:14:42,660 --> 00:14:46,750
you can think of this the for life on land but they can take a

216
00:14:46,750 --> 00:14:47,870
look u

217
00:14:47,890 --> 00:14:49,950
one the other people present

218
00:14:50,820 --> 00:14:52,440
with different viewpoints taken

219
00:14:53,930 --> 00:14:58,000
that's part of the our policy going to be

220
00:14:58,020 --> 00:15:02,300
but also in the first half of the article

221
00:15:02,350 --> 00:15:07,050
got to the free applications that just popped the ball and you

222
00:15:07,060 --> 00:15:09,260
tell you what dirichlet process

223
00:15:09,280 --> 00:15:11,750
they take a break and that

224
00:15:11,760 --> 00:15:15,780
we will go into a bit more details into the how dirichlet process

225
00:15:15,800 --> 00:15:18,570
can be represented are represented

226
00:15:18,610 --> 00:15:23,720
and then will only link back to their applications by talking about how

227
00:15:23,770 --> 00:15:27,710
the dirichlet process then used in terms of modelling and

228
00:15:28,040 --> 00:15:30,780
i will be talking about

229
00:15:30,780 --> 00:15:33,300
that you've been using

230
00:15:33,350 --> 00:15:34,840
for example

231
00:15:34,860 --> 00:15:38,150
the kind of locally optimal

232
00:15:38,170 --> 00:15:42,750
important distribution so as to some for your state OK so you do that

233
00:15:43,520 --> 00:15:48,540
in this case i showed you that essentially the weight

234
00:15:48,550 --> 00:15:51,990
basically the importance weight that's going to appear

235
00:15:52,000 --> 00:15:53,700
it's going to be something

236
00:15:53,700 --> 00:15:56,710
it's going to be essentially the product

237
00:15:57,990 --> 00:16:00,700
sheikh or one two

238
00:16:01,700 --> 00:16:03,680
so choosing instead to two

239
00:16:03,690 --> 00:16:08,530
of p y j xk minus one OK we had this guy

240
00:16:08,560 --> 00:16:11,500
is the integral

241
00:16:12,770 --> 00:16:17,150
OK so you can do that

242
00:16:17,160 --> 00:16:24,030
so assume that you're able to sample what exactly from the local optimum publicity then

243
00:16:24,060 --> 00:16:25,970
time and your weight

244
00:16:26,000 --> 00:16:27,700
it would be basically

245
00:16:27,700 --> 00:16:29,610
the point of the oldest towns

246
00:16:29,660 --> 00:16:31,540
but like this thing

247
00:16:31,570 --> 00:16:34,090
OK is going to have difficulty also

248
00:16:34,090 --> 00:16:39,850
violence or related violence that is going to increase exponentially with time indexed so you

249
00:16:39,850 --> 00:16:44,100
can try to is good it's important to try to put forward basically

250
00:16:44,110 --> 00:16:45,820
sample paul

251
00:16:45,850 --> 00:16:50,980
basically having trying to optimize this morning this important distribution so as to minimize the

252
00:16:50,980 --> 00:16:55,930
loss of the weight but it can only mitigate this problem that cannot be eliminated

253
00:16:55,930 --> 00:17:00,510
because all the time you can and that we were essentially a weight function which

254
00:17:00,510 --> 00:17:05,070
is the product of terms and it basically means the divine stability increase exponentially with

255
00:17:05,070 --> 00:17:06,070
time index o

256
00:17:06,090 --> 00:17:09,680
basically involves something will collapse

257
00:17:09,690 --> 00:17:12,730
so it seems that we have stuck because we have this

258
00:17:12,790 --> 00:17:17,900
importance sampling algorithms very unfortunately

259
00:17:17,900 --> 00:17:19,500
if you basically

260
00:17:19,530 --> 00:17:23,440
use it on particle problem after a few time steps whatever you do

261
00:17:23,440 --> 00:17:29,260
the algorithm collects basically the approximation as to why violence so what can we do

262
00:17:29,290 --> 00:17:33,740
well what we're going to do is something which is actually only

263
00:17:33,750 --> 00:17:36,270
some of the important

264
00:17:36,300 --> 00:17:38,900
on some of its looks like act

265
00:17:38,910 --> 00:17:41,810
but you can show that it's actually principles

266
00:17:41,840 --> 00:17:42,840
so the thing

267
00:17:42,930 --> 00:17:46,740
do what bothers me we've all this despite the categories

268
00:17:46,910 --> 00:17:51,720
i time and increases my clients of the unnormalized weights

269
00:17:51,730 --> 00:17:54,990
increased typically over time as soon as you can see the state space model you

270
00:17:54,990 --> 00:17:59,010
could come up with the example but for the moment interest this increases over time

271
00:17:59,260 --> 00:18:03,460
which means that when you normalized those weight so that the sum to one one

272
00:18:03,460 --> 00:18:06,840
guy's going to basically be very close to one and all the others can be

273
00:18:06,840 --> 00:18:12,710
very close to zero sustainable basically a very poor approximation following very important

274
00:18:13,420 --> 00:18:15,690
we can essentially monitor

275
00:18:15,710 --> 00:18:16,970
it's the

276
00:18:16,980 --> 00:18:19,470
the weights the normalized weight

277
00:18:19,480 --> 00:18:28,190
essentially what we can do is particularly with many of the weight of when basically

278
00:18:28,190 --> 00:18:34,230
i'm going to find the particle i as the weight normalise weight which is small

279
00:18:34,240 --> 00:18:38,920
is always more related to the kind of perfect monte carlo case all the way

280
00:18:38,920 --> 00:18:42,710
to normalise weights would have weight one of event so we're going to find basically

281
00:18:42,710 --> 00:18:45,640
particles with low weights i'm going to try and get rid of it

282
00:18:46,140 --> 00:18:48,940
where basically the particle

283
00:18:48,950 --> 00:18:51,440
as i weights what i'm gonna do i'm gonna

284
00:18:51,860 --> 00:18:58,350
david could it multiple times because basically it means that this particle is in promising

285
00:18:58,350 --> 00:18:59,980
region part of the space

286
00:19:00,000 --> 00:19:01,600
OK so that's really

287
00:19:01,600 --> 00:19:03,410
the singapore kind of

288
00:19:03,420 --> 00:19:09,450
genetic type idea genetic algorithm type idea if feeling good basically original display that means

289
00:19:09,450 --> 00:19:14,980
you weight you're going to basically be given multiple copies if you in a region

290
00:19:14,980 --> 00:19:18,710
where you have lo just forget you your child essentially that's what we're going to

291
00:19:21,050 --> 00:19:23,000
so that's the idea

292
00:19:23,020 --> 00:19:27,780
are really it's gone you can think of is congressional act you try to focus

293
00:19:28,170 --> 00:19:30,720
on the promising original of the space

294
00:19:30,890 --> 00:19:35,240
so we tried to do that basically in a bit more principled frameworks instead of

295
00:19:35,240 --> 00:19:36,200
trying to

296
00:19:36,210 --> 00:19:41,780
it mean deterministically particles with low weights on the black particle evaluates the slides i'm

297
00:19:41,780 --> 00:19:44,460
a little bit subtle so

298
00:19:44,480 --> 00:19:46,210
assume you are time

299
00:19:46,230 --> 00:19:51,470
then on that you like is which is like a sequential importance sampling algorithm so

300
00:19:51,470 --> 00:19:53,320
you have an approximation

301
00:19:53,910 --> 00:19:58,450
basically at time and of the target distribution and we see the weighted

302
00:19:58,470 --> 00:20:00,060
essentially so

303
00:20:00,140 --> 00:20:02,440
of that time as OK

304
00:20:02,450 --> 00:20:05,270
all these women are positive and sum to one

305
00:20:06,020 --> 00:20:07,440
well what do you do

306
00:20:07,440 --> 00:20:10,710
so as to get rid of particles with low weights on the black particles by

307
00:20:12,020 --> 00:20:14,460
well simply wasn't going to do

308
00:20:14,480 --> 00:20:18,180
you're can result paul capital and time

309
00:20:18,190 --> 00:20:20,100
for this

310
00:20:20,110 --> 00:20:21,760
basically distribution

311
00:20:22,620 --> 00:20:26,500
so this thing you can think of it as the discrete distribution of OK

312
00:20:26,520 --> 00:20:28,450
so what you're going to do

313
00:20:28,450 --> 00:20:32,020
you saw core resource for capital n times

314
00:20:33,000 --> 00:20:34,510
these distributions

315
00:20:34,520 --> 00:20:37,450
so i was basically to obtain new distribution

316
00:20:37,450 --> 00:20:39,790
which is of this form OK so

317
00:20:39,810 --> 00:20:44,860
that seems to be crazy you multicolour approximation of the target on what to do

318
00:20:44,860 --> 00:20:51,610
you do a further approximation by resampling capital and time from this

319
00:20:51,640 --> 00:20:55,500
essentially a weighted empirical measure so as to obtain

320
00:20:56,820 --> 00:21:00,460
the new approximation of the target distribution OK

321
00:21:02,030 --> 00:21:04,300
something you should mention obviously

322
00:21:04,320 --> 00:21:09,230
basically when you do we sample from the distribution because the distribution is discrete in

323
00:21:09,230 --> 00:21:11,270
this basically new

324
00:21:11,290 --> 00:21:13,710
approximation of the target distribution

325
00:21:13,720 --> 00:21:16,090
you can basically particles

326
00:21:16,090 --> 00:21:20,400
XI which i call two xj for i from from g because you might have

327
00:21:20,400 --> 00:21:25,590
so we can discuss a lot of this experiment i'm not sure we have time

328
00:21:25,590 --> 00:21:30,190
today so i would just insist on these how can we determine the size of

329
00:21:30,190 --> 00:21:32,550
the atomic nucleus from this experiment

330
00:21:32,580 --> 00:21:34,510
as the first estimate

331
00:21:36,090 --> 00:21:39,880
you have to consider the popu elastic scattering that go back

332
00:21:39,900 --> 00:21:44,150
and you had at the distance is zero and the kinetic energy of the alpha

333
00:21:45,080 --> 00:21:48,880
but you know because it's already at the beginning of given worse

334
00:21:48,900 --> 00:21:52,350
so which is equal to the coulomb repulsion here

335
00:21:52,380 --> 00:21:59,020
so then you know exactly this is zero based distance and with this experiment they

336
00:21:59,020 --> 00:22:02,260
had discovered that the size of the school nucleus

337
00:22:02,280 --> 00:22:06,970
it is around ten minus forty meters

338
00:22:06,980 --> 00:22:11,140
so if we compare the scale with the atomic nucleus

339
00:22:11,150 --> 00:22:13,100
you see that you have

340
00:22:13,130 --> 00:22:14,880
under a thousand times

341
00:22:14,890 --> 00:22:20,140
and it is a hundred thousand times smaller than at home

342
00:22:20,150 --> 00:22:24,520
as i told you this nucleus is made of z protons and neutrons what we

343
00:22:24,520 --> 00:22:26,130
call the nucleons

344
00:22:26,150 --> 00:22:31,290
we characterize the nucleus based mass which is the sum of neutron and proton number

345
00:22:31,300 --> 00:22:33,730
and its atomic number z

346
00:22:33,760 --> 00:22:36,780
so we write it like these

347
00:22:36,790 --> 00:22:38,800
so now the question do

348
00:22:38,830 --> 00:22:42,260
the nuclei all the same ranges

349
00:22:42,280 --> 00:22:44,170
in fact no

350
00:22:44,190 --> 00:22:45,500
here are some

351
00:22:45,770 --> 00:22:50,150
the results were you have to register and family depending on the mass number a

352
00:22:50,150 --> 00:22:53,890
whirlwind foot in some of the proton and neutron

353
00:22:53,900 --> 00:22:57,650
and you see that you have different points from carbon twelve

354
00:22:57,780 --> 00:22:59,460
too late late

355
00:22:59,470 --> 00:23:01,860
that seems to be that are

356
00:23:01,880 --> 00:23:04,350
along the straight line

357
00:23:04,380 --> 00:23:06,280
and you can find this relation

358
00:23:06,290 --> 00:23:08,430
air force one one twenty five

359
00:23:08,440 --> 00:23:10,810
in one the family

360
00:23:11,100 --> 00:23:15,940
so this is a very important feature that the radius increases with a one

361
00:23:15,970 --> 00:23:18,130
so it means that the volume

362
00:23:18,140 --> 00:23:21,300
of the nucleus increases with the number of particles

363
00:23:21,320 --> 00:23:27,390
and this is similar to what we would drop of liquid where you have the

364
00:23:27,390 --> 00:23:31,540
volume that is proportional to the number of molecules and this is one of the

365
00:23:31,540 --> 00:23:32,580
reasons why

366
00:23:32,640 --> 00:23:36,400
you will see that in a second course white people of food that in case

367
00:23:36,400 --> 00:23:41,110
can be interpreted as the fluid liquid drop

368
00:23:41,120 --> 00:23:42,510
so if not

369
00:23:42,520 --> 00:23:45,310
we compare the scale all the thing

370
00:23:45,330 --> 00:23:50,360
so this is the size in matrix with the mass kilogram you are here

371
00:23:50,370 --> 00:23:52,580
ten minus fourteen fifteen

372
00:23:52,630 --> 00:23:54,310
four nuclei nuclear arms

373
00:23:54,320 --> 00:23:59,150
if you go to tony minus my man matt one

374
00:23:59,160 --> 00:24:01,570
and then starts to cluster

375
00:24:01,600 --> 00:24:03,620
support can add two

376
00:24:03,640 --> 00:24:06,700
very huge sort of the cluster

377
00:24:06,720 --> 00:24:10,290
so the nucleons nuclei are really the first stage

378
00:24:10,310 --> 00:24:13,920
in this hierarchy of complex systems

379
00:24:13,970 --> 00:24:15,580
so massive

380
00:24:15,990 --> 00:24:18,400
i'm sure you know that very well

381
00:24:18,440 --> 00:24:20,560
just to remind yourself

382
00:24:20,620 --> 00:24:26,810
but i neutrons problems that they are made of three quarks

383
00:24:26,830 --> 00:24:28,940
they have almost the same as

384
00:24:28,970 --> 00:24:34,780
which is around nine hundred thirty eight any which is two thousand times

385
00:24:34,800 --> 00:24:37,570
bigger than the mass of the electron

386
00:24:37,600 --> 00:24:38,780
so that's why

387
00:24:38,790 --> 00:24:41,250
if the rutherford experiment

388
00:24:41,270 --> 00:24:46,040
these out for particular really don't know these electrodes

389
00:24:46,050 --> 00:24:48,790
to compare with something we

390
00:24:48,800 --> 00:24:55,770
we feel better is mass of the proton is ten minus twenty seven kilograms

391
00:24:55,810 --> 00:25:00,130
so now you know that in in any case it's made of proton and neutron

392
00:25:00,130 --> 00:25:06,290
that do interact so these interaction the strong interaction you have here is interaction

393
00:25:06,300 --> 00:25:10,920
the strong interaction is very intense and of short range

394
00:25:10,930 --> 00:25:14,410
it is amazing isn't it

395
00:25:14,430 --> 00:25:16,170
and you have here is

396
00:25:16,170 --> 00:25:23,240
well we're

397
00:25:25,510 --> 00:25:31,110
the same

398
00:25:39,020 --> 00:25:41,670
these is

399
00:25:50,110 --> 00:25:53,900
i thought i knew

400
00:25:53,920 --> 00:25:56,650
one hundred

401
00:25:56,670 --> 00:25:59,830
after all

402
00:26:01,800 --> 00:26:04,720
by the way

403
00:26:14,000 --> 00:26:16,790
the error rate

404
00:26:36,710 --> 00:26:39,040
where we are

405
00:26:41,950 --> 00:26:45,360
and he

406
00:27:32,700 --> 00:27:34,040
related to

407
00:27:42,750 --> 00:27:52,050
they will

408
00:28:04,200 --> 00:28:07,570
during the right

409
00:28:20,240 --> 00:28:31,590
so all of the world and

410
00:28:34,770 --> 00:28:36,350
for each

411
00:28:48,700 --> 00:28:54,710
you is know i good i

412
00:28:59,310 --> 00:29:07,650
the goal of my life that i will

413
00:29:07,670 --> 00:29:09,500
also you

414
00:29:26,430 --> 00:29:31,550
on the

415
00:29:31,570 --> 00:29:39,590
what i

416
00:29:47,970 --> 00:29:50,800
well he

417
00:30:04,230 --> 00:30:05,810
so what

418
00:30:05,810 --> 00:30:08,130
between you know point one point nine

419
00:30:08,190 --> 00:30:10,980
and everything is bound in the worst possible

420
00:30:11,020 --> 00:30:19,600
but the story is not so nice when you when you log probability

421
00:30:19,620 --> 00:30:25,520
arbitrarily close to zero is still pretty bad things to be pretty large

422
00:30:27,600 --> 00:30:30,650
right your question

423
00:30:40,440 --> 00:30:42,890
if you think about it that way

424
00:30:42,900 --> 00:30:45,150
one way to think about this is

425
00:30:45,190 --> 00:30:46,650
here's the technique

426
00:30:47,500 --> 00:30:50,620
taking any classifier learning algorithm and extracting

427
00:30:50,620 --> 00:30:54,600
because probability a posterior probability

428
00:30:54,600 --> 00:30:57,040
which is maybe not so obvious you can do when

429
00:30:57,150 --> 00:30:58,120
is predicting

430
00:30:58,120 --> 00:31:01,900
either zero or one that something in the interval

431
00:31:18,620 --> 00:31:22,650
all right

432
00:31:22,750 --> 00:31:26,250
so the reason why i'm a lot of this

433
00:31:26,270 --> 00:31:28,400
it is because they can under these

434
00:31:28,420 --> 00:31:31,230
and can compute this

435
00:31:31,750 --> 00:31:35,670
but what you really want to say that this

436
00:31:35,690 --> 00:31:37,400
minus actual probability

437
00:31:37,420 --> 00:31:38,290
is small

438
00:31:39,560 --> 00:31:41,020
in this kind of cool

439
00:31:41,020 --> 00:31:42,150
which is

440
00:31:42,210 --> 00:31:44,560
the are equivalent

441
00:31:44,620 --> 00:31:47,310
the semantic constant

442
00:31:47,390 --> 00:31:53,500
and that the constant is is there for every classifier to the same constant

443
00:31:53,520 --> 00:31:55,350
so roughly speaking

444
00:31:55,790 --> 00:31:56,750
and it is

445
00:31:56,750 --> 00:32:01,060
your plug in the probability of

446
00:32:01,060 --> 00:32:05,060
minus the actual observed y squared to expectation

447
00:32:05,080 --> 00:32:07,830
the and cystatin c

448
00:32:07,830 --> 00:32:09,350
and that's what

449
00:32:09,370 --> 00:32:17,730
the only difference between these two things

450
00:32:17,770 --> 00:32:19,980
OK so that's the problem

451
00:32:20,020 --> 00:32:21,830
and then maybe we want to

452
00:32:21,830 --> 00:32:25,290
i think a little bit about what i wanted to probability sometimes

453
00:32:26,420 --> 00:32:29,330
you know there's a lot of situations we don't want to probability

454
00:32:29,330 --> 00:32:32,600
you want to make some predictions prediction annually the you that prediction tells you what

455
00:32:32,690 --> 00:32:33,400
to do

456
00:32:33,400 --> 00:32:34,650
and the do

457
00:32:34,670 --> 00:32:36,190
you don't have

458
00:32:36,680 --> 00:32:41,250
but there are times when probabilistic and requires

459
00:32:41,270 --> 00:32:42,460
one of them is

460
00:32:42,480 --> 00:32:45,000
now when talking to the doctor

461
00:32:45,270 --> 00:32:49,460
maybe it doesn't want to be unemployed so

462
00:32:49,520 --> 00:32:50,980
if you prefer that

463
00:32:51,000 --> 00:32:55,750
next time probability in existence about what to do

464
00:32:55,770 --> 00:33:00,080
rather than having classifier tell actually do

465
00:33:00,100 --> 00:33:03,270
and then another situation comes up this

466
00:33:03,330 --> 00:33:04,900
sometimes you have

467
00:33:04,920 --> 00:33:06,940
a lot of

468
00:33:07,040 --> 00:33:10,560
data sources which is spread out all over the place

469
00:33:10,620 --> 00:33:12,830
and maybe it's expensive to communicate

470
00:33:12,850 --> 00:33:15,650
between the different places are summarized

471
00:33:15,670 --> 00:33:19,330
the probability of y given x from each individual sensors

472
00:33:19,350 --> 00:33:23,350
and then somewhere later you have use them so instead of

473
00:33:23,400 --> 00:33:25,940
instead of transmitting all information

474
00:33:26,080 --> 00:33:28,120
every location to one location

475
00:33:28,270 --> 00:33:30,600
is transmitted the probability

476
00:33:30,650 --> 00:33:33,310
from one location to central location

477
00:33:33,330 --> 00:33:38,000
and they can be arbitrarily t

478
00:33:39,270 --> 00:33:42,620
these comes up is because you should want compatibility you what you want to think

479
00:33:44,580 --> 00:33:50,670
here's one component systems using perturbation technique and his american-born systems using classify technique and

480
00:33:50,670 --> 00:33:54,540
now maybe one extraction of probabilistic predictions you can

481
00:33:54,580 --> 00:34:00,170
in kenya few things in the right way

482
00:34:04,920 --> 00:34:07,520
the reason

483
00:34:13,310 --> 00:34:17,580
how does this technique were some call technique roving

484
00:34:18,080 --> 00:34:23,480
so this is the basic observation it starts things off

485
00:34:23,480 --> 00:34:26,020
which is that if you have a perfect classifier

486
00:34:26,120 --> 00:34:28,580
and perfect classifier is one

487
00:34:28,600 --> 00:34:29,850
o thing

488
00:34:29,900 --> 00:34:35,960
the probability of y equals one given x is greater than point five

489
00:34:35,960 --> 00:34:39,560
you go that's interesting

490
00:34:39,600 --> 00:34:41,400
what if

491
00:34:41,480 --> 00:34:43,120
i picked some

492
00:34:43,170 --> 00:34:46,620
random uniform random variable between zero and one

493
00:34:46,850 --> 00:34:51,120
make this importance weighted transformation

494
00:34:51,170 --> 00:34:55,000
so i can create an importance weighted dataset

495
00:34:55,020 --> 00:34:56,730
the importance of being

496
00:34:57,750 --> 00:35:05,080
why minus p so if y zero minutes p and flies one minus p

497
00:35:05,080 --> 00:35:08,020
and then you can

498
00:35:08,040 --> 00:35:10,020
think about that bit

499
00:35:10,040 --> 00:35:11,790
and you realize that

500
00:35:11,810 --> 00:35:13,420
if classifier

501
00:35:13,420 --> 00:35:15,210
for this importance weighted

502
00:35:16,440 --> 00:35:18,100
it's perfect

503
00:35:18,100 --> 00:35:21,790
that implies that one class versus one

504
00:35:21,850 --> 00:35:28,400
the probability white was one given x is greater p

505
00:35:28,440 --> 00:35:32,850
so is a predictor is approved

506
00:35:32,870 --> 00:35:34,440
in one table

507
00:35:34,500 --> 00:35:37,040
so if the prediction is

508
00:35:42,290 --> 00:35:44,890
expected so the importance is going to be

509
00:35:46,060 --> 00:35:50,580
and the probability is zero is minus a one given x

510
00:35:50,580 --> 00:35:55,350
and the prediction is one important is one minus p

511
00:35:55,440 --> 00:35:59,670
problem like probably like this one x

512
00:35:59,690 --> 00:36:02,190
and these two are equal

513
00:36:05,100 --> 00:36:06,350
he calls

514
00:36:06,370 --> 00:36:08,670
this probably like this one given x

515
00:36:08,670 --> 00:36:12,960
in the system that are taking the sign which works

516
00:36:13,000 --> 00:36:16,520
it has four campuses about that case first

517
00:36:19,000 --> 00:36:21,020
it is just

518
00:36:22,000 --> 00:36:24,690
thank you for this analysis doesn't matter

519
00:36:24,710 --> 00:36:26,120
you can take it

520
00:36:26,130 --> 00:36:28,190
take it however you want

521
00:36:28,210 --> 00:36:30,000
o point three point two whatever

522
00:36:30,040 --> 00:36:34,560
in this this analysis holds the statement always holds

523
00:36:34,560 --> 00:36:37,870
and the question about how we actually do it

524
00:36:37,890 --> 00:36:40,690
and they will be taking a randomly

525
00:36:40,710 --> 00:36:44,710
or maybe take it from the grid but

526
00:36:44,730 --> 00:36:47,560
sort of and yes

527
00:36:47,620 --> 00:36:50,810
but this is

528
00:36:52,790 --> 00:36:58,850
the one of them is likely to do so the question is is

529
00:36:58,850 --> 00:37:02,020
how do you know the first project

530
00:37:02,020 --> 00:37:04,170
the answer is you don't

531
00:37:04,170 --> 00:37:07,290
but it some tricks which all

532
00:37:11,100 --> 00:37:12,980
or alright so

533
00:37:13,150 --> 00:37:14,790
is probably our them

534
00:37:14,850 --> 00:37:17,270
this is sort of the data flow graph

535
00:37:17,290 --> 00:37:20,730
you start with your binary data set the top

536
00:37:20,750 --> 00:37:23,600
and then you choose mps

537
00:37:23,600 --> 00:37:26,210
however you want

538
00:37:26,250 --> 00:37:31,080
and then you create an importance weighted datasets

539
00:37:31,100 --> 00:37:35,060
and then you have a prior importance weighted learning algorithm

540
00:37:35,060 --> 00:37:40,060
fancy creek that would

541
00:37:40,060 --> 00:37:47,180
you know you know i think it might work at the university

542
00:37:47,780 --> 00:37:50,060
for some reason to work anymore

543
00:37:52,390 --> 00:37:53,910
i mean really wants

544
00:37:54,140 --> 00:37:58,740
what scary

545
00:37:58,990 --> 00:38:06,290
like this one

546
00:38:06,310 --> 00:38:08,060
it's written

547
00:38:08,080 --> 00:38:14,240
i mean it's your personal thing and taking on account of what else this is

548
00:38:14,240 --> 00:38:17,580
the kid in the forward backward with it

549
00:38:17,580 --> 00:38:21,100
are trying to the

550
00:38:26,540 --> 00:38:30,990
you can use to get

551
00:38:41,640 --> 00:38:50,830
the only difference is that now i have to been involved

552
00:38:50,850 --> 00:38:53,830
all right

553
00:38:59,890 --> 00:39:01,600
so for you to always works

554
00:39:07,040 --> 00:39:08,640
his only i

555
00:39:08,660 --> 00:39:14,080
how many of you have solved one or more problems

556
00:39:15,470 --> 00:39:19,470
how many have sort of two or more

557
00:39:20,410 --> 00:39:22,540
three or more

558
00:39:22,560 --> 00:39:29,080
four more

559
00:39:29,100 --> 00:39:33,040
how many have some of the first one

560
00:39:33,060 --> 00:39:36,970
is that

561
00:39:36,990 --> 00:39:43,740
the the first of all sorry for this problem is this one

562
00:39:43,760 --> 00:39:47,060
how many have solved it well enough that they could come on the show is

563
00:39:47,060 --> 00:39:51,640
the solution to your in front

564
00:39:51,660 --> 00:39:55,450
i think most of you probably OK we have a volunteer

565
00:39:58,950 --> 00:39:59,740
thank you

566
00:39:59,760 --> 00:40:04,620
which is you can choose any color any black box

567
00:40:07,700 --> 00:40:08,890
the right

568
00:40:16,010 --> 00:40:18,830
but give you

569
00:40:18,850 --> 00:40:22,510
why should

570
00:40:30,510 --> 00:40:34,290
of showing this one of the positive DNA the dying

571
00:40:35,890 --> 00:40:38,790
she was the

572
00:40:38,810 --> 00:40:40,910
we know that

573
00:41:00,870 --> 00:41:03,310
if you can use this

574
00:41:03,330 --> 00:41:07,120
it believed to more

575
00:41:25,950 --> 00:41:30,330
OK thank you very much so you get you get the point

576
00:41:30,390 --> 00:41:31,810
the so

577
00:41:31,810 --> 00:41:34,520
and the surprising thing here so

578
00:41:34,520 --> 00:41:40,860
so you're solving this optimisation problem in the reproducing kernel hilbert space associated with k

579
00:41:40,880 --> 00:41:48,350
which could be an infinite dimensional space for goes into this process surprising thing is

580
00:41:48,350 --> 00:41:50,410
not the solution has

581
00:41:50,520 --> 00:41:53,890
kernel expansion but the surprising thing is that the kernel expansion is centered on the

582
00:41:53,890 --> 00:41:54,990
training points

583
00:41:55,040 --> 00:41:57,630
so we know already from yesterday

584
00:41:57,670 --> 00:42:02,260
the construction of the reproducing kernel hilbert space the space that effectively all points in

585
00:42:02,260 --> 00:42:04,660
that space have this kind of form

586
00:42:04,670 --> 00:42:09,120
plus some limit points but we didn't

587
00:42:09,130 --> 00:42:13,200
i mean normally in this expansion we could have arbitrary points from my input domain

588
00:42:13,210 --> 00:42:15,720
so here the surprising thing of this

589
00:42:15,970 --> 00:42:21,370
theorem is that the solution only depends on current sitting on the training points in

590
00:42:21,370 --> 00:42:24,770
the training set is a finite set so even if you solve this optimisation problem

591
00:42:24,900 --> 00:42:26,850
if dimensional space

592
00:42:26,850 --> 00:42:29,380
this tells you the solution

593
00:42:29,400 --> 00:42:31,360
really only needs

594
00:42:31,370 --> 00:42:35,610
finally many alfie's if you wanted to compute the solution you only have to optimize

595
00:42:35,610 --> 00:42:39,360
these many coefficients i phi

596
00:42:39,380 --> 00:42:44,570
and the nice thing about theorems that it's very easy to prove the above

597
00:42:44,570 --> 00:42:48,170
i will actually show you the truth

598
00:42:48,200 --> 00:42:54,820
OK so the proof works as follows we first of all decompose

599
00:42:54,820 --> 00:42:58,630
f into a part in the span

600
00:42:58,650 --> 00:42:59,940
of our

601
00:42:59,950 --> 00:43:05,140
kernel sitting and training points and into part which is orthogonal

602
00:43:05,160 --> 00:43:08,630
so this kind of thing can be done we are in the space so it's

603
00:43:08,630 --> 00:43:11,680
it's nice to have the products where you can do things like that we have

604
00:43:11,680 --> 00:43:13,230
things like orthogonality

605
00:43:13,240 --> 00:43:15,360
so we can decompose

606
00:43:16,190 --> 00:43:20,660
possible candidate solutions into two such parts

607
00:43:21,420 --> 00:43:23,790
since this part is orthogonal

608
00:43:23,790 --> 00:43:28,350
the span of the training points we know that for all the training points of

609
00:43:28,350 --> 00:43:33,650
sorry for falcon sitting and training points this equality holds true

610
00:43:33,680 --> 00:43:35,040
so now we apply

611
00:43:35,050 --> 00:43:36,930
this general function

612
00:43:36,950 --> 00:43:42,190
we haven't lost a gender as yet to an arbitrary training point

613
00:43:42,230 --> 00:43:47,850
if we do that so how do we evaluate point function in an RKHS on

614
00:43:47,850 --> 00:43:52,990
training point we just take the dot product with the representer of evaluation which is

615
00:43:52,990 --> 00:43:55,420
the concept on that point

616
00:43:55,490 --> 00:43:58,140
so we do that we plug in

617
00:43:58,150 --> 00:44:00,060
our general form of function

618
00:44:00,060 --> 00:44:01,580
in into here

619
00:44:01,640 --> 00:44:04,520
now we can use linearity of the product

620
00:44:04,530 --> 00:44:07,410
so we have the product of this thing

621
00:44:07,440 --> 00:44:13,070
with this term which i written down here and we have the product of thing

622
00:44:14,280 --> 00:44:16,060
which is zero by assumption

623
00:44:16,080 --> 00:44:21,110
so we left only with this part here

624
00:44:21,130 --> 00:44:25,910
the crucial thing and this problem is that independent of the orthogonal part

625
00:44:25,930 --> 00:44:28,300
OK so if we evaluate

626
00:44:28,310 --> 00:44:31,990
the function f at an arbitrary point

627
00:44:32,020 --> 00:44:37,770
for of the training set solution will be independent of this thing here so therefore

628
00:44:37,770 --> 00:44:38,920
this first part

629
00:44:38,950 --> 00:44:43,780
which only depend on f through the training points

630
00:44:43,800 --> 00:44:47,050
is independent of the orthogonal part

631
00:44:49,160 --> 00:44:57,510
on the other hand if we look at the second term

632
00:44:57,530 --> 00:44:59,960
of the objective function so

633
00:44:59,990 --> 00:45:04,430
now we have shown that if our solution f as

634
00:45:04,440 --> 00:45:10,620
and some nonzero orthogonal components in this orthogonal the component will not affect the first

635
00:45:10,620 --> 00:45:14,410
part of our objective function in the second part of the proof will actually show

636
00:45:14,420 --> 00:45:18,620
that if we have a non zero components it will only increase this

637
00:45:18,620 --> 00:45:21,060
population inference

638
00:45:21,150 --> 00:45:22,300
we need to do inference

639
00:45:22,460 --> 00:45:23,670
around nineteen

640
00:45:25,370 --> 00:45:26,420
no matter what you do

641
00:45:26,430 --> 00:45:27,510
this one

642
00:45:27,540 --> 00:45:31,950
the value of the marginal likelihood of what you need to do is just

643
00:45:31,960 --> 00:45:33,190
marginal likelihoods

644
00:45:33,240 --> 00:45:35,660
friends and if you want the actual results

645
00:45:35,670 --> 00:45:38,750
different to our friends

646
00:45:38,810 --> 00:45:42,650
this is an operation you have to have code for

647
00:45:43,450 --> 00:45:46,220
you object to the best of the

648
00:45:46,240 --> 00:45:47,890
and it is

649
00:45:47,900 --> 00:45:52,000
he can be in a position to write code for the discrete ricci flow inference

650
00:45:52,000 --> 00:45:56,120
the inference was right because have to

651
00:45:56,190 --> 00:46:00,150
so he said all the things that are added do that in the end that

652
00:46:00,420 --> 00:46:02,920
just involves called here are

653
00:46:02,970 --> 00:46:07,220
the language so this is essentially no more work

654
00:46:07,260 --> 00:46:09,390
and you are asking

655
00:46:09,430 --> 00:46:14,680
and we're going to use this procedure monotonically improves life

656
00:46:14,700 --> 00:46:18,900
so always converges to a local optimum of the likelihood

657
00:46:18,920 --> 00:46:21,870
as indeed you are going to

658
00:46:21,890 --> 00:46:24,480
the right wrong

659
00:46:25,780 --> 00:46:33,780
so this optimisation strategy for objective functions can be interpreted like its present

660
00:46:33,790 --> 00:46:37,470
yes it is not an objective function is

661
00:46:37,480 --> 00:46:39,640
but maximum wage

662
00:46:39,780 --> 00:46:41,580
it's just not the end

663
00:46:41,620 --> 00:46:43,910
march mixture gases

664
00:46:44,900 --> 00:46:47,970
you should say

665
00:46:47,980 --> 00:46:50,470
training you

666
00:46:50,510 --> 00:46:53,680
you actually trading psychology

667
00:46:53,690 --> 00:46:56,540
so this just an optimisation and

668
00:46:56,560 --> 00:46:57,700
it is clever

669
00:46:57,710 --> 00:47:00,360
but there's nothing that says

670
00:47:05,550 --> 00:47:10,390
this is the crucial slide so

671
00:47:10,400 --> 00:47:14,540
if you have nothing to do with this proposition

672
00:47:16,610 --> 00:47:18,560
but this explains

673
00:47:20,200 --> 00:47:22,730
i gets really offended

674
00:47:22,750 --> 00:47:26,650
this that turns off and six

675
00:47:26,650 --> 00:47:29,730
it said about

676
00:47:29,780 --> 00:47:32,810
OK that's OK i'm about of this year

677
00:47:32,830 --> 00:47:39,730
so this slide open systems

678
00:47:44,080 --> 00:47:47,820
this might explains the way in which you can use

679
00:47:49,210 --> 00:47:53,150
the algorithm to optimize likelihood

680
00:47:53,150 --> 00:47:56,870
and and what it does is it is actually working

681
00:47:56,890 --> 00:47:57,930
the figure

682
00:47:57,940 --> 00:48:04,560
what it does is it defines a quantity called the expected complete log likelihood

683
00:48:04,590 --> 00:48:10,290
so this complete log likelihood here is the likelihood function that we would get if

684
00:48:10,290 --> 00:48:12,880
we knew the latent variables

685
00:48:12,890 --> 00:48:16,740
so it's just if we evaluate the likelihood of the model at a specific set

686
00:48:16,740 --> 00:48:19,650
of data x and a specific set of latent variables

687
00:48:19,750 --> 00:48:25,160
OK now for any distribution over the latent variables we can define something called the

688
00:48:25,160 --> 00:48:27,440
expected complete log likelihood

689
00:48:27,450 --> 00:48:30,970
which is if we sample the latent variables

690
00:48:30,980 --> 00:48:32,980
from this distribution q

691
00:48:32,990 --> 00:48:37,880
and then building those latent variables alongside our data and evaluate the log likelihood on

692
00:48:37,880 --> 00:48:40,410
average what log likelihood what we get

693
00:48:40,860 --> 00:48:43,890
so this is just some quantity which is the

694
00:48:43,910 --> 00:48:46,080
the expected complete

695
00:48:46,110 --> 00:48:50,410
likelihood under the model you would get if you were generating hidden variables

696
00:48:50,420 --> 00:48:52,370
from this distribution q

697
00:48:52,380 --> 00:48:55,980
OK now i i told think about the distribution q maybe it's

698
00:48:56,090 --> 00:49:00,760
the distribution maybe it's very similar to the true posterior it doesn't matter but for

699
00:49:00,760 --> 00:49:04,110
any distribution q we can write down the

700
00:49:04,240 --> 00:49:08,760
and the amazing fact is that this quantity

701
00:49:08,780 --> 00:49:12,170
can be used to create band on the like

702
00:49:12,190 --> 00:49:17,010
so this is like for my calculation but we're going through so is just area

703
00:49:17,260 --> 00:49:19,570
here is the marginal likelihood is the

704
00:49:20,030 --> 00:49:23,260
they can explore the probability of x

705
00:49:23,280 --> 00:49:29,080
and that is you know a lot of this sort of problem in this

706
00:49:29,360 --> 00:49:33,170
inside the summation of all point by the distribution

707
00:49:33,190 --> 00:49:35,360
q of variables

708
00:49:35,360 --> 00:49:39,690
so by multiplying by about the same thing certain changing his brain

709
00:49:40,360 --> 00:49:41,490
and now

710
00:49:41,510 --> 00:49:44,670
what i'm going do is i'm going to rewrite this

711
00:49:44,670 --> 00:49:46,150
distribution here

712
00:49:46,150 --> 00:49:47,650
as the

713
00:49:47,670 --> 00:49:52,820
first order of the law in this in this reversal of the order of log

714
00:49:52,960 --> 00:49:58,320
some expensive and is making is crucial quality content and called

715
00:49:58,340 --> 00:50:01,610
so i can never remember jensen's inequality goes

716
00:50:01,610 --> 00:50:05,220
whether it's the of the sum is greater than or less than the sum of

717
00:50:05,710 --> 00:50:10,110
but this to will be remembered so there's a lot

718
00:50:10,130 --> 00:50:14,260
and what i'm telling you is that if you take any two points on the

719
00:50:16,260 --> 00:50:20,050
and you take internal combination of

720
00:50:20,070 --> 00:50:22,550
so in interior sound

721
00:50:22,570 --> 00:50:27,650
the sum of those log values is always less than

722
00:50:27,670 --> 00:50:30,550
the log evaluated sun

723
00:50:30,550 --> 00:50:32,360
so once you

724
00:50:32,380 --> 00:50:35,490
so imagine that these are the two values

725
00:50:35,510 --> 00:50:36,690
and want

726
00:50:36,690 --> 00:50:39,780
if i had raised their blogs

727
00:50:39,800 --> 00:50:40,740
is it going to be

728
00:50:40,760 --> 00:50:44,670
more or less than fighting towards the average

729
00:50:45,280 --> 00:50:50,470
here's the to and you to lot at time this picture convince you that for

730
00:50:50,470 --> 00:50:55,740
any average to one average in feature combinations

731
00:50:55,740 --> 00:51:00,210
the average of their lives is always about what happens

732
00:51:00,240 --> 00:51:04,590
now this picture also tells you something else which is easy to forget

733
00:51:04,610 --> 00:51:08,710
if i take an exterior combination extend this line here

734
00:51:08,730 --> 00:51:11,260
this inequality no longer blocks

735
00:51:11,320 --> 00:51:16,690
so please do do not try to to apply this inequality when q is distribution

736
00:51:16,710 --> 00:51:19,510
it uses a set of positive numbers

737
00:51:19,530 --> 00:51:26,960
don't someone want to apply to replicate it extends line here doesn't work so use

738
00:51:28,650 --> 00:51:32,570
you can see that the average velocity of the west

739
00:51:32,590 --> 00:51:36,070
one of of the and that's we get this thing

740
00:51:36,070 --> 00:51:38,060
o three practical use to you

741
00:51:39,160 --> 00:51:45,490
o is algorithms that are present very easy to implement men fifty minutes

742
00:51:46,980 --> 00:51:52,650
really something useful so in the first section i will talk about

743
00:51:52,660 --> 00:51:55,950
spatiotemporal features and the classification

744
00:51:55,970 --> 00:52:01,230
of cells and in particular about shrinkage of the covariance matrix

745
00:52:01,250 --> 00:52:02,930
because of the

746
00:52:02,930 --> 00:52:10,790
explain that and the application that i wish to demonstrate these algorithms is classification of

747
00:52:10,790 --> 00:52:13,000
single trial he's in

748
00:52:13,130 --> 00:52:15,620
attention this fellow

749
00:52:17,910 --> 00:52:23,000
every short some some background is needed to understand the application

750
00:52:23,020 --> 00:52:24,380
so if

751
00:52:24,450 --> 00:52:26,250
it is defined as

752
00:52:26,780 --> 00:52:34,720
presented this is a sequence of stimuli and two types of stimuli one the

753
00:52:37,400 --> 00:52:43,250
presented called standard stimuli and the other type is infrequently

754
00:52:43,270 --> 00:52:45,570
presented that's called the

755
00:52:47,260 --> 00:52:57,200
and that this subject is attending and waiting this infrequent stimuli the specific components

756
00:52:57,280 --> 00:53:02,240
evoked most prominently the p three hundred of p street

757
00:53:02,250 --> 00:53:07,810
which is the positive peak approximately three hundred twenty seconds after stimulus presentation and it's

758
00:53:09,000 --> 00:53:14,750
century and earlier this also negative component and one

759
00:53:14,770 --> 00:53:19,980
and on the other hand if there a visual stimulation

760
00:53:19,980 --> 00:53:24,770
and the participants focuses on the stimulus in the

761
00:53:24,820 --> 00:53:30,240
presentation of the stimulus elicits visual evoked potential which

762
00:53:30,560 --> 00:53:34,050
maybe two components for

763
00:53:34,080 --> 00:53:36,790
according to p one second and one

764
00:53:36,800 --> 00:53:40,330
true positive negative component

765
00:53:40,350 --> 00:53:45,290
one hundred milliseconds after the stimulus presentation and this

766
00:53:45,410 --> 00:53:47,730
the visual cortex accepted

767
00:53:47,770 --> 00:53:55,330
so and based on these components this very well-known classic matrix speller

768
00:53:55,420 --> 00:53:58,880
proposed by don't you use very much in

769
00:53:58,890 --> 00:54:00,260
BCI research

770
00:54:00,280 --> 00:54:01,420
if you

771
00:54:01,440 --> 00:54:07,400
i want to start to be for example in this case new concentrate on the

772
00:54:07,400 --> 00:54:08,970
letter b

773
00:54:08,980 --> 00:54:12,530
and then randomly rows and columns

774
00:54:12,550 --> 00:54:15,490
flash intensified

775
00:54:18,250 --> 00:54:19,150
it's an

776
00:54:19,150 --> 00:54:27,150
kind of what experiment is introduced before so so then they infrequent stimulus type is

777
00:54:27,410 --> 00:54:32,650
the presentation of a stimulus value attention to so in this case the column that

778
00:54:32,690 --> 00:54:34,680
contains to be on the road

779
00:54:34,700 --> 00:54:36,130
containsa b

780
00:54:36,150 --> 00:54:41,230
and the stimuli intensification of other of some kind

781
00:54:41,240 --> 00:54:43,000
so we expect

782
00:54:43,040 --> 00:54:47,330
p three hundred only after presentation of the target drone target kind

783
00:54:49,260 --> 00:54:52,670
in one study that you can

784
00:54:52,670 --> 00:54:57,000
find more details about the poster w seven hundred years later

785
00:54:57,050 --> 00:55:02,280
he compares the classic as well as the something

786
00:55:02,310 --> 00:55:04,340
new that

787
00:55:04,350 --> 00:55:07,410
it is based on the text of the idea

788
00:55:07,410 --> 00:55:13,080
the government has introduced already but no in the visual attention based paradigms

789
00:55:13,100 --> 00:55:17,920
so it is quite similar to this again if you want to the

790
00:55:18,920 --> 00:55:21,240
the two steps to concentrate on

791
00:55:21,320 --> 00:55:23,830
as i said that contains the group

792
00:55:23,880 --> 00:55:25,090
and in

793
00:55:25,130 --> 00:55:27,710
circus flesh and random sequence

794
00:55:28,450 --> 00:55:30,760
if this group is selected then

795
00:55:30,870 --> 00:55:33,290
these five letters of the group

796
00:55:33,370 --> 00:55:34,870
ninety five

797
00:55:34,880 --> 00:55:37,410
two this layout and then the next time

798
00:55:37,430 --> 00:55:40,100
then i concentrate on the e

799
00:55:40,130 --> 00:55:41,890
so that they can be selected

800
00:55:43,550 --> 00:55:44,670
this is that is

801
00:55:44,670 --> 00:55:48,690
caesar's this post comes passes two

802
00:55:49,910 --> 00:55:52,310
better designs and also

803
00:55:52,640 --> 00:55:55,040
comparison of it against

804
00:55:55,050 --> 00:55:57,490
covert attention

805
00:55:57,490 --> 00:56:02,890
so this is in the data set being used to illustrate

806
00:56:02,890 --> 00:56:04,260
the average

807
00:56:04,560 --> 00:56:07,580
for the success rate

808
00:56:07,590 --> 00:56:08,940
spell out

809
00:56:08,950 --> 00:56:13,160
and these are the pieces of this is just an average over

810
00:56:13,990 --> 00:56:17,030
after stimulation of the target

811
00:56:17,340 --> 00:56:20,080
so circulant non targets

812
00:56:20,130 --> 00:56:25,140
then you see in the occipital area p one n one

813
00:56:25,140 --> 00:56:26,080
number of

814
00:56:26,090 --> 00:56:32,480
a small number of very rich people and a high number of poor people then

815
00:56:32,520 --> 00:56:33,570
the slope would be

816
00:56:37,530 --> 00:56:41,240
this would be a this is again graph from MSN messenger

817
00:56:41,280 --> 00:56:44,470
so the number of people

818
00:56:44,530 --> 00:56:50,940
a particular person these talks to on microsoft messenger so you would have

819
00:56:50,990 --> 00:56:54,020
number of people so on

820
00:56:56,550 --> 00:56:58,020
here it is

821
00:56:58,060 --> 00:57:00,700
this is in degree so

822
00:57:00,740 --> 00:57:02,110
a small number of

823
00:57:02,220 --> 00:57:05,450
people would be

824
00:57:05,500 --> 00:57:09,460
a very high number of people would have just a little bit of colleagues on

825
00:57:09,460 --> 00:57:10,990
the on the messenger in

826
00:57:11,060 --> 00:57:13,750
some people would have also

827
00:57:13,840 --> 00:57:17,860
thousands of people which they talk to on so this is

828
00:57:17,910 --> 00:57:20,720
extracted from the

829
00:57:20,730 --> 00:57:37,440
so probably this is done by this is generated from the data and then you

830
00:57:37,440 --> 00:57:40,250
have also these bots which

831
00:57:41,880 --> 00:57:45,300
are not people but they use are still registered users so this is

832
00:57:45,310 --> 00:57:50,410
now let's see if has

833
00:57:51,110 --> 00:57:56,100
about the for me it was not obvious

834
00:57:56,220 --> 00:58:01,170
for me to meet has one of the nice bot which talks to his friends

835
00:58:09,710 --> 00:58:13,910
related topic to this small world is also

836
00:58:13,910 --> 00:58:19,340
long tail so the concept of long tail you might know from

837
00:58:19,390 --> 00:58:24,270
so we got popular this book one of the things

838
00:58:24,310 --> 00:58:26,360
the that long tail was published

839
00:58:26,370 --> 00:58:29,130
from chris anderson i think which is

840
00:58:29,180 --> 00:58:33,080
editor-in-chief of wired magazine so the idea is the following

841
00:58:33,140 --> 00:58:37,010
but if we take amazon

842
00:58:40,030 --> 00:58:42,240
and here we have the

843
00:58:42,250 --> 00:58:45,280
both but with the end this the curve of the states

844
00:58:48,500 --> 00:58:52,300
this would be the peak of the most popular the most popular books which amazon

845
00:58:52,330 --> 00:58:54,020
or any other store

846
00:58:54,310 --> 00:58:56,240
is selling

847
00:58:58,000 --> 00:59:00,890
very much these companies can store

848
00:59:01,490 --> 00:59:05,410
these books in march in the bookstore and so if

849
00:59:05,860 --> 00:59:09,940
they have a bookstore then it would be able to store just does this speak

850
00:59:09,960 --> 00:59:13,770
because at some point you cannot store any more and more than a certain amount

851
00:59:13,770 --> 00:59:18,040
of books in in a bookstore and this will be the long tail the long

852
00:59:18,040 --> 00:59:20,010
tail of books which are

853
00:59:20,050 --> 00:59:25,660
less popular research less relevant for the masses

854
00:59:25,690 --> 00:59:32,640
and also in terms of sales of business among them less relevant but so what's

855
00:59:33,560 --> 00:59:35,680
m is on

856
00:59:35,720 --> 00:59:41,170
makes for even we have the proper science so that the percentage so this would

857
00:59:41,170 --> 00:59:42,570
be what

858
00:59:42,600 --> 00:59:44,880
forty three percent from the

859
00:59:44,890 --> 00:59:46,440
from the peak

860
00:59:46,480 --> 00:59:50,580
the most popular and most so fifty seven percent of all the sets with come

861
00:59:50,590 --> 00:59:52,430
from the long tail

862
00:59:52,480 --> 00:59:58,190
so this is something which which is internet economy brought somehow which reachability of this

863
00:59:58,190 --> 01:00:05,730
long tail and this long tail appears as relatively important concept in many

864
01:00:05,740 --> 01:00:16,090
how successful applications nowadays including wikipedia and everything and so this is kind of underlying

865
01:00:16,120 --> 01:00:19,910
reason for the success of many many applications

866
01:00:19,920 --> 01:00:24,770
including the companies like amazon and so on so

867
01:00:24,780 --> 01:00:27,720
this is well good to know

868
01:00:27,790 --> 01:00:32,350
OK here we have

869
01:00:32,390 --> 01:00:34,160
a few more features of the

870
01:00:34,180 --> 01:00:36,180
networks like

871
01:00:36,240 --> 01:00:43,290
network asylum so this is something which which is relevant from another perspective so we

872
01:00:43,290 --> 01:00:44,540
observe here

873
01:00:44,640 --> 01:00:48,240
clinic DVD of

874
01:00:49,330 --> 01:00:53,790
this never change as if we remove certain vertices

875
01:00:54,700 --> 01:00:59,430
that's a good application is so epidemics starting epidemics

876
01:00:59,640 --> 01:01:02,390
now the question is

877
01:01:02,390 --> 01:01:05,720
i think that he plus one is that minus it that he

878
01:01:05,740 --> 01:01:09,590
times gt right so we can plug that in your

879
01:01:09,600 --> 01:01:11,960
just expanded out

880
01:01:11,980 --> 01:01:16,350
now this first term if you stare at it long enough and hard enough you'll

881
01:01:16,350 --> 01:01:21,560
see that it's actually just land of the key

882
01:01:21,580 --> 01:01:24,840
now we get this

883
01:01:24,850 --> 01:01:26,690
this second term we

884
01:01:26,700 --> 01:01:29,110
we use the product rule

885
01:01:29,160 --> 01:01:32,380
of calculus to split it up

886
01:01:32,430 --> 01:01:33,910
into this partial

887
01:01:33,930 --> 01:01:38,610
and this partial

888
01:01:40,000 --> 01:01:42,790
for the second time i make an approximation

889
01:01:42,810 --> 01:01:47,300
so this is now approximate but i'm not going to justify here it's a reasonable

890
01:01:47,300 --> 01:01:52,920
approximation the second term becomes just a thirty times

891
01:01:52,970 --> 01:01:54,810
and this third term now

892
01:01:54,820 --> 01:01:56,610
is it thirty times

893
01:01:56,630 --> 01:02:00,620
the partial of the gradient with respect to the past

894
01:02:03,920 --> 01:02:08,940
if we we can now use the chain rule as before to split that into

895
01:02:08,940 --> 01:02:12,820
the partial of the gradient with respect to the parameters

896
01:02:12,830 --> 01:02:17,250
and and that is just the haitian the matrix of second derivatives

897
01:02:18,170 --> 01:02:22,150
the partial of the parameters with respect to the log

898
01:02:22,170 --> 01:02:22,980
and now

899
01:02:22,990 --> 01:02:27,540
this phase without the haitian here just land to be i times this

900
01:02:27,600 --> 01:02:28,970
is exactly

901
01:02:28,990 --> 01:02:31,350
what we said is lambda the key

902
01:02:32,960 --> 01:02:35,120
so we wind up with

903
01:02:36,510 --> 01:02:43,560
so out of all the gory maths comes as a nice compact little update equation

904
01:02:43,640 --> 01:02:49,110
it's a simple iterative update it allows us when we have the key to compute

905
01:02:49,260 --> 01:02:50,420
t plus one

906
01:02:50,440 --> 01:02:57,100
at time zero easier is just the room so it's easy to start up

907
01:02:57,110 --> 01:02:58,640
and it's moves

908
01:02:58,650 --> 01:03:04,330
correctly over multiple time steps in and also allows you to handle correlated input signals

909
01:03:04,330 --> 01:03:06,650
which is something that really screws up

910
01:03:06,670 --> 01:03:10,770
the algorithm that just uses pt times TT minus one you can imagine that if

911
01:03:10,770 --> 01:03:13,990
that algorithm gets short-term correlated data

912
01:03:14,000 --> 01:03:17,500
that it will disorder correlation will really be wrong

913
01:03:17,510 --> 01:03:18,690
the measures

914
01:03:18,700 --> 01:03:25,680
a property of the data instead of a property of your optimisation trajectory

915
01:03:26,350 --> 01:03:30,020
the one thing that a bit more some in this update is

916
01:03:30,070 --> 01:03:34,410
this HPV t right this is the haitian times

917
01:03:34,420 --> 01:03:36,280
the vector v

918
01:03:36,300 --> 01:03:42,980
the haitian is an n by n matrix if all search space has an dimensions

919
01:03:43,810 --> 01:03:46,860
so this normally doing this naively

920
01:03:46,880 --> 01:03:52,680
calculating the haitian is order in square multiplying it with the vector also ordered square

921
01:03:52,700 --> 01:03:58,130
so it seems that now we're down here in this realm of of expensive algorithms

922
01:03:58,350 --> 01:04:00,340
and this is not where i want to be

923
01:04:03,620 --> 01:04:06,520
it turns out fortunately

924
01:04:08,030 --> 01:04:12,610
taking its product with the vector of your choice

925
01:04:13,740 --> 01:04:15,140
the only

926
01:04:15,160 --> 01:04:19,290
sheep operations you can perform on the haitian

927
01:04:19,950 --> 01:04:22,000
that's very fortuitous

928
01:04:22,010 --> 01:04:26,650
course the whole algorithm is designed to exploit that

929
01:04:29,080 --> 01:04:32,930
this is sort of a bit of magic and i cannot explain it in great

930
01:04:34,270 --> 01:04:42,010
but for those who know calculus i can i can motivated a little bit

931
01:04:42,020 --> 01:04:45,190
what basically happens is

932
01:04:45,230 --> 01:04:49,370
it's done through differentials differentials

933
01:04:49,420 --> 01:04:51,750
i sort of hasan's two

934
01:04:51,800 --> 01:04:53,890
partial derivatives

935
01:04:55,170 --> 01:05:01,380
derivatives are chained backwards through a function was differentials change forward so

936
01:05:01,380 --> 01:05:04,880
one way to think of differential is it's

937
01:05:04,960 --> 01:05:09,200
it's a perturbation an infinitesimal perturbations

938
01:05:11,980 --> 01:05:17,340
and when you calculate with differentials you calculate how such a perturbation propagates

939
01:05:17,350 --> 01:05:18,890
through a function

940
01:05:20,180 --> 01:05:24,560
if we take a simple example lets say you know if i got the bad

941
01:05:26,930 --> 01:05:28,440
f of x

942
01:05:30,400 --> 01:05:34,120
sin x plus exquisite something

943
01:05:34,180 --> 01:05:36,260
really silly right

944
01:05:36,280 --> 01:05:40,100
now we say we apply a perturbation

945
01:05:40,110 --> 01:05:42,640
two that function

946
01:05:42,650 --> 01:05:44,180
so we

947
01:05:44,200 --> 01:05:46,390
what we're basically

948
01:05:46,640 --> 01:05:48,780
asking is well what is

949
01:05:48,800 --> 01:05:50,490
the f of x

950
01:05:50,500 --> 01:05:55,740
what is the perturbation of the function value

951
01:05:55,790 --> 01:06:00,330
and i want an expression for that now in terms of x and dx which

952
01:06:00,330 --> 01:06:01,200
is the

953
01:06:01,220 --> 01:06:04,610
perturbation of the input

954
01:06:06,670 --> 01:06:07,840
the way you can

955
01:06:07,870 --> 01:06:14,270
do this algebraically is these differentials these days obey the same rules as gradients

956
01:06:14,320 --> 01:06:20,250
in terms of calculus so there's the product rule this the chain rule

957
01:06:20,280 --> 01:06:22,680
that's the rule that any linear

958
01:06:22,780 --> 01:06:26,210
operator it you can just commute with so

959
01:06:27,870 --> 01:06:30,250
i won't do it in detail here but

960
01:06:30,260 --> 01:06:34,940
it will look very familiar to you if i seen on the of perfect

961
01:06:35,830 --> 01:06:39,020
cosine x

962
01:06:39,060 --> 01:06:42,590
and px

963
01:06:43,960 --> 01:06:48,150
two x

964
01:06:48,160 --> 01:06:51,940
now you know this this is called scientist

965
01:06:51,960 --> 01:06:56,450
the derivative of sin two x is the derivative of x square this is just

966
01:06:56,450 --> 01:06:58,260
the chain rule

967
01:06:59,110 --> 01:07:02,010
sorry i should say this is two x

968
01:07:05,310 --> 01:07:07,770
it winds up being sign x

969
01:07:07,790 --> 01:07:10,670
plus two x which is just the derivative

970
01:07:10,670 --> 01:07:12,000
of this function

971
01:07:12,020 --> 01:07:14,410
and dx

972
01:07:14,410 --> 01:07:19,400
so it's just a prime xtx

973
01:07:19,410 --> 01:07:24,570
OK so it's just sort of a standard algebraic manipulations the only thing is you

974
01:07:24,570 --> 01:07:28,180
using differentials where you know what everybody learns in

975
01:07:28,190 --> 01:07:31,190
in first calculus course is

976
01:07:31,200 --> 01:07:34,430
derivatives which work the other way round

977
01:07:34,450 --> 01:07:41,800
so now here's the magic tricks if

978
01:07:41,810 --> 01:07:43,100
i ask

979
01:07:43,160 --> 01:07:44,760
what is

980
01:07:44,760 --> 01:07:46,480
the differential

981
01:07:46,530 --> 01:07:49,260
of the gradient g

982
01:07:49,270 --> 01:07:52,020
one of my parameters

983
01:07:52,870 --> 01:07:55,970
by this argument it's actually

984
01:07:55,980 --> 01:07:57,290
she prime

985
01:07:57,310 --> 01:08:01,790
of the ten times the theatre

986
01:08:01,860 --> 01:08:06,110
ng prime is the haitian so this is a haitian

987
01:08:06,110 --> 01:08:11,740
that's an important difference and the second difference that actually for large residuals so you see here that

988
01:08:11,760 --> 01:08:15,010
the distribution is also much not in this case not much but it's wider

989
01:08:15,010 --> 01:08:19,780
than the two norm distribution so there's some large residuals that are larger than the

990
01:08:19,780 --> 01:08:22,850
largest residuals for the two norm

991
01:08:22,890 --> 01:08:27,600
so it's a wider distribution and you have this high peak at the origin

992
01:08:28,260 --> 01:08:31,530
so these two features are both useful

993
01:08:31,560 --> 01:08:33,200
so the first one that is

994
01:08:34,350 --> 01:08:38,450
is of course used in sparse optimization heuristics if you want a regularization

995
01:08:38,450 --> 01:08:43,760
term that encourages sparsity in a vector X then the one norm is much more useful

996
01:08:43,760 --> 01:08:45,490
than the two norm

997
01:08:45,530 --> 01:08:49,620
so in this application of course it's not surprising that if you have eighty

998
01:08:49,620 --> 01:08:55,680
variables and randomly generated A that you can make eighty of the residuals at exactly zero it's not very

999
01:08:55,680 --> 01:09:00,550
difficult but you might you could imagine an application where B is actually

1000
01:09:00,550 --> 01:09:01,910
equal to A X

1001
01:09:02,450 --> 01:09:08,030
and then an unknown number of values of B were corrupted by noise

1002
01:09:08,510 --> 01:09:13,740
and then you would like to find a solution X that satisfies many of

1003
01:09:13,740 --> 01:09:16,640
the equations exactly it's zero error

1004
01:09:17,220 --> 01:09:20,870
then this will be much more useful than the two norm

1005
01:09:21,640 --> 01:09:25,350
it won't give you exactly the solution with the maximum number of

1006
01:09:25,350 --> 01:09:27,600
equations that are satisfied exactly

1007
01:09:27,710 --> 01:09:32,370
it won't give you the sparsest vector of residuals but it'll give you typically a

1008
01:09:32,370 --> 01:09:33,430
very sparse

1009
01:09:36,220 --> 01:09:43,660
so the behavior at the origin is actually very useful for sparse optimization the

1010
01:09:43,700 --> 01:09:45,890
the fact that it's a wider distribution

1011
01:09:46,180 --> 01:09:49,550
is useful in robust regression

1012
01:09:49,560 --> 01:09:52,700
and of course the fact that its wider comes from the fact that the

1013
01:09:52,830 --> 01:09:54,550
absolute value penalty

1014
01:09:54,620 --> 01:09:59,280
increases much more slowly for large values of residuals the the quadratic penalty

1015
01:09:59,950 --> 01:10:04,530
so there's a smaller relatively smaller penalty on very large residuals than for the

1016
01:10:04,530 --> 01:10:07,780
quadratic penalty

1017
01:10:09,600 --> 01:10:12,580
that's the

1018
01:10:14,990 --> 01:10:18,330
no no it's a fixed matrix I selected the matrix A

1019
01:10:18,810 --> 01:10:20,640
and a vector B

1020
01:10:21,120 --> 01:10:22,990
then I solved these two problems

1021
01:10:23,080 --> 01:10:25,030
and obtained two solutions X

1022
01:10:25,060 --> 01:10:30,470
the least squares solution and the one norm solution and then this is the distribution of the

1023
01:10:30,470 --> 01:10:33,140
residuals in a vector AX minus B

1024
01:10:33,180 --> 01:10:38,580
so AX minus B is a vector of length two hundred you have two hundred errors and those two hundred equations

1025
01:10:39,240 --> 01:10:42,310
and this is the distribution around

1026
01:10:47,680 --> 01:10:51,640
so roughly speaking you would like to you minimize the norm of the vector in

1027
01:10:51,640 --> 01:10:54,430
order to make all the residuals as small as possible

1028
01:10:54,580 --> 01:10:56,490
but you have two hundred residuals

1029
01:10:57,010 --> 01:10:59,410
that you minimize by single scalar objective

1030
01:10:59,850 --> 01:11:02,810
and you see that the distribution of the residuals

1031
01:11:02,810 --> 01:11:05,370
for the two solutions X is actually very different

1032
01:11:05,410 --> 01:11:08,470
and depending on your application you might prefer one or the other

1033
01:11:09,620 --> 01:11:12,240
because of this difference in distributions

1034
01:11:13,680 --> 01:11:21,120
yeah so this has a large number of zeros typically the one norm will encourage sparsity

1035
01:11:21,180 --> 01:11:23,180
in this case in the vector AX minus B

1036
01:11:23,490 --> 01:11:28,490
and a second important difference is that it's a much wider distribution

1037
01:11:28,490 --> 01:11:29,950
than the two norm

1038
01:11:30,220 --> 01:11:34,180
and the second property is used in robust regression

1039
01:11:37,490 --> 01:11:42,640
you've probably seen this suppose you want to fit a straight line to points

1040
01:11:42,640 --> 01:11:47,470
but there are some outliers there's a point here and here then the two norm which is en dashed

1041
01:11:47,470 --> 01:11:50,200
lined will be rotated toward these ourliers

1042
01:11:50,240 --> 01:11:54,510
whereas if you minimize the one norm you minimize the sum of the absolute values of the

1043
01:11:54,760 --> 01:11:58,950
differences between the points and the line then you get something that almost ignores these

1044
01:11:58,950 --> 01:12:03,080
two outliers and gives you this straight approximation

1045
01:12:03,120 --> 01:12:08,100
so that uses the fact that the one norm is much more robust against

1046
01:12:08,260 --> 01:12:11,760
or can tolerate a few large errors

1047
01:12:12,010 --> 01:12:21,720
so I think you're all familiar with linear discrimination and support vector machines of course that's

1048
01:12:21,720 --> 01:12:25,910
a very nice application of linear programming suppose you have two sets of points X I

1049
01:12:26,290 --> 01:12:31,180
and Y I and you try to find a hyperplane that strictly separates the two

1050
01:12:31,530 --> 01:12:33,990
then you can write that as a set of linear inequalities

1051
01:12:33,990 --> 01:12:40,180
so you have strict separation the variables are A and B so the normal vector of the hyperplane and

1052
01:12:40,200 --> 01:12:43,550
the offsets but because it's

1053
01:12:45,200 --> 01:12:50,120
you can replace the right-hand side by one and then use a nonstrict inequality

1054
01:12:50,260 --> 01:12:52,470
and you can write it like this

1055
01:12:52,580 --> 01:12:57,140
if the points are not separable

1056
01:12:57,850 --> 01:12:59,910
you can

1057
01:12:59,950 --> 01:13:06,830
try to find a hyperplane that approximately separates the two points by minimizing this kind of

1058
01:13:06,830 --> 01:13:07,410
penalty it's

1059
01:13:08,250 --> 01:13:12,450
an piece-wise linear function of the variables A and B

1060
01:13:16,160 --> 01:13:18,100
so for each

1061
01:13:18,140 --> 01:13:21,780
points X Y or Y I you have a penalty of zero if the point is on the right

1062
01:13:21,780 --> 01:13:28,350
side of the hyperplane and otherwise you have a penalty that's proportional to the

1063
01:13:28,350 --> 01:13:30,050
difference to the hyperplane

1064
01:13:30,060 --> 01:13:32,550
so you can

1065
01:13:32,560 --> 01:13:35,370
interpret as a non symmetric version of this

1066
01:13:35,780 --> 01:13:39,100
so here you have a penalty on the residuals

1067
01:13:39,120 --> 01:13:44,390
if you use the absolute value that's symmetric so suppose you actually want to minimize the max

1068
01:13:44,390 --> 01:13:48,560
minimize the number of maximize the number of zeros in the vector

1069
01:13:48,620 --> 01:13:53,630
so you really want to minimize the cardinality of this vector then the ideal penalty function would be

1070
01:13:53,630 --> 01:13:56,950
zero at the origin and one for nonzero

1071
01:13:57,760 --> 01:14:01,990
and then there's some of those penalty functions will be the number of nonzero

1072
01:14:01,990 --> 01:14:08,860
two we continue discussing basically the class sequential monte carlo methods so what we do

1073
01:14:08,880 --> 01:14:15,970
is essentially yesterday basically consider the following problem given target distribution pi x only one

1074
01:14:15,970 --> 01:14:21,330
visit the port use some samples approximate distributed according to x on one of the

1075
01:14:21,330 --> 01:14:23,150
still not possible to do that

1076
01:14:23,150 --> 01:14:28,100
basically instead in computational statistics is to build a markov chain to use markov chain

1077
01:14:28,100 --> 01:14:33,560
monte carlo techniques so my multicolour techniques are treaty right or is basically to produce

1078
01:14:33,560 --> 01:14:38,050
samples from a given target distribution to know what i'm considering noise the deform problem

1079
01:14:38,210 --> 01:14:42,010
which is so in some ways spectral little bit more challenging because what i'm trying

1080
01:14:42,020 --> 01:14:47,760
to do is that i'm trying to obtain so paul from sequence of target distribution

1081
01:14:47,870 --> 01:14:53,300
pi events so each physically target distribution is known up to a normalizing constant on

1082
01:14:53,300 --> 01:14:59,370
basically those target distribution defined a sequence of spaces of increasing dimensions so you by

1083
01:14:59,370 --> 01:15:02,440
one that if and only if x one y two x one x two on

1084
01:15:02,440 --> 01:15:07,210
cell and so forth when i want to have an algorithm that portis sequentially approximate

1085
01:15:07,210 --> 01:15:10,870
sample of pi one that approximate sample of by two and so on and so

1086
01:15:10,870 --> 01:15:12,550
forth and this is what i want to do

1087
01:15:12,570 --> 01:15:14,660
two she that basically

1088
01:15:14,830 --> 01:15:20,470
this is going to rely essentially on iterative algorithms such an MCMC so essentially what

1089
01:15:20,470 --> 01:15:25,960
i'm doing and using out it like stone multicolour techniques which is known in the

1090
01:15:25,960 --> 01:15:31,650
literature as importance sampling so it it work involves hoping when you're interested in a

1091
01:15:31,650 --> 01:15:34,410
given in approximating given

1092
01:15:34,430 --> 01:15:37,660
o point two distributions pyrex OK

1093
01:15:37,680 --> 01:15:43,210
then well it properly paul force it is very similar essentially to a rejection sampling

1094
01:15:43,210 --> 01:15:46,970
in the sense that you do introduce

1095
01:15:46,990 --> 01:15:53,830
proposal distribution importance distribution qx with support includes the support of five x on what

1096
01:15:53,830 --> 01:15:58,150
you do some more capital in time from q of x you've picked two x

1097
01:15:58,150 --> 01:16:02,790
so that it's easy to forty samples from it that provide you've basically

1098
01:16:03,380 --> 01:16:08,350
multicellular approximation of qx on then you basically used the fact that prior based on

1099
01:16:08,350 --> 01:16:16,290
the express essentially is the weighted version of qx you substitute basically qx substitutes multicellular

1100
01:16:16,360 --> 01:16:20,020
approximation all you obtain essentially

1101
01:16:20,030 --> 01:16:27,180
now approximation of pi of x as the weighted basically combination convex combination of that

1102
01:16:27,210 --> 01:16:32,550
that york mass located at basically the particle location i which are sampled from q

1103
01:16:32,550 --> 01:16:39,460
x you also obtain an approximation of the normalizing constant

1104
01:16:39,490 --> 01:16:43,900
which is not which is not something you do can obtain easily when you're doing

1105
01:16:45,330 --> 01:16:50,490
so this is very easy matter to use OK i told you basically the problem

1106
01:16:50,490 --> 01:16:55,170
with it unfortunately is that it doesn't scale it doesn't scale in the sense that

1107
01:16:55,420 --> 01:17:00,650
as soon as you're dealing with actually i dimensional problem in defiance of u estimate

1108
01:17:00,650 --> 01:17:02,740
increases exponentially with the dimension

1109
01:17:02,930 --> 01:17:08,370
stephen for the timing we gonna limit also have to to importance sampling of basically

1110
01:17:08,370 --> 01:17:15,270
later we present to show explicitly of it's possible to reduce drastically the violence of

1111
01:17:15,270 --> 01:17:18,870
this important something is made by using a simple trick OK

1112
01:17:18,890 --> 01:17:21,770
so for the time being what i want to do what i'm going to do

1113
01:17:21,840 --> 01:17:23,610
is essentially

1114
01:17:23,620 --> 01:17:25,200
come three

1115
01:17:25,210 --> 01:17:28,370
describe an importance sampling techniques

1116
01:17:28,400 --> 01:17:34,640
which can be implemented sequentially on the computer in england sampling technique that would provide

1117
01:17:34,640 --> 01:17:36,250
me with approximation

1118
01:17:36,270 --> 01:17:43,080
of the sequence of target distribution pi and on basically approximation of the normalising associated

1119
01:17:43,080 --> 01:17:44,770
normalising constants

1120
01:17:45,700 --> 01:17:48,560
so the way policy is very simple

1121
01:17:49,400 --> 01:17:50,900
your time one

1122
01:17:50,920 --> 01:17:56,300
on essentially time one which you're interested in approximating is the target distribution

1123
01:17:56,330 --> 01:17:57,580
pi one OK

1124
01:17:57,580 --> 01:18:03,920
on its associated normalizing constant first iteration what you do you use absolutely the story

1125
01:18:03,930 --> 01:18:06,530
about essentially importance sampling

1126
01:18:06,550 --> 01:18:09,710
i think that is you introduce

1127
01:18:09,740 --> 01:18:15,430
nine poland density that we denote you are on what you do use onboard captain

1128
01:18:15,590 --> 01:18:20,690
the sampled from q one OK that give you a multicolour approximation of q one

1129
01:18:20,920 --> 01:18:25,110
of you substitute that into the importance sampling identity

1130
01:18:25,110 --> 01:18:26,280
OK which is where

1131
01:18:26,290 --> 01:18:32,370
as your recall there on that provide with approximation of pi one basically

1132
01:18:32,370 --> 01:18:38,360
weighted a convex combination of that that york mass on an approximation of the associated

1133
01:18:38,360 --> 01:18:43,990
normalizing constant OK will remember that the weights w one of x one is a

1134
01:18:43,990 --> 01:18:49,240
measure of discrepancy between the target distribution pi while on tour OK so that's at

1135
01:18:49,240 --> 01:18:54,240
iteration one what you do is basically stand on the importance sampling so as to

1136
01:18:54,240 --> 01:18:55,880
approximate by one

1137
01:18:58,100 --> 01:18:59,510
comes time to

1138
01:19:00,530 --> 01:19:04,740
all that time to you interested knowing approximating anymore

1139
01:19:04,760 --> 01:19:12,420
pi one you're interested essentially in approximating i basically the emotional point distribution which is

1140
01:19:13,610 --> 01:19:17,850
OK i need to present arguments x one x two OK

1141
01:19:18,700 --> 01:19:23,340
if i want to use importance sampling to approximate this target distribution

1142
01:19:23,350 --> 01:19:29,010
i need to introduce an important distribution due to which depends on two arguments x

1143
01:19:29,010 --> 01:19:30,240
one x two

1144
01:19:31,120 --> 01:19:35,310
no basically if i were to do that so we set time two

1145
01:19:35,320 --> 01:19:39,360
basically what i do i need to produce new samples from q two so as

1146
01:19:39,360 --> 01:19:45,040
to approximate by two then basically typically the computational time with decrease with time index

1147
01:19:45,040 --> 01:19:48,620
because you have to sample at time and you would like to approximate by and

1148
01:19:48,620 --> 01:19:53,370
of x one x two xn so you would have to introduce the importance distribution

1149
01:19:53,370 --> 01:19:56,820
qn of x one x two xn you would have to sample an increasing number

1150
01:19:56,820 --> 01:20:03,110
of volleyball because the dimension of the target point distribution increased OK so you want

1151
01:20:03,110 --> 01:20:07,660
to be lazy and to say all basically have already done some work at time

1152
01:20:07,660 --> 01:20:13,780
to time one because at time warner generate it's essentially some component x one according

1153
01:20:13,780 --> 01:20:15,200
to q one OK

1154
01:20:15,220 --> 01:20:19,840
so what i want to do is essentially i want to recycle

1155
01:20:19,860 --> 01:20:21,380
the song

1156
01:20:21,450 --> 01:20:24,250
the multicellular sample

1157
01:20:24,300 --> 01:20:30,070
that that generated from q one at time one basically to build my importance sampling

1158
01:20:30,070 --> 01:20:32,430
approximation of pi two

1159
01:20:32,430 --> 01:20:38,680
OK so essentially you some of multicolored sample at time one you want to reuse

1160
01:20:38,680 --> 01:20:42,630
so approximation techniques

1161
01:20:44,520 --> 01:20:49,010
so let's talk about the training media support vector machines for

1162
01:20:49,020 --> 01:20:52,350
so need is a function here

1163
01:20:52,370 --> 01:20:55,300
so this is not in the original space

1164
01:20:55,310 --> 01:20:58,100
so this is

1165
01:20:58,130 --> 01:21:01,250
standard bayesian approach

1166
01:21:01,260 --> 01:21:12,540
when saw it is the solution is one is selected variables so this is going

1167
01:21:12,540 --> 01:21:18,510
to satisfy this condition is maximum of lots

1168
01:21:18,540 --> 01:21:24,570
we must not believe is of course true because we have we now named the

1169
01:21:24,570 --> 01:21:26,600
whole to see that it must be maximum

1170
01:21:26,820 --> 01:21:31,570
most is this is a variable able to the right hand side

1171
01:21:31,590 --> 01:21:34,910
so it is going to articles one minus the flight

1172
01:21:34,910 --> 01:21:37,030
it is greater than this

1173
01:21:39,420 --> 01:21:44,750
so we know it is greater than this but not only that

1174
01:21:44,770 --> 01:21:49,200
even if it is positive so if this is possible

1175
01:21:49,630 --> 01:21:54,590
so that means for example in the states minus five

1176
01:21:54,690 --> 01:22:00,280
it's my so even more minus five for the right hand side so that six

1177
01:22:00,280 --> 01:22:05,930
sort there should be grocery equal six but you really want to use six w

1178
01:22:05,930 --> 01:22:08,740
seven or eight years later

1179
01:22:08,740 --> 01:22:10,480
so why because you

1180
01:22:10,490 --> 01:22:13,330
this certainly in the objective function

1181
01:22:13,350 --> 01:22:17,450
you are minimizing the summation of selected variables

1182
01:22:17,450 --> 01:22:20,720
so you in a situation where you

1183
01:22:20,730 --> 01:22:25,090
you want to make this inequality can be any to be equal to the equality

1184
01:22:25,090 --> 01:22:28,610
so that the change the objective is smaller

1185
01:22:28,750 --> 01:22:33,490
so that's why i'm actually there is such a relationship

1186
01:22:34,960 --> 01:22:39,600
we can source the so we actually read write

1187
01:22:39,630 --> 01:22:42,970
the primal SVM problem in different way

1188
01:22:43,020 --> 01:22:47,740
is to be that these so we actually want to select variable we don't believe

1189
01:22:47,740 --> 01:22:55,910
that because it's already these already selected variables

1190
01:22:55,920 --> 01:23:00,560
and the reason why i write this problem to being useful is not easy to

1191
01:23:00,560 --> 01:23:01,010
see that

1192
01:23:01,460 --> 01:23:03,930
he the number of variables

1193
01:23:05,280 --> 01:23:09,590
so so for this optimisation problem one thing is the number of variables is the

1194
01:23:09,590 --> 01:23:12,320
same as number of features one

1195
01:23:12,330 --> 01:23:18,440
one is for this day this is the city's only scalar variable we plus one

1196
01:23:18,440 --> 01:23:22,510
here and the size of data is the same as the number of features

1197
01:23:22,530 --> 01:23:25,840
so the number of variables is justice

1198
01:23:25,850 --> 01:23:29,770
let me even the number of features is small

1199
01:23:29,810 --> 01:23:33,340
it's not very even very large data sets

1200
01:23:33,350 --> 01:23:36,550
but the number of features is small then

1201
01:23:36,620 --> 01:23:39,580
yes it is probably still

1202
01:23:39,610 --> 01:23:42,400
there's many variables is not

1203
01:23:43,200 --> 01:23:49,060
according to these the number of variables is thing that says that you have to

1204
01:23:49,710 --> 01:23:50,270
two two

1205
01:23:51,170 --> 01:23:53,900
in a situation the easy to so that

1206
01:23:53,920 --> 01:23:55,120
it's not going to

1207
01:23:55,200 --> 01:23:59,910
the details of how to solve these kind of things but

1208
01:23:59,920 --> 01:24:02,150
consensually now because of

1209
01:24:02,160 --> 01:24:08,000
fewer number of variables then they should be easy

1210
01:24:08,010 --> 01:24:18,730
so we can apply traditional optimisation is is directly applying traditional optimisation this to minimize

1211
01:24:18,760 --> 01:24:23,010
these image that this is not the just small her

1212
01:24:23,020 --> 01:24:28,040
maximize the zero something so this is not differentiable so

1213
01:24:28,050 --> 01:24:29,020
you can use

1214
01:24:29,110 --> 01:24:31,600
we thus was optimisation techniques

1215
01:24:31,620 --> 01:24:37,940
they also quite standard optimisation but also seems for use regular news

1216
01:24:42,330 --> 01:24:47,510
they don't going to design a anything this is just traditional once a day

1217
01:24:47,510 --> 01:24:50,740
some of them can be directly used and

1218
01:24:50,750 --> 01:24:53,030
OK so very large

1219
01:24:57,200 --> 01:25:03,630
well it's training similar to missus which is the largest croatian is also in the

1220
01:25:03,650 --> 01:25:05,830
culture minimisation

1221
01:25:05,860 --> 01:25:13,830
so you quite similar to traditional optimisation missus in the training time is

1222
01:25:13,850 --> 01:25:17,090
well then the question is what even the number of features

1223
01:25:17,630 --> 01:25:24,800
also the number of instances of course very large with this is changing case

1224
01:25:24,820 --> 01:25:30,470
so now we know the number of training instances is large and also the dimensionality

1225
01:25:30,480 --> 01:25:34,020
number of features is large then

1226
01:25:34,050 --> 01:25:34,900
it is

1227
01:25:35,040 --> 01:25:37,340
quite difficult

1228
01:25:37,360 --> 01:25:44,260
i think existing new you can use to handle many millions of

1229
01:25:44,270 --> 01:25:45,450
o point

1230
01:25:45,460 --> 01:25:49,450
running after several hours or days

1231
01:25:49,970 --> 01:25:54,770
if you have you know the analysis where

1232
01:25:54,840 --> 01:25:57,410
and that's the problem

1233
01:25:57,980 --> 01:26:02,880
it allows make sure that the problem of this because document

1234
01:26:02,990 --> 01:26:08,460
problems with respect to model so the number of features is actually a large

1235
01:26:08,470 --> 01:26:14,590
and you will see a lot of documents in the last century situation that is

1236
01:26:14,610 --> 01:26:17,980
quite difficult

1237
01:26:17,990 --> 01:26:19,750
OK so

1238
01:26:19,770 --> 01:26:23,030
let's so you have another equation

1239
01:26:23,210 --> 01:26:26,690
because we use decomposition process to solve

1240
01:26:26,700 --> 01:26:28,630
two so that the sphere

1241
01:26:28,660 --> 01:26:33,940
the reason to use it because of handling memory problems

1242
01:26:34,420 --> 01:26:39,690
but if you say oh i don't want to software packages one twenty years here

1243
01:26:39,710 --> 01:26:45,540
and another fourteen years you can can decomposition this is still so

1244
01:26:45,570 --> 01:26:48,540
large is the problem

1245
01:26:48,560 --> 01:26:52,540
well there are several difficulties

1246
01:26:52,690 --> 01:26:56,590
we can show even a number of features

1247
01:26:56,650 --> 01:26:59,790
is small that if you use decomposition this is

1248
01:27:00,200 --> 01:27:08,670
when this season large overseas the penalty parameter introduced the objective function learning is very

1249
01:27:08,670 --> 01:27:16,590
slow convergence that is very small as it is only like this seven hundred instances

1250
01:27:17,280 --> 01:27:23,680
now we are using a k two hundred sixty so many iterations to finish the

1251
01:27:23,680 --> 01:27:26,590
decomposition procedure so this is not good

1252
01:27:26,600 --> 01:27:28,620
so we can resolve projects

1253
01:27:28,630 --> 01:27:34,170
and this one is maybe ten or ten features or something and we're using the

1254
01:27:34,650 --> 01:27:37,770
you so we don't we use kernel home

1255
01:27:37,840 --> 01:27:43,680
so what is happening right now is we have we have a kernel matrix to

1256
01:27:43,680 --> 01:27:50,420
be like is so i j component of the kernel is the inner product

1257
01:27:50,420 --> 01:27:56,420
so this is a bi l matrix and the number of training instances ring

1258
01:27:56,520 --> 01:28:01,680
because of these the rate features or the number of features is small so the

1259
01:28:01,680 --> 01:28:05,550
rank is actually a very very small number so the

1260
01:28:05,580 --> 01:28:11,430
so it's so not like you use and armenia kernels like RDF

1261
01:28:11,440 --> 01:28:17,990
you get you get a positive definite kernel design is is smaller and you only

1262
01:28:17,990 --> 01:28:19,410
have positive side

1263
01:28:19,430 --> 01:28:22,200
kernel matrix

1264
01:28:22,210 --> 01:28:26,600
we suspect that because such kind of

1265
01:28:26,630 --> 01:28:34,010
the optimisation problem is more you conditions so four coordinate wise minimisation convergence is very

1266
01:28:34,010 --> 01:28:39,920
and these gamma functions are basically well they're called gamma functions and they're given by

1267
01:28:39,920 --> 01:28:42,090
this integral okay

1268
01:28:42,240 --> 01:28:48,650
basically that's the normalization constants so Dirichlet distributions are used all over the

1269
01:28:48,650 --> 01:28:53,940
place in probabilistic modeling and the main reason for that is be basically that

1270
01:28:53,940 --> 01:28:58,990
is the standard distribution for probability vectors and this is due to the

1271
01:28:58,990 --> 01:29:02,780
a fact that they are conjugate to the multinominal

1272
01:29:02,800 --> 01:29:07,880
distribution and particularly conjugate to discrete distributions

1273
01:29:10,820 --> 01:29:15,670
just to visualize what these densities look like okay if the

1274
01:29:15,940 --> 01:29:21,280
so here's the Dirichlet density down here

1275
01:29:21,320 --> 01:29:26,000
where I've kind of generalized is to one where alpha here is a vector

1276
01:29:26,320 --> 01:29:28,360
where every entry is positive okay

1277
01:29:28,400 --> 01:29:31,780
so that alpha that's a typo

1278
01:29:31,820 --> 01:29:37,610
should be distribute k goes to one to big k of pi k raise to the alpha k minus one

1279
01:29:39,150 --> 01:29:42,490
and when alpha is

1280
01:29:42,530 --> 01:29:46,720
a vector ones so that's the vector ones then we see that this part

1281
01:29:46,720 --> 01:29:50,840
here's gonna be pi k raise to the one minus one and that's just gonna be

1282
01:29:50,840 --> 01:29:54,170
pi k raise to the zero and that's just gonna be one okay

1283
01:29:54,190 --> 01:29:58,190
and what that says is that the Dirichlet is gonna be a uniform distribution

1284
01:29:58,190 --> 01:30:00,990
over the probability simplex

1285
01:30:01,570 --> 01:30:03,220
when alpha is

1286
01:30:03,260 --> 01:30:06,300
greater than one so for example if it's two two two

1287
01:30:06,860 --> 01:30:10,920
the Dirichlet distribution is gonna be a unimodel distribution

1288
01:30:11,300 --> 01:30:12,210
with a

1289
01:30:12,320 --> 01:30:18,780
with a mean given by basically this vector normalized okay so the mean is gonna be one

1290
01:30:18,780 --> 01:30:20,610
third one third one third

1291
01:30:20,940 --> 01:30:28,280
and the total mass of the alpha here basically describes how concentrated the

1292
01:30:28,320 --> 01:30:29,710
the mode is

1293
01:30:29,710 --> 01:30:30,920
around the

1294
01:30:33,690 --> 01:30:38,490
so when alpha is five five five then we see that the that distribution's

1295
01:30:38,490 --> 01:30:41,030
more concentrated around its mean

1296
01:30:41,070 --> 01:30:47,440
here okay and if the alpha factor is not symmetric so for example as two five five

1297
01:30:48,210 --> 01:30:52,550
the more it gets shifted off the centre of the probability simplex to one of the

1298
01:30:52,550 --> 01:30:53,340
edges or

1299
01:30:53,420 --> 01:30:55,170
to a corner here

1300
01:30:55,650 --> 01:31:00,740
and more interestingly if the alphas are less than one then we see that instead of

1301
01:31:00,740 --> 01:31:06,090
unimodel distribution we have a multimodal distribution where we have high probability around the

1302
01:31:06,090 --> 01:31:11,440
corners and low probability in the centre of the probability simplex okay

1303
01:31:13,440 --> 01:31:14,510
so that's just

1304
01:31:14,510 --> 01:31:19,510
it's a visualization of what the Dirichlet distribution is

1305
01:31:19,550 --> 01:31:21,760
right so as I say

1306
01:31:21,780 --> 01:31:26,990
the Dirichlet distribution is kind of all over the place in probabilistic modeling mainly because its

1307
01:31:26,990 --> 01:31:28,900
conjugate to the multinominal right

1308
01:31:31,170 --> 01:31:32,320
let's see how this

1309
01:31:32,400 --> 01:31:33,920
conjugacy work okay

1310
01:31:34,070 --> 01:31:40,190
if you look at the joint's distribution over the pri over pi and the in

1311
01:31:40,190 --> 01:31:44,610
cluster indicated variables z okay so that's the prior that's

1312
01:31:44,610 --> 01:31:48,800
the conditional probability of each z i given pi okay

1313
01:31:48,820 --> 01:31:52,510
so the prior is given by this Dirichlet

1314
01:31:53,360 --> 01:31:56,990
the conditional probabilities we could collect up

1315
01:31:56,990 --> 01:32:02,260
in into a term like this so basically every time z i takes on value k

1316
01:32:02,280 --> 01:32:07,050
then we're gonna have a pi k contribution and and n k is the number of

1317
01:32:07,050 --> 01:32:09,510
z ies that take on value k okay

1318
01:32:09,530 --> 01:32:13,880
so we can so if there're n keys and ies that take on value k then

1319
01:32:14,050 --> 01:32:17,820
we basically have pi k raise to the n k times right

1320
01:32:18,360 --> 01:32:22,300
and this is over k from one to big k

1321
01:32:22,970 --> 01:32:28,510
and you can see that the conditional probability of the z vector here

1322
01:32:28,510 --> 01:32:33,400
and the pri the Dirichlet prior on pi they kind of have a similar functional

1323
01:32:33,400 --> 01:32:39,720
form and if you actually multiply this two probabilities together you get the joint's probability

1324
01:32:39,760 --> 01:32:46,710
it's gonna be this normalization times product k goes to one to big k of pi k raise to the

1325
01:32:46,710 --> 01:32:50,940
alpha divided by k plus n k minus one rigth

1326
01:32:51,320 --> 01:32:55,050
and so if you normalize this

1327
01:32:55,210 --> 01:32:59,170
so that you get the posterior distribution of pi given z then we see that

1328
01:32:59,170 --> 01:33:04,530
posterior distribution will also be a Dirichlet okay but with parameters updated

1329
01:33:04,580 --> 01:33:05,740
by n k

1330
01:33:07,400 --> 01:33:15,010
well the marginal distribution over the z vector here we have computed by basically taking

1331
01:33:15,010 --> 01:33:17,240
the ratio of these probabilities okay

1332
01:33:17,360 --> 01:33:19,990
so the basically

1333
01:33:19,990 --> 01:33:23,780
yeah so if you take the

1334
01:33:23,860 --> 01:33:28,570
joints divided by the posterior you get the marginal probability okay

1335
01:33:30,170 --> 01:33:34,840
and the fact that I can write down both of these equations says that both

1336
01:33:34,840 --> 01:33:37,090
Sergio Verdú is the

1337
01:33:37,110 --> 01:33:41,420
Eugene Higgins Professor of Electrical Engineering at Princeton University

1338
01:33:41,450 --> 01:33:46,960
and he is one of the premier information theorists of our time.

1339
01:33:47,150 --> 01:33:52,000
Sergio is probably best known for his work on multi-user detection, which is

1340
01:33:52,410 --> 01:33:57,200
fundamental and critical to modern wireless communications.

1341
01:33:57,220 --> 01:34:00,320
He has made fundamental contributions to

1342
01:34:00,340 --> 01:34:01,980
information theory

1343
01:34:02,000 --> 01:34:07,890
in its fundamentals in terms of finding the capacity of several classes of important channels

1344
01:34:07,890 --> 01:34:09,040
that are

1345
01:34:10,610 --> 01:34:14,290
critical in modern wireless communication systems

1346
01:34:14,310 --> 01:34:16,690
and he has also

1347
01:34:16,710 --> 01:34:22,420
developed and uncovered some surprising and very cool connections between

1348
01:34:25,570 --> 01:34:30,920
mean square error estimation and mutual information and I think he's

1349
01:34:30,930 --> 01:34:34,030
going to tell us a little bit about some of that today.

1350
01:34:34,040 --> 01:34:36,250
Sergio has

1351
01:34:36,280 --> 01:34:39,920
too many awards and accolades to mention

1352
01:34:39,950 --> 01:34:43,110
all of them, unless I took up his entire slot

1353
01:34:43,120 --> 01:34:45,150
but I'll mention just a few

1354
01:34:45,220 --> 01:34:47,390
he is a Fellow of the IEEE

1355
01:34:47,480 --> 01:34:52,280
he received the IEEE Third Millennium Medal in 2000

1356
01:34:52,280 --> 01:34:56,200
the Claude Shannon Award in 2007, and I think he was one of the

1357
01:34:57,150 --> 01:34:59,280
people ever to receive that honor

1358
01:34:59,330 --> 01:35:03,750
and he received the Richard Hamming Medal in 2008

1359
01:35:03,760 --> 01:35:05,620
and on a personal note

1360
01:35:05,650 --> 01:35:07,200
Sergio is from

1361
01:35:08,920 --> 01:35:12,680
and he can probably find the best coffee

1362
01:35:12,700 --> 01:35:14,670
in Princeton, if you ask him where to go

1363
01:35:14,670 --> 01:35:17,340
Sergio, thank you again

1364
01:35:17,400 --> 01:35:22,010
and take it away.

1365
01:35:22,060 --> 01:35:30,510
Well I'd like to find the best coffee in Vancouver, then.

1366
01:35:30,530 --> 01:35:32,650
It's probably not too difficult.

1367
01:35:32,670 --> 01:35:36,980
So it's a great pleasure to be here. Thank you for the invitation to address

1368
01:35:36,980 --> 01:35:41,140
this audience. I had heard a lot of good things about this conference but actually

1369
01:35:41,170 --> 01:35:46,810
never had the opportunity to be here, so it's a special pleasure to be

1370
01:35:46,810 --> 01:35:50,850
the first talk in the conference. So

1371
01:35:50,870 --> 01:35:54,840
what I'm going to do here. As you can tell from the title this is going

1372
01:35:54,840 --> 01:35:58,590
to be a very wide audience talk.

1373
01:35:58,650 --> 01:36:02,500
It's early in the morning, we're not going to get into mathematics, it's going to

1374
01:36:02,500 --> 01:36:05,030
be mainly an historical overview,

1375
01:36:05,050 --> 01:36:14,080
touching on some of the big problems that relative entropy has found itself

1376
01:36:14,090 --> 01:36:18,220
itself useful.

1377
01:36:18,470 --> 01:36:25,590
So here's the definition, to start with the definition of relative entropy.

1378
01:36:25,600 --> 01:36:32,590
You can tell immediately that, this of course P and Q are probability distributions, you can tell

1379
01:36:32,590 --> 01:36:35,840
immediately that if P is equal to Q

1380
01:36:35,840 --> 01:36:39,400
then this relative entropy

1381
01:36:39,400 --> 01:36:40,960
is equal to zero.

1382
01:36:40,960 --> 01:36:43,720
You cannot tell immediately that

1383
01:36:43,740 --> 01:36:45,150
if P

1384
01:36:45,160 --> 01:36:47,440
it is not equal to Q

1385
01:36:47,630 --> 01:36:52,650
that this relative entropy is actually going to be non-zero because

1386
01:36:52,660 --> 01:36:56,080
this is an expectation here

1387
01:36:58,150 --> 01:37:03,940
is the expectation of a quantity that can be either positive or negative.

1388
01:37:04,550 --> 01:37:05,550
OK, so

1389
01:37:07,810 --> 01:37:14,050
specially some mathematicians prefer to use an alternative representation where you take the expectation

1390
01:37:14,060 --> 01:37:16,000
with respect to

1391
01:37:17,050 --> 01:37:19,060
reference measure, the Q

1392
01:37:19,060 --> 01:37:20,580
rather than the P

1393
01:37:20,590 --> 01:37:24,460
and this actually turns out to be perhaps more natural in some...

1394
01:37:24,490 --> 01:37:26,870
...in some applications.

1395
01:37:29,750 --> 01:37:37,580
the most important cases of course the discrete case and the continuous case, when both

1396
01:37:37,600 --> 01:37:39,430
P and Q...I should say,

1397
01:37:39,460 --> 01:37:41,870
it's very important that P and Q

1398
01:37:41,880 --> 01:37:44,740
have to be distributions defined on the same

1399
01:37:45,440 --> 01:37:48,340
set, on the same measurable space.

1400
01:37:48,350 --> 01:37:55,960
So, if this space is finite or is countably infinite then we have this representation

1401
01:37:55,960 --> 01:37:56,560
we talk about

1402
01:37:56,960 --> 01:37:58,930
guassian processes for the next couple hours

1403
01:37:59,380 --> 01:38:02,370
so i wanna start by saying that gas processes are

1404
01:38:03,090 --> 01:38:04,530
a mathematical object with

1405
01:38:05,590 --> 01:38:07,020
with a great history and theory

1406
01:38:07,960 --> 01:38:12,780
and also has are or something that's been used to good effect in quite a number of applications

1407
01:38:13,650 --> 01:38:17,600
and we're not going to focus on either those two pieces specifically but rather talk

1408
01:38:17,600 --> 01:38:21,080
about the peace in the middle which is which is a

1409
01:38:21,100 --> 01:38:24,980
from from usability context how as machine learners can we use custom processes

1410
01:38:27,050 --> 01:38:31,050
so here's how we're gonna go about introducing gas processes i wanna start with talking

1411
01:38:31,050 --> 01:38:35,390
about gasses in general both in words and pictures this is just going to be

1412
01:38:36,480 --> 01:38:39,060
ceremony's introduction into this so we can think about

1413
01:38:39,970 --> 01:38:43,110
we can think about what a guassian process actually is from intuitive perspective

1414
01:38:43,570 --> 01:38:45,460
and then we'll go and build out somebody equations

1415
01:38:45,940 --> 01:38:50,220
we'll talk about using gas and processes in a basic regressions setting

1416
01:38:51,610 --> 01:38:54,270
there will be they'll get is about to first hour

1417
01:38:54,740 --> 01:38:58,570
and then we'll take will take a quick break and then we'll come back and we'll think about

1418
01:38:59,140 --> 01:39:03,730
moving beyond the basics of gas in processes so what kind of things can we change

1419
01:39:05,060 --> 01:39:09,270
well connected to some of the different technologies machine learning that we've seen and out

1420
01:39:09,690 --> 01:39:10,360
just about that

1421
01:39:12,390 --> 01:39:16,950
right so what is a guassian as far as machine learning is concerned this tions somebody

1422
01:39:17,960 --> 01:39:21,290
notions bayesian inference that you've seen in the last couple days

1423
01:39:22,990 --> 01:39:28,980
a gas in a gas in distribution is essentially a handy tool four bayesian inference on real valued variables

1424
01:39:30,540 --> 01:39:33,540
here's here's a here's the specific example i'm gonna talk about

1425
01:39:34,470 --> 01:39:37,340
throughout the course this i'm interested in measuring my heart rate

1426
01:39:37,980 --> 01:39:42,910
so how how might do this for modeling modeling perspective and i'm here and a

1427
01:39:42,910 --> 01:39:46,000
measure my heart rate at seven am and the fact that i do the indexed

1428
01:39:46,000 --> 01:39:48,340
by time is gonna be important and we're going to see that in a moment

1429
01:39:49,680 --> 01:39:50,220
okay so

1430
01:39:50,660 --> 01:39:52,470
i'm a reasonably healthy guys you might think

1431
01:39:52,970 --> 01:39:56,390
okay up three i have some belief about what my heart rate is gonna be

1432
01:39:56,390 --> 01:40:01,490
when i measured seventy seven animated somewhere between fifty and sixty beats so i put

1433
01:40:01,490 --> 01:40:02,180
some gas in

1434
01:40:02,580 --> 01:40:04,650
some guassian prior on and there's a density

1435
01:40:06,480 --> 01:40:10,970
knocking can go in on a particular morning and i can measure my heart rate measured at sixty one

1436
01:40:12,610 --> 01:40:16,150
i can go in a couple of days and i can measure the three three more times

1437
01:40:16,820 --> 01:40:18,030
so now i've got these fore

1438
01:40:18,690 --> 01:40:22,720
observations these four noisy observations and measurements four different days my heart rate

1439
01:40:23,480 --> 01:40:26,970
and what the gas allows you do what the notion bayesian inference allows you to

1440
01:40:27,480 --> 01:40:30,210
is i can then take my prior this great distribution

1441
01:40:30,670 --> 01:40:32,750
and those four draws from guassian

1442
01:40:34,530 --> 01:40:39,520
and i can do posterior inference i can call the exterior this up my underlying

1443
01:40:39,520 --> 01:40:43,190
heart rate given the noisy observations they seem you see that's again a guassian

1444
01:40:43,750 --> 01:40:47,330
i now have more confidence about where that is and i see that in fact

1445
01:40:47,980 --> 01:40:50,010
it's it's centered around say

1446
01:40:50,510 --> 01:40:51,470
sixty sixty two

1447
01:40:57,930 --> 01:40:58,770
let's take a at

1448
01:40:59,410 --> 01:41:04,140
univerity in case and move up to multi very guessing so we talked about

1449
01:41:04,790 --> 01:41:06,600
measure my heart rate at seven am

1450
01:41:08,040 --> 01:41:08,700
i could also

1451
01:41:09,170 --> 01:41:11,180
one measure my heart rate at eight am

1452
01:41:11,830 --> 01:41:12,970
and let's think about how

1453
01:41:13,960 --> 01:41:17,780
how those observations and not just a single real valued variable but rather a pair

1454
01:41:17,780 --> 01:41:19,490
of numbers at seven and eight am

1455
01:41:20,040 --> 01:41:24,130
i would change so to do we can use the same univariate gas and we

1456
01:41:24,130 --> 01:41:27,740
want move up to a multivariate gas there should be an objective hour of familiar

1457
01:41:29,870 --> 01:41:33,080
now the multi very guessing where you can actually look at has these

1458
01:41:34,670 --> 01:41:36,780
a flat surfaces ellipsoids f of

1459
01:41:37,310 --> 01:41:38,210
i so probability

1460
01:41:39,030 --> 01:41:43,580
and so what is this distribution telling us this distribution is telling us now got some prior belief

1461
01:41:44,130 --> 01:41:47,690
not on on a single heart rate measurement put on a pair of heart rate measurements

1462
01:41:48,150 --> 01:41:51,280
it shows that there are some positive correlation which is the same if i measure

1463
01:41:51,740 --> 01:41:55,770
if you've got a higher rate is seven am i imagine will be higher at eight am

1464
01:41:58,060 --> 01:42:00,550
and so that we can do the same thing so here's our prior

1465
01:42:01,130 --> 01:42:03,790
i can go in and take four measurements on four different days

1466
01:42:04,250 --> 01:42:07,530
number these measurements now apparent numbers seven and eight am

1467
01:42:08,100 --> 01:42:09,260
so i can get the data

1468
01:42:10,430 --> 01:42:11,930
i can use bayes rule in the same way

1469
01:42:12,520 --> 01:42:14,500
and come up with some posterior inference

1470
01:42:15,640 --> 01:42:17,840
and now i have i have a refined belief

1471
01:42:18,320 --> 01:42:20,840
of what what my heart rate is seven eight am

1472
01:42:20,840 --> 01:42:23,630
that is a closed loop current here

1473
01:42:23,650 --> 01:42:25,860
i want

1474
01:42:25,920 --> 01:42:30,760
and that is a closed loop current here i two

1475
01:42:30,800 --> 01:42:33,990
when i make them clockwise or counterclockwise

1476
01:42:34,050 --> 01:42:38,740
on important i could have chosen one clockwise the other counter-clockwise on import

1477
01:42:38,760 --> 01:42:45,170
however once i choose the direction has consequences as you will see

1478
01:42:45,170 --> 01:42:47,260
and that's all there running

1479
01:42:47,280 --> 01:42:49,090
one current like this

1480
01:42:49,150 --> 01:42:53,150
and one current independently like that

1481
01:42:53,150 --> 01:42:58,240
if i assume that and i have automatically

1482
01:42:58,240 --> 01:43:01,280
automatically i am well being the second

1483
01:43:01,340 --> 01:43:03,320
rule because current

1484
01:43:03,360 --> 01:43:04,920
that goes around

1485
01:43:04,920 --> 01:43:07,710
charge conservation right is no charge piling

1486
01:43:07,720 --> 01:43:08,860
so the second

1487
01:43:08,900 --> 01:43:11,670
the rule of kish of

1488
01:43:11,720 --> 01:43:14,780
is already obeid

1489
01:43:14,780 --> 01:43:17,320
so now i go to the first one

1490
01:43:17,380 --> 01:43:21,460
and i can start now at any point in that circuit

1491
01:43:21,480 --> 01:43:22,570
and go around

1492
01:43:22,590 --> 01:43:26,710
i can go around clockwise angle run counterclockwise it makes no difference as long as

1493
01:43:26,710 --> 01:43:29,090
i return to the same point

1494
01:43:29,090 --> 01:43:31,260
that integral utility on this is

1495
01:43:31,280 --> 01:43:35,090
i'm returning at the same potential

1496
01:43:35,110 --> 01:43:36,610
one is the in the

1497
01:43:36,630 --> 01:43:37,960
of e

1498
01:43:38,050 --> 01:43:40,420
don't pl

1499
01:43:40,440 --> 01:43:44,070
in going from point one to point two

1500
01:43:44,070 --> 01:43:50,740
well that's the potential difference between point one and point two

1501
01:43:50,740 --> 01:43:55,320
so let's start here

1502
01:43:55,360 --> 01:43:58,990
and let's go around

1503
01:43:59,050 --> 01:44:02,690
and we have to adopt to certain convention namely

1504
01:44:02,710 --> 01:44:06,740
if you go up in potential we go down in potential again you free to

1505
01:44:08,170 --> 01:44:09,990
the sign convention but i would say

1506
01:44:10,010 --> 01:44:12,980
when i go up potential i give the the plus sign

1507
01:44:12,990 --> 01:44:17,650
when i go down in potentially give that a minus sign

1508
01:44:17,690 --> 01:44:19,280
i start here

1509
01:44:19,300 --> 01:44:22,300
i could have started their career started there makes no difference as long as they

1510
01:44:22,300 --> 01:44:24,610
don't start here that makes no sense

1511
01:44:24,610 --> 01:44:26,550
so i start here

1512
01:44:26,570 --> 01:44:29,840
and i go around like this

1513
01:44:29,860 --> 01:44:32,360
so right here i go down potential

1514
01:44:32,360 --> 01:44:33,550
v one

1515
01:44:33,590 --> 01:44:35,710
so i get minus the one

1516
01:44:35,740 --> 01:44:41,110
now i go was current i one in the direction

1517
01:44:41,170 --> 01:44:43,110
from left to right

1518
01:44:43,110 --> 01:44:46,490
so that means that the potential must be higher than their

1519
01:44:46,530 --> 01:44:47,990
the because i are

1520
01:44:48,050 --> 01:44:51,790
potentially must be higher than there so i go down in potential so i get

1521
01:44:51,790 --> 01:44:54,530
mine is i one hour one

1522
01:44:54,650 --> 01:45:00,530
now those who are three

1523
01:45:00,610 --> 01:45:01,920
this current i one

1524
01:45:01,960 --> 01:45:03,260
is going down

1525
01:45:03,260 --> 01:45:06,840
this has a higher potential than here so i go down in potential

1526
01:45:06,840 --> 01:45:08,990
so i get minus

1527
01:45:09,010 --> 01:45:10,530
i one

1528
01:45:10,550 --> 01:45:12,710
and are free

1529
01:45:14,010 --> 01:45:16,210
i have independently

1530
01:45:16,300 --> 01:45:19,820
the current i two which is now coming towards me

1531
01:45:19,820 --> 01:45:21,110
when i go down

1532
01:45:21,170 --> 01:45:23,210
and so if it comes towards me

1533
01:45:23,220 --> 01:45:24,860
that current

1534
01:45:24,860 --> 01:45:28,280
but give me an increase in potential this would have to have a higher potential

1535
01:45:28,280 --> 01:45:29,420
than this

1536
01:45:29,440 --> 01:45:31,510
what is current to do this

1537
01:45:31,550 --> 01:45:34,050
so now i climb up the potential hill

1538
01:45:34,130 --> 01:45:36,440
so i get no claws

1539
01:45:36,440 --> 01:45:38,380
i two

1540
01:45:38,400 --> 01:45:40,220
times are three

1541
01:45:42,440 --> 01:45:44,990
o look what i did

1542
01:45:45,030 --> 01:45:46,400
i wrote down

1543
01:45:46,480 --> 01:45:47,860
i one or

1544
01:45:47,920 --> 01:45:50,610
there is no capital are in the whole problem

1545
01:45:50,630 --> 01:45:53,960
i clearly meant i one or one

1546
01:45:53,960 --> 01:45:54,990
so read

1547
01:45:55,010 --> 01:45:57,480
minus i one are one

1548
01:45:57,490 --> 01:46:00,280
sorry for that

1549
01:46:01,940 --> 01:46:06,050
i'm back where i was because these wires have no resistance

1550
01:46:06,050 --> 01:46:09,440
so i'm back where i am so this is zero

1551
01:46:09,460 --> 01:46:13,460
one equation with two unknowns i one i two

1552
01:46:14,110 --> 01:46:15,900
let's go around this one

1553
01:46:15,960 --> 01:46:20,010
we can go clockwise we can go counter-clockwise makes no difference

1554
01:46:20,110 --> 01:46:22,900
that start here

1555
01:46:22,900 --> 01:46:26,740
and i go in this direction ones around

1556
01:46:28,420 --> 01:46:30,260
i go through our three

1557
01:46:30,320 --> 01:46:31,820
and this current i two

1558
01:46:31,940 --> 01:46:33,490
running in this direction

1559
01:46:33,510 --> 01:46:35,960
so i go down in potential

1560
01:46:36,030 --> 01:46:38,170
so i get minus

1561
01:46:38,210 --> 01:46:38,990
i two

1562
01:46:39,070 --> 01:46:41,990
times are three

