1
00:00:00,000 --> 00:00:01,310
and as you have

2
00:00:01,360 --> 00:00:05,060
the special unitary groups because

3
00:00:05,100 --> 00:00:09,770
it's central to how we deal with angular momentum in quantum mechanics you might think

4
00:00:10,650 --> 00:00:13,650
you can do the same sort of thing for the group's it's true you can

5
00:00:13,650 --> 00:00:18,110
do it but it's much more difficult has been like last weekend to computing a

6
00:00:18,110 --> 00:00:21,790
couple of these serious for the case of the symmetric group it takes hours and

7
00:00:21,790 --> 00:00:28,860
hours even for relatively simple finite groups like the symmetric group and many for many

8
00:00:28,860 --> 00:00:33,860
groups this problem is in fact unsolved so even as the of first non trivial

9
00:00:33,860 --> 00:00:38,400
question representation theory and really get you get something which you know is the solution

10
00:00:38,400 --> 00:00:40,460
but it surprisingly difficult

11
00:00:40,480 --> 00:00:46,040
this is the kind of reasons the representation theory is occupied mathematicians

12
00:00:48,500 --> 00:00:49,880
the reason that

13
00:00:52,040 --> 00:00:58,250
i think are important representation theory it is important to the way that we handle

14
00:00:58,250 --> 00:01:02,060
symmetries in machine learning is because

15
00:01:02,080 --> 00:01:09,040
representation theory is what you need to generalized harmonic analysis into this noncommutative and harmonic

16
00:01:09,040 --> 00:01:13,860
analysis is one of those areas of the map which spans across basically the whole

17
00:01:13,860 --> 00:01:19,920
of the spectrum from across geometry and analysis and algebra it's really one of those

18
00:01:19,920 --> 00:01:21,730
unifying concepts

19
00:01:25,330 --> 00:01:29,690
so the generalization of the fourier transform

20
00:01:29,770 --> 00:01:31,150
two group

21
00:01:32,310 --> 00:01:33,860
really very simple

22
00:01:34,270 --> 00:01:37,040
if you think about the fourier transform

23
00:01:37,060 --> 00:01:42,810
on in the classical case is a on the real line

24
00:01:42,830 --> 00:01:47,080
well this just the weighted sum

25
00:01:47,250 --> 00:01:52,790
take a function that takes its fourier transform at a particular

26
00:01:52,810 --> 00:01:54,940
frequency k

27
00:01:54,960 --> 00:01:58,840
it's going to be something like the integral over the real line

28
00:01:58,860 --> 00:02:06,330
of the exponential factor i x times six

29
00:02:06,440 --> 00:02:09,770
to generalize this to groups you do something very similar

30
00:02:11,250 --> 00:02:13,210
start out with the function

31
00:02:14,810 --> 00:02:21,520
group so f is a function defined on the group general complex valued function

32
00:02:21,730 --> 00:02:27,440
some over the group so it's sort of an integral hybrid things in the discrete

33
00:02:27,440 --> 00:02:32,460
language for the sake of simplicity so you imagine you find group just some and

34
00:02:32,460 --> 00:02:37,630
then you wait by something but instead of the exponential factors you wait by the

35
00:02:37,630 --> 00:02:40,210
representation matrices

36
00:02:40,480 --> 00:02:42,750
that's not such a crazy thing to do

37
00:02:43,830 --> 00:02:47,250
representation matrices obey

38
00:02:47,310 --> 00:02:52,860
by definition this multiplicative identity in world x times wrote my because of x y

39
00:02:53,080 --> 00:02:54,290
that's exactly

40
00:02:54,310 --> 00:03:00,130
what these exponential factors the behaviour of a over the

41
00:03:00,150 --> 00:03:01,170
thank you

42
00:03:01,210 --> 00:03:06,460
so this is saying that fourier transformation is kind of this weighted sum by something

43
00:03:06,460 --> 00:03:10,250
which obeys the representation property

44
00:03:10,270 --> 00:03:12,380
in particular

45
00:03:12,380 --> 00:03:14,270
these exponential factors

46
00:03:14,290 --> 00:03:18,540
from the algebraic point of view justified in the case of the

47
00:03:18,690 --> 00:03:24,860
classical fourier transforms because they are themselves representations of the translation group

48
00:03:25,170 --> 00:03:29,360
just that they happen to be one dimensional representations of people don't normally think of

49
00:03:29,360 --> 00:03:32,690
them in terms of matrices but

50
00:03:32,730 --> 00:03:39,170
and in this case because we're in commutative group it turns out that all representations

51
00:03:39,170 --> 00:03:43,190
of one-dimensional and that's just what is taking them so this exponential thing is in

52
00:03:43,190 --> 00:03:48,500
fact a representation is just a special case of this more definition of fourier transformation

53
00:03:48,500 --> 00:03:50,040
on groups

54
00:03:51,100 --> 00:03:53,290
and much of the machinery

55
00:03:53,310 --> 00:03:57,250
from classical fourier analysis does carry through

56
00:03:57,270 --> 00:04:00,580
so for example you have an inverse transform

57
00:04:02,170 --> 00:04:04,040
given by the formula

58
00:04:04,340 --> 00:04:06,790
and all the classical

59
00:04:06,810 --> 00:04:09,420
properties that were used to

60
00:04:09,440 --> 00:04:11,650
so it is of course linearity

61
00:04:11,750 --> 00:04:15,130
you need to parity with respect to the appropriate norms

62
00:04:15,150 --> 00:04:21,270
and the translation and the convolution theorem fall out naturally

63
00:04:21,360 --> 00:04:25,020
but one thing that is different and this is important to bear in mind

64
00:04:25,040 --> 00:04:26,230
is that

65
00:04:26,250 --> 00:04:30,380
the symmetry between the space that you start out from in the space in which

66
00:04:30,380 --> 00:04:34,440
the fourier transform lives is broken

67
00:04:34,460 --> 00:04:39,730
this is something that is the direct result of non commutativity so if you look

68
00:04:39,730 --> 00:04:43,310
at the formula in the on the previous page

69
00:04:43,310 --> 00:04:46,900
i'm taking of OK we'll let alone

70
00:04:46,990 --> 00:04:49,920
now you have the full slide that so

71
00:04:50,230 --> 00:04:52,050
what i was saying is

72
00:04:52,210 --> 00:04:58,050
my take on how to actually make use of semantic web OWL two employed to

73
00:04:58,050 --> 00:05:02,550
exploit to combine all this data is by building applications

74
00:05:02,610 --> 00:05:03,800
is by beating

75
00:05:03,800 --> 00:05:06,110
not application but cool

76
00:05:06,110 --> 00:05:11,300
smart nice and semantic web based applications

77
00:05:11,310 --> 00:05:14,550
just not application that takes an RDF triple store

78
00:05:14,630 --> 00:05:19,230
but it the back and and do something with it but applications that goes on

79
00:05:19,240 --> 00:05:21,690
the web find his data everywhere

80
00:05:21,700 --> 00:05:24,870
put it together and there's something called

81
00:05:25,740 --> 00:05:27,290
what i want to show

82
00:05:27,300 --> 00:05:31,850
is very quickly i mean i want show and a big overview we need to

83
00:05:31,850 --> 00:05:37,160
us three days to tell everything you can do with the semantic web but just

84
00:05:37,160 --> 00:05:41,990
show very quick to develop cost-effective something about

85
00:05:42,040 --> 00:05:47,230
what if you want to be an application what we use what could you currently

86
00:05:47,970 --> 00:05:49,490
with the semantic web

87
00:05:52,290 --> 00:05:56,550
some of you may know this website called programmableweb

88
00:05:56,600 --> 00:05:58,570
that face to be talking about

89
00:05:58,590 --> 00:06:04,360
marsh API and the web as platform so basically

90
00:06:04,380 --> 00:06:06,130
what i think is emerging now

91
00:06:06,140 --> 00:06:12,290
is a programmable about semantic web something that talks about semantic mashups semantic API and

92
00:06:12,340 --> 00:06:16,850
the semantic web as platform something where you can just take bits and pieces of

93
00:06:16,850 --> 00:06:22,520
data bits and pieces of haiti is anywhere and an application in two or three

94
00:06:22,520 --> 00:06:28,510
days that do smart new things and potentially useful things and if you're lucky things

95
00:06:28,510 --> 00:06:32,660
that you get your money so i did was just this is

96
00:06:32,710 --> 00:06:37,470
the domain name and you want to do it for free but

97
00:06:40,210 --> 00:06:41,510
what they will

98
00:06:41,520 --> 00:06:44,120
quickly talk about and end his introduction

99
00:06:44,130 --> 00:06:46,010
is what

100
00:06:46,030 --> 00:06:50,100
there is currently what exists as web API

101
00:06:51,910 --> 00:06:53,210
two balls

102
00:06:53,230 --> 00:06:58,040
for application developers to integrate semantics in their web applications

103
00:06:58,050 --> 00:06:59,910
and i will also show

104
00:06:59,930 --> 00:07:05,580
examples of applications of the example of what can be done today quickly and easily

105
00:07:05,590 --> 00:07:07,360
and maybe

106
00:07:07,400 --> 00:07:12,130
we could discuss about what more can be done in the very near future

107
00:07:12,150 --> 00:07:15,580
so what

108
00:07:15,600 --> 00:07:16,930
i would recover

109
00:07:16,950 --> 00:07:18,050
it is

110
00:07:18,070 --> 00:07:24,120
web API is that they semantic applications semantic data but give you semantic data without

111
00:07:24,120 --> 00:07:29,540
you having to create it without having to collect it without you having to manage

112
00:07:29,540 --> 00:07:32,230
all star they give it to you for free

113
00:07:33,510 --> 00:07:36,850
there are no API that produce semantic information

114
00:07:36,860 --> 00:07:40,360
from input you give you given to like

115
00:07:40,470 --> 00:07:45,050
open calais semantic proxy and API

116
00:07:45,070 --> 00:07:50,580
the collect on the web so is the existing RDF information to the existing RDF

117
00:07:50,740 --> 00:07:54,800
data and allows you to exploit it to make use of

118
00:07:55,640 --> 00:07:58,900
there are many of them and we only talk about the one ball

119
00:07:58,960 --> 00:08:00,250
in particular

120
00:08:00,270 --> 00:08:01,320
so far

121
00:08:01,340 --> 00:08:06,460
this category is talk about sunday watson that are two semantic web search engines i

122
00:08:06,460 --> 00:08:08,000
put watson a bit bigger

123
00:08:08,010 --> 00:08:11,520
because i'm developing it so i think it's better

124
00:08:11,530 --> 00:08:12,890
and we take can be

125
00:08:12,920 --> 00:08:14,510
more time to talk about it

126
00:08:14,560 --> 00:08:19,970
what i will not talk about it and i agree with everybody would think that

127
00:08:19,970 --> 00:08:21,550
it is a pity

128
00:08:21,560 --> 00:08:24,160
it is also kind of API

129
00:08:24,210 --> 00:08:30,090
you could use to integrate semantics into applications semantic search API that i can i

130
00:08:30,790 --> 00:08:32,910
is that you can just simply

131
00:08:32,930 --> 00:08:38,320
using a web API without having in any need for infrastructure being you on semantic

132
00:08:38,320 --> 00:08:39,820
search engine

133
00:08:39,840 --> 00:08:41,040
which is quite cool

134
00:08:41,050 --> 00:08:48,820
so i won't talk about online platforms like the platform to store manipulate and process

135
00:08:49,670 --> 00:08:51,280
semantic information

136
00:08:51,290 --> 00:08:53,460
and i mean not talk about

137
00:08:53,480 --> 00:08:56,840
API is engines two

138
00:08:56,850 --> 00:09:01,530
act on the presentation and browsing of few RDF information

139
00:09:04,400 --> 00:09:08,140
but to make things simple and everything in we show

140
00:09:08,160 --> 00:09:09,060
it was see

141
00:09:10,220 --> 00:09:15,090
based on the very simple model which is the rest service kind of mother which

142
00:09:15,090 --> 00:09:18,270
means that everything is accessed two http

143
00:09:18,390 --> 00:09:21,590
all the API is used simple URL

144
00:09:21,600 --> 00:09:28,360
but our parametrizes you through the URL was http called and return XML JSON or

145
00:09:29,380 --> 00:09:33,430
two simply integrate in any language in any browser all you need

146
00:09:33,570 --> 00:09:37,090
application what they produce

147
00:09:37,900 --> 00:09:40,410
let's finally stop talking about

148
00:09:40,430 --> 00:09:44,920
the topic of the thing and the first category of things i want to talk

149
00:09:44,920 --> 00:09:45,690
about his

150
00:09:45,710 --> 00:09:47,960
so the API that produce

151
00:09:47,980 --> 00:09:54,770
semantic information there are a lot of them let's use name entity recognition natural language

152
00:09:54,770 --> 00:09:59,450
processing of latent semantic analysis

153
00:09:59,540 --> 00:10:03,810
where you can just give them the texts we can give them web documents i

154
00:10:03,810 --> 00:10:07,460
can give them that basis and what they give you for your

155
00:10:07,470 --> 00:10:08,660
it is

156
00:10:08,680 --> 00:10:12,800
five hundred categories thing your text is about that

157
00:10:12,850 --> 00:10:17,820
which is pretty simple but useful or it can recognise elements in your text

158
00:10:17,840 --> 00:10:23,320
of actually extract anything from your database the first one

159
00:10:23,340 --> 00:10:27,150
is the simplest probably is called textwise semantic hacker

160
00:10:27,170 --> 00:10:29,230
these are very very simple API

161
00:10:29,250 --> 00:10:33,600
you give it a web document text and it tells you

162
00:10:33,620 --> 00:10:35,360
to each category

163
00:10:35,380 --> 00:10:39,740
in which category of demos are demos you our it falls into

164
00:10:41,640 --> 00:10:43,700
if you get a very simple example

165
00:10:44,540 --> 00:10:45,750
i tried it with

166
00:10:46,000 --> 00:10:49,640
the web page is one of this tutorial

167
00:10:49,660 --> 00:10:54,080
so if you go there there you put that into your into your rules i

168
00:10:54,080 --> 00:10:55,840
want work see why that

169
00:10:55,960 --> 00:10:58,220
this is more the idea you get

170
00:10:58,270 --> 00:11:00,240
the basic rate of the API

171
00:11:00,410 --> 00:11:04,410
give it to talk and that is the number to give you an overview registered

172
00:11:04,410 --> 00:11:06,280
to use the API

173
00:11:06,300 --> 00:11:07,550
you give it

174
00:11:07,570 --> 00:11:11,550
the URL of the documents so here is the website of

175
00:11:12,010 --> 00:11:13,230
this tutorial

176
00:11:13,230 --> 00:11:18,720
so in the sort complex to code up but it works quite well

177
00:11:18,760 --> 00:11:22,100
so each stage you got a relatively simple a simple

178
00:11:22,110 --> 00:11:25,060
to the analytical solution to your problem

179
00:11:25,070 --> 00:11:31,150
very few numerical operations and i went the data choosing pairs as i go along

180
00:11:31,150 --> 00:11:33,330
for the next can best candidate pair

181
00:11:33,350 --> 00:11:35,760
always pushing my l

182
00:11:36,820 --> 00:11:44,720
my da sorry maximise my w so this will be able to handle any subs

183
00:11:44,740 --> 00:11:46,590
any classification regression

184
00:11:46,620 --> 00:11:49,220
all other non detection tasks

185
00:11:49,230 --> 00:11:53,020
well i've got a lot of the data so i would recommend smile that's what

186
00:11:53,020 --> 00:11:58,530
i really say about it is its stream chunking the two samples only one time

187
00:11:58,550 --> 00:12:01,250
if you get this problem of the data

188
00:12:01,260 --> 00:12:02,550
it's too large

189
00:12:02,920 --> 00:12:07,020
so it's a for the algorithms directly approach training from an optimisation create new ones

190
00:12:07,020 --> 00:12:11,490
could i think might jump over the

191
00:12:11,490 --> 00:12:20,580
right so no issue because i introduced the kernel earlier on like against incoming how

192
00:12:20,590 --> 00:12:22,740
to actually find the kernel parameter

193
00:12:23,430 --> 00:12:27,740
the sigma in the second of all the degree

194
00:12:27,760 --> 00:12:31,760
and the polynomial kernel and indeed the whole issue maybe

195
00:12:31,780 --> 00:12:36,100
what kernel do i use because i said these are different kernels but

196
00:12:36,150 --> 00:12:37,830
i present you some data

197
00:12:37,860 --> 00:12:41,560
i going to do with the linear kernel like the data is separable would just

198
00:12:41,560 --> 00:12:44,010
do a linear kernel but it was not separable

199
00:12:44,030 --> 00:12:47,830
should be using this gas kernel all modified RBF kernel

200
00:12:47,850 --> 00:12:50,880
or something else

201
00:12:50,900 --> 00:12:54,330
one point actually choose the kernel itself you may want to talk to know locally

202
00:12:54,340 --> 00:13:01,650
had a criterion kernel alignment for doing this issue was you can probably describe invented

203
00:13:01,650 --> 00:13:02,420
it but

204
00:13:02,540 --> 00:13:07,500
one issue i want to deal with this

205
00:13:07,500 --> 00:13:10,680
not the choice of kernel having having chosen the kernel

206
00:13:10,690 --> 00:13:13,520
what do you do about the kernel parameter

207
00:13:13,540 --> 00:13:20,100
if this parameter is poorly chosen then the hypothesis is over simple complex leading to

208
00:13:20,100 --> 00:13:23,880
poor generalisation just remind you that this is it actually

209
00:13:23,900 --> 00:13:30,230
this is the data set is actually a toy dataset it's a think symmetry problem

210
00:13:30,230 --> 00:13:34,200
was about that but i'm very much sigma

211
00:13:34,210 --> 00:13:36,410
and the bicycle is too small

212
00:13:36,430 --> 00:13:38,570
i get this poor performance here

213
00:13:38,710 --> 00:13:41,130
in the first was the other and they get

214
00:13:41,460 --> 00:13:43,260
poor performance as well

215
00:13:43,280 --> 00:13:44,760
and just very my

216
00:13:44,790 --> 00:13:51,040
sigma magasin kernel i guess some sort of minimum when my performance is optimized OK

217
00:13:51,040 --> 00:13:55,020
so it does make it difference what that sigma is

218
00:13:55,600 --> 00:14:00,570
actually if in real life you can make such problem i would be inclined forgot

219
00:14:00,570 --> 00:14:01,660
sufficient data

220
00:14:01,770 --> 00:14:02,590
just to do this

221
00:14:02,600 --> 00:14:04,300
cross validation study

222
00:14:04,320 --> 00:14:07,110
to find the best value sigma OK

223
00:14:07,130 --> 00:14:11,680
just keep some data site and event

224
00:14:22,750 --> 00:14:30,930
the question of the paper but

225
00:14:47,070 --> 00:14:53,440
i would say that if i got lots of data around and actually

226
00:14:53,440 --> 00:14:57,400
i don't student projects we've done cross validation versus the criteria is i'm just about

227
00:14:57,420 --> 00:14:58,960
to give you to find me

228
00:14:59,020 --> 00:15:00,230
kernel parameter

229
00:15:00,250 --> 00:15:05,430
and it's actually cross validation in in africa data which wins out but cross validation

230
00:15:05,470 --> 00:15:07,460
simply can't is wasteful of data

231
00:15:07,480 --> 00:15:11,350
and the context in which i can't sometimes do it when i did my will

232
00:15:11,490 --> 00:15:12,960
tumours optimise

233
00:15:12,970 --> 00:15:19,520
my lectures often had twenty nine patients OK i had to do training twenty eight

234
00:15:19,520 --> 00:15:21,960
test and what i just can't do

235
00:15:21,970 --> 00:15:24,350
and sort of

236
00:15:28,990 --> 00:15:36,480
which is why

237
00:15:40,120 --> 00:15:46,900
so i'm just going to say the foundation is is the choice but weights the

238
00:15:46,910 --> 00:15:51,410
data simply can't do in some context were going to just show

239
00:15:51,880 --> 00:15:55,540
the number of schemes have been proposed bayesian otherwise

240
00:15:55,570 --> 00:16:00,670
i wish to give you need to find the kernel parameter without recourse to validation

241
00:16:00,670 --> 00:16:04,650
data OK i'll illustrate one of these

242
00:16:04,660 --> 00:16:06,050
this scheme i give you

243
00:16:06,070 --> 00:16:10,050
it is due to toast and you can use and was in ICML several years

244
00:16:10,880 --> 00:16:14,750
and i suppose i had like that will tumour samples

245
00:16:14,760 --> 00:16:18,860
and perhaps i'm just trying to say something about the leave one out error

246
00:16:19,020 --> 00:16:21,280
that would be this beta here

247
00:16:21,300 --> 00:16:26,090
is an upper bound on the current now if we actually chose against income

248
00:16:26,110 --> 00:16:29,520
the largest value that can become i could have simply one

249
00:16:30,130 --> 00:16:36,070
when we speak my normal distribution OK that's greater than zero

250
00:16:36,130 --> 00:16:37,190
so actually

251
00:16:37,340 --> 00:16:40,070
choice garrison kernel give me a one here

252
00:16:40,090 --> 00:16:43,730
in here so choose against simply everyone here

253
00:16:43,750 --> 00:16:46,960
let me also do for hard margin case

254
00:16:47,050 --> 00:16:52,090
simply set aside variable to zero just to make it simple to the simplest possible

255
00:16:52,760 --> 00:16:54,530
so i guess in kernel

256
00:16:54,550 --> 00:16:55,860
and no

257
00:16:55,940 --> 00:16:57,190
soft margin

258
00:16:57,210 --> 00:17:00,570
what the hell is going to work

259
00:17:01,440 --> 00:17:06,750
let me increase by sigma value OK sourced start the small sigma i train my

260
00:17:07,960 --> 00:17:12,710
finally first i find many instances due to alpha

261
00:17:12,710 --> 00:17:18,590
greater than one this is represents the cardinality perhaps five fifty letters of the alphabet

262
00:17:18,590 --> 00:17:24,320
after hundred i get this criterion violated divided by the total number of patterns i

263
00:17:24,320 --> 00:17:29,260
have which is perhaps one hundred so perhaps is point five my christmas sigma

264
00:17:29,280 --> 00:17:32,250
re-evaluate my album phi nu alpha

265
00:17:32,250 --> 00:17:36,620
find what this number is the number of times throughout for greater than we could

266
00:17:36,620 --> 00:17:38,920
actually has an explicit form

267
00:17:38,930 --> 00:17:41,500
right this is the cost function that we're after

268
00:17:41,510 --> 00:17:44,830
and so if we sort of plug the pieces in

269
00:17:45,330 --> 00:17:47,880
what it says is that are

270
00:17:47,890 --> 00:17:53,340
cumulant generating our log normalisation constant is optimizing over that constraints set that i define

271
00:17:54,390 --> 00:17:56,610
you have a linear term

272
00:17:56,630 --> 00:17:59,890
you have some of entropy is that the nodes

273
00:17:59,900 --> 00:18:04,080
and then you have to subtract out mutual information is for edges

274
00:18:04,380 --> 00:18:08,930
so it's an explicit problem has distilling linear number of constraints it's

275
00:18:09,000 --> 00:18:13,490
something you could plug into any convex optimization solver

276
00:18:13,510 --> 00:18:19,170
what makes this the most interesting is the belief propagation actually comes out of this

277
00:18:20,470 --> 00:18:24,110
and belief propagation comes out of it by

278
00:18:24,120 --> 00:18:27,010
doing a lagrangian formulation of it

279
00:18:27,070 --> 00:18:31,680
so the high-level message here is the belief propagation is just an iterative algorithm

280
00:18:31,760 --> 00:18:35,540
for solving this optimisation problem that's all it's doing

281
00:18:38,230 --> 00:18:39,810
quite surprising i think

282
00:18:39,820 --> 00:18:43,230
because it's not at all clear when you first see belief propagation

283
00:18:43,250 --> 00:18:47,470
why should have anything to do with optimisation you can

284
00:18:47,480 --> 00:18:51,870
sort specified without any cost function no constraint sets

285
00:18:51,870 --> 00:18:55,570
you know it just looks like a local iterative basically dynamic programming

286
00:18:55,580 --> 00:19:00,440
but this is basically that the messages are lagrange multipliers and they are lagrange multipliers

287
00:19:00,440 --> 00:19:02,610
for this problem

288
00:19:02,630 --> 00:19:07,500
so let's let's work through that i won't go into all the details but also

289
00:19:07,510 --> 00:19:08,930
to give you the sketch

290
00:19:08,960 --> 00:19:13,700
and if you're interested it's not too hard to fill in the details here

291
00:19:13,760 --> 00:19:18,110
so what you do is remember this constraint set

292
00:19:18,110 --> 00:19:24,170
the constraints set consists of some nonnegativity constraints let's not worry too much about those

293
00:19:24,190 --> 00:19:26,520
normalisation constraints

294
00:19:26,530 --> 00:19:30,040
and you have marginalisation constraints

295
00:19:30,080 --> 00:19:35,220
so what's the matter to us is it's really the marginalisation constraints that are going

296
00:19:35,220 --> 00:19:41,210
to matter those are going to correspond to the messages going along edges

297
00:19:41,330 --> 00:19:44,980
i guess i don't have time to go into the full theory lagrang multiplier statistically

298
00:19:44,980 --> 00:19:46,740
the high-level intuition

299
00:19:47,060 --> 00:19:49,480
the way to think about them is

300
00:19:49,530 --> 00:19:52,260
but the most police officers

301
00:19:52,310 --> 00:19:57,420
so what you end up doing new form the lagrangian which has your cost function

302
00:19:57,420 --> 00:19:59,560
that's the part in green here

303
00:19:59,570 --> 00:20:01,130
it's right there

304
00:20:01,140 --> 00:20:05,790
and then you take your constraints the ones i'm interested in all these guys

305
00:20:05,810 --> 00:20:08,910
and you sort of assign a police officer each one of them

306
00:20:08,920 --> 00:20:13,860
so the police officers were measuring your speed it seemed to satisfy the constraint or

307
00:20:13,860 --> 00:20:17,960
not and if you violate the police officers gonna is going to find new is

308
00:20:17,960 --> 00:20:19,850
going to penalize you

309
00:20:19,880 --> 00:20:24,310
so what goes on in the lagrangian algorithm is that you sort of end up

310
00:20:24,310 --> 00:20:27,430
adjusting the police officers to a point where

311
00:20:27,450 --> 00:20:30,170
you are all forced to obey the constraints

312
00:20:30,190 --> 00:20:34,200
see sort of think about the policeman continues to find new and fine you find

313
00:20:34,200 --> 00:20:37,880
you up until the point that you first start to say well i'm not going

314
00:20:37,880 --> 00:20:41,310
to speedy anymore i will be your constraint

315
00:20:41,340 --> 00:20:43,930
so that's what's going on you've got the cost function

316
00:20:43,940 --> 00:20:46,060
plus lagrang multiplier

317
00:20:46,060 --> 00:20:47,910
time constraints

318
00:20:47,960 --> 00:20:51,620
and what the theory tells you to do is is actually just to complete treat

319
00:20:51,620 --> 00:20:54,460
this is an unconstrained problem in you this say

320
00:20:54,480 --> 00:20:57,810
you take the gradient of this and you set to zero to try and find

321
00:20:57,810 --> 00:20:59,830
stationary point

322
00:20:59,870 --> 00:21:02,620
so let's let's go and do that

323
00:21:02,760 --> 00:21:06,390
if we take derivatives this is just a bit of algebra

324
00:21:06,430 --> 00:21:10,850
what you find is these equations pop that you have two equations two sets of

325
00:21:10,850 --> 00:21:13,500
equations one for the singletons

326
00:21:13,520 --> 00:21:14,760
and one for the

327
00:21:15,510 --> 00:21:17,380
joint marginals

328
00:21:17,430 --> 00:21:21,880
and what the theory says she set these to zero

329
00:21:21,880 --> 00:21:26,510
and if i do that then i get this kind of form here

330
00:21:26,530 --> 00:21:30,430
so what this is doing is it's it's telling me what the solution this is

331
00:21:30,430 --> 00:21:32,510
what i'm optimizing over

332
00:21:32,520 --> 00:21:35,490
it says there should be a function of the problem data

333
00:21:35,500 --> 00:21:38,630
these are the potentials on your graphical model

334
00:21:38,630 --> 00:21:43,020
and these guys right here are the graves multipliers

335
00:21:43,040 --> 00:21:47,310
and so what it says is the optimal solution should look like this this is

336
00:21:47,310 --> 00:21:50,160
the form that should take

337
00:21:50,170 --> 00:21:54,610
now what's magical here is that if you remember we have a constraint we have

338
00:21:54,610 --> 00:21:57,610
a constraint that if you some over this guy

339
00:21:57,650 --> 00:22:00,470
that should marginalise down to this

340
00:22:00,490 --> 00:22:03,710
right that was our constraint our polytope

341
00:22:03,730 --> 00:22:07,930
that's just to recap that this constraint i have right here something over that should

342
00:22:07,930 --> 00:22:12,310
give me back that so if you know of course that constraint what it does

343
00:22:12,310 --> 00:22:17,500
is it gives a set of equations for the multipliers or equivalently you can define

344
00:22:17,500 --> 00:22:22,330
messages these are exactly the same messages we had before and

345
00:22:22,360 --> 00:22:25,480
what you get is this update rule you should somehow

346
00:22:25,500 --> 00:22:28,700
and you have a product of incoming messages

347
00:22:28,710 --> 00:22:34,110
so that's exactly the sum product algorithm so it says that the sum product algorithm

348
00:22:34,110 --> 00:22:40,420
is just the lagrangian method for solving the simple optimisation problem tree

349
00:22:40,420 --> 00:22:45,870
if you're going to see the goals and some more

350
00:22:47,260 --> 00:22:48,840
it's possible to

351
00:22:48,860 --> 00:22:53,910
have an algorithm to decide whether annexed the formula is an axiom of school recursively

352
00:22:53,910 --> 00:22:58,150
enumerable set of axioms you can recognise whether something is XML

353
00:22:58,200 --> 00:23:00,840
because he continued in the water straight away

354
00:23:00,840 --> 00:23:04,170
so if it's got recursively enumerable set of axioms

355
00:23:05,260 --> 00:23:07,220
it's consistent

356
00:23:07,230 --> 00:23:09,580
so the theory set of axioms

357
00:23:09,620 --> 00:23:12,160
this basic arithmetic inside it

358
00:23:12,170 --> 00:23:17,270
you can tell whether the formulas because it is actually an is an axiom

359
00:23:17,290 --> 00:23:20,000
and it's not inconsistent

360
00:23:21,280 --> 00:23:23,240
there's is the a formula

361
00:23:23,280 --> 00:23:24,140
it can't be

362
00:23:24,220 --> 00:23:31,860
decided know which you can prove it compromise the form the negation

363
00:23:31,930 --> 00:23:32,980
and this

364
00:23:33,040 --> 00:23:35,330
completely destroyed program

365
00:23:35,400 --> 00:23:37,690
because the program

366
00:23:37,750 --> 00:23:42,180
research programme was to have mathematics put on a purely computational basis there will be

367
00:23:42,180 --> 00:23:46,330
an algorithm to decide the truth and falsehood you want to to know

368
00:23:46,380 --> 00:23:49,350
and girls result shows

369
00:23:49,400 --> 00:23:51,330
this is not possible

370
00:23:51,940 --> 00:23:55,430
such series includes the they arithmetic

371
00:23:55,480 --> 00:23:59,880
and this is consistent then it's recursively enumerable set of axioms is completely natural thing

372
00:23:59,890 --> 00:24:01,510
that would be what you would work with

373
00:24:01,560 --> 00:24:02,520
so his

374
00:24:02,590 --> 00:24:07,650
potential theory in mathematics and then this form is you can prove

375
00:24:07,730 --> 00:24:09,330
so it destroyed building

376
00:24:10,160 --> 00:24:13,210
the second problem died with the answer now

377
00:24:13,230 --> 00:24:16,840
so this is a very famous there OK so went back to the point

378
00:24:16,840 --> 00:24:20,710
lucas argument is this destroys i i

379
00:24:24,930 --> 00:24:26,420
and i i system

380
00:24:26,440 --> 00:24:30,030
it is the formal system in the subject to go completely

381
00:24:30,040 --> 00:24:33,370
there will be things that could improve

382
00:24:33,380 --> 00:24:34,750
that was essentially

383
00:24:34,810 --> 00:24:36,710
his his i

384
00:24:36,760 --> 00:24:40,440
so as i say the time it created enormous

385
00:24:40,460 --> 00:24:42,810
now the control

386
00:24:42,850 --> 00:24:46,040
i don't particularly i mean become i could spend the whole summer

387
00:24:46,070 --> 00:25:03,530
well the implication was that humans went from systems

388
00:25:03,580 --> 00:25:09,430
what humans do have inconsistencies

389
00:25:09,430 --> 00:25:13,380
but these arguments against asians

390
00:25:13,380 --> 00:25:15,030
well exactly was

391
00:25:15,090 --> 00:25:19,680
that the agent was the former system and therefore subject to girls incompleteness there

392
00:25:19,740 --> 00:25:22,370
but humans but humans were not subject to this

393
00:25:23,490 --> 00:25:28,320
but the germans were not subject to this limitation

394
00:25:28,320 --> 00:25:33,430
because we much money i don't so not defending the coast

395
00:25:33,440 --> 00:25:38,100
well well roughly speaking that humans had some other mechanism that they didn't have this

396
00:25:38,100 --> 00:25:40,090
limitation we get

397
00:25:43,030 --> 00:25:48,010
so i'm not going to particularly defended because as i say

398
00:25:48,210 --> 00:25:52,230
i guess since nineteen sixty girls incompleteness in mozambique

399
00:25:52,280 --> 00:25:54,670
what i this thirty is also

400
00:25:55,100 --> 00:25:57,600
was took part of being used to create

401
00:25:57,650 --> 00:26:02,750
OK so basically there's this mathematical result from and used against us

402
00:26:05,480 --> 00:26:07,130
another philosopher time

403
00:26:07,140 --> 00:26:10,070
series of books and there along the lines of

404
00:26:10,510 --> 00:26:13,360
computers can do this

405
00:26:13,400 --> 00:26:15,420
and presuming that all the

406
00:26:15,430 --> 00:26:18,520
and then the light and other ones what computers still can't do so this is

407
00:26:18,520 --> 00:26:20,360
the god of the gaps argument

408
00:26:20,390 --> 00:26:22,350
if you know the god of the gaps argument

409
00:26:22,400 --> 00:26:23,580
OK so this is

410
00:26:23,670 --> 00:26:26,600
evolution something you don't understand regarded

411
00:26:26,650 --> 00:26:28,140
so is analogous to that

412
00:26:28,150 --> 00:26:28,870
so the

413
00:26:28,880 --> 00:26:30,600
whatever we do

414
00:26:30,860 --> 00:26:33,670
that's fine but you cannot do this therefore cos

415
00:26:33,690 --> 00:26:35,350
one of them to do that

416
00:26:35,730 --> 00:26:38,730
it isn't really very convincing i mean you got

417
00:26:38,840 --> 00:26:42,610
one of the proof that so finding gaps in what i can do

418
00:26:42,620 --> 00:26:45,760
doesn't actually prove anything this means that they get to be solved

419
00:26:45,780 --> 00:26:47,770
so i don't have much time for one either

420
00:26:49,160 --> 00:26:52,860
his argument was basically the

421
00:26:52,930 --> 00:26:55,120
this two books on this

422
00:26:55,170 --> 00:26:58,170
and i very famous physicist

423
00:27:00,670 --> 00:27:02,250
the emperor's new mind

424
00:27:02,260 --> 00:27:04,760
i was actually amazing

425
00:27:04,770 --> 00:27:08,450
there was one reference paper and i

426
00:27:08,460 --> 00:27:12,840
the entire reference this one paper guess what what

427
00:27:12,850 --> 00:27:16,580
wasn't burns eliza

428
00:27:16,590 --> 00:27:20,990
that was really funny and fill in the hidden field a study what i was

429
00:27:21,010 --> 00:27:22,610
going to prove it was possible

430
00:27:22,660 --> 00:27:26,780
so roughly speaking the argument is that that and this is very common

431
00:27:26,900 --> 00:27:31,040
the that items are true machines into machines kind of intelligence

432
00:27:31,060 --> 00:27:32,730
and as well as that

433
00:27:32,750 --> 00:27:33,680
i mean is

434
00:27:33,690 --> 00:27:35,400
he's done some very strong working

435
00:27:35,410 --> 00:27:36,500
in cosmology

436
00:27:36,520 --> 00:27:37,950
reputation and so on

437
00:27:37,990 --> 00:27:39,820
as well as that there was

438
00:27:40,040 --> 00:27:44,310
i guess stated belief that there was some quantum effect was going on inside the

439
00:27:45,430 --> 00:27:47,370
was somewhere inside our bodies

440
00:27:47,410 --> 00:27:52,370
in this quantum effect was explaining the fact that the intelligence making intelligence possible

441
00:27:52,520 --> 00:27:53,770
two machine

442
00:27:53,830 --> 00:27:56,580
and then do that

443
00:27:56,640 --> 00:27:58,980
and sells chinese room

444
00:27:59,020 --> 00:28:01,100
as a kind of thought experiment

445
00:28:01,120 --> 00:28:03,250
this is pretty famous

446
00:28:03,250 --> 00:28:05,940
what is well the philosopher and skeptic

447
00:28:08,890 --> 00:28:11,290
with a human side

448
00:28:11,330 --> 00:28:13,330
and two slits

449
00:28:13,390 --> 00:28:15,580
web pages coming papers go

450
00:28:15,660 --> 00:28:17,750
and some work

451
00:28:17,790 --> 00:28:19,600
pencils worksheets

452
00:28:19,620 --> 00:28:21,410
and the book of instructions

453
00:28:21,460 --> 00:28:22,620
and human

454
00:28:22,710 --> 00:28:25,460
you can read english

455
00:28:25,460 --> 00:28:28,370
the instruction books written in english

456
00:28:28,430 --> 00:28:31,170
and you imagine a bit of paper coming in through

457
00:28:31,190 --> 00:28:33,120
this led the slide here

458
00:28:33,160 --> 00:28:35,870
with an indecipherable symbols

459
00:28:35,930 --> 00:28:39,600
for human takes the pipe with indecipherable symbols

460
00:28:39,730 --> 00:28:41,540
that's really instruction book

461
00:28:41,580 --> 00:28:44,390
and it follows all the instructions

462
00:28:44,440 --> 00:28:50,350
and produces the paper to add more indecipherable symbols input from others slightly side and

463
00:28:50,350 --> 00:28:53,250
disappears and in this process is repeated

464
00:28:53,290 --> 00:28:55,930
human system simply saying that

465
00:28:55,940 --> 00:28:58,540
following instructions in this book

466
00:28:58,540 --> 00:29:01,790
taking input producing outputs

467
00:29:01,810 --> 00:29:03,520
OK on the outside

468
00:29:03,560 --> 00:29:05,850
there's a bunch of chinese

469
00:29:05,890 --> 00:29:08,000
and they actually have conversation

470
00:29:08,040 --> 00:29:09,480
with this machine

471
00:29:09,500 --> 00:29:11,830
because they're putting in questions in chinese

472
00:29:11,850 --> 00:29:15,850
and sensible answers in chinese coming out the other end

473
00:29:15,870 --> 00:29:18,890
so the chinese on the outside

474
00:29:18,890 --> 00:29:22,560
having a conversation telephone conversation

475
00:29:23,170 --> 00:29:24,560
this change

476
00:29:24,560 --> 00:29:28,390
on the inside him and just looking at the rule book

477
00:29:28,390 --> 00:29:30,500
OK so the moral of this

478
00:29:31,730 --> 00:29:35,100
experiment is the following that

479
00:29:35,190 --> 00:29:37,850
it nineteen look from the outside

480
00:29:37,960 --> 00:29:40,640
as if machines intelligent

481
00:29:40,690 --> 00:29:43,710
clearly in this case it could possibly be intelligent

482
00:29:43,750 --> 00:29:45,250
we look on the inside

483
00:29:45,290 --> 00:29:47,350
the human doesn't understand china

484
00:29:47,810 --> 00:29:49,620
nothing else in in

485
00:29:49,660 --> 00:29:54,230
and inside the understands chinese and yet there is the appearance of this kind country

486
00:29:54,230 --> 00:29:57,540
sensible conversation taking place

487
00:29:57,540 --> 00:30:00,060
so that was cells

488
00:30:00,100 --> 00:30:02,080
chinese room and provides

489
00:30:02,140 --> 00:30:07,270
a lot of controversy so people now argue back-and-forth about whether this reasonable

490
00:30:07,270 --> 00:30:11,040
learning and it's great part of the

491
00:30:11,080 --> 00:30:13,140
of the algorithm we

492
00:30:13,140 --> 00:30:14,480
plain fact

493
00:30:14,480 --> 00:30:15,480
with this

494
00:30:15,480 --> 00:30:18,310
the formulation of this optimisation problem

495
00:30:18,370 --> 00:30:22,020
because you can you can

496
00:30:22,020 --> 00:30:24,710
the main objective

497
00:30:24,730 --> 00:30:26,040
function here

498
00:30:26,190 --> 00:30:29,830
with the constraints is that you can exchange is you know that's not a musician

499
00:30:29,830 --> 00:30:35,980
we can play with very different kind of of optimisation formulation and you

500
00:30:36,000 --> 00:30:40,690
after keep in mind that we have to maximize the journey to magic

501
00:30:40,730 --> 00:30:41,850
and we have

502
00:30:41,870 --> 00:30:42,850
to keep

503
00:30:42,850 --> 00:30:44,250
this margin

504
00:30:44,270 --> 00:30:45,960
so high

505
00:30:46,020 --> 00:30:48,460
because let's focus

506
00:30:48,480 --> 00:30:53,480
so in fact what we do not usually do not transition problems

507
00:30:53,730 --> 00:30:58,120
we are right support them as a minimisation problem

508
00:30:58,370 --> 00:31:04,500
and what we want to do we want to maximize something which is one

509
00:31:04,520 --> 00:31:07,600
but one over w w

510
00:31:07,650 --> 00:31:13,420
and we are going to write seats without changing as sense of meaning of the

511
00:31:13,420 --> 00:31:17,410
optimisation is minimized

512
00:31:17,420 --> 00:31:18,830
is normal

513
00:31:18,850 --> 00:31:20,520
w square

514
00:31:20,560 --> 00:31:27,890
w and as the following constraint

515
00:31:27,940 --> 00:31:34,690
OK so there is a small mistake here because i just keep some days so

516
00:31:36,020 --> 00:31:37,770
so we have no more

517
00:31:39,250 --> 00:31:43,150
and the sorry i've just forgotten to

518
00:31:43,250 --> 00:31:47,520
so we have some function so when you want to optimize some problem

519
00:31:47,600 --> 00:31:51,870
you're going to use language and formulation OK so if this is

520
00:31:52,790 --> 00:31:56,120
what you're going to do is going to take your objective function

521
00:31:56,140 --> 00:31:58,230
and you know you're going to the

522
00:31:58,250 --> 00:32:00,290
you're all constraints

523
00:32:00,290 --> 00:32:02,710
by adding them

524
00:32:02,770 --> 00:32:05,920
we some some here and some

525
00:32:05,960 --> 00:32:12,390
that evolution coefficient OK sorry for science

526
00:32:12,390 --> 00:32:16,500
if you look at this we have this objective function and we have these constraints

527
00:32:17,460 --> 00:32:18,620
what is should

528
00:32:18,650 --> 00:32:24,960
usually the do doing optimisation you take these constraints you and then in the fall

529
00:32:25,100 --> 00:32:26,210
falling from

530
00:32:26,230 --> 00:32:30,040
he you take this one you put minus one

531
00:32:30,060 --> 00:32:30,910
and you

532
00:32:30,920 --> 00:32:32,710
you are going to work

533
00:32:32,770 --> 00:32:34,230
to work

534
00:32:34,250 --> 00:32:36,350
with the new formulation

535
00:32:36,370 --> 00:32:38,960
this formulation is with w

536
00:32:39,620 --> 00:32:45,480
so far as far as i wasn't coefficients so the exciting

537
00:32:47,420 --> 00:32:48,850
if you

538
00:32:48,850 --> 00:32:54,370
the goal is a legend coefficient is an interesting fact is to formulate the dual

539
00:32:55,330 --> 00:32:58,230
and solves the optimisation problem

540
00:32:58,250 --> 00:33:05,750
not guaranteed to permit w that according to permit

541
00:33:05,770 --> 00:33:09,560
and then of course you will be able to come back to the formulation of

542
00:33:09,770 --> 00:33:11,830
parliament w and

543
00:33:11,980 --> 00:33:14,500
so in fact

544
00:33:14,560 --> 00:33:15,770
this quantity

545
00:33:15,770 --> 00:33:18,620
as to be minimized with respect to

546
00:33:18,640 --> 00:33:19,850
w and b

547
00:33:19,870 --> 00:33:23,890
and maximize with respect to the lagrangian coefficient

548
00:33:23,980 --> 00:33:28,060
so what you can do if you want to just two

549
00:33:28,100 --> 00:33:31,190
to get rid of the inquisition

550
00:33:31,210 --> 00:33:32,520
w and b

551
00:33:32,540 --> 00:33:35,770
you can start differentiating

552
00:33:35,790 --> 00:33:37,100
this quantity

553
00:33:38,850 --> 00:33:44,210
put in its equals to zero so often you'll get ready

554
00:33:44,480 --> 00:33:49,710
and writing this equation is well by writing equation you will have

555
00:33:51,270 --> 00:33:55,250
links between w one and five

556
00:33:55,250 --> 00:33:57,520
and you can rewrite

557
00:33:57,540 --> 00:33:59,150
for this formulation

558
00:33:59,170 --> 00:34:02,850
on reason language and coefficient OK

559
00:34:02,890 --> 00:34:03,770
so here

560
00:34:03,890 --> 00:34:06,890
we have i

561
00:34:07,670 --> 00:34:14,270
you have also some constraints and these constraints come from this differentiation of the gradient

562
00:34:14,500 --> 00:34:17,420
and fine sense for the user value b

563
00:34:17,440 --> 00:34:23,500
you can you can use the country condition also to get the value of

564
00:34:23,520 --> 00:34:25,290
but just

565
00:34:25,310 --> 00:34:26,890
look now it's problem

566
00:34:26,890 --> 00:34:29,170
so maximisation problem

567
00:34:29,190 --> 00:34:31,000
what you are here

568
00:34:31,020 --> 00:34:35,080
you have your training data that appear on the

569
00:34:35,440 --> 00:34:37,440
as a dot product

570
00:34:38,040 --> 00:34:40,350
so this is a good find

571
00:34:40,350 --> 00:34:42,140
in this problem of

572
00:34:42,140 --> 00:34:44,170
map genetic matching

573
00:34:44,170 --> 00:34:46,270
musicians will have

574
00:34:46,310 --> 00:34:50,790
the formulation of the optimisation problems on in terms of

575
00:34:50,830 --> 00:34:52,330
scalar products

576
00:34:52,350 --> 00:34:53,350
this means that

577
00:34:53,460 --> 00:35:03,000
when we use we want to use can edit it would be possible

578
00:35:03,020 --> 00:35:10,190
so in fact if we use fuzzy equations set gives you the form of w

579
00:35:10,210 --> 00:35:14,080
you will you we get the following

580
00:35:15,330 --> 00:35:18,250
in fact the solution of this

581
00:35:19,540 --> 00:35:21,690
the problem of optimisation problem of

582
00:35:21,690 --> 00:35:26,540
musician credited mission is the function differing function

583
00:35:26,560 --> 00:35:28,250
you will take this time

584
00:35:28,250 --> 00:35:30,720
take this to be on

585
00:35:30,740 --> 00:35:33,560
these conditions

586
00:35:33,800 --> 00:35:41,080
so we get a lot of you do this in an awful areas

587
00:35:41,100 --> 00:35:42,800
where were people

588
00:35:46,580 --> 00:35:51,820
you know it's difficult to say hey

589
00:35:51,840 --> 00:35:53,910
the cause

590
00:35:53,920 --> 00:35:56,300
what we talk about what will be out at the moment

591
00:35:56,330 --> 00:35:58,050
because see that good services new

592
00:35:58,050 --> 00:36:00,760
it's actually only working with one where every works within

593
00:36:00,770 --> 00:36:04,780
but that will you know about the model support as it gets bigger

594
00:36:04,860 --> 00:36:08,330
splitting down into these groups within your

595
00:36:08,340 --> 00:36:12,820
you have the biomedical community and high energy physics two big ones it's difficult to

596
00:36:12,820 --> 00:36:16,670
judge numbers because the physics community are working as individuals

597
00:36:16,760 --> 00:36:19,760
was about making will and have a portal

598
00:36:19,800 --> 00:36:22,520
for example the support it being set up in glasgow

599
00:36:22,650 --> 00:36:25,100
is common and

600
00:36:27,980 --> 00:36:29,810
project in glasgow

601
00:36:29,850 --> 00:36:33,580
has because they users don't want to have to handle certificates

602
00:36:33,610 --> 00:36:34,980
i have

603
00:36:34,980 --> 00:36:42,580
one certificate shared by different authentication that was negotiated negotiated so we can only see

604
00:36:42,580 --> 00:36:44,530
one user working

605
00:36:44,570 --> 00:36:49,010
yes it's very difficult to know was certainly i nothing on an individual basis will

606
00:36:49,010 --> 00:36:51,630
come go physics is the dominant community

607
00:36:52,560 --> 00:36:57,250
more communities like yourselves you the fact of the talking here recognizing

608
00:36:57,260 --> 00:36:59,860
that could provide a great resource for

609
00:36:59,960 --> 00:37:03,530
the money is going into communities are waking up to it physics has led it

610
00:37:03,530 --> 00:37:08,380
but in physics as i say is no longer a science wing to the research

611
00:37:08,400 --> 00:37:10,000
does that sort of answer

612
00:37:10,000 --> 00:37:11,830
it's difficult to actually

613
00:37:11,850 --> 00:37:14,280
put numbers on i can give you more

614
00:37:16,850 --> 00:37:20,230
be on

615
00:37:20,240 --> 00:37:26,540
that i have and then

616
00:37:26,590 --> 00:37:28,500
well the NGS is

617
00:37:28,500 --> 00:37:30,650
for jobs i mean obviously the

618
00:37:30,710 --> 00:37:35,020
chemistry people are very keen on it for their protein folding stuff and

619
00:37:35,060 --> 00:37:39,880
at the moment there are jobs show running on the national grid service works for

620
00:37:39,890 --> 00:37:44,320
other people and sixty four hundred twenty eight no jobs and they could easily be

621
00:37:44,380 --> 00:37:48,880
i wouldn't be surprised in fact if you just was more orientated that way anyway

622
00:37:48,890 --> 00:37:52,000
because of people who because in injustice news

623
00:37:52,030 --> 00:37:56,480
a lot of physics communities working cells in the old stuff as well

624
00:37:56,560 --> 00:37:58,570
and access to resources

625
00:38:12,080 --> 00:38:17,510
one of the the you can measure who's doing what

626
00:38:17,510 --> 00:38:19,770
there is a monitoring occupancies within

627
00:38:19,780 --> 00:38:20,880
e g

628
00:38:20,900 --> 00:38:24,920
when the NGS and the moment they have taken much more simplistic viewpoint

629
00:38:24,930 --> 00:38:26,680
they've said that

630
00:38:26,690 --> 00:38:28,740
we don't have to worry yet

631
00:38:29,570 --> 00:38:34,580
they have imposed the same tools so that they can start putting constraints on

632
00:38:34,600 --> 00:38:37,130
if it becomes necessary in the future

633
00:38:37,150 --> 00:38:38,400
but at the moment

634
00:38:38,410 --> 00:38:40,250
it to provide the source

635
00:38:40,570 --> 00:38:43,430
it uses more more the queueing system so

636
00:38:43,490 --> 00:38:46,040
it's first-come first-served

637
00:38:46,090 --> 00:38:50,250
and though it does a few good job is not to run for short time

638
00:38:50,290 --> 00:38:53,290
it will run much you'll get to the topic you quicker

639
00:38:53,340 --> 00:38:55,650
and within each is going to allow for

640
00:38:55,690 --> 00:38:58,380
they have different cues we access

641
00:38:58,390 --> 00:39:01,530
we may have different sources in different part in behind them

642
00:39:01,540 --> 00:39:04,870
so you say well my job will take up to two hours

643
00:39:06,300 --> 00:39:10,380
if it's over two hours it will be automatically killed goes inactive

644
00:39:10,410 --> 00:39:15,540
was is twelve our human is a infinite q and that's how they resolve

645
00:39:15,540 --> 00:39:20,810
that's so it's not a case of trading between organisations on that level

646
00:39:25,690 --> 00:39:27,540
used by

647
00:39:27,550 --> 00:39:29,190
the point is

648
00:39:29,250 --> 00:39:32,600
if you notice that what it has when it's one of the k the communities

649
00:39:32,600 --> 00:39:35,050
tend to be orientated but applications

650
00:39:35,070 --> 00:39:38,800
so then so long as it every ten working on the same source software using

651
00:39:38,800 --> 00:39:40,860
the same sort of resources i think

652
00:39:40,970 --> 00:39:42,220
that is

653
00:39:42,230 --> 00:39:46,420
the old based task two somebody's only using it

654
00:39:46,480 --> 00:39:50,470
that is where we have this concept of the management and and it's their responsibility

655
00:39:51,580 --> 00:39:53,210
to handle that

656
00:39:54,240 --> 00:39:57,330
i think there is an interesting talk actually give them

657
00:39:57,390 --> 00:40:01,380
a couple weeks ago chap from oxford working on project with

658
00:40:01,390 --> 00:40:05,270
coming up with the ability to trade screensaver time between the departments in

659
00:40:05,290 --> 00:40:06,110
the city

660
00:40:06,140 --> 00:40:08,640
so you can actually look up credits and then he says you can have the

661
00:40:08,640 --> 00:40:12,470
derivatives market in screensaver time

662
00:40:12,510 --> 00:40:16,010
so you can buy your computing resources by allowing other people to use your resources

663
00:40:16,010 --> 00:40:19,250
and that would actually very directly answer your

664
00:40:19,270 --> 00:40:24,300
question he started to be such that we should i think be quite entertaining

665
00:40:30,460 --> 00:40:34,090
for this is this is

666
00:40:55,530 --> 00:40:59,180
because it

667
00:40:59,190 --> 00:41:00,830
three as are

668
00:41:00,830 --> 00:41:07,320
called the shrinkage estimator shrinking my browser little bit towards that particular solution

669
00:41:07,350 --> 00:41:09,680
you can

670
00:41:21,120 --> 00:41:26,050
not quite sure but

671
00:41:26,060 --> 00:41:30,150
it at least this is the estimator that you will be getting

672
00:41:33,750 --> 00:41:39,310
what you can also show is that this is exactly the regularized risk estimate if

673
00:41:39,310 --> 00:41:47,270
you impose good penalty under normal on the natural parameters

674
00:41:47,280 --> 00:41:51,420
so the idea is really strange things towards the origin

675
00:41:51,500 --> 00:41:53,040
that works better

676
00:41:53,050 --> 00:41:57,660
so it may be that the proof that james stein gave worked for two or

677
00:41:57,660 --> 00:42:00,590
three dimensions so it's quite possible

678
00:42:00,600 --> 00:42:02,310
but this is the basic idea

679
00:42:05,700 --> 00:42:12,970
all the support vectors and gaussianprocess estimators actually do exactly this

680
00:42:12,990 --> 00:42:18,090
but OK the optimisation problem looks all the more fancy so this is why

681
00:42:19,490 --> 00:42:25,300
you would immediately recognise it

682
00:42:27,580 --> 00:42:31,010
is an interesting connection between fisher scores

683
00:42:31,020 --> 00:42:33,250
and exponential families

684
00:42:33,260 --> 00:42:37,500
so if i have a member of the exponential family in the fisher score

685
00:42:37,550 --> 00:42:41,650
it's just fall fix one st theta g theta

686
00:42:41,700 --> 00:42:46,440
so in other words is the same to the version of for fix

687
00:42:46,450 --> 00:42:48,930
and furthermore the fisher information

688
00:42:48,940 --> 00:42:52,730
it just happens to be the second derivative of g if data

689
00:42:52,740 --> 00:42:54,490
this gives rise to

690
00:42:54,540 --> 00:42:57,920
well quite some work on information geometry

691
00:42:57,990 --> 00:43:00,310
is now this

692
00:43:00,320 --> 00:43:04,450
all implies the metric and all that

693
00:43:04,490 --> 00:43:08,540
but the nice thing is that the efficiency of the estimator can be obtained directly

694
00:43:08,540 --> 00:43:13,900
from will take the second derivative of the log partition function

695
00:43:13,940 --> 00:43:16,560
and if you can get this

696
00:43:16,610 --> 00:43:18,970
true covariance matrix explicitly

697
00:43:19,020 --> 00:43:26,670
don't know the privatisation all that you can actually go for the outer product matrix

698
00:43:26,710 --> 00:43:28,930
and will you have to sing to anyway

699
00:43:28,940 --> 00:43:31,310
so what you get is kernel PCA

700
00:43:31,320 --> 00:43:32,860
as a way of obtaining

701
00:43:32,870 --> 00:43:37,710
an empirical estimate of the fisher information matrix

702
00:43:37,730 --> 00:43:44,110
well which is another nice application of kernel PCA

703
00:43:45,830 --> 00:43:51,000
and this is actually very it's a bit more interesting name priors and we so

704
00:43:51,000 --> 00:43:53,070
far this is all really like

705
00:43:53,080 --> 00:43:56,780
thirties forties textbook statistics

706
00:43:56,790 --> 00:43:59,790
well maybe not the kind of peace and so on but in a way it's

707
00:43:59,790 --> 00:44:02,750
really quite standing

708
00:44:02,770 --> 00:44:08,390
problem with maximum likelihood estimation is that if we don't have enough doctor well parameter

709
00:44:08,390 --> 00:44:10,720
estimates will be bad

710
00:44:11,100 --> 00:44:16,020
remember the example of the dice even if i had to talk to the hundred

711
00:44:16,020 --> 00:44:19,540
times i might not get very good list

712
00:44:19,550 --> 00:44:23,690
so well what can we do as well we just use our prior knowledge

713
00:44:23,700 --> 00:44:28,640
quite often we know where the solution should be

714
00:44:29,050 --> 00:44:30,570
well one

715
00:44:30,600 --> 00:44:36,610
way of setting this and that's just for computational convenience for instance is we set

716
00:44:36,620 --> 00:44:41,490
the prior on theta we just assume that theta is somehow small that may be

717
00:44:41,490 --> 00:44:45,490
much smarter ways of doing this through conjugate priors and so on but this is

718
00:44:45,490 --> 00:44:48,640
one way of setting

719
00:44:48,690 --> 00:44:50,150
well in this case

720
00:44:50,160 --> 00:44:52,260
well what's the posterior distribution

721
00:44:52,270 --> 00:44:55,020
well by bayes rule

722
00:44:55,040 --> 00:44:55,940
we know

723
00:44:59,110 --> 00:45:00,240
p of data

724
00:45:00,330 --> 00:45:02,560
given x

725
00:45:02,580 --> 00:45:03,800
it is

726
00:45:03,830 --> 00:45:07,290
p of x given theta one

727
00:45:07,310 --> 00:45:08,520
p of states

728
00:45:08,530 --> 00:45:10,310
divided by

729
00:45:10,370 --> 00:45:11,920
p of x

730
00:45:15,700 --> 00:45:20,260
p x doesn't really occur anywhere in the most important thing

731
00:45:21,040 --> 00:45:24,360
we can just drop this maker proportional

732
00:45:25,940 --> 00:45:28,130
and then if i do that

733
00:45:28,160 --> 00:45:31,200
i can just write up here if they given x

734
00:45:31,250 --> 00:45:34,300
this this expression here

735
00:45:36,590 --> 00:45:39,990
and then i could several knowledge to maximize

736
00:45:40,030 --> 00:45:43,840
the posterior probability

737
00:45:43,860 --> 00:45:48,940
rather than the likelihood

738
00:45:48,940 --> 00:45:50,890
let's see what happens

739
00:45:50,920 --> 00:45:52,560
the same data before

740
00:45:52,700 --> 00:45:57,270
and i probably applied only very very weak prior because of what you can see

741
00:45:58,790 --> 00:46:02,710
so the blue ones for reference all the maximum likelihood estimate

742
00:46:02,710 --> 00:46:06,190
the red one so the maximum posterior systems

743
00:46:06,200 --> 00:46:09,610
what you can see is that even though i didn't see a single four

744
00:46:09,630 --> 00:46:11,380
actually why

745
00:46:11,390 --> 00:46:13,690
max aposteriori

746
00:46:13,690 --> 00:46:19,440
estimate of singapore is pushed up from zero to something like point of

747
00:46:19,450 --> 00:46:21,090
o five

748
00:46:21,420 --> 00:46:25,250
and those really extremes are reduced

749
00:46:25,250 --> 00:46:29,000
just because it's not like in the same pose prior which assumes that all the

750
00:46:29,000 --> 00:46:30,940
coefficients are pretty much

751
00:46:30,950 --> 00:46:33,480
the same

752
00:46:33,520 --> 00:46:34,840
i get more data

753
00:46:34,860 --> 00:46:36,460
you can see that

754
00:46:36,470 --> 00:46:41,000
well the posterior looks more like the maximum likelihood estimate

755
00:46:41,010 --> 00:46:43,760
it just means that as i get more data

756
00:46:43,780 --> 00:46:47,190
my estimate starts believing in the doctor more and more

757
00:46:47,200 --> 00:46:49,510
and discount the prior

758
00:46:49,530 --> 00:46:51,050
which is what it should

759
00:46:51,110 --> 00:46:55,670
after all the prior is there to help in situations we don't have

760
00:46:55,680 --> 00:46:57,760
but it it also shows you

761
00:46:57,790 --> 00:47:02,560
is that well have probably chose my prior prior way to weekly

762
00:47:04,610 --> 00:47:06,620
this after all is advice

763
00:47:06,630 --> 00:47:09,380
and you should see pretty much a uniform distribution

764
00:47:09,400 --> 00:47:14,070
the deviations so that have been looking at are actually

765
00:47:15,840 --> 00:47:20,220
whatever your random number generator would do so there's no reason for me to assume

766
00:47:20,220 --> 00:47:23,460
that this was truly biased

767
00:47:23,500 --> 00:47:26,070
so here's an example of a prior

768
00:47:26,100 --> 00:47:28,680
but actually products too weak

769
00:47:28,680 --> 00:47:34,370
usually people worry about the opposite but maybe should always

770
00:47:35,880 --> 00:47:38,650
now the top prize are conjugate priors

771
00:47:43,560 --> 00:47:45,590
might be something that

772
00:47:45,600 --> 00:47:48,880
you can read a lot about in statistics textbooks

773
00:47:48,990 --> 00:47:53,420
and it's always a little bit of a mystery how people get there

774
00:47:53,430 --> 00:47:54,690
so if you actually

775
00:47:54,740 --> 00:47:59,460
the a big to the bottom of it so it's a very very simple concept

776
00:47:59,470 --> 00:48:02,790
so the problem with the nobel prize in the posterior doesn't look

777
00:48:02,810 --> 00:48:05,850
like the likelihood

778
00:48:05,860 --> 00:48:09,830
that's bad for two reasons first of all you don't know the normalisation anymore

779
00:48:09,830 --> 00:48:11,230
of gene expression

780
00:48:11,240 --> 00:48:18,910
so gene expression its concentration of messenger RNA and it's tricky find since it's quite

781
00:48:18,910 --> 00:48:21,280
simple for

782
00:48:21,310 --> 00:48:24,030
forty four

783
00:48:27,210 --> 00:48:29,460
plans for so for some

784
00:48:29,480 --> 00:48:36,640
for some plants like you had and now for some very simple organisms but for

785
00:48:36,640 --> 00:48:43,740
instance four miles that's all for your mind you it's really difficult to get the

786
00:48:43,740 --> 00:48:46,460
formation and to make the

787
00:48:46,470 --> 00:48:47,960
the technology works

788
00:48:47,980 --> 00:48:49,450
in to get

789
00:48:49,720 --> 00:48:56,760
time point time series in very short with a very short time interval

790
00:48:56,770 --> 00:48:58,190
so in this case

791
00:48:58,200 --> 00:49:00,630
you just take data which are static

792
00:49:00,640 --> 00:49:01,650
and you

793
00:49:01,660 --> 00:49:06,080
you taken organisms you give you

794
00:49:06,100 --> 00:49:10,760
give it an input signal and you you went

795
00:49:10,800 --> 00:49:18,000
and you look at at some instantaneous response at some point that is think it

796
00:49:18,000 --> 00:49:21,730
is really to see the point

797
00:49:21,750 --> 00:49:28,690
so sometimes could from but it's also possible point of interest is the case usage

798
00:49:28,690 --> 00:49:31,150
data and there are a lot of paper

799
00:49:31,160 --> 00:49:34,630
in fact the first paper about this subject

800
00:49:34,670 --> 00:49:37,780
which are from two teams teams of the

801
00:49:37,810 --> 00:49:40,680
daphne koller and the team of the everyman

802
00:49:40,700 --> 00:49:41,740
and what they do

803
00:49:41,740 --> 00:49:47,030
they learn parameter and structure of bayesian networks for this unit rock as a bayesian

804
00:49:47,040 --> 00:49:51,130
tree so of course in this case you can't have looked

805
00:49:51,150 --> 00:49:52,260
the new network

806
00:49:53,000 --> 00:49:56,580
at least you can infer something

807
00:49:56,590 --> 00:49:59,290
and give the first picture

808
00:49:59,300 --> 00:50:01,340
of the network

809
00:50:01,380 --> 00:50:03,750
but i'm going to to talk

810
00:50:05,060 --> 00:50:07,270
is is about

811
00:50:07,290 --> 00:50:14,440
inference of biological networks so it's not exactly so we're not going deep into deep

812
00:50:14,440 --> 00:50:16,120
modelling of the system

813
00:50:16,220 --> 00:50:19,150
we just want to get the structure

814
00:50:19,160 --> 00:50:24,200
so why unsupervised way to do this is to say that is to say well

815
00:50:24,240 --> 00:50:27,090
images that i have example

816
00:50:27,110 --> 00:50:32,140
the input is a couple of genes for instance and the output

817
00:50:32,190 --> 00:50:33,870
is zero or one

818
00:50:33,890 --> 00:50:36,060
and i'm going to say that there is

819
00:50:36,060 --> 00:50:39,620
and interaction between g one and g two

820
00:50:39,680 --> 00:50:45,140
if this function is there is a response one

821
00:50:45,810 --> 00:50:51,580
so it's kind of classification problem is that you can solve with any supervised methods

822
00:50:51,630 --> 00:50:54,880
but of course you need to deal with a couple

823
00:50:54,920 --> 00:50:56,130
of the

824
00:50:56,180 --> 00:51:01,550
so as we may gene for support vector machines to be good at that but

825
00:51:01,550 --> 00:51:06,220
also as it seems like you're not rational learning

826
00:51:06,240 --> 00:51:09,100
and this was part of

827
00:51:09,110 --> 00:51:11,180
recent work on the subject

828
00:51:11,190 --> 00:51:12,840
so is another way to

829
00:51:12,840 --> 00:51:14,530
to look at this problem

830
00:51:14,570 --> 00:51:18,060
so you're going to say is that in fact you have

831
00:51:18,080 --> 00:51:19,580
the partial known

832
00:51:19,600 --> 00:51:21,240
network of interaction

833
00:51:21,280 --> 00:51:23,150
and you want to complete it

834
00:51:23,180 --> 00:51:25,110
so it's again supervised

835
00:51:27,020 --> 00:51:30,280
you can see the problem as the problem of

836
00:51:30,330 --> 00:51:35,970
metrics completion you have you have imagine symmetric such represents across and you want to

837
00:51:37,590 --> 00:51:43,100
this is the work of sudan colleagues and you can see this is a supervised

838
00:51:43,100 --> 00:51:48,020
inference of biological networks as the problem of supervised metric learning

839
00:51:48,040 --> 00:51:54,760
this is what is done by their germination and colleagues in two thousand five and

840
00:51:54,760 --> 00:52:02,200
it's a really nice way to deal with this data again it's really different from

841
00:52:03,020 --> 00:52:07,290
city is a city in fact its unsupervised

842
00:52:07,300 --> 00:52:08,600
you don't know

843
00:52:08,630 --> 00:52:15,130
examples of our rules is insisting we make we assume that we have some examples

844
00:52:15,140 --> 00:52:18,570
of our rules in the internet

845
00:52:18,570 --> 00:52:22,580
and i'm going to talk about answer

846
00:52:23,710 --> 00:52:28,530
which is which can be related to the this to once which is what which

847
00:52:28,550 --> 00:52:32,660
slows us with this one that's quite big began and also point-of-view

848
00:52:32,710 --> 00:52:37,460
the idea is we're going to solve the problem directly

849
00:52:37,610 --> 00:52:39,600
i'm going to solve the problem

850
00:52:39,620 --> 00:52:42,770
by doing first

851
00:52:42,770 --> 00:52:44,600
prediction regression

852
00:52:44,610 --> 00:52:50,660
in in out in an output space which is on the some cannot

853
00:52:50,670 --> 00:52:55,990
of course can then be closely related to the network which is known

854
00:52:56,000 --> 00:52:59,000
in fact that is the

855
00:52:59,120 --> 00:53:04,130
so can then we are going to to use it space is the diffusion can

856
00:53:04,140 --> 00:53:09,820
and it's again and which is able to describe the similarity between nodes in a

857
00:53:11,780 --> 00:53:13,430
and we're going to do

858
00:53:13,440 --> 00:53:15,410
be able to make a prediction

859
00:53:15,430 --> 00:53:16,990
formal description

860
00:53:17,360 --> 00:53:18,440
of some

861
00:53:18,450 --> 00:53:23,120
forty five cents and we are going to

862
00:53:23,360 --> 00:53:27,600
go into an output space on the efficient can

863
00:53:27,620 --> 00:53:33,070
once we will be able to do that we're going to use this function

864
00:53:33,070 --> 00:53:38,320
and we're going to use this function for a couple of the x x y

865
00:53:38,320 --> 00:53:39,430
and z

866
00:53:39,460 --> 00:53:44,630
user scalar product in this open space and make predictions

867
00:53:44,680 --> 00:53:47,810
of the camera so in fact we are going to do

868
00:53:47,820 --> 00:53:49,840
can learning in order

869
00:53:50,240 --> 00:53:54,850
to predict the rules it's the ph is

870
00:53:54,860 --> 00:53:57,810
so the edges will be predicted

871
00:53:57,810 --> 00:53:59,770
once the value of the

872
00:53:59,790 --> 00:54:03,300
i cannot predict it will be high

873
00:54:03,310 --> 00:54:07,760
the ball high is that since that then some fresh

874
00:54:07,780 --> 00:54:10,080
OK so i'm going to work

875
00:54:10,100 --> 00:54:12,130
to explain this in day

876
00:54:12,130 --> 00:54:16,930
so it turns out that this marginal probabilities

877
00:54:16,960 --> 00:54:18,800
it's very hard to compute for

878
00:54:18,810 --> 00:54:21,110
almost all models of interest

879
00:54:21,130 --> 00:54:24,350
i'm not sure that you talked about

880
00:54:24,360 --> 00:54:25,880
whether this

881
00:54:26,600 --> 00:54:27,630
to compute this

882
00:54:27,640 --> 00:54:31,400
probably has have been doing so

883
00:54:31,420 --> 00:54:35,330
forced to do model selection i reaching we'll always need to compute this thing

884
00:54:36,790 --> 00:54:43,170
and the idea is that the full model stations to prevent overfitting underfitting by finding

885
00:54:43,170 --> 00:54:45,140
a model of right complexity class

886
00:54:45,170 --> 00:54:47,970
two exploded and because his

887
00:54:47,990 --> 00:54:50,790
because there's not marginal probabilities hard to compute

888
00:54:50,800 --> 00:54:52,810
is expensive to compute

889
00:54:52,830 --> 00:54:55,690
we we don't really want to do this too often

890
00:54:55,730 --> 00:55:00,220
so so that's one inside there was

891
00:55:00,380 --> 00:55:05,570
not about a few people including callousness and zoubin most recently i think not that

892
00:55:05,570 --> 00:55:12,020
recently like it's about people before that collective high and revenue

893
00:55:12,250 --> 00:55:14,710
is that

894
00:55:14,750 --> 00:55:22,160
because we're integrating all of all parameters where we do this computation or when we

895
00:55:22,160 --> 00:55:26,670
are being bayesian

896
00:55:26,680 --> 00:55:31,420
we're not going to overfit anyway three because we integrated out of parameters

897
00:55:31,910 --> 00:55:36,890
there's nothing to fit to all data so that we can really overfit

898
00:55:36,900 --> 00:55:40,890
and if we can't really overfit then what

899
00:55:40,900 --> 00:55:44,480
is that what is to prevent us from using a really large model

900
00:55:44,490 --> 00:55:48,510
because we if we if we start with a really large model

901
00:55:48,520 --> 00:55:51,110
then we want and the fit to data

902
00:55:51,140 --> 00:55:54,360
and we want to overfit either because we're being bayesian myself

903
00:55:54,370 --> 00:55:56,730
so that's kind of the the basic

904
00:55:56,740 --> 00:55:59,730
the idea of why we might want to do

905
00:55:59,930 --> 00:56:03,680
bayesian nonparametrics from this perspective

906
00:56:06,570 --> 00:56:08,490
so what is nonparametric model

907
00:56:08,500 --> 00:56:12,130
nonparametric models basically a

908
00:56:12,240 --> 00:56:18,030
parametric model that's really liked so when the number of parameters

909
00:56:18,260 --> 00:56:22,500
is it the infinite or increases with the data that we see

910
00:56:24,840 --> 00:56:26,460
we can also think of it as

911
00:56:26,490 --> 00:56:31,100
a model over and some some infinite dimensional function on measure space

912
00:56:31,160 --> 00:56:34,690
so you've seen an example of this in the case of the girls process right

913
00:56:34,700 --> 00:56:36,440
so sokol

914
00:56:36,450 --> 00:56:43,610
place a prior distribution over all possible functions is given by a gaussian process

915
00:56:44,590 --> 00:56:50,090
this is an infinite dimensional space infinite countably infinite an uncountably infinite

916
00:56:50,100 --> 00:56:55,800
dimensional space race we needed to have a latent variable

917
00:56:55,800 --> 00:56:57,800
for every input

918
00:56:57,870 --> 00:57:00,320
after function

919
00:57:00,760 --> 00:57:06,070
another way of thinking about what is an appropriate to model this is the family

920
00:57:06,070 --> 00:57:07,870
of distributions

921
00:57:07,880 --> 00:57:11,830
that is that in some large space so

922
00:57:12,100 --> 00:57:15,620
that's in the sense that if you give me any

923
00:57:15,670 --> 00:57:20,070
any other if you give me any distribution i can find a distribution in this

924
00:57:20,070 --> 00:57:22,560
family that's as close as you want

925
00:57:23,530 --> 00:57:24,880
two your distribution

926
00:57:24,910 --> 00:57:26,260
j and

927
00:57:26,290 --> 00:57:31,210
and so the idea is that with the parametric family when parameterizing it with a

928
00:57:31,210 --> 00:57:34,200
finite number of parameters so we can think of

929
00:57:34,230 --> 00:57:39,690
family of distributions as very low dimensional manifold in our

930
00:57:39,710 --> 00:57:45,160
in in this large space of possible distributions and if you have a small that

931
00:57:45,160 --> 00:57:51,090
if you have a low dimensional manifold you're not going to be able to

932
00:57:51,920 --> 00:57:56,670
approximate any distribution to arbitrary accuracy accuracy

933
00:57:56,690 --> 00:58:00,890
so with not from tree models we're dealing with a family of distributions there so

934
00:58:00,890 --> 00:58:04,960
large that it can approximate any distribution to any arbitrary

935
00:58:07,110 --> 00:58:09,840
so why move

936
00:58:09,840 --> 00:58:14,330
we want to deal with non parametric models in bayesian theory of learning

937
00:58:14,340 --> 00:58:15,140
so a

938
00:58:15,160 --> 00:58:19,030
it's a really nice way of sidestepping model selection and averaging

939
00:58:19,040 --> 00:58:22,030
is we can also think of it as

940
00:58:22,080 --> 00:58:24,860
i have a broad class of priors that

941
00:58:24,920 --> 00:58:26,430
it doesn't restrict

942
00:58:26,450 --> 00:58:28,320
the model to a promontory

943
00:58:28,320 --> 00:58:30,140
family so it can

944
00:58:30,200 --> 00:58:33,880
in this sense allows the data the data to speak for itself right so if

945
00:58:33,880 --> 00:58:37,520
you have lots of data then you get very close to the

946
00:58:37,590 --> 00:58:39,090
distribution that

947
00:58:39,100 --> 00:58:40,900
the underlying the data

948
00:58:45,740 --> 00:58:47,320
i personally find

949
00:58:47,320 --> 00:58:49,550
but if you want to

950
00:58:49,560 --> 00:58:56,020
extract because you have to put a very big quantity of energy

951
00:58:56,030 --> 00:59:01,840
the problems with the liquid drop is related to the fission fragment distributions

952
00:59:01,860 --> 00:59:04,570
so as i told you

953
00:59:04,810 --> 00:59:07,840
we could what was first developed two

954
00:59:07,850 --> 00:59:09,390
the distribution

955
00:59:09,410 --> 00:59:10,310
it's true

956
00:59:10,940 --> 00:59:13,800
for the feature it's OK

957
00:59:13,820 --> 00:59:18,230
but in fact there are many problems and for example as i told you knew

958
00:59:18,250 --> 00:59:19,350
can only

959
00:59:19,360 --> 00:59:23,000
produces symmetric fragments of the same fragment

960
00:59:23,010 --> 00:59:27,110
seven different nations amongst members of number of protons and neutrons

961
00:59:27,120 --> 00:59:29,350
an experiment that is not true at all

962
00:59:29,390 --> 00:59:33,880
this is a very famous experiment results from people from geo site

963
00:59:33,890 --> 00:59:36,150
in darmstadt in germany

964
00:59:36,160 --> 00:59:43,800
you have the a fission fragment distributions from different actin uranium relevant i'm acting

965
00:59:43,820 --> 00:59:47,060
and these are sufficient for distributions

966
00:59:47,070 --> 00:59:51,380
so when you have here when people it means that the most probable from opposition

967
00:59:51,380 --> 00:59:54,370
here correspond to two identical fragments

968
00:59:54,380 --> 00:59:56,430
so symmetry fragment

969
00:59:56,450 --> 00:59:59,470
but can be described the liquid drop

970
00:59:59,490 --> 01:00:04,280
it you uses that for many many nuclei you have these two hand

971
01:00:04,290 --> 01:00:10,940
distributions so and this most fragments correspond to a small fragments and everyone one

972
01:00:10,970 --> 01:00:14,110
so this is what we call asymmetric fission

973
01:00:14,130 --> 01:00:19,390
so from many nuclei and especially all the ones that are interesting for example for

974
01:00:19,390 --> 01:00:20,780
four runs

975
01:00:20,800 --> 01:00:25,240
they do have these asymmetric fission which cannot be reproduced

976
01:00:25,250 --> 01:00:28,850
by these liquid drop model

977
01:00:28,870 --> 01:00:30,140
so conclusions

978
01:00:30,180 --> 01:00:33,470
the nucleus is not a liquid drop you have

979
01:00:33,590 --> 01:00:35,730
structure effects that are

980
01:00:35,740 --> 01:00:39,100
but appears and that can be important

981
01:00:39,110 --> 01:00:40,140
so what

982
01:00:40,150 --> 01:00:45,170
as i told you there are magic numbers so the magic nuclei

983
01:00:45,190 --> 01:00:46,900
which magic

984
01:00:46,920 --> 01:00:50,220
you can imagine proton neutron

985
01:00:50,230 --> 01:00:56,030
and you have very stable nuclei which are doubly magic like led to eight

986
01:00:56,050 --> 01:00:58,790
ten one hundred etcetera

987
01:00:58,800 --> 01:01:03,470
so that's why there's a need for microscopic approaches you have to replace

988
01:01:03,490 --> 01:01:05,250
the technique was drop by

989
01:01:05,270 --> 01:01:06,780
something like this

990
01:01:06,790 --> 01:01:08,870
where you have the nuclear

991
01:01:08,880 --> 01:01:15,530
which are is quite nucleons and the interaction between the

992
01:01:15,550 --> 01:01:16,910
so this is the way

993
01:01:16,920 --> 01:01:23,390
microscopic approaches to do so they consider anything else in strong interactions

994
01:01:23,500 --> 01:01:27,450
then you have two problems to major problems

995
01:01:27,470 --> 01:01:31,420
the first one is quite general problem in the many body problem

996
01:01:31,430 --> 01:01:34,540
whatever it is when you have and

997
01:01:34,560 --> 01:01:37,030
which interactions

998
01:01:37,050 --> 01:01:40,890
it's impossible to solve that problem

999
01:01:41,180 --> 01:01:44,170
and particularly for large systems

1000
01:01:44,180 --> 01:01:48,790
so the same for us if we have to calculate the interaction

1001
01:01:48,800 --> 01:01:53,250
you see how many combination there are for example if there is only

1002
01:01:53,270 --> 01:01:55,910
two twenty four so we have to change

1003
01:01:55,930 --> 01:02:00,170
one by one and three but with three people forwards and then we have to

1004
01:02:00,170 --> 01:02:07,020
to symmetrize everything so it it's impossible to solve these equations of corporate set of

1005
01:02:07,020 --> 01:02:08,690
short linear equations

1006
01:02:08,720 --> 01:02:11,490
so from medical applications when the port

1007
01:02:11,540 --> 01:02:18,290
different magnetic field something like that it's the same problem so it's something really junior

1008
01:02:18,340 --> 01:02:21,650
so that means that you have to make some approximations

1009
01:02:21,750 --> 01:02:26,750
the approximation that we using nuclear physics some common two

1010
01:02:26,760 --> 01:02:29,160
all the different physics case

1011
01:02:29,180 --> 01:02:31,780
some are specified to make our

1012
01:02:31,890 --> 01:02:34,350
nuclear physics

1013
01:02:34,390 --> 01:02:41,890
second problem which is that case specific to nuclear physics in the nucleon nucleon interaction

1014
01:02:41,900 --> 01:02:44,350
we don't know precisely

1015
01:02:44,380 --> 01:02:47,500
and then you can only nucleon interactions

1016
01:02:47,510 --> 01:02:51,250
in need so it's not a free interaction you're in many cases you have the

1017
01:02:51,250 --> 01:02:55,100
medium and then we don't know how to derive that completely

1018
01:02:55,120 --> 01:02:57,660
from the first principal of QCD

1019
01:02:57,690 --> 01:03:02,640
so that is specific because you not only physics when you're dealing with electrons you

1020
01:03:02,640 --> 01:03:04,130
know the coulomb repulsion

1021
01:03:04,140 --> 01:03:05,890
even if it's difficult to treat

1022
01:03:05,950 --> 01:03:10,890
but you know the expression nuclear physics you don't know

1023
01:03:10,890 --> 01:03:16,350
so these two problems which seems to be the copyright are in fact that cooperate

1024
01:03:16,350 --> 01:03:19,480
to be sign theta this path length will be

1025
01:03:19,520 --> 01:03:20,550
such that

1026
01:03:20,560 --> 01:03:26,860
constructive interference will occur in phase radiation will be reflected as such

1027
01:03:26,880 --> 01:03:28,030
and this

1028
01:03:28,040 --> 01:03:31,240
is called regular

1029
01:03:31,260 --> 01:03:35,400
this is bragg's law it's the condition for constructive interference

1030
01:03:38,350 --> 01:03:43,870
condition for constructive interference in the crystal

1031
01:03:43,880 --> 01:03:47,760
and three o nine one in three online one we're going to do

1032
01:03:47,770 --> 01:03:52,520
three nine one we're going to neglect higher order

1033
01:03:52,560 --> 01:03:57,570
reflections on three nine when we neglect higher reflections so let's just put

1034
01:03:57,620 --> 01:04:02,600
the equals one so i should be and equals one so bragg three online one

1035
01:04:02,600 --> 01:04:03,930
will simply

1036
01:04:03,940 --> 01:04:05,870
lambda equal to the

1037
01:04:05,890 --> 01:04:09,230
sin theta

1038
01:04:09,320 --> 01:04:13,670
that's the bragg condition

1039
01:04:15,510 --> 01:04:17,150
suppose i have new

1040
01:04:17,160 --> 01:04:19,040
the system

1041
01:04:19,110 --> 01:04:22,190
and i find that i have

1042
01:04:22,250 --> 01:04:24,510
now adams here

1043
01:04:24,520 --> 01:04:27,400
let's say i go from a simple cubic

1044
01:04:27,400 --> 01:04:29,290
two body centered cubic

1045
01:04:29,300 --> 01:04:32,270
well in body centered cubic there atoms here

1046
01:04:32,320 --> 01:04:36,500
another in a different place relative to the board relative to

1047
01:04:36,540 --> 01:04:40,870
the horizontal there's a plane of atoms at this location so let's see what happens

1048
01:04:40,870 --> 01:04:42,060
in this case

1049
01:04:42,070 --> 01:04:47,570
in this case if i have a third comes in let's call rate three

1050
01:04:47,730 --> 01:04:53,430
and re three also is in phase with re one hundred two

1051
01:04:54,250 --> 01:04:56,360
this is the reflected ray three

1052
01:04:56,370 --> 01:05:00,520
i can define the additional path length of re three as

1053
01:05:01,310 --> 01:05:03,640
he f

1054
01:05:03,760 --> 01:05:05,700
and what we see is that

1055
01:05:05,730 --> 01:05:07,430
it turns out that the

1056
01:05:07,450 --> 01:05:08,780
do you

1057
01:05:08,790 --> 01:05:10,900
plus f

1058
01:05:10,970 --> 01:05:15,400
for n equals one this is going to be one half a wavelength

1059
01:05:15,400 --> 01:05:19,610
if a b plays b c is one wavelength this is one-half wavelength

1060
01:05:19,620 --> 01:05:22,770
and what that means is that the reflected

1061
01:05:22,900 --> 01:05:28,360
reflected three is perfectly out of things

1062
01:05:28,380 --> 01:05:33,270
perfectly out of phase

1063
01:05:34,130 --> 01:05:36,190
raise one and two

1064
01:05:36,200 --> 01:05:38,930
with one and two

1065
01:05:38,990 --> 01:05:43,190
so that means we're get destructive interference

1066
01:05:44,030 --> 01:05:46,950
we're gonna get destructive interference

1067
01:05:51,140 --> 01:05:53,780
and that means if we put in detector

1068
01:05:54,200 --> 01:05:59,900
if we parker detector right here we'll measure zero intensity

1069
01:05:59,900 --> 01:06:02,310
so we measure

1070
01:06:04,020 --> 01:06:06,230
intensity at that angle

1071
01:06:09,640 --> 01:06:13,670
now what i want to do so want want to generalize what we have observed

1072
01:06:13,670 --> 01:06:19,510
here want to generalize this analysis and if i generalize this analysis what i get

1073
01:06:19,510 --> 01:06:23,750
is that if i take the interference criterion

1074
01:06:23,770 --> 01:06:26,970
if i take the interference criterion

1075
01:06:27,010 --> 01:06:32,220
and i combine it with the crystal structure

1076
01:06:32,240 --> 01:06:36,540
i combine it with the crystal structure

1077
01:06:37,020 --> 01:06:38,250
what's that give me

1078
01:06:38,260 --> 01:06:41,470
it gives me the set of expected

1079
01:06:44,030 --> 01:06:45,520
the set of expected

1080
01:06:55,310 --> 01:07:00,680
in essence this is a fingerprint of the crystal structure

1081
01:07:00,730 --> 01:07:04,200
fingerprint of the crystal structure

1082
01:07:04,260 --> 01:07:07,660
it's not a fingerprint of the chemical identity

1083
01:07:07,800 --> 01:07:10,240
it doesn't say anything about identity

1084
01:07:10,250 --> 01:07:12,640
but any crystal structure

1085
01:07:12,640 --> 01:07:13,790
of this

1086
01:07:13,820 --> 01:07:18,420
description will give the reflections that the same set of angles

1087
01:07:18,440 --> 01:07:22,390
any crystal structure of this description will give no reflections

1088
01:07:22,430 --> 01:07:24,680
at the same set of angles

1089
01:07:26,410 --> 01:07:28,250
this for example will show you

1090
01:07:28,260 --> 01:07:32,190
what would happen in the case of

1091
01:07:32,260 --> 01:07:34,050
zero zero one

1092
01:07:34,070 --> 01:07:39,950
zero zero one we saw reflects zero zero one we saw reflects

1093
01:07:40,000 --> 01:07:43,020
in simple cubic

1094
01:07:45,220 --> 01:07:47,280
the given angle theta

1095
01:07:47,300 --> 01:07:49,840
o one

1096
01:07:50,690 --> 01:07:53,320
it does not reflect

1097
01:07:53,950 --> 01:07:58,240
does not reflect

1098
01:07:59,700 --> 01:08:01,620
body centered cubic

1099
01:08:02,540 --> 01:08:03,360
thing to

1100
01:08:03,380 --> 01:08:05,880
o or one because in

1101
01:08:05,920 --> 01:08:09,870
body centered cubic there are atoms in o o two

1102
01:08:09,910 --> 01:08:11,630
and adams n o o two

1103
01:08:11,630 --> 01:08:14,800
will be totally out of phase with

1104
01:08:14,850 --> 01:08:19,120
the reflection it's coming off of all one so we will see no reflection

1105
01:08:19,140 --> 01:08:20,950
at all one

1106
01:08:21,000 --> 01:08:27,030
and likewise with FCC FCC also has atoms at the midpoint on each face

1107
01:08:29,400 --> 01:08:31,720
it does not reflect in BCC

1108
01:08:33,140 --> 01:08:35,640
FCC both of them

1109
01:08:35,690 --> 01:08:38,100
so now i can start to distinguish

1110
01:08:38,140 --> 01:08:40,600
i can distinguish which are

1111
01:08:40,610 --> 01:08:42,810
which are now let's take a look at

1112
01:08:42,820 --> 01:08:45,660
some opportunity to

1113
01:08:45,680 --> 01:08:48,230
make sense of this

1114
01:08:48,290 --> 01:08:51,090
so here's here's just figure that i took from

1115
01:08:51,170 --> 01:08:54,600
one of the text is showing what i've drawn on the board

1116
01:08:54,630 --> 01:08:56,030
zoom through this

1117
01:08:56,070 --> 01:08:58,340
OK so now this is on the handout

1118
01:08:58,380 --> 01:09:01,990
this is the selection rules for reflection in cubic crystals

1119
01:09:02,080 --> 01:09:06,830
what has happened here is is that the geometric analysis

1120
01:09:06,880 --> 01:09:09,810
along these lines has been performed

1121
01:09:09,860 --> 01:09:12,470
taking into account the atom positions

1122
01:09:12,490 --> 01:09:13,650
and what we see

1123
01:09:13,660 --> 01:09:17,710
we see the simple cubic we get reflections at every

1124
01:09:18,630 --> 01:09:20,170
every index

1125
01:09:20,220 --> 01:09:21,830
body centered cubic

1126
01:09:21,830 --> 01:09:24,600
is not reflecting the one all

1127
01:09:24,610 --> 01:09:27,690
because we've shown already that the

1128
01:09:27,750 --> 01:09:33,140
reflection from two all at that angle is perfectly out of phase so the

1129
01:09:33,150 --> 01:09:38,530
computations have been done likewise FCC so this is indicating that you won't see

1130
01:09:38,570 --> 01:09:40,730
i want all reflection in

1131
01:09:40,750 --> 01:09:42,390
face centered cubic or

1132
01:09:42,390 --> 01:09:44,460
the chernoff bound

1133
01:09:44,460 --> 01:09:45,960
right this is an idea

1134
01:09:46,270 --> 01:09:49,540
that's used in in chernoff bounds and and district

1135
01:09:49,560 --> 01:09:53,940
appears in a lot of places all this thing this is an expectation of e

1136
01:09:53,940 --> 01:09:57,390
to the east times some some

1137
01:09:57,410 --> 01:09:59,680
OK that's the moment generating function

1138
01:09:59,830 --> 01:10:03,430
we have a random variable here my considering the expectation of e to the is

1139
01:10:03,460 --> 01:10:06,700
times random variable

1140
01:10:06,710 --> 01:10:08,020
OK who's

1141
01:10:08,060 --> 01:10:09,790
is not met

1142
01:10:09,850 --> 01:10:13,890
moment generating functions before

1143
01:10:13,910 --> 01:10:15,100
if you

1144
01:10:17,290 --> 01:10:19,910
of the birds this is this is

1145
01:10:19,960 --> 01:10:23,750
modular sometimes changes all plus transform so for the

1146
01:10:23,790 --> 01:10:25,730
other electrical engineers

1147
01:10:25,770 --> 01:10:30,020
among its in essentially the same the same as well plus transport

1148
01:10:30,380 --> 01:10:34,640
so what we're concerned with now i mean we can pull it either minus st

1149
01:10:34,640 --> 01:10:40,430
out we're concerned just with controlling the moment generating function of of this random variable

1150
01:10:40,480 --> 01:10:44,520
can we can express that as an expectation of this product have the eyes are

1151
01:10:44,520 --> 01:10:46,310
not independent

1152
01:10:46,330 --> 01:10:50,710
you know the the whole things inequality relies on independence of these things

1153
01:10:50,770 --> 01:10:53,700
we have something close to independence because of their

1154
01:10:55,940 --> 01:11:01,100
we have enough properties to to make things work by by conditioning appropriately like the

1155
01:11:01,100 --> 01:11:05,430
details of that but you know the whole game from now on is controlling

1156
01:11:05,440 --> 01:11:07,160
the moment generating function

1157
01:11:07,160 --> 01:11:11,870
of these other of this random variable

1158
01:11:11,890 --> 01:11:14,040
OK and it turns out

1159
01:11:14,060 --> 01:11:17,410
OK i condition on something appropriate

1160
01:11:17,520 --> 01:11:21,880
all right so if we have a mean zero random variable lies in our a

1161
01:11:21,880 --> 01:11:26,270
bounded range right this is her things inequality

1162
01:11:26,290 --> 01:11:31,060
OK think first prove something of this form may be with the

1163
01:11:31,140 --> 01:11:34,040
what i think you got a different constant

1164
01:11:34,310 --> 01:11:35,660
and that was

1165
01:11:35,680 --> 01:11:39,750
now actually editing and improving the concert so

1166
01:11:39,790 --> 01:11:43,870
you can show the moment generating function of bounded random variable is is bounded in

1167
01:11:43,870 --> 01:11:44,910
terms of

1168
01:11:44,980 --> 01:11:48,120
this either a square

1169
01:11:48,160 --> 01:11:51,040
kind of quantity

1170
01:11:51,060 --> 01:11:52,370
OK and

1171
01:11:52,390 --> 01:11:55,430
it's the bounded the differences property

1172
01:11:55,440 --> 01:11:56,680
lets us

1173
01:11:56,680 --> 01:12:01,040
but let's argue that know the appropriate random variables here abound

1174
01:12:01,080 --> 01:12:03,640
OK so so now we get available on now

1175
01:12:03,700 --> 01:12:07,620
moment generating function in terms of the

1176
01:12:08,480 --> 01:12:12,430
the variable x squared and in terms of the sum of the differences square

1177
01:12:12,480 --> 01:12:16,520
and we just play with the variable as to to come up with to come

1178
01:12:16,520 --> 01:12:20,560
up with the bound on the probability that we get the deviations and that's that's

1179
01:12:20,560 --> 01:12:27,290
the that's make demands inequality right that the probability the deviation between g expectation being

1180
01:12:27,290 --> 01:12:29,160
larger is bounded by

1181
01:12:30,060 --> 01:12:33,850
decreasing exponentially in two script

1182
01:12:33,890 --> 01:12:38,140
right so we get some sort of something that looks like a gas entail

1183
01:12:38,140 --> 01:12:41,120
right provided that the

1184
01:12:41,120 --> 01:12:45,370
the random variable g has this is bounded differences property

1185
01:12:45,430 --> 01:12:46,930
OK so that's

1186
01:12:47,350 --> 01:12:52,460
that's where make demons inequality comes from

1187
01:12:52,480 --> 01:12:53,890
this idea of

1188
01:12:56,410 --> 01:12:58,640
the moment generating function

1189
01:12:58,700 --> 01:12:59,810
the other

1190
01:12:59,830 --> 01:13:05,100
bounded random variable is something that that we'll meet again

1191
01:13:05,120 --> 01:13:12,330
OK so we're only

1192
01:13:12,390 --> 01:13:14,460
so that's my demons inequality

1193
01:13:14,460 --> 01:13:16,640
just remind you a headed for

1194
01:13:16,680 --> 01:13:18,310
for this

1195
01:13:18,390 --> 01:13:20,350
this result

1196
01:13:20,390 --> 01:13:23,700
alright tells us that

1197
01:13:23,730 --> 01:13:27,390
expectations are not much bigger than sample averages

1198
01:13:27,410 --> 01:13:30,350
so far we've got that the supremum

1199
01:13:30,460 --> 01:13:34,460
the class of the deviations is close to the expectation of that thing

1200
01:13:34,500 --> 01:13:36,600
right you know a little bit extra

1201
01:13:36,850 --> 01:13:38,060
because of this

1202
01:13:38,100 --> 01:13:43,620
the arises from victims inequality so now we just want to control the expectation of

1203
01:13:43,620 --> 01:13:44,890
this thing

1204
01:13:49,580 --> 01:13:50,440
all right so

1205
01:13:50,460 --> 01:13:54,830
and this is where the rademacher averages pop up in a very natural way

1206
01:13:54,870 --> 01:13:56,080
right so these

1207
01:13:56,100 --> 01:13:58,040
these things

1208
01:13:58,060 --> 01:14:00,580
very close to the

1209
01:14:00,600 --> 01:14:02,890
rademacher averages

1210
01:14:05,020 --> 01:14:06,270
right song writer

1211
01:14:06,480 --> 01:14:11,020
a string of inequalities and this is really nothing nothing hard you

1212
01:14:11,020 --> 01:14:13,460
was in a big problem

1213
01:14:13,580 --> 01:14:15,310
of the other corner

1214
01:14:15,310 --> 01:14:17,080
a whole bunch of edges

1215
01:14:17,210 --> 01:14:21,900
a whole bunch of possible entering variables all bunch of X is that willing to

1216
01:14:21,900 --> 01:14:24,520
work and

1217
01:14:26,130 --> 01:14:31,340
simplex act and some of those will mean higher costs so we will grow those

1218
01:14:31,670 --> 01:14:35,210
and some will be lower costs so I will pick out

1219
01:14:35,410 --> 01:14:40,550
which 1 which way to go which Newell X should enter

1220
01:14:40,560 --> 01:14:44,830
and then have to go along the edge until

1221
01:14:46,110 --> 01:14:47,520
come to that corner

1222
01:14:47,540 --> 01:14:51,960
for the next corner and then I do it again so that's the that's the

1223
01:14:53,440 --> 01:14:56,540
repeatedly your a corner

1224
01:14:56,580 --> 01:14:58,690
you test directions

1225
01:14:58,690 --> 01:15:04,690
you travel along the direction where the like steepest descent the cost goes down fastest

1226
01:15:05,690 --> 01:15:10,210
step along the edge and then you get to the next corner and you do

1227
01:15:10,210 --> 01:15:10,810
it again

1228
01:15:11,150 --> 01:15:13,830
that the picture of

1229
01:15:15,790 --> 01:15:21,000
how do you know that did not just a local minimum very good question it's

1230
01:15:21,000 --> 01:15:27,840
it's because of this linearity it's uh it's picked from these linear equations and linear

1231
01:15:27,840 --> 01:15:30,900
constraints you can't

1232
01:15:30,920 --> 01:15:32,150
can't get in

1233
01:15:33,940 --> 01:15:40,180
but to and similarly if I had this quadratic 1 and was positive definite I

1234
01:15:40,180 --> 01:15:46,150
could get track but absolutely local minima and so on

1235
01:15:46,230 --> 01:15:53,630
you know a big challenge and then minimization problems in general you give me a

1236
01:15:53,630 --> 01:15:57,390
function of cost function so you most cost functions are linear

1237
01:15:58,040 --> 01:16:01,870
you give me a cost function f

1238
01:16:01,880 --> 01:16:07,040
hundred variables so that you love me to play with and how long have you

1239
01:16:07,040 --> 01:16:12,440
minimize function of a hundred variables boy that's the that's the resolution of the question

1240
01:16:14,420 --> 01:16:15,360
if that function

1241
01:16:19,980 --> 01:16:24,460
then know so if a function is shaped like this maybe with wiggles but still

1242
01:16:24,460 --> 01:16:28,330
convex then you know there is only 1 minimum

1243
01:16:28,330 --> 01:16:33,770
and problem much easier if the function is quite general it have looked like a

1244
01:16:33,770 --> 01:16:38,560
golf course it's graph is some surface with ups and downs of ups and downs

1245
01:16:38,980 --> 01:16:40,940
then when you find a local minimum

1246
01:16:43,170 --> 01:16:47,290
you can just look around in a local minimum

1247
01:16:47,400 --> 01:16:48,710
because everything

1248
01:16:48,730 --> 01:16:49,860
works upward

1249
01:16:52,630 --> 01:16:53,790
at some other point

1250
01:16:54,510 --> 01:16:58,800
there's a whether is thing much further down and and a whole lot of ideas

1251
01:16:58,810 --> 01:17:02,670
are going to this

1252
01:17:02,730 --> 01:17:06,270
subject and I guess there are a lot more to come

1253
01:17:07,860 --> 01:17:12,520
you somehow have 2 jobs that

1254
01:17:12,540 --> 01:17:21,670
that possible local minimum you know enough to get out of the little well and

1255
01:17:21,710 --> 01:17:23,770
on with the job

1256
01:17:23,770 --> 01:17:24,900
but to

1257
01:17:25,950 --> 01:17:28,510
how much do you

1258
01:17:28,520 --> 01:17:33,540
and you don't want to spend all your time worrying about this you won't find

1259
01:17:33,540 --> 01:17:36,520
that local minimum in the 1st place it's best

1260
01:17:38,690 --> 01:17:43,000
OK 1 comment here and I not

1261
01:17:43,020 --> 01:17:48,750
wasn't planning and and don't have time to develop interior point methods but just so

1262
01:17:48,750 --> 01:17:54,440
you could guess what that means that means that we we travel in the interior

1263
01:17:54,440 --> 01:17:59,940
we find a point so Interior Interior interior of the feasible set

1264
01:17:59,960 --> 01:18:02,310
find someplace that solve the equation

1265
01:18:03,130 --> 01:18:07,110
don't worry about the number of zeros and non-zeros this this had to do with

1266
01:18:08,610 --> 01:18:10,080
finer points

1267
01:18:10,770 --> 01:18:14,870
find direction is better traveling

1268
01:18:15,150 --> 01:18:17,130
decide when to stop

1269
01:18:17,170 --> 01:18:21,830
and probably stopped short of the boundary

1270
01:18:21,840 --> 01:18:25,880
then you're still inside calculus still operates so to speak

1271
01:18:26,190 --> 01:18:32,710
look around and find another point travel for a while playing point and eventually

1272
01:18:34,920 --> 01:18:37,290
infinitely many steps

1273
01:18:37,290 --> 01:18:42,630
reach the way I mean because you're always stopping short but getting closer and closer

1274
01:18:42,830 --> 01:18:45,580
that of course in reality we

1275
01:18:45,770 --> 01:18:48,830
are happy together with internal minus

1276
01:18:48,840 --> 01:18:52,170
something of the winter and and that's OK too

1277
01:18:52,170 --> 01:18:55,580
so that's that's iterative and this is

1278
01:18:55,610 --> 01:18:56,650
a finite number

1279
01:18:58,490 --> 01:19:03,230
can't take well we know it's fine of dying thing has only a finite number

1280
01:19:03,230 --> 01:19:06,070
of corners

1281
01:19:06,110 --> 01:19:10,460
when the number of edges in nanokernel backwards on an edge because as we saw

1282
01:19:10,480 --> 01:19:14,960
forward if we went forward on unintelligible was because it improve the cost

1283
01:19:15,530 --> 01:19:21,790
reduce the cost and will never traveled backward on that so it's going be finite

1284
01:19:21,870 --> 01:19:28,840
but the number of edges is exponentially large parameter and so there's a big amid

1285
01:19:28,850 --> 01:19:31,690
the big the theory of the simplex method is

1286
01:19:32,110 --> 01:19:35,450
doesn't actually take exponentially long

1287
01:19:35,460 --> 01:19:39,830
as mn increases or

1288
01:19:41,400 --> 01:19:44,920
be done in polynomial time

1289
01:19:44,960 --> 01:19:50,020
this is the 1 of the the most important problem problem where it was quite

1290
01:19:50,020 --> 01:19:53,290
unclear even the experts whether

1291
01:19:55,310 --> 01:19:58,420
it whether it can be done in polynomial time

1292
01:19:59,710 --> 01:20:03,170
media power of eminent

1293
01:20:03,190 --> 01:20:08,750
or whether they're not whether it grows like the number of corners which is

1294
01:20:08,790 --> 01:20:11,960
M choose them and grows exponentially

1295
01:20:12,570 --> 01:20:16,590
well peers the incident quick report on that

1296
01:20:16,770 --> 01:20:18,860
the simplex method can

1297
01:20:19,920 --> 01:20:22,520
really worked up cases

1298
01:20:22,540 --> 01:20:24,770
take exponentially long

1299
01:20:24,770 --> 01:20:31,750
so if I described this implies that the clear straightforward simplex method also it's average

1300
01:20:31,750 --> 01:20:36,580
cost that is used all the time so of course it doesn't take exponentially long

1301
01:20:36,580 --> 01:20:40,830
hours we would use it but

1302
01:20:40,840 --> 01:20:45,630
it's worse in in in extreme example could be exponential

1303
01:20:45,890 --> 01:20:51,710
we we could travel along an exponential number of edges before we finally settled on

1304
01:20:51,710 --> 01:20:52,470
the web

1305
01:20:52,970 --> 01:20:58,570
and you see because traveling on edges the court that the cost the computing causes

1306
01:20:58,750 --> 01:21:01,170
is the same however along the edge

1307
01:21:01,190 --> 01:21:02,830
so anyway

1308
01:21:02,840 --> 01:21:09,750
but that but average costs are polynomial so a lot of people and the guy

1309
01:21:09,750 --> 01:21:15,770
at MIT is looking at this problem again and found another way of describing an

1310
01:21:15,770 --> 01:21:18,310
average costs and the

1311
01:21:18,520 --> 01:21:25,130
showing that that's polynomial and then there is a polynomial

1312
01:21:25,130 --> 01:21:28,900
so this may a huge range of things you can do and that's that

1313
01:21:28,910 --> 01:21:32,470
the nice thing about machine learning and it's also nice that you're not alone like

1314
01:21:32,470 --> 01:21:37,120
you're even if you you in the next or in the case of the practice

1315
01:21:37,320 --> 01:21:39,200
we can also talk about london

1316
01:21:39,280 --> 01:21:41,930
if you still have something like according to

1317
01:21:41,950 --> 01:21:43,640
so is

1318
01:21:43,660 --> 01:21:48,970
so it's so a single reason to to research in machine learning is really fun

1319
01:21:48,990 --> 01:21:52,970
but some like it also can be very hard and i think it's something which

1320
01:21:52,970 --> 01:21:54,470
might be

1321
01:21:54,510 --> 01:21:59,010
what people already said like the pressure to publish papers and thank you for that

1322
01:21:59,010 --> 01:22:03,860
you have like two years contract for the next ten years of life for the

1323
01:22:03,880 --> 01:22:06,640
and there's something which

1324
01:22:06,660 --> 01:22:08,680
i mean you need to know that before i mean if you want to go

1325
01:22:08,860 --> 01:22:12,010
research in machine learning about this future

1326
01:22:12,010 --> 01:22:14,430
you need to be prepared to

1327
01:22:14,450 --> 01:22:15,430
to be able to

1328
01:22:15,510 --> 01:22:17,450
just very often to to

1329
01:22:17,530 --> 01:22:19,220
well under pressure

1330
01:22:19,240 --> 01:22:21,410
that's nonsense

1331
01:22:21,430 --> 01:22:24,280
there were times when it was something like

1332
01:22:28,970 --> 01:22:36,340
and what you saying that

1333
01:22:36,360 --> 01:22:38,900
very different here

1334
01:22:39,740 --> 01:22:42,180
think that we're talking all

1335
01:22:43,400 --> 01:22:46,220
what would be the frontier between

1336
01:22:46,240 --> 01:22:48,450
machine learning and engineering

1337
01:22:50,010 --> 01:22:53,950
it's a question i don't know exactly where to put it

1338
01:22:53,970 --> 01:22:55,360
is it

1339
01:22:55,380 --> 01:22:57,610
you're doing machine learning

1340
01:22:57,660 --> 01:23:02,490
finding application with is visiting it's also machine learning that it's going

1341
01:23:02,510 --> 01:23:06,410
what's more that analyses which is the engineer john

1342
01:23:06,480 --> 01:23:11,240
what is the the limit between being a researcher and being in charge

1343
01:23:11,260 --> 01:23:16,590
these questions and then it's interesting

1344
01:23:16,680 --> 01:23:19,970
this is what you meant

1345
01:23:22,240 --> 01:23:25,140
you know

1346
01:23:25,360 --> 01:23:27,990
so what you need

1347
01:23:28,200 --> 01:23:33,630
i might have so

1348
01:23:33,820 --> 01:23:38,490
i mean that's exciting thing of machine learning that the people for a few you

1349
01:23:38,490 --> 01:23:40,260
were only there

1350
01:23:40,260 --> 01:23:43,610
implementing or making happen i get something about how

1351
01:23:43,630 --> 01:23:45,220
things were not so

1352
01:23:45,240 --> 01:23:47,470
we have

1353
01:23:47,470 --> 01:23:50,320
you know people have titles as in

1354
01:23:50,320 --> 01:23:55,200
or you see

1355
01:23:55,280 --> 01:23:58,260
that's why

1356
01:23:58,300 --> 01:24:01,610
one of the one of the sort of

1357
01:24:01,640 --> 01:24:02,590
tags can

1358
01:24:02,610 --> 01:24:05,010
how on your teacher is

1359
01:24:05,030 --> 01:24:07,590
research software development engineer

1360
01:24:07,610 --> 01:24:09,470
or are st

1361
01:24:09,490 --> 01:24:16,220
so our is the is an engineers are mentioning and he works together with researchers

1362
01:24:16,220 --> 01:24:19,160
but it he or she doesn't really developed

1363
01:24:19,200 --> 01:24:26,110
new algorithms all that much more sort tries to understand the behaviour of the family

1364
01:24:26,110 --> 01:24:29,910
about was very thing that is the person that's really really good at coding

1365
01:24:29,950 --> 01:24:32,340
and it has enough understanding of machine learning

1366
01:24:32,360 --> 01:24:37,490
just enough to be able to talk to machine learning research to just implement the

1367
01:24:38,410 --> 01:24:41,950
although we have these guys going around the useful

1368
01:24:41,970 --> 01:24:45,430
i really think that as someone who works in machine learning you must be able

1369
01:24:45,430 --> 01:24:48,410
to some to some degree you must be able to do a bit of engineering

1370
01:24:49,880 --> 01:24:53,800
there might be exceptions might people who were very much as well it was saying

1371
01:24:53,800 --> 01:24:58,640
on the more mathematical form there is sort of the of border of of

1372
01:24:58,700 --> 01:25:03,680
of machine learning there almost that's pure mathematics everything that

1373
01:25:03,700 --> 01:25:07,930
in the way a machine learning is something of a patchwork of

1374
01:25:07,950 --> 01:25:12,550
although of different things and some people might have more engineering science and much more

1375
01:25:13,490 --> 01:25:14,610
definitely bit

1376
01:25:14,720 --> 01:25:17,300
there exist

1377
01:25:17,320 --> 01:25:19,200
something about the work

1378
01:25:19,220 --> 01:25:24,140
because of the because it's interesting it's there are some similar work

1379
01:25:26,010 --> 01:25:32,200
i wish to point where i decided that i never called

1380
01:25:32,220 --> 01:25:34,030
my customers

1381
01:25:34,050 --> 01:25:37,160
because i'm not a programmer

1382
01:25:37,180 --> 01:25:39,410
in my skills program

1383
01:25:39,510 --> 01:25:42,280
the next morning solve the problems

1384
01:25:42,300 --> 01:25:45,260
and so i consider myself very much you

1385
01:25:45,280 --> 01:25:47,800
but not sovereignty

1386
01:25:48,800 --> 01:25:53,780
when it comes to new problem because because

1387
01:25:55,220 --> 01:26:00,260
with experimental design because they everything i before

1388
01:26:00,260 --> 01:26:02,400
you know

1389
01:26:03,020 --> 01:26:12,220
if you have a choice between hiring a couple of experts collecting data states going

1390
01:26:12,240 --> 01:26:12,910
to cost two

1391
01:26:13,320 --> 01:26:23,550
four thousand dollars thousand euros per year for machine learning because they say

1392
01:26:23,610 --> 01:26:28,840
and it's hard to describe the money that much money

1393
01:26:28,840 --> 01:26:30,300
you could probably

1394
01:26:30,300 --> 01:26:33,010
connect home data

1395
01:26:33,030 --> 01:26:35,880
go from the data

1396
01:26:35,930 --> 01:26:38,320
more than ten years

1397
01:26:38,340 --> 01:26:42,280
but this incorrect and should know

1398
01:26:42,300 --> 01:26:44,680
how to quickly update

1399
01:26:44,700 --> 01:26:47,450
i can use of interest

1400
01:26:50,720 --> 01:26:59,860
design not that expert experiment design because the statistics is covered with two experimental design

1401
01:26:59,990 --> 01:27:05,180
extensive however if you find a statistic books from

1402
01:27:05,200 --> 01:27:06,130
the only

1403
01:27:06,130 --> 01:27:08,990
two of these n experiments in practice

1404
01:27:09,050 --> 01:27:12,400
in practice so many constraints

1405
01:27:12,410 --> 01:27:16,630
that the usual experimental plants that you

1406
01:27:16,700 --> 01:27:17,720
don't apply

1407
01:27:17,760 --> 01:27:22,610
so there's a lot of engineering that goes into this

1408
01:27:22,630 --> 01:27:24,720
not all of these

1409
01:27:24,740 --> 01:27:27,840
actually most of know

1410
01:27:27,840 --> 01:27:29,460
it helps to have a map

1411
01:27:29,540 --> 01:27:34,080
you have to draw the map you need to know where you are

1412
01:27:34,080 --> 01:27:43,090
typically the way we have a model that is used to represent this

1413
01:27:43,140 --> 01:27:46,400
is something that

1414
01:27:46,410 --> 01:27:50,080
and i'm going to assume that the maps then map such

1415
01:27:50,130 --> 01:27:54,040
but often that's not complicates things

1416
01:27:54,640 --> 01:27:56,680
you might have

1417
01:27:56,710 --> 01:27:59,890
the map in this case is the variable try to to learn and there's different

1418
01:27:59,890 --> 01:28:02,470
parameterizations from the grid

1419
01:28:02,520 --> 01:28:04,720
could be based on landmarks

1420
01:28:04,790 --> 01:28:05,820
you know

1421
01:28:05,830 --> 01:28:07,010
you remember that

1422
01:28:08,030 --> 01:28:09,760
three of the cell phone

1423
01:28:10,590 --> 01:28:13,070
kangaroo corner and so on and

1424
01:28:13,130 --> 01:28:17,760
so i think the features you learn to navigate

1425
01:28:17,770 --> 01:28:20,950
there is also the location

1426
01:28:21,030 --> 01:28:23,470
your location which is a in x

1427
01:28:25,950 --> 01:28:28,200
three d there

1428
01:28:28,260 --> 01:28:31,890
and from that you get some observations

1429
01:28:31,900 --> 01:28:34,760
and then the next time step

1430
01:28:34,770 --> 01:28:38,070
my changes location changes

1431
01:28:38,080 --> 01:28:41,640
and you get another rash

1432
01:28:41,700 --> 01:28:42,940
and these guys

1433
01:28:43,010 --> 01:28:46,410
you location depends on where you are in the math

1434
01:28:46,460 --> 01:28:48,870
the observation that you get depends

1435
01:28:48,880 --> 01:28:51,270
on your location and the map

1436
01:28:51,290 --> 01:28:52,770
and so on

1437
01:28:52,770 --> 01:28:59,500
so you get this type of system where you have to bear

1438
01:29:01,690 --> 01:29:04,680
and we're going to look at the efficient ways of solving these

1439
01:29:04,900 --> 01:29:07,740
first i'm going to give you a demo work

1440
01:29:07,770 --> 01:29:09,470
what happens in practice

1441
01:29:09,480 --> 01:29:17,110
these things

1442
01:29:17,150 --> 01:29:20,890
so here is an example

1443
01:29:21,850 --> 01:29:23,520
what i'm showing you here

1444
01:29:28,940 --> 01:29:32,820
a robot guy over there

1445
01:29:34,860 --> 01:29:41,320
the green so the map is this thing this room with dark indicates the worldwide

1446
01:29:41,320 --> 01:29:42,980
is empty space

1447
01:29:43,320 --> 01:29:47,790
the green indicates observations so this guy sort has

1448
01:29:47,850 --> 01:29:51,870
so said to be get people back knows with the wall is

1449
01:29:51,910 --> 01:29:53,750
with respect to itself

1450
01:29:53,760 --> 01:29:56,790
and this observation usually noisy

1451
01:29:57,620 --> 01:29:58,960
the blue

1452
01:29:58,980 --> 01:30:02,560
indicates the probability of where

1453
01:30:02,620 --> 01:30:04,070
these two things

1454
01:30:04,090 --> 01:30:05,850
he is right now

1455
01:30:07,510 --> 01:30:09,640
in particular

1456
01:30:09,650 --> 01:30:11,290
when we start

1457
01:30:12,280 --> 01:30:15,070
it has no easily taking a few measurements

1458
01:30:15,130 --> 01:30:18,070
he believes it could be in any of these cells

1459
01:30:18,100 --> 01:30:19,820
OK so the map is

1460
01:30:21,780 --> 01:30:24,120
now this type of distribution

1461
01:30:24,240 --> 01:30:32,510
i mean the main point here being that if you have the samples

1462
01:30:32,520 --> 01:30:36,250
by having the samples you can truly account for multimodality

1463
01:30:36,270 --> 01:30:37,110
and four

1464
01:30:37,120 --> 01:30:39,630
all these problems that arise in practice

1465
01:30:40,030 --> 01:30:41,740
mainly due to symmetry

1466
01:30:41,820 --> 01:30:43,690
but when you get here

1467
01:30:43,710 --> 01:30:46,940
it's already got a pretty good estimate of twenty years

1468
01:30:47,020 --> 01:30:51,840
but unfortunately because there's this symmetry this daughters looks like this the or

1469
01:30:51,850 --> 01:30:53,820
you could be here or here

1470
01:30:53,970 --> 01:30:56,870
because of the nature of the world

1471
01:30:56,910 --> 01:31:01,770
the distribution of the u i truly by model

1472
01:31:01,770 --> 01:31:05,780
and some of the comments like one these sort of things happen when you have

1473
01:31:06,440 --> 01:31:11,490
but the model distributions and obviously just using gauss and would not be at the

1474
01:31:23,270 --> 01:31:29,940
he doesn't know that he started off in the left corner doesn't always start

1475
01:31:31,860 --> 01:31:46,040
but he doesn't know that

1476
01:31:47,350 --> 01:31:48,750
i think about it

1477
01:31:48,810 --> 01:31:50,550
that in this case

1478
01:31:50,600 --> 01:31:54,290
the guy has really not in this market doesn't always start

1479
01:31:54,350 --> 01:31:55,930
everything is relative

1480
01:31:59,610 --> 01:32:03,160
in this case actually in this case it does not the maps sorry no you're

1481
01:32:03,160 --> 01:32:04,210
right action

1482
01:32:04,270 --> 01:32:10,460
in the general case it doesn't matter that that would be the case

1483
01:32:12,270 --> 01:32:16,690
as things progress

1484
01:32:16,710 --> 01:32:17,530
while i

1485
01:32:17,540 --> 01:32:19,150
the learned

1486
01:32:28,140 --> 01:32:32,070
and this is of course projects of my students that's why

1487
01:32:32,090 --> 01:32:36,410
robertson looked like it had to do this with a week

1488
01:32:36,660 --> 01:32:39,970
here's another one that basically no learning the

1489
01:32:40,010 --> 01:32:46,910
as the navigates

1490
01:32:47,150 --> 01:32:50,790
and then there is the real robot assists

1491
01:32:50,800 --> 01:32:52,100
the first stuff

1492
01:32:52,110 --> 01:33:00,230
and make it look a bit more impressive than with my students

1493
01:33:00,240 --> 01:33:15,400
this is courtesy of the passenger and stanford

1494
01:33:15,590 --> 01:33:23,640
actually that's how much of this problem as opposed to just showing some

1495
01:33:24,660 --> 01:33:26,640
i mean the observations

1496
01:33:26,730 --> 01:33:33,240
she should the demonstration of articles and so

1497
01:33:33,250 --> 01:33:37,570
but in those are the kind of capabilities that is going to happen place

1498
01:33:37,600 --> 01:33:42,390
when they actually do

1499
01:33:43,230 --> 01:33:45,800
so we started with this yesterday

1500
01:33:46,650 --> 01:33:48,620
we have samples x

1501
01:33:49,070 --> 01:33:52,260
these locations and i'm doing the one d case

1502
01:33:52,560 --> 01:33:53,810
in all

1503
01:33:53,860 --> 01:33:55,790
of service

1504
01:33:55,890 --> 01:34:09,910
we then are going to propagate the samples that i'm ecosystem

1505
01:34:11,400 --> 01:34:13,630
we start with the sample

1506
01:34:13,640 --> 01:34:15,880
x y

1507
01:34:15,880 --> 01:34:17,460
i produce

1508
01:34:17,470 --> 01:34:20,450
one thousand

1509
01:34:26,530 --> 01:34:30,200
you will be blocked

1510
01:34:30,220 --> 01:34:32,150
but it wasn't until i

1511
01:34:32,160 --> 01:34:36,930
a lot because we that

1512
01:34:36,950 --> 01:34:37,920
and by

1513
01:34:39,750 --> 01:34:41,500
which includes

1514
01:34:41,520 --> 01:34:45,850
if i have some more in addition to that because there is no violence

1515
01:34:45,870 --> 01:34:48,210
it would grow

1516
01:34:48,220 --> 01:34:51,240
for a given t

1517
01:35:00,040 --> 01:35:02,660
all right because she

1518
01:35:02,670 --> 01:35:08,740
simply because she but just to show you another effect of the infinite divine i

1519
01:35:08,740 --> 01:35:10,300
told you that you can

1520
01:35:10,320 --> 01:35:13,250
simulate cushy using and all

1521
01:35:13,260 --> 01:35:18,860
but i accept reject we can pretend to do it by importance sampling again you

1522
01:35:18,870 --> 01:35:20,990
so from a normal zero one

1523
01:35:21,000 --> 01:35:22,990
and use the weight

1524
01:35:23,040 --> 01:35:27,180
off because she did you take their it shows the distribution of the cushy of

1525
01:35:27,220 --> 01:35:28,560
decision normal

1526
01:35:28,570 --> 01:35:32,770
and the weight will be the same shape that an exponential

1527
01:35:32,780 --> 01:35:34,920
although one suspects square

1528
01:35:34,940 --> 01:35:37,670
OK and because of the exponential

1529
01:35:37,680 --> 01:35:39,740
the expectation of the square

1530
01:35:40,300 --> 01:35:42,230
weight is infinite

1531
01:35:42,250 --> 01:35:45,880
and if you have infinite violence is tumor

1532
01:35:45,890 --> 01:35:48,410
and i run another experiment

1533
01:35:48,430 --> 01:35:54,560
for estimating the expectation of the exponential sex

1534
01:35:54,580 --> 01:35:56,230
because she

1535
01:35:59,980 --> 01:36:01,680
ten thousand

1536
01:36:01,700 --> 01:36:08,530
to ten thousand normal simulations zero one and you can see

1537
01:36:08,850 --> 01:36:11,210
o five application days

1538
01:36:11,230 --> 01:36:12,310
not at all

1539
01:36:12,320 --> 01:36:15,360
this nice central limit theorems

1540
01:36:15,390 --> 01:36:20,130
decrees of the range but you go along iterations

1541
01:36:20,150 --> 01:36:21,540
no more

1542
01:36:21,550 --> 01:36:25,090
OK this is a very large weights so the range

1543
01:36:25,100 --> 01:36:26,940
he just exploded

1544
01:36:26,990 --> 01:36:34,050
i got a very large weight of about forty four thousand eight hundred iterations

1545
01:36:34,090 --> 01:36:38,590
and and this is where it is so huge that it just doesn't diminish the

1546
01:36:38,590 --> 01:36:40,140
range of to

1547
01:36:40,150 --> 01:36:42,650
and this kind of totally

1548
01:36:44,080 --> 01:36:45,140
we have your

1549
01:36:45,150 --> 01:36:46,770
it typical

1550
01:36:46,800 --> 01:36:48,290
of infinite violence

1551
01:36:48,810 --> 01:36:52,070
this two meters

1552
01:36:55,130 --> 01:36:58,910
the connection with the project which is that if you don't

1553
01:37:01,160 --> 01:37:03,040
four which have g

1554
01:37:03,070 --> 01:37:04,310
is bounded

1555
01:37:04,320 --> 01:37:08,480
they may be problems and if you use it for enough

1556
01:37:08,500 --> 01:37:11,840
functions h then there will be problems

1557
01:37:11,850 --> 01:37:16,090
now let's see what the point of using it was something when you could do

1558
01:37:16,160 --> 01:37:20,440
instead accept project and get real simulations not

1559
01:37:20,450 --> 01:37:23,440
simulation from the function one the point that

1560
01:37:23,460 --> 01:37:25,840
you may have f of g which is bounded

1561
01:37:25,860 --> 01:37:27,680
but the bound may be so

1562
01:37:27,690 --> 01:37:31,190
that in terms of simulation

1563
01:37:31,200 --> 01:37:35,350
it's not a practical because if if m

1564
01:37:35,380 --> 01:37:40,570
is ten thousand it means that you have to run ten thousand iterations simulations to

1565
01:37:40,570 --> 01:37:41,610
produce one

1566
01:37:41,620 --> 01:37:44,550
o point for your target distribution

1567
01:37:44,560 --> 01:37:50,870
so in terms of of recycling of simulations images bounded may not imply

1568
01:37:50,880 --> 01:37:53,430
that exit project is the right choice

1569
01:37:54,630 --> 01:37:58,190
now is the best choice of dual there is

1570
01:37:59,280 --> 01:38:05,000
optimality theory which is the following one that you can find the best possible chance

1571
01:38:05,000 --> 01:38:06,520
of g

1572
01:38:06,530 --> 01:38:07,540
in principle

1573
01:38:07,550 --> 01:38:08,790
it is actually

1574
01:38:08,800 --> 01:38:13,460
the function g stuff that is proportional to absolute value of h

1575
01:38:13,470 --> 01:38:19,860
times have any pages positive this gives you a zero balance this tumor

1576
01:38:22,140 --> 01:38:24,600
so just use one simulations

1577
01:38:25,530 --> 01:38:28,520
you get the right i no violence

1578
01:38:28,530 --> 01:38:29,550
of course the

1579
01:38:29,680 --> 01:38:34,330
fall because simulating from this function

1580
01:38:34,380 --> 01:38:35,640
is usually to totally

1581
01:38:36,930 --> 01:38:38,570
in fact because

1582
01:38:38,590 --> 01:38:43,940
remark that if h is positive to see from this function and use it in

1583
01:38:43,950 --> 01:38:46,910
the in the importance weight you need to know

1584
01:38:46,940 --> 01:38:49,940
the integral you're trying to approximate

1585
01:38:49,960 --> 01:38:53,170
OK so it's like a vicious circle

1586
01:38:53,190 --> 01:38:54,520
issues useless

1587
01:38:54,560 --> 01:39:01,390
you can try to make this result to produce better results than do that

1588
01:39:01,400 --> 01:39:03,670
to choose the right j

1589
01:39:03,690 --> 01:39:08,010
you should be close to age the particularly one example

1590
01:39:09,310 --> 01:39:13,890
of application of this is is that if h is zero

1591
01:39:13,900 --> 01:39:18,820
do you think should be zero so if you are trying to approximate integral over

1592
01:39:18,840 --> 01:39:23,620
this region is useless less to simulate outside this region

1593
01:39:23,640 --> 01:39:27,310
like if you want to approximate interval between two and three

1594
01:39:27,330 --> 01:39:31,980
if you knew the distribution which have the support larger than the interval

1595
01:39:31,990 --> 01:39:33,020
two three

1596
01:39:36,270 --> 01:39:41,840
one one one you've saying that there is another thing that again

1597
01:39:41,860 --> 01:39:45,040
we facing the problem of the normalizing constant

1598
01:39:45,060 --> 01:39:46,580
we're missing the right

1599
01:39:46,590 --> 01:39:48,230
constant either

1600
01:39:48,230 --> 01:39:53,930
in an orangey for trained to mimic the optimal choice of g

1601
01:39:53,950 --> 01:39:58,000
but we can get out of that very quickly but

1602
01:39:58,020 --> 01:39:59,790
approximating the

1603
01:39:59,800 --> 01:40:02,600
the numerator and the denominator integral

1604
01:40:02,600 --> 01:40:09,650
well there's is the senior professors and they behave one way and junior professors are

1605
01:40:09,740 --> 01:40:14,180
and the graduate students or something like that and that choice of where you do

1606
01:40:14,180 --> 01:40:18,130
that is important

1607
01:40:18,170 --> 01:40:20,060
OK so

1608
01:40:20,100 --> 01:40:23,880
the next thing we want to talk about his inference imperial arms and inference in

1609
01:40:25,710 --> 01:40:27,250
one thing you can do is

1610
01:40:27,260 --> 01:40:30,050
just like with the rule base case

1611
01:40:30,660 --> 01:40:34,060
instantiate the business and then

1612
01:40:36,020 --> 01:40:38,580
the reasoning over it so

1613
01:40:38,590 --> 01:40:41,380
this is an example i'm going to use

1614
01:40:42,600 --> 01:40:45,490
i have

1615
01:40:45,540 --> 01:40:52,770
simple skeleton where one of their two papers for reviewers and i'm interested in whether

1616
01:40:52,770 --> 01:40:53,650
or not

1617
01:40:53,660 --> 01:40:58,770
a one is a good writer given that i find out that two other papers

1618
01:40:58,800 --> 01:41:01,200
have been accepted

1619
01:41:03,260 --> 01:41:07,900
i get this big business and i do inference and such

1620
01:41:09,500 --> 01:41:12,450
hopefully we can do something a little bit better than that

1621
01:41:17,060 --> 01:41:20,190
what we want to do is we want to

1622
01:41:20,240 --> 01:41:24,100
make use of the object structure hopefully that's going to help be helpful is not

1623
01:41:24,100 --> 01:41:25,660
guaranteed to be helpful but

1624
01:41:26,050 --> 01:41:28,390
ideally it will be and

1625
01:41:28,410 --> 01:41:32,580
use that to reason about encapsulation

1626
01:41:32,790 --> 01:41:38,170
the probabilistic dependencies and then do some sort of caching so that you can we

1627
01:41:38,170 --> 01:41:40,930
use computations wherever possible

1628
01:41:41,060 --> 01:41:44,920
and that's the idea behind

1629
01:41:44,930 --> 01:41:50,690
obviously first work on structured variable elimination gonna go through kind of an example of

1630
01:41:53,280 --> 01:41:54,550
first of

1631
01:41:54,580 --> 01:41:57,800
we're going to be defined for any

1632
01:41:57,850 --> 01:42:01,040
object the set of variables for ten to the

1633
01:42:01,050 --> 01:42:05,040
the object so if i look at for example reviewer two

1634
01:42:05,170 --> 01:42:06,520
OK there's

1635
01:42:06,530 --> 01:42:09,560
the attributes of

1636
01:42:09,620 --> 01:42:13,270
that object itself but then will also

1637
01:42:13,310 --> 01:42:15,170
i worry about that

1638
01:42:15,670 --> 01:42:17,890
things that it depends on

1639
01:42:18,690 --> 01:42:20,820
it depends on

1640
01:42:20,830 --> 01:42:26,660
good writer attributed author one and

1641
01:42:26,670 --> 01:42:28,050
the interface

1642
01:42:28,050 --> 01:42:30,420
two and j is going to be

1643
01:42:30,460 --> 01:42:32,030
the set

1644
01:42:33,910 --> 01:42:36,080
attributes those are attributes

1645
01:42:37,450 --> 01:42:44,420
the stuff in that object depends on and exported attributes the stuff that other things

1646
01:42:44,420 --> 01:42:49,510
depend on so for any object can go through and define OK these are the

1647
01:42:49,510 --> 01:42:57,640
subset of those attributes that are either imported or exported

1648
01:42:57,650 --> 01:43:01,180
and we're going to use this to

1649
01:43:01,310 --> 01:43:05,890
keep track

1650
01:43:05,900 --> 01:43:09,420
what is encapsulated

1651
01:43:09,450 --> 01:43:10,540
with and

1652
01:43:10,550 --> 01:43:12,000
and object so

1653
01:43:12,010 --> 01:43:13,140
if i look

1654
01:43:15,150 --> 01:43:17,080
interface for

1655
01:43:17,090 --> 01:43:19,030
p one

1656
01:43:19,990 --> 01:43:21,380
this is

1657
01:43:21,420 --> 01:43:22,910
the interface

1658
01:43:26,670 --> 01:43:29,590
objects are encapsulated in p one

1659
01:43:29,630 --> 01:43:30,830
if there is no

1660
01:43:33,690 --> 01:43:35,920
kind outside

1661
01:43:36,940 --> 01:43:39,250
except through this interface

1662
01:43:40,020 --> 01:43:41,170
in a lot of ways

1663
01:43:41,190 --> 01:43:46,300
you know if you're used to cutset conditioning and bayes nets it's a form of

1664
01:43:46,490 --> 01:43:52,480
finding useful cuts in the bayes net that kind of

1665
01:43:52,530 --> 01:43:58,110
can be used to factor in the bayes net during inference

1666
01:43:59,970 --> 01:44:03,140
we're going to

1667
01:44:03,200 --> 01:44:08,000
use encapsulation together with three use to do inference efficiently

1668
01:44:09,050 --> 01:44:10,730
the idea here

1669
01:44:11,830 --> 01:44:13,540
we started off

1670
01:44:13,580 --> 01:44:14,800
and we know

1671
01:44:14,820 --> 01:44:17,460
we're interested in a one

1672
01:44:17,460 --> 01:44:18,860
being a good writer

1673
01:44:19,850 --> 01:44:23,450
right now we have these two

1674
01:44:23,490 --> 01:44:26,100
objects that are encapsulated

1675
01:44:26,730 --> 01:44:27,980
a one

1676
01:44:28,020 --> 01:44:32,350
to do inference all have two

1677
01:44:32,380 --> 01:44:35,730
focus on one of these and

1678
01:44:35,790 --> 01:44:38,610
i'll go ahead and look now

1679
01:44:38,880 --> 01:44:40,180
paper one

1680
01:44:40,220 --> 01:44:43,040
i'm trying to

1681
01:44:43,050 --> 01:44:44,710
get out

1682
01:44:44,730 --> 01:44:48,750
distribution over the cuts

1683
01:44:48,760 --> 01:44:53,280
and so still all the blue stuff i have

1684
01:44:53,290 --> 01:44:56,460
i figured out yet so i'll go and and

1685
01:44:56,470 --> 01:44:58,340
compute say

1686
01:44:58,400 --> 01:45:02,620
the information i need for inference for review two

1687
01:45:02,640 --> 01:45:05,730
so reviewer two

1688
01:45:05,770 --> 01:45:08,260
this again is interface

1689
01:45:09,900 --> 01:45:11,200
at this point

1690
01:45:11,240 --> 01:45:13,270
everything's groundouts

1691
01:45:14,300 --> 01:45:16,590
i can

1692
01:45:16,610 --> 01:45:18,590
perform inference two

1693
01:45:18,630 --> 01:45:21,140
get a distribution over

1694
01:45:21,150 --> 01:45:22,260
good writer

1695
01:45:24,200 --> 01:45:27,070
so i do that

1696
01:45:27,130 --> 01:45:30,320
and now i come back to the place i was

1697
01:45:30,360 --> 01:45:34,110
i have my distribution over the interface here

1698
01:45:34,160 --> 01:45:36,270
and now i need to go here

1699
01:45:36,270 --> 01:45:37,450
and do that

1700
01:45:37,470 --> 01:45:38,620
in fact

1701
01:45:38,680 --> 01:45:41,260
when i go

1702
01:45:43,360 --> 01:45:46,850
do the appropriate marginalization this here

1703
01:45:46,900 --> 01:45:49,080
now i can go here

1704
01:45:49,130 --> 01:45:53,240
now at this point it turns out the review one

1705
01:45:53,280 --> 01:45:55,470
looks exactly the same as

1706
01:45:55,480 --> 01:45:57,140
future did

1707
01:45:57,200 --> 01:45:59,710
i need to compute exactly

1708
01:45:59,720 --> 01:46:01,310
the same

1709
01:46:01,330 --> 01:46:03,120
distribution so

1710
01:46:03,130 --> 01:46:06,750
i don't have to actually do inference here i can just take the cash result

1711
01:46:06,990 --> 01:46:08,270
that i have here

1712
01:46:08,310 --> 01:46:13,070
and inserted here

1713
01:46:14,990 --> 01:46:17,310
at this point

1714
01:46:17,340 --> 01:46:20,820
i go through and do inference i have to add in

1715
01:46:20,910 --> 01:46:23,180
the fact that i have the evidence

1716
01:46:23,180 --> 01:46:24,490
that p

1717
01:46:24,500 --> 01:46:28,580
one was accepted

1718
01:46:29,490 --> 01:46:30,750
i can get

1719
01:46:31,460 --> 01:46:33,550
distribution of the interface for

1720
01:46:33,570 --> 01:46:34,550
paper one

1721
01:46:34,550 --> 01:46:37,970
and i inserted back into the bigger

1722
01:46:38,020 --> 01:46:41,520
friends that i was dealing

1723
01:46:42,320 --> 01:46:46,590
at this point

1724
01:46:46,640 --> 01:46:47,890
i can go to

1725
01:46:47,910 --> 01:46:49,230
paper p two

1726
01:46:49,230 --> 01:46:51,100
paper p two

1727
01:46:53,190 --> 01:46:56,550
the same structure as paper p one had

1728
01:46:56,600 --> 01:46:58,970
has the same evidence so

1729
01:46:58,970 --> 01:47:01,550
i can just reuse that computations

1730
01:47:01,560 --> 01:47:04,110
and then finally compute

1731
01:47:04,120 --> 01:47:08,350
distribution over a one being a good writer

1732
01:47:08,400 --> 01:47:12,070
so that's the idea that i first stuff

1733
01:47:12,110 --> 01:47:14,100
reason about what

1734
01:47:15,650 --> 01:47:21,180
interfaces between sets of objects then i have a good way of cashing in looking

1735
01:47:21,180 --> 01:47:25,700
at any time i need to compute the distribution over the that interface i look

1736
01:47:25,700 --> 01:47:26,390
and see

1737
01:47:26,400 --> 01:47:31,680
there are a computer that and i don't have to do it again and this

1738
01:47:33,330 --> 01:47:35,480
the same

1739
01:47:35,520 --> 01:47:42,190
a lot of work in doing inference and partly it's because

1740
01:47:42,190 --> 01:47:49,920
and all the

1741
01:48:34,980 --> 01:48:40,970
he played

1742
01:48:47,800 --> 01:48:58,770
and he

1743
01:49:21,990 --> 01:49:24,750
you know

1744
01:49:54,350 --> 01:49:58,790
are the whole

1745
01:50:08,010 --> 01:50:12,760
and you can

1746
01:50:29,630 --> 01:50:38,100
can improve results

1747
01:50:41,440 --> 01:50:44,370
he might be

1748
01:50:44,470 --> 01:50:46,350
all of information

1749
01:50:51,130 --> 01:50:53,880
right well i should know

1750
01:50:54,530 --> 01:50:56,440
we want to the

1751
01:51:12,630 --> 01:51:15,790
thank you

1752
01:51:17,260 --> 01:51:23,170
so all

1753
01:51:35,010 --> 01:51:40,120
the station is one

1754
01:51:51,840 --> 01:51:56,280
well i

1755
01:52:00,200 --> 01:52:05,070
you know why

1756
01:52:13,980 --> 01:52:16,500
the people

1757
01:52:24,820 --> 01:52:27,800
and if

1758
01:52:33,420 --> 01:52:34,380
you should

1759
01:52:35,280 --> 01:52:39,330
he was

1760
01:53:03,460 --> 01:53:11,210
it is

1761
01:53:12,810 --> 01:53:15,510
unification based on

1762
01:53:17,490 --> 01:53:20,030
in this case

1763
01:53:20,060 --> 01:53:22,670
instead of money using

1764
01:53:24,750 --> 01:53:26,380
almost all of

1765
01:53:29,550 --> 01:53:40,750
the next year or

1766
01:54:02,010 --> 01:54:03,770
the proof

1767
01:54:25,880 --> 01:54:31,140
o c

1768
01:54:31,940 --> 01:54:38,180
it was part of the

1769
01:54:38,190 --> 01:54:42,610
OK so

1770
01:54:42,620 --> 01:54:44,560
thank you

1771
01:55:00,450 --> 01:55:02,360
and none

1772
01:55:15,930 --> 01:55:17,810
may be

1773
01:55:32,670 --> 01:55:39,810
i mean i think

1774
01:55:55,020 --> 01:55:58,880
the result of the

1775
01:55:58,880 --> 01:56:00,730
you can't and i use every day

1776
01:56:00,740 --> 01:56:05,030
and this how we make lots of money in his picture might of my because

1777
01:56:05,030 --> 01:56:09,670
on so my friends he said he said well this great graphic in machine learning

1778
01:56:09,670 --> 01:56:13,770
so is actually useful so still was you know you know is that which is

1779
01:56:13,780 --> 01:56:15,160
the pressure was that the

1780
01:56:15,560 --> 01:56:21,280
the PCA was the bayesian networks what the level so helpful and since it was

1781
01:56:21,280 --> 01:56:23,150
then that that

1782
01:56:25,930 --> 01:56:31,220
for those of you that i don't know matlab i hope to do that it

1783
01:56:31,250 --> 01:56:36,380
is not hard on and what actually have a short matlab tutorial in one of

1784
01:56:36,390 --> 01:56:41,320
the discussion sections on for those who don't know

1785
01:56:43,450 --> 01:56:51,360
the very last piece of logistical thing is the discussion sections so discussion sections will

1786
01:56:51,360 --> 01:56:57,460
be taught by the TAS and attends the discussion sections is optional although that there

1787
01:56:57,460 --> 01:56:59,990
also be recorded and televised

1788
01:57:00,870 --> 01:57:04,690
and we use the discussion section is mainly the two things on for the next

1789
01:57:04,690 --> 01:57:10,280
two or three weeks we use the discussion section sick over the prerequisites on today's

1790
01:57:10,280 --> 01:57:15,810
castle some we haven't seen the probability or statistics for a while then the algebra

1791
01:57:15,830 --> 01:57:20,530
o look over those in the discussion sections as a refresher for those who don't

1792
01:57:20,530 --> 01:57:25,640
want one of the ten this quarter also use the discussion section has to go

1793
01:57:25,640 --> 01:57:31,160
over extensions to the material that i'm teaching the main lectures so machine learning is

1794
01:57:31,160 --> 01:57:35,080
a huge field under a few extensions that we really want to teach but didn't

1795
01:57:35,080 --> 01:57:38,890
have time in the mean that just for on so they did this quarter we

1796
01:57:38,890 --> 01:57:43,540
use the discussion section to talk about things like convex optimization on to talk about

1797
01:57:43,540 --> 01:57:47,620
the about hidden markov models which is a type of machine learning algorithm for modelling

1798
01:57:47,630 --> 01:57:51,900
time series in the field of things of extensions the materials that i'll be covering

1799
01:57:51,900 --> 01:57:57,760
in the main lectures on and attendance at that discussion sections option

1800
01:58:00,550 --> 01:58:03,130
that was all i had on

1801
01:58:03,130 --> 01:58:05,620
from the just states on

1802
01:58:05,710 --> 01:58:09,420
the final move on to start talking about about machine learning let me check what

1803
01:58:09,420 --> 01:58:14,040
questions you

1804
01:58:32,320 --> 01:58:37,530
so also has been that you welcome to use are but on i would strongly

1805
01:58:37,530 --> 01:58:41,760
advise against the mean because in the last problems which is apply some code that

1806
01:58:41,760 --> 01:58:46,640
will run in matlab octave but that would be somewhat difficult three to translate that

1807
01:58:46,640 --> 01:58:49,480
there will be some people who transition to our yourself

1808
01:58:49,490 --> 01:58:54,730
so for any assignment awesomest solution are that's fine but i i think that was

1809
01:58:54,730 --> 01:58:58,910
actually tell you what learning i i i actually i know are matlab and i

1810
01:58:58,910 --> 01:59:01,080
personally and using matlab

1811
01:59:01,100 --> 01:59:05,830
quite a bit more of the variance

1812
01:59:06,650 --> 01:59:17,330
so the the temperature you welcome to do to smaller groups of three you're welcome

1813
01:59:17,350 --> 01:59:21,470
to do it by itself or in groups of two of grading is the same

1814
01:59:21,470 --> 01:59:24,690
regardless of the group size so for large you

1815
01:59:24,690 --> 01:59:29,700
sorry i i recommend trying to form a team was actually certified to do the

1816
01:59:29,730 --> 01:59:32,460
smaller group if you want

1817
01:59:43,030 --> 01:59:47,550
c programming in this class other than any that you choose to do yourself in

1818
01:59:47,550 --> 01:59:53,850
your project on so that all the whole works can be done in matlab octave

1819
01:59:54,270 --> 01:59:55,520
and on

1820
01:59:55,570 --> 02:00:02,220
c and against the current prerequisites is more the ability to understand big o notation

1821
02:00:02,690 --> 02:00:07,630
and this and knowledge of what data structure like the link research you were our

1822
02:00:07,800 --> 02:00:13,920
values on most knowledge your job specifically

1823
02:00:14,150 --> 02:00:17,900
and in

1824
02:00:17,900 --> 02:00:22,580
but what exactly the testing of the methodology

1825
02:00:22,640 --> 02:00:28,310
the problem is that later led me on in a couple of weeks actually give

1826
02:00:28,310 --> 02:00:31,250
other hand out with some of guidelines for the problem

1827
02:00:31,270 --> 02:00:35,650
but for now i was thinking the goal as being to do on a cool

1828
02:00:35,650 --> 02:00:40,490
piece of machine learning where they will that you experience the joys of machine learning

1829
02:00:40,710 --> 02:00:45,720
first-hand on and we can try to think about in a published book is work

1830
02:00:45,720 --> 02:00:49,950
on so many cities we try to build a cool machine learning applications this for

1831
02:00:49,950 --> 02:00:55,030
the most common project on substance to try to develop we try to improve state

1832
02:00:55,030 --> 02:00:59,010
of the art machine learning something to some of those projects are also very successful

1833
02:00:59,050 --> 02:01:03,640
so hard to do and there's also a smaller minority is still sometimes try to

1834
02:01:03,640 --> 02:01:09,170
prove the developed the theory of machine learning for that try to prove theorems about

1835
02:01:09,170 --> 02:01:11,030
machine learning

1836
02:01:11,080 --> 02:01:15,430
so there there are usually great project all those types of web applications in machine

1837
02:01:15,430 --> 02:01:18,600
learning being the most common

1838
02:01:18,640 --> 02:01:21,550
if not

1839
02:01:21,600 --> 02:01:24,460
the court

1840
02:01:27,350 --> 02:01:29,740
that was the logistics on

1841
02:01:29,800 --> 02:01:31,580
let's talk about

1842
02:01:31,630 --> 02:01:33,410
the album

1843
02:01:34,210 --> 02:01:38,880
so can i have the laptop display

1844
02:01:50,440 --> 02:01:54,740
actually could you know the big screen

