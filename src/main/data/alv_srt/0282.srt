1
00:00:00,000 --> 00:00:05,500
built and paranormal you set up a little just like before becoming an additional term

2
00:00:06,190 --> 00:00:09,690
they just use some massif i'd actually all boils down to the addition to the

3
00:00:09,690 --> 00:00:11,150
kernel parameter

4
00:00:11,150 --> 00:00:13,690
but if there is an exercise you today

5
00:00:13,900 --> 00:00:16,980
so selection right so

6
00:00:17,380 --> 00:00:23,520
so the regression tasks of data classification including task what's the story when you have

7
00:00:23,520 --> 00:00:29,210
a continuous valued output put OK i don't get bogged down in the details

8
00:00:29,230 --> 00:00:32,480
because i just show you the story is very similar

9
00:00:32,500 --> 00:00:35,360
now my function

10
00:00:36,130 --> 00:00:40,690
doesn't have signed function is not classified by function

11
00:00:40,710 --> 00:00:44,920
is continuous valued and is simply going to be w dot f x plus b

12
00:00:47,040 --> 00:00:50,790
well i'm just going to say that regression

13
00:00:51,460 --> 00:00:57,420
like with the sphere you try to minimize model w squares

14
00:00:57,630 --> 00:01:02,330
did exactly for binary classification i have to tell there is now

15
00:01:02,340 --> 00:01:07,730
because i have two times the training areas depending on which side of this functional

16
00:01:08,460 --> 00:01:10,020
so if you like

17
00:01:10,040 --> 00:01:11,690
i could be

18
00:01:12,440 --> 00:01:17,110
documenting this or in commenting it sort of plaster

19
00:01:18,070 --> 00:01:19,710
all stop there

20
00:01:19,710 --> 00:01:23,810
added to this function here taking me away from the correct value i might one

21
00:01:23,810 --> 00:01:25,130
point two here

22
00:01:25,150 --> 00:01:27,150
and that's point one output

23
00:01:27,170 --> 00:01:28,900
plus point one layer

24
00:01:28,920 --> 00:01:31,520
what might be point three

25
00:01:31,560 --> 00:01:36,210
o point four minus OK so two times areas

26
00:01:36,210 --> 00:01:37,770
and that is accounted for

27
00:01:37,770 --> 00:01:42,830
by these two types of articles slack variables very similar to the soft margin slack

28
00:01:42,830 --> 00:01:45,020
variables and use are

29
00:01:45,020 --> 00:01:47,790
now you just play through the mass OK

30
00:01:47,840 --> 00:01:54,810
and what it all boils down to is i have a function which must minimize

31
00:01:54,810 --> 00:02:00,560
which looks like the following this is very similar to the maximize the margin term

32
00:02:00,570 --> 00:02:06,270
this is the to do the errors and system minimizing i want to minimize the

33
00:02:06,270 --> 00:02:11,980
errors both ways domain name and this is the trade between these two

34
00:02:13,270 --> 00:02:16,130
i just say if you start with that

35
00:02:16,130 --> 00:02:18,340
you put in the constraints you have

36
00:02:18,400 --> 00:02:21,420
which are indeed this things here these things here

37
00:02:21,440 --> 00:02:24,190
if you like if this was zero

38
00:02:24,250 --> 00:02:29,590
and this is zero then y i will be w w x plus

39
00:02:29,690 --> 00:02:31,540
active function want to learn

40
00:02:31,540 --> 00:02:35,870
and similarly with other time if it was zero and this is zero then i

41
00:02:35,870 --> 00:02:41,460
have y equals wx speaking so in fact

42
00:02:41,830 --> 00:02:47,040
if there is zero mean ECCV forcing this to equal y

43
00:02:47,040 --> 00:02:51,880
now these two terms this is our you image possible about this epsilon

44
00:02:51,900 --> 00:02:53,610
that is allow allowed

45
00:02:54,020 --> 00:02:55,540
a bit errors

46
00:02:56,380 --> 00:03:01,690
that typically got noise i don't want to have epsilon precisely zero because it only

47
00:03:01,690 --> 00:03:03,230
fit to noise OK

48
00:03:03,250 --> 00:03:07,500
so this is actually a equivalent to soft margin OK

49
00:03:07,520 --> 00:03:09,960
but i don't have an enormous

50
00:03:09,980 --> 00:03:11,020
number of errors

51
00:03:11,040 --> 00:03:12,770
so these other areas

52
00:03:12,770 --> 00:03:16,190
which i can have to take me away from my

53
00:03:16,190 --> 00:03:18,340
the function quite functional mapping

54
00:03:18,360 --> 00:03:20,270
i just want someone to

55
00:03:20,270 --> 00:03:23,690
minimize that xi and so i have to

56
00:03:23,900 --> 00:03:29,540
what all was down to is the objective function which have to do for regression

57
00:03:29,560 --> 00:03:30,770
very similar

58
00:03:30,810 --> 00:03:32,210
the the story for

59
00:03:33,440 --> 00:03:36,340
what you'll notice previously alpha

60
00:03:36,480 --> 00:03:37,830
now the square

61
00:03:37,840 --> 00:03:39,840
one now it's the same sort of thing

62
00:03:39,880 --> 00:03:45,730
linear in alpha and alpha had as a quadratic in alpha and alpha had so

63
00:03:45,730 --> 00:03:49,380
it's convex top problem the quadratic problem just like before

64
00:03:49,400 --> 00:03:50,630
one solution

65
00:03:50,650 --> 00:03:52,380
and that makes it quite nice

66
00:03:52,980 --> 00:03:57,290
one last point i should mention about this is that if you have encountered having

67
00:03:57,290 --> 00:04:02,110
to solve the single one was the most of two user following don't define this

68
00:04:02,110 --> 00:04:04,150
as alpha minus

69
00:04:04,190 --> 00:04:08,440
and this is alpha plus and do the optimisation in terms of those we define

70
00:04:08,440 --> 00:04:11,590
if it's a footballer

71
00:04:11,610 --> 00:04:17,030
and it's about that

72
00:04:17,080 --> 00:04:19,410
socrates is

73
00:04:19,410 --> 00:04:24,310
is a footballer

74
00:04:24,510 --> 00:04:33,760
as use it as in nine

75
00:04:33,770 --> 00:04:36,840
and we can infer that soccer it is

76
00:04:37,660 --> 00:04:42,040
is it like that

77
00:04:42,060 --> 00:04:43,550
and we want some

78
00:04:44,620 --> 00:04:46,770
to be able to

79
00:04:46,820 --> 00:04:49,220
support and enable us to prove

80
00:04:49,260 --> 00:04:52,290
the validity of inferences like that

81
00:04:52,570 --> 00:04:56,430
and of course

82
00:04:56,480 --> 00:05:00,500
many others

83
00:05:09,760 --> 00:05:13,930
i'm going to say that of this again

84
00:05:13,990 --> 00:05:17,370
OK so anybody wasn't bold yet

85
00:05:17,940 --> 00:05:21,700
and i'm going to say a little bit more formal

86
00:05:22,560 --> 00:05:24,800
so the

87
00:05:26,440 --> 00:05:29,440
really good only so that we have a chance

88
00:05:29,490 --> 00:05:33,450
being able to do so

89
00:05:33,470 --> 00:05:35,340
mathematical proofs about this

90
00:05:48,890 --> 00:05:51,540
there's a lot of nature that says anything you're of office what you want to

91
00:05:51,540 --> 00:05:53,430
revert to next

92
00:05:53,470 --> 00:06:02,510
so let's in nature

93
00:06:02,520 --> 00:06:07,280
take this off

94
00:06:07,370 --> 00:06:12,780
we have therefore

95
00:06:12,810 --> 00:06:14,540
a formal language

96
00:06:14,560 --> 00:06:19,470
in which we have variables

97
00:06:19,520 --> 00:06:20,760
x one

98
00:06:20,830 --> 00:06:22,660
x two

99
00:06:24,830 --> 00:06:28,930
x three

100
00:06:28,950 --> 00:06:32,870
and we take ourselves to have infinitely many of those

101
00:06:32,890 --> 00:06:39,470
least there's always another one we can we can

102
00:06:39,600 --> 00:06:45,350
is the

103
00:06:45,410 --> 00:06:47,470
we have

104
00:06:49,810 --> 00:06:53,540
for the two symbols

105
00:06:53,560 --> 00:07:03,660
for which i use case

106
00:07:07,720 --> 00:07:09,550
the function symbol has

107
00:07:09,600 --> 00:07:12,470
some arity some disagee

108
00:07:12,470 --> 00:07:15,080
some number of arguments places

109
00:07:15,120 --> 00:07:18,660
zero or more

110
00:07:18,700 --> 00:07:21,740
logic you get to use this wonderful for a zero more

111
00:07:26,080 --> 00:07:27,640
nice so

112
00:07:28,240 --> 00:07:33,080
if i in careful superscript

113
00:07:33,140 --> 00:07:36,470
therefore with the number of arguments

114
00:07:38,390 --> 00:07:43,970
for every

115
00:07:45,660 --> 00:07:47,760
arity n

116
00:07:47,760 --> 00:07:50,910
we have finitely many function symbols

117
00:07:50,910 --> 00:07:52,290
of that

118
00:07:53,180 --> 00:07:56,060
one f two f three

119
00:08:00,220 --> 00:08:05,490
so we assume that we can

120
00:08:05,490 --> 00:08:07,930
bill compounds these

121
00:08:09,080 --> 00:08:13,410
in terms of that

122
00:08:13,430 --> 00:08:15,790
we can construct

123
00:08:15,790 --> 00:08:19,390
the things in the language the behave like names so

124
00:08:19,390 --> 00:08:24,930
of the reflectivity function because it leads to sparse solutions plus a data fitting term

125
00:08:24,930 --> 00:08:30,970
to be faithful to the data which it took also has an one penalty

126
00:08:30,990 --> 00:08:36,410
a bit later centers and science suggested something very similar

127
00:08:36,430 --> 00:08:41,050
where the proposed to minimize the l one norm of the reflectivity function as to

128
00:08:41,050 --> 00:08:46,890
promote sparsity again but now the user defined by rise different data fitting term they

129
00:08:46,890 --> 00:08:47,890
took the

130
00:08:47,910 --> 00:08:49,970
l two distance

131
00:08:49,990 --> 00:08:56,510
OK so that was happening in the in the mid eighties in geophysics and today

132
00:08:56,530 --> 00:08:58,120
this minimisation

133
00:08:58,450 --> 00:09:00,470
program this is very

134
00:09:00,530 --> 00:09:06,340
well studied in statistics because it has been reintroduced in statistics as the lasso and

135
00:09:06,340 --> 00:09:09,030
in signal processing as business process

136
00:09:09,050 --> 00:09:11,820
which will talk about

137
00:09:11,840 --> 00:09:15,470
OK since so lots of people were working in these things in the late seventies

138
00:09:15,470 --> 00:09:18,220
early eighties and so what did

139
00:09:18,870 --> 00:09:20,510
they find defined

140
00:09:20,510 --> 00:09:22,760
that we want to put in l one norm

141
00:09:22,760 --> 00:09:26,720
to quote centers and signed to effectively construct the solution with the least number of

142
00:09:26,720 --> 00:09:31,800
nonzero components that is to recover sparse spike train and then we have this goodness

143
00:09:31,800 --> 00:09:35,800
of fit criteria which you can take to be able to one depending on your

144
00:09:35,800 --> 00:09:37,620
sensitivity to

145
00:09:41,320 --> 00:09:45,490
all right so the empirical observation that these people did is that OK i have

146
00:09:45,490 --> 00:09:47,070
been limited data

147
00:09:47,070 --> 00:09:48,700
if i have been limited data

148
00:09:48,720 --> 00:09:52,640
it is there missing out a lot of frequency components have don't have no use

149
00:09:53,030 --> 00:09:56,820
in these the reflectivity function is actually sparse by train

150
00:09:56,820 --> 00:10:01,320
then centers enzymes observed that the use of the minimum in one criterion produces under

151
00:10:01,320 --> 00:10:04,320
many circumstances the correct solution

152
00:10:04,340 --> 00:10:09,970
so this people's is committed geophysicist we're very happy because they could actually recovered

153
00:10:09,970 --> 00:10:11,370
two reasons

154
00:10:11,490 --> 00:10:15,200
some accuracy

155
00:10:15,260 --> 00:10:16,780
the upper crust

156
00:10:16,800 --> 00:10:19,840
of the year so by using

157
00:10:19,840 --> 00:10:23,340
this kind of l one minimisation procedure from seismic

158
00:10:25,820 --> 00:10:27,110
all right

159
00:10:27,120 --> 00:10:32,930
now i'm going to move a little bit in time so now we can equate

160
00:10:32,930 --> 00:10:39,320
geophysics and and signal processing and statistics were more recent uses of

161
00:10:39,410 --> 00:10:44,620
in one norm have been you have been suggested in computational harmonic analysis under the

162
00:10:44,620 --> 00:10:49,300
name of basis pursuit by chen donoho and saunders in nineteen six and statistics as

163
00:10:49,300 --> 00:10:54,140
well as two by twos shaw nineteen ninety six and and what these things are

164
00:10:54,140 --> 00:10:57,840
is there really is this kind of proposals except that has been exported in a

165
00:10:57,840 --> 00:11:00,120
completely different context

166
00:11:03,640 --> 00:11:06,110
i'm going to try to go quickly overseas

167
00:11:06,120 --> 00:11:09,890
in computational harmonic analysis it to feel that is trying to

168
00:11:09,990 --> 00:11:17,850
develop representations that are good at representing datasets for example images o o one needed

169
00:11:17,850 --> 00:11:19,990
our three d data and so on

170
00:11:20,030 --> 00:11:25,470
so it's an active field that aims to develop effective representation that is sparse representations

171
00:11:25,930 --> 00:11:28,140
as this field has championed a lot of

172
00:11:28,160 --> 00:11:35,030
interesting construction filters wavelets curvelets brush let's and so on and so forth and so

173
00:11:35,030 --> 00:11:39,490
by the late nineties there was an emerging feeling that there is no universal representation

174
00:11:39,490 --> 00:11:40,970
that is not possible

175
00:11:41,090 --> 00:11:46,840
to find representation that is good for everything simultaneously in different representations are best four

176
00:11:46,840 --> 00:11:50,930
different phenomena and then when i use the word best i mean that they lead

177
00:11:50,930 --> 00:11:54,180
to sparse representations

178
00:11:54,200 --> 00:11:55,180
for example

179
00:11:55,200 --> 00:12:03,200
brush let's or sinusoids are very good at modelling oscillatory phenomena of course periodic phenomena

180
00:12:03,220 --> 00:12:05,890
wavelets are very good for

181
00:12:05,930 --> 00:12:13,010
representing scenes that are discontinuous around points curvelets are good for this representing things that

182
00:12:13,010 --> 00:12:13,950
are discontinuous

183
00:12:14,340 --> 00:12:18,930
long curves and so on and so forth locale cosines brush let's are good for

184
00:12:18,930 --> 00:12:21,530
textures et cetera et cetera

185
00:12:21,700 --> 00:12:24,990
so in the late nineties it was his view that we should try to combine

186
00:12:25,010 --> 00:12:28,030
all this representation together to take

187
00:12:28,050 --> 00:12:31,300
the advantage of the strengths of each representation

188
00:12:34,890 --> 00:12:37,820
in paper in nineteen ninety three

189
00:12:37,820 --> 00:12:43,530
my lines and proposed to kind of combined is these representations so let's say we

190
00:12:43,530 --> 00:12:45,760
have basis that i'm going to call phi

191
00:12:45,780 --> 00:12:50,490
so we have the first basis phi one we have the second fight to the search three

192
00:12:50,510 --> 00:12:53,910
so for example this could be for users could be wavelengths could be currents could

193
00:12:53,910 --> 00:12:56,200
be for and so on

194
00:12:56,220 --> 00:12:59,570
and so now what they propose is now we're going to be we're going to

195
00:12:59,570 --> 00:13:03,530
build a building dictionary by concatenating all these bases

196
00:13:03,550 --> 00:13:08,430
by having all these representations together so we're going to create a uge data matrix

197
00:13:08,510 --> 00:13:13,010
whose columns are the basis functions and because i have lots of basis have much

198
00:13:13,010 --> 00:13:16,820
more columns and rows i want to create a uge

199
00:13:18,640 --> 00:13:25,050
whose columns are basis functions phi i but because i have several representations

200
00:13:25,050 --> 00:13:28,110
i'm going to have much more columns than world

201
00:13:28,200 --> 00:13:33,550
OK because i have all good representation that are all good for different phenomena

202
00:13:33,570 --> 00:13:36,550
the representation is not unique any more

203
00:13:36,570 --> 00:13:41,620
so i can express any images signal is the superposition of basis functions but this

204
00:13:41,620 --> 00:13:44,160
representation is not unique any more

205
00:13:44,200 --> 00:13:48,850
or in it using notation from linear algebra i will just write is as ethical

206
00:13:48,850 --> 00:13:51,180
site times of

207
00:13:51,180 --> 00:13:53,660
five concerns for

208
00:13:53,660 --> 00:13:55,380
the treatment

209
00:13:55,570 --> 00:13:58,320
my estimate goes down

210
00:13:58,340 --> 00:14:00,010
and even if some of those

211
00:14:00,030 --> 00:14:02,180
estimate the

212
00:14:02,200 --> 00:14:08,010
inputs to the treatment a redundant should be that useful but this is still a

213
00:14:08,050 --> 00:14:12,460
better estimate if some of them are useful so what's happened here is that there

214
00:14:12,610 --> 00:14:13,720
a lot

215
00:14:14,450 --> 00:14:18,180
what be deducing from this is the

216
00:14:18,200 --> 00:14:19,740
the first model

217
00:14:19,760 --> 00:14:21,010
this model

218
00:14:21,010 --> 00:14:23,340
which is what this one the state is about

219
00:14:24,030 --> 00:14:25,400
this table

220
00:14:25,410 --> 00:14:26,740
is that models

221
00:14:26,760 --> 00:14:29,910
this table is about the model that

222
00:14:29,950 --> 00:14:32,840
it was three pages but also

223
00:14:32,900 --> 00:14:36,490
so so this is the one that's got the treatment

224
00:14:36,800 --> 00:14:39,930
so this is saying

225
00:14:39,970 --> 00:14:41,800
if this was the right models

226
00:14:44,880 --> 00:14:49,280
when i go from this one to this one because

227
00:14:49,300 --> 00:14:54,610
my estimate of sigma squared is changing from twenty three point nine thousand nine and

228
00:14:54,610 --> 00:14:56,430
a half or not quite six

229
00:14:56,450 --> 00:15:00,470
that's changed quite a lot of good and also indicating

230
00:15:01,970 --> 00:15:04,860
this model here of that one

231
00:15:05,490 --> 00:15:09,340
the sometimes missing from it because

232
00:15:09,360 --> 00:15:11,160
this estimate is too high

233
00:15:13,700 --> 00:15:18,510
because it was so easy for the inputs missing from the model

234
00:15:18,510 --> 00:15:22,550
this estimate was too high relative to the square if i on the other hand

235
00:15:22,550 --> 00:15:27,110
i mean this model i i would really think that if i've included so for

236
00:15:27,110 --> 00:15:28,700
every single treatment

237
00:15:28,700 --> 00:15:33,610
probably nice to get all the possible inputs i could have that

238
00:15:36,320 --> 00:15:40,740
it is an estimate of were so look at least

239
00:15:40,760 --> 00:15:44,900
this is telling me that i might put too many turns into model

240
00:15:45,130 --> 00:15:49,280
but this should still be these estimates sigma square because that's what it said on

241
00:15:49,280 --> 00:15:53,380
the slide before so if

242
00:15:53,410 --> 00:15:57,110
if you have concerns in but i've got all the ones that can

243
00:15:58,050 --> 00:16:00,430
s what is still the rest the square

244
00:16:00,430 --> 00:16:06,860
so this making here is that by adding all those inputs

245
00:16:06,880 --> 00:16:09,930
that i am actually going to

246
00:16:09,950 --> 00:16:13,760
i might include some redundant all of the ones that should be there going to

247
00:16:13,760 --> 00:16:16,180
be in there so

248
00:16:16,200 --> 00:16:18,450
so my assumption is that this number

249
00:16:18,450 --> 00:16:21,400
is that good estimates the square so that the rest

250
00:16:21,400 --> 00:16:23,450
six were not point six

251
00:16:23,470 --> 00:16:26,950
listen is it not not very good estimates work

252
00:16:27,010 --> 00:16:28,410
so it's too high

253
00:16:28,430 --> 00:16:31,220
so it's too high

254
00:16:31,280 --> 00:16:32,110
i mean

255
00:16:32,140 --> 00:16:33,300
the situation

256
00:16:33,300 --> 00:16:37,470
so what does not include one or more inputs that it ought to so then

257
00:16:37,470 --> 00:16:40,910
minus r-squared also square is too big

258
00:16:40,910 --> 00:16:44,280
because it should be

259
00:16:44,300 --> 00:16:48,860
OK so that that's the trick so looking at this kind of formal mechanism for

260
00:16:48,860 --> 00:16:55,430
working out probabilistically how big they should be how big that should be sold

261
00:16:55,510 --> 00:16:58,660
basically it boils down to this but

262
00:16:58,860 --> 00:17:02,910
this holds this role here is measuring the difference between

263
00:17:02,930 --> 00:17:09,230
was the robot treatment is measuring the difference between having just watson having terms what

264
00:17:09,230 --> 00:17:12,490
some treatments so it's one of the treatment

265
00:17:13,050 --> 00:17:16,700
effects treatment input

266
00:17:16,720 --> 00:17:19,470
like this one

267
00:17:19,490 --> 00:17:23,200
i'm not going to get into exactly where that number and so when the number

268
00:17:23,200 --> 00:17:27,510
comes from but you're aware of the was come from all the supplies comes out

269
00:17:30,200 --> 00:17:33,800
this last number here this is what we get that is this

270
00:17:33,860 --> 00:17:37,470
we have a number of the same

271
00:17:38,470 --> 00:17:41,950
the concept was that got to hold of is this

272
00:17:42,010 --> 00:17:43,820
i can look at that and say

273
00:17:43,840 --> 00:17:48,300
well that's not quite six months twenty three point nine to twenty four hours big

274
00:17:48,320 --> 00:17:49,930
o point six i can say that

275
00:17:49,950 --> 00:17:56,380
i'm no acceptable well how do i know that that's not just fly

276
00:17:56,660 --> 00:17:58,490
i don't really

277
00:17:58,510 --> 00:18:00,950
what's this number here

278
00:18:00,970 --> 00:18:02,930
he says if the

279
00:18:02,950 --> 00:18:05,490
what's the chance

280
00:18:05,510 --> 00:18:06,530
of getting

281
00:18:06,550 --> 00:18:11,450
the difference this difference twenty twenty three verses point six

282
00:18:11,450 --> 00:18:14,090
what's the probability that happening

283
00:18:15,570 --> 00:18:20,010
treatment inputs make no difference was the chance of getting a difference five field please

284
00:18:20,010 --> 00:18:25,880
feel something fluctuations well it's not very likely is what it says

285
00:18:25,900 --> 00:18:29,510
so it is unlikely that

286
00:18:29,550 --> 00:18:31,680
this difference

287
00:18:31,720 --> 00:18:33,860
is just random chance

288
00:18:33,880 --> 00:18:40,130
then it's likely that was caused by including the inputs

289
00:18:40,160 --> 00:18:42,450
that's the that's the reason is going on here

290
00:18:42,470 --> 00:18:44,070
basically what you

291
00:18:45,880 --> 00:18:50,260
that's the idea operationally what you do is this you go

292
00:18:50,280 --> 00:18:53,360
right there is a row in this table about treatment

293
00:18:54,260 --> 00:18:59,610
it is what what's called the p value the probability is less than five percent

294
00:18:59,610 --> 00:19:04,010
is less than point zero five it's just the case which is the

295
00:19:04,030 --> 00:19:06,430
then the to ready the model

296
00:19:06,430 --> 00:19:08,010
are important

297
00:19:08,010 --> 00:19:12,300
but puzzling so the mechanism for doing this with the basic

298
00:19:12,320 --> 00:19:15,320
but this is that less than five percent of it is

299
00:19:15,340 --> 00:19:19,110
so what if i said so

300
00:19:19,140 --> 00:19:20,320
it was done

301
00:19:20,340 --> 00:19:22,700
other so saying simply

302
00:19:22,700 --> 00:19:26,570
but it seems like the difference where we might want someone to

303
00:19:26,630 --> 00:19:29,630
well advantage

304
00:19:30,780 --> 00:19:34,450
if you remember the treatments were made up of

305
00:19:34,470 --> 00:19:40,680
so so variety was so in data and there was a density

306
00:19:40,700 --> 00:19:44,630
so we might then go on to say if we think there's the difference caused

307
00:19:44,630 --> 00:19:46,260
by the different treatments

308
00:19:46,280 --> 00:19:50,910
what aspects of the treatments causing the the differences

309
00:19:50,930 --> 00:19:52,220
this is

310
00:19:52,240 --> 00:19:53,820
a table where we

311
00:19:53,860 --> 00:19:56,970
basically it's an expanded version

312
00:19:57,010 --> 00:19:58,530
of this

313
00:19:58,530 --> 00:20:00,820
this this treatment there

314
00:20:00,820 --> 00:20:05,660
o thing to it's to do with sowing data

315
00:20:07,030 --> 00:20:10,320
so intensity on topics

316
00:20:10,320 --> 00:20:12,380
so that's what i'm not

317
00:20:12,430 --> 00:20:16,090
and basically

318
00:20:16,090 --> 00:20:18,660
but the variety is

319
00:20:18,760 --> 00:20:20,320
you start off with a

320
00:20:20,340 --> 00:20:24,640
the plot terms and so starting off with the same basic model

321
00:20:24,660 --> 00:20:28,010
what is this model

322
00:20:29,050 --> 00:20:31,360
we've included inputs

323
00:20:31,360 --> 00:20:32,500
is that

324
00:20:32,530 --> 00:20:33,890
show here

325
00:20:33,900 --> 00:20:38,450
and what does effectively is a in

326
00:20:38,560 --> 00:20:40,010
OK b

327
00:20:40,020 --> 00:20:44,220
in my opinion the difficulties of establishing a hilbert space

328
00:20:45,640 --> 00:20:50,770
in practice many issues one has to make this into a finite approximation

329
00:20:50,810 --> 00:20:53,190
one divided by problems

330
00:20:53,260 --> 00:20:54,060
and also

331
00:20:54,090 --> 00:20:58,360
this is positive and negative eigenvalues shown

332
00:20:58,380 --> 00:20:59,470
down there

333
00:20:59,520 --> 00:21:01,620
and in one of

334
00:21:01,630 --> 00:21:08,010
bounds with particular i values support from negative to estimate the fact that this by

335
00:21:08,090 --> 00:21:10,750
this is what i show you this what you will discover

336
00:21:10,760 --> 00:21:17,500
symmetries of shapes information that is extensive that it is it is

337
00:21:19,420 --> 00:21:21,770
the problem with this method

338
00:21:21,780 --> 00:21:24,360
for symmetry in general you may not have

339
00:21:24,370 --> 00:21:25,410
full data

340
00:21:25,420 --> 00:21:26,500
you have to

341
00:21:26,630 --> 00:21:29,470
and when they come from scotland it is often vital

342
00:21:29,490 --> 00:21:30,300
so i think

343
00:21:30,310 --> 00:21:37,200
hands up for having a complete fault we can compute decided that

344
00:21:37,220 --> 00:21:39,310
not always applicable so we've got to look

345
00:21:39,320 --> 00:21:41,600
what the it

346
00:21:41,650 --> 00:21:47,470
following sections ideas namely that can diffusion point on the surface

347
00:21:47,480 --> 00:21:49,920
one thing of placing a little

348
00:21:49,930 --> 00:21:52,570
china p element the point ninety four

349
00:21:52,610 --> 00:21:56,340
see how these diffuses the future of our time

350
00:21:56,350 --> 00:21:58,140
characterizes somehow

351
00:21:58,810 --> 00:22:04,280
one about the behaviour of point for example point of positive curvature diffuses slowly point

352
00:22:04,280 --> 00:22:07,430
negative curvature diffuses faster

353
00:22:07,440 --> 00:22:08,770
and this is here

354
00:22:10,180 --> 00:22:12,970
diffusion script

355
00:22:13,010 --> 00:22:14,600
it is for

356
00:22:14,610 --> 00:22:17,270
the corresponding told of the drug

357
00:22:17,280 --> 00:22:20,650
and the interesting thing is

358
00:22:20,680 --> 00:22:22,090
after a short time

359
00:22:22,140 --> 00:22:25,280
four four look the same could in fact or four legs look the the same

360
00:22:26,800 --> 00:22:28,650
the future

361
00:22:28,660 --> 00:22:29,740
question two

362
00:22:29,780 --> 00:22:33,390
the large areas of the drug and then you can distinguish the back legs from

363
00:22:33,410 --> 00:22:38,230
the front legs because the diffusion behave differently in the front leg area then the

364
00:22:38,230 --> 00:22:42,910
battle against integer with

365
00:22:42,970 --> 00:22:47,310
and of course from the graphical view interesting topic is to take a shape that's

366
00:22:48,410 --> 00:22:50,140
semantic would like

367
00:22:50,220 --> 00:22:53,250
the same applies to make it more human

368
00:22:53,290 --> 00:22:59,520
it's interesting to the coupled optimisation problem because the four behaviours he

369
00:22:59,520 --> 00:23:04,800
while expressing your objective you want opinion formation space basically what to make the clusters

370
00:23:04,980 --> 00:23:10,910
have both now in the last show here an example of the these are the

371
00:23:12,200 --> 00:23:13,890
i symmetrizing by

372
00:23:14,380 --> 00:23:16,760
making clusters in

373
00:23:16,800 --> 00:23:19,010
OK so i the

374
00:23:19,190 --> 00:23:21,270
the space

375
00:23:21,390 --> 00:23:25,360
in writing the article

376
00:23:35,060 --> 00:23:35,980
the flow

377
00:23:35,990 --> 00:23:42,650
the common theme of his work and what could come in the introduction of the

378
00:23:42,720 --> 00:23:44,650
first places to express

379
00:23:44,660 --> 00:23:50,250
this relation of the object so that a lot of structure discovery and also of

380
00:23:50,250 --> 00:23:56,390
the operations dual happening by understanding the coupling between the three geometry

381
00:23:56,430 --> 00:23:58,620
and what's going on in this task force space

382
00:23:58,740 --> 00:24:02,650
you see you like got it all in the attractor

383
00:24:02,650 --> 00:24:04,820
on the right

384
00:24:05,810 --> 00:24:06,760
so the

385
00:24:07,340 --> 00:24:09,430
the show you how

386
00:24:09,440 --> 00:24:11,340
it's possible to capture

387
00:24:11,390 --> 00:24:16,310
partial approximate any particular shape bias in mechanism

388
00:24:16,320 --> 00:24:18,430
and that this is actually

389
00:24:18,440 --> 00:24:20,700
i efficient discovery documentary

390
00:24:20,800 --> 00:24:24,890
now about the problem

391
00:24:24,900 --> 00:24:28,670
so the problem of what don't have all the data in the same place in

392
00:24:28,980 --> 00:24:30,300
somehow still

393
00:24:30,410 --> 00:24:32,240
discover the abstraction

394
00:24:32,280 --> 00:24:37,560
so you have partial does not have you know what should have a clear

395
00:24:37,570 --> 00:24:39,020
large common area

396
00:24:39,030 --> 00:24:44,390
they want to compute only certain very compact footprints from the scales in the comparing

397
00:24:44,390 --> 00:24:45,520
the fingerprints

398
00:24:45,540 --> 00:24:49,290
i want to discover simulated

399
00:24:49,340 --> 00:24:52,780
thank you for complete shapes using

400
00:24:52,790 --> 00:24:53,760
come back to it

401
00:24:53,770 --> 00:24:58,660
but classical problem the problem here is this is about looking back and looking forward

402
00:24:58,660 --> 00:25:04,680
much the like i don't know much about how much

403
00:25:04,690 --> 00:25:08,360
the point is the signature fingerprint compact

404
00:25:08,470 --> 00:25:12,390
the independent and they have two shapes to compare

405
00:25:12,390 --> 00:25:16,650
his fingerprints to compare the shape

406
00:25:16,670 --> 00:25:20,980
and actually the idea here is an idea adopted from the domain

407
00:25:23,060 --> 00:25:26,520
or duplicate web page

408
00:25:26,680 --> 00:25:29,340
so how does it work

409
00:25:30,740 --> 00:25:31,930
and you actually

410
00:25:31,950 --> 00:25:33,570
break it up into

411
00:25:34,570 --> 00:25:39,060
just a subset of points they can neighborhoods around this point

412
00:25:39,240 --> 00:25:42,390
because of the large this it

413
00:25:42,510 --> 00:25:46,940
and simply fission each objective it into the bag of back

414
00:25:47,000 --> 00:25:48,640
i forget about the object

415
00:25:48,940 --> 00:25:50,950
just like the fact

416
00:25:51,000 --> 00:25:52,870
the question is if you want something

417
00:25:53,630 --> 00:25:56,630
i mean have to have something they symmetrically

418
00:25:56,710 --> 00:26:01,850
so what you have actually we can pull out it's possible to recover the object

419
00:26:01,930 --> 00:26:06,090
by in the fact that this is very much how DNA structure

420
00:26:07,260 --> 00:26:10,380
or you could happen but all

421
00:26:10,410 --> 00:26:12,730
put them together

422
00:26:12,850 --> 00:26:15,600
and no

423
00:26:15,640 --> 00:26:18,550
what we do is we take this back to the conventional

424
00:26:18,560 --> 00:26:20,380
compute descriptor

425
00:26:22,280 --> 00:26:23,810
very much like about

426
00:26:26,090 --> 00:26:27,390
again this

427
00:26:27,490 --> 00:26:28,790
this overlapping

428
00:26:29,650 --> 00:26:33,900
the space in the use of guns is large compared to ten times

429
00:26:33,930 --> 00:26:36,200
this space examples

430
00:26:36,250 --> 00:26:40,600
and so one this by taking all this it's good for every single

431
00:26:40,960 --> 00:26:42,720
we map of shape

432
00:26:42,730 --> 00:26:45,350
and the point cloud high dimensional space

433
00:26:45,380 --> 00:26:48,740
the majority of this

434
00:26:48,880 --> 00:26:53,090
so somehow now measuring similarity of shape

435
00:26:53,100 --> 00:26:58,210
becomes given this kind of the point clouds how many points they should recall how

436
00:26:58,210 --> 00:27:01,890
many of which have other

437
00:27:01,940 --> 00:27:04,530
so compared is actually two bags of points

438
00:27:04,570 --> 00:27:06,090
how space

439
00:27:06,100 --> 00:27:08,510
it very classical music

440
00:27:08,530 --> 00:27:10,390
they can be made it

441
00:27:10,410 --> 00:27:11,870
the distance metric

442
00:27:11,910 --> 00:27:13,110
so what line

443
00:27:13,110 --> 00:27:18,470
and is it and there is an object or the subject or the verb of

444
00:27:18,480 --> 00:27:21,780
the sentence so you can put everything here if you know how to use it

445
00:27:21,780 --> 00:27:23,340
how to use this information

446
00:27:23,360 --> 00:27:26,300
you can extract included into during the innovation

447
00:27:26,310 --> 00:27:28,730
here in the in know our

448
00:27:31,160 --> 00:27:32,010
OK so

449
00:27:32,020 --> 00:27:32,810
we know

450
00:27:32,850 --> 00:27:36,050
we can extract extract everything from the

451
00:27:37,090 --> 00:27:42,130
basically we can stop all development because they have enough for search engine so what

452
00:27:42,160 --> 00:27:45,660
we can do they can take polymerization

453
00:27:45,670 --> 00:27:48,260
it is structure of our from documents

454
00:27:48,290 --> 00:27:50,560
you can have a career

455
00:27:50,580 --> 00:27:54,120
we can come compare this query to what we extracted

456
00:27:54,180 --> 00:27:58,130
and simply return this result when you have image

457
00:27:58,180 --> 00:27:59,190
and this

458
00:27:59,270 --> 00:28:03,860
simply naive can't simply scanned the document

459
00:28:03,920 --> 00:28:06,020
and i think the

460
00:28:06,060 --> 00:28:10,130
you can say that it's not gordon brown but that but i can

461
00:28:10,140 --> 00:28:12,910
object is very good friends

462
00:28:12,910 --> 00:28:14,300
first of all

463
00:28:14,370 --> 00:28:16,540
you have reason for knowledge

464
00:28:16,560 --> 00:28:19,670
you know all the words from the document because this

465
00:28:19,670 --> 00:28:21,720
constructed and there

466
00:28:21,740 --> 00:28:23,490
what's in your

467
00:28:23,810 --> 00:28:26,330
and one that is doing so much

468
00:28:26,380 --> 00:28:30,490
so you can you can do very complex algorithm

469
00:28:30,560 --> 00:28:32,990
you know it's very

470
00:28:33,000 --> 00:28:36,160
good for the current hardware architecture

471
00:28:36,180 --> 00:28:41,010
because you using only linear acts as the euro was one thing

472
00:28:41,040 --> 00:28:46,510
all document by small plod through all infrastructure search engine

473
00:28:46,560 --> 00:28:49,850
the last can be very easily parallelized

474
00:28:49,870 --> 00:28:51,540
imagine that

475
00:28:51,540 --> 00:28:52,560
we have

476
00:28:52,580 --> 00:28:53,910
if you wish

477
00:28:53,930 --> 00:28:56,350
library in alexandria in

478
00:28:56,390 --> 00:28:58,450
two thousand years ago

479
00:28:58,490 --> 00:29:00,510
and are going to

480
00:29:01,520 --> 00:29:03,140
four of

481
00:29:05,290 --> 00:29:09,160
and they don't have index on this particular work what they're going to do

482
00:29:09,450 --> 00:29:13,870
the highlight of people who are literate and the reason this is

483
00:29:13,910 --> 00:29:17,350
navy common one percent but otherwise

484
00:29:17,410 --> 00:29:22,620
and it's much harder than for humans because humans good in reading but very

485
00:29:22,640 --> 00:29:24,510
but in

486
00:29:24,560 --> 00:29:27,100
jumping from one place to another

487
00:29:28,060 --> 00:29:31,580
the good approach and sometimes you can use

488
00:29:31,600 --> 00:29:36,930
because it is doing and if you have a small collection of have collection that

489
00:29:36,930 --> 00:29:39,080
change so frequently

490
00:29:39,140 --> 00:29:41,970
that building index is is not

491
00:29:42,100 --> 00:29:47,310
think simply does doesn't have any sense for this collection because when we already have

492
00:29:47,310 --> 00:29:49,540
in the have already new collection

493
00:29:49,580 --> 00:29:53,600
maybe and in this collection is not so b

494
00:29:53,620 --> 00:29:58,180
maybe it's better to than i can have pretty fast hardware

495
00:29:58,200 --> 00:30:00,350
then to implement some something

496
00:30:02,970 --> 00:30:07,620
but we are going to be faster and we are going to add even

497
00:30:07,660 --> 00:30:10,450
and here unfortunately i want to say

498
00:30:11,260 --> 00:30:15,600
my example this library and librarians doesn't work

499
00:30:18,540 --> 00:30:21,010
librarian point-of-view

500
00:30:21,060 --> 00:30:23,620
in the this is what we are

501
00:30:23,660 --> 00:30:26,510
we are usually called taxonomy

502
00:30:26,560 --> 00:30:31,290
because they extract the most important words from documents and they put this model into

503
00:30:31,290 --> 00:30:34,140
some classes and the things that you

504
00:30:34,200 --> 00:30:38,680
and seen in libraries the index

505
00:30:38,740 --> 00:30:42,930
and for as for developers of computer programs

506
00:30:42,950 --> 00:30:44,660
in the simplest

507
00:30:44,680 --> 00:30:46,520
as special structure

508
00:30:46,540 --> 00:30:49,990
the accelerate data access

509
00:30:51,780 --> 00:30:54,010
it's not

510
00:30:54,140 --> 00:30:59,350
some something that was created by human it's not connected to document it simply

511
00:30:59,370 --> 00:31:00,540
some things that

512
00:31:01,950 --> 00:31:05,910
to improve the speed of our application

513
00:31:05,930 --> 00:31:08,100
and again i really

514
00:31:08,140 --> 00:31:11,080
had such cases when i talk to librarians

515
00:31:11,100 --> 00:31:13,970
and so that's what this system can implement

516
00:31:13,990 --> 00:31:15,910
indexing this church

517
00:31:15,970 --> 00:31:18,260
and she said no way

518
00:31:18,290 --> 00:31:23,240
the can do it because indexing is a very complex task and you need to

519
00:31:23,240 --> 00:31:25,600
understand about documents

520
00:31:28,330 --> 00:31:32,600
is different but the difference in terminology is not different than what they actually are

521
00:31:33,310 --> 00:31:36,010
because librarians indexes on also

522
00:31:37,290 --> 00:31:40,240
sometimes physical structures drivers and car

523
00:31:40,240 --> 00:31:43,990
that accelerates search

524
00:31:44,620 --> 00:31:46,990
so now we are talking about

525
00:31:51,100 --> 00:31:53,510
i recently has graduated

526
00:31:54,620 --> 00:31:57,470
the question is how to accelerate church

527
00:31:57,490 --> 00:32:00,760
and the usual answer is

528
00:32:00,780 --> 00:32:04,010
OK have a special tool for storing

529
00:32:04,040 --> 00:32:08,240
a lot of information charge this is databases and we have a lot of that

530
00:32:08,310 --> 00:32:10,580
basis starting from

531
00:32:12,260 --> 00:32:15,660
like a skylight like something small

532
00:32:15,700 --> 00:32:18,680
and ending two such more of or

533
00:32:21,330 --> 00:32:23,780
take from the shelf one of the

534
00:32:23,790 --> 00:32:25,830
a very powerful tool

535
00:32:25,870 --> 00:32:29,240
and use it as our search engine and we can do it and there is

536
00:32:29,240 --> 00:32:30,870
nothing wrong

537
00:32:32,040 --> 00:32:34,870
this is a construction we have

538
00:32:34,930 --> 00:32:36,720
because we we assume that

539
00:32:36,740 --> 00:32:40,720
i'm going to church and we are putting some set of words and want to

540
00:32:40,720 --> 00:32:42,780
get all documents that

541
00:32:42,780 --> 00:32:45,850
contains this work

542
00:32:46,410 --> 00:32:49,810
we create the structure of the database

543
00:32:49,830 --> 00:32:51,620
so we have a dictionary

544
00:32:51,640 --> 00:32:53,260
you have

545
00:32:53,280 --> 00:32:55,830
description of our documents

546
00:32:55,850 --> 00:33:00,330
and you have ideas for them for normalisation and we have the main table

547
00:33:00,350 --> 00:33:01,850
well being

548
00:33:01,870 --> 00:33:03,660
put together formed

549
00:33:04,950 --> 00:33:06,640
flags field

550
00:33:08,890 --> 00:33:12,510
playstation the document

551
00:33:12,560 --> 00:33:15,370
i'm not very good

552
00:33:15,390 --> 00:33:17,600
database programmer

553
00:33:17,640 --> 00:33:20,220
therefore in this structure is one

554
00:33:20,240 --> 00:33:21,660
one you

555
00:33:22,780 --> 00:33:30,160
every every specialist in relational databases so that it's not it's not right

556
00:33:31,160 --> 00:33:35,810
what you don't like

557
00:33:35,830 --> 00:33:37,160
i think that this is

558
00:33:37,220 --> 00:33:39,520
if you because it

559
00:33:39,580 --> 00:33:42,450
one to many and we need additional addiction

560
00:33:42,490 --> 00:33:43,350
OK but

561
00:33:43,370 --> 00:33:44,810
doesn't so

562
00:33:44,810 --> 00:33:47,830
if are doing our simple search

563
00:33:47,890 --> 00:33:50,790
this is selected for this not

564
00:33:50,850 --> 00:33:54,790
and and everything is OK with this that

565
00:33:54,790 --> 00:33:56,930
except for one small thing

566
00:33:56,930 --> 00:33:58,970
if you are going to implement this

567
00:33:58,990 --> 00:34:04,060
and if you are going to in this in this structure

568
00:34:06,200 --> 00:34:08,910
war and peace by leo tolstoy

569
00:34:08,950 --> 00:34:11,060
even if you're using or

570
00:34:11,060 --> 00:34:12,810
you will be surprised

571
00:34:13,310 --> 00:34:15,370
pretty law

572
00:34:15,370 --> 00:34:21,580
and i will said OK i'm smart guy i read books about information retrieval

573
00:34:21,600 --> 00:34:25,310
and it's not true because the information to your system

574
00:34:25,310 --> 00:34:30,160
and then this lasky that i would like to solve

575
00:34:30,220 --> 00:34:32,950
what are the same circle

576
00:34:33,000 --> 00:34:34,270
but now

577
00:34:34,430 --> 00:34:36,310
now now

578
00:34:36,330 --> 00:34:43,290
the sources on the boundary like there

579
00:34:43,310 --> 00:34:49,020
well that's the circle despite its

580
00:34:49,020 --> 00:34:51,000
and inside

581
00:34:51,020 --> 00:34:52,060
no sources

582
00:34:52,080 --> 00:34:58,200
well the sources on the boundaries on this in this case i'm solving you plastic

583
00:34:58,250 --> 00:34:59,850
qxx was you why

584
00:34:59,910 --> 00:35:01,330
o zero

585
00:35:01,350 --> 00:35:04,000
inside but now on the boundary

586
00:35:04,020 --> 00:35:07,040
you is the delta function is that the boundary

587
00:35:07,060 --> 00:35:08,240
you here

588
00:35:08,250 --> 00:35:10,180
you on the boundary

589
00:35:10,200 --> 00:35:17,200
is delta

590
00:35:17,220 --> 00:35:22,310
so you see that i'm creating a sort of very special

591
00:35:22,350 --> 00:35:24,660
problems and the reason is

592
00:35:24,660 --> 00:35:27,560
as of a show in the next lecture

593
00:35:30,120 --> 00:35:34,020
if i can solve problems with

594
00:35:34,040 --> 00:35:38,620
with delta functions

595
00:35:38,660 --> 00:35:43,450
either inside or on the boundary then i could solve them with

596
00:35:43,500 --> 00:35:48,660
other things i could combine the answers calls are called the green's function as the

597
00:35:48,660 --> 00:35:53,830
green's function for the source at the origin the answer here you will be the

598
00:35:53,830 --> 00:35:55,660
green's function on a circle

599
00:35:55,680 --> 00:35:56,790
for the source

600
00:35:57,850 --> 00:36:02,000
inside point the solution new here once i find it is going to be the

601
00:36:02,000 --> 00:36:02,910
green's function

602
00:36:03,370 --> 00:36:05,540
for this problem on a circle

603
00:36:05,540 --> 00:36:07,930
with the delta function at

604
00:36:07,930 --> 00:36:09,000
the boundary

605
00:36:10,330 --> 00:36:14,430
there are special problems but if we solve those we can use them to solve

606
00:36:14,640 --> 00:36:20,120
a much more general problems because i can take any sources like some combination of

607
00:36:20,120 --> 00:36:22,160
sources at different points

608
00:36:22,180 --> 00:36:28,660
or any boundary conditions would be like some combination of delta's different boundary point

609
00:36:28,700 --> 00:36:30,120
so those are the three

610
00:36:30,140 --> 00:36:34,930
problems and the first one is all

611
00:36:35,060 --> 00:36:39,120
that's where we stand now and now i would like to be able to solve

612
00:36:39,120 --> 00:36:40,220
the other two

613
00:36:40,220 --> 00:36:45,310
and then you would see sort of some of the key points of how to

614
00:36:45,310 --> 00:36:47,560
deal with all possible great

615
00:36:50,810 --> 00:36:55,390
the more much more could be said but the thing

616
00:36:55,430 --> 00:36:58,310
this would give you the main points so let me

617
00:36:58,350 --> 00:36:59,950
race this

618
00:37:02,270 --> 00:37:06,100
and just write down some of the write down the answers

619
00:37:06,160 --> 00:37:08,540
for those last two problems

620
00:37:10,160 --> 00:37:11,600
OK and then

621
00:37:11,620 --> 00:37:15,600
try to understand them let me take the last problem

622
00:37:17,310 --> 00:37:19,950
you call delta

623
00:37:20,000 --> 00:37:21,620
on the boundary

624
00:37:21,640 --> 00:37:26,140
with the passing equation inside

625
00:37:26,160 --> 00:37:29,430
i think the solution to that one is the solution

626
00:37:30,640 --> 00:37:31,430
and this

627
00:37:31,450 --> 00:37:34,240
consider it right

628
00:37:34,270 --> 00:37:38,270
i think it's one minus are in polar

629
00:37:38,290 --> 00:37:44,290
its natural we're dealing with circle so we better stay with four corners or one

630
00:37:44,540 --> 00:37:49,310
class are square minus two are cos

631
00:37:55,890 --> 00:37:58,040
i think that's the right guy

632
00:37:58,140 --> 00:38:05,200
now i need to divide by two pi was these numbers

633
00:38:07,200 --> 00:38:08,600
so how would we

634
00:38:08,600 --> 00:38:14,720
and i actually will arrive at this answer using

635
00:38:15,370 --> 00:38:17,240
fourier series

636
00:38:17,290 --> 00:38:18,890
so let me know try to

637
00:38:18,890 --> 00:38:24,580
anticipate that step of getting to this answer but just look at the answer

638
00:38:24,640 --> 00:38:26,140
after we got it

639
00:38:26,160 --> 00:38:31,350
can you see that satisfies the boundary condition in me reasonably so

640
00:38:34,270 --> 00:38:37,410
so the boundary we're on the unit circle

641
00:38:39,600 --> 00:38:43,830
why don't you come out zero at this point

642
00:38:43,850 --> 00:38:45,370
is it's supposed to

643
00:38:45,370 --> 00:38:46,930
because the sources

644
00:38:47,950 --> 00:38:50,470
the delta function is there

645
00:38:50,480 --> 00:38:51,770
because our is one

646
00:38:52,100 --> 00:38:53,020
no problem

647
00:38:57,580 --> 00:39:01,160
so all in all these other points are is one well why don't we get

648
00:39:01,160 --> 00:39:01,830
a zero or

649
00:39:02,100 --> 00:39:05,750
at that point

650
00:39:05,770 --> 00:39:12,120
what's happening at that special points are is one but they do is

651
00:39:12,180 --> 00:39:17,720
a zero at that point the denominator must be doing something to this point that

652
00:39:18,930 --> 00:39:22,040
i think this is the zero of course

653
00:39:22,080 --> 00:39:24,040
on this re

654
00:39:24,060 --> 00:39:28,480
five i just look on this race they was there i can learn some stock

655
00:39:28,540 --> 00:39:32,040
the state was zero i have one or two pi

656
00:39:32,040 --> 00:39:38,660
one minus are square divided by what

657
00:39:38,680 --> 00:39:42,060
let's see if fate is zero what's that

658
00:39:42,060 --> 00:39:46,310
one was are squared minus two are

659
00:39:46,330 --> 00:39:52,000
we just offline look at one plus one minus two are

660
00:39:52,000 --> 00:39:58,000
because they are square and say what is that

661
00:39:58,020 --> 00:40:01,680
it's one minus our times one minus stars

662
00:40:01,700 --> 00:40:06,500
o five o one minus are times one minus are

663
00:40:06,520 --> 00:40:10,700
and one of those all cancel out here

664
00:40:10,750 --> 00:40:14,180
and give

665
00:40:14,200 --> 00:40:19,100
one plus so it comes out quite nicely if i divided into that i'm left

666
00:40:19,100 --> 00:40:22,680
with the one plus are

667
00:40:22,740 --> 00:40:26,540
so i still have one of these guys below and out of one class are

668
00:40:26,540 --> 00:40:30,560
above all that

669
00:40:30,560 --> 00:40:32,270
that tells you

670
00:40:32,270 --> 00:40:34,040
to science show it

671
00:40:34,050 --> 00:40:38,150
how to decide to find out how what where

672
00:40:38,160 --> 00:40:40,940
not science is shown

673
00:40:40,950 --> 00:40:45,550
but this experiment for this effect as shown

674
00:40:45,560 --> 00:40:47,160
right now

675
00:40:47,230 --> 00:40:53,140
why do we quote fineman

676
00:40:53,150 --> 00:40:55,700
because he's an expert

677
00:40:58,680 --> 00:41:00,000
the literally

678
00:41:00,010 --> 00:41:06,440
expert that the etymology of expertise it means someone who has done experiments

679
00:41:08,100 --> 00:41:12,640
we called him because what he says makes sense

680
00:41:12,690 --> 00:41:18,560
so logic is the fourth way of knowing things so the two ways that we

681
00:41:18,710 --> 00:41:24,350
that we know things in chemistry or in science are experiment and logic and that

682
00:41:24,350 --> 00:41:28,910
the lecture is a little bit more focused on logic layer is more focused on

683
00:41:28,910 --> 00:41:31,430
experiment and you get an unbalanced view

684
00:41:31,490 --> 00:41:34,470
if you do one without the other

685
00:41:34,480 --> 00:41:39,430
OK so modern science got underway in the seventeenth century there's the seventeenth century sixteen

686
00:41:39,430 --> 00:41:41,080
hundred seventeen

687
00:41:41,350 --> 00:41:44,790
and sixteen thirty eight was when new haven

688
00:41:44,840 --> 00:41:46,990
colony was founded

689
00:41:47,050 --> 00:41:51,860
and seventeen one was when yale was founded so that's when everything got underway just

690
00:41:51,860 --> 00:41:54,760
when this enterprise was beginning here

691
00:41:54,770 --> 00:41:56,910
here we are

692
00:41:56,930 --> 00:42:00,040
if you go back one hundred years you get to quantum

693
00:42:00,060 --> 00:42:02,220
quantisation by plaque

694
00:42:02,320 --> 00:42:03,890
and we'll talk about that

695
00:42:03,940 --> 00:42:07,010
and if you go back another hundred years you get to love was the iron

696
00:42:08,330 --> 00:42:10,050
and we'll talk about that

697
00:42:10,100 --> 00:42:13,720
and if you get another hundred years you get the new then gravitation and we'll

698
00:42:13,720 --> 00:42:15,880
talk a little bit about that

699
00:42:15,890 --> 00:42:19,240
and if you go back home or another hundred years you get hopper nickerson and

700
00:42:19,520 --> 00:42:21,030
revolution other

701
00:42:21,040 --> 00:42:22,800
heavenly bodies

702
00:42:22,810 --> 00:42:25,850
and columbus and navigation

703
00:42:25,860 --> 00:42:28,610
and luther and the reformation

704
00:42:28,710 --> 00:42:31,820
and these things all have something in common

705
00:42:32,690 --> 00:42:34,820
as robert hook road

706
00:42:34,830 --> 00:42:40,730
in the seventeenth century his age was an age of all others the most inquisitive

707
00:42:40,740 --> 00:42:44,990
all these things have to do with to do with people inquiring into how people

708
00:42:44,990 --> 00:42:48,120
know things and finding out new things

709
00:42:48,890 --> 00:42:54,610
and in particular an important figure was was francis bacon who were in his in-store

710
00:42:55,620 --> 00:43:00,170
you may not know the inspiration so let's look at that here's francis bacon there's

711
00:43:00,170 --> 00:43:01,310
years he

712
00:43:01,360 --> 00:43:04,780
he was elizabethan and jacobean

713
00:43:04,870 --> 00:43:08,750
he was almost exactly contemporary with shakespeare

714
00:43:08,840 --> 00:43:11,530
and with galileo

715
00:43:11,590 --> 00:43:12,790
he went to school

716
00:43:12,790 --> 00:43:15,550
at the university of cambridge

717
00:43:15,560 --> 00:43:17,310
and here's the cartoon

718
00:43:17,360 --> 00:43:18,930
it shows a man

719
00:43:18,950 --> 00:43:23,590
imagine to modern cartoon imagine in class

720
00:43:23,640 --> 00:43:29,200
cambridge because he wrote of his tutors cambridge they were men of sharp wits

721
00:43:29,230 --> 00:43:36,110
shut up in their cells of a few authors chiefly aristotle their dictator

722
00:43:36,130 --> 00:43:42,010
all the philosophy of nature philosophy and that science in those days all the philosophy

723
00:43:42,010 --> 00:43:44,080
of nature which has now received

724
00:43:44,100 --> 00:43:47,130
it is either the philosophy of the regions

725
00:43:47,140 --> 00:43:50,750
or that of the alchemists

726
00:43:50,750 --> 00:43:52,230
the one is gathering

727
00:43:52,250 --> 00:43:56,570
out of the few vulgar that means common of course observations

728
00:43:56,670 --> 00:44:01,300
and the other out of the few experiments of the furnace

729
00:44:01,520 --> 00:44:05,880
the one never failed to multiply words

730
00:44:05,900 --> 00:44:08,290
and the other ever fail us

731
00:44:08,330 --> 00:44:11,490
the multiplying gold

732
00:44:11,500 --> 00:44:16,440
so here's the book he wrote the in-store asian that's the brothers piece for this

733
00:44:16,440 --> 00:44:19,500
pictures from the binding library with them

734
00:44:19,510 --> 00:44:21,480
that picture the book

735
00:44:21,670 --> 00:44:28,290
notice that was published in sixty twenty what else happened

736
00:44:28,300 --> 00:44:31,130
that's when the pilgrims came or

737
00:44:31,150 --> 00:44:35,210
so the title of the book rather small under his name and the title was

738
00:44:35,210 --> 00:44:40,290
lord chancellor of england is in store at model which means the great restoration

739
00:44:40,400 --> 00:44:43,840
restoration of one of the way of knowing

740
00:44:44,500 --> 00:44:48,530
this is the a bigger part of it is called the novum organon which is

741
00:44:48,940 --> 00:44:53,890
then it develops the inductive scientific method based on experiment

742
00:44:53,920 --> 00:44:58,660
to replace aristotelian deduction which is you may be did one experiment sometime in a

743
00:44:58,660 --> 00:45:01,310
new reason everything from that

744
00:45:01,470 --> 00:45:05,690
he says no you have to do more experiments now there's an interesting thing here

745
00:45:05,700 --> 00:45:10,490
on the device on one of the devices on the title and this from piece

746
00:45:10,740 --> 00:45:13,320
is two pillars

747
00:45:13,340 --> 00:45:16,400
one of the world are they doing there

748
00:45:16,420 --> 00:45:18,730
well the the same pillars

749
00:45:18,790 --> 00:45:20,920
you see on this

750
00:45:20,970 --> 00:45:25,000
that's a piece of a you know treasure island pieces of a

751
00:45:25,040 --> 00:45:27,570
see it's a real is

752
00:45:27,590 --> 00:45:32,680
and it came from the silver of mexico was minted in mexico city

753
00:45:34,700 --> 00:45:39,590
and there you see the same pillars and on them says plus ultra

754
00:45:43,220 --> 00:45:51,340
beyond what

755
00:45:51,380 --> 00:45:55,790
what are the pillars

756
00:45:58,560 --> 00:46:01,040
it's africa and spain

757
00:46:01,120 --> 00:46:02,080
but it's

758
00:46:02,100 --> 00:46:04,070
the pillars of hercules

759
00:46:04,080 --> 00:46:07,870
which are the mouth of the mediterranean the old classical world

760
00:46:07,880 --> 00:46:12,280
right so there is the mountain of moses in morocco and the mountain of tariq

761
00:46:12,280 --> 00:46:14,850
which is the name of gibraltar

762
00:46:15,510 --> 00:46:19,260
so here's the mediterranean the classical world of aristotle

763
00:46:19,270 --> 00:46:22,120
and you can say out into the new world

764
00:46:22,190 --> 00:46:25,230
and bring back silver for example

765
00:46:27,000 --> 00:46:30,190
there's danger of course

766
00:46:31,390 --> 00:46:35,030
but look at what it says at the bottom

767
00:46:35,030 --> 00:46:37,790
what will be brought back

768
00:46:37,850 --> 00:46:39,840
not just silver

769
00:46:39,850 --> 00:46:44,250
multi paratransit want out gaby tour CNT

770
00:46:44,330 --> 00:46:46,690
many will pass through

771
00:46:46,710 --> 00:46:49,480
and knowledge will be increased

772
00:46:49,530 --> 00:46:56,360
right so we go beyond aristotle and experimentally based science and knowledge will be increased

773
00:46:56,380 --> 00:47:00,550
so here are some quotes from the instructs your model

774
00:47:00,630 --> 00:47:04,290
that was the which we have derived principally from the greeks

775
00:47:04,290 --> 00:47:05,900
no offense

776
00:47:07,530 --> 00:47:11,830
it is a but like the boyhood of knowledge

777
00:47:11,840 --> 00:47:16,010
and has the characteristic property of boys

778
00:47:16,010 --> 00:47:18,110
it can talk

779
00:47:18,170 --> 00:47:22,320
but it can not generate

780
00:47:22,460 --> 00:47:26,040
it is but the device for exempting ignorance

781
00:47:26,110 --> 00:47:28,280
from ignominy

782
00:47:28,280 --> 00:47:31,620
that means it's a way of hiding your ignorance

783
00:47:31,650 --> 00:47:35,500
and we'll see you can see examples of that will talk about in lecture eleven

784
00:47:35,500 --> 00:47:37,360
about correlation energy

785
00:47:37,380 --> 00:47:40,750
and we'll talk it lecture thirty two about strain energy

786
00:47:40,790 --> 00:47:44,110
and you'll see that both of these are just words that are used to hide

787
00:47:44,110 --> 00:47:46,780
our ignorance

788
00:47:47,000 --> 00:47:53,740
the and which the science of mind proposes is the invention not of arguments

789
00:47:53,790 --> 00:47:55,290
but of arts

790
00:47:55,340 --> 00:47:57,530
ways of doing things

791
00:47:57,580 --> 00:48:00,420
not so much by instruments

792
00:48:00,490 --> 00:48:04,080
although new instruments report light microscopes and so on

793
00:48:04,150 --> 00:48:08,920
as by experiments skillfully and artificially devised

794
00:48:08,920 --> 00:48:14,810
for the express purpose of determining the point in question so artificial experiments designed to

795
00:48:14,810 --> 00:48:17,780
decide the question

796
00:48:17,780 --> 00:48:18,570
mean group

797
00:48:18,590 --> 00:48:20,880
you picked up right

798
00:48:20,880 --> 00:48:23,210
i was just

799
00:48:23,210 --> 00:48:24,420
there are OK

800
00:48:24,470 --> 00:48:27,930
so here we see a five membered ring created

801
00:48:27,980 --> 00:48:31,540
so here's the thing is not free swing out in free space created a five

802
00:48:31,540 --> 00:48:35,920
member group called the where the end of the site actually covalently linked to

803
00:48:37,300 --> 00:48:42,550
and then also has implications for the structure of proteins because this particular amino acid

804
00:48:42,560 --> 00:48:49,280
whenever it occurs within a polypeptide it doesn't have the flexibility of assuming certain configurations

805
00:48:49,310 --> 00:48:51,240
and the other one was

806
00:48:51,280 --> 00:48:54,890
those four citations are not so encumbered

807
00:48:54,890 --> 00:48:58,970
none of them has total flexibility but this one is far more common in the

808
00:48:58,970 --> 00:49:04,000
in the kind of three-dimensional structures that can assume

809
00:49:04,050 --> 00:49:07,300
and with that in mind we begin to

810
00:49:07,350 --> 00:49:10,000
questions about how

811
00:49:10,000 --> 00:49:13,570
polypeptide chains soon three-dimensional structure

812
00:49:13,590 --> 00:49:17,550
if we talk about polypeptide chain

813
00:49:17,640 --> 00:49:19,210
in our minds

814
00:49:21,970 --> 00:49:22,910
there's only

815
00:49:22,920 --> 00:49:25,830
twenty combinations

816
00:49:25,860 --> 00:49:31,040
i go to one of many

817
00:49:31,050 --> 00:49:31,850
right so

818
00:49:31,860 --> 00:49:37,580
look here and you see this is a typical polypeptide

819
00:49:37,590 --> 00:49:43,440
here we have three thriller called true there's a single letter code which was introduced

820
00:49:43,440 --> 00:49:45,770
around nineteen sixty five seventy

821
00:49:45,880 --> 00:49:49,460
each of the twenty amino acids on single letter code

822
00:49:49,500 --> 00:49:51,300
and to make friends

823
00:49:52,800 --> 00:49:54,420
depressing admission

824
00:49:54,450 --> 00:49:59,450
thirty five years forty years after the single amino acid code was instituted

825
00:49:59,500 --> 00:50:01,640
i still have learn

826
00:50:01,720 --> 00:50:04,510
but we can learn the three-letter code which fortunately

827
00:50:04,530 --> 00:50:08,740
our presence here or in single article l is lucene and

828
00:50:09,180 --> 00:50:11,020
these are is

829
00:50:11,040 --> 00:50:14,020
alan is a

830
00:50:14,030 --> 00:50:17,730
this is another example of not being able to teach old dogs

831
00:50:17,830 --> 00:50:19,500
the tree and so

832
00:50:19,560 --> 00:50:21,260
here we see

833
00:50:21,320 --> 00:50:25,950
the late one way by which one might picked enemies of and keep the amino

834
00:50:25,950 --> 00:50:31,890
acid chain polypeptides it keep in mind this can go on indefinitely

835
00:50:31,920 --> 00:50:35,540
as we begin to wrestle with the three-dimensional structure of the chain we begin to

836
00:50:35,540 --> 00:50:37,300
realise the full

837
00:50:37,320 --> 00:50:41,440
and that is that the change is initially synthesized

838
00:50:41,480 --> 00:50:47,410
initially chaotic and it extends increasingly begins to issue

839
00:50:47,520 --> 00:50:49,520
a very specific

840
00:50:49,610 --> 00:50:55,840
molecular three-dimensional molecular configuration which is indicated down here so the operate

841
00:50:57,530 --> 00:51:02,250
will eventually result in a native configuration over here

842
00:51:02,290 --> 00:51:07,240
which is in many respects often referred the lowest free energy state

843
00:51:07,260 --> 00:51:11,280
since the last forty years people have been able to try to figure out if

844
00:51:11,280 --> 00:51:16,130
you know the amino acid sequence of this primary polypeptide here if you know its

845
00:51:16,130 --> 00:51:18,060
primary structure

846
00:51:18,070 --> 00:51:22,430
when i say primary structure what i mean is the sequence of the amino acid

847
00:51:22,490 --> 00:51:26,580
so if you know the primary structure and you should in principle

848
00:51:26,630 --> 00:51:31,890
be able to develop a computer algorithm that would predict the three-dimensional configurations

849
00:51:31,900 --> 00:51:34,130
which is shown university in that way

850
00:51:34,140 --> 00:51:37,320
and which will discuss in much greater detail shortly

851
00:51:37,340 --> 00:51:38,430
and the fact is

852
00:51:38,430 --> 00:51:42,260
after forty years of trying one still is unable to do that

853
00:51:42,280 --> 00:51:44,200
i mean if i were to give

854
00:51:44,260 --> 00:51:47,790
these three the primary amino acid sequence of

855
00:51:47,810 --> 00:51:51,250
polypeptide to the smartest biochemist in the world

856
00:51:51,290 --> 00:51:53,030
there are some very smart ones

857
00:51:53,080 --> 00:51:57,560
he or she could still not tell me what the three-dimensional structure of this protein

858
00:51:57,570 --> 00:52:00,240
with us to certain it would be

859
00:52:00,280 --> 00:52:02,290
why because there's

860
00:52:02,300 --> 00:52:06,290
an almost infinite number of intramolecular interactions

861
00:52:06,380 --> 00:52:08,530
that greatly complicates

862
00:52:08,570 --> 00:52:12,160
how the protein structure

863
00:52:13,180 --> 00:52:18,250
if we this information estimate the protein we can imagine that there's ways of disrupting

864
00:52:18,250 --> 00:52:24,770
that because much of his native state is created by intramolecular hydrogen bond

865
00:52:24,820 --> 00:52:27,200
remember the hyperbolic relatively weak

866
00:52:27,240 --> 00:52:30,100
and if we need to be

867
00:52:30,150 --> 00:52:31,510
the temperature

868
00:52:31,540 --> 00:52:33,480
then we can break hydrogen bonds

869
00:52:33,520 --> 00:52:35,180
and therefore every time

870
00:52:35,190 --> 00:52:37,310
we we friday

871
00:52:37,330 --> 00:52:39,490
for example we want to get down to earth

872
00:52:39,490 --> 00:52:40,920
we within nature

873
00:52:40,970 --> 00:52:47,750
we break the three-dimensional made the three-dimensional structure of albumin molecules that constitute life

874
00:52:47,770 --> 00:52:52,720
so when everything turns white what we've done is taken a molecule like this

875
00:52:52,730 --> 00:52:54,680
he up temperatures

876
00:52:54,710 --> 00:53:02,770
the intramolecular bonds no longer applies notably higher and no longer stabilise three-dimensional configuration

877
00:53:02,790 --> 00:53:05,680
and we put it into a a d nation-state

878
00:53:05,720 --> 00:53:08,770
which might be all the way up here

879
00:53:08,880 --> 00:53:09,780
and therefore

880
00:53:09,790 --> 00:53:14,740
this acquisition of native configuration or native state

881
00:53:15,320 --> 00:53:17,610
we represent the natural state

882
00:53:17,670 --> 00:53:23,090
it is also reversible in many molecules simply by heating them up

883
00:53:23,190 --> 00:53:26,380
there are to be sure you the molecules which are different from a quite from

884
00:53:26,380 --> 00:53:29,940
the help human egg white working-class back down

885
00:53:29,960 --> 00:53:33,380
well spontaneously we assume their native structure

886
00:53:33,390 --> 00:53:36,530
many proteins most will not so

887
00:53:36,550 --> 00:53:37,330
well how

888
00:53:37,340 --> 00:53:42,070
OK let's go back to to the issue of the position of complex three-dimensional structure

889
00:53:42,110 --> 00:53:44,230
and here we begin to see how

890
00:53:44,240 --> 00:53:46,420
some of this

891
00:53:46,460 --> 00:53:49,040
structures acquired stabilised

892
00:53:49,100 --> 00:53:52,360
through these intramolecular hydrogen bond

893
00:53:52,410 --> 00:53:57,030
and there are many opportunities for the intramolecular hydrogen bonds because here we see one

894
00:53:57,040 --> 00:53:59,700
polypeptide chain here we see another

895
00:53:59,710 --> 00:54:00,700
and we see

896
00:54:00,720 --> 00:54:05,130
the NH group right here and so the nitrogen with

897
00:54:05,190 --> 00:54:07,190
the proton side

898
00:54:07,230 --> 00:54:10,030
and the carbonyl group here with the oxygen

899
00:54:10,050 --> 00:54:16,750
are not encumber they are in principle available to form hydrogen bonds with polypeptide chain

900
00:54:17,000 --> 00:54:18,270
somewhere else

901
00:54:18,280 --> 00:54:24,150
now this other polypeptide chain could once again from another protein from another polypeptide

902
00:54:24,150 --> 00:54:27,590
but more often than not were once again dealing with

903
00:54:31,610 --> 00:54:37,090
but in this case the intramolecular crossings are not disulfide bonds are covalent or to

904
00:54:37,170 --> 00:54:38,500
stabilize the iraq

905
00:54:38,550 --> 00:54:43,700
in the absence of reducing agents here we're talking about much weaker bonds hydrogen bonds

906
00:54:43,710 --> 00:54:45,220
which also

907
00:54:45,310 --> 00:54:46,430
in between

908
00:54:46,460 --> 00:54:49,780
different looks of the protein

909
00:54:49,840 --> 00:54:56,640
and served once again to stabilise the three-dimensional structure of the native state proteins

910
00:54:56,640 --> 00:55:01,450
you can see how the opportunities for for performing multiple hydrogen bonds to create an

911
00:55:01,450 --> 00:55:02,770
enormous degree

912
00:55:02,810 --> 00:55:05,210
of stability

913
00:55:05,300 --> 00:55:10,610
and here are some examples of what we now call the secondary structure of the

914
00:55:10,610 --> 00:55:20,160
i think this is the organisers for letting me to present this work we see

915
00:55:20,170 --> 00:55:25,520
which is joined with michael jordan from UC berkeley kept concrete from the san diego

916
00:55:25,550 --> 00:55:28,250
and as a nationally we from silicon valley

917
00:55:28,270 --> 00:55:30,220
so my today

918
00:55:30,220 --> 00:55:32,220
it's too

919
00:55:32,220 --> 00:55:34,950
so that my go today's two

920
00:55:34,970 --> 00:55:36,000
french you

921
00:55:36,000 --> 00:55:39,730
recent class of techniques from machine learning

922
00:55:39,770 --> 00:55:44,890
they only can methods and how they can be applied to computer vision

923
00:55:44,910 --> 00:55:47,580
so we first have to show that many

924
00:55:47,630 --> 00:55:51,280
vision tasks can be expressed as a machine learning problem

925
00:55:51,300 --> 00:55:54,410
and i will present a kind of methods

926
00:55:54,420 --> 00:55:59,340
essentially the opposite of that we show you how you can do kind of design

927
00:55:59,340 --> 00:56:05,520
for simple vision experiment and then i will go quickly over links between economists methods

928
00:56:05,520 --> 00:56:10,380
and sparsity inducing norms that have been talked about for the this morning

929
00:56:10,380 --> 00:56:15,520
so i guess this is no surprise that there is more and more pictures the

930
00:56:15,520 --> 00:56:19,720
world whether you are in the industry or in a research lab at home

931
00:56:19,770 --> 00:56:24,780
and you can indicate millions of images of about of the pictures and from those

932
00:56:24,780 --> 00:56:27,770
pictures you have a lot of tasks to be solved and we try to show

933
00:56:27,770 --> 00:56:30,200
you in a few slides the next slides

934
00:56:30,220 --> 00:56:34,860
it that each of them you should be given machine learning problem want to to

935
00:56:34,860 --> 00:56:38,160
the task on images to imagine learning problem

936
00:56:38,250 --> 00:56:43,500
and we all be of the same data images would be different different kind

937
00:56:43,500 --> 00:56:46,300
the first one is image retrieval that we

938
00:56:46,310 --> 00:56:48,970
what about in the previous talk

939
00:56:48,970 --> 00:56:53,390
so he this is a sample of the query for good we are so

940
00:56:53,390 --> 00:56:56,590
that's what new york and of good images and this is what you get

941
00:56:56,610 --> 00:57:02,410
and they are all those images at the moment i created the system they were

942
00:57:02,560 --> 00:57:08,110
retrieved only based on surrounding text OK and you can see that would get is

943
00:57:08,110 --> 00:57:12,030
a diverse in terms of images we have pictures of new york

944
00:57:12,030 --> 00:57:16,140
we have maps of new york city maps of the state of new york

945
00:57:17,660 --> 00:57:23,230
you will have to be able to maybe go beyond the surrounding text and more

946
00:57:23,230 --> 00:57:26,290
classification maybe i want to classify

947
00:57:26,300 --> 00:57:28,800
real images from from pictures

948
00:57:28,890 --> 00:57:34,050
you may also want to do outlier detection remove what's really not new york so

949
00:57:34,080 --> 00:57:38,510
there's not so many of those that is the case for a new query paris

950
00:57:38,550 --> 00:57:41,050
and then you can see maps of paris

951
00:57:41,080 --> 00:57:43,330
you have to our right

952
00:57:43,360 --> 00:57:47,390
maps of paris years again i think that this is a key players

953
00:57:47,400 --> 00:57:51,010
and this is what this list is the task of

954
00:57:51,450 --> 00:57:55,110
being able to look at the image it removes some of the players so these

955
00:57:55,140 --> 00:57:57,770
could be an outlier detection problem

956
00:57:57,830 --> 00:58:02,050
similarly you might want to do simple image annotation so you the bank of images

957
00:58:02,050 --> 00:58:06,790
and want to annotate from the animals plain so all is

958
00:58:06,830 --> 00:58:12,620
this is in fact my most changing to me is the set of personal photos

959
00:58:12,640 --> 00:58:16,260
so i guess as baby get you take one megabyte

960
00:58:16,270 --> 00:58:21,110
amounts for the first months in the indicates that of the like the babies but

961
00:58:21,110 --> 00:58:25,010
don't get additional babies to go back to when you go back months and then

962
00:58:25,010 --> 00:58:29,210
you get like many go back in your laptop on your new computer and you

963
00:58:29,210 --> 00:58:30,490
want to organize that

964
00:58:30,990 --> 00:58:33,370
it is organised by that time

965
00:58:33,420 --> 00:58:38,110
you to be able to say all the pictures from kennedy

966
00:58:38,150 --> 00:58:41,580
give me all the pictures on k two and all the pages and get three

967
00:58:41,580 --> 00:58:44,010
so this is a change because of kids

968
00:58:44,020 --> 00:58:47,700
OK to make it look like a lot so it's very hard to distinguish

969
00:58:47,760 --> 00:58:49,200
so with

970
00:58:49,210 --> 00:58:50,770
pictures photos

971
00:58:50,770 --> 00:58:54,610
you have a lot of work to do in terms of working on images you

972
00:58:54,610 --> 00:58:59,230
want to the classification can one can toolkits three wanted to to clustering

973
00:58:59,360 --> 00:59:01,450
maybe discover new kid on the block

974
00:59:02,260 --> 00:59:05,110
this something that some of these accusations

975
00:59:05,490 --> 00:59:10,800
namely how i how can you look at this a few bytes of images

976
00:59:10,920 --> 00:59:13,580
so what i presented is many

977
00:59:13,600 --> 00:59:18,680
twenty task involved with images i want to classify them to them calling them in

978
00:59:18,680 --> 00:59:19,520
some way

979
00:59:19,540 --> 00:59:21,320
and all of those

980
00:59:21,330 --> 00:59:26,330
are the common issues that work on images so the goal is not to deny

981
00:59:26,330 --> 00:59:31,880
one image but to work on the images so immediately massive that data OK so

982
00:59:31,880 --> 00:59:36,620
you have like millions of images which is an sense of millions of pixels so

983
00:59:36,620 --> 00:59:39,100
you need to be careful what you do

984
00:59:39,150 --> 00:59:44,590
the stats complex things that does not for example it's even hard for may hard

985
00:59:44,590 --> 00:59:48,680
for humans and finally the data of very heterogeneous

986
00:59:48,690 --> 00:59:53,390
even for images you have a lot of ways of looking at images you might

987
00:59:53,390 --> 00:59:58,640
want to consider color texture shape and so on so even within one minute you

988
00:59:58,780 --> 01:00:01,070
get a lot of heterogeneity

989
01:00:01,090 --> 01:00:03,710
and of course if you want to consider all the images as well as well

990
01:00:03,760 --> 01:00:08,420
as like text and in some you really have to be careful was also so

991
01:00:08,560 --> 01:00:12,970
type of data so let me try to convince you of today that

992
01:00:12,980 --> 01:00:17,570
kernel methods are well adapted to the task so every community it's all so my

993
01:00:17,570 --> 01:00:18,600
q on

994
01:00:18,900 --> 01:00:23,130
will be a positive definite kernel and we present in slides

995
01:00:23,160 --> 01:00:24,540
before going to that

996
01:00:24,610 --> 01:00:30,160
let me put into what machine learning about supervised machine learning you want to predict

997
01:00:30,170 --> 01:00:34,110
we have some input x this this image

998
01:00:34,110 --> 01:00:37,590
and why is your outputs that you want to predict let's say the identity of

999
01:00:37,590 --> 01:00:39,290
the person in the image

1000
01:00:39,300 --> 01:00:44,800
o with basically know and most of machine learning and statistics and also some processing

1001
01:00:44,800 --> 01:00:48,990
is framed as an optimisation problem when you have an objective function which is the

1002
01:00:48,990 --> 01:00:51,220
sum of the that fitting term

1003
01:00:51,230 --> 01:00:56,170
jamaican data when you predict for exciting instead of why i'm so if is how

1004
01:00:56,170 --> 01:00:58,400
you predict from exile i

1005
01:00:58,490 --> 01:01:03,440
then you have the organisations are usually not so have two main issues in machine

1006
01:01:03,440 --> 01:01:06,420
learning which is which is hard to design a loss

1007
01:01:06,430 --> 01:01:10,670
for specific tasks of course what you know what the symbol whether you're trying to

1008
01:01:11,550 --> 01:01:15,060
clustering classification regression model that

1009
01:01:15,070 --> 01:01:16,000
the y

1010
01:01:16,010 --> 01:01:20,170
but then you have to decide which function space you want to consider how will

1011
01:01:20,170 --> 01:01:21,860
i predict from data

1012
01:01:23,100 --> 01:01:26,420
so it not just with into the first the of main two

1013
01:01:27,070 --> 01:01:30,430
mental tasks which is the regression classification

1014
01:01:30,460 --> 01:01:33,230
but you could design the for all possible

1015
01:01:33,240 --> 01:01:35,400
that described earlier

1016
01:01:35,440 --> 01:01:39,240
so far regression against everybody uses quite cast list

1017
01:01:39,300 --> 01:01:44,800
because between what you want to predict and your new output this is

1018
01:01:45,100 --> 01:01:50,130
very simple important for classification this i wanted to binary classification you want to predict

1019
01:01:50,190 --> 01:01:52,650
the labeling minus one one

1020
01:01:52,660 --> 01:01:56,300
of course if they belong to the space so how do you get from the

1021
01:01:57,250 --> 01:02:01,300
two the manifold wanted to sign

1022
01:02:01,360 --> 01:02:03,490
so you want to design a loss

1023
01:02:03,500 --> 01:02:11,880
which would be adapted to predicting the sign to be adapted to do the sign

1024
01:02:11,880 --> 01:02:14,070
of the of function of a function

1025
01:02:14,100 --> 01:02:18,230
so of course in the canal if only the sign this i disagree

1026
01:02:18,250 --> 01:02:21,550
because if you have the same sign you ought to get if you make a

1027
01:02:21,550 --> 01:02:26,050
lot of positive in the mail on sunday and you want to consider when you

1028
01:02:26,560 --> 01:02:31,240
y f one x have different signs so put away with the product by f

1029
01:02:31,240 --> 01:02:33,260
again has to do with parameter sharing

1030
01:02:33,810 --> 01:02:39,020
and the parameters of the mean time model can be trained that can be tuned using backpropagation algorithm

1031
01:02:40,110 --> 01:02:43,700
and the backdrop algorithm you know it's been it's been around for quite some time

1032
01:02:46,610 --> 01:02:51,010
so one question you might want to ask is is the you know top-down bottom-up

1033
01:02:51,180 --> 01:02:55,750
can we actually design algorithms that maybe have some kind of feedback connections

1034
01:02:56,370 --> 01:02:58,520
and is there more rigorous mathematical formulation

1035
01:02:59,740 --> 01:03:03,600
obviously a lot of these things are driven by empirical success and and yes there's

1036
01:03:03,600 --> 01:03:06,620
a lot of empirical success but the question is is there a little bit so

1037
01:03:06,660 --> 01:03:08,840
something more we can say about these models

1038
01:03:11,280 --> 01:03:16,650
so let me give you a little bit of introduction to graphical model just so

1039
01:03:16,650 --> 01:03:19,400
that we can get the notation right this as we go through the year

1040
01:03:19,690 --> 01:03:20,580
the rest of the tutorial

1041
01:03:21,610 --> 01:03:23,480
how many of you know about graphical models

1042
01:03:26,000 --> 01:03:28,580
what so so i can go i can go fairly quickly there

1043
01:03:29,150 --> 01:03:32,830
just primarily for setting up for setting up the notation forests

1044
01:03:33,380 --> 01:03:38,550
right so it's it's interesting if you look at the history of deep learning started with basically

1045
01:03:39,200 --> 01:03:41,510
looking at these models from graphical model perspective

1046
01:03:42,260 --> 01:03:46,060
right now there are a lot of different algorithms like sparse coding and auditing courses but

1047
01:03:48,120 --> 01:03:52,440
initial models were inspired by graphical models and doing inference and learning graphical models

1048
01:03:53,350 --> 01:03:58,560
so graphical models it's a very powerful framework for representing dependency structure between random variables

1049
01:04:01,000 --> 01:04:02,910
typically when we look at graphical models

1050
01:04:05,500 --> 01:04:08,540
the graph itself will contain a set of nodes vertices

1051
01:04:09,260 --> 01:04:10,940
that's will represent random variables

1052
01:04:11,560 --> 01:04:12,730
and we'll have links that

1053
01:04:13,490 --> 01:04:16,690
tell us something about dependencies between those random variables

1054
01:04:17,510 --> 01:04:22,410
in the joint distribution over those random variables can be decomposed into the product of factors

1055
01:04:24,790 --> 01:04:29,050
and each one of those factors is looking at a subset of all four of these

1056
01:04:30,970 --> 01:04:31,870
subset of the nodes

1057
01:04:34,050 --> 01:04:35,470
there are two types of models

1058
01:04:36,790 --> 01:04:39,950
there are directed graphical models something is called bayesian networks

1059
01:04:40,370 --> 01:04:41,900
and they've been around for quite some time

1060
01:04:42,360 --> 01:04:47,000
and there are also undirected graphical models and these known as markov random fields boltzmann machines

1061
01:04:47,910 --> 01:04:51,000
you know in this tutorial will be focusing on a bit more an undirected part

1062
01:04:51,390 --> 01:04:54,830
just because a lot of models a of these models come from undirected graphical models

1063
01:04:56,330 --> 01:04:58,450
but there are also hybrid graphical models

1064
01:04:59,490 --> 01:05:04,740
and we'll see some of goals and hybrid graphical models model that combined both directed and undirected models together

1065
01:05:05,960 --> 01:05:10,820
so for example you know deep belief networks for those of you who heard about deep belief networks

1066
01:05:11,320 --> 01:05:15,550
these actually hybrid graphical models they have both undirected and directed parts to it

1067
01:05:16,850 --> 01:05:18,450
ones like hierarchical deep models

1068
01:05:20,520 --> 01:05:21,510
and the idea behind

1069
01:05:22,190 --> 01:05:27,490
directed graphical models it's it's directed graphical models actually used a lot in the machine learning

1070
01:05:28,110 --> 01:05:32,530
and they useful for expressing causal relationship between random variables

1071
01:05:34,840 --> 01:05:36,550
so for example in this picture here

1072
01:05:37,260 --> 01:05:41,230
right the joint distribution that specifies this particular

1073
01:05:41,740 --> 01:05:45,840
as consistent with this particular graph is you can say well my joint distribution with

1074
01:05:45,840 --> 01:05:48,930
these random variables which is gonna be written as a product of

1075
01:05:51,120 --> 01:05:54,000
as the conditional distributions of nodes given the parents of the nodes

1076
01:05:54,740 --> 01:05:58,590
right so for example in this case if i look at the joint distribution

1077
01:05:59,090 --> 01:06:03,230
of these seven random variables then it factorises precisely this way

1078
01:06:03,800 --> 01:06:06,210
there is x one peel next appeal x three

1079
01:06:06,770 --> 01:06:12,790
if you look at x four r x four has parents x two x one and x three so that's

1080
01:06:13,260 --> 01:06:14,260
and all that i have here

1081
01:06:16,110 --> 01:06:20,650
end notice that the graphical model tells me something about how the distribution factorizes

1082
01:06:23,990 --> 01:06:27,290
and this is known as a directed acyclic graph with dark

1083
01:06:28,540 --> 01:06:33,910
and it you know graphical models in particular directed graphical models are very useful you been used a lot in

1084
01:06:34,750 --> 01:06:38,140
a lot of different domains but you know maybe one example

1085
01:06:38,650 --> 01:06:41,950
it is just a show you is you know how would you generate an image

1086
01:06:42,530 --> 01:06:43,400
well you might have

1087
01:06:43,900 --> 01:06:46,690
the latent variables like object position orientation

1088
01:06:48,130 --> 01:06:52,230
and then we all of these two random variables might have independent priors

1089
01:06:53,260 --> 01:06:56,120
and then the way you generating the image

1090
01:06:56,600 --> 01:07:00,390
is you say well if i know the object identity the position and orientation

1091
01:07:00,950 --> 01:07:02,960
then i'm gonna have a likelihood function here

1092
01:07:03,510 --> 01:07:06,780
that essentially tells me what's the probability of the image given these two

1093
01:07:07,220 --> 01:07:08,050
variables right

1094
01:07:08,500 --> 01:07:10,970
so given these two random variables i can generate the image

1095
01:07:12,110 --> 01:07:14,840
you know this is sort of a canonical way of how people

1096
01:07:14,840 --> 01:07:20,180
OK so the people are known more about segmentation in various different probabilities maybe they

1097
01:07:20,330 --> 01:07:22,070
chosen different

1098
01:07:22,090 --> 01:07:24,250
but can we go from that so

1099
01:07:24,280 --> 01:07:27,090
step up from segmentation spelling correction

1100
01:07:27,480 --> 01:07:32,660
in egypt were processor used to seeing this square little reliance on my colleague and

1101
01:07:32,660 --> 01:07:38,630
fellow which machine learning connoisseur of the arts army always gets upset

1102
01:07:38,700 --> 01:07:39,980
when he sees little red

1103
01:07:40,020 --> 01:07:42,270
o lines under his name

1104
01:07:42,410 --> 01:07:47,300
and in a common office product when he hits the OK button

1105
01:07:47,310 --> 01:07:48,620
he is that

1106
01:07:48,820 --> 01:07:55,230
which is probably not the most likely capability but you know given that you're going

1107
01:07:55,230 --> 01:07:57,980
from a dictionary and his name was in in the dictionary that's the best you

1108
01:07:57,980 --> 01:08:00,720
can do but if you work from a larger corpus and said maybe you can

1109
01:08:00,720 --> 01:08:01,750
do that

1110
01:08:01,830 --> 01:08:07,630
and so i want to compare get back to this idea of agile programming compare

1111
01:08:07,630 --> 01:08:13,420
this corpus based approach to a more traditional programming based approach

1112
01:08:13,430 --> 01:08:14,590
and so on

1113
01:08:14,600 --> 01:08:18,120
they went on the web and i search for that spelling correctors and i found

1114
01:08:18,120 --> 01:08:19,020
this one

1115
01:08:21,440 --> 01:08:24,790
you know so there's all this stuff in there under the case e

1116
01:08:24,810 --> 01:08:29,020
he said something effects is it's

1117
01:08:29,060 --> 01:08:35,290
CIA's in there were chr LCS is if in CIRC you see why it's dropped

1118
01:08:35,330 --> 01:08:36,270
most i

1119
01:08:36,290 --> 01:08:38,700
CRS USC y

1120
01:08:40,050 --> 01:08:43,560
quick can you verify that they got every case right there

1121
01:08:43,600 --> 01:08:48,730
can anybody did that OK well you know maybe you take you for a little

1122
01:08:48,730 --> 01:08:53,780
while to verify that every possible cases covered there but then that

1123
01:08:53,820 --> 01:08:57,220
trick is give a couple days to do that and that i give you know

1124
01:08:57,220 --> 01:09:00,190
the data say well just let's do simple one more thing and just bought this

1125
01:09:00,190 --> 01:09:01,300
to romanian

1126
01:09:01,310 --> 01:09:05,200
that can be too hard right just modify the total little bits does remain instead

1127
01:09:05,270 --> 01:09:09,900
being how hard could that be and you can see that that's the fragility

1128
01:09:09,950 --> 01:09:14,960
this can code approach as opposed to machine learning approach

1129
01:09:15,680 --> 01:09:17,670
so again

1130
01:09:17,700 --> 01:09:21,540
it's the same old model you fall back to this the probability probably the spelling

1131
01:09:21,540 --> 01:09:26,280
correction it could depends on the probability that the correction sees the word

1132
01:09:26,290 --> 01:09:30,050
and the probability that it's impossible typo for the original word

1133
01:09:30,060 --> 01:09:32,520
best correction is one with the highest probability

1134
01:09:32,530 --> 01:09:36,300
probability of the word we get again by counting just like we did before

1135
01:09:36,330 --> 01:09:39,790
the probability of the type of you some kind of model the a given these

1136
01:09:39,790 --> 01:09:43,820
two words how similar they to each other and how probable one b type of

1137
01:09:43,820 --> 01:09:44,930
for another

1138
01:09:44,970 --> 01:09:49,150
if you've been running a spelling correction service for a long time you've got lots

1139
01:09:49,150 --> 01:09:52,700
of examples of that you really good model of that if you don't you can

1140
01:09:52,700 --> 01:09:57,590
just do something simple like count the edit distance between them and that's what i

1141
01:09:57,710 --> 01:09:59,810
and then there's all program

1142
01:09:59,820 --> 01:10:03,440
it's a little bit longer but not too much again fits on one page

1143
01:10:04,650 --> 01:10:09,410
i i guess i get get something like that i forget the exact number but

1144
01:10:09,410 --> 01:10:12,840
something like eight percent correct with this and if you want to do more

1145
01:10:12,850 --> 01:10:16,460
you probably want to take more context into account and do

1146
01:10:16,470 --> 01:10:21,290
what is probably this word given that it appears before the facts were so

1147
01:10:22,070 --> 01:10:24,850
analysis of a little bit from there

1148
01:10:24,870 --> 01:10:29,730
and say can we do something that's more semantic oriented and this is one of

1149
01:10:29,730 --> 01:10:35,570
the first experiments we did that the google labs google sets them by some time

1150
01:10:35,650 --> 01:10:39,020
and the idea here is to use all the information you have available on the

1151
01:10:39,720 --> 01:10:43,540
to suggest a set of things that are related to each other so here i

1152
01:10:43,540 --> 01:10:46,440
put in our car so anonimity sic

1153
01:10:46,450 --> 01:10:51,160
and i get back matisse picasso and go monday because so

1154
01:10:51,330 --> 01:10:56,310
so it's concentrating on these impressionist artist first and then it has other kinds of

1155
01:10:56,310 --> 01:10:57,940
artists and

1156
01:10:57,950 --> 01:11:03,190
it veers off a little bit now science o atoms is in the bottom there

1157
01:11:03,190 --> 01:11:07,690
is slightly different medium that uses and so on but looks like a pretty good

1158
01:11:10,430 --> 01:11:14,240
if i give the lions and tigers and bears i get back

1159
01:11:14,250 --> 01:11:18,790
tigers lions elephants monkeys giraffes dogs cats snakes and so on looks like it's been

1160
01:11:18,790 --> 01:11:23,330
pretty good the couple things that italics there that are quite right it looks like

1161
01:11:23,490 --> 01:11:26,830
so there's cottonwood musical toddler

1162
01:11:27,610 --> 01:11:31,210
and you might want to think about how to sneak in there

1163
01:11:31,340 --> 01:11:35,320
and we'll see the answers but what you guys do one

1164
01:11:35,610 --> 01:11:40,200
so there are all kind of related animals so what if i gave it two

1165
01:11:40,200 --> 01:11:44,450
different kinds of an animal so what would come up with that

1166
01:11:44,470 --> 01:11:50,210
of course of course

1167
01:11:50,230 --> 01:11:55,450
and then you can see that you know this already from central to peripheral

1168
01:11:55,560 --> 01:11:57,950
so does it out that

1169
01:11:57,950 --> 01:12:00,650
man analysis CPU narayanmurthy

1170
01:12:01,330 --> 01:12:07,200
the typical unix commands and make node in the MIIA are more peripheral

1171
01:12:07,420 --> 01:12:11,020
so does seem to have some kind of understanding

1172
01:12:11,310 --> 01:12:15,080
so how would you do something like this why not show the whole code is

1173
01:12:15,080 --> 01:12:18,860
that doesn't fit on one page building and show you some of the data sources

1174
01:12:18,860 --> 01:12:21,210
so the first thing you can do is just go on the web look for

1175
01:12:22,150 --> 01:12:26,740
and say look like tigers bears curve close to each other maybe they're related and

1176
01:12:26,740 --> 01:12:28,000
that's pretty good clue

1177
01:12:28,130 --> 01:12:32,200
but then you also have to say well some men mostly in but also occur

1178
01:12:32,200 --> 01:12:36,580
in that does that mean they're related so maybe it's not that strong

1179
01:12:36,590 --> 01:12:39,670
but here's a case where they look stronger

1180
01:12:40,580 --> 01:12:45,910
we have HTML structure in the in the case of these list elements in case

1181
01:12:46,080 --> 01:12:52,530
of links and maybe that's a stronger clue that those are related

1182
01:12:52,560 --> 01:12:58,260
and this is example to the puzzler of why those weird things come in

1183
01:12:58,280 --> 01:13:02,990
because the web but does reflect reality and so it does talk about animals but

1184
01:13:02,990 --> 01:13:06,870
the web also reflects an awful lot of commerce and you can't really by lions

1185
01:13:06,870 --> 01:13:08,820
and tigers and bears but you can buy

1186
01:13:08,870 --> 01:13:14,670
stuffed animals and kottler animals and cotton plush toys and so on so those words

1187
01:13:14,670 --> 01:13:17,110
get confused

1188
01:13:17,360 --> 01:13:21,090
another thing you can do is look at your user logs and well somebody was

1189
01:13:21,090 --> 01:13:26,960
looking for a cheetah pics and then leopard pics soon afterwards maybe that's an indication

1190
01:13:27,060 --> 01:13:32,460
and then you can try to understand the structure of english and you can either

1191
01:13:32,470 --> 01:13:37,190
do that with a full parser that understands everything about sentences or you can do

1192
01:13:37,190 --> 01:13:43,360
it with the restricted approaches think marty first perfectly with the first one to suggest

1193
01:13:43,360 --> 01:13:47,810
using phrases that are highly indicative that are our

1194
01:13:47,820 --> 01:13:50,680
high precision but potentially low recall

1195
01:13:50,710 --> 01:13:54,710
and so if you want to find what things are like lions you could search

1196
01:13:54,710 --> 01:13:59,210
for the phrase such as lions and then anything that shows that there is very

1197
01:13:59,210 --> 01:14:03,910
likely that that's going to be in the same sentence lions of course there is

1198
01:14:04,510 --> 01:14:08,460
no countless numbers of different ways that you could say lions and tigers are similar

1199
01:14:08,460 --> 01:14:11,350
and you're gonna miss all the other ones with this but if the web is

1200
01:14:11,350 --> 01:14:14,840
large enough and there's enough examples you get

1201
01:14:15,190 --> 01:14:17,670
what you need

1202
01:14:17,670 --> 01:14:23,190
hey i'm just kidding the right to be indecisive or to have the thing with

1203
01:14:23,190 --> 01:14:27,080
this i haven't made up my mind whether really want to invest in options and

1204
01:14:27,080 --> 01:14:30,010
so by in stocks as well by an option

1205
01:14:30,080 --> 01:14:32,300
that gives me the right to buy

1206
01:14:32,990 --> 01:14:36,190
you know you could say that a lot of people think that way

1207
01:14:36,210 --> 01:14:39,540
like a company will think we're trying to decide whether

1208
01:14:39,540 --> 01:14:41,400
we want to

1209
01:14:41,420 --> 01:14:45,630
build the shopping centre so by an option on land

1210
01:14:45,680 --> 01:14:49,660
underlying what where we would build the shopping centre will think more about it and

1211
01:14:49,660 --> 01:14:52,610
decide whether it's a good idea to build the shopping center

1212
01:14:52,630 --> 01:14:54,760
well that you could do that

1213
01:14:54,780 --> 01:14:59,000
but this is something a little bit misleading about that reasoning

1214
01:15:00,000 --> 01:15:04,810
whether whether or not you decide to build the shopping centre if you buy an

1215
01:15:04,810 --> 01:15:06,330
option on the land

1216
01:15:06,380 --> 01:15:08,590
you will always exercise it

1217
01:15:08,750 --> 01:15:12,300
if it's in the money and exercised it whether you build the shopping centre now

1218
01:15:12,300 --> 01:15:13,910
not right

1219
01:15:13,970 --> 01:15:17,790
i suppose you you you couldn't decide whether to build the shopping centre newport and

1220
01:15:17,810 --> 01:15:19,470
option on land

1221
01:15:19,490 --> 01:15:20,580
and then

1222
01:15:20,630 --> 01:15:23,920
someone comes in and says well we have to make up her mind today

1223
01:15:23,960 --> 01:15:30,040
the land the options exercising is expiring if we don't exercise today is worthless

1224
01:15:30,040 --> 01:15:32,260
so what you discuss it you're meeting

1225
01:15:32,280 --> 01:15:36,620
you don't discuss whether we're going to build the shopping centre and not that's irrelevant

1226
01:15:36,650 --> 01:15:37,920
you discuss

1227
01:15:37,950 --> 01:15:41,960
what can we sell land for and we can sell for more than the exercise

1228
01:15:41,960 --> 01:15:44,780
price we will always exercise it

1229
01:15:44,810 --> 01:15:46,250
there is no

1230
01:15:48,500 --> 01:15:53,890
the assumption in finance is that all options that are in the money and on

1231
01:15:53,910 --> 01:15:55,220
the exercise date

1232
01:15:55,280 --> 01:15:56,780
are exercised

1233
01:15:56,830 --> 01:15:57,770
and there's no

1234
01:15:57,790 --> 01:16:04,090
choice award option might be misleading because you could choose to be dumb and not

1235
01:16:04,090 --> 01:16:06,950
exercise but that's not what it's about

1236
01:16:07,060 --> 01:16:10,910
on the other hand options really are central to our

1237
01:16:10,930 --> 01:16:14,860
i was thinking about a lot of things to i give you an example of

1238
01:16:14,860 --> 01:16:16,670
an option that you might not consider

1239
01:16:16,720 --> 01:16:17,660
an option

1240
01:16:17,740 --> 01:16:24,500
this is the option to marry some happy and so sometimes people complain that their

1241
01:16:24,500 --> 01:16:27,760
boyfriend or girlfriend cannot

1242
01:16:29,850 --> 01:16:34,080
we've been going on for three years it's time that we get married

1243
01:16:34,090 --> 01:16:35,780
but this person

1244
01:16:35,800 --> 01:16:40,760
the other counterparty cannot seem to decide

1245
01:16:40,810 --> 01:16:41,890
but actually

1246
01:16:41,970 --> 01:16:46,350
one view of of that situation could be that this person is just a better

1247
01:16:46,350 --> 01:16:48,920
schooled in finance than the other

1248
01:16:50,420 --> 01:16:56,830
one principal finance is that you should never exercise an american call early

1249
01:16:58,120 --> 01:17:01,520
and that this is article about relationships and i just tell you a story that

1250
01:17:01,520 --> 01:17:02,660
comes to mind

1251
01:17:02,760 --> 01:17:07,530
you never want to come back but you never want to exercise an american call

1252
01:17:07,530 --> 01:17:10,610
early so that's why there's an important distinction between

1253
01:17:10,630 --> 01:17:12,850
european and american

1254
01:17:12,850 --> 01:17:14,890
but just in the prince in the in the

1255
01:17:14,910 --> 01:17:17,490
it's in the case of relationships

1256
01:17:17,510 --> 01:17:22,290
suppose your girlfriend or boyfriend really wants to marry you

1257
01:17:23,940 --> 01:17:25,880
is is still giving you time

1258
01:17:25,890 --> 01:17:28,280
then you instinctively notion which

1259
01:17:28,320 --> 01:17:32,940
this law and so not really perhaps in terms of theory you should wait until

1260
01:17:32,940 --> 01:17:35,290
the last day when this other person

1261
01:17:35,300 --> 01:17:37,430
he says it's not our never

1262
01:17:37,750 --> 01:17:41,970
because there's always exercise hours option values are is the chance

1263
01:17:43,500 --> 01:17:47,330
maybe that will become clear and i feel like my knowledge and not cynical these

1264
01:17:47,330 --> 01:17:49,620
things have some people who are

1265
01:18:01,760 --> 01:18:04,330
this is a call option up here but

1266
01:18:04,380 --> 01:18:07,600
this is the call i'm gonna show you what it but go ahead

1267
01:18:07,610 --> 01:18:17,040
what our tries

1268
01:18:17,050 --> 01:18:20,270
so the price and come back to the price of the option will always be

1269
01:18:20,270 --> 01:18:22,340
above that line

1270
01:18:22,350 --> 01:18:24,870
so there is no evidence that we

1271
01:18:24,880 --> 01:18:28,800
there are possible it depends on if the price is wrong arbitrage when you come

1272
01:18:28,800 --> 01:18:29,670
back to that

1273
01:18:29,690 --> 01:18:32,150
i this is a put option

1274
01:18:32,160 --> 01:18:35,010
this is intrinsic value for a put option

1275
01:18:35,070 --> 01:18:37,350
it's the opposite of the call

1276
01:18:37,580 --> 01:18:42,990
if the strike price is twenty and the stock price signed for fifteen

1277
01:18:43,010 --> 01:18:46,250
then you can see that it in the money it

1278
01:18:46,290 --> 01:18:49,740
right because you can make five dollars by exercising

1279
01:18:49,750 --> 01:18:51,390
you have

1280
01:18:51,410 --> 01:18:53,770
you have the right to sell for twenty

1281
01:18:53,780 --> 01:18:56,710
but you can buy it in the market for fifteen so you buy for fifteen

1282
01:18:56,710 --> 01:18:59,410
cell twenty make five dollars

1283
01:18:59,460 --> 01:19:03,140
but on the other hand up here if have stock prices thirty dollars

1284
01:19:03,170 --> 01:19:06,600
you have the right to sell it for twenty one that's worth nothing right

1285
01:19:06,630 --> 01:19:09,120
i can sell it for thirty in the market

1286
01:19:10,350 --> 01:19:12,990
but then what

1287
01:19:13,380 --> 01:19:16,670
so do i have

1288
01:19:16,750 --> 01:19:21,250
make me jump to this isn't exactly what i wanted to do

1289
01:19:23,460 --> 01:19:27,540
this has to do with our trying using the

1290
01:19:27,540 --> 01:19:29,340
the price in the market

1291
01:19:29,350 --> 01:19:31,340
should always be greater

1292
01:19:31,350 --> 01:19:33,170
any intrinsic value

1293
01:19:33,220 --> 01:19:35,790
until the exercised it

1294
01:19:35,800 --> 01:19:36,970
the last

1295
01:19:37,000 --> 01:19:41,680
and american way even for american or european this this talk american

1296
01:19:44,930 --> 01:19:47,120
this pink line is my

1297
01:19:47,130 --> 01:19:49,120
the price of the option

1298
01:19:49,140 --> 01:19:53,430
now we will talk about how we get the line from theory but

1299
01:19:54,570 --> 01:19:55,660
if the

1300
01:19:55,690 --> 01:20:00,070
let's see if it's an american option got time to go and say the exercise

1301
01:20:00,080 --> 01:20:03,150
isn't for another year OK

1302
01:20:03,350 --> 01:20:07,170
and it out of the money the price of a share is only fifteen dollars

1303
01:20:07,170 --> 01:20:09,600
but the exercise price is twenty dollars

1304
01:20:09,610 --> 01:20:12,550
that option is still worth something to right

1305
01:20:12,560 --> 01:20:13,880
it's not worthless

1306
01:20:13,900 --> 01:20:16,390
it would be worthless if you exercise today

1307
01:20:16,410 --> 01:20:18,660
hey you're not going to exercise

1308
01:20:18,710 --> 01:20:20,510
the reason it has value

1309
01:20:20,530 --> 01:20:25,270
is that the prices might rise above twenty dollars sometime over the next year

1310
01:20:25,290 --> 01:20:29,860
and so it had the you have a chance of making money

1311
01:20:29,860 --> 01:20:31,400
we go to

1312
01:20:31,430 --> 01:20:33,720
integrated application ontology

1313
01:20:34,110 --> 01:20:41,150
integrating application ontology means developing one last thing

1314
01:20:41,170 --> 01:20:43,010
which is

1315
01:20:43,650 --> 01:20:50,760
we need to bridge ontology

1316
01:20:50,790 --> 01:20:55,620
as you will see in the next slide this bridge ontology is basically

1317
01:20:55,640 --> 01:20:58,880
a set of tripos but says

1318
01:20:58,890 --> 01:21:01,570
this time here is this guy here

1319
01:21:01,600 --> 01:21:08,250
an event here is an event here an artist here is an artist here

1320
01:21:08,260 --> 01:21:09,430
OK so

1321
01:21:09,450 --> 01:21:14,610
it connects the concept in that not more than to the concept in the

1322
01:21:15,600 --> 01:21:18,650
content models

1323
01:21:20,860 --> 01:21:22,710
it what it looks like

1324
01:21:22,770 --> 01:21:24,850
it says

1325
01:21:24,860 --> 01:21:33,050
whenever you find an artist in musicbrainz ontology well it is the next performer

1326
01:21:33,060 --> 01:21:35,980
do you remember this idea of the subsets

1327
01:21:36,010 --> 01:21:39,410
you seeing that perform

1328
01:21:39,520 --> 01:21:41,940
as a subset which are

1329
01:21:42,000 --> 01:21:45,920
the artist in musicbrainz

1330
01:21:45,980 --> 01:21:48,070
these one is maximal

1331
01:21:48,110 --> 01:21:50,680
and this one is musicbrainz

1332
01:21:50,730 --> 01:21:53,160
so whenever you have an instance here

1333
01:21:53,180 --> 01:21:55,000
your seven instance here

1334
01:21:55,000 --> 01:21:57,120
so you can query using these terms

1335
01:21:57,220 --> 01:22:00,800
even if the data where everything in these terms

1336
01:22:00,800 --> 01:22:05,780
this is that it's nothing more than that of course in reality is not that

1337
01:22:06,950 --> 01:22:12,070
you we said one-to-one mapping which basically calling for

1338
01:22:12,140 --> 01:22:14,080
why did you do that

1339
01:22:14,090 --> 01:22:17,230
i mean you should have adopted ontology and that was

1340
01:22:17,750 --> 01:22:22,340
in reality what happens quite often is that you have a one to many mapping

1341
01:22:22,430 --> 01:22:26,660
or you need the rule translate from these ontology to his ontology

1342
01:22:26,940 --> 01:22:31,250
so multiple data here are represented as a concept here

1343
01:22:31,290 --> 01:22:35,870
but in this tutorial and then we just use one-to-one mapping as a way to

1344
01:22:35,870 --> 01:22:40,470
develop this bridge ontology so the concept is useful they way it is done is

1345
01:22:40,480 --> 01:22:43,560
trivial but because this tutorial

1346
01:22:43,830 --> 01:22:47,680
here we use of this subproperty

1347
01:22:47,900 --> 01:22:50,860
so we are saying that the related artists

1348
01:22:50,900 --> 01:22:53,440
musicbrainz is a way

1349
01:22:53,450 --> 01:22:58,340
to be related performance in next i guess but you remember that i do

1350
01:22:58,340 --> 01:23:00,580
these things

1351
01:23:00,770 --> 01:23:05,320
so this was the relation related artists here what i'm saying is that there is

1352
01:23:05,320 --> 01:23:08,900
another property is one that really performance

1353
01:23:08,950 --> 01:23:13,930
and whenever you found two artists here but are related using the related artist in

1354
01:23:13,930 --> 01:23:16,400
musicbrainz you should also say

1355
01:23:16,460 --> 01:23:22,020
but there are two two performance-related by they relate to perform

1356
01:23:22,040 --> 01:23:29,960
property so this allows you to model concepts style events and properties

1357
01:23:29,980 --> 01:23:37,880
you read the performer performer style as when and as well

1358
01:23:37,900 --> 01:23:39,850
got it

1359
01:23:39,870 --> 01:23:42,900
any question

1360
01:23:44,190 --> 01:23:46,650
so this is the magic basically

1361
01:23:46,670 --> 01:23:49,040
this is where the reasoning comes soon

1362
01:23:50,150 --> 01:23:55,960
these machines started working because so far it was broken so far you have these

1363
01:23:55,960 --> 01:23:59,690
up you you can put query but noted data we come up because of the

1364
01:23:59,690 --> 01:24:03,750
data were presented down here and the only way to get the data was to

1365
01:24:03,750 --> 01:24:05,540
asking this more

1366
01:24:05,770 --> 01:24:10,600
now you put these bridge ontology and the bridge ontology allows to query from top

1367
01:24:10,600 --> 01:24:22,060
we've all really caring about these people down here

1368
01:24:22,100 --> 01:24:28,110
OK this is just an explanation of what i said

1369
01:24:28,810 --> 01:24:29,850
what to do

1370
01:24:29,850 --> 01:24:33,950
want to know the number of particles then we're doing model selection

1371
01:24:33,960 --> 01:24:37,190
this is an important

1372
01:24:37,190 --> 01:24:42,010
if we select for example the number of possible which is large so very probably

1373
01:24:42,010 --> 01:24:46,620
what will happen is that the fundamental frequency which would be for example four hundred

1374
01:24:46,680 --> 01:24:49,240
will be fitted on two hundred

1375
01:24:49,270 --> 01:24:52,040
and then you will have the first part of the four hundred

1376
01:24:52,080 --> 01:24:54,200
then another one

1377
01:24:54,210 --> 01:24:55,210
three hundred

1378
01:24:55,220 --> 01:25:00,650
and around four hundred so you have artificial waterfall between because you need to replace

1379
01:25:00,650 --> 01:25:01,800
all the possible

1380
01:25:02,700 --> 01:25:04,450
within the

1381
01:25:04,460 --> 01:25:08,460
and if you have too much or too many partial than possible so am not

1382
01:25:08,460 --> 01:25:09,660
be too large

1383
01:25:09,710 --> 01:25:12,800
but if the number of phosphorus too small

1384
01:25:12,860 --> 01:25:17,270
then you will not capture most of the energy we can begin to higher powerful

1385
01:25:17,330 --> 01:25:22,110
so again there is some compromise to do in this environment

1386
01:25:22,110 --> 01:25:23,480
so as possible

1387
01:25:23,510 --> 01:25:26,180
choice is to select the prior

1388
01:25:26,190 --> 01:25:28,200
western distribution

1389
01:25:28,220 --> 01:25:32,740
the fact that several when you run the lambda equal twenty

1390
01:25:33,130 --> 01:25:37,180
this favour solutions twenty powerful

1391
01:25:37,180 --> 01:25:40,210
and again as was the site parameters

1392
01:25:40,470 --> 01:25:43,390
i tried to this lambda parameter here

1393
01:25:43,430 --> 01:25:47,530
and we would like that to be too precise about it because we don't know

1394
01:25:47,530 --> 01:25:50,550
whether there would be five awful awful people

1395
01:25:50,620 --> 01:25:52,600
so a good solution is to keep

1396
01:25:52,640 --> 01:25:54,820
as an unknown parameter

1397
01:25:54,850 --> 01:25:58,890
and try to estimate

1398
01:25:58,910 --> 01:25:59,980
OK so

1399
01:26:00,050 --> 01:26:02,750
this is lake

1400
01:26:04,180 --> 01:26:06,250
to find a prior to find

1401
01:26:06,270 --> 01:26:10,810
it's about the violence of additive noise of the model

1402
01:26:10,820 --> 01:26:12,460
of course we could assume we know

1403
01:26:12,480 --> 01:26:14,210
but if we know it

1404
01:26:14,260 --> 01:26:18,350
then we don't know how to choose the balance between the noise and the company

1405
01:26:18,460 --> 01:26:20,700
we have to learn this from

1406
01:26:21,710 --> 01:26:24,280
if we want to capture most of the energy in the model

1407
01:26:24,310 --> 01:26:29,420
we tend to favor more value the sigma

1408
01:26:29,430 --> 01:26:33,440
so an example is to select an inverted gamma distribution

1409
01:26:33,450 --> 01:26:36,970
and which can that free trial

1410
01:26:38,040 --> 01:26:43,620
which would like to this paper very small values of sigma parameter and it's been

1411
01:26:43,650 --> 01:26:45,780
high for

1412
01:26:45,830 --> 01:26:47,880
and it does a good advantage to be

1413
01:26:47,880 --> 01:26:50,210
the conjugate prior we can write

1414
01:26:50,310 --> 01:26:55,640
very easy some posterior distribution and

1415
01:26:57,570 --> 01:26:58,930
we have to define

1416
01:26:58,930 --> 01:27:00,400
the likelihood

1417
01:27:00,420 --> 01:27:01,950
and we have defined

1418
01:27:01,960 --> 01:27:04,200
the prior now we can

1419
01:27:04,210 --> 01:27:09,320
right the posterior distribution of models

1420
01:27:12,080 --> 01:27:14,260
multiply these basis

1421
01:27:15,130 --> 01:27:16,140
we have we

1422
01:27:16,160 --> 01:27:20,910
and that was a very big expression with which supplied with prior

1423
01:27:21,030 --> 01:27:25,420
she won and what's very nice and this is because of the

1424
01:27:25,440 --> 01:27:26,980
prior was selected

1425
01:27:27,000 --> 01:27:30,260
we can integrate out some of the parameters

1426
01:27:30,270 --> 01:27:33,980
i can try to integrate

1427
01:27:34,030 --> 01:27:37,080
well where that if a right

1428
01:27:37,100 --> 01:27:38,940
with an integral in front of it

1429
01:27:38,990 --> 01:27:40,350
one thing left where

1430
01:27:40,350 --> 01:27:41,310
which includes

1431
01:27:41,350 --> 01:27:42,940
parameter theta

1432
01:27:43,580 --> 01:27:45,520
i can calculate it in closed form

1433
01:27:45,530 --> 01:27:48,430
can also the same with

1434
01:27:48,430 --> 01:27:52,960
and then you can have just above here of the frequencies the the number of

1435
01:27:56,250 --> 01:27:58,630
because i don't have to bother about

1436
01:27:58,640 --> 01:28:02,040
obama tools of amplitude and variance of the

1437
01:28:02,060 --> 01:28:03,210
i don't need

1438
01:28:03,210 --> 01:28:04,390
i can do

1439
01:28:04,510 --> 01:28:07,430
during the frequency

1440
01:28:07,440 --> 01:28:09,380
and it the like

1441
01:28:12,730 --> 01:28:14,730
and a just reward

1442
01:28:14,750 --> 01:28:16,470
to compute these quantities

1443
01:28:16,520 --> 01:28:18,430
we need to compute the metric here

1444
01:28:18,460 --> 01:28:20,460
which look like that

1445
01:28:20,490 --> 01:28:23,040
the important fact is that the universe

1446
01:28:23,770 --> 01:28:25,710
so if you want to

1447
01:28:25,720 --> 01:28:27,510
it would be very costly

1448
01:28:27,780 --> 01:28:29,060
nine inch

1449
01:28:29,080 --> 01:28:31,100
two times the total number of rules

1450
01:28:31,110 --> 01:28:35,360
and the number of amplitude and phase

1451
01:28:35,410 --> 01:28:38,650
it can be very large

1452
01:28:39,710 --> 01:28:41,860
so we integrate out

1453
01:28:41,880 --> 01:28:43,370
five of

1454
01:28:43,420 --> 01:28:47,740
and the amplitude but i think is that we can accomplish

1455
01:28:47,850 --> 01:28:51,280
i assume you know about the pregnancy

1456
01:28:51,690 --> 01:28:53,030
you can write

1457
01:28:53,080 --> 01:28:57,290
the posterior distribution of the variance of the night with respect to the frequency the

1458
01:28:57,290 --> 01:28:59,250
number of children the observation

1459
01:28:59,260 --> 01:29:01,680
you can also the same with the

1460
01:29:01,680 --> 01:29:04,920
and the violence

1461
01:29:10,990 --> 01:29:15,660
so we start with an application which is estimation of the music

1462
01:29:15,710 --> 01:29:17,210
we right to physical model

1463
01:29:17,220 --> 01:29:21,960
when that into a probabilistic model and we end up with equations which are the

1464
01:29:21,960 --> 01:29:24,660
posterior distributions of the parameters

1465
01:29:25,480 --> 01:29:30,770
if we want to estimate its parameters we need to explore this posterior distribution

1466
01:29:32,020 --> 01:29:33,460
the previous one

1467
01:29:35,890 --> 01:29:37,620
OK so the problem now

1468
01:29:37,620 --> 01:29:38,680
is that

1469
01:29:38,730 --> 01:29:44,230
to obtain an estimate of the frequency of the amplitude of the

1470
01:29:44,240 --> 01:29:48,450
we need either to maximize the two either to integrate

1471
01:29:48,490 --> 01:29:51,130
it's a serious about

1472
01:29:51,150 --> 01:29:53,750
so this can be very difficult

1473
01:29:53,780 --> 01:29:55,330
and a good way to do it is

1474
01:29:55,350 --> 01:29:57,200
bayesian computations

1475
01:29:57,220 --> 01:30:02,200
so you have the lecture six lectures on a bike store there

1476
01:30:03,310 --> 01:30:04,960
so i go very fast

1477
01:30:11,740 --> 01:30:13,540
how can we do that

1478
01:30:13,550 --> 01:30:16,390
OK now we have to where we defined model

1479
01:30:16,400 --> 01:30:17,230
we have this

1480
01:30:17,260 --> 01:30:19,230
the set of parameters

1481
01:30:19,270 --> 01:30:21,630
and we want to be able to explain

1482
01:30:21,640 --> 01:30:24,000
and the very difficult

1483
01:30:26,630 --> 01:30:30,420
so i recall a little basic insincere

1484
01:30:30,470 --> 01:30:32,480
so we want to know

1485
01:30:32,510 --> 01:30:36,920
estimated three estimated number of possible maybe

1486
01:30:36,920 --> 01:30:40,460
estimated value of z eighty nine

1487
01:30:40,490 --> 01:30:41,200
you can

1488
01:30:41,200 --> 01:30:44,710
as i mentioned earlier you can do it like an image

1489
01:30:44,760 --> 01:30:47,050
and maybe

1490
01:30:47,100 --> 01:30:50,220
both are going

1491
01:30:50,400 --> 01:30:54,220
but the important thing is that we need to explore this posterior distribution of the

1492
01:30:54,220 --> 01:30:57,860
parameters and we want to exploit around its maximum

1493
01:30:57,900 --> 01:31:01,560
for the main problems prior approach

1494
01:31:01,670 --> 01:31:03,740
OK so the MMSE estimate

1495
01:31:03,820 --> 01:31:06,710
looks like the posterior distribution

1496
01:31:07,020 --> 01:31:08,420
and it returns

1497
01:31:08,530 --> 01:31:10,070
the parameter that we have

1498
01:31:11,510 --> 01:31:15,720
OK which is the expectation with respect to the posterior distribution so we need to

1499
01:31:15,720 --> 01:31:17,980
compute this integral

1500
01:31:18,040 --> 01:31:20,600
and we also

1501
01:31:20,600 --> 01:31:25,210
identify the main focus

1502
01:31:27,560 --> 01:31:29,300
starting to five

1503
01:31:29,360 --> 01:31:31,620
on the column is that just to

1504
01:31:31,630 --> 01:31:34,410
refresh you

1505
01:31:34,420 --> 01:31:36,010
OK so i want to come

1506
01:31:36,130 --> 01:31:37,690
we want to

1507
01:31:37,690 --> 01:31:40,240
integrate out it

1508
01:31:40,260 --> 01:31:42,720
with respect to the parameters

1509
01:31:42,720 --> 01:31:47,120
in health the posterior distribution of the parameters just

1510
01:31:49,470 --> 01:31:52,720
so one element is very simple

1511
01:31:52,870 --> 01:31:54,480
same but

1512
01:31:54,490 --> 01:31:56,290
OK so before the

1513
01:31:56,510 --> 01:31:58,060
a monthly column

1514
01:31:58,080 --> 01:32:00,170
imagine a density p

1515
01:32:01,440 --> 01:32:03,250
is a uniform distribution

1516
01:32:03,310 --> 01:32:06,650
and it's very easy to calculate into

1517
01:32:06,660 --> 01:32:09,900
provided h can integrate

1518
01:32:09,950 --> 01:32:11,460
if it gaussian

1519
01:32:12,490 --> 01:32:16,050
you might be able to calculate it analytic right

1520
01:32:16,060 --> 01:32:19,750
the bad news is that it's never any of these cases

1521
01:32:19,770 --> 01:32:23,210
usually it's much more like that that you have

1522
01:32:23,220 --> 01:32:27,330
both should be very high peaks and very narrow peaks

1523
01:32:29,060 --> 01:32:30,420
local maxima

1524
01:32:30,420 --> 01:32:32,190
o thing

1525
01:32:33,610 --> 01:32:38,190
OK you

1526
01:32:53,990 --> 01:32:56,640
so that

1527
01:32:56,790 --> 01:33:00,880
you see

1528
01:33:23,690 --> 01:33:27,380
you know

1529
01:34:09,000 --> 01:34:13,340
well of the world

1530
01:34:46,870 --> 01:34:48,120
you know

1531
01:34:48,130 --> 01:34:53,890
he told

1532
01:34:53,900 --> 01:35:04,070
it's not same on

1533
01:35:05,930 --> 01:35:08,240
you i of

1534
01:35:21,630 --> 01:35:26,000
the state is

1535
01:35:27,790 --> 01:35:32,170
we're going to be able to solve the problem

1536
01:35:32,330 --> 01:35:36,200
same thing

1537
01:36:03,280 --> 01:36:06,160
well and

1538
01:36:36,980 --> 01:36:42,330
of course

1539
01:37:01,990 --> 01:37:05,830
we need

1540
01:37:35,250 --> 01:37:45,260
we all know

1541
01:38:19,360 --> 01:38:23,540
and what you find

1542
01:38:29,170 --> 01:38:33,510
you know a

1543
01:38:47,450 --> 01:38:51,690
and you for

1544
01:38:58,340 --> 01:39:04,900
he said

1545
01:39:04,970 --> 01:39:07,590
it is

1546
01:39:39,900 --> 01:39:41,570
park you

1547
01:39:47,760 --> 01:39:51,990
there there's no

1548
01:39:52,010 --> 01:39:57,010
the problem is

1549
01:39:57,030 --> 01:39:59,780
how i

1550
01:40:09,610 --> 01:40:10,900
all the time

1551
01:40:17,760 --> 01:40:23,610
among are

1552
01:40:40,170 --> 01:40:47,450
so we

1553
01:40:47,450 --> 01:40:50,580
it has lot structure much like this

1554
01:40:50,690 --> 01:40:57,220
it's it's very hard to guess beta

1555
01:40:57,240 --> 01:41:00,120
the nature of

1556
01:41:00,160 --> 01:41:02,970
the data

1557
01:41:03,050 --> 01:41:07,620
it was the whole inputs that they

1558
01:41:09,140 --> 01:41:10,700
forward if the text

1559
01:41:10,930 --> 01:41:13,080
i would have some kind

1560
01:41:13,140 --> 01:41:14,490
there should

1561
01:41:14,530 --> 01:41:16,910
you don't have any

1562
01:41:18,800 --> 01:41:21,200
i was trying the best

1563
01:41:21,240 --> 01:41:23,200
i don't know

1564
01:41:24,470 --> 01:41:26,470
i would expect

1565
01:41:28,680 --> 01:41:29,910
here she

1566
01:41:29,930 --> 01:41:31,930
but i'm uncertain

1567
01:41:32,720 --> 01:41:36,220
there were ten speed of ten for large datasets

1568
01:41:36,220 --> 01:41:37,780
or not

1569
01:41:42,120 --> 01:41:44,890
you can also compare the speedup versus the number of

1570
01:41:44,910 --> 01:41:47,550
euclidean dimensions

1571
01:41:50,050 --> 01:41:52,050
this was in this

1572
01:41:52,070 --> 01:41:54,530
just about eight hundred features

1573
01:41:54,640 --> 01:41:58,470
this is that images dataset which is about four thousand features

1574
01:41:58,490 --> 01:42:00,660
it's also only has six hundred

1575
01:42:01,140 --> 01:42:05,340
data points we didn't really notice

1576
01:42:05,350 --> 01:42:09,410
so two explanations for what might be sort of a six hundred is about there

1577
01:42:09,430 --> 01:42:16,640
is this one

1578
01:42:18,080 --> 01:42:20,850
maybe what this says is that

1579
01:42:20,850 --> 01:42:23,350
the number of features is not too

1580
01:42:23,350 --> 01:42:26,620
related to the speed up to you see

1581
01:42:26,680 --> 01:42:31,340
in waiting speedups for presiding over the feature so i think these are

1582
01:42:31,350 --> 01:42:37,550
the above twenty understand actually very kd trees but i i think about twenty

1583
01:42:37,570 --> 01:42:39,240
they start to break down

1584
01:42:39,760 --> 01:42:42,850
they just require radial of competition time

1585
01:42:42,870 --> 01:42:43,820
this is

1586
01:42:43,850 --> 01:42:46,350
is like forty or fifty features

1587
01:42:47,640 --> 01:42:55,640
OK so the the expansion constant

1588
01:42:55,680 --> 01:42:58,200
so where you compute expansion constant

1589
01:43:00,760 --> 01:43:01,780
is this way

1590
01:43:01,800 --> 01:43:07,220
you take the point because distance to a point in each sort

1591
01:43:07,410 --> 01:43:10,680
and then you think about a radius

1592
01:43:10,720 --> 01:43:14,180
two that just before one of the sorted elements

1593
01:43:14,240 --> 01:43:15,490
the double that

1594
01:43:15,640 --> 01:43:18,740
you can number of points here which here

1595
01:43:18,800 --> 01:43:22,410
in the iterations needed the worst case several choices of

1596
01:43:22,470 --> 01:43:24,180
this is just before

1597
01:43:24,370 --> 01:43:29,240
you can prove that the worst case risk is one of the sort

1598
01:43:29,410 --> 01:43:34,160
so it's in square in time because for every point you can business point any

1599
01:43:40,470 --> 01:43:42,200
but then you would have the worst case

1600
01:43:44,950 --> 01:43:46,160
try to

1601
01:43:46,180 --> 01:43:48,660
it's a

1602
01:43:48,800 --> 01:43:50,600
actually probably be

1603
01:43:50,620 --> 01:43:53,970
that's more valid in some sense because what you see

1604
01:43:54,030 --> 01:43:56,470
the worst case value c

1605
01:43:56,470 --> 01:43:59,260
this is the the worst case expressed constant

1606
01:43:59,280 --> 01:44:05,300
is not strongly indicative of the speed that we have to

1607
01:44:06,490 --> 01:44:09,450
what you can do is you can take

1608
01:44:09,550 --> 01:44:15,300
every point you can compute some worst case of radius is

1609
01:44:15,410 --> 01:44:17,370
you can sort the points according to that

1610
01:44:17,390 --> 01:44:22,340
worst case this and so was his first cousin the point

1611
01:44:22,350 --> 01:44:24,910
and in the eightieth percentile

1612
01:44:24,930 --> 01:44:26,280
and that's what this is

1613
01:44:26,300 --> 01:44:30,140
the green

1614
01:44:30,180 --> 01:44:33,910
so some sort of average notion

1615
01:44:34,260 --> 01:44:46,180
one taking the very worst case for taking its present our case

1616
01:44:46,760 --> 01:44:48,760
if you do this operation of

1617
01:44:48,760 --> 01:44:50,080
o thing that the

1618
01:44:50,140 --> 01:44:52,390
worst case per point and sorting

1619
01:44:52,410 --> 01:44:55,010
we observe is curves like

1620
01:44:55,430 --> 01:44:58,530
maybe in this goes like this

1621
01:45:00,390 --> 01:45:05,200
maybe maybe it's this one here where you little speed up

1622
01:45:05,200 --> 01:45:07,100
and then

1623
01:45:07,100 --> 01:45:10,910
the ones we we get a large speed but still have large

1624
01:45:10,910 --> 01:45:14,620
its worst case let's just like these

1625
01:45:14,660 --> 01:45:16,800
you see things like this

1626
01:45:16,850 --> 01:45:18,970
so a few points had to be

1627
01:45:18,970 --> 01:45:22,430
we fly away from other points so they have a large expansion constant but that

1628
01:45:22,430 --> 01:45:23,950
doesn't matter much

1629
01:45:23,970 --> 01:45:26,410
the performance of the

1630
01:45:27,490 --> 01:45:32,970
this graph is just looking at sort of that point

1631
01:45:33,210 --> 01:45:36,820
it's going to be g

1632
01:45:36,820 --> 01:45:39,220
yes it is

1633
01:45:39,220 --> 01:45:42,440
so what would be interested in estimating there

1634
01:45:42,460 --> 01:45:47,910
the deal known the theta will be the probability of getting head

1635
01:45:47,930 --> 01:45:50,250
when we don't know the the coins there

1636
01:45:50,270 --> 01:45:55,170
it points for the obviously we know what the chances are half

1637
01:45:55,180 --> 01:45:58,380
i was saying here is we don't know that we try to find out what

1638
01:45:58,380 --> 01:46:03,630
the probability that is of course it's that's still known to try to find out

1639
01:46:03,660 --> 01:46:05,730
and then

1640
01:46:05,760 --> 01:46:09,230
saying that we've got six had so saying

1641
01:46:09,250 --> 01:46:10,750
the like because

1642
01:46:10,760 --> 01:46:16,900
so the likelihood say point one would be if the probability of getting heads point

1643
01:46:16,900 --> 01:46:19,360
well what's the chance of getting six heads

1644
01:46:19,380 --> 01:46:21,830
out of ten games

1645
01:46:21,850 --> 01:46:23,170
and then

1646
01:46:23,180 --> 01:46:27,570
you can change as a function of t c could be possible values this probability

1647
01:46:29,110 --> 01:46:31,490
you work through the what's offensive

1648
01:46:31,500 --> 01:46:34,700
the probability that heads point two what's the probability of getting

1649
01:46:34,720 --> 01:46:37,400
is the probability of getting a head is point two

1650
01:46:37,420 --> 01:46:40,510
what would be the probability of getting

1651
01:46:40,670 --> 01:46:42,990
six had given the image go

1652
01:46:42,990 --> 01:46:44,820
the chance of getting a head pointer

1653
01:46:44,850 --> 01:46:51,080
so this is what this this thing like is measuring so we very t

1654
01:46:51,220 --> 01:46:55,500
and what we try do so

1655
01:46:55,500 --> 01:46:56,810
and what we do

1656
01:46:56,830 --> 01:46:58,230
if we choose

1657
01:46:58,240 --> 01:47:00,690
our estimate of theta

1658
01:47:00,710 --> 01:47:04,450
so with the value of t that makes this function the biggest

1659
01:47:04,470 --> 01:47:09,880
so let's move on to you know there's a standard abusive is this abuse of

1660
01:47:09,880 --> 01:47:15,270
notation is commonly used with forget about calling it to put featuring because i see

1661
01:47:15,270 --> 01:47:17,730
on this side view of new this definition

1662
01:47:17,750 --> 01:47:24,180
so so and the reason is because this is just an unknown number so can't

1663
01:47:24,180 --> 01:47:27,600
really be varying t

1664
01:47:27,620 --> 01:47:29,520
this is very

1665
01:47:29,520 --> 01:47:35,110
so what we can do is we have as we work out the system and

1666
01:47:35,110 --> 01:47:36,690
the way work is

1667
01:47:36,710 --> 01:47:38,620
we choose the value

1668
01:47:38,680 --> 01:47:40,740
that maximizes the likelihood

1669
01:47:41,480 --> 01:47:46,090
which is the value of the in the in sample of talking about the

1670
01:47:46,200 --> 01:47:47,680
it was so now

1671
01:47:47,690 --> 01:47:49,650
that the best choice

1672
01:47:49,740 --> 01:47:54,080
would be to set our estimate for the probability that you had to be six

1673
01:47:54,080 --> 01:47:56,780
out of ten six ten point six

1674
01:47:59,440 --> 01:48:05,520
if you work out the likelihood function and differentiated maximize it you find that is

1675
01:48:05,520 --> 01:48:12,000
maximized the sequels point six for that situation so that's what we tried to try

1676
01:48:12,010 --> 01:48:14,150
again estimate

1677
01:48:14,160 --> 01:48:17,160
i was based on this assumption here

1678
01:48:18,100 --> 01:48:20,510
stand once we do that

1679
01:48:20,520 --> 01:48:25,570
we do very well because we know how the law of theory about this function

1680
01:48:25,680 --> 01:48:31,180
so we can work out always something properties was an alternative

1681
01:48:31,200 --> 01:48:32,370
which is

1682
01:48:32,430 --> 01:48:35,250
bayesian inference

1683
01:48:38,370 --> 01:48:40,670
on the situation

1684
01:48:40,680 --> 01:48:45,460
we've got the same thing with theta that's i found out about but

1685
01:48:45,860 --> 01:48:50,660
we use the subjective probability idea even though these are

1686
01:48:50,660 --> 01:48:57,760
numbers constants we just don't know there was also will have the chance that they

1687
01:48:57,820 --> 01:48:59,060
long term the

1688
01:48:59,080 --> 01:49:03,770
for this dataset is twenty seven point two or something so the probability of b

1689
01:49:03,780 --> 01:49:06,760
twenty seven point two is xy

1690
01:49:06,780 --> 01:49:11,560
say point four seven percent or something like that

1691
01:49:13,320 --> 01:49:17,010
what we do is we reflects believe

1692
01:49:17,020 --> 01:49:20,260
about what the value theta is

1693
01:49:20,280 --> 01:49:23,070
by giving a probability distribution

1694
01:49:23,120 --> 01:49:27,740
which is one of the subjective probabilities so so even though we don't believe

1695
01:49:27,800 --> 01:49:29,780
it's a random quantity

1696
01:49:30,770 --> 01:49:32,650
and actually belief about

1697
01:49:32,650 --> 01:49:37,760
its characteristics by using a probability distributions describe what we think

1698
01:49:37,790 --> 01:49:39,550
it is the text

1699
01:49:40,980 --> 01:49:43,210
so for example

1700
01:49:43,320 --> 01:49:47,180
if we thought that

1701
01:49:50,430 --> 01:49:56,760
if this might be really bad example course matter if we think of vehicles the

1702
01:49:56,760 --> 01:49:58,030
pressure is

1703
01:49:58,220 --> 01:50:01,210
the average the pressure say a hundred

1704
01:50:03,430 --> 01:50:07,330
we have no idea of what the ranges we might say that

1705
01:50:08,470 --> 01:50:11,950
what we think of as our belief about what is that

1706
01:50:11,970 --> 01:50:14,490
the belief about what

1707
01:50:14,540 --> 01:50:18,930
the average the pressure is to be characterized by a normal distribution with mean one

1708
01:50:18,930 --> 01:50:23,500
hundred a very big variance then the variance because we weren't really sure what the

1709
01:50:25,340 --> 01:50:29,410
so that there will be one example of capturing

1710
01:50:31,310 --> 01:50:33,680
belief about a constant

1711
01:50:33,690 --> 01:50:36,820
the constant being average blood pressure across

1712
01:50:36,840 --> 01:50:38,680
everybody in the world of

1713
01:50:38,690 --> 01:50:40,510
if the number of people

1714
01:50:40,570 --> 01:50:46,960
well that's that's the constant but we're trying to say what i think is what

1715
01:50:46,970 --> 01:50:53,330
i believe about it capturing that by probability distribution probability distribution is this thing here

1716
01:50:53,750 --> 01:50:55,940
p theta

1717
01:50:56,100 --> 01:51:02,930
so it's subjective probability and he's telling us what do we believe that the parameters

1718
01:51:04,260 --> 01:51:10,660
now the idea behind it is that if you do any data analysis the client

1719
01:51:10,670 --> 01:51:16,110
we also have to tell you things about the unknown quantities so rather than just

1720
01:51:16,110 --> 01:51:21,460
rely on data collection you can rely on asking you apply what they know about

1721
01:51:22,370 --> 01:51:24,410
so this is the way

1722
01:51:24,410 --> 01:51:27,180
including information from experts

1723
01:51:27,200 --> 01:51:32,530
and what we think of in this context is this is what i thought was

1724
01:51:32,540 --> 01:51:37,040
the case about theta before i started doing and experimentation

1725
01:51:37,040 --> 01:51:43,560
do for the last twenty years actually in a certain sense this string excitations sort

1726
01:51:43,560 --> 01:51:50,340
of energy trajectories are added headings you may say because the particles of the standard

1727
01:51:50,340 --> 01:51:56,090
model of definitely not arranged in marriages trajectories we are pretty sure about that they

1728
01:51:56,100 --> 01:52:02,340
are just corresponding to one trajectory particles so you know there are many trajectories if

1729
01:52:02,340 --> 01:52:08,220
string theory is correct there are particles in the standard model so the real idea

1730
01:52:08,220 --> 01:52:12,700
of the iraq beginning was not is not really what is driving unification on the

1731
01:52:12,700 --> 01:52:15,230
other hand it what has been

1732
01:52:15,280 --> 01:52:20,300
recognise is that one of these particles that it's always fair weather you want it

1733
01:52:20,300 --> 01:52:26,900
or not is the graviton on that these theories are consistent theories of quantum gravity

1734
01:52:26,950 --> 01:52:31,420
and this is really the main motivation for trying to think of string theory as

1735
01:52:31,420 --> 01:52:37,960
opposed to quantum field theory it when one thinks about unification of interactions

1736
01:52:37,970 --> 01:52:44,100
so this is the unification program how about the strong force well there as you

1737
01:52:44,100 --> 01:52:51,590
know in the seventies the beautiful theory has been developed and almost completed quantum chromodynamics

1738
01:52:51,650 --> 01:52:58,960
and furthermore there are other technical tools which are perturbation theory and lattice QCD calculations

1739
01:52:59,160 --> 01:53:03,640
which are adequate for many many context not for everything but for a lot of

1740
01:53:03,640 --> 01:53:09,720
context therefore if string theory is to play a role there it

1741
01:53:09,730 --> 01:53:15,120
really will play a role only as an analytical tool an analytical tool for accessing

1742
01:53:15,120 --> 01:53:19,720
the low energy strong coupling dynamics which are still hard

1743
01:53:19,730 --> 01:53:23,930
to calculate the this we don't have other techniques

1744
01:53:23,970 --> 01:53:26,220
indeed the fact

1745
01:53:26,230 --> 01:53:33,980
that we see these nice energy resonances in hadronic collisions at low energies is not

1746
01:53:33,980 --> 01:53:35,350
a mystery at all

1747
01:53:35,350 --> 01:53:40,280
if you think for instance if you look at lattice calculations of the quark antiquark

1748
01:53:40,280 --> 01:53:44,490
potentiality is here is one such calculation

1749
01:53:44,520 --> 01:53:48,400
it is not very obvious that just plane QCD

1750
01:53:48,410 --> 01:53:54,110
there are linear forces have large separation that's exactly as if there was a string

1751
01:53:54,110 --> 01:53:59,160
between the park and antiquark which was being stretched and because it has been shown

1752
01:53:59,180 --> 01:54:05,660
this creates energy proportional to length and therefore the more than understanding is that QCD

1753
01:54:05,660 --> 01:54:12,460
has strings these are some sort of flux tubes chromo magnetic flux tubes which like

1754
01:54:12,490 --> 01:54:16,010
the case of electrodynamics where they open up know

1755
01:54:16,020 --> 01:54:21,980
here they have a tendency to actually become not one thing when you stretched them

1756
01:54:22,010 --> 01:54:27,380
and this explains the qualitative successes of both the dual models from the beginning and

1757
01:54:27,380 --> 01:54:34,310
also more a more practical effective models like the one model

1758
01:54:35,250 --> 01:54:39,980
the modern reincarnation on the other hand dual mode this as you will see is

1759
01:54:39,980 --> 01:54:45,970
completely different from this world very different from his original ideas and it goes by

1760
01:54:45,970 --> 01:54:53,780
the name of the aviation safety correspondence out talking the last lecture about potential applications

1761
01:54:53,780 --> 01:54:58,330
of these mostly for the quark gluon plasma what there is some hope that one

1762
01:54:58,330 --> 01:55:05,270
will get new input from this approach but theorists are much more excited by the

1763
01:55:05,270 --> 01:55:13,130
idea CFT correspondence for very different reasons because they believe it so far-reaching correspondence which

1764
01:55:13,990 --> 01:55:19,300
allow us to understand quantum gravity and this is the hope and i would say

1765
01:55:19,310 --> 01:55:21,030
think about this

1766
01:55:21,050 --> 01:55:24,100
in the last lecture

1767
01:55:24,110 --> 01:55:30,520
so now today let's go back to very basic things let's try to first understand

1768
01:55:30,520 --> 01:55:36,480
the dynamics of one relativistic string that's where the story begins

1769
01:55:36,530 --> 01:55:43,740
now this is described by a lagrangian connection principle which in theory as you know

1770
01:55:43,750 --> 01:55:51,580
it is very nice way to basically summarize everything including the symmetry equations and the

1771
01:55:51,580 --> 01:55:58,380
quantisation of the thing in the case of relativistic string the simplest such action is

1772
01:55:58,380 --> 01:56:04,030
known as the number of got to action it is basically nothing else but the

1773
01:56:04,030 --> 01:56:10,130
area of the trajectory that string things energy extended object think of it as a

1774
01:56:10,870 --> 01:56:12,840
when it moves it

1775
01:56:13,300 --> 01:56:20,470
it leaves behind the two dimensional trajectory something like a cylinder deformed cylinder now the

1776
01:56:20,470 --> 01:56:21,900
area of these

1777
01:56:21,930 --> 01:56:27,560
it is basically the action the area has dimensions of length squared so there should

1778
01:56:27,560 --> 01:56:33,350
be a dimension for parameter and this parameter is known as the fundamental string tension

1779
01:56:33,350 --> 01:56:37,100
and it is also defined as one of our two pi

1780
01:56:37,110 --> 01:56:43,340
time some other parameter that edges slope parameter which is called alpha prime has dimensions

1781
01:56:43,340 --> 01:56:44,850
of length square

1782
01:56:46,120 --> 01:56:48,470
so x mu of sigma tau

1783
01:56:48,590 --> 01:56:54,480
here the coordinates in space and time of this extended the object as it moves

1784
01:56:54,980 --> 01:56:58,420
sigma tau are to parameters that you can

1785
01:56:58,430 --> 01:57:05,120
use on these what she thought trajectory to describe a at any given point the

1786
01:57:05,120 --> 01:57:06,810
position x mu of sigma tau

1787
01:57:07,350 --> 01:57:16,420
it and using here the lawrence symmetric namely exit the means minus time squared plus

1788
01:57:16,420 --> 01:57:18,930
space square

1789
01:57:18,970 --> 01:57:23,860
now what this is to say well this square root expression may look a bit

1790
01:57:23,860 --> 01:57:29,850
complicated it is really a very simple geometric expect pressure as i said namely it

1791
01:57:29,860 --> 01:57:32,020
is the element of area

1792
01:57:32,050 --> 01:57:37,800
on this trajectory it can be it also as mine is the determinant of something

1793
01:57:37,800 --> 01:57:43,560
about one called the induced metric namely if you have a surface embedded in some

1794
01:57:44,740 --> 01:57:50,390
i'm which has its own notion of distance you can of course measure distances on

1795
01:57:50,390 --> 01:57:55,710
the surface by just taking the same mistake me stick that you had in space

1796
01:57:55,710 --> 01:58:01,320
time and just moving around the except for say on the globe the surface of

1797
01:58:01,320 --> 01:58:06,260
the african of course measuring distances in exactly the same way as outer space except

1798
01:58:06,260 --> 01:58:10,710
you have to stay on the surface of the earth and the induced metric is

1799
01:58:10,710 --> 01:58:12,460
simply this reduced

1800
01:58:12,500 --> 01:58:17,760
a notion of distance on this two-dimensional surface

1801
01:58:17,780 --> 01:58:22,690
now this is the start of the story now when you think about these would

1802
01:58:22,690 --> 01:58:24,490
be if you realize that

1803
01:58:24,500 --> 01:58:29,060
there's nothing totally unique about this action actually if one simply

1804
01:58:29,070 --> 01:58:37,120
but requires the symmetries that are here namely lawrence invariance and re parametrisation invariance it

1805
01:58:37,120 --> 01:58:42,730
shouldn't matter how you choose to label the points shown this trajectory there are many

1806
01:58:42,730 --> 01:58:47,320
more than one can write down however what is unique about this action and that's

1807
01:58:47,320 --> 01:58:51,930
vector machines and you almost certainly going to hear about this and can lectures coming

1808
01:58:54,430 --> 01:58:55,840
red curve

1809
01:58:55,860 --> 01:58:59,340
is a log probability law for logistic loss

1810
01:58:59,350 --> 01:59:05,000
cook from a method called logistic regression which is really common classifier so one we're

1811
01:59:05,020 --> 01:59:06,930
talking about

1812
01:59:09,810 --> 01:59:14,600
you will get very lost if you really clearly predict the right answer but

1813
01:59:14,640 --> 01:59:16,650
instead of just having

1814
01:59:16,740 --> 01:59:19,270
a lot of one if you get the wrong answer

1815
01:59:19,320 --> 01:59:20,290
if you're

1816
01:59:20,300 --> 01:59:22,320
very confident wrong

1817
01:59:22,390 --> 01:59:25,920
then you're going to experience a very big loss and the loss will go up

1818
01:59:25,920 --> 01:59:30,880
to infinity

1819
01:59:30,890 --> 01:59:35,840
maybe that's bad if you're making the financial prediction your very confidently saying the wrong

1820
01:59:40,850 --> 01:59:45,050
just regression can be seen in a couple of different ways and

1821
01:59:45,110 --> 01:59:48,220
it's again going to be an example of ideas that we'll see all over the

1822
01:59:49,610 --> 01:59:54,130
instead of just saying i'm going to come up with the decision boundary logistic regression

1823
01:59:54,130 --> 01:59:57,800
does something a bit more it says i'm going to actually

1824
01:59:57,860 --> 02:00:00,230
try to come up with an explanation for

1825
02:00:00,240 --> 02:00:01,720
all of the labels

1826
02:00:02,140 --> 02:00:04,940
and i'm going to say that

1827
02:00:04,970 --> 02:00:08,290
it's not actually the case that if i have a decision boundary

1828
02:00:08,300 --> 02:00:10,770
and whenever i make observations on this side

1829
02:00:10,770 --> 02:00:13,910
you're always going to see a plus one it's actually possible for there to be

1830
02:00:13,910 --> 02:00:18,250
noise people might enter the wrong number in the computer or the world more complicated

1831
02:00:18,250 --> 02:00:21,900
than the plain so i should always allow for other possibilities

1832
02:00:22,000 --> 02:00:27,410
this defines a probability distribution over what the label might be

1833
02:00:29,360 --> 02:00:33,400
it can sometimes the same w transpose x the same activation

1834
02:00:34,490 --> 02:00:37,470
and if the activation is really large

1835
02:00:37,480 --> 02:00:41,270
then the probability of getting a plus one label is basically one

1836
02:00:41,280 --> 02:00:44,230
so if you are very very far from the decision boundary it does the same

1837
02:00:44,230 --> 02:00:45,060
thing is

1838
02:00:45,120 --> 02:00:47,690
other methods we've seen it says well you're going to be one on this site

1839
02:00:47,920 --> 02:00:50,780
and if you're a long way on the other side then you're going to be

1840
02:00:50,790 --> 02:00:54,570
zero probability of being one which means or minus one

1841
02:00:54,660 --> 02:00:56,620
what's different is that

1842
02:00:56,630 --> 02:00:59,300
near the decision boundary itself

1843
02:00:59,360 --> 02:01:03,890
it explicitly says the probability of getting plus one labelled is the high

1844
02:01:04,000 --> 02:01:07,270
and if you're a little bit on one side or the other

1845
02:01:07,270 --> 02:01:09,190
i'm not sure exactly

1846
02:01:09,250 --> 02:01:12,490
it's more probable that you're going to be one if you're on the positive side

1847
02:01:12,490 --> 02:01:17,990
of the decision boundary but it's also possible that could be a negative one

1848
02:01:20,060 --> 02:01:24,980
that's a richer description of the classification problem than just a single plane is describing

1849
02:01:24,980 --> 02:01:28,420
the whole dataset including sort of outliers and noise

1850
02:01:29,530 --> 02:01:34,300
and it's also possible to fit the w in this description

1851
02:01:34,390 --> 02:01:37,850
so we can make a objective function

1852
02:01:37,890 --> 02:01:39,020
out of

1853
02:01:39,030 --> 02:01:42,250
this model of the labels

1854
02:01:42,290 --> 02:01:47,010
so one objective which is sensible is what's the probability of

1855
02:01:47,040 --> 02:01:52,020
the whole vector my observations of this whole lecture pluses and minuses ones in my

1856
02:01:52,070 --> 02:01:55,490
my training set what was the probability of seeing that

1857
02:01:55,530 --> 02:01:59,610
given the particular matrix of input that i had

1858
02:01:59,630 --> 02:02:03,690
and the weight vector that i've currently got in my hand so given your model

1859
02:02:03,690 --> 02:02:07,420
with this weight vector how probable is that you see the labels that you actually

1860
02:02:07,420 --> 02:02:08,510
did c

1861
02:02:08,610 --> 02:02:11,130
and that's equal to

1862
02:02:11,140 --> 02:02:16,230
the product of the probabilities of all the individual labels if we assume that

1863
02:02:16,240 --> 02:02:19,030
the label during small independent given

1864
02:02:19,050 --> 02:02:21,160
the weight vector

1865
02:02:21,920 --> 02:02:24,910
so now we could maximize this thing

1866
02:02:24,910 --> 02:02:27,560
which is called the likelihood of w

1867
02:02:27,580 --> 02:02:29,430
with respect to w

1868
02:02:29,490 --> 02:02:33,880
so that i'd like to emphasise the pedantic point this thing is called the the

1869
02:02:33,880 --> 02:02:37,180
likelihood of w it is not the likelihood of the data set

1870
02:02:38,260 --> 02:02:42,530
that's what this is sort of the long-established tradition in statistics is called the likelihood

1871
02:02:42,530 --> 02:02:44,630
of the parameters

1872
02:02:44,650 --> 02:02:48,510
so i want to write computer code that will optimize this thing

1873
02:02:51,130 --> 02:02:53,540
you'll quickly realise when you write code

1874
02:02:53,560 --> 02:02:56,970
a bit of a pain to deal with directly in and in computers because they

1875
02:02:56,970 --> 02:02:59,010
tend to be incredibly small

1876
02:03:00,900 --> 02:03:04,330
if you imagine that i flip the coin

1877
02:03:04,450 --> 02:03:05,940
a thousand times

1878
02:03:05,940 --> 02:03:10,310
and i wrote down head tail had had told hotel and i wrote down the

1879
02:03:11,460 --> 02:03:15,260
and i said what's the probability of the data set

1880
02:03:15,280 --> 02:03:18,300
but answer is half the probability of each event

1881
02:03:18,350 --> 02:03:21,850
raise the thousand the number of events half times after high

1882
02:03:21,870 --> 02:03:27,230
a thousand times now half to the thousand is an incredibly small number and

1883
02:03:27,270 --> 02:03:29,650
that under floating if you try to represent it

1884
02:03:31,300 --> 02:03:35,670
you can't deal with probability that small instead you take the lovely so what's the

1885
02:03:35,680 --> 02:03:39,300
log probability and that would be a number of floating point can deal with

1886
02:03:39,340 --> 02:03:42,750
it will also be numerically more stable and

1887
02:03:42,750 --> 02:03:44,910
that night for a variety of reasons

1888
02:03:44,920 --> 02:03:48,660
one reason is nice is now the the log probability of the whole dataset can

1889
02:03:48,660 --> 02:03:50,510
be written over some

1890
02:03:51,270 --> 02:03:54,170
each of the items so the log this term

1891
02:03:54,190 --> 02:03:55,870
is given here

1892
02:03:57,060 --> 02:04:01,490
now we can interpret this as optimizing the loss because we've got a sum of

1893
02:04:01,490 --> 02:04:05,990
independent terms for each individual case in the training set and it looks as though

1894
02:04:05,990 --> 02:04:07,960
we doing something

1895
02:04:08,030 --> 02:04:11,860
like the least squares optimisation we did before we're looking at each

1896
02:04:11,870 --> 02:04:16,590
items the training set and saying what's the little term here that we want to

1897
02:04:16,600 --> 02:04:18,350
that we want to be good

1898
02:04:18,360 --> 02:04:22,740
and so the loss we pay for each example is the log of one class

1899
02:04:22,740 --> 02:04:25,080
e to the minus y transactivation

1900
02:04:26,170 --> 02:04:27,920
that's what

1901
02:04:27,930 --> 02:04:29,530
this red couple sharing

1902
02:04:30,440 --> 02:04:31,690
the loss we pay

1903
02:04:31,700 --> 02:04:33,060
the log probability

1904
02:04:33,080 --> 02:04:37,460
for predicting label exactly correctly is there

1905
02:04:37,460 --> 02:04:40,070
and that makes sense so

1906
02:04:40,120 --> 02:04:42,980
if i'm a long way from the decision boundary

1907
02:04:43,010 --> 02:04:45,930
then the probability assigned to the label is one

1908
02:04:45,980 --> 02:04:49,810
i'm not completely sure that this is going to be a plus one

1909
02:04:50,300 --> 02:04:53,500
so the log of one is around

1910
02:04:54,710 --> 02:04:57,540
if i'm going to be very wrong if i say i'm completely sure that this

1911
02:04:57,540 --> 02:04:58,880
is one

1912
02:04:58,940 --> 02:05:02,770
and i'm sure with probability one and it turns out to be a minus one

1913
02:05:02,940 --> 02:05:05,060
then i'm going to take the logs very

1914
02:05:05,080 --> 02:05:07,500
which is minus infinity

1915
02:05:07,530 --> 02:05:08,600
it's a

1916
02:05:13,830 --> 02:05:21,460
the negative of that which is infinite

1917
02:05:29,790 --> 02:05:34,980
we've got an objective function we can optimize the classification instead of this procedural description

1918
02:05:34,980 --> 02:05:36,100
of the perceptron

1919
02:05:36,150 --> 02:05:38,910
i've got a weight vector and i'm going to

1920
02:05:38,920 --> 02:05:41,530
spend less around according to some arbitrary rule

1921
02:05:41,550 --> 02:05:43,200
i've now got

1922
02:05:44,520 --> 02:05:48,400
an objective function i can say i'm going to spend more around this weight vector

1923
02:05:49,310 --> 02:05:52,220
it's a really good explanation of my labels

