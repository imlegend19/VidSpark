1
00:00:00,000 --> 00:00:06,580
we would have to find that confirmation for the hypothesis can be obtained if we

2
00:00:06,580 --> 00:00:08,580
observe black ravens

3
00:00:08,590 --> 00:00:10,800
because that's something that happens

4
00:00:10,810 --> 00:00:16,810
that's how science works right we make observations and from these observations we try to

5
00:00:16,810 --> 00:00:23,130
conclude general statements and if we observe hundreds of thousands of ravens and were all

6
00:00:23,150 --> 00:00:28,780
black and we have the inclination to think that all ravens may be black

7
00:00:30,100 --> 00:00:31,400
on the other hand

8
00:00:32,170 --> 00:00:36,620
the resolution of the paradox would require that we would have very little or no

9
00:00:36,620 --> 00:00:42,920
confirmation fight for this hypothesis if we observe non non-black non ravens this orange or

10
00:00:42,920 --> 00:00:44,260
red chairs

11
00:00:44,680 --> 00:00:50,510
and finally i would think that the degree of confirmation obtained depends on what we

12
00:00:50,510 --> 00:00:51,530
already know

13
00:00:51,540 --> 00:00:52,640
on on on

14
00:00:52,650 --> 00:00:55,440
on on our beliefs so to speak

15
00:00:59,490 --> 00:01:01,550
i'm not going to present

16
00:01:01,550 --> 00:01:09,700
one way of how a bayesian probabilistic may approach this problem and and possibly resolved

17
00:01:11,690 --> 00:01:18,040
we represent the degree of belief in our hypothesis by p of page

18
00:01:18,060 --> 00:01:21,670
and as i said james would have put p of page given x

19
00:01:22,090 --> 00:01:26,020
because it would be all believe in the prophecies given

20
00:01:26,030 --> 00:01:30,210
they the other knowledge that we have all about the world

21
00:01:30,220 --> 00:01:31,990
and now

22
00:01:32,000 --> 00:01:41,030
we just define this quantity of page given evidence so the proof of the confirmation

23
00:01:41,080 --> 00:01:48,420
that the evidence gives us about this hypothesis by this racial here so we say

24
00:01:48,420 --> 00:01:51,410
that the probability

25
00:01:51,460 --> 00:01:56,410
after the process is given the evidence over the probability e

26
00:01:56,410 --> 00:02:02,440
of the hypothesis without knowing about the evidence that this is our measure of confirmation

27
00:02:04,680 --> 00:02:07,540
this will be a greater one

28
00:02:07,550 --> 00:02:13,650
if our belief in this hypothesis has been has been strengthened by observing the evidence

29
00:02:13,690 --> 00:02:18,570
and if nothing has happened will remain close to one or one

30
00:02:18,580 --> 00:02:23,380
now i you can use base

31
00:02:23,390 --> 00:02:24,460
through now

32
00:02:24,480 --> 00:02:29,600
and exp on this thing and what you find is that this quantity here is

33
00:02:29,600 --> 00:02:33,390
equal to this quantity here i don't want to go through the steps but it

34
00:02:33,390 --> 00:02:37,140
turns out that the crucial evidence dependent quantity here

35
00:02:37,150 --> 00:02:43,120
it is the probability of the evidence given the negation of the the hypothesis over

36
00:02:43,120 --> 00:02:48,230
the probability of the evidence given that the hypothesis is true

37
00:02:48,250 --> 00:02:50,400
so we're going to analyse this

38
00:02:50,400 --> 00:02:52,690
this expression here

39
00:02:52,720 --> 00:03:02,680
by the way is there anyone here who would consider himself a bayesian

40
00:03:04,840 --> 00:03:09,430
half life half bayesian

41
00:03:09,510 --> 00:03:13,410
is there anyone who considers non bayesian

42
00:03:13,410 --> 00:03:24,080
or anti bayesian possibly it's going to be i think it's great that you find

43
00:03:24,100 --> 00:03:29,850
much stronger opinions in other surrounding i think so people can really have a lot

44
00:03:29,850 --> 00:03:31,600
of conflict about this

45
00:03:31,600 --> 00:03:36,380
well anyway so this is the racial we're going to analyse

46
00:03:36,430 --> 00:03:42,590
and let's analyse this racial for evidence type one evidence type one is the evidence

47
00:03:42,590 --> 00:03:45,430
we observe black raven so

48
00:03:45,430 --> 00:03:49,710
this is the racial we're looking at and our observation is an object a which

49
00:03:49,710 --> 00:03:52,660
is the raven and which is black

50
00:03:52,660 --> 00:03:55,300
right so i've just plug in

51
00:03:55,350 --> 00:04:01,340
raven and black here for for e one and here for you want sources here

52
00:04:01,350 --> 00:04:02,340
and here

53
00:04:02,380 --> 00:04:09,090
now i just use the one of these basic rules of probability and right

54
00:04:09,130 --> 00:04:11,220
are a and b

55
00:04:11,240 --> 00:04:16,820
the probability of our and b is the probability of b given are times the

56
00:04:16,820 --> 00:04:19,360
probability of are

57
00:04:19,400 --> 00:04:24,170
and i do the same thing here in the denominator

58
00:04:24,180 --> 00:04:28,250
so now this is the expression we want analyse

59
00:04:29,870 --> 00:04:32,720
so what we have here is

60
00:04:33,400 --> 00:04:37,530
we in this particular expression we assume that the hypothesis is true

61
00:04:37,540 --> 00:04:40,590
the hypothesis that all ravens are black

62
00:04:40,600 --> 00:04:45,940
so the probability that something is black or that a is black given that it

63
00:04:45,950 --> 00:04:48,740
is a raven and the hypothesis is true

64
00:04:48,760 --> 00:04:53,100
a is true is one of course because given that the opposite is true

65
00:04:53,100 --> 00:04:54,690
i think you know

66
00:04:58,430 --> 00:05:01,670
o five that you see when you

67
00:05:02,980 --> 00:05:04,620
so you

68
00:05:04,640 --> 00:05:06,600
right to differentiate

69
00:05:07,790 --> 00:05:09,310
and the that

70
00:05:13,560 --> 00:05:16,250
so now the fact he

71
00:05:16,270 --> 00:05:18,310
when this is true

72
00:05:18,330 --> 00:05:20,160
and if you

73
00:05:20,190 --> 00:05:24,290
particular distribution of foreground background before

74
00:05:24,290 --> 00:05:30,690
we can calculate that an optimal threshold

75
00:05:30,710 --> 00:05:33,890
well one of which is

76
00:05:33,910 --> 00:05:34,790
o thing

77
00:05:43,020 --> 00:05:47,960
color with different coloured all

78
00:05:48,310 --> 00:05:51,690
this of all

79
00:05:53,440 --> 00:05:54,640
by the patient

80
00:05:55,000 --> 00:05:56,750
and the

81
00:05:56,770 --> 00:06:01,620
so i think illustration the book and i was also from the book

82
00:06:01,710 --> 00:06:03,370
this the black and white

83
00:06:03,390 --> 00:06:07,230
picture but major the color and

84
00:06:09,660 --> 00:06:11,600
try to

85
00:06:17,460 --> 00:06:20,540
and this the right

86
00:06:22,460 --> 00:06:26,310
is the segmentation by a

87
00:06:30,120 --> 00:06:33,100
but just to play

88
00:06:37,080 --> 00:06:41,250
of the eight

89
00:06:42,960 --> 00:06:46,350
those who were

90
00:06:50,250 --> 00:06:52,080
so what i

91
00:07:04,100 --> 00:07:06,600
fine partition

92
00:07:06,600 --> 00:07:09,670
the image that just like

93
00:07:11,370 --> 00:07:13,670
that one right

94
00:07:18,170 --> 00:07:21,190
all levels equal to each other

95
00:07:21,210 --> 00:07:22,540
four them

96
00:07:22,620 --> 00:07:24,160
and our

97
00:07:24,190 --> 00:07:29,100
there are six of the one show

98
00:07:29,190 --> 00:07:31,440
and if we

99
00:07:33,890 --> 00:07:36,910
we have all the image

100
00:07:36,960 --> 00:07:40,660
and our original standard

101
00:07:49,310 --> 00:07:50,850
so we have

102
00:07:52,230 --> 00:07:55,790
this is because the initial

103
00:07:55,790 --> 00:07:58,750
and this one pixel

104
00:07:58,810 --> 00:08:01,120
and i try to

105
00:08:01,870 --> 00:08:03,290
o range

106
00:08:04,770 --> 00:08:06,540
like the right

107
00:08:06,560 --> 00:08:08,870
in the brain

108
00:08:08,870 --> 00:08:14,040
changes in the level because some

109
00:08:14,060 --> 00:08:17,390
so we

110
00:08:17,410 --> 00:08:20,140
range of people

111
00:08:21,730 --> 00:08:23,540
i have had

112
00:08:23,580 --> 00:08:25,100
so the goal

113
00:08:25,100 --> 00:08:33,480
to all the children in the qualitative difference between the two

114
00:08:33,580 --> 00:08:35,270
not to

115
00:08:35,330 --> 00:08:36,210
so we we have

116
00:08:36,250 --> 00:08:41,620
this is of the one direction and called the the direction and

117
00:08:45,960 --> 00:08:49,310
why this procedure all pixels of the image

118
00:08:49,310 --> 00:08:50,620
so here know

119
00:08:50,640 --> 00:08:52,890
the result growing

120
00:08:56,730 --> 00:08:59,210
boundaries by segment

121
00:08:59,350 --> 00:09:01,210
i actually

122
00:09:01,230 --> 00:09:07,620
on the top of the original image

123
00:09:07,620 --> 00:09:10,960
and we want to

124
00:09:13,810 --> 00:09:15,640
when tried

125
00:09:18,480 --> 00:09:19,930
we have

126
00:09:21,750 --> 00:09:23,370
there are few

127
00:09:29,080 --> 00:09:31,960
so first of all

128
00:09:31,980 --> 00:09:33,250
one thing

129
00:09:33,250 --> 00:09:35,560
and that not believe

130
00:09:36,480 --> 00:09:37,810
so we

131
00:09:39,810 --> 00:09:41,620
the what

132
00:09:41,660 --> 00:09:46,660
and will you out there is still holds a partition

133
00:09:46,670 --> 00:09:48,000
if it is true

134
00:09:48,000 --> 00:09:51,230
yes is quite a lot of things

135
00:09:51,250 --> 00:09:52,440
if there is no

136
00:09:55,140 --> 00:09:56,330
he was

137
00:09:57,190 --> 00:09:58,560
and the police

138
00:09:59,250 --> 00:10:01,520
four average

139
00:10:01,960 --> 00:10:03,480
in this

140
00:10:03,730 --> 00:10:05,600
good prediction

141
00:10:05,670 --> 00:10:07,120
then we saw

142
00:10:07,120 --> 00:10:09,520
it's not going to partition

143
00:10:09,540 --> 00:10:11,730
and we also

144
00:10:13,440 --> 00:10:17,350
because there are connected range

145
00:10:20,620 --> 00:10:24,790
two decades of

146
00:10:24,890 --> 00:10:27,910
examples like but the

147
00:10:27,910 --> 00:10:29,290
in this way

148
00:10:30,310 --> 00:10:33,690
the predicate is true

149
00:10:33,690 --> 00:10:38,170
you get a big screen TV turned on your high-definition channel you watching a great

150
00:10:38,170 --> 00:10:40,660
movie during the ads

151
00:10:40,670 --> 00:10:41,830
you'll also you'll see

152
00:10:41,850 --> 00:10:43,320
you'll see ads for

153
00:10:43,330 --> 00:10:47,940
you know maybe it's for blind you consider couch with or maybe it's for new

154
00:10:47,940 --> 00:10:51,280
cleaning product review from nineteen again next one three

155
00:10:51,310 --> 00:10:56,180
the reason using those as you're not seeing brand which typically pay higher CPM cost

156
00:10:56,180 --> 00:10:57,350
per thousand

157
00:10:57,360 --> 00:11:01,330
it is because there is no data about the audience you know the the

158
00:11:01,330 --> 00:11:05,440
the fragmentation media is actually samples may only have one person who watch the show

159
00:11:05,440 --> 00:11:10,160
maybe no people and therefore there is no information available to the media by justify

160
00:11:10,160 --> 00:11:14,590
the expense where someone who has product way where they are in they are in

161
00:11:14,590 --> 00:11:16,920
same thing you did call now

162
00:11:16,940 --> 00:11:18,470
they have quantification

163
00:11:18,490 --> 00:11:21,620
the value that advertising because they're receiving phone calls

164
00:11:21,630 --> 00:11:25,470
for someone is building the brand for product you can buy in the store offer

165
00:11:25,480 --> 00:11:30,190
car they don't know they're reaching and they can't justify the expenditure

166
00:11:31,200 --> 00:11:34,960
you know this situation

167
00:11:35,070 --> 00:11:40,470
seems really strange to myself my co-founder concourse four years ago when we were looking

168
00:11:40,470 --> 00:11:43,820
at forming the company and the

169
00:11:43,830 --> 00:11:48,470
reason the thing that really surprised this is the sample based techniques to develop broadcast

170
00:11:49,290 --> 00:11:51,060
they have been applied to the internet

171
00:11:51,070 --> 00:11:53,600
and yet the internet is isn't broadcast

172
00:11:53,620 --> 00:11:58,830
every piece of content is delivered individually do your browsers based on the requests that

173
00:11:58,830 --> 00:12:05,220
you've instructed your browser to make those and those delivery processes actually create records

174
00:12:05,280 --> 00:12:10,080
and so when we launched comcast goal was to

175
00:12:10,120 --> 00:12:12,570
enable addressable advertising scale

176
00:12:12,680 --> 00:12:17,480
was to provide a framework technology analytics framework that would enable third parties

177
00:12:17,500 --> 00:12:21,340
media companies and marketers to participate in two

178
00:12:21,370 --> 00:12:27,210
configure themselves real-time decision making based on models that are pertinent to their objectives

179
00:12:27,450 --> 00:12:30,630
but before you get we had to fix audience measurement

180
00:12:30,690 --> 00:12:34,730
and the way we did this was the launch free service not based on sample

181
00:12:34,730 --> 00:12:40,170
based approaches based on direct measurement media consumption and what we did was use machine

182
00:12:40,170 --> 00:12:45,050
learning to infer the audience characteristics so to understand this direct measurement we just take

183
00:12:45,050 --> 00:12:46,190
a step back

184
00:12:46,410 --> 00:12:52,400
and look at the process of online advertising all fragments case online delivery any piece

185
00:12:52,400 --> 00:12:54,430
of content

186
00:12:54,440 --> 00:13:00,900
whatever content is displayed it's received from server and all measurements this actually use piece

187
00:13:00,910 --> 00:13:08,380
of HTML code instructions to the browser to request the browser also ask for measurement

188
00:13:08,380 --> 00:13:13,460
tag from comcast so anyone can sign up for the service in place comcast measurement

189
00:13:13,460 --> 00:13:15,830
tag within the digital media assets

190
00:13:15,880 --> 00:13:17,890
whatever that tag is fine

191
00:13:17,910 --> 00:13:22,180
individuals browser makes requested as server we send a message back saying we've got your

192
00:13:22,180 --> 00:13:24,750
request and we're able to record

193
00:13:24,750 --> 00:13:31,020
the fact that request occurred and what information they recording is basically weblog http protocol

194
00:13:32,010 --> 00:13:36,970
recording the URL to the actual page the consumer was on recording the time the

195
00:13:36,970 --> 00:13:41,190
request was made by the IP addresses recorded systems needs to know what's in the

196
00:13:41,190 --> 00:13:42,840
content back we know

197
00:13:42,920 --> 00:13:46,410
you know whether they're running mac or PC with a request came from from and

198
00:13:46,410 --> 00:13:50,680
i found this very basic web log data now importantly

199
00:13:50,690 --> 00:13:57,570
we're not collecting personal information such as names addresses and telephone numbers social security numbers

200
00:13:57,810 --> 00:14:01,360
in fact there's nothing in terms of these weblogs actually tell us anything about who

201
00:14:01,360 --> 00:14:02,920
those people are

202
00:14:06,050 --> 00:14:10,560
actually before we launched cost was that obviously this should enable you to count much

203
00:14:10,560 --> 00:14:11,660
more accurately

204
00:14:11,690 --> 00:14:16,680
you know how many times people requested content from a particular site the hypothesis was

205
00:14:16,680 --> 00:14:20,150
that by organizing the data properly and applying machine learning techniques

206
00:14:20,170 --> 00:14:23,880
it will be possible to actually also start to automatically infer

207
00:14:23,890 --> 00:14:25,410
the characteristics of the audience

208
00:14:25,420 --> 00:14:29,170
and we conducted some experiments in in two thousand six to two thousand six convinced

209
00:14:29,170 --> 00:14:30,280
ourselves it possible

210
00:14:30,750 --> 00:14:35,800
but i really need to get more horsepower into our team

211
00:14:35,820 --> 00:14:38,410
we're not where there was there was over there was the reason that

212
00:14:38,470 --> 00:14:43,300
most of the academic papers are written whenever except in places like this and so

213
00:14:43,350 --> 00:14:46,340
we decided to get some way to get some people who can really take take

214
00:14:46,340 --> 00:14:51,760
these core concepts and develop much more comprehensively and we're fortunate were able to recruit

215
00:14:51,760 --> 00:14:57,800
a really fantastic team of scientists at san francisco were based was also very handy

216
00:14:57,800 --> 00:15:02,560
the standard just down the road and were able to persuade trevor hasty jerry friedman

217
00:15:02,560 --> 00:15:06,250
to join our advisory board may be an active participant in our african development for

218
00:15:06,250 --> 00:15:07,150
the past

219
00:15:07,160 --> 00:15:08,730
for the past four years

220
00:15:08,770 --> 00:15:11,370
what they want us to do

221
00:15:11,470 --> 00:15:13,600
it's not just count page views

222
00:15:13,630 --> 00:15:18,680
and and the number of visits to website but actually start to infer and understand

223
00:15:18,690 --> 00:15:23,100
unique audience characteristics is certain things are fairly straightforward

224
00:15:23,110 --> 00:15:28,430
it's reasonably straightforward to geo locate individuals and understand the basis of an IP address

225
00:15:28,430 --> 00:15:32,530
where they're located and therefore understand audiences from a particular area

226
00:15:32,540 --> 00:15:36,330
but we can go way beyond that we can actually start to understand

227
00:15:36,340 --> 00:15:37,350
whether they

228
00:15:37,360 --> 00:15:41,160
people are consuming the content from work well how i would go so far as

229
00:15:41,160 --> 00:15:43,480
to understand the demographics

230
00:15:43,480 --> 00:15:48,420
and for that matter any arbitrary set of audience characteristics for which we can get

231
00:15:48,430 --> 00:15:52,180
key reference points into almost touch on that one

232
00:15:52,210 --> 00:15:56,840
but this approach was was really quite quite revolutionary in the market because it meant

233
00:15:56,840 --> 00:15:59,280
the a web publisher of any size

234
00:15:59,330 --> 00:16:04,060
and typically with with sample based solutions website had have tens or hundreds of thousands

235
00:16:04,060 --> 00:16:07,880
of people for any measurement can be made in the audience with this approach we

236
00:16:07,880 --> 00:16:13,540
could make measurements of audiences all sub audiences which sometimes just the few hundred people

237
00:16:13,760 --> 00:16:16,910
and what it meant is lots of people started to participate in the service was

238
00:16:16,910 --> 00:16:21,660
completely free there's no complex contracts and payments people had to make it come to

239
00:16:21,660 --> 00:16:26,040
our site they could sign up get measurement and what is meant is that over

240
00:16:26,040 --> 00:16:27,090
the last

241
00:16:27,110 --> 00:16:29,510
four years

242
00:16:29,540 --> 00:16:33,260
data volumes have grown dramatically and our entire modeling approach

243
00:16:33,330 --> 00:16:36,240
gets better with more data that has

244
00:16:36,250 --> 00:16:39,210
and you can see over the last three years

245
00:16:39,210 --> 00:16:43,380
this is a hierarchical clustering results you get

246
00:16:43,390 --> 00:16:49,190
it's very nice it's like minimum spanning tree except that you have two components

247
00:16:49,230 --> 00:16:52,980
and then if you do majority vote

248
00:16:53,010 --> 00:16:56,540
well since there is only one labelled data points in each cluster you get this

249
00:16:56,540 --> 00:16:58,800
very nice prediction

250
00:16:58,980 --> 00:17:01,290
corresponds to our notion

251
00:17:01,300 --> 00:17:03,020
but if you do

252
00:17:03,030 --> 00:17:08,160
if you happened to pick something else for example complete linkage

253
00:17:08,160 --> 00:17:09,970
that's the cluster you get

254
00:17:09,990 --> 00:17:16,620
and because our label points are here and here

255
00:17:16,630 --> 00:17:21,310
majority vote with be confused and if you have the rule of breaking the ties

256
00:17:21,310 --> 00:17:26,500
randomly that would be the classification get and which looks terrible

257
00:17:27,810 --> 00:17:31,900
this is just another case to show you that be careful with the assumption that

258
00:17:32,130 --> 00:17:33,340
you make

259
00:17:50,410 --> 00:17:52,160
church so it's not that

260
00:17:52,180 --> 00:17:56,440
it's not that the either but just didn't get you there

261
00:17:57,570 --> 00:18:03,260
and this is an of course this is the artificial example just to show you

262
00:18:03,270 --> 00:18:04,710
what happened

263
00:18:11,040 --> 00:18:11,710
all right

264
00:18:13,190 --> 00:18:14,950
so let's move on to

265
00:18:15,030 --> 00:18:17,250
a very different kind of

266
00:18:17,300 --> 00:18:20,070
so my supervised learning models

267
00:18:21,230 --> 00:18:24,770
co training and multiview learning

268
00:18:24,930 --> 00:18:25,980
so let's

269
00:18:26,030 --> 00:18:27,710
use the example of

270
00:18:27,730 --> 00:18:29,090
named entity

271
00:18:30,290 --> 00:18:32,130
for classification

272
00:18:32,130 --> 00:18:38,280
and it's very simplified the task here so this is a task in natural language

273
00:18:38,300 --> 00:18:43,780
processing in particular when you were some companies crawl the web page they want to

274
00:18:43,780 --> 00:18:51,940
automatically processed texts and identify the person location organisation et cetera in the in the

275
00:18:51,940 --> 00:18:57,800
text and they want to do it using a classifier that scott named entity recognition

276
00:18:58,050 --> 00:19:00,220
named entity is

277
00:19:00,240 --> 00:19:03,940
a phrase which is the name and you want to say oh it's name is

278
00:19:03,940 --> 00:19:08,740
this person or is this name is is the place or is an organisation to

279
00:19:08,920 --> 00:19:12,470
so let's consider the simplified version where we have

280
00:19:12,590 --> 00:19:14,990
person and location

281
00:19:15,490 --> 00:19:18,210
one example would be mister washington

282
00:19:18,220 --> 00:19:24,650
that we know it's a person and then at washington state with the location

283
00:19:28,650 --> 00:19:32,570
being this task

284
00:19:33,780 --> 00:19:35,900
here's what's making it special

285
00:19:36,300 --> 00:19:40,840
beyond the named entity itself

286
00:19:40,880 --> 00:19:44,550
we actually know what's the text surrounding it

287
00:19:45,420 --> 00:19:50,400
so in this case so we have the instance one that is the text surrounding

288
00:19:50,400 --> 00:19:55,990
it in text is headquartered in and i use on the score for this context

289
00:19:56,010 --> 00:20:00,240
and they have the name entity itself and in in the second instance we have

290
00:20:00,240 --> 00:20:02,940
mister washington the vice president could

291
00:20:02,950 --> 00:20:05,490
so this is the a case where you

292
00:20:05,490 --> 00:20:09,030
first of all let's assume you know where are the named entity but secondly also

293
00:20:09,070 --> 00:20:10,860
know what's the next it

294
00:20:15,240 --> 00:20:21,510
because of this we can represent each object in this case each object is named

295
00:20:22,440 --> 00:20:26,440
by a feature vector that consists of two parts

296
00:20:26,450 --> 00:20:28,590
or we will call it two views

297
00:20:29,550 --> 00:20:34,510
so this is my notation but these two views the first few years

298
00:20:34,530 --> 00:20:37,840
the word is often named entity itself

299
00:20:37,860 --> 00:20:44,170
the second view is the words surrounding it in the context

300
00:20:44,220 --> 00:20:47,260
so i divide my feature vector into two parts

301
00:20:47,280 --> 00:20:50,900
now why it useful forward we will see that it

302
00:20:54,090 --> 00:20:57,690
but let's first two and a little exercise

303
00:20:57,710 --> 00:21:01,240
if i give you is an instance one

304
00:21:01,260 --> 00:21:03,720
an instance two

305
00:21:03,740 --> 00:21:09,130
and i label them for you so that you know being states in location and

306
00:21:09,130 --> 00:21:12,550
this p there means it's a person

307
00:21:12,650 --> 00:21:14,400
and i ask you

308
00:21:14,440 --> 00:21:17,220
that's your training that you have to label data points

309
00:21:17,240 --> 00:21:18,340
to classify

310
00:21:18,530 --> 00:21:26,050
those two points

311
00:21:26,070 --> 00:21:30,760
how can we do that

312
00:21:30,800 --> 00:21:32,800
what can we do that

313
00:21:44,280 --> 00:21:45,240
o well

314
00:21:51,590 --> 00:21:58,420
i didn't expect that i but that's a very good guess although i suspect that

315
00:21:58,420 --> 00:22:02,440
will not work too well on the whole corpus

316
00:22:02,450 --> 00:22:07,320
of course we know english so we know what the correct classification of this but

317
00:22:07,320 --> 00:22:11,940
pretend you don't know english so that it's not possible to do that

318
00:22:11,990 --> 00:22:17,240
now my claim is actually if you have more unlabelled data then that might actually

319
00:22:17,240 --> 00:22:19,010
help let's see

320
00:22:19,130 --> 00:22:24,800
so now you have three four five has more unlabelled data points

321
00:22:24,840 --> 00:22:26,150
let's try to do this

322
00:22:27,920 --> 00:22:35,970
i know headquartered in washington state and that its location so by instances three although

323
00:22:35,970 --> 00:22:39,970
i know nothing about it i see the same context

324
00:22:39,990 --> 00:22:43,340
now you have to make the assumption you make this assumption that you have

325
00:22:43,340 --> 00:22:46,380
know saying you know don't those should match the data distribution

326
00:22:47,140 --> 00:22:49,780
but unfortunately this time he is difficult to compute

327
00:22:50,320 --> 00:22:52,570
because there are exponentially many configurations right

328
00:22:54,760 --> 00:22:57,530
and we'll see some ways of doing doing that

329
00:22:58,450 --> 00:23:00,970
you know if you look at markov random fields with latent variables

330
00:23:02,010 --> 00:23:06,200
that becomes even more challenging and that's the challenge when we start looking at deep learning models

331
00:23:06,840 --> 00:23:11,910
here these acts is composed both with something called visible

332
00:23:12,490 --> 00:23:15,800
variable select pixels and images speech signal words in a document

333
00:23:16,280 --> 00:23:17,550
as well as latent variables

334
00:23:18,030 --> 00:23:20,240
and these latent variables you know

335
00:23:20,820 --> 00:23:26,660
i can tell us something about you know you the topics the semantic meaning of of of the data

336
00:23:28,090 --> 00:23:30,880
you can specify the probability in exactly the same way

337
00:23:31,760 --> 00:23:33,880
but the problem is again you have this

338
00:23:34,380 --> 00:23:39,590
normalizing constant but even more problematic to have this summation over the latent variables

339
00:23:40,320 --> 00:23:43,220
so if you try to do inference in these models you basically want say well

340
00:23:43,590 --> 00:23:46,400
what's the distribution of the latent variables that i'm trying to infer

341
00:23:48,820 --> 00:23:53,660
and the parameter learning becomes very very challenging task so a lot of work and

342
00:23:53,860 --> 00:23:55,130
i'll show you some of the worker

343
00:23:55,840 --> 00:23:58,930
in the deep learning communities factor basically figure out how can you do with the

344
00:23:59,140 --> 00:24:02,320
intractability and as i go through the door are gonna make it will be more

345
00:24:02,340 --> 00:24:03,030
more precise

346
00:24:05,510 --> 00:24:06,450
so let me move

347
00:24:06,970 --> 00:24:07,380
in two

348
00:24:08,430 --> 00:24:10,450
a class of models called the stick boss missions

349
00:24:10,910 --> 00:24:14,320
and these kinds of models that are very useful for learning low-level features

350
00:24:19,070 --> 00:24:19,910
these are kinds of

351
00:24:20,360 --> 00:24:21,760
a model sometimes they called

352
00:24:22,200 --> 00:24:25,990
undirected graphical model is a bipartite you have a bipartite structure

353
00:24:26,760 --> 00:24:32,610
in the simplest model you have stochastic binary visible variables again think of them as pixels in your images

354
00:24:33,220 --> 00:24:37,280
and you have stochastic binary hidden variables we can think of them as feature detectors

355
00:24:38,470 --> 00:24:42,400
so feature detectors you know that can tell us something about maybe semantic

356
00:24:42,800 --> 00:24:46,280
representation of documents or some kind of

357
00:24:46,780 --> 00:24:49,050
a little high-level features that you see in the data

358
00:24:49,860 --> 00:24:51,800
right you can specify the energy

359
00:24:52,300 --> 00:24:56,680
of this configuration in the simplest way of specifying the energy is just using air

360
00:24:57,470 --> 00:24:59,240
it's just a linear combination here

361
00:25:00,340 --> 00:25:05,970
what this term is effectively during is it's basically saying well what's the correlation structure between each pixel

362
00:25:06,780 --> 00:25:07,800
as well as each

363
00:25:08,380 --> 00:25:09,090
we can variable

364
00:25:11,470 --> 00:25:15,570
and the parameters theta i just the parameters w as well as these biased and

365
00:25:15,590 --> 00:25:17,360
so these offset terms a common

366
00:25:18,800 --> 00:25:23,090
think you have now you can specify the probability of the joint distribution using a

367
00:25:23,090 --> 00:25:26,490
very standard definition right it's just a

368
00:25:28,140 --> 00:25:31,260
e to the negative of the energy or r if you write it explicitly

369
00:25:31,660 --> 00:25:32,660
you're essentially have

370
00:25:33,380 --> 00:25:36,590
this time where you have a pairwise sometimes is called pairwise potentials

371
00:25:37,050 --> 00:25:42,180
but again just modeling correlations between pixels or latent variables and you have a normalizing constant

372
00:25:42,740 --> 00:25:46,310
and these sometimes called you know if you've heard terms like markov random fields boltzmann

373
00:25:46,310 --> 00:25:48,220
machines log linear models this this

374
00:25:51,090 --> 00:25:54,160
correspond to the same basically the same model just different names

375
00:25:55,010 --> 00:25:57,320
now why these models are called restricted

376
00:25:58,880 --> 00:26:01,470
the reason why they call this that this is because there is no

377
00:26:01,950 --> 00:26:04,360
connections between these hidden variables

378
00:26:06,030 --> 00:26:08,180
and then has an advantage and disadvantage

379
00:26:09,410 --> 00:26:10,510
the key advantage

380
00:26:10,930 --> 00:26:14,530
is that it turns out that computing the distribution

381
00:26:15,160 --> 00:26:17,380
over the latent variables can be done in closed form

382
00:26:19,220 --> 00:26:21,200
and this is a very useful thing to have

383
00:26:22,050 --> 00:26:24,970
in particular you know if i show you a new image i can quickly tell

384
00:26:24,970 --> 00:26:26,680
you what features make up image

385
00:26:27,180 --> 00:26:31,510
well if i show you a document i can quickly tell you what topics make up at the document

386
00:26:32,070 --> 00:26:33,410
right so it's easy to compute

387
00:26:33,860 --> 00:26:38,880
and the same way the conditional probability of observing the image is again has a closed form solution

388
00:26:39,820 --> 00:26:40,590
right so

389
00:26:41,450 --> 00:26:43,470
if you apply this model to you know

390
00:26:44,010 --> 00:26:48,880
handwritten characters again this is the kind of structure that was a very closely related despite scoring model

391
00:26:49,340 --> 00:26:51,860
the one that we see it sort of tax will edges

392
00:26:52,780 --> 00:26:54,840
and then you can say well if i show you a new image

393
00:26:55,570 --> 00:26:58,610
well this new image can be written as a combination of these

394
00:27:00,180 --> 00:27:02,840
and it turns out to be you know useful thing to have because you can

395
00:27:02,840 --> 00:27:06,640
represent a lot of different images just using a subset of of of these bases

396
00:27:07,160 --> 00:27:10,840
so it's kind of like it natural is very simple basically tries to find

397
00:27:11,550 --> 00:27:14,110
this composition of of of these images

398
00:27:14,740 --> 00:27:18,450
most hidden variables are gonna be of these numbers are given like these numbers are

399
00:27:18,450 --> 00:27:21,930
given by the conditional probability of a particular feature being on

400
00:27:22,660 --> 00:27:25,740
and obviously can see for this image you not observing a feature like this

401
00:27:26,380 --> 00:27:28,160
will have a very very small probability

402
00:27:28,160 --> 00:27:30,890
one which

403
00:27:37,370 --> 00:27:38,030
OK so

404
00:27:38,100 --> 00:27:42,000
got kind of life she might have that initial belief

405
00:27:42,030 --> 00:27:46,830
then he comes the data someone actually gives you the labels she

406
00:27:46,830 --> 00:27:48,420
for these guys

407
00:27:48,440 --> 00:27:51,270
OK so now you have a label

408
00:27:51,320 --> 00:27:54,670
that ties the blue thing to hide the the

409
00:27:54,690 --> 00:27:57,170
measurement model

410
00:27:57,210 --> 00:28:00,770
and why there's noise will because you burn so there's the ship of there

411
00:28:00,780 --> 00:28:04,700
and you kind of look over industries and crossing of figure out which

412
00:28:04,710 --> 00:28:07,190
the ship is

413
00:28:07,210 --> 00:28:12,780
and enough instances showing and you know you have a good prior your parents think

414
00:28:12,960 --> 00:28:16,500
start output going on about

415
00:28:16,510 --> 00:28:17,800
and so

416
00:28:17,800 --> 00:28:21,910
you had prior shape you have your observation that brings in data

417
00:28:21,960 --> 00:28:25,690
the data being the tag shape and with that you know get posterior which is

418
00:28:25,690 --> 00:28:28,690
you know kind of nice

419
00:28:31,510 --> 00:28:34,790
i will be using a lot of graphical models which will hear more about the

420
00:28:34,790 --> 00:28:37,390
next week in way to represent this model

421
00:28:37,960 --> 00:28:43,200
the probabilistic models with this graph had a brief introduction to to this yesterday

422
00:28:43,220 --> 00:28:45,350
so no that is unshaded this

423
00:28:45,380 --> 00:28:46,960
something we want to learn

424
00:28:46,980 --> 00:28:51,910
the arrow indicates the conditional probability p of the given age

425
00:28:51,940 --> 00:28:55,920
and not by itself that and say that this is the marginal probability p of

426
00:28:57,430 --> 00:28:59,870
and the this whole thing denotes the joint

427
00:28:59,930 --> 00:29:02,190
of the of the comic

428
00:29:02,210 --> 00:29:06,760
this is the graphical way of representing this

429
00:29:06,770 --> 00:29:11,100
you some examples of the sort of thing

430
00:29:11,140 --> 00:29:14,950
and this one more thing i needed to say that

431
00:29:15,000 --> 00:29:16,770
this also

432
00:29:16,780 --> 00:29:18,650
the trick is

433
00:29:18,690 --> 00:29:21,130
to do this integral

434
00:29:21,620 --> 00:29:25,130
you need to the normalisation to another distribution

435
00:29:25,170 --> 00:29:29,930
so if you have many hypotheses many of current hypothesis could be huge

436
00:29:29,980 --> 00:29:32,770
and this is usually combinatorial integral

437
00:29:32,820 --> 00:29:34,610
a very high dimensional

438
00:29:34,890 --> 00:29:37,760
continues to grow

439
00:29:37,830 --> 00:29:42,510
full bayesian inference there is no optimisation

440
00:29:42,520 --> 00:29:45,570
it's all about computing the center

441
00:29:45,600 --> 00:29:49,650
it's an integration from the optimisation problem

442
00:29:51,210 --> 00:29:54,910
here's another example that i got from kevin murphy

443
00:29:55,590 --> 00:29:58,030
the language model

444
00:29:58,110 --> 00:30:00,110
probability of words in the language

445
00:30:00,120 --> 00:30:04,950
you have the likelihood that indicates how sounds produced given the words and we know

446
00:30:04,950 --> 00:30:09,330
how to the speech synthesis the synthesis pretty well and computers

447
00:30:10,710 --> 00:30:12,830
we have good models of that

448
00:30:14,110 --> 00:30:22,510
given sound we try to infer works for speech recognition

449
00:30:22,580 --> 00:30:25,800
recognise speech

450
00:30:25,850 --> 00:30:28,970
rex features a nudist beach in vancouver

451
00:30:29,070 --> 00:30:31,910
just wondering why kevin murphy came

452
00:30:32,070 --> 00:30:37,910
here's another example computer vision

453
00:30:37,910 --> 00:30:42,620
you have believe beliefs about the world the properties of the world and on how

454
00:30:42,620 --> 00:30:46,920
light comes theorize how it gets replaced an object and so on

455
00:30:46,950 --> 00:30:49,520
maybe some properties of objects

456
00:30:49,600 --> 00:30:52,330
pixels is not fixed

457
00:30:52,380 --> 00:30:54,520
contours and so on

458
00:30:54,570 --> 00:30:57,530
and you also know

459
00:30:57,550 --> 00:31:03,070
this computer graphics that tells you given the model you generate high you render images

460
00:31:03,070 --> 00:31:05,310
so you have an observation model

461
00:31:05,320 --> 00:31:09,180
and then from that observation model you try to do computer vision

462
00:31:09,870 --> 00:31:11,750
decide what the world is about

463
00:31:11,770 --> 00:31:14,080
given an image of the world

464
00:31:14,180 --> 00:31:16,510
again and integration problem

465
00:31:16,530 --> 00:31:20,000
in this sort of perhaps setting

466
00:31:20,010 --> 00:31:26,480
but now it's not only about learning and abstraction it's about

467
00:31:26,490 --> 00:31:28,700
you should learn an abstraction

468
00:31:28,710 --> 00:31:33,190
with the purpose in mind like markers mentioned yesterday

469
00:31:33,270 --> 00:31:38,500
and this is the sort of and it's not all people with actually believe this

470
00:31:38,500 --> 00:31:41,270
this is utilitarian view philosophy

471
00:31:41,280 --> 00:31:45,050
we do everything with you until it in my

472
00:31:45,370 --> 00:31:51,130
let's look at another example was so one yesterday but just a recap

473
00:31:51,180 --> 00:31:52,670
i assume that

474
00:31:52,710 --> 00:31:55,480
we first learn the population model

475
00:31:55,490 --> 00:32:00,250
and we do it this is very trivial density estimate like

476
00:32:00,320 --> 00:32:04,280
we saw yesterday you basically just count how many people healthy how many people have

477
00:32:04,280 --> 00:32:05,910
cancer in the population

478
00:32:05,920 --> 00:32:09,630
and that is that kind of a way of learning

479
00:32:11,220 --> 00:32:12,840
to describe the population

480
00:32:12,840 --> 00:32:16,840
in general this will be a more complex model but the methodology is the same

481
00:32:16,860 --> 00:32:20,600
once you have that you also can learn reward model

482
00:32:21,210 --> 00:32:22,570
a reward

483
00:32:22,570 --> 00:32:27,000
model actually csaba you're going to talk about that

484
00:32:28,350 --> 00:32:35,260
want to be the new

485
00:32:35,430 --> 00:32:42,830
now how do you pronounce it

486
00:32:42,840 --> 00:32:46,780
job you can talk about inverse

487
00:32:48,900 --> 00:32:50,000
now again

488
00:32:50,020 --> 00:32:54,510
but if you need to know more about this talk to him

489
00:32:56,050 --> 00:33:00,910
learning the rewards is a very tough problems

490
00:33:00,980 --> 00:33:06,190
you know usually we have an objective function to optimize objective function come from

491
00:33:06,200 --> 00:33:09,650
that in itself is quite challenging problem

492
00:33:10,110 --> 00:33:14,460
but let's assume we have learned that you're saying tell latest this algorithm that is

493
00:33:14,460 --> 00:33:16,340
not going to talk about

494
00:33:16,350 --> 00:33:20,760
and we know that if a patient is healthy and this might also be decided

495
00:33:20,760 --> 00:33:23,340
by a bunch of people doing policy-making

496
00:33:23,360 --> 00:33:27,160
you know you're the politicians in current government

497
00:33:27,170 --> 00:33:29,770
might decide that if you are healthy

498
00:33:29,820 --> 00:33:32,850
and you receive no treatment then there's no cost

499
00:33:32,870 --> 00:33:34,150
the nation

500
00:33:34,190 --> 00:33:38,810
if you're happy cancer and you receive no treatment then when you die

501
00:33:38,820 --> 00:33:41,350
and that's kind of a lot of resources

502
00:33:41,400 --> 00:33:42,530
to the nation

503
00:33:42,600 --> 00:33:46,740
if you are healthy and you receive treatment

504
00:33:46,800 --> 00:33:48,440
you lose your hair

505
00:33:48,460 --> 00:33:51,460
and the minus thirty cost

506
00:33:51,510 --> 00:33:54,320
and if you have cancer

507
00:33:54,330 --> 00:33:56,520
and you do receive treatment

508
00:33:56,580 --> 00:33:59,340
well you survive but still

509
00:33:59,350 --> 00:34:04,570
you know it's going through this

510
00:34:07,410 --> 00:34:09,690
the principle of expected utility

511
00:34:09,710 --> 00:34:12,330
which is not the axiom is not here

512
00:34:12,400 --> 00:34:16,570
expected utility i don't have time to go into it but

513
00:34:16,580 --> 00:34:17,890
it comes from

514
00:34:17,900 --> 00:34:20,670
actually the work of one nine

515
00:34:22,170 --> 00:34:24,180
the four is game

516
00:34:25,220 --> 00:34:26,410
and it's about

517
00:34:26,420 --> 00:34:29,480
choice if something comes from choice theory

518
00:34:30,060 --> 00:34:33,320
and about combining individual preferences

519
00:34:33,370 --> 00:34:35,930
how to come up with the voting

520
00:34:35,950 --> 00:34:38,060
that makes everyone happy

521
00:34:38,870 --> 00:34:42,210
you know the obvious things in preferences if you prefer it should be

522
00:34:42,270 --> 00:34:46,170
and you prefer b to c then you should prefer a c

523
00:34:46,200 --> 00:34:48,000
and then there are five more

524
00:34:48,050 --> 00:34:49,900
four more axioms

525
00:34:49,910 --> 00:34:52,150
we seem to have been an saleable

526
00:34:52,150 --> 00:34:56,410
and then you try to find the function that a social choice function

527
00:34:56,430 --> 00:35:00,420
that satisfies all the axioms and what arises is this thing

528
00:35:00,470 --> 00:35:02,110
called the expected data

529
00:35:02,110 --> 00:35:04,310
so it's rational in that sense

530
00:35:04,350 --> 00:35:06,470
if you gonna play

531
00:35:06,490 --> 00:35:10,270
the game like in game theory and unit to

532
00:35:11,430 --> 00:35:13,450
arrive at the nash equilibrium

533
00:35:14,430 --> 00:35:19,450
o what nash proved this is the chief you maximize this quantity each player maximizes

534
00:35:19,450 --> 00:35:22,280
how dimensions

535
00:35:24,970 --> 00:35:28,180
there are no complex numbers in high dimensions

536
00:35:28,200 --> 00:35:34,780
ah i thought for some time and maybe he will still be possible that missing

537
00:35:34,780 --> 00:35:40,300
dimension for it can be done with quaternion which is sort of the generalisation of

538
00:35:40,340 --> 00:35:47,410
complex numbers to dimension four but the problem with quaternion son why doesn't work is

539
00:35:47,410 --> 00:35:52,340
that the key property is that both in the mission one and mission two multiplication

540
00:35:52,340 --> 00:35:54,450
of numbers is community

541
00:35:54,450 --> 00:35:58,030
and multiplication what turns is not

542
00:35:58,050 --> 00:35:59,140
and so

543
00:35:59,160 --> 00:36:01,550
are you can no longer

544
00:36:01,610 --> 00:36:03,870
have these nice property

545
00:36:03,890 --> 00:36:05,530
that allows all the

546
00:36:05,610 --> 00:36:10,410
roots and all these business to go through and even even multiplication was community of

547
00:36:10,590 --> 00:36:12,160
how do you compute groups

548
00:36:12,220 --> 00:36:14,410
of polynomial singletons

549
00:36:14,430 --> 00:36:19,120
it is not so trivial as competing with the polynomials in the complex plane

550
00:36:20,280 --> 00:36:25,090
essentially the problem is that being as is based on just with polynomials can be

551
00:36:25,090 --> 00:36:27,970
generalized to more than two dimensions

552
00:36:28,140 --> 00:36:32,490
of course that technique but it doesn't necessarily need to work is just project your

553
00:36:32,530 --> 00:36:36,590
data into dimension one dimension two and then you apply the algorithm

554
00:36:38,240 --> 00:36:41,320
so that that would be a quick way around

555
00:36:43,180 --> 00:36:47,470
OK even though this is very simple and straightforward it actually solves

556
00:36:47,470 --> 00:36:52,880
one of the segmentation problems in vision which is segmentation based on intensities here one

557
00:36:52,880 --> 00:36:57,610
is given an image for instance this image of the penguin here and roughly speaking

558
00:36:57,610 --> 00:37:03,340
let's say there three groups here groups are defined by intensity so why it would

559
00:37:03,340 --> 00:37:04,870
be one group

560
00:37:04,870 --> 00:37:08,910
black would be another group and great would be another groups so here there three

561
00:37:08,930 --> 00:37:14,850
every pixel in the image has an intensity value so it's just a real number

562
00:37:14,950 --> 00:37:16,320
and therefore

563
00:37:16,320 --> 00:37:20,030
you can precisely apply the clustering technique based on

564
00:37:20,030 --> 00:37:25,390
just data believes in one so have to do is to fit a polynomial

565
00:37:26,800 --> 00:37:27,890
o p

566
00:37:27,910 --> 00:37:29,300
image intensities

567
00:37:29,300 --> 00:37:32,820
this is the number of data points here is the number of pixels

568
00:37:32,850 --> 00:37:36,530
and you find the roots of the problem is the problem in one variable

569
00:37:36,550 --> 00:37:37,930
of degree three

570
00:37:37,930 --> 00:37:39,910
and i will give you the

571
00:37:39,930 --> 00:37:44,910
cluster centres which in this case is going to be almost zero for black almost

572
00:37:44,910 --> 00:37:49,430
one for whites and almost point five from great

573
00:37:50,890 --> 00:37:55,370
these are the results of rhyme these onto this image

574
00:37:55,910 --> 00:37:58,390
these results with k means

575
00:37:58,410 --> 00:38:02,890
these results him and these results were disputed by technique and as you can see

576
00:38:02,890 --> 00:38:06,620
there's pretty much no difference in the results

577
00:38:06,640 --> 00:38:12,490
except that the k means any and take a lot longer because they need to

578
00:38:12,490 --> 00:38:15,340
iterate in order to find the solution

579
00:38:15,410 --> 00:38:18,620
while this thing gives solution right away

580
00:38:18,660 --> 00:38:27,030
these are cases in which actually k means and failed to find the right segmentation

581
00:38:27,300 --> 00:38:29,890
this is the case of the penguins

582
00:38:29,930 --> 00:38:33,760
this is supposed to be one group then we go back to the to the

583
00:38:33,760 --> 00:38:35,300
penguin image

584
00:38:35,320 --> 00:38:40,010
remember so white is mostly the bottom and the front of the dependence

585
00:38:43,220 --> 00:38:47,110
so that's what you get here so that group is mostly fine but the second

586
00:38:47,110 --> 00:38:50,050
group is mostly getting the borders of a much which is not one of the

587
00:38:50,050 --> 00:38:51,570
group's you're looking for

588
00:38:51,570 --> 00:38:54,220
and the same thing happens with him

589
00:38:54,240 --> 00:38:59,470
however if you just apply this purely algebraic technique you get a solution that is

590
00:38:59,470 --> 00:39:00,780
reasonably good enough

591
00:39:00,840 --> 00:39:05,840
to start with and use that to initialize both gains in the and they now

592
00:39:05,840 --> 00:39:07,490
converge to the right

593
00:39:07,490 --> 00:39:09,930
so this this example is just to show

594
00:39:09,950 --> 00:39:13,220
that indeed he m and k means can converge

595
00:39:13,220 --> 00:39:17,660
to the wrong place if they are not properly initialize and this quick technique of

596
00:39:17,660 --> 00:39:22,410
just fitting a polynomial in one variable to their points can be of help to

597
00:39:22,410 --> 00:39:25,700
do in the innovation

598
00:39:25,720 --> 00:39:32,320
these are just make comparison this can also be done

599
00:39:32,340 --> 00:39:34,990
four texture calculation segmentation

600
00:39:35,050 --> 00:39:38,280
the problem here is a lot a lot more difficult

601
00:39:40,450 --> 00:39:42,300
basically how

602
00:39:42,320 --> 00:39:45,620
in the world you're going to say that the entire tiger

603
00:39:46,660 --> 00:39:48,470
just one group

604
00:39:48,490 --> 00:39:50,320
based purely on

605
00:39:50,340 --> 00:39:55,970
either intensity or color information there are stripes there's a lot of variability the head

606
00:39:56,140 --> 00:39:58,320
has a lot of white parts

607
00:39:58,340 --> 00:40:01,240
the ten is completely different

608
00:40:01,240 --> 00:40:07,010
so segmenting this from the images is very very hard problem so we have to

609
00:40:07,030 --> 00:40:10,910
do is to come up with some sort of textual description which is in some

610
00:40:10,910 --> 00:40:17,320
sense a measure of the probability distributions of intensities or the colours in the neighborhood

611
00:40:17,320 --> 00:40:20,700
of every pixel in the underlying idea is that

612
00:40:21,700 --> 00:40:28,470
points that belong to the same group would have smaller less the same probability distribution

613
00:40:28,470 --> 00:40:33,350
these results on the bottom here are results from human segmentation

614
00:40:33,370 --> 00:40:36,220
so we can do the task quite well

615
00:40:36,220 --> 00:40:42,450
but we don't quite agree completely bizarre results from about one thousand humans and then

616
00:40:43,450 --> 00:40:49,030
the right in the segmentation in means that everybody agreed that that was a border

617
00:40:49,050 --> 00:40:51,760
in between images so here the tiger

618
00:40:51,780 --> 00:40:56,030
not every it's not perfectly not not everybody agrees but there is mostly in agreement

619
00:40:56,220 --> 00:40:59,890
and then some people separate the water some others don't touch

620
00:40:59,890 --> 00:41:05,140
so this result you can get by applying this polynomial technique

621
00:41:05,160 --> 00:41:07,390
and as you can see there are errors

622
00:41:07,510 --> 00:41:10,740
but it does a reasonably good job to to start with

623
00:41:10,760 --> 00:41:14,820
the water is separated here in three groups there is a big change of intensity

624
00:41:14,820 --> 00:41:20,200
here because this partisan these parties under the shadow similar here and then most of

625
00:41:20,200 --> 00:41:24,910
the body of the tiger is segmented properly but there are some errors here close

626
00:41:24,910 --> 00:41:27,970
to legs is a little bit lighter

627
00:41:27,990 --> 00:41:31,180
the head of course comes out differently because white

628
00:41:31,200 --> 00:41:35,910
and it's impossible to get detailed because you have to be the probability distribution in

629
00:41:35,910 --> 00:41:39,990
the neighborhood and so you pick a neighborhood that is too small

630
00:41:40,390 --> 00:41:43,050
it's very hard to do but if you pick an image that is too large

631
00:41:43,050 --> 00:41:47,570
then you need very fine textures such as the case of the tail here also

632
00:41:47,570 --> 00:41:49,660
the case of the bushes which is very

633
00:41:49,700 --> 00:41:51,240
high-frequency textures

634
00:41:51,370 --> 00:41:56,930
and this is just more examples on on how

635
00:41:57,930 --> 00:42:01,390
incredibly simple technique of just fitting polynomials in one

636
00:42:02,050 --> 00:42:06,410
and one variable can be applied to very complicated problems in vision

637
00:42:06,430 --> 00:42:10,220
such as segmentation based on text

638
00:42:10,220 --> 00:42:15,660
OK so now i'm going to move so that that was just a matter of

639
00:42:16,700 --> 00:42:20,510
now i'm going to move the problem but i want to talk about which is

640
00:42:20,510 --> 00:42:22,520
we're going to ask

641
00:42:22,540 --> 00:42:25,920
after we take alpha hydroxy rear

642
00:42:25,920 --> 00:42:28,590
we're gonna put in coal cement

643
00:42:28,610 --> 00:42:30,190
why do we want to do that

644
00:42:31,000 --> 00:42:34,560
what we really want to do is to ask after the cells have reached have

645
00:42:34,560 --> 00:42:36,000
gone through s phase

646
00:42:36,010 --> 00:42:38,510
how soon does the label cell

647
00:42:38,520 --> 00:42:40,190
move from s phase

648
00:42:40,200 --> 00:42:42,000
all the way

649
00:42:42,040 --> 00:42:43,280
in the face

650
00:42:43,290 --> 00:42:45,710
how long does it take how long history two

651
00:42:45,840 --> 00:42:50,060
and so all is the following we take away the hydroxy urea this allows all

652
00:42:50,070 --> 00:42:54,560
these cells to begin to move out is this a fully synchronous culture

653
00:42:54,560 --> 00:42:57,990
well actually no because some of these cells are over here

654
00:42:58,010 --> 00:43:01,740
there's a whole bunch over here these are synchronous but the rest of the ones

655
00:43:01,780 --> 00:43:05,430
there it is scattered out so there's going to be some pioneers over here moving

656
00:43:06,200 --> 00:43:09,410
of the phalanx and there will be some stragglers and then going to be a

657
00:43:09,410 --> 00:43:14,260
big slide of these cells that are moving as the rear guard ahead

658
00:43:14,280 --> 00:43:18,340
now what we're going to do work after we take away the structure we at

659
00:43:18,340 --> 00:43:21,130
our old friend calls him

660
00:43:21,130 --> 00:43:24,020
and call summit

661
00:43:24,650 --> 00:43:26,390
tried to start right this time

662
00:43:26,400 --> 00:43:30,530
because it is going to block cells as we obviously said before

663
00:43:30,560 --> 00:43:31,930
right over here

664
00:43:32,070 --> 00:43:34,330
and then we're going do is the following

665
00:43:34,330 --> 00:43:38,090
we're going to look every hour to take some cells and put them in coal

666
00:43:38,180 --> 00:43:41,620
image or believe them because some of the whole time and every i'll take cells

667
00:43:41,620 --> 00:43:42,500
out of the petri dish

668
00:43:42,900 --> 00:43:46,250
which of it calls that since they were released in here and we're going to

669
00:43:46,250 --> 00:43:48,430
look at the matter face cells

670
00:43:48,450 --> 00:43:51,760
and how we look at the interface cells will here's amanda a cell

671
00:43:51,780 --> 00:43:54,190
it will be its chromosomes

672
00:43:54,240 --> 00:43:57,870
obviously the chromatic to this point

673
00:43:58,070 --> 00:43:59,730
under the microscope

674
00:43:59,750 --> 00:44:03,070
and they start accumulating up here

675
00:44:03,110 --> 00:44:07,660
and each and every hour take some of these men face cells

676
00:44:07,720 --> 00:44:09,520
will take them out of the petri dish

677
00:44:09,540 --> 00:44:13,950
and after we've put them in the that or will we live the petri dish

678
00:44:13,950 --> 00:44:18,810
if you want a book and after the there and they've they come over here

679
00:44:18,820 --> 00:44:20,970
will fix them onto the plate

680
00:44:21,010 --> 00:44:24,390
i will add some alcohol or something which caused him to stick to the plate

681
00:44:24,560 --> 00:44:29,960
they can swim array and then after that we already had some radioactive emulsion so

682
00:44:29,960 --> 00:44:34,080
here's let's imagine his itself the bottom of the plate metaphor is we're going to

683
00:44:34,080 --> 00:44:37,630
add some photographic emulsion on top of that

684
00:44:37,630 --> 00:44:41,350
like that and then what we're going to see is

685
00:44:41,400 --> 00:44:46,610
how soon we can detect radioactive metastases chromosomes

686
00:44:46,620 --> 00:44:49,430
how can we do that because each time

687
00:44:49,450 --> 00:44:52,910
an electron leaves the treaty is signed d

688
00:44:52,950 --> 00:44:58,030
each time a beta particle leaves it's going to cause a grain of the emulsion

689
00:44:58,030 --> 00:45:00,100
of silver to form

690
00:45:00,140 --> 00:45:01,170
in the

691
00:45:01,190 --> 00:45:05,100
in the photographic emulsion that we've layer above

692
00:45:05,140 --> 00:45:06,820
the cell

693
00:45:06,860 --> 00:45:11,410
so this rate this photo emulsion is only not we're detecting light but we're detecting

694
00:45:11,410 --> 00:45:16,870
when they're whenever this radioactivity in it this being emitted

695
00:45:17,110 --> 00:45:18,650
and what we're going to look for

696
00:45:18,690 --> 00:45:25,390
are grains silver grains that are located above

697
00:45:25,430 --> 00:45:27,430
the chromosomes in the microscope

698
00:45:27,620 --> 00:45:31,360
we can look down through this play through the emulsion on the chromosomes we can

699
00:45:31,360 --> 00:45:33,630
see them clearly i've shown you that before

700
00:45:33,630 --> 00:45:35,850
and we're going to ask ourselves the question

701
00:45:35,900 --> 00:45:39,760
when can we begin to associate silver grains with the chromosomes

702
00:45:39,820 --> 00:45:45,430
because those silver grains must have been incorporated into the chromosomes during

703
00:45:45,430 --> 00:45:50,670
to pay some attention to the possible differences between the training and test this and

704
00:45:51,190 --> 00:45:55,160
then path of was the first one to propose that support vector machines are tailor-made

705
00:45:55,170 --> 00:45:58,890
to to do this task in order to generalise from the training data test database

706
00:45:59,280 --> 00:46:01,890
here are some of the results on you train

707
00:46:01,910 --> 00:46:06,150
if you train a support vector machine to detect for example onset of solar and

708
00:46:06,150 --> 00:46:10,940
see if it can it does it does the job with eighty six percent accuracy

709
00:46:11,190 --> 00:46:16,640
on task four chances fifty percent to detect the onset of the cost and it

710
00:46:16,640 --> 00:46:23,430
does that was seventy percent accuracy and so on

711
00:46:23,530 --> 00:46:28,090
are a certain way in which discriminative classifiers can be pushed a little bit is

712
00:46:28,090 --> 00:46:31,360
to is to combine dynamics and

713
00:46:31,420 --> 00:46:33,110
and sparse data

714
00:46:33,130 --> 00:46:36,710
so for example if the

715
00:46:37,210 --> 00:46:42,260
the landmark detectors have to be trained using small datasets that we need but we

716
00:46:42,260 --> 00:46:45,770
need some kind of information about the dynamics over time

717
00:46:45,770 --> 00:46:50,440
in order to do correct classifications and then it's hard to do that using purely

718
00:46:50,440 --> 00:46:51,930
discriminative methods

719
00:46:53,890 --> 00:46:58,010
in fact the only way that there are there are there are very few methods

720
00:46:58,010 --> 00:47:03,030
developed recently to this discriminatively but it's extremely computationally intensive and we get much

721
00:47:03,070 --> 00:47:08,030
more reasonable methods using using bayesian inference and so let me introduce

722
00:47:08,040 --> 00:47:09,330
bayesian inference

723
00:47:09,350 --> 00:47:10,540
the other

724
00:47:10,550 --> 00:47:14,810
sort of superclass of machine learning methods are the set of bayesian methods that is

725
00:47:14,810 --> 00:47:15,630
to say

726
00:47:15,650 --> 00:47:21,140
the methods in which the methods in which the function is being estimated by the

727
00:47:21,140 --> 00:47:27,220
universal approximator must be correctly normalised probability density it cannot be simply an arbitrary function

728
00:47:27,220 --> 00:47:31,600
it must estimate the likelihood of the label variable y

729
00:47:31,690 --> 00:47:34,990
given the observation x and

730
00:47:35,030 --> 00:47:36,840
that that that

731
00:47:36,850 --> 00:47:41,770
that function estimation problem then naturally gives you any other function estimation problem if you

732
00:47:41,770 --> 00:47:46,920
can correctly estimate the probability density of y given x then you can minimize the

733
00:47:46,920 --> 00:47:51,110
probability of error by just using the maximum of posterior probability

734
00:47:51,210 --> 00:47:55,510
the value of y if you can correctly to estimate the probability density of y

735
00:47:55,510 --> 00:47:58,890
given x then you can choose the minimum mean square error value of

736
00:47:58,910 --> 00:48:03,320
why is your output in a real valued tracking regression problem so if you can

737
00:48:03,320 --> 00:48:08,710
do good function estimation for the probability density itself then you've solved every other problem

738
00:48:08,720 --> 00:48:14,620
the disadvantage of course is that learning the probability density is usually harder than learning

739
00:48:14,630 --> 00:48:16,900
the function that you want to the first place

740
00:48:16,940 --> 00:48:22,770
usually require more data and usually with the same size database you're you're subject to

741
00:48:22,770 --> 00:48:26,750
a larger amount of error in estimating the probability density then you would be simply

742
00:48:26,750 --> 00:48:31,990
estimating y is equal to its effects directly the big advantage is that with correct

743
00:48:31,990 --> 00:48:34,910
normalization you can string together a whole sequence

744
00:48:35,680 --> 00:48:38,530
one of these latent variables in order to

745
00:48:38,530 --> 00:48:41,090
in order to build relatively complex models

746
00:48:41,120 --> 00:48:43,650
so here's perhaps the most

747
00:48:43,660 --> 00:48:48,850
well at least in speed the most famous example of a bayesian system hidden markov

748
00:48:48,850 --> 00:48:52,290
model is a system in which the the label at each time depends on the

749
00:48:52,290 --> 00:48:57,880
labels that all the previous times as well as depending on the acoustic observations

750
00:48:57,890 --> 00:49:03,160
that is to say we we suppose that that your speech articulation system is some

751
00:49:03,160 --> 00:49:04,090
set of

752
00:49:04,090 --> 00:49:07,940
moving tongue and lips and so on and some some sort of intentions in the

753
00:49:07,940 --> 00:49:13,540
vocal folds that where the vocal folds approximate in the vocal folds separate and that

754
00:49:13,560 --> 00:49:19,350
in fact there's only a discrete finite number of settings of all that apparatus that

755
00:49:19,350 --> 00:49:25,530
matters so we move through this space of finite state is the discrete space of

756
00:49:25,530 --> 00:49:28,940
state variables and in a particular state

757
00:49:29,010 --> 00:49:32,850
the this speaker produces a particular set of acoustic spectra

758
00:49:32,860 --> 00:49:37,670
so that in effect the probability of a particular set of acoustic spectra given a

759
00:49:37,670 --> 00:49:40,330
particular set of other words

760
00:49:40,330 --> 00:49:45,030
is the is the probability of the current word given the previous work

761
00:49:45,080 --> 00:49:50,250
times the probability that the speaker's lips and tongue and gods and so on transition

762
00:49:50,250 --> 00:49:54,040
from the state they were in time t minus one to state the time time

763
00:49:54,060 --> 00:49:56,120
t given the world that you're in

764
00:49:56,370 --> 00:50:00,910
times the probability of getting a particular acoustic spectrum given that the lips and tongue

765
00:50:00,910 --> 00:50:04,920
and glottis and soft palate and so on or in a particular state

766
00:50:05,020 --> 00:50:11,510
these things are is this is this is the usual decomposition is used in speech

767
00:50:11,510 --> 00:50:15,970
recognition and hidden markov models i think you've seen in karen's talk on tuesday that

768
00:50:15,970 --> 00:50:19,780
we can decompose it further in order to bring more information to their but the

769
00:50:19,780 --> 00:50:24,200
usual decomposition is to is to talk about the language model which is a lookup

770
00:50:24,200 --> 00:50:28,610
table the pronunciation model which is a lookup table linear acoustic model which is itself

771
00:50:28,610 --> 00:50:35,350
some kind of universal approximator like like a mixture of gaussians

772
00:50:35,380 --> 00:50:41,380
learning and bayesian system is usually some variation of maximum likelihood learning and as in

773
00:50:41,450 --> 00:50:44,710
discriminative system if you have fewer than a thousand tokens you need to do some

774
00:50:44,710 --> 00:50:50,340
kind of regularization the usual to two of the common kinds of regularisation our maximum

775
00:50:50,350 --> 00:50:56,570
posterior probability estimation where instead of maximizing the probability of the data sets we maximize

776
00:50:56,570 --> 00:51:01,080
the probability of the parameters that is we maximize the probability of the log prior

777
00:51:01,080 --> 00:51:02,290
and other ten

778
00:51:02,310 --> 00:51:04,040
if we had a calcium prior

779
00:51:04,070 --> 00:51:05,530
would be that

780
00:51:05,550 --> 00:51:07,770
so it looks like an l two

781
00:51:07,780 --> 00:51:10,450
regularized version of maximum likelihood

782
00:51:13,990 --> 00:51:18,730
this method which was inspired by trying to do bayesian thing trying to do

783
00:51:18,810 --> 00:51:24,050
sensible probabilistic inference turns out to look just like sort of the regularisation schemes that

784
00:51:24,050 --> 00:51:27,410
we cooked up before so that tells us that there is a sensible thing to

785
00:51:29,990 --> 00:51:35,110
what we tried to do before was optimise the negative log probability lost that's how

786
00:51:35,110 --> 00:51:37,180
i introduced logistic regression

787
00:51:37,210 --> 00:51:40,770
here we're just trying to do inference in a probabilistic model

788
00:51:41,840 --> 00:51:45,210
we're not tied to particular loss function say

789
00:51:45,230 --> 00:51:46,740
in a decision rule

790
00:51:46,750 --> 00:51:50,080
we can pick the loss function

791
00:51:50,140 --> 00:51:51,550
in practice

792
00:51:51,550 --> 00:51:56,590
we might do better logprob loss than other losses because it does look a bit

793
00:51:56,590 --> 00:52:02,030
like fitting in

794
00:52:02,090 --> 00:52:05,660
when we made that connection and then

795
00:52:05,680 --> 00:52:09,200
answers questions we ask for like how to set lambda

796
00:52:09,250 --> 00:52:10,210
OK a

797
00:52:10,270 --> 00:52:14,070
in the bayesian framework any question that you might want to ask about how do

798
00:52:14,070 --> 00:52:16,340
i said something given the data is

799
00:52:17,430 --> 00:52:20,650
look at the probability then you do something

800
00:52:20,680 --> 00:52:23,020
sensible which is completely determined by

801
00:52:23,050 --> 00:52:24,820
the rules of probability

802
00:52:27,050 --> 00:52:32,760
what we wanted to do was find the label that optimizes this criterion the expected

803
00:52:32,760 --> 00:52:35,010
loss under model

804
00:52:35,060 --> 00:52:39,630
and this posterior distribution over the weights actually depends on lambda

805
00:52:39,690 --> 00:52:43,070
but we don't know lambda so what we do is we say

806
00:52:43,110 --> 00:52:46,790
well if we did know lambda we know what the posterior distribution is

807
00:52:46,800 --> 00:52:48,750
and we average over

808
00:52:48,760 --> 00:52:51,300
what we think lambda is now we've seen the data

809
00:52:51,350 --> 00:52:54,230
so this integral turns out to be the correct thing to do

810
00:52:54,240 --> 00:52:58,960
do in order to work out this distribution if you don't know lambda

811
00:53:00,590 --> 00:53:04,090
it won't be obvious right now exactly how you compute all these terms but it's

812
00:53:04,090 --> 00:53:09,830
basically applying bayes rule and applying standard probability calculus manipulation

813
00:53:14,420 --> 00:53:18,100
we have a method which is completely defined if we put a prior distribution over

814
00:53:18,100 --> 00:53:20,080
lambda then

815
00:53:20,120 --> 00:53:22,310
we don't need to set lambda i that we can

816
00:53:22,320 --> 00:53:24,890
just average over everything that we don't know

817
00:53:24,910 --> 00:53:29,440
we consider all possible and is in all possible weights simultaneously

818
00:53:29,450 --> 00:53:34,120
something else we could do is optimized lambda so now

819
00:53:34,130 --> 00:53:38,100
as we approximated integral before by optimizing we could say well

820
00:53:38,120 --> 00:53:40,440
maybe this average is dominated by

821
00:53:40,460 --> 00:53:45,810
the best possible and selects just optimize the posterior probability of lambda given the data

822
00:53:45,890 --> 00:53:50,370
and use that one lambda in order to construct a distribution over way to that

823
00:53:50,370 --> 00:53:51,890
thing is used a lot in

824
00:53:51,930 --> 00:53:56,100
it is roughly called empirical bayes or type two maximum likelihood

825
00:53:56,840 --> 00:54:01,900
this framework gives you ways of setting the three parameters without using validation set which

826
00:54:01,900 --> 00:54:03,140
is nice

827
00:54:03,190 --> 00:54:09,300
i would always use validation set as well just to check it maybe maybe this

828
00:54:09,300 --> 00:54:12,850
gives you always setting lots of different lambda simultaneously and then you can use the

829
00:54:12,850 --> 00:54:20,610
validation set right at the end to check your final performance

830
00:54:24,830 --> 00:54:27,480
the main reason i put this slide up as i can tell you guys what

831
00:54:27,480 --> 00:54:30,360
i actually do so now we've got the bit of the party where i can

832
00:54:30,830 --> 00:54:33,070
tell you when my research center

833
00:54:35,550 --> 00:54:39,530
integrals that i've written up once you've been given a bit of practice

834
00:54:39,550 --> 00:54:41,570
very mechanical easy to do

835
00:54:41,630 --> 00:54:45,780
we write down the model is very obvious what the correct bayesian answer should be

836
00:54:45,870 --> 00:54:48,820
but it turns out that the required integrals

837
00:54:48,820 --> 00:54:50,570
quite formidable

838
00:54:50,580 --> 00:54:56,830
the high dimensional integrals the aren't analytically tractable and very difficult to compute numerically

839
00:54:58,980 --> 00:55:01,840
that isn't a reason necessarily not to use

840
00:55:01,890 --> 00:55:06,380
bayesian method but what it does mean is that you need to approximate the integrals

841
00:55:06,380 --> 00:55:11,840
and because this area is seen as important it's the huge research effort in approximate

842
00:55:11,840 --> 00:55:13,640
inference which basically means

843
00:55:13,690 --> 00:55:18,450
the integrals to this form expectations and the probability distributions

844
00:55:18,460 --> 00:55:19,800
how can i

845
00:55:19,800 --> 00:55:22,350
do something which will work well

846
00:55:22,400 --> 00:55:25,240
so again people try and come up with objective functions

847
00:55:25,240 --> 00:55:29,340
the algorithms can optimize so they variational method say

848
00:55:29,340 --> 00:55:33,640
you can reconstruct on moments of the random variable

849
00:55:33,650 --> 00:55:37,430
and if you recall

850
00:55:37,450 --> 00:55:41,330
you may or may not not not not to you know that this

851
00:55:41,340 --> 00:55:45,510
this thing here this exponential thing is is actually a universal kernels

852
00:55:45,590 --> 00:55:50,910
this experience the exponential kernel we can show this is a so-called conformal modification of

853
00:55:50,940 --> 00:55:52,040
gaussian kernel

854
00:55:52,060 --> 00:55:55,840
which is also universal so it's

855
00:55:55,900 --> 00:55:58,430
if the space is dense in the set of continuous functions

856
00:55:58,440 --> 00:56:02,000
it satisfies the conditions of this result here

857
00:56:02,990 --> 00:56:08,410
everything that i said before holds true for this kind of thing but we can

858
00:56:08,410 --> 00:56:12,590
also plug other universal kernels in here

859
00:56:12,600 --> 00:56:17,000
now before i have shown you several times how to convert

860
00:56:17,010 --> 00:56:23,530
such a difference vectors into quantities like this which suggests that we can also handle

861
00:56:23,530 --> 00:56:26,520
them using the methods of statistical learning theory

862
00:56:26,530 --> 00:56:29,390
indeed if you take here the difference vector between

863
00:56:30,600 --> 00:56:35,070
and the sample from the distribution you can rewrite it like this

864
00:56:35,080 --> 00:56:38,900
you know it's an expectation of function here we have the mean of the same

865
00:56:39,840 --> 00:56:43,900
over some function class this is exactly

866
00:56:43,990 --> 00:56:45,810
kind of a

867
00:56:45,820 --> 00:56:48,630
quantities that are studied by VC theory

868
00:56:48,660 --> 00:56:50,750
PAC theory

869
00:56:50,760 --> 00:56:56,680
and one can combined this kind of quality using uniform convergence methods one it's certain

870
00:56:57,130 --> 00:57:00,860
to be more complicated but if basically for idea

871
00:57:02,100 --> 00:57:04,140
of the unit ball

872
00:57:04,180 --> 00:57:08,480
with a given distribution well behaved in this thing will go to like one of

873
00:57:08,500 --> 00:57:12,170
the square

874
00:57:12,180 --> 00:57:14,260
OK so

875
00:57:14,270 --> 00:57:17,010
it's not not talk about some

876
00:57:17,020 --> 00:57:21,680
of before i talk about applications may be just one word about one more word

877
00:57:21,680 --> 00:57:25,370
about this what does it mean if this thing goes to zero

878
00:57:25,400 --> 00:57:30,570
it's not quite the same as saying we are estimating measure because and it's certainly

879
00:57:30,570 --> 00:57:35,810
not saying that we can estimate measuring the general setting independent of the problem because

880
00:57:35,810 --> 00:57:40,450
that's an impossible problem so what this is one this is just saying is that

881
00:57:40,460 --> 00:57:42,450
if we look at this problem

882
00:57:42,480 --> 00:57:47,870
through the unit ball of RKHS then at some point we can detect differences anymore

883
00:57:48,010 --> 00:57:49,310
so all you people

884
00:57:49,380 --> 00:57:52,910
which we have to choose ourselves we have choose the RKHS

885
00:57:52,930 --> 00:57:57,170
by specifying what kind of differences we are interested in at some point maybe that

886
00:57:57,170 --> 00:57:58,290
uniform will not

887
00:57:58,420 --> 00:58:04,870
contain functions that are sufficiently nonsmooth such as to be able to detect differences between

888
00:58:04,870 --> 00:58:08,270
this anymore

889
00:58:08,300 --> 00:58:11,070
and the thing is a little bit similar to

890
00:58:11,110 --> 00:58:18,050
the public's conception of weak convergence of risks so remember in the BC bones you

891
00:58:18,050 --> 00:58:21,990
don't study convert the convergence of the empirical measure

892
00:58:22,000 --> 00:58:23,840
to the true measure

893
00:58:23,860 --> 00:58:26,360
you start the convergence of risks

894
00:58:26,380 --> 00:58:29,340
so we have to risk which depends on the empirical measure and the to measure

895
00:58:29,420 --> 00:58:33,290
you you prove that this kind of risk is zero but that's a weaker condition

896
00:58:33,940 --> 00:58:37,210
the actual measures conventions

897
00:58:37,350 --> 00:58:42,840
OK so let's look at some applications of this kind of stuff

898
00:58:42,840 --> 00:58:46,430
and probably i don't have time to go through all of them and i want

899
00:58:46,430 --> 00:58:50,750
to collaborate leads to fewer than one of them is the two sample problem and

900
00:58:50,750 --> 00:58:51,570
some of you

901
00:58:51,590 --> 00:58:54,700
you might have seen this because of the great and gave a talk about some

902
00:58:54,700 --> 00:58:59,280
of this NIPS last year but we just spent two minutes a little bit about

903
00:58:59,960 --> 00:59:04,040
so we in this case we assume we have two samples x and y are

904
00:59:04,040 --> 00:59:09,140
drawn from distributions p and q respectively and based on these samples we want to

905
00:59:09,140 --> 00:59:13,780
decide whether p is equal to q or not

906
00:59:13,790 --> 00:59:18,610
so if we were CART this difference vectors we can

907
00:59:18,630 --> 00:59:22,920
working on in terms of kernels an expression is only dependent kernels this these three

908
00:59:23,300 --> 00:59:28,500
so all three expectations which we can write a single expectations over this quantity here

909
00:59:28,600 --> 00:59:32,090
which is by the way also positive definite kernel but also it's the kernel of

910
00:59:32,100 --> 00:59:36,840
the so-called you statistics but don't worry about that is this quantity with which does

911
00:59:36,840 --> 00:59:38,910
this kind of pairwise comparisons

912
00:59:38,980 --> 00:59:44,270
and in terms of this thing we can write all difference

913
00:59:44,710 --> 00:59:50,170
and actually will we call it the square of this difference we can write it

914
00:59:50,610 --> 00:59:51,920
as the

915
00:59:51,940 --> 00:59:53,700
expectation over this

916
00:59:53,710 --> 00:59:54,840
colonel h

917
00:59:54,850 --> 00:59:57,450
and we can give an estimate of

918
00:59:57,540 --> 01:00:02,360
d had sample based estimator which is summing over these ages

919
01:00:02,430 --> 01:00:05,110
and excluding the times y is equal to j

920
01:00:05,120 --> 01:00:09,340
in order to avoid the bias and one can prove that this is an unbiased

921
01:00:09,340 --> 01:00:12,300
estimator of this discrepancy between the two

922
01:00:14,790 --> 01:00:18,230
and one of the nice things if you look at this formula because this is

923
01:00:18,790 --> 01:00:20,690
one actually has to compute to perform

924
01:00:20,690 --> 01:00:26,080
such a two sample test it's just it's trivial to implement this just double for

925
01:00:26,770 --> 01:00:31,330
so we can do is relatively fast and also one can do it

926
01:00:31,330 --> 01:00:34,810
basically for any kind of data where one can define counts so one can also

927
01:00:34,810 --> 01:00:38,420
do it for strings in graphs and other kind of stuff

928
01:00:38,460 --> 01:00:42,830
this thing can be shown

929
01:00:42,840 --> 01:00:46,110
so these empirical estimate it can be shown to converge to the true quantity in

930
01:00:46,110 --> 01:00:49,520
probability with a relatively fast rate

931
01:00:49,520 --> 01:00:52,860
so this is again a uniform convergence time bond

932
01:00:53,960 --> 01:00:56,810
one could use this as the basis for a test

933
01:00:56,810 --> 01:00:58,730
but actually it doesn't work very well

934
01:00:58,750 --> 01:01:07,400
which is because as most of us know uniform convergence bounds often very loose pessimistic

935
01:01:07,460 --> 01:01:11,980
so in practice i have to admit that actually classical statistics works better in this

936
01:01:11,980 --> 01:01:16,480
case if we study the asymptotic distribution of this

937
01:01:16,500 --> 01:01:22,420
quantity here so this discrepancy between the estimated difference in the means and the true

938
01:01:22,420 --> 01:01:28,000
difference in the means then one can study what this converges to

939
01:01:28,020 --> 01:01:31,960
both in the case p non equal to q

940
01:01:31,980 --> 01:01:34,000
and in the case p equal to q

941
01:01:34,000 --> 01:01:36,560
in one case we get it goes in the other case we get something more

942
01:01:38,130 --> 01:01:40,400
and i don't want to go into details on it

943
01:01:40,520 --> 01:01:42,830
but based on this one can

944
01:01:42,860 --> 01:01:44,860
device test

945
01:01:45,150 --> 01:01:49,580
test users another hypothesis that p is equal to q

946
01:01:49,610 --> 01:01:53,360
so just to give you an idea so p is equal to queue then this

947
01:01:53,360 --> 01:01:57,330
thing would be zero that's on the hypothesis if we then observe based on the

948
01:01:58,170 --> 01:02:00,040
we have a relatively large sample

949
01:02:00,040 --> 01:02:03,750
and we noticed that the had is significantly different from zero and we know something

950
01:02:03,750 --> 01:02:08,710
about the distribution of t hat that we can sort of see how unlikely it

951
01:02:09,380 --> 01:02:10,710
but in fact the

952
01:02:10,710 --> 01:02:14,940
null hypothesis was all likely it is that the null hypothesis was true or one

953
01:02:14,940 --> 01:02:19,670
likely it was rejected and building that one contesting

954
01:02:19,670 --> 01:02:22,550
that getting rid of letters at the beginning and at the end of the word

955
01:02:22,560 --> 01:02:24,780
and what is left after

956
01:02:24,800 --> 01:02:30,320
same thing is what i did before i was counting how many times with an

957
01:02:30,320 --> 01:02:34,960
individual letter pianist string i can also count how many times does the substring appear

958
01:02:34,960 --> 01:02:35,810
in history

959
01:02:35,820 --> 01:02:39,340
so i'm going to host i denotes with w

960
01:02:39,350 --> 01:02:45,830
bars subscript you the number of times you is inside w so it's really the

961
01:02:45,830 --> 01:02:50,160
number of left different left bates i can get rid of and still find my

962
01:02:50,160 --> 01:02:54,350
w in my of my you inside the

963
01:02:55,450 --> 01:03:00,610
so i just amplification my notation to go just one little bit for

964
01:03:00,690 --> 01:03:04,280
so if i had the word for substring well i've got another word which is

965
01:03:05,580 --> 01:03:10,240
so subsequences the same substring everything is instead of just getting rid of letters at

966
01:03:10,260 --> 01:03:13,420
the beginning and at the end of the word i can get rid of them

967
01:03:13,420 --> 01:03:15,600
in any position

968
01:03:15,680 --> 01:03:20,010
OK so sub sequence is also used in math just to say that i can

969
01:03:20,010 --> 01:03:23,160
take the first the fourth seventh and the eighth letter

970
01:03:23,200 --> 01:03:26,780
so that makes me have a subsequence instead of substrate

971
01:03:26,870 --> 01:03:31,700
and got little recursive definitions to say that obviously a string is a subsequence of

972
01:03:31,700 --> 01:03:37,710
itself and that if i take off one letter then i obtain a subsequence cycle

973
01:03:37,710 --> 01:03:43,080
repeat recursively and through the third rule which is saying that if x is a

974
01:03:43,080 --> 01:03:47,560
subsequence of y and y is itself subsequences z then x is a subsequence of

975
01:03:50,910 --> 01:03:53,090
so it's a little bit of combinatorics

976
01:03:53,220 --> 01:03:58,180
now why am i talking about combinatorics because they will be involved in the algorithms

977
01:03:58,210 --> 01:04:01,740
OK i need to know how to count to be able to then when i'm

978
01:04:01,740 --> 01:04:05,910
going to want to do things like extracting features which we saw earlier on was

979
01:04:05,910 --> 01:04:11,550
important i would want to count how many times does an individual substring or individual

980
01:04:11,550 --> 01:04:15,800
subsequence appear to be able to do that well i need to be able to

981
01:04:15,820 --> 01:04:20,840
wikileaks how many there are in the inferior so if i'm talking about prefixes so

982
01:04:20,840 --> 01:04:27,140
prefixes all substrings which what comes before is the empty string is really the beginnings

983
01:04:27,140 --> 01:04:31,840
of words the luckily the

984
01:04:31,890 --> 01:04:37,110
luckily the semantics likely the words where it used to define the concept defined the

985
01:04:37,110 --> 01:04:42,410
mathematics of these things are pretty of speaking so they correspond to what we'd like

986
01:04:42,440 --> 01:04:43,220
to do

987
01:04:43,410 --> 01:04:47,820
so how many prefixes string have well the string being of length n can have

988
01:04:47,820 --> 01:04:52,320
n plus one different prefixes y plus one count the MT one and the string

989
01:04:53,230 --> 01:04:56,520
so is a prefix of length zero one of length one of things to up

990
01:04:56,530 --> 01:04:58,670
to length

991
01:04:58,710 --> 01:05:05,550
how we substrings can you have well this time the combinatorics correspondence choosing one individual

992
01:05:05,560 --> 01:05:09,590
starting position one individual finishing position your string

993
01:05:09,600 --> 01:05:12,860
OK i think we'll cross each other so that we know that the mathematics if

994
01:05:12,860 --> 01:05:17,490
it is to give you an multiply by n plus one divided by two if

995
01:05:17,490 --> 01:05:18,710
i'm not wrong

996
01:05:18,720 --> 01:05:23,370
it might be a plus one by plus two divided by two to check and

997
01:05:23,370 --> 01:05:28,140
now it's it's quadratic OK the number of substrings is quadratic so it's not something

998
01:05:28,140 --> 01:05:31,620
to be something we can do if we want to actually go through all the

999
01:05:31,620 --> 01:05:36,100
different substrings of the string to be able to compare with something else that's something

1000
01:05:36,100 --> 01:05:37,800
we can actually do

1001
01:05:38,050 --> 01:05:42,760
now the numbers of sequences of string that's that's not quadratic that's not polynomial as

1002
01:05:42,760 --> 01:05:47,610
exponential because basically to be able to obtain a subsequence from a sequence what you

1003
01:05:47,620 --> 01:05:51,120
do is you just take the first letter and decided what i wanted to and

1004
01:05:51,120 --> 01:05:54,700
do i not wanted to choices look at the second letter do i want to

1005
01:05:54,710 --> 01:05:58,780
do i not want two choices third lead two choices and so on till the

1006
01:05:58,780 --> 01:06:02,790
end of two by two by two by two this means that any algorithm that

1007
01:06:02,790 --> 01:06:04,440
was consistent saying who

1008
01:06:04,440 --> 01:06:06,770
we go for each

1009
01:06:06,780 --> 01:06:09,190
subsequent of w do

1010
01:06:09,200 --> 01:06:11,450
is doomed to failure

1011
01:06:11,470 --> 01:06:14,800
not true if w is not very big that's all right if you're talking about

1012
01:06:14,800 --> 01:06:17,060
bioinformatics then that's wrong

1013
01:06:17,070 --> 01:06:19,480
you clearly enough be able to do anything like that

1014
01:06:19,510 --> 01:06:24,550
OK so you have to think about a different way of doing things

1015
01:06:27,370 --> 01:06:32,540
let's just look at a few of these basic algorithmic problems on strings so the

1016
01:06:32,540 --> 01:06:36,580
first thing is that if i have got two

1017
01:06:37,840 --> 01:06:43,230
looking for the maximum subsequence of the two strings we know how to do it

1018
01:06:43,240 --> 01:06:46,830
i mean there are different ways of doing this that can be various maximum so

1019
01:06:46,830 --> 01:06:52,480
that means the longest subsequence common two strings so there's many reasons to do this

1020
01:06:52,480 --> 01:06:55,020
i mean when you buy a set of strings you may be wanting to take

1021
01:06:55,020 --> 01:07:01,080
two strings the same i'm trying to find a machine is reasonably machine learning reasons

1022
01:07:01,110 --> 01:07:05,710
trying to find some sort of pattern that suits the strings someone trying to find

1023
01:07:05,710 --> 01:07:11,060
something that that corresponds OK so the problem about doing this is that it looks

1024
01:07:11,060 --> 01:07:14,300
nice you can do it and we're going to see how to do it but

1025
01:07:14,990 --> 01:07:18,520
for two it's OK we know how to do it but if given a set

1026
01:07:18,520 --> 01:07:19,560
of strings

1027
01:07:19,590 --> 01:07:23,790
right and i want to find this sort of pattern this representative string for a

1028
01:07:23,790 --> 01:07:27,370
set of strings which could be sort of what is in common with all the

1029
01:07:27,410 --> 01:07:31,640
strings then i'm going to be facing what we're going to call an NP hard

1030
01:07:31,640 --> 01:07:33,330
this term

1031
01:07:33,380 --> 01:07:37,720
it is among so many possible w and we we like to to find out

1032
01:07:37,730 --> 01:07:40,280
that is so that

1033
01:07:40,290 --> 01:07:42,560
this is the is maximized

1034
01:07:42,570 --> 01:07:46,230
so these so so-called editors called maximum maximum margin

1035
01:07:46,280 --> 01:07:53,230
and that's one spacious thing about and the whole maximum margin classifiers

1036
01:07:55,390 --> 01:08:00,500
finally we have a quadratic programming problem so this is the first somehow it is

1037
01:08:00,500 --> 01:08:04,420
the first SVM formulation we

1038
01:08:04,430 --> 01:08:10,030
we try to maximize these usually we like to do minimisation will maximizing something easy

1039
01:08:10,040 --> 01:08:15,330
because equivalent to minimizing the reciprocal so we

1040
01:08:15,420 --> 01:08:21,290
we can minimize the normal w divided by two but school is an increasing function

1041
01:08:21,300 --> 01:08:23,040
so we can remove it

1042
01:08:23,050 --> 01:08:27,770
therefore we get stuck w transport stop divided by two so this is the objective

1043
01:08:27,770 --> 01:08:33,570
function that we want to minimize the subject to certain constraints so that constance will

1044
01:08:33,690 --> 01:08:35,300
now we still hold two

1045
01:08:35,310 --> 01:08:39,410
classify all the training instances

1046
01:08:39,430 --> 01:08:45,990
the x i was circles they should be greater or equal to one where we

1047
01:08:45,990 --> 01:08:50,680
suppose feliciano unless now we don't see greater than zero we secretory CO two plus

1048
01:08:51,440 --> 01:08:52,290
and the

1049
01:08:52,300 --> 01:08:56,380
four triangles we say this or you could minus one but we don't like to

1050
01:08:56,380 --> 01:09:02,000
write two types of quality so we tried to come back together

1051
01:09:02,010 --> 01:09:04,780
so you multiply by apple's site

1052
01:09:04,800 --> 01:09:08,480
so remember now we have less than or equal to minus one but if we

1053
01:09:08,480 --> 01:09:11,720
multiply while both sides there

1054
01:09:11,860 --> 01:09:16,940
this minus one times minus one becomes one and they also have to change the

1055
01:09:16,940 --> 01:09:22,240
the inequality sign so then you get with article to one as well so those

1056
01:09:22,260 --> 01:09:23,810
two inequalities

1057
01:09:23,830 --> 01:09:29,890
they can be combined together as a single fault so we have no training services

1058
01:09:29,890 --> 01:09:35,080
and for each one we call it is correctly classified

1059
01:09:35,130 --> 01:09:39,740
this is a quadratic programming that is the source

1060
01:09:39,750 --> 01:09:41,940
of course it is not good

1061
01:09:41,960 --> 01:09:44,620
because there that may not be linearly separable

1062
01:09:45,380 --> 01:09:51,530
we can always use linear by this is an example that you can never use

1063
01:09:51,660 --> 01:09:55,960
a straight line separating the two classes of training instances

1064
01:09:55,970 --> 01:09:59,540
instead use non-linear kernel

1065
01:09:59,560 --> 01:10:04,240
so we do two things here

1066
01:10:04,260 --> 01:10:05,540
first we

1067
01:10:05,550 --> 01:10:07,320
must allow training areas

1068
01:10:07,340 --> 01:10:09,990
the even if you don't do that

1069
01:10:11,180 --> 01:10:15,350
then this is the so-called invisible optimisation problem

1070
01:10:15,450 --> 01:10:20,580
the idea of a constant optimisation problem is that

1071
01:10:20,590 --> 01:10:26,840
from from the set of candidates which satisfies all constraints that you selected one so

1072
01:10:26,840 --> 01:10:30,700
that the objective function is minimized but now

1073
01:10:32,090 --> 01:10:37,870
there's no w be satisfying all those constraints because you cannot find any straight line

1074
01:10:37,870 --> 01:10:40,760
forty separate all the training instances

1075
01:10:40,780 --> 01:10:47,030
so you you don't have any candidates to select before doing the minimization so this

1076
01:10:47,030 --> 01:10:51,500
is the so-called invisible optimisation problem so you need to a large training area

1077
01:10:51,510 --> 01:10:56,570
so here i no idea about separating all the training instances i just try to

1078
01:10:56,570 --> 01:10:58,690
separate and maybe some of the

1079
01:10:58,790 --> 01:11:05,960
but later we will say that it generally not a good idea to feed all

1080
01:11:05,960 --> 01:11:07,580
the training instances

1081
01:11:09,180 --> 01:11:13,500
with the purpose of allowing training error is not too is not only to make

1082
01:11:13,500 --> 01:11:20,340
this optimisation problem is feasible we also whole actually to get even better better testing

1083
01:11:20,340 --> 01:11:21,680
accuracy so you feel the

1084
01:11:22,110 --> 01:11:26,300
training data that may be the performers is good

1085
01:11:26,310 --> 01:11:30,470
so it was it we're going to do and then another thing is

1086
01:11:30,500 --> 01:11:34,360
instead of directly using certain nonlinear curves

1087
01:11:34,380 --> 01:11:42,190
is very difficult to model nineteen because we didn't many nonlinear curves making high school

1088
01:11:42,200 --> 01:11:48,680
a very smart idea is that in stead of modeling of the nucleus in the

1089
01:11:48,680 --> 01:11:52,980
original space we try to make the data into a higher dimensional

1090
01:11:52,990 --> 01:11:54,240
feature space

1091
01:11:54,240 --> 01:11:58,190
and that has major consequences

1092
01:11:58,200 --> 01:12:01,940
now the values for all five as you can imagine depend of course entirely on

1093
01:12:01,940 --> 01:12:03,620
the string

1094
01:12:03,770 --> 01:12:06,440
if you have a very thick strings

1095
01:12:06,480 --> 01:12:10,380
and it is very stiff it's very hard to bend of will be high

1096
01:12:10,470 --> 01:12:13,980
also if young's modulus of the wire is is very high

1097
01:12:13,990 --> 01:12:17,880
that it is extremely difficult to deform it's often will also be high

1098
01:12:17,940 --> 01:12:21,810
so there is no such thing as one value for all of you for all

1099
01:12:21,810 --> 01:12:23,630
your piano strings they all

1100
01:12:23,670 --> 01:12:26,260
different for all the strings

1101
01:12:26,330 --> 01:12:28,580
but i do want

1102
01:12:28,630 --> 01:12:32,820
to do calculation to give you an order of magnitude idea what effect this would

1103
01:12:33,630 --> 01:12:36,240
on the piano

1104
01:12:36,270 --> 01:12:38,570
so i choose a value for all

1105
01:12:38,580 --> 01:12:40,610
which is not

1106
01:12:40,680 --> 01:12:46,610
entirely absurd although with a higher values of of possible

1107
01:12:46,620 --> 01:12:48,730
and then i want to calculate is you

1108
01:12:48,770 --> 01:12:54,300
qualitatively what difference that would make for a particular string in the piano

1109
01:12:54,340 --> 01:12:57,420
and so the case that i have chosen i think of five

1110
01:12:57,510 --> 01:13:01,230
ten to the minus two

1111
01:13:01,270 --> 01:13:04,870
i think the tension which is very common for piano strings which is two hundred

1112
01:13:04,870 --> 01:13:06,410
and fifty newtons

1113
01:13:06,440 --> 01:13:08,740
i think the length of the piano strings

1114
01:13:08,780 --> 01:13:10,200
which is one meter

1115
01:13:10,230 --> 01:13:12,220
just for simplicity

1116
01:13:12,260 --> 01:13:16,070
and i give it a mass per unit

1117
01:13:17,230 --> 01:13:19,010
of ten grams

1118
01:13:19,080 --> 01:13:20,510
from either

1119
01:13:20,510 --> 01:13:22,970
i thought it would be ten to the minus two

1120
01:13:22,970 --> 01:13:26,100
you know as i units to the minus two kilograms

1121
01:13:26,190 --> 01:13:28,330
the media

1122
01:13:28,340 --> 01:13:31,570
and i want to explore ways you tens harmonic

1123
01:13:31,580 --> 01:13:39,970
of this string

1124
01:13:40,770 --> 01:13:42,380
he divided by u

1125
01:13:42,400 --> 01:13:44,710
by the way this

1126
01:13:44,720 --> 01:13:49,310
can be written and i will write it down again in

1127
01:13:49,400 --> 01:13:51,070
this is also team

1128
01:13:51,110 --> 01:13:53,010
provided by

1129
01:13:53,060 --> 01:13:54,620
k square

1130
01:13:54,630 --> 01:13:56,310
because of

1131
01:13:56,360 --> 01:13:58,270
it could afford

1132
01:13:58,280 --> 01:14:00,210
this is the kind of thing

1133
01:14:00,220 --> 01:14:03,010
but i knew

1134
01:14:03,050 --> 01:14:06,200
so t be divided by new

1135
01:14:06,330 --> 01:14:11,100
it's easy enough that is two point five times ten to the force

1136
01:14:15,200 --> 01:14:17,230
i have to know what

1137
01:14:17,270 --> 01:14:18,560
but k is well

1138
01:14:18,580 --> 01:14:20,260
let that ten

1139
01:14:20,260 --> 01:14:24,830
let the one is two meters

1140
01:14:25,800 --> 01:14:27,310
fixed at both ends

1141
01:14:27,320 --> 01:14:28,560
it's one meter

1142
01:14:28,570 --> 01:14:30,270
the langston lambda one this

1143
01:14:30,270 --> 01:14:33,480
to me it's all about ten is ten times smaller

1144
01:14:33,490 --> 01:14:35,840
so that's two divided by ten

1145
01:14:35,900 --> 01:14:36,840
so OK

1146
01:14:38,560 --> 01:14:42,340
which is of course by definition two pi divided by lambda ten

1147
01:14:42,350 --> 01:14:43,870
is then ten by

1148
01:14:47,280 --> 01:14:50,400
so now i have all the ingredients

1149
01:14:50,460 --> 01:14:51,860
to compare

1150
01:14:51,920 --> 01:14:54,170
this omega square this term

1151
01:14:54,210 --> 01:14:57,820
with that term to see by how much the frequency changes

1152
01:14:58,920 --> 01:15:03,050
the case where we have no dispersion

1153
01:15:03,110 --> 01:15:05,860
so this first now

1154
01:15:06,050 --> 01:15:11,670
you can check that of course for yourself level right down omega squared again

1155
01:15:11,690 --> 01:15:14,460
it is that terms

1156
01:15:14,480 --> 01:15:16,050
this deal you

1157
01:15:16,050 --> 01:15:18,280
and then you have to multiply by

1158
01:15:18,340 --> 01:15:22,690
OK square and then i find two point five times ten to the seventies

1159
01:15:22,780 --> 01:15:27,250
and then i can calculate of k two to four and i have all the

1160
01:15:27,250 --> 01:15:28,800
ingredients on the blackboard

1161
01:15:28,900 --> 01:15:35,320
you find that this term is ten to four

1162
01:15:35,340 --> 01:15:38,780
if i calculate roughly what only guys

1163
01:15:38,840 --> 01:15:42,650
i'm not interested in the exact value of omega you will see shortly y

1164
01:15:42,690 --> 01:15:44,070
you'll find that omega

1165
01:15:44,070 --> 01:15:47,360
is about five thousand radians per second in this case

1166
01:15:47,360 --> 01:15:50,280
which translates into a frequency

1167
01:15:50,380 --> 01:15:54,280
of about eight hundred hertz

1168
01:15:54,280 --> 01:15:58,280
the value itself is not so important but what is important now

1169
01:15:58,280 --> 01:16:00,150
as a result of this

1170
01:16:00,170 --> 01:16:02,130
extract number

1171
01:16:02,190 --> 01:16:06,270
the frequency will go up by point zero two percent

1172
01:16:06,280 --> 01:16:08,960
and this of course you can check immediately for yourself

1173
01:16:09,010 --> 01:16:10,320
which are calculated

1174
01:16:10,320 --> 01:16:13,050
and point o two percent increase

1175
01:16:13,070 --> 01:16:16,610
means in this case one six of words

1176
01:16:17,800 --> 01:16:20,770
but the attends harmonic

1177
01:16:20,820 --> 01:16:25,140
if you expected exactly eight hundred hertz this is of course not exact but i

1178
01:16:25,140 --> 01:16:29,460
think round the number then it will be one six of offers higher because of

1179
01:16:29,460 --> 01:16:31,550
the dispersion relation

1180
01:16:31,550 --> 01:16:34,570
but if you make of ten to the minus one which is by no means

1181
01:16:35,940 --> 01:16:41,400
then it will go up by one point six words

1182
01:16:41,480 --> 01:16:44,840
and so the bottom line hours

1183
01:16:44,840 --> 01:16:53,320
that for real piano string omega and is no longer and times only got one

1184
01:16:53,360 --> 01:16:57,300
and that's the reason that is why pianos go sharks

1185
01:16:57,500 --> 01:16:59,190
as musicians saying

1186
01:16:59,210 --> 01:17:03,690
that means that the higher harmonics the frequencies a little higher

1187
01:17:06,050 --> 01:17:07,460
proportional was

1188
01:17:07,480 --> 01:17:08,750
is an

1189
01:17:08,800 --> 01:17:15,110
and and that's what's called mixtape and go sharp

1190
01:17:15,110 --> 01:17:17,090
professor with allows

1191
01:17:17,130 --> 01:17:20,630
developed several years ago when he was lecturing at o three

1192
01:17:20,730 --> 01:17:22,820
a tory model

1193
01:17:22,820 --> 01:17:25,230
which uses on the computer

1194
01:17:25,270 --> 01:17:27,860
and the tory model has the following

1195
01:17:27,900 --> 01:17:33,320
the dispersion relation it is it has no direct connection with the stream

1196
01:17:33,380 --> 01:17:37,650
no direct connection with any physical thing but it simply use

1197
01:17:37,710 --> 01:17:40,190
for the purpose of demonstration

1198
01:17:40,250 --> 01:17:43,340
and that is the only guest grids

1199
01:17:43,360 --> 01:17:45,460
equals the squared

1200
01:17:45,510 --> 01:17:47,690
times case grid

1201
01:17:47,690 --> 01:17:49,210
times one

1202
01:17:49,230 --> 01:17:51,130
as of

1203
01:17:51,150 --> 01:17:53,690
OK script

1204
01:17:53,710 --> 01:17:57,800
and if you're not careful you may think that it is the same as this

1205
01:17:57,840 --> 01:17:59,250
but it's not

1206
01:17:59,250 --> 01:18:00,670
it's very different

1207
01:18:00,780 --> 01:18:02,150
the difference is

1208
01:18:02,150 --> 01:18:05,300
that this term here has the squaring it

1209
01:18:05,320 --> 01:18:12,780
and this term here does not have this these are totally different dispersion relationship

1210
01:18:12,780 --> 01:18:14,210
in this program

1211
01:18:14,230 --> 01:18:16,460
that you're going to see

1212
01:18:16,510 --> 01:18:18,250
bullock which flows

1213
01:18:18,280 --> 01:18:20,820
shows you six ways

1214
01:18:20,960 --> 01:18:24,340
all six have the same amplitude

1215
01:18:24,380 --> 01:18:29,780
but they differ the shortest wavelengths and the longest wavelength differed by twelve percent

1216
01:18:29,780 --> 01:18:36,460
so to neighbouring wavelengths are two-and-a-half percent part

1217
01:18:38,590 --> 01:18:41,150
the velocities of the individual

1218
01:18:41,150 --> 01:18:45,080
during the week and i think i'm going to talk about now is a graph

1219
01:18:45,080 --> 01:18:48,150
cut and the great thing about graph cuts

1220
01:18:48,190 --> 01:18:52,110
is that compared with all these other methods to the problem that i'm talking about

1221
01:18:52,160 --> 01:18:54,110
is actually exact

1222
01:18:54,150 --> 01:19:00,420
which is quite remarkable i mean normally we find the problem which can be solved

1223
01:19:00,420 --> 01:19:04,610
exactly you know that you've oversimplify the problem is not really any interest that this

1224
01:19:04,610 --> 01:19:08,870
is this is the tremendous canopy some middle ground a problem which people actually care

1225
01:19:08,870 --> 01:19:12,850
about you know you sort computer graphics tool that really does have this really is

1226
01:19:12,850 --> 01:19:17,090
solving this problem using this method the solution in this

1227
01:19:17,140 --> 01:19:20,170
the setting is exact

1228
01:19:20,190 --> 01:19:23,800
one of the reasons that turns out to be exact is because we restrict ourselves

1229
01:19:23,800 --> 01:19:24,900
to a binary problem where

1230
01:19:25,270 --> 01:19:30,970
the x's cannot take the value zero and there are many problems like the one

1231
01:19:30,970 --> 01:19:34,550
way you allow transparency you know what we're talking about right at the beginning you

1232
01:19:34,550 --> 01:19:39,280
allow the pixels be mixtures of foreground and background then immediately you need real numbers

1233
01:19:39,280 --> 01:19:45,280
or perhaps integers and that point this method while still extremely powerful is no longer

1234
01:19:45,280 --> 01:19:49,430
exact but you know something we should just take a few minutes to enjoy the

1235
01:19:49,430 --> 01:19:52,870
fact that we've got a problem people really care about the hard problem

1236
01:19:53,270 --> 01:19:54,340
that has

1237
01:19:54,390 --> 01:19:55,860
inexact algorithm

1238
01:19:55,910 --> 01:19:57,700
OK so here is the

1239
01:19:57,710 --> 01:20:00,930
he is the problem

1240
01:20:03,450 --> 01:20:05,120
this is the graphical model

1241
01:20:05,120 --> 01:20:05,830
for that

1242
01:20:05,840 --> 01:20:08,200
problem is just the same

1243
01:20:08,220 --> 01:20:11,860
graphical model that i wrote down algebraic algebraically earlier

1244
01:20:11,870 --> 01:20:13,670
it simply says that we

1245
01:20:13,690 --> 01:20:17,810
look at individual pixels his an observed excel

1246
01:20:17,810 --> 01:20:19,610
and each individual pixel

1247
01:20:19,660 --> 01:20:22,630
is considered to be generated from a palette

1248
01:20:24,370 --> 01:20:27,970
the state of this very well i the foreground or background talent and the you

1249
01:20:27,970 --> 01:20:33,840
know full explanation of the colour content this excel is simply that we drew at

1250
01:20:33,860 --> 01:20:37,720
random from the palate corresponding to foreground background

1251
01:20:37,780 --> 01:20:42,150
but in the graphical model we also have a link between adjacent pixels and this

1252
01:20:42,150 --> 01:20:46,850
was the link which was initially the easing prior that love is the penalty for

1253
01:20:46,850 --> 01:20:51,210
different labelings of adjacent pixels and then we

1254
01:20:51,220 --> 01:20:54,600
went to the modified using priors

1255
01:20:54,640 --> 01:21:01,140
where the prior was now polluted with data to because this graphical model is still

1256
01:21:01,140 --> 01:21:05,280
perfectly legal the graphical model is still perfectly correct the only thing that you can

1257
01:21:05,280 --> 01:21:10,180
no longer say is that these terms are associated exclusively with the prime

1258
01:21:10,240 --> 01:21:14,480
and that's the energy function we have now the

1259
01:21:14,500 --> 01:21:18,470
the graph cut trick is to take this

1260
01:21:18,480 --> 01:21:20,410
graphical model

1261
01:21:20,410 --> 01:21:25,640
and and we're intending to solve for m a p estimation which i guess i

1262
01:21:25,640 --> 01:21:29,200
didn't explicitly say there are many things you can do with posteriors but the thing

1263
01:21:29,200 --> 01:21:33,810
that we're going to do is maximized to find the most probable

1264
01:21:33,830 --> 01:21:35,480
posterior labeling

1265
01:21:35,580 --> 01:21:39,890
and this optimisation can be done by taking all the elements of this graphical model

1266
01:21:40,320 --> 01:21:42,610
and laying it out on the new graph

1267
01:21:42,670 --> 01:21:46,530
this is a different graph now which is not a probabilistic graphical model in the

1268
01:21:46,530 --> 01:21:51,060
same sense this is now the a network

1269
01:21:53,170 --> 01:21:57,940
which are going to think about actually in terms of network flow

1270
01:21:58,000 --> 01:22:02,270
and let's see where all the terms in the original graphical model MAP two in

1271
01:22:02,270 --> 01:22:05,750
this network so the network as the source and the sink so you can think

1272
01:22:05,750 --> 01:22:06,510
of the

1273
01:22:06,590 --> 01:22:12,750
of oil being pumped into the network of pipelines somewhere in siberia and

1274
01:22:12,810 --> 01:22:14,110
coming up

1275
01:22:14,510 --> 01:22:19,400
in next to tilbury docks or something up on the ten and to be distributed

1276
01:22:19,400 --> 01:22:21,170
around the country

1277
01:22:22,370 --> 01:22:24,380
you'd like to know actually

1278
01:22:25,080 --> 01:22:27,760
could you best route the oil for this

1279
01:22:27,780 --> 01:22:30,560
network and you also like to know how much oil can we get through the

1280
01:22:30,560 --> 01:22:32,550
network given that all of the

1281
01:22:32,550 --> 01:22:37,980
pipelines in this network of pipelines all the arcs of the graph have a capacity

1282
01:22:39,100 --> 01:22:43,680
the push oil as hard as you can to this network until you can have

1283
01:22:43,690 --> 01:22:46,840
saturating links everywhere and you just can't get any more

1284
01:22:47,670 --> 01:22:52,850
through this slightly artificial i mean it's not quite oil and pipelines it because you

1285
01:22:52,850 --> 01:22:56,730
know pipelines don't actually have a hard limit on how much oil can flow through

1286
01:22:56,820 --> 01:23:01,930
they have them more like resistors they have all the limits on the relationship between

1287
01:23:01,930 --> 01:23:06,550
pressure and flow but we're going to think of slightly weird kind pipeline where

1288
01:23:06,550 --> 01:23:10,850
there's a capacity limit and it turns out that all of the

1289
01:23:12,800 --> 01:23:14,620
all of the

1290
01:23:14,670 --> 01:23:16,510
numbers from the

1291
01:23:17,950 --> 01:23:23,890
graphical model get transferred into various locations on this pipeline so let's see

1292
01:23:26,520 --> 01:23:27,850
for example

1293
01:23:28,450 --> 01:23:33,940
fk the likelihood associated with this pixel has two values of k of zero in

1294
01:23:34,270 --> 01:23:40,900
one depending whether x is zero one and it turns out that this pixel is

1295
01:23:40,900 --> 01:23:43,670
associated let's say with this node in the pipeline

1296
01:23:43,710 --> 01:23:45,770
so actually there are as many

1297
01:23:45,780 --> 01:23:48,200
nodes in the network

1298
01:23:48,210 --> 01:23:52,020
pipes here as there are pixels in the image one of these for every pixel

1299
01:23:52,030 --> 01:23:52,990
in the image

1300
01:23:53,850 --> 01:23:56,090
this are here from the source

1301
01:23:56,110 --> 01:23:57,370
to the pixel

1302
01:23:57,380 --> 01:24:00,080
carries as its capacity limit

1303
01:24:00,130 --> 01:24:06,350
the likelihood value associated with background the likelihood of the background labels and similarly the

1304
01:24:06,350 --> 01:24:10,610
like of the foreground labeling goes on this

1305
01:24:10,670 --> 01:24:13,980
so that accounts for all of these vertical arcs here in all these the block

1306
01:24:14,950 --> 01:24:17,960
now the horizontal arcs get the

1307
01:24:17,970 --> 01:24:19,670
get the

1308
01:24:19,680 --> 01:24:21,700
using penalties

1309
01:24:22,330 --> 01:24:24,260
this article here

1310
01:24:24,720 --> 01:24:30,630
gets the penalty for this pixel being labeled differently from this pixels

1311
01:24:30,640 --> 01:24:31,810
and now

1312
01:24:32,530 --> 01:24:35,860
and we introduce the idea of the cut

1313
01:24:35,940 --> 01:24:40,770
so now i cut is a line in the network which completely separates the source

1314
01:24:40,780 --> 01:24:42,610
from the sea

1315
01:24:42,670 --> 01:24:43,710
and so

1316
01:24:43,720 --> 01:24:49,370
if i travel along the car i can register every time i cross

1317
01:24:50,570 --> 01:24:54,690
and every time i cross and i look up its value be FK of zero

1318
01:24:54,690 --> 01:24:57,430
and fk one or g value

1319
01:24:57,490 --> 01:25:01,940
and just sum it up so that the value of this cut is the total

1320
01:25:01,940 --> 01:25:06,050
of all of the values of the arcs that were crossed in separating the source

1321
01:25:06,050 --> 01:25:08,080
completely from the sea

1322
01:25:08,120 --> 01:25:09,340
so actually

1323
01:25:10,300 --> 01:25:13,430
there is a one-to-one correspondence now between

1324
01:25:13,450 --> 01:25:15,120
the value of the cut

1325
01:25:16,630 --> 01:25:20,680
the log posterior probability because when you

1326
01:25:20,740 --> 01:25:25,080
evaluate evaluated cut you're just summing up all of the energy terms in this model

1327
01:25:26,260 --> 01:25:28,510
visiting every term once

1328
01:25:28,520 --> 01:25:34,850
exactly and summing up its its energy so actually summing up all of these all

1329
01:25:34,850 --> 01:25:39,300
of these couplings is exactly the same as evaluating

1330
01:25:39,900 --> 01:25:44,670
posterior probability in this graph for particular configuration now which configuration is it

1331
01:25:44,720 --> 01:25:46,180
well let's see

1332
01:25:46,190 --> 01:25:47,930
when i cut this are

1333
01:25:47,930 --> 01:25:51,600
there should be great article one and all others

1334
01:25:51,600 --> 01:25:54,120
it should be less than or equal one

1335
01:25:54,180 --> 01:25:56,890
before because this there is really no

1336
01:25:56,930 --> 01:26:02,820
it's simply not interested in the decision function is you only to be greater than

1337
01:26:02,820 --> 01:26:08,390
or equal to minus one or a simple decision boundaries is what is going to

1338
01:26:08,390 --> 01:26:13,970
be the largest event that's why we use this as makes him to class prediction

1339
01:26:14,030 --> 01:26:15,780
so this is very simple

1340
01:26:16,530 --> 01:26:20,240
so this is for one against the rest

1341
01:26:20,260 --> 01:26:22,870
so from that we know it it's only

1342
01:26:22,890 --> 01:26:26,220
so this one is positive and all the rest

1343
01:26:27,390 --> 01:26:28,530
i think

1344
01:26:28,800 --> 01:26:30,740
well there's no

1345
01:26:33,370 --> 01:26:35,450
but one against one

1346
01:26:36,660 --> 01:26:38,340
well so far then we know

1347
01:26:38,350 --> 01:26:39,410
now we have

1348
01:26:39,410 --> 01:26:45,840
let's train occasions two binary years and each one involves all the four classes of

1349
01:26:45,840 --> 01:26:48,780
training data for one use

1350
01:26:48,990 --> 01:26:55,220
class one school the signal using his life history so one patent free and you

1351
01:26:55,220 --> 01:26:59,820
can find one so we have occasions two two classes

1352
01:26:59,870 --> 01:27:01,620
so i can give

1353
01:27:01,640 --> 01:27:05,200
we give simple here this is the case for

1354
01:27:05,240 --> 01:27:12,930
so four classes support useful means we must intrinsic six

1355
01:27:13,320 --> 01:27:18,530
so the first one is one of those who so for this one because what

1356
01:27:18,530 --> 01:27:21,370
is considered positive crystal

1357
01:27:21,390 --> 01:27:23,200
consider negative

1358
01:27:23,220 --> 01:27:27,780
so we have this is about a single one of the same year we get

1359
01:27:27,780 --> 01:27:31,220
six decision functions

1360
01:27:31,240 --> 01:27:33,340
well the next question is

1361
01:27:33,390 --> 01:27:35,180
how do prediction

1362
01:27:35,260 --> 01:27:42,070
this is also simple so there's just trying to see what is so plainly

1363
01:27:42,100 --> 01:27:43,870
for his

1364
01:27:43,910 --> 01:27:48,530
we just put it into laws policies these functions

1365
01:27:48,550 --> 01:27:53,390
the first one the first one is one of the reasons for it so only

1366
01:27:53,470 --> 01:28:00,640
thing that is because one four four three elastic something in the system so this

1367
01:28:00,640 --> 01:28:06,930
here is is this should be in this one little one she it was one

1368
01:28:06,950 --> 01:28:10,600
in all wonderful is this one and the phrases

1369
01:28:10,620 --> 01:28:13,120
problem then so

1370
01:28:13,340 --> 01:28:14,160
link is

1371
01:28:14,300 --> 01:28:18,530
we have a table showing the number of balls per class

1372
01:28:18,600 --> 01:28:21,680
so this is one of the tree

1373
01:28:21,700 --> 01:28:27,080
the story for you only get one vote so this one of one should be

1374
01:28:30,990 --> 01:28:32,760
this is a reasonable way to do

1375
01:28:32,780 --> 01:28:36,930
prediction so the idea behind the opposing is

1376
01:28:36,950 --> 01:28:38,870
if you have a nice really

1377
01:28:38,890 --> 01:28:44,840
not clear what they for whatever what street whatever it is full

1378
01:28:45,600 --> 01:28:47,490
should say this one

1379
01:28:49,570 --> 01:28:56,580
in a situation in perfect situation you should get minus one false

1380
01:28:56,600 --> 01:28:59,280
from your real class label

1381
01:28:59,350 --> 01:29:04,450
you should take a minus one but but you also believe is a nice place

1382
01:29:04,640 --> 01:29:09,050
you also within other classifiers that a street or performer

1383
01:29:09,100 --> 01:29:14,260
this is for the the idea is that in this situation the addition these events

1384
01:29:14,260 --> 01:29:15,220
is random

1385
01:29:15,220 --> 01:29:21,240
so the other places can attend morning k minus levels so that everyone else is

1386
01:29:21,240 --> 01:29:22,180
doing is

1387
01:29:22,470 --> 01:29:24,870
so reason for using

1388
01:29:24,890 --> 01:29:27,010
it's kind of the exchange

1389
01:29:27,320 --> 01:29:33,240
you see that this is not very reliable but they it works quite well

1390
01:29:33,490 --> 01:29:35,300
in some situations you may have

1391
01:29:35,370 --> 01:29:39,220
in some cases they have the same number of votes

1392
01:29:39,350 --> 01:29:42,140
we have a situation where the tree

1393
01:29:42,140 --> 01:29:43,280
each or

1394
01:29:43,280 --> 01:29:45,950
or you may do so for analysis

1395
01:29:46,510 --> 01:29:51,580
but this is the simplest way to radiation there also ways of using decision that

1396
01:29:52,010 --> 01:29:58,600
this is so i sixty series and i was always trying to

1397
01:29:58,620 --> 01:30:03,640
combat in order to use that to find out the win

1398
01:30:03,660 --> 01:30:07,300
ten really is

1399
01:30:07,300 --> 01:30:15,120
something that

1400
01:30:18,620 --> 01:30:28,340
this is a good question because of these or minus one

1401
01:30:28,340 --> 01:30:29,850
because of his

1402
01:30:32,030 --> 01:30:37,930
he he's not the initial conditions it should be always be less than or equal

1403
01:30:37,930 --> 01:30:39,030
to minus one

1404
01:30:39,080 --> 01:30:41,700
in in his relationship

1405
01:30:41,700 --> 01:30:47,470
well we could process one so he was still work because something could recall two

1406
01:30:47,470 --> 01:30:54,430
possible should be always clear all others there is article minus one so this is

1407
01:30:54,430 --> 01:30:56,680
a normalisation is usually not

1408
01:30:59,430 --> 01:31:02,640
i think i have ever done

1409
01:31:02,640 --> 01:31:05,740
decision by using one against the rest

1410
01:31:05,870 --> 01:31:13,760
these days it is useful for look some basic situation to see if really want

1411
01:31:14,420 --> 01:31:20,260
to have some examples this one this list does it really work

1412
01:31:28,140 --> 01:31:30,160
that's the beach

1413
01:31:30,160 --> 01:31:31,070
of the these

1414
01:31:31,070 --> 01:31:33,970
that's the change

1415
01:31:35,600 --> 01:31:45,320
it is in many practical applications this kind this is you is going to be

1416
01:31:45,320 --> 01:31:47,140
very embarrassed

1417
01:31:47,140 --> 01:31:49,510
you always have

1418
01:31:50,410 --> 01:31:53,320
there are a lot of difficulty

1419
01:31:53,320 --> 01:31:58,030
and you find first which are the the eigenvalues and then you

1420
01:31:58,040 --> 01:32:02,010
replace eigenvalue in here to find would which is the corresponding

1421
01:32:02,020 --> 01:32:09,480
eigenvector so when did the metrics a of a again we're talking

1422
01:32:09,490 --> 01:32:13,200
about square matrices this much is a generalization is for square

1423
01:32:13,210 --> 01:32:17,430
matrices when the metrics phase real-valued and symmetric

1424
01:32:17,970 --> 01:32:23,610
we know that our eigenvalues are also real value

1425
01:32:24,310 --> 01:32:29,870
and we know also that we can find a basis of eigenvector

1426
01:32:30,580 --> 01:32:34,930
which are all or dead which are or to allow one which other

1427
01:32:36,610 --> 01:32:39,810
we can force them to have norm one which is

1428
01:32:40,200 --> 01:32:45,480
an interesting seeing that you in fact can you know that you can

1429
01:32:45,490 --> 01:32:51,510
change you have a change of basis which will give you

1430
01:32:51,880 --> 01:32:56,390
a only being that a data metrics we is

1431
01:32:57,930 --> 01:33:01,030
eigen values on on that they know

1432
01:33:03,130 --> 01:33:06,830
and yeah so a can be rewritten like that

1433
01:33:08,210 --> 01:33:14,740
which which it has really interesting properties since

1434
01:33:15,190 --> 01:33:17,540
it favor right a

1435
01:33:20,700 --> 01:33:26,560
let's call it g this day and matrix t and i want to raise it to

1436
01:33:26,570 --> 01:33:32,250
poer for example what does that mean let's say to to be to begin

1437
01:33:32,260 --> 01:33:39,260
with that would be so two times this metrics

1438
01:33:46,190 --> 01:33:54,580
and because p it's a it's constructed with the eigenvectors

1439
01:33:54,780 --> 01:33:59,510
which are all or to the nile between them and with base one these

1440
01:33:59,520 --> 01:34:04,630
in fact end up to be identity and thus

1441
01:34:05,310 --> 01:34:16,420
this is only p v square the key so for and that yeah of course to

1442
01:34:16,430 --> 01:34:19,760
n because you which is just a and then the same seem

1443
01:34:20,140 --> 01:34:28,140
and so it's really easy to compute the the poer of metrics that

1444
01:34:28,150 --> 01:34:33,330
you have that de lies because the diagonal

1445
01:34:34,020 --> 01:34:39,350
to the poor our to with yeah the core n is only take the

1446
01:34:40,170 --> 01:34:43,140
diagonal elements the raise it to the poor

1447
01:34:48,840 --> 01:34:54,100
another scene is another way of seeing that also which is interesting

1448
01:34:54,670 --> 01:35:00,550
is that you you can fact look at the

1449
01:35:02,200 --> 01:35:06,280
the transform the yeah transformation of vector

1450
01:35:06,690 --> 01:35:15,240
x as in fact also something like that which in fact when you

1451
01:35:15,250 --> 01:35:18,160
give you something like he's

1452
01:35:30,660 --> 01:35:34,660
so at as that's what's called spectral decomposition

1453
01:35:36,980 --> 01:35:40,140
in which in fact you will have

1454
01:35:46,230 --> 01:35:51,710
which heat you will have so as you look at this

1455
01:35:52,590 --> 01:35:57,360
in our transformation in terms of decomposition in the eigenvectors

1456
01:35:58,210 --> 01:36:04,010
and how to give you intuition of that let's say

1457
01:36:05,470 --> 01:36:08,590
i had a really easy a metrics which is

1458
01:36:08,980 --> 01:36:21,040
this one ok so that we give me as eigenvector

1459
01:36:26,540 --> 01:36:31,090
yeah which are the normal in

1460
01:36:34,030 --> 01:36:38,350
the normal based noise due to the user base we take in which we

1461
01:36:38,430 --> 01:36:43,130
we decompose yeah ok then the user base both it

1462
01:36:45,360 --> 01:36:50,050
let's say i have the one one one vector

1463
01:36:56,260 --> 01:36:59,250
which is this transformation with this information

1464
01:36:59,250 --> 01:37:02,870
so i like this representation because it's very easy and very common to compute

1465
01:37:04,010 --> 01:37:06,050
in data point distances yeah

1466
01:37:06,460 --> 01:37:09,880
to do care nearest neighbors or whatever else and in fact i'm gonna base a

1467
01:37:09,880 --> 01:37:12,520
lot of material on computing distances like this

1468
01:37:15,080 --> 01:37:19,400
the dimension normalized between points are drawn from a gamma am and the means to

1469
01:37:19,400 --> 01:37:23,670
sigma square now the interesting thing about this the variance of this distribution is eight

1470
01:37:23,670 --> 01:37:25,300
sigma squared over p e

1471
01:37:25,940 --> 01:37:27,220
so i speedy goes high

1472
01:37:28,230 --> 01:37:29,640
this variance goes to zero

1473
01:37:32,010 --> 01:37:33,670
so only relative if you look at the

1474
01:37:34,170 --> 01:37:37,840
i mean divided by the square root the variance you've got this scaling up even

1475
01:37:37,880 --> 01:37:42,120
as you go to very high dimensions what you'll see is that all the interpoint

1476
01:37:43,630 --> 01:37:44,570
a good dataset

1477
01:37:46,540 --> 01:37:47,540
the same distance apart

1478
01:37:49,510 --> 01:37:54,130
in this calcium egg so any sample you take will be equidistant from every other sample

1479
01:37:55,620 --> 01:37:56,300
which is a

1480
01:37:56,340 --> 01:38:01,320
annoying characteristic if you're trying to do things like came nearest neighbors because as you change kay

1481
01:38:02,760 --> 01:38:06,080
find no well if you if you look at the volume and david you're considering

1482
01:38:07,150 --> 01:38:08,280
as you increase the

1483
01:38:08,950 --> 01:38:11,420
minister there may be some go from known labels to all

1484
01:38:11,920 --> 01:38:17,220
your points neighbours this basically says that for this density every data point is a neighborhood

1485
01:38:17,540 --> 01:38:19,040
over each other in high dimensions

1486
01:38:21,730 --> 01:38:22,240
okay so

1487
01:38:23,840 --> 01:38:27,360
now seems to be a bad thing about our sensation use gaussians you might think

1488
01:38:27,560 --> 01:38:31,270
but actually for any data where you've got independence over features

1489
01:38:31,750 --> 01:38:35,510
the central limit theorem applies to these farms were taking an

1490
01:38:36,030 --> 01:38:41,350
the same results come out they just don't come out quite nicely and analytically in the low sample area

1491
01:38:42,660 --> 01:38:48,160
you get exactly the same thing going on the variance about the mean scales as period one over so

1492
01:38:50,200 --> 01:38:52,350
distribution you sample from independently

1493
01:38:53,240 --> 01:38:55,000
will eat these different effect if you

1494
01:38:55,500 --> 01:38:57,840
one and how it differs well the guassian

1495
01:38:58,630 --> 01:38:59,370
in this video

1496
01:38:59,810 --> 01:39:01,510
it's like if you're one of the things that

1497
01:39:02,370 --> 01:39:07,290
one of the reasons i believe we're here is because there's large telescope here and then i'd like large telescopes

1498
01:39:08,910 --> 01:39:12,820
so the use of analogies as well if you're standing on the

1499
01:39:13,240 --> 01:39:14,600
looking out this hypersphere

1500
01:39:15,340 --> 01:39:20,290
okay so it's high dimensional space of these galcians samples how would look well all

1501
01:39:20,290 --> 01:39:24,260
the samples will be uniformly distributed across space is a bit like the stars they're

1502
01:39:24,260 --> 01:39:28,220
not quite uniformly distributed yet but if you're standing at the center this hypersphere

1503
01:39:28,660 --> 01:39:31,370
so it's a three-dimensional sphere it but it's a hypersphere

1504
01:39:32,690 --> 01:39:37,890
in general looking out these data points like stars distribution over space and beautiful

1505
01:39:39,510 --> 01:39:43,600
if you've nongaussian densities subgaussian or supergaussian

1506
01:39:44,170 --> 01:39:46,340
in the hypersphere the point or cluster

1507
01:39:46,790 --> 01:39:51,390
i e there alongside the axes are forty five degrees axes depending

1508
01:39:52,070 --> 01:39:58,150
on with view super subgaussian so these will be uniformly distributed over this hypersphere anymore

1509
01:39:58,440 --> 01:40:01,420
it would be quite pretty but the same problems will exist

1510
01:40:05,320 --> 01:40:08,390
this is behave very counterintuitively in high dimensions

1511
01:40:10,280 --> 01:40:14,310
we can compute the density squared distances analytically for the gas in case and for

1512
01:40:14,320 --> 01:40:18,440
nongaussian independent systems we can invoke the central limit theorem so let's see how these

1513
01:40:18,440 --> 01:40:22,140
apply for some real data that's what we're gonna do now is we see that

1514
01:40:22,230 --> 01:40:26,940
this problem and we can compute the theoretical interpoint distance between two

1515
01:40:27,870 --> 01:40:30,760
data and it's valid because the central limit theorem

1516
01:40:31,530 --> 01:40:33,760
dataset if the features are independent

1517
01:40:34,420 --> 01:40:39,630
so here's some example datasets well that's just test our intuition so this is sampled

1518
01:40:39,630 --> 01:40:44,670
from a thousand dimensional gaussians and what i'm showing is the distribution of squared distances

1519
01:40:44,670 --> 01:40:45,860
between each data point

1520
01:40:47,410 --> 01:40:51,760
and what i'm showing you there is a histogram and then bang directly is the

1521
01:40:51,870 --> 01:40:54,220
bit about gamma distribution we derived before

1522
01:40:54,870 --> 01:40:57,140
look-alikes leave the theory fits the data

1523
01:40:58,920 --> 01:41:00,240
um that's good

1524
01:41:01,230 --> 01:41:06,640
we can sample from the calcium great dataset thousand samples thousand data points so around

1525
01:41:06,640 --> 01:41:09,070
a million interpoint distances in this histogram

1526
01:41:11,130 --> 01:41:14,820
okay now i use a few examples here and if it's still going this ten

1527
01:41:14,820 --> 01:41:17,780
thousand interpoint distances if it's still excellent

1528
01:41:18,710 --> 01:41:21,590
okay so let's take at and that's applied to real data

1529
01:41:22,120 --> 01:41:24,440
so that we can get on its paper so we have to have a little

1530
01:41:24,630 --> 01:41:26,570
artificial example then a real data example

1531
01:41:27,130 --> 01:41:30,440
okay the real data here is a little bit artificial as well this is a famous

1532
01:41:31,110 --> 01:41:34,050
dataset which is the so-called oil data

1533
01:41:34,450 --> 01:41:34,980
it some

1534
01:41:35,560 --> 01:41:37,030
some oil flowing in a pipeline

1535
01:41:38,070 --> 01:41:43,950
and dates from bishop and james this ninety three papers is widely used as a benchmark and dimensionality reduction

1536
01:41:45,330 --> 01:41:46,310
it's all flowing

1537
01:41:46,830 --> 01:41:49,800
from well so it's got oil gas and water it

1538
01:41:51,390 --> 01:41:53,320
and they according to the speed

1539
01:41:55,230 --> 01:41:56,770
i guess reynolds number and stuff like that

1540
01:41:57,230 --> 01:42:02,580
mix in different ways in turbulent flow you basically get this homogeneous everything mixed together

1541
01:42:03,860 --> 01:42:07,300
but then there's two types and laminar flow co one is gas oil water

1542
01:42:07,710 --> 01:42:10,750
and many other one i think is gas oil water i hope i've got the

1543
01:42:10,820 --> 01:42:13,460
right way around it might be gas not must be guess who knows

1544
01:42:15,050 --> 01:42:15,990
like so that

1545
01:42:17,620 --> 01:42:24,530
now you've got twelve measuring movements which gamma ray densitometry measurements so these are measuring the density across here

1546
01:42:25,750 --> 01:42:30,010
this is the classics and the learning problem because although clearly if you see the

1547
01:42:30,010 --> 01:42:33,730
density on each these you should be able to determine which domain urine

1548
01:42:34,120 --> 01:42:38,870
the physics and the thing is a little bit complicated so in fact it turns

1549
01:42:38,870 --> 01:42:43,070
out this data simulated if you read carefully the original paper and it stimulated by

1550
01:42:43,070 --> 01:42:47,300
understanding the physics of these systems and what these things would really so little a

1551
01:42:47,300 --> 01:42:47,790
warning there

1552
01:42:48,310 --> 01:42:51,990
it's simulated data but it's from a realistic physical system

1553
01:42:54,380 --> 01:42:58,340
what you can see here is you've got i think twelve senses in total

1554
01:42:58,740 --> 01:43:00,300
are measuring the density

1555
01:43:01,210 --> 01:43:02,510
and the section these pipes

1556
01:43:03,510 --> 01:43:04,600
so that's well readings

1557
01:43:06,690 --> 01:43:07,550
if we now compare

1558
01:43:07,950 --> 01:43:08,400
the truth

1559
01:43:08,960 --> 01:43:13,320
to what we theoretically expect a happen we see that this is the theoretical curve

1560
01:43:13,320 --> 01:43:17,350
for thee into data point distances and this is the actual histograms

1561
01:43:18,570 --> 01:43:20,010
case that's not too bad is a little bit

1562
01:43:20,130 --> 01:43:21,540
mismatch twelve

1563
01:43:23,850 --> 01:43:29,140
if the predicted variance if these interpoint distances is point six six

1564
01:43:29,640 --> 01:43:32,400
and actual variance we've observed is is closer to

1565
01:43:32,400 --> 01:43:33,640
so this is a quick nap

1566
01:43:34,130 --> 01:43:35,990
have some other aspects of the course

1567
01:43:37,740 --> 01:43:39,890
will have weekly lectures it's lecturer

1568
01:43:40,290 --> 01:43:41,320
on intensive

1569
01:43:41,800 --> 01:43:43,510
in total is only thirteen lectures

1570
01:43:44,240 --> 01:43:46,250
will meet once a week here for lecturer

1571
01:43:47,650 --> 01:43:48,450
there's readings

1572
01:43:49,190 --> 01:43:54,470
there is voluminous readings there's readings about every topic we'll talk talk about and the

1573
01:43:54,470 --> 01:43:59,990
readings were specifically designed for this course i highly recommend that you become familiar with

1574
01:43:59,990 --> 01:44:05,240
the readings if you have a question after lecture it's probably there is probably explained

1575
01:44:07,900 --> 01:44:10,180
we'll do online tudor problems

1576
01:44:10,720 --> 01:44:14,230
we send you an email if you've pre-registered for the course so you may already

1577
01:44:14,230 --> 01:44:15,910
know about this the idea is going to be

1578
01:44:16,350 --> 01:44:16,720
that is

1579
01:44:18,120 --> 01:44:21,410
ways that you can prepare for the course by doing computer exercises

1580
01:44:22,860 --> 01:44:27,320
and we will also use those same kinds of exercises in all the class sections

1581
01:44:28,920 --> 01:44:31,840
we will have to kinds of lab experiences besides lecturer

1582
01:44:32,430 --> 01:44:38,610
the other two events that have that you have to attend are softer lab and a design pattern

1583
01:44:39,130 --> 01:44:40,370
that's the practice part

1584
01:44:41,840 --> 01:44:44,410
so i have to learn a little bit about the theory by going to a lecture by

1585
01:44:44,830 --> 01:44:48,130
by doing the readings and then you go to the lab and try some things out

1586
01:44:49,830 --> 01:44:51,690
we call the first lab software lab

1587
01:44:52,600 --> 01:44:58,050
it's a short lab it's an hour and few work individually try things out you write little programs that you

1588
01:45:00,090 --> 01:45:02,400
coursework and check the program this year if it's okay

1589
01:45:04,900 --> 01:45:09,630
and it primarily the exercises in the software are due

1590
01:45:10,120 --> 01:45:12,460
during the software lab that on occasion will be

1591
01:45:12,910 --> 01:45:13,840
extra things do

1592
01:45:14,410 --> 01:45:15,190
i dare to later

1593
01:45:16,290 --> 01:45:18,880
the due dates are very clearly written in the tudor exercises

1594
01:45:20,270 --> 01:45:24,070
once a week as a designer that's a three hour session in which you work with a partner

1595
01:45:25,570 --> 01:45:26,650
the reason for the partner

1596
01:45:27,100 --> 01:45:27,760
is that he

1597
01:45:28,370 --> 01:45:33,510
intent the difference between the design labs and the softer land is that the design labs

1598
01:45:34,690 --> 01:45:37,440
ask yourself slightly more open ended questions

1599
01:45:39,520 --> 01:45:41,080
the kind of question that you might

1600
01:45:41,600 --> 01:45:42,810
i have no clue what we're asking

1601
01:45:44,310 --> 01:45:47,040
open and that the kind of thing that you will be asked to do after

1602
01:45:47,210 --> 01:45:50,150
graduating designed the system wouldn't system

1603
01:45:52,860 --> 01:45:56,680
so the idea is not working with a partner will give u

1604
01:45:57,360 --> 01:46:00,020
i second immediate source of help

1605
01:46:01,110 --> 01:46:04,770
a little more confident if neither knows the solution so that you raise your hand

1606
01:46:04,770 --> 01:46:06,390
and say i don't have a clue what's going on here

1607
01:46:07,160 --> 01:46:11,440
so the idea is that's one so we we do softer lab individually what we

1608
01:46:11,480 --> 01:46:14,500
we do a design lab a little more open ended with partners

1609
01:46:15,660 --> 01:46:16,910
there's a little bit every homework

1610
01:46:17,750 --> 01:46:18,310
four total

1611
01:46:18,680 --> 01:46:21,910
not much compared other subjects mostly practice

1612
01:46:23,940 --> 01:46:25,140
there's an no quiz

1613
01:46:26,500 --> 01:46:28,730
just a help you keep pace to make sure that you don't

1614
01:46:29,200 --> 01:46:30,650
get too far behind

1615
01:46:31,110 --> 01:46:35,020
the first fifteen minutes every design lab starts with an end up with is the

1616
01:46:35,020 --> 01:46:37,410
native quasars are intended to be simple

1617
01:46:39,380 --> 01:46:40,130
if you've caught up

1618
01:46:40,690 --> 01:46:41,570
if you are up-to-date

1619
01:46:42,320 --> 01:46:44,470
so the idea is then you go to

1620
01:46:44,980 --> 01:46:47,570
design lab the first thing you do is a little fifteen minutes

1621
01:46:47,840 --> 01:46:53,250
then queries in a quiz uses a tudor much like the homework to do much like the pipeline tudor

1622
01:46:54,650 --> 01:46:56,400
and it's intended to be simple

1623
01:46:57,940 --> 01:46:58,720
but it doesn't mean

1624
01:46:59,680 --> 01:47:01,910
please get a designer one time

1625
01:47:03,150 --> 01:47:06,260
inadequacies are administered by the softwar

1626
01:47:07,010 --> 01:47:11,720
it starts when you hour when the design-led starts it times out fifteen minutes later

1627
01:47:12,630 --> 01:47:16,470
so if you come ten minutes late you'll have five minutes to do something that

1628
01:47:16,470 --> 01:47:18,040
we plan to give me fifteen minutes form

1629
01:47:21,800 --> 01:47:24,950
we will also have exams and interviews interviews are

1630
01:47:25,370 --> 01:47:27,110
intended to be um

1631
01:47:27,750 --> 01:47:30,090
a one-on-one conversation about how the labs went

1632
01:47:33,040 --> 01:47:34,960
and we'll have to midterms and finals

1633
01:47:36,880 --> 01:47:42,820
so that's kinda the logistics and the idea behind the logistics is practice theory practice

1634
01:47:43,360 --> 01:47:47,760
come to the labs try things out make sure you understand develop a little code

1635
01:47:47,760 --> 01:47:49,100
type it and see if it works

1636
01:47:49,710 --> 01:47:51,670
if it works your on top of things

1637
01:47:52,350 --> 01:47:54,250
you're ready to get the next batch

1638
01:47:54,910 --> 01:47:56,570
the information from the lectures and readings

1639
01:47:58,630 --> 01:48:04,370
okay let's go on and let's talk about the technical material in the first module the cause the software module

1640
01:48:05,040 --> 01:48:08,080
we kick the course of talking about software engineering

1641
01:48:09,000 --> 01:48:14,820
for two reasons we'd like you know about software engineering is an incredibly important part of our department

1642
01:48:16,550 --> 01:48:21,620
it's an incredibly important part ivy engineering absolutely any system of any modern system

1643
01:48:23,980 --> 01:48:28,100
but we also like you to know about it because it provides a very convenient

1644
01:48:28,100 --> 01:48:30,580
way to think about it a convenient language

1645
01:48:32,760 --> 01:48:33,660
the think about

1646
01:48:34,290 --> 01:48:35,420
the design issues

1647
01:48:36,080 --> 01:48:40,720
engineering issues in all the other parts of the class so it's a very good place to start

1648
01:48:43,890 --> 01:48:49,590
so it it what what i will do today is talk about some of the very simplest ideas

1649
01:48:50,580 --> 01:48:52,320
about abstraction and modularity

1650
01:48:52,800 --> 01:48:56,080
in what i think of has the lowest level granularity

1651
01:48:56,760 --> 01:49:00,610
how do you think about abstraction and modularity at the micro scale

1652
01:49:02,290 --> 01:49:04,640
at the individual lines of code scale

1653
01:49:06,390 --> 01:49:09,190
as i said earlier we will add we progress

1654
01:49:10,650 --> 01:49:13,720
look at modularity and abstraction at the higher scale

1655
01:49:15,040 --> 01:49:18,570
but we have to start somewhere and we're gonna start by thinking about how do

1656
01:49:18,570 --> 01:49:21,680
you think about abstraction and modularity at the microscale

1657
01:49:23,690 --> 01:49:25,040
special about programming

1658
01:49:28,000 --> 01:49:29,440
what we are trying to do

1659
01:49:29,440 --> 01:49:31,240
by supporting

1660
01:49:31,260 --> 01:49:35,920
by selecting examples that supports meaning analogy and self explanation

1661
01:49:36,010 --> 01:49:39,100
and that also provide an interface

1662
01:49:39,110 --> 01:49:41,720
that's one example is selected

1663
01:49:41,740 --> 01:49:48,090
triggers the proper behaviors and therefore maximises the learning and problem solving that you can

1664
01:49:48,090 --> 01:49:52,510
do by using example just to give you a brief demo of

1665
01:49:52,590 --> 01:49:58,340
this interface this the system is in the domain on newtonian physics so the student

1666
01:49:58,340 --> 01:50:04,190
can select some problems in the system knowledge base and

1667
01:50:04,220 --> 01:50:09,040
the system tracked student can type equation in that kind control

1668
01:50:09,080 --> 01:50:13,800
elements like forces acceleration vectors in the free body diagram

1669
01:50:13,810 --> 01:50:18,390
the system tracks all the students actions and provide feedback on whether their actions are

1670
01:50:18,390 --> 01:50:20,930
correct or not in this case the student draw

1671
01:50:21,180 --> 01:50:26,400
the direction of the force in the in the wrong direction it's turned around the

1672
01:50:26,400 --> 01:50:28,670
student can at any time for

1673
01:50:28,680 --> 01:50:29,800
an example

1674
01:50:29,800 --> 01:50:33,960
the system selects example to present and as you can see the example is

1675
01:50:34,020 --> 01:50:39,050
presented with these boxes that cover different parts of example solution

1676
01:50:39,120 --> 01:50:42,860
and the idea of of having the boxes is to actually try to come up

1677
01:50:42,860 --> 01:50:44,820
with discourage

1678
01:50:44,830 --> 01:50:48,640
claims copying for students within gay we tend to do so

1679
01:50:48,660 --> 01:50:53,520
without these tools and also allow the system to track the students doing how the

1680
01:50:53,520 --> 01:50:55,300
students using examples

1681
01:50:55,300 --> 01:50:59,120
so what

1682
01:50:59,180 --> 01:51:07,830
howard examples selected so i said the main form of scaffolding in this system is

1683
01:51:07,830 --> 01:51:13,100
to try to select an example that is the best that can increase the students

1684
01:51:13,100 --> 01:51:18,470
but engage in the right meta cognitive processes was on problem the way this works

1685
01:51:18,770 --> 01:51:22,280
just going to give you a general description of the system architecture and how

1686
01:51:22,290 --> 01:51:29,280
example selection happened is that the system has a problem solving model that module that

1687
01:51:29,300 --> 01:51:33,810
it takes as input a knowledge base that includes that has the representation of all

1688
01:51:33,810 --> 01:51:35,580
the physics rules relevant

1689
01:51:36,320 --> 01:51:44,280
solve problems in between physics takes the problem specification and generates automatically solution representation of

1690
01:51:44,280 --> 01:51:45,400
the solution

1691
01:51:46,080 --> 01:51:53,210
the specific problem that a graph representation showing how intermediate solution steps derive from physics

1692
01:51:53,210 --> 01:51:59,010
rules and previous steps in the solution this is done for several problems and examples

1693
01:51:59,010 --> 01:52:01,400
to create an example of the problem of poor

1694
01:52:02,740 --> 01:52:08,460
the problem is selected to is selected by the students problems loading the interface and

1695
01:52:08,460 --> 01:52:11,630
everything that the student does as have seen in the video

1696
01:52:11,640 --> 01:52:14,210
get sent to the culture model

1697
01:52:14,270 --> 01:52:16,210
that checks whether

1698
01:52:16,260 --> 01:52:22,570
individual actions are correct or incorrect using the solution graph representation and use information

1699
01:52:22,580 --> 01:52:23,920
two update

1700
01:52:23,950 --> 01:52:27,640
the system systems knowledge the rest of the models

1701
01:52:27,720 --> 01:52:33,620
that is the dynamic work network that represents a any given point the sisters believe

1702
01:52:33,620 --> 01:52:38,650
of the evolution of student knowledge and the based on

1703
01:52:38,660 --> 01:52:43,840
and the cognitive skills based on the solution actions that the student has generated up

1704
01:52:43,870 --> 01:52:45,370
to a certain point

1705
01:52:45,400 --> 01:52:48,550
when the students to see an example

1706
01:52:48,610 --> 01:52:52,990
what happens is that the system uses this to the model

1707
01:52:53,010 --> 01:52:57,770
to engage in what we call the example selection mechanism by

1708
01:52:57,780 --> 01:53:03,140
doing in expected utility calculation for each example knowledge base to see how

1709
01:53:03,160 --> 01:53:08,160
what is the potential of these examples to aid learning and problem-solving and the way

1710
01:53:08,160 --> 01:53:13,990
this not just the very high level i cannot go into details is based using

1711
01:53:13,990 --> 01:53:20,390
this representation of example solution of the problem solution this graph based representation the system

1712
01:53:20,390 --> 01:53:22,890
can compute similarity

1713
01:53:22,930 --> 01:53:29,080
between corresponding steps based on whether two steps have rules in common whether they are

1714
01:53:29,090 --> 01:53:32,550
they use it was used the same objects in the solution

1715
01:53:32,700 --> 01:53:35,690
and uses using this measure of similarity

1716
01:53:35,710 --> 01:53:36,930
the system

1717
01:53:36,980 --> 01:53:40,260
engages in the simulation of how

1718
01:53:40,260 --> 01:53:45,510
a given student the current students with specific level of knowledge of the physics rules

1719
01:53:45,510 --> 01:53:47,590
and specific meta cognitive skills

1720
01:53:47,630 --> 01:53:50,050
can you give an example

1721
01:53:50,060 --> 01:53:52,270
to solve the current problem

1722
01:53:52,280 --> 01:53:54,050
and to learn from it

1723
01:53:54,070 --> 01:53:59,510
OK so the simulation process generates a prediction of learning and problem solving success for

1724
01:53:59,530 --> 01:54:01,040
the specific student

1725
01:54:01,090 --> 01:54:07,220
this prediction is formalized using a multiattribute utility function that gives the expected utility of

1726
01:54:07,220 --> 01:54:12,320
these examples in terms of learning and problem solving success

1727
01:54:12,340 --> 01:54:15,100
this process is done for every example known by

1728
01:54:15,360 --> 01:54:19,150
in the in the course knowledge base and the

1729
01:54:19,200 --> 01:54:24,130
in the end the system selects the example with the maximum expected utility

1730
01:54:24,150 --> 01:54:26,970
so we evaluated this mechanism presented the paper

1731
01:54:26,990 --> 01:54:28,700
this guy two years ago

1732
01:54:28,720 --> 01:54:33,660
we have sixteen participants in a lab setting solving problems with coach

1733
01:54:33,670 --> 01:54:37,760
was within subject design that is also all students

1734
01:54:37,760 --> 01:54:42,850
where even to the two experimental conditions there were two experimental conditions in one condition

1735
01:54:42,850 --> 01:54:47,850
the students solve problems with the aid of examples they were selected by

1736
01:54:47,860 --> 01:54:52,040
the system using the decision theoretic approach that i just described

1737
01:54:52,120 --> 01:54:58,700
the second condition received as examples you problem solving example there was most similar to

1738
01:54:58,700 --> 01:55:00,180
a given problem

1739
01:55:00,210 --> 01:55:04,640
and we use this condition because the approach of selecting the most similar example is

1740
01:55:04,640 --> 01:55:08,120
the one that other tutoring system in the literature have used

1741
01:55:08,140 --> 01:55:11,230
so the result in high-level description

1742
01:55:11,280 --> 01:55:16,960
is that students in the conditional where these examples were selected adopted by the system

1743
01:55:17,010 --> 01:55:24,350
generated significant statistically significantly more self explanation and fewer copy of the students in the

1744
01:55:24,350 --> 01:55:28,490
condition where the examples were given the examples given were the most similar

1745
01:55:28,510 --> 01:55:30,890
and this led to better

1746
01:55:30,920 --> 01:55:34,580
and this was in no cost for problem solving so there were no difference is

1747
01:55:34,580 --> 01:55:40,080
not significant differences in problem solving success between the two conditions so the examples selected

1748
01:55:40,080 --> 01:55:45,090
by the example selected by the system were good for both learning and problem solving

1749
01:55:45,120 --> 01:55:50,120
what we found was that the condition the only difference is that students made on

1750
01:55:50,120 --> 01:55:55,460
average more errors during problem solving and took longer to solve the problems but that

1751
01:55:55,460 --> 01:56:00,510
makes sense that byproduct of learning if you copy from an example definitely fast and

1752
01:56:00,510 --> 01:56:03,640
you don't make mistakes but you also to look at

1753
01:56:04,620 --> 01:56:10,440
we see the result is providing very encouraging evidence that our proposed decision theoretic approach

1754
01:56:10,490 --> 01:56:15,030
two examples select selection can trigger learn by triggering

1755
01:56:15,070 --> 01:56:17,580
good meta cognitive processes

1756
01:56:17,640 --> 01:56:23,460
and as the future work we want to generate a more proactive adaptive interventions as

1757
01:56:23,490 --> 01:56:28,310
the student uses example currently there is no system intervention if the system realizes that

1758
01:56:28,310 --> 01:56:30,940
the students copying too much but want to try to

1759
01:56:30,960 --> 01:56:35,810
january that give hints discourage students from copying if you're doing it too much

1760
01:56:35,910 --> 01:56:38,910
and also we want to explore the use of eye tracking

1761
01:56:38,940 --> 01:56:43,860
instead of the masking interface to capture student reasoning and what to do that because

1762
01:56:43,860 --> 01:56:47,650
we believe that i tracking can give better insight on what the students doing whether

1763
01:56:47,650 --> 01:56:50,680
or not student self explaining away

1764
01:56:50,700 --> 01:56:53,890
i want to use eye tracking

1765
01:56:53,940 --> 01:57:00,090
our belief the possible the potential use utility of my truck is there

1766
01:57:00,120 --> 01:57:05,290
we have already investigated the value of eye tracking using eye tracking information

1767
01:57:05,290 --> 01:57:09,800
to monitor student self explanation in different learning environments

