1
00:00:00,000 --> 00:00:03,010
with a the k time

2
00:00:05,250 --> 00:00:07,520
which is too divided by gunmen

3
00:00:07,540 --> 00:00:12,610
i remember guidelines unit second minus one for this really has units time

4
00:00:12,630 --> 00:00:15,180
so this one is going to be responsible

5
00:00:15,190 --> 00:00:21,090
four that pain that we discussed earlier

6
00:00:21,090 --> 00:00:23,760
what does this now

7
00:00:23,910 --> 00:00:25,850
two power

8
00:00:25,860 --> 00:00:30,150
j and n is obviously a frequency

9
00:00:30,210 --> 00:00:31,290
this is

10
00:00:31,330 --> 00:00:35,550
a striking example in the complex plane of a rotating

11
00:00:35,560 --> 00:00:37,070
that story

12
00:00:37,080 --> 00:00:38,950
and the angular frequency

13
00:00:38,970 --> 00:00:43,330
it is and but we know is

14
00:00:43,340 --> 00:00:48,260
and so this and the really is nothing but omega square and squared is really

15
00:00:48,260 --> 00:00:51,500
omega squared

16
00:00:51,550 --> 00:00:54,140
and so i can replace this and now

17
00:00:54,310 --> 00:00:59,520
by whatever you have here

18
00:00:59,620 --> 00:01:01,610
and so if i write is now

19
00:01:05,090 --> 00:01:06,700
lines come over to

20
00:01:06,730 --> 00:01:09,000
i can write down all j

21
00:01:09,030 --> 00:01:11,120
only get a lot of

22
00:01:11,170 --> 00:01:14,420
and i know exactly now what that omega is

23
00:01:14,500 --> 00:01:16,730
that is the square root

24
00:01:16,850 --> 00:01:20,350
of this number

25
00:01:20,390 --> 00:01:23,090
and so therefore we get square

26
00:01:23,160 --> 00:01:28,590
is omega zeros grant miners gone over four

27
00:01:28,630 --> 00:01:31,040
so those of you the audience

28
00:01:31,080 --> 00:01:36,710
four intuitively senses that the frequency would go down because of the friction there right

29
00:01:36,770 --> 00:01:39,120
psi omega is lower

30
00:01:39,180 --> 00:01:41,010
then omega zero

31
00:01:41,050 --> 00:01:45,360
now of course if gamma is very low the two could be closed again

32
00:01:45,370 --> 00:01:49,840
benson time that's always if you have a lot of friction then

33
00:01:49,860 --> 00:01:54,480
omega lot lot less than omega zero here very little friction

34
00:01:54,510 --> 00:01:56,610
well then they will be very closely

35
00:01:56,720 --> 00:01:58,110
the same

36
00:01:58,170 --> 00:02:01,110
and so we can now write down

37
00:02:01,120 --> 00:02:04,070
in all its glory

38
00:02:04,070 --> 00:02:06,780
the real part of this function

39
00:02:06,790 --> 00:02:08,760
which then becomes x

40
00:02:08,810 --> 00:02:10,560
equal some

41
00:02:10,570 --> 00:02:12,030
amplitude a

42
00:02:12,080 --> 00:02:14,080
times into the mine is gonna

43
00:02:14,090 --> 00:02:16,120
over two times t

44
00:02:16,180 --> 00:02:18,830
i'm in two minds j

45
00:02:18,870 --> 00:02:20,060
times omega

46
00:02:21,130 --> 00:02:23,760
lots of

47
00:02:25,460 --> 00:02:28,940
look what i did

48
00:02:29,110 --> 00:02:33,100
i have equation in the complex plane

49
00:02:33,180 --> 00:02:35,570
it's perfectly OK

50
00:02:35,570 --> 00:02:38,950
and then i went back to the real world going from the

51
00:02:39,020 --> 00:02:40,700
the complex plane

52
00:02:40,750 --> 00:02:43,040
but the real part of z

53
00:02:43,090 --> 00:02:45,060
what i write down

54
00:02:45,070 --> 00:02:46,820
this is fine

55
00:02:46,930 --> 00:02:51,270
then i wrote down into the power miners j omega people all

56
00:02:51,360 --> 00:02:54,560
first of all there should not be a minus sign in front of the j

57
00:02:54,570 --> 00:02:59,560
that's not a major problem but i do not take it out of the

58
00:02:59,580 --> 00:03:04,070
complex brain i believe in the complex plane spoke

59
00:03:04,080 --> 00:03:07,600
clearly what i should have read the which is what i will do now

60
00:03:08,530 --> 00:03:13,130
he calls eighty eight times e to the minus come over two times t

61
00:03:13,150 --> 00:03:14,590
the cosine

62
00:03:14,600 --> 00:03:18,170
of omega t plus of now

63
00:03:18,250 --> 00:03:21,330
we are in the real world

64
00:03:21,330 --> 00:03:23,650
you're going to see this equation

65
00:03:23,700 --> 00:03:28,830
at least for the next five to ten minutes on the tape it is for

66
00:03:28,850 --> 00:03:32,040
my site but there's nothing i can do about it you just have to live

67
00:03:32,040 --> 00:03:32,870
with it

68
00:03:32,910 --> 00:03:34,540
and think of it

69
00:03:34,550 --> 00:03:37,360
as being this

70
00:03:37,370 --> 00:03:39,480
and there are two adjustable

71
00:03:39,530 --> 00:03:43,060
constance which depends entirely on the initial conditions

72
00:03:43,150 --> 00:03:45,000
at time t equals zero

73
00:03:45,030 --> 00:03:48,140
there are two things you can do to the object you can bring it to

74
00:03:48,140 --> 00:03:52,230
a certain point away from equilibrium and you can do with the cake

75
00:03:52,290 --> 00:03:53,750
recall the velocity

76
00:03:53,760 --> 00:03:56,770
you are free to choose any way you want to do that

77
00:03:56,830 --> 00:04:00,580
so it's clear that was those two choices that you have

78
00:04:00,600 --> 00:04:02,460
that you final solution

79
00:04:02,480 --> 00:04:05,230
you end up with two adjustable constant which

80
00:04:05,270 --> 00:04:06,670
depend on the

81
00:04:06,690 --> 00:04:08,650
initial condition

82
00:04:08,730 --> 00:04:12,270
and so you see that this amplitude is going to die out

83
00:04:12,290 --> 00:04:16,100
it was one of the decay time of to overcome

84
00:04:17,040 --> 00:04:18,520
and that the

85
00:04:19,690 --> 00:04:20,830
is lower

86
00:04:20,850 --> 00:04:22,900
then the frequency

87
00:04:22,920 --> 00:04:26,830
of the undamped system

88
00:04:26,940 --> 00:04:28,650
i tried to make a

89
00:04:31,870 --> 00:04:34,400
so here's t

90
00:04:34,460 --> 00:04:37,230
and here is x

91
00:04:37,250 --> 00:04:39,000
and to guide my

92
00:04:39,000 --> 00:04:41,060
and i'm going to first

93
00:04:41,130 --> 00:04:44,440
fourteen is exponential

94
00:04:46,480 --> 00:04:49,920
and now i'm going to put in

95
00:04:52,460 --> 00:04:54,110
term here

96
00:04:54,170 --> 00:05:02,190
well by the frequency is uniquely determined omega is uniquely determined and the period p

97
00:05:02,230 --> 00:05:04,290
is that omega

98
00:05:04,310 --> 00:05:08,690
divided by two pi

99
00:05:10,690 --> 00:05:12,630
could i

100
00:05:12,650 --> 00:05:14,150
look what i wrote

101
00:05:14,190 --> 00:05:15,420
i wrote team

102
00:05:15,440 --> 00:05:17,350
because omega

103
00:05:17,400 --> 00:05:20,290
divided by two pi

104
00:05:20,330 --> 00:05:22,580
it was not my day

105
00:05:23,520 --> 00:05:25,290
it should have been keep

106
00:05:25,290 --> 00:05:29,230
now i think in general that we have about

107
00:05:29,240 --> 00:05:33,330
where there is this specialization relations of things higher up

108
00:05:33,380 --> 00:05:36,540
say more about the higher-ups in more they might be

109
00:05:36,590 --> 00:05:38,630
so pedersen or they might be

110
00:05:38,640 --> 00:05:42,360
it's about piece of that contain more events and more edges

111
00:05:42,410 --> 00:05:44,030
things mall

112
00:05:44,790 --> 00:05:48,790
that's a frequencies of size one recites who and what was so we have lot

113
00:05:48,800 --> 00:05:51,590
of patterns and we would like to find

114
00:05:51,610 --> 00:05:54,750
which things are frequent

115
00:05:56,260 --> 00:05:58,810
one can

116
00:05:58,820 --> 00:06:05,560
defined notions of the border of waste or or a quarter of the same

117
00:06:07,500 --> 00:06:11,800
let's all concrete to think about

118
00:06:11,820 --> 00:06:13,670
frequent sets

119
00:06:13,690 --> 00:06:15,320
so the positive

120
00:06:15,450 --> 00:06:19,580
it's a collection of subsets of the set of variables

121
00:06:20,370 --> 00:06:25,070
the positive border of the set are all maximal elements of maximal with respect to

122
00:06:25,070 --> 00:06:27,190
set inclusion

123
00:06:27,210 --> 00:06:32,720
so all those that are frequent but none of whose supporters have been

124
00:06:32,730 --> 00:06:34,690
so that's the positive border

125
00:06:34,740 --> 00:06:37,760
the negative order is are the minimum sentence

126
00:06:38,490 --> 00:06:43,690
which are not frequent so the sets which are not frequent but all of whose

127
00:06:43,700 --> 00:06:47,420
proper subsets are

128
00:06:47,440 --> 00:06:50,020
OK let's take an example

129
00:06:50,040 --> 00:06:57,090
so we have from a to f and the frequency collection looks like

130
00:06:58,710 --> 00:07:00,070
blah blah blah

131
00:07:00,080 --> 00:07:02,930
the positive border are the maximal sets

132
00:07:03,210 --> 00:07:05,360
all of the maximum so

133
00:07:05,410 --> 00:07:08,950
by inspection one of means

134
00:07:10,200 --> 00:07:15,660
and the same is the pace OK so clearly and maximum because it has the

135
00:07:15,660 --> 00:07:17,740
largest cardinality

136
00:07:17,790 --> 00:07:19,540
and when you look at

137
00:07:19,550 --> 00:07:20,870
all right

138
00:07:20,890 --> 00:07:22,250
everything here

139
00:07:22,260 --> 00:07:24,040
it's actually a

140
00:07:24,060 --> 00:07:26,760
subset of one of these

141
00:07:26,770 --> 00:07:29,620
so this is a positive one

142
00:07:29,660 --> 00:07:34,970
the negative border this the fact are the meaning of things which are not in

143
00:07:34,970 --> 00:07:37,120
this collection

144
00:07:37,130 --> 00:07:38,060
so well

145
00:07:38,080 --> 00:07:38,930
do you

146
00:07:39,010 --> 00:07:41,300
is not frequent it's not

147
00:07:41,340 --> 00:07:44,970
so in the last and then get bored border e is not

148
00:07:46,750 --> 00:07:48,730
it's in the minimum order

149
00:07:48,740 --> 00:07:53,830
BCE is not frequent and b is not free those of the minimum

150
00:07:55,990 --> 00:08:02,210
so possibly motor maximal frequent one's negative water more frequent ones

151
00:08:02,220 --> 00:08:05,810
OK one can actually do the same thing trained general

152
00:08:05,840 --> 00:08:08,500
ask what are the most the

153
00:08:08,520 --> 00:08:11,670
maximal frequent substrings and negative

154
00:08:12,080 --> 00:08:13,750
the border is then the

155
00:08:13,970 --> 00:08:19,820
the frequent substring of let's not talk about that

156
00:08:21,480 --> 00:08:23,250
now the algorithm i

157
00:08:23,550 --> 00:08:25,500
you actually asked

158
00:08:25,510 --> 00:08:28,490
this many questions

159
00:08:28,500 --> 00:08:31,380
questions of five x frequency

160
00:08:31,460 --> 00:08:35,830
for all frequencies lost the negative or the size of the negative water

161
00:08:35,850 --> 00:08:37,760
and actually one can show that in

162
00:08:37,800 --> 00:08:39,880
again in the model this is a

163
00:08:39,920 --> 00:08:43,770
this is what you have to do

164
00:08:43,880 --> 00:08:49,270
OK the sizes of the borders tend to be relatively large

165
00:08:49,280 --> 00:08:54,420
but this thing i wanted to spend a few minutes on the relationship between the

166
00:08:54,420 --> 00:08:57,170
positive and negative border

167
00:08:58,410 --> 00:09:00,060
if i tell you

168
00:09:00,080 --> 00:09:04,280
guys in the back row by using maximal frequent sex

169
00:09:04,290 --> 00:09:06,860
and now specify all

170
00:09:06,890 --> 00:09:08,850
so below the

171
00:09:08,860 --> 00:09:11,550
is frequent and nothing else is

172
00:09:11,560 --> 00:09:13,200
or if i tell you

173
00:09:13,210 --> 00:09:18,000
the minimum frequency again i specified the frequency collections

174
00:09:18,050 --> 00:09:20,010
you need

175
00:09:20,020 --> 00:09:22,540
so given the positive border given

176
00:09:22,600 --> 00:09:25,710
living in that border they both specify

177
00:09:25,720 --> 00:09:29,310
the same collection of subsets

178
00:09:30,660 --> 00:09:34,290
but the thing is that we got the water actually

179
00:09:34,300 --> 00:09:38,760
the set of all minimal transversals of the complement of the set

180
00:09:38,810 --> 00:09:39,600
in the

181
00:09:39,610 --> 00:09:42,760
positive more

182
00:09:42,780 --> 00:09:46,070
what does that mean

183
00:09:49,850 --> 00:09:54,100
it's basically an extension of the upper edges can have more than two

184
00:09:54,110 --> 00:09:55,440
but this is

185
00:09:55,440 --> 00:09:57,320
so given a set u

186
00:09:57,340 --> 00:10:02,180
eight is the hypergraph with it or a simple hypergraph

187
00:10:02,230 --> 00:10:06,410
there is no element is empty and no

188
00:10:06,410 --> 00:10:07,910
with which they enabled

189
00:10:08,450 --> 00:10:11,540
your part to escape further pain

190
00:10:12,230 --> 00:10:15,870
the information is legal in switzerland was then

191
00:10:16,430 --> 00:10:17,830
andy indeed

192
00:10:18,790 --> 00:10:20,870
it led us to believe that

193
00:10:23,000 --> 00:10:27,000
i like this idea that one could control one so there

194
00:10:27,680 --> 00:10:31,390
just as one could control so much in one's own life

195
00:10:33,270 --> 00:10:39,180
he designed a monument for his father and obsessed over the choice of flowers

196
00:10:40,750 --> 00:10:44,140
here he is with charlotte county on part of his

197
00:10:44,600 --> 00:10:45,430
paris life

198
00:10:45,980 --> 00:10:48,660
they designed furniture you know very well

199
00:10:49,180 --> 00:10:52,540
endless stories sister who really deserves credit foreign

200
00:10:54,180 --> 00:10:54,640
his cousin

201
00:10:55,100 --> 00:10:59,470
peers generate work in the office on the corbusier himself

202
00:11:00,890 --> 00:11:08,310
but we all know the furnitur which today is all over the world he did this fantastic that encourage

203
00:11:08,750 --> 00:11:10,230
but this time family

204
00:11:13,310 --> 00:11:16,870
had such an eye for design that to me

205
00:11:17,470 --> 00:11:18,500
anyone who worked

206
00:11:18,950 --> 00:11:26,100
in his office more than anything else would him by his sense rhythm his sense texture

207
00:11:26,620 --> 00:11:28,080
this meant so much

208
00:11:28,710 --> 00:11:35,250
this is a wonderful photograph i think mondrian standing on the roof of the car should on

209
00:11:36,620 --> 00:11:39,230
with his hand on the spiral staircase

210
00:11:39,810 --> 00:11:42,410
the sample of ten short or you know

211
00:11:43,100 --> 00:11:46,520
every time one sees that one discover something else

212
00:11:46,980 --> 00:11:48,540
but above all else

213
00:11:48,950 --> 00:11:49,850
it has many

214
00:11:50,520 --> 00:11:51,890
the same ideals

215
00:11:52,330 --> 00:11:54,520
that we see in the exhibition

216
00:11:55,000 --> 00:11:57,270
i got friend above

217
00:11:57,810 --> 00:12:00,390
which is the point of architecture

218
00:12:01,120 --> 00:12:02,480
is to worship nature

219
00:12:04,480 --> 00:12:05,450
time and again

220
00:12:06,390 --> 00:12:07,680
in his projects

221
00:12:08,160 --> 00:12:10,330
one the problems he wanted to do

222
00:12:10,870 --> 00:12:13,390
with given human nature at you looking

223
00:12:13,620 --> 00:12:16,020
fields have you feel close to the sky

224
00:12:16,700 --> 00:12:20,100
it is they worship nature it's simply the container

225
00:12:20,580 --> 00:12:22,470
for looking at nature and by the way

226
00:12:23,140 --> 00:12:26,930
not a machine from the that's another cliche

227
00:12:27,790 --> 00:12:31,620
which has been blown out of all proportion and the court lucia

228
00:12:32,290 --> 00:12:36,350
took task he wrote about how ridiculous it was

229
00:12:36,830 --> 00:12:42,430
that's something is said to a friend on the street became turned into this hackneyed idea

230
00:12:43,080 --> 00:12:46,410
the corbusier's buildings war machine from the

231
00:12:47,040 --> 00:12:48,520
be emphasis was on

232
00:12:49,270 --> 00:12:53,710
looking and everything else and he used the term with diary

233
00:12:55,370 --> 00:12:57,020
in the late twenties he

234
00:12:57,560 --> 00:12:59,020
got a commission to go to

235
00:13:00,180 --> 00:13:01,180
south america

236
00:13:03,660 --> 00:13:07,620
working on buildings they are he discovered early in the trip

237
00:13:08,480 --> 00:13:09,680
josephine baker

238
00:13:10,160 --> 00:13:12,140
he met her getting on to a both

239
00:13:14,100 --> 00:13:17,350
i just want picture this corpus is swiss

240
00:13:17,930 --> 00:13:18,520
he is

241
00:13:19,560 --> 00:13:21,120
forty three years old

242
00:13:21,560 --> 00:13:22,480
he meets a young

243
00:13:23,410 --> 00:13:26,870
half-black half-white american jazz singer

244
00:13:27,730 --> 00:13:28,710
it's known

245
00:13:29,180 --> 00:13:30,470
photographs are known

246
00:13:32,480 --> 00:13:32,830
and she

247
00:13:33,470 --> 00:13:34,410
had a love affair

248
00:13:35,560 --> 00:13:37,250
and it's always written and all

249
00:13:38,680 --> 00:13:40,890
that's where they met in rio

250
00:13:41,830 --> 00:13:44,250
getting on to a boat that was going to bordeaux

251
00:13:45,370 --> 00:13:47,890
and for ten days they had a romance

252
00:13:49,020 --> 00:13:53,250
with air i was plugging away reed his letters to his mother

253
00:13:54,290 --> 00:13:58,020
i was pretty close to my own mother but i would not have written are

254
00:13:59,930 --> 00:14:00,730
this one and

255
00:14:01,210 --> 00:14:02,290
who represented

256
00:14:04,980 --> 00:14:09,200
great human qualities who was in many ways his great love

257
00:14:11,140 --> 00:14:15,850
he was already engaged to marry bond with whom he was living in paris

258
00:14:16,330 --> 00:14:19,730
and in fact one discovers from the letters from

259
00:14:20,540 --> 00:14:21,290
that he met

260
00:14:21,870 --> 00:14:25,790
josephine two months prior to the crossing to border

261
00:14:26,520 --> 00:14:28,230
when they got on the ship

262
00:14:28,790 --> 00:14:35,410
it had been a carefully planned trip they decided to travel together and they've been spending a lot of

263
00:14:35,480 --> 00:14:40,710
time together already her writing about him is magnificent

264
00:14:41,230 --> 00:14:42,770
she said that the court lucia

265
00:14:43,480 --> 00:14:45,200
you know gay it is there

266
00:14:46,000 --> 00:14:49,470
what we can translate this into english today because it would have

267
00:14:50,930 --> 00:14:55,750
and simple each of which has come to mean something very different i

268
00:14:56,060 --> 00:14:57,750
then they did back then

269
00:14:58,250 --> 00:14:58,970
but he was

270
00:14:59,620 --> 00:15:03,310
the game is played in the sense that the words of

271
00:15:03,370 --> 00:15:05,890
in nineteen twenty nine and a person

272
00:15:07,750 --> 00:15:15,640
flirtatiousness heart hard-charging looking down there are in his white shoes looking at josephine has she glass

273
00:15:15,640 --> 00:15:20,770
molecules that the wires from the eye to the brain and from the brain inside

274
00:15:20,770 --> 00:15:22,330
one place

275
00:15:22,380 --> 00:15:23,330
the other

276
00:15:23,420 --> 00:15:26,380
recognise in order to form and

277
00:15:26,480 --> 00:15:27,810
they on the right track

278
00:15:27,880 --> 00:15:30,230
what are these molecules

279
00:15:30,250 --> 00:15:31,070
so the

280
00:15:31,070 --> 00:15:37,030
miles human genome project have transformed our approach to these questions it used to be

281
00:15:37,080 --> 00:15:39,700
that it would take

282
00:15:39,800 --> 00:15:41,410
maybe three to five years

283
00:15:42,150 --> 00:15:47,200
a graduate student or postdoc fellow to hammer away

284
00:15:47,280 --> 00:15:48,440
find the gene

285
00:15:48,460 --> 00:15:49,870
and then

286
00:15:49,890 --> 00:15:51,220
perhaps cloning

287
00:15:51,240 --> 00:15:52,150
and now

288
00:15:52,160 --> 00:15:58,240
in six months we could do well last and find out what are the genes

289
00:15:58,240 --> 00:16:00,160
that are expressed in a piece of tissue

290
00:16:00,390 --> 00:16:04,350
one of these techniques is to use DNA microarrays

291
00:16:04,360 --> 00:16:06,020
we can little chips

292
00:16:06,070 --> 00:16:11,430
that can be improved to six thousand genes or gene fragments literature like this

293
00:16:11,490 --> 00:16:14,310
and we can extract the piece of brain

294
00:16:14,310 --> 00:16:16,410
in this case from a new-born mouse

295
00:16:16,430 --> 00:16:19,220
the visual cortex of the somatosensory cortex

296
00:16:19,320 --> 00:16:21,200
an extract the irony from it

297
00:16:21,220 --> 00:16:22,550
and then

298
00:16:22,560 --> 00:16:27,030
play this on a on this chip and we can ask what the genes or

299
00:16:27,030 --> 00:16:29,080
gene fragments in

300
00:16:29,120 --> 00:16:33,250
the somatosensory cortex or in the visual cortex and then we can compare them ask

301
00:16:33,250 --> 00:16:36,890
how this part of the brain differs from that at this stage

302
00:16:36,940 --> 00:16:38,870
OK and this tells us

303
00:16:38,880 --> 00:16:42,370
what are the molecules that are likely to be expressed in one part of the

304
00:16:42,370 --> 00:16:48,000
brain which is another and these molecules must be critical for making the pathways that

305
00:16:48,000 --> 00:16:53,070
differentiate the visual from the somatosensory cortex or the pattern of the sense of vision

306
00:16:53,400 --> 00:16:55,130
from the sense of touch

307
00:16:55,160 --> 00:16:58,440
specific pathways to and from cortical areas

308
00:16:58,450 --> 00:17:02,950
likely associated with very specific molecules i haven't told you a bunch of other and

309
00:17:02,950 --> 00:17:07,650
evidence gathering even as i speak on gamma function and loss function study that's how

310
00:17:07,650 --> 00:17:10,330
you understand how to the gene

311
00:17:10,340 --> 00:17:12,970
do what it does is venue move it

312
00:17:13,030 --> 00:17:16,830
by knocking it out you in the extra copies of it or in order to

313
00:17:16,840 --> 00:17:17,970
different players

314
00:17:18,030 --> 00:17:22,460
you can ask does function go away when you remove it does function appears a

315
00:17:22,460 --> 00:17:27,110
new and in different football when you somewhere else and there's the whole technology that

316
00:17:28,880 --> 00:17:33,370
it is networks that process information and make something new

317
00:17:33,380 --> 00:17:34,590
something or

318
00:17:34,610 --> 00:17:39,350
or make a new output from a elementary in one

319
00:17:39,400 --> 00:17:41,140
networks are

320
00:17:42,040 --> 00:17:45,940
he was metaphor that i used earlier the engine of the brain

321
00:17:45,970 --> 00:17:47,790
they are the cost of the brain

322
00:17:47,840 --> 00:17:50,440
and networks exist all over

323
00:17:50,460 --> 00:17:55,120
a model system that my laboratory and many other laboratories used to understand visual network

324
00:17:55,330 --> 00:17:57,330
how to develop the work

325
00:17:57,330 --> 00:17:59,030
in the visual system

326
00:17:59,080 --> 00:18:03,470
in vision is something that we don't think about and for the most part but

327
00:18:03,470 --> 00:18:07,580
least half of our brain devoted directly or indirectly television

328
00:18:07,600 --> 00:18:11,410
vision is such an important modality for us and

329
00:18:11,460 --> 00:18:13,330
we see the way we do

330
00:18:13,370 --> 00:18:15,610
because there are networks in our head

331
00:18:15,660 --> 00:18:20,390
that the visual inputs from the archive which is just the sort of pixels

332
00:18:20,470 --> 00:18:24,470
and transform it into shapes and images

333
00:18:24,520 --> 00:18:29,280
we don't see many things that we could because we do not have the networks

334
00:18:29,330 --> 00:18:33,740
and we see some things that we shouldn't when networks can be fit and that's

335
00:18:35,950 --> 00:18:37,680
we starts with the eyes

336
00:18:37,690 --> 00:18:40,460
but i suggest the front end of vision

337
00:18:40,470 --> 00:18:45,180
from the side information goes to the structure collapsed the already showed you in the

338
00:18:45,910 --> 00:18:47,040
in the schematic

339
00:18:47,040 --> 00:18:51,320
from here information was to the primary visual cortex and in people there are at

340
00:18:51,320 --> 00:18:54,900
last count some part the visual areas

341
00:18:54,920 --> 00:18:56,930
many more than whenever

342
00:18:56,940 --> 00:19:01,640
imagine but due to functional brain imaging and physiology we now understand

343
00:19:01,650 --> 00:19:06,280
some thirty area that make up or nearly half of the back part of the

344
00:19:07,440 --> 00:19:09,490
that has to do with it

345
00:19:10,730 --> 00:19:13,730
yet there is one kind of network

346
00:19:13,780 --> 00:19:19,300
it has become the paradigmatic network for understanding how vision works and that of the

347
00:19:22,930 --> 00:19:26,700
before generating orientation selectivity

348
00:19:28,370 --> 00:19:29,870
two physiologist

349
00:19:31,200 --> 00:19:33,130
given the nobel prize

350
00:19:33,130 --> 00:19:36,340
for discovering orientation selectivity in v one

351
00:19:36,770 --> 00:19:38,740
they were unfortunately at harvard

352
00:19:38,960 --> 00:19:42,830
but they the simplest of experiments

353
00:19:42,870 --> 00:19:46,630
this that an electrode into the primary visual cortex of the cat

354
00:19:46,690 --> 00:19:51,160
and that's what makes these cells respond to

355
00:19:51,170 --> 00:19:54,840
the humblest of questions but it has the deepest around

356
00:19:54,840 --> 00:20:00,000
what makes the cell respond best individual caught in the primary visual cortex vocabulary monkey

357
00:20:00,000 --> 00:20:02,110
and we now know even in humans

358
00:20:03,050 --> 00:20:05,530
in a little edge of life

359
00:20:05,580 --> 00:20:09,300
it's an edge of light that moves in a certain has a certain orientation and

360
00:20:09,300 --> 00:20:11,030
move in a certain direction

361
00:20:11,110 --> 00:20:12,320
OK it's not

362
00:20:12,370 --> 00:20:16,430
light everywhere is not a pixel of light is an edge of life

363
00:20:16,480 --> 00:20:17,880
one edge

364
00:20:18,520 --> 00:20:22,040
that's a hard question because we can of course construct of i can tell you

365
00:20:22,040 --> 00:20:25,110
that adaptation is tails but i

366
00:20:25,180 --> 00:20:29,160
and that's because the world is made up of features or shapes are bounded by

367
00:20:29,160 --> 00:20:32,840
line segments that constitute the edge of the ship

368
00:20:32,910 --> 00:20:36,780
and so the first

369
00:20:36,780 --> 00:20:39,530
elementary but critical step

370
00:20:39,540 --> 00:20:41,080
in perceiving shape

371
00:20:41,090 --> 00:20:43,450
is to is to detect an edge

372
00:20:43,470 --> 00:20:46,060
and the primary visual cortex

373
00:20:46,060 --> 00:20:47,980
and all higher mammals

374
00:20:48,000 --> 00:20:50,270
therefore has this property

375
00:20:50,320 --> 00:20:55,160
that generates the selectivity to the orientation of an edge

376
00:20:55,220 --> 00:21:00,050
and this is created in the cortex because the new record from zero here

377
00:21:00,130 --> 00:21:05,810
they don't have anything like this property they don't respond to this one little blobs

378
00:21:05,890 --> 00:21:12,690
so my own lab we have demonstrated that not only are cells responsible edges but

379
00:21:12,690 --> 00:21:16,380
they are organised in a specific interesting way

380
00:21:16,420 --> 00:21:18,620
and we know that by using now

381
00:21:18,620 --> 00:21:20,190
two the physiology

382
00:21:21,140 --> 00:21:26,330
include not only recording the activity of single celled by loring wires very fine wires

383
00:21:26,340 --> 00:21:30,980
into the brain and picking up the spikes that put out but also using techniques

384
00:21:30,980 --> 00:21:33,530
such as optical imaging with a very similar in principle

385
00:21:33,620 --> 00:21:34,670
the functional

386
00:21:34,750 --> 00:21:36,710
magnetic resonance imaging

387
00:21:36,730 --> 00:21:39,570
you can shine red light on the cortex

388
00:21:40,920 --> 00:21:42,390
active neurons

389
00:21:42,390 --> 00:21:46,940
need oxygen which is brought to them by the micro vasculature and when the oxygen

390
00:21:46,940 --> 00:21:47,960
is given up

391
00:21:48,020 --> 00:21:50,020
and the blood becomes blue

392
00:21:50,030 --> 00:21:52,880
then it absorbs red light

393
00:21:52,930 --> 00:21:55,160
so active neurons

394
00:21:55,170 --> 00:21:59,130
sure the regions in which other active short lives

395
00:21:59,220 --> 00:22:01,350
so here's a snapshot

396
00:22:01,360 --> 00:22:03,370
of the picture

397
00:22:03,370 --> 00:22:08,990
the 2nd thing would then Gilmore cause we the media citizen journalism which is why

398
00:22:08,990 --> 00:22:15,130
you a AUS bloggers actually influence the world today significantly well it's a combination of

399
00:22:15,130 --> 00:22:20,710
things they were self linking community that Google will float to the top and Google

400
00:22:20,710 --> 00:22:25,030
and technologies that I started indexing them in real time and also now you can

401
00:22:25,050 --> 00:22:31,920
get better news feeds than leads from their that you could by doing more cost

402
00:22:32,360 --> 00:22:38,350
not better journalism so read the book with immediate about other things those it through

403
00:22:38,350 --> 00:22:42,910
believe product reviews you by camera go look cruiser when noses but this is collective

404
00:22:42,910 --> 00:22:49,510
knowledge every day we use it kind of course there's amateur academic Wikipedia right now

405
00:22:49,520 --> 00:22:57,550
what about semantic Web visited there was well let's just as not get about is

406
00:22:57,570 --> 00:23:01,170
is it is exactly but ontologies a rule whatever this is all by the technology

407
00:23:01,180 --> 00:23:05,110
in general technology plot this thing and I think we'll find out how are technology

408
00:23:05,210 --> 00:23:12,430
that's and this is what I think what technology can do for collected knowledge system

409
00:23:12,750 --> 00:23:17,210
is a can be of memory can be a sensor actuator

410
00:23:17,590 --> 00:23:24,510
nervous system distributing things we've sort got that nails named adult global telecommunications systems like

411
00:23:24,510 --> 00:23:34,090
mixers and everything databases are basically free the capture happening for free digital sensors um

412
00:23:34,090 --> 00:23:40,870
many-to-many communications happening What's new at was he was the unnamed bounded hard interesting problem

413
00:23:40,870 --> 00:23:46,130
is how to extract value from all the stuff I think that's worsened technology can

414
00:23:46,220 --> 00:23:51,790
play games so in fact I'm that talk about 2 ways that might happen to

415
00:23:51,790 --> 00:23:57,490
be examples of how the intersection of social Web and the semantic Web can happen

416
00:23:57,510 --> 00:24:04,210
for semantic kind of things connected to the first one is about tagging data everyone's

417
00:24:04,210 --> 00:24:08,250
and this a great talent for talk about at dinner with Scientology people has on

418
00:24:08,250 --> 00:24:12,790
the attacks whatever somewhat will talk about that and then the other 1 is blogging

419
00:24:12,790 --> 00:24:19,010
data and how can we provide the 1st case the role of technology Chris got

420
00:24:19,010 --> 00:24:23,270
slots attack tagging all over the place how do you compose many great do create

421
00:24:23,270 --> 00:24:28,530
added value by composing integrate tagging data from other places a 2nd problem is highly

422
00:24:28,550 --> 00:24:34,470
in fact create our knowledge repository where it has more value as a repository then

423
00:24:34,470 --> 00:24:39,330
it were distributed so 1st case if there's a spell Clay Shaw he's a very

424
00:24:39,330 --> 00:24:46,350
elegant writer at the University and he wrote this blog ontologies or irritable blob what

425
00:24:46,350 --> 00:24:51,270
lots of stuff you know everyone knows Dewey decimal system is horrible and therefore semantic

426
00:24:51,270 --> 00:24:56,790
Web is useless was a couple of missing pieces of the inference China by police

427
00:24:56,790 --> 00:25:03,270
basically getting at is that yes of course particles into control taxonomic systems when used

428
00:25:03,270 --> 00:25:07,730
as like Mike librarians used do justice system is not a great with categorize no

429
00:25:07,730 --> 00:25:10,970
information on the Web I completely agree

430
00:25:11,370 --> 00:25:16,030
he just the hate he also says that by the way tagging seems to were

431
00:25:16,050 --> 00:25:21,790
just ridiculous because anyone can do it and everything works everything detector if that's OK

432
00:25:21,790 --> 00:25:27,470
I mean he's not fool says that's OK that's point is making now our order

433
00:25:27,480 --> 00:25:31,750
counter blogger whatever does appear from nowhere said about that but the thing was the

434
00:25:31,750 --> 00:25:38,780
got me thinking that the the whole mistake here was that there equating ontology fact

435
00:25:38,780 --> 00:25:44,610
the whole semantic Web 9 yards was thrown in there just because the failings of

436
00:25:44,610 --> 00:25:48,880
taxonomies as a classification system for the or not as across the criticism is applying

437
00:25:48,880 --> 00:25:54,190
these systems we give the wrong message sometimes like all taxonomy ontology whatever is all

438
00:25:54,190 --> 00:25:59,070
about a library shelf has nothing to do with that it has to do with

439
00:25:59,070 --> 00:26:04,590
an enabling technology my view for interoperability knowledge and if we think of it that

440
00:26:04,590 --> 00:26:09,550
way ontologies are about putting the surface membrane an agent so they can interact with

441
00:26:10,000 --> 00:26:15,690
with other nations is not about creating a single mom the database that everyone has

442
00:26:15,690 --> 00:26:20,850
the lock lock into life and EKG take that view then we can actually take

443
00:26:20,850 --> 00:26:25,910
this question of folk studied turned on its head so was folks where the limitations

444
00:26:25,910 --> 00:26:32,930
of folks ontology can help so I want to the single Kaltag camp which was

445
00:26:32,930 --> 00:26:37,650
upped this is the way the Web 2 . 0 crowd has conferences everybody but

446
00:26:37,650 --> 00:26:43,250
sleeping bags and we slept on the floor of an office building and hacks for

447
00:26:43,250 --> 00:26:48,690
24 hours and had talks and so on and it just amazing it was it

448
00:26:48,690 --> 00:26:52,210
using a small number of features they really for

449
00:26:52,220 --> 00:26:54,670
so random doesn't

450
00:26:54,710 --> 00:26:57,440
surprise just randomly select features

451
00:26:57,450 --> 00:26:59,940
it grows at the beginning of a little bit and then

452
00:26:59,950 --> 00:27:02,220
almost remain constant

453
00:27:02,270 --> 00:27:08,070
information gain i mentioned that uses feature occurrence and non occurrence of the feature

454
00:27:08,090 --> 00:27:10,630
is not is not specially with here

455
00:27:10,650 --> 00:27:13,070
so you can see a small peak around point

456
00:27:14,680 --> 00:27:20,870
performance so by the performance is a combination of precision and recall

457
00:27:20,890 --> 00:27:21,800
after all

458
00:27:21,800 --> 00:27:24,190
and we want to maximize it

459
00:27:24,220 --> 00:27:27,130
so mutual information cross entropy for text

460
00:27:27,140 --> 00:27:30,920
peak at around the same point alteration is about

461
00:27:31,270 --> 00:27:37,210
the same point here corresponds to about hundred features so using about hundred features

462
00:27:37,220 --> 00:27:41,400
out of you know you so we go from about a thousand

463
00:27:42,040 --> 00:27:46,060
i don't know what was thirty thousand features so using a hundred and getting good

464
00:27:46,060 --> 00:27:50,040
performance is really good if you have thirty thousand features

465
00:27:50,100 --> 00:27:53,960
and alterations this curve so it's kind of

466
00:27:54,020 --> 00:27:57,290
dropping in the performance of bit slower

467
00:27:58,230 --> 00:28:01,500
like cross entropy and mutual information

468
00:28:01,500 --> 00:28:04,020
and that's on one of the data dataset

469
00:28:04,080 --> 00:28:05,600
next i'll just show

470
00:28:05,610 --> 00:28:08,690
a table with all the

471
00:28:08,710 --> 00:28:13,670
naive bayesian classifier think this was using a bayesian classifier

472
00:28:13,690 --> 00:28:19,440
for document categorisation and also these results using naive bayesian classifier so these are five

473
00:28:20,810 --> 00:28:23,810
eight thousand categories how categories

474
00:28:23,830 --> 00:28:28,480
the first column is the name of the feature scoring measures

475
00:28:28,480 --> 00:28:29,600
so you can see

476
00:28:29,600 --> 00:28:32,420
oddsratio term frequency

477
00:28:32,460 --> 00:28:34,460
being up there

478
00:28:34,480 --> 00:28:38,100
and these are all performances

479
00:28:38,110 --> 00:28:41,690
when you think about fifty two hundred best features

480
00:28:41,690 --> 00:28:43,290
which turns out to be

481
00:28:43,310 --> 00:28:47,370
you know zero points two to two five percent of all the features

482
00:28:47,380 --> 00:28:51,100
which i think is very nice if you can reduce the feature subset

483
00:28:51,150 --> 00:28:55,270
and you can see that is really improved performance of naive bayes

484
00:28:55,330 --> 00:28:57,130
if you compare it to the last

485
00:28:57,130 --> 00:28:58,810
row in each set

486
00:28:58,830 --> 00:29:00,400
random performance

487
00:29:00,400 --> 00:29:02,790
so performance here

488
00:29:04,230 --> 00:29:05,690
frank is basically

489
00:29:05,710 --> 00:29:06,500
you know

490
00:29:06,540 --> 00:29:08,960
if you have

491
00:29:08,960 --> 00:29:11,310
of their eight thousand different categories

492
00:29:11,380 --> 00:29:16,600
it's hierarchical classification the classifier here is induced in the way that for each category

493
00:29:16,630 --> 00:29:18,850
there is a binary classifier

494
00:29:18,920 --> 00:29:23,960
and then when you have new testing document you you ask each of those eight

495
00:29:23,960 --> 00:29:25,880
thousand binary classifiers

496
00:29:25,940 --> 00:29:29,150
what is the probability that a document belongs to this class

497
00:29:29,210 --> 00:29:33,520
and and then you sort of the categories based on the predictive probability

498
00:29:33,580 --> 00:29:37,810
and of course you can check for the rank of the really correct category

499
00:29:37,830 --> 00:29:43,250
so that's the first revelation just saying on which position the correct one occurs

500
00:29:43,310 --> 00:29:46,330
and for us it was impressive to see that

501
00:29:46,380 --> 00:29:50,750
on the sixth position after eight thousand you get the correct category and that's of

502
00:29:50,750 --> 00:29:54,270
course and rachel cross validation experiments

503
00:29:54,310 --> 00:29:56,210
i have to measure

504
00:29:56,230 --> 00:30:00,060
and then the precision and recall split into precision and recall

505
00:30:00,100 --> 00:30:02,940
and the last column is showing something

506
00:30:02,960 --> 00:30:06,630
connected to efficiency

507
00:30:06,670 --> 00:30:11,230
you should start worrying when i said you know for each test example you should

508
00:30:11,270 --> 00:30:14,960
ask eight thousand classifiers what they think about that

509
00:30:14,960 --> 00:30:19,730
so the last column saying no you don't need to use a thousand classifiers

510
00:30:19,750 --> 00:30:23,870
it's enough if you ask a hundred of them but of course not

511
00:30:23,870 --> 00:30:25,830
you know quite smartly

512
00:30:25,880 --> 00:30:27,560
selective hundred

513
00:30:27,560 --> 00:30:31,630
and how is that achieved this very simple

514
00:30:31,630 --> 00:30:32,920
naive bayes and

515
00:30:32,920 --> 00:30:34,440
formula the model

516
00:30:34,440 --> 00:30:35,440
he has

517
00:30:35,440 --> 00:30:39,170
conditional probabilities of occurrence of some of the features

518
00:30:39,250 --> 00:30:42,810
connected to the class and if you look into the best in document

519
00:30:42,850 --> 00:30:44,600
it is very sparse

520
00:30:44,630 --> 00:30:47,080
so only the models they have some

521
00:30:47,100 --> 00:30:51,900
overlap between the features which are testing documents should be

522
00:30:51,900 --> 00:30:56,330
because to try you don't you don't need to classify documents if there is none

523
00:30:56,350 --> 00:30:59,310
of the words from the document is actually in the model

524
00:30:59,310 --> 00:31:02,750
then you know you predict the prior probability of the class

525
00:31:02,770 --> 00:31:04,710
so that's the idea behind

526
00:31:04,730 --> 00:31:09,350
and what is the kind of contrition why information gain

527
00:31:09,400 --> 00:31:12,400
it is not performing on this data so well

528
00:31:12,400 --> 00:31:18,080
as maybe on some other data i should say that or some other experiments using

529
00:31:18,130 --> 00:31:19,980
the nearest neighbour algorithm

530
00:31:20,000 --> 00:31:23,150
four classifying documents will

531
00:31:23,150 --> 00:31:28,400
on the data which is not so unbalanced in the class distribution it performs well

532
00:31:28,400 --> 00:31:32,560
so i don't want to miss guided using information gain is is not good it's

533
00:31:32,560 --> 00:31:33,730
not here

534
00:31:33,750 --> 00:31:38,750
on this setting with unbalanced data set and naive bayes classifier

535
00:31:38,830 --> 00:31:43,810
and why not it selects when you know if you look into the features

536
00:31:43,830 --> 00:31:46,880
which are the features that information gain has selected

537
00:31:46,920 --> 00:31:48,540
you know how they look

538
00:31:48,560 --> 00:31:50,380
they are

539
00:31:50,400 --> 00:31:54,330
you can see that most of them occur negative documents

540
00:31:54,380 --> 00:31:55,790
and you can see that

541
00:31:55,830 --> 00:31:58,230
it's also for some of the features is

542
00:31:58,230 --> 00:32:00,150
the fact that they don't occur

543
00:32:00,150 --> 00:32:03,270
the reason why they are selected by information gain

544
00:32:03,290 --> 00:32:04,380
and that's how the

545
00:32:04,380 --> 00:32:05,750
you know that's not the

546
00:32:05,750 --> 00:32:07,100
the message you want to

547
00:32:07,110 --> 00:32:10,560
give to your mother

548
00:32:20,100 --> 00:32:26,460
it's because have so it's not one but two

549
00:32:26,480 --> 00:32:31,630
which puts more emphasis on recall and the reason behind his

550
00:32:31,630 --> 00:32:36,040
you know this is not a very good way because you treat all the unobserved

551
00:32:36,040 --> 00:32:40,550
terms equal irrespective of the overall corpus frequency right so you get a very high

552
00:32:40,550 --> 00:32:41,520
probability to

553
00:32:41,890 --> 00:32:43,270
to turn say

554
00:32:43,270 --> 00:32:43,820
you know

555
00:32:45,810 --> 00:32:49,290
to terms that might just occur once in the whole corpus write things that are

556
00:32:49,290 --> 00:32:50,510
very improbable that

557
00:32:50,550 --> 00:32:54,780
you know it just doesn't seem to make sense so more

558
00:32:54,790 --> 00:32:56,740
a useful approach here

559
00:32:56,760 --> 00:33:02,400
it is to do smoothing by some form of interpolation where you actually

560
00:33:02,420 --> 00:33:07,450
use the document collection to come up with a reference distribution

561
00:33:07,560 --> 00:33:11,860
so when you look at the overall frequency of the word try to use some

562
00:33:11,860 --> 00:33:14,470
or all documents in your collection

563
00:33:14,760 --> 00:33:17,070
so that gives you the overall frequency

564
00:33:17,090 --> 00:33:21,580
if we to take that as a reference distribution and now

565
00:33:21,660 --> 00:33:27,970
and in the simplest case here right here just

566
00:33:27,980 --> 00:33:32,580
the model now this as a as a as a convex combination so your estimate

567
00:33:32,580 --> 00:33:34,680
now as a convex combination of the

568
00:33:34,760 --> 00:33:40,530
frequency right the maximum likelihood estimate and this reference distribution and you might have you

569
00:33:40,530 --> 00:33:45,140
know parameter lambda d for every document but in the simplest case

570
00:33:45,160 --> 00:33:46,520
that will actually

571
00:33:46,590 --> 00:33:51,660
you know you might just use it to be constant between zero and one

572
00:33:51,680 --> 00:33:53,540
OK so

573
00:33:54,150 --> 00:33:58,540
so basically so that's kind of the way of dealing with

574
00:33:58,550 --> 00:34:01,900
query terms that do not occur in the document right so then you're not getting

575
00:34:01,900 --> 00:34:05,510
zero probability but sort of you inherit you know the the

576
00:34:05,570 --> 00:34:09,390
the probability that part here from the reference distribution

577
00:34:10,710 --> 00:34:12,680
OK and then

578
00:34:12,700 --> 00:34:15,200
you know there are various ways that you can actually

579
00:34:15,990 --> 00:34:22,060
you know the parameters say using techniques like expectation maximisation algorithm

580
00:34:22,080 --> 00:34:25,110
because this is basically a two component mixture model

581
00:34:31,240 --> 00:34:33,370
there are other techniques

582
00:34:35,640 --> 00:34:40,800
actually works fairly well in practice is called absolute discounting

583
00:34:40,810 --> 00:34:47,340
where what you do is is first you subtract probability mass of delta

584
00:34:47,390 --> 00:34:49,950
from every nonzero count

585
00:34:49,950 --> 00:34:51,600
OK i see at most

586
00:34:51,630 --> 00:34:54,540
well if delta is less than one

587
00:34:54,570 --> 00:34:59,790
you can do OK and then you redistribute the probability mass

588
00:34:59,810 --> 00:35:01,390
that you've subtracted

589
00:35:01,400 --> 00:35:03,740
according to the reference distribution

590
00:35:03,750 --> 00:35:05,150
OK so

591
00:35:05,170 --> 00:35:08,890
it with the previously with the interpolation

592
00:35:11,140 --> 00:35:16,110
you would that you wouldn't look at what's the number of distinct terms that that's

593
00:35:16,110 --> 00:35:20,200
actually occurs in the document keywords here you know

594
00:35:20,270 --> 00:35:23,070
depending on how many distinct terms you have

595
00:35:23,120 --> 00:35:24,740
that's kind of

596
00:35:24,750 --> 00:35:26,800
you know you use that to

597
00:35:26,810 --> 00:35:32,100
generate some from set some probability mass aside and that gets distributed according to this

598
00:35:32,110 --> 00:35:35,580
distribution case the nice thing about this

599
00:35:35,630 --> 00:35:37,770
formula is actually that

600
00:35:38,190 --> 00:35:40,780
the redistribution

601
00:35:40,810 --> 00:35:44,160
will be proportional to the number of unique words

602
00:35:44,180 --> 00:35:46,090
in the document so

603
00:35:46,110 --> 00:35:49,760
by that i mean if you have a very long document compared to very short

604
00:35:51,820 --> 00:35:59,840
then it's probably true financial documents that the length of the documents increases asymptotically faster

605
00:35:59,840 --> 00:36:02,010
than the number of new words

606
00:36:02,020 --> 00:36:06,480
in the document is a document of links thousand might have a hundred different words

607
00:36:06,480 --> 00:36:09,950
by having you know of two thousand it might not have two hundred but it

608
00:36:09,950 --> 00:36:14,750
might grow slower rates among others for asymptotic so that means then in terms of

609
00:36:14,750 --> 00:36:15,950
the smoothing

610
00:36:15,970 --> 00:36:21,070
the smoothing gets automatically adapted to do more smoothing for longer documents

611
00:36:21,090 --> 00:36:23,510
and you do less moving four

612
00:36:23,510 --> 00:36:24,880
the show you

613
00:36:24,910 --> 00:36:29,040
so do less moving for long documents and you do more smoothing for the short

614
00:36:30,350 --> 00:36:35,230
which seems to be reasonable heuristic to use here

615
00:36:35,240 --> 00:36:40,060
right because you know very very long documents in the darkness a whole book about

616
00:36:40,060 --> 00:36:43,200
a particular term does not occur in the book

617
00:36:43,210 --> 00:36:49,180
well maybe it should be right but if i have a very short news article

618
00:36:49,210 --> 00:36:50,010
you know

619
00:36:50,020 --> 00:36:52,540
and a certain term does not occur well maybe

620
00:36:52,560 --> 00:36:56,110
it's just because of the constraint that

621
00:36:56,130 --> 00:36:58,270
all of the links constraint of the dark

622
00:37:01,780 --> 00:37:05,070
so the the final

623
00:37:05,140 --> 00:37:09,800
more that i want to talk about getting the

624
00:37:09,840 --> 00:37:13,310
model for a little bit

625
00:37:13,330 --> 00:37:17,540
better motivated in terms of statistical modeling

626
00:37:17,610 --> 00:37:20,490
another popular approach is

627
00:37:20,880 --> 00:37:23,340
to use the a hierarchical bayesian

628
00:37:26,970 --> 00:37:30,300
if we denote this probability

629
00:37:30,310 --> 00:37:33,740
that a certain term occurs in the document by theta d

630
00:37:33,740 --> 00:37:34,550
and then

631
00:37:34,580 --> 00:37:37,500
we've called

632
00:37:37,510 --> 00:37:38,340
it was

633
00:38:20,000 --> 00:38:23,940
this is a

634
00:38:25,810 --> 00:38:28,730
know what

635
00:38:28,740 --> 00:38:30,120
the station

636
00:38:30,150 --> 00:38:32,570
yes or no

637
00:38:57,530 --> 00:39:04,070
a s

638
00:39:36,800 --> 00:39:41,510
what happened

639
00:39:41,530 --> 00:39:45,320
you want

640
00:39:48,290 --> 00:39:53,900
it's not nothing

641
00:40:41,150 --> 00:40:45,460
i will

642
00:41:11,680 --> 00:41:12,600
these four

643
00:41:33,540 --> 00:41:36,570
so here

644
00:41:45,240 --> 00:41:47,640
on the

645
00:42:09,980 --> 00:42:17,530
here he

646
00:42:20,740 --> 00:42:25,560
when we

647
00:42:30,530 --> 00:42:45,250
what is

648
00:43:11,890 --> 00:43:16,730
and that

649
00:43:16,730 --> 00:43:22,600
how much pattern is there well that's roughly in over two bits of patterns

650
00:43:22,660 --> 00:43:26,430
on the other extreme is if you have been in which you get by independent

651
00:43:26,430 --> 00:43:27,740
art fair coin

652
00:43:27,820 --> 00:43:32,490
with very high probability it will have zero bits of pattern

653
00:43:32,520 --> 00:43:35,950
OK because the smallest program will be the same size

654
00:43:35,970 --> 00:43:37,740
so this is

655
00:43:37,770 --> 00:43:41,150
this is a a very simple domain in which you can

656
00:43:41,160 --> 00:43:46,940
you can define a very general notion of pattern

657
00:43:50,620 --> 00:43:56,560
the next question

658
00:43:56,570 --> 00:44:01,230
i'd like to give you an example

659
00:44:01,240 --> 00:44:06,850
of something that comes up in theoretical computer science it has no patterns

660
00:44:06,860 --> 00:44:08,220
in my examples

661
00:44:08,310 --> 00:44:10,130
is an elegant program

662
00:44:10,170 --> 00:44:17,420
so these are these are strings of the form of nanoparticles

663
00:44:17,460 --> 00:44:20,770
what an elegant program well i define elegant program

664
00:44:20,790 --> 00:44:25,950
to be programme which is the most concise programming that computer programming language you fix

665
00:44:25,950 --> 00:44:30,070
the computer programming language that produces the output that it does OK so this is

666
00:44:30,070 --> 00:44:31,470
the best theory

667
00:44:31,620 --> 00:44:36,670
if you view of computer programmers theory for its output and elegant program is the

668
00:44:36,670 --> 00:44:40,980
oak cancer rates are the most concise the simplest theory for what it

669
00:44:41,020 --> 00:44:46,680
for what it explains but to put it in computer science terms programming bell again

670
00:44:46,890 --> 00:44:50,640
p is elegant

671
00:44:50,680 --> 00:44:52,880
if you know

672
00:44:55,700 --> 00:44:57,780
that's smaller

673
00:44:57,870 --> 00:45:01,210
no program in the same language you understand you fix the language

674
00:45:01,420 --> 00:45:03,320
programming language that smaller

675
00:45:03,410 --> 00:45:10,450
produces the same output

676
00:45:10,540 --> 00:45:14,380
OK so the first thing to notice is there's always an elegant programs

677
00:45:14,390 --> 00:45:15,700
there may be several

678
00:45:15,730 --> 00:45:17,680
but for any computational task

679
00:45:17,750 --> 00:45:21,400
there are an infinity of programs and some of them had better be smaller and

680
00:45:21,400 --> 00:45:24,390
more concise than the others i don't care about time

681
00:45:27,090 --> 00:45:28,940
this is very theoretical

682
00:45:29,000 --> 00:45:36,120
so also you can read my handwriting which makes it even more difficult but i

683
00:45:36,120 --> 00:45:38,460
can really the actually

684
00:45:39,380 --> 00:45:41,180
OK so

685
00:45:41,230 --> 00:45:45,420
these turn out to be so to speak a practical example you know not very

686
00:45:46,860 --> 00:45:51,890
of something that has no pattern according to the definition i've just given y y

687
00:45:51,890 --> 00:45:55,500
y do well for the a lot of elegant programs are infinite infinity and their

688
00:45:55,500 --> 00:45:58,210
bit strings y is an elegant program have to be

689
00:46:00,010 --> 00:46:02,110
in the sense that i just

690
00:46:02,140 --> 00:46:05,630
well the reason is as follows

691
00:46:06,810 --> 00:46:09,160
a a

692
00:46:09,220 --> 00:46:11,670
if the program has pattern

693
00:46:12,660 --> 00:46:16,360
a more concise program that calculates the same output

694
00:46:17,500 --> 00:46:21,160
the smallest program that calculates the the

695
00:46:21,180 --> 00:46:22,680
original programmes

696
00:46:22,710 --> 00:46:23,890
so instead of

697
00:46:23,890 --> 00:46:26,560
taking the program and running it directly what i do

698
00:46:26,580 --> 00:46:30,130
if i take the most concise program that calculates

699
00:46:30,170 --> 00:46:33,040
the the the the program i'm interested in

700
00:46:33,100 --> 00:46:36,530
and then i produce that program as a lot of the calculation and then i

701
00:46:36,530 --> 00:46:40,180
run into rice running and then i produced the output of the original programmes

702
00:46:40,190 --> 00:46:41,570
is a so

703
00:46:41,580 --> 00:46:45,100
in other words if there is any structure in a computer program it cannot be

704
00:46:45,100 --> 00:46:49,570
the most concise program because i can sort of run it through compression program

705
00:46:49,610 --> 00:46:55,750
and and and then decompressor before running it and that gives me a smaller programs

706
00:46:55,790 --> 00:46:57,930
you see you have a little bit to the size of the program you that

707
00:46:58,250 --> 00:47:00,860
you have to add the decompression routine

708
00:47:01,780 --> 00:47:05,600
if the complete the program is compressed substantially the decompression routine is going to be

709
00:47:05,600 --> 00:47:07,730
fixed size if the program is so

710
00:47:07,750 --> 00:47:10,310
so the compressed substantially

711
00:47:10,350 --> 00:47:14,910
the compressed version of the program you want through LCD or something

712
00:47:14,920 --> 00:47:15,970
plus the

713
00:47:15,990 --> 00:47:19,910
the decompression algorithm which is fixed size is going to be smaller than the original

714
00:47:19,910 --> 00:47:24,440
programme so what you do is you know decompress it and then you run and

715
00:47:24,440 --> 00:47:28,440
you get the original output but you have a smaller programs you see so

716
00:47:28,530 --> 00:47:32,630
so this means that an elegant program you know every bit of elegant program has

717
00:47:32,630 --> 00:47:35,010
to be a complete complete surprise

718
00:47:35,070 --> 00:47:37,660
if it's not a complete surprise each successive but

719
00:47:37,670 --> 00:47:40,920
programme it means this is not the most concise programs

720
00:47:40,950 --> 00:47:44,420
because there was of structure in the program and then you make a macro or

721
00:47:44,420 --> 00:47:45,940
you do something to

722
00:47:46,010 --> 00:47:48,990
get rid of that structure you see that redundancy

723
00:47:49,010 --> 00:47:50,440
OK so

724
00:47:50,460 --> 00:47:55,340
so so these these are examples of elegant programs are so the speaker an example

725
00:47:55,340 --> 00:47:59,320
in the real world of something that has no pattern according to the definition i

726
00:47:59,320 --> 00:48:01,220
gave which is

727
00:48:01,280 --> 00:48:02,730
the idea that there

728
00:48:02,770 --> 00:48:07,720
there is no program that's more concise that calculates that object that object being itself

729
00:48:07,720 --> 00:48:10,620
computer program and elegant program

730
00:48:15,760 --> 00:48:19,890
i'm going much too fast i think

731
00:48:20,990 --> 00:48:25,500
it's not a problem

732
00:48:25,520 --> 00:48:28,710
unless you stop me and ask questions

733
00:48:28,730 --> 00:48:33,940
OK now i would like to go to another topic having given an example of

734
00:48:34,020 --> 00:48:36,640
something that has no patterns

735
00:48:36,640 --> 00:48:38,220
being intelligent program

736
00:48:38,250 --> 00:48:40,920
i'd like to

737
00:48:40,970 --> 00:48:45,330
i'm dealing with a very general notion of pattern it's an impractical notion of patterns

738
00:48:46,920 --> 00:48:47,680
you see

739
00:48:47,730 --> 00:48:49,010
my notion of pattern

740
00:48:49,020 --> 00:48:51,530
there is no limit on the time

741
00:48:53,760 --> 00:48:56,090
so for me the the pattern you see so

742
00:48:56,090 --> 00:48:57,430
so this is a very

743
00:48:57,450 --> 00:48:59,560
the theoretical notion of pattern

744
00:48:59,590 --> 00:49:06,030
but because it's so theoretical in general you can prove as strong theorem about it

745
00:49:06,080 --> 00:49:11,210
you know this the trade usually mathematics can be useful or can be beautiful

746
00:49:11,220 --> 00:49:16,090
and usually one or the other you can do both i'm forgive me for this

747
00:49:18,020 --> 00:49:20,970
this is enough to get you put in jail in some places to make mark

748
00:49:20,980 --> 00:49:26,080
like this but i'm theoretician so this is the

749
00:49:26,160 --> 00:49:30,090
OK so with this notion of pattern that i explained which is very very general

750
00:49:30,090 --> 00:49:32,870
based on the size of the smallest program really

751
00:49:33,240 --> 00:49:38,320
it turns out that we get these these surprising question if you're looking at the

752
00:49:38,320 --> 00:49:41,490
amount of pattern in a finite stripping

753
00:49:41,730 --> 00:49:45,340
if you're just looking at the amount of pattern individual

754
00:49:45,410 --> 00:49:49,950
individual bit strings

755
00:49:49,980 --> 00:49:54,690
if you just concerned with individual bit strings and you have the pattern isn't pattern

756
00:49:54,690 --> 00:49:57,420
and i remember i can measure how much partners

757
00:49:57,500 --> 00:49:58,230
you know

758
00:49:58,290 --> 00:50:02,080
from zero bits to the full number of bits in the string that's the maximum

759
00:50:02,080 --> 00:50:03,080
amount of

760
00:50:03,120 --> 00:50:05,890
so what if you're looking at individual bit strings

761
00:50:07,640 --> 00:50:11,910
you can another way to but it is you can always prove

762
00:50:12,000 --> 00:50:16,020
lower bounds on the amount of power

763
00:50:16,080 --> 00:50:18,980
if you're interested in the amount of pattern individual bitstream

764
00:50:19,020 --> 00:50:23,520
you can prove or lower bounds

765
00:50:23,600 --> 00:50:27,310
all you can prove or lower bounds on the amount of pattern in individual based

766
00:50:27,310 --> 00:50:29,480
on their interests

767
00:50:29,500 --> 00:50:38,640
however you can prove no upper bound on the amount of patterns

768
00:50:38,640 --> 00:50:40,460
has something like

769
00:50:40,600 --> 00:50:43,550
people were actually taken aback doesn't but

770
00:50:43,560 --> 00:50:45,160
some of the others do

771
00:50:45,170 --> 00:50:48,680
but for many of them they don't and so approximations and even the algorithm so

772
00:50:48,700 --> 00:50:52,130
much slower for example for generalized linear models

773
00:50:53,300 --> 00:50:55,360
there's no piecewise linearity

774
00:50:55,410 --> 00:51:01,040
and so for logistic regression for example the path of piecewise smooth and you have

775
00:51:01,040 --> 00:51:02,680
to do a lot of computations

776
00:51:02,810 --> 00:51:07,550
so the company and so the algorithm seize up for large problems

777
00:51:07,560 --> 00:51:10,320
so there is a need for efficient algorithms

778
00:51:10,570 --> 00:51:12,900
four for larger problems

779
00:51:12,920 --> 00:51:16,570
so this brings us to the topic of today coordinate descent

780
00:51:16,800 --> 00:51:19,610
the idea is to solve the lasso problem

781
00:51:19,650 --> 00:51:21,320
by coordinate descent

782
00:51:21,320 --> 00:51:26,140
which means optimise each parameter separately holding the others fixed

783
00:51:26,150 --> 00:51:27,040
now the

784
00:51:27,060 --> 00:51:30,950
it doesn't seem like it's going to lead to fast algorithms

785
00:51:30,960 --> 00:51:35,560
for solving systems of linear equations is the gauss seidel

786
00:51:35,580 --> 00:51:37,340
in the context of lasso

787
00:51:37,350 --> 00:51:42,480
it's something similar but you optimize in one corner to the times

788
00:51:42,520 --> 00:51:43,870
so why is this

789
00:51:43,930 --> 00:51:46,610
lead to an efficient algorithm

790
00:51:46,630 --> 00:51:49,780
convince you of this today

791
00:51:50,320 --> 00:51:55,080
and what do with it turns out that all the updates are trivial and that's

792
00:51:55,080 --> 00:51:57,570
one of the reasons it's very efficient

793
00:51:57,670 --> 00:52:01,220
and what we do is we do this on a grid of lambda values that's

794
00:52:01,220 --> 00:52:03,270
the regularisation parameter

795
00:52:03,320 --> 00:52:06,300
so we don't get the exact path but to get it on a finer grid

796
00:52:06,310 --> 00:52:07,540
as we choose

797
00:52:07,900 --> 00:52:11,010
starting from some lambda max down to lambda min

798
00:52:11,020 --> 00:52:13,560
and and we use warm start

799
00:52:13,570 --> 00:52:16,410
and it turns out that's very efficient

800
00:52:16,710 --> 00:52:20,560
what's really attractive you can do this with a variety of loss functions

801
00:52:20,570 --> 00:52:22,740
and and additive penalties

802
00:52:23,680 --> 00:52:27,080
you get this same kind of efficiency

803
00:52:27,120 --> 00:52:29,260
so coordinate descent

804
00:52:29,300 --> 00:52:36,370
achieves dramatic speedups over all competitors by factors of ten sometimes a hundred and more

805
00:52:36,430 --> 00:52:39,870
so the next few slides i'm and i'm going to show you know

806
00:52:40,330 --> 00:52:45,480
i'm going to demonstrate some of these these speedup results

807
00:52:45,500 --> 00:52:46,790
so use that same

808
00:52:46,800 --> 00:52:50,230
coefficient profile for the for the little example i had

809
00:52:50,240 --> 00:52:52,580
and we have a large profiles

810
00:52:52,640 --> 00:52:54,770
all the lines and the

811
00:52:54,770 --> 00:52:59,560
coordinate descent profile superimposed with dots and so you see there a hundred dots this

812
00:52:59,560 --> 00:53:01,770
so we approximate in the path

813
00:53:01,810 --> 00:53:06,170
as accurately as unity and we can even make the image fine for like

814
00:53:06,230 --> 00:53:07,750
without much loss in

815
00:53:10,650 --> 00:53:14,780
i'm going to show you some simulations and some and some performance results on real

816
00:53:16,020 --> 00:53:21,160
and then i'll tell you about all the details after so the competitors are laws

817
00:53:21,170 --> 00:53:24,530
algorithm is implemented in the r package

818
00:53:24,540 --> 00:53:26,820
for squared error loss

819
00:53:26,840 --> 00:53:28,810
eleven that's the package

820
00:53:28,810 --> 00:53:30,870
that implements coordinate ascent

821
00:53:30,930 --> 00:53:33,230
so that's the training base package

822
00:53:33,270 --> 00:53:34,730
which is

823
00:53:34,770 --> 00:53:36,560
the front end in or

824
00:53:36,580 --> 00:53:41,740
and it does squared error and logistic regression

825
00:53:43,430 --> 00:53:47,710
both for two and multiple class logistic regression

826
00:53:47,800 --> 00:53:51,990
there's no reason competitor was l one logreg

827
00:53:52,030 --> 00:53:54,970
that's the best two logistic regression package

828
00:53:54,980 --> 00:54:02,210
my colleagues in as in electrical engineering at stanford steveboyington two of his students

829
00:54:02,490 --> 00:54:04,390
cohen came

830
00:54:04,450 --> 00:54:06,840
and that came out a year ago

831
00:54:07,070 --> 00:54:12,560
using state-of-the-art interior point methods for convex optimisation and this is for two class logistic

832
00:54:14,850 --> 00:54:16,320
and then

833
00:54:16,370 --> 00:54:18,760
b b are in BMR are

834
00:54:18,770 --> 00:54:23,320
that's a sense for bayesian binomial multinomial regression

835
00:54:23,340 --> 00:54:25,160
this is the package biking can

836
00:54:25,170 --> 00:54:26,550
lewis and madigan

837
00:54:26,550 --> 00:54:32,850
now surprisingly they also use coordinate descent nodes bayesian actually go for the posterior mode

838
00:54:32,880 --> 00:54:36,300
using the class prior that amounts to

839
00:54:36,300 --> 00:54:39,210
lasso penalized logistic regression

840
00:54:39,230 --> 00:54:42,750
and so they use coordinate descent as well

841
00:54:43,610 --> 00:54:47,780
and this is an example where the devil's in the details because we have you'll

842
00:54:47,780 --> 00:54:52,220
see we have a dramatic speedups over them as well

843
00:54:52,260 --> 00:54:55,140
so let me show you the results

844
00:54:55,160 --> 00:54:56,170
so yes

845
00:54:56,180 --> 00:54:58,740
linear regression squared error loss

846
00:54:58,740 --> 00:55:01,860
dense since features in other words and bigger than

847
00:55:05,540 --> 00:55:06,980
forget in big p

848
00:55:06,990 --> 00:55:10,750
we cannot distinguish between sparse features in dense features so

849
00:55:10,760 --> 00:55:11,880
you see that if the

850
00:55:11,900 --> 00:55:15,610
features of past such as the bag of words model the features are sparse in

851
00:55:15,610 --> 00:55:19,320
and it sends back what is what the map

852
00:55:19,340 --> 00:55:22,840
estimate would be under the prior and that's the only thing you have to do

853
00:55:22,840 --> 00:55:29,060
right so it's quite i think it's kind of cool it's not not obvious

854
00:55:29,060 --> 00:55:33,770
OK so now there are some examples of start with a little baby one

855
00:55:33,770 --> 00:55:36,650
to see can see visually what it is and i am sure here i don't

856
00:55:36,650 --> 00:55:39,560
have to go into all of this so i'll make it very very fast enough

857
00:55:39,650 --> 00:55:42,710
you will be classification so

858
00:55:42,730 --> 00:55:47,400
you know i have a bunch of a billion outcomes and feature vectors and i

859
00:55:47,400 --> 00:55:50,670
form a margin my variables are WNV

860
00:55:50,670 --> 00:55:54,540
omega lost you can make anything you like

861
00:55:54,820 --> 00:55:58,500
any any the obvious options and you want to minimize an average

862
00:55:58,520 --> 00:56:02,710
on average loss plus a regularisation term that could be able to squared l one

863
00:56:02,710 --> 00:56:04,000
whatever you like

864
00:56:04,000 --> 00:56:06,230
exponential anything like that

865
00:56:06,250 --> 00:56:12,130
another then you the idea is is split data and use ADMM consensus to solve

866
00:56:12,130 --> 00:56:16,880
will look at a really super small example just one that you don't expect you

867
00:56:16,880 --> 00:56:21,340
don't even need to shouldn't be doing it but anyway people was look at it

868
00:56:21,360 --> 00:56:26,210
there are two features and four hundred examples but so i can plot right

869
00:56:26,230 --> 00:56:29,320
and i will split the four hundred examples into twenty groups but we do it

870
00:56:29,320 --> 00:56:33,590
in a sick way each group of twenty has only seen either all positive or

871
00:56:33,590 --> 00:56:35,420
all negative examples

872
00:56:35,440 --> 00:56:40,730
right so so it's the most scuba so each box of twenty examples has seen

873
00:56:40,730 --> 00:56:44,400
all spam all my right so that that's how it works so so they're going

874
00:56:44,420 --> 00:56:48,190
have to collaborate to two i mean but probably the right way to do this

875
00:56:48,190 --> 00:56:51,360
actually is to hash the data in which case each of them by the time

876
00:56:51,380 --> 00:56:54,810
by themselves actually gets a pretty good estimate and the only thing to do by

877
00:56:54,840 --> 00:56:57,090
collaborating is get statistical power

878
00:56:57,110 --> 00:56:59,880
right so this is not that this is just for the fish to to show

879
00:56:59,880 --> 00:57:01,090
the works

880
00:57:01,210 --> 00:57:05,000
OK so let's see how this works

881
00:57:05,040 --> 00:57:09,520
well these are all the private local estimates of course there are terrible because they've all been

882
00:57:11,730 --> 00:57:15,540
they've all been given terrible ideas of what the data looks like is able only

883
00:57:15,540 --> 00:57:20,060
seen all possible role negative things about the way the reason not all way off

884
00:57:20,060 --> 00:57:23,540
in the middle of nowhere because the quadratic regularisation terms OK

885
00:57:23,900 --> 00:57:28,490
the actually embarrassingly after one step which consists of averaging all of those you don't

886
00:57:28,590 --> 00:57:31,790
get some is not so bad right so

887
00:57:31,840 --> 00:57:33,110
if you then

888
00:57:34,150 --> 00:57:37,790
i ADMM like five steps you get to something like this and you can can

889
00:57:37,810 --> 00:57:41,920
you get the picture right so that's what happens after forty steps right hand but

890
00:57:41,920 --> 00:57:45,270
the interesting thing here is just to this to visualize actually the data flow in

891
00:57:45,270 --> 00:57:49,610
the communication here each box has seen like europe box

892
00:57:49,610 --> 00:57:55,340
you've seen only positive examples you have absolutely no idea who else has an opinion

893
00:57:55,340 --> 00:58:00,860
about about this problem will collaborating with to former class which you know nothing the

894
00:58:00,860 --> 00:58:04,480
only thing you have is a protocol that implements map

895
00:58:05,860 --> 00:58:09,230
that's it and what happens is they come back and they say well

896
00:58:09,250 --> 00:58:12,750
the weight vector prior looks like this what would you do that and you say

897
00:58:12,750 --> 00:58:17,380
well it's very strange is highly consistent with my data frankly but OK whatever and

898
00:58:17,400 --> 00:58:22,920
returns w and then actually when these things come to convergence it means the use

899
00:58:22,920 --> 00:58:26,770
of the global problem each box has absolutely no idea

900
00:58:26,790 --> 00:58:30,540
how many others there are what they look like anything i mean so i don't

901
00:58:30,540 --> 00:58:34,250
make too big a deal about it but it's worth thinking about what actually happens

902
00:58:36,920 --> 00:58:38,860
let's look at another one

903
00:58:38,880 --> 00:58:41,040
the distributed lasso example

904
00:58:41,040 --> 00:58:44,170
and this is just made this is just for fun just to show that you

905
00:58:44,170 --> 00:58:49,420
can do this so this is probably bigger than anyone would ever solved so it's

906
00:58:49,610 --> 00:58:56,360
dense problem with four hundred thousand four four hundred thousand examples in a thousand regressors

907
00:58:56,360 --> 00:58:58,650
or something like that

908
00:58:58,670 --> 00:59:04,090
and it's about thirty to get back to that it really people like would we

909
00:59:04,110 --> 00:59:08,000
routinely solve problems with this amount of data for sparse problems this one is not

910
00:59:08,000 --> 00:59:10,750
sparse or dense so anyway i

911
00:59:10,810 --> 00:59:14,110
probably somebody has solve problems like this are right

912
00:59:14,130 --> 00:59:16,270
so just to do something real simple

913
00:59:16,270 --> 00:59:22,520
you take this is split split the data into a eighty subsystems that's ten core

914
00:59:22,520 --> 00:59:24,520
machines write something

915
00:59:24,540 --> 00:59:25,920
really simple

916
00:59:26,040 --> 00:59:28,820
not totally unoptimized extremely short

917
00:59:28,960 --> 00:59:32,520
just the shortest think about possibly have and then they are just to give you

918
00:59:32,520 --> 00:59:36,210
a rough idea of what this does it does something like this factorisation by the

919
00:59:36,210 --> 00:59:39,210
way is a that's parallel

920
00:59:39,230 --> 00:59:44,170
ridge regression statistically that's what you're doing you're doing a parallel ridge regression and it

921
00:59:44,170 --> 00:59:45,590
takes about five minutes

922
00:59:45,610 --> 00:59:49,540
subsequent iterations take about a second so again it's totally irrelevant how many steps you

923
00:59:49,540 --> 00:59:53,900
take i mean i totally irrelevant but it's it's certainly it's not as interesting is

924
00:59:54,190 --> 00:59:58,040
arguing about whether some takes twenty steps or a hundred total now that's that's kind

925
00:59:58,040 --> 01:00:03,230
of irrelevant and the total allows all these things like five minutes by the way

926
01:00:03,230 --> 01:00:08,110
this makes a very nice story right so there was a story

927
01:00:08,150 --> 01:00:12,340
you could say about interior point methods that would go like this

928
01:00:12,750 --> 01:00:15,860
the story would say so to say what are you doing so when solving convex

929
01:00:15,860 --> 01:00:20,540
optimization problem and you you so great indeed profile code and find out that in

930
01:00:20,540 --> 01:00:22,920
fact with they really doing is twenty

931
01:00:22,940 --> 01:00:26,060
for twenty times their solving least squares system

932
01:00:27,170 --> 01:00:30,230
you know that's what it is that that's that's one to point method does it

933
01:00:30,230 --> 01:00:34,980
solves the KKT system like twenty times thirty times whenever something like that so solving

934
01:00:34,980 --> 01:00:35,980
least squares

935
01:00:36,000 --> 01:00:39,880
right so someone could say well why would you do this convex optimisation you say

936
01:00:39,880 --> 01:00:42,000
well it's much more expressive

937
01:00:42,000 --> 01:00:43,640
is the bipartite graph

938
01:00:43,650 --> 01:00:48,380
we're on one side we have a bunch of products on the other we have

939
01:00:49,190 --> 01:00:51,730
and we have the links

940
01:00:51,730 --> 01:00:53,740
indicating whether

941
01:00:54,630 --> 01:00:56,640
users prefer

942
01:00:56,650 --> 01:01:00,920
some products and to what degree

943
01:01:00,940 --> 01:01:02,120
OK so

944
01:01:02,130 --> 01:01:08,050
if we consider them in matrix form you're a horizontally we consider users and vertically

945
01:01:08,080 --> 01:01:14,240
consider items or products and readings here represents how much

946
01:01:14,270 --> 01:01:17,840
a user prefers products

947
01:01:18,910 --> 01:01:19,890
also the

948
01:01:19,900 --> 01:01:24,330
we have many missing links and then the missing ratings here

949
01:01:24,330 --> 01:01:25,960
are represented as

950
01:01:25,980 --> 01:01:27,480
the question marks

951
01:01:27,500 --> 01:01:28,730
OK so

952
01:01:28,760 --> 01:01:29,870
algorithm y

953
01:01:29,900 --> 01:01:33,780
we have initially you know in the history of

954
01:01:33,780 --> 01:01:39,900
recommendation systems we have user base which is to consider similarity between users and to

955
01:01:39,900 --> 01:01:42,440
group similar users together

956
01:01:42,460 --> 01:01:49,850
and semantically we can use and base and more recently people adopt model based it

957
01:01:49,870 --> 01:01:53,110
turns of model based methods are

958
01:01:55,490 --> 01:02:00,980
formerly are in fact factorisation methods matrix factorisation methods

959
01:02:01,570 --> 01:02:04,550
now so what i try to focus on here

960
01:02:04,600 --> 01:02:12,070
is that in recommendation systems we often have the problem of data sparsity

961
01:02:12,070 --> 01:02:16,980
OK which is why we don't have much readings to start with we want to

962
01:02:16,980 --> 01:02:19,040
train the model

963
01:02:19,050 --> 01:02:20,960
and applied to the test data

964
01:02:20,990 --> 01:02:22,240
the performance

965
01:02:22,270 --> 01:02:28,560
can be dramatically reduced if we have a very sparse matrix to start with

966
01:02:28,580 --> 01:02:31,280
OK so cold start problem

967
01:02:31,290 --> 01:02:37,950
data sparsity problem these terms all refer to this drop in performance and this is

968
01:02:37,950 --> 01:02:40,410
exactly the the thing we try to

969
01:02:40,440 --> 01:02:42,350
we try to address

970
01:02:43,640 --> 01:02:49,090
we we addressed that from a transfer learning perspective is to consider this suppose we

971
01:02:49,090 --> 01:02:51,670
have some new

972
01:02:51,690 --> 01:02:55,510
product line fall selling books where

973
01:02:55,590 --> 01:02:58,540
we want to recommend books people

974
01:02:58,580 --> 01:03:03,180
but we don't have much data about books because maybe it's a new domain maybe

975
01:03:03,180 --> 01:03:05,370
they are about new users

976
01:03:05,390 --> 01:03:06,600
OK so

977
01:03:06,610 --> 01:03:12,040
then where we can look at is some similar domains may be on different products

978
01:03:12,040 --> 01:03:14,380
maybe on movies

979
01:03:14,390 --> 01:03:21,390
and because of the similarity or inherent similarity between either the product or the users

980
01:03:22,330 --> 01:03:27,500
is it possible for us to transfer some knowledge tool into the target domain to

981
01:03:28,340 --> 01:03:34,980
alleviate the data sparsity problem OK that's the question OK is it possible

982
01:03:36,600 --> 01:03:39,810
with this in mind we did a number of

983
01:03:39,810 --> 01:03:42,370
experiments try to

984
01:03:42,490 --> 01:03:46,410
there's ways in which the auxiliary domain can help

985
01:03:46,450 --> 01:03:52,270
OK so this first work is codebook transfer and believe sitting there

986
01:03:52,270 --> 01:03:54,240
a yellow shirt he's

987
01:03:54,250 --> 01:03:57,980
so the first author here

988
01:03:57,990 --> 01:04:02,120
now so let me let me give you a brief description of this

989
01:04:02,140 --> 01:04:05,690
and being can answer questions thereafter

990
01:04:05,700 --> 01:04:07,330
OK so

991
01:04:07,340 --> 01:04:09,100
if you look at

992
01:04:09,110 --> 01:04:14,140
if you look at the matrix representation of of the rating record

993
01:04:14,190 --> 01:04:17,780
one of the domain we often consider it as

994
01:04:18,910 --> 01:04:24,690
you know because the user group and and group are very large we can use

995
01:04:24,690 --> 01:04:31,090
usually consider it as consisting of a group of users that have similar tastes and

996
01:04:31,100 --> 01:04:33,540
a group of atoms that have similar

997
01:04:35,560 --> 01:04:37,240
OK so if you can

998
01:04:37,270 --> 01:04:45,030
group them together then we can compose a compressed version of the rating matrix which

999
01:04:45,030 --> 01:04:50,490
can be more compactly used to represent the original matrix

1000
01:04:50,500 --> 01:04:57,060
OK which is inside each cell is a small version of the reader

1001
01:04:57,080 --> 01:04:59,790
and we can call that a cookbook

1002
01:05:00,450 --> 01:05:05,530
suppose we have a dense enough matrix then we can build a very good codebook

1003
01:05:05,540 --> 01:05:07,990
the question is can this codebook

1004
01:05:08,930 --> 01:05:12,840
so solve the the cold start problem for some other

1005
01:05:12,850 --> 01:05:14,910
it is it's OK so

1006
01:05:14,920 --> 01:05:20,270
if you have a matrix say in movies where we have many more ratings missing

1007
01:05:20,270 --> 01:05:24,250
as compared to a bookstore domain then perhaps

1008
01:05:24,250 --> 01:05:26,670
if he align their groups

1009
01:05:26,700 --> 01:05:32,880
you know the correct way OK that group and the other group then we can

1010
01:05:32,920 --> 01:05:38,530
because of their inherent similarity and the relatedness we can use the dense virgin to

1011
01:05:38,530 --> 01:05:44,410
help the sparse version his that's the intuition behind this work

1012
01:05:44,410 --> 01:05:50,580
part of the function which lies outside of minus over to the plus over to

1013
01:05:52,060 --> 01:05:57,350
OK so the next thing that we did then was to show that the truncated

1014
01:05:58,730 --> 01:06:00,930
in the sink weighted sinusoids

1015
01:06:00,940 --> 01:06:02,540
both in fact spend

1016
01:06:02,590 --> 01:06:03,460
l two

1017
01:06:03,490 --> 01:06:05,630
in other words any function

1018
01:06:05,660 --> 01:06:07,190
and l two

1019
01:06:07,240 --> 01:06:14,120
after you represented in terms of that series what you have left over has to

1020
01:06:14,120 --> 01:06:19,600
be orthogonal in other words if you take this entire set

1021
01:06:19,780 --> 01:06:23,190
truncated sinusoids

1022
01:06:23,220 --> 01:06:26,060
every nonzero function

1023
01:06:26,080 --> 01:06:29,030
is orthogonal to all of them

1024
01:06:29,040 --> 01:06:33,370
every nonzero function as l two equivalent sense

1025
01:06:34,280 --> 01:06:37,430
so so in fact the thing you get out of this

1026
01:06:37,480 --> 01:06:39,690
this is part of

1027
01:06:39,710 --> 01:06:43,330
looking at functions which we always ignored

1028
01:06:43,350 --> 01:06:47,940
OK there is another issue that we ignored when we're looking at functions

1029
01:06:47,960 --> 01:06:50,240
o which comes out of this

1030
01:06:50,250 --> 01:06:54,920
since sensitivity by assumption is in l two vector

1031
01:06:54,970 --> 01:06:58,060
in other words in l two function aside from this

1032
01:06:58,110 --> 01:07:00,150
l two equivalent staff

1033
01:07:00,180 --> 01:07:03,850
and face the band personnel two vector also

1034
01:07:03,850 --> 01:07:06,500
this inner product has to be finite

1035
01:07:06,520 --> 01:07:09,170
by the shorts and quality

1036
01:07:09,180 --> 01:07:14,220
OK how do we get around this we are dealing with fourier series anybody remember

1037
01:07:14,290 --> 01:07:18,040
of course you remember i hardly remember that

1038
01:07:18,060 --> 01:07:22,990
the way we got around it was by saying that the

1039
01:07:23,020 --> 01:07:28,420
that is orthonormal function which was a truncated sinusoid

1040
01:07:28,440 --> 01:07:33,380
he was was truncated to minus over two and cluster over two and the waveform

1041
01:07:33,380 --> 01:07:34,460
was just the

1042
01:07:34,480 --> 01:07:37,940
to the i two pi after

1043
01:07:37,990 --> 01:07:43,790
the magnitude of that function was always less than or equal to one

1044
01:07:44,580 --> 01:07:48,480
and because the magnitude was always less than or equal to one

1045
01:07:48,490 --> 01:07:50,350
you could just take the integral

1046
01:07:50,360 --> 01:07:51,770
of the of two

1047
01:07:51,780 --> 01:07:56,290
time space abandoned today and you can show directly to that in the world always

1048
01:07:57,590 --> 01:08:00,830
because this special property of the sinusoids

1049
01:08:00,870 --> 01:08:05,290
here what we're saying is you don't have to worry about that anymore

1050
01:08:05,310 --> 01:08:08,580
you can use arbitrary l two functions

1051
01:08:08,600 --> 01:08:11,710
as the confidence of an l two

1052
01:08:11,730 --> 01:08:14,050
orthonormal expansion

1053
01:08:14,300 --> 01:08:16,110
and it still works

1054
01:08:16,120 --> 01:08:19,700
so every one of these things has to be finite also

1055
01:08:19,720 --> 01:08:22,250
so in fact we're buying something out of this

1056
01:08:22,270 --> 01:08:26,110
at the end of the notes on lectures eight to ten

1057
01:08:26,150 --> 01:08:29,030
you will notice something that

1058
01:08:29,090 --> 01:08:32,230
and i'm certainly not going to hold you responsible for

1059
01:08:32,240 --> 01:08:37,310
it's something called the prolate spheroidin expansion

1060
01:08:37,320 --> 01:08:43,390
and it's just given their primarily as one more example of an orthonormal expansion

1061
01:08:43,730 --> 01:08:48,690
it's a very interesting orthonormal expansion because it has the property

1062
01:08:48,700 --> 01:08:52,860
that if you start out asking

1063
01:08:52,880 --> 01:08:56,480
how much energy can i concentrate within

1064
01:08:56,580 --> 01:09:00,310
a fixed time interval and affect spend with

1065
01:09:00,320 --> 01:09:04,300
OK which is one of the things which make this whole subject a little bit

1066
01:09:04,300 --> 01:09:06,430
fishy and the whole

1067
01:09:06,440 --> 01:09:12,610
and certainly very very messy if i start off with the time limit function

1068
01:09:12,660 --> 01:09:17,110
and go through the fourier series these truncated sinusoids

1069
01:09:17,120 --> 01:09:18,650
spiller energy

1070
01:09:18,660 --> 01:09:20,070
the band

1071
01:09:22,270 --> 01:09:26,950
in fact one of the problems at the end of lectures eight to ten

1072
01:09:27,300 --> 01:09:31,520
in fact carries you through the process of just how much energy

1073
01:09:31,550 --> 01:09:38,400
you can have outside banned by using these truncated sinusoid because truncated sinusoid when you

1074
01:09:38,400 --> 01:09:43,210
look at them in a in a fourier series have a lot of energy outside

1075
01:09:43,210 --> 01:09:46,050
of work where it is where it should be

1076
01:09:46,060 --> 01:09:47,910
so this

1077
01:09:47,930 --> 01:09:54,280
so this particular set of prolates parietal function gives you the answer to the following

1078
01:09:56,160 --> 01:09:59,070
i would like to find that function

1079
01:09:59,090 --> 01:10:04,070
which is limited to minor city over two plus three over two

1080
01:10:04,090 --> 01:10:08,550
which has the largest amount of energy larger fraction of its energy

1081
01:10:08,590 --> 01:10:11,990
within the band minus w to plus w

1082
01:10:12,060 --> 01:10:13,690
what is that function

1083
01:10:13,730 --> 01:10:18,850
well i found it happens to be the senior water prolates toroidal functions

1084
01:10:18,850 --> 01:10:20,380
okay so thank you for coming back

1085
01:10:20,880 --> 01:10:21,910
and we talked about

1086
01:10:22,970 --> 01:10:24,490
statistical learning theory yesterday

1087
01:10:26,380 --> 01:10:27,640
just some basic ideas

1088
01:10:28,870 --> 01:10:30,730
i think you're probably

1089
01:10:31,400 --> 01:10:33,460
understood so you should have understood

1090
01:10:34,350 --> 01:10:35,420
so what is it

1091
01:10:35,550 --> 01:10:37,260
the complexity of a function class

1092
01:10:38,290 --> 01:10:39,550
so when we do machine learning

1093
01:10:40,150 --> 01:10:40,700
we have some

1094
01:10:41,150 --> 01:10:41,770
the function class

1095
01:10:42,600 --> 01:10:45,980
we choose a function from based on empirical data and we want this function to

1096
01:10:47,000 --> 01:10:51,230
be such that has a small risk small risk means uh we have some loss

1097
01:10:51,230 --> 01:10:54,280
function we compute the expectation about loss function over

1098
01:10:54,880 --> 01:10:58,730
an unknown but fixed distribution which generates both training and test data

1099
01:11:00,390 --> 01:11:01,740
we want the expectation that

1100
01:11:02,240 --> 01:11:03,240
loss function to be small

1101
01:11:04,020 --> 01:11:09,330
so from all classes functions we want to choose a function such that this expectation is as small as possible

1102
01:11:11,420 --> 01:11:12,350
it turns out that

1103
01:11:12,770 --> 01:11:15,220
this is not trivial we cannot simply are

1104
01:11:16,530 --> 01:11:21,650
we can simply choose the function based on which one has the lowest training error and then

1105
01:11:23,030 --> 01:11:24,730
some other conditions are satisfied

1106
01:11:25,510 --> 01:11:28,850
and these conditions are related to the complexity of function classes

1107
01:11:29,400 --> 01:11:30,560
so it turns out that we have

1108
01:11:32,950 --> 01:11:33,370
below below

1109
01:11:34,380 --> 01:11:35,420
if we have a function class

1110
01:11:36,470 --> 01:11:37,400
also we see dimension

1111
01:11:38,070 --> 01:11:39,040
is finite then

1112
01:11:39,770 --> 01:11:41,750
within in the limit is always be able

1113
01:11:44,740 --> 01:11:46,820
uh be consistent so to get there

1114
01:11:47,940 --> 01:11:52,260
together risk which is as low as the lowest possible risk and of function class

1115
01:11:54,130 --> 01:11:59,490
it's a single number that characterizes starting from which number of observations

1116
01:12:00,670 --> 01:12:01,290
learning machine

1117
01:12:01,780 --> 01:12:05,310
no longer has full complexity of full capacity so up to

1118
01:12:06,000 --> 01:12:09,240
number of observations which possibly see dimension the learning machine

1119
01:12:09,780 --> 01:12:13,950
well the cluster functions is as complicated as it could be so you can realize

1120
01:12:13,950 --> 01:12:16,000
all possible separations between the data

1121
01:12:16,920 --> 01:12:18,850
end afterwards it can't be any more

1122
01:12:19,680 --> 01:12:21,840
and if we can explain all the time

1123
01:12:23,500 --> 01:12:25,360
we can explain the large set of data are

1124
01:12:25,830 --> 01:12:28,310
with the learning machine that has finite dimension

1125
01:12:29,040 --> 01:12:29,880
this dimension is

1126
01:12:30,410 --> 01:12:32,950
you can significantly smaller than the number of data points

1127
01:12:33,670 --> 01:12:34,160
and then

1128
01:12:34,870 --> 01:12:39,310
this cannot be chance or is is unlikely to be chance that we can explain the data well

1129
01:12:39,970 --> 01:12:41,700
because we're working with a learning machine

1130
01:12:42,350 --> 01:12:47,140
who's we dimension is such that it usually cannot explain arbitrary large datasets

1131
01:12:47,610 --> 01:12:48,530
so that's the basic idea

1132
01:12:49,570 --> 01:12:53,440
and in the end it comes down to find a good function class and if we want to

1133
01:12:53,880 --> 01:12:54,420
learn well

1134
01:12:55,560 --> 01:12:55,920
and dr

1135
01:12:57,380 --> 01:12:59,290
good means the complexity should be

1136
01:13:01,070 --> 01:13:02,790
ideally we should be able to

1137
01:13:03,190 --> 01:13:06,730
give some bones and complexity to understand what controls the complexity

1138
01:13:07,560 --> 01:13:08,800
how to logarithmically

1139
01:13:09,400 --> 01:13:10,090
minimize it

1140
01:13:10,570 --> 01:13:11,420
end of course

1141
01:13:13,040 --> 01:13:16,960
it's not just about having small complexity but at the same time the function class should be able to

1142
01:13:17,650 --> 01:13:20,530
explain real world that has such a big complex enough to

1143
01:13:21,260 --> 01:13:24,600
model the kind of tim dependencies that we want to model in the real world

1144
01:13:25,540 --> 01:13:27,490
so that's certainly a trade-off

1145
01:13:29,190 --> 01:13:31,910
i started introducing this kernel function classes

1146
01:13:32,950 --> 01:13:37,110
it's nice examples where we can give bounds on the capacity

1147
01:13:38,490 --> 01:13:39,600
say more about this later

1148
01:13:40,430 --> 01:13:40,920
even though

1149
01:13:41,360 --> 01:13:41,790
there are

1150
01:13:42,340 --> 01:13:44,140
and complex classes of functions

1151
01:13:44,810 --> 01:13:48,870
and because they correspond to the linear function classes in some other space and we

1152
01:13:48,980 --> 01:13:51,150
can do the theoretical analysis in the space

1153
01:13:52,640 --> 01:13:53,360
that was the pretty

1154
01:13:54,560 --> 01:13:54,850
and the

1155
01:13:55,660 --> 01:13:56,470
and i showed u

1156
01:13:57,840 --> 01:13:58,170
a few

1157
01:13:59,580 --> 01:14:03,310
give you an introduction to kernels and feature spaces so i was saying

1158
01:14:03,830 --> 01:14:05,140
we preprocess the data

1159
01:14:05,780 --> 01:14:07,170
and then we look at some examples

1160
01:14:07,810 --> 01:14:12,190
wearing the hat emissions space we compute dot products and we find out that they correspond to

1161
01:14:13,390 --> 01:14:15,780
simple functions in the low dimensional space

1162
01:14:16,500 --> 01:14:17,560
which we call kernels

1163
01:14:18,870 --> 01:14:22,350
so this was an example of a kernel this kernel equals the

1164
01:14:22,980 --> 01:14:23,910
the deed poll

1165
01:14:24,410 --> 01:14:25,020
of the

1166
01:14:25,540 --> 01:14:27,570
usually the product taken in input space

1167
01:14:28,390 --> 01:14:29,310
and it computes the

1168
01:14:30,110 --> 01:14:35,110
all possible products of these features of of decoding it's in the high dimensional space

1169
01:14:36,890 --> 01:14:42,470
so it computes all possible products of decoding it's in this uh constitutes a mapping into the high dimensional space

1170
01:14:44,200 --> 01:14:46,450
when we look at mentioned mercer's theorem

1171
01:14:47,960 --> 01:14:48,470
i mentioned

1172
01:14:48,970 --> 01:14:51,070
the class of positive definite kernels

1173
01:14:52,320 --> 01:14:54,080
so these are this symmetric functions

1174
01:14:54,900 --> 01:14:55,490
two variables

1175
01:14:56,650 --> 01:14:57,920
now we a bit of laser pointer

1176
01:15:01,920 --> 01:15:02,990
functions of two variables

1177
01:15:03,680 --> 01:15:04,910
and symmetric functions

1178
01:15:05,430 --> 01:15:07,550
such that if you compute the kernel matrix

1179
01:15:08,200 --> 01:15:12,290
which we get by only substituting the finite dataset in here and here

1180
01:15:13,240 --> 01:15:15,130
this matrix is positive definite

1181
01:15:16,120 --> 01:15:19,280
and this matrix is called the gram matrix or kernel matrix

1182
01:15:22,900 --> 01:15:23,280
we then

1183
01:15:24,950 --> 01:15:26,470
i think we also covered this space

1184
01:15:28,300 --> 01:15:30,410
give examples of kernels

1185
01:15:31,110 --> 01:15:32,000
and dimension that

1186
01:15:33,000 --> 01:15:35,620
kernels have this benefit that we can

1187
01:15:36,400 --> 01:15:38,690
construct nonlinear algorithms from linear ones

1188
01:15:40,040 --> 01:15:41,660
it maybe more importantly we can

1189
01:15:42,270 --> 01:15:46,530
apply linear methods to data that don't leave in a vector space to begin with

1190
01:15:47,180 --> 01:15:50,490
as long as we can construct a kernel on this data so we have some

1191
01:15:50,490 --> 01:15:52,330
data which is from some arbitrary domain

1192
01:15:53,090 --> 01:15:55,970
and maybe even just a set of a set of graphs so strings

1193
01:15:56,640 --> 01:16:03,640
as long as we able to construct a similarity measure okay are which satisfies these properties of positive definiteness

1194
01:16:04,410 --> 01:16:07,430
then we get a vector space representation of the state of a free

1195
01:16:11,970 --> 01:16:12,790
so i am

1196
01:16:14,660 --> 01:16:16,040
so i was going to do today

1197
01:16:16,500 --> 01:16:19,150
my plan is to show you the feature map uh

1198
01:16:19,500 --> 01:16:21,520
of a positive definite kernel is very hard to

1199
01:16:21,520 --> 01:16:27,970
meaning that it should work reasonably for most of the signals that contain oscillation least

1200
01:16:27,970 --> 01:16:33,300
because the model consensus selection so we have this reason label frequency estimates that quite

1201
01:16:33,300 --> 01:16:37,920
robust on the black box you don't have to turn the nob slight what time

1202
01:16:37,920 --> 01:16:41,420
window do you know what do you want to which way you want to use

1203
01:16:41,420 --> 01:16:46,260
which scales we want to compute just press about them and get the result and

1204
01:16:46,260 --> 01:16:51,660
not only did the time-frequency relevant remember that behind each of those blocks are exact

1205
01:16:51,660 --> 01:16:55,220
parameters of the Garber functions that were used to compute

1206
01:16:56,680 --> 01:16:59,040
and we shall explore this feature

1207
01:17:00,380 --> 01:17:09,140
no we get back to the pool craftsmen craftsmen the of encephalographic hostility between the

1208
01:17:09,190 --> 01:17:14,280
let's take the example of sleep analysis if you think about sleep recordings the in

1209
01:17:14,280 --> 01:17:19,500
the old days when it was on paper it was over how the kilometres long

1210
01:17:19,550 --> 01:17:24,680
with of paper and to make a non-GM that is used for the diagnosis the

1211
01:17:24,680 --> 01:17:28,440
doctor has to go through this House has work and this 1 has to work

1212
01:17:28,440 --> 01:17:32,860
at least 1 kilometer because he has to go through this uh this recording this

1213
01:17:32,860 --> 01:17:38,500
mortgage our recording and twice stone tool tools you get back into integration such etc.

1214
01:17:39,280 --> 01:17:44,840
so what what is he doing actually the encephalographic

1215
01:17:45,480 --> 01:17:52,400
encephalographic stated by provision my appears in the of art and and amount of science

1216
01:17:52,400 --> 01:17:56,890
in the game and the science is by definition that leaves us with is believed

1217
01:17:56,890 --> 01:17:59,200
that science is based upon repeatable

1218
01:17:59,370 --> 01:18:04,760
experiments if something is repeatable maybe crystals logarithmic and what we want to do is

1219
01:18:04,760 --> 01:18:09,870
to extract the algorithmic part of not to replace the encephalographic just to free them

1220
01:18:09,870 --> 01:18:16,060
from their repeatable model that can be taken over to combine a computer so we

1221
01:18:16,060 --> 01:18:24,520
take a look at the definitions of sleep stages and structures that for synthesis 1968

1222
01:18:25,160 --> 01:18:31,060
the standardization for standardization of since stage that was published so all anaphora quite a

1223
01:18:31,060 --> 01:18:36,600
few years we know for instance that they work and encephalographic consider sleep spindles is

1224
01:18:36,600 --> 01:18:45,170
actually very well defined structure and the definition is exactly in time frequency terms if

1225
01:18:45,170 --> 01:18:51,950
we assume that that if we are only able to translate cycles per 2nd into

1226
01:18:51,960 --> 01:18:57,260
credits because there is is more or less obvious similarly

1227
01:18:58,240 --> 01:19:06,160
more or less more or less that easy we can go with other structures like

1228
01:19:06,180 --> 01:19:08,460
like for instance slow waves

1229
01:19:09,240 --> 01:19:13,780
and we put together the automatic detection

1230
01:19:15,000 --> 01:19:21,960
all features if we need to find out that these structures which are fitted to

1231
01:19:21,960 --> 01:19:24,220
the signal in the general procedure

1232
01:19:24,660 --> 01:19:32,180
can correspond to for example sleep spindles slow-wave then it's just as easy as taking

1233
01:19:32,180 --> 01:19:39,620
these parameters right the future on top of the matching pursuit decomposition which is not

1234
01:19:39,620 --> 01:19:45,150
computed with a particular idea of making sleep analysis is a general parameterization and out

1235
01:19:45,170 --> 01:19:49,440
of these parameters without looking at the time frequency map and saying OK maybe there's

1236
01:19:49,440 --> 01:19:55,800
something here this is the planes very simple filter that has only few conditions mentally

1237
01:19:55,810 --> 01:20:00,180
get an automatic and I'd say quite reliable

1238
01:20:01,400 --> 01:20:07,960
detection and the parameterization of say sleep spindles and slow waves how it goes this

1239
01:20:07,960 --> 01:20:13,440
is example for example for example this example of how far away if there is

1240
01:20:13,440 --> 01:20:14,420
a blob

1241
01:20:14,940 --> 01:20:21,680
in frequency about 1200 right maybe it's not part of our that's probably sleep spindles

1242
01:20:22,080 --> 01:20:27,000
but there were probably and the parameters

1243
01:20:27,200 --> 01:20:32,970
underlying this block tell us exactly the kind which which is the out of which

1244
01:20:32,970 --> 01:20:37,280
more or less the help with all the Gaussian envelope in frequency and its amplitude

1245
01:20:37,480 --> 01:20:43,320
and all that we need to know here we have the instances of slow waves

1246
01:20:44,700 --> 01:20:49,560
that can be also parameters in the same way and for instance in the definition

1247
01:20:50,360 --> 01:20:54,860
the sleep stages 3 and 4 is based upon a percentage of time

1248
01:20:55,600 --> 01:21:01,960
occupied by slow wave activity if is between 20 and 50 % stage 3 even

1249
01:21:01,960 --> 01:21:09,790
more than half of that kind of 1 airport and the hit counted the 22nd

1250
01:21:09,960 --> 01:21:13,880
books the sculpted but I guess you all know what hit if there are no

1251
01:21:13,880 --> 01:21:21,080
questions right that's the case so if more than half of that kind of thing

1252
01:21:21,160 --> 01:21:21,540
that book

1253
01:21:22,280 --> 01:21:28,160
in most cases 2004 country is occupied by slow waves then it is supposed to

1254
01:21:28,160 --> 01:21:33,440
be or less stage 4 now before we had the basis this is basically the

1255
01:21:33,440 --> 01:21:40,220
1st signal processing method that depends exactly where things start and where it ends

1256
01:21:40,760 --> 01:21:42,960
and explicitly

1257
01:21:43,340 --> 01:21:50,020
from the parameters so can easily just out of the times occupied by these waves

1258
01:21:50,020 --> 01:21:55,400
that were classified as slow weights based upon other criteria as well and we know

1259
01:21:56,820 --> 01:22:02,980
well into the histogram automatic right not only that we can look and I think

1260
01:22:02,980 --> 01:22:06,580
when for example plotting this is an example of

1261
01:22:07,500 --> 01:22:15,830
a cold backward compatibility came because a before visually is considered only if it is

1262
01:22:16,070 --> 01:22:18,560
50 we can plot this %

1263
01:22:19,000 --> 01:22:20,960
over there a book

1264
01:22:21,080 --> 01:22:26,260
the book is 20 seconds and here are hours so in 1 hour would have

1265
01:22:26,260 --> 01:22:28,980
treebanks 60 of box right

1266
01:22:29,180 --> 01:22:34,460
and Bayerische airport we plot alliance that is proportional to the

1267
01:22:34,860 --> 01:22:41,480
the percentage of time occupied by slow waves within this at right so if you

1268
01:22:41,500 --> 01:22:46,420
threshold this at 20 and 50 per cent of the more-or-less stages 3 and 4

1269
01:22:46,700 --> 01:22:50,560
but you also see that it is a continuous process and is in line with

1270
01:22:50,560 --> 01:22:53,220
so what would be a good idea too

1271
01:22:54,640 --> 01:23:00,080
had the memory below that tries to approximate the state seems that that that should

1272
01:23:01,870 --> 01:23:05,220
he can connect two two filtering

1273
01:23:05,240 --> 01:23:07,460
and and state estimation

1274
01:23:08,860 --> 01:23:11,480
in the rest just going to show

1275
01:23:11,640 --> 01:23:15,130
that there is some mechanism that's what are they doing that

1276
01:23:15,140 --> 01:23:17,750
if i don't care about how this is done

1277
01:23:17,780 --> 01:23:22,130
particle filtering is is great that could be a really good

1278
01:23:22,220 --> 01:23:24,210
the solution for this

1279
01:23:24,220 --> 01:23:29,640
i just assumed that that we some whole has estimated the state

1280
01:23:29,680 --> 01:23:32,800
in particle i don't i'm not

1281
01:23:32,860 --> 01:23:35,150
at the beginning i wouldn't care about

1282
01:23:36,410 --> 01:23:40,410
resulting from state estimation just

1283
01:23:40,480 --> 01:23:44,960
make that we have the state information

1284
01:23:45,810 --> 01:23:47,520
if this effects that this

1285
01:23:47,530 --> 01:23:52,240
OK so what i'm doing here is that i'm simplifying the problem from its fortunate

1286
01:23:52,250 --> 01:23:54,060
in various ways

1287
01:23:54,900 --> 01:23:56,240
and and

1288
01:23:56,260 --> 01:24:01,210
i want to get to a place where i can really do something

1289
01:24:01,210 --> 01:24:05,280
so we see that even if we we do it is if we assume that

1290
01:24:05,280 --> 01:24:07,230
the state is measurable

1291
01:24:07,260 --> 01:24:10,810
the problem is going to be very tough and so you are going to study

1292
01:24:10,810 --> 01:24:11,840
the problem

1293
01:24:11,890 --> 01:24:13,490
in quite the details

1294
01:24:13,540 --> 01:24:17,180
and then maybe later if you have time you can come back to to this

1295
01:24:17,590 --> 01:24:22,420
even after program yes

1296
01:24:29,900 --> 01:24:30,910
OK so

1297
01:24:30,930 --> 01:24:35,200
yes OK so that's true but

1298
01:24:35,220 --> 01:24:39,660
so the the length of the observations of the length of the history is growing

1299
01:24:39,660 --> 01:24:41,410
this time so if you want to

1300
01:24:42,200 --> 01:24:46,120
somehow get to the visible the computational resources

1301
01:24:46,130 --> 01:24:49,170
then somehow you want to compress the he stays

1302
01:24:49,180 --> 01:24:55,410
so the problem is that it's pretty hard to generalize across history and

1303
01:24:56,080 --> 01:24:58,920
as the prime reason

1304
01:25:00,680 --> 01:25:02,010
it's a good question

1305
01:25:02,060 --> 01:25:07,200
you should get a cookie

1306
01:25:10,940 --> 01:25:15,170
so please ask more questions like that because it seems that i need some practice

1307
01:25:15,170 --> 01:25:19,170
or in this case

1308
01:25:19,220 --> 01:25:25,000
OK let's do i you

1309
01:25:37,160 --> 01:25:39,030
i don't know

1310
01:25:39,090 --> 01:25:41,670
five go see the

1311
01:25:41,710 --> 01:25:46,830
there are going to talk about this this is axillary asian

1312
01:25:56,060 --> 01:25:58,880
but luckily you have already finish or talk so

1313
01:25:58,890 --> 01:26:02,400
OK so

1314
01:26:02,420 --> 01:26:07,200
if we accept that we have state information then the next question is

1315
01:26:07,420 --> 01:26:09,960
how do you compute a good action

1316
01:26:10,010 --> 01:26:13,750
and that is the county there are two approaches

1317
01:26:13,800 --> 01:26:17,160
so one is called the model based control

1318
01:26:18,720 --> 01:26:21,770
and the other is more that the crown trauma

1319
01:26:21,790 --> 01:26:27,130
in the case of more than this control you somehow try to estimate the model

1320
01:26:27,230 --> 01:26:29,810
the dynamics

1321
01:26:32,200 --> 01:26:36,260
so trying to estimate the model so if you forget vault

1322
01:26:36,280 --> 01:26:39,050
the observation equation because we said that

1323
01:26:39,670 --> 01:26:42,160
that we also have the state

1324
01:26:42,170 --> 01:26:45,020
then what's left is just as

1325
01:26:45,050 --> 01:26:49,110
so if you knew then this would be just the planning problem

1326
01:26:49,120 --> 01:26:53,920
just the planning problem so talk about that

1327
01:26:53,940 --> 01:26:55,880
we have some guests

1328
01:26:57,130 --> 01:27:00,990
they should have special sensory

1329
01:27:01,010 --> 01:27:06,760
so why should be set up to you as i said previously

1330
01:27:10,440 --> 01:27:12,960
the problem is that

1331
01:27:12,980 --> 01:27:16,190
that different locations in different communities

1332
01:27:16,190 --> 01:27:17,870
and while

1333
01:27:17,880 --> 01:27:19,010
in control

1334
01:27:19,040 --> 01:27:21,830
typically denotes observation and

1335
01:27:21,910 --> 01:27:29,000
i tend to use the contrast here annotation but here in mdps left sometimes dino's

1336
01:27:29,010 --> 01:27:33,440
the next and so everything would become

1337
01:27:33,460 --> 01:27:39,400
unfortunately this state some of the survivors from all the changes that i made to

1338
01:27:39,410 --> 01:27:40,880
the slides

1339
01:27:40,900 --> 01:27:45,210
times so anyway so that these two different ways of

1340
01:27:45,290 --> 01:27:47,680
approaching the problem

1341
01:27:47,720 --> 01:27:50,840
so one approach is to estimate the model

1342
01:27:50,840 --> 01:27:55,270
this axiomatic scheme i'm saying that these

1343
01:27:55,310 --> 01:27:57,900
matches one of shapes

1344
01:27:57,940 --> 01:28:00,630
right matches the shape

1345
01:28:00,630 --> 01:28:02,480
you do the p norm

1346
01:28:02,520 --> 01:28:03,830
you decide

1347
01:28:03,840 --> 01:28:07,670
and uniformly you move the pain the p one over here but could boxes in

1348
01:28:07,670 --> 01:28:10,770
front of the box in front of their

1349
01:28:10,770 --> 01:28:13,230
so this one is independent again

1350
01:28:13,250 --> 01:28:14,730
this is an axiom

1351
01:28:14,750 --> 01:28:17,710
because it's an instance of that

1352
01:28:17,710 --> 01:28:21,540
now i do modus ponens what is modus ponens states is we the left hand

1353
01:28:21,540 --> 01:28:23,190
side alone

1354
01:28:23,190 --> 01:28:25,090
take this guy

1355
01:28:25,110 --> 01:28:28,590
check whether it's the same as this guy and if it is

1356
01:28:28,590 --> 01:28:31,670
then the sky down here

1357
01:28:32,960 --> 01:28:36,440
the rules are trying so you should be able to follow and now

1358
01:28:36,860 --> 01:28:41,670
put this whole thing because i haven't got placed on my page here

1359
01:28:41,690 --> 01:28:42,790
so that

1360
01:28:42,810 --> 01:28:44,560
this guy

1361
01:28:44,630 --> 01:28:47,290
is the same as these

1362
01:28:47,330 --> 01:28:51,020
and up here is the space where you put all of this stuff right so

1363
01:28:51,020 --> 01:28:53,630
this is the derivation

1364
01:28:53,650 --> 01:28:55,330
now i start here

1365
01:28:55,340 --> 01:28:56,770
p nor is

1366
01:28:56,790 --> 01:29:00,460
deducible from these guys because it's in there

1367
01:29:00,480 --> 01:29:04,110
this dissertation sick leave the left hand side alone just put a box in front

1368
01:29:04,110 --> 01:29:04,860
of it

1369
01:29:04,880 --> 01:29:07,290
and now apply modus ponens again

1370
01:29:07,310 --> 01:29:09,210
the the left hand side along

1371
01:29:09,270 --> 01:29:10,610
take this

1372
01:29:10,650 --> 01:29:13,170
see if it's the same as this if it is

1373
01:29:13,190 --> 01:29:16,880
in bring that one down there

1374
01:29:16,900 --> 01:29:21,110
prior to two derivations of the same thing

1375
01:29:26,170 --> 01:29:28,520
so what we have now

1376
01:29:28,540 --> 01:29:29,400
we have

1377
01:29:29,420 --> 01:29:32,710
characterisation or definition of what it means

1378
01:29:32,770 --> 01:29:35,500
for there to be introduction of five from gamma

1379
01:29:35,520 --> 01:29:40,090
just apply the rules see if you can feel the derivation

1380
01:29:43,090 --> 01:29:47,230
is the syntactically formulated logic so now i want to say

1381
01:29:47,270 --> 01:29:48,730
i'm going to take

1382
01:29:48,840 --> 01:29:49,880
these bits

1383
01:29:49,900 --> 01:29:53,290
and then we define a modal logic

1384
01:29:53,340 --> 01:29:55,420
using the syntax only

1385
01:29:56,170 --> 01:29:57,440
define it

1386
01:29:59,000 --> 01:30:04,090
this is the set of all formulae five which is deducible from the empty set

1387
01:30:04,090 --> 01:30:06,900
of assumptions

1388
01:30:08,480 --> 01:30:10,210
when you go on

1389
01:30:11,670 --> 01:30:15,310
you not allowed to have anything on the left-hand side and if you can still

1390
01:30:15,310 --> 01:30:17,090
do the production

1391
01:30:21,190 --> 01:30:23,460
five is deducible from the empty set

1392
01:30:23,500 --> 01:30:28,810
so will say that is theorem exactly when it deducible from the empty set

1393
01:30:28,860 --> 01:30:31,270
and this is the traditional way

1394
01:30:31,290 --> 01:30:33,860
modal logic is was defined

1395
01:30:33,920 --> 01:30:36,110
until kripke you came along

1396
01:30:36,170 --> 01:30:37,770
as a set of rules

1397
01:30:37,770 --> 01:30:39,210
syntactic rules

1398
01:30:39,230 --> 01:30:42,130
and then you talked about the theorems of the two

1399
01:30:46,860 --> 01:30:48,750
i want to show

1400
01:30:48,790 --> 01:30:52,420
the soundness completeness results

1401
01:30:54,750 --> 01:30:59,630
what's out timetables timetable is we supposed to stop it eleven have a right

1402
01:30:59,670 --> 01:31:03,810
quarter past OK good and do they have any coffee or anything at this point

1403
01:31:03,810 --> 01:31:05,520
nine OK alright

1404
01:31:06,830 --> 01:31:09,060
what i'm going to do is i'm going to try and do this and this

1405
01:31:09,060 --> 01:31:10,340
proof now

1406
01:31:10,360 --> 01:31:14,040
and let's leave the completeness proof for the next the next next lecture

1407
01:31:14,060 --> 01:31:18,480
because that almost catches mia so what i want to prove i want to prove

1408
01:31:18,480 --> 01:31:21,730
soundness that's this direction

1409
01:31:21,920 --> 01:31:24,540
just going let you read that from moment because

1410
01:31:24,590 --> 01:31:28,540
to prove by induction so some of you may be able to follow

1411
01:31:35,480 --> 01:31:37,110
so what am i trying to say

1412
01:31:37,110 --> 01:31:39,630
i'm trying to prove that

1413
01:31:39,650 --> 01:31:44,880
if there is the deduction of site from gamma

1414
01:31:44,900 --> 01:31:51,230
in size logical consequence of gamma

1415
01:31:52,150 --> 01:31:55,650
i'm going to do it by induction on the length of the derivation so if

1416
01:31:55,650 --> 01:31:59,630
i tell you that size is deducible from gamma you go back and look up

1417
01:31:59,630 --> 01:32:00,920
what does that mean

1418
01:32:00,940 --> 01:32:03,310
it means that

1419
01:32:03,360 --> 01:32:08,070
there is a tree with the root data and whatever right so

1420
01:32:08,090 --> 01:32:10,210
our assumption is

1421
01:32:10,210 --> 01:32:14,040
size is deducible from gamma so here we are

1422
01:32:14,060 --> 01:32:17,710
it's unfortunate that i've done this boards this way because the

1423
01:32:17,730 --> 01:32:19,170
OK so

1424
01:32:19,210 --> 01:32:21,020
here we are

1425
01:32:21,060 --> 01:32:23,650
site is deducible from gamma

1426
01:32:23,650 --> 01:32:27,500
and you know that these things are all instances of the idea rule

1427
01:32:27,500 --> 01:32:29,770
or the axiom rule

1428
01:32:29,830 --> 01:32:35,420
and then in here are the other rules modus ponens necessitation suppose there's modus ponens

1429
01:32:36,500 --> 01:32:39,750
and suppose this necessitation here right

1430
01:32:39,790 --> 01:32:41,610
modus ponens

1431
01:32:43,150 --> 01:32:48,210
and everything else is either modus ponens necessitation is going to do it by induction

1432
01:32:48,210 --> 01:32:50,360
on the length of this thing

1433
01:32:50,380 --> 01:32:52,150
OK what's the base case

1434
01:32:52,230 --> 01:32:56,250
the base case is when size in gamma

1435
01:32:56,310 --> 01:32:58,190
straight away

1436
01:32:58,190 --> 01:32:59,710
in other words

1437
01:32:59,770 --> 01:33:04,310
if size in gamma industry doesn't really exist because this guy is just an instance

1438
01:33:04,310 --> 01:33:05,830
of i

1439
01:33:07,110 --> 01:33:09,020
what's the shortest possible tree

1440
01:33:09,020 --> 01:33:12,770
the shortest possible derivation all size just in guam

1441
01:33:12,790 --> 01:33:14,540
what i have to show

1442
01:33:17,310 --> 01:33:20,840
that size the logical consequence of gamma

1443
01:33:21,380 --> 01:33:24,290
if size in gamma

1444
01:33:25,440 --> 01:33:30,360
and we have to show that whenever all of the things in gamma true everywhere

1445
01:33:30,400 --> 01:33:32,190
inside true everywhere

1446
01:33:32,190 --> 01:33:34,060
well the size in gamma

1447
01:33:34,110 --> 01:33:35,830
because it holds right

1448
01:33:35,880 --> 01:33:39,840
besides in gamma then our assumption is all the things in gamma true one of

1449
01:33:39,840 --> 01:33:44,170
them inside the you ask inside to everywhere of course

1450
01:33:45,360 --> 01:33:48,630
what's the other one the other one is that well it could also be an

1451
01:33:48,630 --> 01:33:50,440
instance of an axiom

1452
01:33:50,440 --> 01:33:54,980
right so again this thing doesn't exist it's just that you use AOL

1453
01:33:54,980 --> 01:33:58,460
look at this formula here is actually one of the shapes

1454
01:33:58,500 --> 01:34:01,960
it's one of its instance of one of shapes

1455
01:34:01,980 --> 01:34:04,270
and then you have to go here and say

1456
01:34:04,270 --> 01:34:06,850
get rid of

1457
01:34:06,870 --> 01:34:08,300
it is

1458
01:34:15,870 --> 01:34:25,070
the rest

1459
01:34:28,230 --> 01:34:31,280
the rest

1460
01:34:42,610 --> 01:34:46,780
it's not for

1461
01:34:46,830 --> 01:34:53,080
one of the word

1462
01:34:58,840 --> 01:35:03,670
original model

1463
01:35:03,810 --> 01:35:07,820
but i don't know

1464
01:35:10,630 --> 01:35:14,550
it is reasonable

1465
01:35:14,670 --> 01:35:16,300
all right

1466
01:35:48,400 --> 01:36:01,660
one of the

1467
01:36:01,680 --> 01:36:03,810
that's not

1468
01:36:11,700 --> 01:36:12,780
all right

1469
01:36:19,710 --> 01:36:22,310
world war

1470
01:36:22,330 --> 01:36:24,250
this was

1471
01:36:33,280 --> 01:36:41,720
last year

1472
01:36:52,720 --> 01:36:54,610
these are

1473
01:37:14,850 --> 01:37:17,180
he was

1474
01:37:19,540 --> 01:37:22,470
first of all

1475
01:37:29,750 --> 01:37:37,660
robert c

1476
01:37:48,680 --> 01:37:50,250
the only we

1477
01:37:53,810 --> 01:37:57,320
much more

1478
01:38:05,450 --> 01:38:08,200
so all

1479
01:38:08,220 --> 01:38:12,800
one of problems

1480
01:38:12,900 --> 01:38:16,540
the role

1481
01:38:23,330 --> 01:38:29,420
on the one part

1482
01:38:29,440 --> 01:38:31,980
there are

1483
01:38:34,800 --> 01:38:39,100
so you're

1484
01:38:41,010 --> 01:38:43,580
two more

1485
01:38:43,600 --> 01:38:47,950
or are

1486
01:38:47,970 --> 01:38:50,190
and but you

1487
01:38:52,380 --> 01:38:54,150
one she

1488
01:38:54,170 --> 01:38:57,320
five of

1489
01:39:05,620 --> 01:39:07,630
four days

1490
01:39:08,560 --> 01:39:13,590
well that's all

1491
01:39:20,010 --> 01:39:26,400
very simple model

1492
01:39:26,420 --> 01:39:32,370
o lord of the

1493
01:39:33,750 --> 01:39:35,270
o were

1494
01:39:38,760 --> 01:39:41,150
these organisms have

1495
01:39:43,760 --> 01:39:45,430
these there

1496
01:39:52,400 --> 01:40:00,510
from one of the problem

1497
01:40:02,390 --> 01:40:12,720
o thing

1498
01:40:12,730 --> 01:40:15,450
you are

1499
01:40:16,910 --> 01:40:18,010
from the

1500
01:40:18,150 --> 01:40:20,310
one two

1501
01:40:20,330 --> 01:40:24,330
one of the

1502
01:40:24,700 --> 01:40:27,580
not of world

1503
01:40:31,860 --> 01:40:38,060
this shows you know who

1504
01:40:38,120 --> 01:40:41,730
a lot

1505
01:40:49,520 --> 01:40:51,210
there are

1506
01:40:51,210 --> 01:40:56,560
and we know that order of all documents is always the same

1507
01:40:56,580 --> 01:40:58,520
we can simply recreate

1508
01:40:58,540 --> 01:41:03,460
previous estimations of plagiarized from sound file

1509
01:41:03,520 --> 01:41:04,810
and that's it sold

1510
01:41:04,830 --> 01:41:06,310
what we're are doing

1511
01:41:06,360 --> 01:41:08,250
we get

1512
01:41:08,290 --> 01:41:12,630
string number i mean from from all

1513
01:41:12,690 --> 01:41:13,630
from owl

1514
01:41:13,650 --> 01:41:15,290
my medics presentation

1515
01:41:15,290 --> 01:41:20,290
the read one probability sort of pagerank

1516
01:41:20,330 --> 01:41:21,900
and there

1517
01:41:21,900 --> 01:41:23,790
because we now know

1518
01:41:23,810 --> 01:41:25,830
number of things that

1519
01:41:25,840 --> 01:41:27,810
can here so it's

1520
01:41:27,860 --> 01:41:31,270
we can simply

1521
01:41:31,290 --> 01:41:36,670
get previous radio and and do this all this calculation

1522
01:41:38,860 --> 01:41:43,900
now we are getting now we are producing so it's like in this formula

1523
01:41:43,920 --> 01:41:46,380
what what what what what we get

1524
01:41:46,400 --> 01:41:51,600
we are getting our results not for one page as in previous on previous slide

1525
01:41:51,610 --> 01:41:55,380
but that we are putting them into into this array

1526
01:41:55,900 --> 01:42:00,770
adding them into different lots of this different cell

1527
01:42:00,840 --> 01:42:03,670
so we still need one error

1528
01:42:03,690 --> 01:42:07,170
that has all elements

1529
01:42:07,210 --> 01:42:12,710
so the size of this is equal to the size of our collection

1530
01:42:12,770 --> 01:42:16,600
cannot handle the situation can really

1531
01:42:16,610 --> 01:42:19,420
reduce the size of the story

1532
01:42:19,460 --> 01:42:21,610
actually can

1533
01:42:22,440 --> 01:42:25,230
in this case what what we're going to do

1534
01:42:25,290 --> 01:42:26,020
we can

1535
01:42:26,020 --> 01:42:28,480
simply divide this area

1536
01:42:28,500 --> 01:42:30,460
for example

1537
01:42:30,500 --> 01:42:33,420
by two we can

1538
01:42:33,460 --> 01:42:36,460
use only half of this right

1539
01:42:36,520 --> 01:42:37,980
in this case

1540
01:42:38,000 --> 01:42:41,670
for every iteration for every calculation

1541
01:42:41,750 --> 01:42:46,230
we are going to run through all matrix twice

1542
01:42:46,270 --> 01:42:47,540
so where

1543
01:42:47,560 --> 01:42:51,150
go through all this metric

1544
01:42:51,150 --> 01:42:52,600
the first time

1545
01:42:52,630 --> 01:42:54,810
and we are adding only four

1546
01:42:54,830 --> 01:42:57,580
the first half of elements

1547
01:42:59,670 --> 01:43:04,460
we are doing that so into to some temporary file

1548
01:43:04,500 --> 01:43:09,920
and then to replace this array is second half of my of

1549
01:43:09,940 --> 01:43:14,860
a way to force the pagerank for the second half of collection

1550
01:43:14,860 --> 01:43:17,860
and again run through through the whole

1551
01:43:17,880 --> 01:43:19,520
metrics presentation

1552
01:43:19,540 --> 01:43:23,560
and and in every case we simply ignore

1553
01:43:24,560 --> 01:43:28,980
things in our ports links post lists that are

1554
01:43:29,810 --> 01:43:31,790
in our output

1555
01:43:34,460 --> 01:43:35,790
in this case

1556
01:43:37,060 --> 01:43:39,610
managed to avoid

1557
01:43:40,420 --> 01:43:41,810
output array

1558
01:43:41,830 --> 01:43:46,080
but what we want to go on what want to be lost actually in real

1559
01:43:47,920 --> 01:43:52,360
one reading of the whole matrix presentation for every iteration

1560
01:43:52,380 --> 01:43:53,130
so now

1561
01:43:53,940 --> 01:43:55,580
every iteration

1562
01:43:55,630 --> 01:43:57,920
we need to or read

1563
01:43:57,920 --> 01:44:01,040
this matrix presentation so many times

1564
01:44:01,060 --> 01:44:03,540
as the number of

1565
01:44:03,560 --> 01:44:06,980
there is that we have for output

1566
01:44:08,830 --> 01:44:14,500
it's not that i should not use using this approach we can fit the collation

1567
01:44:14,500 --> 01:44:16,540
of which around four

1568
01:44:16,560 --> 01:44:18,420
any size of

1569
01:44:20,360 --> 01:44:25,840
four in memory they have only this story so it fixed number

1570
01:44:25,900 --> 01:44:27,980
and they always can

1571
01:44:28,000 --> 01:44:33,880
you can create can take some smaller a and only in this case we need

1572
01:44:33,900 --> 01:44:36,730
more iterations

1573
01:44:36,750 --> 01:44:40,710
so on every computer can calculate pagerank for

1574
01:44:41,130 --> 01:44:43,000
every size of the

1575
01:44:43,020 --> 01:44:47,500
of our metrics for every forever size of the collection

1576
01:44:47,500 --> 01:44:48,790
the only

1577
01:44:48,840 --> 01:44:52,810
the main difference is that it will be slower and slower because you need more

1578
01:44:52,810 --> 01:44:58,920
and more iterations you and wrote to presentation of adjacency matrix

1579
01:45:00,460 --> 01:45:02,230
actually managed to do that

1580
01:45:02,230 --> 01:45:04,900
so we can calculate pagerank

1581
01:45:04,960 --> 01:45:06,230
every computer

1582
01:45:07,560 --> 01:45:12,520
very very on the very small compared the problem be that if you take a

1583
01:45:12,540 --> 01:45:14,020
very long time

1584
01:45:14,060 --> 01:45:16,230
and how we can

1585
01:45:16,270 --> 01:45:18,980
increased speed of all

1586
01:45:20,670 --> 01:45:23,540
first of all we are doing iteration

1587
01:45:24,500 --> 01:45:27,170
the first approach that can help

1588
01:45:27,250 --> 01:45:29,940
to increase gate

1589
01:45:29,960 --> 01:45:33,980
it's so my last iteration

1590
01:45:34,080 --> 01:45:37,960
there are and this is the part of

1591
01:45:37,960 --> 01:45:42,110
science and there are a lot of articles how to

1592
01:45:42,130 --> 01:45:43,960
creating this

1593
01:45:43,980 --> 01:45:46,460
optimize pagerank calculation

1594
01:45:47,670 --> 01:45:51,500
the engineers about thinking about practical approach

1595
01:45:51,520 --> 01:45:55,540
and in my opinion the most simple and practical approach

1596
01:45:55,540 --> 01:45:57,020
it is to use

1597
01:45:57,020 --> 01:45:59,170
as initial where

1598
01:45:59,520 --> 01:46:02,630
this course and that we have

1599
01:46:02,630 --> 01:46:05,250
in this formula

1600
01:46:06,750 --> 01:46:10,420
well it from our previous calculations

1601
01:46:10,460 --> 01:46:16,330
the intuition behind this is that we don't have a lot of changes all in

1602
01:46:17,600 --> 01:46:24,150
and we have a theoretical example the theoretical result that usually pagerank is pretty stable

1603
01:46:24,150 --> 01:46:26,560
so based on this weekend

1604
01:46:26,610 --> 01:46:28,670
i believe that if we have

1605
01:46:28,670 --> 01:46:33,130
have to run both the observational and interventional resumes to check it but the belief

1606
01:46:33,130 --> 01:46:35,800
that actually there is enough information here

1607
01:46:35,850 --> 01:46:40,620
but if you condition on those variables you're going to get the the a is

1608
01:46:41,730 --> 01:46:45,850
model here which you can transform and this and even if you don't observe you

1609
01:46:45,870 --> 01:46:47,510
and that's the common case

1610
01:46:48,350 --> 01:46:50,550
then this is indeed

1611
01:46:50,590 --> 01:46:52,830
it is if you like

1612
01:46:52,850 --> 01:46:58,050
it's just it if i didn't have you in there would be an arrow from

1613
01:46:58,050 --> 01:46:59,320
fifty to one

1614
01:46:59,360 --> 01:47:04,180
but this is a special case that is not completely general

1615
01:47:04,630 --> 01:47:09,580
so we have to and so we have to argue that this would be appropriate

1616
01:47:09,590 --> 01:47:17,020
one of the or or

1617
01:47:17,040 --> 01:47:19,670
but it

1618
01:47:19,720 --> 01:47:23,850
it's not all around the variable it is the variable which tells you what regime

1619
01:47:23,850 --> 01:47:25,740
you're talking about so

1620
01:47:25,760 --> 01:47:28,550
if i can i can only condition on it

1621
01:47:28,560 --> 01:47:30,730
it always conditioned on

1622
01:47:30,870 --> 01:47:34,170
that means i said i tell you what this value is and and so i

1623
01:47:34,170 --> 01:47:39,230
consider if you like consider the joint distribution of these three variables conditioned on f

1624
01:47:39,250 --> 01:47:43,950
t and there's that's really three different distributions because there are three that fifty

1625
01:47:43,970 --> 01:47:47,130
all represented in one picture

1626
01:48:13,860 --> 01:48:15,020
you can think of

1627
01:48:15,540 --> 01:48:22,130
agents sometimes exploring sometimes intervening taking your random coin toss is not quite the same

1628
01:48:22,130 --> 01:48:26,120
deciding ahead of time what to do and you actually even think there's even a

1629
01:48:26,120 --> 01:48:30,480
bit of argument y randomisation works because it's not obvious that it works

1630
01:48:33,360 --> 01:48:40,900
randomisation is an observation regime is just that you know more about this probability structure

1631
01:48:40,980 --> 01:48:43,150
it's not an international regime

1632
01:48:43,210 --> 01:48:47,110
o so one point to point out is that it's often the case you can

1633
01:48:47,110 --> 01:48:51,200
say i would value knew more about the the patients

1634
01:48:51,210 --> 01:48:56,190
health economic status and all sorts of other things that will be enough knowledge that

1635
01:48:56,190 --> 01:48:57,110
i know

1636
01:48:57,120 --> 01:49:00,800
how we respond to treatment and it doesn't matter how we got here

1637
01:49:00,810 --> 01:49:04,880
if these are all if i do everything the doctor talk into account when you

1638
01:49:04,880 --> 01:49:10,040
decided how to assign the treatment then that will be adjusting for that will make

1639
01:49:10,040 --> 01:49:11,170
it OK

1640
01:49:11,180 --> 01:49:13,930
but that's fine i would but i would have to know is if i don't

1641
01:49:13,930 --> 01:49:18,120
observe you understood in trouble for don't deserve you then i would still get an

1642
01:49:18,120 --> 01:49:22,400
arrow from fifty to one and i could just take the body channel

1643
01:49:22,420 --> 01:49:27,490
despite the what the distribution of y given t only without putting you in the

1644
01:49:27,490 --> 01:49:31,460
problem i couldn't just transform at from one

1645
01:49:31,470 --> 01:49:33,110
to another

1646
01:49:33,120 --> 01:49:39,670
OK let's suppose then that you convincingly tell that story you believe it

1647
01:49:39,750 --> 01:49:43,300
and new is the variable set of variables which you you you know what you're

1648
01:49:43,300 --> 01:49:45,970
talking about and you can observe

1649
01:49:45,980 --> 01:49:48,120
one thing you can do

1650
01:49:48,140 --> 01:49:50,450
it is

1651
01:49:50,700 --> 01:49:56,260
you can compute what you might call the average causal effect conditional on somebody's got

1652
01:49:56,260 --> 01:49:57,720
characteristics you

1653
01:49:57,730 --> 01:49:59,770
which is what it says so

1654
01:49:59,800 --> 01:50:04,600
it's the expected you just change the treatment from one to zero for somebody with

1655
01:50:04,600 --> 01:50:07,020
those characteristics and notice

1656
01:50:07,040 --> 01:50:10,350
i haven't put an indicator of the treatment regime here

1657
01:50:10,790 --> 01:50:12,820
and that's because i don't need to

1658
01:50:12,860 --> 01:50:18,850
because the distribution of y given you and is the same in all three regimes

1659
01:50:18,880 --> 01:50:23,180
so that's why some meaningful thing and i could calculate it i can computed by

1660
01:50:23,180 --> 01:50:26,590
modeling the observation regime and it will be relevant

1661
01:50:26,600 --> 01:50:30,390
causally meaningful in the interventional regimes

1662
01:50:30,400 --> 01:50:33,920
and i can use that as the formula here

1663
01:50:34,420 --> 01:50:39,600
to estimate the the overall average causal effect so here's the story

1664
01:50:39,660 --> 01:50:46,360
the average causal effect what is the is is expectation why when i intervenes set

1665
01:50:47,090 --> 01:50:51,000
this set the treatment week one minus the same thing

1666
01:50:51,000 --> 01:50:54,350
when i said it is zero so let's consider each of those terms looks like

1667
01:50:54,350 --> 01:50:59,610
this so this is just the standard probability calculus rule of extending the conversation to

1668
01:50:59,610 --> 01:51:02,990
bring in additional variable in this case you

1669
01:51:05,090 --> 01:51:07,260
now i realize that when

1670
01:51:07,260 --> 01:51:10,050
well i'm intervening articles t

1671
01:51:10,570 --> 01:51:16,050
automatically the value of t will be little t because that's part of my story

1672
01:51:16,240 --> 01:51:19,440
that's what i mean by intervening so redundant information

1673
01:51:19,510 --> 01:51:23,590
but once i could added additional condition here

1674
01:51:23,610 --> 01:51:25,820
i no longer need

1675
01:51:25,850 --> 01:51:30,350
to put in f too coarse too because the expectation of y

1676
01:51:30,350 --> 01:51:32,100
given c and u

1677
01:51:32,120 --> 01:51:36,310
and information about fifty actually doesn't matter what if fifty years by this assumption so

1678
01:51:36,310 --> 01:51:41,230
i without leaving out means think about the observation regime

1679
01:51:41,230 --> 01:51:44,580
so when i don't mention it is the observation regime

1680
01:51:44,590 --> 01:51:46,670
so that's the same as this

1681
01:51:48,950 --> 01:51:49,740
and now

1682
01:51:49,750 --> 01:51:51,070
having left it out

1683
01:51:51,080 --> 01:51:57,560
the only random variable left in this quantity is the variable u

1684
01:51:57,780 --> 01:52:00,650
and the variable u is independent of st t

1685
01:52:00,790 --> 01:52:04,820
and therefore this out the expectation doesn't make any difference

1686
01:52:06,030 --> 01:52:10,260
i can remove it and i just get the in thing and and if i

1687
01:52:10,260 --> 01:52:15,140
just take the value fatigue was one subtracts value from google zero i discover

1688
01:52:15,580 --> 01:52:21,760
the average causal effect is the overall expectations all the observational regime of the

1689
01:52:22,040 --> 01:52:27,430
of the the average causal effect for the sub population of people with a particular

1690
01:52:27,460 --> 01:52:31,770
value of and i just average over all possible values of you would kind of

1691
01:52:31,770 --> 01:52:35,320
the distribution this for reasons which may or may not become clear

1692
01:52:35,330 --> 01:52:38,350
it's turned the back door formula

1693
01:52:38,370 --> 01:52:46,900
and not just the expectation but this this calculation goes to the whole distribution and

1694
01:52:46,910 --> 01:52:51,090
looks like this in terms of the whole distribution where everything on the right can

1695
01:52:51,090 --> 01:52:57,160
be got from observational data so i can estimate the whole interventional distribution by this

1696
01:52:57,160 --> 01:53:02,570
formula ford or formula from purely observational data so long as i can believe the

1697
01:53:02,570 --> 01:53:05,030
story i told

1698
01:53:05,030 --> 01:53:08,870
that's important and so long as i can observe you because that's one of the

1699
01:53:10,910 --> 01:53:15,460
so i can do this modelling in the observation regime

1700
01:53:15,480 --> 01:53:21,070
that story then allows me to transfer that information to the intervention regime and here's

1701
01:53:21,080 --> 01:53:26,040
the way of doing quite a simple story but not too trivial

1702
01:53:26,190 --> 01:53:30,620
and a very useful one

1703
01:53:30,650 --> 01:53:32,370
you could also

1704
01:53:32,440 --> 01:53:37,280
use a similar story to so what how what i justify

1705
01:53:37,320 --> 01:53:40,480
my original story my original story

1706
01:53:40,500 --> 01:53:42,380
it was that i

1707
01:53:42,440 --> 01:53:47,190
i don't i don't have to worry at all about the this completely ignorable treatment

