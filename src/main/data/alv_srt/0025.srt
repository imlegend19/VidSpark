1
00:00:00,000 --> 00:00:00,800
that was my

2
00:00:00,820 --> 00:00:03,030
the pattern in the past you can see that

3
00:00:03,040 --> 00:00:04,360
i branch for

4
00:00:04,380 --> 00:00:05,340
a release

5
00:00:05,350 --> 00:00:07,840
it took three years so

6
00:00:07,850 --> 00:00:09,740
you know we don't want that to happen again

7
00:00:10,100 --> 00:00:15,540
so it's available sources are available of course binary packages available this turns out to

8
00:00:15,540 --> 00:00:19,360
be really important because more people use binary packages and stores

9
00:00:19,370 --> 00:00:22,970
people these days they're like what i do with this you know they download from

10
00:00:22,970 --> 00:00:24,890
the FTP site and they say

11
00:00:24,940 --> 00:00:28,580
i double clicked on it but it didn't do anything it's just the jesus park

12
00:00:28,830 --> 00:00:34,030
right so they they know they think it's supposed to install itself

13
00:00:34,950 --> 00:00:37,280
they don't even understand what this sort what source

14
00:00:37,290 --> 00:00:41,750
binary understand binary meets they say what we mean binary

15
00:00:47,220 --> 00:00:49,780
in addition active itself there's

16
00:00:49,830 --> 00:00:54,460
there's another or companion project that was started by paul cancer when he was at

17
00:00:54,460 --> 00:00:56,740
this time not sure if still there are not

18
00:00:58,760 --> 00:01:03,360
it is the collection of contributed functions initially was just intended as the players are

19
00:01:03,360 --> 00:01:07,830
the doctor four was hosted on sourceforge net was intended as a place where people

20
00:01:07,830 --> 00:01:12,830
could collaborate to write additional functions rocket that maybe didn't belong in active core

21
00:01:13,900 --> 00:01:16,420
domain specific packages

22
00:01:18,250 --> 00:01:21,830
now we actually have a package system so there's a little bit more organisation to

23
00:01:21,830 --> 00:01:23,200
this and

24
00:01:23,530 --> 00:01:26,470
there are seventy domain specific packages there

25
00:01:26,520 --> 00:01:29,290
and i think it's growing and i don't know when i made that counts for

26
00:01:29,290 --> 00:01:31,310
a few months so that could be

27
00:01:31,320 --> 00:01:33,570
that could be even bigger now

28
00:01:34,550 --> 00:01:36,380
but especially

29
00:01:37,450 --> 00:01:39,720
people companies always want to know

30
00:01:39,740 --> 00:01:42,860
how many people are using octave how how many people

31
00:01:42,880 --> 00:01:44,930
i have no idea

32
00:01:44,940 --> 00:01:46,890
you know i can count them because

33
00:01:48,350 --> 00:01:52,690
people are downloading and getting from places like controller distribution

34
00:01:52,710 --> 00:01:56,260
so my guess is hundreds of thousands and how do i guess that we have

35
00:01:56,410 --> 00:01:58,320
a little evidence and that's that

36
00:01:59,760 --> 00:02:03,100
there there were twenty nine thousand downloads of the octave

37
00:02:03,150 --> 00:02:04,600
three source code

38
00:02:05,570 --> 00:02:06,690
four months

39
00:02:06,710 --> 00:02:09,840
and that's a lot from the previous stable

40
00:02:13,710 --> 00:02:17,280
but that source code right now many people use source could i don't even know

41
00:02:17,280 --> 00:02:21,580
what if i'm installing activize so computers for people i don't installed from source i

42
00:02:21,580 --> 00:02:22,870
get the debian package

43
00:02:23,090 --> 00:02:25,950
why i installed from source when i can just download

44
00:02:25,960 --> 00:02:27,600
and install quickly

45
00:02:27,780 --> 00:02:33,720
now the doctor foresight distance binary for windows this this numbers even more interesting to

46
00:02:33,720 --> 00:02:36,450
me sixty five thousand downloads of same

47
00:02:36,510 --> 00:02:38,840
version of active in four months

48
00:02:40,150 --> 00:02:44,740
that's just the windows users and i think we have a lot more

49
00:02:45,260 --> 00:02:48,430
in linux users

50
00:02:51,000 --> 00:02:53,080
i don't know about so much about the

51
00:02:53,090 --> 00:02:55,890
mac OS ten users but certainly

52
00:02:55,940 --> 00:02:59,750
there's a lot of a lot of people using active

53
00:03:01,500 --> 00:03:04,720
unix systems and they

54
00:03:04,770 --> 00:03:07,400
so these numbers these these numbers of you know

55
00:03:07,570 --> 00:03:10,100
sixty five thousand i think that's probably

56
00:03:11,000 --> 00:03:17,350
small compared to the number of

57
00:03:17,360 --> 00:03:23,740
now mailing list subscribers are you know there's seven hundred seventy five subscribers to the

58
00:03:23,740 --> 00:03:24,590
help list

59
00:03:24,660 --> 00:03:27,050
partition participation is increasing

60
00:03:27,060 --> 00:03:31,430
overall traffic approximately doubled in the last two years i

61
00:03:32,600 --> 00:03:35,560
i don't know about you but i don't subscribe to the mailing list of every

62
00:03:35,570 --> 00:03:39,550
software package i use so i don't know the multiplayer you would use for this

63
00:03:40,180 --> 00:03:44,480
mailing list subscription number but it's going to be fairly large

64
00:03:46,230 --> 00:03:49,470
posted the list over this is all the time

65
00:03:49,480 --> 00:03:52,320
i started it i started mailing list

66
00:03:52,330 --> 00:03:54,200
almost at the same time i started

67
00:03:54,350 --> 00:03:56,360
writing software

68
00:03:56,690 --> 00:03:59,740
i guess actually right around the time released it to the web for the for

69
00:04:00,530 --> 00:04:02,090
was was an

70
00:04:02,180 --> 00:04:04,330
for the first time

71
00:04:06,130 --> 00:04:09,530
it really took off around two thousand two or three

72
00:04:09,600 --> 00:04:13,730
started really getting increasing participation and i think that happen

73
00:04:13,780 --> 00:04:15,840
around the same time that

74
00:04:15,940 --> 00:04:19,440
a particular contributor sort of arrived on the scene his name is david bateman anywhere

75
00:04:20,910 --> 00:04:22,440
paris and

76
00:04:22,450 --> 00:04:24,720
he did a lot of things he worked on sparse matrix

77
00:04:24,750 --> 00:04:28,540
functions and some of the multidimensional array functions

78
00:04:29,550 --> 00:04:32,160
incredibly active i don't think he sleeps

79
00:04:33,320 --> 00:04:38,340
how serious he again now for almost twenty four hours a day

80
00:04:39,240 --> 00:04:43,510
when he was working the unfortunately his job motorola which allowed him to work on

81
00:04:43,510 --> 00:04:44,450
active life

82
00:04:44,470 --> 00:04:48,730
i ended and now he's working for start-up company which is consuming all this time

83
00:04:48,730 --> 00:04:51,750
twenty four hours a day along with a small children so

84
00:04:52,770 --> 00:04:55,080
he's going to do quite as much work on

85
00:04:55,240 --> 00:04:57,140
also this depth but the end

86
00:04:57,140 --> 00:05:00,180
so for any

87
00:05:00,330 --> 00:05:04,140
vertices u and v

88
00:05:04,200 --> 00:05:09,200
in the ground for any two vertices

89
00:05:09,220 --> 00:05:11,540
all patterns

90
00:05:11,540 --> 00:05:13,660
from u to v

91
00:05:13,700 --> 00:05:20,200
i have the same weight

92
00:05:20,410 --> 00:05:26,370
as they did before well not quite they have the same freeway

93
00:05:29,020 --> 00:05:31,410
so if you look at all the different patterns

94
00:05:31,430 --> 00:05:34,560
and you see well what's the difference between VH

95
00:05:34,660 --> 00:05:36,660
sorry this let's say

96
00:05:36,680 --> 00:05:38,890
delta which is the alternative sparse

97
00:05:38,910 --> 00:05:42,470
and all this of h which is shortest path weights according to this new weight

98
00:05:43,430 --> 00:05:46,310
then that difference is the same

99
00:05:46,330 --> 00:05:51,260
so all these pairs are reweighted by the same amount

100
00:05:51,330 --> 00:05:54,680
it is actually statement about all has not just shortest paths

101
00:05:54,790 --> 00:05:57,490
there we go

102
00:06:00,540 --> 00:06:03,430
how many people is obvious already

103
00:06:03,450 --> 00:06:05,640
if you're here it is

104
00:06:05,700 --> 00:06:08,760
one word

105
00:06:08,760 --> 00:06:13,120
OK maybe not that obvious

106
00:06:13,140 --> 00:06:15,990
i checked out the word when you figure it out

107
00:06:16,000 --> 00:06:18,520
i mean already this

108
00:06:18,540 --> 00:06:20,060
rather verbose

109
00:06:20,080 --> 00:06:24,760
is one word per

110
00:06:24,770 --> 00:06:26,910
still waiting

111
00:06:29,580 --> 00:06:38,000
so let's just take one of these patterns starts at you and we

112
00:06:38,020 --> 00:06:39,330
the very

113
00:06:39,350 --> 00:06:42,560
it is going to see what's new weight is relative to to its old ways

114
00:06:42,560 --> 00:06:46,260
so this is really out w seven age of the past

115
00:06:46,270 --> 00:06:51,370
which we define usual way as the sum over all edges

116
00:06:51,450 --> 00:06:53,370
the weights

117
00:06:53,590 --> 00:06:55,350
of the

118
00:06:55,370 --> 00:06:56,560
the new ways

119
00:06:56,580 --> 00:06:57,770
the edge

120
00:06:57,790 --> 00:07:02,830
from the eye to the i plus one

121
00:07:02,850 --> 00:07:04,490
in other words

122
00:07:04,830 --> 00:07:07,790
it's possible OK

123
00:07:07,810 --> 00:07:11,080
so the best definition of the the weight of path

124
00:07:11,220 --> 00:07:18,870
and then we know this thing is just w

125
00:07:18,930 --> 00:07:19,720
yeah i

126
00:07:19,740 --> 00:07:21,390
the first one

127
00:07:21,410 --> 00:07:26,310
right plus the weight of the first vertex classes

128
00:07:26,370 --> 00:07:28,850
the reweighting of the i

129
00:07:28,900 --> 00:07:33,450
minus the rewriting of the one

130
00:07:33,490 --> 00:07:35,540
this is all in parentheses

131
00:07:35,560 --> 00:07:37,680
it's all summed over i

132
00:07:37,700 --> 00:07:40,100
now i need the magic word

133
00:07:40,140 --> 00:07:46,020
telescopes now it's obvious these telescopes with an extra previous except the very beginning in

134
00:07:46,020 --> 00:07:48,560
the very and so this is

135
00:07:48,580 --> 00:07:52,870
some of these weights of edges

136
00:07:52,890 --> 00:07:53,830
but then

137
00:07:54,270 --> 00:07:56,120
outside the sum

138
00:07:56,120 --> 00:07:57,500
we have a plus

139
00:07:57,520 --> 00:07:58,970
HIV one

140
00:07:58,990 --> 00:08:00,120
and minus

141
00:08:00,140 --> 00:08:02,000
h v

142
00:08:02,770 --> 00:08:08,160
because those guys don't quite council weren't looking at the cycle just passed

143
00:08:08,180 --> 00:08:10,850
and this thing is just

144
00:08:12,500 --> 00:08:15,060
the past

145
00:08:15,100 --> 00:08:15,850
as the

146
00:08:16,290 --> 00:08:20,640
normal way to the past and so the change in the difference between w h

147
00:08:20,640 --> 00:08:23,740
p and WP is this thing

148
00:08:23,740 --> 00:08:27,220
which is just a you

149
00:08:27,220 --> 00:08:28,620
minus HIV

150
00:08:28,640 --> 00:08:32,120
and the point is that the same as long as you fix the endpoints u

151
00:08:32,120 --> 00:08:32,470
and v

152
00:08:32,910 --> 00:08:34,200
the short

153
00:08:34,220 --> 00:08:36,740
you're changing the shortest this pathway

154
00:08:36,760 --> 00:08:38,930
by the same thing

155
00:08:38,970 --> 00:08:41,770
for all passes for any path from u to v

156
00:08:41,790 --> 00:08:45,620
and that person so the one word here with telescopes

157
00:08:45,620 --> 00:08:47,880
and it starts moving

158
00:08:47,880 --> 00:08:50,790
with a certain velocity

159
00:08:50,790 --> 00:08:52,930
let's say this velocity v

160
00:08:53,140 --> 00:08:55,970
and this angle is estate

161
00:08:56,030 --> 00:08:59,720
so this component of the velocity in our direction

162
00:09:00,480 --> 00:09:03,780
cosine theta called the radial component

163
00:09:03,820 --> 00:09:05,860
and one the stars moved to us

164
00:09:05,940 --> 00:09:09,680
we to to star makes no difference

165
00:09:09,740 --> 00:09:14,580
if the velocity of this star is much much smaller than c

166
00:09:15,430 --> 00:09:17,870
f prime one that you will receive

167
00:09:17,920 --> 00:09:20,560
because at times one class

168
00:09:20,700 --> 00:09:21,820
he overseas

169
00:09:21,860 --> 00:09:23,640
the the cosine of the

170
00:09:23,670 --> 00:09:25,620
this equation is only your exam

171
00:09:25,740 --> 00:09:27,620
she is three

172
00:09:27,630 --> 00:09:29,360
i'm standing

173
00:09:31,990 --> 00:09:34,040
cosine theta is positive

174
00:09:34,050 --> 00:09:35,770
the object is approaching

175
00:09:35,790 --> 00:09:37,770
you observe the higher frequencies

176
00:09:37,820 --> 00:09:39,490
cosine figures negative

177
00:09:39,520 --> 00:09:43,420
object is going away radial component is away from us

178
00:09:43,480 --> 00:09:45,660
and the frequency is low

179
00:09:45,720 --> 00:09:50,830
in optical astronomy we cannot measure frequencies we can only measure the wavelength

180
00:09:50,890 --> 00:09:52,400
and the connection between

181
00:09:52,460 --> 00:09:54,090
wavelength and frequency

182
00:09:55,280 --> 00:09:58,030
equals the speed of light divided by

183
00:09:59,470 --> 00:10:01,800
so we can substitute in that equation

184
00:10:01,850 --> 00:10:03,070
f equals

185
00:10:03,080 --> 00:10:04,780
he divided by land

186
00:10:04,830 --> 00:10:06,260
and prime

187
00:10:06,350 --> 00:10:09,050
will see divided by lambda

188
00:10:09,100 --> 00:10:10,320
when we do that

189
00:10:10,360 --> 00:10:14,050
and we also make use of this approximation

190
00:10:14,290 --> 00:10:16,780
one divided by one plus access

191
00:10:16,820 --> 00:10:19,160
it is approximately one minus x

192
00:10:19,230 --> 00:10:22,090
as long as axis much much more than one

193
00:10:22,090 --> 00:10:25,280
this is the first order term in the taylor expansion

194
00:10:25,290 --> 00:10:27,620
you can find that now let the prime

195
00:10:27,630 --> 00:10:29,430
becomes lambda

196
00:10:29,690 --> 00:10:31,590
one minus

197
00:10:31,630 --> 00:10:34,420
minus sign the plus changes to minus sign

198
00:10:34,470 --> 00:10:35,860
for that reason

199
00:10:35,900 --> 00:10:38,160
times the divided by c

200
00:10:38,210 --> 00:10:40,430
i'm schools to think

201
00:10:40,440 --> 00:10:42,060
notice here

202
00:10:42,060 --> 00:10:44,360
you have the radial velocity

203
00:10:44,370 --> 00:10:47,330
you have that you

204
00:10:47,340 --> 00:10:49,400
eleven the prime

205
00:10:49,420 --> 00:10:51,310
is larger than life

206
00:10:51,310 --> 00:10:53,580
the object is going away from us

207
00:10:53,630 --> 00:10:54,640
we call that

208
00:10:54,650 --> 00:10:56,840
red shift

209
00:10:56,840 --> 00:10:58,880
object is receding

210
00:10:58,930 --> 00:11:00,340
lambda prime

211
00:11:00,350 --> 00:11:01,920
less than wavelength

212
00:11:01,940 --> 00:11:06,610
one of the last the land that recall the solution

213
00:11:06,610 --> 00:11:09,230
because the wavelength becomes shorter in this time

214
00:11:09,290 --> 00:11:12,930
move towards the blue end of the spectrum becomes larger

215
00:11:12,990 --> 00:11:15,770
fifty ret part of speech

216
00:11:15,850 --> 00:11:19,880
roche it means the object is approaching

217
00:11:19,930 --> 00:11:24,290
during the accident we discussed this we took an example and the prime of lambda

218
00:11:24,290 --> 00:11:26,270
there's one point zero zero

219
00:11:29,310 --> 00:11:29,940
if your

220
00:11:29,960 --> 00:11:31,980
substitute that into this equation

221
00:11:32,030 --> 00:11:33,760
you'll find that we

222
00:11:33,810 --> 00:11:35,200
cosine theta

223
00:11:36,870 --> 00:11:38,640
equals minus one hundred

224
00:11:41,200 --> 00:11:42,420
what it means

225
00:11:42,440 --> 00:11:46,530
all you know is the radial velocity you never get any information about the angle

226
00:11:46,530 --> 00:11:50,630
that the radial velocities hundred kilometres seconds away from us

227
00:11:51,550 --> 00:11:54,540
we are moving on the club's second away from the star

228
00:11:54,550 --> 00:11:55,710
that's the same thing

229
00:12:02,950 --> 00:12:05,300
if we apply this to sound

230
00:12:05,320 --> 00:12:06,240
then we get

231
00:12:06,260 --> 00:12:09,610
very similar equations

232
00:12:09,660 --> 00:12:11,070
suppose you sit still

233
00:12:11,260 --> 00:12:12,660
twenty six one hundred

234
00:12:12,740 --> 00:12:15,610
and i move sound source to you

235
00:12:15,640 --> 00:12:17,660
you don't move

236
00:12:18,890 --> 00:12:23,880
you get a similar equation that francis crick residual here

237
00:12:23,930 --> 00:12:25,110
he calls f

238
00:12:25,200 --> 00:12:26,820
and one was

239
00:12:26,840 --> 00:12:28,660
POV the sound

240
00:12:28,700 --> 00:12:29,910
the cosine

241
00:12:29,930 --> 00:12:31,450
of the

242
00:12:32,760 --> 00:12:35,890
is about three hundred forty metres a second

243
00:12:35,890 --> 00:12:39,140
at room temperature

244
00:12:39,160 --> 00:12:40,260
in other words

245
00:12:40,280 --> 00:12:44,340
if i trolled something around in a circle

246
00:12:44,430 --> 00:12:45,890
u i

247
00:12:45,930 --> 00:12:48,680
in the plane of that circle you are here

248
00:12:48,780 --> 00:12:51,490
when the object comes to you

249
00:12:51,620 --> 00:12:53,030
the maximum

250
00:12:53,110 --> 00:12:55,990
frequency of prime is larger than that

251
00:12:56,010 --> 00:12:58,300
when it goes away from you

252
00:12:58,340 --> 00:13:01,140
we decrease in frequency

253
00:13:01,180 --> 00:13:02,740
more than half

254
00:13:02,790 --> 00:13:04,890
many objects here

255
00:13:04,950 --> 00:13:06,820
and when is here

256
00:13:06,860 --> 00:13:10,110
when the angle is ninety degrees the cosine say zero

257
00:13:10,110 --> 00:13:12,890
you'll find a prime equals f

258
00:13:12,890 --> 00:13:15,430
the primary process

259
00:13:15,430 --> 00:13:17,640
twelve something around

260
00:13:17,680 --> 00:13:19,860
let's suppose i truly the around was a

261
00:13:20,110 --> 00:13:24,220
orbital velocity if i call the orbital velocity

262
00:13:24,320 --> 00:13:28,200
three point four meters per second just to get some nice numbers

263
00:13:28,220 --> 00:13:29,910
and this is

264
00:13:29,930 --> 00:13:31,340
o point o one

265
00:13:31,360 --> 00:13:35,240
so if it comes to you and increase to one percent in the frequency when

266
00:13:35,240 --> 00:13:36,820
it goes away from you

267
00:13:36,820 --> 00:13:38,390
you hear decrease

268
00:13:38,450 --> 00:13:40,240
one percent

269
00:13:40,240 --> 00:13:43,530
i have here

270
00:13:43,570 --> 00:13:47,570
sound source which produces roughly fifteen hundred hertz

271
00:13:47,640 --> 00:13:52,280
i could turn it around

272
00:13:52,300 --> 00:13:55,680
i can do it once the second around with you twice

273
00:13:56,910 --> 00:13:58,160
so you would here

274
00:13:58,180 --> 00:14:00,640
two percent increase when it comes to you

275
00:14:00,640 --> 00:14:06,260
two percent decrease when it goes away from you

276
00:14:06,320 --> 00:14:09,470
you the doppler shift

277
00:14:09,510 --> 00:14:13,340
it's always difficult intellectual because you get reflections from the wall

278
00:14:13,410 --> 00:14:16,160
you know it when it comes to you the pitch is higher

279
00:14:16,160 --> 00:14:18,650
this parabolic containment function

280
00:14:18,690 --> 00:14:22,620
so that's how these getting units behave in the negative log prob domain if you

281
00:14:22,620 --> 00:14:27,290
think of them in terms of native approach costs the parabolic containment and then linear

282
00:14:27,290 --> 00:14:31,030
facts coming from the hidden units

283
00:14:31,070 --> 00:14:34,420
this is just one example of something in the exponential family

284
00:14:36,050 --> 00:14:37,590
the paper by welling

285
00:14:37,610 --> 00:14:39,130
that shows how

286
00:14:39,140 --> 00:14:43,750
you can apply this kind of idea anything the exponential family most combinations work some

287
00:14:43,750 --> 00:14:45,780
combinations like gas in use

288
00:14:45,820 --> 00:14:49,440
guess visible units engaged in this will blow up on you if you're not careful

289
00:14:49,510 --> 00:14:53,230
they can go improper they can get weights that are big enough

290
00:14:53,310 --> 00:14:57,360
because both sets against him that as you increase the activities of the visible units

291
00:14:57,360 --> 00:14:59,280
and the hidden units together

292
00:14:59,290 --> 00:15:02,840
if the weights is there's been a value value your weight matrix they can block

293
00:15:02,840 --> 00:15:05,560
faster than the quadratic containment function

294
00:15:05,580 --> 00:15:09,620
and that was it is and i evaluate matrix is bigger than the coefficient in

295
00:15:09,620 --> 00:15:10,680
front of this

296
00:15:10,690 --> 00:15:14,230
then think can explode and you can get in from the negative energy so it's

297
00:15:14,230 --> 00:15:16,490
not appropriate proper distribution anymore

298
00:15:16,490 --> 00:15:20,320
so this problems in using getting invisible units engaged in units

299
00:15:20,330 --> 00:15:24,240
but it's fine to use cases visible units in binary or binary visible and goes

300
00:15:25,410 --> 00:15:27,920
and that's because one of these two numbers complex

301
00:15:27,970 --> 00:15:32,690
and here this complex quadratically if this between zero and one

302
00:15:32,720 --> 00:15:36,150
they're both consumers that this can blow quadratically

303
00:15:36,170 --> 00:15:39,510
this is trying to keep it under control automatically is these weights are big enough

304
00:15:39,510 --> 00:15:42,010
it won't standard

305
00:15:43,380 --> 00:15:45,580
i just want show nice example

306
00:15:45,580 --> 00:15:51,090
of another graduate student alex krizhevsky learning a big model with binary hidden units

307
00:15:52,320 --> 00:15:54,300
case in visible units

308
00:15:54,310 --> 00:15:58,890
and you can only we know regularizer there's nothing saying the hidden activity should be sparse

309
00:15:58,910 --> 00:16:01,580
they just turn out to be extremely sparse

310
00:16:01,590 --> 00:16:03,770
it just that's what he wants to do

311
00:16:03,770 --> 00:16:07,600
and what he did was he learned on a few million thirty two by thirty

312
00:16:07,600 --> 00:16:11,930
two colour images were got by taking photos and downsampling and searching for the photos

313
00:16:11,930 --> 00:16:18,520
using the names of fifty three thousand different kinds of things so it

314
00:16:18,540 --> 00:16:22,020
it's a collection of images from the web is very representative

315
00:16:22,030 --> 00:16:24,850
and what influences something really interesting

316
00:16:24,930 --> 00:16:29,890
it has each image is represented by RGB chances thirty two by thirty two

317
00:16:30,290 --> 00:16:31,880
for the red green and blue

318
00:16:31,900 --> 00:16:33,580
just the naive encoding

319
00:16:33,600 --> 00:16:36,640
and when i show you what filters learned like this one

320
00:16:36,690 --> 00:16:40,070
it's got three thousand weights

321
00:16:40,100 --> 00:16:45,580
and i'm showing the three thousand weights by for each pixel showing you the RGB

322
00:16:46,660 --> 00:16:51,380
for pixel so showing the three different weights learned as an RGB value

323
00:16:51,430 --> 00:16:56,260
and what you see is that almost all of them almost learn to be almost

324
00:16:56,260 --> 00:16:58,570
exactly black and white

325
00:16:58,570 --> 00:17:02,430
and this is clearly just property what real images alike

326
00:17:02,450 --> 00:17:05,810
we know the visual the human visual system does this it learns is high figures

327
00:17:05,810 --> 00:17:08,740
a black things in this low frequency kind colour wash

328
00:17:09,030 --> 00:17:12,820
and that's what kind of watercolour pictures work

329
00:17:12,830 --> 00:17:18,390
it's interesting to see a little restricted boltzmann machines a bigger story boards machine it's

330
00:17:18,390 --> 00:17:21,340
got ten thousand to three it's got thirty million websites

331
00:17:21,360 --> 00:17:26,110
and actually took him like several days to learn this on the cluster

332
00:17:26,120 --> 00:17:28,640
you can and probably like in a day on a GPU

333
00:17:28,660 --> 00:17:34,540
and these kind of filters almost exactly balance intensity

334
00:17:34,580 --> 00:17:40,530
and the colour is very wrong in this printer but it has roughly two different

335
00:17:40,530 --> 00:17:45,450
colour contrasts it has that kind of blue yellow and red green many

336
00:17:45,470 --> 00:17:49,610
and the colour filters almost always low frequency fact they're all low frequency and high

337
00:17:49,610 --> 00:17:53,400
frequency the few low frequency black-and-white terms

338
00:17:53,430 --> 00:17:56,250
anyway it does the right thing

339
00:17:56,260 --> 00:17:59,580
one reason i'm pointing this out is the paper from MIT and these tiny images

340
00:17:59,580 --> 00:18:04,420
saying they tried to apply builds machines using getting visible units binary hidden units and

341
00:18:04,420 --> 00:18:05,700
it didn't work

342
00:18:05,720 --> 00:18:09,230
there's also a paper from the cambridge research lab tries to play but machines like

343
00:18:09,230 --> 00:18:11,340
that gets not very good filters

344
00:18:11,400 --> 00:18:14,080
it turns out you have to try quite hard

345
00:18:14,080 --> 00:18:16,220
in a GPU

346
00:18:16,320 --> 00:18:18,600
and they needed several weeks of

347
00:18:18,620 --> 00:18:22,280
actually a couple of months me tell graduate student no try harder

348
00:18:22,280 --> 00:18:25,420
and that's the role of the device

349
00:18:25,440 --> 00:18:28,820
and filling around so is not trivial to get these things to work but they

350
00:18:28,820 --> 00:18:30,640
can produce very nice solutions

351
00:18:30,690 --> 00:18:37,580
now i want to talk about some work by ruslan salakhutdinov me

352
00:18:40,080 --> 00:18:45,990
getting processes with deep belief nets so getting press a really good not finding structuring

353
00:18:48,200 --> 00:18:50,940
that's important thing to be really good at

354
00:18:50,950 --> 00:18:53,380
so if you have just a few labels

355
00:18:53,390 --> 00:18:57,410
they're very good things to use but you have to decide on your kind of

356
00:18:57,410 --> 00:19:00,840
metric and

357
00:19:00,840 --> 00:19:03,680
one way to do it is to build deep belief net

358
00:19:03,700 --> 00:19:06,580
entirely unsupervised and then

359
00:19:06,590 --> 00:19:12,100
say that two images are similar if the the final vectors produced by the deep

360
00:19:12,100 --> 00:19:13,360
belief nets in

361
00:19:13,400 --> 00:19:16,110
maybe that's better measure similarity there

362
00:19:16,110 --> 00:19:17,250
and the

363
00:19:17,310 --> 00:19:20,680
it's sign of the critical angle

364
00:19:20,730 --> 00:19:23,500
which i write c are

365
00:19:23,540 --> 00:19:24,800
is and two

366
00:19:24,850 --> 00:19:28,940
divided by and one that there is a condition and the condition is that and

367
00:19:28,940 --> 00:19:31,540
one must be larger than n two

368
00:19:31,560 --> 00:19:33,000
that's not the case

369
00:19:33,010 --> 00:19:34,140
then there is not

370
00:19:39,240 --> 00:19:41,100
in total reflection

371
00:19:41,140 --> 00:19:43,250
it's actually very interesting

372
00:19:43,290 --> 00:19:48,140
it has practical applications which i will discuss with you shortly

373
00:19:48,190 --> 00:19:51,620
but i first want to do demonstration in which i want to show this to

374
00:19:52,290 --> 00:19:53,490
i have here

375
00:19:55,160 --> 00:19:56,850
and here is air

376
00:19:56,870 --> 00:20:00,620
and i have a laser beam which i can shine in

377
00:20:00,630 --> 00:20:04,890
and i can change angle theta one and slowly increase it and you will see

378
00:20:04,890 --> 00:20:07,510
that when i approached fifty degrees

379
00:20:07,560 --> 00:20:12,920
first of all you will see that they data to increases increases increases and then

380
00:20:12,930 --> 00:20:17,810
when i approached the critical angle and exceeded then we have a hundred percent

381
00:20:20,270 --> 00:20:23,920
let me first on the laser sort of this a little bit of light

382
00:20:23,960 --> 00:20:27,610
and i'm going to show it to you there for those of you

383
00:20:27,620 --> 00:20:28,770
who are not

384
00:20:28,790 --> 00:20:30,410
sitting very close

385
00:20:30,560 --> 00:20:37,750
and that means i have to set the light situation

386
00:20:37,760 --> 00:20:39,320
OK so there you see

387
00:20:39,330 --> 00:20:40,910
the lights

388
00:20:40,920 --> 00:20:43,940
coming in just the way we had on the blackboard

389
00:20:44,000 --> 00:20:46,740
this is the way it comes in in water

390
00:20:46,830 --> 00:20:48,610
this is the reflective part

391
00:20:48,640 --> 00:20:51,850
and this is the one that is refracted into the air

392
00:20:51,870 --> 00:20:53,820
that this one here

393
00:20:53,830 --> 00:20:56,110
and i'm going to increase the angle

394
00:20:56,120 --> 00:20:59,440
when i touch the table the water will start to wiggle a little

395
00:20:59,450 --> 00:21:05,510
you will probably see that

396
00:21:05,520 --> 00:21:09,200
so i'm going to say OK i decree i'm going to increase the angle

397
00:21:09,360 --> 00:21:11,990
come increasing the that they had to

398
00:21:12,000 --> 00:21:13,670
getting larger is going to

399
00:21:13,680 --> 00:21:15,960
approach to ninety degrees

400
00:21:16,060 --> 00:21:18,180
increasing one

401
00:21:18,250 --> 00:21:21,280
increase in one look at data two

402
00:21:21,290 --> 00:21:25,390
almost ninety degrees and very close to the critical angle now

403
00:21:25,400 --> 00:21:27,790
almost at right now

404
00:21:27,800 --> 00:21:30,810
and now all the light is being reflected

405
00:21:31,020 --> 00:21:32,080
o percent

406
00:21:34,590 --> 00:21:42,520
remarkable phenomenon

407
00:21:42,540 --> 00:21:46,440
and this has practical applications

408
00:21:46,490 --> 00:21:51,200
and we're going to show you some of these practical applications two

409
00:21:51,280 --> 00:21:57,320
the most important practical applications fibre optics

410
00:21:57,380 --> 00:22:01,180
if i have a fibre this properly designed

411
00:22:01,230 --> 00:22:08,510
so this is the a fiver

412
00:22:08,580 --> 00:22:12,190
and some light comes in here

413
00:22:12,240 --> 00:22:16,760
and if here so this is some plastic or glass and this is an area

414
00:22:16,850 --> 00:22:20,430
if this angle of incidence is larger than the critical angle

415
00:22:20,440 --> 00:22:22,300
hundred percent reflected

416
00:22:22,310 --> 00:22:24,630
so nothing comes out in the air

417
00:22:24,640 --> 00:22:26,460
on the percent reflected

418
00:22:27,210 --> 00:22:29,530
again the critical angle is exceeded

419
00:22:29,540 --> 00:22:31,690
so hundred percent is reflected

420
00:22:31,700 --> 00:22:35,480
you can go through this whole thing for miles on and you can put even

421
00:22:35,480 --> 00:22:39,260
not in there as long as you never exceeds the critical angle

422
00:22:39,280 --> 00:22:40,150
that light

423
00:22:40,170 --> 00:22:41,600
will propagate

424
00:22:41,610 --> 00:22:43,430
and they will never be any loss

425
00:22:43,440 --> 00:22:47,450
all flights that's why people are very much interested in it you can transport

426
00:22:47,510 --> 00:22:49,570
images as i will show you

427
00:22:49,580 --> 00:22:51,830
through fiber optics

428
00:22:51,880 --> 00:22:54,710
i have here fibre optics

429
00:22:54,770 --> 00:22:57,160
which has four thousand five was

430
00:22:57,220 --> 00:23:00,590
it's fifty microns in diameter each

431
00:23:00,650 --> 00:23:02,960
and we have a laser beam here

432
00:23:03,050 --> 00:23:05,310
and the laser light will come out

433
00:23:05,360 --> 00:23:08,790
i will sorry laser light shortly there and it doesn't matter what i do is

434
00:23:08,790 --> 00:23:13,190
the fibres i can even go on eighty degrees in china there as long as

435
00:23:13,200 --> 00:23:14,680
inside the fibre

436
00:23:14,730 --> 00:23:16,280
i always

437
00:23:16,330 --> 00:23:18,050
exceeds the

438
00:23:18,100 --> 00:23:25,860
critical angle

439
00:23:26,290 --> 00:23:27,940
i don't want the

440
00:23:27,980 --> 00:23:30,630
television anymore so we can turn this off

441
00:23:30,650 --> 00:23:32,390
and let's hear have the

442
00:23:32,430 --> 00:23:36,710
laser lights

443
00:23:36,890 --> 00:23:37,900
it is

444
00:23:38,070 --> 00:23:40,350
see laser light

445
00:23:40,410 --> 00:23:42,530
OK now look at this

446
00:23:42,550 --> 00:23:45,380
this bundle that i have here

447
00:23:45,430 --> 00:23:48,700
and turn it into an absurd snake

448
00:23:48,710 --> 00:23:50,340
almost like an s

449
00:23:50,350 --> 00:23:52,230
all light still comes out

450
00:23:52,290 --> 00:23:54,440
so it goes all the way through

451
00:23:54,480 --> 00:23:55,240
i'm going to

452
00:23:55,310 --> 00:23:58,490
one hundred eighty degrees around two hundred

453
00:23:59,190 --> 00:24:01,180
there is

454
00:24:01,230 --> 00:24:03,360
is an amazing phenomenon

455
00:24:03,380 --> 00:24:04,410
at this light

456
00:24:04,430 --> 00:24:06,380
doesn't get out in the air

457
00:24:06,440 --> 00:24:08,860
stays inside the fibre

458
00:24:08,920 --> 00:24:10,840
and that's the idea behind

459
00:24:10,900 --> 00:24:13,490
fibre optics

460
00:24:13,550 --> 00:24:15,190
i have another

461
00:24:15,240 --> 00:24:17,520
application of fiber optics

462
00:24:17,530 --> 00:24:21,140
right here which is very similar

463
00:24:21,180 --> 00:24:24,710
you can send an image through fiber optics this is my

464
00:24:24,730 --> 00:24:26,920
fibre optics now thousands of

465
00:24:26,940 --> 00:24:29,450
small fibres

466
00:24:29,460 --> 00:24:33,310
and i send the message in here this site an image could be a person

467
00:24:33,310 --> 00:24:35,010
could be text

468
00:24:35,100 --> 00:24:37,410
and here we have the TV camera

469
00:24:37,450 --> 00:24:38,940
and we can watch

470
00:24:38,980 --> 00:24:40,110
that image

471
00:24:40,130 --> 00:24:43,080
on this side of the fibre appears that image

472
00:24:43,090 --> 00:24:44,690
this television camera

473
00:24:44,700 --> 00:24:45,890
will be able

474
00:24:45,900 --> 00:24:48,310
to see that image

475
00:24:49,790 --> 00:24:53,670
let's see whether we can

476
00:24:53,710 --> 00:24:59,880
show you that message

477
00:24:59,930 --> 00:25:01,490
OK i have

478
00:25:01,540 --> 00:25:03,630
do something here again

479
00:25:03,640 --> 00:25:06,210
i think is going on there is

480
00:25:06,270 --> 00:25:10,230
so we actually can see individual five how interesting

481
00:25:10,290 --> 00:25:12,810
each one the individual fibres

482
00:25:12,810 --> 00:25:17,160
the monte carlo estimate is not the only one there's other estimators

483
00:25:17,190 --> 00:25:21,480
and tomorrow article mentions some of them because

484
00:25:21,520 --> 00:25:25,630
i mean maybe alittle provide introduction to them

485
00:25:25,690 --> 00:25:27,710
if you just

486
00:25:29,400 --> 00:25:33,000
from what it would be

487
00:25:33,170 --> 00:25:40,790
very briefly if there's other ways of doing this like some people put it distribution

488
00:25:40,790 --> 00:25:42,990
on this for example and

489
00:25:43,000 --> 00:25:45,560
through that they managed to summon analytical

490
00:25:46,960 --> 00:25:49,490
but this is sort of the sheep easy way

491
00:25:49,500 --> 00:25:51,630
of doing all this stuff

492
00:25:51,630 --> 00:25:55,470
not the best of a certain cheap

493
00:25:55,580 --> 00:25:57,180
and it's

494
00:25:57,230 --> 00:26:02,020
it's an average it's an ergodic averages the laws of large numbers apply

495
00:26:02,030 --> 00:26:04,690
you can get central limit theorem

496
00:26:04,750 --> 00:26:08,840
we know the cell culture

497
00:26:08,910 --> 00:26:12,140
and this is the problem

498
00:26:12,160 --> 00:26:13,780
we don't know how to sample

499
00:26:13,790 --> 00:26:15,570
how to generate some

500
00:26:15,580 --> 00:26:19,170
if you knew how to generate samples there would be no curse of dimensionality there

501
00:26:19,170 --> 00:26:20,480
would be no problem

502
00:26:20,500 --> 00:26:23,610
one the carla has no curse of dimensionality in

503
00:26:23,650 --> 00:26:27,130
mainly because of the thing you know how to generate samples evening

504
00:26:27,200 --> 00:26:31,320
and because the young dimensions if you put the samples in the right place you

505
00:26:31,320 --> 00:26:32,810
can do it

506
00:26:33,780 --> 00:26:37,420
we don't know how to get the samples

507
00:26:38,440 --> 00:26:43,030
i guess this is just somebody that you can read again

508
00:26:43,030 --> 00:26:47,250
this is the sort of delta measure the histogram

509
00:26:50,160 --> 00:26:53,370
and this is just what we do now

510
00:26:54,130 --> 00:26:55,190
four recaps

511
00:26:55,200 --> 00:26:59,680
i forgot and i had this slide

512
00:26:59,690 --> 00:27:03,270
again it's a question of taking this guy replacing it here

513
00:27:03,280 --> 00:27:05,740
and like child is that

514
00:27:15,690 --> 00:27:17,630
now the thing

515
00:27:26,610 --> 00:27:29,130
now if you had one sample

516
00:27:29,140 --> 00:27:31,230
positioned at the mean

517
00:27:31,240 --> 00:27:32,830
in high dimensions

518
00:27:32,830 --> 00:27:38,400
and i will give you the minister that's what i'm saying so its positioning the

519
00:27:38,400 --> 00:27:40,720
samples that is

520
00:27:40,780 --> 00:27:42,950
so i have to have just kind of washed

521
00:27:42,970 --> 00:27:45,860
the problem into another problem

522
00:27:45,870 --> 00:27:51,140
but i haven't solved you correct

523
00:27:51,190 --> 00:27:54,040
and so now we're going to try to solve the problem

524
00:27:54,040 --> 00:27:56,180
keep them on the cover

525
00:27:56,420 --> 00:28:04,380
OK so that's pretty much did it replaced at histogram estimator and just so that

526
00:28:04,380 --> 00:28:06,020
you have it in your notes

527
00:28:06,030 --> 00:28:07,630
then you end up

528
00:28:08,910 --> 00:28:11,360
f of x

529
00:28:11,360 --> 00:28:13,830
and this becomes an approximation now

530
00:28:13,850 --> 00:28:16,410
f of x times one of and

531
00:28:16,480 --> 00:28:19,350
some from michael want to and

532
00:28:19,390 --> 00:28:20,730
delta x

533
00:28:27,940 --> 00:28:31,100
you get the estimator will be chosen

534
00:28:31,140 --> 00:28:32,660
one of the

535
00:28:32,710 --> 00:28:54,780
hi a call one g and f of x i

536
00:28:54,790 --> 00:28:56,890
and this is sometimes referred

537
00:28:56,940 --> 00:29:00,650
sometimes referred to the

538
00:29:00,670 --> 00:29:12,270
the estimator based on in sample for the function

539
00:29:12,390 --> 00:29:14,990
so i was

540
00:29:15,050 --> 00:29:18,280
monte carlo can also be used for optimisation

541
00:29:21,140 --> 00:29:22,970
so sometimes

542
00:29:23,030 --> 00:29:26,210
well if you have lots of samples

543
00:29:26,240 --> 00:29:30,020
if you draw a thousand samples from a distribution then you can look at their

544
00:29:30,400 --> 00:29:32,660
their heights and you pick the highest

545
00:29:32,680 --> 00:29:34,790
that's one of the optimisation

546
00:29:34,830 --> 00:29:38,400
a better way is that to make sure that when you draw the samples that

547
00:29:38,400 --> 00:29:42,090
the samples concentrate the peaks

548
00:29:42,820 --> 00:29:45,980
and we're going to see several ways of doing that one of them that's very

549
00:29:45,980 --> 00:29:47,880
popular simulated annealing

550
00:29:47,890 --> 00:29:52,080
where the samples try to concentrate at the peak of the distribution because what you

551
00:29:52,080 --> 00:29:57,100
care there's about optimisation about finding the peak you don't care about integration

552
00:29:57,130 --> 00:29:58,170
just care about

553
00:29:58,220 --> 00:30:01,110
finding the max

554
00:30:01,150 --> 00:30:05,900
so now let's look at the first basic technique for generating samples and it's called

555
00:30:05,900 --> 00:30:08,040
rejection sampling

556
00:30:08,140 --> 00:30:10,130
the algorithm is very simple

557
00:30:10,180 --> 00:30:13,390
and it works as follows

558
00:30:15,610 --> 00:30:17,990
you iterate steps

559
00:30:18,090 --> 00:30:19,860
what we're going to do

560
00:30:19,890 --> 00:30:23,350
is we can construct another distribution

561
00:30:23,360 --> 00:30:24,480
q effects

562
00:30:24,500 --> 00:30:28,080
so q facsism nice distribution think of it this account

563
00:30:28,080 --> 00:30:31,330
we know how to draw samples from agustin distribution

564
00:30:31,400 --> 00:30:36,800
OK matlab is just around them and you can generate a sample from constant revision

565
00:30:36,930 --> 00:30:39,850
so q of of friendly distribution

566
00:30:39,890 --> 00:30:43,420
again i'm washing out a moving the problem somewhere else and moving the problem to

567
00:30:43,420 --> 00:30:47,820
one of choosing could kill effects let's assume we have a good cure effects for

568
00:30:47,820 --> 00:30:49,950
the time being

569
00:30:50,030 --> 00:30:53,980
and i'm going to try number uniformly between zero and one

570
00:30:54,000 --> 00:30:56,360
in matlab just ex aequo around

571
00:30:56,410 --> 00:30:58,750
one common one

572
00:30:58,810 --> 00:31:01,390
or something in python

573
00:31:01,440 --> 00:31:03,570
i'm going to use maps for this

574
00:31:03,570 --> 00:31:06,590
how many people know matlab

575
00:31:08,850 --> 00:31:11,470
OK good you're moving

576
00:31:11,490 --> 00:31:14,360
pythons the way

577
00:31:14,400 --> 00:31:18,090
i still have everything on my was in matlab unfortunately

578
00:31:18,130 --> 00:31:20,410
OK and so what we're going to do then

579
00:31:20,470 --> 00:31:23,310
if we're going to generate

580
00:31:24,500 --> 00:31:25,800
we generate x i

581
00:31:25,800 --> 00:31:29,890
we generate you and what we excise of the samples that we care so we

582
00:31:29,890 --> 00:31:33,530
generate them from q and then we're can await them in the way

583
00:31:33,580 --> 00:31:36,410
so that their actions samples from p

584
00:31:36,430 --> 00:31:37,600
that's the trick

585
00:31:37,600 --> 00:31:40,080
generate something that's easy

586
00:31:40,120 --> 00:31:42,170
weights in some sort of way

587
00:31:42,230 --> 00:31:44,560
so that they become samples of p

588
00:31:44,560 --> 00:31:48,180
because once they have samples of b then you can solve all the expectations

589
00:31:48,280 --> 00:31:50,080
so first we need to

590
00:31:50,090 --> 00:31:52,900
almost all the expectations

591
00:31:54,800 --> 00:31:56,400
the technique we're going to do

592
00:31:56,420 --> 00:31:59,630
it is we're going to come up with a big number and

593
00:31:59,680 --> 00:32:03,180
and now mention with send this later

594
00:32:03,540 --> 00:32:08,770
that's going and we're going to accept a sample exercise it's going to be only

595
00:32:08,770 --> 00:32:10,350
valid sample

596
00:32:10,450 --> 00:32:13,470
if p is sort of a much larger

597
00:32:13,520 --> 00:32:17,770
mq effects so let me kind of give you

598
00:32:17,780 --> 00:32:19,600
an illustration of this

599
00:32:19,640 --> 00:32:22,670
we try to come up with the distribution q of x

600
00:32:22,680 --> 00:32:24,730
we multiplied by a number

601
00:32:24,740 --> 00:32:27,380
to ensure that this q facts will sort of

602
00:32:27,390 --> 00:32:30,120
dominate facts

603
00:32:30,210 --> 00:32:32,400
so that it provides a good cover

604
00:32:32,480 --> 00:32:35,580
in particular when necessary condition is

605
00:32:35,610 --> 00:32:37,290
but i can't have

606
00:32:37,310 --> 00:32:38,920
q effectively zero

607
00:32:38,940 --> 00:32:41,930
in the region with few facts is not zero

608
00:32:41,980 --> 00:32:47,350
because that would make sense because then you will never generate samples there were vectors

609
00:32:47,350 --> 00:32:53,220
here so you will never get a good estimate of q p of x

610
00:32:53,340 --> 00:33:00,390
MR justice scalia

611
00:33:00,390 --> 00:33:03,840
three eight if

612
00:33:04,000 --> 00:33:06,460
it's there

613
00:33:07,830 --> 00:33:15,810
it's apparently actually once to this one is that the the coin has a

614
00:33:15,840 --> 00:33:18,950
fifty percent probability of coming up heads

615
00:33:21,430 --> 00:33:24,330
the other is that the three flips are independent of each other

616
00:33:24,380 --> 00:33:26,560
that has no memory

617
00:33:26,590 --> 00:33:29,910
in that case because it could be that the help that you could have a

618
00:33:29,930 --> 00:33:33,710
magic coin comes up tails tells tells how the time and heads heads heads half

619
00:33:33,740 --> 00:33:36,340
the time that have had to have tails right

620
00:33:36,380 --> 00:33:38,460
but it's not the case the

621
00:33:38,460 --> 00:33:42,680
two then two hundred would have probability zero

622
00:33:42,810 --> 00:33:45,000
the probability three

623
00:33:45,020 --> 00:33:46,960
so we're making some assumptions here

624
00:33:46,960 --> 00:33:49,630
for most places this option

625
00:33:53,580 --> 00:33:55,890
it's just another example

626
00:33:55,920 --> 00:34:00,450
yes i don't have to beat over the head

627
00:34:00,490 --> 00:34:01,700
and and or

628
00:34:02,630 --> 00:34:05,620
and of course the example when we ask

629
00:34:05,620 --> 00:34:09,550
for the probability the probability that the second letter is o

630
00:34:09,640 --> 00:34:13,660
the rest of the total probability of all events

631
00:34:13,670 --> 00:34:18,500
all sentences whose second

632
00:34:18,520 --> 00:34:21,800
so there's an infinite number of is actually

633
00:34:22,800 --> 00:34:27,170
as you know you can have an infinite sum the infinite summations whose sum is

634
00:34:27,170 --> 00:34:32,280
fine one half plus one core plus one as sentences longer

635
00:34:32,300 --> 00:34:35,880
what happens to their probabilities

636
00:34:35,910 --> 00:34:39,460
you're multiplying more dice together right

637
00:34:39,490 --> 00:34:42,800
the the probability of the die coming up in a particular way is less than

638
00:34:42,800 --> 00:34:46,590
or equal to one so the probabilities will tend to go down

639
00:34:46,620 --> 00:34:49,810
as these things get longer so it's like a geometric sequence

640
00:34:49,840 --> 00:34:53,950
one half plus one color plus one and you do end up something

641
00:34:54,000 --> 00:34:58,920
all strings to one when the sum over all strings second letters

642
00:34:58,930 --> 00:35:02,670
you'll get something less than one

643
00:35:02,840 --> 00:35:05,990
that is going to be

644
00:35:06,000 --> 00:35:07,670
the probability

645
00:35:07,680 --> 00:35:10,290
the second of so that's all it means

646
00:35:10,290 --> 00:35:12,720
OK so let's go back to this program here

647
00:35:12,750 --> 00:35:15,990
i want to suggest that we have a couple alternatives

648
00:35:16,010 --> 00:35:18,620
here's the difference

649
00:35:18,630 --> 00:35:23,800
so what i'm trying to buy you know emphasizing general back over here is say

650
00:35:23,830 --> 00:35:27,450
what you got different choices here when you build model about how you divide the

651
00:35:28,420 --> 00:35:31,090
how what you picture variables to be

652
00:35:31,100 --> 00:35:32,700
how do you decide to

653
00:35:32,740 --> 00:35:37,830
or things in the factor things and what you just what conditional independence assumptions you

654
00:35:37,830 --> 00:35:40,430
try to make so i want to point out something that's kind of wrong about

655
00:35:43,170 --> 00:35:45,670
it doesn't have a dictionary

656
00:35:46,470 --> 00:35:49,800
forces is becoming a very

657
00:35:49,810 --> 00:35:52,740
and while i was missing the forces common more

658
00:35:52,750 --> 00:35:56,430
is more common than the driver that would would predict

659
00:35:56,460 --> 00:36:00,050
because the trichromatic predicts all kinds of words that are not in the dictionary

660
00:36:00,090 --> 00:36:02,420
in which are really rare

661
00:36:03,560 --> 00:36:09,710
it's when we try to model does it stealing probability mass that should rightfully belonged

662
00:36:09,710 --> 00:36:10,630
to horses

663
00:36:10,640 --> 00:36:14,460
i mean i don't know why we take courses much forces

664
00:36:14,520 --> 00:36:16,550
it's forces of

665
00:36:16,620 --> 00:36:18,800
probably going well it's courses

666
00:36:18,800 --> 00:36:21,080
now first one torso is

667
00:36:21,180 --> 00:36:25,000
we could have picked ources instead of course is a dry ground model with like

668
00:36:25,000 --> 00:36:26,550
that just as well

669
00:36:26,560 --> 00:36:28,630
maybe even more

670
00:36:28,880 --> 00:36:33,100
but what happens to affect horses and all

671
00:36:33,390 --> 00:36:37,660
only knows that you know hl are as far as support

672
00:36:37,680 --> 00:36:40,540
are likely

673
00:36:40,550 --> 00:36:43,550
OK so how can we learn from the corpus

674
00:36:43,560 --> 00:36:46,700
the first is the common word one we do

675
00:36:46,710 --> 00:36:52,050
it is instead of talking about random variables like as one of the first letter

676
00:36:52,340 --> 00:36:57,260
we could talk about random variables like w one of the first four

677
00:36:57,280 --> 00:37:01,000
so we can divide you know as long as we have some function which given

678
00:37:01,000 --> 00:37:04,930
the sequence is able to split a sequence of characters able was split white space

679
00:37:05,220 --> 00:37:08,750
we can compute w one given that

680
00:37:08,780 --> 00:37:12,700
so we can talk about the probability that w one is forces and if we

681
00:37:12,700 --> 00:37:16,120
run our white space weather on the brown corpus then we can find the number

682
00:37:16,130 --> 00:37:19,460
of sentences use first water sources

683
00:37:19,470 --> 00:37:20,720
that's this

684
00:37:20,740 --> 00:37:23,040
as expected

685
00:37:23,060 --> 00:37:28,290
so i back to just how which is forces in

686
00:37:28,300 --> 00:37:29,340
and we get

687
00:37:29,350 --> 00:37:32,700
seven point two times ten to the minus

688
00:37:32,990 --> 00:37:34,760
actually is a lot

689
00:37:34,790 --> 00:37:37,180
higher i think than what we got

690
00:37:40,010 --> 00:37:49,330
was on the previous

691
00:37:49,330 --> 00:37:54,750
so was five point four times ten to the minus seven

692
00:37:59,580 --> 00:38:01,160
here we have

693
00:38:01,170 --> 00:38:04,630
more than one hundred times more likely than the driver model for

694
00:38:04,640 --> 00:38:06,550
so this is just really signal

695
00:38:06,560 --> 00:38:07,910
the tribunal model

696
00:38:07,910 --> 00:38:11,890
it's getting something wrong the driver about things that forces us to be

697
00:38:11,920 --> 00:38:17,300
more than one hundred times less frequent than it actually was in the brain

698
00:38:17,300 --> 00:38:20,840
and that's because it just doesn't know that in what happened to pick the workhorses

699
00:38:21,130 --> 00:38:22,540
rather than forces

700
00:38:22,560 --> 00:38:24,340
he gives them both of them

701
00:38:24,350 --> 00:38:26,280
people in probability

702
00:38:26,280 --> 00:38:28,020
that was

703
00:38:28,050 --> 00:38:30,120
OK so

704
00:38:30,130 --> 00:38:33,080
one advantage to using a model based on words

705
00:38:33,100 --> 00:38:36,080
it is that

706
00:38:37,250 --> 00:38:40,920
you know it gets gets the probabilities were writing knows what the dictionary

707
00:38:40,950 --> 00:38:45,850
a disadvantage is what happens if the word shows up like savage which is not

708
00:38:45,850 --> 00:38:48,350
in the brain called

709
00:38:48,370 --> 00:38:54,180
well the triangular would still give some probability because all those sequences of three letters

710
00:38:54,180 --> 00:38:58,960
are still probably the brown corpus somewhere but that word that polish names nor in

711
00:38:58,960 --> 00:39:02,160
the brown corpus so the brown corpus could estimate

712
00:39:02,170 --> 00:39:04,300
probability zero without backing

713
00:39:04,310 --> 00:39:10,170
OK so the advantage is that we focus more probability mass stuff that we've actually

714
00:39:11,460 --> 00:39:14,960
in some ways this is like a six model or something right because you know

715
00:39:14,960 --> 00:39:18,920
it's looking at longer sequences to know whether whether the thing is likely but at

716
00:39:18,920 --> 00:39:23,200
the same disadvantage that any of these higher order models have like six kramer seventeen

717
00:39:23,200 --> 00:39:27,390
he was you know this is you found with respect to each other the field

718
00:39:27,390 --> 00:39:30,820
published this information after

719
00:39:30,840 --> 00:39:32,510
now of course you know this

720
00:39:32,510 --> 00:39:36,030
it's really you know when i was twelve

721
00:39:36,090 --> 00:39:40,320
it's actually original jumbled picture but now we know to time

722
00:39:40,320 --> 00:39:41,930
good photograph

723
00:39:42,090 --> 00:39:44,110
now points you

724
00:39:44,620 --> 00:39:50,740
find just as these zakri run people down and DNA but that's

725
00:39:50,930 --> 00:39:51,820
two hundred

726
00:39:51,820 --> 00:39:53,510
first of all why

727
00:39:53,660 --> 00:39:55,470
we really need to find out

728
00:39:55,490 --> 00:39:58,140
like can to be made this

729
00:39:58,200 --> 00:39:59,660
after creation of graph

730
00:39:59,740 --> 00:40:02,410
you know what is going to look like

731
00:40:04,260 --> 00:40:05,110
and also

732
00:40:05,120 --> 00:40:07,760
is finding subgraphs in large graphs with people

733
00:40:07,780 --> 00:40:11,850
from that he seems like an issue but

734
00:40:11,870 --> 00:40:17,030
well shows small graphs like the unique

735
00:40:17,050 --> 00:40:18,870
and efficiently findable

736
00:40:19,300 --> 00:40:24,260
and in particular in graph theory and you simply connected pairs

737
00:40:24,280 --> 00:40:26,590
your new account simulation probably

738
00:40:28,320 --> 00:40:33,800
in first one trace of he said found itself

739
00:40:33,820 --> 00:40:36,390
now fold in order to three

740
00:40:36,410 --> 00:40:38,820
and i learned relations

741
00:40:38,850 --> 00:40:40,640
which wanted to

742
00:40:42,550 --> 00:40:43,820
that is

743
00:40:43,910 --> 00:40:45,950
the first one to spend maybe

744
00:40:46,370 --> 00:40:47,780
three or four minutes going to

745
00:40:47,800 --> 00:40:49,430
a few specific

746
00:40:50,550 --> 00:40:56,370
so we have roughly shown roughly two like this and

747
00:40:56,430 --> 00:40:57,300
this system know

748
00:40:58,140 --> 00:40:59,070
three two one

749
00:40:59,070 --> 00:41:01,410
we swear

750
00:41:01,430 --> 00:41:03,840
in fact not three squared

751
00:41:03,990 --> 00:41:10,590
experiments again taking our model systems formerly known large graph which part of the problem

752
00:41:12,300 --> 00:41:14,550
in fact i said

753
00:41:15,120 --> 00:41:16,390
and therefore the

754
00:41:16,410 --> 00:41:19,740
twenty four hundred relations between the

755
00:41:19,760 --> 00:41:25,780
four power amplification square ten square from so seven

756
00:41:25,800 --> 00:41:27,990
new accounts for twenty four

757
00:41:28,490 --> 00:41:31,070
were intended to see

758
00:41:31,140 --> 00:41:34,260
first begin breaching privacy with

759
00:41:34,320 --> 00:41:37,680
small work and so forth

760
00:41:37,780 --> 00:41:39,280
notion of a has

761
00:41:39,300 --> 00:41:44,070
one to understand what's going on the space of so what

762
00:41:45,280 --> 00:41:47,430
many of you you can do for this

763
00:41:47,530 --> 00:41:49,140
even advance preparation

764
00:41:49,160 --> 00:41:54,180
i mean the probability that the USA six your friends to

765
00:41:54,260 --> 00:41:58,990
we can be so people can actually be encouraged to ten users just because the

766
00:41:59,890 --> 00:42:01,610
you say is actually

767
00:42:02,930 --> 00:42:06,830
you need to find all the way to hotel

768
00:42:06,910 --> 00:42:10,910
the french relations so reactions leave the four fortress

769
00:42:11,070 --> 00:42:13,800
each of the sort

770
00:42:17,570 --> 00:42:21,390
so it's going to scared when he was

771
00:42:21,390 --> 00:42:28,370
one one hundred mapping rap because interesting here is that understand why small random graphs

772
00:42:28,370 --> 00:42:36,550
actually working understanding the notion of space fuzzy graphs so huge that also is from

773
00:42:36,550 --> 00:42:39,300
this last commentary school

774
00:42:39,320 --> 00:42:40,430
ramsey theory

775
00:42:40,450 --> 00:42:45,430
which has a a very extreme and it's really much from that it's

776
00:42:45,450 --> 00:42:47,280
the idea is to use

777
00:42:47,280 --> 00:42:49,510
this one one calculation here

778
00:42:49,530 --> 00:42:53,340
which essentially shows you were too long and so

779
00:42:53,390 --> 00:42:59,180
all things come through the twentieth century that exists in france

780
00:42:59,220 --> 00:43:03,490
which has no sleep and no one set size two times

781
00:43:03,760 --> 00:43:04,890
number two one

782
00:43:05,890 --> 00:43:13,910
we want to honestly connected by seven points nodes all mutually connected with the decision

783
00:43:13,910 --> 00:43:15,280
related all

784
00:43:15,300 --> 00:43:17,760
is there which essentially said one of the other class

785
00:43:17,780 --> 00:43:19,010
as in any graph

786
00:43:19,030 --> 00:43:20,590
of science about the

787
00:43:21,510 --> 00:43:22,430
it was

788
00:43:22,820 --> 00:43:26,740
in fact we need not be one two times one

789
00:43:26,760 --> 00:43:30,010
and the this population when is false

790
00:43:31,120 --> 00:43:37,010
so this profession exists in this problem you need reference and only

791
00:43:37,010 --> 00:43:38,820
just waiting

792
00:43:38,910 --> 00:43:41,390
so i'm going to take

793
00:43:41,530 --> 00:43:46,300
he said well i could actually working set

794
00:43:46,390 --> 00:43:48,030
well it's OK

795
00:43:48,090 --> 00:43:50,720
it might be more

796
00:43:50,720 --> 00:43:51,820
and to the

797
00:43:53,990 --> 00:43:56,340
well any of those

798
00:43:56,410 --> 00:43:58,090
so we

799
00:43:58,090 --> 00:44:00,740
there are species two possible relation

800
00:44:00,780 --> 00:44:02,590
and the

801
00:44:02,620 --> 00:44:04,470
so to the stage

802
00:44:04,510 --> 00:44:06,180
we're all

803
00:44:06,200 --> 00:44:07,140
that's too much

804
00:44:08,280 --> 00:44:11,990
so that's why it for

805
00:44:12,010 --> 00:44:16,120
so far too much space to any these later and

806
00:44:18,410 --> 00:44:22,530
from a huge number time there is a huge number of things that might be

807
00:44:22,590 --> 00:44:25,140
considered to be but the problem is

808
00:44:25,160 --> 00:44:27,140
it's actually trying to

809
00:44:27,490 --> 00:44:31,820
so competition between two terms and problem

810
00:44:32,640 --> 00:44:38,930
this probably just below ones can get three models students

811
00:44:40,530 --> 00:44:41,910
in this

812
00:44:41,950 --> 00:44:43,070
this happens

813
00:44:44,300 --> 00:44:47,430
well case the

814
00:44:47,820 --> 00:44:49,510
three but was forced

815
00:44:50,430 --> 00:44:52,910
it turns out that in china

816
00:44:52,930 --> 00:44:54,280
the social work

817
00:44:54,300 --> 00:44:56,990
the random

818
00:44:57,030 --> 00:44:59,910
roughly same population works

819
00:44:59,930 --> 00:45:02,950
when we got want to be unique

820
00:45:05,260 --> 00:45:07,030
other things that could be a

821
00:45:07,030 --> 00:45:10,680
we want to get the probability that two myspace

822
00:45:10,700 --> 00:45:13,760
i don't in graph is like driving

823
00:45:13,780 --> 00:45:18,160
right like drop in the ocean number possible graphs so huge it's very unlikely any

824
00:45:19,180 --> 00:45:20,340
that's really what

825
00:45:20,340 --> 00:45:25,010
and in this confusing and

826
00:45:26,800 --> 00:45:28,450
reflections on

827
00:45:28,470 --> 00:45:30,240
and i think it is

828
00:45:30,260 --> 00:45:33,050
so the

829
00:45:33,070 --> 00:45:34,840
there also even in

830
00:45:34,840 --> 00:45:36,570
well it's i completely

831
00:45:36,640 --> 00:45:41,370
graph data no individuals and that's it but they were

832
00:45:41,550 --> 00:45:42,840
no just

833
00:45:42,850 --> 00:45:46,620
when the world is not a social network of mars for any comments whatsoever such

834
00:45:46,640 --> 00:45:48,570
that the world we live

835
00:45:48,570 --> 00:45:54,870
times that say the that goes to new new would be binomially distributed variables

836
00:45:54,880 --> 00:45:56,300
and the parameters p

837
00:45:56,320 --> 00:45:58,580
corresponds to the branching ratio

838
00:45:58,680 --> 00:46:00,090
for that particular mode

839
00:46:00,190 --> 00:46:03,840
so we use that all the time for for analysing

840
00:46:03,940 --> 00:46:07,780
in the case of the particle to some specific final state

841
00:46:07,810 --> 00:46:12,570
OK now this i think we'll get a generalization of the binomial distribution to the

842
00:46:12,570 --> 00:46:13,950
case where you don't have

843
00:46:13,960 --> 00:46:19,940
two possible outcomes but n possible outcomes that's called the multinomial distribution and i'll let

844
00:46:19,940 --> 00:46:22,190
you look at that neuron

845
00:46:22,200 --> 00:46:25,500
i want to belong to the same distribution

846
00:46:25,510 --> 00:46:29,380
which is i think probably one of the most important distributions in

847
00:46:29,390 --> 00:46:32,160
particle physics and

848
00:46:32,190 --> 00:46:36,030
if you consider the binomial distribution

849
00:46:36,060 --> 00:46:39,900
in the limit where the number of trials the number of bernoulli trials becomes very

850
00:46:40,890 --> 00:46:43,780
and the number the probability of success

851
00:46:43,810 --> 00:46:45,210
per trial

852
00:46:45,220 --> 00:46:46,650
becomes very small

853
00:46:46,660 --> 00:46:49,690
but the product of those two factors remains

854
00:46:49,720 --> 00:46:53,820
equal to some finite constants which are called new greek letter mu

855
00:46:54,020 --> 00:46:59,390
that's the expectation value of the number of successes so in that limiting case

856
00:46:59,500 --> 00:47:01,070
the binomial distribution

857
00:47:01,280 --> 00:47:04,340
becomes what is called the posterior distribution

858
00:47:04,370 --> 00:47:09,300
and the number and then can range anywhere from zero up to

859
00:47:09,320 --> 00:47:12,350
any integer from zero up to infinity

860
00:47:12,380 --> 00:47:16,550
and that's the formula which is is almost important enough you should just memorize it

861
00:47:19,080 --> 00:47:23,600
would you should remember that the expectation value then is able to new that somehow

862
00:47:23,650 --> 00:47:25,390
had to emerge from the

863
00:47:25,390 --> 00:47:28,390
the limit of the binomial that we consider

864
00:47:28,460 --> 00:47:34,060
and furthermore the balance of the past so n is equal to its means

865
00:47:34,080 --> 00:47:39,950
so those are two very important problem properties of the posterior distribution

866
00:47:39,950 --> 00:47:41,350
now an example

867
00:47:41,360 --> 00:47:44,750
of the price of distribution is that if we

868
00:47:44,780 --> 00:47:46,900
well the accelerator for

869
00:47:46,950 --> 00:47:51,770
a certain amount of time with the same certain integrated luminosity we can count the

870
00:47:51,770 --> 00:47:53,060
number of

871
00:47:53,100 --> 00:47:58,690
events of some type that occur in that number will follow apart some distribution

872
00:47:58,700 --> 00:47:59,570
right so

873
00:47:59,580 --> 00:48:02,400
that means that a

874
00:48:02,410 --> 00:48:06,990
what's the distribution will be given by the integrated luminosity times the cross section

875
00:48:07,050 --> 00:48:10,440
for the event that we're considering

876
00:48:10,450 --> 00:48:13,070
and you might wonder why is that while

877
00:48:13,080 --> 00:48:15,530
i mean if you think about it if you run accelerator for a certain length

878
00:48:15,530 --> 00:48:17,830
of time to get a certain number of events

879
00:48:17,840 --> 00:48:23,240
thirty seven if you repeat the experiment with the exact same integrated luminosity you've got

880
00:48:23,240 --> 00:48:26,990
to get thirty seven events were you probably won't get thirty seven events again you

881
00:48:26,990 --> 00:48:30,610
may get thirty nine or thirty five and if you were to consider repeating the

882
00:48:30,610 --> 00:48:32,320
entire experiment

883
00:48:32,330 --> 00:48:37,430
under identical conditions this many times you get some sort of distribution and that's the

884
00:48:37,430 --> 00:48:39,070
poisson distribution the

885
00:48:39,530 --> 00:48:40,690
thank you

886
00:48:40,700 --> 00:48:44,720
cross section times the luminosity of course doesn't give you that integer number of events

887
00:48:44,720 --> 00:48:49,690
that you realize on any specific experiment that gives you the mean number of events

888
00:48:49,730 --> 00:48:54,030
but now why should russia should the number of distribution and if you think about

889
00:48:54,770 --> 00:48:59,410
when we collide together particles in a accelerator you have for example a bunch with

890
00:48:59,410 --> 00:49:03,610
ten to eleven protons and another bunch with ten to the eleven protons and you

891
00:49:03,610 --> 00:49:08,580
will allow them to pass through each other and the probability that any specific proton

892
00:49:08,590 --> 00:49:10,580
here is going to collide with

893
00:49:10,600 --> 00:49:15,310
a specific proton there introduced the event is extremely small

894
00:49:15,850 --> 00:49:19,330
on the other hand the number of potential collisions is extremely large because i have

895
00:49:19,330 --> 00:49:22,160
ten to the eleven protons here in ten of the eleven protons there

896
00:49:22,550 --> 00:49:26,220
and they're going to pass through each other and so that exactly satisfies the condition

897
00:49:26,670 --> 00:49:29,450
that i said the number of

898
00:49:29,460 --> 00:49:33,470
trials is actually extremely large and the probability of getting

899
00:49:33,480 --> 00:49:35,410
an event of the kind you're looking for

900
00:49:35,420 --> 00:49:39,790
per trial is is very small but the product of the two

901
00:49:39,820 --> 00:49:43,730
gives you whatever the mean number of events

902
00:49:43,820 --> 00:49:46,480
so we almost always model number of

903
00:49:46,500 --> 00:49:47,380
of course

904
00:49:47,400 --> 00:49:48,600
events that we see

905
00:49:48,600 --> 00:49:53,480
it's very basic to the very formulation of classical mechanics and was the great achievement

906
00:49:53,560 --> 00:49:55,230
of descartes galileo

907
00:49:55,250 --> 00:50:01,110
and newton to realize the importance of introducing this primary concept of mass

908
00:50:01,160 --> 00:50:03,460
as an example

909
00:50:04,110 --> 00:50:08,520
central equation of classical mechanics ethical zalmay which relates

910
00:50:08,530 --> 00:50:13,470
on the left-hand side of the dynamical concept of force that you

911
00:50:13,480 --> 00:50:17,540
computer make rules for

912
00:50:17,610 --> 00:50:20,660
based on the properties of particles

913
00:50:20,830 --> 00:50:23,110
their intrinsic properties

914
00:50:24,360 --> 00:50:30,120
the motion through space and time that their kinematic quantity of acceleration

915
00:50:30,210 --> 00:50:33,860
and to relate those two you need some kind of conversion factor and that's the

916
00:50:33,860 --> 00:50:37,040
role of mass plays something that didn't have mass

917
00:50:37,060 --> 00:50:40,550
something that could giving origin to mass wouldn't know which way to move

918
00:50:40,570 --> 00:50:42,560
it would be senseless object

919
00:50:42,670 --> 00:50:44,110
siemens similarly

920
00:50:44,120 --> 00:50:44,800
if you

921
00:50:44,840 --> 00:50:48,520
think about newton's law of gravity

922
00:50:48,570 --> 00:50:54,480
the gravitational force exerted by conglomeration of matter is proportional to its total mass

923
00:50:54,500 --> 00:50:57,630
if you try to build up that of things that had no mass you be

924
00:50:57,630 --> 00:51:01,240
trying to build up the gravitational force out of the sons of some of things

925
00:51:01,240 --> 00:51:03,270
all of which were zero

926
00:51:03,300 --> 00:51:05,880
so in newtonian mechanics

927
00:51:05,910 --> 00:51:10,220
and in the following development of physics really right up to the beginning of the

928
00:51:10,220 --> 00:51:14,290
twentieth century the question of the origin of mass just didn't arise

929
00:51:14,340 --> 00:51:19,500
mass was the primary concept that could be reduced to anything else

930
00:51:19,570 --> 00:51:22,200
this all changed

931
00:51:22,250 --> 00:51:24,300
at the beginning of the twentieth century

932
00:51:24,310 --> 00:51:27,200
when einstein introduced his second law

933
00:51:27,260 --> 00:51:31,020
m equals a divided by c squared

934
00:51:31,120 --> 00:51:35,600
you may think this is a curious thing to be calling the second law

935
00:51:35,630 --> 00:51:40,240
it's called the second law because there's familiar first law equals MC squared

936
00:51:43,770 --> 00:51:46,300
it's actually inspired

937
00:51:47,600 --> 00:51:51,690
army's instructional manual for radio engineers

938
00:51:51,710 --> 00:51:56,300
which has as in its first chapter owns three laws

939
00:51:56,340 --> 00:51:57,070
which are

940
00:51:57,080 --> 00:52:00,120
holmes first love equals i r

941
00:52:00,140 --> 00:52:03,090
on second law i be over r

942
00:52:04,850 --> 00:52:08,770
and the third law which i'll leave it as an exercise due to

943
00:52:11,660 --> 00:52:15,000
in this case however

944
00:52:15,010 --> 00:52:18,870
and for that matter in that case it's not entirely a joke

945
00:52:18,920 --> 00:52:24,300
because the way you write equation suggest different things

946
00:52:24,340 --> 00:52:26,720
this way of life of writing

947
00:52:26,780 --> 00:52:28,170
einstein's laws

948
00:52:28,170 --> 00:52:31,460
of the conversion of mass into energy

949
00:52:33,640 --> 00:52:37,970
perhaps it's possible to explain mass in terms of the different things that could be

950
00:52:37,970 --> 00:52:40,800
primary that is energy

951
00:52:40,800 --> 00:52:45,020
in fact i einstein's original paper on the subject

952
00:52:45,030 --> 00:52:46,990
has this form of the equation

953
00:52:47,000 --> 00:52:49,250
not equals MC squared

954
00:52:49,300 --> 00:52:52,130
and the title of the that paper

955
00:52:52,160 --> 00:52:56,160
was does the inertia of the body depend on its energy content

956
00:52:56,270 --> 00:52:57,850
right from the very beginning

957
00:52:57,940 --> 00:53:01,800
einstein was thinking about the question of whether he could

958
00:53:02,020 --> 00:53:03,710
get rid of the concept of mass

959
00:53:04,120 --> 00:53:06,260
in favour of the concept of energy

960
00:53:06,270 --> 00:53:11,140
and the theory of special relativity energy plays a much more important and

961
00:53:11,220 --> 00:53:13,750
irreducible role than mass

962
00:53:13,800 --> 00:53:16,620
for instance photons or

963
00:53:16,920 --> 00:53:24,350
he was also think about photons at that time photons or maxwell's equations have the

964
00:53:24,350 --> 00:53:25,750
concept of energy

965
00:53:25,760 --> 00:53:30,090
but not the content of the mass and them

966
00:53:30,170 --> 00:53:34,270
so i einstein from the very beginning i was thinking about the origin of mass

967
00:53:34,270 --> 00:53:38,630
not the origin of bombs which is what equals MC squared HS

968
00:53:39,850 --> 00:53:47,640
so then so now the the the big starting with that the cons the question

969
00:53:47,640 --> 00:53:51,270
of the origin of mass began to make sense

970
00:53:51,280 --> 00:53:52,920
and let's refine it

971
00:53:52,930 --> 00:53:56,430
to the question i promise to address namely the origin of the mass of ordinary

972
00:54:00,210 --> 00:54:06,870
ordinary matter has mass which is mostly overwhelmingly concentrated inside atomic nuclei

973
00:54:06,880 --> 00:54:11,380
well over ninety nine percent of the mass of ordinary matter is concentrated in atomic

974
00:54:13,240 --> 00:54:15,150
and as you learn in high school

975
00:54:16,090 --> 00:54:21,950
are made from protons and neutrons

976
00:54:21,960 --> 00:54:24,540
and as you should learn in high school

977
00:54:24,550 --> 00:54:25,930
but i'm not sure that

978
00:54:25,940 --> 00:54:28,160
everyone that has

979
00:54:28,170 --> 00:54:31,000
by now it's established just as well

980
00:54:31,010 --> 00:54:36,710
that protons and neutrons are in turn made from quarks and gluons

981
00:54:36,740 --> 00:54:38,150
and that's good

982
00:54:38,170 --> 00:54:41,100
if we want to address the origin of mass matter

983
00:54:41,170 --> 00:54:44,820
because we have marvellous theory for quarks and gluons

984
00:54:44,950 --> 00:54:47,620
quantum chromodynamics QCD

985
00:54:47,690 --> 00:54:50,040
it's a theory that's marvellous in many ways

986
00:54:50,060 --> 00:54:52,430
it's a theory that has a tremendous

987
00:54:52,440 --> 00:54:53,800
amount of symmetry

988
00:54:53,840 --> 00:54:56,810
it's based on symmetry really

989
00:54:57,550 --> 00:55:01,790
it's a theory that's very mathematically precise and has few

990
00:55:01,830 --> 00:55:04,670
very few parameters

991
00:55:04,670 --> 00:55:10,100
very few disposable parameters that's the consequence of its high degree of symmetry

992
00:55:10,340 --> 00:55:12,780
finally grand generalizations

993
00:55:12,820 --> 00:55:16,580
of quantum electrodynamics another great theory of nature

994
00:55:17,800 --> 00:55:19,300
best of all though

995
00:55:19,340 --> 00:55:20,120
is not

996
00:55:20,130 --> 00:55:25,310
that's pure that we think it's beautiful appears beautiful is consistent in fact it's the

997
00:55:25,310 --> 00:55:32,920
only really consistent theory that combines special relativity and quantum mechanics that we have

998
00:55:33,000 --> 00:55:35,010
but the best thing about it

999
00:55:35,020 --> 00:55:36,570
is that it's true

1000
00:55:36,590 --> 00:55:42,100
how do we know

1001
00:55:49,350 --> 00:55:56,420
taken from an experiment of a dominated by MIT l three experiment the web

1002
00:55:56,470 --> 00:55:59,400
it shows the body and soul of QCD

1003
00:55:59,430 --> 00:56:02,680
in a very tangible form

1004
00:56:02,720 --> 00:56:04,300
the body

1005
00:56:04,300 --> 00:56:09,530
well first let me the body are the quarks and gluons

1006
00:56:09,560 --> 00:56:12,280
so let me explain what's going on here

1007
00:56:12,300 --> 00:56:18,240
this is a picture of an event where electrons and positrons travel down

1008
00:56:19,560 --> 00:56:22,430
well that's this red can read thing here

1009
00:56:23,660 --> 00:56:30,310
they collide and the collision results in three sprays of particles which we call jets

1010
00:56:30,310 --> 00:56:33,120
coming out

1011
00:56:33,130 --> 00:56:37,580
and i want to argue that these jets are actually quarks and gluons

1012
00:56:37,690 --> 00:56:39,630
properly interpreted

1013
00:56:39,650 --> 00:56:41,770
how do we come to a conclusion like that

1014
00:56:41,830 --> 00:56:44,240
well let's put this in context

1015
00:56:44,490 --> 00:56:49,060
when one collides electrons and positrons at these high energies

1016
00:56:49,150 --> 00:56:51,380
different things can happen

1017
00:56:51,420 --> 00:56:54,300
roughly half the time what happens

1018
00:56:55,330 --> 00:56:59,420
a kind of process is governed by quantum electrodynamics

1019
00:56:59,460 --> 00:57:04,950
where you see the electron and positron colliding and out comes another electron positron pair

1020
00:57:04,950 --> 00:57:06,720
in a different direction

1021
00:57:06,720 --> 00:57:08,800
all the twenty two w

1022
00:57:08,810 --> 00:57:14,230
c by all those the the quantity that point it is that you can find

1023
00:57:15,620 --> 00:57:21,780
and i use the linearity of the inner product i do the computations

1024
00:57:21,810 --> 00:57:23,000
and i get

1025
00:57:24,290 --> 00:57:30,730
OK let's let's let's go through through the computation so

1026
00:57:30,740 --> 00:57:35,260
if you take the inner product between w and x minus c

1027
00:57:40,420 --> 00:57:45,900
so if you take this you just use the linearity of the inner product gives

1028
00:57:45,900 --> 00:57:47,270
you is

1029
00:57:48,800 --> 00:57:50,950
w is just this guy

1030
00:57:50,970 --> 00:57:55,720
so we replace w by c plus minus minus

1031
00:57:55,770 --> 00:58:00,810
and use the linearity of the inner product again and you get this

1032
00:58:00,830 --> 00:58:04,250
for his part

1033
00:58:04,260 --> 00:58:07,250
you just take again

1034
00:58:07,260 --> 00:58:08,970
the definition of w

1035
00:58:08,980 --> 00:58:15,800
the definition of c and you had is that

1036
00:58:15,810 --> 00:58:21,800
i think there is a missing one hundred something somewhere three

1037
00:58:23,890 --> 00:58:29,000
i i still again i place symbolized by its definition

1038
00:58:29,000 --> 00:58:33,590
on the top on the top of the slide c-minus by definition

1039
00:58:33,640 --> 00:58:35,050
and i get these

1040
00:58:35,100 --> 00:58:40,620
i use again the linearity of the inner product and i get that

1041
00:58:40,670 --> 00:58:43,760
x x is computed as

1042
00:58:46,120 --> 00:58:51,170
and the second very important message here is that

1043
00:58:51,220 --> 00:58:53,070
the decision function

1044
00:58:53,090 --> 00:58:54,720
it's just computed as

1045
00:58:54,720 --> 00:58:56,150
something like that

1046
00:58:56,170 --> 00:58:57,800
so you first

1047
00:58:57,800 --> 00:58:59,880
some of

1048
00:58:59,910 --> 00:59:03,080
in know product between the training patterns

1049
00:59:03,100 --> 00:59:05,450
and the test points

1050
00:59:05,590 --> 00:59:09,300
and that constant

1051
00:59:09,370 --> 00:59:14,010
and that is all there is to it if you have a linear algorithm usually

1052
00:59:14,010 --> 00:59:19,050
you end up with something like this it is very important to see that the

1053
00:59:19,050 --> 00:59:23,100
only thing that you need to know is to compute you know how to compute

1054
00:59:23,190 --> 00:59:25,110
the inner product between two

1055
00:59:25,160 --> 00:59:27,340
pairs of points

1056
00:59:27,350 --> 00:59:31,370
and once you you know how to do that you're good

1057
00:59:31,420 --> 00:59:35,490
the fact that the sort of thing that is very important to see that

1058
00:59:36,050 --> 00:59:40,840
here is the real business kind and it's very easy to compute is just set

1059
00:59:40,840 --> 00:59:42,560
c plus this

1060
00:59:42,580 --> 00:59:46,640
some of inappropriate of you know products

1061
00:59:47,960 --> 00:59:49,280
it just

1062
00:59:49,290 --> 00:59:50,790
it depends

1063
00:59:50,810 --> 00:59:53,220
on the change training data so

1064
00:59:53,240 --> 00:59:57,820
if you can compute the inner product between the training data

1065
00:59:57,840 --> 00:59:59,960
which you you know how to do

1066
00:59:59,980 --> 01:00:04,470
then you good because computer b is computing these very easy

1067
01:00:04,480 --> 01:00:10,200
and and the rest of it is just easy so

1068
01:00:10,250 --> 01:00:13,690
i intentionally used these these writing here

1069
01:00:13,700 --> 01:00:15,390
with some of the

1070
01:00:16,490 --> 01:00:18,270
and the product plus

1071
01:00:18,280 --> 01:00:23,620
b because is this is what is going going to be a very crucial points

1072
01:00:23,620 --> 01:00:25,550
of of the rest of the talk

1073
01:00:25,560 --> 01:00:26,880
and here

1074
01:00:27,130 --> 01:00:30,160
very simple very

1075
01:00:32,000 --> 01:00:33,310
handy thing

1076
01:00:33,350 --> 01:00:36,750
which is that the out for ice can be computed

1077
01:00:38,580 --> 01:00:41,370
the reason why i say that is a very simple classifiers

1078
01:00:41,470 --> 01:00:42,960
here if

1079
01:00:43,290 --> 01:00:47,960
i is the positive points that are far is

1080
01:00:48,010 --> 01:00:50,390
one divided by n plus

1081
01:00:51,410 --> 01:00:53,300
i is and negative point

1082
01:00:53,310 --> 01:00:57,940
it is this label is minus one then alpha i is equal to

1083
01:00:58,000 --> 01:00:59,860
minus one divided by

1084
01:00:59,880 --> 01:01:02,450
n minus so this only

1085
01:01:02,450 --> 01:01:03,880
these here

1086
01:01:04,020 --> 01:01:06,260
here sorry is a compact way

1087
01:01:06,260 --> 01:01:08,540
to write these here with

1088
01:01:08,550 --> 01:01:09,380
i phi

1089
01:01:09,390 --> 01:01:11,240
defined as as

1090
01:01:20,810 --> 01:01:21,900
so b

1091
01:01:21,940 --> 01:01:24,660
is computed like that

1092
01:01:26,870 --> 01:01:29,340
so if you place c

1093
01:01:29,390 --> 01:01:31,060
by definition

1094
01:01:31,110 --> 01:01:34,840
it is a little bit of a for a

1095
01:01:35,060 --> 01:01:40,150
calculations then you end up with this

1096
01:01:50,920 --> 01:01:54,670
so to summarize the important thing is

1097
01:01:56,520 --> 01:02:02,310
once you're able to compute dot products you're happy there's nothing more than that

1098
01:02:02,410 --> 01:02:06,750
because you know precisely how to compute i for i and you know precisely how

1099
01:02:06,750 --> 01:02:08,760
to compute p

1100
01:02:08,770 --> 01:02:10,980
and in order to

1101
01:02:10,990 --> 01:02:16,320
to anticipate on what i'm going to to say for SVM for for instance

1102
01:02:19,200 --> 01:02:24,200
learning phase is just intended to two

1103
01:02:24,250 --> 01:02:26,650
assign values to alfie and b

1104
01:02:26,670 --> 01:02:30,250
here is very simple here you can do it analytically if you can program it

1105
01:02:30,250 --> 01:02:35,650
you can do it in just just five five minutes you can do everything i

1106
01:02:35,650 --> 01:02:37,370
told you about

1107
01:02:37,520 --> 01:02:39,840
but for SVM

1108
01:02:39,850 --> 01:02:44,050
these are what we're going to do in the practical tomorrow's practical sessions

1109
01:02:44,060 --> 01:02:46,390
we're going to program are

1110
01:02:46,400 --> 01:02:52,960
an SVM so if you're already done that then don't come but otherwise you can

1111
01:02:55,450 --> 01:03:01,140
and the question that is very important that and that we are going to have

1112
01:03:01,140 --> 01:03:03,280
to address

1113
01:03:03,290 --> 01:03:07,330
for the these these talk is

1114
01:03:08,300 --> 01:03:12,790
if the data set at hand is not linearly separable

1115
01:03:12,800 --> 01:03:15,240
which is the case here you can try to

1116
01:03:15,250 --> 01:03:16,690
they design

1117
01:03:16,700 --> 01:03:19,820
but it here or here or here or

1118
01:03:19,880 --> 01:03:21,630
whatever you want

1119
01:03:21,740 --> 01:03:27,380
wherever you want you won't be able to separate the the the the train set

1120
01:03:27,390 --> 01:03:31,250
train sets so

1121
01:03:31,250 --> 01:03:36,610
i can see

1122
01:04:03,760 --> 01:04:05,630
that's one

1123
01:04:30,690 --> 01:04:32,870
step one

1124
01:04:32,890 --> 01:04:35,950
in one

1125
01:04:44,890 --> 01:04:49,590
one more

1126
01:04:58,910 --> 01:05:04,750
and the goal

1127
01:05:22,580 --> 01:05:26,830
all how to one

1128
01:06:17,970 --> 01:06:22,590
that is

1129
01:07:05,460 --> 01:07:08,510
of course

1130
01:07:16,330 --> 01:07:22,320
it doesn't

1131
01:07:50,090 --> 01:07:53,300
the how

1132
01:08:12,230 --> 01:08:22,500
you are

1133
01:08:22,500 --> 01:08:25,610
so in this case

1134
01:08:25,650 --> 01:08:27,320
so if i go slide back

1135
01:08:27,320 --> 01:08:32,480
you know what nodewise more likely to join because they have different squad were more

1136
01:08:32,480 --> 01:08:35,210
tightly connected inside him OK

1137
01:08:35,250 --> 01:08:40,400
so basically number connectedness of france's what it turns out this if your formalise this

1138
01:08:40,400 --> 01:08:44,900
as a an kind of prediction task as machine learning problem and you're to train

1139
01:08:44,900 --> 01:08:48,900
your decision tree what you find is that the number of connectedness of friends that

1140
01:08:48,920 --> 01:08:51,630
are already members of the community is like the most

1141
01:08:51,650 --> 01:08:56,960
the most important feature that determines whether a particular knowledge or joined the network

1142
01:08:57,860 --> 01:09:02,540
here's another question so if connectedness among friends promotes joint so

1143
01:09:02,550 --> 01:09:06,690
the highly clustered groups and groups that are highly connected

1144
01:09:06,690 --> 01:09:08,150
do they grow

1145
01:09:08,290 --> 01:09:11,440
right so if i'm more likely to join

1146
01:09:11,460 --> 01:09:16,650
the network the group the group all that people are already well connected then

1147
01:09:16,650 --> 01:09:20,820
what we should means that the groups that are more connected to grow fast

1148
01:09:20,960 --> 01:09:26,040
and what clustering is just the number of closed triads in the network over the

1149
01:09:26,040 --> 01:09:32,050
number of open triads and i want to look all to grow from p one

1150
01:09:32,050 --> 01:09:32,920
to p two

1151
01:09:32,940 --> 01:09:36,770
as a function of the number of closed triads in the group

1152
01:09:36,790 --> 01:09:41,670
and the question is how do you do more cluster groups grow growl

1153
01:09:43,400 --> 01:09:45,550
here's a plot i hope you can see it

1154
01:09:45,590 --> 01:09:50,480
i have the clustering so this would be in low clustering with friends people are

1155
01:09:50,480 --> 01:09:54,860
for people are not connected to each other or prime doesn't close and on the

1156
01:09:54,920 --> 01:09:59,340
on the right the right side is heavily clustered groups and this would be the

1157
01:10:00,400 --> 01:10:01,840
and what you can see is that

1158
01:10:01,860 --> 01:10:06,210
the growth rate decreases so we are sort of

1159
01:10:06,210 --> 01:10:08,900
having gone

1160
01:10:10,110 --> 01:10:14,820
the sort of thing contradiction right

1161
01:10:14,820 --> 01:10:17,480
no this is this is already

1162
01:10:17,480 --> 01:10:20,840
so so i hope i hope you are able to follow right so first what

1163
01:10:20,860 --> 01:10:25,190
we saw is that people do if you can if you have the same number

1164
01:10:25,190 --> 01:10:28,380
of friends in the network then you're more likely to join with your friends and

1165
01:10:28,380 --> 01:10:30,130
so what this means that now

1166
01:10:30,150 --> 01:10:33,440
node y joins the network and this continues so the

1167
01:10:33,440 --> 01:10:39,590
the the groups that are more connected should grow faster

1168
01:10:39,650 --> 01:10:46,820
so i can go who

1169
01:10:47,090 --> 01:10:52,550
so the way the way the study was done

1170
01:10:52,570 --> 01:10:56,650
is that i don't you go you're going to grow your network every week for

1171
01:10:56,750 --> 01:10:59,000
forty four years ago and now

1172
01:10:59,020 --> 01:11:00,630
now you can see

1173
01:11:00,650 --> 01:11:05,920
from between two particular times people to join the people did not join

1174
01:11:05,980 --> 01:11:06,650
right so

1175
01:11:06,670 --> 01:11:09,150
and we take a snapshot of your network every week

1176
01:11:09,190 --> 01:11:12,710
and i can say i will look for months worth of time

1177
01:11:12,710 --> 01:11:16,520
and see which people in this month joined the groups in which people

1178
01:11:16,610 --> 01:11:17,900
could jointly

1179
01:11:17,900 --> 01:11:20,650
so you have this information is the

1180
01:11:22,980 --> 01:11:25,310
so this is just a bus

1181
01:11:25,400 --> 01:11:27,170
but there are more

1182
01:11:27,210 --> 01:11:31,840
subterfuge is also going so for example via marketing one could say

1183
01:11:31,860 --> 01:11:35,710
the sending more recommendations influence more purchases right

1184
01:11:37,130 --> 01:11:40,690
this is this is this is what's going on right so i have a number

1185
01:11:41,270 --> 01:11:45,070
recommendations that a particular person sense in the number of purchases

1186
01:11:45,090 --> 01:11:46,340
the number of people

1187
01:11:46,340 --> 01:11:51,790
that purchase after receiving recommendations of this is becoming purchases a particular person in influence

1188
01:11:51,810 --> 01:11:56,270
and you can see that the first the thing grows superlinearly and then sort of

1189
01:11:56,270 --> 01:11:57,820
slows down

1190
01:11:57,880 --> 01:11:59,690
which is also something that

1191
01:11:59,690 --> 01:12:03,710
our models that can capture then a second thing that you can

1192
01:12:03,750 --> 01:12:09,570
investigate this morning asking what is this effect in the subset subsequent recommendations right so

1193
01:12:09,630 --> 01:12:14,230
if if two people exchange more and more recommendations what's going on with the probability

1194
01:12:14,230 --> 01:12:15,130
of buying

1195
01:12:15,190 --> 01:12:19,590
so here i'm here i'm blocking the number of exchanged recommendations between the pair of

1196
01:12:20,290 --> 01:12:25,520
and the probability of buying OK and what is basically says it's best if you

1197
01:12:25,520 --> 01:12:27,670
just command one product

1198
01:12:27,670 --> 01:12:30,980
two of particular friends so as you start sending

1199
01:12:30,980 --> 01:12:32,790
two particular products

1200
01:12:32,880 --> 01:12:37,500
more and more and more recommendations so you're basically spamming the purpose the

1201
01:12:37,590 --> 01:12:40,090
the probability drops and then flattens out

1202
01:12:40,150 --> 01:12:43,090
some basic

1203
01:12:43,360 --> 01:12:48,290
and this is all done over this data set the of sixteen million recommendations was

1204
01:12:48,290 --> 01:12:49,460
in fact

1205
01:12:49,460 --> 01:12:55,020
you may have no penalty whatsoever because you may have to distributions that share

1206
01:12:55,060 --> 01:12:57,440
the same huffman code

1207
01:12:58,830 --> 01:13:01,060
and that way you see that there is really

1208
01:13:01,080 --> 01:13:04,440
not necessarily any asymptotic penalty

1209
01:13:06,290 --> 01:13:12,730
you have IID distributions when n goes to infinity and the huffman codes are

1210
01:13:12,750 --> 01:13:15,790
sufficiently far apart that indeed

1211
01:13:15,810 --> 01:13:18,230
you incur in

1212
01:13:18,230 --> 01:13:20,480
in the penalty

1213
01:13:20,520 --> 01:13:24,850
now here's another one that has to do with reliable communication

1214
01:13:24,860 --> 01:13:28,400
back in shannon's original formulation

1215
01:13:28,420 --> 01:13:30,650
channel capacity

1216
01:13:30,670 --> 01:13:36,460
is the number of bits to push through the channel divided by the time the

1217
01:13:36,460 --> 01:13:39,330
time it takes to push the bits

1218
01:13:39,350 --> 01:13:47,020
through the channel now here imagine that instead of

1219
01:13:47,040 --> 01:13:50,310
instead of paying by the time you pay

1220
01:13:50,330 --> 01:13:52,350
by the lateral by the euro

1221
01:13:52,360 --> 01:13:54,210
so some symbols

1222
01:13:54,210 --> 01:13:57,480
cost you one one unit

1223
01:13:57,500 --> 01:13:59,620
and some symbols are free

1224
01:13:59,670 --> 01:14:01,270
right so

1225
01:14:01,270 --> 01:14:05,560
now what happens is that the output develops you what to send you only see

1226
01:14:05,560 --> 01:14:06,960
some distribution

1227
01:14:07,020 --> 01:14:08,670
induced by the chance

1228
01:14:08,810 --> 01:14:14,670
so so that the maximum number of bits per unit cost than that's going to

1229
01:14:16,880 --> 01:14:20,380
the relative entropy between p and q

1230
01:14:20,400 --> 01:14:21,730
so in essence

1231
01:14:23,940 --> 01:14:27,650
when new normalized by cost rather than time

1232
01:14:27,670 --> 01:14:32,900
in one of the symbols has zero cost that's important then

1233
01:14:32,920 --> 01:14:37,130
rather than having to maximize mutual information like we do in the

1234
01:14:37,210 --> 01:14:46,080
and the formula for total capacity you maximize or relative entropy

1235
01:14:46,100 --> 01:14:48,250
OK so here's is the connection with

1236
01:14:53,170 --> 01:14:55,040
that i alluded to before

1237
01:14:55,060 --> 01:15:00,900
so you so that there is this

1238
01:15:00,940 --> 01:15:06,710
this interpretation of relative entropy as mismatch data compression well there's also

1239
01:15:06,730 --> 01:15:11,290
according to this formula and is actually a very recent formula would take i got

1240
01:15:11,460 --> 01:15:17,690
i published earlier this year you can ride is relative entropy for arbitrary distributions p

1241
01:15:17,690 --> 01:15:20,080
and q

1242
01:15:21,540 --> 01:15:26,500
and integration of the difference between two mean score was

1243
01:15:26,520 --> 01:15:30,120
and then this integration is over signal to noise ratio

1244
01:15:32,000 --> 01:15:33,630
actually i think i have

1245
01:15:33,750 --> 01:15:35,620
and others line here that

1246
01:15:35,670 --> 01:15:38,690
this is in a little bit more

1247
01:15:38,710 --> 01:15:41,400
detail although i'm running out of time

1248
01:15:41,560 --> 01:15:46,670
so here we have the mean square error of course you all know it attained

1249
01:15:46,670 --> 01:15:49,460
by the conditional mean

1250
01:15:49,480 --> 01:15:54,500
right so if the noise is gaussian if i observe sex in gaussian noise then

1251
01:15:54,500 --> 01:15:58,310
i can write that conditional mean

1252
01:15:58,330 --> 01:16:00,250
as the ratio of

1253
01:16:00,270 --> 01:16:02,210
two integrals

1254
01:16:02,230 --> 01:16:04,100
but now imagine

1255
01:16:05,540 --> 01:16:09,540
i have in mind the wrong distribution for x

1256
01:16:09,560 --> 01:16:11,350
so i have

1257
01:16:11,350 --> 01:16:15,270
mismatch in my MMSE estimator

1258
01:16:15,270 --> 01:16:20,560
so this would be the case this would be the case where x is gaussian

1259
01:16:20,560 --> 01:16:25,900
in which case what you have is of course a linear estimator

1260
01:16:25,920 --> 01:16:32,120
that would be the case when x is equal then you have hyperbolic tangent

1261
01:16:32,900 --> 01:16:35,500
i'm going to denote the MMSE

1262
01:16:35,520 --> 01:16:37,810
a particular signal to noise ratio

1263
01:16:38,770 --> 01:16:41,560
this notation MMSE

1264
01:16:41,650 --> 01:16:47,900
now mismatches timation is when

1265
01:16:47,920 --> 01:16:50,020
and this and this is up q

1266
01:16:50,040 --> 01:16:54,830
tells us that the distribution and thinking is through is q

1267
01:16:54,850 --> 01:16:59,040
but in fact the x is distributed according to pt

1268
01:17:00,980 --> 01:17:05,210
in fact this q actually also depends on p because p is the truth and

1269
01:17:05,210 --> 01:17:07,520
q is what i think it is

1270
01:17:07,540 --> 01:17:09,960
all right so the MMSE would be

1271
01:17:09,980 --> 01:17:12,420
dmc one i think

1272
01:17:12,960 --> 01:17:15,980
these are two one and it is indeed the true one

1273
01:17:16,020 --> 01:17:19,380
all right so then

1274
01:17:19,380 --> 01:17:24,620
ireland during this is joint work between me

1275
01:17:25,520 --> 01:17:32,120
jiang and gentleman a so talks creating was decision trees on hadoop

1276
01:17:32,130 --> 01:17:36,460
i've had the luxury to work well if do for a couple years down

1277
01:17:36,500 --> 01:17:42,320
or sometimes it's a pain but this is kind of more of talk about the

1278
01:17:42,320 --> 01:17:48,160
process of how we got to a relatively useful implementation GBDT on

1279
01:17:48,180 --> 01:17:49,960
and i do

1280
01:17:50,080 --> 01:17:52,200
so the gender is

1281
01:17:52,210 --> 01:17:56,060
i'm going first described what GBDT is

1282
01:17:56,110 --> 01:18:04,120
globo like the different implementations that we tried and the two different ones like native

1283
01:18:04,120 --> 01:18:09,100
mapreduce one as well as one in MPI on top of hadoop

1284
01:18:09,150 --> 01:18:12,660
and i go into my reasons why we tried different versions later

1285
01:18:12,680 --> 01:18:15,250
and finally the results

1286
01:18:15,270 --> 01:18:21,480
so great was the decision trees is a machine learning algorithm that constructs

1287
01:18:21,570 --> 01:18:26,180
a ensemble of weak learners with was weak learners are decision trees

1288
01:18:26,190 --> 01:18:32,120
and still a process called boosting where the target of the tree is

1289
01:18:32,130 --> 01:18:34,540
it's based on the gradient of

1290
01:18:34,670 --> 01:18:36,710
the error in the

1291
01:18:36,720 --> 01:18:40,860
ensemble before training at tree

1292
01:18:40,870 --> 01:18:44,680
so it was first introduced by jerome friedman nineteen ninety nine

1293
01:18:46,400 --> 01:18:55,600
at stanford and it process that favors many trees but more shallow ones so typical

1294
01:18:55,600 --> 01:18:57,430
numbers they were fleeing power

1295
01:18:57,780 --> 01:19:02,010
a couple hundred trees six node tree

1296
01:19:04,020 --> 01:19:07,370
one of the reasons why you would have many trees is that

1297
01:19:07,380 --> 01:19:11,740
you actually we start with data set at the top of each tree so

1298
01:19:11,750 --> 01:19:16,800
your entire dataset starts again i mean you look into the dataset the root of

1299
01:19:16,800 --> 01:19:20,710
each tree so you don't lose any dataset as you continue

1300
01:19:20,850 --> 01:19:24,460
splitting nodes like correctly decision tree

1301
01:19:24,470 --> 01:19:30,430
and there's numerous applications of this within yahoo in terms of regression

1302
01:19:34,440 --> 01:19:36,270
the classification so

1303
01:19:36,280 --> 01:19:40,170
in terms of ranking we've had quite a bit of success with this with different

1304
01:19:40,310 --> 01:19:42,650
learners as you rank which says

1305
01:19:42,700 --> 01:19:48,800
pairwise is the pairwise loss function smoothly seaview which is like a listwise loss function

1306
01:19:48,810 --> 01:19:54,800
and recently it was used as the blender in and of course solution to the

1307
01:19:54,800 --> 01:19:59,300
netflix prize so that like how he combined different

1308
01:20:00,480 --> 01:20:03,620
and recommendation

1309
01:20:03,630 --> 01:20:10,000
so some advantages of GBDT is that feature normalisation is generally not required unlike some

1310
01:20:10,000 --> 01:20:12,590
other learning algorithms such as SVM

1311
01:20:12,610 --> 01:20:18,290
feature selection is also an inherent part of the algorithm it is the features present

1312
01:20:18,290 --> 01:20:19,810
in the tree

1313
01:20:19,890 --> 01:20:22,840
i mean it was useful and it wasn't it

1314
01:20:22,930 --> 01:20:27,890
there might have been like in near-identical future something that's also thing is not prone

1315
01:20:27,890 --> 01:20:32,360
to collinear down tentacle features

1316
01:20:32,580 --> 01:20:36,690
the models are relatively easy to interpret you can just follow the path

1317
01:20:36,700 --> 01:20:42,870
the the giving tree that might be arguable is you start having thousands of trees

1318
01:20:42,890 --> 01:20:47,070
and it's relatively easy to different loss functions

1319
01:20:47,090 --> 01:20:53,560
so some disadvantages that it's boosting right so that inherently not parallelizable if you want

1320
01:20:53,560 --> 01:20:54,940
exact solutions

1321
01:20:54,960 --> 01:20:58,130
and this extremely compute intensive

1322
01:20:58,180 --> 01:21:03,510
for every single feature every feature value we actually have the computer game to find

1323
01:21:03,520 --> 01:21:07,290
optimal split point

1324
01:21:07,340 --> 01:21:14,200
and it might perform poorly on high dimensional sparse data sets so that the amount

1325
01:21:14,200 --> 01:21:15,480
of data that you can

1326
01:21:15,530 --> 01:21:18,520
i mean how model the structure model is

1327
01:21:18,540 --> 01:21:19,750
it is based on

1328
01:21:19,760 --> 01:21:24,030
how many nodes you have an entry so if you have

1329
01:21:24,080 --> 01:21:28,170
like six no trees you have a thousand of them you only have about three

1330
01:21:28,170 --> 01:21:31,690
thousand split point to it so you have six million sparse

1331
01:21:31,740 --> 01:21:37,080
the features are not going to be able to model it

1332
01:21:37,100 --> 01:21:44,750
so some known implementations early commercially available treanor from suffered implementation in our planet from

1333
01:21:44,750 --> 01:21:50,730
google which is recent papers one and the actual implementation we built on top of

1334
01:21:50,730 --> 01:21:54,620
it is limited by time general use at YRL

1335
01:21:54,640 --> 01:22:03,330
and there's actually a couple of other implementations here at this workshop

1336
01:22:03,340 --> 01:22:09,010
so that the algorithm overview freeman's paper is as described

1337
01:22:09,750 --> 01:22:12,440
the first lies basically to initialize the

1338
01:22:12,460 --> 01:22:15,020
there you response initial response to

1339
01:22:15,030 --> 01:22:20,120
minimize loss function so if you're using something like least squares loss function the mindset

1340
01:22:20,120 --> 01:22:23,270
that to the mean of your target

1341
01:22:23,290 --> 01:22:27,620
and for every single tree you would want to

1342
01:22:27,670 --> 01:22:32,020
for every tree you would look at every sample and compute the gradient of the

1343
01:22:32,710 --> 01:22:34,840
function so between the

1344
01:22:34,900 --> 01:22:37,750
the target and the current score so far

1345
01:22:37,770 --> 01:22:43,330
and for this set of new targets you which had decision tree into

1346
01:22:43,380 --> 01:22:48,230
l terminal l terminal node decision tree where each of the responses from each of

1347
01:22:48,240 --> 01:22:49,590
the terminal nodes

1348
01:22:49,640 --> 01:22:52,000
minimizes the loss function

1349
01:22:53,420 --> 01:22:55,250
your score is

1350
01:22:55,270 --> 01:22:56,860
essentially the

1351
01:22:56,870 --> 01:22:57,800
every tree

1352
01:22:57,810 --> 01:23:08,310
plus learning rate which is new and the response to the sample and so on

1353
01:23:08,360 --> 01:23:15,250
CSO training especially for you just find our sample propagates in some of the scores

1354
01:23:15,260 --> 01:23:16,450
so the

1355
01:23:16,460 --> 01:23:21,030
learning process to break it down is essentially to

1356
01:23:21,040 --> 01:23:24,170
splitting node right so you compute with

1357
01:23:24,190 --> 01:23:25,730
for every given feature

1358
01:23:25,750 --> 01:23:30,500
the can split points what's the best split for this feature than for

1359
01:23:30,550 --> 01:23:33,310
amongst other features what is the best overall

1360
01:23:33,330 --> 01:23:36,950
split point given symmetric as well

1361
01:23:37,250 --> 01:23:39,560
information gain a

1362
01:23:39,580 --> 01:23:42,950
and what you have to split point you're partition data

1363
01:23:42,970 --> 01:23:45,890
into the left and right side and

1364
01:23:45,910 --> 01:23:50,530
then you update residuals of how close you to the target you repeat that process

1365
01:23:50,530 --> 01:23:53,070
and to construct tree

1366
01:23:53,080 --> 01:23:57,440
what you have done territory then you do gradient boosting and update target for the

1367
01:23:57,440 --> 01:23:59,750
next race

1368
01:24:01,200 --> 01:24:05,440
so for example is a free here

1369
01:24:05,450 --> 01:24:06,620
it's a like you

1370
01:24:06,620 --> 01:24:10,770
and actually put alpha and beta here found it

1371
01:24:13,740 --> 01:24:16,410
that that's what a linear function of x

1372
01:24:16,450 --> 01:24:20,120
so now consider one sort of a particular sort of linear function which is the

1373
01:24:20,120 --> 01:24:24,330
evaluation function was that what it does is it takes a function

1374
01:24:24,370 --> 01:24:27,310
and simply evaluated at the point x

1375
01:24:27,410 --> 01:24:29,620
the evaluation function of

1376
01:24:29,680 --> 01:24:31,970
six of

1377
01:24:31,990 --> 01:24:33,620
evaluated at x

1378
01:24:33,640 --> 01:24:38,350
you can see that linear because if i take f let gn evaluated at x

1379
01:24:38,390 --> 01:24:40,740
and i'll just get fx gx

1380
01:24:40,760 --> 01:24:42,240
we can of course

1381
01:24:42,240 --> 01:24:46,220
evaluation of light left less evaluation applied to g

1382
01:24:46,260 --> 01:24:48,540
OK so this is the linear function

1383
01:24:48,640 --> 01:24:51,310
and i would like this to be bounded

1384
01:24:51,330 --> 01:24:52,390
that means

1385
01:24:52,410 --> 01:24:54,580
that if i take the

1386
01:24:54,620 --> 01:24:59,790
supremum over all functions that the more or less like the maximum over all functions

1387
01:24:59,790 --> 01:25:01,270
all of this

1388
01:25:01,290 --> 01:25:05,600
divided by the norm of so every f is a member of my hilbert space

1389
01:25:05,600 --> 01:25:07,470
so it has an on

1390
01:25:07,510 --> 01:25:09,970
and evaluation s is the number

1391
01:25:09,990 --> 01:25:12,770
so take the absolute value of this number

1392
01:25:12,790 --> 01:25:15,060
divided by the norm of a function f

1393
01:25:15,080 --> 01:25:16,580
this whole thing is the number

1394
01:25:16,600 --> 01:25:20,430
and i think the supremum of this over all possible functions

1395
01:25:20,450 --> 01:25:24,580
and i want this to be finite

1396
01:25:25,620 --> 01:25:30,100
that will guarantee that if fixed point x

1397
01:25:30,160 --> 01:25:35,010
for any two functions h and g in my head but

1398
01:25:35,010 --> 01:25:37,950
if h and g are close in the norm

1399
01:25:38,990 --> 01:25:47,530
h evaluated xng evaluated x will be small

1400
01:25:47,600 --> 01:25:49,120
in other words

1401
01:25:49,160 --> 01:25:54,350
if this holds uniformly actually you can show this

1402
01:25:54,410 --> 01:25:57,740
that if d one x minus seventy two x

1403
01:25:57,790 --> 01:26:00,850
without two functions f one and f two

1404
01:26:02,060 --> 01:26:07,490
this is evaluated indexed the supreme over all x that means the max the the

1405
01:26:08,760 --> 01:26:10,490
thing on the left is actually

1406
01:26:10,510 --> 01:26:12,850
trying to say that

1407
01:26:12,890 --> 01:26:16,060
if you take fifty one x and f two x and measured its prediction of

1408
01:26:16,060 --> 01:26:17,830
the point x

1409
01:26:17,850 --> 01:26:21,120
this is the prediction of the fd one of the point x is the prediction

1410
01:26:21,120 --> 01:26:26,030
of fifty two of the point x this prediction of these two predictions are not

1411
01:26:26,060 --> 01:26:28,080
very far from each other

1412
01:26:29,010 --> 01:26:35,240
the functions are near each other in the hilbert space norm

1413
01:26:35,290 --> 01:26:36,580
that's basically

1414
01:26:37,990 --> 01:26:40,310
the intuition that we one

1415
01:26:40,450 --> 01:26:45,370
we would like this to hold uniformly overall f because i don't know what f

1416
01:26:45,370 --> 01:26:46,830
of the one is going to be

1417
01:26:46,850 --> 01:26:49,490
f of the one is going to depend upon the one and i have no

1418
01:26:49,490 --> 01:26:50,890
control over the ones

1419
01:26:50,890 --> 01:26:55,470
given an arbitrary data set d one i get f of the one arbitrator said

1420
01:26:55,470 --> 01:26:58,040
to have a for the two and then i want to make sure that this

1421
01:26:58,040 --> 01:27:00,640
distance is not one

1422
01:27:00,850 --> 01:27:04,080
is not too large

1423
01:27:04,120 --> 01:27:07,390
so this is the property that we would like additional property that we would like

1424
01:27:07,390 --> 01:27:13,290
to find both space that evaluation functionals are going to be bounded

1425
01:27:13,310 --> 01:27:16,950
and then there's the basic theorem which says that any hilbert space

1426
01:27:16,970 --> 01:27:19,870
the evaluation function is unbounded

1427
01:27:19,910 --> 01:27:26,810
is essentially a reproducing kernel hilbert space

1428
01:27:26,850 --> 01:27:28,850
so now

1429
01:27:28,870 --> 01:27:34,010
i have to tell you what a reproducing kernel hilbert spaces

1430
01:27:34,080 --> 01:27:38,290
so the development as follows that we started with linear function

1431
01:27:38,350 --> 01:27:41,490
and this all kinds of nice things we could do with linear functions

1432
01:27:41,540 --> 01:27:45,580
and now we would like to to introduce a class of nonlinear functions

1433
01:27:45,640 --> 01:27:47,910
which class of functions

1434
01:27:47,930 --> 01:27:55,260
and we asked somehow minimum properties of this rich class of functions

1435
01:27:55,310 --> 01:27:58,910
but if you have a linear structure have a hilbert space structure and then the

1436
01:27:58,910 --> 01:28:01,490
the evaluation functionals should be bounded

1437
01:28:01,490 --> 01:28:05,450
he wanted to maximize with respect to theta you

1438
01:28:05,930 --> 01:28:07,570
i wanted to push this bound

1439
01:28:07,590 --> 01:28:10,590
the likelihood as much as possible

1440
01:28:10,610 --> 01:28:12,090
given a fixed q

1441
01:28:12,110 --> 01:28:13,700
by manipulating data

1442
01:28:13,760 --> 01:28:15,140
OK so

1443
01:28:15,530 --> 01:28:21,070
this likelihood that appears really in the first term of f and doesn't appear in

1444
01:28:21,070 --> 01:28:27,680
the second term here this is the standard rewriting of f is an expected

1445
01:28:27,680 --> 01:28:29,140
energy here

1446
01:28:30,010 --> 01:28:31,890
this entropy term

1447
01:28:32,700 --> 01:28:36,450
you can see that the system only

1448
01:28:36,470 --> 01:28:40,240
i just think that the only time we can effect is this term in other

1449
01:28:40,240 --> 01:28:44,780
words the parameters given a fixed distribution over the latent variables

1450
01:28:45,860 --> 01:28:48,340
affect the free energy

1451
01:28:48,340 --> 01:28:51,970
through this term expected complete likelihood

1452
01:28:51,990 --> 01:28:58,360
and so the the first term is the expected complete log likelihood of the expected

1453
01:28:58,360 --> 01:29:02,450
energy if you want to think of log likelihood as energy and the second term

1454
01:29:02,550 --> 01:29:07,450
doesn't depend on theta it's just the entropy of q so in order to maximize

1455
01:29:07,510 --> 01:29:12,180
with respect to the need to do is to maximize this term with respect to

1456
01:29:12,200 --> 01:29:19,950
data and that turns to just be maximizing the linear combination of fully observed models

1457
01:29:21,510 --> 01:29:23,390
this expression you

1458
01:29:23,410 --> 01:29:29,360
it is the expression most often associated with the algorithm because it's the business part

1459
01:29:29,360 --> 01:29:30,860
of the m step

1460
01:29:30,870 --> 01:29:33,410
but then leads to some confusion

1461
01:29:33,450 --> 01:29:41,010
because it looks like the algorithm is all about maximizing this expected complete log likelihood

1462
01:29:41,070 --> 01:29:45,530
no the name is about maximizing this thing

1463
01:29:45,550 --> 01:29:50,200
sometimes i suspect and sometimes with respect to q and it just so happens that

1464
01:29:50,200 --> 01:29:55,910
when you maximizing with respect to it reduces to this form so don't confuse this

1465
01:29:56,030 --> 01:30:01,030
with the actual function which he m is guaranteed to maximize this is the function

1466
01:30:01,030 --> 01:30:06,110
which m is guaranteed to monotonically maximize and this is just a piece of that

1467
01:30:06,110 --> 01:30:10,660
which we actually manipulate during the m step but the good news here is if

1468
01:30:10,660 --> 01:30:14,470
you have here then this is just the problem we already know how to solve

1469
01:30:14,470 --> 01:30:16,430
this is of fully observed

1470
01:30:17,550 --> 01:30:23,410
model and we want to take the derivative of the log likelihood with respect to

1471
01:30:23,470 --> 01:30:27,280
to its maximum likelihood learning in fully observed models

1472
01:30:28,970 --> 01:30:34,180
the m step in the algorithm is just like the maximum likelihood learning in a

1473
01:30:34,180 --> 01:30:38,660
fully observed models where the model is the sort of a linear combination

1474
01:30:38,720 --> 01:30:43,780
of some of the models given by this distribution q

1475
01:30:43,800 --> 01:30:44,970
and we're now going

1476
01:30:44,970 --> 01:30:49,660
go and look at the e step to see how should we think that distribution

1477
01:30:49,760 --> 01:30:54,180
so the the e

1478
01:30:54,410 --> 01:30:58,490
it takes advantage of this claim and they're going to do something in the style

1479
01:30:58,490 --> 01:31:00,870
of mathematical proof

1480
01:31:00,890 --> 01:31:02,570
which is the style i don't like

1481
01:31:02,570 --> 01:31:06,390
but all tell you what the style the style is identical to the answer to

1482
01:31:07,970 --> 01:31:11,050
and that i will give you proof which convinces you that the answer is right

1483
01:31:11,050 --> 01:31:14,030
that gives you no idea how you would have gotten the answer if you didn't

1484
01:31:14,030 --> 01:31:15,640
already know it

1485
01:31:15,680 --> 01:31:17,890
OK so

1486
01:31:17,910 --> 01:31:24,780
my claim is that the optimum setting of humans e step is to post variables

1487
01:31:24,800 --> 01:31:28,030
given the current parameters in the data

1488
01:31:28,050 --> 01:31:32,680
and i can prove to you that the optimal setting by showing that setting that

1489
01:31:32,680 --> 01:31:38,240
choice of q actually saturates the bound it makes at equal to the likelihood and

1490
01:31:38,240 --> 01:31:41,860
since we know that that has to be strictly less than or equal to likelihood

1491
01:31:41,860 --> 01:31:46,340
there's obviously no better q than the one which saturates the bound

1492
01:31:46,990 --> 01:31:51,240
this is kind of an illuminating it may be convinced that this is right but

1493
01:31:51,240 --> 01:31:53,320
it doesn't show you why this is so

1494
01:31:53,340 --> 01:31:57,050
the real figure out that this would be the answer would be to use some

1495
01:31:57,050 --> 01:31:59,470
kind of variational calculus or

1496
01:31:59,490 --> 01:32:04,450
to rewrite these in terms of cross entropy is KL divergence is this is really

1497
01:32:04,490 --> 01:32:07,820
boltzmann distribution from statistical physics

1498
01:32:08,200 --> 01:32:10,180
so you ask yourself

1499
01:32:10,200 --> 01:32:12,070
why doesn't

1500
01:32:12,090 --> 01:32:13,260
this latter

1501
01:32:13,260 --> 01:32:15,180
which is sitting here at the table

1502
01:32:15,340 --> 01:32:21,890
why doesn't it go into the lowest energy state which would just be a crystal

1503
01:32:22,930 --> 01:32:25,030
isn't there are some rules that

1504
01:32:25,050 --> 01:32:29,340
physical systems should minimize energy

1505
01:32:29,340 --> 01:32:32,510
well of course that's not the rule the rule of the physical system should minimize

1506
01:32:32,510 --> 01:32:37,660
free energy and the reason it doesn't go into crystal state at room temperature is

1507
01:32:37,660 --> 01:32:42,740
because there's many many many many more configurations of the water in the liquid state

1508
01:32:42,740 --> 01:32:46,010
then there are viewed in a crystal state and so get a huge win on

1509
01:32:46,010 --> 01:32:47,570
the entropy term

1510
01:32:47,590 --> 01:32:51,570
it's true that the expected energy of this configuration of the water is much higher

1511
01:32:51,570 --> 01:32:55,800
than expected energy in the crystal state but there's only a few crystal configurations and

1512
01:32:55,800 --> 01:32:59,820
there is a massive number of liquid configurations and so the x two

1513
01:32:59,820 --> 01:33:03,240
cash you get back on the entropy term more than makes up for what you

1514
01:33:03,240 --> 01:33:04,740
lose energy

1515
01:33:04,760 --> 01:33:10,390
OK that's the same thing here in the algorithm right so he says it is

1516
01:33:10,410 --> 01:33:15,070
you going to put your money on low-energy configurations but it's good to have some

1517
01:33:15,070 --> 01:33:16,800
entropy in the latent variables

1518
01:33:16,800 --> 01:33:20,890
so this q the kind of trade-off between those two things is exactly what the

1519
01:33:20,890 --> 01:33:23,030
boltzmann distribution from physics would be

1520
01:33:23,050 --> 01:33:26,530
so so

1521
01:33:26,530 --> 01:33:30,860
after all that talk you need to know is that this is the choice of

1522
01:33:30,860 --> 01:33:34,840
q should make in the e step if you want the EM algorithm to work

1523
01:33:36,630 --> 01:33:41,430
this choice here is nothing more than inferring the posterior over the latent variables given

1524
01:33:41,430 --> 01:33:44,320
the data and your current parameters

1525
01:33:45,470 --> 01:33:47,720
that's the that's the main idea

1526
01:33:48,890 --> 01:33:52,640
now that we have all these pieces i can try and to convince you of

1527
01:33:52,640 --> 01:33:57,720
that link i made between the iraq intuition of the what's the right intuition well

1528
01:33:57,740 --> 01:34:02,140
guess what you think the hidden variables should be like and then some values that

1529
01:34:02,140 --> 01:34:07,470
gas to make the learning problem into the learning problem that's more likely observe learning

1530
01:34:07,490 --> 01:34:12,680
OK mean specified exactly what does it mean guess guess means compute the posterior and

1531
01:34:12,680 --> 01:34:17,590
what does it mean somehow use that distribution it means compute the expected complete log

1532
01:34:17,590 --> 01:34:23,140
likelihood in the step and optimize that and those two together are guaranteed to increase

1533
01:34:23,140 --> 01:34:27,450
the function f which is the lower bound on likelihood so here's a picture which

1534
01:34:27,450 --> 01:34:34,660
i hope you have enough information understand this picture in which the abstractly represents the

1535
01:34:34,660 --> 01:34:39,700
parameter space that we're trying to search through giving particular datasets is well conditioned on

1536
01:34:39,700 --> 01:34:43,280
some dataset you have something on your to desk this is the likelihood of the

1537
01:34:44,110 --> 01:34:47,180
and the likelihood function is now some complicated

1538
01:34:47,200 --> 01:34:49,360
multimodal page

1539
01:34:49,390 --> 01:34:53,030
and we're trying to maximize it so what do we just need to compute you

1540
01:34:53,030 --> 01:34:58,200
know start here or wherever compute the gradient and follow the gradient and get here

1541
01:34:58,220 --> 01:35:01,910
and we show that just computing the gradient involves summing over all

1542
01:35:01,930 --> 01:35:06,570
the configurations of z n you actually have to do inference now one

1543
01:35:06,590 --> 01:35:10,630
look at what is he doing in this picture of what he is doing it's

1544
01:35:10,630 --> 01:35:12,870
constructing sequential

1545
01:35:12,910 --> 01:35:16,870
convex lower bounds this function

1546
01:35:16,890 --> 01:35:19,340
well conditioned on a particular q

1547
01:35:19,360 --> 01:35:22,720
becomes convex function of data

1548
01:35:22,740 --> 01:35:26,760
that's the reason we were able to maximise its so easily in the amsterdam system

1549
01:35:26,800 --> 01:35:30,680
right because it's a linear combination of fully observed models

1550
01:35:32,590 --> 01:35:37,240
what it does is add a particular set of parameters theta that your current guess

1551
01:35:37,240 --> 01:35:44,240
at the parameters it constructs a distribution q over the latent variables that distribution q

1552
01:35:44,240 --> 01:35:48,260
induces a convex function

1553
01:35:48,260 --> 01:35:50,660
f of theta given that you q

1554
01:35:50,680 --> 01:35:54,140
which is guaranteed to be little bit more likely

1555
01:35:55,130 --> 01:35:59,110
well if you think that this is concave we can call concave

1556
01:35:59,140 --> 01:36:00,140
that's fine

1557
01:36:00,140 --> 01:36:01,010
can do this

1558
01:36:01,030 --> 01:36:03,510
in general

1559
01:36:03,530 --> 01:36:05,010
there's so

1560
01:36:05,010 --> 01:36:06,200
right you can

1561
01:36:06,220 --> 01:36:09,700
you can take this further and you can actually move to

1562
01:36:09,700 --> 01:36:13,370
reproducing kernel hilbert space you

1563
01:36:13,370 --> 01:36:18,950
don't necessarily just limit yourself to like finite dimensional representations and if you do that

1564
01:36:18,950 --> 01:36:20,620
just to sketch that here

1565
01:36:20,680 --> 01:36:22,260
basically you could

1566
01:36:22,260 --> 01:36:27,120
so ended up in the following way that you have hilbert space of functions f

1567
01:36:27,120 --> 01:36:31,260
and these functions are defined basically as

1568
01:36:31,310 --> 01:36:37,760
linear combinations over some sample set of points that's and these points are actually is

1569
01:36:37,760 --> 01:36:43,030
the sample of input output pairs OK so point here is that is really an

1570
01:36:43,030 --> 01:36:44,570
input output pair

1571
01:36:44,600 --> 01:36:48,580
and and then you have these coefficients offers it right so

1572
01:36:48,590 --> 01:36:52,170
so this is a way that you can think of

1573
01:36:52,460 --> 01:36:54,620
you know you have generating kernel

1574
01:36:56,350 --> 01:36:59,120
and basically all the things that you too

1575
01:36:59,130 --> 01:37:03,600
with end is a reproducing kernel hilbert space you can also do by just basically

1576
01:37:03,600 --> 01:37:06,120
treating of the input output pair

1577
01:37:06,130 --> 01:37:10,800
really is your input and sort of the kernel then just instead of just dependent

1578
01:37:10,800 --> 01:37:12,680
on on on the single

1579
01:37:12,710 --> 01:37:18,970
said here it just depends on this cross product so this is not

1580
01:37:18,990 --> 01:37:22,530
this is not too deep and of course

1581
01:37:23,170 --> 01:37:27,000
you know you can think of these kernels as a special case that they are

1582
01:37:27,000 --> 01:37:33,250
defined as we define it before basically as as inner products of some feature representation

1583
01:37:33,250 --> 01:37:34,750
that you extract

1584
01:37:35,950 --> 01:37:39,920
and and again you know i said this we already said this before if you

1585
01:37:39,920 --> 01:37:44,530
think it in the criminal world again right for the tensor product kernels for instance

1586
01:37:44,880 --> 01:37:52,110
you basically just get factorisation over kernels that are just defined over input pairs and

1587
01:37:52,110 --> 01:37:55,280
kind of functions that just defined over pairs

1588
01:37:55,290 --> 01:37:59,590
for something like this tensor product

1589
01:37:59,600 --> 01:38:04,850
OK so they have two purposes here really so one is what you already know

1590
01:38:04,870 --> 01:38:10,460
you know in general from kind machines that you know they are flexible and efficient

1591
01:38:10,460 --> 01:38:15,960
featureextraction they often help you to avoid the limit yourself to

1592
01:38:15,990 --> 01:38:25,130
you know finite dimensional feature representations they might help in longer much faster learning by

1593
01:38:25,130 --> 01:38:31,310
implicitly using the kernel functions instead of explicitly computing inner product in this context here

1594
01:38:31,380 --> 01:38:36,180
you also have always the problem of combining things on the input and output sides

1595
01:38:36,340 --> 01:38:40,210
which might be a problem in terms of the common the combinatorics of the combinations

1596
01:38:40,210 --> 01:38:43,810
that you actually want to look at so like in this case here right so

1597
01:38:43,810 --> 01:38:48,500
then kernels by if you have a conferences that factorizes nicely like this

1598
01:38:48,500 --> 01:38:53,210
right then that's also an advantage on the computational side that

1599
01:38:53,540 --> 01:38:56,040
a kernel approach would give you

1600
01:38:56,040 --> 01:38:58,160
OK so then

1601
01:38:58,850 --> 01:39:03,740
let me move to the story and the

1602
01:39:03,760 --> 01:39:06,090
slower than i thought let me move to

1603
01:39:06,100 --> 01:39:12,540
talk a little bit about statistical inference problems for structured prediction

1604
01:39:17,540 --> 01:39:18,670
in general

1605
01:39:18,700 --> 01:39:25,930
if you look at it without committing to a particular learning algorithm or particular formulation

1606
01:39:25,930 --> 01:39:30,390
even of the learning problem you can look at it in the most general setting

1607
01:39:30,840 --> 01:39:33,990
as follows you have a training sample s

1608
01:39:34,000 --> 01:39:39,460
your training sample these are now input output pairs right x y i and you

1609
01:39:39,460 --> 01:39:40,840
have some

1610
01:39:40,880 --> 01:39:43,390
cost function

1611
01:39:43,410 --> 01:39:45,580
our functional that takes

1612
01:39:45,590 --> 01:39:48,040
as input let's say a function from the

1613
01:39:48,090 --> 01:39:49,950
from RKHS

1614
01:39:49,970 --> 01:39:53,810
and he basically measures the goodness of fit of f right we will look at

1615
01:39:53,810 --> 01:39:56,880
different possibilities for that

1616
01:39:56,890 --> 01:40:00,540
and then we have some regularizer in there

1617
01:40:00,620 --> 01:40:04,200
typically we would use basically something that depends

1618
01:40:04,250 --> 01:40:09,030
there's a simple function of the hilbert space norm of our function f if we

1619
01:40:09,030 --> 01:40:15,550
have something like that and we want to minimize such regularized objective function with

1620
01:40:16,040 --> 01:40:21,500
func function here that depends on the training sample and this ondition stabilizer

1621
01:40:21,550 --> 01:40:26,580
then we get the following representer theorem and there's basically one thing that you need

1622
01:40:26,620 --> 01:40:32,710
to understand here that's different from the general case so basically you can show that

1623
01:40:33,000 --> 01:40:35,930
the minimiser of that have had

1624
01:40:35,950 --> 01:40:37,510
for a given sample

1625
01:40:37,530 --> 01:40:42,330
is it is the function that will can be written as the sum over all

1626
01:40:42,330 --> 01:40:47,530
your training example OK that's usual but now you have a sum over all possible

1627
01:40:47,530 --> 01:40:53,050
outputs y of in your output space and then you have to wait a better

1628
01:40:53,100 --> 01:40:58,950
i y for this depends on the training in x here i and on the

1629
01:40:58,950 --> 01:41:01,290
actual some output y

1630
01:41:01,380 --> 01:41:05,450
note that this is not just why i it's it you know you need

1631
01:41:06,720 --> 01:41:10,710
all of the otherwise basically show up in the some you need to consider and

1632
01:41:10,710 --> 01:41:13,080
then it's the kernel function

1633
01:41:13,120 --> 01:41:17,960
basically this is the argument you going to apply this to write an input output

1634
01:41:17,960 --> 01:41:22,290
pair and the second argument is basically the pair x i y

1635
01:41:22,300 --> 01:41:24,130
OK so

1636
01:41:24,140 --> 01:41:25,350
so what is

1637
01:41:25,420 --> 01:41:30,490
if you just do this in the without you output learning in the in the

1638
01:41:30,490 --> 01:41:35,040
case that you've probably seen this it'll just be a summary of training points then

1639
01:41:35,040 --> 01:41:39,670
there will be a bit i and then it will be k a blank comics

1640
01:41:39,670 --> 01:41:48,290
yes yes so row I tells you what

1641
01:41:48,290 --> 01:41:56,950
which cluster data item I belongs to and row backslash I tells you the clustering

1642
01:41:56,950 --> 01:42:03,890
of all the other data items except the Ith one is basically the same as

1643
01:42:03,890 --> 01:42:10,010
that yeah the only difference between Z and row is that zet also

1644
01:42:10,010 --> 01:42:16,450
includes not just a partition but also a labelling of each cluster of the partition

1645
01:42:16,450 --> 01:42:26,790
yeah yeah this is basically an unlabelled clustering yeah so I guess there was a question

1646
01:42:26,790 --> 01:42:32,850
just now about how do we set alpha okay we also saw that you know

1647
01:42:32,850 --> 01:42:37,310
we can derive the expectation and the variance of the number of clusters as as

1648
01:42:37,310 --> 01:42:43,250
a function of alpha and N and they kind of look both look like this and we can

1649
01:42:43,250 --> 01:42:48,570
see that so I think this is for restaurant with one hundred customers or a data

1650
01:42:48,570 --> 01:42:55,110
set with a hundred data items when alpha equals to one this distribution over the

1651
01:42:55,110 --> 01:43:02,430
number of clusters tells us that the the Chinese restaurant prior Chinese restaurant process prior

1652
01:43:02,430 --> 01:43:10,210
gives a very strong assumption about the number of clusters that are used to model

1653
01:43:10,210 --> 01:43:16,430
one hundred eight items right in this case when alpha equals to one basically this is about

1654
01:43:16,430 --> 01:43:21,850
I don't know six or seven so what is says is that it believes the prior says

1655
01:43:21,850 --> 01:43:28,690
that it believes very strongly that the number of clusters exhibited in a data set

1656
01:43:28,690 --> 01:43:33,690
of size hundred should be on the order zero to ten there should be

1657
01:43:33,690 --> 01:43:41,410
at most ten clusters okay when alpha equals to a ten then the number of clusters ranges between

1658
01:43:41,410 --> 01:43:45,770
twenty and forty and when alpha is a hundred then the number of clusters ranges

1659
01:43:45,770 --> 01:43:55,850
between sixty and eighty okay and basically this Chinese restaurant prior Chinese restaurant process

1660
01:43:55,850 --> 01:44:01,250
prior it's a very strong prior it tells even without seeing the

1661
01:44:01,250 --> 01:44:07,790
the data it is kind of saying that it knows the number of clusters

1662
01:44:07,790 --> 01:44:17,990
that is in the data set yes yes in fact in fact it will approach a

1663
01:44:17,990 --> 01:44:22,670
Gaussian in fact if will approach a Poisson which does approach a Gaussian as N goes

1664
01:44:22,670 --> 01:44:32,690
to infinity yeah so how do we set alpha we kind of have to set alpha

1665
01:44:32,690 --> 01:44:39,970
such that the number of clusters under the Chinese restaurant process reflects the number

1666
01:44:39,970 --> 01:44:43,710
of clusters we believe should be in in our data set right so if

1667
01:44:43,710 --> 01:44:48,770
we believe that there should be fifteen clusters in our data set of a size hundred

1668
01:44:48,770 --> 01:44:53,830
then we should set alpha such that it has a mean about fifteen yes

1669
01:45:08,970 --> 01:45:16,890
that's true in fact under a Dirichlet process there will be actually not

1670
01:45:16,890 --> 01:45:24,270
that many clusters which are small-sized basically up to the if you knew the

1671
01:45:24,270 --> 01:45:30,290
number of clusters it's basically kind of a gives a uniform distribution over the mixing portions among

1672
01:45:30,290 --> 01:45:36,650
that number of clusters more or less so that's kind of not as much of an effect

1673
01:45:36,650 --> 01:45:47,130
so yes yes that's right so if you do

1674
01:45:47,140 --> 01:45:51,450
believe that there's like at most ten clusters in the dataset even though you have

1675
01:45:51,450 --> 01:45:57,010
ten million data points then you shouldn't use a Dirichlet process Dirichlet model then you should

1676
01:45:57,010 --> 01:46:01,010
do something else okay in that case you do believe that it's a parametric model

1677
01:46:01,010 --> 01:46:09,670
in a sense at most ten clusters yes yes

1678
01:46:09,670 --> 01:46:14,810
yes that's right because if you really have no idea that the number

1679
01:46:14,810 --> 01:46:18,710
of clusters then you cannot set alpha to any fixed value because once you

1680
01:46:18,710 --> 01:46:23,990
fix alpha then it has a very small variance around its mean right so what

1681
01:46:24,000 --> 01:46:29,950
you could do is actually could give a prior on alpha that has

1682
01:46:29,950 --> 01:46:35,510
a kind of a wide range over all possible values of alpha so that basically

1683
01:46:35,510 --> 01:46:40,640
you remix a different values of alpha together we're gonna get a distribution over

1684
01:46:40,640 --> 01:46:44,610
a number of clusters that's a lot broader and in fact that's kind of what

1685
01:46:44,610 --> 01:46:48,250
I do in practice I kind of think about what is the minimum number of

1686
01:46:48,250 --> 01:46:53,010
clusters kind of and what's the maximum number of clusters that I expect to

1687
01:46:53,010 --> 01:46:58,790
see and then with that range I could set a range for alpha that would have high probability

1688
01:46:58,790 --> 01:47:05,850
over that range of the number of clusters yes

1689
01:47:05,850 --> 01:47:12,310
yeah so it is quite similar in the

1690
01:47:12,310 --> 01:47:16,670
sense that you have this hyper parameter which in the case of a finite mixture

1691
01:47:16,670 --> 01:47:21,590
model you have the number of clusters and in this case you have alpha okay the

1692
01:47:21,590 --> 01:47:28,790
difference is that it is and this it seems to be easier to to

1693
01:47:28,790 --> 01:47:33,710
learn about alpha in one single model than it is to learn about the

1694
01:47:33,710 --> 01:47:38,850
number of clusters in a mixture model where basically the number of clusters is this

1695
01:47:38,850 --> 01:47:44,560
discrete quantity and is a bit hard to optimize it ought to compute posterior distribution of it

1696
01:47:44,600 --> 01:47:52,110
of course there are approaches did Peter Green talk about reversible CMC no

1697
01:47:52,110 --> 01:47:58,750
okay so yes there are ways around it but it's a lot more complex than this

1698
01:47:58,930 --> 01:48:02,430
so the nice thing with this approach is that you know one model and that

1699
01:48:02,440 --> 01:48:07,530
model will tell you the number of clusters in your data while in the other

1700
01:48:07,530 --> 01:48:11,070
approach you cannot say that there are lots of different models each of which has

1701
01:48:11,070 --> 01:48:14,230
a different number of clusters and then you have to decide which which of

1702
01:48:14,230 --> 01:48:18,290
the models is best explains the data and that's kind of a more

1703
01:48:18,290 --> 01:48:24,370
complex processing it kind of has to be a double looped type of algorithm where you do learning

1704
01:48:24,370 --> 01:48:29,090
of your models in the inner loop and then determining which model to use in the

1705
01:48:29,090 --> 01:48:36,350
outer loop and that's kind of a lot more expensive to do yes

1706
01:48:51,010 --> 01:48:53,970
in this case in this kind of power-law but it's kind of a bit of

1707
01:48:53,970 --> 01:48:58,990
a degenerate power-law yeah I'll come to that actually a bit later when we talk about Pitman

1708
01:48:58,990 --> 01:49:07,930
Yor processes because those are actually give you much richer power-laws okay so that's the

1709
01:49:07,930 --> 01:49:17,510
CRP representation right actually any other questions about the this clustering CRPs

1710
01:49:17,510 --> 01:49:22,250
okay so another representation is this degrading construction and in the case of the

1711
01:49:22,250 --> 01:49:27,050
stick breaking construction the nice thing is that we can basically dissect our Dirichlet

1712
01:49:27,050 --> 01:49:29,000
and so on

1713
01:49:29,050 --> 01:49:30,410
all right

1714
01:49:30,420 --> 01:49:32,070
so here's the problem

1715
01:49:32,080 --> 01:49:33,030
and now

1716
01:49:33,040 --> 01:49:35,760
let me show you the solution

1717
01:49:36,850 --> 01:49:44,230
well let's graph function

1718
01:49:44,280 --> 01:49:47,040
so would say here's the graph

1719
01:49:47,060 --> 01:49:49,020
here's some points

1720
01:49:50,400 --> 01:49:55,910
maybe i should write lower so that i don't

1721
01:49:55,920 --> 01:49:57,680
so here's the point he

1722
01:49:57,730 --> 01:49:58,900
maybe it

1723
01:49:58,920 --> 01:50:02,450
about point zero

1724
01:50:02,460 --> 01:50:04,190
x zero by the way

1725
01:50:04,200 --> 01:50:08,370
this was supposed to be an auxiliary that was the some fixed place

1726
01:50:08,390 --> 01:50:13,370
on the x axis

1727
01:50:13,410 --> 01:50:15,190
and now

1728
01:50:15,210 --> 01:50:17,310
in order to perform this this

1729
01:50:17,330 --> 01:50:18,780
might be

1730
01:50:18,790 --> 01:50:21,300
i will

1731
01:50:21,350 --> 01:50:23,320
here's another color charge

1732
01:50:26,770 --> 01:50:27,870
so here it is

1733
01:50:27,880 --> 01:50:29,870
there's the potential

1734
01:50:29,880 --> 01:50:32,380
i went straight to no

1735
01:50:33,380 --> 01:50:34,880
i did it

1736
01:50:34,900 --> 01:50:38,760
all right that's the and that's the geometric problem i achieved

1737
01:50:38,810 --> 01:50:40,550
what i wanted to do

1738
01:50:42,840 --> 01:50:46,280
it's kind of an interesting question which unfortunately i can't

1739
01:50:46,330 --> 01:50:49,730
solve for you in this class which is how can i do that

1740
01:50:49,770 --> 01:50:52,110
that's how physically did i managed to know

1741
01:50:52,130 --> 01:50:53,170
what to do

1742
01:50:53,210 --> 01:50:56,720
to draw the tangent line but that's what geometric problems are like

1743
01:50:57,140 --> 01:51:01,900
we visualize that we can figure it out somewhere in our brains it happens

1744
01:51:01,910 --> 01:51:04,360
and the test that we have now

1745
01:51:04,400 --> 01:51:08,110
is to figure out how to do it analytically

1746
01:51:08,130 --> 01:51:10,260
to do it in a way

1747
01:51:12,940 --> 01:51:18,680
machine could do just as well as i did in drawing attention

1748
01:51:22,100 --> 01:51:25,520
so what do we learn in high school about

1749
01:51:25,570 --> 01:51:29,860
what the tangent line is well the tangent line has an equation

1750
01:51:29,900 --> 01:51:33,850
anyone who point out the equation y mynydd y zero

1751
01:51:33,860 --> 01:51:35,180
people to

1752
01:51:35,260 --> 01:51:37,640
and slovak tech

1753
01:51:38,640 --> 01:51:40,430
so here's the

1754
01:51:42,620 --> 01:51:45,380
equation for that line

1755
01:51:45,430 --> 01:51:46,360
and now

1756
01:51:46,370 --> 01:51:47,520
there are two

1757
01:51:47,540 --> 01:51:52,060
pieces of information that we're going to need to work out

1758
01:51:52,110 --> 01:51:53,860
what the line is

1759
01:51:53,880 --> 01:51:59,140
the first one is the point at that point he

1760
01:51:59,200 --> 01:52:01,290
there and to specify p

1761
01:52:01,950 --> 01:52:04,080
given x we need to know

1762
01:52:04,090 --> 01:52:08,660
the the the the level of y which is of course just like

1763
01:52:09,580 --> 01:52:13,350
it's not a calculus problem but anyway that's a very important part

1764
01:52:13,370 --> 01:52:14,790
the process

1765
01:52:14,800 --> 01:52:18,320
so that's the first thing we need to know

1766
01:52:18,340 --> 01:52:21,120
and the second thing we need to know

1767
01:52:21,210 --> 01:52:22,240
it is

1768
01:52:22,350 --> 01:52:25,650
this led

1769
01:52:25,660 --> 01:52:28,590
and that's the number and

1770
01:52:28,630 --> 01:52:32,860
and in calculus we have another name for we call it

1771
01:52:32,870 --> 01:52:34,710
after prime that zero

1772
01:52:35,660 --> 01:52:37,320
the derivative

1773
01:52:37,340 --> 01:52:41,510
of so that's the calculus for that's the tricky part and that's the part that

1774
01:52:41,510 --> 01:52:44,120
we have to discuss

1775
01:52:44,220 --> 01:52:46,890
so just to make that clear

1776
01:52:46,900 --> 01:52:49,400
explicit here i'm going to make a definition

1777
01:52:49,430 --> 01:52:53,260
which is the prime suspect zero

1778
01:52:53,300 --> 01:52:55,180
which is known as the derivative

1779
01:52:58,160 --> 01:53:01,490
of f

1780
01:53:01,500 --> 01:53:04,650
at zero

1781
01:53:06,320 --> 01:53:08,080
this slow

1782
01:53:20,430 --> 01:53:22,170
y equals back

1783
01:53:23,990 --> 01:53:25,400
the point

1784
01:53:25,450 --> 01:53:29,340
let's just call it

1785
01:53:29,520 --> 01:53:36,370
all right

1786
01:53:37,410 --> 01:53:41,130
so that's what it is

1787
01:53:41,140 --> 01:53:46,050
but still i haven't made any progress in figuring out any better how i drew

1788
01:53:46,050 --> 01:53:47,520
that line

1789
01:53:47,540 --> 01:53:48,880
so i have to

1790
01:53:48,940 --> 01:53:52,900
say something that's more concrete because i want to be able to cook up what

1791
01:53:52,900 --> 01:53:55,760
these numbers are i have to figure out what this

1792
01:53:55,880 --> 01:53:57,640
number and is

1793
01:53:57,650 --> 01:54:01,490
in one way of thinking about that and just

1794
01:54:03,300 --> 01:54:07,190
so i certainly am taking for granted and sort of non calculus part that i

1795
01:54:07,190 --> 01:54:09,220
know what line through point is

1796
01:54:09,270 --> 01:54:10,960
so i know this equation

1797
01:54:10,970 --> 01:54:13,090
another possibility might be

1798
01:54:13,130 --> 01:54:17,630
you know this line here how do i know unfortunately i don't quite straight there

1799
01:54:17,630 --> 01:54:18,670
it is

1800
01:54:18,680 --> 01:54:22,650
how do i know that this orange line is not

1801
01:54:22,700 --> 01:54:24,260
the tangent line

1802
01:54:24,300 --> 01:54:26,490
but this other line

1803
01:54:26,540 --> 01:54:31,510
its attention

1804
01:54:35,860 --> 01:54:38,060
it's actually not so obvious

1805
01:54:39,530 --> 01:54:41,760
but i'm going to

1806
01:54:41,770 --> 01:54:44,670
describe it a little bit it's not really the fact

