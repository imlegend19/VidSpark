1
00:00:00,000 --> 00:00:03,550
discoveries during the early years of x-rays astronomy we

2
00:00:03,640 --> 00:00:05,270
discovered five no

3
00:00:05,310 --> 00:00:07,720
x-ray sources so we double the the number of

4
00:00:07,730 --> 00:00:09,710
sources that were are known from

5
00:00:09,750 --> 00:00:12,290
rocket flights before us

6
00:00:12,300 --> 00:00:15,370
and some of these sources that we saw from balloons

7
00:00:15,400 --> 00:00:16,720
highly variable

8
00:00:16,720 --> 00:00:17,890
we noticed

9
00:00:17,950 --> 00:00:19,610
an x-ray flare

10
00:00:19,760 --> 00:00:23,430
x-ray intensity went up by a factor of three or four

11
00:00:23,480 --> 00:00:27,150
on a little time is ten minutes and that was completely new in those days

12
00:00:27,170 --> 00:00:31,700
and that could not have been discovered from rockets because the rockers themselves only five

13
00:00:31,700 --> 00:00:33,530
minutes above the earth's atmosphere

14
00:00:33,560 --> 00:00:36,160
and they're not looking at one source all the time

15
00:00:36,180 --> 00:00:40,030
they are scanning the sky because their objective was to find as many x-ray sources

16
00:00:40,030 --> 00:00:41,240
as they could

17
00:00:41,270 --> 00:00:45,030
but we're up sometimes twenty six hours so we have plenty of time to look

18
00:00:45,030 --> 00:00:46,830
at one portion of the sky

19
00:00:46,850 --> 00:00:49,010
for a long time for hours on

20
00:00:49,010 --> 00:00:51,890
and so it was not an accident that we discovered these

21
00:00:51,910 --> 00:00:53,470
flaring events

22
00:00:53,510 --> 00:00:57,540
this lasted up to ten minutes longer

23
00:00:57,590 --> 00:01:01,610
we also discovered an object which we called gx one plus four

24
00:01:01,620 --> 00:01:04,210
the number has to do is we're this in the sky

25
00:01:04,250 --> 00:01:08,790
and we noticed much to our surprise that the x-rays seemed to fluctuate in a

26
00:01:08,790 --> 00:01:10,310
periodic fashion

27
00:01:10,350 --> 00:01:11,850
two point three minutes

28
00:01:11,890 --> 00:01:14,250
at the time we had no clue what that meant

29
00:01:14,250 --> 00:01:16,400
but later as you will see very shortly

30
00:01:16,420 --> 00:01:17,760
it became clear

31
00:01:17,780 --> 00:01:19,730
that was the rotation period

32
00:01:19,770 --> 00:01:23,100
of a neutron star

33
00:01:23,120 --> 00:01:27,310
so the big question was in the early days what i these objects

34
00:01:27,310 --> 00:01:30,550
and this is something that we have discussed

35
00:01:30,590 --> 00:01:32,310
in a one

36
00:01:32,350 --> 00:01:35,110
and i will go over it very briefly again but we discuss it in you

37
00:01:35,110 --> 00:01:36,200
even at some

38
00:01:36,200 --> 00:01:37,920
homework problems on it

39
00:01:37,950 --> 00:01:39,930
these objects are

40
00:01:39,990 --> 00:01:42,070
x-ray binaries

41
00:01:42,090 --> 00:01:45,320
whereby by one object is very compact

42
00:01:45,370 --> 00:01:47,310
which could be a neutron star

43
00:01:47,310 --> 00:01:49,870
or in some cases even the black hole

44
00:01:49,930 --> 00:01:53,030
and the other object other star

45
00:01:53,070 --> 00:01:54,520
is a normal

46
00:01:54,570 --> 00:01:59,280
nuclear burning star something like our sun and they're very close to get

47
00:01:59,320 --> 00:02:00,980
they're so close together

48
00:02:01,810 --> 00:02:03,010
the matter

49
00:02:03,060 --> 00:02:04,500
which is here

50
00:02:04,520 --> 00:02:09,460
is attracted by the neutron star stronger than that is attracted by

51
00:02:09,500 --> 00:02:10,840
the star itself

52
00:02:10,860 --> 00:02:14,370
and so it starts to find its way to the neutron star is a binary

53
00:02:15,250 --> 00:02:16,700
so to go around each other

54
00:02:16,710 --> 00:02:18,260
this metric and not just

55
00:02:18,270 --> 00:02:19,680
going radially

56
00:02:19,700 --> 00:02:21,600
but it was filed

57
00:02:21,710 --> 00:02:25,610
slowly and find its way to the neutron star strangely enough

58
00:02:25,710 --> 00:02:27,470
but we still don't understand

59
00:02:27,500 --> 00:02:31,670
how it makes it but it does make it ultimately the neutron star and this

60
00:02:31,670 --> 00:02:33,430
is that what we call

61
00:02:33,450 --> 00:02:34,730
the accretion disk

62
00:02:35,070 --> 00:02:37,800
this is the of pretoria

63
00:02:37,840 --> 00:02:39,770
and this is the donor

64
00:02:39,860 --> 00:02:42,710
donor provides the fuel

65
00:02:42,800 --> 00:02:44,420
that finds its way

66
00:02:44,480 --> 00:02:46,580
the the neutron star

67
00:02:46,680 --> 00:02:48,650
and if you take a little bit

68
00:02:49,250 --> 00:02:55,280
you drop that on the neutron star a neutron star has mass capital and say

69
00:02:55,290 --> 00:02:57,430
and raise capital are

70
00:02:57,490 --> 00:03:01,200
then the kinetic energy that is released that impact is something that all of you

71
00:03:01,200 --> 00:03:03,360
should be able to do next monday

72
00:03:03,420 --> 00:03:04,450
that is

73
00:03:06,750 --> 00:03:10,260
g provided by or

74
00:03:10,370 --> 00:03:11,720
equals one half

75
00:03:11,720 --> 00:03:13,510
the career

76
00:03:13,580 --> 00:03:20,090
this is the gravitational potential energy that becomes available an object of mass m

77
00:03:20,120 --> 00:03:22,030
crashes onto the star

78
00:03:22,060 --> 00:03:24,770
the surface of the star has a radius capitol are

79
00:03:24,780 --> 00:03:28,730
the mass of the stars capital and that is converted to kinetic energy which is

80
00:03:28,730 --> 00:03:32,260
one half MV squared so this is the speed at impact

81
00:03:32,280 --> 00:03:34,960
of course is always independent of the land

82
00:03:35,010 --> 00:03:39,330
and so you can calculate that speed and that speed is horrendous for neutron star

83
00:03:39,390 --> 00:03:40,670
the reason being

84
00:03:40,680 --> 00:03:45,670
that the radius of the neutron star is so absurdly small is only ten kilometres

85
00:03:45,710 --> 00:03:50,630
it is roughly a hundred thousand times smaller than the radius of our sun

86
00:03:50,740 --> 00:03:54,480
the mass of the neutron star is comparable to that of our sun a little

87
00:03:54,480 --> 00:03:58,450
larger but it's comparable but it is the radius which is so small

88
00:03:58,490 --> 00:04:02,890
and that's why you get speed at impact which is about one third of the

89
00:04:02,890 --> 00:04:04,780
speed of light

90
00:04:04,870 --> 00:04:08,240
and this kinetic energy is converted to heat

91
00:04:08,280 --> 00:04:11,080
for the same reason that when we drop something here on the floor

92
00:04:11,100 --> 00:04:13,850
the kinetic energy ultimately goes into heat

93
00:04:13,900 --> 00:04:16,420
and so it it's up the surface layers

94
00:04:16,430 --> 00:04:18,040
of the neutron star

95
00:04:18,090 --> 00:04:22,630
and the temperature becomes horrendously high ten to seven ten thirty eight degrees ten million

96
00:04:22,630 --> 00:04:23,990
hundred million degrees

97
00:04:23,990 --> 00:04:25,830
and that very high temperature

98
00:04:25,860 --> 00:04:28,530
almost all the energy almost all

99
00:04:28,570 --> 00:04:32,090
electromagnetic radiation comes out in in the form of x-rays

100
00:04:32,140 --> 00:04:33,050
the song

101
00:04:33,050 --> 00:04:36,710
as the temperature of only six thousand degrees most of it comes out in the

102
00:04:36,710 --> 00:04:40,760
form of optical light but when you go to ten million degrees that's no longer

103
00:04:40,760 --> 00:04:44,120
the case the spectrum shifts in favour of the

104
00:04:46,030 --> 00:04:48,410
the amount of energy that is released is

105
00:04:48,430 --> 00:04:51,010
horrendous to give you some feeling for that

106
00:04:51,080 --> 00:04:52,870
if you take a marshmallow

107
00:04:52,880 --> 00:04:56,850
you throw marshmallow from a large distance onto the neutron star

108
00:04:57,600 --> 00:04:59,670
the energy that is released

109
00:04:59,720 --> 00:05:01,250
which is this an idea

110
00:05:01,260 --> 00:05:02,610
it's comparable

111
00:05:02,630 --> 00:05:04,750
so the energy that was released

112
00:05:04,810 --> 00:05:07,700
the atomic bomb that was thrown hiroshima

113
00:05:07,790 --> 00:05:11,160
and nagasaki that tells you something about

114
00:05:12,880 --> 00:05:15,450
gravitational forces that i at work

115
00:05:15,450 --> 00:05:17,110
on the surface of a

116
00:05:17,160 --> 00:05:19,710
neutron star

117
00:05:20,590 --> 00:05:25,120
no now what these systems on the evidence is overwhelming

118
00:05:25,130 --> 00:05:26,500
we have observed the

119
00:05:26,530 --> 00:05:31,290
rotation of the neutron stars the two point three minutes that we found we now

120
00:05:31,290 --> 00:05:34,290
know is the rotation of the neutron star

121
00:05:34,290 --> 00:05:37,240
neutron stars have strong magnetic field

122
00:05:37,290 --> 00:05:40,250
and the metadata that creates onto the neutron star

123
00:05:40,260 --> 00:05:42,470
reaches the magnetic poles

124
00:05:42,520 --> 00:05:44,400
o eight o two you will see

125
00:05:44,490 --> 00:05:46,030
you will learn why

126
00:05:46,040 --> 00:05:48,360
this last mile which is highly ionized

127
00:05:48,420 --> 00:05:51,320
but it cannot just reach the neutron star anywhere

128
00:05:51,330 --> 00:05:52,620
but it is forced to

129
00:05:52,630 --> 00:05:53,980
only enter

130
00:05:53,980 --> 00:05:56,130
the neutron star need magnetic poles

131
00:05:56,150 --> 00:06:00,550
and if the neutron star rotates than the magnetic poles can rotate like this and

132
00:06:00,550 --> 00:06:06,680
the new owners you see x-rays x-rays x-ray no x-rays x-ray so you see positions

133
00:06:06,730 --> 00:06:11,210
so these positions have been seen from any neutron stars now many of these binary

134
00:06:12,280 --> 00:06:14,880
it's very clear that it's a binary system

135
00:06:15,020 --> 00:06:18,280
if you are in the plane or near the plane

136
00:06:18,290 --> 00:06:20,520
of the orbits of the two stars

137
00:06:20,530 --> 00:06:21,960
then the neutron star

138
00:06:21,970 --> 00:06:23,800
and go behind the donor

139
00:06:23,840 --> 00:06:27,710
then you don't see any x-rays because the x-rays are then absorbed by the donor

140
00:06:27,710 --> 00:06:31,270
and there you see an x-ray eclipse only x-rays vanish

141
00:06:31,320 --> 00:06:32,760
so you would see

142
00:06:32,760 --> 00:06:34,140
people solutions

143
00:06:34,170 --> 00:06:37,470
strong x-ray signal and all of a sudden boom it's gone

144
00:06:37,480 --> 00:06:39,750
and then a few hours later it starts up again

145
00:06:39,770 --> 00:06:40,990
when the neutron star

146
00:06:41,010 --> 00:06:43,780
reappears re-emerges from the donor star

147
00:06:43,830 --> 00:06:45,310
but that picture is all

148
00:06:47,450 --> 00:06:49,050
but i don't want to show you

149
00:06:49,060 --> 00:06:50,140
at least

150
00:06:51,720 --> 00:06:53,870
of what we think such a system

151
00:06:53,870 --> 00:06:55,020
and you actually

152
00:06:56,700 --> 00:06:59,980
well the most relevant document

153
00:07:00,020 --> 00:07:01,600
actually like with high

154
00:07:02,850 --> 00:07:07,020
is the probability of any

155
00:07:07,140 --> 00:07:09,270
in the first and entity

156
00:07:09,270 --> 00:07:10,520
and this

157
00:07:10,560 --> 00:07:13,640
this probability is considered to be

158
00:07:13,640 --> 00:07:16,890
proportional to its value

159
00:07:18,200 --> 00:07:21,700
you have all the probability

160
00:07:21,700 --> 00:07:24,160
of course system entity text

161
00:07:24,180 --> 00:07:25,850
it just copies the data

162
00:07:27,620 --> 00:07:30,890
so we UK

163
00:07:32,160 --> 00:07:34,750
initially you can

164
00:07:34,810 --> 00:07:36,680
the don't

165
00:07:36,700 --> 00:07:37,850
please note

166
00:07:37,870 --> 00:07:40,350
initial probabilities

167
00:07:40,410 --> 00:07:41,580
and then

168
00:07:41,910 --> 00:07:45,020
used to perform this

169
00:07:47,750 --> 00:07:49,790
you go out

170
00:07:49,810 --> 00:07:50,680
of system

171
00:07:52,910 --> 00:07:57,450
and this is a the probability of going from one page

172
00:07:57,500 --> 00:07:59,640
and next page

173
00:07:59,660 --> 00:08:05,870
taking into account the page according to rank probability p

174
00:08:05,930 --> 00:08:09,750
of the of the p that you come

175
00:08:09,750 --> 00:08:12,950
but we're by the probability

176
00:08:12,960 --> 00:08:15,390
that you actually that the

177
00:08:16,290 --> 00:08:17,430
that link

178
00:08:17,430 --> 00:08:20,100
so is it is often

179
00:08:20,180 --> 00:08:25,790
computer and this transition is often computed uniformly

180
00:08:25,790 --> 00:08:28,230
five outgoing links

181
00:08:28,250 --> 00:08:29,560
of each

182
00:08:29,640 --> 00:08:31,290
the show

183
00:08:31,450 --> 00:08:35,660
so you get twenty percent chance of going

184
00:08:35,660 --> 00:08:39,350
two serve to the next page

185
00:08:39,370 --> 00:08:40,660
and considering

186
00:08:40,680 --> 00:08:42,790
on the beach

187
00:08:42,830 --> 00:08:44,200
that point

188
00:08:44,220 --> 00:08:46,230
two of new page

189
00:08:46,370 --> 00:08:49,680
so the point h

190
00:08:53,200 --> 00:08:54,870
o point for age

191
00:09:00,950 --> 00:09:04,160
you can of course one line also

192
00:09:04,480 --> 00:09:06,250
just actually

193
00:09:06,320 --> 00:09:09,720
randomly to another entity text

194
00:09:09,730 --> 00:09:11,500
which is considered here

195
00:09:11,520 --> 00:09:12,850
the same page

196
00:09:13,930 --> 00:09:15,580
these probabilities

197
00:09:15,580 --> 00:09:20,850
the user decides to make sure not to follow outgoing

198
00:09:20,870 --> 00:09:25,330
any more

199
00:09:25,350 --> 00:09:26,410
and not

200
00:09:26,430 --> 00:09:28,000
all the models

201
00:09:30,390 --> 00:09:32,850
if you that more

202
00:09:32,870 --> 00:09:36,020
and i know that because

203
00:09:36,040 --> 00:09:37,540
i think it's also

204
00:09:39,790 --> 00:09:41,620
to look at smaller

205
00:09:41,640 --> 00:09:44,810
because today we have various

206
00:09:44,830 --> 00:09:46,330
well we have a u

207
00:09:46,390 --> 00:09:48,430
technology here

208
00:09:48,430 --> 00:09:51,930
for making in the network

209
00:09:52,660 --> 00:09:54,520
for information retrieval

210
00:09:54,520 --> 00:09:57,250
i have not yet been explored

211
00:09:58,960 --> 00:10:02,850
nineteen ninety two turtle and cross

212
00:10:02,870 --> 00:10:06,310
published quite famous information

213
00:10:06,330 --> 00:10:08,180
the true model

214
00:10:13,250 --> 00:10:16,310
as the propagation of probabilities

215
00:10:16,480 --> 00:10:19,120
in in the network

216
00:10:19,160 --> 00:10:21,140
so you have

217
00:10:21,910 --> 00:10:25,020
consider that we the network

218
00:10:25,080 --> 00:10:27,120
where you have to note

219
00:10:27,140 --> 00:10:28,890
in graph theory

220
00:10:28,910 --> 00:10:31,480
a directed acyclic graph

221
00:10:31,500 --> 00:10:36,350
and you have to know which are present that they are you model

222
00:10:36,410 --> 00:10:44,230
the causal relationship between parents and children of which you can see also the condition

223
00:10:45,540 --> 00:10:49,350
all the world to which i don't

224
00:10:50,080 --> 00:10:54,980
you have also moved in in such a network which will can give

225
00:11:00,100 --> 00:11:03,500
we find that retrieval

226
00:11:04,350 --> 00:11:09,180
the building

227
00:11:09,230 --> 00:11:11,930
they built a document network

228
00:11:11,950 --> 00:11:15,930
which is the already for document collection

229
00:11:18,040 --> 00:11:20,960
as a directed acyclic graph

230
00:11:20,980 --> 00:11:22,500
and the document

231
00:11:24,140 --> 00:11:26,680
prior probabilities

232
00:11:27,270 --> 00:11:29,640
usually these are uniformly

233
00:11:29,790 --> 00:11:32,730
find of document collection

234
00:11:32,770 --> 00:11:36,370
but you can give a higher probability priors to

235
00:11:36,390 --> 00:11:39,500
some some set of documents if you have

236
00:11:39,600 --> 00:11:43,350
so i think this would be competing models

237
00:11:43,350 --> 00:11:47,870
and the documents in this model of to

238
00:11:47,890 --> 00:11:50,020
a document generation after

239
00:11:52,270 --> 00:11:54,730
have a certain concept

240
00:11:54,750 --> 00:11:57,930
i have here

241
00:11:57,950 --> 00:12:02,180
the strength of the things being the conditions

242
00:12:02,200 --> 00:12:04,430
probability that child

243
00:12:06,120 --> 00:12:08,180
generate give your

244
00:12:10,810 --> 00:12:12,790
and also for the way

245
00:12:12,790 --> 00:12:13,370
to ten

246
00:12:16,220 --> 00:12:19,850
directed acyclic graph

247
00:12:19,930 --> 00:12:27,120
but now use that kind of upside down the find certain concept

248
00:12:27,140 --> 00:12:29,540
that generates the query terms

249
00:12:29,540 --> 00:12:32,100
and the craters again for

250
00:12:32,120 --> 00:12:35,080
this particular class

251
00:12:35,080 --> 00:12:40,040
document retrieval then is seen as the propagation of

252
00:12:40,040 --> 00:12:43,370
probabilities into the network

253
00:12:43,700 --> 00:12:47,160
so that each document you can compute

254
00:12:47,180 --> 00:12:49,250
system probability

255
00:12:49,270 --> 00:12:51,500
of earliest c

256
00:12:51,560 --> 00:12:57,700
by putting so by putting all the other documents in a not active

257
00:12:57,720 --> 00:13:00,500
you can one by

258
00:13:00,520 --> 00:13:07,180
computer propagation of probabilities until you reach to query

259
00:13:07,200 --> 00:13:10,640
so you can do this for each document and then think

260
00:13:11,620 --> 00:13:13,700
and source document

261
00:13:13,720 --> 00:13:15,330
by the end of the

262
00:13:15,350 --> 00:13:18,100
probability of that you have compute

263
00:13:18,160 --> 00:13:19,770
for query

264
00:13:19,790 --> 00:13:21,370
at each node

265
00:13:21,370 --> 00:13:23,040
you can use

266
00:13:23,060 --> 00:13:26,020
these types of

267
00:13:26,020 --> 00:13:29,290
i didn't make too many mistakes it didn't kick me out but i'm moving on

268
00:13:30,270 --> 00:13:33,800
two two boolean which sort of implies me

269
00:13:42,210 --> 00:13:44,640
there was a question of when

270
00:13:44,690 --> 00:13:49,190
when should we have a break those suggestion having one half an hour break after

271
00:13:49,190 --> 00:13:52,020
one and a half hours is that

272
00:13:52,040 --> 00:13:55,610
sort of okay with everybody

273
00:13:56,570 --> 00:13:59,860
make it yes

274
00:13:59,900 --> 00:14:04,510
right so i'm going to talk about something which is

275
00:14:04,640 --> 00:14:06,830
the sort of a bit

276
00:14:06,910 --> 00:14:12,120
due to what john was talking about john try to explain how to

277
00:14:12,160 --> 00:14:14,970
detector spot patterns

278
00:14:15,350 --> 00:14:17,100
in data

279
00:14:17,120 --> 00:14:21,620
so you try to find dependency in data using

280
00:14:22,050 --> 00:14:27,440
several types of algorithms and i would like to

281
00:14:27,450 --> 00:14:31,300
teach you a different point of view which is also very popular now in the

282
00:14:31,300 --> 00:14:34,150
area of machine learning so we have

283
00:14:34,160 --> 00:14:37,910
some things in machine learning and other things and the other things that i'm talking

284
00:14:37,910 --> 00:14:41,660
about is probabilistic modeling so the idea is now

285
00:14:42,280 --> 00:14:44,420
you try to come up with models

286
00:14:44,430 --> 00:14:48,650
how the data are generated and of course there's a lot of uncertainty in there

287
00:14:48,670 --> 00:14:50,100
so it must be

288
00:14:50,190 --> 00:14:53,020
a model that involves probabilities

289
00:14:53,070 --> 00:14:57,860
and these models are usually adaptive they have a lot of little things to tune

290
00:14:58,360 --> 00:14:59,860
and you can tune them

291
00:14:59,970 --> 00:15:02,440
by training them on data

292
00:15:02,510 --> 00:15:09,150
and once you have the model then somehow the algorithm at least the ideal algorithm

293
00:15:09,160 --> 00:15:14,110
depending on your philosophy will automatically pop out so you don't start with an algorithm

294
00:15:14,120 --> 00:15:17,120
looking at the data you start with your ideas

295
00:15:17,240 --> 00:15:23,110
how these data are generated and then the algorithm comes

296
00:15:23,130 --> 00:15:25,560
more or less from

297
00:15:25,600 --> 00:15:30,520
ideas from statistics so let me see if i can make this running display pictures

298
00:15:30,530 --> 00:15:31,740
so yes

299
00:15:33,220 --> 00:15:37,810
as i said we're trying to bases models on

300
00:15:38,360 --> 00:15:39,400
the way

301
00:15:39,420 --> 00:15:42,500
we generate the data so-called called generative models

302
00:15:42,510 --> 00:15:44,380
i have adaptive parameters

303
00:15:45,380 --> 00:15:46,430
and then

304
00:15:46,440 --> 00:15:48,260
we can see

305
00:15:48,360 --> 00:15:52,000
it's going on

306
00:15:53,810 --> 00:15:59,890
and then we have statistical methods for learning these parameters from the training data

307
00:15:59,930 --> 00:16:03,510
and in a sense if you have a good model of course i mean that

308
00:16:03,510 --> 00:16:08,130
depends on your knowledge of model assumptions then

309
00:16:08,630 --> 00:16:13,310
well in some sense these algorithms are in some sense optimal and we are going

310
00:16:13,310 --> 00:16:15,620
to discuss in what sense they might be optimal

311
00:16:15,770 --> 00:16:18,120
and some interesting point is also

312
00:16:18,560 --> 00:16:24,080
it seems you can you could even try to generate new data or similar data

313
00:16:24,080 --> 00:16:25,460
and see

314
00:16:25,530 --> 00:16:29,560
how well how similar they would be too

315
00:16:29,660 --> 00:16:34,530
the original data so i have a very simple example o

316
00:16:34,570 --> 00:16:36,110
i got even an overview

317
00:16:36,120 --> 00:16:40,010
you never know what i have here so what i do in the beginning is

318
00:16:40,010 --> 00:16:41,780
i talk about very simple

319
00:16:41,890 --> 00:16:46,730
probabilistic models and maybe i should just before i do anything

320
00:16:46,850 --> 00:16:48,580
we have already e

321
00:16:48,600 --> 00:16:51,650
a bit of knowledge sort of in probability

322
00:16:51,670 --> 00:16:54,130
but in statistics

323
00:16:54,140 --> 00:16:56,790
well some you know just elementary probability

324
00:16:56,800 --> 00:16:58,490
coins and sting

325
00:16:58,510 --> 00:17:02,710
and from time to time again assumed density or something like that OK let's go

326
00:17:02,750 --> 00:17:05,340
so well i mean

327
00:17:05,350 --> 00:17:09,840
so this is why i love this kind of stuff it's always you know you

328
00:17:09,840 --> 00:17:16,270
should not expect that everybody likes to say way i like it right so i

329
00:17:16,270 --> 00:17:20,240
i start with a couple of simple probabilistic models and then

330
00:17:20,320 --> 00:17:27,260
talk about so-called maximum likelihood estimation for the parameter of these models and also explain linear regression

331
00:17:27,270 --> 00:17:31,340
related to gaussians

332
00:17:31,390 --> 00:17:36,470
and then i'll just talk about some more advanced examples coming up with a model

333
00:17:36,470 --> 00:17:41,100
called ICA at least a probabilistic version of well

334
00:17:41,110 --> 00:17:46,690
maximum likelihood version of it and then i'll talk about bayes estimation

335
00:17:46,740 --> 00:17:51,320
right so the first thing is the simplest model is

336
00:17:51,330 --> 00:17:53,500
well very primitive not so

337
00:17:53,510 --> 00:17:58,900
not so interesting in fact but is always start with something simpler and then build

338
00:17:58,920 --> 00:18:03,500
conformal complicated things so this is a biased coin what i mean by a biased

339
00:18:03,500 --> 00:18:08,640
coin there can be two possible outcomes zero and one of the head and to

340
00:18:10,980 --> 00:18:17,100
well he we even think that all these data are generated independently

341
00:18:17,150 --> 00:18:21,990
and we believe since it's a biased coin zero and one are not equally probable

342
00:18:21,990 --> 00:18:27,830
but it's not one have but there is a certain parameter theta which is the

343
00:18:29,020 --> 00:18:30,460
being one

344
00:18:30,830 --> 00:18:37,020
so we're interested in the question what's the probability of an entire sequence of we

345
00:18:37,020 --> 00:18:41,610
have a sequence x one two x ten and these are zeros and ones and

346
00:18:41,610 --> 00:18:46,890
can show reshaping or assumptions which way that we get to the right eventually

347
00:18:52,500 --> 00:18:55,140
OK but what we have here is that

348
00:18:55,170 --> 00:18:57,490
kernel machines can approximate anything we want

349
00:18:57,510 --> 00:19:01,220
or even the two layer neural nets for that matter can approximate any see what

350
00:19:01,450 --> 00:19:04,850
do we do but those here and say absolutely nothing about the efficiency of the

351
00:19:04,850 --> 00:19:09,030
representation with which the efficiency with which we can represent

352
00:19:09,040 --> 00:19:10,900
of those

353
00:19:10,920 --> 00:19:12,780
you know any function with

354
00:19:12,790 --> 00:19:14,560
with those things

355
00:19:14,570 --> 00:19:21,540
and for example there are there is a very good theoretical evidence that shallow architectures

356
00:19:21,580 --> 00:19:27,550
you can really implement things like very visual recognition efficiently

357
00:19:27,570 --> 00:19:32,160
so i'm going to just show flash a couple examples to use so that the

358
00:19:32,690 --> 00:19:36,640
you know some of the resulting this actually for a few here

359
00:19:36,660 --> 00:19:42,290
and you know some of the models of the some of them are shallow

360
00:19:42,310 --> 00:19:47,230
but you know i'm expect to kind of at this but basically the one that

361
00:19:47,230 --> 00:19:49,310
works best the points

362
00:19:49,330 --> 00:19:53,500
and not so work best actually train faster and faster to run at the end

363
00:19:53,510 --> 00:19:58,500
OK i'm going to show you a slightly more specific example that in a minute

364
00:19:59,240 --> 00:20:04,270
so here is the slightly more example of that and should this example queries goes

365
00:20:04,270 --> 00:20:08,740
well but merely here so let's say you want to recognise objects images that are

366
00:20:08,740 --> 00:20:10,070
roughly hundred hundred pixels

367
00:20:10,500 --> 00:20:15,670
you actually get to them because it's there which is plugged into a linear classifier

368
00:20:15,670 --> 00:20:18,440
to get thirty percent error on the test set OK with the test and so

369
00:20:18,450 --> 00:20:19,930
this is the training

370
00:20:20,000 --> 00:20:22,890
yes it is there and for each of those images you have multiple images of

371
00:20:22,890 --> 00:20:28,120
each each object so it's kind of event recognition problem i also thirty percent for

372
00:20:28,120 --> 00:20:30,380
our classifier are

373
00:20:30,390 --> 00:20:35,840
eighty to ninety percent fourteen years never something like eleven percent for the

374
00:20:35,890 --> 00:20:39,820
five hundred percent for commercial that's nice and comparison of course because the SVM has

375
00:20:39,820 --> 00:20:42,360
no idea what we know it's looking at images

376
00:20:42,380 --> 00:20:48,750
but still the difference here is between sort of show and deep

377
00:20:48,760 --> 00:20:51,500
there are differences as well of course you can always on the cards but then

378
00:20:51,500 --> 00:20:51,740
you know

379
00:20:52,160 --> 00:20:55,050
if should on the kernel you again you have a deep deep

380
00:20:57,400 --> 00:21:03,070
it gets worse if you look at the time it takes to train the system

381
00:21:03,110 --> 00:21:05,890
of odes to audition going from the screen so

382
00:21:05,900 --> 00:21:09,580
the new point if one is OK

383
00:21:11,530 --> 00:21:14,560
so this works so if you look at the time it takes to transport information

384
00:21:14,590 --> 00:21:17,870
to get to eleven point six percent takes four hundred eighty

385
00:21:17,890 --> 00:21:20,760
minutes on a sort of virtual when vigorous machine

386
00:21:22,400 --> 00:21:26,060
so that's kind of a long time to get the same accuracy comes from that

387
00:21:26,060 --> 00:21:28,360
you need sixty four

388
00:21:28,490 --> 00:21:30,120
i mean gigahertz

389
00:21:30,960 --> 00:21:34,710
on the same thing data so it's basically a times faster

390
00:21:34,720 --> 00:21:38,620
OK know put the convergence snowbound no

391
00:21:38,640 --> 00:21:42,550
you know whether that's the this of original bands

392
00:21:42,560 --> 00:21:44,300
nonconvexity certainly

393
00:21:44,660 --> 00:21:46,540
but it's faster it works better

394
00:21:46,550 --> 00:21:50,030
and if you the training for the same length of time and support vector machine

395
00:21:50,030 --> 00:21:51,670
you get happier right

396
00:21:51,760 --> 00:21:54,960
the test is even worse is about three times faster to test

397
00:21:54,970 --> 00:21:57,220
comes from that is here

398
00:21:57,240 --> 00:21:58,810
this is considerably smaller

399
00:21:58,820 --> 00:22:01,490
OK so changing the architecture

400
00:22:01,540 --> 00:22:07,740
makes a huge difference and if you have to for a convexity for this fine

401
00:22:08,250 --> 00:22:10,470
it gets even

402
00:22:10,500 --> 00:22:11,450
works are

403
00:22:11,470 --> 00:22:17,580
if you take those images that are sort of club versions of the previous ones

404
00:22:17,600 --> 00:22:21,620
truancy and we get some kind of get forty three percent error basically it doesn't

405
00:22:21,620 --> 00:22:24,790
work you know pretty much all the points are support vectors and it's not clear

406
00:22:24,790 --> 00:22:29,020
that the there was any convergence to speak of here but essentially the best implementation

407
00:22:29,020 --> 00:22:32,700
we had at the time of support vector machine which was one of actually was

408
00:22:32,700 --> 00:22:34,310
one of implementation was

409
00:22:34,860 --> 00:22:41,840
hospital aggressive because it implementation on parallel machine you can get a on that level

410
00:22:41,850 --> 00:22:45,130
commercial net five foot nine percent error

411
00:22:45,150 --> 00:22:49,590
that's actually amount of the sort of raw commercial that is less than a percent

412
00:22:49,610 --> 00:22:54,720
and the training time or even worse into something worse

413
00:22:54,740 --> 00:22:59,080
to can actually ten thousand minutes seconds to give us the wrong answer

414
00:23:00,420 --> 00:23:04,650
in about half the time the commercial against point

415
00:23:05,680 --> 00:23:07,150
enormous differences

416
00:23:07,220 --> 00:23:10,900
and with the non convex you know

417
00:23:10,920 --> 00:23:17,970
not particularly well justified theoretically methods

418
00:23:17,980 --> 00:23:22,800
so you have to be careful with

419
00:23:22,850 --> 00:23:25,970
you know with with things like that

420
00:23:31,910 --> 00:23:35,050
OK so one of the things that has become pretty clear over the last few

421
00:23:35,050 --> 00:23:40,120
years including for things like SVM and search is that people have been writing in

422
00:23:40,120 --> 00:23:43,030
terms of papers on various optimisation algorithms

423
00:23:43,040 --> 00:23:46,260
four is the and you know in the in

424
00:23:46,270 --> 00:23:49,920
but still there is as yet no one that's and those of the learning machine

425
00:23:49,920 --> 00:23:56,900
CRF on but which came out of the you know the controls work in the

426
00:23:56,900 --> 00:24:00,660
over to his work and some of some work in the in the past is

427
00:24:00,660 --> 00:24:06,570
basically a simple stochastic gradient descent beats everything by a factor of ten one hundred

428
00:24:06,580 --> 00:24:08,430
or sometimes thousands

429
00:24:10,920 --> 00:24:15,850
discover knowing because you come up with all those really sophisticated optimization methods to the

430
00:24:16,000 --> 00:24:20,810
into its and you compare it to the right method you different and it's but

431
00:24:20,810 --> 00:24:23,250
you might as well not comparative methods

432
00:24:23,270 --> 00:24:29,270
so in fact i can wasted time my life and i was in toronto suppose

433
00:24:29,280 --> 00:24:34,510
that actually implementing those optimisation algorithms like stories yes and yes yes and

434
00:24:34,680 --> 00:24:39,300
you know all kinds of cuisine using that conjugate gradient comparing them with stochastic gradient

435
00:24:39,300 --> 00:24:42,290
after six months and was this time with the time i figured which is you

436
00:24:42,320 --> 00:24:46,160
stochastic gradient surfaces shopkeeper then when you read and

437
00:24:46,380 --> 00:24:48,070
the my life and

438
00:24:48,590 --> 00:24:51,640
you know we can have to be just we discovered this and get some data

439
00:24:51,650 --> 00:24:57,050
on new learning algorithms to get convince people that that's the case

440
00:24:57,070 --> 00:25:02,540
so one problem is that

441
00:25:02,560 --> 00:25:04,860
let me see

442
00:25:04,880 --> 00:25:07,170
we want to go here

443
00:25:07,370 --> 00:25:13,030
so that's an argument for why why

444
00:25:13,050 --> 00:25:19,070
deep architectures are more efficient than the this and what kind of of you know

445
00:25:19,970 --> 00:25:22,500
however the head really too far down the

446
00:25:22,510 --> 00:25:28,870
in applying so OK so are still several strategies that people can sort of use

447
00:25:28,870 --> 00:25:33,920
in machine learning to kind of to taxes i problem or cancel the sort general

448
00:25:33,920 --> 00:25:38,680
problems this is sort of philosophical and so recycling an argument that geoff hinton has

449
00:25:38,680 --> 00:25:41,270
written in his review papers

450
00:25:41,290 --> 00:25:47,160
so it's slightly different but it's kind of paraphrasing what he said

451
00:25:47,180 --> 00:25:51,400
so one possibility to this defeatism since there is no good proposition for kind of

452
00:25:51,400 --> 00:25:55,110
really complex functions of you know we should stick to kind of

453
00:25:55,130 --> 00:26:02,830
building or feature set by hand and stick convex classifier on top and then for

454
00:26:02,830 --> 00:26:06,430
each new problem will just find a new set of features or new kernel and

455
00:26:06,450 --> 00:26:07,050
you know

456
00:26:07,060 --> 00:26:10,120
the happy with that i will keep drugs

457
00:26:10,140 --> 00:26:18,130
the second deduces is denial no correlations can approximate you want and the VC bounds

458
00:26:18,130 --> 00:26:20,880
if feature based representations

459
00:26:21,180 --> 00:26:25,370
is forty objects we are dealing with is not easy to obtain

460
00:26:25,380 --> 00:26:31,180
maybe the classical example is when the objects to be described to be cut to

461
00:26:32,010 --> 00:26:35,070
are represented in terms of graphs

462
00:26:35,130 --> 00:26:38,860
namely the objects can be decomposed into parts

463
00:26:38,890 --> 00:26:40,280
and parts

464
00:26:40,370 --> 00:26:46,390
do not happen without any relation so there is some relation between the parts and

465
00:26:46,390 --> 00:26:48,010
this means that the objects

466
00:26:48,020 --> 00:26:50,490
can be described in terms of the graph

467
00:26:50,530 --> 00:26:54,070
now once you have the graph representation for another this is

468
00:26:54,120 --> 00:26:58,050
very difficult to obtain the color presentation

469
00:26:58,090 --> 00:27:00,330
it's quite impossible to

470
00:27:00,490 --> 00:27:03,190
start from the ground which is actually

471
00:27:03,890 --> 00:27:07,630
by dimensional structure relational structure and to match

472
00:27:07,680 --> 00:27:11,300
this structure into a point in the future

473
00:27:11,930 --> 00:27:16,880
in this case it is quite difficult to obtain feature based representation

474
00:27:16,890 --> 00:27:19,900
on the other hand and this is the good news

475
00:27:19,950 --> 00:27:24,630
there are ways for computing similarities between graphs

476
00:27:24,690 --> 00:27:27,750
compute similarity between arbitrary graphs may be

477
00:27:27,760 --> 00:27:31,510
a difficult problem is NP hard in general case

478
00:27:31,520 --> 00:27:35,670
but for some instances of graphs for example trees

479
00:27:35,700 --> 00:27:39,630
the similarities between trees can be computed in polynomial time

480
00:27:39,640 --> 00:27:41,920
so in this case and lacking

481
00:27:41,930 --> 00:27:48,340
a feature space representation i can use k means for example or any other central

482
00:27:48,370 --> 00:27:55,340
but nevertheless i'm able to obtain similarities between the so what i get a similar

483
00:27:55,370 --> 00:28:02,470
OK that's the other situations where actually it is difficult to obtain a pictorial representation

484
00:28:02,470 --> 00:28:06,020
back on the other hand you may obtain quite easily

485
00:28:06,310 --> 00:28:09,890
a similarity between the objects you want to cluster

486
00:28:09,950 --> 00:28:16,230
so pairwise algorithms so that was framework does apply exactly in these cases it is

487
00:28:16,230 --> 00:28:22,620
a pairwise clustering algorithm accepts as input magics of similarities

488
00:28:22,990 --> 00:28:27,330
it doesn't it doesn't accept it doesn't know anything about

489
00:28:27,380 --> 00:28:30,430
the the way in which we are representing the objects

490
00:28:30,440 --> 00:28:35,470
they need they like the graphs or vector or whatever it just accepts as input

491
00:28:35,630 --> 00:28:37,130
matrix of

492
00:28:37,220 --> 00:28:40,720
and starting from from the similarities with try

493
00:28:40,820 --> 00:28:44,820
to partition the data according to certain coherence

494
00:28:44,820 --> 00:28:46,570
so in one sense

495
00:28:46,640 --> 00:28:50,590
pairwise clustering is more general than central class because he

496
00:28:50,650 --> 00:28:51,950
we need to know

497
00:28:51,970 --> 00:28:55,660
something essential about the way in which we are represented

498
00:28:55,690 --> 00:29:01,000
that we also need that the object be represented in terms of vectors here

499
00:29:01,090 --> 00:29:02,470
we just forget about

500
00:29:02,480 --> 00:29:04,320
in representation is OK

501
00:29:04,330 --> 00:29:07,890
provided that i have the means to compute similarities

502
00:29:09,490 --> 00:29:13,650
in this talk be talking about it was classical problem

503
00:29:14,690 --> 00:29:21,100
we accept the pairwise clustering problem algorithm accepts as input a set of n objects

504
00:29:21,310 --> 00:29:26,120
and then find a matrix of pairwise similarities and the idea of course is to

505
00:29:27,350 --> 00:29:31,450
input data into objects into maximally

506
00:29:31,450 --> 00:29:33,090
coherent groups

507
00:29:33,340 --> 00:29:37,360
so you are a very simple example we have

508
00:29:37,370 --> 00:29:38,730
i mean we can

509
00:29:38,740 --> 00:29:41,570
see there are four clusters

510
00:29:41,590 --> 00:29:43,070
so for someone

511
00:29:43,120 --> 00:29:44,240
in this case

512
00:29:44,250 --> 00:29:48,910
the by the objects are represented in terms of point in the plane

513
00:29:48,970 --> 00:29:53,570
and the distance maybe can be euclidean distance and of course we want is the

514
00:29:53,570 --> 00:29:56,870
output level

515
00:29:56,930 --> 00:29:59,750
and i said

516
00:30:00,080 --> 00:30:03,440
OK otherwise

517
00:30:03,450 --> 00:30:05,950
so we have four groups but of course

518
00:30:05,950 --> 00:30:07,910
this is a very simple case

519
00:30:07,950 --> 00:30:12,400
because there are there can be very different situations for example

520
00:30:12,450 --> 00:30:16,340
here we have a situation where four clusters

521
00:30:16,350 --> 00:30:18,000
different size

522
00:30:18,020 --> 00:30:22,600
there we have also for clusters but the difference between the second case and the

523
00:30:22,600 --> 00:30:24,550
first one is the clusters

524
00:30:24,640 --> 00:30:31,520
you have a very arbitrary shape they did not come clusters in the final example

525
00:30:31,530 --> 00:30:35,690
both with complex classes and we have an arbitrary shape so

526
00:30:35,800 --> 00:30:41,550
the problem is quite challenging and actually it's the main problem in unsupervised learning actually

527
00:30:41,570 --> 00:30:43,250
supervised learning machines

528
00:30:43,270 --> 00:30:44,890
that class

529
00:30:44,900 --> 00:30:51,120
so the problem of clustering objects into groups into coherent groups find applications in a

530
00:30:51,120 --> 00:30:58,350
number of different domains from image processing computer vision bioinformatics information retrieval and so on

531
00:30:58,350 --> 00:31:00,690
and so forth it's a very

532
00:31:00,740 --> 00:31:03,590
important problems

533
00:31:03,640 --> 00:31:05,850
so now we have

534
00:31:05,910 --> 00:31:08,820
so just start from the from the beginning

535
00:31:08,840 --> 00:31:11,580
ask a very simple question what is the cluster

536
00:31:11,590 --> 00:31:14,500
because this would be the starting point from this

537
00:31:14,580 --> 00:31:16,960
from this framework

538
00:31:16,970 --> 00:31:20,860
let me also say that

539
00:31:23,140 --> 00:31:27,860
they what clustering algorithms exist solely to start from a different question

540
00:31:27,870 --> 00:31:29,950
they basically what i'm thinking for example

541
00:31:29,950 --> 00:31:33,400
the number in which actually encounter later called

542
00:31:33,420 --> 00:31:35,160
example normalized cut

543
00:31:35,170 --> 00:31:36,980
and capt

544
00:31:37,040 --> 00:31:37,900
and right

545
00:31:37,950 --> 00:31:41,140
ectodysplasin algorithm cells

546
00:31:41,150 --> 00:31:43,500
quite popular i think so

547
00:31:44,770 --> 00:31:47,910
kind of algorithms actually start from a different question

548
00:31:47,970 --> 00:31:52,450
i give you a set of objects give you similarities between objects and then the

549
00:31:52,450 --> 00:31:55,300
question is how can i partition

550
00:31:55,370 --> 00:31:59,280
the input data the best possible way in such a way that the classes are

551
00:31:59,280 --> 00:32:00,870
maximally coherent

552
00:32:00,930 --> 00:32:04,950
so the starting question is not what is a cluster and the i to find

553
00:32:05,120 --> 00:32:10,270
a cluster and then i go ahead by defining partition

554
00:32:10,340 --> 00:32:12,320
starting from the notion

555
00:32:12,400 --> 00:32:17,120
there is that how can i partition the data and vesicles so usually what is

556
00:32:17,120 --> 00:32:20,250
known is that one defines an objective function among

557
00:32:20,310 --> 00:32:24,440
all possible partitions of data and so the idea is just

558
00:32:24,480 --> 00:32:25,590
to find

559
00:32:25,650 --> 00:32:29,710
the optimal partition according to the the objective facts

560
00:32:29,770 --> 00:32:34,160
so now we start from a different question just write one so technically this question

561
00:32:34,160 --> 00:32:35,590
what is the cluster

562
00:32:35,810 --> 00:32:37,390
there is no

563
00:32:37,430 --> 00:32:40,320
universally accepted definition of plaster

564
00:32:40,320 --> 00:32:41,730
i mean formerly

565
00:32:41,750 --> 00:32:45,860
but at least we can say that everybody will confer

566
00:32:46,670 --> 00:32:51,430
a cluster whatever it means must satisfy these two informal criteria

567
00:32:51,450 --> 00:32:56,320
internal criterion and the next most important

568
00:32:56,370 --> 00:32:58,740
before defining the notion

569
00:32:58,750 --> 00:33:01,190
the internal criterion says that

570
00:33:01,230 --> 00:33:02,870
a set of objects

571
00:33:02,950 --> 00:33:08,820
is the cluster if all the objects inside the the cluster are similar to each

572
00:33:10,150 --> 00:33:15,060
this is not enough for defining the notion of the cluster because for example if

573
00:33:15,060 --> 00:33:16,450
i take

574
00:33:16,500 --> 00:33:17,370
in this way

575
00:33:17,400 --> 00:33:19,490
if i think

576
00:33:19,540 --> 00:33:22,740
for example this cluster here

577
00:33:23,320 --> 00:33:25,320
plus here

578
00:33:25,360 --> 00:33:29,530
OK so if i think it's i strict subset proper subset of this class and

579
00:33:29,680 --> 00:33:30,900
say this one

580
00:33:30,940 --> 00:33:32,580
this is not a classification

581
00:33:32,620 --> 00:33:35,950
because it is included in the a larger coherence

582
00:33:35,990 --> 00:33:38,500
this satisfies internal criterion

583
00:33:38,560 --> 00:33:41,730
it doesn't satisfy the external criterion so in order

584
00:33:41,740 --> 00:33:46,990
four class forty four set of objects to be defined the cluster we also have

585
00:33:46,990 --> 00:33:48,080
to ask that

586
00:33:48,140 --> 00:33:53,410
all objects outside the cluster should be highly similar to the one

587
00:33:53,450 --> 00:33:57,040
the support of these two criteria actually would be

588
00:33:57,050 --> 00:34:04,230
the the basic starting point from the definition of the dominant

589
00:34:04,240 --> 00:34:10,470
so usually when we deal with the pairwise clustering problem we represent the data the

590
00:34:10,480 --> 00:34:13,810
objects to be clustered in terms of the graph

591
00:34:13,850 --> 00:34:17,860
were the vertices of the graph represents the

592
00:34:17,860 --> 00:34:22,460
the products have the property that this is negative but we don't know that yet

593
00:34:22,470 --> 00:34:23,940
so what are we going to do

594
00:34:23,970 --> 00:34:29,170
well we're going to do is we use the fact that this f is an

595
00:34:29,170 --> 00:34:32,400
element of all space so we can write this function

596
00:34:34,740 --> 00:34:36,840
in this form we know there are some

597
00:34:36,860 --> 00:34:42,050
coefficients such that the function can be written like this

598
00:34:48,860 --> 00:34:50,150
OK so we don't know

599
00:34:50,150 --> 00:34:52,900
what this function is but we know we can write it like this

600
00:34:53,320 --> 00:34:57,090
and we also know that this thing is symmetric bilinear forms and now we can

601
00:34:57,090 --> 00:35:01,240
use the linear linearity again and take the sums over the of five out

602
00:35:01,260 --> 00:35:03,090
of this spelling from again

603
00:35:03,150 --> 00:35:05,240
if we take them out

604
00:35:05,280 --> 00:35:08,460
we get this thing here

605
00:35:08,470 --> 00:35:13,460
and now

606
00:35:19,990 --> 00:35:22,900
so now we have the product between two individual kernels

607
00:35:23,070 --> 00:35:27,010
and the good thing is we know what this dot product is because by construction

608
00:35:27,260 --> 00:35:31,920
we just look at how this thing was defined

609
00:35:31,940 --> 00:35:34,030
now this is equal

610
00:35:40,570 --> 00:35:45,150
because this was what i call the reproducing kernel property before

611
00:35:45,240 --> 00:35:50,090
so we know this is equal to i phi i phi j with the kernel

612
00:35:50,090 --> 00:35:51,220
matrix here

613
00:35:51,550 --> 00:35:54,780
and since we have started with the positive definite kernel and that's the first page

614
00:35:54,780 --> 00:35:58,700
where we're going to use the fact that the kernel is positive definite we know

615
00:35:58,720 --> 00:36:01,470
that this is lower bounded by zero

616
00:36:01,490 --> 00:36:06,070
so therefore we also know that this is bounded from below by zero

617
00:36:06,090 --> 00:36:10,760
therefore we know that this thing with the angular brackets that have called asymmetric piling

618
00:36:10,760 --> 00:36:15,300
up form so far this thing is a positive definite kernel

619
00:36:15,320 --> 00:36:19,460
OK so that's the next thing know we have some additional tools because we have

620
00:36:19,470 --> 00:36:23,530
proven certain properties of positive definite kernels and we're going to

621
00:36:23,690 --> 00:36:25,860
let them loose on this problem

622
00:36:28,010 --> 00:36:29,300
we're almost there

623
00:36:36,800 --> 00:36:43,320
this is not movable

624
00:36:43,340 --> 00:36:45,190
we have

625
00:37:07,670 --> 00:37:09,490
so now we have two

626
00:37:09,510 --> 00:37:13,740
can we know it's a positive definite kernel nice we know it's by linear but

627
00:37:13,740 --> 00:37:17,380
for for this thing to be at the product we have to show that it

628
00:37:17,380 --> 00:37:22,090
strictly positive definite strictly define it in the sense that

629
00:37:22,090 --> 00:37:25,170
whenever the dot product we have to show

630
00:37:25,170 --> 00:37:32,320
whenever this thing of function with itself itself is zero then the function must be

631
00:37:32,340 --> 00:37:35,240
equal to zero so only the zero function

632
00:37:35,240 --> 00:37:39,050
has this property that the dot product with itself is zero

633
00:37:39,050 --> 00:37:40,860
that's what's left to show

634
00:37:40,880 --> 00:37:45,340
well the rest we already know we know it's symmetric it's palinites nonnegative and so

635
00:37:45,340 --> 00:37:49,820
on and so very actually nonnegative it doesn't have to be

636
00:37:51,030 --> 00:37:54,170
so we have to show this thing

637
00:37:54,190 --> 00:37:56,150
and how do we show this

638
00:37:56,260 --> 00:38:00,320
OK we're going to use some of the properties that we had before

639
00:38:00,340 --> 00:38:03,670
so in order to show the function is zero we have to show that is

640
00:38:03,670 --> 00:38:05,920
zero at every point

641
00:38:05,970 --> 00:38:11,300
let's actually square this thing because then we can use the cushy schwartz inequality

642
00:38:12,490 --> 00:38:14,650
OK so the first thing is

643
00:38:15,070 --> 00:38:17,470
we're going to rewrite this

644
00:38:18,530 --> 00:38:21,690
the kernel between

645
00:38:21,690 --> 00:38:23,760
o point into to the next

646
00:38:23,860 --> 00:38:25,220
and the function

647
00:38:27,190 --> 00:38:32,740
so that's a special case of of the dot product definition that we had before

648
00:38:32,760 --> 00:38:34,240
so if you

649
00:38:34,380 --> 00:38:36,510
look up here

650
00:38:36,530 --> 00:38:44,720
so let's let's turn that say this is the function f that

651
00:38:44,740 --> 00:38:50,490
cut out some of tasks and only evaluated one specific points so we take the

652
00:38:50,490 --> 00:38:55,050
dot product between a function f and one individual kernel also so we remove this

653
00:38:55,070 --> 00:38:58,420
and the bit is also removed here

654
00:38:58,420 --> 00:39:00,200
OK here will

655
00:39:00,250 --> 00:39:02,790
right OK so when starting

656
00:39:02,800 --> 00:39:04,510
so i'm so are

657
00:39:04,530 --> 00:39:07,530
so my

658
00:39:07,570 --> 00:39:08,680
my field of

659
00:39:08,690 --> 00:39:11,400
researches on logic not only

660
00:39:11,430 --> 00:39:15,760
it's mainly on what were former methods faustus software

661
00:39:17,150 --> 00:39:21,870
and logic the is very very important to them because it is used as the

662
00:39:21,880 --> 00:39:27,970
specification language to describe the properties that you want to the to verify and assist

663
00:39:33,480 --> 00:39:35,940
so that

664
00:39:35,970 --> 00:39:39,550
the lecture here is about on

665
00:39:39,610 --> 00:39:43,120
very expressive and the gene which is the mu calculus

666
00:39:46,070 --> 00:39:47,680
can work yes

667
00:39:47,720 --> 00:39:50,820
and the connection well the way too

668
00:39:50,830 --> 00:39:51,940
let's say

669
00:39:51,950 --> 00:39:54,080
handle muc whose

670
00:39:54,110 --> 00:39:57,900
in the automated manner because this is the

671
00:39:57,900 --> 00:40:00,800
the purpose of form verification is two

672
00:40:00,820 --> 00:40:02,860
we cannot derive automatic

673
00:40:02,870 --> 00:40:04,330
automated methods

674
00:40:04,340 --> 00:40:08,090
to verify the properties on objects that are programs

675
00:40:08,230 --> 00:40:11,950
so you have to think you have do not make it to the you know

676
00:40:11,970 --> 00:40:14,750
the programs that are

677
00:40:14,750 --> 00:40:21,730
i mean described in programming language and we extract programs to derive operational models you've

678
00:40:21,730 --> 00:40:27,590
already seen some of them i believe new lectures like key conspirators for example

679
00:40:27,590 --> 00:40:31,030
and we've seen a lot of the trees

680
00:40:31,040 --> 00:40:35,970
when you have the models and you have the specification language given the logic

681
00:40:36,030 --> 00:40:37,940
you have to connect both

682
00:40:37,950 --> 00:40:40,530
nine does the model that they have here

683
00:40:40,610 --> 00:40:42,470
satisfies for example

684
00:40:42,480 --> 00:40:46,220
the formula that even in this language all

685
00:40:46,230 --> 00:40:47,610
the idea is to have

686
00:40:47,610 --> 00:40:50,120
a mathematical clean mathematical setting

687
00:40:50,140 --> 00:40:52,110
to deal with these things

688
00:40:52,120 --> 00:40:53,480
so now

689
00:40:53,500 --> 00:41:07,860
what does he wants this one

690
00:41:07,930 --> 00:41:10,460
the problem is that it's

691
00:41:13,390 --> 00:41:16,170
so you're lucky because i cannot start

692
00:41:16,180 --> 00:41:22,450
for the better this when the

693
00:41:22,450 --> 00:41:24,510
it's common

694
00:41:37,040 --> 00:41:40,600
great hope now the

695
00:41:58,670 --> 00:42:14,300
which is the

696
00:42:14,420 --> 00:42:17,290
this morning when i read it was finally

697
00:42:26,300 --> 00:42:28,630
so here we are

698
00:42:28,860 --> 00:42:31,920
right so now we can start so the

699
00:42:31,930 --> 00:42:33,760
title of the of the lecture

700
00:42:33,790 --> 00:42:37,850
it is about them in the connection that one can do between logic

701
00:42:40,480 --> 00:42:45,380
these are intimidated tools that enable to handle the

702
00:42:45,430 --> 00:42:47,800
manipulate logical formulas

703
00:42:48,730 --> 00:42:50,700
how to reason the to matter

704
00:42:50,790 --> 00:42:52,480
by using

705
00:42:53,480 --> 00:42:55,520
two was OK

706
00:42:55,540 --> 00:42:57,580
so feel free to interact of course

707
00:42:57,600 --> 00:42:59,510
ask questions i have

708
00:43:01,700 --> 00:43:04,570
keep more precise precise details on

709
00:43:04,580 --> 00:43:07,200
what i'm telling you

710
00:43:07,210 --> 00:43:08,450
so the

711
00:43:08,490 --> 00:43:12,430
that line would be very simple we briefly

712
00:43:12,450 --> 00:43:13,140
i mean

713
00:43:13,160 --> 00:43:17,150
i to remember hollywood use logic for programs

714
00:43:17,160 --> 00:43:22,810
then we will have to focus on a particular logic the most expressive one one

715
00:43:22,810 --> 00:43:25,620
can think of in the way for

716
00:43:27,070 --> 00:43:31,210
operational aspects of programs so that's new calculus

717
00:43:31,420 --> 00:43:32,480
then we'll will see

718
00:43:34,020 --> 00:43:35,910
as tools really connected

719
00:43:35,920 --> 00:43:39,750
so it will not be clear at the beginning how to model connected to this

720
00:43:39,750 --> 00:43:43,210
logic but that would be the purpose of

721
00:43:44,540 --> 00:43:48,990
the last part here when we go from the mu calculus the alternating parity tree

722
00:43:48,990 --> 00:43:51,240
automata special kind of automata

723
00:43:52,670 --> 00:43:53,770
vice versa

724
00:43:53,860 --> 00:43:58,970
and in between between ultimate that we can use the ideas of objects too

725
00:43:59,050 --> 00:44:01,340
accept language of models

726
00:44:02,150 --> 00:44:04,540
you already know but finite state automata

727
00:44:04,550 --> 00:44:07,590
it except the finite set of finite words

728
00:44:07,600 --> 00:44:10,040
you can have automaton that accepts

729
00:44:10,100 --> 00:44:14,250
sets of trees finitary is infinite trees this was seen

730
00:44:17,350 --> 00:44:20,100
the tools that one would be use

731
00:44:20,110 --> 00:44:21,910
to answer questions

732
00:44:21,920 --> 00:44:27,540
o automaton like membership of non emptiness of the language so that's the

733
00:44:27,550 --> 00:44:29,880
that's the main part of the

734
00:44:30,150 --> 00:44:34,110
so let me tell you about model checking which is the main motivation for us

735
00:44:34,110 --> 00:44:36,060
to use logic programs

736
00:44:36,300 --> 00:44:38,320
so one checking

737
00:44:38,410 --> 00:44:40,380
is a very mean

738
00:44:41,430 --> 00:44:47,370
well known problem problem now we will mean and you say established program

739
00:44:47,860 --> 00:44:49,190
well the

740
00:44:49,230 --> 00:44:52,490
whatever kind of mathematical framework use

741
00:44:52,670 --> 00:44:54,300
you have some

742
00:44:54,320 --> 00:44:58,040
system which is represented in the model

743
00:44:58,040 --> 00:45:02,810
so for example for problems you can have the set of executions the

744
00:45:02,860 --> 00:45:05,600
sequences of even the problem can perform

745
00:45:05,610 --> 00:45:09,610
finite or infinite depending on what you want to talk about

746
00:45:09,610 --> 00:45:11,660
and you have also language

747
00:45:11,670 --> 00:45:15,110
four specification that's what i've explained

748
00:45:15,180 --> 00:45:17,490
and the model checking

749
00:45:17,540 --> 00:45:20,130
is to decide

750
00:45:20,150 --> 00:45:25,650
so you have some compute computation computational less aspects there and this problem is to

751
00:45:25,650 --> 00:45:29,170
decide whether this system satisfies the specification

752
00:45:29,180 --> 00:45:34,240
so everything should be obviously given the clear mathematical setting both the models and the

753
00:45:34,240 --> 00:45:36,310
specification language

754
00:45:37,810 --> 00:45:39,610
deciding what to do

755
00:45:39,630 --> 00:45:41,530
may know should know

756
00:45:41,540 --> 00:45:45,790
is to know whether there is an automated message or an algorithm

757
00:45:45,800 --> 00:45:46,960
to answer this

758
00:45:46,970 --> 00:45:50,040
so obviously if you want to make things i mean

759
00:45:51,270 --> 00:45:54,970
you may use able

760
00:45:54,980 --> 00:45:57,940
in reality in the real world

761
00:45:57,970 --> 00:46:01,030
the same thing doesn't mean only that you you have to

762
00:46:01,040 --> 00:46:04,230
to know whether you this problem is decidable or not

763
00:46:04,300 --> 00:46:08,320
but you also very interested in its complexity because you have to know the problem

764
00:46:08,320 --> 00:46:10,020
is tractable

765
00:46:10,050 --> 00:46:11,550
so this is the

766
00:46:11,650 --> 00:46:13,320
general picture

767
00:46:13,360 --> 00:46:17,610
and so here is an example that i will tell you how we naturally derive

768
00:46:17,650 --> 00:46:18,810
some models

769
00:46:18,880 --> 00:46:20,440
from programs

770
00:46:20,460 --> 00:46:24,940
so it's a bit of a little introductory example where you start from something which

771
00:46:24,990 --> 00:46:27,360
reasonably looks like problem

772
00:46:27,370 --> 00:46:30,790
and i will take you to mathematical model the that we call

773
00:46:30,800 --> 00:46:32,420
operational ones

774
00:46:32,440 --> 00:46:34,380
well things are based on say

775
00:46:34,380 --> 00:46:42,480
and so the point is that in this case the racing line is given and

776
00:46:42,480 --> 00:46:46,170
it's all about how he would try and managed to stay

777
00:46:46,650 --> 00:46:50,900
with the car on the racing line and

778
00:46:50,920 --> 00:46:56,570
he was one the very small model of how you could go about that you

779
00:46:56,570 --> 00:47:01,790
could represent the state at any given point in time as the angular difference in

780
00:47:01,790 --> 00:47:03,440
the heading

781
00:47:03,480 --> 00:47:08,570
of of your car with respect to the to the ideal line that you're trying

782
00:47:08,570 --> 00:47:15,290
to follow and ideally you have had purchased a description of course and then you

783
00:47:17,590 --> 00:47:23,480
as a result he will you want a mapping from states to actions policy and

784
00:47:23,480 --> 00:47:31,340
and find an appropriate steering angle for fractionation achieving this being on on the racing

785
00:47:31,340 --> 00:47:34,650
line and here is the

786
00:47:34,710 --> 00:47:40,860
the training data and you're essentially fit the curve and and thereby learn the particular

787
00:47:40,860 --> 00:47:44,710
controller that the human is using now this is

788
00:47:44,730 --> 00:47:49,330
can you know a very boring model because the racing line is already given

789
00:47:49,340 --> 00:47:58,400
following up on on what sam lectured about the latent variables the more interesting problem

790
00:47:58,400 --> 00:48:02,880
that people are working on is to observe someone's driving

791
00:48:02,900 --> 00:48:03,920
and now

792
00:48:03,920 --> 00:48:10,170
using latent variable model the latent variable being the racing line and this person is

793
00:48:10,170 --> 00:48:11,960
trying to follow

794
00:48:11,960 --> 00:48:14,590
which then produces the

795
00:48:14,610 --> 00:48:19,880
together with the policy for following it the particular steering actions that the person is

796
00:48:19,880 --> 00:48:24,370
is taking and then the job of course of learning is not only to infer

797
00:48:24,370 --> 00:48:29,230
the control policy but also to infer what the intended racing line may may have

798
00:48:30,060 --> 00:48:36,060
and so that would be more elaborate model for this particular example but it turns

799
00:48:36,060 --> 00:48:38,960
out to be quite hard to two

800
00:48:38,980 --> 00:48:44,710
actually work with these games that may be an interesting point

801
00:48:44,710 --> 00:48:48,090
in order to do some of these experiments we

802
00:48:48,110 --> 00:48:51,920
i went to the game producers and ask them could you give us the code

803
00:48:51,920 --> 00:48:52,770
base of

804
00:48:52,840 --> 00:48:57,770
recently released game or something and they did but in

805
00:48:57,840 --> 00:49:04,190
game software development is very special for you if you like because they are fixed

806
00:49:04,190 --> 00:49:09,650
release dates and these games are never really finished there's this

807
00:49:09,670 --> 00:49:17,630
at some point development stopped and then testing periods started and then there's this french

808
00:49:17,630 --> 00:49:22,770
period where all the arrows have to be eliminated from the cold and this results

809
00:49:22,770 --> 00:49:26,960
in the code beings become extremely ugly if you have some are there and there's

810
00:49:26,960 --> 00:49:32,360
only one way one day two to go to the to release or something then

811
00:49:32,360 --> 00:49:37,310
you have to do hacks and it's quite amazing how many dirty hacks finding these

812
00:49:37,330 --> 00:49:40,920
codebase is you know usually these if

813
00:49:40,920 --> 00:49:48,710
very specific condition occurs then do this very specific strange thing and it makes coding

814
00:49:48,710 --> 00:49:54,830
within these codebase extremely hard once the game has gone gone through this transition period

815
00:49:54,840 --> 00:49:57,570
it also has to do with the fact by the way that these games are

816
00:49:57,570 --> 00:50:03,650
not revise for a second version something they once they are done the codebase is

817
00:50:03,650 --> 00:50:08,570
essentially discarded and for the next version of the game people start new in in

818
00:50:08,570 --> 00:50:15,000
that respect the games industry is a rather immature software development and

819
00:50:15,060 --> 00:50:21,790
is a very poor culture in that respect because reusability isn't really used to its

820
00:50:24,980 --> 00:50:31,710
it's a lot like newspapers the right

821
00:50:31,750 --> 00:50:34,190
but these games can make money

822
00:50:36,190 --> 00:50:37,980
so this is then

823
00:50:37,980 --> 00:50:43,340
if these were the true control actions then these are the the ones that the

824
00:50:43,360 --> 00:50:47,060
the neural network would do in this particular example

825
00:50:48,310 --> 00:50:56,630
this was an example in driving games and another great challenge comes from first-person shooters

826
00:50:57,060 --> 00:51:03,230
so first in half-life two the one i showed is a first-person shooter they the

827
00:51:03,770 --> 00:51:08,110
terminology is relatively simple you see

828
00:51:08,130 --> 00:51:10,790
the world from the first-person perspective

829
00:51:10,810 --> 00:51:15,500
so that's first-person shooter comes from holding the gun in your hand so

830
00:51:16,040 --> 00:51:23,090
in this role creating non-playing characters that behave like humans

831
00:51:23,090 --> 00:51:25,210
is one of the great gold

832
00:51:25,210 --> 00:51:32,730
and anyone who creates such game essentially ask themselves the question should incorporate current parts

833
00:51:34,130 --> 00:51:38,150
which looks pretty stupid but at least provide something or should we just leave it

834
00:51:38,750 --> 00:51:42,730
and so for example in halo two which is one of the most successful first-person

835
00:51:42,730 --> 00:51:47,710
shooters ever they just left it out because they figured the state of the art

836
00:51:47,710 --> 00:51:53,020
is so bad we'd rather have no bots then stupid bot which i think was

837
00:51:53,020 --> 00:51:56,380
pretty good decisions

838
00:51:56,400 --> 00:52:01,230
so one idea for creating bots is of course to learn the behavior from from

839
00:52:01,230 --> 00:52:06,960
humans because there's a lot of potential training data around you wouldn't believe how many

840
00:52:06,960 --> 00:52:12,840
kids you can get for generating data if you just provide them with

841
00:52:12,860 --> 00:52:15,520
as yet unreleased game or you know

842
00:52:15,520 --> 00:52:22,770
something that's amazing you can really data collection is not hard task in this particular

843
00:52:22,770 --> 00:52:29,400
business but the problem of course is that you have

844
00:52:29,460 --> 00:52:33,940
and what you want is tactical behavior in a very complex and more importantly in

845
00:52:33,940 --> 00:52:35,960
a hostile environment

846
00:52:35,980 --> 00:52:42,810
so let me give you an example of of

847
00:52:42,860 --> 00:52:46,980
what this involves this is the skin game halo two that i was talking about

848
00:52:47,130 --> 00:52:51,520
and i'll show you a a little bit how people play online in multiplayer game

849
00:52:51,560 --> 00:52:55,480
there's no bots involved in these are humans but i want to show you what

850
00:52:55,480 --> 00:52:58,500
kind of behavior is required here

851
00:53:00,420 --> 00:53:06,860
you see i think i'll just do this idea trick again

852
00:53:08,560 --> 00:53:13,070
so what you see here

853
00:53:13,090 --> 00:53:16,250
but if you like

854
00:53:16,570 --> 00:53:22,230
so what you see here is very rich world with various vehicles i i don't

855
00:53:22,230 --> 00:53:24,540
know if you could see that the tank

856
00:53:24,560 --> 00:53:26,170
you have

857
00:53:26,190 --> 00:53:31,360
you rocks it's the big valley with trees there's other people involved there and the

858
00:53:31,360 --> 00:53:34,500
goal of this particular game is to capture the flag

859
00:53:34,520 --> 00:53:36,440
this is the red guy

860
00:53:36,440 --> 00:53:45,040
it interacts the longer it stays in equilibrium and the lower will be its freeze out of abundance

861
00:53:45,040 --> 00:53:54,840
sofor a coldthermal relic the more weakly interacting it is the larger it`s abundance

862
00:53:54,840 --> 00:54:00,540
so the week shall inherit the universe they`re not going inherit the earth that's

863
00:54:00,540 --> 00:54:08,640
clear but they will inherit the universe you can you can solve the Boltzmann

864
00:54:08,640 --> 00:54:17,080
equation of the rate equations and calculate a freeze out temperature and the mass over

865
00:54:17,080 --> 00:54:21,400
the freeze out temperature depends upon the mass of the particle the plant maps which

866
00:54:21,400 --> 00:54:32,160
determines the expansion rate and this cross section sigmazerobut it only depends logarithmically

867
00:54:32,160 --> 00:54:38,120
on those and you can calculate the freeze out abundance of the particle relative to

868
00:54:38,120 --> 00:54:43,750
the entropy densityor if you wish the photon density and as the mass over the temp

869
00:54:43,780 --> 00:54:49,800
freeze out temperature to some power and one over the masstimesa plug mass

870
00:54:49,820 --> 00:54:57,020
time sigma zero now if you want to know the contribution to omega you multiply the number

871
00:54:57,020 --> 00:55:06,100
density times the mass in here you see that this mass cancels this mass so except for the

872
00:55:06,100 --> 00:55:12,400
logarithmic dependence of them out of the mass of the freeze out temperature this this

873
00:55:12,400 --> 00:55:18,460
is independent of the mass of the particle it only depends upon the scale of the

874
00:55:18,460 --> 00:55:24,220
annihilation cross section of course in the particle physics model the scale of the annihilation

875
00:55:24,220 --> 00:55:29,640
cross section can depend upon the mask but it only depends on the annihilation cross

876
00:55:29,640 --> 00:55:36,370
section one over the annihilation cross section and is not depend upon the mass

877
00:55:36,370 --> 00:55:42,480
so that`s the general result of course it's not quite so clean when you dig into

878
00:55:42,480 --> 00:55:48,660
the details you have to ask whether it's s-wave or p-wave annihilation or

879
00:55:48,660 --> 00:55:55,020
you talking about annihilation of scattering cross-sections are also possibilities that the particle interact with

880
00:55:55,020 --> 00:55:59,920
different strength with different other particles in the the thermal bath about this is known as

881
00:55:59,920 --> 00:56:07,700
co-annihiliationand this sub-leading dependences on the results of dependences on the mass at

882
00:56:07,700 --> 00:56:12,200
the number of degrees of freedom etc. so it's not quite so clean but that

883
00:56:12,200 --> 00:56:18,580
gives you a good ideathe gross features of what goes on

884
00:56:18,580 --> 00:56:26,560
so a cold thermalrelic has the following really remarkable and beautiful relations if you know

885
00:56:26,580 --> 00:56:31,840
omega we want omega omega H squared to be point one two nine

886
00:56:31,860 --> 00:56:37,540
you know we know very well what omega is for the dark matter then this

887
00:56:37,540 --> 00:56:43,220
tells us for a cold thermal relic whatthe annihilation cross section is it has to

888
00:56:43,220 --> 00:56:48,900
annihilate into light particles let's imagine it in it annihilates in the quarks so we know

889
00:56:48,910 --> 00:56:54,890
something about this matrix elementwe know something about the value of the matrix element

890
00:56:55,020 --> 00:57:00,880
we don't have to know whether it's supersymmetryorsupergravity or the Tooth Fairy it doesn't

891
00:57:00,880 --> 00:57:07,180
matter we know something about this matrix elementso if we know something about that

892
00:57:07,180 --> 00:57:14,220
process by crossing we also know something about the scattering cross section of the dark

893
00:57:14,220 --> 00:57:18,680
matter particle with thequarkand if we know this we can work a little

894
00:57:18,680 --> 00:57:25,540
harder and get the scattering cross section with a proton and nucleon or nucleus and

895
00:57:25,540 --> 00:57:31,000
we also would know the production cross section at least we know the matrix element

896
00:57:31,000 --> 00:57:36,880
that enters in this production cross-section so just by knowing omega in the particle you

897
00:57:36,880 --> 00:57:41,720
know something about the annihilation cross section the scattering cross section of the production

898
00:57:41,720 --> 00:57:52,140
cross-sectionyou don't know the mass so the reason cold thermal relics are so incredibly popular

899
00:57:52,140 --> 00:57:59,980
in the Oran object a particular veneration is it leads to very rich possibilities

900
00:57:59,980 --> 00:58:07,620
in experimental physics and astronomy so we could detect we know the scattering cross-section if

901
00:58:07,620 --> 00:58:12,380
you go through the details it's a little bit weaker than weak but it's scattering

902
00:58:12,380 --> 00:58:20,620
cross-section may be large enough to detect indirect detection experiments and sensitive underground experiments that

903
00:58:20,620 --> 00:58:26,280
are run by people who of lost their had it looks like and you know here

904
00:58:26,280 --> 00:58:32,800
I'm not gonna go into this because people here are very familiar with these ideas

905
00:58:32,800 --> 00:58:35,000
in the subjective setting for example

906
00:58:35,540 --> 00:58:38,210
hierarchical priors it's just this notion there

907
00:58:39,040 --> 00:58:40,360
i may have a prior

908
00:58:41,350 --> 00:58:43,490
over some complicated parameter space

909
00:58:44,490 --> 00:58:48,680
uh i don't know how to specify the pryr so i write down some distributions

910
00:58:48,680 --> 00:58:50,620
and they have some other parameters alpha

911
00:58:51,360 --> 00:58:54,620
and say okay well you specify a prior over the parameters alpha

912
00:58:56,640 --> 00:59:02,210
and those parameters alpha maybe are specified there's distributions with some other parameters beta

913
00:59:03,260 --> 00:59:05,670
so then i need to specify a prior over the data and so on

914
00:59:06,540 --> 00:59:06,910
okay now

915
00:59:07,360 --> 00:59:08,760
people often look at this and they say

916
00:59:09,190 --> 00:59:14,170
o with this goes infinitely up and that's completely ridiculous okay this only goes up

917
00:59:14,940 --> 00:59:16,530
to the point where r

918
00:59:17,530 --> 00:59:24,230
what's the point of these hyperparameters these hyperparameters here there are tying together parameters at lower levels

919
00:59:25,940 --> 00:59:28,910
once you get single hyperparameters at the top

920
00:59:29,320 --> 00:59:31,550
you just put down some distribution over them

921
00:59:32,100 --> 00:59:34,850
and there's no point having extra layers above the

922
00:59:35,300 --> 00:59:40,550
because you can marginalize out all those extra layers above it just get some other distribution

923
00:59:41,090 --> 00:59:41,950
o ver say

924
00:59:42,590 --> 00:59:43,180
beta here

925
00:59:43,770 --> 00:59:46,760
so this a hierarchical process doesn't stop

926
00:59:47,330 --> 00:59:48,080
at some point

927
00:59:48,680 --> 00:59:53,850
ah wear you just have single hyperparameters that you just need to put some distributions over

928
00:59:56,020 --> 00:59:57,660
and then empirical bayes

929
00:59:58,100 --> 00:59:59,450
is this idea that

930
00:59:59,850 --> 01:00:04,530
i write down prior are often in the form of a hierarchical priors on hyperparameters

931
01:00:05,090 --> 01:00:10,250
i don't know how to that the hyperparameters so somehow learned more optimize and from my data

932
01:00:11,000 --> 01:00:11,310
this is

933
01:00:11,930 --> 01:00:13,160
an approximation to

934
01:00:14,040 --> 01:00:15,980
what do we do if you actually averaging them

935
01:00:17,400 --> 01:00:18,500
get any questions about their

936
01:00:27,210 --> 01:00:29,800
let's talk a little bit about some of these different frameworks

937
01:00:31,340 --> 01:00:33,030
so subjective priors

938
01:00:33,620 --> 01:00:36,830
the idea is that priors capture our beliefs as well as possible

939
01:00:39,230 --> 01:00:40,300
you know what's the point

940
01:00:43,870 --> 01:00:45,550
now how do we know our beliefs

941
01:00:48,160 --> 01:00:48,360
you know

942
01:00:49,750 --> 01:00:52,060
it really depends on the problem domain

943
01:00:52,480 --> 01:00:56,730
so if i'm trying to solve a particular problem i think about the problem domain

944
01:00:57,950 --> 01:00:59,180
try to come up with some

945
01:01:00,270 --> 01:01:02,750
don't be too scared about the notion of beliefs

946
01:01:03,510 --> 01:01:04,510
i think a bit more has

947
01:01:06,750 --> 01:01:09,350
not ridiculous ranges fourier parameters

948
01:01:10,750 --> 01:01:15,740
all you really trying to do in your prior is specify ranges for your parameters

949
01:01:16,720 --> 01:01:18,880
the spread out probability mass

950
01:01:19,450 --> 01:01:23,400
so you can capture what's actually going on that's sort asymptotic

951
01:01:25,490 --> 01:01:26,800
you know convergence idea

952
01:01:28,130 --> 01:01:32,050
so you wanna spend all your probability mass but maybe don't spread out to ridiculously

953
01:01:32,050 --> 01:01:35,570
because are making very silly predictions about what possible data

954
01:01:36,090 --> 01:01:36,660
you could have been

955
01:01:37,360 --> 01:01:37,920
in your model

956
01:01:44,390 --> 01:01:46,580
how do you evaluate your prior well

957
01:01:49,190 --> 01:01:53,040
one way to do it is if your model is fully specified

958
01:01:53,530 --> 01:01:55,400
then you can generate data from your model

959
01:01:56,360 --> 01:02:00,590
right that's what it means so you can generate data and you can look at the data you can say

960
01:02:01,040 --> 01:02:02,560
is there what i expect

961
01:02:03,590 --> 01:02:05,700
for example if i had a prior on

962
01:02:07,270 --> 01:02:08,810
attributes of human beings

963
01:02:10,360 --> 01:02:13,290
i generated from it and it is shown me there

964
01:02:14,300 --> 01:02:16,420
people's heights with varying from

965
01:02:16,950 --> 01:02:21,860
you know minus one meter seven meters i would say that's that's wrong i messed up

966
01:02:22,310 --> 01:02:27,420
okay people's heights don't go negative and no and no human will never be seven meters tall

967
01:02:28,040 --> 01:02:29,660
okay so i change my prior

968
01:02:30,050 --> 01:02:32,320
so that i can actually generate something more reasonable

969
01:02:35,540 --> 01:02:38,740
even very vague priors can be useful since the data

970
01:02:39,680 --> 01:02:44,920
that's all very quickly exponentially concentrate its mass or unreasonable models

971
01:02:50,260 --> 01:02:53,560
and as i mentioned before the key ingredient is the idea is is not the

972
01:02:53,560 --> 01:02:57,020
prime here is the idea of averaging over the different possibilities

973
01:03:00,770 --> 01:03:02,290
now empirical priors

974
01:03:03,540 --> 01:03:04,480
here's a slide for

975
01:03:05,100 --> 01:03:08,550
imagine you have a hierarchical model with some hyperparameters alpha

976
01:03:09,280 --> 01:03:12,830
and what you can do is you can estimate those hyperparameters from the data

977
01:03:14,300 --> 01:03:15,640
this is called level two

978
01:03:16,060 --> 01:03:22,360
for type two maximum likelihood you optimize these hyperparameters while integrating out these parameters theta

979
01:03:23,260 --> 01:03:24,340
and then for prediction

980
01:03:24,950 --> 01:03:25,830
use sneak in

981
01:03:27,370 --> 01:03:31,250
these optimise hyperparameters and the data in you do this for prediction

982
01:03:32,430 --> 01:03:33,780
now clearly we have

983
01:03:34,090 --> 01:03:36,250
we've broken the sum and product rule here

984
01:03:38,510 --> 01:03:39,780
but there are some advantages

985
01:03:40,320 --> 01:03:43,630
kennedy advantages are this is actually quite robust

986
01:03:45,240 --> 01:03:47,410
your are allowing yourself to fit

987
01:03:47,830 --> 01:03:48,810
alpha to the data

988
01:03:49,720 --> 01:03:53,290
and so if your data was something that you totally wouldn't have expected

989
01:03:53,990 --> 01:03:57,840
then your uroplakin can go fit somehow anyway

990
01:03:58,250 --> 01:04:01,770
so overcome some limitations of mis specification of the prior

991
01:04:03,140 --> 01:04:08,810
the problem is that you're double counting the data in the sense that you're counting the data once here

992
01:04:09,270 --> 01:04:11,610
and once also in your estimation of alpha

993
01:04:12,310 --> 01:04:15,890
but if you are only estimating a few hyperparameters then usually

994
01:04:16,400 --> 01:04:21,020
there there will result in a lot of it it will be very very minimal you'll never notice it

995
01:04:22,840 --> 01:04:23,510
questions about their

996
01:04:26,840 --> 01:04:32,590
level one maximum likelihood is just a regular old max likelihood where you are optimise the status

997
01:04:34,290 --> 01:04:37,810
so level to just says integrate out the status optimise the office

998
01:04:37,810 --> 01:04:41,080
days later on

999
01:04:41,090 --> 01:04:45,940
opportunities are

1000
01:04:58,300 --> 01:05:02,910
it is

1001
01:05:26,190 --> 01:05:29,580
three of

1002
01:05:53,860 --> 01:05:58,060
i like

1003
01:06:00,120 --> 01:06:02,960
the anchor

1004
01:06:02,970 --> 01:06:06,490
get ready

1005
01:06:21,780 --> 01:06:24,800
right four

1006
01:07:32,100 --> 01:07:38,460
we can

1007
01:08:38,090 --> 01:08:41,700
rather a lot of the people

1008
01:08:44,940 --> 01:08:49,490
in which the

1009
01:09:04,020 --> 01:09:06,950
in the

1010
01:09:11,090 --> 01:09:16,430
this is for

1011
01:09:26,080 --> 01:09:28,390
last is

1012
01:09:32,140 --> 01:09:44,970
you know it

1013
01:09:48,270 --> 01:09:58,470
well let me to

1014
01:09:58,470 --> 01:10:00,750
as you know right

1015
01:13:18,430 --> 01:13:21,390
one of them

1016
01:17:00,670 --> 01:17:35,170
but this

1017
01:17:35,170 --> 01:17:36,610
i talk about today

1018
01:17:36,660 --> 01:17:41,400
not so much tomorrow but today will be for these for a very generic state

1019
01:17:41,400 --> 01:17:43,830
space stochastic models

1020
01:17:43,840 --> 01:17:50,170
so would like to be able to estimate hidden process online i think i said

1021
01:17:50,170 --> 01:17:53,730
all this i don't need to go through any more because there's just a handful

1022
01:17:53,740 --> 01:17:57,420
of applications that we move on

1023
01:17:58,640 --> 01:18:03,070
people these all go online people could people can read that that stuff afterwards so

1024
01:18:03,070 --> 01:18:04,710
what i talk about today

1025
01:18:04,720 --> 01:18:08,120
well i guess most people will be familiar with

1026
01:18:08,130 --> 01:18:11,620
quite a lot of this list actually especially i guess the first one so i

1027
01:18:11,620 --> 01:18:16,090
just established the bayes theorem form that we need for today

1028
01:18:16,170 --> 01:18:22,560
i was talking a little bit about monte carlo methods i know that marries a

1029
01:18:22,560 --> 01:18:28,710
nice tutorial already i wasn't here that's exactly what he's covered of the artists survey

1030
01:18:28,710 --> 01:18:33,440
if few bits monte carlo methods that that we need

1031
01:18:35,080 --> 01:18:41,750
defined the framework wherein so we pose these things using state space model stochastic state

1032
01:18:41,750 --> 01:18:49,720
space models and the estimation tasks that were interested in particular filtering which is basically

1033
01:18:49,830 --> 01:18:53,920
learning about the state of the current time as the data evolves so as soon

1034
01:18:53,920 --> 01:18:56,750
as you see the data you want to learn about the posterior distribution for the

1035
01:18:56,750 --> 01:18:58,310
state at that time

1036
01:18:58,480 --> 01:19:01,050
then if you want to give you or alternatively if you want to give you

1037
01:19:01,050 --> 01:19:07,780
a bit of self-image retrospective look awful forward look through the data to get more

1038
01:19:07,780 --> 01:19:13,910
reliable estimates out of the data we can we may do smoothing that's statistical smoothing

1039
01:19:15,290 --> 01:19:19,740
one of the main building blocks for sequential estimation

1040
01:19:19,760 --> 01:19:22,140
and indeed

1041
01:19:22,160 --> 01:19:27,330
a building block for some of the particle filtering algorithms see tomorrow is the kalman

1042
01:19:27,330 --> 01:19:32,560
filter so i'm going to go through the analytic updating formerly

1043
01:19:32,570 --> 01:19:36,730
the kalman filter the extended kalman filter which works for non linear

1044
01:19:36,740 --> 01:19:43,930
models under certain conditions but consider the color filter in a rather neat probabilistic framework

1045
01:19:43,930 --> 01:19:49,410
some of you would have seen this before it's available in various textbooks already for

1046
01:19:49,410 --> 01:19:55,020
example i first saw it in the textbook by colman but there are also not

1047
01:19:55,020 --> 01:20:00,600
common common doesn't like the probabilistic interpretation of the color filter it also we had

1048
01:20:00,600 --> 01:20:02,360
to belong to those

1049
01:20:02,370 --> 01:20:03,330
we have

1050
01:20:03,440 --> 01:20:09,340
and to belongs to a workshop on particle filtering in cambridge of you in nineteen

1051
01:20:09,340 --> 01:20:14,150
ninety in two thousand six and he pretty well disagreed with the whole foundation the

1052
01:20:14,160 --> 01:20:18,450
whole workshop but he gave us a couple of nights someone else there was great

1053
01:20:18,450 --> 01:20:23,640
as long but he doesn't really agree and doesn't really agree that stochastic processes even

1054
01:20:23,640 --> 01:20:27,800
exist so physically not in the mathematical sense

1055
01:20:27,810 --> 01:20:32,900
so but i will talk about the colour filter from the probabilistic point viewpoint bayesian

1056
01:20:32,900 --> 01:20:38,980
updating of probability densities for me the need derivation of the of the common fields

1057
01:20:38,980 --> 01:20:43,610
shows a lot of light on this type of particle filtering area

1058
01:20:43,620 --> 01:20:48,470
then moving to monte carlo filtering area to just oppose it generally how you do

1059
01:20:48,470 --> 01:20:53,320
monte carlo filtering filtering that doesn't quite solve the problem doesn't give you the sequential

1060
01:20:53,320 --> 01:20:59,870
updating that you need with the particle filter or at least not completely tractable form

1061
01:20:59,940 --> 01:21:03,120
so then i'll talk about the basic way of doing that

1062
01:21:03,160 --> 01:21:06,020
the sequential monte carlo bootstrap

1063
01:21:07,010 --> 01:21:10,960
that's the view i've that's what i call the vanilla version of particle filters the

1064
01:21:10,960 --> 01:21:17,560
simplest version then i'll talk about the journal version that that allows you to deal

1065
01:21:17,560 --> 01:21:23,360
with harlem models using few particles through the choice of appropriate important functions

1066
01:21:23,370 --> 01:21:29,460
tomorrow we'll move on to other types of particle filtering also talk about some applications

1067
01:21:29,460 --> 01:21:35,730
that we've worked on things like using particle filter smoothing particle filters the rare blackwellized

1068
01:21:35,730 --> 01:21:39,650
particle filter a very important area at the moment MCMC

1069
01:21:39,720 --> 01:21:44,240
with particle filters so they can be coupled and very powerful ways

1070
01:21:44,250 --> 01:21:49,510
but bayesian inference well expect we do about this but i'm going to be talking

1071
01:21:49,510 --> 01:21:53,470
about marginal inference we have some observations y

1072
01:21:53,520 --> 01:21:56,380
quantity of interest let's say that x

1073
01:21:56,390 --> 01:22:00,700
and we have some nuisance parameters in the model call those things

1074
01:22:00,710 --> 01:22:04,430
but we can form if you like a sensor model tells us what's the probability

1075
01:22:04,430 --> 01:22:08,120
of observations conditional on all of those are

1076
01:22:08,650 --> 01:22:15,860
required texts and the nuisance parameters theta formulate the joint posterior probability for all of

1077
01:22:15,860 --> 01:22:19,580
the unknowns in terms of that the likelihood function

1078
01:22:19,630 --> 01:22:21,340
and the prior distribution

1079
01:22:21,360 --> 01:22:26,390
jointly over all the unknown this features and of course on the bottom line of

1080
01:22:26,390 --> 01:22:32,190
bayes theorem we have this effectively constant is constant for any given model and data

1081
01:22:32,190 --> 01:22:35,730
set y so that there is

1082
01:22:36,170 --> 01:22:40,440
as well as in many bayesian calculation is going to be neglected as constant in

1083
01:22:40,440 --> 01:22:45,040
each step of the particle filtering algorithm and we do the bayesian updating

1084
01:22:45,060 --> 01:22:48,960
and of course if we want to model inference for the quantity of interest we

1085
01:22:48,960 --> 01:22:55,760
will integrate out x using the more the marginalization identity from probability theory and that

1086
01:22:55,760 --> 01:22:59,000
thing would be the thing if it's just sex were interested in that's the that's

1087
01:22:59,000 --> 01:23:00,580
the quantity was used to do

1088
01:23:00,590 --> 01:23:03,760
the inference and of the joint

1089
01:23:04,410 --> 01:23:06,490
monte carlo methods

1090
01:23:06,530 --> 01:23:11,050
so this is actually an output from one of the particle filters are two dimensional

1091
01:23:11,060 --> 01:23:12,890
particle filter out a particular

1092
01:23:12,940 --> 01:23:19,840
two is two-dimensional state vector overtake particular time and it comes from a nonlinear model

1093
01:23:19,860 --> 01:23:27,050
is that very simple model is used a lot as benchmark test model in particle

1094
01:23:27,050 --> 01:23:31,750
filtering and this was one of the posterior pdfs that came out of that particular

1095
01:23:31,780 --> 01:23:38,200
time is the kernelized density estimates obtained for it and we obtain that instead of

1096
01:23:38,200 --> 01:23:46,250
trying to solve this very intractable density is certainly multimodal is probably not galaxy in

1097
01:23:46,250 --> 01:23:47,130
the story doesn't look

1098
01:23:47,140 --> 01:23:51,650
calcium doesn't really look like a mixture of gaussians not not a convenient number of

1099
01:23:52,110 --> 01:23:57,290
components anyway so we represented instead using monte carlo representation

1100
01:23:57,340 --> 01:24:04,330
that simply means that ideally we like independent random draws from this PDF instead of

1101
01:24:04,340 --> 01:24:10,610
the density well represented directly in terms of number of randomly sampled points from this

1102
01:24:10,610 --> 01:24:19,680
PDF montecarlo representation and that's very convenient representation in particular for calculating difficult expectations

1103
01:24:19,700 --> 01:24:21,430
so let's suppose we want to

1104
01:24:21,450 --> 01:24:26,370
work out expectations with respect to some highly complex probability density p of x so

1105
01:24:26,370 --> 01:24:29,070
this one then the expectations

1106
01:24:29,100 --> 01:24:34,580
we may require in general be some functional h of the state variable the integral

1107
01:24:34,630 --> 01:24:35,930
with respect to

1108
01:24:35,950 --> 01:24:40,070
the probability density p

1109
01:24:42,860 --> 01:24:45,300
as i said with the that with monte carlo

1110
01:24:45,320 --> 01:24:49,440
thereby by generating random samples from p of x in the notation are used random

1111
01:24:49,440 --> 01:24:55,110
samples throughout is a superscript so if i have the example of a large collection

1112
01:24:55,110 --> 01:25:00,790
of n random samples and label that x superscripts i for the life of those

1113
01:25:03,100 --> 01:25:05,850
informally it's quite nice to think about that

1114
01:25:05,860 --> 01:25:09,900
it monte carlo approximation to the density function itself

1115
01:25:09,970 --> 01:25:12,720
and using direct functions

1116
01:25:12,770 --> 01:25:17,200
the probability theorist like it's very much but it's it certainly gives you the right

1117
01:25:17,200 --> 01:25:23,590
answers and it's a nice intuitive way of thinking about monte carlo representation so once

1118
01:25:23,590 --> 01:25:27,370
we've got a random set of samples for a large number

1119
01:25:27,450 --> 01:25:32,520
of samples and all drawn from the probability density function p that somehow and said

1120
01:25:32,520 --> 01:25:36,820
i would generate with thinking were approximating p of x

1121
01:25:36,870 --> 01:25:43,110
as a lot of point masses basically because these are all the samples independent samples

1122
01:25:43,110 --> 01:25:46,640
from p of x the there is an unweighted samples so we have one over

1123
01:25:46,640 --> 01:25:51,950
and just to normalise the approximation to pdx

1124
01:25:51,970 --> 01:25:56,090
and then using this into intuitive interpretation

1125
01:25:56,110 --> 01:25:58,100
we can easily see how things like

1126
01:25:58,120 --> 01:26:03,910
approximations to expectations drop out of the formula because we're going to take the expectation

1127
01:26:03,910 --> 01:26:06,430
you can see the tree

1128
01:26:06,450 --> 01:26:13,390
or you can use all the iterative algorithm to continue current you can appear to

1129
01:26:13,390 --> 01:26:15,950
be some kind of

1130
01:26:15,970 --> 01:26:18,680
expectation maximisation

1131
01:26:18,860 --> 01:26:22,780
so that for your

1132
01:26:24,510 --> 01:26:28,870
the first classification model

1133
01:26:28,870 --> 01:26:34,240
but for the information extraction i will not speak a lot about in the course

1134
01:26:34,280 --> 01:26:43,320
because it can cause because our proposals for the experiment on semantic role labeling

1135
01:26:43,600 --> 01:26:46,570
it very hard to language

1136
01:26:46,570 --> 01:26:49,740
two weeks later by the

1137
01:26:49,740 --> 01:26:56,950
because we have so many real natural language for you

1138
01:26:56,970 --> 01:26:59,390
you have

1139
01:26:59,430 --> 01:27:06,100
certain combinations of words syntactical structures that occur the

1140
01:27:07,300 --> 01:27:15,680
you can do it because you have many of the language you have usually very

1141
01:27:15,680 --> 01:27:19,820
long so you have many many exceptions

1142
01:27:19,870 --> 01:27:23,140
many many real situation which

1143
01:27:23,140 --> 01:27:25,950
you're my because just more

1144
01:27:27,470 --> 01:27:29,910
so this may be weakly supervised

1145
01:27:29,930 --> 01:27:35,100
the reason why they are especially for information extraction

1146
01:27:35,120 --> 01:27:38,120
quite a good job

1147
01:27:38,140 --> 01:27:41,160
but in the early nineties

1148
01:27:41,180 --> 01:27:46,510
and i think people will be less than in this area

1149
01:27:46,530 --> 01:27:50,490
quite quite well for many many years

1150
01:27:51,280 --> 01:27:54,200
it was studied

1151
01:28:00,640 --> 01:28:10,200
information extraction we're not quite well go back to

1152
01:28:10,240 --> 01:28:13,470
actually we heard coming

1153
01:28:13,490 --> 01:28:14,990
it goes back to the night

1154
01:28:15,010 --> 01:28:24,930
i think the really people of artificial intelligence community where by hand back to refer

1155
01:28:26,160 --> 01:28:27,720
in fact

1156
01:28:29,530 --> 01:28:35,490
i mean that sense with for very limited domains because if you have a hand

1157
01:28:35,490 --> 01:28:40,340
crafted rules and the you doing that

1158
01:28:40,370 --> 01:28:41,490
it a lot

1159
01:28:41,490 --> 01:28:47,620
a lot of human to end there are many domains languages so there that you

1160
01:28:47,640 --> 01:28:50,870
have so many things that you would have to

1161
01:28:50,890 --> 01:28:54,890
so you can only do so for now

1162
01:28:54,910 --> 01:28:57,280
x subject

1163
01:28:58,140 --> 01:29:05,140
later on in the his career people start started see in the core

1164
01:29:05,160 --> 01:29:09,490
also we think that supervised learning

1165
01:29:09,510 --> 01:29:12,090
so using machine learning techniques

1166
01:29:12,100 --> 01:29:14,010
in the that would

1167
01:29:14,050 --> 01:29:18,010
that's how this problem of manual

1168
01:29:18,030 --> 01:29:20,120
the manual work people

1169
01:29:20,140 --> 01:29:26,470
of course you have to and think you have to annotate labelled the

1170
01:29:26,550 --> 01:29:28,660
in your country

1171
01:29:28,720 --> 01:29:31,220
but usually

1172
01:29:31,490 --> 01:29:36,720
well it is usually done by top to them in summer time high hired is

1173
01:29:36,860 --> 01:29:43,570
in the time domain the document collection and competed at her less tedious

1174
01:29:43,590 --> 01:29:45,470
along the coast

1175
01:29:46,620 --> 01:29:51,550
right so that y

1176
01:29:51,570 --> 01:29:54,090
but of course there are

1177
01:29:54,740 --> 01:30:02,800
people now look into it seems like lot thinking

1178
01:30:02,800 --> 01:30:08,030
i have a few more like general and then you can go into

1179
01:30:08,030 --> 01:30:12,390
the first real action symbolic techniques

1180
01:30:12,410 --> 01:30:17,970
you you might

1181
01:30:17,970 --> 01:30:23,970
but really this type of semantically labeling related if we see the name of person

1182
01:30:24,010 --> 01:30:30,050
or if we look at an image and have heard or here we have someone

1183
01:30:30,780 --> 01:30:34,410
are we there angry

1184
01:30:34,410 --> 01:30:37,100
where do we get the play

1185
01:30:37,990 --> 01:30:41,970
you can even ask yourself the question do need label

1186
01:30:42,360 --> 01:30:48,220
but if we use the name in the classification within class

1187
01:30:48,320 --> 01:30:52,890
we can define why don't you

1188
01:30:55,320 --> 01:30:57,410
i think

1189
01:30:58,510 --> 01:31:07,910
i also had like references only some symmetry for to another dimension that like bill

1190
01:31:07,910 --> 01:31:11,010
clinton to younger brother

1191
01:31:13,100 --> 01:31:16,280
we want to make it

1192
01:31:16,280 --> 01:31:19,050
and that you can perform

1193
01:31:19,100 --> 01:31:23,200
on the other hand suppose you're working in biomedical domain

1194
01:31:23,200 --> 01:31:25,390
you know very specific

1195
01:31:25,410 --> 01:31:29,930
in chemistry and biochemistry for instance that might

1196
01:31:29,950 --> 01:31:37,510
i have that is name or they only over that particular

1197
01:31:37,820 --> 01:31:41,620
when you try to extract the creation

1198
01:31:43,010 --> 01:31:50,030
the becoming my head writer of

1199
01:31:50,050 --> 01:31:52,470
you can also exploit when you're

1200
01:31:53,680 --> 01:31:56,220
in your

1201
01:31:57,760 --> 01:31:59,030
you can exploit

1202
01:31:59,030 --> 01:32:01,200
this much higher

1203
01:32:01,260 --> 01:32:06,120
and you know binary

1204
01:32:06,140 --> 01:32:09,870
and of course many people speak about ontologies

1205
01:32:10,550 --> 01:32:12,240
when you really

1206
01:32:12,280 --> 01:32:14,300
find a certain domain

1207
01:32:14,320 --> 01:32:20,050
by number of concepts and relationships can be called

1208
01:32:20,100 --> 01:32:22,360
so these ontologies

1209
01:32:22,410 --> 01:32:25,220
they are used to provide

1210
01:32:27,260 --> 01:32:30,140
certain words that have

1211
01:32:30,160 --> 01:32:31,070
they have

1212
01:32:31,070 --> 01:32:36,900
basically parliament image processing algorithm to obtain an efficient algorithm on essentially there's no black

1213
01:32:36,900 --> 01:32:41,580
box with to OK so obviously basically it's going to be you are going to

1214
01:32:41,580 --> 01:32:46,040
put a lot of work for you MCMC algorithm to work OK so all

1215
01:32:46,050 --> 01:32:52,120
it's very similar advice metropolis to gibbs sampler if you update the stuff the valuable

1216
01:32:52,120 --> 01:32:56,540
at the time either i dependent on the target is not going to work well

1217
01:32:56,690 --> 01:33:02,140
if you try to design basically to update a lot of highly correlated variables simultaneously

1218
01:33:02,140 --> 01:33:04,590
it's very difficult to model we see away

1219
01:33:04,600 --> 01:33:10,810
two abate simultaneously like one the correlated viable using kind of mixture of particle methods

1220
01:33:10,810 --> 01:33:16,390
on elm MCMC but still no methods actually quite complicated tune on design

1221
01:33:16,400 --> 01:33:20,260
OK so one way that has been proposed which are was supposed to talk about

1222
01:33:20,260 --> 01:33:24,610
but i actually decided not to because i wanted to talk particle one way you

1223
01:33:24,610 --> 01:33:27,570
can essentially tried to

1224
01:33:27,610 --> 01:33:34,770
black box with some of try to import basically you MCMC type degrees is to

1225
01:33:34,770 --> 01:33:40,070
build an adaptive MCMC type algorithm so those guys libraries which i don't discuss how

1226
01:33:40,080 --> 01:33:42,680
we put some slide on my webpage and you can have a look at them

1227
01:33:42,680 --> 01:33:43,430
if you will

1228
01:33:43,510 --> 01:33:48,410
and after MCMC algorithm what they try to do the tried to basically

1229
01:33:48,430 --> 01:33:52,840
use the output of the markov chains that use simulated so as to tune the

1230
01:33:52,840 --> 01:33:57,310
parameters of the proposal k on that so as to optimize in some respects some

1231
01:33:57,310 --> 01:34:04,130
cretan it's way to impose some or all of MCMC algorithm of basically the naive

1232
01:34:07,940 --> 01:34:14,510
it's all right

1233
01:34:18,420 --> 01:34:25,660
he so the so the the answer is you don't actually is true there's no

1234
01:34:25,670 --> 01:34:30,140
actually not for any kind of realistic modelled that essentially no results yet so what

1235
01:34:30,140 --> 01:34:31,130
people do

1236
01:34:31,140 --> 01:34:35,740
on this in the important particles typically from a particle point of view going after

1237
01:34:35,740 --> 01:34:40,580
us some empirically was the algorithm works well not so one thing people are doing

1238
01:34:40,580 --> 01:34:44,240
is typically that they're gonna run obviously

1239
01:34:44,260 --> 01:34:50,490
many alisation wntn secularism starting from some are very diffuse distribution so you're not starting

1240
01:34:50,490 --> 01:34:54,490
all the markov chain from the same same state obviously you can

1241
01:34:54,510 --> 01:35:00,460
use like value essentially nasalisation of the of the markov chain or one of markov

1242
01:35:00,460 --> 01:35:05,980
chain algorithm on t essentially few these degrees are converging towards the same

1243
01:35:06,000 --> 01:35:11,800
the same the same essentially mode explored due the same estimate OK so that obviously

1244
01:35:11,800 --> 01:35:13,640
that's not to pull it off

1245
01:35:14,140 --> 01:35:20,510
a bulletproof guarantee that you have conventional attire distribution but at least that you've explored

1246
01:35:20,530 --> 01:35:26,720
actually interesting same so like local node of the target distribution of things you can

1247
01:35:26,720 --> 01:35:29,170
do to kind of SS was

1248
01:35:29,310 --> 01:35:34,580
you algorithms that list even if it's within the model whether it's essentially exploring properly

1249
01:35:34,580 --> 01:35:38,840
this mode is essentially once again generates basically

1250
01:35:38,880 --> 01:35:43,100
your your your market chains so you can have sequence of parliament to six i

1251
01:35:43,250 --> 01:35:47,660
on one thing you can do is basically compute the autocorrelation function

1252
01:35:47,690 --> 01:35:53,800
of this of this marker markov chain to give you an idea of essentially all

1253
01:35:53,800 --> 01:35:59,810
dependent the on you generate all depend on the sample you generates OK so you

1254
01:35:59,810 --> 01:36:00,180
do see

1255
01:36:00,610 --> 01:36:03,030
you compute the autocorrelation say

1256
01:36:03,040 --> 01:36:04,370
of the

1257
01:36:04,400 --> 01:36:08,940
simulated sequence for one parameter on you see that is of the correlation function becomes

1258
01:36:09,010 --> 01:36:14,560
extremely slowly towards zero then it means that actually algorithm is not working with a

1259
01:36:14,560 --> 01:36:20,540
talk because essentially you're part during the successive values are generated i e dependent one

1260
01:36:20,540 --> 01:36:25,480
from each other so it's the kind of indication that the algorithm is mixing extremely

1261
01:36:25,480 --> 01:36:29,920
slowly but there is no bulletproof so there are some diagnostic you can use obviously

1262
01:36:29,920 --> 01:36:34,150
to kind of onshore always the idea of the kind of work and not but

1263
01:36:34,150 --> 01:36:37,240
really that's just know

1264
01:36:37,320 --> 01:36:42,880
i mean i agree some maturity called justification for complex problem is very empirical what

1265
01:36:42,890 --> 01:36:44,530
is done in the literature

1266
01:36:45,710 --> 01:36:52,790
so that's the limitation of ncnc you have to accept it i that's serious limitation

1267
01:36:52,800 --> 01:36:54,700
commonly were

1268
01:37:15,870 --> 01:37:18,880
there's the technical report in the paper

1269
01:37:18,900 --> 01:37:20,900
by gelman genetics

1270
01:37:20,910 --> 01:37:27,530
although it appeared applied point between nineteen ninety five something like that eats

1271
01:37:27,540 --> 01:37:31,570
the poor and so for this thing became extremely popular

1272
01:37:33,350 --> 01:37:36,470
the portal is actually a of the of the

1273
01:37:36,590 --> 01:37:41,340
what i the same is only valid extremely strong conditions

1274
01:37:41,340 --> 01:37:46,600
which are essentially they're dealing with high dimensional target

1275
01:37:46,620 --> 01:37:48,310
OK to distribution

1276
01:37:48,320 --> 01:37:53,070
which factorized so the component is an independent

1277
01:37:53,150 --> 01:37:54,670
they are i i d

1278
01:37:54,700 --> 01:38:01,090
OK on the problem the solution either on the more that factorizes century

1279
01:38:01,090 --> 01:38:07,150
on the components of i i d on by basically that limiting arguement they show

1280
01:38:07,180 --> 01:38:11,340
that's the thing to do when they try to come to optimize their thing is

1281
01:38:12,330 --> 01:38:16,370
what i thought tried forgot forgotten thing where they tried to optimize the try to

1282
01:38:18,010 --> 01:38:22,350
maximize the thing was back to the to the scaling of the on the mortgage

1283
01:38:22,360 --> 01:38:24,560
the expectation

1284
01:38:24,620 --> 01:38:26,110
of this the

1285
01:38:26,120 --> 01:38:29,660
the square of distance between x and plus one xn OK so to try to

1286
01:38:29,660 --> 01:38:33,730
maximize to estimate these distances are maximal as possible

1287
01:38:34,430 --> 01:38:39,640
you've got this magic zero or two forty four that get comes out there should

1288
01:38:39,640 --> 01:38:44,120
be already in that makes that's based on the stupa strong assumptions so it's very

1289
01:38:44,120 --> 01:38:46,840
interesting results this is like

1290
01:38:46,850 --> 01:38:48,160
but it's

1291
01:38:48,300 --> 01:38:52,720
the and i am not quite convinced this is so useful particularly because the assumption

1292
01:38:52,800 --> 01:38:55,360
actually never satisfied

1293
01:38:55,400 --> 01:38:57,080
but is it before

1294
01:38:58,730 --> 01:39:00,080
what we could do

1295
01:39:00,110 --> 01:39:02,730
OK let's try to pull the whether discuss

1296
01:39:02,750 --> 01:39:04,650
he is to use those

1297
01:39:04,830 --> 01:39:09,260
went into type techniques like that i will discuss it at the MCMC is really

1298
01:39:09,710 --> 01:39:14,000
you use the pages don't put up the simulated markov chain on relatively you try

1299
01:39:14,060 --> 01:39:19,380
to tune essentially the parameter of the algorithm so as to satisfy the given criteria

1300
01:39:19,470 --> 01:39:24,610
so for example you can try to tune it so that the acceptance rate of

1301
01:39:24,660 --> 01:39:28,470
your metropolises o point two four fall on you try to tune it so i

1302
01:39:28,500 --> 01:39:34,230
basically what are they doing that trying to basically goes to maximize the distance between

1303
01:39:34,630 --> 01:39:40,900
the squared distance between two two successive state of himachal chain you can almost OK

1304
01:39:40,900 --> 01:39:46,500
so you can do that on only section called related technically to stochastic approximation literature

1305
01:39:46,530 --> 01:39:50,220
skip so what people doing well so

1306
01:39:50,250 --> 01:39:55,010
people nodes pollinators and well like quite well on complex point four four

1307
01:39:55,050 --> 01:39:59,200
for a long time to do active our for fifty years OK

1308
01:40:00,690 --> 01:40:04,030
most of the techniques that have been developed to try to come

1309
01:40:04,030 --> 01:40:09,180
we with some more to bypass the limitation of still MCMC agrees

1310
01:40:09,210 --> 01:40:14,160
he is only we see this is exactly the same thing in the context

1311
01:40:14,190 --> 01:40:21,450
of particle method they tried to modify the initial something problem by essentially adding some

1312
01:40:21,450 --> 01:40:25,990
kind of oxygen are volleyball on CD target solutions we try to modify the original

1313
01:40:25,990 --> 01:40:28,630
problem so that the something that's easy

1314
01:40:28,640 --> 01:40:35,180
o is really something which i think policy very disturbing for people working saying optimisation

1315
01:40:35,190 --> 01:40:40,380
of kind of learning theory where people have tried to do like sampled bones they're

1316
01:40:40,380 --> 01:40:44,610
trying to come up with a grenade grenade essentially this is very

1317
01:40:44,630 --> 01:40:48,260
so let's look at this i would have been proposed based on what like an

1318
01:40:48,260 --> 01:40:54,180
intrusion on basically is as i say always very difficult to obtain some kind of

1319
01:40:54,180 --> 01:40:58,570
theoretical reason for that sort of the same reason that the cities like we the

1320
01:40:58,570 --> 01:41:05,140
theoretical justification for that it's more basically people have intrusions going increase empolis beating the

1321
01:41:05,140 --> 01:41:08,590
mixing of the markov chain on basically might have been able to put it on

1322
01:41:08,590 --> 01:41:11,460
speech he he is is that he take

1323
01:41:11,480 --> 01:41:15,540
clark was text corpus domain specific problem which is kind of a lot of things

1324
01:41:16,040 --> 01:41:21,120
which is example our domain which is monday and then

1325
01:41:23,870 --> 01:41:28,440
problems and put all the information

1326
01:41:28,440 --> 01:41:30,580
try to capture the information and put them

1327
01:41:30,600 --> 01:41:38,140
into ontology so we concentrate in this in this content on the text content and

1328
01:41:38,140 --> 01:41:45,350
try to bring the content into ontology and model the ontology sold except for this

1329
01:41:47,140 --> 01:41:49,160
but this ontology learning

1330
01:41:49,210 --> 01:41:54,480
the first step is that we try to capture the terms and the problem of

1331
01:41:54,480 --> 01:41:57,940
this is that we have a lot of terms which are located in

1332
01:41:58,460 --> 01:42:02,890
the was but it must find out the meaning to be we can use the

1333
01:42:02,890 --> 01:42:07,810
to to find a lot of the terms to continue if in this weekend's called

1334
01:42:07,830 --> 01:42:12,830
yes i have and what is known from the university part of speech tracking and

1335
01:42:12,850 --> 01:42:15,830
so on so we can find out what's going on in a certain way and

1336
01:42:15,830 --> 01:42:20,230
what's the problem can give them something but in the end the user must decide

1337
01:42:20,230 --> 01:42:25,480
what is big shock probability and how you arrange them in the hierarchy and so

1338
01:42:25,480 --> 01:42:30,290
on so but try to find out how to get to him is list of

1339
01:42:30,690 --> 01:42:37,390
which out in the second step for instance we tried to some relations between this

1340
01:42:37,390 --> 01:42:43,830
concept he captured in the first step to be taken to determine put concept into

1341
01:42:43,960 --> 01:42:47,390
the ontology arrange them in the right way and now we try to capture this

1342
01:42:47,440 --> 01:42:53,520
taxonomic is is relation between the different concepts the next step would be to find

1343
01:42:53,520 --> 01:43:00,290
some non-toxic and also fifteen users on his some data-mining techniques and is that i

1344
01:43:00,810 --> 01:43:04,250
i mean if you in the next slide a little bit more

1345
01:43:04,370 --> 01:43:11,210
details about this that and because of the time constraints sitting steps

1346
01:43:11,350 --> 01:43:14,080
maybe you can download some

1347
01:43:14,080 --> 01:43:19,390
it just goes down the last point i want mentioned is the index is a

1348
01:43:19,390 --> 01:43:24,730
lot of resources for instance but it's quite large and not only focused following the

1349
01:43:24,730 --> 01:43:28,660
domain if you are interested in what they really want to hold about the site

1350
01:43:28,750 --> 01:43:35,980
of the whole english information that you need some steps forward under the under certain

1351
01:43:35,980 --> 01:43:41,170
domain and for this we developed some pruning technique also some of the things i

1352
01:43:41,170 --> 01:43:47,670
find to be talking about in this problems in that to do lots of ontology

1353
01:43:47,670 --> 01:43:54,960
and we think also the we have no basis to start modelling a smaller basis

1354
01:43:54,980 --> 01:43:57,330
to start so

1355
01:43:57,350 --> 01:44:03,540
that's coming towards semantics and regulations which is one of the ontology learning from this

1356
01:44:03,850 --> 01:44:10,290
for an ontology from the semantic web so we have on the right side a

1357
01:44:10,290 --> 01:44:12,520
small ontology

1358
01:44:12,560 --> 01:44:18,040
and an example of ontology which is about events areas fishing and so on and

1359
01:44:18,040 --> 01:44:21,980
on the left but we have some website which we can take

1360
01:44:22,000 --> 01:44:29,270
that's important what was to derive some further information goal is to find out which

1361
01:44:29,270 --> 01:44:34,190
relation exists between some concepts and for this we apply

1362
01:44:34,250 --> 01:44:36,190
some of the station woods

1363
01:44:36,210 --> 01:44:42,620
which i hope is very very knowing and what we made is the

1364
01:44:42,620 --> 01:44:49,270
he capture some relations of the terms in these documents and we count if often

1365
01:44:49,270 --> 01:44:50,940
occur together for instance the

1366
01:44:51,480 --> 01:44:57,060
it's an area which concepts of the ontology and you can have a lot of

1367
01:44:57,060 --> 01:45:00,000
terms from this document to this concept

1368
01:45:00,080 --> 01:45:05,420
how often then there must be a relation between

1369
01:45:05,440 --> 01:45:12,190
this the concepts the drive an relation an accommodation for for instance relations

1370
01:45:13,480 --> 01:45:16,100
i'm not only did she has to

1371
01:45:16,140 --> 01:45:21,020
this is located in a certain area and

1372
01:45:21,040 --> 01:45:23,870
the problem is that you know already

1373
01:45:23,890 --> 01:45:26,140
that's the accommodation and

1374
01:45:26,210 --> 01:45:30,210
gary are related to each other but we you don't know how they related to

1375
01:45:30,420 --> 01:45:34,980
one of the problem we have we can only present the user data relation but

1376
01:45:34,980 --> 01:45:39,190
we don't know which relation and this one that is that you can do we

1377
01:45:39,190 --> 01:45:45,940
can try to find out how they are related you can use some grass it's

1378
01:45:45,940 --> 01:45:47,920
normally gives some people to

1379
01:45:47,920 --> 01:45:53,230
how you rate but it's the next step so it's one of the example that

1380
01:45:53,230 --> 01:45:56,060
i want to show you another

1381
01:45:56,080 --> 01:46:01,310
now it's it's the next step maybe have and what or ontology which is given

1382
01:46:01,330 --> 01:46:05,290
to us and the problem the next problem is that we need some instances to

1383
01:46:05,290 --> 01:46:10,170
fill up this ontology can be can only

1384
01:46:10,190 --> 01:46:15,160
brokerage this ontology if you have some instances of that have some ideas to bring

1385
01:46:16,580 --> 01:46:22,540
the instances automatically automatically into the ontology

1386
01:46:22,580 --> 01:46:24,580
according to our knowledge base

1387
01:46:24,600 --> 01:46:30,080
so we have on the top of his life and example from ontology and we

1388
01:46:30,080 --> 01:46:32,440
had this

1389
01:46:32,480 --> 01:46:42,000
that aside i presented last slide and now we are interested in find some

1390
01:46:42,020 --> 01:46:43,890
in fact

1391
01:46:44,500 --> 01:46:50,500
between one into the ontology and he can do something nice from the information is

1392
01:46:50,500 --> 01:46:51,920
one example

1393
01:46:51,920 --> 01:46:54,880
where you can actually solve problems

1394
01:46:54,900 --> 01:47:00,000
by running the junction tree our or belief propagation

1395
01:47:00,050 --> 01:47:09,750
and not all

1396
01:47:09,820 --> 01:47:11,500
every triangulation

1397
01:47:11,500 --> 01:47:14,320
i will give you the same marginals

1398
01:47:14,340 --> 01:47:19,230
i mean these are with them is an exact power no approximation

1399
01:47:19,250 --> 01:47:23,300
it's just making things convenient in the sense that

1400
01:47:23,340 --> 01:47:30,340
it's very structured our from that creates these nice but the structure

1401
01:47:30,340 --> 01:47:32,070
when you first compile

1402
01:47:33,070 --> 01:47:37,880
computation that you can we compute everything you can precompute

1403
01:47:37,940 --> 01:47:41,710
so that the query time you just use exactly

1404
01:47:41,710 --> 01:47:45,860
the minimum of information create time

1405
01:47:47,130 --> 01:47:50,270
everything will be equal

1406
01:47:52,570 --> 01:47:54,610
approximate computational

1407
01:47:57,880 --> 01:48:03,020
OK so just illustration of some practical examples that you can solve the graphical models

1408
01:48:03,050 --> 01:48:07,190
i mean he just illustrate

1409
01:48:07,230 --> 01:48:10,460
first of all let's assume you want to solve that some

1410
01:48:10,500 --> 01:48:15,130
well probably very large cluster very complex pattern recognition problems

1411
01:48:15,150 --> 01:48:17,050
which is matching structure

1412
01:48:17,110 --> 01:48:21,750
we do this all the time right when we are recognising people in recognizing faces

1413
01:48:21,840 --> 01:48:23,460
recognising objects

1414
01:48:23,500 --> 01:48:30,980
and particularly fusion but also text of every every recombination activity involves

1415
01:48:32,360 --> 01:48:35,840
some sort of matching between your model

1416
01:48:35,840 --> 01:48:37,880
that you have in your brain

1417
01:48:38,790 --> 01:48:40,630
the observation

1418
01:48:42,130 --> 01:48:43,880
and if that model

1419
01:48:43,960 --> 01:48:45,920
if that match score

1420
01:48:45,940 --> 01:48:50,400
is higher than the given value then you classify something as belonging to some classes

1421
01:48:52,940 --> 01:48:54,400
i mean

1422
01:48:54,400 --> 01:48:57,590
its population is very very complex problem i mean

1423
01:48:57,630 --> 01:49:02,250
you can model this pattern recognition problems as a graphical model why because graphical models

1424
01:49:02,250 --> 01:49:05,520
are very natural things to model for example

1425
01:49:05,550 --> 01:49:07,650
geometric constraints

1426
01:49:07,730 --> 01:49:12,480
distances relative distances can be modeled as pairwise potential

1427
01:49:12,520 --> 01:49:14,590
between also fire

1428
01:49:14,650 --> 01:49:17,230
angles can be modelled as

1429
01:49:17,250 --> 01:49:21,170
potentials involve at triple the fire

1430
01:49:22,820 --> 01:49:24,840
by using

1431
01:49:26,770 --> 01:49:28,900
for example you can enforce

1432
01:49:28,920 --> 01:49:32,820
region matching by matching distances

1433
01:49:32,860 --> 01:49:36,230
you just defined distances of being potential

1434
01:49:36,300 --> 01:49:39,520
of pairs of fire

1435
01:49:39,570 --> 01:49:40,400
you can do

1436
01:49:40,420 --> 01:49:44,250
even more offensive stuff for example you do so a fine

1437
01:49:44,250 --> 01:49:46,340
matching what

1438
01:49:46,340 --> 01:49:49,710
u one to match

1439
01:49:49,820 --> 01:49:54,750
it happens that are different by find transformation what you find the information is is

1440
01:49:55,710 --> 01:50:00,920
but the most general linear transformation that you have something of these five

1441
01:50:01,030 --> 01:50:04,840
so you can actually stretched according to scale the

1442
01:50:04,880 --> 01:50:06,360
and translator

1443
01:50:07,150 --> 01:50:11,190
have constant here so they can translate this thing

1444
01:50:11,250 --> 01:50:12,630
and you have access

1445
01:50:12,650 --> 01:50:16,000
and they have this is major fear of coefficients

1446
01:50:16,050 --> 01:50:20,980
a one one one two two one and two two that basically definable the scale

1447
01:50:23,210 --> 01:50:24,940
of your path

1448
01:50:25,000 --> 01:50:29,500
and you can in called these in a graphical models

1449
01:50:31,900 --> 01:50:33,770
in this way

1450
01:50:33,790 --> 01:50:37,340
this is an example some work done in the past

1451
01:50:37,480 --> 01:50:39,770
so for example if you have

1452
01:50:39,790 --> 01:50:41,980
o point correspondences of this time

1453
01:50:42,000 --> 01:50:44,860
you need three point correspondences to uniquely

1454
01:50:46,250 --> 01:50:50,480
because this system here this is the general linear transformation in the plane

1455
01:50:50,480 --> 01:50:52,860
it's a system of two equations

1456
01:50:53,500 --> 01:50:55,770
you have six via

1457
01:50:55,820 --> 01:50:57,750
in order to solve the system i you

1458
01:50:57,770 --> 01:50:59,690
three of these

1459
01:50:59,750 --> 01:51:01,730
pairs of equation

1460
01:51:02,770 --> 01:51:06,270
so if i have three correspondences

1461
01:51:06,320 --> 01:51:08,110
like this

1462
01:51:08,130 --> 01:51:09,650
i'm able to give

1463
01:51:09,690 --> 01:51:12,610
three sets of these equations for x and y

1464
01:51:12,610 --> 01:51:17,480
i can solve this system that uniquely determine the final transformation

1465
01:51:17,530 --> 01:51:20,520
well it turns out that they can

1466
01:51:20,550 --> 01:51:24,090
use a graph of this type of graphical model of this talk

1467
01:51:24,150 --> 01:51:29,960
working every clique of size three for example i called one possible if i'm transformation

1468
01:51:30,000 --> 01:51:32,750
by running the junction tree our here

1469
01:51:32,770 --> 01:51:34,250
i force

1470
01:51:34,270 --> 01:51:37,050
i can enforce it every

1471
01:51:37,050 --> 01:51:38,670
the trouble is that get something

1472
01:51:39,170 --> 01:51:43,770
is you can have serious rejection sampling as a component in those

1473
01:51:44,530 --> 01:51:45,460
so let's move on now

1474
01:51:45,900 --> 01:51:47,420
and talk about the metropolis and

1475
01:51:49,750 --> 01:51:51,500
so the methods we've looked at so far

1476
01:51:53,460 --> 01:51:54,250
much elements

1477
01:51:57,530 --> 01:51:58,340
the blindfold

1478
01:51:59,710 --> 01:52:02,320
i was told to go to a random place and look there

1479
01:52:02,840 --> 01:52:06,130
you start and maybe some other information you want

1480
01:52:06,610 --> 01:52:07,570
moving around

1481
01:52:09,610 --> 01:52:11,750
every move is independent

1482
01:52:12,340 --> 01:52:16,480
the previous so the new location we said to just rules into every time you

1483
01:52:17,440 --> 01:52:18,570
above something which

1484
01:52:19,900 --> 01:52:21,420
now the metropolis that

1485
01:52:22,070 --> 01:52:24,900
it's going to be an example of a markov chain

1486
01:52:25,820 --> 01:52:26,480
what elements

1487
01:52:27,400 --> 01:52:29,380
which means that instead of asking

1488
01:52:31,820 --> 01:52:33,000
you go to a new location

1489
01:52:33,940 --> 01:52:36,270
it's completely independent where he's been or

1490
01:52:36,880 --> 01:52:41,590
there's going to be a dependence on where he is at the moment so when he is now

1491
01:52:42,440 --> 01:52:43,980
going to move into a new location

1492
01:52:44,520 --> 01:52:46,230
that depends on the current location

1493
01:52:47,420 --> 01:52:51,380
why should help is not at all clear that why we want to be

1494
01:52:51,840 --> 01:52:52,730
dependence on

1495
01:52:54,420 --> 01:52:55,880
but i want it

1496
01:52:56,570 --> 01:52:58,340
so now you have two of

1497
01:53:02,690 --> 01:53:03,150
this is

1498
01:53:04,190 --> 01:53:05,090
the next location

1499
01:53:08,170 --> 01:53:08,770
and this is

1500
01:53:09,570 --> 01:53:10,520
the current location

1501
01:53:11,900 --> 01:53:13,270
and this assumption is going to be

1502
01:53:13,980 --> 01:53:15,150
we can draw from q

1503
01:53:15,750 --> 01:53:20,170
so q can be something simple like a gas in distribution or something then points

1504
01:53:21,460 --> 01:53:23,570
and we're going to assume that we can evaluate you

1505
01:53:26,800 --> 01:53:28,050
so we have an acid solution

1506
01:53:31,380 --> 01:53:31,840
you read

1507
01:53:34,960 --> 01:53:35,710
is nothing

1508
01:53:38,610 --> 01:53:39,630
look like

1509
01:53:41,110 --> 01:53:42,480
this example if

1510
01:53:44,360 --> 01:53:45,590
this is the current location

1511
01:53:47,980 --> 01:53:48,880
hand if

1512
01:53:52,070 --> 01:53:53,270
this is an application

1513
01:53:55,190 --> 01:53:56,190
the right

1514
01:53:57,050 --> 01:53:58,610
and maybe you might look like

1515
01:54:03,050 --> 01:54:04,920
that's the sort of concept we have maybe

1516
01:54:05,670 --> 01:54:09,980
was just distribution is centered on the current location and the things that

1517
01:54:10,860 --> 01:54:12,610
it can be any distribution like

1518
01:54:14,130 --> 01:54:17,210
as long as you can draw from it and you can evaluate it so you

1519
01:54:17,210 --> 01:54:19,300
can think of this as being completely general

1520
01:54:20,750 --> 01:54:26,340
you can draw from this is the sort of thing that you it is a small large state

1521
01:54:27,170 --> 01:54:31,250
or perhaps a bit much you could use very broadcasting want

1522
01:54:32,750 --> 01:54:34,650
or any distribution that you can draw from

1523
01:54:36,300 --> 01:54:37,300
so it's okay

1524
01:54:39,770 --> 01:54:40,460
which should to be

1525
01:54:46,000 --> 01:54:47,130
something to emphasise their

1526
01:54:49,380 --> 01:54:49,980
it's okay

1527
01:54:54,750 --> 01:54:55,500
look like it

1528
01:54:59,730 --> 01:55:00,650
importance sampling

1529
01:55:01,290 --> 01:55:02,190
rejection sampling

1530
01:55:02,770 --> 01:55:06,730
this work really well if q looks just like he presented

1531
01:55:07,550 --> 01:55:08,090
it's like

1532
01:55:09,820 --> 01:55:11,630
if you can make that looks like an

1533
01:55:11,880 --> 01:55:12,460
draw from it

1534
01:55:12,920 --> 01:55:14,770
and that means you can solve the original problem the

1535
01:55:16,520 --> 01:55:17,210
some of

1536
01:55:19,250 --> 01:55:20,900
in the metropolis it's okay

1537
01:55:20,900 --> 01:55:23,890
but it's there classical logic it isn't there or not

1538
01:55:23,900 --> 01:55:27,400
but it's not a lot nicer having this adding

1539
01:55:27,450 --> 01:55:29,420
distribution in a primitive

1540
01:55:30,350 --> 01:55:34,690
because it's just ugly doing that i mean just saying blk all got these rules

1541
01:55:34,690 --> 01:55:38,730
for the connectives and that's what we mean by the connectives but i yeah there's

1542
01:55:38,760 --> 01:55:40,460
one two

1543
01:55:41,070 --> 01:55:44,490
and you say where does that come from all comes from the river and talked

1544
01:55:44,490 --> 01:55:48,710
about the cleanest proofs you've got a

1545
01:55:48,740 --> 01:55:52,980
model theory and you got proof theory it's nice to see the match up because

1546
01:55:52,980 --> 01:55:54,850
they both seem into it

1547
01:55:54,860 --> 01:55:59,000
well here is that really matchups we got patch

1548
01:55:59,030 --> 01:56:03,190
we can throw in another arxiv o now the match

1549
01:56:03,200 --> 01:56:07,440
well is it with a new proof theory wasn't doing the job giving

1550
01:56:07,450 --> 01:56:09,450
a real understanding

1551
01:56:09,460 --> 01:56:13,510
proof in the system is this is just say oh yeah distribution which is going

1552
01:56:13,510 --> 01:56:18,000
to throw it and in fact the interesting belnap systems original ones and i'm not

1553
01:56:18,000 --> 01:56:22,500
having go them i'm not saying there are a bunch of tricks or anything like

1554
01:56:22,500 --> 01:56:26,030
this now they were they are very smart but they were doing this sort of

1555
01:56:26,030 --> 01:56:31,360
you know after hope they did their ground-breaking work a lot of other people thought

1556
01:56:31,360 --> 01:56:32,970
about it

1557
01:56:32,980 --> 01:56:35,210
and eventually rose came up with this

1558
01:56:35,210 --> 01:56:39,260
which is as i say quite elegant and easy

1559
01:56:39,290 --> 01:56:43,230
and very intuitive because you say OK well this is what the proof by cases

1560
01:56:43,230 --> 01:56:47,880
it we're going to take a look at in parallel what happens

1561
01:56:50,440 --> 01:56:52,440
to each of these things

1562
01:56:52,480 --> 01:56:54,910
and instead here we did

1563
01:56:54,950 --> 01:56:58,200
was we introduced a new subscript

1564
01:56:58,220 --> 01:56:59,720
right and that's why

1565
01:56:59,790 --> 01:57:04,010
we had a conjunction here we had to say see here

1566
01:57:04,020 --> 01:57:08,960
and we could also are d vere you see here we could

1567
01:57:08,970 --> 01:57:10,380
bringing in here

1568
01:57:10,390 --> 01:57:14,740
why because it this is discovered different subscript right for conjunction

1569
01:57:14,800 --> 01:57:18,200
introduction we eat the same subscript

1570
01:57:18,320 --> 01:57:24,300
that was the that's what ruins that having this and script restriction four

1571
01:57:24,360 --> 01:57:28,220
the conjunction is what makes things difficult

1572
01:57:29,870 --> 01:57:35,450
i have the distribution true for classical logic for it

1573
01:57:35,450 --> 01:57:37,230
it's thirteen lines long

1574
01:57:37,280 --> 01:57:39,140
list what i came up with

1575
01:57:39,170 --> 01:57:41,540
and i think that's probably about it

1576
01:57:43,530 --> 01:57:47,760
but you but also says why it fails and with relevant logic

1577
01:57:47,850 --> 01:57:50,850
but this pretty systems were much

1578
01:57:50,860 --> 01:57:54,850
as i said before and

1579
01:57:54,860 --> 01:57:58,750
ross has a

1580
01:57:58,810 --> 01:58:02,320
if you come to read his writing materials as this is difficult stuff and all

1581
01:58:03,430 --> 01:58:05,910
you have to read it a few times because what he's dealing with this very

1582
01:58:05,910 --> 01:58:10,790
hard in some ways but you can see beneath the surface is very elegant and

1583
01:58:11,940 --> 01:58:17,160
intuitions and is very good mathematical logician

1584
01:58:17,890 --> 01:58:20,950
is proven a lot of really standing

1585
01:58:21,000 --> 01:58:27,440
things one of which will talk about in the last half

1586
01:58:44,860 --> 01:58:49,010
we have plenty of time OK negation

1587
01:58:49,030 --> 01:58:52,540
the negation in relevant logic

1588
01:58:52,600 --> 01:58:55,180
looked at properly

1589
01:58:55,180 --> 01:58:56,260
my way

1590
01:58:56,650 --> 01:59:02,450
it is i think quite a bit like negation intuitionist logic and classical logic

1591
01:59:02,460 --> 01:59:05,640
and the key to this

1592
01:59:05,670 --> 01:59:07,840
is again false

1593
01:59:08,800 --> 01:59:13,720
but the falsum here is going to be an interesting thing

1594
01:59:14,440 --> 01:59:16,010
remember i told you

1595
01:59:16,040 --> 01:59:20,210
that in relevant logics

1596
01:59:43,810 --> 01:59:45,710
a or b

1597
01:59:45,720 --> 01:59:49,990
is this is called the variable sharing property that we talked about

1598
01:59:50,960 --> 01:59:53,430
a erobi

1599
01:59:53,450 --> 01:59:55,210
is going to be a theory

1600
01:59:55,230 --> 01:59:58,110
however out of tracks

1601
01:59:58,120 --> 02:00:03,720
only have a and b share least one propositional variables

1602
02:00:04,960 --> 02:00:06,110
that's too

1603
02:00:06,150 --> 02:00:11,010
make them in some sense relevant one of the only thing i think makes rather

1604
02:00:11,070 --> 02:00:16,120
i think the notion of real use a premises in introduction is what really defines

1605
02:00:21,190 --> 02:00:26,510
is something interesting has an interesting property and i think it's important property the one

1606
02:00:26,710 --> 02:00:28,170
thing reasons why

1607
02:00:28,170 --> 02:00:29,650
it's interesting

1608
02:00:29,660 --> 02:00:35,370
it is because of the treatment of contradiction server we saw in

1609
02:00:35,380 --> 02:00:41,100
intuitionist logic as it's normally construed by the BHK interpretation

1610
02:00:42,090 --> 02:00:44,130
the notion of contradiction

1611
02:00:44,170 --> 02:00:49,620
it doesn't include the notion negation can because it used to define negation

1612
02:00:49,630 --> 02:00:51,780
and we don't want to circular definitions

1613
02:00:54,260 --> 02:00:56,670
relevant laundry

1614
02:00:56,670 --> 02:00:59,510
the notion of contradiction isn't

1615
02:00:59,570 --> 02:01:04,480
we need national contradiction actually it's very well

1616
02:01:04,500 --> 02:01:06,710
develop notion yet

1617
02:01:06,710 --> 02:01:11,780
relevant logic isn't that new goes back to nineteen fifties are actually the very first

1618
02:01:11,780 --> 02:01:15,950
relevant logic was developed in nineteen twenty eight by a guy named ivan or love

1619
02:01:15,950 --> 02:01:18,180
and we know nothing about him apart this one

1620
02:01:18,190 --> 02:01:22,860
paper he wrote maybe now people know that more about them but i think

1621
02:01:22,950 --> 02:01:27,460
use user action he wrote this paper in russian it wasn't rediscovered until the nineteen

1622
02:01:28,600 --> 02:01:36,190
our lady it is by cost division whose serbian but also happens to regression and

1623
02:01:36,460 --> 02:01:42,800
he revealed to the world translated as quite nice paper but

1624
02:01:42,820 --> 02:01:44,370
but all of that he was

1625
02:01:44,370 --> 02:01:52,020
giving after broward intuitions mathematics people want to come up with intuitionist logic and and

1626
02:01:52,030 --> 02:01:56,150
height eventually is one who came up with what we call intuitions logic now but

1627
02:01:56,150 --> 02:01:59,430
so there this very common

1628
02:01:59,710 --> 02:02:03,560
reduction in the book being concerned about which is multiclass

1629
02:02:03,600 --> 02:02:05,640
two binary classification

1630
02:02:05,690 --> 02:02:07,920
so as the first thing i'm going to talk about

1631
02:02:12,480 --> 02:02:13,860
the next thing

1632
02:02:13,880 --> 02:02:17,160
his cousins the classification the classification

1633
02:02:17,170 --> 02:02:20,360
the cool thing ideas about this

1634
02:02:20,370 --> 02:02:21,780
is that so general

1635
02:02:21,790 --> 02:02:23,640
as a classification problem

1636
02:02:23,690 --> 02:02:27,910
and you can think of every classification sort of problem is

1637
02:02:27,920 --> 02:02:30,080
cousins defuzzification

1638
02:02:30,160 --> 02:02:35,740
the idea is that later

1639
02:02:35,790 --> 02:02:38,030
OK so multiclass classification

1640
02:02:38,040 --> 02:02:40,400
it is the same as binary classification

1641
02:02:40,410 --> 02:02:41,950
except we have more labels

1642
02:02:43,070 --> 02:02:47,350
seventeen zero one you predict one two k

1643
02:02:47,390 --> 02:02:50,870
any classifier to predict one get

1644
02:02:50,880 --> 02:02:51,640
and then

1645
02:02:51,650 --> 02:02:53,830
given your multiclass datasets

1646
02:02:53,920 --> 02:02:56,410
you define a multiclass classifier

1647
02:02:56,420 --> 02:02:59,110
with small areas so that means that

1648
02:02:59,120 --> 02:02:59,680
you know

1649
02:02:59,720 --> 02:03:02,540
the probability predict around from d

1650
02:03:07,720 --> 02:03:09,870
once again i'm not going to be

1651
02:03:09,920 --> 02:03:12,300
throughout everything i'm assuming that

1652
02:03:13,420 --> 02:03:15,300
examples are actually drawn from d

1653
02:03:15,380 --> 02:03:17,950
it's not going to be the now

1654
02:03:17,960 --> 02:03:23,390
OK so this should be straightforward is using binary classification

1655
02:03:23,400 --> 02:03:28,660
just smart labels

1656
02:03:28,670 --> 02:03:31,330
and then

1657
02:03:31,340 --> 02:03:34,930
you want do but you want you to solve multiclass to binary

1658
02:03:34,940 --> 02:03:36,410
so there's is this technique

1659
02:03:36,860 --> 02:03:38,510
first by

1660
02:03:38,890 --> 02:03:40,770
in bacteria

1661
02:03:40,820 --> 02:03:41,960
what is this

1662
02:03:41,970 --> 02:03:44,840
pretty nice actually

1663
02:03:44,890 --> 02:03:45,890
it says

1664
02:03:45,910 --> 02:03:48,440
suppose you by binding learning our rules

1665
02:03:48,520 --> 02:03:51,810
and you can solve multiclass classification the following way

1666
02:03:51,860 --> 02:03:53,480
for every

1667
02:03:53,500 --> 02:03:56,120
you can create a weighted binary problems

1668
02:03:56,170 --> 02:04:00,520
which are distinguishing between a subset labels in those subset cells

1669
02:04:00,600 --> 02:04:02,570
first by many problems here

1670
02:04:02,610 --> 02:04:06,900
and the correct prediction is one

1671
02:04:06,950 --> 02:04:10,580
it is the label is one or two things which predict one

1672
02:04:10,650 --> 02:04:15,580
in three or four users predict zero

1673
02:04:15,650 --> 02:04:18,930
and then you can create another minor problem which is here

1674
02:04:18,980 --> 02:04:22,430
it's one of its one three thinking about a problem which is

1675
02:04:22,480 --> 02:04:29,520
one of its water for

1676
02:04:29,580 --> 02:04:30,690
and then

1677
02:04:30,970 --> 02:04:34,460
at classification time with you want to try to predict

1678
02:04:34,470 --> 02:04:35,710
what is the right

1679
02:04:37,110 --> 02:04:38,460
multi class

1680
02:04:39,950 --> 02:04:42,560
you can run it if you want classifiers

1681
02:04:42,570 --> 02:04:46,740
you can get some predictions like for example in

1682
02:04:46,850 --> 02:04:48,940
in this case it is clear that

1683
02:04:48,990 --> 02:04:50,410
oppose it looks pretty clear

1684
02:04:52,270 --> 02:04:53,790
it is the right

1685
02:04:53,800 --> 02:04:56,780
multi class label predict

1686
02:04:58,780 --> 02:05:01,490
we have a look at least one error no matter where we are

1687
02:05:01,500 --> 02:05:03,570
and the possibilities of one are

1688
02:05:03,580 --> 02:05:05,490
one three or four

1689
02:05:05,500 --> 02:05:09,310
so maybe randomize over this

1690
02:05:09,410 --> 02:05:10,680
and then

1691
02:05:10,730 --> 02:05:15,020
you can have a very strange

1692
02:05:15,040 --> 02:05:17,190
it's two areas where you

1693
02:05:18,160 --> 02:05:23,620
are now is one are here

1694
02:05:25,610 --> 02:05:29,170
the way you do this you define some sort of set of labels

1695
02:05:29,180 --> 02:05:31,100
the binary training data

1696
02:05:31,110 --> 02:05:32,670
you just x

1697
02:05:34,280 --> 02:05:38,320
the indicator function of with not the actual labels in the set

1698
02:05:38,360 --> 02:05:40,800
that is how you do it for each of these

1699
02:05:44,180 --> 02:05:47,530
and then finally the way to make a prediction is you just find the closest

1700
02:05:48,590 --> 02:05:53,200
in hamming distance anschluss

1701
02:06:00,900 --> 02:06:02,240
there was a very nice

1702
02:06:02,270 --> 02:06:03,740
simple analysis

1703
02:06:03,760 --> 02:06:09,460
plessy reduction step analysis

1704
02:06:09,530 --> 02:06:13,210
so we're going to use that one test classifier trick so we can take all

1705
02:06:13,220 --> 02:06:16,830
these different binary problems

1706
02:06:18,280 --> 02:06:19,400
and we can embed

1707
02:06:19,410 --> 02:06:21,110
the name of the set

1708
02:06:21,150 --> 02:06:23,150
into the feature space

1709
02:06:23,190 --> 02:06:28,140
the nice think about having any classifier

1710
02:06:28,150 --> 02:06:29,680
OK so

1711
02:06:30,610 --> 02:06:33,500
but you see that one but classifier

1712
02:06:33,550 --> 02:06:35,560
i think about

1713
02:06:35,570 --> 02:06:36,420
this is

1714
02:06:37,580 --> 02:06:39,930
the multiclass distribution

1715
02:06:39,940 --> 02:06:42,780
is the induced distribution in the binary

1716
02:06:42,830 --> 02:06:47,500
one by the classifier

1717
02:06:47,510 --> 02:06:49,700
and the claim is that for every

1718
02:06:49,760 --> 02:06:52,180
number of multi class labels

