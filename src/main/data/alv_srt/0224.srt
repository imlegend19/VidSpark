1
00:00:00,000 --> 00:00:08,740
control the difference between arcade just normal function and its at and finally the approximation

2
00:00:08,740 --> 00:00:10,850
error so that

3
00:00:12,370 --> 00:00:22,350
OK so I will be fast and firstly show how to the estimation error this

4
00:00:22,350 --> 00:00:29,910
is just an implication of theory from Scotland Steinbach in the article frustrates for SVM

5
00:00:29,910 --> 00:00:37,740
with Gaussian kernel so here we can just grab that are are entries in the

6
00:00:37,740 --> 00:00:42,540
restaurants and nothing more there's no there's no trick

7
00:00:43,310 --> 00:00:49,990
and so that is the only work we had to do was determining the smallest

8
00:00:50,000 --> 00:00:57,600
functional subspace where we know for sure that the estimator of signal lights

9
00:00:58,230 --> 00:01:03,670
so of course we know by definition of sigma hat that it belongs to the

10
00:01:04,270 --> 00:01:11,530
arcade just speech signal but what we can show is that it belongs more precisely

11
00:01:11,560 --> 00:01:20,450
to the blue subsets which is up to this multiplicative constants the unit ball of

12
00:01:20,470 --> 00:01:24,570
of this part of the subset

13
00:01:25,250 --> 00:01:31,750
so true to see that the instances like representer theorem that we used to show

14
00:01:31,750 --> 00:01:36,330
that the solution of this and can be expanded on

15
00:01:36,410 --> 00:01:40,020
this kind of functions where it's only belongs

16
00:01:40,040 --> 00:01:42,790
compact subsets

17
00:01:42,990 --> 00:01:49,050
and it's really interesting and so is important for us to have a compact index

18
00:01:49,050 --> 00:01:55,990
here since it enables them to nicely controlled the covering numbers

19
00:01:56,370 --> 00:01:57,450
of the

20
00:01:58,190 --> 00:02:00,990
of this set so

21
00:02:01,080 --> 00:02:01,770
here we

22
00:02:02,560 --> 00:02:09,270
took a result directly from going straight up so we bound to the number of

23
00:02:09,500 --> 00:02:15,390
this you need both with respect to the entire recall as not still t is

24
00:02:15,390 --> 00:02:22,020
the training sample and in this case is that we're interested in and so we

25
00:02:22,020 --> 00:02:27,200
get a bound on the covering number will you be able have time interval

26
00:02:27,290 --> 00:02:33,350
so more anyway that's not where I want to put the focus so

27
00:02:33,390 --> 00:02:34,810
it turns out that

28
00:02:36,240 --> 00:02:42,240
still directly using the removed from school in China but we have the following by

29
00:02:42,520 --> 00:02:45,080
on the excessive summarized OK

30
00:02:45,510 --> 00:02:51,450
so I just know that this bound is parameterized by

31
00:02:51,470 --> 00:02:57,310
2 values that key that you can play with we can take its between 0

32
00:02:57,310 --> 00:03:00,270
and 1 and also

33
00:03:00,350 --> 00:03:06,970
all I forgot the reserve number of delta that we consider as we want you

34
00:03:06,970 --> 00:03:13,060
just have to be a positive quantity of course playing with B and that that

35
00:03:13,060 --> 00:03:17,810
affects the constant K 1 so here k 1 is a constant

36
00:03:18,160 --> 00:03:23,540
well it does not depend on the aren't any there and signal that depends on

37
00:03:23,600 --> 00:03:26,740
all other quantities

38
00:03:26,790 --> 00:03:31,270
so that's the way we the estimation error so that's really

39
00:03:31,490 --> 00:03:40,970
there is no surprise you just use the theory is so we've got to more

40
00:03:41,010 --> 00:03:52,110
terms to belong to a solicitor regularization so to control of this term it's very

41
00:03:52,110 --> 00:04:00,630
convenient to work with the following characterization of the Gaussians populations so that's what understand

42
00:04:00,770 --> 00:04:03,290
say here

43
00:04:04,050 --> 00:04:10,070
we can say that the Gaussian architects in the set of continuous functions vanishing at

44
00:04:12,330 --> 00:04:15,270
that are integrable on on the

45
00:04:15,510 --> 00:04:26,110
and which have a fully transform that satisfies this integrability constraint OK so these constraints

46
00:04:26,110 --> 00:04:35,510
means that the Fourier transform vanishes at infinity exponentially fast and this has to be

47
00:04:35,510 --> 00:04:39,330
in general so we don't old the frequencies

48
00:04:39,470 --> 00:04:47,990
and so this is the correct transition of the Gaussian Park H S and the

49
00:04:48,420 --> 00:04:51,710
just norms is precisely

50
00:04:51,770 --> 00:04:57,290
this quantity here we have this multiplicative constants

51
00:05:03,490 --> 00:05:15,710
OK so from this characterization we can show to interesting properties 1st

52
00:05:16,170 --> 00:05:23,550
when sigma until all the this way than the corresponding are older and this way

53
00:05:23,560 --> 00:05:28,950
that means that the when the signal gets smaller and smaller

54
00:05:28,970 --> 00:05:36,770
the associated architectures H sigma gets larger and larger forms of nested collections of additional

55
00:05:36,770 --> 00:05:43,000
spaces at the end we almost reached as to

56
00:05:43,580 --> 00:05:51,330
and the 2nd property that is more quantitative relates the arcade just normally signal to

57
00:05:51,330 --> 00:05:59,130
the market norm in H for 2 different values of the mind so to pass

58
00:05:59,130 --> 00:06:02,150
from 1 to another we have to

59
00:06:02,160 --> 00:06:10,520
to pay this price sigma squared over 2 0 squared this is way to pass

60
00:06:10,560 --> 00:06:15,310
from H signal to edge to it

61
00:06:22,970 --> 00:06:28,090
and so from those properties

62
00:06:28,100 --> 00:06:40,410
we can know gonna see here how how to bomb the regularisation so this is

63
00:06:40,450 --> 00:06:46,160
the regularisation error so this is the difference between the arcade just Norman as to

64
00:06:46,180 --> 00:06:53,810
1 of these functions so can just use the images go back we just use

65
00:06:53,810 --> 00:06:57,700
this inequality we told

66
00:06:57,700 --> 00:07:00,900
and what's interesting

67
00:07:01,090 --> 00:07:06,080
what's interesting to speculate is

68
00:07:06,480 --> 00:07:09,140
if there is more space for this kind of

69
00:07:11,660 --> 00:07:16,120
biological complex structure what's the equivalent idea before

70
00:07:16,160 --> 00:07:19,820
engineered structures for buildings for software

71
00:07:19,860 --> 00:07:24,460
the operating rules of the hospital

72
00:07:24,500 --> 00:07:28,900
know got this notion that we can take inspiration from biology from that we can

73
00:07:28,900 --> 00:07:35,240
apply ideas that we find work there in in the engineering realm so could we

74
00:07:35,490 --> 00:07:39,300
so could we make use of this idea of more could that tell us about

75
00:07:39,300 --> 00:07:42,800
what's possible in architecture for example

76
00:07:42,840 --> 00:07:46,840
OK so taking another step down downs we start off with

77
00:07:46,880 --> 00:07:53,060
colony of organisms then organ inside the organisms the development process that generates the organ

78
00:07:53,060 --> 00:07:58,120
underpinning all of this is the process of evolution that has designed the developmental processes

79
00:07:58,120 --> 00:08:01,240
or steers the developmental processes such that we see the organisms the we

80
00:08:02,320 --> 00:08:05,580
and this is another for me

81
00:08:05,620 --> 00:08:13,060
you know perplexingly wondrous sort of thing that you can say about nature which is

82
00:08:13,060 --> 00:08:18,100
that you can in this single line here you can sort of encapsulate what it

83
00:08:18,100 --> 00:08:22,080
is for system to be evolutionary and yet

84
00:08:22,120 --> 00:08:24,010
the potentiality of

85
00:08:24,240 --> 00:08:25,980
natural selection

86
00:08:26,020 --> 00:08:32,500
the ability of natural selection to generate such a huge variety of well adaptive forms

87
00:08:32,540 --> 00:08:38,300
just does not in order to do justice to was nice

88
00:08:38,340 --> 00:08:40,360
rather politically incorrect

89
00:08:41,660 --> 00:08:45,480
i mean there's hundreds of these cartoons of like evolution

90
00:08:45,520 --> 00:08:48,210
starts here and ends up there

91
00:08:50,780 --> 00:08:54,720
something that i've found interesting

92
00:08:55,270 --> 00:09:00,520
to speculate about in in the book

93
00:09:00,600 --> 00:09:03,860
fifty years ago

94
00:09:03,880 --> 00:09:07,160
if you or a hundred years ago

95
00:09:07,260 --> 00:09:11,080
if you want to talk in psychology it would be full of

96
00:09:11,140 --> 00:09:13,540
words that had been adopted from physics

97
00:09:13,800 --> 00:09:19,820
but the nuclear family fission fusion processes critical mass

98
00:09:20,420 --> 00:09:23,270
chain reaction

99
00:09:23,480 --> 00:09:27,060
and now if you go to the same lecture theatre it will be full of

100
00:09:27,060 --> 00:09:31,480
words that have been borrowed from biology cells

101
00:09:31,960 --> 00:09:34,740
the ecology of this

102
00:09:35,600 --> 00:09:43,320
and i'd really evolutionary biology biology but really evolutionary biology is is now the organising

103
00:09:43,320 --> 00:09:45,760
metaphor war of choice throughout

104
00:09:46,460 --> 00:09:47,480
i would say

105
00:09:47,520 --> 00:09:53,620
it's a bold claim but really we're in a kind of biological era where biological

106
00:09:53,620 --> 00:09:56,940
processes evolution come evolution

107
00:09:57,000 --> 00:09:59,980
are being co-opted taken

108
00:10:00,000 --> 00:10:02,800
and we used it everywhere

109
00:10:03,210 --> 00:10:06,300
they and they bring a certain way of looking at systems

110
00:10:06,360 --> 00:10:08,420
but sometimes that's very useful

111
00:10:09,920 --> 00:10:14,860
but it but with the within within evolution within evolutionary science there are some important

112
00:10:14,860 --> 00:10:17,780
unsolved questions which implicate complexity

113
00:10:19,380 --> 00:10:24,000
evolution is typically characterised as a gradualist process

114
00:10:24,020 --> 00:10:30,780
so mutation in this small changes happen to genomes as people have as offspring are

115
00:10:30,780 --> 00:10:35,840
generated and the offspring which is slightly better than their parents have more offspring and

116
00:10:35,940 --> 00:10:41,540
over time the population changes to reflect the environment the organisms found themselves in but

117
00:10:41,540 --> 00:10:42,920
there's also

118
00:10:42,960 --> 00:10:44,920
within evolution

119
00:10:44,960 --> 00:10:49,360
if you look over evolutionary history major transition so where

120
00:10:49,600 --> 00:10:50,620
we see

121
00:10:50,640 --> 00:10:57,560
major evolutionary innovations so the innovation of sexual reproduction the move from cellular

122
00:10:57,580 --> 00:11:06,620
so multicellular to multicellular creatures you carry carriers language sociology there's lots of them how

123
00:11:06,620 --> 00:11:11,600
did they come about via this gradualist process of tinkering

124
00:11:11,680 --> 00:11:13,300
is the big question

125
00:11:13,400 --> 00:11:16,700
what the genesis is another name for development

126
00:11:16,740 --> 00:11:22,000
we start somewhere people know very little about how development works

127
00:11:22,240 --> 00:11:27,280
and a big the big elephant in the room is the extent to which we

128
00:11:27,280 --> 00:11:32,220
can understand human behaviour via evolutionary thinking

129
00:11:32,240 --> 00:11:35,520
which is politically highly charged

130
00:11:35,700 --> 00:11:39,120
moving outside of evolutionary theory

131
00:11:39,140 --> 00:11:40,140
within AI

132
00:11:40,640 --> 00:11:43,920
with this is strong desire

133
00:11:43,960 --> 00:11:45,560
given how simple

134
00:11:45,600 --> 00:11:50,000
the evolutionary algorithm is to state i grew up i mean i'm part of the

135
00:11:50,000 --> 00:11:54,840
generation that grew up and read the selfish gene and then they read the blind

136
00:11:54,840 --> 00:12:01,460
watchmaker and extended phenotype and they wrote a little program that involved a sentence and

137
00:12:01,490 --> 00:12:04,600
just was seductively easy

138
00:12:04,620 --> 00:12:07,840
you know if richard dawkins says that's what evolution is and i just coded it

139
00:12:07,840 --> 00:12:11,700
in basic on a ZX eighty the world is my oyster

140
00:12:11,840 --> 00:12:16,540
so there's been this idea that you could harness the power of the creative power

141
00:12:16,540 --> 00:12:21,940
of evolutionary design ever since then i think that it's proved amazingly frustrating

142
00:12:21,980 --> 00:12:25,980
this proved far more difficult than dawkins led us to believe he really led us

143
00:12:25,980 --> 00:12:30,440
up a blind alley well it's not a blind alley but i think it's proven

144
00:12:30,440 --> 00:12:31,400
to be

145
00:12:31,490 --> 00:12:35,100
much more challenging and i think it kind of reveals slide wrong headedness about the

146
00:12:35,100 --> 00:12:37,840
way that we think about evolution if we really

147
00:12:38,040 --> 00:12:41,860
we really understood evolution then when we wrote a computer program

148
00:12:41,880 --> 00:12:45,680
surely we would get better results than what we do

149
00:12:45,720 --> 00:12:48,590
what's last on the list

150
00:12:49,220 --> 00:12:53,010
and we can move even further away from biological evolution

151
00:12:53,160 --> 00:12:59,090
and and talk to people across a ridiculously large number of disciplines now who are

152
00:12:59,440 --> 00:13:08,760
applying evolutionary ideas models borrowed from evolution to other kinds of system so when i

153
00:13:08,760 --> 00:13:11,300
was finishing off my phd

154
00:13:11,340 --> 00:13:15,140
you know i would do is i got to the point where i would do

155
00:13:15,140 --> 00:13:20,360
anything other than write my thesis so i went to the library and i counted

156
00:13:20,400 --> 00:13:23,900
how many journals had evolutionary in their title

157
00:13:24,520 --> 00:13:32,880
and that really was this profusion of new journals called evolutionary linguistics evolutionary psychology evolutionary

158
00:13:33,000 --> 00:13:37,480
cosmology people i don't think they had a journal

159
00:13:37,480 --> 00:13:42,060
the same sub sequences appear in both sequence in the bow strings right and then

160
00:13:42,060 --> 00:13:45,110
we just do the same thing sort of that's what the feature is going to

161
00:13:45,110 --> 00:13:50,250
appear for one string for the other string so that enumeration of the little dots

162
00:13:50,270 --> 00:13:53,570
do you mean that there is an infinity of them there is a need for

163
00:13:54,020 --> 00:13:57,210
an enumeration of all things possible

164
00:13:57,220 --> 00:14:01,060
over this alphabet sigma

165
00:14:01,180 --> 00:14:06,060
and then we're going to count how many times does each sub sequences appearing here

166
00:14:06,060 --> 00:14:10,630
remember we were talking about subsequences yesterday and i was put into two their whatever

167
00:14:10,650 --> 00:14:15,220
all over the place saying the things we exponential so we're walking in murky waters

168
00:14:15,220 --> 00:14:16,600
here so

169
00:14:16,650 --> 00:14:21,430
this is my future lectures which i'm showing the factor saying hey it does exist

170
00:14:21,470 --> 00:14:25,730
now remember its implicit we don't want to play with it and the result is

171
00:14:25,730 --> 00:14:29,640
that when we want to count it we look at it here we're going to

172
00:14:29,640 --> 00:14:35,400
obtain a six in this case so how do compute six well perhaps i look

173
00:14:35,400 --> 00:14:39,580
at this from the the decomposition so we have got several

174
00:14:39,600 --> 00:14:45,100
because i got the one letter words is a which are three of here two

175
00:14:45,100 --> 00:14:46,330
of here

176
00:14:46,340 --> 00:14:48,940
so that's three times six

177
00:14:48,960 --> 00:14:53,630
and we have but one and one seven that's for the one letter words now

178
00:14:53,630 --> 00:14:58,970
if i look at a large one a another a another a three on this

179
00:14:59,760 --> 00:15:02,580
one of the sites of three times one

180
00:15:02,600 --> 00:15:09,140
and i could be a also and i got structure with a b

181
00:15:09,150 --> 00:15:10,100
and a b

182
00:15:10,100 --> 00:15:11,840
the last two

183
00:15:11,850 --> 00:15:15,050
because are going a be here and i will be a and b a so

184
00:15:15,050 --> 00:15:16,370
that's one here

185
00:15:16,390 --> 00:15:19,230
and i don't have any other sub sequence

186
00:15:19,230 --> 00:15:20,780
if i counted right

187
00:15:20,810 --> 00:15:25,020
OK so seven plus three plus two plus one equals thirteen

188
00:15:25,070 --> 00:15:26,310
there is locally

189
00:15:26,340 --> 00:15:32,130
tricky bit about them string land sometimes we're interested in seeing counted once in each

190
00:15:32,130 --> 00:15:35,210
case and just whatever sometimes counted zero

191
00:15:35,250 --> 00:15:38,210
so it's not going to be more complicated than that

192
00:15:38,230 --> 00:15:43,680
obviously noticed that my way of counting here has been exactly the wrong way of

193
00:15:43,680 --> 00:15:48,220
actually counted the features themselves and if i did this on really long strings and

194
00:15:48,220 --> 00:15:51,500
you get very bored but i mean i get more time before that

195
00:15:51,560 --> 00:15:55,110
so you have find a way of avoiding that we show that in a second

196
00:15:55,130 --> 00:15:58,960
but before that i'm just going to turn into just another one of these strings

197
00:15:58,960 --> 00:16:01,650
just to give the idea of the flavour of it

198
00:16:01,810 --> 00:16:07,710
this all subsequence kernels so that idea remember the kansas trying to measure somehow how

199
00:16:07,710 --> 00:16:10,760
many things in common do these two strings has

200
00:16:10,890 --> 00:16:17,260
OK the the value should be as big as things are similar

201
00:16:17,270 --> 00:16:22,120
the problem here is the way saying that as a sequence

202
00:16:22,620 --> 00:16:27,390
that is very spread out over the strings is going to cost to say so

203
00:16:27,400 --> 00:16:31,690
that way the same as the strings that is actually a substring right that very

204
00:16:31,690 --> 00:16:35,630
close whereas experience might tell us that you know is the perhaps something where the

205
00:16:35,670 --> 00:16:39,420
whether with the different elements of the stream they which checking out already close one

206
00:16:39,430 --> 00:16:43,470
to each other should count more than something they spread out so people have invented

207
00:16:43,470 --> 00:16:49,980
a really nice kernel nice meeting it's mathematically elegant i'm not sure it works better

208
00:16:50,100 --> 00:16:54,350
but a nice good which i'm just showing here which is called the gap weighted

209
00:16:54,350 --> 00:16:58,500
subsequences ideas just say we're going to do the same competitions before

210
00:16:58,510 --> 00:17:03,860
but instead of in the feature vector that we saw before just giving a value

211
00:17:03,860 --> 00:17:07,010
and number we're actually going to give a polynomial

212
00:17:07,020 --> 00:17:13,300
i polynomial where each monomial of the polynomial correspond to sort of the length of

213
00:17:13,300 --> 00:17:19,970
that that subsequent is occupying in the initial sequence so since it may occupy five

214
00:17:19,970 --> 00:17:25,590
positions and three positions well there will be one lambda to the five and one

215
00:17:25,590 --> 00:17:31,060
lambda two three actually not noticed five five three so be multiplied by lambda factor

216
00:17:31,850 --> 00:17:35,550
so for example here is that if we look at c a t we can

217
00:17:35,550 --> 00:17:37,940
see that eighty appears twice

218
00:17:37,980 --> 00:17:43,140
once it occupies two positions and the time it occupies three positions in the initial

219
00:17:43,140 --> 00:17:44,930
string so here

220
00:17:45,050 --> 00:17:47,550
the five the

221
00:17:47,560 --> 00:17:52,490
but the corresponding feature well will be aligned at the squared plus lambda to q

222
00:17:52,490 --> 00:17:57,230
this is just to show some elegant things with here the bizarrely enough not learn

223
00:17:57,230 --> 00:17:59,780
we take it out and give it is zero

224
00:17:59,790 --> 00:18:02,720
in case the inputs are were

225
00:18:06,780 --> 00:18:09,710
we have to do some work in the machine to check with the inputs for

226
00:18:09,710 --> 00:18:12,310
this machine is had the right values

227
00:18:12,330 --> 00:18:13,570
and if they do

228
00:18:13,580 --> 00:18:16,060
we can take put is zero in

229
00:18:16,060 --> 00:18:19,290
and generally follow the recipe here

230
00:18:19,340 --> 00:18:21,000
for every

231
00:18:21,030 --> 00:18:23,820
supposed hold checker s

232
00:18:23,840 --> 00:18:28,330
calculate the new machine or find new using

233
00:18:28,380 --> 00:18:29,560
now we can apply

234
00:18:29,580 --> 00:18:31,600
g itself is true

235
00:18:31,750 --> 00:18:33,350
perfectly good turing machine

236
00:18:33,400 --> 00:18:37,610
it's possible to give the definition of i've given the definition of just indicated i

237
00:18:38,670 --> 00:18:42,420
now in inside silent machine itself has the machine description

238
00:18:42,470 --> 00:18:46,220
so it's part of the sort so it's one of the kinds of machines that

239
00:18:46,230 --> 00:18:47,300
might have

240
00:18:47,350 --> 00:18:51,820
holt at all might not have halted when given the sun machine description is an

241
00:18:53,540 --> 00:18:58,560
so we're going to plug that straight back into some of these definitions

242
00:18:58,580 --> 00:19:02,530
and we'll see what happens

243
00:19:02,540 --> 00:19:03,460
if there was

244
00:19:03,510 --> 00:19:08,140
we ask the question does machine g stop when given its own

245
00:19:08,210 --> 00:19:10,800
machine description as input

246
00:19:12,490 --> 00:19:13,800
if it did

247
00:19:14,620 --> 00:19:16,800
this would be up to one

248
00:19:18,310 --> 00:19:22,330
if this can tell the g when given its own description

249
00:19:23,530 --> 00:19:24,770
went to one

250
00:19:27,690 --> 00:19:28,980
with me

251
00:19:29,060 --> 00:19:32,660
he started the reductio one minus the other way

252
00:19:32,720 --> 00:19:34,960
if s to do we do that

253
00:19:36,110 --> 00:19:39,000
by golly looking up the definition of g

254
00:19:39,030 --> 00:19:41,960
since it's got a value one

255
00:19:41,970 --> 00:19:44,590
then g when given time machine description

256
00:19:44,650 --> 00:19:46,000
must be under four

257
00:19:49,630 --> 00:19:53,900
g when given the time machine description does not hold

258
00:19:53,940 --> 00:19:56,550
but now we go back to the definition of s

259
00:19:56,590 --> 00:19:59,480
we say g by definition of s then

260
00:19:59,520 --> 00:20:02,060
if it does not hold

261
00:20:02,100 --> 00:20:03,560
it's gg

262
00:20:03,560 --> 00:20:07,140
i can write that should get the value zero

263
00:20:07,190 --> 00:20:10,300
so starting assumption that s

264
00:20:11,690 --> 00:20:14,850
equals one

265
00:20:14,870 --> 00:20:16,850
we get down to this

266
00:20:18,730 --> 00:20:20,800
r equals zero

267
00:20:20,950 --> 00:20:25,490
that's right that's just reductio was just logic used to doing that

268
00:20:25,540 --> 00:20:27,560
so we see from this assumption

269
00:20:28,550 --> 00:20:31,800
produce the opposite side the opposite is true

270
00:20:32,560 --> 00:20:35,140
ggms be easier after all

271
00:20:35,200 --> 00:20:36,370
heart not

272
00:20:36,440 --> 00:20:37,720
now we start

273
00:20:37,800 --> 00:20:41,880
conversely from the possibility that STG is zero

274
00:20:41,890 --> 00:20:44,350
go through a similar argument

275
00:20:44,370 --> 00:20:46,080
if STG zero

276
00:20:46,090 --> 00:20:48,500
in g given its own input

277
00:20:48,560 --> 00:20:50,490
by its definition

278
00:20:52,300 --> 00:20:54,960
see definition top line the definition

279
00:20:55,210 --> 00:20:58,150
gg equals zero

280
00:20:58,950 --> 00:21:00,860
g of g must be easier

281
00:21:03,440 --> 00:21:05,650
in particular it stops

282
00:21:05,710 --> 00:21:07,220
but if it stops

283
00:21:07,230 --> 00:21:09,610
that's the definition of s

284
00:21:09,630 --> 00:21:15,540
it passed it's GA gene asking that you want

285
00:21:15,590 --> 00:21:17,060
so from here

286
00:21:17,160 --> 00:21:20,050
we can argue refers to like that

287
00:21:20,050 --> 00:21:23,730
and now we argue that

288
00:21:23,740 --> 00:21:26,040
list counted things

289
00:21:26,090 --> 00:21:28,700
so the conclusion we reach is that there isn't

290
00:21:28,740 --> 00:21:31,500
turing machine

291
00:21:31,740 --> 00:21:34,510
so the theorem says

292
00:21:34,600 --> 00:21:39,060
suppose we have a picture machine is a particular problem that solving

293
00:21:39,170 --> 00:21:43,440
was it's able to observe with another machine

294
00:21:43,480 --> 00:21:47,190
we can do something like diagonalisation

295
00:21:48,240 --> 00:21:51,400
could produce new turing machine

296
00:21:51,420 --> 00:21:53,780
with the with properties

297
00:21:53,820 --> 00:21:57,010
that conflict that

298
00:21:59,520 --> 00:22:04,370
it is

299
00:22:15,730 --> 00:22:17,070
i think that would be fine

300
00:22:17,540 --> 00:22:21,230
i was worried about being known for about being

301
00:22:27,520 --> 00:22:31,190
the two possibilities one is that this doesn't stop OK

302
00:22:31,240 --> 00:22:35,350
and the other is that produces the value one

303
00:22:38,560 --> 00:22:41,620
i doubt remains as you

304
00:22:41,680 --> 00:22:48,470
that can be you he

305
00:22:48,520 --> 00:22:52,230
the whole thing is that is an argument around in

306
00:22:52,310 --> 00:22:57,040
i think you're right that i could leave after preprocessing i i actually can remember

307
00:22:57,040 --> 00:22:59,350
putting it thinking why my doing this

308
00:22:59,350 --> 00:23:04,950
the suboptimal sixteen some hope then you might take those but we would like to

309
00:23:04,950 --> 00:23:11,890
to show convergence for some theoretical properties you should assume the biggest error is minimized

310
00:23:12,820 --> 00:23:16,130
what this is

311
00:23:17,590 --> 00:23:27,820
yes it is indeed so this is part of the collection going to talk about

312
00:23:27,820 --> 00:23:33,430
how one can change other posts such that it was less receptive one less

313
00:23:33,430 --> 00:23:41,090
sensitive to two of outliers so essentially what what i'm going to show you is

314
00:23:41,300 --> 00:23:45,990
all the underlying what i mean my understanding of what it was how however the

315
00:23:46,010 --> 00:23:50,660
optimisation works so trying to understand the underlying optimisation problems

316
00:23:50,740 --> 00:23:55,910
then we can change the underlying optimisation problems and such that the solution is more

317
00:23:55,910 --> 00:24:00,280
robust and then it can generate a new posting

318
00:24:00,280 --> 00:24:04,510
OK so but we need someone to take a detour to the theory to understand

319
00:24:04,510 --> 00:24:06,260
what's actually going on so

320
00:24:06,260 --> 00:24:10,260
of course you could directly go to the algorithm change something i'm also showing you

321
00:24:10,260 --> 00:24:15,890
want to change something that works then it's a bit more robust but i think

322
00:24:15,890 --> 00:24:17,010
it's better to

323
00:24:17,030 --> 00:24:20,740
two something i mean which is based on the theory

324
00:24:24,360 --> 00:24:30,860
overfitting problem but

325
00:24:30,860 --> 00:24:35,930
by increasing capital OK

326
00:24:37,820 --> 00:24:44,590
yes and no of course when you have only one iteration then you have a

327
00:24:44,590 --> 00:24:49,840
very simple baseline this is unlikely to have overfitting so but i'm going to talk

328
00:24:49,840 --> 00:24:55,990
about it tomorrow morning first first lecture that this overfitting was not as strong as

329
00:24:55,990 --> 00:24:59,840
one would expect to have it i mean there which he says is when you

330
00:24:59,840 --> 00:25:00,590
have more

331
00:25:00,880 --> 00:25:05,380
more hypothesis when you combine more more policies so the combined hypothesis is getting more

332
00:25:05,380 --> 00:25:10,860
and more complex and therefore we should see operating as we will see something which

333
00:25:10,860 --> 00:25:16,860
is related to margins that place somehow against it so the complexity

334
00:25:16,890 --> 00:25:23,180
somehow the margin increased therefore the complexities lower and actually this avoids overfitting it OK

335
00:25:23,180 --> 00:25:25,010
so it's not complete

336
00:25:25,030 --> 00:25:26,840
so in a sense

337
00:25:26,840 --> 00:25:31,910
it depends on the number of iterations but not as much as one the me

338
00:25:31,930 --> 00:25:36,510
more cases

339
00:25:36,530 --> 00:25:37,490
OK very good

340
00:25:45,050 --> 00:25:48,430
four fifteen k

341
00:25:51,280 --> 00:25:59,530
OK so the next part that is abusive behavior on the

342
00:25:59,590 --> 00:26:04,880
training sample so that is not talking about generalisation is just talking about the training

343
00:26:06,380 --> 00:26:11,530
so the main sources you are going to appear in ninety six and also

344
00:26:11,550 --> 00:26:16,570
for ship you find part of the ninety eight

345
00:26:17,200 --> 00:26:22,700
OK so far we only looked at the function which is either i mean the

346
00:26:22,700 --> 00:26:26,910
classification function which is either correct or incorrect so somehow we

347
00:26:26,930 --> 00:26:33,860
look at what the actual combined output was so and somehow we lose information by

348
00:26:33,860 --> 00:26:36,860
by not looking at the actual value

349
00:26:36,880 --> 00:26:42,590
so we have these combined the functions of this force this one here

350
00:26:42,610 --> 00:26:44,260
OK on the bottom here

351
00:26:44,280 --> 00:26:47,660
and these htx those plus minus one

352
00:26:47,680 --> 00:26:49,340
these are alpha

353
00:26:49,490 --> 00:26:54,610
the first actually some some to one so normalised so therefore this is one here

354
00:26:54,610 --> 00:26:57,650
is actually between minus one one

355
00:26:57,660 --> 00:27:02,030
OK all alright

356
00:27:02,130 --> 00:27:08,150
and usually just look at the threshold value so serving above zero then plus one

357
00:27:08,740 --> 00:27:13,220
then classifier plus one and it's below zero then it's minus one OK but we

358
00:27:13,220 --> 00:27:17,410
lose information how sure example is classified

359
00:27:17,430 --> 00:27:23,720
OK so we have OK the following observations so we have a higher confidence in

360
00:27:23,740 --> 00:27:29,070
the graph in the example when the output is somehow larger closer to one than

361
00:27:29,070 --> 00:27:31,150
the more sure about the classification

362
00:27:31,860 --> 00:27:34,840
and this what would like to

363
00:27:34,860 --> 00:27:37,570
use this output somehow

364
00:27:37,660 --> 00:27:41,070
so let's say we have a linear classification so we have a point on the

365
00:27:41,070 --> 00:27:44,880
side of the positive points and point of this site the negative side

366
00:27:44,990 --> 00:27:48,840
and then we have this point here which is very far away from the hyperplane

367
00:27:48,840 --> 00:27:50,410
so it's very positive

368
00:27:50,410 --> 00:27:56,280
very confident here and here which is close to the hyperplane me unsure about the

369
00:27:57,450 --> 00:28:02,430
OK so and the idea is that we define something which is called the marginal

370
00:28:02,590 --> 00:28:03,630
that is simply

371
00:28:03,650 --> 00:28:07,570
the product of y times f of x

372
00:28:07,590 --> 00:28:12,010
OK so this is the but this is a zero so

373
00:28:12,010 --> 00:28:14,950
he was threshold

374
00:28:14,970 --> 00:28:19,340
and it's very very much positive and on the right side then the study should

375
00:28:19,340 --> 00:28:20,320
be large

376
00:28:21,880 --> 00:28:29,410
and if it's negative but it should be positive then this this is martin stated

377
00:28:29,430 --> 00:28:34,320
OK and drawing like hyperplane here and this hyperplane can be just

378
00:28:34,590 --> 00:28:39,050
this is the normal person on the hyperplane so the question is why why using

379
00:28:39,380 --> 00:28:40,610
the explain that

380
00:28:40,740 --> 00:28:45,990
OK so essentially this function set eight

381
00:28:46,010 --> 00:28:48,990
this one here is so this

382
00:28:49,950 --> 00:28:53,760
and then we think hypotheses out of this function set

383
00:28:53,780 --> 00:28:59,470
and what we are generating another poster featuring really a linear combination of functions in

384
00:28:59,470 --> 00:29:04,760
this have this this hypothesis OK such a linear combination here

385
00:29:04,780 --> 00:29:06,530
so in these

386
00:29:06,550 --> 00:29:11,880
rates of h they are greater than zero and sum to one so the convex

387
00:29:12,320 --> 00:29:14,910
how of this function class

388
00:29:14,930 --> 00:29:17,610
so the first are essentially the parameters

389
00:29:19,360 --> 00:29:24,610
essentially you can see this as a hyperplane where the others are

390
00:29:24,660 --> 00:29:29,910
somehow determining the rate the direction of the hyperplane so this is a

391
00:29:30,030 --> 00:29:34,450
this is equivalent to finding a hyperplane in the feature space which is spanned by

392
00:29:34,450 --> 00:29:40,390
the hypothesis set so every direction every hypothesis is essentially one direction we get a

393
00:29:40,390 --> 00:29:41,990
picture for that

394
00:29:42,050 --> 00:29:46,820
OK so we have the input space x here in the positive points negative points

395
00:29:46,820 --> 00:29:48,530
and then let's say we have two

396
00:29:48,550 --> 00:29:54,970
functions here so this is let's say the first coordinate x x is cut and

397
00:29:55,150 --> 00:30:01,720
second one OK and what we do is we met these this input space into

398
00:30:01,720 --> 00:30:07,070
a so called feature space we take all the hypothesis which and in the hypothesis

399
00:30:07,070 --> 00:30:11,410
class so here's the positive class h one h two and so on and we

400
00:30:11,410 --> 00:30:14,820
take those just make it into the feature space here

401
00:30:14,860 --> 00:30:19,950
OK so in in this feature space it's just a linear classification so and essentially

402
00:30:19,950 --> 00:30:24,340
the distance which we have here somehow transfers into a distance which is here i

403
00:30:24,340 --> 00:30:28,240
mean here we come we measure the distance we measure the distance you OK because

404
00:30:28,240 --> 00:30:29,630
you well

405
00:30:29,630 --> 00:30:31,390
at the time

406
00:30:31,470 --> 00:30:35,410
so essentially you have a linear separation the feature space which is stands for all

407
00:30:35,410 --> 00:30:41,130
the forces that corresponds to a non linear separation in the input space OK

408
00:30:41,150 --> 00:30:44,970
so i mean that's shown you like an example for example that the linear combination

409
00:30:44,970 --> 00:30:46,470
is something on

410
00:30:46,490 --> 00:30:50,840
and also he is a non linear separation what we do this we achieve this

411
00:30:50,840 --> 00:30:55,530
by actually having linear separation in the feature space

412
00:30:59,550 --> 00:31:00,720
let me go back

413
00:31:00,740 --> 00:31:05,260
so that is the

414
00:31:05,280 --> 00:31:10,260
OK in boosting generate t hypotheses and we have the corresponding alpha TV's so this

415
00:31:10,260 --> 00:31:14,840
is essentially the the distance of the point to the hyperplane

416
00:31:16,780 --> 00:31:19,990
this is this distance here

417
00:31:20,010 --> 00:31:21,860
the case is clear

418
00:31:27,700 --> 00:31:30,490
OK so then let let me to find the

419
00:31:30,490 --> 00:31:32,780
into schools this 1

420
00:31:32,800 --> 00:31:39,920
on the which are populated by phosphorylation has presented by Mr. on all defined to

421
00:31:39,920 --> 00:31:50,010
potential function which are divided on rated by the registered relations on Monday aviation and

422
00:31:50,110 --> 00:31:56,010
also potential function is undefined her Romanian

423
00:31:56,130 --> 00:32:03,630
what and everything on the was after in another a hideout persisting is that 1

424
00:32:03,650 --> 00:32:08,920
of the potential is there really cheap to the accounts of others and persisted

425
00:32:15,720 --> 00:32:24,840
so what is so we have explored the men of information he retrieved led calendar

426
00:32:24,880 --> 00:32:32,740
is given to enact such a strike Germany yesterday's affects the extension of the classic

427
00:32:32,980 --> 00:32:42,040
part demonstrates recovery duty-free into space of mattresses measles is kind of a complex Matthew

428
00:32:42,620 --> 00:32:51,700
Wells the imaginary part splitting editions the book is so a salute to the recall

429
00:32:51,700 --> 00:32:56,800
of what about like at base so in fact

430
00:32:57,070 --> 00:33:02,690
a more modest funds from its positive actions that comes from the God things of

431
00:33:02,920 --> 00:33:10,790
what that they want to eat said on your boss also collect calls for all

432
00:33:10,810 --> 00:33:18,510
which we also schools space assisted by US-based was a unique disk of like I

433
00:33:18,570 --> 00:33:25,050
was makes followed about space is given by well-known domestic foes the history the well-known

434
00:33:25,270 --> 00:33:31,730
point delegates taking you need the 1 with that of WGBH so well that way

435
00:33:31,730 --> 00:33:37,770
we have a that information Jimmy treating these vehicles when you have a picture for

436
00:33:37,770 --> 00:33:45,600
instance you have been using it on to evaluations on the Interfax exceeding might use

437
00:33:45,600 --> 00:33:55,210
a bomb and his career in fact are in the space of about Middle so

438
00:33:55,230 --> 00:34:02,970
you might see on on a L For you are going to gods based on

439
00:34:02,970 --> 00:34:13,160
its official between each for a good is given by excretion and that was that

440
00:34:13,160 --> 00:34:17,810
they must not optical and all the value of anything that 2 of the values

441
00:34:17,820 --> 00:34:25,620
of sea lions in the square of trade between the 2 of you I tried

442
00:34:25,620 --> 00:34:37,210
to a hollow information each which is due Monday to see situation where are the

443
00:34:37,240 --> 00:34:53,420
victory given by immigrants and not are which is due by secretion which is exactly

444
00:34:53,420 --> 00:35:02,510
what the richly goes to work as well as space on Societe GOZ ideas we

445
00:35:02,550 --> 00:35:09,900
wasn't saying that point at can put the company's valuable and she began and lose

446
00:35:09,900 --> 00:35:17,340
no consideration so you you do too complex a valuable and on you couldn't consul

447
00:35:17,340 --> 00:35:20,630
it does some kind of exponential stability

448
00:35:20,650 --> 00:35:24,650
satisfies some kind of exponential stability condition

449
00:35:25,720 --> 00:35:27,630
what does it mean it means that

450
00:35:27,700 --> 00:35:31,310
assume you that if you were initializing

451
00:35:31,320 --> 00:35:32,520
your system

452
00:35:32,530 --> 00:35:35,400
basically at the location x one

453
00:35:35,430 --> 00:35:40,070
if you initializing at different initial conditions x one y

454
00:35:40,070 --> 00:35:42,390
and if you were running the two

455
00:35:42,420 --> 00:35:46,320
basically when computing the posterior distribution

456
00:35:46,330 --> 00:35:51,610
which are subject to different initial conditions if this this and this and one

457
00:35:51,630 --> 00:35:54,340
west converging to zero exponentially

458
00:35:54,350 --> 00:35:57,390
we quickly as n increases

459
00:35:57,400 --> 00:35:59,150
then it means that

460
00:35:59,170 --> 00:36:04,000
you're trying to approximate dynamic model which can forget

461
00:36:04,050 --> 00:36:06,370
basically past OK

462
00:36:06,390 --> 00:36:09,300
if you have this kind of conditions

463
00:36:09,300 --> 00:36:11,290
well you can show

464
00:36:11,310 --> 00:36:14,160
results which are much stronger ridiculous

465
00:36:14,880 --> 00:36:16,490
consider for example

466
00:36:16,580 --> 00:36:18,720
that you're interested in

467
00:36:18,750 --> 00:36:21,680
just computing the expectation

468
00:36:21,690 --> 00:36:26,790
the function which depends on the last component x

469
00:36:26,790 --> 00:36:32,690
in this case if you have exponentially many ascension then you get bones which are

470
00:36:32,690 --> 00:36:37,080
essentially time independent so it means in this case

471
00:36:37,090 --> 00:36:38,730
that essentially

472
00:36:38,750 --> 00:36:46,110
you to provide an approximation numerical approximation of the marginal qnx and even the fields

473
00:36:46,110 --> 00:36:47,130
of salvation

474
00:36:47,150 --> 00:36:50,480
which basically doesn't degenerate over time

475
00:36:50,650 --> 00:36:56,540
so the approximation error remains bounded on this is this result is why people use

476
00:36:56,540 --> 00:37:02,210
particle filters is that in most scenarios essentially when you only look at the marginal

477
00:37:02,210 --> 00:37:04,940
distribution there's no accumulation of

478
00:37:04,960 --> 00:37:07,630
that's what makes the technique attracted

479
00:37:07,650 --> 00:37:13,890
so what you can show this was under the same condition is that basically

480
00:37:13,920 --> 00:37:18,520
if you look at your estimate of the marginal likelihood where

481
00:37:18,560 --> 00:37:24,020
the violence obviously going to increase over time because it only increases increases linearly over

482
00:37:24,040 --> 00:37:30,560
time OK on that night because if you use stone basically in pollen sampling technique

483
00:37:30,810 --> 00:37:32,960
to approximate the

484
00:37:32,980 --> 00:37:35,210
marginal likelihood then typically

485
00:37:35,230 --> 00:37:41,620
this violence would increase exponentially with time indexed so far to resampling you've moved to

486
00:37:41,750 --> 00:37:47,190
buy a from of violence because it increases exponentially with time to violence increasing only

487
00:37:47,190 --> 00:37:51,630
linearly with the time and that which is one of the many basically benefit of

488
00:37:51,630 --> 00:37:54,690
using this kind of resampling

489
00:37:55,100 --> 00:37:59,670
so these are the kind of the kind of basic categories over the past few

490
00:37:59,670 --> 00:38:03,520
years there has been an actually a lot of techniques which have been proposed to

491
00:38:03,520 --> 00:38:06,770
import basically the uris

492
00:38:06,790 --> 00:38:10,060
i'm just going to detail so very briefly

493
00:38:10,060 --> 00:38:11,250
OK so

494
00:38:11,270 --> 00:38:18,440
in the naive algorithm what you're doing is that you're propagating the uris according to

495
00:38:18,440 --> 00:38:19,790
the dynamic of the

496
00:38:19,810 --> 00:38:25,390
on your reweighting them according to the likelihood of this acceleration of this is almost

497
00:38:25,390 --> 00:38:30,040
something which is very naive because it's like shooting in the dark because imagine you

498
00:38:30,040 --> 00:38:32,460
have the likelihood which is very picky

499
00:38:32,480 --> 00:38:36,290
that is the salvation which sheltered assumes you like

500
00:38:36,350 --> 00:38:38,390
would like she's quite picky

501
00:38:38,400 --> 00:38:43,650
OK on you instead of using the salvation essentially to guide the particles in the

502
00:38:43,650 --> 00:38:49,050
region of my life you you're doing just sampling naively according to the point that

503
00:38:49,050 --> 00:38:53,460
sort of this is something that you know you can really important those techniques easily

504
00:38:53,460 --> 00:39:00,670
by simply building a mechanism where the particles are propagated in full while using the

505
00:39:01,650 --> 00:39:06,400
conveyed by deed television one so they have their values were to do that essentially

506
00:39:06,400 --> 00:39:12,460
the best way to do it is simply to sample particle propagator particle fall on

507
00:39:12,460 --> 00:39:18,170
the according to the players back on to this conditional distribution which is simply proportional

508
00:39:18,170 --> 00:39:22,440
to the likely time points is a lot of techniques which have been discussed in

509
00:39:22,440 --> 00:39:28,390
the literature like this on when you cannot so use basically this distribution to propagate

510
00:39:28,390 --> 00:39:33,650
the particle fallout where you can use approximation of it so you can come up

511
00:39:33,650 --> 00:39:35,750
with approximation of this

512
00:39:35,770 --> 00:39:41,040
distribution using set ensemble kalman filter or the extended kalman filter in which case you

513
00:39:41,040 --> 00:39:45,710
need to reweight apart according to different ways which correct for the bias introduced by

514
00:39:45,710 --> 00:39:50,210
this this proposal distribution so the techniques which have been done here i'm not going

515
00:39:50,210 --> 00:39:54,120
to discuss it very very in depth

516
00:39:54,190 --> 00:39:58,290
one thing i like to mention because it be user we use later on

517
00:39:58,330 --> 00:40:03,420
is that there is one stop away which have been proposed to increase diversity in

518
00:40:03,420 --> 00:40:04,670
the number of particles

519
00:40:04,690 --> 00:40:05,890
so we're not

520
00:40:05,900 --> 00:40:11,870
you're doing this kind of sampling was something step after resampling step called capital in

521
00:40:11,870 --> 00:40:17,020
time from the weighted empirical measures to obtain capital in sampled on because you sample

522
00:40:17,020 --> 00:40:21,410
some of the data fitting term and idealisation

523
00:40:21,430 --> 00:40:27,150
then if you consider linear functions of fuel excise OK and you only use on

524
00:40:27,270 --> 00:40:33,760
only only do that for predicting then whatever also cost because the convex not convex

525
00:40:33,850 --> 00:40:39,640
whatever you want with the the dimension of feature so she may be one-dimensional but

526
00:40:39,640 --> 00:40:44,370
if can be infinite dimensional figure no full input space then in the end you

527
00:40:45,040 --> 00:40:47,500
that the solution of the problem

528
00:40:47,520 --> 00:40:51,660
will be always a linear combination of your point

529
00:40:51,680 --> 00:40:53,660
OK so i know answer

530
00:40:54,540 --> 00:40:59,990
which should be your vector your from going into hyperspace to be in combination

531
00:41:00,020 --> 00:41:02,620
of alpha phi j

532
00:41:02,640 --> 00:41:07,690
of of the human genome project the features the jokes of the new data so

533
00:41:07,760 --> 00:41:09,370
the end back from one

534
00:41:09,380 --> 00:41:14,890
former public from the optimisation problem in hilbert space in an optimisation problem in in

535
00:41:14,890 --> 00:41:17,440
rn where n is the number of points

536
00:41:17,450 --> 00:41:22,770
of course if the dimension of f is small is useless but in many cases

537
00:41:22,810 --> 00:41:23,720
this is

538
00:41:23,770 --> 00:41:28,400
this allows you to this allows you to

539
00:41:28,410 --> 00:41:32,510
to two big space at the price of a small space

540
00:41:32,520 --> 00:41:37,410
so want to know that f is of the form you can parameterise using that

541
00:41:37,440 --> 00:41:42,500
so i've often called and what by OK and then if you plot back into

542
00:41:42,510 --> 00:41:46,990
the objective function you get exactly is equivalent problem OK

543
00:41:47,000 --> 00:41:49,020
which we only depend on alpha

544
00:41:49,030 --> 00:41:53,900
as the data x we only into the problem with the so-called kernel matrix

545
00:41:53,900 --> 00:41:55,540
then you just need

546
00:41:55,560 --> 00:41:58,640
to know the dot product between

547
00:41:58,640 --> 00:42:00,600
between x x i and j

548
00:42:00,610 --> 00:42:02,840
so then what this means is that even

549
00:42:02,890 --> 00:42:08,030
if e is very low dimensional OK you can't even write he on your computer

550
00:42:08,160 --> 00:42:12,070
but if you can compute that if you can compute the dot product efficiently then

551
00:42:12,070 --> 00:42:13,310
you can

552
00:42:13,320 --> 00:42:18,820
on functions can be found in the function of the so this is a list

553
00:42:18,820 --> 00:42:21,560
of the awkwardness about gonna methods

554
00:42:21,600 --> 00:42:23,270
this is the so-called kernel trick

555
00:42:23,280 --> 00:42:26,230
so you want to be able to rely on the feet which is very very

556
00:42:27,030 --> 00:42:29,040
but you can't even expressing

557
00:42:29,050 --> 00:42:32,920
but if you can express the product then you take it you know that by

558
00:42:32,920 --> 00:42:37,660
the way you are and what the only thing that you are the new are

559
00:42:37,680 --> 00:42:42,350
needs is set of conditions and with this

560
00:42:42,360 --> 00:42:43,490
and as you to do

561
00:42:43,620 --> 00:42:44,810
is to do

562
00:42:44,840 --> 00:42:49,190
to do an implicit use of very large feature space and of course is allowed

563
00:42:49,190 --> 00:42:53,240
to go from in in the nineteen thirties OK so initially

564
00:42:53,280 --> 00:42:57,070
we always in know the parameters be doesn't mean that you know in the input

565
00:42:57,910 --> 00:43:02,380
so this has led to a very nice but the beauty of methods so the

566
00:43:02,380 --> 00:43:04,490
community is divided into it to to

567
00:43:04,520 --> 00:43:10,070
that people people that worked on algorithms that only depend on the metrics

568
00:43:10,090 --> 00:43:15,010
given the metrics that we try to do classification clustering regression

569
00:43:15,020 --> 00:43:19,050
independently of the other whether kinematics comes from

570
00:43:19,110 --> 00:43:22,240
the people that works on specific sites

571
00:43:22,280 --> 00:43:26,190
people from vision that were designed kernels especially for

572
00:43:26,290 --> 00:43:33,390
why images of people from bioinformatics that designed kernels especially for four molecules or proteins

573
00:43:33,390 --> 00:43:39,890
and in this separation between hope with national issues and algorithms is a forceful because

574
00:43:39,890 --> 00:43:42,470
what everything that people do

575
00:43:42,520 --> 00:43:46,530
to apply with the kind of people designed so that they are very nice but

576
00:43:46,550 --> 00:43:47,420
i t

577
00:43:47,430 --> 00:43:51,040
we should be very important for the last thing years

578
00:43:51,070 --> 00:43:56,400
so it's one slide describe people do is either the class

579
00:43:56,400 --> 00:43:57,630
they just do

580
00:43:57,630 --> 00:44:01,930
you can do all the tasks are presented in the first few slides can be

581
00:44:01,930 --> 00:44:04,450
done using a kind kernel algorithms

582
00:44:04,470 --> 00:44:07,950
many classification clustering and ranking

583
00:44:07,980 --> 00:44:10,750
integration of data like that

584
00:44:10,770 --> 00:44:15,250
and of course all those developed independently of the type of data is going to

585
00:44:15,250 --> 00:44:20,180
be applied apply to this is there is one slide to summarize the ten years

586
00:44:20,180 --> 00:44:23,830
of work from the statistics and machine learning community

587
00:44:23,830 --> 00:44:27,450
so now let's get down to business so we want to to design a kernel

588
00:44:27,450 --> 00:44:29,030
for a given that types

589
00:44:29,100 --> 00:44:33,650
is given an image i want to be able to design a kernel images

590
00:44:33,700 --> 00:44:36,770
so there will be pretty boy what is the kernel is just the way of

591
00:44:36,770 --> 00:44:41,970
the the image and it took about a defining how hard you are going to

592
00:44:41,970 --> 00:44:46,300
predict all the images this would be a space of functions at the n plus

593
00:44:46,300 --> 00:44:47,630
the way been allies

594
00:44:47,630 --> 00:44:51,110
so you have to design principles that you might want to consider

595
00:44:51,160 --> 00:44:56,450
one is to be or nucleon from old going my simple algebraic operations

596
00:44:56,490 --> 00:45:01,200
OK and the man one which i want you to what the focus on is

597
00:45:01,200 --> 00:45:05,870
something else OK if used to positive function you get a positive function

598
00:45:05,940 --> 00:45:08,000
but is a bit more to it

599
00:45:08,080 --> 00:45:13,430
if you assume that you can also features falcon one and features looking to

600
00:45:13,520 --> 00:45:17,910
what does it mean to some together it means that to concatenate the two sets

601
00:45:17,910 --> 00:45:18,980
of features

602
00:45:19,600 --> 00:45:23,720
so in some kernels is like adding features

603
00:45:23,800 --> 00:45:26,740
so of course this is very useful in practice because if you have two sets

604
00:45:26,740 --> 00:45:31,470
of features you can simply sum the kernels which are suited to the features

605
00:45:31,530 --> 00:45:35,560
and of course you have you want to be able to design kernels out of

606
00:45:35,750 --> 00:45:36,640
from scratch

607
00:45:36,670 --> 00:45:41,010
as is essentially you need to be able to do this efficient summing OK and

608
00:45:41,010 --> 00:45:43,630
this is what i refer to this line

609
00:45:43,630 --> 00:45:47,140
so you want to be able to use x which is of high dimension

610
00:45:47,160 --> 00:45:50,130
i think being being like one billion

611
00:45:50,210 --> 00:45:52,940
and of course you want to be able to do the product

612
00:45:52,960 --> 00:45:57,390
between two that the city was x and y in the in the small of

613
00:45:58,210 --> 00:46:01,590
and always will be a very very small or of p in the and that

614
00:46:01,690 --> 00:46:05,560
if you want to be able to do that operation in constant time

615
00:46:05,570 --> 00:46:10,740
OK if can of be very large so he of course you have lot to

616
00:46:10,860 --> 00:46:15,400
to work with and you have two million people to matrix people use the first

617
00:46:15,400 --> 00:46:20,180
one is sparsity in the old sense today despite several times you know in your

618
00:46:20,180 --> 00:46:21,390
in your in your next

619
00:46:21,440 --> 00:46:26,200
and if you can enumerate is nonzero elements then you can compute that are very

620
00:46:27,320 --> 00:46:31,240
these are to have a nice application in

621
00:46:31,290 --> 00:46:35,940
four images by common and but one because it is simply the one of the

622
00:46:35,940 --> 00:46:39,790
relations OK let's take this let's take this kernel

623
00:46:39,790 --> 00:46:41,920
OK so i've been looking

624
00:46:41,950 --> 00:46:44,340
in the end you can expand this dickson

625
00:46:44,340 --> 00:46:47,430
OK as a sort of being the coefficients times

626
00:46:47,540 --> 00:46:49,070
o point one another

627
00:46:49,160 --> 00:46:56,320
then what this means for have constructed the features associated with the following kernel market

628
00:46:56,350 --> 00:46:57,120
for all

629
00:46:57,140 --> 00:46:59,830
set of indices as far some to one

630
00:46:59,840 --> 00:47:04,830
with the feature associated one is simply the corresponding monomial the end

631
00:47:04,880 --> 00:47:06,070
i consider

632
00:47:06,110 --> 00:47:09,540
all possible monomials of given degree as features

633
00:47:09,580 --> 00:47:12,900
of course the if n is very large and so this is a very large

634
00:47:12,900 --> 00:47:18,540
set but you can't even describe the computer but difficult to the the product

635
00:47:19,170 --> 00:47:22,350
it turned out that by the addition can easily

636
00:47:22,400 --> 00:47:25,100
estimate that using a very simple dot product

637
00:47:25,120 --> 00:47:29,390
and the power the power that to to me this example summarizes

638
00:47:29,410 --> 00:47:32,050
all of what's nice with kernels

639
00:47:32,050 --> 00:47:37,010
there large some of many many many many many monomials and then you can compute

640
00:47:37,180 --> 00:47:42,360
the conditions in constant time in a situation of course there are many more many

641
00:47:42,360 --> 00:47:47,550
more possibilities for two nice book but should like this and in

642
00:47:47,700 --> 00:47:52,950
go over all the possible people use but all based on the similar technique

643
00:47:52,970 --> 00:47:55,900
so let's look at how it works images

644
00:47:55,900 --> 00:47:57,850
so let's close it

645
00:47:58,280 --> 00:48:02,530
the case when i want to go to work on the same images so here

646
00:48:02,540 --> 00:48:08,370
so many ways to do segmentations but i don't work in the sense that if

647
00:48:08,370 --> 00:48:10,190
you ask for two segments

648
00:48:10,300 --> 00:48:14,170
you would never get one night segment the the object in one i think man

649
00:48:14,170 --> 00:48:17,600
was about one of is this works if you have a black background but in

650
00:48:17,600 --> 00:48:19,120
practice it will never get

651
00:48:19,130 --> 00:48:20,310
what you want

652
00:48:20,330 --> 00:48:25,480
and many people instead work on the oversegmentation OK so you assume that it is

653
00:48:25,480 --> 00:48:31,290
symmetry that you mention it you know twenty segments that you be cut objective interesting

654
00:48:31,290 --> 00:48:33,140
function class should we be choosing

655
00:48:33,620 --> 00:48:37,420
so we might be tempted to say well we just should we should if we

656
00:48:37,420 --> 00:48:41,570
choose from which shouldn't make a restriction a priority we should take all possible

657
00:48:41,570 --> 00:48:47,970
functions so all functions that take our input domain in mapid into plus minus one

658
00:48:47,970 --> 00:48:50,640
cause we don't know before which one is the right one so we have training data

659
00:48:50,650 --> 00:48:55,060
we use the class of all functions and we let the rest we let machine learning

660
00:48:55,060 --> 00:49:00,940
do the rest or we did our empirical inference empirical risk minimization principle do the rest

661
00:49:00,940 --> 00:49:04,860
and it turns out that cannot work and it's easy to see that this cannot

662
00:49:06,570 --> 00:49:11,920
and I'm going to switch to my later slides for that

663
00:49:12,050 --> 00:49:20,600
okay so I think these are the slides you have and we are now we've now arrived here

664
00:49:22,750 --> 00:49:26,990
so we were talking about the size of the function last give me one second

665
00:49:26,990 --> 00:49:33,380
okay so why don't we take all functions well let's assume we have some training set let's

666
00:49:33,380 --> 00:49:39,880
say names as before defaults and someone gives me some test points and for simplicity let's assume

667
00:49:39,880 --> 00:49:43,840
the test points are this joint from the training points we don't want to see

668
00:49:43,840 --> 00:49:49,690
exactly the same point again but if we have some continuous scenario it's

669
00:49:49,990 --> 00:49:54,230
it's basically the probability that this happens is zero so let's assume this

670
00:49:54,230 --> 00:50:00,330
is not gonna happen then I claim that if you give me a function

671
00:50:00,330 --> 00:50:02,620
F so suppose you

672
00:50:02,640 --> 00:50:06,570
I give you the training points you give me a functions F and you tell me I reckon this is a

673
00:50:06,570 --> 00:50:08,830
a good solution for my problem

674
00:50:08,900 --> 00:50:13,420
then I'll give you a second function FF star which

675
00:50:13,470 --> 00:50:18,920
has exactly the same outputs on all the training points but which says the opposite

676
00:50:18,920 --> 00:50:23,750
on all the test points and it's easy to construct this function because

677
00:50:24,010 --> 00:50:29,640
we are allowing all functions from X to plus minus one so I'll just define

678
00:50:29,650 --> 00:50:31,530
my function to do this

679
00:50:34,750 --> 00:50:40,580
and therefore my function says the opposite in all the test points now the

680
00:50:40,800 --> 00:50:46,660
for the uniform convergence we're interested in the values of the functions everywhere and

681
00:50:46,660 --> 00:50:50,030
if we compute the risk then it's obvious to you I think I don't have

682
00:50:50,030 --> 00:50:52,580
to go through the details that

683
00:50:53,310 --> 00:50:59,160
for this function the risk on training test set is similar to each other

684
00:50:59,160 --> 00:51:04,310
then for this function because this function has the opposite on the test set then for this function

685
00:51:04,330 --> 00:51:08,400
the risks on the training and the test set can be different from each so these

686
00:51:08,410 --> 00:51:14,160
two functions have the F and F star have the same risk on the training set but they say

687
00:51:14,170 --> 00:51:17,660
the opposite on the test set so they might have very different risk on the test set

688
00:51:17,710 --> 00:51:21,570
so if if for one of these functions

689
00:51:21,660 --> 00:51:25,880
if our one of these functions the training and the test error are very near

690
00:51:25,890 --> 00:51:29,710
to each other then for the other function they might be very different from each

691
00:51:29,710 --> 00:51:34,030
other and we have no restriction so it's a roughly speaking in this picture what I am

692
00:51:34,030 --> 00:51:37,790
telling you if you if you give me function that's here for which these two risks are close

693
00:51:37,790 --> 00:51:42,210
I can construct another function which is over here for which the two risks are

694
00:51:42,210 --> 00:51:47,140
very different so it's not possible that these curves are close to each other everywhere

695
00:51:49,010 --> 00:51:53,340
so why it is not possible it's not possible because we were using the class of all

696
00:51:53,340 --> 00:51:55,100
functions from

697
00:51:55,160 --> 00:51:56,920
X to plus minus one

698
00:51:57,290 --> 00:52:02,860
so we need a restriction so this is by the way this is called the no-free-lunch theorem

699
00:52:02,860 --> 00:52:07,900
well this has this was always known to people in to some people in statistics

700
00:52:07,900 --> 00:52:13,600
and machine learning and it was later called the no-free-lunch theorem and so we

701
00:52:13,600 --> 00:52:16,570
need a restriction on the class of functions

702
00:52:17,070 --> 00:52:25,580
and we need this everywhere in machine learning in learning theory the restriction is done in

703
00:52:25,580 --> 00:52:27,230
terms of the capacity

704
00:52:27,270 --> 00:52:32,090
of a class of functions but also in other dom areas like in the Bayesian

705
00:52:32,090 --> 00:52:37,360
community we need to place prior distributions over the class of functions so

706
00:52:37,380 --> 00:52:40,710
we somehow have to restrict the class of functions

707
00:52:40,990 --> 00:52:47,050
either in a hard way or in a soft way and then ideally at least in learning theory we would

708
00:52:47,050 --> 00:52:51,770
like to make statements about how stronger restriction is this what kind of

709
00:52:51,770 --> 00:52:55,420
guaranties does it to allow us to make

710
00:52:55,810 --> 00:53:03,250
to to analyze this a bit let's look at some details and I don't want to

711
00:53:03,250 --> 00:53:07,200
go into all the details because you will have a course next week

712
00:53:07,200 --> 00:53:14,210
by Nicolò Cesa-Bianchi who I think will also cover some of this but

713
00:53:14,210 --> 00:53:19,080
I think I want to spend maybe half an hour or a bit more on this because

714
00:53:19,080 --> 00:53:25,510
it's also useful as basis for some aspects of of kernel methods and anyway it's

715
00:53:25,510 --> 00:53:29,750
if you don't know this stuff I think it's good to maybe hear the main

716
00:53:29,750 --> 00:53:34,080
ideas twice let's see so let's not spend ages on by nothing

717
00:53:34,120 --> 00:53:42,770
it's sufficiently interesting so let's define our or let's take define this short end

718
00:53:42,770 --> 00:53:48,310
xi for our loss the loss of point XI YI remember this was our zero one

719
00:53:48,310 --> 00:53:50,460
that will talk about it

720
00:53:50,500 --> 00:53:53,480
the second person i have set up now

721
00:53:53,500 --> 00:53:57,140
because the data on the syllabus online is

722
00:53:57,180 --> 00:53:58,410
going to be changed

723
00:53:58,430 --> 00:54:00,580
is andrew readily

724
00:54:00,600 --> 00:54:04,160
who is a year also a young graduate

725
00:54:04,160 --> 00:54:09,680
and who set up a hedge fund called white box advisers

726
00:54:09,680 --> 00:54:11,210
and has done

727
00:54:11,230 --> 00:54:12,600
phenomenally wealthy

728
00:54:12,620 --> 00:54:14,940
in investing

729
00:54:18,890 --> 00:54:20,710
i think

730
00:54:20,770 --> 00:54:23,890
i have on the syllabus i have a new york times article about these are

731
00:54:23,890 --> 00:54:25,290
very original

732
00:54:25,350 --> 00:54:26,890
and creative thinker

733
00:54:27,140 --> 00:54:31,310
who looks at things from unique perspective of

734
00:54:31,330 --> 00:54:35,000
and i find it very interesting talking with him because

735
00:54:35,040 --> 00:54:39,460
to do well and investing you have to have your own independent view of things

736
00:54:39,460 --> 00:54:42,000
and really be thinking about how things work

737
00:54:42,040 --> 00:54:46,310
and he is someone who does that incident linear content in another article

738
00:54:48,260 --> 00:54:51,640
saying that he was really one of the first person

739
00:54:51,660 --> 00:54:53,640
two clearly

740
00:54:53,660 --> 00:54:57,160
delineate the sub-prime crisis that we're now and

741
00:54:57,180 --> 00:54:58,890
he saw it coming

742
00:54:58,910 --> 00:55:02,810
and i have to say profited from that if you know the sub-prime crisis is

743
00:55:02,810 --> 00:55:05,830
coming there's always way to profit from that

744
00:55:06,180 --> 00:55:08,040
and that's what he did that

745
00:55:08,080 --> 00:55:10,580
he also has a philanthropic sites

746
00:55:10,600 --> 00:55:12,960
it all comes out very

747
00:55:15,140 --> 00:55:20,160
i think in the remaining time

748
00:55:20,180 --> 00:55:22,480
i'll just go through

749
00:55:22,500 --> 00:55:25,750
an outline of the

750
00:55:25,870 --> 00:55:27,980
an outline of the

751
00:55:31,100 --> 00:55:38,080
and that means through the topics of the various lectures

752
00:55:38,560 --> 00:55:41,080
and then i'll let you go for today

753
00:55:42,100 --> 00:55:48,540
this course is divided up is different than the financial theory course if you look

754
00:55:48,540 --> 00:55:51,620
at john john accomplice course and financial theory

755
00:55:54,270 --> 00:55:55,560
concepts are

756
00:55:55,580 --> 00:55:57,390
central to his

757
00:55:57,440 --> 00:55:59,210
outline of the course

758
00:55:59,210 --> 00:56:01,660
but this being in financial markets caused

759
00:56:01,660 --> 00:56:04,230
i'm dividing it up more in terms of

760
00:56:05,410 --> 00:56:07,250
and institutions

761
00:56:07,290 --> 00:56:09,890
but still i want to start with some theory

762
00:56:09,910 --> 00:56:15,430
and i thought that i will i plan to start by talking about

763
00:56:15,660 --> 00:56:17,230
the basic

764
00:56:17,270 --> 00:56:22,020
the most basic concepts of risk management which underlie finance

765
00:56:22,140 --> 00:56:24,730
and so that would be

766
00:56:24,810 --> 00:56:26,210
wednesday's lecture

767
00:56:26,250 --> 00:56:29,210
i call it the universal principle of risk management

768
00:56:29,250 --> 00:56:31,730
pooling their hedging risk

769
00:56:31,730 --> 00:56:35,830
i think it's the most important theoretical concept

770
00:56:35,850 --> 00:56:40,500
that underlies finance and insurance which will also talk about a little bit in this

771
00:56:41,410 --> 00:56:44,680
and the idea is that if you spread risks

772
00:56:45,930 --> 00:56:48,060
they don't disappear there still there

773
00:56:48,080 --> 00:56:50,520
but there are spread out over many people

774
00:56:50,520 --> 00:56:52,750
and the impact on any one person

775
00:56:52,810 --> 00:56:57,120
is reduced so the basic principle of insurances

776
00:56:58,640 --> 00:57:02,700
each persons for each family suffers the risk for example that

777
00:57:02,720 --> 00:57:05,940
apparent father mother might die

778
00:57:05,960 --> 00:57:07,210
which is terrible

779
00:57:07,230 --> 00:57:08,750
blow to the family

780
00:57:08,770 --> 00:57:12,390
but it's not a blow to society as a whole because people die

781
00:57:12,410 --> 00:57:14,980
and it has a certain statistical regularity

782
00:57:15,000 --> 00:57:17,480
so it makes sense that we

783
00:57:18,160 --> 00:57:22,810
families who have lost a father mother so that they can keep going

784
00:57:22,830 --> 00:57:25,250
it benefits everyone to have

785
00:57:25,270 --> 00:57:28,160
the situation in place for that

786
00:57:28,180 --> 00:57:32,210
i want to talk about that with a little bit a reference to probability theory

787
00:57:32,230 --> 00:57:33,870
and so that's what

788
00:57:33,910 --> 00:57:35,960
i will be covering the the

789
00:57:35,960 --> 00:57:38,870
the next lecture will be among the more mathematical

790
00:57:38,930 --> 00:57:43,060
although it's very elementary if you had a course in probability and statistics

791
00:57:43,160 --> 00:57:46,250
you find it easy to follow

792
00:57:46,250 --> 00:57:52,000
but it's self-contained again i feel like i have to introduce concepts like

793
00:57:52,020 --> 00:57:54,100
variance and covariance

794
00:57:54,120 --> 00:57:59,020
correlation in order to talk about finances so that's what we'll do

795
00:57:59,040 --> 00:58:01,210
in lecture two

796
00:58:01,290 --> 00:58:05,540
then the next lecture i want to come back to some basic themes

797
00:58:06,000 --> 00:58:07,770
that the third lecture

798
00:58:07,790 --> 00:58:10,270
about technology

799
00:58:10,580 --> 00:58:13,910
and this relates to another book that i wrote

800
00:58:13,910 --> 00:58:15,790
i'm not signing

801
00:58:15,850 --> 00:58:17,350
i wrote a book called new

802
00:58:20,520 --> 00:58:23,290
in two thousand three

803
00:58:23,290 --> 00:58:26,810
about technology and finance

804
00:58:26,810 --> 00:58:29,430
a theme of that book was there

805
00:58:29,430 --> 00:58:35,680
i've already said this to you but it's very important point that financial technology is

806
00:58:35,680 --> 00:58:38,430
evolving and improving just the way

807
00:58:38,520 --> 00:58:44,710
engineering technology or biochemical technology is improving it's getting better year by year

808
00:58:44,730 --> 00:58:48,370
and in the course of finance

809
00:58:48,430 --> 00:58:52,390
over your lifetime will be dramatic

810
00:58:53,100 --> 00:58:57,290
the financial institutions that we have ten years from now

811
00:58:57,310 --> 00:59:00,100
will look very different from the ones we have now

812
00:59:04,430 --> 00:59:09,790
o thing to we have to understand in understanding the progress of financial technology is

813
00:59:09,790 --> 00:59:13,560
its fundamental relation to information technology

814
00:59:13,560 --> 00:59:15,220
variables was living in sequence

815
00:59:15,220 --> 00:59:18,660
and consider the set of blue groups like that

816
00:59:18,700 --> 00:59:22,970
and there is this is over all of the groups so you will set to

817
00:59:22,970 --> 00:59:25,950
zero some of the groups so what you're left with

818
00:59:25,990 --> 00:59:30,450
it is contiguous but can you remove stuff on the right some stuff on the

819
00:59:30,450 --> 00:59:36,990
left to what you what what is remaining is something in the middle because away

820
00:59:37,040 --> 00:59:43,100
of getting sparsity patterns which can be not anything but from a particular class continues

821
00:59:43,100 --> 00:59:44,370
but terms

822
00:59:44,390 --> 00:59:48,810
we can do the same thing with the rectangles into d so you take all

823
00:59:48,810 --> 00:59:50,270
the groups which are

824
00:59:50,290 --> 00:59:57,990
hyperplanes which are parallel to the axis again those once all the all the other

825
00:59:58,790 --> 01:00:02,510
all the others underwent as well so what you're left with

826
01:00:02,540 --> 01:00:06,990
when you remove some of the album's songs about some of the top some on

827
01:00:06,990 --> 01:00:09,770
the left and some on the right you get back

828
01:00:09,790 --> 01:00:14,720
OK so this can be applicable to also all the other angles

829
01:00:16,240 --> 01:00:17,540
and just like that

830
01:00:17,540 --> 01:00:24,160
you also removed hyperplanes which are not parallel to x is a way to get

831
01:00:24,160 --> 01:00:28,450
get diamond shape but you can go on and go back and you can go

832
01:00:28,450 --> 01:00:29,990
and more angles

833
01:00:29,990 --> 01:00:33,450
to get to get convex but

834
01:00:33,450 --> 01:00:36,700
i will show you in later

835
01:00:36,740 --> 01:00:38,100
five times

836
01:00:38,120 --> 01:00:42,330
how this can be applied to to the primal sparse PCA

837
01:00:42,330 --> 01:00:43,890
so here's the goal is that

838
01:00:43,950 --> 01:00:46,330
we have a way of

839
01:00:46,350 --> 01:00:48,560
of getting

840
01:00:48,580 --> 01:00:51,450
but the prior the different sparsity patterns

841
01:00:51,470 --> 01:00:55,180
again i would like to see how it is relevant to

842
01:00:55,220 --> 01:01:00,970
so this can be done in other ways and for you keys which actually before

843
01:01:01,120 --> 01:01:02,540
so here we focused

844
01:01:02,600 --> 01:01:07,870
on intersection closed set of of seaport support vector

845
01:01:07,930 --> 01:01:10,370
convex sets are closed by intersection

846
01:01:10,390 --> 01:01:13,220
and there is a nice work by jacques

847
01:01:13,220 --> 01:01:14,660
which does the opposite

848
01:01:14,680 --> 01:01:18,830
which is considered sparsity patterns which are closed under union

849
01:01:19,450 --> 01:01:21,700
depending on your application it might be

850
01:01:21,740 --> 01:01:25,830
more relevant to be to have been enclosed to intersection closed

851
01:01:25,850 --> 01:01:28,810
the fact

852
01:01:28,830 --> 01:01:30,930
so this is it for sparse

853
01:01:30,990 --> 01:01:33,580
structured metadata that and then i will go

854
01:01:33,600 --> 01:01:34,990
so very quickly

855
01:01:35,020 --> 01:01:38,100
but mister on march on matrices

856
01:01:38,160 --> 01:01:41,790
and this is to me a very nice topic and i wish i had more

857
01:01:41,790 --> 01:01:45,180
time to do it you can look at this lies later on if you want

858
01:01:46,020 --> 01:01:50,430
the discovery of something which i one go

859
01:01:50,450 --> 01:01:55,370
so far what you want to is the simplest the simplest example is the problem

860
01:01:55,370 --> 01:01:59,620
of collaborative filtering

861
01:01:59,620 --> 01:02:06,410
the goal is to essentially to complete the matrix so you assume that you have

862
01:02:06,410 --> 01:02:11,160
some movies and some customers so this is related to the netflix challange

863
01:02:11,180 --> 01:02:14,310
and you have you observe some of the ratings for some of the customers and

864
01:02:14,310 --> 01:02:15,470
the goal is to

865
01:02:15,490 --> 01:02:18,750
find all possible writing for all possible customers

866
01:02:18,790 --> 01:02:22,160
this is essentially the matrix completion problem

867
01:02:22,160 --> 01:02:27,600
what was one is the parliament learning on matrices multi-task learning

868
01:02:27,620 --> 01:02:30,410
so if you assume that you have k prediction tasks

869
01:02:31,160 --> 01:02:35,560
so you want to cluster to have a different asking you want so for each

870
01:02:35,560 --> 01:02:38,390
of them you might want to consider only our model

871
01:02:38,390 --> 01:02:42,240
we have k with the dance OK so if you stack them in a metric

872
01:02:42,250 --> 01:02:44,060
get p times q matrix

873
01:02:44,080 --> 01:02:45,430
and essentially

874
01:02:45,700 --> 01:02:51,310
what people have been considering is they want to share parameters more the different divisions

875
01:02:51,350 --> 01:02:53,850
these as we show is equivalent to put

876
01:02:53,870 --> 01:02:57,240
sparse priors on the other big matrix of all predictors

877
01:02:57,290 --> 01:02:59,990
OK so this as many applications

878
01:03:00,040 --> 01:03:02,720
in one of them which i think is interesting

879
01:03:02,740 --> 01:03:04,040
just multiclass

880
01:03:04,200 --> 01:03:05,680
you want to share

881
01:03:05,700 --> 01:03:11,040
statistical power between different classes and this is actually the topic of

882
01:03:11,040 --> 01:03:13,910
multitask learning

883
01:03:13,930 --> 01:03:16,180
all the examples image denoising

884
01:03:16,180 --> 01:03:17,910
so here in image denoising

885
01:03:17,990 --> 01:03:23,020
very recent approach which was launched by

886
01:03:23,040 --> 01:03:26,200
got its name because p and later slides

887
01:03:26,220 --> 01:03:31,410
you do know all patches of a given images simultaneously so you take an image

888
01:03:31,430 --> 01:03:34,930
x high although also small patches the eight times eights

889
01:03:34,950 --> 01:03:35,990
so there you

890
01:03:36,040 --> 01:03:40,060
you get a lot of pages and you you want to know them all of

891
01:03:40,060 --> 01:03:43,830
them at the same time we we see that this can be done by putting

892
01:03:43,830 --> 01:03:44,930
a sparse pile

893
01:03:44,990 --> 01:03:49,580
on the metrics of all of all patches

894
01:03:49,600 --> 01:03:54,260
so now what other types of sparsity for matrices so you have to be that

895
01:03:54,260 --> 01:04:03,120
types the first you put zero directly on the elements of the matrix

896
01:04:03,140 --> 01:04:07,080
so the first one is to put zeros all over the place

897
01:04:07,220 --> 01:04:11,470
the second one is to put two all columns being set to zero so this

898
01:04:11,470 --> 01:04:12,270
would be

899
01:04:12,350 --> 01:04:17,120
a classical example for task that we shall so those are

900
01:04:17,120 --> 01:04:22,010
are not actually considering the fact that the matrix OK is just because it was

901
01:04:22,010 --> 01:04:23,640
anywhere for the first case

902
01:04:23,640 --> 01:04:27,270
and you put columns to zeros

903
01:04:27,290 --> 01:04:29,680
the second case they don't use the matrices

904
01:04:29,700 --> 01:04:32,720
i can be multiplied together by the

905
01:04:32,740 --> 01:04:37,660
and the second time we have essentially consider the fact that matrices can be factorized

906
01:04:37,660 --> 01:04:42,160
so if you can factorize matrix as being you times transpose

907
01:04:42,180 --> 01:04:43,640
well as a

908
01:04:43,660 --> 01:04:46,510
you OK you have two types of sparsity here

909
01:04:46,520 --> 01:04:52,270
either you assume that the big matrix can be decomposed into sum of the product

910
01:04:52,270 --> 01:04:56,490
of two skinny matrices this is adding a low rank so n is small

911
01:04:56,540 --> 01:04:58,930
or you might consider the case where

912
01:04:59,020 --> 01:05:02,910
you may consider the case where

913
01:05:03,030 --> 01:05:07,910
you have still being matrices u and v are hired by a large number of

914
01:05:09,290 --> 01:05:12,350
but you want to put some zeros inside one of them

915
01:05:12,450 --> 01:05:15,850
it's all about two different types of sparsity that we consider

916
01:05:16,180 --> 01:05:19,220
one is low rank one so m is small

917
01:05:19,220 --> 01:05:22,350
the other one is you being sparse

918
01:05:22,370 --> 01:05:28,830
so these in many instances of matrix factorisation

919
01:05:28,850 --> 01:05:30,720
you have a

920
01:05:30,910 --> 01:05:37,720
this later became the first task so multitasking told you you consider all predictors

921
01:05:38,450 --> 01:05:41,510
you have to give credit to put in a single matrix now you have two

922
01:05:41,510 --> 01:05:43,330
sets of

923
01:05:43,390 --> 01:05:48,430
two men set works in that city using sparsity inducing norms the first one is

924
01:05:48,430 --> 01:05:51,850
to penalize the norm the sum of the norms

925
01:05:51,870 --> 01:05:53,080
of each hole

926
01:05:53,100 --> 01:05:54,740
of of w

927
01:05:54,770 --> 01:05:56,790
so it's initially

928
01:05:56,790 --> 01:05:59,930
if we try to write some of the most zero

929
01:05:59,970 --> 01:06:05,560
so things all essentially is the weight vector for all possible ask for given feature

930
01:06:05,770 --> 01:06:08,010
is then move future for all

931
01:06:08,020 --> 01:06:09,580
tasks simultaneously

932
01:06:09,600 --> 01:06:11,490
this is often called joint

933
01:06:11,560 --> 01:06:12,790
viable selection

934
01:06:12,810 --> 01:06:15,330
this is this is well by obozinski

935
01:06:15,330 --> 01:06:17,990
and the other type

936
01:06:18,020 --> 01:06:21,290
the other type of

937
01:06:21,370 --> 01:06:24,080
for multitask learning is to consider

938
01:06:24,100 --> 01:06:27,580
john produce selection so here

939
01:06:27,580 --> 01:06:31,020
you would have to be careful about what you mean but to mean by future

940
01:06:31,040 --> 01:06:34,390
so he about what i mean by bibles is just one of the p

941
01:06:34,410 --> 01:06:35,770
well is not viable

942
01:06:35,790 --> 01:06:37,370
and what i mean by future

943
01:06:37,370 --> 01:06:43,470
is there any linear combination of the original variables and essentially what the trace known

944
01:06:43,470 --> 01:06:45,270
we will allow you to do

945
01:06:45,290 --> 01:06:47,700
it cost to construct specific

946
01:06:47,720 --> 01:06:50,450
features and select pond

947
01:06:50,470 --> 01:06:54,540
so in the bible selection you don't create anything in this one you create new

948
01:06:54,540 --> 01:06:58,670
we find that there is a combination for age information in the model

949
01:06:58,820 --> 01:07:06,090
the so age information flow but

950
01:07:06,090 --> 01:07:08,500
so look get information there

951
01:07:08,520 --> 01:07:13,090
but that's not less than five percent sixty six percent so so that was that

952
01:07:13,460 --> 01:07:16,300
does not model therefore my

953
01:07:18,000 --> 01:07:21,190
to explain what relationships in the model

954
01:07:21,250 --> 01:07:23,900
which is called conditional independence graph

955
01:07:23,920 --> 01:07:27,250
then we have to draw a line from that calls

956
01:07:27,270 --> 01:07:28,750
the interaction is in the

957
01:07:28,770 --> 01:07:33,880
the age car information that is not right so so what is this

958
01:07:33,900 --> 01:07:36,590
table and tell me

959
01:07:36,650 --> 01:07:38,570
it tells me

960
01:07:38,590 --> 01:07:45,670
which of these variables are related to one another basically so

961
01:07:46,610 --> 01:07:47,770
i can look at this

962
01:07:47,790 --> 01:07:54,130
perhaps the most important thing that i'm interested in this model is survival people

963
01:07:54,190 --> 01:07:56,770
so what affects their survival

964
01:07:56,790 --> 01:07:58,320
so what you want

965
01:07:58,320 --> 01:08:00,380
what is it that

966
01:08:00,420 --> 01:08:05,650
has an effect on the basis of the number of survivors goal

967
01:08:09,090 --> 01:08:13,150
basically what this is telling me is this

968
01:08:13,150 --> 01:08:16,630
this is the from one of the labels

969
01:08:16,650 --> 01:08:20,630
two survival without going to do anything else than directly

970
01:08:21,610 --> 01:08:23,380
so saying

971
01:08:25,400 --> 01:08:27,730
is affected by sense

972
01:08:27,730 --> 01:08:31,800
so we sent to diagnose and the appearance of the so whether or not it

973
01:08:33,540 --> 01:08:35,070
it looked

974
01:08:35,290 --> 01:08:37,860
all good

975
01:08:37,860 --> 01:08:43,320
if you have these pieces of information and a patient and knowing how

976
01:08:43,340 --> 01:08:46,540
the information status doesn't matter because

977
01:08:46,960 --> 01:08:54,210
in particular information about age is captured in information center that's the thing that directly

978
01:08:54,230 --> 01:08:55,670
affecting the survival

979
01:08:59,360 --> 01:09:02,710
the effects of the centre of

980
01:09:02,730 --> 01:09:08,230
sensors related to information services like right to appearance so it is related to a

981
01:09:08,230 --> 01:09:09,270
but if

982
01:09:09,420 --> 01:09:12,690
if somebody tells me the centre i don't need to know the age to work

983
01:09:12,690 --> 01:09:14,520
out what effects on survival

984
01:09:14,540 --> 01:09:18,020
that's what the pictures tell me how these things relate to one another on the

985
01:09:18,020 --> 01:09:20,520
way constructive table

986
01:09:20,520 --> 01:09:23,520
the songwriter diagram

987
01:09:23,540 --> 01:09:25,360
was i went to

988
01:09:25,380 --> 01:09:27,400
all these terms here

989
01:09:27,420 --> 01:09:29,400
one of the colon

990
01:09:29,420 --> 01:09:30,210
and they

991
01:09:30,270 --> 01:09:31,650
five percent

992
01:09:31,670 --> 01:09:36,610
the last five percent in this column then began line to have having the debt

993
01:09:39,920 --> 01:09:43,670
which is what got here

994
01:09:43,710 --> 01:09:47,670
all those lines correspond to terms in the model

995
01:09:47,940 --> 01:09:53,730
so this is the an idea of relationships across

996
01:09:53,800 --> 01:09:58,800
o five variables if i think survives the poem one the so

997
01:09:58,860 --> 01:10:03,570
the things are are actually directly related to survival as an independent

998
01:10:03,590 --> 01:10:09,840
i find know those two variables that age information tell me anything about survival

999
01:10:19,920 --> 01:10:23,730
but just regression is another

1000
01:10:23,730 --> 01:10:25,480
generalisation the services

1001
01:10:27,130 --> 01:10:30,770
your outputs is binary

1002
01:10:30,800 --> 01:10:33,710
and what is model is the

1003
01:10:33,730 --> 01:10:37,570
the chance of being one of zero

1004
01:10:37,590 --> 01:10:41,960
so because binary the the expected value is the same as the probability of getting

1005
01:10:41,960 --> 01:10:48,270
the one so the expected value which is the probability that one what you all

1006
01:10:48,340 --> 01:10:52,290
data point j

1007
01:10:54,420 --> 01:10:56,130
this model but

1008
01:10:56,150 --> 01:10:57,210
this thing which is

1009
01:10:57,210 --> 01:10:58,900
i don't recall

1010
01:10:58,920 --> 01:11:01,710
this because the most common just call

1011
01:11:01,730 --> 01:11:05,110
this is the transformation of

1012
01:11:05,230 --> 01:11:08,820
is related to the businesses

1013
01:11:08,840 --> 01:11:13,750
we can use the same data before went after to see what's going on if

1014
01:11:13,750 --> 01:11:16,860
you think about the probability of survival

1015
01:11:16,900 --> 01:11:21,750
if you think really fast the model think of these patients

1016
01:11:21,770 --> 01:11:24,610
otherwise not the count it's the

1017
01:11:24,630 --> 01:11:26,920
one if the patient survives

1018
01:11:26,940 --> 01:11:32,000
so three years and sierra didn't so we refine the model

1019
01:11:32,000 --> 01:11:39,230
so actually actively concentrate on survival rather than looking at the relationship between different levels

1020
01:11:39,440 --> 01:11:44,070
then the same idea we get

1021
01:11:44,170 --> 01:11:45,610
table that's

1022
01:11:45,630 --> 01:11:48,820
we'll bring the terms in and

1023
01:11:48,840 --> 01:11:51,730
these numbers are telling us the

1024
01:11:51,750 --> 01:11:57,820
basically how models change however what the contribution of each of these terms is we

1025
01:11:57,820 --> 01:11:59,520
go after him because

1026
01:11:59,540 --> 01:12:01,920
of these

1027
01:12:01,940 --> 01:12:07,170
values against a before we call the five percent business

1028
01:12:07,170 --> 01:12:11,380
so what we've got here which allows approximately one

1029
01:12:11,400 --> 01:12:13,860
that one

1030
01:12:17,060 --> 01:12:20,230
OK well known

1031
01:12:20,960 --> 01:12:22,790
sensor appearance

1032
01:12:22,820 --> 01:12:27,400
have an effect upon the probability of survival

1033
01:12:27,440 --> 01:12:29,480
being one also

1034
01:12:29,500 --> 01:12:31,540
that was telling us

1035
01:12:31,730 --> 01:12:36,800
so we tells us we don't need to include

1036
01:12:36,820 --> 01:12:39,570
any of the inputs like to this law

1037
01:12:39,570 --> 01:12:41,790
also a ge

1038
01:12:41,820 --> 01:12:43,190
with this includes

1039
01:12:43,210 --> 01:12:44,650
dummy variables for

1040
01:12:45,860 --> 01:12:47,460
and appearance

1041
01:12:50,070 --> 01:12:54,540
the source come out of this come out of

1042
01:12:54,560 --> 01:12:56,420
then model

1043
01:12:56,420 --> 01:12:59,750
we look at all these different data

1044
01:12:59,760 --> 01:13:04,790
so all the way in which we try to recognise the people

1045
01:13:04,850 --> 01:13:11,060
for the first and then we might put me on the first we can recognise

1046
01:13:11,500 --> 01:13:16,020
want recognise we here kick in the image

1047
01:13:16,090 --> 01:13:17,880
that that can

1048
01:13:17,900 --> 01:13:19,960
that you may be interested party

1049
01:13:19,970 --> 01:13:22,760
and also that people look very happy or

1050
01:13:22,800 --> 01:13:29,700
so in image recognition they want to do in motion detection the fact

1051
01:13:29,700 --> 01:13:31,120
in other media

1052
01:13:31,150 --> 01:13:34,760
you can extract the and the mind

1053
01:13:34,790 --> 01:13:39,180
and the way he did in fact extraction

1054
01:13:40,670 --> 01:13:47,430
ten clubs are competing but

1055
01:13:47,440 --> 01:13:55,660
now in the core i will not images because we are not image group regions

1056
01:13:56,680 --> 01:14:01,810
in our approach we worked very closely with people

1057
01:14:03,730 --> 01:14:08,240
because both sides we can show

1058
01:14:08,260 --> 01:14:10,370
but now

1059
01:14:11,600 --> 01:14:14,260
information extraction from text

1060
01:14:14,260 --> 01:14:16,500
it's quite a lot

1061
01:14:16,930 --> 01:14:25,740
it's actually how well we're doing between NLP

1062
01:14:25,760 --> 01:14:29,130
natural language processing and information

1063
01:14:29,150 --> 01:14:35,410
so from natural language processing you have you heard

1064
01:14:35,420 --> 01:14:39,230
the description of the structural properties of the language

1065
01:14:40,720 --> 01:14:49,380
the fact extraction which you might use some semantic concept of my company

1066
01:14:51,710 --> 01:14:54,500
many features that used the

1067
01:14:54,520 --> 01:15:00,910
can you direct can be like the a natural language for

1068
01:15:00,950 --> 01:15:05,780
the information in the field that could be not

1069
01:15:08,450 --> 01:15:09,350
and more

1070
01:15:09,430 --> 01:15:16,570
and the work that is called the blue and from here

1071
01:15:16,620 --> 01:15:22,240
in the US organised by the national institutes of technology

1072
01:15:22,280 --> 01:15:24,740
you have no complication

1073
01:15:24,780 --> 01:15:26,840
maybe you have heard

1074
01:15:26,860 --> 01:15:31,210
we are not really aware

1075
01:15:33,760 --> 01:15:39,280
but if you look many many times the american complication so the way to semantic

1076
01:15:40,480 --> 01:15:45,430
come in the eighty eight eighty nine

1077
01:15:45,480 --> 01:15:50,840
they these automatic text content extraction conference

1078
01:15:50,900 --> 01:15:54,750
and now we're here to make an alliance

1079
01:15:57,830 --> 01:16:01,280
i know

1080
01:16:01,340 --> 01:16:11,430
so we can look at the role of natural language processing to

1081
01:16:11,470 --> 01:16:17,930
one of the text written in natural language

1082
01:16:17,940 --> 01:16:20,920
the might be number of features

1083
01:16:20,970 --> 01:16:26,820
which we could use in our extraction you now take that point

1084
01:16:26,880 --> 01:16:30,160
so if we look at the lexical

1085
01:16:30,160 --> 01:16:32,310
we were just text

1086
01:16:32,340 --> 01:16:36,990
we look at more technical features the syntactic structure

1087
01:16:37,830 --> 01:16:39,940
this tactic worked class

1088
01:16:39,940 --> 01:16:44,560
which might be revealed by morphological like keith

1089
01:16:46,720 --> 01:16:48,270
the might

1090
01:16:48,590 --> 01:16:51,160
if have these

1091
01:16:51,230 --> 01:16:56,550
have the already this name is first point and the might

1092
01:16:56,610 --> 01:16:58,260
there are

1093
01:17:00,520 --> 01:17:02,550
a little bit more

1094
01:17:02,580 --> 01:17:08,280
details about

1095
01:17:09,740 --> 01:17:10,500
in the first

1096
01:17:10,510 --> 01:17:12,570
four west

1097
01:17:12,600 --> 01:17:15,530
the first course information

1098
01:17:15,540 --> 01:17:17,450
you might probably no

1099
01:17:17,600 --> 01:17:24,700
in other words and information but i mean it into to include

1100
01:17:24,750 --> 01:17:27,780
the first four one one

1101
01:17:30,560 --> 01:17:33,050
so you have to take

1102
01:17:33,070 --> 01:17:34,630
it's just silly

1103
01:17:34,680 --> 01:17:36,380
of character

1104
01:17:36,400 --> 01:17:41,490
and you would like to the the first thing that you is recognised it on

1105
01:17:41,490 --> 01:17:42,620
the talk

1106
01:17:42,670 --> 01:17:44,190
used to talk

1107
01:17:46,290 --> 01:17:48,690
which are like

1108
01:17:50,650 --> 01:17:52,020
so you can work

1109
01:17:52,040 --> 01:17:55,070
characters in two separate

1110
01:17:58,440 --> 01:18:01,700
language which he limit

1111
01:18:01,820 --> 01:18:05,310
so you have to

1112
01:18:05,330 --> 01:18:07,980
these are the intuition

1113
01:18:08,010 --> 01:18:11,660
between the wars

1114
01:18:12,520 --> 01:18:15,470
it's not also simple as it looks

1115
01:18:17,690 --> 01:18:20,150
well for

1116
01:18:20,160 --> 01:18:21,940
the first one

1117
01:18:21,950 --> 01:18:27,210
many languages which you do not have space it's more difficult so

1118
01:18:27,260 --> 01:18:33,210
in chinese i find the languages you do not have to be so you have

1119
01:18:33,260 --> 01:18:35,940
to have or dictionary

1120
01:18:37,050 --> 01:18:42,420
so that you can find this on the talk

1121
01:18:42,420 --> 01:18:49,290
or you can be in an automatic could we don't

1122
01:18:49,800 --> 01:18:54,260
now if you need to implement

1123
01:18:54,300 --> 01:19:00,340
such a tool for talking these you well it doesn't seem to

1124
01:19:00,600 --> 01:19:04,680
you need quite a lot language

1125
01:19:04,790 --> 01:19:08,970
who think that you have core

1126
01:19:08,980 --> 01:19:10,650
when think you have

1127
01:19:11,160 --> 01:19:14,170
that was the week

1128
01:19:14,180 --> 01:19:19,000
but in different languages at might things

1129
01:19:19,340 --> 01:19:20,720
it is

1130
01:19:20,830 --> 01:19:22,930
have to oppose

1131
01:19:22,940 --> 01:19:24,270
which is that the

1132
01:19:25,880 --> 01:19:29,800
the journey before it actually work

1133
01:19:31,590 --> 01:19:33,750
don't have the words

1134
01:19:33,760 --> 01:19:36,260
the same with your that

1135
01:19:37,810 --> 01:19:40,660
you have

1136
01:19:41,940 --> 01:19:43,630
this is what

1137
01:19:46,020 --> 01:19:47,920
this is not the

1138
01:19:47,930 --> 01:19:50,540
you might have

1139
01:19:50,560 --> 01:19:58,750
you could also imagine that is a numbers when the eight

1140
01:20:00,170 --> 01:20:02,770
and because of that

1141
01:20:02,800 --> 01:20:04,760
we can quite uniform

1142
01:20:04,760 --> 01:20:07,880
so you might want to take it

1143
01:20:07,900 --> 01:20:09,500
more letters

1144
01:20:09,500 --> 01:20:10,710
the climate is

1145
01:20:10,720 --> 01:20:12,900
of pairwise similarity between

1146
01:20:13,100 --> 01:20:19,870
so for first entry i j of matrix similarity between sequences i wouldn't change

1147
01:20:19,920 --> 01:20:23,820
and this would be the inputs or so

1148
01:20:23,990 --> 01:20:28,000
it sounds like a very simple solution now

1149
01:20:28,010 --> 01:20:31,040
but from this already can imagine things

1150
01:20:31,060 --> 01:20:36,020
first of all this is that what you need what finally

1151
01:20:36,080 --> 01:20:39,500
don't care about what was originally the fact the you

1152
01:20:39,550 --> 01:20:44,020
this is to say that you are sequences and you are able to develop

1153
01:20:44,040 --> 01:20:49,340
visual similarity between sequences your friend is here you have some features and for some

1154
01:20:49,340 --> 01:20:50,790
reason why don't

1155
01:20:50,810 --> 01:20:51,540
to make

1156
01:20:51,560 --> 01:20:53,920
the first problem we can

1157
01:20:53,970 --> 01:20:58,450
during the result of distributions will also be an entry sites

1158
01:20:58,460 --> 01:21:01,440
so this is the the algorithm again that

1159
01:21:01,450 --> 01:21:04,200
i think this important will be able to process

1160
01:21:04,210 --> 01:21:09,020
strings as well as feature as well as the world as well as graph

1161
01:21:09,070 --> 01:21:13,770
you've got the nation between indian and what was here

1162
01:21:13,780 --> 01:21:16,390
so the solution is the size of the

1163
01:21:16,990 --> 01:21:22,510
definition is the number of points you analyse so here three sequences

1164
01:21:22,530 --> 01:21:24,590
you would have three by symmetries

1165
01:21:24,600 --> 01:21:28,880
one of the sequences with the one thousand by one thousand metrics

1166
01:21:30,750 --> 01:21:34,490
there absolutely no need to take

1167
01:21:36,000 --> 01:21:39,580
that's this a lot of the time so this is the end

1168
01:21:39,840 --> 01:21:45,290
in some cases with have some difficulty to handle large datasets you have one million

1169
01:21:46,670 --> 01:21:52,530
you can't imagine the metrics of one million one hundred

1170
01:21:53,540 --> 01:21:59,000
so this is basically what what would be the distribution of these countries if you

1171
01:21:59,000 --> 01:22:00,600
can follow the

1172
01:22:00,610 --> 01:22:06,280
four languages start from data computers commentaries and this is something that metrics so here

1173
01:22:06,390 --> 01:22:11,330
is one example support vector machines are one example of the reason that you can

1174
01:22:11,330 --> 01:22:15,020
see these were the first to point to the symmetries can all

1175
01:22:15,070 --> 01:22:18,990
similarity and that these are the input to the right

1176
01:22:19,270 --> 01:22:20,000
but not

1177
01:22:20,060 --> 01:22:24,610
OK so now i need to be more precise on the type of supervision we

1178
01:22:24,610 --> 01:22:29,740
use the lot entries in fact we focus on particular similarity which i call positive

1179
01:22:29,740 --> 01:22:31,310
definite kernels

1180
01:22:31,330 --> 01:22:34,500
i mean i just want to give you the a precise definition of what it

1181
01:22:34,500 --> 01:22:40,160
is so i can only be a measure of similarity in order to

1182
01:22:40,180 --> 01:22:46,020
i feel the entries of the matrix so you could be function k

1183
01:22:46,030 --> 01:22:49,690
they need to input that you want to hear we assume that

1184
01:22:49,700 --> 01:22:55,260
we focus on that input sequences we call capitalism this set of all sequences so

1185
01:22:55,270 --> 01:22:57,160
we don't assume any structure on

1186
01:22:57,210 --> 01:23:00,650
we don't seem to be just a set of this film is notable whatever

1187
01:23:00,700 --> 01:23:03,950
it can be finite and it continues to whatever

1188
01:23:03,970 --> 01:23:09,370
and here would just that can only be sure that these two objects and was

1189
01:23:09,420 --> 01:23:14,410
and we want to focus on properties the first being that it has to be

1190
01:23:14,410 --> 01:23:16,790
symmetric taken into objects

1191
01:23:16,800 --> 01:23:21,000
when you get prime is similar to that of the same

1192
01:23:21,020 --> 01:23:22,040
you get

1193
01:23:22,050 --> 01:23:23,570
OK you its front

1194
01:23:23,580 --> 01:23:28,050
so that's the easy part and the second part is the first time it may

1195
01:23:28,050 --> 01:23:34,590
be confusing i would try to spend it is what's called it

1196
01:23:34,600 --> 01:23:37,560
so one think is still in these equations

1197
01:23:37,570 --> 01:23:43,760
that if you give me any sample sequences a finite set of sequences of points

1198
01:23:43,770 --> 01:23:46,510
like i could explicitly said

1199
01:23:46,530 --> 01:23:51,130
and if you need any set of real valued real numbers

1200
01:23:51,150 --> 01:23:57,080
positive and negative then you should also called a i j k fixates j

1201
01:23:57,090 --> 01:23:59,280
this has to be nonnegative numbers

1202
01:23:59,290 --> 01:24:00,110
so i think

1203
01:24:00,120 --> 01:24:01,350
so that

1204
01:24:01,370 --> 01:24:05,350
it is simply to say that if we go back to the metrics

1205
01:24:05,360 --> 01:24:07,870
one is a set of points to give me

1206
01:24:07,890 --> 01:24:13,270
when you look to the of similarities is that has to be positive semidefinite

1207
01:24:13,290 --> 01:24:15,580
that's something we we heard about this morning

1208
01:24:15,680 --> 01:24:22,340
and whatever you have function so that it always puts positive semidefinite basically to pursue

1209
01:24:24,150 --> 01:24:27,810
so you see that one of the reasons sort of one of the why we

1210
01:24:28,180 --> 01:24:31,040
when we end with show that the

1211
01:24:31,050 --> 01:24:31,750
now the

1212
01:24:31,760 --> 01:24:32,600
function k

1213
01:24:32,610 --> 01:24:38,040
has this property then we show that metrics with the positive semidefinite and in fact

1214
01:24:38,040 --> 01:24:44,960
can be thought as categories and that they input positive semidefinite

1215
01:24:44,980 --> 01:24:49,080
so why why focus on that well it turns out that there are many reasons

1216
01:24:49,080 --> 01:24:49,840
for it

1217
01:24:50,500 --> 01:24:55,320
one way to solve some of these two provinces opposite the internals to give you

1218
01:24:55,320 --> 01:24:59,440
some ideas of what we do when we focus on this count

1219
01:24:59,440 --> 01:25:06,470
by semi-definite of second-order cone constraints but still it would be useful if they were handled

1220
01:25:06,470 --> 01:25:27,710
directly are there questions about  this yes it will be very useful so what's CVX CVX

1221
01:25:27,710 --> 01:25:33,660
does actually handle exponential and logarithms so it can solve a geometric programming problems for

1222
01:25:33,660 --> 01:25:41,420
example but it'll approximate the it has a a quite sophisticated approximation of these functions as

1223
01:25:41,420 --> 01:25:48,380
polynomial functions and then polynomial constraints can be of a form that can be written as

1224
01:25:48,380 --> 01:25:52,980
semi-definite constraints so it's not the most efficient way of handling these constraints but you

1225
01:25:52,980 --> 01:25:59,100
can do it in CVX but if and the reason is that the reason why they don't

1226
01:25:59,100 --> 01:26:03,300
do it directly is that the solvers that are used by CVX don't handle these

1227
01:26:03,300 --> 01:26:08,400
two cones they're restricted to linear the nonactive orthant second s second order cone and

1228
01:26:08,400 --> 01:26:12,640
the semi-definite cone  if the solvers handled these cones that would be a very nice

1229
01:26:12,640 --> 01:26:29,220
extension because it would be a more efficient way of handling these constraints yes is it one of the drawbacks that you have to auxiliary variables

1230
01:26:29,230 --> 01:26:43,090
translating the problem the exponential or the yeah there are customized solvers that do it

1231
01:26:43,090 --> 01:26:49,500
directly but in this general framework that's used in the by CVX for example

1232
01:26:49,540 --> 01:26:54,900
it translates everything in the  as a semi-definite program or a second-order cone program and

1233
01:26:54,900 --> 01:27:01,580
then solves this using general-purpose software but it is possible to exploit structure in these

1234
01:27:01,580 --> 01:27:12,920
problems for example and solve them much more efficiently than by CVX but then that

1235
01:27:12,920 --> 01:27:18,520
has to be developed for every type of constraint separately as a custom interior point solver

1236
01:27:18,520 --> 01:27:28,610
so to finish this section on conic optimization so one of the

1237
01:27:28,720 --> 01:27:34,020
advantages of using this conic linear optimization format is that it it's

1238
01:27:34,020 --> 01:27:41,300
very easy to generalize from linear programming so for example duality  for a conic

1239
01:27:41,300 --> 01:27:46,060
linear programming problem is almost exactly the same as for linear programming so if we just

1240
01:27:46,340 --> 01:27:50,840
start with just a standard linear programming pro duality so this is a linear program with

1241
01:27:50,840 --> 01:27:57,270
componentwise inequalities so that's the dual of this problem so you have an

1242
01:27:57,270 --> 01:28:04,260
vector Z of dual variables one for each inequality in the primal problem Z

1243
01:28:04,270 --> 01:28:10,290
has to be nonnegative and  so they have equality constraints that involve A transpose

1244
01:28:10,290 --> 01:28:17,800
and the objective and so on so that's the primal and the dual LP we'll

1245
01:28:17,800 --> 01:28:23,040
call the optimal value of this P star the optimal value of this maximization D star

1246
01:28:23,040 --> 01:28:27,900
with the usual conventions it this infeasible if there are no feasible X then we

1247
01:28:27,910 --> 01:28:33,860
say P star is plus infinity because it's a minimization problem if this is infeasible we say D star

1248
01:28:33,860 --> 01:28:39,060
is minus infinity and then we can also have minus infinity as an optimal value

1249
01:28:39,060 --> 01:28:45,370
in the primal and that means that it's unbounded below you can just decrease X in a fi

1250
01:28:45,370 --> 01:28:49,980
and stay in the feasible set you can increase X without bounds so the main

1251
01:28:50,660 --> 01:28:57,440
duality theorem in linear programming is that first the optimal value of the primal problem

1252
01:28:57,480 --> 01:29:02,860
is always greater or equal than the optimal value of the dual problem with no exception

1253
01:29:02,860 --> 01:29:08,920
that's called weak duality it's actually very easy to show but that's a one-line proof

1254
01:29:08,980 --> 01:29:15,620
usually actually or almost always have equality between these two and that's called strong duality

1255
01:29:15,620 --> 01:29:21,700
there is only one exception and that is it's if the primal is infeasible and the

1256
01:29:21,700 --> 01:29:27,420
dual is infeasible then P star is plus infinity this is minus infinity and you don't

1257
01:29:27,420 --> 01:29:32,200
have equality and that's possible you can easy write LPs that are

1258
01:29:32,200 --> 01:29:37,240
infeasible and have an infeasible dual but as soon as one of the two

1259
01:29:37,240 --> 01:29:42,920
is feasible you have equality between P star and D star so how do we extend

1260
01:29:42,920 --> 01:29:48,680
this to a conic li program where these are vector inequalities respect to a

1261
01:29:48,690 --> 01:29:54,260
nonpu polyhedral cone well first you have to define so here you see you have the same type of inequality in

1262
01:29:54,260 --> 01:30:01,060
the primal and dual  this is componentwise inequality and this is componentwise inequality in general for a

1263
01:30:01,060 --> 01:30:07,040
conic problem we'll have different inequality in the primal and the dual an inequality we'll have in

1264
01:30:07,130 --> 01:30:14,700
the dual is a inequality respect to the dual cone so if you have a cone K then you can define a

1265
01:30:14,700 --> 01:30:20,380
dual cone K star defined like this so Y is in the dual cone if it

1266
01:30:20,380 --> 01:30:25,460
makes a non-negative inner product with all X in the primal cone and you can show

1267
01:30:25,460 --> 01:30:28,210
which are packed with text

1268
01:30:28,320 --> 01:30:31,960
labels these are called taxi

1269
01:30:32,020 --> 01:30:38,870
now these taxa somehow is a collection of small documents which are textual description of

1270
01:30:38,880 --> 01:30:40,470
the image we are looking at

1271
01:30:40,480 --> 01:30:43,950
OK so if you have lots and lots of those and we have lots and

1272
01:30:43,950 --> 01:30:46,580
lots of text then we can

1273
01:30:46,600 --> 01:30:49,500
we can relate these tools

1274
01:30:49,560 --> 01:30:52,250
OK in terms of

1275
01:30:52,270 --> 01:30:55,200
in our task it is a line

1276
01:30:55,210 --> 01:30:57,470
using using this to align

1277
01:30:57,480 --> 01:31:02,280
the topics that we build for the documents and the topic that we have for

1278
01:31:02,280 --> 01:31:06,860
the images then it is the same set of topics

1279
01:31:08,730 --> 01:31:11,200
boost the performance of the models

1280
01:31:11,220 --> 01:31:18,390
OK so as a result in this experiment there was a improvement in clustering performance

1281
01:31:18,420 --> 01:31:22,280
encouraged by this success we went down to

1282
01:31:22,300 --> 01:31:26,830
ask the next question so suppose you have read a lot of

1283
01:31:26,850 --> 01:31:31,280
text documents say about lions and

1284
01:31:31,310 --> 01:31:35,020
apples and bananas can you help with the classification

1285
01:31:35,080 --> 01:31:37,970
OK and this involves supervised learning

1286
01:31:37,970 --> 01:31:41,350
so here's an example OK so let's say

1287
01:31:41,370 --> 01:31:46,170
we have a few labeled images and and they are so feel that they are

1288
01:31:46,170 --> 01:31:47,680
not sufficient

1289
01:31:47,690 --> 01:31:48,990
for building good

1290
01:31:49,010 --> 01:31:50,880
image classifiers

1291
01:31:50,900 --> 01:31:55,200
OK so that test data consists of the

1292
01:31:55,270 --> 01:31:59,350
just the labelled images

1293
01:32:00,230 --> 01:32:05,650
in addition suppose i give you the tools resources one is just like the one

1294
01:32:05,650 --> 01:32:06,800
we had just now

1295
01:32:06,840 --> 01:32:13,100
is the tag images say from flickr of from others and other resources

1296
01:32:13,190 --> 01:32:19,230
and on the other hand we have a large collection of auxiliary text documents that

1297
01:32:19,230 --> 01:32:22,970
are unlabelled OK these are unlabelled

1298
01:32:22,990 --> 01:32:24,650
so then

1299
01:32:24,680 --> 01:32:29,800
by but putting this together we can still see some relationship between them OK so

1300
01:32:29,800 --> 01:32:32,600
for example as illustrated

1301
01:32:32,600 --> 01:32:37,900
in this next image we can for the images we try to build a relation

1302
01:32:37,900 --> 01:32:44,230
between images and text and those are from the social web right and the documents

1303
01:32:44,230 --> 01:32:48,770
which are unlabelled even though there are labeled we we still have a relation between

1304
01:32:48,770 --> 01:32:50,930
documents and words

1305
01:32:50,950 --> 01:32:53,670
and on the beach because

1306
01:32:54,960 --> 01:32:59,220
matrix decomposition to build their relevant

1307
01:32:59,230 --> 01:33:01,190
latent semantic models

1308
01:33:01,210 --> 01:33:04,510
OK so for example for the images we have these models

1309
01:33:04,520 --> 01:33:07,330
image times the latent

1310
01:33:07,340 --> 01:33:14,880
clusters and and their semantic model times the text and here we have the documents

1311
01:33:15,170 --> 01:33:16,900
likewise in the same way

1312
01:33:18,450 --> 01:33:24,370
the only difficulty we have here which is a major difficulty is that these semantic

1313
01:33:24,730 --> 01:33:31,230
latent models are not aligned and we can try to impose a constraint to align

1314
01:33:31,230 --> 01:33:33,530
them to the latent factors

1315
01:33:33,630 --> 01:33:38,990
can be aligned so what the result is if we have these images which c

1316
01:33:38,990 --> 01:33:41,980
which are seemingly unrelated we don't know how

1317
01:33:42,000 --> 01:33:43,350
similar they are

1318
01:33:45,030 --> 01:33:48,430
this alignment then we can have a better

1319
01:33:48,610 --> 01:33:55,020
the relationship between them like they are all about exercising and exercising machines and so

1320
01:33:56,190 --> 01:34:02,600
so that's the intuition in the mathematical model behind this is a matrix model where

1321
01:34:03,360 --> 01:34:10,320
simultaneously composed a collection of images and the collection of documents and in the same

1322
01:34:10,320 --> 01:34:14,730
time making sure that the model v

1323
01:34:14,770 --> 01:34:16,860
is the same

1324
01:34:16,870 --> 01:34:20,690
OK the latent semantic models are the same

1325
01:34:20,700 --> 01:34:25,510
then in the end we can accomplish this so

1326
01:34:25,530 --> 01:34:30,240
so then you know because there are many unknowns because we we have two

1327
01:34:30,320 --> 01:34:35,640
you some tractable to make sure the model is convex and can converge

1328
01:34:35,760 --> 01:34:39,610
so that the details will be ommitted now

1329
01:34:39,670 --> 01:34:42,390
in the end we did a number of

1330
01:34:42,420 --> 01:34:46,830
experiments to see if our intuition conformal to

1331
01:34:46,850 --> 01:34:49,760
the experiments OK the first thing is

1332
01:34:49,770 --> 01:34:50,620
if three

1333
01:34:50,650 --> 01:34:52,590
increase the number of

1334
01:34:52,620 --> 01:34:57,980
auxiliary documents that is you know is it true is

1335
01:34:58,000 --> 01:35:03,850
the more we read the more knowledgeable we are OK so here on this horizontal

1336
01:35:03,850 --> 01:35:08,220
axis is the number of labelled documents that the system really

1337
01:35:08,260 --> 01:35:09,770
and we can see that

1338
01:35:09,800 --> 01:35:11,010
the performance

1339
01:35:11,020 --> 01:35:13,030
which is the accuracy

1340
01:35:13,060 --> 01:35:15,020
increases sharply

1341
01:35:15,050 --> 01:35:18,510
for the first four hundred documents and then

1342
01:35:18,520 --> 01:35:20,430
it flattens out after

1343
01:35:21,460 --> 01:35:23,750
OK so this tells us that

1344
01:35:23,970 --> 01:35:26,500
yes the more you read

1345
01:35:26,520 --> 01:35:27,860
this you are

1346
01:35:27,880 --> 01:35:30,200
but it's not true

1347
01:35:30,210 --> 01:35:36,730
if you continue reading OK so after we pointed you can graduate that's why we

1348
01:35:39,220 --> 01:35:40,600
and you flight out

1349
01:35:40,610 --> 01:35:42,000
and in fact

1350
01:35:42,020 --> 01:35:48,030
it drops a little bit because there's more noise that are introduced so

1351
01:35:48,050 --> 01:35:50,380
you may get a little bit more stupid

1352
01:35:50,390 --> 01:35:58,230
OK and if you introduce more tagged images again the performance

1353
01:35:59,330 --> 01:36:05,490
after a certain point that is the translator itself has to be

1354
01:36:05,510 --> 01:36:08,270
of good quality good enough quality

1355
01:36:09,530 --> 01:36:16,750
the more noise you introduced in nonrelevant tag images the performance

1356
01:36:16,760 --> 01:36:18,930
we'll we'll drop

1357
01:36:18,930 --> 01:36:23,460
OK so these are some preliminary experiments which are

1358
01:36:23,470 --> 01:36:28,830
quite encouraging at first and again here we are trying to

1359
01:36:28,850 --> 01:36:33,950
use the unlabelled text as the auxiliary data

1360
01:36:33,970 --> 01:36:40,340
we use the social web as the brain age and we try to improve image

1361
01:36:40,340 --> 01:36:44,410
classification which is a very difficult task

1362
01:36:44,430 --> 01:36:49,150
the same work and tried to introduce use

1363
01:36:49,160 --> 01:36:51,160
some recent work done by

1364
01:36:51,180 --> 01:36:52,970
my students on

1365
01:36:53,000 --> 01:36:56,260
using social recommendations themselves

1366
01:36:56,300 --> 01:36:57,020
as of

1367
01:36:57,360 --> 01:36:59,710
as the source data in transfer learning

1368
01:36:59,720 --> 01:37:06,180
OK so so that the target here are people and the relation between people and

1369
01:37:06,180 --> 01:37:11,720
people and people and items and products in recommendation systems

1370
01:37:13,330 --> 01:37:16,660
because this this workshop is about social

1371
01:37:16,660 --> 01:37:22,460
media social recommendation so i don't need to give you much introduction here

1372
01:37:22,480 --> 01:37:24,130
but we are we should

1373
01:37:24,140 --> 01:37:28,930
b all familiar with recommendation right so if you if you have a query

1374
01:37:28,950 --> 01:37:32,510
you come in and there are a number of recommendations

1375
01:37:32,780 --> 01:37:36,680
that can be given to you the familiar

1376
01:37:38,180 --> 01:37:45,480
workflow of MSR if you're right you if you buy this book these books are

1377
01:37:45,480 --> 01:37:47,710
recommended to you

1378
01:37:47,730 --> 01:37:54,720
and we all know the advantage of recommendation system is as opposed to search based

1379
01:37:54,720 --> 01:38:02,170
system its active it gives you recommendation knowing something about two it can personalise it

1380
01:38:02,170 --> 01:38:02,970
can be

1381
01:38:03,020 --> 01:38:04,890
up close

1382
01:38:06,050 --> 01:38:12,620
now you want to model social recommendation systems we can consider it as a mathematical

1383
01:38:12,620 --> 01:38:15,890
parameters and model

1384
01:38:15,910 --> 01:38:20,290
and all model selection so if we have a sequence

1385
01:38:20,290 --> 01:38:22,660
they have one and two and so on

1386
01:38:22,690 --> 01:38:25,020
and we want to figure out which one is the best

1387
01:38:25,670 --> 01:38:26,820
so we

1388
01:38:26,860 --> 01:38:28,030
to do that we

1389
01:38:29,140 --> 01:38:33,280
calculate the marginal likelihood of the data x

1390
01:38:33,290 --> 01:38:34,820
given our model

1391
01:38:34,840 --> 01:38:38,020
integrating out the prior to within the model

1392
01:38:38,030 --> 01:38:41,040
and then we can do model selection by saying that we should choose the model

1393
01:38:41,040 --> 01:38:44,850
which maximizes the marginal likelihood of the data

1394
01:38:44,860 --> 01:38:48,890
if the more bayesian there but we don't want to do model selection to do

1395
01:38:48,890 --> 01:38:52,280
model averaging in this case we could

1396
01:38:52,300 --> 01:38:54,100
in the project

1397
01:38:54,120 --> 01:38:56,710
case we could say that

1398
01:38:56,770 --> 01:39:00,080
the probability of new data has a point x power

1399
01:39:00,100 --> 01:39:01,700
given training

1400
01:39:01,720 --> 01:39:03,510
set x

1401
01:39:03,540 --> 01:39:07,010
is a sum over all possible model

1402
01:39:08,290 --> 01:39:11,570
of the probability of x given the model

1403
01:39:12,320 --> 01:39:14,370
actually does

1404
01:39:14,430 --> 01:39:16,360
x missing in the story

1405
01:39:16,440 --> 01:39:17,760
and we

1406
01:39:17,810 --> 01:39:22,010
average the the likelihood of x on the model

1407
01:39:23,260 --> 01:39:27,350
over the posterior over model

1408
01:39:27,360 --> 01:39:28,630
but it was

1409
01:39:28,680 --> 01:39:32,690
with the for with the risk of a given by this thing prior and likelihood

1410
01:39:32,690 --> 01:39:33,980
of life

1411
01:39:34,210 --> 01:39:36,780
but the problem with this

1412
01:39:37,470 --> 01:39:38,830
the problem with

1413
01:39:38,860 --> 01:39:41,290
the major question to ask is

1414
01:39:41,340 --> 01:39:43,490
this this marginal likelihood

1415
01:39:43,510 --> 01:39:45,300
OK this this happening here

1416
01:39:45,310 --> 01:39:46,260
which is the

1417
01:39:46,270 --> 01:39:49,710
the most important in terms of evaluating

1418
01:39:50,060 --> 01:39:52,430
both model selection and model averaging

1419
01:39:52,720 --> 01:39:57,120
we actually compute the marginal likelihood

1420
01:39:57,160 --> 01:39:59,960
it turns out that

1421
01:40:00,220 --> 01:40:04,420
the marginal is extremely hard to compute virtually all

1422
01:40:05,420 --> 01:40:06,700
which were interested in

1423
01:40:08,480 --> 01:40:09,650
so that

1424
01:40:09,710 --> 01:40:13,340
that makes it hard to do model selection and model averaging

1425
01:40:13,340 --> 01:40:14,850
and of course

1426
01:40:14,870 --> 01:40:18,970
both models that can model averaging very important in terms of preventing

1427
01:40:18,990 --> 01:40:23,370
overfitting underfitting of models we have to find a model of the right complexity to

1428
01:40:23,370 --> 01:40:25,030
fit to data

1429
01:40:27,180 --> 01:40:28,780
here the idea is that

1430
01:40:28,820 --> 01:40:30,190
if we actually have

1431
01:40:32,290 --> 01:40:34,360
and probably not the

1432
01:40:34,390 --> 01:40:39,770
it what well basically we put reasonable prior on over parameters and we integrate over

1433
01:40:39,770 --> 01:40:41,880
all possible parameters

1434
01:40:43,780 --> 01:40:44,890
the model

1435
01:40:44,920 --> 01:40:47,960
the method should not be overfitting the data

1436
01:40:49,060 --> 01:40:53,670
so what does that mean if model does overfit the data

1437
01:40:53,720 --> 01:40:57,070
that we should simply use as last model as possible

1438
01:40:58,470 --> 01:41:02,160
we use the really large model they have infinity

1439
01:41:03,200 --> 01:41:04,270
and then

1440
01:41:04,270 --> 01:41:06,760
so i'll let the data speak for itself

1441
01:41:07,820 --> 01:41:10,510
a couple of example

1442
01:41:10,590 --> 01:41:14,920
what happens here in the indicates that in the case of a mixture model the

1443
01:41:14,930 --> 01:41:18,050
when you are trying to model data with a mixture model

1444
01:41:18,490 --> 01:41:21,550
and you have to figure out the number of components and

1445
01:41:21,560 --> 01:41:23,420
you could actually take

1446
01:41:23,420 --> 01:41:25,460
so when i generate

1447
01:41:25,520 --> 01:41:27,400
i have to go backwards and forwards here,

1448
01:41:27,400 --> 01:41:30,690
and each time i go down here, i have to do some mean field share, or

1449
01:41:30,710 --> 01:41:34,150
in fact when you generate it properly, update these one at a the time

1450
01:41:34,250 --> 01:41:36,460
then go back up there

1451
01:41:36,500 --> 01:41:39,840
then come back down, update these one at a time, do that for a long time

1452
01:41:39,840 --> 01:41:41,690
and then get sample here

1453
01:41:41,710 --> 01:41:44,190
then given that sample there

1454
01:41:44,190 --> 01:41:49,730
go down here, update these one at a time, then go

1455
01:41:50,500 --> 01:41:52,820
don't go back up. update these one at a time,

1456
01:41:53,570 --> 01:41:55,650
do that many times

1457
01:41:55,690 --> 01:41:57,190
and then given that sample there

1458
01:41:57,190 --> 01:41:58,190
then i can,

1459
01:41:58,230 --> 01:42:02,170
you don't actually, you only have to update these once, because they're not laterally connected

1460
01:42:03,380 --> 01:42:11,070
lateral connections are gaussian units, so it's harder to learn using contrastive divergence.

1461
01:42:11,070 --> 01:42:15,880
so it's just a bad reason

1462
01:42:15,980 --> 01:42:21,380
OK so that's how he generates

1463
01:42:21,440 --> 01:42:23,900
but when he does inference in this model

1464
01:42:23,960 --> 01:42:25,800
these are just part of the

1465
01:42:25,840 --> 01:42:27,070
generative model, so to do inference,

1466
01:42:27,090 --> 01:42:29,570
you just go chunk, chunk, chunk

1467
01:42:29,610 --> 01:42:33,530
inference is just feedforward and doesn't use lateral interactions

1468
01:42:33,550 --> 01:42:39,380
and that's very surprising to vision people

1469
01:42:39,380 --> 01:42:42,340
so these lateral connections are a markov random field

1470
01:42:42,380 --> 01:42:47,980
it's used during learning, it's used for generation, but is not part of the inference procedure

1471
01:42:48,020 --> 01:42:50,820
and vision people hate that because they assume that if you have an MRF, you better

1472
01:42:50,820 --> 01:42:52,320
use it for inference

1473
01:42:52,360 --> 01:42:56,000
but because of the way we learn this

1474
01:42:56,110 --> 01:42:58,960
the MRF is enforcing constraints

1475
01:42:59,000 --> 01:43:00,860
these smoothness constraints

1476
01:43:00,860 --> 01:43:06,070
and you don't need those smoothness constraints during inference because the data enforces them.

1477
01:43:06,130 --> 01:43:09,980
you only need them during

1478
01:43:12,000 --> 01:43:14,050
you need them during learning

1479
01:43:20,270 --> 01:43:22,880
you have to run the lateral interactions as you're learning

1480
01:43:22,920 --> 01:43:27,000
but you have very fast inference at the end

1481
01:43:27,020 --> 01:43:30,190
and you think about it, when we whiten data, the reason we do it is so

1482
01:43:30,980 --> 01:43:33,690
you don't learn

1483
01:43:33,710 --> 01:43:36,020
higher level models

1484
01:43:39,090 --> 01:43:43,360
that use all the resources of these hidden units to model pairwise correlations. if you

1485
01:43:43,360 --> 01:43:46,880
get rid of the pairwise correlations then you can use the resources in the hidden units to model higher

1486
01:43:46,880 --> 01:43:49,420
order correlations which are more interesting

1487
01:43:49,500 --> 01:43:54,750
so we remove the second order statistics

1488
01:43:54,790 --> 01:43:58,030
but then when you get to the hidden layer, you've got binary units and it's

1489
01:43:58,030 --> 01:44:02,940
much harder to see how you would whiten those

1490
01:44:02,980 --> 01:44:06,900
so what we're doing in effect is widening the learning signal

1491
01:44:06,960 --> 01:44:10,940
when you learn these lateral interactions as part of the generative model,

1492
01:44:11,000 --> 01:44:16,550
what you're doing is saying if i can correctly predict a pixel using lateral interactions,

1493
01:44:16,670 --> 01:44:19,590
then it'll be the same in the reconstruction of the data

1494
01:44:20,480 --> 01:44:23,610
the hidden units won't need to try to model anything because there'll be no discrepancy

1495
01:44:23,610 --> 01:44:24,800
between the

1496
01:44:24,880 --> 01:44:26,460
data and reconstruction.

1497
01:44:26,460 --> 01:44:30,210
for pixels that i can't model using the latter interactions,

1498
01:44:30,230 --> 01:44:33,550
i'll need the hidden units, and that'll be exactly edges

1499
01:44:33,670 --> 01:44:36,670
so the hidden units will get devoted to modeling edges

1500
01:44:36,670 --> 01:44:41,150
rather than being devoted to modeling the fact that images are smooth

1501
01:44:41,210 --> 01:44:43,860
so you get much better features out of it.

1502
01:44:43,900 --> 01:44:46,000
and that's one reason why this thing learns

1503
01:44:46,020 --> 01:44:49,960
to generate much better images

1504
01:44:56,130 --> 01:44:59,380
now i want to talk about probably the most complicated thing i'm going to talk

1505
01:45:01,610 --> 01:45:05,650
which is how to make a better module than a restricted boltzmann machine

1506
01:45:05,690 --> 01:45:09,690
how to go further than just put--putting lateral interactions between visibles

1507
01:45:11,650 --> 01:45:13,550
and it's going to allow us to

1508
01:45:13,570 --> 01:45:17,290
get multiplicative interactions which you couldn't get so

1509
01:45:17,340 --> 01:45:18,820
so essentially

1510
01:45:18,820 --> 01:45:25,030
you should think about

1511
01:45:25,110 --> 01:45:28,500
in the belief net part of the model

1512
01:45:28,530 --> 01:45:31,230
the sort of standard model we've had so far

1513
01:45:31,230 --> 01:45:33,820
has top-down effects like that

1514
01:45:33,880 --> 01:45:34,750
and then

1515
01:45:34,770 --> 01:45:37,050
learning these lateral interactions

1516
01:45:37,050 --> 01:45:39,820
in the generative model you also get effects like that

1517
01:45:39,840 --> 01:45:41,670
now what i want to put in

1518
01:45:41,860 --> 01:45:44,500
top-down effects like this

1519
01:45:44,520 --> 01:45:46,820
where the state of the hidden unit

1520
01:45:46,860 --> 01:45:50,880
can determine the lateral interactions between two visible units

1521
01:45:50,920 --> 01:45:51,960
and that's much more power--

1522
01:45:51,980 --> 01:45:56,230
much more powerful. that's like when the officer tells the soldiers, you know, how far

1523
01:45:56,230 --> 01:45:57,730
apart they should be

1524
01:45:57,750 --> 01:46:03,840
and modules like this can learn

1525
01:46:03,900 --> 01:46:04,790
better things

1526
01:46:04,800 --> 01:46:08,730
well they should be able to

1527
01:46:08,770 --> 01:46:13,340
so the standard energy function we're using has pairwise interactions like this

1528
01:46:13,340 --> 01:46:19,270
let's write down an energy function with triplewise interactions

1529
01:46:20,290 --> 01:46:24,670
you can think of this as if, if sk is on,

1530
01:46:24,710 --> 01:46:27,630
then we have a pairwise interaction between i and j.

1531
01:46:27,650 --> 01:46:31,880
if sk is off, that pairwise interaction just disappeared

1532
01:46:31,920 --> 01:46:39,530
for those of you who know about vision, this is like the game in-game line process

1533
01:46:39,550 --> 01:46:42,820
but you can view--because the symmetry--you can view i is acting as switch for whether

1534
01:46:42,820 --> 01:46:49,340
j and k interact too

1535
01:46:49,400 --> 01:46:54,790
so you can use things like that for example for extracting transformations between images

1536
01:46:54,840 --> 01:46:56,710
so here's an image at time t

1537
01:46:56,710 --> 01:46:59,530
here's an image at time t plus one

1538
01:46:59,550 --> 01:47:01,960
here's some hidden units

1539
01:47:01,960 --> 01:47:05,090
this represents three way interactions

1540
01:47:05,090 --> 01:47:07,090
so this hidden unit is saying

1541
01:47:07,130 --> 01:47:09,130
if i'm on

1542
01:47:09,130 --> 01:47:11,590
and this pixel is on

1543
01:47:11,590 --> 01:47:13,570
if i've got a big positive weight here

1544
01:47:13,570 --> 01:47:15,670
it says if i'm on, and this pixel is on

1545
01:47:15,730 --> 01:47:18,800
then it's a good idea to turn this pixel on.

1546
01:47:18,840 --> 01:47:21,790
it's also a good idea if this pixel's on

1547
01:47:21,860 --> 01:47:23,840
to turn this pixel on

1548
01:47:23,940 --> 01:47:28,710
so he represents the translation from there to there

1549
01:47:28,730 --> 01:47:30,210
and this hidden unit,

1550
01:47:30,230 --> 01:47:34,630
presumably shouldn't come on at the same time, represents a different translation

1551
01:47:35,480 --> 01:47:38,610
a translation if you think about it is

1552
01:47:38,630 --> 01:47:41,460
a correlation between pixels here and pixels here

1553
01:47:41,480 --> 01:47:46,460
so these units are now specifying not activities of units but correlations between units

1554
01:47:46,520 --> 01:47:49,090
patterns of correlation

1555
01:47:49,130 --> 01:47:51,230
and if you apply this to pairs of images,

1556
01:47:51,230 --> 01:47:54,570
it works really nicely, and these learn to be

1557
01:47:54,610 --> 01:47:58,380
to represent trans, transformations between images

1558
01:47:58,420 --> 01:48:01,980
like if you, if they translate these learn different translations. if they rotate, they learn

1559
01:48:01,980 --> 01:48:05,750
to represent different rotations and so on

1560
01:48:05,770 --> 01:48:07,210
but there's a problem

1561
01:48:07,250 --> 01:48:08,770
and the problem is

1562
01:48:08,770 --> 01:48:11,320
if you write down this three-way energy function

1563
01:48:11,380 --> 01:48:13,610
there's three indices here

1564
01:48:13,650 --> 01:48:16,250
and that means if we have a lot of pixels

1565
01:48:16,290 --> 01:48:20,530
in the first image in pixels and a lot of pixels in the second image and a lot of

1566
01:48:20,550 --> 01:48:21,940
elements of trans-

1567
01:48:21,960 --> 01:48:24,840
formation, then we got a lot of this

1568
01:48:24,900 --> 01:48:27,840
cubically many of these

1569
01:48:27,840 --> 01:48:30,980
so what we're going to do is we're going to factor that

1570
01:48:31,000 --> 01:48:32,300
we're gonna say

1571
01:48:32,360 --> 01:48:35,800
i'm going to try and represent

1572
01:48:35,820 --> 01:48:38,320
what this energy function was doing

1573
01:48:38,400 --> 01:48:41,000
by having a whole bunch of factors

1574
01:48:41,050 --> 01:48:42,840
this is just like doing PCA

1575
01:48:42,880 --> 01:48:45,050
but for tensors instead of for matrices

1576
01:48:45,070 --> 01:48:46,730
a whole bunch of factors

1577
01:48:46,890 --> 01:48:49,170
and each factor

1578
01:48:49,170 --> 01:48:51,550
so as i have these three guys

1579
01:48:51,690 --> 01:48:56,800
but now this weight is factored into the product of these three pairwise weights

1580
01:48:56,820 --> 01:49:00,790
and there's only two indices there, so there's three

1581
01:49:00,790 --> 01:49:04,280
it is in the first two weeks ramp everybody up

1582
01:49:05,540 --> 01:49:09,790
some level of software security where you feel comfortable

1583
01:49:11,330 --> 01:49:11,780
so they

1584
01:49:12,350 --> 01:49:14,620
the first two weeks of this course is intended

1585
01:49:15,060 --> 01:49:19,760
to make you comfortable with programming we don't assume you've done extensive programming before

1586
01:49:22,810 --> 01:49:26,060
we want you to become comfortable that you're not behind

1587
01:49:27,190 --> 01:49:30,310
and that's the focus on the first two weeks exhibit the exercise

1588
01:49:32,050 --> 01:49:39,180
if you have little or no previous background if you are uncomfortable please do the pipeline tudor exercises

1589
01:49:41,100 --> 01:49:42,200
if you have not

1590
01:49:42,910 --> 01:49:43,190
if you

1591
01:49:44,120 --> 01:49:47,130
do not have a lot of experience programming if you're uncomfortable

1592
01:49:47,550 --> 01:49:52,300
with with the expectation that you can do programming do first

1593
01:49:54,260 --> 01:49:58,130
back takes priority over all the other assignments during the first two weeks

1594
01:50:00,340 --> 01:50:02,590
in particular if you're uncomfortable

1595
01:50:03,550 --> 01:50:06,370
we will run a special pipeline help session on sunday

1596
01:50:08,610 --> 01:50:10,100
and if you talking back

1597
01:50:11,160 --> 01:50:13,370
you can get a free extension

1598
01:50:16,200 --> 01:50:21,970
the idea is be completing the tudor exercises intended to make you feel comfortable that

1599
01:50:21,970 --> 01:50:25,170
you have a software background to finish the rest of the course

1600
01:50:26,990 --> 01:50:27,960
that's first

1601
01:50:28,680 --> 01:50:29,700
we will forgive

1602
01:50:30,100 --> 01:50:31,920
falling behind in other things

1603
01:50:32,810 --> 01:50:33,700
so that's

1604
01:50:33,830 --> 01:50:35,490
you feel comfortable with programming

1605
01:50:37,350 --> 01:50:41,470
if at the end of two weeks you still feel uncomfortable

1606
01:50:42,690 --> 01:50:44,960
we have a deal with six hundred

1607
01:50:46,030 --> 01:50:47,600
the python programming class

1608
01:50:48,690 --> 01:50:52,560
that they will allow you to switch your registration from six o one the six hundred

1609
01:50:54,440 --> 01:50:55,560
the ban expires

1610
01:50:56,250 --> 01:50:57,050
valentine's day

1611
01:50:59,870 --> 01:51:04,040
so you have to make up your mind before valentine's day if you'd like to use the adoption

1612
01:51:05,600 --> 01:51:06,740
the idea is

1613
01:51:07,180 --> 01:51:11,610
we'd like you to be comfortable with programming if you have a program befor do

1614
01:51:11,610 --> 01:51:14,200
the python tutori exercises good software lab

1615
01:51:14,970 --> 01:51:17,650
good design but work on the two directors actors

1616
01:51:18,420 --> 01:51:19,640
the staff will help you with long

1617
01:51:21,310 --> 01:51:23,890
you can go of office hours office hours listed

1618
01:51:24,290 --> 01:51:25,510
in on the home page

1619
01:51:26,380 --> 01:51:30,750
you should try to become comfortable and you should try that set as you going

1620
01:51:30,750 --> 01:51:32,540
to be comfortable before valentine's day

1621
01:51:33,520 --> 01:51:35,350
and if you are not talk to stack

1622
01:51:35,910 --> 01:51:37,020
a staff member of back

1623
01:51:41,420 --> 01:51:43,450
so what i want you know operating

1624
01:51:44,870 --> 01:51:46,340
well we're gonna use pipeline

1625
01:51:47,310 --> 01:51:48,520
we selected pi on

1626
01:51:49,310 --> 01:51:50,660
because it's very simple

1627
01:51:53,060 --> 01:51:53,930
and because

1628
01:51:54,410 --> 01:52:01,770
it lets us illustrate some very important ideas in software engineering in a very simple context

1629
01:52:03,180 --> 01:52:03,920
that's the reason

1630
01:52:05,090 --> 01:52:07,290
one of the reasons that simple

1631
01:52:08,540 --> 01:52:09,860
is that it's an interpreter

1632
01:52:12,690 --> 01:52:14,080
after some initial isation

1633
01:52:15,800 --> 01:52:18,940
the behavior of pipeline is the fall into an interpreter

1634
01:52:19,990 --> 01:52:21,260
interpreter loopy is

1635
01:52:21,960 --> 01:52:24,840
ask the user what what he would like me to do all

1636
01:52:28,180 --> 01:52:30,040
reap what they user types

1637
01:52:31,390 --> 01:52:34,550
figure out what they are talking about the print the result repeat

1638
01:52:36,450 --> 01:52:38,160
very simple what that means is

1639
01:52:38,850 --> 01:52:40,680
that's you can learn by doing

1640
01:52:43,130 --> 01:52:45,680
that's one pointsof today's software lab

1641
01:52:46,730 --> 01:52:49,940
you can simply walk up to a computer typed the word pipeline

1642
01:52:51,950 --> 01:52:53,210
what you type is in red

1643
01:52:54,710 --> 01:52:55,760
take the word pipeline

1644
01:52:56,800 --> 01:52:58,170
it will prompt all

1645
01:52:58,880 --> 01:53:00,000
so this chevron

1646
01:53:00,680 --> 01:53:01,310
that's axes

1647
01:53:01,790 --> 01:53:03,910
i'd like you to tell me something to do i have nothing to do

1648
01:53:05,590 --> 01:53:07,220
if you'd type tools

1649
01:53:07,980 --> 01:53:10,460
i found tries to interpret than in this particular case

1650
01:53:11,040 --> 01:53:12,290
by found says oh i see

1651
01:53:13,050 --> 01:53:14,330
that's a primitive data item

1652
01:53:14,870 --> 01:53:15,620
that's an integer

1653
01:53:16,310 --> 01:53:18,370
this person wants me to understand an integer

1654
01:53:18,860 --> 01:53:20,090
and so it will echo

1655
01:53:22,330 --> 01:53:26,850
indicating that it things you've when you wanted to understand a simple integer

1656
01:53:29,240 --> 01:53:33,700
similarly if you type five point seven it says oh i got back that's afloat

1657
01:53:35,380 --> 01:53:37,840
the person wants me to remember a floating point number

1658
01:53:39,140 --> 01:53:41,100
it will similarly echoed the flow

1659
01:53:43,340 --> 01:53:45,040
now course floats don't have any

1660
01:53:46,760 --> 01:53:48,990
so there is no exact representation for floats

1661
01:53:48,990 --> 01:53:51,460
the solution

1662
01:53:51,490 --> 01:53:55,070
OK thanks very much indeed i like to thank the the organisers for inviting me

1663
01:53:55,070 --> 01:53:59,040
to this music it's been extremely stimulating i've certainly learnt a lot and i'm sure

1664
01:53:59,640 --> 01:54:01,650
what i will then

1665
01:54:01,660 --> 01:54:03,310
make a difference to

1666
01:54:03,310 --> 01:54:05,880
the work of myself my team and i go back home i hope that we

1667
01:54:05,880 --> 01:54:09,140
can have various collaborations

1668
01:54:10,500 --> 01:54:16,340
what i want to do to begin with is just introduce the

1669
01:54:16,360 --> 01:54:19,070
thing because anything i say which is original today

1670
01:54:19,070 --> 01:54:23,160
it's probably done by members of my research group rather than myself

1671
01:54:23,490 --> 01:54:25,380
as you can see from this

1672
01:54:25,380 --> 01:54:32,900
i've got quite large research groups they work on on a number of different project

1673
01:54:32,950 --> 01:54:39,300
i'm talking about fraud detection today and the people in red are the primary and

1674
01:54:39,310 --> 01:54:42,030
you certainly see some of their work today

1675
01:54:42,050 --> 01:54:44,590
in fact during the course of my talk i'm going to give one or two

1676
01:54:44,590 --> 01:54:49,590
examples of the sorts of work we've been doing but fairly superficial level and the

1677
01:54:49,590 --> 01:54:53,880
talk immediately following line given by dave weston is going to go into one of

1678
01:54:53,910 --> 01:54:57,990
the projects in considerable detail so give you a better idea of the sorts of

1679
01:54:57,990 --> 01:54:59,700
things we've been doing

1680
01:54:59,710 --> 01:55:04,700
other people in the group work on on different things so for example

1681
01:55:04,710 --> 01:55:09,570
the the people in light grey had amount of heating in sand dunes

1682
01:55:09,620 --> 01:55:16,100
daniel as if work on particular aspects of the retail banking industry which i do

1683
01:55:16,100 --> 01:55:17,540
quite a lot of work in

1684
01:55:17,660 --> 01:55:22,180
others in red in green for example work on

1685
01:55:22,200 --> 01:55:26,400
royal theoretical problems of multivariate streaming data

1686
01:55:26,410 --> 01:55:30,630
with missing values and other kinds of distortions which can obviously apply to fraud can

1687
01:55:30,630 --> 01:55:33,960
also apply and a lot of other areas

1688
01:55:34,010 --> 01:55:41,180
so a lot of people working broadly speaking in the same direction run overlapping projects

1689
01:55:41,200 --> 01:55:44,120
so this is the structure of my talk on going to give some

1690
01:55:44,130 --> 01:55:45,710
general background

1691
01:55:45,730 --> 01:55:49,450
i'm going to sort of quantify the problem so that we can see why it's

1692
01:55:49,450 --> 01:55:54,040
important what's interesting the then i'm going to focus down on fraud in banking because

1693
01:55:54,040 --> 01:55:55,130
that's my

1694
01:55:55,240 --> 01:55:59,520
the banking the bank here is one of my particular research interests and i'm going

1695
01:55:59,520 --> 01:56:02,740
to look at fraud within that area

1696
01:56:02,750 --> 01:56:06,710
if i have time which i may well not too

1697
01:56:06,770 --> 01:56:09,110
quickly glance fraud in science

1698
01:56:09,120 --> 01:56:11,210
because this is

1699
01:56:11,270 --> 01:56:16,460
the issues there are the different often they also involve involve large and complex datasets

1700
01:56:16,500 --> 01:56:20,280
but the motivation and and other issues are rather different so we see how it

1701
01:56:20,280 --> 01:56:22,870
goes and then one of the some conclusions

1702
01:56:22,890 --> 01:56:28,890
and given the title of my talk which was statistical techniques for fraud detection and

1703
01:56:28,890 --> 01:56:29,770
so on

1704
01:56:29,780 --> 01:56:31,650
i would just like to

1705
01:56:31,670 --> 01:56:36,030
define what i mean by statistics and i've adopted john chambers definition is

1706
01:56:36,050 --> 01:56:40,670
everything related to learning from data so very broad definition

1707
01:56:40,710 --> 01:56:44,270
we can argue about definitions if you like it's probably pointless that's what i mean

1708
01:56:44,270 --> 01:56:49,120
so i'm going to include if you like machine learning techniques support vector machines whatever

1709
01:56:49,120 --> 01:56:53,330
anything related to data analysis if

1710
01:56:53,340 --> 01:56:55,620
so let's begin with some background

1711
01:56:55,670 --> 01:56:58,050
first definition of fraud

1712
01:56:58,050 --> 01:57:03,370
here's one from an important dictionary criminal deception the use of false representations to gain

1713
01:57:03,400 --> 01:57:06,030
an unjust advantage

1714
01:57:06,050 --> 01:57:07,840
fraud is

1715
01:57:07,880 --> 01:57:11,990
from that definition older as old as you like you can trace back was only

1716
01:57:12,020 --> 01:57:17,060
like and you could even argue that older than humanity and certainly animals perpetrate what

1717
01:57:17,060 --> 01:57:22,990
one might regard as fraud fraud in other contexts through camouflage and deception and so

1718
01:57:22,990 --> 01:57:25,340
on so it's all always light

1719
01:57:25,360 --> 01:57:26,050
it's not

1720
01:57:26,050 --> 01:57:29,860
it's not just the sin of humanity if you like

1721
01:57:29,900 --> 01:57:31,240
four because

1722
01:57:31,250 --> 01:57:38,580
everywhere basically i briefly referred to the semantic motivation isn't always the same in

1723
01:57:38,620 --> 01:57:41,740
many context perhaps most context money

1724
01:57:41,780 --> 01:57:46,150
or perhaps power is is is is the motivating

1725
01:57:46,560 --> 01:57:51,370
it is the motive so this is the case in banking fraud why what people

1726
01:57:51,720 --> 01:57:55,890
the banking fraud because they want to get rich basically and the same applies to

1727
01:57:56,180 --> 01:58:02,220
most other areas like telecoms fraud insurance healthcare fraud click fraud or whatever

1728
01:58:02,270 --> 01:58:06,810
in some other areas however there is a rather different motivation and as i say

1729
01:58:06,810 --> 01:58:10,480
if i have a chance to come to the city and so in scientific fraud

1730
01:58:10,480 --> 01:58:11,890
it's pierre garden

1731
01:58:11,910 --> 01:58:16,990
which is the motivating generally motivating influence rather than hoping to get rid if you

1732
01:58:17,240 --> 01:58:19,040
a rich and you're in science

1733
01:58:19,050 --> 01:58:21,630
i think you might say

1734
01:58:21,640 --> 01:58:23,690
in in terrorism

1735
01:58:23,820 --> 01:58:27,930
a lot of fraud is associated with terrorism in the short term gain areas two

1736
01:58:27,940 --> 01:58:33,080
get money to fund terrorism but in some sense the longer the bigger aim of

1737
01:58:33,080 --> 01:58:36,550
terrorists is some sort of higher obviously put that in

1738
01:58:36,550 --> 01:58:40,870
inverted commas in speech marks is some sort of higher motivation the primary aim is

1739
01:58:40,870 --> 01:58:44,120
isn't money it's some other ideological sort of

1740
01:58:46,510 --> 01:58:48,410
a couple of comments about

1741
01:58:48,450 --> 01:58:51,380
social aspects of fraud management

1742
01:58:51,410 --> 01:58:55,690
one of the difficulties in this area is that there is often an unwillingness to

1743
01:58:55,740 --> 01:58:59,100
i admit to being defrauded and i was at the conference not long ago

1744
01:58:59,110 --> 01:59:02,870
when a banker said no we have no fraud in our bank

1745
01:59:03,120 --> 01:59:06,380
sure he doesn't it just doesn't want to admit it because if he

1746
01:59:06,410 --> 01:59:10,680
if if a story appears in the papers that his bank lost a hundred million

1747
01:59:10,690 --> 01:59:14,660
o dollars last year to fraud it's not going to do is banking shares much

1748
01:59:16,430 --> 01:59:20,730
and then there are other issues such as the public beliefs

1749
01:59:20,740 --> 01:59:23,690
the bank has a very good system to detect fraud

1750
01:59:23,740 --> 01:59:28,120
it's great stopping force this targeting that whether or not it really does have such

1751
01:59:28,120 --> 01:59:29,160
a system so the

1752
01:59:29,570 --> 01:59:31,550
subtle social issues

1753
01:59:31,600 --> 01:59:33,010
and of course is also

1754
01:59:33,100 --> 01:59:39,050
a tremendous reporting bias associated with these things in general

1755
01:59:39,330 --> 01:59:43,700
a few words about something i call economic imperative which has got several aspects the

1756
01:59:43,700 --> 01:59:47,280
first is that it's not worth spending a huge sum of money to start a

