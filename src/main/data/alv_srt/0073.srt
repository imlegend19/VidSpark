1
00:00:00,000 --> 00:00:01,940
probability bound

2
00:00:02,000 --> 00:00:04,830
which means that the grant will be

3
00:00:04,850 --> 00:00:06,670
bonded by

4
00:00:06,670 --> 00:00:10,100
something like this with high probability

5
00:00:10,130 --> 00:00:12,150
which is important

6
00:00:12,170 --> 00:00:14,850
in practice because

7
00:00:15,670 --> 00:00:18,480
you want something that's robust

8
00:00:18,480 --> 00:00:23,770
right you want to grow up to be bounded with high probability versus in expectation

9
00:00:23,790 --> 00:00:28,830
so this original algorithm does not give a high probability bond

10
00:00:28,850 --> 00:00:31,440
but you can modify it

11
00:00:31,500 --> 00:00:34,960
to get one

12
00:00:34,960 --> 00:00:42,830
and all the techniques were already present in that paper that they come manually

13
00:00:42,830 --> 00:00:44,130
so here is the

14
00:00:44,150 --> 00:00:46,040
summary so far

15
00:00:47,270 --> 00:00:49,250
in the supervised in the

16
00:00:49,270 --> 00:00:50,630
full feedback

17
00:00:50,790 --> 00:00:53,440
you'll unsupervised setting

18
00:00:53,480 --> 00:00:58,730
you get full feedback in the the neighborhood of every action

19
00:00:58,770 --> 00:01:00,100
at training time

20
00:01:00,110 --> 00:01:01,350
and the degree that

21
00:01:01,420 --> 00:01:03,440
scales as c

22
00:01:03,480 --> 00:01:04,900
log and

23
00:01:04,940 --> 00:01:09,460
use and standard sample complexity machinery

24
00:01:09,500 --> 00:01:16,750
it's usually not phrased this way because you're talking about this sort of regret per

25
00:01:16,750 --> 00:01:20,960
sample but you can sum over so the standard

26
00:01:21,000 --> 00:01:22,500
way of

27
00:01:22,690 --> 00:01:26,900
sample complexity bounds give logan over c

28
00:01:26,920 --> 00:01:29,980
but when the sum over two rounds that the

29
00:01:30,000 --> 00:01:32,810
to make it comparable to the other brands

30
00:01:32,850 --> 00:01:36,270
you get something that scales is t

31
00:01:36,290 --> 00:01:39,850
and yes it sufficient

32
00:01:39,870 --> 00:01:43,650
so this is the

33
00:01:43,690 --> 00:01:49,440
where you explore first and then exploit the leader on the exploration around

34
00:01:49,460 --> 00:01:53,310
it works in the stochastic setting

35
00:01:53,310 --> 00:01:56,690
was partial feedback

36
00:01:56,730 --> 00:01:59,290
they were the

37
00:01:59,310 --> 00:02:02,900
very good at scale says t to the two thirds

38
00:02:02,900 --> 00:02:05,170
which is worse than routine

39
00:02:05,210 --> 00:02:08,880
and yes it is sufficient

40
00:02:08,880 --> 00:02:13,790
in the number of policies that you're competing with there's no

41
00:02:16,310 --> 00:02:21,900
there is no intrinsic the dependence on the number of policies

42
00:02:21,920 --> 00:02:24,710
because you can use for example

43
00:02:24,730 --> 00:02:30,580
any of learning algorithm to choose the best on the exploration around

44
00:02:30,650 --> 00:02:33,440
you can just be that in into

45
00:02:33,480 --> 00:02:36,580
empirical risk minimization oracle to give you

46
00:02:36,600 --> 00:02:38,440
an approximant

47
00:02:39,540 --> 00:02:43,330
approximately best policy and EXP four

48
00:02:43,380 --> 00:02:49,250
works in the adversarial setting with partial feedback gives you this optimal regret but it's

49
00:02:49,250 --> 00:02:54,080
not efficient because we had to keep a weight on every policy and we had

50
00:02:54,080 --> 00:02:58,600
to update those weights so if you're competing with a large set

51
00:02:58,670 --> 00:03:00,960
this is not efficient

52
00:03:00,980 --> 00:03:03,330
so ideally we would like

53
00:03:03,370 --> 00:03:10,960
an algorithm that is like EXP four but didn't have to explicitly than the weights

54
00:03:10,980 --> 00:03:12,730
on every policy

55
00:03:12,780 --> 00:03:14,710
and this is one of the

56
00:03:14,710 --> 00:03:19,830
important problems in this area to get something

57
00:03:19,850 --> 00:03:26,630
that for example depends on an empirical risk minimizer nor home

58
00:03:31,230 --> 00:03:33,730
so let's switch gears and

59
00:03:34,580 --> 00:03:40,420
so the so that this is our unbiased estimate of the word

60
00:03:40,420 --> 00:03:42,440
right so

61
00:03:42,440 --> 00:03:46,560
you want the argmax over policies in pi

62
00:03:46,610 --> 00:03:48,270
of this as t me

63
00:03:48,270 --> 00:03:50,020
which sounds

64
00:03:50,040 --> 00:03:56,920
or some sort examples where the policy agrees with the action taken of the reward

65
00:03:56,920 --> 00:04:00,580
of action divided by the probability of taking that action

66
00:04:00,600 --> 00:04:02,880
and so how can we

67
00:04:02,900 --> 00:04:05,370
compute that quantity for

68
00:04:05,380 --> 00:04:08,130
reasonable policy classes by

69
00:04:08,190 --> 00:04:13,810
and this is a hard problem in general but we can try to reuse existing

70
00:04:13,880 --> 00:04:20,500
supervised learning algorithms to give you a practical solutions to these problems so if you

71
00:04:20,500 --> 00:04:26,830
encounter such a problem in practice can you use existing supervised learning algorithms to give

72
00:04:26,830 --> 00:04:28,830
you a solution

73
00:04:28,830 --> 00:04:39,300
scores as prime

74
00:04:39,330 --> 00:04:42,430
r b and b is

75
00:04:44,430 --> 00:04:46,880
notation but the simply giving

76
00:04:46,920 --> 00:04:48,200
a previous state

77
00:04:48,210 --> 00:04:49,760
previous action

78
00:04:49,770 --> 00:04:52,160
distribution over possible next

79
00:04:52,200 --> 00:04:55,440
OK here is is continuous if a continuous

80
00:04:55,450 --> 00:05:02,820
probability density function in the discrete is just a big table of probabilities

81
00:05:02,860 --> 00:05:05,380
OK reward function

82
00:05:05,390 --> 00:05:09,910
so imagine that i wanted to paralyze the number of car

83
00:05:11,820 --> 00:05:17,640
it assumed that makes is interchanged light every second

84
00:05:19,200 --> 00:05:20,600
or maybe a bit more

85
00:05:20,600 --> 00:05:23,000
OK let let let's say every minute

86
00:05:23,580 --> 00:05:28,300
and i want to minimize the number of car minutes

87
00:05:28,360 --> 00:05:32,060
car minutes waiting time

88
00:05:32,110 --> 00:05:34,170
OK well what reward function b

89
00:05:34,260 --> 00:05:36,340
and again

90
00:05:36,380 --> 00:05:40,190
so the reward function is reward i get for being the current state

91
00:05:40,210 --> 00:05:49,220
and gamma tells me how it read off reward time

92
00:06:02,620 --> 00:06:04,360
so some over

93
00:06:04,410 --> 00:06:08,490
well get so now

94
00:06:09,950 --> 00:06:12,800
but for for being in estate one time

95
00:06:12,800 --> 00:06:17,130
i i i just some parity and then i can some all those penalties over

96
00:06:17,130 --> 00:06:19,640
all times to get the sum of the waiting times

97
00:06:19,650 --> 00:06:23,700
so i can sum up my local reward

98
00:06:23,720 --> 00:06:26,430
reward as a function of the state

99
00:06:26,450 --> 00:06:30,820
in this case a it doesn't depend on the actions just state

100
00:06:30,820 --> 00:06:35,660
it is the sum over i corps one to four

101
00:06:35,740 --> 00:06:38,320
of case of IQ being

102
00:06:38,360 --> 00:06:40,470
q being the real value

103
00:06:40,490 --> 00:06:43,590
rising how long was

104
00:06:43,590 --> 00:06:45,900
and negative

105
00:06:45,950 --> 00:06:47,400
they get paralyzed

106
00:06:47,440 --> 00:06:53,080
right i saw something i want to maximize reward that paralyzed for every car swinging

107
00:06:54,310 --> 00:06:56,560
this is the single steps

108
00:06:57,430 --> 00:06:59,500
and then what i take

109
00:06:59,510 --> 00:07:01,140
reward at time

110
00:07:01,140 --> 00:07:02,550
zero plus

111
00:07:02,570 --> 00:07:08,650
gamma tend toward time one plus scalar square to reward time

112
00:07:08,700 --> 00:07:10,720
times reward time two

113
00:07:10,730 --> 00:07:12,870
out to say infinity

114
00:07:12,910 --> 00:07:15,150
so let's it arises infinity

115
00:07:15,200 --> 00:07:16,990
h equals infinity

116
00:07:18,000 --> 00:07:22,670
if i really care about car second waiting time irrespective of whether occurring

117
00:07:22,680 --> 00:07:27,170
the weight is now in the future that one gamma to be

118
00:07:29,450 --> 00:07:33,660
does not prize-winning features we now i want to wait exactly the same

119
00:07:35,650 --> 00:07:39,110
the chemical one

120
00:07:39,170 --> 00:07:41,570
there were one is probably not

121
00:07:41,640 --> 00:07:42,960
for various reasons

122
00:07:43,100 --> 00:07:46,140
but but it is just the model

123
00:07:46,140 --> 00:07:50,000
it's first fire model you solve it feels horribly this happen to me

124
00:07:50,030 --> 00:07:54,960
i had income student who did a great job in place for model we really

125
00:07:55,140 --> 00:07:56,910
realize the end

126
00:07:56,930 --> 00:07:58,310
the model was wrong

127
00:07:58,350 --> 00:08:02,520
back to the drawing board OK so you want to model the right level of

128
00:08:02,520 --> 00:08:05,170
abstraction and you can always categories first

129
00:08:05,180 --> 00:08:07,150
the first right

130
00:08:07,380 --> 00:08:09,230
but this is one possible model

131
00:08:09,280 --> 00:08:12,600
state actions observations observation function

132
00:08:12,600 --> 00:08:16,220
not a dispute over states given the observations

133
00:08:16,280 --> 00:08:21,720
transition function this over next is given season actions so for example

134
00:08:21,760 --> 00:08:26,610
what is one stochastic aspect of the transition function

135
00:08:26,620 --> 00:08:31,610
we know that

136
00:08:35,000 --> 00:08:40,280
imagine there are ten cars

137
00:08:40,320 --> 00:08:41,640
ten cars

138
00:08:41,650 --> 00:08:43,710
in two three

139
00:08:43,730 --> 00:08:46,270
now let's let's look at the next

140
00:08:50,560 --> 00:08:52,570
that's in their zero cars here

141
00:08:52,570 --> 00:08:55,050
zero cars here

142
00:08:55,090 --> 00:08:57,960
there are ten cars here

143
00:08:58,170 --> 00:09:04,280
ten cursor

144
00:09:04,310 --> 00:09:07,140
embarrassing to zero

145
00:09:07,150 --> 00:09:10,160
so the only two with with cars waiting is q one

146
00:09:10,210 --> 00:09:12,210
going up

147
00:09:13,260 --> 00:09:17,200
now after one time step where this light is green and the traffic flowing in

148
00:09:17,200 --> 00:09:18,430
this direction

149
00:09:18,480 --> 00:09:20,360
the next time step how many

150
00:09:21,950 --> 00:09:24,750
should be two three

151
00:09:24,760 --> 00:09:30,720
they are exactly exactly they they could turn right

152
00:09:31,450 --> 00:09:34,330
so even if i know that are if i let them do if i find

153
00:09:34,330 --> 00:09:39,730
like label long enough cars the start-up of times that that stochastic cannot predict that

154
00:09:39,810 --> 00:09:44,130
but it if i let the slide stay green for five minutes

155
00:09:44,250 --> 00:09:47,780
guaranteed rate only ten cars but you don't know

156
00:09:47,800 --> 00:09:52,130
that all ten cars went straight so you don't you can nokia three that if

157
00:09:52,130 --> 00:09:57,060
this light is read during this time because some cars could have gone

158
00:09:58,210 --> 00:10:01,720
so you need a transition model that says the probability of turning

159
00:10:01,730 --> 00:10:03,990
i dislike this this time of day

160
00:10:04,000 --> 00:10:05,400
o point four

161
00:10:05,440 --> 00:10:08,890
so you can estimate the expectation how many cars go we try

162
00:10:09,270 --> 00:10:11,910
but this is all the transition function

163
00:10:11,910 --> 00:10:16,170
this distribution over next states based on the current state and actions

164
00:10:16,230 --> 00:10:19,970
i've got these cars in these cues i've got these lights green

165
00:10:19,990 --> 00:10:21,850
you distribution over next

166
00:10:21,850 --> 00:10:25,030
state qx

167
00:10:25,630 --> 00:10:30,000
any questions at all about this if you would not understand a high-level what's going

168
00:10:30,000 --> 00:10:32,890
on here is going to make the rest of the what to called

169
00:10:32,910 --> 00:10:36,750
and if you have this are saying this great because then i realize were misleading

170
00:10:38,920 --> 00:10:49,330
yes it does make competition is it just the geometric discounting is is cut it

171
00:10:49,330 --> 00:10:53,560
is very easy especially in real in receding horizon control you to so it is

172
00:10:53,560 --> 00:10:55,160
going to be to by gamma

173
00:10:55,170 --> 00:10:57,810
and then in how discount everything

174
00:10:57,820 --> 00:11:00,690
right but it also has this justification of

175
00:11:00,690 --> 00:11:04,830
you impose this condition for all T

176
00:11:04,900 --> 00:11:06,310
for all T

177
00:11:06,330 --> 00:11:12,570
In this case infinitely many T you get an intersection of many of those halfspaces

178
00:11:12,610 --> 00:11:17,180
so for each T you have two parallel halfspaces and then if you change T

179
00:11:17,760 --> 00:11:21,330
the halfspaces rotate and it describes a non polyhydral convex set

180
00:11:21,730 --> 00:11:27,470
so it's not polyhydral because you need infinitely many inequalities to describe it but it's a

181
00:11:27,560 --> 00:11:29,420
clearly convex

182
00:11:29,560 --> 00:11:35,180
so that's all we'll need about to know about convex sets a convex function is defined

183
00:11:35,180 --> 00:11:38,680
like this so a function is convex if its domain is convex

184
00:11:38,690 --> 00:11:44,350
the sets of points where it's defined as convex and on its domain Jensen's inequality holds so

185
00:11:45,380 --> 00:11:49,810
Jensen's inequality so graphically it means that if you have two points on the graph of

186
00:11:49,810 --> 00:11:50,830
the function

187
00:11:51,280 --> 00:11:57,110
the value F of X F of Y then the linear the segment defined by those two points on the graph

188
00:11:57,140 --> 00:12:04,180
lies above the graph of the function that's a convex function and if it's concave if minus F is

189
00:12:05,610 --> 00:12:08,320
so other some simple examples

190
00:12:08,800 --> 00:12:14,040
so obviously linear or affine functions are convex and also concave

191
00:12:14,440 --> 00:12:20,800
exponential the negative of a logarithm negative entropy X log X are convex

192
00:12:21,930 --> 00:12:24,950
certain powers are convex for example if X is

193
00:12:25,020 --> 00:12:31,230
positive if that's the domain of F then X to the alpha is convex if alpha is greater than one

194
00:12:32,180 --> 00:12:34,090
or negative

195
00:12:34,160 --> 00:12:35,170
et cetera

196
00:12:35,280 --> 00:12:38,330
any norm is convex

197
00:12:38,380 --> 00:12:43,800
this is a function that's not immediately clear but it's also convex if you have a quadratic function N transpose X

198
00:12:43,900 --> 00:12:51,730
devided by T as a function of X and T jointly where T is positive

199
00:12:51,780 --> 00:12:58,280
that's a convex function of X and T a geometric mean of an a non-negative

200
00:12:58,330 --> 00:13:01,450
variables is concave

201
00:13:03,020 --> 00:13:08,020
log that of the log of the determinant of a positive definite matrix is concave

202
00:13:08,060 --> 00:13:13,040
so it's concave on the set of positive definite matrices so that function will be important

203
00:13:13,060 --> 00:13:18,330
when talk about interior point methods and in the last one we'll also encounter is

204
00:13:18,540 --> 00:13:23,000
the log of the sum of exponentials of variables is convex

205
00:13:23,450 --> 00:13:28,640
of course there's some of exponentials by itself is convex because exponentials are convex but even if you take the log

206
00:13:29,380 --> 00:13:32,920
or apply the logarithm to it you still have a convex function

207
00:13:32,950 --> 00:13:40,730
so there is also a simple connection between convex functions and Convex sets so

208
00:13:41,450 --> 00:13:46,070
convex a function is convex if its epigraph is convex

209
00:13:46,110 --> 00:13:47,260
convex it's a convex set

210
00:13:47,300 --> 00:13:49,410
so the epigraph of the function is simply

211
00:13:49,560 --> 00:13:53,900
the set defined off as everything that lies above the graph of the function the

212
00:13:53,900 --> 00:13:55,760
graph itself and everything above

213
00:13:55,900 --> 00:14:01,780
so if X F is a function of N variables its epigraph is a set in R N plus one

214
00:14:01,810 --> 00:14:02,970
cause T is

215
00:14:03,020 --> 00:14:05,090
extra variable

216
00:14:05,140 --> 00:14:09,540
and the so this is if and only if a function is convex if and

217
00:14:09,540 --> 00:14:13,190
only if its epigraph is a convex set

218
00:14:13,230 --> 00:14:17,590
you can also consider a sub level sets of a function convex function so it's a

219
00:14:17,590 --> 00:14:20,940
set of points X in the domain of F that has a function have a function

220
00:14:20,970 --> 00:14:25,090
value lies on a certain alpha

221
00:14:25,140 --> 00:14:28,420
now sublevel set of a convex function is always convex

222
00:14:28,450 --> 00:14:33,640
and that follows again directly from the definition but the converse is not true there are functions that are

223
00:14:33,650 --> 00:14:35,850
not convex and still have

224
00:14:36,110 --> 00:14:41,190
convex sublevel sets for all values of alpha

225
00:14:42,210 --> 00:14:48,830
so convex functions were defined in general using Jensen's inequality so that don't have

226
00:14:48,830 --> 00:14:53,800
to be differentiable if they are also differentiable once or twice differentiable than you

227
00:14:53,800 --> 00:14:54,330
can also

228
00:14:54,880 --> 00:15:00,680
give equivalent conditions that can replace Jensen's inequality in the definition for

229
00:15:00,680 --> 00:15:02,680
example if

230
00:15:02,780 --> 00:15:05,640
the function is differentiable then

231
00:15:05,680 --> 00:15:09,060
an equivalent condition is that the function value

232
00:15:09,070 --> 00:15:10,760
Of the graph

233
00:15:10,760 --> 00:15:13,480
function satisfies this inequality

234
00:15:13,700 --> 00:15:19,300
so on the right-hand side you see the linear approximation of the function around the point X

235
00:15:19,330 --> 00:15:22,000
so the first order linear approximation of

236
00:15:22,130 --> 00:15:25,800
a function from its function value and its gradient

237
00:15:25,800 --> 00:15:32,220
well if the is convex than that linear approximation must be below the graph of the function

238
00:15:32,220 --> 00:15:34,850
for all possible points where you take the approximation

239
00:15:34,980 --> 00:15:40,390
so this first order linear approximation is not only a local approximation of the function

240
00:15:40,440 --> 00:15:42,300
but it's also a global

241
00:15:42,300 --> 00:15:46,370
lower band on the function value so it's a very fundamental

242
00:15:47,760 --> 00:15:53,070
and if of course you also have second if the function is twice differentiable then you can express convexity

243
00:15:53,070 --> 00:15:56,850
also simply in terms of the Hessian and say that the Hessian matrix of the

244
00:15:56,850 --> 00:16:01,350
function must be positive semidefinite everywhere in its domain

245
00:16:01,700 --> 00:16:08,200
so those are the definitions so the Jensen's inequality and these two equivalent definitions for differnetiable

246
00:16:08,440 --> 00:16:10,150
or twice differentiable functions

247
00:16:10,150 --> 00:16:15,370
in practice one first time process

248
00:16:17,990 --> 00:16:18,960
OK so

249
00:16:18,970 --> 00:16:21,680
example why this is

250
00:16:21,690 --> 00:16:22,640
good for

251
00:16:22,650 --> 00:16:29,140
part two part one is that this inheriting statistical strength effect which was you an

252
00:16:29,140 --> 00:16:34,300
example later on again the second example is this idea of generating

253
00:16:34,300 --> 00:16:39,420
a class of clustering in sort of estimating the number of clusters automatically

254
00:16:39,450 --> 00:16:46,130
so one example is if p of x y given simply because the distribution of

255
00:16:46,130 --> 00:16:49,020
theta i corresponds to the center and the covariance

256
00:16:49,030 --> 00:16:51,660
of course distribution

257
00:16:51,690 --> 00:16:56,160
so let's look at the graphical representation of the same thing as before only that

258
00:16:56,500 --> 00:17:03,880
we only have always one data point for every sample said OK that's only one

259
00:17:04,680 --> 00:17:06,770
and this is the cost distribution

260
00:17:06,840 --> 00:17:08,820
so your for x

261
00:17:09,050 --> 00:17:12,770
it's got students at the centers the the covariances

262
00:17:12,820 --> 00:17:16,430
of course and

263
00:17:16,850 --> 00:17:21,700
in the in so we can do the CRP type something and then we observe

264
00:17:21,700 --> 00:17:23,050
that all the

265
00:17:23,090 --> 00:17:28,110
data points which are assigned to the same table all inherit the same parameter

266
00:17:28,120 --> 00:17:33,700
we can think now that all this data manually generated by an identical thousand distribution

267
00:17:33,700 --> 00:17:36,880
and so we can think of all this data from one class to

268
00:17:36,890 --> 00:17:40,090
with the parameters sort associated with the

269
00:17:40,090 --> 00:17:44,020
elston derived from this is from these data

270
00:17:44,350 --> 00:17:49,330
and so do the sampling we use this data points would jump around a little

271
00:17:49,330 --> 00:17:53,190
bit between the different colours since according to the probability is but if you look

272
00:17:53,190 --> 00:17:58,390
at the incident time there will be a certain number of tables will occupied

273
00:17:58,420 --> 00:18:01,130
and this number will also some fluctuations

274
00:18:01,140 --> 00:18:04,380
go up or down a little bit but on average it gives you a good

275
00:18:04,380 --> 00:18:09,370
indication about something like the true number of clusters in your problem

276
00:18:09,380 --> 00:18:15,080
and this is one of the attractive features here that the sampling procedure automatically gives

277
00:18:15,080 --> 00:18:18,620
you a good estimate of the number of classes in your domain

278
00:18:18,620 --> 00:18:23,200
which of course is sort of depending on the often not change so you can

279
00:18:23,200 --> 00:18:28,690
sort of do a rough tuning by the astronauts you think my domain there's probably

280
00:18:28,690 --> 00:18:32,850
have a large number of clusters of a small number of clusters this can be

281
00:18:32,850 --> 00:18:36,660
tuned by the infinite not but given we fix that there still some freedom in

282
00:18:36,660 --> 00:18:41,080
the model to decide exactly on the sort of correct number of clusters and there

283
00:18:41,080 --> 00:18:42,200
are two ways to

284
00:18:42,250 --> 00:18:47,550
estimate is i fine not and then this is a complete automatic process of automatically

285
00:18:47,550 --> 00:18:52,870
automatically determining so something like the right number of clusters and other methods have a

286
00:18:52,870 --> 00:18:57,410
much harder time of doing that to it's sort of trial and error procedures so

287
00:18:57,410 --> 00:19:01,210
this is one reason why these do sleep with someone

288
00:19:01,220 --> 00:19:04,580
that's what happened

289
00:19:04,590 --> 00:19:07,530
and to illustrate this point

290
00:19:07,580 --> 00:19:13,150
look at this picture so these are

291
00:19:13,210 --> 00:19:16,650
the gaussians which are associated with different

292
00:19:16,670 --> 00:19:20,400
OK with the chinese restaurant in this case we would have

293
00:19:20,410 --> 00:19:25,900
six tables being occupied and these are the data

294
00:19:25,910 --> 00:19:28,990
so this would be a snapshot of the same process this might be the data

295
00:19:28,990 --> 00:19:33,590
associated with this with the stable over here these are the data sources which is

296
00:19:33,990 --> 00:19:38,930
stable over here and these data probably can jump between these tables for that

297
00:19:38,940 --> 00:19:40,820
and based on the data

298
00:19:40,820 --> 00:19:44,370
the associated with a given table we would estimate the skulls

299
00:19:44,390 --> 00:19:48,450
OK surely we don't do it all the time but the officially template this would

300
00:19:48,450 --> 00:19:53,050
be the palestinians would be estimated by the data points listed in the table

301
00:19:53,060 --> 00:20:01,950
so the data points are identical to the customs in this case because there's only

302
00:20:01,950 --> 00:20:04,530
one data point for each station

303
00:20:07,490 --> 00:20:11,840
so this is one of the attractive things about this if we persist it can

304
00:20:11,840 --> 00:20:15,590
do clustering and automatically determine the number of

305
00:20:17,580 --> 00:20:20,380
we can now

306
00:20:20,420 --> 00:20:24,000
come to the last part of the physical section before we come to the station

307
00:20:24,170 --> 00:20:25,850
and there's also

308
00:20:26,380 --> 00:20:28,160
direct relationship to

309
00:20:28,170 --> 00:20:30,050
finite mixture models

310
00:20:31,490 --> 00:20:32,880
i mean

311
00:20:32,900 --> 00:20:37,330
by the related to all of this but in the infinite case we can even

312
00:20:37,330 --> 00:20:38,430
make them

313
00:20:38,660 --> 00:20:41,720
the identical so if you chose

314
00:20:41,730 --> 00:20:48,610
a finite mixture model and you choose as your distribution for the indicator variables usually

315
00:20:48,610 --> 00:20:51,670
process for not divided by our

316
00:20:51,680 --> 00:20:53,870
let r go to infinity

317
00:20:53,870 --> 00:20:56,140
it's actually processed

318
00:20:56,280 --> 00:20:59,030
the last thing is when you got these processes going on

319
00:20:59,050 --> 00:21:01,310
you've got to keep track

320
00:21:01,330 --> 00:21:07,410
and we monitor everything the throughput systems to the temperature CPU use something like three

321
00:21:07,410 --> 00:21:11,970
hundred fifty thousand metrics that are updated every fifteen seconds and we have a lot

322
00:21:11,970 --> 00:21:16,780
of analysis focused on just interpreting these metrics so that we can understand trends plan

323
00:21:16,780 --> 00:21:22,890
accordingly the course systems can also recover ultimately so people focusing on building models don't

324
00:21:22,890 --> 00:21:26,890
have to worry about these things

325
00:21:29,220 --> 00:21:32,850
modelling pipelines is really what is really where the magic happens this is this is

326
00:21:32,850 --> 00:21:38,180
the stuff that we started the company with this with this in mind as a

327
00:21:38,180 --> 00:21:42,450
bunch of things that have been done in terms of the audience inference like traffic

328
00:21:42,450 --> 00:21:47,470
modelling and look like not to touch upon the environment we've created how we develop

329
00:21:47,470 --> 00:21:50,600
models and rapidly get those into production

330
00:21:50,600 --> 00:21:52,660
first of all really important

331
00:21:52,700 --> 00:21:58,970
sort of ten of math team environment is to combine the best of collaboration we

332
00:21:58,970 --> 00:22:04,570
have very collaborative environment we regularly brainstorms as an entire group say it's not a

333
00:22:04,570 --> 00:22:07,350
large group a whole companies about one hundred people

334
00:22:07,470 --> 00:22:12,260
so this so the team and don't know maybe this maybe this twenty scientists also

335
00:22:12,280 --> 00:22:18,370
they regularly get together and brainstorm new approaches we review everyone else's work in a

336
00:22:18,370 --> 00:22:22,490
group every week everyone presents the progress and how their models performing in the new

337
00:22:22,490 --> 00:22:27,280
ideas we share all of our information should work environment and everyone has team goals

338
00:22:28,080 --> 00:22:32,160
but also we're really trying to encourage independence everyone is free

339
00:22:32,180 --> 00:22:33,910
to implement their own ideas

340
00:22:33,930 --> 00:22:38,680
they're free to create new models to improve models to create better metrics for assessing

341
00:22:38,680 --> 00:22:42,920
models also develop new visualization techniques to help them interpret how well the models of

342
00:22:44,620 --> 00:22:48,910
and to do that is really important we have an objective way of assessing progress

343
00:22:48,910 --> 00:22:53,450
in making decisions about you know when is the model better and therefore when can

344
00:22:53,450 --> 00:22:56,450
we stop using the model out in production

345
00:22:56,490 --> 00:23:00,180
we do this you're look-alike pipeline sandbox so

346
00:23:01,140 --> 00:23:02,310
your typical day

347
00:23:02,330 --> 00:23:05,990
there's about ten terrabytes of new data coming in from the internet

348
00:23:06,060 --> 00:23:10,060
and for a given market there might be ten thousand to twenty thousand or maybe

349
00:23:10,060 --> 00:23:12,180
just a few hundred people

350
00:23:12,200 --> 00:23:16,310
that have actually done something of interest to that market let's say that before given

351
00:23:17,330 --> 00:23:19,330
well those converters

352
00:23:19,350 --> 00:23:25,930
actually fit within our training sandbox training set will contain any number of different models

353
00:23:25,950 --> 00:23:29,640
the different groups different scientists are actually work

354
00:23:29,930 --> 00:23:34,930
those models are assessed against typically about half petabytes of of

355
00:23:34,930 --> 00:23:39,350
data so we have a fixed data we can use the training and assessing these

356
00:23:40,350 --> 00:23:44,890
and the sandbox allows us to make rapid comparisons between

357
00:23:44,890 --> 00:23:46,830
models in these text datasets

358
00:23:46,910 --> 00:23:52,140
basically everyone is constantly working on new challenges we want to develop new models to

359
00:23:52,140 --> 00:23:56,450
replace our current here are the current heroes the model is actually out there making

360
00:23:56,470 --> 00:24:00,870
real time decisions about you know hundreds of thousands of requests per second and the

361
00:24:00,870 --> 00:24:04,830
objective everyone in our in look-alikes teams to build model

362
00:24:04,850 --> 00:24:06,350
the can be the hero

363
00:24:06,370 --> 00:24:11,330
so challenges to the production here are developed in the sandbox and once we have

364
00:24:11,330 --> 00:24:15,240
lots of we have lots of data so much data that obviously would be very

365
00:24:15,240 --> 00:24:18,850
careful not to be overfitting one advantage having so much data is you always get

366
00:24:18,870 --> 00:24:22,680
fresh data set and test it out terms generalizations

367
00:24:23,150 --> 00:24:25,280
once the model appears to be

368
00:24:25,280 --> 00:24:29,790
good and relevant it will actually be pushed into production that will go into production

369
00:24:29,790 --> 00:24:34,260
and just take over what actually be run in parallel with existing here

370
00:24:34,280 --> 00:24:38,450
to look for improvements assessment the way these models work is what to developed features

371
00:24:38,490 --> 00:24:43,260
you have you you have your model look across the billion internet users and find

372
00:24:43,260 --> 00:24:47,760
the next set maybe the next ten million maybe the next one million maybe next

373
00:24:47,760 --> 00:24:52,010
twenty million the number depend very much on the objectives of the market and those

374
00:24:52,010 --> 00:24:55,310
with the people who would consider that content that is relevant

375
00:24:56,330 --> 00:24:57,660
assessing model

376
00:24:57,700 --> 00:25:00,050
is really looking at the response we get

377
00:25:01,160 --> 00:25:04,100
our current hero to existing challenge

378
00:25:04,100 --> 00:25:05,620
and we can see here

379
00:25:05,790 --> 00:25:07,470
we have significantly

380
00:25:07,470 --> 00:25:12,600
in this challenge over the here that we're really interested of course the cumulative left

381
00:25:12,680 --> 00:25:16,260
and can be a little difficult to see here one of the things to bear

382
00:25:16,260 --> 00:25:17,970
in mind is

383
00:25:17,990 --> 00:25:19,430
the most products

384
00:25:19,450 --> 00:25:21,970
that is not relevant to most consumers

385
00:25:22,030 --> 00:25:25,830
we're not looking for model gives his level down because there just aren't that many

386
00:25:25,830 --> 00:25:29,760
people are going to be interested in the product and extreme case maybe half the

387
00:25:29,760 --> 00:25:33,330
population will be interested in the product certainly not all of them and in many

388
00:25:33,330 --> 00:25:35,780
cases a tiny fraction of the population

389
00:25:35,780 --> 00:25:37,470
so that difference there

390
00:25:37,470 --> 00:25:41,740
in terms of left left here is proportional to how well does this model compare

391
00:25:41,760 --> 00:25:43,580
two randomly selected people

392
00:25:43,600 --> 00:25:48,100
so the top and here are previous model was about twenty five times better than

393
00:25:48,100 --> 00:25:52,330
random the new model is over sixty times better and that new model can find

394
00:25:52,330 --> 00:25:54,140
its way into production quickly

395
00:25:54,160 --> 00:25:58,720
and that's really important not just getting involved into production but also the rate at

396
00:26:00,180 --> 00:26:05,870
people can develop new models and test them out because ultimately

397
00:26:05,890 --> 00:26:09,120
the building a start-up businesses about learning

398
00:26:09,140 --> 00:26:15,060
it's about discoveries about finding the right way to address market and your learning is

399
00:26:15,060 --> 00:26:20,140
directly proportional to rate of experimentation more experiments you can do the more you can

400
00:26:20,140 --> 00:26:23,700
learn faster going to get to where you want to say so this environment is

401
00:26:23,700 --> 00:26:26,510
so imagine that you have

402
00:26:26,670 --> 00:26:28,680
you have a restaurant with

403
00:26:28,690 --> 00:26:32,140
the really big restaurant with infinite number of tables in the restaurant

404
00:26:32,300 --> 00:26:37,370
and the first customer comes in at the first table

405
00:26:37,470 --> 00:26:40,440
so i mean draw this

406
00:26:40,450 --> 00:26:43,420
so you have restaurant

407
00:26:43,440 --> 00:26:45,700
and you would are

408
00:26:45,770 --> 00:26:49,180
and the first half of comes in and the table

409
00:26:49,460 --> 00:26:50,720
then the

410
00:26:50,730 --> 00:26:52,590
the second comes

411
00:26:52,610 --> 00:26:55,560
and decide between two choices

412
00:26:55,580 --> 00:26:56,890
the second summer

413
00:26:56,940 --> 00:27:02,510
chooses to sit at this table with probability proportional to the customer already sitting there

414
00:27:02,510 --> 00:27:04,740
in that case the probability proportional to one

415
00:27:05,310 --> 00:27:10,510
while with probability proportional to to alpha cuts might sit at a new table

416
00:27:10,520 --> 00:27:13,820
let's say that the second last year

417
00:27:14,140 --> 00:27:17,280
label on

418
00:27:18,400 --> 00:27:20,320
and the customer comes in

419
00:27:20,420 --> 00:27:25,050
with probability proportional to the number of customers sitting at the table

420
00:27:25,060 --> 00:27:27,580
the the customers city

421
00:27:27,590 --> 00:27:28,930
and was

422
00:27:29,560 --> 00:27:34,810
well with probability proportional to alpha the first in this is yet another new people

423
00:27:34,860 --> 00:27:37,000
let's say that the

424
00:27:39,830 --> 00:27:42,250
so we can repeat this in general

425
00:27:42,260 --> 00:27:43,860
and customer when

426
00:27:43,910 --> 00:27:46,090
when the customer comes in

427
00:27:46,100 --> 00:27:50,360
he was sitting at table k with probability and he

428
00:27:50,380 --> 00:27:55,910
offshoot and where n is the number of revisiting it will

429
00:27:55,960 --> 00:27:59,660
and with probability proportional to alpha this

430
00:27:59,690 --> 00:28:02,350
as customer will sit

431
00:28:02,410 --> 00:28:03,780
a new table

432
00:28:03,930 --> 00:28:07,800
so here the customers correspond to integer

433
00:28:07,800 --> 00:28:09,910
and the tables correspond to clusters

434
00:28:12,990 --> 00:28:17,050
basically what it states that the chinese restaurant process xt but

435
00:28:17,060 --> 00:28:21,920
that's being property of the dirichlet process

436
00:28:22,390 --> 00:28:25,050
we can get

437
00:28:25,060 --> 00:28:27,900
from a chinese restaurant process back work

438
00:28:28,020 --> 00:28:31,030
blackwell macqueen urn scheme in the following way

439
00:28:31,040 --> 00:28:33,200
so we first from our

440
00:28:33,220 --> 00:28:38,410
random partitions of the integers one to and from the chinese restaurant process

441
00:28:38,430 --> 00:28:39,910
and then

442
00:28:39,950 --> 00:28:42,400
on each table we will draw

443
00:28:43,660 --> 00:28:49,640
it's a bit on table k will draw it back from a base distribution

444
00:28:50,940 --> 00:28:52,510
then for

445
00:28:52,530 --> 00:28:55,340
that we simply by the i

446
00:28:55,350 --> 00:28:59,090
b they are that i was that i was the table

447
00:28:59,110 --> 00:29:01,320
o which customize

448
00:29:01,370 --> 00:29:05,960
so this construct a sequence of the

449
00:29:05,970 --> 00:29:08,850
which turns out to be equivalent to

450
00:29:08,850 --> 00:29:11,350
the blackwell macqueen urn scheme

451
00:29:11,350 --> 00:29:15,280
and this is actually quite useful because what it says is that the chinese restaurant

452
00:29:15,280 --> 00:29:16,890
process is applied

453
00:29:16,900 --> 00:29:19,300
so there's two things happening here

454
00:29:19,760 --> 00:29:21,150
in the dirichlet process

455
00:29:21,170 --> 00:29:22,370
one is

456
00:29:22,450 --> 00:29:25,400
the clustering property of the dirichlet process which

457
00:29:25,860 --> 00:29:28,320
now cluster value

458
00:29:28,500 --> 00:29:29,770
together it

459
00:29:29,990 --> 00:29:33,180
that's the random variables together

460
00:29:33,200 --> 00:29:36,790
and the second one is safe seats that for each cluster

461
00:29:36,830 --> 00:29:39,650
the value is simply from the base distribution

462
00:29:39,710 --> 00:29:44,610
and what the chinese restaurant process this is the idea to act in

463
00:29:44,610 --> 00:29:48,150
in the first part

464
00:29:48,560 --> 00:29:52,010
the last representation is to begin construction

465
00:29:52,020 --> 00:29:57,700
i will i think i'll skip the revision

466
00:29:57,720 --> 00:30:01,120
but basically what happens is

467
00:30:01,160 --> 00:30:03,110
the following

468
00:30:03,110 --> 00:30:05,430
what happens is that it shows that

469
00:30:07,010 --> 00:30:09,550
are drawn from a dirichlet process

470
00:30:09,590 --> 00:30:12,390
it's simply an infinite number of functions

471
00:30:15,280 --> 00:30:17,540
and the highest

472
00:30:17,660 --> 00:30:20,700
on the point from high is simply

473
00:30:20,790 --> 00:30:26,710
but that it and the other from one minus one one minus if the i

474
00:30:26,780 --> 00:30:28,690
where each base

475
00:30:28,750 --> 00:30:31,110
it it that is drawn independently

476
00:30:31,190 --> 00:30:32,830
from it that one of

477
00:30:34,370 --> 00:30:37,080
so this is called the rate fraction

478
00:30:37,110 --> 00:30:37,870
he has

479
00:30:37,950 --> 00:30:40,490
we can imagine what

480
00:30:40,550 --> 00:30:45,240
so what's happening here in the following we start with the big of length one

481
00:30:45,420 --> 00:30:49,820
and we break this because the point here

482
00:30:49,870 --> 00:30:54,530
so that's going to be all i one

483
00:30:54,530 --> 00:30:56,180
we take the the rest of

484
00:30:56,240 --> 00:30:59,340
and we break at some point again

485
00:30:59,340 --> 00:31:05,550
to get up . 6 that starts to veer off and then asymptotically approach

486
00:31:08,930 --> 00:31:14,730
errf z equals 1 so in fact we can write here from the from 1

487
00:31:14,730 --> 00:31:19,490
equals 0 . 8 4 you can see now it's not linear

488
00:31:19,560 --> 00:31:21,430
and refers to

489
00:31:21,500 --> 00:31:25,860
people 0 . 9 9 5 so what you get out here to your practically

490
00:31:25,860 --> 00:31:34,470
at the asymptotic and it's defined here's the integral is defined from z is the

491
00:31:34,480 --> 00:31:36,640
integral from z

492
00:31:38,240 --> 00:31:43,450
from 0 . 3 0 2 C of E to the minus the square the

493
00:31:44,340 --> 00:31:49,320
so that's that's that's e to the minus you squared e to the minus you

494
00:31:49,320 --> 00:31:53,510
squared as a function of you is the Gaussian error function right that's the bell

495
00:31:53,510 --> 00:31:57,490
curve so this is the area under the bell curve we started x that u

496
00:31:57,490 --> 00:32:02,970
equals 0 we're going to integrate as far as we need to out here and

497
00:32:02,970 --> 00:32:08,620
the integral from 0 to infinity of this thing is rude pi over 2 and

498
00:32:08,620 --> 00:32:14,030
we want to go from 0 to 1 so we put the factor to over

499
00:32:14,300 --> 00:32:19,330
pilots from that when we integrate this from 0 to infinity birth of infinity becomes

500
00:32:19,570 --> 00:32:24,810
1 so er friends from 0 that's the area under this curve so that's why

501
00:32:24,810 --> 00:32:30,310
some people call this the galaxy error function because the gaussian curve is associated with

502
00:32:30,310 --> 00:32:33,730
random statistics in terms of of errors

503
00:32:33,920 --> 00:32:38,080
so this now is the time place this is the template if I have in

504
00:32:38,080 --> 00:32:47,770
any reaction this is all I have to fit using these using these multiplication factors

505
00:32:47,790 --> 00:32:52,920
so let's play Mr. dress up here just use this function to describe all of

506
00:32:52,920 --> 00:33:00,950
the typical of industrial processes only 2 types of processes those that diffusing substance it

507
00:33:01,270 --> 00:33:04,750
and then those that pulling substance out so let's look at the 2 cases so

508
00:33:04,750 --> 00:33:09,400
let's look 1st of all at the 1 that's shown right up there so this

509
00:33:09,400 --> 00:33:18,660
would be for something like outgassing outgassing for drawing many uh many processes involved drying

510
00:33:18,660 --> 00:33:23,740
which is to get the water out of the the content of a of a

511
00:33:23,740 --> 00:33:30,550
solid piece and so as you can imagine you've got some initial concentration seen

512
00:33:30,690 --> 00:33:35,910
you take the surface concentration whatever this is an obviously if you want to draw

513
00:33:35,980 --> 00:33:41,450
something out and move matter from right to left in the surface concentration by necessity

514
00:33:41,450 --> 00:33:45,730
must be less than the initial concentration otherwise why something

515
00:33:45,860 --> 00:33:51,790
so this means this all call the fusion something is you think so this is

516
00:33:51,790 --> 00:33:58,510
a fusion the flux is moving from right to left on the determining here is

517
00:33:58,510 --> 00:34:04,160
seen on is greater than CS the surface concentration is less than the initial concentration

518
00:34:04,440 --> 00:34:11,200
so I'm appear the concentration at C I've got C 0 deep inside and this

519
00:34:11,200 --> 00:34:13,160
is telling me what the shape of the curve

520
00:34:13,990 --> 00:34:17,450
this is telling me what the shape of the curve I just write with impunity

521
00:34:17,460 --> 00:34:25,930
C minus overseen minus C F equals the error function x over 2 routes of

522
00:34:27,510 --> 00:34:32,450
and that's the answer for all such problems and because it's a linear doesn't matter

523
00:34:32,450 --> 00:34:37,990
if I change the in initial concentration to new value I can just add

524
00:34:38,010 --> 00:34:42,810
we can add and multiply because linear equation when functionality but I think you're we've

525
00:34:42,810 --> 00:34:47,680
got some are point we looked at those This is the tabulation this is in

526
00:34:47,680 --> 00:34:53,120
the supplemental tax there's a table of an error function values so that if you

527
00:34:53,120 --> 00:34:56,600
happen to get into this regime beyond that

528
00:34:56,700 --> 00:35:02,230
z equals 0 . 6 shows you have the the values that you need

529
00:35:02,400 --> 00:35:08,080
so let's look at another case let's look at the driving material so that for

530
00:35:08,080 --> 00:35:12,680
example something like on adult

531
00:35:12,750 --> 00:35:20,270
building or tonight writing the opening-night trying carburizing

532
00:35:20,690 --> 00:35:23,950
but if we want case-harden

533
00:35:24,030 --> 00:35:30,390
material will raise the carbon content of the free surface have something that's often ductile

534
00:35:30,390 --> 00:35:35,950
inside alright so for that here's the here's the scheme

535
00:35:36,470 --> 00:35:41,470
there's scheme we have a surface concentrations C

536
00:35:41,530 --> 00:35:43,640
we have an initial concentration

537
00:35:44,290 --> 00:35:52,470
getting crowded with cleanest of surface concentration initial concentration seen on and in this case

538
00:35:52,490 --> 00:35:59,580
the not is less than CS surface concentrations higher so material now wants to move

539
00:35:59,710 --> 00:36:06,030
from left to right and so this is infusion infusion

540
00:36:06,550 --> 00:36:09,870
and what's the kernel look like books like this

541
00:36:10,010 --> 00:36:13,870
looks like this which is simply that 1

542
00:36:14,030 --> 00:36:18,330
flipped upside down by subtracting it from from 1 which I'll show you in a

543
00:36:18,330 --> 00:36:23,330
2nd and again the same general solution applies the same general solution applies with the

544
00:36:23,330 --> 00:36:30,070
sea surface concentration is higher or lower than the initial ball concentration it comes out

545
00:36:30,070 --> 00:36:35,470
in the math here so again we're write see . S overseeing on minus C

546
00:36:35,470 --> 00:36:40,120
S equals x over 2 routes of DT

547
00:36:40,120 --> 00:36:42,280
if we look at to find class

548
00:36:43,740 --> 00:36:45,640
if you want to capital and

549
00:36:45,710 --> 00:36:51,820
then by using n times capital in times of the inequality we can prove that

550
00:36:51,860 --> 00:36:57,390
all functions simultaneously i mean what a large set of training samples

551
00:36:58,170 --> 00:37:01,830
ninety nine percent of the examples for all functions similar to be

552
00:37:01,880 --> 00:37:04,870
the difference will be bounded by this point

553
00:37:04,880 --> 00:37:07,940
and they said we can then apply these two

554
00:37:08,010 --> 00:37:09,020
the empirical

555
00:37:09,030 --> 00:37:11,120
loss minimisation because it will be

556
00:37:11,130 --> 00:37:12,670
part of this

557
00:37:12,740 --> 00:37:15,200
oceans here

558
00:37:15,750 --> 00:37:18,710
we get the following statement if we

559
00:37:18,720 --> 00:37:20,200
use also the

560
00:37:20,750 --> 00:37:23,520
fundamental inequalities that allowed us to really is

561
00:37:24,890 --> 00:37:25,840
the actual

562
00:37:25,850 --> 00:37:29,700
actually these quantities of the supremum already of these on the right

563
00:37:30,440 --> 00:37:33,830
the similarity of these quantities about that

564
00:37:33,840 --> 00:37:36,560
but this place this is problem that

565
00:37:36,570 --> 00:37:39,240
so we get this statement

566
00:37:39,260 --> 00:37:44,550
basically what it tells is that our empirical risk minimiser in these fine class

567
00:37:44,560 --> 00:37:48,260
will perform almost as well as the best function

568
00:37:48,270 --> 00:37:52,740
in the last two and if we knew the probability distribution

569
00:37:52,760 --> 00:37:55,360
plus some extra factor which

570
00:37:55,370 --> 00:37:58,660
it goes like one of those words of small and

571
00:37:58,710 --> 00:38:03,210
and the size of the class winter as the logarithm

572
00:38:03,260 --> 00:38:05,700
OK so this is really

573
00:38:06,240 --> 00:38:09,870
you know the most typical

574
00:38:09,880 --> 00:38:12,990
four of these bounds and you always

575
00:38:13,000 --> 00:38:17,290
i mean you will also see the same exact same here in the online setting

576
00:38:17,380 --> 00:38:18,370
this is really

577
00:38:18,390 --> 00:38:20,730
very typical you always have these

578
00:38:20,820 --> 00:38:25,890
one of the square root of n kind of dependent on the sample size n

579
00:38:25,920 --> 00:38:30,280
log n that depends on the size of the first phase

580
00:38:30,290 --> 00:38:32,530
and this

581
00:38:32,550 --> 00:38:36,910
maybe a not on this little one of the top then here is

582
00:38:36,950 --> 00:38:40,980
the confidence with which is bound holds right so if you want to have fun

583
00:38:41,350 --> 00:38:44,370
which i mean if you want to have guarantee that holds for

584
00:38:44,390 --> 00:38:47,410
ninety nine point nine after training set

585
00:38:47,430 --> 00:38:51,350
then this would increase so that the body bigger if you want to have a

586
00:38:51,350 --> 00:38:56,790
stronger confidence

587
00:38:58,130 --> 00:39:02,050
let's say this does not depend on the distribution on p right this is one

588
00:39:02,110 --> 00:39:05,450
for any possible be it just assumes that

589
00:39:05,490 --> 00:39:09,370
the sampling is actually i so independent

590
00:39:09,380 --> 00:39:11,400
it just rely on the independence assumption

591
00:39:11,420 --> 00:39:15,440
of the training data

592
00:39:17,000 --> 00:39:19,370
an interpretation is that if you do

593
00:39:19,380 --> 00:39:22,370
empirical risk minimization in your class

594
00:39:22,380 --> 00:39:26,150
it will converge to best function in your so that's a nice way

595
00:39:27,630 --> 00:39:32,010
actually solve this problem this learning problem is the estimation part of it then africa

596
00:39:32,070 --> 00:39:33,410
approximation parts

597
00:39:33,420 --> 00:39:37,700
you need to have a class that these regions right and that's where you see

598
00:39:37,790 --> 00:39:40,420
that the slogan might enter because

599
00:39:40,430 --> 00:39:43,260
we have another reason that as well

600
00:39:43,280 --> 00:39:44,570
when the class is small

601
00:39:44,580 --> 00:39:46,200
but when the class is small

602
00:39:46,290 --> 00:39:50,060
we cannot across all possible functions and in particular

603
00:39:50,110 --> 00:39:54,280
if we take you know just linear classifiers well they would be there would be

604
00:39:54,280 --> 00:39:58,500
infinitely many but let's say we take a small set of linear classifiers

605
00:39:58,520 --> 00:40:01,680
then we cannot approximate any possible function with that

606
00:40:02,980 --> 00:40:06,370
we will pay the price of having too small class

607
00:40:06,380 --> 00:40:10,200
not in the estimation part but in the output emission apart when we compare this

608
00:40:12,240 --> 00:40:14,120
two energy star

609
00:40:14,700 --> 00:40:18,430
this estimation versus approximation is really trade-off

610
00:40:19,070 --> 00:40:21,700
better approximation you have

611
00:40:21,710 --> 00:40:23,090
the larger your class

612
00:40:23,100 --> 00:40:25,870
and then there were estimation twenty the

613
00:40:25,880 --> 00:40:31,340
estimation deviation

614
00:40:31,720 --> 00:40:32,930
so again

615
00:40:32,980 --> 00:40:34,520
just to summarise

616
00:40:34,640 --> 00:40:36,110
these bonds are

617
00:40:36,160 --> 00:40:39,920
with respect to sampling of the training data so we have an almost all the

618
00:40:39,920 --> 00:40:41,100
training examples

619
00:40:42,620 --> 00:40:45,750
and you have this one of the world's top and here and this

620
00:40:45,830 --> 00:40:49,930
lochinver over and if you take into this is about one function and then if

621
00:40:49,930 --> 00:40:51,780
you a plus size and

622
00:40:51,790 --> 00:40:54,180
supreme and behaves like the in

623
00:40:54,190 --> 00:40:54,930
these are

624
00:40:54,950 --> 00:41:00,040
the main factor that you should recall from that

625
00:41:00,090 --> 00:41:03,660
one possible interpretation is that

626
00:41:03,670 --> 00:41:07,340
you should consider that the difference between those statements is that

627
00:41:07,390 --> 00:41:11,370
you have some more vulnerability here because the gene is no longer big but also

628
00:41:11,370 --> 00:41:14,030
fluctuates with i mean changes with the data

629
00:41:14,050 --> 00:41:17,950
it is like the discovery that i was showing before

630
00:41:18,030 --> 00:41:19,790
four given a point on the curve

631
00:41:19,840 --> 00:41:21,130
deviations as well

632
00:41:21,140 --> 00:41:22,750
but if you look if you

633
00:41:23,430 --> 00:41:24,780
it changes

634
00:41:24,790 --> 00:41:26,750
along the axis

635
00:41:26,770 --> 00:41:31,130
you in you increase the viability of this deviation you increase by a factor of

636
00:41:35,780 --> 00:41:39,730
OK so now let's see whether we can improve these bounds

637
00:41:39,740 --> 00:41:42,380
we can actually go further

638
00:41:42,500 --> 00:41:44,910
in many ways why is that

639
00:41:44,920 --> 00:41:48,030
i have things inequality if you recall

640
00:41:48,040 --> 00:41:51,140
we used the the fact that we need to apply to any possible random variable

641
00:41:51,140 --> 00:41:54,860
and we use the fact that the random variable was born which is the case

642
00:41:54,860 --> 00:41:58,850
here because the run by wings either zero one that was

643
00:42:00,210 --> 00:42:03,720
we can use more information about be available

644
00:42:03,950 --> 00:42:07,590
the other thing is that when we apply this union bound we can look at

645
00:42:07,590 --> 00:42:12,770
the worst case right we looked at the training set for which all any of

646
00:42:12,770 --> 00:42:14,610
these of these inequalities

647
00:42:16,800 --> 00:42:19,860
but this is kind of a worst case scenario

648
00:42:21,090 --> 00:42:24,380
so when we look at here

649
00:42:24,390 --> 00:42:28,690
we can look at the bottom that is but it's all possible functions in our

650
00:42:29,360 --> 00:42:33,930
and maybe that our algorithm does not just any function in the set and actually

651
00:42:33,930 --> 00:42:38,300
we know that just because one in particular which is the empirical risk minimizer

652
00:42:38,310 --> 00:42:39,430
so if we know

653
00:42:39,440 --> 00:42:42,360
where this risk minimizer might be

654
00:42:42,370 --> 00:42:45,690
then we may have about that you know does not take into account the size

655
00:42:45,690 --> 00:42:47,350
of the whole class but only

656
00:42:47,430 --> 00:42:50,570
the size of this subclass where we know

657
00:42:50,620 --> 00:42:52,570
our algorithm will be the function

658
00:42:55,780 --> 00:43:01,380
let's focus on on the first this part our refining the union about

659
00:43:02,560 --> 00:43:06,070
and what's going on let's have a closer look at how

660
00:43:06,150 --> 00:43:09,200
this a binomial

661
00:43:09,280 --> 00:43:12,860
variables because this and call

662
00:43:14,710 --> 00:43:17,790
is a binomial random is a binomial random variable

663
00:43:17,830 --> 00:43:22,190
it's a sum of variable that is either i mean it's very is either one

664
00:43:22,190 --> 00:43:26,370
or zero and we take another independent and some of the

665
00:43:27,950 --> 00:43:30,160
what is it right

666
00:43:30,220 --> 00:43:32,410
this is really what we call

667
00:43:33,700 --> 00:43:36,140
this the empirical error

668
00:43:36,150 --> 00:43:39,200
it follows a binomial distribution of parameter

669
00:43:41,410 --> 00:43:44,280
n being the number of samples that we have

670
00:43:44,300 --> 00:43:47,180
and keeping being the expected value

671
00:43:47,200 --> 00:43:48,180
like the

672
00:43:48,190 --> 00:43:49,550
the probability that

673
00:43:49,760 --> 00:43:53,100
the loss will be zero the loss will be one

674
00:43:53,160 --> 00:43:57,450
which is actually the more energy in the observation

675
00:43:57,500 --> 00:44:01,200
so energy is a binomial variables of you

676
00:44:01,210 --> 00:44:03,090
the end of that is about

677
00:44:03,140 --> 00:44:05,660
well about

678
00:44:06,130 --> 00:44:09,820
we can actually write precisely

679
00:44:09,830 --> 00:44:11,440
what are the aims

680
00:44:11,490 --> 00:44:14,420
four one function that is

681
00:44:14,430 --> 00:44:19,560
before one function we have inequality not just an upper bound for the deviation just

682
00:44:19,580 --> 00:44:21,100
you know this is

683
00:44:21,150 --> 00:44:22,690
the probability

684
00:44:22,710 --> 00:44:24,530
that if you take

685
00:44:24,580 --> 00:44:25,600
this and

686
00:44:25,650 --> 00:44:30,610
instances are this an independent bernoulli variables

687
00:44:30,650 --> 00:44:32,520
k of them will be equal to one

688
00:44:32,520 --> 00:44:36,550
so you said that you know we said that we assume that the state is

689
00:44:36,550 --> 00:44:38,560
also available

690
00:44:41,050 --> 00:44:45,480
then you could try to just as the mean absolute given

691
00:44:45,500 --> 00:44:47,430
the current acts of t

692
00:44:47,450 --> 00:44:48,970
current AFP

693
00:44:48,990 --> 00:44:51,320
and the next at x of t

694
00:44:51,350 --> 00:44:57,560
this is as if you want a regression problem right or maybe density estimation problem

695
00:44:57,560 --> 00:44:59,920
because the next state could be

696
00:44:59,930 --> 00:45:04,990
stochastic and so you want to know the next a distribution and not just the

697
00:45:04,990 --> 00:45:06,620
mean next state

698
00:45:07,860 --> 00:45:09,970
then you can be out of that

699
00:45:09,980 --> 00:45:14,150
and once you have been the more than just use the technique that shows that

700
00:45:14,240 --> 00:45:16,160
we are given no more than

701
00:45:16,220 --> 00:45:19,840
and just solve for the optimal control

702
00:45:19,900 --> 00:45:25,280
we'll see some approaches that felt like that that that they are shown that you're

703
00:45:25,280 --> 00:45:29,220
given the model we see that

704
00:45:29,230 --> 00:45:30,510
even if you have

705
00:45:30,550 --> 00:45:35,210
if you're given the model coming up with good action is is a highly non-trivial

706
00:45:37,280 --> 00:45:43,160
so surprisingly there is this other approach which is called a model free approach

707
00:45:43,250 --> 00:45:45,970
but you're not getting a more at all

708
00:45:46,140 --> 00:45:51,850
some home actually come up with a single controller it's not magic at all

709
00:45:54,150 --> 00:45:57,210
one very simple way of doing that is that

710
00:45:57,210 --> 00:45:58,470
you see that

711
00:45:58,490 --> 00:46:04,720
i'm going to have this capital as function parametrized with some parameters

712
00:46:05,110 --> 00:46:08,770
the parameters leaving some parameter space euclidean space

713
00:46:08,790 --> 00:46:14,230
and i just view this problem as an optimisation problem i use the gradient

714
00:46:14,360 --> 00:46:16,520
as can procedure

715
00:46:16,520 --> 00:46:19,080
to optimise my parameters

716
00:46:19,110 --> 00:46:23,860
and so that's quite policies so that's one approach but there are some other approaches

717
00:46:23,860 --> 00:46:26,890
and we are going to take a look at them

718
00:46:26,900 --> 00:46:31,550
it's it's very interesting so what's the deal here which approach should be

719
00:46:32,790 --> 00:46:35,850
that we don't know we we have no idea

720
00:46:35,900 --> 00:46:37,530
to tell you the truth

721
00:46:37,550 --> 00:46:39,770
so if you have

722
00:46:40,080 --> 00:46:45,140
a good prior knowledge about the what about the more that might be

723
00:46:45,250 --> 00:46:48,850
so you want to use that knowledge so there is no it's the same old

724
00:46:48,850 --> 00:46:51,220
story if you have some prior knowledge

725
00:46:51,260 --> 00:46:54,890
you want to put prior knowledge into the system and use it

726
00:46:54,910 --> 00:46:55,740
if you

727
00:46:56,010 --> 00:47:00,810
we don't have such a prior knowledge that you can still try both approaches then

728
00:47:00,810 --> 00:47:04,780
you have to somehow this nonparametric way

729
00:47:04,830 --> 00:47:10,680
and the problem that is that one then you're building the more you introduce some

730
00:47:10,680 --> 00:47:13,720
errors and those areas are going to propagate

731
00:47:13,720 --> 00:47:15,680
through the planning phase

732
00:47:15,700 --> 00:47:18,930
and it's not very much clearer

733
00:47:18,930 --> 00:47:23,780
from the point of view of the particle application which approach should be preferred

734
00:47:25,830 --> 00:47:27,180
so i can cannot

735
00:47:27,180 --> 00:47:29,410
have too much so once

736
00:47:29,430 --> 00:47:32,050
same here though

737
00:47:32,070 --> 00:47:32,930
is there

738
00:47:32,950 --> 00:47:37,700
building models rely heavily on being able to

739
00:47:37,740 --> 00:47:40,760
to estimate the state of knowledge the state

740
00:47:40,810 --> 00:47:45,930
and so one problem this is the approach could be that if that assumption is

741
00:47:45,930 --> 00:47:47,490
not met

742
00:47:47,490 --> 00:47:52,140
or is violated strongly

743
00:47:52,160 --> 00:47:54,830
then you're more that could be the

744
00:47:54,830 --> 00:47:59,580
and then you're planning solution is is going to be way off

745
00:48:13,930 --> 00:48:17,660
yes so this is sometimes true

746
00:48:19,950 --> 00:48:24,660
because you can come up with more than three approaches that like to be data

747
00:48:24,780 --> 00:48:29,870
fusion it's it's largely unclear to me if this is in chennai tree

748
00:48:29,870 --> 00:48:37,030
regarding the uniform isolation

749
00:48:37,040 --> 00:48:41,500
we got similar classification for manifold

750
00:48:41,580 --> 00:48:48,060
remember for two manifold perhaps a kind of geometry but here for manifold such thing

751
00:48:48,070 --> 00:48:49,460
conjectured that

752
00:48:49,460 --> 00:48:52,220
there are eight canonical geometry is

753
00:48:52,260 --> 00:48:54,340
and as a special case

754
00:48:54,350 --> 00:48:57,980
puckering conjecture treated with

755
00:48:57,980 --> 00:48:59,980
such kind of stuff

756
00:49:00,020 --> 00:49:04,460
the manifold which is closed compact and the simply connected

757
00:49:04,500 --> 00:49:09,090
it conjectured that this kind of many folks are actually three spheres

758
00:49:09,210 --> 00:49:11,640
and this conjecture has been proven

759
00:49:11,760 --> 00:49:14,550
using the ricci flow

760
00:49:14,810 --> 00:49:22,510
and our work focused on another kind of the manifold which is hyperbolic manifold with

761
00:49:23,700 --> 00:49:30,440
more gates some formulas for the flow

762
00:49:30,750 --> 00:49:35,300
on such kind of manifold and also proved the convergence for that flow

763
00:49:35,370 --> 00:49:38,770
and later some discrete out with arms

764
00:49:38,780 --> 00:49:45,900
was developed to compute this flow and to elastic constant curvature metric

765
00:49:45,920 --> 00:49:51,660
here is a result of the discrete curvature flow for hyperbolic three manifold with boundaries

766
00:49:52,180 --> 00:49:54,140
the figure on on the left

767
00:49:54,200 --> 00:50:00,280
it shows the surface the boundary surface of only on the volume is basically war

768
00:50:00,280 --> 00:50:02,560
with multiple tunnels

769
00:50:04,850 --> 00:50:09,590
and top logic like this is hyperbolic the manifold

770
00:50:09,610 --> 00:50:11,090
the figure in between

771
00:50:11,110 --> 00:50:15,200
in the middle shows the canonical metric

772
00:50:15,200 --> 00:50:20,200
which is embedded in the hyperbolic space

773
00:50:20,260 --> 00:50:22,790
and the figure on the right

774
00:50:22,820 --> 00:50:27,020
in some multiple copy of the new ones actually that's you know it's the kind

775
00:50:28,350 --> 00:50:32,270
embedded in the hyperbolic space

776
00:50:32,590 --> 00:50:35,680
the same animal food curvature flow

777
00:50:35,810 --> 00:50:40,050
we believe that it could have to study of the three manifold john janet apology

778
00:50:40,050 --> 00:50:44,480
because that part away to realize something

779
00:50:44,500 --> 00:50:50,030
also we are expecting that there should be some applications in engineering fields like the

780
00:50:50,030 --> 00:50:55,010
one of my take from transition or shape comparison stuff

781
00:50:55,360 --> 00:50:57,760
i was so

782
00:50:58,130 --> 00:51:03,530
we would like to generalise this flow to other kinds of fuzzy manifold because there

783
00:51:03,530 --> 00:51:08,960
are eight john in in total

784
00:51:09,100 --> 00:51:14,160
i like to conclusion conclude the talk briefly

785
00:51:14,180 --> 00:51:20,080
curvature flow is the fundamental to discover the intrinsic nature of geometry

786
00:51:20,220 --> 00:51:26,160
basically it just deform the metric according to a local according to the

787
00:51:26,180 --> 00:51:28,720
curvature is

788
00:51:28,740 --> 00:51:35,720
it can be scrutinized properly to so many practical problems in engineering fields for surfaces

789
00:51:36,220 --> 00:51:41,740
the discrete curvature flow like them is already allowable for all three geometry is

790
00:51:41,760 --> 00:51:44,080
for three manifold salary

791
00:51:44,090 --> 00:51:47,790
the discrete curvature flow with them has been studied only four

792
00:51:48,370 --> 00:51:53,570
a certain kind of three manifold and the much more work has been done house

793
00:51:54,030 --> 00:51:57,960
is expected to be down in the future

794
00:51:58,000 --> 00:52:01,010
for more details

795
00:52:01,060 --> 00:52:05,690
please refer to on two books one is the computation the complement john tree that

796
00:52:05,700 --> 00:52:09,540
there is the variation of press parts of discrete surfaces

797
00:52:09,610 --> 00:52:15,070
and all the data sets in the source codes in this in this work is

798
00:52:15,070 --> 00:52:20,630
available at the following website

799
00:52:20,680 --> 00:52:25,580
again i would like to thank the organisers and the sponsors of this conference and

800
00:52:25,590 --> 00:52:32,960
also like to thank all of our collaborators from at different universities and different fields

801
00:52:32,980 --> 00:52:37,590
last but not least i'd like to thank all audience here for your attention thank

802
00:52:38,830 --> 00:52:47,460
i think you mean question

803
00:52:47,460 --> 00:53:03,270
OK i have question the think about taking into account some scalar fields at the

804
00:53:03,270 --> 00:53:08,900
same time than the geometry in that apology for instance to make your school take

805
00:53:08,900 --> 00:53:13,400
there is no reason that each of these it is what it is

806
00:53:13,460 --> 00:53:19,860
essentially the the only the the simplest reason that the initial bits of what they

807
00:53:19,860 --> 00:53:22,340
are is those bits themselves

808
00:53:22,360 --> 00:53:26,320
you know there is no more concise axiom from which you can deduce this

809
00:53:26,380 --> 00:53:30,270
so in other words it looks very even it's very it's a perfect simulation within

810
00:53:30,270 --> 00:53:33,020
pure mass of contingent truths

811
00:53:33,040 --> 00:53:38,380
as opposed to necessary truths because like this distinguishes between necessary truths which are

812
00:53:39,130 --> 00:53:43,440
true by reason by pure your thoughts are you know the opposite the contradiction and

813
00:53:43,440 --> 00:53:47,750
contingent truths like the president of france is

814
00:53:48,770 --> 00:53:52,670
you know you don't expect to be able to prove that logically and mathematically it's

815
00:53:52,670 --> 00:53:57,590
a contingent truth you know all accidental so to speak or historical truth and these

816
00:53:57,590 --> 00:54:00,880
these bits even though they in the world of pure thought in the islamic world

817
00:54:00,880 --> 00:54:06,170
of human are perfect simulation they look a lot because because of our finite limitations

818
00:54:06,170 --> 00:54:08,110
like contingent

819
00:54:08,110 --> 00:54:11,610
and they look a lot like independent as a fair coin

820
00:54:11,610 --> 00:54:13,040
OK so

821
00:54:13,070 --> 00:54:16,860
so in and what this shows is that pure math is not simple

822
00:54:16,880 --> 00:54:21,040
the way the hilbert for the pure maths who came from a small number of

823
00:54:21,040 --> 00:54:24,400
ideas that we can all agree on and that's why you think is that because

824
00:54:24,400 --> 00:54:28,730
of beautiful this makes it look a lot more like biology said

825
00:54:28,750 --> 00:54:31,770
even in pure mathematics we have infinite complexity

826
00:54:34,270 --> 00:54:38,730
so this is the message i i wanted to give you focus and maybe it's

827
00:54:38,730 --> 00:54:40,790
time to open the floor to

828
00:54:42,750 --> 00:54:45,020
questions discussions so

829
00:54:45,040 --> 00:54:49,020
i mean i have been taking questions all along does anybody have any thing they

830
00:54:49,020 --> 00:54:52,340
want to comment or ask if not i can go on a little bit more

831
00:54:52,340 --> 00:54:56,520
i think i still have ten minutes or so

832
00:54:56,750 --> 00:55:01,570
i want to disagree with this or let me tell you how the math community

833
00:55:01,570 --> 00:55:05,000
has reacted to this OK

834
00:55:05,070 --> 00:55:09,790
OK well i guess what is the physics meeting and nine and i have not

835
00:55:09,790 --> 00:55:13,500
being invited to jerusalem after thirty years for physics moving really complex systems in physics

836
00:55:13,500 --> 00:55:14,750
right sort of

837
00:55:18,230 --> 00:55:21,320
also in high

838
00:55:29,980 --> 00:55:43,020
and they said don't invite it's crazy

839
00:55:49,150 --> 00:55:51,480
really that's great to hear

840
00:55:51,540 --> 00:55:55,820
well let me tell you let me tell you how it is going was basically

841
00:55:55,820 --> 00:55:59,860
a first class a lot of shock in the math community i mean as a

842
00:55:59,900 --> 00:56:06,400
young student eurasian younger i would be by hermann biological online by other mathematicians they

843
00:56:06,400 --> 00:56:12,590
released goal really disturbed it took away the their belief in the

844
00:56:12,590 --> 00:56:16,190
on the world of ideas in in the that mathematical truths black or white and

845
00:56:16,190 --> 00:56:17,670
provide absolute certainty

846
00:56:17,670 --> 00:56:22,340
and and this upset a lot of mathematicians and then strangely enough such of the

847
00:56:22,340 --> 00:56:27,590
powers of the human mind you know to suppress not think about things that are

848
00:56:27,590 --> 00:56:34,050
unpleasant that complete the math community completely ignores chemical completeness and goes on exactly as

849
00:56:34,050 --> 00:56:38,460
before in a what i would say i hilbertian style believing in formal methods an

850
00:56:38,460 --> 00:56:42,130
axiomatic method or you could say about pakistan if you want to use the

851
00:56:42,130 --> 00:56:45,960
the french school that came from over and and in and and this is still

852
00:56:45,960 --> 00:56:49,790
the official religion in the math community and the math community

853
00:56:50,250 --> 00:56:55,960
the reaction the larger community for example the reaction to this work is first of

854
00:56:55,960 --> 00:57:00,040
all i think it's crazy because they don't understand this concept of program size which

855
00:57:00,040 --> 00:57:04,610
are really connected to entropy complexity to them these are very strange ideas

856
00:57:04,690 --> 00:57:06,610
and to talk about

857
00:57:06,630 --> 00:57:13,980
something that's imitates randomness you know in mathematical logic is heresy because logicians are people

858
00:57:13,980 --> 00:57:19,360
who hate randomness you know that's why they became magicians so so even gonality completeness

859
00:57:19,360 --> 00:57:24,690
is unpopular in the world logic or in the world of mathematics if you look

860
00:57:24,690 --> 00:57:30,480
at the reaction of the larger community mathematical logic community together is very ambiguous on

861
00:57:30,480 --> 00:57:33,900
the one hand they have to claim him because he is the most famous magician

862
00:57:35,380 --> 00:57:38,380
they have to claim about on the other hand if you look at article because

863
00:57:38,400 --> 00:57:41,590
there what obviously says that logic is the failure

864
00:57:41,610 --> 00:57:43,170
and therefore the subject can be

865
00:57:43,170 --> 00:57:46,880
thrown away and you have to teach courses and this of course no logician would

866
00:57:46,880 --> 00:57:50,190
like you to take home as a message from a course in logic you know

867
00:57:50,190 --> 00:57:54,690
that they should file the logics professors so normally books on logic have the last

868
00:57:54,690 --> 00:57:57,770
chapter my talk about computers and of course is you know you never get to

869
00:57:57,770 --> 00:58:00,020
the last chapter when you give course

870
00:58:00,960 --> 00:58:03,290
so these ideas

871
00:58:03,900 --> 00:58:07,810
the you know those physics these ideas are really very close to ideas from physics

872
00:58:08,190 --> 00:58:13,110
this idea of looking for randomness in mathematics randomness selected structure is connected with entropy

873
00:58:13,540 --> 00:58:18,840
these are sort of versions of ideas of statistical mechanics these ideas to physicists these

874
00:58:18,840 --> 00:58:20,480
ideas seem much

875
00:58:20,500 --> 00:58:24,810
more familiar and and and there is a much more comfortable with this which is

876
00:58:24,810 --> 00:58:30,540
why here i am at a physics meeting at the mass meeting so so

877
00:58:30,540 --> 00:58:34,630
so so i would say but but i think

878
00:58:34,690 --> 00:58:38,980
two take stick out to stick my neck out i think that what this really

879
00:58:38,980 --> 00:58:43,920
shows you is the pure mathematics is and is is that is not like theoretical

880
00:58:43,920 --> 00:58:48,400
physics but it's not that different i mean pure mathematicians like to think that they

881
00:58:48,400 --> 00:58:54,040
have absolute truth and you know we physicists i consider myself an honorary physicists we

882
00:58:54,040 --> 00:58:58,070
said you know our preferred just to restrict their on rigorous they're worthless

883
00:58:58,070 --> 00:59:01,960
but you know what my what i really get my

884
00:59:01,960 --> 00:59:06,110
my feeling about all of this is that it's a continuous absolute truth you can

885
00:59:06,110 --> 00:59:10,570
only approach asymptotically you know in the limit and they respect proofs of the kind

886
00:59:10,570 --> 00:59:15,610
of mathematical physicist feels comfortable with that are not rigorous proofs and it's a continuous

887
00:59:15,710 --> 00:59:19,360
you know i don't see a sharp divide between the world of pure mathematics and

888
00:59:19,360 --> 00:59:23,570
the kind of reasoning that theoretical physicist would be happy with i mean it's nice

889
00:59:23,570 --> 00:59:26,820
if you can have a a rigorous absolute proof

890
00:59:27,610 --> 00:59:30,750
what would be the you know what would be the other extreme if you take

891
00:59:30,750 --> 00:59:34,340
a little bit is very seriously you could say i work for i i have

892
00:59:34,340 --> 00:59:37,460
the property of the prime numbers which i talked on the computer

893
00:59:37,460 --> 00:59:40,880
and it it you know there's is a beautiful curve in its fit beautifully by

894
00:59:40,880 --> 00:59:47,050
a very simple equation and his were published this right you know and you mathematician

895
00:59:47,050 --> 00:59:51,400
will say i don't care how accurately this very simple formula fits the curve and

896
00:59:51,400 --> 00:59:56,630
it works out to know hundred billion or trillion you know i need to prove

897
00:59:56,670 --> 01:00:01,570
and so what i'm saying what what happens if you mathematicians have

898
01:00:01,610 --> 01:00:06,440
i have been promoting what they call experimental mathematics is the journal of experimental mathematics

899
01:00:06,440 --> 01:00:12,090
there are a few books on experimental mathematics as research methodology that for example john

900
01:00:12,090 --> 01:00:16,480
for wine has co-authored co-edited these books and

901
01:00:16,500 --> 01:00:21,420
he's not as extreme as i am i think what this does is provide some

902
01:00:21,420 --> 01:00:23,020
theoretical grounds

903
01:00:23,070 --> 01:00:27,880
four thinking that maybe two masses quasi empirical i would say like at out and

904
01:00:27,880 --> 01:00:31,360
let me say that there are cases in the world of pure mathematics and computer

905
01:00:31,360 --> 01:00:35,120
it gives the same result as comparing the partition function of the PKB

906
01:00:36,680 --> 01:00:40,300
so now all that we need to do is weighted model counting that if we

907
01:00:40,300 --> 01:00:43,810
know how to do it model counting we know how to compute conditional probabilities

908
01:00:43,850 --> 01:00:46,440
so how are we going to do that

909
01:00:46,460 --> 01:00:48,800
well first of all

910
01:00:48,810 --> 01:00:49,750
here's the

911
01:00:49,770 --> 01:00:51,870
our level call right

912
01:00:51,900 --> 01:00:55,350
this is sort of like the high-level probabilistic improving routine

913
01:00:55,350 --> 01:00:57,700
what i do is

914
01:00:57,750 --> 01:01:01,140
ptp is the routine i give it PKB and the query

915
01:01:01,880 --> 01:01:05,570
and now what i do is i have query as hard formula right this is

916
01:01:05,570 --> 01:01:08,600
what is zero means that the phi i zero

917
01:01:08,610 --> 01:01:10,650
as far from the to the PKB

918
01:01:10,700 --> 01:01:13,250
and then i returned the ratio of these two things

919
01:01:13,250 --> 01:01:16,620
the PKB i converted to weighted CNF

920
01:01:17,340 --> 01:01:19,250
i could between model count

921
01:01:19,260 --> 01:01:20,840
this is the wikipedia

922
01:01:20,850 --> 01:01:24,580
this is the p can be without you the rest of the is my answer

923
01:01:24,900 --> 01:01:30,460
so i can feel my conditional probabilities by computing russia to partition functions

924
01:01:30,500 --> 01:01:32,450
notice how similar this is

925
01:01:32,470 --> 01:01:36,080
so the basic theorem proving schema in logic i

926
01:01:36,090 --> 01:01:39,210
in logic what i do is i start with a degree in the CBI and

927
01:01:39,210 --> 01:01:41,330
the negation of the query to the KB

928
01:01:41,340 --> 01:01:45,050
and then i returned not of that

929
01:01:45,070 --> 01:01:46,530
and CNN

930
01:01:46,540 --> 01:01:49,060
here i also have created the PKB

931
01:01:49,080 --> 01:01:52,040
in fact i cannot negate on on the get you know the effect is the

932
01:01:53,180 --> 01:01:58,120
the difference is that instead of computing sat i just computes the the ratio of

933
01:01:58,120 --> 01:02:00,000
the two model counts

934
01:02:00,030 --> 01:02:04,160
notice in particular that if i know how to do this i know how to

935
01:02:04,170 --> 01:02:05,510
do this because

936
01:02:05,860 --> 01:02:10,850
sachs is false if the weighted model count is zero

937
01:02:10,910 --> 01:02:15,010
so by doing this in particular we also know how to do you know ordinary

938
01:02:15,410 --> 01:02:17,860
logic theorem proving

939
01:02:22,120 --> 01:02:28,620
finite domains right as i mentioned very previously on monday the finite domains

940
01:02:28,700 --> 01:02:31,850
we haven't done this for different domains and it's an interesting problem

941
01:02:31,990 --> 01:02:35,860
so i'm assuming function free finite domains her interpretation

942
01:02:36,040 --> 01:02:38,530
otherwise things get more complicated

943
01:02:38,540 --> 01:02:40,860
as you know

944
01:02:40,870 --> 01:02:42,680
my question

945
01:02:42,710 --> 01:02:46,170
but you know for most purposes finite domains are sufficient

946
01:02:46,230 --> 01:02:49,660
so we certainly want to solve the problem and we we've made some progress towards

947
01:02:49,660 --> 01:02:54,370
that but you know for most applications that doesn't really arise

948
01:02:54,370 --> 01:02:55,900
OK so now the

949
01:02:55,920 --> 01:02:58,870
next question is hardly the way to model counting

950
01:02:58,880 --> 01:03:02,820
so here's another them to the weighted model counting and therefore is the result of

951
01:03:02,820 --> 01:03:03,950
computing now

952
01:03:04,040 --> 01:03:04,920
it was

953
01:03:04,940 --> 01:03:08,810
this is going to be an extension of the PLO

954
01:03:08,820 --> 01:03:12,660
so like ppll i'm going to start with the base case the base case is

955
01:03:12,660 --> 01:03:16,920
the case where all the clauses are satisfied right so if all clauses are satisfied

956
01:03:17,110 --> 01:03:19,540
then basically all the world's

957
01:03:19,540 --> 01:03:22,210
so it's the former so i just need to compute the sum of the weights

958
01:03:22,210 --> 01:03:25,450
of all the world but i don't want to do that exponentially in i don't

959
01:03:25,450 --> 01:03:29,780
need to because everything factorizes briefly and i can just to the product

960
01:03:29,830 --> 01:03:32,190
overall that happens in the CNF

961
01:03:32,210 --> 01:03:36,110
of the some of the positive and the negative little of that data

962
01:03:36,960 --> 01:03:40,090
so this is a very nice is because it's very efficient

963
01:03:40,170 --> 01:03:45,320
this is the way to get if all the world satisfy the CN

964
01:03:45,320 --> 01:03:48,850
if CNF has an empty unsatisfied clause of course in that case still need to

965
01:03:48,850 --> 01:03:54,720
return zero because there are no satisfying world's next comes the decomposition step

966
01:03:54,750 --> 01:03:59,050
it's actually very important step because it's what's going to simplify our problems

967
01:03:59,060 --> 01:04:01,200
this is the following steps

968
01:04:01,230 --> 01:04:05,320
if you're CNF can be partitioned into sub CNN's

969
01:04:05,320 --> 01:04:08,660
that do not share any atoms with each other

970
01:04:08,750 --> 01:04:13,000
then i can solve the model cont for each of the CNS separately then just

971
01:04:13,000 --> 01:04:14,830
multiply the results

972
01:04:15,000 --> 01:04:19,280
this should be thrown into the relative to CNS don't share any atoms i can

973
01:04:19,280 --> 01:04:22,700
do the model called for the first CNF into the model come for the second

974
01:04:22,700 --> 01:04:25,920
c and f and the total count is just the product of the two

975
01:04:25,930 --> 01:04:29,260
so if i'm not in the basic case is the first thing i look for

976
01:04:29,260 --> 01:04:32,950
is the composition of the CNF into smaller cn eps

977
01:04:32,970 --> 01:04:36,960
and if i can find such a decomposition you know it i mean all in

978
01:04:36,960 --> 01:04:41,270
good shape because i have just bought myself an exponential reduction in the amount of

979
01:04:41,270 --> 01:04:42,680
work that i have to do

980
01:04:42,690 --> 01:04:46,690
of course in and then what i do is i just turned the product of

981
01:04:46,910 --> 01:04:51,180
the individual model counts each of these being a recursive call to to the

982
01:04:52,850 --> 01:04:56,950
of course it will often typically at least in the beginning

983
01:04:57,130 --> 01:05:00,160
things will not decompose nice like that so then what i have to do is

984
01:05:00,160 --> 01:05:02,260
again choose an atom

985
01:05:02,300 --> 01:05:04,090
and i'm going to decompose

986
01:05:04,110 --> 01:05:06,130
my model count into two parts

987
01:05:06,220 --> 01:05:09,080
the model count of the world where the atom is true

988
01:05:09,120 --> 01:05:12,300
and the model counts of the world where the thing is false and only to

989
01:05:12,300 --> 01:05:15,950
notice that these two sets of walls are disjoint so i can just about the

990
01:05:15,950 --> 01:05:18,670
to model counts so this is what i'm going to do

991
01:05:18,770 --> 01:05:20,530
i'm going to return

992
01:05:20,550 --> 01:05:24,320
the weight of the at times the model accounts for the world's where the atom

993
01:05:24,320 --> 01:05:25,160
is true

994
01:05:25,180 --> 01:05:28,330
plus the weight of the norm of the negation of that

995
01:05:28,520 --> 01:05:33,640
and the model count of the world's worst that this falls the and again as

996
01:05:33,640 --> 01:05:37,330
DPLL one says that the and the atom to true i can simplify went to

997
01:05:37,340 --> 01:05:40,070
things and get smaller CNN

998
01:05:40,080 --> 01:05:44,530
and there's my seniors get smaller smaller i'll get into something can decompose

999
01:05:44,540 --> 01:05:47,770
at which point they can get even smaller and then eventually i get to the

1000
01:05:47,770 --> 01:05:50,200
base case and you know and i get my result

1001
01:05:50,260 --> 01:05:55,160
so this is your basic proposition weighted model counting works

1002
01:05:55,250 --> 01:05:58,750
of course we want to do this at the first order level

1003
01:05:58,760 --> 01:06:03,040
we don't just want to do this for propositional knowledge base

1004
01:06:03,060 --> 01:06:04,610
so what changes

1005
01:06:04,630 --> 01:06:09,620
well the basic probabilistic theorem proving schemas there exactly the same nothing trees at the

1006
01:06:09,730 --> 01:06:11,950
level again just as in first-order logic

1007
01:06:12,050 --> 01:06:14,410
but there's a couple of things that to change

1008
01:06:14,460 --> 01:06:18,400
the first one is when i convert my probabilistic knowledge base

1009
01:06:18,440 --> 01:06:21,940
two the form of a CNF and weights

1010
01:06:21,960 --> 01:06:24,130
now each new actor

1011
01:06:24,140 --> 01:06:27,540
i remember i had to define a new at for each formula and the the

1012
01:06:27,540 --> 01:06:29,670
and then there shift comes up again

1013
01:06:29,900 --> 01:06:32,120
if a lead shift goal

1014
01:06:32,130 --> 01:06:34,200
grow to infinity

1015
01:06:35,170 --> 01:06:41,910
depending on the value of w right this quantity will become some dominant and so

1016
01:06:41,960 --> 01:06:44,930
you end up with the maximum

1017
01:06:45,010 --> 01:06:49,220
the stuff that is the maximum distance which you can see an object

1018
01:06:49,250 --> 01:06:53,230
so should remind you of the concept of an horizon

1019
01:06:53,250 --> 01:06:57,030
so we get back to eyes and later on because our eyes and it's a

1020
01:06:57,030 --> 01:07:02,460
concept that comes in over and over and over again in cosmology and it's considered

1021
01:07:02,620 --> 01:07:08,150
the basis of what created all their primordial perturbations that then grew and gravity and

1022
01:07:08,150 --> 01:07:12,550
created the galaxies the planets and all the structure that was in the universe today

1023
01:07:12,600 --> 01:07:14,230
but this is the first time

1024
01:07:14,250 --> 01:07:19,220
you meet the concept of the cosmological horizon

1025
01:07:19,250 --> 01:07:20,330
OK good

1026
01:07:21,330 --> 01:07:25,480
if there is only one component and it wasn't cosmological constant in order to write

1027
01:07:25,510 --> 01:07:29,360
equation around to write a fixed function is the the age of the universe except

1028
01:07:29,430 --> 01:07:34,610
except but if it was a cosmological constant and then you can to do the

1029
01:07:34,610 --> 01:07:41,500
outside with before t to the some power but but hey since it's it's constant

1030
01:07:41,850 --> 01:07:43,430
then you're

1031
01:07:43,450 --> 01:07:47,360
friedmann equation becomes something like that the whole look

1032
01:07:47,380 --> 01:07:52,620
it tells me mean that they are lot parameter is always constant

1033
01:07:52,620 --> 01:07:56,030
so if the hubble parameter is called the means that they don't have a is

1034
01:07:56,030 --> 01:08:03,750
constant which means that they have an exponential expansion

1035
01:08:03,810 --> 01:08:05,720
makes sense

1036
01:08:05,790 --> 01:08:07,430
if you have nothing

1037
01:08:07,480 --> 01:08:09,370
and there's nothing has got some

1038
01:08:10,900 --> 01:08:15,260
the costume often to create more nothing and so you end up having an exponential

1039
01:08:19,040 --> 01:08:23,280
remember this equation

1040
01:08:23,300 --> 01:08:28,300
because this kind of equation will be at the base of what is called inflation

1041
01:08:28,350 --> 01:08:31,520
and inflation is something that is basically tell you

1042
01:08:31,560 --> 01:08:35,250
what created the universe as we see today

1043
01:08:36,280 --> 01:08:41,260
and it's all based on an equation like this

1044
01:08:42,370 --> 01:08:44,120
so let's write down

1045
01:08:44,900 --> 01:08:46,340
the distance

1046
01:08:46,430 --> 01:08:48,950
some physical properties

1047
01:08:49,000 --> 01:08:53,440
and you end up that if you like that in this case

1048
01:08:53,460 --> 01:08:57,380
this grows exponentially

1049
01:08:57,430 --> 01:09:01,630
these grows exponentially and see all the rage

1050
01:09:01,670 --> 01:09:03,190
is much smaller

1051
01:09:04,400 --> 01:09:08,940
and it is

1052
01:09:08,960 --> 01:09:14,540
so what happened here

1053
01:09:14,560 --> 01:09:19,800
you do have another arise

1054
01:09:19,850 --> 01:09:22,370
because this is

1055
01:09:22,430 --> 01:09:23,860
as far as you can

1056
01:09:23,860 --> 01:09:29,000
that any signal can travel

1057
01:09:29,050 --> 01:09:31,070
we go back to this again

1058
01:09:31,120 --> 01:09:33,550
later on

1059
01:09:37,480 --> 01:09:42,090
today we know that the composition of the universe is given by the thing which

1060
01:09:42,090 --> 01:09:43,720
we call the cosmic right

1061
01:09:43,780 --> 01:09:49,160
and i told you that we have to introduce the cosmological constant and live with

1062
01:09:49,160 --> 01:09:56,050
it because OK in this plot is called the dark energy but it it's so

1063
01:09:56,050 --> 01:10:02,610
you called that kennedy something that does about this equation of state parameter w that

1064
01:10:02,610 --> 01:10:08,170
makes the universe accelerator so it's less than minus one said but is not necessarily

1065
01:10:08,170 --> 01:10:14,070
minus one inequality cosmological constant with added to parameters exactly minus one and so on

1066
01:10:14,090 --> 01:10:20,310
this cosmic by has been denied by looking at data and actually fitting the data

1067
01:10:20,310 --> 01:10:26,050
with the cosmological constant but anyway so what this is telling you and later on

1068
01:10:26,050 --> 01:10:29,980
i'll tell you how we got to know the cosmic by there's a lot of

1069
01:10:29,980 --> 01:10:33,800
called out there's a little bit of atoms there is a lot of this very

1070
01:10:33,800 --> 01:10:37,450
we the stuff and some radiation the base of the

1071
01:10:38,760 --> 01:10:46,220
so let me try to motivate there is more than meets the eye

1072
01:10:46,310 --> 01:10:50,830
so in the solar system we know that there is the sun and the planets

1073
01:10:50,850 --> 01:10:53,130
the planet them shine the sun does

1074
01:10:53,150 --> 01:10:57,400
so if you if you were an alien living in on another sort of system

1075
01:10:57,730 --> 01:11:01,390
it would see the sun but it's not necessarily see the planets

1076
01:11:01,430 --> 01:11:06,340
and the probability of hair the whole view of you or your thread that there

1077
01:11:06,340 --> 01:11:09,860
is a lot of work going on in trying to find what are called extrasolar

1078
01:11:09,860 --> 01:11:13,620
planets and it's and it's a tough job

1079
01:11:13,620 --> 01:11:18,940
if the ball styrofoam but it continues going in that direction

1080
01:11:20,150 --> 01:11:26,650
so the european pong ball has actually better being than you found

1081
01:11:26,660 --> 01:11:28,480
and then we get all

1082
01:11:28,480 --> 01:11:30,150
i'm going to a

1083
01:11:30,180 --> 01:11:31,630
who's involved

1084
01:11:33,730 --> 01:11:36,010
what we got to do now

1085
01:11:36,030 --> 01:11:37,350
some of you the

1086
01:11:37,510 --> 01:11:40,280
in the ball position

1087
01:11:40,320 --> 01:11:46,180
they latter i invite you to come down to get a little closer

1088
01:11:48,370 --> 01:11:51,280
all right now

1089
01:11:51,290 --> 01:11:53,510
are you roughly ready

1090
01:11:54,280 --> 01:11:58,490
right but there is one other piece of information is very important and i got

1091
01:11:58,500 --> 01:12:00,430
to tell you

1092
01:12:00,480 --> 01:12:03,780
and that as only falls

1093
01:12:03,790 --> 01:12:05,840
for their chemistry professor

1094
01:12:06,370 --> 01:12:10,380
of thanks

1095
01:12:10,390 --> 01:12:12,710
going to what your

1096
01:12:35,690 --> 01:13:21,410
i have all the particle want

1097
01:13:22,250 --> 01:13:23,900
and as

1098
01:13:23,930 --> 01:13:26,890
i now comes the moment of truth

1099
01:13:26,910 --> 01:13:34,520
how many of you had an alpha particle that share

1100
01:13:34,540 --> 01:13:37,690
OK now i got it now

1101
01:13:37,700 --> 01:13:38,720
so i see

1102
01:13:38,730 --> 01:13:42,180
keep your hands up because there's something like that makes a little hard for me

1103
01:13:42,180 --> 01:13:45,280
to see so i don't see anybody in the section here

1104
01:13:55,900 --> 01:13:57,040
any more

1105
01:13:57,900 --> 01:14:01,530
get everybody

1106
01:14:01,530 --> 01:14:03,020
at that

1107
01:14:03,030 --> 01:14:06,610
six alpha particles that they get OK

1108
01:14:06,630 --> 01:14:08,600
so now

1109
01:14:08,610 --> 01:14:13,290
we have to calculate some probability theory

1110
01:14:13,300 --> 01:14:18,040
probability of backscattering

1111
01:14:18,320 --> 01:14:21,380
the number six

1112
01:14:21,460 --> 01:14:24,140
divided by

1113
01:14:24,270 --> 01:14:27,200
two hundred eighty seven

1114
01:14:27,240 --> 01:14:29,980
and that gives me

1115
01:14:30,030 --> 01:14:32,580
zero point zero two

1116
01:14:34,230 --> 01:14:37,230
here in one extra significant figure

1117
01:14:37,250 --> 01:14:41,220
and to find out their probability

1118
01:14:41,240 --> 01:14:44,860
and i love this in year

1119
01:14:44,880 --> 01:14:51,150
i've already done the calculation i got you find that the diameter

1120
01:14:51,160 --> 01:14:57,300
the correct number of significant figures is zero points have

1121
01:14:57,310 --> 01:15:02,340
and the actual diameter of these walls

1122
01:15:02,350 --> 01:15:04,140
zero point eight

1123
01:15:04,160 --> 01:15:07,830
so you guys that's very great job

1124
01:15:13,830 --> 01:15:16,530
so i gave it works right

1125
01:15:16,530 --> 01:15:20,040
that's how they calculated the diameter of the nucleus

1126
01:15:20,120 --> 01:15:21,260
that that

1127
01:15:21,290 --> 01:15:26,290
no one or nothing else involved in this edit the principle behind how it was

1128
01:15:27,150 --> 01:15:28,550
OK very simple

1129
01:15:28,550 --> 01:15:29,950
the principle

1130
01:15:29,950 --> 01:15:31,120
behind how

1131
01:15:31,150 --> 01:15:33,820
the four was discovered

1132
01:15:33,830 --> 01:15:38,490
OK and the five corps was revealed

1133
01:15:41,750 --> 01:15:45,050
that's right

1134
01:15:46,140 --> 01:15:50,120
we got the problem they to

1135
01:15:50,140 --> 01:15:52,390
the scientific community here

1136
01:15:52,400 --> 01:15:59,680
in the early nineteen eleven nineteen twelve right after the discovery of the nucleus

1137
01:15:59,690 --> 01:16:02,570
the problem is now

1138
01:16:02,620 --> 01:16:07,180
we know we have gotten away as we know it began electron

1139
01:16:07,190 --> 01:16:10,240
what the structure of the atom how does the

1140
01:16:10,290 --> 01:16:12,380
electron and the nucleus

1141
01:16:15,490 --> 01:16:17,880
in particular what we have to do

1142
01:16:17,900 --> 01:16:19,500
is we have to ask

1143
01:16:19,500 --> 01:16:22,460
what is the force of attraction

1144
01:16:22,610 --> 01:16:26,660
keeps the nucleus and the electron together

1145
01:16:27,770 --> 01:16:30,290
so we're going to talk here about this

1146
01:16:30,320 --> 01:16:32,790
o description

1147
01:16:39,590 --> 01:16:44,590
and we talk about the force of the interaction works

1148
01:16:45,520 --> 01:16:49,090
there are four known fundamental forces

1149
01:16:49,120 --> 01:16:50,830
what the force

1150
01:16:50,840 --> 01:16:54,020
that is the weakest force

1151
01:16:54,020 --> 01:16:56,360
gravity and

1152
01:16:56,380 --> 01:16:57,840
what is the cause

1153
01:16:57,850 --> 01:17:01,610
that's the stronger force

1154
01:17:01,630 --> 01:17:04,240
electromagnetic absolutely

1155
01:17:04,240 --> 01:17:08,840
what's the next round of applause

1156
01:17:08,900 --> 01:17:10,430
the weak force

1157
01:17:10,430 --> 01:17:12,950
and four

1158
01:17:14,400 --> 01:17:17,360
all right we can strong force

1159
01:17:17,450 --> 01:17:19,430
these are trouble

1160
01:17:19,430 --> 01:17:21,000
nuclear forces

1161
01:17:21,010 --> 01:17:23,010
with the nucleus

1162
01:17:23,060 --> 01:17:27,610
that's what keeps the protons and neutrons in the core

1163
01:17:29,580 --> 01:17:34,330
for the most part the weak and the strong force don't

1164
01:17:35,550 --> 01:17:37,050
in chemistry

1165
01:17:37,060 --> 01:17:42,690
with the exception for beta decay for some radioactive element that's where you need to

1166
01:17:42,690 --> 01:17:46,230
think about particular the weak force

1167
01:17:47,730 --> 01:17:50,130
gravity also

1168
01:17:50,210 --> 01:17:51,220
o gravity

1169
01:17:51,220 --> 01:17:52,840
actually has no

1170
01:17:52,860 --> 01:17:55,780
non consequences camp

1171
01:17:55,790 --> 01:18:01,750
i mean there is one but there is no known consequence

1172
01:18:01,760 --> 01:18:03,130
all of chemistry

1173
01:18:03,160 --> 01:18:05,320
tied up in the four

1174
01:18:05,330 --> 01:18:08,050
the electromagnetic force

1175
01:18:08,050 --> 01:18:11,560
now what i'm going to do is i'm going to simplify it a little bit

1176
01:18:11,560 --> 01:18:14,820
and just call this the coulomb force

1177
01:18:16,650 --> 01:18:19,830
and why is going to do that will be you will more obvious when you

1178
01:18:19,830 --> 01:18:24,400
get a o to call it the coulomb interaction

1179
01:18:24,410 --> 01:18:27,290
and we know what the the coulomb force law

1180
01:18:29,140 --> 01:18:30,780
the coulomb force law

1181
01:18:30,880 --> 01:18:36,580
it is found

1182
01:18:36,700 --> 01:18:39,430
i have a hard positive time

1183
01:18:39,430 --> 01:18:41,200
which is slightly

1184
01:18:41,240 --> 01:18:44,350
call for e

1185
01:18:44,370 --> 01:18:49,830
and at some distance are the distance now between the nucleus

1186
01:18:49,880 --> 01:18:52,300
and the negatively charged particles

1187
01:18:52,340 --> 01:18:54,790
which is my electron

1188
01:18:54,800 --> 01:18:56,840
i can describe

1189
01:18:56,920 --> 01:18:58,540
the four

1190
01:18:58,540 --> 01:19:00,980
of attraction between these

1191
01:19:01,000 --> 01:19:06,910
the electron the nucleus by the following expression that force

1192
01:19:07,000 --> 01:19:12,290
which is a function of the distance between the two charges

1193
01:19:12,310 --> 01:19:14,350
was just the man to

1194
01:19:14,370 --> 01:19:18,520
o of the charge and the negatively charged particles

1195
01:19:18,520 --> 01:19:23,620
nine the magnitude and the positively charged particles

1196
01:19:24,650 --> 01:19:27,560
four five one or

1197
01:19:27,580 --> 01:19:29,910
we're where

1198
01:19:30,510 --> 01:19:35,690
i'm going to treat this force just for simplicity purposes right now

1199
01:19:35,740 --> 01:19:37,030
a scalable

1200
01:19:37,040 --> 01:19:40,300
i'm not going to talk about the direction

1201
01:19:40,330 --> 01:19:42,640
but because this is

1202
01:19:42,690 --> 01:19:48,820
the force between a positive and negative charges an attractive force

1203
01:19:48,850 --> 01:19:52,350
that's one down here the steps one

1204
01:19:52,370 --> 01:19:54,180
permittivity of vacuum

1205
01:19:54,180 --> 01:19:56,180
it's in there for

1206
01:19:56,230 --> 01:19:58,830
doing our unit conversion correctly

1207
01:19:58,830 --> 01:20:01,600
morning everyone

1208
01:20:01,750 --> 01:20:03,970
collateral here primarily

1209
01:20:04,220 --> 01:20:08,440
accounting this tournament he is number students

1210
01:20:09,210 --> 01:20:11,130
has shown

1211
01:20:11,140 --> 01:20:16,180
we return to familiar story

1212
01:20:20,070 --> 01:20:22,270
this is part two

1213
01:20:22,380 --> 01:20:27,270
the empire strikes back so last time our adversary the

1214
01:20:27,280 --> 01:20:28,950
the graph

1215
01:20:28,950 --> 01:20:30,340
came to us with

1216
01:20:30,370 --> 01:20:33,920
the problem we had to find we had a source and we have a directed

1217
01:20:33,920 --> 01:20:36,200
graph with weights on the edges

1218
01:20:36,230 --> 01:20:38,330
and they were all nonnegative

1219
01:20:38,340 --> 01:20:43,580
and there was happiness and the we try and over the empire by designing dykstra

1220
01:20:43,600 --> 01:20:50,480
algorithm and you know very efficiently finding a single source for pastors pathway from s

1221
01:20:50,480 --> 01:20:53,440
to every other vertex today however

1222
01:20:53,450 --> 01:20:54,630
the death star

1223
01:20:54,650 --> 01:20:59,910
has a new tricks up its sleeve and we have a negative weights potentially we're

1224
01:20:59,910 --> 01:21:03,790
going to have to somehow deal with in particular negative weight cycles

1225
01:21:03,810 --> 01:21:06,200
and we saw that when we have a negative weight cycle we could just keep

1226
01:21:06,200 --> 01:21:09,190
going around and around and around and go back in time further and further and

1227
01:21:09,190 --> 01:21:13,130
further we can get to be arbitrarily far back in the past and so there's

1228
01:21:14,520 --> 01:21:18,400
there's no shortest path because whatever path you take a shorter one

1229
01:21:18,410 --> 01:21:21,050
so we want to address that issue today

1230
01:21:21,070 --> 01:21:23,480
and we're going to come up with

1231
01:21:23,490 --> 01:21:27,550
a new algorithm actually simpler than dykstra but not as fast

1232
01:21:27,570 --> 01:21:30,070
called the bellman ford algorithm

1233
01:21:30,120 --> 01:21:43,150
and it's going to allow negative weights and in some sense allow negative weight cycles

1234
01:21:43,160 --> 01:21:44,260
maybe not

1235
01:21:44,260 --> 01:21:48,540
much as you will find out

1236
01:21:48,570 --> 01:21:51,070
we have to leave room for a sequel

1237
01:21:52,150 --> 01:21:59,590
OK so the bellman ford algorithm invented by two guys as you might expect it

1238
01:22:00,880 --> 01:22:03,000
the shortest path weights

1239
01:22:03,020 --> 01:22:07,550
so it makes no assumption about the weights weights are arbitrary and

1240
01:22:07,580 --> 01:22:15,680
it's going to compute the shortest path weights remember this notation delta as comedy

1241
01:22:15,680 --> 01:22:18,710
it is that the weight of the shortest path from s to t

1242
01:22:18,710 --> 01:22:24,580
as it was called source vertex

1243
01:22:29,770 --> 01:22:36,640
and so we want to compute these weights for all vertices little

1244
01:22:36,650 --> 01:22:42,150
the claim is that computing from s everywhere

1245
01:22:42,180 --> 01:22:43,140
is known

1246
01:22:43,170 --> 01:22:47,550
harder than computing as to a particular location these so we're going to do for

1247
01:22:47,550 --> 01:22:48,470
all of the

1248
01:22:48,490 --> 01:22:50,300
still going to be the case here

1249
01:22:50,310 --> 01:22:52,710
and it allows negative weights

1250
01:22:52,960 --> 01:22:56,770
and this is the good case but there's an alternative

1251
01:22:56,780 --> 01:23:01,960
which is the bellman ford may just say well this is the negative weight cycle

1252
01:23:02,610 --> 01:23:05,460
and in that case it'll just say so

1253
01:23:05,580 --> 01:23:14,210
this is a negative weight cycle exists

1254
01:23:15,680 --> 01:23:17,830
some of the

1255
01:23:17,840 --> 01:23:21,310
some of these deltas or minus infinity

1256
01:23:21,310 --> 01:23:27,340
and that seems weird so bellman ford it is well presented today is intended for

1257
01:23:27,340 --> 01:23:30,810
the case where there are no negative weight cycles which is more intuitive

1258
01:23:30,830 --> 01:23:34,120
it's sort of allows them but it will just report them in that case it

1259
01:23:34,120 --> 01:23:38,340
will not give delta values you can change the to give delta values in that

1260
01:23:38,340 --> 01:23:39,890
case but we're not going to see

1261
01:23:39,890 --> 01:23:42,410
OK so

1262
01:23:42,490 --> 01:23:45,650
let's start with this

1263
01:23:45,660 --> 01:23:50,170
mini tutorial on line learning

1264
01:23:50,180 --> 01:23:54,830
this is i think therefore to school i i think in this area

1265
01:23:54,840 --> 01:23:58,540
sometimes it happens in other large deviations in the number of

1266
01:23:58,560 --> 01:24:03,160
courses and you give me and there was a a peak easier

1267
01:24:03,170 --> 01:24:07,400
so some of you may have seen these things may be interesting in our some

1268
01:24:07,490 --> 01:24:09,520
place a so

1269
01:24:09,860 --> 01:24:15,510
there's gonna be some overlapping with with what i said to bring in a couple

1270
01:24:15,510 --> 01:24:17,340
of months ago

1271
01:24:19,070 --> 01:24:23,620
the plan is to plan which is managed a little bit of ambitious assignments keep

1272
01:24:23,630 --> 01:24:25,010
some something

1273
01:24:25,020 --> 01:24:30,590
and this is meant to be a very simple and basic introduction to the themes

1274
01:24:30,810 --> 01:24:37,030
certain things in online learning will learn in particularly i will

1275
01:24:37,050 --> 01:24:39,550
look at the are binary classification

1276
01:24:40,350 --> 01:24:48,770
most of essentially all of online learning has to deal with it deals with linear

1277
01:24:49,930 --> 01:24:51,320
and the

1278
01:24:51,520 --> 01:24:56,290
simplest is the summit is the binary problem so that's why we will focus on

1279
01:24:56,670 --> 01:25:02,300
then i will see a few as a certain number of things on the perceptron

1280
01:25:03,190 --> 01:25:09,310
which is the most basic example of online in about for binary classification

1281
01:25:09,320 --> 01:25:15,160
and then we'll move on to analyse a little bit in detail in the case

1282
01:25:15,900 --> 01:25:24,440
linearly separable datasets and all the possible mistake bounds so that you can prove for

1283
01:25:24,440 --> 01:25:28,310
this specific sequence sequences of data

1284
01:25:28,350 --> 01:25:31,420
and the further we

1285
01:25:31,430 --> 01:25:36,050
want to discuss it with the relationship between online learning and convex optimisation and this

1286
01:25:36,050 --> 01:25:43,460
is something that has been booming in in the last two or three years

1287
01:25:43,500 --> 01:25:50,030
and i will say very few very simple things and so very basic and that

1288
01:25:50,400 --> 01:25:57,730
shows how few concepts basic concepts from convex optimization can help in the analysis of

1289
01:25:57,770 --> 01:26:02,200
then i will time permitting i will talk a bit about all the kernel based

1290
01:26:02,240 --> 01:26:09,410
on line learning especially memory bounded only conveys the learning and possibly

1291
01:26:09,420 --> 01:26:14,680
online SVM and active learning and maybe not outright two took over the

1292
01:26:15,310 --> 01:26:17,430
this last

1293
01:26:17,450 --> 01:26:19,710
part two which is the relation the weight

1294
01:26:19,730 --> 01:26:28,830
you can convert analysis four o four online algorithms to use classical statistical risk bound

1295
01:26:28,830 --> 01:26:29,740
for the

1296
01:26:30,260 --> 01:26:34,810
statistical learning model which is may be more familiar to many of you

1297
01:26:34,830 --> 01:26:37,720
OK let's start with the classification

1298
01:26:37,890 --> 01:26:44,260
so the basic scenario the mind when you talk about online classification is this one

1299
01:26:44,270 --> 01:26:45,650
you can think of a

1300
01:26:45,790 --> 01:26:49,370
a system for providing

1301
01:26:49,380 --> 01:26:51,580
online classification of text

1302
01:26:51,590 --> 01:26:55,900
documents so you have a stream of unlabelled the text

1303
01:26:55,920 --> 01:26:57,810
that documents that

1304
01:26:57,820 --> 01:27:03,540
are fed into classification system and every time a new document comes in the system

1305
01:27:03,540 --> 01:27:05,430
guesses labelled

1306
01:27:05,450 --> 01:27:09,260
which we think of as the binary labels of for instance is this document about

1307
01:27:10,770 --> 01:27:19,010
and that the label is given to the user that possibly provide feedback

1308
01:27:19,030 --> 01:27:22,600
and if they make a supposed to be the true the the ground truth for

1309
01:27:22,600 --> 01:27:24,500
the semantics of

1310
01:27:24,540 --> 01:27:30,570
for the category of that document and in in the most general case the label

1311
01:27:30,570 --> 01:27:32,650
is actually something that

1312
01:27:32,700 --> 01:27:35,410
the system asks explicitly to the user

1313
01:27:35,420 --> 01:27:40,240
so the most the most scenario is an active learning scenario in which the labels

1314
01:27:40,240 --> 01:27:46,170
might be requested or not but for the most part of this talk we will

1315
01:27:46,320 --> 01:27:49,270
think that the label is always provided by the user

1316
01:27:49,400 --> 01:27:53,080
so after each classification of the document to the user is providing the labels to

1317
01:27:53,080 --> 01:27:56,650
the to the true label the ground truth to the system so this might be

1318
01:27:56,960 --> 01:28:02,430
plausible or implausible depending on the specific application that you have in mind but i'll

1319
01:28:02,530 --> 01:28:07,210
argument about it so i want to talk about linear classifiers usually

1320
01:28:08,730 --> 01:28:11,620
i mean these things so we have

1321
01:28:11,630 --> 01:28:16,070
before the online model we have a stream of data instances

1322
01:28:16,770 --> 01:28:20,650
is is actually this this is our data

1323
01:28:20,660 --> 01:28:26,750
and these data instances are encoded as a vector scenario dimensional space

1324
01:28:26,760 --> 01:28:34,730
as usual for many applications for most the learning algorithms and actually for linear learning

1325
01:28:34,730 --> 01:28:35,960
algorithms let's say

1326
01:28:35,980 --> 01:28:41,460
and then there's four with each data instance there's is a binary label associated with

1327
01:28:41,470 --> 01:28:47,690
it which tells the true classification of the document or that they tighter so why

1328
01:28:47,690 --> 01:28:50,240
he is going to be the true label of xt

1329
01:28:52,230 --> 01:28:57,410
we will assume there is a linear learner maintains something classifier

1330
01:28:57,430 --> 01:29:03,210
wt minus one which predicts the label of the current instance with by taking this

1331
01:29:03,460 --> 01:29:07,400
the sign of the sign of the inner product between

1332
01:29:07,410 --> 01:29:10,220
the with parameter and the current system

1333
01:29:10,230 --> 01:29:14,490
OK so that's the picture and of course you you know them

1334
01:29:14,540 --> 01:29:18,380
you might know this notion of margin which he's

1335
01:29:18,400 --> 01:29:26,060
can be associated with a certain with the confidence of classification of a certain except

1336
01:29:26,670 --> 01:29:28,990
given we parameter w

1337
01:29:29,010 --> 01:29:35,140
and so the margin is just the distance between the distance of the

1338
01:29:35,150 --> 01:29:42,670
let the tip of x point x to the hyperplane that defines that separates the

1339
01:29:42,670 --> 01:29:44,240
positive classification of w

1340
01:29:44,240 --> 01:29:48,020
if you can see this can read this it's like a power law with exponential

1341
01:29:48,230 --> 01:29:52,970
exp with exponential cutoff right so now i'm blocking access to the minus itself five

1342
01:29:52,980 --> 01:29:56,770
times e to the minus x lambda

1343
01:29:56,790 --> 01:30:00,950
and you know you can see now i feel i feel my data perfectly that's

1344
01:30:00,950 --> 01:30:01,900
on in this case

1345
01:30:01,960 --> 01:30:06,860
the value of the exponent doesn't basically doesn't change what changes as here i get

1346
01:30:06,860 --> 01:30:08,920
this exponential cutoff thing

1347
01:30:08,980 --> 01:30:09,610
coming in

1348
01:30:09,650 --> 01:30:13,200
so the point of all these being is

1349
01:30:14,910 --> 01:30:17,460
one has to be careful how to how to fit

1350
01:30:17,780 --> 01:30:20,810
we estimate power law degree distributions

1351
01:30:21,920 --> 01:30:23,220
and the last

1352
01:30:23,240 --> 01:30:28,300
picture shown is this is an example so both these networks have the same number

1353
01:30:28,300 --> 01:30:31,390
of nodes in the same number of edges this is how

1354
01:30:31,450 --> 01:30:34,650
the network will look like if the network would be random and this is how

1355
01:30:34,650 --> 01:30:36,980
it looks if it's a power law or is

1356
01:30:37,090 --> 01:30:41,070
the other if it has the power law degree distribution or any other sort of

1357
01:30:41,120 --> 01:30:46,420
foot is that things is called scale free networks and just looking at the sort

1358
01:30:46,420 --> 01:30:47,820
of you can see that

1359
01:30:47,840 --> 01:30:51,100
in scale free networks try to have a lot of nodes have degree one is

1360
01:30:51,100 --> 01:30:52,080
sort of a few

1361
01:30:52,120 --> 01:30:55,030
you know this hubs where everyone is connected

1362
01:30:55,110 --> 01:30:59,950
and you know webgraphs web graph looks like that and doesn't look like this

1363
01:30:59,960 --> 01:31:04,040
and then i'll tell you later what is the consequence is that since the graph

1364
01:31:04,040 --> 01:31:06,830
when graph looks like this what can i do that i could do if the

1365
01:31:06,830 --> 01:31:08,350
web would be like that

1366
01:31:09,370 --> 01:31:13,040
so now we can ask OK what kind of simple mechanisms

1367
01:31:13,040 --> 01:31:14,340
would generate

1368
01:31:14,410 --> 01:31:18,720
networks of such structures and not such structure

1369
01:31:18,720 --> 01:31:20,870
it's sort of

1370
01:31:20,890 --> 01:31:25,370
the ingredients that make a lot of this model ceased this one can use the

1371
01:31:25,370 --> 01:31:29,410
following so what we think of is that i have a network lowers becoming one

1372
01:31:29,410 --> 01:31:32,970
of the time so a new knowledge economy to create and link next to the

1373
01:31:32,970 --> 01:31:35,810
existing nodes of the network so now the question is

1374
01:31:35,850 --> 01:31:39,830
what kind of simple mechanism can this node use

1375
01:31:40,520 --> 01:31:45,150
basically the idea is to have this preferential attachment with the idea is that

1376
01:31:45,170 --> 01:31:49,750
the probability of linking to a particular node is proportional to the degree so basically

1377
01:31:49,840 --> 01:31:52,930
is proportional to the number of connections the node already has

1378
01:31:52,950 --> 01:31:56,450
so for example on the web what this corresponds to is to say if i

1379
01:31:56,450 --> 01:31:57,960
create a new web page

1380
01:31:58,020 --> 01:32:03,140
how likely you might link to some you know small degree that page versus i

1381
01:32:03,140 --> 01:32:06,080
don't agree with the outcome of something that already has a lot of things right

1382
01:32:06,080 --> 01:32:09,980
so yes we are more likely to link to hide the web page or in

1383
01:32:10,530 --> 01:32:15,350
citation networks you could ask what was basically discovered a long time ago in sixty

1384
01:32:15,350 --> 01:32:20,980
five was the proportion of new citations of paper gets is basically proportional to proportional

1385
01:32:21,000 --> 01:32:22,640
to the number of citations

1386
01:32:22,660 --> 01:32:26,660
it already has so the idea is if you have a lot of citations you

1387
01:32:26,750 --> 01:32:30,590
acquire more while if you have few you sort of applied less and less

1388
01:32:30,600 --> 01:32:36,820
and this is this model is based on this herb simon result the basic he

1389
01:32:36,820 --> 01:32:37,730
called it

1390
01:32:37,850 --> 01:32:39,930
basically that power laws

1391
01:32:40,260 --> 01:32:45,240
much from his rich get rich get rich get richer or cumulative advantage principle where

1392
01:32:45,280 --> 01:32:48,720
the idea is sort of the more than more you have

1393
01:32:48,740 --> 01:32:52,600
the more you know the more you are going to have to write like probability

1394
01:32:52,600 --> 01:32:56,120
of u getting a link is proportional to how many links you already have so

1395
01:32:56,120 --> 01:32:59,180
the more things you can the higher probability of having

1396
01:32:59,530 --> 01:33:01,780
and what you can prove is that

1397
01:33:01,810 --> 01:33:07,660
such mechanism when applied in the network case we give you graphs that have probably

1398
01:33:07,680 --> 01:33:10,890
degree distributions with exponent of three

1399
01:33:11,700 --> 01:33:18,180
and what should they also say is that there are many flavors images preferential attachment

1400
01:33:18,180 --> 01:33:20,300
comes into

1401
01:33:20,310 --> 01:33:25,010
in two modelling networks and there are many extensions so one drawback of sort of

1402
01:33:25,010 --> 01:33:29,780
the the the process that i just described is that for example the only knowledge

1403
01:33:29,780 --> 01:33:33,260
we have advantage right sort of the older than out the longer it is in

1404
01:33:33,260 --> 01:33:37,620
the network the more likely it is to attract edges so he knows will tend

1405
01:33:37,620 --> 01:33:41,800
to have higher degrees than sort of nodes are joined the in the network later

1406
01:33:41,800 --> 01:33:43,280
and you can

1407
01:33:43,280 --> 01:33:47,910
you can forget to get rid of this by introducing something that the people call

1408
01:33:47,930 --> 01:33:51,810
note fitness when the idea is that you have some kind of a priority stickiness

1409
01:33:51,810 --> 01:33:55,620
of often also on outcomes with simply assigned stickiness in this way we can sort

1410
01:33:55,620 --> 01:33:56,990
of make up

1411
01:33:57,120 --> 01:34:00,780
forty four for the fact that it is younger not particularly old

1412
01:34:00,800 --> 01:34:07,180
the other sort of different model that came from a more computer science

1413
01:34:07,200 --> 01:34:11,850
he point of view is is is something that's called the copying model basically the

1414
01:34:11,850 --> 01:34:12,830
idea is

1415
01:34:12,890 --> 01:34:18,410
that you know realization that picking up node proportional to to the degree is basically

1416
01:34:18,410 --> 01:34:22,970
the same speaking and an edge in the network right because i males have more

1417
01:34:22,970 --> 01:34:24,970
edges adjacent to them

1418
01:34:24,970 --> 01:34:28,280
and then how this model which is to say and you know comes

1419
01:34:28,330 --> 01:34:34,070
it selects uniformly at random at a particular node then it links to its neighbour

1420
01:34:34,080 --> 01:34:37,950
basically you are getting the same effect as if you would be

1421
01:34:37,990 --> 01:34:39,260
making these guys

1422
01:34:39,260 --> 01:34:42,570
proportional to their degree

1423
01:34:42,580 --> 01:34:44,260
two a

1424
01:34:44,280 --> 01:34:48,410
so what you can also do then is to ask this preferential attachment really hold

1425
01:34:48,430 --> 01:34:52,600
so if i would have some network data when i see how edges appear over

1426
01:34:52,600 --> 01:34:53,850
time meaning that you know

1427
01:34:53,890 --> 01:34:57,610
i get to see exactly the edge arrival sequence of of the

1428
01:34:58,100 --> 01:35:02,800
just in the network then i can directly observe the mechanisms that drive

1429
01:35:02,930 --> 01:35:04,810
evolution or creation of the network

1430
01:35:05,760 --> 01:35:09,720
we were able to get this for for online social networks in flickr

1431
01:35:09,720 --> 01:35:11,910
delicious answers the game

1432
01:35:11,910 --> 01:35:16,300
but basically in all of this we have users that people and to create links

1433
01:35:16,300 --> 01:35:20,160
among themselves and for every link you know the exact moment when the link to

1434
01:35:20,160 --> 01:35:21,180
link up here

1435
01:35:21,200 --> 01:35:24,510
so what we can do then is you can on the road this through evolution

1436
01:35:24,510 --> 01:35:27,080
of the network and whenever a new edge

1437
01:35:27,260 --> 01:35:30,660
yes you can ask what was the degree of the target where the note that

1438
01:35:30,660 --> 01:35:32,700
the edge attached to

1439
01:35:32,700 --> 01:35:34,490
and here is the massive graphs

1440
01:35:34,550 --> 01:35:39,260
so what now and such go to explain it so basically the exercise side of

1441
01:35:39,260 --> 01:35:44,010
this is the degree of the target node and this is the probability of linking

1442
01:35:44,030 --> 01:35:45,490
one of the

1443
01:35:45,580 --> 01:35:51,830
OK so gnp so not in a graph where i think we just created uniformly

1444
01:35:51,830 --> 01:35:53,410
at random then

1445
01:35:53,430 --> 01:35:56,640
the probability of an edge does not depend on the degree of the node right

1446
01:35:56,640 --> 01:35:58,350
regardless of what is your degree

1447
01:35:58,370 --> 01:36:01,570
the probability of getting a match is the same with all the way right so

1448
01:36:01,570 --> 01:36:04,840
good evening

1449
01:36:04,840 --> 01:36:07,570
can you give me the back

1450
01:36:07,850 --> 01:36:13,870
thank you for these kind words and i'm very pleased to be here and to

1451
01:36:13,930 --> 01:36:17,520
i think this the beginning we started a few minutes early

1452
01:36:17,530 --> 01:36:19,910
i'm very pleased to be here and two

1453
01:36:19,910 --> 01:36:22,390
you will little bit of historical perspective

1454
01:36:22,390 --> 01:36:25,450
on data clustering it's not a new field

1455
01:36:25,460 --> 01:36:27,910
and the large number of

1456
01:36:27,910 --> 01:36:29,930
people in different disciplines

1457
01:36:29,930 --> 01:36:34,800
who have been interested in data clustering i was first exposed to data clustering in

1458
01:36:34,800 --> 01:36:37,590
nineteen seventy when i first took my

1459
01:36:37,650 --> 01:36:42,260
graduate course in pattern recognition at that time the classical book by two down heart

1460
01:36:42,260 --> 01:36:43,060
was not

1461
01:36:43,170 --> 01:36:47,060
yet available it was published in nineteen seventy two but he had the

1462
01:36:47,090 --> 01:36:49,190
the draft form of the book

1463
01:36:49,190 --> 01:36:53,300
and i learned something from about data clustering from that book

1464
01:36:53,310 --> 01:36:59,550
and it's still one of the classic books are so like this information

1465
01:37:03,690 --> 01:37:07,910
perhaps give a slightly different perspective on

1466
01:37:11,660 --> 01:37:15,820
probably not

1467
01:37:15,900 --> 01:37:17,650
about all the different

1468
01:37:17,710 --> 01:37:20,690
one which has been done in this area

1469
01:37:20,710 --> 01:37:23,240
i'll do my best

1470
01:37:23,260 --> 01:37:26,320
so as it so happens

1471
01:37:26,320 --> 01:37:27,980
about two months ago

1472
01:37:27,990 --> 01:37:30,460
and archaeologist contacted me

1473
01:37:30,510 --> 01:37:32,510
about doing some analysis

1474
01:37:32,510 --> 01:37:37,180
of the images in and for what i'm not sure how many of you know

1475
01:37:37,180 --> 01:37:40,820
about or what these beautiful temples in cambodia

1476
01:37:40,820 --> 01:37:44,040
which were lost after the coming kingdom

1477
01:37:44,040 --> 01:37:45,740
i went out

1478
01:37:45,760 --> 01:37:47,980
were discovered in the

1479
01:37:48,020 --> 01:37:51,880
about eighty hundreds by by the friends who were

1480
01:37:51,930 --> 01:37:54,130
who had occupied cambodia

1481
01:37:54,190 --> 01:37:57,550
and since then many countries japan

1482
01:37:57,570 --> 01:37:59,570
franz germany

1483
01:37:59,590 --> 01:38:06,240
as well as united nations investing huge amount of money to reconstruct these temples

1484
01:38:06,260 --> 01:38:08,380
it's even been featured in the

1485
01:38:08,380 --> 01:38:10,760
in the action thriller

1486
01:38:10,780 --> 01:38:13,380
a tomb raider by

1487
01:38:13,410 --> 01:38:16,750
the lower graph

1488
01:38:16,810 --> 01:38:20,250
so the problem pose the following

1489
01:38:20,290 --> 01:38:22,510
the temple contains

1490
01:38:22,510 --> 01:38:25,190
i was there about a year and a half ago so

1491
01:38:25,220 --> 01:38:27,440
i remember the temple quite well

1492
01:38:27,450 --> 01:38:31,010
it contains a unique value of about two thousand

1493
01:38:31,850 --> 01:38:35,190
who have been depicted in full-body portraits

1494
01:38:36,690 --> 01:38:41,260
the full body portrait can see as well as some of the facial images

1495
01:38:41,280 --> 01:38:47,000
and this archaeologist and wanted to know what facial bites represented in these

1496
01:38:47,010 --> 01:38:51,570
portraits and contacted the number of face recognition companies

1497
01:38:51,580 --> 01:38:55,250
and then he was directed to us because we have been doing some face recognition

1498
01:38:56,420 --> 01:38:59,200
so the problem is basically to understand

1499
01:38:59,260 --> 01:39:05,760
what kind of fish types represented at the commander of the high bar no or

1500
01:39:05,810 --> 01:39:08,230
or whatever it is

1501
01:39:08,290 --> 01:39:11,480
so this was the problem of how to we cluster the

1502
01:39:11,540 --> 01:39:14,700
the answer of faces up to rise a

1503
01:39:14,730 --> 01:39:18,080
sounds to me for goddess of theory

1504
01:39:18,100 --> 01:39:20,570
and one of the first things we did was to

1505
01:39:20,580 --> 01:39:22,760
identify landmarks

1506
01:39:22,780 --> 01:39:23,910
on the fission

1507
01:39:24,950 --> 01:39:28,440
and then shape alignment in order to find

1508
01:39:28,480 --> 01:39:29,700
the similarity

1509
01:39:29,720 --> 01:39:31,480
between two faces

1510
01:39:31,540 --> 01:39:36,110
and once you have a similarity between two faces can start doing the grouping and

1511
01:39:36,110 --> 01:39:38,330
in many ways in which we can compute the

1512
01:39:38,350 --> 01:39:43,320
similarity between two vectors can compute the euclidean distance between the points system

1513
01:39:43,330 --> 01:39:45,380
there were templates line

1514
01:39:45,590 --> 01:39:48,040
energy model in order to to

1515
01:39:48,470 --> 01:39:52,030
to align the two sets of points and then you can do in this particular

1516
01:39:52,130 --> 01:39:53,970
case started out with ten

1517
01:39:53,980 --> 01:39:59,070
ten facial images and you can construct what is called a dendrogram using the singling

1518
01:39:59,070 --> 01:40:01,890
out more completely

1519
01:40:01,910 --> 01:40:03,190
and then you can

1520
01:40:03,220 --> 01:40:08,290
so that showed some similarity value in order to find clusters so if i look

1521
01:40:08,310 --> 01:40:09,760
at single link

1522
01:40:09,780 --> 01:40:11,450
clustering algorithm

1523
01:40:11,470 --> 01:40:15,100
i can see that there are there is a cluster clusters shown in green

1524
01:40:15,130 --> 01:40:16,480
brits is

1525
01:40:16,540 --> 01:40:22,010
which is this one two nine ten cluster consisting of three insects

1526
01:40:22,020 --> 01:40:26,240
and the cluster consisting of seven and eight and four five don't seem to form

1527
01:40:26,330 --> 01:40:27,460
in two and

1528
01:40:27,470 --> 01:40:33,370
distinct clusters with the point is that these two different approaches of group starting with

1529
01:40:33,380 --> 01:40:36,220
the same similarity matrix in

1530
01:40:36,250 --> 01:40:38,320
very different

1531
01:40:39,680 --> 01:40:45,120
next thing he said is there another way of obtaining a similarity matrix

1532
01:40:45,170 --> 01:40:46,010
and this

1533
01:40:46,030 --> 01:40:51,670
can be done by using the well-known concept called ordination of multidimensional scaling which was

1534
01:40:51,670 --> 01:40:52,870
proposed in

1535
01:40:52,880 --> 01:40:58,250
in nineteen fifty or nineteen sixties bell labs and the idea is given in the

1536
01:40:58,250 --> 01:41:01,550
similarity matrix like in and these points

1537
01:41:01,570 --> 01:41:07,300
in some metric space and this case you ten points over ten by ten similarity

1538
01:41:08,320 --> 01:41:13,190
and i can start the system by the similarity matrix and i can embed them

1539
01:41:13,330 --> 01:41:20,520
two-dimensional space you can embed them in four dimensional five but for visualisation purposes i

1540
01:41:20,530 --> 01:41:22,390
three-dimensional space

1541
01:41:22,400 --> 01:41:25,610
so these are the points on and i apply

1542
01:41:25,670 --> 01:41:26,880
the well known

1543
01:41:26,890 --> 01:41:29,440
clustering algorithm to the data

1544
01:41:29,450 --> 01:41:32,030
and then i get three

1545
01:41:32,270 --> 01:41:34,030
for last year

1546
01:41:34,570 --> 01:41:37,930
shown here in terms of these classes

1547
01:41:37,970 --> 01:41:42,030
fairly well that's one of the clusters optimal singling

1548
01:41:43,320 --> 01:41:46,650
so the sound a little bit and added top the

1549
01:41:46,700 --> 01:41:50,530
the knowledge of the things that this grouping seems to be

1550
01:41:50,700 --> 01:41:53,090
to be somehow consistent

1551
01:41:53,110 --> 01:41:56,930
with what he thinks in terms of the facial patterns and of

1552
01:41:56,970 --> 01:41:59,190
getting a database of two thousand

1553
01:41:59,240 --> 01:42:02,050
visual images and we will try to continue

1554
01:42:02,080 --> 01:42:03,740
this work

1555
01:42:04,440 --> 01:42:07,070
this is just one example of where

1556
01:42:09,330 --> 01:42:10,570
tools like

1557
01:42:10,580 --> 01:42:13,690
cluster analysis in order to to analyse the data

1558
01:42:13,690 --> 01:42:16,800
but a large variety of problems

1559
01:42:16,810 --> 01:42:18,560
we need to

1560
01:42:18,640 --> 01:42:21,760
i and effectively

1561
01:42:21,780 --> 01:42:23,470
utilize this data

1562
01:42:23,520 --> 01:42:25,210
and in order to to do that

1563
01:42:25,220 --> 01:42:27,800
we need tools for data visualization

1564
01:42:27,840 --> 01:42:29,720
and categorisation

1565
01:42:29,750 --> 01:42:32,310
and this is the in the form of

1566
01:42:32,360 --> 01:42:37,760
that's lots of the form images and video the amount of data which is becoming

1567
01:42:37,760 --> 01:42:40,510
available is huge and in order to

1568
01:42:40,520 --> 01:42:42,760
i understand or interpret data

1569
01:42:42,810 --> 01:42:44,400
we need some tools

1570
01:42:44,460 --> 01:42:45,680
and this is where

1571
01:42:45,690 --> 01:42:48,720
the field of exploratory data analysis

1572
01:42:48,760 --> 01:42:53,610
got started to was one of the better-known statistician who was

1573
01:42:53,620 --> 01:42:59,140
one of the first proponents of the sort of basically point system exploratory data analysis

1574
01:42:59,240 --> 01:43:02,270
a collection of techniques to gain insight into the data

1575
01:43:02,370 --> 01:43:08,400
and what underlying structural generate hypotheses detect anomalies are outliers

1576
01:43:08,400 --> 01:43:09,960
solved problem

1577
01:43:09,980 --> 01:43:13,420
OK so how are we going to do this we put faces of people as

1578
01:43:13,440 --> 01:43:17,100
the columns of the data images within applied this the algorithm

1579
01:43:17,110 --> 01:43:21,400
and the algorithm reason will remove all the defects we see in in faces of

1580
01:43:21,400 --> 01:43:27,000
images that renders the recognition problematic so if we applied to and three images like

1581
01:43:27,770 --> 01:43:31,870
and what we get is we get you know clean face in the middle

1582
01:43:31,870 --> 01:43:35,600
and then we get all season on the idea it is like cast shadows specular

1583
01:43:35,600 --> 01:43:39,130
points that show up in the sparse component

1584
01:43:39,210 --> 01:43:42,880
in practice to make it work though

1585
01:43:42,900 --> 01:43:47,330
it's not true that the major faces will be exactly around because to be lower

1586
01:43:47,330 --> 01:43:49,310
and they have to be well aligned

1587
01:43:49,310 --> 01:43:51,100
and so

1588
01:43:51,130 --> 01:43:54,540
the next slide is to say which is due to my colleague and now i

1589
01:43:54,540 --> 01:43:58,650
have not much to do with the slide you're about to see the next slide

1590
01:43:58,670 --> 01:44:00,250
is to say well

1591
01:44:00,250 --> 01:44:06,000
it's only after alignment that you have low rank plus sparse structure so we have

1592
01:44:06,000 --> 01:44:07,960
to find the alignment as well

1593
01:44:08,000 --> 01:44:12,750
and so the idea is quite simply says well let's try to find

1594
01:44:12,750 --> 01:44:17,250
and not an alignment that is the transformation so that after transformation my data set

1595
01:44:17,250 --> 01:44:20,520
is as low rank plus sparse as possible

1596
01:44:20,540 --> 01:44:24,350
can we can just now we leave the complex world because the fact that we

1597
01:44:24,350 --> 01:44:29,130
have to deal find is defamation is is not convex but we can try to

1598
01:44:29,130 --> 01:44:30,580
do things anyway

1599
01:44:32,520 --> 01:44:33,170
all right

1600
01:44:33,190 --> 01:44:35,270
so just to show you in principle

1601
01:44:35,270 --> 01:44:40,540
the interest of these methods let's suppose look at the building from a different angle

1602
01:44:40,540 --> 01:44:42,170
and then

1603
01:44:42,170 --> 01:44:45,880
OK in front of it have trees and so on some shots another shot it's

1604
01:44:45,880 --> 01:44:51,410
not enough to try to apply this is find the defamation so that defamation and

1605
01:44:51,410 --> 01:44:55,560
low income and low rank plus sparse as possible and to show you the results

1606
01:44:55,560 --> 01:45:00,650
here the result is is the way she transforms the images this is a low

1607
01:45:00,650 --> 01:45:06,060
rank component which is the building unoccluded and sparse component which is of course this

1608
01:45:06,060 --> 01:45:08,270
tree including the building

1609
01:45:08,350 --> 01:45:09,980
so it seems to work

1610
01:45:10,020 --> 01:45:15,480
quite well in practice so people have done e and his colleagues have done a

1611
01:45:15,480 --> 01:45:19,210
lot of work and again it's not mine anymore what you see is you take

1612
01:45:19,210 --> 01:45:22,520
people the faces of of people

1613
01:45:22,540 --> 01:45:25,170
this faces are misaligned as you can see

1614
01:45:25,190 --> 01:45:28,310
and then they are corrupted so i don't see if you see patches that are

1615
01:45:28,310 --> 01:45:31,980
in front of the eyes his nose and so on and so you're going to

1616
01:45:31,980 --> 01:45:35,480
try to align them so you run this algorithm trying to find for each image

1617
01:45:35,480 --> 01:45:39,020
deformations that as a whole you've got as much low rank plus sparse structure as

1618
01:45:41,380 --> 01:45:43,460
and so you see the output

1619
01:45:44,270 --> 01:45:50,480
the data matrix of the low component is that actually perfectly almost perfectly aligned those

1620
01:45:50,480 --> 01:45:56,210
images and of course removed those occlusion that include eyes nose mouth

1621
01:45:56,250 --> 01:45:57,940
and so on and so forth

1622
01:45:58,000 --> 01:46:02,650
here is the thing that i find the most

1623
01:46:02,750 --> 01:46:08,690
i don't actually almost check it before i can talk about that here we so

1624
01:46:08,690 --> 01:46:11,920
what they've done in this case it took pictures of the guy

1625
01:46:12,040 --> 01:46:14,110
under serious

1626
01:46:14,650 --> 01:46:20,880
conditions so is the same person but he is wearing sunglasses here is the car

1627
01:46:21,040 --> 01:46:24,920
is has something in front of his mouth he has incentive in from his wearing

1628
01:46:24,920 --> 01:46:28,420
hat he again and so on

1629
01:46:28,480 --> 01:46:31,560
and then you apply designers and so we're not going to do in face recognition

1630
01:46:31,560 --> 01:46:34,850
on something like it's going to be very difficult so let's use this as a

1631
01:46:34,850 --> 01:46:36,650
preprocessing step

1632
01:46:36,650 --> 01:46:43,130
so use as a pre processing step you the alignment alone can sparse component simultaneously

1633
01:46:43,170 --> 01:46:46,100
and what you get out of these cells the low rank component that is sparse

1634
01:46:46,100 --> 01:46:52,400
component and so you see all these non idealogies sunglasses issue here then you impute

1635
01:46:52,400 --> 01:46:53,460
the eyes

1636
01:46:53,520 --> 01:46:58,980
so hats showing here you know you've removed had removed the whig you remove the

1637
01:46:58,980 --> 01:47:01,190
hand in front of your mouse and so now

1638
01:47:01,210 --> 01:47:04,670
perhaps you can see this to face recognition

1639
01:47:07,080 --> 01:47:12,250
as you know if you do for example if you retrieve images of the internet

1640
01:47:12,380 --> 01:47:16,830
neural you do averaging of faces to get the typical average phase before you do

1641
01:47:16,830 --> 01:47:24,420
alignment and repairing you gonna get extraordinarily blurry pictures it's very hard to recognise people

1642
01:47:24,560 --> 01:47:28,870
but if you to applies alignment and repairing the entity performs is low rank plus

1643
01:47:28,870 --> 01:47:35,650
sparse decomposition before you take averages then all of a sudden everything gets much much

1644
01:47:36,730 --> 01:47:39,020
because you align the right stuff

1645
01:47:39,040 --> 01:47:41,540
you have the right stuff

1646
01:47:41,540 --> 01:47:46,850
of course you can use it for other applications as well

1647
01:47:46,870 --> 01:47:51,480
you can say well you know this is an digit recognition problem you know this

1648
01:47:51,480 --> 01:47:55,400
is the way people could ride number three you could say well what if i

1649
01:47:55,400 --> 01:47:57,920
my name is high low

1650
01:47:57,920 --> 01:48:00,620
and from new york university of torrent all

1651
01:48:00,680 --> 01:48:02,070
and the

1652
01:48:02,080 --> 01:48:08,050
people going to pretend is uncorrelated multilinear principal component analysis

1653
01:48:08,060 --> 01:48:13,560
three successive variance maximisation

1654
01:48:13,580 --> 01:48:18,630
so here is the outline of my talk study the motivation of this work

1655
01:48:18,630 --> 01:48:20,510
and then i propose the

1656
01:48:20,520 --> 01:48:23,700
uncorrelated multilinear PCA anger angry them

1657
01:48:23,720 --> 01:48:24,880
and that's the

1658
01:48:24,880 --> 01:48:27,820
experimental evaluations we've be presented

1659
01:48:27,840 --> 01:48:28,910
and finally

1660
01:48:28,920 --> 01:48:31,740
i we try to conclude it

1661
01:48:31,750 --> 01:48:35,810
so here is the data that we consider in this work

1662
01:48:35,850 --> 01:48:41,090
so it is tensorial data it tends to have the various definitions in very few

1663
01:48:41,140 --> 01:48:43,810
the cancer in this work we refer to

1664
01:48:43,930 --> 01:48:45,910
multidimensional arrays

1665
01:48:45,920 --> 01:48:51,520
and the generalisation of vector and metric concepts so

1666
01:48:51,600 --> 01:48:56,020
yes there and most we call it and sold it has

1667
01:48:56,060 --> 01:48:57,320
so better

1668
01:48:57,340 --> 01:49:03,540
in the first order has and that is a second order tensor

1669
01:49:03,560 --> 01:49:05,320
and the tensorial data

1670
01:49:05,370 --> 01:49:09,180
it has a wide range of applications

1671
01:49:09,260 --> 01:49:12,740
such as applications involving images

1672
01:49:12,790 --> 01:49:14,280
we use it this

1673
01:49:14,290 --> 01:49:17,510
and stream these all data mining data

1674
01:49:17,540 --> 01:49:19,130
and here i chose

1675
01:49:19,150 --> 01:49:23,980
two example of tensors the first one is a second order tensor

1676
01:49:24,040 --> 01:49:26,340
it has two most

1677
01:49:26,340 --> 01:49:30,510
the color special column understand vision role models

1678
01:49:30,520 --> 01:49:32,370
the the typical

1679
01:49:32,400 --> 01:49:34,480
graylevel CC image

1680
01:49:34,540 --> 01:49:36,920
and the second one

1681
01:49:36,930 --> 01:49:43,590
it's the third order tensor which is the binary silhouettes its gates

1682
01:49:43,620 --> 01:49:45,180
and has three modes

1683
01:49:45,230 --> 01:49:47,630
the spatial color vision rules

1684
01:49:47,650 --> 01:49:48,990
at the time of

1685
01:49:49,090 --> 01:49:53,280
so in the the second order tensor

1686
01:49:53,320 --> 01:49:56,210
so the problem we have it

1687
01:49:56,230 --> 01:50:00,590
mention entity reductions so why do we want to reduce the dimension of the hands

1688
01:50:00,620 --> 01:50:03,910
of this value and a very high dimensional

1689
01:50:03,960 --> 01:50:08,180
this leads to the problem of true of dimensionality

1690
01:50:08,240 --> 01:50:10,180
so why this is so

1691
01:50:10,190 --> 01:50:16,260
it's true because it's still this kind of high dimensional data is computationally very expensive

1692
01:50:16,260 --> 01:50:17,930
to handle

1693
01:50:17,940 --> 01:50:19,620
and many classifiers

1694
01:50:19,640 --> 01:50:21,240
they perform very poorly

1695
01:50:21,240 --> 01:50:24,660
in high dimensional space given the small number of

1696
01:50:24,710 --> 01:50:29,490
ten years and the which is of often two in practice

1697
01:50:29,520 --> 01:50:30,940
but in fact

1698
01:50:30,960 --> 01:50:34,210
if you look at a cost of tens of death

1699
01:50:34,250 --> 01:50:37,560
they are mostly highly constrained to a subspace

1700
01:50:37,590 --> 01:50:40,090
a manifold of intrinsically

1701
01:50:40,120 --> 01:50:43,590
low dimensional like the face the game they also

1702
01:50:43,610 --> 01:50:45,810
constant right

1703
01:50:46,740 --> 01:50:49,810
dimensionality to popular methods

1704
01:50:49,810 --> 01:50:51,810
and they also

1705
01:50:51,830 --> 01:50:54,280
some kind of feature extraction

1706
01:50:54,280 --> 01:50:57,590
the mission of we transform this high dimensional

1707
01:50:58,490 --> 01:51:01,640
into a low dimensional space

1708
01:51:01,650 --> 01:51:03,560
well this information

1709
01:51:03,590 --> 01:51:05,440
raise up to two time two

1710
01:51:05,490 --> 01:51:09,740
retain most of the the underlying structure

1711
01:51:09,750 --> 01:51:16,800
and the focus of this work is unsupervised dimensionality reduction and in particular PCA based

1712
01:51:16,800 --> 01:51:21,940
angry that using the well-known linear methods to produce

1713
01:51:22,020 --> 01:51:23,900
uncorrelated features

1714
01:51:23,910 --> 01:51:29,330
it retain as much as possible the variations of the original data

1715
01:51:29,340 --> 01:51:32,090
but there's one

1716
01:51:32,150 --> 01:51:35,120
one thing we should be aware that PC

1717
01:51:35,180 --> 01:51:37,990
the lingam method who need to reshape

1718
01:51:38,020 --> 01:51:40,870
this time ten tournament to

1719
01:51:41,800 --> 01:51:46,300
are you will hire you need to reshape the into vector

1720
01:51:46,310 --> 01:51:48,470
so this will result in a very high

1721
01:51:50,590 --> 01:51:54,140
the two high computational and memory demand

1722
01:51:54,150 --> 01:51:59,620
and another thing that this kind of receiving the really great natural structure in original

1723
01:52:01,520 --> 01:52:04,280
so this kind of of

1724
01:52:04,280 --> 01:52:07,240
to the development of marketing year

1725
01:52:07,240 --> 01:52:08,310
and rhythms

1726
01:52:08,330 --> 01:52:09,860
the track features

1727
01:52:09,870 --> 01:52:11,540
directly from the

1728
01:52:11,570 --> 01:52:18,610
tensor objects from the tensor representation so here are some of the existing work that

1729
01:52:18,610 --> 01:52:20,380
is around one

1730
01:52:21,530 --> 01:52:23,550
it's probably in two thousand one

1731
01:52:23,580 --> 01:52:27,460
and the two dimensional PCA it in two thousand four

1732
01:52:27,470 --> 01:52:31,130
and the generalised to low rank approximation of matrices

1733
01:52:31,170 --> 01:52:33,330
which is i think the best the

1734
01:52:33,340 --> 01:52:34,900
good to know what people

1735
01:52:35,020 --> 01:52:37,450
i think that about two thousand five

1736
01:52:37,460 --> 01:52:39,210
and of the generalized

1737
01:52:39,520 --> 01:52:40,550
he is

1738
01:52:40,580 --> 01:52:46,110
and the other one is the cold concurrent subspaces analysis

1739
01:52:46,130 --> 01:52:49,400
it's true proposed in two thousand five

1740
01:52:49,470 --> 01:52:50,860
and another one

1741
01:52:50,910 --> 01:52:57,890
is martin it he's a probably eight is really

1742
01:52:57,900 --> 01:53:02,980
the difference between this

1743
01:53:03,210 --> 01:53:09,570
in this monument at the linear methods that one property that these kind of monuments

