1
00:00:00,000 --> 00:00:05,060
to press one thing in order to get food somewhere else is completely crazy for

2
00:00:05,060 --> 00:00:06,500
active learning

3
00:00:06,520 --> 00:00:09,890
and still they managed to learning

4
00:00:09,970 --> 00:00:12,620
OK it's just that continue for a long time

5
00:00:12,640 --> 00:00:23,580
so how is this related to reinforcement learning

6
00:00:23,600 --> 00:00:25,140
so the idea is that

7
00:00:25,160 --> 00:00:29,540
if animals can learn something arbitrary like this can probably learn

8
00:00:29,540 --> 00:00:31,430
any the arbitrary policy

9
00:00:31,450 --> 00:00:35,560
to you know some degree might take them longer and longer to learn about policy

10
00:00:35,560 --> 00:00:41,200
that's really really complex but so does so so is the case for our simulations

11
00:00:43,430 --> 00:00:49,060
but this policy specifically learned in order to obtain rewards and avoid punishments

12
00:00:49,080 --> 00:00:53,200
in some causal way because here if the rat doesn't press the lever you won't

13
00:00:53,200 --> 00:00:55,020
get food at all

14
00:00:55,040 --> 00:00:55,930
and so

15
00:00:55,950 --> 00:00:59,970
this is kind of the second part of reinforcement learning so we have prediction learning

16
00:00:59,970 --> 00:01:08,890
learning of values and now we have a policy learning learning of actions

17
00:01:08,910 --> 00:01:11,680
so so far what we have is

18
00:01:11,740 --> 00:01:16,240
the world presenting animals or humans everything i said now is true also for humans

19
00:01:16,520 --> 00:01:20,520
with the huge reinforcement learning problem or many maybe it's better to look at it

20
00:01:20,520 --> 00:01:23,640
has many small reinforcement learning problems

21
00:01:23,700 --> 00:01:28,120
and we know that animals and humans can learn both prediction and control which are

22
00:01:28,120 --> 00:01:32,390
the two important ingredients to solve these problems

23
00:01:32,410 --> 00:01:33,930
now the question is

24
00:01:34,000 --> 00:01:38,200
now that we know that the brain does reinforcement learning can reinforcement learning help us

25
00:01:38,220 --> 00:01:40,160
understand how the brain

26
00:01:40,180 --> 00:01:47,540
solve this problem however realizes these algorithms

27
00:01:47,560 --> 00:01:51,560
and this brings us to the biggest success story of computational neuroscience in the last

28
00:01:51,560 --> 00:01:52,620
twenty years

29
00:01:52,640 --> 00:01:55,080
which is the relationship between dopamine

30
00:01:55,310 --> 00:01:58,160
prediction errors

31
00:01:58,160 --> 00:02:00,830
so you might have heard about the mean

32
00:02:00,830 --> 00:02:03,830
how many people have heard about the mean

33
00:02:03,870 --> 00:02:05,160
most of you

34
00:02:05,180 --> 00:02:07,830
the dopamine is or modulator

35
00:02:07,850 --> 00:02:09,370
that is to create

36
00:02:09,370 --> 00:02:11,390
in the brain from two

37
00:02:12,870 --> 00:02:13,890
kind of in the

38
00:02:14,680 --> 00:02:16,620
so an area called the midbrain

39
00:02:17,330 --> 00:02:22,180
called the ventral stream mental area and the substantia nigra etc etc and all over

40
00:02:22,200 --> 00:02:23,810
the place for the whole race

41
00:02:23,830 --> 00:02:28,560
mostly see these hours here because these are its main targets the straighter and the

42
00:02:28,560 --> 00:02:30,180
prefrontal cortex

43
00:02:30,240 --> 00:02:32,750
and actually the middle as well

44
00:02:32,770 --> 00:02:36,000
are the main target

45
00:02:36,850 --> 00:02:41,560
from energy projections the domain basically goes everywhere even in the visual cortex there is

46
00:02:41,870 --> 00:02:43,220
some dopamine

47
00:02:43,240 --> 00:02:46,430
and the reason that people are really really interested in dopamine

48
00:02:46,450 --> 00:02:51,770
well you've all heard about dopamine probably because of its relationship to parkinson's disease

49
00:02:53,220 --> 00:02:55,410
the disease is caused by

50
00:02:55,430 --> 00:02:59,200
the death of the promenade decks neurons there is less and less dopamine in the

51
00:02:59,200 --> 00:03:04,930
brain at some point people developed parkinson's disease from losing these neurons

52
00:03:04,930 --> 00:03:05,930
and so

53
00:03:05,930 --> 00:03:10,410
this is the original theories about what don't mean doesn't the brain were very related

54
00:03:10,430 --> 00:03:13,750
to this people thought well of parkinson's disease what

55
00:03:13,810 --> 00:03:20,040
gets damaged it what we see is that people start behaving more darkly as they

56
00:03:20,040 --> 00:03:24,520
normally did they have a problem initiating action so maybe that is the role of

57
00:03:24,520 --> 00:03:30,410
dopamine in the brain helping us initiated actions in control movement

58
00:03:30,450 --> 00:03:34,970
but later turned on the dopamine is also related to a host of other things

59
00:03:34,970 --> 00:03:41,220
to drug addiction so virtually all addictive drugs have some influence through dopamine if not

60
00:03:42,430 --> 00:03:50,390
on dopamine so cocaine amphetamine speed directly affect dopamine some affect dopamine less directly alcohols

61
00:03:50,390 --> 00:03:52,930
may be the only one that does it go

62
00:03:52,970 --> 00:03:54,930
we dopamine benefit

63
00:03:54,970 --> 00:04:00,410
i think that some of the influence of alcohol is also through to me

64
00:04:00,410 --> 00:04:02,700
gambling addictions

65
00:04:03,810 --> 00:04:05,850
related to dopamine

66
00:04:05,890 --> 00:04:11,540
natural rewards in general learning with natural rewards turns out to be related to dopamine

67
00:04:12,370 --> 00:04:14,310
this causes people to think that

68
00:04:14,330 --> 00:04:18,850
i don't mean this may be involved in reward during learning which is very different

69
00:04:18,850 --> 00:04:23,720
from motor control so now we already have three different functions for dopamine

70
00:04:23,720 --> 00:04:27,520
non zero wj and the gradient is smaller than

71
00:04:29,540 --> 00:04:30,820
when the bridges

72
00:04:30,880 --> 00:04:33,910
and this is optimal medical missions and

73
00:04:34,470 --> 00:04:39,300
for the square loss is always can be specialized because the gradient can be expressed

74
00:04:40,350 --> 00:04:43,370
so throughout this tutorial xnj

75
00:04:43,390 --> 00:04:48,400
will essentially the design matrix corresponding to variables which are indexed by j

76
00:04:48,410 --> 00:04:49,970
OK so x

77
00:04:54,300 --> 00:04:57,990
data for the viable for this is just the gradient of the loss

78
00:04:58,400 --> 00:05:00,130
like we saw in the

79
00:05:00,610 --> 00:05:03,540
previous life as well for the square loss

80
00:05:03,560 --> 00:05:07,530
OK this is one of the optimality conditions

81
00:05:07,540 --> 00:05:10,970
so now this will tell you if the are similar not

82
00:05:10,990 --> 00:05:14,680
and now you have to see how do we get to those two those conditions

83
00:05:14,680 --> 00:05:18,730
because those cannot be inverted with the gradient then with the square loss the cool

84
00:05:18,740 --> 00:05:22,290
thing is that if you write down the optimal medical missions to again brought to

85
00:05:22,350 --> 00:05:24,230
the system and get a solution

86
00:05:24,360 --> 00:05:25,840
his is not the case

87
00:05:25,850 --> 00:05:29,840
and usually iterative algorithms are needed

88
00:05:29,890 --> 00:05:34,260
so here focused for instance most of the musicians just to give you review

89
00:05:34,270 --> 00:05:37,020
of existing results is sitting

90
00:05:37,030 --> 00:05:43,110
so we've mainly focus on the gradient descent where you when you go from WT

91
00:05:43,130 --> 00:05:47,720
two w t plus one last years going on to get

92
00:05:47,720 --> 00:05:52,790
following the opposite direction of the gradient with a small primate facility

93
00:05:52,810 --> 00:05:56,180
so you have a lot of strategies to do

94
00:05:58,110 --> 00:06:02,480
to choose city and the one which i like the order was is to do

95
00:06:02,860 --> 00:06:06,950
a line search not an exact line search is because it is kind of useless

96
00:06:06,950 --> 00:06:12,760
to spend some so much time optimizing the function but line searches

97
00:06:12,770 --> 00:06:17,960
stopping all the major one and also you can consider fixed diminishing step size way

98
00:06:17,960 --> 00:06:18,770
which we decay in

99
00:06:19,430 --> 00:06:24,880
as one of the key plus some constant so that those with those two line

100
00:06:24,880 --> 00:06:29,490
searches there is a strong body of literature trying to see how fast it to

101
00:06:31,150 --> 00:06:35,640
how far they converge to the global minimum of the function so here i assume

102
00:06:35,640 --> 00:06:38,510
that f is convex so if there is no

103
00:06:38,520 --> 00:06:41,010
there are no local minima

104
00:06:41,080 --> 00:06:44,330
and if you look at the on the nice book but still have

105
00:06:44,560 --> 00:06:50,260
then you can really find common interest rates for all of for these methods so

106
00:06:50,260 --> 00:06:52,860
what the take-home message is that

107
00:06:52,880 --> 00:06:59,450
the nice function looks the first the commanders right is the same as the k

108
00:06:59,450 --> 00:07:03,620
so if it is convex and just which is so this image is

109
00:07:03,640 --> 00:07:09,730
the simplest assumption we can make on on the function then you converges to to

110
00:07:09,730 --> 00:07:12,940
the their value at rate one of those code of t

111
00:07:13,000 --> 00:07:15,150
which is rather slow

112
00:07:15,180 --> 00:07:19,950
even though you assume that you are differentiable so what people call about the smooth

113
00:07:19,950 --> 00:07:25,120
with early sheets averages gradients so what were not familiar to

114
00:07:25,140 --> 00:07:31,640
lynch's gradients it is it essentially means that if you function was twice differentiable it

115
00:07:31,640 --> 00:07:36,260
means that the second derivative is uniformly bounded

116
00:07:36,330 --> 00:07:37,950
from from above

117
00:07:38,040 --> 00:07:39,550
by l

118
00:07:39,570 --> 00:07:41,300
so if you assume that your

119
00:07:41,300 --> 00:07:44,800
a function is differentiable you know to go faster

120
00:07:44,850 --> 00:07:50,560
and if you assume moreover that the function is strongly convex so essentially means that

121
00:07:50,560 --> 00:07:53,980
if you function was twice differentiable

122
00:07:53,980 --> 00:07:59,290
the second derivatives of bounded from below this time then you get an exponential rate

123
00:07:59,290 --> 00:08:04,300
of convergence for it goes goes a lot faster than the inverse of t

124
00:08:04,310 --> 00:08:05,620
he goes as

125
00:08:05,630 --> 00:08:06,540
as this

126
00:08:06,580 --> 00:08:11,620
the exponential function of t so this number of health

127
00:08:11,630 --> 00:08:15,490
it is exactly what people call the condition number of of the of the optimisation

128
00:08:16,300 --> 00:08:19,140
essentially if this is small

129
00:08:19,210 --> 00:08:21,180
and for the record is zero

130
00:08:21,190 --> 00:08:25,350
the gradient descent they converge very slowly because you get back to these and of

131
00:08:25,350 --> 00:08:29,490
the world or in the cloud of the world but if you find new of

132
00:08:29,570 --> 00:08:30,710
n is large

133
00:08:30,720 --> 00:08:32,960
then you get exponential convergence

134
00:08:33,030 --> 00:08:35,020
the system is very important

135
00:08:35,030 --> 00:08:39,830
because it will tell you when a simple methods i expected to work on that

136
00:08:39,940 --> 00:08:42,160
and this depends on the condition number

137
00:08:42,190 --> 00:08:47,360
and again if your function is twice differentiable this is just the ratio

138
00:08:47,410 --> 00:08:51,870
of the minimum the value of the history over the maximum value of the of

139
00:08:51,870 --> 00:08:53,540
the history

140
00:08:53,540 --> 00:08:59,010
the this and this is similar types of property

141
00:08:59,030 --> 00:09:03,150
and just to mention that the gradient descent is by no means the best possible

142
00:09:03,150 --> 00:09:09,220
methods best possible first order methods for optimization and you have also in the nice

143
00:09:09,220 --> 00:09:13,940
thing this of you have a lot of all the schemes which we obtain a

144
00:09:13,940 --> 00:09:16,620
lot but the amount of coal as a

145
00:09:16,640 --> 00:09:24,750
sometimes instead of nifty of methods which we achieve rates which are quicker because he

146
00:09:24,750 --> 00:09:26,330
manages to go quicker

147
00:09:26,340 --> 00:09:29,610
and square would have

148
00:09:29,630 --> 00:09:34,320
of the over and is larger than you

149
00:09:34,340 --> 00:09:40,040
so just keep in mind that the reason is not the end of the story

150
00:09:40,040 --> 00:09:46,840
and now for the disease was for smooth optimisation now for nonsmooth optimization then you

151
00:09:46,840 --> 00:09:52,090
have no gradients so you have this for simple method is to follow the subgradient

152
00:09:52,090 --> 00:09:56,530
so i do i didn't want to define the gradient in the sense that i

153
00:09:56,530 --> 00:09:57,540
have to here

154
00:09:57,560 --> 00:10:01,050
the subgradient is simply

155
00:10:01,070 --> 00:10:03,360
do subgradient if it is

156
00:10:03,610 --> 00:10:08,750
below the function is if is if it is locally below the function so it

157
00:10:08,790 --> 00:10:13,490
the just under eighty years of greater than the questioning of functions so in a

158
00:10:13,490 --> 00:10:18,270
sense this is the gradient with the defamatory differentiable and it is something which is

159
00:10:18,730 --> 00:10:22,540
tangent to which defines the tangent when the function is not differentiable

160
00:10:23,570 --> 00:10:28,530
you have to be very careful if you replace just gradient descent by civilian dissent

161
00:10:28,530 --> 00:10:32,720
you might not converge even you if you do the exact line search

162
00:10:32,800 --> 00:10:35,020
so this is somewhat counter-intuitive

163
00:10:35,020 --> 00:10:36,940
and there is the

164
00:10:36,950 --> 00:10:40,220
countries example pundits slide i won't go one go

165
00:10:40,270 --> 00:10:45,020
but this is a very important symbol methods in the context of nonsmooth optimization might

166
00:10:45,020 --> 00:10:50,940
not work exactly that and so she doesn't work but far diminishing step size doesn't

167
00:10:50,940 --> 00:10:54,280
converge to the global minimum so it's kind of counter-intuitive

168
00:10:54,300 --> 00:10:59,480
and that the expansion y but sometimes it works like this sometimes it doesn't

169
00:10:59,500 --> 00:11:05,050
also coordinate descent which is also very simple methods mine is not always converge to

170
00:11:05,060 --> 00:11:10,020
the global optimum so is given for these fuzzy is a simple counterexample

171
00:11:10,040 --> 00:11:14,690
so is a function of two variables vote at the level sets value one two

172
00:11:14,690 --> 00:11:20,620
three four five one two three four five so the function goes in that direction

173
00:11:20,630 --> 00:11:23,520
so if you want to get that if you if you start from zero

174
00:11:23,520 --> 00:11:24,740
if you look

175
00:11:24,760 --> 00:11:28,250
i along with the other two axes along w one

176
00:11:28,260 --> 00:11:29,180
you will

177
00:11:29,830 --> 00:11:31,950
you are you think you should stop

178
00:11:31,950 --> 00:11:36,320
if you don't look around w to you go see things should stop

179
00:11:37,470 --> 00:11:42,150
any coordinate descent algorithm would stop at that point but they the direction

180
00:11:42,210 --> 00:11:45,220
direction of descent which is this one

181
00:11:45,220 --> 00:11:50,630
now there are other ways to incorporate a probabilistic model inside the loss function as

182
00:11:50,630 --> 00:11:56,390
the maximum likelihood or maximum security you can use for example cross validation use bayesian

183
00:11:56,390 --> 00:12:04,110
estimators for large sample approximations to the business that you're not really just focus on

184
00:12:04,110 --> 00:12:05,180
this year

185
00:12:05,190 --> 00:12:07,050
likelihood remarks

186
00:12:07,070 --> 00:12:12,330
so you get your problem you choose representation in the hypothesis class which is just

187
00:12:12,360 --> 00:12:18,670
way of specifying the family of distributions graphical model and then you learn the parameters

188
00:12:18,680 --> 00:12:26,320
geographical model by maximizing the likelihood of like by the maximum that's going to be

189
00:12:26,320 --> 00:12:28,430
or are

190
00:12:28,440 --> 00:12:33,850
so this is very simple derivation here which she IID assumption is so

191
00:12:33,860 --> 00:12:39,710
so for independent and identically distributed data the probability of the entire dataset d

192
00:12:39,720 --> 00:12:44,990
given the parameters is just a product over each case of the probability that it

193
00:12:45,190 --> 00:12:47,240
would means to be

194
00:12:47,250 --> 00:12:49,050
if so then

195
00:12:49,070 --> 00:12:52,060
the probability of the event

196
00:12:52,070 --> 00:12:57,030
probability and now we take the law this product it was

197
00:12:57,080 --> 00:12:58,300
so you

198
00:12:58,380 --> 00:13:04,140
likelihood estimation is the setting of the parameters that are most likely to have generated

199
00:13:04,140 --> 00:13:09,850
and this is very convenient because each case continues at time

200
00:13:09,860 --> 00:13:15,630
so we have to turn each one turn by some is to think of these

201
00:13:16,780 --> 00:13:19,940
it is and vote for what i think should be

202
00:13:20,010 --> 00:13:23,420
and averaging of trying to take the best

203
00:13:23,620 --> 00:13:29,030
so maximum likelihood is commonly used as a baseline on statistics you should always think

204
00:13:29,030 --> 00:13:31,170
of maximum likelihood simple

205
00:13:31,190 --> 00:13:35,800
you know that one as priors over parameters regularisation

206
00:13:37,090 --> 00:13:41,100
it these to very intuitive were queueing estimators

207
00:13:41,150 --> 00:13:47,110
first the idea something that makes a lot of on

208
00:13:47,130 --> 00:13:49,040
so just to give you a flavor

209
00:13:49,060 --> 00:13:53,210
maximum likelihood let's talk about the

210
00:13:53,230 --> 00:13:55,410
so let's say that i have quarry

211
00:13:55,420 --> 00:13:59,770
and it has some unknown by probably come

212
00:14:00,090 --> 00:14:02,860
this is not the

213
00:14:02,880 --> 00:14:08,410
probability of the best in the country tails just one might say OK

214
00:14:08,440 --> 00:14:11,380
you're going to get value

215
00:14:11,430 --> 00:14:15,020
and you call and i just love the show

216
00:14:15,040 --> 00:14:21,950
so going to look at scale and you want to estimate what bias

217
00:14:23,510 --> 00:14:28,030
what you should do is you should according to the principle of maximum likelihood write

218
00:14:28,030 --> 00:14:31,930
down the likelihood function and trying to find the values that maximize that this is

219
00:14:31,940 --> 00:14:34,610
a very simple calculation want to go through

220
00:14:34,620 --> 00:14:36,090
because this

221
00:14:37,150 --> 00:14:39,080
right down the likelihood function

222
00:14:39,100 --> 00:14:40,550
he wrote down the law

223
00:14:40,570 --> 00:14:41,680
you have to do that

224
00:14:41,700 --> 00:14:47,670
the likelihood function and its derivative to find values that's the same process go a

225
00:14:47,700 --> 00:14:53,620
very complicated models so i'm going start by driving the estimator

226
00:14:53,640 --> 00:14:55,940
but this is just to show the sketch

227
00:14:55,960 --> 00:14:56,790
so here's is

228
00:14:56,820 --> 00:14:58,330
data model

229
00:14:58,340 --> 00:15:02,720
here's the likelihood function is the probability of the data is the product of a

230
00:15:02,720 --> 00:15:07,170
long and use exponential notation true here

231
00:15:07,190 --> 00:15:10,390
this is this is like this is very important because

232
00:15:10,550 --> 00:15:12,590
again steer this moment

233
00:15:12,600 --> 00:15:17,750
here is just all i did here is that if the head represent the slivers

234
00:15:17,750 --> 00:15:19,080
of zero one

235
00:15:19,100 --> 00:15:22,330
so is the point comes out to be zero

236
00:15:22,350 --> 00:15:27,780
then this term is there is zero one and this one i think in the

237
00:15:27,780 --> 00:15:28,840
head before

238
00:15:28,980 --> 00:15:32,780
and one in the german state and this term is

239
00:15:32,800 --> 00:15:37,750
one so just always selecting whether they do or one minus the writing

240
00:15:37,760 --> 00:15:39,710
just as the notation

241
00:15:39,730 --> 00:15:43,190
but the thing is i think a lot of this i get a very simple

242
00:15:43,190 --> 00:15:47,040
expressions for the log likelihood is the number of heads

243
00:15:48,130 --> 00:15:51,230
say want this and the number of tails

244
00:15:51,270 --> 00:15:53,810
times or once say

245
00:15:53,830 --> 00:15:56,770
so we take the derivative with respect to the data

246
00:15:57,180 --> 00:15:58,500
set this to

247
00:15:58,540 --> 00:16:03,500
these are the maximum likelihood estimator is never over

248
00:16:05,000 --> 00:16:08,870
now you can say well i'm going to all of this a lot product in

249
00:16:09,220 --> 00:16:12,000
middle of this just to get this and i could have told that you know

250
00:16:12,060 --> 00:16:12,930
that was

251
00:16:13,010 --> 00:16:17,850
how many have to use it was actually used to but this is important and

252
00:16:17,850 --> 00:16:21,320
it is also important because i'm going to illustrate to you

253
00:16:21,340 --> 00:16:24,860
why this maximum likelihood learning can be very dangerous

254
00:16:24,870 --> 00:16:29,640
let's say that would require twice and it comes up heads

255
00:16:29,660 --> 00:16:36,380
what is the probability that the next little details

256
00:16:36,400 --> 00:16:42,800
OK if you do maximum likelihood maximum likelihood ideal case the pointwise so that two

257
00:16:42,810 --> 00:16:45,340
divided by the total number two

258
00:16:45,350 --> 00:16:48,790
so the maximum likelihood estimator for the probability of is

259
00:16:49,840 --> 00:16:52,020
so the probability of tails is

260
00:16:54,460 --> 00:17:01,200
they were very obviously maximum likelihood can be very dangerous very limited amount of time

261
00:17:01,430 --> 00:17:06,040
maximum likelihood that you are very good at because you know i basically if you

262
00:17:06,040 --> 00:17:07,020
have a point

263
00:17:07,030 --> 00:17:08,320
perfectly valid point

264
00:17:08,330 --> 00:17:13,050
probability that one was the chance that you'll see two tales of what we're all

265
00:17:13,230 --> 00:17:16,890
right so in twenty five percent data size two

266
00:17:16,910 --> 00:17:21,260
you'll see you have to estimate the probability of tails be so

267
00:17:21,280 --> 00:17:26,050
this is why the estimators are doing now if you saw a million reflects you

268
00:17:26,050 --> 00:17:31,370
can be pretty sure this estimator is the correct so if you have a lot

269
00:17:31,370 --> 00:17:32,490
of data

270
00:17:32,510 --> 00:17:37,580
maximum likelihood is you don't have very much data you should be able to think

271
00:17:37,580 --> 00:17:41,570
about this is the laws of probability events

272
00:17:42,860 --> 00:17:46,790
how much money you be willing to bet on that event occurred

273
00:17:46,830 --> 00:17:52,230
you want to lose k so if you use this probability estimator and you have

274
00:17:52,230 --> 00:17:57,530
to flip and you say the probability of tails is equal to zero vote lord

275
00:17:57,750 --> 00:17:59,380
of zero

276
00:17:59,400 --> 00:18:02,030
k is negative infinity

277
00:18:02,040 --> 00:18:06,510
so that says that you would be willing to take a lots of negativeinfinity dollars

278
00:18:06,510 --> 00:18:07,830
you cite fails

279
00:18:08,030 --> 00:18:10,830
OK so this is a

280
00:18:11,000 --> 00:18:13,200
and we don't really believe

281
00:18:13,220 --> 00:18:18,820
i just another example to to give you flavor things so if you have a

282
00:18:18,820 --> 00:18:21,420
guassian model to univariate normal

283
00:18:21,420 --> 00:18:23,440
actually the cell is able

284
00:18:23,450 --> 00:18:25,820
two lower

285
00:18:25,880 --> 00:18:30,170
the the free energy state between these two and that makes it possible for the

286
00:18:30,170 --> 00:18:32,390
equilibrium to be much more favoured

287
00:18:32,400 --> 00:18:39,380
let's look at this this glycolysis pathway likely refers obviously glycolysis and here we start

288
00:18:39,400 --> 00:18:43,870
out with glucose we're trying out flat ground the the circular structure we talked about

289
00:18:43,880 --> 00:18:49,400
last time and let's look at what happens here again not because anybody wants you

290
00:18:49,400 --> 00:18:55,280
to memorize this but because some of the details are in themselves very illustrative

291
00:18:55,300 --> 00:18:59,750
the goal of this exercise is to create ATP for the cell

292
00:18:59,760 --> 00:19:04,480
but the first step in the reaction is actually totally counterproductive

293
00:19:04,490 --> 00:19:06,150
look at the first thing that happens

294
00:19:06,260 --> 00:19:11,230
the first thing that happens is that the cell invest in ATP molecule to make

295
00:19:11,280 --> 00:19:13,460
glucose six phosphate

296
00:19:13,510 --> 00:19:18,740
i've advertised the the goal of this has been to generate ATP from ADP

297
00:19:18,760 --> 00:19:23,060
then he died possibly but the first thing we go here we don't

298
00:19:23,080 --> 00:19:28,800
this is an organic reaction in which the cell invest energy to create this molecule

299
00:19:29,620 --> 00:19:31,730
so this doesn't make sense

300
00:19:31,730 --> 00:19:35,700
what is ostensibly must make sense in one level mother because

301
00:19:35,830 --> 00:19:39,870
you and i were all here and everybody in this room at least this moment

302
00:19:39,870 --> 00:19:45,500
is metabolic lee active all right so we've got this molecule of glucose six phosphate

303
00:19:45,600 --> 00:19:48,240
and this can i summarise

304
00:19:48,280 --> 00:19:52,950
you see here is glucose six phosphate fructose six phosphate

305
00:19:52,970 --> 00:19:56,370
and the fact of the matter is there is no oxidation reduction reactions here is

306
00:19:56,370 --> 00:20:02,700
just an isomerisation and this molecule in this molecule are virtually in the same free

307
00:20:02,700 --> 00:20:04,200
energy state

308
00:20:04,220 --> 00:20:08,090
it happens to be the case that their profile looks very much like the one

309
00:20:08,090 --> 00:20:09,930
i drew you before

310
00:20:09,950 --> 00:20:13,640
their energy profile look like this

311
00:20:13,670 --> 00:20:15,870
and one is an enzyme to lower it

312
00:20:15,930 --> 00:20:18,560
but there is no energy that needs to be invested in

313
00:20:18,670 --> 00:20:22,930
converting one to the other because they are very similar molecules and therefore incomparable free

314
00:20:22,930 --> 00:20:24,370
energy states

315
00:20:24,430 --> 00:20:29,650
now look at the next step in the next step is again an ostensibly totally

316
00:20:29,650 --> 00:20:31,640
totally counter-productive way

317
00:20:32,040 --> 00:20:34,820
of generating energy because once again ATP

318
00:20:34,830 --> 00:20:40,380
the the gamma phosphate is in its energy is invested in creating a di phosphorylated

319
00:20:41,280 --> 00:20:46,350
for fructose one six di phosphate where the numbers refer obviously the the the identities

320
00:20:46,350 --> 00:20:49,820
of the carbon and now we have dive phosphorylated

321
00:20:49,880 --> 00:20:52,480
fructose molecules

322
00:20:52,480 --> 00:20:56,520
and so here you can actually see what the three-dimensional

323
00:20:56,570 --> 00:21:01,000
what we what we imagine closer to what the three-dimensional structures of these molecules look

324
00:21:02,810 --> 00:21:04,460
and we don't have to

325
00:21:04,460 --> 00:21:07,230
we should focus this time with this this or this

326
00:21:07,280 --> 00:21:11,290
for all practical purposes let's just focus on this pathway here

327
00:21:11,320 --> 00:21:15,150
and here for the first time what we now what now happens is that this

328
00:21:15,150 --> 00:21:19,710
hexose is broken down into two trials

329
00:21:19,740 --> 00:21:22,870
i into two three carbon sugars

330
00:21:22,890 --> 00:21:29,090
and this is the slightly actually gonna reaction yields it happens without the investment of

331
00:21:29,090 --> 00:21:33,690
energy and there's an enzyme once again is required in order to catalyze

332
00:21:33,720 --> 00:21:35,680
but let's be really clear now

333
00:21:35,690 --> 00:21:38,930
now we have to follow the fate of two molecules

334
00:21:38,940 --> 00:21:40,540
prior the first trials

335
00:21:40,550 --> 00:21:43,850
and the second trials they have different names but we're not going to focus on

336
00:21:43,850 --> 00:21:45,420
the names

337
00:21:45,460 --> 00:21:50,870
one thing i notice about these trials is is that the rate readily inter convertible

338
00:21:50,890 --> 00:21:55,190
once again we can imagine we have a situation that looks like this these are

339
00:21:55,190 --> 00:21:56,710
flipping back and forth

340
00:21:56,710 --> 00:22:01,310
and therefore for all practical purposes from our point of view these two are equivalent

341
00:22:01,310 --> 00:22:06,010
because they can be exchanged virtually instantaneously one with the other

342
00:22:06,030 --> 00:22:11,720
now so far we've actually expended energy we haven't we haven't harvested energy

343
00:22:11,770 --> 00:22:14,510
but keep in mind the old economic the the

344
00:22:14,560 --> 00:22:16,370
you have to invest money

345
00:22:16,460 --> 00:22:19,890
to make money and that's what's going on here

346
00:22:19,920 --> 00:22:26,550
the first thing happens is we have an oxidation reaction

347
00:22:26,680 --> 00:22:31,830
what's an oxidation reaction we want to strip some electrons pair of electrons off of

348
00:22:31,830 --> 00:22:39,250
this particular trials the three carbon sugar and by stripping off the pair of electrons

349
00:22:39,250 --> 00:22:40,720
we donate

350
00:22:40,740 --> 00:22:46,040
the electrons from NADH plaster NADH and here these structures are given in your book

351
00:22:46,070 --> 00:22:49,080
but NADH it turns out is

352
00:22:49,500 --> 00:22:54,560
the electrons are pulled away from the trials appear are used to reduce NAD to

353
00:22:55,890 --> 00:23:01,780
keep in mind the oxidation reactions one molecule it's being oxidized

354
00:23:01,810 --> 00:23:06,870
is deprived he's denied the pair of electrons the other molecules being reduced in this

355
00:23:06,870 --> 00:23:08,010
case a

356
00:23:08,050 --> 00:23:10,050
acquires a pair of electrons

357
00:23:10,060 --> 00:23:14,280
and you can focus if you want about the the charge of these molecules one

358
00:23:14,280 --> 00:23:15,310
of the other

359
00:23:15,350 --> 00:23:20,830
but keep in mind that in these oxidation reduction reactions whether it's plus charge minus

360
00:23:20,830 --> 00:23:22,790
charges here is irrelevant

361
00:23:22,840 --> 00:23:24,640
the real name of the game is

362
00:23:24,650 --> 00:23:25,790
the electrons

363
00:23:25,830 --> 00:23:29,530
forget about the protons where there is a plus charter to the real name of

364
00:23:29,530 --> 00:23:33,220
the game here is two electrons are being used to reduce this molecule to this

365
00:23:33,420 --> 00:23:34,340
by the way

366
00:23:34,390 --> 00:23:36,980
third mistake i forgot to tell you before

367
00:23:36,990 --> 00:23:39,960
there's a double bond and one of the pavilions in the book that doesn't make

368
00:23:39,960 --> 00:23:41,780
any sense

369
00:23:41,830 --> 00:23:45,750
whoever finds it gets surprised but no one's feel with surprises you OK

370
00:23:46,920 --> 00:23:52,320
so here i love this double bond gets reduced you see the difference between this

371
00:23:52,320 --> 00:23:53,670
and this over here

372
00:23:53,720 --> 00:23:57,730
and this NADH it turns out is a high energy molecules

373
00:23:57,740 --> 00:24:01,180
the street value of any NADH is three ATP

374
00:24:01,230 --> 00:24:03,960
i e in the mitochondria

375
00:24:04,010 --> 00:24:07,720
NADH can be used to generate three ATP

376
00:24:07,760 --> 00:24:09,180
and that's worth something

377
00:24:09,230 --> 00:24:12,550
so NADH on its own is a high energy molecules

378
00:24:12,610 --> 00:24:14,880
it can be used for that many things

379
00:24:14,930 --> 00:24:19,140
but it can be pulled into the mitochondria works converted to three ATP's

380
00:24:19,210 --> 00:24:23,210
so we say we're starting to make some money out of this investment because we've

381
00:24:23,210 --> 00:24:25,920
made in fact

382
00:24:25,960 --> 00:24:29,670
two we've made these

383
00:24:29,690 --> 00:24:33,850
ATP we made his NADH is see right here

384
00:24:33,900 --> 00:24:36,210
why do we say two NADH is

385
00:24:36,210 --> 00:24:39,940
because each of these two trials as we're working with

386
00:24:39,950 --> 00:24:43,350
and each one of the trials this gives you an NADH

387
00:24:43,380 --> 00:24:45,020
so everything that's going on

388
00:24:45,030 --> 00:24:47,370
after this starting from the top here

389
00:24:47,430 --> 00:24:52,090
is now double because we're we're looking at the parallel behaviors of two identical three

390
00:24:52,090 --> 00:24:57,560
carbon sugars so here we have so far generated in principle

391
00:24:57,610 --> 00:24:59,600
six ATP

392
00:24:59,650 --> 00:25:02,620
how much did we invest already up to this point

393
00:25:03,730 --> 00:25:06,690
we have to what we harvested six

394
00:25:06,700 --> 00:25:09,810
already was done to make a little money because i told the street value of

395
00:25:09,870 --> 00:25:12,040
NADH is three a piece

396
00:25:12,070 --> 00:25:14,280
on the black market OK so

397
00:25:14,330 --> 00:25:15,940
what happens next

398
00:25:16,010 --> 00:25:20,280
next is another good thing

399
00:25:20,310 --> 00:25:21,850
each of the trials is

400
00:25:21,870 --> 00:25:29,140
one can actually cause each of trials is to generate an eight eighty people to

401
00:25:29,140 --> 00:25:34,190
form ATP what happens here it turns out that this phosphate over here is actually

402
00:25:34,200 --> 00:25:35,930
pretty high energy state

403
00:25:35,940 --> 00:25:40,430
in no small part because of electron a negative negative repulsion

404
00:25:40,450 --> 00:25:43,920
and by stripping this phosphate of this high-energy phosphate

405
00:25:43,990 --> 00:25:48,820
stripped off of this molecule whose name we will ignore allows us to force for

406
00:25:50,140 --> 00:25:54,200
and ATP synthase two re two trials has been converted we're going to get two

407
00:25:55,230 --> 00:25:58,500
so in effect now we're actually ahead

408
00:25:58,540 --> 00:26:00,860
we start out investing two

409
00:26:00,870 --> 00:26:04,700
we've got six back from the mediators and we're going to more back

410
00:26:05,760 --> 00:26:08,320
so we've made two ATP is this is a good thing

411
00:26:08,470 --> 00:26:12,220
keep in mind ATP lower energy ATP so high high energy

412
00:26:12,260 --> 00:26:19,550
once again we have a an isomerisation were these two molecules are comparable states

413
00:26:19,550 --> 00:26:21,670
has zero resistance

414
00:26:21,860 --> 00:26:24,350
that's of course

415
00:26:24,400 --> 00:26:27,910
a pretty stupid thing to do to short out the battery

416
00:26:27,990 --> 00:26:29,970
but more dangerous

417
00:26:30,020 --> 00:26:31,330
forty watts

418
00:26:31,370 --> 00:26:34,960
i think that's the warm big deal so let's do it

419
00:26:34,980 --> 00:26:36,950
so i have here

420
00:26:36,960 --> 00:26:38,950
the voltage that you can see

421
00:26:39,030 --> 00:26:41,360
that we measure

422
00:26:41,360 --> 00:26:42,850
nine fold

423
00:26:42,900 --> 00:26:46,080
duracell battery i have the better here

424
00:26:46,090 --> 00:26:47,550
and you can read here

425
00:26:47,600 --> 00:26:52,300
i hope the decimal point is in there but it's about nine point six vols

426
00:26:52,340 --> 00:26:55,150
and now i'm going to do something stupid but again

427
00:26:55,170 --> 00:26:56,930
it's not dangerous

428
00:26:56,940 --> 00:27:01,100
i'm going to take my car keys and i'm going to show it out

429
00:27:01,100 --> 00:27:02,800
the battery

430
00:27:02,910 --> 00:27:05,670
simply connected point a with point b

431
00:27:05,710 --> 00:27:06,930
and so the

432
00:27:06,980 --> 00:27:11,120
voltage going to see is going maybe not go to zero exactly because mikey made

433
00:27:11,120 --> 00:27:14,420
of zero resistance but it goes very low

434
00:27:14,470 --> 00:27:17,670
and what you can all experience is something that i can do this but we

435
00:27:17,670 --> 00:27:18,910
will get hot

436
00:27:18,910 --> 00:27:20,220
this forty what

437
00:27:20,230 --> 00:27:23,910
will be generated inside here it is possible that when the

438
00:27:23,960 --> 00:27:28,170
betty gets hot that the internal resistance may even go up a little

439
00:27:28,340 --> 00:27:33,010
remember that resistance goes up when temperature goes up in which case the power will

440
00:27:33,010 --> 00:27:35,570
go down to mainly the full forty watts

441
00:27:35,570 --> 00:27:38,380
but i can assure you that i can feel this thing anymore

442
00:27:38,430 --> 00:27:40,300
so let me shorted out now

443
00:27:40,310 --> 00:27:41,950
i'm doing this now

444
00:27:42,000 --> 00:27:43,550
you read the voltage

445
00:27:43,550 --> 00:27:45,840
i can see two here

446
00:27:46,280 --> 00:27:49,110
is always not so easy was akin to do that

447
00:27:49,110 --> 00:27:53,420
it's very low a look at some other tens of volts

448
00:27:53,480 --> 00:27:55,820
and i feel this thing getting hot

449
00:27:55,820 --> 00:28:00,760
really one up now

450
00:28:00,820 --> 00:28:03,560
so i'm going to better

451
00:28:03,570 --> 00:28:06,670
this is a terrible thing to do better is like that

452
00:28:06,700 --> 00:28:08,390
but when i take

453
00:28:08,450 --> 00:28:11,110
after the active resistance some of that may come back

454
00:28:11,170 --> 00:28:17,450
may not be permanently damaged you see it's really a than half falls

455
00:28:17,470 --> 00:28:19,220
so is no way

456
00:28:19,240 --> 00:28:21,300
that you can start a car

457
00:28:21,340 --> 00:28:22,590
was in nine fold

458
00:28:22,600 --> 00:28:23,890
duracell battery

459
00:28:23,900 --> 00:28:27,230
because you just can't get the current you need for you start

460
00:28:27,250 --> 00:28:31,760
you start to motor needs a few hundred mps

461
00:28:31,770 --> 00:28:36,690
if you take a car battery

462
00:28:36,740 --> 00:28:38,590
that's about travel

463
00:28:39,820 --> 00:28:42,780
it has a very low internal resistance

464
00:28:42,870 --> 00:28:45,080
of about one fifteen

465
00:28:45,120 --> 00:28:47,190
over now

466
00:28:47,210 --> 00:28:51,880
so that means that the maximum current you can draw if you would circuit

467
00:28:51,890 --> 00:28:56,340
it would be something like six hundred mps

468
00:28:56,350 --> 00:29:01,070
and so the maximum power if you were so stupid to short-circuited

469
00:29:01,170 --> 00:29:03,950
we would all be generated inside the battery

470
00:29:03,970 --> 00:29:09,140
it would be something like seven kilowatts

471
00:29:09,200 --> 00:29:12,400
if you ever work on your car

472
00:29:12,400 --> 00:29:13,950
make sure

473
00:29:14,010 --> 00:29:18,120
but you never dropped accidentally the range that you're using onto the better

474
00:29:18,120 --> 00:29:20,130
because if you did

475
00:29:20,140 --> 00:29:22,120
inside the battery

476
00:29:22,140 --> 00:29:24,340
about six kilowatts

477
00:29:24,350 --> 00:29:26,860
seven thousand joules per second are going to be

478
00:29:26,870 --> 00:29:29,240
produced in terms of heat

479
00:29:29,260 --> 00:29:30,700
and this all theory

480
00:29:30,710 --> 00:29:32,860
as it is going to boil

481
00:29:32,860 --> 00:29:35,130
the case may melt

482
00:29:35,140 --> 00:29:36,780
and that's no good

483
00:29:36,830 --> 00:29:38,420
not only is that

484
00:29:40,000 --> 00:29:41,360
but it's also

485
00:29:41,430 --> 00:29:43,370
very dangerous

486
00:29:43,440 --> 00:29:46,090
so let's do it

487
00:29:46,100 --> 00:29:49,630
i have here is better it

488
00:29:49,680 --> 00:29:50,760
and i have here

489
00:29:50,770 --> 00:29:51,750
the rams

490
00:29:51,760 --> 00:29:59,010
just in case

491
00:29:59,070 --> 00:30:02,410
i'm going to show that that battery

492
00:30:02,420 --> 00:30:03,800
and as i do that

493
00:30:03,800 --> 00:30:08,110
you will clearly see that the better doesn't like it

494
00:30:08,130 --> 00:30:10,480
i would be very careful not to

495
00:30:10,500 --> 00:30:15,310
hold on this range school because it would welcome to with actually can well wouldn't

496
00:30:15,310 --> 00:30:17,690
stay there currently so why i

497
00:30:17,770 --> 00:30:20,500
go up to six hundred and because it can well known to with and then

498
00:30:20,500 --> 00:30:22,910
you can get off more

499
00:30:22,950 --> 00:30:24,670
in case that happens

500
00:30:24,720 --> 00:30:27,640
i walk out of here

501
00:30:27,650 --> 00:30:32,120
and i advise you to do the same

502
00:30:32,130 --> 00:30:35,700
you ready

503
00:30:35,760 --> 00:30:38,570
OK i go now

504
00:30:40,800 --> 00:30:44,000
that's what happens very high current

505
00:30:44,060 --> 00:30:45,900
you do this

506
00:30:45,920 --> 00:30:47,370
two often two batteries

507
00:30:47,510 --> 00:30:51,060
they're not going to live very long they don't like

508
00:30:51,160 --> 00:30:54,020
but i wasn't joking when i said when you work on the cart it you

509
00:30:54,020 --> 00:30:58,920
so yes is the title says i'm going to talk about approximate bayesian computation

510
00:30:58,980 --> 00:31:04,290
ABC is often known or likelihood free inference

511
00:31:04,330 --> 00:31:07,320
now obviously in the past few days i realise that most people who are doing

512
00:31:07,320 --> 00:31:12,590
approximate bayesian computation some sense so what i mean is a class of monte carlo

513
00:31:13,930 --> 00:31:18,220
that i used to do inference in stochastic models

514
00:31:18,270 --> 00:31:25,110
in particular they don't use when the function in the model is not available

515
00:31:25,160 --> 00:31:28,370
the likelihood free inference

516
00:31:28,440 --> 00:31:33,500
so i'll do i'll highlights the basic idea behind the ABC method

517
00:31:34,620 --> 00:31:36,550
i don't give a couple of extensions

518
00:31:36,990 --> 00:31:40,100
and illustrate them

519
00:31:40,110 --> 00:31:45,180
with reference to a particular kind of problem i would on from evolutionary biology

520
00:31:45,250 --> 00:31:49,790
and that while the general idea is a very generic as an extension of the

521
00:31:49,790 --> 00:31:51,570
best illustrated

522
00:31:53,220 --> 00:31:55,690
specific reference to problems

523
00:31:55,720 --> 00:32:02,440
i should just mention this work with tavare a lot my phd in cambridge

524
00:32:02,490 --> 00:32:03,790
OK so

525
00:32:03,800 --> 00:32:09,490
this is two main types of statistical model that we we tend to build

526
00:32:09,490 --> 00:32:15,290
the first type your prescribed models so you get your data and you decide sit

527
00:32:15,290 --> 00:32:21,330
in a new division process or a normal distribution to add to or whatever

528
00:32:21,350 --> 00:32:25,960
a very complicated i hierarchical cost and and so on but ultimately can write down

529
00:32:25,960 --> 00:32:30,290
the likelihood function and you can calculate the likelihood function or some

530
00:32:30,330 --> 00:32:34,550
like version likelihood function of the normalizing constant

531
00:32:34,570 --> 00:32:39,080
so that the model that we use an implicit models

532
00:32:39,100 --> 00:32:45,960
so these mechanisms to simulate observations on the model can tell you that some parameters

533
00:32:46,160 --> 00:32:50,550
the future works the way it produces some sample output

534
00:32:52,660 --> 00:32:57,770
the is impossible to models often give scientists much more freedom to more accurately model

535
00:32:57,770 --> 00:33:00,070
the phenomena that interested in the

536
00:33:00,080 --> 00:33:04,510
and until recently with the interesting thing about it was really very many ways to

537
00:33:04,510 --> 00:33:06,580
do inference in in these models

538
00:33:06,590 --> 00:33:12,680
so the approximate bayesian computation i went away to inference in implicit models

539
00:33:12,700 --> 00:33:16,230
this is by implicit model this is a branching process

540
00:33:16,240 --> 00:33:24,140
a line here in my problem represent different species of primates but it could equally

541
00:33:24,150 --> 00:33:26,670
be different cell and cancer tumour

542
00:33:26,710 --> 00:33:32,400
or a different human in the population

543
00:33:32,460 --> 00:33:35,020
and the red dots are some kind of discrete sampling

544
00:33:35,050 --> 00:33:41,050
so my problem again the primate species so the red dots represent the fossil finds

545
00:33:41,140 --> 00:33:44,180
that fossils here and so on

546
00:33:44,210 --> 00:33:49,080
or they could be mutations inhibited it a

547
00:33:49,110 --> 00:33:54,170
so what interested in doing

548
00:33:54,180 --> 00:34:00,020
it is between most today because of a where talking to machine machine learning background

549
00:34:00,330 --> 00:34:05,450
that we define terminology in statistics were talking about the same thing called the time

550
00:34:05,450 --> 00:34:09,080
but we don't understand each so sometimes with walking across the site

551
00:34:09,110 --> 00:34:15,900
but they calibration data assimilation computing the inverse problem the manifold to build a forwards

552
00:34:16,770 --> 00:34:22,520
we specify parameter values initial conditions in the output is the reverse you we want

553
00:34:22,520 --> 00:34:25,140
to do

554
00:34:25,140 --> 00:34:30,210
so here the representation of the primates so we've got some kind of phylogeny here

555
00:34:30,210 --> 00:34:34,140
and the fossil represents the data we have available

556
00:34:34,170 --> 00:34:36,950
eighteen here is too

557
00:34:36,960 --> 00:34:39,740
if the depth of the trees away this route is

558
00:34:39,760 --> 00:34:45,900
so it's been very flat my terminology is that the time the first primate appeared

559
00:34:45,990 --> 00:34:47,890
the time the first of all you like

560
00:34:47,930 --> 00:34:51,920
the divergence time

561
00:34:52,960 --> 00:34:57,420
this is just notation of amazing because the the parameters that are

562
00:34:57,430 --> 00:35:03,430
and it's going to be because we want to draw samples from the posterior distribution

563
00:35:03,480 --> 00:35:05,830
the distribution of theta given d

564
00:35:07,760 --> 00:35:09,800
and let me just say yes

565
00:35:09,830 --> 00:35:14,390
for many of the tree based models appear in genetics

566
00:35:14,390 --> 00:35:19,070
and grow up and will be expected the external environment and they would not be

567
00:35:19,070 --> 00:35:22,350
easy stressed if she doesn't like them along

568
00:35:22,370 --> 00:35:28,960
that would be covered by a neurotic now the point is that you wrote that

569
00:35:28,960 --> 00:35:35,390
novel long people or who have neurotic offspring they also bring she would also not

570
00:35:35,390 --> 00:35:41,760
liquor offspring this is part of the syndrome and they get that blood Lakers will

571
00:35:41,760 --> 00:35:47,870
have bravely rats were confident that have that been the that date the doctors will

572
00:35:47,870 --> 00:35:52,610
also make their so good you have to live images of rats which are different

573
00:35:52,610 --> 00:35:57,280
in that behavioral and in there the way that their respond to stress

574
00:35:57,290 --> 00:36:02,450
and that the some genetically they live in the same environment and yet there are

575
00:36:02,450 --> 00:36:07,340
different and different because something happened in the history at some point a replica according

576
00:36:07,420 --> 00:36:09,920
a new world order tourism

577
00:36:10,260 --> 00:36:13,890
now what is interesting about this now this kind of thing has been known for

578
00:36:13,930 --> 00:36:18,920
a long time but the reason that it became very interesting for people who is

579
00:36:18,920 --> 00:36:24,220
that now we understand a little bit about the molecular basis of this behavior we

580
00:36:24,240 --> 00:36:29,390
understand what is going on in terms of the genes that are acting in the

581
00:36:29,390 --> 00:36:32,700
brain and what is happening is that

582
00:36:33,050 --> 00:36:42,390
In did not linked threats this June the geology of the geology and the glucocorticoid

583
00:36:42,390 --> 00:36:49,260
receptor gene is methylated wears in well electrodes it is being reflected it happens in

584
00:36:49,300 --> 00:36:51,700
people follows in the bright

585
00:36:52,110 --> 00:36:57,310
and it's a very persistent stage 1 it happens and the critical period sensitive period

586
00:36:57,310 --> 00:37:01,200
for the is between 1 and 6 days

587
00:37:01,420 --> 00:37:05,780
this is where it has to be or more click

588
00:37:05,790 --> 00:37:11,240
so when we know a little bit about it and we understand that the genetic

589
00:37:11,240 --> 00:37:16,040
mechanism that can be responsible for selling Americans can also be responsible for some memory

590
00:37:16,040 --> 00:37:20,480
here there's no Southern heritage because here we are what we have is a reconstruction

591
00:37:20,480 --> 00:37:26,320
development and reconstruction of behavior and we have all kinds of things by

592
00:37:26,360 --> 00:37:30,790
which are similar we know that food preferences in rats

593
00:37:30,880 --> 00:37:32,810
In my in hostess

594
00:37:33,390 --> 00:37:38,160
what what the mom can influence the food preferences of her offspring and we know

595
00:37:38,180 --> 00:37:43,940
a little bit also attributable about those working humans but is also suggested that what

596
00:37:43,940 --> 00:37:51,390
the mother its can affect the full preferences or for office and we don't know

597
00:37:51,390 --> 00:37:55,570
he the molecular mechanisms by better it will be 1 of those at the genetic

598
00:37:55,570 --> 00:38:02,400
exotic can't be anything other unless we find another mechanism which is very likely and

599
00:38:02,400 --> 00:38:06,050
here we have a different kind of transmission of information and this is the transmission

600
00:38:06,050 --> 00:38:12,610
of some dialects in the songbirds and we know that the are members of the

601
00:38:12,610 --> 00:38:17,290
belong to the same species to the saying in a kid can have different populations

602
00:38:17,590 --> 00:38:22,920
and different populations can have different dialects and is Ireland's all characteristic of this population

603
00:38:22,920 --> 00:38:27,480
samples to some extent not with very high fidelity transmitted from 1 generation to the

604
00:38:27,480 --> 00:38:31,200
next so that's another route of transmission

605
00:38:31,620 --> 00:38:38,180
here we have a picture in almost all of the various tables and very intelligent

606
00:38:39,050 --> 00:38:45,550
good anymore in ways potato and Japanese and that the story is very nice it's

607
00:38:45,550 --> 00:38:51,620
a story about Japanese researchers to start to look at their original started looking at

608
00:38:51,620 --> 00:38:59,240
the Japanese Mikako and they in order to observe the potatoes near the whose River

609
00:38:59,510 --> 00:39:03,760
was running to the seat of the wanted them to stay with the title and

610
00:39:03,760 --> 00:39:08,180
1 of those said I'm a Cox a few according won't start the war should

611
00:39:08,200 --> 00:39:15,890
the potatoes in the water and this behavior spread and became the norm in the

612
00:39:15,890 --> 00:39:22,420
population afterward today the were all kinds of variations on the theme that developed in

613
00:39:22,420 --> 00:39:28,240
this population for example they start the proportion in the seat because it's nice when

614
00:39:28,240 --> 00:39:29,120
the goal

615
00:39:30,680 --> 00:39:31,840
most simple

616
00:39:33,040 --> 00:39:35,110
some symbols required

617
00:39:40,880 --> 00:39:41,760
general has

618
00:39:47,700 --> 00:39:48,320
what the

619
00:39:50,420 --> 00:39:51,890
what else can be said

620
00:39:52,990 --> 00:39:53,440
when you

621
00:39:55,110 --> 00:39:57,130
so this is the right

622
00:40:00,030 --> 00:40:00,570
all this

623
00:40:02,790 --> 00:40:03,240
the people

624
00:40:03,990 --> 00:40:04,740
i believe in

625
00:40:09,670 --> 00:40:11,100
some all

626
00:40:12,560 --> 00:40:14,290
serious misunderstanding

627
00:40:15,080 --> 00:40:15,590
on the

628
00:40:20,070 --> 00:40:21,410
must be involved in

629
00:40:22,040 --> 00:40:23,180
small steps

630
00:40:23,680 --> 00:40:24,270
one year

631
00:40:25,940 --> 00:40:27,500
he takes place in the

632
00:40:28,260 --> 00:40:28,870
and dance

633
00:40:34,310 --> 00:40:35,210
however the

634
00:40:35,740 --> 00:40:37,000
these assumptions

635
00:40:38,170 --> 00:40:38,910
the answer

636
00:40:44,370 --> 00:40:45,290
what was

637
00:40:46,080 --> 00:40:46,710
the court

638
00:40:48,200 --> 00:40:49,090
the she

639
00:40:49,890 --> 00:40:52,010
emerging from certainly

640
00:40:58,770 --> 00:41:00,840
one work

641
00:41:02,100 --> 00:41:04,090
well of course is constant star

642
00:41:05,730 --> 00:41:06,480
that's the problem

643
00:41:06,650 --> 00:41:07,160
if man

644
00:41:08,150 --> 00:41:10,090
historical changes not be

645
00:41:13,170 --> 00:41:13,930
this should be

646
00:41:15,220 --> 00:41:15,940
the reason say

647
00:41:17,500 --> 00:41:18,400
from the

648
00:41:20,400 --> 00:41:22,570
literature and languages

649
00:41:24,480 --> 00:41:25,660
speculation about

650
00:41:25,750 --> 00:41:26,920
historical change

651
00:41:28,280 --> 00:41:31,030
in the technical sense of the word solution

652
00:41:31,600 --> 00:41:33,170
languages don't know all

653
00:41:34,110 --> 00:41:37,840
so there is no such fuels evolution by the

654
00:41:38,900 --> 00:41:39,660
they change

655
00:41:42,840 --> 00:41:44,520
language users of

656
00:41:44,890 --> 00:41:45,820
the organisms

657
00:41:46,450 --> 00:41:47,620
would like

658
00:41:49,190 --> 00:41:49,900
not less

659
00:41:50,710 --> 00:41:52,670
fifty thousand years scenes and

660
00:41:53,770 --> 00:41:55,170
not since e

661
00:41:55,340 --> 00:41:56,580
the emergence of

662
00:41:56,880 --> 00:41:57,760
they model

663
00:42:00,170 --> 00:42:01,560
not only before it

664
00:42:05,910 --> 00:42:07,170
back and

665
00:42:16,040 --> 00:42:18,280
and don't seem to be terrorists

666
00:42:19,260 --> 00:42:20,270
very shortly before

667
00:42:24,580 --> 00:42:25,400
well terms

668
00:42:26,060 --> 00:42:27,110
language acquisition

669
00:42:28,090 --> 00:42:29,610
even the first moment

670
00:42:30,170 --> 00:42:31,010
r very

671
00:42:34,950 --> 00:42:36,020
surrounded by the

672
00:42:37,320 --> 00:42:39,630
called with james cawley

673
00:42:40,340 --> 00:42:42,010
mean buzzing confusion

674
00:42:45,670 --> 00:42:46,670
the infamous

675
00:42:47,210 --> 00:42:48,990
extract from this confusion

676
00:42:50,570 --> 00:42:52,210
there are specific for language

677
00:42:54,730 --> 00:42:55,370
easy to

678
00:42:56,310 --> 00:42:56,770
you try

679
00:42:59,040 --> 00:43:00,110
you do that

680
00:43:01,360 --> 00:43:02,190
when steering

681
00:43:02,930 --> 00:43:04,860
this very recent evidence that he

682
00:43:05,430 --> 00:43:07,500
what system age

683
00:43:09,610 --> 00:43:11,970
this is very similar to humans

684
00:43:12,690 --> 00:43:15,020
and even response to be used

685
00:43:18,810 --> 00:43:20,310
so was features

686
00:43:23,480 --> 00:43:26,780
it's just an undifferentiated part generated

687
00:43:28,970 --> 00:43:29,540
that's true

688
00:43:29,950 --> 00:43:31,000
other organisms

689
00:43:32,940 --> 00:43:34,430
specific parts in

690
00:43:36,700 --> 00:43:37,200
here we

691
00:43:37,770 --> 00:43:39,740
nearly undetectable by humans

692
00:43:40,610 --> 00:43:41,710
so there's gotta be some

693
00:43:45,140 --> 00:43:46,290
additional mechanisms

694
00:43:46,810 --> 00:43:47,570
that is used by

695
00:43:48,190 --> 00:43:49,430
human sense that

696
00:43:50,530 --> 00:43:51,360
the organism

697
00:43:51,830 --> 00:43:53,280
today the very first

698
00:43:55,470 --> 00:43:58,460
the something that the line

699
00:44:03,440 --> 00:44:04,610
place has shown

700
00:44:05,390 --> 00:44:08,060
this achievement is in part to me

701
00:44:09,090 --> 00:44:10,010
that means we

702
00:44:11,320 --> 00:44:12,910
can distinguish e

703
00:44:13,010 --> 00:44:14,120
like his mother

704
00:44:15,410 --> 00:44:15,980
i j

705
00:44:16,670 --> 00:44:17,440
was spoken

706
00:44:18,370 --> 00:44:19,220
it was

707
00:44:21,630 --> 00:44:22,350
and it turns out

708
00:44:25,740 --> 00:44:27,170
discoveries shown that

709
00:44:27,660 --> 00:44:29,810
languages fall into the wrong

710
00:44:30,010 --> 00:44:31,490
possible categories

711
00:44:33,500 --> 00:44:35,080
when you put in

712
00:44:35,910 --> 00:44:37,270
distinguish can

713
00:44:37,940 --> 00:44:38,490
it was

714
00:44:38,840 --> 00:44:40,450
is presiding

715
00:44:40,580 --> 00:44:41,760
features rhythm

716
00:44:41,760 --> 00:44:44,700
which is just a double sum over these two examples that we want to

717
00:44:45,210 --> 00:44:45,740
compare with

718
00:44:46,560 --> 00:44:47,630
the interesting thing is that

719
00:44:48,550 --> 00:44:49,400
of course since this is

720
00:44:49,890 --> 00:44:50,820
working with kernels

721
00:44:51,380 --> 00:44:54,000
so this age is defined from the kernel

722
00:44:54,950 --> 00:44:56,100
we can also do this fall

723
00:44:56,670 --> 00:44:59,340
we can do this for all kinds of data where we can define kernels also

724
00:44:59,380 --> 00:45:02,520
structured data graphs trees strings and so on

725
00:45:06,250 --> 00:45:07,580
and there's a second application

726
00:45:08,180 --> 00:45:09,800
which is essentially also to sample

727
00:45:11,190 --> 00:45:12,230
application and that's

728
00:45:13,410 --> 00:45:15,550
independence testing dependence measures

729
00:45:17,530 --> 00:45:18,970
and i will actually start

730
00:45:19,880 --> 00:45:20,360
and this one

731
00:45:23,130 --> 00:45:24,240
one way to do it

732
00:45:24,550 --> 00:45:25,600
independence testing

733
00:45:26,000 --> 00:45:27,350
between two variables x and why

734
00:45:29,070 --> 00:45:30,190
at is formally to say

735
00:45:30,650 --> 00:45:33,690
well the question of independence is just a question whether the

736
00:45:34,360 --> 00:45:35,270
joint distribution

737
00:45:36,000 --> 00:45:38,820
is identical to the product of the marginal distributions

738
00:45:40,190 --> 00:45:40,660
and so

739
00:45:41,430 --> 00:45:45,530
if we could embed the joint distribution into a reproducing kernel hilbert space and do

740
00:45:45,530 --> 00:45:47,310
the same for the product of the marginals

741
00:45:48,030 --> 00:45:51,260
and then compute the difference vector the length of the difference after we have a

742
00:45:51,260 --> 00:45:53,560
beautiful test statistics four four independence

743
00:45:54,580 --> 00:45:55,020
in there

744
00:45:56,410 --> 00:45:57,940
because we need to do this now

745
00:45:58,360 --> 00:46:02,340
based on empirical data times the simplest methods not the best but it was the

746
00:46:02,340 --> 00:46:03,480
first one that we work on but

747
00:46:04,220 --> 00:46:06,210
the simplest methods that to obtain

748
00:46:06,710 --> 00:46:08,500
empirical estimates that these two things

749
00:46:09,300 --> 00:46:10,800
is feasible if we

750
00:46:11,430 --> 00:46:13,820
have a sample of observations x and why

751
00:46:14,360 --> 00:46:16,190
that's a sample from the joint distribution so

752
00:46:16,840 --> 00:46:18,510
this is the thing that we embed over here

753
00:46:19,420 --> 00:46:20,340
now given the sample

754
00:46:21,000 --> 00:46:22,110
o pairs x and why

755
00:46:22,880 --> 00:46:24,560
and how do we create a sample from

756
00:46:25,180 --> 00:46:26,080
this factorized

757
00:46:26,600 --> 00:46:28,300
distribution product the marginals

758
00:46:28,860 --> 00:46:31,670
one way to do that is simply to permute all the axis

759
00:46:32,160 --> 00:46:33,710
so if we permute the axis then

760
00:46:34,390 --> 00:46:36,000
the relationship between so

761
00:46:36,660 --> 00:46:37,420
we have to write this

762
00:46:42,490 --> 00:46:44,790
so i have a set of observations that looks like this

763
00:46:48,550 --> 00:46:52,540
and then sample from some distribution joint distribution over x and why

764
00:46:53,440 --> 00:46:57,690
and know if i take a fixed permutation sigma

765
00:46:59,450 --> 00:47:00,140
i permute

766
00:47:00,570 --> 00:47:01,740
sorry one

767
00:47:10,560 --> 00:47:11,560
i do this limitation

768
00:47:12,330 --> 00:47:12,990
and then of course

769
00:47:14,890 --> 00:47:20,050
the pairs are no longer samples from the joint distribution i have broken the dependence between the two things

770
00:47:20,710 --> 00:47:26,050
but of course exercise still samples from the marginal distribution of x and the sigma of

771
00:47:26,450 --> 00:47:27,270
why story

772
00:47:28,530 --> 00:47:29,550
sigma of why

773
00:47:31,830 --> 00:47:34,630
asked still samples from the model distribution of why so that

774
00:47:35,150 --> 00:47:35,530
these are

775
00:47:36,590 --> 00:47:37,920
nice samples from

776
00:47:38,600 --> 00:47:41,260
there must be some the product of the marginal distributions

777
00:47:42,090 --> 00:47:45,900
so that's an idea how to to independence testing and it works quite well and you can

778
00:47:47,720 --> 00:47:51,900
it's a real independence testing is not just first-order is not just looking for a

779
00:47:51,900 --> 00:47:53,470
zero covariance or something like that

780
00:47:54,350 --> 00:47:58,650
and actually what it is will depend on the kernel that you choose so for instance if you're

781
00:47:59,480 --> 00:48:04,530
take a polynomial kernel of a certain degree and to compute this kind of a test statistic here

782
00:48:07,780 --> 00:48:11,530
this thing the empirical estimate is that of course he will test for independence after

783
00:48:11,530 --> 00:48:13,840
a certain order but if you take a course in can with

784
00:48:14,480 --> 00:48:16,510
principal before full independence

785
00:48:19,070 --> 00:48:21,080
so this one i think i'm going to skip

786
00:48:22,590 --> 00:48:25,730
and i want to talk about the representer theorem that maybe

787
00:48:26,210 --> 00:48:27,500
now is the time to

788
00:48:28,600 --> 00:48:30,380
briefly ask if you have any questions

789
00:48:35,150 --> 00:48:36,400
there was one here and then here

790
00:48:42,500 --> 00:48:43,990
why i

791
00:48:50,020 --> 00:48:50,400
and say it

792
00:48:50,810 --> 00:48:51,860
how many computations

793
00:48:54,460 --> 00:48:55,350
how many permutations

794
00:48:56,430 --> 00:48:58,310
well in principle you could take one computation

795
00:49:01,270 --> 00:49:03,030
maybe you can do a better job i've been doing more

796
00:49:03,800 --> 00:49:04,460
but in principle

797
00:49:04,860 --> 00:49:06,230
already wanted to sample

798
00:49:09,090 --> 00:49:09,630
you can do some

799
00:49:10,050 --> 00:49:10,290
i mean

800
00:49:11,490 --> 00:49:14,100
you could also some modifications but in principle you can do one

801
00:49:16,490 --> 00:49:17,590
there was a second question

802
00:49:28,030 --> 00:49:28,550
so can you

803
00:49:29,060 --> 00:49:33,390
can you compress the data set by by doing the kernel mean embeddings as question

804
00:49:34,040 --> 00:49:34,970
interesting questions so

805
00:49:35,450 --> 00:49:36,020
the rationale

806
00:49:37,010 --> 00:49:38,450
we're not losing information

807
00:49:39,000 --> 00:49:40,630
uh by doing this mapping

808
00:49:43,080 --> 00:49:43,830
and it looks like

809
00:49:44,600 --> 00:49:46,890
the whole dataset represented by this one object

810
00:49:47,740 --> 00:49:48,980
was which in principle it's

811
00:49:49,580 --> 00:49:50,920
in three-dimensional space

812
00:49:53,090 --> 00:49:54,480
so one in a sense one could say

813
00:49:57,500 --> 00:50:00,830
we represent the data set by this function and if we are able to

814
00:50:01,890 --> 00:50:04,590
in code this function more compactly than the data set then

815
00:50:05,580 --> 00:50:06,360
we might have some

816
00:50:06,820 --> 00:50:08,180
some means of compressing

817
00:50:08,900 --> 00:50:09,630
the dataset

818
00:50:12,890 --> 00:50:16,900
i is that a question marks one is a question mark can we compress the function

819
00:50:17,580 --> 00:50:19,280
more easily than the data set

820
00:50:19,680 --> 00:50:20,440
maybe that's the case

821
00:50:21,300 --> 00:50:23,080
and if we can do that and

822
00:50:23,500 --> 00:50:26,500
how do we know about the mappings so now we know in principle it's invisible

823
00:50:27,290 --> 00:50:30,020
and we don't have an explicit form for the inversion

824
00:50:33,040 --> 00:50:33,460
actually that

825
00:50:34,130 --> 00:50:35,670
inverting this kind of mapping is

826
00:50:36,130 --> 00:50:37,240
the general question in

827
00:50:38,320 --> 00:50:40,560
support vector machines and other kernel methods

828
00:50:41,570 --> 00:50:44,340
is related to what people call the preimage problem solving

829
00:50:45,270 --> 00:50:46,960
support vector machines we

830
00:50:47,400 --> 00:50:48,090
we typically

831
00:50:49,320 --> 00:50:51,830
keep talking about this means haven't properly explain them yet

832
00:50:52,450 --> 00:50:53,730
we typically end up with the

833
00:50:54,400 --> 00:50:55,630
in the case of classification

834
00:50:57,330 --> 00:50:59,240
we end up with a solution that looks like this

835
00:51:03,500 --> 00:51:04,450
with a constant here

836
00:51:05,270 --> 00:51:05,680
in the

837
00:51:06,170 --> 00:51:08,130
can be the case that so the some

838
00:51:09,900 --> 00:51:11,360
well i would say more about this later

839
00:51:12,020 --> 00:51:13,800
it goes over all training points but

840
00:51:14,250 --> 00:51:18,230
usually a large fraction of their advisors zero but nevertheless you could say that if

841
00:51:18,230 --> 00:51:20,170
you have millions of training points you could still have

842
00:51:20,800 --> 00:51:22,350
i don't know hundreds of thousands of

843
00:51:22,930 --> 00:51:26,540
expansion points where this doesn't vanish so you you might have a very

844
00:51:27,350 --> 00:51:31,440
very rich between the hilbert space now let's rewrite this a little bit

845
00:51:32,340 --> 00:51:34,000
and this is actually the same as

846
00:51:34,670 --> 00:51:35,630
so we have some

847
00:51:37,470 --> 00:51:38,060
without some

848
00:51:38,900 --> 00:51:40,400
we have some weight vector

849
00:51:41,040 --> 00:51:41,900
and the dot product

850
00:51:42,650 --> 00:51:43,580
we phi of x

851
00:51:45,610 --> 00:51:46,320
plus being

852
00:51:47,490 --> 00:51:48,770
now if we were able to

853
00:51:48,770 --> 00:51:51,590
i said we're going to eliminate c one c two

854
00:51:51,670 --> 00:51:56,480
he thinks one obviously two likes to be eliminated

855
00:51:56,520 --> 00:51:58,360
let's call it back

856
00:51:58,420 --> 00:52:01,440
look at this equation

857
00:52:01,480 --> 00:52:03,550
substitutes in here

858
00:52:03,630 --> 00:52:07,800
well because credit substituted omega minus

859
00:52:07,860 --> 00:52:10,590
but you get an answer for c one c

860
00:52:10,610 --> 00:52:14,110
what think that the answer is

861
00:52:14,170 --> 00:52:15,690
what do you think the answer is

862
00:52:17,190 --> 00:52:20,320
not even one plus y

863
00:52:22,070 --> 00:52:23,900
so you get one of two

864
00:52:24,000 --> 00:52:25,290
is possible

865
00:52:25,460 --> 00:52:26,610
if you

866
00:52:26,630 --> 00:52:28,290
this omega zero

867
00:52:28,300 --> 00:52:29,210
you put it

868
00:52:30,610 --> 00:52:33,520
you get on with zero square a minus sign

869
00:52:33,520 --> 00:52:39,550
a plus sign you only get squared divided by only gets credit plus one

870
00:52:39,590 --> 00:52:42,110
and now you take the second solution

871
00:52:42,150 --> 00:52:43,360
and you put it

872
00:52:44,960 --> 00:52:47,250
what i think you're going to find

873
00:52:47,360 --> 00:52:50,170
my wife

874
00:52:50,190 --> 00:52:51,000
he one

875
00:52:51,000 --> 00:52:52,610
over c two

876
00:52:52,650 --> 00:52:55,420
it's not mine

877
00:52:55,500 --> 00:52:57,320
so what you have seen now

878
00:52:57,340 --> 00:53:00,400
is that the general

879
00:53:02,070 --> 00:53:03,570
comes up

880
00:53:04,380 --> 00:53:08,030
frequencies comes up with the ratios of the amplitude

881
00:53:08,090 --> 00:53:12,110
not with the individual amplitudes because you don't know the initial condition

882
00:53:12,170 --> 00:53:13,980
comes up with the ratio of the

883
00:53:14,020 --> 00:53:16,920
amplitudes and now you can write

884
00:53:18,880 --> 00:53:20,440
provided that you know

885
00:53:20,440 --> 00:53:21,960
the initial conditions

886
00:53:21,960 --> 00:53:23,630
note initial conditions

887
00:53:23,650 --> 00:53:25,340
you can also find

888
00:53:25,380 --> 00:53:27,420
c one

889
00:53:27,460 --> 00:53:29,480
and therefore node ratio

890
00:53:29,550 --> 00:53:31,570
automatically c two

891
00:53:31,670 --> 00:53:33,610
and you can find this one

892
00:53:33,610 --> 00:53:35,570
i'll give a minus sign

893
00:53:35,610 --> 00:53:36,820
it's a plus sign

894
00:53:36,880 --> 00:53:40,880
and then you know to ratio in that model

895
00:53:40,940 --> 00:53:43,790
so now you may think erroneously

896
00:53:43,840 --> 00:53:45,520
and life is easy

897
00:53:47,050 --> 00:53:50,090
that's far from the truth

898
00:53:50,130 --> 00:53:54,360
so all you have something as simple as this

899
00:53:57,150 --> 00:54:03,130
called double pendulum

900
00:54:03,150 --> 00:54:06,340
there is no symmetry

901
00:54:06,360 --> 00:54:10,090
one the test your intuition

902
00:54:10,130 --> 00:54:11,690
one of the two modes

903
00:54:11,730 --> 00:54:16,710
two normal modes low and high one because the two object

904
00:54:16,730 --> 00:54:20,380
in the lowest frequency use your hands and your legs

905
00:54:20,400 --> 00:54:27,000
what will this pendulum look like

906
00:54:27,070 --> 00:54:29,420
all doing sort of the right thing

907
00:54:29,480 --> 00:54:33,750
would look like this

908
00:54:33,770 --> 00:54:37,800
what would it look like this

909
00:54:37,840 --> 00:54:40,690
in other words let me make this one a little bit

910
00:54:40,730 --> 00:54:43,110
let me make the difference in the larger

911
00:54:43,110 --> 00:54:47,380
would look like this

912
00:54:47,420 --> 00:54:50,710
what would it look like this

913
00:54:50,800 --> 00:54:53,090
slightly exaggerated

914
00:54:53,090 --> 00:54:55,900
it was for this one

915
00:54:55,920 --> 00:54:58,050
OK that means that the ratio

916
00:54:58,090 --> 00:55:00,000
c to oversee one

917
00:55:00,020 --> 00:55:02,420
it's going to be a plus two

918
00:55:04,050 --> 00:55:07,050
was for this one

919
00:55:08,300 --> 00:55:09,440
believe it or not

920
00:55:09,500 --> 00:55:10,650
c two

921
00:55:10,670 --> 00:55:12,840
over c one

922
00:55:12,920 --> 00:55:14,420
is going to be

923
00:55:18,770 --> 00:55:20,360
the square root of two

924
00:55:20,590 --> 00:55:23,420
welcome to the grindhouse

925
00:55:23,440 --> 00:55:26,650
to find and i'm not exaggerating when i say thirty minutes

926
00:55:26,670 --> 00:55:29,590
you have to go through the whole procedure

927
00:55:29,630 --> 00:55:30,940
and then you'll find

928
00:55:31,000 --> 00:55:32,550
this ratio

929
00:55:32,570 --> 00:55:36,000
two point four

930
00:55:36,050 --> 00:55:40,000
and i will demonstrate

931
00:55:40,020 --> 00:55:41,340
the highest the

932
00:55:43,250 --> 00:55:45,570
must be something like this

933
00:55:45,590 --> 00:55:48,480
right because

934
00:55:50,090 --> 00:55:51,610
they must be out of place

935
00:55:51,630 --> 00:55:54,920
and the idea of the ratio

936
00:55:54,960 --> 00:55:56,840
c one over c two

937
00:55:56,880 --> 00:55:58,320
called c one

938
00:55:58,340 --> 00:56:06,210
all this into any one of you any intuition

939
00:56:06,210 --> 00:56:07,690
one is

940
00:56:07,690 --> 00:56:13,590
well you

941
00:56:13,730 --> 00:56:15,650
minus two point four

942
00:56:15,650 --> 00:56:16,710
this one

943
00:56:16,880 --> 00:56:19,460
be much further away than this one

944
00:56:19,480 --> 00:56:23,190
any one of you want to make a guess with omega minus is anyone to

945
00:56:23,190 --> 00:56:26,050
make because we only have pluses

946
00:56:26,070 --> 00:56:28,110
no way on earth that you

947
00:56:28,130 --> 00:56:32,400
five anyone else can look at this and say oh yes of course on minuses

948
00:56:32,460 --> 00:56:33,320
one hundred

949
00:56:33,340 --> 00:56:35,340
thirty minutes of calculated

950
00:56:35,340 --> 00:56:38,460
and then out of that pop these

951
00:56:38,500 --> 00:56:41,380
frequencies and out of that then pop the

952
00:56:41,400 --> 00:56:43,920
the ratios

953
00:56:43,980 --> 00:56:46,340
and now i'm going to demonstrate to you

954
00:56:46,380 --> 00:56:49,300
just this case

955
00:56:49,320 --> 00:56:51,610
the way and going to do that

956
00:56:51,650 --> 00:56:55,190
even though we have not discussed driven coupled oscillators

957
00:56:55,250 --> 00:56:59,630
in order to set it off in these normal modes normal mode

958
00:56:59,670 --> 00:57:02,360
is also a resonance frequency

959
00:57:02,380 --> 00:57:05,380
recall it often natural frequency

960
00:57:05,420 --> 00:57:09,880
is what the system likes to do if you leave it alone

961
00:57:09,960 --> 00:57:14,070
it is easy for me when i just to use a little bit excited in

962
00:57:14,070 --> 00:57:15,380
the rest of the world

963
00:57:15,440 --> 00:57:19,300
so therefore i am going to drive the only very briefly

964
00:57:19,300 --> 00:57:21,430
and the complete

965
00:57:21,450 --> 00:57:25,000
relationship is that led the prime the one that you will receive

966
00:57:27,340 --> 00:57:29,790
and one minus better

967
00:57:29,850 --> 00:57:31,930
cosine fade

968
00:57:31,980 --> 00:57:33,430
divided by

969
00:57:33,500 --> 00:57:35,270
the square root

970
00:57:35,280 --> 00:57:37,970
one minus beta screen

971
00:57:38,010 --> 00:57:39,240
and they

972
00:57:39,760 --> 00:57:40,580
this is the

973
00:57:40,590 --> 00:57:43,020
over c

974
00:57:43,030 --> 00:57:47,640
c being the speed of light

975
00:57:47,700 --> 00:57:50,800
so you see that if fatah

976
00:57:50,850 --> 00:57:54,050
it's smaller than ninety degrees

977
00:57:54,110 --> 00:57:56,630
but larger than zero

978
00:57:56,670 --> 00:57:59,480
that means you are approaching each other

979
00:57:59,520 --> 00:58:03,160
this stands for approaching

980
00:58:03,180 --> 00:58:05,410
that means that led the prime

981
00:58:05,490 --> 00:58:06,380
is then

982
00:58:06,390 --> 00:58:08,350
smaller than that

983
00:58:08,390 --> 00:58:09,220
so you

984
00:58:09,230 --> 00:58:12,710
caught a smaller wavelengths when coming to you

985
00:58:12,760 --> 00:58:13,560
and we

986
00:58:13,590 --> 00:58:15,100
have a name for that

987
00:58:15,120 --> 00:58:16,350
and the name for that

988
00:58:16,360 --> 00:58:18,380
is blue shift

989
00:58:18,390 --> 00:58:22,500
and the reason why that is called blue shift is obvious

990
00:58:22,510 --> 00:58:24,760
blue shift

991
00:58:24,950 --> 00:58:28,890
because blue light has a shorter wavelength than red light

992
00:58:28,910 --> 00:58:31,590
so even though this

993
00:58:31,630 --> 00:58:35,460
this doppler effect may not even be indivisible part of the spectrum we still call

994
00:58:35,460 --> 00:58:37,090
that blue shift

995
00:58:37,130 --> 00:58:39,260
so when you approach

996
00:58:39,310 --> 00:58:41,800
the relative velocities approaching

997
00:58:41,810 --> 00:58:45,510
you go towards shorter wavelengths we call the blue shift

998
00:58:45,520 --> 00:58:47,280
now if a down

999
00:58:47,920 --> 00:58:49,880
is between hundred eighty degrees

1000
00:58:50,330 --> 00:58:51,800
ninety degrees

1001
00:58:51,810 --> 00:58:54,930
so that means you are receding from each other

1002
00:58:54,960 --> 00:58:58,870
well and the prime is larger than life

1003
00:58:58,910 --> 00:59:00,950
and of course you've guessed it

1004
00:59:00,960 --> 00:59:05,590
that for redshift

1005
00:59:05,730 --> 00:59:17,200
and astronomers talk all the time about redshift and blue shift

1006
00:59:17,210 --> 00:59:20,750
if beta square is much much smaller than one

1007
00:59:20,790 --> 00:59:24,860
you can forget about the downstairs if you like that

1008
00:59:24,870 --> 00:59:27,080
if you take desecrated

1009
00:59:27,100 --> 00:59:29,730
if you take

1010
00:59:31,400 --> 00:59:36,260
ten percent of the speed of light which is substantial right reaches thirty thousand kilometres

1011
00:59:36,260 --> 00:59:37,610
per second

1012
00:59:37,630 --> 00:59:39,950
so the data is o point one

1013
00:59:40,020 --> 00:59:42,020
then they desecrated

1014
00:59:42,050 --> 00:59:44,960
is approximately o point o one

1015
00:59:45,000 --> 00:59:46,930
and so downstairs

1016
00:59:46,950 --> 00:59:49,350
one minus one hundred you could

1017
00:59:49,390 --> 00:59:51,150
reasonable approximation

1018
00:59:51,160 --> 00:59:52,430
it nor that

1019
00:59:52,450 --> 00:59:55,590
but that depends of course on the accuracy that you want

1020
00:59:55,630 --> 00:59:58,540
in that case you would get lambda prime

1021
00:59:58,630 --> 01:00:00,610
is approximately lambda

1022
01:00:00,650 --> 01:00:02,790
times one minus cosine

1023
01:00:02,850 --> 01:00:04,290
one minus

1024
01:00:05,200 --> 01:00:08,260
and cosine theta

1025
01:00:09,870 --> 01:00:15,470
and now you want to know it in terms of frequency

1026
01:00:15,540 --> 01:00:16,990
then f prime

1027
01:00:17,010 --> 01:00:18,370
is roughly f

1028
01:00:18,390 --> 01:00:19,770
times one plus

1029
01:00:20,600 --> 01:00:22,260
cosine theta

1030
01:00:22,370 --> 01:00:25,880
and i'll leave you with proving that this is indeed the

1031
01:00:25,880 --> 01:00:28,230
good approximation

1032
01:00:28,270 --> 01:00:31,350
doppler shift being used in sports

1033
01:00:31,360 --> 01:00:33,840
to measure the speed of tennis balls and

1034
01:00:33,890 --> 01:00:36,090
baseball's when they're being pitched

1035
01:00:36,130 --> 01:00:39,990
radar is sent to the object and it reflects of the object and so you

1036
01:00:39,990 --> 01:00:41,580
can measure this way

1037
01:00:41,600 --> 01:00:42,750
the speed

1038
01:00:42,760 --> 01:00:46,600
weather radar works the same way you can measure the speed with which

1039
01:00:46,640 --> 01:00:50,100
weather approaches you or goes away from you

1040
01:00:50,110 --> 01:00:53,330
the police can check your speed of your car

1041
01:00:53,370 --> 01:00:56,150
century or to you'd bounces of your car

1042
01:00:56,160 --> 01:01:01,480
and then it can send it sense you speed and in my problem seven in

1043
01:01:01,480 --> 01:01:05,020
my problems at seven i think i have a problem based on that if you're

1044
01:01:05,020 --> 01:01:06,920
approaching the police

1045
01:01:08,520 --> 01:01:12,530
so if data is zero then of course you would simply get one minus baby

1046
01:01:12,860 --> 01:01:15,760
one was played data can be zero of course

1047
01:01:15,770 --> 01:01:21,310
doesn't have to be but it can be

1048
01:01:21,370 --> 01:01:22,640
you can imagine how

1049
01:01:22,650 --> 01:01:24,160
important to role

1050
01:01:24,170 --> 01:01:26,740
was that displayed in astronomy

1051
01:01:26,750 --> 01:01:28,500
because this allows you know

1052
01:01:28,530 --> 01:01:32,120
to calculate the velocity of stars relative to

1053
01:01:32,160 --> 01:01:34,660
our solar system relative to us

1054
01:01:34,680 --> 01:01:39,470
because almost all stars in their respective show absorption lines

1055
01:01:39,540 --> 01:01:45,000
and if you know the absorption line frequency already absorption line wavelength as measured in

1056
01:01:45,000 --> 01:01:50,810
the following content is provided under creative commons license your support will help MIT opencourseware

1057
01:01:50,810 --> 01:01:54,890
continue to offer high quality educational resources for free

1058
01:01:54,910 --> 01:01:59,710
to make a donation or to view additional materials from hundreds of MIT courses

1059
01:01:59,730 --> 01:02:05,610
this visit MIT opencourseware OCW MIT that EDU

1060
01:02:05,660 --> 01:02:08,500
settle down there

1061
01:02:08,550 --> 01:02:10,460
this is not

1062
01:02:10,520 --> 01:02:13,840
some other class three online

1063
01:02:13,840 --> 01:02:18,240
so i have an important announcement there will be a class will be lecture next

1064
01:02:18,240 --> 01:02:21,730
wednesday instead of be another celebration of learning

1065
01:02:21,730 --> 01:02:23,780
you're going to comment going to show us

1066
01:02:23,800 --> 01:02:26,770
what you've learned it's going to be so

1067
01:02:29,940 --> 01:02:33,550
i want to make sure that we're all celebrating the same thing

1068
01:02:36,950 --> 01:02:41,720
i meant the patch didn't get patched i apologize so let me tell you what

1069
01:02:41,800 --> 01:02:42,530
it is

1070
01:02:42,590 --> 01:02:44,940
what we're going to look at is

1071
01:02:45,470 --> 01:02:47,160
lectures starting with

1072
01:02:47,200 --> 01:02:48,760
lecture seventeen

1073
01:02:48,840 --> 01:02:50,830
which was

1074
01:02:50,890 --> 01:02:55,980
on x-rays and halfway through that lecture was the breakpoint for the last test so

1075
01:02:55,980 --> 01:02:59,480
we will start with the material

1076
01:02:59,530 --> 01:03:03,560
beginning with bragg's law we didn't talk about diffraction

1077
01:03:03,610 --> 01:03:06,250
the last test the coverage

1078
01:03:06,270 --> 01:03:11,860
talk about generation of x-rays but not their using diffraction so starting with x-ray diffraction

1079
01:03:11,880 --> 01:03:15,920
up through the end of last days lecture

1080
01:03:15,940 --> 01:03:20,830
and the reason i'm doing this is that tomorrow is a holiday there are no

1081
01:03:20,830 --> 01:03:23,060
academic exercises tomorrow

1082
01:03:23,060 --> 01:03:25,550
and so i didn't think it made

1083
01:03:25,560 --> 01:03:29,340
good pedagogical sense to teachers something today

1084
01:03:29,360 --> 01:03:32,030
you have no recitation on thursday

1085
01:03:32,050 --> 01:03:36,720
you don't need again with your recitation structure until tuesday and wednesday is the test

1086
01:03:38,440 --> 01:03:42,170
being mindful that i'm going to start talking about solutions today none of that will

1087
01:03:42,170 --> 01:03:44,580
be on the test so starting with

1088
01:03:44,590 --> 01:03:49,110
x-ray diffraction and ending with diffusion so you

1089
01:03:49,160 --> 01:03:50,640
as i got up here

1090
01:03:50,720 --> 01:03:55,760
the suffix law's first and second laws are function all that stuff

1091
01:03:57,740 --> 01:04:02,690
and of course we have test a weekly quiz on tuesday

1092
01:04:02,750 --> 01:04:04,880
will have plenty to

1093
01:04:04,930 --> 01:04:06,840
to work on four wednesday so

1094
01:04:06,900 --> 01:04:08,370
let's leave it that way

1095
01:04:08,380 --> 01:04:12,000
so last day we completed the discussion of diffusion

1096
01:04:12,060 --> 01:04:15,380
going through analytical

1097
01:04:15,470 --> 01:04:18,530
presentation of fixed laws and how they are used in

1098
01:04:18,530 --> 01:04:20,090
quantitative measure of

1099
01:04:20,090 --> 01:04:23,160
of range phenomena in diffusion

1100
01:04:23,220 --> 01:04:25,060
today what i want to do is that

1101
01:04:25,160 --> 01:04:28,930
introduce another topic from the general realm of physical chemistry

1102
01:04:28,970 --> 01:04:31,880
and this is the topic of solutions

1103
01:04:31,930 --> 01:04:34,400
and for that we all the way back to the

1104
01:04:34,410 --> 01:04:38,250
the very first lecture three online one we looked at this

1105
01:04:38,250 --> 01:04:40,620
figure from the text

1106
01:04:40,660 --> 01:04:45,560
and what we've been doing up until now was largely working over in the lower

1107
01:04:45,560 --> 01:04:46,840
left corner

1108
01:04:46,850 --> 01:04:49,690
this taxonomy we've been looking at

1109
01:04:49,690 --> 01:04:54,720
the properties of elements we've looked at a few compounds but we really haven't gotten

1110
01:04:54,720 --> 01:04:58,600
up into here everything has been pure pretty much

1111
01:04:58,660 --> 01:05:01,380
but we have had to

1112
01:05:01,440 --> 01:05:06,030
make the odd for a we talked about glasses we got into the discussion of

1113
01:05:06,030 --> 01:05:11,180
solutions because we were talking about how you could modify the properties detail the properties

1114
01:05:11,180 --> 01:05:17,750
by changing composition of the glass so that was really in this domain of of

1115
01:05:17,750 --> 01:05:22,940
mixtures and we talked about doping of semiconductors and we don't the semiconductor actually put

1116
01:05:22,940 --> 01:05:28,500
phosphorus or boron substitution for silicon solar masses were making a solution

1117
01:05:28,530 --> 01:05:32,510
so that's all of them here but i am a little bit

1118
01:05:32,530 --> 01:05:37,960
uncomfortable with this broad definition and make sure so i've taken the liberty of refining

1119
01:05:37,960 --> 01:05:39,070
that someone

1120
01:05:39,090 --> 01:05:44,540
so what i'd prefer you recognise is that there's two types of mixtures

1121
01:05:44,590 --> 01:05:48,750
as homogeneous mixtures and heterogeneous mixtures

1122
01:05:49,740 --> 01:05:55,410
homogeneous mixtures are what we would call solutions and that gives you mixing right down

1123
01:05:55,410 --> 01:05:57,910
at the molecular level

1124
01:05:57,960 --> 01:06:00,250
whereas heterogeneous

1125
01:06:00,810 --> 01:06:05,890
mixtures are not mixed down to the molecular level there mixed down to the level

1126
01:06:05,890 --> 01:06:07,290
of clusters

1127
01:06:08,640 --> 01:06:10,520
so for example

1128
01:06:10,570 --> 01:06:12,900
if i look at something like brian

1129
01:06:12,940 --> 01:06:14,070
brian is

1130
01:06:14,100 --> 01:06:17,310
salt dissolved in water it's visually

1131
01:06:19,060 --> 01:06:25,740
we cannot tell that there is more than one component and its this homogeneity uniformity

1132
01:06:25,750 --> 01:06:27,420
that makes the solution

1133
01:06:27,520 --> 01:06:32,260
whereas if you look at something like milk milk is a mixture it's an aggregate

1134
01:06:32,260 --> 01:06:36,120
milk actually consists of two phases there's the fatty phase

1135
01:06:36,140 --> 01:06:40,250
which is insoluble in the aqueous phase

1136
01:06:40,310 --> 01:06:44,260
and what's happened is we've got to a find dispersion

1137
01:06:44,270 --> 01:06:47,800
the second phase in the first phase

1138
01:06:47,830 --> 01:06:52,250
and i find dispersion flow to the fact that is a clear and colorless

1139
01:06:52,260 --> 01:06:57,180
and the aqueous phase is clear and colorless but that dispersion scatters light that's where

1140
01:06:57,180 --> 01:06:58,940
we get the the white

1141
01:06:58,960 --> 01:07:00,650
in milk

1142
01:07:00,740 --> 01:07:02,810
and i don't like the notion that

1143
01:07:02,810 --> 01:07:04,850
whether it's milking or

1144
01:07:04,870 --> 01:07:07,760
clear and colorless in other words

1145
01:07:07,770 --> 01:07:12,480
coarsely next or finally makes we simply under so i'm going to be talking today

1146
01:07:12,490 --> 01:07:14,560
about about solutions

1147
01:07:16,570 --> 01:07:20,020
making sure that we understand the the underlying

1148
01:07:21,380 --> 01:07:22,670
so you might say well

1149
01:07:22,680 --> 01:07:26,890
we signed up for solid state chemistry wisely talking about solutions to give you three

1150
01:07:27,830 --> 01:07:30,120
three reasons first of all

1151
01:07:30,180 --> 01:07:31,690
we have

1152
01:07:32,900 --> 01:07:36,000
the systems that we're looking at are

1153
01:07:37,990 --> 01:07:39,670
it's very rare to use

1154
01:07:39,680 --> 01:07:41,260
pure substances

1155
01:07:41,270 --> 01:07:46,300
in engineering applications so we we need to understand solutions from that standpoint is we've

1156
01:07:46,300 --> 01:07:51,250
learned how to tailor the properties of glasses head to tail properties of semiconductors until

1157
01:07:51,330 --> 01:07:57,290
the properties of metal alloys and so on secondly main materials are processed out of

1158
01:07:58,850 --> 01:08:00,860
the way we make fine powders

1159
01:08:00,880 --> 01:08:02,190
it's not too

1160
01:08:02,250 --> 01:08:08,180
make little bolton castle liquid into tiny tiny little malls we make fine powders typically

1161
01:08:08,180 --> 01:08:13,860
by precipitation out of solution so we need to understand the laws that govern

1162
01:08:13,880 --> 01:08:19,940
disillusioned and x solution precipitation and thirdly what one of the major units that we're

1163
01:08:19,940 --> 01:08:24,320
going to cover before the end of the semester involves biochemistry

1164
01:08:24,350 --> 01:08:26,500
and the environment for

1165
01:08:26,500 --> 01:08:28,630
most of

1166
01:08:28,690 --> 01:08:33,510
while chemicals is the wet environment so i think its requisite that we know something

1167
01:08:33,510 --> 01:08:36,850
about that when far so that's

1168
01:08:36,860 --> 01:08:39,240
that's sort of the background preamble

1169
01:08:39,280 --> 01:08:43,240
that's what i want to do today is to introduce some definitions so wilburys through

1170
01:08:43,240 --> 01:08:49,460
some material on the handouts and ultimately get to the emissivity rules that's we really

1171
01:08:49,460 --> 01:08:54,320
want we want to know the admissibility rules the rules that govern when

1172
01:08:54,380 --> 01:08:59,810
two substances are three or more substances are going to max and for solutions so

1173
01:08:59,810 --> 01:09:04,380
that's what we're heading to miss ability rules

1174
01:09:04,380 --> 01:09:10,580
so that's summarize on this slide so first F star again was dif defined like this

1175
01:09:10,580 --> 01:09:16,120
we know it's a convex function from the calculus of subgradients we know how to

1176
01:09:16,120 --> 01:09:21,480
calculate a subgradient of the conjugate and the rule that we saw in the

1177
01:09:21,510 --> 01:09:26,800
when we talked about subgradient calculus is as follows so to evaluate F star

1178
01:09:26,800 --> 01:09:31,300
and a conjugate at a certain Y you have to find the X that

1179
01:09:31,300 --> 01:09:38,760
maximizes this either numerically or analytically so the value of this maximum gives you the conjugate

1180
01:09:38,760 --> 01:09:48,320
function value the X that maximizes it is a subgradient of this function differentiated

1181
01:09:48,320 --> 01:09:53,860
respect to Y so that's very easy normally if you can evaluate the  conjugate

1182
01:09:53,860 --> 01:10:01,080
you also know  which vector X maximizes this in the supremum and any maximizer will

1183
01:10:01,080 --> 01:10:08,840
be a subgradient if the maximizer is unique then there is a unique subgradient and that

1184
01:10:08,840 --> 01:10:14,400
means the function is actually differentiable at Y so when is that the case well for example

1185
01:10:14,400 --> 01:10:22,600
if F of X is strictly convex and this supremum is finite then

1186
01:10:22,600 --> 01:10:28,660
there can be only one maximizer right because F is strictly convex and in that case F

1187
01:10:28,660 --> 01:10:33,420
star would be differentiable  and this maximizing X is not just a subgradient but also

1188
01:10:33,420 --> 01:10:42,620
the gradient so differentiability of the conjugate has to do with strict convexity of

1189
01:10:42,620 --> 01:10:54,180
the function F and then a stronger condition that's useful for these proximal gradient and

1190
01:10:54,180 --> 01:10:59,380
fast proximal gradient methods in the convergence analysis of the proximal gradient method to

1191
01:10:59,380 --> 01:11:11,840
be used we had this condition on G the differentiable part at the gradient

1192
01:11:12,220 --> 01:11:18,200
is Lipschitz continuous with constant L so what does that mean for the if G is the

1193
01:11:18,200 --> 01:11:27,140
conjugate of some convex function F well that means that the function F is

1194
01:11:27,190 --> 01:11:34,280
strongly convex and a strongly convex function is defined like this F is strongly convex

1195
01:11:34,300 --> 01:11:42,240
with parameter mu if you can subtract this quadratic function with coefficient mu from it

1196
01:11:42,260 --> 01:11:47,720
and the remainder is still a convex function if that's the case you call F strongly convex

1197
01:11:47,720 --> 01:11:55,320
with parameter mu and a nice property is that if F is strongly convex then its

1198
01:11:55,320 --> 01:12:03,240
conjugate is defined everywhere and the gradient of the conjugate is Lipschitz continuous with parameter one over mu

1199
01:12:03,240 --> 01:12:08,520
so the L  in the definition of the Lipschitz continu continuity is one over mu so

1200
01:12:08,520 --> 01:12:16,300
we'll see why this matters but now let's look at some possible applications of dual

1201
01:12:16,300 --> 01:12:23,580
methods suppose so we have a simple problem with only equality constraints and we use the

1202
01:12:23,620 --> 01:12:28,380
we want to solve this by a dual method then we've seen the dual is

1203
01:12:28,380 --> 01:12:33,340
the dual function is the negative of this B transpose nu plus the conjugate

1204
01:12:33,340 --> 01:12:38,640
of F applied to minus A transpose nu so one method we can always apply

1205
01:12:38,640 --> 01:12:44,860
is the  subgradient method  because this is always this is a convex problem

1206
01:12:44,860 --> 01:12:52,160
so we know how to compute a subgradient so to compute a subgradient we compute

1207
01:12:52,160 --> 01:12:59,780
the we minimize or we solve the maximization problem in the definition of F star which

1208
01:12:59,780 --> 01:13:04,400
is this maximization over X so if you write that as the minimization problem that

1209
01:13:04,400 --> 01:13:11,100
means you minimize F of X plus nu transpose A X that's the maximizing the solution

1210
01:13:11,100 --> 01:13:16,820
of that is the maximizer in this definition of their conjugate and we know that

1211
01:13:16,820 --> 01:13:22,260
it's also an gradient a subgradient of the conjugate from this previous slide

1212
01:13:22,890 --> 01:13:29,560
so this X plus  will be an the subgradient of F star and then

1213
01:13:29,560 --> 01:13:35,900
the subgradient of F star times will be the subgradient of F star

1214
01:13:35,900 --> 01:13:45,740
respect to nu  and then if you multiply with A you get

1215
01:13:46,020 --> 01:13:52,120
the subgradient so the subgradient of minus G this convex function will be

1216
01:13:52,120 --> 01:13:56,600
B equal to B that's that's the gradient of this first term and the

1217
01:13:56,600 --> 01:14:02,260
subgradient of the second term  is A times the vector X that minimizes this and this

1218
01:14:02,260 --> 01:14:09,080
second step is just a subgradient update for the dual variable nu it's nu

1219
01:14:09,080 --> 01:14:17,160
minus T times a subgradient and the subgradient was B minus A X right so this dual update is just a

1220
01:14:17,160 --> 01:14:22,220
subgradient update in an subgradient method the first step is how you compute

1221
01:14:22,220 --> 01:14:30,400
the subgradient of F star or if this has a unique maximizer or this

1222
01:14:30,400 --> 01:14:36,360
has a unique minimizer X then this is actually a gradient because then the subgradient is unique and

1223
01:14:36,360 --> 01:14:49,780
this is a gradient update is that clear so an

1224
01:14:49,780 --> 01:14:54,650
example of a strictly convex function that's not strongly convex would be one  over X

1225
01:14:54,650 --> 01:15:06,400
for example over the positive X so for  differentiable twice differentiable functions strong convexity

1226
01:15:06,640 --> 01:15:12,940
means that the smallest eigenvalue of the Hessian is greater than mu everywhere

1227
01:15:12,940 --> 01:15:19,340
so it's bounded away from zero  by a positive mu right so one over X is strictly

1228
01:15:19,340 --> 01:15:25,520
convex but it's not strongly convex because its second derivative is

1229
01:15:26,360 --> 01:15:39,600
goes to zero so when is this interesting this dual gradient method for solving this

1230
01:15:39,600 --> 01:15:45,520
problem well it's interesting because the dual update is very simple and it's interesting

1231
01:15:45,520 --> 01:15:52,360
when this minimization is easier than the original minimization because this is unconstrained you

1232
01:15:52,360 --> 01:15:57,440
just added a linear term to X and minimize this and so if this is easier

1233
01:15:57,440 --> 01:16:02,280
than the first problem then this could be an interesting method and one reason why

1234
01:16:02,280 --> 01:16:06,200
this might be easier is for example if this becomes separable because F of

1235
01:16:06,200 --> 01:16:13,600
X is separable as a sum of separable functions each depending on only one component of

1236
01:16:13,600 --> 01:16:19,860
X then this entire thing is separable and you can get a decoupled problem for

1237
01:16:19,860 --> 01:16:25,780
this minimization so that's used in dual decomposition so this is the same idea but in

1238
01:16:25,780 --> 01:16:32,920
more detail  just here I used inequality constraints but the idea is exactly the same so

1239
01:16:32,920 --> 01:16:35,220
here we have a separable objective

1240
01:16:35,360 --> 01:16:39,320
one term that depends on X one second term depends on the vec

1241
01:16:39,320 --> 01:16:45,840
variables X two but the constraints couple everything so you cannot if you didn't have constraints you

1242
01:16:45,840 --> 01:16:48,490
i've got enough vectors are not too many

1243
01:16:48,490 --> 01:16:50,280
it's a natural idea

1244
01:16:50,290 --> 01:16:51,640
of the basis

1245
01:16:51,640 --> 01:16:55,350
so basis is a bunch of vectors in this space

1246
01:16:55,410 --> 01:17:01,240
and it's so it's as sequence of vectors with two properties with

1247
01:17:08,880 --> 01:17:12,290
they are independent

1248
01:17:12,530 --> 01:17:18,940
and two

1249
01:17:19,040 --> 01:17:21,160
you know what's coming

1250
01:17:21,170 --> 01:17:28,930
they span the space

1251
01:17:35,000 --> 01:17:37,310
let me take

1252
01:17:37,320 --> 01:17:40,690
so time for examples of course

1253
01:17:40,790 --> 01:17:42,990
so i'm asking an output

1254
01:17:43,030 --> 01:17:44,670
definition one

1255
01:17:44,670 --> 01:17:47,930
the definition of independence

1256
01:17:47,940 --> 01:17:50,660
together with definition two

1257
01:17:50,680 --> 01:17:57,660
and let's look at examples because this is this combination means the set of vectors

1258
01:17:57,660 --> 01:17:59,530
i have it just right

1259
01:17:59,590 --> 01:18:02,250
and so this idea of the basis

1260
01:18:02,290 --> 01:18:05,760
will be central always be asking you now

1261
01:18:05,810 --> 01:18:10,150
four basis whenever i look at the subspace

1262
01:18:10,180 --> 01:18:13,480
if i ask you if you give me a basis for the subspace you've told

1263
01:18:13,480 --> 01:18:15,140
me what it is

1264
01:18:15,190 --> 01:18:19,710
you've told me everything i need to know about that subspace

1265
01:18:19,730 --> 01:18:24,580
those i take their combinations and i know that i need all the combination

1266
01:18:27,180 --> 01:18:32,180
OK so examples of the basis let me start with two dimensional space

1267
01:18:32,190 --> 01:18:33,360
follows the

1268
01:18:33,380 --> 01:18:35,320
the space a example

1269
01:18:35,390 --> 01:18:40,560
this space is

1270
01:18:40,690 --> 01:18:43,130
let's make it answering

1271
01:18:43,180 --> 01:18:48,220
real three-dimensional space

1272
01:18:48,230 --> 01:18:53,650
give me one basis one basis is

1273
01:18:53,770 --> 01:18:57,240
so i want

1274
01:18:57,350 --> 01:18:59,710
some vectors

1275
01:18:59,750 --> 01:19:05,930
so if by asking for basis i'm asking for vectors a little list of vectors

1276
01:19:07,860 --> 01:19:15,070
there should be just right so what would be a basis for three-dimensional space

1277
01:19:15,090 --> 01:19:19,470
well the first basis that comes to mind when we write that down

1278
01:19:19,480 --> 01:19:22,330
in the first place that comes to mind is

1279
01:19:22,420 --> 01:19:24,920
this vector

1280
01:19:24,940 --> 01:19:27,320
this vector

1281
01:19:28,640 --> 01:19:30,990
this fact

1282
01:19:32,670 --> 01:19:35,740
as one basis

1283
01:19:35,790 --> 01:19:39,410
not the only basis it's going to be my point

1284
01:19:39,420 --> 01:19:42,170
but let's just say yes that's the basis

1285
01:19:42,180 --> 01:19:48,360
why is every are those vectors independent

1286
01:19:48,410 --> 01:19:51,630
so that's the like the x y and zee axes so

1287
01:19:51,650 --> 01:19:53,990
if those are not independent we're in trouble

1288
01:19:54,000 --> 01:19:55,830
so that they are

1289
01:19:55,840 --> 01:20:01,140
take a combinations c one of this vector plus c two of this vector plus

1290
01:20:01,200 --> 01:20:07,520
the three of that vector and try to make it give the zero vector

1291
01:20:07,540 --> 01:20:10,160
what this is

1292
01:20:10,200 --> 01:20:14,410
if c one of that policy to that policy three of that gives me zero

1293
01:20:14,410 --> 01:20:17,810
zero zero then the seas are all

1294
01:20:17,840 --> 01:20:22,220
zero right so that the test for independence

1295
01:20:22,220 --> 01:20:26,050
in the language of matrices

1296
01:20:26,110 --> 01:20:27,040
which was

1297
01:20:27,060 --> 01:20:28,930
under that board

1298
01:20:28,970 --> 01:20:31,910
i could make those the columns of the matrix

1299
01:20:31,930 --> 01:20:34,990
well it would be the identity matrix

1300
01:20:35,130 --> 01:20:39,040
but i would ask what's the null space of the identity matrix

1301
01:20:39,060 --> 01:20:40,680
and you would say

1302
01:20:40,720 --> 01:20:42,780
it's only the zero vector

1303
01:20:42,810 --> 01:20:46,990
and i would say fine then the columns are independent

1304
01:20:47,020 --> 01:20:52,140
the only thing the identity times of vector giving zero the only that vector that

1305
01:20:52,140 --> 01:20:55,050
does that is zero OK

1306
01:20:56,550 --> 01:21:02,100
that's not the only basis far from it tell me another basis and second basis

1307
01:21:02,110 --> 01:21:06,460
another basis

1308
01:21:11,690 --> 01:21:13,740
ah give me

1309
01:21:13,740 --> 01:21:15,140
OK so this tree

1310
01:21:15,150 --> 01:21:18,690
has several properties the first thing is that it's a binary search tree

1311
01:21:18,710 --> 01:21:22,120
OK so you can check in order to reversal should give these numbers in sorted

1312
01:21:22,120 --> 01:21:27,150
order three seven eight ten eleven eighteen twenty two twenty six so it's a valid

1313
01:21:27,150 --> 01:21:28,420
binary search tree

1314
01:21:28,430 --> 01:21:32,530
we depend on these leaves with no keys and they just hanging around as those

1315
01:21:32,530 --> 01:21:37,060
of the null pointers each of these you can call now

1316
01:21:37,640 --> 01:21:38,840
they're all just

1317
01:21:38,850 --> 01:21:42,830
marks there wherever there is absent child

1318
01:21:42,850 --> 01:21:45,030
and then double circles some of the nodes

1319
01:21:45,040 --> 01:21:46,940
the color than red

1320
01:21:46,990 --> 01:21:49,500
if i didn't the black knights wouldn't

1321
01:21:49,510 --> 01:21:51,190
match so

1322
01:21:51,210 --> 01:21:52,800
to be a little bit careful

1323
01:21:52,820 --> 01:21:53,820
for every

1324
01:21:53,830 --> 01:21:55,790
note we'd like to measure

1325
01:21:55,800 --> 01:22:00,630
the number of black nodes from that down to any descendant leaf so for example

1326
01:22:00,630 --> 01:22:03,760
the new pointers there black-eyed is

1327
01:22:03,770 --> 01:22:10,270
zero good sources answer so these guys all had black eyed zero just

1328
01:22:10,320 --> 01:22:14,500
some black high school zero

1329
01:22:14,550 --> 01:22:19,480
OK with the black of three

1330
01:22:19,530 --> 01:22:21,340
zero not

1331
01:22:22,190 --> 01:22:24,090
because these nodes are black

1332
01:22:24,100 --> 01:22:26,650
so of like i was one

1333
01:22:26,670 --> 01:22:29,780
they right that we don't count three even though it's black

1334
01:22:30,130 --> 01:22:33,120
we it's not included in the count but the

1335
01:22:33,130 --> 01:22:34,800
leaves count

1336
01:22:34,810 --> 01:22:37,740
and there's only two pass here in each of the same number

1337
01:22:37,760 --> 01:22:42,940
black nodes as they should over here let's say eights also has like i one

1338
01:22:42,950 --> 01:22:44,520
even though it's red

1339
01:22:44,540 --> 01:22:48,220
same with eleven

1340
01:22:48,270 --> 01:22:50,700
same with twenty six

1341
01:22:50,840 --> 01:22:55,580
each of them only has two pairs each of them has each path has one

1342
01:22:55,600 --> 01:22:57,160
like node on it

1343
01:22:59,060 --> 01:23:01,340
the black i

1344
01:23:01,350 --> 01:23:03,690
so one good

1345
01:23:03,700 --> 01:23:07,300
they don't count ten that there's now four

1346
01:23:07,310 --> 01:23:12,190
pastor leaves each of them contains exactly one like plus the route which we don't

1347
01:23:13,610 --> 01:23:15,820
twenty two same thing

1348
01:23:17,060 --> 01:23:20,270
OK this is getting a little more interesting is one pattern here which is one

1349
01:23:20,270 --> 01:23:23,990
block now this other paz here which are longer but they still only had one

1350
01:23:25,000 --> 01:23:27,990
so if we just sort of ignore the red nodes all these patterns have the

1351
01:23:27,990 --> 01:23:29,330
same like

1352
01:23:29,350 --> 01:23:34,990
eighteen should be bigger hopefully like i did too

1353
01:23:35,000 --> 01:23:39,800
because each of these pairs now has black that here one black node in leaves

1354
01:23:39,800 --> 01:23:43,480
or one black here in one like that in the leaves

1355
01:23:43,500 --> 01:23:45,830
and finally the richard have

1356
01:23:45,840 --> 01:23:49,190
black height of

1357
01:23:50,790 --> 01:23:54,550
and seriously over here i guess should has two black this

1358
01:23:54,590 --> 01:23:56,750
same over here

1359
01:23:58,300 --> 01:24:02,680
so that's just hopefully these properties make sense we didn't check all of them every

1360
01:24:02,760 --> 01:24:04,890
node has a black parents

1361
01:24:04,900 --> 01:24:06,850
look at all these paths

1362
01:24:06,860 --> 01:24:09,760
we sort of alternating red

1363
01:24:09,770 --> 01:24:14,710
black most or we're just a bunch of blacks we never repeat erez in around

1364
01:24:14,720 --> 01:24:17,640
the root leaves are black we used

1365
01:24:17,650 --> 01:24:19,210
much by definition

1366
01:24:19,220 --> 01:24:22,150
every node is red black that's easy

1367
01:24:23,390 --> 01:24:27,000
this is a particular set of properties that may seem a bit arbitrary this point

1368
01:24:27,000 --> 01:24:29,660
they'll make sense a lot more sense as we see

1369
01:24:29,670 --> 01:24:33,540
what consequences they have but there are a couple of goals

1370
01:24:33,590 --> 01:24:37,640
they were trying to achieve here one is that these properties should force the tree

1371
01:24:37,640 --> 01:24:39,310
to have logarithmic height

1372
01:24:39,330 --> 01:24:40,510
what i like and i

1373
01:24:40,530 --> 01:24:43,420
and they do although that's probably not obvious at this point

1374
01:24:43,430 --> 01:24:48,920
it follows mainly from this press from property from all the properties three and four

1375
01:24:48,920 --> 01:24:53,230
are the main ones but it's pretty much all of

1376
01:24:53,280 --> 01:24:55,500
the other thing the other

1377
01:24:55,900 --> 01:25:00,260
desire we have from these properties is that there are some easy to maintain

1378
01:25:00,350 --> 01:25:04,780
i can create a tree in the beginning there has this property for example

1379
01:25:04,800 --> 01:25:06,780
i could make

1380
01:25:06,960 --> 01:25:09,310
have to be a little bit careful but i can make

1381
01:25:09,330 --> 01:25:12,160
certainly if i take a perfectly balanced

1382
01:25:12,170 --> 01:25:13,760
binary tree

1383
01:25:13,810 --> 01:25:15,960
and make all the nodes black

1384
01:25:16,010 --> 01:25:18,860
it will satisfy those properties

1385
01:25:18,880 --> 01:25:22,270
this is a red black tree

1386
01:25:22,440 --> 01:25:29,120
this is not too hard to make these properties hold just from the beginning

1387
01:25:29,140 --> 01:25:31,060
the tricky part is to maintain

1388
01:25:31,070 --> 01:25:34,920
when i insert nodes to the tree and we from history i want to get

1389
01:25:34,960 --> 01:25:37,850
i want to make it not too hard on log n time i got to

1390
01:25:37,850 --> 01:25:40,720
well done when you very

1391
01:25:40,740 --> 01:25:42,230
with row

1392
01:25:42,440 --> 01:25:47,660
a you create a new role

1393
01:25:50,890 --> 01:25:53,150
three if you want to use

1394
01:25:53,270 --> 01:25:54,900
with you

1395
01:25:54,900 --> 01:25:58,580
that for the

1396
01:26:00,170 --> 01:26:03,420
so the

1397
01:26:03,440 --> 01:26:08,940
this may

1398
01:26:08,960 --> 01:26:11,900
the new in

1399
01:26:11,900 --> 01:26:13,440
in the

1400
01:26:16,740 --> 01:26:19,660
maybe three million

1401
01:26:23,310 --> 01:26:31,710
if we see what he meant he is not

1402
01:26:31,720 --> 01:26:32,670
what i mean

1403
01:26:33,210 --> 01:26:37,420
only for the very first order

1404
01:26:45,390 --> 01:26:47,660
perhaps there

1405
01:26:47,690 --> 01:26:50,460
general assembly

1406
01:26:53,130 --> 01:26:55,450
how however found in the

1407
01:26:56,340 --> 01:26:57,150
how old

1408
01:27:07,230 --> 01:27:10,150
so here we are

1409
01:27:20,860 --> 01:27:24,230
and by

1410
01:27:24,730 --> 01:27:35,150
i mean very

1411
01:27:35,500 --> 01:27:38,400
it's all over

1412
01:27:38,420 --> 01:27:43,990
or are we

1413
01:27:44,140 --> 01:27:48,060
all of order

1414
01:27:48,370 --> 01:27:52,060
four weeks

1415
01:27:52,060 --> 01:27:55,550
what if

1416
01:27:55,570 --> 01:28:00,960
which we are one and

1417
01:28:00,980 --> 01:28:01,760
all right

1418
01:28:01,770 --> 01:28:04,450
we want more

1419
01:28:04,460 --> 01:28:07,520
women were

1420
01:28:09,680 --> 01:28:13,660
but more than one more

1421
01:28:13,900 --> 01:28:20,120
if i may need to be

1422
01:28:26,640 --> 01:28:33,890
the reason for doing that

1423
01:28:33,910 --> 01:28:35,870
so are we

1424
01:28:36,320 --> 01:28:37,750
and the old

1425
01:28:43,130 --> 01:28:45,340
well with within the right

1426
01:28:45,390 --> 01:28:51,860
but they

1427
01:29:02,740 --> 01:29:06,070
and for that will be

1428
01:29:06,080 --> 01:29:10,860
all of

1429
01:29:11,390 --> 01:29:15,470
it not known

1430
01:29:23,140 --> 01:29:25,160
the government

1431
01:29:27,330 --> 01:29:29,470
so where are

1432
01:29:29,670 --> 01:29:31,440
o thing here

1433
01:29:31,490 --> 01:29:33,380
and for role

1434
01:29:33,390 --> 01:29:36,340
and for all

1435
01:29:37,160 --> 01:29:45,220
so i wish that you get the answer

1436
01:29:45,250 --> 01:29:47,860
four or

1437
01:29:47,870 --> 01:29:50,430
the total number

1438
01:29:52,000 --> 01:29:55,580
it is important that we

1439
01:29:55,610 --> 01:30:00,180
and from one hundred and two hundred eighty one o

1440
01:30:01,680 --> 01:30:06,770
they were all humans on all

1441
01:30:14,200 --> 01:30:16,550
so that's what we have

1442
01:30:22,480 --> 01:30:24,790
from the

1443
01:30:24,830 --> 01:30:30,250
of course because i want you

1444
01:30:30,340 --> 01:30:35,690
i would like

1445
01:30:35,720 --> 01:30:42,140
we're all one hundred by one

1446
01:30:46,050 --> 01:30:51,850
through what you

1447
01:30:54,540 --> 01:30:58,420
what are you

1448
01:30:58,490 --> 01:31:00,010
well want

1449
01:31:00,010 --> 01:31:05,830
so here i just written down the likelihood function explicitly integrating out the latent variables

1450
01:31:05,830 --> 01:31:10,920
solely variables don't appear here yet so i can't really look at the sun and

1451
01:31:10,930 --> 01:31:15,270
realize how to do yet but this is you an important piece of situation which

1452
01:31:15,270 --> 01:31:16,190
is that

1453
01:31:16,210 --> 01:31:20,100
the here the likelihood distribution is actually

1454
01:31:20,130 --> 01:31:25,960
two the only church in which they appear here is in this as which is

1455
01:31:25,970 --> 01:31:29,380
the sample covariance of the data

1456
01:31:29,400 --> 01:31:36,610
i can see another factor analysis y unconstrained gaussians fitting has some sufficient statistics and

1457
01:31:36,610 --> 01:31:41,140
the sufficient statistics are just the mean which are removed and the covariance of the

1458
01:31:41,140 --> 01:31:42,600
data which appears here

1459
01:31:42,610 --> 01:31:48,150
an introduction to factor analysis is that you're trying to make the covariance which is

1460
01:31:48,150 --> 01:31:53,130
as big close to the model covariance which is the except that the model covariance

1461
01:31:53,130 --> 01:31:57,530
is kind of constraint so we can make these things exactly the same and close

1462
01:31:58,340 --> 01:32:00,500
he is the trace of the ratio

1463
01:32:00,530 --> 01:32:02,950
the trace of the of the matrix ratio

1464
01:32:03,690 --> 01:32:07,420
so that's really what you're trying to do in fact announces get

1465
01:32:07,430 --> 01:32:12,460
now the covariance to be close to the observed data covering

1466
01:32:12,480 --> 01:32:15,650
so here is the actual ECM algorithm

1467
01:32:15,680 --> 01:32:20,450
and it's exactly the same as the EM algorithm for mixture models except now the

1468
01:32:20,450 --> 01:32:23,010
latent variables are continuous not discrete

1469
01:32:23,030 --> 01:32:24,560
so the same thing e

1470
01:32:24,580 --> 01:32:29,780
we need to compute the distribution over the latent variables given the data and the

1471
01:32:29,780 --> 01:32:31,130
current parameters

1472
01:32:31,150 --> 01:32:35,470
and in this case the that distribution rather than just being a multinomial or we

1473
01:32:35,470 --> 01:32:39,930
say what's the probability of the latent variables being equal to one two three up

1474
01:32:39,930 --> 01:32:43,390
to k it follows distribution

1475
01:32:43,400 --> 01:32:46,210
so some gaussian distribution here and in the end

1476
01:32:46,320 --> 01:32:52,820
for exactly the same you just maximize the expected complete log likelihood where the expectation

1477
01:32:52,820 --> 01:32:53,940
is taken under

1478
01:32:53,950 --> 01:32:55,390
this continuous

1479
01:32:56,600 --> 01:33:06,730
so i want to make a point about inference in factor analysis remember inference looked

1480
01:33:06,860 --> 01:33:12,510
it's in mixture models you took the probability of the data under the particular component

1481
01:33:12,520 --> 01:33:13,900
you're interested in

1482
01:33:13,930 --> 01:33:18,720
and you divided by the probability of the data summed over all components

1483
01:33:18,760 --> 01:33:20,300
it was just that ratio

1484
01:33:20,320 --> 01:33:24,420
what's the probability of the model k divided by what's the total probability and rome

1485
01:33:24,430 --> 01:33:27,030
in fact the analysis

1486
01:33:27,040 --> 01:33:32,220
inference in principle requires you to take the ratio

1487
01:33:32,230 --> 01:33:37,560
the ratio between the probability of generating the data if the latent variable

1488
01:33:37,650 --> 01:33:41,510
the continuously variable had a particular setting x divided by

1489
01:33:41,560 --> 01:33:46,650
the total probability of generating the data locally that total probability

1490
01:33:46,710 --> 01:33:50,520
is an integral which we can do analytically closed

1491
01:33:50,530 --> 01:33:55,930
so you do not integral analytically and the result is the inference in factor analysis

1492
01:33:55,930 --> 01:33:57,000
is actually

1493
01:33:58,640 --> 01:34:02,530
the distribution of the variable x given the data y

1494
01:34:02,570 --> 01:34:06,680
is a galaxy in with some mean and in some covariance b

1495
01:34:06,720 --> 01:34:08,470
it turns out that new

1496
01:34:08,480 --> 01:34:13,430
and it's just a linear function of the data you just take the data and

1497
01:34:13,430 --> 01:34:15,030
you subtract the mean

1498
01:34:15,050 --> 01:34:19,830
and after you subtract the mean is multiplied by this magic matrix here

1499
01:34:19,850 --> 01:34:25,710
this matrix is just the inference matrix so inference in factor analysis which might be

1500
01:34:25,710 --> 01:34:30,110
might be kind of complicated operation because you have to sum over an infinite number

1501
01:34:30,110 --> 01:34:34,440
of possible with variable settings that could have generated the data actually because you can

1502
01:34:34,440 --> 01:34:38,040
do that in general closed form reduces to linear operation

1503
01:34:38,060 --> 01:34:39,750
so what

1504
01:34:39,750 --> 01:34:41,310
the covariance

1505
01:34:42,170 --> 01:34:46,470
of the latent variables is independent of the data

1506
01:34:46,490 --> 01:34:49,920
it is equal to this expression

1507
01:34:49,940 --> 01:34:54,450
that's very counterintuitive thing about the analysis as q

1508
01:34:54,520 --> 01:34:58,370
here's the point here in factor analysis

1509
01:34:58,390 --> 01:35:03,110
what do you think the latent variables were set when i generated the data

1510
01:35:03,120 --> 01:35:06,700
you can give me an answer which is in the form of gaussians distribution the

1511
01:35:06,700 --> 01:35:10,290
mean and covariance that means depends on the data here

1512
01:35:10,310 --> 01:35:12,310
the covariance doesn't

1513
01:35:12,320 --> 01:35:17,770
which means your uncertainty about the latent variables does not depend on the data files

1514
01:35:17,780 --> 01:35:20,720
that's really different than in mixture models

1515
01:35:20,730 --> 01:35:23,000
in mixture models if i give you the point

1516
01:35:23,010 --> 01:35:26,280
that's right on top of one of the gaussians and miles away from all the

1517
01:35:26,280 --> 01:35:29,320
other gaussians you're pretty sure from adults

1518
01:35:29,350 --> 01:35:33,010
and that's reflected in the entropy of the posterior as a huge spike on one

1519
01:35:33,010 --> 01:35:37,250
model almost zero progress in factor analysis

1520
01:35:37,250 --> 01:35:41,870
equally unsure about the latent variables are very much i'm sure

1521
01:35:41,890 --> 01:35:43,500
no matter what the data once

1522
01:35:43,500 --> 01:35:47,700
that's the kind of strange property factor analysis that should set alarm bells in your

1523
01:35:47,700 --> 01:35:51,020
head that this is a very very simple model

1524
01:35:51,030 --> 01:36:00,830
so this is the sort of final algorithm again i'm not hoping you'll be able

1525
01:36:00,830 --> 01:36:05,600
to understand these equations right now lecture i just provide into there so you can

1526
01:36:05,610 --> 01:36:09,360
look them up later and that this is derived in exactly the same way as

1527
01:36:09,360 --> 01:36:12,550
we derive the previous algorithms

1528
01:36:15,640 --> 01:36:20,330
factor analysis is closely related to another model which many of you may have heard

1529
01:36:21,070 --> 01:36:25,690
which is the principal components analysis or PCA or for those of you who come

1530
01:36:25,690 --> 01:36:30,540
from or engineering background the car in the left transform KLT

1531
01:36:30,560 --> 01:36:31,860
the same

1532
01:36:31,870 --> 01:36:32,580
the idea

1533
01:36:32,600 --> 01:36:38,990
right so in factor analysis of what we do is we assume that the

1534
01:36:38,990 --> 01:36:45,460
i covariance matrix we're doing constraint as model and the covariance is the product of

1535
01:36:45,460 --> 01:36:49,320
some long and skinny matrices plus some diagonal

1536
01:36:49,350 --> 01:36:52,620
and of course we said we we had to constrain this to be diagonal otherwise

1537
01:36:52,620 --> 01:36:55,900
we would have the same problem with number of frames that we have original

1538
01:36:55,900 --> 01:36:59,800
twenty minutes that's

1539
01:36:59,900 --> 01:37:02,630
OK good

1540
01:37:02,650 --> 01:37:03,440
OK so

1541
01:37:03,460 --> 01:37:08,980
let's consider again the barrier optimisation so let's choose very optimisation technique is a the

1542
01:37:08,980 --> 01:37:12,000
method of choice to optimise this method

1543
01:37:12,150 --> 01:37:17,960
to optimise the optimisation problem OK so this was the ideas so we define this

1544
01:37:17,960 --> 01:37:22,650
barrier function with this exponential area gives you

1545
01:37:22,670 --> 01:37:27,610
OK so i'm going to change the new LP problem so that one with new

1546
01:37:27,670 --> 01:37:31,960
slightly so this is equivalent formulation is to be easier to drive OK

1547
01:37:32,070 --> 01:37:35,570
so instead of fixing

1548
01:37:36,070 --> 01:37:39,340
optimizing role fixed o to one

1549
01:37:39,420 --> 01:37:43,000
and and then optimize adolphus and we don't fix of two of the one norm

1550
01:37:43,000 --> 01:37:47,610
up to one case is just equivalent formulation

1551
01:37:47,630 --> 01:37:51,750
OK so how can we derive very optimisation objective

1552
01:37:51,750 --> 01:37:57,050
so this is the optimisation now we just follow the the normal strategy for deriving

1553
01:37:57,050 --> 01:38:01,300
barrier function this is just taking the objective you

1554
01:38:01,340 --> 01:38:07,150
writing it here and then for every constraint this one here what this one here

1555
01:38:07,150 --> 01:38:11,670
for instance we get we get an additional term and the very up upset optimisation

1556
01:38:12,710 --> 01:38:14,030
and also this one

1557
01:38:14,050 --> 01:38:15,670
this one here leads to

1558
01:38:16,050 --> 01:38:17,500
this term here

1559
01:38:17,780 --> 01:38:22,380
the other great because you're making some people decided constraints

1560
01:38:22,380 --> 01:38:25,210
so we have this optimisation object

1561
01:38:25,250 --> 01:38:27,780
this depends on beta alpha psi

1562
01:38:28,730 --> 01:38:36,880
so these are the variables of this optimisation problem often cited beta this very primitive

1563
01:38:37,030 --> 01:38:41,530
OK so i reverted here so i just read it again so now we can

1564
01:38:41,530 --> 01:38:47,000
actually determine optimal exciting for us so we can just set the the derivatives with

1565
01:38:47,000 --> 01:38:50,690
respect to size zero so

1566
01:38:50,690 --> 01:38:52,550
this is the solution

1567
01:38:53,050 --> 01:38:57,000
clock this back into the optimisation objective and then we get this

1568
01:38:57,170 --> 01:39:01,070
here the structure

1569
01:39:01,250 --> 01:39:03,630
OK when you have a look at this

1570
01:39:03,880 --> 01:39:07,550
maybe this reminds you of logistic loss

1571
01:39:07,570 --> 01:39:08,960
so this

1572
01:39:08,980 --> 01:39:11,420
very similar to logistic loss

1573
01:39:11,570 --> 01:39:19,860
OK so we have this up par optimisation function and this is the general form

1574
01:39:20,250 --> 01:39:21,730
which we have considered for

1575
01:39:25,610 --> 01:39:30,710
according wise descent method for so we just had to optimise this kind of function

1576
01:39:30,710 --> 01:39:34,980
this had to form in the in the court descent and then we had some

1577
01:39:34,980 --> 01:39:40,570
additional time here so we can just use this leveraging scheme to solve this optimisation

1578
01:39:40,570 --> 01:39:44,320
problem or to just to solve this optimisation problem so once we have to solve

1579
01:39:44,320 --> 01:39:48,090
this optimisation problem for fixed beta can decrease peter

1580
01:39:48,110 --> 01:39:50,250
that's all continue optimizing

1581
01:39:50,300 --> 01:39:52,070
and this leads essentially

1582
01:39:52,550 --> 01:39:58,610
to this on some details which i great out so just details to make appropriate

1583
01:39:58,730 --> 01:40:01,170
but it's not important but i already

1584
01:40:01,320 --> 01:40:06,360
so essentially just train a weak learner you find the output hypothesis coefficient just by

1585
01:40:06,360 --> 01:40:12,730
minimising some loss function you update your your combined function debates in a little more

1586
01:40:12,730 --> 01:40:14,360
complicated manner

1587
01:40:14,530 --> 01:40:19,050
and then under certain conditions you decrease the speed of parameter you let it go

1588
01:40:19,050 --> 01:40:26,360
to zero and then you try to follow this then you can show the convergence

1589
01:40:26,480 --> 01:40:29,190
so this algorithm

1590
01:40:29,300 --> 01:40:38,070
minimizes this optimizes this linear programme with this with the soft margin

1591
01:40:38,090 --> 01:40:41,960
you can do the same for regression so you just have a different optimisation problem

1592
01:40:41,960 --> 01:40:48,050
so you here for instance that you try to find a function that is close

1593
01:40:48,250 --> 01:40:53,840
to the labels so it's in some excellent you just different optimisation problem we can

1594
01:40:53,840 --> 01:40:57,900
use the same technique so just looks the same

1595
01:40:59,980 --> 01:41:01,650
other questions for that

1596
01:41:01,800 --> 01:41:07,960
so this is a very i mean a general strategy of how to go from

1597
01:41:07,960 --> 01:41:13,500
an idea of the problem or of of improvement to an article with proofs of

1598
01:41:15,440 --> 01:41:21,480
that is what has been done several times with different versions of articles for regression

1599
01:41:21,520 --> 01:41:25,380
one class four four soft margins and so on

1600
01:41:25,400 --> 01:41:32,960
is a principle to very of correct driving on

1601
01:41:33,020 --> 01:41:35,030
no questions

1602
01:41:35,590 --> 01:41:37,300
good everything else

1603
01:41:41,650 --> 01:41:44,960
i have i guess i'm going to close early so

1604
01:41:46,670 --> 01:41:51,150
there are some applications of boosting i would like to mention but just briefly

1605
01:41:51,170 --> 01:41:56,920
so here's some practical issues so generally so this this posting is

1606
01:41:57,320 --> 01:42:02,840
like meta algorithm that can use any reasonable can so when whenever i have something

1607
01:42:02,840 --> 01:42:07,690
which could be a problem then you can use boosting to somehow prove it so

1608
01:42:07,730 --> 01:42:12,050
and essentially it has the original adaboost has only a single parameter see number of

1609
01:42:12,050 --> 01:42:14,090
iterations which you need to

1610
01:42:14,710 --> 01:42:18,480
so and it's a kind of very fast and easy

1611
01:42:18,480 --> 01:42:21,670
program i mean it can be very easy to implement

1612
01:42:21,670 --> 01:42:28,830
better evaluation than just sort of this statistics and look how similar they are and of course

1613
01:42:28,830 --> 01:42:35,290
if I would be taking larger matrix I would get a better fit but

1614
01:42:35,290 --> 01:42:42,030
but for details for that I have them in the paper here again I'll just show you briefly I

1615
01:42:42,040 --> 01:42:47,850
have a larger graph it's a social network of opinions that has seventy-six thousand nodes and half

1616
01:42:47,850 --> 01:42:53,630
a million edges again we are searching over the space of size ten to the million we

1617
01:42:53,630 --> 01:43:00,090
can do this in two hours and these are some properties just to show that we are getting something

1618
01:43:00,090 --> 01:43:05,850
that sort of matches the structure and again and it scales linearly so let me just conclude and

1619
01:43:05,850 --> 01:43:10,150
will be done so what I showed at least for the so for the first

1620
01:43:10,150 --> 01:43:17,480
first part I'll go here so what we saw is that we saw first we we went through

1621
01:43:17,490 --> 01:43:21,870
the statistical properties of the networks from various domains and this is basically the

1622
01:43:21,880 --> 01:43:27,690
key to understand how do this I know independent behavior of people or the nodes bring

1623
01:43:27,690 --> 01:43:32,060
this nice structure out of it right so what is nice with this is that you have

1624
01:43:32,060 --> 01:43:36,350
this big social network my actions are sort of are independent of the actions

1625
01:43:36,350 --> 01:43:40,630
of someone who's in the I know in states or in China or in somewhere far away right but as

1626
01:43:40,630 --> 01:43:46,110
as you compute some kind of aggregate statistics nice patterns start of emerge so this is

1627
01:43:46,110 --> 01:43:50,990
why this is interesting so we have this sort of independent random players doing something

1628
01:43:50,990 --> 01:43:55,970
but when you look at enough of them you get you patterns start of emerge so

1629
01:43:55,970 --> 01:44:00,990
this was the first part the second part was about the the models of

1630
01:44:00,990 --> 01:44:05,350
structure and growth right so the reason why I show them was that this would give

1631
01:44:05,350 --> 01:44:10,550
us intuitions and help us explain why do these properties emerge right how can

1632
01:44:10,550 --> 01:44:16,280
my local behavior lead to to some global behavior and and the last

1633
01:44:16,280 --> 01:44:20,390
the last part the fitting part hat I skimmed briefly was to about

1634
01:44:20,410 --> 01:44:26,330
understanding the structure and prediction so why would you care about these things why why

1635
01:44:26,330 --> 01:44:31,810
do we need them as I presented them right so here are a few possible applications right first it gives

1636
01:44:31,810 --> 01:44:35,190
us insight into why these things are going wrong but then you can use this

1637
01:44:35,230 --> 01:44:38,810
for anomaly detection for example if you want to say is this graph

1638
01:44:38,810 --> 01:44:42,570
normal graph is this something that I would expect or is it strange so you need a model you need to

1639
01:44:42,570 --> 01:44:48,310
understand what is normal for when when we need fitting we saw that using this now

1640
01:44:48,310 --> 01:44:52,830
we can generate larger or smaller graphs so we can use these for predictions for

1641
01:44:52,830 --> 01:44:57,300
example many times you you have your new I know your routing algorithm or something and you'd

1642
01:44:57,310 --> 01:45:01,970
like to test it on some synthetic graph and it would be nice if this synthetic graph has

1643
01:45:01,970 --> 01:45:05,840
the same properties as real graphs have that's is this is also the reason why we

1644
01:45:05,850 --> 01:45:10,980
why you want models and why you want to fit these models similar reverse from

1645
01:45:10,980 --> 01:45:14,970
predictions is like graph sampling right the idea would be you have a big graph it's

1646
01:45:14,970 --> 01:45:18,990
too big for you to handle so now you would like to have create some smaller graphs run your

1647
01:45:19,430 --> 01:45:24,930
algorithm on it but then sort of make claims how your algorithm on a smaller graph would run on a bigger graph so

1648
01:45:24,930 --> 01:45:28,400
we want to sort of shrink your graph but maintain the properties and

1649
01:45:28,400 --> 01:45:32,070
one way to do this would be through this sampling scenario right you

1650
01:45:32,070 --> 01:45:35,800
would fit the model and now given the model you can generate bigger or smaller graphs and

1651
01:45:35,800 --> 01:45:40,630
run your algorithms over them and

1652
01:45:40,970 --> 01:45:46,260
I think this is my last slide so here I just have some questions that

1653
01:45:46,270 --> 01:45:50,090
I think will come in the future right so one question is how do we

1654
01:45:50,090 --> 01:45:57,050
systematically characterize the network structure right given a graph how do

1655
01:45:57,050 --> 01:46:01,350
we describe it so that all of us sort of agree this is this is this kind

1656
01:46:01,350 --> 01:46:05,050
of graph with this particular structures so what are the important properties we need to

1657
01:46:05,050 --> 01:46:08,670
measure we don't know whether the ones we have today are the ones that we really

1658
01:46:08,680 --> 01:46:12,210
care about or is there something that we are missing or we haven't yet thought

1659
01:46:12,210 --> 01:46:19,030
of another question is how do these properties relate to one another how they are interconnected what are

1660
01:46:19,250 --> 01:46:23,530
what are the ones we would like for example if you would like to

1661
01:46:23,530 --> 01:46:29,550
design a system so this should be our next one right so the question is how how could we design systems

1662
01:46:29,560 --> 01:46:33,430
like systems or social network of websites and so on how how could

1663
01:46:33,440 --> 01:46:38,590
we design them to to for example be robust to failure so to support local search

1664
01:46:38,590 --> 01:46:42,340
for example in case of peer to peer networks or if we want them to

1665
01:46:42,450 --> 01:46:49,690
evolve how how how would we stimulate I know our social networking books website to sort of evolve in a

1666
01:46:49,890 --> 01:46:55,990
organic way so that the network doesn't fall apart so this this is one way right so

1667
01:46:55,990 --> 01:46:59,050
if you look at the network I know as your little plant so how should

1668
01:46:59,140 --> 01:47:04,730
you be taking care of your little plant that it really grows into something nice how do you

1669
01:47:04,730 --> 01:47:10,010
predict the structure of the network right you'd like to do this to be able to

1670
01:47:10,010 --> 01:47:14,150
say what will be going on with the future to be able to say how I know the

1671
01:47:14,150 --> 01:47:19,370
Internet will change how do we need to scale our algorithms protocols and things like that

1672
01:47:19,370 --> 01:47:23,560
and I think the most basic of them all is why are the networks the way

1673
01:47:23,560 --> 01:47:30,850
they are right how how did this independent or non-correlated behavior of people

1674
01:47:30,850 --> 01:47:36,510
gave us gave rise to these patterns that we see when we are looking at a large scale properties

1675
01:47:36,510 --> 01:47:42,110
we eliminated all the actors below that short and for these actors

1676
01:47:42,110 --> 01:47:45,700
we networks in the following two cases

1677
01:47:45,710 --> 01:47:46,990
that is

1678
01:47:47,000 --> 01:47:48,320
in the triplet

1679
01:47:48,330 --> 01:47:53,410
if it contains a key subject key action and an object or if it contains

1680
01:47:53,410 --> 01:47:55,480
a subject with the key actions

1681
01:47:55,490 --> 01:47:59,610
but the key object we feel that this and then we create networks out of

1682
01:48:00,470 --> 01:48:07,110
we also try to create works out of frequent triplets but they were not really

1683
01:48:07,110 --> 01:48:14,590
revealing interesting relations because the frequent triplets did not have interesting actions they were like

1684
01:48:15,540 --> 01:48:21,900
policing something to somebody or reporters reporting something so we thought this method is much

1685
01:48:22,740 --> 01:48:31,560
interesting and reveals more information so we generated directed networks using cytoscape which is denoted

1686
01:48:31,560 --> 01:48:36,400
platform for network analysis and the networks had

1687
01:48:36,410 --> 01:48:42,440
subjects and objects as nodes and the verbs linking them as edges

1688
01:48:42,460 --> 01:48:47,320
so this is a simple network in two thousand two on the left

1689
01:48:47,340 --> 01:48:52,130
it shows the whole network and i have highlighted the north trees

1690
01:48:52,150 --> 01:48:56,240
and the interaction of this north people other nodes in the network and it was

1691
01:48:56,240 --> 01:49:02,070
like you could see the priest was involved in abusing and listing all the time

1692
01:49:02,110 --> 01:49:07,290
and you could especially it's really clear that you can see the actors like

1693
01:49:07,320 --> 01:49:10,340
mike cannot more is saying

1694
01:49:11,650 --> 01:49:16,760
blackwell james support on all these people were

1695
01:49:16,780 --> 01:49:18,570
victims of those crimes

1696
01:49:18,590 --> 01:49:20,630
and the girls and boys

1697
01:49:20,640 --> 01:49:23,640
or the victim so this shows

1698
01:49:24,320 --> 01:49:28,730
this is from this that you could see you could also see the direction

1699
01:49:28,740 --> 01:49:30,870
we do that all this well

1700
01:49:32,590 --> 01:49:37,350
if you look at the network of prosecutors you could see there were always in

1701
01:49:37,350 --> 01:49:39,370
warding investigating

1702
01:49:40,270 --> 01:49:46,380
characterizing believing answering of conducting so you could see at this in more in what

1703
01:49:46,380 --> 01:49:53,060
kind of what type of actions so the networks itself provides information for us

1704
01:49:53,070 --> 01:49:55,610
this is another network showing

1705
01:49:55,620 --> 01:49:57,870
the interactions between judge

1706
01:49:57,880 --> 01:50:02,730
and the jury and then the actors in the network so

1707
01:50:02,740 --> 01:50:05,490
with this network we could identify

1708
01:50:05,510 --> 01:50:10,650
this type of actors in more than this these kind of actions this is really

1709
01:50:10,650 --> 01:50:16,840
important in the field of quantitative narrative analysis because currently all these are done manually

1710
01:50:16,890 --> 01:50:20,600
and it takes a lot of that

1711
01:50:20,630 --> 01:50:26,410
a similar kind of work has been done in the humanities recently by

1712
01:50:26,440 --> 01:50:29,920
frank from the stanford lead to the left

1713
01:50:29,940 --> 01:50:34,110
so they have converted no always into to

1714
01:50:34,110 --> 01:50:40,000
graphs like this and then they have identified the centrality of actors but again

1715
01:50:40,020 --> 01:50:43,670
they do it by hand so here it is automated

1716
01:50:45,540 --> 01:50:51,580
once the networks that then we could analyse the topological properties of the net so

1717
01:50:51,610 --> 01:50:55,470
we thought of analyzing the centrality of networks using

1718
01:50:55,480 --> 01:51:01,350
measures like of betweenness centrality and then the indegree and outdegree and the page rank

1719
01:51:01,490 --> 01:51:06,500
and the hits we also apply the HITS algorithm which is by jon kleinberg which

1720
01:51:06,500 --> 01:51:11,220
talks about hubs and authorities and we apply to all these two all the networks

1721
01:51:11,220 --> 01:51:13,060
and then ran

1722
01:51:13,070 --> 01:51:18,470
active according to these values you could see on the right it was the the

1723
01:51:19,690 --> 01:51:27,640
the tracks actors according to these four betweenness centrality indegree and outdegree and here again

1724
01:51:27,660 --> 01:51:32,900
you could see the actors the first line shows something called long

1725
01:51:32,920 --> 01:51:35,320
this is quite an b this here because

1726
01:51:35,960 --> 01:51:40,790
the person who was involved in the sexual crime at low

1727
01:51:41,790 --> 01:51:46,810
that could also mean that and then archdiocese and again the same kind of factors

1728
01:51:46,810 --> 01:51:52,880
appear there the in degree was again showing something like in this case

1729
01:51:52,890 --> 01:51:55,610
allegations boys and things like that

1730
01:51:55,620 --> 01:52:01,610
and the degree was fairly high for the priests and judges again

1731
01:52:01,610 --> 01:52:04,040
the same thing would be page five

1732
01:52:04,660 --> 01:52:06,600
cobb and authority

1733
01:52:06,610 --> 01:52:10,990
so all these measures were done using cytoscape

1734
01:52:11,010 --> 01:52:15,490
and if he which is also in network analysis tool which is an open source

1735
01:52:15,490 --> 01:52:18,780
tool as well

1736
01:52:18,810 --> 01:52:20,360
and then

1737
01:52:20,560 --> 01:52:21,590
know this

1738
01:52:21,600 --> 01:52:25,560
actors according to this and this is shown in the table so these are the

1739
01:52:25,560 --> 01:52:28,720
top ten most central actors in two thousand two

1740
01:52:28,740 --> 01:52:32,740
according to on the punishers

1741
01:52:32,830 --> 01:52:40,920
and the next we've also interested in identifying spheres of interaction because it is also

1742
01:52:40,920 --> 01:52:44,720
common to investigate this in q and a

1743
01:52:44,720 --> 01:52:50,530
the spheres of interaction between this means for example communication

1744
01:52:50,550 --> 01:52:54,630
all i could be relation could be violent crime except run so what we did

1745
01:52:54,630 --> 01:52:58,850
is we classify actions into different types

1746
01:52:58,860 --> 01:53:02,370
and then we analyse the triplets according to that so

1747
01:53:02,400 --> 01:53:07,120
we classified them according to these two categories

1748
01:53:07,470 --> 01:53:11,160
in two thousand two which is a crime against person

1749
01:53:11,180 --> 01:53:12,890
and crime against property

1750
01:53:12,910 --> 01:53:15,870
for example actions like

1751
01:53:15,870 --> 01:53:19,660
here's a murder mostly it would appear in

1752
01:53:19,690 --> 01:53:21,370
crime against person

1753
01:53:21,370 --> 01:53:26,360
and crime against property it would be like slaughtered or

1754
01:53:26,370 --> 01:53:28,600
t or steel properties so

1755
01:53:28,600 --> 01:53:31,590
in two thousand two the most of the network was

1756
01:53:31,600 --> 01:53:37,220
the game was was about crime against person a little bit about crime against property

1757
01:53:37,270 --> 01:53:39,370
so once we have this

1758
01:53:39,400 --> 01:53:44,270
we identify what the frequent subjects and objects in the world in this

1759
01:53:44,280 --> 01:53:50,300
so this shows the top ten ranked subjects and objects in crime against person and

1760
01:53:50,320 --> 01:53:55,190
crime against property so all this is done with only the key actors and actions

1761
01:53:55,190 --> 01:53:58,050
that we have identified so

1762
01:53:58,070 --> 01:54:02,400
here you could say that dog highest-ranked subject

1763
01:54:02,430 --> 01:54:06,600
in crime against person was placed and then men

1764
01:54:06,610 --> 01:54:12,200
and then the objects which i'll go and so it is interesting to see here

1765
01:54:12,200 --> 01:54:18,460
that men responsible for crimes and while women and children are most often the objects

1766
01:54:18,460 --> 01:54:24,690
of crimes and protect the crime against property again and police and soldiers with the

1767
01:54:24,700 --> 01:54:29,460
subjects and the most frequent object was money and back

1768
01:54:30,080 --> 01:54:36,780
we also performed in time series analysis on actors

1769
01:54:37,260 --> 01:54:40,590
we thought it's interesting to to see the

1770
01:54:40,610 --> 01:54:45,640
the role of factors in a time series so we were analysing data from nineteen

1771
01:54:45,640 --> 01:54:49,880
eighty seven to two thousand seven in europe and so it's it's really twenty years

1772
01:54:51,350 --> 01:54:55,350
we did all the experiments for

1773
01:54:55,350 --> 01:54:58,420
all these twenty years and

1774
01:54:58,450 --> 01:55:04,110
for the time series analysis we considered the frequency the out degree

1775
01:55:04,160 --> 01:55:06,390
and they have value to plant

1776
01:55:06,400 --> 01:55:09,110
these at this so according to

1777
01:55:09,120 --> 01:55:13,600
the active archdiocese his frequency was really high in two thousand two

1778
01:55:14,910 --> 01:55:18,660
the out degree and hub also denotes the

1779
01:55:18,680 --> 01:55:23,600
his he was the key key actor in two thousand two and it's the same

1780
01:55:23,600 --> 01:55:25,900
for actions as well

1781
01:55:25,920 --> 01:55:30,180
you could see the first one is more lists which is really high in two

