1
00:00:00,000 --> 00:00:01,160
and if we

2
00:00:01,180 --> 00:00:06,250
o thing i want to make a procedure in which global

3
00:00:08,470 --> 00:00:10,060
they like

4
00:00:10,970 --> 00:00:13,450
that's like right

5
00:00:17,100 --> 00:00:21,640
i don't believe thresholding when the original

6
00:00:23,850 --> 00:00:25,790
so just to our

7
00:00:25,810 --> 00:00:28,910
which she and the police

8
00:00:28,930 --> 00:00:30,450
threshold when

9
00:00:30,470 --> 00:00:31,430
for every

10
00:00:32,330 --> 00:00:34,040
and actually think man

11
00:00:36,310 --> 00:00:37,830
independent from

12
00:00:38,930 --> 00:00:42,600
that's all think

13
00:00:42,620 --> 00:00:44,290
and that is because

14
00:00:44,310 --> 00:00:47,870
depends on the

15
00:00:49,830 --> 00:00:53,670
on the web of because that's the only way and

16
00:00:53,720 --> 00:00:56,100
eleven of this particular age

17
00:00:59,990 --> 00:01:02,470
i'm my just

18
00:01:02,560 --> 00:01:07,870
o thing but not as the global image but also for particular radio

19
00:01:07,890 --> 00:01:10,020
this segmentation

20
00:01:10,060 --> 00:01:11,540
it also

21
00:01:11,560 --> 00:01:13,220
as the act like

22
00:01:13,270 --> 00:01:14,430
but the

23
00:01:22,470 --> 00:01:25,990
and on my on

24
00:01:27,370 --> 00:01:32,250
so much so the that way know

25
00:01:32,270 --> 00:01:35,410
probability distribution of background

26
00:01:40,540 --> 00:01:44,790
these people are

27
00:01:44,830 --> 00:01:47,680
the distribution over

28
00:01:47,750 --> 00:01:52,540
i think and this is the probability distribution of

29
00:01:52,600 --> 00:01:53,350
thank you

30
00:01:56,580 --> 00:01:57,930
is that probabilities

31
00:01:57,950 --> 00:02:00,470
the big the long approach

32
00:02:00,580 --> 00:02:02,200
the pixel belongs

33
00:02:02,220 --> 00:02:04,700
four of the two

34
00:02:04,720 --> 00:02:06,270
peter big two

35
00:02:06,290 --> 00:02:08,100
the probability of the event

36
00:02:08,140 --> 00:02:09,560
because of the way

37
00:02:11,910 --> 00:02:17,250
so the distribution can be described this way in the first democratic just equal to

38
00:02:18,850 --> 00:02:24,930
because of its role in the world i there are a of

39
00:02:25,100 --> 00:02:27,790
dry what

40
00:02:27,810 --> 00:02:30,500
you have some threshold will see

41
00:02:30,520 --> 00:02:33,060
again the

42
00:02:33,100 --> 00:02:35,290
which the

43
00:02:36,560 --> 00:02:40,450
and can this error this area

44
00:02:40,450 --> 00:02:43,700
so output the pixel which actually

45
00:02:43,720 --> 00:02:45,890
has been the level of

46
00:02:46,660 --> 00:02:47,680
then he

47
00:02:47,700 --> 00:02:50,640
but we chat belongs to the

48
00:02:53,120 --> 00:02:54,620
so which at these

49
00:02:57,310 --> 00:02:59,660
these pixels of

50
00:03:04,560 --> 00:03:06,540
time by the

51
00:03:08,330 --> 00:03:10,270
and the thing

52
00:03:13,720 --> 00:03:16,040
you well because of best

53
00:03:16,080 --> 00:03:17,890
because the threshold

54
00:03:17,910 --> 00:03:22,200
but there there's something about that they belong to the

55
00:03:22,220 --> 00:03:25,120
four the architecture so they can be a

56
00:03:25,140 --> 00:03:28,080
i think the bank

57
00:03:28,100 --> 00:03:30,870
and when they belong to

58
00:03:30,890 --> 00:03:34,020
so here is the

59
00:03:38,810 --> 00:03:44,310
background pixels just signed by you take part in the brain

60
00:03:46,140 --> 00:03:49,520
and then

61
00:03:51,770 --> 00:03:55,370
part of

62
00:03:55,370 --> 00:03:59,470
nine there are some things we do

63
00:04:00,080 --> 00:04:05,160
assume that all been level much bigger than you make

64
00:04:05,250 --> 00:04:08,410
and think about

65
00:04:08,430 --> 00:04:12,250
the same

66
00:04:13,450 --> 00:04:14,080
on this

67
00:04:14,100 --> 00:04:17,220
well the parameters if have further

68
00:04:21,120 --> 00:04:22,020
what are you

69
00:04:22,140 --> 00:04:28,430
the royal authority they here it should be from people who live in thank

70
00:04:28,500 --> 00:04:33,500
so human rights this area

71
00:04:33,580 --> 00:04:35,680
i think you might have be

72
00:04:35,770 --> 00:04:37,430
so we have this

73
00:04:37,550 --> 00:04:38,930
dogs are

74
00:04:38,950 --> 00:04:42,680
and our goal is to minimize the error

75
00:04:42,700 --> 00:04:43,270
so i

76
00:04:43,290 --> 00:04:44,640
to minimize

77
00:04:44,640 --> 00:04:47,140
that involves the smallest number flip

78
00:04:47,200 --> 00:04:53,430
that's the correct universal answer to the problem and has an awful complexity scales is

79
00:04:53,430 --> 00:04:55,220
due to the k

80
00:04:55,240 --> 00:04:56,430
two before

81
00:04:56,430 --> 00:05:02,330
sixteen so we have to look at all sixteen hypothesis but there's actually a sneaky

82
00:05:02,330 --> 00:05:03,870
way of getting there

83
00:05:04,010 --> 00:05:05,640
more efficiently

84
00:05:05,700 --> 00:05:10,490
we can find the most probable answer by taking a single diagram

85
00:05:10,510 --> 00:05:11,640
and writing

86
00:05:11,740 --> 00:05:13,930
received one received two

87
00:05:13,950 --> 00:05:16,270
c three c four

88
00:05:16,270 --> 00:05:20,310
c five c six hundred fifty seven

89
00:05:20,330 --> 00:05:26,250
into this diagram and then seeing which circles are happy and which are sad

90
00:05:26,370 --> 00:05:29,540
here we go

91
00:05:29,620 --> 00:05:33,990
right in one one zero zero

92
00:05:37,660 --> 00:05:42,990
just to remind you there's a little secret that's the one we actually clicked but

93
00:05:42,990 --> 00:05:45,850
we don't know that any more

94
00:05:45,870 --> 00:05:49,870
and now we create a green and red and we say which circle the happy

95
00:05:49,870 --> 00:05:54,470
remember the rule is the number of ones in circle has to be even and

96
00:05:54,470 --> 00:05:55,890
circle number one

97
00:05:55,930 --> 00:05:57,770
so there is no no i'm unhappy

98
00:05:57,790 --> 00:05:59,270
which is red

99
00:05:59,290 --> 00:06:02,700
so the number two says i'm unhappy

100
00:06:02,770 --> 00:06:09,720
and second of three thousand one or two i'm happy

101
00:06:09,810 --> 00:06:16,540
what we're doing here is computing a thing called the syndrome is

102
00:06:16,680 --> 00:06:21,370
like talking to the doctor about the things you're feeling at the moment which onto

103
00:06:21,370 --> 00:06:25,330
description of the actual thing that's wrong with you that the signs of the underlying

104
00:06:25,330 --> 00:06:29,180
problem which is maybe some of these bits conflict with certain at least one of

105
00:06:29,180 --> 00:06:33,970
them got because all the circles should have been happy if no flip so definitely

106
00:06:33,970 --> 00:06:39,140
something that happened who was the the troublesome guy well the decoding rule

107
00:06:39,180 --> 00:06:45,270
the decoder says find a single bit lies inside all unhappy circles and outside all

108
00:06:45,290 --> 00:06:46,520
happy circles

109
00:06:46,580 --> 00:06:51,330
and that is this one because it's inside all the red and it's outside all

110
00:06:51,330 --> 00:06:52,060
the green

111
00:06:52,100 --> 00:06:55,790
and then use a conflict that one

112
00:06:55,810 --> 00:06:58,350
and fled the one bit

113
00:06:58,350 --> 00:06:59,600
it has that property

114
00:06:59,600 --> 00:07:01,520
so we do that

115
00:07:01,540 --> 00:07:03,910
and we get one zero

116
00:07:06,120 --> 00:07:08,410
so on this

117
00:07:08,540 --> 00:07:10,060
we solve the problem

118
00:07:10,060 --> 00:07:12,930
in this case we got the right answer

119
00:07:12,950 --> 00:07:18,510
and wonderful homework problem for you to enjoy is to confirm that if you

120
00:07:22,810 --> 00:07:24,270
single bit

121
00:07:27,010 --> 00:07:30,200
where there is one of the source bits in the middle all one of the

122
00:07:30,200 --> 00:07:31,750
parity bits

123
00:07:31,770 --> 00:07:34,680
this code can detect and correct

124
00:07:34,720 --> 00:07:36,470
this decoder

125
00:07:37,290 --> 00:07:39,180
detect clearer

126
00:07:48,060 --> 00:07:49,540
i think that's exciting

127
00:07:52,830 --> 00:07:55,680
what's the probability of error for this seven four

128
00:07:55,700 --> 00:08:00,270
hamming code well it's going to be in trouble in fact as you can confirm

129
00:08:00,270 --> 00:08:04,410
if more than one bit flip because then the decoder is going to detect and

130
00:08:04,410 --> 00:08:06,870
correct single error and you end up with three

131
00:08:08,790 --> 00:08:11,180
failure still occur you end up with s

132
00:08:11,200 --> 00:08:18,890
some believe as not equalling what was it was meant to be this happens

133
00:08:19,990 --> 00:08:21,450
there exists

134
00:08:23,120 --> 00:08:24,810
all or more

135
00:08:27,520 --> 00:08:33,660
so you end up with some errors occurring and another homework problem for use to

136
00:08:33,660 --> 00:08:36,750
work out exactly what the the probability is

137
00:08:36,790 --> 00:08:38,830
of a particular bit

138
00:08:38,870 --> 00:08:40,950
not including

139
00:08:40,950 --> 00:08:42,830
what it ought to be equal

140
00:08:42,850 --> 00:08:44,290
and what you find

141
00:08:44,350 --> 00:08:49,010
as before with the repetition code is since it messes up when there's two or

142
00:08:49,010 --> 00:08:53,250
more errors this is gonna end up scaling as x squared and the constant references

143
00:08:53,250 --> 00:08:55,470
something like nine

144
00:08:59,010 --> 00:09:01,470
that's the probability

145
00:09:03,830 --> 00:09:08,370
and this is very exciting why it exciting as exciting because

146
00:09:08,390 --> 00:09:10,600
it scaling is that square

147
00:09:10,620 --> 00:09:16,720
which is the same scaling there are three had

148
00:09:16,740 --> 00:09:20,600
but we've got a much bigger rate so generate for seven

149
00:09:20,620 --> 00:09:25,040
and their probabilities in the ballpark of s where happens to be nine x squared

150
00:09:25,040 --> 00:09:25,750
a little bit

151
00:09:25,810 --> 00:09:29,700
a little bit bigger three times bigger

152
00:09:29,700 --> 00:09:34,110
his switch to any open markets such as europe or asia

153
00:09:34,120 --> 00:09:38,680
and watch the little thing go up and down and just try to predict try

154
00:09:38,710 --> 00:09:41,700
to predict your brain when it's going to switch

155
00:09:41,720 --> 00:09:46,100
and this is a interesting exercise very interesting i've done this a lot in fact

156
00:09:46,100 --> 00:09:47,660
it's almost obsessive

157
00:09:48,870 --> 00:09:54,230
because it's really what it's really to me with the fluctuations all about that by

158
00:09:54,230 --> 00:10:00,160
stating the daily returns but i call them microtrends little migratory not then something like

159
00:10:00,180 --> 00:10:04,750
and you cannot predict anything you think you know it's true then it doesn't and

160
00:10:04,780 --> 00:10:08,780
why because it does turn up a little bit then that the same way so

161
00:10:08,780 --> 00:10:09,710
what we do

162
00:10:09,790 --> 00:10:14,430
is only one more picture come but have to build up to it

163
00:10:14,440 --> 00:10:19,110
we have to look for signs that look like critical for you see my dream

164
00:10:19,110 --> 00:10:20,930
is to see that there is some

165
00:10:21,040 --> 00:10:25,990
insights and economics based on what we all learn from spending phase transitions

166
00:10:26,010 --> 00:10:29,620
so we want to look for that and to do that we need data and

167
00:10:29,620 --> 00:10:31,160
the data

168
00:10:31,180 --> 00:10:33,280
that tobias price broad

169
00:10:33,290 --> 00:10:35,860
in addition to its brain which is the main thing

170
00:10:35,900 --> 00:10:40,150
was the database of fourteen billion trades

171
00:10:41,360 --> 00:10:45,910
in just one item that one item is the dark future contract

172
00:10:45,930 --> 00:10:50,420
and it's traded so much that in nine months there fourteen million trade was people

173
00:10:50,420 --> 00:10:51,810
literally sit there

174
00:10:51,840 --> 00:10:56,250
and trained this object it's hard to believe that they do do it

175
00:10:56,310 --> 00:11:00,890
betting on horses i can understand betting on this this thing i don't know what

176
00:11:00,890 --> 00:11:04,390
to do it but i was doing it because they think they can make money

177
00:11:04,630 --> 00:11:08,250
so what did they do p now means price which is also the name of

178
00:11:08,260 --> 00:11:13,000
the student price and of this tax futures

179
00:11:13,030 --> 00:11:14,790
you know

180
00:11:14,800 --> 00:11:18,720
this is just an american one second through the data and it goes down it

181
00:11:18,720 --> 00:11:19,600
goes up

182
00:11:19,610 --> 00:11:23,800
and the question is how we can analyse this and since the fluctuations on all

183
00:11:25,220 --> 00:11:29,340
the only thing you have to do first is set some scale i'd like this

184
00:11:29,350 --> 00:11:33,510
you have to say within this window the local maximum this red

185
00:11:33,560 --> 00:11:38,140
and this went the local maximum is redundant so forth and then you could change

186
00:11:38,140 --> 00:11:41,860
the scale later but right now just fix the scale and the growth of maximum

187
00:11:43,320 --> 00:11:46,780
that's which show you now in the real data is not schematic

188
00:11:46,790 --> 00:11:49,390
the time window seventy five takes

189
00:11:49,400 --> 00:11:54,300
and and you see that the maximum to allowed meaning

190
00:11:54,470 --> 00:11:59,520
is maximum rate is minimum and of course in between the local maxima and minima

191
00:11:59,520 --> 00:12:02,500
over there are fewer than seventy five six

192
00:12:02,550 --> 00:12:07,710
so everyone knows this the fluctuations the market can be is just words can be

193
00:12:07,710 --> 00:12:12,120
described as microtrends microtrends trend and down migratory

194
00:12:12,130 --> 00:12:15,240
so nothing new that nothing at all no

195
00:12:15,260 --> 00:12:16,550
now what's new

196
00:12:16,630 --> 00:12:18,450
because you have everything

197
00:12:19,200 --> 00:12:23,780
everything is you everything everything everything so you know if you're on your laptop and

198
00:12:23,780 --> 00:12:28,740
find something more please tell me this is all there is one simple one-dimensional variable

199
00:12:29,490 --> 00:12:33,950
not one of these is gone so i can make fun of him thomas looks

200
00:12:34,010 --> 00:12:36,220
want everything to be motivated

201
00:12:36,220 --> 00:12:38,280
this is what you univariate

202
00:12:38,300 --> 00:12:43,510
and again left because viruses simple-minded but you can if you can get something out

203
00:12:43,510 --> 00:12:45,640
of univariate why do monte verde

204
00:12:45,660 --> 00:12:49,610
and the univariate thing is the time interval between trades

205
00:12:51,340 --> 00:12:56,820
and between trade them between maximum and minimum between individual traits

206
00:12:56,840 --> 00:13:01,050
and there are forty million traces one three thousand so this so this has five

207
00:13:01,050 --> 00:13:06,040
thousand trades so there are five thousand trades in every territory as the time intervals

208
00:13:06,040 --> 00:13:08,360
so there are four thousand nine hundred ninety nine

209
00:13:08,380 --> 00:13:10,030
little points here

210
00:13:10,050 --> 00:13:14,320
and what can we see with our mother's gift mothers give teaches us

211
00:13:14,380 --> 00:13:17,780
that's the time interval between trains

212
00:13:17,820 --> 00:13:22,800
has a dip near these extreme so if you find one of these extreme maybe

213
00:13:22,800 --> 00:13:27,800
it's a good one here is a dip each extreme another little dip and dip

214
00:13:27,800 --> 00:13:33,680
in the time that their trade just like in the cartoon of thomas looks

215
00:13:33,700 --> 00:13:37,410
and and and similarly the vi v

216
00:13:37,430 --> 00:13:41,680
is doing the inverse of that near the near the

217
00:13:43,950 --> 00:13:47,010
the was going you because obviously people are

218
00:13:47,160 --> 00:13:51,010
are buying treats every microsecond or something

219
00:13:51,050 --> 00:13:56,590
and then there is the volume in a given time interval is better

220
00:13:57,510 --> 00:14:02,300
so these are the data the question is what to do so they key observation

221
00:14:02,300 --> 00:14:04,380
is a very naive one

222
00:14:04,450 --> 00:14:07,630
actually i made it is so rare event that i do anything i have to

223
00:14:07,630 --> 00:14:11,510
so what you do is you take all three forces

224
00:14:11,560 --> 00:14:15,460
and take a bayesian average over it and you can do that efficiently in linear

225
00:14:17,410 --> 00:14:22,430
so i mean this from the perspective the best compressor for finite resources and it's

226
00:14:22,430 --> 00:14:23,910
very efficient

227
00:14:23,930 --> 00:14:27,360
OK rods factor five ten flow on computers so

228
00:14:27,360 --> 00:14:31,080
if you it doesn't matter because of the constant but in practice that matters

229
00:14:31,130 --> 00:14:34,860
and that's another reason for the beautiful compressor

230
00:14:38,260 --> 00:14:40,510
so now we have everything

231
00:14:40,910 --> 00:14:44,570
we have a metric is is normalized and

232
00:14:44,570 --> 00:14:47,920
we can compute this approximation let's do something with it

233
00:14:47,980 --> 00:14:51,180
so now instead of having to up bunch of objects

234
00:14:51,230 --> 00:14:53,720
we compute the distance between the PS

235
00:14:53,730 --> 00:14:56,910
so this gives you a similarity metrics

236
00:14:56,940 --> 00:14:59,510
and now we cluster similar objects

237
00:14:59,560 --> 00:15:01,640
there are many ways of doing that

238
00:15:02,250 --> 00:15:03,470
one way is

239
00:15:03,780 --> 00:15:06,960
to construct when the minimum spanning tree

240
00:15:07,030 --> 00:15:09,370
so what

241
00:15:09,370 --> 00:15:13,990
cilibrasi and we tried it it construct the minimum spanning tree but they constructed something

242
00:15:13,990 --> 00:15:18,230
similar which can has into an every internal node

243
00:15:18,320 --> 00:15:20,670
and they use the so-called quartet method

244
00:15:20,670 --> 00:15:24,630
until finding this because it's an NP complete

245
00:15:24,630 --> 00:15:26,710
o point p hard problem

246
00:15:27,020 --> 00:15:31,110
OK so that's the theory

247
00:15:31,160 --> 00:15:33,330
he one application

248
00:15:33,350 --> 00:15:37,400
it took i think it was twenty four members of cell

249
00:15:37,480 --> 00:15:39,960
the to the middle DNA

250
00:15:40,970 --> 00:15:42,680
it is

251
00:15:42,690 --> 00:15:46,220
o thing over the alphabet a c t g

252
00:15:46,300 --> 00:15:47,670
so it's

253
00:15:47,720 --> 00:15:48,440
o thing or

254
00:15:49,600 --> 00:15:52,700
can easily convert to binary string

255
00:15:52,760 --> 00:15:58,410
and then just compressed samples if the bill passes compressed put into formless and compute

256
00:15:58,430 --> 00:15:59,640
these numbers here

257
00:16:00,480 --> 00:16:02,870
what you see on the diagonal

258
00:16:03,890 --> 00:16:07,600
the norm of the brown bear is very similar to the genome of brown bear

259
00:16:07,600 --> 00:16:09,400
the number is very small

260
00:16:10,140 --> 00:16:12,660
it makes sense

261
00:16:12,670 --> 00:16:16,030
well you also see the genome of the gorilla is very similar to the genome

262
00:16:16,030 --> 00:16:17,460
of human

263
00:16:17,480 --> 00:16:20,430
but not as similar as human to human

264
00:16:20,480 --> 00:16:24,980
much similar they're so chimpanzees even more similar to humans

265
00:16:24,990 --> 00:16:26,170
all right

266
00:16:26,950 --> 00:16:31,470
luckily we are not so similar to cow

267
00:16:32,180 --> 00:16:35,190
so you can look at these numbers and they all make sense but it's a

268
00:16:35,190 --> 00:16:37,950
little bit hard you know to get the main picture and also to put the

269
00:16:37,950 --> 00:16:39,840
whole table here because it is

270
00:16:39,960 --> 00:16:41,250
five countries

271
00:16:41,310 --> 00:16:43,450
so if you look at the minimum spanning tree

272
00:16:43,510 --> 00:16:44,810
and then plot the

273
00:16:44,810 --> 00:16:46,340
in way

274
00:16:46,450 --> 00:16:47,970
often done

275
00:16:48,000 --> 00:16:51,150
so if you these memos

276
00:16:51,150 --> 00:16:53,460
and this is the minimum spanning tree

277
00:16:53,510 --> 00:16:58,370
and as you maybe have recognised car is not exactly a memory but they just

278
00:16:58,370 --> 00:17:01,620
put the car into seen what happens and indeed

279
00:17:01,660 --> 00:17:04,020
it falls completely out of this class

280
00:17:04,070 --> 00:17:06,750
and you see you look the whales are together

281
00:17:06,770 --> 00:17:07,970
the BS

282
00:17:08,950 --> 00:17:11,910
i mean best to get this user to their act together and here we have

283
00:17:11,910 --> 00:17:12,720
all the

284
00:17:12,770 --> 00:17:15,660
all the monkeys of the primus

285
00:17:15,660 --> 00:17:20,650
and so you have the australian animals wallaroo huebner plotted course

286
00:17:21,860 --> 00:17:24,150
and that's the together housemouse

287
00:17:24,830 --> 00:17:27,360
it perfectly reconstructed

288
00:17:27,410 --> 00:17:29,410
the phylogenetic tree

289
00:17:29,410 --> 00:17:32,710
we biologists you know

290
00:17:32,760 --> 00:17:34,170
built up

291
00:17:34,190 --> 00:17:35,600
in many many years

292
00:17:35,630 --> 00:17:40,810
without any biological knowledge just using levels of compression

293
00:17:42,350 --> 00:17:46,670
and it with i mean this was sort of postage dictionaries was there for them

294
00:17:46,670 --> 00:17:51,760
to start stimulus came out the classifier the different kinds of biases

295
00:17:51,810 --> 00:17:54,610
i have followed up the work over the turned out to be

296
00:17:54,610 --> 00:17:55,710
correct or not

297
00:17:55,760 --> 00:17:58,250
if biologists have classified them now

298
00:18:00,180 --> 00:18:03,210
OK here's some viruses

299
00:18:04,980 --> 00:18:06,000
and so on

300
00:18:07,080 --> 00:18:11,550
then there and some test examples so they took some

301
00:18:11,620 --> 00:18:17,250
genes of foxes some media music of hendrix some musical work

302
00:18:17,270 --> 00:18:22,070
some classes some ask text summary notes executables

303
00:18:22,960 --> 00:18:25,670
i just want to know that relying on the hard disk

304
00:18:25,700 --> 00:18:28,090
and you get this classification trees so

305
00:18:30,550 --> 00:18:35,120
fox and red together the best together again here than the music

306
00:18:35,170 --> 00:18:37,000
is classified correctly

307
00:18:37,010 --> 00:18:42,370
and the other five types OK i mean of the easy case but still mean

308
00:18:43,170 --> 00:18:46,340
language tree reconstruction

309
00:18:46,390 --> 00:18:49,360
but it is the two sixty languages

310
00:18:49,370 --> 00:18:50,700
some text

311
00:18:50,720 --> 00:18:53,780
they could have taken the bible because that is written in all languages but they

312
00:18:53,810 --> 00:18:54,600
took the

313
00:18:54,640 --> 00:18:56,460
declaration of human rights

314
00:18:56,520 --> 00:19:00,410
it's also written in both languages

315
00:19:01,490 --> 00:19:07,330
the same thing i mean concatenative languages compressed with little four b set

316
00:19:08,980 --> 00:19:11,990
this tree

317
00:19:14,410 --> 00:19:17,960
it's not perfect but pretty good

318
00:19:17,990 --> 00:19:24,000
i mean he appoints afrikaans dutch luxembourgish and german together

319
00:19:24,070 --> 00:19:31,830
finnish and estonian this together

320
00:19:31,830 --> 00:19:37,660
the roman roman languages are mostly together

321
00:19:38,370 --> 00:19:40,320
i think there one or two errors

322
00:19:40,470 --> 00:19:41,380
i mean have

323
00:19:41,580 --> 00:19:47,120
greek letters in greek is missing because

324
00:19:47,160 --> 00:19:49,630
greeks have defined the alphabet

325
00:19:50,570 --> 00:19:53,510
there were some problems

326
00:19:53,530 --> 00:19:58,570
russian i think is that this is russian

327
00:19:58,620 --> 00:20:02,450
should be also

328
00:20:02,470 --> 00:20:04,500
and garin is the wrong place

329
00:20:10,140 --> 00:20:12,090
a job is not here anymore but

330
00:20:12,220 --> 00:20:16,560
i mean hungarian is also special like like finisher they

331
00:20:19,300 --> 00:20:22,960
on i mean except for one or two exceptions

332
00:20:23,060 --> 00:20:25,550
it seems correct

333
00:20:26,100 --> 00:20:30,570
and this is pretty impressive i mean there's no bias towards you know the sort

334
00:20:30,570 --> 00:20:34,320
of application just use some standard compressed

335
00:20:34,330 --> 00:20:38,490
i mean it works if you have a reasonably clear data if you noisy data

336
00:20:38,490 --> 00:20:40,390
levels if not good

337
00:20:40,400 --> 00:20:46,800
you need to compress images incrementally improving so which takes advantage of if it is

338
00:20:46,800 --> 00:20:51,330
compressed the first part then compressible the second part of its movies and you just

339
00:20:51,330 --> 00:20:54,050
use and pick compressing it emits separately

340
00:20:54,060 --> 00:20:58,810
you cannot detect the similarities between movie movie parts so you would need some real

341
00:20:58,810 --> 00:21:03,400
salt intake or so it really is an incremental pictures and nothing has been done

342
00:21:03,400 --> 00:21:04,410
in the

343
00:21:04,420 --> 00:21:06,500
four for comparing images

344
00:21:06,510 --> 00:21:08,450
so that would be interesting to do

345
00:21:08,470 --> 00:21:09,740
but you need incrementally

346
00:21:09,740 --> 00:21:15,230
you can do an ordination of the thirty five i five similarity matrix present into

347
00:21:15,230 --> 00:21:18,640
some two-dimensional space using multidimensional scaling

348
00:21:18,640 --> 00:21:24,070
and now you see someone who of these different well known from which are available

349
00:21:24,160 --> 00:21:26,680
so the fact of millions

350
00:21:26,740 --> 00:21:28,920
and you know the cluster

351
00:21:28,930 --> 00:21:34,410
the four different different variants million which for cluster together

352
00:21:34,490 --> 00:21:41,320
so this shows a distinction between a clustering method and the clustering algorithms that the

353
00:21:41,370 --> 00:21:47,420
approach to doing things the data analysis from is one instance which utilizes the particular

354
00:21:48,480 --> 00:21:53,650
two to implement that and so it's quite interesting gave spectral

355
00:21:53,660 --> 00:21:57,980
make sure all of you for the link and they all seem to be clustered

356
00:21:57,980 --> 00:22:00,160
together in space

357
00:22:00,210 --> 00:22:05,020
perhaps we should look at some empty space try to

358
00:22:05,030 --> 00:22:06,940
and that

359
00:22:06,990 --> 00:22:09,250
empty space

360
00:22:09,260 --> 00:22:13,390
it turns out that you can also do an analysis it is beginning to happen

361
00:22:13,390 --> 00:22:18,680
the the the various clustering methods which have been published there have some underlying links

362
00:22:18,710 --> 00:22:22,150
and it turns out that there are several methods

363
00:22:22,150 --> 00:22:24,220
which can be tied together

364
00:22:24,280 --> 00:22:28,930
in terms of the i can analysis of the data of the similarity matrix so

365
00:22:28,930 --> 00:22:30,640
even though they appear to be

366
00:22:30,640 --> 00:22:35,640
very different but if you're going to be put into how the

367
00:22:35,650 --> 00:22:40,020
what approaches it turns out that they are quite similar

368
00:22:40,070 --> 00:22:42,130
for some events in the cells

369
00:22:42,140 --> 00:22:43,450
in terms of that

370
00:22:43,470 --> 00:22:45,480
but what i like most

371
00:22:45,530 --> 00:22:47,250
it is how do we

372
00:22:47,300 --> 00:22:50,580
evaluate compared algorithms without

373
00:22:50,590 --> 00:22:52,910
without looking at any particular data

374
00:22:52,930 --> 00:22:56,310
but looking at the head and characteristics

375
00:22:56,350 --> 00:23:01,970
and again i must point out the classical papers by fisher this is that most

376
00:23:01,980 --> 00:23:03,100
of the

377
00:23:03,120 --> 00:23:07,990
most of the people working in machine learning are not aware of appeared environments

378
00:23:08,010 --> 00:23:10,060
in nineteen seventy one

379
00:23:10,060 --> 00:23:12,140
and they proposed admissible

380
00:23:12,150 --> 00:23:13,030
thank you

381
00:23:13,050 --> 00:23:19,450
of clustering so this well let's first define some properties in a set of properties

382
00:23:21,640 --> 00:23:25,610
clustering reasonable clustering algorithm must satisfy

383
00:23:25,660 --> 00:23:30,830
right and then find out whether the clustering algorithm is admissible with respect to those

384
00:23:30,830 --> 00:23:37,870
properties not the notion of the admissible if algorithm satisfies the property

385
00:23:39,520 --> 00:23:43,530
fisher in one this proposal about eight to ten different properties

386
00:23:43,580 --> 00:23:45,890
four i thought would be easy to

387
00:23:45,900 --> 00:23:47,910
two to explain here

388
00:23:48,050 --> 00:23:50,200
one is the point proportional

389
00:23:50,240 --> 00:23:54,130
class proportion last mission and one of the

390
00:23:54,190 --> 00:23:56,940
one of these properties so basically

391
00:23:56,990 --> 00:24:02,910
these are the properties that the sensitivity of a clustering algorithm with respect to changes

392
00:24:02,910 --> 00:24:04,210
that not all

393
00:24:04,220 --> 00:24:06,870
essential structure of the

394
00:24:06,940 --> 00:24:10,410
so one simply leave you with monotonic transformation

395
00:24:10,410 --> 00:24:13,580
of the distances between the points

396
00:24:13,590 --> 00:24:17,420
last mission is what happens if we delete one of the clusters

397
00:24:17,440 --> 00:24:18,650
from the data

398
00:24:20,030 --> 00:24:22,730
pakistan essentially remain unchanged

399
00:24:22,740 --> 00:24:25,310
after deleting cluster

400
00:24:25,350 --> 00:24:27,970
the proportion what happens if we

401
00:24:27,990 --> 00:24:32,510
change the number of points increase the number of points in one cluster

402
00:24:32,570 --> 00:24:34,640
relative to the other cluster

403
00:24:34,680 --> 00:24:36,910
o point proportional means

404
00:24:36,910 --> 00:24:40,370
that just increase the number of points in the cluster

405
00:24:40,380 --> 00:24:42,200
and it turns out that

406
00:24:42,230 --> 00:24:45,850
all the clustering algorithm known at that time

407
00:24:45,880 --> 00:24:47,440
did not satisfy

408
00:24:47,470 --> 00:24:49,300
many of these properties

409
00:24:49,380 --> 00:24:54,080
and then you probably are about lindbergh possibility to which

410
00:24:54,140 --> 00:24:56,340
perhaps it works along similar lines

411
00:24:56,350 --> 00:24:59,360
life and he proposed three different

412
00:25:00,000 --> 00:25:03,590
the idea of constraints scale invariance richness

413
00:25:03,590 --> 00:25:05,500
and consistency property

414
00:25:05,560 --> 00:25:09,270
and scale very similar to what we should one is proposed

415
00:25:09,330 --> 00:25:14,870
maybe one of on the to simply say all possible partitions should should be possible

416
00:25:18,110 --> 00:25:20,850
what happens if you improve the

417
00:25:20,860 --> 00:25:22,680
increase the

418
00:25:22,700 --> 00:25:28,340
distances between the clusters and decrease the distances within the cluster

419
00:25:28,380 --> 00:25:31,470
and is impossible to basically said

420
00:25:31,490 --> 00:25:35,260
there is no clustering and it satisfies all of these three

421
00:25:35,310 --> 00:25:39,710
constraints what does it mean it basically says that it's very difficult to come up

422
00:25:39,710 --> 00:25:42,060
with a point

423
00:25:42,110 --> 00:25:46,080
concept clustering and we always make some trade-offs

424
00:25:46,090 --> 00:25:51,410
when we design a clustering but we we don't really understand what and i think

425
00:25:51,480 --> 00:25:52,700
the point is that

426
00:25:52,730 --> 00:25:57,410
we should try to understand what the requirements of the clustering algorithm model what property

427
00:25:57,510 --> 00:26:01,940
this slide for a given domain and then try to choose an algorithm we're design

428
00:26:02,120 --> 00:26:04,620
one based on that

429
00:26:05,210 --> 00:26:06,310
so this is

430
00:26:06,330 --> 00:26:08,580
the summary is that even though we have

431
00:26:08,630 --> 00:26:12,660
very many clustering algorithm hundreds of thousands of the best

432
00:26:12,670 --> 00:26:17,250
clustering algorithm each algorithm implicitly them

433
00:26:17,270 --> 00:26:21,080
explicitly imposes the structure of the

434
00:26:21,090 --> 00:26:24,550
if the match is between the

435
00:26:24,560 --> 00:26:27,850
in having structures you have the most important thing

436
00:26:27,860 --> 00:26:33,390
and the structure of the data from successful otherwise it's not going to be successful

437
00:26:33,450 --> 00:26:35,440
so here are two different

438
00:26:35,490 --> 00:26:40,110
dataset mixture of three gaussians and two half rings

439
00:26:40,110 --> 00:26:44,500
and if i apply it out mixture model with equal to three predefined

440
00:26:44,560 --> 00:26:48,310
actually the three cluster doesn't work very well on this

441
00:26:48,360 --> 00:26:50,080
if i try spectral

442
00:26:50,090 --> 00:26:55,160
how to it doesn't work very well on the mixture of about but it works

443
00:26:55,190 --> 00:26:55,980
very well on this

444
00:26:56,410 --> 00:26:59,880
two half the data

445
00:26:59,890 --> 00:27:02,570
so in spite of these sort of

446
00:27:02,580 --> 00:27:05,870
limitations of clustering algorithms

447
00:27:05,920 --> 00:27:12,180
it is being used have a lot of success in a variety of different applications

448
00:27:12,230 --> 00:27:13,340
and one of the

449
00:27:13,350 --> 00:27:17,620
applications of data compression in this particular case the showing

450
00:27:17,620 --> 00:27:19,460
this is with one hidden layer

451
00:27:19,520 --> 00:27:21,880
and this is with fourteen this

452
00:27:21,910 --> 00:27:26,070
that shows you that adding extra layers help separate helps separate

453
00:27:27,950 --> 00:27:29,540
one to use four letters

454
00:27:29,550 --> 00:27:33,810
and i don't pre training and i do gradient descent actually do worse than when

455
00:27:33,810 --> 00:27:35,060
i use one

456
00:27:35,120 --> 00:27:36,990
one is one

457
00:27:37,040 --> 00:27:38,430
if i use

458
00:27:38,460 --> 00:27:43,260
pre training when he fall as i do better than phase one

459
00:27:43,310 --> 00:27:47,380
that's a very satisfying result if you want to claim the network to better prize

460
00:27:47,380 --> 00:27:49,760
you appreciate

461
00:27:50,000 --> 00:27:54,920
so this is the test error

462
00:27:54,950 --> 00:27:58,050
as a function of how many layers used without pre training

463
00:27:58,120 --> 00:28:02,000
two less is slightly better than one so the results to john john platt tried

464
00:28:02,000 --> 00:28:04,690
this kind of network i tried this kind of network and those were the ones

465
00:28:04,690 --> 00:28:07,450
where you get down to about one point six and they see it

466
00:28:07,480 --> 00:28:13,040
which is a relief

467
00:28:13,080 --> 00:28:16,460
as you add more layers it doesn't really work any better in fact by follows

468
00:28:16,460 --> 00:28:17,670
is worse

469
00:28:17,710 --> 00:28:19,760
whereas with pre training

470
00:28:19,860 --> 00:28:23,350
if you ignore the last data point and things just get better

471
00:28:23,390 --> 00:28:28,860
and if you don't want ignore the last stage point at least you can have

472
00:28:28,870 --> 00:28:33,080
this explains why the brain

473
00:28:36,410 --> 00:28:39,750
three or four

474
00:28:43,140 --> 00:28:46,010
three it is all search

475
00:28:47,180 --> 00:28:49,300
i'm things two problems

476
00:28:49,310 --> 00:28:51,750
the ones that you can't find a good optimum

477
00:28:51,800 --> 00:28:53,370
and the other is that

478
00:28:53,380 --> 00:28:55,310
you could be

479
00:28:55,320 --> 00:28:58,500
the pre training put in good region the search space

480
00:28:58,510 --> 00:29:01,300
which you don't get if you just use the labeled with

481
00:29:01,310 --> 00:29:03,350
it is also used

482
00:29:03,370 --> 00:29:06,320
the reason i guess there are

483
00:29:06,350 --> 00:29:08,010
we were commissioned

484
00:29:08,020 --> 00:29:11,200
it using a kind of gradient descent to model the

485
00:29:11,230 --> 00:29:13,360
structure in the input data right

486
00:29:13,370 --> 00:29:16,060
which normal back provisions in trying to model

487
00:29:16,070 --> 00:29:19,860
so is the fact that using an unconditional density model

488
00:29:19,870 --> 00:29:21,480
that means that you get it

489
00:29:21,490 --> 00:29:24,430
you get much less problem with overfitting

490
00:29:24,500 --> 00:29:29,510
and it also means with very few labels you can do quite well

491
00:29:29,560 --> 00:29:31,850
so you see

492
00:29:37,440 --> 00:29:40,620
on the training set if you try backprop

493
00:29:42,000 --> 00:29:47,240
it's still improving on the validation set when zero tester

494
00:29:47,250 --> 00:29:49,360
so i guess that means the answer is no

495
00:29:57,560 --> 00:30:00,900
this is using a dimensionality reduction technique

496
00:30:02,040 --> 00:30:05,610
model different nets too

497
00:30:05,630 --> 00:30:07,390
so you can take the

498
00:30:07,410 --> 00:30:10,790
function from inputs to outputs that is you can have a whole bunch of test

499
00:30:10,790 --> 00:30:11,910
input vectors

500
00:30:11,930 --> 00:30:16,180
and you look at the output against feature these test input vectors and then concatenate

501
00:30:16,180 --> 00:30:17,660
all those outputs

502
00:30:17,700 --> 00:30:21,180
over the span to test in practice to integrate big vector

503
00:30:21,230 --> 00:30:25,430
and nine great big vector you can say is how this network behaves

504
00:30:25,520 --> 00:30:30,940
this particular network behaves and then you can try training lots of times and see

505
00:30:30,940 --> 00:30:33,070
if you can networks to behave the same way

506
00:30:33,080 --> 00:30:34,440
so you're ignoring

507
00:30:34,440 --> 00:30:38,350
the internal structure saying how to behave as an input device on this set of

508
00:30:39,430 --> 00:30:41,160
which is kind of except

509
00:30:43,200 --> 00:30:50,590
they used a dimensionality reduction technique called need to take a whole bunch of

510
00:30:50,610 --> 00:30:52,580
let's train pure backprop

511
00:30:52,640 --> 00:30:54,330
we start off here

512
00:30:56,180 --> 00:30:58,590
this is after all

513
00:30:58,660 --> 00:31:03,100
one iteration two iterations and so on and you say they'll go off to

514
00:31:03,130 --> 00:31:05,220
slightly different solutions

515
00:31:05,230 --> 00:31:10,060
but this set of solutions has nothing in common with this set is solutions these

516
00:31:10,060 --> 00:31:11,840
are the ones that were trained with

517
00:31:11,900 --> 00:31:15,150
pre training followed by

518
00:31:15,160 --> 00:31:16,520
fine tuning

519
00:31:16,530 --> 00:31:25,700
so really it's finding different part of the space there's no overlap here

520
00:31:25,710 --> 00:31:30,530
so this is probably the most important transparency the slide

521
00:31:30,570 --> 00:31:33,350
shows you how old i am

522
00:31:34,920 --> 00:31:39,450
the whole tutorial i think is why this deep stuff makes sense

523
00:31:39,470 --> 00:31:42,180
and use the technical concept called stuff

524
00:31:44,020 --> 00:31:47,230
the axioms for stuff like it's happens

525
00:31:47,240 --> 00:31:51,660
and the stuff that we don't know that we don't know and all OK so

526
00:31:51,660 --> 00:31:55,170
there's this stuff goes on and stuff creates images

527
00:31:55,210 --> 00:31:58,270
now if you believe the way the world works is the stuff we create images

528
00:31:58,270 --> 00:32:00,310
and images and create labels

529
00:32:00,330 --> 00:32:05,400
so that the label is conditionally independent of the stuff once you see the image

530
00:32:05,400 --> 00:32:09,340
the magic it's like they from better

531
00:32:09,340 --> 00:32:13,490
again it's completely unchanged to all of this is exactly the same as what i

532
00:32:13,490 --> 00:32:14,700
had before

533
00:32:14,720 --> 00:32:16,650
except for this step with

534
00:32:16,670 --> 00:32:21,540
you move you get a new x from the latest y by taking a step

535
00:32:21,540 --> 00:32:25,640
in the negative gradient direction that has to change because we want the wise to

536
00:32:25,640 --> 00:32:28,580
stay inside the set my omega

537
00:32:28,580 --> 00:32:33,200
and in fact this is how it changes you choose the next next to be

538
00:32:33,220 --> 00:32:36,500
the as close as you can get to that point that you would have gone

539
00:32:36,500 --> 00:32:39,670
to any unconstrained case while staying in america

540
00:32:40,350 --> 00:32:42,980
that's really the only change made to the math

541
00:32:43,000 --> 00:32:47,210
on the convergence theory pretty much those side

542
00:32:47,250 --> 00:32:50,740
now what about when you've got not an explicit constraints that make it but you're

543
00:32:50,740 --> 00:32:56,080
trying to minimize f plus ten time some regularisation term

544
00:32:56,210 --> 00:33:00,990
and this was the topic of bellman tutorial this morning it there are lot of

545
00:33:00,990 --> 00:33:04,170
applications like compressed sensing and less so and so on

546
00:33:04,190 --> 00:33:06,550
we've got some smooth objective here

547
00:33:06,550 --> 00:33:10,860
and you got something nonsmooth here but simple like an l one norm

548
00:33:10,870 --> 00:33:13,750
so how do these methods generalize to this case

549
00:33:13,970 --> 00:33:17,370
all the methods of to almost assume smoothness but

550
00:33:17,410 --> 00:33:21,630
so the generalizable to nonsmooth so what do you do in that case well again

551
00:33:21,640 --> 00:33:22,710
a lot of

552
00:33:22,760 --> 00:33:24,470
generalized if you can

553
00:33:24,510 --> 00:33:30,210
explicitly handle the size of the regularisation term if you can deal with that explicitly

554
00:33:30,290 --> 00:33:33,090
they often generalize very easily

555
00:33:33,170 --> 00:33:34,490
so for example

556
00:33:34,500 --> 00:33:37,750
the one where we would move where we just take a short step in the

557
00:33:37,750 --> 00:33:40,360
negative gradient direction

558
00:33:40,370 --> 00:33:42,150
if i can replace that

559
00:33:42,970 --> 00:33:44,850
minimizing this thing

560
00:33:44,850 --> 00:33:47,650
which is you know as close as you can get to that short step

561
00:33:48,750 --> 00:33:54,880
the regularizer exactly as it appears here if this problem is easy to solve

562
00:33:54,910 --> 00:33:59,040
which it is in many cases such as when this is the l one norm

563
00:33:59,040 --> 00:34:02,480
then most of those methods like astros methods

564
00:34:02,490 --> 00:34:07,470
gradient methods and so on generalize immediately the theory can go right through

565
00:34:07,490 --> 00:34:10,150
so the first method for instance

566
00:34:10,170 --> 00:34:12,870
which is one of the ones i had up earlier on

567
00:34:12,880 --> 00:34:18,030
it immediately general in factor analysis and first

568
00:34:18,040 --> 00:34:22,930
just works with this regularize case and if all you care about the unconstrained thinking

569
00:34:22,930 --> 00:34:24,900
just said this to zero in

570
00:34:24,930 --> 00:34:28,140
and it will still work exactly the same

571
00:34:28,150 --> 00:34:32,600
so the bottom line here is that for a lot of unconstrained gradient simple gradient

572
00:34:32,600 --> 00:34:34,390
first order methods

573
00:34:34,400 --> 00:34:37,730
it's often not that big a deal if you want to toss in the regularisation

574
00:34:37,730 --> 00:34:40,720
term or an explicit constraints

575
00:34:40,740 --> 00:34:42,760
so this is the end of the first

576
00:34:42,810 --> 00:34:47,230
and these are a few references we can go for further information i would be

577
00:34:47,230 --> 00:34:48,880
happy to give you more if you

578
00:34:48,930 --> 00:34:51,550
the more is two thousand four book by

579
00:34:51,550 --> 00:34:55,810
nesterov this paper by back into bull actually is the two thousand seven or eight

580
00:34:56,660 --> 00:35:00,170
they've written a review paper which you can get from to bills website which is

581
00:35:00,530 --> 00:35:03,350
a little more self-contained and up date

582
00:35:03,360 --> 00:35:06,440
this an album by polyak which is where the heavy ball

583
00:35:06,470 --> 00:35:11,080
come from it's very beautiful book actually is got conjugate gradient

584
00:35:11,080 --> 00:35:14,740
and and steepest ascent things like that and

585
00:35:14,760 --> 00:35:18,380
this is the barzilai borwein paper that i mentioned and this is

586
00:35:18,380 --> 00:35:21,920
very recent paper by nesterov

587
00:35:21,950 --> 00:35:25,070
OK second part of the talk this is the part where we can't even get

588
00:35:25,070 --> 00:35:27,160
the first derivative

589
00:35:27,180 --> 00:35:31,510
OK so i'm going to talk about stochastic incremental gradient so the setting here is

590
00:35:31,510 --> 00:35:32,760
that i'm going to allow

591
00:35:32,780 --> 00:35:36,890
after being nonsmooth i'm going to assume that

592
00:35:36,900 --> 00:35:38,370
you know i can't

593
00:35:38,400 --> 00:35:41,640
economically get a function that is so i can

594
00:35:41,700 --> 00:35:45,110
gov and test to point to see how good it is

595
00:35:45,450 --> 00:35:47,910
at least i can reliably do that

596
00:35:47,930 --> 00:35:51,950
and i'm going to assume that at any point i've got access to some sort

597
00:35:51,950 --> 00:35:57,550
of estimate of the gradient or subgradient in the case of the non smooth functions

598
00:35:57,590 --> 00:36:00,910
so here's a couple of common problems that fall into this setting

599
00:36:00,930 --> 00:36:03,490
there's one way or objective is actually the

600
00:36:03,630 --> 00:36:07,930
well you've got a function f which depends on your variables x but also on

601
00:36:07,930 --> 00:36:12,680
some random variable and what you're interested in minimizing is the expectation is that of

602
00:36:12,700 --> 00:36:14,010
the random variable

603
00:36:14,110 --> 00:36:16,630
OK so that's your objective facts

604
00:36:16,660 --> 00:36:21,630
related settings where you're fx is actually made up of the sum of objective some

605
00:36:21,630 --> 00:36:23,110
finite size

606
00:36:23,140 --> 00:36:27,050
OK so these were each of these is convex and nonsmooth

607
00:36:27,050 --> 00:36:30,570
now these would ring a bell with a lot of you because SVM

608
00:36:30,660 --> 00:36:34,240
standard SVM setting has this for primal SVM

609
00:36:34,260 --> 00:36:38,910
typically has some sort of regularizer here one norm and two norm squared and then

610
00:36:38,950 --> 00:36:42,280
has this loss function which is made up of the sum of losses over each

611
00:36:42,280 --> 00:36:43,470
piece of data

612
00:36:43,660 --> 00:36:46,760
in the training in the training set

613
00:36:46,760 --> 00:36:51,300
so for instance in linear SVM this loss function l might have this form

614
00:36:51,300 --> 00:36:53,340
the variable might be w

615
00:36:53,400 --> 00:36:57,110
a new loss function is a function of w transpose the feature vectors

616
00:36:57,110 --> 00:37:00,680
and the classifier and the label while k

617
00:37:00,700 --> 00:37:07,300
another very closely related cases logistic regression where the loss functions of some of these

618
00:37:07,300 --> 00:37:11,180
log functions and the regularizer might be one of

619
00:37:11,200 --> 00:37:13,630
so this is something that obviously

620
00:37:13,640 --> 00:37:16,970
the setting obviously fits well in machine learning

621
00:37:17,010 --> 00:37:24,880
OK just mention here of terminology and notation since i'm now dealing more explicitly with

622
00:37:24,900 --> 00:37:30,160
nonsmooth functions i have to talk about subgradients i've got a picture subgradients here this

623
00:37:30,160 --> 00:37:33,200
is mine on the graph of my nonsmooth f

624
00:37:33,220 --> 00:37:37,840
you can see that it's mostly smooth except at this point he was the king

625
00:37:37,880 --> 00:37:42,300
so at that can i can define a bunch of supporting hyperplanes in because this

626
00:37:42,300 --> 00:37:48,220
is the one the function hyperplanes just winds and they just plain kind of libel

627
00:37:48,220 --> 00:37:49,800
of the graph of the function

628
00:37:49,820 --> 00:37:54,380
the slopes of each of these lines would be an element of the subgradient OK

629
00:37:54,380 --> 00:37:57,660
so this upgrade in this case would be an interval on the real line

630
00:37:57,680 --> 00:38:01,660
so in general subgradient is some sort of a convex set

631
00:38:01,660 --> 00:38:06,050
we we can still talk about strong convexity when i was dealing with smooth functions

632
00:38:06,360 --> 00:38:09,470
mu is the lower bound on the i can values of passion

633
00:38:09,470 --> 00:38:12,490
i can still even though the hashing doesn't exist because they don't even have a

634
00:38:12,490 --> 00:38:15,570
great the nonsmooth case i can still talk about

635
00:38:15,590 --> 00:38:21,110
modulus of convexity mu and i can still have a relationship that looks like a

636
00:38:21,110 --> 00:38:23,430
lower down on the second order taylor series

637
00:38:26,530 --> 00:38:32,470
OK so classical stochastic approximation for stochastic gradient descent method

638
00:38:32,510 --> 00:38:37,300
what a shame here is that i've got access at any point x got access

639
00:38:37,300 --> 00:38:39,740
to this thing big effects inside

640
00:38:39,740 --> 00:38:43,380
which is some estimate of the gradient of this gradient or subgradient

641
00:38:43,410 --> 00:38:47,880
x OK so you can get there by sampling site from that distribution with that

642
00:38:47,880 --> 00:38:51,180
points and all this is happening in the feature space so we have a new

643
00:38:51,180 --> 00:38:55,980
point we map that new point also in the feature space and now we

644
00:38:55,980 --> 00:38:59,680
want to classify it somehowso let's say the new points are bold-faced points on

645
00:38:59,680 --> 00:39:04,700
vectors in in my feature space in the reproducing kernel hilbert space I want

646
00:39:04,780 --> 00:39:09,010
to classify this new points somehow and the simplest way of doing this might be just

647
00:39:09,120 --> 00:39:15,020
to say I'll assign it to the class whose class mean is closest so I'll compute the mean

648
00:39:15,020 --> 00:39:19,500
of the positive class so let's say this is a number of positive points compute the mean

649
00:39:19,500 --> 00:39:25,700
of the negative class actually this shouldprobablysay C plus and C minus here not c

650
00:39:26,120 --> 00:39:32,760
two and c one and i'll just check which of the two is closer and

651
00:39:32,760 --> 00:39:37,940
there's different ways of checking that for instance I could say I'll compute this vector

652
00:39:37,940 --> 00:39:44,960
Wand I'll compute this vector X minus C where C is the midpoint between the

653
00:39:44,970 --> 00:39:50,760
two classes between the two class means and then I'll check whether the angling close by

654
00:39:50,760 --> 00:39:54,140
by these two is is larger than ninety degrees or smaller than ninety degrees

655
00:39:54,140 --> 00:39:59,160
which I can check at by looking at the cosine of the angle whether cosine

656
00:39:59,290 --> 00:40:05,260
is positive or negative and the cosine in turn I just compute by taking the dot product

657
00:40:05,300 --> 00:40:08,440
so that's nice because I like to do everything in terms with dot products because we

658
00:40:08,440 --> 00:40:13,620
want to work using kernels we don't want to explicitly map into that space so

659
00:40:13,620 --> 00:40:16,480
that's one way of doing it the other way of doing it would

660
00:40:16,480 --> 00:40:23,280
be just to compute the distance square distance between X and this center and the square distance

661
00:40:23,280 --> 00:40:28,180
is between X and this center that's an exercise that maybe we shouldn't do now

662
00:40:28,240 --> 00:40:32,780
in the interest of time but you could you should try it sometime you compute

663
00:40:32,780 --> 00:40:38,300
these two squared distances take the difference between the two and then check whether

664
00:40:38,300 --> 00:40:42,540
that difference is positive or negative so that will be our decision function if one of

665
00:40:42,540 --> 00:40:46,500
them is larger than the other one we will assign the point to the other

666
00:40:46,500 --> 00:40:53,000
class and vice versa so if we do that it's just a few lines of

667
00:40:53,060 --> 00:41:01,560
algebra then we end up of course everything will nicely combine into dot products so

668
00:41:01,570 --> 00:41:06,520
all this dot products will then be replaced by kernel functions and it turns out we then

669
00:41:06,540 --> 00:41:14,900
get a decision function which consists of an expansion in terms of all

670
00:41:14,900 --> 00:41:21,060
the positive points so the points with positive labels divided by the number of positive points

671
00:41:21,060 --> 00:41:25,660
until all the negative points divided by the number again and here we have some

672
00:41:25,660 --> 00:41:32,920
constant that doesn't depend on the test point X so let's not think about this

673
00:41:32,920 --> 00:41:37,420
constant for a second but uh what I want to say is that uh you can

674
00:41:37,420 --> 00:41:46,920
think of this as a density estimation based classified because in the case where

675
00:41:47,150 --> 00:41:50,160
for instance we use a gaussian kernel here let's say we use a gaussian kernel

676
00:41:50,170 --> 00:41:55,300
that's normalized to have ones so this is the density model and

677
00:41:55,300 --> 00:41:59,550
then we're just putting such a gaussian on each positive training point we divide by

678
00:41:59,550 --> 00:42:05,240
the number of these points that's what's called a parzen windows density estimate in statistics

679
00:42:05,260 --> 00:42:11,200
of the positive class here we have corresponding parzen windows extensive estimate of the negative class

680
00:42:11,380 --> 00:42:16,200
here's a constant so roughly speaking I mean i don't go into details on the content and so on

681
00:42:16,320 --> 00:42:23,740
roughly speaking we have these density estimates and we check whether the probability of the point

682
00:42:23,740 --> 00:42:30,180
X is is higher under this density model or higher under this density model

683
00:42:30,260 --> 00:42:33,800
so we're looking roughly speaking whether it's more likely that the point comes from the

684
00:42:33,800 --> 00:42:39,660
one class or the other class and we arrive at this with this purely geometric

685
00:42:39,660 --> 00:42:44,040
point of view so it's just simply looking whether we're closer to the one mean or to the

686
00:42:44,050 --> 00:42:52,660
other mean so these means are kind of interesting and we will return to the

687
00:42:52,660 --> 00:42:57,180
means in a minute before that I just want to briefly show you we will

688
00:42:57,190 --> 00:43:01,160
come back to support vector machines a little later but just to give you the

689
00:43:01,160 --> 00:43:13,260
first idea so a support vector machine is almost the same as what I just showed you alright it's

690
00:43:13,260 --> 00:43:20,640
almost the same so it also computes hyperplane in some other space remember this decision rule

691
00:43:20,640 --> 00:43:24,500
that I told you about in the last slide so you have a mean of

692
00:43:24,500 --> 00:43:27,680
one class and a mean of the other class and you're checking whether you're closer to here or to here

693
00:43:27,680 --> 00:43:34,900
these induces a decision boundary that's a hyperplane support vector machine also a

694
00:43:34,900 --> 00:43:39,260
which with hyperplanes only that these hyperplanes are a bit different for the hyperplane that's

695
00:43:39,300 --> 00:43:44,920
orthogonal to the vector connecting the class means here now we

696
00:43:44,920 --> 00:43:50,420
now will use a hyperplane that maximizes this margin of separation between the classes we'll

697
00:43:50,420 --> 00:43:54,150
get back to that in a minute but it's also between the feature space and

698
00:43:54,150 --> 00:43:59,340
then this would correspond to a non-linear decision rule in the input space and also

699
00:43:59,340 --> 00:44:03,200
if you look how how the decision rule looks it will also be a kernel

700
00:44:03,210 --> 00:44:08,160
expansion like before so it will be this sign of a kernel expansion and some constant

701
00:44:08,160 --> 00:44:13,900
and the difference now is in the previous case the kernel expansion was almost trivial

702
00:44:13,900 --> 00:44:19,770
because all the weights were uniformed so all positive points had the same weight and

703
00:44:19,900 --> 00:44:24,700
negative points had also the same weight with the negative class as we didn't

704
00:44:24,720 --> 00:44:27,800
have to do any training ce could directly write down the solution in the

705
00:44:27,800 --> 00:44:31,500
case of support vector machines it turnes out we have to do some training to find

706
00:44:31,500 --> 00:44:36,800
these lambda Is but the advantage will be that typically many of these lambda Is

707
00:44:36,800 --> 00:44:44,680
will become zero okay and if you now choose different kernel functions here this means

708
00:44:44,690 --> 00:44:50,220
that you choosing different kinds of mapping into the upper space and therefore how

709
00:44:50,220 --> 00:44:52,280
what do you do want to say

710
00:44:52,290 --> 00:44:54,910
what exactly was in your mind

711
00:44:54,930 --> 00:44:58,300
and he was the one giving cos those tools

712
00:44:58,320 --> 00:45:02,100
to analyse and criticize

713
00:45:02,120 --> 00:45:05,310
getting the arguments back

714
00:45:05,360 --> 00:45:09,390
and in english it's much better than slovenian because argument

715
00:45:09,440 --> 00:45:11,910
it's not only you know the proof

716
00:45:11,940 --> 00:45:15,050
but it's also a conflict

717
00:45:15,100 --> 00:45:19,530
and that's what happened when you expect people to defend themselves

718
00:45:19,570 --> 00:45:22,130
later said writer on

719
00:45:23,360 --> 00:45:25,580
i realize that is three four

720
00:45:25,600 --> 00:45:28,500
it's three quarters of all know

721
00:45:28,540 --> 00:45:32,830
three out of four conversations that so what does was running

722
00:45:32,890 --> 00:45:35,980
given no constructive results

723
00:45:36,020 --> 00:45:39,520
that's how it is in our meetings as well isn't it

724
00:45:39,540 --> 00:45:42,150
because we also read the very

725
00:45:42,160 --> 00:45:43,180
so much

726
00:45:43,290 --> 00:45:46,630
let's have another idea that was to follow

727
00:45:46,650 --> 00:45:49,500
you know with this idea that

728
00:45:49,560 --> 00:45:51,330
the truth

729
00:45:51,380 --> 00:45:52,860
is he

730
00:45:52,880 --> 00:45:54,890
and we have to find it

731
00:45:55,060 --> 00:45:58,950
in that case trolls

732
00:45:59,060 --> 00:46:01,710
is only one

733
00:46:01,710 --> 00:46:03,260
if only

734
00:46:03,270 --> 00:46:05,970
one truth exists

735
00:46:05,990 --> 00:46:07,470
who owns it

736
00:46:07,520 --> 00:46:09,140
was the order of it

737
00:46:09,190 --> 00:46:16,820
if god exists

738
00:46:18,190 --> 00:46:19,820
it's my

739
00:46:19,840 --> 00:46:23,040
i am right

740
00:46:23,040 --> 00:46:24,540
which means you

741
00:46:24,590 --> 00:46:27,320
are not

742
00:46:27,330 --> 00:46:28,660
that's how we think

743
00:46:28,680 --> 00:46:32,490
i'm right and you're wrong

744
00:46:33,120 --> 00:46:35,470
whenever my mother says

745
00:46:35,470 --> 00:46:38,350
this is how you should make it you can

746
00:46:38,470 --> 00:46:43,360
that means that she wrote later to much

747
00:46:43,370 --> 00:46:48,380
now that's what i think i think i'm the the only one who has possession

748
00:46:48,400 --> 00:46:49,460
to have

749
00:46:50,740 --> 00:46:53,000
and then there is there is totally

750
00:46:53,040 --> 00:46:57,430
you know making everything into categories

751
00:46:57,440 --> 00:47:00,770
this is the table this is not the table this is the chair this is

752
00:47:00,770 --> 00:47:04,760
not the chair everything should be know like this and if i see the

753
00:47:05,040 --> 00:47:07,040
aristotle is in problems

754
00:47:07,060 --> 00:47:11,010
because it's not in the categories in this black and this is what

755
00:47:11,040 --> 00:47:12,580
and this is life

756
00:47:12,610 --> 00:47:14,520
it's something in between

757
00:47:14,580 --> 00:47:16,200
and but you know

758
00:47:16,240 --> 00:47:17,820
from politics

759
00:47:17,880 --> 00:47:20,010
left and the right

760
00:47:20,050 --> 00:47:22,640
no blacks and the rats

761
00:47:22,730 --> 00:47:27,980
this is our team this is the others we are right there on and we

762
00:47:27,980 --> 00:47:29,620
will prove them

763
00:47:29,630 --> 00:47:32,830
how constructive is it

764
00:47:32,840 --> 00:47:34,320
going nowhere

765
00:47:34,370 --> 00:47:36,720
maybe slovenia going backwards

766
00:47:36,750 --> 00:47:38,350
not helpful at all

767
00:47:38,380 --> 00:47:42,580
we can't have it like this

768
00:47:42,630 --> 00:47:52,740
sheet of the ocean OK OK

769
00:47:53,390 --> 00:47:55,480
to leave behind the time

770
00:47:55,730 --> 00:47:59,040
what happens is that you know

771
00:47:59,090 --> 00:48:02,200
with church

772
00:48:02,200 --> 00:48:04,840
after the greeks after romance

773
00:48:04,870 --> 00:48:09,260
the black the black h the dark ages k so that are

774
00:48:09,310 --> 00:48:14,140
the euro was actually illiterate

775
00:48:14,160 --> 00:48:19,800
charles the great united europe for awhile was an illiterate man

776
00:48:19,800 --> 00:48:26,540
where was all the knowledge where was all the reading

777
00:48:26,590 --> 00:48:31,690
you know that in one thousand years of dark ages there was only three percent

778
00:48:31,700 --> 00:48:32,220
of course

779
00:48:33,300 --> 00:48:35,140
of national product

780
00:48:35,190 --> 00:48:37,100
in one thousand years

781
00:48:37,130 --> 00:48:39,090
let's hope of activities

782
00:48:39,100 --> 00:48:43,500
it was all happening in charge actually all the reading was happening in church

783
00:48:43,720 --> 00:48:48,960
and they actually do this tools of greek thinking

784
00:48:48,970 --> 00:48:51,790
which is easy for them because they have the dopamine

785
00:48:51,800 --> 00:48:54,160
it's fixed position

786
00:48:54,240 --> 00:48:55,860
you know like six position

787
00:48:55,880 --> 00:48:56,880
this is

788
00:48:56,930 --> 00:48:58,730
half litre

789
00:48:58,730 --> 00:49:03,650
we all agreed upon it we can't fight about this is half litre

790
00:49:03,710 --> 00:49:05,850
but we can already fight

791
00:49:05,870 --> 00:49:08,800
whether this water is called

792
00:49:08,850 --> 00:49:11,100
or war

793
00:49:11,110 --> 00:49:12,660
but is my perception

794
00:49:12,700 --> 00:49:14,110
i think it's war

795
00:49:14,140 --> 00:49:16,210
you think it's called

796
00:49:16,250 --> 00:49:18,090
and then we fight

797
00:49:18,150 --> 00:49:22,880
most of our thinking mistakes of perceptual thinking

798
00:49:22,890 --> 00:49:25,480
with the and

799
00:49:25,520 --> 00:49:29,760
i think something is like this you think it's like that and then we find

800
00:49:29,760 --> 00:49:31,380
out about it

801
00:49:31,400 --> 00:49:34,880
but what we should do is to accept it

802
00:49:35,850 --> 00:49:40,470
we are so used because of size because of charge because of greek

803
00:49:40,470 --> 00:49:45,780
probability theory that are called extension theorems of projective limit theorems which guarantee that this is actually the case

804
00:49:48,220 --> 00:49:48,820
the idea is

805
00:49:49,320 --> 00:49:51,300
you have a one-to-one correspondence between

806
00:49:52,120 --> 00:49:54,160
singer infinite dimensional distribution

807
00:49:55,010 --> 00:49:56,660
an infinite family of

808
00:49:57,070 --> 00:49:58,390
finite-dimensional distributions

809
00:50:00,550 --> 00:50:01,280
all right you need some

810
00:50:04,620 --> 00:50:05,890
you have to be careful with these kind of

811
00:50:06,600 --> 00:50:09,530
results because we need some you have to do some extra work

812
00:50:10,010 --> 00:50:11,740
to make sure that what you sample from that

813
00:50:12,240 --> 00:50:15,160
is actually a continuous function can you know this defines

814
00:50:16,010 --> 00:50:18,360
using this differ gives a distribution on functions

815
00:50:18,930 --> 00:50:21,490
but you would like to have more than that you would like to have continuous

816
00:50:22,120 --> 00:50:24,090
the continuity property and that's a bit harder

817
00:50:24,620 --> 00:50:25,550
but i cannot tell u

818
00:50:26,220 --> 00:50:30,360
how to do that without starting to talk about things like topology and i think

819
00:50:30,360 --> 00:50:32,030
you will be all glad i don't do so

820
00:50:32,950 --> 00:50:33,490
have avoided

821
00:50:34,640 --> 00:50:35,280
this is a warning

822
00:50:37,390 --> 00:50:38,720
but basically be deum

823
00:50:40,200 --> 00:50:43,070
the idea is you can define a stochastic process so

824
00:50:43,590 --> 00:50:44,930
the probability distribution on

825
00:50:45,370 --> 00:50:47,860
infinite objective are infinite dimensional objects

826
00:50:49,070 --> 00:50:52,590
means of family by all its finite-dimensional marginals

827
00:50:53,090 --> 00:50:54,530
that is the basic the basic idea

828
00:50:56,680 --> 00:51:00,540
well the gas in process is the distribution the definition that we always use or

829
00:51:00,540 --> 00:51:02,620
the construction think of this as a construction

830
00:51:03,740 --> 00:51:04,490
that we always use

831
00:51:05,030 --> 00:51:05,550
and i said

832
00:51:06,010 --> 00:51:10,140
before that we have this problem that we don't have density representations for these models

833
00:51:10,680 --> 00:51:12,120
now this year is a representation

834
00:51:14,120 --> 00:51:16,090
it's not a density is not as nicely

835
00:51:17,590 --> 00:51:20,450
we are not able to write it down as nicely as the density but this

836
00:51:20,450 --> 00:51:22,840
is a representation of an infinite dimensional distribution

837
00:51:23,820 --> 00:51:27,620
and this is in fact this kind of these kind of results are the most

838
00:51:28,760 --> 00:51:31,030
generally applicable substitutes we have

839
00:51:31,620 --> 00:51:33,390
so far density representations

840
00:51:33,970 --> 00:51:34,990
on stochastic processes

841
00:51:38,320 --> 00:51:40,910
okay now can we do something similar other directly process

842
00:51:42,390 --> 00:51:43,390
yes we can actually

843
00:51:44,470 --> 00:51:45,030
so we can

844
00:51:45,450 --> 00:51:47,260
yesterday i showed you this this nice

845
00:51:48,030 --> 00:51:50,870
simple a constructive definition of the iterative process which

846
00:51:51,260 --> 00:51:52,820
which tells us simply how to sample

847
00:51:53,300 --> 00:51:55,140
this discrete random error piece by piece

848
00:51:56,240 --> 00:51:59,620
but we can use a similar construction as was the other girls process

849
00:52:00,160 --> 00:52:03,300
in the gaussianprocess case we have constructed a random function theta

850
00:52:04,240 --> 00:52:04,570
and now

851
00:52:05,160 --> 00:52:06,820
theta is a random measure so

852
00:52:07,320 --> 00:52:11,780
i called you again right because it plays the same role but it's it's this

853
00:52:11,800 --> 00:52:14,620
random measure and random discrete measure remember the some

854
00:52:15,070 --> 00:52:17,220
over the weighted sum over classifications

855
00:52:19,950 --> 00:52:21,030
all right so now

856
00:52:21,590 --> 00:52:23,600
the the corresponding results

857
00:52:24,470 --> 00:52:29,160
to this construction of the girls and processes this we have to find in the gaussianprocess case

858
00:52:29,680 --> 00:52:32,780
in the gaussianprocess case what we did we looked at marginals and how did we

859
00:52:32,780 --> 00:52:37,550
get the marginals we chose points down here and then restricted ourselves to these points

860
00:52:41,640 --> 00:52:43,890
four probability measures it's not really very helpful

861
00:52:44,930 --> 00:52:45,680
because they are not

862
00:52:46,140 --> 00:52:48,950
you know that that they are functions on sets right so

863
00:52:49,620 --> 00:52:54,410
this this year this random function is a function on points but probability measure is a function sets

864
00:52:54,840 --> 00:52:55,700
or put differently

865
00:52:56,120 --> 00:52:59,890
for most probable like say continuous probability measures you put it in a single point

866
00:53:01,530 --> 00:53:03,600
the probability is always zero right for gasses

867
00:53:04,240 --> 00:53:05,160
so that's not very helpful

868
00:53:08,010 --> 00:53:12,010
so what we do instead is to get the marginals we don't use points we use sets

869
00:53:13,990 --> 00:53:19,050
and once again we want from the marginals to be finite-dimensional so we choose a finite number of sets

870
00:53:19,700 --> 00:53:25,220
and we choose sets which partition our space imagine that this square here are the rectangular either

871
00:53:27,450 --> 00:53:30,780
it's space on which we would like to put process or some other random

872
00:53:33,590 --> 00:53:36,490
now what do we do we partition it into a finite number of sets which

873
00:53:36,490 --> 00:53:38,740
i have called it one two three and so on you

874
00:53:39,950 --> 00:53:41,950
so we can write these sets into a vector

875
00:53:42,950 --> 00:53:45,930
together they from the whole space and now we take our random

876
00:53:46,820 --> 00:53:48,090
a random probability measure

877
00:53:48,700 --> 00:53:51,100
and we evaluated on each of these sets

878
00:53:51,640 --> 00:53:54,410
so just as before we took a random function reflecting the point

879
00:53:54,910 --> 00:53:56,280
now we take a random probability

880
00:53:56,720 --> 00:54:00,620
and we say how much probability doesn't put on set one on saturday to and

881
00:54:00,620 --> 00:54:01,910
so on and that gives us a vector

882
00:54:02,570 --> 00:54:05,550
which properties well vector have well it's finite

883
00:54:07,300 --> 00:54:10,620
all these entries you probability so there are all between zero and one

884
00:54:11,390 --> 00:54:15,320
and they will sum up to one because these sets together form the entire space

885
00:54:18,700 --> 00:54:20,370
so that's now are analog of

886
00:54:21,300 --> 00:54:22,360
what we did before so

887
00:54:23,180 --> 00:54:25,010
the intuition that john mentioned this morning is

888
00:54:25,450 --> 00:54:25,910
think of

889
00:54:26,660 --> 00:54:31,620
take a vector and think of estimates vector larger and larger it becomes something like

890
00:54:31,680 --> 00:54:34,970
function right the infinite limit becomes the vector becomes a function

891
00:54:36,320 --> 00:54:38,280
this seriously analog of that we have a finite

892
00:54:38,870 --> 00:54:44,180
probability distribution now we make these this partition into sets we make that finer and

893
00:54:44,180 --> 00:54:46,160
finer makes a set smaller and smaller

894
00:54:46,550 --> 00:54:49,930
you get a larger and larger vector and that you can roughly think of that

895
00:54:50,010 --> 00:54:51,570
is converging to a probability measure

896
00:54:53,550 --> 00:54:53,820
all right

897
00:54:54,890 --> 00:54:55,740
once again if we know

898
00:54:55,740 --> 00:54:58,010
model so the

899
00:54:58,030 --> 00:55:01,950
the expansion coefficients of the

900
00:55:01,960 --> 00:55:04,870
of whatever the basis happens to be

901
00:55:04,880 --> 00:55:07,830
so we want the joint probability

902
00:55:08,840 --> 00:55:13,770
more or less everything is associated with the model so that the probability of

903
00:55:13,780 --> 00:55:15,390
target values

904
00:55:15,400 --> 00:55:19,310
the model parameters given unknowns and

905
00:55:19,320 --> 00:55:24,390
as mikhail sure this graphical models we can read we can we can decompose this

906
00:55:24,390 --> 00:55:26,830
and a number of ways

907
00:55:29,370 --> 00:55:31,320
using those expressions

908
00:55:31,330 --> 00:55:35,380
we can invent those probabilities to obtain

909
00:55:35,390 --> 00:55:37,510
the probability of our

910
00:55:37,530 --> 00:55:42,470
the model parameters given the data and everything else that we know as a function

911
00:55:43,620 --> 00:55:45,570
the likelihood

912
00:55:45,580 --> 00:55:50,450
this probability here which we'll talk about shortly and then this here which is the

913
00:55:50,450 --> 00:55:57,700
marginal probabilities in other words we've taken the likelihood and this prior probability and integrated

914
00:55:57,700 --> 00:56:01,700
over this piece of

915
00:56:01,750 --> 00:56:04,660
values of the support that

916
00:56:04,680 --> 00:56:06,790
as given for

917
00:56:06,810 --> 00:56:09,080
those coefficients

918
00:56:09,080 --> 00:56:12,560
so the only thing then it's really new

919
00:56:12,570 --> 00:56:13,830
is this

920
00:56:13,840 --> 00:56:15,950
distribution here

921
00:56:15,960 --> 00:56:20,040
the prior distribution that's just talk about this a but then

922
00:56:20,090 --> 00:56:24,200
so in the bayesian traditions

923
00:56:24,260 --> 00:56:27,100
the prior probability is the

924
00:56:29,460 --> 00:56:33,310
i the distribution of probability mass

925
00:56:33,330 --> 00:56:34,270
that you

926
00:56:34,750 --> 00:56:35,870
i sign

927
00:56:35,910 --> 00:56:38,160
two your parameters

928
00:56:38,190 --> 00:56:40,850
before you have observed in the data

929
00:56:40,880 --> 00:56:42,790
and so it basically

930
00:56:43,710 --> 00:56:53,100
encapsulates the core defies any prior assumptions of prior says that you half approach model

931
00:56:54,700 --> 00:56:56,870
and this is something that we can

932
00:56:56,890 --> 00:57:01,090
we can say in any way that we choose we can either demonstrate a complete

933
00:57:02,070 --> 00:57:06,940
all of the sort of complexity of our model of the distribution of the parameters

934
00:57:06,940 --> 00:57:13,100
of our model or we may actually have some some idea of what the

935
00:57:13,120 --> 00:57:17,630
the overall distribution is going to be an again you can we can quantify the

936
00:57:17,640 --> 00:57:20,100
and this prior distribution

937
00:57:21,680 --> 00:57:23,710
we met the likelihood before

938
00:57:23,740 --> 00:57:25,900
so that's all fine

939
00:57:25,910 --> 00:57:28,880
no the posterior

940
00:57:30,290 --> 00:57:31,850
of course is obtained

941
00:57:31,890 --> 00:57:35,070
after observing of data

942
00:57:35,080 --> 00:57:38,820
in in this case here

943
00:57:38,830 --> 00:57:40,680
we have

944
00:57:40,700 --> 00:57:47,880
just said that the prior distribution on the weights the coefficients of our

945
00:57:47,890 --> 00:57:51,640
basis functions is independent

946
00:57:51,660 --> 00:57:56,900
clearly of x but is also independent of sigma no

947
00:57:56,950 --> 00:58:04,340
a more realistic modeling situation we would actually probably put a prior distribution jointly on

948
00:58:05,390 --> 00:58:07,630
w and then sigma

949
00:58:08,040 --> 00:58:11,140
but at the moment we are as i said we just assuming that this is

950
00:58:11,140 --> 00:58:14,500
determined and we know what that is

951
00:58:17,140 --> 00:58:22,200
this term here the marginal likelihood as i said is just obtained by integrating over

952
00:58:22,210 --> 00:58:23,370
the support

953
00:58:23,390 --> 00:58:25,390
of the

954
00:58:25,410 --> 00:58:32,040
the the weights and it's it's weighted by the prior probability

955
00:58:32,060 --> 00:58:34,030
and this is

956
00:58:34,130 --> 00:58:40,270
in many interesting models this this is just a measure of great importance but one

957
00:58:40,270 --> 00:58:41,840
which is far from

958
00:58:41,870 --> 00:58:44,750
straightforward to obtain

959
00:58:44,810 --> 00:58:46,400
OK so

960
00:58:46,440 --> 00:58:49,230
i guess this is again some schoolboy

961
00:58:51,130 --> 00:58:53,910
let's just remember that the posterior

962
00:58:53,970 --> 00:58:58,310
is basically the likelihood weighted by the ratio of the priors

963
00:58:58,370 --> 00:58:59,860
and the

964
00:58:59,880 --> 00:59:04,030
overall likelihood of the data of the marginal likelihood

965
00:59:04,050 --> 00:59:06,490
and that

966
00:59:06,560 --> 00:59:08,410
that little formula

967
00:59:08,420 --> 00:59:12,520
apparently it's just about the was in machine learning more or less

968
00:59:12,530 --> 00:59:16,120
so if you remember that you don't remember anything else is

969
00:59:16,270 --> 00:59:18,470
you've learned something right

970
00:59:18,490 --> 00:59:20,510
let's get onto this prior

971
00:59:20,520 --> 00:59:23,640
now as i said we are perfectly free

972
00:59:23,640 --> 00:59:25,810
so we get

973
00:59:25,850 --> 00:59:27,030
OK so

974
00:59:27,050 --> 00:59:33,330
so state of this for a few seconds

975
00:59:33,390 --> 00:59:35,040
a very common form

976
00:59:35,050 --> 00:59:37,280
the unknown voltage

977
00:59:37,350 --> 00:59:40,260
is equal to the stuff on on the right-hand side

978
00:59:40,370 --> 00:59:42,260
the stuff on the right-hand side

979
00:59:44,560 --> 00:59:49,420
i multiplying the source voltage v

980
00:59:49,450 --> 00:59:52,170
and some of the multiplying the

981
00:59:52,240 --> 00:59:54,170
current i

982
00:59:54,200 --> 00:59:55,720
OK and

983
00:59:55,790 --> 00:59:59,800
if i to put distance of the symbolic forms

984
00:59:59,850 --> 01:00:02,620
my unknown or voltage is something

985
01:00:02,670 --> 01:00:04,920
constantine of e one

986
01:00:05,760 --> 01:00:07,540
some constantine

987
01:00:07,540 --> 01:00:15,920
is of the form constantine's

988
01:00:15,930 --> 01:00:18,620
the source current constantine the

989
01:00:18,640 --> 01:00:21,030
source voltage and so on

990
01:00:21,130 --> 01:00:24,930
the other units is these different because here

991
01:00:24,940 --> 01:00:26,750
in this case

992
01:00:26,770 --> 01:00:30,540
and it has no units because these voltage so is e

993
01:00:30,690 --> 01:00:33,910
in this case is b has units of resistance

994
01:00:33,920 --> 01:00:37,820
OK so that the times i give me a voltage

995
01:00:39,960 --> 01:00:42,310
standard this equation for a few seconds

996
01:00:42,330 --> 01:00:47,420
and this should help us build up some insight that will allow us to write

997
01:00:48,820 --> 01:00:51,190
the answer almost by inspection

998
01:00:51,200 --> 01:00:53,040
OK i'm going show you method now

999
01:00:53,050 --> 01:00:57,190
in a few minutes which would allow you to write down the and so you

1000
01:00:57,200 --> 01:01:01,570
just by staring at that so could not have to go through node equations one

1001
01:01:01,630 --> 01:01:04,680
and the more and more about that issue the more you will be able to

1002
01:01:04,680 --> 01:01:07,600
do a lot of this completely by yourself

1003
01:01:07,650 --> 01:01:13,170
in this particular example simple circuits but these methods would be particularly useful when you

1004
01:01:13,170 --> 01:01:17,160
have more complicated situations

1005
01:01:17,940 --> 01:01:23,390
before i go on the media

1006
01:01:23,400 --> 01:01:26,300
spend a few minutes pontificating on

1007
01:01:28,250 --> 01:01:31,550
so that's a linear circuit and this equation

1008
01:01:31,570 --> 01:01:33,590
is made on one or more pitch e

1009
01:01:33,680 --> 01:01:40,680
as a linear sum of source voltages and source currents

1010
01:01:42,920 --> 01:01:45,660
implies two properties

1011
01:01:45,750 --> 01:01:50,260
the property

1012
01:01:51,930 --> 01:01:58,450
and also give rise to the properties of the position

1013
01:01:58,540 --> 01:02:02,040
let's two hundred eighty first

1014
01:02:02,120 --> 01:02:07,920
so what it says

1015
01:02:08,970 --> 01:02:12,710
if the circuit

1016
01:02:12,770 --> 01:02:14,170
some circuit

1017
01:02:15,420 --> 01:02:21,580
i feel some sort of inputs

1018
01:02:23,830 --> 01:02:27,800
let's say the output is as

1019
01:02:27,840 --> 01:02:30,330
if you're feeling hungry think of these as

1020
01:02:30,340 --> 01:02:34,190
apples and the circuit converts them into applesauce

1021
01:02:35,940 --> 01:02:37,660
so what i we he says

1022
01:02:37,680 --> 01:02:39,250
is that

1023
01:02:39,300 --> 01:02:40,390
what i can do

1024
01:02:40,400 --> 01:02:43,150
is if i take to my apples

1025
01:02:43,180 --> 01:02:45,170
and in the feeding into apple

1026
01:02:45,170 --> 01:02:48,720
or if i give it three quarters of an apple

1027
01:02:48,780 --> 01:02:50,570
so multiply

1028
01:02:50,630 --> 01:02:53,330
all my input by some constant alpha

1029
01:02:53,380 --> 01:02:54,570
three quarters

1030
01:02:54,630 --> 01:02:58,940
what that says is that the outputs of getting one full model of applesauce and

1031
01:02:58,940 --> 01:03:00,100
we get

1032
01:03:00,190 --> 01:03:03,320
three quarters of the model that was source

1033
01:03:03,380 --> 01:03:07,720
OK so if i proportionally reduced all the inputs and this is a linear circuit

1034
01:03:07,720 --> 01:03:10,890
then social my output it using the same

1035
01:03:14,260 --> 01:03:15,800
and what

1036
01:03:15,810 --> 01:03:23,300
next let's let's look at super position

1037
01:03:23,370 --> 01:03:32,140
the values of position said the following

1038
01:03:32,180 --> 01:03:33,770
synchronous circuit

1039
01:03:33,810 --> 01:03:36,120
few apples

1040
01:03:36,140 --> 01:03:37,600
and they get

1041
01:03:37,660 --> 01:03:41,510
apple sauce

1042
01:03:42,240 --> 01:03:43,910
i think the same circuit

1043
01:03:45,400 --> 01:03:47,810
i think the same circuit

1044
01:03:47,820 --> 01:03:52,550
and this terminology for feed the circuit in a different set of inputs

1045
01:03:56,530 --> 01:03:58,320
this is very

1046
01:03:58,420 --> 01:04:01,500
and let's say

1047
01:04:01,510 --> 01:04:06,170
my output of it this way

1048
01:04:06,310 --> 01:04:11,070
so output i get blueberry sauce such exist

1049
01:04:11,090 --> 01:04:13,790
sample that doesn't really give very sparse

1050
01:04:14,770 --> 01:04:20,300
what i want to get if i mixed up the two

1051
01:04:20,320 --> 01:04:23,520
so let's take my circuit the same circuit

1052
01:04:23,520 --> 01:04:27,500
that's the say that the effective charge is growing as a function of distance just

1053
01:04:30,380 --> 01:04:33,210
so we have this charge it's building up and building up

1054
01:04:33,230 --> 01:04:38,210
and of course that's a runaway process because the things that make the charger interns

1055
01:04:38,210 --> 01:04:42,420
quarks and gluons virtual quarks and gluons they generate their own fields and so we

1056
01:04:42,420 --> 01:04:49,250
have the charts building up and building up and making a sort of away

1057
01:04:49,290 --> 01:04:54,230
so we have charges of course then we have fields associated with that and fields

1058
01:04:54,230 --> 01:04:55,650
have energy

1059
01:04:55,650 --> 01:05:00,460
so we're building up the configuration from our little tiny seed that carries more and

1060
01:05:00,460 --> 01:05:04,350
more energy we go to larger and larger distances

1061
01:05:04,400 --> 01:05:08,100
and in fact it's runaway process is nothing to stop it so the energy just

1062
01:05:08,100 --> 01:05:09,900
becomes infinite

1063
01:05:11,870 --> 01:05:13,040
that would provoke

1064
01:05:13,040 --> 01:05:15,150
a severe energy crisis

1065
01:05:15,190 --> 01:05:17,690
the feed single quark

1066
01:05:17,710 --> 01:05:20,190
and if it really happen that way

1067
01:05:20,340 --> 01:05:23,400
it would be very dangerous to turn on the machine like lap

1068
01:05:23,400 --> 01:05:26,420
but mother nature has a way of solving that problem

1069
01:05:26,440 --> 01:05:29,920
that's more parsimonious

1070
01:05:29,940 --> 01:05:32,640
that is

1071
01:05:32,790 --> 01:05:38,350
instead of building up this enormous clout this enormous thunderclouds just as if you in

1072
01:05:38,350 --> 01:05:42,230
the weather if you try to build up an enormous thunderclouds starts to discharge because

1073
01:05:42,230 --> 01:05:43,290
it's cheaper

1074
01:05:44,540 --> 01:05:49,940
to to discharge the iron spring positive and negative charges together

1075
01:05:51,020 --> 01:05:56,420
no the nature's response to trying to put a cork in empty space by itself

1076
01:05:56,480 --> 01:05:57,710
is to produce

1077
01:05:57,710 --> 01:06:00,500
quark antiquark pair sparks if you like

1078
01:06:01,670 --> 01:06:05,330
i have one of those and i quarks with the opposite charge sit right on

1079
01:06:06,290 --> 01:06:07,170
the charge

1080
01:06:07,210 --> 01:06:10,690
if it if it's right on top of the original work that you put in

1081
01:06:10,730 --> 01:06:13,190
that we can cancel of the field entirely

1082
01:06:13,230 --> 01:06:16,400
so that's the way you could get rid of all the energy of this

1083
01:06:18,440 --> 01:06:22,940
however there is a price to be paid

1084
01:06:22,960 --> 01:06:24,540
in quantum mechanics

1085
01:06:24,560 --> 01:06:27,350
it costs energy to localize particles

1086
01:06:27,400 --> 01:06:32,250
so to localize charged particles have wave character and if you try to localize the

1087
01:06:32,250 --> 01:06:35,380
position very precisely

1088
01:06:35,400 --> 01:06:39,580
the waves don't want to do that and that caused a lot of energy to

1089
01:06:40,790 --> 01:06:43,380
if you try to put it that i quark

1090
01:06:43,400 --> 01:06:46,230
right on top of the court to cancel its charge

1091
01:06:46,270 --> 01:06:52,500
that would be persist very infinitely precise localisation that will also cause an infinite amount

1092
01:06:52,500 --> 01:06:53,650
of energy

1093
01:06:53,650 --> 01:06:57,600
one way to think about this is in terms of heisenberg's uncertainty principle

1094
01:06:57,600 --> 01:07:03,690
if you try to localize something within a given positional uncertainty its momentum gets correspondingly

1095
01:07:06,380 --> 01:07:10,830
and of course momentum transfer large momentum transfers into

1096
01:07:10,850 --> 01:07:16,620
corresponds to large energy so it costs energy to localize particles according to heisenberg's uncertainty

1097
01:07:18,540 --> 01:07:20,560
so there's competition

1098
01:07:20,620 --> 01:07:22,580
this is on the one hand

1099
01:07:22,710 --> 01:07:27,480
to discharge this thunder cloud of growing effective charge and fields

1100
01:07:27,580 --> 01:07:29,020
you want to

1101
01:07:29,080 --> 01:07:31,850
cancel off

1102
01:07:32,370 --> 01:07:34,080
charge of the quark

1103
01:07:34,140 --> 01:07:35,690
go on

1104
01:07:35,710 --> 01:07:39,480
on the other hand if you try to cancel off exactly this the price to

1105
01:07:39,480 --> 01:07:41,620
be paid

1106
01:07:41,670 --> 01:07:44,310
so there has to be some sort of compromise

1107
01:07:44,350 --> 01:07:50,290
and in fact there are many many scores stable part compromises

1108
01:07:50,330 --> 01:07:52,230
this may be reminiscent to you

1109
01:07:52,230 --> 01:07:56,480
if you thought about the hydrogen atom and how it gets stable in quantum mechanics

1110
01:07:56,540 --> 01:07:59,770
the way the hydrogen atom gets stable in quantum mechanics is

1111
01:07:59,810 --> 01:08:00,750
you have

1112
01:08:00,750 --> 01:08:04,400
the coulomb field of the proton which has a lot of field energy

1113
01:08:04,580 --> 01:08:08,310
and you have an electron electron electron around it would like to sit right on

1114
01:08:08,310 --> 01:08:12,650
top of the proton ideally to get rid of all that energy quantum mechanics was

1115
01:08:12,650 --> 01:08:14,750
invented in fact in some ways

1116
01:08:16,100 --> 01:08:19,540
keep it from doing that and it is precisely this way

1117
01:08:19,540 --> 01:08:23,920
in this way that you get stable hydrogen atoms and in fact there are number

1118
01:08:23,920 --> 01:08:30,830
of stable compromises and those are the different energy levels of hydrogen

1119
01:08:30,870 --> 01:08:33,940
similarly here

1120
01:08:33,940 --> 01:08:37,920
we can have different sorts of compromises

1121
01:08:37,940 --> 01:08:42,560
those are different stable compromises of ways of putting quarks and antique works together or

1122
01:08:42,560 --> 01:08:47,150
it turns out you can also neutralize the charge by having three quarks

1123
01:08:47,170 --> 01:08:50,770
and these different stable patterns of equilibrium these differences

1124
01:08:50,940 --> 01:08:58,040
compromises are what makes protons neutrons and the other strongly interacting particles

1125
01:08:58,080 --> 01:09:02,960
when einstein learned about wars theory of hydrogen he equal the highest form of musicality

1126
01:09:02,960 --> 01:09:05,250
in the sphere of thought

1127
01:09:05,290 --> 01:09:08,460
let me remind you that bores original theory was

1128
01:09:08,480 --> 01:09:11,270
sort of the solar system and i presume he was

1129
01:09:11,290 --> 01:09:15,310
harking back to have kept his idea of music of the spheres

1130
01:09:15,350 --> 01:09:19,650
he never accepted it but real quantum mechanics is much more musical

1131
01:09:19,650 --> 01:09:26,620
real quantum mechanics uses the mathematics of musical instruments vibrating waves in stationary wave patterns

1132
01:09:27,370 --> 01:09:29,600
the mathematics of QCD

1133
01:09:31,150 --> 01:09:35,440
the mathematics of chords because we have three different colors or oscillating at the same

1134
01:09:37,080 --> 01:09:42,460
anyway these different stable patterns which if we could visualize them with look extraordinarily intricate

1135
01:09:42,480 --> 01:09:43,640
and beautiful

1136
01:09:43,650 --> 01:09:46,750
are nothing but the different particles

1137
01:09:46,810 --> 01:09:48,020
these different to

1138
01:09:48,230 --> 01:09:52,040
compromises between localisation energy

1139
01:09:52,120 --> 01:09:54,870
course from quantum mechanics and

1140
01:09:54,920 --> 01:09:56,440
field builds up

1141
01:09:57,500 --> 01:09:59,290
color field

1142
01:09:59,330 --> 01:10:01,440
college color charge

1143
01:10:01,540 --> 01:10:05,620
now because each of these compromises is not perfect

1144
01:10:05,620 --> 01:10:08,350
doesn't cancel field energy exactly

1145
01:10:08,370 --> 01:10:15,500
four and doesn't have zero quantum mechanical energy each one has some characteristic energy

1146
01:10:17,330 --> 01:10:19,650
einstein second law tells us then

1147
01:10:19,650 --> 01:10:26,540
through m equals the divided by c squared that have the corresponding mass even if

1148
01:10:26,600 --> 01:10:32,710
the underlying stuff the quarks and gluons had no mess to begin with

1149
01:10:32,730 --> 01:10:37,310
that's the origin of mass

1150
01:10:37,330 --> 01:10:41,230
as martha stewart might say it's a good thing

1151
01:10:41,290 --> 01:10:44,290
to get rid of mass

1152
01:10:44,350 --> 01:10:48,600
because removing it enhances the symmetry of our equation

1153
01:10:48,650 --> 01:10:50,310
already discussed feel

1154
01:10:51,480 --> 01:10:56,290
the gluons absolutely have to have zero mass to ensure the underlying symmetry gauge their

1155
01:10:56,290 --> 01:10:58,150
symmetry of QCD

1156
01:10:58,420 --> 01:11:02,920
removing the and quite masses also gives

1157
01:11:03,000 --> 01:11:06,060
as in QCD light enhances the symmetry of that theory

1158
01:11:06,100 --> 01:11:09,650
it gives us the so-called chiral theory

1159
01:11:09,650 --> 01:11:13,710
and more generally and i theories of physics and the standard model we understand things

1160
01:11:13,710 --> 01:11:16,650
much much better if necessary here

1161
01:11:17,830 --> 01:11:21,080
mass is somewhat of an embarrassment so the fact that we can and should do

1162
01:11:21,080 --> 01:11:25,150
bed times this term t minus to so if you're role that's

1163
01:11:25,310 --> 01:11:28,760
then you effectively get a big sum of terms

1164
01:11:28,940 --> 01:11:34,170
where the the weight is decreasing exponentially fast

1165
01:11:34,320 --> 01:11:38,180
so that's why i on yes

1166
01:11:51,580 --> 01:11:56,110
so the question is i guess why summing the gradients is that of

1167
01:11:56,190 --> 01:11:59,970
average sorry why averaging great is then summing

1168
01:12:00,120 --> 01:12:04,120
and his relationship between using mini-batches and the optimal

1169
01:12:04,130 --> 01:12:08,920
learning rate so so average motivation is to think of it as you're

1170
01:12:08,930 --> 01:12:12,620
trying to get an estimate of the expected gradient on the training

1171
01:12:12,630 --> 01:12:16,450
distribution so it's just a monte-carlo estimate essentially

1172
01:12:16,810 --> 01:12:21,790
and indeed if you have more examples mini-batch can think that

1173
01:12:21,790 --> 01:12:25,270
your gradient is more accurate there's as variance in your estimate

1174
01:12:25,270 --> 01:12:27,770
so you might be able to move faster so be

1175
01:12:28,340 --> 01:12:31,390
maybe able to make faster or bigger steps

1176
01:12:32,270 --> 01:12:35,400
and there's probably you can probably construct cases where

1177
01:12:35,400 --> 01:12:38,130
you can sort of make a recommendation as to what is the

1178
01:12:38,130 --> 01:12:41,170
relationship between a multiplier on the learning rate in

1179
01:12:41,180 --> 01:12:45,510
the number of examples i i i actually not sure what it is but

1180
01:12:46,760 --> 01:12:50,530
ok i'm going move on yeah for the interest of time

1181
01:12:51,120 --> 01:12:55,800
because i know enclosed finishing so

1182
01:12:56,600 --> 01:13:00,570
there are other more sophisticated optimizes out there

1183
01:13:00,690 --> 01:13:04,330
that you might read up in papers and that can also help you

1184
01:13:04,340 --> 01:13:08,480
if you have you know neural net that seems to be harder to optimize

1185
01:13:09,710 --> 01:13:14,580
one is adagrad so thickly all these you is using a form of adaptive

1186
01:13:14,590 --> 01:13:16,800
learning rate that is for each parameter

1187
01:13:17,090 --> 01:13:20,950
effectively have their own learning rate that is changing during

1188
01:13:20,960 --> 01:13:29,120
training so one so adagrad the way it's scales different learning

1189
01:13:29,120 --> 01:13:31,980
rate so here instead of showing that scaling the learning rate i'm

1190
01:13:31,980 --> 01:13:35,950
just going to scale the gradient ok but that's the same thing

1191
01:13:36,480 --> 01:13:39,880
and in this case adagrad would you do you compute

1192
01:13:40,230 --> 01:13:44,190
the cumulative sum of the square of the gradient

1193
01:13:44,400 --> 01:13:48,820
on your parameters so this is this squares done element wise over

1194
01:13:48,830 --> 01:13:53,260
this gradient vector and so what i just do is i keep this gamma

1195
01:13:53,270 --> 01:13:57,240
variable where in which i accumulate the square of the gradient

1196
01:13:57,250 --> 01:14:00,420
on my parameters and then what i do that my a

1197
01:14:00,680 --> 01:14:04,870
descent direction i will divided element-wise by the square

1198
01:14:04,880 --> 01:14:08,220
root element-wise over this gamma term

1199
01:14:08,450 --> 01:14:10,670
thus maps just to make sure that this term

1200
01:14:11,030 --> 01:14:13,880
for numerical stability in case this term is too small

1201
01:14:14,690 --> 01:14:18,370
so in this case because you keep summing positive terms because

1202
01:14:18,370 --> 01:14:21,070
these are squared the learning rate is always decreasing

1203
01:14:21,070 --> 01:14:25,480
that graham for a convex optimization problem that's

1204
01:14:25,720 --> 01:14:28,970
fine yeah if it's non-convex can construct cases where if you

1205
01:14:28,970 --> 01:14:32,190
learning rate decreases to much eventually might read something like

1206
01:14:32,190 --> 01:14:35,370
a slight plateau and in that case you might want to actually increase

1207
01:14:35,370 --> 01:14:38,590
the learning rate so it might make more sense to use something

1208
01:14:38,590 --> 01:14:43,560
like this prop which is instead of using derivative some it's

1209
01:14:44,040 --> 01:14:47,160
exponential moving average fuzzy this is more like a moving

1210
01:14:47,160 --> 01:14:50,090
average because you always have like an average between this term

1211
01:14:50,090 --> 01:14:55,160
and that term but anyways there's exponentially decreasing

1212
01:14:55,430 --> 01:14:58,700
contribution of the squared gradient terms

1213
01:14:58,940 --> 01:15:02,280
in this case here so it could be that in this this term here is

1214
01:15:02,290 --> 01:15:06,130
not selly i'm always decreasing with steps as

1215
01:15:06,430 --> 01:15:10,380
with the number of steps and then finally one that's quite popular

1216
01:15:10,390 --> 01:15:14,350
right now as adam you can look it up if you want a full description

1217
01:15:14,820 --> 01:15:17,910
in short it's rmsprop with some momentum term

1218
01:15:18,250 --> 01:15:23,570
and it seems to work pretty well like some people just only use

1219
01:15:23,580 --> 01:15:27,600
this instead of some you know manual schedule over the learning

1220
01:15:27,610 --> 01:15:32,770
rate and momentum and i wouldn't re-size making

1221
01:15:32,970 --> 01:15:35,140
absolute claims about which one is better

1222
01:15:35,240 --> 01:15:38,680
if you're a situation feel like you're not the model doesn't seem

1223
01:15:38,680 --> 01:15:42,330
to be training well changing the optimizers def something to consider

1224
01:15:44,920 --> 01:15:49,280
so and funny just about debugging so imagine you've implemented

1225
01:15:49,290 --> 01:15:52,010
neural net it's a new type of neural nets unit

1226
01:15:52,010 --> 01:15:55,080
you know you've implemented some of the backprop yourself some

1227
01:15:55,080 --> 01:15:57,220
of the gradients for thing you should do

1228
01:15:57,440 --> 01:16:01,410
the only at test whether you're computations of the gradients are

1229
01:16:01,420 --> 01:16:04,540
correct one way to do that is with find a difference

1230
01:16:04,830 --> 01:16:07,850
approximation comparing that with your computer gradients

1231
01:16:07,850 --> 01:16:12,590
geographical are so landscape and no

1232
01:16:12,610 --> 01:16:15,330
topics which are more

1233
01:16:15,330 --> 01:16:21,980
relevant or events are have been more documents inside it would be represented mountains higher

1234
01:16:22,860 --> 01:16:27,990
well the less relevant topics would be smaller smaller mountains here c

1235
01:16:28,640 --> 01:16:30,380
is the biggest mountain is

1236
01:16:30,400 --> 01:16:31,480
this one

1237
01:16:31,490 --> 01:16:34,080
which is described with

1238
01:16:34,090 --> 01:16:42,080
typical keywords so this would be israel israeli-palestinian so three three keywords which would

1239
01:16:42,410 --> 01:16:45,690
corresponds roughly to this topic well it's a here would be

1240
01:16:46,580 --> 01:16:49,270
actions you gonna or chania

1241
01:16:49,280 --> 01:16:50,960
this is still

1242
01:16:50,990 --> 01:16:55,410
what's most this was the war in bosnia at the time and so on and

1243
01:16:55,440 --> 01:16:56,320
you can

1244
01:16:56,350 --> 01:16:57,790
various topics

1245
01:16:57,810 --> 01:17:02,150
there are going see live demo of

1246
01:17:02,160 --> 01:17:07,140
somewhat improved version of so this is one typical example where you see

1247
01:17:07,950 --> 01:17:09,750
each document here

1248
01:17:09,760 --> 01:17:13,760
which you can see exactly but each document is one dots one

1249
01:17:13,890 --> 01:17:15,770
but in this map

1250
01:17:15,830 --> 01:17:21,870
and it's a document is basically presented in two two dimensions although the document itself

1251
01:17:21,870 --> 01:17:28,990
in original form is presented in much higher dimensional space maybe ten or hundred dollars

1252
01:17:33,820 --> 01:17:36,590
again very classical example

1253
01:17:36,620 --> 01:17:39,310
from the literature would be

1254
01:17:39,320 --> 01:17:46,190
this picture which was most probably was made manually are most mostly manually so this

1255
01:17:46,190 --> 01:17:47,750
is analysis of

1256
01:17:47,760 --> 01:17:52,140
fidel castro speeches from december nineteen sixty two

1257
01:17:52,170 --> 01:17:54,180
june sixteen one

1258
01:17:54,200 --> 01:17:57,300
and it's

1259
01:17:57,310 --> 01:18:00,340
betty castor was known to

1260
01:18:00,350 --> 01:18:06,080
or that he was not able to heights much

1261
01:18:08,590 --> 01:18:15,650
five of political guys from the time i think they all improvising mostly and of

1262
01:18:15,650 --> 01:18:20,750
course if he was met then this obviously his speech was also

1263
01:18:20,790 --> 01:18:24,640
he expressed his madness in one of the other way

1264
01:18:24,670 --> 01:18:30,360
and if you analyse this speeches then you will they so what he thought this

1265
01:18:35,230 --> 01:18:37,130
so this is the time period of this

1266
01:18:39,120 --> 01:18:43,130
crisis this rocket crisis in cuba

1267
01:18:43,150 --> 01:18:45,000
and you can see

1268
01:18:45,000 --> 01:18:47,320
how different words

1269
01:18:47,330 --> 01:18:50,570
we are different topics were present in particular

1270
01:18:50,600 --> 01:18:54,570
moments of time and by the smallest u

1271
01:18:54,630 --> 01:18:56,630
one from the american and

1272
01:18:59,340 --> 01:19:03,280
able to to reconstruct basically what's going on in the head of costs

1273
01:19:03,300 --> 01:19:08,080
later on again would see this time time analysis of texts in a little bit

1274
01:19:08,080 --> 01:19:10,180
different form

1275
01:19:11,430 --> 01:19:18,950
next musician which i can show i think life is going to com this physician

1276
01:19:21,060 --> 01:19:25,460
results this is again the same pattern

1277
01:19:25,520 --> 01:19:27,820
nobody's going to

1278
01:19:27,860 --> 01:19:30,180
it's not OK

1279
01:19:41,670 --> 01:19:42,940
it's a

1280
01:19:42,960 --> 01:19:46,590
this image is

1281
01:19:46,600 --> 01:19:47,750
search query

1282
01:19:47,880 --> 01:19:50,740
and so what

1283
01:19:50,760 --> 01:19:54,800
the system does it it's kind of metasearch engine

1284
01:19:54,810 --> 01:19:58,150
it issues this query two

1285
01:19:58,180 --> 01:20:03,130
one or several other search engines collect the results

1286
01:20:03,140 --> 01:20:04,410
it's a

1287
01:20:04,460 --> 01:20:08,700
class that performs clustering kind of published results in an visualizes

1288
01:20:08,710 --> 01:20:10,560
these different concepts

1289
01:20:12,840 --> 01:20:17,390
so this would be the main keywords words which appear as part of

1290
01:20:18,280 --> 01:20:20,000
and this would be

1291
01:20:20,070 --> 01:20:23,980
well the main concepts like

1292
01:20:24,030 --> 01:20:25,870
book hotel travel

1293
01:20:25,900 --> 01:20:28,850
time well it might not be

1294
01:20:28,890 --> 01:20:32,590
relevant or maybe a generic

1295
01:20:32,610 --> 01:20:35,160
what would be

1296
01:20:39,990 --> 01:20:42,000
so the

1297
01:20:44,440 --> 01:20:45,570
well they deserve

1298
01:20:45,590 --> 01:20:48,200
all this generic stuff

1299
01:20:48,230 --> 01:20:50,450
try that's fine

1300
01:20:50,480 --> 01:21:01,760
that's our collaborators OK what this

1301
01:21:01,800 --> 01:21:05,960
keyword that networks and collaborative

1302
01:21:05,980 --> 01:21:08,570
training consists of

1303
01:21:08,650 --> 01:21:13,620
you can you

1304
01:21:13,630 --> 01:21:18,300
when and and now if the click

1305
01:21:18,350 --> 01:21:19,900
european then OK

1306
01:21:19,930 --> 01:21:21,330
twenty two

1307
01:21:21,340 --> 01:21:26,710
and then you get slightly different so this is all automatically generated

1308
01:21:26,730 --> 01:21:28,890
sort of two-dimensional graph

1309
01:21:28,990 --> 01:21:31,960
of the terms which were found in

1310
01:21:33,700 --> 01:21:37,540
you know results which we

1311
01:21:37,570 --> 01:21:39,130
bishop that the

1312
01:21:39,150 --> 01:21:41,820
search engine produced sometimes

1313
01:21:41,830 --> 01:21:45,760
it can be quite informative sometimes not too much

1314
01:21:48,830 --> 01:21:54,250
you can recognise some some terms but

1315
01:21:54,260 --> 01:21:59,980
but we are showing this is an example of visualisation of search results

1316
01:22:04,150 --> 01:22:06,850
searchpoint so this is

1317
01:22:07,090 --> 01:22:10,330
one of them which we our guys produced

1318
01:22:15,060 --> 01:22:18,270
and it's again similar type for

1319
01:22:18,450 --> 01:22:21,930
musician accepted the idea is slightly different

1320
01:22:22,850 --> 01:22:25,310
so this looks like

1321
01:22:25,310 --> 01:22:31,610
the search but it's not just the muscor google search so now if we

1322
01:22:31,670 --> 01:22:34,460
five no

1323
01:22:34,520 --> 01:22:41,230
it is a key or some such query past

1324
01:22:41,230 --> 01:22:46,020
four search and then we get classical google results as

1325
01:22:46,020 --> 01:22:53,680
and this is guaranteed to converge number in a finite number

1326
01:22:53,690 --> 01:22:57,040
that is called a column generation one cutting plane

1327
01:22:57,050 --> 01:23:00,490
i think alex smola about it

1328
01:23:03,220 --> 01:23:05,470
this is but

1329
01:23:05,580 --> 01:23:13,820
i would like to use this to have to ship short one is shogun toolbox

1330
01:23:13,930 --> 01:23:18,930
machine learning toolbox implements most of the string kernels and also

1331
01:23:18,940 --> 01:23:20,600
includes like

1332
01:23:20,670 --> 01:23:23,730
implementations of thirty different kernels

1333
01:23:23,780 --> 01:23:28,810
which are based on a mission to the on the efficient string data structures which

1334
01:23:28,810 --> 01:23:35,730
are described can handle different data types can i think there seven different implementations in

1335
01:23:35,730 --> 01:23:41,350
that can just used from mid level take what i think are written in c

1336
01:23:41,350 --> 01:23:45,500
and t plus but you can use it for any of these programming languages for

1337
01:23:45,590 --> 01:23:47,880
interface functions to it

1338
01:23:47,890 --> 01:23:52,340
and it's available under GPL license

1339
01:23:52,350 --> 01:23:59,640
other advertisements so if you're self interested in writing open source of than this

1340
01:23:59,650 --> 01:24:05,180
special topic in JMLR might be interesting to you so it has started just a

1341
01:24:05,180 --> 01:24:06,100
few weeks ago

1342
01:24:08,010 --> 01:24:11,180
c'mon now supports open source

1343
01:24:11,190 --> 01:24:14,800
the open source movement and they are are going to have an open source

1344
01:24:14,840 --> 01:24:21,010
special topics so we can submit papers which are just all pages which are description

1345
01:24:21,030 --> 01:24:22,920
of software package

1346
01:24:22,940 --> 01:24:30,010
this could be like implementations of machine learning algorithms for machine learning toolboxes the sherwood

1347
01:24:30,820 --> 01:24:32,930
or languages

1348
01:24:33,890 --> 01:24:37,940
so and you find this information here

1349
01:24:41,980 --> 01:24:42,870
OK so

1350
01:24:43,150 --> 01:24:46,350
it's not

1351
01:24:47,450 --> 01:24:49,750
main topics

1352
01:24:49,770 --> 01:24:54,820
so you don't you know i'm going many going to talk about the applications

1353
01:24:54,840 --> 01:25:00,440
so this one is going to to be an alignment where we use structured output

1354
01:25:00,440 --> 01:25:01,660
learning for

1355
01:25:01,710 --> 01:25:06,680
coming up with a better alignment

1356
01:25:06,940 --> 01:25:08,260
on the

1357
01:25:08,310 --> 01:25:18,020
OK so this is a joint venture with two the students of mind changes when

1358
01:25:18,020 --> 01:25:21,640
i posted

1359
01:25:21,660 --> 01:25:26,940
OK so the most abundant kind of data which we have computational biology are seeking

1360
01:25:26,940 --> 01:25:32,820
so and that the genomic sequences but we also have a sequence parts of them

1361
01:25:32,830 --> 01:25:34,130
on so

1362
01:25:35,660 --> 01:25:41,570
called used t expressed sequence tags so this is usually just a small part of

1363
01:25:41,570 --> 01:25:49,640
an so you know from within a the transcription and then employ explain

1364
01:25:49,660 --> 01:25:53,540
used is are small facet of the model

1365
01:25:53,560 --> 01:25:56,540
now given these MRI can now

1366
01:25:56,680 --> 01:25:59,730
tried to make peace on its back to within

1367
01:25:59,740 --> 01:26:03,360
to try to find out where the exons the

1368
01:26:05,240 --> 01:26:09,150
we try to reconstruct these get here

1369
01:26:09,170 --> 01:26:11,820
so we can do this

1370
01:26:12,210 --> 01:26:16,450
in order to discover new genes so because we have sequences of on a story

1371
01:26:16,450 --> 01:26:19,430
by mapping back we can find new genes

1372
01:26:19,550 --> 01:26:21,010
we can use

1373
01:26:21,030 --> 01:26:26,900
we can find the exonintron boundaries you can find alternative splicing in several variants of

1374
01:26:28,070 --> 01:26:32,810
and sometimes you also used for finding affordable

1375
01:26:32,880 --> 01:26:38,130
the problems are that sometimes the genomic sequence is not unique so there might be

1376
01:26:38,130 --> 01:26:41,470
several places where people on a committee

1377
01:26:42,380 --> 01:26:49,040
then of course you decide where we would like to align so then they are

1378
01:26:49,040 --> 01:26:52,000
far looks easily duplicated genes or

1379
01:26:52,010 --> 01:26:56,910
pseudo genes which aren't active anymore and you have you know

1380
01:26:56,920 --> 01:27:03,200
and of course that sequencing errors polymorphism select changes in the genome and there are

1381
01:27:03,200 --> 01:27:09,440
some rare cases of canonical life science and another problem in x i really really

1382
01:27:09,440 --> 01:27:17,760
short just like five nokia something that it's really hard to unambiguously match behind these

1383
01:27:17,770 --> 01:27:22,550
and i'm going to talk about this a bit later

1384
01:27:22,600 --> 01:27:24,680
so the problem is given

1385
01:27:27,330 --> 01:27:28,370
there has been

1386
01:27:28,710 --> 01:27:32,880
get more than ten years of research on how to do this kind of alignments

1387
01:27:33,240 --> 01:27:37,780
and i just want to mention a few of the approaches here some of them

1388
01:27:37,780 --> 01:27:43,590
are just really likes him for blocked use kind

1389
01:27:43,640 --> 01:27:50,620
c algorithms so they start off looking for all substrings what certain substring of a

1390
01:27:50,620 --> 01:27:55,110
certain length and up in the morning try to find it in the DNA

1391
01:27:55,160 --> 01:27:59,410
and once we find a match which is long and they tried to extend the

1392
01:28:00,490 --> 01:28:02,300
and come up with some aligned

1393
01:28:02,360 --> 01:28:04,500
this is like you like

1394
01:28:04,710 --> 01:28:06,520
but there are few other

1395
01:28:06,540 --> 01:28:11,730
so the problem with these tools is that we usually only look at the the

1396
01:28:11,730 --> 01:28:13,000
alignment i mean

1397
01:28:13,060 --> 01:28:16,050
how good the sequence is actually found in between

1398
01:28:16,670 --> 01:28:21,320
and what we would like to do is we would like to integrate additional information

1399
01:28:21,910 --> 01:28:25,060
information about splicing into these line

1400
01:28:25,070 --> 01:28:26,550
OK because you know

1401
01:28:26,570 --> 01:28:29,680
not only the secret

1402
01:28:29,690 --> 01:28:33,920
but also that my side and at the boundary between exonintron so those should also

1403
01:28:33,920 --> 01:28:35,940
be showing us here

1404
01:28:35,950 --> 01:28:40,690
so there some additional information and use it for line

1405
01:28:42,900 --> 01:28:47,990
so users by science we also used some

1406
01:28:48,000 --> 01:28:51,700
link model for the entrance entrance should have the

1407
01:28:51,720 --> 01:28:57,750
and we impose this using structured output learning somewhat principle combination of these different sources

1408
01:28:57,750 --> 01:29:02,950
of information

1409
01:29:02,960 --> 01:29:09,220
OK so i talked about this two class sort mission use the machines but this

1410
01:29:09,240 --> 01:29:15,770
and so how can we use this kind of information that we first describe typical

1411
01:29:15,770 --> 01:29:18,530
i mean the typical alignment

1412
01:29:18,540 --> 01:29:20,980
so you usually have to stick it

1413
01:29:20,980 --> 01:29:26,270
so in our case is the DNA sequence and use to the east i think

1414
01:29:26,400 --> 01:29:28,290
this part of mind

1415
01:29:28,310 --> 01:29:30,920
so we have some kind of substitution matrix

1416
01:29:31,260 --> 01:29:37,670
there the score for every pair of looking up to him

1417
01:29:37,700 --> 01:29:40,990
in alignment is essentially

1418
01:29:41,010 --> 01:29:44,020
sequence alignment sequence of here

1419
01:29:44,030 --> 01:29:45,130
of let

1420
01:29:45,160 --> 01:29:48,570
o which comes from fifteen and from the east

1421
01:29:48,570 --> 01:29:51,800
OK so you can just

1422
01:29:55,180 --> 01:29:58,120
we have two sequences a eighty

1423
01:29:58,180 --> 01:30:00,110
a and

1424
01:30:00,120 --> 01:30:05,430
a g a then maybe he

1425
01:30:09,360 --> 01:30:12,100
he so you see this

1426
01:30:12,110 --> 01:30:13,440
this built up here

1427
01:30:13,460 --> 01:30:14,460
this another here

1428
01:30:14,470 --> 01:30:16,940
this is here but it's not matching

1429
01:30:16,950 --> 01:30:19,940
so we use the

1430
01:30:20,040 --> 01:30:22,190
metrics and

1431
01:30:22,210 --> 01:30:24,170
the score of can

1432
01:30:27,800 --> 01:30:30,430
and you see this actually matters

1433
01:30:30,450 --> 01:30:33,130
so actually there to get in between

1434
01:30:33,170 --> 01:30:37,160
and with these gaps we also scoring

1435
01:30:37,160 --> 01:30:41,800
thank you very much and thank all of you coming on as he sat on

1436
01:30:41,800 --> 01:30:47,060
my Culver with Amazon Web Services and act before argued started

1437
01:30:47,070 --> 01:30:51,910
1st full apologies that I cannot speak English but up before it started how many

1438
01:30:51,910 --> 01:30:55,530
people in the room had heard of Amazon Web Services before they heard about this

1439
01:30:57,810 --> 01:30:59,930
updates coming you tried

1440
01:31:00,030 --> 01:31:05,850
but now that's that's about what they so what I'm going to do it this

1441
01:31:05,850 --> 01:31:10,730
sounds agreeable to everybody is amended kind of divide this talk into 3 parts

1442
01:31:11,030 --> 01:31:14,190
I'm going to start by talking a little bit about what it means to be

1443
01:31:14,190 --> 01:31:19,570
successful as a start-up because there's a lot told me that there's a number of

1444
01:31:19,570 --> 01:31:20,550
books and this

1445
01:31:20,590 --> 01:31:24,470
community that are thinking about start up the kinds of things but even if you're

1446
01:31:24,470 --> 01:31:29,910
not thinking about being a software start-up but it's still a really good way to

1447
01:31:29,910 --> 01:31:34,350
look at it at a project Oregon 0 what is some of the scaling issues

1448
01:31:34,350 --> 01:31:38,970
that I'm going to talk about it talk about being successful I think applied always

1449
01:31:39,990 --> 01:31:43,490
I'm going to spend a little bit of time just kind of talking about some

1450
01:31:43,490 --> 01:31:47,890
but not all of our Web services and then finally I'd like to end with

1451
01:31:48,090 --> 01:31:52,170
water to gambles that I guess the big when I want shows how to set

1452
01:31:52,630 --> 01:31:57,890
Our virtual server environment but it also showed a more rounded database of somebody wants

1453
01:31:57,890 --> 01:32:00,130
to see that this under

1454
01:32:00,810 --> 01:32:04,550
but so before actually start on any of this

1455
01:32:04,670 --> 01:32:05,950
I always start

1456
01:32:05,990 --> 01:32:10,730
explaining who were not the result of 3 parts that Amazon is you can see

1457
01:32:10,730 --> 01:32:15,030
a clear of and I think most people know what our 0 . com partly

1458
01:32:15,030 --> 01:32:19,190
on online retail business is you can see I know we've been there for a

1459
01:32:19,190 --> 01:32:21,990
while and I don't need to say much about that because I think most of

1460
01:32:21,990 --> 01:32:23,270
us know about that

1461
01:32:23,390 --> 01:32:28,150
we also have a merchant business and the merchant businesses sort of divided into 2

1462
01:32:28,150 --> 01:32:34,330
parts on 1 hand we run high scale websites for other companies that would like

1463
01:32:34,330 --> 01:32:40,490
to adjust outsourced the the problem in the issues around Gardena Heiskell website so for

1464
01:32:40,490 --> 01:32:44,190
example in the UK Marks & Spencer 1st website we run

1465
01:32:44,470 --> 01:32:46,190
the other half of them

1466
01:32:46,230 --> 01:32:52,450
line of business are vendors who sell through the Amazon platform sold for example

1467
01:32:52,490 --> 01:32:56,850
if you were to go to Amazon . com and orders for example and although

1468
01:32:56,850 --> 01:33:01,770
park or you know something like that chances are high that it would your order

1469
01:33:01,770 --> 01:33:05,050
would be filled by 1 of these other merchant partners

1470
01:33:05,510 --> 01:33:09,990
and then would have a technology business and that's the part that I am here

1471
01:33:09,990 --> 01:33:11,390
to talk about

1472
01:33:12,210 --> 01:33:19,530
this is the essentially making available to software developers a set of salt Web services

1473
01:33:19,530 --> 01:33:25,950
that with the Web services come operational best practices because of Amazon's operating with services

1474
01:33:26,350 --> 01:33:31,110
in the same operational best practices in the same principles although not directly always the

1475
01:33:31,110 --> 01:33:34,910
same software that runs on Amazon . com

1476
01:33:35,350 --> 01:33:38,510
so we believe that there is a high

1477
01:33:38,650 --> 01:33:41,570
among demand out there in the market for it

1478
01:33:41,610 --> 01:33:44,130
1 of the the things that makes me say that as you can see we

1479
01:33:44,130 --> 01:33:50,850
got over 3 130 thousand software developers who who've already registered to use Amazon Web

1480
01:33:50,850 --> 01:33:57,590
Services so I believe that the young people are paying attention in Salfordville Persia using

1481
01:33:57,850 --> 01:34:02,550
our act also want say before move off of the slide that everything I'm talking

1482
01:34:02,550 --> 01:34:04,830
about this afternoon

1483
01:34:04,870 --> 01:34:09,770
it is designed for it a platform EPI Web service API

1484
01:34:09,810 --> 01:34:14,990
designed for software developers or somebody who's technical at any rate like in case of

1485
01:34:14,990 --> 01:34:18,770
a server that we stepped in Doha run server it's not designed for an end

1486
01:34:19,930 --> 01:34:24,230
and it's not designed for a non-technical business persons were really looking to all of

1487
01:34:24,240 --> 01:34:28,090
you to build solutions on top of the platform it if it's going to be

1488
01:34:28,090 --> 01:34:31,030
ready for someone it that's an inducer

1489
01:34:31,150 --> 01:34:33,650
or software array of business use

1490
01:34:35,310 --> 01:34:35,810
so far

1491
01:34:36,030 --> 01:34:40,750
just before I came across a came over to this session I shot some screen

1492
01:34:40,750 --> 01:34:44,510
shots and when we explain what we have here so

1493
01:34:44,990 --> 01:34:48,250
the 1st shot here is a years

1494
01:34:48,290 --> 01:34:55,090
sliding window of elite PG used for BBC that Kodak that came in the reason

1495
01:34:55,090 --> 01:34:57,450
I'm using this is what I'm trying to do

1496
01:34:57,470 --> 01:35:01,570
In establish a baseline for really high-volume website

1497
01:35:01,870 --> 01:35:02,770
it on

1498
01:35:02,790 --> 01:35:07,910
so on the x-axis here we have a three-year sliding window

1499
01:35:08,110 --> 01:35:08,520
and so on

1500
01:35:08,910 --> 01:35:12,150
yesterday according to this it's essential 3rd

1501
01:35:12,190 --> 01:35:17,550
and at the Y axis is really please use per million delivered a top of

1502
01:35:17,570 --> 01:35:18,610
the Texas

1503
01:35:18,650 --> 01:35:25,640
is 3 thousand page views per million Internet users now these steps were measured by

1504
01:35:25,640 --> 01:35:31,260
a but that time which is a subsidiary of Amazon's but I'm not using it

1505
01:35:31,270 --> 01:35:36,270
did not using these steps because Amazon owns them using them because they have about

1506
01:35:36,270 --> 01:35:42,550
300 terabytes of Web Kroll information that I was able to plot graph against now

1507
01:35:43,210 --> 01:35:48,390
my assertion that if I must start or if I can get some idea of

1508
01:35:48,390 --> 01:35:53,410
reusable this line whether it's over here or whether it's over there as long as

1509
01:35:53,410 --> 01:35:57,730
I'm making money along the way I probably have a successful business

1510
01:35:58,070 --> 01:36:02,570
so let's look at what this means to what this looks like some other sites

1511
01:36:02,870 --> 01:36:05,590
In his 1st when his flicker

1512
01:36:06,190 --> 01:36:10,410
so you know this is you can see the fairly successful they started off back

1513
01:36:10,410 --> 01:36:17,490
here in 2004 2005 as just an idea and along the way you know they're

1514
01:36:17,490 --> 01:36:21,910
all BBC's page you numbers and think that they're doing quite well I think this

1515
01:36:21,910 --> 01:36:24,650
is probably young but I'm not certain

1516
01:36:25,130 --> 01:36:26,650
and I

1517
01:36:26,730 --> 01:36:31,490
saying that I know anything about our special about flickers operations I because I don't

1518
01:36:32,270 --> 01:36:33,810
but I do think

1519
01:36:33,850 --> 01:36:37,740
that I would not want to to be the guy in operations and feature that

1520
01:36:40,650 --> 01:36:46,630
liquor because this is the dark side of success you there's always something to you

1521
01:36:46,630 --> 01:36:48,850
when you have an inflection point like that

1522
01:36:48,890 --> 01:36:51,800
you know that there's something that went wrong you don't know what it will be

1523
01:36:51,800 --> 01:36:57,030
an advance unfortunately you don't know when that day will come but that it that

1524
01:36:57,310 --> 01:37:01,330
it was only after about them is that had a week's worth but

1525
01:37:01,330 --> 01:37:06,320
it's important to know how things really happened so then what happens is

1526
01:37:06,380 --> 01:37:07,990
the bacteria

1527
01:37:08,030 --> 01:37:12,550
are separated from these particles and it turns out these particles are the viral particles

1528
01:37:12,550 --> 01:37:17,700
are much lighter much less dense than the bacteria so how do we separate the

1529
01:37:17,720 --> 01:37:23,560
centrifuge them we centrifuge them bacterial particles are there up in the super may turn

1530
01:37:23,560 --> 01:37:27,340
out to be are age caps eds

1531
01:37:27,370 --> 01:37:28,800
and now what we do

1532
01:37:28,810 --> 01:37:32,790
we take this stuff we measure the radioactivity

1533
01:37:32,800 --> 01:37:34,680
in the super latent

1534
01:37:34,680 --> 01:37:36,250
that is the material that

1535
01:37:36,390 --> 01:37:40,580
stays above and we measure the radioactivity

1536
01:37:41,280 --> 01:37:42,370
in the palate

1537
01:37:43,000 --> 01:37:48,920
and what we end up seeing

1538
01:37:48,930 --> 01:37:51,000
whereas most of the

1539
01:37:51,050 --> 01:37:54,440
p thirty two what shows up in the palace

1540
01:37:54,480 --> 01:37:58,030
mostly p thirty two

1541
01:37:58,080 --> 01:38:00,260
it shows up in the palace

1542
01:38:00,320 --> 01:38:05,970
is there no s thirty five in the palace

1543
01:38:06,010 --> 01:38:08,780
you know in the textbooks story of course there's no s thirty five was they

1544
01:38:08,780 --> 01:38:12,970
want to be nice and clean but in reality there's going to be some

1545
01:38:12,990 --> 01:38:14,530
s thirty five

1546
01:38:14,670 --> 01:38:15,450
it was you know

1547
01:38:15,460 --> 01:38:17,240
less than one percent

1548
01:38:19,070 --> 01:38:20,760
o VS thirty five

1549
01:38:20,760 --> 01:38:22,360
ends up in the hell

1550
01:38:22,420 --> 01:38:24,710
most of the s thirty five

1551
01:38:24,780 --> 01:38:29,090
stays up here in the supernova

1552
01:38:29,270 --> 01:38:31,760
there's all the phosphorus going

1553
01:38:31,760 --> 01:38:35,330
and of course some of the viruses that even attach and everything is and so

1554
01:38:35,330 --> 01:38:39,380
the story that the phosphorus up in super names but the striking thing is that

1555
01:38:39,380 --> 01:38:45,870
the palace primarily has gotten the radioactive phosphorus not reactive sulfur sulfur and therefore we

1556
01:38:45,870 --> 01:38:49,680
can conclude that what

1557
01:38:49,690 --> 01:38:51,480
well more DNA went in

1558
01:38:51,490 --> 01:38:54,550
and protein

1559
01:38:54,610 --> 01:38:59,510
we therefore entitled to include the DNA is the hereditary material

1560
01:39:09,610 --> 01:39:12,950
i mean i suppose that one percent sulphur

1561
01:39:12,970 --> 01:39:17,150
is tracking one minor protein that is the secret

1562
01:39:17,180 --> 01:39:21,730
you can it's very hard to rule out there is no contaminants travelling along with

1563
01:39:21,730 --> 01:39:26,420
the DNA and if you really truly disbelieve DNA you could be churlish and say

1564
01:39:26,420 --> 01:39:30,420
well i just don't believe that that you've sold purified it you can completely rule

1565
01:39:30,420 --> 01:39:34,940
out that some minor protein component is really conferring credit

1566
01:39:34,960 --> 01:39:37,230
in fact we really look closely

1567
01:39:37,270 --> 01:39:40,270
every mccarty mccloud biochemistry i believe

1568
01:39:40,270 --> 01:39:41,690
i was pure

1569
01:39:41,710 --> 01:39:44,250
then the purity of this experiment

1570
01:39:45,210 --> 01:39:48,520
by this point thinking it began to shift toward

1571
01:39:48,550 --> 01:39:53,800
DNA being reasonable hereditary molecule in addition it was the second line of the proof

1572
01:39:53,800 --> 01:39:55,980
different from this from the number carcass

1573
01:39:56,070 --> 01:40:01,690
using a different system both pointing to the same answer and the intellectual tied shifted

1574
01:40:01,750 --> 01:40:05,770
to recognising that this probably was right and the reason these experiments were pointing to

1575
01:40:05,770 --> 01:40:10,020
DNA was DNA had to be the right answer

1576
01:40:10,070 --> 01:40:12,150
but of course how is the right answer

1577
01:40:12,150 --> 01:40:15,380
what was it about DNA that could confer these properties

1578
01:40:15,380 --> 01:40:20,210
this was still unclear in nineteen fifty three but not that long

1579
01:40:20,230 --> 01:40:22,440
it became clarified

1580
01:40:22,460 --> 01:40:26,250
relatively soon thereafter

1581
01:40:26,250 --> 01:40:32,880
and of course it became clarified

1582
01:40:34,170 --> 01:40:35,630
the understanding

1583
01:40:35,650 --> 01:40:38,150
of DNA structure

1584
01:40:38,210 --> 01:40:43,130
the double helix

1585
01:40:43,230 --> 01:40:50,050
nobody here has not heard of the double helix probably is nobody

1586
01:40:50,110 --> 01:40:53,190
you know grown-up up it doesn't have the double helix and all that but nonetheless

1587
01:40:53,210 --> 01:40:57,230
i want to stop and take a little bit about also al-samoud personal this is

1588
01:40:57,230 --> 01:41:00,960
this is the first year of talked this class after

1589
01:41:01,690 --> 01:41:04,730
the first time after this class when

1590
01:41:04,820 --> 01:41:08,170
crick and watson not both the lines of you may know the francis crick died

1591
01:41:08,170 --> 01:41:09,790
just this this past summer

1592
01:41:09,880 --> 01:41:12,770
which is very sad is an incredible person

1593
01:41:12,800 --> 01:41:15,940
but actually you know as i've said mendel was one of my heroes francis crick

1594
01:41:15,940 --> 01:41:19,820
was also one of my this is just an extraordinary person but jim watson is

1595
01:41:19,820 --> 01:41:25,630
still alive and kicking and still quite active and so in any case you're not

1596
01:41:25,630 --> 01:41:28,770
far removed so i tell you a little bit about this stuff is history but

1597
01:41:28,770 --> 01:41:32,980
this this history i'm telling you about these people are for the most part francis's

1598
01:41:32,980 --> 01:41:38,720
passing notwithstanding alive and kicking jim watson is still quite actually mccarty is still it's

1599
01:41:38,720 --> 01:41:42,110
really anyway so nineteen

1600
01:41:42,270 --> 01:41:43,590
fifty three

1601
01:41:43,590 --> 01:41:46,500
just a year later jim watson

1602
01:41:46,550 --> 01:41:48,650
and francis crick

1603
01:41:48,860 --> 01:41:53,070
are working in england

1604
01:41:53,110 --> 01:41:56,130
watson is a is a

1605
01:41:56,150 --> 01:42:04,180
student from indiana former waterfall just had his interest in ornithology originally and then studied

1606
01:42:04,180 --> 01:42:09,400
more biology came to england as he wanted to study the gene francis crick physicists

1607
01:42:09,400 --> 01:42:17,240
density plus three-times the pressure is negative so this factor is negative canceling that sign

1608
01:42:17,240 --> 01:42:24,280
to given an overall an acceleration and this requires a fluid but the an equation of

1609
01:42:24,280 --> 01:42:32,640
statepover rho smaller than one-third and one possibility for that is an equation

1610
01:42:32,640 --> 01:42:39,110
of state of vacuum energy which is p equals minus rho or

1611
01:42:39,110 --> 01:42:47,920
W equal to minusoneso the idea behind the inflation is that there is some field that

1612
01:42:47,940 --> 01:42:57,520
we will introduce call the inflaton field which dominates the energy density during inflation then we`ll be quantum

1613
01:42:57,520 --> 01:43:04,580
fluctuations during inflation which produce perturbations in the inflaton field which in turn

1614
01:43:04,580 --> 01:43:12,560
produce perturbations and the temperatureandenergy density then the field travels outside the landscale

1615
01:43:12,660 --> 01:43:21,000
travels outside the Hubble radius eventually to come back in the hubble radius where we observe density perturbations

1616
01:43:21,000 --> 01:43:26,770
and temperatureperturbationsthat are related to theperturbationsin the inflaton field

1617
01:43:26,780 --> 01:43:29,260
produced by quantum effects

1618
01:43:29,480 --> 01:43:38,520
soh barthe action of quanta will play a role in generating the large-scale

1619
01:43:38,530 --> 01:43:48,760
perturbationsin the universe now the idea of the quantum world having something to do with

1620
01:43:48,760 --> 01:43:58,360
these perturbations is related to the creation of particles in the expanding universe this is

1621
01:43:58,360 --> 01:44:05,380
not a particularly new idea that the expansion of the universe leads to particle creation

1622
01:44:05,380 --> 01:44:13,730
but in fact it was discovered bythe earliest particle cosmologistSchrdingerin the min

1623
01:44:13,930 --> 01:44:20,660
Schrdingerdid some something of importance in the nineteen twenties but in the nineteen thirties

1624
01:44:20,730 --> 01:44:27,320
his interest turn to cosmology he was influenced by Eddington and Lemaitre

1625
01:44:27,620 --> 01:44:38,140
and there was a very interesting paper written by Schrdinger over the period of nineteen thirty-eight to nineteen thirty-nine

1626
01:44:38,140 --> 01:44:45,480
this was alsoa verytumultuous time in the life ofSchrdinger in nineteen thirty-eight he was

1627
01:44:45,480 --> 01:44:50,860
a professor in Austria at the University of Graz and then when there was the

1628
01:44:50,860 --> 01:44:58,120
annexation of Austriahehad to leaveAustriabecause of his political writings and his political

1629
01:44:58,120 --> 01:45:07,180
statements andhe snuckout of Austria and went to Rome there`s some interesting correspondence

1630
01:45:07,180 --> 01:45:14,460
in the University of Chicagoarchives betweenfermi andSchrdinger when Schrdingergot off of a train

1631
01:45:14,610 --> 01:45:20,820
in Rome he was met by Fermi Schrdingerof course could not take any money with him

1632
01:45:20,880 --> 01:45:29,180
so Fermi lent him a couple of thousand lira and then in the early nineteen fifties as a long series

1633
01:45:29,180 --> 01:45:36,160
of calculations that Fermi andSchrdingerwere doing trying to convert a thousand pre-war lira

1634
01:45:36,160 --> 01:45:47,380
into dollars in nineteen fifties Fermi never did get repaid so he went to the vatican to find asylum

1635
01:45:47,380 --> 01:45:52,840
in the vatican and those of you who know something about the

1636
01:45:52,840 --> 01:46:00,970
personality ofSchrdinger realise that the cloistered life of the vatican was not an ideal place forSchrdingerto live

1637
01:46:00,970 --> 01:46:08,380
so he decided in nineteen thirty-nine it would be a very safe bet to move to belgium

1638
01:46:08,380 --> 01:46:15,340
and then of coursea life before the outbreak of the second World War he

1639
01:46:15,340 --> 01:46:18,940
realized that this wasn't so safe so he found his way to dublin

1640
01:46:18,940 --> 01:46:28,560
soduringthis periodSchrdinger wrote a paper called proper vibrations of the expanding universe this

1641
01:46:28,560 --> 01:46:34,640
is a paper that`sremarkable to me and not very well appreciated today

1642
01:46:34,780 --> 01:46:42,280
In the introduction to this paperSchrdinger wrote that proper vibrations by which he meant the positive

1643
01:46:42,280 --> 01:46:46,660
and negative terms to field theory e to the plus or minus I

1644
01:46:46,980 --> 01:46:52,480
omega t cannot be rigorously separated in the expanding universe

1645
01:46:52,620 --> 01:46:59,120
then he wrote in the introduction that this is a phenomenon of outstanding importance by which he

1646
01:46:59,240 --> 01:47:05,000
didn't actually say this but it's implied that he was talking about density perturbations from

1647
01:47:05,000 --> 01:47:10,100
inflation and tomorrow we'll talk about wimpzillasso he didn't actually use the word

1648
01:47:10,100 --> 01:47:18,000
wimpzillas but he could haveSchrdinger wrote with particles it would mean production or annihilation

1649
01:47:18,000 --> 01:47:26,300
of matter merely by expansion alarmed by these prospects I`d examined the matter in more detail soSchrdinger went

1650
01:47:26,300 --> 01:47:33,000
through a lot of calculationsand came to the conclusion that there will be a mutual adulteration

1651
01:47:33,000 --> 01:47:39,430
of positive and negative frequencytermsin the course of time giving rise to the alarming phenomenon

1652
01:47:39,870 --> 01:47:47,840
of particle creation thereare two words thatSchrdingerused again and again in this paper one

1653
01:47:48,280 --> 01:47:59,460
word is alarming and the other word that he was very fond of was adulteration now

1654
01:48:00,160 --> 01:48:05,800
I don't know whySchrdingerwas alarmed by this but which

1655
01:48:05,800 --> 01:48:13,920
Schrdinger was alarmed at was the possibility that sometime in the next ten billion years

1656
01:48:14,140 --> 01:48:20,790
in a volume of ten to the fifty-seven cubic centimeters there could be created a particle with an energy of

1657
01:48:20,790 --> 01:48:30,320
ten to the minus thirty-three electron volts does that alarm anyone here

1658
01:48:30,320 --> 01:48:37,020
I think it alarmedSchrdingerSchrdinger was alarmed by this when he was actually running for his life

1659
01:48:37,020 --> 01:48:43,400
which questions in which thinkers should one pick up for oneself

1660
01:48:43,410 --> 01:48:48,630
perhaps the oldest and most fundamental questions that i wish to

1661
01:48:48,680 --> 01:48:52,560
examined in the course of the semester

1662
01:48:52,610 --> 01:48:57,050
it is the question what is a regime

1663
01:48:57,070 --> 01:48:58,890
what are regimes

1664
01:48:58,940 --> 01:48:59,860
what are

1665
01:48:59,870 --> 01:49:01,610
regime politics

1666
01:49:01,700 --> 01:49:02,690
the term

1667
01:49:02,710 --> 01:49:05,280
regime is the familiar one

1668
01:49:05,290 --> 01:49:11,070
we often hear today about shaping regimes or about changing regimes

1669
01:49:11,110 --> 01:49:13,440
but what is the regime

1670
01:49:13,460 --> 01:49:16,190
how many kinds are there

1671
01:49:16,230 --> 01:49:18,470
how are they defined

1672
01:49:18,480 --> 01:49:23,690
what holds them together and what causes them to fall apart is there a single

1673
01:49:23,720 --> 01:49:26,840
best regime

1674
01:49:26,890 --> 01:49:30,230
those are the questions i want us to consider

1675
01:49:30,240 --> 01:49:35,060
the concept of the regime is perhaps the oldest and most fundamental

1676
01:49:35,070 --> 01:49:37,620
of political ideas it goes back to play-doh

1677
01:49:37,670 --> 01:49:39,670
and even before

1678
01:49:39,730 --> 01:49:43,210
in fact the title of the book that you will be reading

1679
01:49:43,220 --> 01:49:46,660
part of this semester platelets republic

1680
01:49:46,670 --> 01:49:49,880
it is actually a translation of the title of it

1681
01:49:49,930 --> 01:49:52,460
of the greek word pull the tail

1682
01:49:52,510 --> 01:49:54,780
that means constitution

1683
01:49:54,810 --> 01:49:56,820
or regime

1684
01:49:56,870 --> 01:50:00,510
the republic is a book about the regime

1685
01:50:00,550 --> 01:50:03,290
and all later political philosophy

1686
01:50:03,310 --> 01:50:05,800
this a series of footnotes to play-doh

1687
01:50:05,840 --> 01:50:11,650
and that means that it must provide a series of variations so to speak

1688
01:50:11,700 --> 01:50:13,590
and players conception

1689
01:50:13,600 --> 01:50:17,760
one of the best regime

1690
01:50:17,780 --> 01:50:20,820
but what is the regime broadly speaking

1691
01:50:20,900 --> 01:50:23,750
regime indicates a form of government

1692
01:50:23,800 --> 01:50:26,000
whether it is ruled by the one

1693
01:50:26,080 --> 01:50:27,330
i feel

1694
01:50:29,370 --> 01:50:32,460
it is more common some mixture combination

1695
01:50:32,560 --> 01:50:35,880
these three ruling powers

1696
01:50:35,930 --> 01:50:40,160
a regime is defined in the first instance by how people are governed

1697
01:50:40,170 --> 01:50:41,000
and how

1698
01:50:41,010 --> 01:50:42,470
public offices

1699
01:50:45,950 --> 01:50:47,780
by birth

1700
01:50:47,790 --> 01:50:53,550
by lot by outstanding personal while personal qualities and achievements

1701
01:50:53,570 --> 01:50:55,190
and what constitutes

1702
01:50:55,200 --> 01:50:56,800
the people's rights

1703
01:50:56,850 --> 01:50:59,410
and responsibilities

1704
01:50:59,590 --> 01:51:04,680
the ricci regime again refers above all to form of government

1705
01:51:04,700 --> 01:51:10,650
the political world is not present itself as simply an infinite variety

1706
01:51:10,760 --> 01:51:14,480
of different shapes it is structured and ordered

1707
01:51:14,490 --> 01:51:15,810
in into a few

1708
01:51:15,830 --> 01:51:17,940
basic regime types

1709
01:51:17,950 --> 01:51:19,950
in this i take it to be

1710
01:51:19,960 --> 01:51:22,860
one of the most important propositions

1711
01:51:22,940 --> 01:51:29,180
and insights of political science

1712
01:51:30,420 --> 01:51:33,580
so far

1713
01:51:33,590 --> 01:51:34,920
but there is

1714
01:51:34,930 --> 01:51:37,620
corollary to this insight

1715
01:51:37,630 --> 01:51:42,290
the regime is always something particular

1716
01:51:42,340 --> 01:51:44,440
it stands in relation

1717
01:51:44,480 --> 01:51:46,180
of opposition

1718
01:51:46,190 --> 01:51:49,160
two other regime types

1719
01:51:49,240 --> 01:51:51,010
and as a consequence

1720
01:51:51,060 --> 01:51:53,420
the possibility of conflict

1721
01:51:55,040 --> 01:51:56,730
and war

1722
01:51:56,740 --> 01:51:59,360
it is built in to the very structure

1723
01:51:59,430 --> 01:52:01,350
of politics

1724
01:52:01,400 --> 01:52:05,020
regimes are necessarily partisan that is to say the

1725
01:52:05,040 --> 01:52:08,690
install certain loyalties in passions

1726
01:52:08,710 --> 01:52:10,260
in the same way that one may

1727
01:52:10,280 --> 01:52:12,030
i feel partisanship

1728
01:52:12,050 --> 01:52:13,740
to the new york yankees

1729
01:52:13,750 --> 01:52:15,900
the boston red sox

1730
01:52:15,950 --> 01:52:17,240
but yale

1731
01:52:17,300 --> 01:52:19,410
overall rival

1732
01:52:19,470 --> 01:52:22,830
colleges and institutions right

1733
01:52:22,840 --> 01:52:24,280
fierce loyalty

1734
01:52:25,770 --> 01:52:27,630
inseparable from

1735
01:52:27,640 --> 01:52:31,690
the character of regime politics is passionate attachments

1736
01:52:31,700 --> 01:52:33,970
not merely something that take place

1737
01:52:34,040 --> 01:52:34,790
you might say

1738
01:52:34,800 --> 01:52:38,480
between different regimes but even with in

1739
01:52:38,490 --> 01:52:41,120
there's different parties and groups

1740
01:52:41,130 --> 01:52:45,790
with loyalties and attachments contend for power

1741
01:52:45,800 --> 01:52:47,170
for honor

1742
01:52:47,220 --> 01:52:49,970
and for interest

1743
01:52:50,030 --> 01:52:58,530
henry adams once cynically reflected the politics is simply the organisation of hatreds

1744
01:52:58,540 --> 01:53:02,950
and there is more than a grain of truth to to this

1745
01:53:04,630 --> 01:53:07,350
although he did not say that is also

1746
01:53:07,430 --> 01:53:09,850
the temple an attempt to channel

1747
01:53:11,120 --> 01:53:13,600
those hatreds and animosities

1748
01:53:13,610 --> 01:53:16,130
twenty something like common good this

1749
01:53:16,140 --> 01:53:20,740
raise the question is it possible to transform politics

1750
01:53:20,770 --> 01:53:21,830
to replace

1751
01:53:23,170 --> 01:53:26,630
in factional conflict with friendship

1752
01:53:26,640 --> 01:53:27,610
to replace

1753
01:53:28,940 --> 01:53:31,430
with harmony

1754
01:53:31,480 --> 01:53:35,110
today is it is the hope of many people both here

1755
01:53:35,110 --> 01:53:38,460
and abroad we might even overcome

1756
01:53:38,480 --> 01:53:43,650
and even transcend the basic structure of regime politics altogether

1757
01:53:43,670 --> 01:53:45,610
and organize our world

1758
01:53:45,650 --> 01:53:46,910
around global

1759
01:53:46,920 --> 01:53:48,620
norms of justice

1760
01:53:48,620 --> 01:53:49,960
in international law

1761
01:53:51,090 --> 01:53:53,670
is such a thing possible

1762
01:53:53,700 --> 01:53:56,030
it cannot be ruled out

1763
01:53:56,070 --> 01:54:00,540
but such a world i would note that is to say world administered by international

1764
01:54:00,540 --> 01:54:02,150
courts of law

1765
01:54:02,170 --> 01:54:03,630
by judges

1766
01:54:03,660 --> 01:54:06,020
in judicial tribunals

1767
01:54:06,100 --> 01:54:09,330
would no longer be a political world

1768
01:54:10,520 --> 01:54:12,480
it only takes place

1769
01:54:12,500 --> 01:54:15,940
within the context of the particular

1770
01:54:15,950 --> 01:54:17,990
it is only possible

1771
01:54:18,070 --> 01:54:22,100
within the structure of the regime itself

1772
01:54:22,150 --> 01:54:25,430
but the regime is more than simply set

1773
01:54:25,440 --> 01:54:26,590
a formal

1774
01:54:26,620 --> 01:54:28,560
principal formal

1775
01:54:28,620 --> 01:54:31,870
structures and institutions OK

1776
01:54:31,890 --> 01:54:36,160
it consists of the entire way of life

1777
01:54:36,210 --> 01:54:41,280
the moral and religious practices habits customs and sentiment

1778
01:54:41,280 --> 01:54:44,540
that make people what they are

1779
01:54:44,590 --> 01:54:47,870
the regime constitutes an ethos

1780
01:54:47,920 --> 01:54:50,840
that is to say distinctive character

1781
01:54:50,880 --> 01:54:55,150
nurtures distinctive human types

1782
01:54:55,170 --> 01:54:58,450
every regime shapes a common character

1783
01:54:58,490 --> 01:55:03,150
a common character types with distinctive traits and qualities

1784
01:55:03,230 --> 01:55:08,670
so the study of regime politics is in part a study of the distinctive national

1785
01:55:08,720 --> 01:55:11,400
character types that constitutes

1786
01:55:11,450 --> 01:55:13,290
a citizen body

1787
01:55:13,380 --> 01:55:15,770
to take an example of what i mean

1788
01:55:15,780 --> 01:55:22,170
when tokyo study the american regime with a democratic regime properly speaking in democracy in

1789
01:55:23,450 --> 01:55:29,210
he started first with our formal political institutions is enumerated in the constitution such things

1790
01:55:29,210 --> 01:55:35,000
as the separation of powers division between state and what federal government and so on

1791
01:55:35,050 --> 01:55:41,890
but then went on to look at such informal practices is american manners and morals

1792
01:55:41,940 --> 01:55:48,160
a tendency to form small civic associations are peculiar moralism and religious life

1793
01:55:48,200 --> 01:55:49,840
o defensiveness

1794
01:55:49,840 --> 01:55:54,380
about democracy and so on all of these intellectual

1795
01:55:54,400 --> 01:55:57,130
and moral customs and habits

1796
01:55:57,130 --> 01:56:00,860
help to constitute the democratic regime in this

