1
00:00:00,000 --> 00:00:04,680
little subspace and it's true subspace then

2
00:00:04,690 --> 00:00:08,410
there's got to be some more in right i have to be able to multiply

3
00:00:08,410 --> 00:00:10,660
that by two

4
00:00:10,710 --> 00:00:15,060
and that and that double vector has to be included

5
00:00:15,060 --> 00:00:17,230
i have to be able to multiplied by zero

6
00:00:17,320 --> 00:00:19,330
the vector or by

7
00:00:19,340 --> 00:00:23,390
or by three-quarters of all these vectors or by minus

8
00:00:23,410 --> 00:00:25,880
or by minus one

9
00:00:25,880 --> 00:00:30,110
i have to be able to multiplied by any number

10
00:00:30,160 --> 00:00:34,910
so that is going to say that i have to have a whole line

11
00:00:34,920 --> 00:00:37,700
did you see that

12
00:00:37,750 --> 00:00:41,970
one side get a vector in there i got the whole line of all

13
00:00:41,980 --> 00:00:44,360
multiples of that vector i can't

14
00:00:44,400 --> 00:00:47,300
can be a vector space without

15
00:00:47,350 --> 00:00:53,030
included without extending to to get those multiples in there now i still have to

16
00:00:53,030 --> 00:00:56,340
check addition

17
00:00:56,360 --> 00:01:00,070
but that comes out OK this line is going to work

18
00:01:00,070 --> 00:01:03,050
because i could add something on the lines

19
00:01:03,050 --> 00:01:07,010
there's something else on the line and i'm still line

20
00:01:07,990 --> 00:01:09,930
so for example

21
00:01:09,950 --> 00:01:12,280
so this is all examples

22
00:01:12,740 --> 00:01:14,740
example of a subspace

23
00:01:14,780 --> 00:01:17,990
our example is a line

24
00:01:18,010 --> 00:01:19,990
in r two

25
00:01:20,130 --> 00:01:26,530
actually not as any

26
00:01:26,590 --> 00:01:30,900
if i took this line

27
00:01:30,900 --> 00:01:35,400
with that so all the vectors on that line so that factor and that factor

28
00:01:35,400 --> 00:01:39,260
in this vector in the vector

29
00:01:39,280 --> 00:01:40,200
so in end

30
00:01:43,240 --> 00:01:44,610
i am

31
00:01:44,700 --> 00:01:46,840
drawing something that doesn't work

32
00:01:46,860 --> 00:01:48,910
it's not a subspace

33
00:01:48,910 --> 00:01:52,680
the line in order to have to be a subspace the line are two

34
00:01:52,680 --> 00:01:55,820
must go through

35
00:01:55,860 --> 00:02:00,720
the zero vector

36
00:02:00,760 --> 00:02:08,840
because why is this line of good let me do it dashed line

37
00:02:08,910 --> 00:02:13,930
because if i multiply that vector on the dashed line by zero then it does

38
00:02:14,160 --> 00:02:17,970
not on the dashed line zero is going to be

39
00:02:18,030 --> 00:02:23,400
every subspace has got to contain zero because i must be allowed to multiplied by

40
00:02:23,400 --> 00:02:27,660
zero and that will always give me the zero

41
00:02:29,400 --> 00:02:31,360
now i was going to

42
00:02:32,800 --> 00:02:35,760
create some subspaces

43
00:02:37,550 --> 00:02:39,840
well i mean are two

44
00:02:39,900 --> 00:02:42,510
why we think of all the possibility

45
00:02:42,530 --> 00:02:45,180
are two there can be that many

46
00:02:45,200 --> 00:02:49,380
so what are the possible subspaces of r two

47
00:02:49,380 --> 00:02:51,360
i me list

48
00:02:51,410 --> 00:02:56,550
so all those so i'm listing now the subspaces

49
00:02:56,590 --> 00:02:58,990
of the art

50
00:03:02,570 --> 00:03:05,990
one possibility that we always allow is

51
00:03:05,990 --> 00:03:07,570
all of our two

52
00:03:07,590 --> 00:03:09,070
the whole thing

53
00:03:09,180 --> 00:03:11,130
the whole space

54
00:03:11,180 --> 00:03:15,410
that counts as a subspace of itself

55
00:03:15,450 --> 00:03:17,200
you always want to allow that

56
00:03:17,260 --> 00:03:19,130
than others

57
00:03:19,180 --> 00:03:21,130
or lines

58
00:03:21,140 --> 00:03:23,570
any line

59
00:03:23,680 --> 00:03:30,240
line meaning infinitely far in both directions through

60
00:03:30,280 --> 00:03:36,490
the then zero

61
00:03:36,530 --> 00:03:38,340
so that's what that's like

62
00:03:38,400 --> 00:03:41,880
the whole space is like a whole two d space

63
00:03:41,970 --> 00:03:47,910
this is like one is that is this line the same as or one no

64
00:03:47,950 --> 00:03:48,860
i don't know

65
00:03:48,860 --> 00:03:52,450
you could say it looks a lot like are one

66
00:03:52,450 --> 00:03:57,010
our one was just a line and this is the line but this is online

67
00:03:57,010 --> 00:04:01,860
inside our two that the vectors here i have two components

68
00:04:01,880 --> 00:04:06,950
so that's not the same as are one because they are the vectors only one

69
00:04:06,970 --> 00:04:08,470
very closely

70
00:04:08,470 --> 00:04:09,820
but that no

71
00:04:09,820 --> 00:04:10,990
about the same

72
00:04:11,760 --> 00:04:17,930
and now there's a third possibility

73
00:04:17,930 --> 00:04:21,590
there's a third subspace

74
00:04:21,630 --> 00:04:26,070
that's a far too that's not not the whole thing

75
00:04:26,140 --> 00:04:29,380
and it's not line

76
00:04:29,400 --> 00:04:31,530
it's even less

77
00:04:31,590 --> 00:04:34,180
it's just the zero vector alone

78
00:04:34,260 --> 00:04:38,860
the zero vector along only

79
00:04:42,640 --> 00:04:46,950
often call this subspace the just for you

80
00:04:46,990 --> 00:04:49,090
here's the line l

81
00:04:49,110 --> 00:04:51,780
here's the plane all of our two

82
00:04:52,320 --> 00:04:55,680
you see that the zero vector is OK

83
00:04:55,700 --> 00:05:01,200
you just to understand subspaces we have to know the rules and knowing the rules

84
00:05:01,220 --> 00:05:03,970
means that we have to see that yes

85
00:05:04,030 --> 00:05:06,820
the zero vector by itself

86
00:05:06,820 --> 00:05:12,740
just this guy alone satisfies the rules wise that out too dumb to tell you

87
00:05:12,930 --> 00:05:17,860
if i took that and added to itself i'm still there

88
00:05:17,860 --> 00:05:21,720
if i took that and multiply by seventeen i'm still there

89
00:05:21,740 --> 00:05:26,470
so i've i've done the operations adding and multiplying by numbers

90
00:05:26,530 --> 00:05:29,640
that are required and i didn't go outside

91
00:05:29,700 --> 00:05:31,410
this one point

92
00:05:35,800 --> 00:05:41,210
that's always as that's the little a subspace the largest subspaces the whole thing and

93
00:05:41,210 --> 00:05:43,720
in between come all

94
00:05:43,740 --> 00:05:45,680
whatever is in between

95
00:05:45,700 --> 00:05:49,070
OK so for example what's in between for our three

96
00:05:49,090 --> 00:05:51,740
so if i'm an ordinary three dimensions

97
00:05:51,760 --> 00:05:53,470
the subspaces are

98
00:05:53,490 --> 00:05:54,910
all of our three

99
00:05:54,990 --> 00:05:58,970
one extreme the zero vector at the bottom

100
00:05:59,030 --> 00:06:01,680
r and then

101
00:06:01,740 --> 00:06:02,840
a plane

102
00:06:02,840 --> 00:06:06,660
so this is interesting for several reasons

103
00:06:06,670 --> 00:06:08,090
what did he

104
00:06:08,100 --> 00:06:10,650
managed to pull out information why

105
00:06:10,670 --> 00:06:16,240
from using these relationship and actually to do this he made a few more observations

106
00:06:16,270 --> 00:06:20,870
the first which i just stated that the deflection of the negative particle was just

107
00:06:20,870 --> 00:06:27,340
far away are more extreme much much larger than that of the positive particle

108
00:06:27,350 --> 00:06:31,640
the other assumption that he made here is that the charge on the two particles

109
00:06:31,640 --> 00:06:32,790
with equal

110
00:06:32,860 --> 00:06:36,160
so how could you know that the charge in the two particles with equal and

111
00:06:36,160 --> 00:06:39,620
actually can exactly know it was a very good

112
00:06:39,640 --> 00:06:42,900
the assumption that he made and he could make the assumption

113
00:06:42,910 --> 00:06:47,970
because he in fact know that what he started when was this hydrogen gas so

114
00:06:47,970 --> 00:06:53,020
he was starting with hydrogen if some the negative particle was popping out from the

115
00:06:53,020 --> 00:06:56,810
hydrogen then what he must be left with his age plus

116
00:06:56,820 --> 00:07:01,780
and since hydrogen itself is neutral the each class and the electron had to add

117
00:07:01,780 --> 00:07:05,390
up to be in neutral charge so that means the charges of the two pieces

118
00:07:05,400 --> 00:07:12,250
the positive and negative particle must be equal in terms of absolute charge

119
00:07:12,270 --> 00:07:18,440
so using this relationship you can actually figure out by knowing which she knows how

120
00:07:19,340 --> 00:07:24,230
each of them were deflected he can now try to think about

121
00:07:24,270 --> 00:07:26,560
whether or not he could

122
00:07:26,570 --> 00:07:29,800
make some relationship between the masses between the

123
00:07:29,810 --> 00:07:33,480
the mass of the positive and the negative particle

124
00:07:33,500 --> 00:07:37,900
so this relationship he was looking at was starting

125
00:07:37,950 --> 00:07:41,510
with the deflection

126
00:07:41,520 --> 00:07:46,810
and the absolute distance that the particles were deflected so what he can set equal

127
00:07:46,840 --> 00:07:51,730
to if he knows what x is proportional to in terms of the negative particle

128
00:07:51,730 --> 00:07:55,740
so that just the absolute value of the charge

129
00:07:56,700 --> 00:08:00,160
the mass of the negative particle

130
00:08:00,240 --> 00:08:03,000
you could divide all of that

131
00:08:03,010 --> 00:08:05,010
by the absolute value

132
00:08:05,020 --> 00:08:07,700
of the charge of the positive particle

133
00:08:07,720 --> 00:08:09,340
all over the map

134
00:08:09,400 --> 00:08:11,760
the positive particle

135
00:08:11,770 --> 00:08:16,160
and as we said he made the assumption that those two charges were equal

136
00:08:16,220 --> 00:08:19,100
so we can go ahead and cross those right now

137
00:08:19,140 --> 00:08:23,230
so what that told him with if he knew the relationship between how far they

138
00:08:23,230 --> 00:08:24,400
each displaced

139
00:08:24,470 --> 00:08:27,560
you could also know something about the relationships

140
00:08:27,620 --> 00:08:30,640
of the two masses so essentially

141
00:08:30,690 --> 00:08:33,130
there was an inversely proportional

142
00:08:33,140 --> 00:08:38,430
the relationship between how far the particles would place and what the math that you

143
00:08:39,620 --> 00:08:41,280
it turned out to be

144
00:08:41,320 --> 00:08:44,010
so because you have caused the observed that

145
00:08:44,020 --> 00:08:49,000
the negative particle traveled it was deflected much much further by the way

146
00:08:49,040 --> 00:08:54,430
what he could also assumes and make the conclusion of is that the math

147
00:08:54,440 --> 00:08:56,220
the negative particle

148
00:08:56,230 --> 00:08:59,620
is actually larger smaller

149
00:08:59,660 --> 00:09:03,840
much much smaller exactly the the mass

150
00:09:03,860 --> 00:09:09,550
of the positive particle so essentially what he found here is the relationship between the

151
00:09:09,740 --> 00:09:11,260
mass of an electron

152
00:09:11,300 --> 00:09:15,930
the mass of the rest of the out the rest of the hydrogen atom there

153
00:09:15,930 --> 00:09:17,290
which is an iron in this case

154
00:09:17,690 --> 00:09:22,320
and in fact it so so much smaller than it's close to two thousand times

155
00:09:22,320 --> 00:09:26,020
smaller that we can make the assumption

156
00:09:26,020 --> 00:09:27,660
that essentially

157
00:09:27,670 --> 00:09:33,660
the electrons take no massimino take up a teeny bit essentially when we're thinking about

158
00:09:33,700 --> 00:09:37,090
the set up of the and we don't have to call for them using up

159
00:09:37,090 --> 00:09:40,470
a lot of them as we're discussing

160
00:09:40,480 --> 00:09:43,930
so thompson came up with a model

161
00:09:44,020 --> 00:09:48,150
for the atom due to this and this is called the plum pudding model of

162
00:09:48,150 --> 00:09:50,010
the atom

163
00:09:50,020 --> 00:09:55,470
and he was said english so plum pudding is kind of british food is anyone

164
00:09:55,470 --> 00:09:58,360
here had one putting

165
00:09:58,370 --> 00:09:59,900
a couple people OK

166
00:09:59,900 --> 00:10:02,780
i've never even seen so that's good must

167
00:10:02,790 --> 00:10:07,550
but travel than i so the idea that he had here was he treated the

168
00:10:07,550 --> 00:10:12,880
whole of the atom sort of the positive class are the positive

169
00:10:13,910 --> 00:10:22,750
so the majority of the and was just kind of this could be positive stuff

170
00:10:22,750 --> 00:10:28,520
that you can think about with the winning he had all these negative charges

171
00:10:28,530 --> 00:10:31,770
which were the electrons

172
00:10:31,780 --> 00:10:35,220
they were the ravens are the ones that were in the pudding

173
00:10:35,230 --> 00:10:40,690
so this was a revolutionary model of an atom when we thought of the fact

174
00:10:40,690 --> 00:10:45,570
that before this experiment understanding was an atom could not be divisible into smaller parts

175
00:10:45,670 --> 00:10:52,090
and now here we are with subatomic particles with electrons and this wonderful plum pudding

176
00:10:53,490 --> 00:10:57,920
so for those of you that have actually had plum pudding which myself included i

177
00:10:57,920 --> 00:11:03,000
drew picture up here this is my first place upon planning and i guess you

178
00:11:03,000 --> 00:11:08,200
and is related to the question did the universe actually have a begining I won`t speak

179
00:11:08,200 --> 00:11:16,240
about the beginning because there is not yet been established any observations that would give us any

180
00:11:16,380 --> 00:11:24,340
answer to these questions there is an end to inflation and the at the

181
00:11:24,340 --> 00:11:35,980
end of inflation the universe is frozen it is an extremely low entropy universe dominated by vacuum energy

182
00:11:35,980 --> 00:11:41,880
somehow the universe dominated by vacuum energy has to be changed into the

183
00:11:41,880 --> 00:11:50,680
hot universe dominated by radiation the energy density driving inflation has to be extracted and

184
00:11:50,680 --> 00:11:58,480
converted into radiation how this happens we don't know the universe has to be defrosted

185
00:11:58,480 --> 00:12:05,920
and whether it`s heating preheating reheating we don't know we have names for various ideas that happen

186
00:12:05,920 --> 00:12:13,100
however we don't know how it happened but it's possible there were observable consequences of this

187
00:12:13,100 --> 00:12:21,220
phase perhaps baryogenesis occurred here perhaps there was phase transitions or dark matter produced I won't

188
00:12:21,220 --> 00:12:27,020
have time to talk about this phase but it's something that's amenable at least in principle

189
00:12:27,020 --> 00:12:33,080
to observations the middle of inflation is what I would like to talk about where density

190
00:12:33,080 --> 00:12:39,500
perturbations in gravitational waves were produced and again we imagine this is due to the

191
00:12:39,500 --> 00:12:48,460
fact that particles are produced due to the expanding universe now we're going to

192
00:12:48,460 --> 00:12:58,220
envision the dynamics of the expansion of the universe during inflation as the dynamics that are driven

193
00:12:58,230 --> 00:13:05,800
by the action of scalar field and we're going to employ everything out of this information we

194
00:13:05,800 --> 00:13:13,000
know about scalar quantum field theory so scalar quantum field theory is going

195
00:13:13,000 --> 00:13:21,560
to be the tool that we use to connect fluctuations in a metric density fluctuations

196
00:13:21,560 --> 00:13:29,180
curvature fluctuations to quantum fluctuations in a scalar field and the reason we do

197
00:13:29,270 --> 00:13:35,240
this is that of course as I mentioned before there are so many fundamental scalar fields that are known

198
00:13:35,260 --> 00:13:42,280
so why not use everything we know about scalar field dynamics

199
00:13:42,590 --> 00:13:48,150
however the thing to remember in this demonstration is that we're using scalar field dynamics is

200
00:13:48,160 --> 00:13:56,660
a tool and if a hammer is your only tool everything looks like a nail

201
00:13:56,660 --> 00:14:02,480
so we don't really know whether it was driven by scalar field dynamics because that's but that's

202
00:14:02,480 --> 00:14:08,640
the way we model it because scalar field scalar quantum field theory is a well-developed

203
00:14:08,720 --> 00:14:16,580
tool that we can use so in scalar quantum field theory we imagine that

204
00:14:16,580 --> 00:14:22,780
there is a scalar field and we have a wonderful name for the scalar field we call it

205
00:14:22,780 --> 00:14:29,240
the inflaton but of course naming is not explaining we just have a name for it's

206
00:14:29,300 --> 00:14:37,860
a cool name the inflaton and there is some potential energy associated with the inflaton

207
00:14:37,860 --> 00:14:43,920
the potential energy of a scalar field you can imagine as the energy of the infinite wavelength mode

208
00:14:43,920 --> 00:14:51,150
the zero momentum mode of the inflaton field and if you would ask in a particle language what is

209
00:14:51,150 --> 00:14:58,780
the particle contnent of the universe associated with the vacuum energy it would be a condensate

210
00:14:58,780 --> 00:15:06,720
infinite wavelength particles of the inflaton field so you can imagine for some reason that

211
00:15:06,720 --> 00:15:13,160
the inflaton field is not at the minimum of the potential which is conveniently adjusted

212
00:15:13,180 --> 00:15:20,600
in this case to have zero vacuum energy zero potential energy while the inflaton is displaced

213
00:15:20,600 --> 00:15:30,200
from its minimum it has an energy density and effective cosmological constant dark energy

214
00:15:30,200 --> 00:15:39,500
but component contributing W equal to minus one then through just classical equations of motion you

215
00:15:39,500 --> 00:15:45,100
can imagine the inflaton field rolling down the potential relaxing to its

216
00:15:45,100 --> 00:15:55,010
minimum where the potential energy is zero magically radiating all its kinetic energy to form radiation and

217
00:15:55,010 --> 00:16:04,400
inflation comes to an end so this is the classical picture of a universe that's dominated by

218
00:16:04,560 --> 00:16:10,720
inflation and then evolves in such a way that inflation ends when the field reaches

219
00:16:10,720 --> 00:16:17,760
the minimum of its potential however if there is particle creation due to

220
00:16:17,790 --> 00:16:24,880
the expansion of the universe the expansion of the universe will create particles

221
00:16:24,880 --> 00:16:31,080
of the inflaton field the particles of the inflaton field created by the expansion

222
00:16:31,080 --> 00:16:38,240
of the universe have a finite wavelength so you`re creating finite wavelength mode to the field

223
00:16:38,240 --> 00:16:44,260
and the distribution of the field will not be smooth so in addition to the classical

224
00:16:44,260 --> 00:16:51,060
smooth background there are fluctuations in the field as a result of particle creation in

225
00:16:51,070 --> 00:16:59,700
the expanding universe which lead to fluctuations in the field since the field dominates the energy density

226
00:16:59,700 --> 00:17:07,660
fluctuations in the field amount to fluctuations in the energy density curvature fluctuations and will

227
00:17:07,660 --> 00:17:18,880
also appear as temperature fluctuations so that is the it's something we described word calculus

228
00:17:18,880 --> 00:17:25,660
there's no equation gen just words but a physical feeling I hope you can get about

229
00:17:25,660 --> 00:17:33,300
the origin of the density perturbations and one of the questions that`s related to high-energy

230
00:17:33,300 --> 00:17:41,420
physics is we have this scalar field we have this inflaton who is this inflaton

231
00:17:41,420 --> 00:17:48,640
what where does this scalar field come from no one well maybe people who believe in the anthropic principle

232
00:17:48,640 --> 00:17:54,360
but no reasonable person would think that this scalar field exists only for the purpose of making

233
00:17:54,360 --> 00:18:01,780
nice perturbations in the universe so we can go out and see a nice sky at night presumably this scalar field

234
00:18:01,780 --> 00:18:09,340
is related to some particle physics models and phenomenologically I want to

235
00:18:09,340 --> 00:18:14,640
captured and i the number of samples captain and is the number of samples

236
00:18:14,710 --> 00:18:19,250
for the base points and capital and is the number of samples used for the

237
00:18:19,250 --> 00:18:22,640
next state of those they those based point

238
00:18:22,690 --> 00:18:26,900
and you can prove born in this form

239
00:18:26,900 --> 00:18:30,510
and i'm not going to go into the details of this the good news is

240
00:18:30,510 --> 00:18:32,340
that this bald

241
00:18:32,600 --> 00:18:37,520
goes down to zero is and and and goes to infinity

242
00:18:37,530 --> 00:18:43,050
this is an interesting guy so this one has to terms this guy here

243
00:18:43,070 --> 00:18:47,200
are actually see terms this this guy here is called the the bomb and the

244
00:18:47,200 --> 00:18:48,760
estimation error

245
00:18:48,770 --> 00:18:51,510
so since you are using monte carlo

246
00:18:51,520 --> 00:18:55,940
everything you do is is more is little bit so you have some variance

247
00:18:55,950 --> 00:18:57,520
check out two

248
00:18:57,580 --> 00:19:02,420
reduce the variance the way he use a very interesting is the number of samples

249
00:19:02,470 --> 00:19:04,630
so the other turning the ball

250
00:19:04,640 --> 00:19:06,080
is this guy here

251
00:19:06,090 --> 00:19:08,780
so this is a distance between

252
00:19:08,790 --> 00:19:13,530
the image of the function space the bellman operator under bellman operator

253
00:19:13,550 --> 00:19:18,040
so hard to imagine that so you take your function space

254
00:19:24,570 --> 00:19:27,010
and you pick the function there are

255
00:19:27,020 --> 00:19:30,160
and you compute its image under thirty so maybe

256
00:19:30,230 --> 00:19:35,320
the maps this function outside of origin of function space maps here

257
00:19:35,360 --> 00:19:39,900
you do this for all the functions here or it's function space

258
00:19:39,940 --> 00:19:41,490
and you get he

259
00:19:41,500 --> 00:19:43,020
so the image space

260
00:19:43,020 --> 00:19:44,780
and what you do is that

261
00:19:44,790 --> 00:19:46,260
well you say that

262
00:19:46,510 --> 00:19:52,330
well i am doing value iteration my target function so i'm going to become from

263
00:19:52,330 --> 00:19:53,650
this set

264
00:19:53,660 --> 00:19:55,360
and what i'm doing fitting

265
00:19:55,370 --> 00:19:58,780
my functions actually coming from this set

266
00:19:58,790 --> 00:20:00,790
so if i can keep

267
00:20:00,990 --> 00:20:04,860
so the distance between these two sets small

268
00:20:04,870 --> 00:20:08,800
in then in each iteration my errors would be small so the

269
00:20:08,810 --> 00:20:10,920
finally should be small as well

270
00:20:10,930 --> 00:20:14,760
so what you do is you compute the distance which is defined in the following

271
00:20:14,760 --> 00:20:16,910
way so you begin the function

272
00:20:16,920 --> 00:20:21,570
you pick up you pick the function that's farthest away

273
00:20:21,580 --> 00:20:23,200
from f in

274
00:20:23,220 --> 00:20:26,610
so maybe this functions and is going to be the stuff

275
00:20:26,640 --> 00:20:28,900
so you want to design

276
00:20:28,970 --> 00:20:33,500
this function spaces such as such a way that this sense is small so

277
00:20:33,540 --> 00:20:40,140
that term is depicted in the regression it is required that call the approximation error

278
00:20:40,170 --> 00:20:42,450
so this is a little bit different here

279
00:20:42,550 --> 00:20:48,720
because we are doing good reinforcement learning and not just regression but otherwise is pretty

280
00:20:48,720 --> 00:20:51,390
much the same so you have the approximation error

281
00:20:51,400 --> 00:20:53,640
you have the estimation error bars

282
00:20:53,650 --> 00:20:59,020
and so you have this other term here that decays exponentially fast is the number

283
00:20:59,020 --> 00:21:00,270
of iterations

284
00:21:00,300 --> 00:21:04,900
and so you know there's the truncation error so after k iterations you stopped

285
00:21:04,910 --> 00:21:08,430
one of the resulting error is proportional to the government to the power of case

286
00:21:08,430 --> 00:21:09,400
so this is

287
00:21:09,520 --> 00:21:11,520
for the same reason as

288
00:21:11,570 --> 00:21:15,860
i've shown you before this just that works

289
00:21:15,900 --> 00:21:19,300
OK so the next question is how to choose

290
00:21:19,380 --> 00:21:22,070
the function space and

291
00:21:22,120 --> 00:21:26,060
well it seems that it would be a good idea to to use the latter

292
00:21:26,060 --> 00:21:29,830
function space but if you use the latter function space and may be heard about

293
00:21:30,630 --> 00:21:35,270
you you run the risk of for fitting this user more some less

294
00:21:35,280 --> 00:21:37,220
but if you know the number of

295
00:21:37,260 --> 00:21:38,900
some persist fixed

296
00:21:38,950 --> 00:21:42,850
well you're not allowed to increase the number of samples in other ways

297
00:21:42,860 --> 00:21:46,790
then if you increase the size of the function space then you are going to

298
00:21:46,790 --> 00:21:48,490
off the

299
00:21:50,400 --> 00:21:55,400
the moment of or if anything is a termite hall reach their function class is

300
00:21:55,400 --> 00:21:57,650
and there are various quantities

301
00:21:57,740 --> 00:21:59,490
how two

302
00:21:59,530 --> 00:22:01,520
quantify that saying

303
00:22:01,520 --> 00:22:04,760
and i'm not going to go into the deaths of that

304
00:22:04,770 --> 00:22:08,930
but the thing is that the fingers are function space

305
00:22:08,990 --> 00:22:11,550
to go back to the bond i was not

306
00:22:11,650 --> 00:22:14,140
very very not explaining what turned here

307
00:22:14,150 --> 00:22:18,060
we have this absolute got here so this depends on the size of the function

308
00:22:18,880 --> 00:22:22,970
so if you increase the size of the function space this increases

309
00:22:23,020 --> 00:22:27,170
this for degrees but this increases so there is this trade this

310
00:22:27,190 --> 00:22:31,860
sometimes called the bias variance tradeoff because for certain is approximation error that sort of

311
00:22:33,140 --> 00:22:34,390
the second term

312
00:22:34,400 --> 00:22:35,800
bombing the

313
00:22:35,810 --> 00:22:37,410
estimation error

314
00:22:37,430 --> 00:22:40,190
and that's sort of the very same

315
00:22:40,220 --> 00:22:41,150
and so

316
00:22:41,170 --> 00:22:45,520
you have this dilemma of how to choose a function space f

317
00:22:46,650 --> 00:22:51,120
it seems that their myspace of choosing the function space so

318
00:22:51,150 --> 00:22:57,230
for example you could have this archaeologists spaces and you could

319
00:22:57,250 --> 00:23:01,320
user in particular function space like that and that he is

320
00:23:01,330 --> 00:23:05,850
the capacity after as could be bonded by

321
00:23:05,940 --> 00:23:11,980
just the lower the number of samples in this case so that studies by chance

322
00:23:11,990 --> 00:23:13,070
and so

323
00:23:13,080 --> 00:23:16,400
the good news about this bond is that this is independent of

324
00:23:17,360 --> 00:23:19,150
the dimensionality of phi

325
00:23:19,150 --> 00:23:23,740
so you can really use this bizarre creatures which are infinite dimensional

326
00:23:23,760 --> 00:23:25,930
so the only

327
00:23:26,410 --> 00:23:28,470
lacking detail is that

328
00:23:28,480 --> 00:23:33,350
well i guess no one really understand what the corresponding class family for which this

329
00:23:33,350 --> 00:23:35,760
method would work pretty well

330
00:23:35,810 --> 00:23:36,880
would be

331
00:23:37,570 --> 00:23:41,250
that's again an open problem and

332
00:23:41,260 --> 00:23:45,010
good topic for period is this may

333
00:23:45,020 --> 00:23:50,220
OK so

334
00:23:50,270 --> 00:23:54,900
this is all what i wanted to say about planning

335
00:23:54,910 --> 00:24:01,610
and let's move on to learning

336
00:24:01,750 --> 00:24:18,230
so its contents so first i'm going to just define the problem and then i'm

337
00:24:18,230 --> 00:24:20,400
talking about

338
00:24:20,420 --> 00:24:27,040
it's the end of it

339
00:24:27,060 --> 00:24:29,950
that are accessible from either pages found

340
00:24:29,970 --> 00:24:33,940
but the official website is going to be updated with

341
00:24:34,020 --> 00:24:40,400
the newer slides from

342
00:24:40,450 --> 00:24:44,190
so i'm going to define the problem first and then

343
00:24:44,200 --> 00:24:48,250
the first stopping is is learning optimally allowing fast way

344
00:24:48,250 --> 00:24:52,130
the have scanning it was too rare and still to this day very very expensive

345
00:24:52,130 --> 00:24:53,770
so now he's moving across

346
00:24:53,830 --> 00:24:58,800
the transition metals he gets to copper and you can use zinc for reasons i'm

347
00:24:58,800 --> 00:25:02,930
going to show you very shortly namely that the energy in that to be so

348
00:25:02,930 --> 00:25:06,980
high that is going to melt physics so he says instead of use brass which

349
00:25:06,980 --> 00:25:09,230
is an alloy of copper and zinc

350
00:25:09,250 --> 00:25:12,310
and so this is this is the data from

351
00:25:12,390 --> 00:25:13,460
this study

352
00:25:13,530 --> 00:25:17,660
and so here's the first one of the first page of the paper the author

353
00:25:17,660 --> 00:25:22,030
intends to make a general survey of the principal types high-frequency radiation

354
00:25:22,050 --> 00:25:25,430
examine the specter of a few elements in greater detail

355
00:25:25,490 --> 00:25:30,190
results already obtained showed that such data have an important bearing on the question of

356
00:25:30,190 --> 00:25:33,110
the internal structure of the atom

357
00:25:33,120 --> 00:25:37,730
and further support the views of rutherford and of bore

358
00:25:37,740 --> 00:25:39,170
this is what he is

359
00:25:39,180 --> 00:25:43,940
this is what he's putting up here for us

360
00:25:46,860 --> 00:25:49,510
what i want to show is that

361
00:25:49,540 --> 00:25:51,760
when he took his results

362
00:25:51,840 --> 00:25:53,790
this is what he found he tried to

363
00:25:53,800 --> 00:25:58,600
get out a better functionality of this relationship between

364
00:25:58,670 --> 00:26:01,260
wavelengths an atomic weights

365
00:26:01,270 --> 00:26:03,460
here's what he found work best

366
00:26:03,470 --> 00:26:05,730
he found that if he plotted

367
00:26:05,740 --> 00:26:11,140
not the wavelength but the wave number we've seen before the wave number as a

368
00:26:11,140 --> 00:26:14,440
function of proton number

369
00:26:15,260 --> 00:26:18,830
in fact the square of the proton number

370
00:26:18,840 --> 00:26:21,420
he got a linear relationship

371
00:26:22,610 --> 00:26:28,310
and let's just say this is this would be l alpha

372
00:26:28,370 --> 00:26:29,490
l beta

373
00:26:29,630 --> 00:26:31,210
o k alpha

374
00:26:32,910 --> 00:26:34,380
let's say you're copper

375
00:26:34,390 --> 00:26:37,260
so copper would be twenty nine

376
00:26:37,260 --> 00:26:42,520
square so he would get he would get data for the l alpha line of

377
00:26:42,520 --> 00:26:49,050
copper they'll be copper keil full copper KB line of copper he might not get

378
00:26:49,050 --> 00:26:53,880
all four lines for every element but he would take sets of data and plot

379
00:26:53,880 --> 00:26:58,510
them only found that if you plotted the reciprocal of the wavelength versus the square

380
00:26:58,510 --> 00:27:04,380
of the proton number the points lay online so over here would be eliminated

381
00:27:04,390 --> 00:27:05,830
and over here would be

382
00:27:08,050 --> 00:27:09,880
over here would be gold

383
00:27:09,900 --> 00:27:15,780
and this was extremely important this changed so much because look at what he's able

384
00:27:15,780 --> 00:27:18,430
to conclude here let's let's read his

385
00:27:18,440 --> 00:27:23,490
this paper here we have you're proved that there is in the atom of fundamental

386
00:27:23,490 --> 00:27:28,730
quantity which increases by regular steps as we pass from one element to the next

387
00:27:28,740 --> 00:27:33,130
this quantity can only be the charge on the central positive nucleus

388
00:27:33,160 --> 00:27:37,210
of the existence of which we have already definite proof

389
00:27:37,220 --> 00:27:42,650
and i remember when mandalay have enunciated the law of periodicity he said that the

390
00:27:42,650 --> 00:27:47,360
properties of the elements are a function of the atomic weight

391
00:27:47,370 --> 00:27:52,730
the function of the atomic weight and people were perplexed by the fact that potassium

392
00:27:53,940 --> 00:27:59,720
less massive than our but no one would put our gone under sony here

393
00:27:59,740 --> 00:28:05,620
but mandolins law is that you arrange things by atomic mass

394
00:28:05,660 --> 00:28:07,360
cobalt and nickel

395
00:28:07,390 --> 00:28:09,050
i reversed

396
00:28:09,100 --> 00:28:10,640
what to do

397
00:28:10,640 --> 00:28:15,580
here's mostly we are therefore led by experiment to view that and and his capital

398
00:28:15,580 --> 00:28:18,170
and is what we call proton number

399
00:28:18,230 --> 00:28:22,190
is the same as the number of the place occupied by the element in the

400
00:28:22,190 --> 00:28:23,620
periodic system

401
00:28:23,630 --> 00:28:29,420
this atomic number is coining the term this atomic numbers then for hydrogen one for

402
00:28:29,420 --> 00:28:34,590
helium two for lithium three four calcium twenty four zinc thirty et cetera

403
00:28:34,630 --> 00:28:38,290
we can confidently predict that in a few cases in which the order of the

404
00:28:38,290 --> 00:28:43,960
atomic weights a clashes with the atomic or the periodic system the chemical properties are

405
00:28:43,960 --> 00:28:45,390
governed by and

406
00:28:45,480 --> 00:28:47,240
while a itself

407
00:28:47,250 --> 00:28:50,380
probably a complicated function of

408
00:28:50,390 --> 00:28:57,380
this is brilliant this is a graduate student in nineteen thirty

409
00:28:57,390 --> 00:29:00,500
correcting mandalay if

410
00:29:01,140 --> 00:29:05,590
what are the implications what are the implications of this proton number we're now going

411
00:29:05,590 --> 00:29:08,330
to give it its do we're going to call it z

412
00:29:08,350 --> 00:29:12,470
the atomic number this is the social security number of every atom

413
00:29:12,480 --> 00:29:14,840
the atomic number

414
00:29:14,880 --> 00:29:16,180
atomic number

415
00:29:16,880 --> 00:29:18,740
what's the significance of

416
00:29:18,800 --> 00:29:23,010
this paper first of all corrected mandalay

417
00:29:23,050 --> 00:29:26,900
not to say that mandalay was

418
00:29:26,910 --> 00:29:30,410
back and the live was brilliant and what he did but there are a few

419
00:29:30,410 --> 00:29:34,860
things that live couldn't account for so now we know

420
00:29:34,900 --> 00:29:37,360
in the post mostly world

421
00:29:37,370 --> 00:29:39,380
that periodicity

422
00:29:39,420 --> 00:29:45,970
periodicity that is the periodic variation in properties is not a function of

423
00:29:47,090 --> 00:29:52,920
mass it's no longer function of atomic mass as per mandalay have but rather a

424
00:29:52,930 --> 00:29:55,650
function of atomic number

425
00:29:55,740 --> 00:29:58,510
the function of atomic number

426
00:29:58,520 --> 00:29:59,720
OK and this

427
00:29:59,730 --> 00:30:02,660
now resolve the problems with are gone

428
00:30:06,280 --> 00:30:08,600
cobalt nickel

429
00:30:08,640 --> 00:30:11,300
to learning and iodine

430
00:30:11,360 --> 00:30:13,510
these three were known

431
00:30:13,560 --> 00:30:16,980
these three were known in mandalay have kept saying going measure them again the atomic

432
00:30:16,980 --> 00:30:18,660
masses are wrong

433
00:30:18,780 --> 00:30:23,510
and he was right in certain instances they did find a number of

434
00:30:23,560 --> 00:30:29,000
in the incorrect values but these were among that group and lastly appear that they

435
00:30:29,000 --> 00:30:30,640
wouldn't have known uranium

436
00:30:30,650 --> 00:30:32,760
and that too many more in the wrong

437
00:30:33,510 --> 00:30:37,160
he said OK well that's that's cute but let's show something else

438
00:30:37,220 --> 00:30:43,650
by understanding that the atomic number is the critical factor in determining where something belongs

439
00:30:43,650 --> 00:30:47,160
in the periodic table he was able to

440
00:30:47,220 --> 00:30:50,670
place the land finite if you look at most periodic tables from the early part

441
00:30:50,670 --> 00:30:55,260
of the twentieth century they don't know where to put the there their try valence

442
00:30:55,260 --> 00:30:58,850
so they tend to put them under alone maybe understanding they don't know what to

443
00:31:00,150 --> 00:31:04,380
so with this he was able to place the land tonight's

444
00:31:05,660 --> 00:31:08,380
and more importantly he was able to

445
00:31:11,080 --> 00:31:15,770
predict that there are fourteen of

446
00:31:15,780 --> 00:31:18,810
there are fourteen all

447
00:31:18,860 --> 00:31:21,320
how do you do that

448
00:31:21,330 --> 00:31:22,720
it's really simple

449
00:31:24,020 --> 00:31:26,380
it was discovered in nineteen thirty nine

450
00:31:26,390 --> 00:31:28,640
eleven was discovered in nineteen thirty nine

451
00:31:28,690 --> 00:31:31,100
it has an atomic mass

452
00:31:31,130 --> 00:31:32,990
one hundred thirty eight points

453
00:31:33,020 --> 00:31:36,910
nine one and there are various other landslides knives that had been discovered

454
00:31:36,930 --> 00:31:41,090
and i'm going to put in the TCM which had been discovered only recently in

455
00:31:41,090 --> 00:31:42,500
nineteen o seven

456
00:31:42,500 --> 00:31:43,860
that's cheque

457
00:31:44,020 --> 00:32:18,610
much happier

458
00:32:18,650 --> 00:32:21,670
that is

459
00:32:22,920 --> 00:32:26,900
all rights which

460
00:32:32,420 --> 00:32:35,060
persons law

461
00:32:35,080 --> 00:32:37,150
that should be

462
00:32:37,310 --> 00:32:42,060
that we get that

463
00:32:44,210 --> 00:32:47,750
she become more apparent

464
00:32:47,790 --> 00:32:53,040
in in classical logic we don't get that intuitionist logic OK so what's the difference

465
00:32:53,040 --> 00:32:55,290
in difference is going to be the negation

466
00:32:55,290 --> 00:32:57,860
fragment racer systems

467
00:32:57,900 --> 00:33:00,560
in case is wrong and i will leave it up to the next person saying

468
00:33:00,560 --> 00:33:04,110
as personal i think lookup chris long google

469
00:33:04,130 --> 00:33:18,400
and you find the statement of charge anyway that's what you get with classical logic

470
00:33:18,400 --> 00:33:23,460
and not intuitionist logic i should know these things i'm inclined lag

471
00:33:25,190 --> 00:33:27,290
OK now what else

472
00:33:28,920 --> 00:33:34,600
it's the intuition the intuitionist negation is what's going to get to us OK which

473
00:33:34,610 --> 00:33:39,400
the negation rules

474
00:33:39,580 --> 00:33:49,100
OK the negation introduction rule

475
00:33:49,130 --> 00:33:57,540
is fairly straightforward

476
00:33:57,610 --> 00:34:04,730
the negation

477
00:34:05,190 --> 00:34:11,460
elimination rule that i'm giving it's not the usual

478
00:34:11,560 --> 00:34:13,190
straightforward to

479
00:34:13,210 --> 00:34:14,150
he said OK

480
00:34:14,170 --> 00:34:18,170
all those that we actually many another rule

481
00:34:18,210 --> 00:34:20,520
what are the world we need

482
00:34:20,560 --> 00:34:23,610
i mean

483
00:34:23,750 --> 00:34:26,270
o point which instigation

484
00:34:26,270 --> 00:34:28,710
we introduce this thing called as

485
00:34:28,710 --> 00:34:30,670
we know the rules to govern it

486
00:34:30,690 --> 00:34:34,420
so rule which i think would you use ref

487
00:34:34,420 --> 00:34:40,400
f is also to foster

488
00:34:45,980 --> 00:34:50,610
anything follows from that

489
00:34:50,670 --> 00:34:54,100
OK that's coming important thing for us and the

490
00:34:54,110 --> 00:34:58,040
and that's in intuitionist logic so anything follows from a contradiction f stands for the

491
00:34:59,480 --> 00:35:03,020
because in intuitionist logic unlike relevant articles with say

492
00:35:03,020 --> 00:35:04,980
all contradictions are equal

493
00:35:05,020 --> 00:35:12,130
are all equivalent

494
00:35:12,170 --> 00:35:14,980
any contradiction leads to everything

495
00:35:15,020 --> 00:35:18,670
so it is intuitionist logic is like classical logic

496
00:35:18,690 --> 00:35:20,460
now what happens

497
00:35:20,480 --> 00:35:27,940
OK sorry that's are that f elimination and f elimination rule

498
00:35:28,130 --> 00:35:34,900
with the introduction rule

499
00:35:34,940 --> 00:35:37,810
we think

500
00:35:37,900 --> 00:35:42,480
well this is actually kind of the same story this is the introduction rule

501
00:35:43,690 --> 00:35:47,250
usually what people say there isn't one introduction

502
00:35:48,330 --> 00:35:51,190
you can never actually get to a point where you can

503
00:35:51,210 --> 00:36:00,360
there should be also there but it's true this is really the introduction

504
00:36:00,360 --> 00:36:02,540
so we don't need any more rules

505
00:36:07,190 --> 00:36:11,500
what about that one

506
00:36:11,520 --> 00:36:13,670
let's get rid of the person

507
00:36:13,750 --> 00:36:18,000
if we get rid of that we actually get a different larger corsica different logic

508
00:36:18,040 --> 00:36:20,150
if you have rules

509
00:36:20,170 --> 00:36:22,670
one in which f does not

510
00:36:22,690 --> 00:36:25,290
give us

511
00:36:28,000 --> 00:36:35,920
that logic is called minimal logic

512
00:36:35,940 --> 00:36:40,100
and that's something you should have written and you know it

513
00:36:40,110 --> 00:36:43,150
is intuitionist logic minus

514
00:36:43,150 --> 00:36:43,940
and so courses

515
00:36:44,390 --> 00:36:45,220
a huge body of

516
00:36:45,760 --> 00:36:50,050
research on a news about that when it converges in superlinear convergence it

517
00:36:50,400 --> 00:36:51,540
and so on and so forth and

518
00:36:53,340 --> 00:36:55,650
all that's research is a special case of

519
00:36:56,130 --> 00:36:57,110
interior point methods

520
00:36:59,160 --> 00:36:59,480
and so

521
00:37:00,390 --> 00:37:03,610
it doesn't always converge this only converges if x is

522
00:37:05,070 --> 00:37:06,310
less than or equal to one

523
00:37:06,810 --> 00:37:11,430
now you can make newton's method converge by not being so bold as taking a

524
00:37:11,430 --> 00:37:13,460
whole step but shortening the step

525
00:37:15,970 --> 00:37:17,630
so you have to try to do something like that

526
00:37:21,140 --> 00:37:22,180
one of the things that

527
00:37:22,820 --> 00:37:23,370
some people

528
00:37:23,870 --> 00:37:24,490
of of

529
00:37:25,740 --> 00:37:28,410
implemented is is is a filter type methods

530
00:37:29,220 --> 00:37:31,180
to help guide the choice of the steplength

531
00:37:33,190 --> 00:37:34,770
anthony tried it briefly

532
00:37:35,460 --> 00:37:37,460
give some hints as to what this is about

533
00:37:39,530 --> 00:37:41,480
we end up with this hash and matrix

534
00:37:41,960 --> 00:37:44,740
plus a transpose w inverse why eh

535
00:37:48,950 --> 00:37:49,600
well i guess

536
00:37:52,460 --> 00:37:55,810
you can see that when i saw when i solve these equations at least one

537
00:37:55,890 --> 00:37:58,070
the ways i got something very much like this

538
00:37:58,450 --> 00:37:58,840
i got

539
00:37:59,390 --> 00:38:04,460
i i this works see what this was my matrix this was my passion matrix

540
00:38:04,950 --> 00:38:06,560
so x inverses my

541
00:38:07,570 --> 00:38:10,040
o so identity and the way that the dual

542
00:38:10,830 --> 00:38:14,090
this is done by the dual i guess like all the dual form matrix

543
00:38:14,800 --> 00:38:15,590
do that

544
00:38:16,480 --> 00:38:19,510
but it anyway use all

545
00:38:20,070 --> 00:38:21,920
over there that i got something that looks like this

546
00:38:22,530 --> 00:38:26,230
if you go in the other direction and somebody other variable first

547
00:38:27,200 --> 00:38:29,610
delta delta x and so delta why here

548
00:38:30,800 --> 00:38:35,980
you get a image a matrix that looks like this multiplying your system equations

549
00:38:36,430 --> 00:38:41,110
you get eight transposed times eight with the diagonal matrix in the middle

550
00:38:42,790 --> 00:38:43,660
the piece that was up

551
00:38:45,620 --> 00:38:47,290
are were was

552
00:38:47,860 --> 00:38:49,040
yeah appear age

553
00:38:49,900 --> 00:38:50,770
the hessian matrix

554
00:38:52,620 --> 00:38:56,390
and so that's the matrix that you have this invert

555
00:38:57,620 --> 00:38:59,850
to get your delta x is in this case

556
00:39:00,320 --> 00:39:01,430
because it's the dual it

557
00:39:02,070 --> 00:39:04,890
end and so this is the matrix that determines everything

558
00:39:06,420 --> 00:39:07,930
and if this is positive definite

559
00:39:08,380 --> 00:39:12,370
then than than than than has a unique inverse and you can

560
00:39:14,080 --> 00:39:15,080
calculate your delta

561
00:39:15,650 --> 00:39:17,580
x and u delta one delta w

562
00:39:19,260 --> 00:39:24,890
so if the so what we see with this theorem say exactly suppose that this matrix is positive definite

563
00:39:25,390 --> 00:39:28,090
if the current solution is primal and feasible

564
00:39:28,660 --> 00:39:29,520
the delta x

565
00:39:30,270 --> 00:39:32,980
delta w the step direction and the primal side

566
00:39:33,390 --> 00:39:39,210
is a descent direction for the infeasibility vector not for the measurement is ability the norm

567
00:39:39,820 --> 00:39:40,980
aged x minus w

568
00:39:42,350 --> 00:39:43,700
and so far invisible

569
00:39:44,660 --> 00:39:49,920
and this is positive definite then one step improves the situation with respect feasibility

570
00:39:51,160 --> 00:39:52,350
if the current solution

571
00:39:53,090 --> 00:39:53,840
is feasible

572
00:39:54,540 --> 00:39:56,820
then delta x delta w is the set

573
00:39:57,490 --> 00:39:59,010
direction for the barrier function

574
00:39:59,980 --> 00:40:03,090
that's all the objective function with the very terms added in

575
00:40:03,800 --> 00:40:06,330
and as you get smaller becomes close to the objective function

576
00:40:09,670 --> 00:40:10,980
so when more feasible

577
00:40:12,470 --> 00:40:18,420
things improve with every iteration be infeasibility gets smaller with every iteration would once we get feasible

578
00:40:18,880 --> 00:40:22,640
the barrier function gets better with every iteration and if we decrease mu at every

579
00:40:22,640 --> 00:40:27,340
iteration the barrier function gets closer and closer to the actual objective function and this

580
00:40:27,340 --> 00:40:29,060
all this says it all work

581
00:40:30,050 --> 00:40:31,880
but it always has a descent direction

582
00:40:32,510 --> 00:40:34,710
that means initially for small enough

583
00:40:35,320 --> 00:40:39,340
step it will work so and i added do the line search the figure out

584
00:40:40,280 --> 00:40:40,980
how help

585
00:40:41,450 --> 00:40:46,520
how big a step like you can take that still maintains that you have these properties

586
00:40:47,760 --> 00:40:49,320
so from linear programming that was

587
00:40:49,720 --> 00:40:50,730
descent direction and

588
00:40:51,310 --> 00:40:55,040
and it continued to be a descent direction causes linear nothing there is no curvature

589
00:40:55,040 --> 00:40:57,740
to it and was going down it was always going down

590
00:40:59,140 --> 00:41:02,660
you know it's only initially good and so you have to do a line search the

591
00:41:03,440 --> 00:41:06,060
figure out how far you can go before things start to go bad

592
00:41:07,570 --> 00:41:08,940
so you have to take shorter steps

593
00:41:10,010 --> 00:41:11,770
that's for convex optimization

594
00:41:11,770 --> 00:41:13,900
works just fine

595
00:41:13,920 --> 00:41:19,650
so this is the paper on shift and rotation invariant reconstruction but again it's either

596
00:41:19,650 --> 00:41:25,190
shift or rotation invariant actually this is the thing is quite useful

597
00:41:25,210 --> 00:41:31,310
in time series analysis like e g PG EEG signals people actually use this sort

598
00:41:31,310 --> 00:41:33,190
of representation when we want to

599
00:41:33,190 --> 00:41:39,130
recognise patterns in that gained what we're interested in is generalizing this theory

600
00:41:39,150 --> 00:41:40,230
to the case

601
00:41:40,230 --> 00:41:47,840
of noncommutative groups for such as translation and rotation together

602
00:41:48,540 --> 00:41:52,940
and this leads us to one music by spectral

603
00:41:53,110 --> 00:41:55,040
so the idea

604
00:41:55,040 --> 00:41:55,940
is that

605
00:41:55,940 --> 00:41:59,980
what drives classical by spectra

606
00:41:59,980 --> 00:42:04,290
it is still present in the noncommutative constraints in the world when we talk about

607
00:42:04,290 --> 00:42:09,360
the general properties of harmonic analysis on groups we saw something very similar to what

608
00:42:09,360 --> 00:42:13,880
we have to except that now of course we're are dealing with matrices

609
00:42:16,650 --> 00:42:20,590
what you know this fact

610
00:42:20,610 --> 00:42:21,730
we can

611
00:42:22,730 --> 00:42:25,480
we can prove that these proposed the theory of

612
00:42:25,480 --> 00:42:30,400
power spectrum by spectral even on the music groups so

613
00:42:30,440 --> 00:42:35,570
for example we can write down a power spectrum in this form

614
00:42:35,590 --> 00:42:40,960
which is just the product of the four

615
00:42:40,980 --> 00:42:44,270
transfer matrices with complex conjugate

616
00:42:47,210 --> 00:42:49,000
translation property

617
00:42:49,020 --> 00:42:52,190
it's going to be invariant to again

618
00:42:52,210 --> 00:42:53,500
maybe i should

619
00:42:53,730 --> 00:42:55,500
i remind you that under

620
00:42:55,540 --> 00:42:57,560
left translation

621
00:42:57,560 --> 00:43:03,690
so it's the same thing here is that we have a function

622
00:43:03,690 --> 00:43:07,920
so the complex numbers

623
00:43:07,960 --> 00:43:09,000
then we

624
00:43:09,020 --> 00:43:12,000
to its fourier transform

625
00:43:12,810 --> 00:43:16,880
it's going to be a bunch of matrices indexed by the irreducible representations

626
00:43:16,900 --> 00:43:22,980
and we knew that if we translate the function of effect on the fourier transform

627
00:43:22,980 --> 00:43:24,380
is going to be

628
00:43:24,460 --> 00:43:27,500
left multiplying this by

629
00:43:27,520 --> 00:43:30,090
representation matrices

630
00:43:30,110 --> 00:43:40,090
so just as in the euclidean case if we take the fourier matrices and multiply

631
00:43:41,670 --> 00:43:46,730
complex by their own admission conjugates

632
00:43:54,790 --> 00:43:59,210
we we call this the power spectrum which they use the q

633
00:43:59,230 --> 00:44:06,150
then the power spectrum of translated functions

634
00:44:06,170 --> 00:44:16,790
it's going to be the same except that we have these representations actresses appearing

635
00:44:16,810 --> 00:44:19,440
the same is also a function

636
00:44:19,460 --> 00:44:23,480
but we have the representation matrices appear

637
00:44:33,440 --> 00:44:40,500
and then provided that we started out with unitary representations and our set of irreducible

638
00:44:40,590 --> 00:44:44,310
representations representation matrices and so

639
00:44:44,310 --> 00:44:46,020
and we see that

640
00:44:46,060 --> 00:44:50,540
power spectrum is invariant

641
00:44:50,610 --> 00:44:54,330
but again we lose a lot of information because if you take a matrix and

642
00:44:54,330 --> 00:45:00,420
multiplied by its omission conjugate bringing us get positive definite matrix so these positive definite

643
00:45:00,420 --> 00:45:06,180
matrices can't possibly capture everything about the origin of fourier transforms

644
00:45:06,190 --> 00:45:10,430
the question is how do we generalize this idea we introduce the concept of by

645
00:45:10,430 --> 00:45:15,090
spectrum so how do we now a couple of the different noncommutative

646
00:45:15,760 --> 00:45:19,250
forty components together

647
00:45:19,260 --> 00:45:24,590
so we started out trying to do something very similar as in the euclidean case

648
00:45:24,630 --> 00:45:27,890
we start that we know that we're going to be looking some out looking good

649
00:45:27,890 --> 00:45:29,460
looking at cubic

650
00:45:29,480 --> 00:45:32,020
things in the original signals so we

651
00:45:32,180 --> 00:45:34,050
to try and combine

652
00:45:34,060 --> 00:45:39,760
three forty components and not just to performed by spectrum and think about how to

653
00:45:39,760 --> 00:45:41,390
get the same sort of

654
00:45:44,720 --> 00:45:45,570
in the

655
00:45:45,600 --> 00:45:48,650
translator and the translation as we had in the original

656
00:45:48,670 --> 00:45:52,730
by spectral so how do we combined three fourier components now in such a way

657
00:45:52,730 --> 00:45:56,760
that the combination is going to be invariant and in such a way that also

658
00:45:56,760 --> 00:46:02,520
captures all the relative phase information so now that we so we have completeness on

659
00:46:02,550 --> 00:46:05,390
noncommutative groups

660
00:46:05,430 --> 00:46:11,220
in the classical case we just multiply together the first two components right in saying

661
00:46:11,220 --> 00:46:14,920
that the classical bispectrum look like

662
00:46:15,000 --> 00:46:19,680
one three components that k one

663
00:46:19,680 --> 00:46:26,050
times another african k two

664
00:46:26,100 --> 00:46:31,070
these two conjugated and multiply by

665
00:46:31,100 --> 00:46:33,550
the k one

666
00:46:33,570 --> 00:46:36,140
and this was not because these two

667
00:46:37,290 --> 00:46:39,890
behave in a complementary way to this

668
00:46:40,890 --> 00:46:47,000
individual for companies behave in way prescribed fashion under translation and then their product behave

669
00:46:47,010 --> 00:46:47,760
well two

670
00:46:48,270 --> 00:46:49,570
if we take these

671
00:46:49,600 --> 00:46:51,430
matrices now

672
00:46:51,440 --> 00:46:57,470
three matrices in the noncommutative case and just to ordinary matrix multiplication unfortunately that's not

673
00:46:57,470 --> 00:47:01,500
going to do the trick that's not going to transform in simple way

674
00:47:01,520 --> 00:47:07,650
instead we have to take the direct product because again the direct product of representations

675
00:47:07,650 --> 00:47:08,630
is game

676
00:47:08,640 --> 00:47:11,100
representation so

677
00:47:11,180 --> 00:47:13,390
the direct product of fourier

678
00:47:13,390 --> 00:47:15,750
component is going to

679
00:47:15,760 --> 00:47:24,110
transform according to the corresponding direct product of representation matrices so on the one side

680
00:47:24,140 --> 00:47:29,260
we're going to have the direct product of fourier matrices the question is what do

681
00:47:29,260 --> 00:47:34,720
we multiply that with on the other side so as to get invariance

682
00:47:34,810 --> 00:47:37,220
and this is the same

683
00:47:37,250 --> 00:47:39,170
time sort of problem as we had

684
00:47:39,390 --> 00:47:43,550
just before the break which is fermented harmonic analysis in the first half of the

685
00:47:44,640 --> 00:47:46,540
this is how to

686
00:47:46,550 --> 00:47:49,350
decompose as direct product

687
00:47:53,710 --> 00:47:57,330
representations so this is the questions

688
00:47:59,760 --> 00:48:04,330
how to take two representation had to take the two REP representation through one hundred

689
00:48:04,940 --> 00:48:10,150
multiply them together and then decomposed into a sum of irreducible

690
00:48:10,170 --> 00:48:16,510
and we know that this always exists might be difficult to compute but since this

691
00:48:16,510 --> 00:48:21,800
is just the representation and we know that representation is always reducible into sum of

692
00:48:21,800 --> 00:48:23,760
irreducible as we know that

693
00:48:23,800 --> 00:48:25,880
some matrices c

694
00:48:25,900 --> 00:48:33,650
like constant matrices will exist in some sequence of irreducible here exist such that this

695
00:48:33,650 --> 00:48:36,050
holds for any x

696
00:48:36,070 --> 00:48:38,570
we can just

697
00:48:38,610 --> 00:48:40,440
like take

698
00:48:40,470 --> 00:48:47,090
together big but from representation theory from library look up the corresponding text gordon

699
00:48:47,090 --> 00:48:50,730
so that's what do recall color these guys

700
00:48:50,750 --> 00:48:52,280
and get

701
00:48:52,300 --> 00:48:54,420
ten which is red

702
00:48:54,460 --> 00:48:57,670
get eight which is black

703
00:48:57,750 --> 00:49:00,130
eleven which is black

704
00:49:01,170 --> 00:49:03,360
these things to change

705
00:49:03,420 --> 00:49:09,030
everything else doesn't change really fifteen red it's no longer in violation

706
00:49:09,050 --> 00:49:11,650
fifteen straight announced parodies black

707
00:49:11,650 --> 00:49:15,210
we now have new violation up here with the team

708
00:49:15,250 --> 00:49:18,610
eighteen is also read

709
00:49:18,630 --> 00:49:23,480
the only violation we have in general have most one violation any time

710
00:49:23,510 --> 00:49:25,000
so we fix it

711
00:49:25,070 --> 00:49:27,270
and we'll have zero emissions

712
00:49:39,980 --> 00:49:46,590
now we have a violation between ten and eighteen

713
00:49:46,630 --> 00:49:48,340
somehow always counterintuitive

714
00:49:48,420 --> 00:49:50,230
you have to look at the youtube again

715
00:49:53,920 --> 00:49:59,000
OK good and so we can re colouring more

716
00:49:59,010 --> 00:50:01,750
now that so

717
00:50:01,750 --> 00:50:04,940
what we'd like to do is again look at the grandparent of ten which is

718
00:50:04,940 --> 00:50:08,300
now so the root of the tree it is black

719
00:50:08,300 --> 00:50:11,670
but one of its children is black the other red so we can play the

720
00:50:11,670 --> 00:50:16,280
same game of taking the black seven and moving down to the two children never

721
00:50:16,280 --> 00:50:19,940
mind that the response to stay black will ignore that property for now

722
00:50:19,960 --> 00:50:24,270
we can make these two black this one red because then we get an imbalance

723
00:50:24,270 --> 00:50:25,820
this was already black

724
00:50:25,840 --> 00:50:26,730
so now

725
00:50:26,750 --> 00:50:31,900
has going down here will have one fewer black then has going down here

726
00:50:31,920 --> 00:50:34,770
so we can just three color seven as children

727
00:50:34,780 --> 00:50:38,900
so i said we've got to rotation better be near the end

728
00:50:38,920 --> 00:50:41,940
so what i will do is rotate this edge

729
00:50:41,960 --> 00:50:43,420
i'm going to rotate eight

730
00:50:43,420 --> 00:50:45,780
to the right

731
00:50:45,800 --> 00:50:46,940
that's the next

732
00:51:11,920 --> 00:51:20,860
the only one more operation after this so

733
00:51:20,900 --> 00:51:25,340
we're right eighteen so there it stays the same seven three

734
00:51:25,460 --> 00:51:27,170
its children

735
00:51:27,190 --> 00:51:32,800
now the right child of seven is no longer now ten

736
00:51:32,880 --> 00:51:36,710
eighteen becomes the right child ten

737
00:51:37,940 --> 00:51:43,090
eight over here too

738
00:51:44,030 --> 00:51:48,800
eleven and team that subtree in between ten and eighteen

739
00:51:48,820 --> 00:51:50,980
so it goes here

740
00:51:51,000 --> 00:51:56,780
eleven fifteen

741
00:51:56,900 --> 00:52:05,250
and then there's the right subtree everything to the right of eighteen that goes over

742
00:52:06,340 --> 00:52:08,130
twenty two

743
00:52:08,150 --> 00:52:09,920
and twenty six

744
00:52:09,920 --> 00:52:16,860
we're not changing any colours during that operation

745
00:52:16,880 --> 00:52:19,010
i did let me know

746
00:52:19,300 --> 00:52:20,650
looks good

747
00:52:20,670 --> 00:52:26,460
so i still have this violation still in trouble between ten and eighteen but i've

748
00:52:26,460 --> 00:52:28,440
made this straighter

749
00:52:28,500 --> 00:52:32,250
that's what we want to do it turns out is make the connection between eighteen

750
00:52:32,250 --> 00:52:35,380
the violator and it's grandparents straight

751
00:52:35,400 --> 00:52:37,270
connection to write sort to less

752
00:52:37,280 --> 00:52:41,150
here we have a zigzag right left like to make it straight

753
00:52:41,170 --> 00:52:45,630
it doesn't look like much more balanced tree this one fact it looks a worse

754
00:52:45,710 --> 00:52:47,250
but what we can do is now

755
00:52:47,250 --> 00:52:48,820
i rotate these guys

756
00:52:49,280 --> 00:52:50,360
or rather

757
00:52:50,380 --> 00:52:52,010
protect the search

758
00:52:52,030 --> 00:52:55,780
can rotate seventy two the the left

759
00:52:55,800 --> 00:52:57,070
thanks to ten the room

760
00:52:57,110 --> 00:52:59,980
and then things will start to look balanced

761
00:53:01,030 --> 00:53:05,500
this is a rotate left

762
00:53:10,900 --> 00:53:13,000
and i also going some recovery

763
00:53:14,070 --> 00:53:15,300
the same time

764
00:53:15,320 --> 00:53:17,920
just the same injuring one more picture

765
00:53:17,940 --> 00:53:22,710
because the rule has to be black i'm gonna make ten black immediately

766
00:53:22,900 --> 00:53:25,070
make seven red

767
00:53:25,090 --> 00:53:28,460
that's the change and the rest is just the rotation

768
00:53:28,570 --> 00:53:33,670
so we have eighteen over here

769
00:53:33,690 --> 00:53:37,230
i think i actually have to rotate in order to keep some red black

770
00:53:37,230 --> 00:53:38,460
this year

771
00:53:42,840 --> 00:53:48,190
eight comes to be between seven and ten so goes here

772
00:53:48,210 --> 00:53:54,270
eleven goes between ten and eighteen

773
00:53:54,280 --> 00:53:55,920
so it goes here

774
00:54:03,650 --> 00:54:07,840
twenty two and twenty six come after eighteen

775
00:54:14,400 --> 00:54:17,780
if i'm lucky i should satisfy all the properties of that one

776
00:54:17,780 --> 00:54:22,750
every node is red or black every red node has a black child and this

777
00:54:22,750 --> 00:54:27,320
is the last place change have headline children and all the black kites

778
00:54:27,320 --> 00:54:30,210
should be well defined for every node

779
00:54:30,230 --> 00:54:34,750
the number of black nodes along any road any node to leave

780
00:54:34,780 --> 00:54:36,400
is the same

781
00:54:36,420 --> 00:54:41,280
and check that was true before and i do a little bit of trickery with

782
00:54:41,320 --> 00:54:44,570
the re colouring here but it's still true

783
00:54:44,840 --> 00:54:47,300
you can check that is locally around

784
00:54:47,320 --> 00:54:48,550
this rotation

785
00:54:48,570 --> 00:54:50,750
OK will do that a little bit for now

786
00:54:50,770 --> 00:54:51,980
it's just an example

787
00:54:52,010 --> 00:54:56,340
probably not terribly clear where these re colorings and rotations come from necessarily but it

788
00:54:57,090 --> 00:55:00,420
these convinces you that it's possible

789
00:55:00,440 --> 00:55:03,230
and now i will give the general algorithm for doing it

790
00:55:03,250 --> 00:55:05,030
any questions before we go on

791
00:55:11,150 --> 00:55:13,860
it's not exactly

792
00:55:13,880 --> 00:55:17,880
we just running down the algorithm is not terribly intuitive

793
00:55:17,920 --> 00:55:21,010
it was red black trees the sort of thing where

794
00:55:21,030 --> 00:55:24,190
you play around a bit new to you say OK i'm going to just think

795
00:55:24,190 --> 00:55:29,460
about we colorings and rotation let's restrict myself to those operations what can i do

796
00:55:29,460 --> 00:55:33,480
try to re color that works great pushes the problem of higher

797
00:55:33,500 --> 00:55:37,780
and there's only logan levels or order level so it's going to take a look

798
00:55:37,780 --> 00:55:41,500
at the time at some point i'll get stuck can color any more than it

799
00:55:41,500 --> 00:55:43,980
turns out a couple of rotations will do it

800
00:55:44,010 --> 00:55:47,050
always two rotations will suffice

801
00:55:47,090 --> 00:55:50,650
and you just play with it and that turns out to work

802
00:55:50,650 --> 00:55:55,010
and here's how

803
00:55:55,010 --> 00:55:58,630
function right so here is the influence functions of the first one of the site

804
00:55:58,780 --> 00:56:02,280
when it starts when you mention information and then i have the one of the

805
00:56:02,280 --> 00:56:04,660
real and i have the one of the w

806
00:56:04,680 --> 00:56:09,530
the way i can predict the volume in the future is just a the the

807
00:56:09,630 --> 00:56:13,840
influence the mass of the influence functions at this point in the future i want

808
00:56:13,840 --> 00:56:18,280
to make prediction some to sum them up and that is my prediction right so

809
00:56:18,300 --> 00:56:20,950
really what i've been trying to do is i would say OK how can i

810
00:56:20,950 --> 00:56:24,630
estimate the shapes so that whenever i want to bring them all and in the

811
00:56:24,630 --> 00:56:29,490
future this this this shapes of this influence functions will add up to the to

812
00:56:29,490 --> 00:56:35,050
the value in the future right so really that is that is the thing that

813
00:56:35,050 --> 00:56:39,030
they want to do and as i said my prediction about the popularity of of

814
00:56:39,030 --> 00:56:41,700
an item in the future we seem to be that you simply be the sum

815
00:56:41,950 --> 00:56:47,700
of appropriately aligned influence functions right and what i mean by the line because different

816
00:56:47,700 --> 00:56:51,990
sites mention information different points in time the function starts at the time and the

817
00:56:51,990 --> 00:56:56,660
site mentions information and information to more people times of course to get more deeply

818
00:56:56,660 --> 00:56:59,320
influence functions put together and so on OK

819
00:57:00,140 --> 00:57:03,550
now the question is how to how can i figure out the shape of this

820
00:57:03,550 --> 00:57:07,590
influence functions so the way we will do this is to use the following right

821
00:57:08,660 --> 00:57:13,360
as as i said right this influence functions are not observable need to estimate the

822
00:57:13,360 --> 00:57:16,590
way we think about we do this is we make no assumption about the shape

823
00:57:16,590 --> 00:57:20,930
of this do this in a non parametric way so really what do is we

824
00:57:20,930 --> 00:57:25,070
will be able to say we will put together some kind of character and try

825
00:57:25,090 --> 00:57:30,090
to optimize it and discover what the shapes of influence functions so the way to

826
00:57:30,090 --> 00:57:34,660
go is we will assume that influence functions just as sets of numbers let's say

827
00:57:34,680 --> 00:57:38,930
twenty four numbers right so every influence gets to zero after twenty four hours so

828
00:57:38,930 --> 00:57:43,740
i can measure which characterise the shape of the influence function by by a set

829
00:57:43,740 --> 00:57:48,490
of twenty four of twenty four numbers right so the value here and i truncated

830
00:57:48,490 --> 00:57:51,800
let's say at twenty four and then what you want to do is i want

831
00:57:51,800 --> 00:57:56,910
to find the the shape the should this twenty four numbers for each website such

832
00:57:56,910 --> 00:58:01,280
that the following expression is minimized right so here is the the

833
00:58:01,300 --> 00:58:03,030
here is that the

834
00:58:03,090 --> 00:58:07,140
for a particular piece of information and for a particular time this is the true

835
00:58:07,160 --> 00:58:10,640
number of mentions of the is of information in the future and this is my

836
00:58:10,640 --> 00:58:15,180
estimate reckons the sum over the web sites mention information in the past and it

837
00:58:15,200 --> 00:58:18,800
is the sum of their influences right now i can ask how should i how

838
00:58:18,800 --> 00:58:23,240
should i said these values by such as this this summation

839
00:58:23,280 --> 00:58:27,640
this character is the smallest possible and so what is the point is that the

840
00:58:27,640 --> 00:58:31,630
point is that this looks like a least squares problem and it really boils down

841
00:58:31,630 --> 00:58:36,010
to the squares problem right so idea and and doing nothing else than a big

842
00:58:36,320 --> 00:58:40,900
interestingly quest to show you how i can put everything together to to solve a

843
00:58:40,900 --> 00:58:45,110
big least squares problem the idea is the following so instead of working with one

844
00:58:45,130 --> 00:58:50,260
contagion one piece of information i work with k pieces of information so each

845
00:58:50,260 --> 00:58:54,840
i mean come and i will now composed these things in the matrix column vector

846
00:58:54,840 --> 00:58:57,820
in another column vectors of this column vector of equality and these are sort of

847
00:58:57,820 --> 00:59:02,280
just stick together volume time series of each of my k

848
00:59:02,300 --> 00:59:08,160
different pieces of information and then i had the influence functions that they want to

849
00:59:08,160 --> 00:59:13,240
estimate and then this matrix here is sort of like it's some kind of indicator

850
00:59:13,240 --> 00:59:18,130
matrix ritual process called the web this particular piece of information in the past right

851
00:59:18,130 --> 00:59:20,550
so sort of the set

852
00:59:20,990 --> 00:59:26,380
this an element of this matrix to one if no one gets infected by by

853
00:59:26,380 --> 00:59:29,110
a particular piece of information k at time t

854
00:59:29,130 --> 00:59:32,930
for each of these pieces so this is a simple binary matrix that sort of

855
00:59:32,930 --> 00:59:37,930
says half the volume in the future is the sum of particular influences of particular

856
00:59:37,930 --> 00:59:40,110
websites and now i can go go

857
00:59:40,110 --> 00:59:43,360
so solve this matrix equation

858
00:59:43,380 --> 00:59:47,570
to figure out what influences are this is something that is called nonnegative least squares

859
00:59:47,880 --> 00:59:51,590
there are super effective methods to do this so for example if you have a

860
00:59:51,610 --> 00:59:56,160
matrix of a hundred thousand times for four thousand you can do this

861
00:59:56,220 --> 01:00:00,550
in less than a second also then how we make predictions is just add influences

862
01:00:00,550 --> 01:00:05,100
together and we have to have the prediction so here is here are some results

863
01:00:05,100 --> 01:00:08,970
so the setting but i view

864
01:00:19,260 --> 01:00:23,450
no no i'm doing everything together so the idea is this is this is this

865
01:00:23,450 --> 01:00:29,070
is the volume of a particular piece of information so i have cases of information

866
01:00:29,070 --> 01:00:34,590
here i have influence functions for each website so the question is this is some

867
01:00:34,590 --> 01:00:40,070
kind of indicator matrix i want to figure out this so that so that what

868
01:00:40,090 --> 01:00:43,990
after i multiply these matrix with respect vector idea this time series as close as

869
01:00:43,990 --> 01:00:46,930
possible so really my optimisation problems be

870
01:00:47,800 --> 01:00:51,910
and here is my prediction

871
01:00:53,320 --> 01:00:59,880
exactly half

872
01:01:08,410 --> 01:01:10,860
i mean

873
01:01:18,400 --> 01:01:22,780
that's a good point so so the question is what what happens if information gets

874
01:01:22,800 --> 01:01:26,130
old or if you know sort of order of sites is also important so what

875
01:01:26,130 --> 01:01:29,930
i'm showing here is the basic version of the model you can add this actually

876
01:01:29,950 --> 01:01:34,110
in the paper the sort of there are these extensions are also try to account

877
01:01:34,110 --> 01:01:37,590
for how all of this information and things like that here it's sort of the

878
01:01:37,590 --> 01:01:41,280
most basic the essence of the model but you can very easily and those sort

879
01:01:41,280 --> 01:01:45,410
of penalty so it's a good point OK so here is here is the setting

880
01:01:45,410 --> 01:01:48,720
and i'll show you how well this works so the point is the following i

881
01:01:48,720 --> 01:01:55,680
will take top one thousand quotes by by total number of mentions and this one

882
01:01:55,680 --> 01:02:01,550
thousand quotes in this case got mentioned three hundred seventy two thousand times across a

883
01:02:01,550 --> 01:02:04,630
set of sixteen thousand websites and now

884
01:02:04,630 --> 01:02:07,030
what i want to do is the following i want to predict what is the

885
01:02:07,030 --> 01:02:13,680
future number of mentions of a particular quote based by only one monitoring one hundred

886
01:02:13,680 --> 01:02:17,320
highest volume websites right so the idea is i want to learn a hundred influence

887
01:02:17,320 --> 01:02:22,640
functions to predict what is going on at the overall population of sixteen thousand different

888
01:02:22,640 --> 01:02:26,590
websites right so i'm monitoring a small set of the media space and i'm trying

889
01:02:26,590 --> 01:02:30,090
to make a prediction about what's going on over sixty thousand

890
01:02:30,140 --> 01:02:35,340
the media sites space right and the way i want to compare against here is

891
01:02:35,540 --> 01:02:42,780
a standard time series regression models like and auto regression and so on the right

892
01:02:42,820 --> 01:02:43,840
and the

893
01:02:45,010 --> 01:02:50,030
my best maybe will be just sort of in the future for whatever is happening

894
01:02:50,030 --> 01:02:54,860
at the present sort of what i call one one time lag predictor and what

895
01:02:54,860 --> 01:02:59,410
is the point the point is that time series prediction around seven to eight percent

896
01:02:59,410 --> 01:03:03,480
amongst the observed variables this is actually the causal DAG right

897
01:03:03,500 --> 01:03:06,040
x causes the y causes w

898
01:03:06,060 --> 01:03:10,210
h is an observed variable right so it's not in the causal DAG

899
01:03:10,260 --> 01:03:14,080
so the causal DAG containing the observed variables

900
01:03:14,090 --> 01:03:16,840
c and w are independent

901
01:03:16,850 --> 01:03:22,230
but they not independently observed distribution

902
01:03:22,240 --> 01:03:27,190
so since we observed distribution does not satisfy the markov condition with the causal DAG

903
01:03:27,190 --> 01:03:30,310
containing the observed variables

904
01:03:30,320 --> 01:03:33,420
the markov assumption is not warranted see this

905
01:03:33,500 --> 01:03:35,450
so the causal DAG concerning

906
01:03:35,500 --> 01:03:39,680
the observed variables and we only observe x the one w

907
01:03:39,690 --> 01:03:41,490
the observed distribution

908
01:03:41,500 --> 01:03:43,610
it's based on this causal there

909
01:03:47,290 --> 01:03:50,230
independence is that satisfy the markov condition

910
01:03:50,280 --> 01:03:52,470
with this causal DAG

911
01:03:52,480 --> 01:03:55,850
and the problem here i think i have another in the same area the problem

912
01:03:55,850 --> 01:03:59,340
is there is a hidden common cause

913
01:03:59,470 --> 01:04:01,010
so she sometimes

914
01:04:01,050 --> 01:04:02,800
in certain situations

915
01:04:02,810 --> 01:04:07,970
we can make the causal faithfulness assumption and our results show us has to be

916
01:04:07,970 --> 01:04:11,910
a hidden common cause i made a mistake by assuming there's no i ended up

917
01:04:11,910 --> 01:04:13,050
with the goal

918
01:04:13,100 --> 01:04:15,940
for waiting aero going both ways

919
01:04:17,380 --> 01:04:20,590
it's so in certain cases the data will show is this

920
01:04:20,610 --> 01:04:24,200
but in other cases they will not show is that in other cases if we

921
01:04:24,200 --> 01:04:26,580
make the causal faithfulness assumption

922
01:04:26,590 --> 01:04:29,920
it could be a hidden common cause in the data will tell us that and

923
01:04:29,920 --> 01:04:32,160
will end up making causal

924
01:04:32,170 --> 01:04:34,680
conclusions which is incorrect

925
01:04:34,730 --> 01:04:37,200
so what we want to do is is

926
01:04:37,250 --> 01:04:39,820
to relax the assumption

927
01:04:39,870 --> 01:04:42,040
this is the

928
01:04:43,450 --> 01:04:46,190
trained the slide i hate putting up here

929
01:04:47,150 --> 01:04:51,940
because i'm not talking and you guys that this is in hand

930
01:04:51,950 --> 01:04:55,750
and that's my website so you can you can read this later i think i

931
01:04:55,750 --> 01:04:59,150
mean i was appointed me saying this because it is not going to do about

932
01:04:59,150 --> 01:05:00,260
the good

933
01:05:00,270 --> 01:05:02,810
so i'm just going to skip over this

934
01:05:03,880 --> 01:05:05,550
go to the next slide

935
01:05:06,430 --> 01:05:11,620
i can give a better intuitive feel for what the causal embedded faithfulness assumption

936
01:05:11,630 --> 01:05:13,670
basically what it means is

937
01:05:13,710 --> 01:05:16,470
we're assuming the a causal DAG

938
01:05:16,490 --> 01:05:19,500
with which the observed distribution is faithful

939
01:05:19,510 --> 01:05:22,840
but then you might contain hidden variables

940
01:05:22,900 --> 01:05:25,420
so assuming is a causal DAG out there

941
01:05:25,430 --> 01:05:29,690
and the observed distribution is faithful to it but i don't know that doesn't contain

942
01:05:29,690 --> 01:05:36,090
hidden variables that could contain hidden variables previous should case one must contain

943
01:05:36,100 --> 01:05:38,270
so if it

944
01:05:38,280 --> 01:05:40,370
we allow for hidden variables

945
01:05:40,380 --> 01:05:41,910
once we do this

946
01:05:41,920 --> 01:05:44,360
can we still learn any causes

947
01:05:44,370 --> 01:05:47,820
and i'm going to show that that we can still on some clauses but we

948
01:05:49,020 --> 01:05:51,340
as money

949
01:05:51,350 --> 01:05:54,570
it's alright let's look at how we learn causes

950
01:05:54,620 --> 01:05:56,310
if we make this

951
01:05:56,360 --> 01:05:59,960
i mean you revisit all the examples

952
01:05:59,980 --> 01:06:00,950
i suppose

953
01:06:00,960 --> 01:06:04,230
we find that x and y are independent with three variables

954
01:06:04,280 --> 01:06:07,870
we find x and y are independent

955
01:06:07,880 --> 01:06:10,330
apps what i do here

956
01:06:10,350 --> 01:06:16,460
from ireland that would

957
01:06:16,520 --> 01:06:21,370
the probability distribution is embedded faithfully in both these dags

958
01:06:21,480 --> 01:06:25,580
our before when i made the faithfulness assumption i

959
01:06:25,630 --> 01:06:29,290
i learned that from labeled krazy and i was happy because from from from only

960
01:06:29,290 --> 01:06:33,560
these three variables and i was able to learn to causal relationships

961
01:06:33,570 --> 01:06:35,060
but the problem is

962
01:06:35,070 --> 01:06:38,850
if i assume that could be hidden variables each one of these edges could be

963
01:06:38,850 --> 01:06:40,770
a hidden variable

964
01:06:40,820 --> 01:06:45,190
these also say x and y are independent

965
01:06:45,200 --> 01:06:51,600
by the way just mentioned there a noted that the separation implies that's why is

966
01:06:51,600 --> 01:06:55,570
the a number of the markov condition node is independent

967
01:06:55,620 --> 01:06:58,850
of its non descendants given its parents

968
01:07:00,730 --> 01:07:03,500
the markov condition strict police

969
01:07:03,540 --> 01:07:05,770
state practices act

970
01:07:05,790 --> 01:07:08,120
is independent of the y

971
01:07:08,140 --> 01:07:09,640
given h right

972
01:07:09,650 --> 01:07:15,310
the the statement of the markov condition does not say x and y are independent

973
01:07:17,960 --> 01:07:20,730
the from the markov condition you can

974
01:07:20,740 --> 01:07:23,080
to do so other independencies

975
01:07:23,130 --> 01:07:26,680
and those is called the separations

976
01:07:26,690 --> 01:07:30,710
and so the set of all independence is entailed by the markov condition from the

977
01:07:30,720 --> 01:07:34,060
set of all these separations and gag

978
01:07:34,080 --> 01:07:37,870
and for this particular one you can see why this has the whole

979
01:07:39,190 --> 01:07:41,600
one is independent of h two

980
01:07:41,610 --> 01:07:44,290
right it's like the earthquake

981
01:07:44,390 --> 01:07:46,070
burglar example

982
01:07:46,090 --> 01:07:50,950
well some since these two variables are in the independent intuitively you can see that

983
01:07:50,950 --> 01:07:54,060
actually have to be independent of h two

984
01:07:54,200 --> 01:07:58,420
what if the if h one and h two independent how could the factor of

985
01:08:00,300 --> 01:08:02,020
depend on h two n

986
01:08:02,170 --> 01:08:06,030
that's the kind of h two then x would also be independent of y

987
01:08:06,080 --> 01:08:09,990
again i understood intuition but it can be proven mathematically

988
01:08:10,010 --> 01:08:11,240
and if you make the

989
01:08:11,320 --> 01:08:13,440
and the markov assumption

990
01:08:13,490 --> 01:08:16,590
then you can conclude that x and y are independent

991
01:08:16,740 --> 01:08:19,740
right so let me just go on with that in just

992
01:08:19,760 --> 01:08:21,100
getting intuition point

993
01:08:21,110 --> 01:08:26,820
the point is is that probability distribution is therefore embedded faithfully in both in stances

994
01:08:26,870 --> 01:08:31,890
so you can't really conclude that causes the you can't really conclude why causes the

995
01:08:31,890 --> 01:08:37,520
because both of these could be due to a hidden common causes

996
01:08:37,580 --> 01:08:41,270
so the causal relationships among the observed variables may be

997
01:08:41,270 --> 01:08:44,180
that's basically would need to integrate over

998
01:08:44,810 --> 01:08:51,450
slightly intractable seem you definitely shouldn't want to drive up

999
01:08:51,470 --> 01:08:54,320
in some cases

1000
01:08:54,340 --> 01:08:55,910
we might not necessarily

1001
01:08:55,920 --> 01:08:59,270
i want to the density estimation but just novelty detection

1002
01:08:59,280 --> 01:09:02,350
just to find out with an observation is

1003
01:09:02,390 --> 01:09:04,360
and you looks unusual or not

1004
01:09:04,480 --> 01:09:06,910
we do not necessarily need

1005
01:09:06,920 --> 01:09:10,960
to get the normalisation

1006
01:09:10,980 --> 01:09:13,300
so for instance

1007
01:09:16,180 --> 01:09:20,610
if i wanted novelty detection so if i want to find unusual observations and if

1008
01:09:20,630 --> 01:09:22,250
well the

1009
01:09:22,290 --> 01:09:27,100
green line is my density then well what i would do is i would threshold

1010
01:09:27,100 --> 01:09:28,630
is based here

1011
01:09:28,650 --> 01:09:29,800
so here

1012
01:09:29,820 --> 01:09:32,170
and in this region there

1013
01:09:32,220 --> 01:09:33,660
the region out there

1014
01:09:33,680 --> 01:09:37,380
and that region would be considered normal

1015
01:09:37,650 --> 01:09:41,250
if i wanted to be a bit more aggressive

1016
01:09:41,270 --> 01:09:42,570
well maybe

1017
01:09:42,580 --> 01:09:44,560
i would pick i threshold

1018
01:09:44,580 --> 01:09:46,490
up there

1019
01:09:46,540 --> 01:09:49,360
OK i could do something like this

1020
01:09:50,730 --> 01:09:52,420
in fact

1021
01:09:55,420 --> 01:09:57,840
what's happening

1022
01:09:57,920 --> 01:10:02,560
there should be another few slides in there that might just try to the whiteboard

1023
01:10:06,170 --> 01:10:14,400
it's actually quite easy to get intuition for it

1024
01:10:25,460 --> 01:10:27,660
say that's my domain

1025
01:10:27,860 --> 01:10:31,480
and this is the things that are trying to estimate

1026
01:10:32,960 --> 01:10:38,860
and i going threshold here

1027
01:10:38,870 --> 01:10:41,930
now what happens on top there

1028
01:10:41,970 --> 01:10:44,430
doesn't really influenced my novelty detector

1029
01:10:44,720 --> 01:10:49,160
so if this function instead of that were to behave

1030
01:10:49,180 --> 01:10:55,660
like so

1031
01:10:55,670 --> 01:10:57,590
and then maybe

1032
01:10:57,630 --> 01:11:00,150
it just remain completely flat

1033
01:11:00,170 --> 01:11:03,660
i would not really matter for that level of

1034
01:11:04,150 --> 01:11:07,300
but for the threshold

1035
01:11:08,740 --> 01:11:13,310
so the high density regions don't really matter so much work with this but the

1036
01:11:13,310 --> 01:11:15,920
novelty detector

1037
01:11:17,060 --> 01:11:19,560
you're interested low density regions

1038
01:11:22,270 --> 01:11:27,050
now is the next thing that doesn't matter

1039
01:11:27,760 --> 01:11:34,180
here is that

1040
01:11:34,350 --> 01:11:36,820
the next thing that doesn't matter as well

1041
01:11:36,830 --> 01:11:39,000
i could scale this function by

1042
01:11:39,010 --> 01:11:41,590
one half

1043
01:11:41,940 --> 01:11:53,210
OK it's not evinced any more wanted integrate out to one

1044
01:11:53,280 --> 01:11:57,740
but if i pick the right threshold

1045
01:11:57,780 --> 01:12:01,390
treating my way through a little bit

1046
01:12:09,030 --> 01:12:15,570
OK OK and cheating but

1047
01:12:15,620 --> 01:12:17,150
i think you get the idea

1048
01:12:17,170 --> 01:12:21,110
if i didn't have something that's the proper density

1049
01:12:21,130 --> 01:12:25,830
and then we adjust my threshold i'll get the same region

1050
01:12:26,670 --> 01:12:31,230
so neither do i need to do particularly well in high density regions

1051
01:12:31,450 --> 01:12:36,260
nor do actually need to get this normalisation that several pain in the neck anyway

1052
01:12:37,730 --> 01:12:40,090
why not try doing something

1053
01:12:40,130 --> 01:12:46,840
that a does away with the normalisation he doesn't care about the high density regions

1054
01:12:46,850 --> 01:12:50,300
and also you is that if you design

1055
01:12:50,310 --> 01:12:54,260
an estimator based on those two strategies

1056
01:12:54,310 --> 01:12:55,770
departing from

1057
01:12:55,780 --> 01:12:57,450
density estimation

1058
01:12:57,460 --> 01:13:08,790
will get exactly burn of novelty detector

1059
01:13:11,180 --> 01:13:14,350
what data

1060
01:13:14,360 --> 01:13:15,810
the first thing is

1061
01:13:15,860 --> 01:13:20,020
well so that would be my max foster estimation

1062
01:13:21,610 --> 01:13:22,700
i go

1063
01:13:22,720 --> 01:13:28,220
and look at rather log p x i look at the ratio between p effects

1064
01:13:28,220 --> 01:13:29,250
on data

1065
01:13:29,260 --> 01:13:31,890
in some reference things

1066
01:13:31,890 --> 01:13:33,300
should be using

1067
01:13:33,330 --> 01:13:37,300
then you just then you know that this is just a fancy multidimensional version of

1068
01:13:37,300 --> 01:13:39,750
this one

1069
01:13:39,760 --> 01:13:44,000
OK so don't be scared to talk about my ten kernels

1070
01:13:44,010 --> 01:13:48,660
so this is really just the names that chaos

1071
01:13:49,780 --> 01:13:53,280
you can actually draw from such a stochastic process

1072
01:13:53,310 --> 01:13:53,940
so that

1073
01:13:53,950 --> 01:13:56,250
using gauss in RDF common just

1074
01:13:56,520 --> 01:13:59,820
taking some state and trying to values

1075
01:13:59,830 --> 01:14:03,140
that's what you get this like a simple function

1076
01:14:03,150 --> 01:14:04,980
another sample from

1077
01:14:05,000 --> 01:14:07,780
so what this story tells is that

1078
01:14:07,800 --> 01:14:09,120
well if i have

1079
01:14:09,170 --> 01:14:13,070
like regression classification system which behave like that so that by might be one class

1080
01:14:13,070 --> 01:14:17,940
to another one and it's the uncertainty region then actually using this kernel is going

1081
01:14:17,940 --> 01:14:20,840
to give you something fairly reasonable

1082
01:14:20,860 --> 01:14:22,960
it another one

1083
01:14:23,000 --> 01:14:24,160
see i told you

1084
01:14:24,170 --> 01:14:27,300
some of the slides is not very scary

1085
01:14:27,320 --> 01:14:30,620
the RBF kernel

1086
01:14:30,640 --> 01:14:32,560
it's not silly

1087
01:14:32,620 --> 01:14:35,170
this see the difference

1088
01:14:35,200 --> 01:14:39,080
basically gives a linear gradient here

1089
01:14:39,100 --> 01:14:40,670
the one

1090
01:14:40,670 --> 01:14:43,970
so it doesn't matter which can be

1091
01:14:44,010 --> 01:14:45,820
it doesn't like matter

1092
01:14:45,830 --> 01:14:48,560
eugen months

1093
01:14:48,570 --> 01:14:51,450
it's more important to get the scalar right

1094
01:14:51,490 --> 01:14:53,250
but obviously such

1095
01:14:53,430 --> 01:14:56,810
very kindly not going to be very good if you have a circular structure sitting

1096
01:14:56,820 --> 01:15:00,210
it out

1097
01:15:00,220 --> 01:15:02,640
it was not

1098
01:15:07,170 --> 01:15:11,980
i basically stop here and tell you will everything else is now left as an

1099
01:15:11,980 --> 01:15:15,180
exercise to the reader

1100
01:15:16,220 --> 01:15:20,470
all you do is you pick some so suitable sufficient statistic phi x and y

1101
01:15:20,620 --> 01:15:26,200
which describes the conditional dependency y given x

1102
01:15:27,230 --> 01:15:30,470
for instance conditionally multinomial distribution

1103
01:15:30,480 --> 01:15:33,960
i will give us casting process multiclass estimator

1104
01:15:34,010 --> 01:15:36,980
we get the distribution over in different classes

1105
01:15:37,030 --> 01:15:40,710
well we have to do is to find the right parameters

1106
01:15:40,720 --> 01:15:45,820
and the kernel will tell us how smoothly the distribution can change

1107
01:15:46,410 --> 01:15:47,580
if we take

1108
01:15:47,700 --> 01:15:50,700
conditionally normal distribution

1109
01:15:50,740 --> 01:15:54,000
o to get that process regression estimator

1110
01:15:54,010 --> 01:15:54,700
and we

1111
01:15:54,710 --> 01:15:55,600
get some

1112
01:15:55,640 --> 01:15:57,840
normal distribution over random variables

1113
01:15:57,850 --> 01:16:01,240
which just depend on the location

1114
01:16:01,250 --> 01:16:02,420
if you want to

1115
01:16:02,420 --> 01:16:05,430
play both the me and the variance

1116
01:16:05,480 --> 01:16:09,340
well then you will get heteroskedastic estimator

1117
01:16:09,370 --> 01:16:13,520
and this is actually was going to solve an really interesting problem

1118
01:16:13,540 --> 01:16:18,540
so until recently people have been trying a lot of heteroskedastic estimators but none of

1119
01:16:18,540 --> 01:16:21,060
the optimisation problem is convex

1120
01:16:21,070 --> 01:16:22,360
i don't know why

1121
01:16:22,410 --> 01:16:24,010
it's actually very easy

1122
01:16:24,020 --> 01:16:26,090
you could easily do that

1123
01:16:26,100 --> 01:16:30,230
you can have normal distribution member of the exponential family

1124
01:16:30,290 --> 01:16:31,460
condition on it

1125
01:16:31,500 --> 01:16:33,520
still in the spanish family

1126
01:16:34,520 --> 01:16:35,510
so they

1127
01:16:35,510 --> 01:16:38,540
we will get a convex optimisation problem

1128
01:16:38,590 --> 01:16:43,770
and this is exactly what we did actually be based in matters so not new

1129
01:16:43,870 --> 01:16:47,230
it is actually the name

1130
01:16:47,240 --> 01:16:48,410
sorry about that

1131
01:16:48,420 --> 01:16:50,540
conditional puzzles diffusion

1132
01:16:50,600 --> 01:16:53,510
this just leaves look you look very

1133
01:16:53,550 --> 01:16:54,750
some process

1134
01:16:54,750 --> 01:16:57,990
and this is what's called special plus some models

1135
01:16:58,830 --> 01:17:02,980
it's crisis book is full of that

1136
01:17:02,980 --> 01:17:09,410
and you could use a conditional discharge you could use the current conditional additionally whatever

1137
01:17:09,430 --> 01:17:11,150
o conditional gamma

1138
01:17:11,200 --> 01:17:13,530
or conditional beta distribution

1139
01:17:13,550 --> 01:17:20,190
and this is where all the flexibility in the estimation procedure comes in

1140
01:17:20,190 --> 01:17:24,520
say you want to estimate some y given x

1141
01:17:24,540 --> 01:17:26,940
y is defined on some

1142
01:17:26,940 --> 01:17:30,770
you might have some idea what the distribution roughly should look like

1143
01:17:30,790 --> 01:17:36,060
and then you just going to estimate the conditional

1144
01:17:36,110 --> 01:17:38,940
it's very straightforward

1145
01:17:38,940 --> 01:17:42,550
and don't be scared if you end up with some estimated that nobody has three

1146
01:17:42,550 --> 01:17:44,360
written not yet

1147
01:17:44,500 --> 01:17:45,590
actually good news

1148
01:17:45,590 --> 01:17:49,460
there this implicitly sensitive to the right priors they know

1149
01:17:49,480 --> 01:17:54,380
somehow something about the the distributions of these different classes of events that enable them

1150
01:17:54,380 --> 01:17:57,690
to make the right predictions and you can get this unless you plan to write

1151
01:17:57,690 --> 01:18:01,690
priors in particular given that they're such different shapes across the different domains and people

1152
01:18:01,690 --> 01:18:06,380
seem to get exactly what more moral as the right form of the prediction function

1153
01:18:06,570 --> 01:18:08,050
not just

1154
01:18:08,090 --> 01:18:10,650
the actual values

1155
01:18:12,980 --> 01:18:14,320
all right so

1156
01:18:16,230 --> 01:18:19,980
i let's us pretty much the end of the of the sun

1157
01:18:20,000 --> 01:18:23,030
first unit and i can take questions on that if you want

1158
01:18:23,090 --> 01:18:24,920
or i can

1159
01:18:27,000 --> 01:18:30,920
the point

1160
01:18:30,940 --> 01:18:34,340
it was

1161
01:18:34,360 --> 01:18:37,230
yes there is

1162
01:18:37,920 --> 01:18:42,130
so a point prior meaning like they just think all poems of the same length

1163
01:18:43,110 --> 01:18:50,000
well well OK so that's a really good question and it's somebody published a paper

1164
01:18:50,110 --> 01:18:55,820
arguing version version of that mike mozer and how powerful are published a paper saying

1165
01:18:55,820 --> 01:18:59,150
hey you could get the same thing probably not with the point prior but if

1166
01:18:59,190 --> 01:19:02,930
people just how it just people had in memory only a few poems or a

1167
01:19:02,930 --> 01:19:07,940
few examples of movies they should you could fit this data pretty well just using

1168
01:19:07,940 --> 01:19:14,420
like two or three examples in memory that's interesting i think there's something interesting deep

1169
01:19:14,530 --> 01:19:18,690
right about that also something wrong there's lots of other evidence we have an thomas

1170
01:19:20,030 --> 01:19:24,780
a response to this showing that people really with even within individual subjects you can

1171
01:19:24,780 --> 01:19:28,010
show evidence that people have these

1172
01:19:30,150 --> 01:19:35,470
you have not not exactly that not everybody has the same distribution for actually but

1173
01:19:35,470 --> 01:19:41,070
people have very much like a full distribution and he used this very

1174
01:19:41,090 --> 01:19:46,380
clever all remarkable method that adam sanborn and who i guess atomism here but some

1175
01:19:46,380 --> 01:19:50,610
of you may know him he's post-doc at the gatsby right now and in time

1176
01:19:50,610 --> 01:19:55,590
developed the so-called human markov chain monte carlo to just to show within individual subjects

1177
01:19:55,820 --> 01:19:59,170
so we're using MCMC within individual subjects to map out the prior and a show

1178
01:19:59,190 --> 01:20:03,070
the individual subject to have prior that look like this

1179
01:20:03,070 --> 01:20:07,170
but i think there's also something very much right about the point which is that

1180
01:20:07,170 --> 01:20:09,880
in this kind of study and a lot of other

1181
01:20:09,880 --> 01:20:15,550
studies in bayesian cognitive modeling things the data are are often consistent with and sometimes

1182
01:20:15,550 --> 01:20:20,420
when you look really closely individual subjects which which we can do here

1183
01:20:20,480 --> 01:20:25,630
there are consistent with the idea that people's inferences are based on taking a very

1184
01:20:25,630 --> 01:20:28,590
small number of samples from the posterior so i would say it's not the case

1185
01:20:28,590 --> 01:20:33,090
that people have in mind are the implicitly in our long-term memory we only have

1186
01:20:33,360 --> 01:20:36,550
two samples but what we might bring to mind one case might be a very

1187
01:20:36,550 --> 01:20:38,590
small number of samples from the posterior

1188
01:20:38,610 --> 01:20:43,630
and that's that's the thing that's that's emerged across the number of different places and

1189
01:20:43,630 --> 01:20:48,250
a bunch of us are interested in exploring this idea that

1190
01:20:48,940 --> 01:20:54,110
the way people might implement bayesian inference and learning first of all might be a

1191
01:20:54,110 --> 01:20:58,340
kind of monte carlo sampling based approximate inference but also that they might use a

1192
01:20:58,340 --> 01:21:01,050
very very small number of samples which

1193
01:21:01,050 --> 01:21:05,420
you know to a normal statistician would seem crazy statisticians work really hard to get

1194
01:21:05,440 --> 01:21:10,000
to get very large numbers of of independent samples

1195
01:21:11,900 --> 01:21:14,340
empirically in practice

1196
01:21:14,510 --> 01:21:18,460
machine learning people have found that often very small

1197
01:21:18,500 --> 01:21:22,840
samples were actually work quite well actually the first time i remember seeing that was

1198
01:21:22,840 --> 01:21:25,550
talking to engage back in grad school

1199
01:21:25,550 --> 01:21:29,270
remember in year pretty sure was in your factorial learning

1200
01:21:29,280 --> 01:21:32,650
o thing you're doing this really really fast gibbs sampling like ten gives samples or

1201
01:21:32,650 --> 01:21:41,500
something like that what made impact on me but anecdotally many of you have done

1202
01:21:42,050 --> 01:21:44,030
you know MCMC based on

1203
01:21:44,050 --> 01:21:49,880
learn your learning where where there is an inner loop of of MCMC inference might

1204
01:21:49,880 --> 01:21:52,500
have noticed something like that and i

1205
01:21:52,670 --> 01:21:55,840
again out of i was planning related to to dwell on that but i'm happy

1206
01:21:55,840 --> 01:21:59,110
to talk about it and i think it's quite an interesting place where human human

1207
01:21:59,110 --> 01:22:04,000
learning machine learning can meet up sort of thinking about mechanisms for approximate inference and

1208
01:22:04,000 --> 01:22:07,170
thinking about when you might actually be able to get away with and maybe even

1209
01:22:07,170 --> 01:22:10,940
when a cost-benefit analysis where you take into account the cost of sampling might actually

1210
01:22:10,940 --> 01:22:16,150
favor working with very small numbers of samples each subject was given only one question

1211
01:22:16,150 --> 01:22:20,570
in for each of these topics but across subjects the that you different subjects got

1212
01:22:20,570 --> 01:22:23,590
different numbers plugin for these different topics

1213
01:22:23,610 --> 01:22:26,980
and it was just the one page questionnaire and they

1214
01:22:26,980 --> 01:22:27,570
you know they

1215
01:22:27,590 --> 01:22:29,860
road number next each line

1216
01:22:35,550 --> 01:22:38,480
i mean like in the graph or something i'm not sure

1217
01:22:38,510 --> 01:22:41,800
yeah i mean in general so as i did mean to laugh at the i

1218
01:22:41,800 --> 01:22:46,250
mean it was a really good question in general that's something that where you know

1219
01:22:46,270 --> 01:22:48,190
it's really

1220
01:22:48,210 --> 01:22:51,820
something to worry about and to think about right but it's it's sort of it's

1221
01:22:51,820 --> 01:22:55,440
both and good thing like i think the the brain makes sense to think of

1222
01:22:55,440 --> 01:22:58,980
the brain as a kind of intuitive statistician and also makes sense to separately think

1223
01:22:58,980 --> 01:23:03,470
of the visual system as an intuitive statisticians sometimes the visual system might be a

1224
01:23:03,470 --> 01:23:07,480
smarter statistician then the rest of the brain the more linguistic higher-level part of the

1225
01:23:07,480 --> 01:23:11,960
brain that's often why is very helpful to graph are data like we i mean

1226
01:23:11,960 --> 01:23:15,900
good statisticians and learning people know this don't just compute numbers don't just look at

1227
01:23:15,900 --> 01:23:19,940
tables but plot your data and look at it because you're able to to use

1228
01:23:20,030 --> 01:23:21,630
it's you know the the

1229
01:23:21,670 --> 01:23:29,920
very smart unsupervised learning exploratory data analysis stuff that's basically built into your visual system

1230
01:23:34,210 --> 01:23:41,960
because of

1231
01:23:52,880 --> 01:23:56,110
read the way you want

1232
01:23:56,130 --> 01:23:57,570
one of

1233
01:23:57,900 --> 01:24:02,900
we need more

1234
01:24:03,000 --> 01:24:12,320
one of the more

1235
01:24:14,840 --> 01:24:19,000
yeah there's a few papers like that i wrote one of them are rather konrad

1236
01:24:19,000 --> 01:24:21,840
kording ronnie one of them when he was a postdoc with me and i see

1237
01:24:21,980 --> 01:24:26,320
my name on it in marie also had had a nice paper like that recently

1238
01:24:26,320 --> 01:24:34,110
NIPS with with resemblance and a few others and people like david at rochester have

1239
01:24:34,110 --> 01:24:36,840
there's a bunch of models like that alan newell has

1240
01:24:36,920 --> 01:24:44,360
proposed models of basically multimodal sensory fusions sensory integration

1241
01:24:44,380 --> 01:24:49,380
i guess actually is a little sort on that this thesis from a bayesian standpoint

1242
01:24:51,130 --> 01:24:56,210
i mean yes is that that's that's rich rich literature and not sure

1243
01:24:56,440 --> 01:25:01,280
i mean i think roughly it's roughly i would say it certainly similar to these

1244
01:25:01,280 --> 01:25:06,130
things konrad actually quite come according described it actually has a causal influence the way

1245
01:25:06,130 --> 01:25:12,240
fifty percent is not terrible in this case using only data again we're plodding here

1246
01:25:12,240 --> 01:25:16,660
it is function of the number of training examples using only data

1247
01:25:16,670 --> 01:25:19,440
the area look something like this

1248
01:25:19,460 --> 01:25:20,840
and if we combine

1249
01:25:20,880 --> 01:25:23,420
both the prior knowledge and the data

1250
01:25:23,430 --> 01:25:24,510
and we end up with

1251
01:25:24,520 --> 01:25:28,400
the curve like this which early on in the case we have a small amount

1252
01:25:28,400 --> 01:25:34,450
of data is doing better than either one

1253
01:25:34,550 --> 01:25:38,590
so another problem we came up with this one there was talk about quite a

1254
01:25:38,590 --> 01:25:44,210
bit in the tutorial yesterday morning which is the problem that the labels themselves are

1255
01:25:44,210 --> 01:25:47,070
quite expensive compared to the other data

1256
01:25:47,080 --> 01:25:49,790
so getting once the system is deployed

1257
01:25:49,840 --> 01:25:54,700
getting examples in general is cheap because you just have to record what the person

1258
01:25:54,700 --> 01:25:55,730
is saying

1259
01:25:55,850 --> 01:25:58,850
the getting labels is expensive

1260
01:25:58,870 --> 01:26:02,000
because label any one of these utterances

1261
01:26:02,000 --> 01:26:07,240
a human actually has to annotate has to sit there and listen to what was

1262
01:26:07,330 --> 01:26:08,680
an annotated

1263
01:26:08,770 --> 01:26:11,790
and that's expensive you can only afford to get a few hundred of these in

1264
01:26:11,790 --> 01:26:13,940
the day

1265
01:26:13,950 --> 01:26:16,840
so how can we reduce the number of labels needed

1266
01:26:16,860 --> 01:26:20,480
well we can use exactly the kind of active learning techniques that were talking about

1267
01:26:20,480 --> 01:26:21,900
yesterday morning

1268
01:26:22,170 --> 01:26:26,270
so the idea is the same we can use selective sampling to choose which examples

1269
01:26:26,270 --> 01:26:27,130
to label

1270
01:26:27,150 --> 01:26:31,230
and then we want to focus on the least confident examples this was kind of

1271
01:26:31,230 --> 01:26:35,230
the general principle that was talked about yesterday

1272
01:26:35,250 --> 01:26:40,280
after boosting we have a nice way of measuring confidence we can measure confidence using

1273
01:26:40,280 --> 01:26:42,690
the margin or the absolute value

1274
01:26:42,780 --> 01:26:47,500
of the marginal so we can just apply these general active learning principles

1275
01:26:47,530 --> 01:26:49,690
boosting where we use margin

1276
01:26:49,690 --> 01:26:52,030
as our measure of confidence

1277
01:26:52,030 --> 01:26:55,690
so the idea is something like this so you start out with a pool of

1278
01:26:55,690 --> 01:26:58,030
unlabeled examples which is usually

1279
01:26:58,090 --> 01:27:01,360
quite large in the tens of thousands hundreds of thousands

1280
01:27:01,650 --> 01:27:06,130
then we choose a five hundred examples of random

1281
01:27:06,130 --> 01:27:07,650
and labeled those

1282
01:27:07,650 --> 01:27:09,150
to get things started

1283
01:27:09,170 --> 01:27:13,360
we run boosting on all of the labelled examples that we have so far

1284
01:27:13,440 --> 01:27:16,340
that gives us the combined classifier as

1285
01:27:16,400 --> 01:27:21,770
linear combination of the weak classifiers and then we choose an additional two hundred fifty

1286
01:27:21,770 --> 01:27:26,630
examples for labeling where we select the examples

1287
01:27:26,630 --> 01:27:29,780
for which there is absolute value

1288
01:27:29,780 --> 01:27:35,150
of the prediction is smallest because this is proportional to the absolute value of the

1289
01:27:37,530 --> 01:27:41,420
and then we can just repeat that process we have these two hundred fifty examples

1290
01:27:41,420 --> 01:27:43,840
to our pool of unlabeled examples

1291
01:27:43,840 --> 01:27:46,340
we retrain and repeat this process

1292
01:27:46,340 --> 01:27:51,210
this kind of mimics what could actually happen in real system where you could have

1293
01:27:51,310 --> 01:27:54,340
overnight train the system

1294
01:27:54,360 --> 01:27:57,050
and then during the day could have the humans

1295
01:27:57,070 --> 01:28:03,150
annotators label but two hundred fifty examples that you act overnight

1296
01:28:03,170 --> 01:28:06,360
OK so here here's how well that works

1297
01:28:06,360 --> 01:28:11,900
so the red curve is showing the error rate as a function of the number

1298
01:28:11,900 --> 01:28:14,840
of labelled examples

1299
01:28:14,860 --> 01:28:19,980
where we choose the examples at random rather than using this active process you just

1300
01:28:19,980 --> 01:28:21,900
use the examples for labeling

1301
01:28:23,500 --> 01:28:27,630
and the blue curve is showing that same error if you choose the examples actively

1302
01:28:27,630 --> 01:28:29,880
using this method that i described

1303
01:28:30,030 --> 01:28:33,480
and you can see the beginning a big improvement in terms of the number of

1304
01:28:34,480 --> 01:28:39,210
there you need so for instance to get a test error rate of twenty five

1305
01:28:40,210 --> 01:28:44,960
you would need all forty thousand random examples

1306
01:28:44,980 --> 01:28:49,710
but if you choose the examples actively you only need about thirteen thousand so that's

1307
01:28:49,710 --> 01:28:52,320
the savings of about sixty eight percent

1308
01:28:52,380 --> 01:28:55,710
so yesterday in the tutorial for those of you who came to the tutorial they

1309
01:28:55,710 --> 01:28:57,130
were talking about getting

1310
01:28:57,130 --> 01:29:01,590
exponential improvements in terms of the number of labels you need

1311
01:29:02,270 --> 01:29:06,460
i've never seen that in practice certainly not with boosting but these kinds of improvements

1312
01:29:06,460 --> 01:29:07,460
we get

1313
01:29:07,480 --> 01:29:11,510
a big constant factor maybe it three to one to forty to one improvements are

1314
01:29:11,510 --> 01:29:16,030
quite common not just for boosting but other methods also support vector machines and so

1315
01:29:17,170 --> 01:29:21,000
the thing that comes up i don't think they talked about this yesterday but this

1316
01:29:21,000 --> 01:29:22,440
kind of

1317
01:29:22,670 --> 01:29:27,110
interesting that this curve goes down and then up again

1318
01:29:27,130 --> 01:29:30,860
this is partly a result of the way we set up the experiment

1319
01:29:30,860 --> 01:29:35,980
that in this experiment we were using a fixed pool of forty thousand examples so

1320
01:29:35,980 --> 01:29:38,550
eventually these two curves have two

1321
01:29:38,550 --> 01:29:43,130
converge when you get out here what's interesting is that the curve goes down and

1322
01:29:43,130 --> 01:29:44,570
then up again

1323
01:29:44,590 --> 01:29:48,050
so it seems to be happening is that early on you're adding

1324
01:29:48,070 --> 01:29:54,300
these highly informative examples but that at some point because you have a fixed pool

1325
01:29:54,440 --> 01:29:58,670
you have to start adding the other examples and the other examples would seem to

1326
01:29:58,670 --> 01:29:59,980
actually be

1327
01:30:00,000 --> 01:30:02,860
kind of this informative they seem to be hurting

1328
01:30:02,940 --> 01:30:07,670
the performance rather just leave them out

1329
01:30:07,730 --> 01:30:12,130
here's another character you don't get this step on this one but do get these

1330
01:30:12,130 --> 01:30:15,900
big improvements in terms of number of labels

1331
01:30:15,920 --> 01:30:20,340
OK and then finally the last thing i want to talk about is an application

1332
01:30:20,340 --> 01:30:26,900
to boosting to detecting faces this is really nice work by viola and jones so

1333
01:30:26,900 --> 01:30:31,710
the problem is to find faces in the photograph or movie

1334
01:30:31,710 --> 01:30:34,450
all these things based on very mundane

1335
01:30:34,510 --> 01:30:37,820
but if you're in business during this is is going do it

1336
01:30:37,840 --> 01:30:39,270
you shouldn't be doing

1337
01:30:39,270 --> 01:30:43,720
that's just just formula for just one right never

1338
01:30:43,740 --> 01:30:47,970
you're not being serious about doing this correctly

1339
01:30:48,580 --> 01:30:50,100
techniques at twenty two

1340
01:30:50,120 --> 01:30:55,620
this is the whole database actually had his license well but unfortunately like above-mentioned is

1341
01:30:56,070 --> 01:31:01,680
tools out there that actually have been built to address this very crucial problem

1342
01:31:01,720 --> 01:31:03,390
pragmatic grand challenge five

1343
01:31:03,600 --> 01:31:05,220
effectiveness measurement

1344
01:31:05,260 --> 01:31:08,720
if i had to give examples of public someone

1345
01:31:08,740 --> 01:31:10,390
and metrics

1346
01:31:10,390 --> 01:31:12,200
how important it is to attach

1347
01:31:12,220 --> 01:31:15,890
success metrics to the project

1348
01:31:15,890 --> 01:31:17,280
that takes the value of world

1349
01:31:17,300 --> 01:31:18,280
keep people alive

1350
01:31:18,410 --> 01:31:21,030
six tell you want to kill

1351
01:31:21,050 --> 01:31:22,720
that takes a few minutes

1352
01:31:22,740 --> 01:31:24,200
measures for success

1353
01:31:24,220 --> 01:31:27,550
so the people doing the data mining that gaming the system

1354
01:31:27,600 --> 01:31:30,240
and the people working with the star

1355
01:31:30,260 --> 01:31:35,430
there's also politics and the with start denying the truth of that stuff

1356
01:31:35,450 --> 01:31:40,010
coming up with framework methodology for how do we come up with metric

1357
01:31:40,080 --> 01:31:41,510
the we make them

1358
01:31:41,530 --> 01:31:44,700
other distributed to business people are doing that

1359
01:31:44,800 --> 01:31:49,200
study the source of all that is very

1360
01:31:49,220 --> 01:31:51,030
as much technical challenges

1361
01:31:51,050 --> 01:31:54,220
two cover today and their families give

1362
01:31:54,220 --> 01:31:56,680
i would probably highlight some of them in my talk

1363
01:31:59,470 --> 01:32:03,930
on tuesday public public benchmark datasets but worked on this

1364
01:32:03,970 --> 01:32:07,570
i agree to pushing for a a long time we still don't have decent public

1365
01:32:07,570 --> 01:32:09,280
benchmark datasets

1366
01:32:10,070 --> 01:32:13,530
truly know test with that

1367
01:32:13,550 --> 01:32:14,620
thank you

1368
01:32:14,680 --> 01:32:17,660
one of the electrodes

1369
01:32:17,680 --> 01:32:21,660
right big databases just become big by random sampling

1370
01:32:21,680 --> 01:32:24,010
they actually collected over years

1371
01:32:24,050 --> 01:32:28,580
the market underneath them is changing the population drawing from is changing

1372
01:32:28,600 --> 01:32:32,950
all sorts of things that in make invented all sorts of statistical theory

1373
01:32:33,050 --> 01:32:39,350
how do you rid complexity and understandability had been

1374
01:32:39,370 --> 01:32:41,700
interesting as i mentioned in the beginning

1375
01:32:41,800 --> 01:32:46,070
and hopefully they will come up with some examples they

1376
01:32:46,080 --> 01:32:54,080
x was reminded something ronny kohavi told me

1377
01:32:54,300 --> 01:32:58,010
a few years back so i'll use an example here

1378
01:32:58,030 --> 01:32:59,760
which is when they were st

1379
01:32:59,780 --> 01:33:02,780
and they

1380
01:33:02,800 --> 01:33:04,640
the first thing

1381
01:33:06,600 --> 01:33:08,700
frequent itemsets

1382
01:33:08,720 --> 01:33:11,570
so stationary was on

1383
01:33:11,570 --> 01:33:13,350
absolutely is

1384
01:33:13,370 --> 01:33:17,390
in the u s five thousand animals per household

1385
01:33:18,560 --> 01:33:21,780
those were were work it came back and

1386
01:33:21,810 --> 01:33:26,850
is the rule told ninety nine point three percent accuracy

1387
01:33:26,850 --> 01:33:29,100
all has been said to me

1388
01:33:31,760 --> 01:33:38,760
it took you less the second ground that's silly thing right the machine friendly reasonable

1389
01:33:38,800 --> 01:33:40,930
and actually some very very powerful

1390
01:33:40,930 --> 01:33:44,120
correlation that you may not be very interesting

1391
01:33:44,140 --> 01:33:47,390
would love this semantic syntactic

1392
01:33:47,410 --> 01:33:51,260
scalability beyond large datasets but dimensions

1393
01:33:51,260 --> 01:33:54,870
he was talking about that someone on tuesday

1394
01:33:54,870 --> 01:33:56,430
and here is what we do

1395
01:33:56,450 --> 01:34:00,450
and want to talk about that this is this is the beginning

1396
01:34:00,450 --> 01:34:04,410
there's a lot of data mining activities we just do that without even knowing how

1397
01:34:04,430 --> 01:34:05,240
do it

1398
01:34:05,410 --> 01:34:09,930
what problem trying to solve hard solving what does it mean

1399
01:34:10,070 --> 01:34:15,720
what is the effect you know change all that stuff

1400
01:34:15,740 --> 01:34:19,850
so my summary of the magnetic challenges i exposed

1401
01:34:19,890 --> 01:34:21,620
as with the data

1402
01:34:21,640 --> 01:34:24,030
but the data mining inside the process

1403
01:34:24,050 --> 01:34:28,600
but in domain knowledge in many settings where you can do

1404
01:34:28,620 --> 01:34:31,160
very very much better humans

1405
01:34:31,200 --> 01:34:34,390
life cycle maintenance and met

1406
01:34:34,410 --> 01:34:42,800
hopefully they will just some of the technical and to give you some more examples

1407
01:34:44,660 --> 01:34:48,620
in the meantime i go back to this last item

1408
01:34:48,620 --> 01:34:49,640
which is

1409
01:34:49,680 --> 01:34:50,870
that is

1410
01:34:52,700 --> 01:34:56,280
i don't our ultimate users don't understand data mining

1411
01:34:56,310 --> 01:35:00,260
but they understand it and business and our responsibility to figure out how to type

1412
01:35:04,390 --> 01:35:08,070
i talked about his slide before i think

1413
01:35:08,160 --> 01:35:15,050
it's basically is the mapping of the capabilities enabled by data in driving the business

1414
01:35:15,080 --> 01:35:21,260
integration of data driven capabilities in driving the business

1415
01:35:21,280 --> 01:35:25,050
and addition of metrics are you managed business

1416
01:35:25,070 --> 01:35:27,760
that is the area where

1417
01:35:30,510 --> 01:35:32,370
that actually brings me be

1418
01:35:32,430 --> 01:35:36,280
near the end of the story of the it's not the ultimate

1419
01:35:36,310 --> 01:35:42,010
so the mixed group actually probably the happiest of my life

1420
01:35:42,530 --> 01:35:45,850
so why join you

1421
01:35:45,870 --> 01:35:47,120
o thing

1422
01:35:48,850 --> 01:35:54,330
after many many months me saying no matter what appeared to be acquired really

1423
01:35:54,330 --> 01:35:59,060
this is what I'll show and it sort of follows a timeline from like ninteen sixtees

1424
01:35:59,060 --> 01:36:00,440
to pretty much today

1425
01:36:02,390 --> 01:36:09,140
and we'll start with the simplest one so the one that I already introduced at some intuitive level the

1426
01:36:09,140 --> 01:36:16,730
the erdos-renyi random graph model, right, where we have N vertices and we'll connect each pair I I D with some probability P

1427
01:36:16,760 --> 01:36:22,680
and there are two variants to the model, one is called G N P and the other one is called G N M G N P

1428
01:36:22,720 --> 01:36:23,820
is the is the

1429
01:36:24,120 --> 01:36:28,210
is the formulation up here right, so we will we will have

1430
01:36:30,160 --> 01:36:31,980
we will have

1431
01:36:32,000 --> 01:36:37,940
we will have each edge appear with probability P so

1432
01:36:37,980 --> 01:36:39,620
in ex this is the this is the

1433
01:36:40,460 --> 01:36:54,620
the probability that we will have M edges in the network, so here here and in the other one we fix the number of nodes and the number of edges and we are just throwing them in, so, right, this is this this one would have some variation in the number of edges we've seen in the network, while this one

1434
01:36:54,640 --> 01:36:57,540
is exactly specified

1435
01:36:58,700 --> 01:37:02,640
as I said these does not mimic reality because because it has poisson

1436
01:37:03,560 --> 01:37:04,710
degree distributions

1437
01:37:04,760 --> 01:37:12,130
but what is nice about it is that you can analyze it very it's you can analyze it easily so to say

1438
01:37:12,160 --> 01:37:25,810
right so for example you it's easy to show that it has a poisson degree distribution and you can just ask what's the probability of a node having a degree K right, and it needs to have K nodes that K K of the edges have to succeed and

1439
01:37:25,830 --> 01:37:31,210
and N minus K edges must not succeed and this is the number of ways I can I can choose

1440
01:37:31,220 --> 01:37:37,140
K K nodes to connect to out of N that I have in the network and if you play with this a bit

1441
01:37:37,180 --> 01:37:40,980
you get the the form of the poisson distribution, again

1442
01:37:41,000 --> 01:37:47,560
what you can be interested in is is to say okay I have N nodes, now I'm throwing in edges one by one and the question is

1443
01:37:48,460 --> 01:37:53,240
how is the graph being connected as the edges are thrown in and

1444
01:37:53,260 --> 01:38:03,300
here is the result, right, so what I talk what I'm interested in is the size of the largest connected component, what I mean by that is what is what is the largest number of nodes that are connected

1445
01:38:03,310 --> 01:38:15,460
and what turns out is that when when my average degree is less than one, then all components are are small, right, they are of size log N but but as soon as my average degree gets just a bit about above

1446
01:38:16,240 --> 01:38:33,820
one, I will have a giant component that pretty much spans the whole graph, right, the giant compo I will have a component that is of linear size and all other components are of size log N and they are pre they are a tree plus an edge, right, they are they are cycles and you you you can easily easily show this

1447
01:38:36,390 --> 01:38:48,350
let me skip this one, what you can also do and it's the calculation are easy, but it's sort of impressive, is you can calculate the probability that in a particular graph you will you will have a particular subgraph

1448
01:38:48,370 --> 01:38:52,500
right, so what I'm showing here is

1449
01:38:53,660 --> 01:38:54,850
what are so

1450
01:38:54,870 --> 01:38:57,060
this would be the increasing P

1451
01:38:57,800 --> 01:39:10,680
so P is if I have N nodes, P is proportional to N to this power Z right, if Z is minus infinite, then I have my P zero, I have no edges, so my graph looks like this

1452
01:39:10,700 --> 01:39:18,360
if if I have it two minus two then I mostly have just diods like two connected edges and as I as I as I am increasing this

1453
01:39:18,370 --> 01:39:22,020
my graph gets richer, richer and richer and my

1454
01:39:23,640 --> 01:39:26,040
my structures in there get

1455
01:39:26,060 --> 01:39:37,620
get more and more connected and what is nice is because everything is independent, we can easily calculate the probability of suc struc, of such structure existing in a particular graph on N nodes and, I don't know,

1456
01:39:37,640 --> 01:39:39,790
probability of an edge being P

1457
01:39:39,810 --> 01:39:40,790
okay so

1458
01:39:41,460 --> 01:39:43,920
all I want to illustrate here is that

1459
01:39:43,940 --> 01:39:51,420
a lot is known for erdos-renyi graph and the reason why a lot a lot is known because everything is independent, so the math works out nicely

1460
01:39:54,350 --> 01:39:56,430
so just to finish up about this

1461
01:39:56,440 --> 01:40:02,160
why they are good is because they are simple and tractable, right, there are these

1462
01:40:02,200 --> 01:40:14,280
phase transitions on the size of giant component right as I said before if your average degree is less than one or a bit less than one right your graph is disonnected, is just trees, as soon as the average degree crosses one, you get this

1463
01:40:14,300 --> 01:40:17,900
huge giant connected component that is linear in the size of the graph

1464
01:40:17,910 --> 01:40:20,710
what is not good is that

1465
01:40:20,720 --> 01:40:25,840
they don't necessarily correspond to the real graphs or they don't have the properties that we saw in the real graphs

1466
01:40:26,620 --> 01:40:33,060
and there are many extensions of this erdos-renyi or poisson random graphs and one of them is called configuration model

1467
01:40:34,120 --> 01:40:34,830
this is

1468
01:40:34,850 --> 01:40:42,560
how how you can think of, so this would be a way how to generate graphs with a pre-described degree distribution, the idea is that I will have some nodes

1469
01:40:42,560 --> 01:40:54,480
at I will assign a degree to every nodes sampled from some distribution and this is this is what I mean by that, right, this would be a node of degree six this is a degree three, so all I need to do now is know is

1470
01:40:54,500 --> 01:41:01,740
to connect these spikes or viscers that are pointing out of the nodes and

1471
01:41:02,350 --> 01:41:06,960
the so this is one way to think about it, the other way to think about it is to say every node here

1472
01:41:06,960 --> 01:41:22,080
has, so I have these supernodes, but then have then have little nodes inside here, right, so this has degree of six, so there are six nodes six nodes it has, so what I can do now is I can generate a random graph on these blue blue notes

1473
01:41:22,100 --> 01:41:25,710
and if an and if everything is fine, then

1474
01:41:25,740 --> 01:41:35,300
most of the edges will go across inbetween these super supernodes and I will get a random graph with a pre-described degree sequence, right, what what would what

1475
01:41:35,310 --> 01:41:42,310
what is what what becomes a problem in this model is that it can happen that you will connect nodes inside the same supernode

1476
01:41:42,330 --> 01:41:44,290
or that you will create selfloops

1477
01:41:45,040 --> 01:41:51,780
but if your probability is small enough, then then this error turn gets small and you can sort of neglect that

1478
01:41:53,000 --> 01:41:55,440
so another model that is

1479
01:41:57,780 --> 01:42:04,950
coming from more social network side is called exponential random graphs or P star models

1480
01:42:04,980 --> 01:42:08,020
and this I think also comes from like seventies or so

1481
01:42:10,310 --> 01:42:12,430
so what is the idea

1482
01:42:13,500 --> 01:42:17,840
the idea is to to take a graph and measure

1483
01:42:18,760 --> 01:42:25,360
some properties of it, so like this epsilon be some measurable property of the graph, for example, it could be

1484
01:42:25,370 --> 01:42:37,020
number of edges in the network number of sta structures like this, number of tristars, number of the triangles in the network and so on, so and all that you do then is to say

1485
01:42:37,020 --> 01:42:40,880
probability of observing a graph I have is like

1486
01:42:42,640 --> 01:42:50,250
is is a linear combination of my properties times some weight, right, so I am having this exponential distribution

1487
01:42:50,280 --> 01:42:57,480
or a log linear model, right, where I have where I need to figure out these weights when I'm fitting the model and I'm given these properties

1488
01:42:57,520 --> 01:43:00,060
that that I can that I can define

1489
01:43:00,060 --> 01:43:03,040
from just moving the parentheses

1490
01:43:03,120 --> 01:43:07,310
it's it's like and it's not like that

1491
01:43:07,330 --> 01:43:08,760
easy to

1492
01:43:08,770 --> 01:43:09,540
i mean the

1493
01:43:09,540 --> 01:43:12,700
to prove it to prove that this is correct after

1494
01:43:12,700 --> 01:43:17,390
go into the gory details of matrix multiplication do it both ways and see that

1495
01:43:17,390 --> 01:43:19,930
you come out the same

1496
01:43:19,970 --> 01:43:21,560
maybe i'll leave the

1497
01:43:21,580 --> 01:43:23,850
after do that

1498
01:43:25,510 --> 01:43:26,790
there we go

1499
01:43:29,870 --> 01:43:35,010
that's how i so there's a single matrix i could call it e

1500
01:43:35,100 --> 01:43:40,850
now all what we're talking about these matrices

1501
01:43:40,870 --> 01:43:46,450
tell me one other there is another type of elementary matrix

1502
01:43:46,450 --> 01:43:49,060
and we already said why we might need

1503
01:43:49,060 --> 01:43:55,060
we didn't need it in this case but it's the matrix that exchanges two rows

1504
01:43:55,080 --> 01:43:58,510
it's called the permutation matrix

1505
01:43:58,560 --> 01:44:01,140
and you just like tell me what that

1506
01:44:01,200 --> 01:44:02,260
would be

1507
01:44:02,260 --> 01:44:05,620
so i'm just like this is a slight digression

1508
01:44:05,680 --> 01:44:13,390
and with you so let me get some me where i put a permutation matrix

1509
01:44:13,410 --> 01:44:18,580
you'll see i'm always squeezing stuff and so permutation

1510
01:44:18,600 --> 01:44:21,450
or in fact

1511
01:44:21,470 --> 01:44:27,260
this one like exchange

1512
01:44:30,010 --> 01:44:34,450
so i exchange rows one and two just to make life easy

1513
01:44:34,470 --> 01:44:37,930
so if i had my matrix images due to by two

1514
01:44:40,910 --> 01:44:44,100
suppose i want to find the matrix

1515
01:44:44,890 --> 01:44:48,310
that exchanges those rows

1516
01:44:51,890 --> 01:44:54,990
what is

1517
01:44:55,010 --> 01:45:00,100
so the matrix that exchanges those rows the the role i wanna cdns is there

1518
01:45:00,100 --> 01:45:03,640
so i i better take one of it

1519
01:45:03,660 --> 01:45:07,560
the rule i want here is up top so i'll take one of their them

1520
01:45:07,580 --> 01:45:09,760
so actually i just

1521
01:45:09,770 --> 01:45:13,660
the easy way this is my matrix that i'll call p

1522
01:45:13,660 --> 01:45:18,010
for permutation

1523
01:45:18,060 --> 01:45:19,220
it's the

1524
01:45:20,510 --> 01:45:25,970
actually the easy way to find is just do the thing to the identity matrix

1525
01:45:26,010 --> 01:45:30,220
exchange rose exchange the rows of the identity matrix and then

1526
01:45:30,270 --> 01:45:32,350
that's the matrix that'll do

1527
01:45:32,370 --> 01:45:35,010
do ro exchanges for you

1528
01:45:35,060 --> 01:45:39,540
could i suppose i wanted to exchange columns in

1529
01:45:39,540 --> 01:45:42,490
columns of hardly got into today's lecture but they

1530
01:45:42,510 --> 01:45:44,990
they going to be around

1531
01:45:45,010 --> 01:45:49,700
how could i if i started with this matrix a b c d

1532
01:45:49,720 --> 01:45:51,950
that i one

1533
01:45:51,970 --> 01:45:57,240
i'm not even write this down is going to ask you

1534
01:45:57,260 --> 01:46:01,700
because it because in elimination would we're doing rose

1535
01:46:01,790 --> 01:46:04,790
but suppose we wanted to exchange

1536
01:46:04,830 --> 01:46:06,080
the columns

1537
01:46:06,100 --> 01:46:09,760
of the matrix

1538
01:46:09,890 --> 01:46:15,640
how would i do that what what matrix multiplication what do that job

1539
01:46:15,720 --> 01:46:18,510
actually why not all right it

1540
01:46:18,530 --> 01:46:20,930
so this is like just

1541
01:46:20,950 --> 01:46:23,240
all right under here and then

1542
01:46:23,450 --> 01:46:24,700
did get

1543
01:46:24,720 --> 01:46:30,100
OK suppose i had my matrix a b c d

1544
01:46:30,160 --> 01:46:32,240
and i want to get to

1545
01:46:32,260 --> 01:46:41,240
AC over here and be the here

1546
01:46:41,260 --> 01:46:48,720
what matrix does that

1547
01:46:48,740 --> 01:46:50,220
can i multiply

1548
01:46:50,240 --> 01:46:52,990
i caps matrix

1549
01:46:54,240 --> 01:46:56,620
produces that answer

1550
01:46:56,640 --> 01:47:02,580
and you can see from where i put my hand i was really asking

1551
01:47:02,600 --> 01:47:05,760
can i put the matrix here on the left

1552
01:47:07,240 --> 01:47:10,450
exchange columns and the answer is

1553
01:47:11,990 --> 01:47:15,410
if i'm on this i'm just bringing out again this point that when i multiply

1554
01:47:15,410 --> 01:47:17,370
on the left

1555
01:47:17,390 --> 01:47:19,470
i'm doing row operation

1556
01:47:19,490 --> 01:47:24,510
if i want to do a column operation where do i put a permutation matrix

1557
01:47:26,030 --> 01:47:27,490
on the right

1558
01:47:27,510 --> 01:47:29,200
if i put it here

1559
01:47:29,200 --> 01:47:31,720
where i just barely left room for it

1560
01:47:31,770 --> 01:47:36,790
so i'll exchange the two columns of the identity

1561
01:47:36,830 --> 01:47:39,330
then it comes out right

1562
01:47:39,370 --> 01:47:42,160
because now i multiply wine column at a time

1563
01:47:42,200 --> 01:47:46,770
this the this is the first column and says take one take none of that

1564
01:47:46,770 --> 01:47:50,430
column one of this one and the guy

1565
01:47:50,450 --> 01:47:54,910
over here take one and this one and this one and you've got a c

1566
01:47:55,930 --> 01:47:56,910
in short

1567
01:47:56,930 --> 01:48:00,850
to do column operations the matrix multiplies on the right

1568
01:48:00,850 --> 01:48:04,080
to do operations it multiplies on the left

1569
01:48:05,560 --> 01:48:09,530
OK and its role operations that we're really doing

1570
01:48:15,330 --> 01:48:16,510
and of course

1571
01:48:16,540 --> 01:48:17,930
do it

1572
01:48:18,100 --> 01:48:25,290
i mentioned in passing but i better say it very clearly

1573
01:48:25,330 --> 01:48:30,200
that you can exchange the orders of matrices and and that's just the point i

1574
01:48:30,200 --> 01:48:31,740
was making in here

1575
01:48:31,790 --> 01:48:36,040
eight times b is not the same as the times

1576
01:48:36,060 --> 01:48:40,600
you have to keep these matrices in there

1577
01:48:40,620 --> 01:48:43,310
gauss given order here

1578
01:48:44,240 --> 01:48:46,390
you but

1579
01:48:46,740 --> 01:48:47,890
but you can

1580
01:48:47,890 --> 01:48:50,580
and your q never lasted big jam

1581
01:48:50,620 --> 01:48:52,320
then you can be trapped

1582
01:48:52,400 --> 01:48:54,030
so your

1583
01:48:54,040 --> 01:48:57,910
at the end of two thousand a million samples or

1584
01:48:57,990 --> 01:49:02,810
five million samples you're that happens in practice

1585
01:49:02,890 --> 01:49:06,210
we have these transitions

1586
01:49:06,280 --> 01:49:07,680
are still there

1587
01:49:07,730 --> 01:49:09,800
if you make you too broad

1588
01:49:09,830 --> 01:49:15,580
then you get a lot of rejections they tend to stay alive in the sense

1589
01:49:15,590 --> 01:49:17,560
and this also can happen from

1590
01:49:17,600 --> 01:49:20,280
with these things are useful for signals ising models

1591
01:49:20,280 --> 01:49:23,920
you know it's quite common that you could perform market share with takes a ten

1592
01:49:23,930 --> 01:49:25,680
to the fifteenth steps

1593
01:49:25,720 --> 01:49:27,530
to make the transition

1594
01:49:28,990 --> 01:49:30,670
choose the right heu

1595
01:49:30,710 --> 01:49:32,820
well i everything works well

1596
01:49:32,880 --> 01:49:37,720
so next will also see throughout this hour how to come up with good cues

1597
01:49:37,820 --> 01:49:41,810
that this works well

1598
01:49:42,510 --> 01:49:45,190
to answer your question

1599
01:49:45,240 --> 01:49:46,600
this is how you

1600
01:49:46,620 --> 01:49:48,800
it's not as bad

1601
01:49:50,040 --> 01:49:54,300
this is the transition model suppose you're currently state x

1602
01:49:54,350 --> 01:49:57,290
and you want to jump into a continuous space

1603
01:49:57,300 --> 01:50:01,030
you want jump to a subset of the space called b

1604
01:50:01,040 --> 01:50:02,190
capital b

1605
01:50:02,220 --> 01:50:04,610
so we're looking at the probability

1606
01:50:04,670 --> 01:50:06,220
after that if you are

1607
01:50:06,230 --> 01:50:09,180
if your current status here

1608
01:50:09,220 --> 01:50:14,420
what is the chances that you will move to the set of the state space

1609
01:50:14,560 --> 01:50:19,360
and essentially that algorithm i wrote it is an algorithm but

1610
01:50:19,380 --> 01:50:22,940
what that this is really a conditional probability

1611
01:50:22,940 --> 01:50:25,170
you can write to conditional probability

1612
01:50:25,190 --> 01:50:28,340
not on this table you can actually write it in terms of number of mixes

1613
01:50:30,770 --> 01:50:33,170
and what it does is this

1614
01:50:33,190 --> 01:50:37,640
when there is acceptance

1615
01:50:37,660 --> 01:50:42,960
the probability of moving is just the proposal probability times the acceptance

1616
01:50:43,090 --> 01:50:50,060
weighted by the acceptance probability so in blue is what happens when you get accepted

1617
01:50:50,790 --> 01:50:52,860
so you start with x

1618
01:50:52,910 --> 01:50:56,800
you propose to move to the space the if it gets accepted

1619
01:50:56,860 --> 01:51:01,420
then that gets weighted by the acceptance of x given b

1620
01:51:02,020 --> 01:51:06,250
so like an important so proprietary from q in your weight by the weight

1621
01:51:06,610 --> 01:51:09,790
eight in this case

1622
01:51:09,800 --> 01:51:12,460
when you reject

1623
01:51:12,500 --> 01:51:13,610
that is

1624
01:51:15,620 --> 01:51:18,520
and by the way i the way i wrote x does not belong to be

1625
01:51:18,520 --> 01:51:20,540
is because i want to avoid the case

1626
01:51:20,550 --> 01:51:24,240
because if you start here you end up here and it's because of the rejection

1627
01:51:24,300 --> 01:51:26,920
still in the same place

1628
01:51:28,870 --> 01:51:31,450
rejection is the opposite is one

1629
01:51:31,450 --> 01:51:36,370
minus the probability of going anywhere else in the space

1630
01:51:36,390 --> 01:51:39,120
excluding b

1631
01:51:41,840 --> 01:51:43,180
and we can

1632
01:51:43,180 --> 01:51:45,530
right is not compact notation

1633
01:51:45,550 --> 01:51:48,690
i'm actually going to leave it as an exercise another thing you can ask me

1634
01:51:48,690 --> 01:51:49,520
to break

1635
01:51:49,520 --> 01:51:52,730
well you can take that

1636
01:51:52,750 --> 01:51:56,510
and using indicator functions you can write it in this form and if you pick

1637
01:51:56,510 --> 01:51:58,790
any book on markov chain monte carlo

1638
01:51:58,840 --> 01:52:03,160
this is the first equation you find describing metropolis hastings

1639
01:52:03,190 --> 01:52:04,990
and this ensured telling is

1640
01:52:05,020 --> 01:52:11,240
with this is the acceptance probability and this is the rejection problem

1641
01:52:11,290 --> 01:52:14,760
and this is the way of

1642
01:52:14,760 --> 01:52:19,060
this is the conditional probability the probability of b given x

1643
01:52:19,090 --> 01:52:20,240
and that's the

1644
01:52:20,240 --> 01:52:21,770
transition matrix

1645
01:52:21,780 --> 01:52:26,320
courses in the country in a discrete state space you this gets replaced by some

1646
01:52:26,320 --> 01:52:29,270
and you can rewrite the some matrix

1647
01:52:29,320 --> 01:52:34,210
in the continuous case you need to know by two hundred

1648
01:52:36,220 --> 01:52:40,680
as an exercise you can take that expression and you can show that the expression

1649
01:52:40,690 --> 01:52:43,600
satisfies detailed problems

1650
01:52:43,620 --> 01:52:46,740
and because of that it's about

1651
01:52:47,240 --> 01:52:52,640
the first time there was proposed in the forties by the way

1652
01:52:54,600 --> 01:52:57,800
random walks are random walk is symmetric

1653
01:52:57,820 --> 01:53:00,170
if you if this is the gaussians

1654
01:53:00,210 --> 01:53:04,540
because the constant of x star minus XI square's the same six i minus six

1655
01:53:04,540 --> 01:53:06,090
times square

1656
01:53:06,110 --> 01:53:08,460
these two terms castle

1657
01:53:08,500 --> 01:53:12,100
when the council which we have something called the metropolis algorithm

1658
01:53:12,120 --> 01:53:14,920
the first one that was proposed in the past

1659
01:53:14,960 --> 01:53:17,180
in hastings in the seventies

1660
01:53:17,230 --> 01:53:20,700
them up with the idea of using an arbitrary proposal distribution

1661
01:53:20,700 --> 01:53:22,350
is zero

1662
01:53:22,370 --> 01:53:24,200
so we only need to keep

1663
01:53:24,210 --> 01:53:25,130
we want

1664
01:53:25,150 --> 01:53:27,400
the probability mass that's above that line

1665
01:53:27,420 --> 01:53:31,790
and that's what this this that's the situation is going to be

1666
01:53:31,970 --> 01:53:35,040
two clips multivariate gaussians

1667
01:53:35,050 --> 01:53:39,170
and so to do this project have to find the mean and covariance matrix of

1668
01:53:39,170 --> 01:53:41,850
the clip multivariate gaussians

1669
01:53:44,380 --> 01:53:46,110
so sounds think it might be a lot of work

1670
01:53:47,520 --> 01:53:49,660
chris bishop pointed out in his lecture yesterday

1671
01:53:49,680 --> 01:53:55,840
you can find moments of exponential family distributions very easily thing derivatives of the normalizing

1672
01:53:56,840 --> 01:53:58,800
in fact that's the trick that i'm going to use

1673
01:53:58,880 --> 01:54:01,450
but in fact the trick i recommend you should always use one could when doing

1674
01:54:01,450 --> 01:54:03,110
EP updates

1675
01:54:03,150 --> 01:54:06,360
and for this particular problem is actually quite simple to use

1676
01:54:11,610 --> 01:54:13,100
by the way if you want the

1677
01:54:13,110 --> 01:54:14,130
if you want the

1678
01:54:17,250 --> 01:54:20,310
you want the general form of the DP updates

1679
01:54:20,320 --> 01:54:23,850
you can go to my what web page you find this any document called EP

1680
01:54:23,920 --> 01:54:25,830
quick reference

1681
01:54:25,840 --> 01:54:29,550
and it gives you all the equations you need to give you the the equations

1682
01:54:29,550 --> 01:54:32,860
for dividing multivariate gaussians month-by-month gaussians

1683
01:54:32,960 --> 01:54:35,930
and also gives you the equations for computing

1684
01:54:35,940 --> 01:54:37,190
the moment

1685
01:54:37,290 --> 01:54:39,040
of this

1686
01:54:39,060 --> 01:54:41,800
function fw thank you might

1687
01:54:41,850 --> 01:54:47,150
and the former is actually quite nice all you do is compute the normalizing constant

1688
01:54:47,190 --> 01:54:49,760
take it is the normalizing constant

1689
01:54:49,760 --> 01:54:51,700
and you can directly compute you

1690
01:54:51,720 --> 01:54:56,760
you should do some linear algebra and you get the mean and covariance matrix

1691
01:54:56,960 --> 01:55:05,190
OK so for this problem

1692
01:55:05,250 --> 01:55:06,910
all we need to compute

1693
01:55:08,740 --> 01:55:19,360
try this again

1694
01:55:19,380 --> 01:55:22,590
so for this problem we just need to compute the normalizing constant and then take

1695
01:55:22,610 --> 01:55:27,420
because there's so what is the normalizing constants the normalizing constant let's make sure i'm

1696
01:55:27,420 --> 01:55:30,430
using red here

1697
01:55:30,490 --> 01:55:34,310
it is normally called that z and it's going to be a function of ni

1698
01:55:35,020 --> 01:55:38,110
in not i

1699
01:55:38,180 --> 01:55:40,990
that's the integral over w

1700
01:55:42,330 --> 01:55:45,820
my factor

1701
01:55:46,090 --> 01:55:48,190
q i w

1702
01:55:48,340 --> 01:55:50,400
so is one integral

1703
01:55:50,420 --> 01:55:53,300
an integrative that i don't need to compute the

1704
01:55:53,310 --> 01:55:58,370
in two rule the mean of the interval the variance which would be very complicated

1705
01:56:00,100 --> 01:56:03,610
so what does that also that's just the integral of w

1706
01:56:03,850 --> 01:56:05,530
this indicator function

1707
01:56:05,540 --> 01:56:08,190
y i x i transpose w

1708
01:56:08,240 --> 01:56:09,940
one zero

1709
01:56:09,960 --> 01:56:17,760
and we have a gaussian distribution

1710
01:56:18,990 --> 01:56:21,820
now here's a nice trick so

1711
01:56:21,840 --> 01:56:23,620
this quantity here

1712
01:56:23,650 --> 01:56:26,610
i can call you

1713
01:56:26,610 --> 01:56:30,260
all right and the only thing random here's w

1714
01:56:30,270 --> 01:56:33,760
OK so the just taking a linear come a linear

1715
01:56:33,770 --> 01:56:36,290
operation ww w

1716
01:56:36,330 --> 01:56:37,940
so i can get

1717
01:56:37,970 --> 01:56:41,260
distribution on you

1718
01:56:41,260 --> 01:56:43,010
so if w is distributed

1719
01:56:43,030 --> 01:56:45,690
according to this multivariate gaussians

1720
01:56:45,710 --> 01:56:47,770
what's the distribution of u

1721
01:56:48,800 --> 01:56:50,610
if i knew the distribution of u

1722
01:56:50,630 --> 01:56:55,180
this integral is just comes down to what's the probability that he was going zero

1723
01:56:55,460 --> 01:56:59,990
so one way to interpret this integral is just what's the probability that y times

1724
01:56:59,990 --> 01:57:02,340
actually transfer of is zero

1725
01:57:02,350 --> 01:57:05,060
and so the final decision view i can deduce that you want to provide the

1726
01:57:05,060 --> 01:57:08,210
user interface

1727
01:57:08,330 --> 01:57:12,620
so in other words

1728
01:57:12,650 --> 01:57:16,350
this whole thing is just the probability that user

1729
01:57:16,390 --> 01:57:19,390
where u is that is that quality there

1730
01:57:19,440 --> 01:57:23,830
so what's the what is the distribution of u

1731
01:57:23,830 --> 01:57:25,350
anyone remember there

1732
01:57:25,370 --> 01:57:28,830
gaussian identities

1733
01:57:29,580 --> 01:57:32,500
so if i have multivariate gaussians mean

1734
01:57:33,110 --> 01:57:34,780
and not i

1735
01:57:34,790 --> 01:57:37,120
and i want to take the inner product and i think the inner product that

1736
01:57:37,120 --> 01:57:39,470
goes in very well with exi transpose

1737
01:57:39,520 --> 01:57:40,740
what's the mean

1738
01:57:46,110 --> 01:57:50,080
it's all just linear and everything is linear so

1739
01:57:50,120 --> 01:57:52,930
the mean of that is just going to be

1740
01:57:55,820 --> 01:58:00,590
and the covariance matrix is also easy to work out is was going to be

1741
01:58:00,630 --> 01:58:05,600
thanks i transpose v not i x

1742
01:58:05,600 --> 01:58:11,880
and the way it is america's it's only plus or minus one

1743
01:58:11,900 --> 01:58:13,690
OK so that's the distribution of u

1744
01:58:14,510 --> 01:58:17,850
the whole antibodies was and it was probably the user and zero

1745
01:58:17,900 --> 01:58:20,710
and we know what to do that that's just the in CDF

1746
01:58:20,750 --> 01:58:23,640
so the answer is

1747
01:58:23,660 --> 01:58:26,260
the goes and this is the this is the symbol used for the gaussian in

1748
01:58:27,430 --> 01:58:30,660
and we just take the mean

1749
01:58:30,700 --> 01:58:33,320
of that variable

1750
01:58:33,360 --> 01:58:39,470
divided by the standard deviation of that variable

1751
01:58:39,520 --> 01:58:44,790
i session drummer should i say OK let's let's go to a different page

1752
01:58:45,900 --> 01:58:50,090
so the probability that used in zero

1753
01:58:50,140 --> 01:58:52,670
is a gaussian cdf

1754
01:58:52,720 --> 01:58:55,610
first some the general form it's always going to be

1755
01:58:55,660 --> 01:58:57,610
the the

1756
01:58:57,610 --> 01:59:00,610
i mean if you divided by the standard deviation you

1757
01:59:00,710 --> 01:59:02,610
so is the case for gaussians

1758
01:59:02,620 --> 01:59:05,110
in this particular case

1759
01:59:05,150 --> 01:59:06,760
that is why i

1760
01:59:07,090 --> 01:59:09,740
transpose that i

1761
01:59:09,780 --> 01:59:11,360
over the square root

1762
01:59:11,380 --> 01:59:14,170
x i transpose v nine

1763
01:59:14,190 --> 01:59:17,460
x i

1764
01:59:17,620 --> 01:59:20,580
OK so now we have essentially closed form

1765
01:59:20,600 --> 01:59:23,620
formula for that normalizing constant

1766
01:59:23,650 --> 01:59:27,080
and now to compute the mean and variance that the PRC and we just take

1767
01:59:28,490 --> 01:59:32,320
and i won't go into all that there easy because of the gaussians here is

1768
01:59:34,250 --> 01:59:39,200
got its report completely serves

1769
01:59:39,260 --> 01:59:43,060
so if we all that's it's clear now how easy it is to compute the

1770
01:59:43,070 --> 01:59:44,420
the moment

1771
01:59:44,450 --> 01:59:46,390
the distribution p

1772
01:59:50,450 --> 01:59:52,240
coming back to the problem

1773
01:59:52,250 --> 01:59:55,310
and what's the performance on this problem so

1774
01:59:55,350 --> 01:59:56,610
what i'm showing here

1775
01:59:56,620 --> 01:59:58,110
is that i'm showing the

1776
01:59:58,140 --> 02:00:01,860
different ways of doing bayesian inference for this problem again which is that those three

1777
02:00:01,860 --> 02:00:02,940
data points

1778
02:00:02,990 --> 02:00:09,520
just very synthetic problem with the data points if you want if you want

1779
02:00:09,520 --> 02:00:13,530
you know running times accuracy and sort of real life datasets and you can find

1780
02:00:13,530 --> 02:00:17,610
those in my thesis by robert results here is going to give example of the

1781
02:00:20,190 --> 02:00:23,090
so what i measuring years and measuring the computation time

1782
02:00:23,150 --> 02:00:25,590
first is the air the air is

1783
02:00:25,610 --> 02:00:28,060
the distance to the true posterior mean

1784
02:00:28,080 --> 02:00:31,490
it's the same sort of error measure been using in the previous slide so is

1785
02:00:31,490 --> 02:00:35,090
is arrested the posterior mean close to the true posterior mean

1786
02:00:36,970 --> 02:00:38,910
albums and went to compare against

1787
02:00:39,800 --> 02:00:41,560
first of all the SVM

1788
02:00:41,570 --> 02:00:46,290
secondly which is maybe you you might put on their secondly there's a monte carlo

