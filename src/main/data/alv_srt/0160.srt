1
00:00:00,000 --> 00:00:02,820
not use your brain and just use the computer

2
00:00:03,080 --> 00:00:05,270
gives you much work this

3
00:00:05,280 --> 00:00:08,200
so what you can do is you can just pick a random pair x and

4
00:00:08,200 --> 00:00:10,530
x prime from the training data

5
00:00:10,550 --> 00:00:13,940
look at the distribution of the squared distances

6
00:00:13,960 --> 00:00:19,130
and i think the point one point five point nine one

7
00:00:19,180 --> 00:00:22,880
you pick those as values to sigma square

8
00:00:22,890 --> 00:00:25,220
the knowledge that produce the problem of

9
00:00:25,270 --> 00:00:30,200
finding the right kind with the one of choosing among maybe three different kernel with

10
00:00:30,220 --> 00:00:33,080
they're all going to be in the right order of magnitude

11
00:00:33,090 --> 00:00:37,330
at least for my experiment so far i've seen that

12
00:00:38,750 --> 00:00:41,780
this never performed significantly worse than really

13
00:00:41,790 --> 00:00:44,530
during the optimal value

14
00:00:44,590 --> 00:00:47,210
so therefore it immediately end up

15
00:00:47,230 --> 00:00:49,230
with a SVM classifier

16
00:00:49,240 --> 00:00:52,590
we get the kernel with right we get the scale right

17
00:00:52,600 --> 00:00:55,290
consider well fiddling around with c

18
00:00:55,300 --> 00:00:59,660
and you get something that will work fairly recently spread out of the box without

19
00:00:59,660 --> 00:01:03,000
being too much validation

20
00:01:04,430 --> 00:01:08,260
and you can see the incredible exactly

21
00:01:08,280 --> 00:01:10,080
to make it

22
00:01:11,930 --> 00:01:14,820
i think it's a good time out

23
00:01:16,350 --> 00:01:21,610
basically question then break and will break for ten minutes and i'm supposed to tell

24
00:01:21,610 --> 00:01:24,690
you that you're not supposed to be at the of

25
00:01:24,700 --> 00:01:27,570
OK a question

26
00:01:27,580 --> 00:01:33,550
but how that will reconvene at twelve thirty

27
00:01:35,200 --> 00:01:42,900
i failed and it's just me but but

28
00:01:46,230 --> 00:01:47,310
this is going to be

29
00:01:47,320 --> 00:01:49,110
a little bit more

30
00:01:49,130 --> 00:01:52,590
challenging in terms of mass

31
00:01:52,610 --> 00:01:55,350
and also contains a bit more challenging

32
00:01:56,660 --> 00:02:00,090
what people here now is also going to help you

33
00:02:00,130 --> 00:02:03,580
four other pictures that may have a graphical models

34
00:02:03,790 --> 00:02:05,850
and during this week next week

35
00:02:05,860 --> 00:02:07,510
it's going to be useful

36
00:02:07,520 --> 00:02:11,250
but what we will do is now take a sharp deviation from what you might

37
00:02:11,250 --> 00:02:15,900
have seen until now namely essentially problems of regression and classification

38
00:02:15,910 --> 00:02:18,640
which deal with very simple scalar values

39
00:02:18,690 --> 00:02:20,740
random variables

40
00:02:20,810 --> 00:02:22,690
and this is something that happened over

41
00:02:22,700 --> 00:02:24,690
over the past five years

42
00:02:24,700 --> 00:02:29,670
and that's i think we're at the moment the most exciting part is the current

43
00:02:31,650 --> 00:02:32,990
at some point

44
00:02:33,000 --> 00:02:37,460
given that i probably don't have enough time to recover both have to ask you

45
00:02:37,460 --> 00:02:39,200
what you prefer namely

46
00:02:39,220 --> 00:02:40,340
the prefer

47
00:02:40,360 --> 00:02:44,250
the page ranking for sequence annotation

48
00:02:47,000 --> 00:02:48,930
OK well i can

49
00:02:48,940 --> 00:02:50,750
talk about twice as fast

50
00:02:50,770 --> 00:02:57,530
and you will probably understand very little of all other but prefers web pages

51
00:02:58,200 --> 00:03:02,890
and like sequence annotation that being well basically i have some binary sequence i want

52
00:03:03,510 --> 00:03:09,080
annotated with certain properties of prince's nineteen tagging about that

53
00:03:10,090 --> 00:03:15,660
now none the wiser was i think the numbers are about even

54
00:03:15,720 --> 00:03:17,260
well let's see

55
00:03:17,390 --> 00:03:21,530
maybe i'll briefly cover the sequence then look at the web pages

56
00:03:21,580 --> 00:03:23,830
are not

57
00:03:23,880 --> 00:03:27,450
without the first though i look at multiple

58
00:03:27,500 --> 00:03:31,960
the reason for that being that actually these are the things we fancy version of

59
00:03:31,990 --> 00:03:35,610
the multiclass

60
00:03:36,510 --> 00:03:42,830
but what the best all love know well you should be able full graph

61
00:03:42,850 --> 00:03:44,850
optimal separating hyperplane

62
00:03:46,320 --> 00:03:51,040
i'm just going to nominate it looks breakfast out of the multiclass condition

63
00:03:52,910 --> 00:03:55,850
so what i'm going to do is i'm going to say

64
00:03:55,890 --> 00:03:58,870
well remember what we had one being one

65
00:03:58,950 --> 00:04:03,430
i have i can also imposed a condition that looks as follows excited of w

66
00:04:03,480 --> 00:04:08,240
it's great for a then one but it i thought one of the right

67
00:04:08,250 --> 00:04:11,930
why markets what i can do the same thing

68
00:04:11,950 --> 00:04:13,410
up after bit

69
00:04:13,430 --> 00:04:17,160
but what i can actually do is i can say well this is an instance

70
00:04:17,160 --> 00:04:20,010
of multiclass classification problem

71
00:04:20,110 --> 00:04:22,150
if i know one thing here

72
00:04:22,200 --> 00:04:23,940
the these are called

73
00:04:25,670 --> 00:04:27,960
so what do we get

74
00:04:27,960 --> 00:04:29,270
in the perspective

75
00:04:31,780 --> 00:04:32,990
so on this

76
00:04:33,000 --> 00:04:35,390
can be visualized like this

77
00:04:35,390 --> 00:04:36,920
in the matrix times

78
00:04:36,930 --> 00:04:42,290
cross channel and then you will see very nice well the differences

79
00:04:42,380 --> 00:04:43,510
in time

80
00:04:43,530 --> 00:04:45,570
and how they are distributed in

81
00:04:45,580 --> 00:04:47,190
across the channel

82
00:04:47,200 --> 00:04:48,700
then we can

83
00:04:49,620 --> 00:04:53,730
so he had to manually but of course you can also have some some algorithms

84
00:04:53,730 --> 00:04:55,240
to make sure

85
00:04:55,260 --> 00:04:57,470
based on the matrix

86
00:04:57,490 --> 00:04:59,270
appropriate interval

87
00:04:59,430 --> 00:05:01,180
here which was done manually

88
00:05:01,900 --> 00:05:03,200
these in

89
00:05:03,210 --> 00:05:06,860
these values and when value for each gender

90
00:05:06,870 --> 00:05:08,580
it shows these various

91
00:05:10,420 --> 00:05:11,900
so let see here

92
00:05:11,910 --> 00:05:13,290
differences for this

93
00:05:13,320 --> 00:05:15,860
bishop p one component thank you for the

94
00:05:15,880 --> 00:05:18,310
visual and one

95
00:05:18,660 --> 00:05:20,270
p three component

96
00:05:20,360 --> 00:05:23,210
so this

97
00:05:23,250 --> 00:05:24,110
gives an

98
00:05:24,110 --> 00:05:26,850
idea what individuals are good too

99
00:05:29,350 --> 00:05:36,620
OK and so this is that for first first step is is just to use

100
00:05:36,630 --> 00:05:39,180
one with purely for

101
00:05:39,180 --> 00:05:43,430
special features features one way each each end

102
00:05:43,450 --> 00:05:46,370
and if we train on such features

103
00:05:47,450 --> 00:05:50,020
classifiers the nice thing is that

104
00:05:50,050 --> 00:05:52,350
this classifier

105
00:05:52,390 --> 00:05:54,290
is the spatial filters

106
00:05:54,310 --> 00:06:00,000
so we have a a for example is a linear classifier training on purely spatial

107
00:06:01,350 --> 00:06:03,130
we get a bit vector

108
00:06:03,460 --> 00:06:04,990
that is

109
00:06:05,510 --> 00:06:09,670
and the length of the

110
00:06:10,470 --> 00:06:21,140
it became clear to pick up for this

111
00:06:21,220 --> 00:06:24,050
so maybe

112
00:06:24,110 --> 00:06:26,580
just about anything

113
00:06:26,750 --> 00:06:34,680
OK so this is not very very important that because the classifier that that was

114
00:06:34,720 --> 00:06:40,160
trained on this purely spatial features is spatial filtering so you can apply it to

115
00:06:41,070 --> 00:06:42,670
the EEG signal

116
00:06:42,710 --> 00:06:46,460
and then the the weights to different channels and we get

117
00:06:46,490 --> 00:06:48,190
filtered signals that

118
00:06:48,210 --> 00:06:51,070
emphasizes the difference between

119
00:06:51,100 --> 00:06:55,970
the conditions and the nice thing is that we can visualize this filters as the

120
00:06:57,040 --> 00:06:57,950
so there

121
00:06:57,980 --> 00:07:01,380
the classifiers to theatre is one

122
00:07:01,380 --> 00:07:04,200
great value for each channel so we can

123
00:07:04,220 --> 00:07:06,220
displayed is a map and c

124
00:07:06,230 --> 00:07:10,520
what is machine learning and and that learn from from the data and what's the

125
00:07:10,520 --> 00:07:13,350
discrimination is based on

126
00:07:13,360 --> 00:07:19,120
so in this case for the p three hundred component spatial filters if focused centrally

127
00:07:19,130 --> 00:07:22,510
recipes round has evolved and tests

128
00:07:22,530 --> 00:07:24,600
also as weights

129
00:07:28,670 --> 00:07:31,030
and if if we use this

130
00:07:31,050 --> 00:07:36,890
simple classification of separately for for each interval of components that we

131
00:07:36,890 --> 00:07:45,190
detect again this interesting components we get the following error rate this is indicated here

132
00:07:45,360 --> 00:07:48,470
before each component we get different error rate

133
00:07:48,530 --> 00:07:50,060
ranging from o

134
00:07:50,110 --> 00:07:51,500
twelve two

135
00:07:52,960 --> 00:07:54,600
here we already see that

136
00:07:55,830 --> 00:07:57,710
visual components

137
00:07:57,720 --> 00:07:59,900
good for discrimination you

138
00:07:59,910 --> 00:08:01,200
but also this

139
00:08:03,680 --> 00:08:06,410
give some contributions

140
00:08:06,460 --> 00:08:08,820
so and then of course we will not

141
00:08:08,830 --> 00:08:11,930
to stop here we want to integrate this

142
00:08:11,940 --> 00:08:13,750
information from

143
00:08:13,770 --> 00:08:17,750
different temporal intervals so the next step is that we

144
00:08:17,760 --> 00:08:21,430
extract spatiotemporal features

145
00:08:21,440 --> 00:08:24,450
and that's again very

146
00:08:24,460 --> 00:08:29,030
very simple it's just the same as before but now we do it for different

147
00:08:29,030 --> 00:08:32,140
interviews so we have different intervals self-interest

148
00:08:32,180 --> 00:08:34,530
the calculate in each gender

149
00:08:34,550 --> 00:08:37,430
the average value in each of these intervals

150
00:08:37,470 --> 00:08:38,770
and now we get to the

151
00:08:39,810 --> 00:08:42,690
long feature vector in large feature vector

152
00:08:42,690 --> 00:08:43,700
because we get

153
00:08:43,710 --> 00:08:48,370
not only one value for each gender but as many as we have the

154
00:08:48,390 --> 00:08:49,290
so we can

155
00:08:51,120 --> 00:08:52,420
those features

156
00:08:52,430 --> 00:08:56,670
map of the

157
00:08:56,680 --> 00:08:58,050
as a matter of

158
00:08:58,580 --> 00:09:02,560
series of scott

159
00:09:02,570 --> 00:09:07,600
o we cannot allow us to think of it as a as matrix of course

160
00:09:07,600 --> 00:09:09,430
so we have to two dimensions

161
00:09:09,440 --> 00:09:11,750
shannon space and

162
00:09:13,320 --> 00:09:14,530
so but

163
00:09:14,550 --> 00:09:16,670
then these features

164
00:09:16,690 --> 00:09:19,640
already quite high dimensional space here

165
00:09:19,660 --> 00:09:24,220
fifty nine EEG channels in example seven time in terms of interest

166
00:09:24,720 --> 00:09:27,890
dimensionality is fun searching

167
00:09:27,890 --> 00:09:29,200
now it

168
00:09:29,210 --> 00:09:33,300
this is a before and now i added classification

169
00:09:33,330 --> 00:09:37,020
ordinary LDA on the spatial temporal features

170
00:09:37,040 --> 00:09:38,980
and so this is of course

171
00:09:39,000 --> 00:09:41,720
disappointing for years they resided

172
00:09:41,720 --> 00:09:44,310
twenty five percent so it's was

173
00:09:44,380 --> 00:09:46,830
the best thing into

174
00:09:46,850 --> 00:09:47,800
so we

175
00:09:47,830 --> 00:09:50,400
and all the information together

176
00:09:50,410 --> 00:09:54,330
on this concatenated feature vectors classification because

177
00:09:54,390 --> 00:09:56,300
became worse so

178
00:09:56,360 --> 00:09:59,460
i think most of you know what the problem is that if you have a

179
00:09:59,460 --> 00:10:03,300
high dimensional data and some some overfitting

180
00:10:03,310 --> 00:10:05,580
i don't have enough training centers

181
00:10:06,330 --> 00:10:07,900
the good singing

182
00:10:07,910 --> 00:10:10,140
in this situation is that it's not

183
00:10:12,050 --> 00:10:17,680
overfitting on miss estimation but this systematic bias and that one can do something about

184
00:10:18,640 --> 00:10:21,150
and so therefore

185
00:10:21,200 --> 00:10:27,270
we need some formulas these linear classifiers based on the estimation of the mean and

186
00:10:28,550 --> 00:10:30,350
so we have

187
00:10:30,610 --> 00:10:36,650
so here we assume that we have some vectors drawn from a gaussian distribution

188
00:10:36,670 --> 00:10:40,760
and in classification we need to estimate the mean

189
00:10:40,790 --> 00:10:42,910
and the covariance

190
00:10:44,310 --> 00:10:45,680
and and he says

191
00:10:45,690 --> 00:10:48,660
basic formulas to estimate mean and covariance

192
00:10:48,730 --> 00:10:52,770
and the observation is that if we have a high dimensional data not

193
00:10:52,820 --> 00:10:57,890
so many samples then this is a systematic bias in their

194
00:10:57,890 --> 00:11:03,330
the empirical covariance matrix and the system that because of the large i value

195
00:11:03,350 --> 00:11:05,580
of the estimated

196
00:11:05,610 --> 00:11:10,060
the range too large and small i'm not too small

197
00:11:10,080 --> 00:11:14,230
so this is misinformation and then of course it a

198
00:11:14,240 --> 00:11:15,190
which is

199
00:11:15,200 --> 00:11:17,930
simply this formula the difference of the mean

200
00:11:17,950 --> 00:11:20,710
weighted by the inverse covariance matrix

201
00:11:21,110 --> 00:11:22,530
it's not

202
00:11:22,550 --> 00:11:24,740
very good

203
00:11:24,750 --> 00:11:27,560
this covariance matrix is badly estimated

204
00:11:27,570 --> 00:11:30,240
so what does this mean

205
00:11:30,250 --> 00:11:31,960
i can value

206
00:11:31,970 --> 00:11:33,020
so this is

207
00:11:33,690 --> 00:11:35,790
cut to one we have

208
00:11:36,960 --> 00:11:40,910
data distribution of the data were generated talk

209
00:11:40,930 --> 00:11:45,740
some distribution on the true distribution is indicated by the black ellipsoid

210
00:11:45,800 --> 00:11:50,220
and we could draw some samples from the distribution is as shown in blue

211
00:11:50,220 --> 00:11:52,540
and from based on this template

212
00:11:52,560 --> 00:11:55,310
the empirical covariance matrix is calculated and

213
00:11:55,330 --> 00:11:58,790
this is the structure like the red the sort

214
00:12:00,140 --> 00:12:01,330
i value

215
00:12:01,350 --> 00:12:03,770
of the latter diagnosis

216
00:12:03,770 --> 00:12:07,970
disconnected body that's your like very far away articles

217
00:12:10,700 --> 00:12:13,830
given the sensitivity

218
00:12:13,850 --> 00:12:18,340
we found these things to be quite stable actually because once you're

219
00:12:18,380 --> 00:12:22,180
because the way we are doing it is some kind of sequential greedy way

220
00:12:22,230 --> 00:12:26,740
we find sort of go we we use the same node multiple times so that

221
00:12:26,740 --> 00:12:31,700
are actually not so many competitors so there are not too many different places

222
00:12:31,710 --> 00:12:34,670
and because you are just asking sort of

223
00:12:34,690 --> 00:12:38,990
we keep high degree connected nodes speaking and then you do many bands go through

224
00:12:38,990 --> 00:12:43,510
them or they just go separately and that doesn't mean that there are multiple separate

225
00:12:43,520 --> 00:12:45,990
that's because there is just one pattern

226
00:12:46,160 --> 00:12:49,630
they don't make much difference in the in the graph itself

227
00:12:50,020 --> 00:13:01,860
this work

228
00:13:15,370 --> 00:13:17,210
good so

229
00:13:17,250 --> 00:13:20,450
i think we can find elements of machine learning

230
00:13:21,840 --> 00:13:24,720
here for like fitting estimation right

231
00:13:25,240 --> 00:13:26,570
i think

232
00:13:26,590 --> 00:13:28,050
he so

233
00:13:28,100 --> 00:13:32,140
here the difference is that instead of comparing statistics you you can call you are

234
00:13:32,160 --> 00:13:35,040
now comparing likelihoods so it's again sort of

235
00:13:35,050 --> 00:13:36,630
initial statistic

236
00:13:38,100 --> 00:13:40,800
so yeah i would say i would say most here

237
00:13:40,830 --> 00:13:44,920
may be here but this is more like application

238
00:13:46,690 --> 00:13:48,150
i would say that works

239
00:13:48,170 --> 00:13:50,330
if i have to

240
00:13:51,790 --> 00:14:06,380
a show

241
00:14:10,840 --> 00:14:15,330
it's similarly minds

242
00:14:18,510 --> 00:14:21,700
would like send you you know this is the state so they have this sort

243
00:14:21,700 --> 00:14:24,940
of time points when they and it's not time it's more like

244
00:14:24,980 --> 00:14:26,780
what level

245
00:14:33,450 --> 00:14:41,670
a lot of this is a

246
00:14:48,920 --> 00:14:50,010
that's good

247
00:14:50,790 --> 00:14:52,080
even though

248
00:14:52,090 --> 00:14:58,530
it's very different it's like many different networks right so now we say that we

249
00:14:58,530 --> 00:15:02,090
don't see as the network gets larger so we say as the same editor gets

250
00:15:02,090 --> 00:15:04,930
larger diameter gets more but you don't say

251
00:15:04,960 --> 00:15:07,790
you know they can any network of hundred nodes and now take some of the

252
00:15:07,790 --> 00:15:11,710
network of ten million nodes that one of ten million times smaller than the

253
00:15:11,720 --> 00:15:13,820
one of the four hundred

254
00:15:13,840 --> 00:15:18,210
so i think we are talking about two different networks

255
00:15:19,350 --> 00:15:25,710
of course who talks to whom and writing messenger for example what is even surprising

256
00:15:25,840 --> 00:15:29,740
maybe like we have because in africa writing in in

257
00:15:29,750 --> 00:15:32,920
after africa or you know like you see people using internet like the coast of

258
00:15:32,920 --> 00:15:37,250
africa central africa is missing so you like the question is how well does this

259
00:15:37,250 --> 00:15:40,980
six point six correspond to real world because in one sense you could say oh

260
00:15:40,980 --> 00:15:44,330
you have these holes in africa so we have to go around that the concept

261
00:15:44,330 --> 00:15:46,340
of getting through

262
00:15:47,140 --> 00:15:50,310
so you could say you know i mean that i would like to find reasons

263
00:15:50,330 --> 00:15:53,940
both for the same in the real world there should be a bit higher singular

264
00:15:53,940 --> 00:15:57,100
which should be a bit long but i think what is interesting is that you

265
00:15:57,100 --> 00:16:01,800
know taking four million times more data like six orders of magnitude more data

266
00:16:02,790 --> 00:16:04,130
u q

267
00:16:04,160 --> 00:16:06,040
around them

268
00:16:13,290 --> 00:16:15,940
it is

269
00:16:15,950 --> 00:16:23,180
sure that was that was debated but they were using different measure right i mean

270
00:16:23,180 --> 00:16:26,810
the question is whether you blocked what kind of measure plot there and how do

271
00:16:26,810 --> 00:16:31,280
you find out how do find it and the depression is also home getting from

272
00:16:31,280 --> 00:16:35,630
your plastic quite right and the difference that was also that i think that it

273
00:16:35,630 --> 00:16:39,480
was done by bisection here we're not cutting away using realistic the full network and

274
00:16:39,500 --> 00:16:42,500
say the best part of the size and i don't think the full network and

275
00:16:42,500 --> 00:16:47,230
so was the best part of this so we never got anything going

276
00:16:47,240 --> 00:16:51,320
because that was made this so we be sections so it's not clear whether small

277
00:16:51,330 --> 00:16:55,350
pieces where do they come from whether they come from because the when you do

278
00:16:55,350 --> 00:16:56,320
that half

279
00:16:56,340 --> 00:16:57,260
o point guard

280
00:16:57,470 --> 00:17:03,100
like artificially creating this more pieces because you're cutting some dancing to me

281
00:17:29,630 --> 00:17:38,710
and and region and its grounds for sure

282
00:17:53,380 --> 00:17:55,140
maybe i

283
00:17:55,450 --> 00:18:08,110
i mean we should do they can definitely good good it's about tries

284
00:18:08,110 --> 00:18:12,990
a desirable way so for example if we have some kind of statistical significance more

285
00:18:13,010 --> 00:18:18,330
significant pattern is maybe that's what you want to keep them less significant once you

286
00:18:18,330 --> 00:18:20,130
can you can throw them away

287
00:18:20,160 --> 00:18:23,460
but it is in these contrast could be a huge number

288
00:18:23,460 --> 00:18:26,520
you know they could you know potentially they can be

289
00:18:27,000 --> 00:18:29,390
exponential in terms of

290
00:18:29,520 --> 00:18:32,730
the features

291
00:18:33,750 --> 00:18:38,070
so for example how we can use contrast data mining so for example we can

292
00:18:38,880 --> 00:18:41,910
in domain understanding so for example let's say

293
00:18:41,920 --> 00:18:46,960
in a medical context one could say young children with diabetes

294
00:18:46,960 --> 00:18:49,450
have a greater risk of hospital admission

295
00:18:49,460 --> 00:18:51,510
compared to the rest of the population

296
00:18:51,520 --> 00:18:52,540
so this could be

297
00:18:52,550 --> 00:18:57,080
just an observation that say you take all the datasets from children

298
00:18:57,240 --> 00:19:00,310
you know and then if you find the ones who

299
00:19:00,340 --> 00:19:03,150
generally come to the hospital more often

300
00:19:03,160 --> 00:19:04,630
could be the one from

301
00:19:04,780 --> 00:19:06,090
i don't like diabetes

302
00:19:08,530 --> 00:19:10,320
so therefore you conclude that

303
00:19:10,340 --> 00:19:13,770
if the child is that the take the chance of the child to come to

304
00:19:13,770 --> 00:19:17,780
the hospital is much higher compared to a child who is

305
00:19:20,490 --> 00:19:22,250
and of course it could be much more

306
00:19:22,260 --> 00:19:25,640
nine of sophisticated than those kinds of simple rules

307
00:19:26,200 --> 00:19:30,510
they could be used for building classifiers so for example if you do this contrast

308
00:19:31,340 --> 00:19:34,740
they're like like like some kind of fingerprints so if you see

309
00:19:34,910 --> 00:19:39,650
these these signatures several times then you can easily said that i get this

310
00:19:39,670 --> 00:19:44,570
object has all these patterns that i've seen that belong to to this particular class

311
00:19:44,860 --> 00:19:47,510
and therefore there's a good chance that it belongs to

312
00:19:50,150 --> 00:19:54,980
so i'm going to cover several examples how to build

313
00:19:55,450 --> 00:19:58,530
robert classifiers using these techniques

314
00:20:00,360 --> 00:20:01,460
and we also

315
00:20:01,460 --> 00:20:02,710
showed that

316
00:20:02,730 --> 00:20:06,550
you can also see what the distances so for example

317
00:20:06,600 --> 00:20:10,540
when you when you look at now you know that it's coming in

318
00:20:10,580 --> 00:20:17,150
sometimes the datasets that even though the individuals do some kind of classification earlier you

319
00:20:17,150 --> 00:20:22,450
know when collect the doctor it is quite probable some of them were misclassified or

320
00:20:22,500 --> 00:20:25,020
they're not as accurate because some

321
00:20:25,140 --> 00:20:28,040
that parts were not measured properly

322
00:20:28,040 --> 00:20:30,050
and you know like for example

323
00:20:30,060 --> 00:20:32,800
let's say somebody collected several

324
00:20:32,850 --> 00:20:35,400
diagnostics for an individual

325
00:20:36,570 --> 00:20:40,650
medical experts sees is part of the data and comes to conclusion

326
00:20:40,670 --> 00:20:43,400
and some are not seen in the other part of the data

327
00:20:43,410 --> 00:20:47,480
and then simply said OK this patient must this particular

328
00:20:47,480 --> 00:20:49,520
i know this is all

329
00:20:49,560 --> 00:20:51,330
all or some some

330
00:20:51,390 --> 00:20:52,540
some problems

331
00:20:52,580 --> 00:20:53,480
but this

332
00:20:53,490 --> 00:20:55,450
as you see

333
00:20:55,540 --> 00:20:59,510
more of information that could be wrong and generally people also say that

334
00:20:59,520 --> 00:21:01,230
the more you know typically

335
00:21:01,330 --> 00:21:04,480
we are actually this is fired

336
00:21:04,500 --> 00:21:07,470
i was making in one of his keynote speeches

337
00:21:07,830 --> 00:21:09,170
in the NASA

338
00:21:09,230 --> 00:21:11,720
just to tackle these that's all

339
00:21:14,800 --> 00:21:18,520
people who look at the skies take the pictures and then they want to know

340
00:21:18,520 --> 00:21:22,130
about exist are already below or a galaxy

341
00:21:22,180 --> 00:21:24,890
apparently you know ten or fifteen years ago

342
00:21:25,320 --> 00:21:29,810
they would have huge numbers scientists to go to these images and this this is

343
00:21:29,810 --> 00:21:34,730
a galaxy the this is an nebula of this is the start this is how

344
00:21:34,730 --> 00:21:36,000
they work

345
00:21:37,950 --> 00:21:43,020
and the reason that only a few of them could do that is the number

346
00:21:43,030 --> 00:21:47,110
of dimensions for each object you know the look the spectral components the look at

347
00:21:47,910 --> 00:21:48,430
you know

348
00:21:48,490 --> 00:21:50,290
the location of the space

349
00:21:50,300 --> 00:21:53,240
and they may be you know lots of dimension data

350
00:21:53,340 --> 00:21:54,180
like them

351
00:21:54,200 --> 00:21:59,130
and humans are not good to see you know more than the dimensions to two

352
00:21:59,150 --> 00:22:00,760
to classify these things

353
00:22:00,820 --> 00:22:04,030
so so therefore the more information you like

354
00:22:04,070 --> 00:22:07,660
the experts can actually make more mistakes

355
00:22:08,640 --> 00:22:14,560
now we we not without looking at large complex information and interrelationships between these

356
00:22:14,560 --> 00:22:15,960
these kinds of things

357
00:22:16,630 --> 00:22:20,700
so later when they started playing in the machine

358
00:22:20,840 --> 00:22:23,930
they found that their grandfather these

359
00:22:24,100 --> 00:22:28,310
physicists who are employed to just to the identification of these

360
00:22:28,340 --> 00:22:32,100
these objects that's taken from these

361
00:22:32,150 --> 00:22:36,030
optical telescopes all radio telescopes and so forth

362
00:22:37,510 --> 00:22:42,070
so so you can see that in the US as the dimensions go

363
00:22:42,120 --> 00:22:45,310
the you know some kind of classification

364
00:22:45,360 --> 00:22:51,510
automated way of doing is more important and people do make mistakes in which case

365
00:22:51,520 --> 00:22:54,210
you should be able to correct

366
00:22:54,830 --> 00:22:58,860
those kind of corrections could be done by giving some kind of weighting so for

367
00:22:59,740 --> 00:23:05,960
an instance is classified as lexical class one supposed to be class three let's say

368
00:23:06,010 --> 00:23:10,610
then instead of using you know zero one kind of assignment you might use some

369
00:23:10,610 --> 00:23:15,020
kind of probabilities to statistics things or some weights to them saying that the chance

370
00:23:15,020 --> 00:23:20,760
of this belonging to the classes point seven minutes to the other classes point three

371
00:23:21,420 --> 00:23:25,520
when you have these kinds of waiting then you can do much better analysis and

372
00:23:25,520 --> 00:23:29,960
you can build but similar class for so what we found is that using this

373
00:23:29,960 --> 00:23:34,460
contrast patterns you can actually do the waiting to these things so for example

374
00:23:34,490 --> 00:23:37,680
let's say we collected two datasets

375
00:23:37,740 --> 00:23:39,790
we somehow found the patterns

376
00:23:40,650 --> 00:23:43,850
in each of these things and not dominant in the other

377
00:23:43,900 --> 00:23:47,870
and then you have a take an instance and then you look at how many

378
00:23:47,870 --> 00:23:52,140
of these patterns is that it has so for example that in this particular instance

379
00:23:52,140 --> 00:23:54,300
here is a

380
00:23:54,410 --> 00:23:56,130
let this be

381
00:23:56,150 --> 00:23:58,520
the resonance

382
00:23:58,560 --> 00:24:00,540
frequency omega zero

383
00:24:00,580 --> 00:24:05,740
i'll make it a little straighter

384
00:24:07,500 --> 00:24:12,660
so you start at very low frequency this is zero you start here was have

385
00:24:12,660 --> 00:24:14,680
zero divided by k

386
00:24:15,340 --> 00:24:17,740
we all agreed that was obvious

387
00:24:17,740 --> 00:24:20,740
and then the amplitude will build up

388
00:24:20,750 --> 00:24:22,970
go through maximum

389
00:24:23,010 --> 00:24:24,240
goes down

390
00:24:24,250 --> 00:24:25,660
and ultimately

391
00:24:25,730 --> 00:24:27,650
goes to zero

392
00:24:27,680 --> 00:24:30,370
and this value omega zero

393
00:24:30,510 --> 00:24:32,640
this value is q

394
00:24:32,680 --> 00:24:33,840
times have zero

395
00:24:33,860 --> 00:24:37,230
divided by k

396
00:24:37,670 --> 00:24:40,990
now for those of you

397
00:24:41,000 --> 00:24:42,830
you look very carefully

398
00:24:42,890 --> 00:24:44,660
you may have noticed

399
00:24:44,700 --> 00:24:48,430
that the maximum you that i have drawn

400
00:24:48,500 --> 00:24:49,870
it is not

401
00:24:49,890 --> 00:24:55,090
at omega equals omega zero which may go against your instinct

402
00:24:55,140 --> 00:24:56,950
this maximum

403
00:24:56,960 --> 00:25:02,800
or curse at a frequency which we will call omega max

404
00:25:02,820 --> 00:25:07,740
which is always a little bit below omega zero but for high IQ systems

405
00:25:07,750 --> 00:25:09,620
as i will show you shortly

406
00:25:09,680 --> 00:25:12,970
it is effectively the same

407
00:25:12,970 --> 00:25:15,220
i will come back to this

408
00:25:21,680 --> 00:25:24,060
the face

409
00:25:24,100 --> 00:25:26,900
as a function of omega

410
00:25:26,910 --> 00:25:27,980
this is why

411
00:25:28,070 --> 00:25:31,390
and this is by over two

412
00:25:31,400 --> 00:25:34,560
and if this is omega zero

413
00:25:35,740 --> 00:25:40,620
that delta will change in the following way

414
00:25:40,710 --> 00:25:45,170
it's really hard to imagine then that what he's doing

415
00:25:45,230 --> 00:25:49,070
you are in face at very low frequencies

416
00:25:49,090 --> 00:25:53,480
and resonance precisely omega zero you the by over two

417
00:25:53,520 --> 00:25:55,260
ninety degrees out of phase

418
00:25:55,280 --> 00:25:57,250
and at very high frequencies

419
00:25:57,260 --> 00:25:59,210
you will see that the two out of phase

420
00:25:59,220 --> 00:26:04,090
and i will be able to demonstrate that to you

421
00:26:04,570 --> 00:26:05,950
coming back to this

422
00:26:05,950 --> 00:26:09,520
mysterious maximum not so mysterious actually

423
00:26:09,560 --> 00:26:16,200
where is this what frequency do we have really the maximum amplitude

424
00:26:16,200 --> 00:26:18,150
well to calculate that

425
00:26:18,210 --> 00:26:23,730
you would have to take the derivative of that monster equation

426
00:26:23,740 --> 00:26:28,560
you will have to take the eighty omega

427
00:26:28,590 --> 00:26:31,590
if you go you have to have to be zero

428
00:26:31,600 --> 00:26:33,840
so that's when the maximum occurs

429
00:26:33,900 --> 00:26:37,980
and i'll leave you with that exercise may take you few minutes to do that

430
00:26:37,990 --> 00:26:39,290
you will find them

431
00:26:39,300 --> 00:26:43,980
that omega max

432
00:26:44,000 --> 00:26:46,220
so the real maximum

433
00:26:46,270 --> 00:26:49,700
it is located the maximum in terms of amplitude

434
00:26:49,840 --> 00:26:53,650
is omega zero squared

435
00:26:53,920 --> 00:26:57,530
is gone muskrat over two not for the two

436
00:26:57,590 --> 00:26:58,900
to the power

437
00:26:59,130 --> 00:27:03,090
not so intuitive that it is there

438
00:27:03,100 --> 00:27:07,370
and if you like to write that in terms of q which is often done

439
00:27:08,600 --> 00:27:10,510
omega max

440
00:27:10,550 --> 00:27:13,520
so that is the frequency at which the amplitude

441
00:27:13,600 --> 00:27:15,820
reaches a maximum

442
00:27:15,820 --> 00:27:18,460
is omega zero

443
00:27:18,480 --> 00:27:21,040
times one minus one over two

444
00:27:21,060 --> 00:27:23,520
q squared

445
00:27:23,560 --> 00:27:25,290
and then

446
00:27:25,340 --> 00:27:28,630
the square root of the whole thing

447
00:27:28,680 --> 00:27:32,500
and the reason why this is nice you can immediately if you know q

448
00:27:32,500 --> 00:27:34,560
you can immediately evaluate

449
00:27:34,610 --> 00:27:36,490
but the differences

450
00:27:36,490 --> 00:27:38,800
percentage-wise between omega max

451
00:27:38,810 --> 00:27:42,990
and omega zero

452
00:27:43,030 --> 00:27:44,220
if you want to know

453
00:27:44,230 --> 00:27:47,740
what the maximum amplitude itself is

454
00:27:47,780 --> 00:27:50,750
so what a maxis

455
00:27:50,770 --> 00:27:52,960
so that's really this value

456
00:27:52,980 --> 00:27:56,980
it must be very close to q times have zero overkill but is a little

457
00:27:59,550 --> 00:28:00,860
you can write that

458
00:28:00,930 --> 00:28:03,670
in the following four

459
00:28:03,680 --> 00:28:06,960
that just a matter of algebraic manipulations

460
00:28:06,970 --> 00:28:09,530
you get to q here

461
00:28:09,540 --> 00:28:10,660
you expect

462
00:28:10,680 --> 00:28:12,260
and then downstairs

463
00:28:12,260 --> 00:28:14,740
you get something like this one minus

464
00:28:16,530 --> 00:28:22,540
over four and then you get q squared to the power half

465
00:28:22,870 --> 00:28:25,380
so now let's

466
00:28:25,390 --> 00:28:29,370
but in some numbers that you get some feeling for

467
00:28:29,420 --> 00:28:31,310
the answer is that we have

468
00:28:31,360 --> 00:28:33,370
suppose we have

469
00:28:33,390 --> 00:28:36,470
an example of IQ equals

470
00:28:38,960 --> 00:28:40,990
some of value for q

471
00:28:42,030 --> 00:28:48,100
pendulums that we have q way higher than five take among number before q

472
00:28:48,160 --> 00:28:51,980
if i go to this equation here

473
00:28:52,030 --> 00:28:54,110
two squares twenty five

474
00:28:54,130 --> 00:28:56,150
two ten twenty five fifty

475
00:28:56,160 --> 00:28:59,670
that's two percent but i have to take the square root so there's only one

476
00:28:59,670 --> 00:29:00,980
percent of

477
00:29:01,060 --> 00:29:02,830
so omega zero

478
00:29:02,840 --> 00:29:04,620
omega max

479
00:29:04,630 --> 00:29:08,390
divided by omega zero

480
00:29:08,430 --> 00:29:09,780
is o point

481
00:29:09,780 --> 00:29:13,310
nine nine only one percent lower

482
00:29:13,360 --> 00:29:16,590
there's only one percent below omega zero

483
00:29:16,640 --> 00:29:20,170
and then if you want to know now what a max

484
00:29:20,220 --> 00:29:23,590
so you would think that a max is very close

485
00:29:23,610 --> 00:29:25,420
two q

486
00:29:25,430 --> 00:29:28,460
times have zero over k

487
00:29:29,550 --> 00:29:31,110
it is not

488
00:29:31,170 --> 00:29:33,700
two times it is a little larger

489
00:29:33,740 --> 00:29:34,540
and so

490
00:29:34,550 --> 00:29:37,550
if we quite a max divided by a zero

491
00:29:37,560 --> 00:29:39,520
a zero now

492
00:29:39,520 --> 00:29:40,540
it's meant to be

493
00:29:40,560 --> 00:29:45,460
the amplitude when omega equals zero then is the shorthand notation this number is not

494
00:29:46,320 --> 00:29:50,480
a little higher is now five point oh three

495
00:29:51,410 --> 00:29:53,710
you can see if q is higher

496
00:29:53,720 --> 00:29:54,860
then of course

497
00:29:54,880 --> 00:29:56,060
these numbers

498
00:29:56,060 --> 00:29:58,120
become even closer than omega

499
00:29:58,810 --> 00:30:01,460
max becomes even closer to omega zero

500
00:30:01,470 --> 00:30:06,320
and then this maximum eight becomes even closer to queue times

501
00:30:06,530 --> 00:30:09,840
i have zero for k

502
00:30:09,890 --> 00:30:11,630
rarely ever will be

503
00:30:11,760 --> 00:30:13,850
the body too much was the fact

504
00:30:13,850 --> 00:30:15,720
of data

505
00:30:15,740 --> 00:30:18,930
and we're going to have a set of policies which is the current version space

506
00:30:18,930 --> 00:30:19,910
is the set of

507
00:30:19,930 --> 00:30:22,160
but is that we might eventually converge to

508
00:30:22,180 --> 00:30:28,910
and we're going to return

509
00:30:28,930 --> 00:30:33,580
the max over the centre meaning passes

510
00:30:33,600 --> 00:30:35,790
and possible labels

511
00:30:35,830 --> 00:30:41,870
of the difference in losses

512
00:30:41,930 --> 00:30:43,930
so this is a generalisation

513
00:30:43,950 --> 00:30:47,270
four essentially arbitrary loss functions as long as

514
00:30:47,290 --> 00:30:49,290
is there and

515
00:30:49,520 --> 00:30:55,700
so these were imagining of these losses are made between zero and one

516
00:30:55,720 --> 00:31:04,310
so this is the view what you care about an example in proportion to

517
00:31:04,310 --> 00:31:11,680
so the the difference in losses which could be achieved amongst the remaining plausible boxes

518
00:31:11,740 --> 00:31:18,330
OK so

519
00:31:18,350 --> 00:31:20,430
the previous safety guarantee

520
00:31:20,450 --> 00:31:22,220
doesn't quite hold for this

521
00:31:23,410 --> 00:31:26,100
because sometimes its value could be zero

522
00:31:26,120 --> 00:31:27,180
so you need to

523
00:31:27,200 --> 00:31:28,120
to prove

524
00:31:28,140 --> 00:31:32,330
this is also say if it's a for exactly the same reason that a squared

525
00:31:32,330 --> 00:31:35,010
is safe

526
00:31:35,020 --> 00:31:37,640
what's past this is not the

527
00:31:37,700 --> 00:31:42,040
best possible predictor it will never be the best possible predictor went on to prove

528
00:31:42,040 --> 00:31:46,680
this is not the best thing or may not the best in good

529
00:31:47,310 --> 00:31:51,100
this now we want to prove some speedups

530
00:31:51,100 --> 00:31:54,870
and if we want to do with arbitrary loss functions it seems like we need

531
00:31:54,870 --> 00:31:57,810
to do to go a little beyond the disagreement coefficient

532
00:31:57,830 --> 00:32:02,680
so we need to define a new quantity

533
00:32:02,700 --> 00:32:04,890
which is the so symmetry

534
00:32:07,080 --> 00:32:08,970
that you are this obvious symmetry

535
00:32:10,740 --> 00:32:13,620
is so we have two predictions

536
00:32:13,620 --> 00:32:16,270
xeons prime

537
00:32:17,240 --> 00:32:19,750
we have different possible label

538
00:32:19,770 --> 00:32:22,100
which which could be the truth

539
00:32:22,120 --> 00:32:24,080
we don't know which which those are

540
00:32:24,100 --> 00:32:25,330
so interested in

541
00:32:25,330 --> 00:32:28,120
what is the maximum ratio between

542
00:32:28,990 --> 00:32:30,100
the losses

543
00:32:31,540 --> 00:32:32,790
the losses

544
00:32:34,310 --> 00:32:35,790
a single label

545
00:32:35,810 --> 00:32:37,270
and the minimum

546
00:32:37,290 --> 00:32:41,200
the difference in losses for single label

547
00:32:41,290 --> 00:32:43,740
this turns out to be

548
00:32:43,990 --> 00:32:49,350
the right easily you analyse the label complexity

549
00:32:49,350 --> 00:32:50,770
OK so

550
00:32:50,790 --> 00:32:56,350
when you think a little bit about what supersymmetry means so for zero one loss

551
00:32:58,790 --> 00:33:01,890
elysian prime agree

552
00:33:01,910 --> 00:33:03,020
in which case

553
00:33:03,620 --> 00:33:05,540
the losses will be the same

554
00:33:05,580 --> 00:33:09,220
in iteration define zero zero is one

555
00:33:09,250 --> 00:33:11,830
or they disagree in which case

556
00:33:12,350 --> 00:33:18,830
this difference is always one in service of the symmetry is one

557
00:33:19,020 --> 00:33:23,520
four hinge loss where you just have hand starting at one

558
00:33:23,520 --> 00:33:25,970
and going up to minus one

559
00:33:25,990 --> 00:33:28,350
this also into our to zero

560
00:33:28,410 --> 00:33:31,080
because you want to be minus one

561
00:33:31,100 --> 00:33:35,310
that's going to be one

562
00:33:35,330 --> 00:33:40,370
for logistic loss turned out to be something of more complex one place you be

563
00:33:41,220 --> 00:33:42,750
inverse for square loss to to be

564
00:33:44,950 --> 00:33:49,870
it may be worthwhile to think about so this is the difference of losses

565
00:33:49,890 --> 00:33:51,560
is the function of

566
00:33:51,580 --> 00:33:56,100
xeon see prime right so this is the thing like a derivative to generalization of

567
00:33:58,870 --> 00:34:03,370
so if you look at logistic loss it look something like this

568
00:34:03,390 --> 00:34:05,540
on a fixed interval

569
00:34:05,560 --> 00:34:06,810
you can

570
00:34:06,830 --> 00:34:07,970
you have some

571
00:34:07,990 --> 00:34:09,970
derivative magnitude here

572
00:34:09,990 --> 00:34:10,950
the sum of

573
00:34:10,950 --> 00:34:13,990
ninety it from this new into a

574
00:34:14,010 --> 00:34:17,560
the ratio of these two start ups to compute

575
00:34:17,580 --> 00:34:19,140
the symmetry

576
00:34:19,140 --> 00:34:27,100
so this also tells you what i squared loss as an infinite so symmetry because

577
00:34:27,100 --> 00:34:29,370
it go to zero

578
00:34:37,520 --> 00:34:40,490
you have to make sure the zero is not the domain and then it would

579
00:34:42,430 --> 00:34:45,560
and i guess

580
00:34:45,600 --> 00:34:49,620
in applications of square loss that i've seen typically one only the zero inside of

581
00:34:53,850 --> 00:35:01,430
OK so we're almost so

582
00:35:01,450 --> 00:35:05,390
we also need to generalize disagreement coefficient just very slightly so this is the definition

583
00:35:05,390 --> 00:35:08,370
of this coalition before

584
00:35:08,370 --> 00:35:12,240
and the claims that you can rewrite is the probability is really an expectation of

585
00:35:12,240 --> 00:35:14,640
an indicator function

586
00:35:14,660 --> 00:35:15,770
and i can

587
00:35:15,770 --> 00:35:17,120
right out

588
00:35:17,140 --> 00:35:19,890
this is there exists h and h prime

589
00:35:19,890 --> 00:35:21,240
in my set

590
00:35:21,270 --> 00:35:24,810
about these which the degree

591
00:35:24,810 --> 00:35:25,700
and now

592
00:35:25,700 --> 00:35:28,490
now becomes there no units attached to it

593
00:35:29,400 --> 00:35:33,980
so so this is dimensionless

594
00:35:33,990 --> 00:35:39,360
all right now how actually do I change the variable in the equation

595
00:35:39,420 --> 00:35:41,220
0 watch this is

596
00:35:41,280 --> 00:35:48,100
utterly trivial idea and utterly important but don't slog around doing it this way trying

597
00:35:48,100 --> 00:35:49,750
to stuff and

598
00:35:49,890 --> 00:35:55,920
lifers 1st instead do the inverse of the words right it instead as t equals

599
00:35:56,040 --> 00:35:57,680
M T 1

600
00:35:59,740 --> 00:36:04,430
the reason being that it's the that's facing you in that equation and the 1481

601
00:36:04,450 --> 00:36:09,740
a substitute for so let's do it the new equation will be what well the

602
00:36:10,720 --> 00:36:17,040
since this is a constant the left hand side becomes DT was

603
00:36:17,050 --> 00:36:18,070
times M

604
00:36:20,510 --> 00:36:22,250
equals k

605
00:36:22,360 --> 00:36:24,810
times 10 to the 4

606
00:36:24,840 --> 00:36:28,810
minus end to the 4th T 1 to the 4th so I'm going to factor

607
00:36:28,810 --> 00:36:33,480
out that the 4th and make it 1 minus T 1 to the 4th of

608
00:36:35,580 --> 00:36:40,020
now I can divide through by and get rid of 1 of those and so

609
00:36:40,030 --> 00:36:43,320
the new equation now is

610
00:36:43,340 --> 00:36:47,310
the T 1 take the time of is equal to

611
00:36:47,430 --> 00:36:52,530
now I have K cubed out front here I'm going to just give that a

612
00:36:52,530 --> 00:36:55,270
new name k 1

613
00:36:55,290 --> 00:36:58,320
but but but but but the

614
00:37:00,120 --> 00:37:05,200
essentially is the same equation is no harder to solve normal

615
00:37:05,630 --> 00:37:10,700
and no easier to solve than the original 1

616
00:37:10,790 --> 00:37:16,490
but it's been simplify study for what looks better it looks better lets us all

617
00:37:16,670 --> 00:37:21,070
right to compare the 2 but this 1 up and green and this 1 agreeing

618
00:37:21,100 --> 00:37:25,990
to just images the same with indicate that has the same equations

619
00:37:27,600 --> 00:37:34,200
notice so t 1 has been rendered is now dimensionless so I don't have to

620
00:37:34,230 --> 00:37:38,380
even ask when I solve this equation of please tell me what the units of

621
00:37:38,380 --> 00:37:44,960
temperature are what kind of LU measuring temperature it makes no difference to this equation

622
00:37:45,200 --> 00:37:52,270
k 1 still has units what units doesn't have a it's it's been simplified because

623
00:37:52,270 --> 00:37:54,750
it now has the units of blood

624
00:37:54,770 --> 00:38:00,100
of since this is dimensionless and this is dimensionless the units of inverse lines so

625
00:38:00,100 --> 00:38:06,990
K. 1 words that units involving both degrees and seconds before but now it's inverse

626
00:38:11,100 --> 00:38:12,900
as its units

627
00:38:13,080 --> 00:38:20,980
and were like this 1 less constant so that 1 less constant in the equation

628
00:38:20,990 --> 00:38:23,760
it just looks better

629
00:38:24,500 --> 00:38:26,560
short of

630
00:38:27,120 --> 00:38:29,820
this business so I think you know

631
00:38:29,840 --> 00:38:34,750
that k 1 the process of forming k 1 out of k 1

632
00:38:34,770 --> 00:38:37,130
out of a K and Q

633
00:38:37,240 --> 00:38:43,250
it is called lumping constant so I think that use the standard terminology in physics

634
00:38:43,250 --> 00:38:45,390
and engineering courses

635
00:38:47,100 --> 00:38:52,670
and triangular constants together like this and then you make and you want them there

636
00:38:52,690 --> 00:38:55,860
they are a lot for you and then you just wanted do

637
00:38:55,960 --> 00:39:02,720
so as an example scaling of watch out for when you can use it for

638
00:39:02,720 --> 00:39:07,740
example they would have probably been a good thing to use on the 1st problem

639
00:39:07,740 --> 00:39:14,100
set when you're handling this problem of drug elimination and hormone elimination production side of

640
00:39:14,200 --> 00:39:18,270
things but you could lump constants and as was done to some extent on the

641
00:39:18,270 --> 00:39:20,070
solutions to work

642
00:39:20,170 --> 00:39:24,940
get needle looking answer 1 without somebody constants

643
00:39:25,600 --> 00:39:30,890
OK now let's now go to serious stuff where we're actually going to make changes

644
00:39:30,890 --> 00:39:33,740
of variables which we hope will render

645
00:39:33,770 --> 00:39:42,290
unsolvable equations suddenly solvable now I'm going to make the thereby making substitutions but

646
00:39:42,320 --> 00:39:51,480
it's quite a quite important to watch out there 2 kinds of substitutions direct substitutions

647
00:39:51,750 --> 00:39:56,460
but that's where you introduce a new variable I don't know how to write this

648
00:39:56,460 --> 00:40:01,370
on the board of just scheme so the new it's it's 1 which says that

649
00:40:01,370 --> 00:40:03,290
the new variable

650
00:40:03,320 --> 00:40:07,670
is equal to some combination of the old variables

651
00:40:12,010 --> 00:40:18,830
the other kind of substitution is inverse is just the reverse here you say that

652
00:40:18,830 --> 00:40:20,700
the old variables

653
00:40:20,770 --> 00:40:26,080
or some combination of the new of all the you'll have to

654
00:40:26,100 --> 00:40:31,740
stake in the fuel variables to with the basic it's what's appears on the left-hand

655
00:40:31,740 --> 00:40:33,290
side of

656
00:40:33,360 --> 00:40:37,100
it is it a new variable that appears on the left-hand side by itself or

657
00:40:37,100 --> 00:40:38,820
is it the old variable that appears

658
00:40:39,460 --> 00:40:43,940
on the left hand side now right here we have an example

659
00:40:45,080 --> 00:40:50,530
if I did it as a direct substitution I would've written t 1 equals to

660
00:40:50,740 --> 00:40:54,940
that's the way I defined the new variable which of course you have to do

661
00:40:54,940 --> 00:41:00,150
if you're introducing it but what I actually did this substitution I did the inverse

662
00:41:00,150 --> 00:41:07,860
substitution namely I used t equals t 1 of m times t 1

663
00:41:08,200 --> 00:41:13,390
and the reason for doing that was because it was the capital T. is that

664
00:41:13,390 --> 00:41:20,320
zero as a function of the original parameters and are vector of lagrange multipliers

665
00:41:21,680 --> 00:41:22,760
and then

666
00:41:22,760 --> 00:41:25,730
we take derivatives of this thing

667
00:41:25,740 --> 00:41:28,370
with respect to theta kl

668
00:41:28,390 --> 00:41:32,870
a particular element here and if we take the derivative what we get

669
00:41:32,890 --> 00:41:42,080
so we get n k l

670
00:41:42,080 --> 00:41:44,780
divided by

671
00:41:44,830 --> 00:41:47,700
OK now

672
00:41:53,520 --> 00:41:54,810
all right

673
00:41:54,860 --> 00:41:58,550
and now if we set this equal zero

674
00:41:58,560 --> 00:42:02,020
and so theta kl

675
00:42:02,100 --> 00:42:04,710
we get

676
00:42:09,510 --> 00:42:12,100
that i get this wrong

677
00:42:12,110 --> 00:42:15,620
for all k start

678
00:42:21,750 --> 00:42:27,850
solve this by setting it equal to zero we get that data

679
00:42:27,870 --> 00:42:29,130
OK kl

680
00:42:29,150 --> 00:42:30,160
it is

681
00:42:30,180 --> 00:42:36,420
we have one minus lambda k times the theta KL and then we divide by

682
00:42:36,430 --> 00:42:38,800
OK so we get

683
00:42:41,820 --> 00:42:48,310
so essentially is proportional to

684
00:42:48,350 --> 00:42:50,090
and k l

685
00:42:50,090 --> 00:42:51,350
with some

686
00:42:51,370 --> 00:42:56,370
the grande multiplier here what we want is probably cale's two

687
00:42:56,380 --> 00:42:59,090
some to one when we some two l

688
00:42:59,100 --> 00:43:02,450
because that's the constraint that we get when we take derivatives with respect to lambda

689
00:43:03,370 --> 00:43:07,150
so this becomes

690
00:43:07,160 --> 00:43:11,130
the maximum likelihood estimate is in a ball

691
00:43:11,140 --> 00:43:13,270
over some

692
00:43:14,460 --> 00:43:17,430
time of k

693
00:43:17,440 --> 00:43:19,750
l products

694
00:43:20,600 --> 00:43:23,150
a really happy with that

695
00:43:23,830 --> 00:43:27,730
i think this is sort of a gory detail and i apologize for people who

696
00:43:27,730 --> 00:43:32,960
think it's really trivial just because i want to demystify there there's a very mechanical

697
00:43:32,960 --> 00:43:34,570
way of doing this

698
00:43:34,620 --> 00:43:36,300
we have a model

699
00:43:36,310 --> 00:43:41,000
we write down the likelihood function we may have to consider constraints in the likelihood

700
00:43:41,000 --> 00:43:46,720
function and then we take derivatives and we try to solve and sometimes in these

701
00:43:46,720 --> 00:43:49,350
simple cases we can analytically solved for

702
00:43:49,370 --> 00:43:50,860
the parameter values

703
00:43:50,870 --> 00:43:55,300
and the beauty of this is that the answer we get is really really simple

704
00:43:56,940 --> 00:43:58,430
an obvious

705
00:44:00,710 --> 00:44:05,280
so if we want to learn the maximum likelihood parameters for data to the parameters

706
00:44:05,280 --> 00:44:06,100
are really

707
00:44:06,140 --> 00:44:08,920
x two to the parent x one

708
00:44:08,930 --> 00:44:13,680
then we take the counts that we observe four different configurations of x one x

709
00:44:14,600 --> 00:44:20,360
and we take those counts really renormalize the rows sum to one

710
00:44:21,530 --> 00:44:24,330
and that's what we get here in this tiny corner here

711
00:44:24,350 --> 00:44:26,360
now do you guys think this is reasonable

712
00:44:26,370 --> 00:44:27,860
is this a good

713
00:44:28,030 --> 00:44:31,030
a way of estimating things

714
00:44:38,300 --> 00:44:40,070
well that's that's a good

715
00:44:40,100 --> 00:44:47,110
observation so for this particular configuration of the parent and child we didn't observe

716
00:44:47,160 --> 00:44:49,790
that's occurring in our data set of

717
00:44:49,790 --> 00:44:55,020
fifteen data points we have fifteen any big n equals fifteen data points to generate

718
00:44:55,840 --> 00:44:58,600
matrix of frequencies and so on

719
00:44:58,620 --> 00:45:03,090
our maximum likelihood estimate was zero for that element

720
00:45:03,100 --> 00:45:07,330
and that seems uncomfortable if we think of the model is the way of predicting

721
00:45:07,330 --> 00:45:08,680
things about

722
00:45:08,740 --> 00:45:14,700
feature data because essentially what that saying is just because i didn't observe this particular

723
00:45:14,700 --> 00:45:18,780
configuration i think it's impossible in all future data

724
00:45:22,020 --> 00:45:29,290
so that was it for maximum likelihood estimation discrete graphs with

725
00:45:29,380 --> 00:45:31,810
fully observed data

726
00:45:31,810 --> 00:45:38,100
now let's move on to slightly more interesting set right questions the press conferences

727
00:45:42,090 --> 00:45:44,100
is quite reasonable

728
00:45:44,130 --> 00:45:46,480
you could say

729
00:45:47,390 --> 00:45:52,330
but but i mean the the question was philosophically you can say anything exists if

730
00:45:52,330 --> 00:45:55,480
you never observed it but if you take that point

731
00:45:55,520 --> 00:46:00,580
then actually can do any machine learning because if you have the data set

732
00:46:00,700 --> 00:46:05,850
then you've never observed in n plus first data point so you could say that

733
00:46:05,850 --> 00:46:09,530
could possibly exist and so the only thing that you could predict is the stuff

734
00:46:09,530 --> 00:46:11,050
that you have already observed

735
00:46:11,060 --> 00:46:13,380
so i couldn't for example predict tomorrow

736
00:46:13,410 --> 00:46:16,810
the sun will rise because i haven't observed that you

737
00:46:17,700 --> 00:46:20,260
so i think philosophically philosophically

738
00:46:20,260 --> 00:46:24,830
you may be able to argue that but your life would be very difficult

739
00:46:24,850 --> 00:46:28,860
he would agonize lot which is related to the philosophers to do that

740
00:46:31,310 --> 00:46:37,760
now let's consider a slightly more interesting problem let's imagine we have a graphical model

741
00:46:38,980 --> 00:46:39,910
but now

742
00:46:39,950 --> 00:46:45,840
only some of our variables are observed in the rest of the variables are hidden

743
00:46:45,850 --> 00:46:48,780
now will be use interchangeably

744
00:46:48,780 --> 00:46:50,950
the terms hidin

745
00:46:50,980 --> 00:46:55,230
and missing in latent people often use these

746
00:46:55,240 --> 00:46:59,630
the terms interchangeably so let's just call these axes here

747
00:46:59,650 --> 00:47:03,700
and the y observed and now i'm going to ask us to

748
00:47:04,790 --> 00:47:06,840
the parameters of this model

749
00:47:06,850 --> 00:47:09,650
just from observations of y

750
00:47:09,670 --> 00:47:13,600
you might think well that's a little fishy right you want me to learn the

751
00:47:13,600 --> 00:47:16,540
relationship between x one and x three

752
00:47:16,540 --> 00:47:21,190
given by this parameter theta three when the only thing i observed is why

753
00:47:21,210 --> 00:47:24,000
i mean i'm going to say yes but try to do

754
00:47:24,040 --> 00:47:30,260
where you know where machine learning researchers and statisticians we can boldly go into problems

755
00:47:30,260 --> 00:47:32,840
that other people might think of fish

756
00:47:36,670 --> 00:47:40,240
let's apply the same principle we did before

757
00:47:40,250 --> 00:47:47,590
which is to maximize the parameter log likelihood given the observed data that's perfectly reasonable

758
00:47:47,600 --> 00:47:49,210
o thing to start out with

759
00:47:49,230 --> 00:47:54,620
so we can write down our log likelihood is a function of our parameters

760
00:47:54,640 --> 00:47:59,000
and this is the probability of the observed data given the parameters

761
00:47:59,000 --> 00:48:00,860
now the only thing

762
00:48:00,880 --> 00:48:06,240
weird about this function is that to compute the probability of the observed data given

763
00:48:06,240 --> 00:48:07,590
the parameters

764
00:48:08,780 --> 00:48:13,960
it's hard to do that directly from this because we don't know the values of

765
00:48:13,960 --> 00:48:16,290
all of these hidden variables

766
00:48:17,100 --> 00:48:19,350
our graphical model gives us

767
00:48:19,360 --> 00:48:23,240
a joint distribution over all of these x and y variables

768
00:48:23,240 --> 00:48:25,460
so if we just put it

769
00:48:25,530 --> 00:48:31,400
different possible values over the next variables and we some of those out

770
00:48:31,420 --> 00:48:36,530
we get by the basic marginalization rule of probability the probability of the observed data

771
00:48:36,530 --> 00:48:41,610
given the parameters is the sum over all possible settings of the hidden variables of

772
00:48:41,610 --> 00:48:45,840
the joint probability of the observed hidden variables given the parameters

773
00:48:45,850 --> 00:48:49,500
so now in the same kind of ballpark as we were before

774
00:48:49,510 --> 00:48:53,500
we have a lot like function to try to maximize it

775
00:48:53,510 --> 00:48:57,020
and here's a nice trick to pull up our sleeves

776
00:48:57,040 --> 00:48:59,510
and that's the algorithm

777
00:49:00,570 --> 00:49:04,550
we as well as grande multipliers and i'm going to ask about the EM algorithm

778
00:49:04,560 --> 00:49:09,100
raise your hand if you know what the algorithm is

779
00:49:09,160 --> 00:49:12,660
raise your hand if you don't know what the EM is

780
00:49:12,700 --> 00:49:15,100
OK a small number of people

781
00:49:15,260 --> 00:49:16,870
which is reasonable given the

782
00:49:16,890 --> 00:49:20,900
we have a very diverse set of backgrounds people here

783
00:49:22,020 --> 00:49:25,700
here's what we're gonna do we're going to maximize the parameter log likelihood given the

784
00:49:25,700 --> 00:49:31,910
observed variables and the EM algorithm is in a very intuitive sort of

785
00:49:31,910 --> 00:49:42,450
now I set ahomework question yesterday andthe answer was is the same yes it's amazing yes

786
00:49:42,450 --> 00:49:50,590
isn't it so just arecapyou you draw a you choose an unknowrandom you you

787
00:49:50,590 --> 00:49:55,970
you draw ball from that random earnings it'sblue andyou really want a red ball and so

788
00:49:55,980 --> 00:50:02,010
you have asecond draw and youhave a choice of sticking with the sameearn or switching

789
00:50:02,140 --> 00:50:08,750
sticking or switching and most of you preparedpreferdswitching Ithink what you're seeing

790
00:50:08,750 --> 00:50:15,870
is something like this you're you're you'rethinking wellif itif it was a blue

791
00:50:15,870 --> 00:50:19,250
the first time is much more likely to be this earnwasdrawing from

792
00:50:19,270 --> 00:50:24,330
and there aren'tmanyred balls in thereso I better switchbutactuallywhen you do the numbers

793
00:50:24,330 --> 00:50:30,610
it is exactly equaleleven totwelvetheodds are against youboth ways by exactly

794
00:50:30,610 --> 00:50:34,390
the same amountI think it's quite curiousthis I chose this example completely at

795
00:50:34,390 --> 00:50:39,750
random whatever that means I and Ithought I've made amistake when I found that

796
00:50:39,750 --> 00:50:43,370
and it's notit' not generallytrueabsolutely not generally true itjust happens to

797
00:50:43,370 --> 00:50:49,950
be true of this particular numbers of balls and and try to understand why intuitively that's

798
00:50:49,950 --> 00:50:54,940
notthe right answer I think it has something to do with the fact that intuitively we we are

799
00:50:54,940 --> 00:51:00,670
too certain about our initial inference it is more likely to be the the the

800
00:51:00,670 --> 00:51:06,310
right end you've drawn frombut only by a factor of abouttwo to one but

801
00:51:06,310 --> 00:51:11,930
I think mentally we kind of downweight the possibility that it couldbe the other earn

802
00:51:11,950 --> 00:51:15,590
when youwhen thethe laws of probability get at rightthey properly propagate

803
00:51:15,590 --> 00:51:22,830
that uncertaintythrough into the season stage andit really doesn't matter whether youstick or switch

804
00:51:22,830 --> 00:51:30,330
really you're stuffed anywayokay today we there's gonna beslightly higher

805
00:51:30,330 --> 00:51:35,850
content to concept ratio andI'm gonnabe talking about some practical issuesa little bit

806
00:51:35,850 --> 00:51:42,130
about bayesian computation you've had a verydetailed look atsampling methods

807
00:51:42,130 --> 00:51:45,250
from from from I know I'm not going to say very much in this

808
00:51:45,250 --> 00:51:52,590
sectionI'm gonna say quitea bit about questioning models sothis is model-based inference

809
00:51:52,590 --> 00:51:56,830
wherethe models come from we talked aboutthat a bitlast timebut you know you're

810
00:51:56,830 --> 00:52:02,390
never quite sure andwhat can wethisisgonna be a

811
00:52:02,390 --> 00:52:10,150
paintheyou can can you use data to criticize

812
00:52:10,150 --> 00:52:14,490
your model and to choose choose between modelsthat's themain point of today's talk and

813
00:52:14,490 --> 00:52:19,630
then there's a a few small items at the end and then after coffee I'm

814
00:52:19,630 --> 00:52:24,230
going to talk about some recent applicationsI've been involved with and trying to go

815
00:52:24,250 --> 00:52:30,310
through themand picking out the lessonsyeah the lessons learned in thefirst two

816
00:52:30,310 --> 00:52:37,350
tutorials see where they where where those ideas come into the to to themodeling

817
00:52:37,350 --> 00:52:45,860
and analysis that I've doneokaythe key point about bayesian computation is

818
00:52:45,860 --> 00:52:52,130
whichever whatever methodyou used to dois that we're talking about integration and not optimization so that's

819
00:52:52,140 --> 00:52:57,510
quite a bit different from any other paradigms this is rather weaktag line

820
00:52:57,510 --> 00:53:02,370
not due to mewe are integratingand being greatrather than optimizing and

821
00:53:02,370 --> 00:53:08,870
being wise and the fact that there is an integral therewell it'sintegrals therein almost

822
00:53:08,870 --> 00:53:16,690
everything we do with withwith probability models essentially because you marginalizing out variables

823
00:53:16,690 --> 00:53:23,190
and not the not concern at a particular time so there's an integral in thedenominator

824
00:53:23,190 --> 00:53:27,910
in the posterior probability andwhen youdo prediction this integrals top and bottom if you

825
00:53:27,910 --> 00:53:32,170
want to estimatedthe expectation of a function in this and another is an

826
00:53:32,170 --> 00:53:36,450
integral there as well so we'realways we're always integrating and that's not just a sort

827
00:53:36,450 --> 00:53:44,810
of numerical computation point is because bayesian inference is essentially about integratingout uncertainty you know

828
00:53:44,810 --> 00:53:50,750
I've used the word integratenot as it were sequentially making the best decision at

829
00:53:50,750 --> 00:53:54,870
each stage and is quite crucial to the Bayesian way of thinking that that's that's the

830
00:53:54,870 --> 00:54:00,750
way it works sonumerical integration very wellunderstood subject as a huge

831
00:54:00,750 --> 00:54:05,850
literaturebut but it seems thatquite a lot of the mainstream

832
00:54:05,850 --> 00:54:13,180
literatureand numerical integration isn't terribly relevant and there's several reasons for thatfirst of all

833
00:54:13,180 --> 00:54:20,610
we're almost always concerned with pretty high-dimensional problems and classical methods of numerical integration

834
00:54:20,610 --> 00:54:23,440
as an example but i'll tell you exactly how we do it

835
00:54:23,460 --> 00:54:29,880
and for discovering powerful in distinguishing features between datasets like example i give you

836
00:54:29,960 --> 00:54:35,880
like the other patterns composed of conjunctive combination of elements basically we're saying this feature

837
00:54:35,880 --> 00:54:38,730
values this feature value and this feature value

838
00:54:39,090 --> 00:54:44,060
can also capture patterns support change over time you can do that so for example

839
00:54:44,080 --> 00:54:48,090
you know it's a very first example i gave you

840
00:54:48,150 --> 00:54:53,880
papers published in this period were papers published in the year you can you can

841
00:54:53,880 --> 00:54:57,000
tell you know i mean are they you know you have to do some kind

842
00:54:57,000 --> 00:54:57,880
of feature

843
00:54:57,920 --> 00:55:02,340
mapping for these things and then you can ask questions what's happening

844
00:55:03,750 --> 00:55:06,980
so as i said the you know the cancer cells

845
00:55:07,000 --> 00:55:10,690
was the was the front cover the people appear

846
00:55:10,690 --> 00:55:14,900
the patterns have been applied in medical applications for diagnosis of

847
00:55:14,900 --> 00:55:20,580
acute lymphoblastic expressive so now let's look at the the space of these so

848
00:55:20,630 --> 00:55:22,980
so one of the ways we can look at is that

849
00:55:23,040 --> 00:55:27,880
think of this where there the the two classes two datasets we have two types

850
00:55:27,880 --> 00:55:29,150
d one and d two

851
00:55:29,150 --> 00:55:32,340
it's a cancer this is class one and class two

852
00:55:32,400 --> 00:55:35,840
then we can have all these patterns so suppose let's say

853
00:55:35,840 --> 00:55:39,090
there's an exponential combination of patterns so

854
00:55:39,290 --> 00:55:41,540
so obviously you choose one pattern

855
00:55:41,590 --> 00:55:42,840
and then you find

856
00:55:42,880 --> 00:55:46,150
how many instances have this pattern

857
00:55:46,190 --> 00:55:48,080
OK so that gives you

858
00:55:49,150 --> 00:55:51,900
martin value so let's say

859
00:55:51,900 --> 00:55:54,540
in the that's to you chose the next

860
00:55:54,540 --> 00:55:55,650
let's assume

861
00:55:55,670 --> 00:55:56,820
the pattern x here

862
00:55:56,840 --> 00:55:57,670
that is the

863
00:55:57,730 --> 00:56:00,290
the support at this point and then

864
00:56:00,290 --> 00:56:02,690
and then you look at the data set two

865
00:56:02,710 --> 00:56:04,020
and then it gives you

866
00:56:04,020 --> 00:56:07,290
what is the support for that which could be zero or

867
00:56:07,340 --> 00:56:11,270
four hundred percent so that gives you a point so basically what you can do

868
00:56:11,270 --> 00:56:15,900
is you can learn this metrics or this box

869
00:56:15,920 --> 00:56:17,290
with all these factors

870
00:56:17,340 --> 00:56:22,440
OK so remember this an order of the pattern of each pattern has two values

871
00:56:22,480 --> 00:56:23,980
it's supporting

872
00:56:24,020 --> 00:56:27,020
in one it supported the two so now

873
00:56:27,040 --> 00:56:29,170
each point has two values and you

874
00:56:29,190 --> 00:56:30,960
you just drop those points

875
00:56:31,090 --> 00:56:33,150
into this into the space

876
00:56:33,210 --> 00:56:37,940
now the interesting part is that the lower region is the reason that we are

877
00:56:37,940 --> 00:56:41,560
really interested we want to find all those patterns that

878
00:56:41,560 --> 00:56:42,610
following to this

879
00:56:42,610 --> 00:56:47,360
in this and similar you can grow in this side as well but just showing

880
00:56:48,150 --> 00:56:51,690
the important patterns in data or this celebration

881
00:56:51,710 --> 00:56:53,230
so what it says is that

882
00:56:53,230 --> 00:56:55,230
if it has the probability

883
00:56:55,270 --> 00:56:59,480
all support less than a i'm not interested in could be noisy

884
00:56:59,540 --> 00:57:02,840
so that means you want some minimality that you're interested

885
00:57:02,920 --> 00:57:06,750
and then what you want is that the problem you know the support value has

886
00:57:06,750 --> 00:57:11,480
to be much greater than forty one otherwise if they just equal suppose at this

887
00:57:11,480 --> 00:57:14,400
point that means all these patterns are

888
00:57:14,400 --> 00:57:18,690
equally probable or have exactly the same support in all of those things that means

889
00:57:18,920 --> 00:57:22,110
any any pattern that sits in the

890
00:57:22,250 --> 00:57:26,210
in this forty five degree line because we're using same scale

891
00:57:26,290 --> 00:57:30,560
there are interesting because they want to tell us anything because there

892
00:57:30,580 --> 00:57:35,980
equally likely on both datasets so therefore we really interested in mining these so the

893
00:57:35,980 --> 00:57:40,060
whole research goes is how do i find the patterns

894
00:57:40,090 --> 00:57:41,860
easily from here

895
00:57:41,880 --> 00:57:43,480
so all i'm given is

896
00:57:43,540 --> 00:57:45,690
this slope

897
00:57:45,690 --> 00:57:47,170
and given

898
00:57:48,560 --> 00:57:51,170
short run these are the two values are given

899
00:57:51,190 --> 00:57:52,920
and i have to find

900
00:57:52,960 --> 00:57:56,590
the question is some people would ask how do i know to choose these things

901
00:57:56,730 --> 00:57:57,880
you know in any

902
00:57:57,940 --> 00:58:02,380
machine learning you know there is no absolute answers you know like if if anybody

903
00:58:02,380 --> 00:58:03,730
thinks that

904
00:58:03,750 --> 00:58:07,420
machine learning going to solve all the problems wrong

905
00:58:07,440 --> 00:58:09,460
because you know the way

906
00:58:09,480 --> 00:58:12,770
i would say you know you've given n points

907
00:58:12,840 --> 00:58:14,650
i can find the perfect function

908
00:58:14,670 --> 00:58:16,190
that goes to the end

909
00:58:16,190 --> 00:58:17,250
the moment

910
00:58:17,380 --> 00:58:21,460
so i have infinite functions i can give you that you've got to these

911
00:58:22,230 --> 00:58:23,540
and points

912
00:58:23,590 --> 00:58:25,540
i give you an plus one

913
00:58:25,540 --> 00:58:28,420
infinity of then

914
00:58:28,480 --> 00:58:30,630
and i go to the n plus one point

915
00:58:30,670 --> 00:58:34,250
but i'm still left with infinity because that's what i start right

916
00:58:34,250 --> 00:58:36,250
therefore as you can see

917
00:58:36,270 --> 00:58:38,590
you will never be able to find

918
00:58:38,610 --> 00:58:41,170
perfect function in that sense

919
00:58:41,170 --> 00:58:43,900
you know we will never know the real more

920
00:58:43,960 --> 00:58:45,630
you know like in physics

921
00:58:45,630 --> 00:58:49,730
let's get going to a and

922
00:58:49,740 --> 00:58:52,740
what we're talking about last time

923
00:58:52,840 --> 00:58:55,490
is the discovery the piano

924
00:58:55,540 --> 00:58:59,670
it was not the most basic constituents of matter

925
00:58:59,760 --> 00:59:02,130
so there was the particle

926
00:59:02,180 --> 00:59:08,020
that was even less massive and that is the electron

927
00:59:08,040 --> 00:59:11,040
but today we're going to discover the nucleus

928
00:59:11,060 --> 00:59:12,850
and so

929
00:59:12,870 --> 00:59:15,880
says that nineteen eleven

930
00:59:15,930 --> 00:59:18,440
this is our brother

931
00:59:20,900 --> 00:59:22,500
in england

932
00:59:22,550 --> 00:59:26,770
and what he was interested in doing

933
00:59:26,820 --> 00:59:29,700
i was studying the emissions

934
00:59:29,710 --> 00:59:36,850
the emission from these newly radioactive elements that were being discovered

935
00:59:36,900 --> 00:59:41,760
so the emission from radium bromide radium for example

936
00:59:41,800 --> 00:59:44,840
and so on

937
00:59:44,860 --> 00:59:47,380
what he did was he got a sample

938
00:59:48,330 --> 00:59:50,210
radium bromide

939
00:59:50,230 --> 00:59:54,080
from his friend marie curie

940
00:59:54,100 --> 00:59:56,040
and what was known

941
00:59:56,060 --> 00:59:59,040
is that this radium bromide

942
00:59:59,050 --> 01:00:04,660
he needed something called alpha particles

943
01:00:04,710 --> 01:00:09,350
and the exact nature of the the alpha particles was not known

944
01:00:09,370 --> 01:00:11,510
however what was known

945
01:00:11,560 --> 01:00:15,110
was that the world heavy particles

946
01:00:15,130 --> 01:00:17,570
that they were charged

947
01:00:17,580 --> 01:00:18,860
and they

948
01:00:18,920 --> 01:00:21,030
they energy

949
01:00:21,050 --> 01:00:24,800
course today we know what these alpha particles

950
01:00:24,820 --> 01:00:30,210
feeling of loss of helium with two electrons removed

951
01:00:30,700 --> 01:00:33,570
all right so he got his radium bromide

952
01:00:35,120 --> 01:00:39,150
alpha particles are being emitted from radium bromide

953
01:00:39,170 --> 01:00:43,970
and he had some kind of detector way out here

954
01:00:43,990 --> 01:00:48,200
which detected these alpha particles

955
01:00:48,260 --> 01:00:52,440
what found is that at the detector

956
01:00:52,450 --> 01:00:53,390
there were

957
01:00:53,570 --> 01:00:56,160
one hundred thirty two thousand

958
01:00:56,210 --> 01:00:58,050
alpha particles

959
01:00:58,070 --> 01:01:00,350
that was the

960
01:01:00,370 --> 01:01:04,260
that he was able to measure

961
01:01:04,270 --> 01:01:07,800
and then because he was trying to figure out what is the nature of these

962
01:01:08,900 --> 01:01:10,560
he did this experiment

963
01:01:10,610 --> 01:01:15,070
he took a gold foil

964
01:01:16,890 --> 01:01:18,860
gold foil

965
01:01:18,900 --> 01:01:20,920
i really meaning

966
01:01:20,930 --> 01:01:23,930
two times ten to the minus five inches

967
01:01:23,940 --> 01:01:28,990
that towards the magnet to better than the diameter of here

968
01:01:29,010 --> 01:01:32,220
i've always wondered how we handle there

969
01:01:32,240 --> 01:01:34,250
that's pretty then

970
01:01:35,500 --> 01:01:38,220
what that

971
01:01:38,240 --> 01:01:39,900
and the whole

972
01:01:39,910 --> 01:01:42,120
but how great he said

973
01:01:42,140 --> 01:01:45,640
once this gold foil was placed

974
01:01:45,650 --> 01:01:47,360
can operate was still

975
01:01:47,790 --> 01:01:53,260
one hundred thirty two thousand alpha particles that

976
01:01:53,270 --> 01:01:55,000
it appeared that

977
01:01:56,170 --> 01:02:01,800
the alpha particles that were being emitted just went right through the foil

978
01:02:01,810 --> 01:02:03,980
right to the detector even

979
01:02:04,010 --> 01:02:10,180
seem like those alpha particles the world the same company

980
01:02:11,370 --> 01:02:15,200
doesn't sound like a very interesting experiment

981
01:02:15,210 --> 01:02:18,660
but at that time he was working with person name guy

982
01:02:18,800 --> 01:02:24,560
post guide the same geiger is the geiger counter

983
01:02:24,590 --> 01:02:30,590
and geiger wasn't too happy about the results of these experiments and in addition

984
01:02:30,600 --> 01:02:32,300
tiger here and this

985
01:02:32,340 --> 01:02:36,620
undergraduate student hanging around the land

986
01:02:36,670 --> 01:02:38,580
his undergraduate name

987
01:02:40,500 --> 01:02:46,570
morrison was really excited about doing science he just hung around there's really want to

988
01:02:46,570 --> 01:02:48,370
do some

989
01:02:48,380 --> 01:02:49,620
and geiger

990
01:02:49,630 --> 01:02:51,610
these one my going with this guy

991
01:02:51,630 --> 01:02:56,420
geiger goes rather further says i got marks and here he really wants to do

992
01:02:56,420 --> 01:02:59,060
something which with

993
01:02:59,080 --> 01:03:00,690
and rather for

994
01:03:03,350 --> 01:03:06,640
what we have here and build a detector

995
01:03:06,690 --> 01:03:08,740
can swing around

996
01:03:08,760 --> 01:03:10,860
goes here

997
01:03:10,910 --> 01:03:17,410
what happened to all the detector that will rotate around such that the detector can

998
01:03:17,410 --> 01:03:18,870
be positioned

999
01:03:18,890 --> 01:03:19,880
in this way

1000
01:03:19,890 --> 01:03:23,620
so that we can look

1001
01:03:23,630 --> 01:03:24,770
for any

1002
01:03:24,780 --> 01:03:27,280
alpha particles might be

1003
01:03:29,810 --> 01:03:31,420
cannot meaning

1004
01:03:32,990 --> 01:03:36,650
in the direction from which they came

1005
01:03:37,900 --> 01:03:43,110
geiger said i thought he was going to be out of line here

1006
01:03:43,130 --> 01:03:50,300
problems are at that maximize the site to build the detector the detector

1007
01:03:50,310 --> 01:03:55,930
and the geiger OK right let's let's try this

1008
01:03:57,260 --> 01:03:58,600
and so on

1009
01:03:58,620 --> 01:04:02,960
put the radium bromide there in this arrangement

1010
01:04:03,070 --> 01:04:05,650
there is there at the detector

1011
01:04:05,670 --> 01:04:07,280
and a year

1012
01:04:10,600 --> 01:04:12,230
it takes

1013
01:04:16,060 --> 01:04:17,600
the detectors

1014
01:04:17,640 --> 01:04:21,440
and they go well there must be a general background

1015
01:04:21,460 --> 01:04:23,700
so what they do that thing

1016
01:04:24,130 --> 01:04:25,710
while away

1017
01:04:25,790 --> 01:04:30,000
so they are presumably all the particles are going in that direction

1018
01:04:30,020 --> 01:04:31,020
take it away

1019
01:04:31,190 --> 01:04:34,820
and they listen and they here

1020
01:04:36,160 --> 01:04:38,910
and they put the ball way back

1021
01:04:38,960 --> 01:04:41,400
and they're less than a year the

1022
01:04:46,880 --> 01:04:50,030
alpha particles per minute

1023
01:04:51,820 --> 01:04:52,540
and they a

1024
01:04:54,290 --> 01:04:55,540
another way

1025
01:04:55,560 --> 01:04:57,750
what we're

1026
01:04:57,800 --> 01:05:01,040
same is basically the same result

1027
01:05:01,050 --> 01:05:05,340
they go up for others for get down in the lavatory

1028
01:05:05,360 --> 01:05:08,680
rather for example over their shoulder

1029
01:05:08,730 --> 01:05:09,750
they like

1030
01:05:09,800 --> 01:05:11,630
but they

1031
01:05:11,770 --> 01:05:15,230
there are some alpha particles coming on

1032
01:05:15,250 --> 01:05:16,790
not many

1033
01:05:16,800 --> 01:05:20,170
right look at what the probability theories

1034
01:05:20,260 --> 01:05:22,160
the best

1035
01:05:24,300 --> 01:05:27,460
we can calculate the probability

1036
01:05:27,500 --> 01:05:32,880
is simply the number of that nanoparticles twenty

1037
01:05:33,180 --> 01:05:37,070
the number to calibrate the particles twenty

1038
01:05:37,100 --> 01:05:39,140
over-the-counter trades

1039
01:05:39,230 --> 01:05:40,860
the infinite number

1040
01:05:40,870 --> 01:05:46,680
articles one hundred and thirty two thousand eight the probability is too

1041
01:05:46,690 --> 01:05:49,100
times ten to the minus four

1042
01:05:49,100 --> 01:05:50,770
i will describe this

1043
01:05:50,790 --> 01:05:55,000
right next door going to

1044
01:05:57,330 --> 01:06:01,140
the mathematics of this so the idea given the solution one

1045
01:06:01,170 --> 01:06:03,920
and you want to compute the next one

1046
01:06:03,960 --> 01:06:06,210
how can i reuse

1047
01:06:06,290 --> 01:06:09,710
the one of the first things so

1048
01:06:09,730 --> 01:06:13,520
recall as i said use all these things by

1049
01:06:13,560 --> 01:06:16,080
setting up a graph

1050
01:06:16,120 --> 01:06:16,920
and then

1051
01:06:19,100 --> 01:06:22,980
algorithm to work out what the maximum flow through this graph is that gives you

1052
01:06:26,040 --> 01:06:27,500
it gives you

1053
01:06:29,560 --> 01:06:33,640
the solution now one of the important things about this which i didn't actually state

1054
01:06:33,640 --> 01:06:35,080
this point

1055
01:06:35,100 --> 01:06:37,420
is that when you said the graph

1056
01:06:37,520 --> 01:06:40,270
for you to be able to solve this using this

1057
01:06:40,310 --> 01:06:42,140
max flow problem

1058
01:06:45,600 --> 01:06:48,600
must have

1059
01:06:48,670 --> 01:06:51,640
positive weight

1060
01:06:53,790 --> 01:06:56,480
you must have positive weight on the edges

1061
01:06:56,500 --> 01:07:00,000
the reason for this is

1062
01:07:00,600 --> 01:07:03,940
if you don't have positive weight on the edges

1063
01:07:03,940 --> 01:07:06,420
mean flow algorithm does not work

1064
01:07:06,460 --> 01:07:08,310
there is no such thing

1065
01:07:08,370 --> 01:07:09,670
on the graph with

1066
01:07:09,710 --> 01:07:11,370
negative edges

1067
01:07:11,410 --> 01:07:14,520
the thing was how much how much

1068
01:07:15,650 --> 01:07:17,850
four water whatever can you

1069
01:07:17,850 --> 01:07:20,480
pass along an edge which has in the past

1070
01:07:20,520 --> 01:07:22,920
the solution the question doesn't really

1071
01:07:24,960 --> 01:07:28,640
and so the must have positive weight

1072
01:07:28,650 --> 01:07:30,790
now it turns out that

1073
01:07:30,850 --> 01:07:32,040
does occur

1074
01:07:32,040 --> 01:07:33,290
a lot the problems

1075
01:07:33,310 --> 01:07:38,770
most problems going to have positive weight and hence you can solve using this

1076
01:07:38,810 --> 01:07:40,140
using this

1077
01:07:40,190 --> 01:07:42,500
graph cut out i should say

1078
01:07:42,560 --> 01:07:46,750
if you have some the edges of the graph is the graph

1079
01:07:46,770 --> 01:07:48,830
the the graph corresponds to

1080
01:07:48,920 --> 01:07:53,690
energy function

1081
01:07:53,730 --> 01:07:56,170
now the energy function

1082
01:07:56,290 --> 01:07:58,540
is represented by the graph

1083
01:07:58,540 --> 01:08:03,290
energy functions can have negative to right and so the question is minimizing the energy

1084
01:08:03,290 --> 01:08:05,960
function makes perfectly good sense

1085
01:08:06,000 --> 01:08:07,040
even if you have

1086
01:08:07,390 --> 01:08:11,670
its negative and in places in the terms of the negative in the sense that

1087
01:08:11,810 --> 01:08:13,060
it will become clear

1088
01:08:13,100 --> 01:08:15,500
so this problem makes sense

1089
01:08:15,500 --> 01:08:18,710
and you could think of solving it

1090
01:08:18,750 --> 01:08:22,390
the way we solve it is to turn into the graph and solver using the

1091
01:08:22,390 --> 01:08:24,410
graphical form max

1092
01:08:25,140 --> 01:08:26,500
it doesn't work

1093
01:08:26,540 --> 01:08:30,250
if you have a negative to negative edges in the graph and in fact in

1094
01:08:30,250 --> 01:08:32,330
that particular case

1095
01:08:32,330 --> 01:08:35,440
you can draw graphs negative edges and finding

1096
01:08:35,480 --> 01:08:37,500
max finding the

1097
01:08:37,540 --> 01:08:38,940
optimal cuts

1098
01:08:39,830 --> 01:08:40,690
the graph

1099
01:08:40,710 --> 01:08:43,290
with negative images in fact NP hard

1100
01:08:44,170 --> 01:08:46,250
if the edge positive

1101
01:08:47,060 --> 01:08:50,500
this flow algorithm which has polynomial time

1102
01:08:50,520 --> 01:08:54,890
if not the negative is an NP hard problem so you don't like

1103
01:08:54,940 --> 01:08:56,190
negative edges

1104
01:08:56,210 --> 01:08:58,710
on graphs

1105
01:08:59,100 --> 01:09:03,060
that that's prelude to what carbon few slides so his general idea

1106
01:09:03,100 --> 01:09:08,460
you gotta unfortunately have got videos here but i think it's not entirely necessary he

1107
01:09:08,460 --> 01:09:09,920
got the top cow

1108
01:09:09,940 --> 01:09:11,460
one around the field

1109
01:09:12,600 --> 01:09:15,580
the bottom left could represent the

1110
01:09:15,690 --> 01:09:18,230
in contrast terms on the edges

1111
01:09:19,500 --> 01:09:24,150
it's obvious contrast in the image and so you get this contrast terms

1112
01:09:24,210 --> 01:09:27,390
which become part of your cost function and you can segment

1113
01:09:29,600 --> 01:09:31,350
OK so you sigma this thing

1114
01:09:31,370 --> 01:09:32,560
and you get

1115
01:09:36,020 --> 01:09:40,640
you get this you get your you get you contrast terms your solution

1116
01:09:40,690 --> 01:09:44,440
and now you come up with the second problem with the cow move too much

1117
01:09:44,480 --> 01:09:45,170
and you

1118
01:09:45,270 --> 01:09:48,140
thirty the second later in different frame

1119
01:09:48,210 --> 01:09:51,210
very similar contrast terms

1120
01:09:52,410 --> 01:09:55,980
and i think that that that having to redo the whole computation

1121
01:09:56,020 --> 01:09:59,770
well the idea is to look at the difference and update the graphs

1122
01:09:59,770 --> 01:10:02,980
how does the graph get updated one of the original graph

1123
01:10:03,000 --> 01:10:06,750
we like correspond to the it's almost the same

1124
01:10:06,770 --> 01:10:09,020
in most parts of the image right

1125
01:10:09,040 --> 01:10:14,310
i think obviously the unary terms are unchanged from assuming certain background

1126
01:10:14,370 --> 01:10:18,000
nothing's changed your returns also

1127
01:10:18,000 --> 01:10:20,850
you will die terms and unchanged except

1128
01:10:20,870 --> 01:10:24,620
around the edges of the object and so the number of nodes in the graph

1129
01:10:24,690 --> 01:10:34,170
change is very small

1130
01:10:37,170 --> 01:10:42,390
that of course typically the EU returns to its history the binary to cost which

1131
01:10:42,390 --> 01:10:43,010
which would be

1132
01:10:44,270 --> 01:10:46,000
drive the complexity of this

1133
01:10:46,020 --> 01:10:50,270
i thing in fact but you're right if lighting changes

1134
01:10:50,350 --> 01:10:53,270
then you know maybe calc slightly different colours

1135
01:10:54,410 --> 01:10:57,350
maybe it doesn't make a lot of terms probably quite the same

1136
01:10:57,420 --> 01:11:00,920
and the edges of the the way of the edges

1137
01:11:00,920 --> 01:11:04,600
they were change to match mental potts model which is the

1138
01:11:04,640 --> 01:11:07,420
doesn't depend on the rules that doesn't change all

1139
01:11:07,440 --> 01:11:08,640
part of the function

1140
01:11:08,650 --> 01:11:10,890
so for instance here

1141
01:11:25,140 --> 01:11:29,850
yes if the if the differences between the two graphs and minor

1142
01:11:29,870 --> 01:11:31,230
in fact you can actually have

1143
01:11:31,290 --> 01:11:35,060
don't have to have the same structure in fact normal will have the same structure

1144
01:11:35,100 --> 01:11:37,870
the same graph with different weights

1145
01:11:37,870 --> 01:11:41,690
most weights will be unchanged then we'll see how this

1146
01:11:48,310 --> 01:11:50,120
oh well

1147
01:11:50,120 --> 01:11:52,460
doesn't accumulate i mean that from from

1148
01:11:53,140 --> 01:11:57,150
time time you see so you just comparing it with the graph

1149
01:11:57,210 --> 01:11:59,120
in the last time

1150
01:11:59,140 --> 01:12:02,920
it's a bit and you will see how this works

1151
01:12:04,730 --> 01:12:08,020
the unary terms in this particular case maybe dynamic

1152
01:12:08,020 --> 01:12:09,830
maybe life changed in

1153
01:12:09,920 --> 01:12:11,750
given area and so on

1154
01:12:11,950 --> 01:12:13,290
you can

1155
01:12:13,310 --> 01:12:14,440
do that

1156
01:12:14,710 --> 01:12:23,410
time in some of these five here another one in this particular case it's best

1157
01:12:25,020 --> 01:12:27,210
it's best to change

1158
01:12:27,230 --> 01:12:31,730
so OK let's let's get down a little bit detail here what happens

1159
01:12:33,440 --> 01:12:37,980
so you get more mathematics not to do this but i have the graphs formed

1160
01:12:38,020 --> 01:12:39,670
makes it

1161
01:12:39,670 --> 01:12:41,980
computing the st mincut

1162
01:12:41,980 --> 01:12:44,040
the of this graph

1163
01:12:44,120 --> 01:12:47,980
well OK so

1164
01:12:49,270 --> 01:12:50,920
forget about the middle

1165
01:12:50,920 --> 01:12:54,560
basically this this augmenting path algorithm which used to

1166
01:12:54,920 --> 01:12:58,600
not supposing this is this is a graph which you start with

1167
01:12:58,620 --> 01:13:02,520
the original graph zero those the weights on the k

1168
01:13:02,580 --> 01:13:03,750
so there's

1169
01:13:04,480 --> 01:13:06,250
one example

1170
01:13:06,330 --> 01:13:08,600
so this

1171
01:13:08,710 --> 01:13:14,000
this is the thing here called re parametrisation which i'll talk about the more the

1172
01:13:14,000 --> 01:13:15,350
general idea is

1173
01:13:15,370 --> 01:13:17,980
behind re parametrisation is

1174
01:13:18,000 --> 01:13:22,940
i prefer to call reweighting you're assigning different weights to the edges

1175
01:13:23,000 --> 01:13:24,690
in a way that

1176
01:13:24,690 --> 01:13:28,920
the function of the cost function represented by the graph

1177
01:13:28,980 --> 01:13:31,250
does not change so

1178
01:13:31,270 --> 01:13:32,920
there are two basic

1179
01:13:33,020 --> 01:13:35,730
reweighting or re parametrisation

1180
01:13:35,810 --> 01:13:37,730
things you can do one of them

1181
01:13:37,780 --> 01:13:41,000
is you can add see alpha on the right-hand branch

1182
01:13:41,020 --> 01:13:44,410
right hand passed down through their i've added alpha

1183
01:13:44,420 --> 01:13:46,710
to each of those

1184
01:13:47,540 --> 01:13:50,410
where alpha is any value right

1185
01:13:50,480 --> 01:13:53,650
and it turns out that does not change

1186
01:13:53,650 --> 01:13:55,240
those are the two

1187
01:13:55,240 --> 01:13:58,980
population definition f of x is a is the

1188
01:13:59,460 --> 01:14:02,370
continuous probability distribution

1189
01:14:02,430 --> 01:14:03,660
four x

1190
01:14:03,720 --> 01:14:05,600
it is

1191
01:14:05,630 --> 01:14:12,240
so that's different when you have continuous values you you don't have the probability that

1192
01:14:12,240 --> 01:14:16,790
x equals x survive because it's always zero probability that

1193
01:14:16,790 --> 01:14:21,870
the temperature is exactly a hundred degrees is zero because it could be a hundred

1194
01:14:21,870 --> 01:14:26,360
point zero zero zero zero one degrees or something else is an infinite number of

1195
01:14:26,360 --> 01:14:28,520
possibilities so we have instead

1196
01:14:28,560 --> 01:14:31,960
was called the probability density in

1197
01:14:31,960 --> 01:14:34,640
we have continuous random variables

1198
01:14:34,660 --> 01:14:35,810
not going to

1199
01:14:35,820 --> 01:14:38,850
we need to know a lot about this manuscript this is i want to get

1200
01:14:38,850 --> 01:14:43,500
the is the basic idea is that

1201
01:14:43,540 --> 01:14:44,500
so these are

1202
01:14:44,520 --> 01:14:46,990
called population measures

1203
01:14:47,010 --> 01:14:48,280
because they

1204
01:14:48,290 --> 01:14:53,320
refer to the whole population of possible outcomes and they measure

1205
01:14:53,340 --> 01:14:55,140
the probabilities

1206
01:14:55,160 --> 01:14:57,250
so it's a it's the truth

1207
01:14:57,260 --> 01:14:59,330
but there are also sample

1208
01:14:59,350 --> 01:15:00,760
it means

1209
01:15:00,790 --> 01:15:03,510
when you get this is

1210
01:15:03,510 --> 01:15:06,260
the two partner counting the leaves three

1211
01:15:06,280 --> 01:15:08,780
you can estimate from a sample

1212
01:15:08,930 --> 01:15:11,590
the population expected values

1213
01:15:11,600 --> 01:15:13,610
so the population

1214
01:15:13,660 --> 01:15:16,000
i mean

1215
01:15:16,080 --> 01:15:18,250
is often written x bar

1216
01:15:18,260 --> 01:15:20,990
if you have a sample with n observations

1217
01:15:21,040 --> 01:15:23,870
it's the summation i equals one

1218
01:15:26,130 --> 01:15:27,270
x by

1219
01:15:27,280 --> 01:15:30,540
divided by right that's the average

1220
01:15:30,550 --> 01:15:34,880
you know that formula right at you get your heart and leaves and we count

1221
01:15:34,880 --> 01:15:36,260
the number of leaves

1222
01:15:40,490 --> 01:15:45,350
well you have an branches on the tree you count the number of leaves

1223
01:15:45,400 --> 01:15:46,870
and some of up

1224
01:15:46,880 --> 01:15:48,450
one would be

1225
01:15:48,470 --> 01:15:50,090
there is a gift

1226
01:15:50,110 --> 01:15:52,110
i have a little trouble putting this into the

1227
01:15:52,150 --> 01:15:56,520
the two-part story but you see the idea you know the average person that's most

1228
01:15:56,540 --> 01:15:58,060
elementary concepts

1229
01:15:58,080 --> 01:16:02,980
and so you can use this

1230
01:16:02,980 --> 01:16:07,970
to estimate either a discrete or continuous expected value

1231
01:16:08,040 --> 01:16:14,060
now in finance is often reference to another kind of average which i want to

1232
01:16:14,060 --> 01:16:17,470
refer to an which in the jeremy siegel

1233
01:16:17,700 --> 01:16:20,040
there's a lot is made of this

1234
01:16:20,060 --> 01:16:24,340
and the other kind of average is called the geometric average

1235
01:16:24,350 --> 01:16:26,760
and so

1236
01:16:26,800 --> 01:16:28,130
we call that

1237
01:16:28,140 --> 01:16:32,290
i only show the sample version of it

1238
01:16:32,390 --> 01:16:34,780
g of x

1239
01:16:34,780 --> 01:16:36,370
he calls

1240
01:16:36,400 --> 01:16:38,250
the left parentheses

1241
01:16:40,750 --> 01:16:42,490
i equals one

1242
01:16:44,350 --> 01:16:46,290
act so by

1243
01:16:46,300 --> 01:16:49,140
there's one other and power

1244
01:16:49,200 --> 01:16:51,370
so what you see that

1245
01:16:52,760 --> 01:16:56,130
if you have trouble seeing that

1246
01:16:56,160 --> 01:17:02,880
it's different instead of summing and dividing by i multiply them all together

1247
01:17:02,890 --> 01:17:05,840
and take the and through of the

1248
01:17:05,850 --> 01:17:08,780
so this is called the geometric average

1249
01:17:08,790 --> 01:17:10,190
and it's used

1250
01:17:11,590 --> 01:17:13,450
for positive numbers

1251
01:17:13,450 --> 01:17:15,560
so if you have any negative numbers

1252
01:17:15,580 --> 01:17:19,100
you have a problem if you had one negative number in the product would be

1253
01:17:19,100 --> 01:17:22,950
a negative number and if you took a route of that you might get an

1254
01:17:22,960 --> 01:17:26,290
imaginary number so we don't want to

1255
01:17:26,390 --> 01:17:28,370
use it in that case

1256
01:17:29,810 --> 01:17:33,250
this appendix to one of the chapters in germany

1257
01:17:33,360 --> 01:17:35,780
siegel's board

1258
01:17:35,790 --> 01:17:37,110
where he says that

1259
01:17:37,140 --> 01:17:40,050
one of the most important applications of

1260
01:17:40,060 --> 01:17:43,680
this theory is to measure how successful and investors

1261
01:17:49,210 --> 01:17:51,940
suppose someone is managing money

1262
01:17:53,330 --> 01:17:55,730
have they done well

1263
01:17:55,740 --> 01:17:57,380
so you would say well they

1264
01:17:57,400 --> 01:18:01,520
investing money over a number of different years let's take the average over all the

1265
01:18:01,520 --> 01:18:03,430
different years OK

1266
01:18:03,460 --> 01:18:08,110
so i suppose someone has been investing money for an years annexed to by is

1267
01:18:08,110 --> 01:18:10,050
the return on investment

1268
01:18:10,060 --> 01:18:11,580
in a given year

1269
01:18:11,600 --> 01:18:14,900
what is the average performance with the natural thing to do would be the average

1270
01:18:14,900 --> 01:18:17,020
them up

1271
01:18:19,930 --> 01:18:24,030
jeremy says that maybe that's not a very good thing to do

1272
01:18:24,970 --> 01:18:28,330
but he says you should do instead

1273
01:18:29,360 --> 01:18:33,390
take the geometric average of gross returns

1274
01:18:35,090 --> 01:18:39,650
what does it mean the return on investment is how much you made from the

1275
01:18:39,650 --> 01:18:44,450
investment as a percentage of the money invested OK

1276
01:18:44,450 --> 01:18:48,340
the gross return is the return plus one

1277
01:18:48,360 --> 01:18:48,960
all right

1278
01:18:48,970 --> 01:18:50,730
the worst you can ever do

1279
01:18:50,740 --> 01:18:54,980
investing is to lose all of their investment one hundred percent

1280
01:18:55,880 --> 01:19:00,580
if we add one to the return you've got to number that's never negative

1281
01:19:00,590 --> 01:19:04,700
and we can then use geometric returns OK

1282
01:19:04,740 --> 01:19:07,760
and so jeremy siegel says

1283
01:19:07,770 --> 01:19:12,080
really in finance we should be using geometric and arithmetic

1284
01:19:12,240 --> 01:19:16,180
why is that well i tell you in very simple terms i think suppose someone

1285
01:19:16,180 --> 01:19:17,960
is investing your money

1286
01:19:17,960 --> 01:19:18,660
and he

1287
01:19:19,860 --> 01:19:24,720
i have had very good returns i have invested i produced twenty percent a year

1288
01:19:24,750 --> 01:19:27,440
for nine of the last ten years

1289
01:19:27,440 --> 01:19:28,240
and so

1290
01:19:28,260 --> 01:19:31,230
eighty that's great but what about the last year here

1291
01:19:31,250 --> 01:19:34,000
and is all i lost one hundred percent

1292
01:19:34,050 --> 01:19:36,050
in that year OK

1293
01:19:36,070 --> 01:19:39,580
so you might say all right that's good so i i i would add up

1294
01:19:39,580 --> 01:19:41,610
to twenty percent a year

1295
01:19:41,630 --> 01:19:43,450
for nine years

1296
01:19:43,450 --> 01:19:45,500
and then put zero

1297
01:19:45,550 --> 01:19:50,550
nine hundred twenty business return for nine years and put in zero for one year

1298
01:19:50,600 --> 01:19:53,320
and maybe that doesn't look right

1299
01:19:54,100 --> 01:19:58,000
think about it if you're investing your money with someone like that would you end

1300
01:19:58,000 --> 01:19:59,290
up with

1301
01:19:59,410 --> 01:20:01,100
you end up with nothing

1302
01:20:01,120 --> 01:20:05,280
if a one year when they lose everything doesn't matter how much they made in

1303
01:20:05,280 --> 01:20:06,590
the other years

1304
01:20:07,040 --> 01:20:12,490
so jeremy says in the in the text is that the geometric return is always

1305
01:20:12,490 --> 01:20:14,760
lower than the arithmetic return

1306
01:20:14,790 --> 01:20:17,260
unless all the numbers are the same

1307
01:20:17,270 --> 01:20:19,090
and so it's a less

1308
01:20:19,800 --> 01:20:22,180
it's a less optimistic version

1309
01:20:22,190 --> 01:20:24,550
and so we should use that but people

1310
01:20:24,560 --> 01:20:27,040
in finance resist using that

1311
01:20:27,050 --> 01:20:31,650
because it's a lower number when you're advertising every thirty one

1312
01:20:31,660 --> 01:20:33,780
the biggest possible

1313
01:20:34,550 --> 01:20:37,780
we also need some measure

1314
01:20:37,800 --> 01:20:41,950
we've been talking here about measures of central tendency only

1315
01:20:41,990 --> 01:20:44,770
and in finance we need as well

1316
01:20:44,810 --> 01:20:46,620
measures of

1317
01:20:48,260 --> 01:20:52,010
that's how much something very

1318
01:20:52,020 --> 01:20:58,240
central tendency is the manager of the center of a probability distribution of the like

1319
01:20:58,280 --> 01:21:01,610
central tendency is to measure

1320
01:21:01,630 --> 01:21:05,150
variance is a measure of how much things change

1321
01:21:05,160 --> 01:21:07,570
from one observation to another

1322
01:21:07,580 --> 01:21:09,190
so we have

1323
01:21:09,190 --> 01:21:12,620
they given the lecture

1324
01:21:12,630 --> 01:21:16,020
on text mining and information extraction

1325
01:21:16,030 --> 01:21:21,010
and we'll focus well we've seen in the last lecture

1326
01:21:25,190 --> 01:21:28,010
extract information

1327
01:21:28,030 --> 01:21:30,690
and yes the

1328
01:21:30,700 --> 01:21:32,590
application which

1329
01:21:32,630 --> 01:21:36,680
extract from

1330
01:21:36,730 --> 01:21:38,050
they don't

1331
01:21:38,190 --> 01:21:40,230
it the

1332
01:21:41,970 --> 01:21:45,470
and then because of course of information

1333
01:21:48,780 --> 01:21:50,750
the cast

1334
01:21:50,760 --> 01:21:51,830
the degree

1335
01:21:51,840 --> 01:21:55,000
thank you much

1336
01:22:02,100 --> 01:22:06,530
i think it's a very weak cue of information from

1337
01:22:06,550 --> 01:22:08,330
you probably know

1338
01:22:08,980 --> 01:22:12,780
it gives kind of the work we're we trying to

1339
01:22:12,790 --> 01:22:16,450
the information actually

1340
01:22:16,470 --> 01:22:18,890
information retrieval

1341
01:22:19,490 --> 01:22:22,070
the core

1342
01:22:22,090 --> 01:22:23,810
part of this

1343
01:22:23,860 --> 01:22:27,730
of information items in databases and repositories

1344
01:22:28,660 --> 01:22:29,970
according to

1345
01:22:29,980 --> 01:22:31,850
any information

1346
01:22:31,900 --> 01:22:34,660
i think i to do that

1347
01:22:34,700 --> 01:22:37,010
also during the the class

1348
01:22:37,940 --> 01:22:44,120
thank you for all of the information on the content more

1349
01:22:44,170 --> 01:22:45,540
we have no

1350
01:22:47,760 --> 01:22:49,280
the new material

1351
01:22:49,300 --> 01:22:53,190
make sure you can take a look

1352
01:22:53,270 --> 01:22:54,580
text audio

1353
01:22:54,630 --> 01:22:56,350
so on

1354
01:22:56,360 --> 01:22:58,800
and all the information

1355
01:22:58,830 --> 01:23:00,460
we are talking about

1356
01:23:00,500 --> 01:23:01,990
by q

1357
01:23:02,660 --> 01:23:08,240
typing and key words but there are other ways of

1358
01:23:08,250 --> 01:23:09,970
expressing information

1359
01:23:11,250 --> 01:23:13,630
and it's not

1360
01:23:13,680 --> 01:23:16,690
show great by

1361
01:23:16,690 --> 01:23:18,190
type of query

1362
01:23:18,190 --> 01:23:24,150
when you actually can live with multimedia objects in the image and you want to

1363
01:23:24,150 --> 01:23:25,940
find similar

1364
01:23:25,940 --> 01:23:29,250
you could import some music cues

1365
01:23:29,260 --> 01:23:31,320
and find music files

1366
01:23:31,380 --> 01:23:33,540
that are quite similar

1367
01:23:33,540 --> 01:23:35,780
according the

1368
01:23:35,790 --> 01:23:42,060
we use these into multimodal information needs so need

1369
01:23:42,070 --> 01:23:44,000
in a career that i

1370
01:23:44,010 --> 01:23:47,530
that can be a mixture of text

1371
01:23:48,380 --> 01:23:51,880
images for

1372
01:23:51,940 --> 01:23:57,970
well i don't think i have to tell otherwise would not be cool

1373
01:23:58,030 --> 01:24:01,450
but we have also for information

1374
01:24:01,480 --> 01:24:04,090
if you look at the success of search engine

1375
01:24:04,540 --> 01:24:06,380
on the web the

1376
01:24:07,260 --> 01:24:10,420
of course there are many different

1377
01:24:12,170 --> 01:24:15,000
building information systems

1378
01:24:15,000 --> 01:24:20,700
and i cited here you know

1379
01:24:20,750 --> 01:24:24,410
and i would like to information retrieval rules

1380
01:24:25,480 --> 01:24:27,410
as we know it

1381
01:24:27,410 --> 01:24:30,630
then we have

1382
01:24:30,670 --> 01:24:43,440
maybe i i'll get just means get points

1383
01:24:49,320 --> 01:24:52,660
the club information system

1384
01:24:52,850 --> 01:24:55,280
before the really cool

1385
01:24:55,320 --> 01:24:57,620
this might be words

1386
01:24:57,630 --> 01:25:00,900
and we also have to make few

1387
01:25:02,030 --> 01:25:05,750
the data structures in this

1388
01:25:05,760 --> 01:25:07,670
the data structure

1389
01:25:07,690 --> 01:25:09,850
for large collection

1390
01:25:09,910 --> 01:25:14,820
and read what we try to perform a kind of matching ranking

1391
01:25:14,850 --> 01:25:17,100
we try to find the

1392
01:25:17,130 --> 01:25:19,540
with the the query

1393
01:25:20,540 --> 01:25:24,820
we have all types of ranking functions here

1394
01:25:24,840 --> 01:25:26,550
they can be used

1395
01:25:26,560 --> 01:25:31,210
for type of and i also think about the project

1396
01:25:31,850 --> 01:25:37,720
and what if we could just list of relevant documents

1397
01:25:39,040 --> 01:25:40,660
well but is square

1398
01:25:40,660 --> 01:25:47,010
can be also express the by some sort of to find you will find in

1399
01:25:47,020 --> 01:25:49,550
the article

1400
01:25:49,580 --> 01:25:52,100
we used to live in a few keyword

1401
01:25:53,200 --> 01:25:55,920
also you have a question answering systems

1402
01:25:55,970 --> 01:25:57,790
where you

1403
01:25:57,850 --> 01:25:59,530
in the input

1404
01:25:59,870 --> 01:26:05,120
based in natural language which you a question but it can also be seen in

1405
01:26:05,120 --> 01:26:06,730
natural language

1406
01:26:06,780 --> 01:26:14,550
but you have other forms so is a question answering system

1407
01:26:14,560 --> 01:26:18,120
you really is a question of natural language

1408
01:26:19,160 --> 01:26:21,340
and in our life

1409
01:26:21,350 --> 01:26:24,260
in my what's

1410
01:26:26,540 --> 01:26:29,200
currently in question answering systems

1411
01:26:29,270 --> 01:26:30,290
in the

1412
01:26:30,340 --> 01:26:32,120
then you're

1413
01:26:32,160 --> 01:26:36,310
question is not compared to complete that

1414
01:26:36,340 --> 01:26:43,160
but the first sentence of the paragraph so small snippets of the document

1415
01:26:43,160 --> 01:26:45,200
these are are the observed variables

1416
01:26:45,270 --> 01:26:47,760
then you have a hidden variable y

1417
01:26:47,810 --> 01:26:53,540
which could be state variables so for instance this could be a secondary structure and

1418
01:26:53,540 --> 01:27:00,290
in particular structure could be that some of all the she walked walk while

1419
01:27:00,310 --> 01:27:04,620
so and then they can be relationship between the hidden variables and all of the

1420
01:27:08,910 --> 01:27:11,930
here x i y i

1421
01:27:11,990 --> 01:27:17,160
you can also have a relationship between one in very well with the next one

1422
01:27:17,280 --> 01:27:19,220
this is relation

1423
01:27:19,240 --> 01:27:23,950
then the rest is assumed to be independent and it is this probability model

1424
01:27:23,970 --> 01:27:29,160
like that before it

1425
01:27:29,200 --> 01:27:32,010
OK so and they begin fight

1426
01:27:36,010 --> 01:27:39,700
the estimation what to fit these

1427
01:27:39,760 --> 01:27:45,790
find the parameters of before

1428
01:27:45,810 --> 01:27:50,060
so now the question is once you have this property model so can you point

1429
01:27:50,080 --> 01:27:55,970
the label sequence the state sequence of both are the most likely state

1430
01:27:58,740 --> 01:28:00,810
this is the

1431
01:28:00,830 --> 01:28:08,080
the log of probability everything coupled with the product and then you get

1432
01:28:08,100 --> 01:28:13,870
some of these elements one with the emission probability one with

1433
01:28:13,890 --> 01:28:16,260
and this is is

1434
01:28:17,010 --> 01:28:20,350
the likelihood what one state

1435
01:28:20,410 --> 01:28:22,470
so what

1436
01:28:22,490 --> 01:28:28,100
problem the problem is even sequence you would like to find the label sequence y

1437
01:28:28,100 --> 01:28:34,600
y such that the probability of p x and y

1438
01:28:34,640 --> 01:28:37,330
for that we use dynamic programming

1439
01:28:39,060 --> 01:28:42,010
you and i wanted it

1440
01:28:42,040 --> 01:28:45,040
the idea is that for every position in sequence

1441
01:28:45,060 --> 01:28:51,740
so we consider all cases private position in sequence for every possible label that he

1442
01:28:51,810 --> 01:28:55,490
consider all cases where we could come

1443
01:28:55,560 --> 01:28:58,910
then you maximizing of all people tried

1444
01:28:59,010 --> 01:29:02,970
and since then we have this independent

1445
01:29:04,390 --> 01:29:06,760
we have independent here

1446
01:29:06,790 --> 01:29:12,810
we can compute these quantities efficiently so we can prove in the end that we

1447
01:29:13,600 --> 01:29:15,140
we can iteratively

1448
01:29:15,140 --> 01:29:17,950
extend the optimal path

1449
01:29:18,640 --> 01:29:19,450
so maybe

1450
01:29:19,450 --> 01:29:24,140
i mean i think you don't know you every time in the end i think

1451
01:29:28,950 --> 01:29:33,280
so that's the generalisation of this which is called the generalized hidden markov models

1452
01:29:33,330 --> 01:29:36,810
we don't assume

1453
01:29:36,850 --> 01:29:40,280
then we have lot transition every

1454
01:29:40,600 --> 01:29:43,220
every time step at every position

1455
01:29:43,240 --> 01:29:46,450
but we might if you be might switch

1456
01:29:46,450 --> 01:29:49,490
we might want to say so

1457
01:29:49,510 --> 01:29:50,640
you would like to

1458
01:29:50,660 --> 01:29:52,970
identify segments where certain

1459
01:29:53,390 --> 01:29:59,890
every transition is not a single letter is consumed but several letters you know so

1460
01:30:01,100 --> 01:30:05,330
the image one letter at time when he had made several levels

1461
01:30:05,430 --> 01:30:09,060
and we have the markov property on the second level

1462
01:30:09,080 --> 01:30:13,040
and we have within the segment we can have non markovian

1463
01:30:13,040 --> 01:30:19,170
o point depending on the length of the sequence

1464
01:30:19,220 --> 01:30:27,840
this is important for modelling segment segment length three points in the secondary structure k

1465
01:30:27,850 --> 01:30:31,500
known why some properties which

1466
01:30:31,800 --> 01:30:35,410
there are some

1467
01:30:35,410 --> 01:30:38,580
the things to which depend on the length of the segment

1468
01:30:38,590 --> 01:30:48,280
OK and there's also a dynamic programming algorithm for finding the most likely

1469
01:30:48,300 --> 01:30:52,920
so in case of hidden markov models we assume independent of of position from one

1470
01:30:52,960 --> 01:30:56,880
actually the independence of the

1471
01:30:58,660 --> 01:31:01,880
one position from the wise up with

1472
01:31:01,930 --> 01:31:06,450
and we had to do this in order to be able to compute the probability

1473
01:31:06,450 --> 01:31:07,430
in this way

1474
01:31:07,540 --> 01:31:12,880
OK so there's an extension of this which is called conditional random fields and here

1475
01:31:13,910 --> 01:31:16,220
in very might depend on any

1476
01:31:16,250 --> 01:31:18,700
input variables x

1477
01:31:18,710 --> 01:31:21,340
and what we're doing here and

1478
01:31:21,390 --> 01:31:24,160
you have i x and y

1479
01:31:24,160 --> 01:31:27,200
the internet which in the x and y

1480
01:31:27,220 --> 01:31:29,880
thank you for compute your w vectors

1481
01:31:30,310 --> 01:31:33,890
and you

1482
01:31:36,000 --> 01:31:41,370
the conditional probability in this case the difference is that they not modelling

1483
01:31:41,380 --> 01:31:46,240
the probability of p x p of x and y but the problem what in

1484
01:31:46,240 --> 01:31:47,930
the probability of x

1485
01:31:48,050 --> 01:31:52,500
y given x

1486
01:31:52,500 --> 01:31:57,720
but this can handle non independent features and this can also be extended to all

1487
01:31:57,770 --> 01:32:03,200
the markov condition

1488
01:32:03,210 --> 01:32:07,500
so what i'm going to talk about the a bit more

1489
01:32:07,500 --> 01:32:09,120
the other i

1490
01:32:10,500 --> 01:32:16,520
and i with which is called max margin structured output learning and here

1491
01:32:16,550 --> 01:32:18,170
i would like to know function

1492
01:32:18,210 --> 01:32:19,470
y given x

1493
01:32:19,490 --> 01:32:25,580
it is going to segmentation patients y for sequence x

1494
01:32:25,590 --> 01:32:27,460
so if you would like to

1495
01:32:27,500 --> 01:32:35,210
what didn't function why we can find the patient by maximizing the function by

1496
01:32:37,210 --> 01:32:44,930
so the training to be an n input output pairs of input

1497
01:32:44,990 --> 01:32:48,460
input and labels

1498
01:32:48,510 --> 01:32:53,430
and now we would like to add such that has a large margin between any

1499
01:32:53,430 --> 01:32:59,640
true segmentation and approximate to consider this as a two class classification problem any

1500
01:32:59,660 --> 01:33:01,120
true segmentation

1501
01:33:01,140 --> 01:33:06,590
so for example any wrong segmentations is negative

1502
01:33:06,670 --> 01:33:08,670
so we can write this stuff

1503
01:33:10,220 --> 01:33:14,260
f y ten if x minus

1504
01:33:14,260 --> 01:33:18,630
OK so it actually log exp function so you know it's convex but it's actually

1505
01:33:18,650 --> 01:33:23,840
very complicated log exp function he probably talked about the easy to compute this is

1506
01:33:23,860 --> 01:33:27,760
log x function that has an exponential number of extra terms so it's not easy

1507
01:33:27,760 --> 01:33:31,380
to compute so if you want to compute the likelihood that's that's what you're up

1508
01:33:34,110 --> 01:33:39,340
OK but based on what we did before

1509
01:33:39,360 --> 01:33:42,050
it's very natural to think of the same thing

1510
01:33:42,190 --> 01:33:50,860
right this likelihood the summation is a hard problem

1511
01:33:50,900 --> 01:33:52,550
on the general graph

1512
01:33:52,570 --> 01:33:55,750
but if i had a tree it's an easy problem i can compute the likelihood

1513
01:33:55,750 --> 01:33:59,940
is very fast on any tree using the sum product algorithm

1514
01:33:59,940 --> 01:34:02,860
right so the same idea what

1515
01:34:02,880 --> 01:34:06,170
with the different function what i just do the exact same thing not just a

1516
01:34:06,170 --> 01:34:09,420
convex combinations of trees as before

1517
01:34:09,440 --> 01:34:12,570
right so this is the example we looked at before i was taking my original

1518
01:34:12,570 --> 01:34:16,050
problem now splitting it into trees

1519
01:34:16,070 --> 01:34:19,880
but all this do it on this new function

1520
01:34:20,030 --> 01:34:23,210
so if i do that then i immediately get an upper bound on the log

1521
01:34:23,210 --> 01:34:24,650
likelihood of the data

1522
01:34:24,670 --> 01:34:28,130
and so the

1523
01:34:28,150 --> 01:34:32,520
this is the log likelihood on your original problem and what it's saying is it's

1524
01:34:32,530 --> 01:34:34,000
a convex combination

1525
01:34:34,070 --> 01:34:38,440
this is the log likelihood on treaty and that's whatever weight you put on that

1526
01:34:39,730 --> 01:34:43,090
so this is the same thing we can efficiently evaluate as long as we don't

1527
01:34:43,090 --> 01:34:46,070
have too many trees you can always compute that upper bound

1528
01:34:46,130 --> 01:34:49,570
and then the question is what is the best upper bound

1529
01:34:49,590 --> 01:34:51,550
i would like to find the

1530
01:34:51,570 --> 01:34:57,250
split into trees that gives you the tightest possible upper bound on the log likelihood

1531
01:34:59,940 --> 01:35:01,730
so let me let me show you

1532
01:35:03,000 --> 01:35:10,670
what this is doing

1533
01:35:10,670 --> 01:35:14,940
so let me just write a little bit here if you don't mind

1534
01:35:14,960 --> 01:35:19,960
what i'm going to show you in a minute is that the best possible upper

1535
01:35:26,570 --> 01:35:29,840
related to something known as the bay entropy

1536
01:35:29,920 --> 01:35:34,090
in this case because we're doing convex combinations is related to what's known as the

1537
01:35:34,110 --> 01:35:36,920
reweighted entropy

1538
01:35:36,940 --> 01:35:55,110
so let me let me define that for you

1539
01:35:55,110 --> 01:36:10,550
OK almost down

1540
01:36:10,570 --> 01:36:23,110
so many people know what an entropy is

1541
01:36:26,280 --> 01:36:28,190
and entropy sort of the way of

1542
01:36:28,190 --> 01:36:32,820
measuring the uncertainty that's that's implicit in the distribution

1543
01:36:32,920 --> 01:36:35,670
and they sort of pop up a lot

1544
01:36:35,690 --> 01:36:40,690
in physics there's this sort of a reason physical reason why you would get entropy

1545
01:36:40,840 --> 01:36:44,840
something known as the gibbs variational principle but we don't need to know that that

1546
01:36:44,860 --> 01:36:45,570
right now

1547
01:36:45,590 --> 01:36:48,230
what's important for us is the following

1548
01:36:50,710 --> 01:36:55,050
if you remember the things that were used optimizing or from this morning used optimizing

1549
01:36:55,050 --> 01:36:58,980
over things that are the marginal distributions at node c of the vector at every

1550
01:36:59,960 --> 01:37:03,320
and if you have the vector every node you can define

1551
01:37:03,690 --> 01:37:07,980
it's the probability distribution you compute you can compute its entropy

1552
01:37:08,070 --> 01:37:10,860
its entropy is just the average of the

1553
01:37:12,800 --> 01:37:17,840
of one over the distribution or it's minus the average of the log sort think

1554
01:37:17,840 --> 01:37:20,130
it's and it's a measure of

1555
01:37:20,190 --> 01:37:25,050
its maximal uniform distribution it's zero if you have a distribution puts all its mass

1556
01:37:25,050 --> 01:37:27,210
and one point

1557
01:37:27,230 --> 01:37:28,420
right so that's

1558
01:37:28,440 --> 01:37:30,520
that's actually a

1559
01:37:30,530 --> 01:37:35,250
concave function if you play around with a little bit it's concave is a function

1560
01:37:35,250 --> 01:37:38,400
of the vector the marginal vector the note

1561
01:37:41,730 --> 01:37:44,920
if you remember we had a matrix that was sitting on every edge

1562
01:37:45,050 --> 01:37:49,960
i member from this morning you had matrices that we're sitting on the edge and

1563
01:37:49,960 --> 01:37:53,900
we are putting constraints on those you other matrix you can use it to compute

1564
01:37:53,900 --> 01:37:56,940
something called the mutual information

1565
01:37:56,940 --> 01:37:58,930
o continued to drop

1566
01:37:58,960 --> 01:38:04,010
OK this is what you would expect from from the previous bond because somehow we

1567
01:38:04,030 --> 01:38:08,950
would expect that the complexity goes down the complexity increases when you use more iterations

1568
01:38:09,590 --> 01:38:13,370
so therefore this is high potential for doing so we we

1569
01:38:13,500 --> 01:38:15,720
we would expect that

1570
01:38:15,750 --> 01:38:21,980
with this with a simple hypothesis the generalized but what was found that the generalisation

1571
01:38:21,990 --> 01:38:24,510
goes down so this was

1572
01:38:24,790 --> 01:38:27,750
something else what we would have expected from the band

1573
01:38:29,090 --> 01:38:31,000
the question is is occam's razor

1574
01:38:31,190 --> 01:38:34,550
that is that simple things perform better

1575
01:38:34,660 --> 01:38:37,510
is the wrong in this case

1576
01:38:37,530 --> 01:38:41,850
so just i mean it turned out just need to be explanation

1577
01:38:41,850 --> 01:38:48,780
and that came in nineteen ninety eight by p finding the point about the only

1578
01:38:48,810 --> 01:38:52,670
and they came up with a different bound so they were able to show that

1579
01:38:53,510 --> 01:38:55,980
expected error of a function

1580
01:38:56,110 --> 01:38:58,020
it is bounded by the

1581
01:38:58,040 --> 01:39:00,280
the empirical margin error

1582
01:39:00,320 --> 01:39:02,200
OK this quantity to you

1583
01:39:02,200 --> 01:39:08,350
so the classification of the margin plus something which again depends on the VC dimension

1584
01:39:08,350 --> 01:39:13,600
of the basic policies class but it somehow divided by p day and this is

1585
01:39:14,260 --> 01:39:17,910
in march i mean this empirical margin

1586
01:39:17,920 --> 01:39:21,460
and then again sometimes which depends on it

1587
01:39:21,470 --> 01:39:25,970
so no that we don't have it depends on t anymore

1588
01:39:26,030 --> 01:39:28,150
so previously had depends on

1589
01:39:28,150 --> 01:39:34,060
on the complexity of this combined hypothesis class so he is only

1590
01:39:34,200 --> 01:39:36,480
the VC dimension of of the single

1591
01:39:36,510 --> 01:39:40,050
of the basic says class k but on the other hand we have this depends

1592
01:39:40,050 --> 01:39:41,230
on the day

1593
01:39:41,300 --> 01:39:43,460
so if you could somehow lucky

1594
01:39:43,470 --> 01:39:50,000
that peter large and on the same the same time disappear commodity small then this

1595
01:39:50,000 --> 01:39:51,960
bone is going to be small

1596
01:39:53,310 --> 01:39:57,200
but if somehow for for some reasonably large theta

1597
01:39:57,220 --> 01:39:59,160
this is not very large

1598
01:39:59,180 --> 01:40:02,290
then we can't i mean the response

1599
01:40:02,320 --> 01:40:07,130
what is largely

1600
01:40:36,660 --> 01:40:39,980
i mean the the

1601
01:40:40,000 --> 01:40:43,950
essentially could say even given a certain number of examples of the sea the different

1602
01:40:43,950 --> 01:40:50,400
classes fixed simply to logan over chemistry iterations i mean combinations of songs i mean

1603
01:40:51,020 --> 01:40:58,060
this number of iterations of selecting from the basic forces classes t convex hull right

1604
01:40:58,060 --> 01:41:01,420
so this function classes fixed

1605
01:41:01,720 --> 01:41:04,900
i mean in the

1606
01:41:04,910 --> 01:41:07,620
OK so

1607
01:41:07,630 --> 01:41:15,280
it's it's true so i mean many more examples then this becomes more complex

1608
01:41:15,490 --> 01:41:27,150
i mean you could say it is fixed class then OK only works for certain

1609
01:41:27,200 --> 01:41:34,630
OK so

1610
01:41:34,650 --> 01:41:39,710
so we have this bound if if they have a large theta so then this

1611
01:41:39,770 --> 01:41:43,880
i mean if you if you're lucky so then the empirical margin small so this

1612
01:41:43,890 --> 01:41:47,140
term is small and on the other hand this system small

1613
01:41:47,140 --> 01:41:53,150
so then then we are lacking but if you do we consider small theta then

1614
01:41:53,390 --> 01:41:59,260
the empirical margin error is just as large as points in the training error

1615
01:41:59,320 --> 01:42:02,230
OK this is large so

1616
01:42:02,250 --> 01:42:03,810
and then we have the somehow

1617
01:42:03,820 --> 01:42:06,600
then you could also easily the other part

1618
01:42:11,200 --> 01:42:15,640
this is just like

1619
01:42:15,790 --> 01:42:20,510
i think it was not generalize to real

1620
01:42:20,610 --> 01:42:27,590
but i haven't seen as result

1621
01:42:27,760 --> 01:42:32,420
maybe should be free to get so

1622
01:42:32,420 --> 01:42:35,340
and I changed t-minus say that

1623
01:42:35,360 --> 01:42:40,790
now have changed the wisely way to say it is I changed teams

1624
01:42:40,790 --> 01:42:46,790
because the team is always there t 2 t plus a you get this by

1625
01:42:49,050 --> 01:42:53,580
applied t plus to get the right hand side

1626
01:42:54,310 --> 01:43:03,790
I replace this by t plus a and that this into team and that's the

1627
01:43:03,790 --> 01:43:05,570
end of the

1628
01:43:05,580 --> 01:43:09,470
that's a universal rule for doing it OK now I'm going to use that same

1629
01:43:09,470 --> 01:43:13,440
rules for transforming U of T minus a

1630
01:43:13,660 --> 01:43:18,470
I find that the the the the problem is now I function like t square

1631
01:43:18,490 --> 01:43:22,330
assigned t which is written in terms of T minus

1632
01:43:22,380 --> 01:43:23,550
and I don't know what to do with

1633
01:43:23,930 --> 01:43:27,350
the answer is by brute force

1634
01:43:27,360 --> 01:43:30,860
right in terms of T minus what is approved for

1635
01:43:31,110 --> 01:43:34,650
reinforces the following

1636
01:43:35,630 --> 01:43:41,430
and put it to you might say the children T TV minus

1637
01:43:41,840 --> 01:43:48,610
so that is there analyticity by a of it was

1638
01:43:49,110 --> 01:43:54,870
OK and now what's the rule I just following with the same

1639
01:43:54,880 --> 01:43:59,550
what's the sauce for the goose is sauce for the gander minus x

1640
01:43:59,580 --> 01:44:05,510
plus transform of f of you know what I ideas

1641
01:44:05,510 --> 01:44:10,010
where I C a T I change the t plus said OK here I C

1642
01:44:10,010 --> 01:44:10,940
a T

1643
01:44:10,990 --> 01:44:12,580
all change that at t plus a

1644
01:44:13,230 --> 01:44:14,110
well I got

1645
01:44:16,490 --> 01:44:19,400
the plus a minus a

1646
01:44:19,420 --> 01:44:23,750
plus a smoking taking account what is

1647
01:44:24,990 --> 01:44:34,330
16 plus a of the

1648
01:44:42,080 --> 01:44:50,100
by pieces passive understanding OK let's calculate systems of examples and suddenly you breathe a

1649
01:44:50,100 --> 01:44:58,330
sigh of relief that this all is doable anywhere along with what's happening

1650
01:44:58,580 --> 01:45:03,080
but when that covering up any crucial I OK and covering up the things but

1651
01:45:03,090 --> 01:45:04,050
you know that

1652
01:45:04,510 --> 01:45:10,190
a let's see which we calculate for about what I discovered let's calculate will plus

1653
01:45:10,190 --> 01:45:11,110
transform of

1654
01:45:11,700 --> 01:45:16,570
you of the

1655
01:45:16,620 --> 01:45:21,140
OK 2nd well 1st of all right out what it is in terms of the

1656
01:45:21,140 --> 01:45:24,400
unit step function

1657
01:45:24,440 --> 01:45:27,990
and that formula

1658
01:45:28,630 --> 01:45:33,470
yeah you see it now you don't love

1659
01:45:34,110 --> 01:45:36,990
so it's all plus transform is going to be

1660
01:45:37,190 --> 01:45:43,940
well plus transform T minus say that follows that a special case here where the

1661
01:45:43,950 --> 01:45:46,860
function this function is 1

1662
01:45:46,880 --> 01:45:51,200
whatever that was what it doesn't either 1 makes a difference so it simply going

1663
01:45:51,200 --> 01:45:52,310
to be

1664
01:45:52,310 --> 01:46:01,140
will plus transform of what's he would have been would have been which is

1665
01:46:07,890 --> 01:46:15,590
signal plus transform you have to be this 1 that 1 restaurant

1666
01:46:15,600 --> 01:46:17,540
because as the function 1

1667
01:46:17,580 --> 01:46:21,790
and we don't care about the fact that it's zero for negative values of the

1668
01:46:21,800 --> 01:46:27,510
so that's my events and so I multiply it by e to the minus state

1669
01:46:27,510 --> 01:46:30,770
s times 1 over a

1670
01:46:31,920 --> 01:46:36,360
I'm using this formula you the minus AS times will plus transform the unit step

1671
01:46:36,360 --> 01:46:41,600
function which is 1 over and about the translation that was taken care of by

1672
01:46:41,610 --> 01:46:42,940
the exponential

1673
01:46:43,270 --> 01:46:50,310
and minus because this is minus the same thing with be

1674
01:46:50,310 --> 01:46:56,230
so this is will plus transform of the the box

1675
01:46:56,230 --> 01:46:59,480
in the beginning the system doesn't like

1676
01:47:00,520 --> 01:47:01,750
hence me

1677
01:47:01,810 --> 01:47:06,330
it fights me it doesn't like that omega it wants to do something different which

1678
01:47:06,330 --> 01:47:07,650
is part of next

1679
01:47:07,670 --> 01:47:09,270
weak selection

1680
01:47:09,310 --> 01:47:11,690
and you will see that in the beginning

1681
01:47:11,710 --> 01:47:13,920
so we have to be a little patient

1682
01:47:13,940 --> 01:47:15,120
before my real

1683
01:47:15,140 --> 01:47:17,230
so five

1684
01:47:17,290 --> 01:47:18,310
ready for that

1685
01:47:18,330 --> 01:47:20,210
so i'm going to start now

1686
01:47:20,230 --> 01:47:21,600
to drive assistance

1687
01:47:21,620 --> 01:47:24,580
at the frequency which is below resonance

1688
01:47:24,580 --> 01:47:27,390
i want you to see two things that they go hand in hand

1689
01:47:27,440 --> 01:47:28,960
and i want you to see

1690
01:47:29,020 --> 01:47:30,620
that the

1691
01:47:30,640 --> 01:47:31,640
you're going to

1692
01:47:31,650 --> 01:47:34,290
very low frequency

1693
01:47:34,330 --> 01:47:38,370
i'll give you the amplitude this is twice the amplitude of the driver

1694
01:47:38,370 --> 01:47:40,350
now it is here spring

1695
01:47:40,410 --> 01:47:42,100
now spring is here

1696
01:47:42,120 --> 01:47:44,120
so there's only this much is

1697
01:47:44,210 --> 01:47:45,540
to add to zero

1698
01:47:45,580 --> 01:47:48,750
at the zero isn't small no more than three-quarters of an inch

1699
01:47:48,850 --> 01:47:51,060
and now we're going to

1700
01:47:51,080 --> 01:47:53,790
that that's object

1701
01:47:53,810 --> 01:47:55,330
the expos

1702
01:47:55,410 --> 01:47:56,390
to this

1703
01:47:59,170 --> 01:48:01,600
given a little bit of time

1704
01:48:01,670 --> 01:48:05,670
to recognise me

1705
01:48:05,730 --> 01:48:11,000
it takes a little bit of time to reach the steady state solution

1706
01:48:11,020 --> 01:48:12,640
and next time we will

1707
01:48:13,730 --> 01:48:16,540
how much time it actually takes

1708
01:48:16,560 --> 01:48:20,100
so if you want to be a little bit patient

1709
01:48:20,120 --> 01:48:28,420
and you will see

1710
01:48:28,420 --> 01:48:29,460
if we give it

1711
01:48:29,460 --> 01:48:33,520
too much then being too little and then of course it starts to

1712
01:48:33,560 --> 01:48:40,100
get stuck

1713
01:48:40,370 --> 01:48:42,690
are we close

1714
01:48:42,750 --> 01:48:46,000
we're close will be close enough now look at it

1715
01:48:46,000 --> 01:48:48,140
because going both to the left for me

1716
01:48:48,150 --> 01:48:50,230
both right for me

1717
01:48:50,310 --> 01:48:52,940
for you to go in both to right

1718
01:48:52,960 --> 01:48:55,150
you going boasted left

1719
01:48:55,210 --> 01:48:57,310
both the right

1720
01:48:57,350 --> 01:48:58,620
and both to the

1721
01:48:59,060 --> 01:49:00,730
and both to the left

1722
01:49:01,670 --> 01:49:03,890
this was the amplitude

1723
01:49:03,940 --> 01:49:06,150
twice the amplitude of the driver

1724
01:49:06,170 --> 01:49:08,980
may you look carefully you it's less

1725
01:49:09,000 --> 01:49:11,210
it is that the zero to the

1726
01:49:11,230 --> 01:49:12,620
this gentleman

1727
01:49:12,690 --> 01:49:16,440
immediately noticed because we have two strings

1728
01:49:16,440 --> 01:49:17,940
so you see here

1729
01:49:18,020 --> 01:49:20,150
apart from the factor of two

1730
01:49:20,170 --> 01:49:21,580
you see the delta zero

1731
01:49:21,600 --> 01:49:24,250
and you see that the amplitude indeed

1732
01:49:24,250 --> 01:49:25,330
is half of

1733
01:49:25,350 --> 01:49:30,910
the amplitude of the driver because of the two springs now we're going to residents

1734
01:49:30,910 --> 01:49:32,440
omega zero

1735
01:49:34,650 --> 01:49:43,250
nasty things may happen

1736
01:49:43,310 --> 01:49:46,810
you may break

1737
01:49:46,870 --> 01:49:49,250
we have to give it time

1738
01:49:49,310 --> 01:49:52,940
see what's funny things it's doing

1739
01:49:53,020 --> 01:49:54,580
well the study stated

1740
01:49:54,620 --> 01:50:03,850
have to wait

1741
01:50:03,920 --> 01:50:22,190
just a little patience

1742
01:50:22,250 --> 01:50:35,230
with more time

1743
01:50:35,330 --> 01:50:38,790
notice also that the remember this is only moving this much

1744
01:50:38,810 --> 01:50:44,440
newcomers this is moving

1745
01:50:44,480 --> 01:50:47,370
i'm a even be exactly address and we can only

1746
01:50:47,390 --> 01:50:49,000
do the best we can here

1747
01:50:49,020 --> 01:51:01,350
may not be exactly of resonance

1748
01:51:01,460 --> 01:51:05,520
boy close to resonance now all you have

1749
01:51:05,580 --> 01:51:09,560
only look at that too

1750
01:51:09,650 --> 01:51:12,250
my and resonance

1751
01:51:12,250 --> 01:51:13,850
i think i got it you see

1752
01:51:14,330 --> 01:51:16,330
in phase no out of phase

1753
01:51:16,390 --> 01:51:20,980
now we see the ninety degrees and look at this teeny-weeny little displacement here and

1754
01:51:20,980 --> 01:51:23,140
look with this man is doing

1755
01:51:23,150 --> 01:51:26,330
that is resonance

1756
01:51:26,330 --> 01:51:27,750
and this is

1757
01:51:27,760 --> 01:51:32,810
basically what we're going to be talking about and this this lecture

1758
01:51:32,870 --> 01:51:34,290
now in the UK

1759
01:51:34,300 --> 01:51:36,730
well certainly in in london

1760
01:51:39,480 --> 01:51:43,540
we are very proud that the olympic games are coming to the UK in two

1761
01:51:43,540 --> 01:51:45,740
thousand twelve

1762
01:51:46,310 --> 01:51:52,770
the scots are quite pleased as well but i think probably more so but

1763
01:51:52,800 --> 01:51:54,690
so it struck me that

1764
01:51:55,750 --> 01:52:00,110
the book is the the gambling shops will probably be taking

1765
01:52:00,120 --> 01:52:03,680
lots of dates for the

1766
01:52:03,740 --> 01:52:04,980
and the

1767
01:52:05,040 --> 01:52:11,680
the time that wins the gold medal in a hundred metres or in this case

1768
01:52:11,680 --> 01:52:16,720
the distance and the the long jump right so what is the the distance to

1769
01:52:16,740 --> 01:52:19,490
the wins the gold medal in the long jump

1770
01:52:19,490 --> 01:52:23,770
as i said what we do is we try and use some very basic machine

1771
01:52:26,490 --> 01:52:29,450
lamb a trajectory

1772
01:52:29,470 --> 01:52:31,510
of what the

1773
01:52:31,520 --> 01:52:34,370
gold medal distance is going to be in the long jump

1774
01:52:34,380 --> 01:52:36,160
once you've done that

1775
01:52:36,170 --> 01:52:40,790
we can make a prediction and then we can go into some money and bit

1776
01:52:40,790 --> 01:52:43,520
of what distance will be

1777
01:52:45,910 --> 01:52:48,250
what do we do

1778
01:52:48,330 --> 01:52:51,320
we've got some data

1779
01:52:51,360 --> 01:52:53,110
quick chat

1780
01:52:53,120 --> 01:52:55,670
corresponds to the winning distance

1781
01:52:55,680 --> 01:52:57,490
and the year

1782
01:52:57,500 --> 01:53:00,330
the games took place

1783
01:53:00,340 --> 01:53:05,490
now of course there are lots of other attributes

1784
01:53:05,510 --> 01:53:11,320
then we could have the columns form of all the top athletes that potentially maybe

1785
01:53:11,320 --> 01:53:16,620
competing what the weather may well be light can and so forth

1786
01:53:17,490 --> 01:53:22,630
and clearly if we're really going to put any money down and making a bet

1787
01:53:22,630 --> 01:53:25,670
as to what the actual distance would be

1788
01:53:25,740 --> 01:53:29,490
then we would want to use as much information as we possibly could so we

1789
01:53:29,490 --> 01:53:30,790
would we would gather as

1790
01:53:30,800 --> 01:53:36,830
as many attributes as possible as many indicators of what the the likely winning distance

1791
01:53:36,830 --> 01:53:38,250
would be

1792
01:53:38,380 --> 01:53:45,370
but some are just from our the perspective here from from the book perspective what

1793
01:53:45,370 --> 01:53:49,980
we want to do is we'll see what sort of predictions we can make

1794
01:53:50,050 --> 01:53:54,340
if all the we take into account is the amount of time that has elapsed

1795
01:53:54,350 --> 01:53:57,230
from the very first modern olympic games

1796
01:53:57,260 --> 01:54:03,820
so in many ways discounting a lot of valuable information but we'll use time

1797
01:54:03,830 --> 01:54:06,610
our last time as a surrogate

1798
01:54:08,120 --> 01:54:10,220
all of that information

1799
01:54:10,230 --> 01:54:12,580
that potentially could be available

1800
01:54:12,600 --> 01:54:16,550
and also on clutters it doesn't clutter the the

1801
01:54:17,280 --> 01:54:20,970
what i'm going to develop here

1802
01:54:21,010 --> 01:54:24,990
so as one of the speakers said last we can always look at your data

1803
01:54:25,660 --> 01:54:29,590
let's just do that let's look at the data that we have

1804
01:54:29,650 --> 01:54:31,830
andrew plot to be

1805
01:54:31,840 --> 01:54:34,130
gold medal winning distance

1806
01:54:34,140 --> 01:54:39,510
against the time so the amount of time elapsed from the start of the first

1807
01:54:39,510 --> 01:54:43,890
modern games i'm not going to use this laser pointer too much because you can

1808
01:54:43,890 --> 01:54:47,620
see that all shaking hand because

1809
01:54:47,630 --> 01:54:52,690
the video was thinking last night was about stronger than anticipated

1810
01:54:52,740 --> 01:54:57,690
OK so let's let's look at the data and see what information that is so

1811
01:54:57,690 --> 01:55:01,860
the first question is our tribute which is the

1812
01:55:01,900 --> 01:55:08,290
the time elapsed from the first modern games and the winning distance as a functional

1813
01:55:08,290 --> 01:55:12,860
relationship between the

1814
01:55:12,870 --> 01:55:15,160
it the last time

1815
01:55:15,220 --> 01:55:16,250
i predict

1816
01:55:16,260 --> 01:55:19,820
all distance

1817
01:55:19,830 --> 01:55:22,250
someone showed please

1818
01:55:22,270 --> 01:55:23,750
yes and no

1819
01:55:24,720 --> 01:55:28,820
you might have to look at it and you can see there is a relationship

1820
01:55:28,980 --> 01:55:30,970
OK so it

1821
01:55:31,010 --> 01:55:33,940
there's a number of things so those of you who are really keen on all

1822
01:55:33,950 --> 01:55:41,420
the long jump well of course the games uninterrupted during the the two wars and

1823
01:55:41,440 --> 01:55:43,730
you can see this little

1824
01:55:43,740 --> 01:55:49,500
this thing he was of course bow beam in nineteen sixty nine games held in

1825
01:55:51,050 --> 01:55:56,130
OK but anyway there there's certainly a functional relationship they want to make this

1826
01:55:56,140 --> 01:55:59,920
sort of objective visually we can see the something

1827
01:55:59,950 --> 01:56:03,190
so the for a formal perspective what we want to do

1828
01:56:03,420 --> 01:56:07,960
as you want to find a class of functions which will map

1829
01:56:08,000 --> 01:56:11,830
our attributes which in this case i are integers

1830
01:56:11,870 --> 01:56:15,350
want to the real line was electrified onto the

1831
01:56:16,200 --> 01:56:20,150
the positive half of the real line so we're looking for some function

1832
01:56:21,700 --> 01:56:26,140
and given what we observed it would seem sort of reasonable

1833
01:56:26,240 --> 01:56:29,540
that a linear relationship exists

1834
01:56:29,620 --> 01:56:33,340
so we could say that the class of functional that we have is is going

1835
01:56:33,340 --> 01:56:36,450
to be from the linear class which is parameterized

1836
01:56:36,500 --> 01:56:40,950
by an intercept value w not and

1837
01:56:40,970 --> 01:56:43,020
some slope value w one

1838
01:56:43,070 --> 01:56:45,120
so here's a model

1839
01:56:45,160 --> 01:56:46,480
the sort of thing that we

1840
01:56:46,490 --> 01:56:48,370
land in second year

1841
01:56:48,370 --> 01:56:49,730
high school

1842
01:56:49,750 --> 01:56:51,160
linear models

1843
01:56:51,190 --> 01:56:58,590
so the W's other free parameters of our model with this linear functional class

1844
01:56:58,600 --> 01:57:02,370
now what we have to do is we have to identify

1845
01:57:02,380 --> 01:57:04,960
the model parameters

1846
01:57:06,070 --> 01:57:08,410
one way of doing this

1847
01:57:08,440 --> 01:57:12,720
is by considering what's called a loss function and i think someone may be talking

1848
01:57:12,720 --> 01:57:18,530
about more detail and more depth of about loss functions and classes of loss functions

1849
01:57:18,910 --> 01:57:22,010
but in essence what we're going to do is we which is going to define

1850
01:57:22,010 --> 01:57:22,940
the loss function

1851
01:57:23,500 --> 01:57:26,870
which will enumerate miss much

1852
01:57:26,910 --> 01:57:29,620
between what the model predicts

1853
01:57:29,630 --> 01:57:33,550
and the data that we have actually observed which is our target value t

1854
01:57:34,030 --> 01:57:36,490
the distance

1855
01:57:39,870 --> 01:57:41,970
clearly have lost should be

1856
01:57:43,500 --> 01:57:47,370
over all possible observations

1857
01:57:47,460 --> 01:57:52,450
so we would take up an expectation but he was going to take an empirical

1858
01:57:52,600 --> 01:57:55,510
expectation so we define our laws

1859
01:57:55,520 --> 01:57:57,380
for all of the available

1860
01:57:57,440 --> 01:58:02,120
input output on attribute target example peers

