1
00:00:00,000 --> 00:00:05,250
choice trials in which is a certain stimulus was chewed was chosen

2
00:00:05,260 --> 00:00:09,830
and all the forced trials of that same stimulus in aligned one on the other

3
00:00:09,830 --> 00:00:12,300
side of magic responses and they look

4
00:00:12,320 --> 00:00:13,950
this is for that

5
00:00:13,970 --> 00:00:18,630
one hundred percent seventy five percent fifty percent twenty five percent for each one of

6
00:00:18,630 --> 00:00:19,910
them they aligned

7
00:00:19,920 --> 00:00:22,680
very well on each other

8
00:00:22,710 --> 00:00:25,130
so the bottom line here is

9
00:00:25,140 --> 00:00:26,660
it seems like

10
00:00:26,660 --> 00:00:31,670
the prediction errors in different trajectories look like a star so prediction error not like

11
00:00:31,680 --> 00:00:32,750
a critic

12
00:00:34,300 --> 00:00:43,630
prediction error state value prediction urban action value q value prediction error

13
00:00:45,970 --> 00:00:49,130
i think i'm not going to go through this in detail

14
00:00:49,180 --> 00:00:53,240
but just to say that in a different study

15
00:00:53,280 --> 00:00:57,250
there have been only two studies on this the second study for rho should always

16
00:00:57,250 --> 00:00:58,450
in rats and they

17
00:00:58,500 --> 00:01:03,300
they had this somewhat different design where they had always predicting

18
00:01:03,340 --> 00:01:09,410
immediate reward delayed rewards large reward small rewards et cetera and we won't go

19
00:01:09,460 --> 00:01:13,710
through all this now but when they looked at their responses it looked more like

20
00:01:13,750 --> 00:01:16,750
they saw they thought they were c

21
00:01:16,760 --> 00:01:21,920
q learning rather than star so so they were seeing

22
00:01:21,960 --> 00:01:26,070
in of four-stroke trial where the rat had to get

23
00:01:27,750 --> 00:01:29,250
let's look at this level

24
00:01:29,260 --> 00:01:32,740
i there are rewarded with a short delay or with the long delay rewards with

25
00:01:32,750 --> 00:01:37,390
long delays are discounted by time right so they had more fiery called the firing

26
00:01:37,390 --> 00:01:39,830
here this is already the time the reward

27
00:01:39,830 --> 00:01:41,280
more firing four

28
00:01:41,290 --> 00:01:46,080
the short delay rewards it for the long delayed rewards and the same or firing

29
00:01:46,080 --> 00:01:49,330
for high for large reward for small reward

30
00:01:49,390 --> 00:01:52,340
but when they got a choice

31
00:01:52,390 --> 00:01:56,170
in both cases they saw the same fiery whether they are going to choose the

32
00:01:56,170 --> 00:01:58,210
short or the long delay

33
00:01:58,250 --> 00:02:02,410
they first saw the same kind of q value of the best possible option so

34
00:02:02,420 --> 00:02:04,070
this fits this

35
00:02:04,160 --> 00:02:07,760
regardless of whether they are going to choose the best what we're going to choose

36
00:02:07,760 --> 00:02:08,890
the second one

37
00:02:08,910 --> 00:02:11,030
the long delay one

38
00:02:11,050 --> 00:02:12,430
in the same year

39
00:02:17,420 --> 00:02:21,340
so this some of the summary of the story here is that the jury is

40
00:02:21,340 --> 00:02:25,710
still out on starts versus q learning or even science versus q learning versus actor

41
00:02:25,710 --> 00:02:26,710
critic there

42
00:02:26,710 --> 00:02:30,910
are a small handful of studies looking at this directly and what really needs to

43
00:02:30,910 --> 00:02:31,920
be done is

44
00:02:31,970 --> 00:02:37,790
more neuroscientists to do more experiments recording from dopamine neurons especially in

45
00:02:37,820 --> 00:02:43,330
i wrote here telltale tasks tasks that are designed especially for this purpose could really

46
00:02:43,330 --> 00:02:46,180
tease apart these different models

47
00:02:46,210 --> 00:02:48,130
and the reason

48
00:02:48,210 --> 00:02:50,390
we want them to do that is that

49
00:02:50,470 --> 00:02:55,490
as i said this is one area where the brain can for reinforcement learning because

50
00:02:55,490 --> 00:03:00,800
the brain actually learns in task it's much harder than normal reinforcement learning task write

51
00:03:00,800 --> 00:03:04,910
it learns in real time with real noise in real problems and if you look

52
00:03:04,910 --> 00:03:06,870
at these signals sorry

53
00:03:06,960 --> 00:03:08,080
you see there

54
00:03:08,100 --> 00:03:10,080
very very noisy and

55
00:03:10,080 --> 00:03:11,490
kind of you know not

56
00:03:11,500 --> 00:03:15,460
what we expect from a nice prediction error signal but the brain is noisy the

57
00:03:15,460 --> 00:03:16,620
world is noisy

58
00:03:16,670 --> 00:03:22,340
it's true that also are specific measurement devices putting electrodes into the brain don't give

59
00:03:22,340 --> 00:03:25,580
us the nice signal in the world and it could be that the brain itself

60
00:03:25,580 --> 00:03:27,080
see the cleaner signal

61
00:03:27,170 --> 00:03:28,800
that we're seeing here

62
00:03:28,860 --> 00:03:30,070
but in general

63
00:03:30,080 --> 00:03:34,750
since the brain learns so well it would be interesting to know what algorithm it

64
00:03:34,750 --> 00:03:40,030
uses to do that learning and that could maybe help the debate between these different

65
00:03:40,030 --> 00:03:43,200
algorithms in reinforcement learning

66
00:03:48,330 --> 00:03:51,080
they have half an hour left

67
00:03:51,100 --> 00:03:55,000
which means that will probably not get arrested sitiv ity in the brain but will

68
00:03:55,030 --> 00:03:58,360
do these two so that's good

69
00:03:59,010 --> 00:04:02,760
so until now i've talked about three algorithms for model free learning

70
00:04:02,780 --> 00:04:04,420
but one question is

71
00:04:04,470 --> 00:04:09,250
do animals really only learn model free trial and error prediction errors you you you

72
00:04:09,250 --> 00:04:15,030
know run around the world blindly and see what happens or is it or is

73
00:04:15,030 --> 00:04:18,880
there also model based reinforcement learning in the brain

74
00:04:18,930 --> 00:04:21,030
and it turns out that

75
00:04:21,040 --> 00:04:27,710
way before reinforcement learning came into the picture neuroscientists are at that point psychologists had

76
00:04:27,750 --> 00:04:29,750
fierce debates on this

77
00:04:30,880 --> 00:04:32,050
where some

78
00:04:32,070 --> 00:04:38,330
psychologists like thorndike thought that everything was stimulus response see stimulus you learn response to

79
00:04:38,330 --> 00:04:40,860
basically learn a policy for this

80
00:04:40,880 --> 00:04:44,320
model free and some people like torment thoughts about that

81
00:04:44,320 --> 00:04:47,640
rats were much smarter than that

82
00:04:47,680 --> 00:04:50,490
so what experimenter told did

83
00:04:50,530 --> 00:04:51,800
to show this

84
00:04:51,830 --> 00:04:54,390
with this experiment so in this experiment

85
00:04:54,420 --> 00:04:56,330
there are starting from here

86
00:04:56,380 --> 00:04:59,250
i ran into this big arena to find this

87
00:04:59,290 --> 00:05:04,080
the second pathway running down the pathway and got food over here

88
00:05:04,240 --> 00:05:07,380
do this again and again over several days

89
00:05:07,430 --> 00:05:12,120
so the nineteen forty six i don't have a video to show you

90
00:05:12,130 --> 00:05:16,600
and then in the test phase changes situation

91
00:05:16,660 --> 00:05:17,720
this saying

92
00:05:17,780 --> 00:05:22,130
table was now connected to a bunch of different

93
00:05:22,180 --> 00:05:30,280
array of of pathways the main pathway this pathway was blocked

94
00:05:30,290 --> 00:05:34,250
and he asked himself were with the rats go which passed what they choose

95
00:05:34,300 --> 00:05:36,170
and so

96
00:05:36,170 --> 00:05:40,120
you can probably see that this is where the goal is to be before

97
00:05:40,130 --> 00:05:44,630
so the question is will they know to go down pathway number six

98
00:05:44,640 --> 00:05:46,280
and indeed

99
00:05:46,290 --> 00:05:51,790
most of the rats chose pathway six cells and chose one something went right

100
00:05:51,790 --> 00:05:52,970
which is kind of

101
00:05:53,080 --> 00:05:56,620
the policy here the policy closest to the food

102
00:05:56,640 --> 00:05:58,370
but many of the

103
00:05:59,130 --> 00:06:03,030
a completely new policy that they'd never been trained on before

104
00:06:03,040 --> 00:06:05,920
which can which basically showed told

105
00:06:05,970 --> 00:06:07,630
in his words

106
00:06:07,680 --> 00:06:11,620
all his words are humble rat he said even the humble rat can learn spatial

107
00:06:11,620 --> 00:06:15,750
structure and use it to plan flexibly so the right just learned

108
00:06:15,820 --> 00:06:19,130
straight left right right food they learned

109
00:06:19,140 --> 00:06:22,880
something about the world they learned what he called the cognitive map

110
00:06:22,920 --> 00:06:28,640
of the environment which is what we call a transition function between states

111
00:06:29,360 --> 00:06:37,330
model of the world

112
00:06:37,360 --> 00:06:38,720
a more modern way

113
00:06:38,760 --> 00:06:41,050
of getting the same thing

114
00:06:41,080 --> 00:06:45,540
which actually looks at the other half now transitions between states but the reward function

115
00:06:45,540 --> 00:06:50,390
to get a probabilistic representation of these hidden causes and hidden states that all levels

116
00:06:50,820 --> 00:06:51,370
in the model

117
00:06:51,850 --> 00:06:54,710
noting that they are all subject these random fluctuations

118
00:06:55,360 --> 00:06:56,020
both on they

119
00:06:57,740 --> 00:07:00,330
on the emotional the flow of the hidden states here

120
00:07:00,800 --> 00:07:05,100
and the scheme you get from the predictive coding formulation in terms the prediction errors

121
00:07:05,100 --> 00:07:09,910
is very very simple all you're doing is going from sort causes consequences

122
00:07:10,530 --> 00:07:18,690
these bayesian inversion basically maps from consequences sensory consequences to courses replacing these structures these random fluctuations here

123
00:07:19,130 --> 00:07:20,320
with prediction errors

124
00:07:20,980 --> 00:07:24,490
just inverting the direction of these arrows so

125
00:07:24,940 --> 00:07:28,160
you just be inversion scheme has the same architecture

126
00:07:28,560 --> 00:07:29,550
as a generative model

127
00:07:30,360 --> 00:07:31,240
all we're doing now

128
00:07:31,800 --> 00:07:33,580
is basically informing

129
00:07:34,480 --> 00:07:36,320
conditional on the posterior

130
00:07:36,950 --> 00:07:38,870
expectations sale averages

131
00:07:41,270 --> 00:07:45,560
way informing them in terms of by giving them the prediction error

132
00:07:47,270 --> 00:07:48,360
using forward

133
00:07:52,250 --> 00:07:55,450
prediction errors passing into deep into the hierarchy

134
00:07:55,950 --> 00:08:02,730
i found these prediction errors are simply formed by passing predictions down or outputs to the periphery of the hierarchy

135
00:08:03,670 --> 00:08:08,500
i've written down the equations in the previous slide district for those people interested at

136
00:08:08,630 --> 00:08:11,490
intuition is is is quite simple here is just about

137
00:08:12,010 --> 00:08:13,340
the form of they e

138
00:08:13,710 --> 00:08:19,060
and the internal brain states and how they talk to each other should somehow recapitulate

139
00:08:19,270 --> 00:08:23,020
architectural the form of the generative process itself

140
00:08:23,690 --> 00:08:26,100
and once you've got the form it's quite simple to see

141
00:08:27,400 --> 00:08:32,750
one could interpret message passing in the real brain conform to the sort of hierarchical

142
00:08:32,750 --> 00:08:35,780
architecture and i this cartoon that's here

143
00:08:36,450 --> 00:08:37,650
in terms of

144
00:08:38,300 --> 00:08:40,600
different brain systems and different

145
00:08:41,520 --> 00:08:42,310
parts of the

146
00:08:42,870 --> 00:08:45,090
cortico-subcortical of the brain

147
00:08:46,670 --> 00:08:49,550
distinguishing between the representation predictions

148
00:08:50,300 --> 00:08:51,210
prediction errors

149
00:08:51,600 --> 00:08:54,760
which i thought particularly efficient posed by david mumford here too

150
00:08:55,240 --> 00:08:56,290
be encoded by

151
00:08:57,950 --> 00:09:05,990
deep and superficial pyramidal cells because cell population in the brain respectively so the red triangles correspond these prediction errors

152
00:09:06,450 --> 00:09:12,000
and the black black triangles corresponds to these posters predictions are expectations

153
00:09:12,510 --> 00:09:15,660
and you get the same sort of message passing that described in the previous

154
00:09:17,840 --> 00:09:18,900
exactly in the brain

155
00:09:19,750 --> 00:09:20,860
and you get this sort of

156
00:09:21,560 --> 00:09:25,700
for example we have this approach presented there coming from the eye muscles and the

157
00:09:25,710 --> 00:09:27,350
visual information coming from the retina

158
00:09:27,890 --> 00:09:32,570
that's passed up it's compared with top-down predictions to love its prediction error

159
00:09:32,970 --> 00:09:39,510
the prediction errors and sent forward to improve the predictions and this process continues hierarchically

160
00:09:39,510 --> 00:09:46,350
until some higher consensus is achieved whereby pictures minimize all levels in the hierarchy and

161
00:09:46,350 --> 00:09:48,750
you have an increasingly abstract representation

162
00:09:49,280 --> 00:09:51,090
of the causes of your visual

163
00:09:54,850 --> 00:09:56,570
using that you want know

164
00:10:04,730 --> 00:10:09,950
no okay i'll give you examples of that's a very good point and sometimes the

165
00:10:10,040 --> 00:10:15,170
performance process in the real world doesn't correspond to the form of the generative model

166
00:10:15,190 --> 00:10:19,790
and you get some interesting mismatches in the world and what the world out here

167
00:10:19,790 --> 00:10:20,740
but i have about

168
00:10:21,880 --> 00:10:26,500
the model the world is brain in but the real world that you want in presentation

169
00:10:27,060 --> 00:10:27,880
will bring

170
00:10:30,560 --> 00:10:34,800
o yes sorry yes so the model of the world is completely within the brain you

171
00:10:35,550 --> 00:10:40,810
have enormous hierarchical depth and separation temporal scales so for example in in the final

172
00:10:40,830 --> 00:10:43,490
part of the brain that could be structure representations

173
00:10:44,160 --> 00:10:47,090
provide context which could be some minutes hours and days

174
00:10:47,620 --> 00:10:54,130
whereas representations and low levels of the cortex usually condition poster beliefs expectations about hidden

175
00:10:54,130 --> 00:10:59,940
states fluctuate over time scales millisecond of tens or hundreds of milliseconds and all hierarchy

176
00:10:59,940 --> 00:11:01,460
compose and optimize with this

177
00:11:02,610 --> 00:11:04,800
message for parallel message passing

178
00:11:05,610 --> 00:11:06,330
to minimize

179
00:11:06,330 --> 00:11:06,970
talk about is

180
00:11:07,400 --> 00:11:11,100
bayesian discriminative learning and the reason for that is

181
00:11:12,420 --> 00:11:13,690
in machine learning

182
00:11:14,800 --> 00:11:17,600
we have certain terminology in their effects

183
00:11:18,130 --> 00:11:21,630
the way we think about different kinds of models so one particular

184
00:11:22,080 --> 00:11:23,710
kinda terminology is this

185
00:11:25,220 --> 00:11:27,690
generative versus discriminative model

186
00:11:29,230 --> 00:11:30,930
so let's look at classification

187
00:11:31,490 --> 00:11:34,530
when you have some inputs x and some class labels why

188
00:11:35,910 --> 00:11:37,190
the generative model

189
00:11:39,090 --> 00:11:42,470
models so you can think of is the generative process more

190
00:11:42,980 --> 00:11:46,070
objects out there in the world if you are trying to classify

191
00:11:46,710 --> 00:11:48,430
cats versus dogs

192
00:11:49,020 --> 00:11:49,460
what you do

193
00:11:49,920 --> 00:11:53,220
is the state's images of cats versus dogs to be more precise

194
00:11:54,060 --> 00:11:54,820
you have some

195
00:11:56,890 --> 00:11:59,040
prior proportions o

196
00:11:59,420 --> 00:11:59,700
you know

197
00:12:00,210 --> 00:12:02,820
how many cats versus dogs images you expect sea

198
00:12:03,350 --> 00:12:04,160
and then you have

199
00:12:04,730 --> 00:12:06,640
it class conditional density

200
00:12:07,150 --> 00:12:11,570
which gives you for each of your class is a probability distribution over

201
00:12:12,760 --> 00:12:14,400
we observed features of the class

202
00:12:14,950 --> 00:12:16,770
so this will be the probability distribution over

203
00:12:17,630 --> 00:12:23,880
image features assuming your cat another one assuming your dog or turkey or whatever

204
00:12:25,400 --> 00:12:31,200
in discriminative modeling what you do is you try to directly models the conditional distribution

205
00:12:32,420 --> 00:12:37,870
which would be say the probability of the label given the class or it could be building a decision

206
00:12:39,200 --> 00:12:42,350
system that just simply outputs this hard decision

207
00:12:42,760 --> 00:12:43,530
about a label

208
00:12:44,540 --> 00:12:46,590
sorry given me input feature there

209
00:12:52,110 --> 00:12:54,530
it is often the case that's

210
00:12:55,370 --> 00:12:58,830
a lot of machine learning is based on discriminative models

211
00:13:02,150 --> 00:13:09,090
the bayesian bits of machine learning often talking about generative models probability distributions over the whole data

212
00:13:09,770 --> 00:13:11,870
and so it is only natural that this sort of

213
00:13:12,460 --> 00:13:14,450
conception has emerged there

214
00:13:15,290 --> 00:13:16,610
the bayesian

215
00:13:17,340 --> 00:13:22,440
ah frequentist maps onto the generative discriminative divide

216
00:13:22,900 --> 00:13:25,000
and that's not really accurate

217
00:13:25,460 --> 00:13:26,990
it is perfectly possible

218
00:13:27,510 --> 00:13:30,150
to be bayesian about discriminative

219
00:13:32,190 --> 00:13:34,390
similarly expressed perfectly possible

220
00:13:34,900 --> 00:13:35,620
to build

221
00:13:37,850 --> 00:13:42,840
generative models in eighty frequentist way for example you could do something like

222
00:13:43,710 --> 00:13:45,060
kernel density estimation

223
00:13:45,680 --> 00:13:49,730
or feed a mixture model using maximum likelihood or something like that

224
00:13:54,340 --> 00:14:00,570
in particular i think it's very may be useful to look at the example of linear classification this is something

225
00:14:01,390 --> 00:14:03,450
we are all extremely familiar with

226
00:14:03,870 --> 00:14:06,910
so imagine we have a few data points

227
00:14:07,430 --> 00:14:10,870
let's say these two blue data points and this one red data points

228
00:14:11,320 --> 00:14:13,250
and we want to build a linear classifier

229
00:14:14,660 --> 00:14:15,270
and clearly

230
00:14:15,800 --> 00:14:17,950
all these great lines

231
00:14:19,420 --> 00:14:21,550
are consistent with

232
00:14:22,600 --> 00:14:24,710
this data set of three data points

233
00:14:26,130 --> 00:14:26,930
all these

234
00:14:27,500 --> 00:14:28,430
great lines

235
00:14:29,280 --> 00:14:32,350
perfectly separate the blue class from the red class

236
00:14:33,830 --> 00:14:35,690
so how do we choose one of these

237
00:14:36,590 --> 00:14:37,990
to make predictions

238
00:14:39,680 --> 00:14:41,300
well if you could be

239
00:14:41,850 --> 00:14:46,210
let's call it the maximum margin principle for choosing these it would say

240
00:14:47,650 --> 00:14:48,720
choose this

241
00:14:49,450 --> 00:14:53,790
blue line here this blue dashed line that's what the support vector machine will give you

242
00:14:54,250 --> 00:14:57,070
it is a line that maximizes the margin

243
00:14:58,520 --> 00:15:04,050
the distance to points in either of these two classes in this noise free classification problem

244
00:15:06,440 --> 00:15:08,090
now let's contrast there

245
00:15:11,540 --> 00:15:14,310
you know some people have called the bayes point machine

246
00:15:14,980 --> 00:15:16,190
sort of you know used to be

247
00:15:17,730 --> 00:15:22,410
restaurants and things like that's it was all about machines a while ago so

248
00:15:22,970 --> 00:15:24,980
you know the support vector machine kicked out of it

249
00:15:25,380 --> 00:15:26,880
a revolution in naming

250
00:15:27,450 --> 00:15:34,560
love machine learning methods to different kinds machine so the bayes point machine corresponds to this very very simple idea

251
00:15:35,190 --> 00:15:36,000
which is

252
00:15:36,910 --> 00:15:42,320
take a distribution over a linear classifiers for example in this case the uniform distribution

253
00:15:42,720 --> 00:15:43,460
as your prior

254
00:15:46,560 --> 00:15:49,100
so uniform over all possible orientations

255
00:15:50,770 --> 00:15:51,260
and then

256
00:15:53,000 --> 00:15:55,920
conditioned on your noise free data here

257
00:15:57,220 --> 00:15:59,880
your distribution over classifiers looks like this

258
00:16:00,380 --> 00:16:04,540
great thing i've sampled a few hundred points from the distribution

259
00:16:06,180 --> 00:16:08,120
and the bayes point machine just says

260
00:16:09,500 --> 00:16:11,840
find the meaning parameters space

261
00:16:12,370 --> 00:16:15,650
all the posterior distribution over europe classifiers

262
00:16:16,080 --> 00:16:17,860
okay and that's what this red curve gives you

263
00:16:19,190 --> 00:16:19,790
so what is the

264
00:16:20,370 --> 00:16:21,630
what does that do for us

265
00:16:22,130 --> 00:16:22,900
it says

266
00:16:25,230 --> 00:16:27,320
well first of all the answers different from the

267
00:16:28,290 --> 00:16:33,540
maximum margin classifier and the difference comes from the fact that we have this data point

268
00:16:35,220 --> 00:16:36,960
which essentially eliminates

269
00:16:37,380 --> 00:16:42,750
makes the problem is symmetric it eliminates a bunch of classifiers on this side so it tools

270
00:16:43,350 --> 00:16:45,680
the base point classifiers in in the

271
00:16:46,130 --> 00:16:47,010
right direction

272
00:16:48,160 --> 00:16:48,740
if we had

273
00:16:49,610 --> 00:16:55,120
one another data point here i course chosen pathological examples shoyu u

274
00:16:55,520 --> 00:17:00,640
the difference is to exaggerated differences but if i had one extra datapoint here and

275
00:17:00,640 --> 00:17:05,000
one extra datapoint here that's a maximum margin doesn't change at all

276
00:17:05,760 --> 00:17:08,810
but the basic point shifts further in this direction

277
00:17:09,680 --> 00:17:12,660
which is what you get from just averaging over over these

278
00:17:13,370 --> 00:17:14,030
guys here

279
00:17:14,570 --> 00:17:22,280
similarly if i add cluster of points here and the costs are points here not which are actually support vectors

280
00:17:23,080 --> 00:17:29,940
right there are further away from this smooth line these two points but they act in the opposite direction repelling

281
00:17:31,880 --> 00:17:33,180
the classifier in the in

282
00:17:33,980 --> 00:17:34,340
in the

283
00:17:34,860 --> 00:17:35,640
leftward direction

284
00:17:36,840 --> 00:17:38,000
so any questions about that

285
00:17:41,540 --> 00:17:42,690
the termites

286
00:17:44,020 --> 00:17:46,860
the right

287
00:17:47,650 --> 00:17:48,420
the difference between the

288
00:17:52,670 --> 00:17:53,890
right so this is

289
00:17:54,360 --> 00:18:01,190
this is not about hard versus soft classification is not about noise models are what i would call likelihood functions

290
00:18:03,050 --> 00:18:04,060
it's about

291
00:18:04,650 --> 00:18:05,300
the averaging

292
00:18:06,160 --> 00:18:06,780
but bayesian

293
00:18:07,330 --> 00:18:08,910
methods are all about averaging

294
00:18:10,160 --> 00:18:12,870
that's the core idea that the core idea is

295
00:18:13,300 --> 00:18:18,290
given this small dataset i have uncertainty about these classifier

296
00:18:19,240 --> 00:18:22,430
end i need to average the predictions of these classifiers

297
00:18:22,830 --> 00:18:26,030
this is not an exact bayesian solution what we do here

298
00:18:26,530 --> 00:18:30,330
is slightly incorrect which is where averaging in parameter space

299
00:18:30,720 --> 00:18:31,950
the pick any classifier

300
00:18:31,950 --> 00:18:33,610
more formally

301
00:18:33,640 --> 00:18:40,280
we can write the problem as i have on the slide which is minimized the

302
00:18:40,280 --> 00:18:45,660
differential relative entropy between the gas parameters by a not in the guessing parameterized by

303
00:18:45,660 --> 00:18:51,760
a subject to the constraints similarity and dissimilarity constraints and we also require that a

304
00:18:51,760 --> 00:18:53,890
be a positive definite

305
00:18:53,940 --> 00:18:59,140
now if you look at this it's a little bit a little bit nasty so

306
00:18:59,140 --> 00:19:01,110
it's not clear

307
00:19:01,120 --> 00:19:06,230
immediately how you might go about solving it so what we do is we appeal

308
00:19:06,950 --> 00:19:13,460
result which which shows that if we fix the means of two multivariate gaussians we

309
00:19:13,460 --> 00:19:18,400
can actually rewrite the differential relative entropy as

310
00:19:18,410 --> 00:19:23,930
one half times the log determinant divergence between a and and not where the log

311
00:19:23,930 --> 00:19:31,150
determinant divergence is matrix divergences defined at the bottom of the slide

312
00:19:31,160 --> 00:19:38,320
so we can we can just directly map from that the relative entropy formulation of

313
00:19:38,320 --> 00:19:40,380
the KL formulation

314
00:19:40,390 --> 00:19:44,610
two what i call the log determinant formulation

315
00:19:45,590 --> 00:19:49,500
the objective is just rewritten and the constraints

316
00:19:49,510 --> 00:19:55,640
the distance constraints i just turned into standard form so it's clear that there are

317
00:19:55,640 --> 00:19:59,620
linear constraints on a so

318
00:19:59,640 --> 00:20:04,240
so we map it over to this equivalent formulation the log determinant formulation

319
00:20:04,320 --> 00:20:07,150
now we can try to solve this problem

320
00:20:07,210 --> 00:20:15,750
now if you look at what the definition of the log determinant divergence is it's

321
00:20:15,760 --> 00:20:20,350
it looks almost like an SDP has got has this trace term but then there's

322
00:20:20,350 --> 00:20:23,950
this this log that determine actually

323
00:20:25,400 --> 00:20:31,080
that will mean that we can't we can't quite use semidefinite programming

324
00:20:31,090 --> 00:20:34,350
so we need to develop an algorithm for solving this

325
00:20:34,480 --> 00:20:38,600
and i want to go into too much detail but we use the method of

326
00:20:38,780 --> 00:20:42,870
bregman projections which are very

327
00:20:42,890 --> 00:20:45,980
briefly is dual coordinate descent method

328
00:20:46,030 --> 00:20:49,720
essentially what you do is you have on the whole of this matrix at each

329
00:20:50,620 --> 00:20:52,910
and you project

330
00:20:52,950 --> 00:20:59,340
that that the whole of this matrix onto a single constraint features the constraint you

331
00:20:59,340 --> 00:21:04,260
projected and then and then you do some correction and you do this iteratively and

332
00:21:04,260 --> 00:21:08,500
as it turns out that the update this projection can be done

333
00:21:08,510 --> 00:21:11,720
and a very simple way it turns out to be a rank one

334
00:21:11,730 --> 00:21:13,550
update so that

335
00:21:13,570 --> 00:21:18,620
the cost of doing a single projection is ordered the squared

336
00:21:18,640 --> 00:21:20,340
d is the dimensionality

337
00:21:20,420 --> 00:21:22,150
of the data

338
00:21:22,220 --> 00:21:27,250
and briefly some of the nice things about this algorithm first it's it's very scalable

339
00:21:27,250 --> 00:21:32,820
since we have these simple projections and we we don't have to worry about

340
00:21:32,840 --> 00:21:37,560
enforcing the positive definiteness constraint because

341
00:21:37,580 --> 00:21:40,410
the projection does that for us automatically

342
00:21:40,450 --> 00:21:42,470
and there's no

343
00:21:42,480 --> 00:21:48,150
there's no line surgery it's all closed form and no i can vector computation is

344
00:21:48,160 --> 00:21:50,550
very simple

345
00:21:50,570 --> 00:21:55,430
at the beginning of the talk i mentioned

346
00:21:55,870 --> 00:22:01,110
two two nice aspects of the of the of our approach one was this connection

347
00:22:01,110 --> 00:22:06,320
to kernel learning and one was a connection to online metric learning

348
00:22:06,340 --> 00:22:11,440
so i'd like to talk about that now for for a couple of slides

349
00:22:11,450 --> 00:22:15,390
on the left i have the log determinant formulation

350
00:22:15,400 --> 00:22:19,960
i did mention it before but we also can add slack variables two to to

351
00:22:19,960 --> 00:22:24,670
the formulation is to guarantee the existence of a solution which we do

352
00:22:25,050 --> 00:22:30,170
in general so OK so on the left with the log determinant formulation which is

353
00:22:30,740 --> 00:22:34,000
over d by d matrices

354
00:22:35,060 --> 00:22:39,160
it might be the case that the that the dimensionality of the data is very

355
00:22:39,160 --> 00:22:45,310
high in which case maybe it's not feasible to work with a directly

356
00:22:45,450 --> 00:22:49,020
so on the right here is is what i call the kernel formulation which is

357
00:22:49,020 --> 00:22:51,210
over now and by n matrices

358
00:22:51,220 --> 00:22:52,740
where n is the number of

359
00:22:52,760 --> 00:22:58,820
data points in your training set and the constraints are slightly different

360
00:22:58,840 --> 00:23:01,360
but it turns out that if

361
00:23:01,370 --> 00:23:07,940
OK star is the optimal solution here to this kernel problem and it's k stars

362
00:23:07,940 --> 00:23:11,460
the optimal solution to the kernel problem and a stars the optimal solution to the

363
00:23:11,750 --> 00:23:14,790
log determinant problem

364
00:23:14,810 --> 00:23:20,910
then k star equals x transpose a star x where x is your

365
00:23:20,960 --> 00:23:23,330
input data

366
00:23:23,340 --> 00:23:24,760
the input data vectors

367
00:23:25,630 --> 00:23:28,220
so what what this will allow us to do

368
00:23:28,230 --> 00:23:34,280
his work with the kernel formulation when the dimensionality of the data is very high

369
00:23:34,290 --> 00:23:39,750
so so to make them a little bit more clear if if we want to

370
00:23:39,750 --> 00:23:43,110
if we want to learn a metric in kernel space

371
00:23:43,120 --> 00:23:45,710
so say we have some kernel function

372
00:23:45,970 --> 00:23:51,270
phi x transpose y and now we want to learn we want to learn some

373
00:23:51,270 --> 00:23:54,250
DNA of phoenix come you why

374
00:23:54,270 --> 00:23:58,600
now if you expand that just using the definition of the mahalanobis distance

375
00:23:58,610 --> 00:24:01,520
what it's equivalent to

376
00:24:01,570 --> 00:24:04,900
is learning a kernel function of the form

377
00:24:04,910 --> 00:24:07,760
px transpose a few y

378
00:24:08,050 --> 00:24:11,310
so the question is how can we learn given the fact that we can compute

379
00:24:11,360 --> 00:24:16,380
the extras few why how do we compute px transpose a fee one

380
00:24:16,390 --> 00:24:20,090
and if you solve this kernel formulation

381
00:24:20,100 --> 00:24:24,700
you can actually do this and what we show is that the learned kernel

382
00:24:24,720 --> 00:24:30,450
has this form so it's it's the original kernel plus a quadratic

383
00:24:30,470 --> 00:24:32,730
term which is

384
00:24:32,750 --> 00:24:38,040
based on the x i and x j are are are

385
00:24:38,080 --> 00:24:41,630
data points that are part of your constraints

386
00:24:41,640 --> 00:24:50,570
so another another aspect of the algorithm of this of this formulation which is which

387
00:24:50,570 --> 00:24:52,950
is nice is that we can also do

388
00:24:53,420 --> 00:24:54,990
metric learning

389
00:24:55,010 --> 00:24:57,110
in an online manner

390
00:24:57,130 --> 00:24:58,400
we can we can

391
00:24:58,410 --> 00:25:00,990
we can

392
00:25:01,030 --> 00:25:06,310
two variant of this projection method that i talked about a few slides ago

393
00:25:06,310 --> 00:25:10,780
the amount of discount will vary with the action so you have continuous time

394
00:25:10,790 --> 00:25:12,120
the discrete events

395
00:25:13,010 --> 00:25:16,250
events we get to choose actions and interval dependent

396
00:25:17,940 --> 00:25:23,800
so the idea of options is exactly that we're going to start with an MDP

397
00:25:23,810 --> 00:25:26,660
now we can overlay on top

398
00:25:26,710 --> 00:25:29,450
these higher-level actions

399
00:25:29,480 --> 00:25:35,660
that give me variable temporal resolution allows me plan and whatever the right temporal scale

400
00:25:35,710 --> 00:25:37,910
one of the problem is

401
00:25:37,920 --> 00:25:44,240
so you can think of this seminar composite process as overlaid on the underlying primitive

402
00:25:44,240 --> 00:25:47,390
level markov decision process yes

403
00:26:03,770 --> 00:26:07,430
OK good let me let me make sure this point is being it being understood

404
00:26:09,530 --> 00:26:10,490
what's the goal

405
00:26:10,500 --> 00:26:14,450
again the goal is to think of actions much more flexibly than the primitive actions

406
00:26:14,480 --> 00:26:20,330
uniform discrete time muscle twitch simple actions right so what we want we want actions

407
00:26:20,330 --> 00:26:21,980
to be things like

408
00:26:22,180 --> 00:26:23,640
the library

409
00:26:23,660 --> 00:26:24,570
the airport

410
00:26:24,600 --> 00:26:25,360
the door

411
00:26:25,370 --> 00:26:27,630
and so on policies

412
00:26:27,660 --> 00:26:29,560
that can react to

413
00:26:29,610 --> 00:26:34,780
the noisy situation that you're in and yes you have a finite number discrete number

414
00:26:34,790 --> 00:26:36,000
of options

415
00:26:36,070 --> 00:26:38,180
but you want to plan in terms of

416
00:26:38,240 --> 00:26:42,300
right and within it when you choose not been you choose to go to the

417
00:26:43,050 --> 00:26:48,350
then you are doing these all these little action that you're doing underneath right

418
00:26:49,130 --> 00:26:53,380
the point is that you can now thinking reasoning planning learn in terms of these

419
00:26:53,380 --> 00:26:56,160
higher-level things call options

420
00:26:56,170 --> 00:26:58,650
that doesn't answer your question what was question

421
00:26:58,690 --> 00:27:02,530
for example

422
00:27:14,110 --> 00:27:21,750
yes so we have very formal definition of options you write an option is to

423
00:27:21,890 --> 00:27:23,530
stop given

424
00:27:23,530 --> 00:27:28,950
you could start set of initiation sets initiation state you follow a fixed policy

425
00:27:28,960 --> 00:27:31,530
until you tell me go to these conditions

426
00:27:31,670 --> 00:27:36,040
and the point is

427
00:27:36,150 --> 00:27:39,640
if you want to build an option that picture library by the coffee shop you

428
00:27:39,640 --> 00:27:41,780
could do that but you have a policy that

429
00:27:41,800 --> 00:27:45,680
that that competition i'm not sure exactly what you're trying to you trying to say

430
00:27:50,580 --> 00:27:57,150
the core content of

431
00:27:57,160 --> 00:28:03,170
is conditional on its conditional in the sense that

432
00:28:03,180 --> 00:28:07,080
all of the official policy that in some states to actions

433
00:28:07,110 --> 00:28:10,650
now learned policy that maps states of

434
00:28:10,660 --> 00:28:12,540
so the conditional in the sense

435
00:28:12,560 --> 00:28:16,850
after a mapping from states to actions might say i want to

436
00:28:16,870 --> 00:28:19,020
and then execute a particular policy

437
00:28:19,030 --> 00:28:24,520
oh i see what you might say you might encounter and consistency

438
00:28:24,530 --> 00:28:25,800
why would door

439
00:28:25,810 --> 00:28:30,770
like can't copy of you all i want coffee

440
00:28:31,400 --> 00:28:32,960
you see how

441
00:28:32,980 --> 00:28:35,400
so you can you can try that option

442
00:28:35,420 --> 00:28:43,230
and do something different because somehow randomly into a state that make something more yes

443
00:28:43,240 --> 00:28:48,660
but i will make sure the semantics of not is is

444
00:28:48,670 --> 00:28:52,050
make sure that that's the case

445
00:28:52,060 --> 00:28:55,170
OK so let's go

446
00:28:55,180 --> 00:28:55,990
here the

447
00:28:56,000 --> 00:28:56,880
here is the

448
00:28:57,620 --> 00:28:59,270
powerful results

449
00:29:00,380 --> 00:29:02,320
basically it is the

450
00:29:02,350 --> 00:29:10,660
this is probably holes from the problem all is also we have been so far

451
00:29:10,670 --> 00:29:14,420
markov processes are reported or case law

452
00:29:14,440 --> 00:29:22,890
that's what he said well you might want any options this graph shows the office

453
00:29:23,100 --> 00:29:25,260
at each iteration

454
00:29:25,280 --> 00:29:28,730
i mean that in a sense of loss

455
00:29:28,740 --> 00:29:30,100
and as a a result

456
00:29:30,190 --> 00:29:31,710
there are many ways

457
00:29:32,020 --> 00:29:38,650
programme all stand to the case not so let's work through that

458
00:29:38,690 --> 00:29:41,990
let me show you now how all the results have been talking about a markov

459
00:29:41,990 --> 00:29:45,420
decision process is applied to the settings i'm is going to work to some notation

460
00:29:46,000 --> 00:29:49,830
so we're going to learn policies over options that you learn mappings

461
00:29:49,850 --> 00:29:55,440
then assigned to each state distribution or option so in our policies very mapping some

462
00:29:56,240 --> 00:29:57,780
distribution two options

463
00:29:57,830 --> 00:30:01,330
value function is going to be of value functions

464
00:30:01,390 --> 00:30:05,360
four options so this is the value functions for options policy in state s

465
00:30:05,400 --> 00:30:09,630
this is the q value function for policy new

466
00:30:09,640 --> 00:30:11,400
four state option

467
00:30:11,440 --> 00:30:12,660
state and option

468
00:30:12,700 --> 00:30:16,260
the optimal values are defined this way

469
00:30:16,270 --> 00:30:20,600
we can talk about models in this way we talk about planning methods this way

470
00:30:21,030 --> 00:30:26,510
and really all the resources available obtain so let's let's build built towards that so

471
00:30:26,510 --> 00:30:31,000
long as to find the first principal definition of value functions for options

472
00:30:31,000 --> 00:30:32,900
thanks for the introduction

473
00:30:32,960 --> 00:30:38,080
so first before i start i would like to give a very special thanks to

474
00:30:38,860 --> 00:30:41,310
students in both those who have help me

475
00:30:41,320 --> 00:30:45,540
brains slide particular although not always MRI

476
00:30:45,540 --> 00:30:48,570
and the human obozinski

477
00:30:48,570 --> 00:30:52,640
also so what i will try to do today is to give you an overview

478
00:30:52,640 --> 00:30:58,460
of the these hard to pick of sparse methods for machine learning and trying as

479
00:30:58,460 --> 00:31:01,960
much as i can to work to talk about the work of others

480
00:31:01,980 --> 00:31:05,590
but as the and as i said something that is hard to do so and

481
00:31:05,590 --> 00:31:10,090
you always try to snippets some of your own work which we do what was

482
00:31:10,090 --> 00:31:12,120
limited amounts

483
00:31:12,120 --> 00:31:15,510
so i've been doing a lot of reading in in the last two or three

484
00:31:15,510 --> 00:31:19,800
weeks but i may have forgotten someone so if you don't see yourself as

485
00:31:19,880 --> 00:31:21,310
yourself on the slides

486
00:31:21,340 --> 00:31:22,480
don't get mad at me

487
00:31:22,480 --> 00:31:26,270
and it does not mean that you don't do good work on sparse methods

488
00:31:26,280 --> 00:31:32,770
so let me start by giving an overview before i go more into the details

489
00:31:32,780 --> 00:31:37,940
so this is very usual sitting in machine learning at least for supervised learning you

490
00:31:37,940 --> 00:31:44,000
have input data x belonging to some input space capital x

491
00:31:44,030 --> 00:31:48,580
so i'm not used to manipulating this little hands it so

492
00:31:48,640 --> 00:31:52,250
i would try to point towards the screen with an

493
00:31:52,270 --> 00:31:55,560
so this is the capital x being a kind of input space and you want

494
00:31:55,560 --> 00:31:56,450
to predict

495
00:31:56,470 --> 00:31:58,910
a function y value y

496
00:31:58,920 --> 00:32:00,220
and half

497
00:32:00,230 --> 00:32:07,120
machine learning is considering this empirical risk minimisation framework when you minimize the objective function

498
00:32:07,190 --> 00:32:12,420
which will try to so you want to optimize with respect to a function from

499
00:32:12,420 --> 00:32:15,410
x to y in x to predict y

500
00:32:15,450 --> 00:32:20,170
and you have that the fitting term along the points which is the sum of

501
00:32:20,170 --> 00:32:22,330
all the points of the loss

502
00:32:22,340 --> 00:32:26,190
between your true value y i your prediction for x i

503
00:32:26,200 --> 00:32:28,750
and the regularisation term

504
00:32:28,800 --> 00:32:31,970
which is you know or square

505
00:32:31,980 --> 00:32:33,470
but a lot of issues

506
00:32:33,520 --> 00:32:39,310
associated with this framework and the men to issues which loss should you choose for

507
00:32:39,310 --> 00:32:40,720
which type of wine

508
00:32:40,750 --> 00:32:43,990
and this i want to consider these two matches this tutorial

509
00:32:44,000 --> 00:32:48,940
will make use of this is sold other if you use least squares regression if

510
00:32:48,940 --> 00:32:54,180
y i is the only number of users support vector machine logistic if you use

511
00:32:54,380 --> 00:32:56,190
why i being

512
00:32:56,220 --> 00:32:57,530
the discrete

513
00:32:57,530 --> 00:33:00,850
just can value the number

514
00:33:00,870 --> 00:33:04,630
so we mainly focus on this and the choice of the function space and the

515
00:33:05,410 --> 00:33:08,090
now we consider the various types of non

516
00:33:08,100 --> 00:33:08,810
and here

517
00:33:08,840 --> 00:33:13,380
you have to the goal of adding an norm of course is to avoid overfitting

518
00:33:13,560 --> 00:33:16,810
and now you have two main lines of work is in this setting we have

519
00:33:16,810 --> 00:33:21,320
two camps first came in the one of on so bash norms

520
00:33:21,340 --> 00:33:26,030
and this is essentially the camp of kernel methods

521
00:33:26,090 --> 00:33:28,650
and the good thing is it it allows you to do

522
00:33:28,690 --> 00:33:33,630
not enough data and also the theory at least as far as i'm concerned is

523
00:33:33,630 --> 00:33:37,280
well developed and you have a lot of algorithms which are available for this type

524
00:33:37,280 --> 00:33:40,250
of of regularized

525
00:33:40,260 --> 00:33:43,370
the second camp the camp of sparsity

526
00:33:43,620 --> 00:33:48,560
in using norms so here are usually as a start is restricted to only about

527
00:33:48,650 --> 00:33:50,710
the task you want to predict

528
00:33:50,750 --> 00:33:55,240
from x as a linear function of x and of course the main example is

529
00:33:55,240 --> 00:33:57,940
the one on which is just the sun

530
00:33:57,960 --> 00:34:01,760
of the absolute value so here's the cool thing in that it does two things

531
00:34:01,760 --> 00:34:02,970
at the same time

532
00:34:02,990 --> 00:34:10,350
it was both to avoid overfitting and also create something goes into URI estimator estimator

533
00:34:10,370 --> 00:34:11,320
for w

534
00:34:11,380 --> 00:34:16,030
so it does both model selection and regularisation and here

535
00:34:16,090 --> 00:34:18,010
this is a matter because the tutorial

536
00:34:18,030 --> 00:34:23,030
the theory the theoretical part and i don't make work is in progress and this

537
00:34:23,030 --> 00:34:24,600
is the topic of this tutorial

538
00:34:24,620 --> 00:34:26,820
so the first aspect of recovery

539
00:34:26,840 --> 00:34:31,870
if the aspect of algorithms so which one which one is fast it and two

540
00:34:33,220 --> 00:34:34,960
so here this is

541
00:34:35,000 --> 00:34:39,810
picture taken from the statistics and kindly given to me by stefan can

542
00:34:39,870 --> 00:34:42,880
this is the gas here again the laplace and of those

543
00:34:42,910 --> 00:34:45,500
so gauss in favour and two

544
00:34:45,540 --> 00:34:48,730
in the lab class in favour of of one

545
00:34:48,750 --> 00:34:53,320
and since one is not differentiable would see that it might look it is hard

546
00:34:53,320 --> 00:34:56,060
to optimize this is the president of toys

547
00:34:56,120 --> 00:35:02,570
and since and two usually it's too nice smooth convex optimisation problems where if you

548
00:35:02,570 --> 00:35:06,960
would square loss to get just enough system is supposed to be easier this is

549
00:35:07,010 --> 00:35:10,110
goals in here and of course we which

550
00:35:10,120 --> 00:35:14,700
which one wins this is not always OK and then we will see

551
00:35:14,750 --> 00:35:17,770
so first is nice for two things because laplace french

552
00:35:17,800 --> 00:35:23,740
and even better lab class professor article value so you see that it's going to

553
00:35:23,740 --> 00:35:26,630
be important to pick school

554
00:35:26,690 --> 00:35:31,560
and this will be achieved by essentially first order methods with which we will see

555
00:35:31,730 --> 00:35:32,640
the same

556
00:35:33,150 --> 00:35:34,510
for anyone have two

557
00:35:34,520 --> 00:35:39,650
and also i think also to be methods which can use the fact that you're

558
00:35:39,670 --> 00:35:41,380
going to get a sparse solution

559
00:35:41,900 --> 00:35:45,990
get that into the details we get into the details of the

560
00:35:46,000 --> 00:35:50,020
then we got a bit of the article was is associated with with this type

561
00:35:50,020 --> 00:35:51,690
of regularisation

562
00:35:51,710 --> 00:35:55,500
and then we mainly focus on two types of results the first result to be

563
00:35:56,430 --> 00:36:01,450
support recovery and this is in this setting things that we was kind of late

564
00:36:01,500 --> 00:36:05,420
with respect to to applications people have been using one of four

565
00:36:06,750 --> 00:36:11,650
and the problem is that it does two things it will prevent overfitting and select

566
00:36:11,670 --> 00:36:12,650
good zero

567
00:36:12,700 --> 00:36:17,750
and it has been only recently that people have to analyse how which zeros are

568
00:36:17,750 --> 00:36:23,550
we actually getting do we get the correct and whatever show you that you get

569
00:36:23,560 --> 00:36:31,130
this is a sufficient condition for the exact support recovery and it involves the covariance

570
00:36:31,130 --> 00:36:32,420
matrix of the covariance

571
00:36:34,120 --> 00:36:39,800
and essentially if you remove the this parts in says that to get the correct

572
00:36:40,600 --> 00:36:44,310
you q to have low correlation between variables

573
00:36:44,330 --> 00:36:46,950
we see that this is both the positive result

574
00:36:46,960 --> 00:36:49,830
in would tell you when you don't get a good support

575
00:36:49,880 --> 00:36:52,750
both so to me very negative one in the sense that

576
00:36:52,770 --> 00:36:55,000
c to data often call eighty

577
00:36:55,010 --> 00:36:57,600
this shows that we never get good support

578
00:36:57,640 --> 00:37:03,990
then would get to the second aspect which is the high dimensional inference where people

579
00:37:04,390 --> 00:37:07,130
try to derive results we say that

580
00:37:07,180 --> 00:37:10,920
you can still predict well if the number of features p

581
00:37:10,930 --> 00:37:17,760
it is at most exponential in the number of observations throughout this tutorial people the

582
00:37:17,760 --> 00:37:18,940
number of features

583
00:37:18,950 --> 00:37:22,440
and be the number of observations

584
00:37:22,460 --> 00:37:27,070
then we go around trying to go beyond that so because i

585
00:37:27,120 --> 00:37:32,120
that's the only thing that apply in the last directly is almost never useful

586
00:37:32,140 --> 00:37:36,990
in a sense that you well you only have a few lineup problems in machine

587
00:37:36,990 --> 00:37:38,810
learning and you really want to be

588
00:37:38,860 --> 00:37:44,760
but to go beyond that and i will consider first nonlinearities through adding kernels

589
00:37:45,330 --> 00:37:51,300
then trying to see if you can actually do is exponentially many features can you

590
00:37:51,330 --> 00:37:54,420
really or an algorithm where the peak was an

591
00:37:54,420 --> 00:37:56,670
so the definition of convexity

592
00:37:56,690 --> 00:38:01,010
is exactly that statement but where n equals two

593
00:38:01,040 --> 00:38:04,480
alpha one alpha two alpha and beta

594
00:38:04,480 --> 00:38:07,900
this is just the statement for general and

595
00:38:08,030 --> 00:38:15,280
and you can interpret this in some

596
00:38:15,300 --> 00:38:16,860
year way which i will

597
00:38:16,880 --> 00:38:18,320
and into

598
00:38:18,420 --> 00:38:21,170
sure when i j

599
00:38:21,880 --> 00:38:25,260
this is saying take several points on this curve

600
00:38:25,280 --> 00:38:27,130
take the polygon they

601
00:38:29,030 --> 00:38:33,150
these are straight line segments you take the interior

602
00:38:33,170 --> 00:38:36,360
if you take an affine combination like that you will get point

603
00:38:36,380 --> 00:38:39,150
inside the polygon possibly on the boundary

604
00:38:39,170 --> 00:38:42,170
the claim is that all those points are above the curve

605
00:38:42,230 --> 00:38:46,980
again intuitively true from if you draw nice canonical convex

606
00:38:48,190 --> 00:38:49,980
but in fact it's true

607
00:38:49,990 --> 00:38:52,460
algebraically to always a good thing

608
00:38:52,510 --> 00:38:58,820
any suggestions how we might prove this theorem this is this lemma pretty easy

609
00:38:58,860 --> 00:39:01,070
so what technique might we use

610
00:39:08,150 --> 00:39:10,380
one more

611
00:39:10,400 --> 00:39:12,840
induction always a good answer

612
00:39:14,980 --> 00:39:16,420
induction should out

613
00:39:16,420 --> 00:39:20,840
you here because we already know that this is true by definition of convexity for

614
00:39:20,840 --> 00:39:21,900
n equals two

615
00:39:21,900 --> 00:39:23,820
the base case is clear

616
00:39:23,840 --> 00:39:26,780
in fact there an even simpler base case which is when

617
00:39:26,800 --> 00:39:29,190
because one

618
00:39:29,190 --> 00:39:30,730
if n equals one

619
00:39:31,510 --> 00:39:36,090
you have one number that sums to one so alpha one and so one and

620
00:39:36,840 --> 00:39:39,530
nothing's going on here this is just saying

621
00:39:40,690 --> 00:39:42,150
as of

622
00:39:42,170 --> 00:39:45,210
one times x one

623
00:39:45,210 --> 00:39:46,750
is it most

624
00:39:46,800 --> 00:39:50,530
one times fx one so not

625
00:39:50,550 --> 00:39:52,340
really exciting

626
00:39:52,360 --> 00:39:55,300
those that hold with equality

627
00:39:55,420 --> 00:39:59,650
so we don't even need any close to this case

628
00:39:59,670 --> 00:40:04,940
so the interesting part of the still not terribly interesting is the induction step

629
00:40:04,980 --> 00:40:06,480
this is good

630
00:40:06,530 --> 00:40:08,980
in practice and induction

631
00:40:08,980 --> 00:40:10,760
so what we care about

632
00:40:10,760 --> 00:40:13,670
is this the sum

633
00:40:13,760 --> 00:40:21,030
this linear combination of this linear combination f one combination xk xk some over k

634
00:40:21,030 --> 00:40:22,860
now what i would like to do

635
00:40:22,900 --> 00:40:25,820
his apply induction what i know about inductively

636
00:40:25,840 --> 00:40:30,530
it's a of this some if it's some only up to n minus one instead

637
00:40:30,530 --> 00:40:31,920
of all the way up to

638
00:40:31,980 --> 00:40:35,300
any smaller some i can deal with by induction

639
00:40:35,320 --> 00:40:39,630
so i'm going to try and get rid of the terminal well separated out

640
00:40:39,690 --> 00:40:41,360
and this is

641
00:40:41,450 --> 00:40:43,210
only natural

642
00:40:44,150 --> 00:40:46,550
if you play with my combinations before

643
00:40:46,630 --> 00:40:48,730
was just some algebra

644
00:40:53,480 --> 00:40:55,960
so i want to separate out the output and x

645
00:40:55,980 --> 00:40:57,900
and turn

646
00:40:59,590 --> 00:41:03,730
i'd also like to make an apple in combination this is the trick

647
00:41:03,900 --> 00:41:08,760
if you

648
00:41:08,800 --> 00:41:20,530
if i just removed the last term the alpha case from one of n minus

649
00:41:20,530 --> 00:41:25,170
one wouldn't sum to one anymore some something smaller so i can't just take out

650
00:41:25,170 --> 00:41:27,150
this term i'm going to have to do some

651
00:41:27,170 --> 00:41:29,840
trickery here

652
00:41:39,550 --> 00:41:43,320
you should see why this is true because the one myself and cancel

653
00:41:43,340 --> 00:41:46,320
and then i'm just going to some alpha kxk

654
00:41:46,320 --> 00:41:48,110
OK equal one to n minus one

655
00:41:48,130 --> 00:41:52,030
plus the alpha an eccentric i haven't done anything here these are

656
00:41:53,340 --> 00:41:53,980
but now

657
00:41:54,010 --> 00:41:59,210
this nifty feature that on the one hand these two numbers alpha one myself and

658
00:41:59,210 --> 00:42:00,610
sum to one

659
00:42:00,650 --> 00:42:04,340
and on the other hand if i didn't write these numbers should sum up to

660
00:42:05,440 --> 00:42:09,190
just going from one up to k one of two n minus one

661
00:42:09,210 --> 00:42:10,800
one of the sum of two

662
00:42:10,820 --> 00:42:16,010
one well these numbers summed up to one minus alpha

663
00:42:16,070 --> 00:42:19,570
and so on dividing everything by one minus and so they will sum to one

664
00:42:19,590 --> 00:42:22,360
so now i have to have one combinations

665
00:42:22,360 --> 00:42:24,490
i just apply the two things that i know

666
00:42:24,550 --> 00:42:25,820
i know this

667
00:42:25,820 --> 00:42:27,860
the sampling combination will work

668
00:42:31,530 --> 00:42:36,050
why can i say this is alpha and xn

669
00:42:37,590 --> 00:42:39,320
one myself

670
00:42:39,340 --> 00:42:43,150
of this crazy

671
00:43:00,170 --> 00:43:04,440
two possible answers one is correct in one is incorrect

672
00:43:04,460 --> 00:43:13,800
so which would be

673
00:43:23,170 --> 00:43:39,110
on the board to consider

674
00:43:39,110 --> 00:43:43,420
so i'm treating this is just one big x value

675
00:43:43,420 --> 00:43:47,190
i have some x and i have some

676
00:43:49,280 --> 00:43:53,610
one after the fly combination of those two x values is at most the combinations

677
00:43:53,610 --> 00:43:55,150
of the those

678
00:43:55,170 --> 00:43:58,550
x values

679
00:43:58,570 --> 00:43:59,980
this is

680
00:43:59,980 --> 00:44:04,830
by which i mean there

681
00:44:17,000 --> 00:44:23,630
and here's some pseudocode for this is actually slightly different from the book

682
00:44:23,640 --> 00:44:28,110
the book has one in fact there's a nice problem in the book that has

683
00:44:28,130 --> 00:44:29,860
even a different one

684
00:44:29,870 --> 00:44:31,290
OK so

685
00:44:31,340 --> 00:44:34,410
but there are all the basically the same

686
00:44:35,380 --> 00:44:38,810
patricia AP q

687
00:44:40,660 --> 00:44:48,370
we're looking at at the depth of the recursion is the from p q

688
00:44:48,380 --> 00:44:52,360
basically we pick up pivot which were not just because the first element of the

689
00:44:52,360 --> 00:44:53,790
array a page

690
00:45:00,310 --> 00:45:05,130
and book justin for your information uses a few

691
00:45:05,210 --> 00:45:06,550
i use a p

692
00:45:06,570 --> 00:45:08,330
doesn't really matter

693
00:45:09,550 --> 00:45:11,960
and then we set index

694
00:45:12,010 --> 00:45:13,010
to pay

695
00:45:13,030 --> 00:45:15,120
now we have a little

696
00:46:02,720 --> 00:46:14,030
OK so this is the code so basically a

697
00:46:14,090 --> 00:46:19,640
the structure is a for loop with an if statement in the middle

698
00:46:24,660 --> 00:46:25,740
so the

699
00:46:25,760 --> 00:46:30,130
the structure of the algorithm best partitioning step

700
00:46:30,190 --> 00:46:35,040
it looks as far as we set the pivot

701
00:46:35,080 --> 00:46:37,990
to be the first element so here's p

702
00:46:38,010 --> 00:46:40,640
here's q

703
00:46:40,660 --> 00:46:43,100
OK so this is going to be are invariant

704
00:46:50,880 --> 00:46:54,790
any time during the execution of loop

705
00:46:54,800 --> 00:46:56,630
i essentially have

706
00:46:56,840 --> 00:47:01,240
some values up to i which already less equal to x

707
00:47:01,250 --> 00:47:03,630
and then some values

708
00:47:03,650 --> 00:47:06,380
that and the j minus one

709
00:47:06,390 --> 00:47:09,160
that are greater than equal to x

710
00:47:09,300 --> 00:47:13,190
and then i don't know about the rest

711
00:47:13,200 --> 00:47:18,200
so we start out with i equal to p

712
00:47:18,220 --> 00:47:20,970
and j equal to p plus one

713
00:47:20,980 --> 00:47:23,170
that said he plus one

714
00:47:23,180 --> 00:47:26,440
OK so that everything is unknown except for

715
00:47:26,450 --> 00:47:28,050
x here

716
00:47:32,990 --> 00:47:36,720
the idea is that is going to preserve the invariant and the way it does

717
00:47:36,720 --> 00:47:41,120
it is as we go through the loop is it looks at a GA and

718
00:47:41,120 --> 00:47:43,990
it says is the greater than or equal to x

719
00:47:44,010 --> 00:47:46,200
so is less than or equal to x

720
00:47:46,250 --> 00:47:50,460
OK if it's greater than or equal to x it does nothing

721
00:47:50,470 --> 00:47:54,420
OK because what can happen if this is great and called axon essentially just go

722
00:47:54,420 --> 00:47:57,690
to the next iteration loop which moved this boundary

723
00:47:57,730 --> 00:48:02,320
on the invariant satisfied the receiver

724
00:48:04,240 --> 00:48:07,700
but it is less or equal to some kind of problem

725
00:48:07,710 --> 00:48:11,020
if i want to maintain the invariant this next element is less than or equal

726
00:48:11,020 --> 00:48:12,470
to x

727
00:48:12,480 --> 00:48:15,300
so what it does then is it says i'll

728
00:48:15,320 --> 00:48:17,830
let me just move this boundary

729
00:48:17,850 --> 00:48:21,660
and swap this element here which is greater than equal to x with this one

730
00:48:21,660 --> 00:48:24,920
here this less than or equal to x

731
00:48:24,940 --> 00:48:26,140
thereby by

732
00:48:26,190 --> 00:48:28,100
increasing the size of this

733
00:48:28,110 --> 00:48:32,060
subarray and then the invariant satisfied again

734
00:48:32,800 --> 00:48:34,550
so it's fairly simple

735
00:48:35,760 --> 00:48:39,970
it's actually very tight and easy algorithm you so that that's one reason that this

736
00:48:39,970 --> 00:48:42,310
is such a great piece of code

737
00:48:42,890 --> 00:48:46,290
is very efficient

738
00:48:47,020 --> 00:48:53,340
now in principle also this basically the running time for this on n elements

739
00:48:53,420 --> 00:48:55,890
is or and

740
00:49:04,300 --> 00:49:09,370
and basically just going through the n elements and is doing cost amount of work

741
00:49:09,390 --> 00:49:12,860
and then just the cost amount of work outside

742
00:49:12,880 --> 00:49:16,880
so this is a clever piece of code in fact in principle partition is easy

743
00:49:17,700 --> 00:49:21,770
OK thank you were worrying about doing it in place it's really pretty easy thing

744
00:49:21,770 --> 00:49:23,370
to do i take an element

745
00:49:23,410 --> 00:49:26,110
just compare every other element with that

746
00:49:26,120 --> 00:49:28,650
from one to one then one into the other

747
00:49:28,690 --> 00:49:30,870
it's clearly linear time

748
00:49:30,930 --> 00:49:32,690
OK but off and

749
00:49:32,730 --> 00:49:37,420
what you find is that just because you can do it that way theoretically doesn't

750
00:49:37,420 --> 00:49:39,870
mean that that's going to end up giving you

751
00:49:39,920 --> 00:49:43,460
good code and this is a nice piece of code allows you to do it

752
00:49:43,460 --> 00:49:45,110
in place

753
00:49:45,120 --> 00:49:48,990
OK and that's one reason why this is a particularly good album is the constants

754
00:49:48,990 --> 00:49:50,640
are good

755
00:49:50,660 --> 00:49:54,840
OK so yes it when we do asymptotic analysis we tend to ignore the constants

756
00:49:55,180 --> 00:49:59,740
for actually building codes you care about the constants

757
00:50:00,570 --> 00:50:06,370
but first you care much more than just about the constants is whether overall it's

758
00:50:06,370 --> 00:50:09,330
going to be a fast algorithm

759
00:50:09,340 --> 00:50:12,200
OK let's go through an example of this

760
00:50:12,220 --> 00:50:14,040
so there over here

761
00:50:14,050 --> 00:50:17,660
just so we get to get the gist

762
00:50:17,760 --> 00:50:20,820
so here's a

763
00:50:22,470 --> 00:50:23,990
re that i've

764
00:50:24,080 --> 00:50:26,790
created out of whole cloth

765
00:50:31,980 --> 00:50:34,980
here we set x the pivot to be

766
00:50:35,100 --> 00:50:36,320
to be

767
00:50:37,230 --> 00:50:44,010
OK so let's look to see how this algorithm works

768
00:50:44,020 --> 00:50:46,590
so i starts out here

769
00:50:46,640 --> 00:50:51,480
and j starts out here

770
00:50:51,570 --> 00:50:52,670
OK we

771
00:50:56,790 --> 00:50:59,680
what we do is we start scanning right

772
00:50:59,690 --> 00:51:03,500
century that code scanning right until it gets something which is less than or equal

773
00:51:03,500 --> 00:51:09,190
to the pivot so keep going until it finds jake heaps incrementing until you find

774
00:51:09,200 --> 00:51:12,890
something that's less than or equal to the pivot and in that case it's

775
00:51:12,900 --> 00:51:14,150
number five

776
00:51:14,160 --> 00:51:19,580
and it says well swap these two things

777
00:51:19,600 --> 00:51:21,520
does that

778
00:51:21,530 --> 00:51:23,270
so we get six

779
00:51:23,280 --> 00:51:26,370
o five thirteen ten

780
00:51:27,950 --> 00:51:30,430
two eleven

781
00:51:30,440 --> 00:51:33,750
OK and meanwhile now i gets incrementally

782
00:51:33,920 --> 00:51:36,780
j continues where it left off

783
00:51:38,190 --> 00:51:41,640
so now we keep scanning right until we get something that's less than or equal

784
00:51:41,640 --> 00:51:42,630
to the pivot

785
00:51:42,680 --> 00:51:44,530
in this case it's three

786
00:51:44,610 --> 00:51:47,550
was one three and five

787
00:51:47,590 --> 00:51:48,450
we get

788
00:51:48,460 --> 00:51:51,320
sixty three

789
00:51:51,430 --> 00:51:58,720
and now

790
00:51:58,800 --> 00:52:03,890
this step we we increment i we increment starchy out here

791
00:52:03,900 --> 00:52:05,970
in this case right off the bat

792
00:52:05,990 --> 00:52:07,070
we have something

793
00:52:07,080 --> 00:52:08,410
which is

794
00:52:08,450 --> 00:52:13,800
which is less than or equal to x is we swap these two

795
00:52:14,640 --> 00:52:15,990
were nine

796
00:52:16,010 --> 00:52:18,120
groups what i do

797
00:52:18,130 --> 00:52:20,430
that's what the wrong thing to here

798
00:52:20,440 --> 00:52:27,240
it's why i'm not a computer

799
00:52:27,240 --> 00:52:29,040
guys what you would

800
00:52:29,080 --> 00:52:31,410
we have all these guys essentially

801
00:52:31,430 --> 00:52:34,620
it means that all this weight are very very close to zero

802
00:52:34,640 --> 00:52:36,320
on this one here

803
00:52:36,320 --> 00:52:39,390
basically dominate so very very close to one

804
00:52:39,420 --> 00:52:44,580
so essentially what it means that you approximating highdimensional probability distribution

805
00:52:45,330 --> 00:52:50,950
by simple that that you like mass because essentially one of the particle as all

806
00:52:50,950 --> 00:52:54,360
the the weight very close to one on all the others are very close to

807
00:52:54,360 --> 00:52:56,840
zero OK so that doesn't work

808
00:52:56,850 --> 00:53:00,650
so that's why it's important sampling of basically

809
00:53:02,960 --> 00:53:07,420
it's importance sampling on a notable something doesn't scale in i mentioned so what could

810
00:53:07,420 --> 00:53:08,740
i do

811
00:53:08,750 --> 00:53:10,730
two kind of try to

812
00:53:10,740 --> 00:53:13,040
minimize essentially

813
00:53:13,060 --> 00:53:14,540
the visionary c

814
00:53:14,550 --> 00:53:16,550
let me take the degeneracy

815
00:53:16,600 --> 00:53:20,850
of the importance sampling technique so i want to come up with a method is

816
00:53:20,850 --> 00:53:26,730
going to try to reduce the violence of this topic alice teammates so all well

817
00:53:26,750 --> 00:53:31,440
you basically what i've presented here are only degree of freedom i have currently to

818
00:53:31,440 --> 00:53:36,880
deal with this problem too limited the violence and most may consist of these conditional

819
00:53:38,700 --> 00:53:42,370
it is going to show the solution is the only degree of freedom the algorithm

820
00:53:42,380 --> 00:53:43,480
OK so

821
00:53:43,490 --> 00:53:45,560
is there way so long

822
00:53:45,580 --> 00:53:46,640
to pick it

823
00:53:46,670 --> 00:53:48,560
so as assume essentially

824
00:53:48,560 --> 00:53:52,620
meanwhile the violence of my important something is to

825
00:53:52,650 --> 00:53:54,140
OK so that's

826
00:53:54,180 --> 00:53:59,010
so the question i tried to team essentially the only degree of freedom i have

827
00:53:59,050 --> 00:54:01,920
in the important something

828
00:54:02,670 --> 00:54:04,880
so let's look basically

829
00:54:04,890 --> 00:54:09,090
basically to pick to find out what's essentially the best thing i could do

830
00:54:09,090 --> 00:54:10,730
entails selecting the

831
00:54:10,740 --> 00:54:15,010
contain part of the solution to any time well let's come back to the expression

832
00:54:15,010 --> 00:54:17,400
of the weight

833
00:54:17,440 --> 00:54:18,650
importance weights

834
00:54:18,670 --> 00:54:22,710
at time and starting partners with time and i tell you what it is essentially

835
00:54:23,170 --> 00:54:25,050
well this is the

836
00:54:25,060 --> 00:54:27,310
normalized target distribution

837
00:54:27,320 --> 00:54:33,750
divided essentially by the importance distribution which is remember structure that's full of OK

838
00:54:33,760 --> 00:54:35,650
by construction

839
00:54:35,810 --> 00:54:39,280
so nice to play a little bit that this expression

840
00:54:39,300 --> 00:54:45,410
uses include components components waiting time is what they basically you

841
00:54:45,430 --> 00:54:46,910
this down here

842
00:54:46,930 --> 00:54:48,890
which is essentially a function

843
00:54:48,900 --> 00:54:53,060
of the n minus one feels component x one x two x minus one

844
00:54:53,070 --> 00:54:54,760
time basically

845
00:54:54,780 --> 00:54:59,250
this term which is the conditional distribution of xn given all remaining company and the

846
00:54:59,460 --> 00:55:03,230
current target distribution divided by basically

847
00:55:03,250 --> 00:55:05,490
the importance distribution at time

848
00:55:06,740 --> 00:55:12,400
when it comes time you've already essentially sample the company x one x two xn

849
00:55:12,400 --> 00:55:14,620
minus one so the only thing to do

850
00:55:14,640 --> 00:55:19,240
you can do is try to minimize the violence of these guys with respect to

851
00:55:19,240 --> 00:55:21,380
q and all of this is the best thing to do

852
00:55:21,400 --> 00:55:25,840
so as to minimize the violence of this guy respect to qn is to pick

853
00:55:25,880 --> 00:55:30,740
basically two and a quarter pi and so that it is very simple reason

854
00:55:30,790 --> 00:55:32,640
there's nothing really can do

855
00:55:32,670 --> 00:55:37,390
yes so that's basically what this is somewhat the kind of locally

856
00:55:37,750 --> 00:55:42,530
o two more in city at time and if i want to minimize the violence

857
00:55:42,530 --> 00:55:47,800
and the importance weight okay then and only then can do essentially is ph

858
00:55:48,940 --> 00:55:50,450
in part of the solution

859
00:55:50,460 --> 00:55:56,150
as the conditional distribution of x then under the target distribution in which case basically

860
00:55:56,150 --> 00:56:02,200
what you observe is that the resulting importance it is actually independent of the current

861
00:56:02,200 --> 00:56:03,550
sample of it

862
00:56:03,700 --> 00:56:05,480
that's a bit you will have

863
00:56:05,580 --> 00:56:06,860
OK so

864
00:56:06,880 --> 00:56:11,100
that makes sense this thing is a kind of intuitive so let's come back to

865
00:56:11,100 --> 00:56:15,830
the example where you're dealing with the case of the state space model so that

866
00:56:15,830 --> 00:56:17,960
essentially the joint

867
00:56:17,980 --> 00:56:23,410
target distribution you're trying to sample from is the posterior distribution of the infield state

868
00:56:23,410 --> 00:56:29,100
of the state space model given the first salvation where in this case basically

869
00:56:29,160 --> 00:56:30,490
because of the

870
00:56:30,500 --> 00:56:35,210
the should have been making all state space model that is the underlying underlying essentially

871
00:56:35,210 --> 00:56:42,600
state processes markov process on the observation processes such that the observation are conditionally independent

872
00:56:42,840 --> 00:56:49,020
given the states process of physically acceleration density g one given x and then you

873
00:56:49,020 --> 00:56:50,250
can easily show

874
00:56:50,330 --> 00:56:55,810
that in these very specific case the are similar some are locally optimal importance distribution

875
00:56:55,810 --> 00:57:01,410
you can use is the posterior distribution of xn given the cold acceleration of the

876
00:57:01,410 --> 00:57:05,420
previous state stages sample x n minus one so some are you can think of

877
00:57:05,420 --> 00:57:10,580
it this is was basically proportional to the point where the state space with time

878
00:57:10,580 --> 00:57:16,100
because the likelihood of the observed nationwide so some kind of optimal tradeoff between the

879
00:57:16,100 --> 00:57:20,090
dynamic of the state space model on the salvation equation

880
00:57:21,590 --> 00:57:25,430
so you can play with that so essentially if you have to use this kind

881
00:57:26,700 --> 00:57:32,070
it's like importance sampling type algorithm to do estimation in state space models this the

882
00:57:32,070 --> 00:57:36,090
kind of persona the the kind of sampling distribution you should try to

883
00:57:36,100 --> 00:57:40,310
to some poor form so as to compute sequentially to state

884
00:57:41,350 --> 00:57:46,740
in chased where basically your own algorithm to solve all five

885
00:57:46,750 --> 00:57:52,410
these conditional distribution because my not be the case that you can sample exactly from

886
00:57:52,410 --> 00:57:53,300
this guy

887
00:57:53,340 --> 00:57:56,030
o even you might be able to solve all

888
00:57:56,040 --> 00:58:01,390
from this conditional distribution using cell rejection sampling but your own ball two so important

889
00:58:01,430 --> 00:58:06,920
to compute the associated importance weights because essentially it involved into this tale of x

890
00:58:06,940 --> 00:58:14,060
and then never mind basically what you can do is essentially a footpath basically

891
00:58:14,070 --> 00:58:16,780
come with the analytic approximation

892
00:58:16,800 --> 00:58:22,500
of these distributions on proposed use sample from it's OK

893
00:58:22,510 --> 00:58:27,370
so you could use a extended kalman filter unscented chemistry to any like violation without

894
00:58:27,370 --> 00:58:33,580
techniques all basically you will cry for the discrepancy anyway between the target distribution on

895
00:58:33,600 --> 00:58:36,570
the proposal density using importance sampling OK so

896
00:58:36,660 --> 00:58:37,920
the second big

897
00:58:37,940 --> 00:58:42,800
advantage of basically kind of of the calorimeter you can reuse or your factory it's

898
00:58:42,800 --> 00:58:50,500
less almost deterministic approximation technique also used in the context so second possibly as an

899
00:58:50,500 --> 00:58:55,610
important distribution to propose some fourteen one of and part of the space on you

900
00:58:55,620 --> 00:58:56,980
correct for

901
00:58:57,020 --> 00:59:04,410
the discrepancy by simply reweighting use important according to the all of the components which

902
00:59:04,680 --> 00:59:08,260
that's kind of the nice thing about this kind of the techniques

903
00:59:08,910 --> 00:59:10,500
so you can do that

904
00:59:10,500 --> 00:59:13,600
OK you can do all those things OK so for example

905
00:59:13,630 --> 00:59:18,650
assume that you're going to do inference in state space models

906
00:59:18,650 --> 00:59:24,880
corollary of the definition the negative border is the set of many for of the

907
00:59:24,880 --> 00:59:28,840
complement of the sets in the positive more

908
00:59:32,980 --> 00:59:36,850
we call these

909
00:59:36,880 --> 00:59:40,750
well i already said action

910
00:59:40,770 --> 00:59:46,600
we can use this in a so-called willis and an the sort person computing some

911
00:59:46,600 --> 00:59:50,800
maximum frequency using randomized algorithm going up

912
00:59:50,890 --> 00:59:51,810
that is

913
00:59:51,820 --> 00:59:56,020
then compute the minimum frequency very find them against database

914
00:59:56,040 --> 00:59:58,370
and continued this until no more

915
00:59:58,390 --> 01:00:02,800
users are all and this actually at least in some

916
01:00:02,810 --> 01:00:08,370
five days of this works pretty well that we get them that's representation of

917
01:00:08,430 --> 01:00:12,350
the collection of all frequent items

918
01:00:12,360 --> 01:00:15,520
how important that in the end is

919
01:00:15,560 --> 01:00:20,310
i don't really know

920
01:00:20,330 --> 01:00:20,890
OK o

921
01:00:33,490 --> 01:00:38,550
OK yet another more very small theoretical results

922
01:00:38,560 --> 01:00:41,100
i suppose

923
01:00:41,110 --> 01:00:45,950
somebody now in the back row which is misbehaving badly today

924
01:00:45,960 --> 01:00:50,890
the back somebody in the back row is is telling me that OK yeah

925
01:00:50,910 --> 01:00:55,300
this is exactly the set of frequent

926
01:00:55,300 --> 01:00:58,050
set in

927
01:00:58,080 --> 01:00:59,430
in the data

928
01:00:59,450 --> 01:01:01,800
how many questions do i have

929
01:01:01,810 --> 01:01:03,920
to verify

930
01:01:06,630 --> 01:01:09,740
this claim

931
01:01:09,760 --> 01:01:13,490
actually it's pretty easy to so that you need to to exactly

932
01:01:13,500 --> 01:01:15,480
the things in the water

933
01:01:15,500 --> 01:01:23,110
both in the negative and in in the positive more

934
01:01:23,130 --> 01:01:25,690
so it

935
01:01:25,710 --> 01:01:28,370
again in our example a

936
01:01:28,380 --> 01:01:33,790
somebody claims that this is exactly the collection of sequence that we have to cheque

937
01:01:33,960 --> 01:01:35,760
exactly the same

938
01:01:35,800 --> 01:01:40,760
these are frequent and this unknown when it goes on for all you don't have

939
01:01:40,760 --> 01:01:45,310
to anything else

940
01:01:45,330 --> 01:01:48,430
alright that was just a glimpse into

941
01:01:48,450 --> 01:01:53,930
it certain theoretical aspects related to to the borders of on

942
01:01:54,000 --> 01:02:00,440
frequent collections of frequent patterns now i moved to the next part

943
01:02:00,450 --> 01:02:01,590
which is

944
01:02:01,650 --> 01:02:06,150
basically about inclusion exclusion principle and maximum entropy

945
01:02:06,170 --> 01:02:07,880
and the goal here

946
01:02:11,340 --> 01:02:14,610
find out how we can use the frequency

947
01:02:14,640 --> 01:02:17,880
to infer something about the underlying distribution

948
01:02:17,890 --> 01:02:21,850
so early on i said that there is smoke modelling and global modelling

949
01:02:21,870 --> 01:02:24,130
and i'm talking about local modelling

950
01:02:24,140 --> 01:02:27,570
that's what fans still talking about but now i'm thinking about how

951
01:02:27,640 --> 01:02:33,820
information about frequent sets how we could do use that to tell something of the

952
01:02:33,820 --> 01:02:36,050
whole distribution

953
01:02:37,350 --> 01:02:42,440
to recap what we know about treatments that can be an exponential number of them

954
01:02:42,440 --> 01:02:44,950
into the in the number of

955
01:02:46,100 --> 01:02:51,210
they are down supposed called collection under some subset relation we can find them many

956
01:02:52,610 --> 01:02:57,510
in the number of innovations variables algorithm is very simple

957
01:02:57,510 --> 01:03:02,890
and also finding frequently true positive conjunctions

958
01:03:02,920 --> 01:03:08,010
so now the basic question is what do these frequent set was about the underlying

959
01:03:09,120 --> 01:03:10,920
so we are given

960
01:03:10,930 --> 01:03:14,090
a data set of n rows and m columns

961
01:03:14,130 --> 01:03:16,350
and the distribution is

962
01:03:17,600 --> 01:03:21,760
the reason overall you don't one vectors of length n

963
01:03:22,650 --> 01:03:27,370
so the basic question is OK we know these things frequencies which we happen to

964
01:03:27,370 --> 01:03:29,600
be able to compute quite quickly

965
01:03:29,650 --> 01:03:34,230
what do they tell us about the underlying distribution

966
01:03:36,400 --> 01:03:41,200
simple observation is that the case we wouldn't know the zero frequency sets

967
01:03:41,250 --> 01:03:44,610
so basically the frequency of all conjunctions

968
01:03:44,670 --> 01:03:46,000
then we will know

969
01:03:48,710 --> 01:03:51,430
you know everything about the data

970
01:03:51,680 --> 01:03:55,030
just a very simple example suppose

971
01:03:55,060 --> 01:03:56,180
we would have

972
01:03:56,200 --> 01:03:57,930
two variables and you

973
01:03:57,980 --> 01:04:01,330
and if we have a frequency of a so

974
01:04:01,340 --> 01:04:03,660
how many times we have a one

975
01:04:03,700 --> 01:04:05,330
the frequency of blue

976
01:04:05,330 --> 01:04:09,400
the frequency of eighty which is the frequency of the conjunction was one of the

977
01:04:09,400 --> 01:04:10,600
because one

978
01:04:10,670 --> 01:04:13,320
then we can put express

979
01:04:14,200 --> 01:04:17,840
the probability of each variable combinations

980
01:04:17,860 --> 01:04:18,570
as they a

981
01:04:19,020 --> 01:04:23,130
some combination of all these terms

982
01:04:23,190 --> 01:04:28,480
one one of the user well that's a new one zero and a mindset a

983
01:04:28,490 --> 01:04:29,750
b and so on

984
01:04:29,790 --> 01:04:34,560
so if we would not zero frequency says we will know the whole distribution

985
01:04:35,330 --> 01:04:36,890
OK that's completely

986
01:04:37,100 --> 01:04:41,510
in a way that's useless observation because you're all frequencies will be an exponential number

987
01:04:42,440 --> 01:04:49,630
so that's basically saying that the just the description of the distribution

988
01:04:49,640 --> 01:04:54,840
what is a little bit more interesting is suppose we would like to

989
01:04:56,160 --> 01:05:00,910
find out that say the frequency of the d stands

990
01:05:00,920 --> 01:05:02,440
so how many

991
01:05:02,490 --> 01:05:08,000
rose satisfy at least one of the variables a high

992
01:05:10,590 --> 01:05:12,090
as mistakes are made

993
01:05:12,110 --> 01:05:15,150
the the the set x year

994
01:05:15,160 --> 01:05:18,360
continuous variables AI goes from one

995
01:05:18,950 --> 01:05:22,640
we want to find out how many arrows

996
01:05:22,650 --> 01:05:28,090
in the data have at least one of the variables they are equal to one

997
01:05:28,100 --> 01:05:32,920
previously we had been talking about content conjunction now we try to estimate the frequency

998
01:05:32,920 --> 01:05:36,840
of roles satisfying this time

999
01:05:36,840 --> 01:05:41,520
and the idea that what one wants to do is find

1000
01:05:41,540 --> 01:05:47,250
eight rank something approximation to the data and the optimal solutions in the case of

1001
01:05:47,250 --> 01:05:50,260
gaussian noise is given by the singular value decomposition

1002
01:05:50,640 --> 01:05:57,240
but one could think probably more general distributions and in the case that the noise

1003
01:05:57,240 --> 01:06:00,240
is any member of the exponential family

1004
01:06:00,350 --> 01:06:04,610
finding the basis for the subspace

1005
01:06:04,620 --> 01:06:11,040
it's actually an optimisation problem and rather than being purely problems before it's actually nonlinear

1006
01:06:11,040 --> 01:06:17,340
optimization problems but fortunately it's been shown recently that the problem is

1007
01:06:18,280 --> 01:06:20,660
and so it admits a unique global

1008
01:06:21,640 --> 01:06:28,770
and therefore one can just run any optimisation algorithms will converge to the optimal

1009
01:06:28,790 --> 01:06:35,500
so this this extension of PCA is is being known as probabilistic PCA

1010
01:06:35,700 --> 01:06:41,200
but there have been other type of extensions of PCA to different domains in one

1011
01:06:41,200 --> 01:06:46,410
of them has to do with well what happens if this space rather than just

1012
01:06:46,410 --> 01:06:50,850
being a linear subspace is instead a nonlinear manifold

1013
01:06:50,860 --> 01:06:55,380
in that case the tradition with being to think well

1014
01:06:55,400 --> 01:06:58,820
maybe i can just take my data points

1015
01:06:59,670 --> 01:07:02,790
it certain nonlinear and betting

1016
01:07:02,810 --> 01:07:05,240
and after applying the embedding

1017
01:07:05,260 --> 01:07:07,760
the data that we will live in a subspace

1018
01:07:07,780 --> 01:07:12,310
and so i could just apply PCA in the high dimensional space

1019
01:07:12,330 --> 01:07:15,010
and therefore get my set of principal components

1020
01:07:15,080 --> 01:07:19,850
of course that is not very efficient because these high dimensional space can be very

1021
01:07:20,410 --> 01:07:24,500
unfortunately there is a beautiful theory developed by

1022
01:07:24,510 --> 01:07:27,980
including alex smola which is present here

1023
01:07:28,000 --> 01:07:30,490
call kernel PCA

1024
01:07:30,510 --> 01:07:36,120
that allows you to do the computation of these principal components just by looking at

1025
01:07:36,130 --> 01:07:40,750
both products of the original data rather than looking at the actually

1026
01:07:40,760 --> 01:07:42,530
the embedding of the day

1027
01:07:42,570 --> 01:07:44,900
the problem is well

1028
01:07:44,910 --> 01:07:48,320
if i have no idea what the manifold looks like

1029
01:07:48,370 --> 01:07:52,790
then really i have no idea what embedding actually use in these

1030
01:07:53,690 --> 01:07:58,800
the idea of the embedding at least finite dimensional spaces

1031
01:07:58,810 --> 01:08:03,030
well what inventing should use if the manifold is of type a maybe i could

1032
01:08:03,030 --> 01:08:07,430
use embedding a and so in the end is just a model assumption i think

1033
01:08:07,430 --> 01:08:11,820
my data and i believe that this embedding will map it into a linear subspace

1034
01:08:12,200 --> 01:08:14,080
and not be true

1035
01:08:14,130 --> 01:08:17,100
the the embedding i choose may not be the correct one

1036
01:08:18,280 --> 01:08:23,950
learning what type of embedding to use is is an active problem today in the

1037
01:08:23,950 --> 01:08:26,290
machine learning community

1038
01:08:26,300 --> 01:08:32,480
and the problem is well what happens if rather than having a very small and

1039
01:08:32,480 --> 01:08:37,350
a single manifold now i have a mixture of manifold and in this particular case

1040
01:08:37,350 --> 01:08:39,860
is a mixture of subspaces

1041
01:08:39,880 --> 01:08:45,160
of course is no longer a smooth manifold there is no singularity particle the intersection

1042
01:08:45,180 --> 01:08:52,780
in this case the difficulty is that i not know which points belong to which

1043
01:08:54,380 --> 01:08:55,340
and so

1044
01:08:55,360 --> 01:09:00,190
it's not a problem of estimating some model for my data that is just the

1045
01:09:00,210 --> 01:09:07,090
smooth manifold but also is the problem of segmentation because i need to simultaneously estimate

1046
01:09:07,190 --> 01:09:12,350
basis for the subspaces as well as which point belongs to which subspaces

1047
01:09:12,390 --> 01:09:18,670
and this can be solved within a probabilistic framework using techniques such as the expectation

1048
01:09:18,670 --> 01:09:23,730
maximisation algorithm and in nineteen nine tipping and bishop

1049
01:09:23,750 --> 01:09:30,170
presented a solution to this problem based on expectation maximisation

1050
01:09:30,220 --> 01:09:34,680
so this is precisely the problem would be talking today but rather than taking a

1051
01:09:34,680 --> 01:09:39,450
probabilistic approach it will be a purely algebraic approach

1052
01:09:41,050 --> 01:09:46,180
this type of techniques or how to solve this problem is actually fundamental to many

1053
01:09:47,300 --> 01:09:50,170
and in particular in computer vision

1054
01:09:50,180 --> 01:09:55,050
it can be applied to the problem of computing vanishing points this is in a

1055
01:09:55,050 --> 01:09:58,800
sense if you have a set of parallel lines in an image

1056
01:10:01,120 --> 01:10:05,820
parallel three d space the projection is not parallel so they actually converge and they

1057
01:10:05,820 --> 01:10:09,330
intersect a point that point is called the vanishing point

1058
01:10:09,350 --> 01:10:13,430
and in this case for instance one might be interested in clustering which lines are

1059
01:10:13,430 --> 01:10:14,720
parallel to each other

1060
01:10:14,830 --> 01:10:19,620
by precisely finding which lines intersect in which vanishing point

1061
01:10:19,640 --> 01:10:23,570
that can be cast as the problem of clustering

1062
01:10:23,620 --> 01:10:25,060
better living in

1063
01:10:25,080 --> 01:10:27,660
place in history

1064
01:10:27,670 --> 01:10:31,560
compression one of the most popular techniques for compression

1065
01:10:31,570 --> 01:10:34,000
is to find basis

1066
01:10:34,110 --> 01:10:38,710
describe the image that is exactly the principal component analysis problems

1067
01:10:38,800 --> 01:10:44,130
and one could think of maybe getting richer description of a major even better compression

1068
01:10:44,130 --> 01:10:48,090
by using multiple principal components of possibly slower dimensions

1069
01:10:48,100 --> 01:10:52,080
segmentation i'll show examples on how to apply

1070
01:10:52,170 --> 01:10:57,730
these techniques to segmentation of images based on intensity or segmentation of images based on

1071
01:10:59,570 --> 01:11:04,450
i'll spend the entire second lecture talking about how to do motion segmentation based on

1072
01:11:04,450 --> 01:11:09,390
these type techniques where one has a camera that's possibly moving and an object moving

1073
01:11:09,820 --> 01:11:12,610
and how it is possible to take that

1074
01:11:12,630 --> 01:11:18,270
the sequence and converted to problem of how the car is moving one subspace and

1075
01:11:18,270 --> 01:11:23,580
the camera is moving in another subspace and so it is to be seeing april

1076
01:11:23,600 --> 01:11:28,780
talk about the type segmentation problems here there is somebody who is speaking

1077
01:11:28,900 --> 01:11:33,620
it is host and the guest and they're both of them is impossible to separate

1078
01:11:33,620 --> 01:11:39,080
these video sequence into three groups mtime each corresponding to each one of the shots

1079
01:11:39,080 --> 01:11:41,460
in this video sequence

1080
01:11:41,500 --> 01:11:47,060
just briefly mention one example and face recognition

1081
01:11:47,070 --> 01:11:54,590
essentially the key underlying idea is the basis of one single person under different illumination

1082
01:11:54,590 --> 01:11:58,000
conditions in a subspace

1083
01:11:58,020 --> 01:11:58,800
and so

1084
01:11:58,810 --> 01:12:04,030
u one could attempt to cluster different faces by clustering subspace

1085
01:12:04,100 --> 01:12:09,710
i will not talk about this but their ideas on

1086
01:12:10,910 --> 01:12:14,930
given a video sequence of the person that's walking and imagine that i could track

1087
01:12:14,940 --> 01:12:16,330
the joints of the person

1088
01:12:16,350 --> 01:12:22,000
then the trajectories that the joint of the person described as a function of time

1089
01:12:22,010 --> 01:12:24,350
live in some subspace

1090
01:12:24,370 --> 01:12:26,250
when the person is walking

1091
01:12:26,270 --> 01:12:30,820
and even different subspaces when the person is running and so one could attempt to

1092
01:12:30,820 --> 01:12:34,880
do recognition of activities by clustering subspaces

1093
01:12:35,640 --> 01:12:37,940
in this case apply to human data

1094
01:12:37,960 --> 01:12:43,830
in an absolutely similar fashion there is very recent

1095
01:12:43,840 --> 01:12:45,850
in computer vision to model

1096
01:12:45,860 --> 01:12:51,330
complicated dynamics in video sequences and these are called dynamic textures

1097
01:12:51,380 --> 01:12:54,830
in which one has to understand preserve water

1098
01:12:54,840 --> 01:12:57,290
or you can notice but the

1099
01:12:57,300 --> 01:13:01,630
the big this mostly in the middle is actually a toilet flushing

1100
01:13:01,650 --> 01:13:07,540
and both of them can be considered a different types of dynamic textures so in

1101
01:13:07,540 --> 01:13:12,490
this case one would be interested in segmenting the video sequence into different regions each

1102
01:13:12,500 --> 01:13:16,730
region corresponding to a different dynamic textures

1103
01:13:16,750 --> 01:13:23,290
and we have been recently working on applications on this in biomedical imaging

1104
01:13:23,300 --> 01:13:27,080
these i don't know how well you can see i can see very well on

1105
01:13:27,080 --> 01:13:29,880
my screen but i guess you can hardly see

1106
01:13:29,900 --> 01:13:31,860
that's an image of the heart

1107
01:13:31,870 --> 01:13:35,270
so what you can see

1108
01:13:35,620 --> 01:13:39,620
in here is the chest of the person

1109
01:13:39,630 --> 01:13:43,550
and here you have the heart beating

1110
01:13:43,560 --> 01:13:47,670
and what we're trying to do is essentially

1111
01:13:47,680 --> 01:13:48,590
the text

1112
01:13:48,640 --> 01:13:54,800
different type of arrhythmias by looking at the evolution of the heart as a function

1113
01:13:54,800 --> 01:14:00,260
of time and of course the test is just going to moving only according to

1114
01:14:00,260 --> 01:14:05,380
the breeding but the heart is moving complexlly according to a certain dynamic texture so

1115
01:14:05,380 --> 01:14:08,880
by applying essentially the same techniques for separating water

1116
01:14:08,890 --> 01:14:10,080
first small

1117
01:14:10,100 --> 01:14:15,330
one cannot separate the heart versus the rest and then by analyzing how they move

1118
01:14:15,340 --> 01:14:17,110
the term in certain

1119
01:14:17,120 --> 01:14:19,540
i read his

1120
01:14:19,590 --> 01:14:23,870
then i will talk about this editor but for those of you

1121
01:14:23,870 --> 01:14:27,360
finished around this

1122
01:14:27,380 --> 01:14:30,660
learning algorithms

1123
01:14:30,680 --> 01:14:32,600
you mention this perceptron

1124
01:14:34,080 --> 01:14:36,350
which i said that

1125
01:14:36,370 --> 01:14:37,780
quite simple

1126
01:14:37,790 --> 01:14:42,170
on one side but on the other side is very efficient

1127
01:14:42,180 --> 01:14:44,060
and then we saw these them all

1128
01:14:44,070 --> 01:14:46,400
support vector machines

1129
01:14:50,390 --> 01:14:55,350
going to the results of this learning curve maybe just briefly mention

1130
01:14:55,370 --> 01:14:57,760
so part of every scientific

1131
01:14:57,810 --> 01:15:04,730
work or even commercial work is also the revelation and also in text mining and

1132
01:15:04,730 --> 01:15:11,690
related fields we have a couple of very standard evaluation measures like probably you had

1133
01:15:11,690 --> 01:15:18,440
already for this name so precision recall accuracy and f f measure so they would

1134
01:15:18,440 --> 01:15:21,110
normally appearing

1135
01:15:21,130 --> 01:15:28,030
of scientific papers and instead of going out to the formula maybe just the intuition

1136
01:15:28,030 --> 01:15:31,290
behind this

1137
01:15:31,300 --> 01:15:34,910
formulas so precision is basically

1138
01:15:34,950 --> 01:15:42,540
measuring the truth while recall is measuring the whole truth so

1139
01:15:42,590 --> 01:15:47,070
michael mccord whole truth the hotels and physically

1140
01:15:47,090 --> 01:15:50,440
precision recall means how much the through so how much

1141
01:15:50,450 --> 01:15:53,310
of everything what's out there through

1142
01:15:54,100 --> 01:15:57,480
right recover accuracy

1143
01:15:57,500 --> 01:15:58,580
it's very simple

1144
01:15:58,590 --> 01:16:00,020
measure so

1145
01:16:00,300 --> 01:16:08,870
just the proportion of right stuff versus everything as it's not used often and f

1146
01:16:08,870 --> 01:16:13,980
measure is just a combination due general geometric mean between through some the whole truth

1147
01:16:14,010 --> 01:16:16,940
between precision and recall so

1148
01:16:17,030 --> 01:16:21,920
we one go here too much into the details but these are four standard measures

1149
01:16:21,920 --> 01:16:25,660
and the higher they are the better they are

1150
01:16:27,530 --> 01:16:36,370
OK not not just think you're more words about the learning so

1151
01:16:36,380 --> 01:16:41,730
very standard datasets of india evaluating

1152
01:16:41,740 --> 01:16:49,060
machine learning algorithms on taxes is this so-called writers dataset we are categorizing that documents

1153
01:16:49,720 --> 01:16:57,490
set of hundred twenty that categories and so these are some of these categories or

1154
01:16:57,630 --> 01:17:02,900
acquire corn and so on and these are just so we have two datasets one

1155
01:17:02,950 --> 01:17:08,380
something over twenty thousand documents beak and the other one is a kind of thirty

1156
01:17:08,380 --> 01:17:17,240
thousand documents and business leads document has zero one or several of these labels and

1157
01:17:17,240 --> 01:17:19,680
machine learning task is to

1158
01:17:19,700 --> 01:17:21,350
make model which

1159
01:17:21,370 --> 01:17:26,830
science this labels back the documents

1160
01:17:26,850 --> 01:17:30,830
so that was the message here so here i want to show

1161
01:17:30,850 --> 01:17:37,760
three algorithms which we mention so support vector machine perceptron small algorithm which showed before

1162
01:17:37,760 --> 01:17:43,660
and we know which is equally small and very efficient algorithm which is used also

1163
01:17:43,660 --> 01:17:48,070
by many companies section instead of SVM where we try to

1164
01:17:48,080 --> 01:17:50,510
categorize text

1165
01:17:50,530 --> 01:17:53,240
on this small writers domain

1166
01:17:54,410 --> 01:17:55,480
here we see

1167
01:17:56,040 --> 01:18:02,620
so SVM perceptron and winnow and here we use different representations of the document so

1168
01:18:03,030 --> 01:18:08,520
the message here is seated between SVM is the most advanced methods and let's say

1169
01:18:08,550 --> 01:18:13,440
perceptron which is just a small town lines of code is very small and we

1170
01:18:13,440 --> 01:18:18,490
know is a little bit less if you apply

1171
01:18:18,500 --> 01:18:24,280
a couple of very small improvements on the perceptron and know then they practically equal

1172
01:18:24,290 --> 01:18:30,320
and this is this is what is the message for machine learners and they all

1173
01:18:30,320 --> 01:18:33,090
use linear so SVM

1174
01:18:33,200 --> 01:18:36,280
this linear linear kernel

1175
01:18:37,090 --> 01:18:39,770
a rather simple scenario which is

1176
01:18:39,850 --> 01:18:42,550
as i said quite efficient

1177
01:18:42,570 --> 01:18:49,030
in other categorisation task which is similar to the one we chose just showed this

1178
01:18:49,290 --> 01:18:51,520
categorization IN tool

1179
01:18:51,530 --> 01:18:52,970
in two

1180
01:18:52,990 --> 01:18:56,890
subject higher here we don't have just the

1181
01:18:57,510 --> 01:19:00,940
least of categories into which we would like to see

1182
01:19:00,950 --> 01:19:05,280
categorized document but we have a large hierarchy of

1183
01:19:05,330 --> 01:19:09,310
categories and so i will

1184
01:19:09,330 --> 01:19:13,550
actually i will make them all here is keep a couple of slides

1185
01:19:14,670 --> 01:19:15,590
so if

1186
01:19:21,150 --> 01:19:26,860
you might be aware of this the most higher here the moss is the largest

1187
01:19:27,190 --> 01:19:30,950
higher he on the that which was made more less in these two point zero

1188
01:19:30,950 --> 01:19:38,500
point zero partial but it's all it has over six hundred thousand categories so this

1189
01:19:38,500 --> 01:19:40,260
is the three

1190
01:19:40,280 --> 01:19:46,820
which has six hundred thousand categories top-level you have something like twenty categories but then

1191
01:19:46,860 --> 01:19:54,990
when you go into these categories but if you go into a science

1192
01:19:55,010 --> 01:19:59,630
then we would get another new categories and if get to agriculture then this will

1193
01:19:59,630 --> 01:20:01,410
be further on so on

1194
01:20:01,430 --> 01:20:06,620
OK so what's important is that we have six hundred thousand categories not just like

1195
01:20:06,790 --> 01:20:10,980
hundred twenty years before and fuel

1196
01:20:10,990 --> 01:20:16,950
million websites which are mainly categorized into this six hundred thousand competitors now what we

1197
01:20:16,950 --> 01:20:18,190
would like to do

1198
01:20:18,210 --> 01:20:20,360
here is to build a classifier

1199
01:20:20,370 --> 01:20:24,470
which with the new which would take taken new document and classified

1200
01:20:24,480 --> 01:20:26,540
into the right spot to this

1201
01:20:26,560 --> 01:20:28,010
make huge three

1202
01:20:30,860 --> 01:20:32,390
and basically

1203
01:20:32,410 --> 01:20:34,620
we did this we use just

1204
01:20:35,150 --> 01:20:37,100
the nearest neighbour nothing special

1205
01:20:37,560 --> 01:20:43,040
and it works quite well and i will demonstrate how this works

1206
01:20:45,230 --> 01:20:50,210
i have it here on the local machine

1207
01:20:50,270 --> 01:20:53,230
hope still works so

1208
01:20:53,240 --> 01:20:59,710
here we based represen documents into this small window and press submit and then we

1209
01:20:59,710 --> 01:21:04,210
get categories here i prepared so

1210
01:21:04,230 --> 01:21:05,540
and coming from this

1211
01:21:06,040 --> 01:21:12,190
joseph stefan institute slovenia

1212
01:21:12,230 --> 01:21:15,290
and let's say we can take the

1213
01:21:20,790 --> 01:21:24,870
english description of our institute

1214
01:21:24,880 --> 01:21:26,560
small argument

1215
01:21:26,610 --> 01:21:28,940
this is just a few lines of text

1216
01:21:28,950 --> 01:21:32,140
and we try to categorize categorize this

1217
01:21:32,370 --> 01:21:38,340
in two d moss into the most science park because it wouldn't fit the whole

1218
01:21:38,340 --> 01:21:42,690
classified fit this notable press submit

1219
01:21:42,710 --> 01:21:46,500
and you get the categories from the moss

1220
01:21:47,450 --> 01:21:54,390
are apparently the most relevant ones so this is science science society academic departments or

1221
01:21:54,400 --> 01:22:01,460
science institutions research institute so which is exactly what we actually want to happen

1222
01:22:01,510 --> 01:22:08,240
environmental organisations research institutes and so on and so these are the actual categories that

1223
01:22:08,240 --> 01:22:09,430
test point

1224
01:22:11,060 --> 01:22:13,820
but of course this

1225
01:22:13,830 --> 01:22:19,270
cannot be computed because we don't know the distribution we can only approximated then usually

1226
01:22:19,360 --> 01:22:22,280
natural way to approximate this quantity is

1227
01:22:22,330 --> 01:22:25,240
given your training sample compute the average

1228
01:22:25,960 --> 01:22:27,460
on this training sample

1229
01:22:27,470 --> 01:22:31,340
so you have the x i y i compute these for i from one to

1230
01:22:31,340 --> 01:22:35,520
n the average and this is called the empirical

1231
01:22:35,540 --> 01:22:37,270
so the eighty will be true

1232
01:22:39,830 --> 01:22:40,930
this quantity

1233
01:22:41,910 --> 01:22:43,830
its empirical estimate

1234
01:22:43,850 --> 01:22:45,660
and of course we know

1235
01:22:45,710 --> 01:22:49,240
i mean it's a simple fact that the expected value

1236
01:22:49,260 --> 01:22:51,840
this empirical error

1237
01:22:51,860 --> 01:22:54,380
is equal to the loss

1238
01:22:55,060 --> 01:22:56,210
but here

1239
01:22:56,230 --> 01:23:01,140
becareful we'll see in a minute why this is dangerous statement it

1240
01:23:01,150 --> 01:23:03,750
this is true when g

1241
01:23:03,810 --> 01:23:05,110
is if fixed function

1242
01:23:05,160 --> 01:23:08,430
so it will no longer be true when g

1243
01:23:08,510 --> 01:23:13,280
it depends on the sample so when you have if you put here you classifiers

1244
01:23:13,430 --> 01:23:17,410
the function output by your training algorithms

1245
01:23:19,280 --> 01:23:21,450
this will no longer be true

1246
01:23:21,510 --> 01:23:26,490
OK so now

1247
01:23:26,570 --> 01:23:30,990
i have put here in the in these two slides

1248
01:23:31,010 --> 01:23:32,430
a couple of

1249
01:23:32,430 --> 01:23:34,450
kind of a very basic but

1250
01:23:34,470 --> 01:23:37,090
useful tools

1251
01:23:37,090 --> 01:23:41,890
are inequalities that come from basic probability theory

1252
01:23:41,910 --> 01:23:43,070
what i can

1253
01:23:43,140 --> 01:23:44,820
propose you is too

1254
01:23:44,930 --> 01:23:48,810
try to think about how you would prove these results

1255
01:23:48,860 --> 01:23:53,400
most of them can be proved quite directly

1256
01:23:53,420 --> 01:23:57,240
so if you want i can lead to some say five minutes to think about

1257
01:23:57,240 --> 01:23:59,390
it and then i can i give you the

1258
01:23:59,470 --> 01:24:00,930
the answer to just

1259
01:24:01,000 --> 01:24:03,270
because then in paper in

1260
01:24:03,280 --> 01:24:05,310
try to prove the things

1261
01:24:08,120 --> 01:24:10,930
OK this this does not require proof and we

1262
01:24:10,990 --> 01:24:14,330
this one but those two and this one

1263
01:24:20,440 --> 01:24:24,970
and i recall what we know about probabilities are

1264
01:24:28,070 --> 01:24:31,360
probability of something larger than zero

1265
01:24:31,380 --> 01:24:35,430
the probability of

1266
01:24:36,320 --> 01:24:40,440
so at each event is a subset

1267
01:24:40,480 --> 01:24:41,790
of this

1268
01:24:41,840 --> 01:24:43,370
everything and you know it

1269
01:24:43,380 --> 01:24:47,060
this is one

1270
01:24:47,070 --> 01:24:49,430
and the probability of the union

1271
01:24:52,270 --> 01:25:01,490
the sum minus the intersection

1272
01:25:01,500 --> 01:25:28,750
OK maybe a bit trivial

1273
01:25:29,150 --> 01:25:33,110
so if we look at the first one is of course from what i've written

1274
01:25:34,390 --> 01:25:36,230
well usually

1275
01:25:36,260 --> 01:25:37,580
this is not

1276
01:25:37,830 --> 01:25:41,440
fatalities so when you write it like this direct

1277
01:25:41,470 --> 01:25:45,930
it will be these combining these and non negativity

1278
01:25:47,430 --> 01:25:49,850
for this one

1279
01:25:49,900 --> 01:25:51,430
so anyone has

1280
01:25:51,490 --> 01:25:55,180
the proof

1281
01:25:55,220 --> 01:25:57,520
OK so inclusion sorry

1282
01:25:57,530 --> 01:26:02,090
the implication here is synonymous for inclusion so it means

1283
01:26:02,090 --> 01:26:04,840
eight is the subset of b

1284
01:26:04,840 --> 01:26:11,220
the four

1285
01:26:11,920 --> 01:26:16,650
and the thing and then i'm going to this picture which talks about the types

1286
01:26:16,650 --> 01:26:19,960
of learning techniques that have been used in the learning problems that have been looked

1287
01:26:19,970 --> 01:26:23,770
at and i was going to spend a lot more large one of them learning

1288
01:26:23,770 --> 01:26:25,160
such control

1289
01:26:25,200 --> 01:26:30,140
then talk about learning domain models and then finally talk about learning strategies

1290
01:26:35,990 --> 01:26:40,400
i i think i missed the flight here but i want you to remember in

1291
01:26:40,400 --> 01:26:46,550
your head that slide that i showed you earlier which said that basically mention this

1292
01:26:46,550 --> 01:26:50,950
point earlier it said that as impressive as the domain independent learners one and in

1293
01:26:50,950 --> 01:26:53,830
fact you know what our discussion i just complicated now

1294
01:26:53,880 --> 01:26:59,260
explain how we got from sorry looking graphs too much happier looking graphs in that

1295
01:26:59,300 --> 01:27:04,330
no importance like video having munich airport and then i also said that from the

1296
01:27:04,330 --> 01:27:09,890
munich airport graph you also have planners if you allow people to write

1297
01:27:09,900 --> 01:27:14,390
the main knowledge as to extract the in knowledge about fundamental physics this seems to

1298
01:27:14,390 --> 01:27:17,380
be good at writing all sorts of nifty knowledge

1299
01:27:17,390 --> 01:27:20,320
this the plan is that it will be used to

1300
01:27:20,340 --> 01:27:21,810
in competitions

1301
01:27:21,820 --> 01:27:28,270
even with even bigger margins than the very best domain specific domain independent and so

1302
01:27:28,370 --> 01:27:30,370
now if you're learning

1303
01:27:30,450 --> 01:27:35,530
that's good news to you because all you interested in is eliminating humans from the

1304
01:27:36,040 --> 01:27:41,110
without respect and so if people are writing this externality we want to lobotomized them

1305
01:27:41,110 --> 01:27:44,200
and stuff like that so that we don't need them anymore and you know so

1306
01:27:44,200 --> 01:27:45,590
they give you

1307
01:27:45,640 --> 01:27:48,060
the problems that the learning can

1308
01:27:49,380 --> 01:27:54,570
i basically getting into that by first talking to you about knowledge based planning track

1309
01:27:54,570 --> 01:27:58,020
and what kinds of knowledge people rule for these plants

1310
01:27:58,040 --> 01:28:01,040
that gives you an idea as to what being successful so you know those are

1311
01:28:01,040 --> 01:28:02,840
the kinds of things you might want

1312
01:28:02,860 --> 01:28:07,430
OK one thing ali told you for example you know figuring out many a heuristic

1313
01:28:07,430 --> 01:28:12,690
is actually underestimating and remembering of learning what it is it looks like a nice

1314
01:28:12,690 --> 01:28:16,470
idea i sort of thought that actually that works quite well so there are other

1315
01:28:16,470 --> 01:28:17,350
types of

1316
01:28:17,390 --> 01:28:20,720
a hand according to my knowledge which seems to work even better so i want

1317
01:28:20,720 --> 01:28:22,120
to give you some

1318
01:28:22,130 --> 01:28:24,730
i feel but how these sorts of knowledge

1319
01:28:24,780 --> 01:28:27,860
OK and then we can spend time talking to learn

1320
01:28:27,880 --> 01:28:33,800
OK so this whole thing sort of comes from the point of view of a

1321
01:28:33,820 --> 01:28:37,770
but looking at these two pictures which is the acme of what was planned which

1322
01:28:37,770 --> 01:28:41,320
are little requires is domain physics a bunch of actions schemas

1323
01:28:41,360 --> 01:28:46,360
and that's all it requires and it takes the initial state of problem articles another

1324
01:28:46,360 --> 01:28:51,750
problem spew of solutions this is domain independent planning so i think it's about is

1325
01:28:51,750 --> 01:28:55,370
the physics of the domain and make up the lands

1326
01:28:55,380 --> 01:29:01,180
the other extreme is sort of writing it separates the program for each possible landing

1327
01:29:01,180 --> 01:29:05,200
would you have so if your life depends on your bosses you know you're in

1328
01:29:05,260 --> 01:29:09,850
your cell is completely dependent on how fast you can do blocks planning i would

1329
01:29:09,970 --> 01:29:14,230
recommend that you don't planning just try to see program you know that that in

1330
01:29:14,230 --> 01:29:18,980
fact i don't know there are people who have look exactly at this point in

1331
01:29:19,440 --> 01:29:24,390
john slaney has looked at how fast can you do blocks planning if you want

1332
01:29:24,390 --> 01:29:25,310
to write

1333
01:29:25,320 --> 01:29:27,530
the best seat following

1334
01:29:27,540 --> 01:29:30,950
OK it's an interesting thing to do and so these would be

1335
01:29:30,970 --> 01:29:37,060
unabashedly domain specific so each time you get to the main you write c program

1336
01:29:37,080 --> 01:29:40,670
OK that's not very very obviously

1337
01:29:40,680 --> 01:29:44,070
in the community i'm planning somehow

1338
01:29:44,090 --> 01:29:48,010
you are much more interested in the domain independent in all but if the problem

1339
01:29:48,050 --> 01:29:51,220
i think is the program somewhere doesn't have anything FEEIT

1340
01:29:51,230 --> 01:29:57,550
OK it turns out that there is a happy middle ground village easiness innocence given

1341
01:29:57,550 --> 01:30:01,960
a domain and given a plan and a new domain

1342
01:30:01,970 --> 01:30:03,450
OK rather than sport

1343
01:30:03,460 --> 01:30:08,270
looking at the domain and spotlighting useit program what i was our what you could

1344
01:30:08,270 --> 01:30:13,510
do is you start with a customizable plan which is a domain independent planning

1345
01:30:13,530 --> 01:30:16,640
it would work even if you don't give it an extra knowledge

1346
01:30:16,650 --> 01:30:19,170
the island acquired system in physics

1347
01:30:20,610 --> 01:30:25,020
if you give it more and more static knowledge about how to find plans

1348
01:30:25,030 --> 01:30:27,430
then they could do better

1349
01:30:27,440 --> 01:30:33,430
so think of it as in expertise solution

1350
01:30:33,470 --> 01:30:34,840
OK so

1351
01:30:34,850 --> 01:30:37,840
this is sort of any expertise solution if you have more expertise i'll give you

1352
01:30:37,860 --> 01:30:42,780
plans faster if you know what expertise than just the main physics i still do

1353
01:30:42,780 --> 01:30:46,120
the best we can withstand domain independent

1354
01:30:46,160 --> 01:30:47,410
OK so

1355
01:30:47,460 --> 01:30:50,640
in many thinking about knowledge base plan is this is the picture you have in

1356
01:30:50,640 --> 01:30:55,260
mind the real question is very the domain specific knowledge coming

1357
01:30:55,300 --> 01:30:56,930
OK it could come

1358
01:30:56,950 --> 01:30:58,910
but from humans

1359
01:30:58,960 --> 01:31:02,930
which is what we talk about fast because the knowledge planning track people actually wrote

1360
01:31:03,890 --> 01:31:09,620
OK and you could actually combine only you look at you analyse the plans in

1361
01:31:09,620 --> 01:31:13,750
this domain and then you figure out what's the best

1362
01:31:13,800 --> 01:31:17,750
what's the kind of knowledge to learn and add to your customers

1363
01:31:17,770 --> 01:31:24,460
OK so i was talking about this and then we'll talk about

1364
01:31:24,510 --> 01:31:28,780
OK so it's customers and almost in the beginning of repeating what i said earlier

1365
01:31:28,780 --> 01:31:34,150
so that movie itself customizing one is given by humans humans will customize the planners

1366
01:31:34,490 --> 01:31:39,330
and it turns out that they tend to customize in all sorts of interesting reflecting

1367
01:31:39,330 --> 01:31:42,960
different kinds of extra knowledge and it's just what looking at the types of money

1368
01:31:42,960 --> 01:31:44,040
doing the right

1369
01:31:44,060 --> 01:31:48,900
and in particular see that they might extract the collective knowledge

1370
01:31:48,910 --> 01:31:54,290
which is just specific domain and has nothing to do with the planet they can

1371
01:31:54,290 --> 01:31:57,480
write procedural knowledge which is sort of

1372
01:31:57,530 --> 01:32:01,730
almost going to the extreme of writing and that the program acceptance of i didn't

1373
01:32:01,730 --> 01:32:04,070
see light and some high-level language

1374
01:32:04,080 --> 01:32:05,750
OK so

1375
01:32:05,760 --> 01:32:10,030
that's basically much more of a procedural control for that domain and people have done

1376
01:32:10,040 --> 01:32:11,540
both of these

1377
01:32:11,590 --> 01:32:15,880
OK and then we'll see how about the possibility of doing it is through machine

1378
01:32:15,880 --> 01:32:19,480
learning and we can learn how many of these sorts of things and i likely

1379
01:32:19,480 --> 01:32:22,500
to give you an idea of some of them which are the ones with

1380
01:32:22,510 --> 01:32:24,610
and which if you are interested you can go on

1381
01:32:24,610 --> 01:32:28,250
and if in a couple of all

1382
01:32:28,360 --> 01:32:32,420
OK so this is a more recent development

1383
01:32:32,440 --> 01:32:38,080
and that has shown to improve generalisation performance of the models one

1384
01:32:38,090 --> 01:32:41,960
OK this some work that you can look at you

1385
01:32:42,080 --> 01:32:46,460
kid to download my presentation point

1386
01:32:46,540 --> 01:32:49,880
the which

1387
01:32:49,880 --> 01:32:51,920
OK so i'm doing time was

1388
01:32:58,730 --> 01:33:01,150
the rest is really no so

1389
01:33:01,170 --> 01:33:05,850
it doesn't take that much of the time what i'd like to show you now

1390
01:33:06,790 --> 01:33:10,860
different variations around this idea so the first thing is

1391
01:33:10,880 --> 01:33:17,610
well if you extract these concepts right shouldn't these concepts some also help you in

1392
01:33:17,670 --> 01:33:20,850
supervised learning task for instance in text categorisation

1393
01:33:21,580 --> 01:33:24,670
and here most text categorisation algorithms like

1394
01:33:25,730 --> 01:33:30,360
k nearest neighbors and also adaboost text system whereas the end

1395
01:33:30,440 --> 01:33:34,400
they usually are based on the just the term based representation right

1396
01:33:34,440 --> 01:33:39,080
but the question is if i can extract these nice concept somehow can i can

1397
01:33:39,130 --> 01:33:43,480
exploit this also to improve text categorisation and what we do here this is the

1398
01:33:46,130 --> 01:33:48,440
next was the people

1399
01:33:49,560 --> 01:33:52,630
basically combine adaboost

1400
01:33:52,650 --> 01:33:55,130
two different versions of that

1401
01:33:55,150 --> 01:33:58,520
where we use weak learners

1402
01:33:58,560 --> 01:34:01,650
you know the set of all hypotheses

1403
01:34:01,710 --> 01:34:08,020
consisted of two different type of simple hypothesis one is just an indicator function for

1404
01:34:08,020 --> 01:34:09,500
term features

1405
01:34:09,560 --> 01:34:15,400
so it's whatever minus one the terms not present in document otherwise it's one

1406
01:34:15,440 --> 01:34:18,900
and the other one what we did is we would run

1407
01:34:19,400 --> 01:34:24,460
this problem probabilistic latent semantic analysis

1408
01:34:24,480 --> 01:34:27,210
over the over the whole document collection

1409
01:34:27,230 --> 01:34:29,480
and that would give us these

1410
01:34:30,480 --> 01:34:32,980
concept probabilities for every document

1411
01:34:33,000 --> 01:34:36,940
right and we would take that as a kind of representation for the document that

1412
01:34:38,860 --> 01:34:39,980
and we just

1413
01:34:40,000 --> 01:34:41,790
you know you very simple

1414
01:34:41,810 --> 01:34:46,090
the indicator function here we have some threshold that we also optimize over

1415
01:34:46,110 --> 01:34:50,960
beta where we said well the probability is below that right and some of the

1416
01:34:50,960 --> 01:34:54,350
document is not on the concepts of minus one

1417
01:34:54,440 --> 01:34:57,130
so we have these very simple functions

1418
01:34:57,130 --> 01:35:00,310
and what we do now is we combine them using out

1419
01:35:00,360 --> 01:35:03,310
it's just standard

1420
01:35:05,060 --> 01:35:07,290
just show you that experimentally

1421
01:35:07,290 --> 01:35:10,090
you know you can

1422
01:35:10,110 --> 01:35:13,080
can gain by doing that so this is on the

1423
01:35:13,190 --> 01:35:15,960
i don't want to get too much into the details of the

1424
01:35:16,000 --> 01:35:18,250
the experimental setup the the

1425
01:35:18,290 --> 01:35:21,360
well known writers twenty one thousand something

1426
01:35:21,400 --> 01:35:23,860
dataset and you know

1427
01:35:23,900 --> 01:35:26,590
census taker paper you know reduced

1428
01:35:26,630 --> 01:35:31,830
one thousand different metrics and thousand different set of what but this is just showing

1429
01:35:31,830 --> 01:35:33,850
improvement in terms of

1430
01:35:33,860 --> 01:35:37,000
f one measure in the average precision

1431
01:35:37,000 --> 01:35:42,950
four you know micro and macro averaged depends on how you treat the different ways

1432
01:35:42,950 --> 01:35:46,900
to different categories and you see that you get some improvement

1433
01:35:49,440 --> 01:35:52,400
very consistent that's not what i mean

1434
01:35:52,440 --> 01:35:54,270
it's not revolutionary

1435
01:35:54,270 --> 01:35:59,900
but it's it's solid improvement across the board and also we looked at more interesting

1436
01:35:59,900 --> 01:36:05,080
data datasets this now is the subset of the medline collection

1437
01:36:05,130 --> 01:36:08,920
fifty thousand something edits datasets the ottoman

1438
01:36:08,920 --> 01:36:14,880
dataset and and we see here improvement in average precision around relative improvement of about

1439
01:36:14,880 --> 01:36:20,040
to get an estimate good enough to get a certain accuracy of the expectation

1440
01:36:20,050 --> 01:36:25,470
of random variables the number of samples is completely independent of the number of values

1441
01:36:25,480 --> 01:36:27,980
and variable can take

1442
01:36:28,000 --> 01:36:29,230
and it's just an idea

1443
01:36:29,250 --> 01:36:31,120
exploited industry

1444
01:36:31,300 --> 01:36:35,540
and that's that's how it's it's speeding cars to measure

1445
01:36:35,560 --> 01:36:39,280
OK and the and the current state of the art algorithms are inspired by UCT

1446
01:36:39,280 --> 01:36:44,520
which are much cleverer than this the foundational idea they decide much more cleverly which

1447
01:36:44,520 --> 01:36:47,910
actions to explore which one's not

1448
01:36:47,940 --> 01:36:50,610
but but but the underlying idea is this

1449
01:36:51,370 --> 01:36:53,540
so i'm going to talk lots of

1450
01:36:53,740 --> 01:36:58,370
lots of nice results and applications available reinforcement learning but

1451
01:36:58,400 --> 01:37:00,130
all of them

1452
01:37:02,010 --> 01:37:05,840
you know the state space you know the action space you know the reward space

1453
01:37:06,080 --> 01:37:07,580
what i'm interested in

1454
01:37:07,580 --> 01:37:12,180
what what drives my interest these days has been for a while but for the

1455
01:37:12,180 --> 01:37:13,850
near future is

1456
01:37:13,870 --> 01:37:16,980
trying to build flexible intelligent

1457
01:37:16,980 --> 01:37:18,520
i systems

1458
01:37:18,560 --> 01:37:23,310
that's how it connects to cognitive science because the the example of the flexible intelligence

1459
01:37:23,310 --> 01:37:25,160
are animals and human things

1460
01:37:25,390 --> 01:37:30,250
if you're doing narrow and you're doing now is a pejorative work now just means

1461
01:37:30,250 --> 01:37:35,590
you particular problem you want to make a helicopter flight you want to schedule you

1462
01:37:35,590 --> 01:37:38,600
know the trucking companies

1463
01:37:38,610 --> 01:37:43,330
she you know trucks you want to solve a specific engineering problem you know the

1464
01:37:43,340 --> 01:37:48,030
state space because thousands of person is referred to as input into figuring out what's

1465
01:37:48,080 --> 01:37:52,360
good state is one of the good representation of actions forty objectives were the reward

1466
01:37:53,270 --> 01:37:55,130
you know

1467
01:37:55,190 --> 01:38:00,580
great we have good methods for not off the shelf but

1468
01:38:00,580 --> 01:38:03,750
pretty good methods for solving that class

1469
01:38:03,800 --> 01:38:08,890
those types of problems if on the other hand if you are interested in flexibly

1470
01:38:08,890 --> 01:38:10,410
i these are rarely given

1471
01:38:10,440 --> 01:38:16,070
right now

1472
01:38:20,240 --> 01:38:26,100
so if on the other hand you are you magically get hardware for android

1473
01:38:26,270 --> 01:38:31,350
you log deal with amazon com any or a household robot that is going to

1474
01:38:31,350 --> 01:38:32,840
be a companion in

1475
01:38:33,010 --> 01:38:35,080
what is going to be state space

1476
01:38:35,100 --> 01:38:39,420
how to represent his actions what reward function should have was on the program into

1477
01:38:40,050 --> 01:38:41,380
into that robot

1478
01:38:41,500 --> 01:38:45,310
maybe i was on the reward function should be don't come back but you know

1479
01:38:45,320 --> 01:38:46,590
come back to that

1480
01:38:46,600 --> 01:38:51,470
OK so back that brought these questions are much trickier now

1481
01:38:51,610 --> 01:38:55,730
and those are the questions i want to give you one

1482
01:38:55,750 --> 01:38:57,250
the basic idea of our

1483
01:38:57,270 --> 01:39:00,060
three to those questions for the for the rest of the rest

1484
01:39:00,100 --> 01:39:03,640
so this is my tutorial if anybody has questions about this part

1485
01:39:03,650 --> 01:39:07,110
along with that tag twenty five

1486
01:39:07,940 --> 01:39:09,720
a question about sport yes

1487
01:39:14,140 --> 01:39:15,820
o thing

1488
01:39:20,660 --> 01:39:37,640
right so having defined flexible AI what that means exactly in opposition to

1489
01:39:37,660 --> 01:39:42,500
classical engineering look up control where they were very well defined problem and you can

1490
01:39:42,500 --> 01:39:47,940
take flexible AI in many different directions i'm define except the we're motivating these questions

1491
01:39:48,260 --> 01:39:51,080
or what the state space should be what the action space to be with reward

1492
01:39:52,120 --> 01:39:54,760
adjustable economy is an interesting approach that

1493
01:39:54,780 --> 01:39:56,970
agent people have

1494
01:39:56,990 --> 01:39:59,030
make progress on but

1495
01:39:59,080 --> 01:40:03,520
let me come back to that if in the end talk about what i'm not

1496
01:40:03,630 --> 01:40:05,640
much of the system so let's

1497
01:40:05,660 --> 01:40:09,660
let's hold off that maybe this time at the end we can we can improve

1498
01:40:09,660 --> 01:40:11,130
on this OK so

1499
01:40:11,260 --> 01:40:14,610
let's see i already said this is what should be

1500
01:40:15,180 --> 01:40:20,230
the state space for dog person what state space is not very clear like what

1501
01:40:20,230 --> 01:40:21,750
we know what perceptions are

1502
01:40:21,900 --> 01:40:23,780
we don't know what state spaces

1503
01:40:23,780 --> 01:40:31,380
in a way that's all symmetrisation is doing is formalising independence assumption

1504
01:40:31,410 --> 01:40:36,000
i mean just saying that the likelihood of

1505
01:40:36,020 --> 01:40:39,440
all of these things happening in the second half is well

1506
01:40:40,990 --> 01:40:43,280
but it's it's just four right

1507
01:40:43,320 --> 01:40:49,630
sounds good

1508
01:40:53,590 --> 01:40:56,980
so how many people have

1509
01:40:56,980 --> 01:41:00,630
succeeded in this

1510
01:41:01,000 --> 01:41:05,240
slide just put up

1511
01:41:05,300 --> 01:41:09,730
derivation here to show you how it works

1512
01:41:17,980 --> 01:41:20,030
so basically your setting that

1513
01:41:20,120 --> 01:41:26,490
two to minus epsilon and over two equal to delta over two BH and

1514
01:41:26,500 --> 01:41:33,530
take logs to base two on both sides

1515
01:41:35,290 --> 01:41:40,060
and through life through by minus and you get epsilon over to here

1516
01:41:40,080 --> 01:41:41,780
and log of the

1517
01:41:41,790 --> 01:41:48,660
inverse ratio here two bhm delta that's because of the minus going through the log

1518
01:41:48,680 --> 01:41:51,770
then you put epsilon is two over

1519
01:41:51,790 --> 01:41:57,090
and substituting in so should be to ensure that should be too

1520
01:42:00,090 --> 01:42:03,410
should be too

1521
01:42:03,430 --> 01:42:08,570
and then substituting in cellars lemon

1522
01:42:08,620 --> 01:42:13,410
well firstly separating this is the logo of the product into the sum of the

1523
01:42:13,410 --> 01:42:18,080
two logd log two over delta and log ph brand and then subst

1524
01:42:18,090 --> 01:42:21,570
substituting in the cell bound which is the

1525
01:42:21,620 --> 01:42:27,330
two EMI over del over data the taking log that the d comes down and

1526
01:42:27,330 --> 01:42:28,690
you get d log

1527
01:42:28,700 --> 01:42:31,380
two you and over d

1528
01:42:31,680 --> 01:42:35,800
so again it's just to manipulation of the

1529
01:42:35,830 --> 01:42:37,830
of the formulae

1530
01:42:38,070 --> 01:42:41,160
as everyone happy with that

1531
01:42:41,600 --> 01:42:46,730
OK i just want to give you a chance to

1532
01:42:46,730 --> 01:42:49,840
chosen because around so there it is

1533
01:42:49,870 --> 01:42:55,100
so what that tells you this is if you like the

1534
01:42:55,270 --> 01:42:59,130
central result of learning theory in some sense

1535
01:42:59,150 --> 01:42:59,930
so i

1536
01:42:59,940 --> 01:43:02,810
there doesn't hurt

1537
01:43:04,790 --> 01:43:05,790
it says

1538
01:43:05,800 --> 01:43:06,390
we have

1539
01:43:06,410 --> 01:43:11,100
but the formulas we so with probability at least one minus delta

1540
01:43:11,270 --> 01:43:15,800
if you the generalisation error of a function h in the class h

1541
01:43:15,840 --> 01:43:17,700
with these dimension d

1542
01:43:17,700 --> 01:43:21,350
which has zero training error is bounded by the following quantity

1543
01:43:21,370 --> 01:43:22,630
two over and

1544
01:43:22,650 --> 01:43:27,470
d the VC dimension log two you know the delta plus log two delta

1545
01:43:28,030 --> 01:43:34,540
so no it is again the same form we've been seeing coming out repeatedly

1546
01:43:34,840 --> 01:43:36,400
the inverse

1547
01:43:36,410 --> 01:43:38,200
dependence on n

1548
01:43:38,210 --> 01:43:43,610
so as the training set size goes up you're getting decrease in the training error

1549
01:43:43,640 --> 01:43:45,650
the log two of the delta the

1550
01:43:45,730 --> 01:43:48,960
delta coming in very benignly under the log

1551
01:43:49,470 --> 01:43:55,070
in this case is locked the base to science natural others i remember

1552
01:43:55,100 --> 01:43:56,300
not much in it

1553
01:43:56,300 --> 01:44:02,040
and now this complexity term which if you remember before we had logged one over

1554
01:44:03,160 --> 01:44:06,840
and where we chose this prior sort of

1555
01:44:06,860 --> 01:44:08,200
weightings of the

1556
01:44:08,210 --> 01:44:09,470
the functions

1557
01:44:09,480 --> 01:44:12,640
has is replaced by this uniform turn

1558
01:44:12,660 --> 01:44:14,700
which is equal for all functions

1559
01:44:14,710 --> 01:44:18,030
and it's d log two m over d

1560
01:44:18,050 --> 01:44:23,530
so you can think of d in some sense measuring the complexity of the function

1561
01:44:24,800 --> 01:44:28,780
so the VC dimension is some nice measure the function class

1562
01:44:28,820 --> 01:44:30,580
and notice that the

1563
01:44:30,600 --> 01:44:37,590
polynomial growth we had directly translates into this logarithmic growth here

1564
01:44:37,700 --> 01:44:42,960
in in the sample size and we're setting that against the inverse and which is

1565
01:44:42,960 --> 01:44:44,600
what is controlling

1566
01:44:45,740 --> 01:44:48,210
generalisation error so provided we

1567
01:44:48,220 --> 01:44:50,340
take a sufficiently large sample

1568
01:44:50,360 --> 01:44:52,230
this is going to be

1569
01:44:52,480 --> 01:44:56,970
dominated by this and this is already quite small

1570
01:44:56,980 --> 01:45:00,520
and we're going to get a good bounds on the generalization

1571
01:45:01,820 --> 01:45:05,130
if on the other hand we deployed

1572
01:45:05,980 --> 01:45:07,260
result where

1573
01:45:07,270 --> 01:45:09,790
we still had exponential growth

1574
01:45:09,810 --> 01:45:10,880
in the

1575
01:45:10,900 --> 01:45:11,980
growth function

1576
01:45:11,980 --> 01:45:15,260
then this would have been log of two to be

1577
01:45:15,410 --> 01:45:17,280
two and

1578
01:45:17,290 --> 01:45:20,340
take the log of that and you just have to and and you get the

1579
01:45:20,340 --> 01:45:23,020
ems cancel and you get a trivial bound

1580
01:45:23,040 --> 01:45:25,390
seem to need that

1581
01:45:25,410 --> 01:45:30,800
significant decay of the growth function below the exponential growth in order to get

1582
01:45:31,300 --> 01:45:33,230
and effective

1583
01:45:37,230 --> 01:45:41,470
so this is you know say was was heralded as a very sort of

1584
01:45:41,500 --> 01:45:45,930
key result and it it's very nice in the sense that characterizes learning in the

1585
01:45:45,930 --> 01:45:48,990
following sense that there are lower bounds

1586
01:45:49,190 --> 01:45:52,940
again i'm not going to go into the process these but there are lower bounds

1587
01:45:52,940 --> 01:45:54,300
that say that

1588
01:45:54,320 --> 01:45:58,450
given a class of VC dimension d

1589
01:45:58,460 --> 01:46:00,780
there exist distributions

1590
01:46:00,810 --> 01:46:05,690
such that with probability at least delta over m random examples the error of h

1591
01:46:05,690 --> 01:46:08,350
is at least this quantity

1592
01:46:08,770 --> 01:46:11,970
so it's it's sort of like

1593
01:46:11,970 --> 01:46:15,790
in inverse in the sense that it says

1594
01:46:15,800 --> 01:46:19,660
you know there are cases in which you can do is roughly is bad i

1595
01:46:19,660 --> 01:46:24,040
mean OK there's a lot of factor missing here but in a lot of factors

1596
01:46:24,040 --> 01:46:27,070
between learning theorists don't count for much

1597
01:46:27,080 --> 01:46:31,240
so there is the the log to anybody here

1598
01:46:31,250 --> 01:46:33,450
and here this is the cover and

1599
01:46:33,500 --> 01:46:36,710
but the constants are a bit different and st

1600
01:46:36,760 --> 01:46:40,910
but apart from that i mean this is very very close

1601
01:46:40,990 --> 01:46:47,070
so in a sense what this is telling us is that the VC dimension is

1602
01:46:48,130 --> 01:46:52,820
the right characterisation of learning in this in this classification sense

1603
01:46:52,840 --> 01:46:58,240
so it's a very nice close theory in that sense it all looks very beautiful

1604
01:47:01,940 --> 01:47:07,280
the corresponding results for non zero training which again i mean the techniques are very

1605
01:47:07,280 --> 01:47:11,650
are very short millisecond in the case of the

1606
01:47:11,700 --> 01:47:16,000
one that bounces back one millisecond indicates that once that one clunk

1607
01:47:16,060 --> 01:47:17,420
and the average

1608
01:47:18,380 --> 01:47:20,720
it's about five hundred fifty times

1609
01:47:20,780 --> 01:47:23,140
their normal weight

1610
01:47:23,170 --> 01:47:24,920
i'd like to show you

1611
01:47:24,970 --> 01:47:26,450
fast photography

1612
01:47:26,480 --> 01:47:30,110
on not these very same ball with on another one that we did

1613
01:47:30,170 --> 01:47:31,860
let me take this

1614
01:47:31,900 --> 01:47:34,240
this out

1615
01:47:35,710 --> 01:47:38,390
that is the ball that comes down

1616
01:47:38,400 --> 01:47:41,400
with the speed of four meters per second

1617
01:47:41,470 --> 01:47:44,310
and each train is one millisecond

1618
01:47:44,360 --> 01:47:46,410
so you will see a rule

1619
01:47:46,430 --> 01:47:49,970
and the ruler indicates has marked in centimetres

1620
01:47:50,020 --> 01:47:52,940
so you will see it going for

1621
01:47:52,940 --> 01:47:55,020
seconds it will go one centimetre

1622
01:47:55,040 --> 01:47:57,550
so it has the speed of two and a half meters per second it will

1623
01:47:57,550 --> 01:48:01,330
hit the floor and then we can count the number of milli seconds that it

1624
01:48:01,330 --> 01:48:03,070
takes contact

1625
01:48:03,080 --> 01:48:05,860
and going back up again it's not going to be too many second it's a

1626
01:48:05,860 --> 01:48:10,790
bit longer but again impressively short

1627
01:48:10,800 --> 01:48:15,110
all right we're going to make it rather dark in order to get

1628
01:48:15,190 --> 01:48:17,040
decent quality

1629
01:48:17,100 --> 01:48:18,700
i'm going to turn

1630
01:48:18,710 --> 01:48:21,280
five of these off

1631
01:48:21,460 --> 01:48:24,410
and i'm going to set the TV two

1632
01:48:24,500 --> 01:48:28,730
and now i will start to this

1633
01:48:28,770 --> 01:48:30,710
there we go

1634
01:48:30,750 --> 01:48:33,420
let's hope that that will go

1635
01:48:33,490 --> 01:48:35,630
OK that comfortable down

1636
01:48:35,730 --> 01:48:36,980
these marks are

1637
01:48:36,990 --> 01:48:38,630
in centimetres

1638
01:48:38,670 --> 01:48:43,920
so it is a rule in centimetres

1639
01:48:43,970 --> 01:48:45,730
this is one centimeter

1640
01:48:45,780 --> 01:48:47,740
a reminder little

1641
01:48:47,810 --> 01:48:48,860
because we

1642
01:48:48,860 --> 01:48:51,610
well little too late

1643
01:48:52,300 --> 01:48:53,630
let's start again

1644
01:48:53,630 --> 01:48:55,020
so what

1645
01:48:55,080 --> 01:48:57,910
when it passes this mark one

1646
01:48:59,680 --> 01:49:03,060
see for many seconds for about one centimeter

1647
01:49:03,230 --> 01:49:07,210
two and a half meters per second novel count the number of seconds

1648
01:49:07,220 --> 01:49:10,060
and in fact

1649
01:49:15,830 --> 01:49:19,880
and it's of about six maybe seventy seconds

1650
01:49:19,950 --> 01:49:21,870
this no special ball

1651
01:49:21,890 --> 01:49:23,710
these impact times are

1652
01:49:23,760 --> 01:49:42,380
amazingly short

1653
01:49:42,480 --> 01:49:44,250
now have something

1654
01:49:44,350 --> 01:49:46,140
very special

1655
01:49:46,170 --> 01:49:48,750
something really special

1656
01:49:48,770 --> 01:49:52,900
something that has kept me awake a lot of things keep me awake physics and

1657
01:49:52,900 --> 01:49:54,440
not only in physics

1658
01:49:54,480 --> 01:49:56,250
but this

1659
01:49:56,300 --> 01:49:57,960
but this is very special

1660
01:49:58,000 --> 01:49:59,440
this is very special

1661
01:49:59,480 --> 01:50:05,660
i have here a basketball

1662
01:50:05,700 --> 01:50:08,350
not completely elastic but not bad

1663
01:50:08,370 --> 01:50:09,880
tennis ball

1664
01:50:09,930 --> 01:50:12,960
not completely elastic not bad

1665
01:50:13,040 --> 01:50:15,500
now i'm going to drop them together

1666
01:50:15,510 --> 01:50:17,570
vertically down

1667
01:50:17,710 --> 01:50:20,020
and then

1668
01:50:20,070 --> 01:50:21,330
this ball

1669
01:50:21,370 --> 01:50:23,270
will bounce up somehow

1670
01:50:23,290 --> 01:50:25,410
and so the question that i have for you is

1671
01:50:25,510 --> 01:50:28,230
so you think if i drop it from this height

1672
01:50:28,240 --> 01:50:29,510
that this tennis ball

1673
01:50:29,520 --> 01:50:32,290
well sort of come up at most to this high to do you think it

1674
01:50:32,290 --> 01:50:33,570
will be lower

1675
01:50:33,580 --> 01:50:34,710
think it will be

1676
01:50:35,960 --> 01:50:41,260
so usually intuition in the worst case it can be wrong is already pushing this

1677
01:50:42,670 --> 01:50:44,160
so what do you think

1678
01:50:44,210 --> 01:50:48,430
well the tennis ball reach about the same height who is in favor of that

1679
01:50:48,530 --> 01:50:50,030
was in favor of

1680
01:50:51,930 --> 01:50:55,970
while i was in favor of a lot time

1681
01:50:57,370 --> 01:50:58,850
OK yeah i'll try it now

1682
01:50:58,860 --> 01:51:03,980
i cannot guarantee that this ball will go straight up after the impact because clearly

1683
01:51:03,990 --> 01:51:08,580
it's impossible that has zero chance of probably go off at some direction

1684
01:51:08,640 --> 01:51:10,080
but you will see

1685
01:51:10,140 --> 01:51:14,140
the fact that i had in mind

1686
01:51:14,180 --> 01:51:17,680
so there we go

1687
01:51:17,700 --> 01:51:20,900
you see that indeed tennis ball

1688
01:51:23,300 --> 01:51:27,710
goes way i tried once more to see what i can get it

1689
01:51:27,720 --> 01:51:30,980
go up a little bit more vertically with that is very difficult

1690
01:51:30,990 --> 01:51:32,480
it goes way high

1691
01:51:32,490 --> 01:51:35,620
and this is something that you should be able to calculate

1692
01:51:35,660 --> 01:51:37,330
you can you will

1693
01:51:37,430 --> 01:51:41,610
believe this part of the simon number sixty haven't seen it yet

1694
01:51:41,620 --> 01:51:43,210
there we go

1695
01:51:43,270 --> 01:51:45,110
what does matter

1696
01:51:51,160 --> 01:51:54,970
and you know anyone of you know approximately how much higher it goes invisible is

1697
01:51:54,980 --> 01:51:58,950
way higher mass than this one because the mass ratio comes into it

1698
01:51:58,970 --> 01:52:01,360
any ideas

1699
01:52:01,360 --> 01:52:03,080
prices high

1700
01:52:03,100 --> 01:52:05,930
you'll be surprised when he resigned six

1701
01:52:05,940 --> 01:52:07,880
what's hard

1702
01:52:07,930 --> 01:52:10,830
OK for thank you could even see if he always there was quite a bit

1703
01:52:10,830 --> 01:52:12,850
higher than twice

1704
01:52:12,870 --> 01:52:16,350
great great experiment in something you can do

1705
01:52:16,390 --> 01:52:19,750
the self geometry

1706
01:52:19,780 --> 01:52:23,690
now i want to discuss the remaining time about rockets

1707
01:52:23,720 --> 01:52:25,850
a rocket

1708
01:52:25,900 --> 01:52:28,480
experience an impulse

1709
01:52:28,500 --> 01:52:29,500
from the engine

1710
01:52:29,540 --> 01:52:31,460
and that changes the momentum

1711
01:52:32,300 --> 01:52:33,890
the rocket

1712
01:52:33,930 --> 01:52:36,980
before we go into the details of the rocket

1713
01:52:37,000 --> 01:52:41,680
think about gandhi's ideas of throwing objects on the floor

1714
01:52:41,710 --> 01:52:44,220
and that's turned to tomatoes

1715
01:52:44,280 --> 01:52:48,150
i want to made because i want to complete inelastic collisions

1716
01:52:48,180 --> 01:52:50,290
so tomatoes hit the floor

1717
01:52:51,580 --> 01:52:54,880
it's not that i not only going to throw one made on the floor but

1718
01:52:54,880 --> 01:52:59,490
i'm very angry today i'm going to a lot of these tomatoes on the floor

1719
01:52:59,510 --> 01:53:03,790
and as in nancy tomatoes on the floor

1720
01:53:03,870 --> 01:53:06,850
if one of the major hits the floor

1721
01:53:06,890 --> 01:53:10,190
the change in momentum is envy if m is the mass of the tomato

1722
01:53:10,210 --> 01:53:12,560
but i'm going to throw and on the floor

1723
01:53:12,580 --> 01:53:14,810
so to change of momentum

1724
01:53:14,950 --> 01:53:16,960
is an as in nancy

1725
01:53:16,970 --> 01:53:18,510
times the mass of the

1726
01:53:18,620 --> 01:53:19,460
you may know

1727
01:53:20,430 --> 01:53:21,550
and this

1728
01:53:21,580 --> 01:53:23,890
is the number of kilograms per second

1729
01:53:23,970 --> 01:53:27,260
after may know that i throw on the floor

1730
01:53:27,310 --> 01:53:28,430
so this is

1731
01:53:28,470 --> 01:53:30,450
the change of momentum

1732
01:53:30,620 --> 01:53:33,210
so this is called delta p

1733
01:53:33,220 --> 01:53:36,630
dividing by delta t

1734
01:53:36,670 --> 01:53:39,520
and that's an average force

1735
01:53:45,200 --> 01:53:47,720
real experience

1736
01:53:47,770 --> 01:53:49,640
the force in down direction

1737
01:53:49,710 --> 01:53:53,210
of course that was also the case here when the ball experience

1738
01:53:53,220 --> 01:53:54,510
four supt

1739
01:53:54,560 --> 01:53:56,850
which we calculated here

1740
01:53:56,890 --> 01:54:01,200
the from our experience of course the same force in down direction actually act action

1741
01:54:01,200 --> 01:54:04,790
equals minus reaction newton's third law

1742
01:54:04,850 --> 01:54:09,530
so the force experiences the different experiences a force in down direction

1743
01:54:09,680 --> 01:54:11,220
and i can write it down

1744
01:54:11,250 --> 01:54:13,900
in a somewhat more civilized four

1745
01:54:13,910 --> 01:54:16,470
f equals

1746
01:54:16,480 --> 01:54:18,570
the NDT

1747
01:54:18,690 --> 01:54:20,230
times the velocity

1748
01:54:20,250 --> 01:54:23,310
if the velocity of the tomatoes it hit the floor

1749
01:54:23,350 --> 01:54:24,850
o is constant

1750
01:54:24,860 --> 01:54:29,000
and we're going to apply these rockets were by exhaust out of rockets as relative

1751
01:54:29,000 --> 01:54:31,140
to the rock at constant speed

1752
01:54:31,190 --> 01:54:33,000
and this is then

1753
01:54:33,040 --> 01:54:35,210
number of kilograms per second

1754
01:54:37,010 --> 01:54:39,170
i threw on the floor

1755
01:54:39,300 --> 01:54:41,210
this is very real

1756
01:54:41,240 --> 01:54:44,310
i can throw his tomatoes on a bathroom scale

1757
01:54:44,320 --> 01:54:48,530
and if i through four kilogrammes per second on a bathroom scale

1758
01:54:48,630 --> 01:54:52,720
and it's the bathroom scale was five meters per second you better believe it you

1759
01:54:52,720 --> 01:54:53,470
will see

1760
01:54:53,470 --> 01:54:56,480
see also we can we can solve problems like this one so here's that means

1761
01:54:56,480 --> 01:54:57,910
that i that i showed before

1762
01:54:57,930 --> 01:55:03,630
we gave a chinese restaurant process prior over the MDP so we basically said that

1763
01:55:03,640 --> 01:55:07,440
states can cluster we don't know exactly how many clusters there are there may be

1764
01:55:07,440 --> 01:55:11,590
one as many clusters are states in the environment but that's unlikely to probably clustering

1765
01:55:11,590 --> 01:55:15,390
together and we let it run environment we don't see the wall switches but see

1766
01:55:15,390 --> 01:55:18,490
what i mean this state here's what i was what happened when i was there

1767
01:55:18,490 --> 01:55:22,020
and i was this takeover happened i was there they may be the same so

1768
01:55:22,040 --> 01:55:25,800
the chinese restaurant prior let's those kind of cluster together empirically

1769
01:55:25,830 --> 01:55:29,220
and ends up four outperforming algorithm that

1770
01:55:29,230 --> 01:55:34,470
streets of state separately and outperforms an algorithm that assumes all states have the same

1771
01:55:34,470 --> 01:55:39,770
cluster which is incorrect assumption so caustic can do well but it basically it figures

1772
01:55:39,770 --> 01:55:42,880
out the number of clusters that are there learned about

1773
01:55:43,190 --> 01:55:47,120
and better about the states this it's often you can if you the colors to

1774
01:55:47,120 --> 01:55:49,480
represent the state question you can see some of them are not quite the right

1775
01:55:49,480 --> 01:55:51,740
color but enough the right column

1776
01:55:51,760 --> 01:55:56,150
OK it's it's enough right color that

1777
01:55:56,160 --> 01:55:59,200
that the behaviour that comes out of this is what you what you hope

1778
01:55:59,930 --> 01:56:01,730
that's kind of cute

1779
01:56:01,740 --> 01:56:06,730
we're still working on that there's various difficulties in in using big complicated bayesian reasoning

1780
01:56:06,730 --> 01:56:09,270
algorithms in the context of the sort of thing

1781
01:56:09,320 --> 01:56:13,140
OK so next change gears is to talk a little bit about the fact that

1782
01:56:13,140 --> 01:56:14,730
computation matters so

1783
01:56:14,800 --> 01:56:18,990
what talk about so far is that in the in the model based setting if

1784
01:56:18,990 --> 01:56:23,000
you have the right model class for the environment we have various kinds of you

1785
01:56:23,000 --> 01:56:26,890
know quickly found type algorithms that can quickly acquired transition models

1786
01:56:26,900 --> 01:56:30,290
and then we hit with the planning systematic planning stack

1787
01:56:30,320 --> 01:56:35,720
and then we behave nearly optimal the thing is that the magic planning stack is

1788
01:56:35,720 --> 01:56:38,460
much heavier than you might think when you first see it lying on the ground

1789
01:56:39,020 --> 01:56:43,970
this is even planning is just a computational problem it's not a learning problem you

1790
01:56:44,580 --> 01:56:47,840
we're treating it as an oracle actually leads to

1791
01:56:47,860 --> 01:56:51,550
solutions to problems that we've looked at that you can't really run it just is

1792
01:56:51,550 --> 01:56:53,540
too slow to do the planning

1793
01:56:53,560 --> 01:56:57,890
especially the more powerful the german generalization is in the learning the bigger the model

1794
01:56:57,890 --> 01:57:00,950
is that we can learn the less likely it is that we can plan and

1795
01:57:00,950 --> 01:57:04,790
so we've got to do something about these large models otherwise these great things that

1796
01:57:04,790 --> 01:57:08,560
we're learning our users we can use them to make decisions in the world

1797
01:57:09,410 --> 01:57:13,950
what does the bunch ways people are doing this i'm going really not do it

1798
01:57:13,950 --> 01:57:16,890
justice and i apologize by

1799
01:57:16,900 --> 01:57:20,250
but i give you the high level view see is a highly i talked about

1800
01:57:20,250 --> 01:57:25,720
three different classes of reinforcement learning approaches policy search research policy value function search try

1801
01:57:25,920 --> 01:57:29,150
q function and model based we learn the model and then playing with the model

1802
01:57:29,300 --> 01:57:32,670
what turns out that this plan with the model things really expensive so what can

1803
01:57:32,670 --> 01:57:35,080
you do what thing you can do is say well model

1804
01:57:35,550 --> 01:57:37,460
it's kind of like an environment

1805
01:57:37,460 --> 01:57:38,370
and so

1806
01:57:38,410 --> 01:57:42,060
we could just make use any reinforcement or we want as a learner for that

1807
01:57:43,160 --> 01:57:46,900
now it's a fake environments learning environment but you know if we can learn them

1808
01:57:46,900 --> 01:57:50,310
efficiently and we can learn to accurately then that's OK

1809
01:57:50,340 --> 01:57:54,320
so if learning if reinforcement learning is more efficient than planning

1810
01:57:54,340 --> 01:57:58,970
then we can put a policy search algorithm inside model based algorithms to learn the

1811
01:57:58,970 --> 01:58:04,340
model and then playing with it by pretending its environment and reinforcement learning we can

1812
01:58:04,340 --> 01:58:09,000
put a value function methods inside model based method to learn the model

1813
01:58:09,050 --> 01:58:12,830
and then pretend the models environment and use value function learner to actually decide what

1814
01:58:12,840 --> 01:58:14,720
to do

1815
01:58:14,720 --> 01:58:18,320
it seems so crazy just might work right so in cases where we can do

1816
01:58:18,320 --> 01:58:22,130
efficient policy search value function search with the kinds of models we learned this can

1817
01:58:22,130 --> 01:58:25,500
work really well one thing i didn't put up here is you learn the model

1818
01:58:25,500 --> 01:58:29,450
and then inside user model based learner for the reinforcement learner that somehow seems like

1819
01:58:29,450 --> 01:58:33,530
an infinite regress that that's meaningful haven't seen anybody do that in the literature that

1820
01:58:33,530 --> 01:58:35,430
i wouldn't be surprised now somebody

1821
01:58:35,440 --> 01:58:39,020
because it does that just to screw up my figure

1822
01:58:39,050 --> 01:58:41,970
so that's the basic idea and this is a very natural way for cannot see

1823
01:58:42,140 --> 01:58:46,740
ideas to fit with model based reinforcement learning is just use your favourite regression algorithm

1824
01:58:46,740 --> 01:58:51,000
to represent the transition function used the transition function the simulator and run your favourite

1825
01:58:51,000 --> 01:58:53,920
reinforcement learning algorithm on that to decide what to do

1826
01:58:53,970 --> 01:58:56,740
so this is the idea that actually goes way back i think it's in many

1827
01:58:56,740 --> 01:59:00,080
ways but under explored but i found in the first place i ever saw it

1828
01:59:00,080 --> 01:59:04,810
was in a paper by markets in charlotte that instance based models for control where

1829
01:59:04,820 --> 01:59:08,630
they say we can learn this problem which we now know as mountain car but

1830
01:59:08,650 --> 01:59:11,030
at that time they called bead on a wire

1831
01:59:11,050 --> 01:59:15,540
but i think they created the problem in that paper where you have to exert

1832
01:59:15,540 --> 01:59:17,860
force to get your car to go up to the top of the hill but

1833
01:59:17,860 --> 01:59:21,760
it's really high hill so you actually have to drive backwards get some momentum going

1834
01:59:21,760 --> 01:59:25,490
on and then come up to the top and a show that you can actually

1835
01:59:25,490 --> 01:59:32,990
k times t even now compare that with the general way the general equation as

1836
01:59:32,990 --> 01:59:37,230
follows at that yellow boxes over the standard linear form

1837
01:59:38,390 --> 01:59:40,730
how are they going to compare well

1838
01:59:41,450 --> 01:59:49,530
this is about 30 general function this is general a general function of the you

1839
01:59:49,530 --> 01:59:51,850
know I can make the external temperature

1840
01:59:51,870 --> 01:59:58,690
I can suppose it behaves in any way I like steadily rising decaying exponentially maybe

1841
01:59:58,690 --> 02:00:02,210
oscillating back and forth for some reason

1842
02:00:02,530 --> 02:00:07,990
the only way which is not general is that if K is a constant so

1843
02:00:08,500 --> 02:00:10,690
I will ask you to

1844
02:00:10,690 --> 02:00:18,110
we it let's imagine the conductivity is changing over time so this is usually constant

1845
02:00:18,120 --> 02:00:20,860
but there is no which says it has to be

1846
02:00:22,270 --> 02:00:30,110
how good conductivity change over time well we could suppose for example that the

1847
02:00:30,170 --> 02:00:39,080
so we can suppose that this was made of slowly congealing jails for instance you

1848
02:00:39,080 --> 02:00:44,780
know starts out as liquid that gets solid in the jell-o gel doesn't transmitting a

1849
02:00:44,780 --> 02:00:53,460
I believe quite as well as as a liquid diet as liquid is jealous solid

1850
02:00:53,500 --> 02:00:59,970
or liquid oxygen and once you about that

1851
02:01:00,930 --> 02:01:05,680
so with this understanding but so let's not necessarily

1852
02:01:05,950 --> 02:01:13,820
but not necessarily all I can think of this therefore by allowing can vary with

1853
02:01:13,820 --> 02:01:18,430
time and the external temperature vary with time I can think of it as a

1854
02:01:18,430 --> 02:01:24,250
general linear equation

1855
02:01:27,100 --> 02:01:32,250
so these models are not special shared fairly general

1856
02:01:33,910 --> 02:01:38,280
I did promise you won't solve an equation that this lecture I still have not

1857
02:01:38,280 --> 02:01:41,230
solved any equations OK

1858
02:01:41,970 --> 02:01:46,470
time to start temporizing and solve so I'm going to in order not to play

1859
02:01:46,470 --> 02:01:50,630
favorites with these 2 models all go back to and to get used to thinking

1860
02:01:50,630 --> 02:01:55,770
of new variables all the time that is not uh the eclectic switching from one

1861
02:01:55,770 --> 02:02:01,070
variable to another according to which a particular lecture you happened to be sitting here

1862
02:02:01,070 --> 02:02:05,570
and so let's take our equation in the form of why prime plus P of

1863
02:02:05,570 --> 02:02:13,250
X Y with general form using the old variables equal to Q of X solve

1864
02:02:13,270 --> 02:02:29,330
me well there are different ways of describing the solution process no matter how you

1865
02:02:29,330 --> 02:02:33,710
do it amounts to the same amount of work and there's always a trick involved

1866
02:02:33,710 --> 02:02:37,650
in each 1 of them since you can suppress the trick by doing the problem

1867
02:02:37,660 --> 02:02:39,910
some other way

1868
02:02:39,930 --> 02:02:44,430
the way I'm going to do it I think this is the best that's why

1869
02:02:44,430 --> 02:02:49,860
I'm giving it to you at least 2 it's easiest remember the least word but

1870
02:02:49,910 --> 02:02:54,480
by and colleagues who would fight with me on that point so since they are

1871
02:02:54,480 --> 02:02:58,320
not fight with me I am free to do whatever I like

1872
02:02:59,740 --> 02:03:02,250
1 of the main reasons is that

1873
02:03:02,570 --> 02:03:06,110
of doing it the way I'm going to do is because I want you to

1874
02:03:06,110 --> 02:03:12,630
get 1 lowered into your conscience attach consciousness and that's the word or 2 words

1875
02:03:12,630 --> 02:03:14,310
integrating factor

1876
02:03:14,360 --> 02:03:21,840
I'm going to solve this equation by finding an integrating factor of the form U

1877
02:03:21,840 --> 02:03:23,650
of X

1878
02:03:23,650 --> 02:03:29,230
was integrating factor well

1879
02:03:29,250 --> 02:03:34,800
show you the not by writing elaborate definition on the board which show you what

1880
02:03:34,800 --> 02:03:38,820
its function is it's a certain function of X I don't know what it is

1881
02:03:39,230 --> 02:03:42,750
but here's what I wanted to do I want to

1882
02:03:42,780 --> 02:03:49,760
multiplying under drop the axes of just so that the thing looks less complicated so

1883
02:03:49,760 --> 02:03:54,490
what I wanna do is multiply this equation ruled by U of X that's why

1884
02:03:54,490 --> 02:03:58,670
it's called the factor because you're going to multiply everything through by it so it's

1885
02:03:58,670 --> 02:04:01,990
gonna look like you why prime plus p

1886
02:04:04,890 --> 02:04:06,990
equals Q U

1887
02:04:08,670 --> 02:04:09,740
right now

1888
02:04:09,760 --> 02:04:15,470
this thing is so far it's just a fact what makes it integrating factor is

1889
02:04:15,470 --> 02:04:20,100
that this after I do that I want this to turn out to be the

