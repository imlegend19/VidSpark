1
00:00:00,000 --> 00:00:04,190
i have a

2
00:00:23,430 --> 00:00:34,460
you will be

3
00:00:37,270 --> 00:00:40,460
although he

4
00:01:03,480 --> 00:01:06,480
one of

5
00:01:06,500 --> 00:01:10,840
on the long run

6
00:01:10,840 --> 00:01:12,790
in proc

7
00:01:12,820 --> 00:01:14,740
it is

8
00:01:15,840 --> 00:01:20,370
this the result

9
00:02:16,700 --> 00:02:20,380
which a

10
00:02:23,060 --> 00:02:27,130
all which is

11
00:02:27,140 --> 00:02:29,990
so far from home

12
00:02:30,020 --> 00:02:32,600
he or even more

13
00:02:46,940 --> 00:02:51,250
and this is the

14
00:02:51,280 --> 00:02:55,480
we're going to do

15
00:03:02,660 --> 00:03:05,230
i mean

16
00:03:05,250 --> 00:03:07,570
are you

17
00:03:24,570 --> 00:03:27,770
and all of

18
00:03:29,750 --> 00:03:35,470
o o o

19
00:03:49,480 --> 00:03:51,280
well the

20
00:03:51,420 --> 00:03:55,220
with the mass

21
00:03:55,240 --> 00:03:57,840
this is just

22
00:04:03,960 --> 00:04:09,260
these are

23
00:04:13,430 --> 00:04:17,300
according to

24
00:04:21,020 --> 00:04:28,950
one of the

25
00:04:32,850 --> 00:04:37,390
belong to a class three or

26
00:04:39,650 --> 00:04:48,190
of the problem with

27
00:04:51,710 --> 00:04:53,470
so we

28
00:05:14,760 --> 00:05:17,500
there are some also

29
00:05:17,500 --> 00:05:19,100
let's rewind

30
00:05:19,130 --> 00:05:23,410
so the first question

31
00:05:23,420 --> 00:05:25,590
first question is do

32
00:05:25,590 --> 00:05:29,180
we do six six five o five

33
00:05:29,310 --> 00:05:30,680
so four

34
00:05:30,700 --> 00:05:36,830
this is three kilometres one the possible outcomes

35
00:05:40,180 --> 00:05:42,080
and this

36
00:05:42,080 --> 00:05:44,660
so that's the random variable with alphabet three

37
00:05:44,680 --> 00:05:46,320
possible values

38
00:05:46,420 --> 00:05:49,420
and let's shorey

39
00:05:49,440 --> 00:05:54,660
that's right down the probabilities are

40
00:05:54,740 --> 00:05:59,370
of those three outcomes the different things you can do a few a six against

41
00:05:59,370 --> 00:06:03,920
six what's the probability that does that zero

42
00:06:03,940 --> 00:06:07,980
by symmetry the other properties must be high

43
00:06:07,980 --> 00:06:11,870
high if you a four against before leaving for

44
00:06:11,890 --> 00:06:14,070
balls on the table

45
00:06:14,080 --> 00:06:17,610
what's the probability that does that

46
00:06:17,630 --> 00:06:20,640
it's got to be one of the four you left on the table has four

47
00:06:20,640 --> 00:06:25,380
of them is the third right the probability that balances

48
00:06:25,410 --> 00:06:28,930
is the third by symmetry is a third of the

49
00:06:28,980 --> 00:06:30,630
this and this

50
00:06:30,660 --> 00:06:36,350
what's the probability let's right this another way we could write this as

51
00:06:37,070 --> 00:06:40,330
six twelve six twelve

52
00:06:40,330 --> 00:06:41,310
this is

53
00:06:41,320 --> 00:06:42,900
four twelve

54
00:06:42,920 --> 00:06:45,070
four twelve

55
00:06:45,110 --> 00:06:52,890
so we define against five live two

56
00:06:52,940 --> 00:06:54,040
on the table

57
00:06:54,050 --> 00:06:57,590
what's the probability that balances

58
00:06:59,430 --> 00:07:05,350
as the number we live on the table over twelve fairly obviously six over twelve

59
00:07:06,980 --> 00:07:14,920
ten twelve and then the remaining staff divided into five

60
00:07:16,510 --> 00:07:19,350
three o twelve three

61
00:07:19,370 --> 00:07:22,150
two thousand two

62
00:07:22,160 --> 00:07:23,730
one hundred twelve

63
00:07:23,730 --> 00:07:27,820
the twelve OK we need a little leonardo only we need to know

64
00:07:27,890 --> 00:07:31,410
how do you maximise the favour probability distribution

65
00:07:31,490 --> 00:07:35,930
so have attached your neighbour if you've got to pay diffusion and so as to

66
00:07:35,930 --> 00:07:38,790
maximize entropy change your name

67
00:07:58,850 --> 00:08:00,930
one over i

68
00:08:00,980 --> 00:08:02,930
for all i

69
00:08:02,990 --> 00:08:09,140
and you are not agree with uniform and on agree with for a brilliant so

70
00:08:09,140 --> 00:08:12,790
which of these has maximum entropy

71
00:08:12,800 --> 00:08:13,650
this one

72
00:08:13,670 --> 00:08:18,500
four against so shannon tells us way for against don't bother looking at other bits

73
00:08:18,670 --> 00:08:22,290
this is the way to get max information so we chose to wait for the

74
00:08:22,290 --> 00:08:23,540
for john get

75
00:08:23,540 --> 00:08:24,740
and now we

76
00:08:24,750 --> 00:08:27,310
struggle possibly with the next step

77
00:08:27,370 --> 00:08:30,460
so let's look at these let's work out

78
00:08:30,480 --> 00:08:33,230
what's the probability of

79
00:08:36,740 --> 00:08:41,940
and this

80
00:08:41,960 --> 00:08:45,180
OK so suggestion number one was

81
00:08:45,230 --> 00:08:49,060
during her home on k

82
00:08:49,080 --> 00:08:50,650
so how do we

83
00:08:50,670 --> 00:08:53,350
think about this let's think about the

84
00:08:53,350 --> 00:08:58,770
probability of a balancing first it will balance

85
00:08:58,820 --> 00:09:01,090
if one of these actually about

86
00:09:01,160 --> 00:09:02,850
and the three candidates

87
00:09:02,900 --> 00:09:06,880
and the total number of candidates that could be that is a moment so must

88
00:09:06,880 --> 00:09:08,000
be a three

89
00:09:08,000 --> 00:09:10,350
a chance of

90
00:09:15,700 --> 00:09:17,400
what's the probability of

91
00:09:19,790 --> 00:09:22,640
of that could happen

92
00:09:22,670 --> 00:09:26,450
this is the bad guy and is having

93
00:09:27,530 --> 00:09:31,810
if this is the bad guy and his life could happen in any way

94
00:09:34,520 --> 00:09:37,610
it's and then let's just checked that adds up

95
00:09:37,630 --> 00:09:41,560
how could this happen what happen this or this or this is the bad guy

96
00:09:41,560 --> 00:09:44,850
and that's all that's three a and that sums to one of is a good

97
00:09:44,850 --> 00:09:46,990
way to check yourself

98
00:09:47,900 --> 00:09:50,410
that's impossible strategy

99
00:09:55,550 --> 00:10:04,870
let's see that anyone yes this is a good let's look at three

100
00:10:04,900 --> 00:10:06,810
what's the probability

101
00:10:06,810 --> 00:10:11,570
this will balance if you go strategy three as your second

102
00:10:14,510 --> 00:10:16,110
for over a

103
00:10:17,560 --> 00:10:20,040
what's the probability that all the

104
00:10:25,590 --> 00:10:27,300
four right

105
00:10:27,320 --> 00:10:29,830
what's is the probability that will do the other things

106
00:10:33,370 --> 00:10:34,870
what's the entropy

107
00:10:34,880 --> 00:10:38,920
of the distribution can you do it in your head

108
00:10:38,940 --> 00:10:41,420
one bit

109
00:10:41,430 --> 00:10:43,520
OK so we this

110
00:10:43,530 --> 00:10:44,870
is one that

111
00:10:44,900 --> 00:10:48,750
we actually don't care exactly what NPOV just maximizing

112
00:10:48,750 --> 00:10:51,740
how does the entropy of two three three

113
00:10:51,750 --> 00:10:54,890
compare if you want to share a piece of cake between three children is it

114
00:10:54,890 --> 00:10:59,820
fair to given for four there are three three two

115
00:10:59,830 --> 00:11:05,710
i think you get the three three two yes so this is not entirely

116
00:11:07,490 --> 00:11:11,420
people often get the second stage of this wrong they do four against for first

117
00:11:11,690 --> 00:11:13,990
and then the second step

118
00:11:14,030 --> 00:11:15,330
they screwed up

119
00:11:15,340 --> 00:11:17,680
and they don't get

120
00:11:17,700 --> 00:11:19,190
three three two

121
00:11:19,300 --> 00:11:24,510
is very common that this is an extreme case will be quite often people do

122
00:11:24,510 --> 00:11:26,500
four two two

123
00:11:26,520 --> 00:11:30,150
that's a fairly common thing for example let's imagine the first outcome was a balance

124
00:11:30,440 --> 00:11:34,070
so balance as long

125
00:11:34,120 --> 00:11:37,010
and so we know it's one of these four on the table but we don't

126
00:11:37,010 --> 00:11:38,340
know which one

127
00:11:38,340 --> 00:11:42,130
about carla

128
00:11:42,180 --> 00:11:45,450
we call that she she was an undergraduate students

129
00:11:45,510 --> 00:11:49,060
o for the nice

130
00:11:49,100 --> 00:11:54,330
we see something very similar so these also student but in student and you have

131
00:11:54,330 --> 00:11:58,260
not seen and you have different advisor namely e

132
00:11:58,270 --> 00:11:59,670
for these

133
00:11:59,690 --> 00:12:03,840
two ladies are related to you in our in our

134
00:12:03,840 --> 00:12:06,800
ables already

135
00:12:06,850 --> 00:12:11,450
and for the supervisor of color we have also something already stated in our

136
00:12:11,480 --> 00:12:13,620
you know a book namely

137
00:12:13,630 --> 00:12:17,930
eve is an assistant professor

138
00:12:17,940 --> 00:12:20,380
OK so to make this

139
00:12:20,450 --> 00:12:24,740
i think he also look what we know about fiona we now know already that

140
00:12:24,740 --> 00:12:28,950
she has supervisor of nice but we also know about feeling that is a full

141
00:12:30,760 --> 00:12:33,710
OK when you want to compute the LSE

142
00:12:33,770 --> 00:12:38,380
what is to be what is taken into account is

143
00:12:38,390 --> 00:12:42,670
also the concept definitions that we might already have on the tee box

144
00:12:42,690 --> 00:12:46,840
so if you look now at the taxonomy and the concepts that we have to

145
00:12:46,840 --> 00:12:49,370
find them

146
00:12:51,270 --> 00:12:53,090
then we can look at

147
00:12:53,180 --> 00:12:57,690
the graduate student for instance we know that ten is a graduate student and

148
00:12:57,730 --> 00:13:02,120
take a look at how this concept is defined so what we see here is

149
00:13:02,120 --> 00:13:06,370
a lot of information so the credit student has told primitive divisional head

150
00:13:07,580 --> 00:13:11,010
definition from the user that

151
00:13:11,050 --> 00:13:12,980
it is

152
00:13:13,060 --> 00:13:15,020
the kind of person

153
00:13:16,820 --> 00:13:18,170
this is something that

154
00:13:18,190 --> 00:13:21,160
it takes the course and that is a graduate course

155
00:13:21,200 --> 00:13:25,350
and also get some other information about synonyms and parents and children which gives us

156
00:13:25,350 --> 00:13:30,530
information about the taxonomy but this doesn't interest us from now

157
00:13:30,530 --> 00:13:33,180
OK so what's

158
00:13:33,200 --> 00:13:42,970
the expected to seek to be taken into account and then have to look quickly

159
00:13:43,030 --> 00:13:50,190
who was the r v evans assistant professors so we can also see

160
00:13:50,210 --> 00:13:53,580
what we know and that he was only about assistant professors

161
00:13:54,150 --> 00:13:59,380
we can describe this concept and say

162
00:13:59,450 --> 00:14:01,440
see that

163
00:14:01,480 --> 00:14:06,410
look at the description of concept and see that this is an professor is actually

164
00:14:06,420 --> 00:14:08,550
kind of professors and

165
00:14:08,550 --> 00:14:13,790
it's actually collapsing with our coinciding with the content

166
00:14:15,590 --> 00:14:16,390
for now

167
00:14:16,420 --> 00:14:17,850
if you are interested in

168
00:14:17,850 --> 00:14:21,770
and seeing what

169
00:14:21,780 --> 00:14:26,140
happens if we generalize this these individuals so we know the is linked if you

170
00:14:26,140 --> 00:14:30,710
know which who is kind of professor then we can simply is beautiful but in

171
00:14:30,710 --> 00:14:37,490
here which computes all which invokes the evening service and generalizes now this kind of

172
00:14:37,490 --> 00:14:40,450
individuals into concept description

173
00:14:40,470 --> 00:14:41,620
so we do this now

174
00:14:41,620 --> 00:14:43,760
and we obtain

175
00:14:45,630 --> 00:14:48,970
carl is the kind of undergraduate OK this is something we have

176
00:14:49,120 --> 00:14:54,400
directly attached to to come forward speak and and then comes something that boils down

177
00:14:54,400 --> 00:14:58,320
from the definition of undergraduatestudent namely that you the person

178
00:14:58,380 --> 00:14:59,390
and there's something

179
00:15:01,540 --> 00:15:03,910
today the because which is the cause

180
00:15:04,120 --> 00:15:08,140
but there's also more that is being discovered by

181
00:15:08,150 --> 00:15:11,540
by sonic by has been found out by the system

182
00:15:11,550 --> 00:15:15,740
namely that you have some some kind of advice OK we said that's the feeling

183
00:15:15,740 --> 00:15:19,150
as her advisor but

184
00:15:19,170 --> 00:15:23,530
also the things that we know about fionnuala coming into play and you see that

185
00:15:23,530 --> 00:15:26,790
if you know the full professors and

186
00:15:26,830 --> 00:15:31,910
see also things that were used in the definition of full professor

187
00:15:31,970 --> 00:15:36,340
and this is an also contributing to the most specific concept that you find out

188
00:15:36,650 --> 00:15:39,180
about color

189
00:15:39,190 --> 00:15:43,140
often not have done that

190
00:15:43,190 --> 00:15:47,270
OK so here on outcomes

191
00:15:47,280 --> 00:15:51,920
OK now comes the window where we can actually edit the concept description but for

192
00:15:51,930 --> 00:15:52,680
now we

193
00:15:52,710 --> 00:15:56,560
we take its own

194
00:15:58,030 --> 00:16:02,330
when i would take it as it is

195
00:16:11,340 --> 00:16:15,340
so we say maybe

196
00:16:15,440 --> 00:16:18,450
with the concept that we cut out of college

197
00:16:18,470 --> 00:16:21,160
and this is our carla concept

198
00:16:21,170 --> 00:16:22,800
you should also

199
00:16:22,800 --> 00:16:25,610
at the correct namespace is really useful

200
00:16:25,730 --> 00:16:27,850
again the

201
00:16:27,860 --> 00:16:30,580
and our data

202
00:16:30,590 --> 00:16:32,080
you can click OK

203
00:16:32,100 --> 00:16:33,810
and looking at in our

204
00:16:38,700 --> 00:16:42,120
well this concept shows up and we see in fact

205
00:16:42,120 --> 00:16:43,040
we have now

206
00:16:43,060 --> 00:16:47,900
find the kind of undergraduate students

207
00:16:47,960 --> 00:16:48,770
of you

208
00:16:48,770 --> 00:16:51,840
we have found another kind of undergraduate student which is

209
00:16:51,850 --> 00:16:56,360
the color concept that we have a right derived from the individual

210
00:16:56,370 --> 00:16:59,780
OK i quickly just do this also with the other individuals

211
00:16:59,800 --> 00:17:01,160
so for the nice

212
00:17:01,220 --> 00:17:05,370
the also say want to see the MSE

213
00:17:05,680 --> 00:17:07,640
even if it's

214
00:17:07,690 --> 00:17:11,290
having the message

215
00:17:11,330 --> 00:17:17,010
and if you recall the needs of infected graduate students q OK

216
00:17:17,030 --> 00:17:21,990
and here we see also literary adviser is the kind of assistant professors are not

217
00:17:22,090 --> 00:17:26,630
full professor in the other case

218
00:17:26,680 --> 00:17:28,380
and you can see

219
00:17:28,410 --> 00:17:30,200
we can then this

220
00:17:30,210 --> 00:17:35,020
against the correct namespace

221
00:17:36,740 --> 00:17:39,050
constant OK and we added

222
00:17:39,050 --> 00:17:46,110
to knowledge base is standard step into this also for the other guys

223
00:17:46,180 --> 00:17:50,820
and the see OK find out about you basically what is captured in the definition

224
00:17:50,820 --> 00:17:53,800
of assistant professor associate professor

225
00:17:54,060 --> 00:17:55,660
this number of issues

226
00:17:55,720 --> 00:17:59,490
then faculty and person and she works what kind of organisation

227
00:17:59,530 --> 00:18:03,400
which in this case most likely is the university

228
00:18:05,150 --> 00:18:07,980
unions also quickly

229
00:18:08,020 --> 00:18:15,530
the eve concepts

230
00:18:15,650 --> 00:18:18,050
we do this for the fourth one

231
00:18:19,760 --> 00:18:23,770
and here we see OK we find again what what has been

232
00:18:23,810 --> 00:18:33,550
supplied by the definition so for these

233
00:18:40,470 --> 00:18:45,510
if you look now again detects enemy

234
00:18:45,510 --> 00:18:46,980
we we see that

235
00:18:47,060 --> 00:18:49,110
actually these quantities

236
00:18:49,120 --> 00:18:54,640
refining our gradient student and the colour concept is refining undergraduatestudent

237
00:18:54,660 --> 00:18:55,640
but see

238
00:18:55,660 --> 00:18:57,870
that he

239
00:18:58,150 --> 00:19:04,030
really surprisingly the concept of assistant professor coincides with the if concept since we didn't

240
00:19:04,030 --> 00:19:05,620
know anything more about it

241
00:19:05,620 --> 00:19:10,360
but the chief in system professor cilliers was again attacked by the c

242
00:19:10,360 --> 00:19:12,150
this was is easy

243
00:19:12,410 --> 00:19:16,920
that we were trying to optimize i can actually write an upper bound on the

244
00:19:16,920 --> 00:19:17,740
scene now

245
00:19:17,790 --> 00:19:20,900
which is now linear

246
00:19:20,910 --> 00:19:22,340
in terms of the

247
00:19:22,360 --> 00:19:29,290
observations here over all possible lives now this is great i can perform just simple

248
00:19:29,290 --> 00:19:37,530
dynamic programming programming pass in n compute the optimal weight updates for or can't compute

249
00:19:37,530 --> 00:19:41,850
dizzy value for all part or all features

250
00:19:41,860 --> 00:19:45,370
OK and because i'm trying to minimize this now

251
00:19:45,390 --> 00:19:50,860
with one part one dynamic pro programming has i can actually find ways which of

252
00:19:50,890 --> 00:19:54,370
feature selection include it the situation into my own somewhat

253
00:19:54,420 --> 00:19:58,530
and when i have the value when i have that feature

254
00:20:00,790 --> 00:20:08,410
finding the optimal weight update is just an analytic computation

255
00:20:08,430 --> 00:20:14,300
now one problem with this is that it's in sequences these upper and lower bounds

256
00:20:14,300 --> 00:20:20,800
are can can vary from negative that the negative of the length of this sequence

257
00:20:21,070 --> 00:20:25,340
to positive of the length of the sequence so this can be say minus forty

258
00:20:25,340 --> 00:20:29,080
to forty and this this found is going to be almost useless very it's going

259
00:20:29,080 --> 00:20:31,030
to be extremely

260
00:20:32,460 --> 00:20:36,620
one alternative is instead of this

261
00:20:36,630 --> 00:20:41,550
this nearby we can have a piecewise linear bound that is defined in terms of

262
00:20:41,740 --> 00:20:49,470
each sentence now so if we have various sentence sequences of varying lengths then we

263
00:20:49,470 --> 00:20:52,920
might we might actually have much tighter bounds

264
00:20:53,770 --> 00:21:00,090
so what we can define a piecewise linear bounds now the problem is we don't

265
00:21:00,090 --> 00:21:02,900
have an analytic solution in terms of the

266
00:21:03,780 --> 00:21:09,630
optimal weight updates by the this is just one dimensional optimisation problem and it's

267
00:21:09,680 --> 00:21:14,260
really fast college in a couple of iterations you can in a solution

268
00:21:14,330 --> 00:21:16,120
and a

269
00:21:17,180 --> 00:21:22,590
a better approach is what we have selected which feature we're gonna use we can

270
00:21:22,590 --> 00:21:24,470
actually go back to the order of

271
00:21:24,870 --> 00:21:27,350
the optimisation problem the original z

272
00:21:27,360 --> 00:21:31,540
and finally the optimal way of a new

273
00:21:31,550 --> 00:21:37,840
on that the function OK so we can we can perform an exact optimisation maybe

274
00:21:37,860 --> 00:21:42,850
the that we've selected is not optimal one but we can we can at least

275
00:21:43,020 --> 00:21:48,180
for the feature we have selected we can at least take an exact of

276
00:21:49,280 --> 00:21:53,900
updates that

277
00:21:53,920 --> 00:21:58,070
OK it's it's it's too real

278
00:21:59,130 --> 00:22:05,480
general is that it is very trivial to show that the structured adaboost approach maximizes

279
00:22:05,480 --> 00:22:10,480
the number of training instances with large margin as the standard adaboost has been doing

280
00:22:10,490 --> 00:22:14,840
if there is a possible positive margin achievable

281
00:22:14,850 --> 00:22:18,380
and we can also show that the generalisation bounds

282
00:22:18,430 --> 00:22:23,100
the generalisation error can be bounded in terms of the the number of training instances

283
00:22:23,100 --> 00:22:29,690
with some margin plus a term that grows linearly with the length of the

284
00:22:29,890 --> 00:22:33,790
the with the size of her structure

285
00:22:33,830 --> 00:22:36,960
all right i'm going to going to

286
00:22:36,980 --> 00:22:40,620
some experiments but one one

287
00:22:40,670 --> 00:22:42,780
method that i'm gonna compare

288
00:22:42,800 --> 00:22:45,550
the second the structured boosting

289
00:22:45,570 --> 00:22:46,580
it is

290
00:22:46,630 --> 00:22:52,210
is this approach here so here we are considering a sequence matter and i will

291
00:22:52,210 --> 00:22:59,180
be presenting experiments in sequences in this work will be

292
00:22:59,190 --> 00:23:04,260
and we should bear in city centre had proposed training new boosting

293
00:23:04,270 --> 00:23:08,330
so in sequences we can actually use

294
00:23:08,330 --> 00:23:10,010
we can actually ignore

295
00:23:10,170 --> 00:23:17,270
any sequence information rate and train train a classifier for each position in in your

296
00:23:18,630 --> 00:23:24,550
so this will be knowing any any sequence information it in more advanced approach we

297
00:23:24,550 --> 00:23:26,390
might consider OK

298
00:23:26,400 --> 00:23:27,180
during training

299
00:23:27,200 --> 00:23:32,770
you know what the correct neighbouring of what the correct neighboring label is so i

300
00:23:32,770 --> 00:23:36,970
can use that is given to me and then changed my stance classifier

301
00:23:36,980 --> 00:23:42,430
stable state and then during testing now i don't know the correct label but i

302
00:23:42,430 --> 00:23:44,830
can do if itterby like optimisation

303
00:23:44,870 --> 00:23:50,040
right and this is the method proposed here and in their experiments they actually show

304
00:23:50,060 --> 00:23:56,370
that this approach performs worse than an independent classification approach so if i were to

305
00:23:56,370 --> 00:24:00,230
completely ignore the sequence information i would have gotten the better

306
00:24:00,250 --> 00:24:03,130
better results the reason here is that

307
00:24:03,410 --> 00:24:08,290
during training and using some information is not going to be available to me during

308
00:24:08,290 --> 00:24:10,120
testing exactly

309
00:24:10,130 --> 00:24:11,960
i'm using some some

310
00:24:11,990 --> 00:24:17,030
information that i'm going to be predicting so it's going to include some nice and

311
00:24:17,030 --> 00:24:20,280
this line is actually ignored during training

312
00:24:20,290 --> 00:24:26,250
right so this this group discrepancy results in a worse performance than

313
00:24:27,220 --> 00:24:31,450
well considering any dependency information

314
00:24:31,460 --> 00:24:32,940
and one

315
00:24:32,980 --> 00:24:38,530
one of previous work than than the one that i just presented its causes approach

316
00:24:38,530 --> 00:24:41,630
where he uses an external classifier

317
00:24:42,400 --> 00:24:45,830
so to select some best labels

318
00:24:45,850 --> 00:24:51,850
in this case he works on passing so he uses an external passing to select

319
00:24:51,850 --> 00:24:56,640
and best parts list and then changes standard boosting algorithm

320
00:24:56,650 --> 00:25:01,830
now this approach has the advantage that dizzy now can be computed exactly because it's

321
00:25:01,830 --> 00:25:04,370
just cook it's just accounting

322
00:25:04,420 --> 00:25:09,360
meta rate because the sum is over a small subset

323
00:25:09,400 --> 00:25:10,970
the sum over c

324
00:25:12,840 --> 00:25:14,250
over the labels

325
00:25:14,270 --> 00:25:18,200
in disease now or in limited set

326
00:25:18,210 --> 00:25:24,460
so it is going to be more efficient but the disadvantage of this approach is

327
00:25:24,460 --> 00:25:29,480
that it's going to need an external classifier so he uses his already very good

328
00:25:29,480 --> 00:25:31,940
of p is a PCFG parse

329
00:25:33,900 --> 00:25:37,310
you know in general we want have such an that we want having access to

330
00:25:37,310 --> 00:25:39,460
such a classifier

331
00:25:42,640 --> 00:25:47,350
so let's see the experiment some pitch accent prediction

332
00:25:47,370 --> 00:25:51,810
so i'm saying independent of boosting classifiers

333
00:25:51,830 --> 00:25:55,540
and i get seventy one percent class error

334
00:25:55,560 --> 00:25:59,070
o accuracy if i if i change

335
00:25:59,090 --> 00:26:03,940
of the metadata just described in hackney at all paper is you see a a

336
00:26:03,940 --> 00:26:07,220
lose some accuracy in the worst and then

337
00:26:07,420 --> 00:26:13,390
you know just ignoring the dependency information now i look at the CRF model

338
00:26:13,390 --> 00:26:15,420
and it does pretty well OK

339
00:26:15,430 --> 00:26:17,880
computing the

340
00:26:17,890 --> 00:26:24,060
eg exponential loss exactly using some of the shelf gradient matter it does pretty well

341
00:26:24,340 --> 00:26:28,370
and so the structured boosting also does

342
00:26:28,380 --> 00:26:32,260
equally well the advantage of the boosting is obviously that is going to have a

343
00:26:32,260 --> 00:26:35,660
sparse representation and if you look at it on

344
00:26:35,730 --> 00:26:40,080
ten percent of the features are selected now this is going to be important for

345
00:26:40,080 --> 00:26:41,710
four tasks were

346
00:26:41,770 --> 00:26:46,490
the the prediction time is is really crucial

347
00:26:46,510 --> 00:26:49,240
so this will be an event stages

348
00:26:49,260 --> 00:26:53,330
method for for such problem

349
00:26:53,340 --> 00:26:55,290
for such a task

350
00:26:55,350 --> 00:26:57,120
all right i'm going to show

351
00:26:57,120 --> 00:27:00,350
it's the

352
00:27:00,360 --> 00:27:03,880
it's a logical sort of importance

353
00:27:03,910 --> 00:27:09,040
OK now the basic postulates and usually we have another two supplementary PUS let one

354
00:27:09,040 --> 00:27:11,890
sort of explain now but essentially what they

355
00:27:11,900 --> 00:27:15,760
are just generalizations of some of the only apostles for

356
00:27:18,140 --> 00:27:22,400
OK now in particular good in the following

357
00:27:23,260 --> 00:27:26,980
i want to give up the sentence inside

358
00:27:27,000 --> 00:27:30,320
it's sufficient for me to give up one of the two

359
00:27:32,300 --> 00:27:36,170
so if i want to give up the conjunction phi and psi will adjusting them

360
00:27:36,170 --> 00:27:39,560
to give up five or any to give up so

361
00:27:39,610 --> 00:27:42,880
on the other hand if i want to give up the disjunction five also going

362
00:27:42,880 --> 00:27:46,420
to give up both file size because if i leave one of them around will

363
00:27:46,420 --> 00:27:51,160
phi implies file size and sign players file size so i can leave either them

364
00:27:51,160 --> 00:27:53,000
around going to give up

365
00:27:54,300 --> 00:27:57,900
just keep that in mind because people get find side give give up but now

366
00:27:57,900 --> 00:27:58,690
it's not

367
00:27:58,750 --> 00:28:00,160
that literally

368
00:28:00,220 --> 00:28:03,130
you want to give up the conjunction you gotta give up one of the other

369
00:28:03,180 --> 00:28:05,940
the disjunction have to give up

370
00:28:06,020 --> 00:28:10,910
OK i'll leave this here for you to look at some consequences of those properties

371
00:28:11,270 --> 00:28:15,410
and escape to because otherwise like

372
00:28:15,450 --> 00:28:18,300
OK so

373
00:28:18,340 --> 00:28:22,040
and then we got a set of properties and they seem to look reasonably intuitive

374
00:28:22,050 --> 00:28:23,570
i hope

375
00:28:23,680 --> 00:28:27,590
and i have tried to sort of vaguely argue that they also satisfy those rationality

376
00:28:28,530 --> 00:28:30,340
now i want to do is we want to see

377
00:28:30,350 --> 00:28:32,630
how can we obtain the function

378
00:28:32,710 --> 00:28:36,820
that actually satisfies those properties was laid down on the properties of the function should

379
00:28:38,020 --> 00:28:41,630
NASA the states we're not interested in just figuring out one function like we have

380
00:28:41,640 --> 00:28:42,840
for expansion

381
00:28:42,890 --> 00:28:49,870
we're really just interested in sort delineating all circumscribing the class of possible functions that

382
00:28:49,940 --> 00:28:53,460
satisfy this property may not be one

383
00:28:53,510 --> 00:28:55,230
OK so

384
00:28:55,430 --> 00:28:59,430
how we can construct such a function we can store all the possible mappings

385
00:28:59,510 --> 00:29:02,860
the first idea is just to look at the beliefs

386
00:29:02,870 --> 00:29:05,420
and try and look at a set which is

387
00:29:05,430 --> 00:29:08,710
as similar as possible to the original beliefs

388
00:29:08,720 --> 00:29:11,720
but doesn't imply the formula that trying to give up

389
00:29:11,900 --> 00:29:18,350
can we can formally with this

390
00:29:18,360 --> 00:29:20,560
the notion of remain the same

391
00:29:20,670 --> 00:29:24,830
also go through a very briefly but the idea is that we do we define

392
00:29:24,830 --> 00:29:29,060
the set k primes in a sense what k prime is is so small it's

393
00:29:29,060 --> 00:29:32,170
sort of the largest subset of k

394
00:29:32,210 --> 00:29:35,830
it doesn't imply the former trying to contract

395
00:29:35,870 --> 00:29:38,110
so if we're trying to contract by phi

396
00:29:38,150 --> 00:29:42,340
it's the largest subset of k doesn't imply five

397
00:29:42,340 --> 00:29:45,520
and you can formally defined in the following way it's OK prime must be a

398
00:29:45,520 --> 00:29:46,890
subset of k

399
00:29:46,940 --> 00:29:50,250
five can be in k prime and furthermore

400
00:29:50,930 --> 00:29:53,860
properties equivalent to sort of saying that there is no

401
00:29:53,890 --> 00:29:56,610
set which is larger than k primes

402
00:29:56,660 --> 00:30:00,540
and smaller than k which also satisfies the first two properties

403
00:30:03,480 --> 00:30:04,430
of course

404
00:30:04,440 --> 00:30:08,810
from the examples that we've seen already it's fairly obvious that more than one candidate

405
00:30:09,570 --> 00:30:12,070
so there are several of these candidates

406
00:30:12,120 --> 00:30:13,840
the largest

407
00:30:14,130 --> 00:30:18,110
subsets of k don't qualify

408
00:30:18,140 --> 00:30:23,220
we grew all the possible subsets into this cave bottom five

409
00:30:23,230 --> 00:30:26,780
which is the set of all blacks nineteen playing subset so these are all the

410
00:30:26,780 --> 00:30:29,100
subsets are as large as possible

411
00:30:29,100 --> 00:30:32,380
yet smaller than came down platform

412
00:30:32,430 --> 00:30:35,960
now also what we do is we've got this notion of selection function that can

413
00:30:35,960 --> 00:30:38,890
select elements of k bottom five depending on

414
00:30:38,900 --> 00:30:41,710
the city which were

415
00:30:41,870 --> 00:30:42,670
if if

416
00:30:42,760 --> 00:30:49,360
this selection function selects one of these maximum implying something called an opinionated substances

417
00:30:49,370 --> 00:30:55,390
sorry opinionated selection function you see why

418
00:30:55,550 --> 00:31:00,610
i the first ideas this notion of what's called maxi choice contraction and essentially what

419
00:31:00,610 --> 00:31:03,330
makes features contraction does it just takes

420
00:31:03,370 --> 00:31:06,930
one of these maximal implying some this is the one you're is where you get

421
00:31:08,230 --> 00:31:09,290
and that

422
00:31:09,290 --> 00:31:12,600
is that because it answer reasonable

423
00:31:12,610 --> 00:31:15,640
OK go to figure out which ones OK we're going to delay the choice which

424
00:31:15,640 --> 00:31:21,450
is going to prove which is going to choose one of these maximum points some

425
00:31:21,510 --> 00:31:24,100
we have that

426
00:31:24,110 --> 00:31:27,930
what sort of reasonable right because we giving up as little as possible

427
00:31:27,980 --> 00:31:31,060
six away five so it does make some sense

428
00:31:31,060 --> 00:31:33,350
and sure enough

429
00:31:33,840 --> 00:31:37,760
if we got function we defined in this way it satisfies properties

430
00:31:38,110 --> 00:31:40,370
but we have a problem here

431
00:31:40,390 --> 00:31:42,920
the problem is the following one

432
00:31:46,850 --> 00:31:49,360
OK it's going to take quick digression if we

433
00:31:49,370 --> 00:31:50,820
want to look at

434
00:31:50,870 --> 00:31:54,040
if you want to try to find the revision function

435
00:31:54,190 --> 00:31:56,940
one revised k by fire rather than just

436
00:31:57,110 --> 00:31:58,720
contracting by five

437
00:31:58,770 --> 00:32:02,190
then one way to do that is to apply what is often called believe identity

438
00:32:02,190 --> 00:32:05,150
and the basic idea is to take your sensibilities

439
00:32:05,180 --> 00:32:11,110
and remove any impediment to you believing finally any impediments to you believing final five

440
00:32:11,110 --> 00:32:14,920
because if you retain not five accountability not find you retain

441
00:32:14,980 --> 00:32:17,700
then you can end up in an inconsistent beliefs

442
00:32:17,750 --> 00:32:19,940
so what you do is you may not find

443
00:32:20,080 --> 00:32:24,840
once you may not find well you can happily just fine

444
00:32:24,850 --> 00:32:27,920
OK so one way of defining the revision

445
00:32:27,980 --> 00:32:30,180
it's just by saying look

446
00:32:30,190 --> 00:32:31,790
take away not five

447
00:32:31,790 --> 00:32:35,530
and then i can easily just that financial will be consistent

448
00:32:35,540 --> 00:32:39,510
while remember this is that the closer there's going be no way in which this

449
00:32:39,510 --> 00:32:40,960
is going to run

450
00:32:41,430 --> 00:32:44,500
close contradiction which are not for

451
00:32:44,540 --> 00:32:46,250
OK now if you

452
00:32:46,300 --> 00:32:51,200
if the minus the use here is maxi choice

453
00:32:51,270 --> 00:32:55,150
then we end up with the following problem that one week one we define revision

454
00:32:55,150 --> 00:32:57,900
operator in this way

455
00:32:57,960 --> 00:33:02,290
and if i take any finer revised k by five

456
00:33:02,350 --> 00:33:04,850
if i look at any other sentence in my language

457
00:33:04,900 --> 00:33:08,310
i'm going to end up with the revision in which i believe that sentence or

458
00:33:08,310 --> 00:33:13,580
i believe its negation

459
00:33:13,670 --> 00:33:17,770
which so i'm sort of at the end of his favourite think this sentence being

460
00:33:17,770 --> 00:33:20,270
light source so

461
00:33:20,520 --> 00:33:25,900
for any sentence i'm going to end up with this possibility either

462
00:33:26,000 --> 00:33:29,400
site is an element of k stuff

463
00:33:33,150 --> 00:33:34,540
it's an element of

464
00:33:37,310 --> 00:33:38,600
OK so that

465
00:33:38,670 --> 00:33:40,270
that look problematic

466
00:33:40,310 --> 00:33:45,270
so look back

467
00:33:45,290 --> 00:33:50,170
you see why this might be a problem

468
00:33:50,190 --> 00:33:53,400
well it might be a problem because initially i might not have had any opinion

469
00:33:54,670 --> 00:33:56,540
size any former remember

470
00:33:56,560 --> 00:33:59,580
i might not have had any opinion about with this i was true not suddenly

471
00:33:59,580 --> 00:34:00,790
i have an opinion

472
00:34:00,880 --> 00:34:04,420
not only is to whether size true because as service this happens to be the

473
00:34:04,420 --> 00:34:09,020
case for also suddenly i have an opinion as to the truth or falsity of

474
00:34:09,020 --> 00:34:11,310
every sentence in language

475
00:34:11,400 --> 00:34:14,500
certainly believe that it's either true or false

476
00:34:14,560 --> 00:34:18,670
so from perhaps you know not knowing very much suddenly belief that everything that's why

477
00:34:18,670 --> 00:34:21,670
this selection function is called opinionated because

478
00:34:21,750 --> 00:34:25,730
from having maybe a few opinions now you have an opinion on every

479
00:34:25,730 --> 00:34:34,430
i think it depends on the prior to

480
00:34:37,580 --> 00:34:41,280
second have a uniform prior

481
00:34:45,030 --> 00:34:52,350
so you think the answer depends on the prior

482
00:34:52,370 --> 00:35:00,800
i mean you're right

483
00:35:01,860 --> 00:35:06,040
i mean this paradox there i mean it's there

484
00:35:06,060 --> 00:35:08,870
look like fun but i mean if you if you

485
00:35:08,920 --> 00:35:11,780
can you know so this simple problem from in

486
00:35:11,790 --> 00:35:13,110
how could you

487
00:35:13,300 --> 00:35:17,040
single for the more complex problems i mean people i mean to do something and

488
00:35:17,040 --> 00:35:19,270
sometimes it works i mean that

489
00:35:19,290 --> 00:35:24,350
it's also an approach but you know maybe you really want to principle way

490
00:35:24,420 --> 00:35:27,420
and let maybe you have some proof that works

491
00:35:28,350 --> 00:35:29,900
OK so

492
00:35:29,920 --> 00:35:32,630
let's come now to the bayes mixture distribution

493
00:35:32,650 --> 00:35:36,170
with the smallest evidence or the marginal

494
00:35:36,480 --> 00:35:38,370
and so on

495
00:35:38,380 --> 00:35:43,800
assume now you take a sample from the distribution and this vision is called move

496
00:35:43,850 --> 00:35:45,130
my talk

497
00:35:45,230 --> 00:35:48,980
but this distribution is not known because it's known everything

498
00:35:49,030 --> 00:35:53,050
OK so the bayesian approaches that more less you're place

499
00:35:53,060 --> 00:35:54,130
whatever you do

500
00:35:54,220 --> 00:35:58,130
is true unknown distribution by a bayes mixture distribution

501
00:35:58,180 --> 00:35:59,980
so what you do is you

502
00:36:00,300 --> 00:36:03,480
take some set of distributions for several months

503
00:36:04,920 --> 00:36:08,230
i confine myself to countably

504
00:36:08,360 --> 00:36:10,880
a to countable to countable sets

505
00:36:10,910 --> 00:36:15,540
and you also assume that your true distribution in this in this class

506
00:36:15,590 --> 00:36:17,660
so that means the class should not be too small

507
00:36:20,800 --> 00:36:22,420
so what you do is here

508
00:36:22,470 --> 00:36:27,610
it's just a new notation written so new of x that is the probability of

509
00:36:29,150 --> 00:36:31,000
on the hypothesis h nu

510
00:36:31,040 --> 00:36:32,420
times the prior

511
00:36:32,970 --> 00:36:37,290
w nu is the prior probability of new edge into some of all hypotheses and

512
00:36:37,290 --> 00:36:38,720
then you get the evidence

513
00:36:38,730 --> 00:36:41,490
it's an economy based mixture distribution

514
00:36:41,500 --> 00:36:45,810
and the prior of course is the sum to one another also assume is that

515
00:36:46,110 --> 00:36:48,150
the prior probabilities never zero

516
00:36:48,230 --> 00:36:52,350
because you know i mean if it's theory can drop it out of the set

517
00:36:57,480 --> 00:37:03,770
the prior can be interpreted as the prior degree of belief that the true environment

518
00:37:03,770 --> 00:37:05,710
is nu

519
00:37:05,790 --> 00:37:08,540
i will come to the very briefly later

520
00:37:09,050 --> 00:37:11,780
if you take the logarithm of i mean you know

521
00:37:11,790 --> 00:37:14,360
if i have a probability distribution anything

522
00:37:15,750 --> 00:37:17,520
minus the lock

523
00:37:17,850 --> 00:37:22,730
then there is always a code of your data which has this code length so

524
00:37:22,730 --> 00:37:29,160
you can regard log probabilities called length and code length code are related to complexity

525
00:37:29,210 --> 00:37:33,810
so we can regard the log of the prior as a complexity penalty

526
00:37:35,740 --> 00:37:39,110
dixie itself is based makes this sort of the

527
00:37:39,120 --> 00:37:43,220
belief probability in observing x integrated out

528
00:37:44,850 --> 00:37:47,810
just a little bit notation here at this point

529
00:37:47,860 --> 00:37:51,210
OK so now let's it's proof something

530
00:37:51,230 --> 00:37:56,810
so again we have a sequence x one up to and for convenience property minus

531
00:37:56,810 --> 00:37:58,590
one and they want to predict t

532
00:38:00,300 --> 00:38:05,110
and the expectation of functions defined as usual

533
00:38:05,140 --> 00:38:08,160
well that he was just some move x for example or

534
00:38:08,750 --> 00:38:11,730
so if f is a function which only depends on the first

535
00:38:11,750 --> 00:38:13,510
time instances

536
00:38:13,620 --> 00:38:15,930
the kullback leibler divergence

537
00:38:16,930 --> 00:38:19,320
the most common

538
00:38:19,350 --> 00:38:24,610
distance between probability distributions so you have the true distribution here

539
00:38:24,620 --> 00:38:27,230
it is based makes the distribution fee

540
00:38:27,990 --> 00:38:29,630
and you take new lock

541
00:38:29,780 --> 00:38:31,370
mu divided by c

542
00:38:31,380 --> 00:38:33,130
some all arguments

543
00:38:33,150 --> 00:38:36,850
which is the same as taking expectation of l and divided by x

544
00:38:36,860 --> 00:38:40,120
the second level divergence between a and c

545
00:38:40,160 --> 00:38:42,370
in this case you can easily show

546
00:38:42,460 --> 00:38:46,310
it is bounded by loch of the weight to the minus one

547
00:38:46,350 --> 00:38:47,820
the proof is very easy

548
00:38:47,830 --> 00:38:53,330
what you do is

549
00:38:53,340 --> 00:38:54,700
so we have

550
00:38:55,920 --> 00:39:02,440
c o x is equal to some w nu nu x all new and

551
00:39:02,450 --> 00:39:05,500
since we we assume the true environment in the class

552
00:39:05,510 --> 00:39:07,650
i can drop to some keep just one

553
00:39:07,680 --> 00:39:08,730
i commend

554
00:39:08,730 --> 00:39:11,150
w nu

555
00:39:11,210 --> 00:39:17,980
w nu nu that are sorry blue one

556
00:39:27,570 --> 00:39:32,360
and then i divide vaccines or have mu of x divided by

557
00:39:32,400 --> 00:39:33,850
the x

558
00:39:33,880 --> 00:39:38,900
it is bounded by this bring to the other side that minus one

559
00:39:38,910 --> 00:39:41,110
i take the logarithm

560
00:39:41,350 --> 00:39:46,910
and i'm in the right hand side is independent it's text expectations bounded operators

561
00:39:46,940 --> 00:39:51,250
so this is bound so is very easy

562
00:39:51,800 --> 00:39:55,520
what we really care about is we want to use now

563
00:39:55,940 --> 00:39:59,780
so ideally we would no movie was use it for prediction so we have our

564
00:39:59,780 --> 00:40:03,430
past data x one to t minus one and you want to predict the probability

565
00:40:03,430 --> 00:40:06,470
of sixteen of a given history

566
00:40:06,480 --> 00:40:08,130
you can't do that

567
00:40:08,190 --> 00:40:10,990
because we don't know movies use the bayes mixture

568
00:40:11,050 --> 00:40:13,770
but prediction so we take the conditional

569
00:40:14,760 --> 00:40:17,700
and now we have to go copy

570
00:40:17,800 --> 00:40:20,650
what we really would like to do to what we can do

571
00:40:20,680 --> 00:40:22,940
and you can find several distance measure

572
00:40:22,960 --> 00:40:29,010
for instance i choose here the hellinger distance the most convenient mathematically so the difference

573
00:40:29,020 --> 00:40:30,300
we have

574
00:40:30,340 --> 00:40:34,850
the some overall outcomes and the hellinger means that experience here

575
00:40:34,890 --> 00:40:37,710
the point is that

576
00:40:38,020 --> 00:40:43,310
its parents

577
00:40:43,460 --> 00:40:45,110
just for technical reasons

578
00:40:45,820 --> 00:40:48,340
what you then can show is

579
00:40:48,350 --> 00:40:52,640
so that's HT omega t minus one is the

580
00:40:52,700 --> 00:40:57,780
the difference between the predictive distributions of music see at time t

581
00:40:57,850 --> 00:40:59,980
now take expectation over history

582
00:40:59,980 --> 00:41:03,240
and some all type instances from one to infinity

583
00:41:03,250 --> 00:41:05,640
and what you can show is

584
00:41:05,650 --> 00:41:07,220
in one page so

585
00:41:07,220 --> 00:41:14,010
that is that there is a clear is that that that that holds

586
00:41:14,540 --> 00:41:20,300
nominate I don't want to lose it so this was a case where a mixed

587
00:41:20,300 --> 00:41:22,940
strategy was right

588
00:41:22,960 --> 00:41:25,200
so that the

589
00:41:25,730 --> 00:41:27,790
for both of them

590
00:41:27,900 --> 00:41:33,770
let's go back to that what what was an easy case of the story

591
00:41:33,830 --> 00:41:38,290
and and and just think it through again so again are C and that we

592
00:41:38,600 --> 00:41:43,070
that I have something like say 1 2 4 7

593
00:41:45,870 --> 00:41:48,940
and then what were the best strategies

594
00:41:48,940 --> 00:41:50,640
for this game

595
00:41:50,680 --> 00:41:55,480
that this was the easy game remind me what what should are in this game

596
00:41:55,500 --> 00:42:00,070
2 wrote to all the time so fraction should be 0 and 1

597
00:42:00,780 --> 00:42:04,580
and what should see do all the time

598
00:42:04,640 --> 00:42:09,870
which runs from 1 so fraction should be 1 0 so this is why 1

599
00:42:09,870 --> 00:42:14,240
should be 1 and X 2 should be 1 so there is a case in

600
00:42:17,600 --> 00:42:24,700
because we were bumping up against the limits the 0 1 limits we didn't

601
00:42:25,140 --> 00:42:32,570
we can find a mixed strategy this was this was the this is like the

602
00:42:32,570 --> 00:42:37,400
key with this inequality constraints either you're kind of in the middle

603
00:42:40,140 --> 00:42:44,530
is we're in the middle and you could go either way a little more and

604
00:42:44,530 --> 00:42:46,920
column to a little more on column ones

605
00:42:47,280 --> 00:42:51,180
when you there it's like being in the middle of functions

606
00:42:53,060 --> 00:42:56,800
saying like this

607
00:42:57,160 --> 00:43:01,180
like like minimizing when you're in the middle

608
00:43:02,740 --> 00:43:07,180
you going way and the derivative has to be 0

609
00:43:07,220 --> 00:43:12,090
but if we were maximizing the function what over this interval what would be the

610
00:43:14,040 --> 00:43:17,060
the maximum would would be an

611
00:43:17,070 --> 00:43:20,570
and the derivative isn't 0 there it's like

612
00:43:20,590 --> 00:43:25,010
you have an inequalities that can only be a winner

613
00:43:25,030 --> 00:43:26,940
for the for the guy who's

614
00:43:29,540 --> 00:43:33,900
the derivative is positive because then you would wanna come

615
00:43:33,920 --> 00:43:37,830
you wouldn't wanna come again which is what we only direction you would be allowed

616
00:43:37,830 --> 00:43:41,070
to move that this is

617
00:43:41,090 --> 00:43:48,940
this method is so we're talking about maximum minimum optimisation with constraints that that if

618
00:43:48,940 --> 00:43:53,480
the constraints are active if you're at the edge of of the constraint is an

619
00:43:53,480 --> 00:43:55,920
inequality constraint because there's only 1 way

620
00:43:56,920 --> 00:43:58,520
where if you're at the center

621
00:43:59,940 --> 00:44:04,420
the constraint is an equality constraints and derivative has to be 0 could you could

622
00:44:04,420 --> 00:44:05,580
go both ways

623
00:44:05,720 --> 00:44:08,500
so this was a case where you can go both ways

624
00:44:08,530 --> 00:44:14,000
so equalities had all we had the equations like this would made a week we

625
00:44:14,000 --> 00:44:18,820
made it so that C had no reason to prefer 1 over the other because

626
00:44:18,830 --> 00:44:20,980
it could both ways here

627
00:44:23,870 --> 00:44:26,980
well we're facing see whether choices

628
00:44:26,980 --> 00:44:32,660
that that losing for all the time

629
00:44:33,330 --> 00:44:39,640
that started we're at the edge of the of of the of the would like

630
00:44:39,740 --> 00:44:40,790
chooses even less

631
00:44:41,500 --> 00:44:48,580
but it or even sort is well anyway where we're 1 0 situation for both

632
00:44:48,580 --> 00:44:53,540
of them in fact so that it is imminent saying that that are is not

633
00:44:53,540 --> 00:44:59,940
presenting see the equal choice if they forest keeps choosing this this

634
00:45:00,320 --> 00:45:05,440
sees choices to lose for 7

635
00:45:05,480 --> 00:45:07,660
so we always pixel using for

636
00:45:07,710 --> 00:45:11,790
that there was a case there were rumors that there was a equality of 4

637
00:45:11,790 --> 00:45:18,270
was less than 7 therefore they're all the marbles went on column 1 this was

638
00:45:18,270 --> 00:45:24,300
a case where in either direction it didn't matter and we ended up with mixed

639
00:45:25,720 --> 00:45:28,400
maybe that's the

640
00:45:28,440 --> 00:45:30,280
the theme of

641
00:45:30,310 --> 00:45:36,280
of the uh optimization with inequality constraints

642
00:45:36,290 --> 00:45:38,520
and may be I express that now

643
00:45:38,550 --> 00:45:47,010
it in a in a language that's not game theory but but a typical problems

644
00:45:47,070 --> 00:45:52,480
of minimizing some cost function

645
00:45:52,480 --> 00:45:54,530
subject to some

646
00:45:56,660 --> 00:46:05,180
In the equality constraints so now I'm really these last minutes

647
00:46:05,800 --> 00:46:09,360
speaking about optimisation as we really know

648
00:46:12,160 --> 00:46:17,300
when minimizing something that's optimisation

649
00:46:17,310 --> 00:46:22,240
there are constraints they can be non-linear that can be nonlinear

650
00:46:22,250 --> 00:46:23,830
and this

651
00:46:23,870 --> 00:46:25,610
and they can be an equal

652
00:46:26,540 --> 00:46:32,070
or equations so that there is like a general optimisation problems

653
00:46:33,060 --> 00:46:35,880
and how would you handle such a problem

654
00:46:36,880 --> 00:46:38,760
that's not an easy problem

655
00:46:40,480 --> 00:46:42,640
because you don't know

656
00:46:43,310 --> 00:46:48,350
if you know if the if if you

657
00:46:48,370 --> 00:46:51,440
so the best X and looking for the best text

658
00:46:51,440 --> 00:46:56,000
X star the winning where

659
00:46:56,090 --> 00:47:02,180
any number of dimensions and we have any number of inequalities then

660
00:47:02,330 --> 00:47:07,250
if we know that the best X if we know where it was hitting the

661
00:47:07,260 --> 00:47:13,040
boundary if we know in which components a of x star was equal the corresponding

662
00:47:13,060 --> 00:47:17,940
B and which components it was last we would we would be golden

663
00:47:18,920 --> 00:47:25,580
because then we would have equations you say that the unfortunate we don't know that

664
00:47:25,590 --> 00:47:29,920
so so that x is

665
00:47:30,200 --> 00:47:36,080
and at end different numbers so you got any numbers to choose a and B

666
00:47:36,120 --> 00:47:42,580
is and different numbers forget and inequality constraints

667
00:47:42,680 --> 00:47:46,350
and that the &

668
00:47:46,420 --> 00:47:50,140
might my point is that if we 0

669
00:47:50,190 --> 00:47:55,050
which is where the qualities were

670
00:47:55,080 --> 00:47:57,810
then those would give us some equations for

671
00:47:58,640 --> 00:48:03,160
and then in the end we knew the other ones were inequalities then we would

672
00:48:03,160 --> 00:48:09,600
know some derivatives in those components or 0 because we would be in in those

673
00:48:09,610 --> 00:48:13,810
components we would be able to go left or right and therefore a derivative with

674
00:48:13,820 --> 00:48:17,440
respect to that would have to be 0 we would have a set of equations

675
00:48:17,440 --> 00:48:18,980
we itself but we don't know

676
00:48:21,290 --> 00:48:25,420
which constraints are active in which enough that's when it comes to

677
00:48:25,440 --> 00:48:33,500
an active constraint is 1 where where where you're pushing that where equality holds right

678
00:48:33,500 --> 00:48:37,640
up in my picture here there is an active constraint that the X can't go

679
00:48:37,640 --> 00:48:39,290
beyond that value

680
00:48:39,920 --> 00:48:44,400
at forcing us to be there and if we that once if we know that

681
00:48:47,080 --> 00:48:48,800
we there

682
00:48:49,500 --> 00:48:56,030
if we knew it didn't hold we can look at beyond the maybe but

683
00:48:56,030 --> 00:48:57,780
kind of people are up to

684
00:48:57,780 --> 00:49:02,780
so the the representer theorem is one of the central theorems in

685
00:49:04,420 --> 00:49:07,900
because of the following so we are given some kernel

686
00:49:07,960 --> 00:49:09,840
given a training set of point

687
00:49:09,840 --> 00:49:17,280
and this derivative this quantity here and some objective function so suppose our learning consists

688
00:49:17,280 --> 00:49:22,650
of minimising this objective function so the objective function here has the first part which

689
00:49:22,650 --> 00:49:25,260
is some

690
00:49:26,010 --> 00:49:32,590
function we assume it's strictly monotonic we'll sorry it's it's an arbitrary cost function c

691
00:49:32,610 --> 00:49:35,150
which takes all input points

692
00:49:35,170 --> 00:49:36,440
all outputs

693
00:49:36,460 --> 00:49:40,880
x y of the all the points and also

694
00:49:40,900 --> 00:49:42,880
the function value

695
00:49:42,880 --> 00:49:48,240
of the function that we're estimating on those points so typically learning consists of making

696
00:49:48,240 --> 00:49:54,260
sure that f of x i is equal to y i but in principle the

697
00:49:54,260 --> 00:49:58,210
most cost functions were just say to compute the square difference between this thing in

698
00:49:58,210 --> 00:50:01,730
this thing but we can do modern also let's say of course we can depend

699
00:50:01,730 --> 00:50:05,340
on all three and you can even depend on the whole training set in some

700
00:50:05,340 --> 00:50:10,710
complicated joint way so this is the most general cost function you can think of

701
00:50:10,730 --> 00:50:14,880
and in addition here we have a more important part which is the regularizer

702
00:50:14,880 --> 00:50:20,420
and this is known in europe producing company but space induced by kernel

703
00:50:20,420 --> 00:50:21,440
if f is

704
00:50:21,460 --> 00:50:27,010
an arbitrary function of all from our previous encounters space and we are allowing to

705
00:50:27,010 --> 00:50:29,000
somehow transformed is known

706
00:50:29,130 --> 00:50:34,070
with a strictly monotonic function omega so they could be for square you could be

707
00:50:34,070 --> 00:50:36,110
squared norm

708
00:50:36,130 --> 00:50:39,710
and we're saying if we optimise this thing over f

709
00:50:39,730 --> 00:50:44,360
arbitrary functions from our space in this space can be big can be dense in

710
00:50:44,360 --> 00:50:48,840
the set of continuous functions then it turns out that the solution can be written

711
00:50:48,840 --> 00:50:49,630
like this

712
00:50:49,650 --> 00:50:54,670
kernel expansion and the crucial point is that the solution will have and will only

713
00:50:54,670 --> 00:50:55,860
need kernels

714
00:50:55,880 --> 00:50:58,210
sitting on the training points

715
00:50:58,230 --> 00:51:04,670
so this is optimisation problem in potentially infinite dimensional space and it turns out

716
00:51:04,690 --> 00:51:10,230
the solution for or a solution can be cases where is not unique but a

717
00:51:10,230 --> 00:51:14,150
solution which is as good as any other one can be written

718
00:51:14,170 --> 00:51:17,280
as a convex function centered on the training points

719
00:51:17,300 --> 00:51:23,460
so this reduces the infinite dimensional optimisation problem to the problem of just finding

720
00:51:23,480 --> 00:51:26,900
coefficients here so n dimensional problems

721
00:51:26,920 --> 00:51:32,320
so that's kind of surprising and i remember also in the in early days of

722
00:51:32,320 --> 00:51:36,940
kernel work we we knew that this theorem exist it's increase while last book but

723
00:51:36,940 --> 00:51:42,010
somehow it was very opaque and nobody clients didn't well maybe nobody bothered to go

724
00:51:42,010 --> 00:51:47,010
through the whole book because somehow complicated proven at some point i just thought maybe

725
00:51:47,070 --> 00:51:51,010
maybe easier to try easier than trying to understand grace as proof is just to

726
00:51:51,010 --> 00:51:54,800
try to prove it yourself and it turns out is a very simple proof and

727
00:51:54,800 --> 00:51:55,980
briefly show that

728
00:51:58,170 --> 00:52:02,670
proof is simply probably is essentially the same same idea

729
00:52:02,730 --> 00:52:07,090
just with less code take an arbitrary function from the space

730
00:52:07,110 --> 00:52:11,320
decompose into part which is in the span of the training points and the path

731
00:52:11,440 --> 00:52:13,400
orthogonal to it

732
00:52:13,440 --> 00:52:19,570
so these are decomposition we know the the orthogonal part is in particular orthogonal to

733
00:52:19,570 --> 00:52:23,050
each individual kernel sitting on the training point

734
00:52:23,070 --> 00:52:24,840
now we apply

735
00:52:24,860 --> 00:52:25,980
our function

736
00:52:25,980 --> 00:52:29,010
to some arbitrary training point

737
00:52:29,050 --> 00:52:31,050
so we're interested now in this

738
00:52:31,050 --> 00:52:35,650
the first part here because here the function only sees the training points of course

739
00:52:35,650 --> 00:52:39,300
the loss something that evaluated on the training points so we want to see what

740
00:52:39,300 --> 00:52:40,630
are these values here

741
00:52:40,650 --> 00:52:46,800
so we apply a function to an arbitrary training point substitute this function in here

742
00:52:46,820 --> 00:52:51,570
get these two terms we know that the second part gives us zero so this

743
00:52:51,570 --> 00:52:55,840
in fact lots of works you do you do you want to study really are

744
00:52:55,850 --> 00:53:02,570
quite big whatever that turns out to me so so lot literature you'll find

745
00:53:02,620 --> 00:53:05,710
networks being talked about in the limit of

746
00:53:05,890 --> 00:53:08,420
number of nodes just getting bigger and bigger

747
00:53:08,540 --> 00:53:13,330
so what you do that it has for the for twelve

748
00:53:13,430 --> 00:53:14,770
random graphs

749
00:53:14,790 --> 00:53:16,810
how does when network

750
00:53:16,830 --> 00:53:19,330
as we seen expected number of edges is

751
00:53:19,380 --> 00:53:25,220
it is written as k in pointy brackets is is number of times probability that

752
00:53:25,220 --> 00:53:29,300
we don't want to blow up the standard to just hold the whole the

753
00:53:29,310 --> 00:53:33,340
mean number of connections per node fixed and scale p

754
00:53:33,350 --> 00:53:36,250
like that in which case you can you can then

755
00:53:36,260 --> 00:53:40,110
increase the number of nodes but you get the end up with a kind of

756
00:53:40,110 --> 00:53:47,930
of a standard finite fixed finite mean number of nodes mean of connections per node

757
00:53:48,450 --> 00:53:50,600
and in in a large network

758
00:53:50,650 --> 00:53:52,160
limit this

759
00:53:52,180 --> 00:53:53,700
OK basically

760
00:53:53,710 --> 00:53:55,550
completely specified model

761
00:53:57,640 --> 00:54:02,100
just i'm going to mention this point and will come up again later but the

762
00:54:02,110 --> 00:54:08,520
completely standard way of doing this for for australian network how to scale the parameters

763
00:54:08,530 --> 00:54:10,450
you get to sort of reasonable

764
00:54:10,920 --> 00:54:15,650
network in the large limited is not always that obvious it's not spatial networks as

765
00:54:16,620 --> 00:54:17,830
see later

766
00:54:17,840 --> 00:54:23,130
now we're going to go counter what is probably the the single most important statistic

767
00:54:23,130 --> 00:54:24,700
associated with random

768
00:54:24,720 --> 00:54:26,370
ensembles networks

769
00:54:26,380 --> 00:54:30,060
or in fact single networks which is the degree distribution

770
00:54:30,100 --> 00:54:35,300
the degree of node being simply the number of connections associated with that node is

771
00:54:35,300 --> 00:54:40,100
perhaps most study most striking feature the most obvious feature to look at the bus

772
00:54:40,110 --> 00:54:43,590
network it doesn't describe the network completely but it is

773
00:54:43,600 --> 00:54:46,570
so important was it's exactly what we mean by

774
00:54:46,790 --> 00:54:51,890
that is k is degree of just pick out some node random boolean network what's

775
00:54:51,890 --> 00:54:52,970
its degree

776
00:54:53,170 --> 00:54:55,580
and we get some random variable k

777
00:54:55,590 --> 00:54:57,480
and pk is probability

778
00:54:57,500 --> 00:55:01,860
that will find a node with that degree degree k

779
00:55:01,870 --> 00:55:03,870
a couple of things to note about it

780
00:55:03,880 --> 00:55:05,750
k is a random variable

781
00:55:05,790 --> 00:55:08,740
this makes sense for single network as well as an ensemble you just

782
00:55:08,760 --> 00:55:12,830
pick a random node from single network

783
00:55:12,880 --> 00:55:14,150
it's ensemble

784
00:55:14,170 --> 00:55:16,050
what you possibly want to do is to pick

785
00:55:16,060 --> 00:55:21,630
a network of the ensemble sample network from the space that your your networks and

786
00:55:21,630 --> 00:55:24,610
then pick a random node from that

787
00:55:24,660 --> 00:55:29,010
but the warning here i found some very confusing the network literature which is is

788
00:55:29,010 --> 00:55:33,440
often not clear whether people are talking about if you like an ensemble statistics all

789
00:55:33,460 --> 00:55:35,800
statistic for single network

790
00:55:36,090 --> 00:55:38,780
that's particularly vexing because sometimes

791
00:55:38,800 --> 00:55:41,980
it doesn't amount to the same sex

792
00:55:41,990 --> 00:55:43,600
anyway important

793
00:55:43,610 --> 00:55:50,430
the most important quantities mean degree simply the expectation of that random variable the statistics

794
00:55:50,430 --> 00:55:52,890
associated with the distribution

795
00:55:52,990 --> 00:55:56,530
this would be the variance of the node degree as you

796
00:55:56,540 --> 00:55:58,070
one day around the network

797
00:55:58,740 --> 00:56:01,260
and again that one for instance

798
00:56:01,270 --> 00:56:06,850
and the different things like for a single network of an ensemble and and how

799
00:56:07,010 --> 00:56:09,010
average might become significant

800
00:56:10,740 --> 00:56:12,420
so we go

801
00:56:12,440 --> 00:56:15,740
for the degree distribution for the

802
00:56:16,190 --> 00:56:18,620
was really network is binomial

803
00:56:18,720 --> 00:56:19,880
and that

804
00:56:19,890 --> 00:56:22,420
and in the large network limits

805
00:56:22,430 --> 00:56:24,410
he comes across on distribution

806
00:56:24,420 --> 00:56:25,780
and hence

807
00:56:25,790 --> 00:56:29,110
hence the name basically

808
00:56:29,120 --> 00:56:33,460
and if you like the cross on distribution reflects the complete independence

809
00:56:33,480 --> 00:56:36,550
of choosing connections between nodes

810
00:56:37,800 --> 00:56:39,410
next interesting thing is

811
00:56:39,710 --> 00:56:44,210
in general connections are not independent they are not just read the model but

812
00:56:44,220 --> 00:56:45,700
generally they're not going to be

813
00:56:45,710 --> 00:56:49,770
for the degrees of difference nodes are not going to be independent going to be

814
00:56:50,930 --> 00:56:57,360
in particular what you might want to know about network is if i have

815
00:56:58,180 --> 00:57:00,220
neighboring nodes

816
00:57:00,270 --> 00:57:04,560
what is the relation between the degrees of the nodes which happened to be connected

817
00:57:05,700 --> 00:57:09,890
so what you might want to look at is this conditional degree distributions which says

818
00:57:09,890 --> 00:57:13,900
basically if i have to given that they are connected

819
00:57:13,910 --> 00:57:17,760
what what is the degree of that once a given that happens to be connected

820
00:57:17,760 --> 00:57:20,030
to a node of another degree

821
00:57:20,140 --> 00:57:24,340
that's the characterisation of

822
00:57:24,370 --> 00:57:27,040
the to the degree distribution if you like

823
00:57:27,150 --> 00:57:30,210
psychiatry division that is

824
00:57:30,240 --> 00:57:32,960
given the network is connected to another

825
00:57:32,980 --> 00:57:37,670
in this case a degree five what is the expected degree of node going to

826
00:57:37,670 --> 00:57:40,400
be given that connects to the nodes that degree

827
00:57:40,780 --> 00:57:44,610
and there's some terminology here core network is sort of

828
00:57:44,630 --> 00:57:47,020
is that expectation increases with

829
00:57:47,030 --> 00:57:50,870
with k dash not is for instance

830
00:57:50,900 --> 00:57:54,520
the high degree nodes tend to connect to other high degree nodes or in the

831
00:57:54,520 --> 00:57:57,330
case of disassortative vice versa

832
00:57:57,330 --> 00:58:00,250
but also text speech and things that type

833
00:58:00,250 --> 00:58:03,200
us but really what deep learning is trying to do his learn

834
00:58:03,200 --> 00:58:05,830
the problem with we need to solve with deep learning is learning

835
00:58:05,830 --> 00:58:09,930
representations of the world and this is all about height high-dimensional

836
00:58:09,940 --> 00:58:14,270
data representation so i read how do we learn about this additions

837
00:58:14,270 --> 00:58:16,910
of the world but just observing it does or does the question we need

838
00:58:16,910 --> 00:58:19,680
to answer this question neuroscientists have been as the

839
00:58:19,680 --> 00:58:22,380
cells so for long time how does the brain does that

840
00:58:22,450 --> 00:58:27,700
do that so the visual cortex this article we on that for

841
00:58:28,030 --> 00:58:32,350
are quite some time the the notion of our as in the visual cortex

842
00:58:32,350 --> 00:58:34,920
are not exactly equivalent to the notion of layers in a neural

843
00:58:34,920 --> 00:58:39,660
net that sort of and it's the fact that or on as feed-forward is

844
00:58:39,660 --> 00:58:42,830
the is a good simplification of really the architecture that

845
00:58:44,350 --> 00:58:47,630
biology uses which has organs feedback and lateral connections

846
00:58:47,630 --> 00:58:51,210
and things of that type but this is sort of a sense that for

847
00:58:51,220 --> 00:58:55,580
very fast perception or the visual cortex is essentially feed-forward

848
00:58:55,590 --> 00:58:59,770
process because it just goes to fast for any kind of feedback really

849
00:58:59,780 --> 00:59:03,890
have an impact this something we're not modeling you know in our

850
00:59:04,080 --> 00:59:07,570
feed-forward recognition system which is that the cortex is really

851
00:59:07,570 --> 00:59:11,710
has to to different pathways one that is used to

852
00:59:12,180 --> 00:59:16,230
recognize objects independently position another one that's used

853
00:59:16,230 --> 00:59:18,550
to localize objects independently where they are

854
00:59:18,550 --> 00:59:24,030
so the second one helps when you're navigating in avoiding obstacles

855
00:59:24,040 --> 00:59:26,460
grabbing things look at the localising things

856
00:59:26,690 --> 00:59:28,700
rest the first one is just identifying

857
00:59:30,930 --> 00:59:36,030
this of course talk to each other so i'm going to use prophase

858
00:59:36,030 --> 00:59:38,830
some some of the things that are neurons talked about

859
00:59:39,060 --> 00:59:41,880
this morning which is the fact that

860
00:59:44,710 --> 00:59:50,170
a good way to kind of build a stage that ends up extracting features

861
00:59:50,650 --> 00:59:54,340
is to have have to do a sequence of two operations that's

862
00:59:54,350 --> 00:59:58,230
really very old idea says or not so you know something that

863
00:59:58,580 --> 01:00:02,490
was recently arrived that from the deep learning side it's very

864
01:00:02,490 --> 01:00:04,450
old idea for parameterizing functions

865
01:00:04,470 --> 01:00:07,570
you have any vector you have first

866
01:00:07,570 --> 01:00:10,700
layer that essentially is a non-linear mapping to a higher dimensional

867
01:00:10,700 --> 01:00:14,370
space expended mentioned nonlinear way it's very important

868
01:00:14,380 --> 01:00:18,050
this non-linear and the role of this of this stage is to as this

869
01:00:18,060 --> 01:00:20,300
layers to actually essentially break apart

870
01:00:20,690 --> 01:00:22,590
regions of this based on semantic is

871
01:00:23,110 --> 01:00:26,740
dissimilar that need to be classified as different things

872
01:00:27,170 --> 01:00:30,660
and so you break things apart and then you have another layer that

873
01:00:30,660 --> 01:00:34,380
reduces the dimension that essentially we aggregates things

874
01:00:34,390 --> 01:00:38,360
that are semantically similar to happen to have been working apart

875
01:00:38,370 --> 01:00:42,070
by the for stage ok so bec things apart non-linearly and then

876
01:00:42,070 --> 01:00:44,830
group rhythm back together and so you have this sort of

877
01:00:44,830 --> 01:00:47,700
expansion or reduction which is similar to

878
01:00:48,030 --> 01:00:51,630
this is the basic module lawns was or was talking about this morning

879
01:00:51,660 --> 01:00:55,180
where you know it would have know are real business functions

880
01:00:55,190 --> 01:00:59,180
here or gaussian or bones whatever but you can do whatever

881
01:00:59,190 --> 01:01:06,920
you want ok so to build our architectures by essentially stacking

882
01:01:06,930 --> 01:01:09,720
stages of this basic sequence of operation

883
01:01:09,830 --> 01:01:12,850
first there is optional normalization in fact in practice

884
01:01:12,850 --> 01:01:16,950
we don't have that very often then well have a linear dimension

885
01:01:16,950 --> 01:01:20,080
expansion in the form of a filter bank for images for sort of

886
01:01:20,080 --> 01:01:23,800
array signals in general but but figured as just matrix

887
01:01:24,310 --> 01:01:26,560
then with have point-wise non-linearity

888
01:01:27,230 --> 01:01:31,700
there's all kinds of arguments for why it is just fine it's as nonlinearities

889
01:01:31,710 --> 01:01:34,890
point-wise this point and stiffener have

890
01:01:35,190 --> 01:01:39,600
argument about this and then this feature pooling which is places

891
01:01:39,610 --> 01:01:43,280
world of abrogating features over space or over kind of variabilities

892
01:01:43,290 --> 01:01:47,780
of various kinds by sort of reducing dimensions so very often

893
01:01:47,780 --> 01:01:51,010
in when things like commercial net but also in other types of

894
01:01:51,010 --> 01:01:53,650
of networks this feature pooling fixed there's no rule

895
01:01:53,650 --> 01:01:57,140
learned parameters in this all learned parameters and filters acts

896
01:01:57,670 --> 01:02:00,520
as you take those stage the repeated multiple times to

897
01:02:00,520 --> 01:02:04,180
stick classifier on top in fact classifier is nothing more than another

898
01:02:04,180 --> 01:02:07,260
stage of those filter bank non-linearity actually so that

899
01:02:07,260 --> 01:02:09,230
but i think of this as kind of a different

900
01:02:09,230 --> 01:02:12,620
or as different in nature from this is just another one of those

901
01:02:14,970 --> 01:02:18,680
the pooling operation is very often nowadays a max operation although

902
01:02:18,680 --> 01:02:21,710
people are doing other things and the non-linearity very often

903
01:02:21,710 --> 01:02:26,690
if there is one here is a max of x zero that's half rectification

904
01:02:27,140 --> 01:02:30,570
this technique by in goodfellow and this collaborators in toronto

905
01:02:30,760 --> 01:02:34,150
where they actually do they do away with point-wise on narrative

906
01:02:34,150 --> 01:02:37,300
here and the pooling directly doesn't max over multiple feature

907
01:02:37,300 --> 01:02:41,580
types and over space that seems to actually work quite well to call

908
01:02:41,590 --> 01:02:42,480
that max out

909
01:02:44,590 --> 01:02:49,540
so that's what a a deep norm that are sort of modern day practical

910
01:02:49,540 --> 01:02:52,810
deep neural net look looks like it's a stack of linear operation

911
01:02:52,810 --> 01:02:56,530
interspersed with max max is essentially you can think of max

912
01:02:56,540 --> 01:03:00,580
kind of which this which is between zero and executes value or

913
01:03:00,580 --> 01:03:03,470
between the different values and taking the biggest one

914
01:03:03,470 --> 01:03:04,480
if it's max pooling

915
01:03:08,790 --> 01:03:10,960
ok you train as with backprop that's just

916
01:03:11,300 --> 01:03:14,860
application of general essentially compute some gradients

917
01:03:14,860 --> 01:03:18,450
functions you have to optimize when you do this and being very highly

918
01:03:18,450 --> 01:03:21,600
non-convex there are tons and tons and tons of set of points in

919
01:03:21,600 --> 01:03:23,660
fact that combinatorial number of set of points

920
01:03:23,660 --> 01:03:27,560
and local minima there is a picture that emerged that a lot of

921
01:03:27,570 --> 01:03:31,890
us knew of non for long time intuitively but not not really have

922
01:03:31,900 --> 01:03:34,170
been able to figure one which is that

923
01:03:34,810 --> 01:03:38,150
you never have local minima problems with deep learning systems

924
01:03:38,450 --> 01:03:41,890
whenever you try to your is get solutions that have the same kind

925
01:03:41,890 --> 01:03:45,720
of performance the system as we know have combinatorially large number

926
01:03:45,730 --> 01:03:50,500
of on the map but what local find is always kind of the same type

927
01:03:50,510 --> 01:03:54,030
of performance they always to those different but they always the

928
01:03:54,040 --> 01:03:57,810
same kind performance so you know there's two different types

929
01:03:57,810 --> 01:04:00,240
among convexity there is national complexity and not going

930
01:04:00,240 --> 01:04:03,470
to get deep learning systems particularly they all over sized

931
01:04:03,470 --> 01:04:05,580
is not nasty sensei doesn't matter which

932
01:04:05,680 --> 01:04:10,120
find but echoes also have lots of cell points whenever you have

933
01:04:10,120 --> 01:04:12,450
to local many others so the point in between

934
01:04:12,620 --> 01:04:15,100
or perhaps multiple because it's high dimension

935
01:04:15,360 --> 01:04:18,850
and and back it's kind of landscape this extremely complicated

936
01:04:18,850 --> 01:04:21,280
that none of us really begins to understand so

937
01:04:21,600 --> 01:04:23,940
i hope these people in the audience you can

938
01:04:24,500 --> 01:04:27,570
get interested in this question this is complicated for me

939
01:04:28,770 --> 01:04:34,200
so right so let's think about a deep network with you know max with

940
01:04:34,210 --> 01:04:39,420
values max what does it do so let's say has a single output

941
01:04:39,710 --> 01:04:42,480
you could write it as you can take a single

942
01:04:42,670 --> 01:04:47,210
path in that network and what the influence of this path from the

943
01:04:47,220 --> 01:04:51,270
output ok and this class so it's actually take an input multiplied

944
01:04:51,270 --> 01:04:54,180
by the first weight and then by second weight and by the third way

945
01:04:54,180 --> 01:04:57,850
and you accumulate that the output and if path is active which

946
01:04:57,860 --> 01:05:02,190
means if all units are in there are in their linear region

947
01:05:02,450 --> 01:05:07,450
ok the value this is this this fast contribute to the to the output

948
01:05:08,850 --> 01:05:12,640
and so you do the some of all path all contributions and you get

949
01:05:12,640 --> 01:05:15,280
the input that relations what's going to happen is that some of the

950
01:05:15,280 --> 01:05:18,120
path to be turned off because the corresponding

951
01:05:18,480 --> 01:05:22,300
unit is in the flat spot or the the max which have been selected

952
01:05:22,500 --> 01:05:26,320
ok so essentially think of it as a for these sort of switch path

953
01:05:26,320 --> 01:05:27,530
have one

954
01:05:27,550 --> 01:05:29,070
because then the

955
01:05:31,820 --> 01:05:34,320
this person procedure does in the

956
01:05:36,260 --> 01:05:40,530
of course

957
01:05:46,300 --> 01:05:49,200
trying to win this i would like to

958
01:05:52,370 --> 01:05:55,990
the first people

959
01:05:57,380 --> 01:06:02,300
the probability of the data so actually there where is

960
01:06:03,340 --> 01:06:06,450
and the

961
01:06:08,930 --> 01:06:13,260
it is

962
01:06:50,240 --> 01:06:52,840
my part the

963
01:06:52,860 --> 01:06:56,470
ten which much like the way

964
01:07:04,490 --> 01:07:07,010
none of these

965
01:07:14,140 --> 01:07:17,430
imagine you're going to

966
01:07:20,640 --> 01:07:22,550
depending on the same level

967
01:07:22,570 --> 01:07:24,680
the line

968
01:07:24,680 --> 01:07:27,430
that that

969
01:07:28,070 --> 01:07:31,890
the pixel in the image

970
01:07:31,910 --> 01:07:34,740
so you can when

971
01:07:34,840 --> 01:07:36,930
and the like

972
01:07:36,950 --> 01:07:40,840
because when it was one of the most high

973
01:07:43,680 --> 01:07:45,800
that is this one

974
01:07:45,800 --> 01:07:47,090
most like

975
01:07:47,110 --> 01:07:48,240
course the white

976
01:07:50,490 --> 01:07:53,110
so you can select the big because

977
01:07:53,510 --> 01:07:56,820
that a

978
01:07:56,840 --> 01:08:00,160
you know it's like

979
01:08:00,180 --> 01:08:01,780
so we have

980
01:08:04,680 --> 01:08:09,180
the whole and this is what

981
01:08:09,220 --> 01:08:13,510
the amount of one

982
01:08:13,590 --> 01:08:15,590
first of all

983
01:08:15,610 --> 01:08:17,090
part of

984
01:08:17,140 --> 01:08:19,180
will be filled with water

985
01:08:19,200 --> 01:08:20,660
and the

986
01:08:20,680 --> 01:08:21,660
will they

987
01:08:24,360 --> 01:08:26,380
so here

988
01:08:26,390 --> 01:08:29,390
so it was one of the the

989
01:08:30,320 --> 01:08:34,260
that's one of the first one was what

990
01:08:34,700 --> 01:08:37,490
these are flanked by the

991
01:08:37,640 --> 01:08:42,610
don't like that be same

992
01:08:44,660 --> 01:08:46,680
it is more

993
01:08:47,760 --> 01:08:50,450
and finally we have

994
01:08:55,990 --> 01:08:58,010
high image

995
01:08:58,030 --> 01:08:59,590
by the one

996
01:08:59,610 --> 01:09:01,740
problem we

997
01:09:02,220 --> 01:09:05,660
some threshold a wall

998
01:09:05,680 --> 01:09:07,090
one man the

999
01:09:08,180 --> 01:09:12,380
two different ways of of life

1000
01:09:12,380 --> 01:09:14,030
i j

1001
01:09:14,050 --> 01:09:16,120
we don't allow them to do use

1002
01:09:16,700 --> 01:09:18,160
some walls

1003
01:09:18,250 --> 01:09:22,550
and here lies close to what you should do

1004
01:09:22,550 --> 01:09:27,070
and i will have only one child one

1005
01:09:28,490 --> 01:09:30,720
one hundred

1006
01:09:31,180 --> 01:09:35,120
the pixels which is what this one

1007
01:09:40,550 --> 01:09:43,640
then the

1008
01:09:43,680 --> 01:09:50,200
the following

1009
01:09:50,510 --> 01:09:55,010
this that

1010
01:09:55,760 --> 01:09:57,200
he was

1011
01:10:05,140 --> 01:10:07,950
but how draw

1012
01:10:09,490 --> 01:10:11,490
with believe that

1013
01:10:11,510 --> 01:10:13,590
in the next slide

1014
01:10:13,700 --> 01:10:18,430
and to use some predicates with them

1015
01:10:18,570 --> 01:10:26,390
you've all heard

1016
01:10:26,490 --> 01:10:29,220
change the

1017
01:10:29,240 --> 01:10:31,610
and somehow

1018
01:10:31,620 --> 01:10:32,990
the walls

1019
01:10:33,050 --> 01:10:34,570
one of them

1020
01:10:34,590 --> 01:10:37,320
to build on average

1021
01:10:37,340 --> 01:10:41,240
and the way it or they

1022
01:10:47,660 --> 01:10:53,550
how do the one that should be quite good at this point

1023
01:10:54,800 --> 01:11:01,740
they have local minima are mark more than and and that they start with one

1024
01:11:01,760 --> 01:11:03,070
went to

1025
01:11:04,570 --> 01:11:10,430
well first of all you have what ever and

1026
01:11:13,930 --> 01:11:16,990
and we want to obey

1027
01:11:20,720 --> 01:11:23,160
at some point

1028
01:11:23,180 --> 01:11:25,640
when a of water

1029
01:11:25,660 --> 01:11:31,030
every single under the waters of the lake to black one lake

1030
01:11:31,050 --> 01:11:32,430
so we have

1031
01:11:32,780 --> 01:11:35,280
this is

1032
01:11:36,610 --> 01:11:38,660
but at

1033
01:11:41,620 --> 01:11:43,470
we also

1034
01:11:43,490 --> 01:11:46,050
so in the end what

1035
01:11:47,940 --> 01:11:50,180
one way

1036
01:11:53,530 --> 01:11:54,320
which should

1037
01:11:54,380 --> 01:11:56,930
after this

1038
01:11:56,950 --> 01:11:58,340
what need

1039
01:12:00,530 --> 01:12:05,590
the result it is that we had to

1040
01:12:06,680 --> 01:12:11,680
and started to apply variational procedure what is

1041
01:12:11,760 --> 01:12:14,550
and to get my

1042
01:12:20,220 --> 01:12:22,700
we must always you

1043
01:12:22,740 --> 01:12:25,820
and we'll see you all the

1044
01:12:26,800 --> 01:12:29,640
because of of the

1045
01:12:29,660 --> 01:12:31,800
we want to

1046
01:12:31,820 --> 01:12:33,620
here we have the most

1047
01:12:35,030 --> 01:12:40,930
first of all the lakes will be like this is great

1048
01:12:41,010 --> 01:12:43,840
this image we

1049
01:12:45,140 --> 01:12:46,490
so we have

1050
01:12:48,890 --> 01:12:50,470
the which

1051
01:12:50,490 --> 01:12:53,340
so this section of the math was selling

1052
01:12:57,160 --> 01:12:57,840
we can

1053
01:12:57,860 --> 01:12:59,320
put down here

1054
01:12:59,340 --> 01:13:01,990
and at the the

1055
01:13:02,010 --> 01:13:03,260
pixels of the

1056
01:13:03,260 --> 01:13:04,050
the later

1057
01:13:04,070 --> 01:13:07,490
but also neighboring pixels like this one

1058
01:13:07,510 --> 01:13:10,550
and then what is great

1059
01:13:10,570 --> 01:13:14,490
we can put down here is the centre of the because your

1060
01:13:14,510 --> 01:13:18,180
and in this case of the pixels of the image

1061
01:13:18,200 --> 01:13:20,320
so that is what

1062
01:13:20,360 --> 01:13:22,590
and so on

1063
01:13:22,620 --> 01:13:24,680
after the first

1064
01:13:24,780 --> 01:13:26,610
little big lake

1065
01:13:29,620 --> 01:13:32,180
like great

1066
01:13:32,200 --> 01:13:37,970
we way so we can apply our donations procedure for the first

1067
01:13:39,470 --> 01:13:41,490
and we

1068
01:13:41,490 --> 01:13:43,800
the you

1069
01:13:43,910 --> 01:13:47,490
during that

1070
01:13:47,490 --> 01:13:50,050
two people

1071
01:13:50,070 --> 01:13:51,430
so it

1072
01:13:51,720 --> 01:13:54,130
it the distance

1073
01:13:54,220 --> 01:14:00,220
services so the distance between between people

1074
01:14:00,470 --> 01:14:05,680
the function is always

1075
01:14:05,700 --> 01:14:12,340
we a so

1076
01:14:22,700 --> 01:14:24,930
we also

1077
01:14:26,780 --> 01:14:30,970
so this is what

1078
01:14:34,010 --> 01:14:37,070
is sum of all possible solutions to the issues

1079
01:14:38,800 --> 01:14:41,780
you know

1080
01:14:49,150 --> 01:14:51,610
one used

1081
01:14:51,630 --> 01:14:54,010
so really gesture

1082
01:14:54,030 --> 01:14:55,550
so here

1083
01:14:55,680 --> 01:14:57,510
we want to do

1084
01:14:59,990 --> 01:15:02,680
we want to do with respect to the

1085
01:15:02,680 --> 01:15:03,930
the to

1086
01:15:03,950 --> 01:15:07,760
so the projection of the way

1087
01:15:19,300 --> 01:15:22,260
it's always the case

1088
01:15:28,280 --> 01:15:32,280
it is will be

1089
01:15:32,380 --> 01:15:33,490
we do

1090
01:15:54,150 --> 01:15:58,430
so this is given

1091
01:16:09,720 --> 01:16:11,720
she which also

1092
01:16:11,740 --> 01:16:17,050
we want to do something with

1093
01:16:17,130 --> 01:16:19,740
he was small

1094
01:16:20,650 --> 01:16:22,240
the reason

1095
01:16:22,240 --> 01:16:23,840
well you

1096
01:16:24,180 --> 01:16:27,470
two minutes

1097
01:16:29,900 --> 01:16:33,650
we see

1098
01:16:36,860 --> 01:16:39,170
with respect

1099
01:16:48,530 --> 01:16:50,930
these constraint

1100
01:16:54,380 --> 01:16:57,280
it is a lot

1101
01:16:59,320 --> 01:17:00,930
this is

1102
01:17:02,840 --> 01:17:05,010
the rule is

1103
01:17:05,030 --> 01:17:05,950
we this

1104
01:17:07,050 --> 01:17:08,820
we're doing this

1105
01:17:08,840 --> 01:17:11,760
this where it all

1106
01:17:11,780 --> 01:17:15,800
so i fail to see where we all

1107
01:17:16,340 --> 01:17:17,930
it is

1108
01:17:21,570 --> 01:17:23,720
so all the

1109
01:17:27,200 --> 01:17:28,010
i j

1110
01:17:28,030 --> 01:17:31,650
do you want to say

1111
01:17:34,590 --> 01:17:38,880
that the true something

1112
01:17:45,610 --> 01:17:52,300
so in sense

1113
01:17:52,300 --> 01:17:54,130
so why true

1114
01:17:58,430 --> 01:18:04,050
to you c

1115
01:18:10,680 --> 01:18:13,910
so so

1116
01:18:14,030 --> 01:18:19,110
what i want to give you

1117
01:18:27,680 --> 01:18:29,720
this work

1118
01:18:29,740 --> 01:18:31,470
we want to said

1119
01:18:31,490 --> 01:18:34,450
i mean it

1120
01:18:34,490 --> 01:18:37,070
great rooms

1121
01:18:37,070 --> 01:18:42,340
refused to really

1122
01:18:42,360 --> 01:18:44,090
in the world

1123
01:18:44,240 --> 01:18:47,110
through the radio

1124
01:18:47,130 --> 01:18:48,510
you to

1125
01:18:48,510 --> 01:18:51,150
it is

1126
01:18:51,180 --> 01:18:54,430
you do

1127
01:18:54,610 --> 01:18:57,010
it is

1128
01:18:57,010 --> 01:19:05,930
you to do

1129
01:19:05,950 --> 01:19:13,220
it's zero

1130
01:19:13,240 --> 01:19:17,010
o two

1131
01:19:18,150 --> 01:19:23,260
the solution in

1132
01:19:23,280 --> 01:19:25,380
so a great read

1133
01:19:25,380 --> 01:19:29,070
so four

1134
01:19:30,220 --> 01:19:33,930
you've got all these

1135
01:19:33,930 --> 01:19:35,080
given any

1136
01:19:35,080 --> 01:19:38,350
allocation of the variables if i swap one of them

1137
01:19:38,370 --> 01:19:40,230
for another variable

1138
01:19:40,260 --> 01:19:43,410
how much can the value of f change

1139
01:19:43,430 --> 01:19:48,950
and what it this is requiring is that if i change the i've been put

1140
01:19:48,950 --> 01:19:50,820
it must be bounded by c

1141
01:19:50,820 --> 01:19:52,060
so by

1142
01:19:53,070 --> 01:19:57,920
so it provides band on if you like the variability of s

1143
01:19:57,950 --> 01:20:00,400
as you change individual elements

1144
01:20:00,420 --> 01:20:02,940
but it's assumed over all possible

1145
01:20:02,960 --> 01:20:06,600
choices so it must apply whatever the other variables are

1146
01:20:06,610 --> 01:20:10,570
if you change this one you can't change it by more CI so it's quite

1147
01:20:10,570 --> 01:20:17,910
a strong conditions and much of the advanced tab concentration inequalities are about weakening that

1148
01:20:17,910 --> 01:20:26,080
condition and using other estimates that indicate the range of change in the slightly weaker

1149
01:20:27,700 --> 01:20:30,070
OK so

1150
01:20:30,080 --> 01:20:32,670
the first question i guess is

1151
01:20:32,690 --> 01:20:33,380
you know

1152
01:20:33,390 --> 01:20:35,350
how does her fifteen

1153
01:20:35,360 --> 01:20:36,600
come from this

1154
01:20:36,610 --> 01:20:40,290
and perhaps there is something that you might want to just spend

1155
01:20:40,360 --> 01:20:44,620
two minutes commencing itself a piece of paper if you've got one

1156
01:20:44,630 --> 01:20:50,090
you know what what making substitution that function how do you actually get back to

1157
01:20:50,090 --> 01:20:51,510
what thing

1158
01:20:51,510 --> 01:20:53,350
OK this which was that

1159
01:20:54,180 --> 01:20:55,770
so one

1160
01:20:55,770 --> 01:20:58,360
but just to minutes small

1161
01:20:58,410 --> 01:20:59,910
so there's

1162
01:20:59,920 --> 01:21:00,880
it's just the

1163
01:21:00,890 --> 01:21:05,260
the average node is it's one over and some of its side critical

1164
01:21:07,710 --> 01:21:10,850
what you have to do is work out what CI is in the case of

1165
01:21:11,560 --> 01:21:14,330
how much can the average change

1166
01:21:14,350 --> 01:21:16,180
if you substitute

1167
01:21:17,870 --> 01:21:19,880
the i th component

1168
01:21:20,100 --> 01:21:23,490
remember the i th component has this

1169
01:21:23,510 --> 01:21:26,590
rangebound AIP i

1170
01:21:29,600 --> 01:21:35,690
so if we substitute that song

1171
01:23:50,350 --> 01:23:57,870
the functions you just replaced by mindset and you can actually get both inequalities holdings

1172
01:23:57,870 --> 01:24:02,700
so that you actually compounded from both sides but i won't go into the details

1173
01:24:02,700 --> 01:24:06,080
of that OK so that's where the two come from

1174
01:24:06,130 --> 01:24:09,160
and you can apply the same technique with climate

1175
01:24:09,310 --> 01:24:12,310
and put the absolute values here

1176
01:24:12,310 --> 01:24:18,780
and put it to here so it's just a simple union bound to get that

1177
01:24:20,970 --> 01:24:25,570
now the next thing i think is worth looking at briefly as well

1178
01:24:25,570 --> 01:24:27,970
is inverting this

1179
01:24:29,950 --> 01:24:33,870
if you imagine we're getting sort of used to doing this now

1180
01:24:33,890 --> 01:24:40,820
whenever we have these sort of bounds on probabilities of things going wrong over oversample

1181
01:24:40,840 --> 01:24:46,110
so notice i should emphasise again if i look at this here this actually is

1182
01:24:46,130 --> 01:24:50,800
a mild irritation peter the end you know this is a sample probability here's the

1183
01:24:50,800 --> 01:24:52,680
probability of the whole sample

1184
01:24:52,700 --> 01:24:54,800
doing something

1185
01:24:54,820 --> 01:24:58,070
being less than this well we know what we always do with that we set

1186
01:24:58,070 --> 01:25:02,280
it equal to delta we invert right i mean just that hands that so we

1187
01:25:02,300 --> 01:25:06,180
just plug it in and do it so if you plug that in equal to

1188
01:25:06,180 --> 01:25:07,450
delta here

1189
01:25:07,470 --> 01:25:10,630
do the usual thing take logs

1190
01:25:10,640 --> 01:25:15,240
take the minus sign over you get log one over delta

1191
01:25:15,260 --> 01:25:18,490
you get two ones some CI squared here

1192
01:25:18,510 --> 01:25:21,550
you bring over the some CI squared

1193
01:25:21,570 --> 01:25:26,260
and the divided by the two cigarettes one squared is some CI square over two

1194
01:25:26,260 --> 01:25:27,840
log one delta

1195
01:25:27,870 --> 01:25:30,820
and the only difference got here

1196
01:25:30,870 --> 01:25:33,090
that we didn't have before

1197
01:25:33,110 --> 01:25:36,800
is that steps long squared not up

1198
01:25:36,820 --> 01:25:40,610
and that's where the square root comes from the indicated in the

1199
01:25:40,630 --> 01:25:42,490
if you remember the

1200
01:25:42,510 --> 01:25:45,640
in case of errors in the

1201
01:25:45,660 --> 01:25:47,950
in the

1202
01:25:48,720 --> 01:25:50,840
dimension case so that

1203
01:25:50,860 --> 01:25:55,300
corresponds to the square root entering in on the right-hand side here

1204
01:25:56,530 --> 01:26:00,550
some CI spread over two log one delta

1205
01:26:03,090 --> 01:26:07,740
we also bring over there so you can then bound for the particular sample is

1206
01:26:07,740 --> 01:26:10,010
less than the expected value

1207
01:26:10,030 --> 01:26:12,180
of the function class

1208
01:26:12,200 --> 01:26:13,930
this sort of

1209
01:26:13,930 --> 01:26:16,550
the term that involves like one of the delta

1210
01:26:16,680 --> 01:26:21,090
and now we can that applies now with probability one minus delta so it's the

1211
01:26:21,090 --> 01:26:22,410
usual idea

1212
01:26:22,430 --> 01:26:25,450
we bound the probability of things going wrong

1213
01:26:25,450 --> 01:26:29,240
you know they were too far away from the mean and so with probability one

1214
01:26:29,240 --> 01:26:31,910
minus delta we're closer to the mean that

1215
01:26:31,930 --> 01:26:37,050
so this is almost always the way in which one wants to use things in

1216
01:26:37,050 --> 01:26:39,280
the learning context

1217
01:26:39,390 --> 01:26:44,390
frequently we in our case will be using

1218
01:26:44,490 --> 01:26:49,010
a sample in which case these will be identically distributed

1219
01:26:50,160 --> 01:26:54,550
typically therefore the CI will be the same and very frequently also it will be

1220
01:26:54,550 --> 01:26:58,700
something like c divided by n where n is the number of examples

1221
01:26:58,740 --> 01:27:01,470
and if we just plug that in

1222
01:27:01,490 --> 01:27:04,780
we can see that the whole we get then is this

1223
01:27:04,890 --> 01:27:09,780
when the end comes on the bottom now because the sea square was on the

1224
01:27:10,550 --> 01:27:14,010
notice that the we had one of n square had

1225
01:27:14,010 --> 01:27:19,430
the sum is from one up to and so we get n copies of c

1226
01:27:19,800 --> 01:27:23,950
and one of the inns therefore cancels so we end up with c squared divided

1227
01:27:23,950 --> 01:27:25,490
by two and

1228
01:27:26,260 --> 01:27:28,740
now that's

1229
01:27:28,760 --> 01:27:32,950
always the former again again with the square root in the rest of

1230
01:27:32,950 --> 01:27:34,860
in the segment a b

1231
01:27:34,870 --> 01:27:43,310
and that the expectation of any function f of x is what what's there

1232
01:27:43,320 --> 01:27:45,930
the integral of f of x on a b

1233
01:27:46,050 --> 01:27:49,630
divided by b minus

1234
01:27:49,630 --> 01:27:56,820
another distribution that you'll find all the time is the gaussian distribution

1235
01:27:56,870 --> 01:28:03,090
it is defined by its it's totally characterised by its mean and its variance

1236
01:28:03,320 --> 01:28:04,640
this square

1237
01:28:04,660 --> 01:28:07,780
i write this as an of

1238
01:28:07,790 --> 01:28:17,140
and square and it's defined by its probability density which is exponential

1239
01:28:17,150 --> 01:28:23,460
of mine is the square of x minus an

1240
01:28:24,240 --> 01:28:29,060
there's an x here that to the definitely should be there so

1241
01:28:29,820 --> 01:28:36,710
the density is exponential x minus and square over two sigma square

1242
01:28:36,740 --> 01:28:39,110
and this term here

1243
01:28:39,320 --> 01:28:41,490
the normalisation so that

1244
01:28:41,870 --> 01:28:47,050
the figure of this thing is exactly equal to one so it's a probability

1245
01:28:54,360 --> 01:28:58,820
the random vector that's you learn that that shows in an

1246
01:28:58,830 --> 01:29:00,380
countries where are

1247
01:29:00,390 --> 01:29:02,340
ghost victory

1248
01:29:03,440 --> 01:29:09,210
we use the same location as before with no being

1249
01:29:10,230 --> 01:29:11,480
of the means

1250
01:29:11,490 --> 01:29:16,400
of of of the gaussian vector x

1251
01:29:16,450 --> 01:29:20,060
and sigma square is the covariance matrix

1252
01:29:20,060 --> 01:29:21,780
i feel gaussian vectors

1253
01:29:22,470 --> 01:29:28,690
four gas and you just need to compute the mean and the covariance matrix

1254
01:29:28,700 --> 01:29:33,850
and you've defined the whole the probability of the whole of the

1255
01:29:33,860 --> 01:29:35,390
of the gaussians vectors

1256
01:29:35,440 --> 01:29:39,710
now there

1257
01:29:39,730 --> 01:29:44,650
the probability density is similar to the one before you you have to

1258
01:29:45,220 --> 01:29:50,960
here i use matrix and the matrix vector notation so x minus

1259
01:29:50,990 --> 01:29:52,820
and in the vector

1260
01:29:52,820 --> 01:29:56,400
because sigma is the covariance matrix

1261
01:29:56,430 --> 01:29:58,830
of size d times

1262
01:29:58,870 --> 01:30:09,540
and so you are you have to inverting matter to find the pdf

1263
01:30:09,710 --> 01:30:13,250
in in

1264
01:30:18,200 --> 01:30:23,280
and no once you have rendered images random vectors

1265
01:30:23,290 --> 01:30:28,700
you may or several unavoidable you may

1266
01:30:28,730 --> 01:30:34,340
think about studying both variables at the same time since and so that would be

1267
01:30:34,340 --> 01:30:35,920
using the joint

1268
01:30:40,870 --> 01:30:48,070
again if you assume that you have to go into that are independent

1269
01:30:48,080 --> 01:30:50,340
then you may consider

1270
01:30:50,350 --> 01:30:54,570
you may want to consider the

1271
01:30:54,620 --> 01:30:58,150
the during the vector x one x two

1272
01:30:58,210 --> 01:31:01,410
and wonder when

1273
01:31:03,240 --> 01:31:05,430
again was to the

1274
01:31:05,440 --> 01:31:08,150
probabilities are on this random vector

1275
01:31:08,150 --> 01:31:13,910
than if x one and x two are two independant condenses then you see that

1276
01:31:13,910 --> 01:31:16,590
the probability of a

1277
01:31:16,610 --> 01:31:18,320
that is being basically

1278
01:31:18,330 --> 01:31:23,010
the product of the two provinces of x one and x two

1279
01:31:23,070 --> 01:31:25,840
OK so the joint probability

1280
01:31:25,880 --> 01:31:31,730
is the study of the joint probability of x and y with x y and

1281
01:31:31,730 --> 01:31:33,300
i are

1282
01:31:33,310 --> 01:31:38,080
two random variable is the study of the problem is the probability of the the

1283
01:31:38,080 --> 01:31:43,780
two events at the same times and if you have independent

1284
01:31:43,790 --> 01:31:48,320
random variables then do for it is really easy because it is the product of

1285
01:31:48,320 --> 01:31:52,430
the two to probabilities

1286
01:31:54,180 --> 01:32:01,180
now when the variables are not independent

1287
01:32:01,240 --> 01:32:07,190
and this decomposition of the joint probability into a product is not true anymore

1288
01:32:07,210 --> 01:32:11,690
and that actually happens in most of the time right if y

1289
01:32:11,740 --> 01:32:16,180
is that the random parity viability

1290
01:32:16,200 --> 01:32:20,010
sorry if y is the random variable

1291
01:32:20,060 --> 01:32:23,920
it doesn't matter whether someone is sikora is not sick

1292
01:32:23,980 --> 01:32:30,970
and x is the random variable that measures whether the best on on this pathology

1293
01:32:30,970 --> 01:32:34,260
has been positive or negative

1294
01:32:34,310 --> 01:32:38,880
i write here the total numbers

1295
01:32:38,890 --> 01:32:40,840
for each

1296
01:32:40,880 --> 01:32:44,250
of the columns in rules

1297
01:32:44,330 --> 01:32:46,410
then you may see that the

1298
01:32:46,890 --> 01:32:48,620
to be positive

1299
01:32:48,620 --> 01:32:51,850
to be that the test is positive

1300
01:32:51,890 --> 01:32:55,430
this one the probability of being sick

1301
01:32:55,430 --> 01:32:56,800
and again

1302
01:32:56,850 --> 01:32:59,930
it this problem

1303
01:32:59,970 --> 01:33:03,610
so this is an amazing way how we RLC circuits

1304
01:33:03,660 --> 01:33:08,010
you can do wonderful things because you can manipulate omega because zero very easily by

1305
01:33:08,030 --> 01:33:09,760
chasing alan c

1306
01:33:09,780 --> 01:33:11,200
you can manipulate

1307
01:33:11,200 --> 01:33:12,700
the driving frequency

1308
01:33:12,720 --> 01:33:19,110
also very easily

1309
01:33:19,120 --> 01:33:20,610
so it is clear that

1310
01:33:22,240 --> 01:33:24,410
response strongly

1311
01:33:24,450 --> 01:33:29,570
when they are exposed to the resonance frequency we've seen that full pendulums

1312
01:33:29,620 --> 01:33:31,620
we've seen that first springs

1313
01:33:31,660 --> 01:33:34,220
we've seen it for wine glass

1314
01:33:34,240 --> 01:33:38,680
and we've seen is now for an RLC circuit

1315
01:33:38,680 --> 01:33:40,140
so these systems

1316
01:33:40,140 --> 01:33:41,620
at resonance

1317
01:33:41,660 --> 01:33:44,280
absorb a large amount of energy

1318
01:33:44,300 --> 01:33:45,760
unit time

1319
01:33:45,780 --> 01:33:50,590
out of the driver

1320
01:33:50,640 --> 01:33:52,910
i have here

1321
01:33:52,910 --> 01:33:54,930
two tuning forks

1322
01:33:54,990 --> 01:33:57,090
which have

1323
01:33:57,200 --> 01:34:01,240
extremely high q

1324
01:34:01,350 --> 01:34:06,050
my attempts to measure it i concluded this way larger than even a thousand

1325
01:34:06,110 --> 01:34:07,890
and they have exactly

1326
01:34:07,930 --> 01:34:10,070
the same frequency both

1327
01:34:10,120 --> 01:34:11,990
two hundred and fifty six hertz

1328
01:34:12,010 --> 01:34:19,030
this one noone this one noone was one of six and that's the way they

1329
01:34:19,050 --> 01:34:21,970
are designed to a high degree of accuracy

1330
01:34:22,070 --> 01:34:26,050
better than the fraction of one hundred

1331
01:34:26,070 --> 01:34:29,410
but the queues are so high

1332
01:34:29,430 --> 01:34:32,780
that if you were to plot

1333
01:34:32,820 --> 01:34:34,700
drive these tuning forks

1334
01:34:34,700 --> 01:34:37,050
you want to plot here

1335
01:34:37,050 --> 01:34:39,590
his average power

1336
01:34:39,640 --> 01:34:41,620
as a function of omega

1337
01:34:41,660 --> 01:34:47,200
then you would get something like this

1338
01:34:47,260 --> 01:34:48,300
the things

1339
01:34:48,300 --> 01:34:52,010
you have to drive it exactly at the right frequency otherwise

1340
01:34:52,070 --> 01:34:53,320
it will not

1341
01:34:53,320 --> 01:34:56,350
going to residents

1342
01:34:56,430 --> 01:35:00,870
well we know how to get this going into baggage that means you don't spectrum

1343
01:35:00,870 --> 01:35:04,640
on it picks out if we consider the lights

1344
01:35:04,680 --> 01:35:07,050
now i'm going to show you something remarkable

1345
01:35:07,140 --> 01:35:10,970
when this one generates two hundred fifty six pressure waves

1346
01:35:11,120 --> 01:35:12,280
this one

1347
01:35:12,370 --> 01:35:14,320
feels the pressure wave

1348
01:35:14,340 --> 01:35:17,260
and he was it

1349
01:35:17,280 --> 01:35:21,260
because it just at the right frequency

1350
01:35:21,320 --> 01:35:24,550
and so it starts to oscillate

1351
01:35:24,590 --> 01:35:26,870
and so when i stop this one

1352
01:35:26,890 --> 01:35:28,320
hear this one

1353
01:35:28,410 --> 01:35:29,720
that's called

1354
01:35:33,930 --> 01:35:37,430
let's do that first

1355
01:35:37,510 --> 01:35:41,910
you must understand that the sound waves go from here

1356
01:35:41,930 --> 01:35:43,050
two there

1357
01:35:43,050 --> 01:35:44,340
not much

1358
01:35:44,340 --> 01:35:46,260
power reaches that point

1359
01:35:46,300 --> 01:35:48,660
so when i stop this one

1360
01:35:48,700 --> 01:35:50,470
you sound but it's not

1361
01:35:52,140 --> 01:35:56,180
we have to be very quiet

1362
01:35:56,280 --> 01:36:09,700
la niemi move and i can do the same by hitting this one and this

1363
01:36:09,700 --> 01:36:11,800
one will start to resonate

1364
01:36:11,870 --> 01:36:21,180
the to noon

1365
01:36:24,120 --> 01:36:28,120
if the driving frequency

1366
01:36:28,140 --> 01:36:32,780
is off by a fraction of the first one hundred is enough

1367
01:36:32,820 --> 01:36:34,760
one hundred difference

1368
01:36:34,820 --> 01:36:36,570
because the q is so high

1369
01:36:36,660 --> 01:36:41,110
and this system will not be able to get this one go going

1370
01:36:41,160 --> 01:36:45,350
and i can make this frequency a little more than two hundred and fifty six

1371
01:36:46,010 --> 01:36:49,300
by pulling his weight on here

1372
01:36:49,390 --> 01:36:50,820
we have measured

1373
01:36:50,840 --> 01:36:52,410
the frequency

1374
01:36:53,220 --> 01:36:54,280
loaded way

1375
01:36:54,300 --> 01:36:59,450
it's roughly two hundred and fifty five

1376
01:36:59,490 --> 01:37:02,370
you somewhere here because it's so narrow

1377
01:37:02,410 --> 01:37:05,780
resonance absorption peak is very sharp by the way

1378
01:37:05,780 --> 01:37:09,560
maxwell's equations

1379
01:37:10,600 --> 01:37:13,770
the victory for maxwell

1380
01:37:13,830 --> 01:37:16,800
maxwell was not only able to predict the existence

1381
01:37:16,800 --> 01:37:18,550
of electromagnetic waves

1382
01:37:18,560 --> 01:37:20,640
but he was even able to predict

1383
01:37:20,660 --> 01:37:22,840
that they would move through vacuum

1384
01:37:22,920 --> 01:37:25,040
with that speed

1385
01:37:25,080 --> 01:37:27,410
what an unbelievable victory

1386
01:37:27,480 --> 01:37:31,670
well you come to think of it that actually non-zero

1387
01:37:31,740 --> 01:37:33,300
can be measured

1388
01:37:33,310 --> 01:37:34,690
in a standard way

1389
01:37:34,720 --> 01:37:37,570
it follows from coulombs law

1390
01:37:37,620 --> 01:37:40,680
nothing to do with stability have nothing to do with the

1391
01:37:40,690 --> 01:37:42,810
has nothing to do with traveling waves

1392
01:37:42,890 --> 01:37:44,160
actually non-zero

1393
01:37:44,180 --> 01:37:47,090
is about eight point eight five

1394
01:37:48,100 --> 01:37:50,370
times ten to the minus twelve

1395
01:37:50,520 --> 01:37:52,920
as i units

1396
01:37:52,970 --> 01:37:58,280
museo is equally static can be measured from the force which two

1397
01:37:58,280 --> 01:38:01,280
why is which on the current attract each other

1398
01:38:01,330 --> 01:38:03,560
note ability no the immunity

1399
01:38:03,560 --> 01:38:06,150
it's nothing to do with electromagnetic waves

1400
01:38:07,780 --> 01:38:09,970
it's about one point two six

1401
01:38:10,010 --> 01:38:14,060
times ten to the the minus sixteen s i units

1402
01:38:14,070 --> 01:38:16,830
and if you multiply them

1403
01:38:16,850 --> 01:38:19,230
substitute in this equation

1404
01:38:19,230 --> 01:38:21,130
you'll find that c

1405
01:38:21,180 --> 01:38:23,430
because two point nine nine

1406
01:38:23,440 --> 01:38:25,160
times ten to eight

1407
01:38:26,180 --> 01:38:28,280
per second

1408
01:38:28,930 --> 01:38:31,160
believable wallis success

1409
01:38:34,210 --> 01:38:36,970
it always baffles me how two quantities

1410
01:38:36,980 --> 01:38:38,980
so static

1411
01:38:38,990 --> 01:38:42,220
and seemingly so related to

1412
01:38:42,270 --> 01:38:44,070
moving away with you

1413
01:38:44,080 --> 01:38:46,750
it is indeed it is all over the place

1414
01:38:46,760 --> 01:38:47,690
how they

1415
01:38:47,730 --> 01:38:49,060
can predict

1416
01:38:49,080 --> 01:38:52,210
the speed of light

1417
01:38:52,230 --> 01:38:55,070
suppose i ask you to measure the

1418
01:38:55,080 --> 01:38:57,860
pressure in your priors of your car

1419
01:38:57,960 --> 01:39:01,710
and i would ask you to also measure the voltage of your battery

1420
01:39:01,720 --> 01:39:06,100
and then to predict the speed of the car it's almost something like that

1421
01:39:06,280 --> 01:39:07,820
this bizarre

1422
01:39:07,900 --> 01:39:12,280
but it works and it was a great victory and of course it justified

1423
01:39:12,320 --> 01:39:13,300
and finally

1424
01:39:13,310 --> 01:39:14,790
this one term

1425
01:39:14,840 --> 01:39:16,570
which is this displacement

1426
01:39:16,570 --> 01:39:20,360
current term

1427
01:39:20,600 --> 01:39:23,140
together you and i

1428
01:39:23,160 --> 01:39:24,820
the proof

1429
01:39:24,820 --> 01:39:28,030
this is indeed necessary condition

1430
01:39:28,080 --> 01:39:30,610
so these equations satisfied

1431
01:39:30,620 --> 01:39:32,240
maxwell's equations

1432
01:39:32,240 --> 01:39:34,400
we'll do it together i will do

1433
01:39:34,460 --> 01:39:35,710
fifty percent

1434
01:39:35,720 --> 01:39:37,670
you will do the out of fifty percent

1435
01:39:37,690 --> 01:39:40,430
and assigned number nine so we split this bill

1436
01:39:40,440 --> 01:39:44,990
fifty fifty

1437
01:39:44,990 --> 01:39:47,100
o making new drawing

1438
01:39:47,160 --> 01:39:51,370
and i will do something that i really have to do in lectures i will

1439
01:39:51,430 --> 01:39:53,100
give you

1440
01:39:53,150 --> 01:39:57,550
eight minutes of hardcore mass you hate

1441
01:39:57,790 --> 01:40:01,410
i'm going to make a new drawing

1442
01:40:01,470 --> 01:40:02,880
which is

1443
01:40:02,900 --> 01:40:06,270
not too different from what you have there

1444
01:40:06,300 --> 01:40:08,300
this is the

1445
01:40:08,320 --> 01:40:09,970
this is diseasex

1446
01:40:10,040 --> 01:40:11,810
and this is why

1447
01:40:11,860 --> 01:40:14,580
and it equal zero

1448
01:40:14,630 --> 01:40:17,320
i'm going to draw here

1449
01:40:17,360 --> 01:40:18,840
the electric field

1450
01:40:19,040 --> 01:40:25,480
like so

1451
01:40:25,510 --> 01:40:30,140
this is a zero

1452
01:40:30,170 --> 01:40:31,680
electric field

1453
01:40:31,690 --> 01:40:35,110
the strength

1454
01:40:35,120 --> 01:40:38,460
and i'm going to apply to MP's law

1455
01:40:39,160 --> 01:40:41,280
part the bargain

1456
01:40:41,370 --> 01:40:45,320
close to the integral of the delta

1457
01:40:45,380 --> 01:40:46,990
you see there

1458
01:40:46,990 --> 01:40:49,230
equals actually non-zero

1459
01:40:49,280 --> 01:40:50,940
times new zero

1460
01:40:51,000 --> 01:40:54,420
time the phi tdt

1461
01:40:54,480 --> 01:40:55,960
we're dealing with vacuum

1462
01:40:56,010 --> 01:40:57,470
so kappa is one

1463
01:40:57,490 --> 01:40:59,850
and the dielectric constant is one

1464
01:40:59,910 --> 01:41:04,000
and there is no such thing as a current i because we are an empty

1465
01:41:04,000 --> 01:41:08,100
space this whole term does not exist

1466
01:41:08,150 --> 01:41:09,580
what is law

1467
01:41:10,650 --> 01:41:12,610
it close to

1468
01:41:12,620 --> 01:41:17,310
and i need to surfaces i attach an open surface to that close loop

1469
01:41:17,350 --> 01:41:19,540
i'm going to choose

1470
01:41:19,650 --> 01:41:21,100
close look

1471
01:41:21,130 --> 01:41:23,900
in the plane y c

1472
01:41:23,910 --> 01:41:25,330
and this is going to be like

1473
01:41:25,340 --> 01:41:31,960
closed loop

1474
01:41:31,960 --> 01:41:33,000
this year

1475
01:41:33,010 --> 01:41:35,320
it's going to be my close loop

1476
01:41:35,360 --> 01:41:37,710
this length is l

1477
01:41:37,710 --> 01:41:38,850
and the length

1478
01:41:38,870 --> 01:41:40,330
what do we

1479
01:41:40,380 --> 01:41:41,330
this site

1480
01:41:41,340 --> 01:41:47,520
is land and divided by four

1481
01:41:47,560 --> 01:41:51,030
i have to close to integral of b delta you know i will do that

1482
01:41:52,230 --> 01:41:54,580
i will first go the hardest part

1483
01:41:54,590 --> 01:41:57,350
which is the time derivative of the electric flux

1484
01:41:57,400 --> 01:41:59,910
through this surface

1485
01:41:59,920 --> 01:42:03,080
the problem is that the electric field is not constant

1486
01:42:03,140 --> 01:42:05,960
the electric field is zero here

1487
01:42:06,070 --> 01:42:09,400
it has a maximum here and falls off in this way so i have to

1488
01:42:09,400 --> 01:42:11,220
do an integral

1489
01:42:11,530 --> 01:42:19,350
so i'm going to make a slice here

1490
01:42:19,380 --> 01:42:25,050
and this slice here has the right disease

1491
01:42:25,140 --> 01:42:27,540
and a very narrow slice

1492
01:42:27,540 --> 01:42:30,730
the electric field is approximately constant

1493
01:42:30,780 --> 01:42:32,140
right here

1494
01:42:32,170 --> 01:42:33,750
electric field

1495
01:42:33,790 --> 01:42:36,720
has this value but everywhere in that slice

1496
01:42:36,730 --> 01:42:39,650
it has the same value because remember the plane wave

1497
01:42:40,540 --> 01:42:41,120
and so

1498
01:42:41,150 --> 01:42:44,330
i draw your aligned

1499
01:42:45,230 --> 01:42:48,050
and so everywhere in that slice

1500
01:42:48,080 --> 01:42:49,880
electric field

1501
01:42:49,900 --> 01:42:51,180
has exactly

1502
01:42:51,230 --> 01:42:57,320
this value

1503
01:42:57,330 --> 01:43:02,310
and that value is given by this equation you tell me what you see is

1504
01:43:02,310 --> 01:43:05,540
time people zero i know that value is

1505
01:43:05,590 --> 01:43:07,620
that's this fellow

1506
01:43:07,790 --> 01:43:09,860
so now i have to calculate

1507
01:43:11,910 --> 01:43:14,580
electric flux

1508
01:43:14,580 --> 01:43:19,390
so finally

1509
01:43:19,530 --> 01:43:23,920
i have to take the dot product between the a and the

1510
01:43:23,930 --> 01:43:29,080
electric field remember fluxes the dot product it can be a little too would be

1511
01:43:29,080 --> 01:43:30,730
a out

1512
01:43:30,770 --> 01:43:33,420
because he is also what makes life easy

1513
01:43:33,630 --> 01:43:37,460
i have to remember that later than when i did close the integral of e

1514
01:43:37,460 --> 01:43:38,760
dot pl

1515
01:43:38,770 --> 01:43:40,550
then looking from below

1516
01:43:40,560 --> 01:43:43,090
i have to go clockwise because i remember the

1517
01:43:43,110 --> 01:43:45,000
right hand corkscrew

1518
01:43:45,080 --> 01:43:49,230
so i get all my mind signs of plus signs just right

1519
01:43:49,310 --> 01:43:52,570
so the a and b are are in the same direction so what is the

1520
01:43:52,570 --> 01:43:55,100
a of this little slide

1521
01:43:55,160 --> 01:43:58,220
that's is al times easy

1522
01:43:58,220 --> 01:44:00,080
so i get l

1523
01:44:00,140 --> 01:44:01,210
i'm thinking

1524
01:44:02,420 --> 01:44:06,140
one is the local electric field in this lies well that's

1525
01:44:06,150 --> 01:44:07,810
that equation

1526
01:44:07,810 --> 01:44:09,960
so i get a zero

1527
01:44:10,030 --> 01:44:14,860
times the cosine

1528
01:44:14,880 --> 01:44:16,010
of cases

1529
01:44:16,160 --> 01:44:18,910
miners omega t

1530
01:44:19,040 --> 01:44:21,860
this actual of course is gonna

1531
01:44:21,860 --> 01:44:26,010
so here amongst all the choices i find the closest one in the set and

1532
01:44:26,010 --> 01:44:28,650
that's w star

1533
01:44:31,860 --> 01:44:35,500
now pick any other one in the convex set

1534
01:44:35,500 --> 01:44:37,390
sort of compared to

1535
01:44:37,410 --> 01:44:39,030
and if you do that

1536
01:44:39,100 --> 01:44:41,110
then this relationship holds

1537
01:44:41,170 --> 01:44:43,210
the distance

1538
01:44:43,220 --> 01:44:45,510
from here to here

1539
01:44:45,520 --> 01:44:47,970
it's the same as the distance of this

1540
01:44:48,000 --> 01:44:52,000
last is now use distance and this is actually quite wrong

1541
01:44:52,040 --> 01:44:54,660
it's not the distance because the triangle inequality

1542
01:44:54,680 --> 01:44:57,760
doesn't hold should use divergence the divergence

1543
01:44:57,760 --> 01:45:01,670
which is just another term we had to use because this this is misleading and

1544
01:45:01,680 --> 01:45:05,120
the term closest is also misleading because its distance

1545
01:45:05,140 --> 01:45:06,820
so these are

1546
01:45:06,860 --> 01:45:10,940
not distances with mistress that the divergence from here to here is at least as

1547
01:45:10,940 --> 01:45:12,830
big as the versions from here

1548
01:45:12,830 --> 01:45:16,970
clustered versions from here to here that you take euclidean distance squared

1549
01:45:17,030 --> 01:45:18,880
and your convex set

1550
01:45:18,910 --> 01:45:21,320
is a line

1551
01:45:21,330 --> 01:45:24,350
then you would have a right angle here and you would have to normal per

1552
01:45:24,350 --> 01:45:26,500
factory in theorem with

1553
01:45:26,540 --> 01:45:27,750
the quality

1554
01:45:27,760 --> 01:45:29,120
you would have

1555
01:45:29,190 --> 01:45:33,970
euclidean distance squared between this and this is the same as euclidean distance between this

1556
01:45:33,970 --> 01:45:34,720
and this

1557
01:45:34,740 --> 01:45:37,860
thus this and this

1558
01:45:37,890 --> 01:45:39,900
if it is convex

1559
01:45:39,910 --> 01:45:44,660
set it becomes an inequality even for euclidean distance squared

1560
01:45:44,750 --> 01:45:46,610
so in the picture here

1561
01:45:46,620 --> 01:45:47,850
i sort of

1562
01:45:48,020 --> 01:45:54,220
the euclidean case at least pictorially because i drew circles around here

1563
01:45:54,250 --> 01:45:56,570
you see which is distance

1564
01:45:56,610 --> 01:45:59,300
and then he is sort of a right angle

1565
01:46:00,350 --> 01:46:04,370
if i take other divergences these curves are not going to be straight lines to

1566
01:46:04,380 --> 01:46:09,370
gonna be curves as well and all kinds of fancy things

1567
01:46:09,430 --> 01:46:13,830
the beautiful thing is that this holds in great generality

1568
01:46:14,040 --> 01:46:21,780
examples so my take is my convex function now have this picture in mind again

1569
01:46:21,780 --> 01:46:24,280
this is the picture of mind right

1570
01:46:24,330 --> 01:46:30,430
and i take as my convex function

1571
01:46:30,550 --> 01:46:38,650
euclidean distance squared over two for convenience the derivative now

1572
01:46:38,700 --> 01:46:42,760
it's just a linear function

1573
01:46:42,760 --> 01:46:45,680
and then the divergence would be

1574
01:46:46,970 --> 01:46:53,310
the apply this function to the first arguement minus applying the function the second function

1575
01:46:54,890 --> 01:46:58,540
w two w times the gradient which happens to be this

1576
01:46:58,540 --> 01:47:02,700
and then you can simplify this and get euclidean distance squared

1577
01:47:02,760 --> 01:47:05,110
i magic

1578
01:47:05,130 --> 01:47:10,530
good know how do you get the relative entropy this is not additive rhythms

1579
01:47:10,530 --> 01:47:14,850
multiplicative updates additive multiplicative OK entropy entropy now

1580
01:47:14,890 --> 01:47:17,820
you take this kind of

1581
01:47:17,830 --> 01:47:20,830
convex functions

1582
01:47:20,850 --> 01:47:25,870
derivative of this is the logarithm of w this is a vector so it's the

1583
01:47:25,870 --> 01:47:27,950
vector but you apply

1584
01:47:27,960 --> 01:47:30,740
the logarithm to every component that's what this means

1585
01:47:30,770 --> 01:47:33,190
so it's the logarithm

1586
01:47:33,220 --> 01:47:37,390
and if you plan this and then use the same kind of former

1587
01:47:37,430 --> 01:47:40,140
that we always use f of w

1588
01:47:40,150 --> 01:47:46,160
twenty minus f of w minus w two minus w times the gradient

1589
01:47:46,210 --> 01:47:47,250
you end up

1590
01:47:47,250 --> 01:47:51,830
where the unnormalized relative entropy

1591
01:47:51,840 --> 01:47:56,060
OK so these are just examples

1592
01:47:56,080 --> 01:47:58,610
of this general bregman divergence

1593
01:47:58,620 --> 01:48:00,950
their fancier ones if you

1594
01:48:02,380 --> 01:48:06,380
the p p m algorithms are going to be motivated by this if you take

1595
01:48:06,880 --> 01:48:11,120
PMQ's that sum to one you take this you're

1596
01:48:11,130 --> 01:48:13,670
function this is the gradient

1597
01:48:13,680 --> 01:48:17,160
then you're bregman divergence happens to be this

1598
01:48:17,210 --> 01:48:18,110
thank you

1599
01:48:18,110 --> 01:48:27,560
p classic using a half then this is the euclidean distance as you see here

1600
01:48:27,560 --> 01:48:32,890
and this algorithms associated with each of these divergence they call PM algorithm for two

1601
01:48:32,890 --> 01:48:36,150
it's going to be the which are half gone say

1602
01:48:36,170 --> 01:48:40,370
another example is the balkan trippy

1603
01:48:40,430 --> 01:48:44,250
you take issue link as you as you convex function to be the sum of

1604
01:48:44,250 --> 01:48:45,860
the logs

1605
01:48:45,900 --> 01:48:49,970
the derivatives are going to be minus one of the w again vectorize

1606
01:48:50,020 --> 01:48:52,880
and then the entropy is going to be this kind of thing

1607
01:48:52,900 --> 01:48:56,560
this corresponds to the relative entropy between two

1608
01:48:56,600 --> 01:48:58,190
gaussians when the

1609
01:48:58,210 --> 01:49:00,910
axes are aligned

1610
01:49:00,950 --> 01:49:05,100
to be the unit axis

1611
01:49:05,110 --> 01:49:06,280
turns out you can

1612
01:49:06,650 --> 01:49:14,970
all these two online algorithms and the motivation using bregman divergence recently has become

1613
01:49:15,770 --> 01:49:17,740
very important because we

1614
01:49:17,750 --> 01:49:18,950
and now sort of

1615
01:49:18,970 --> 01:49:20,840
beginning to understand

1616
01:49:20,880 --> 01:49:25,260
how you have to change the when you go from ectopic to parameters to matrix

1617
01:49:25,260 --> 01:49:29,350
parameters so so far i kept always vector of

1618
01:49:33,890 --> 01:49:35,260
comm one of us

1619
01:49:35,440 --> 01:49:38,750
it's also here it's the same notation

1620
01:49:38,800 --> 01:49:41,200
it's componentwise

1621
01:49:42,960 --> 01:49:47,110
thank you so

1622
01:49:47,120 --> 01:49:50,410
so far i talked about parameters being vectors

1623
01:49:50,420 --> 01:49:52,560
of weights

1624
01:49:52,700 --> 01:49:58,290
the m and imagine that you want to update matrices

1625
01:49:58,330 --> 01:49:59,370
and then there is

1626
01:49:59,430 --> 01:50:06,000
nice generalizations of these divergences and the corresponding at different multiplicative updates long story talk

1627
01:50:06,000 --> 01:50:10,280
about some of the stuff in my research talks

1628
01:50:10,280 --> 01:50:14,050
i just want a flash of what the corresponding divergences are

1629
01:50:15,590 --> 01:50:19,110
this is something something called and we got the divergence

1630
01:50:19,110 --> 01:50:21,680
this would be a function this would be the

1631
01:50:21,700 --> 01:50:22,840
let's now

1632
01:50:22,870 --> 01:50:27,510
these are all matrix operations in other words

1633
01:50:27,560 --> 01:50:29,310
this is

1634
01:50:30,680 --> 01:50:34,400
and the logo for matrix is you take it stick i can value decomposition apply

1635
01:50:34,410 --> 01:50:36,990
the law to to the diagonal

1636
01:50:37,020 --> 01:50:40,210
and a density matrix is the generalisation

1637
01:50:42,300 --> 01:50:47,050
a probability distributions in the following sense if you look at the

1638
01:50:47,050 --> 01:50:56,510
i do additions to the right

1639
01:52:51,990 --> 01:52:57,130
in nineteen ninety seven

1640
01:53:03,990 --> 01:53:08,240
so one

1641
01:53:08,270 --> 01:53:13,130
what would you like

1642
01:53:13,180 --> 01:53:15,440
one two

1643
01:53:15,510 --> 01:53:18,220
we use

1644
01:53:18,230 --> 01:53:21,180
this is

1645
01:53:27,070 --> 01:53:29,020
this is a

1646
01:53:32,200 --> 01:53:36,050
i was

1647
01:53:41,670 --> 01:53:50,020
this is

1648
01:55:13,470 --> 01:55:19,400
we call

1649
01:55:19,400 --> 01:55:27,040
but i'm not sure it's necessarily need another one

1650
01:55:27,040 --> 01:55:29,520
so if you think of an answer i would just come along and i will

1651
01:55:29,520 --> 01:55:31,840
talk about it afterwards

1652
01:55:34,290 --> 01:55:37,090
my story about

1653
01:55:37,900 --> 01:55:39,970
abacus machines

1654
01:55:40,020 --> 01:55:45,160
and we introduce abacus machines purely as a technical device

1655
01:55:45,220 --> 01:55:47,840
helpless prisoners there

1656
01:55:47,920 --> 01:55:52,350
trying really to show the the turing machines and recursive functions are equivalent

1657
01:55:52,360 --> 01:55:55,730
and we do it by means of this intermediate arts

1658
01:55:55,790 --> 01:55:58,150
really that's the only reason

1659
01:55:58,160 --> 01:56:00,390
so again

1660
01:56:00,400 --> 01:56:01,910
consider anyway

1661
01:56:01,910 --> 01:56:07,170
the possibility of trying to show that all of these forms of computability sign

1662
01:56:07,190 --> 01:56:08,910
it turns out the way to do that

1663
01:56:08,970 --> 01:56:12,180
to show that we have this machine to turing machines

1664
01:56:12,210 --> 01:56:15,540
for every abacus machine is an equivalent turing machine

1665
01:56:15,560 --> 01:56:17,590
every recursive function

1666
01:56:17,670 --> 01:56:21,830
is equivalent abacus machine calculates that function

1667
01:56:21,900 --> 01:56:24,800
and then what a little bit messier hard

1668
01:56:24,810 --> 01:56:27,410
it is to show the

1669
01:56:27,420 --> 01:56:29,800
everything it you could do the turing machine

1670
01:56:29,850 --> 01:56:32,180
could is a recursive function

1671
01:56:32,890 --> 01:56:34,650
that completes

1672
01:56:34,660 --> 01:56:37,500
little circle of is if you like

1673
01:56:37,520 --> 01:56:40,600
various different people responsible for this

1674
01:56:40,670 --> 01:56:42,670
this is not the way

1675
01:56:42,730 --> 01:56:43,490
OK go

1676
01:56:43,520 --> 01:56:46,790
fruit is there OK let's we're heading to

1677
01:56:46,800 --> 01:56:48,040
it's this is all

1678
01:56:48,090 --> 01:56:49,850
stuff happened afterwards

1679
01:56:49,850 --> 01:56:52,370
to try to figure out how it all works

1680
01:56:52,410 --> 01:56:53,350
look in

1681
01:56:54,310 --> 01:56:58,350
here is that girls original papers translate

1682
01:56:58,410 --> 01:57:01,670
his collected works volume one or something from oxford

1683
01:57:01,730 --> 01:57:02,910
you can look in there

1684
01:57:02,930 --> 01:57:04,930
you want to see this sort of stuff

1685
01:57:04,940 --> 01:57:07,970
but this this is the sort of cleaning up that occurred

1686
01:57:07,980 --> 01:57:09,750
in the years afterwards

1687
01:57:09,810 --> 01:57:14,490
well the question computability was also lives well from turing

1688
01:57:14,540 --> 01:57:15,670
it was

1689
01:57:15,680 --> 01:57:20,030
not necessarily working on the same issues

1690
01:57:21,440 --> 01:57:24,090
we're going to have a look at the way for

1691
01:57:25,840 --> 01:57:27,540
three things god

1692
01:57:29,050 --> 01:57:32,800
this is stuff we talked about yesterday soldiers quickly go over

1693
01:57:32,860 --> 01:57:35,750
you don't just studying on an arbitrary turing machine

1694
01:57:35,790 --> 01:57:40,200
he really starting on the first of all by cleaning up during the shape

1695
01:57:40,220 --> 01:57:41,770
and producing what

1696
01:57:41,810 --> 01:57:44,680
this was officially called the canonical form OK

1697
01:57:44,740 --> 01:57:48,300
so we try regulate lecturing machines so that they

1698
01:57:48,360 --> 01:57:54,200
do specific things in specific ways unless we can get control the arguments

1699
01:57:54,260 --> 01:57:55,140
because the

1700
01:57:55,170 --> 01:57:57,210
first really really

1701
01:57:58,540 --> 01:58:02,960
internal first step is to reorganize the work in any turing machine

1702
01:58:02,970 --> 01:58:05,860
and this is actually how talking about yesterday

1703
01:58:05,870 --> 01:58:07,630
things like this that

1704
01:58:07,640 --> 01:58:09,680
we're trying to compute binary function

1705
01:58:10,640 --> 01:58:15,670
and the arguments with the numbers set up in black box ones

1706
01:58:15,680 --> 01:58:18,390
OK just a single link between them

1707
01:58:18,430 --> 01:58:20,780
and when we computed value

1708
01:58:20,790 --> 01:58:23,770
that will just be a single block of ones and that's what we understand by

1709
01:58:24,790 --> 01:58:27,260
nothing in the original definition turing machine

1710
01:58:27,300 --> 01:58:30,900
he said that you had to do this way just this is the cleanup

1711
01:58:30,910 --> 01:58:35,720
we could space arguments and values and computations how we want we don't do that

1712
01:58:35,720 --> 01:58:39,190
it's just too hard to argue that things

1713
01:58:39,750 --> 01:58:44,000
what we do is decide that whenever we have a turing machine

1714
01:58:44,010 --> 01:58:47,780
we always start and stop at the left must the number of ones

1715
01:58:47,790 --> 01:58:48,910
so receiving

1716
01:58:48,910 --> 01:58:50,440
there must be

1717
01:58:50,810 --> 01:58:54,640
but is only superfluous zeros here

1718
01:58:54,680 --> 01:58:58,490
and this is the first one coming in from the left

1719
01:58:58,540 --> 01:59:01,130
that's how we started with their arguments

1720
01:59:01,140 --> 01:59:02,130
and the blank

1721
01:59:02,140 --> 01:59:03,990
another argument

1722
01:59:04,060 --> 01:59:07,320
generally turing machines don't to like that can be

1723
01:59:07,380 --> 01:59:10,060
reorganized away like

1724
01:59:10,520 --> 01:59:13,460
if this is just an a number of finite number

1725
01:59:13,490 --> 01:59:18,270
then indeed the right hand had this will also have superfluous here at some point

1726
01:59:18,270 --> 01:59:20,500
in time

1727
01:59:20,630 --> 01:59:22,310
it's funny

1728
01:59:22,330 --> 01:59:25,520
at the end of the talk this is where things start the end of the

1729
01:59:27,000 --> 01:59:28,940
be someone's

1730
01:59:30,300 --> 01:59:33,380
the machine the left most of block ones

1731
01:59:33,460 --> 01:59:35,390
was superfluous zero

1732
01:59:35,400 --> 01:59:39,290
not going use this technique today i'm just trying to say we clean things up

1733
01:59:39,290 --> 01:59:42,990
we do not have to work with an arbitrary conception

1734
01:59:45,140 --> 01:59:48,200
so that's the sort of stuff we do here

1735
01:59:48,250 --> 01:59:52,430
we also do the same thing with abacus machines

1736
01:59:52,440 --> 01:59:57,790
it specifies standard configuration in canonical form if you like for abacus machines

1737
01:59:57,830 --> 02:00:00,770
so abacus machines might be calculating

1738
02:00:00,790 --> 02:00:01,960
some function of

1739
02:00:01,990 --> 02:00:04,930
n variables x one to xn

1740
02:00:05,740 --> 02:00:07,360
what you do is you put

1741
02:00:08,020 --> 02:00:11,950
let's talk sorry put x one xn

1742
02:00:11,960 --> 02:00:15,010
the arguments in the first n registers

1743
02:00:15,030 --> 02:00:17,510
or adjacent registers

1744
02:00:17,550 --> 02:00:19,590
if we encounter them

1745
02:00:19,610 --> 02:00:23,130
is an infinite number registers just like in adjacent bunch

1746
02:00:23,150 --> 02:00:24,940
so now we have a picture

1747
02:00:25,000 --> 02:00:28,290
of some registers like this

1748
02:00:28,300 --> 02:00:30,220
and we put x one and here

1749
02:00:30,250 --> 02:00:33,310
two songs the end actors

1750
02:00:33,320 --> 02:00:36,190
have a flowchart to control chart that

1751
02:00:36,250 --> 02:00:38,070
operates on those registers

1752
02:00:38,080 --> 02:00:40,200
all remaining registers are empty

1753
02:00:40,260 --> 02:00:41,410
and the results

1754
02:00:42,010 --> 02:00:43,790
just death

1755
02:00:43,790 --> 02:00:48,510
mike to be definitive that the result in some particular nine register might be the

1756
02:00:48,510 --> 02:00:50,760
next register back from all these

1757
02:00:50,780 --> 02:00:53,660
and that this will be the result

1758
02:00:55,450 --> 02:01:00,390
we can do that sort of thing

1759
02:01:02,270 --> 02:01:04,890
just those ideas this is the

1760
02:01:04,900 --> 02:01:06,720
proof idea for

1761
02:01:06,780 --> 02:01:12,810
proving that all abacus machines that during all can be represented as turing machines

1762
02:01:13,650 --> 02:01:16,010
the sort of says that there i guess

1763
02:01:17,070 --> 02:01:18,810
the idea of the proof

1764
02:01:18,870 --> 02:01:24,010
it's just that we're gonna take initial configuration of an abacus machine translated to turing

1765
02:01:25,220 --> 02:01:26,240
and that

1766
02:01:26,250 --> 02:01:30,470
and then translate each of the operations into things that you would do to a

1767
02:01:30,470 --> 02:01:33,030
turing machine is equivalent

1768
02:01:34,720 --> 02:01:35,790
i'm not going to give you

1769
02:01:35,790 --> 02:01:38,120
all the details even really

1770
02:01:38,240 --> 02:01:40,940
a great amount of the detail of the bits we do

1771
02:01:41,000 --> 02:01:45,280
but we should look at what happens with had to register and take from a

1772
02:01:45,280 --> 02:01:49,640
register which i think i did a little bit about

1773
02:01:51,050 --> 02:01:53,410
so we know that somewhere inside this

1774
02:01:53,430 --> 02:01:55,540
abacus machine description

1775
02:01:55,570 --> 02:01:56,790
they'll be

1776
02:01:58,010 --> 02:02:00,050
the case where we

1777
02:02:00,130 --> 02:02:03,140
added to register

1778
02:02:03,150 --> 02:02:05,260
and we've started translation

1779
02:02:06,820 --> 02:02:09,700
translating this abacus machine into some

1780
02:02:09,790 --> 02:02:15,870
specification of turing machine just like that the exact humans instead of them being registers

1781
02:02:15,910 --> 02:02:17,990
they can be whole series of

1782
02:02:18,000 --> 02:02:20,150
blocks separated by blanks

1783
02:02:20,150 --> 02:02:25,920
to look sort of like cross entropy

1784
02:02:28,090 --> 02:02:31,200
there's always seem like magic but it follows certain

1785
02:02:32,420 --> 02:02:39,140
once you have the routine it's OK

1786
02:02:39,160 --> 02:02:43,770
and people have done this for a long time they wrote books and books of

1787
02:02:43,820 --> 02:02:46,910
these exponential families and

1788
02:02:46,960 --> 02:02:49,810
basically they were too hard to read for me

1789
02:02:50,000 --> 02:02:55,450
so i got rid of them and use bregman divergences which asia

1790
02:02:55,570 --> 02:02:59,820
what's on another famous case

1791
02:02:59,890 --> 02:03:04,380
instances are natural numbers this is the distribution eroded again in terms of the expectation

1792
02:03:05,560 --> 02:03:09,100
the natural parameters log of the

1793
02:03:09,120 --> 02:03:10,430
if you write it

1794
02:03:10,450 --> 02:03:14,090
as function of the natural parameter becomes e to the linear minus to the data

1795
02:03:14,280 --> 02:03:19,440
this is your convex function obviously the exponential is convex

1796
02:03:20,730 --> 02:03:26,220
the derivative of this function you get it is the seat of back

1797
02:03:26,240 --> 02:03:26,910
and now

1798
02:03:26,940 --> 02:03:29,230
inverse of e to the data

1799
02:03:29,280 --> 02:03:32,480
you get a logarithmic

1800
02:03:32,530 --> 02:03:37,080
so you take e to the state sequence itself munich at the other function

1801
02:03:37,160 --> 02:03:39,690
it's an integral that

1802
02:03:39,720 --> 02:03:43,010
is the convex conjugate function

1803
02:03:43,020 --> 02:03:43,840
these two

1804
02:03:46,920 --> 02:03:52,730
is simply the log of this

1805
02:03:53,390 --> 02:03:58,050
and this sort of looks like unnormalized cross entropy

1806
02:03:58,060 --> 02:03:59,830
i will make all of this sort of more

1807
02:04:00,050 --> 02:04:02,600
that's the moment so this is part song

1808
02:04:02,740 --> 02:04:05,610
only process all

1809
02:04:06,850 --> 02:04:09,640
let's two

1810
02:04:09,650 --> 02:04:11,890
a more general observation

1811
02:04:11,910 --> 02:04:15,660
how these two relate

1812
02:04:15,670 --> 02:04:19,340
OK so you whenever you have a distribution the only

1813
02:04:19,360 --> 02:04:26,860
a reasonable measure of divergence between the distributions relative entropy does the coding theoretic interpretation

1814
02:04:27,790 --> 02:04:29,220
in terms of the

1815
02:04:29,240 --> 02:04:33,540
this local complaints if you take the wrong code available are a little bit too

1816
02:04:33,540 --> 02:04:35,860
much it's this

1817
02:04:35,870 --> 02:04:38,040
it goes back to shannon

1818
02:04:38,630 --> 02:04:42,010
you can read the first couple of chapters called this book

1819
02:04:42,060 --> 02:04:44,070
so this is the relative entropy

1820
02:04:44,080 --> 02:04:46,400
and then measure the relative entropy between

1821
02:04:46,410 --> 02:04:49,810
two distributions from the same family characterized by g

1822
02:04:51,270 --> 02:04:52,590
parameter theta

1823
02:04:52,610 --> 02:04:56,580
one parameter theta twelve and what i'm going to get out of this

1824
02:04:57,540 --> 02:05:04,790
that this is a bregman divergence this is familiar bregman divergence

1825
02:05:07,340 --> 02:05:09,000
how do prove this

1826
02:05:12,060 --> 02:05:16,180
this had this e to the linear form right

1827
02:05:16,200 --> 02:05:19,570
and e to the linear matches with the log because if you take logs the

1828
02:05:19,570 --> 02:05:22,870
linear pulls down and what have you know how to do a linear

1829
02:05:25,640 --> 02:05:29,830
e to the linear log only to the linear that's this part

1830
02:05:29,880 --> 02:05:33,070
the reference tends to cancer cells

1831
02:05:34,010 --> 02:05:37,440
minus log either you to the same thing

1832
02:05:37,460 --> 02:05:42,310
becomes this piece these two terms

1833
02:05:42,320 --> 02:05:46,180
and i have an integral over the evidence understand the step from to here

1834
02:05:46,190 --> 02:05:47,860
OK i just plugged in

1835
02:05:47,900 --> 02:05:49,090
the e to the

1836
02:05:49,110 --> 02:05:51,310
linear form until clocks

1837
02:05:51,360 --> 02:05:53,860
so i'm here

1838
02:05:54,610 --> 02:05:55,890
i distribute

1839
02:05:55,900 --> 02:06:00,570
over this whole thing because everything is constant except x so i have

1840
02:06:04,110 --> 02:06:10,340
i have here

1841
02:06:10,360 --> 02:06:16,660
the integral only staying over the x but this is an expectation

1842
02:06:16,680 --> 02:06:19,480
so that is only

1843
02:06:19,530 --> 02:06:23,160
and while i was actually need your data

1844
02:06:23,180 --> 02:06:25,620
so this year

1845
02:06:25,650 --> 02:06:27,830
is our normal form of the

1846
02:06:27,880 --> 02:06:30,180
which you should recognise now

1847
02:06:30,200 --> 02:06:32,530
for this relative entropy it is

1848
02:06:32,580 --> 02:06:39,860
convex function minus the linear approximation where this is g of the time

1849
02:06:39,870 --> 02:06:41,550
OK now if you take

1850
02:06:42,280 --> 02:06:48,340
line here and plug in the definition of convex conjugate

1851
02:06:48,360 --> 02:06:50,540
four different function

1852
02:06:50,560 --> 02:06:51,450
then the

1853
02:06:51,470 --> 02:06:55,430
transformation you end up with a very similar looking formula

1854
02:06:55,480 --> 02:06:57,440
we're now

1855
02:06:57,480 --> 02:07:01,720
you use the convex conjugate function as the convex function

1856
02:07:01,730 --> 02:07:06,480
twenty minus the linear approximation and here this is a little f of

1857
02:07:08,480 --> 02:07:10,350
so this

1858
02:07:12,960 --> 02:07:16,090
is the same as this

1859
02:07:16,110 --> 02:07:18,970
everything comes in pairs

1860
02:07:18,990 --> 02:07:24,250
it's related to the in some sense convex problem all of the dual problem the

1861
02:07:24,250 --> 02:07:26,110
dual of the dual is the original

1862
02:07:26,160 --> 02:07:28,310
and you see here

1863
02:07:28,320 --> 02:07:30,010
that as well because

1864
02:07:30,030 --> 02:07:32,430
if you take the complex conjugate

1865
02:07:32,440 --> 02:07:36,500
conversely operation applied twice you would go back to here and then this argument would

1866
02:07:36,500 --> 02:07:37,960
switch once more

1867
02:07:38,010 --> 02:07:41,210
and go to the other domain and you would end up here

1868
02:07:41,230 --> 02:07:42,620
so you can kind of see

1869
02:07:42,660 --> 02:07:46,100
that if you apply twice to back to the original

1870
02:07:46,110 --> 02:07:48,470
so this is equal to this

1871
02:07:48,480 --> 02:07:51,360
so what i have i would have i done so far

1872
02:07:51,400 --> 02:07:53,630
and i'm going to go through examples again

1873
02:07:53,650 --> 02:08:00,100
the moment the same kind of example i defined exponential families division very simple form

1874
02:08:00,150 --> 02:08:04,310
this is the convex function to characterize the family to make sure that things integrate

1875
02:08:04,310 --> 02:08:05,580
to one

1876
02:08:05,590 --> 02:08:11,180
i defined another parameter the expectation parameter which is supposedly the derivative of the cumulant

1877
02:08:12,930 --> 02:08:16,420
we find a second convex function convex conjugate

1878
02:08:16,440 --> 02:08:22,840
differential case you don't need to so you have an explicit formula

1879
02:08:22,920 --> 02:08:29,860
and again went through a bunch of examples and then established this relationship

1880
02:08:29,880 --> 02:08:35,500
if you never ever comics come a exponential family if you compute relative entropy is

1881
02:08:35,500 --> 02:08:36,730
you get

1882
02:08:36,770 --> 02:08:38,610
bregman divergences

1883
02:08:38,630 --> 02:08:43,250
but it is not true that an arbitrary bregman divergence corresponds to an exponential family

1884
02:08:43,260 --> 02:08:47,820
not all intuitively what you need to assure for it to be an exponential family

1885
02:08:47,820 --> 02:08:50,010
is that the that

1886
02:08:50,060 --> 02:08:55,150
the there's some kind of probabilistic interpretation and something integrates to one

1887
02:08:55,160 --> 02:09:01,930
for arbitrary bregman divergences starts to start with a convex function

1888
02:09:05,260 --> 02:09:06,910
this form

1889
02:09:09,070 --> 02:09:15,190
this form of the bregman divergence convex functions linear approximation was popularized by bregman

1890
02:09:15,210 --> 02:09:18,680
and i actually

1891
02:09:18,680 --> 02:09:21,780
discovered or rediscovered this family

1892
02:09:21,790 --> 02:09:24,600
in a different way as an integral

1893
02:09:24,630 --> 02:09:27,410
it turns out

1894
02:09:28,900 --> 02:09:32,820
if you write down the sigmoid function and you compute these kind of integrals underneath

1895
02:09:32,820 --> 02:09:36,070
the sigmoid to become relative entropy is and i was

1896
02:09:36,120 --> 02:09:40,180
that's how i rediscovered this in more general terms

1897
02:09:40,200 --> 02:09:42,970
if you have a little g function

1898
02:09:42,980 --> 02:09:45,010
and integrate from

1899
02:09:45,020 --> 02:09:46,770
theta twiddle

1900
02:09:48,080 --> 02:09:50,860
see table is theta here

1901
02:09:53,340 --> 02:09:58,230
integral on the nice happens to be the bregman divergence

1902
02:09:58,250 --> 02:10:00,980
this is the kind of the matching loss type

1903
02:10:01,240 --> 02:10:04,990
motivation that i talked about already

1904
02:10:06,950 --> 02:10:09,100
before but bregman divergences

1905
02:10:09,100 --> 02:10:12,890
and even convicts conjugacy in all of these things

1906
02:10:12,940 --> 02:10:16,630
i this version there's very intuitive version of

